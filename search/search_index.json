{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-\\.]+$"},"docs":[{"location":"","text":"\u8bba\u6587\u9605\u8bfb \u5bf9\u8fdb\u884c\u7684\u8bba\u6587\u9605\u8bfb\u8fdb\u884c\u8bb0\u5f55\u3002 \u6ce8\u610f\u7531\u4e8e\u76ee\u524d\u5b58\u50a8\u7684\u8bba\u6587\u6570\u91cf\u8d8a\u6765\u8d8a\u591a\uff0c\u5982\u679c\u60f3\u8981\u67e5\u627e\u67d0\u4e9b\u7279\u5b9a\u7684\u8bba\u6587\u5efa\u8bae\u6309's'\u952e\u6216\u8005\u70b9\u51fb\u53f3\u4e0a\u65b9\u7684\"search\"\u6309\u94ae\uff0c\u8f93\u5165\u5bf9\u5e94\u82f1\u6587\u9898\u76ee\u8fdb\u884c\u641c\u7d22\u3002 \u6587\u7ae0\u5404\u81ea\u7684\u76ee\u5f55\u4e2d\u4f1a\u8868\u660epdf\u4ee5\u53ca\u5f00\u6e90\u4ee3\u7801(\u5982\u6709)\u7684\u5916\u94fe\u3002 This volume will be basically written in Chinese. The target is to provide fast reference. Papers are now sorted in Several categories 3D object detection. mainly on monocular detection and also many papers in sensor fusion. Building Blocks for neural networks that boost performance Robotics with DL: using deep learning related technology to boost robotics algorithms The Theory: General advices for understanding and boosting the performance of deep learning other interesting or useful papers including 1. 2D object detection 2. Advanced SLAM 3. Segmentation 4. End to end navigation \u63a8\u8350\u7684\u7efc\u8ff0\u578b\u9605\u8bfb Monocular 3D Object Detection-KITTI Stereo 3D Object Detection-KITTI Stereo Matching - KITTI Yolov4 & Review of Structure and Tricks for Object Detection Comprehensive Practiacl Cookbook for visualDet3D Github Github \u7f51\u9875 \u6b22\u8fce\u5171\u540c\u8bb0\u5f55\u8bba\u6587\u9605\u8bfb,\u5bf9\u6587\u7ae0\u7684\u89e3\u8bfb\u6709\u8d28\u7591\u548c\u8865\u5145\u7684\u5efa\u8bae\u63d0\u4ea4github issue. Comments (since 2022.01.28) \u672c\u535a\u5ba2\u7ad9\u4f7f\u7528 utterances ,\u5bf9\u6bcf\u4e00\u7bc7\u6587\u7ae0\u5f00\u542f\u5355\u72ec\u7684\u8bc4\u8bba\u533a.","title":"\u8bba\u6587\u9605\u8bfb"},{"location":"#_1","text":"\u5bf9\u8fdb\u884c\u7684\u8bba\u6587\u9605\u8bfb\u8fdb\u884c\u8bb0\u5f55\u3002 \u6ce8\u610f\u7531\u4e8e\u76ee\u524d\u5b58\u50a8\u7684\u8bba\u6587\u6570\u91cf\u8d8a\u6765\u8d8a\u591a\uff0c\u5982\u679c\u60f3\u8981\u67e5\u627e\u67d0\u4e9b\u7279\u5b9a\u7684\u8bba\u6587\u5efa\u8bae\u6309's'\u952e\u6216\u8005\u70b9\u51fb\u53f3\u4e0a\u65b9\u7684\"search\"\u6309\u94ae\uff0c\u8f93\u5165\u5bf9\u5e94\u82f1\u6587\u9898\u76ee\u8fdb\u884c\u641c\u7d22\u3002 \u6587\u7ae0\u5404\u81ea\u7684\u76ee\u5f55\u4e2d\u4f1a\u8868\u660epdf\u4ee5\u53ca\u5f00\u6e90\u4ee3\u7801(\u5982\u6709)\u7684\u5916\u94fe\u3002 This volume will be basically written in Chinese. The target is to provide fast reference. Papers are now sorted in Several categories 3D object detection. mainly on monocular detection and also many papers in sensor fusion. Building Blocks for neural networks that boost performance Robotics with DL: using deep learning related technology to boost robotics algorithms The Theory: General advices for understanding and boosting the performance of deep learning other interesting or useful papers including 1. 2D object detection 2. Advanced SLAM 3. Segmentation 4. End to end navigation","title":"\u8bba\u6587\u9605\u8bfb"},{"location":"#_2","text":"","title":"\u63a8\u8350\u7684\u7efc\u8ff0\u578b\u9605\u8bfb"},{"location":"#monocular-3d-object-detection-kitti","text":"","title":"Monocular 3D Object Detection-KITTI"},{"location":"#stereo-3d-object-detection-kitti","text":"","title":"Stereo 3D Object Detection-KITTI"},{"location":"#stereo-matching-kitti","text":"","title":"Stereo Matching - KITTI"},{"location":"#yolov4-review-of-structure-and-tricks-for-object-detection","text":"","title":"Yolov4 &amp; Review of Structure and Tricks for Object Detection"},{"location":"#comprehensive-practiacl-cookbook-for-visualdet3d","text":"","title":"Comprehensive Practiacl Cookbook for visualDet3D"},{"location":"#github","text":"Github \u7f51\u9875 \u6b22\u8fce\u5171\u540c\u8bb0\u5f55\u8bba\u6587\u9605\u8bfb,\u5bf9\u6587\u7ae0\u7684\u89e3\u8bfb\u6709\u8d28\u7591\u548c\u8865\u5145\u7684\u5efa\u8bae\u63d0\u4ea4github issue.","title":"Github"},{"location":"#comments-since-20220128","text":"\u672c\u535a\u5ba2\u7ad9\u4f7f\u7528 utterances ,\u5bf9\u6bcf\u4e00\u7bc7\u6587\u7ae0\u5f00\u542f\u5355\u72ec\u7684\u8bc4\u8bba\u533a.","title":"Comments (since 2022.01.28)"},{"location":"3dDetection/AFDet/","text":"CenterNet for Point cloud \u8fd9\u91cc\u5f15\u5165\u4e24\u7bc7paper\uff0c\u90fd\u4f7f\u7528 object as point \u4f5c\u4e3a\u57fa\u7840\u8fdb\u884cobject 3D detection. Center-based 3D Object Detection and Tracking pdf code \u8fd9\u7bc7\u662fXingyi Zhou\u7ec4\u7684\u5b98\u65b9\u5ef6\u7eed\u4ee3\u7801\uff0c\u4e3b\u8981\u4fee\u6539\uff0cHeat Map\u8c03\u6574\u4e86 \u6b63\u6837\u672c\u7684\u8986\u76d6\u9762\u79ef\u3002Detection head\u589e\u52a0\u4e86\u53ef\u53d8\u5f62\u5377\u79ef\u3002\u4f7f\u7528\u57fa\u4e8e\u534a\u5f84\u7684NMS\u53d6\u4ee3\u57fa\u4e8eIOU\u7684NMS\uff1b\u4e24\u8f74\u7ffb\u8f6c\u7684\u6570\u636e\u589e\u5f3a AFDet: Anchor Free One Stage 3D Object Detection pdf","title":"CenterNet for Point cloud"},{"location":"3dDetection/AFDet/#centernet-for-point-cloud","text":"\u8fd9\u91cc\u5f15\u5165\u4e24\u7bc7paper\uff0c\u90fd\u4f7f\u7528 object as point \u4f5c\u4e3a\u57fa\u7840\u8fdb\u884cobject 3D detection.","title":"CenterNet for Point cloud"},{"location":"3dDetection/AFDet/#center-based-3d-object-detection-and-tracking","text":"pdf code \u8fd9\u7bc7\u662fXingyi Zhou\u7ec4\u7684\u5b98\u65b9\u5ef6\u7eed\u4ee3\u7801\uff0c\u4e3b\u8981\u4fee\u6539\uff0cHeat Map\u8c03\u6574\u4e86 \u6b63\u6837\u672c\u7684\u8986\u76d6\u9762\u79ef\u3002Detection head\u589e\u52a0\u4e86\u53ef\u53d8\u5f62\u5377\u79ef\u3002\u4f7f\u7528\u57fa\u4e8e\u534a\u5f84\u7684NMS\u53d6\u4ee3\u57fa\u4e8eIOU\u7684NMS\uff1b\u4e24\u8f74\u7ffb\u8f6c\u7684\u6570\u636e\u589e\u5f3a","title":"Center-based 3D Object Detection and Tracking"},{"location":"3dDetection/AFDet/#afdet-anchor-free-one-stage-3d-object-detection","text":"pdf","title":"AFDet: Anchor Free One Stage 3D Object Detection"},{"location":"3dDetection/AM3D/","text":"Accurate Monocular Object Detection via Color-Embedded 3D Reconstruction for Autonomous Driving \u8fd9\u7bc7\u6587\u7ae0\u5229\u7528\u4e86depth estimation\u4ee5\u53caPseudo lidar\u7684\u601d\u8def\uff0c\u662f\u76ee\u524d\u5355\u76ee\u89c6\u89c93D\u68c0\u6d4b\u7684SOTA,\u4f46\u662f\u989d\u5916\u51fa\u5f69\u7684\u5730\u65b9\u5728\u4e8e\u5176\u5bf9RGB\u4fe1\u606f\u4ee5\u53ca\u6df1\u5ea6\u4fe1\u606f\u878d\u5408\u65f6\u7684\u505a\u6cd5\u3002 \u4e3b\u8981\u7ed3\u6784\u4e0e\u6d41\u7a0b\u56fe \u5bf9\u5355\u4e00\u56fe\u50cf\u5206\u522b\u4f7f\u7528RPN\u63d0\u53d6RoI\u4ee5\u53ca\u4f7f\u7528\u6df1\u5ea6\u7f51\u7edc\u751f\u6210\u6df1\u5ea6\u4fe1\u606f\uff0c\u7136\u540e\u5bf9\u63d0\u53d6\u51fa\u6765\u7684\u533a\u57df\uff0c\u8fdb\u884c\u7b80\u5355\u7684\u6570\u636e\u8f6c\u6362\u3001\u5206\u5272\u3001\u878d\u5408\u68c0\u6d4b\u56de\u5f52 \u5c40\u90e8\u8be6\u89e3 \u6570\u636e\u8f6c\u6362 \u5df2\u77e5\u6df1\u5ea6d\uff0c\u4f7f\u7528 \\left\\{\\begin{array}{l}{z=d} \\\\ {x=\\left(u-C_{x}\\right) * z / f} \\\\ {y=\\left(v-C_{y}\\right) * z / f}\\end{array}\\right. \u5c06RoI\u533a\u57df\u8f6c\u6362\u4e3a\u76ee\u6807\u70b9\u4e91(\u5bc6\u96c6\u8868\u8fbe)\u3002 \u70b9\u4e91\u5206\u5272 \u65b9\u6cd5\u7b80\u5355\uff0c\u5728RoI\u533a\u57df\u5185\uff0c\u6c42\u51fa\u6df1\u5ea6\u5e73\u5747\u503c\uff0c\u6bd4\u5747\u503c\u9ad8\u51fa\u4e00\u4e2a\u9608\u503c\u4ee5\u4e0a\u7684\u5224\u65ad\u4e3a\u80cc\u666f\u70b9 S^{\\prime}=\\left\\{p | p_{v} \\leq \\frac{\\sum_{p \\in S} p_{v}}{|S|}+r, p \\in S\\right\\} \u5e76\u4ece\u4e2d\u968f\u673a\u9009\u53d6\u4e00\u5b9a\u91cf\u7684\u70b9\u4f5c\u4e3a\u8f93\u51fa\u70b9\uff0c\u5927\u5c0f\u4e00\u81f4\u4fbf\u4e8e\u540e\u671f\u5904\u7406 \u5750\u6807\u8f6c\u6362 \u7528\u4e00\u4e2a\u7ecf\u5178 \u6587\u7ae0 \u7684\u65b9\u6cd5\uff0c\u5229\u7528\u4e00\u4e2a\u5c0f\u7f51\u7edc\u4f30\u8ba1\u51faRoI\u7684\u4e2d\u5fc3\uff0c\u7136\u540e\u5c06\u6240\u6709\u70b9\u8f6c\u5230\u76f8\u5bf9\u5750\u6807\u7cfb\u4e2d S^{\\prime \\prime}=\\left\\{p | p-\\delta, p \\in S^{\\prime}\\right\\} \u7136\u540e\u7528pointnet\u9884\u6d4b \u878d\u5408 \u7528\u5982\u56fe\u65b9\u5f0f\u878d\u5408\u70b9\u4e91\u4ee5\u53ca\u5f69\u8272\u70b9\u3002 G \u662f\u4e00\u4e2a\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u5f97\u5230\u5b83\u7684\u516c\u5f0f\u662f G = \\sigma(f([F^{xyz}_{max}, F^{xyz}_{avg}])) ,\u6709\u4e00\u70b9 CBAM \u7684\u6548\u679c\uff0c \u540e\u9762\u7684\u70b9\u4e58\u4e0e\u76f8\u52a0\u53ef\u4ee5\u5199\u6210 \\mathbf{F}^{x y z} \\leftarrow \\mathbf{F}^{x y z}+\\mathbf{G} \\odot \\mathbf{F}^{r g b} \u53e6\u4e00\u65b9\u9762\uff0c\u4ece\u56fe\u4e2d\u76842D RoI\u4e2d\uff0c\u4f7f\u7528RoIAlign\u8c03\u5230 128\\times 128 \uff0c\u7528CNN\u63d0\u53d6\u4e00\u4e2a\u7279\u5f81\u5411\u91cf\uff0c\u5e76\u8054 \u8bad\u7ec3\u7ec6\u8282 \u4e24\u9636\u6bb5\u8bad\u7ec3\uff0c\u5148\u8bad\u7ec3\u4e24\u4e2a\u4e2d\u95f4\u7f51\u7edc\uff0c\u7136\u540e\u53ea\u75283D\u4fe1\u606f\u8bad\u7ec3\u6574\u4e2a\u7f51\u7edc \u635f\u5931\u51fd\u6570\u5305\u62ec\uff0c\u4e2d\u95f4\u8f7b\u91cf\u7ea7\u7684\u5bf9center\u7684\u4f30\u8ba1\uff0c\u9884\u6d4b\u8f93\u51fa\u7684corner loss(8\u4e2a\u89d2)","title":"Accurate Monocular Object Detection via Color-Embedded 3D Reconstruction for Autonomous Driving"},{"location":"3dDetection/AM3D/#accurate-monocular-object-detection-via-color-embedded-3d-reconstruction-for-autonomous-driving","text":"\u8fd9\u7bc7\u6587\u7ae0\u5229\u7528\u4e86depth estimation\u4ee5\u53caPseudo lidar\u7684\u601d\u8def\uff0c\u662f\u76ee\u524d\u5355\u76ee\u89c6\u89c93D\u68c0\u6d4b\u7684SOTA,\u4f46\u662f\u989d\u5916\u51fa\u5f69\u7684\u5730\u65b9\u5728\u4e8e\u5176\u5bf9RGB\u4fe1\u606f\u4ee5\u53ca\u6df1\u5ea6\u4fe1\u606f\u878d\u5408\u65f6\u7684\u505a\u6cd5\u3002","title":"Accurate Monocular Object Detection via Color-Embedded 3D Reconstruction for Autonomous Driving"},{"location":"3dDetection/AM3D/#_1","text":"\u5bf9\u5355\u4e00\u56fe\u50cf\u5206\u522b\u4f7f\u7528RPN\u63d0\u53d6RoI\u4ee5\u53ca\u4f7f\u7528\u6df1\u5ea6\u7f51\u7edc\u751f\u6210\u6df1\u5ea6\u4fe1\u606f\uff0c\u7136\u540e\u5bf9\u63d0\u53d6\u51fa\u6765\u7684\u533a\u57df\uff0c\u8fdb\u884c\u7b80\u5355\u7684\u6570\u636e\u8f6c\u6362\u3001\u5206\u5272\u3001\u878d\u5408\u68c0\u6d4b\u56de\u5f52","title":"\u4e3b\u8981\u7ed3\u6784\u4e0e\u6d41\u7a0b\u56fe"},{"location":"3dDetection/AM3D/#_2","text":"","title":"\u5c40\u90e8\u8be6\u89e3"},{"location":"3dDetection/AM3D/#_3","text":"\u5df2\u77e5\u6df1\u5ea6d\uff0c\u4f7f\u7528 \\left\\{\\begin{array}{l}{z=d} \\\\ {x=\\left(u-C_{x}\\right) * z / f} \\\\ {y=\\left(v-C_{y}\\right) * z / f}\\end{array}\\right. \u5c06RoI\u533a\u57df\u8f6c\u6362\u4e3a\u76ee\u6807\u70b9\u4e91(\u5bc6\u96c6\u8868\u8fbe)\u3002","title":"\u6570\u636e\u8f6c\u6362"},{"location":"3dDetection/AM3D/#_4","text":"\u65b9\u6cd5\u7b80\u5355\uff0c\u5728RoI\u533a\u57df\u5185\uff0c\u6c42\u51fa\u6df1\u5ea6\u5e73\u5747\u503c\uff0c\u6bd4\u5747\u503c\u9ad8\u51fa\u4e00\u4e2a\u9608\u503c\u4ee5\u4e0a\u7684\u5224\u65ad\u4e3a\u80cc\u666f\u70b9 S^{\\prime}=\\left\\{p | p_{v} \\leq \\frac{\\sum_{p \\in S} p_{v}}{|S|}+r, p \\in S\\right\\} \u5e76\u4ece\u4e2d\u968f\u673a\u9009\u53d6\u4e00\u5b9a\u91cf\u7684\u70b9\u4f5c\u4e3a\u8f93\u51fa\u70b9\uff0c\u5927\u5c0f\u4e00\u81f4\u4fbf\u4e8e\u540e\u671f\u5904\u7406","title":"\u70b9\u4e91\u5206\u5272"},{"location":"3dDetection/AM3D/#_5","text":"","title":""},{"location":"3dDetection/AM3D/#_6","text":"\u7528\u4e00\u4e2a\u7ecf\u5178 \u6587\u7ae0 \u7684\u65b9\u6cd5\uff0c\u5229\u7528\u4e00\u4e2a\u5c0f\u7f51\u7edc\u4f30\u8ba1\u51faRoI\u7684\u4e2d\u5fc3\uff0c\u7136\u540e\u5c06\u6240\u6709\u70b9\u8f6c\u5230\u76f8\u5bf9\u5750\u6807\u7cfb\u4e2d S^{\\prime \\prime}=\\left\\{p | p-\\delta, p \\in S^{\\prime}\\right\\} \u7136\u540e\u7528pointnet\u9884\u6d4b","title":"\u5750\u6807\u8f6c\u6362"},{"location":"3dDetection/AM3D/#_7","text":"\u7528\u5982\u56fe\u65b9\u5f0f\u878d\u5408\u70b9\u4e91\u4ee5\u53ca\u5f69\u8272\u70b9\u3002 G \u662f\u4e00\u4e2a\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u5f97\u5230\u5b83\u7684\u516c\u5f0f\u662f G = \\sigma(f([F^{xyz}_{max}, F^{xyz}_{avg}])) ,\u6709\u4e00\u70b9 CBAM \u7684\u6548\u679c\uff0c \u540e\u9762\u7684\u70b9\u4e58\u4e0e\u76f8\u52a0\u53ef\u4ee5\u5199\u6210 \\mathbf{F}^{x y z} \\leftarrow \\mathbf{F}^{x y z}+\\mathbf{G} \\odot \\mathbf{F}^{r g b} \u53e6\u4e00\u65b9\u9762\uff0c\u4ece\u56fe\u4e2d\u76842D RoI\u4e2d\uff0c\u4f7f\u7528RoIAlign\u8c03\u5230 128\\times 128 \uff0c\u7528CNN\u63d0\u53d6\u4e00\u4e2a\u7279\u5f81\u5411\u91cf\uff0c\u5e76\u8054","title":"\u878d\u5408"},{"location":"3dDetection/AM3D/#_8","text":"\u4e24\u9636\u6bb5\u8bad\u7ec3\uff0c\u5148\u8bad\u7ec3\u4e24\u4e2a\u4e2d\u95f4\u7f51\u7edc\uff0c\u7136\u540e\u53ea\u75283D\u4fe1\u606f\u8bad\u7ec3\u6574\u4e2a\u7f51\u7edc \u635f\u5931\u51fd\u6570\u5305\u62ec\uff0c\u4e2d\u95f4\u8f7b\u91cf\u7ea7\u7684\u5bf9center\u7684\u4f30\u8ba1\uff0c\u9884\u6d4b\u8f93\u51fa\u7684corner loss(8\u4e2a\u89d2)","title":"\u8bad\u7ec3\u7ec6\u8282"},{"location":"3dDetection/CDN/","text":"Wasserstein Distances for Stereo Disparity Estimation \u8fd9\u7bc7paper\u63d0\u51fa\u4e86\u53cc\u76ee\u5339\u914d\u7684\u4e00\u4e2a\u65b0\u7684\u8f93\u51fa\u65b9\u5f0f\u4e0e\u8bad\u7ec3\u65b9\u5f0f\uff0c\u4f5c\u8005\u5c06\u5176\u66ff\u6362\u5230\u4e0d\u540c\u7684\u53cc\u76ee\u5339\u914d\u4ee5\u53ca\u53cc\u76ee\u68c0\u6d4b\u7684\u65b9\u6848\u4e2d\uff0c\u90fd\u5f97\u5230\u4e86\u76f4\u63a5\u7684\u6027\u80fd\u63d0\u5347\u3002 \u8f93\u51fa\u65b9\u5f0f \u6b64\u524d\u7684\u53cc\u76ee\u5339\u914d\u662f\u5efa\u7acb\u5728\u5355\u6a21\u9884\u6d4b\u7684\u5047\u8bbe\u4e0a\u7684\uff0c\u6bcf\u4e00\u4e2a\u70b9\u9884\u6d4b\u4e00\u4e2a\u6982\u7387\uff0c\u7136\u540e\u671f\u671b\u5c06\u7ed3\u679c\u8bad\u7ec3\u6210\u4e3a\u4e00\u4e2a\u5355\u5cf0\u7684\u9ad8\u65af\u5206\u5e03\u3002 \u4f46\u662f\u4f5c\u8005\u6307\u51fa\uff0c\u5728\u7269\u4f53\u8fb9\u754c\u4e0a\u7684\u70b9\uff0c\u5927\u6982\u7387\u5f97\u5230\u7684disparity\u662f\u4e00\u4e2a\u53cc\u6a21\u7684\u5206\u5e03(disparity\u53ef\u80fd\u5bf9\u5e94\u524d\u666f\u4e5f\u53ef\u80fd\u5bf9\u5e94\u540e\u666f)\uff0c\u5982\u679c\u76f4\u63a5\u5bf9\u7ed3\u679c\u8fdb\u884c\u52a0\u6743\u5e73\u5747\uff0c\u5c31\u4f1a\u89c2\u5bdf\u5230\u50cf P-lidar \u6587\u7ae0\u56fe\u7247\u4e2d\u770b\u5230\u7684\u60c5\u51b5\uff0c\u4e5f\u5c31\u662f\u8fb9\u7f18\u70b9\u6d12\u843d\u5728\u7269\u4f53\u524d\u666f\u4e0e\u540e\u666f\u4e4b\u95f4\u3002 \u4f5c\u8005\u7ed9\u51fa\u7684\u65b9\u6848\u662f\u8ba9\u6bcf\u4e00\u4e2abase disparity\u72ec\u7acb\u5730\u8ba1\u7b97ground truth\u4e0e\u81ea\u5df1\u7684offset\u4ee5\u53ca\u6982\u7387\uff0c\u7f51\u7edc\u7684\u8f93\u51fa\u5219\u662f\u76f4\u63a5\u8f93\u51fa\u5176\u4e2d\u6982\u7387\u6700\u9ad8\u7684\u6a21\u6001\u3002 \u8bad\u7ec3\u65b9\u5f0f \u5bf9\u4e8e\u5355\u6a21\u6001\u8bad\u7ec3\uff0c\u4f5c\u8005\u63d0\u51fa\u4f7f\u7528wasserstein\u8ddd\u79bb\u8fdb\u884c\u8bad\u7ec3: \\begin{aligned} W_{p}\\left(\\tilde{p}, p^{\\star}\\right)=\\left(\\mathbb{E}_{\\tilde{p}}\\left\\|d^{\\prime}-d^{*}\\right\\|^{p}\\right)^{1 / p} &=\\left(\\sum_{d \\in \\mathcal{D}} p(d \\mid u, v)\\left\\|d+b(u, v, d)-d^{\\star}\\right\\|^{p}\\right)^{1 / p} \\\\ &=\\left(\\sum_{d \\in \\mathcal{D}} \\operatorname{softmax}\\left(-S_{\\text {disp }}(u, v, d)\\right)\\left\\|d+b(u, v, d)-d^{\\star}\\right\\|^{p}\\right)^{1 / p} \\end{aligned}","title":"Wasserstein Distances for Stereo Disparity Estimation"},{"location":"3dDetection/CDN/#wasserstein-distances-for-stereo-disparity-estimation","text":"\u8fd9\u7bc7paper\u63d0\u51fa\u4e86\u53cc\u76ee\u5339\u914d\u7684\u4e00\u4e2a\u65b0\u7684\u8f93\u51fa\u65b9\u5f0f\u4e0e\u8bad\u7ec3\u65b9\u5f0f\uff0c\u4f5c\u8005\u5c06\u5176\u66ff\u6362\u5230\u4e0d\u540c\u7684\u53cc\u76ee\u5339\u914d\u4ee5\u53ca\u53cc\u76ee\u68c0\u6d4b\u7684\u65b9\u6848\u4e2d\uff0c\u90fd\u5f97\u5230\u4e86\u76f4\u63a5\u7684\u6027\u80fd\u63d0\u5347\u3002","title":"Wasserstein Distances for Stereo Disparity Estimation"},{"location":"3dDetection/CDN/#_1","text":"\u6b64\u524d\u7684\u53cc\u76ee\u5339\u914d\u662f\u5efa\u7acb\u5728\u5355\u6a21\u9884\u6d4b\u7684\u5047\u8bbe\u4e0a\u7684\uff0c\u6bcf\u4e00\u4e2a\u70b9\u9884\u6d4b\u4e00\u4e2a\u6982\u7387\uff0c\u7136\u540e\u671f\u671b\u5c06\u7ed3\u679c\u8bad\u7ec3\u6210\u4e3a\u4e00\u4e2a\u5355\u5cf0\u7684\u9ad8\u65af\u5206\u5e03\u3002 \u4f46\u662f\u4f5c\u8005\u6307\u51fa\uff0c\u5728\u7269\u4f53\u8fb9\u754c\u4e0a\u7684\u70b9\uff0c\u5927\u6982\u7387\u5f97\u5230\u7684disparity\u662f\u4e00\u4e2a\u53cc\u6a21\u7684\u5206\u5e03(disparity\u53ef\u80fd\u5bf9\u5e94\u524d\u666f\u4e5f\u53ef\u80fd\u5bf9\u5e94\u540e\u666f)\uff0c\u5982\u679c\u76f4\u63a5\u5bf9\u7ed3\u679c\u8fdb\u884c\u52a0\u6743\u5e73\u5747\uff0c\u5c31\u4f1a\u89c2\u5bdf\u5230\u50cf P-lidar \u6587\u7ae0\u56fe\u7247\u4e2d\u770b\u5230\u7684\u60c5\u51b5\uff0c\u4e5f\u5c31\u662f\u8fb9\u7f18\u70b9\u6d12\u843d\u5728\u7269\u4f53\u524d\u666f\u4e0e\u540e\u666f\u4e4b\u95f4\u3002 \u4f5c\u8005\u7ed9\u51fa\u7684\u65b9\u6848\u662f\u8ba9\u6bcf\u4e00\u4e2abase disparity\u72ec\u7acb\u5730\u8ba1\u7b97ground truth\u4e0e\u81ea\u5df1\u7684offset\u4ee5\u53ca\u6982\u7387\uff0c\u7f51\u7edc\u7684\u8f93\u51fa\u5219\u662f\u76f4\u63a5\u8f93\u51fa\u5176\u4e2d\u6982\u7387\u6700\u9ad8\u7684\u6a21\u6001\u3002","title":"\u8f93\u51fa\u65b9\u5f0f"},{"location":"3dDetection/CDN/#_2","text":"\u5bf9\u4e8e\u5355\u6a21\u6001\u8bad\u7ec3\uff0c\u4f5c\u8005\u63d0\u51fa\u4f7f\u7528wasserstein\u8ddd\u79bb\u8fdb\u884c\u8bad\u7ec3: \\begin{aligned} W_{p}\\left(\\tilde{p}, p^{\\star}\\right)=\\left(\\mathbb{E}_{\\tilde{p}}\\left\\|d^{\\prime}-d^{*}\\right\\|^{p}\\right)^{1 / p} &=\\left(\\sum_{d \\in \\mathcal{D}} p(d \\mid u, v)\\left\\|d+b(u, v, d)-d^{\\star}\\right\\|^{p}\\right)^{1 / p} \\\\ &=\\left(\\sum_{d \\in \\mathcal{D}} \\operatorname{softmax}\\left(-S_{\\text {disp }}(u, v, d)\\right)\\left\\|d+b(u, v, d)-d^{\\star}\\right\\|^{p}\\right)^{1 / p} \\end{aligned}","title":"\u8bad\u7ec3\u65b9\u5f0f"},{"location":"3dDetection/CameraDistanceAware/","text":"Camera Distance-aware Top-down Approach for 3D Multi-person Pose Estimation from a Single RGB Image \u8fd9\u7bc7\u8bba\u6587\u91c7\u7528\u4e86\u57fa\u4e8e\u51e0\u4f55\u4e0e\u76f8\u673a\u7684\u7279\u6027\u7684\u65b9\u5f0f\u5bf9\u4eba\u7684\u80a2\u4f53\u8fdb\u884c3D detection\uff0cpose estimation\u7684\u8f93\u51fa\u662f\u5173\u8282\u5750\u6807\uff0c\u672c\u6587\u8fdb\u4e00\u6b65\u9700\u8981\u8ba1\u7b97\u4eba\u4e0e\u76f8\u673a\u7684\u8ddd\u79bb\u3002\u4ee3\u7801\u5206\u4e24\u90e8\u5206\u5f00\u6e90 \u5206\u522b\u662f rootnet \u4e0e posenet . \u603b\u4f53\u6d41\u7a0b \u6d41\u7a0b\u5206\u4e3a\u4e09\u4e2a\u7f51\u7edc\uff0c\u7b2c\u4e00\u4e2a\u7f51\u7edc\u4e3aDetectNet,\u7b80\u5355\u6765\u8bf4\u5c31\u662ftwo-stage object detection\u7684proposal\u9636\u6bb5\uff0c\u91c7\u7528\u7684res\u7f51\u7edc\u4ee5\u53caroialign\u64cd\u4f5c\u7686\u4e3a\u5e38\u89c4\u3002 \u7b2c\u4e8c\u4e2a\u7f51\u7edc\u4e3arootNet\u4e3b\u8981\u8d1f\u8d23\u9884\u6d4b\u4eba\u4f53\u57283\u7ef4\u7a7a\u95f4\u4e2d\u7684\u4f4d\u7f6e\uff0c\u5728\u4e0b\u6587\u8bb2\u89e3\u3002 \u7b2c\u4e09\u4e2a\u7f51\u7edc\u4e3aPoseNet\uff0c\u91c7\u7528\u7684\u7c7b\u4f3c\u4e8e\u5e38\u89c4\u7684pose-estimation\u7f51\u7edc\uff0c\u8f93\u51fa\u7684\u662f\u5c0f\u56fe\u7247\u4e2d\u4eba\u4f53\u5404\u4e2a\u5173\u8282\u7684heatmap\u3002 RootNet rootnet\u4e3b\u8981\u89c2\u6d4b\u7684\u662f\u56fe\u7247\u4e2d2D\u6846\u5927\u5c0f\u4e0e\u6df1\u5ea6\u7684\u4e00\u4e2a\u76f8\u5173\u6027 \u7f51\u7edc\u7ed3\u6784\u5982\u4e0b\uff0c k=\\sqrt{\\alpha_{x} \\alpha_{y} \\frac{A_{r e a l}}{A_{i m g}}} \u5176\u4e2d \\alpha_{x} \u4e3a\u76f8\u673a\u5185\u53c2\u7684 f_x , A \u5206\u522b\u4e3a\u5b9e\u9645\u9762\u79ef\u5927\u5c0f\u4e0e\u56fe\u7247\u4e2d\u7684\u5927\u5c0f\u3002 A_{real} \u4e3a\u4e00\u4e2a\u4f30\u8ba1\u503c\uff0c\u7f51\u7edc\u7684\u8d23\u4efb\u5728\u4e8e\u4f30\u8ba1\u8fd9\u4e2ak\u503c.","title":"Camera Distance-aware Top-down Approach for 3D Multi-person Pose Estimation from a Single RGB Image"},{"location":"3dDetection/CameraDistanceAware/#camera-distance-aware-top-down-approach-for-3d-multi-person-pose-estimation-from-a-single-rgb-image","text":"\u8fd9\u7bc7\u8bba\u6587\u91c7\u7528\u4e86\u57fa\u4e8e\u51e0\u4f55\u4e0e\u76f8\u673a\u7684\u7279\u6027\u7684\u65b9\u5f0f\u5bf9\u4eba\u7684\u80a2\u4f53\u8fdb\u884c3D detection\uff0cpose estimation\u7684\u8f93\u51fa\u662f\u5173\u8282\u5750\u6807\uff0c\u672c\u6587\u8fdb\u4e00\u6b65\u9700\u8981\u8ba1\u7b97\u4eba\u4e0e\u76f8\u673a\u7684\u8ddd\u79bb\u3002\u4ee3\u7801\u5206\u4e24\u90e8\u5206\u5f00\u6e90 \u5206\u522b\u662f rootnet \u4e0e posenet .","title":"Camera Distance-aware Top-down Approach for 3D Multi-person Pose Estimation from a Single RGB Image"},{"location":"3dDetection/CameraDistanceAware/#_1","text":"\u6d41\u7a0b\u5206\u4e3a\u4e09\u4e2a\u7f51\u7edc\uff0c\u7b2c\u4e00\u4e2a\u7f51\u7edc\u4e3aDetectNet,\u7b80\u5355\u6765\u8bf4\u5c31\u662ftwo-stage object detection\u7684proposal\u9636\u6bb5\uff0c\u91c7\u7528\u7684res\u7f51\u7edc\u4ee5\u53caroialign\u64cd\u4f5c\u7686\u4e3a\u5e38\u89c4\u3002 \u7b2c\u4e8c\u4e2a\u7f51\u7edc\u4e3arootNet\u4e3b\u8981\u8d1f\u8d23\u9884\u6d4b\u4eba\u4f53\u57283\u7ef4\u7a7a\u95f4\u4e2d\u7684\u4f4d\u7f6e\uff0c\u5728\u4e0b\u6587\u8bb2\u89e3\u3002 \u7b2c\u4e09\u4e2a\u7f51\u7edc\u4e3aPoseNet\uff0c\u91c7\u7528\u7684\u7c7b\u4f3c\u4e8e\u5e38\u89c4\u7684pose-estimation\u7f51\u7edc\uff0c\u8f93\u51fa\u7684\u662f\u5c0f\u56fe\u7247\u4e2d\u4eba\u4f53\u5404\u4e2a\u5173\u8282\u7684heatmap\u3002","title":"\u603b\u4f53\u6d41\u7a0b"},{"location":"3dDetection/CameraDistanceAware/#rootnet","text":"rootnet\u4e3b\u8981\u89c2\u6d4b\u7684\u662f\u56fe\u7247\u4e2d2D\u6846\u5927\u5c0f\u4e0e\u6df1\u5ea6\u7684\u4e00\u4e2a\u76f8\u5173\u6027 \u7f51\u7edc\u7ed3\u6784\u5982\u4e0b\uff0c k=\\sqrt{\\alpha_{x} \\alpha_{y} \\frac{A_{r e a l}}{A_{i m g}}} \u5176\u4e2d \\alpha_{x} \u4e3a\u76f8\u673a\u5185\u53c2\u7684 f_x , A \u5206\u522b\u4e3a\u5b9e\u9645\u9762\u79ef\u5927\u5c0f\u4e0e\u56fe\u7247\u4e2d\u7684\u5927\u5c0f\u3002 A_{real} \u4e3a\u4e00\u4e2a\u4f30\u8ba1\u503c\uff0c\u7f51\u7edc\u7684\u8d23\u4efb\u5728\u4e8e\u4f30\u8ba1\u8fd9\u4e2ak\u503c.","title":"RootNet"},{"location":"3dDetection/DSGN/","text":"DSGN: Deep Stereo Geometry Network for 3D Object Detection \u8fd9\u7bc7\u8bba\u6587\u4e0d\u540c\u4e8e\u6b64\u524d\u7684\u4f7f\u7528Pseudo-lidar\u7684\u65b9\u5f0f\uff0c\u91c7\u7528\u4e86\u7c7b\u4f3c\u4e8eplane-sweeping \u7684\u601d\u8def\u3002\u5c06\u53cc\u76ee\u56fe\u50cf\u8f6c\u6362\u4e3a\u4e09\u7ef4\u5750\u6807\u4e0b\u7684\u4fe1\u606f\u3002 \u5728\u603b\u4f53\u4e0a\u6765\u8bf4\u8ba1\u7b97\u786e\u5b9e\u5f88\u66b4\u529b\uff0cinference\u4e5f\u5f88\u6162\uff0c\u4f46\u662f\u5b83\u7684\u7ed3\u6784\u80fd\u8ba9\u5b83\u540c\u65f6\u8fdb\u884c\u53cc\u76ee\u6df1\u5ea6\u4f30\u8ba1\uff0c\u4e5f\u53ef\u4ee5\u8bfb\u53d6\u70b9\u4e91\u4f5c\u4e3a\u8f85\u52a9\u7684\u76d1\u7763\u4fe1\u606f\uff0c\u5e76\u7ecf\u8fc7\u6d4b\u8bd5\u80fd\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002 Update: 20200409: \u4f5c\u8005\u5df2\u7ecf\u5f00\u6e90\uff0c\u91cc\u9762\u6709CostVolume\u7684cuda\u5b9e\u73b0\uff0c\u503c\u5f97\u5173\u6ce8\u5b66\u4e60 Pipeline \u4ee5\u53cc\u76ee\u4e3a\u4f8b\u5b50\uff0c\u5982\u56fe\uff0c\u4e24\u5e27\u56fe\u50cf\u540c\u65f6\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\uff0c Plane Sweep Volumn PSV\u6a21\u5757\u91cc\u9762\u6bcf\u4e00\u4e2aindex (u,v,d) \u662f\u5728\u6240\u8c13\u7684\u56fe\u7247\u5750\u6807\u4e2d\u8868\u8fbe\u7684,\u6240\u4ee5\u8fd9\u91cc\u7684\u505a\u6cd5\u662f\uff0c\u5bf9\u6bcf\u4e00\u4e2a\u5e73\u9762 d_{i} ,\u5c06\u53f3\u76ee\u7684\u7279\u5f81\u5728\u8fd9\u4e2a\u5e73\u9762\u5185\u627e\u5230\u4e0e\u5de6\u76ee\u5bf9\u5e94\u7684\u5750\u6807\uff0c\u5c06\u7279\u5f81concat\u8d77\u6765\u3002\u7136\u540e\u75283D\u5377\u79ef\u7684 hourglass \u6a21\u578b\u8fdb\u884c\u5904\u7406. 3D Geometry Volumn \u4e0b\u4e00\u6b65\u8981\u505a\u7684\u5c31\u662f\u5c06\u5efa\u7acb\u5728image-depth space\u7684PSV\u7279\u5f81\u6295\u5f71\u5230\u4e16\u754c\u5750\u6807\u4e2d\uff0c\u8fd9\u91cc\u7684\u505a\u6cd5\u662f\uff0c\u5bf9\u4e8e\u6bcf\u4e00\u4e2aworld space\u70b9\uff0c\u627e\u5230\u5b83\u5728 image-depth space\u4e2d\u7684\u4f4d\u7f6e\uff0c\u4f7f\u7528\u4e09\u5411\u63d2\u503c\u5f97\u5230\u5176\u503c\u3002(differential warping) \u68c0\u6d4b\u5206\u652f \u4e4b\u540e\u4f7f\u75283D \u5377\u79ef\u4e0e2D hourglass\uff0c\u7c7b\u4f3c OFT \u6216\u8005 MV3D \u7684\u6a21\u5f0f\u538b\u7f29\u5904\u7406BEV\u56fe\u7247,\u7269\u4f53\u68c0\u6d4b\u7684\u6a21\u5f0f\u662f\u6309\u7167 FCOS \u7684\u7b97\u6cd5\u8fdb\u884c\u7684\uff0c\u8fd9\u662f\u4e00\u79cdanchor-less\u7684\u7b97\u6cd5\u3002 \u6df1\u5ea6\u4f30\u8ba1\u5206\u652f\uff0c\u7ecf\u8fc7\u51e0\u4e2aConv3D\u4ee5\u53ca\u4e0a\u91c7\u6837\u5904\u7406\uff0c\u5f97\u5230 (H_1,W_1,D_1,1) \u7684\u77e9\u9635\u3002\u5bf9 D \u7ef4\u5ea6\u53d6softmin\u5e76\u52a0\u6743\u6c42\u548c\uff0c\u5f97\u5230\u5bf9\u6df1\u5ea6\u7684\u4f30\u8ba1\u3002 \u540e\u9762\u7684\u5b9e\u9a8c\u4e2d\uff0c\u4f5c\u8005\u7684implementation\u662f\u540c\u65f6\u7528\u70b9\u4e91\u4e0e\u7269\u4f53\u5728\u4e24\u4e2a\u5206\u652f\u4e0a\u540c\u65f6\u8fdb\u884c\u76d1\u7763\uff0c\u5f97\u5230\u7684\u6027\u80fd\u662f\u6700\u597d\u7684\u3002\u53e6\u5916\u672c\u6587\u91cc\u9762\u6240\u6709\u7684\u6a21\u578b\u90fd\u6ca1\u6709pretrain\uff0c\u90fd\u662ftrain from scratch\u3002","title":"DSGN: Deep Stereo Geometry Network for 3D Object Detection"},{"location":"3dDetection/DSGN/#dsgn-deep-stereo-geometry-network-for-3d-object-detection","text":"\u8fd9\u7bc7\u8bba\u6587\u4e0d\u540c\u4e8e\u6b64\u524d\u7684\u4f7f\u7528Pseudo-lidar\u7684\u65b9\u5f0f\uff0c\u91c7\u7528\u4e86\u7c7b\u4f3c\u4e8eplane-sweeping \u7684\u601d\u8def\u3002\u5c06\u53cc\u76ee\u56fe\u50cf\u8f6c\u6362\u4e3a\u4e09\u7ef4\u5750\u6807\u4e0b\u7684\u4fe1\u606f\u3002 \u5728\u603b\u4f53\u4e0a\u6765\u8bf4\u8ba1\u7b97\u786e\u5b9e\u5f88\u66b4\u529b\uff0cinference\u4e5f\u5f88\u6162\uff0c\u4f46\u662f\u5b83\u7684\u7ed3\u6784\u80fd\u8ba9\u5b83\u540c\u65f6\u8fdb\u884c\u53cc\u76ee\u6df1\u5ea6\u4f30\u8ba1\uff0c\u4e5f\u53ef\u4ee5\u8bfb\u53d6\u70b9\u4e91\u4f5c\u4e3a\u8f85\u52a9\u7684\u76d1\u7763\u4fe1\u606f\uff0c\u5e76\u7ecf\u8fc7\u6d4b\u8bd5\u80fd\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002 Update: 20200409: \u4f5c\u8005\u5df2\u7ecf\u5f00\u6e90\uff0c\u91cc\u9762\u6709CostVolume\u7684cuda\u5b9e\u73b0\uff0c\u503c\u5f97\u5173\u6ce8\u5b66\u4e60","title":"DSGN: Deep Stereo Geometry Network for 3D Object Detection"},{"location":"3dDetection/DSGN/#pipeline","text":"\u4ee5\u53cc\u76ee\u4e3a\u4f8b\u5b50\uff0c\u5982\u56fe\uff0c\u4e24\u5e27\u56fe\u50cf\u540c\u65f6\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\uff0c","title":"Pipeline"},{"location":"3dDetection/DSGN/#plane-sweep-volumn","text":"PSV\u6a21\u5757\u91cc\u9762\u6bcf\u4e00\u4e2aindex (u,v,d) \u662f\u5728\u6240\u8c13\u7684\u56fe\u7247\u5750\u6807\u4e2d\u8868\u8fbe\u7684,\u6240\u4ee5\u8fd9\u91cc\u7684\u505a\u6cd5\u662f\uff0c\u5bf9\u6bcf\u4e00\u4e2a\u5e73\u9762 d_{i} ,\u5c06\u53f3\u76ee\u7684\u7279\u5f81\u5728\u8fd9\u4e2a\u5e73\u9762\u5185\u627e\u5230\u4e0e\u5de6\u76ee\u5bf9\u5e94\u7684\u5750\u6807\uff0c\u5c06\u7279\u5f81concat\u8d77\u6765\u3002\u7136\u540e\u75283D\u5377\u79ef\u7684 hourglass \u6a21\u578b\u8fdb\u884c\u5904\u7406.","title":"Plane Sweep Volumn"},{"location":"3dDetection/DSGN/#3d-geometry-volumn","text":"\u4e0b\u4e00\u6b65\u8981\u505a\u7684\u5c31\u662f\u5c06\u5efa\u7acb\u5728image-depth space\u7684PSV\u7279\u5f81\u6295\u5f71\u5230\u4e16\u754c\u5750\u6807\u4e2d\uff0c\u8fd9\u91cc\u7684\u505a\u6cd5\u662f\uff0c\u5bf9\u4e8e\u6bcf\u4e00\u4e2aworld space\u70b9\uff0c\u627e\u5230\u5b83\u5728 image-depth space\u4e2d\u7684\u4f4d\u7f6e\uff0c\u4f7f\u7528\u4e09\u5411\u63d2\u503c\u5f97\u5230\u5176\u503c\u3002(differential warping)","title":"3D Geometry Volumn"},{"location":"3dDetection/DSGN/#_1","text":"\u4e4b\u540e\u4f7f\u75283D \u5377\u79ef\u4e0e2D hourglass\uff0c\u7c7b\u4f3c OFT \u6216\u8005 MV3D \u7684\u6a21\u5f0f\u538b\u7f29\u5904\u7406BEV\u56fe\u7247,\u7269\u4f53\u68c0\u6d4b\u7684\u6a21\u5f0f\u662f\u6309\u7167 FCOS \u7684\u7b97\u6cd5\u8fdb\u884c\u7684\uff0c\u8fd9\u662f\u4e00\u79cdanchor-less\u7684\u7b97\u6cd5\u3002 \u6df1\u5ea6\u4f30\u8ba1\u5206\u652f\uff0c\u7ecf\u8fc7\u51e0\u4e2aConv3D\u4ee5\u53ca\u4e0a\u91c7\u6837\u5904\u7406\uff0c\u5f97\u5230 (H_1,W_1,D_1,1) \u7684\u77e9\u9635\u3002\u5bf9 D \u7ef4\u5ea6\u53d6softmin\u5e76\u52a0\u6743\u6c42\u548c\uff0c\u5f97\u5230\u5bf9\u6df1\u5ea6\u7684\u4f30\u8ba1\u3002 \u540e\u9762\u7684\u5b9e\u9a8c\u4e2d\uff0c\u4f5c\u8005\u7684implementation\u662f\u540c\u65f6\u7528\u70b9\u4e91\u4e0e\u7269\u4f53\u5728\u4e24\u4e2a\u5206\u652f\u4e0a\u540c\u65f6\u8fdb\u884c\u76d1\u7763\uff0c\u5f97\u5230\u7684\u6027\u80fd\u662f\u6700\u597d\u7684\u3002\u53e6\u5916\u672c\u6587\u91cc\u9762\u6240\u6709\u7684\u6a21\u578b\u90fd\u6ca1\u6709pretrain\uff0c\u90fd\u662ftrain from scratch\u3002","title":"\u68c0\u6d4b\u5206\u652f"},{"location":"3dDetection/Disentangling_Monocular_3D_Object_Detection/","text":"Disentangling Monocular 3D Object Detection \u8fd9\u7bc7\u8bba\u6587\u76ee\u524d\u5728nuScene\u4ee5\u53caKitti\u4e0a\u5b9e\u73b0\u4e86\u5355\u76ee\u89c6\u89c9\u4e09\u7ef4\u68c0\u6d4b\u7684SOTA\u7684\u6027\u80fd\u3002 \u4f7f\u7528Two Stage\u7684\u68c0\u6d4b\u65b9\u5f0f\uff0c\u4e0eM3D-RPN\u7c7b\u4f3c\uff0c\u5229\u7528\u4e86\u591a\u5c3a\u5ea6\u7279\u5f81\uff0c\u589e\u52a0\u4e86\u591a\u4e2a\u7279\u6b8a\u8bbe\u8ba1\u7684Loss function \u6574\u4f53\u7ed3\u6784 \u7f51\u7edc\u5206\u4e3abackbone, 2D head, 3d head\u4e09\u4e2a\u90e8\u5206\uff0c\u7ed3\u6784\u5206\u522b\u4e3a \u76f4\u89c9\u4e0a\u6765\u8bf4\uff0c\u5c31\u662f\u4f7f\u7528Res34\u4ee5\u53cafeature pyramid network(FPN)\u63d0\u53d6\u591a\u5c3a\u5ea6\u7279\u5f81\uff0c\u8f93\u51fa\u591a\u4e2afeature map\uff0c\u6bcf\u4e00\u4e2afeature map\u8f93\u51fa2D detection Output.\u5bf9\u6bcf\u4e00\u4e2aProposal\uff0c\u4f7f\u7528ROIAlign\u5c42\u5c06\u5bf9\u5e94\u6846resize\u4e3a14*14\uff0c\u63d0\u53d6\u51fa\u7279\u5f81\u540e\u4e0e2D\u8f93\u51fa\u8fde\u63a5\u518d\u8f93\u5165\u5230\u5168\u8fde\u63a5\u5c42\uff0c\u6700\u7ec8\u8f93\u51fa3D output. \u6ce8\u610f\u5176\u4e2d\u7684iABN\u4e3aIn-Place Activated BatchNorm,\u662f\u4e00\u4e2a\u53ef\u4ee5\u8f83\u5c11training time memory cost\u7684\u6a21\u5757\uff0c\u5176pytorch\u5b9e\u73b0\u53ef\u4ee5\u5728 \u6b64\u5904 \u627e\u5230 \u5176\u8f93\u51fa\u5b9a\u4e49: \u6ce8\u610f\u65cb\u8f6c\u8f93\u51fa\u7684\u662f\u56db\u5143\u6570 \u635f\u5931\u51fd\u6570 2D Loss Focal loss \u6e90\u81ea\u4e0e \u8fd9\u7bc7\u6587\u7ae0 L_{2 \\mathrm{D}}^{\\mathrm{conf}}\\left(p_{2 \\mathrm{D}}, y\\right)=-\\alpha y\\left(1-p_{2 \\mathrm{D}}\\right)^{\\gamma} \\log p_{2 \\mathrm{D}}-\\overline{\\alpha} \\overline{y} p_{2 \\mathrm{D}}^{\\gamma} \\log \\left(1-p_{2 \\mathrm{D}}\\right) function focal_loss_y(x, gamma){ return - Math.pow(1-x, gamma) * Math.log(x) } function get_focal_loss_list(p, gamma){ focal = [] for (j = 0; j < 100;j++){ focal.push(focal_loss_y(p[j], gamma)) } return focal } focalLoss = document.getElementById('focalLoss'); var p = []; for (i = 0; i < 100;i++){ p.push(i * 0.01); } var focal = get_focal_loss_list(p, 0.2) slider_steps = [] for (i = 0.2; i < 4; i += 0.2){ slider_steps.push( { method: 'animate', label: Math.floor(i * 100) /100, args: [ { data: [{ x:p, y: get_focal_loss_list(p, i)}], }, { transition: {duration: 20}, frame: {duration: 20, redraw: false}, } ] } ) } Plotly.plot(focalLoss, [{ x: p, y: focal, }], { title: 'Focal Loss for positive samples', xaxis: { title: 'p' }, yaxis: { title: 'loss' }, sliders: [{ pad: {t: 30}, currentvalue: { xanchor: 'right', prefix: 'gamma: ', font: { color: '#888', size: 20 } }, steps: slider_steps }] }); Loss based on sIoU L_{2 \\mathrm{D}}^{\\mathrm{bb}}(\\boldsymbol{b}, \\hat{\\boldsymbol{b}})=1-\\operatorname{sIoU}(\\boldsymbol{b}, \\hat{\\boldsymbol{b}}) sIoU\u7684\u8ba1\u7b97\u516c\u5f0f\u4e3a\uff1a \u5b9e\u9645\u4e0a\u6709\u4ee5\u4e0b\u4e94\u79cd\u60c5\u51b5\uff0csIoU\u4f1a\u5728[-1, 1]\u4e4b\u95f4\uff0c 3D Loss Regression \u7f51\u7edc\u8f93\u51fa10\u4e2a\u53c2\u6570x,y,z,w,h,l,quarternion,\u4f7f\u7528lifting transformation \u5c06\u8fd9\u7ec4\u53c2\u6570\u8f6c\u6362\u4e3a\u76ee\u6807\u53c2\u65708\u4e2a\uff0c\u5bf98\u4e2a\u53c2\u6570\u8fdb\u884c\u56de\u5f52 Classification Focal loss lifting transform \u5b9e\u9645\u4e0a\u5c31\u662f\u5c06\u9884\u6d4b\u70b9\u63d0\u5347\u4e3a8\u4e2a\u89d2\u70b9 \u4ee4 z, c = (u_c, v_c), s = (W,H,D), q \u4e3a\u9884\u6d4b\u7684\u4e2d\u5fc3\u70b9\u6df1\u5ea6\u3001\u5728\u56fe\u4e2d\u6295\u5f71\u7684\u5750\u6807\uff0c\u8f66\u8f86\u4e09\u7ef4\u4ee5\u53ca\u56db\u5143\u6570\u3002K\u4e3a\u76f8\u673a\u5185\u53c2 K=\\left[\\begin{array}{ccc}{f_{x}} & {0} & {c_{x}} \\\\ {0} & {f_{y}} & {c_{y}} \\\\ {0} & {0} & {1}\\end{array}\\right] \u4ee4 C=\\left(\\begin{array}{cc}{\\frac{u_{c}-c_{x}}{f_{x}} z,} & {\\left.\\frac{v_{c}-c_{y}}{f_{y}} z, \\quad z\\right)^{\\top}=\\left(C_{x}, C_{y}, C_{z}\\right)^{\\top}}\\end{array}\\right. \u8fd9\u4e2a\u662f\u4e2d\u5fc3\u70b9\u662f\u5b9e\u9645\u5750\u6807 lifting transform \\mathcal{F} \u4e3a \\mathcal{F}(\\boldsymbol{\\theta})=\\frac{1}{2} R_{\\boldsymbol{q}_{\\mathrm{c}}} S B_{0}+\\boldsymbol{C} Disentangling 2D and 3D Detection Losses \u76f4\u89c9\u6765\u8bf4\uff0c\u4e8c\u7ef4Regression loss\u4e0e\u4e09\u7ef4Regression loss,\u76f4\u63a5\u53e0\u52a0\u7684\u597d\u5904\u662f\u4e24\u8005\u4e0d\u4f1a\u76f8\u4e92\u5f71\u54cd\uff0c\u4f46\u662f\u5b83\u4eec\u4f1a\u4ea7\u751f\u4e0d\u5e73\u8861\uff0c\u5f71\u54cd\u4f18\u5316\u7684\u8fc7\u7a0b \u6bd4\u5982\u5bf9\u4e8e\u672c\u6587\u7684\u4e09\u7ef4\u56de\u5f52\u6765\u8bf4\uff0c\u539f\u59cb\u53c2\u6570\u5206\u4e3a\u56db\u7ec4\uff0c\u4e5f\u5c31\u662f\u6df1\u5ea6\u3001\u76f8\u673a\u5750\u6807\u3001\u4e09\u7ef4\u4ee5\u53ca\u89d2\u5ea6\uff0c\u56de\u5f52\u65f6\u9700\u8981\u8ba1\u7b97\u7684\u53c2\u6570\u662f\u516b\u4e2a\u89d2\u70b9\u7684\u5750\u6807\uff0c\u5176\u4e2d\u9700\u8981\u4e00\u4e2a\u8f6c\u6362\u3002 \u5728\u8ba1\u7b97loss\u65f6\uff0c\u5206\u6210\u56db\u7ec4\u3002\u5176\u4e2d\u7b2c i \u7ec4\u8ba1\u7b97loss\u65f6\uff0c\u7b2c i \u7ec4\u53c2\u6570\u7528\u9884\u6d4b\u503c\uff0c\u5176\u4f59\u7528ground_truth\uff0c\u5982\u6b64\u6bcf\u4e00\u7ec4\u90fd\u4f1a\u5206\u5f00\u4f18\u5316\u3002 \u5177\u4f53\u76f4\u89c9\u770b\u539f\u6587","title":"Disentangling Monocular 3D Object Detection"},{"location":"3dDetection/Disentangling_Monocular_3D_Object_Detection/#disentangling-monocular-3d-object-detection","text":"\u8fd9\u7bc7\u8bba\u6587\u76ee\u524d\u5728nuScene\u4ee5\u53caKitti\u4e0a\u5b9e\u73b0\u4e86\u5355\u76ee\u89c6\u89c9\u4e09\u7ef4\u68c0\u6d4b\u7684SOTA\u7684\u6027\u80fd\u3002 \u4f7f\u7528Two Stage\u7684\u68c0\u6d4b\u65b9\u5f0f\uff0c\u4e0eM3D-RPN\u7c7b\u4f3c\uff0c\u5229\u7528\u4e86\u591a\u5c3a\u5ea6\u7279\u5f81\uff0c\u589e\u52a0\u4e86\u591a\u4e2a\u7279\u6b8a\u8bbe\u8ba1\u7684Loss function","title":"Disentangling Monocular 3D Object Detection"},{"location":"3dDetection/Disentangling_Monocular_3D_Object_Detection/#_1","text":"\u7f51\u7edc\u5206\u4e3abackbone, 2D head, 3d head\u4e09\u4e2a\u90e8\u5206\uff0c\u7ed3\u6784\u5206\u522b\u4e3a \u76f4\u89c9\u4e0a\u6765\u8bf4\uff0c\u5c31\u662f\u4f7f\u7528Res34\u4ee5\u53cafeature pyramid network(FPN)\u63d0\u53d6\u591a\u5c3a\u5ea6\u7279\u5f81\uff0c\u8f93\u51fa\u591a\u4e2afeature map\uff0c\u6bcf\u4e00\u4e2afeature map\u8f93\u51fa2D detection Output.\u5bf9\u6bcf\u4e00\u4e2aProposal\uff0c\u4f7f\u7528ROIAlign\u5c42\u5c06\u5bf9\u5e94\u6846resize\u4e3a14*14\uff0c\u63d0\u53d6\u51fa\u7279\u5f81\u540e\u4e0e2D\u8f93\u51fa\u8fde\u63a5\u518d\u8f93\u5165\u5230\u5168\u8fde\u63a5\u5c42\uff0c\u6700\u7ec8\u8f93\u51fa3D output. \u6ce8\u610f\u5176\u4e2d\u7684iABN\u4e3aIn-Place Activated BatchNorm,\u662f\u4e00\u4e2a\u53ef\u4ee5\u8f83\u5c11training time memory cost\u7684\u6a21\u5757\uff0c\u5176pytorch\u5b9e\u73b0\u53ef\u4ee5\u5728 \u6b64\u5904 \u627e\u5230 \u5176\u8f93\u51fa\u5b9a\u4e49: \u6ce8\u610f\u65cb\u8f6c\u8f93\u51fa\u7684\u662f\u56db\u5143\u6570","title":"\u6574\u4f53\u7ed3\u6784"},{"location":"3dDetection/Disentangling_Monocular_3D_Object_Detection/#_2","text":"","title":"\u635f\u5931\u51fd\u6570"},{"location":"3dDetection/Disentangling_Monocular_3D_Object_Detection/#2d-loss","text":"Focal loss \u6e90\u81ea\u4e0e \u8fd9\u7bc7\u6587\u7ae0 L_{2 \\mathrm{D}}^{\\mathrm{conf}}\\left(p_{2 \\mathrm{D}}, y\\right)=-\\alpha y\\left(1-p_{2 \\mathrm{D}}\\right)^{\\gamma} \\log p_{2 \\mathrm{D}}-\\overline{\\alpha} \\overline{y} p_{2 \\mathrm{D}}^{\\gamma} \\log \\left(1-p_{2 \\mathrm{D}}\\right) function focal_loss_y(x, gamma){ return - Math.pow(1-x, gamma) * Math.log(x) } function get_focal_loss_list(p, gamma){ focal = [] for (j = 0; j < 100;j++){ focal.push(focal_loss_y(p[j], gamma)) } return focal } focalLoss = document.getElementById('focalLoss'); var p = []; for (i = 0; i < 100;i++){ p.push(i * 0.01); } var focal = get_focal_loss_list(p, 0.2) slider_steps = [] for (i = 0.2; i < 4; i += 0.2){ slider_steps.push( { method: 'animate', label: Math.floor(i * 100) /100, args: [ { data: [{ x:p, y: get_focal_loss_list(p, i)}], }, { transition: {duration: 20}, frame: {duration: 20, redraw: false}, } ] } ) } Plotly.plot(focalLoss, [{ x: p, y: focal, }], { title: 'Focal Loss for positive samples', xaxis: { title: 'p' }, yaxis: { title: 'loss' }, sliders: [{ pad: {t: 30}, currentvalue: { xanchor: 'right', prefix: 'gamma: ', font: { color: '#888', size: 20 } }, steps: slider_steps }] }); Loss based on sIoU L_{2 \\mathrm{D}}^{\\mathrm{bb}}(\\boldsymbol{b}, \\hat{\\boldsymbol{b}})=1-\\operatorname{sIoU}(\\boldsymbol{b}, \\hat{\\boldsymbol{b}}) sIoU\u7684\u8ba1\u7b97\u516c\u5f0f\u4e3a\uff1a \u5b9e\u9645\u4e0a\u6709\u4ee5\u4e0b\u4e94\u79cd\u60c5\u51b5\uff0csIoU\u4f1a\u5728[-1, 1]\u4e4b\u95f4\uff0c","title":"2D Loss"},{"location":"3dDetection/Disentangling_Monocular_3D_Object_Detection/#3d-loss","text":"Regression \u7f51\u7edc\u8f93\u51fa10\u4e2a\u53c2\u6570x,y,z,w,h,l,quarternion,\u4f7f\u7528lifting transformation \u5c06\u8fd9\u7ec4\u53c2\u6570\u8f6c\u6362\u4e3a\u76ee\u6807\u53c2\u65708\u4e2a\uff0c\u5bf98\u4e2a\u53c2\u6570\u8fdb\u884c\u56de\u5f52 Classification Focal loss","title":"3D Loss"},{"location":"3dDetection/Disentangling_Monocular_3D_Object_Detection/#lifting-transform","text":"\u5b9e\u9645\u4e0a\u5c31\u662f\u5c06\u9884\u6d4b\u70b9\u63d0\u5347\u4e3a8\u4e2a\u89d2\u70b9 \u4ee4 z, c = (u_c, v_c), s = (W,H,D), q \u4e3a\u9884\u6d4b\u7684\u4e2d\u5fc3\u70b9\u6df1\u5ea6\u3001\u5728\u56fe\u4e2d\u6295\u5f71\u7684\u5750\u6807\uff0c\u8f66\u8f86\u4e09\u7ef4\u4ee5\u53ca\u56db\u5143\u6570\u3002K\u4e3a\u76f8\u673a\u5185\u53c2 K=\\left[\\begin{array}{ccc}{f_{x}} & {0} & {c_{x}} \\\\ {0} & {f_{y}} & {c_{y}} \\\\ {0} & {0} & {1}\\end{array}\\right] \u4ee4 C=\\left(\\begin{array}{cc}{\\frac{u_{c}-c_{x}}{f_{x}} z,} & {\\left.\\frac{v_{c}-c_{y}}{f_{y}} z, \\quad z\\right)^{\\top}=\\left(C_{x}, C_{y}, C_{z}\\right)^{\\top}}\\end{array}\\right. \u8fd9\u4e2a\u662f\u4e2d\u5fc3\u70b9\u662f\u5b9e\u9645\u5750\u6807 lifting transform \\mathcal{F} \u4e3a \\mathcal{F}(\\boldsymbol{\\theta})=\\frac{1}{2} R_{\\boldsymbol{q}_{\\mathrm{c}}} S B_{0}+\\boldsymbol{C}","title":"lifting transform"},{"location":"3dDetection/Disentangling_Monocular_3D_Object_Detection/#disentangling-2d-and-3d-detection-losses","text":"\u76f4\u89c9\u6765\u8bf4\uff0c\u4e8c\u7ef4Regression loss\u4e0e\u4e09\u7ef4Regression loss,\u76f4\u63a5\u53e0\u52a0\u7684\u597d\u5904\u662f\u4e24\u8005\u4e0d\u4f1a\u76f8\u4e92\u5f71\u54cd\uff0c\u4f46\u662f\u5b83\u4eec\u4f1a\u4ea7\u751f\u4e0d\u5e73\u8861\uff0c\u5f71\u54cd\u4f18\u5316\u7684\u8fc7\u7a0b \u6bd4\u5982\u5bf9\u4e8e\u672c\u6587\u7684\u4e09\u7ef4\u56de\u5f52\u6765\u8bf4\uff0c\u539f\u59cb\u53c2\u6570\u5206\u4e3a\u56db\u7ec4\uff0c\u4e5f\u5c31\u662f\u6df1\u5ea6\u3001\u76f8\u673a\u5750\u6807\u3001\u4e09\u7ef4\u4ee5\u53ca\u89d2\u5ea6\uff0c\u56de\u5f52\u65f6\u9700\u8981\u8ba1\u7b97\u7684\u53c2\u6570\u662f\u516b\u4e2a\u89d2\u70b9\u7684\u5750\u6807\uff0c\u5176\u4e2d\u9700\u8981\u4e00\u4e2a\u8f6c\u6362\u3002 \u5728\u8ba1\u7b97loss\u65f6\uff0c\u5206\u6210\u56db\u7ec4\u3002\u5176\u4e2d\u7b2c i \u7ec4\u8ba1\u7b97loss\u65f6\uff0c\u7b2c i \u7ec4\u53c2\u6570\u7528\u9884\u6d4b\u503c\uff0c\u5176\u4f59\u7528ground_truth\uff0c\u5982\u6b64\u6bcf\u4e00\u7ec4\u90fd\u4f1a\u5206\u5f00\u4f18\u5316\u3002 \u5177\u4f53\u76f4\u89c9\u770b\u539f\u6587","title":"Disentangling 2D and 3D Detection Losses"},{"location":"3dDetection/EGFN/","text":"EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection \u8fd9\u7bc7paper\u57fa\u4e8e YOLOStereo3D \u5f00\u53d1,\u589e\u52a0\u4e86image-view \u4e0e BEV feature\u7684\u8f6c\u5316\u4ee5\u53ca\u66f4\u52a0\u6fc0\u8fdb\u7684\u84b8\u998f\u7b56\u7565. \u6839\u636e\u4e0a\u56fe,\u7b80\u5355\u6765\u8bf4,\u5728YOLOStereo3D\u7684\u57fa\u7840\u4e0a,\u8fdb\u4e00\u6b65\u901a\u8fc7IM3D\u6a21\u5757\u628a\u7279\u5f81\u8f6c\u52303D space. \u7136\u540e\u53c2\u8003LiDAR\u7684\u8f93\u5165\u8fdb\u884c\u84b8\u998f\u8bad\u7ec3.\u8f93\u51fa\u5934\u6539\u6210 LIGAStereo \u7684\u65b9\u6848. \u6574\u4f53\u7684\u601d\u8def\u7c7b\u4f3c\u4e8e\u7ed3\u5408\u4e86 YOLOStereo3D \u7684\u524d\u7aef\u7ed3\u6784&\u5339\u914d\u601d\u8def\u548c LIGAStereo \u7684\u70b9\u4e91\u84b8\u998f&\u4e09\u7ef4\u63a8\u7406. \u5f97\u5230\u4e00\u4e2a\u66f4\u9ad8\u7684\u901f\u5ea6&\u7cbe\u5ea6\u5e73\u8861\u70b9.","title":"EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection"},{"location":"3dDetection/EGFN/#egfn-efficient-geometry-feature-network-for-fast-stereo-3d-object-detection","text":"\u8fd9\u7bc7paper\u57fa\u4e8e YOLOStereo3D \u5f00\u53d1,\u589e\u52a0\u4e86image-view \u4e0e BEV feature\u7684\u8f6c\u5316\u4ee5\u53ca\u66f4\u52a0\u6fc0\u8fdb\u7684\u84b8\u998f\u7b56\u7565. \u6839\u636e\u4e0a\u56fe,\u7b80\u5355\u6765\u8bf4,\u5728YOLOStereo3D\u7684\u57fa\u7840\u4e0a,\u8fdb\u4e00\u6b65\u901a\u8fc7IM3D\u6a21\u5757\u628a\u7279\u5f81\u8f6c\u52303D space. \u7136\u540e\u53c2\u8003LiDAR\u7684\u8f93\u5165\u8fdb\u884c\u84b8\u998f\u8bad\u7ec3.\u8f93\u51fa\u5934\u6539\u6210 LIGAStereo \u7684\u65b9\u6848. \u6574\u4f53\u7684\u601d\u8def\u7c7b\u4f3c\u4e8e\u7ed3\u5408\u4e86 YOLOStereo3D \u7684\u524d\u7aef\u7ed3\u6784&\u5339\u914d\u601d\u8def\u548c LIGAStereo \u7684\u70b9\u4e91\u84b8\u998f&\u4e09\u7ef4\u63a8\u7406. \u5f97\u5230\u4e00\u4e2a\u66f4\u9ad8\u7684\u901f\u5ea6&\u7cbe\u5ea6\u5e73\u8861\u70b9.","title":"EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection"},{"location":"3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/","text":"End-to-end Learning of Multi-sensor 3D Tracking by Detection \u8fd9\u7bc7\u8bba\u6587\u4e3b\u8981\u8ba8\u8bba\u7684\u662f\u4e00\u4e2atracking\u95ee\u9898\uff0c\u7a81\u51fa\u8d21\u732e\u662f\u4f7f\u7528\u4e86\u79bb\u6563\u4f18\u5316\u7684\u6982\u5ff5(\u5728\u6c42\u89e3\u65f6\u8f6c\u6362\u4e3a\u4e86\u7ebf\u6027\u4f18\u5316\u95ee\u9898)\uff0c\u5e76\u89e3\u51b3\u4e86\u5982\u4f55\u8bad\u7ec3\u8fd9\u4e2a\u95ee\u9898\uff0c\u8fc7\u7a0b\u4e2d\u6709\u8f83\u591a\u7684\u6570\u5b66\u5de7\u5408\uff0c\u4f46\u662f\u5176\u5b9e\u8fd9\u662f\u4e00\u7c7b\u95ee\u9898\u7684\u89e3\u51b3\u65b9\u6cd5,\u8fd9\u4e2a\u95ee\u9898\u79f0\u4e3ageneral matching problem,\u53ef\u4ee5 \u53c2\u8003\u5f15\u65876 \u7684\u95ee\u9898\u9610\u8ff0\u4ee5\u53ca\u9644\u5f551\u3002 \u63a8\u65ad\u7ed3\u6784 \u9010\u4e2a\u5206\u6790\uff1a Detection Net\u7528\u7684\u662f MV3D Scoring Net\uff0c\u5c06\u6bcf\u4e00\u4e2a3D\u6846\u6295\u5f71\u56de\u56fe\u7247\u4e2d\uff0c\u7528VGG\u603b\u7ed3\uff0c\u7ed9\u51fa\u8fd9\u4e2a\u56fe\u7247\u662ftrue positive\u7684cost Matching Net\uff0c\u5206\u4e24\u652f\uff0c\u4e00\u4e2a\u662fSiamese Network\uff0c\u5c06\u4e24\u4e2a3D\u6846\u5bf9\u5e94\u7684\u56fe\u7247\u9001\u5230network\u4e2d\u63d0\u53d6\u5171\u540c\u4fe1\u606f\uff0c\u7b2c\u4e8c\u4e2a\u662f\u8fd0\u52a8\u77eb\u6b63\u540e\u76843D\u6295\u5f71\u56fe\uff0c\u9001\u5230network\u4e2d\u63d0\u53d6\u4fe1\u606f\u3002\u7136\u540e\u6700\u540e\u7531FC Net\u5f97\u5230\u9884\u6d4b\u3002\u8fd9\u4e2anetwork\u9700\u8981\u5bf9\u6bcf\u4e00\u5bf9\u53ef\u80fd\u7684\u5339\u914d\u8ba1\u7b97\u3002 new cost and end cost:\u5bf9\u4e00\u4e2adetection\u662f\u65b0\u7684\u6216\u662f\u6700\u7ec8\u51fa\u73b0\u7684\u9884\u6d4b\uff0c\u662f\u4e00\u4e2a\u5e38\u91cf\uff0c\u53ef\u5b66\u4e60\u3002 \u8fc7\u7a0b\u7b80\u4ecb \u4e24\u4e2aLinear\u3001Integer Programming\u7684\u5b9a\u4e49 \u7ea6\u675f\u77e9\u9635 y_j^{new} + \\sum_{k of last frame}y^{link}_{j,k} = y^{end}_{j} + \\sum_{k of next frame} y_{j,k}^{link} = y_j^{det} \u7b80\u5355\u5730\u8bf4\u5c31\u662f\u65b0\u751f\u7684or\u4ece\u4e0a\u4e00\u5e27\u6765\u7684object = \u7ed3\u675f\u7684or\u53bb\u5f80\u4e0b\u4e00\u5e27\u7684object = \u8fd9\u4e00\u5e27\u8fd9\u4e2aobject\u662f\u5426\u662ftrue positive \u5728\u7eaf\u7cb9inference\u7684\u65f6\u5019\uff0c\u4f18\u5316\u76ee\u6807\u5c31\u662f \\theta_{\\bold w}(\\bold x) y ,\u6bcf\u4e00\u4e2a\u5e03\u5c14\u51b3\u7b56y\u524d\u7684cost\u7531\u524d\u6587\u7684Matching Net\uff0c scoring net\uff0c\u4ee5\u53ca\u53ef\u5b66\u4e60\u5e38\u91cf\u7ed9\u51fa\u3002\u76f4\u63a5\u7528OR-tools\u6c42\u89e3,\u76f8\u5f53\u4e8e\u5df2\u77e5reward\u53c2\u6570\u627e\u51fa\u6700\u597d\u7684\u89e3 \u5728\u9700\u8981train\u7684\u65f6\u5019\uff0c\u4f18\u5316\u76ee\u6807\u5c31\u662f L(x, y, W) = \\sum_x[\\max_{y}(\\Delta(y, \\hat y) + \\theta_{\\bold w}(\\bold x) (y-\\hat y)))] \u91cc\u9762\u7684 \\max_{y}(\\Delta(y, \\hat y) + \\theta_{\\bold w}(\\bold x) (y-\\hat y)) \u4f5c\u4e3ainference\u65f6\u5019\u9700\u8981\u6c42\u89e3\u7684LP\u51fd\u6570\u3002\u601d\u8def\u7c7b\u4f3c\u4e8eSVM\u4e2d\u7684\u5bf9\u5076\u4f18\u5316\u95ee\u9898\uff0c\u5728\u8bad\u7ec3\u7684\u65f6\u5019\uff0c\u6211\u4eec\u9700\u8981\u627e\u5230 w \uff0c\u4f7f\u5f97,worst case\u7684\u4e00\u4e2a\"\u9519\u5224loss\"\u6700\u5c0f\uff0c\u800c\u8fd9\u4e2aworst case\u7684\u9519\u5224loss\uff0c\u9996\u5148\u8981\u5b9a\u4e49\u9519\u5224loss\uff0c\u5176\u6b21\u662f\u7528max\u627e\u51faworst case(\u7b80\u5355).\u800c\u9519\u5224loss\u8fd9\u91cc\u7528\u6c49\u660e\u8ddd\u79bb+reward difference,\u7406\u60f3\u60c5\u51b5\u4e0b\uff0c\u6c49\u660e\u8ddd\u79bb\u8d8a\u5927\uff0creward difference\u5c31\u5e94\u8be5\u8d8a\u5c0f,\u53cd\u4e4b\u4ea6\u7136\uff0c\u5e76\u6700\u597d\u4fdd\u6301\u4e3a\u4e00\u4e2a\u5e38\u6570 \u5bf9\u4e8e\u9519\u5224loss\uff0c\u9996\u5148\u6c49\u660e\u8ddd\u79bb\u5c0f\u65f6reward difference\u81ea\u7136\u4e5f\u4f1a\u5c0f\uff0c\u800c\u6c49\u660e\u8ddd\u79bb\u5927\u65f6reward difference\u4e5f\u5e94\u8be5\u4e3a\u4e00\u4e2a\u5927\u7684\u8d1f\u6570\u4ee5\u62b5\u6d88\uff0c\u5982\u679c\u51fa\u73b0\u6c49\u660e\u8ddd\u79bb\u5927\u800creward difference\u4e0d\u591f\u5c0f\uff0c\u5219\u8fd9\u4e2aworst case\u9700\u8981\u88ab\u4fee\u6b63\uff0c\u4e5f\u5c31\u662f\u8fd9\u4e2a\u73a9\u610f\u513f\u7684intuition\u3002 \u5173\u4e8e\u5176\u68af\u5ea6\uff0c\u5efa\u8bae\u53c2\u8003 \u8fd9\u7bc7 \u3002\u5f97\u5230\u7684\u7ed3\u679c\u662f \u5173\u952e\u662f\u6839\u636e\u5f15\u65876\uff0c\u8fd9\u4e2a\u7ea6\u675f\u77e9\u9635A\u662f\u4e00\u4e2a\u5355\u6a21\u77e9\u9635\uff0c\u6240\u4ee5\u5176\u9006\u77e9\u9635\u7684\u89e3\u90fd\u662f\u6574\u6570\uff0c\u6240\u4ee5Integer programming\u88ab\u8f6c\u6362\u4e3aLinear Programming\u5e76\u540c\u65f6\u4fdd\u8bc1\u4e86\u6700\u4f18\u89e3\u4e00\u5b9a\u662f\u6574\u6570\u3002","title":"End-to-end Learning of Multi-sensor 3D Tracking by Detection"},{"location":"3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/#end-to-end-learning-of-multi-sensor-3d-tracking-by-detection","text":"\u8fd9\u7bc7\u8bba\u6587\u4e3b\u8981\u8ba8\u8bba\u7684\u662f\u4e00\u4e2atracking\u95ee\u9898\uff0c\u7a81\u51fa\u8d21\u732e\u662f\u4f7f\u7528\u4e86\u79bb\u6563\u4f18\u5316\u7684\u6982\u5ff5(\u5728\u6c42\u89e3\u65f6\u8f6c\u6362\u4e3a\u4e86\u7ebf\u6027\u4f18\u5316\u95ee\u9898)\uff0c\u5e76\u89e3\u51b3\u4e86\u5982\u4f55\u8bad\u7ec3\u8fd9\u4e2a\u95ee\u9898\uff0c\u8fc7\u7a0b\u4e2d\u6709\u8f83\u591a\u7684\u6570\u5b66\u5de7\u5408\uff0c\u4f46\u662f\u5176\u5b9e\u8fd9\u662f\u4e00\u7c7b\u95ee\u9898\u7684\u89e3\u51b3\u65b9\u6cd5,\u8fd9\u4e2a\u95ee\u9898\u79f0\u4e3ageneral matching problem,\u53ef\u4ee5 \u53c2\u8003\u5f15\u65876 \u7684\u95ee\u9898\u9610\u8ff0\u4ee5\u53ca\u9644\u5f551\u3002","title":"End-to-end Learning of Multi-sensor 3D Tracking by Detection"},{"location":"3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/#_1","text":"\u9010\u4e2a\u5206\u6790\uff1a Detection Net\u7528\u7684\u662f MV3D Scoring Net\uff0c\u5c06\u6bcf\u4e00\u4e2a3D\u6846\u6295\u5f71\u56de\u56fe\u7247\u4e2d\uff0c\u7528VGG\u603b\u7ed3\uff0c\u7ed9\u51fa\u8fd9\u4e2a\u56fe\u7247\u662ftrue positive\u7684cost Matching Net\uff0c\u5206\u4e24\u652f\uff0c\u4e00\u4e2a\u662fSiamese Network\uff0c\u5c06\u4e24\u4e2a3D\u6846\u5bf9\u5e94\u7684\u56fe\u7247\u9001\u5230network\u4e2d\u63d0\u53d6\u5171\u540c\u4fe1\u606f\uff0c\u7b2c\u4e8c\u4e2a\u662f\u8fd0\u52a8\u77eb\u6b63\u540e\u76843D\u6295\u5f71\u56fe\uff0c\u9001\u5230network\u4e2d\u63d0\u53d6\u4fe1\u606f\u3002\u7136\u540e\u6700\u540e\u7531FC Net\u5f97\u5230\u9884\u6d4b\u3002\u8fd9\u4e2anetwork\u9700\u8981\u5bf9\u6bcf\u4e00\u5bf9\u53ef\u80fd\u7684\u5339\u914d\u8ba1\u7b97\u3002 new cost and end cost:\u5bf9\u4e00\u4e2adetection\u662f\u65b0\u7684\u6216\u662f\u6700\u7ec8\u51fa\u73b0\u7684\u9884\u6d4b\uff0c\u662f\u4e00\u4e2a\u5e38\u91cf\uff0c\u53ef\u5b66\u4e60\u3002","title":"\u63a8\u65ad\u7ed3\u6784"},{"location":"3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/#_2","text":"","title":"\u8fc7\u7a0b\u7b80\u4ecb"},{"location":"3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/#linearinteger-programming","text":"\u7ea6\u675f\u77e9\u9635 y_j^{new} + \\sum_{k of last frame}y^{link}_{j,k} = y^{end}_{j} + \\sum_{k of next frame} y_{j,k}^{link} = y_j^{det} \u7b80\u5355\u5730\u8bf4\u5c31\u662f\u65b0\u751f\u7684or\u4ece\u4e0a\u4e00\u5e27\u6765\u7684object = \u7ed3\u675f\u7684or\u53bb\u5f80\u4e0b\u4e00\u5e27\u7684object = \u8fd9\u4e00\u5e27\u8fd9\u4e2aobject\u662f\u5426\u662ftrue positive \u5728\u7eaf\u7cb9inference\u7684\u65f6\u5019\uff0c\u4f18\u5316\u76ee\u6807\u5c31\u662f \\theta_{\\bold w}(\\bold x) y ,\u6bcf\u4e00\u4e2a\u5e03\u5c14\u51b3\u7b56y\u524d\u7684cost\u7531\u524d\u6587\u7684Matching Net\uff0c scoring net\uff0c\u4ee5\u53ca\u53ef\u5b66\u4e60\u5e38\u91cf\u7ed9\u51fa\u3002\u76f4\u63a5\u7528OR-tools\u6c42\u89e3,\u76f8\u5f53\u4e8e\u5df2\u77e5reward\u53c2\u6570\u627e\u51fa\u6700\u597d\u7684\u89e3 \u5728\u9700\u8981train\u7684\u65f6\u5019\uff0c\u4f18\u5316\u76ee\u6807\u5c31\u662f L(x, y, W) = \\sum_x[\\max_{y}(\\Delta(y, \\hat y) + \\theta_{\\bold w}(\\bold x) (y-\\hat y)))] \u91cc\u9762\u7684 \\max_{y}(\\Delta(y, \\hat y) + \\theta_{\\bold w}(\\bold x) (y-\\hat y)) \u4f5c\u4e3ainference\u65f6\u5019\u9700\u8981\u6c42\u89e3\u7684LP\u51fd\u6570\u3002\u601d\u8def\u7c7b\u4f3c\u4e8eSVM\u4e2d\u7684\u5bf9\u5076\u4f18\u5316\u95ee\u9898\uff0c\u5728\u8bad\u7ec3\u7684\u65f6\u5019\uff0c\u6211\u4eec\u9700\u8981\u627e\u5230 w \uff0c\u4f7f\u5f97,worst case\u7684\u4e00\u4e2a\"\u9519\u5224loss\"\u6700\u5c0f\uff0c\u800c\u8fd9\u4e2aworst case\u7684\u9519\u5224loss\uff0c\u9996\u5148\u8981\u5b9a\u4e49\u9519\u5224loss\uff0c\u5176\u6b21\u662f\u7528max\u627e\u51faworst case(\u7b80\u5355).\u800c\u9519\u5224loss\u8fd9\u91cc\u7528\u6c49\u660e\u8ddd\u79bb+reward difference,\u7406\u60f3\u60c5\u51b5\u4e0b\uff0c\u6c49\u660e\u8ddd\u79bb\u8d8a\u5927\uff0creward difference\u5c31\u5e94\u8be5\u8d8a\u5c0f,\u53cd\u4e4b\u4ea6\u7136\uff0c\u5e76\u6700\u597d\u4fdd\u6301\u4e3a\u4e00\u4e2a\u5e38\u6570 \u5bf9\u4e8e\u9519\u5224loss\uff0c\u9996\u5148\u6c49\u660e\u8ddd\u79bb\u5c0f\u65f6reward difference\u81ea\u7136\u4e5f\u4f1a\u5c0f\uff0c\u800c\u6c49\u660e\u8ddd\u79bb\u5927\u65f6reward difference\u4e5f\u5e94\u8be5\u4e3a\u4e00\u4e2a\u5927\u7684\u8d1f\u6570\u4ee5\u62b5\u6d88\uff0c\u5982\u679c\u51fa\u73b0\u6c49\u660e\u8ddd\u79bb\u5927\u800creward difference\u4e0d\u591f\u5c0f\uff0c\u5219\u8fd9\u4e2aworst case\u9700\u8981\u88ab\u4fee\u6b63\uff0c\u4e5f\u5c31\u662f\u8fd9\u4e2a\u73a9\u610f\u513f\u7684intuition\u3002 \u5173\u4e8e\u5176\u68af\u5ea6\uff0c\u5efa\u8bae\u53c2\u8003 \u8fd9\u7bc7 \u3002\u5f97\u5230\u7684\u7ed3\u679c\u662f \u5173\u952e\u662f\u6839\u636e\u5f15\u65876\uff0c\u8fd9\u4e2a\u7ea6\u675f\u77e9\u9635A\u662f\u4e00\u4e2a\u5355\u6a21\u77e9\u9635\uff0c\u6240\u4ee5\u5176\u9006\u77e9\u9635\u7684\u89e3\u90fd\u662f\u6574\u6570\uff0c\u6240\u4ee5Integer programming\u88ab\u8f6c\u6362\u4e3aLinear Programming\u5e76\u540c\u65f6\u4fdd\u8bc1\u4e86\u6700\u4f18\u89e3\u4e00\u5b9a\u662f\u6574\u6570\u3002","title":"\u4e24\u4e2aLinear\u3001Integer Programming\u7684\u5b9a\u4e49"},{"location":"3dDetection/FCOS3D_mono/","text":"FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection \u672c\u6587\u5c06 FCOS \u8f6c\u4e3a\u4e863D\u7684\u7248\u672c.","title":"FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection"},{"location":"3dDetection/FCOS3D_mono/#fcos3d-fully-convolutional-one-stage-monocular-3d-object-detection","text":"\u672c\u6587\u5c06 FCOS \u8f6c\u4e3a\u4e863D\u7684\u7248\u672c.","title":"FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection"},{"location":"3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/","text":"Fast and Furious: Real Time End-to-End 3D Detection, Tracking and MotionForecasting with a Single Convolutional Net \u8fd9\u7bc7\u8bba\u6587\u7684\u5173\u952e\u662f\u7ed3\u5408\u65f6\u5e8f\u4fe1\u606f\uff0c\u8981\u540c\u65f6\u5b9e\u73b03D detection\u4ee5\u53catracking\u751a\u81f3forecasting,\u4f7f\u7528\u5355\u4e00\u7684Lidar\u6570\u636e \u70b9\u4e91\u4fe1\u606f\u7684\u5904\u7406\u529e\u6cd5 \u5c06\u5355\u5e27\u70b9\u4e91\u8f6c\u6362\u4e3a2Dcostmap\uff0c\u70b9\u7684\u9ad8\u5ea6\u4f5c\u4e3a\u8f93\u5165\u7684\u4e00\u4e2afeature\u3002 \u5c06\u591a\u5e27\u70b9\u4e91\u8f6c\u6362\u5230\u540c\u4e00\u4e2a\u5750\u6807\u7cfb\u4e2d\uff0c\u6bcf\u4e00\u5e27\u53ef\u4ee5\u5355\u72ec\u5f62\u6210\u4e00\u4e2a3D Tensor\uff0c\u7ed3\u5408\u8d77\u6765\u6210\u4e3a\u4e00\u4e2a4D Tensor \u878d\u5408\u65f6\u5e8f\u4fe1\u606f\u7684\u65b9\u6cd5 \u7b2c\u4e00\u79cd\u65b9\u6cd5\u662fEarly Fusion,\u5728\u4e00\u5f00\u59cb\u5c31\u4f7f\u75281D Conv\u5c06\u591a\u4e2aCostmap\u7ed3\u5408\u6210\u4e00\u4e2aCostmap \u7b2c\u4e8c\u79cd\u65b9\u6cd5\u662fLate Fusion,\u4f7f\u75283D Conv\u5206\u4e24\u6b65\u7ed3\u5408\u3002 \u8f93\u51fabounding box \u4ee5\u53ca\u9884\u6d4b \u76f4\u63a5\u8f93\u51fa\u4e00\u7cfb\u5217\u7684bounding box\u548c\u79cd\u7c7b\u4fe1\u606f","title":"Fast and Furious: Real Time End-to-End 3D Detection, Tracking and MotionForecasting with a Single Convolutional Net"},{"location":"3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/#fast-and-furious-real-time-end-to-end-3d-detection-tracking-and-motionforecasting-with-a-single-convolutional-net","text":"\u8fd9\u7bc7\u8bba\u6587\u7684\u5173\u952e\u662f\u7ed3\u5408\u65f6\u5e8f\u4fe1\u606f\uff0c\u8981\u540c\u65f6\u5b9e\u73b03D detection\u4ee5\u53catracking\u751a\u81f3forecasting,\u4f7f\u7528\u5355\u4e00\u7684Lidar\u6570\u636e","title":"Fast and Furious: Real Time End-to-End 3D Detection, Tracking and MotionForecasting with a Single Convolutional Net"},{"location":"3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/#_1","text":"\u5c06\u5355\u5e27\u70b9\u4e91\u8f6c\u6362\u4e3a2Dcostmap\uff0c\u70b9\u7684\u9ad8\u5ea6\u4f5c\u4e3a\u8f93\u5165\u7684\u4e00\u4e2afeature\u3002 \u5c06\u591a\u5e27\u70b9\u4e91\u8f6c\u6362\u5230\u540c\u4e00\u4e2a\u5750\u6807\u7cfb\u4e2d\uff0c\u6bcf\u4e00\u5e27\u53ef\u4ee5\u5355\u72ec\u5f62\u6210\u4e00\u4e2a3D Tensor\uff0c\u7ed3\u5408\u8d77\u6765\u6210\u4e3a\u4e00\u4e2a4D Tensor","title":"\u70b9\u4e91\u4fe1\u606f\u7684\u5904\u7406\u529e\u6cd5"},{"location":"3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/#_2","text":"\u7b2c\u4e00\u79cd\u65b9\u6cd5\u662fEarly Fusion,\u5728\u4e00\u5f00\u59cb\u5c31\u4f7f\u75281D Conv\u5c06\u591a\u4e2aCostmap\u7ed3\u5408\u6210\u4e00\u4e2aCostmap \u7b2c\u4e8c\u79cd\u65b9\u6cd5\u662fLate Fusion,\u4f7f\u75283D Conv\u5206\u4e24\u6b65\u7ed3\u5408\u3002","title":"\u878d\u5408\u65f6\u5e8f\u4fe1\u606f\u7684\u65b9\u6cd5"},{"location":"3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/#bounding-box","text":"\u76f4\u63a5\u8f93\u51fa\u4e00\u7cfb\u5217\u7684bounding box\u548c\u79cd\u7c7b\u4fe1\u606f","title":"\u8f93\u51fabounding box \u4ee5\u53ca\u9884\u6d4b"},{"location":"3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/","text":"Frustum PointNets for 3D Object Detection from RGB-D Data \u8fd9\u7bc7\u8bba\u6587\u7ed9\u51fa\u4e86\u4f7f\u7528RGB-D\u6570\u636e\u8fdb\u884c3D\u68c0\u6d4b\u7684baseline pipeline. \u7f51\u7edc\u7ed3\u6784 \u5bf9RGB\u56fe\u7247\u6267\u884c2D detection\uff0c\u7ed9\u51fa2D proposal\u4ee5\u53ca\u5206\u7c7bone-hot\u77e2\u91cf,\u4eceproposal\u4e2d\u91c7\u6837\u51fan\u4e2a\u70b9\uff0c\u4f7f\u7528pointnet\u8fdb\u884cInstance Segmentation\u533a\u5206\u80cc\u666f\u4ee5\u53caforeground,\u6267\u884cmasking\u518d\u91c7\u6837m\u4e2a\u70b9\uff0c\u4f7f\u7528T-Net\u7b49\u56de\u5f52\u4f30\u8ba1\u4e09\u7ef4\u6846\u7ed3\u679c \u4ee3\u7801\u4e2d\u4f7f\u7528\u7684\u5750\u6807\u7cfb\u793a\u610f\u56fe PointNet\u4f7f\u7528\u7684\u7ed3\u6784 \u70b9\u4e91\u8bed\u4e49\u5206\u5272\u4e2d\uff0c\u9664\u4e86\u4f7f\u7528pointnet\u4e4b\u5916\uff0c\u5728\u8f93\u5165\u7279\u5f81\u4e2d\u8fd8concat\u4e86\u8bed\u4e49\u5206\u7c7b\u7684category\u3002 \u5728\u70b9\u4e91\u56de\u5f52\u65f6\uff0c\u5148\u7528T-Net\u6c42\u51fa\u4e2d\u5fc3\u70b9\u5750\u6807\uff0c\u5c06\u5269\u4f59\u70b9\u8f6c\u6362\u4e3a\u4ee5\u4e2d\u5fc3\u70b9\u4e3a\u4e2d\u5fc3\u7684\u4f4d\u7f6e\u4e0a\u5728\u4f30\u8ba1box size Loss Multitask Loss Corner Loss","title":"Frustum PointNets for 3D Object Detection from RGB-D Data"},{"location":"3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/#frustum-pointnets-for-3d-object-detection-from-rgb-d-data","text":"\u8fd9\u7bc7\u8bba\u6587\u7ed9\u51fa\u4e86\u4f7f\u7528RGB-D\u6570\u636e\u8fdb\u884c3D\u68c0\u6d4b\u7684baseline pipeline.","title":"Frustum PointNets for 3D Object Detection from RGB-D Data"},{"location":"3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/#_1","text":"\u5bf9RGB\u56fe\u7247\u6267\u884c2D detection\uff0c\u7ed9\u51fa2D proposal\u4ee5\u53ca\u5206\u7c7bone-hot\u77e2\u91cf,\u4eceproposal\u4e2d\u91c7\u6837\u51fan\u4e2a\u70b9\uff0c\u4f7f\u7528pointnet\u8fdb\u884cInstance Segmentation\u533a\u5206\u80cc\u666f\u4ee5\u53caforeground,\u6267\u884cmasking\u518d\u91c7\u6837m\u4e2a\u70b9\uff0c\u4f7f\u7528T-Net\u7b49\u56de\u5f52\u4f30\u8ba1\u4e09\u7ef4\u6846\u7ed3\u679c \u4ee3\u7801\u4e2d\u4f7f\u7528\u7684\u5750\u6807\u7cfb\u793a\u610f\u56fe PointNet\u4f7f\u7528\u7684\u7ed3\u6784 \u70b9\u4e91\u8bed\u4e49\u5206\u5272\u4e2d\uff0c\u9664\u4e86\u4f7f\u7528pointnet\u4e4b\u5916\uff0c\u5728\u8f93\u5165\u7279\u5f81\u4e2d\u8fd8concat\u4e86\u8bed\u4e49\u5206\u7c7b\u7684category\u3002 \u5728\u70b9\u4e91\u56de\u5f52\u65f6\uff0c\u5148\u7528T-Net\u6c42\u51fa\u4e2d\u5fc3\u70b9\u5750\u6807\uff0c\u5c06\u5269\u4f59\u70b9\u8f6c\u6362\u4e3a\u4ee5\u4e2d\u5fc3\u70b9\u4e3a\u4e2d\u5fc3\u7684\u4f4d\u7f6e\u4e0a\u5728\u4f30\u8ba1box size","title":"\u7f51\u7edc\u7ed3\u6784"},{"location":"3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/#loss","text":"Multitask Loss Corner Loss","title":"Loss"},{"location":"3dDetection/Generalize3DDet/","text":"Train in Germany, Test in The USA: Making 3D Object Detectors Generalize \u8fd9\u7bc7paper\u8ba8\u8bba\u7684\u662f\u70b9\u4e913D\u68c0\u6d4b\u5668\u7684\u9c81\u68d2\u6027\u95ee\u9898\u3002\u5e76\u63d0\u51fadoman transfer\u7684\u95ee\u9898 Biased in Datasets \u4f5c\u8005\u5206\u6790\u7814\u7a76\u4e86\u4e94\u4e2a\u6570\u636e\u96c6\u7684\u7279\u70b9\uff0c\u5206\u522b\u662f KITTI , Argoverse , nuScenes , Lyft , Waymo . (\u6ce8\uff0cArgoverse\u4e0eKITTI\u90fd\u6709\u53cc\u76ee) \u5148\u4f7f\u7528\u4e00\u4e2a PointRCNN ,\u5728\u5404\u4e2a\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u770b\u770b\u76f4\u63a5\u8de8\u6570\u636e\u96c6test\u7684\u7ed3\u679c\u3002 \u4f5c\u8005\u6307\u51fanuScenes\u4e0eArgoverse\u4f5c\u4e3atarget\u7684\u65f6\u5019\u6027\u80fd\u4e0b\u964d\u7279\u522b\u5389\u5bb3\uff0c\u8fd9\u53ef\u80fd\u662f\u4e0eLidar\u70b9\u6570\u7684\u8f93\u5165\u6709\u5173\u7684\u3002nuScenes\u53ea\u6709\u4e00\u4e2a32\u7ebf\u7684\u96f7\u8fbe\uff0c\u800cArgoverse\u662f\u7528\u4e24\u4e2a32\u7ebf\u7684\u96f7\u8fbe\u76f4\u63a5\u4e0a\u4e0b\u5806\u53e0\u5f62\u6210\u4e00\u4e2a64\u7ebf\u7684\u7ed3\u679c\u3002 \u540c\u65f6\u7531\u4e8e\u5730\u70b9\u4ee5\u53ca\u6807\u6ce8\u504f\u5dee\u7684\u539f\u56e0\uff0c\u4e0d\u540c\u6570\u636e\u96c6\u8f66\u8f86\u957f\u5bbd\u9ad8\u7684\u5206\u5e03\u5dee\u8ddd\u5f88\u5927\uff0c\u4f5c\u8005\u8ba4\u4e3a\u8fd9\u662f\u56e0\u4e3a\u8f66\u5b50\u7684\u5730\u533a\u5dee\u5f02\u3002 (\u6ce8\uff1a\u4e2a\u4eba\u89c2\u5bdf\u4e0d\u540c\u6570\u636e\u96c6\u65f6\uff0c\u5f97\u5230\u7684\u7ed3\u8bba\u5176\u5b9e\u662f\u53ea\u6709KITTI\u76843D\u6846\u662f\u4e25\u683c\u4e0e\u7269\u4f53\u7d27\u8d34\u7684\uff0c\u5176\u4ed6\u6570\u636e\u96c6\u7684\u6846\u6295\u5f71\u5230\u76f8\u673a\u4e0a\u80fd\u770b\u5230\u663e\u7136\u662f\u4e0d\u7d27\u8d34\u7684) \u4f5c\u8005\u505a\u4e86\u4e00\u4e2a\u5b9e\u9a8c\uff0c\u5bf9\u6240\u6709IoU > 0.2\u4e3a\u754c\u9650\u5339\u914d\u7684prediction\uff0c\u4fdd\u6301\u5176\u4e2d\u5fc3\u4f4d\u7f6e\u4e0e\u89d2\u5ea6\u4e0d\u53d8\uff0c\u5c06whl\u6539\u4e3a\u6b63\u786e\u503c\uff0ccross domain\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002\u4f5c\u8005\u4e0b\u7684\u7ed3\u8bba\u5c31\u662f\u7f51\u7edc\u9884\u6d4b\u4e0d\u6b63\u786e\u7684\u539f\u56e0\u5355\u7eaf\u5730\u5c31\u662f\u56e0\u4e3awhl\u4e0d\u6b63\u786e\u3002 Generalize to differet datasets \u505a\u6cd5\u4e0e\u7ed3\u8bba\u5982\u4e0b: fine tuning\u6709\u7528\uff0c\u4ec5\u572820\u4e2ascene\u4e0a\u5fae\u8c03\u7684\u7ed3\u679c\u80fd\u591f\u6bd4\u5f97\u4e0a\u5728\u539f\u6765500\u4e2ascene\u4e0a\u76f4\u63a5\u8bad\u7ec3\u7684\u6027\u80fd\u3002 \u4e0a\u56fe\u4f7f\u7528\u7684Statistical Normalization\u8fdb\u884c\u5fae\u8c03\uff08\u6ca1\u6709\u4f7f\u7528\u65b0domain\u7684\u6807\u6ce8\u6570\u636e\uff09\uff0c\u6027\u80fd\u4e5f\u80fd\u5f88\u597d\u3002 \u4e0dfinetune,\u5728\u70b9\u4e91\u7ed3\u679c\u76f4\u63a5\u9644\u52a0\u4e00\u4e2a\u6570\u636e\u96c6\u7684mean\u504f\u79fb\uff0c\u6027\u80fd\u6709\u90e8\u5206\u63d0\u5347\uff0c\u4f46\u662f\u53ef\u80fd\u4f1aover-react(\u56e0\u4e3a\u7f51\u7edc\u672c\u8eab\u5c31\u4f1aadjust\u4e00\u90e8\u5206\uff0c\u518d\u52a0\u504f\u79fb\u4f1aover-react)","title":"Train in Germany, Test in The USA: Making 3D Object Detectors Generalize"},{"location":"3dDetection/Generalize3DDet/#train-in-germany-test-in-the-usa-making-3d-object-detectors-generalize","text":"\u8fd9\u7bc7paper\u8ba8\u8bba\u7684\u662f\u70b9\u4e913D\u68c0\u6d4b\u5668\u7684\u9c81\u68d2\u6027\u95ee\u9898\u3002\u5e76\u63d0\u51fadoman transfer\u7684\u95ee\u9898","title":"Train in Germany, Test in The USA: Making 3D Object Detectors Generalize"},{"location":"3dDetection/Generalize3DDet/#biased-in-datasets","text":"\u4f5c\u8005\u5206\u6790\u7814\u7a76\u4e86\u4e94\u4e2a\u6570\u636e\u96c6\u7684\u7279\u70b9\uff0c\u5206\u522b\u662f KITTI , Argoverse , nuScenes , Lyft , Waymo . (\u6ce8\uff0cArgoverse\u4e0eKITTI\u90fd\u6709\u53cc\u76ee) \u5148\u4f7f\u7528\u4e00\u4e2a PointRCNN ,\u5728\u5404\u4e2a\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u770b\u770b\u76f4\u63a5\u8de8\u6570\u636e\u96c6test\u7684\u7ed3\u679c\u3002 \u4f5c\u8005\u6307\u51fanuScenes\u4e0eArgoverse\u4f5c\u4e3atarget\u7684\u65f6\u5019\u6027\u80fd\u4e0b\u964d\u7279\u522b\u5389\u5bb3\uff0c\u8fd9\u53ef\u80fd\u662f\u4e0eLidar\u70b9\u6570\u7684\u8f93\u5165\u6709\u5173\u7684\u3002nuScenes\u53ea\u6709\u4e00\u4e2a32\u7ebf\u7684\u96f7\u8fbe\uff0c\u800cArgoverse\u662f\u7528\u4e24\u4e2a32\u7ebf\u7684\u96f7\u8fbe\u76f4\u63a5\u4e0a\u4e0b\u5806\u53e0\u5f62\u6210\u4e00\u4e2a64\u7ebf\u7684\u7ed3\u679c\u3002 \u540c\u65f6\u7531\u4e8e\u5730\u70b9\u4ee5\u53ca\u6807\u6ce8\u504f\u5dee\u7684\u539f\u56e0\uff0c\u4e0d\u540c\u6570\u636e\u96c6\u8f66\u8f86\u957f\u5bbd\u9ad8\u7684\u5206\u5e03\u5dee\u8ddd\u5f88\u5927\uff0c\u4f5c\u8005\u8ba4\u4e3a\u8fd9\u662f\u56e0\u4e3a\u8f66\u5b50\u7684\u5730\u533a\u5dee\u5f02\u3002 (\u6ce8\uff1a\u4e2a\u4eba\u89c2\u5bdf\u4e0d\u540c\u6570\u636e\u96c6\u65f6\uff0c\u5f97\u5230\u7684\u7ed3\u8bba\u5176\u5b9e\u662f\u53ea\u6709KITTI\u76843D\u6846\u662f\u4e25\u683c\u4e0e\u7269\u4f53\u7d27\u8d34\u7684\uff0c\u5176\u4ed6\u6570\u636e\u96c6\u7684\u6846\u6295\u5f71\u5230\u76f8\u673a\u4e0a\u80fd\u770b\u5230\u663e\u7136\u662f\u4e0d\u7d27\u8d34\u7684) \u4f5c\u8005\u505a\u4e86\u4e00\u4e2a\u5b9e\u9a8c\uff0c\u5bf9\u6240\u6709IoU > 0.2\u4e3a\u754c\u9650\u5339\u914d\u7684prediction\uff0c\u4fdd\u6301\u5176\u4e2d\u5fc3\u4f4d\u7f6e\u4e0e\u89d2\u5ea6\u4e0d\u53d8\uff0c\u5c06whl\u6539\u4e3a\u6b63\u786e\u503c\uff0ccross domain\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002\u4f5c\u8005\u4e0b\u7684\u7ed3\u8bba\u5c31\u662f\u7f51\u7edc\u9884\u6d4b\u4e0d\u6b63\u786e\u7684\u539f\u56e0\u5355\u7eaf\u5730\u5c31\u662f\u56e0\u4e3awhl\u4e0d\u6b63\u786e\u3002","title":"Biased in Datasets"},{"location":"3dDetection/Generalize3DDet/#generalize-to-differet-datasets","text":"\u505a\u6cd5\u4e0e\u7ed3\u8bba\u5982\u4e0b: fine tuning\u6709\u7528\uff0c\u4ec5\u572820\u4e2ascene\u4e0a\u5fae\u8c03\u7684\u7ed3\u679c\u80fd\u591f\u6bd4\u5f97\u4e0a\u5728\u539f\u6765500\u4e2ascene\u4e0a\u76f4\u63a5\u8bad\u7ec3\u7684\u6027\u80fd\u3002 \u4e0a\u56fe\u4f7f\u7528\u7684Statistical Normalization\u8fdb\u884c\u5fae\u8c03\uff08\u6ca1\u6709\u4f7f\u7528\u65b0domain\u7684\u6807\u6ce8\u6570\u636e\uff09\uff0c\u6027\u80fd\u4e5f\u80fd\u5f88\u597d\u3002 \u4e0dfinetune,\u5728\u70b9\u4e91\u7ed3\u679c\u76f4\u63a5\u9644\u52a0\u4e00\u4e2a\u6570\u636e\u96c6\u7684mean\u504f\u79fb\uff0c\u6027\u80fd\u6709\u90e8\u5206\u63d0\u5347\uff0c\u4f46\u662f\u53ef\u80fd\u4f1aover-react(\u56e0\u4e3a\u7f51\u7edc\u672c\u8eab\u5c31\u4f1aadjust\u4e00\u90e8\u5206\uff0c\u518d\u52a0\u504f\u79fb\u4f1aover-react)","title":"Generalize to differet datasets"},{"location":"3dDetection/GeneralizedIoU/","text":"Generalized IoU for 2D and 3D \u8fd9\u91cc\u5c1d\u8bd5\u540c\u65f6\u8bb0\u5f55\u4e24\u7bc7\u76f8\u5173\u7684\u8bba\u6587\uff0c\u5206\u522b\u63cf\u8ff0\u7684Generalized 2D IoU \u4e0eGeneralized 3D IoU\uff0c\u6700\u7ec8\u89e3\u51b3\u7684\u90fd\u662f\u5728\u6ca1\u6709intersection\u7684\u60c5\u51b5\u4e0b\u5f97\u5230\u4e00\u4e2a\u5bf9IoU\u7684\u4f30\u8ba1 Generalized Intersection over Union: A Metric and A Loss for Bounding Box Regression \u8fd9\u7bc7\u662fCVPR\u4f7f\u7528Generalized 2D IoU\u7684\u539f\u6587\uff0c\u81f3\u4e8e\u662f\u8fd9\u4e2a\u7b97\u6cd5\u597d\uff0c\u8fd8\u662f signed_iou \u6548\u679c\u597d\uff0c\u6709\u5f85\u6d4b\u8bd5 3D-GIoU: 3D Generalized Intersection over Union for Object Detection in Point Cloud 3D IoU\u7684\u7b97\u6cd5\u5728 \u8fd9\u7bc7\u6587\u7ae0 \u6709\u8be6\u7ec6\u63cf\u8ff0 \u7b97\u6cd5","title":"Generalized IoU for 2D and 3D"},{"location":"3dDetection/GeneralizedIoU/#generalized-iou-for-2d-and-3d","text":"\u8fd9\u91cc\u5c1d\u8bd5\u540c\u65f6\u8bb0\u5f55\u4e24\u7bc7\u76f8\u5173\u7684\u8bba\u6587\uff0c\u5206\u522b\u63cf\u8ff0\u7684Generalized 2D IoU \u4e0eGeneralized 3D IoU\uff0c\u6700\u7ec8\u89e3\u51b3\u7684\u90fd\u662f\u5728\u6ca1\u6709intersection\u7684\u60c5\u51b5\u4e0b\u5f97\u5230\u4e00\u4e2a\u5bf9IoU\u7684\u4f30\u8ba1","title":"Generalized IoU for 2D and 3D"},{"location":"3dDetection/GeneralizedIoU/#generalized-intersection-over-union-a-metric-and-a-loss-for-bounding-box-regression","text":"\u8fd9\u7bc7\u662fCVPR\u4f7f\u7528Generalized 2D IoU\u7684\u539f\u6587\uff0c\u81f3\u4e8e\u662f\u8fd9\u4e2a\u7b97\u6cd5\u597d\uff0c\u8fd8\u662f signed_iou \u6548\u679c\u597d\uff0c\u6709\u5f85\u6d4b\u8bd5","title":"Generalized Intersection over Union: A Metric and A Loss for Bounding Box Regression"},{"location":"3dDetection/GeneralizedIoU/#3d-giou-3d-generalized-intersection-over-union-for-object-detection-in-point-cloud","text":"3D IoU\u7684\u7b97\u6cd5\u5728 \u8fd9\u7bc7\u6587\u7ae0 \u6709\u8be6\u7ec6\u63cf\u8ff0 \u7b97\u6cd5","title":"3D-GIoU: 3D Generalized Intersection over Union for Object Detection in Point Cloud"},{"location":"3dDetection/GroundAwareConvultion/","text":"Ground-aware Monocular 3D Object Detection for Autonomous Driving (EN) This is my paper accepcted by RAL 2021. The open-sourced code is in https://github.com/Owen-Liuyuxuan/visualDet3D . The basic idea is an attempt to mimic how people perceive depth from a single image, and more importantly try to incorperate calibration matrix and ground plane information into the detection model. Core Operations and Code Placement Precomputing statistics for anchors: script github page Using the statistics for anchors: head github page Ground-Aware Convolution Module: block github page Change the \"cfg.detector.name\" in config to Yolo3D and experiment with DeformConv (which also provide robust and top performance). Result for the published model: Release Page Benchmark Easy Moderate Hard Car Detection 92.35 79.57 59.61 Car Orientation 90.87 77.47 57.99 Car 3D Detection 21.60 13.17 9.94 Car Bird's Eye View 29.38 18.00 13.14","title":"Ground-aware Monocular 3D Object Detection for Autonomous Driving (EN)"},{"location":"3dDetection/GroundAwareConvultion/#ground-aware-monocular-3d-object-detection-for-autonomous-driving-en","text":"This is my paper accepcted by RAL 2021. The open-sourced code is in https://github.com/Owen-Liuyuxuan/visualDet3D . The basic idea is an attempt to mimic how people perceive depth from a single image, and more importantly try to incorperate calibration matrix and ground plane information into the detection model.","title":"Ground-aware Monocular 3D Object Detection for Autonomous Driving (EN)"},{"location":"3dDetection/GroundAwareConvultion/#core-operations-and-code-placement","text":"Precomputing statistics for anchors: script github page Using the statistics for anchors: head github page Ground-Aware Convolution Module: block github page Change the \"cfg.detector.name\" in config to Yolo3D and experiment with DeformConv (which also provide robust and top performance).","title":"Core Operations and Code Placement"},{"location":"3dDetection/GroundAwareConvultion/#result-for-the-published-model","text":"Release Page Benchmark Easy Moderate Hard Car Detection 92.35 79.57 59.61 Car Orientation 90.87 77.47 57.99 Car 3D Detection 21.60 13.17 9.94 Car Bird's Eye View 29.38 18.00 13.14","title":"Result for the published model:"},{"location":"3dDetection/H23D_RCNN/","text":"From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN) \u8fd9\u7bc7paper\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u8de8\u89c6\u89d2\u878d\u5408\u7684two-stage\u70b9\u4e91\u68c0\u6d4b\u7b97\u6cd5\u3002\u5176\u4e3b\u8981\u7684motivation\u662f\u8bf4\u70b9\u4e91\u662f\u5728\u7269\u4f53\u8868\u9762\u7684\uff0c\u6240\u4ee5\u7528two-stage\u53ef\u80fd\u66f4\u597d\u3002 \u65b9\u6cd5: \u591a\u89c6\u89d2\u7279\u5f81\u63d0\u53d6: Multiview Grid Indexing: BEV\u4f7f\u7528\u7b1b\u5361\u5c14\u5750\u6807\u7cfb\uff0c\u800cPerspective View(PV)\u4f7f\u7528\u5706\u67f1\u5750\u6807\u7cfb\u3002 \u5bf9\u6bcf\u4e2a\u70b9\u5148\u7528\u5168\u8fde\u63a5\u63d0\u4e00\u6b21\u7279\u5f81\uff0c\u5728\u6295\u5c04\u5230PV/BEV\u65f6\uff0c\u7531\u4e8evoxelization,\u4f1a\u6709\u91cd\u53e0\u70b9\uff0c\u8fd9\u91cc\u91c7\u7528\u7684\u65b9\u6848\u662fmax-pooling. PV backbone \u4e0e BEV backbone\u6743\u91cd\u5171\u4eab\u3002 \u4f46\u662fBEV\u80fd\u83b7\u53d6PV\u7684\u4fe1\u606f\u3002 H3D Feature Hallucination \u8fd9\u4e2a\u6a21\u5757\u7684\u8f93\u5165\u662f\u6765\u81ea\u4e24\u4e2a\u6a21\u6001\u7684\u70b9\u96c6\u7279\u5f81\uff0c\u8f93\u51fa\u7684\u5f62\u6001\u4e0a\u6765\u8bf4\u662f proposal generator, \u4e5f\u5c31\u662f\u7b2c\u4e00\u9636\u6bb5\u7684\u7ed3\u679c. \u8fd9\u91cc\u548cPointRCNN\u5f88\u50cf\uff0c\u8ba9\u6bcf\u4e00\u4e2a\u70b9\u7ed9\u51fa\u4e00\u4e2aproposal. \u878d\u5408\u7684\u65f6\u5019\uff0c\u6587\u7ae0\u63d0\u51fa\u65b0\u878d\u5408\u6a21\u5757 Bilaterally Guided Multi-View Fusion(BGMVF) \u8f93\u51fa\u662fpoint-wise H3D \u7279\u5f81\uff0c\u518d\u6b21\u8fdb\u884cvoxelization, \u5176indexing \u65b9\u6cd5\u662f i_n, j_n, k_n \uff0c \u5176\u4e2d (i_n, j_n) \u5bf9\u5e94BEV\u7684\u4f4d\u7f6e\uff0c k_n \u5bf9\u5e94PV\u7684\u9ad8\u5ea6\u3002\u518d\u6b21\u7528max-pooling\u8fdb\u884cvoxelization. Box Refinement \u8fd9\u6307\u7684\u662f\u65b9\u6cd5\u7684second stage. \u65b9\u6cd5\u7684meta architecture \u5982\u56fe","title":"From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)"},{"location":"3dDetection/H23D_RCNN/#from-multi-view-to-hollow-3d-hallucinated-hollow-3d-r-cnn-for-3d-object-detection-h23d-rcnn","text":"\u8fd9\u7bc7paper\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u8de8\u89c6\u89d2\u878d\u5408\u7684two-stage\u70b9\u4e91\u68c0\u6d4b\u7b97\u6cd5\u3002\u5176\u4e3b\u8981\u7684motivation\u662f\u8bf4\u70b9\u4e91\u662f\u5728\u7269\u4f53\u8868\u9762\u7684\uff0c\u6240\u4ee5\u7528two-stage\u53ef\u80fd\u66f4\u597d\u3002","title":"From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)"},{"location":"3dDetection/H23D_RCNN/#_1","text":"","title":"\u65b9\u6cd5:"},{"location":"3dDetection/H23D_RCNN/#_2","text":"Multiview Grid Indexing: BEV\u4f7f\u7528\u7b1b\u5361\u5c14\u5750\u6807\u7cfb\uff0c\u800cPerspective View(PV)\u4f7f\u7528\u5706\u67f1\u5750\u6807\u7cfb\u3002 \u5bf9\u6bcf\u4e2a\u70b9\u5148\u7528\u5168\u8fde\u63a5\u63d0\u4e00\u6b21\u7279\u5f81\uff0c\u5728\u6295\u5c04\u5230PV/BEV\u65f6\uff0c\u7531\u4e8evoxelization,\u4f1a\u6709\u91cd\u53e0\u70b9\uff0c\u8fd9\u91cc\u91c7\u7528\u7684\u65b9\u6848\u662fmax-pooling. PV backbone \u4e0e BEV backbone\u6743\u91cd\u5171\u4eab\u3002 \u4f46\u662fBEV\u80fd\u83b7\u53d6PV\u7684\u4fe1\u606f\u3002","title":"\u591a\u89c6\u89d2\u7279\u5f81\u63d0\u53d6:"},{"location":"3dDetection/H23D_RCNN/#h3d-feature-hallucination","text":"\u8fd9\u4e2a\u6a21\u5757\u7684\u8f93\u5165\u662f\u6765\u81ea\u4e24\u4e2a\u6a21\u6001\u7684\u70b9\u96c6\u7279\u5f81\uff0c\u8f93\u51fa\u7684\u5f62\u6001\u4e0a\u6765\u8bf4\u662f proposal generator, \u4e5f\u5c31\u662f\u7b2c\u4e00\u9636\u6bb5\u7684\u7ed3\u679c. \u8fd9\u91cc\u548cPointRCNN\u5f88\u50cf\uff0c\u8ba9\u6bcf\u4e00\u4e2a\u70b9\u7ed9\u51fa\u4e00\u4e2aproposal. \u878d\u5408\u7684\u65f6\u5019\uff0c\u6587\u7ae0\u63d0\u51fa\u65b0\u878d\u5408\u6a21\u5757 Bilaterally Guided Multi-View Fusion(BGMVF) \u8f93\u51fa\u662fpoint-wise H3D \u7279\u5f81\uff0c\u518d\u6b21\u8fdb\u884cvoxelization, \u5176indexing \u65b9\u6cd5\u662f i_n, j_n, k_n \uff0c \u5176\u4e2d (i_n, j_n) \u5bf9\u5e94BEV\u7684\u4f4d\u7f6e\uff0c k_n \u5bf9\u5e94PV\u7684\u9ad8\u5ea6\u3002\u518d\u6b21\u7528max-pooling\u8fdb\u884cvoxelization.","title":"H3D Feature Hallucination"},{"location":"3dDetection/H23D_RCNN/#box-refinement","text":"\u8fd9\u6307\u7684\u662f\u65b9\u6cd5\u7684second stage. \u65b9\u6cd5\u7684meta architecture \u5982\u56fe","title":"Box Refinement"},{"location":"3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/","text":"Improving 3D Object Detection for Pedestrians with Virtual Multi-View Synthesis Orientation Estimation \u8fd9\u7bc7\u8bba\u6587\u8ba8\u8bba\u7684\u95ee\u9898\u662f\u63d0\u53473D\u884c\u4eba\u68c0\u6d4b\u8fc7\u7a0b\u4e2d\u5bf9\u884c\u4eba\u671d\u5411\u89d2\u5ea6\u7684\u9884\u6d4b\u7cbe\u5ea6\u95ee\u9898\u3002\u91c7\u7528\u7684\u601d\u8def\u662f\u4f7f\u7528\u51e0\u4e2a\u4eba\u5de5\u89d2\u5ea6\u5408\u6210\u76f8\u7247\u5b9e\u73b0\u5f88\u5de7\u5999\u7684pooling.\u8f93\u5165\u662fRGB + Lidar Update: 2019.12.21: Code has been open-sourced \u603b\u4f53\u601d\u8def \u603b\u4f53\u601d\u8def: \u4f7f\u7528RGB\u56fe\u50cf\u4e0eLidar\u6570\u636e\uff0c\u8fdb\u884cDepth Completion and colorization \u5f62\u6210\u5f69\u8272\u7684\u66f4\u5bc6\u96c6\u7684\u70b9\u4e91\u573a\u666f \u8fdb\u884c3D\u7269\u4f53\u68c0\u6d4b,\u6c42\u51fa3D box\u53c2\u6570\u4ee5\u53ca\u7269\u4f53\u4e2d\u5fc3\u3002 \u6839\u636e\u7269\u4f53\u4e2d\u5fc3\uff0c\u5728\u8bbe\u8ba1\u7684\u7269\u4f53\u653e\u7f6e\u6570\u4e2a\u4e2a\u865a\u62df\u7684\u76f8\u673a\uff0c\u5c06\u5f69\u8272\u70b9\u4e91\u6295\u5f71\u5230\u8fd9\u51e0\u4e2a\u76f8\u673a\u5185\uff0c\u5f97\u5230\u51e0\u5f20\u5408\u6210\u7684\u56fe\u7247 \u5c06\u8fd9\u4e2a\u5408\u6210\u7684\u56fe\u7247\u8f93\u5165\u5230CNN\u4e2d\u8fdb\u4e00\u6b65\u8f93\u51fa\u89d2\u5ea6\u9884\u6d4b\u3002 \u5408\u6210\u76f8\u673a\u5e03\u7f6e \u76f8\u673a\u653e\u7f6e\u65b9\u5f0f,\u4e0e\u539f\u6765\u76f8\u5bf9\u5c04\u7ebf\u76f8\u5bf9\u00b125\u00b0\u4e4b\u95f4\u8fde\u7eed\u653e\u7f6e\uff0c\u672c\u6587\u91c7\u7528\u7684\u662f11\u4e2a\u865a\u62df\u76f8\u673a\u7684\u6392\u5217\u3002 \u6df1\u5ea6\u8865\u5168\u4e0a\u8272 \u539f\u6765\u7684\u70b9\u4e91\u53ea\u80fd\u63d0\u4f9b\u4e00\u4e2a\u7a00\u758f\u7684\u6df1\u5ea6\u4f30\u8ba1\uff0c\u672c\u6587\u5229\u7528\u4e86\u4f5c\u8005\u7684 \u53e6\u4e00\u7bc7\u6df1\u5ea6\u8865\u5168\u7b97\u6cd5.pdf , \u7b80\u4ecb ,\u5c06\u7a00\u758f\u7684\u70b9\u4e91\u53d8\u6210\u5bc6\u96c6\u7684\u6df1\u5ea6\u56fe\uff0c\u4e5f\u5c31\u53ef\u4ee5\u5bf9\u5e94\u50cf\u7d20\u4e0a\u8272\u3002 \u5176\u4ed6\u8bad\u7ec3\u7ec6\u8282 \u57fa\u7840\u76843D\u68c0\u6d4b\u4f7f\u7528\u7684\u662f AVOD.pdf ,\u89d2\u5ea6\u4f30\u8ba1\u4f7f\u7528\u7684\u662f\u5728kitti 2D\u68c0\u6d4b\u95ee\u9898\u4e2d\u9884\u8bad\u7ec3\u7684Res101(\u4e2a\u4eba\u7406\u89e3\u8fd9\u662f\u4e00\u4e2a\u76f8\u5f53\u5927\u7684\u7f51\u7edc) \u672c\u6587\u7684\u6210\u679c\u662f\u5c06\u884c\u4eba\u4e09\u7ef4\u68c0\u6d4b\u7684\u89d2\u5ea6\u4f30\u8ba1\u7cbe\u5ea6\u5927\u5e45\u5ea6\u63d0\u5347\u3002","title":"Improving 3D Object Detection for Pedestrians with Virtual Multi-View Synthesis Orientation Estimation"},{"location":"3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/#improving-3d-object-detection-for-pedestrians-with-virtual-multi-view-synthesis-orientation-estimation","text":"\u8fd9\u7bc7\u8bba\u6587\u8ba8\u8bba\u7684\u95ee\u9898\u662f\u63d0\u53473D\u884c\u4eba\u68c0\u6d4b\u8fc7\u7a0b\u4e2d\u5bf9\u884c\u4eba\u671d\u5411\u89d2\u5ea6\u7684\u9884\u6d4b\u7cbe\u5ea6\u95ee\u9898\u3002\u91c7\u7528\u7684\u601d\u8def\u662f\u4f7f\u7528\u51e0\u4e2a\u4eba\u5de5\u89d2\u5ea6\u5408\u6210\u76f8\u7247\u5b9e\u73b0\u5f88\u5de7\u5999\u7684pooling.\u8f93\u5165\u662fRGB + Lidar Update: 2019.12.21: Code has been open-sourced","title":"Improving 3D Object Detection for Pedestrians with Virtual Multi-View Synthesis Orientation Estimation"},{"location":"3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/#_1","text":"\u603b\u4f53\u601d\u8def: \u4f7f\u7528RGB\u56fe\u50cf\u4e0eLidar\u6570\u636e\uff0c\u8fdb\u884cDepth Completion and colorization \u5f62\u6210\u5f69\u8272\u7684\u66f4\u5bc6\u96c6\u7684\u70b9\u4e91\u573a\u666f \u8fdb\u884c3D\u7269\u4f53\u68c0\u6d4b,\u6c42\u51fa3D box\u53c2\u6570\u4ee5\u53ca\u7269\u4f53\u4e2d\u5fc3\u3002 \u6839\u636e\u7269\u4f53\u4e2d\u5fc3\uff0c\u5728\u8bbe\u8ba1\u7684\u7269\u4f53\u653e\u7f6e\u6570\u4e2a\u4e2a\u865a\u62df\u7684\u76f8\u673a\uff0c\u5c06\u5f69\u8272\u70b9\u4e91\u6295\u5f71\u5230\u8fd9\u51e0\u4e2a\u76f8\u673a\u5185\uff0c\u5f97\u5230\u51e0\u5f20\u5408\u6210\u7684\u56fe\u7247 \u5c06\u8fd9\u4e2a\u5408\u6210\u7684\u56fe\u7247\u8f93\u5165\u5230CNN\u4e2d\u8fdb\u4e00\u6b65\u8f93\u51fa\u89d2\u5ea6\u9884\u6d4b\u3002","title":"\u603b\u4f53\u601d\u8def"},{"location":"3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/#_2","text":"\u76f8\u673a\u653e\u7f6e\u65b9\u5f0f,\u4e0e\u539f\u6765\u76f8\u5bf9\u5c04\u7ebf\u76f8\u5bf9\u00b125\u00b0\u4e4b\u95f4\u8fde\u7eed\u653e\u7f6e\uff0c\u672c\u6587\u91c7\u7528\u7684\u662f11\u4e2a\u865a\u62df\u76f8\u673a\u7684\u6392\u5217\u3002","title":"\u5408\u6210\u76f8\u673a\u5e03\u7f6e"},{"location":"3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/#_3","text":"\u539f\u6765\u7684\u70b9\u4e91\u53ea\u80fd\u63d0\u4f9b\u4e00\u4e2a\u7a00\u758f\u7684\u6df1\u5ea6\u4f30\u8ba1\uff0c\u672c\u6587\u5229\u7528\u4e86\u4f5c\u8005\u7684 \u53e6\u4e00\u7bc7\u6df1\u5ea6\u8865\u5168\u7b97\u6cd5.pdf , \u7b80\u4ecb ,\u5c06\u7a00\u758f\u7684\u70b9\u4e91\u53d8\u6210\u5bc6\u96c6\u7684\u6df1\u5ea6\u56fe\uff0c\u4e5f\u5c31\u53ef\u4ee5\u5bf9\u5e94\u50cf\u7d20\u4e0a\u8272\u3002","title":"\u6df1\u5ea6\u8865\u5168\u4e0a\u8272"},{"location":"3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/#_4","text":"\u57fa\u7840\u76843D\u68c0\u6d4b\u4f7f\u7528\u7684\u662f AVOD.pdf ,\u89d2\u5ea6\u4f30\u8ba1\u4f7f\u7528\u7684\u662f\u5728kitti 2D\u68c0\u6d4b\u95ee\u9898\u4e2d\u9884\u8bad\u7ec3\u7684Res101(\u4e2a\u4eba\u7406\u89e3\u8fd9\u662f\u4e00\u4e2a\u76f8\u5f53\u5927\u7684\u7f51\u7edc) \u672c\u6587\u7684\u6210\u679c\u662f\u5c06\u884c\u4eba\u4e09\u7ef4\u68c0\u6d4b\u7684\u89d2\u5ea6\u4f30\u8ba1\u7cbe\u5ea6\u5927\u5e45\u5ea6\u63d0\u5347\u3002","title":"\u5176\u4ed6\u8bad\u7ec3\u7ec6\u8282"},{"location":"3dDetection/IoU%20Loss%20for%202D/","text":"IoU Loss for 2D/3D Object Detection \u8fd9\u7bc7\u8bba\u6587\u7684\u6838\u5fc3\u8d21\u732e\u662f\u7ed9\u51fa\u4e86\u6240\u8c13\u7684IoU Loss,\u56e0\u4e3a\u62fc\u70b9\u7684\u8fc7\u7a0b\u4e2d\u5173\u952e\u70b9\u5728\u4e8e\u63d0\u5347IoU\u7684\u503c\uff0c\u4f46\u662f\u6211\u4eec\u76ee\u524d\u6ca1\u6709\u76f4\u63a5\u68af\u5ea6\u4f18\u5316IoU\u7684\u65b9\u6cd5\uff0c\u8fd9\u7bc7\u6587\u7ae0\u7ed9\u51fa\u7684IoU Loss\u5c31\u662f\u7ed9\u51fa\u4e00\u4e2a\u53ef\u5fae\u5206\u7684\u635f\u5931\u51fd\u6570\uff0c\u901a\u8fc7\u76f4\u63a5\u4f18\u5316IoU\u63d0\u9ad8\u5206\u6570 \u4e00\u822c\u5e26\u6709\u65cb\u8f6c\u76842D IoU\u7b97\u6cd5 \u8ba1\u7b97\u4e24\u4e2a\u5e73\u9762\u533a\u57df\u7684\u9762\u79ef \u627e\u51fa\u4e24\u4e2a\u533a\u57df\u76f8\u4ea4\u5f97\u5230\u7684\u51f8\u591a\u8fb9\u5f62\u7684\u70b9\uff0c\u8fd9\u4e9b\u70b9\u6709\u4e24\u79cd\u6765\u6e90\u53ef\u80fd\uff0c\u4e00\u4e2a\u662f\u4e24\u4e2abox\u8fb9\u7f18\u7684\u4ea4\u70b9\uff0c\u4e00\u4e2a\u662f\u51fa\u73b0\u5728\u53e6\u4e00\u4e2abbox\u533a\u57df\u4e2d\u7684\u539f\u6765box\u7684\u70b9 \u5c06\u8fd9\u4e9b\u51f8\u591a\u8fb9\u5f62\u7684\u8fb9\u7f18\u70b9\u9006\u65f6\u9488\u6216\u8005\u987a\u65f6\u9488\u6392\u5e8f\uff0c\u7b97\u6cd5\u662f\uff1a\u5148\u6c42\u51fa\u51f8\u591a\u8fb9\u5f62\u7684\u4e2d\u5fc3\u70b9\uff0c\u7136\u540e\u9010\u4e2a\u6c42\u51fa\u65cb\u8f6c\u89d2\u5ea6\uff0c\u7136\u540e\u5c06\u8fd9\u4e2a\u65cb\u8f6c\u89d2\u5ea6\u6392\u5e8f \u5c06\u51f8\u591a\u8fb9\u5f62\u5206\u89e3\u4e3a\u591a\u4e2a\u5c0f\u4e09\u89d2\u5f62\uff0c\u5e76\u6c42\u51fa\u603b\u5408\u9762\u79ef \u5f97\u5230overlap\u9762\u79ef\u540e\u6c42\u51faIoU 3D IoU \u53ea\u9700\u8981\u5728\u6c42\u51faover lap\u7684\u57fa\u7840\u4e0a\uff0c\u5728\u9ad8\u5ea6\u4e0a\u8865\u5145\u4e00\u4e9b\u53c2\u6570\u5373\u53ef\uff1a IoU_{3D} = \\frac{Area_{overlap} \\times h_{overlap}} {Area_g \\times h_g + Area_d \\times h_d - Area_{overlap} \\times h_{overlap}}","title":"IoU Loss for 2D/3D Object Detection"},{"location":"3dDetection/IoU%20Loss%20for%202D/#iou-loss-for-2d3d-object-detection","text":"\u8fd9\u7bc7\u8bba\u6587\u7684\u6838\u5fc3\u8d21\u732e\u662f\u7ed9\u51fa\u4e86\u6240\u8c13\u7684IoU Loss,\u56e0\u4e3a\u62fc\u70b9\u7684\u8fc7\u7a0b\u4e2d\u5173\u952e\u70b9\u5728\u4e8e\u63d0\u5347IoU\u7684\u503c\uff0c\u4f46\u662f\u6211\u4eec\u76ee\u524d\u6ca1\u6709\u76f4\u63a5\u68af\u5ea6\u4f18\u5316IoU\u7684\u65b9\u6cd5\uff0c\u8fd9\u7bc7\u6587\u7ae0\u7ed9\u51fa\u7684IoU Loss\u5c31\u662f\u7ed9\u51fa\u4e00\u4e2a\u53ef\u5fae\u5206\u7684\u635f\u5931\u51fd\u6570\uff0c\u901a\u8fc7\u76f4\u63a5\u4f18\u5316IoU\u63d0\u9ad8\u5206\u6570","title":"IoU Loss for 2D/3D Object Detection"},{"location":"3dDetection/IoU%20Loss%20for%202D/#2d-iou","text":"\u8ba1\u7b97\u4e24\u4e2a\u5e73\u9762\u533a\u57df\u7684\u9762\u79ef \u627e\u51fa\u4e24\u4e2a\u533a\u57df\u76f8\u4ea4\u5f97\u5230\u7684\u51f8\u591a\u8fb9\u5f62\u7684\u70b9\uff0c\u8fd9\u4e9b\u70b9\u6709\u4e24\u79cd\u6765\u6e90\u53ef\u80fd\uff0c\u4e00\u4e2a\u662f\u4e24\u4e2abox\u8fb9\u7f18\u7684\u4ea4\u70b9\uff0c\u4e00\u4e2a\u662f\u51fa\u73b0\u5728\u53e6\u4e00\u4e2abbox\u533a\u57df\u4e2d\u7684\u539f\u6765box\u7684\u70b9 \u5c06\u8fd9\u4e9b\u51f8\u591a\u8fb9\u5f62\u7684\u8fb9\u7f18\u70b9\u9006\u65f6\u9488\u6216\u8005\u987a\u65f6\u9488\u6392\u5e8f\uff0c\u7b97\u6cd5\u662f\uff1a\u5148\u6c42\u51fa\u51f8\u591a\u8fb9\u5f62\u7684\u4e2d\u5fc3\u70b9\uff0c\u7136\u540e\u9010\u4e2a\u6c42\u51fa\u65cb\u8f6c\u89d2\u5ea6\uff0c\u7136\u540e\u5c06\u8fd9\u4e2a\u65cb\u8f6c\u89d2\u5ea6\u6392\u5e8f \u5c06\u51f8\u591a\u8fb9\u5f62\u5206\u89e3\u4e3a\u591a\u4e2a\u5c0f\u4e09\u89d2\u5f62\uff0c\u5e76\u6c42\u51fa\u603b\u5408\u9762\u79ef \u5f97\u5230overlap\u9762\u79ef\u540e\u6c42\u51faIoU","title":"\u4e00\u822c\u5e26\u6709\u65cb\u8f6c\u76842D IoU\u7b97\u6cd5"},{"location":"3dDetection/IoU%20Loss%20for%202D/#3d-iou","text":"\u53ea\u9700\u8981\u5728\u6c42\u51faover lap\u7684\u57fa\u7840\u4e0a\uff0c\u5728\u9ad8\u5ea6\u4e0a\u8865\u5145\u4e00\u4e9b\u53c2\u6570\u5373\u53ef\uff1a IoU_{3D} = \\frac{Area_{overlap} \\times h_{overlap}} {Area_g \\times h_g + Area_d \\times h_d - Area_{overlap} \\times h_{overlap}}","title":"3D IoU"},{"location":"3dDetection/Kinematic_video3d/","text":"Kinematic 3D Object Detection in Monocular Video \u8fd9\u7bc7paper\u662f\u57fa\u4e8e M3D-RPN \u7684\u5355\u76ee\u65f6\u5e8f\u878d\u5408\u3002 \u6574\u4f53\u63a8\u7406\u7ed3\u6784 Modified M3D-RPN \u4fee\u6539\u7684\u7248\u672c\u4e3b\u8981\u662f\u4e24\u4e2a\uff0c\u4e00\u4e2a\u662f\u89d2\u5ea6\u7684\u9884\u6d4b\uff0c\u4e00\u4e2a\u662f\u81ea\u5e73\u8861\u635f\u5931\u51fd\u6570\u3002 \u89d2\u5ea6\u7684\u9884\u6d4b\u88ab\u5206\u4e3a\u4e24\u4e2a\u5206\u7c7b\u4ee5\u53ca\u4e00\u4e2a\u56de\u5f52 \u81ea\u5e73\u8861\u635f\u5931\u6709\u4e24\u4e2a\u4f5c\u7528\uff0c\u4e00\u65b9\u9762\u662f\u8ba9\u7f51\u7edc\u51b3\u5b9a\u4e0d\u786e\u5b9a\u6027\uff0c\u7528\u4e8e\u5361\u5c14\u66fc\u6ee4\u6ce2\uff0c\u53e6\u4e00\u65b9\u9762\u51cf\u5c11\u5bf9\u8fc7\u4e8e\u56f0\u96be\u7684instance\u7684\u8fc7\u62df\u5408\u3002 L=L_{2 \\mathrm{D}}+\\omega \\cdot L_{3 \\mathrm{D}}+\\lambda_{L} \\cdot(1-\\omega) \u5176\u4e2d \\lambda{L} \u4e3a\u8fc7\u53bb n_L \u4e2a\u6700\u8fd1\u7684 L_{3D} \u7684\u5747\u503c\u3002 \u5728\u63a8\u7406\u7684\u65f6\u5019\uff0c\u6700\u7ec8\u7684\u4e0d\u786e\u5b9a\u6027\u5c31\u662f \\mu = c \\cdot \\omega Ego-Motion \u4f5c\u8005\u4f7f\u7528raw data\u91cc\u9762\u7684devkit\u5728training set\u4e0a\u751f\u6210\u4e86odometry ground truth, \u7136\u540e\u7528\u7f51\u7edcDensely predict 6DOF\u7684ego motion.\u6700\u540e\u76f8\u5f53\u4e8e\u6c42\u5168\u5c40\u7684\u5747\u503c\u4f5c\u4e3a\u9884\u6d4b\u3002 \u56e0\u800c\u4e5f\u5c31\u53ef\u4ee5\u7528\u5728test\u65f6\u5c06\u5176\u4ed6\u8f66\u8f86\u7684\u8fd0\u52a8\u9650\u5236\u5728\u81ea\u5df1\u7684orientation\u4e0a\u3002 3D Kalman Filter \u72b6\u6001\u7a7a\u95f4\u7684\u8bbe\u7f6e\u4e3a \\tau = [\\tau_x, \\tau_y, \\tau_z, \\tau_w, \\tau_h, \\tau_l, \\tau_\\theta, \\tau_{\\theta_h}, \\tau_v] 3D\u4e2d\u5fc3\u70b9\uff0c3D\u5927\u5c0f\uff0c\u65b9\u5411\u4ee5\u53ca\u901f\u5ea6 \u7ebf\u6027\u7cfb\u7edf\u77e9\u9635 \\mathbf{F}=\\left[\\mathbf{I}^{9 \\times 8} \\begin{array}{cc} \\cos \\left(\\tau_{\\theta}+\\pi\\left\\lfloor\\tau_{\\theta_{h}}\\right\\rceil\\right) \\\\ 0 \\\\ -\\sin \\left(\\tau_{\\theta}+\\pi\\left\\lfloor\\tau_{\\theta_{h}}\\right\\rceil\\right) \\\\ 0 \\\\ \\vdots \\\\ 1 \\end{array}\\right] \u9884\u6d4b\u65f6\u6ce8\u610f\u9700\u8981\u5904\u7406ego-motion,\u72b6\u6001\u65b9\u7a0b\u662f\u5728\u4e16\u754c\u5750\u6807\u7cfb\u4e0a\u7684\uff0c\u800c\u7f51\u7edc\u7684\u9884\u6d4b\u662f\u5728\u76f8\u673a\u7684\u5750\u6807\u7cfb\u4e0b\u7684\u3002 \\mathbf{P}_{t}^{\\prime}=\\mathbf{F}_{t-1} \\cdot \\mathbf{P}_{t-1} \\cdot \\mathbf{F}_{t-1}^{T}+\\mathbf{I}^{9 \\times 9} \\cdot\\left(1-\\mu_{t-1}\\right) Association \u5c06\u5361\u5c14\u66fc\u6ee4\u6ce2\u7684\u8f93\u51fa\u4e0e\u65b0\u4e00\u5e27\u7684\u9884\u6d4b\u8fdb\u884c\u7ec4\u5408\uff0c\u8fd9\u8fb9\u9009\u62e9\u7684\u65b9\u6848\u662f\u8d2a\u5fc3\u7684\u6700\u4f4e\u8ddd\u79bb+2D IoU\u5339\u914d\u3002 \u89c2\u6d4b\u66f4\u65b0 \\mathbf{K}=\\mathbf{P}^{\\prime} \\mathbf{H}^{T}\\left(\\mathbf{H} \\mathbf{P}^{\\prime} \\mathbf{H}^{T}+\\mathbf{I}^{8 \\times 8}(1-\\mu) \\cdot \\lambda_{o}\\right)^{-1} \\tau_{t}=\\tau_{t}^{\\prime}+\\mathbf{K}\\left(b-\\mathbf{H} \\tau_{t}^{\\prime}\\right), \\quad \\mathbf{P}_{t}=\\left(\\mathbf{I}^{9 \\times 9}-\\mathbf{K} \\mathbf{H}\\right) \\mathbf{P}_{t}^{\\prime}","title":"Kinematic 3D Object Detection in Monocular Video"},{"location":"3dDetection/Kinematic_video3d/#kinematic-3d-object-detection-in-monocular-video","text":"\u8fd9\u7bc7paper\u662f\u57fa\u4e8e M3D-RPN \u7684\u5355\u76ee\u65f6\u5e8f\u878d\u5408\u3002","title":"Kinematic 3D Object Detection in Monocular Video"},{"location":"3dDetection/Kinematic_video3d/#_1","text":"","title":"\u6574\u4f53\u63a8\u7406\u7ed3\u6784"},{"location":"3dDetection/Kinematic_video3d/#modified-m3d-rpn","text":"\u4fee\u6539\u7684\u7248\u672c\u4e3b\u8981\u662f\u4e24\u4e2a\uff0c\u4e00\u4e2a\u662f\u89d2\u5ea6\u7684\u9884\u6d4b\uff0c\u4e00\u4e2a\u662f\u81ea\u5e73\u8861\u635f\u5931\u51fd\u6570\u3002 \u89d2\u5ea6\u7684\u9884\u6d4b\u88ab\u5206\u4e3a\u4e24\u4e2a\u5206\u7c7b\u4ee5\u53ca\u4e00\u4e2a\u56de\u5f52 \u81ea\u5e73\u8861\u635f\u5931\u6709\u4e24\u4e2a\u4f5c\u7528\uff0c\u4e00\u65b9\u9762\u662f\u8ba9\u7f51\u7edc\u51b3\u5b9a\u4e0d\u786e\u5b9a\u6027\uff0c\u7528\u4e8e\u5361\u5c14\u66fc\u6ee4\u6ce2\uff0c\u53e6\u4e00\u65b9\u9762\u51cf\u5c11\u5bf9\u8fc7\u4e8e\u56f0\u96be\u7684instance\u7684\u8fc7\u62df\u5408\u3002 L=L_{2 \\mathrm{D}}+\\omega \\cdot L_{3 \\mathrm{D}}+\\lambda_{L} \\cdot(1-\\omega) \u5176\u4e2d \\lambda{L} \u4e3a\u8fc7\u53bb n_L \u4e2a\u6700\u8fd1\u7684 L_{3D} \u7684\u5747\u503c\u3002 \u5728\u63a8\u7406\u7684\u65f6\u5019\uff0c\u6700\u7ec8\u7684\u4e0d\u786e\u5b9a\u6027\u5c31\u662f \\mu = c \\cdot \\omega","title":"Modified M3D-RPN"},{"location":"3dDetection/Kinematic_video3d/#ego-motion","text":"\u4f5c\u8005\u4f7f\u7528raw data\u91cc\u9762\u7684devkit\u5728training set\u4e0a\u751f\u6210\u4e86odometry ground truth, \u7136\u540e\u7528\u7f51\u7edcDensely predict 6DOF\u7684ego motion.\u6700\u540e\u76f8\u5f53\u4e8e\u6c42\u5168\u5c40\u7684\u5747\u503c\u4f5c\u4e3a\u9884\u6d4b\u3002 \u56e0\u800c\u4e5f\u5c31\u53ef\u4ee5\u7528\u5728test\u65f6\u5c06\u5176\u4ed6\u8f66\u8f86\u7684\u8fd0\u52a8\u9650\u5236\u5728\u81ea\u5df1\u7684orientation\u4e0a\u3002","title":"Ego-Motion"},{"location":"3dDetection/Kinematic_video3d/#3d-kalman-filter","text":"\u72b6\u6001\u7a7a\u95f4\u7684\u8bbe\u7f6e\u4e3a \\tau = [\\tau_x, \\tau_y, \\tau_z, \\tau_w, \\tau_h, \\tau_l, \\tau_\\theta, \\tau_{\\theta_h}, \\tau_v] 3D\u4e2d\u5fc3\u70b9\uff0c3D\u5927\u5c0f\uff0c\u65b9\u5411\u4ee5\u53ca\u901f\u5ea6 \u7ebf\u6027\u7cfb\u7edf\u77e9\u9635 \\mathbf{F}=\\left[\\mathbf{I}^{9 \\times 8} \\begin{array}{cc} \\cos \\left(\\tau_{\\theta}+\\pi\\left\\lfloor\\tau_{\\theta_{h}}\\right\\rceil\\right) \\\\ 0 \\\\ -\\sin \\left(\\tau_{\\theta}+\\pi\\left\\lfloor\\tau_{\\theta_{h}}\\right\\rceil\\right) \\\\ 0 \\\\ \\vdots \\\\ 1 \\end{array}\\right] \u9884\u6d4b\u65f6\u6ce8\u610f\u9700\u8981\u5904\u7406ego-motion,\u72b6\u6001\u65b9\u7a0b\u662f\u5728\u4e16\u754c\u5750\u6807\u7cfb\u4e0a\u7684\uff0c\u800c\u7f51\u7edc\u7684\u9884\u6d4b\u662f\u5728\u76f8\u673a\u7684\u5750\u6807\u7cfb\u4e0b\u7684\u3002 \\mathbf{P}_{t}^{\\prime}=\\mathbf{F}_{t-1} \\cdot \\mathbf{P}_{t-1} \\cdot \\mathbf{F}_{t-1}^{T}+\\mathbf{I}^{9 \\times 9} \\cdot\\left(1-\\mu_{t-1}\\right)","title":"3D Kalman Filter"},{"location":"3dDetection/Kinematic_video3d/#association","text":"\u5c06\u5361\u5c14\u66fc\u6ee4\u6ce2\u7684\u8f93\u51fa\u4e0e\u65b0\u4e00\u5e27\u7684\u9884\u6d4b\u8fdb\u884c\u7ec4\u5408\uff0c\u8fd9\u8fb9\u9009\u62e9\u7684\u65b9\u6848\u662f\u8d2a\u5fc3\u7684\u6700\u4f4e\u8ddd\u79bb+2D IoU\u5339\u914d\u3002 \u89c2\u6d4b\u66f4\u65b0 \\mathbf{K}=\\mathbf{P}^{\\prime} \\mathbf{H}^{T}\\left(\\mathbf{H} \\mathbf{P}^{\\prime} \\mathbf{H}^{T}+\\mathbf{I}^{8 \\times 8}(1-\\mu) \\cdot \\lambda_{o}\\right)^{-1} \\tau_{t}=\\tau_{t}^{\\prime}+\\mathbf{K}\\left(b-\\mathbf{H} \\tau_{t}^{\\prime}\\right), \\quad \\mathbf{P}_{t}=\\left(\\mathbf{I}^{9 \\times 9}-\\mathbf{K} \\mathbf{H}\\right) \\mathbf{P}_{t}^{\\prime}","title":"Association"},{"location":"3dDetection/LaserNet/","text":"LaserNet: An Efficient Probabilistic 3D Object Detector for Autonomous Driving \u8fd9\u7bc7\u8bba\u6587\u5b9e\u73b03D object detection\u5229\u7528\u7684\u6570\u636e\u7c7b\u578b\u4e3b\u8981\u662f\u70b9\u4e91\u7684Range view\u56fe\u50cf\u3002\u76ee\u7684\u662f\u907f\u514dBEV\u8fc7\u4e8e\u7a00\u758f\u7684\u95ee\u9898\u3002 \u603b\u4f53pipeline \u6570\u636e\u7ec4\u7ec7\u65b9\u5f0f Lidar\u4f1a\u968f\u7740\u65cb\u8f6c\u751f\u6210\u4e00\u4e2a\u5706\u67f1\u72b6\u7684range image,\u6570\u636e\u96c6\u4e2d\u4f7f\u7528\u7684\u6fc0\u5149\u96f7\u8fbevelodyne 64\u7ebf\uff0c\u5782\u76f4\u65b9\u5411\u89d2\u5ea6\u5206\u8fa8\u7387\u7ea6\u4e3a0.4\u00b0\uff0c\u6c34\u5e73\u65b9\u5411\u7684\u89d2\u5ea6\u5206\u8fa8\u7387\u7ea6\u4e3a0.2\u00b0,\u76f4\u63a5\u5c06\u5706\u67f1\u5750\u6807\u7684lidar\u8f6c\u6362\u4e3a\"\u56fe\u50cf\",\u8fd9\u4e2a\u7a0d\u5fae\u7c7b\u4f3c\u4e8e MV3D \u8ddd\u79bb\u3001\u9ad8\u5ea6\u3001\u53cd\u5c04\u5f3a\u5ea6\u3001\u662f\u5426\u5b58\u5728\u70b9\u7684flag\u5206\u522b\u6210\u4e3a\u4e94\u4e2arange view image\u7684channel \u7f51\u7edc\u7ed3\u6784 \u672c\u6587\u91c7\u7528\u4e00\u4e2a\u5168\u5377\u79ef\u7684\u7ed3\u6784\uff0c\u8f93\u51fa\u7684\u662f\u6bcf\u4e00\u4e2aRange view image\u5bf9\u5e94\u4e00\u4e2apixel\u5bf9\u5e94\u4e00\u4e2aprediction\u3002 \u9884\u6d4b\u8f93\u51fa \u7531\u4e8e\u4e0d\u540c\u7684\u70b9\u6709\u4e0d\u540c\u7684\u89c2\u6d4b\uff0c\u800c\u70b9\u4e91\u89c2\u6d4b\u6709\u4e0d\u786e\u5b9a\u6027\uff0c\u8fd9\u91cc\u5c31\u5c06\u4e0d\u786e\u5b9a\u6027\u5177\u4f53\u7684\u8003\u8651\uff0c(\u591a\u9884\u6d4bvariance)\u3002 \u540c\u65f6\u5047\u8bbe\u6240\u6709\u7269\u4f53\u90fd\u5728\u540c\u4e00\u4e2aground plane\u4e0a\uff0c\u56e0\u6b64\u4e00\u4e2abounding box\u7531\u4e00\u4e2a\u56db\u4e2a\u89d2\u843d\u786e\u5b9a(BEV\u4e2d)\uff0c\u5e76\u4e14\u9ad8\u5ea6\u56fa\u5b9a\u3002 \u8fd9\u91cc\u5177\u4f53\u7684\u8f93\u51fa\u662frelative center (d_x, d_y) \uff0c\u76f8\u5bf9\u8f6c\u89d2 (\\omega_x, \\omega_y) = (cos\\omega, sin\\omega) \u4ee5\u53cadimensions= (l,w) \u3002\u8fd9\u4e2a \\omega \u662f\u89c2\u6d4b\u89d2(\u89c6\u89c9\u63a2\u6d4b\u91cc\u9762\u5e38\u7528\u7684)\u3002\u6700\u7ec8\u8f93\u51fa\u77e2\u91cf ={d_{x,k}, d_{y,k}, \\omega_{x,k}, \\omega_{y,k},l_k, w_k}^K_{k=1} + {s_k}_{k=1}^K + {\\alpha_k}^K_{k=1} .\u5176\u4e2d \\alpha \u4e3amixture weights, K \u4e3amixture model\u7684\u6570\u91cf\u5b50\u6a21\u578b\u3002 \u5747\u503c\u504f\u79fb\u805a\u7c7b(Mean Shift Clustering) \u8fd9\u91cc\u7684\u8003\u8651\u662f\uff0c\u7531\u4e8e\u9884\u6d4b\u662f\u5bc6\u96c6\u7684\uff0c\u4f46\u662f\u76f8\u540c\u70b9\u9644\u8fd1\u7684\u9884\u6d4b\u5f88\u6709\u53ef\u80fd\u4f1a\u6709\u5f88\u5927\u7684\u76f8\u4f3c(\u5411\u540c\u4e00\u4e2a\u7269\u4f53\u9760\u62e2)\uff0c\u800c\u5355\u4e2a\u9884\u6d4b\u4f1a\u6709\u566a\u58f0\uff0c\u8fd9\u91cc\u8003\u8651\u7684\u662f\u8ba9\u9644\u8fd1\u591a\u4e2a\u70b9\u7684prediction\u5bf9\u540c\u4e00\u4e2aobject\u8fdb\u884c\u8d21\u732e\u3002 \u8fd9\u91cc\u5bf9\u4e2d\u5fc3\u70b9\u7684xy\u70b9\u8fd9\u4e24\u4e2a\u7ef4\u5ea6\u8fdb\u884c\u805a\u7c7b\u3002\u5173\u4e8emean shift clustering \u53ef\u4ee5\u53c2\u89c1 wiki . \u672c\u6587\u7b97\u6cd5\uff1a \u5c06\u4fef\u89c6\u56fe\u4f7f\u7528 \\Delta x \\Delta y \u79bb\u6563\u5316\uff0c\u521d\u59cb\u5316\u6876(bin) m_i \uff0c \u5bf9\u4e8e\u62e5\u6709\u591a\u4e8e\u4e00\u4e2abox center\u7684\u6876 m_i ,\u7528\u5747\u503c\u8ba1\u7b97\uff0c\u5f97\u5230\u6876\u5bf9\u4e2d\u5fc3\u5750\u6807\u7684\u521d\u59cb\u4f30\u8ba1\uff0c\u7528 S_i \u8bb0\u5f55\u6876\u5185\u7684\u70b9\u96c6 \u66f4\u65b0\u65b0\u7684\u4e2d\u5fc3 m_i = \\frac{\\sum_{j \\in i \\bigcup N(i)} K_{i,j}(m_j * |S_j|)}{\\sum_{j \\in i \\bigcup N(i)} K_{i,j}|S_j|} \u5176\u4e2d, K = exp(-\\frac{||m_i-m_j||^2}{\\Delta x^2 + \\Delta y^2}) \u4e3a\u6838\u51fd\u6570\uff0c N(i) \u4e3a\u7b2c i \u4e2a\u6876\u9644\u8fd1\u76848\u4e2a\u6876.\u5176\u4e2d\u5bf9\u96c6\u5408\u7684 || \u64cd\u4f5c\u4e3a\u53d6\u96c6\u5408\u4e2d\u70b9\u7684\u6570\u91cf\u3002 \u5982\u679c\u7b2c i \u4e2a\u6876\u7684\u4e2d\u5fc3\u5728 j \u4e2a\u6876\u7684\u5185\u90e8\uff0c\u90a3\u4e48\u5c06 i,j \u52a0\u6743\u878d\u5408\uff0c\u7b2c i \u4e2a\u6876\u7684\u4e2d\u5fc3\u548c S_i \u5f520 \u4f5c\u8005\u8868\u793a\u8fd9\u4e9b\u5185\u5bb9\u90fd\u662f\u53ef\u4ee5\u7528GPU\u52a0\u901f\u7684\u3002\u5bf9\u7b2c3,4\u6b65\u8fed\u4ee3\u53ef\u5f97\u5230\u878d\u5408\u70b9\u3002 \u5bf9\u4e8e\u6bcf\u4e00\u4e2a\u96c6\u5408\uff0c\u53ef\u4ee5\u7528\u5982\u4e0b\u516c\u5f0f\u5c06\u6876\u5185\u7684\u9884\u6d4b\u503c\u8fdb\u884c\u6c42\u548c\uff0c\u65b9\u5dee\u91cd\u7b97 \\hat b_i = \\frac{\\sum_{j \\in S_i} w_j b_j}{\\sum_{j \\in S_i} w_j} \\hat \\sigma^2 = (\\sum_{j\\in S_i}\\frac{1}{\\sigma_j^2})^{-1} \u672c\u6587\u5efa\u8bae \\Delta x = \\Delta y = 0.5 \u8bad\u7ec3\u635f\u5931 \u6700\u7ec8\u53ef\u4ee5\u5e26\u7740\u4e0d\u786e\u5b9a\u6027\u8f93\u51fa\u3002 \u9996\u5148 \\mathcal{L}_{cls} \u4e3afocal loss\uff0c\u8fd9\u91cc\u53ef\u4ee5\u53c2\u8003 \u8fd9\u7bc7 \\mathcal{L}_{box} = \\sum_n \\frac{1}{\\hat\\sigma_{k*}} |\\hat b_{n, k*} - b^{gt}_n| + log\\hat\\sigma_k \u81ea\u9002\u5e94NMS \u7ecf\u8fc7\u672c\u56fe\u7684\u5206\u6790 \u5f97\u5230\u53ef\u5bb9\u5fcd\u7684\u6700\u5927IoU\u4e3a t = max(\\frac{\\sigma_1 + \\sigma_2}{0.5 * w_1 + 0.5 * w_2 - \\sigma_1 - \\sigma2} ,0) \u5176\u4e2d w_1, w_2 \u4e3a\u5bbd\u5ea6\u3002","title":"LaserNet: An Efficient Probabilistic 3D Object Detector for Autonomous Driving"},{"location":"3dDetection/LaserNet/#lasernet-an-efficient-probabilistic-3d-object-detector-for-autonomous-driving","text":"\u8fd9\u7bc7\u8bba\u6587\u5b9e\u73b03D object detection\u5229\u7528\u7684\u6570\u636e\u7c7b\u578b\u4e3b\u8981\u662f\u70b9\u4e91\u7684Range view\u56fe\u50cf\u3002\u76ee\u7684\u662f\u907f\u514dBEV\u8fc7\u4e8e\u7a00\u758f\u7684\u95ee\u9898\u3002","title":"LaserNet: An Efficient Probabilistic 3D Object Detector for Autonomous Driving"},{"location":"3dDetection/LaserNet/#pipeline","text":"","title":"\u603b\u4f53pipeline"},{"location":"3dDetection/LaserNet/#_1","text":"Lidar\u4f1a\u968f\u7740\u65cb\u8f6c\u751f\u6210\u4e00\u4e2a\u5706\u67f1\u72b6\u7684range image,\u6570\u636e\u96c6\u4e2d\u4f7f\u7528\u7684\u6fc0\u5149\u96f7\u8fbevelodyne 64\u7ebf\uff0c\u5782\u76f4\u65b9\u5411\u89d2\u5ea6\u5206\u8fa8\u7387\u7ea6\u4e3a0.4\u00b0\uff0c\u6c34\u5e73\u65b9\u5411\u7684\u89d2\u5ea6\u5206\u8fa8\u7387\u7ea6\u4e3a0.2\u00b0,\u76f4\u63a5\u5c06\u5706\u67f1\u5750\u6807\u7684lidar\u8f6c\u6362\u4e3a\"\u56fe\u50cf\",\u8fd9\u4e2a\u7a0d\u5fae\u7c7b\u4f3c\u4e8e MV3D \u8ddd\u79bb\u3001\u9ad8\u5ea6\u3001\u53cd\u5c04\u5f3a\u5ea6\u3001\u662f\u5426\u5b58\u5728\u70b9\u7684flag\u5206\u522b\u6210\u4e3a\u4e94\u4e2arange view image\u7684channel","title":"\u6570\u636e\u7ec4\u7ec7\u65b9\u5f0f"},{"location":"3dDetection/LaserNet/#_2","text":"\u672c\u6587\u91c7\u7528\u4e00\u4e2a\u5168\u5377\u79ef\u7684\u7ed3\u6784\uff0c\u8f93\u51fa\u7684\u662f\u6bcf\u4e00\u4e2aRange view image\u5bf9\u5e94\u4e00\u4e2apixel\u5bf9\u5e94\u4e00\u4e2aprediction\u3002","title":"\u7f51\u7edc\u7ed3\u6784"},{"location":"3dDetection/LaserNet/#_3","text":"\u7531\u4e8e\u4e0d\u540c\u7684\u70b9\u6709\u4e0d\u540c\u7684\u89c2\u6d4b\uff0c\u800c\u70b9\u4e91\u89c2\u6d4b\u6709\u4e0d\u786e\u5b9a\u6027\uff0c\u8fd9\u91cc\u5c31\u5c06\u4e0d\u786e\u5b9a\u6027\u5177\u4f53\u7684\u8003\u8651\uff0c(\u591a\u9884\u6d4bvariance)\u3002 \u540c\u65f6\u5047\u8bbe\u6240\u6709\u7269\u4f53\u90fd\u5728\u540c\u4e00\u4e2aground plane\u4e0a\uff0c\u56e0\u6b64\u4e00\u4e2abounding box\u7531\u4e00\u4e2a\u56db\u4e2a\u89d2\u843d\u786e\u5b9a(BEV\u4e2d)\uff0c\u5e76\u4e14\u9ad8\u5ea6\u56fa\u5b9a\u3002 \u8fd9\u91cc\u5177\u4f53\u7684\u8f93\u51fa\u662frelative center (d_x, d_y) \uff0c\u76f8\u5bf9\u8f6c\u89d2 (\\omega_x, \\omega_y) = (cos\\omega, sin\\omega) \u4ee5\u53cadimensions= (l,w) \u3002\u8fd9\u4e2a \\omega \u662f\u89c2\u6d4b\u89d2(\u89c6\u89c9\u63a2\u6d4b\u91cc\u9762\u5e38\u7528\u7684)\u3002\u6700\u7ec8\u8f93\u51fa\u77e2\u91cf ={d_{x,k}, d_{y,k}, \\omega_{x,k}, \\omega_{y,k},l_k, w_k}^K_{k=1} + {s_k}_{k=1}^K + {\\alpha_k}^K_{k=1} .\u5176\u4e2d \\alpha \u4e3amixture weights, K \u4e3amixture model\u7684\u6570\u91cf\u5b50\u6a21\u578b\u3002","title":"\u9884\u6d4b\u8f93\u51fa"},{"location":"3dDetection/LaserNet/#mean-shift-clustering","text":"\u8fd9\u91cc\u7684\u8003\u8651\u662f\uff0c\u7531\u4e8e\u9884\u6d4b\u662f\u5bc6\u96c6\u7684\uff0c\u4f46\u662f\u76f8\u540c\u70b9\u9644\u8fd1\u7684\u9884\u6d4b\u5f88\u6709\u53ef\u80fd\u4f1a\u6709\u5f88\u5927\u7684\u76f8\u4f3c(\u5411\u540c\u4e00\u4e2a\u7269\u4f53\u9760\u62e2)\uff0c\u800c\u5355\u4e2a\u9884\u6d4b\u4f1a\u6709\u566a\u58f0\uff0c\u8fd9\u91cc\u8003\u8651\u7684\u662f\u8ba9\u9644\u8fd1\u591a\u4e2a\u70b9\u7684prediction\u5bf9\u540c\u4e00\u4e2aobject\u8fdb\u884c\u8d21\u732e\u3002 \u8fd9\u91cc\u5bf9\u4e2d\u5fc3\u70b9\u7684xy\u70b9\u8fd9\u4e24\u4e2a\u7ef4\u5ea6\u8fdb\u884c\u805a\u7c7b\u3002\u5173\u4e8emean shift clustering \u53ef\u4ee5\u53c2\u89c1 wiki . \u672c\u6587\u7b97\u6cd5\uff1a \u5c06\u4fef\u89c6\u56fe\u4f7f\u7528 \\Delta x \\Delta y \u79bb\u6563\u5316\uff0c\u521d\u59cb\u5316\u6876(bin) m_i \uff0c \u5bf9\u4e8e\u62e5\u6709\u591a\u4e8e\u4e00\u4e2abox center\u7684\u6876 m_i ,\u7528\u5747\u503c\u8ba1\u7b97\uff0c\u5f97\u5230\u6876\u5bf9\u4e2d\u5fc3\u5750\u6807\u7684\u521d\u59cb\u4f30\u8ba1\uff0c\u7528 S_i \u8bb0\u5f55\u6876\u5185\u7684\u70b9\u96c6 \u66f4\u65b0\u65b0\u7684\u4e2d\u5fc3 m_i = \\frac{\\sum_{j \\in i \\bigcup N(i)} K_{i,j}(m_j * |S_j|)}{\\sum_{j \\in i \\bigcup N(i)} K_{i,j}|S_j|} \u5176\u4e2d, K = exp(-\\frac{||m_i-m_j||^2}{\\Delta x^2 + \\Delta y^2}) \u4e3a\u6838\u51fd\u6570\uff0c N(i) \u4e3a\u7b2c i \u4e2a\u6876\u9644\u8fd1\u76848\u4e2a\u6876.\u5176\u4e2d\u5bf9\u96c6\u5408\u7684 || \u64cd\u4f5c\u4e3a\u53d6\u96c6\u5408\u4e2d\u70b9\u7684\u6570\u91cf\u3002 \u5982\u679c\u7b2c i \u4e2a\u6876\u7684\u4e2d\u5fc3\u5728 j \u4e2a\u6876\u7684\u5185\u90e8\uff0c\u90a3\u4e48\u5c06 i,j \u52a0\u6743\u878d\u5408\uff0c\u7b2c i \u4e2a\u6876\u7684\u4e2d\u5fc3\u548c S_i \u5f520 \u4f5c\u8005\u8868\u793a\u8fd9\u4e9b\u5185\u5bb9\u90fd\u662f\u53ef\u4ee5\u7528GPU\u52a0\u901f\u7684\u3002\u5bf9\u7b2c3,4\u6b65\u8fed\u4ee3\u53ef\u5f97\u5230\u878d\u5408\u70b9\u3002 \u5bf9\u4e8e\u6bcf\u4e00\u4e2a\u96c6\u5408\uff0c\u53ef\u4ee5\u7528\u5982\u4e0b\u516c\u5f0f\u5c06\u6876\u5185\u7684\u9884\u6d4b\u503c\u8fdb\u884c\u6c42\u548c\uff0c\u65b9\u5dee\u91cd\u7b97 \\hat b_i = \\frac{\\sum_{j \\in S_i} w_j b_j}{\\sum_{j \\in S_i} w_j} \\hat \\sigma^2 = (\\sum_{j\\in S_i}\\frac{1}{\\sigma_j^2})^{-1} \u672c\u6587\u5efa\u8bae \\Delta x = \\Delta y = 0.5","title":"\u5747\u503c\u504f\u79fb\u805a\u7c7b(Mean Shift Clustering)"},{"location":"3dDetection/LaserNet/#_4","text":"\u6700\u7ec8\u53ef\u4ee5\u5e26\u7740\u4e0d\u786e\u5b9a\u6027\u8f93\u51fa\u3002 \u9996\u5148 \\mathcal{L}_{cls} \u4e3afocal loss\uff0c\u8fd9\u91cc\u53ef\u4ee5\u53c2\u8003 \u8fd9\u7bc7 \\mathcal{L}_{box} = \\sum_n \\frac{1}{\\hat\\sigma_{k*}} |\\hat b_{n, k*} - b^{gt}_n| + log\\hat\\sigma_k","title":"\u8bad\u7ec3\u635f\u5931"},{"location":"3dDetection/LaserNet/#nms","text":"\u7ecf\u8fc7\u672c\u56fe\u7684\u5206\u6790 \u5f97\u5230\u53ef\u5bb9\u5fcd\u7684\u6700\u5927IoU\u4e3a t = max(\\frac{\\sigma_1 + \\sigma_2}{0.5 * w_1 + 0.5 * w_2 - \\sigma_1 - \\sigma2} ,0) \u5176\u4e2d w_1, w_2 \u4e3a\u5bbd\u5ea6\u3002","title":"\u81ea\u9002\u5e94NMS"},{"location":"3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/","text":"M3D-RPN: Monocular 3D Region Proposal Network for Object Detection \u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u4f7f\u7528\u5355\u76ee\u76f8\u673a\u8fdb\u884c\u76ee\u6807\u68c0\u6d4b\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528\u4e86\u878d\u54082D-3D proposal\u7684\u65b9\u5f0f\uff0cuse depth-aware convolution,\u540e\u7aef\u4f18\u5316\u7ed3\u5408\u5404\u79cd\u4fe1\u606f\uff0c\u63d0\u5347\u5bf9 \\theta \u7684\u9884\u6d4b\u6027\u80fd \u603b\u4f53\u601d\u8def \u8fd9\u4e2a\u7f51\u7edc\u88ab\u8981\u6c42\u540c\u65f6\u8f93\u51fa2D\u4e0e3D\u9884\u6d4b\uff0ccost function\u540c\u65f6\u67093D\u6846\u4ee5\u53ca2D\u6846 depth-aware convolution\u5982\u56fe\uff0c\u5c31\u662f\u5728\u4e0d\u540c\u7684\u9ad8\u5ea6\u4e0a\u4f7f\u7528\u4e0d\u540c\u7684\u5377\u79ef\u6838\uff0c\u4e3a\u4e86\u4f7f\u5f97\u8fd0\u7b97\u5feb\uff0cpytorch implementation\u4e0a\u4f5c\u8005\u5c06\u5176reshape\uff0c\u7136\u540e\u4f7f\u7528depth-wise convolution(channel\u7ef4\u5ea6\u5206\u7ec4convolution) \u5176\u540e\u4f7f\u7528\u7684\u662f\u9010\u6b65\u4fee\u6539 \\theta \u4f7f\u5f97\u4e09\u7ef4\u5750\u6807\u4e0e\u4e8c\u7ef4\u6846\u5bf9\u9f50\uff0c\u7c7b\u4f3c\u4e8e\u6a21\u62df\u9000\u706b \u5177\u4f53\u7b97\u4e0a\u6765\u8bf4\u5c31\u662f\u6bcf\u4e00\u6b65\u8ba1\u7b973D\u5bf9\u5e942D\u6846\u4e0e\u9884\u6d4b2D\u6846\u7684L1-loss\uff0c\u4ece\u800c\u6765\u7ed9\u9884\u6d4b\u7684 \\theta \u6253\u5206","title":"M3D-RPN: Monocular 3D Region Proposal Network for Object Detection"},{"location":"3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/#m3d-rpn-monocular-3d-region-proposal-network-for-object-detection","text":"\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u4f7f\u7528\u5355\u76ee\u76f8\u673a\u8fdb\u884c\u76ee\u6807\u68c0\u6d4b\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528\u4e86\u878d\u54082D-3D proposal\u7684\u65b9\u5f0f\uff0cuse depth-aware convolution,\u540e\u7aef\u4f18\u5316\u7ed3\u5408\u5404\u79cd\u4fe1\u606f\uff0c\u63d0\u5347\u5bf9 \\theta \u7684\u9884\u6d4b\u6027\u80fd","title":"M3D-RPN: Monocular 3D Region Proposal Network for Object Detection"},{"location":"3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/#_1","text":"\u8fd9\u4e2a\u7f51\u7edc\u88ab\u8981\u6c42\u540c\u65f6\u8f93\u51fa2D\u4e0e3D\u9884\u6d4b\uff0ccost function\u540c\u65f6\u67093D\u6846\u4ee5\u53ca2D\u6846 depth-aware convolution\u5982\u56fe\uff0c\u5c31\u662f\u5728\u4e0d\u540c\u7684\u9ad8\u5ea6\u4e0a\u4f7f\u7528\u4e0d\u540c\u7684\u5377\u79ef\u6838\uff0c\u4e3a\u4e86\u4f7f\u5f97\u8fd0\u7b97\u5feb\uff0cpytorch implementation\u4e0a\u4f5c\u8005\u5c06\u5176reshape\uff0c\u7136\u540e\u4f7f\u7528depth-wise convolution(channel\u7ef4\u5ea6\u5206\u7ec4convolution) \u5176\u540e\u4f7f\u7528\u7684\u662f\u9010\u6b65\u4fee\u6539 \\theta \u4f7f\u5f97\u4e09\u7ef4\u5750\u6807\u4e0e\u4e8c\u7ef4\u6846\u5bf9\u9f50\uff0c\u7c7b\u4f3c\u4e8e\u6a21\u62df\u9000\u706b \u5177\u4f53\u7b97\u4e0a\u6765\u8bf4\u5c31\u662f\u6bcf\u4e00\u6b65\u8ba1\u7b973D\u5bf9\u5e942D\u6846\u4e0e\u9884\u6d4b2D\u6846\u7684L1-loss\uff0c\u4ece\u800c\u6765\u7ed9\u9884\u6d4b\u7684 \\theta \u6253\u5206","title":"\u603b\u4f53\u601d\u8def"},{"location":"3dDetection/Metric_3d/","text":"3D detection evaluation metric \u672c\u6587\u4e3b\u8981\u5c1d\u8bd5\u7efc\u8ff03D\u68c0\u6d4b\u7684\u8bc4\u4ef7\u65b9\u6cd5\u4ee5\u53ca\u5bf9\u5e94\u7684\u4e00\u4e9bcode\u7684\u5206\u6790\uff0c\u76ee\u6807\u662f\u6bd4\u8f83\u7ec6\u81f4\u7684\u5206\u6790\uff0c\u5728\u6709\u65b0metric\u63d0\u51fa\u7684\u8fc7\u7a0b\u4e2d\u4f1a\u6301\u7eed\u66f4\u65b0\uff0c\u5148\u7efc\u8ff0\u7684\u662fKITTI\u4e0eNuscene\u4e24\u5927benchmark\u7684\u8bc4\u4ef7\u5206\u6570\u3002 Update 2020.09.08: Add Cityscapes 3D Average Precision - Kitti Official cpp code fast numba code \u6574\u4f53\u601d\u8def\u4e0a\u6765\u8bf4\uff0cKITTI\u7684\u6d4b\u8bd5\u662f\u5c062D\uff0cBEV\u4ee5\u53ca3D\u68c0\u6d4b\u7684\u8bc4\u4ef7\u8fc7\u7a0b\u5b8c\u5168\u5206\u5f00\u3002\u56e0\u800c\u4e09\u79cdevaluation\u4e2d\u95f4\u9700\u8981\u7684matching\u662f\u4e0d\u76f8\u5173\u7684\uff0c\u4f46\u662f\u7531\u4e8e\u6709\u76f8\u4f3c\u7684\u641c\u7d22\u601d\u8def\uff0c\u56e0\u800c\u4ee3\u7801\u5b9e\u73b0\u4e0a\u6709\u4e00\u5b9a\u7684\u590d\u7528\u3002 \u5339\u914d(true positive\u5224\u5b9a) KITTI\u7684matching\u662f\u4eceground truth boxes\u51fa\u53d1\uff0c\u7b80\u5355\u5730\u5faa\u73af\uff0c\u8d2a\u5a6a\u5730\u5bfb\u627e\u4e0e\u5176IoU(\u4f7f\u7528\u4ec0\u4e48IoU\u7531\u5f53\u524d\u4efb\u52a1\u51b3\u5b9a)\u6700\u5927\u7684prediction.\u4f46\u662f\u8981\u6ce8\u610f\u7684\u662f\uff0c\u8981\u8ba1\u7b97AP,\u8fd8\u662f\u8981\u8003\u8651score\u7684\u9ad8\u4f4e\u7684\u3002\u5b98\u65b9\u7684\u5b9e\u73b0\u4e0e\u52a0\u901f\u5b9e\u73b0\u7684\u601d\u8def\u662f\u4e00\u81f4\u7684\u3002 \u5148\u8003\u8651\u6240\u6709prediction,\u8ba1\u7b97\u4e00\u904dmatching\uff0c\u5e76\u8bb0\u5f55\u6bcf\u4e00\u4e2amatching\u7684score\uff0c\u7136\u540e\u5728PR\u66f2\u7ebf\u4e0a\u91c7\u683741\u4e2a\u70b9\uff0c\u5f97\u523041\u4e2aconfidence\u9608\u503c,\u5bf9\u4e8e\u6bcf\u4e00\u4e2a\u9608\u503c\uff0c\u6ee4\u6389\u6bd4\u8fd9\u4e2a\u9608\u503c\u66f4\u4f4e\u7684boxes\uff0c\u7136\u540e\u91cd\u65b0match\uff0c\u8ba1\u7b97\u8fd9\u4e2a\u70b9\u4e0a\u7684recall\uff08\u8fd9\u4e2a\u4f1a\u4e0e\u91c7\u6837\u70b9\u4e00\u81f4\uff09 precision.\u4ee5\u6b64\u523b\u753bpr\u66f2\u7ebf\u3002 Easy, Medium, Hard KITTI\u4e00\u4e2a\u5f88\u7279\u6b8a\u7684\u673a\u5236\u5728\u4e8e\u5206\u8fa8\u4e86 easy, medium\u4e0ehard\u7684\u7ed3\u679c\u3002\u4ece\u4ee3\u7801\u5b9e\u73b0\u4e0a\uff0c\u53ef\u4ee5\u53d1\u73b0\u8fd9\u662f\u4e00\u4e2a\u4ee52D\u4ee5\u53ca\u906e\u6321\u4e3a\u4e3b\u8981\u6839\u636e\u7684\u5206\u522b\u3002Easy\u5305\u542b2D\u6846\u9ad8\u5ea6\u5927\u4e8e40 pixs\uff0c\u906e\u6321\u7b49\u7ea7\u6700\u4f4e\u7684objects\uff1bMedium\u5305\u542b2D\u6846\u9ad8\u4e8e25 pixs\uff0c\u906e\u6321\u7b49\u7ea7 0,1\u7684\u7269\u4f53\uff1bHard\u5305\u542b2D\u6846\u9ad8\u4e8e25 pixs\uff0c\u906e\u6321\u7b49\u7ea70-2\u7684\u7269\u4f53(\u4e5f\u5305\u542b\u524d\u9762\u63d0\u53ca\u7684\u6240\u6709\u7269\u4f53)\u3002 \u5173\u4e8e\u52a0\u901f \u524d\u9762\u63d0\u53ca\u7684Fast numba code\u76f8\u5bf9\u4e8e\u539f\u6765\u7684cpp\u4ee3\u7801\u5728\u4ee3\u7801\u6838\u5fc3\u903b\u8f91\u4e0a\u5dee\u8ddd\u4e0d\u5927\u3002\u6838\u5fc3\u53d8\u5316\u4e3b\u8981\u6709\u4e24\u4e2a\uff0c\u7b2c\u4e00\u4e2a\u662f\u4f7f\u7528numba.cudaGPU\u52a0\u901f\u4e863D IoU\u7684\u8ba1\u7b97,\u672c\u8d28\u4e0a\u662f\u7b80\u5355\u7684\u5e76\u884c\u8fd0\u7b97(\u5355\u4e2a\u7269\u4f53\u7684\u7b97\u6cd5\u5e76\u6ca1\u6709\u6539\u53d8)\u3002\u7b2c\u4e8c\u4e2a\u662f\u5229\u7528\u4e86numpy\u4ee5\u53canumba.jit\u7684CPU\u5e76\u884c\u4f18\u5316\uff0c\u4ee3\u7801\u4e2d\u6709\u4e00\u4e2a\u51fd\u6570\u7684\u4f5c\u7528\u662fcalculate_iou_partly,\u5c3d\u7ba1\u5b9e\u9645\u8fd0\u7b97\u91cf\u63d0\u5347\u4e86\uff0c\u4f46\u662f\u51cf\u5c11python\u5faa\u73af\u7684\u6b21\u6570\uff0c\u4e14\u5145\u5206\u5229\u7528numpy\u4ee5\u53canumba.jit\u7684\u5e76\u884c\u4f18\u5316\u3002 nuScenes detection score(NDS) - nuScenes pdf code nuscene \u7684\u8bc4\u4ef7metric\u662f\u76f8\u5f53\u72ec\u7279\u7684\uff0c\u4f5c\u8005\u7684\u539f\u610f\u662f\u5e0c\u671b\u6709metric\u5206\u522b\u8868\u8fbe\u5bf9\u4e2d\u5fc3\u8ddd\u79bb\u3001\u671d\u5411\u3001\u5927\u5c0f\u751a\u81f3\u901f\u5ea6\u7b49\u7ec6\u9879\u7684\u8ba1\u7b97\u7ed3\u679c\u3002\u6700\u540e\u7684NDS\u5355\u4e00\u6570\u503c\u4f1a\u662f\u591a\u4e2ametric\u7684\u5747\u503c\u3002 \u5339\u914d(true positive\u5224\u5b9a) nuscene\u7684matching\u662f\u4ecesorted predicted boxes\u51fa\u53d1\uff0c\u6309confidence\u5927\u5c0f\u5faa\u73af(\u6ce8\u610f\u8fd9\u91cc\u7684\u6392\u5e8f\u4ee5\u53ca\u5faa\u73af\u662f\u4ee5\u6240\u6709\u5e27\u7684\u9884\u6d4bbounding box\u4e3a\u51c6\uff0c\u800c\u5339\u914d\u7684\u65f6\u5019\u6839\u636esample_token\u53ea\u5728\u540c\u4e00\u5e27\u7684gt\u4e2d\u8fdb\u884c\u5339\u914d\uff0c sample_token\u4e5f\u662f\u8fd9\u4e2a\u65b9\u6cd5\u80fd\u65b9\u4fbf\u5730\u5b9e\u73b0\u7684\u539f\u56e0\uff0c\u5982\u679c\u5bf9KITTI\u7ed3\u679c\u8981\u7528\u540c\u6837\u7684\u7b97\u6cd5\uff0c\u5219\u9700\u8981\u5bf9\u6bcf\u4e00\u4e2abox\u90fd\u8bb0\u5f55\u81ea\u5df1\u5728\u54ea\u4e00\u5e27)\uff0c\u8d2a\u5a6a\u5730\u5bfb\u627e\u5c1a\u672a\u88ab\u5339\u914d\u7684\u4e14\u4e0e\u5176\u5e73\u9762\u8ddd\u79bb\u6700\u8fd1\u7684ground truth\uff0c\u5982\u679c\u6700\u8fd1\u7684\u8ddd\u79bb\u5728\u9608\u503c\u5185\uff0c\u5219\u8ba4\u4e3a\u662f\u4e00\u5bf9true positive\u3002\u7531\u4e8e\u8fd9\u4e2amatch\u5df2\u7ecf\u662f\u4ece\u9ad8confidence\u5230\u4f4econfidence\u4e86\uff0c\u6240\u4ee5\u53ef\u4ee5\u76f4\u63a5\u8ba1\u7b97precisions\u8ddfrecall\u66f2\u7ebf\u3002 Thresholds\u5206\u56db\u6b21\uff0c\u5206\u522b\u53d6 [0.5, 1, 2, 4] ,\u5f97\u5230\u7684mAP\u53d6\u5747\u503c\u3002\u6b64\u5916\u4ee5 threshold=2 \u65f6\u7684\u5339\u914d\u4e3a\u51c6\uff0c\u8ba1\u7b97\u5176\u4ed6 True positive metrics(TP metrics) \"\"\" tp: List[Union[0, 1]], len(tp) = num_prediction fp: List[Union[1, 0]], len(fp) = num_prediction fp[i] = 1 - tp[i],\u76f8\u5f53\u4e8e conf: List[float], len(conf) = num_prediction (sorted) They recorded whether each predicted object is true positive or false positive, and also its score. npos: number of positive ground truth \"\"\" tp_array = np.cumsum(tp).astype(np.float) fp_array = np.cumsum(fp).astype(np.float) conf_array = np.array(conf) prec = tp_array / (fp_array + tp_array) rec = tp_array / float(npos) rec_interp = np.linspace(0, 1, 101) # sample 0,0.01,0.02,..0.99,1;totally 101 samples prec = np.interp(rec_interp, rec, prec, right=0) # 1d-interpolate conf = np.interp(rec_interp, rec, conf, right=0) # np.interp(x_coordinate, x_data, y_data) -> interpolated_y rec = rec_interp TP Metrics \u8bba\u6587\u4e2d\u7ed9\u51fa\u4e86\u4e94\u4e2ametrics\uff0c\u5206\u522b\u662f \u4e2d\u5fc3\u70b9\u5e73\u9762\u76f4\u7ebf\u8ddd\u79bb scaled IoU (\u5047\u8bbe\u4f4d\u7f6e\u4e0e\u65b9\u5411\u6b63\u786e\uff0cpredicted whl\u957f\u65b9\u4f53\u4e0egt whl\u957f\u65b9\u4f53\u7684iou) yaw\u89d2\u5dee\u503c(radian) 2D\u901f\u5ea6\u5dee\u503c(m/s) \u7ec6\u5206\u7c7b\u5206\u7c7b\u51c6\u786e\u5ea6(nuscene\u5bf9\u90e8\u5206\u7c7b\u522b\u4f1a\u7ee7\u7eed\u7ec6\u5206) \u5404\u4e2aTP\u503c\u4e3a\u8bef\u5dee\u503c\u7684\u5728\u5404\u4e2arecall\u70b9\u4e0a\u7684\u7d2f\u79ef\u5747\u503c\u7684\u5747\u503c\u3002 \\mathrm{NDS}=\\frac{1}{10}\\left[5 \\mathrm{mAP}+\\sum_{\\mathrm{mTP} \\in \\mathrm{TP}}(1-\\min (1, \\mathrm{mTP}))\\right] Average Precision Weighted by Heading(APH) - Waymo \u9875\u9762 waymo\u7684\u7b97\u6cd5\u4e0eKITTI\u7684\u6781\u5ea6\u76f8\u4f3c\uff0c\u533a\u522b\u5728\u4e8e: Easy/Difficult\u5206\u8fa8\u65b9\u6cd5\u4e3b\u8981\u662f\u906e\u6321\u7a0b\u5ea6\u4ee5\u53cabox\u5185\u90e8\u70b9\u4e91\u7684\u6570\u91cf\u3002\u56e0\u800c\u662f\u4e00\u4e2a\u5b8c\u5168\u76843D-oriented\u7684\u5206\u7c7b\u6807\u51c6. \u6bcf\u5f53\u53d1\u73b0\u4e00\u4e2atrue-positive matching, tp = \\frac{\\Delta\\theta}{\\pi} ,\u76f8\u5f53\u4e8e\u53ea\u6709\u89d2\u5ea6\u662f\u51c6\u786e\u7684\u624d\u80fd\u5f97\u5230\u5b8c\u6574\u7684\u4e00\u4e2atrue-positive,\u5426\u5219\u4f1a\u52a0\u4e0a\u4e00\u4e2a\u60e9\u7f5a\u6743\u91cd\u3002\u800cFalse positive\u548cFalse negative\u6ca1\u6709\u53d8\u5316\u3002 mean Detection Score - Cityscapes 3D pdf code Cityscapes \u57fa\u4e8e\u4ed6\u539f\u6709\u7684\u6570\u636e\u4e5f\u53d1\u5e03\u4e86\u4e00\u4e2a\u4e09\u7ef4\u68c0\u6d4b\u6570\u636e\u6807\u6ce8\u96c6\u3002\u4e0e\u4e4b\u524d\u7684\u6570\u636e\u6700\u5927\u7684\u4e0d\u540c\u6709\u4e8c\uff0c\u9996\u5148\u662f\u5b83\u4ec5\u4f7f\u7528\u53cc\u76ee\u6570\u636e\u8fdb\u884c\u6807\u6ce8\uff0c\u5176\u6b21\u662f\u5b83\u6807\u6ce8\u4e863\u4e2a\u7ef4\u5ea6\u7684\u65cb\u8f6c\u3002\u8fd9\u4e2a\u6570\u636e\u96c6\u7684\u8bbe\u8ba1\u5c31\u662f\u4e3a\u4e86 \u8bc4\u4ef7\u5355\u76ee\u76843D\u68c0\u6d4b\u7684\uff0c\u56e0\u800c\u4e0d\u91c7\u75283D IoU\u800c\u9009\u62e9\u4e86\u4e0d\u540c\u7684\u8bbe\u8ba1 \u3002 \u5176\u8bc4\u4ef7\u6307\u6807 mDS \u7531\u4ee5\u4e0b\u51e0\u4e2a\u53c2\u6570\u7ec4\u6210: 2D AP: \u4e0e\u56fe\u50cf2D\u4e00\u81f4, IoU \\geq 0.7 Center Distance: \u4fef\u77b0\u56fe\u8ddd\u79bb \\mathrm{BEVCD} . Yaw Similarity: yaw\u89d2 Pitch-Roll Similarity: pitch-roll\u4f5c\u8005\u8ba4\u4e3a\u5728\u65e0\u4eba\u9a7e\u9a76\u573a\u666f\u5f80\u5f80\u662f\u8026\u5408\u7684\uff0c\u56e0\u800c\u8981\u653e\u5728\u4e00\u8d77\u8bc4\u5224. Size Similarity: \u5927\u5c0f whl \u6709\u51e0\u4e2a\u70b9\u9700\u8981\u6ce8\u610f: AP\u7684\u8ba1\u7b97\u6839\u636e\u4ee3\u7801\uff0c\u5728PR\u66f2\u7ebf\u4e0a\u91c7\u6837\u4e8650\u4e2a\u70b9. \u540e\u9762\u56db\u9879\u7684\u8ba1\u7b97\uff0c\u90fd\u662fdepth-dependent\u7684\u3002\u4ee3\u7801\u4e0a\u6bcf5\u7c73\u5206\u4e00\u4e2abin\uff0c\u7edf\u8ba1\u8fd9\u4e2a\u533a\u95f4\u5185\u7684match,\u6839\u636e\u8fd9\u4e2a\u533a\u95f4\u7684match\u8ba1\u7b97\u5bf9\u5e94\u56db\u4e2a\u9879\u76ee\u4e2d\u8be5bin\u7684score\uff0c\u7136\u540e\u6bcf\u4e2a\u9879\u76ee\u4f1a\u5bf9\u5404\u4e2abin\u6c42\u5e73\u5747. \u540e\u9762\u56db\u9879\u8ba1\u7b97\u7684\u65f6\u5019\uff0cconfidence threshold\u662f\u56fa\u5b9a\u7684\uff0c2D AP\u8ba1\u7b97\u7684\u65f6\u5019\u4f1a\u901a\u8fc7\u53d8\u5316confidence threshold\u5728\u4e00\u7cfb\u5217\u7684recall\u503c\u4e0a\u8ba1\u7b97aP.\u8fd9\u91cc\u7684confidence threshold\u56fa\u5b9a\u4e3a c_{w}=\\underset{c \\in|0,1|}{\\operatorname{argmax}} p(c) r(c) \u4f5c\u8005\u7684intuition\u662f\u8bf4\u8fd9\u4e2a\u8bc4\u4ef7\u65b9\u6848\u548c\u73b0\u5b9e\u90e8\u7f72\u7684\u65f6\u5019\u66f4\u4e3a\u4e00\u81f4(\u6211\u4eec\u4f1a\u76f4\u63a5\u91c7\u7528\u4e00\u4e2a\u5e73\u8861\u597drecall and precision\u7684 threshold\u7ed9\u51fa\u9884\u6d4b). BEVCD\u7684\u8ba1\u7b97:\u5176\u4e2d X_{max} \u662f100\u7c73 \\begin{aligned} \\mathrm{BEVCD}&=1-\\frac{1}{\\mathrm{X}_{\\max }^{2}} \\int_{0}^{\\mathrm{X}_{\\max }} k(s) \\mathrm{d} s \\\\ k(s)&=\\frac{1}{\\mathrm{N}} \\underset{d, g \\in \\mathrm{D}\\left(s, c_{w}\\right)}{\\sum} \\min \\left(\\mathrm{X}_{\\max }, \\sqrt{\\sum_{i \\in|x, y|}\\left(d_{i}-g_{i}\\right)^{2}}\\right) \\end{aligned} Yaw similarity \u8ba1\u7b97: \\begin{aligned} \\text { Yawsim }&=\\frac{1}{X_{\\text {max }}} \\int_{0}^{X_{\\max }} k(s) \\mathrm{d} s \\\\ k(s)&=\\frac{1}{N} \\sum_{d, g \\in D\\left(s, c_{w}\\right)} \\frac{1+\\cos \\left(\\Delta_{Y a w}\\right)}{2} \\end{aligned} Pitch-Roll Similarity \u8ba1\u7b97: \\begin{aligned} \\text { PRSim }&=\\frac{1}{X_{\\text {max }}} \\int_{0}^{X_{\\max }} k(s) \\mathrm{d} s \\\\ k(s)&=\\frac{1}{N} \\sum_{d, g \\in D\\left(s, c_{w}\\right)} \\frac{2+\\cos \\left(\\Delta_{Pitch}\\right) + \\cos \\left(\\Delta_{Roll}\\right) }{4} \\end{aligned} Size Similarity \u8ba1\u7b97: \\begin{aligned} \\text { SizeSim }&=\\frac{1}{X_{\\text {max }}} \\int_{0}^{X_{\\max }} k(s) \\mathrm{d} s \\\\ k(s)&=\\frac{1}{N} \\sum_{d, g \\in D\\left(s, c_{w}\\right)} \\prod_{x\\in \\{l, w, h\\}} \\min \\left(\\frac{d_{x}}{g_{x}}, \\frac{g_{x}}{d_{x}}\\right) \\end{aligned} DS = AP \\times \\frac{BEVCD + YawSim + PRSim + SizeSim}{4}","title":"3D detection evaluation metric"},{"location":"3dDetection/Metric_3d/#3d-detection-evaluation-metric","text":"\u672c\u6587\u4e3b\u8981\u5c1d\u8bd5\u7efc\u8ff03D\u68c0\u6d4b\u7684\u8bc4\u4ef7\u65b9\u6cd5\u4ee5\u53ca\u5bf9\u5e94\u7684\u4e00\u4e9bcode\u7684\u5206\u6790\uff0c\u76ee\u6807\u662f\u6bd4\u8f83\u7ec6\u81f4\u7684\u5206\u6790\uff0c\u5728\u6709\u65b0metric\u63d0\u51fa\u7684\u8fc7\u7a0b\u4e2d\u4f1a\u6301\u7eed\u66f4\u65b0\uff0c\u5148\u7efc\u8ff0\u7684\u662fKITTI\u4e0eNuscene\u4e24\u5927benchmark\u7684\u8bc4\u4ef7\u5206\u6570\u3002 Update 2020.09.08: Add Cityscapes 3D","title":"3D detection evaluation metric"},{"location":"3dDetection/Metric_3d/#average-precision-kitti","text":"Official cpp code fast numba code \u6574\u4f53\u601d\u8def\u4e0a\u6765\u8bf4\uff0cKITTI\u7684\u6d4b\u8bd5\u662f\u5c062D\uff0cBEV\u4ee5\u53ca3D\u68c0\u6d4b\u7684\u8bc4\u4ef7\u8fc7\u7a0b\u5b8c\u5168\u5206\u5f00\u3002\u56e0\u800c\u4e09\u79cdevaluation\u4e2d\u95f4\u9700\u8981\u7684matching\u662f\u4e0d\u76f8\u5173\u7684\uff0c\u4f46\u662f\u7531\u4e8e\u6709\u76f8\u4f3c\u7684\u641c\u7d22\u601d\u8def\uff0c\u56e0\u800c\u4ee3\u7801\u5b9e\u73b0\u4e0a\u6709\u4e00\u5b9a\u7684\u590d\u7528\u3002","title":"Average Precision - Kitti"},{"location":"3dDetection/Metric_3d/#true-positive","text":"KITTI\u7684matching\u662f\u4eceground truth boxes\u51fa\u53d1\uff0c\u7b80\u5355\u5730\u5faa\u73af\uff0c\u8d2a\u5a6a\u5730\u5bfb\u627e\u4e0e\u5176IoU(\u4f7f\u7528\u4ec0\u4e48IoU\u7531\u5f53\u524d\u4efb\u52a1\u51b3\u5b9a)\u6700\u5927\u7684prediction.\u4f46\u662f\u8981\u6ce8\u610f\u7684\u662f\uff0c\u8981\u8ba1\u7b97AP,\u8fd8\u662f\u8981\u8003\u8651score\u7684\u9ad8\u4f4e\u7684\u3002\u5b98\u65b9\u7684\u5b9e\u73b0\u4e0e\u52a0\u901f\u5b9e\u73b0\u7684\u601d\u8def\u662f\u4e00\u81f4\u7684\u3002 \u5148\u8003\u8651\u6240\u6709prediction,\u8ba1\u7b97\u4e00\u904dmatching\uff0c\u5e76\u8bb0\u5f55\u6bcf\u4e00\u4e2amatching\u7684score\uff0c\u7136\u540e\u5728PR\u66f2\u7ebf\u4e0a\u91c7\u683741\u4e2a\u70b9\uff0c\u5f97\u523041\u4e2aconfidence\u9608\u503c,\u5bf9\u4e8e\u6bcf\u4e00\u4e2a\u9608\u503c\uff0c\u6ee4\u6389\u6bd4\u8fd9\u4e2a\u9608\u503c\u66f4\u4f4e\u7684boxes\uff0c\u7136\u540e\u91cd\u65b0match\uff0c\u8ba1\u7b97\u8fd9\u4e2a\u70b9\u4e0a\u7684recall\uff08\u8fd9\u4e2a\u4f1a\u4e0e\u91c7\u6837\u70b9\u4e00\u81f4\uff09 precision.\u4ee5\u6b64\u523b\u753bpr\u66f2\u7ebf\u3002","title":"\u5339\u914d(true positive\u5224\u5b9a)"},{"location":"3dDetection/Metric_3d/#easy-medium-hard","text":"KITTI\u4e00\u4e2a\u5f88\u7279\u6b8a\u7684\u673a\u5236\u5728\u4e8e\u5206\u8fa8\u4e86 easy, medium\u4e0ehard\u7684\u7ed3\u679c\u3002\u4ece\u4ee3\u7801\u5b9e\u73b0\u4e0a\uff0c\u53ef\u4ee5\u53d1\u73b0\u8fd9\u662f\u4e00\u4e2a\u4ee52D\u4ee5\u53ca\u906e\u6321\u4e3a\u4e3b\u8981\u6839\u636e\u7684\u5206\u522b\u3002Easy\u5305\u542b2D\u6846\u9ad8\u5ea6\u5927\u4e8e40 pixs\uff0c\u906e\u6321\u7b49\u7ea7\u6700\u4f4e\u7684objects\uff1bMedium\u5305\u542b2D\u6846\u9ad8\u4e8e25 pixs\uff0c\u906e\u6321\u7b49\u7ea7 0,1\u7684\u7269\u4f53\uff1bHard\u5305\u542b2D\u6846\u9ad8\u4e8e25 pixs\uff0c\u906e\u6321\u7b49\u7ea70-2\u7684\u7269\u4f53(\u4e5f\u5305\u542b\u524d\u9762\u63d0\u53ca\u7684\u6240\u6709\u7269\u4f53)\u3002","title":"Easy, Medium, Hard"},{"location":"3dDetection/Metric_3d/#_1","text":"\u524d\u9762\u63d0\u53ca\u7684Fast numba code\u76f8\u5bf9\u4e8e\u539f\u6765\u7684cpp\u4ee3\u7801\u5728\u4ee3\u7801\u6838\u5fc3\u903b\u8f91\u4e0a\u5dee\u8ddd\u4e0d\u5927\u3002\u6838\u5fc3\u53d8\u5316\u4e3b\u8981\u6709\u4e24\u4e2a\uff0c\u7b2c\u4e00\u4e2a\u662f\u4f7f\u7528numba.cudaGPU\u52a0\u901f\u4e863D IoU\u7684\u8ba1\u7b97,\u672c\u8d28\u4e0a\u662f\u7b80\u5355\u7684\u5e76\u884c\u8fd0\u7b97(\u5355\u4e2a\u7269\u4f53\u7684\u7b97\u6cd5\u5e76\u6ca1\u6709\u6539\u53d8)\u3002\u7b2c\u4e8c\u4e2a\u662f\u5229\u7528\u4e86numpy\u4ee5\u53canumba.jit\u7684CPU\u5e76\u884c\u4f18\u5316\uff0c\u4ee3\u7801\u4e2d\u6709\u4e00\u4e2a\u51fd\u6570\u7684\u4f5c\u7528\u662fcalculate_iou_partly,\u5c3d\u7ba1\u5b9e\u9645\u8fd0\u7b97\u91cf\u63d0\u5347\u4e86\uff0c\u4f46\u662f\u51cf\u5c11python\u5faa\u73af\u7684\u6b21\u6570\uff0c\u4e14\u5145\u5206\u5229\u7528numpy\u4ee5\u53canumba.jit\u7684\u5e76\u884c\u4f18\u5316\u3002","title":"\u5173\u4e8e\u52a0\u901f"},{"location":"3dDetection/Metric_3d/#nuscenes-detection-scorends-nuscenes","text":"pdf code nuscene \u7684\u8bc4\u4ef7metric\u662f\u76f8\u5f53\u72ec\u7279\u7684\uff0c\u4f5c\u8005\u7684\u539f\u610f\u662f\u5e0c\u671b\u6709metric\u5206\u522b\u8868\u8fbe\u5bf9\u4e2d\u5fc3\u8ddd\u79bb\u3001\u671d\u5411\u3001\u5927\u5c0f\u751a\u81f3\u901f\u5ea6\u7b49\u7ec6\u9879\u7684\u8ba1\u7b97\u7ed3\u679c\u3002\u6700\u540e\u7684NDS\u5355\u4e00\u6570\u503c\u4f1a\u662f\u591a\u4e2ametric\u7684\u5747\u503c\u3002","title":"nuScenes detection score(NDS) - nuScenes"},{"location":"3dDetection/Metric_3d/#true-positive_1","text":"nuscene\u7684matching\u662f\u4ecesorted predicted boxes\u51fa\u53d1\uff0c\u6309confidence\u5927\u5c0f\u5faa\u73af(\u6ce8\u610f\u8fd9\u91cc\u7684\u6392\u5e8f\u4ee5\u53ca\u5faa\u73af\u662f\u4ee5\u6240\u6709\u5e27\u7684\u9884\u6d4bbounding box\u4e3a\u51c6\uff0c\u800c\u5339\u914d\u7684\u65f6\u5019\u6839\u636esample_token\u53ea\u5728\u540c\u4e00\u5e27\u7684gt\u4e2d\u8fdb\u884c\u5339\u914d\uff0c sample_token\u4e5f\u662f\u8fd9\u4e2a\u65b9\u6cd5\u80fd\u65b9\u4fbf\u5730\u5b9e\u73b0\u7684\u539f\u56e0\uff0c\u5982\u679c\u5bf9KITTI\u7ed3\u679c\u8981\u7528\u540c\u6837\u7684\u7b97\u6cd5\uff0c\u5219\u9700\u8981\u5bf9\u6bcf\u4e00\u4e2abox\u90fd\u8bb0\u5f55\u81ea\u5df1\u5728\u54ea\u4e00\u5e27)\uff0c\u8d2a\u5a6a\u5730\u5bfb\u627e\u5c1a\u672a\u88ab\u5339\u914d\u7684\u4e14\u4e0e\u5176\u5e73\u9762\u8ddd\u79bb\u6700\u8fd1\u7684ground truth\uff0c\u5982\u679c\u6700\u8fd1\u7684\u8ddd\u79bb\u5728\u9608\u503c\u5185\uff0c\u5219\u8ba4\u4e3a\u662f\u4e00\u5bf9true positive\u3002\u7531\u4e8e\u8fd9\u4e2amatch\u5df2\u7ecf\u662f\u4ece\u9ad8confidence\u5230\u4f4econfidence\u4e86\uff0c\u6240\u4ee5\u53ef\u4ee5\u76f4\u63a5\u8ba1\u7b97precisions\u8ddfrecall\u66f2\u7ebf\u3002 Thresholds\u5206\u56db\u6b21\uff0c\u5206\u522b\u53d6 [0.5, 1, 2, 4] ,\u5f97\u5230\u7684mAP\u53d6\u5747\u503c\u3002\u6b64\u5916\u4ee5 threshold=2 \u65f6\u7684\u5339\u914d\u4e3a\u51c6\uff0c\u8ba1\u7b97\u5176\u4ed6 True positive metrics(TP metrics) \"\"\" tp: List[Union[0, 1]], len(tp) = num_prediction fp: List[Union[1, 0]], len(fp) = num_prediction fp[i] = 1 - tp[i],\u76f8\u5f53\u4e8e conf: List[float], len(conf) = num_prediction (sorted) They recorded whether each predicted object is true positive or false positive, and also its score. npos: number of positive ground truth \"\"\" tp_array = np.cumsum(tp).astype(np.float) fp_array = np.cumsum(fp).astype(np.float) conf_array = np.array(conf) prec = tp_array / (fp_array + tp_array) rec = tp_array / float(npos) rec_interp = np.linspace(0, 1, 101) # sample 0,0.01,0.02,..0.99,1;totally 101 samples prec = np.interp(rec_interp, rec, prec, right=0) # 1d-interpolate conf = np.interp(rec_interp, rec, conf, right=0) # np.interp(x_coordinate, x_data, y_data) -> interpolated_y rec = rec_interp","title":"\u5339\u914d(true positive\u5224\u5b9a)"},{"location":"3dDetection/Metric_3d/#tp-metrics","text":"\u8bba\u6587\u4e2d\u7ed9\u51fa\u4e86\u4e94\u4e2ametrics\uff0c\u5206\u522b\u662f \u4e2d\u5fc3\u70b9\u5e73\u9762\u76f4\u7ebf\u8ddd\u79bb scaled IoU (\u5047\u8bbe\u4f4d\u7f6e\u4e0e\u65b9\u5411\u6b63\u786e\uff0cpredicted whl\u957f\u65b9\u4f53\u4e0egt whl\u957f\u65b9\u4f53\u7684iou) yaw\u89d2\u5dee\u503c(radian) 2D\u901f\u5ea6\u5dee\u503c(m/s) \u7ec6\u5206\u7c7b\u5206\u7c7b\u51c6\u786e\u5ea6(nuscene\u5bf9\u90e8\u5206\u7c7b\u522b\u4f1a\u7ee7\u7eed\u7ec6\u5206) \u5404\u4e2aTP\u503c\u4e3a\u8bef\u5dee\u503c\u7684\u5728\u5404\u4e2arecall\u70b9\u4e0a\u7684\u7d2f\u79ef\u5747\u503c\u7684\u5747\u503c\u3002 \\mathrm{NDS}=\\frac{1}{10}\\left[5 \\mathrm{mAP}+\\sum_{\\mathrm{mTP} \\in \\mathrm{TP}}(1-\\min (1, \\mathrm{mTP}))\\right]","title":"TP Metrics"},{"location":"3dDetection/Metric_3d/#average-precision-weighted-by-headingaph-waymo","text":"\u9875\u9762 waymo\u7684\u7b97\u6cd5\u4e0eKITTI\u7684\u6781\u5ea6\u76f8\u4f3c\uff0c\u533a\u522b\u5728\u4e8e: Easy/Difficult\u5206\u8fa8\u65b9\u6cd5\u4e3b\u8981\u662f\u906e\u6321\u7a0b\u5ea6\u4ee5\u53cabox\u5185\u90e8\u70b9\u4e91\u7684\u6570\u91cf\u3002\u56e0\u800c\u662f\u4e00\u4e2a\u5b8c\u5168\u76843D-oriented\u7684\u5206\u7c7b\u6807\u51c6. \u6bcf\u5f53\u53d1\u73b0\u4e00\u4e2atrue-positive matching, tp = \\frac{\\Delta\\theta}{\\pi} ,\u76f8\u5f53\u4e8e\u53ea\u6709\u89d2\u5ea6\u662f\u51c6\u786e\u7684\u624d\u80fd\u5f97\u5230\u5b8c\u6574\u7684\u4e00\u4e2atrue-positive,\u5426\u5219\u4f1a\u52a0\u4e0a\u4e00\u4e2a\u60e9\u7f5a\u6743\u91cd\u3002\u800cFalse positive\u548cFalse negative\u6ca1\u6709\u53d8\u5316\u3002","title":"Average Precision Weighted by Heading(APH) - Waymo"},{"location":"3dDetection/Metric_3d/#mean-detection-score-cityscapes-3d","text":"pdf code Cityscapes \u57fa\u4e8e\u4ed6\u539f\u6709\u7684\u6570\u636e\u4e5f\u53d1\u5e03\u4e86\u4e00\u4e2a\u4e09\u7ef4\u68c0\u6d4b\u6570\u636e\u6807\u6ce8\u96c6\u3002\u4e0e\u4e4b\u524d\u7684\u6570\u636e\u6700\u5927\u7684\u4e0d\u540c\u6709\u4e8c\uff0c\u9996\u5148\u662f\u5b83\u4ec5\u4f7f\u7528\u53cc\u76ee\u6570\u636e\u8fdb\u884c\u6807\u6ce8\uff0c\u5176\u6b21\u662f\u5b83\u6807\u6ce8\u4e863\u4e2a\u7ef4\u5ea6\u7684\u65cb\u8f6c\u3002\u8fd9\u4e2a\u6570\u636e\u96c6\u7684\u8bbe\u8ba1\u5c31\u662f\u4e3a\u4e86 \u8bc4\u4ef7\u5355\u76ee\u76843D\u68c0\u6d4b\u7684\uff0c\u56e0\u800c\u4e0d\u91c7\u75283D IoU\u800c\u9009\u62e9\u4e86\u4e0d\u540c\u7684\u8bbe\u8ba1 \u3002 \u5176\u8bc4\u4ef7\u6307\u6807 mDS \u7531\u4ee5\u4e0b\u51e0\u4e2a\u53c2\u6570\u7ec4\u6210: 2D AP: \u4e0e\u56fe\u50cf2D\u4e00\u81f4, IoU \\geq 0.7 Center Distance: \u4fef\u77b0\u56fe\u8ddd\u79bb \\mathrm{BEVCD} . Yaw Similarity: yaw\u89d2 Pitch-Roll Similarity: pitch-roll\u4f5c\u8005\u8ba4\u4e3a\u5728\u65e0\u4eba\u9a7e\u9a76\u573a\u666f\u5f80\u5f80\u662f\u8026\u5408\u7684\uff0c\u56e0\u800c\u8981\u653e\u5728\u4e00\u8d77\u8bc4\u5224. Size Similarity: \u5927\u5c0f whl \u6709\u51e0\u4e2a\u70b9\u9700\u8981\u6ce8\u610f: AP\u7684\u8ba1\u7b97\u6839\u636e\u4ee3\u7801\uff0c\u5728PR\u66f2\u7ebf\u4e0a\u91c7\u6837\u4e8650\u4e2a\u70b9. \u540e\u9762\u56db\u9879\u7684\u8ba1\u7b97\uff0c\u90fd\u662fdepth-dependent\u7684\u3002\u4ee3\u7801\u4e0a\u6bcf5\u7c73\u5206\u4e00\u4e2abin\uff0c\u7edf\u8ba1\u8fd9\u4e2a\u533a\u95f4\u5185\u7684match,\u6839\u636e\u8fd9\u4e2a\u533a\u95f4\u7684match\u8ba1\u7b97\u5bf9\u5e94\u56db\u4e2a\u9879\u76ee\u4e2d\u8be5bin\u7684score\uff0c\u7136\u540e\u6bcf\u4e2a\u9879\u76ee\u4f1a\u5bf9\u5404\u4e2abin\u6c42\u5e73\u5747. \u540e\u9762\u56db\u9879\u8ba1\u7b97\u7684\u65f6\u5019\uff0cconfidence threshold\u662f\u56fa\u5b9a\u7684\uff0c2D AP\u8ba1\u7b97\u7684\u65f6\u5019\u4f1a\u901a\u8fc7\u53d8\u5316confidence threshold\u5728\u4e00\u7cfb\u5217\u7684recall\u503c\u4e0a\u8ba1\u7b97aP.\u8fd9\u91cc\u7684confidence threshold\u56fa\u5b9a\u4e3a c_{w}=\\underset{c \\in|0,1|}{\\operatorname{argmax}} p(c) r(c) \u4f5c\u8005\u7684intuition\u662f\u8bf4\u8fd9\u4e2a\u8bc4\u4ef7\u65b9\u6848\u548c\u73b0\u5b9e\u90e8\u7f72\u7684\u65f6\u5019\u66f4\u4e3a\u4e00\u81f4(\u6211\u4eec\u4f1a\u76f4\u63a5\u91c7\u7528\u4e00\u4e2a\u5e73\u8861\u597drecall and precision\u7684 threshold\u7ed9\u51fa\u9884\u6d4b). BEVCD\u7684\u8ba1\u7b97:\u5176\u4e2d X_{max} \u662f100\u7c73 \\begin{aligned} \\mathrm{BEVCD}&=1-\\frac{1}{\\mathrm{X}_{\\max }^{2}} \\int_{0}^{\\mathrm{X}_{\\max }} k(s) \\mathrm{d} s \\\\ k(s)&=\\frac{1}{\\mathrm{N}} \\underset{d, g \\in \\mathrm{D}\\left(s, c_{w}\\right)}{\\sum} \\min \\left(\\mathrm{X}_{\\max }, \\sqrt{\\sum_{i \\in|x, y|}\\left(d_{i}-g_{i}\\right)^{2}}\\right) \\end{aligned} Yaw similarity \u8ba1\u7b97: \\begin{aligned} \\text { Yawsim }&=\\frac{1}{X_{\\text {max }}} \\int_{0}^{X_{\\max }} k(s) \\mathrm{d} s \\\\ k(s)&=\\frac{1}{N} \\sum_{d, g \\in D\\left(s, c_{w}\\right)} \\frac{1+\\cos \\left(\\Delta_{Y a w}\\right)}{2} \\end{aligned} Pitch-Roll Similarity \u8ba1\u7b97: \\begin{aligned} \\text { PRSim }&=\\frac{1}{X_{\\text {max }}} \\int_{0}^{X_{\\max }} k(s) \\mathrm{d} s \\\\ k(s)&=\\frac{1}{N} \\sum_{d, g \\in D\\left(s, c_{w}\\right)} \\frac{2+\\cos \\left(\\Delta_{Pitch}\\right) + \\cos \\left(\\Delta_{Roll}\\right) }{4} \\end{aligned} Size Similarity \u8ba1\u7b97: \\begin{aligned} \\text { SizeSim }&=\\frac{1}{X_{\\text {max }}} \\int_{0}^{X_{\\max }} k(s) \\mathrm{d} s \\\\ k(s)&=\\frac{1}{N} \\sum_{d, g \\in D\\left(s, c_{w}\\right)} \\prod_{x\\in \\{l, w, h\\}} \\min \\left(\\frac{d_{x}}{g_{x}}, \\frac{g_{x}}{d_{x}}\\right) \\end{aligned} DS = AP \\times \\frac{BEVCD + YawSim + PRSim + SizeSim}{4}","title":"mean Detection Score - Cityscapes 3D"},{"location":"3dDetection/Mono3D_virtualcam/","text":"Towards Generalization Across Depth for Monocular 3D Object Detection (Former: Single-Stage Monocular 3D Object Detection with Virtual Cameras) \u8fd9\u7bc7\u6587\u7ae0\u6765\u81ea\u4e8e MonoDIS \u7684\u4f5c\u8005\u7ec4\uff0c\u91c7\u7528\u7684\u662fvirtual camera\u7684\u65b9\u6cd5\uff0c\u5f97\u5230\u4e86\u76f8\u5bf9\u4e0d\u9519\u7684\u7ed3\u679c Update : \u4f5c\u8005\u57282020.08\u5c06arxiv\u4e0a\u7684\u9898\u76ee\u505a\u4e86\u4fee\u6539\uff0c\u6587\u7ae0\u5185\u5bb9\u8868\u8fbe\u4e0a\u4e5f\u53d1\u751f\u4e86\u5927\u6539\uff0c\u5e76\u4e2d\u4e86ECCV. \u66f4\u65b0\u540e\u7684\u6574\u4f53pipeline ECCV\u7248\u672c\u57fa\u672c\u662f\u9664\u4e86\u57fa\u7840\u601d\u60f3\u5ef6\u7eed\u4e4b\u5916\u7684\u5b8c\u5168\u91cd\u505a\u3002\u4f5c\u8005\u7684\u601d\u8def\u662f\u901a\u8fc7virtual camera \u5f62\u6210\u5bf9\u8ddd\u79bb\u4e0d\u654f\u611f\u7684\u5206\u7c7b\u5668\uff0c \u5728\u8bad\u7ec3\u7684\u65f6\u5019\uff0c\u5bf9\u6bcf\u4e2aground truth,\u968f\u673a\u5728\u4e00\u5b9a\u8303\u56f4\u5185\u91c7\u6837 virtual camera viewpoint. \u5728\u63a8\u7406\u7684\u65f6\u5019\uff0c\u5747\u5300\u91c7\u96c6\u591a\u4e2a3D viewpoints\uff0c\u7136\u540e\u5728\u5404\u81eaviewpoint\u4e2d\u8fdb\u884cdetection. \u5176\u6027\u80fd\u6ca1\u6709\u5728KITTI\u6392\u884c\u699c\u4e0a\u516c\u5f00\uff0c\u4e3a [15.19, 10.90, 9.26] virtual camera (\u66f4\u65b0\u524d) \u6838\u5fc3\u601d\u8def\uff0c\u5728\u56fe\u4e2d\u6838\u5fc3\u533a\u57dfcrop\u51fa\u591a\u4e2a\u6709\u6548\u533a\u57df\uff0c\u7136\u540e\u5728\u91cc\u9762\u8fdb\u884c3D detection\uff0c\u91cd\u8981\u7684\u6709\u51e0\u4e2ainsight. \u601d\u8def\u4e0e\u4f20\u7edf\u7684RCNN\u6709\u76f8\u4f3c\u4e4b\u5904\uff0c\u4e5f\u5c31\u662f\u4f7f\u7528\u4f20\u7edf\u65b9\u6cd5(\u6839\u636e3D\u7a7a\u95f4\u904d\u5386\u6216\u8005\u5176\u4ed6\u63d0\u793a)\uff0c\u4ece\u539f\u56fe\u4e2dcrop\u51fa\u6709\u6548\u6846\u518d\u8fdb\u884c\u5206\u6790\uff0c\u533a\u522b\u5728\u4e8ecrop\u51fa\u6765\u7684\u6bcf\u4e00\u5f20\u5b50\u90fd\u8fd8\u53ef\u80fd\u6709\u591a\u4e2atarget \u6bcf\u4e00\u4e2a3Ddetection\u9884\u6d4b\u7684\u6df1\u5ea6\u662f\u76f8\u5bf9\u4e8e\u865a\u62df\u76f8\u673a\u4f4d\u7f6e\u7684\u6df1\u5ea6\uff0c\u7a81\u51fa\u4e00\u4e2ascale invariance. virtual camera\u5177\u4f53\u7684\u5b9e\u73b0trick\u8f83\u591a\uff0c\u8d85\u53c2\u6570\u5f88\u591a\uff0c\u82e5\u60f3\u8981\u590d\u73b0\uff0c\u8fd9\u91cc\u5efa\u8bae\u56de\u770b\u8bba\u6587\u7684\u7b2c4\u7ae0\u8282\u4ee5\u53ca\u7b2c6.2\u7ae0\u8282\u3002\u7531\u4e8e\u6b20\u7f3a\u4ee3\u7801\uff0c\u8fd9\u4e2a\u7ed3\u679c\u6bd4\u8f83\u96be\u4ee5\u786e\u8ba4\u3002 DL structure(\u66f4\u65b0\u524d) \u7f51\u7edc\u7684\u8f93\u5165\u8f93\u51fa\u4e0e MonoDIS \u662f\u4e00\u6837\u7684\u3002","title":"Towards Generalization Across Depth for Monocular 3D Object Detection (Former: Single-Stage Monocular 3D Object Detection with Virtual Cameras)"},{"location":"3dDetection/Mono3D_virtualcam/#towards-generalization-across-depth-for-monocular-3d-object-detection-former-single-stage-monocular-3d-object-detection-with-virtual-cameras","text":"\u8fd9\u7bc7\u6587\u7ae0\u6765\u81ea\u4e8e MonoDIS \u7684\u4f5c\u8005\u7ec4\uff0c\u91c7\u7528\u7684\u662fvirtual camera\u7684\u65b9\u6cd5\uff0c\u5f97\u5230\u4e86\u76f8\u5bf9\u4e0d\u9519\u7684\u7ed3\u679c Update : \u4f5c\u8005\u57282020.08\u5c06arxiv\u4e0a\u7684\u9898\u76ee\u505a\u4e86\u4fee\u6539\uff0c\u6587\u7ae0\u5185\u5bb9\u8868\u8fbe\u4e0a\u4e5f\u53d1\u751f\u4e86\u5927\u6539\uff0c\u5e76\u4e2d\u4e86ECCV.","title":"Towards Generalization Across Depth for Monocular 3D Object Detection (Former: Single-Stage Monocular 3D Object Detection with Virtual Cameras)"},{"location":"3dDetection/Mono3D_virtualcam/#pipeline","text":"ECCV\u7248\u672c\u57fa\u672c\u662f\u9664\u4e86\u57fa\u7840\u601d\u60f3\u5ef6\u7eed\u4e4b\u5916\u7684\u5b8c\u5168\u91cd\u505a\u3002\u4f5c\u8005\u7684\u601d\u8def\u662f\u901a\u8fc7virtual camera \u5f62\u6210\u5bf9\u8ddd\u79bb\u4e0d\u654f\u611f\u7684\u5206\u7c7b\u5668\uff0c \u5728\u8bad\u7ec3\u7684\u65f6\u5019\uff0c\u5bf9\u6bcf\u4e2aground truth,\u968f\u673a\u5728\u4e00\u5b9a\u8303\u56f4\u5185\u91c7\u6837 virtual camera viewpoint. \u5728\u63a8\u7406\u7684\u65f6\u5019\uff0c\u5747\u5300\u91c7\u96c6\u591a\u4e2a3D viewpoints\uff0c\u7136\u540e\u5728\u5404\u81eaviewpoint\u4e2d\u8fdb\u884cdetection. \u5176\u6027\u80fd\u6ca1\u6709\u5728KITTI\u6392\u884c\u699c\u4e0a\u516c\u5f00\uff0c\u4e3a [15.19, 10.90, 9.26]","title":"\u66f4\u65b0\u540e\u7684\u6574\u4f53pipeline"},{"location":"3dDetection/Mono3D_virtualcam/#virtual-camera","text":"\u6838\u5fc3\u601d\u8def\uff0c\u5728\u56fe\u4e2d\u6838\u5fc3\u533a\u57dfcrop\u51fa\u591a\u4e2a\u6709\u6548\u533a\u57df\uff0c\u7136\u540e\u5728\u91cc\u9762\u8fdb\u884c3D detection\uff0c\u91cd\u8981\u7684\u6709\u51e0\u4e2ainsight. \u601d\u8def\u4e0e\u4f20\u7edf\u7684RCNN\u6709\u76f8\u4f3c\u4e4b\u5904\uff0c\u4e5f\u5c31\u662f\u4f7f\u7528\u4f20\u7edf\u65b9\u6cd5(\u6839\u636e3D\u7a7a\u95f4\u904d\u5386\u6216\u8005\u5176\u4ed6\u63d0\u793a)\uff0c\u4ece\u539f\u56fe\u4e2dcrop\u51fa\u6709\u6548\u6846\u518d\u8fdb\u884c\u5206\u6790\uff0c\u533a\u522b\u5728\u4e8ecrop\u51fa\u6765\u7684\u6bcf\u4e00\u5f20\u5b50\u90fd\u8fd8\u53ef\u80fd\u6709\u591a\u4e2atarget \u6bcf\u4e00\u4e2a3Ddetection\u9884\u6d4b\u7684\u6df1\u5ea6\u662f\u76f8\u5bf9\u4e8e\u865a\u62df\u76f8\u673a\u4f4d\u7f6e\u7684\u6df1\u5ea6\uff0c\u7a81\u51fa\u4e00\u4e2ascale invariance. virtual camera\u5177\u4f53\u7684\u5b9e\u73b0trick\u8f83\u591a\uff0c\u8d85\u53c2\u6570\u5f88\u591a\uff0c\u82e5\u60f3\u8981\u590d\u73b0\uff0c\u8fd9\u91cc\u5efa\u8bae\u56de\u770b\u8bba\u6587\u7684\u7b2c4\u7ae0\u8282\u4ee5\u53ca\u7b2c6.2\u7ae0\u8282\u3002\u7531\u4e8e\u6b20\u7f3a\u4ee3\u7801\uff0c\u8fd9\u4e2a\u7ed3\u679c\u6bd4\u8f83\u96be\u4ee5\u786e\u8ba4\u3002","title":"virtual camera (\u66f4\u65b0\u524d)"},{"location":"3dDetection/Mono3D_virtualcam/#dl-structure","text":"\u7f51\u7edc\u7684\u8f93\u5165\u8f93\u51fa\u4e0e MonoDIS \u662f\u4e00\u6837\u7684\u3002","title":"DL structure(\u66f4\u65b0\u524d)"},{"location":"3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/","text":"MonoGRNet: A Geometric Reasoning Network for Monocular 3D Object Localization \u672c\u6587\u63d0\u51fa\u7684\u7f51\u7edc\u7ed3\u6784\u53ef\u4ee5\u57280.06s\u79d2\u5185\u5b8c\u6210\u4e00\u5f20\u56fe\u7247\u7684inference","title":"MonoGRNet: A Geometric Reasoning Network for Monocular 3D Object Localization"},{"location":"3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/#monogrnet-a-geometric-reasoning-network-for-monocular-3d-object-localization","text":"\u672c\u6587\u63d0\u51fa\u7684\u7f51\u7edc\u7ed3\u6784\u53ef\u4ee5\u57280.06s\u79d2\u5185\u5b8c\u6210\u4e00\u5f20\u56fe\u7247\u7684inference","title":"MonoGRNet: A Geometric Reasoning Network for Monocular 3D Object Localization"},{"location":"3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/","text":"Monocular 3D Object Detection and Box Fitting Trained End-to-End Using Intersection-over-Union Loss \u672c\u6587\u63d0\u51fa\u4e86\u5728\u5355\u76ee3D\u68c0\u6d4b\u4e2d\u8003\u8651\u5f02\u8d28\u566a\u58f0\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2abackprop through optimization\u7684\u65b9\u6cd5\u3002 \u6d41\u7a0b\u603b\u89c8 \u9996\u5148\u4e00\u4e2aCNN\u8fdb\u884cobject detection\uff0c\u8f93\u51fa\u7c7b\u522bscore\u4ee5\u53ca2D bounding box\uff0c\u6267\u884cNMS\u5220\u9664\u90e8\u5206\u5197\u4f59\uff0c\u6700\u540e\u4f7f\u7528\u4e00\u4e2a\u975e\u7ebf\u6027\u4f18\u5316\u3002 \u7f51\u7edc\u8f93\u51fa \u7f51\u7edc\u8f93\u51fa\u7ed3\u679c\u9664\u4e86\u5206\u7c7b\u7ed3\u679c\u4e4b\u5916\u8fd8\u670926\u7ef4\u3002 2D bounding box\u5bf9\u5e94\u4e24\u4e2a\u89d2\u70b94\u4e2a\u503c\u3002 \u7269\u4f53\u4e2d\u5fc3\u4e0e\u76f8\u673a\u7684\u8ddd\u79bb(\u4e5f\u53ef\u4ee5\u7528Z\u8f74\u8ddd\u79bb\uff0c\u6027\u80fd\u5dee\u8ddd\u4e0d\u5927)\uff0c\u4e00\u4e2a\u503c \u89c2\u5bdf\u89d2 \\alpha \u9884\u6d4b\u5176sin\u4e0ecos\u503c\uff0c\u51712\u4e2a\u503c \u7269\u4f53\u5c3a\u5ea6\u957f\u5bbd\u9ad8\uff0c\u7528log\u503c\u4ee3\u8868\uff0c\u51713\u4e2a\u503c\u3002 3D bounding box\u76848\u4e2a\u70b9\u5728\u76f8\u673a\u5750\u6807\u7cfb\u7684\u6295\u5f71\uff0c\u517116\u4e2a\u503c\u3002 \u5bf93Dbox\u7684\u521d\u59cb\u89e3,\u4ee52D\u6846\u7684\u4e2d\u5fc3\u4f5c\u4e3a\u7269\u4f533D\u4e2d\u5fc3\u5728\u56fe\u7247\u7684\u6295\u5f71,\u5c31\u53ef\u4ee5\u5f97\u5230\u5bf93Dbox\u7684\u521d\u59cb\u4f30\u8ba1. taskNet\u4f7f\u5f97\u6bcf\u4e00\u4e2a\u8f93\u51fa\u4f1a\u540c\u65f6\u5e26\u4e0a\u4e86\u5bf9\u4e0d\u786e\u5b9a\u6027\u7684\u4f30\u8ba1 \u4e09\u79cdLoss\u8bad\u7ec3\u65b9\u6cd5 \u7b2c\u4e00\u79cd\u4f20\u7edf\u5b9a\u4e49\u662f\u5e73\u7a33\u566a\u58f0 Loss\u51fd\u6570\u5b9a\u4e49\u5982\u4e0b: \\begin{aligned} \\mathcal{L}_{1} &=\\mathcal{L}_{c l s} \\\\ &+\\frac{1}{N} \\sum_{j=1}^{N} \\sum_{k=1}^{26}\\left(\\frac{\\left\\|\\mathbf{y}_{k}^{(j)}-f_{k}\\left(\\mathbf{b}^{(j)}\\right)\\right\\|^{2}}{2 \\sigma_{k}^{2}}+\\log \\sigma_{k}\\right) \\end{aligned} \u672c\u8d28\u4e0a\u5c31\u662f Multi Task Learning \uff0c \u4e5f\u5c31\u662f\u8bf4\u6bcf\u4e00\u4e2aloss\u7c7b\u522b\u4f1a\u6709\u4e00\u4e2a\u5355\u72ec\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1. \u7b2c\u4e8c\u79cd\u5b9a\u4e49\u662f\u5f02\u65b9\u5dee\u566a\u58f0 \\begin{aligned} \\mathcal{L}_{2} &=\\mathcal{L}_{c l s} \\\\ &+\\frac{1}{N} \\sum_{i}\\left(\\frac{\\left(y_{i}-f_{i}\\left(\\mathbf{b}_{i}\\right)^{2}\\right)}{2 \\sigma_{i}^{2}}+\\log \\sigma_{i}-\\log P_{\\text {prior }}\\left(\\sigma_{i}^{-2}\\right)\\right) \\end{aligned} \u4e5f\u5c31\u662f\u9700\u8981tasknet\u8ddf\u968f\u6bcf\u4e00\u4e2a\u8f93\u51fa\u8f93\u51fa\u4e00\u4e2a\u8bef\u5dee\u503c \u5176\u4e2d P_{prior} \u5728\u8fd9\u91cc\u88ab\u5b9a\u4e49\u4e3a (\\alpha, \\beta) = (1, 0.5) \u7684 \u4f3d\u9a6c\u5206\u5e03 \u4ee5\u4e0a\u4e24\u79cd\u65b9\u5f0f\u5f97\u5230\u7684variance\u90fd\u53ef\u4ee5\u5728inference\u7684\u65f6\u5019\u4f5c\u4e3a\u5bf9\u4e0d\u786e\u5b9a\u6027\u7684\u4f30\u8ba1\uff0c\u4e5f\u5c31\u4fbf\u4e8e\u5bf9\u540e\u7aef\u4f18\u5316 \u7b2c\u4e09\u79cd\u8bad\u7ec3\u65b9\u5f0f\u662fbackprop through optimization \u9996\u5148\u4f18\u5316\u95ee\u9898\u7684\u5b9a\u4e49\u6765\u81ea\u4e8e E(\\mathbf{b} ; \\mathbf{y})=\\sum_{i=1}^{26} r_{i}(\\mathbf{b} ; \\mathbf{y})^{2}=\\sum_{i=1}^{26}\\left(w_{i}\\left(y_{i}-f_{i}(\\mathbf{b})\\right)\\right)^{2} \u524d\u4e24\u79cd\u65b9\u5f0f\u7684\u903b\u8f91\u5c31\u662f\u5728\u8bad\u7ec3\u7684\u65f6\u5019\u901a\u8fc7loss\u5b66\u4e60variance\uff0c\u7136\u540e\u76f4\u63a5\u7528\u4e8einference\u3002\u800c\u7b2c\u4e09\u79cd\u8bad\u7ec3\u65b9\u5f0f\u5c31\u662f\u628aoptimization\u653e\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u4e5f\u5c31\u80fd\u76f4\u63a5\u8bad\u7ec3\u8bef\u5deevariance. \u8fd9\u91cc\u5b83\u5728\u8bad\u7ec3\u7684\u65f6\u5019\u4f7f\u75283D IoU\u635f\u5931\u3002\u5728\u53cd\u4f20\u7684\u65f6\u5019\uff0c\u6839\u636e \\hat b = argmin_bE(b,y,\\sigma) \u4ee5\u53ca \\nabla_bE(b, y, \\sigma) = 0 \u4f7f\u7528 \u9690\u51fd\u6570\u5b9a\u7406 (\u4e2a\u4eba\u6682\u65f6\u672a\u80fd\u7406\u89e3) \u5f97\u5230 \\frac{\\partial \\hat{\\mathbf{b}}}{\\partial \\mathbf{y}}=-\\left[\\frac{\\partial^{2} E}{\\partial \\hat{\\mathbf{b}}^{2}}\\right]^{-1}\\left[\\frac{\\partial^{2} E}{\\partial \\mathbf{y} \\partial \\hat{\\mathbf{b}}}\\right] \\approx-\\left[\\frac{\\partial \\mathbf{r}}{\\partial \\hat{\\mathbf{b}}}\\right]^{+}\\left[\\frac{\\partial \\mathbf{r}}{\\partial \\mathbf{y}}\\right]","title":"Monocular 3D Object Detection and Box Fitting Trained End-to-End Using Intersection-over-Union Loss"},{"location":"3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/#monocular-3d-object-detection-and-box-fitting-trained-end-to-end-using-intersection-over-union-loss","text":"\u672c\u6587\u63d0\u51fa\u4e86\u5728\u5355\u76ee3D\u68c0\u6d4b\u4e2d\u8003\u8651\u5f02\u8d28\u566a\u58f0\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2abackprop through optimization\u7684\u65b9\u6cd5\u3002","title":"Monocular 3D Object Detection and Box Fitting Trained End-to-End Using Intersection-over-Union Loss"},{"location":"3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/#_1","text":"\u9996\u5148\u4e00\u4e2aCNN\u8fdb\u884cobject detection\uff0c\u8f93\u51fa\u7c7b\u522bscore\u4ee5\u53ca2D bounding box\uff0c\u6267\u884cNMS\u5220\u9664\u90e8\u5206\u5197\u4f59\uff0c\u6700\u540e\u4f7f\u7528\u4e00\u4e2a\u975e\u7ebf\u6027\u4f18\u5316\u3002","title":"\u6d41\u7a0b\u603b\u89c8"},{"location":"3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/#_2","text":"\u7f51\u7edc\u8f93\u51fa\u7ed3\u679c\u9664\u4e86\u5206\u7c7b\u7ed3\u679c\u4e4b\u5916\u8fd8\u670926\u7ef4\u3002 2D bounding box\u5bf9\u5e94\u4e24\u4e2a\u89d2\u70b94\u4e2a\u503c\u3002 \u7269\u4f53\u4e2d\u5fc3\u4e0e\u76f8\u673a\u7684\u8ddd\u79bb(\u4e5f\u53ef\u4ee5\u7528Z\u8f74\u8ddd\u79bb\uff0c\u6027\u80fd\u5dee\u8ddd\u4e0d\u5927)\uff0c\u4e00\u4e2a\u503c \u89c2\u5bdf\u89d2 \\alpha \u9884\u6d4b\u5176sin\u4e0ecos\u503c\uff0c\u51712\u4e2a\u503c \u7269\u4f53\u5c3a\u5ea6\u957f\u5bbd\u9ad8\uff0c\u7528log\u503c\u4ee3\u8868\uff0c\u51713\u4e2a\u503c\u3002 3D bounding box\u76848\u4e2a\u70b9\u5728\u76f8\u673a\u5750\u6807\u7cfb\u7684\u6295\u5f71\uff0c\u517116\u4e2a\u503c\u3002 \u5bf93Dbox\u7684\u521d\u59cb\u89e3,\u4ee52D\u6846\u7684\u4e2d\u5fc3\u4f5c\u4e3a\u7269\u4f533D\u4e2d\u5fc3\u5728\u56fe\u7247\u7684\u6295\u5f71,\u5c31\u53ef\u4ee5\u5f97\u5230\u5bf93Dbox\u7684\u521d\u59cb\u4f30\u8ba1. taskNet\u4f7f\u5f97\u6bcf\u4e00\u4e2a\u8f93\u51fa\u4f1a\u540c\u65f6\u5e26\u4e0a\u4e86\u5bf9\u4e0d\u786e\u5b9a\u6027\u7684\u4f30\u8ba1","title":"\u7f51\u7edc\u8f93\u51fa"},{"location":"3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/#loss","text":"\u7b2c\u4e00\u79cd\u4f20\u7edf\u5b9a\u4e49\u662f\u5e73\u7a33\u566a\u58f0 Loss\u51fd\u6570\u5b9a\u4e49\u5982\u4e0b: \\begin{aligned} \\mathcal{L}_{1} &=\\mathcal{L}_{c l s} \\\\ &+\\frac{1}{N} \\sum_{j=1}^{N} \\sum_{k=1}^{26}\\left(\\frac{\\left\\|\\mathbf{y}_{k}^{(j)}-f_{k}\\left(\\mathbf{b}^{(j)}\\right)\\right\\|^{2}}{2 \\sigma_{k}^{2}}+\\log \\sigma_{k}\\right) \\end{aligned} \u672c\u8d28\u4e0a\u5c31\u662f Multi Task Learning \uff0c \u4e5f\u5c31\u662f\u8bf4\u6bcf\u4e00\u4e2aloss\u7c7b\u522b\u4f1a\u6709\u4e00\u4e2a\u5355\u72ec\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1. \u7b2c\u4e8c\u79cd\u5b9a\u4e49\u662f\u5f02\u65b9\u5dee\u566a\u58f0 \\begin{aligned} \\mathcal{L}_{2} &=\\mathcal{L}_{c l s} \\\\ &+\\frac{1}{N} \\sum_{i}\\left(\\frac{\\left(y_{i}-f_{i}\\left(\\mathbf{b}_{i}\\right)^{2}\\right)}{2 \\sigma_{i}^{2}}+\\log \\sigma_{i}-\\log P_{\\text {prior }}\\left(\\sigma_{i}^{-2}\\right)\\right) \\end{aligned} \u4e5f\u5c31\u662f\u9700\u8981tasknet\u8ddf\u968f\u6bcf\u4e00\u4e2a\u8f93\u51fa\u8f93\u51fa\u4e00\u4e2a\u8bef\u5dee\u503c \u5176\u4e2d P_{prior} \u5728\u8fd9\u91cc\u88ab\u5b9a\u4e49\u4e3a (\\alpha, \\beta) = (1, 0.5) \u7684 \u4f3d\u9a6c\u5206\u5e03 \u4ee5\u4e0a\u4e24\u79cd\u65b9\u5f0f\u5f97\u5230\u7684variance\u90fd\u53ef\u4ee5\u5728inference\u7684\u65f6\u5019\u4f5c\u4e3a\u5bf9\u4e0d\u786e\u5b9a\u6027\u7684\u4f30\u8ba1\uff0c\u4e5f\u5c31\u4fbf\u4e8e\u5bf9\u540e\u7aef\u4f18\u5316 \u7b2c\u4e09\u79cd\u8bad\u7ec3\u65b9\u5f0f\u662fbackprop through optimization \u9996\u5148\u4f18\u5316\u95ee\u9898\u7684\u5b9a\u4e49\u6765\u81ea\u4e8e E(\\mathbf{b} ; \\mathbf{y})=\\sum_{i=1}^{26} r_{i}(\\mathbf{b} ; \\mathbf{y})^{2}=\\sum_{i=1}^{26}\\left(w_{i}\\left(y_{i}-f_{i}(\\mathbf{b})\\right)\\right)^{2} \u524d\u4e24\u79cd\u65b9\u5f0f\u7684\u903b\u8f91\u5c31\u662f\u5728\u8bad\u7ec3\u7684\u65f6\u5019\u901a\u8fc7loss\u5b66\u4e60variance\uff0c\u7136\u540e\u76f4\u63a5\u7528\u4e8einference\u3002\u800c\u7b2c\u4e09\u79cd\u8bad\u7ec3\u65b9\u5f0f\u5c31\u662f\u628aoptimization\u653e\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u4e5f\u5c31\u80fd\u76f4\u63a5\u8bad\u7ec3\u8bef\u5deevariance. \u8fd9\u91cc\u5b83\u5728\u8bad\u7ec3\u7684\u65f6\u5019\u4f7f\u75283D IoU\u635f\u5931\u3002\u5728\u53cd\u4f20\u7684\u65f6\u5019\uff0c\u6839\u636e \\hat b = argmin_bE(b,y,\\sigma) \u4ee5\u53ca \\nabla_bE(b, y, \\sigma) = 0 \u4f7f\u7528 \u9690\u51fd\u6570\u5b9a\u7406 (\u4e2a\u4eba\u6682\u65f6\u672a\u80fd\u7406\u89e3) \u5f97\u5230 \\frac{\\partial \\hat{\\mathbf{b}}}{\\partial \\mathbf{y}}=-\\left[\\frac{\\partial^{2} E}{\\partial \\hat{\\mathbf{b}}^{2}}\\right]^{-1}\\left[\\frac{\\partial^{2} E}{\\partial \\mathbf{y} \\partial \\hat{\\mathbf{b}}}\\right] \\approx-\\left[\\frac{\\partial \\mathbf{r}}{\\partial \\hat{\\mathbf{b}}}\\right]^{+}\\left[\\frac{\\partial \\mathbf{r}}{\\partial \\mathbf{y}}\\right]","title":"\u4e09\u79cdLoss\u8bad\u7ec3\u65b9\u6cd5"},{"location":"3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/","text":"Multi-View 3D Detection Network for autonomous Driving \u8fd9\u7bc7\u8bba\u6587\u7ed3\u5408\u4e86\u591a\u89c6\u89d2\uff0c\u878d\u5408Lidar\u548ccamera\u8fdb\u884c\u4e09\u7ef4\u68c0\u6d4b\u3002 \u7f51\u7edc\u7ed3\u6784 \u6570\u636e\u51c6\u5907 \u6fc0\u5149\u96f7\u8fbe\u9e1f\u77b0\u56fe\uff1a \u9e1f\u77b0\u56fe\u5206\u4e3a\u9ad8\u5ea6\u56fe\u3001\u5f3a\u5ea6\u56fe\u4ee5\u53ca\u5bc6\u5ea6\u56fe\u3002\u90fd\u5148\u5c06\u9e1f\u77b0\u56fe\u5206\u62102D\u7f51\u683c \u9ad8\u5ea6\u56fe\uff1a\u5c06\u70b9\u4e91\u968f\u673a\u5206\u6210M\u4efd\uff0c\u6bcf\u4e00\u4efd\u4e2d\uff0c2D\u7f51\u683c\u4e2d\u7684\u6bcf\u4e00\u4e2a\u7f51\u683c\u53d6\u8be5\u5904\u7684\u6700\u9ad8\u70b9\u7684\u9ad8\u5ea6\uff0c\u6700\u7ec8\u5f97\u5230M-channel\u7684\u9ad8\u5ea6\u56fe \u5f3a\u5ea6\u56fe\uff1a\u4e0d\u5206\u62c6\uff0c\u6bcf\u4e00\u7f51\u683c\u53d6\u8be5\u5904\u6700\u9ad8\u70b9\u7684\u53cd\u5c04\u5f3a\u5ea6 \u5bc6\u5ea6\u56fe\uff1a\u4e0d\u5206\u62c6\uff0c\u6bcf\u4e00\u4e2a\u7f51\u683c\u53d6\u8be5\u5904\u70b9\u7684\u5bc6\u5ea6\uff0c\u7528\u4e00\u4e2alog\u51fd\u6570\u89c4\u8303\u5316 \u6fc0\u5149\u96f7\u8fbe\u524d\u89c6\u56fe\uff1a \u524d\u89c6\u56fe\u4e0d\u5206\u62c6\uff0c\u5c06\u70b9\u4e91\u6295\u5f71\u5230\u5706\u67f1\u9762\u4e0a\uff0c\u7528\u6781\u5750\u6807 (r, c) \u8868\u793a\u6295\u5f71\u56fe\u7684 (x, y) \uff0c\u5e76\u540c\u6837\u7ed9\u51fa\u9ad8\u5ea6\u56fe\uff0c\u5f3a\u5ea6\u56fe\uff0c\u5bc6\u5ea6\u56fe RGB\u56fe\u7247\uff1a\u6b63\u5e38\u4f7f\u7528 \u6570\u636e\u878d\u5408\u64cd\u4f5c \u56fe\u4e2d\u9009\u62e9\u7684M\u4e3aelemental\u2014\u2014mean\u64cd\u4f5c \u5b9e\u9a8c\u4e2d\u91cd\u70b9\u8fd8\u662f\u8981\u641e\u597dablation study","title":"Multi-View 3D Detection Network for autonomous Driving"},{"location":"3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/#multi-view-3d-detection-network-for-autonomous-driving","text":"\u8fd9\u7bc7\u8bba\u6587\u7ed3\u5408\u4e86\u591a\u89c6\u89d2\uff0c\u878d\u5408Lidar\u548ccamera\u8fdb\u884c\u4e09\u7ef4\u68c0\u6d4b\u3002","title":"Multi-View 3D Detection Network for autonomous Driving"},{"location":"3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/#_1","text":"","title":"\u7f51\u7edc\u7ed3\u6784"},{"location":"3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/#_2","text":"\u6fc0\u5149\u96f7\u8fbe\u9e1f\u77b0\u56fe\uff1a \u9e1f\u77b0\u56fe\u5206\u4e3a\u9ad8\u5ea6\u56fe\u3001\u5f3a\u5ea6\u56fe\u4ee5\u53ca\u5bc6\u5ea6\u56fe\u3002\u90fd\u5148\u5c06\u9e1f\u77b0\u56fe\u5206\u62102D\u7f51\u683c \u9ad8\u5ea6\u56fe\uff1a\u5c06\u70b9\u4e91\u968f\u673a\u5206\u6210M\u4efd\uff0c\u6bcf\u4e00\u4efd\u4e2d\uff0c2D\u7f51\u683c\u4e2d\u7684\u6bcf\u4e00\u4e2a\u7f51\u683c\u53d6\u8be5\u5904\u7684\u6700\u9ad8\u70b9\u7684\u9ad8\u5ea6\uff0c\u6700\u7ec8\u5f97\u5230M-channel\u7684\u9ad8\u5ea6\u56fe \u5f3a\u5ea6\u56fe\uff1a\u4e0d\u5206\u62c6\uff0c\u6bcf\u4e00\u7f51\u683c\u53d6\u8be5\u5904\u6700\u9ad8\u70b9\u7684\u53cd\u5c04\u5f3a\u5ea6 \u5bc6\u5ea6\u56fe\uff1a\u4e0d\u5206\u62c6\uff0c\u6bcf\u4e00\u4e2a\u7f51\u683c\u53d6\u8be5\u5904\u70b9\u7684\u5bc6\u5ea6\uff0c\u7528\u4e00\u4e2alog\u51fd\u6570\u89c4\u8303\u5316 \u6fc0\u5149\u96f7\u8fbe\u524d\u89c6\u56fe\uff1a \u524d\u89c6\u56fe\u4e0d\u5206\u62c6\uff0c\u5c06\u70b9\u4e91\u6295\u5f71\u5230\u5706\u67f1\u9762\u4e0a\uff0c\u7528\u6781\u5750\u6807 (r, c) \u8868\u793a\u6295\u5f71\u56fe\u7684 (x, y) \uff0c\u5e76\u540c\u6837\u7ed9\u51fa\u9ad8\u5ea6\u56fe\uff0c\u5f3a\u5ea6\u56fe\uff0c\u5bc6\u5ea6\u56fe RGB\u56fe\u7247\uff1a\u6b63\u5e38\u4f7f\u7528","title":"\u6570\u636e\u51c6\u5907"},{"location":"3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/#_3","text":"\u56fe\u4e2d\u9009\u62e9\u7684M\u4e3aelemental\u2014\u2014mean\u64cd\u4f5c \u5b9e\u9a8c\u4e2d\u91cd\u70b9\u8fd8\u662f\u8981\u641e\u597dablation study","title":"\u6570\u636e\u878d\u5408\u64cd\u4f5c"},{"location":"3dDetection/Orthographic_Feature_Transform_3D_detection/","text":"Orthographic Feature Transform for Monocular 3D Object Detection \u8fd9\u7bc7\u8bba\u6587\u8ba4\u4e3a\u5bf9\u5355\u4e2a\u6444\u50cf\u5934\u5f97\u5230\u7684\u6444\u50cf\u5934\u8fdb\u884c3D detection\u7684\u65f6\u5019\uff0c\u8fd8\u662f\u9700\u8981\u8003\u8651\u6574\u4e2a\u573a\u666f\u7684\u4e09\u7ef4\u7ed3\u6784\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u5728\u56fe\u50cf\u5750\u6807\u7cfb\u8fdb\u884c\u7814\u7a76\u3002 \u5728\u65e0\u4eba\u8f66\u7684\u4f20\u7edf\u89c6\u89c9\u7b97\u6cd5\u4e2d\uff0c\u5728\u5355\u76ee\u89c6\u89c9\u6761\u4ef6\u4e0b\uff0c\u5982\u679c\u8981\u6d4b\u5f97\u8ddd\u79bb\uff0c\u9700\u8981\u77e5\u9053\u6444\u50cf\u5934\u7684pitch\u89d2\u5ea6\u4ee5\u53ca\u6240\u63cf\u8ff0\u7684\u70b9\u5230\u6444\u50cf\u5934\u7684Z\u8f74\u8ddd\u79bb(\u7ecf\u5e38\u662f\u6444\u50cf\u5934\u76f8\u5bf9\u5730\u9762\u7684\u9ad8\u5ea6)\u3002\u56e0\u6b64\u8fd9\u4e2a\u76f4\u89c9\u5c31\u662f3d detection\u9700\u8981\u6444\u50cf\u5934\u7684\u5916\u53c2 \u4e3b\u8981\u7b97\u6cd5\u8d21\u732e \u795e\u7ecf\u7f51\u7edc\u7ed3\u6784\u5982\u4e0a\u56fe\u3002\u4ee5\u4e0b\u4e3a\u5177\u4f53\u4ecb\u7ecd\uff1a 1. \u524d\u7279\u5f81\u63d0\u53d6 \u8fd9\u4e2a\u7ed3\u6784\u91c7\u7528\u7684\u662fResNet18 \u4f5c\u4e3a\u524d\u7aef\u7684\u7279\u5f81\u63d0\u53d6\uff0c\u540c\u65f6\u5c06\u4e0d\u540cscale\u7684feature maps\u540c\u65f6\u4f20\u7ed9\u4e0b\u4e00\u6b65\u7684\u6b63\u4ea4\u53d8\u6362\u3002\u8fd9\u4e2a\u8f83\u4e3a\u5e38\u89c4\u3002 2. \u6b63\u4ea4\u53d8\u6362 \u8fd9\u4e2a\u662f\u672c\u6587\u7684\u7b2c\u4e00\u4e2a\u4e3b\u8981\u8d21\u732e\uff0c\u7528\u5982\u56fe\u7684\u65b9\u5f0f\u5c06\u56fe\u50cf\u5750\u6807\u7cfb\u7684feature map\u8f6c\u6362\u4e3abird eye view\u89c6\u89d2\u7684feature map \u8ba1\u7b97\u8fc7\u7a0b\uff1a 1. \u9884\u5b9a\u4e49\u4e09\u7ef4\u7684\u65b9\u5757\u7a7a\u95f4\uff0c\u5bf9\u4e8e\u6bcf\u4e00\u4e2a\u5927\u5c0f\u4e3ar\u7684\u683c\u5b50\uff0c\u6839\u636e\u5176\u4e2d\u5fc3\u70b9\u7684\u5750\u6807\u9006\u6295\u5f71\u5230\u56fe\u50cf\u7a7a\u95f4\u4e2d\uff0c\u6295\u5f71\u7ed3\u679c\u518d\u8fd1\u4f3c\u4e3a\u4e00\u4e2a\u77e9\u5f62\uff0c\u56fe\u7247\u5750\u6807\u7684\u5de6\u4e0a\u89d2\u4e0e\u53f3\u4e0a\u89d2\u5750\u6807\u7531\u4ee5\u4e0b\u516c\u5f0f\u51b3\u5b9a 2. \u5bf9\u4e8e\u70b9 (x, y, z) \u5bf9\u5e94\u7684\u7ed3\u679c g(x, y, z) \u4e3a\u9006\u6295\u5f71\u4e0a\u7684\u77e9\u5f62\u6846\u5185\u7684\u6240\u6709feature map\u7684\u6570\u503c\u5747\u503c\u3002 3. \u5bf9\u4e8e\u4fef\u77b0\u56fe(\u6ca1\u6709Z)\u4e0a\u6bcf\u4e00\u4e2a\u70b9\u7684\u8f93\u51fa\u7ed3\u679c\u4e3a\u5bf9\u5e94 (x, y) \u7ad6\u76f4\u65b9\u5411\u4e0a\u6240\u6709 g(x, y, z) \u7684\u52a0\u6743\u6c42\u548c\uff0c\u5176\u6743\u91cd\u4e3a\u53ef\u5b66\u4e60\u7684\u53c2\u6570\u3002 3. Topdown network \u7528\u4e00\u822c\u7684\u4e8c\u7ef4\u5377\u79ef\u4ee5\u53caResNet\u98ce\u683c\u7684skip-connection\u8fdb\u884c\u5904\u7406\uff0c\u5f97\u5230\u8f93\u51fa 4. output \u7f6e\u4fe1\u5ea6map(\u6982\u7387\u56fe)\uff0c\u8bad\u7ec3\u65f6\u5176\u771f\u503c\u7531\u771f\u5b9eobject\u4e3a\u4e2d\u5fc3\u7684\u9ad8\u65af\u51fd\u6570\u5f97\u5230( \\sigma \u672a\u7ed9\u51fa\uff0c\u591a\u4e2a\u969c\u788d\u7269\u7684\u8bdd\u53d6\u5404\u4e2a\u9ad8\u65af\u51fd\u6570\u7684\u6700\u5927\u503c) position offset \u548cdimension offset\uff0c\u672c\u6765\u6bcf\u4e00\u4e2a\u65b9\u5757\u7684\u7f6e\u4fe1\u5ea6map\u5df2\u7ecf\u9884\u6d4b\u4e86\u5728resolution\u4e3ar\u7684\u5730\u56fe\u4e2d\u6bcf\u4e00\u4e2a\u70b9\u662f\u969c\u788d\u7684\u6982\u7387\uff0c\u8fd9\u91cc\u518d\u8865\u5145\u4e86\u76f8\u5bf9\u4e8e\u65b9\u5757\u4e2d\u5fc3\u7684\u504f\u79fb\u9884\u6d4b\u3002 \u89d2\u5ea6vector map \u6709\u7528\u7684\u5b9e\u8df5\u7ec6\u8282 Integral Feature Map \u5728\u8fdb\u884c\u6b63\u4ea4\u53d8\u6362\u7684\u65f6\u5019\uff0c\u7531\u4e8e\u9700\u8981\u591a\u6b21\u6c42\u56fe\u50cf\u4e2d\u4e00\u4e2a\u4e2a\u77e9\u5f62\u6846\u5185\u6570\u503c\u7684\u548c\uff0c\u8fd9\u91cc\u4f7f\u7528\u52a8\u6001\u89c4\u5212\u7684\u7b97\u6cd5\uff0c\u5148\u6c42\u51faintegral feature map\uff0c F \u3002\u5176\u4e2d F(u,v) \u4e3a (u,v) \u70b9\u5de6\u4e0a\u89d2\u6574\u4e2a\u77e9\u5f62\u7684\u6570\u503c\u6c42\u548c\u3002\u4e4b\u540e\u6bcf\u4e00\u6b21\u6c42 g(x, y, z) \u65f6\u9700\u8981\u505a\u6c42\u548c\u7684\u65f6\u5019\uff0c\u53ea\u9700\u8981\u505a\u4e00\u4e2a\u5229\u7528 F \u505a\u4e00\u4e2aO(1)\u7684\u8fd0\u7b97\u5373\u53ef\u3002 Skew Loss Function \u5728\u505aobject detection\u65f6\u7531\u4e8e\u6b63\u8d1f\u6837\u672c\u7684\u95ee\u9898\uff0c\u4e00\u822c\u90fd\u9700\u8981\u5bf9\u8d1f\u6837\u672c\u7ed9\u4e88\u6bd4\u8f83\u5c11\u7684\u60e9\u7f5a\uff0c\u6587\u4e2d\u5728\u8bad\u7ec3confidence map prediction\u65f6\uff0c\u5bf9\u8bad\u7ec3\u76ee\u6807 S<0.05 \u7684\u4f4d\u7f6e\uff0c\u7ed9\u4e88\u4e00\u4e2a0.01\u7684\u56e0\u5b50\u3002 Normalizing Dimension and Angle Prediction \u5bf9\u4e8e\u4e00\u4e2a\u7279\u5b9a\u7684class\uff0c\u5df2\u77e5\u5b83\u7684\u5e73\u5747\u957f\u5bbd\u9ad8\uff0c\u5219\u9700\u8981\u9884\u6d4b\u7684\u957f\u5bbd\u9ad8\u53d8\u5316\u4e3a \\Delta_{dim}(x, z) = [log \\frac{w_i}{w},log \\frac{h_i}{h} ,log \\frac{l_i}{l}] \u89d2\u5ea6\u8f93\u51fa\u7528 sin\uff0ccos \u3002 \\Delta_{ang}(x,z) = [sin\\theta_i, cos\\theta_i] \u53e6\u5916\u672c\u6587\u7684regression\u90fd\u662f\u7528 l_1 loss. NMS(non max suppression) \u5148\u5c06\u7f6e\u4fe1\u5ea6map\u505a\u4e00\u4e2a\u9ad8\u65af\u5e73\u6ed1\uff0c\u7136\u540e\u627e\u5176\u4e2d\u5c40\u90e8\u6700\u5927\u503c\u70b9\u4f5c\u4e3a\u8f93\u51fa","title":"Orthographic Feature Transform for Monocular 3D Object Detection"},{"location":"3dDetection/Orthographic_Feature_Transform_3D_detection/#orthographic-feature-transform-for-monocular-3d-object-detection","text":"\u8fd9\u7bc7\u8bba\u6587\u8ba4\u4e3a\u5bf9\u5355\u4e2a\u6444\u50cf\u5934\u5f97\u5230\u7684\u6444\u50cf\u5934\u8fdb\u884c3D detection\u7684\u65f6\u5019\uff0c\u8fd8\u662f\u9700\u8981\u8003\u8651\u6574\u4e2a\u573a\u666f\u7684\u4e09\u7ef4\u7ed3\u6784\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u5728\u56fe\u50cf\u5750\u6807\u7cfb\u8fdb\u884c\u7814\u7a76\u3002 \u5728\u65e0\u4eba\u8f66\u7684\u4f20\u7edf\u89c6\u89c9\u7b97\u6cd5\u4e2d\uff0c\u5728\u5355\u76ee\u89c6\u89c9\u6761\u4ef6\u4e0b\uff0c\u5982\u679c\u8981\u6d4b\u5f97\u8ddd\u79bb\uff0c\u9700\u8981\u77e5\u9053\u6444\u50cf\u5934\u7684pitch\u89d2\u5ea6\u4ee5\u53ca\u6240\u63cf\u8ff0\u7684\u70b9\u5230\u6444\u50cf\u5934\u7684Z\u8f74\u8ddd\u79bb(\u7ecf\u5e38\u662f\u6444\u50cf\u5934\u76f8\u5bf9\u5730\u9762\u7684\u9ad8\u5ea6)\u3002\u56e0\u6b64\u8fd9\u4e2a\u76f4\u89c9\u5c31\u662f3d detection\u9700\u8981\u6444\u50cf\u5934\u7684\u5916\u53c2","title":"Orthographic Feature Transform for Monocular 3D Object Detection"},{"location":"3dDetection/Orthographic_Feature_Transform_3D_detection/#_1","text":"\u795e\u7ecf\u7f51\u7edc\u7ed3\u6784\u5982\u4e0a\u56fe\u3002\u4ee5\u4e0b\u4e3a\u5177\u4f53\u4ecb\u7ecd\uff1a","title":"\u4e3b\u8981\u7b97\u6cd5\u8d21\u732e"},{"location":"3dDetection/Orthographic_Feature_Transform_3D_detection/#1","text":"\u8fd9\u4e2a\u7ed3\u6784\u91c7\u7528\u7684\u662fResNet18 \u4f5c\u4e3a\u524d\u7aef\u7684\u7279\u5f81\u63d0\u53d6\uff0c\u540c\u65f6\u5c06\u4e0d\u540cscale\u7684feature maps\u540c\u65f6\u4f20\u7ed9\u4e0b\u4e00\u6b65\u7684\u6b63\u4ea4\u53d8\u6362\u3002\u8fd9\u4e2a\u8f83\u4e3a\u5e38\u89c4\u3002","title":"1. \u524d\u7279\u5f81\u63d0\u53d6"},{"location":"3dDetection/Orthographic_Feature_Transform_3D_detection/#2","text":"\u8fd9\u4e2a\u662f\u672c\u6587\u7684\u7b2c\u4e00\u4e2a\u4e3b\u8981\u8d21\u732e\uff0c\u7528\u5982\u56fe\u7684\u65b9\u5f0f\u5c06\u56fe\u50cf\u5750\u6807\u7cfb\u7684feature map\u8f6c\u6362\u4e3abird eye view\u89c6\u89d2\u7684feature map \u8ba1\u7b97\u8fc7\u7a0b\uff1a 1. \u9884\u5b9a\u4e49\u4e09\u7ef4\u7684\u65b9\u5757\u7a7a\u95f4\uff0c\u5bf9\u4e8e\u6bcf\u4e00\u4e2a\u5927\u5c0f\u4e3ar\u7684\u683c\u5b50\uff0c\u6839\u636e\u5176\u4e2d\u5fc3\u70b9\u7684\u5750\u6807\u9006\u6295\u5f71\u5230\u56fe\u50cf\u7a7a\u95f4\u4e2d\uff0c\u6295\u5f71\u7ed3\u679c\u518d\u8fd1\u4f3c\u4e3a\u4e00\u4e2a\u77e9\u5f62\uff0c\u56fe\u7247\u5750\u6807\u7684\u5de6\u4e0a\u89d2\u4e0e\u53f3\u4e0a\u89d2\u5750\u6807\u7531\u4ee5\u4e0b\u516c\u5f0f\u51b3\u5b9a 2. \u5bf9\u4e8e\u70b9 (x, y, z) \u5bf9\u5e94\u7684\u7ed3\u679c g(x, y, z) \u4e3a\u9006\u6295\u5f71\u4e0a\u7684\u77e9\u5f62\u6846\u5185\u7684\u6240\u6709feature map\u7684\u6570\u503c\u5747\u503c\u3002 3. \u5bf9\u4e8e\u4fef\u77b0\u56fe(\u6ca1\u6709Z)\u4e0a\u6bcf\u4e00\u4e2a\u70b9\u7684\u8f93\u51fa\u7ed3\u679c\u4e3a\u5bf9\u5e94 (x, y) \u7ad6\u76f4\u65b9\u5411\u4e0a\u6240\u6709 g(x, y, z) \u7684\u52a0\u6743\u6c42\u548c\uff0c\u5176\u6743\u91cd\u4e3a\u53ef\u5b66\u4e60\u7684\u53c2\u6570\u3002","title":"2. \u6b63\u4ea4\u53d8\u6362"},{"location":"3dDetection/Orthographic_Feature_Transform_3D_detection/#3-topdown-network","text":"\u7528\u4e00\u822c\u7684\u4e8c\u7ef4\u5377\u79ef\u4ee5\u53caResNet\u98ce\u683c\u7684skip-connection\u8fdb\u884c\u5904\u7406\uff0c\u5f97\u5230\u8f93\u51fa","title":"3. Topdown network"},{"location":"3dDetection/Orthographic_Feature_Transform_3D_detection/#4-output","text":"\u7f6e\u4fe1\u5ea6map(\u6982\u7387\u56fe)\uff0c\u8bad\u7ec3\u65f6\u5176\u771f\u503c\u7531\u771f\u5b9eobject\u4e3a\u4e2d\u5fc3\u7684\u9ad8\u65af\u51fd\u6570\u5f97\u5230( \\sigma \u672a\u7ed9\u51fa\uff0c\u591a\u4e2a\u969c\u788d\u7269\u7684\u8bdd\u53d6\u5404\u4e2a\u9ad8\u65af\u51fd\u6570\u7684\u6700\u5927\u503c) position offset \u548cdimension offset\uff0c\u672c\u6765\u6bcf\u4e00\u4e2a\u65b9\u5757\u7684\u7f6e\u4fe1\u5ea6map\u5df2\u7ecf\u9884\u6d4b\u4e86\u5728resolution\u4e3ar\u7684\u5730\u56fe\u4e2d\u6bcf\u4e00\u4e2a\u70b9\u662f\u969c\u788d\u7684\u6982\u7387\uff0c\u8fd9\u91cc\u518d\u8865\u5145\u4e86\u76f8\u5bf9\u4e8e\u65b9\u5757\u4e2d\u5fc3\u7684\u504f\u79fb\u9884\u6d4b\u3002 \u89d2\u5ea6vector map","title":"4. output"},{"location":"3dDetection/Orthographic_Feature_Transform_3D_detection/#_2","text":"","title":"\u6709\u7528\u7684\u5b9e\u8df5\u7ec6\u8282"},{"location":"3dDetection/Orthographic_Feature_Transform_3D_detection/#integral-feature-map","text":"\u5728\u8fdb\u884c\u6b63\u4ea4\u53d8\u6362\u7684\u65f6\u5019\uff0c\u7531\u4e8e\u9700\u8981\u591a\u6b21\u6c42\u56fe\u50cf\u4e2d\u4e00\u4e2a\u4e2a\u77e9\u5f62\u6846\u5185\u6570\u503c\u7684\u548c\uff0c\u8fd9\u91cc\u4f7f\u7528\u52a8\u6001\u89c4\u5212\u7684\u7b97\u6cd5\uff0c\u5148\u6c42\u51faintegral feature map\uff0c F \u3002\u5176\u4e2d F(u,v) \u4e3a (u,v) \u70b9\u5de6\u4e0a\u89d2\u6574\u4e2a\u77e9\u5f62\u7684\u6570\u503c\u6c42\u548c\u3002\u4e4b\u540e\u6bcf\u4e00\u6b21\u6c42 g(x, y, z) \u65f6\u9700\u8981\u505a\u6c42\u548c\u7684\u65f6\u5019\uff0c\u53ea\u9700\u8981\u505a\u4e00\u4e2a\u5229\u7528 F \u505a\u4e00\u4e2aO(1)\u7684\u8fd0\u7b97\u5373\u53ef\u3002","title":"Integral Feature Map"},{"location":"3dDetection/Orthographic_Feature_Transform_3D_detection/#skew-loss-function","text":"\u5728\u505aobject detection\u65f6\u7531\u4e8e\u6b63\u8d1f\u6837\u672c\u7684\u95ee\u9898\uff0c\u4e00\u822c\u90fd\u9700\u8981\u5bf9\u8d1f\u6837\u672c\u7ed9\u4e88\u6bd4\u8f83\u5c11\u7684\u60e9\u7f5a\uff0c\u6587\u4e2d\u5728\u8bad\u7ec3confidence map prediction\u65f6\uff0c\u5bf9\u8bad\u7ec3\u76ee\u6807 S<0.05 \u7684\u4f4d\u7f6e\uff0c\u7ed9\u4e88\u4e00\u4e2a0.01\u7684\u56e0\u5b50\u3002","title":"Skew Loss Function"},{"location":"3dDetection/Orthographic_Feature_Transform_3D_detection/#normalizing-dimension-and-angle-prediction","text":"\u5bf9\u4e8e\u4e00\u4e2a\u7279\u5b9a\u7684class\uff0c\u5df2\u77e5\u5b83\u7684\u5e73\u5747\u957f\u5bbd\u9ad8\uff0c\u5219\u9700\u8981\u9884\u6d4b\u7684\u957f\u5bbd\u9ad8\u53d8\u5316\u4e3a \\Delta_{dim}(x, z) = [log \\frac{w_i}{w},log \\frac{h_i}{h} ,log \\frac{l_i}{l}] \u89d2\u5ea6\u8f93\u51fa\u7528 sin\uff0ccos \u3002 \\Delta_{ang}(x,z) = [sin\\theta_i, cos\\theta_i] \u53e6\u5916\u672c\u6587\u7684regression\u90fd\u662f\u7528 l_1 loss.","title":"Normalizing Dimension and Angle Prediction"},{"location":"3dDetection/Orthographic_Feature_Transform_3D_detection/#nmsnon-max-suppression","text":"\u5148\u5c06\u7f6e\u4fe1\u5ea6map\u505a\u4e00\u4e2a\u9ad8\u65af\u5e73\u6ed1\uff0c\u7136\u540e\u627e\u5176\u4e2d\u5c40\u90e8\u6700\u5927\u503c\u70b9\u4f5c\u4e3a\u8f93\u51fa","title":"NMS(non max suppression)"},{"location":"3dDetection/Pseudo-Lidar/","text":"Monocular 3D Object Detection with Pseudo-LiDAR Point Cloud \u8fd9\u7bc7\u8bba\u6587\u6709\u591a\u4e2a\u91cd\u8981\u8d21\u732e\uff0c\u4e00\u662f\u4f7f\u7528\u5355\u76ee\u9884\u6d4b\u6df1\u5ea6\u56fe\uff0c\u5f62\u6210\u5047Lidar\u6570\u636e\uff0c\u5e76\u5f97\u5230\u4eba\u5de5\u7535\u4e91\uff0c\u7136\u540e\u4f7f\u7528 Frustum Pointnet \u3002\u5bf9\u5f97\u5230\u4e00\u4e2a\u53ef\u9760\u7684\u521d\u59cb\u89e3\u3002 \u7b2c\u4e8c\u7531\u4e8e\u5047Lidar\u6709\u5f88\u591a\u566a\u70b9\uff0c\u566a\u97f3\u4f53\u73b0\u5728\u4e24\u4e2a\u65b9\u9762\uff0c\u4e00\u4e2a\u662f\u6709\u504f\u79fb\uff0c\u5f71\u54cd\u5bf9\u8ddd\u79bb\u7684\u4f30\u8ba1\uff0c\u4e00\u4e2a\u662f\u6709\u957f\u5c3e\u2014\u2014\u7269\u4f53\u8fb9\u7f18\u7684\u4e00\u4e9b\u70b9\u4e91\u4f1a\u62c9\u5f97\u5f88\u957f\uff0c\u539f\u56e0\u662f\u7269\u4f53\u8fb9\u7f18\u5904\u7684\u6df1\u5ea6\u4f30\u8ba1\u4e0d\u51c6\u786e\u3002 \u4e3a\u4e86\u7f13\u89e3\u7b2c\u4e00\u4e2a\u95ee\u9898\uff0c\u4f7f\u75282D-3D box\u7ea6\u675f\u8c03\u65743D box\u7684\u4f4d\u7f6e\uff0c\u8fd9\u91cc\u5f15\u5165\u4e86\u4e00\u4e2aloss\u51fd\u6570\u5728training\u8fc7\u7a0b\u4e2d\u5904\u7406\u8fd9\u4e2a\u95ee\u9898\uff0c\u5728inference\u7684\u65f6\u5019\u5c06\u95ee\u9898\u8f6c\u4e3a\u4f18\u5316\u518d\u63d0\u9ad8\u6027\u80fd\u3002 \u4e3a\u4e86\u7f13\u89e3\u7b2c\u4e8c\u4e2a\u95ee\u9898\uff0c\u4f7f\u7528instance mask\u6765\u4ee3\u88682D proposal\u800c\u4e0d\u662fbounding box\u3002\u76f8\u5f53\u4e8e\u7528segmentation\u7684\u50cf\u7d20\u70b9\u7ea7\u7684\u7ed3\u679c\u8f93\u51fa 2D 3D\u7ea6\u675f \u5c063D\u6846\u8f6c\u6362\u4e3a8\u4e2a\u5750\u6807\u70b9\uff0c\u7136\u540e\u8f6c\u6362\u4e3a\u56fe\u7247\u5750\u6807\uff0c\u6c42\u51fa\u6700\u5c0fbounding rectangle\u5bf9\u5e94\u7684\u56db\u4e2a\u5750\u6807\u3002BBCL\u5c31\u662f\u8fd9\u56db\u4e2a\u5750\u6807\u4e0e2Dbounding box\u7684L1\u8ddd\u79bb\u3002BBCO\u5219\u662f\u5728inference\u7684\u65f6\u5019\u4f7f\u7528global search\u7684\u65b9\u5f0f(\u591a\u534a\u662f\u6a21\u62df\u9000\u706b)\u6709\u8d27\u5bf9\u5e94\u7684L1\u8ddd\u79bb\u3002 \u8fd9\u662f\u4e00\u5e74\u524d\u7684SOTA","title":"Monocular 3D Object Detection with Pseudo-LiDAR Point Cloud"},{"location":"3dDetection/Pseudo-Lidar/#monocular-3d-object-detection-with-pseudo-lidar-point-cloud","text":"\u8fd9\u7bc7\u8bba\u6587\u6709\u591a\u4e2a\u91cd\u8981\u8d21\u732e\uff0c\u4e00\u662f\u4f7f\u7528\u5355\u76ee\u9884\u6d4b\u6df1\u5ea6\u56fe\uff0c\u5f62\u6210\u5047Lidar\u6570\u636e\uff0c\u5e76\u5f97\u5230\u4eba\u5de5\u7535\u4e91\uff0c\u7136\u540e\u4f7f\u7528 Frustum Pointnet \u3002\u5bf9\u5f97\u5230\u4e00\u4e2a\u53ef\u9760\u7684\u521d\u59cb\u89e3\u3002 \u7b2c\u4e8c\u7531\u4e8e\u5047Lidar\u6709\u5f88\u591a\u566a\u70b9\uff0c\u566a\u97f3\u4f53\u73b0\u5728\u4e24\u4e2a\u65b9\u9762\uff0c\u4e00\u4e2a\u662f\u6709\u504f\u79fb\uff0c\u5f71\u54cd\u5bf9\u8ddd\u79bb\u7684\u4f30\u8ba1\uff0c\u4e00\u4e2a\u662f\u6709\u957f\u5c3e\u2014\u2014\u7269\u4f53\u8fb9\u7f18\u7684\u4e00\u4e9b\u70b9\u4e91\u4f1a\u62c9\u5f97\u5f88\u957f\uff0c\u539f\u56e0\u662f\u7269\u4f53\u8fb9\u7f18\u5904\u7684\u6df1\u5ea6\u4f30\u8ba1\u4e0d\u51c6\u786e\u3002 \u4e3a\u4e86\u7f13\u89e3\u7b2c\u4e00\u4e2a\u95ee\u9898\uff0c\u4f7f\u75282D-3D box\u7ea6\u675f\u8c03\u65743D box\u7684\u4f4d\u7f6e\uff0c\u8fd9\u91cc\u5f15\u5165\u4e86\u4e00\u4e2aloss\u51fd\u6570\u5728training\u8fc7\u7a0b\u4e2d\u5904\u7406\u8fd9\u4e2a\u95ee\u9898\uff0c\u5728inference\u7684\u65f6\u5019\u5c06\u95ee\u9898\u8f6c\u4e3a\u4f18\u5316\u518d\u63d0\u9ad8\u6027\u80fd\u3002 \u4e3a\u4e86\u7f13\u89e3\u7b2c\u4e8c\u4e2a\u95ee\u9898\uff0c\u4f7f\u7528instance mask\u6765\u4ee3\u88682D proposal\u800c\u4e0d\u662fbounding box\u3002\u76f8\u5f53\u4e8e\u7528segmentation\u7684\u50cf\u7d20\u70b9\u7ea7\u7684\u7ed3\u679c\u8f93\u51fa","title":"Monocular 3D Object Detection with Pseudo-LiDAR Point Cloud"},{"location":"3dDetection/Pseudo-Lidar/#2d-3d","text":"\u5c063D\u6846\u8f6c\u6362\u4e3a8\u4e2a\u5750\u6807\u70b9\uff0c\u7136\u540e\u8f6c\u6362\u4e3a\u56fe\u7247\u5750\u6807\uff0c\u6c42\u51fa\u6700\u5c0fbounding rectangle\u5bf9\u5e94\u7684\u56db\u4e2a\u5750\u6807\u3002BBCL\u5c31\u662f\u8fd9\u56db\u4e2a\u5750\u6807\u4e0e2Dbounding box\u7684L1\u8ddd\u79bb\u3002BBCO\u5219\u662f\u5728inference\u7684\u65f6\u5019\u4f7f\u7528global search\u7684\u65b9\u5f0f(\u591a\u534a\u662f\u6a21\u62df\u9000\u706b)\u6709\u8d27\u5bf9\u5e94\u7684L1\u8ddd\u79bb\u3002 \u8fd9\u662f\u4e00\u5e74\u524d\u7684SOTA","title":"2D 3D\u7ea6\u675f"},{"location":"3dDetection/RecentCollectionForMono3D/","text":"Recent Collections for Mono 3D detection \u5728IROS2020\u6295\u7a3f\u524d\u540e\u79ef\u6512\u4e86\u4e00\u7cfb\u5217\u5355\u76ee3D\u68c0\u6d4bpaper\u7684\u9605\u8bfb\u3002\u8fd9\u91cc\u4e00\u6b21\u8fc7\u8fdb\u884c\u8bb0\u5f55,\u5f00\u6e90\u5728\u524d\uff0c\u672a\u5f00\u6e90\u5728\u540e. \u8fd9\u91cc\u5217\u51fa\u76ee\u524d\u6709\u6587\u7ae0\u53ef\u5bfb\u7684KITTI\u6392\u884c\u699c(2022.07.23) Update(2020.04.02):Update scores for YOLOMono3D Update(2020.07.24): Update Kinematic 3D Update(2020.09.08): Update SSL-RTM3D Update(2021.02.01): Update Ground Aware 3D Update(2021.03.17): Update CaDDN Update(2021.04.10): Update MonoFlex, MonoRun, MonoRCNN Update(2021.06.26): Update MonoEF Methods Moderate Easy Hard Time LPCG 17.80 25.56 15.38 0.03 PS-fld 17.74 23.74 15.14 0.25 dd3d 16.34 23.22 14.20 - DID-M3D 16.29 22.50 13.95 0.02 MonoDETR 16.26 24.52 13.93 0.04 MonoDTR 15.39 21.99 12.73 0.04 GUPNet 15.02 22.26 13.12 - DEVIANT 14.46 21.88 11.89 0.04 DLE 14.33 24.23 10.30 0.06 AutoShape 14.17 22.47 11.36 0.05 MonoFlex 13.89 19.94 12.07 0.03 MonoEF 13.87 21.29 11.74 0.03 CaDDN 13.41 19.17 11.46 0.64 GroundAware3D 13.17 21.60 9.94 0.05 Aug3DRPN 12.99 17.82 9.78 0.08 Kinematic3D 12.72 19.07 9.17 0.12 MonoRCNN 12.65 18.36 10.03 0.07 MonoRun 12.30 19.65 10.58 0.07 YOLOMono3D 12.06 18.28 8.42 0.05 D4LCN 11.72 16.65 9.51 0.2 M3DSSD 11.46 17.51 8.98 * SSL-RTM3D 11.45 16.73 9.92 0.04 Refined-MPL 11.14 18.09 8.94 0.15 AM3D 10.74 16.50 9.52 0.4 RTM3D 10.34 14.41 8.77 0.05 MonoPair 9.99 13.04 8.65 0.06 SMOKE 9.76 14.03 7.84 0.03 M3D-RPN 9.71 14.76 7.42 0.16 \u76ee\u5f55: Recent Collections for Mono 3D detection LPCG Pseudo-Stereo DiD-M3D MonoDETR MonoDTR GUPNet MonoFlex MonoEF MonoRCNN Aug3DRPN D4LCN M3DSSD RTM3D MonoPair SMOKE YOLOMono3D LPCG pdf code \u8fd9\u7bc7paper\u5e26\u6765\u4e86\u5f88\u5f3a\u7684\u5b9e\u7528\u6027\u3002\u5229\u7528\u96f7\u8fbe\uff0c\u5229\u7528\u81ea\u76d1\u7763\uff0c\u5229\u7528\u4e0a\u5927\u91cfkitti raw\u4e2d\u6ca1\u6709\u88ab\u6807\u8bb0\u7684\u6570\u636e\u3002 Step 1\uff0c \u8bad\u7ec3\u4e00\u4e2a\u96f7\u8fbe\u7684detector, \u5e76\u4e14\u51c6\u5907\u4e00\u4e2a\u57fa\u4e8e\u96f7\u8fbe\u7684 unsupervised detector. Step 2. \u8bad\u7ec3\u56fe\u7247\u7684\u65f6\u5019\uff0c\u5229\u7528\u96f7\u8fbe\u7684detector\u4ee5\u53caunsupervised detector, \u5728\u6ca1\u6709\u88ab\u6807\u8bb0\u7684\u6570\u636e\u4e0a\u751f\u6210 pseudo label. \u6269\u5927\u8bad\u7ec3\u6570\u636e\u91cf\u3002\u5f97\u5230\u5f88\u597d\u7684\u6027\u80fd\u3002 Pseudo-Stereo pdf code \u8fd9\u7bc7paper\u505a\u4e86\u5f88\u795e\u5947\u7684\u64cd\u4f5c\uff0c\u5c31\u662f\u91c7\u7528\u5355\u76ee\u4f2a\u88c5\u53cc\u76ee\uff0c\u7136\u540e\u4f7f\u7528\u53cc\u76ee\u7684detector ( LIGA , YOLOStereo3D )\u51fa\u7ed3\u679c\uff0c\u5f97\u5230\u76f8\u5f53\u4e0d\u9519\u7684\u6027\u80fd\u3002 \u4f5c\u8005\u5c1d\u8bd5\u4e86\u51e0\u79cd\u65b9\u6848\uff0c \u56fe\u50cf\u7ea7\u522b\u7684\u751f\u6210\uff1a\u5355\u76ee\u9884\u6d4b\u4e00\u4e2adisparity map,\u5b8c\u5168\u76f4\u63a5\u6784\u5efa\u53cc\u76ee\u56fe\u50cf\u3002 \u6027\u80fd\u4e00\u822c \u7279\u5f81\u7ea7\u522b\u7684\u751f\u6210\uff1a\u4f7f\u7528\u4e0a\u8ff0\u7684Disparity map\u76f4\u63a5\u6784\u5efa\u4e00\u4e2a\u7279\u5f81\u56fe\uff0c\u548c\u53f3\u56fe\u7684\u7279\u5f81\u56fe\u8f93\u5165\u53cc\u76ee\u5339\u914d\u90e8\u5206\u3002\u6548\u679c\u6700\u597d \u514b\u9686\uff0c\u76f4\u63a5\u590d\u5236\u539f\u6765\u7684\u5355\u76ee\u7279\u5f81\uff0c\u8fdb\u5165\u53cc\u76ee\u5339\u914d\u90e8\u5206\u3002\u6548\u679c\u8fd8\u4e0d\u9519 DiD-M3D pdf code \u8fd9\u7bc7paper\u6574\u4f53\u65b9\u6848\u4e0a\u4e0e GUPNet \u662f\u4e00\u81f4\u7684\uff0c\u662f\u4e00\u4e2a\u4e8c\u9636\u6bb5\u7684\u7b97\u6cd5\u3002\u5b83\u7b2c\u4e00\u4e2a\u5f15\u5165\u5e76\u5f3a\u8c03\u4e86 visual depth / attribute depth\u7684\u6982\u5ff5\u3002 visual depth\u4e5f\u5c31\u662f\u7269\u4f53\u8868\u9762\u7684\u6df1\u5ea6\uff0c\u53ef\u4ee5\u7406\u89e3\u4e3a\u8be5\u50cf\u7d20\u5b9e\u9645\u7269\u4f53\u70b9\u7684\u6df1\u5ea6\uff0cattribute depth\u5219\u662f\u8be5\u70b9\u5230\u7269\u4f53\u4e2d\u5fc3\u7684\u6df1\u5ea6\u8ddd\u79bb\u3002\u4f5c\u8005\u5f3a\u8c03\u8ba4\u4e3a\uff0c\u6211\u4eec\u4e0d\u5e94\u8be5\u76f4\u63a5\u9884\u6d4b\u4e2d\u5fc3\u7684\u6df1\u5ea6\u8ddd\u79bb\uff0c\u800c\u5e94\u8be5\u628a\u8fd9\u4e24\u4e2a\u6df1\u5ea6\u89e3\u8026\u3002\u5e76\u4e14\u8fd9\u4e2a\u7b97\u6cd5\u73b0\u5728\u53ef\u4ee5\u5bf9\u5185\u53c2\u5b8c\u5168\u4e0d\u654f\u611f\u4e86\u3002 Visual Depth \u662f\u7269\u4f53\u50cf\u7d20\u5230\u76f8\u673a\u7684\u8ddd\u79bb\uff0c\u5982\u679c\u6211\u4eec\u628a\u56fe\u7247\u88c1\u526a/\u653e\u5927\uff0c\u90a3\u4e48\u5bf9\u5e94\u7684\uff0c\u6211\u4eec\u9884\u6d4b\u7684\u6df1\u5ea6\u50cf\u7d20\u5e94\u8be5\u4f1a\u968f\u7740\u7126\u8ddd\u6539\u53d8\uff0c\u8fd9\u90e8\u5206\u7684\u6df1\u5ea6\u662faffine-sensitive\u7684\u3002 attribute depth\u662f\u7269\u4f53\u8868\u9762\u5230\u4e09\u7ef4\u76ee\u6807\u4e2d\u5fc3\u70b9\u7684\u8ddd\u79bb\uff0c\u8fd9\u90e8\u5206\u6df1\u5ea6\u662f\u7531\u8f66\u5b50\u7684\u59ff\u6001\u548c\u8f66\u5b50\u7684\u5927\u5c0f\u51b3\u5b9a\u7684\uff0c\u56e0\u800c\u53ef\u4ee5\u8ba4\u4e3a\u8fd9\u90e8\u5206\u6df1\u5ea6\u662f affine-insensitive, \u4e0e\u6211\u4eec\u653e\u5927\u7f29\u5c0f\u56fe\u7247\u65e0\u5173\u3002 GT\u548c MultiSensor Refinement \u90a3\u7bc7\u7684\u505a\u6cd5\u6bd4\u8f83\u63a5\u8fd1\uff0c\u5229\u75283D\u6846\u5185\u7684\u70b9\u4e91\u70b9\u6784\u9020\u4e00\u4e2a\u7a00\u758f\u7684visual depth label.\u7136\u540e\u7528gt \u51cf\u53bb\u5b83\u5c31\u53ef\u4ee5\u5f97\u5230 attribute loss. \u540e\u7eed\u505a\u6570\u636e\u589e\u5f3a\u7684\u65f6\u5019 visual depth\u968f\u7740scale\u6539\u53d8\uff0c\u4f46\u662fattribute loss\u5219\u4e0d\u6539\u53d8\u3002 \u4f7f\u7528L1 Loss, \u90fd\u9884\u6d4bLaplacian uncertainty, \u878d\u5408\u4e5f\u4e00\u6837\u3002 MonoDETR pdf code \u8fd9\u7bc7paper\u5219\u5b8c\u5168\u5f15\u5165\u4e86DETR, \u4f3c\u4e4e\u662f\u7b2c\u4e00\u7bc7\u628aDETR\u7684\u8f93\u51fa\u5934\u5b8c\u5168\u5730\u5f15\u5165Mono3D\u8fd9\u4e2a\u4efb\u52a1\u3002 \u7279\u5f81\u4e0a\u5206\u5f00visual encoder\u548c\u6709\u72ec\u7acb\u6df1\u5ea6\u4fe1\u606f\u8865\u5145\u7684depth encoder; depth predictor\u9884\u6d4b\u4e00\u4e2a\u591a\u5206\u7c7b\u6df1\u5ea6\u4f30\u8ba1\uff0c\u7531object depth\u8fdb\u884c\u76d1\u7763\uff0c2d\u6846\u5185\u7684\u6240\u6709\u70b9\u6df1\u5ea6gt\u4e3a\u7269\u4f53\u7684\u6df1\u5ea6\uff0c\u76f8\u4ea4\u90e8\u5206\u53d6\u6df1\u5ea6\u6700\u5c0f\u503c\uff0c\u6846\u5916\u5206\u7c7b\u4e3a\u72ec\u7acb\u7684\u80cc\u666f\u7c7b\u3002 \u5728depth cross-attention\u4e2d\uff0c\u5229\u7528\u6df1\u5ea6\u7684\u9884\u6d4b\u503c\u6784\u5efa\u6df1\u5ea6positional embedding\uff1b\u628a\u5bf9\u5e94\u5750\u6807\u7684\u6df1\u5ea6\u9884\u6d4b\u5206\u5e03\uff0c\u5f15\u5165\u4f5c\u4e3apositional embedding. DETR\u6df1\u5ea6\u7684\u6700\u7ec8\u8f93\u51fa\uff0c\u7531 d_{geo}=f\\frac{h_{3D}}{t+b} \uff0c( t,b \u5206\u522b\u662f\u4e0a\u4e0b\u8fb9\u52303D\u6295\u5f71\u4e2d\u5fc3\u7684\u8ddd\u79bb\uff0c\u7f51\u7edc\u4e5f\u4ee5\u6b64\u4f5c\u4e3aprediction attribute) \uff0c d_reg \u76f4\u63a5\u9884\u6d4b\u4ee5\u53ca d_{map}(x_{3D}, y_{3D}) \u9884\u6d4b\u7684\u6df1\u5ea6\u56fe\u5728\u5bf9\u5e94\u70b9\u7684\u63d2\u503c\u7ec4\u6210\u3002 MonoDTR pdf code \u6574\u4f53\u6765\u8bf4\u8fd9\u7bc7paper\u91c7\u7528\u4e86\u5377\u79efbackbone\u52a0transformer head\u7684\u6574\u4f53\u65b9\u6848\u6765\u6784\u9020\u7f51\u7edc\uff0c\u8f93\u51fa\u7684\u5f62\u6001\u4e0e YOLOMono3D \u662f\u4e00\u6837\u7684\u3002 \u4e5f\u901a\u8fc7DFE\u5f15\u5165\u4e86\u6df1\u5ea6\u76d1\u7763\u3002 GUPNet pdf code \u8fd9\u7bc7paper\u662f\u76ee\u524d\u5c11\u6709\u7684\u4f7f\u7528\u4e8c\u9636\u6bb5\u68c0\u6d4b\u7f51\u7edc\u5b9e\u73b0\u9ad8\u6027\u80fd\u7684\u7ed3\u679c.\u4f7f\u7528centerNet\u5b8c\u62102D\u68c0\u6d4b\uff0c\u7136\u540e\u4f7f\u7528RoIAlign\u63d0\u53d6\u7279\u5f81.3D\u7684\u8f93\u51fa\u4e0e MonoFlex \u6709\u70b9\u76f8\u4f3c\uff0c\u8f93\u51fa\u7684\u662f\u5e26\u6709\u4e0d\u786e\u5b9a\u7684\u6df1\u5ea6\uff0c\u91c7\u7528\u7684\u662f2D\u7684\u7b80\u8981\u63a8\u7406\u52a0\u4e0a\u7f51\u7edc\u7684\u4e00\u4e2a\u5b66\u4e60\u503c: \\begin{aligned} d_{p} &=\\frac{f \\cdot h_{3 d}}{h_{2 d}}=\\frac{f \\cdot\\left(\\lambda_{h} \\cdot X+\\mu_{h}\\right)}{h_{2 d}} \\\\ &=\\frac{f \\cdot \\lambda_{h}}{h_{2 d}} \\cdot X+\\frac{f \\cdot \\mu_{h}}{h_{2 d}} \\end{aligned} \\begin{array}{c} d=L a\\left(\\mu_{p}, \\sigma_{p}\\right)+L a\\left(\\mu_{b}, \\sigma_{b}\\right) \\\\ \\mu_{d}=\\mu_{p}+\\mu_{b}, \\quad \\sigma_{d}=\\sqrt{\\left(\\sigma_{p}\\right)^{2}+\\left(\\sigma_{b}\\right)^{2}} \\end{array} \\mathcal{L}_{\\text {depth }}=\\frac{\\sqrt{2}}{\\sigma_{d}}\\left|\\mu_{d}-d^{g t}\\right|+\\log \\left(\\sigma_{d}\\right) \u672c\u6587\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e2ahierarchical task learning.\u6307\u4f18\u5148\u628a2D\u68c0\u6d4b\u8bad\u7ec3\u597d\uff0c\u518d\u628aWHL\u8bad\u7ec3\u597d\uff0c\u6700\u540e\u518d\u8bad\u7ec3depth\u3002\"\u8bad\u7ec3\u597d\"\u7684\u8bc4\u5224\u6807\u51c6\u662fepochs\u4e4b\u95f4\u7684mean loss\u76f8\u5dee\u4e0d\u5927\u3002 self.loss_graph = {'seg_loss':[], 'size2d_loss':[], 'offset2d_loss':[], 'offset3d_loss':['size2d_loss','offset2d_loss'], 'size3d_loss':['size2d_loss','offset2d_loss'], 'heading_loss':['size2d_loss','offset2d_loss'], 'depth_loss':['size2d_loss','size3d_loss','offset2d_loss']} def compute_weight(self,current_loss,epoch): T=140 #compute initial weights loss_weights = {} eval_loss_input = torch.cat([_.unsqueeze(0) for _ in current_loss.values()]).unsqueeze(0) for term in self.loss_graph: if len(self.loss_graph[term])==0: loss_weights[term] = torch.tensor(1.0).to(current_loss[term].device) else: loss_weights[term] = torch.tensor(0.0).to(current_loss[term].device) #update losses list if len(self.past_losses)==self.stat_epoch_nums: past_loss = torch.cat(self.past_losses) mean_diff = (past_loss[:-2]-past_loss[2:]).mean(0) if not hasattr(self, 'init_diff'): self.init_diff = mean_diff c_weights = 1-(mean_diff/self.init_diff).relu().unsqueeze(0) time_value = min(((epoch-5)/(T-5)),1.0) for current_topic in self.loss_graph: if len(self.loss_graph[current_topic])!=0: control_weight = 1.0 for pre_topic in self.loss_graph[current_topic]: control_weight *= c_weights[0][self.term2index[pre_topic]] loss_weights[current_topic] = time_value**(1-control_weight) #pop first list self.past_losses.pop(0) self.past_losses.append(eval_loss_input) return loss_weights MonoFlex pdf code \u8fd9\u7bc7paper\u5728\u8ba1\u7b97\u6df1\u5ea6\u7684\u65f6\u5019\u548cMonoRCNN\u4e0d\u8c0b\u800c\u5408\uff0c\u89c9\u5f97\u5c3d\u7ba1\u540c\u65f6\u627e\u51fa\u591a\u4e2a\u89d2\u70b9\u5f88\u6709\u8bf1\u60d1\u529b\uff0c\u4f46\u662f\u4e00\u70b9\u70b9\u7684\u8bef\u5dee\u5c31\u4f1a\u9020\u62103D\u56de\u5f52\u5f88\u5927\u7684\u9519\u8bef\uff0c\u6240\u4ee5\u4e3b\u8981\u4f9d\u9760\u8ba1\u7b97\u89d2\u70b9\u5728\u56fe\u50cf\u4e0a\u7684\u9ad8\u5ea6\u6765\u63a8\u7406\u6df1\u5ea6\u3002 MonoEF pdf code \u8fd9\u7bc7paper\u5173\u6ce8\u5916\u53c2\u7684\u53d8\u52a8\u4ee5\u53ca\u5916\u53c2\u53d8\u52a8\u6700\u76f8\u673a\u611f\u77e5\u6548\u679c\u7684\u5f71\u54cd\u3002\u540c\u65f6\u4f1a\u4e3b\u52a8\u5730\u901a\u8fc7\u5e8f\u5217\u56fe\u7247\u9884\u5224\u8f66\u8f86\u7684\u8fd0\u52a8. MonoRCNN pdf code \u8fd9\u7bc7paper\u4e0eMonoFlex\u6709\u76f8\u4f3c\u7684\u601d\u8def\uff0c\u90fd\u662f\u7528projected visual height\u6765\u8ba1\u7b97\u6df1\u5ea6\u3002\u8fd9\u7bc7\u6587\u7ae0\u5219\u662f\u57fa\u4e8etwo-stage\u7684\u7b97\u6cd5\u3002\u6587\u7ae0\u76f4\u63a5\u6307\u51fa\u4e86keypoints\u7684\u95ee\u9898\u3002 Aug3DRPN pdf \u8fd9\u7bc7paper\u57fa\u4e8e M3D-RPN \u7684\u505a\u6cd5 \u989d\u5916\u7684\u6df1\u5ea6\u9884\u6d4b \u5229\u7528\u6df1\u5ea6\u9884\u6d4b\uff0c\u6e32\u67d3\u865a\u62df\u76f8\u673a\u7ed3\u679c,\u589e\u5f3a\u8bad\u7ec3\u7ed3\u679c. D4LCN pdf code \u8fd9\u7bc7paper\u5b8c\u5168\u7ee7\u627f\u4e86 M3D-RPN \u7684\u8863\u94b5\uff0c\u5b83\u4e0d\u540c\u7684\u5730\u65b9\u5728\u4e8e\uff0c\u6452\u5f03\u4e86M3D-RPN\u5904\u7406\u7f13\u6162\u7684height-wise convolution,\u800c\u662f\u4f7f\u7528\u5355\u76ee\u4f30\u8ba1\u6df1\u5ea6\uff0c\u7136\u540e\u4f7f\u7528\u6df1\u5ea6\u4f5c\u4e3a\u5377\u79ef\u6838\u7684guide, \u8fd9\u4e2aguide\u7c7b\u4f3c\u4e8e\u8fd9\u51e0\u7bc7\u6587\u7ae0\u7684\u64cd\u4f5c: guidenet ; DFN M3DSSD pdf code \u8fd9\u7bc7paper\u91c7\u7528\u4e86\u5927backbone,\u7528deformable convolution\u7684\u65b9\u6cd5\uff0c\u91cd\u70b9\u89e3\u51b32D detection\u4e2d\u611f\u53d7\u91ce\u4e0eanchor\u4e0d\u5339\u914d\u95ee\u9898\u4ee5\u53ca3D\u4e2d\u5fc3\u4e0e2D\u4e2d\u5fc3\u4e0d\u5339\u914d\u7684\u95ee\u9898. Shape alignment \u4e0e center alignment \u5bf9\u5e94\u57fa\u4e8e2D\u4e0e3D\u7684\u4e24\u79cddeformable base shift\u7684\u4ea7\u751f\u65b9\u5f0f\u3002 \u53e6\u5916\u672c\u6587\u53c8\u63d0\u51fa\u4e86 Asymmetric Non-Local Attention Block (ANAB)\u6a21\u5757, \u7528\u4e8e\u589e\u5f3a\u611f\u53d7\u91ce. RTM3D pdf code \u8fd9\u7bc7\u6587\u7ae0\u8fd8\u6ca1\u6709\u6b63\u5f0f\u5f00\u6e90\uff0c\u4f46\u662fgithub\u5c31\u5148\u5f00\u7740\u4e86\u3002\u8fd9\u7bc7\u6587\u7ae0\u5728\u6280\u672f\u4e0a\u6709\u4e00\u5b9a\u7684\u65b0\u610f\uff0c\u5b83\u4f7f\u7528 CenterNet \u7684\u67b6\u6784\u4f30\u8ba1\u5927\u91cf\u7684keypoints\u4ee5\u53ca\u5197\u4f59\u76843D\u4fe1\u606f\uff0c\u6700\u540e\u901a\u8fc7\u6700\u4f18\u5316\u878d\u5408\u3002\u4f7f\u7528\u5927\u91cf\u5197\u4f59\u4fe1\u606f\u5b83\u4e0d\u662f\u7b2c\u4e00\u4e2a,\u524d\u8005\u6bd4\u5982\u6709 SS3D ,\u4f46\u662f\u5b83\u7ed5\u8fc7\u4e86anchor\u4f7f\u7528CenterNet\u6709\u4e00\u5b9a\u7684\u65b0\u610f\u3002 MonoPair pdf \u8fd9\u7bc7\u6587\u7ae0\u662f\u5b9e\u9a8c\u5ba4\u5927\u5e08\u5144\u90b0\u78ca\u5728\u963f\u91cc\u7684CVPR2020 paper\u3002\u6709 \u5b98\u65b9\u7f51\u7ad9 \u6587\u7ae0\u7684\u6838\u5fc3\u521b\u65b0\u662f\u7b2c\u4e00\u4e2a\u4f7f\u7528\u573a\u666f\u4e2d\u4e0d\u540c\u7269\u4f53\u4e4b\u95f4\u7684\u76f8\u4e92\u7ea6\u675f\u8fdb\u884c\u4f18\u5316\u7684paper\u3002 SMOKE pdf \u8fd9\u7bc7paper\u7684\u521b\u65b0\u70b9\u4e0d\u7b97\u7279\u522b\u591a\u3002 1. \u4f7f\u7528\u4e86 CenterNet \u7684\u67b6\u6784\u8fdb\u884c\u4e2d\u5fc3\u70b9\u7684\u4f30\u8ba1\u3002 2. \u4f7f\u7528\u4e86distangling loss, \u8fd9\u4e2a\u6765\u81ea\u4e8e MonoDIS 3. \u6570\u636e\u589e\u5f3a\u4e0a\u4f7f\u7528\u4e86shifting\u7b49\u65b9\u6cd5\uff0c\u4f46\u662f\u53ea\u662f\u7528\u6765train keypoint\u70ed\u56fe\u7b49\u7ed3\u6784\u3002\u5c5e\u4e8especialized augmentation for specialized cost.\u53ef\u8c13\u6df1\u5ea6\u8c03\u53c2 YOLOMono3D \u4e0d\u591a\u8bf4\u4e86\uff0c\u5feb\u4e0a\u8f66","title":"Recent Collections for Mono 3D detection"},{"location":"3dDetection/RecentCollectionForMono3D/#recent-collections-for-mono-3d-detection","text":"\u5728IROS2020\u6295\u7a3f\u524d\u540e\u79ef\u6512\u4e86\u4e00\u7cfb\u5217\u5355\u76ee3D\u68c0\u6d4bpaper\u7684\u9605\u8bfb\u3002\u8fd9\u91cc\u4e00\u6b21\u8fc7\u8fdb\u884c\u8bb0\u5f55,\u5f00\u6e90\u5728\u524d\uff0c\u672a\u5f00\u6e90\u5728\u540e. \u8fd9\u91cc\u5217\u51fa\u76ee\u524d\u6709\u6587\u7ae0\u53ef\u5bfb\u7684KITTI\u6392\u884c\u699c(2022.07.23) Update(2020.04.02):Update scores for YOLOMono3D Update(2020.07.24): Update Kinematic 3D Update(2020.09.08): Update SSL-RTM3D Update(2021.02.01): Update Ground Aware 3D Update(2021.03.17): Update CaDDN Update(2021.04.10): Update MonoFlex, MonoRun, MonoRCNN Update(2021.06.26): Update MonoEF Methods Moderate Easy Hard Time LPCG 17.80 25.56 15.38 0.03 PS-fld 17.74 23.74 15.14 0.25 dd3d 16.34 23.22 14.20 - DID-M3D 16.29 22.50 13.95 0.02 MonoDETR 16.26 24.52 13.93 0.04 MonoDTR 15.39 21.99 12.73 0.04 GUPNet 15.02 22.26 13.12 - DEVIANT 14.46 21.88 11.89 0.04 DLE 14.33 24.23 10.30 0.06 AutoShape 14.17 22.47 11.36 0.05 MonoFlex 13.89 19.94 12.07 0.03 MonoEF 13.87 21.29 11.74 0.03 CaDDN 13.41 19.17 11.46 0.64 GroundAware3D 13.17 21.60 9.94 0.05 Aug3DRPN 12.99 17.82 9.78 0.08 Kinematic3D 12.72 19.07 9.17 0.12 MonoRCNN 12.65 18.36 10.03 0.07 MonoRun 12.30 19.65 10.58 0.07 YOLOMono3D 12.06 18.28 8.42 0.05 D4LCN 11.72 16.65 9.51 0.2 M3DSSD 11.46 17.51 8.98 * SSL-RTM3D 11.45 16.73 9.92 0.04 Refined-MPL 11.14 18.09 8.94 0.15 AM3D 10.74 16.50 9.52 0.4 RTM3D 10.34 14.41 8.77 0.05 MonoPair 9.99 13.04 8.65 0.06 SMOKE 9.76 14.03 7.84 0.03 M3D-RPN 9.71 14.76 7.42 0.16 \u76ee\u5f55: Recent Collections for Mono 3D detection LPCG Pseudo-Stereo DiD-M3D MonoDETR MonoDTR GUPNet MonoFlex MonoEF MonoRCNN Aug3DRPN D4LCN M3DSSD RTM3D MonoPair SMOKE YOLOMono3D","title":"Recent Collections for Mono 3D detection"},{"location":"3dDetection/RecentCollectionForMono3D/#lpcg","text":"pdf code \u8fd9\u7bc7paper\u5e26\u6765\u4e86\u5f88\u5f3a\u7684\u5b9e\u7528\u6027\u3002\u5229\u7528\u96f7\u8fbe\uff0c\u5229\u7528\u81ea\u76d1\u7763\uff0c\u5229\u7528\u4e0a\u5927\u91cfkitti raw\u4e2d\u6ca1\u6709\u88ab\u6807\u8bb0\u7684\u6570\u636e\u3002 Step 1\uff0c \u8bad\u7ec3\u4e00\u4e2a\u96f7\u8fbe\u7684detector, \u5e76\u4e14\u51c6\u5907\u4e00\u4e2a\u57fa\u4e8e\u96f7\u8fbe\u7684 unsupervised detector. Step 2. \u8bad\u7ec3\u56fe\u7247\u7684\u65f6\u5019\uff0c\u5229\u7528\u96f7\u8fbe\u7684detector\u4ee5\u53caunsupervised detector, \u5728\u6ca1\u6709\u88ab\u6807\u8bb0\u7684\u6570\u636e\u4e0a\u751f\u6210 pseudo label. \u6269\u5927\u8bad\u7ec3\u6570\u636e\u91cf\u3002\u5f97\u5230\u5f88\u597d\u7684\u6027\u80fd\u3002","title":"LPCG"},{"location":"3dDetection/RecentCollectionForMono3D/#pseudo-stereo","text":"pdf code \u8fd9\u7bc7paper\u505a\u4e86\u5f88\u795e\u5947\u7684\u64cd\u4f5c\uff0c\u5c31\u662f\u91c7\u7528\u5355\u76ee\u4f2a\u88c5\u53cc\u76ee\uff0c\u7136\u540e\u4f7f\u7528\u53cc\u76ee\u7684detector ( LIGA , YOLOStereo3D )\u51fa\u7ed3\u679c\uff0c\u5f97\u5230\u76f8\u5f53\u4e0d\u9519\u7684\u6027\u80fd\u3002 \u4f5c\u8005\u5c1d\u8bd5\u4e86\u51e0\u79cd\u65b9\u6848\uff0c \u56fe\u50cf\u7ea7\u522b\u7684\u751f\u6210\uff1a\u5355\u76ee\u9884\u6d4b\u4e00\u4e2adisparity map,\u5b8c\u5168\u76f4\u63a5\u6784\u5efa\u53cc\u76ee\u56fe\u50cf\u3002 \u6027\u80fd\u4e00\u822c \u7279\u5f81\u7ea7\u522b\u7684\u751f\u6210\uff1a\u4f7f\u7528\u4e0a\u8ff0\u7684Disparity map\u76f4\u63a5\u6784\u5efa\u4e00\u4e2a\u7279\u5f81\u56fe\uff0c\u548c\u53f3\u56fe\u7684\u7279\u5f81\u56fe\u8f93\u5165\u53cc\u76ee\u5339\u914d\u90e8\u5206\u3002\u6548\u679c\u6700\u597d \u514b\u9686\uff0c\u76f4\u63a5\u590d\u5236\u539f\u6765\u7684\u5355\u76ee\u7279\u5f81\uff0c\u8fdb\u5165\u53cc\u76ee\u5339\u914d\u90e8\u5206\u3002\u6548\u679c\u8fd8\u4e0d\u9519","title":"Pseudo-Stereo"},{"location":"3dDetection/RecentCollectionForMono3D/#did-m3d","text":"pdf code \u8fd9\u7bc7paper\u6574\u4f53\u65b9\u6848\u4e0a\u4e0e GUPNet \u662f\u4e00\u81f4\u7684\uff0c\u662f\u4e00\u4e2a\u4e8c\u9636\u6bb5\u7684\u7b97\u6cd5\u3002\u5b83\u7b2c\u4e00\u4e2a\u5f15\u5165\u5e76\u5f3a\u8c03\u4e86 visual depth / attribute depth\u7684\u6982\u5ff5\u3002 visual depth\u4e5f\u5c31\u662f\u7269\u4f53\u8868\u9762\u7684\u6df1\u5ea6\uff0c\u53ef\u4ee5\u7406\u89e3\u4e3a\u8be5\u50cf\u7d20\u5b9e\u9645\u7269\u4f53\u70b9\u7684\u6df1\u5ea6\uff0cattribute depth\u5219\u662f\u8be5\u70b9\u5230\u7269\u4f53\u4e2d\u5fc3\u7684\u6df1\u5ea6\u8ddd\u79bb\u3002\u4f5c\u8005\u5f3a\u8c03\u8ba4\u4e3a\uff0c\u6211\u4eec\u4e0d\u5e94\u8be5\u76f4\u63a5\u9884\u6d4b\u4e2d\u5fc3\u7684\u6df1\u5ea6\u8ddd\u79bb\uff0c\u800c\u5e94\u8be5\u628a\u8fd9\u4e24\u4e2a\u6df1\u5ea6\u89e3\u8026\u3002\u5e76\u4e14\u8fd9\u4e2a\u7b97\u6cd5\u73b0\u5728\u53ef\u4ee5\u5bf9\u5185\u53c2\u5b8c\u5168\u4e0d\u654f\u611f\u4e86\u3002 Visual Depth \u662f\u7269\u4f53\u50cf\u7d20\u5230\u76f8\u673a\u7684\u8ddd\u79bb\uff0c\u5982\u679c\u6211\u4eec\u628a\u56fe\u7247\u88c1\u526a/\u653e\u5927\uff0c\u90a3\u4e48\u5bf9\u5e94\u7684\uff0c\u6211\u4eec\u9884\u6d4b\u7684\u6df1\u5ea6\u50cf\u7d20\u5e94\u8be5\u4f1a\u968f\u7740\u7126\u8ddd\u6539\u53d8\uff0c\u8fd9\u90e8\u5206\u7684\u6df1\u5ea6\u662faffine-sensitive\u7684\u3002 attribute depth\u662f\u7269\u4f53\u8868\u9762\u5230\u4e09\u7ef4\u76ee\u6807\u4e2d\u5fc3\u70b9\u7684\u8ddd\u79bb\uff0c\u8fd9\u90e8\u5206\u6df1\u5ea6\u662f\u7531\u8f66\u5b50\u7684\u59ff\u6001\u548c\u8f66\u5b50\u7684\u5927\u5c0f\u51b3\u5b9a\u7684\uff0c\u56e0\u800c\u53ef\u4ee5\u8ba4\u4e3a\u8fd9\u90e8\u5206\u6df1\u5ea6\u662f affine-insensitive, \u4e0e\u6211\u4eec\u653e\u5927\u7f29\u5c0f\u56fe\u7247\u65e0\u5173\u3002 GT\u548c MultiSensor Refinement \u90a3\u7bc7\u7684\u505a\u6cd5\u6bd4\u8f83\u63a5\u8fd1\uff0c\u5229\u75283D\u6846\u5185\u7684\u70b9\u4e91\u70b9\u6784\u9020\u4e00\u4e2a\u7a00\u758f\u7684visual depth label.\u7136\u540e\u7528gt \u51cf\u53bb\u5b83\u5c31\u53ef\u4ee5\u5f97\u5230 attribute loss. \u540e\u7eed\u505a\u6570\u636e\u589e\u5f3a\u7684\u65f6\u5019 visual depth\u968f\u7740scale\u6539\u53d8\uff0c\u4f46\u662fattribute loss\u5219\u4e0d\u6539\u53d8\u3002 \u4f7f\u7528L1 Loss, \u90fd\u9884\u6d4bLaplacian uncertainty, \u878d\u5408\u4e5f\u4e00\u6837\u3002","title":"DiD-M3D"},{"location":"3dDetection/RecentCollectionForMono3D/#monodetr","text":"pdf code \u8fd9\u7bc7paper\u5219\u5b8c\u5168\u5f15\u5165\u4e86DETR, \u4f3c\u4e4e\u662f\u7b2c\u4e00\u7bc7\u628aDETR\u7684\u8f93\u51fa\u5934\u5b8c\u5168\u5730\u5f15\u5165Mono3D\u8fd9\u4e2a\u4efb\u52a1\u3002 \u7279\u5f81\u4e0a\u5206\u5f00visual encoder\u548c\u6709\u72ec\u7acb\u6df1\u5ea6\u4fe1\u606f\u8865\u5145\u7684depth encoder; depth predictor\u9884\u6d4b\u4e00\u4e2a\u591a\u5206\u7c7b\u6df1\u5ea6\u4f30\u8ba1\uff0c\u7531object depth\u8fdb\u884c\u76d1\u7763\uff0c2d\u6846\u5185\u7684\u6240\u6709\u70b9\u6df1\u5ea6gt\u4e3a\u7269\u4f53\u7684\u6df1\u5ea6\uff0c\u76f8\u4ea4\u90e8\u5206\u53d6\u6df1\u5ea6\u6700\u5c0f\u503c\uff0c\u6846\u5916\u5206\u7c7b\u4e3a\u72ec\u7acb\u7684\u80cc\u666f\u7c7b\u3002 \u5728depth cross-attention\u4e2d\uff0c\u5229\u7528\u6df1\u5ea6\u7684\u9884\u6d4b\u503c\u6784\u5efa\u6df1\u5ea6positional embedding\uff1b\u628a\u5bf9\u5e94\u5750\u6807\u7684\u6df1\u5ea6\u9884\u6d4b\u5206\u5e03\uff0c\u5f15\u5165\u4f5c\u4e3apositional embedding. DETR\u6df1\u5ea6\u7684\u6700\u7ec8\u8f93\u51fa\uff0c\u7531 d_{geo}=f\\frac{h_{3D}}{t+b} \uff0c( t,b \u5206\u522b\u662f\u4e0a\u4e0b\u8fb9\u52303D\u6295\u5f71\u4e2d\u5fc3\u7684\u8ddd\u79bb\uff0c\u7f51\u7edc\u4e5f\u4ee5\u6b64\u4f5c\u4e3aprediction attribute) \uff0c d_reg \u76f4\u63a5\u9884\u6d4b\u4ee5\u53ca d_{map}(x_{3D}, y_{3D}) \u9884\u6d4b\u7684\u6df1\u5ea6\u56fe\u5728\u5bf9\u5e94\u70b9\u7684\u63d2\u503c\u7ec4\u6210\u3002","title":"MonoDETR"},{"location":"3dDetection/RecentCollectionForMono3D/#monodtr","text":"pdf code \u6574\u4f53\u6765\u8bf4\u8fd9\u7bc7paper\u91c7\u7528\u4e86\u5377\u79efbackbone\u52a0transformer head\u7684\u6574\u4f53\u65b9\u6848\u6765\u6784\u9020\u7f51\u7edc\uff0c\u8f93\u51fa\u7684\u5f62\u6001\u4e0e YOLOMono3D \u662f\u4e00\u6837\u7684\u3002 \u4e5f\u901a\u8fc7DFE\u5f15\u5165\u4e86\u6df1\u5ea6\u76d1\u7763\u3002","title":"MonoDTR"},{"location":"3dDetection/RecentCollectionForMono3D/#gupnet","text":"pdf code \u8fd9\u7bc7paper\u662f\u76ee\u524d\u5c11\u6709\u7684\u4f7f\u7528\u4e8c\u9636\u6bb5\u68c0\u6d4b\u7f51\u7edc\u5b9e\u73b0\u9ad8\u6027\u80fd\u7684\u7ed3\u679c.\u4f7f\u7528centerNet\u5b8c\u62102D\u68c0\u6d4b\uff0c\u7136\u540e\u4f7f\u7528RoIAlign\u63d0\u53d6\u7279\u5f81.3D\u7684\u8f93\u51fa\u4e0e MonoFlex \u6709\u70b9\u76f8\u4f3c\uff0c\u8f93\u51fa\u7684\u662f\u5e26\u6709\u4e0d\u786e\u5b9a\u7684\u6df1\u5ea6\uff0c\u91c7\u7528\u7684\u662f2D\u7684\u7b80\u8981\u63a8\u7406\u52a0\u4e0a\u7f51\u7edc\u7684\u4e00\u4e2a\u5b66\u4e60\u503c: \\begin{aligned} d_{p} &=\\frac{f \\cdot h_{3 d}}{h_{2 d}}=\\frac{f \\cdot\\left(\\lambda_{h} \\cdot X+\\mu_{h}\\right)}{h_{2 d}} \\\\ &=\\frac{f \\cdot \\lambda_{h}}{h_{2 d}} \\cdot X+\\frac{f \\cdot \\mu_{h}}{h_{2 d}} \\end{aligned} \\begin{array}{c} d=L a\\left(\\mu_{p}, \\sigma_{p}\\right)+L a\\left(\\mu_{b}, \\sigma_{b}\\right) \\\\ \\mu_{d}=\\mu_{p}+\\mu_{b}, \\quad \\sigma_{d}=\\sqrt{\\left(\\sigma_{p}\\right)^{2}+\\left(\\sigma_{b}\\right)^{2}} \\end{array} \\mathcal{L}_{\\text {depth }}=\\frac{\\sqrt{2}}{\\sigma_{d}}\\left|\\mu_{d}-d^{g t}\\right|+\\log \\left(\\sigma_{d}\\right) \u672c\u6587\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e2ahierarchical task learning.\u6307\u4f18\u5148\u628a2D\u68c0\u6d4b\u8bad\u7ec3\u597d\uff0c\u518d\u628aWHL\u8bad\u7ec3\u597d\uff0c\u6700\u540e\u518d\u8bad\u7ec3depth\u3002\"\u8bad\u7ec3\u597d\"\u7684\u8bc4\u5224\u6807\u51c6\u662fepochs\u4e4b\u95f4\u7684mean loss\u76f8\u5dee\u4e0d\u5927\u3002 self.loss_graph = {'seg_loss':[], 'size2d_loss':[], 'offset2d_loss':[], 'offset3d_loss':['size2d_loss','offset2d_loss'], 'size3d_loss':['size2d_loss','offset2d_loss'], 'heading_loss':['size2d_loss','offset2d_loss'], 'depth_loss':['size2d_loss','size3d_loss','offset2d_loss']} def compute_weight(self,current_loss,epoch): T=140 #compute initial weights loss_weights = {} eval_loss_input = torch.cat([_.unsqueeze(0) for _ in current_loss.values()]).unsqueeze(0) for term in self.loss_graph: if len(self.loss_graph[term])==0: loss_weights[term] = torch.tensor(1.0).to(current_loss[term].device) else: loss_weights[term] = torch.tensor(0.0).to(current_loss[term].device) #update losses list if len(self.past_losses)==self.stat_epoch_nums: past_loss = torch.cat(self.past_losses) mean_diff = (past_loss[:-2]-past_loss[2:]).mean(0) if not hasattr(self, 'init_diff'): self.init_diff = mean_diff c_weights = 1-(mean_diff/self.init_diff).relu().unsqueeze(0) time_value = min(((epoch-5)/(T-5)),1.0) for current_topic in self.loss_graph: if len(self.loss_graph[current_topic])!=0: control_weight = 1.0 for pre_topic in self.loss_graph[current_topic]: control_weight *= c_weights[0][self.term2index[pre_topic]] loss_weights[current_topic] = time_value**(1-control_weight) #pop first list self.past_losses.pop(0) self.past_losses.append(eval_loss_input) return loss_weights","title":"GUPNet"},{"location":"3dDetection/RecentCollectionForMono3D/#monoflex","text":"pdf code \u8fd9\u7bc7paper\u5728\u8ba1\u7b97\u6df1\u5ea6\u7684\u65f6\u5019\u548cMonoRCNN\u4e0d\u8c0b\u800c\u5408\uff0c\u89c9\u5f97\u5c3d\u7ba1\u540c\u65f6\u627e\u51fa\u591a\u4e2a\u89d2\u70b9\u5f88\u6709\u8bf1\u60d1\u529b\uff0c\u4f46\u662f\u4e00\u70b9\u70b9\u7684\u8bef\u5dee\u5c31\u4f1a\u9020\u62103D\u56de\u5f52\u5f88\u5927\u7684\u9519\u8bef\uff0c\u6240\u4ee5\u4e3b\u8981\u4f9d\u9760\u8ba1\u7b97\u89d2\u70b9\u5728\u56fe\u50cf\u4e0a\u7684\u9ad8\u5ea6\u6765\u63a8\u7406\u6df1\u5ea6\u3002","title":"MonoFlex"},{"location":"3dDetection/RecentCollectionForMono3D/#monoef","text":"pdf code \u8fd9\u7bc7paper\u5173\u6ce8\u5916\u53c2\u7684\u53d8\u52a8\u4ee5\u53ca\u5916\u53c2\u53d8\u52a8\u6700\u76f8\u673a\u611f\u77e5\u6548\u679c\u7684\u5f71\u54cd\u3002\u540c\u65f6\u4f1a\u4e3b\u52a8\u5730\u901a\u8fc7\u5e8f\u5217\u56fe\u7247\u9884\u5224\u8f66\u8f86\u7684\u8fd0\u52a8.","title":"MonoEF"},{"location":"3dDetection/RecentCollectionForMono3D/#monorcnn","text":"pdf code \u8fd9\u7bc7paper\u4e0eMonoFlex\u6709\u76f8\u4f3c\u7684\u601d\u8def\uff0c\u90fd\u662f\u7528projected visual height\u6765\u8ba1\u7b97\u6df1\u5ea6\u3002\u8fd9\u7bc7\u6587\u7ae0\u5219\u662f\u57fa\u4e8etwo-stage\u7684\u7b97\u6cd5\u3002\u6587\u7ae0\u76f4\u63a5\u6307\u51fa\u4e86keypoints\u7684\u95ee\u9898\u3002","title":"MonoRCNN"},{"location":"3dDetection/RecentCollectionForMono3D/#aug3drpn","text":"pdf \u8fd9\u7bc7paper\u57fa\u4e8e M3D-RPN \u7684\u505a\u6cd5 \u989d\u5916\u7684\u6df1\u5ea6\u9884\u6d4b \u5229\u7528\u6df1\u5ea6\u9884\u6d4b\uff0c\u6e32\u67d3\u865a\u62df\u76f8\u673a\u7ed3\u679c,\u589e\u5f3a\u8bad\u7ec3\u7ed3\u679c.","title":"Aug3DRPN"},{"location":"3dDetection/RecentCollectionForMono3D/#d4lcn","text":"pdf code \u8fd9\u7bc7paper\u5b8c\u5168\u7ee7\u627f\u4e86 M3D-RPN \u7684\u8863\u94b5\uff0c\u5b83\u4e0d\u540c\u7684\u5730\u65b9\u5728\u4e8e\uff0c\u6452\u5f03\u4e86M3D-RPN\u5904\u7406\u7f13\u6162\u7684height-wise convolution,\u800c\u662f\u4f7f\u7528\u5355\u76ee\u4f30\u8ba1\u6df1\u5ea6\uff0c\u7136\u540e\u4f7f\u7528\u6df1\u5ea6\u4f5c\u4e3a\u5377\u79ef\u6838\u7684guide, \u8fd9\u4e2aguide\u7c7b\u4f3c\u4e8e\u8fd9\u51e0\u7bc7\u6587\u7ae0\u7684\u64cd\u4f5c: guidenet ; DFN","title":"D4LCN"},{"location":"3dDetection/RecentCollectionForMono3D/#m3dssd","text":"pdf code \u8fd9\u7bc7paper\u91c7\u7528\u4e86\u5927backbone,\u7528deformable convolution\u7684\u65b9\u6cd5\uff0c\u91cd\u70b9\u89e3\u51b32D detection\u4e2d\u611f\u53d7\u91ce\u4e0eanchor\u4e0d\u5339\u914d\u95ee\u9898\u4ee5\u53ca3D\u4e2d\u5fc3\u4e0e2D\u4e2d\u5fc3\u4e0d\u5339\u914d\u7684\u95ee\u9898. Shape alignment \u4e0e center alignment \u5bf9\u5e94\u57fa\u4e8e2D\u4e0e3D\u7684\u4e24\u79cddeformable base shift\u7684\u4ea7\u751f\u65b9\u5f0f\u3002 \u53e6\u5916\u672c\u6587\u53c8\u63d0\u51fa\u4e86 Asymmetric Non-Local Attention Block (ANAB)\u6a21\u5757, \u7528\u4e8e\u589e\u5f3a\u611f\u53d7\u91ce.","title":"M3DSSD"},{"location":"3dDetection/RecentCollectionForMono3D/#rtm3d","text":"pdf code \u8fd9\u7bc7\u6587\u7ae0\u8fd8\u6ca1\u6709\u6b63\u5f0f\u5f00\u6e90\uff0c\u4f46\u662fgithub\u5c31\u5148\u5f00\u7740\u4e86\u3002\u8fd9\u7bc7\u6587\u7ae0\u5728\u6280\u672f\u4e0a\u6709\u4e00\u5b9a\u7684\u65b0\u610f\uff0c\u5b83\u4f7f\u7528 CenterNet \u7684\u67b6\u6784\u4f30\u8ba1\u5927\u91cf\u7684keypoints\u4ee5\u53ca\u5197\u4f59\u76843D\u4fe1\u606f\uff0c\u6700\u540e\u901a\u8fc7\u6700\u4f18\u5316\u878d\u5408\u3002\u4f7f\u7528\u5927\u91cf\u5197\u4f59\u4fe1\u606f\u5b83\u4e0d\u662f\u7b2c\u4e00\u4e2a,\u524d\u8005\u6bd4\u5982\u6709 SS3D ,\u4f46\u662f\u5b83\u7ed5\u8fc7\u4e86anchor\u4f7f\u7528CenterNet\u6709\u4e00\u5b9a\u7684\u65b0\u610f\u3002","title":"RTM3D"},{"location":"3dDetection/RecentCollectionForMono3D/#monopair","text":"pdf \u8fd9\u7bc7\u6587\u7ae0\u662f\u5b9e\u9a8c\u5ba4\u5927\u5e08\u5144\u90b0\u78ca\u5728\u963f\u91cc\u7684CVPR2020 paper\u3002\u6709 \u5b98\u65b9\u7f51\u7ad9 \u6587\u7ae0\u7684\u6838\u5fc3\u521b\u65b0\u662f\u7b2c\u4e00\u4e2a\u4f7f\u7528\u573a\u666f\u4e2d\u4e0d\u540c\u7269\u4f53\u4e4b\u95f4\u7684\u76f8\u4e92\u7ea6\u675f\u8fdb\u884c\u4f18\u5316\u7684paper\u3002","title":"MonoPair"},{"location":"3dDetection/RecentCollectionForMono3D/#smoke","text":"pdf \u8fd9\u7bc7paper\u7684\u521b\u65b0\u70b9\u4e0d\u7b97\u7279\u522b\u591a\u3002 1. \u4f7f\u7528\u4e86 CenterNet \u7684\u67b6\u6784\u8fdb\u884c\u4e2d\u5fc3\u70b9\u7684\u4f30\u8ba1\u3002 2. \u4f7f\u7528\u4e86distangling loss, \u8fd9\u4e2a\u6765\u81ea\u4e8e MonoDIS 3. \u6570\u636e\u589e\u5f3a\u4e0a\u4f7f\u7528\u4e86shifting\u7b49\u65b9\u6cd5\uff0c\u4f46\u662f\u53ea\u662f\u7528\u6765train keypoint\u70ed\u56fe\u7b49\u7ed3\u6784\u3002\u5c5e\u4e8especialized augmentation for specialized cost.\u53ef\u8c13\u6df1\u5ea6\u8c03\u53c2","title":"SMOKE"},{"location":"3dDetection/RecentCollectionForMono3D/#yolomono3d","text":"\u4e0d\u591a\u8bf4\u4e86\uff0c\u5feb\u4e0a\u8f66","title":"YOLOMono3D"},{"location":"3dDetection/RecentCollectionForStereo3D/","text":"Recent Collections for Stereo 3D detection \u8fd1\u671f\u79ef\u6512\u4e86\u4e00\u7cfb\u5217\u53cc\u76ee3D\u68c0\u6d4bpaper\u7684\u9605\u8bfb\u3002\u8fd9\u91cc\u4e00\u6b21\u8fc7\u8fdb\u884c\u8bb0\u5f55,\u4ee5\u7ed3\u679c\u6392\u5217\u4e3a\u987a\u5e8f\u3002 \u8fd9\u91cc\u5217\u51fa\u76ee\u524d\u6709\u6587\u7ae0\u53ef\u5bfb\u7684KITTI\u6392\u884c\u699c(2020.04.09) Update: 2020.0409: Add Disp-RCNN and PL E2E. 2020.0714: Add CDN 2021.0126: Add RTS3D Methods Moderate Easy Hard Time LIGAStereo 64.66 % 81.39 % 57.22 % 0.4 s CDN 54.22 % 74.52 % 46.36 % 0.6 s CG-Stereo 53.58 % 74.39 % 46.50 % 0.57 s DSGN 52.18 % 73.50 % 45.14 % 0.67 s EGFN 46.39 % 65.80 % 38.42 % 0.09 s CDN P-LiDAR++ 44.86 % 64.31 % 38.11 % 0.4 s Pseudo-LiDAR E2E 43.92 % 64.75 % 38.14 % 0.4 s Pseudo-LiDAR++ 42.43 % 61.11 % 36.99 % 0.4 s YOLOStereo3D 40.71 % 65.77 % 29.99 % 0.08s Disp R-CNN (velo) 39.34 % 59.58 % 31.99 % 0.42 s ZoomNet 38.64 % 55.98 % 30.97 % 0.3 s OC Stereo 37.60 % 55.15 % 30.25 % 0.35 s RTS3D 37.38 % 58.51 % 31.12 % 0.03 s Pseudo-Lidar 34.05 % 54.53 % 28.25 % 0.4 s Stereo R-CNN 30.23 % 47.58 % 23.72 % 0.3 s RT3DStereo 23.28 % 29.90 % 18.96 % 0.08 s \u76ee\u5f55: Recent Collections for Stereo 3D detection LIGAStereo CG-stereo Pseudo-LiDAR E2E Pseudo-Lidar++ Disp-RCNN ZoomNet OC Stereo RTS3D Pseudo-Lidar Stereo R-CNN RT3D Stereo LIGAStereo pdf code \u8fd9\u7bc7paper\u662f\u8fd1\u671f\u6765\u6027\u80fd\u7b2c\u4e00\u4e2a\u8d85\u8fc7\u4e09/\u56db\u5e74\u524d\u70b9\u4e91\u68c0\u6d4b\u7684\u53cc\u76ee\u68c0\u6d4b\u7f51\u7edc\u3002\u6027\u80fd\u4e0a\u5168\u9762\u8d85\u8d8a\u53cc\u76ee\u6b64\u524d\u7684\u7f51\u7edc\u3002\u5176\u4e3b\u8981\u601d\u60f3\u662f\u4f7f\u7528\u7c7b\u4f3c\u4e8e DSGN \u7684\u7f51\u7edc\uff0c \u4f46\u662f\u4e8e\u6b64\u540c\u65f6\uff0c\u53e6\u5916\u8bad\u7ec3\u4e00\u4e2a\u70b9\u4e91\u68c0\u6d4b\u7f51\u7edc\uff0c\u4e24\u4e2a\u7f51\u7edc\u8f93\u51fa\u53d7\u76f8\u540c\u7684\u76d1\u7763\u635f\u5931\u8bad\u7ec3\uff0c\u6b64\u5916\u53cc\u76ee\u7684cost volume\u5206\u652f\u5728\u6700\u540e\u4e00\u5c42\u8fd8\u4f1a\u53d7\u70b9\u4e91\u6700\u540e\u4e00\u5c42\u7684\u77e5\u8bc6\u84b8\u998f\u8bad\u7ec3\u3002 \u77e5\u8bc6\u84b8\u998f\u635f\u5931: \\mathcal{L}_{i m}=\\sum_{\\mathcal{F}_{i m} \\in \\mathbb{F}_{i m}} \\frac{1}{N_{p o s}}\\left\\|M_{f g} M_{s p}\\left(g\\left(\\mathcal{F}_{i m}\\right)-\\frac{\\mathcal{F}_{i m}^{l i d a r}}{\\mathbb{E}\\left[\\left|\\mathcal{F}_{i m}^{l i d a r}\\right|\\right]}\\right)\\right\\|_{2}^{2} CG-stereo pdf \u8fd9\u7bc7paper\u6765\u81ea\u4e8e Jason Ku\u90a3\u4e00\u7ec4\uff0c\u662f\u76ee\u524d(2020.03.15)Stereo\u7684SOTA\uff0c\u7ed9pseudo-lidar\u7cfb\u5217\u63d0\u4f9b\u4e86\u4e24\u4e2a\u5f88\u6709\u6548\u7684idea\u3002 \u9996\u5148\u662f\u53cc\u76ee\u4f30\u8ba1\u4e2d\uff0cforground\u4e0ebackground\u7684\u7279\u6027\u5dee\u8ddd\u662f\u5f88\u5927\u7684\uff0c\u4f5c\u8005\u8ba4\u4e3a\u5e94\u5f53\u4f7f\u7528\u4e24\u4e2a\u5206\u522b\u7684decoder\u5904\u7406\u524d\u666f\u4e0e\u80cc\u666f\u7684\u7269\u4f53\uff0c\u5206\u53c9\u7684\u4f9d\u636e\u662f\u8bed\u4e49\u5206\u5272\u7684\u7ed3\u679c\u3002 \u5176\u6b21\u662f\u53cc\u76ee\u4f30\u8ba1\u5f97\u5230\u7684\u70b9\u4e4b\u4e2d\u6709\u5f88\u591a\u7684\u566a\u97f3\uff0c\u4e0d\u540c\u70b9\u7684confidence\u4e0d\u540c\uff0c\u5bf9\u540e\u7aef\u70b9\u4e91\u7684\u5f71\u54cd\u5f88\u5927\uff0c\u4f5c\u8005\u8fd9\u91cc\u6839\u636estereo matching\u7684\u4e00\u4e2aconfidence map\u4f5c\u4e3a\u4e00\u4e2aattention \u5c42\u8f93\u5165\u5230\u70b9\u4e91\u540e\u7aef\u5904\u7406\u4e2d\u3002 Pseudo-LiDAR E2E pdf code \u8fd9\u7bc7paper\u7684\u8d21\u732e\u975e\u5e38\u6709\u610f\u4e49\uff0c\u63d0\u5230\u7684\u662f\u8fc7\u53bb\u7684Pseudo-lidar\u7b97\u6cd5\u57fa\u672c\u90fd\u662f\u5b8c\u5168\u7684\u4e8c\u9636\u6bb5\u7b97\u6cd5\uff0c\u4e5f\u5c31\u662f\u53cc\u76ee\u751f\u6210\u70b9\u4e91\u4e0e\u70b9\u4e913D\u68c0\u6d4b\u4e4b\u95f4\u662f\u65e0\u6cd5End2End\u8bad\u7ec3\u7684\uff0c\u4e2d\u95f4\u7684\u8f6c\u6362\u8fc7\u7a0b\u662f\u4e0d\u53ef\u5bfc\u7684\uff0c\u56e0\u800c\u5c3d\u7ba1\u53ef\u4ee5fine-tune,\u4f46\u662f\u68af\u5ea6\u7684\u6d41\u52a8\u4f1a\u4e2d\u65ad,\u8fd9\u7bc7paper\u7684\u6700\u5927\u8d21\u732e\u5728\u4e8e\u63d0\u51fa\u4e00\u4e2aCoR\u6a21\u5757\u4f7f\u5f97\u540c\u65f6\u53ef\u4ee5\u8bad\u7ec3\u70b9\u4e91\u751f\u6210\u4ee5\u53ca\u57fa\u4e8e\u70b9\u4e91\u7684\u7269\u4f53\u68c0\u6d4b\u3002 \\boldsymbol{T}\\left(m, m^{\\prime}\\right)=\\left\\{\\begin{array}{cc} 0 & \\text { if }\\left|P_{m^{\\prime}}\\right|=0 \\\\ \\frac{1}{\\left|\\boldsymbol{P}_{m^{\\prime}}\\right|} \\sum_{\\boldsymbol{p} \\in \\boldsymbol{P}_{m^{\\prime}}} e^{-\\frac{\\left\\|\\boldsymbol{p}-\\hat{p}_{m}\\right\\|^{2}}{\\sigma^{2}}} & \\text { if }\\left|P_{m^{\\prime}}\\right|>0 \\end{array}\\right. \\boldsymbol{T}(m)=\\boldsymbol{T}(m, m)+\\frac{1}{\\left|\\mathcal{N}_{m}\\right|} \\sum_{m^{\\prime} \\in \\mathcal{N}_{m}} \\boldsymbol{T}\\left(m, m^{\\prime}\\right) voxel\u91cc\u9762\u6bcf\u4e00\u4e2abin\u90fd\u5bf9\u5e94\u4e00\u4e2a\u57fa\u7c7b\uff0c\u6bcf\u4e00\u4e2abin\u662f\u81ea\u5df1\u4e0e\u5468\u56f4\u7684\u52a0\u6743\u6c42\u548c\u3002\u4ece\u800cvoxel\u53ef\u5bfc Pseudo-Lidar++ pdf code \u8fd9\u7bc7paper\u7684\u4e3b\u8981\u65b0\u610f\u6709\u4e24\u70b9 \u5728\u6df1\u5ea6\u4f30\u8ba1\u4e0a\uff0c\u4f5c\u8005\u7ed9\u51fa\u4e00\u4e2a\u65b0\u7684insight\uff0c\u5c31\u662f\u5747\u5300\u76843D\u5377\u79ef\u5f88\u53ef\u80fd\u662fdisparity-based cost volome\u7684\u4e00\u4e2aerror source\uff0c\u6bd4\u5982\u8bf4\u5bf9\u4e8edisparity\u6bd4\u8f83\u9ad8\u7684\u70b9\uff0c\u53ef\u4ee5smooth out\uff0c\u4f46\u662f\u5bf9\u4e8edisparity\u6bd4\u8f83\u5c0f\u7684\u70b9\u5219\u4e0d\u5e94\u8be5\u540c\u7b49\u7ea7\u522b\u7684smooth out(\u4f1a\u4ea7\u751f\u5f88\u5927\u8bef\u5dee)\u3002\u6240\u4ee5\u4f5c\u8005\u5c06disparity cost volume\u7684\u6df1\u5ea6\u65b9\u5411\u6c42\u5012\u6570\uff0c\u5e76\u7ebf\u6027\u63d2\u503c\u5f97\u5230depth cost volume,\u7136\u540e\u5728depth cost volume\u4e0a\u9762\u505a3D\u5377\u79ef\u3002 \u5728\u540e\u5904\u7406\u4e0a\uff0c\u4f5c\u8005\u878d\u5408\u4e86\u6df1\u5ea6\u8865\u5168(depth completion)\u7684\u601d\u60f3\uff0c\u7531\u4e8edisparity\u662f\u79bb\u6563\u7684\uff0c\u6240\u4ee5\u4f1a\u5f15\u8d77\u5f88\u591a\u4e0d\u5e94\u8be5\u7684\u8bef\u5dee\uff0c\u8fdb\u800c\u4f5c\u8005\u8003\u8651\u4f7f\u7528\u4f4e\u7ebf\u6570\u7684lidar(\u5f00\u6e90\u7684\u4e00\u4e2a\u65b9\u6848\u7ebf\u6570\u662f4)\u4f5c\u4e3a\u4e00\u4e2aground truth\u7684\u8865\u507f\u3002\u8fd9\u91cc\u4e0d\u8fdb\u4e00\u6b65\u5c55\u5f00\u3002 \u4f5c\u8005\u5728KITTI\u4e0a\u63d0\u4ea4\u4e86\u4e24\u4e2a\u6210\u7ee9(PL++),\u6807\u9898\u4e0b\u9762\u7ed9\u51fa\u7684\u662f\u6625\u53cc\u76ee\u800c\u6ca1\u6709GDC\u7684\u6210\u7ee9\uff0c\u6709GDC\u7684\u6210\u7ee9\u4f1a\u66f4\u9ad8\u4e00\u4e9b\u3002 Disp-RCNN pdf code \u8fd9\u7bc7paper\u7684\u4e3b\u8981\u4e0d\u540c\u70b9\u5728\u4e8e\u4f7f\u7528RoIPooling\u4ece\u539f\u56fe(\u4f5c\u8005\u6307\u51fa\u4e0d\u5e94\u8be5\u4ecefeature\u4e2d\u91c7\u6837\uff0c\u56e0\u4e3aDisparity\u8981\u6c42\u90bb\u8fd1\u50cf\u7d20\u7ed3\u679c\u4e0d\u540c\uff0c\u4f46\u662fInstance Segmentation\u4f1a\u8981\u6c42\u90bb\u8fd1\u50cf\u7d20\u7ed3\u679c\u76f8\u540c)\u4e2d\u91c7\u6837,\u7136\u540e\u4ecePooling\u540e\u7684\u7ed3\u679c\u91cd\u5efaDisparity\u4ee5\u53ca\u5c40\u90e8\u70b9\u4e91(\u8fd9\u4e2a\u6a21\u5757\u5728\u6570\u5b66\u5904\u7406\u4e0a\u8981\u5c0f\u5fc3)\uff0c\u7136\u540e\u4f7f\u7528\u70b9\u4e91\u68c0\u6d4b\u8f93\u51fa\u7ed3\u679c\u3002 \u8fd9\u7bc7paper\u8fd8\u6709\u4f7f\u7528pretrained\u7684\u53cc\u76ee\u91cd\u5efa\u7f51\u7edc\uff0c\u5f97\u5230\u5bc6\u96c6\u7684pseudo Ground Truth Disparity. Loss\u7684\u6784\u6210\u6bd4\u8f83\u590d\u6742\uff0c\u5177\u4f53\u770bpaper ZoomNet pdf code \u4f7f\u75282D\u68c0\u6d4b\u5148\u5f97\u5230\u4e24\u4e2a\u8f66\u5b50\u548b\u56fe\u7247\u7684\u4f4d\u7f6e\uff0c\u7136\u540e\u5206\u522bresize,\u5e76\u4e14\u8c03\u8282\u540d\u4e49\u76f8\u673a\u53c2\u6570(zooming)\u3002 \u4e2d\u4ecb\u8f85\u52a9\u9884\u6d4b\u7684\u5185\u5bb9\u5305\u62ecdisparity, instance segmentation, part location(\u6bcf\u4e00\u4e2a\u50cf\u7d20\u76f8\u5bf9\u4e8e\u8f66\u5b50\u4e2d\u5fc3x, y, z\u8f74\u7684\u4f4d\u7f6e\uff0c\u8fd9\u4e2a\u4e00\u822c\u4f7f\u7528\u70b9\u4e91\u548c\u7a20\u5bc6\u6df1\u5ea6\u56fe\u8fdb\u884c\u6807\u6ce8)\u3002\u5f97\u5230\u70b9\u4e91\u540e\u5c06feature \u94fe\u63a5\uff0c\u7136\u540e\u7528\u7c7b\u4f3c\u4e8epoint net\u7684\u65b9\u5f0f\u9884\u6d4b\u6700\u7ec8\u7ed3\u679c\u3002 OC Stereo pdf \u8fd9\u7bc7paper\u7684\u60f3\u6cd5\u662f\u4f7f\u7528RoIAlign\u5c06\u5de6\u53f3\u76ee\u4e24\u4e2a\u533a\u57df\u7684feature \u63d0\u51fa\u6765\uff0c\u7136\u540e\u4f7f\u7528instance seg\u4e0eCost volumn\u8ba1\u7b97\u5bf9\u5e94pixel\u5904\u7684disparity\u3002 \u4f5c\u8005\u5bf9\u4e8eRoIAlign\u524d\u540e\u7684segmentation pixel\u7684\u4f4d\u7f6e\u5173\u7cfb\u505a\u4e86\u5f88\u7ec6\u81f4\u7684\u89e3\u91ca\u3002 \u5728\u5f97\u5230\u5c40\u90e8RGB\u70b9\u4e91\u4e4b\u540e\u4f5c\u8005\u4f7f\u7528 AVOD \u8fdb\u884c3D\u68c0\u6d4b\u3002 RTS3D pdf code \u8fd9\u7bc7paper\u63a8\u7406\u65f6\u7684\u64cd\u4f5c\u662f\u5148\u8ba9\u4e00\u4e2a\u5355\u76ee\u68c0\u6d4b\u7f51\u7edc\u8f93\u51fa\u4e00\u4e2a\u68c0\u6d4b\u7ed3\u679c\u3002 \u7136\u540e\u53e6\u5916\u4f7f\u7528\u4e00\u4e2a\u7c7b\u4f3cpointnet\u7684\u673a\u5236\u53bb\u589e\u5f3amono\u7684\u7ed3\u679c. \u7528\u4e24\u4e2aresnet\u5206\u522b\u8f93\u51fa\u4e24\u5f20\u56fe\u7684feature \u5728predicted bounding box\u7684\u9644\u8fd1\u751f\u6210\u4e00\u4e2acube voxel \u6839\u636ecalibration matrix\u4ee5\u53cacube voxel\u4ece\u53cc\u76eeFeature \u4e0asample\u7279\u5f81\u3002 \u4f7f\u7528CNN\u5904\u7406sample\u5f97\u5230\u7684\u7279\u5f81\u8f93\u51fa\u6700\u7ec8\u7ed3\u679c Pseudo-Lidar pdf code \u8fd9\u7bc7paper\u7406\u8bba\u4e0a\u6765\u8bf4\u662fpseudo-lidar\u7684\u7b2c\u4e00\u7bc7\u6587\u7ae0 \u601d\u8def\u76ee\u524d\u56de\u770b\u6bd4\u8f83\u5730\u76f4\u63a5\uff0c\u53cc\u76ee\u6df1\u5ea6\u4f30\u8ba1\u65b9\u9762\uff0c\u4f5c\u8005\u4f7f\u7528pretrain PSMNet .\u6ce8\u610f\u8fd9\u4e2aPSMNet\u662f\u5728sceneflow\u6570\u636e\u96c6\uff0c\u4ee5\u53catraining set\u7684\u70b9\u4e91\u6570\u636e\u4f5c\u4e3a\u76d1\u7763\u7684\u3002lidar 3D\u68c0\u6d4b\u65b9\u9762\uff0c\u4f5c\u8005\u4f7f\u7528 AVOD Stereo R-CNN pdf code \u51e0\u4e2a\u8bad\u7ec3\u7ec6\u8282: positive anchors \u7684threshold\u63d0\u9ad8\u4e86\u3002 \u591a\u9884\u6d4b\u4e00\u4e2aKeypoint\u7684\u4f4d\u7f6e\uff0c\u5982\u4e0b\u56fe SSIM\uff0c\u5229\u7528\u53cc\u76ee\u7684disparity\uff0c\u540e\u5904\u7406\u4f18\u5316\u6df1\u5ea6\u503c\u3002 RT3D Stereo pdf \u8bad\u7ec3\u7ec6\u8282: \u4f7f\u7528\u5355\u4e00\u4e00\u4e2aResNet\u89e3\u51b32D \u68c0\u6d4b\u4ee5\u53ca\u8bed\u4e49\u5206\u5272\u7684encoding.\u7528\u7684\u662f \u4e8c\u4f5c\u4f5c\u8005\u7684\u540c\u65f6\u68c0\u6d4b\u4e0e\u8bed\u4e49\u5206\u5272\u7f51\u7edc.pdf Disparity\u4f7f\u7528\u7684\u662fblock matching\u7684\u4f20\u7edf\u65b9\u6cd5\uff0c \u4f5c\u8005\u6839\u636e\u8bed\u4e49\u5206\u5272\u4ee5\u53cadetector\u7ed3\u679c\u5206\u5272\u51fa\u76f8\u5173\u50cf\u7d20\uff0c\u7136\u540e\u805a\u7c7b\uff0c\u7136\u540e\u4ee5\u4f18\u5316\u51f8\u5305\u7684\u65b9\u5f0f\u5f97\u51fa\u7ed3\u679c\u3002\u7531\u4e8e\u4f5c\u8005\u6ca1\u6709\u5f00\u6e90\uff0c\u5f88\u591a\u5185\u5bb9\u6709\u5f85\u5546\u69b7\u3002 argmin (N_{out}/N+ (k_l(l-\u03bc_l)/\u03c3_l )^2+(k_w(w-\u03bc_w)/\u03c3_w )^2)","title":"Recent Collections for Stereo 3D detection"},{"location":"3dDetection/RecentCollectionForStereo3D/#recent-collections-for-stereo-3d-detection","text":"\u8fd1\u671f\u79ef\u6512\u4e86\u4e00\u7cfb\u5217\u53cc\u76ee3D\u68c0\u6d4bpaper\u7684\u9605\u8bfb\u3002\u8fd9\u91cc\u4e00\u6b21\u8fc7\u8fdb\u884c\u8bb0\u5f55,\u4ee5\u7ed3\u679c\u6392\u5217\u4e3a\u987a\u5e8f\u3002 \u8fd9\u91cc\u5217\u51fa\u76ee\u524d\u6709\u6587\u7ae0\u53ef\u5bfb\u7684KITTI\u6392\u884c\u699c(2020.04.09) Update: 2020.0409: Add Disp-RCNN and PL E2E. 2020.0714: Add CDN 2021.0126: Add RTS3D Methods Moderate Easy Hard Time LIGAStereo 64.66 % 81.39 % 57.22 % 0.4 s CDN 54.22 % 74.52 % 46.36 % 0.6 s CG-Stereo 53.58 % 74.39 % 46.50 % 0.57 s DSGN 52.18 % 73.50 % 45.14 % 0.67 s EGFN 46.39 % 65.80 % 38.42 % 0.09 s CDN P-LiDAR++ 44.86 % 64.31 % 38.11 % 0.4 s Pseudo-LiDAR E2E 43.92 % 64.75 % 38.14 % 0.4 s Pseudo-LiDAR++ 42.43 % 61.11 % 36.99 % 0.4 s YOLOStereo3D 40.71 % 65.77 % 29.99 % 0.08s Disp R-CNN (velo) 39.34 % 59.58 % 31.99 % 0.42 s ZoomNet 38.64 % 55.98 % 30.97 % 0.3 s OC Stereo 37.60 % 55.15 % 30.25 % 0.35 s RTS3D 37.38 % 58.51 % 31.12 % 0.03 s Pseudo-Lidar 34.05 % 54.53 % 28.25 % 0.4 s Stereo R-CNN 30.23 % 47.58 % 23.72 % 0.3 s RT3DStereo 23.28 % 29.90 % 18.96 % 0.08 s \u76ee\u5f55: Recent Collections for Stereo 3D detection LIGAStereo CG-stereo Pseudo-LiDAR E2E Pseudo-Lidar++ Disp-RCNN ZoomNet OC Stereo RTS3D Pseudo-Lidar Stereo R-CNN RT3D Stereo","title":"Recent Collections for Stereo 3D detection"},{"location":"3dDetection/RecentCollectionForStereo3D/#ligastereo","text":"pdf code \u8fd9\u7bc7paper\u662f\u8fd1\u671f\u6765\u6027\u80fd\u7b2c\u4e00\u4e2a\u8d85\u8fc7\u4e09/\u56db\u5e74\u524d\u70b9\u4e91\u68c0\u6d4b\u7684\u53cc\u76ee\u68c0\u6d4b\u7f51\u7edc\u3002\u6027\u80fd\u4e0a\u5168\u9762\u8d85\u8d8a\u53cc\u76ee\u6b64\u524d\u7684\u7f51\u7edc\u3002\u5176\u4e3b\u8981\u601d\u60f3\u662f\u4f7f\u7528\u7c7b\u4f3c\u4e8e DSGN \u7684\u7f51\u7edc\uff0c \u4f46\u662f\u4e8e\u6b64\u540c\u65f6\uff0c\u53e6\u5916\u8bad\u7ec3\u4e00\u4e2a\u70b9\u4e91\u68c0\u6d4b\u7f51\u7edc\uff0c\u4e24\u4e2a\u7f51\u7edc\u8f93\u51fa\u53d7\u76f8\u540c\u7684\u76d1\u7763\u635f\u5931\u8bad\u7ec3\uff0c\u6b64\u5916\u53cc\u76ee\u7684cost volume\u5206\u652f\u5728\u6700\u540e\u4e00\u5c42\u8fd8\u4f1a\u53d7\u70b9\u4e91\u6700\u540e\u4e00\u5c42\u7684\u77e5\u8bc6\u84b8\u998f\u8bad\u7ec3\u3002 \u77e5\u8bc6\u84b8\u998f\u635f\u5931: \\mathcal{L}_{i m}=\\sum_{\\mathcal{F}_{i m} \\in \\mathbb{F}_{i m}} \\frac{1}{N_{p o s}}\\left\\|M_{f g} M_{s p}\\left(g\\left(\\mathcal{F}_{i m}\\right)-\\frac{\\mathcal{F}_{i m}^{l i d a r}}{\\mathbb{E}\\left[\\left|\\mathcal{F}_{i m}^{l i d a r}\\right|\\right]}\\right)\\right\\|_{2}^{2}","title":"LIGAStereo"},{"location":"3dDetection/RecentCollectionForStereo3D/#cg-stereo","text":"pdf \u8fd9\u7bc7paper\u6765\u81ea\u4e8e Jason Ku\u90a3\u4e00\u7ec4\uff0c\u662f\u76ee\u524d(2020.03.15)Stereo\u7684SOTA\uff0c\u7ed9pseudo-lidar\u7cfb\u5217\u63d0\u4f9b\u4e86\u4e24\u4e2a\u5f88\u6709\u6548\u7684idea\u3002 \u9996\u5148\u662f\u53cc\u76ee\u4f30\u8ba1\u4e2d\uff0cforground\u4e0ebackground\u7684\u7279\u6027\u5dee\u8ddd\u662f\u5f88\u5927\u7684\uff0c\u4f5c\u8005\u8ba4\u4e3a\u5e94\u5f53\u4f7f\u7528\u4e24\u4e2a\u5206\u522b\u7684decoder\u5904\u7406\u524d\u666f\u4e0e\u80cc\u666f\u7684\u7269\u4f53\uff0c\u5206\u53c9\u7684\u4f9d\u636e\u662f\u8bed\u4e49\u5206\u5272\u7684\u7ed3\u679c\u3002 \u5176\u6b21\u662f\u53cc\u76ee\u4f30\u8ba1\u5f97\u5230\u7684\u70b9\u4e4b\u4e2d\u6709\u5f88\u591a\u7684\u566a\u97f3\uff0c\u4e0d\u540c\u70b9\u7684confidence\u4e0d\u540c\uff0c\u5bf9\u540e\u7aef\u70b9\u4e91\u7684\u5f71\u54cd\u5f88\u5927\uff0c\u4f5c\u8005\u8fd9\u91cc\u6839\u636estereo matching\u7684\u4e00\u4e2aconfidence map\u4f5c\u4e3a\u4e00\u4e2aattention \u5c42\u8f93\u5165\u5230\u70b9\u4e91\u540e\u7aef\u5904\u7406\u4e2d\u3002","title":"CG-stereo"},{"location":"3dDetection/RecentCollectionForStereo3D/#pseudo-lidar-e2e","text":"pdf code \u8fd9\u7bc7paper\u7684\u8d21\u732e\u975e\u5e38\u6709\u610f\u4e49\uff0c\u63d0\u5230\u7684\u662f\u8fc7\u53bb\u7684Pseudo-lidar\u7b97\u6cd5\u57fa\u672c\u90fd\u662f\u5b8c\u5168\u7684\u4e8c\u9636\u6bb5\u7b97\u6cd5\uff0c\u4e5f\u5c31\u662f\u53cc\u76ee\u751f\u6210\u70b9\u4e91\u4e0e\u70b9\u4e913D\u68c0\u6d4b\u4e4b\u95f4\u662f\u65e0\u6cd5End2End\u8bad\u7ec3\u7684\uff0c\u4e2d\u95f4\u7684\u8f6c\u6362\u8fc7\u7a0b\u662f\u4e0d\u53ef\u5bfc\u7684\uff0c\u56e0\u800c\u5c3d\u7ba1\u53ef\u4ee5fine-tune,\u4f46\u662f\u68af\u5ea6\u7684\u6d41\u52a8\u4f1a\u4e2d\u65ad,\u8fd9\u7bc7paper\u7684\u6700\u5927\u8d21\u732e\u5728\u4e8e\u63d0\u51fa\u4e00\u4e2aCoR\u6a21\u5757\u4f7f\u5f97\u540c\u65f6\u53ef\u4ee5\u8bad\u7ec3\u70b9\u4e91\u751f\u6210\u4ee5\u53ca\u57fa\u4e8e\u70b9\u4e91\u7684\u7269\u4f53\u68c0\u6d4b\u3002 \\boldsymbol{T}\\left(m, m^{\\prime}\\right)=\\left\\{\\begin{array}{cc} 0 & \\text { if }\\left|P_{m^{\\prime}}\\right|=0 \\\\ \\frac{1}{\\left|\\boldsymbol{P}_{m^{\\prime}}\\right|} \\sum_{\\boldsymbol{p} \\in \\boldsymbol{P}_{m^{\\prime}}} e^{-\\frac{\\left\\|\\boldsymbol{p}-\\hat{p}_{m}\\right\\|^{2}}{\\sigma^{2}}} & \\text { if }\\left|P_{m^{\\prime}}\\right|>0 \\end{array}\\right. \\boldsymbol{T}(m)=\\boldsymbol{T}(m, m)+\\frac{1}{\\left|\\mathcal{N}_{m}\\right|} \\sum_{m^{\\prime} \\in \\mathcal{N}_{m}} \\boldsymbol{T}\\left(m, m^{\\prime}\\right) voxel\u91cc\u9762\u6bcf\u4e00\u4e2abin\u90fd\u5bf9\u5e94\u4e00\u4e2a\u57fa\u7c7b\uff0c\u6bcf\u4e00\u4e2abin\u662f\u81ea\u5df1\u4e0e\u5468\u56f4\u7684\u52a0\u6743\u6c42\u548c\u3002\u4ece\u800cvoxel\u53ef\u5bfc","title":"Pseudo-LiDAR E2E"},{"location":"3dDetection/RecentCollectionForStereo3D/#pseudo-lidar","text":"pdf code \u8fd9\u7bc7paper\u7684\u4e3b\u8981\u65b0\u610f\u6709\u4e24\u70b9 \u5728\u6df1\u5ea6\u4f30\u8ba1\u4e0a\uff0c\u4f5c\u8005\u7ed9\u51fa\u4e00\u4e2a\u65b0\u7684insight\uff0c\u5c31\u662f\u5747\u5300\u76843D\u5377\u79ef\u5f88\u53ef\u80fd\u662fdisparity-based cost volome\u7684\u4e00\u4e2aerror source\uff0c\u6bd4\u5982\u8bf4\u5bf9\u4e8edisparity\u6bd4\u8f83\u9ad8\u7684\u70b9\uff0c\u53ef\u4ee5smooth out\uff0c\u4f46\u662f\u5bf9\u4e8edisparity\u6bd4\u8f83\u5c0f\u7684\u70b9\u5219\u4e0d\u5e94\u8be5\u540c\u7b49\u7ea7\u522b\u7684smooth out(\u4f1a\u4ea7\u751f\u5f88\u5927\u8bef\u5dee)\u3002\u6240\u4ee5\u4f5c\u8005\u5c06disparity cost volume\u7684\u6df1\u5ea6\u65b9\u5411\u6c42\u5012\u6570\uff0c\u5e76\u7ebf\u6027\u63d2\u503c\u5f97\u5230depth cost volume,\u7136\u540e\u5728depth cost volume\u4e0a\u9762\u505a3D\u5377\u79ef\u3002 \u5728\u540e\u5904\u7406\u4e0a\uff0c\u4f5c\u8005\u878d\u5408\u4e86\u6df1\u5ea6\u8865\u5168(depth completion)\u7684\u601d\u60f3\uff0c\u7531\u4e8edisparity\u662f\u79bb\u6563\u7684\uff0c\u6240\u4ee5\u4f1a\u5f15\u8d77\u5f88\u591a\u4e0d\u5e94\u8be5\u7684\u8bef\u5dee\uff0c\u8fdb\u800c\u4f5c\u8005\u8003\u8651\u4f7f\u7528\u4f4e\u7ebf\u6570\u7684lidar(\u5f00\u6e90\u7684\u4e00\u4e2a\u65b9\u6848\u7ebf\u6570\u662f4)\u4f5c\u4e3a\u4e00\u4e2aground truth\u7684\u8865\u507f\u3002\u8fd9\u91cc\u4e0d\u8fdb\u4e00\u6b65\u5c55\u5f00\u3002 \u4f5c\u8005\u5728KITTI\u4e0a\u63d0\u4ea4\u4e86\u4e24\u4e2a\u6210\u7ee9(PL++),\u6807\u9898\u4e0b\u9762\u7ed9\u51fa\u7684\u662f\u6625\u53cc\u76ee\u800c\u6ca1\u6709GDC\u7684\u6210\u7ee9\uff0c\u6709GDC\u7684\u6210\u7ee9\u4f1a\u66f4\u9ad8\u4e00\u4e9b\u3002","title":"Pseudo-Lidar++"},{"location":"3dDetection/RecentCollectionForStereo3D/#disp-rcnn","text":"pdf code \u8fd9\u7bc7paper\u7684\u4e3b\u8981\u4e0d\u540c\u70b9\u5728\u4e8e\u4f7f\u7528RoIPooling\u4ece\u539f\u56fe(\u4f5c\u8005\u6307\u51fa\u4e0d\u5e94\u8be5\u4ecefeature\u4e2d\u91c7\u6837\uff0c\u56e0\u4e3aDisparity\u8981\u6c42\u90bb\u8fd1\u50cf\u7d20\u7ed3\u679c\u4e0d\u540c\uff0c\u4f46\u662fInstance Segmentation\u4f1a\u8981\u6c42\u90bb\u8fd1\u50cf\u7d20\u7ed3\u679c\u76f8\u540c)\u4e2d\u91c7\u6837,\u7136\u540e\u4ecePooling\u540e\u7684\u7ed3\u679c\u91cd\u5efaDisparity\u4ee5\u53ca\u5c40\u90e8\u70b9\u4e91(\u8fd9\u4e2a\u6a21\u5757\u5728\u6570\u5b66\u5904\u7406\u4e0a\u8981\u5c0f\u5fc3)\uff0c\u7136\u540e\u4f7f\u7528\u70b9\u4e91\u68c0\u6d4b\u8f93\u51fa\u7ed3\u679c\u3002 \u8fd9\u7bc7paper\u8fd8\u6709\u4f7f\u7528pretrained\u7684\u53cc\u76ee\u91cd\u5efa\u7f51\u7edc\uff0c\u5f97\u5230\u5bc6\u96c6\u7684pseudo Ground Truth Disparity. Loss\u7684\u6784\u6210\u6bd4\u8f83\u590d\u6742\uff0c\u5177\u4f53\u770bpaper","title":"Disp-RCNN"},{"location":"3dDetection/RecentCollectionForStereo3D/#zoomnet","text":"pdf code \u4f7f\u75282D\u68c0\u6d4b\u5148\u5f97\u5230\u4e24\u4e2a\u8f66\u5b50\u548b\u56fe\u7247\u7684\u4f4d\u7f6e\uff0c\u7136\u540e\u5206\u522bresize,\u5e76\u4e14\u8c03\u8282\u540d\u4e49\u76f8\u673a\u53c2\u6570(zooming)\u3002 \u4e2d\u4ecb\u8f85\u52a9\u9884\u6d4b\u7684\u5185\u5bb9\u5305\u62ecdisparity, instance segmentation, part location(\u6bcf\u4e00\u4e2a\u50cf\u7d20\u76f8\u5bf9\u4e8e\u8f66\u5b50\u4e2d\u5fc3x, y, z\u8f74\u7684\u4f4d\u7f6e\uff0c\u8fd9\u4e2a\u4e00\u822c\u4f7f\u7528\u70b9\u4e91\u548c\u7a20\u5bc6\u6df1\u5ea6\u56fe\u8fdb\u884c\u6807\u6ce8)\u3002\u5f97\u5230\u70b9\u4e91\u540e\u5c06feature \u94fe\u63a5\uff0c\u7136\u540e\u7528\u7c7b\u4f3c\u4e8epoint net\u7684\u65b9\u5f0f\u9884\u6d4b\u6700\u7ec8\u7ed3\u679c\u3002","title":"ZoomNet"},{"location":"3dDetection/RecentCollectionForStereo3D/#oc-stereo","text":"pdf \u8fd9\u7bc7paper\u7684\u60f3\u6cd5\u662f\u4f7f\u7528RoIAlign\u5c06\u5de6\u53f3\u76ee\u4e24\u4e2a\u533a\u57df\u7684feature \u63d0\u51fa\u6765\uff0c\u7136\u540e\u4f7f\u7528instance seg\u4e0eCost volumn\u8ba1\u7b97\u5bf9\u5e94pixel\u5904\u7684disparity\u3002 \u4f5c\u8005\u5bf9\u4e8eRoIAlign\u524d\u540e\u7684segmentation pixel\u7684\u4f4d\u7f6e\u5173\u7cfb\u505a\u4e86\u5f88\u7ec6\u81f4\u7684\u89e3\u91ca\u3002 \u5728\u5f97\u5230\u5c40\u90e8RGB\u70b9\u4e91\u4e4b\u540e\u4f5c\u8005\u4f7f\u7528 AVOD \u8fdb\u884c3D\u68c0\u6d4b\u3002","title":"OC Stereo"},{"location":"3dDetection/RecentCollectionForStereo3D/#rts3d","text":"pdf code \u8fd9\u7bc7paper\u63a8\u7406\u65f6\u7684\u64cd\u4f5c\u662f\u5148\u8ba9\u4e00\u4e2a\u5355\u76ee\u68c0\u6d4b\u7f51\u7edc\u8f93\u51fa\u4e00\u4e2a\u68c0\u6d4b\u7ed3\u679c\u3002 \u7136\u540e\u53e6\u5916\u4f7f\u7528\u4e00\u4e2a\u7c7b\u4f3cpointnet\u7684\u673a\u5236\u53bb\u589e\u5f3amono\u7684\u7ed3\u679c. \u7528\u4e24\u4e2aresnet\u5206\u522b\u8f93\u51fa\u4e24\u5f20\u56fe\u7684feature \u5728predicted bounding box\u7684\u9644\u8fd1\u751f\u6210\u4e00\u4e2acube voxel \u6839\u636ecalibration matrix\u4ee5\u53cacube voxel\u4ece\u53cc\u76eeFeature \u4e0asample\u7279\u5f81\u3002 \u4f7f\u7528CNN\u5904\u7406sample\u5f97\u5230\u7684\u7279\u5f81\u8f93\u51fa\u6700\u7ec8\u7ed3\u679c","title":"RTS3D"},{"location":"3dDetection/RecentCollectionForStereo3D/#pseudo-lidar_1","text":"pdf code \u8fd9\u7bc7paper\u7406\u8bba\u4e0a\u6765\u8bf4\u662fpseudo-lidar\u7684\u7b2c\u4e00\u7bc7\u6587\u7ae0 \u601d\u8def\u76ee\u524d\u56de\u770b\u6bd4\u8f83\u5730\u76f4\u63a5\uff0c\u53cc\u76ee\u6df1\u5ea6\u4f30\u8ba1\u65b9\u9762\uff0c\u4f5c\u8005\u4f7f\u7528pretrain PSMNet .\u6ce8\u610f\u8fd9\u4e2aPSMNet\u662f\u5728sceneflow\u6570\u636e\u96c6\uff0c\u4ee5\u53catraining set\u7684\u70b9\u4e91\u6570\u636e\u4f5c\u4e3a\u76d1\u7763\u7684\u3002lidar 3D\u68c0\u6d4b\u65b9\u9762\uff0c\u4f5c\u8005\u4f7f\u7528 AVOD","title":"Pseudo-Lidar"},{"location":"3dDetection/RecentCollectionForStereo3D/#stereo-r-cnn","text":"pdf code \u51e0\u4e2a\u8bad\u7ec3\u7ec6\u8282: positive anchors \u7684threshold\u63d0\u9ad8\u4e86\u3002 \u591a\u9884\u6d4b\u4e00\u4e2aKeypoint\u7684\u4f4d\u7f6e\uff0c\u5982\u4e0b\u56fe SSIM\uff0c\u5229\u7528\u53cc\u76ee\u7684disparity\uff0c\u540e\u5904\u7406\u4f18\u5316\u6df1\u5ea6\u503c\u3002","title":"Stereo R-CNN"},{"location":"3dDetection/RecentCollectionForStereo3D/#rt3d-stereo","text":"pdf \u8bad\u7ec3\u7ec6\u8282: \u4f7f\u7528\u5355\u4e00\u4e00\u4e2aResNet\u89e3\u51b32D \u68c0\u6d4b\u4ee5\u53ca\u8bed\u4e49\u5206\u5272\u7684encoding.\u7528\u7684\u662f \u4e8c\u4f5c\u4f5c\u8005\u7684\u540c\u65f6\u68c0\u6d4b\u4e0e\u8bed\u4e49\u5206\u5272\u7f51\u7edc.pdf Disparity\u4f7f\u7528\u7684\u662fblock matching\u7684\u4f20\u7edf\u65b9\u6cd5\uff0c \u4f5c\u8005\u6839\u636e\u8bed\u4e49\u5206\u5272\u4ee5\u53cadetector\u7ed3\u679c\u5206\u5272\u51fa\u76f8\u5173\u50cf\u7d20\uff0c\u7136\u540e\u805a\u7c7b\uff0c\u7136\u540e\u4ee5\u4f18\u5316\u51f8\u5305\u7684\u65b9\u5f0f\u5f97\u51fa\u7ed3\u679c\u3002\u7531\u4e8e\u4f5c\u8005\u6ca1\u6709\u5f00\u6e90\uff0c\u5f88\u591a\u5185\u5bb9\u6709\u5f85\u5546\u69b7\u3002 argmin (N_{out}/N+ (k_l(l-\u03bc_l)/\u03c3_l )^2+(k_w(w-\u03bc_w)/\u03c3_w )^2)","title":"RT3D Stereo"},{"location":"3dDetection/RefinedMPL/","text":"RefinedMPL: Refined Monocular PseudoLiDAR for 3D Object Detection in Autonomous Driving \u8fd9\u7bc7\u8bba\u6587\u7684\u76ee\u6807\u5728\u4e8e\u4f18\u5316pseudolidar\u7684\u524d\u7aef\u90e8\u5206(\u70b9\u7684\u751f\u6210\u4e0e\u6570\u91cf\u7cbe\u7b80)\u3002\u4f5c\u8005\u5199\u4e86\u8f83\u957f\u7684introduction\u8bf4\u660e\u4e86\u672c\u6587\u7684\u52a8\u673a\u3002\u5728\u76ee\u524d\u7684pseudo-lidar\u6846\u67b6\u4e0b\uff0c\u6211\u4eec\u9700\u8981\u6839\u636eRGB\u56fe\u7247\u751f\u6210\u4e00\u4e2a\u5bc6\u96c6\u7684\u70b9\u4e91\u6df1\u5ea6\u4f30\u8ba1\uff0c\u4f46\u662f\u7b80\u5355\u8ba1\u7b97\u53ef\u4ee5\u53d1\u73b0\uff0c\u8fd9\u79cd\u65b9\u6cd5\u751f\u6210\u7684\u5bc6\u96c6\u70b9\u4e91\u6bd464\u7ebf\u751a\u81f3128\u7ebflidar\u751f\u6210\u7684\u70b9\u4e91\u66f4\u52a0\u5bc6\u96c6\uff0c\u5c24\u5176\u662f\u5bf9\u8ddd\u79bb\u8f83\u4e3a\u8fd1\u7684\u7269\u4f53\uff0c\u53ef\u77e5pseudo-lidar\u7684\u5bc6\u96c6\u70b9\u4e91\u5176\u5b9e\u5bf9\u68c0\u6d4b\u7cbe\u5ea6\u6ca1\u6709\u4ec0\u4e48\u663e\u8457\u7684\u63d0\u5347\uff0c\u56e0\u800c\u7cbe\u7b80\u70b9\u4e91\u5bf9\u8ba1\u7b97\u901f\u5ea6\u4ee5\u53ca\u6027\u80fd\u90fd\u6709\u5e2e\u52a9. \u5728\u4e0d\u8003\u8651\u76ee\u6807\u68c0\u6d4b\u7684\u60c5\u51b5\u4e0b\uff0c\u672c\u6587\u63d0\u4f9b\u4e86\u4e0b\u91c7\u6837\u70b9\u4e91\u7684\u65b9\u5f0f.(\u672a\u5f00\u6e90) \u6d41\u7a0b\u603b\u89c8 \u672c\u6587\u63d0\u4f9b\u4e86\u4e24\u79cd\u65b9\u5f0f\uff0c\u4e00\u79cd\u662f\u6709\u76d1\u7763\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u70b9\u4e91\u9884\u5904\u7406\uff0c\u4e00\u79cd\u65b9\u5f0f\u662f\u65e0\u76d1\u7763\u5730\u4f7f\u7528\u70b9\u4e91\u9884\u5904\u7406\u3002\u672c\u6587\u8fd9\u91cc\u7528\u7684\u662fSparsification,\u56e0\u4e3a\u4e3b\u8981\u76ee\u6807\u662f\u6ee4\u6389\u4e0d\u9700\u8981\u7684\u8fc7\u591a\u7684\u70b9 \u76d1\u7763\u7684\u65b9\u5f0f\uff1a * \u8bed\u4e49\u5206\u5272\u7f51\u7edc\u540c\u65f6\u8f93\u51fa\u6df1\u5ea6\u4f30\u8ba1\u4ee5\u53ca2D\u76ee\u6807\u68c0\u6d4b\u6846 * \u4ece2D\u76ee\u6807\u68c0\u6d4b\u6846\u4e2d\u9009\u51fa\u6846\u5185\u7684\u70b9 * DSD\u91c7\u6837 \u975e\u76d1\u7763\u65b9\u5f0f\uff1a * \u5355\u76ee\u8f93\u51fa\u6df1\u5ea6\u4f30\u8ba1 * \u4ece\u539f\u56fe\u8f93\u51fa2D\u5173\u952e\u70b9(LoG maximum) * \u6295\u5f71\u5173\u952e\u70b9 * \u80cc\u666f\u70b9\u5206\u79bb * DSD\u91c7\u6837 DSD\u91c7\u6837 \u968f\u673a\u4e0b\u91c7\u6837\u7684\u95ee\u9898\u5728\u4e0e\u8fd9\u6837\u5bb9\u6613\u5927\u5e45\u5ea6\u5730\u51cf\u5c11\u8f83\u8fdc\u5904\u8f83\u5c0f\u7684(\u540c\u65f6\u8f83\u4e3a\u96be\u7684)\u7269\u4f53\u7684\u70b9\uff0c\u8fd9\u6837\u4f1a\u4f7f\u5f97detection\u7ed3\u679c\u53d8\u5dee\uff0c\u4f5c\u8005\u7684\u601d\u8def\u662f\u6839\u636e\u70b9\u5230\u76f8\u673a\u7684\u8ddd\u79bb\u8fdb\u884c\u91c7\u6837(\u4e2a\u4eba\u611f\u89c9\u63cf\u8ff0\u4e0d\u6e05) \u672c\u6587\u540e\u7eed\u76843D\u68c0\u6d4b\u4f7f\u7528\u7684\u662fpointRCNN\uff0c\u6700\u7ec8\u5b9e\u73b0\u4e86\u76ee\u524d(2019-12-24\u67e5\u770b)\u7684SOTA\u7b2c\u4e8c\u540d","title":"RefinedMPL: Refined Monocular PseudoLiDAR for 3D Object Detection in Autonomous Driving"},{"location":"3dDetection/RefinedMPL/#refinedmpl-refined-monocular-pseudolidar-for-3d-object-detection-in-autonomous-driving","text":"\u8fd9\u7bc7\u8bba\u6587\u7684\u76ee\u6807\u5728\u4e8e\u4f18\u5316pseudolidar\u7684\u524d\u7aef\u90e8\u5206(\u70b9\u7684\u751f\u6210\u4e0e\u6570\u91cf\u7cbe\u7b80)\u3002\u4f5c\u8005\u5199\u4e86\u8f83\u957f\u7684introduction\u8bf4\u660e\u4e86\u672c\u6587\u7684\u52a8\u673a\u3002\u5728\u76ee\u524d\u7684pseudo-lidar\u6846\u67b6\u4e0b\uff0c\u6211\u4eec\u9700\u8981\u6839\u636eRGB\u56fe\u7247\u751f\u6210\u4e00\u4e2a\u5bc6\u96c6\u7684\u70b9\u4e91\u6df1\u5ea6\u4f30\u8ba1\uff0c\u4f46\u662f\u7b80\u5355\u8ba1\u7b97\u53ef\u4ee5\u53d1\u73b0\uff0c\u8fd9\u79cd\u65b9\u6cd5\u751f\u6210\u7684\u5bc6\u96c6\u70b9\u4e91\u6bd464\u7ebf\u751a\u81f3128\u7ebflidar\u751f\u6210\u7684\u70b9\u4e91\u66f4\u52a0\u5bc6\u96c6\uff0c\u5c24\u5176\u662f\u5bf9\u8ddd\u79bb\u8f83\u4e3a\u8fd1\u7684\u7269\u4f53\uff0c\u53ef\u77e5pseudo-lidar\u7684\u5bc6\u96c6\u70b9\u4e91\u5176\u5b9e\u5bf9\u68c0\u6d4b\u7cbe\u5ea6\u6ca1\u6709\u4ec0\u4e48\u663e\u8457\u7684\u63d0\u5347\uff0c\u56e0\u800c\u7cbe\u7b80\u70b9\u4e91\u5bf9\u8ba1\u7b97\u901f\u5ea6\u4ee5\u53ca\u6027\u80fd\u90fd\u6709\u5e2e\u52a9. \u5728\u4e0d\u8003\u8651\u76ee\u6807\u68c0\u6d4b\u7684\u60c5\u51b5\u4e0b\uff0c\u672c\u6587\u63d0\u4f9b\u4e86\u4e0b\u91c7\u6837\u70b9\u4e91\u7684\u65b9\u5f0f.(\u672a\u5f00\u6e90)","title":"RefinedMPL: Refined Monocular PseudoLiDAR for 3D Object Detection in Autonomous Driving"},{"location":"3dDetection/RefinedMPL/#_1","text":"\u672c\u6587\u63d0\u4f9b\u4e86\u4e24\u79cd\u65b9\u5f0f\uff0c\u4e00\u79cd\u662f\u6709\u76d1\u7763\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u70b9\u4e91\u9884\u5904\u7406\uff0c\u4e00\u79cd\u65b9\u5f0f\u662f\u65e0\u76d1\u7763\u5730\u4f7f\u7528\u70b9\u4e91\u9884\u5904\u7406\u3002\u672c\u6587\u8fd9\u91cc\u7528\u7684\u662fSparsification,\u56e0\u4e3a\u4e3b\u8981\u76ee\u6807\u662f\u6ee4\u6389\u4e0d\u9700\u8981\u7684\u8fc7\u591a\u7684\u70b9 \u76d1\u7763\u7684\u65b9\u5f0f\uff1a * \u8bed\u4e49\u5206\u5272\u7f51\u7edc\u540c\u65f6\u8f93\u51fa\u6df1\u5ea6\u4f30\u8ba1\u4ee5\u53ca2D\u76ee\u6807\u68c0\u6d4b\u6846 * \u4ece2D\u76ee\u6807\u68c0\u6d4b\u6846\u4e2d\u9009\u51fa\u6846\u5185\u7684\u70b9 * DSD\u91c7\u6837 \u975e\u76d1\u7763\u65b9\u5f0f\uff1a * \u5355\u76ee\u8f93\u51fa\u6df1\u5ea6\u4f30\u8ba1 * \u4ece\u539f\u56fe\u8f93\u51fa2D\u5173\u952e\u70b9(LoG maximum) * \u6295\u5f71\u5173\u952e\u70b9 * \u80cc\u666f\u70b9\u5206\u79bb * DSD\u91c7\u6837","title":"\u6d41\u7a0b\u603b\u89c8"},{"location":"3dDetection/RefinedMPL/#dsd","text":"\u968f\u673a\u4e0b\u91c7\u6837\u7684\u95ee\u9898\u5728\u4e0e\u8fd9\u6837\u5bb9\u6613\u5927\u5e45\u5ea6\u5730\u51cf\u5c11\u8f83\u8fdc\u5904\u8f83\u5c0f\u7684(\u540c\u65f6\u8f83\u4e3a\u96be\u7684)\u7269\u4f53\u7684\u70b9\uff0c\u8fd9\u6837\u4f1a\u4f7f\u5f97detection\u7ed3\u679c\u53d8\u5dee\uff0c\u4f5c\u8005\u7684\u601d\u8def\u662f\u6839\u636e\u70b9\u5230\u76f8\u673a\u7684\u8ddd\u79bb\u8fdb\u884c\u91c7\u6837(\u4e2a\u4eba\u611f\u89c9\u63cf\u8ff0\u4e0d\u6e05) \u672c\u6587\u540e\u7eed\u76843D\u68c0\u6d4b\u4f7f\u7528\u7684\u662fpointRCNN\uff0c\u6700\u7ec8\u5b9e\u73b0\u4e86\u76ee\u524d(2019-12-24\u67e5\u770b)\u7684SOTA\u7b2c\u4e8c\u540d","title":"DSD\u91c7\u6837"},{"location":"3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/","text":"SHIFT R-CNN: DEEP MONOCULAR 3D OBJECT DETECTION WITH CLOSED-FORM GEOMETRIC CONSTRAINTS \u8fd9\u7bc7\u8bba\u6587\u7684\u5728\u7406\u8bba\u4e0a\u4e3b\u8981\u662f\u4e09\u5927\u8d21\u732e\uff0c\u7b2c\u4e00\u662f\u4e09\u6b65\u8d70\u7684\u57fa\u4e8eFaster-RCNN\u7684 Shift R-CNN\uff0c\u7b2c\u4e8c\u662fVolume Displacement Loss (VDL)\u7528\u4e8e\u8bad\u7ec3\u7f51\u7edc\u3002 \u5de5\u4f5c\u6d41\u7a0b 2D \u68c0\u6d4b\u4e0e3D\u7ed3\u6784\u53c2\u6570\u4f30\u8ba1 \u4f7f\u7528Faster-RCNN\u7684RPN\u8f93\u51faProposal\u4ee5\u53ca2D\u6846\u9884\u6d4b\uff0c\u540e\u7eed\u5168\u8fde\u63a5\u5c42\u8f93\u51fa\u5206\u7c7b\u3001\u7269\u4f53\u5927\u5c0f\u4ee5\u53ca\u7269\u4f53\u65b9\u5411\u3002 \u7269\u4f53\u65b9\u5411\u56de\u5f52\u540c\u6837\u5f97\u9009\u62e9\u56de\u5f52 sin , cos \u503c\uff0c\u540c\u65f6\u591a\u4e00\u4e2a L_1 cost\u8981\u6c42 sin^2 + cos^2 = 1 \uff0c\u8fd9\u91cc\u53ea\u8981\u6c42\u8f93\u51fa\u89c2\u6d4b\u89d2\\alpht_L \u4e5f\u5c31\u662f \\alpha_G - \\theta_{ray}$ \u7269\u4f53\u5927\u5c0f\u56de\u5f52\u540c\u6837\u9009\u62e9\u56de\u5f52 log \u503c\uff0c \u6700\u7ec8\u52a0\u6743\u8f93\u51fa\u603b\u548c \u95ed\u73af\u7ea6\u675f\u6c42\u51fa\u76f8\u5bf9\u4f4d\u7f6e \u8fd9\u4e2a\u95ee\u9898\u6700\u7ec8\u80fd\u8f6c\u6362\u4e3a\u4e00\u4e2a\u6700\u5c0f\u4e8c\u4e58\u7684\u95ee\u9898 ShiftNet\u8fdb\u4e00\u6b65\u4f18\u5316 \u628a\u4e0a\u4e00\u90e8\u5206\u4ee5\u53ca\u7b2c\u4e00\u90e8\u5206\u7684\u4fe1\u606f\uff0c\u5305\u62ec t, \\bold b_{2D}\uff0c \\bold d, (sin(\\alpha_L), cos(\\alpha_L)), (sin(\\alpha_G), cos(\\alpha_G)) ,\u8f93\u5165\u5230\u4e24\u5c42\u5168\u8fde\u63a5\u5c42\u4e2d\u7136\u540e\u8f93\u51fa\u6700\u7ec8\u76ee\u6807\u3002 Volume Displacement Loss \u76ee\u7684\u662f\u6b63\u786e\u5730\u63d0\u53473D IOU,\u4f46\u662f3D IOU\u76f4\u63a5\u641e\u5e76\u4e0d\u53ef\u5bfc\u3002\u8fd9\u91cc\u7ed9\u51fa\u65b0\u7684\u601d\u8def, \\Delta t \u4e3a\u4e16\u754c\u5750\u6807\u4e2d\u7684 x, y, z \u5dee\u503c \\Delta t_{\\alpha G} = R_y(\\alpha G) \\Delta t L = w \\times h \\times |\\Delta x_{\\alpha G}| + w \\times l \\times |\\Delta y_{\\alpha G}| + h \\times l \\times |\\Delta z_{\\alpha G}|","title":"SHIFT R-CNN: DEEP MONOCULAR 3D OBJECT DETECTION WITH CLOSED-FORM GEOMETRIC CONSTRAINTS"},{"location":"3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/#shift-r-cnn-deep-monocular-3d-object-detection-with-closed-form-geometric-constraints","text":"\u8fd9\u7bc7\u8bba\u6587\u7684\u5728\u7406\u8bba\u4e0a\u4e3b\u8981\u662f\u4e09\u5927\u8d21\u732e\uff0c\u7b2c\u4e00\u662f\u4e09\u6b65\u8d70\u7684\u57fa\u4e8eFaster-RCNN\u7684 Shift R-CNN\uff0c\u7b2c\u4e8c\u662fVolume Displacement Loss (VDL)\u7528\u4e8e\u8bad\u7ec3\u7f51\u7edc\u3002","title":"SHIFT R-CNN: DEEP MONOCULAR 3D OBJECT DETECTION WITH CLOSED-FORM GEOMETRIC CONSTRAINTS"},{"location":"3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/#_1","text":"","title":"\u5de5\u4f5c\u6d41\u7a0b"},{"location":"3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/#2d-3d","text":"\u4f7f\u7528Faster-RCNN\u7684RPN\u8f93\u51faProposal\u4ee5\u53ca2D\u6846\u9884\u6d4b\uff0c\u540e\u7eed\u5168\u8fde\u63a5\u5c42\u8f93\u51fa\u5206\u7c7b\u3001\u7269\u4f53\u5927\u5c0f\u4ee5\u53ca\u7269\u4f53\u65b9\u5411\u3002 \u7269\u4f53\u65b9\u5411\u56de\u5f52\u540c\u6837\u5f97\u9009\u62e9\u56de\u5f52 sin , cos \u503c\uff0c\u540c\u65f6\u591a\u4e00\u4e2a L_1 cost\u8981\u6c42 sin^2 + cos^2 = 1 \uff0c\u8fd9\u91cc\u53ea\u8981\u6c42\u8f93\u51fa\u89c2\u6d4b\u89d2\\alpht_L \u4e5f\u5c31\u662f \\alpha_G - \\theta_{ray}$ \u7269\u4f53\u5927\u5c0f\u56de\u5f52\u540c\u6837\u9009\u62e9\u56de\u5f52 log \u503c\uff0c \u6700\u7ec8\u52a0\u6743\u8f93\u51fa\u603b\u548c","title":"2D \u68c0\u6d4b\u4e0e3D\u7ed3\u6784\u53c2\u6570\u4f30\u8ba1"},{"location":"3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/#_2","text":"\u8fd9\u4e2a\u95ee\u9898\u6700\u7ec8\u80fd\u8f6c\u6362\u4e3a\u4e00\u4e2a\u6700\u5c0f\u4e8c\u4e58\u7684\u95ee\u9898","title":"\u95ed\u73af\u7ea6\u675f\u6c42\u51fa\u76f8\u5bf9\u4f4d\u7f6e"},{"location":"3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/#shiftnet","text":"\u628a\u4e0a\u4e00\u90e8\u5206\u4ee5\u53ca\u7b2c\u4e00\u90e8\u5206\u7684\u4fe1\u606f\uff0c\u5305\u62ec t, \\bold b_{2D}\uff0c \\bold d, (sin(\\alpha_L), cos(\\alpha_L)), (sin(\\alpha_G), cos(\\alpha_G)) ,\u8f93\u5165\u5230\u4e24\u5c42\u5168\u8fde\u63a5\u5c42\u4e2d\u7136\u540e\u8f93\u51fa\u6700\u7ec8\u76ee\u6807\u3002","title":"ShiftNet\u8fdb\u4e00\u6b65\u4f18\u5316"},{"location":"3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/#volume-displacement-loss","text":"\u76ee\u7684\u662f\u6b63\u786e\u5730\u63d0\u53473D IOU,\u4f46\u662f3D IOU\u76f4\u63a5\u641e\u5e76\u4e0d\u53ef\u5bfc\u3002\u8fd9\u91cc\u7ed9\u51fa\u65b0\u7684\u601d\u8def, \\Delta t \u4e3a\u4e16\u754c\u5750\u6807\u4e2d\u7684 x, y, z \u5dee\u503c \\Delta t_{\\alpha G} = R_y(\\alpha G) \\Delta t L = w \\times h \\times |\\Delta x_{\\alpha G}| + w \\times l \\times |\\Delta y_{\\alpha G}| + h \\times l \\times |\\Delta z_{\\alpha G}|","title":"Volume Displacement Loss"},{"location":"3dDetection/SSL_RTM3D/","text":"Monocular 3D Detection with Geometric Constraints Embedding and Semi-supervised Training \u8fd9\u7bc7 paper\u4e0e RTM3D \u662f\u540c\u4e00\u4f5c\u8005\uff0c\u601d\u8def\u4e5f\u662f\u4e00\u8109\u76f8\u627f\uff0c\u9700\u8981\u6ce8\u610f\u7684\u662f\u8fd9\u91cc\u5173\u4e8esemi-supervised training\u7684\u5b9a\u4e49\u662f\u4f7f\u7528\u7684ground truth \u66f4\u5c11\uff0c\u540c\u65f6\u53ef\u4ee5\u5728test set\u4e0a\u9762\u505afine-tune\u4f7f\u5f97\u7ed3\u679c\u66f4\u9c81\u68d2. \u5176\u6700\u7ec8\u6027\u80fd\u5f88\u4e0d\u9519\uff0c\u5177\u4f53\u5728 Collections for Mono3D \u91cc\u9762\u6709\u6392\u540d\u3002 KM3D-Net \u6838\u5fc3\u76d1\u7763\u7ed3\u6784\u5982\u56fe \u67b6\u6784\u4e0a\u5ef6\u7eed\u4e86 Object as Point \u4ee5\u53ca] RTM3D \u7684\u601d\u8def\uff0c\u70ed\u56fe\u9884\u6d4b\u89d2\u70b9\u4e3a\u6838\u5fc3\u7279\u70b9. \u53e6\u5916\u53ef\u4ee5\u770b\u5230\u6700\u5927\u7684\u4e0d\u540c\u5728\u4e8e\u4e0d\u518d\u76f4\u63a5\u9884\u6d4b\u6df1\u5ea6\u503c\uff0c3D \u4f4d\u7f6e\u8f6c\u800c\u901a\u8fc7\u6700\u4f18\u5316\u8fdb\u884c\u6c42\u89e3\uff0c\u5e76\u4e14\u7531\u4e8e\u8fd0\u7b97\u8fc7\u7a0b\u53ef\u5bfc\uff0c\u4e5f\u4f7f\u5f97\u8ddd\u79bb\u503c\u80fd\u591f\u76f4\u63a5\u53cd\u9988\u5230\u8f93\u51fa\u7aef\u3002(\u4e2a\u4eba\u7406\u89e3\u8fd9\u662f\u4e00\u4e2a\u5bf9\u4e8eanchor free\u6bd4\u8f83\u5927\u7684\u7a81\u7834\uff0c\u5728\u6b64\u4e4b\u524danchor free\u6ca1\u6cd5\u901a\u8fc7\u7269\u4f532D bounding boxes\u7684\u5927\u5c0f\u5f97\u5230\u6df1\u5ea6\u503c\u7684\u5148\u9a8c\uff0c\u5f71\u54cd\u56de\u5f52\u7b97\u6cd5\u7684\u7cbe\u5ea6\uff0c\u5982\u4eca\u672c\u6587\u4f7f\u7528\u4f2a\u9006\u6c42\u89e3\u5c06\u7ea6\u675f\u6db5\u76d6\u5728\u8fd0\u7b97\u8fc7\u7a0b\u4e2d\uff0c\u7406\u8bba\u4e0a\u53ef\u4ee5\u5927\u5e45\u5ea6\u63d0\u5347anchor-free\u7684\u4e0a\u9650,\u5728\u591a\u81ea\u7531\u5ea6\u7684\u6570\u636e\u96c6 cityscapes \u4e0a\uff0c\u8fd9\u4e5f\u5f00\u542f\u4e86\u4f7f\u7528 BPnP \u7684\u53ef\u80fd\u6027) \u6700\u540e\u6c42\u89e3 [X,Y,Z] \u7684\u7ebf\u6027\u7cfb\u7edf\u65b9\u7a0b\u4e3a: Unsupervised \u672c\u6587\u7ed9\u51fa\u7684\u6240\u8c13 unsupervised\u7684\u601d\u8def\u4e3b\u8981\u662f\u4e3a\u4e86\u63d0\u9ad8\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\uff0c\u4e5f\u5c31\u662f\u5728 unlabeled data\u4e0a\uff0c\u8981\u6c42\u7ecf\u5386\u4e0d\u540c\u6570\u636e\u589e\u5f3a\u540e\u7684\u9884\u6d4b\u503c\u76f8\u4f3c\u5ea6\u6bd4\u8f83\u9ad8\u3002\u8fd9\u4e2a\u672c\u8d28\u4e0a\u4e0d\u80fd\u66ff\u4ee3 supervised\u7684\u4f5c\u7528\uff0csupervised\u4f9d\u7136\u5f88\u91cd\u8981\uff0c\u4f46\u662f\u8fd9\u4e2a\u65b9\u6cd5\u5728\u4f7f\u7528\u6070\u5f53\u7684\u60c5\u51b5\u4e0b\u53ef\u4ee5\u63d0\u5347\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u3002 \u4f5c\u8005\u7684\u8bad\u7ec3\u65b9\u6cd5\u662f\u8ba9supervised\u548cunsupervised\u6570\u636e\u6210batch\u4e00\u8d77\u8bad\u7ec3\u3002","title":"Monocular 3D Detection with Geometric Constraints Embedding and Semi-supervised Training"},{"location":"3dDetection/SSL_RTM3D/#monocular-3d-detection-with-geometric-constraints-embedding-and-semi-supervised-training","text":"\u8fd9\u7bc7 paper\u4e0e RTM3D \u662f\u540c\u4e00\u4f5c\u8005\uff0c\u601d\u8def\u4e5f\u662f\u4e00\u8109\u76f8\u627f\uff0c\u9700\u8981\u6ce8\u610f\u7684\u662f\u8fd9\u91cc\u5173\u4e8esemi-supervised training\u7684\u5b9a\u4e49\u662f\u4f7f\u7528\u7684ground truth \u66f4\u5c11\uff0c\u540c\u65f6\u53ef\u4ee5\u5728test set\u4e0a\u9762\u505afine-tune\u4f7f\u5f97\u7ed3\u679c\u66f4\u9c81\u68d2. \u5176\u6700\u7ec8\u6027\u80fd\u5f88\u4e0d\u9519\uff0c\u5177\u4f53\u5728 Collections for Mono3D \u91cc\u9762\u6709\u6392\u540d\u3002","title":"Monocular 3D Detection with Geometric Constraints Embedding and Semi-supervised Training"},{"location":"3dDetection/SSL_RTM3D/#km3d-net","text":"\u6838\u5fc3\u76d1\u7763\u7ed3\u6784\u5982\u56fe \u67b6\u6784\u4e0a\u5ef6\u7eed\u4e86 Object as Point \u4ee5\u53ca] RTM3D \u7684\u601d\u8def\uff0c\u70ed\u56fe\u9884\u6d4b\u89d2\u70b9\u4e3a\u6838\u5fc3\u7279\u70b9. \u53e6\u5916\u53ef\u4ee5\u770b\u5230\u6700\u5927\u7684\u4e0d\u540c\u5728\u4e8e\u4e0d\u518d\u76f4\u63a5\u9884\u6d4b\u6df1\u5ea6\u503c\uff0c3D \u4f4d\u7f6e\u8f6c\u800c\u901a\u8fc7\u6700\u4f18\u5316\u8fdb\u884c\u6c42\u89e3\uff0c\u5e76\u4e14\u7531\u4e8e\u8fd0\u7b97\u8fc7\u7a0b\u53ef\u5bfc\uff0c\u4e5f\u4f7f\u5f97\u8ddd\u79bb\u503c\u80fd\u591f\u76f4\u63a5\u53cd\u9988\u5230\u8f93\u51fa\u7aef\u3002(\u4e2a\u4eba\u7406\u89e3\u8fd9\u662f\u4e00\u4e2a\u5bf9\u4e8eanchor free\u6bd4\u8f83\u5927\u7684\u7a81\u7834\uff0c\u5728\u6b64\u4e4b\u524danchor free\u6ca1\u6cd5\u901a\u8fc7\u7269\u4f532D bounding boxes\u7684\u5927\u5c0f\u5f97\u5230\u6df1\u5ea6\u503c\u7684\u5148\u9a8c\uff0c\u5f71\u54cd\u56de\u5f52\u7b97\u6cd5\u7684\u7cbe\u5ea6\uff0c\u5982\u4eca\u672c\u6587\u4f7f\u7528\u4f2a\u9006\u6c42\u89e3\u5c06\u7ea6\u675f\u6db5\u76d6\u5728\u8fd0\u7b97\u8fc7\u7a0b\u4e2d\uff0c\u7406\u8bba\u4e0a\u53ef\u4ee5\u5927\u5e45\u5ea6\u63d0\u5347anchor-free\u7684\u4e0a\u9650,\u5728\u591a\u81ea\u7531\u5ea6\u7684\u6570\u636e\u96c6 cityscapes \u4e0a\uff0c\u8fd9\u4e5f\u5f00\u542f\u4e86\u4f7f\u7528 BPnP \u7684\u53ef\u80fd\u6027) \u6700\u540e\u6c42\u89e3 [X,Y,Z] \u7684\u7ebf\u6027\u7cfb\u7edf\u65b9\u7a0b\u4e3a:","title":"KM3D-Net"},{"location":"3dDetection/SSL_RTM3D/#unsupervised","text":"\u672c\u6587\u7ed9\u51fa\u7684\u6240\u8c13 unsupervised\u7684\u601d\u8def\u4e3b\u8981\u662f\u4e3a\u4e86\u63d0\u9ad8\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\uff0c\u4e5f\u5c31\u662f\u5728 unlabeled data\u4e0a\uff0c\u8981\u6c42\u7ecf\u5386\u4e0d\u540c\u6570\u636e\u589e\u5f3a\u540e\u7684\u9884\u6d4b\u503c\u76f8\u4f3c\u5ea6\u6bd4\u8f83\u9ad8\u3002\u8fd9\u4e2a\u672c\u8d28\u4e0a\u4e0d\u80fd\u66ff\u4ee3 supervised\u7684\u4f5c\u7528\uff0csupervised\u4f9d\u7136\u5f88\u91cd\u8981\uff0c\u4f46\u662f\u8fd9\u4e2a\u65b9\u6cd5\u5728\u4f7f\u7528\u6070\u5f53\u7684\u60c5\u51b5\u4e0b\u53ef\u4ee5\u63d0\u5347\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u3002 \u4f5c\u8005\u7684\u8bad\u7ec3\u65b9\u6cd5\u662f\u8ba9supervised\u548cunsupervised\u6570\u636e\u6210batch\u4e00\u8d77\u8bad\u7ec3\u3002","title":"Unsupervised"},{"location":"3dDetection/SingleStage3DPoseEstimation/","text":"Real-Time Seamless Single Shot 6D Object Pose Prediction \u8fd9\u7bc7\u8bba\u6587\u662f\u4e00\u7bc7\u8f83\u4e3a\u57fa\u7840\u76843D\u7269\u4f53\u68c0\u6d4b\u6587\u7ae0\uff0c\u8fdb\u884c\u7684\u662f\u5728\u57fa\u672c\u5df2\u77e5\u7269\u4f53scale\u7684\u60c5\u51b5\u4e0b,\u4ece\u5355\u4e00RGB\u56fe\u7247\u4e2d\u8fd8\u539f\u7269\u4f53\u7684\u4f4d\u7f6e\u4e0e\u59ff\u6001\u7684\u4efb\u52a1\u3002 \u8fd9\u7bc7\u6587\u7ae0\u6709\u5f00\u6e90\u7684\u4ee3\u7801\uff0c\u540c\u65f6\u5bf9\u4e8e\u672c\u6587\u800c\u8a00\u662f \u8fd9\u7bc7\u6587\u7ae0 \u7684\u76f8\u5173\u524d\u7f6e. \u603b\u4f53\u67b6\u6784 \u8fd9\u7bc7\u6587\u7ae0\u501f\u7528YOLO\u7684\u67b6\u6784,\u8f93\u5165\u4e3a\u4e00\u5f20\u56fe\uff0c\u7528dense anchor box\u9884\u6d4b\u5404\u4e2a\u4f4d\u7f6e\u4e0a\u4e0d\u540c\u7269\u4f53\u7684\u5927\u81f4\u4f4d\u7f6e\uff0c\u540c\u65f6\u8fd9\u4e2aanchor box\u8fd8\u4f1a\u8f93\u51fa\u7269\u4f53\u76849\u4e2a\u63a7\u5236\u70b9\u5728\u56fe\u4e2d\u7684\u76f8\u5f53\u5750\u6807\uff0c\u5305\u542b\u4e00\u4e2a\u4e2d\u5fc3\u70b9\u4ee5\u53ca8\u4e2a\u957f\u65b9\u4f53\u6846\u7684\u89d2\u70b9\u3002\u53e6\u5916\u8f93\u51fa C \u4e2a\u5206\u7c7b\u6307\u6807\u4ee5\u53ca 1 \u4e2aobjectness\u6307\u6807\uff0c\u8fd9\u4e2a\u6307\u6807\u7684\u76ee\u6807\uff0c\u7531\u4e00\u4e2a\u6307\u6570\u51fd\u6570\u51b3\u5b9a \u5df2\u77e59\u4e2a\u70b9\u4e4b\u540e\uff0c\u4f7f\u7528opencv\u7684 solvePnP \u51fd\u6570\u76f4\u63a5\u6c42\u5f97\u7269\u4f53\u7684\u76f8\u5bf9\u4f4d\u79fb\u4e0e\u76f8\u5bf9\u59ff\u6001\u3002","title":"Real-Time Seamless Single Shot 6D Object Pose Prediction"},{"location":"3dDetection/SingleStage3DPoseEstimation/#real-time-seamless-single-shot-6d-object-pose-prediction","text":"\u8fd9\u7bc7\u8bba\u6587\u662f\u4e00\u7bc7\u8f83\u4e3a\u57fa\u7840\u76843D\u7269\u4f53\u68c0\u6d4b\u6587\u7ae0\uff0c\u8fdb\u884c\u7684\u662f\u5728\u57fa\u672c\u5df2\u77e5\u7269\u4f53scale\u7684\u60c5\u51b5\u4e0b,\u4ece\u5355\u4e00RGB\u56fe\u7247\u4e2d\u8fd8\u539f\u7269\u4f53\u7684\u4f4d\u7f6e\u4e0e\u59ff\u6001\u7684\u4efb\u52a1\u3002 \u8fd9\u7bc7\u6587\u7ae0\u6709\u5f00\u6e90\u7684\u4ee3\u7801\uff0c\u540c\u65f6\u5bf9\u4e8e\u672c\u6587\u800c\u8a00\u662f \u8fd9\u7bc7\u6587\u7ae0 \u7684\u76f8\u5173\u524d\u7f6e.","title":"Real-Time Seamless Single Shot 6D Object Pose Prediction"},{"location":"3dDetection/SingleStage3DPoseEstimation/#_1","text":"\u8fd9\u7bc7\u6587\u7ae0\u501f\u7528YOLO\u7684\u67b6\u6784,\u8f93\u5165\u4e3a\u4e00\u5f20\u56fe\uff0c\u7528dense anchor box\u9884\u6d4b\u5404\u4e2a\u4f4d\u7f6e\u4e0a\u4e0d\u540c\u7269\u4f53\u7684\u5927\u81f4\u4f4d\u7f6e\uff0c\u540c\u65f6\u8fd9\u4e2aanchor box\u8fd8\u4f1a\u8f93\u51fa\u7269\u4f53\u76849\u4e2a\u63a7\u5236\u70b9\u5728\u56fe\u4e2d\u7684\u76f8\u5f53\u5750\u6807\uff0c\u5305\u542b\u4e00\u4e2a\u4e2d\u5fc3\u70b9\u4ee5\u53ca8\u4e2a\u957f\u65b9\u4f53\u6846\u7684\u89d2\u70b9\u3002\u53e6\u5916\u8f93\u51fa C \u4e2a\u5206\u7c7b\u6307\u6807\u4ee5\u53ca 1 \u4e2aobjectness\u6307\u6807\uff0c\u8fd9\u4e2a\u6307\u6807\u7684\u76ee\u6807\uff0c\u7531\u4e00\u4e2a\u6307\u6570\u51fd\u6570\u51b3\u5b9a \u5df2\u77e59\u4e2a\u70b9\u4e4b\u540e\uff0c\u4f7f\u7528opencv\u7684 solvePnP \u51fd\u6570\u76f4\u63a5\u6c42\u5f97\u7269\u4f53\u7684\u76f8\u5bf9\u4f4d\u79fb\u4e0e\u76f8\u5bf9\u59ff\u6001\u3002","title":"\u603b\u4f53\u67b6\u6784"},{"location":"3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/","text":"Triangulation Learning Network: from Monocular to Stereo 3D Object Detection \u8fd9\u7bc7\u8bba\u6587\u7684\u4e3b\u8981contribution\u662f\u4e09\u90e8\u5206\uff0c\u7b2c\u4e00\u90e8\u5206\u662f\u4e00\u4e2a\u53ef\u9760\u7684\u5355\u76ee3D detector baseline\uff1b\u7b2c\u4e8c\u90e8\u5206\u662fTriangulation Learning\uff1b\u7b2c\u4e09\u90e8\u5206\u662ffeature reweighting strategy\u3002 Baseline Monocular Network\u5355\u76eebaseline \u6b63\u9762\u89c6\u89d2 Anchor Generation \u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u4e0b\u91c7\u6837\uff0c\u5f97\u5230\u4e00\u4e2a G_x \\times G_y \u7684\u7f51\u683c,\u6bcf\u4e00\u4e2a\u7f51\u683c\u91cc\u9762\u7684\u503c\u4ee3\u8868\u8fd9\u4e2a\u7f51\u683c\u6709\u76ee\u6807\u7684\u6982\u7387\uff0c\u4ece\u8fd9\u4e9bpotential cells\u51fa\u53d1\u5c04\u51fa\u4e00\u4e2a\u5706\u53f0\uff0c\u5728\u53f0\u4f53\u4e2d\u5747\u5300\u53d6\u6837\u5f62\u62103D anchors proposals\u3002 3D \u76d2\u4f53proposal\u4ee5\u53carefinement \u4f7f\u7528 RoIAlign\uff0c\u4e5f\u5c31\u662f\u4f20\u7edf\u7684RCNN\u65b9\u5f0f\u505a Triangulation Learning Network TL-Net:Object level triangulation \u8f93\u5165\u662f\u4e00\u5bf9ROI\uff0c\u4e00\u81f4\u6027\u8bc4\u5206 Pairwise Coherence Score\u662f\u4e00\u4e2a 1 \\times 1 \\times C_{roi} \u7684\u6743\u91cd\u5f20\u91cf\uff0c\u7136\u540e\u5206\u522b\u6539\u53d8\u4e24\u4e2a\u56fe\u4e2d\u5404\u4e2a\u7279\u5f81\u901a\u9053\u7684\u6743\u91cd\uff0c\u7136\u540e\u76f8\u52a0\uff0c\u518d\u5168\u8fde\u63a5\u8f93\u51fa\u3002 \u8fd9\u4e2aCoherence Score\u662f\u4e24\u4e2a\u7279\u5f81feature map\u7684\u4f59\u5f26\u8ddd\u79bb\uff0c\u4f5c\u8005\u7684\u8bba\u70b9\u662f\u5982\u679c3D\u4f4d\u7f6e\u6b63\u786e\uff0c\u90a3\u4e48anchor box\u6295\u5f71\u5230\u5de6\u53f3\u76f8\u673a\u4e2d\u7684\u7279\u5f81\u4f1a\u76f8\u4f3c\uff0c\u4e5f\u90fd\u662f\u7269\u4f53\u7684feature\uff0c \u82e5\u504f\u4e86\u5219\u5dee\u8ddd\u4f1a\u5f88\u5927\uff0c\u968f\u673a\u3001\u5b8c\u5168\u65e0\u5173\u7684\u6761\u4ef6\u4e0breweight \u6743\u91cd\u4f1a\u4e3a0\uff0c\u6700\u7ec8\u8f93\u51fa\u7684confidence\u4e5f\u4f1a\u6709\u6240\u5f71\u54cd\u3002","title":"Triangulation Learning Network: from Monocular to Stereo 3D Object Detection"},{"location":"3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/#triangulation-learning-network-from-monocular-to-stereo-3d-object-detection","text":"\u8fd9\u7bc7\u8bba\u6587\u7684\u4e3b\u8981contribution\u662f\u4e09\u90e8\u5206\uff0c\u7b2c\u4e00\u90e8\u5206\u662f\u4e00\u4e2a\u53ef\u9760\u7684\u5355\u76ee3D detector baseline\uff1b\u7b2c\u4e8c\u90e8\u5206\u662fTriangulation Learning\uff1b\u7b2c\u4e09\u90e8\u5206\u662ffeature reweighting strategy\u3002","title":"Triangulation Learning Network: from Monocular to Stereo 3D Object Detection"},{"location":"3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/#baseline-monocular-networkbaseline","text":"","title":"Baseline Monocular Network\u5355\u76eebaseline"},{"location":"3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/#anchor-generation","text":"\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u4e0b\u91c7\u6837\uff0c\u5f97\u5230\u4e00\u4e2a G_x \\times G_y \u7684\u7f51\u683c,\u6bcf\u4e00\u4e2a\u7f51\u683c\u91cc\u9762\u7684\u503c\u4ee3\u8868\u8fd9\u4e2a\u7f51\u683c\u6709\u76ee\u6807\u7684\u6982\u7387\uff0c\u4ece\u8fd9\u4e9bpotential cells\u51fa\u53d1\u5c04\u51fa\u4e00\u4e2a\u5706\u53f0\uff0c\u5728\u53f0\u4f53\u4e2d\u5747\u5300\u53d6\u6837\u5f62\u62103D anchors proposals\u3002","title":"\u6b63\u9762\u89c6\u89d2 Anchor Generation"},{"location":"3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/#3d-proposalrefinement","text":"\u4f7f\u7528 RoIAlign\uff0c\u4e5f\u5c31\u662f\u4f20\u7edf\u7684RCNN\u65b9\u5f0f\u505a","title":"3D \u76d2\u4f53proposal\u4ee5\u53carefinement"},{"location":"3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/#triangulation-learning-network","text":"","title":"Triangulation Learning Network"},{"location":"3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/#tl-netobject-level-triangulation","text":"\u8f93\u5165\u662f\u4e00\u5bf9ROI\uff0c\u4e00\u81f4\u6027\u8bc4\u5206 Pairwise Coherence Score\u662f\u4e00\u4e2a 1 \\times 1 \\times C_{roi} \u7684\u6743\u91cd\u5f20\u91cf\uff0c\u7136\u540e\u5206\u522b\u6539\u53d8\u4e24\u4e2a\u56fe\u4e2d\u5404\u4e2a\u7279\u5f81\u901a\u9053\u7684\u6743\u91cd\uff0c\u7136\u540e\u76f8\u52a0\uff0c\u518d\u5168\u8fde\u63a5\u8f93\u51fa\u3002 \u8fd9\u4e2aCoherence Score\u662f\u4e24\u4e2a\u7279\u5f81feature map\u7684\u4f59\u5f26\u8ddd\u79bb\uff0c\u4f5c\u8005\u7684\u8bba\u70b9\u662f\u5982\u679c3D\u4f4d\u7f6e\u6b63\u786e\uff0c\u90a3\u4e48anchor box\u6295\u5f71\u5230\u5de6\u53f3\u76f8\u673a\u4e2d\u7684\u7279\u5f81\u4f1a\u76f8\u4f3c\uff0c\u4e5f\u90fd\u662f\u7269\u4f53\u7684feature\uff0c \u82e5\u504f\u4e86\u5219\u5dee\u8ddd\u4f1a\u5f88\u5927\uff0c\u968f\u673a\u3001\u5b8c\u5168\u65e0\u5173\u7684\u6761\u4ef6\u4e0breweight \u6743\u91cd\u4f1a\u4e3a0\uff0c\u6700\u7ec8\u8f93\u51fa\u7684confidence\u4e5f\u4f1a\u6709\u6240\u5f71\u54cd\u3002","title":"TL-Net:Object level triangulation"},{"location":"3dDetection/VoteNetImVote/","text":"VoteNet & ImVoteNet \u53e6\u5916ImVoteNet\u7684pdf: pdf \u8fd9\u4e24\u7bc7paper\u662f\u76f8\u5173\u4e14\u8fde\u7eed\u7684idea\uff0c\u8fd9\u91cc\u8fde\u7eed\u9605\u8bfb VoteNet \u4f5c\u8005\u7684\u7406\u89e3\u662f\uff0c\u7531\u4e8e\u70b9\u4e91\u4e0e\u5b9e\u9645\u70b9\u7684\u4f4d\u7f6e\u6709\u4e00\u5b9a\u7684\u8ddd\u79bb\uff0c\u56e0\u800cinference \u7269\u4f53\u7684\u4e2d\u5fc3\u6709\u96be\u5ea6\uff0c\u8fd9\u91cc\u501f\u52a9hough-voting\u7684\u601d\u8def. \u7b97\u6cd5pipeline: \u4f7f\u7528PointNet++ \u63d0\u53d6\u7279\u5f81\uff0c\u4f7f\u7528\u6700\u8fdc\u70b9\u53d6\u6837\uff0c\u4e0b\u91c7\u6837\u7684\u5230M\u4e2aseed point. \u6bcf\u4e00\u4e2aseedpoint \u7ecf\u8fc7MLP\uff0c\u5f97\u5230object \u4e2d\u5fc3\u4e0eseed point\u7684 dx, dy, dz \u4ee5\u53ca\u7279\u5f81\u6b8b\u5dee df . \u6839\u636e\u6700\u8fdc\u70b9\u91c7\u6837\u4ee5\u53ca\u57fa\u7840\u7684threshold\u8ddd\u79bb\u8fdb\u884c\u805a\u7c7b\u3002 \u6bcf\u4e00\u4e2a\u805a\u7c7b\u91cc\u9762\u7684\u70b9\u7528\u7ebf\u6027\u5c42\u8fdb\u884c\u878d\u5408\uff0c \u5176\u4e2d z_i' \u4e3a\u5f52\u4e00\u5316\u7684\u8ddd\u79bb\u503c\uff0c h_i \u4e3a\u6bcf\u4e00\u4e2a\u70b9\u7684\u7279\u5f81 p(\\mathcal{C})=\\operatorname{MLP}_{2}\\left\\{\\max _{i=1, \\ldots, n}\\left\\{\\operatorname{MLP}_{1}\\left(\\left[z_{i}^{\\prime} ; h_{i}\\right]\\right)\\right\\}\\right\\} ImVoteNet \u8fd9\u7bc7\u8bba\u6587\u5728votenet\u7684\u57fa\u7840\u4e0a\u89e3\u51b3\u4e24\u4e2a\u95ee\u9898\uff0c\u7b2c\u4e00\u4e2a\u662f\u5982\u4f55\u4f7f\u7528image detector\u8f85\u52a9seed points\u8fdb\u884cvoting\uff0c\u7b2c\u4e8c\u4e2a\u662f\u5982\u4f55\u878d\u5408\u4e24\u8005\u7684feature \u5bf9\u6700\u540e\u76843D\u4fe1\u606f\u8fdb\u884c\u56de\u5f52\u3002 \u5728\u7b2c\u4e00\u4e2a\u95ee\u9898\u4e0a\uff0cImVoteNet\u7684\u8bbe\u5b9a\u662f2D detector\u7684\u4e2d\u5fc3\u4e0e\u7269\u4f53\u76843D\u4e2d\u5fc3\u5728\u540c\u4e00\u4e2a\u6295\u5f71\u7ebf\u4e0a\u3002\u5982\u4e0b\u56fe \u5047\u8bbe p \u662f\u7269\u4f53\u4e0a\u7684\u70b9\uff0c\u4e5f\u5c31\u662f\u70b9\u4e91\u7684\u4e00\u4e2a\u70b9\uff0c\u70b9P\u4ea7\u751fvote\u65f6\u9700\u8981\u9884\u6d4b\u4e00\u4e2a \\vec{PC} .\u73b0\u6709\u7684\u6570\u636e\u4e3a\u70b9 P \u7684\u4e09\u7ef4\u5750\u6807\u3001\u5728\u56fe\u7247\u4e0a\u7684\u6295\u5f71 p , \u4ee5\u53ca C \u5728\u56fe\u7247\u4e2d\u7684\u6295\u5f71 c . \\begin{aligned} \\vec{p} \\vec{c} &=\\left(u_{2}-u_{1}, v_{2}-v_{1}\\right)=(\\Delta u, \\Delta v) \\\\ &=\\left(f\\left(\\frac{x_{2}}{z_{2}}-\\frac{x_{1}}{z_{1}}\\right), f\\left(\\frac{y_{2}}{z_{2}}-\\frac{y_{1}}{z_{1}}\\right)\\right) \\end{aligned} \\overrightarrow{P C^{\\prime}}=\\left(\\frac{\\Delta u}{f} z_{1}, \\frac{\\Delta v}{f} z_{1}, 0\\right) 3D voting\u65f6\u53ea\u9700\u8981\u518d\u9884\u6d4b \\Delta z = z' - z \uff0c\u964d\u4f4e\u4e86\u641c\u7d22\u7a7a\u95f4\u3002 \u5173\u4e8e\u7b2c\u4e8c\u4e2a\u95ee\u9898\uff0c\u4f5c\u8005\u63d0\u51fa\u4f7f\u7528\u4e09\u4e2a\u4e0e2D detector\u65e0\u5173\u7684feature Geometric Cues for each seed point: \\left(\\frac{\\Delta u}{f} z_{1}, \\frac{\\Delta v}{f} z_{1}, \\frac{\\overrightarrow{O C^{\\prime}}}{\\|\\overrightarrow{O C^{\\prime}}\\|}\\right) Semantic Cues for each seed point: \u5bf9\u5e942D detector\u8f93\u51fa\u65f6\u7684\u5206\u7c7b\u77e2\u91cf(\u4e0d\u4f7f\u7528RoI Align\u5904\u7406\u540e\u7684\u77e2\u91cf\uff0c\u4e3a\u4e86\u8ba9\u8fd9\u4e2a\u7f51\u7edc\u4e0e detector \u65e0\u5173) Texture Cues for each seed point: \u5c06seed point\u6295\u5f71\u5230\u539f\u6765\u7684RGB\u56fe\u4e0a\uff0c\u5c06\u8fd9\u4e2aRGB vector \u4f5c\u4e3a texture cues\u8d4b\u4e88\u7ed9\u8fd9\u4e2a\u70b9\u3002 Fusion: \u4f5c\u8005\u8ba9\u70b9\u4e91\u3001\u56fe\u7247\u5206\u522b\u5404\u81ea\u8fdb\u884cdetection\uff0ctraining\u65f6\u76f4\u63a5\u878d\u5408\u3002\u4f7f\u7528\u4e86 gradient blending.pdf \u8fd9\u4e2atrick.","title":"VoteNet &amp; ImVoteNet"},{"location":"3dDetection/VoteNetImVote/#votenet-imvotenet","text":"\u53e6\u5916ImVoteNet\u7684pdf: pdf \u8fd9\u4e24\u7bc7paper\u662f\u76f8\u5173\u4e14\u8fde\u7eed\u7684idea\uff0c\u8fd9\u91cc\u8fde\u7eed\u9605\u8bfb","title":"VoteNet &amp; ImVoteNet"},{"location":"3dDetection/VoteNetImVote/#votenet","text":"\u4f5c\u8005\u7684\u7406\u89e3\u662f\uff0c\u7531\u4e8e\u70b9\u4e91\u4e0e\u5b9e\u9645\u70b9\u7684\u4f4d\u7f6e\u6709\u4e00\u5b9a\u7684\u8ddd\u79bb\uff0c\u56e0\u800cinference \u7269\u4f53\u7684\u4e2d\u5fc3\u6709\u96be\u5ea6\uff0c\u8fd9\u91cc\u501f\u52a9hough-voting\u7684\u601d\u8def. \u7b97\u6cd5pipeline: \u4f7f\u7528PointNet++ \u63d0\u53d6\u7279\u5f81\uff0c\u4f7f\u7528\u6700\u8fdc\u70b9\u53d6\u6837\uff0c\u4e0b\u91c7\u6837\u7684\u5230M\u4e2aseed point. \u6bcf\u4e00\u4e2aseedpoint \u7ecf\u8fc7MLP\uff0c\u5f97\u5230object \u4e2d\u5fc3\u4e0eseed point\u7684 dx, dy, dz \u4ee5\u53ca\u7279\u5f81\u6b8b\u5dee df . \u6839\u636e\u6700\u8fdc\u70b9\u91c7\u6837\u4ee5\u53ca\u57fa\u7840\u7684threshold\u8ddd\u79bb\u8fdb\u884c\u805a\u7c7b\u3002 \u6bcf\u4e00\u4e2a\u805a\u7c7b\u91cc\u9762\u7684\u70b9\u7528\u7ebf\u6027\u5c42\u8fdb\u884c\u878d\u5408\uff0c \u5176\u4e2d z_i' \u4e3a\u5f52\u4e00\u5316\u7684\u8ddd\u79bb\u503c\uff0c h_i \u4e3a\u6bcf\u4e00\u4e2a\u70b9\u7684\u7279\u5f81 p(\\mathcal{C})=\\operatorname{MLP}_{2}\\left\\{\\max _{i=1, \\ldots, n}\\left\\{\\operatorname{MLP}_{1}\\left(\\left[z_{i}^{\\prime} ; h_{i}\\right]\\right)\\right\\}\\right\\}","title":"VoteNet"},{"location":"3dDetection/VoteNetImVote/#imvotenet","text":"\u8fd9\u7bc7\u8bba\u6587\u5728votenet\u7684\u57fa\u7840\u4e0a\u89e3\u51b3\u4e24\u4e2a\u95ee\u9898\uff0c\u7b2c\u4e00\u4e2a\u662f\u5982\u4f55\u4f7f\u7528image detector\u8f85\u52a9seed points\u8fdb\u884cvoting\uff0c\u7b2c\u4e8c\u4e2a\u662f\u5982\u4f55\u878d\u5408\u4e24\u8005\u7684feature \u5bf9\u6700\u540e\u76843D\u4fe1\u606f\u8fdb\u884c\u56de\u5f52\u3002 \u5728\u7b2c\u4e00\u4e2a\u95ee\u9898\u4e0a\uff0cImVoteNet\u7684\u8bbe\u5b9a\u662f2D detector\u7684\u4e2d\u5fc3\u4e0e\u7269\u4f53\u76843D\u4e2d\u5fc3\u5728\u540c\u4e00\u4e2a\u6295\u5f71\u7ebf\u4e0a\u3002\u5982\u4e0b\u56fe \u5047\u8bbe p \u662f\u7269\u4f53\u4e0a\u7684\u70b9\uff0c\u4e5f\u5c31\u662f\u70b9\u4e91\u7684\u4e00\u4e2a\u70b9\uff0c\u70b9P\u4ea7\u751fvote\u65f6\u9700\u8981\u9884\u6d4b\u4e00\u4e2a \\vec{PC} .\u73b0\u6709\u7684\u6570\u636e\u4e3a\u70b9 P \u7684\u4e09\u7ef4\u5750\u6807\u3001\u5728\u56fe\u7247\u4e0a\u7684\u6295\u5f71 p , \u4ee5\u53ca C \u5728\u56fe\u7247\u4e2d\u7684\u6295\u5f71 c . \\begin{aligned} \\vec{p} \\vec{c} &=\\left(u_{2}-u_{1}, v_{2}-v_{1}\\right)=(\\Delta u, \\Delta v) \\\\ &=\\left(f\\left(\\frac{x_{2}}{z_{2}}-\\frac{x_{1}}{z_{1}}\\right), f\\left(\\frac{y_{2}}{z_{2}}-\\frac{y_{1}}{z_{1}}\\right)\\right) \\end{aligned} \\overrightarrow{P C^{\\prime}}=\\left(\\frac{\\Delta u}{f} z_{1}, \\frac{\\Delta v}{f} z_{1}, 0\\right) 3D voting\u65f6\u53ea\u9700\u8981\u518d\u9884\u6d4b \\Delta z = z' - z \uff0c\u964d\u4f4e\u4e86\u641c\u7d22\u7a7a\u95f4\u3002 \u5173\u4e8e\u7b2c\u4e8c\u4e2a\u95ee\u9898\uff0c\u4f5c\u8005\u63d0\u51fa\u4f7f\u7528\u4e09\u4e2a\u4e0e2D detector\u65e0\u5173\u7684feature Geometric Cues for each seed point: \\left(\\frac{\\Delta u}{f} z_{1}, \\frac{\\Delta v}{f} z_{1}, \\frac{\\overrightarrow{O C^{\\prime}}}{\\|\\overrightarrow{O C^{\\prime}}\\|}\\right) Semantic Cues for each seed point: \u5bf9\u5e942D detector\u8f93\u51fa\u65f6\u7684\u5206\u7c7b\u77e2\u91cf(\u4e0d\u4f7f\u7528RoI Align\u5904\u7406\u540e\u7684\u77e2\u91cf\uff0c\u4e3a\u4e86\u8ba9\u8fd9\u4e2a\u7f51\u7edc\u4e0e detector \u65e0\u5173) Texture Cues for each seed point: \u5c06seed point\u6295\u5f71\u5230\u539f\u6765\u7684RGB\u56fe\u4e0a\uff0c\u5c06\u8fd9\u4e2aRGB vector \u4f5c\u4e3a texture cues\u8d4b\u4e88\u7ed9\u8fd9\u4e2a\u70b9\u3002 Fusion: \u4f5c\u8005\u8ba9\u70b9\u4e91\u3001\u56fe\u7247\u5206\u522b\u5404\u81ea\u8fdb\u884cdetection\uff0ctraining\u65f6\u76f4\u63a5\u878d\u5408\u3002\u4f7f\u7528\u4e86 gradient blending.pdf \u8fd9\u4e2atrick.","title":"ImVoteNet"},{"location":"3dDetection/YOLOStereo3D/","text":"YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection (EN) This is my paper accepcted by ICRA 2021. The open-sourced code is in https://github.com/Owen-Liuyuxuan/visualDet3D . The basic idea is to train Stereo 3D detection model \"like\" a Monocular one, to obtain fast inference speed and reasonable performance. Multiple modules are introduced and merged. The re-production of the stereo/monocular results of this paper should be rather stable provided with the open-source repo. Core Operations and Code Placement Precomputing statistics for anchors: script github page Using the statistics for anchors: head github page Matching Module: lib github page Ghost Module: lib github page Multi-Scale Fusion: core github page Disparity Loss: loss github page To obtain the monocular results in this paper, just trained the with the monocular 3D settings like the GAC paper and get rid of additiona modules (DeformConv/GAC/...). Result for the published model: Release Page Benchmark Easy Moderate Hard Car Detection 94.75 % 84.50 % 62.13 % Car Orientation 93.65 % 82.88 % 60.92 % Car 3D Detection 65.77 % 40.71 % 29.99 % Car Bird's Eye View 74.00 % 49.54 % 36.30 % Pedestrian Detection 58.34 % 49.54 % 36.30 % Pedestrian Orientation 50.41 % 36.81 % 31.51 % Pedestrian 3D Detection 31.03 % 20.67 % 18.34 % Pedestrian Bird's Eye View 32.52 % 22.74 % 19.16 %","title":"YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection (EN)"},{"location":"3dDetection/YOLOStereo3D/#yolostereo3d-a-step-back-to-2d-for-efficient-stereo-3d-detection-en","text":"This is my paper accepcted by ICRA 2021. The open-sourced code is in https://github.com/Owen-Liuyuxuan/visualDet3D . The basic idea is to train Stereo 3D detection model \"like\" a Monocular one, to obtain fast inference speed and reasonable performance. Multiple modules are introduced and merged. The re-production of the stereo/monocular results of this paper should be rather stable provided with the open-source repo.","title":"YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection (EN)"},{"location":"3dDetection/YOLOStereo3D/#core-operations-and-code-placement","text":"Precomputing statistics for anchors: script github page Using the statistics for anchors: head github page Matching Module: lib github page Ghost Module: lib github page Multi-Scale Fusion: core github page Disparity Loss: loss github page To obtain the monocular results in this paper, just trained the with the monocular 3D settings like the GAC paper and get rid of additiona modules (DeformConv/GAC/...).","title":"Core Operations and Code Placement"},{"location":"3dDetection/YOLOStereo3D/#result-for-the-published-model","text":"Release Page Benchmark Easy Moderate Hard Car Detection 94.75 % 84.50 % 62.13 % Car Orientation 93.65 % 82.88 % 60.92 % Car 3D Detection 65.77 % 40.71 % 29.99 % Car Bird's Eye View 74.00 % 49.54 % 36.30 % Pedestrian Detection 58.34 % 49.54 % 36.30 % Pedestrian Orientation 50.41 % 36.81 % 31.51 % Pedestrian 3D Detection 31.03 % 20.67 % 18.34 % Pedestrian Bird's Eye View 32.52 % 22.74 % 19.16 %","title":"Result for the published model:"},{"location":"3dDetection/caddn/","text":"Categorical Depth Distribution Network for Monocular 3D Object Detection \u8fd9\u7bc7paper (caddn) \u63d0\u51fa\u4e86\u4e00\u4e2aone-stage\u7684\u5355\u76ee3D detection\u6a21\u578b\uff0c\u901f\u5ea6\u4e0d\u5feb\u4f46\u662f\u70b9\u6570\u8f83\u4e3a\u7406\u60f3\u3002\u601d\u8def\u4e0a\u7c7b\u4f3c\u4e8e DSGN \u7684\u7aef\u5230\u7aef\u5750\u6807\u8f6c\u6362\uff0c\u662f\u6bd4\u8f83\u6709\u6cdb\u7528\u6027\u7684\u7b97\u6cd5\u3002 Model Architecture \u7cfb\u7edf\u6a21\u578b Frustum Feature Network\u8f93\u51fa\u7684Image Feature \u4e3abackbone \u4e0b\u91c7\u6837\u6bd4\u4f8b\u4e3a4\u7684\u8f93\u51fa\uff0c\u4f7f\u7528 1\\times 1 \u5377\u79ef\u5bf9\u901a\u9053\u6570\u8fdb\u884c\u4e0b\u91c7\u6837, \u5f97\u5230 F \\in \\mathbb{R}^{W_F \\times H_F \\times C} . Depth Distribution Network \u91c7\u7528 ASPP\u878d\u5408\u591a\u5c3a\u5ea6\u6570\u636e\uff0c\u540c\u6837\u8f93\u51fa\u4e0b\u91c7\u6837\u6bd4\u4f8b\u4e3a4\u7684\u8f93\u51fa,\u8f93\u51fa\u7684\u662f\u6df1\u5ea6bin\u5206\u5e03, \u5f97\u5230 D \\in \\mathbb{R}^{W_F \\times H_F \\times D} . Sampler \u7684\u8f93\u5165\u4e3a F , D \u7684\u5916\u79ef G \\in \\mathbb{R}^{W_F\\times H_F\\times D\\times C} . \u4ece\u76f8\u673a\u5750\u6807\u8f6c\u6362\u5230BEV\u7684\u505a\u6cd5\u7cfb\u7edf\u5982\u4e0b\u56fe: \u4f30\u8ba1\u5b9e\u73b0\u65b9\u6cd5\u662f\u91c7\u7528\u7a7a\u95f4\u4e2d\u7684 grid_sample \u51fd\u6570\uff0c\u5b9e\u73b0\u6587\u4e2d\u63d0\u53ca\u7684\u4e09\u7ebf\u6027\u63d2\u503c. BEV Detection \u4f5c\u8005\u4f7f\u7528\u5377\u79ef\u5c42\u5c06Voxel Feature V \\in \\mathbb{R}^{X\\times Y\\times Z\\times C} \u574d\u7f29\u6210\u5355\u5c42\u7684BEV\u7279\u5f81 B\\in \\mathbb{R}^{X\\times Y\\times C} . \u4e4b\u540e\u91c7\u7528 PointPillars \u7684\u8f93\u51faHead\u8f93\u51fa\u6700\u7ec8\u7ed3\u679c. Depth Encoding \u7f51\u7edc\u4e2d\u5b9e\u9645\u4e0a\u5d4c\u5165\u4e86\u4e00\u4e2a\u57fa\u4e8ebin\u5206\u7c7b\u6df1\u5ea6\u9884\u6d4b\u7f51\u7edc\uff0c\u8fd9\u4e2a\u6df1\u5ea6\u9884\u6d4b\u7f51\u7edc\u5bf9\u6df1\u5ea6\u7684encoding/discretization\u662f\u5173\u952e\u3002 \u672c\u6587\u91c7\u7528\u7684\u662f\u7ebf\u6027\u589e\u52a0\u79bb\u6563(linear-increasing discretization LID).\u5176\u516c\u5f0f\u4e3a: d_c = d_{min} + \\frac{d_{max} - d_{min}}{D(D+1)} \\cdot d_i(d_i+1) \u8bad\u7ec3\u8fd9\u4e2a\u7f51\u7edc\u7684\u65f6\u5019\u4f7f\u7528Focal Loss, \u540c\u65f6\u5bf9\u4e0d\u540c\u70b9\u7684\u6df1\u5ea6\u7ed9\u4e88\u4e0d\u540c\u7684\u6743\u91cd\u3002\u5982\u679c\u4e00\u4e2a\u70b9\u57282D bounding box\u5185\uff0c\u5219\u5176\u6743\u91cd \\alpha_{fg} = 3.25 , \u5426\u5219\u4e3a \\alpha_{bg} = 0.25","title":"Categorical Depth Distribution Network for Monocular 3D Object Detection"},{"location":"3dDetection/caddn/#categorical-depth-distribution-network-for-monocular-3d-object-detection","text":"\u8fd9\u7bc7paper (caddn) \u63d0\u51fa\u4e86\u4e00\u4e2aone-stage\u7684\u5355\u76ee3D detection\u6a21\u578b\uff0c\u901f\u5ea6\u4e0d\u5feb\u4f46\u662f\u70b9\u6570\u8f83\u4e3a\u7406\u60f3\u3002\u601d\u8def\u4e0a\u7c7b\u4f3c\u4e8e DSGN \u7684\u7aef\u5230\u7aef\u5750\u6807\u8f6c\u6362\uff0c\u662f\u6bd4\u8f83\u6709\u6cdb\u7528\u6027\u7684\u7b97\u6cd5\u3002","title":"Categorical Depth Distribution Network for Monocular 3D Object Detection"},{"location":"3dDetection/caddn/#model-architecture","text":"Frustum Feature Network\u8f93\u51fa\u7684Image Feature \u4e3abackbone \u4e0b\u91c7\u6837\u6bd4\u4f8b\u4e3a4\u7684\u8f93\u51fa\uff0c\u4f7f\u7528 1\\times 1 \u5377\u79ef\u5bf9\u901a\u9053\u6570\u8fdb\u884c\u4e0b\u91c7\u6837, \u5f97\u5230 F \\in \\mathbb{R}^{W_F \\times H_F \\times C} . Depth Distribution Network \u91c7\u7528 ASPP\u878d\u5408\u591a\u5c3a\u5ea6\u6570\u636e\uff0c\u540c\u6837\u8f93\u51fa\u4e0b\u91c7\u6837\u6bd4\u4f8b\u4e3a4\u7684\u8f93\u51fa,\u8f93\u51fa\u7684\u662f\u6df1\u5ea6bin\u5206\u5e03, \u5f97\u5230 D \\in \\mathbb{R}^{W_F \\times H_F \\times D} . Sampler \u7684\u8f93\u5165\u4e3a F , D \u7684\u5916\u79ef G \\in \\mathbb{R}^{W_F\\times H_F\\times D\\times C} . \u4ece\u76f8\u673a\u5750\u6807\u8f6c\u6362\u5230BEV\u7684\u505a\u6cd5\u7cfb\u7edf\u5982\u4e0b\u56fe: \u4f30\u8ba1\u5b9e\u73b0\u65b9\u6cd5\u662f\u91c7\u7528\u7a7a\u95f4\u4e2d\u7684 grid_sample \u51fd\u6570\uff0c\u5b9e\u73b0\u6587\u4e2d\u63d0\u53ca\u7684\u4e09\u7ebf\u6027\u63d2\u503c.","title":"Model Architecture \u7cfb\u7edf\u6a21\u578b"},{"location":"3dDetection/caddn/#bev-detection","text":"\u4f5c\u8005\u4f7f\u7528\u5377\u79ef\u5c42\u5c06Voxel Feature V \\in \\mathbb{R}^{X\\times Y\\times Z\\times C} \u574d\u7f29\u6210\u5355\u5c42\u7684BEV\u7279\u5f81 B\\in \\mathbb{R}^{X\\times Y\\times C} . \u4e4b\u540e\u91c7\u7528 PointPillars \u7684\u8f93\u51faHead\u8f93\u51fa\u6700\u7ec8\u7ed3\u679c.","title":"BEV Detection"},{"location":"3dDetection/caddn/#depth-encoding","text":"\u7f51\u7edc\u4e2d\u5b9e\u9645\u4e0a\u5d4c\u5165\u4e86\u4e00\u4e2a\u57fa\u4e8ebin\u5206\u7c7b\u6df1\u5ea6\u9884\u6d4b\u7f51\u7edc\uff0c\u8fd9\u4e2a\u6df1\u5ea6\u9884\u6d4b\u7f51\u7edc\u5bf9\u6df1\u5ea6\u7684encoding/discretization\u662f\u5173\u952e\u3002 \u672c\u6587\u91c7\u7528\u7684\u662f\u7ebf\u6027\u589e\u52a0\u79bb\u6563(linear-increasing discretization LID).\u5176\u516c\u5f0f\u4e3a: d_c = d_{min} + \\frac{d_{max} - d_{min}}{D(D+1)} \\cdot d_i(d_i+1) \u8bad\u7ec3\u8fd9\u4e2a\u7f51\u7edc\u7684\u65f6\u5019\u4f7f\u7528Focal Loss, \u540c\u65f6\u5bf9\u4e0d\u540c\u70b9\u7684\u6df1\u5ea6\u7ed9\u4e88\u4e0d\u540c\u7684\u6743\u91cd\u3002\u5982\u679c\u4e00\u4e2a\u70b9\u57282D bounding box\u5185\uff0c\u5219\u5176\u6743\u91cd \\alpha_{fg} = 3.25 , \u5426\u5219\u4e3a \\alpha_{bg} = 0.25","title":"Depth Encoding"},{"location":"3dDetection/digmono3d/","text":"Digging Into Output Representation For Monocular 3D Object Detection \u8fd9\u7bc7paper\u662fICLR 2022\u7684\u4e00\u7bc7\u64a4\u7a3f\u3002\u8bba\u8bc4\u4ef7\u4ee5\u53ca\u7406\u8bba\u4e0a\u7684\u539f\u521b\u6027\u5176\u5b9e\u5e76\u4e0d\u591f\u9ad8\uff0c\u751a\u81f3\u4e0d\u4e00\u5b9a\u6709\u5b9e\u7528\u6027\uff0c\u4f46\u662f\u5b83\u6570\u503c\u4e0a\u6270\u52a8\u4e86\u5355\u76ee\u68c0\u6d4b\u7684\u5237\u5206\u751f\u6001\u3002\u662f\u503c\u5f97\u6ce8\u610f\u7684\u8bba\u6587\u3002 \u8fd9\u7bc7\u8bba\u6587\u91c7\u7528\u589e\u52a0\u91c7\u6837\u7684\u65b9\u6848\uff0c\u6570\u503c\u4e0a\u63d0\u5347\u4e863D recall\u5e76\u4e14\u5f97\u5230\u7684AP\u5f88\u9ad8\u3002 \u91c7\u7528\u7684\u65b9\u6848\u662f\u5bf9\u6bcf\u4e00\u4e2a\u7f51\u7edc\u73b0\u6210\u7684\u8f93\u51fa\u505a\u4e86\u4e00\u4e9b\u589e\u5e7f\u5904\u7406\u3002\u7b80\u5355\u6765\u8bf4\u5c31\u662f\u5bf9\u6bcf\u4e00\u4e2a\u68c0\u6d4b\u7ed3\u679c\uff0c\u591a\u9884\u6d4b\u6570\u4e2a\u4f4e\u6982\u7387\u7684\u6df1\u5ea6\u4e0a\u6709\u504f\u79fb\u76843D\u9884\u6d4b\u6846\u3002 \u504f\u79fb\u540e\u7684\u6df1\u5ea6\u4fee\u6b63\u6982\u7387(\u7528\u6b64\u6982\u7387\u4e58\u4ee5\u539f\u6709\u7684\u5206\u7c7b\u6982\u7387\u5f97\u5230\u65b0\u7684\u7f6e\u4fe1\u5ea6)\u4e3a: t(s) = e^{-\\frac{(s-z)^2}{\\sigma^2}} \\sigma = e^{\\frac{z}{\\lambda}} \u5728KITTI\u4e0a \\lambda=80 ,\u5bf9\u6bcf\u4e00\u4e2a\u68c0\u6d4b\u51fa\u6765\u7684\u7269\u4f53\uff0c\u591a\u8f93\u51fa6\u4e2a\u4f30\u8ba1\u503c\u3002\u5bf9\u6df1\u5ea6\u505a [\\plusmn 2, \\plusmn 1, \\plusmn 0.5] \u7684\u6270\u52a8\uff0c\u540c\u65f6\u8fd1\u8ddd\u79bb(10\u7c73\u5185)\u7684\u68c0\u6d4b\u4e0d\u505a\u8fd9\u6837\u7684\u6270\u52a8\u3002 \u5237\u699c\u7684\u63d0\u5347\u76f8\u5f53\u60ca\u4eba\u3002\u611f\u89c9\u8fd9\u4e2a\u6587\u7ae0\u7684\u505a\u6cd5\u662f\u6570\u503c\u4e0a\u5c1d\u8bd5\u5e73\u8861precision recall \u7406\u8bba\u4e0a\u4e0d\u662f\u5f88\u7279\u522b\uff0c\u4f46\u662f\u6027\u80fd\u4e0a\u5f88\u795e\u5947\u3002","title":"Digging Into Output Representation For Monocular 3D Object Detection"},{"location":"3dDetection/digmono3d/#digging-into-output-representation-for-monocular-3d-object-detection","text":"\u8fd9\u7bc7paper\u662fICLR 2022\u7684\u4e00\u7bc7\u64a4\u7a3f\u3002\u8bba\u8bc4\u4ef7\u4ee5\u53ca\u7406\u8bba\u4e0a\u7684\u539f\u521b\u6027\u5176\u5b9e\u5e76\u4e0d\u591f\u9ad8\uff0c\u751a\u81f3\u4e0d\u4e00\u5b9a\u6709\u5b9e\u7528\u6027\uff0c\u4f46\u662f\u5b83\u6570\u503c\u4e0a\u6270\u52a8\u4e86\u5355\u76ee\u68c0\u6d4b\u7684\u5237\u5206\u751f\u6001\u3002\u662f\u503c\u5f97\u6ce8\u610f\u7684\u8bba\u6587\u3002 \u8fd9\u7bc7\u8bba\u6587\u91c7\u7528\u589e\u52a0\u91c7\u6837\u7684\u65b9\u6848\uff0c\u6570\u503c\u4e0a\u63d0\u5347\u4e863D recall\u5e76\u4e14\u5f97\u5230\u7684AP\u5f88\u9ad8\u3002 \u91c7\u7528\u7684\u65b9\u6848\u662f\u5bf9\u6bcf\u4e00\u4e2a\u7f51\u7edc\u73b0\u6210\u7684\u8f93\u51fa\u505a\u4e86\u4e00\u4e9b\u589e\u5e7f\u5904\u7406\u3002\u7b80\u5355\u6765\u8bf4\u5c31\u662f\u5bf9\u6bcf\u4e00\u4e2a\u68c0\u6d4b\u7ed3\u679c\uff0c\u591a\u9884\u6d4b\u6570\u4e2a\u4f4e\u6982\u7387\u7684\u6df1\u5ea6\u4e0a\u6709\u504f\u79fb\u76843D\u9884\u6d4b\u6846\u3002 \u504f\u79fb\u540e\u7684\u6df1\u5ea6\u4fee\u6b63\u6982\u7387(\u7528\u6b64\u6982\u7387\u4e58\u4ee5\u539f\u6709\u7684\u5206\u7c7b\u6982\u7387\u5f97\u5230\u65b0\u7684\u7f6e\u4fe1\u5ea6)\u4e3a: t(s) = e^{-\\frac{(s-z)^2}{\\sigma^2}} \\sigma = e^{\\frac{z}{\\lambda}} \u5728KITTI\u4e0a \\lambda=80 ,\u5bf9\u6bcf\u4e00\u4e2a\u68c0\u6d4b\u51fa\u6765\u7684\u7269\u4f53\uff0c\u591a\u8f93\u51fa6\u4e2a\u4f30\u8ba1\u503c\u3002\u5bf9\u6df1\u5ea6\u505a [\\plusmn 2, \\plusmn 1, \\plusmn 0.5] \u7684\u6270\u52a8\uff0c\u540c\u65f6\u8fd1\u8ddd\u79bb(10\u7c73\u5185)\u7684\u68c0\u6d4b\u4e0d\u505a\u8fd9\u6837\u7684\u6270\u52a8\u3002 \u5237\u699c\u7684\u63d0\u5347\u76f8\u5f53\u60ca\u4eba\u3002\u611f\u89c9\u8fd9\u4e2a\u6587\u7ae0\u7684\u505a\u6cd5\u662f\u6570\u503c\u4e0a\u5c1d\u8bd5\u5e73\u8861precision recall \u7406\u8bba\u4e0a\u4e0d\u662f\u5f88\u7279\u522b\uff0c\u4f46\u662f\u6027\u80fd\u4e0a\u5f88\u795e\u5947\u3002","title":"Digging Into Output Representation For Monocular 3D Object Detection"},{"location":"3dDetection/is_plidar_needed/","text":"Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D) \u8fd9\u7bc7paper\u5728\u5355\u76ee3D\u68c0\u6d4b\u4e0a\u878d\u5408\u4e86\u5f88\u591a\u7ec6\u8282\u7684\u60f3\u6cd5\uff0c\u8fbe\u5230\u4e86\u5f88\u9ad8\u7684\u6027\u80fd\u70b9\u6570 \u4f7f\u7528\u53d7\u76d1\u7763\u7684\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u4f5c\u4e3a\u9884\u8bad\u7ec3\uff0c\u4e00\u65b9\u9762\u6570\u636e\u91cf\u5f88\u5927\uff0c\u53e6\u4e00\u65b9\u9762\u53ef\u4ee5\u5141\u8bb8\u8de8domain\u7684\u9884\u8bad\u7ec3(\u68c0\u6d4b\u6570\u636e\u91cf\u5c11\u800c\u6df1\u5ea6\u4f30\u8ba1\u6570\u636e\u91cf\u5927\uff0c\u53ef\u4ee5\u5728\u522b\u7684\u6570\u636e\u96c6\u4e0a\u5148\u9884\u8bad\u7ec3\u6df1\u5ea6\u4f30\u8ba1\uff0c\u518d\u5728\u76ee\u6807\u73af\u5883\u4e0b\u8bad\u7ec3\u68c0\u6d4b)\u3002 \u4f7f\u7528FCOS anchor-free\u6846\u67b6\uff0c\u4e0e2D\u68c0\u6d4b\u76ee\u524d\u7684\u505a\u524d\u6cbf\u65b9\u6848\u878d\u5408\u3002 \u9884\u6d4b\u7684\u6df1\u5ea6\u503c\u53d7camera parameter\u5f71\u54cd\uff0c\u5373\u6df1\u5ea6\u4f1a\u88ab F_x \u5f52\u4e00\u5316\uff0c\u8003\u8651\u4e86\u5185\u53c2\u7684\u53d8\u5316\u540e\uff0c\u9884\u6d4b\u7ed3\u679c\u4e0d\u518d\u4e0e\u67d0\u4e00\u76f8\u673a\u53c2\u6570\u6302\u94a9\uff0c\u4e14\u8bad\u7ec3\u65f6\u53ef\u4ee5\u7528\u66f4\u4e30\u5bcc\u7684\u6570\u636e\u589e\u5f3a\u3002 \u4f7f\u7528\u4e86 disentangled loss . \u9884\u6d4b\u8f93\u51fa\u89c6\u89c9\u8f6c\u6362\u540e\u7684\u56db\u5143\u6570\uff0c\u4e0ecityscape\u7b49\u65b0\u6570\u636e\u96c6\u7684\u8981\u6c42\u7b26\u5408\uff0c\u5bf9\u76f8\u673a\u4fef\u4ef0\u89d2\u7684\u53d8\u5316\u66f4\u9c81\u68d2\uff0c\u66f4\u5bb9\u6613\u76f4\u63a5\u4f7f\u7528\u3002 \u8f93\u51fa\u5934\u7684\u63cf\u8ff0: \u56db\u5143\u6570 (q_w, q_x, q_y, q_z) , \u8868\u793a\u7684\u662fallocentric orientation\u7684\u56db\u5143\u6570\uff0c\u4e5f\u5c31\u662f\u9700\u8981\u8003\u8651\u76f8\u673a\u7684\u89c2\u6d4b\u3002\u4f46\u662f\u53ea\u4f30\u8ba1\u4e09\u4e2a\u81ea\u7531\u5ea6 \u6df1\u5ea6\u4f30\u8ba1\u7f51\u7edc\u8f93\u51fa z_{\\{c,p\\}} , z_c \u5bf9\u5e94\u7269\u4f53\u7684\u4e2d\u5fc3\u8ddd\u79bb\uff0c z_p \u5bf9\u5e94\u5355\u76ee\u6df1\u5ea6\u9884\u8bad\u7ec3\u65f6\u4f7f\u7528\u7684\u6df1\u5ea6\uff0c\u800c\u7f51\u7edc\u8f93\u51fa\u4e0e\u5b9e\u9645\u6df1\u5ea6\u4e4b\u95f4\u7684\u5173\u7cfb\u5982\u4e0b\uff0c\u5176\u4e2d \\sigma_l . \\mu_l \u662f\u6bcf\u4e00\u4e2aFPN\u5c42\u91cc\u9762\u53ef\u4ee5\u5b66\u4e60\u7684\u53c2\u6570: \\begin{array}{l} d=\\frac{c}{p} \\cdot\\left(\\sigma_{l} \\cdot z+\\mu_{l}\\right) \\\\ p=\\sqrt{\\frac{1}{f_{x}^{2}}+\\frac{1}{f_{y}^{2}}} \\end{array} \\mathbf{o} = (\\Delta_u, \\Delta_v) \u4ee3\u8868 \\delta=(\\delta_W, \\delta_H, \\delta_L) \u4ee3\u8868\u548c\u5f53\u524d\u7c7b\u522b\u7684bounding boxes\u7684deviation\u3002 \\beta_{3d} \u8868\u793a3D bounding boxes\u9884\u6d4b\u7684\u7f6e\u4fe1\u5ea6\u3002\u5b83\u53ef\u4ee5\u88ab\u8f6c\u6362\u4e3a\u4e00\u4e2a\u6982\u7387\u503c P_{3D} = (1 + e^{\\beta_{3D}})^{-1} , \u9884\u6d4b\u65f6\u4e0e\u5206\u7c7b\u7684\u7ed3\u679c\u76f8\u4e58\uff0c\u4f5c\u4e3aNMS\u548c\u6392\u5e8f\u7684\u6807\u51c6\uff0c\u8bad\u7ec3\u65f6\u7528\u7f51\u7edc\u7684average disentangled L1 Loss\u4f5c\u4e3a\u76d1\u7763\u3002 \\mathcal{L}_{2 \\mathrm{D}}=\\mathcal{L}_{\\mathrm{reg}}+\\mathcal{L}_{c l_{8}}+\\mathcal{L}_{c t r} \\mathcal{L}_{3 \\mathrm{D}}\\left(\\mathbf{B}^{*}, \\hat{\\mathbf{B}}\\right)=\\frac{1}{8}\\left\\|\\mathbf{B}^{*}-\\hat{\\mathbf{B}}\\right\\|_{1}","title":"Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)"},{"location":"3dDetection/is_plidar_needed/#is-pseudo-lidar-needed-for-monocular-3d-object-detection-dd3d","text":"\u8fd9\u7bc7paper\u5728\u5355\u76ee3D\u68c0\u6d4b\u4e0a\u878d\u5408\u4e86\u5f88\u591a\u7ec6\u8282\u7684\u60f3\u6cd5\uff0c\u8fbe\u5230\u4e86\u5f88\u9ad8\u7684\u6027\u80fd\u70b9\u6570 \u4f7f\u7528\u53d7\u76d1\u7763\u7684\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u4f5c\u4e3a\u9884\u8bad\u7ec3\uff0c\u4e00\u65b9\u9762\u6570\u636e\u91cf\u5f88\u5927\uff0c\u53e6\u4e00\u65b9\u9762\u53ef\u4ee5\u5141\u8bb8\u8de8domain\u7684\u9884\u8bad\u7ec3(\u68c0\u6d4b\u6570\u636e\u91cf\u5c11\u800c\u6df1\u5ea6\u4f30\u8ba1\u6570\u636e\u91cf\u5927\uff0c\u53ef\u4ee5\u5728\u522b\u7684\u6570\u636e\u96c6\u4e0a\u5148\u9884\u8bad\u7ec3\u6df1\u5ea6\u4f30\u8ba1\uff0c\u518d\u5728\u76ee\u6807\u73af\u5883\u4e0b\u8bad\u7ec3\u68c0\u6d4b)\u3002 \u4f7f\u7528FCOS anchor-free\u6846\u67b6\uff0c\u4e0e2D\u68c0\u6d4b\u76ee\u524d\u7684\u505a\u524d\u6cbf\u65b9\u6848\u878d\u5408\u3002 \u9884\u6d4b\u7684\u6df1\u5ea6\u503c\u53d7camera parameter\u5f71\u54cd\uff0c\u5373\u6df1\u5ea6\u4f1a\u88ab F_x \u5f52\u4e00\u5316\uff0c\u8003\u8651\u4e86\u5185\u53c2\u7684\u53d8\u5316\u540e\uff0c\u9884\u6d4b\u7ed3\u679c\u4e0d\u518d\u4e0e\u67d0\u4e00\u76f8\u673a\u53c2\u6570\u6302\u94a9\uff0c\u4e14\u8bad\u7ec3\u65f6\u53ef\u4ee5\u7528\u66f4\u4e30\u5bcc\u7684\u6570\u636e\u589e\u5f3a\u3002 \u4f7f\u7528\u4e86 disentangled loss . \u9884\u6d4b\u8f93\u51fa\u89c6\u89c9\u8f6c\u6362\u540e\u7684\u56db\u5143\u6570\uff0c\u4e0ecityscape\u7b49\u65b0\u6570\u636e\u96c6\u7684\u8981\u6c42\u7b26\u5408\uff0c\u5bf9\u76f8\u673a\u4fef\u4ef0\u89d2\u7684\u53d8\u5316\u66f4\u9c81\u68d2\uff0c\u66f4\u5bb9\u6613\u76f4\u63a5\u4f7f\u7528\u3002 \u8f93\u51fa\u5934\u7684\u63cf\u8ff0: \u56db\u5143\u6570 (q_w, q_x, q_y, q_z) , \u8868\u793a\u7684\u662fallocentric orientation\u7684\u56db\u5143\u6570\uff0c\u4e5f\u5c31\u662f\u9700\u8981\u8003\u8651\u76f8\u673a\u7684\u89c2\u6d4b\u3002\u4f46\u662f\u53ea\u4f30\u8ba1\u4e09\u4e2a\u81ea\u7531\u5ea6 \u6df1\u5ea6\u4f30\u8ba1\u7f51\u7edc\u8f93\u51fa z_{\\{c,p\\}} , z_c \u5bf9\u5e94\u7269\u4f53\u7684\u4e2d\u5fc3\u8ddd\u79bb\uff0c z_p \u5bf9\u5e94\u5355\u76ee\u6df1\u5ea6\u9884\u8bad\u7ec3\u65f6\u4f7f\u7528\u7684\u6df1\u5ea6\uff0c\u800c\u7f51\u7edc\u8f93\u51fa\u4e0e\u5b9e\u9645\u6df1\u5ea6\u4e4b\u95f4\u7684\u5173\u7cfb\u5982\u4e0b\uff0c\u5176\u4e2d \\sigma_l . \\mu_l \u662f\u6bcf\u4e00\u4e2aFPN\u5c42\u91cc\u9762\u53ef\u4ee5\u5b66\u4e60\u7684\u53c2\u6570: \\begin{array}{l} d=\\frac{c}{p} \\cdot\\left(\\sigma_{l} \\cdot z+\\mu_{l}\\right) \\\\ p=\\sqrt{\\frac{1}{f_{x}^{2}}+\\frac{1}{f_{y}^{2}}} \\end{array} \\mathbf{o} = (\\Delta_u, \\Delta_v) \u4ee3\u8868 \\delta=(\\delta_W, \\delta_H, \\delta_L) \u4ee3\u8868\u548c\u5f53\u524d\u7c7b\u522b\u7684bounding boxes\u7684deviation\u3002 \\beta_{3d} \u8868\u793a3D bounding boxes\u9884\u6d4b\u7684\u7f6e\u4fe1\u5ea6\u3002\u5b83\u53ef\u4ee5\u88ab\u8f6c\u6362\u4e3a\u4e00\u4e2a\u6982\u7387\u503c P_{3D} = (1 + e^{\\beta_{3D}})^{-1} , \u9884\u6d4b\u65f6\u4e0e\u5206\u7c7b\u7684\u7ed3\u679c\u76f8\u4e58\uff0c\u4f5c\u4e3aNMS\u548c\u6392\u5e8f\u7684\u6807\u51c6\uff0c\u8bad\u7ec3\u65f6\u7528\u7f51\u7edc\u7684average disentangled L1 Loss\u4f5c\u4e3a\u76d1\u7763\u3002 \\mathcal{L}_{2 \\mathrm{D}}=\\mathcal{L}_{\\mathrm{reg}}+\\mathcal{L}_{c l_{8}}+\\mathcal{L}_{c t r} \\mathcal{L}_{3 \\mathrm{D}}\\left(\\mathbf{B}^{*}, \\hat{\\mathbf{B}}\\right)=\\frac{1}{8}\\left\\|\\mathbf{B}^{*}-\\hat{\\mathbf{B}}\\right\\|_{1}","title":"Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)"},{"location":"3dDetection/mono_differentiable_rendering/","text":"Monocular Differentiable Rendering for Self-Supervised 3D Object Detection \u8fd9\u7bc7paper\u5904\u7406\u7684\u95ee\u9898\u662f\u81ea\u76d1\u7763\u5355\u76ee3D\u68c0\u6d4b\u3002\u505a\u6cd5\u662f\u4f18\u5316\u6700\u5c0f\u7684\u6e32\u67d3\u8bef\u5dee\u3002 \u4ece2D instance mask\u51fa\u53d1\uff0c\u4ece\u4e00\u4e9b\u9690\u53d8\u91cf h \u51fa\u53d1\u89e3\u7801\u751f\u6210\u6295\u5f71render\u56fe\u50cf\uff0c\u4e0e\u56fe\u7247\u7ed3\u679c\u8fdb\u884c\u6bd4\u8f83\uff0c\u8fed\u4ee3\u4fee\u6b63\u91cd\u6295\u5f71\u8bef\u5dee\u5f97\u5230\u76ee\u6807\u89e3\u3002 Pipeline \u603b\u4f53pipeline\u5982\u56fe\u3002\u6bd4\u8f83\u7b80\u5355\u7684\u5185\u5bb9\u662f\u56fe\u7247\u53f3\u4fa7\u7684\u90e8\u5206\uff0c\u4ece\u56fe\u7247\u4e2d\u63d0\u53d6\u7269\u4f53\u5206\u5272\uff0c\u7269\u4f53\u68c0\u6d4b\u4ee5\u53ca\u6df1\u5ea6\u9884\u6d4b\u7ed3\u679c\uff0c\u524d\u4e24\u8005\u4f7f\u7528\u7684\u662f\u5728COCO\u4e0a\u8bad\u7ec3\u597d\u7684Mask RCNN\u7f51\u7edc\uff0c\u5728\u8fd9\u91cc\u4e0d\u518d\u5fae\u8c03\u3002\u6df1\u5ea6\u9884\u6d4b\u4f7f\u7528\u7684\u662fself-supervised\u7684 PackNet mesh\u7684\u751f\u6210\u91c7\u7528\u7684\u662fGAN\u7684 2D to 3D https://arxiv.org/pdf/1811.10719.pdf rendering\u91c7\u7528\u7684\u662f neural renderer https://arxiv.org/pdf/1711.07566.pdf","title":"Monocular Differentiable Rendering for Self-Supervised 3D Object Detection"},{"location":"3dDetection/mono_differentiable_rendering/#monocular-differentiable-rendering-for-self-supervised-3d-object-detection","text":"\u8fd9\u7bc7paper\u5904\u7406\u7684\u95ee\u9898\u662f\u81ea\u76d1\u7763\u5355\u76ee3D\u68c0\u6d4b\u3002\u505a\u6cd5\u662f\u4f18\u5316\u6700\u5c0f\u7684\u6e32\u67d3\u8bef\u5dee\u3002 \u4ece2D instance mask\u51fa\u53d1\uff0c\u4ece\u4e00\u4e9b\u9690\u53d8\u91cf h \u51fa\u53d1\u89e3\u7801\u751f\u6210\u6295\u5f71render\u56fe\u50cf\uff0c\u4e0e\u56fe\u7247\u7ed3\u679c\u8fdb\u884c\u6bd4\u8f83\uff0c\u8fed\u4ee3\u4fee\u6b63\u91cd\u6295\u5f71\u8bef\u5dee\u5f97\u5230\u76ee\u6807\u89e3\u3002","title":"Monocular Differentiable Rendering for Self-Supervised 3D Object Detection"},{"location":"3dDetection/mono_differentiable_rendering/#pipeline","text":"\u603b\u4f53pipeline\u5982\u56fe\u3002\u6bd4\u8f83\u7b80\u5355\u7684\u5185\u5bb9\u662f\u56fe\u7247\u53f3\u4fa7\u7684\u90e8\u5206\uff0c\u4ece\u56fe\u7247\u4e2d\u63d0\u53d6\u7269\u4f53\u5206\u5272\uff0c\u7269\u4f53\u68c0\u6d4b\u4ee5\u53ca\u6df1\u5ea6\u9884\u6d4b\u7ed3\u679c\uff0c\u524d\u4e24\u8005\u4f7f\u7528\u7684\u662f\u5728COCO\u4e0a\u8bad\u7ec3\u597d\u7684Mask RCNN\u7f51\u7edc\uff0c\u5728\u8fd9\u91cc\u4e0d\u518d\u5fae\u8c03\u3002\u6df1\u5ea6\u9884\u6d4b\u4f7f\u7528\u7684\u662fself-supervised\u7684 PackNet mesh\u7684\u751f\u6210\u91c7\u7528\u7684\u662fGAN\u7684 2D to 3D https://arxiv.org/pdf/1811.10719.pdf rendering\u91c7\u7528\u7684\u662f neural renderer https://arxiv.org/pdf/1711.07566.pdf","title":"Pipeline"},{"location":"3dDetection/monorun/","text":"MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation \u8fd9\u7bc7paper\u7528\u4e00\u4e2a\u4e8c\u9636\u6bb5\u7684\u6a21\u5757\u4ee5\u53ca\u7a20\u5bc6\u7684PnP\u5339\u914d\uff0c\u7ed5\u8fc7\u4e86\u5bf9\u8ddd\u79bb\u7684\u76f4\u63a5\u9884\u6d4b\u3002 \u505a\u6cd5 \u5c3d\u7ba1\u672c\u6587\u5728self-supervision\u4e0a\u7684\u7528\u8bcd\u503c\u5f97\u5546\u69b7\uff0c\u4f46\u662f\u5176\u7b97\u6cd5\u503c\u5f97\u6a21\u4eff\u3002 \u9996\u5148\u662f\u4f7f\u7528\u4e8c\u9636\u6bb5\u7269\u4f53\u68c0\u6d4b\u5668\u6846\u51fa\u7269\u4f53\u5e76\u63d0\u53d6\u5176\u7279\u5f81\u3002\u8fd9\u4e2a\u7279\u5f81\u8f93\u51fa score, latent vector\u4ee5\u53ca\u7269\u4f53\u7684dimensions. \u518d\u8f93\u51fa\u6bcf\u4e00\u4e2a\u50cf\u7d20\u70b9\u5728\u8f66\u8f86\u5750\u6807\u7cfb\u4e0b\u7684\u4f4d\u7f6e normalized by dimensions\u3002 \u8bad\u7ec3\u7684\u65f6\u5019\u5229\u7528ground truth \u59ff\u6001\u8ba1\u7b97\u6bcf\u4e00\u4e2apixel predict\u7684\u4e09\u7ef4\u5750\u6807\uff0c\u7136\u540e\u53cd\u6295\u5f71\u56de\u6765,\u5f97\u5230\u8bef\u5dee\u5982\u4e0b \\mathbf{r}_{(u, v)}=K\\left(\\mathbf{R} \\mathbf{x}_{(u, v)}^{\\mathrm{OC}}+\\mathbf{t}\\right)-\\left[\\begin{array}{l} u \\\\ v \\end{array}\\right] \\mathrm{x}^{\\mathrm{OC}}=\\mathrm{x}^{\\mathrm{NOC}} \\circ \\mathrm{d} \u5728\u63a8\u7406\u7684\u65f6\u5019\u4f7f\u7528 Uncertainty aware PnP \u5bf9\u8fd9\u4e2a\u7a20\u5bc6PnP\u95ee\u9898\u8fdb\u884c\u6c42\u89e3\u3002","title":"MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation"},{"location":"3dDetection/monorun/#monorun-monocular-3d-object-detection-by-reconstruction-and-uncertainty-propagation","text":"\u8fd9\u7bc7paper\u7528\u4e00\u4e2a\u4e8c\u9636\u6bb5\u7684\u6a21\u5757\u4ee5\u53ca\u7a20\u5bc6\u7684PnP\u5339\u914d\uff0c\u7ed5\u8fc7\u4e86\u5bf9\u8ddd\u79bb\u7684\u76f4\u63a5\u9884\u6d4b\u3002","title":"MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation"},{"location":"3dDetection/monorun/#_1","text":"\u5c3d\u7ba1\u672c\u6587\u5728self-supervision\u4e0a\u7684\u7528\u8bcd\u503c\u5f97\u5546\u69b7\uff0c\u4f46\u662f\u5176\u7b97\u6cd5\u503c\u5f97\u6a21\u4eff\u3002 \u9996\u5148\u662f\u4f7f\u7528\u4e8c\u9636\u6bb5\u7269\u4f53\u68c0\u6d4b\u5668\u6846\u51fa\u7269\u4f53\u5e76\u63d0\u53d6\u5176\u7279\u5f81\u3002\u8fd9\u4e2a\u7279\u5f81\u8f93\u51fa score, latent vector\u4ee5\u53ca\u7269\u4f53\u7684dimensions. \u518d\u8f93\u51fa\u6bcf\u4e00\u4e2a\u50cf\u7d20\u70b9\u5728\u8f66\u8f86\u5750\u6807\u7cfb\u4e0b\u7684\u4f4d\u7f6e normalized by dimensions\u3002 \u8bad\u7ec3\u7684\u65f6\u5019\u5229\u7528ground truth \u59ff\u6001\u8ba1\u7b97\u6bcf\u4e00\u4e2apixel predict\u7684\u4e09\u7ef4\u5750\u6807\uff0c\u7136\u540e\u53cd\u6295\u5f71\u56de\u6765,\u5f97\u5230\u8bef\u5dee\u5982\u4e0b \\mathbf{r}_{(u, v)}=K\\left(\\mathbf{R} \\mathbf{x}_{(u, v)}^{\\mathrm{OC}}+\\mathbf{t}\\right)-\\left[\\begin{array}{l} u \\\\ v \\end{array}\\right] \\mathrm{x}^{\\mathrm{OC}}=\\mathrm{x}^{\\mathrm{NOC}} \\circ \\mathrm{d} \u5728\u63a8\u7406\u7684\u65f6\u5019\u4f7f\u7528 Uncertainty aware PnP \u5bf9\u8fd9\u4e2a\u7a20\u5bc6PnP\u95ee\u9898\u8fdb\u884c\u6c42\u89e3\u3002","title":"\u505a\u6cd5"},{"location":"3dDetection/multisensor_refinement/","text":"Multi-Sensor 3D Object Box Refinement for Autonomous Driving \u8fd9\u7bc7\u8bba\u6587\u6e90\u81ea\u4e8e\u6c88\u5c11\u6770\u5b9e\u9a8c\u5ba4\u7684\u5b66\u957f\uff0c\u975e\u5e38\u503c\u5f97\u6ce8\u610f\u7684\u662f\u8fd9\u7bc7\u8bba\u6587\u4ece\u5355\u76ee\u89c6\u89c93D\u68c0\u6d4b\u51fa\u53d1\uff0c\u901a\u8fc7\u4e0d\u540c\u7684trick\u878d\u5408\u53cc\u76ee\u4ee5\u53ca\u70b9\u4e91\u4fe1\u606f(\u4e5f\u5c31\u662f\u8bf4\u5728\u6dfb\u52a0\u53cc\u76ee\u4ee5\u53ca\u6dfb\u52a0\u70b9\u4e91\u7684\u65f6\u5019\u4e0d\u9700\u8981\u91cd\u65b0train\u5168\u65b0\u7684\u7f51\u7edc\uff0c\u53ea\u9700\u8981train\u5c0f\u7f51\u7edc\u6216\u8005\u66f4\u6539post-opimization),\u662f\u4e00\u5957\u975e\u5e38\u79d1\u5b66\u53ef\u7528\u7684\u65b9\u6848\u3002 \u7ed3\u679c\u6765\u770b\uff0c\u5728\u5355\u76ee\u4e0a\u521b\u65b0\u6709\u9650\uff0c\u5728\u5355\u76ee-\u53cc\u76ee\u8054\u5408\u4e0a\u7528\u5904\u5f88\u5927\uff0c\u4e5f\u8fbe\u5230\u4e86\u53cc\u76ee\u7684SOTA(\u4ec5\u6b21\u4e8e\u8fd0\u7b97\u91cf\u66f4\u591a\u7684pseudo lidar)\uff0c\u53ef\u60dc\u5728lidar\u878d\u5408\u4e0a\u6570\u636e\u7ed3\u679c\u5e76\u6ca1\u6709\u663e\u8457\u9ad8\u4e8e\u7eafLidar\u7684SOTA\u65b9\u6848\u3002 \u6574\u4f53\u7ed3\u6784 \u4ece\u4e2d\u53ef\u77e5\u4e2d\u95f4\u4e00\u6761pipeline\u662f\u5b8c\u6574\u7684\u5355\u76ee\u9884\u6d4b\u8fc7\u7a0b\uff0c\u53cc\u76ee\u4ee5\u53ca\u70b9\u4e91\u5728\u8fd9\u91cc\u4e3b\u8981\u4f5c\u4e3a\u8f85\u52a9 \u5355\u76ee\u68c0\u6d4b\u505a\u6cd5 \u7b97\u662f\u6bd4\u8f83\u5e38\u89c1\u7684\u7ed3\u6784\uff0cFaster RCNN->RoI Align->residual prediction \u5bf9\u4e8e\u6df1\u5ea6\uff0c\u7f51\u7edc\u8f93\u51fa\u7684\u4e5f\u662f\u6b8b\u5dee\uff0c\u5bf9\u9ad8\u5ea6\u5176\u6b8b\u5dee\u7684base\u4f30\u8ba1\u4e3a z_{roi}=f_y\\frac{h}{h_{roio}} \u5176\u4e2d h \u4e3a3D\u7269\u4f53\u7684\u9ad8\u5ea6\uff0c h_{roi} \u4e3a2D RoI\u9ad8\u5ea6,\u7f51\u7edc\u8f93\u51fa\u7684\u6df1\u5ea6\u6b8b\u5dee\u4e3a log\\frac{z_{gt}}{z_{roi}} \uff0c\u5b9a\u4e49\u7684\u7f51\u7edc\u8f93\u51fa\u7684\u957f\u5bbd\u9ad8\u6b8b\u5dee \\Delta d = \\frac{d-p_d}{\\sigma_d} d \uff0c\u5176\u4e2d d \u4e3a (w,h,l) \u6700\u7ec8\u7684\u8f93\u51fa\u4e3a\u4e2d\u5fc3\u5728\u76f8\u673a\u4e2d\u7684\u6295\u5f71,\u6df1\u5ea6,\u957f\u5bbd\u9ad8\u7684\u6b8b\u5dee\uff0c\u4ee5\u53ca\u76f8\u5bf9\u89c2\u5bdf\u89d2\u7684sin,cos\u503c(\u89c2\u5bdf\u89d2\u7528multi-bin\u56de\u5f52) \u53cc\u76eeRefinement \u7a0d\u5fae\u6709\u70b9\u50cf Stereo RCNN \u7684\u60f3\u6cd5,\u901a\u8fc7\u5728\u5c40\u90e8\u56fe\u50cf\u4ece\u53f3\u76eewarp\u5230\u5de6\u76ee\u4e0aminimizing\u4e00\u4e2a\u5339\u914derror\u6765\u4f30\u8ba1\u6df1\u5ea6\u3002 \u8fd9\u91cc\u7684\u7b97\u6cd5\uff1a \u5728RoI\u4e2d\u7c7b\u4f3cSegmentation\u8f93\u51fa28*28 * 4 * classs\u7684sigmoid\u7ed3\u679c\uff0c\u5305\u542b\u4e00\u4e2a\u5206\u7c7b\u5668+\u4e09\u4e2a\u56de\u5f52\u5668,\u56de\u5f52\u90e8\u5206\u8868\u660e\u56fe\u50cf\u8fd9\u4e2a\u70b9\u5bf9\u5e94\u5750\u6807(normalized to [0,1] for each dimensions) \u6839\u636e\u5de6\u56fe\u8fd9\u4e2a\u533a\u57df\u6bcf\u4e00\u4e2a\u70b9\u9884\u6d4b\u7684normalized\u5750\u6807\u4ee5\u53ca\u9884\u6d4b\u7684\u5c3a\u5bf8\uff0c\u5c06\u8be5\u70b9\u5728\u56fe\u4e2d\u7684\u4f4d\u7f6e\u8f6c\u6362\u5230\u4e16\u754c\u5750\u6807\uff0c\u518d\u6295\u5f71\u5230\u7b2c\u4e8c\u5f20\u56fe\u4e2d\u3002 \u6211\u4eec\u9700\u8981\u51cf\u5c11\u5bf9\u5e94\u70b9\u4e4b\u95f4\u7684\u539f\u56fe\u5dee\u503c \u4f18\u5316\u6df1\u5ea6\u503c \u5b50\u7f51\u7edc\u7684label\u6765\u6e90\uff1a\u501f\u52a9\u70b9\u4e91\uff0c\u5148\u5f97\u5230\u8be5\u70b9\u4e91\u7684\u5b9e\u9645 instance vector\u503c\uff0c\u7136\u540e\u6295\u5f71\u56de\u56fe\u7247\u4e2d\u5f97\u5230\u50cf\u7d20\u7ea7label \u70b9\u4e91Refinement \u4f7f\u75282D detection\u7ed3\u679c\u7684 RoI\u63d0\u53d6\u51fa\u90e8\u5206\u70b9\u4e91\uff0c \u91cd\u91c7\u6837\u56fa\u5b9an\u4e2a\u70b9 point-wise instance seg\u7528\u4e8e\u533a\u5206foreground \u6839\u636eforeground\u7684probability,\u91cd\u91c7\u6837m\u4e2a\u70b9 T-Net\u7528\u4e8e\u4f30\u8ba1\u7269\u4f53\u4e2d\u5fc3\u4e0e\u51e0\u4f55\u3001\u65b9\u5411\u4fe1\u606f\uff0c\u8fd8\u8981\u505a\u4e00\u4e2ainstance vector\u7684\u4f30\u8ba1(\u7c7b\u4f3c\u4e8e\u53cc\u76ee\u7684\u7ed3\u679c) \u4f18\u5316\u8fd9\u4e2a\u51fd\u6570,\u5176\u4e2d ^cp_i \u4e3alidar\u70b9\u7684\u5750\u6807\uff0c ^c\\hat p_i \u4e3a\u6839\u636e\u4e2d\u5fc3\u70b9\u3001pose\u3001\u5f62\u72b6\u3001instsance vector\u4f30\u8ba1\u7684\u5750\u6807\u503c\uff0c\u7531\u4e8e\u53ea\u9700\u8981\u4f18\u5316\u6df1\u5ea6\u503c\uff0c\u6240\u4ee5\u53ef\u4ee5\u7ebf\u6027\u6c42\u89e3 \\mathbf{E}_{p} :=\\sum_{i=0}^{m}\\left\\|^{c} \\mathbf{p}_{i}-^{c} \\hat{\\mathbf{p}}_{i}\\right\\|, \\text { with } \\quad^{c} \\hat{\\mathbf{p}}_{i}=\\hat{\\mathbf{p}}_{o}+\\mathbf{R}(\\theta)^{o} \\mathbf{p}_{i} \u66f4\u591a\u4f20\u611f\u5668\u7684\u878d\u5408 \\begin{aligned} \\mathbf{p}_{o}=\\underset{\\mathbf{p}_{o}}{\\arg \\min } & \\sum_{i=0}^{n}\\left\\|I_{l}\\left(\\mathbf{u}_{i}\\right)-I_{r}\\left(\\pi\\left(^{c} \\hat{\\mathbf{p}}_{i}-\\mathbf{b}\\right)\\right)\\right\\|_{\\sum_{s}}+\\\\ & \\sum_{j=0}^{m}\\left\\|^{c} \\mathbf{p}_{j}-^{c} \\hat{\\mathbf{p}}_{j}\\right\\|_{\\Sigma_{p}} \\end{aligned} \u4e24\u9879\u5206\u522b\u5904\u7406\u76f8\u673a\u4ee5\u53ca\u70b9\u4e91\u6570\u636e\u3002 \u5176\u4ed6\u8bad\u7ec3\u7ec6\u8282 \u635f\u5931\u51fd\u6570\u5305\u62ecRPN, 2D\u68c0\u6d4b\uff0c\u89d2\u5ea6\uff0c\u4f4d\u7f6e\uff0c\u7ef4\u5ea6\u3002\u7528 multi-loss \u7ed3\u5408 \u4e3a\u4e86\u7ed9\u56fe\u7247\u7f51\u8def\u63d0\u4f9b\u4f4d\u7f6e\uff0c\u5728\u8f93\u51fa\u7aef\u52a0\u5165u,v\u5750\u6807(grid channels),\u7b2c\u4e00\u4e2aConv\u7684\u6743\u91cd\u88ab\u590d\u5236\u91cd\u7528\u7ed9\u8fd9\u65b0\u7684channel.","title":"Multi-Sensor 3D Object Box Refinement for Autonomous Driving"},{"location":"3dDetection/multisensor_refinement/#multi-sensor-3d-object-box-refinement-for-autonomous-driving","text":"\u8fd9\u7bc7\u8bba\u6587\u6e90\u81ea\u4e8e\u6c88\u5c11\u6770\u5b9e\u9a8c\u5ba4\u7684\u5b66\u957f\uff0c\u975e\u5e38\u503c\u5f97\u6ce8\u610f\u7684\u662f\u8fd9\u7bc7\u8bba\u6587\u4ece\u5355\u76ee\u89c6\u89c93D\u68c0\u6d4b\u51fa\u53d1\uff0c\u901a\u8fc7\u4e0d\u540c\u7684trick\u878d\u5408\u53cc\u76ee\u4ee5\u53ca\u70b9\u4e91\u4fe1\u606f(\u4e5f\u5c31\u662f\u8bf4\u5728\u6dfb\u52a0\u53cc\u76ee\u4ee5\u53ca\u6dfb\u52a0\u70b9\u4e91\u7684\u65f6\u5019\u4e0d\u9700\u8981\u91cd\u65b0train\u5168\u65b0\u7684\u7f51\u7edc\uff0c\u53ea\u9700\u8981train\u5c0f\u7f51\u7edc\u6216\u8005\u66f4\u6539post-opimization),\u662f\u4e00\u5957\u975e\u5e38\u79d1\u5b66\u53ef\u7528\u7684\u65b9\u6848\u3002 \u7ed3\u679c\u6765\u770b\uff0c\u5728\u5355\u76ee\u4e0a\u521b\u65b0\u6709\u9650\uff0c\u5728\u5355\u76ee-\u53cc\u76ee\u8054\u5408\u4e0a\u7528\u5904\u5f88\u5927\uff0c\u4e5f\u8fbe\u5230\u4e86\u53cc\u76ee\u7684SOTA(\u4ec5\u6b21\u4e8e\u8fd0\u7b97\u91cf\u66f4\u591a\u7684pseudo lidar)\uff0c\u53ef\u60dc\u5728lidar\u878d\u5408\u4e0a\u6570\u636e\u7ed3\u679c\u5e76\u6ca1\u6709\u663e\u8457\u9ad8\u4e8e\u7eafLidar\u7684SOTA\u65b9\u6848\u3002","title":"Multi-Sensor 3D Object Box Refinement for Autonomous Driving"},{"location":"3dDetection/multisensor_refinement/#_1","text":"\u4ece\u4e2d\u53ef\u77e5\u4e2d\u95f4\u4e00\u6761pipeline\u662f\u5b8c\u6574\u7684\u5355\u76ee\u9884\u6d4b\u8fc7\u7a0b\uff0c\u53cc\u76ee\u4ee5\u53ca\u70b9\u4e91\u5728\u8fd9\u91cc\u4e3b\u8981\u4f5c\u4e3a\u8f85\u52a9","title":"\u6574\u4f53\u7ed3\u6784"},{"location":"3dDetection/multisensor_refinement/#_2","text":"\u7b97\u662f\u6bd4\u8f83\u5e38\u89c1\u7684\u7ed3\u6784\uff0cFaster RCNN->RoI Align->residual prediction \u5bf9\u4e8e\u6df1\u5ea6\uff0c\u7f51\u7edc\u8f93\u51fa\u7684\u4e5f\u662f\u6b8b\u5dee\uff0c\u5bf9\u9ad8\u5ea6\u5176\u6b8b\u5dee\u7684base\u4f30\u8ba1\u4e3a z_{roi}=f_y\\frac{h}{h_{roio}} \u5176\u4e2d h \u4e3a3D\u7269\u4f53\u7684\u9ad8\u5ea6\uff0c h_{roi} \u4e3a2D RoI\u9ad8\u5ea6,\u7f51\u7edc\u8f93\u51fa\u7684\u6df1\u5ea6\u6b8b\u5dee\u4e3a log\\frac{z_{gt}}{z_{roi}} \uff0c\u5b9a\u4e49\u7684\u7f51\u7edc\u8f93\u51fa\u7684\u957f\u5bbd\u9ad8\u6b8b\u5dee \\Delta d = \\frac{d-p_d}{\\sigma_d} d \uff0c\u5176\u4e2d d \u4e3a (w,h,l) \u6700\u7ec8\u7684\u8f93\u51fa\u4e3a\u4e2d\u5fc3\u5728\u76f8\u673a\u4e2d\u7684\u6295\u5f71,\u6df1\u5ea6,\u957f\u5bbd\u9ad8\u7684\u6b8b\u5dee\uff0c\u4ee5\u53ca\u76f8\u5bf9\u89c2\u5bdf\u89d2\u7684sin,cos\u503c(\u89c2\u5bdf\u89d2\u7528multi-bin\u56de\u5f52)","title":"\u5355\u76ee\u68c0\u6d4b\u505a\u6cd5"},{"location":"3dDetection/multisensor_refinement/#refinement","text":"\u7a0d\u5fae\u6709\u70b9\u50cf Stereo RCNN \u7684\u60f3\u6cd5,\u901a\u8fc7\u5728\u5c40\u90e8\u56fe\u50cf\u4ece\u53f3\u76eewarp\u5230\u5de6\u76ee\u4e0aminimizing\u4e00\u4e2a\u5339\u914derror\u6765\u4f30\u8ba1\u6df1\u5ea6\u3002 \u8fd9\u91cc\u7684\u7b97\u6cd5\uff1a \u5728RoI\u4e2d\u7c7b\u4f3cSegmentation\u8f93\u51fa28*28 * 4 * classs\u7684sigmoid\u7ed3\u679c\uff0c\u5305\u542b\u4e00\u4e2a\u5206\u7c7b\u5668+\u4e09\u4e2a\u56de\u5f52\u5668,\u56de\u5f52\u90e8\u5206\u8868\u660e\u56fe\u50cf\u8fd9\u4e2a\u70b9\u5bf9\u5e94\u5750\u6807(normalized to [0,1] for each dimensions) \u6839\u636e\u5de6\u56fe\u8fd9\u4e2a\u533a\u57df\u6bcf\u4e00\u4e2a\u70b9\u9884\u6d4b\u7684normalized\u5750\u6807\u4ee5\u53ca\u9884\u6d4b\u7684\u5c3a\u5bf8\uff0c\u5c06\u8be5\u70b9\u5728\u56fe\u4e2d\u7684\u4f4d\u7f6e\u8f6c\u6362\u5230\u4e16\u754c\u5750\u6807\uff0c\u518d\u6295\u5f71\u5230\u7b2c\u4e8c\u5f20\u56fe\u4e2d\u3002 \u6211\u4eec\u9700\u8981\u51cf\u5c11\u5bf9\u5e94\u70b9\u4e4b\u95f4\u7684\u539f\u56fe\u5dee\u503c \u4f18\u5316\u6df1\u5ea6\u503c \u5b50\u7f51\u7edc\u7684label\u6765\u6e90\uff1a\u501f\u52a9\u70b9\u4e91\uff0c\u5148\u5f97\u5230\u8be5\u70b9\u4e91\u7684\u5b9e\u9645 instance vector\u503c\uff0c\u7136\u540e\u6295\u5f71\u56de\u56fe\u7247\u4e2d\u5f97\u5230\u50cf\u7d20\u7ea7label","title":"\u53cc\u76eeRefinement"},{"location":"3dDetection/multisensor_refinement/#refinement_1","text":"\u4f7f\u75282D detection\u7ed3\u679c\u7684 RoI\u63d0\u53d6\u51fa\u90e8\u5206\u70b9\u4e91\uff0c \u91cd\u91c7\u6837\u56fa\u5b9an\u4e2a\u70b9 point-wise instance seg\u7528\u4e8e\u533a\u5206foreground \u6839\u636eforeground\u7684probability,\u91cd\u91c7\u6837m\u4e2a\u70b9 T-Net\u7528\u4e8e\u4f30\u8ba1\u7269\u4f53\u4e2d\u5fc3\u4e0e\u51e0\u4f55\u3001\u65b9\u5411\u4fe1\u606f\uff0c\u8fd8\u8981\u505a\u4e00\u4e2ainstance vector\u7684\u4f30\u8ba1(\u7c7b\u4f3c\u4e8e\u53cc\u76ee\u7684\u7ed3\u679c) \u4f18\u5316\u8fd9\u4e2a\u51fd\u6570,\u5176\u4e2d ^cp_i \u4e3alidar\u70b9\u7684\u5750\u6807\uff0c ^c\\hat p_i \u4e3a\u6839\u636e\u4e2d\u5fc3\u70b9\u3001pose\u3001\u5f62\u72b6\u3001instsance vector\u4f30\u8ba1\u7684\u5750\u6807\u503c\uff0c\u7531\u4e8e\u53ea\u9700\u8981\u4f18\u5316\u6df1\u5ea6\u503c\uff0c\u6240\u4ee5\u53ef\u4ee5\u7ebf\u6027\u6c42\u89e3 \\mathbf{E}_{p} :=\\sum_{i=0}^{m}\\left\\|^{c} \\mathbf{p}_{i}-^{c} \\hat{\\mathbf{p}}_{i}\\right\\|, \\text { with } \\quad^{c} \\hat{\\mathbf{p}}_{i}=\\hat{\\mathbf{p}}_{o}+\\mathbf{R}(\\theta)^{o} \\mathbf{p}_{i}","title":"\u70b9\u4e91Refinement"},{"location":"3dDetection/multisensor_refinement/#_3","text":"\\begin{aligned} \\mathbf{p}_{o}=\\underset{\\mathbf{p}_{o}}{\\arg \\min } & \\sum_{i=0}^{n}\\left\\|I_{l}\\left(\\mathbf{u}_{i}\\right)-I_{r}\\left(\\pi\\left(^{c} \\hat{\\mathbf{p}}_{i}-\\mathbf{b}\\right)\\right)\\right\\|_{\\sum_{s}}+\\\\ & \\sum_{j=0}^{m}\\left\\|^{c} \\mathbf{p}_{j}-^{c} \\hat{\\mathbf{p}}_{j}\\right\\|_{\\Sigma_{p}} \\end{aligned} \u4e24\u9879\u5206\u522b\u5904\u7406\u76f8\u673a\u4ee5\u53ca\u70b9\u4e91\u6570\u636e\u3002","title":"\u66f4\u591a\u4f20\u611f\u5668\u7684\u878d\u5408"},{"location":"3dDetection/multisensor_refinement/#_4","text":"\u635f\u5931\u51fd\u6570\u5305\u62ecRPN, 2D\u68c0\u6d4b\uff0c\u89d2\u5ea6\uff0c\u4f4d\u7f6e\uff0c\u7ef4\u5ea6\u3002\u7528 multi-loss \u7ed3\u5408 \u4e3a\u4e86\u7ed9\u56fe\u7247\u7f51\u8def\u63d0\u4f9b\u4f4d\u7f6e\uff0c\u5728\u8f93\u51fa\u7aef\u52a0\u5165u,v\u5750\u6807(grid channels),\u7b2c\u4e00\u4e2aConv\u7684\u6743\u91cd\u88ab\u590d\u5236\u91cd\u7528\u7ed9\u8fd9\u65b0\u7684channel.","title":"\u5176\u4ed6\u8bad\u7ec3\u7ec6\u8282"},{"location":"3dDetection/my_cookbook/","text":"Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS One of the most often required features of visual detection 3D is a full tool chain including data / model training / model testing / ROS deployment / demonstration and more. In this cookbook, a set of tools are introduced to help this entire process. It mainly involves three open-source repo: kitti_visualize visualDet3D visualDet3D_ros Menu: Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS 1. Data 2. Training / Numerical Testing 3. Visualize offline test results (Optional) 4. Visualize real-time prediction results in ROS. 5. Visualize real-time streaming in ROS. 6. Streaming results on other/customized datasets 1. Data Download the KITTI object dataset to local. The file structure should end up like /data/kitti/{training/testing}/{image_2/calib/label_2...}. Please follow the setup process of kitti_visualize and make sure you could correctly visualize kitti object data ( unselect isSequential in the GUI, and playing around with the index number). Now you can play around with the data and also confirm the file structure is fine. 2. Training / Numerical Testing Checkout the training pipeline in visualDet3D for mono3D . If you do not want to train, one of the methods is to download the code and assets at Release 1.0 of visualDet3D to reproduce the result of a pre-uploaded checkpoint. 3. Visualize offline test results (Optional) Checkout the additional labels part. Basically, you need to copy the texts under workdirs/Yolo3D/output/testing/data of visualDet3D into /data/kitti_obj/testing/additional_label_2 , and launch kitti_visualize on the testing split to visual the offline results. 4. Visualize real-time prediction results in ROS. Checkout the setup process of visualDet3D_ros . Notice to modify the \"visual3d_path\", \"cfg_file\", \"weight_path\" parameters based on your own setting of visualDet3D. Launch both kitti_visualize and visualDet3D_ros , unselect isSequential in the GUI. With the default launch file, the two node publish and read on the same image/camera_param topics. By adjusting what is shown in the RVIZ, we can now visualize the inference result in real-time. 5. Visualize real-time streaming in ROS. We suggest download the KITTI raw dataset to streaming out image/lidar data. Please follow the setup process of kitti_visualize and make sure you could correctly visualize kitti raw data ( select isSequential in the GUI, and playing around with the index number). Launch visualDet3D_ros as usual. With the sequence data in kitti_visualize keep streaming into ROS, visualDet3D_ros will also streamingly conduct inference on the images, producing demo results like the one on the readme page of visualDet3D_ros . Notice, that the IO/computational stress will be high at this step. You could adjust the streaming speed of kitti_visualize by changing local param in the launch file(UPDATE_FREQUENCY) or directly modifying the code. 6. Streaming results on other/customized datasets After sucessfully setting up visualDet3D_ros , practically we can test the inference of visualDet3D on any image streams with ROS interface (like USB webcam). We provide ROS interface to nuScenes dataset and KITTI-360 dataset . To get more robust results, training on customized datasets, or trying camera-insensitive algorithms (like the adapted version of MonoFlex in visualDet3D) worth the efforts. By adapting your own dataset to kitti/kitti360/nuscenes formats, you can also make use of these existing tools to boost development of your 3D detection project.","title":"Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS"},{"location":"3dDetection/my_cookbook/#synthetic-cookbook-for-usingtestingdemonstrating-visualdet3d-in-ros","text":"One of the most often required features of visual detection 3D is a full tool chain including data / model training / model testing / ROS deployment / demonstration and more. In this cookbook, a set of tools are introduced to help this entire process. It mainly involves three open-source repo: kitti_visualize visualDet3D visualDet3D_ros Menu: Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS 1. Data 2. Training / Numerical Testing 3. Visualize offline test results (Optional) 4. Visualize real-time prediction results in ROS. 5. Visualize real-time streaming in ROS. 6. Streaming results on other/customized datasets","title":"Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS"},{"location":"3dDetection/my_cookbook/#1-data","text":"Download the KITTI object dataset to local. The file structure should end up like /data/kitti/{training/testing}/{image_2/calib/label_2...}. Please follow the setup process of kitti_visualize and make sure you could correctly visualize kitti object data ( unselect isSequential in the GUI, and playing around with the index number). Now you can play around with the data and also confirm the file structure is fine.","title":"1. Data"},{"location":"3dDetection/my_cookbook/#2-training-numerical-testing","text":"Checkout the training pipeline in visualDet3D for mono3D . If you do not want to train, one of the methods is to download the code and assets at Release 1.0 of visualDet3D to reproduce the result of a pre-uploaded checkpoint.","title":"2. Training / Numerical Testing"},{"location":"3dDetection/my_cookbook/#3-visualize-offline-test-results-optional","text":"Checkout the additional labels part. Basically, you need to copy the texts under workdirs/Yolo3D/output/testing/data of visualDet3D into /data/kitti_obj/testing/additional_label_2 , and launch kitti_visualize on the testing split to visual the offline results.","title":"3. Visualize offline test results (Optional)"},{"location":"3dDetection/my_cookbook/#4-visualize-real-time-prediction-results-in-ros","text":"Checkout the setup process of visualDet3D_ros . Notice to modify the \"visual3d_path\", \"cfg_file\", \"weight_path\" parameters based on your own setting of visualDet3D. Launch both kitti_visualize and visualDet3D_ros , unselect isSequential in the GUI. With the default launch file, the two node publish and read on the same image/camera_param topics. By adjusting what is shown in the RVIZ, we can now visualize the inference result in real-time.","title":"4. Visualize real-time prediction results in ROS."},{"location":"3dDetection/my_cookbook/#5-visualize-real-time-streaming-in-ros","text":"We suggest download the KITTI raw dataset to streaming out image/lidar data. Please follow the setup process of kitti_visualize and make sure you could correctly visualize kitti raw data ( select isSequential in the GUI, and playing around with the index number). Launch visualDet3D_ros as usual. With the sequence data in kitti_visualize keep streaming into ROS, visualDet3D_ros will also streamingly conduct inference on the images, producing demo results like the one on the readme page of visualDet3D_ros . Notice, that the IO/computational stress will be high at this step. You could adjust the streaming speed of kitti_visualize by changing local param in the launch file(UPDATE_FREQUENCY) or directly modifying the code.","title":"5. Visualize real-time streaming in ROS."},{"location":"3dDetection/my_cookbook/#6-streaming-results-on-othercustomized-datasets","text":"After sucessfully setting up visualDet3D_ros , practically we can test the inference of visualDet3D on any image streams with ROS interface (like USB webcam). We provide ROS interface to nuScenes dataset and KITTI-360 dataset . To get more robust results, training on customized datasets, or trying camera-insensitive algorithms (like the adapted version of MonoFlex in visualDet3D) worth the efforts. By adapting your own dataset to kitti/kitti360/nuscenes formats, you can also make use of these existing tools to boost development of your 3D detection project.","title":"6. Streaming results on other/customized datasets"},{"location":"3dDetection/pointnet_related/","text":"Collections on PointNet and follow-ups PointNet\u662f\u76ee\u524d\u70b9\u4e91\u5904\u7406\u4e2d\u76f8\u5f53\u91cd\u8981\u7684\u4e00\u4e2a\u90e8\u5206\uff0c\u4e0eVoxel\u603b\u4f53\u5e76\u884c\u3002 Related paper: Frustum Pointnet PointNet pdf code PointNet\u6709\u4e00\u4e2a\u5f88\u597d\u7684CSDN\u4ecb\u7ecd \u535a\u5ba2 . \u6838\u5fc3\u601d\u60f3: \u8fd0\u7b97\u5e94\u5f53\u662f\u8f6e\u6362\u4e0d\u53d8\u7684\uff0c\u9002\u5e94\u65e0\u5e8f\u7684\u70b9\u4e91\u3002 \u7406\u89e3\u70b9\u4e91\u7684\u5173\u952e\u70b9\u5728\u4e8e\u627e\u5230\u5408\u9002\u7684\u6295\u5f71\uff0c\u8fb9\u754c/\u9762\u4e0a\u7684\u5173\u952e\u70b9\u51b3\u5b9a\u4e86\u5bf9\u70b9\u4e91\u7684\u7406\u89e3\u3002 \u7b2c\u4e00\u70b9\u7ed9\u51fa\u7684\u542f\u53d1\u662f\u4ee5\u5168\u8fde\u63a5\u5c42\u3001\u5750\u6807\u8f6c\u6362\u5c42\u4ee5\u53camax-pooling\u4e3a\u4e3b\u8981\u8fd0\u7b97\u65b9\u5f0f\u3002 \u5750\u6807\u8f6c\u6362\u5c42\u4e0e\u5168\u8fde\u63a5\u5c42\u4e0d\u540c\u7684\u5730\u65b9\u5728\u4e8e\u8f6c\u6362\u6743\u91cd\u77e9\u9635\u7684\u884c\u5217\u5f0f\u5e94\u4e0e\u65cb\u8f6c\u77e9\u9635\u6027\u8d28\u7c7b\u4f3c\uff0c\u4f5c\u8005\u7ed9\u51fa\u4e86\u8fd9\u6837Regularization Loss \u4f5c\u4e3a\u7ea6\u675f: L_{r e g}=\\left\\|I-A A^{T}\\right\\|_{F}^{2} \u7b2c\u4e8c\u70b9\u5f15\u5165\u4e86Critical Points\u7684\u6982\u5ff5\uff0c\u7b80\u5355\u800c\u8a00\u5c31\u662f\u6700\u540eMaxpooling\u4e2d\uff0c\u5982\u679c\u70b9 p \u6709\u81f3\u5c11\u4e00\u4e2achannel\u662fMaxpooling\u64cd\u4f5c\u5bf9\u5e94\u7684 argmax ,\u90a3\u4e48\u70b9 p \u5c31\u662f\u4e00\u4e2acritical point.\u663e\u7136\uff0cCritical Point\u7684\u6570\u91cf\u4e0d\u80fd\u8d85\u8fc7\u8fd9\u4e2a\u77e2\u91cf\u7684channel\u6570\uff0c\u56fe\u4e2d\u4e3a1024. \u4f5c\u8005\u901a\u8fc7\u53ef\u89c6\u5316\u8868\u660e\u4e86\u8fd9\u91cc\u7684Critical Point\u5728\u5b9e\u9645\u4e2d\u4e5f\u57fa\u672c\u5bf9\u5e94\u7a00\u758f\u7684\u3001\u4e00\u7ec4\u6700\u6709\u8868\u8fbe\u529b\u7684\u70b9\u3002 \u8bed\u4e49\u5206\u5272\u7f51\u7edc\u4e2d\uff0c\u4f5c\u8005\u5c06\u5168\u5c40\u77e2\u91cfconcat\u5230\u6bcf\u4e2a\u70b9\u81ea\u5df1\u7684feature\u4e2d\uff0c\u7136\u540e\u5168\u8fde\u63a5\u8f93\u51fa\u5206\u7c7b\u3002 PointNet++ pdf code Faster Pytorch \u4ece\u5e94\u7528\u573a\u666f\u53ef\u77e5\uff0cPointNet\u9762\u5bf9\u7684\u662f\u7b80\u5355\u7269\u4f53\u7684mesh/point cloud\u7ec4\u6210\uff0c\u4f46\u662f\u5728\u9762\u5bf9\u5927\u573a\u666f\u65f6\u6709\u4e24\u4e2a\u95ee\u9898\u3002\u7b2c\u4e00\u4e2a\u662f\u65e0\u6cd5\u5904\u7406\u6570\u91cf\u5de8\u5927\u7684\u70b9\u4e91\uff0c\u7b2c\u4e8c\u4e2a\u662f\u7531\u4e8e\u53ca\u5176\u5f3a\u8c03\u65e0\u5e8f\u6027\u4ee5\u53ca\u4f9d\u8d56\u4e8e\u5168\u5c40feature\uff0c\u5e26\u6765\u7684\u6838\u5fc3\u95ee\u9898\u662f\u65e0\u6cd5\u5bf9\u5c40\u90e8\u70b9\u4e91\u7ed3\u6784\u8fdb\u884c\u63a8\u7406\uff0c\u6ca1\u6709\u5229\u7528\u5927\u573a\u666f\u4e2d\u70b9\u4e91\u591a\u5904\u5c0f\u89c4\u6a21\u805a\u7c7b\u7684\u7279\u70b9\u3002 Set Abstraction Layer = sampling + grouping + pointnet; Sampling\u4f7f\u7528\u6700\u8fdc\u70b9\u91c7\u6837(FPS), code .\u5faa\u73af\u91c7\u6837\uff0c\u6bcf\u4e00\u4e2a\u65b0\u91c7\u6837\u7684\u70b9\u8ddd\u79bb\u73b0\u6709\u70b9\u7684\u8ddd\u79bb\u3002 Grouping\u4f7f\u7528Ball query\u5bfb\u627ekeypoint\u9644\u8fd1K\u4e2a\u6700\u8fd1\u7684\u70b9\uff0c\u8f93\u5165\u4e3a N\\times (d+C) \u4e2a\u70b9\uff0c\u4e2d\u5fc3\u70b9 N'\\times d ,\u8f93\u51fa\u4e3a N'\\times K\\times (d+C) , code PointNet Layer,\u5c06K\u4e2a\u6700\u8fd1\u7684\u70b9\u8f93\u5165\u5230pointNet\u4e2d\u3002\u5f97\u5230\u8fd9\u4e2a\u805a\u7c7b\u7684\u65b0features Point Feature Propagation for Set Segmentation \u8fd9\u5bf9\u5e94\u7684\u662fskip connection \u4ee5\u53ca\u4e0a\u91c7\u6837\u7684\u6b65\u9aa4\uff0c\u4f5c\u8005\u91c7\u7528\u7684\u662f\u4e0e\u73b0\u6709\u70b9\u8ddd\u79bb\u6210\u53cd\u6bd4\u76843D interpolation, code . f^{(j)}(x)=\\frac{\\sum_{i=1}^{k} w_{i}(x) f_{i}^{(j)}}{\\sum_{i=1}^{k} w_{i}(x)} \\quad \\text { where } \\quad w_{i}(x)=\\frac{1}{d\\left(x, x_{i}\\right)^{p}}, j=1, \\ldots, C \u5c06\u7f51\u7edc\u524d\u9762\u7684features \u7ecf\u8fc7 1\\times 1 \u5377\u79efconcat\u5230\u63d2\u503c\u7ed3\u679c\u4e2d\u3002 PointRCNN: 3D Object Proposal Generation and Detection from Point Cloud pdf code \u8fd9\u662f\u4e00\u4e2atwo-stage\u7684model, \u4f7f\u7528segmentation pointcloud\u7f51\u7edc\uff0c\u5f97\u5230\u6bcf\u4e00\u4e2a\u70b9\u7684features\uff0c\u751f\u6210\u4e00\u4e2aanchor 3D box\uff0c\u4ee5\u53ca\u524d\u540e\u666f\u5206\u5272\u3002\u4f5c\u8005\u5bf9anchor 3D box\u7528\u7684\u662fbin-based generation,\u4f7f\u5f97\u8fd9\u91cc\u7684\u6bcf\u4e00\u4e2a\u8f93\u51fa\u90fd\u4f1a\u662f\u5206\u7c7b\u8f93\u51fa\u3002","title":"Collections on PointNet and follow-ups"},{"location":"3dDetection/pointnet_related/#collections-on-pointnet-and-follow-ups","text":"PointNet\u662f\u76ee\u524d\u70b9\u4e91\u5904\u7406\u4e2d\u76f8\u5f53\u91cd\u8981\u7684\u4e00\u4e2a\u90e8\u5206\uff0c\u4e0eVoxel\u603b\u4f53\u5e76\u884c\u3002 Related paper: Frustum Pointnet","title":"Collections on PointNet and follow-ups"},{"location":"3dDetection/pointnet_related/#pointnet","text":"pdf code PointNet\u6709\u4e00\u4e2a\u5f88\u597d\u7684CSDN\u4ecb\u7ecd \u535a\u5ba2 . \u6838\u5fc3\u601d\u60f3: \u8fd0\u7b97\u5e94\u5f53\u662f\u8f6e\u6362\u4e0d\u53d8\u7684\uff0c\u9002\u5e94\u65e0\u5e8f\u7684\u70b9\u4e91\u3002 \u7406\u89e3\u70b9\u4e91\u7684\u5173\u952e\u70b9\u5728\u4e8e\u627e\u5230\u5408\u9002\u7684\u6295\u5f71\uff0c\u8fb9\u754c/\u9762\u4e0a\u7684\u5173\u952e\u70b9\u51b3\u5b9a\u4e86\u5bf9\u70b9\u4e91\u7684\u7406\u89e3\u3002 \u7b2c\u4e00\u70b9\u7ed9\u51fa\u7684\u542f\u53d1\u662f\u4ee5\u5168\u8fde\u63a5\u5c42\u3001\u5750\u6807\u8f6c\u6362\u5c42\u4ee5\u53camax-pooling\u4e3a\u4e3b\u8981\u8fd0\u7b97\u65b9\u5f0f\u3002 \u5750\u6807\u8f6c\u6362\u5c42\u4e0e\u5168\u8fde\u63a5\u5c42\u4e0d\u540c\u7684\u5730\u65b9\u5728\u4e8e\u8f6c\u6362\u6743\u91cd\u77e9\u9635\u7684\u884c\u5217\u5f0f\u5e94\u4e0e\u65cb\u8f6c\u77e9\u9635\u6027\u8d28\u7c7b\u4f3c\uff0c\u4f5c\u8005\u7ed9\u51fa\u4e86\u8fd9\u6837Regularization Loss \u4f5c\u4e3a\u7ea6\u675f: L_{r e g}=\\left\\|I-A A^{T}\\right\\|_{F}^{2} \u7b2c\u4e8c\u70b9\u5f15\u5165\u4e86Critical Points\u7684\u6982\u5ff5\uff0c\u7b80\u5355\u800c\u8a00\u5c31\u662f\u6700\u540eMaxpooling\u4e2d\uff0c\u5982\u679c\u70b9 p \u6709\u81f3\u5c11\u4e00\u4e2achannel\u662fMaxpooling\u64cd\u4f5c\u5bf9\u5e94\u7684 argmax ,\u90a3\u4e48\u70b9 p \u5c31\u662f\u4e00\u4e2acritical point.\u663e\u7136\uff0cCritical Point\u7684\u6570\u91cf\u4e0d\u80fd\u8d85\u8fc7\u8fd9\u4e2a\u77e2\u91cf\u7684channel\u6570\uff0c\u56fe\u4e2d\u4e3a1024. \u4f5c\u8005\u901a\u8fc7\u53ef\u89c6\u5316\u8868\u660e\u4e86\u8fd9\u91cc\u7684Critical Point\u5728\u5b9e\u9645\u4e2d\u4e5f\u57fa\u672c\u5bf9\u5e94\u7a00\u758f\u7684\u3001\u4e00\u7ec4\u6700\u6709\u8868\u8fbe\u529b\u7684\u70b9\u3002 \u8bed\u4e49\u5206\u5272\u7f51\u7edc\u4e2d\uff0c\u4f5c\u8005\u5c06\u5168\u5c40\u77e2\u91cfconcat\u5230\u6bcf\u4e2a\u70b9\u81ea\u5df1\u7684feature\u4e2d\uff0c\u7136\u540e\u5168\u8fde\u63a5\u8f93\u51fa\u5206\u7c7b\u3002","title":"PointNet"},{"location":"3dDetection/pointnet_related/#pointnet_1","text":"pdf code Faster Pytorch \u4ece\u5e94\u7528\u573a\u666f\u53ef\u77e5\uff0cPointNet\u9762\u5bf9\u7684\u662f\u7b80\u5355\u7269\u4f53\u7684mesh/point cloud\u7ec4\u6210\uff0c\u4f46\u662f\u5728\u9762\u5bf9\u5927\u573a\u666f\u65f6\u6709\u4e24\u4e2a\u95ee\u9898\u3002\u7b2c\u4e00\u4e2a\u662f\u65e0\u6cd5\u5904\u7406\u6570\u91cf\u5de8\u5927\u7684\u70b9\u4e91\uff0c\u7b2c\u4e8c\u4e2a\u662f\u7531\u4e8e\u53ca\u5176\u5f3a\u8c03\u65e0\u5e8f\u6027\u4ee5\u53ca\u4f9d\u8d56\u4e8e\u5168\u5c40feature\uff0c\u5e26\u6765\u7684\u6838\u5fc3\u95ee\u9898\u662f\u65e0\u6cd5\u5bf9\u5c40\u90e8\u70b9\u4e91\u7ed3\u6784\u8fdb\u884c\u63a8\u7406\uff0c\u6ca1\u6709\u5229\u7528\u5927\u573a\u666f\u4e2d\u70b9\u4e91\u591a\u5904\u5c0f\u89c4\u6a21\u805a\u7c7b\u7684\u7279\u70b9\u3002","title":"PointNet++"},{"location":"3dDetection/pointnet_related/#set-abstraction-layer-sampling-grouping-pointnet","text":"Sampling\u4f7f\u7528\u6700\u8fdc\u70b9\u91c7\u6837(FPS), code .\u5faa\u73af\u91c7\u6837\uff0c\u6bcf\u4e00\u4e2a\u65b0\u91c7\u6837\u7684\u70b9\u8ddd\u79bb\u73b0\u6709\u70b9\u7684\u8ddd\u79bb\u3002 Grouping\u4f7f\u7528Ball query\u5bfb\u627ekeypoint\u9644\u8fd1K\u4e2a\u6700\u8fd1\u7684\u70b9\uff0c\u8f93\u5165\u4e3a N\\times (d+C) \u4e2a\u70b9\uff0c\u4e2d\u5fc3\u70b9 N'\\times d ,\u8f93\u51fa\u4e3a N'\\times K\\times (d+C) , code PointNet Layer,\u5c06K\u4e2a\u6700\u8fd1\u7684\u70b9\u8f93\u5165\u5230pointNet\u4e2d\u3002\u5f97\u5230\u8fd9\u4e2a\u805a\u7c7b\u7684\u65b0features","title":"Set Abstraction Layer = sampling + grouping + pointnet;"},{"location":"3dDetection/pointnet_related/#point-feature-propagation-for-set-segmentation","text":"\u8fd9\u5bf9\u5e94\u7684\u662fskip connection \u4ee5\u53ca\u4e0a\u91c7\u6837\u7684\u6b65\u9aa4\uff0c\u4f5c\u8005\u91c7\u7528\u7684\u662f\u4e0e\u73b0\u6709\u70b9\u8ddd\u79bb\u6210\u53cd\u6bd4\u76843D interpolation, code . f^{(j)}(x)=\\frac{\\sum_{i=1}^{k} w_{i}(x) f_{i}^{(j)}}{\\sum_{i=1}^{k} w_{i}(x)} \\quad \\text { where } \\quad w_{i}(x)=\\frac{1}{d\\left(x, x_{i}\\right)^{p}}, j=1, \\ldots, C \u5c06\u7f51\u7edc\u524d\u9762\u7684features \u7ecf\u8fc7 1\\times 1 \u5377\u79efconcat\u5230\u63d2\u503c\u7ed3\u679c\u4e2d\u3002","title":"Point Feature Propagation for Set Segmentation"},{"location":"3dDetection/pointnet_related/#pointrcnn-3d-object-proposal-generation-and-detection-from-point-cloud","text":"pdf code \u8fd9\u662f\u4e00\u4e2atwo-stage\u7684model, \u4f7f\u7528segmentation pointcloud\u7f51\u7edc\uff0c\u5f97\u5230\u6bcf\u4e00\u4e2a\u70b9\u7684features\uff0c\u751f\u6210\u4e00\u4e2aanchor 3D box\uff0c\u4ee5\u53ca\u524d\u540e\u666f\u5206\u5272\u3002\u4f5c\u8005\u5bf9anchor 3D box\u7528\u7684\u662fbin-based generation,\u4f7f\u5f97\u8fd9\u91cc\u7684\u6bcf\u4e00\u4e2a\u8f93\u51fa\u90fd\u4f1a\u662f\u5206\u7c7b\u8f93\u51fa\u3002","title":"PointRCNN: 3D Object Proposal Generation and Detection from Point Cloud"},{"location":"3dDetection/pvnet/","text":"PVNet: Pixel-wise Voting Network for 6DoF Pose Estimation \u8fd9\u7bc7\u6587\u7ae0\u89e3\u51b3\u7684\u4e5f\u662f\u5728\u57fa\u672c\u5df2\u77e5\u7269\u4f53scale\u751a\u81f3\u66f4\u591a\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b,\u4ece\u5355\u4e00RGB\u56fe\u7247\u4e2d\u8fd8\u539f\u7269\u4f53\u7684\u4f4d\u7f6e\u4e0e\u59ff\u6001\u7684\u4efb\u52a1\u3002 Motivation \u5982 \u524d\u6587 \u7684\u505a\u6cd5\uff0c\u5728\u505akeypoint\u56de\u5f52\u65f6\u4f5c\u8005\u53d1\u73b0\u4e86\u4e24\u4e2a\u95ee\u9898. \u7b2c\u4e00\uff0c\u906e\u6321\u751a\u81f3\u7269\u4f53\u8d85\u51fa\u56fe\u7247\u8303\u56f4\u7684\u60c5\u51b5\u3002\u76f4\u63a5\u56de\u5f52\u7cbe\u5ea6\u6709\u9650\u800c\u4f7f\u7528heatmap( \u6bd4\u5982PAF )\u4f1a\u9762\u4e34\u7684\u95ee\u9898\u5219\u662f\u5b8c\u5168\u65e0\u5904\u7406\u7269\u4f53\u8d85\u51fa\u56fe\u7247\u8303\u56f4\u7684\u60c5\u51b5\u3002 \u7b2c\u4e8c, \u76f4\u63a5\u4f7f\u7528PnP\u6c42\u89e3\u59ff\u6001\u7684\u95ee\u9898\u5728\u4e8e\u6ca1\u6709\u8003\u8651\u4e0d\u540ckeypoint\u6709\u4e0d\u540c\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5982\u4f55\u6c42\u5f97\u8fd9\u4e2a\u4e0d\u786e\u5b9a\u6027(\u672c\u6587\u63d0\u4f9b\u4e86\u66f4\u6709\u8da3\u7684\u601d\u8def)\u4e5f\u662f\u4e00\u5927\u95ee\u9898\u3002 \u7b2c\u4e09\uff0c \u524d\u6587 \u8bbe\u5b9a\u5176PnP\u95ee\u9898\u7684\u63a7\u5236\u70b9\u4e3a\u957f\u65b9\u4f53\u76848\u4e2a\u89d2\u70b9\u4ee5\u53ca\u4e2d\u5fc3\u70b9\uff0c\u7136\u800c\u4f5c\u8005\u53d1\u73b0\u7531\u4e8e\u89d2\u70b9\u5bb9\u6613\u504f\u79bb\u7269\u4f53\u8868\u9762\uff0c\u56e0\u800c\u7f51\u7edc\u6839\u636e\u56fe\u7247\u50cf\u7d20\u7279\u5f81\u8fdb\u884c\u7684\u56de\u5f52\u7ed3\u679c\u6548\u679c\u4e00\u822c\uff0c\u4f5c\u8005\u6307\u51fa\u5e94\u5f53\u5c3d\u53ef\u80fd\u8ba9\u63a7\u5236\u70b9\u5728\u7269\u4f53\u8868\u9762 \u7ed3\u6784 \u603b\u4f53pipeline \u5982\u56fe: \u8f93\u5165\u56fe\u7247\uff0c\u795e\u7ecf\u7f51\u7edc\u8f93\u51fa\u8bed\u4e49\u5206\u5272\u7ed3\u679c\uff0c\u4ee5\u53cakeypoints\u77e2\u91cf\u573a\u3002\u7136\u540e\u4f7f\u7528voting\u786e\u5b9a\u5404\u4e2a\u7269\u4f53\u7684keypoints\u7684\u671f\u671b\u503c\u4e0e\u65b9\u5dee\uff0c\u6700\u540e\u4f7f\u7528PnP\u6c42\u89e3\u7269\u4f53\u7684\u4f4d\u59ff Keypoints as Intersections \u7f51\u7edc\u5728keypoints\u77e2\u91cf\u573a\u8fd9\u91cc\u9700\u8981\u8f93\u51fa\u7684\u662f\u8fd9\u4e2a\u70b9\u5230\u5bf9\u5e94\u7269\u4f53\u5404\u4e2akeypoints\u7684\u5355\u4f4d\u65b9\u5411\u77e2\u91cf\u3002\u5219\u5bf9\u5e94keypoints\u7531\u672cobject\u6240\u6709\u50cf\u7d20\u7684\u65b9\u5411\u77e2\u91cf\u51b3\u5b9a\u3002\u672c\u6587\u4f7f\u7528\u57fa\u4e8eransac\u7684voting\u65b9\u5f0f\u51b3\u5b9akeypoints\u7684\u6700\u7ec8\u6982\u7387\u5206\u5e03\u3002 \u7b97\u6cd5\uff1a 1. \u4f7f\u7528\u8bed\u4e49\u5206\u5272label\u8fc7\u6ee4\u76ee\u6807\u50cf\u7d20 2. \u968f\u673a\u9009\u62e9\u4e24\u4e2a\u77e2\u91cf\uff0c\u6c42\u51fa\u5b83\u4eec\u7684\u89d2\u70b9\uff0c\u91cd\u590d N \u6b21\uff0c\u5f97\u5230 N \u4e2ahypothesis h_i 3. \u6240\u6709\u50cf\u7d20\u5bf9\u8fd9 N \u4e2aprior\u8fdb\u884c\u6295\u7968,\u6bcf\u4e2ahypothesis\u7684\u6743\u91cd\u4e3a w_{k, i}=\\sum_{\\mathbf{p} \\in O} \\mathbb{I}\\left(\\frac{\\left(\\mathbf{h}_{k, i}-\\mathbf{p}\\right)^{T}}{\\left\\|\\mathbf{h}_{k, i}-\\mathbf{p}\\right\\|_{2}} \\mathbf{v}_{k}(\\mathbf{p}) \\geq \\theta\\right) \u5176\u4e2d \\theta \u4e3a\u70b9\u4e58\u7ed3\u679c\u7684threshold\uff0c\u8fd9\u91cc\u8bbe\u5b9a\u4e3a0.99 4. \u6c42\u5f97\u8fd9\u4e2akeypoint\u7684\u5747\u503c\u4e0e\u65b9\u5dee \\begin{aligned} &\\boldsymbol{\\mu}_{k}=\\frac{\\sum_{i=1}^{N} w_{k, i} \\mathbf{h}_{k, i}}{\\sum_{i=1}^{N} w_{k, i}}\\\\ &\\boldsymbol{\\Sigma}_{k}=\\frac{\\sum_{i=1}^{N} w_{k, i}\\left(\\mathbf{h}_{k, i}-\\boldsymbol{\\mu}_{k}\\right)\\left(\\mathbf{h}_{k, i}-\\boldsymbol{\\mu}_{k}\\right)^{T}}{\\sum_{i=1}^{N} w_{k, i}} \\end{aligned} Groud Truth Keypoints Selection \u4f5c\u8005\u8fd9\u91cc\u9009\u62e9\u4e86\u66f4\u5168\u9762\u7684\u5efa\u6a21\u6548\u679c\uff0c\u662f\u4e00\u4e2a\u57fa\u4e8eFPS(farthest point sampling)\u7684\u7b97\u6cd5,\u4f7f\u5f97\u70b9\u5c3d\u53ef\u80fd\u5728\u7269\u4f53\u8868\u9762\uff0c\u540c\u65f6\u9694\u5f97\u8db3\u591f\u8fdc\uff0c\u4f46\u662f\u9700\u8981\u7269\u4f53\u7684\u5b8c\u6574\u5efa\u6a21\uff0c\u8fd9\u91cc\u6682\u4e14\u7565\u8fc7\u3002 Uncertainty-driven PnP \u7531\u4e8e\u524d\u9762\u5f97\u5230\u7684Keypoints\u5e26\u6709\u4e0d\u786e\u5b9a\u6027\u7684\u4f30\u8ba1\uff0c\u8fd9\u91cc\u5728\u8ba1\u7b97\u7684\u65f6\u5019\u5c31\u53ef\u4ee5\u5e26\u4e0a\u4e0d\u786e\u5b9a\u6027\u8fdb\u884cPnP\u4f18\u5316\uff0c\u4e5f\u5c31\u662f\u6c42\u89e3\u4e00\u4e2a\u975e\u7ebf\u6027\u4f18\u5316: \\begin{aligned} \\underset{R, \\mathbf{t}}{\\operatorname{minimize}} & \\sum_{k=1}^{K}\\left(\\tilde{\\mathbf{x}}_{k}-\\boldsymbol{\\mu}_{k}\\right)^{T} \\mathbf{\\Sigma}_{k}^{-1}\\left(\\tilde{\\mathbf{x}}_{k}-\\boldsymbol{\\mu}_{k}\\right) \\\\ & \\tilde{\\mathbf{x}}_{k}=\\pi\\left(R \\mathbf{X}_{k}+\\mathbf{t}\\right) \\end{aligned} \u5728\u672c\u6587\u7684\u5f00\u6e90\u5e93\u4e2d\uff0c\u4f5c\u8005\u4f7f\u7528CUDA\u52a0\u901f\u4e86 Ransac voting,\u7528opencv\u7684 solvePnP \u5f97\u5230\u4f4d\u59ff\u7684\u521d\u59cb\u89e3\uff0c\u7136\u540e\u4f7f\u7528Ceres\u5e93\u8003\u8651\u4e0d\u786e\u5b9a\u6027\u8fed\u4ee3\u5f97\u5230\u6700\u540e\u7ed3\u679c\u3002","title":"PVNet: Pixel-wise Voting Network for 6DoF Pose Estimation"},{"location":"3dDetection/pvnet/#pvnet-pixel-wise-voting-network-for-6dof-pose-estimation","text":"\u8fd9\u7bc7\u6587\u7ae0\u89e3\u51b3\u7684\u4e5f\u662f\u5728\u57fa\u672c\u5df2\u77e5\u7269\u4f53scale\u751a\u81f3\u66f4\u591a\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b,\u4ece\u5355\u4e00RGB\u56fe\u7247\u4e2d\u8fd8\u539f\u7269\u4f53\u7684\u4f4d\u7f6e\u4e0e\u59ff\u6001\u7684\u4efb\u52a1\u3002","title":"PVNet: Pixel-wise Voting Network for 6DoF Pose Estimation"},{"location":"3dDetection/pvnet/#motivation","text":"\u5982 \u524d\u6587 \u7684\u505a\u6cd5\uff0c\u5728\u505akeypoint\u56de\u5f52\u65f6\u4f5c\u8005\u53d1\u73b0\u4e86\u4e24\u4e2a\u95ee\u9898. \u7b2c\u4e00\uff0c\u906e\u6321\u751a\u81f3\u7269\u4f53\u8d85\u51fa\u56fe\u7247\u8303\u56f4\u7684\u60c5\u51b5\u3002\u76f4\u63a5\u56de\u5f52\u7cbe\u5ea6\u6709\u9650\u800c\u4f7f\u7528heatmap( \u6bd4\u5982PAF )\u4f1a\u9762\u4e34\u7684\u95ee\u9898\u5219\u662f\u5b8c\u5168\u65e0\u5904\u7406\u7269\u4f53\u8d85\u51fa\u56fe\u7247\u8303\u56f4\u7684\u60c5\u51b5\u3002 \u7b2c\u4e8c, \u76f4\u63a5\u4f7f\u7528PnP\u6c42\u89e3\u59ff\u6001\u7684\u95ee\u9898\u5728\u4e8e\u6ca1\u6709\u8003\u8651\u4e0d\u540ckeypoint\u6709\u4e0d\u540c\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5982\u4f55\u6c42\u5f97\u8fd9\u4e2a\u4e0d\u786e\u5b9a\u6027(\u672c\u6587\u63d0\u4f9b\u4e86\u66f4\u6709\u8da3\u7684\u601d\u8def)\u4e5f\u662f\u4e00\u5927\u95ee\u9898\u3002 \u7b2c\u4e09\uff0c \u524d\u6587 \u8bbe\u5b9a\u5176PnP\u95ee\u9898\u7684\u63a7\u5236\u70b9\u4e3a\u957f\u65b9\u4f53\u76848\u4e2a\u89d2\u70b9\u4ee5\u53ca\u4e2d\u5fc3\u70b9\uff0c\u7136\u800c\u4f5c\u8005\u53d1\u73b0\u7531\u4e8e\u89d2\u70b9\u5bb9\u6613\u504f\u79bb\u7269\u4f53\u8868\u9762\uff0c\u56e0\u800c\u7f51\u7edc\u6839\u636e\u56fe\u7247\u50cf\u7d20\u7279\u5f81\u8fdb\u884c\u7684\u56de\u5f52\u7ed3\u679c\u6548\u679c\u4e00\u822c\uff0c\u4f5c\u8005\u6307\u51fa\u5e94\u5f53\u5c3d\u53ef\u80fd\u8ba9\u63a7\u5236\u70b9\u5728\u7269\u4f53\u8868\u9762","title":"Motivation"},{"location":"3dDetection/pvnet/#_1","text":"\u603b\u4f53pipeline \u5982\u56fe: \u8f93\u5165\u56fe\u7247\uff0c\u795e\u7ecf\u7f51\u7edc\u8f93\u51fa\u8bed\u4e49\u5206\u5272\u7ed3\u679c\uff0c\u4ee5\u53cakeypoints\u77e2\u91cf\u573a\u3002\u7136\u540e\u4f7f\u7528voting\u786e\u5b9a\u5404\u4e2a\u7269\u4f53\u7684keypoints\u7684\u671f\u671b\u503c\u4e0e\u65b9\u5dee\uff0c\u6700\u540e\u4f7f\u7528PnP\u6c42\u89e3\u7269\u4f53\u7684\u4f4d\u59ff","title":"\u7ed3\u6784"},{"location":"3dDetection/pvnet/#keypoints-as-intersections","text":"\u7f51\u7edc\u5728keypoints\u77e2\u91cf\u573a\u8fd9\u91cc\u9700\u8981\u8f93\u51fa\u7684\u662f\u8fd9\u4e2a\u70b9\u5230\u5bf9\u5e94\u7269\u4f53\u5404\u4e2akeypoints\u7684\u5355\u4f4d\u65b9\u5411\u77e2\u91cf\u3002\u5219\u5bf9\u5e94keypoints\u7531\u672cobject\u6240\u6709\u50cf\u7d20\u7684\u65b9\u5411\u77e2\u91cf\u51b3\u5b9a\u3002\u672c\u6587\u4f7f\u7528\u57fa\u4e8eransac\u7684voting\u65b9\u5f0f\u51b3\u5b9akeypoints\u7684\u6700\u7ec8\u6982\u7387\u5206\u5e03\u3002 \u7b97\u6cd5\uff1a 1. \u4f7f\u7528\u8bed\u4e49\u5206\u5272label\u8fc7\u6ee4\u76ee\u6807\u50cf\u7d20 2. \u968f\u673a\u9009\u62e9\u4e24\u4e2a\u77e2\u91cf\uff0c\u6c42\u51fa\u5b83\u4eec\u7684\u89d2\u70b9\uff0c\u91cd\u590d N \u6b21\uff0c\u5f97\u5230 N \u4e2ahypothesis h_i 3. \u6240\u6709\u50cf\u7d20\u5bf9\u8fd9 N \u4e2aprior\u8fdb\u884c\u6295\u7968,\u6bcf\u4e2ahypothesis\u7684\u6743\u91cd\u4e3a w_{k, i}=\\sum_{\\mathbf{p} \\in O} \\mathbb{I}\\left(\\frac{\\left(\\mathbf{h}_{k, i}-\\mathbf{p}\\right)^{T}}{\\left\\|\\mathbf{h}_{k, i}-\\mathbf{p}\\right\\|_{2}} \\mathbf{v}_{k}(\\mathbf{p}) \\geq \\theta\\right) \u5176\u4e2d \\theta \u4e3a\u70b9\u4e58\u7ed3\u679c\u7684threshold\uff0c\u8fd9\u91cc\u8bbe\u5b9a\u4e3a0.99 4. \u6c42\u5f97\u8fd9\u4e2akeypoint\u7684\u5747\u503c\u4e0e\u65b9\u5dee \\begin{aligned} &\\boldsymbol{\\mu}_{k}=\\frac{\\sum_{i=1}^{N} w_{k, i} \\mathbf{h}_{k, i}}{\\sum_{i=1}^{N} w_{k, i}}\\\\ &\\boldsymbol{\\Sigma}_{k}=\\frac{\\sum_{i=1}^{N} w_{k, i}\\left(\\mathbf{h}_{k, i}-\\boldsymbol{\\mu}_{k}\\right)\\left(\\mathbf{h}_{k, i}-\\boldsymbol{\\mu}_{k}\\right)^{T}}{\\sum_{i=1}^{N} w_{k, i}} \\end{aligned}","title":"Keypoints as Intersections"},{"location":"3dDetection/pvnet/#groud-truth-keypoints-selection","text":"\u4f5c\u8005\u8fd9\u91cc\u9009\u62e9\u4e86\u66f4\u5168\u9762\u7684\u5efa\u6a21\u6548\u679c\uff0c\u662f\u4e00\u4e2a\u57fa\u4e8eFPS(farthest point sampling)\u7684\u7b97\u6cd5,\u4f7f\u5f97\u70b9\u5c3d\u53ef\u80fd\u5728\u7269\u4f53\u8868\u9762\uff0c\u540c\u65f6\u9694\u5f97\u8db3\u591f\u8fdc\uff0c\u4f46\u662f\u9700\u8981\u7269\u4f53\u7684\u5b8c\u6574\u5efa\u6a21\uff0c\u8fd9\u91cc\u6682\u4e14\u7565\u8fc7\u3002","title":"Groud Truth Keypoints Selection"},{"location":"3dDetection/pvnet/#uncertainty-driven-pnp","text":"\u7531\u4e8e\u524d\u9762\u5f97\u5230\u7684Keypoints\u5e26\u6709\u4e0d\u786e\u5b9a\u6027\u7684\u4f30\u8ba1\uff0c\u8fd9\u91cc\u5728\u8ba1\u7b97\u7684\u65f6\u5019\u5c31\u53ef\u4ee5\u5e26\u4e0a\u4e0d\u786e\u5b9a\u6027\u8fdb\u884cPnP\u4f18\u5316\uff0c\u4e5f\u5c31\u662f\u6c42\u89e3\u4e00\u4e2a\u975e\u7ebf\u6027\u4f18\u5316: \\begin{aligned} \\underset{R, \\mathbf{t}}{\\operatorname{minimize}} & \\sum_{k=1}^{K}\\left(\\tilde{\\mathbf{x}}_{k}-\\boldsymbol{\\mu}_{k}\\right)^{T} \\mathbf{\\Sigma}_{k}^{-1}\\left(\\tilde{\\mathbf{x}}_{k}-\\boldsymbol{\\mu}_{k}\\right) \\\\ & \\tilde{\\mathbf{x}}_{k}=\\pi\\left(R \\mathbf{X}_{k}+\\mathbf{t}\\right) \\end{aligned} \u5728\u672c\u6587\u7684\u5f00\u6e90\u5e93\u4e2d\uff0c\u4f5c\u8005\u4f7f\u7528CUDA\u52a0\u901f\u4e86 Ransac voting,\u7528opencv\u7684 solvePnP \u5f97\u5230\u4f4d\u59ff\u7684\u521d\u59cb\u89e3\uff0c\u7136\u540e\u4f7f\u7528Ceres\u5e93\u8003\u8651\u4e0d\u786e\u5b9a\u6027\u8fed\u4ee3\u5f97\u5230\u6700\u540e\u7ed3\u679c\u3002","title":"Uncertainty-driven PnP"},{"location":"3dDetection/pyramid-rcnn/","text":"Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection \u8fd9\u7bc7\u6587\u7ae0\u4e3b\u8981\u5904\u7406\u7684\u662ftwo-stage\u7684 \u57fa\u4e8e\u70b9\u4e91\u7684\u4e09\u7ef4\u68c0\u6d4b\u3002 \u5176\u6838\u5fc3\u601d\u8def\u5982\u4e0b: \u6539\u5584\u4e86two-stage detection\u7684\u7279\u5f81\u63d0\u53d6\u6a21\u5757\u3002 \u89c2\u5bdf\u5230\u6570\u636e\u96c6\u4e2d\u70b9\u7684\u7a00\u758f\u6027\uff0c\u67097%\u7684\u7269\u4f53\u4e2d\u53ea\u6709\u4e0d\u8d85\u8fc710\u4e2a\u70b9\u3002\u56e0\u800c\u8003\u8651\u9700\u8981initial prediction\u5916\u9762\u7684\u6d4b\u91cf\u70b9\u7684\u7279\u5f81\u6765\u8f85\u52a9\u5224\u65ad\u7269\u4f53\u7684\u5b9e\u9645\u4f4d\u7f6e\u548c\u5f62\u6001\u3002 \u505a\u6cd5\u4e0a\uff0c\u9996\u5148\u662f\u63d0\u51fa\u4e86RoI-grid Pyramid, \u5c42\u7ea7\u5f0f\u5730\u589e\u52a0\u53ef\u4ee5\u88ab\"RoI\"\u5305\u542b\u7684\u70b9\u7684\u8303\u56f4\uff0c\u4ece\u800c\u8fdb\u4e00\u6b65\u8003\u8651\u4e86\u9644\u8fd1\u7684\u70b9\u3002 \u63d0\u51fa\u4e86\u9ad8\u6cdb\u7528\u5ea6\u7684\u81ea\u9002\u5e94\u7684\u7279\u5f81\u63d0\u53d6\u65b9\u6cd5 RoI-grid Attention\u3002 \u53ef\u5b66\u4e60\u7684\uff0c\u81ea\u9002\u5e94\u5730\u5bf9\u4e34\u8fd1\u70b9\u641c\u7d22\u7684\u534a\u5f84\u8fdb\u884c\u9884\u6d4b\u3002 \u6709\u4e00\u5927\u4f18\u70b9\uff0c\u53ef\u4ee5\u5ac1\u63a5\u4e8e\u591a\u79cdone-stage\u7f51\u7edc(backbone\u7f51\u7edc)\uff0c\u9002\u7528\u591a\u79cd\u5173\u952e\u70b9\u8868\u8fbe\u3002 RoI-grid Pyramid \u672c\u6587\u9996\u5148\u89e3\u91ca\u4e86\u4e3a\u4ec0\u4e48\u70b9\u4e91\u9700\u8981\u8003\u8651proposal\u4e4b\u5916\u7684\u70b9\uff0c\u800c\u56fe\u7247\u4e0d\u9700\u8981\u3002 - \u70b9\u4e91\u8fc7\u4e8e\u7a00\u758f - \u56fe\u7247Dense, \u4e14\u6709\u660e\u786e\u7684\u66f4\u5927\u7684\u611f\u53d7\u91ce\u3002 p_{\\text {grid }}^{i j k}=\\left(\\frac{W}{N_{w}}, \\frac{L}{N_{l}}, \\frac{H}{N_{h}}\\right) \\cdot(0.5+(i, j, k))+\\left(x_{c}, y_{c}, z_{c}\\right) RoI-grid Attention \u5df2\u7ecf\u6709\u4e86\u4e00\u4e9bkeypoint\u4ee5\u53caRoI, \u4ece\u4e0a\u6587\u7684\u516c\u5f0f\u4f1a\u5f97\u5230\u4e00\u7cfb\u5217\u9700\u8981\u91c7\u6837\u7684grid\u70b9\u7684\u4f4d\u7f6e, \u5bf9\u6bcf\u4e00\u4e2a\u70b9\u7684\u9644\u8fd1 r \u8ddd\u79bb\u5185\u7684\u6240\u6709\u70b9\u7684\u7279\u5f81\u8fdb\u884c\u878d\u5408\uff0c\u8fd9\u4e2a r \u7684\u8ba1\u7b97\u5728\u540e\u9762\u8868\u8ff0\uff0c\u878d\u5408\u7684\u65f6\u5019\u6709\u4e00\u7cfb\u5217\u7684\u53ef\u80fd\u7684\u65b9\u6cd5\u3002 Pooling-based Operator: f_{grid}^{pool} = \\underset{i\\in\\Omega(r)}{\\text{maxpool}} (\\text{MLP}(\\text{cat}([f_i, p_i - p_{grid}]))) Graph-based Operator: f_{\\text {grid }}^{\\text {graph }}=\\sum_{i \\in \\Omega(r)} W\\left(Q_{\\text {pos }}^{i}\\right) \\odot V^{i} = \\sum_{i \\in \\Omega(r)} W\\left(Q_{\\text {pos }}^{i}\\right) \\odot \\text{MLP}(f_i) = \\sum_{i \\in \\Omega(r)} W\\left(\\text{Linear}(p_i - p_grid)\\right) \\odot \\text{MLP}(f_i) Attention-based Operator: f_{\\text {grid }}^{\\text {atten }}=\\sum_{i \\in \\Omega(r)} W\\left(Q_{\\text {pos }}^{i} K^{i}\\right) \\odot V^{i} RoI-grid Attention: f_{\\text {grid }}=\\sum_{i \\in \\Omega(r)} W\\left(\\sigma_{k} K^{i}+\\sigma_{q} Q_{p o s}^{i}+\\sigma_{q k} Q_{\\text {pos }}^{i} K^{i}\\right) \\odot\\left(V^{i}+\\sigma_{v} Q_{\\text {pos }}^{i}\\right) \u901a\u8fc7 \\sigma \u7684\u4e0d\u540c\u53d6\u503c\uff0c\u53ef\u4ee5\u628a\u672c\u6587\u63d0\u51fa\u7684\u878d\u5408\u65b9\u6cd5\u53d8\u6210\u4e0a\u9762\u51e0\u4e2a\u65b9\u6cd5\u3002\u8fd9\u4e9b\u53c2\u6570\u662f\u7528\u5168\u8fde\u63a5\u5c42\u8f93\u51fa\u7684sigmoid\u51fd\u6570\u6765\u81ea\u9002\u5e94\u5b66\u4e60\u7684\u3002","title":"Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection"},{"location":"3dDetection/pyramid-rcnn/#pyramid-r-cnn-towards-better-performance-and-adaptability-for-3d-object-detection","text":"\u8fd9\u7bc7\u6587\u7ae0\u4e3b\u8981\u5904\u7406\u7684\u662ftwo-stage\u7684 \u57fa\u4e8e\u70b9\u4e91\u7684\u4e09\u7ef4\u68c0\u6d4b\u3002 \u5176\u6838\u5fc3\u601d\u8def\u5982\u4e0b: \u6539\u5584\u4e86two-stage detection\u7684\u7279\u5f81\u63d0\u53d6\u6a21\u5757\u3002 \u89c2\u5bdf\u5230\u6570\u636e\u96c6\u4e2d\u70b9\u7684\u7a00\u758f\u6027\uff0c\u67097%\u7684\u7269\u4f53\u4e2d\u53ea\u6709\u4e0d\u8d85\u8fc710\u4e2a\u70b9\u3002\u56e0\u800c\u8003\u8651\u9700\u8981initial prediction\u5916\u9762\u7684\u6d4b\u91cf\u70b9\u7684\u7279\u5f81\u6765\u8f85\u52a9\u5224\u65ad\u7269\u4f53\u7684\u5b9e\u9645\u4f4d\u7f6e\u548c\u5f62\u6001\u3002 \u505a\u6cd5\u4e0a\uff0c\u9996\u5148\u662f\u63d0\u51fa\u4e86RoI-grid Pyramid, \u5c42\u7ea7\u5f0f\u5730\u589e\u52a0\u53ef\u4ee5\u88ab\"RoI\"\u5305\u542b\u7684\u70b9\u7684\u8303\u56f4\uff0c\u4ece\u800c\u8fdb\u4e00\u6b65\u8003\u8651\u4e86\u9644\u8fd1\u7684\u70b9\u3002 \u63d0\u51fa\u4e86\u9ad8\u6cdb\u7528\u5ea6\u7684\u81ea\u9002\u5e94\u7684\u7279\u5f81\u63d0\u53d6\u65b9\u6cd5 RoI-grid Attention\u3002 \u53ef\u5b66\u4e60\u7684\uff0c\u81ea\u9002\u5e94\u5730\u5bf9\u4e34\u8fd1\u70b9\u641c\u7d22\u7684\u534a\u5f84\u8fdb\u884c\u9884\u6d4b\u3002 \u6709\u4e00\u5927\u4f18\u70b9\uff0c\u53ef\u4ee5\u5ac1\u63a5\u4e8e\u591a\u79cdone-stage\u7f51\u7edc(backbone\u7f51\u7edc)\uff0c\u9002\u7528\u591a\u79cd\u5173\u952e\u70b9\u8868\u8fbe\u3002","title":"Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection"},{"location":"3dDetection/pyramid-rcnn/#roi-grid-pyramid","text":"\u672c\u6587\u9996\u5148\u89e3\u91ca\u4e86\u4e3a\u4ec0\u4e48\u70b9\u4e91\u9700\u8981\u8003\u8651proposal\u4e4b\u5916\u7684\u70b9\uff0c\u800c\u56fe\u7247\u4e0d\u9700\u8981\u3002 - \u70b9\u4e91\u8fc7\u4e8e\u7a00\u758f - \u56fe\u7247Dense, \u4e14\u6709\u660e\u786e\u7684\u66f4\u5927\u7684\u611f\u53d7\u91ce\u3002 p_{\\text {grid }}^{i j k}=\\left(\\frac{W}{N_{w}}, \\frac{L}{N_{l}}, \\frac{H}{N_{h}}\\right) \\cdot(0.5+(i, j, k))+\\left(x_{c}, y_{c}, z_{c}\\right)","title":"RoI-grid Pyramid"},{"location":"3dDetection/pyramid-rcnn/#roi-grid-attention","text":"\u5df2\u7ecf\u6709\u4e86\u4e00\u4e9bkeypoint\u4ee5\u53caRoI, \u4ece\u4e0a\u6587\u7684\u516c\u5f0f\u4f1a\u5f97\u5230\u4e00\u7cfb\u5217\u9700\u8981\u91c7\u6837\u7684grid\u70b9\u7684\u4f4d\u7f6e, \u5bf9\u6bcf\u4e00\u4e2a\u70b9\u7684\u9644\u8fd1 r \u8ddd\u79bb\u5185\u7684\u6240\u6709\u70b9\u7684\u7279\u5f81\u8fdb\u884c\u878d\u5408\uff0c\u8fd9\u4e2a r \u7684\u8ba1\u7b97\u5728\u540e\u9762\u8868\u8ff0\uff0c\u878d\u5408\u7684\u65f6\u5019\u6709\u4e00\u7cfb\u5217\u7684\u53ef\u80fd\u7684\u65b9\u6cd5\u3002 Pooling-based Operator: f_{grid}^{pool} = \\underset{i\\in\\Omega(r)}{\\text{maxpool}} (\\text{MLP}(\\text{cat}([f_i, p_i - p_{grid}]))) Graph-based Operator: f_{\\text {grid }}^{\\text {graph }}=\\sum_{i \\in \\Omega(r)} W\\left(Q_{\\text {pos }}^{i}\\right) \\odot V^{i} = \\sum_{i \\in \\Omega(r)} W\\left(Q_{\\text {pos }}^{i}\\right) \\odot \\text{MLP}(f_i) = \\sum_{i \\in \\Omega(r)} W\\left(\\text{Linear}(p_i - p_grid)\\right) \\odot \\text{MLP}(f_i) Attention-based Operator: f_{\\text {grid }}^{\\text {atten }}=\\sum_{i \\in \\Omega(r)} W\\left(Q_{\\text {pos }}^{i} K^{i}\\right) \\odot V^{i} RoI-grid Attention: f_{\\text {grid }}=\\sum_{i \\in \\Omega(r)} W\\left(\\sigma_{k} K^{i}+\\sigma_{q} Q_{p o s}^{i}+\\sigma_{q k} Q_{\\text {pos }}^{i} K^{i}\\right) \\odot\\left(V^{i}+\\sigma_{v} Q_{\\text {pos }}^{i}\\right) \u901a\u8fc7 \\sigma \u7684\u4e0d\u540c\u53d6\u503c\uff0c\u53ef\u4ee5\u628a\u672c\u6587\u63d0\u51fa\u7684\u878d\u5408\u65b9\u6cd5\u53d8\u6210\u4e0a\u9762\u51e0\u4e2a\u65b9\u6cd5\u3002\u8fd9\u4e9b\u53c2\u6570\u662f\u7528\u5168\u8fde\u63a5\u5c42\u8f93\u51fa\u7684sigmoid\u51fd\u6570\u6765\u81ea\u9002\u5e94\u5b66\u4e60\u7684\u3002","title":"RoI-grid Attention"},{"location":"3dDetection/renderocc/","text":"RenderOcc: Vision-Centric 3D Occupancy Prediction with 2D Rendering Supervision \u8fd9\u7bc7paper\u5448\u73b0\u4e86\u57fa\u672c\u5b8c\u6574\u7684\u7531NeRF\u4f5c\u4e3a\u8bad\u7ec3\u65b9\u6cd5\u7684\u7eaf\u76f8\u673a\u89c6\u89c9\u4e09\u7ef4Occupancy\u7684\u65b9\u6848\u3002 \u6b64\u524d\u4e3b\u8981\u76843D Occupancy\u8bad\u7ec3\u90fd\u9700\u8981\u4f7f\u75283D Label\u8fdb\u884c\u76d1\u7763\uff0c\u800c3D Label\u7684\u6210\u672c\u4ee5\u53ca\u7eaf\u89c6\u89c9\u8f66\u8f86\u4e0a\u76d1\u7763\u6570\u636e\u7684\u83b7\u53d6\u96be\u5ea6\u4e00\u76f4\u662f\u8f66\u8f86\u90e8\u7f72\u7684\u75db\u70b9\u3002 NeRF\u7684\u8bde\u751f\u4f7f\u5f97\u53ef\u5fae\u5206\u6e32\u67d3\u8d70\u8fdb\u4e86\u6df1\u5ea6\u5b66\u4e60CV\u793e\u533a\uff0c\u80fd\u5426\u901a\u8fc7\u53ef\u5fae\u5206\u6e32\u67d3\u3001\u5c31\u50cf\u91cd\u6295\u5f71\u8bad\u7ec3\u5355\u76ee\u6df1\u5ea6\u4e00\u6837\u3001\u51e0\u4e4e\u81ea\u76d1\u7763\u5730\u8bad\u7ec3\u4e09\u7ef4\u5360\u636e\u7f51\u683c?\u8fd9\u4e2a\u95ee\u9898\u5bf9\u4e8e\u7eaf\u89c6\u89c9\u7684\u611f\u77e5\u843d\u5730\u65b9\u6848\u6709\u76f8\u5f53\u91cd\u8981\u7684\u4ef7\u503c\u3002\u76f8\u6bd4\u4e8e\u5355\u76ee\u6df1\u5ea6+\u8bed\u4e49\u5206\u5272+\u6295\u5f71\u7684\u5206\u5e03\u65b9\u6848\uff0c\u4e09\u7ef4\u5360\u636e\u7f51\u683c\u65b9\u6848\u7684\u4f18\u52bf\u4e5f\u662f\u5f88\u591a\u7684: \u4ece\u8f93\u51fa\u7684\u6570\u636e\u6392\u5217\u6765\u8bf4\uff0c\u5728\u7a7a\u95f4\u4e2d\u5747\u5300\u5206\u5e03\u7684\u6805\u683c\u6bd4\u5728\u56fe\u7247\u57df\u4e2d\u5747\u5300\u5206\u5e03\u7684\u50cf\u7d20\u66f4\u5b9e\u7528\uff0c\u6295\u5f71\u8fc7\u7a0b\u4e2d\u6709\u5f88\u591a\u7b97\u529b\u662f\u6d6a\u8d39\u5728\u5bc6\u96c6\u5730\u8fd1\u7aef\u573a\u666f\u3002 \u4ece\u8f93\u51fa\u7684\u7ed3\u6784\u6765\u8bf4\uff0c\u7531\u4e8eMonoDepth\u9650\u5b9a\u4e86\u88ab\u906e\u6321\u7684\u7269\u4f53\u3001\u7269\u4f53\u88ab\u906e\u6321\u7684\u90e8\u5206\u5b8c\u5168\u6ca1\u6709\u4efb\u4f55\u8f93\u51fa\uff0c\u800c\u6805\u683c\u8f93\u51fa\u4f7f\u5f97\u7f51\u7edc\u53ef\u4ee5\u901a\u8fc7\u76f8\u90bb\u5e27\u7684\u76d1\u7763\u3001\u5e2e\u52a9\u672c\u5e27\u9884\u6d4b\u573a\u666f\u4e2d\u88ab\u906e\u6321\u7684\u90e8\u5206\uff1b\u731c\u6d4b\u88ab\u906e\u6321\u7684\u90e8\u5206\uff0c\u751a\u81f3\u8f93\u51fa\u88ab\u906e\u6321\u90e8\u5206\u7684\u4e0d\u786e\u5b9a\u6027\u3002\u800c\u4e0d\u50cfMonoDepth\u4e00\u6837\u76f4\u63a5\u65e0\u6cd5\u8f93\u51fa\u3002 \u4e09\u7ef4\u5360\u636e\u7f51\u683c\u65b9\u6848\u7406\u8bba\u4e0a\u53ef\u4ee5\u4e0e\u8f66\u8f86planning / prediction\u7b49\u4e00\u540c\u8bad\u7ec3\uff0c\u6709\u66f4\u5e7f\u7684\u6269\u5c55\u7a7a\u95f4\u3002 \u4e0e\u6b64\u540c\u65f6NeRF\u91cd\u6295\u5f71\u6e32\u67d3\u4e0e\u5355\u76ee\u6df1\u5ea6\u8bad\u7ec3\u6709\u4e00\u5b9a\u4e0d\u540c\uff0c\u4e5f\u5e26\u6765\u4e86\u76f8\u5e94\u7684\u6311\u6218: \u5bf9\u4e8e\u52a8\u6001\u7269\u4f53\uff0c\u5355\u76ee\u6df1\u5ea6\u53ef\u4ee5\u5229\u7528\u540c\u5411\u540c\u901f\u7269\u4f53\u5728\u76f8\u673a\u4e0a\u76f8\u5bf9\u4f4d\u7f6e\u76f8\u5bf9\u7a33\u5b9a\u3001\u4e0e\u8fdc\u5904\u80cc\u666f\u7269\u4f53\u76f8\u4f3c\u8fd9\u4e2a\u6781\u5f3a\u7684\u5148\u9a8c\uff0c\u540c\u65f6\u6ee4\u6389\u5927\u90e8\u5206\u7684\u52a8\u6001\u7269\u4f53\u4ee5\u53ca\u5929\u7a7a\u7b49\u52a8\u6001\u7269\u4f53\u3002\u4f46\u662f\u5bf9\u4e8e3D Occupancy\u7684\u8bad\u7ec3\u6765\u8bf4\u65e0\u6cd5\u4f7f\u7528\u8fd9\u4e2aTrick\u3002\u53bb\u9664\u80cc\u666f\u3001\u5904\u7406\u52a8\u6001\u7269\u4f53\u9700\u8981\u65b0\u7684\u65b9\u6848\u3002 \u5355\u76ee\u6df1\u5ea6\u7f51\u7edc\u57fa\u672c\u7531\u7eaf\u5377\u79ef\u7f51\u7edc\u7ec4\u6210\uff0c\u7f51\u7edc\u7ec4\u6210\u7b80\u5355\u3001\u901f\u5ea6\u5feb\u3001\u5360\u7528\u663e\u5b58\u5c11\u3002\u800c2D-3D\u7684\u7f51\u7edc\uff0c\u5c3d\u7ba1\u6709 DeformAttention \u5e26\u6765\u6027\u80fd\u7684\u5927\u5e45\u63d0\u5347\uff0c\u4f46\u662f\u6574\u4f53\u6a21\u578b\u5927\u5c0f\u4ecd\u7136\u8f83\u5927\u3002\u5728\u8fdb\u884c\u91cd\u6295\u5f71\uff0c\u5c24\u5176\u662f\u57fa\u4e8eNeRF\u7684\u53ef\u5fae\u5206\u6e32\u67d3\u65f6\uff0c\u82b1\u8d39\u7684\u663e\u5b58\u8d44\u6e90\u8f83\u5927\u3002 \u5728\u4e09\u7ef4\u6570\u636e\u4e2d\uff0c\u52a0\u5267\u4e86\u6570\u636elabel\u4e0d\u5e73\u8861\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u7531\u4e8e\u6a21\u578b\u89c4\u6a21\u3001NeRF\u3001\u591a\u5e27\u6e32\u67d3\u7684\u663e\u5b58\u8017\u8d39\uff0c\u6700\u7ec8\u53ef\u5fae\u5206\u6e32\u67d3\u5f80\u5f80\u88ab\u8feb\u53ea\u80fd\u968f\u673a\u91c7\u6837\u4e00\u5b9a\u6570\u91cf\u7684\u5c04\u7ebf\uff0c\u5c11\u6837\u672c\u6570\u636e\u7684\u4fdd\u5b58\u4e5f\u6210\u4e3a\u95ee\u9898\u3002 \u8fd9\u7bc7paper\u540c\u6837\u9762\u5bf9\u524d\u6587\u63d0\u5230\u7684\u8fd9\u51e0\u4e2a\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u76f8\u5e94\u7684\u4fee\u6b63Trick\u3002 Network Structure \u7406\u8bba\u4e0a\u6765\u8bf4\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u662f\u5bf92D-3D\u7f51\u7edc\u662f\u4e0d\u654f\u611f\u7684\uff0c\u5b9e\u9a8c\u4e2d\u91c7\u7528\u7684\u7f51\u7edc\u662fBEVStereo. \u800c\u5728\u6700\u7ec8\u8f93\u51fa\u5bc6\u5ea6density\u4e0esemantic\u7684\u65f6\u5019,density\u7684activation function\u4e3a softplus \\text{Softplus}(x) = \\frac{1}{\\beta} * \\text{log}(1+\\text{exp}(\\beta * x)) \u4fdd\u8bc1\u4e86: \u6fc0\u6d3b\u51fd\u6570\u975e\u8d1f\uff0c\u4e14\u53ef\u4ee5\u975e\u5e38\u9ad8\uff0c\u5168\u90e8\u5b9a\u4e49\u57df\u53ef\u4ee5\u8bad\u7ec3\u3002 \u5728\u521d\u59cb\u5316\u7684\u65f6\u5019\uff0c\u5bc6\u5ea6\u7ea6\u4e3a0.7, \u6e32\u67d3\u91c7\u6837\u4f1a\u504f\u5411\u4e8e\u8fd1\u5904\u7684\u7f51\u683c\u3002 Rendering Supervision \u8fd9\u7bc7paper\u91c7\u7528\u7684\u662f\u6df1\u5ea6\u4e0e\u8bed\u4e49\u5206\u5272\u76d1\u7763\u3002\u6df1\u5ea6\u9ed8\u8ba4\u6765\u81ea\u4e8e\u5f53\u5e27\u70b9\u4e91\uff0c\u5b9e\u9a8c\u4e2d\u6765\u81ea\u4e8eSFM\u7684\u6df1\u5ea6\u6295\u5f71\u540c\u6837\u80fd\u5b8c\u6210\u8bad\u7ec3\uff0c\u53ea\u662f\u51c6\u786e\u5ea6\u6709\u4e00\u5b9a\u7684\u533a\u522b\u3002\u800c\u8bed\u4e49\u5206\u5272\u6765\u81ea\u4e8e\u8bed\u4e49\u70b9\u4e91\u5bf9\u56fe\u7247\u7684\u6295\u5f71\u3002 \u6295\u5f71\u65b9\u6cd5\u4e0e\u8bad\u7ec3\u65b9\u5f0f\u4e0eNeRF\u4e00\u81f4\u3002 \\begin{aligned} & \\alpha\\left(z_k\\right)=1-\\exp \\left(-\\sigma\\left(z_k\\right) \\beta_k\\right) \\\\ & T\\left(z_k\\right)=\\exp \\left(-\\sum_{t=1}^{k-1} \\sigma\\left(z_t\\right) \\beta_t\\right) \\end{aligned} \\begin{aligned} & S^{\\text {pix }}(r)=\\sum_{k=1}^N T\\left(z_k\\right) \\alpha\\left(z_k\\right) S\\left(z_k\\right), \\\\ & D^{\\text {pix }}(r)=\\sum_{k=1}^N T\\left(z_k\\right) \\alpha\\left(z_k\\right) z_k, \\end{aligned} L=L_{\\text {seg }}\\left(S^{p i x}, \\hat{S}^{p i x}\\right)+L_{\\text {depth }}\\left(D^{p i x}, \\hat{D}^{p i x}\\right)+L_{r e g}(\\sigma) \u5176\u4e2d\u8bed\u4e49\u5206\u5272\u7684\u635f\u5931\u4e3a cross-entropy, \u6df1\u5ea6\u635f\u5931\u4e3a SILog, regularization Loss\u91c7\u7528\u7684\u662f distortion loss\u548cTV loss. \u800c\u6e32\u67d3\u70b9\u7684\u91c7\u6837\u65b9\u6848\uff0c\u91c7\u7528\u7684\u662f mip360-sampling Auxiliary Rays: \u5728nuscenes\u8fd9\u79cd\u76f8\u90bb\u76f8\u673a\u51e0\u4e4e\u6ca1\u6709\u91cd\u53e0\u533a\u57df\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u91c7\u6837\uff0c\u4e0d\u53ef\u907f\u514d\u9700\u8981\u5904\u7406\u591a\u5e27\u6570\u636e\u3002\u8fd9\u7bc7paper\u5728nuscenes\u91c7\u7528\u524d\u540e\u5171\u4e03\u5e27\u7684\u56fe\u50cf\u8fdb\u884c\u91cd\u6295\u5f71\u3002\u968f\u673a\u91c7\u6837\u7684\u65f6\u5019\u662f\u6709\u4fa7\u91cd\u7684\uff0c \u7c7b\u522b\u5e73\u8861\uff0c\u6839\u636e\u50cf\u7d20\u70b9\u51fa\u73b0\u7684\u9891\u7387\u8ba1\u7b97\u6743\u91cd W_b W_b(r)=\\exp \\left(\\lambda_s *\\left(\\frac{\\max (M)}{N(\\mathbf{C}(r))}-1\\right)\\right) \u52a8\u6001\u6e05\u9664\u3002\u964d\u4f4e\u76f8\u90bb\u5e27\u7684\u6743\u91cd\uff0c\u5e76\u4e14\u76f8\u90bb\u5e27\u52a8\u6001\u7c7b\u522b\u7684\u6743\u91cd\u57fa\u672c\u8bbe\u7f6e\u4e3a0","title":"RenderOcc: Vision-Centric 3D Occupancy Prediction with 2D Rendering Supervision"},{"location":"3dDetection/renderocc/#renderocc-vision-centric-3d-occupancy-prediction-with-2d-rendering-supervision","text":"\u8fd9\u7bc7paper\u5448\u73b0\u4e86\u57fa\u672c\u5b8c\u6574\u7684\u7531NeRF\u4f5c\u4e3a\u8bad\u7ec3\u65b9\u6cd5\u7684\u7eaf\u76f8\u673a\u89c6\u89c9\u4e09\u7ef4Occupancy\u7684\u65b9\u6848\u3002 \u6b64\u524d\u4e3b\u8981\u76843D Occupancy\u8bad\u7ec3\u90fd\u9700\u8981\u4f7f\u75283D Label\u8fdb\u884c\u76d1\u7763\uff0c\u800c3D Label\u7684\u6210\u672c\u4ee5\u53ca\u7eaf\u89c6\u89c9\u8f66\u8f86\u4e0a\u76d1\u7763\u6570\u636e\u7684\u83b7\u53d6\u96be\u5ea6\u4e00\u76f4\u662f\u8f66\u8f86\u90e8\u7f72\u7684\u75db\u70b9\u3002 NeRF\u7684\u8bde\u751f\u4f7f\u5f97\u53ef\u5fae\u5206\u6e32\u67d3\u8d70\u8fdb\u4e86\u6df1\u5ea6\u5b66\u4e60CV\u793e\u533a\uff0c\u80fd\u5426\u901a\u8fc7\u53ef\u5fae\u5206\u6e32\u67d3\u3001\u5c31\u50cf\u91cd\u6295\u5f71\u8bad\u7ec3\u5355\u76ee\u6df1\u5ea6\u4e00\u6837\u3001\u51e0\u4e4e\u81ea\u76d1\u7763\u5730\u8bad\u7ec3\u4e09\u7ef4\u5360\u636e\u7f51\u683c?\u8fd9\u4e2a\u95ee\u9898\u5bf9\u4e8e\u7eaf\u89c6\u89c9\u7684\u611f\u77e5\u843d\u5730\u65b9\u6848\u6709\u76f8\u5f53\u91cd\u8981\u7684\u4ef7\u503c\u3002\u76f8\u6bd4\u4e8e\u5355\u76ee\u6df1\u5ea6+\u8bed\u4e49\u5206\u5272+\u6295\u5f71\u7684\u5206\u5e03\u65b9\u6848\uff0c\u4e09\u7ef4\u5360\u636e\u7f51\u683c\u65b9\u6848\u7684\u4f18\u52bf\u4e5f\u662f\u5f88\u591a\u7684: \u4ece\u8f93\u51fa\u7684\u6570\u636e\u6392\u5217\u6765\u8bf4\uff0c\u5728\u7a7a\u95f4\u4e2d\u5747\u5300\u5206\u5e03\u7684\u6805\u683c\u6bd4\u5728\u56fe\u7247\u57df\u4e2d\u5747\u5300\u5206\u5e03\u7684\u50cf\u7d20\u66f4\u5b9e\u7528\uff0c\u6295\u5f71\u8fc7\u7a0b\u4e2d\u6709\u5f88\u591a\u7b97\u529b\u662f\u6d6a\u8d39\u5728\u5bc6\u96c6\u5730\u8fd1\u7aef\u573a\u666f\u3002 \u4ece\u8f93\u51fa\u7684\u7ed3\u6784\u6765\u8bf4\uff0c\u7531\u4e8eMonoDepth\u9650\u5b9a\u4e86\u88ab\u906e\u6321\u7684\u7269\u4f53\u3001\u7269\u4f53\u88ab\u906e\u6321\u7684\u90e8\u5206\u5b8c\u5168\u6ca1\u6709\u4efb\u4f55\u8f93\u51fa\uff0c\u800c\u6805\u683c\u8f93\u51fa\u4f7f\u5f97\u7f51\u7edc\u53ef\u4ee5\u901a\u8fc7\u76f8\u90bb\u5e27\u7684\u76d1\u7763\u3001\u5e2e\u52a9\u672c\u5e27\u9884\u6d4b\u573a\u666f\u4e2d\u88ab\u906e\u6321\u7684\u90e8\u5206\uff1b\u731c\u6d4b\u88ab\u906e\u6321\u7684\u90e8\u5206\uff0c\u751a\u81f3\u8f93\u51fa\u88ab\u906e\u6321\u90e8\u5206\u7684\u4e0d\u786e\u5b9a\u6027\u3002\u800c\u4e0d\u50cfMonoDepth\u4e00\u6837\u76f4\u63a5\u65e0\u6cd5\u8f93\u51fa\u3002 \u4e09\u7ef4\u5360\u636e\u7f51\u683c\u65b9\u6848\u7406\u8bba\u4e0a\u53ef\u4ee5\u4e0e\u8f66\u8f86planning / prediction\u7b49\u4e00\u540c\u8bad\u7ec3\uff0c\u6709\u66f4\u5e7f\u7684\u6269\u5c55\u7a7a\u95f4\u3002 \u4e0e\u6b64\u540c\u65f6NeRF\u91cd\u6295\u5f71\u6e32\u67d3\u4e0e\u5355\u76ee\u6df1\u5ea6\u8bad\u7ec3\u6709\u4e00\u5b9a\u4e0d\u540c\uff0c\u4e5f\u5e26\u6765\u4e86\u76f8\u5e94\u7684\u6311\u6218: \u5bf9\u4e8e\u52a8\u6001\u7269\u4f53\uff0c\u5355\u76ee\u6df1\u5ea6\u53ef\u4ee5\u5229\u7528\u540c\u5411\u540c\u901f\u7269\u4f53\u5728\u76f8\u673a\u4e0a\u76f8\u5bf9\u4f4d\u7f6e\u76f8\u5bf9\u7a33\u5b9a\u3001\u4e0e\u8fdc\u5904\u80cc\u666f\u7269\u4f53\u76f8\u4f3c\u8fd9\u4e2a\u6781\u5f3a\u7684\u5148\u9a8c\uff0c\u540c\u65f6\u6ee4\u6389\u5927\u90e8\u5206\u7684\u52a8\u6001\u7269\u4f53\u4ee5\u53ca\u5929\u7a7a\u7b49\u52a8\u6001\u7269\u4f53\u3002\u4f46\u662f\u5bf9\u4e8e3D Occupancy\u7684\u8bad\u7ec3\u6765\u8bf4\u65e0\u6cd5\u4f7f\u7528\u8fd9\u4e2aTrick\u3002\u53bb\u9664\u80cc\u666f\u3001\u5904\u7406\u52a8\u6001\u7269\u4f53\u9700\u8981\u65b0\u7684\u65b9\u6848\u3002 \u5355\u76ee\u6df1\u5ea6\u7f51\u7edc\u57fa\u672c\u7531\u7eaf\u5377\u79ef\u7f51\u7edc\u7ec4\u6210\uff0c\u7f51\u7edc\u7ec4\u6210\u7b80\u5355\u3001\u901f\u5ea6\u5feb\u3001\u5360\u7528\u663e\u5b58\u5c11\u3002\u800c2D-3D\u7684\u7f51\u7edc\uff0c\u5c3d\u7ba1\u6709 DeformAttention \u5e26\u6765\u6027\u80fd\u7684\u5927\u5e45\u63d0\u5347\uff0c\u4f46\u662f\u6574\u4f53\u6a21\u578b\u5927\u5c0f\u4ecd\u7136\u8f83\u5927\u3002\u5728\u8fdb\u884c\u91cd\u6295\u5f71\uff0c\u5c24\u5176\u662f\u57fa\u4e8eNeRF\u7684\u53ef\u5fae\u5206\u6e32\u67d3\u65f6\uff0c\u82b1\u8d39\u7684\u663e\u5b58\u8d44\u6e90\u8f83\u5927\u3002 \u5728\u4e09\u7ef4\u6570\u636e\u4e2d\uff0c\u52a0\u5267\u4e86\u6570\u636elabel\u4e0d\u5e73\u8861\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u7531\u4e8e\u6a21\u578b\u89c4\u6a21\u3001NeRF\u3001\u591a\u5e27\u6e32\u67d3\u7684\u663e\u5b58\u8017\u8d39\uff0c\u6700\u7ec8\u53ef\u5fae\u5206\u6e32\u67d3\u5f80\u5f80\u88ab\u8feb\u53ea\u80fd\u968f\u673a\u91c7\u6837\u4e00\u5b9a\u6570\u91cf\u7684\u5c04\u7ebf\uff0c\u5c11\u6837\u672c\u6570\u636e\u7684\u4fdd\u5b58\u4e5f\u6210\u4e3a\u95ee\u9898\u3002 \u8fd9\u7bc7paper\u540c\u6837\u9762\u5bf9\u524d\u6587\u63d0\u5230\u7684\u8fd9\u51e0\u4e2a\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u76f8\u5e94\u7684\u4fee\u6b63Trick\u3002","title":"RenderOcc: Vision-Centric 3D Occupancy Prediction with 2D Rendering Supervision"},{"location":"3dDetection/renderocc/#network-structure","text":"\u7406\u8bba\u4e0a\u6765\u8bf4\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u662f\u5bf92D-3D\u7f51\u7edc\u662f\u4e0d\u654f\u611f\u7684\uff0c\u5b9e\u9a8c\u4e2d\u91c7\u7528\u7684\u7f51\u7edc\u662fBEVStereo. \u800c\u5728\u6700\u7ec8\u8f93\u51fa\u5bc6\u5ea6density\u4e0esemantic\u7684\u65f6\u5019,density\u7684activation function\u4e3a softplus \\text{Softplus}(x) = \\frac{1}{\\beta} * \\text{log}(1+\\text{exp}(\\beta * x)) \u4fdd\u8bc1\u4e86: \u6fc0\u6d3b\u51fd\u6570\u975e\u8d1f\uff0c\u4e14\u53ef\u4ee5\u975e\u5e38\u9ad8\uff0c\u5168\u90e8\u5b9a\u4e49\u57df\u53ef\u4ee5\u8bad\u7ec3\u3002 \u5728\u521d\u59cb\u5316\u7684\u65f6\u5019\uff0c\u5bc6\u5ea6\u7ea6\u4e3a0.7, \u6e32\u67d3\u91c7\u6837\u4f1a\u504f\u5411\u4e8e\u8fd1\u5904\u7684\u7f51\u683c\u3002","title":"Network Structure"},{"location":"3dDetection/renderocc/#rendering-supervision","text":"\u8fd9\u7bc7paper\u91c7\u7528\u7684\u662f\u6df1\u5ea6\u4e0e\u8bed\u4e49\u5206\u5272\u76d1\u7763\u3002\u6df1\u5ea6\u9ed8\u8ba4\u6765\u81ea\u4e8e\u5f53\u5e27\u70b9\u4e91\uff0c\u5b9e\u9a8c\u4e2d\u6765\u81ea\u4e8eSFM\u7684\u6df1\u5ea6\u6295\u5f71\u540c\u6837\u80fd\u5b8c\u6210\u8bad\u7ec3\uff0c\u53ea\u662f\u51c6\u786e\u5ea6\u6709\u4e00\u5b9a\u7684\u533a\u522b\u3002\u800c\u8bed\u4e49\u5206\u5272\u6765\u81ea\u4e8e\u8bed\u4e49\u70b9\u4e91\u5bf9\u56fe\u7247\u7684\u6295\u5f71\u3002 \u6295\u5f71\u65b9\u6cd5\u4e0e\u8bad\u7ec3\u65b9\u5f0f\u4e0eNeRF\u4e00\u81f4\u3002 \\begin{aligned} & \\alpha\\left(z_k\\right)=1-\\exp \\left(-\\sigma\\left(z_k\\right) \\beta_k\\right) \\\\ & T\\left(z_k\\right)=\\exp \\left(-\\sum_{t=1}^{k-1} \\sigma\\left(z_t\\right) \\beta_t\\right) \\end{aligned} \\begin{aligned} & S^{\\text {pix }}(r)=\\sum_{k=1}^N T\\left(z_k\\right) \\alpha\\left(z_k\\right) S\\left(z_k\\right), \\\\ & D^{\\text {pix }}(r)=\\sum_{k=1}^N T\\left(z_k\\right) \\alpha\\left(z_k\\right) z_k, \\end{aligned} L=L_{\\text {seg }}\\left(S^{p i x}, \\hat{S}^{p i x}\\right)+L_{\\text {depth }}\\left(D^{p i x}, \\hat{D}^{p i x}\\right)+L_{r e g}(\\sigma) \u5176\u4e2d\u8bed\u4e49\u5206\u5272\u7684\u635f\u5931\u4e3a cross-entropy, \u6df1\u5ea6\u635f\u5931\u4e3a SILog, regularization Loss\u91c7\u7528\u7684\u662f distortion loss\u548cTV loss. \u800c\u6e32\u67d3\u70b9\u7684\u91c7\u6837\u65b9\u6848\uff0c\u91c7\u7528\u7684\u662f mip360-sampling","title":"Rendering Supervision"},{"location":"3dDetection/renderocc/#auxiliary-rays","text":"\u5728nuscenes\u8fd9\u79cd\u76f8\u90bb\u76f8\u673a\u51e0\u4e4e\u6ca1\u6709\u91cd\u53e0\u533a\u57df\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u91c7\u6837\uff0c\u4e0d\u53ef\u907f\u514d\u9700\u8981\u5904\u7406\u591a\u5e27\u6570\u636e\u3002\u8fd9\u7bc7paper\u5728nuscenes\u91c7\u7528\u524d\u540e\u5171\u4e03\u5e27\u7684\u56fe\u50cf\u8fdb\u884c\u91cd\u6295\u5f71\u3002\u968f\u673a\u91c7\u6837\u7684\u65f6\u5019\u662f\u6709\u4fa7\u91cd\u7684\uff0c \u7c7b\u522b\u5e73\u8861\uff0c\u6839\u636e\u50cf\u7d20\u70b9\u51fa\u73b0\u7684\u9891\u7387\u8ba1\u7b97\u6743\u91cd W_b W_b(r)=\\exp \\left(\\lambda_s *\\left(\\frac{\\max (M)}{N(\\mathbf{C}(r))}-1\\right)\\right) \u52a8\u6001\u6e05\u9664\u3002\u964d\u4f4e\u76f8\u90bb\u5e27\u7684\u6743\u91cd\uff0c\u5e76\u4e14\u76f8\u90bb\u5e27\u52a8\u6001\u7c7b\u522b\u7684\u6743\u91cd\u57fa\u672c\u8bbe\u7f6e\u4e3a0","title":"Auxiliary Rays:"},{"location":"3dDetection/siaNMS/","text":"siaNMS: Non-Maximum Suppression with Siamese Networks for Multi-Camera 3D Object Detection \u8fd9\u7bc7paper\u5c1d\u8bd5\u89e3\u51b3\u7684\u95ee\u9898\u662f\u76f8\u673a\u4e0e\u70b9\u4e91\u878d\u5408\u573a\u666f\u4e2d\u7684\u4e00\u4e2a\u95ee\u9898\uff0c\u5c31\u662f\u8bf4\u4e00\u4e2a\u7269\u4f53\u540c\u65f6\u88ab\u4e24\u4e2a\u76f8\u673a\u62cd\u6444\u65f6\uff0c\u5982\u4f55\u5145\u5206\u5229\u7528\u4e24\u4e2a\u76f8\u673a\u7684\u4fe1\u606f\u8fdb\u884c\u878d\u5408\u3002 \u603b\u4f53\u7ed3\u6784 \u8fd9\u7bc7\u6587\u7ae0\u603b\u4f53\u67b6\u6784\u8ddf\u968f Frustum Network \u7684\u8bbe\u8ba1\u3002\u4f7f\u75282D \u7269\u4f53\u68c0\u6d4b\u7b97\u6cd5\u5f97\u5230\u7269\u4f53\u76842\u7ef4\u6846\uff0c\u5728\u4e8c\u7ef4\u6846\u5bf9\u5e94\u7684\u65b9\u5f62\u67f1\u53f0\u4e2d\u627e\u51fa\u70b9\u4e91\u5b50\u96c6\uff0c\u4f7f\u7528Point Net\u4f5c\u6700\u540e\u76843D\u524d\u540e\u666f\u5206\u5272\u5df2\u7ecf3D\u53c2\u6570\u56de\u5f52\u3002 siaNMS\u7684\u7406\u89e3\u4e0e\u8bad\u7ec3\u65b9\u6cd5 \u4f5c\u8005\u91c7\u53d6\u4e00\u4e2a\u6bd4\u8f83\u76f4\u89c2\u7684\u8bbe\u8ba1\u8003\u8651\uff0c\u5bf9\u4e8eRoIpooling\u5f97\u5230\u7684\u6bcf\u4e00\u4e2a\u8f66\u5b50\u7684Feature Map\uff0c\u5982\u679c\u5b83\u4eec\u5c5e\u4e8e\u540c\u4e00\u8f86\u8f66\uff0c\u90a3\u4e48\u8fd9\u4e24\u4e2afeature\u5c31\u4f1a\u6bd4\u8f83\u63a5\u8fd1\u3002 Inference\u8fc7\u7a0b \u5bf9\u4e8e\u4e24\u4e2a\u76f8\u673a\u91cc\u9762\u53d1\u73b0\u7684object\uff0c\u7ecf\u8fc7siaNMS\u5f97\u5230\u5b83\u4eec\u7684feature\uff0c\u8ba1\u7b97\u5b83\u4eec\u4e4b\u95f4\u7684\u8ddd\u79bb\u3002\u4f7f\u7528 \u5308\u7259\u5229\u7b97\u6cd5 \u5f97\u5230\u6700\u4f73\u4e24\u4e24\u5339\u914d\u7ed3\u679c\uff0c\u5982\u679c\u4e24\u4e2a\u5339\u914d\u6846\u5e76\u96c6(union)\u5185\u6709\u70b9\u4e91\uff0c\u5219\u8fd9\u4e9b\u70b9\u4e91\u90fd\u4f1a\u4f5c\u4e3a\u8fd9\u4e2a\u7269\u4f53Frustum\u5185\u7684\u70b9\u3002 \u7b80\u5355\u7684\u8868\u8ff0\u6765\u8bf4\u5c31\u662f\u88ab\u5339\u914d\u8ba4\u4e3a\u662f\u540c\u4e00\u4e2a\u7269\u4f53\u7684\u76f8\u673a\u6846\u5185\u7684\u70b9\uff0c\u5408\u5e76\u540e\u7684\u76f8\u673a\u4e3b\u8f74\u8bbe\u8ba1\u4e3a Training \u65b9\u6cd5 \u8fd9\u91cc\u4f7f\u7528 nuscene \u6570\u636e\u96c6\uff0c nuscene \u7684\u4e00\u4e2a\u7279\u5f81\u662f\u6807\u8bb0\u6bcf\u4e00\u4e2a\u573a\u666f\u91cc\u9762\u6bcf\u4e00\u8f86\u8f66\u7684instance ID\u3002\u4f5c\u8005\u5148\u4f7f\u75282D detector\u5f97\u5230\u6bcf\u4e00\u4e2a\u573a\u666f\u91cc\u9762\u6bcf\u4e00\u8f86\u8f66\u5728\u6bcf\u4e00\u4e2a\u6444\u50cf\u5934\u4e2dRoIAlign\u540e\u7684\u7279\u5f81\u56fe\u3002 \u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u6bcf\u4e00\u4e2a\u6837\u672c\u7531\u4e00\u8f86\u540c\u65f6\u5728\u4e24\u4e2a\u6444\u50cf\u5934\u4e2d\u51fa\u73b0\u7684\u8f66\u7684\u7279\u5f81\u56fe\uff0c\u4e00\u4e2a\u4e3a\u53c2\u8003\u6837\u672c x^r , \u4e00\u4e2a\u4e3a\u6b63\u6837\u672c x^p \uff0c\u540c\u573a\u666f\u53e6\u4e00\u8f86\u968f\u673a\u8f66\u8f86\u4f5c\u4e3a\u8d1f\u6837\u672c x^n \u3002\u8bad\u7ec3\u4e00\u4e2asiaNMS\u7f51\u7edc\uff0c\u8f93\u5165\u662f2D aligned\u540e\u7684\u7279\u5f81\u56fe\uff0c\u8f93\u51fa\u662f\u4e00\u4e2a d \u7ef4\u7684\u77e2\u91cf\u3002 \u6211\u4eec\u5e0c\u671b\u6b63\u6837\u672c\u4e0e\u53c2\u8003\u6837\u672c\u7684\u77e2\u91cf\u63a5\u8fd1\u800c\u8d1f\u6837\u672c\u4e0e\u53c2\u8003\u6837\u672c\u7684\u77e2\u91cf\u8ddd\u79bb\u8fdc\u3002 \\begin{array}{l} \\left\\|f\\left(x_{i}^{r}\\right)-f\\left(x_{i}^{p}\\right)\\right\\|_{2}<\\alpha \\\\ \\left\\|f\\left(x_{i}^{r}\\right)-f\\left(x_{i}^{n}\\right)\\right\\|_{2}>\\beta \\end{array} \u56e0\u800c\u635f\u5931\u51fd\u6570\u4e3a\uff1a \\begin{aligned} \\mathcal{L}=& \\frac{1}{2} \\sum_{i}^{N}\\left[\\max \\left(\\left\\|f\\left(x_{i}^{r}\\right)-f\\left(x_{i}^{p}\\right)\\right\\|_{2}-\\alpha, 0\\right)^{2}+\\right.\\\\ &\\left.\\max \\left(\\beta-\\left\\|f\\left(x_{i}^{r}\\right)-f\\left(x_{i}^{n}\\right)\\right\\|_{2}, 0\\right)^{2}\\right] \\end{aligned} \u8bad\u7ec3\u5b8c\u4e4b\u540e\u8fd9\u4e2a\u6a21\u5757\u53ef\u4ee5\u4f5c\u4e3a\u4e00\u4e2aplug-and-play\u7684\u72ec\u7acb\u6a21\u5757\u52a0\u5165\u7f51\u7edc\u4e4b\u4e2d\uff0c\u7528\u4e8e\u878d\u5408\u4e0d\u540c\u76f8\u673a\u56fe\u7247\u7684\u56fe\u50cf\u4fe1\u606f\u3002 \u4ece\u8bba\u6587\u793a\u4f8b\u56fe\u53ef\u4ee5\u770b\u5230\u8fd9\u4e2a\u6a21\u5757\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u8fb9\u754c\u5904\u7684detection\u7ed3\u679c\u3002","title":"siaNMS: Non-Maximum Suppression with Siamese Networks for Multi-Camera 3D Object Detection"},{"location":"3dDetection/siaNMS/#sianms-non-maximum-suppression-with-siamese-networks-for-multi-camera-3d-object-detection","text":"\u8fd9\u7bc7paper\u5c1d\u8bd5\u89e3\u51b3\u7684\u95ee\u9898\u662f\u76f8\u673a\u4e0e\u70b9\u4e91\u878d\u5408\u573a\u666f\u4e2d\u7684\u4e00\u4e2a\u95ee\u9898\uff0c\u5c31\u662f\u8bf4\u4e00\u4e2a\u7269\u4f53\u540c\u65f6\u88ab\u4e24\u4e2a\u76f8\u673a\u62cd\u6444\u65f6\uff0c\u5982\u4f55\u5145\u5206\u5229\u7528\u4e24\u4e2a\u76f8\u673a\u7684\u4fe1\u606f\u8fdb\u884c\u878d\u5408\u3002","title":"siaNMS: Non-Maximum Suppression with Siamese Networks for Multi-Camera 3D Object Detection"},{"location":"3dDetection/siaNMS/#_1","text":"\u8fd9\u7bc7\u6587\u7ae0\u603b\u4f53\u67b6\u6784\u8ddf\u968f Frustum Network \u7684\u8bbe\u8ba1\u3002\u4f7f\u75282D \u7269\u4f53\u68c0\u6d4b\u7b97\u6cd5\u5f97\u5230\u7269\u4f53\u76842\u7ef4\u6846\uff0c\u5728\u4e8c\u7ef4\u6846\u5bf9\u5e94\u7684\u65b9\u5f62\u67f1\u53f0\u4e2d\u627e\u51fa\u70b9\u4e91\u5b50\u96c6\uff0c\u4f7f\u7528Point Net\u4f5c\u6700\u540e\u76843D\u524d\u540e\u666f\u5206\u5272\u5df2\u7ecf3D\u53c2\u6570\u56de\u5f52\u3002","title":"\u603b\u4f53\u7ed3\u6784"},{"location":"3dDetection/siaNMS/#sianms","text":"\u4f5c\u8005\u91c7\u53d6\u4e00\u4e2a\u6bd4\u8f83\u76f4\u89c2\u7684\u8bbe\u8ba1\u8003\u8651\uff0c\u5bf9\u4e8eRoIpooling\u5f97\u5230\u7684\u6bcf\u4e00\u4e2a\u8f66\u5b50\u7684Feature Map\uff0c\u5982\u679c\u5b83\u4eec\u5c5e\u4e8e\u540c\u4e00\u8f86\u8f66\uff0c\u90a3\u4e48\u8fd9\u4e24\u4e2afeature\u5c31\u4f1a\u6bd4\u8f83\u63a5\u8fd1\u3002","title":"siaNMS\u7684\u7406\u89e3\u4e0e\u8bad\u7ec3\u65b9\u6cd5"},{"location":"3dDetection/siaNMS/#inference","text":"\u5bf9\u4e8e\u4e24\u4e2a\u76f8\u673a\u91cc\u9762\u53d1\u73b0\u7684object\uff0c\u7ecf\u8fc7siaNMS\u5f97\u5230\u5b83\u4eec\u7684feature\uff0c\u8ba1\u7b97\u5b83\u4eec\u4e4b\u95f4\u7684\u8ddd\u79bb\u3002\u4f7f\u7528 \u5308\u7259\u5229\u7b97\u6cd5 \u5f97\u5230\u6700\u4f73\u4e24\u4e24\u5339\u914d\u7ed3\u679c\uff0c\u5982\u679c\u4e24\u4e2a\u5339\u914d\u6846\u5e76\u96c6(union)\u5185\u6709\u70b9\u4e91\uff0c\u5219\u8fd9\u4e9b\u70b9\u4e91\u90fd\u4f1a\u4f5c\u4e3a\u8fd9\u4e2a\u7269\u4f53Frustum\u5185\u7684\u70b9\u3002 \u7b80\u5355\u7684\u8868\u8ff0\u6765\u8bf4\u5c31\u662f\u88ab\u5339\u914d\u8ba4\u4e3a\u662f\u540c\u4e00\u4e2a\u7269\u4f53\u7684\u76f8\u673a\u6846\u5185\u7684\u70b9\uff0c\u5408\u5e76\u540e\u7684\u76f8\u673a\u4e3b\u8f74\u8bbe\u8ba1\u4e3a","title":"Inference\u8fc7\u7a0b"},{"location":"3dDetection/siaNMS/#training","text":"\u8fd9\u91cc\u4f7f\u7528 nuscene \u6570\u636e\u96c6\uff0c nuscene \u7684\u4e00\u4e2a\u7279\u5f81\u662f\u6807\u8bb0\u6bcf\u4e00\u4e2a\u573a\u666f\u91cc\u9762\u6bcf\u4e00\u8f86\u8f66\u7684instance ID\u3002\u4f5c\u8005\u5148\u4f7f\u75282D detector\u5f97\u5230\u6bcf\u4e00\u4e2a\u573a\u666f\u91cc\u9762\u6bcf\u4e00\u8f86\u8f66\u5728\u6bcf\u4e00\u4e2a\u6444\u50cf\u5934\u4e2dRoIAlign\u540e\u7684\u7279\u5f81\u56fe\u3002 \u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u6bcf\u4e00\u4e2a\u6837\u672c\u7531\u4e00\u8f86\u540c\u65f6\u5728\u4e24\u4e2a\u6444\u50cf\u5934\u4e2d\u51fa\u73b0\u7684\u8f66\u7684\u7279\u5f81\u56fe\uff0c\u4e00\u4e2a\u4e3a\u53c2\u8003\u6837\u672c x^r , \u4e00\u4e2a\u4e3a\u6b63\u6837\u672c x^p \uff0c\u540c\u573a\u666f\u53e6\u4e00\u8f86\u968f\u673a\u8f66\u8f86\u4f5c\u4e3a\u8d1f\u6837\u672c x^n \u3002\u8bad\u7ec3\u4e00\u4e2asiaNMS\u7f51\u7edc\uff0c\u8f93\u5165\u662f2D aligned\u540e\u7684\u7279\u5f81\u56fe\uff0c\u8f93\u51fa\u662f\u4e00\u4e2a d \u7ef4\u7684\u77e2\u91cf\u3002 \u6211\u4eec\u5e0c\u671b\u6b63\u6837\u672c\u4e0e\u53c2\u8003\u6837\u672c\u7684\u77e2\u91cf\u63a5\u8fd1\u800c\u8d1f\u6837\u672c\u4e0e\u53c2\u8003\u6837\u672c\u7684\u77e2\u91cf\u8ddd\u79bb\u8fdc\u3002 \\begin{array}{l} \\left\\|f\\left(x_{i}^{r}\\right)-f\\left(x_{i}^{p}\\right)\\right\\|_{2}<\\alpha \\\\ \\left\\|f\\left(x_{i}^{r}\\right)-f\\left(x_{i}^{n}\\right)\\right\\|_{2}>\\beta \\end{array} \u56e0\u800c\u635f\u5931\u51fd\u6570\u4e3a\uff1a \\begin{aligned} \\mathcal{L}=& \\frac{1}{2} \\sum_{i}^{N}\\left[\\max \\left(\\left\\|f\\left(x_{i}^{r}\\right)-f\\left(x_{i}^{p}\\right)\\right\\|_{2}-\\alpha, 0\\right)^{2}+\\right.\\\\ &\\left.\\max \\left(\\beta-\\left\\|f\\left(x_{i}^{r}\\right)-f\\left(x_{i}^{n}\\right)\\right\\|_{2}, 0\\right)^{2}\\right] \\end{aligned} \u8bad\u7ec3\u5b8c\u4e4b\u540e\u8fd9\u4e2a\u6a21\u5757\u53ef\u4ee5\u4f5c\u4e3a\u4e00\u4e2aplug-and-play\u7684\u72ec\u7acb\u6a21\u5757\u52a0\u5165\u7f51\u7edc\u4e4b\u4e2d\uff0c\u7528\u4e8e\u878d\u5408\u4e0d\u540c\u76f8\u673a\u56fe\u7247\u7684\u56fe\u50cf\u4fe1\u606f\u3002 \u4ece\u8bba\u6587\u793a\u4f8b\u56fe\u53ef\u4ee5\u770b\u5230\u8fd9\u4e2a\u6a21\u5757\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u8fb9\u754c\u5904\u7684detection\u7ed3\u679c\u3002","title":"Training \u65b9\u6cd5"},{"location":"3dDetection/voxelNet/","text":"VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection \u70b9\u4e91\u6295\u5f71\u5230\u4e09\u7ef4\u7f51\u683c\uff0c\u6bcf\u4e00\u4e2a\u7f51\u683c\u5185\u901a\u8fc7\u968f\u673a\u53d6\u70b9\u52a0\u5168\u8fde\u63a5\u4f20\u64ad\uff0c\u5f97\u5230\u4e00\u4e2a\u5355\u4e00\u7684feature-vector\u3002\u5173\u952e\u662f\u5bf9\u4e8e\u8fd9\u6837\u4e00\u4e2a\u7a00\u758f\u76844D\u77e9\u9635\uff0c\u5728\u540e\u9762\u5982\u4f55\u5feb\u901f\u8fdb\u884c\u77e2\u91cf\u8fd0\u7b97\uff0c\u8fd9\u91cc\u4f7f\u7528\u7a00\u758f\u8868\u8fbe\uff0c\u7528hash table\u5efa\u7acb\u5feb\u901findex\u3002","title":"VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection"},{"location":"3dDetection/voxelNet/#voxelnet-end-to-end-learning-for-point-cloud-based-3d-object-detection","text":"\u70b9\u4e91\u6295\u5f71\u5230\u4e09\u7ef4\u7f51\u683c\uff0c\u6bcf\u4e00\u4e2a\u7f51\u683c\u5185\u901a\u8fc7\u968f\u673a\u53d6\u70b9\u52a0\u5168\u8fde\u63a5\u4f20\u64ad\uff0c\u5f97\u5230\u4e00\u4e2a\u5355\u4e00\u7684feature-vector\u3002\u5173\u952e\u662f\u5bf9\u4e8e\u8fd9\u6837\u4e00\u4e2a\u7a00\u758f\u76844D\u77e9\u9635\uff0c\u5728\u540e\u9762\u5982\u4f55\u5feb\u901f\u8fdb\u884c\u77e2\u91cf\u8fd0\u7b97\uff0c\u8fd9\u91cc\u4f7f\u7528\u7a00\u758f\u8868\u8fbe\uff0c\u7528hash table\u5efa\u7acb\u5feb\u901findex\u3002","title":"VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection"},{"location":"3dDetection/weakly_super/","text":"Weakly Supervised 3D Object Detection from Point Clouds \u8fd9\u7bc7paper\u63d0\u51fa\u4e86VS3D\u6846\u67b6\uff0c\u4efb\u52a1\u662f\u4e0d\u4f7f\u75283D bounding boxes\u4f5c\u4e3a\u76d1\u7763\u4fe1\u53f7\uff0c\u5b9e\u73b0\u57fa\u4e8e\u70b9\u4e91\u76843D\u7269\u4f53\u68c0\u6d4b\u3002 \u6574\u4f53\u6846\u67b6 Unsupervised 3D object proposal module (UPM) \u8fd9\u4e00\u90e8\u5206\u7684\u4efb\u52a1\u662f\u63d0\u51faproposal, \u4f5c\u8005\u7684\u601d\u8def\u662f\u9996\u5148\u57283D\u4e16\u754c\u4e2d\u5bc6\u96c6\u5730\u91c7\u6837proposals,\u7136\u540e\u6839\u636eprior\uff0c\u5982\u679c\u4e00\u4e2aproposal\u5185\u70b9\u7684\u5bc6\u5ea6\u6bd4\u8f83\u5927\uff0c\u90a3\u4e48\u8fd9\u4e2a\u6846\u6709\u66f4\u5927\u7684\u6982\u7387\u6709\u611f\u5174\u8da3\u7684\u7269\u4f53\u3002\u4f46\u662f\u70b9\u7684\u5bc6\u5ea6\u4e0e\u8ddd\u79bb\u4e5f\u6709\u5173\u7cfb\uff0c\u56e0\u800c\u9700\u8981\u6392\u9664\u8ddd\u79bb\u7684\u56e0\u7d20\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86 normalized point cloud density D_c \u8fd9\u4e2a\u6982\u5ff5\u3002\u6ce8\u610f\u5730\u9762\u4e0a\u7684\u70b9\u9700\u8981\u7528RANSAC\u5e73\u9762\u62df\u5408\u5148\u53bb\u9664\u3002 \u6240\u6709 D_c < \\delta \u7684proposals\u90fd\u88ab\u7406\u89e3\u4e3a\u8d1f\u6837\u672c\uff0c\u800c\u5269\u4e0b\u7684\u6837\u672c\u6839\u636e\u4e0a\u56fe(c)\u4e2d\u7684\u505a\u6cd5\u6269\u5927 1 + \\epsilon \u500d,\u4fdd\u8bc1proposal\u57fa\u672c\u80fd\u5b8c\u6574\u7684\u5305\u542b\u6574\u4e2aobject. \u7136\u540e\u4f5c\u8005\u53d1\u73b0\u5bf9\u4e8e\u6b63\u786e\u68c0\u6d4b\u7684\u7269\u4f53\uff0c\u70b9\u4e91\u7684\u70b9\u90fd\u5728 bounding boxes\u7684\u8fb9\u7f18\u3002\u5728bounding box\u5750\u6807\u7cfb\u4e0b\uff0c\u5728x,y,z\u65b9\u5411\u4e0a\u5e73\u79fb\uff0c\u4f7f\u5f97x,y,z\u65b9\u5411\u4e0a\u90fd\u4e0e\u6700\u504f\u7684\u70b9\u76f8\u5207\u3002 \u81f3\u6b64\uff0c\u5f97\u5230\u7684proposal\u662f\u4e0d\u5e26\u65cb\u8f6c\uff0c\u4e14\u6ca1\u6709\u7c7b\u522b\u6807\u6ce8\u7684\u3002 cross-model transfer learning module \u4f5c\u8005\u4f7f\u7528Pascal3D+\u7684label\uff0c\u5f97\u5230\u9884\u8bad\u7ec3\u7684detector\u4ee5\u53caview-point regressor. \u5206\u7c7b\u8bad\u7ec3\u7684\u65f6\u5019\uff0c\u7531\u4e8e\u9700\u8981\u8003\u8651teacher network\u7684\u4e0d\u786e\u5b9a\u6027,\u5bf9\u4e8escore\u9ad8\u4e8e s_j \u6216\u8005\u4f4e\u4e8e s_l (\u4e5f\u5c31\u662f\u8f83\u4e3a\u786e\u5b9a\u7684\u7ed3\u679c)\u7684\u7ed3\u679c\u3002 \\mathcal{L}_{r}=-[\\hat{s} \\log (\\tilde{s})+(1-\\hat{s}) \\log (1-\\tilde{s})] \\cdot \\mathbb{1}\\left(s \\notin\\left[s_{l}, s_{h}\\right]\\right)","title":"Weakly Supervised 3D Object Detection from Point Clouds"},{"location":"3dDetection/weakly_super/#weakly-supervised-3d-object-detection-from-point-clouds","text":"\u8fd9\u7bc7paper\u63d0\u51fa\u4e86VS3D\u6846\u67b6\uff0c\u4efb\u52a1\u662f\u4e0d\u4f7f\u75283D bounding boxes\u4f5c\u4e3a\u76d1\u7763\u4fe1\u53f7\uff0c\u5b9e\u73b0\u57fa\u4e8e\u70b9\u4e91\u76843D\u7269\u4f53\u68c0\u6d4b\u3002","title":"Weakly Supervised 3D Object Detection from Point Clouds"},{"location":"3dDetection/weakly_super/#_1","text":"","title":"\u6574\u4f53\u6846\u67b6"},{"location":"3dDetection/weakly_super/#unsupervised-3d-object-proposal-module-upm","text":"\u8fd9\u4e00\u90e8\u5206\u7684\u4efb\u52a1\u662f\u63d0\u51faproposal, \u4f5c\u8005\u7684\u601d\u8def\u662f\u9996\u5148\u57283D\u4e16\u754c\u4e2d\u5bc6\u96c6\u5730\u91c7\u6837proposals,\u7136\u540e\u6839\u636eprior\uff0c\u5982\u679c\u4e00\u4e2aproposal\u5185\u70b9\u7684\u5bc6\u5ea6\u6bd4\u8f83\u5927\uff0c\u90a3\u4e48\u8fd9\u4e2a\u6846\u6709\u66f4\u5927\u7684\u6982\u7387\u6709\u611f\u5174\u8da3\u7684\u7269\u4f53\u3002\u4f46\u662f\u70b9\u7684\u5bc6\u5ea6\u4e0e\u8ddd\u79bb\u4e5f\u6709\u5173\u7cfb\uff0c\u56e0\u800c\u9700\u8981\u6392\u9664\u8ddd\u79bb\u7684\u56e0\u7d20\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86 normalized point cloud density D_c \u8fd9\u4e2a\u6982\u5ff5\u3002\u6ce8\u610f\u5730\u9762\u4e0a\u7684\u70b9\u9700\u8981\u7528RANSAC\u5e73\u9762\u62df\u5408\u5148\u53bb\u9664\u3002 \u6240\u6709 D_c < \\delta \u7684proposals\u90fd\u88ab\u7406\u89e3\u4e3a\u8d1f\u6837\u672c\uff0c\u800c\u5269\u4e0b\u7684\u6837\u672c\u6839\u636e\u4e0a\u56fe(c)\u4e2d\u7684\u505a\u6cd5\u6269\u5927 1 + \\epsilon \u500d,\u4fdd\u8bc1proposal\u57fa\u672c\u80fd\u5b8c\u6574\u7684\u5305\u542b\u6574\u4e2aobject. \u7136\u540e\u4f5c\u8005\u53d1\u73b0\u5bf9\u4e8e\u6b63\u786e\u68c0\u6d4b\u7684\u7269\u4f53\uff0c\u70b9\u4e91\u7684\u70b9\u90fd\u5728 bounding boxes\u7684\u8fb9\u7f18\u3002\u5728bounding box\u5750\u6807\u7cfb\u4e0b\uff0c\u5728x,y,z\u65b9\u5411\u4e0a\u5e73\u79fb\uff0c\u4f7f\u5f97x,y,z\u65b9\u5411\u4e0a\u90fd\u4e0e\u6700\u504f\u7684\u70b9\u76f8\u5207\u3002 \u81f3\u6b64\uff0c\u5f97\u5230\u7684proposal\u662f\u4e0d\u5e26\u65cb\u8f6c\uff0c\u4e14\u6ca1\u6709\u7c7b\u522b\u6807\u6ce8\u7684\u3002","title":"Unsupervised 3D object proposal module (UPM)"},{"location":"3dDetection/weakly_super/#cross-model-transfer-learning-module","text":"\u4f5c\u8005\u4f7f\u7528Pascal3D+\u7684label\uff0c\u5f97\u5230\u9884\u8bad\u7ec3\u7684detector\u4ee5\u53caview-point regressor. \u5206\u7c7b\u8bad\u7ec3\u7684\u65f6\u5019\uff0c\u7531\u4e8e\u9700\u8981\u8003\u8651teacher network\u7684\u4e0d\u786e\u5b9a\u6027,\u5bf9\u4e8escore\u9ad8\u4e8e s_j \u6216\u8005\u4f4e\u4e8e s_l (\u4e5f\u5c31\u662f\u8f83\u4e3a\u786e\u5b9a\u7684\u7ed3\u679c)\u7684\u7ed3\u679c\u3002 \\mathcal{L}_{r}=-[\\hat{s} \\log (\\tilde{s})+(1-\\hat{s}) \\log (1-\\tilde{s})] \\cdot \\mathbb{1}\\left(s \\notin\\left[s_{l}, s_{h}\\right]\\right)","title":"cross-model transfer learning module"},{"location":"Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/","text":"4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural Networks \u8fd9\u7bc7\u8bba\u6587\u4f7f\u7528\u7a00\u758f\u77e2\u91cf\u3001\u5e7f\u4e49\u7a00\u758f\u5377\u79ef\uff0c\u62d3\u5c55\u4e863D\u30014D\u5377\u79ef\uff0c\u52a0\u901f\u4e86\u7a00\u758f\u7684\u70b9\u4e91\u7684\u9ad8\u7ef4\u5377\u79ef\u3002 \u76ee\u524d\u6ca1\u770b\u61c2\u539f\u7406\uff0c\u800c\u4e14\u6709\u5927\u91cf\u57fa\u4e8eGPU cuda\u7684\u5de5\u7a0b\u5de5\u4f5c\u3002","title":"4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural Networks"},{"location":"Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/#4d-spatio-temporal-convnets-minkowski-convolutional-neural-networks","text":"\u8fd9\u7bc7\u8bba\u6587\u4f7f\u7528\u7a00\u758f\u77e2\u91cf\u3001\u5e7f\u4e49\u7a00\u758f\u5377\u79ef\uff0c\u62d3\u5c55\u4e863D\u30014D\u5377\u79ef\uff0c\u52a0\u901f\u4e86\u7a00\u758f\u7684\u70b9\u4e91\u7684\u9ad8\u7ef4\u5377\u79ef\u3002 \u76ee\u524d\u6ca1\u770b\u61c2\u539f\u7406\uff0c\u800c\u4e14\u6709\u5927\u91cf\u57fa\u4e8eGPU cuda\u7684\u5de5\u7a0b\u5de5\u4f5c\u3002","title":"4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural Networks"},{"location":"Building_Blocks/ASL_loss/","text":"Asymmetric Loss For Multi-Label Classification (ASL Loss) \u8fd9\u7bc7paper\u63d0\u51fa\u4e86 Asymmetric Loss. \u7528\u4e8e\u89e3\u51b3\u6b63\u8d1f\u6837\u672c\u4e0d\u5e73\u8861\u7684\u95ee\u9898\u3002\u8fdb\u4e00\u6b65\u5730\u63d0\u4f9b\u4e86\u6982\u7387\u89e3\u91ca\u4ee5\u53ca\u81ea\u9002\u5e94\u7248\u672c\u3002 Focal Loss L=-y L_{+}-(1-y) L_{-} \\left\\{\\begin{array}{l} L_{+}=(1-p)^{\\gamma} \\log (p) \\\\ L_{-}=p^{\\gamma} \\log (1-p) \\end{array}\\right. Asymmetric Focusing \u4f5c\u8005\u6dfb\u52a0\u591a\u4e00\u4e2a\u53d8\u91cf,\u63d0\u51fa\u8ba9\u8d1f\u6837\u672c\u4e0e\u6b63\u6837\u672c\u7684 \\gamma \u4e0d\u5bf9\u79f0\uff0c\u4e14\u4e00\u822c\u6765\u8bf4 \\gamma_{-} > \\gamma_{+} \\left\\{\\begin{array}{l} L_{+}=(1-p)^{\\gamma_{+}} \\log (p) \\\\ L_{-}=p^{\\gamma_{-}}\\log (1-p) \\end{array}\\right. \u8fdb\u4e00\u6b65\u4f5c\u8005\u63d0\u51fa Asymmetric Probability Shifting. \u5bf9\u4e8e\u8d1f\u6837\u672c\u4e2d\u7b80\u5355\u7684\u90e8\u5206\u76f4\u63a5\u6e05\u9664\u6389. \\begin{aligned} &p_m = max(p - m, 0) \\\\ &L_{-} = (p_m)^{\\gamma_{-}} \\log{1 - p_m} \\end{aligned} \u6982\u7387\u5b9e\u9a8c \u4f5c\u8005\u5b9a\u4e49\u5bf9\u9884\u6d4b\u7ed3\u679c\u7684\u6709\u6548\u7f6e\u4fe1\u5ea6 p_t p_{t}=\\left\\{\\begin{array}{ll} \\bar{p} & \\text { if } y=1 \\\\ 1-\\bar{p} & \\text { otherwise } \\end{array}\\right. \u6b63\u8d1f\u6837\u672c\u6982\u7387gap: \\Delta p = p_t^+ - p_t^- \u5b9e\u9a8c\u53d1\u73b0ASL Loss\u53ef\u4ee5\u4f7f\u5f97\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u6b63\u8d1f\u6837\u672c\u7684\u6982\u7387gap\u6bd4\u8f83\u5c0f\u3002 \u81ea\u9002\u5e94\u8c03\u53c2 \u7531\u4ee5\u4e0a\u5b9e\u9a8c\uff0c\u4f5c\u8005\u53d1\u73b0\u5176\u5b9e\u53ef\u4ee5\u52a8\u6001\u5730\u8c03\u6574 \\gamma_- \u4f7f\u5f97\u6982\u7387gap\u4fdd\u6301\u5728\u4e00\u5b9a\u7684\u8303\u56f4\u5185\u3002 \\gamma_{-} \\leftarrow \\gamma_{-}+\\lambda\\left(\\Delta p-\\Delta p_{\\text {target }}\\right)","title":"Asymmetric Loss For Multi-Label Classification (ASL Loss)"},{"location":"Building_Blocks/ASL_loss/#asymmetric-loss-for-multi-label-classification-asl-loss","text":"\u8fd9\u7bc7paper\u63d0\u51fa\u4e86 Asymmetric Loss. \u7528\u4e8e\u89e3\u51b3\u6b63\u8d1f\u6837\u672c\u4e0d\u5e73\u8861\u7684\u95ee\u9898\u3002\u8fdb\u4e00\u6b65\u5730\u63d0\u4f9b\u4e86\u6982\u7387\u89e3\u91ca\u4ee5\u53ca\u81ea\u9002\u5e94\u7248\u672c\u3002","title":"Asymmetric Loss For Multi-Label Classification (ASL Loss)"},{"location":"Building_Blocks/ASL_loss/#focal-loss","text":"L=-y L_{+}-(1-y) L_{-} \\left\\{\\begin{array}{l} L_{+}=(1-p)^{\\gamma} \\log (p) \\\\ L_{-}=p^{\\gamma} \\log (1-p) \\end{array}\\right.","title":"Focal Loss"},{"location":"Building_Blocks/ASL_loss/#asymmetric-focusing","text":"\u4f5c\u8005\u6dfb\u52a0\u591a\u4e00\u4e2a\u53d8\u91cf,\u63d0\u51fa\u8ba9\u8d1f\u6837\u672c\u4e0e\u6b63\u6837\u672c\u7684 \\gamma \u4e0d\u5bf9\u79f0\uff0c\u4e14\u4e00\u822c\u6765\u8bf4 \\gamma_{-} > \\gamma_{+} \\left\\{\\begin{array}{l} L_{+}=(1-p)^{\\gamma_{+}} \\log (p) \\\\ L_{-}=p^{\\gamma_{-}}\\log (1-p) \\end{array}\\right. \u8fdb\u4e00\u6b65\u4f5c\u8005\u63d0\u51fa Asymmetric Probability Shifting. \u5bf9\u4e8e\u8d1f\u6837\u672c\u4e2d\u7b80\u5355\u7684\u90e8\u5206\u76f4\u63a5\u6e05\u9664\u6389. \\begin{aligned} &p_m = max(p - m, 0) \\\\ &L_{-} = (p_m)^{\\gamma_{-}} \\log{1 - p_m} \\end{aligned}","title":"Asymmetric Focusing"},{"location":"Building_Blocks/ASL_loss/#_1","text":"\u4f5c\u8005\u5b9a\u4e49\u5bf9\u9884\u6d4b\u7ed3\u679c\u7684\u6709\u6548\u7f6e\u4fe1\u5ea6 p_t p_{t}=\\left\\{\\begin{array}{ll} \\bar{p} & \\text { if } y=1 \\\\ 1-\\bar{p} & \\text { otherwise } \\end{array}\\right. \u6b63\u8d1f\u6837\u672c\u6982\u7387gap: \\Delta p = p_t^+ - p_t^- \u5b9e\u9a8c\u53d1\u73b0ASL Loss\u53ef\u4ee5\u4f7f\u5f97\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u6b63\u8d1f\u6837\u672c\u7684\u6982\u7387gap\u6bd4\u8f83\u5c0f\u3002","title":"\u6982\u7387\u5b9e\u9a8c"},{"location":"Building_Blocks/ASL_loss/#_2","text":"\u7531\u4ee5\u4e0a\u5b9e\u9a8c\uff0c\u4f5c\u8005\u53d1\u73b0\u5176\u5b9e\u53ef\u4ee5\u52a8\u6001\u5730\u8c03\u6574 \\gamma_- \u4f7f\u5f97\u6982\u7387gap\u4fdd\u6301\u5728\u4e00\u5b9a\u7684\u8303\u56f4\u5185\u3002 \\gamma_{-} \\leftarrow \\gamma_{-}+\\lambda\\left(\\Delta p-\\Delta p_{\\text {target }}\\right)","title":"\u81ea\u9002\u5e94\u8c03\u53c2"},{"location":"Building_Blocks/AUGMIX/","text":"AUGMIX: A SIMPLE DATA PROCESSING METHOD TO IMPROVE ROBUSTNESS AND UNCERTAINTY \u8fd9\u7bc7\u8c37\u6b4c\u7684\u6587\u7ae0\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5bf9\u4e8e\u5206\u7c7bmodel\u5bb9\u6613\u4f7f\u7528\u7684\u81ea\u52a8\u6570\u636e\u589e\u5f3a\u8303\u5f0f\u3002\u4e0e\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u7b49\u5176\u4ed6\u65b9\u6cd5\u4e0d\u540c\uff0c\u8c37\u6b4c\u63d0\u51fa\u7684\u7b97\u6cd5\u53ea\u5305\u542b\u968f\u673a\u91c7\u6837\u52a0\u4e0a\u4e00\u4e2a\u9644\u52a0\u7684\u635f\u5931\u51fd\u6570\u9879\u3002\u5f00\u6e90\u7684\u4ee3\u7801\u4e3a\u7b80\u5355\u7684numpy\u4e0epytorch\uff0c\u8f83\u4e3a\u6613\u61c2\u3002 \u4f2a\u4ee3\u7801 \u4f5c\u8005\u63d0\u51fa\u7684\u6570\u636e\u52a0\u5f3a\u7684\u91c7\u6837\u7ed3\u679c\u4e0d\u662f\u591a\u4e2a\u6570\u636e\u52a0\u5f3a\u7684\u7b80\u5355\u5c42\u53e0(\u6df1\u5ea6\u4e0a\u7684\u94fe\u63a5)\uff0c\u4f5c\u8005\u8ba4\u4e3a\u8fd9\u79cd\u505a\u6cd5\u5f88\u5bb9\u6613\u4f7f\u5f97\u5f97\u5230\u7684\u6570\u636e\u504f\u79bb\u771f\u5b9e\u6570\u636e\u96c6\u592a\u8fdc\uff0c\u6240\u4ee5\u63d0\u51fa\u7684\u662f\u4e00\u4e2a\u7efc\u5408\u4e86\u5e7f\u5ea6\u548c\u6df1\u5ea6\u590d\u5408\u7ec4\u5408\u65b9\u5f0f\uff0c\u4e00\u4e2a\u4f8b\u5b50\u5982\u56fe \u635f\u5931\u51fd\u6570\u4e0a\u9700\u8981\u52a0\u4e0a Jensen-Shannon divergence . Jensen-Shannon divergence Jensen-Shannon divergence\u672c\u8d28\u4e0a\u662f KL divergence \u7684\u4e00\u4e2a\u6269\u5c55. \u672c\u6587\u7684\u4e09\u5143\u7684JS divergence\u5b9a\u4e49\u4e3a \\operatorname{JS}\\left(p_{\\text {orig }} ; p_{\\text {augmix } 1} ; p_{\\text {augmix } 2}\\right)=\\frac{1}{3}\\left(\\mathrm{KL}\\left[p_{\\text {orig }} \\| M\\right]+\\mathrm{KL}\\left[p_{\\text {augmix } 1} \\| M\\right]+\\mathrm{KL}\\left[p_{\\text {augmix } 2} \\| M\\right]\\right) \u6ce8\u610f Pytorch\u7684KL-divergence \u5728\u5b9e\u73b0\u4e0a\u7a0d\u7a0d\u6709\u5751\u3002\u672c\u6587\u4ee3\u7801\u6709\u5b8c\u6574\u7684\u5b9e\u73b0\u3002","title":"AUGMIX: A SIMPLE DATA PROCESSING METHOD TO IMPROVE ROBUSTNESS AND UNCERTAINTY"},{"location":"Building_Blocks/AUGMIX/#augmix-a-simple-data-processing-method-to-improve-robustness-and-uncertainty","text":"\u8fd9\u7bc7\u8c37\u6b4c\u7684\u6587\u7ae0\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5bf9\u4e8e\u5206\u7c7bmodel\u5bb9\u6613\u4f7f\u7528\u7684\u81ea\u52a8\u6570\u636e\u589e\u5f3a\u8303\u5f0f\u3002\u4e0e\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u7b49\u5176\u4ed6\u65b9\u6cd5\u4e0d\u540c\uff0c\u8c37\u6b4c\u63d0\u51fa\u7684\u7b97\u6cd5\u53ea\u5305\u542b\u968f\u673a\u91c7\u6837\u52a0\u4e0a\u4e00\u4e2a\u9644\u52a0\u7684\u635f\u5931\u51fd\u6570\u9879\u3002\u5f00\u6e90\u7684\u4ee3\u7801\u4e3a\u7b80\u5355\u7684numpy\u4e0epytorch\uff0c\u8f83\u4e3a\u6613\u61c2\u3002","title":"AUGMIX: A SIMPLE DATA PROCESSING METHOD TO IMPROVE ROBUSTNESS AND UNCERTAINTY"},{"location":"Building_Blocks/AUGMIX/#_1","text":"\u4f5c\u8005\u63d0\u51fa\u7684\u6570\u636e\u52a0\u5f3a\u7684\u91c7\u6837\u7ed3\u679c\u4e0d\u662f\u591a\u4e2a\u6570\u636e\u52a0\u5f3a\u7684\u7b80\u5355\u5c42\u53e0(\u6df1\u5ea6\u4e0a\u7684\u94fe\u63a5)\uff0c\u4f5c\u8005\u8ba4\u4e3a\u8fd9\u79cd\u505a\u6cd5\u5f88\u5bb9\u6613\u4f7f\u5f97\u5f97\u5230\u7684\u6570\u636e\u504f\u79bb\u771f\u5b9e\u6570\u636e\u96c6\u592a\u8fdc\uff0c\u6240\u4ee5\u63d0\u51fa\u7684\u662f\u4e00\u4e2a\u7efc\u5408\u4e86\u5e7f\u5ea6\u548c\u6df1\u5ea6\u590d\u5408\u7ec4\u5408\u65b9\u5f0f\uff0c\u4e00\u4e2a\u4f8b\u5b50\u5982\u56fe \u635f\u5931\u51fd\u6570\u4e0a\u9700\u8981\u52a0\u4e0a Jensen-Shannon divergence .","title":"\u4f2a\u4ee3\u7801"},{"location":"Building_Blocks/AUGMIX/#jensen-shannon-divergence","text":"Jensen-Shannon divergence\u672c\u8d28\u4e0a\u662f KL divergence \u7684\u4e00\u4e2a\u6269\u5c55. \u672c\u6587\u7684\u4e09\u5143\u7684JS divergence\u5b9a\u4e49\u4e3a \\operatorname{JS}\\left(p_{\\text {orig }} ; p_{\\text {augmix } 1} ; p_{\\text {augmix } 2}\\right)=\\frac{1}{3}\\left(\\mathrm{KL}\\left[p_{\\text {orig }} \\| M\\right]+\\mathrm{KL}\\left[p_{\\text {augmix } 1} \\| M\\right]+\\mathrm{KL}\\left[p_{\\text {augmix } 2} \\| M\\right]\\right) \u6ce8\u610f Pytorch\u7684KL-divergence \u5728\u5b9e\u73b0\u4e0a\u7a0d\u7a0d\u6709\u5751\u3002\u672c\u6587\u4ee3\u7801\u6709\u5b8c\u6574\u7684\u5b9e\u73b0\u3002","title":"Jensen-Shannon divergence"},{"location":"Building_Blocks/AdaIN/","text":"Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization \u8fd9\u7bc7\u6587\u7ae0\u63d0\u51fa\u4e86Style Transfer\u76f8\u5f53\u957f\u4e00\u6bb5\u65f6\u95f4\u7684SOTA\u6216\u8005baseline\uff0c\u4e5f\u5c31\u662fAdaIN\u6a21\u5757\u3002 \u80cc\u666f\u77e5\u8bc6 Normalization \u56de\u987e \u672c\u6587\u9664\u4e86\u57fa\u7840\u7684BatchNorm(\u5b8c\u6574batch)\u4ee5\u53caInstanceNorm(\u4e0d\u8003\u8651batch)\u8fd8\u56de\u987e\u4e86Conditional Instance Normalization(CIN). CIN\u5c42\u4e3a\u6bcf\u4e00\u4e2astyle\u5b66\u4e60\u4e00\u5957InstanceNorm\u7684\u53c2\u6570 \\gamma^s, \\beta^s \\operatorname{CIN}(x ; s)=\\gamma^{s}\\left(\\frac{x-\\mu(x)}{\\sigma(x)}\\right)+\\beta^{s} \u5b9e\u9a8c\u53ef\u77e5\u5bf9CIN\u7f51\u7edc\u800c\u8a00\uff0cBN\u7684\u7ed3\u679c\u603b\u4f53\u4e0d\u5982IN\u7684\u7ed3\u679c Adaptive Instance Normalization AdaIN\u516c\u5f0f: \\operatorname{AdaIN}(x, y)=\\sigma(y)\\left(\\frac{x-\\mu(x)}{\\sigma(x)}\\right)+\\mu(y) \u5176\u4e2d x \u4e3acontent input\u800c y \u4e3astyle input.\u672c\u8d28\u4e0a\u6765\u8bf4\u5c31\u662f\u5bf9content instance\u4f7f\u7528instance normalization \u8f6c\u6362\u4e3astyle instance\u7684\u5747\u503c\u4e0e\u65b9\u5dee. \u7f51\u7edc\u7ed3\u6784\u4e0e\u8bad\u7ec3\u65b9\u6cd5 \u7f51\u7edc\u7ed3\u6784\u5982\u56fe \u5c06\u4e24\u5f20\u56fe\u540c\u65f6\u8f93\u5165\u5230VGG encoder\u4e2d\uff0c\u6267\u884cAdaIN\uff0c\u5bf9content\u7ed3\u679cdecode\u5f97\u5230\u8f93\u51fa\u56fe Content Loss \u5229\u7528VGG encode style-transfered picture, \\mathcal{L_c} = ||f(g(t)) - t||_2 \u5176\u4e2d f \u4e3aencoder, g \u4e3adecoder\uff0c t \u4e3aAdaIN\u5904\u7684\u8f93\u51fa\uff0c\u4e5f\u5c31\u662f\u5bf9\u7279\u5f81\u5411\u91cf\u53d6L2\u8bef\u5dee Style Loss \u4ee5\u524d\u6709\u4f7f\u7528 \u683c\u62c9\u59c6\u77e9\u9635\u7684\u76f8\u5173\u6027\u8bef\u5dee\u7684 ,\u8fd9\u91cc\u8d85\u94fe\u63a5\u7ed9\u7684\u4f8b\u5b50\u6765\u81ea\u4e8ekeras\u7684\u4f8b\u5b50\u3002\u672c\u6587\u8fd9\u91cc\u4f7f\u7528\u53e6\u4e00\u4e2a \u522b\u4eba \u63d0\u51fa\u7684\u8bef\u5dee \\begin{array}{c}{\\mathcal{L}_{s}=\\sum_{i=1}^{L}\\left\\|\\mu\\left(\\phi_{i}(g(t))\\right)-\\mu\\left(\\phi_{i}(s)\\right)\\right\\|_{2}+} \\\\ {\\sum_{i=1}^{L}\\left\\|\\sigma\\left(\\phi_{i}(g(t))\\right)-\\sigma\\left(\\phi_{i}(s)\\right)\\right\\|_{2}}\\end{array} \u5176\u4e2d \\mu, \\sigma \u6307\u6c42\u5747\u503c\u4e0e\u65b9\u5dee, \\phi_i \u6307VGG-19\u4e2d\u7684\u67d0\u4e00\u5c42\u7684\u8f93\u51fa\u3002","title":"Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization"},{"location":"Building_Blocks/AdaIN/#arbitrary-style-transfer-in-real-time-with-adaptive-instance-normalization","text":"\u8fd9\u7bc7\u6587\u7ae0\u63d0\u51fa\u4e86Style Transfer\u76f8\u5f53\u957f\u4e00\u6bb5\u65f6\u95f4\u7684SOTA\u6216\u8005baseline\uff0c\u4e5f\u5c31\u662fAdaIN\u6a21\u5757\u3002","title":"Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization"},{"location":"Building_Blocks/AdaIN/#normalization","text":"\u672c\u6587\u9664\u4e86\u57fa\u7840\u7684BatchNorm(\u5b8c\u6574batch)\u4ee5\u53caInstanceNorm(\u4e0d\u8003\u8651batch)\u8fd8\u56de\u987e\u4e86Conditional Instance Normalization(CIN). CIN\u5c42\u4e3a\u6bcf\u4e00\u4e2astyle\u5b66\u4e60\u4e00\u5957InstanceNorm\u7684\u53c2\u6570 \\gamma^s, \\beta^s \\operatorname{CIN}(x ; s)=\\gamma^{s}\\left(\\frac{x-\\mu(x)}{\\sigma(x)}\\right)+\\beta^{s} \u5b9e\u9a8c\u53ef\u77e5\u5bf9CIN\u7f51\u7edc\u800c\u8a00\uff0cBN\u7684\u7ed3\u679c\u603b\u4f53\u4e0d\u5982IN\u7684\u7ed3\u679c","title":"\u80cc\u666f\u77e5\u8bc6 Normalization \u56de\u987e"},{"location":"Building_Blocks/AdaIN/#adaptive-instance-normalization","text":"AdaIN\u516c\u5f0f: \\operatorname{AdaIN}(x, y)=\\sigma(y)\\left(\\frac{x-\\mu(x)}{\\sigma(x)}\\right)+\\mu(y) \u5176\u4e2d x \u4e3acontent input\u800c y \u4e3astyle input.\u672c\u8d28\u4e0a\u6765\u8bf4\u5c31\u662f\u5bf9content instance\u4f7f\u7528instance normalization \u8f6c\u6362\u4e3astyle instance\u7684\u5747\u503c\u4e0e\u65b9\u5dee.","title":"Adaptive Instance Normalization"},{"location":"Building_Blocks/AdaIN/#_1","text":"\u7f51\u7edc\u7ed3\u6784\u5982\u56fe \u5c06\u4e24\u5f20\u56fe\u540c\u65f6\u8f93\u5165\u5230VGG encoder\u4e2d\uff0c\u6267\u884cAdaIN\uff0c\u5bf9content\u7ed3\u679cdecode\u5f97\u5230\u8f93\u51fa\u56fe","title":"\u7f51\u7edc\u7ed3\u6784\u4e0e\u8bad\u7ec3\u65b9\u6cd5"},{"location":"Building_Blocks/AdaIN/#content-loss","text":"\u5229\u7528VGG encode style-transfered picture, \\mathcal{L_c} = ||f(g(t)) - t||_2 \u5176\u4e2d f \u4e3aencoder, g \u4e3adecoder\uff0c t \u4e3aAdaIN\u5904\u7684\u8f93\u51fa\uff0c\u4e5f\u5c31\u662f\u5bf9\u7279\u5f81\u5411\u91cf\u53d6L2\u8bef\u5dee","title":"Content Loss"},{"location":"Building_Blocks/AdaIN/#style-loss","text":"\u4ee5\u524d\u6709\u4f7f\u7528 \u683c\u62c9\u59c6\u77e9\u9635\u7684\u76f8\u5173\u6027\u8bef\u5dee\u7684 ,\u8fd9\u91cc\u8d85\u94fe\u63a5\u7ed9\u7684\u4f8b\u5b50\u6765\u81ea\u4e8ekeras\u7684\u4f8b\u5b50\u3002\u672c\u6587\u8fd9\u91cc\u4f7f\u7528\u53e6\u4e00\u4e2a \u522b\u4eba \u63d0\u51fa\u7684\u8bef\u5dee \\begin{array}{c}{\\mathcal{L}_{s}=\\sum_{i=1}^{L}\\left\\|\\mu\\left(\\phi_{i}(g(t))\\right)-\\mu\\left(\\phi_{i}(s)\\right)\\right\\|_{2}+} \\\\ {\\sum_{i=1}^{L}\\left\\|\\sigma\\left(\\phi_{i}(g(t))\\right)-\\sigma\\left(\\phi_{i}(s)\\right)\\right\\|_{2}}\\end{array} \u5176\u4e2d \\mu, \\sigma \u6307\u6c42\u5747\u503c\u4e0e\u65b9\u5dee, \\phi_i \u6307VGG-19\u4e2d\u7684\u67d0\u4e00\u5c42\u7684\u8f93\u51fa\u3002","title":"Style Loss"},{"location":"Building_Blocks/Attention_Augmented_Conv/","text":"Attention Augmented Convolutional Networks \u8fd9\u7bc7\u6587\u7ae0\u57fa\u4e8e transformer \uff0c\u5c06attention\u76f4\u63a5\u9644\u52a0\u5728\u5377\u79ef\u5c42\u4e2d,\u7406\u8bba\u4e0a\u6765\u8bf4\u53ef\u4ee5\u7528\u4e8e\u66ff\u4ee3\u5377\u79ef\u5c42. \u56fe\u7247\u4e0a\u7684self-attention \u8bbe\u8f93\u5165\u5f20\u91cf\u5f62\u72b6\u4e3a (H, W, F_{in}) \uff0c\u8fd9\u91cc\u5ffd\u7565Batch.\u9996\u5148\u644a\u5e73\u4e3a\u4e00\u7ef4\u53d8\u4e3a X \u77e2\u91cf\uff0c\u7136\u540e\u76f4\u63a5\u4f7f\u7528 transformer \u4e2d\u7684\u81ea\u6ce8\u610f\u529b\u5c42 O_h = Softmax(\\frac{(XW_q)(XW_k)^T}{\\sqrt{d_k^h}})(XW_v) \u5176\u4e2d W_q, W_k, W_v \u5206\u522b\u662f\u8f93\u51fa\u7ef4\u5ea6\u4e3a d_k, d_k, d_v \u7684\u5168\u8fde\u63a5\u5c42\u7684\u6743\u91cd\u77e9\u9635\u3002 \u8f93\u51fa\u518dConcat\u4e3aMHA MHA(X) = Concat[O_1,...,O_{N_h}]W^O \u6700\u540e\u4f1a\u88abreshape\u6210\u4e3a (H, W, d_v) \u7684\u5f62\u72b6. \u4e8c\u7ef4positional embedding \u540c transformer ,\u6211\u4eec\u9700\u8981\u4e00\u4e2a\u4f20\u9012\u76f8\u5bf9\u4f4d\u7f6e\u4fe1\u606f\u7684\u65b9\u5f0f\uff0c\u6211\u4eec\u8981\u6c42\u8fd9\u4e2aembedding\u80fd\u591f\u4f7f\u7f51\u7edc\u5bf9\u4f4d\u7f6e\u654f\u611f\uff0c\u4f46\u662f\u4e0d\u80fd\u8ba9\u7f51\u7edc\u5bf9\u5e73\u79fb\u654f\u611f,\u8fd9\u91cc\u6765\u6e90\u6765\u81ea\u4e8emusic transformer. \u50cf\u7d20 i=(i_x, i_y) \u5bf9\u50cf\u7d20 j=(j_x,j_y) \u7684attention logits\u4e3a l_{i,j} = \\frac{q_i^T}{\\sqrt(d^h_k)}(k_j + r^W_{j_x-i_x} + r^H_{j_y-i_y}) \u5176\u4e2d q_i \u662f\u50cf\u7d20 i \u5bf9\u5e94\u7684query vector, k_j \u662f\u50cf\u7d20 j \u7684key vector, \u4e0a\u6587\u7684self-Attention\u8ba1\u7b97\u53ef\u4ee5\u8f6c\u5316\u4e3a O_h = Softmax(\\frac{QK^T + S^{rel}_H + S^{rel}_W}{\\sqrt{d^h_k}})V \u4e5f\u5c31\u662fsoftmax\u5206\u5b50\u52a0\u4e86\u4e24\u9879,\u5176\u4e2d S^{rel}_H[i,j] = q_i^Tr^H_{j_y,i_y}, S^{rel}_W = q_i^Tr^W_{j_x-i_x} \u3002 \u8bba\u6587\u8bf4\u8fd9\u4e2a r \u7684\u4e00\u79cd\u505a\u6cd5\u662f\u76f4\u63a5\u5b58\u6210\u4e00\u4e2a (HW, HW, d_k^h) \u77e9\u9635\uff0c\u4f46\u662f\u8fd9\u6837\u4e0d\u592a\u597d\uff0c\u6240\u4ee5\u91c7\u53d6\u4e86Music Transform\u7684\u7b97\u6cd5\uff0c\u66f4\u7701\u5185\u5b58\uff0c\u5177\u4f53\u770b\u4ee3\u7801(\u539f\u6587\u5c31\u662f\u8fd9\u6837)\uff0c\u8fd9\u4e9b\u53c2\u6570\u53ef\u5b66\u4e60\u3002 Attention Augmented conv AAConv(X) = Concat[Conv(X), MHA(X)]","title":"Attention Augmented Convolutional Networks"},{"location":"Building_Blocks/Attention_Augmented_Conv/#attention-augmented-convolutional-networks","text":"\u8fd9\u7bc7\u6587\u7ae0\u57fa\u4e8e transformer \uff0c\u5c06attention\u76f4\u63a5\u9644\u52a0\u5728\u5377\u79ef\u5c42\u4e2d,\u7406\u8bba\u4e0a\u6765\u8bf4\u53ef\u4ee5\u7528\u4e8e\u66ff\u4ee3\u5377\u79ef\u5c42.","title":"Attention Augmented Convolutional Networks"},{"location":"Building_Blocks/Attention_Augmented_Conv/#self-attention","text":"\u8bbe\u8f93\u5165\u5f20\u91cf\u5f62\u72b6\u4e3a (H, W, F_{in}) \uff0c\u8fd9\u91cc\u5ffd\u7565Batch.\u9996\u5148\u644a\u5e73\u4e3a\u4e00\u7ef4\u53d8\u4e3a X \u77e2\u91cf\uff0c\u7136\u540e\u76f4\u63a5\u4f7f\u7528 transformer \u4e2d\u7684\u81ea\u6ce8\u610f\u529b\u5c42 O_h = Softmax(\\frac{(XW_q)(XW_k)^T}{\\sqrt{d_k^h}})(XW_v) \u5176\u4e2d W_q, W_k, W_v \u5206\u522b\u662f\u8f93\u51fa\u7ef4\u5ea6\u4e3a d_k, d_k, d_v \u7684\u5168\u8fde\u63a5\u5c42\u7684\u6743\u91cd\u77e9\u9635\u3002 \u8f93\u51fa\u518dConcat\u4e3aMHA MHA(X) = Concat[O_1,...,O_{N_h}]W^O \u6700\u540e\u4f1a\u88abreshape\u6210\u4e3a (H, W, d_v) \u7684\u5f62\u72b6.","title":"\u56fe\u7247\u4e0a\u7684self-attention"},{"location":"Building_Blocks/Attention_Augmented_Conv/#positional-embedding","text":"\u540c transformer ,\u6211\u4eec\u9700\u8981\u4e00\u4e2a\u4f20\u9012\u76f8\u5bf9\u4f4d\u7f6e\u4fe1\u606f\u7684\u65b9\u5f0f\uff0c\u6211\u4eec\u8981\u6c42\u8fd9\u4e2aembedding\u80fd\u591f\u4f7f\u7f51\u7edc\u5bf9\u4f4d\u7f6e\u654f\u611f\uff0c\u4f46\u662f\u4e0d\u80fd\u8ba9\u7f51\u7edc\u5bf9\u5e73\u79fb\u654f\u611f,\u8fd9\u91cc\u6765\u6e90\u6765\u81ea\u4e8emusic transformer. \u50cf\u7d20 i=(i_x, i_y) \u5bf9\u50cf\u7d20 j=(j_x,j_y) \u7684attention logits\u4e3a l_{i,j} = \\frac{q_i^T}{\\sqrt(d^h_k)}(k_j + r^W_{j_x-i_x} + r^H_{j_y-i_y}) \u5176\u4e2d q_i \u662f\u50cf\u7d20 i \u5bf9\u5e94\u7684query vector, k_j \u662f\u50cf\u7d20 j \u7684key vector, \u4e0a\u6587\u7684self-Attention\u8ba1\u7b97\u53ef\u4ee5\u8f6c\u5316\u4e3a O_h = Softmax(\\frac{QK^T + S^{rel}_H + S^{rel}_W}{\\sqrt{d^h_k}})V \u4e5f\u5c31\u662fsoftmax\u5206\u5b50\u52a0\u4e86\u4e24\u9879,\u5176\u4e2d S^{rel}_H[i,j] = q_i^Tr^H_{j_y,i_y}, S^{rel}_W = q_i^Tr^W_{j_x-i_x} \u3002 \u8bba\u6587\u8bf4\u8fd9\u4e2a r \u7684\u4e00\u79cd\u505a\u6cd5\u662f\u76f4\u63a5\u5b58\u6210\u4e00\u4e2a (HW, HW, d_k^h) \u77e9\u9635\uff0c\u4f46\u662f\u8fd9\u6837\u4e0d\u592a\u597d\uff0c\u6240\u4ee5\u91c7\u53d6\u4e86Music Transform\u7684\u7b97\u6cd5\uff0c\u66f4\u7701\u5185\u5b58\uff0c\u5177\u4f53\u770b\u4ee3\u7801(\u539f\u6587\u5c31\u662f\u8fd9\u6837)\uff0c\u8fd9\u4e9b\u53c2\u6570\u53ef\u5b66\u4e60\u3002","title":"\u4e8c\u7ef4positional embedding"},{"location":"Building_Blocks/Attention_Augmented_Conv/#attention-augmented-conv","text":"AAConv(X) = Concat[Conv(X), MHA(X)]","title":"Attention Augmented conv"},{"location":"Building_Blocks/Attention_is_all_you_need/","text":"Attention is all you need \u8fd9\u7bc7\u8bba\u6587\u662f\u81ea\u6ce8\u610f\u673a\u5236\u5728\u65f6\u5e8f\u6a21\u578b\u4e2d\u5e7f\u6cdb\u4f7f\u7528\u7684\u5f00\u59cb\u3002\u672c\u6587\u63d0\u5230\u7684Transformer\u4ee5\u53ca\u5bf9\u5e94\u7684\u5c42\u90fd\u5df2\u7ecf\u6210\u4e3a\u4e86Pytorch\u7684\u6807\u51c6\u5c42. attention \u57fa\u7840\u56de\u987e \u8fd9\u91cc\u5f15\u7528\u51e0\u5f20Udaicty\u7eb3\u7c73\u5b66\u4f4d\u7684\u8bfe\u7a0b\u622a\u56fe \u9996\u5148Attention network\u7684Embedding\u5c06\u65f6\u5e8f\u7684\u6bcf\u4e00\u4e2a\u8f93\u5165(\u5355\u8bcd)\uff0c\u7ffb\u8bd1\u6210\u4e00\u4e2a\u4e2a\u5bf9\u5e94\u7684key vector K . \u7528\u672c\u8bba\u6587\u4e2d\u7684\u7528\u8bcd\u6765\u63cf\u8ff0\uff0c\u5c31\u662f\u8bf4RNN\u7684\u9690\u85cf\u5c42\u662fQuery Q , RNN\u89e3\u7801\u8fc7\u7a0b\u4e2d\u7684\u8f93\u5165\u4e3a value vector V Transformer \u7cfb\u7edf\u603b\u89c8 \u5728\"RNN\"\u7684\u6bcf\u4e00\u5c42\u7684\u7ed3\u6784\u5982\u56fe,\u5728embedding\u4e4b\u540e,\u8f93\u5165\u4e0e\u8f93\u51fa\u5206\u522b\u8f93\u5165\u5230\u4e00\u4e2amulti-head self attention\u6a21\u5757\u91cc\u9762\uff0c\u76f8\u5f53\u4e8e\u5728\u8f93\u5165\u8f93\u51fa\u5185\u90e8\u81ea\u5df1\u5148\u505a\u4e00\u6b21attention,\u7136\u540e\u8f93\u5165\u7ecf\u8fc7\u4e00\u4e2a\u5168\u8fde\u63a5\u524d\u9988\u7f51\u7edc\u540e\uff0c\u6210\u4e3a\u4e0b\u4e00\u4e2aattention module\u7684 Q \u4e0e K ,\u8f93\u51fa\u90e8\u5206\u7684processed embedding\u76f4\u63a5\u4f5c\u4e3a\u8fd9\u4e2a\u6a21\u5757\u7684 V .\u6700\u540e\u8f93\u51fa\u7ecf\u8fc7\u524d\u9988\u7f51\u7edc\u5904\u7406\u8f93\u51fa\u3002 \u6ce8\u610f\u672c\u6587\u7cfb\u7edf\u5b9e\u9645\u4e0a\u6ca1\u6709\u4f7f\u7528RNN,\u53ea\u662f\u501f\u7528\u4e86\u4e0a\u6587\u7684notation. Multi-head Attention \u5de6\u4fa7attention\u7684\u516c\u5f0f Attention(Q,K,V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V \u53f3\u4fa7\u7684\u516c\u5f0f \\begin{aligned} MultiHead(Q,K,V)&=Concat(head1,..head_h) W^O \\\\ where head_i &= Attention(QW_i^Q, KW_i^k, VW_i^V) \\end{aligned} \u672c\u6587\u4ee5\u53capytorch\u7684\u5b9e\u73b0\u4e2d\u9009\u62e9\u7684 h=8 \u524d\u9988\u6a21\u5757 FFN(x) = ReLU(W_1x+b_1)W_2 + b_2 Positional Encoding \u7531\u4e8e\u672c\u6587\u6700\u7ec8\u5b8c\u5168\u629b\u5f03RNN, \u8fd9\u91cc\u9700\u8981\u7ed9\u6a21\u578b\u8f93\u5165\u4f4d\u7f6e\u4fe1\u606f\uff0c\u4ee5\u8ba9model\u8ba4\u8bc6\u5230\u8fd9\u662f\u4e00\u4e2a\u5e8f\u5217\u95ee\u9898\u3002 Positional encoding\u7684\u7ef4\u5ea6\u4e3a d_{model} \u4e0eembedding\u4e00\u81f4 \u8fd9\u91cc\u7528\u7684\u516c\u5f0f\u662f \\begin{aligned} PE_{(pos,2i)} &= sin(pos/10000^{2i/d_{model}}) \\\\ PE_{(pos,2i+1)}&=cos(pos/10000^{2i/d_{model}}) \\end{aligned} pos \u6307\u4f4d\u7f6e\uff0c\u800c i \u6307\u7ef4\u5ea6,\u6bcf\u4e00\u7ef4\u5bf9\u5e94\u4e00\u4e2a\u6b63\u5f26,\u8fd9\u4e2a\u77e2\u91cf\u548cembedding\u76f8\u52a0\u3002 Self attention Udacity supplyment","title":"Attention is all you need"},{"location":"Building_Blocks/Attention_is_all_you_need/#attention-is-all-you-need","text":"\u8fd9\u7bc7\u8bba\u6587\u662f\u81ea\u6ce8\u610f\u673a\u5236\u5728\u65f6\u5e8f\u6a21\u578b\u4e2d\u5e7f\u6cdb\u4f7f\u7528\u7684\u5f00\u59cb\u3002\u672c\u6587\u63d0\u5230\u7684Transformer\u4ee5\u53ca\u5bf9\u5e94\u7684\u5c42\u90fd\u5df2\u7ecf\u6210\u4e3a\u4e86Pytorch\u7684\u6807\u51c6\u5c42.","title":"Attention is all you need"},{"location":"Building_Blocks/Attention_is_all_you_need/#attention","text":"\u8fd9\u91cc\u5f15\u7528\u51e0\u5f20Udaicty\u7eb3\u7c73\u5b66\u4f4d\u7684\u8bfe\u7a0b\u622a\u56fe \u9996\u5148Attention network\u7684Embedding\u5c06\u65f6\u5e8f\u7684\u6bcf\u4e00\u4e2a\u8f93\u5165(\u5355\u8bcd)\uff0c\u7ffb\u8bd1\u6210\u4e00\u4e2a\u4e2a\u5bf9\u5e94\u7684key vector K . \u7528\u672c\u8bba\u6587\u4e2d\u7684\u7528\u8bcd\u6765\u63cf\u8ff0\uff0c\u5c31\u662f\u8bf4RNN\u7684\u9690\u85cf\u5c42\u662fQuery Q , RNN\u89e3\u7801\u8fc7\u7a0b\u4e2d\u7684\u8f93\u5165\u4e3a value vector V","title":"attention \u57fa\u7840\u56de\u987e"},{"location":"Building_Blocks/Attention_is_all_you_need/#transformer","text":"\u5728\"RNN\"\u7684\u6bcf\u4e00\u5c42\u7684\u7ed3\u6784\u5982\u56fe,\u5728embedding\u4e4b\u540e,\u8f93\u5165\u4e0e\u8f93\u51fa\u5206\u522b\u8f93\u5165\u5230\u4e00\u4e2amulti-head self attention\u6a21\u5757\u91cc\u9762\uff0c\u76f8\u5f53\u4e8e\u5728\u8f93\u5165\u8f93\u51fa\u5185\u90e8\u81ea\u5df1\u5148\u505a\u4e00\u6b21attention,\u7136\u540e\u8f93\u5165\u7ecf\u8fc7\u4e00\u4e2a\u5168\u8fde\u63a5\u524d\u9988\u7f51\u7edc\u540e\uff0c\u6210\u4e3a\u4e0b\u4e00\u4e2aattention module\u7684 Q \u4e0e K ,\u8f93\u51fa\u90e8\u5206\u7684processed embedding\u76f4\u63a5\u4f5c\u4e3a\u8fd9\u4e2a\u6a21\u5757\u7684 V .\u6700\u540e\u8f93\u51fa\u7ecf\u8fc7\u524d\u9988\u7f51\u7edc\u5904\u7406\u8f93\u51fa\u3002 \u6ce8\u610f\u672c\u6587\u7cfb\u7edf\u5b9e\u9645\u4e0a\u6ca1\u6709\u4f7f\u7528RNN,\u53ea\u662f\u501f\u7528\u4e86\u4e0a\u6587\u7684notation.","title":"Transformer \u7cfb\u7edf\u603b\u89c8"},{"location":"Building_Blocks/Attention_is_all_you_need/#multi-head-attention","text":"\u5de6\u4fa7attention\u7684\u516c\u5f0f Attention(Q,K,V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V \u53f3\u4fa7\u7684\u516c\u5f0f \\begin{aligned} MultiHead(Q,K,V)&=Concat(head1,..head_h) W^O \\\\ where head_i &= Attention(QW_i^Q, KW_i^k, VW_i^V) \\end{aligned} \u672c\u6587\u4ee5\u53capytorch\u7684\u5b9e\u73b0\u4e2d\u9009\u62e9\u7684 h=8","title":"Multi-head Attention"},{"location":"Building_Blocks/Attention_is_all_you_need/#_1","text":"FFN(x) = ReLU(W_1x+b_1)W_2 + b_2","title":"\u524d\u9988\u6a21\u5757"},{"location":"Building_Blocks/Attention_is_all_you_need/#positional-encoding","text":"\u7531\u4e8e\u672c\u6587\u6700\u7ec8\u5b8c\u5168\u629b\u5f03RNN, \u8fd9\u91cc\u9700\u8981\u7ed9\u6a21\u578b\u8f93\u5165\u4f4d\u7f6e\u4fe1\u606f\uff0c\u4ee5\u8ba9model\u8ba4\u8bc6\u5230\u8fd9\u662f\u4e00\u4e2a\u5e8f\u5217\u95ee\u9898\u3002 Positional encoding\u7684\u7ef4\u5ea6\u4e3a d_{model} \u4e0eembedding\u4e00\u81f4 \u8fd9\u91cc\u7528\u7684\u516c\u5f0f\u662f \\begin{aligned} PE_{(pos,2i)} &= sin(pos/10000^{2i/d_{model}}) \\\\ PE_{(pos,2i+1)}&=cos(pos/10000^{2i/d_{model}}) \\end{aligned} pos \u6307\u4f4d\u7f6e\uff0c\u800c i \u6307\u7ef4\u5ea6,\u6bcf\u4e00\u7ef4\u5bf9\u5e94\u4e00\u4e2a\u6b63\u5f26,\u8fd9\u4e2a\u77e2\u91cf\u548cembedding\u76f8\u52a0\u3002","title":"Positional Encoding"},{"location":"Building_Blocks/Attention_is_all_you_need/#self-attention-udacity-supplyment","text":"","title":"Self attention Udacity supplyment"},{"location":"Building_Blocks/Bpnp/","text":"End-to-End Learnable Geometric Vision by Backpropagating PnP Optimization \u8fd9\u7bc7\u8bba\u6587\u7684\u6838\u5fc3\u8d21\u732e\u662f\u5c06PnP \u4f18\u5316\u8fc7\u7a0b\u53d8\u4e3a\u4e00\u4e2a\u53ef\u5bfc\u7684\u90e8\u4ef6\u63d2\u5165\u5230\u7f51\u7edc\u4e4b\u4e2d\uff0c\u5bf9\u4e8e\u66f4\u591a\u7aef\u5230\u7aef\u7684\u6a21\u578b\u6709\u5f88\u597d\u7684\u5e2e\u52a9\u3002 \u9690\u51fd\u6570\u5b9a\u7406 Implicit function theorem \u5bf9\u4e8e f(a^*, b^*) = 0, \u4e14 b^* = g(a^*) \u82e5\u5176\u4e2d\u7684\u96c5\u514b\u6bd4\u77e9\u9635\u53ef\u5bfc\uff0c\u6709 \\frac{\\partial g}{\\partial x_{j}}(\\mathbf{x})=-\\left[J_{f, \\mathbf{y}}(\\mathbf{x}, g(\\mathbf{x}))\\right]_{m \\times m}^{-1}\\left[\\frac{\\partial f}{\\partial x_{j}}(\\mathbf{x}, g(\\mathbf{x}))\\right]_{m \\times 1} PnP\u95ee\u9898\uff0c\u4f5c\u8005\u9009\u62e9\u7684\u63cf\u8ff0\u662f\uff0c\u5bfb\u627e6 DoF\u4f4d\u59ff\uff0c\u4f7f\u5f973D\u70b9\u57282D\u5e73\u9762\u4e0a\u7684\u6295\u5f71\u7684\u5bf9\u5e94\u70b9\u4e2a\u70b9\u5728\u56fe\u7247\u4e2d\u7684\u8bef\u5dee\u5e73\u65b9\u548c\u6700\u5c0f\uff0c \u8fd9\u91cc\u4f5c\u8005\u9009\u62e9 f \u4e3a\u8bef\u5dee\u51fd\u6570\u5173\u4e8e\u8f93\u51fa\u7684\u5bfc\u6570 \\frac{\\partial f}{\\partial y} \uff0c\u5728pnp\u4f18\u5316\u597d\u540e\uff0c\u5176\u4e2d\u8fd9\u4e2a\u5bfc\u6570\u5e94\u4e3a\u96f6\u3002 \u5177\u4f53\u8ba1\u7b97\u89c1\u4ee3\u7801\u4e0e\u8bba\u6587 \u5176\u4e2dforward\u51fd\u6570opencv\u5b8c\u6210\uff0cbackward\u51fd\u6570\u8be6\u89c1\u4ee3\u7801\u3002\u6574\u4e2a\u51fd\u6570\u53ef\u4ee5\u4f5c\u4e3a\u4e00\u4e2a\u5373\u63d2\u5373\u7528\u7684\u90e8\u4ef6\u3002\u4e14\u4f5c\u8005\u6709\u8bad\u7ec3\u7684\u4e00\u4e2asample\u811a\u672c\u3002 \u4f5c\u8005\u5728paper\u7b2c\u4e94\u7ae0\u8282\u7ed9\u51fa\u4e86\u4e00\u4e2aobject pose estimation\u7684\u4f8b\u5b50\u3002","title":"End-to-End Learnable Geometric Vision by Backpropagating PnP Optimization"},{"location":"Building_Blocks/Bpnp/#end-to-end-learnable-geometric-vision-by-backpropagating-pnp-optimization","text":"\u8fd9\u7bc7\u8bba\u6587\u7684\u6838\u5fc3\u8d21\u732e\u662f\u5c06PnP \u4f18\u5316\u8fc7\u7a0b\u53d8\u4e3a\u4e00\u4e2a\u53ef\u5bfc\u7684\u90e8\u4ef6\u63d2\u5165\u5230\u7f51\u7edc\u4e4b\u4e2d\uff0c\u5bf9\u4e8e\u66f4\u591a\u7aef\u5230\u7aef\u7684\u6a21\u578b\u6709\u5f88\u597d\u7684\u5e2e\u52a9\u3002","title":"End-to-End Learnable Geometric Vision by Backpropagating PnP Optimization"},{"location":"Building_Blocks/Bpnp/#_1","text":"Implicit function theorem \u5bf9\u4e8e f(a^*, b^*) = 0, \u4e14 b^* = g(a^*) \u82e5\u5176\u4e2d\u7684\u96c5\u514b\u6bd4\u77e9\u9635\u53ef\u5bfc\uff0c\u6709 \\frac{\\partial g}{\\partial x_{j}}(\\mathbf{x})=-\\left[J_{f, \\mathbf{y}}(\\mathbf{x}, g(\\mathbf{x}))\\right]_{m \\times m}^{-1}\\left[\\frac{\\partial f}{\\partial x_{j}}(\\mathbf{x}, g(\\mathbf{x}))\\right]_{m \\times 1} PnP\u95ee\u9898\uff0c\u4f5c\u8005\u9009\u62e9\u7684\u63cf\u8ff0\u662f\uff0c\u5bfb\u627e6 DoF\u4f4d\u59ff\uff0c\u4f7f\u5f973D\u70b9\u57282D\u5e73\u9762\u4e0a\u7684\u6295\u5f71\u7684\u5bf9\u5e94\u70b9\u4e2a\u70b9\u5728\u56fe\u7247\u4e2d\u7684\u8bef\u5dee\u5e73\u65b9\u548c\u6700\u5c0f\uff0c \u8fd9\u91cc\u4f5c\u8005\u9009\u62e9 f \u4e3a\u8bef\u5dee\u51fd\u6570\u5173\u4e8e\u8f93\u51fa\u7684\u5bfc\u6570 \\frac{\\partial f}{\\partial y} \uff0c\u5728pnp\u4f18\u5316\u597d\u540e\uff0c\u5176\u4e2d\u8fd9\u4e2a\u5bfc\u6570\u5e94\u4e3a\u96f6\u3002 \u5177\u4f53\u8ba1\u7b97\u89c1\u4ee3\u7801\u4e0e\u8bba\u6587 \u5176\u4e2dforward\u51fd\u6570opencv\u5b8c\u6210\uff0cbackward\u51fd\u6570\u8be6\u89c1\u4ee3\u7801\u3002\u6574\u4e2a\u51fd\u6570\u53ef\u4ee5\u4f5c\u4e3a\u4e00\u4e2a\u5373\u63d2\u5373\u7528\u7684\u90e8\u4ef6\u3002\u4e14\u4f5c\u8005\u6709\u8bad\u7ec3\u7684\u4e00\u4e2asample\u811a\u672c\u3002 \u4f5c\u8005\u5728paper\u7b2c\u4e94\u7ae0\u8282\u7ed9\u51fa\u4e86\u4e00\u4e2aobject pose estimation\u7684\u4f8b\u5b50\u3002","title":"\u9690\u51fd\u6570\u5b9a\u7406"},{"location":"Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/","text":"CBAM: Convolutional Block Attention Module","title":"CBAM: Convolutional Block Attention Module"},{"location":"Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/#cbam-convolutional-block-attention-module","text":"","title":"CBAM: Convolutional Block Attention Module"},{"location":"Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/","text":"Can GCNs Go as Deep as CNNs \u8fd9\u7bc7\u8bba\u6587\u662f\u8fd1\u671f\u56fe\u5377\u79ef\u7f51\u7edc\u7684\u91cd\u8981\u7a81\u7834\u4e4b\u4e00\u3002\u610f\u4e49\u5728\u4e8e\u5bf9\u57fa\u672c\u5377\u79ef\u7684\u4e00\u4e2a\u590d\u5236\uff0c\u5c06\u57fa\u7840\u5377\u79ef\u76f8\u5173\u7684\u8fde\u63a5\u7b97\u6cd5\u79fb\u690d\u5230\u56fe\u5377\u79ef\u4e2d\u3002 \u56fe\u5377\u79ef\u7f51\u7edc\u7684Res\u8fde\u63a5\u4ee5\u53caDense\u8fde\u63a5 \u4e0e\u57fa\u7840\u7684ResNet\u548cDenseNet\u4e00\u81f4\uff0cRes\u4ee3\u8868\u5c42\u8f93\u5165\u4e0e\u5c42\u8f93\u51fa\u76f8\u52a0\u3002Dense\u5219\u662f\u591a\u5c42\u8f93\u5165\u3001\u8f93\u51fa\u7684Concatenation\u3002 \u56fe\u5377\u79ef\u7f51\u7edc\u7684Dilated Convolution \u672c\u6587\u5377\u79ef\u91c7\u53d6\u7684\u65b9\u6848\u662fK-nearest-neighbor\u7684\u65b9\u6848\uff0c\u5bfb\u627e\u4e0e\u5f53\u524d\u70b9\u8ddd\u79bb\u6700\u8fd1\u7684k\u4e2a\u70b9\u8fdb\u884c\u5377\u79ef\uff0c\u82e5\u6269\u5c55\u4e3adilatedConv\uff0c\u5219\u8ddd\u79bb\u6700\u8fd1\u7684\u70b9\u8fdb\u884c\u6392\u5e8f\uff0c\u6bcf\u9694\u51e0\u4e2a\u70b9\u627e\u51fa\u4e00\u4e2a\u5377\u79ef\u70b9\u3002 \u4ece\u5b9e\u65f6\u8fd0\u7b97\u7684\u89d2\u5ea6\u6765\u770b\uff0c\u9700\u8981\u5efa\u7acb\u4e00\u4e2aKDtree\uff0c \u5bf9\u4e8e\u6bcf\u4e00\u5c42\u7684\u7f51\u7edc\u90fd\u8981\u5bf9\u6bcf\u4e00\u4e2a\u70b9\u8fdb\u884cKNN\u7684\u641c\u7d22\u3002\u5b9e\u65f6\u8fd0\u7b97\u901f\u7387\u4e00\u822c\uff0c\u4f46\u662f\u80fd\u5f97\u5230\u66f4\u597d\u7684\u7ed3\u679c\u3002","title":"Can GCNs Go as Deep as CNNs"},{"location":"Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/#can-gcns-go-as-deep-as-cnns","text":"\u8fd9\u7bc7\u8bba\u6587\u662f\u8fd1\u671f\u56fe\u5377\u79ef\u7f51\u7edc\u7684\u91cd\u8981\u7a81\u7834\u4e4b\u4e00\u3002\u610f\u4e49\u5728\u4e8e\u5bf9\u57fa\u672c\u5377\u79ef\u7684\u4e00\u4e2a\u590d\u5236\uff0c\u5c06\u57fa\u7840\u5377\u79ef\u76f8\u5173\u7684\u8fde\u63a5\u7b97\u6cd5\u79fb\u690d\u5230\u56fe\u5377\u79ef\u4e2d\u3002","title":"Can GCNs Go as Deep as CNNs"},{"location":"Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/#resdense","text":"\u4e0e\u57fa\u7840\u7684ResNet\u548cDenseNet\u4e00\u81f4\uff0cRes\u4ee3\u8868\u5c42\u8f93\u5165\u4e0e\u5c42\u8f93\u51fa\u76f8\u52a0\u3002Dense\u5219\u662f\u591a\u5c42\u8f93\u5165\u3001\u8f93\u51fa\u7684Concatenation\u3002","title":"\u56fe\u5377\u79ef\u7f51\u7edc\u7684Res\u8fde\u63a5\u4ee5\u53caDense\u8fde\u63a5"},{"location":"Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/#dilated-convolution","text":"\u672c\u6587\u5377\u79ef\u91c7\u53d6\u7684\u65b9\u6848\u662fK-nearest-neighbor\u7684\u65b9\u6848\uff0c\u5bfb\u627e\u4e0e\u5f53\u524d\u70b9\u8ddd\u79bb\u6700\u8fd1\u7684k\u4e2a\u70b9\u8fdb\u884c\u5377\u79ef\uff0c\u82e5\u6269\u5c55\u4e3adilatedConv\uff0c\u5219\u8ddd\u79bb\u6700\u8fd1\u7684\u70b9\u8fdb\u884c\u6392\u5e8f\uff0c\u6bcf\u9694\u51e0\u4e2a\u70b9\u627e\u51fa\u4e00\u4e2a\u5377\u79ef\u70b9\u3002 \u4ece\u5b9e\u65f6\u8fd0\u7b97\u7684\u89d2\u5ea6\u6765\u770b\uff0c\u9700\u8981\u5efa\u7acb\u4e00\u4e2aKDtree\uff0c \u5bf9\u4e8e\u6bcf\u4e00\u5c42\u7684\u7f51\u7edc\u90fd\u8981\u5bf9\u6bcf\u4e00\u4e2a\u70b9\u8fdb\u884cKNN\u7684\u641c\u7d22\u3002\u5b9e\u65f6\u8fd0\u7b97\u901f\u7387\u4e00\u822c\uff0c\u4f46\u662f\u80fd\u5f97\u5230\u66f4\u597d\u7684\u7ed3\u679c\u3002","title":"\u56fe\u5377\u79ef\u7f51\u7edc\u7684Dilated Convolution"},{"location":"Building_Blocks/Container_context_aggregation_network/","text":"Container: Context Aggregation Network \u8fd9\u7bc7paper\u4ece\u4e34\u8fd1\u77e9\u9635\u7684\u5f62\u6210\u65b9\u5f0f\u7684\u89d2\u5ea6\uff0c\u5c06MLP Mixer, CNN\u4ee5\u53catransformer\u7edf\u4e00\u8d77\u6765. Method: \u4e00\u4e2a\u6b8b\u5dee\u7f51\u7edc\u5c42\u7684\u8ba1\u7b97\u8868\u8fbe\u53ef\u4ee5\u62bd\u8c61\u5730\u8868\u8fbe\u4e3a Y = \\mathcal{F}(X, \\{W_i\\}) + X \u5176\u4e2d W_i \u662f\u53ef\u5b66\u4e60\u7684\u53c2\u6570. \u5176\u4e2d\uff0c\u5b9a\u4e49\u5173\u8054\u77e9\u9635 \\mathcal{A} \\in \\mathbb{R}^{N \\times N} , \u4ee3\u8868\u90bb\u57df\u7684\u5173\u6ce8\uff0c\u90a3\u4e48\u7f51\u7edc\u5c42\u53ef\u4ee5\u8868\u8fbe\u4e3a Y = (\\mathcal{A}V)W_1 + X \u5176\u4e2d V\\in\\mathbb{R}^{N\\times C} = XW_2 \u662fX\u7684\u4e00\u4e2a\u7ebf\u6027\u6295\u5f71.\u901a\u8fc7\u5f15\u5165\u4e0d\u540c\u7684\u5173\u8054\u77e9\u9635\uff0c \u8fd9\u4e2a\u6a21\u5757\u7684\u62df\u5408capacity\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63d0\u5347\uff0c\u5176\u4e2d\u53ef\u4ee5\u91c7\u7528multi-head\u7684\u7248\u672c Y = \\text{Concat}(\\mathcal{A}_1V_1, ..., \\mathcal{A}_MV_M)W_2 + X Typical instance of the Context Aggregation Module Transformer : A_m^{sa} = \\text{Softmax}(Q_mK^T_m / \\sqrt{C/M}) Depthwise Convolution : \\mathcal{A}_{m i j}^{\\text {conv }}=\\left\\{\\begin{array}{cl} \\operatorname{Ker}[m, 0,|i-j|] & |i-j| \\leq k \\\\ 0 & |i-j|>k \\end{array}\\right. \u8fd9\u4e2a\u77e9\u9635\u7684\u5f62\u6001\u662f\u9759\u6001\u7684\uff0c\u503c\u662f\u53ef\u4ee5\u5b66\u4e60\u7684. MLP-Mixer : \u5176\u8ba1\u7b97\u516c\u5f0f\u4e3a X=X + (V^TW_{MLP})^T , \u5173\u8054\u77e9\u9635\u4e3a A^{mlp} = (W_{MLP})^T \u56e0\u800c\u8fd9\u4e2a\u77e9\u9635\u662f\u5b8c\u5168\u9759\u6001\u7684\u4e14\u5b8c\u5168\u53ef\u5b66\u4e60\u7684\u3002\u4f46\u662f\u5b8c\u5168\u6ca1\u6709\u53c2\u6570\u5171\u4eab\u3002 class Attention(nn.Module): def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0., seq_l=196): super().__init__() self.num_heads = num_heads head_dim = dim // num_heads self.scale = qk_scale or head_dim ** -0.5 self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias) #Uncomment this line for Container-PAM #self.static_a = nn.Parameter(torch.Tensor(1, num_heads, 1 + seq_l , 1 + seq_l)) #trunc_normal_(self.static_a) self.attn_drop = nn.Dropout(attn_drop) self.proj = nn.Linear(dim, dim) self.proj_drop = nn.Dropout(proj_drop) def forward(self, x): B, N, C = x.shape qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads). permute(2, 0, 3, 1, 4) q, k, v = qkv[0], qkv[1], qkv[2] attn = (q @ k.transpose(-2, -1)) * self.scale attn = attn.softmax(dim=-1) #Uncomment this line for Container-PAM #attn = attn + self.static_a attn = self.attn_drop(attn) x = (attn @ v).transpose(1, 2).reshape(B, N, C) x = self.proj(x) x = self.proj_drop(x) return x","title":"Container: Context Aggregation Network"},{"location":"Building_Blocks/Container_context_aggregation_network/#container-context-aggregation-network","text":"\u8fd9\u7bc7paper\u4ece\u4e34\u8fd1\u77e9\u9635\u7684\u5f62\u6210\u65b9\u5f0f\u7684\u89d2\u5ea6\uff0c\u5c06MLP Mixer, CNN\u4ee5\u53catransformer\u7edf\u4e00\u8d77\u6765.","title":"Container: Context Aggregation Network"},{"location":"Building_Blocks/Container_context_aggregation_network/#method","text":"\u4e00\u4e2a\u6b8b\u5dee\u7f51\u7edc\u5c42\u7684\u8ba1\u7b97\u8868\u8fbe\u53ef\u4ee5\u62bd\u8c61\u5730\u8868\u8fbe\u4e3a Y = \\mathcal{F}(X, \\{W_i\\}) + X \u5176\u4e2d W_i \u662f\u53ef\u5b66\u4e60\u7684\u53c2\u6570. \u5176\u4e2d\uff0c\u5b9a\u4e49\u5173\u8054\u77e9\u9635 \\mathcal{A} \\in \\mathbb{R}^{N \\times N} , \u4ee3\u8868\u90bb\u57df\u7684\u5173\u6ce8\uff0c\u90a3\u4e48\u7f51\u7edc\u5c42\u53ef\u4ee5\u8868\u8fbe\u4e3a Y = (\\mathcal{A}V)W_1 + X \u5176\u4e2d V\\in\\mathbb{R}^{N\\times C} = XW_2 \u662fX\u7684\u4e00\u4e2a\u7ebf\u6027\u6295\u5f71.\u901a\u8fc7\u5f15\u5165\u4e0d\u540c\u7684\u5173\u8054\u77e9\u9635\uff0c \u8fd9\u4e2a\u6a21\u5757\u7684\u62df\u5408capacity\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63d0\u5347\uff0c\u5176\u4e2d\u53ef\u4ee5\u91c7\u7528multi-head\u7684\u7248\u672c Y = \\text{Concat}(\\mathcal{A}_1V_1, ..., \\mathcal{A}_MV_M)W_2 + X","title":"Method:"},{"location":"Building_Blocks/Container_context_aggregation_network/#typical-instance-of-the-context-aggregation-module","text":"Transformer : A_m^{sa} = \\text{Softmax}(Q_mK^T_m / \\sqrt{C/M}) Depthwise Convolution : \\mathcal{A}_{m i j}^{\\text {conv }}=\\left\\{\\begin{array}{cl} \\operatorname{Ker}[m, 0,|i-j|] & |i-j| \\leq k \\\\ 0 & |i-j|>k \\end{array}\\right. \u8fd9\u4e2a\u77e9\u9635\u7684\u5f62\u6001\u662f\u9759\u6001\u7684\uff0c\u503c\u662f\u53ef\u4ee5\u5b66\u4e60\u7684. MLP-Mixer : \u5176\u8ba1\u7b97\u516c\u5f0f\u4e3a X=X + (V^TW_{MLP})^T , \u5173\u8054\u77e9\u9635\u4e3a A^{mlp} = (W_{MLP})^T \u56e0\u800c\u8fd9\u4e2a\u77e9\u9635\u662f\u5b8c\u5168\u9759\u6001\u7684\u4e14\u5b8c\u5168\u53ef\u5b66\u4e60\u7684\u3002\u4f46\u662f\u5b8c\u5168\u6ca1\u6709\u53c2\u6570\u5171\u4eab\u3002 class Attention(nn.Module): def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0., seq_l=196): super().__init__() self.num_heads = num_heads head_dim = dim // num_heads self.scale = qk_scale or head_dim ** -0.5 self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias) #Uncomment this line for Container-PAM #self.static_a = nn.Parameter(torch.Tensor(1, num_heads, 1 + seq_l , 1 + seq_l)) #trunc_normal_(self.static_a) self.attn_drop = nn.Dropout(attn_drop) self.proj = nn.Linear(dim, dim) self.proj_drop = nn.Dropout(proj_drop) def forward(self, x): B, N, C = x.shape qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads). permute(2, 0, 3, 1, 4) q, k, v = qkv[0], qkv[1], qkv[2] attn = (q @ k.transpose(-2, -1)) * self.scale attn = attn.softmax(dim=-1) #Uncomment this line for Container-PAM #attn = attn + self.static_a attn = self.attn_drop(attn) x = (attn @ v).transpose(1, 2).reshape(B, N, C) x = self.proj(x) x = self.proj_drop(x) return x","title":"Typical instance of the Context Aggregation Module"},{"location":"Building_Blocks/DGCNN/","text":"Dynamic Graph CNN for Learning on Point Clouds \u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86edgeConv\u4f5c\u4e3a\u65b0\u7684pointNet\u64cd\u4f5c\u7b26,\u5b83\u7684\u7279\u70b9\u5728\u4e8e\u8003\u8651\u4e86\u70b9\u7684\u5750\u6807\u4ee5\u53ca\u76f8\u90bb\u70b9\u7684\u8ddd\u79bb\u3002\u8fd9\u91cc\u63a8\u8350 \u4e2d\u6587\u8bba\u6587\u7b14\u8bb0 EdgeConv \u76f4\u89c9\u4ecb\u7ecd\uff0c\u7531\u4e8e\u6bcf\u4e00\u4e2a\u70b9\u7684\u7279\u5f81\u7ecf\u5e38\u5305\u542b\u5750\u6807\u3001\u989c\u8272\u7684\u4fe1\u606f\uff0c\u672c\u6587\u7684\u7b97\u6cd5\u662f\u663e\u5f0f\u5730\u8ba9\u76f8\u90bb\u7684\u70b9\u76f4\u63a5\u76f8\u4e92\u878d\u5408\u8fd0\u7b97\uff0c\u8fd9\u6837\u65b9\u4fbf\u5f97\u5230\u4e24\u8005\u7684\u5dee\u6216\u8005\u4e24\u8005\u7684\u5747\u503c\uff0c\u8fd9\u4e5f\u662f\u672c\u6587edgeConv\u7684\u540d\u5b57\u6765\u6e90\u3002 \u5b9a\u4e49\u8fb9\u7f18\u7279\u5f81 e_{ij} = h_{\\Theta}(x_i,x_j) \u5176\u4e2d h_{\\Theta} \u4e3a\u53ef\u5b66\u4e60\u7684\u975e\u7ebf\u6027\u51fd\u6570\uff0c\u8fd9\u4f1a\u8f93\u51fa F' \u4e2a\u7279\u5f81\uff0c\u5f53\u524d\u70b9\u7684\u8f93\u51fa\u4e3a\u9644\u8fd1\u591a\u4e2a\u76f8\u90bb\u8fb9\u7f18\u503c\u7684\u7efc\u5408\uff0c\u53ef\u4ee5\u662f\u6c42\u548c\u4e5f\u53ef\u4ee5\u662f\u6c42\u6700\u5927\u503c\u3002 \u5bf9 h_{\\Theta} \u7684\u5b9a\u4e49\u4e5f\u662f\u591a\u6837\u5316\u7684\uff0c1.\u5bf9\u5355\u4e2a\u5bf9\u65b9\u70b9\u7684\u52a0\u6743\u6c42\u548c;2.\u5bf9\u672c\u5730\u70b9\u7279\u5f81\u7684\u52a0\u6743\u6c42\u548c(\u4e0epointNet\u4e00\u81f4);3. h_{\\Theta}(x_i,x_j) = h_\\Theta(x_j - x_i) \u8fd9\u6837\u5c31\u53ea\u4f1a\u6709\u5c40\u90e8\u4fe1\u606f\u5dee\u503c\u800c\u7f3a\u5c11\u5168\u5c40\u4fe1\u606f(\u5f53\u524d\u70b9\u7684\u5168\u5c40\u4f4d\u7f6e);4. h_\\Theta(x_i,x_j) = h_\\Theta(x_i, x_j - x_i) \u4ece\u800c\u7efc\u5408\u5168\u5c40\u4e0e\u5c40\u90e8\u4fe1\u606f\uff0c\u4e5f\u662f\u4e3b\u6d41\u9009\u62e9\u3002(\u539f\u8bba\u6587\u8fd8\u63d0\u5230\u4e00\u79cd\u4f7f\u7528\u9ad8\u65af\u6838\u7684\u65b9\u6cd5\uff0c\u8fd9\u91cc\u7701\u7565) \u7efc\u5408\u4e0d\u540c\u8fb9\u7f18\u7684\u8fd0\u7b97\u7b26\u9009\u62e9\u7684\u662fmax\uff0c\u8fd9\u6837\u6574\u4e2a\u8ba1\u7b97\u53ef\u4ee5\u7528\u4e00\u4e2a\u5171\u4eab\u53c2\u6570\u7684MLP\u5b9e\u73b0\u3002 \u52a8\u6001\u91cd\u7ec4 \u4e00\u4e9b\u4f20\u7edf\u7684pointnet\u4f1a\u5c06\u70b9\u7684\u4f4d\u7f6e\u56fa\u5b9a\u4e0b\u6765\uff0c\u6bcf\u6b21\u7684\u5377\u79ef\u90fd\u53ea\u5728\u70b9\u539f\u6765\u7684\u76f8\u90bb\u8fb9\u4e2d\u6311\u9009\u3002\u672c\u6587\u7684\u601d\u8def\u662f\u6bcf\u5c42\u8fd0\u7b97\u540e\uff0c\u7528\u65b0\u7684\u7279\u5f81\u4f5c\u4e3afeature vector\uff0c\u5728\u6574\u4e2a\u70b9\u4e91\u4e2d\u91cd\u65b0\u8fdb\u884cKNN\u8fd0\u7b97\u627e\u5728\u65b0\u7279\u5f81\u6761\u4ef6\u4e0b\u6700\u9760\u8fd1\u7684\u70b9\u3002\u5982\u6b64\u4ee5\u6765\u6bcf\u4e00\u5c42\u8fd0\u7b97\u540e\u70b9\u7684\u90bb\u57df\u90fd\u4f1a\u88ab\u6d17\u4e71\u3002 \u7f51\u7edc\u603b\u4f53\u7ed3\u6784 \u4ee5PointNet\u4e3a\u57fa\u7840\u3002\u5177\u4f53\u53ef\u4ee5\u89c2\u5bdf\u4ee3\u7801\u4ee5\u53ca\u56fe\u7247\u89e3\u91ca\u3002","title":"Dynamic Graph CNN for Learning on Point Clouds"},{"location":"Building_Blocks/DGCNN/#dynamic-graph-cnn-for-learning-on-point-clouds","text":"\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86edgeConv\u4f5c\u4e3a\u65b0\u7684pointNet\u64cd\u4f5c\u7b26,\u5b83\u7684\u7279\u70b9\u5728\u4e8e\u8003\u8651\u4e86\u70b9\u7684\u5750\u6807\u4ee5\u53ca\u76f8\u90bb\u70b9\u7684\u8ddd\u79bb\u3002\u8fd9\u91cc\u63a8\u8350 \u4e2d\u6587\u8bba\u6587\u7b14\u8bb0","title":"Dynamic Graph CNN for Learning on Point Clouds"},{"location":"Building_Blocks/DGCNN/#edgeconv","text":"\u76f4\u89c9\u4ecb\u7ecd\uff0c\u7531\u4e8e\u6bcf\u4e00\u4e2a\u70b9\u7684\u7279\u5f81\u7ecf\u5e38\u5305\u542b\u5750\u6807\u3001\u989c\u8272\u7684\u4fe1\u606f\uff0c\u672c\u6587\u7684\u7b97\u6cd5\u662f\u663e\u5f0f\u5730\u8ba9\u76f8\u90bb\u7684\u70b9\u76f4\u63a5\u76f8\u4e92\u878d\u5408\u8fd0\u7b97\uff0c\u8fd9\u6837\u65b9\u4fbf\u5f97\u5230\u4e24\u8005\u7684\u5dee\u6216\u8005\u4e24\u8005\u7684\u5747\u503c\uff0c\u8fd9\u4e5f\u662f\u672c\u6587edgeConv\u7684\u540d\u5b57\u6765\u6e90\u3002 \u5b9a\u4e49\u8fb9\u7f18\u7279\u5f81 e_{ij} = h_{\\Theta}(x_i,x_j) \u5176\u4e2d h_{\\Theta} \u4e3a\u53ef\u5b66\u4e60\u7684\u975e\u7ebf\u6027\u51fd\u6570\uff0c\u8fd9\u4f1a\u8f93\u51fa F' \u4e2a\u7279\u5f81\uff0c\u5f53\u524d\u70b9\u7684\u8f93\u51fa\u4e3a\u9644\u8fd1\u591a\u4e2a\u76f8\u90bb\u8fb9\u7f18\u503c\u7684\u7efc\u5408\uff0c\u53ef\u4ee5\u662f\u6c42\u548c\u4e5f\u53ef\u4ee5\u662f\u6c42\u6700\u5927\u503c\u3002 \u5bf9 h_{\\Theta} \u7684\u5b9a\u4e49\u4e5f\u662f\u591a\u6837\u5316\u7684\uff0c1.\u5bf9\u5355\u4e2a\u5bf9\u65b9\u70b9\u7684\u52a0\u6743\u6c42\u548c;2.\u5bf9\u672c\u5730\u70b9\u7279\u5f81\u7684\u52a0\u6743\u6c42\u548c(\u4e0epointNet\u4e00\u81f4);3. h_{\\Theta}(x_i,x_j) = h_\\Theta(x_j - x_i) \u8fd9\u6837\u5c31\u53ea\u4f1a\u6709\u5c40\u90e8\u4fe1\u606f\u5dee\u503c\u800c\u7f3a\u5c11\u5168\u5c40\u4fe1\u606f(\u5f53\u524d\u70b9\u7684\u5168\u5c40\u4f4d\u7f6e);4. h_\\Theta(x_i,x_j) = h_\\Theta(x_i, x_j - x_i) \u4ece\u800c\u7efc\u5408\u5168\u5c40\u4e0e\u5c40\u90e8\u4fe1\u606f\uff0c\u4e5f\u662f\u4e3b\u6d41\u9009\u62e9\u3002(\u539f\u8bba\u6587\u8fd8\u63d0\u5230\u4e00\u79cd\u4f7f\u7528\u9ad8\u65af\u6838\u7684\u65b9\u6cd5\uff0c\u8fd9\u91cc\u7701\u7565) \u7efc\u5408\u4e0d\u540c\u8fb9\u7f18\u7684\u8fd0\u7b97\u7b26\u9009\u62e9\u7684\u662fmax\uff0c\u8fd9\u6837\u6574\u4e2a\u8ba1\u7b97\u53ef\u4ee5\u7528\u4e00\u4e2a\u5171\u4eab\u53c2\u6570\u7684MLP\u5b9e\u73b0\u3002","title":"EdgeConv"},{"location":"Building_Blocks/DGCNN/#_1","text":"\u4e00\u4e9b\u4f20\u7edf\u7684pointnet\u4f1a\u5c06\u70b9\u7684\u4f4d\u7f6e\u56fa\u5b9a\u4e0b\u6765\uff0c\u6bcf\u6b21\u7684\u5377\u79ef\u90fd\u53ea\u5728\u70b9\u539f\u6765\u7684\u76f8\u90bb\u8fb9\u4e2d\u6311\u9009\u3002\u672c\u6587\u7684\u601d\u8def\u662f\u6bcf\u5c42\u8fd0\u7b97\u540e\uff0c\u7528\u65b0\u7684\u7279\u5f81\u4f5c\u4e3afeature vector\uff0c\u5728\u6574\u4e2a\u70b9\u4e91\u4e2d\u91cd\u65b0\u8fdb\u884cKNN\u8fd0\u7b97\u627e\u5728\u65b0\u7279\u5f81\u6761\u4ef6\u4e0b\u6700\u9760\u8fd1\u7684\u70b9\u3002\u5982\u6b64\u4ee5\u6765\u6bcf\u4e00\u5c42\u8fd0\u7b97\u540e\u70b9\u7684\u90bb\u57df\u90fd\u4f1a\u88ab\u6d17\u4e71\u3002","title":"\u52a8\u6001\u91cd\u7ec4"},{"location":"Building_Blocks/DGCNN/#_2","text":"\u4ee5PointNet\u4e3a\u57fa\u7840\u3002\u5177\u4f53\u53ef\u4ee5\u89c2\u5bdf\u4ee3\u7801\u4ee5\u53ca\u56fe\u7247\u89e3\u91ca\u3002","title":"\u7f51\u7edc\u603b\u4f53\u7ed3\u6784"},{"location":"Building_Blocks/DO-Conv/","text":"DO-Conv: Depthwise Over-parameterized Convolutional Layer \u4f5c\u8005\u5f15\u5165\u4e86over-parameterized\u7684\u5377\u79ef\u5c42\uff0c\u8fd9\u79cd\u5377\u79ef\u5c42\u7684\u597d\u5904\u662f\u5728training\u7684\u65f6\u5019\u5bb9\u6613\u8bad\u7ec3\uff0c\u5728inference\u7684\u65f6\u5019\u53ef\u4ee5\u5c06\u5bbd\u53c2\u6570\u878d\u5408\uff0c\u5f97\u5230\u5355\u4e2a\u5377\u79ef\u5c42\u8fdb\u884c\u63a8\u7406\u3002 \\begin{aligned} \\mathbb{O} &=(\\mathbb{D}, \\mathbb{W}) \\otimes \\mathbb{P} \\\\ &=\\mathbb{W} *(\\mathbb{D} \\circ \\mathbb{P}) \\quad \\text { (Fig. 3-a, feature composition) } \\\\ &=\\left(\\mathbb{D}^{T} \\circ \\mathbb{W}\\right) * \\mathbb{P}, \\quad \\text { (Fig. 3-b, kernel composition) } \\end{aligned}","title":"DO-Conv: Depthwise Over-parameterized Convolutional Layer"},{"location":"Building_Blocks/DO-Conv/#do-conv-depthwise-over-parameterized-convolutional-layer","text":"\u4f5c\u8005\u5f15\u5165\u4e86over-parameterized\u7684\u5377\u79ef\u5c42\uff0c\u8fd9\u79cd\u5377\u79ef\u5c42\u7684\u597d\u5904\u662f\u5728training\u7684\u65f6\u5019\u5bb9\u6613\u8bad\u7ec3\uff0c\u5728inference\u7684\u65f6\u5019\u53ef\u4ee5\u5c06\u5bbd\u53c2\u6570\u878d\u5408\uff0c\u5f97\u5230\u5355\u4e2a\u5377\u79ef\u5c42\u8fdb\u884c\u63a8\u7406\u3002 \\begin{aligned} \\mathbb{O} &=(\\mathbb{D}, \\mathbb{W}) \\otimes \\mathbb{P} \\\\ &=\\mathbb{W} *(\\mathbb{D} \\circ \\mathbb{P}) \\quad \\text { (Fig. 3-a, feature composition) } \\\\ &=\\left(\\mathbb{D}^{T} \\circ \\mathbb{W}\\right) * \\mathbb{P}, \\quad \\text { (Fig. 3-b, kernel composition) } \\end{aligned}","title":"DO-Conv: Depthwise Over-parameterized Convolutional Layer"},{"location":"Building_Blocks/DiCENet/","text":"DiCENet: Dimension-wise Convolutions for Efficient Networks \u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86DiCE\u6a21\u5757\uff0cDiCE\u6a21\u5757\u662f\u53ef\u4ee5\u7406\u89e3\u4e3a\u662f\u5347\u7ea7\u7248\u7684pointwise convolusion(\u6bcf\u4e2aspatial location\u4e0d\u540c\u5377\u79ef\u6838)\u6216\u8005 depthwise convolution (\u672c\u6587\u4f5c\u8005\u5c06\u5176\u7406\u89e3\u4e3aHeight-wise convolution) \u76ee\u524d\u8fd9\u91cc\u9009\u8bfb\u8fd9\u4e00\u7bc7\u662f\u56e0\u4e3a\u770b\u597d\u5b83\u5728\u5355\u76ee3D\u68c0\u6d4b\u4e2d\u7684\u6f5c\u529b\u3002\u56e0\u4e3a\u5b83\u5728\u591a\u4e2a\u8f74\u4e0a\u5206\u7ec4\u5377\u79ef\uff0c\u4f1a\u5f7b\u5e95\u6253\u7834\u7cfb\u7edf\u7684\u5e73\u79fb\u4e0d\u53d8\u6027\u4ee5\u53ca\u5bf9\u79f0\u6027\u3002 DiCE unit depth-wise convolution\u6307\u4ee3\u7684\u662f\u57fa\u7840\u7684\u5206\u7ec4\u5377\u79ef width-wise convolution\u4e0eheight-wise\u7684\u7b97\u6cd5\u662f\u5c06\u5bf9\u5e94\u7ef4\u5ea6\u65cb\u8f6c\u5230dim=1\u5904(pytorch cnn\u9ed8\u8ba4\u7684\u7279\u5f81\u7ef4\u5ea6)\uff0c\u7136\u540e\u76f4\u63a5\u5206\u7ec4CNN \u518dtranspose\u56de\u53bb\uff0c\u4e09\u4e2a\u5206\u652f\u5f97\u5230\u4e09\u4e2a C H W \u7684\u77e9\u9635 \u5408\u5e76\u7684\u65b9\u6cd5\uff0c\u4f5c\u8005\u4ee3\u7801\u7684\u7b97\u6cd5\u662f\u5148torch.cat,\u7136\u540e\u6267\u884c channel shuffle .\u76f4\u63a5\u7cc5\u5408\u8d77\u6765\u7684\u4ee3\u7801\u662fstack(dim=2) -> reshape \u5408\u5e76\u4e4b\u540e\u4f7f\u7528\u5206\u7ec4\u5377\u79ef\u878d\u5408\u6765\u81ea\u4e0d\u540c\u5206\u652f\u7684\u7279\u5f81\uff0c\u6b64\u540e\u518d\u5206\u652f\uff0c\u6a21\u4eff\u7684\u662f Squeeze_and_excitation .\u5176\u4e2d\u4e00\u4e2a\u5206\u652f\u901a\u8fc7spatial\u5e73\u5747\u4e0eFC\u8fde\u63a5\u5f97\u5230channel wise\u7684\u6743\u91cd\uff0c\u7136\u540e\u505a\u4e00\u4e2a\u5206\u7ec4\u5377\u79ef\u5f97\u5230 Y_S \uff0c\u4e24\u8005\u76f8\u4e58\u8f93\u51fa\u3002 \u4e0d\u540c\u5f62\u72b6\u7684\u8f93\u5165 \u672c\u6587\u8fd9\u91cc\u4f7f\u7528\u7684\u662fpytorch\u7684\u81ea\u9002\u5e94avgpool\u6765\u5177\u4f53\u5b9e\u73b0\u3002","title":"DiCENet: Dimension-wise Convolutions for Efficient Networks"},{"location":"Building_Blocks/DiCENet/#dicenet-dimension-wise-convolutions-for-efficient-networks","text":"\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86DiCE\u6a21\u5757\uff0cDiCE\u6a21\u5757\u662f\u53ef\u4ee5\u7406\u89e3\u4e3a\u662f\u5347\u7ea7\u7248\u7684pointwise convolusion(\u6bcf\u4e2aspatial location\u4e0d\u540c\u5377\u79ef\u6838)\u6216\u8005 depthwise convolution (\u672c\u6587\u4f5c\u8005\u5c06\u5176\u7406\u89e3\u4e3aHeight-wise convolution) \u76ee\u524d\u8fd9\u91cc\u9009\u8bfb\u8fd9\u4e00\u7bc7\u662f\u56e0\u4e3a\u770b\u597d\u5b83\u5728\u5355\u76ee3D\u68c0\u6d4b\u4e2d\u7684\u6f5c\u529b\u3002\u56e0\u4e3a\u5b83\u5728\u591a\u4e2a\u8f74\u4e0a\u5206\u7ec4\u5377\u79ef\uff0c\u4f1a\u5f7b\u5e95\u6253\u7834\u7cfb\u7edf\u7684\u5e73\u79fb\u4e0d\u53d8\u6027\u4ee5\u53ca\u5bf9\u79f0\u6027\u3002","title":"DiCENet: Dimension-wise Convolutions for Efficient Networks"},{"location":"Building_Blocks/DiCENet/#dice-unit","text":"depth-wise convolution\u6307\u4ee3\u7684\u662f\u57fa\u7840\u7684\u5206\u7ec4\u5377\u79ef width-wise convolution\u4e0eheight-wise\u7684\u7b97\u6cd5\u662f\u5c06\u5bf9\u5e94\u7ef4\u5ea6\u65cb\u8f6c\u5230dim=1\u5904(pytorch cnn\u9ed8\u8ba4\u7684\u7279\u5f81\u7ef4\u5ea6)\uff0c\u7136\u540e\u76f4\u63a5\u5206\u7ec4CNN \u518dtranspose\u56de\u53bb\uff0c\u4e09\u4e2a\u5206\u652f\u5f97\u5230\u4e09\u4e2a C H W \u7684\u77e9\u9635 \u5408\u5e76\u7684\u65b9\u6cd5\uff0c\u4f5c\u8005\u4ee3\u7801\u7684\u7b97\u6cd5\u662f\u5148torch.cat,\u7136\u540e\u6267\u884c channel shuffle .\u76f4\u63a5\u7cc5\u5408\u8d77\u6765\u7684\u4ee3\u7801\u662fstack(dim=2) -> reshape \u5408\u5e76\u4e4b\u540e\u4f7f\u7528\u5206\u7ec4\u5377\u79ef\u878d\u5408\u6765\u81ea\u4e0d\u540c\u5206\u652f\u7684\u7279\u5f81\uff0c\u6b64\u540e\u518d\u5206\u652f\uff0c\u6a21\u4eff\u7684\u662f Squeeze_and_excitation .\u5176\u4e2d\u4e00\u4e2a\u5206\u652f\u901a\u8fc7spatial\u5e73\u5747\u4e0eFC\u8fde\u63a5\u5f97\u5230channel wise\u7684\u6743\u91cd\uff0c\u7136\u540e\u505a\u4e00\u4e2a\u5206\u7ec4\u5377\u79ef\u5f97\u5230 Y_S \uff0c\u4e24\u8005\u76f8\u4e58\u8f93\u51fa\u3002","title":"DiCE unit"},{"location":"Building_Blocks/DiCENet/#_1","text":"\u672c\u6587\u8fd9\u91cc\u4f7f\u7528\u7684\u662fpytorch\u7684\u81ea\u9002\u5e94avgpool\u6765\u5177\u4f53\u5b9e\u73b0\u3002","title":"\u4e0d\u540c\u5f62\u72b6\u7684\u8f93\u5165"},{"location":"Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/","text":"Discovery of Latent 3D Keypoints via End-to-end Geometric Reasoning \u8fd9\u7bc7\u8bba\u6587\u5b8c\u6210\u7684\u4efb\u52a1\u662f\uff0c\u5bf9\u5355\u5f20\u56fe\u7247\u8f93\u51fa\u4e00\u7cfb\u5217\u6709\u5e8f\u7684keypoints\uff0c\u8fd9\u4e9bkeypoints\u5728\u4e0d\u540c\u89c6\u89d2\u4e0b\u6709\u76f8\u540c\u7684\u8868\u8fbe\u3002\u8bad\u7ec3\u65f6\u5219\u901a\u8fc7\u540c\u4e00\u7269\u4f53\u4e0d\u540c\u89c6\u89d2\u7684\u4e00\u5bf9\u56fe\u7247\u8fdb\u884c\u8bad\u7ec3\u3002 \u4e3b\u4f53Pipeline \u5df2\u77e5\u7684\u76f8\u5bf9\u521a\u4f53\u8fd0\u52a8\u4f5c\u4e3a\u76d1\u7763\u4fe1\u53f7\u3002inference\u7684\u65f6\u5019\uff0cKeypointNet\u4ece\u5355\u4e2a\u8f93\u5165\u56fe\u4e2d\u8f93\u51fa3D\u5173\u952e\u70b9 \u5177\u4f53\u4ecb\u7ecd \u4e00\u4e2a3D keypoint\u88ab\u5b9a\u4e49\u4e3a\u50cf\u7d20\u5750\u6807\u52a0\u4e0a\u5bf9\u5e94\u7684\u6df1\u5ea6\u503c\u3002\u7f51\u7edc\u8f93\u51fa N\u4e2a\u5206\u652f\uff0c\u9884\u6d4bN\u4e2a\u5173\u952e\u70b9\u3002 \u76ee\u6807\u51fd\u6570\u7531\u4e24\u4e2a\u90e8\u5206\u7ec4\u6210 Multi-view Consistency \u63cf\u8ff0\u4e24\u7ec4\u70b9\u5728ground truth\u8f6c\u6362\u524d\u540e\u70b9\u7684\u8ddd\u79bb \\begin{aligned}[\\hat{u}, \\hat{v}, \\hat{z}, 1]^{\\top} & \\sim \\pi T \\pi^{-1}\\left([u, v, z, 1]^{\\top}\\right) \\\\\\left[\\hat{u}^{\\prime}, \\hat{v}^{\\prime}, \\hat{z}^{\\prime}, 1\\right]^{\\top} & \\sim \\pi T^{-1} \\pi^{-1}\\left(\\left[u^{\\prime}, v^{\\prime}, z^{\\prime}, 1\\right]^{\\top}\\right) \\end{aligned} \\pi\\left([x, y, z, 1]^{\\top}\\right)=\\left[\\frac{f x}{z}, \\frac{f y}{z}, z, 1\\right]^{\\top}=[u, v, z, 1]^{\\top} L_{\\mathrm{con}}=\\frac{1}{2 N} \\sum_{i=1}^{N}\\left\\|\\left[u_{i}, v_{i}, u_{i}^{\\prime}, v_{i}^{\\prime}\\right]^{\\top}-\\left[\\hat{u}_{i}^{\\prime}, \\hat{v}_{i}^{\\prime}, \\hat{u}_{i}, \\hat{v}_{i}\\right]^{\\top}\\right\\|^{2} \u672c\u8d28\u4e0a\u5c31\u662f\u5c06A\u56fe\u4e0a\u7684\u70b9\u8f6c\u6362\u5230B\u56fe\u4e0a\u6c42\u5dee\u503c\uff0c\u518d\u5c06B\u56fe\u4e0a\u7684\u70b9\u8f6c\u6362\u5230A\u56fe\u4e0a\u6c42\u5dee\u503c\uff0c\u6c42\u548c\u3002 \u76f8\u5bf9\u59ff\u6001\u4f30\u8ba1\u3002 \u8fd9\u91cc\u53ea\u8981\u6c42\u5bf9\u4f30\u8ba1\u7684\u65cb\u8f6c\u77e9\u9635\u8ba1\u7b97loss L_{\\mathrm{pose}}=2 \\arcsin \\left(\\frac{1}{2 \\sqrt{2}}\\|\\hat{R}-R\\|_{F}\\right) \u5176\u4e2d\u4f30\u8ba1 \\hat R ,\u4ee4 X \u548c X' \u6307\u4ee3\u4e24\u4e2a\u56fe\u8ba1\u7b97\u51fa\u6765\u7684keypoints\u7684 X \\equiv\\left[X_{1}, \\ldots, X_{N}\\right] and X_{i} \\equiv\\left(\\pi^{-1} p_{i}\\right)[: 3] \\hat{R}=V \\operatorname{diag}\\left(1,1, \\ldots, \\operatorname{det}\\left(V U^{\\top}\\right)\\right) U^{\\top} U, \\Sigma, V^{\\top}=\\operatorname{SVD}\\left(\\tilde{X} \\tilde{X}^{\\prime \\top}\\right) \u8fd9\u4e2a\u65b9\u6cd5\u88ab\u79f0\u4e3a Procrustes problem SVD\u662f\u53ef\u4ee5backprob\u7684\u5b9e\u73b0\u7684(\u540c\u6837\u4f7f\u7528\u53ef\u5fae\u5206SVD\u7684 \u8fd9\u7bc7\u6587\u7ae0 \u4e5f\u63d0\u5230tensorflow\u6709\u53ef\u5fae\u5206SVD) KeypointNet \u7ed3\u6784 \u672c\u6587\u4f7f\u7528\u7684KeypointNet,\u4e00\u4e2a\u91cd\u8981\u7684\u6027\u8d28\u5728\u4e8e\u5e73\u79fb\u7b49\u4ef7\uff0c\u4e5f\u5c31\u662f\u8bf4\u5e73\u79fb\u4e00\u4e2a\u50cf\u7d20\uff0c\u8f93\u51fa\u4f4d\u7f6e\u4e5f\u4f1a\u79fb\u52a8\u4e00\u4e2a\u5355\u4f4d\u3002\u8981\u6c42\u8f93\u51faheatmap g_i(u,v) \u4ee3\u8868\u7b2c i \u4e2akeypoint\u51fa\u73b0\u5728 (u,v) ,\u8981\u6c42 \\sum_{u,v}g_i(u,v)=1 \u4f7f\u7528spatial softmax\u53bb\u5b9e\u73b0\u3002 \u7528\u52a0\u6743\u5e73\u5747\u6c42\u51fa\u5bf9\u5e94keypoint\u7684\u4f4d\u7f6e\u4ee5\u53ca\u6df1\u5ea6 [u_i,v_i]^T = \\sum_{u,v}[u * g_i(u,v), v * g_i(u,v)]^T z_i = \\sum_{u,v}d_i(u,v)g_i(u,v) \u8f85\u52a9training \u5206\u79bbloss:\u5bf9\u8fc7\u4e8e\u9760\u8fd1\u7684keypoints\u7ed9\u4e00\u4e2aloss L_{\\mathrm{sep}}=\\frac{1}{N^{2}} \\sum_{i=1}^{N} \\sum_{j \\neq i}^{N} \\max \\left(0, \\delta^{2}-\\left\\|X_{i}-X_{j}\\right\\|^{2}\\right) \u8f6e\u5ed3\u4e00\u81f4\u6027:\u9f13\u52b1keypoints\u5728\u7269\u4f53\u5185\u90e8\uff0c\u539f\u56e0\u662ftraining\u7684\u65f6\u5019\u80fd\u5f97\u5230\u56fe\u7247\u6bcf\u4e00\u4e2a\u5750\u6807\u662f\u5426\u5bf9\u5e94\u4e00\u4e2aobject\uff0c\u8fd9\u4e2amask\u6807\u8bb0\u4e3a b(u,v)\\in{0,1} L_{obj} = \\frac{1}{N}\\sum^N_{i=1} - log\\sum_{u,v}b(u,v)g_i(u,v)","title":"Discovery of Latent 3D Keypoints via End-to-end Geometric Reasoning"},{"location":"Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/#discovery-of-latent-3d-keypoints-via-end-to-end-geometric-reasoning","text":"\u8fd9\u7bc7\u8bba\u6587\u5b8c\u6210\u7684\u4efb\u52a1\u662f\uff0c\u5bf9\u5355\u5f20\u56fe\u7247\u8f93\u51fa\u4e00\u7cfb\u5217\u6709\u5e8f\u7684keypoints\uff0c\u8fd9\u4e9bkeypoints\u5728\u4e0d\u540c\u89c6\u89d2\u4e0b\u6709\u76f8\u540c\u7684\u8868\u8fbe\u3002\u8bad\u7ec3\u65f6\u5219\u901a\u8fc7\u540c\u4e00\u7269\u4f53\u4e0d\u540c\u89c6\u89d2\u7684\u4e00\u5bf9\u56fe\u7247\u8fdb\u884c\u8bad\u7ec3\u3002","title":"Discovery of Latent 3D Keypoints via End-to-end Geometric Reasoning"},{"location":"Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/#pipeline","text":"\u5df2\u77e5\u7684\u76f8\u5bf9\u521a\u4f53\u8fd0\u52a8\u4f5c\u4e3a\u76d1\u7763\u4fe1\u53f7\u3002inference\u7684\u65f6\u5019\uff0cKeypointNet\u4ece\u5355\u4e2a\u8f93\u5165\u56fe\u4e2d\u8f93\u51fa3D\u5173\u952e\u70b9","title":"\u4e3b\u4f53Pipeline"},{"location":"Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/#_1","text":"\u4e00\u4e2a3D keypoint\u88ab\u5b9a\u4e49\u4e3a\u50cf\u7d20\u5750\u6807\u52a0\u4e0a\u5bf9\u5e94\u7684\u6df1\u5ea6\u503c\u3002\u7f51\u7edc\u8f93\u51fa N\u4e2a\u5206\u652f\uff0c\u9884\u6d4bN\u4e2a\u5173\u952e\u70b9\u3002 \u76ee\u6807\u51fd\u6570\u7531\u4e24\u4e2a\u90e8\u5206\u7ec4\u6210","title":"\u5177\u4f53\u4ecb\u7ecd"},{"location":"Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/#multi-view-consistency","text":"\u63cf\u8ff0\u4e24\u7ec4\u70b9\u5728ground truth\u8f6c\u6362\u524d\u540e\u70b9\u7684\u8ddd\u79bb \\begin{aligned}[\\hat{u}, \\hat{v}, \\hat{z}, 1]^{\\top} & \\sim \\pi T \\pi^{-1}\\left([u, v, z, 1]^{\\top}\\right) \\\\\\left[\\hat{u}^{\\prime}, \\hat{v}^{\\prime}, \\hat{z}^{\\prime}, 1\\right]^{\\top} & \\sim \\pi T^{-1} \\pi^{-1}\\left(\\left[u^{\\prime}, v^{\\prime}, z^{\\prime}, 1\\right]^{\\top}\\right) \\end{aligned} \\pi\\left([x, y, z, 1]^{\\top}\\right)=\\left[\\frac{f x}{z}, \\frac{f y}{z}, z, 1\\right]^{\\top}=[u, v, z, 1]^{\\top} L_{\\mathrm{con}}=\\frac{1}{2 N} \\sum_{i=1}^{N}\\left\\|\\left[u_{i}, v_{i}, u_{i}^{\\prime}, v_{i}^{\\prime}\\right]^{\\top}-\\left[\\hat{u}_{i}^{\\prime}, \\hat{v}_{i}^{\\prime}, \\hat{u}_{i}, \\hat{v}_{i}\\right]^{\\top}\\right\\|^{2} \u672c\u8d28\u4e0a\u5c31\u662f\u5c06A\u56fe\u4e0a\u7684\u70b9\u8f6c\u6362\u5230B\u56fe\u4e0a\u6c42\u5dee\u503c\uff0c\u518d\u5c06B\u56fe\u4e0a\u7684\u70b9\u8f6c\u6362\u5230A\u56fe\u4e0a\u6c42\u5dee\u503c\uff0c\u6c42\u548c\u3002","title":"Multi-view Consistency"},{"location":"Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/#_2","text":"\u8fd9\u91cc\u53ea\u8981\u6c42\u5bf9\u4f30\u8ba1\u7684\u65cb\u8f6c\u77e9\u9635\u8ba1\u7b97loss L_{\\mathrm{pose}}=2 \\arcsin \\left(\\frac{1}{2 \\sqrt{2}}\\|\\hat{R}-R\\|_{F}\\right) \u5176\u4e2d\u4f30\u8ba1 \\hat R ,\u4ee4 X \u548c X' \u6307\u4ee3\u4e24\u4e2a\u56fe\u8ba1\u7b97\u51fa\u6765\u7684keypoints\u7684 X \\equiv\\left[X_{1}, \\ldots, X_{N}\\right] and X_{i} \\equiv\\left(\\pi^{-1} p_{i}\\right)[: 3] \\hat{R}=V \\operatorname{diag}\\left(1,1, \\ldots, \\operatorname{det}\\left(V U^{\\top}\\right)\\right) U^{\\top} U, \\Sigma, V^{\\top}=\\operatorname{SVD}\\left(\\tilde{X} \\tilde{X}^{\\prime \\top}\\right) \u8fd9\u4e2a\u65b9\u6cd5\u88ab\u79f0\u4e3a Procrustes problem SVD\u662f\u53ef\u4ee5backprob\u7684\u5b9e\u73b0\u7684(\u540c\u6837\u4f7f\u7528\u53ef\u5fae\u5206SVD\u7684 \u8fd9\u7bc7\u6587\u7ae0 \u4e5f\u63d0\u5230tensorflow\u6709\u53ef\u5fae\u5206SVD)","title":"\u76f8\u5bf9\u59ff\u6001\u4f30\u8ba1\u3002"},{"location":"Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/#keypointnet","text":"\u672c\u6587\u4f7f\u7528\u7684KeypointNet,\u4e00\u4e2a\u91cd\u8981\u7684\u6027\u8d28\u5728\u4e8e\u5e73\u79fb\u7b49\u4ef7\uff0c\u4e5f\u5c31\u662f\u8bf4\u5e73\u79fb\u4e00\u4e2a\u50cf\u7d20\uff0c\u8f93\u51fa\u4f4d\u7f6e\u4e5f\u4f1a\u79fb\u52a8\u4e00\u4e2a\u5355\u4f4d\u3002\u8981\u6c42\u8f93\u51faheatmap g_i(u,v) \u4ee3\u8868\u7b2c i \u4e2akeypoint\u51fa\u73b0\u5728 (u,v) ,\u8981\u6c42 \\sum_{u,v}g_i(u,v)=1 \u4f7f\u7528spatial softmax\u53bb\u5b9e\u73b0\u3002 \u7528\u52a0\u6743\u5e73\u5747\u6c42\u51fa\u5bf9\u5e94keypoint\u7684\u4f4d\u7f6e\u4ee5\u53ca\u6df1\u5ea6 [u_i,v_i]^T = \\sum_{u,v}[u * g_i(u,v), v * g_i(u,v)]^T z_i = \\sum_{u,v}d_i(u,v)g_i(u,v)","title":"KeypointNet \u7ed3\u6784"},{"location":"Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/#training","text":"\u5206\u79bbloss:\u5bf9\u8fc7\u4e8e\u9760\u8fd1\u7684keypoints\u7ed9\u4e00\u4e2aloss L_{\\mathrm{sep}}=\\frac{1}{N^{2}} \\sum_{i=1}^{N} \\sum_{j \\neq i}^{N} \\max \\left(0, \\delta^{2}-\\left\\|X_{i}-X_{j}\\right\\|^{2}\\right) \u8f6e\u5ed3\u4e00\u81f4\u6027:\u9f13\u52b1keypoints\u5728\u7269\u4f53\u5185\u90e8\uff0c\u539f\u56e0\u662ftraining\u7684\u65f6\u5019\u80fd\u5f97\u5230\u56fe\u7247\u6bcf\u4e00\u4e2a\u5750\u6807\u662f\u5426\u5bf9\u5e94\u4e00\u4e2aobject\uff0c\u8fd9\u4e2amask\u6807\u8bb0\u4e3a b(u,v)\\in{0,1} L_{obj} = \\frac{1}{N}\\sum^N_{i=1} - log\\sum_{u,v}b(u,v)g_i(u,v)","title":"\u8f85\u52a9training"},{"location":"Building_Blocks/DynamicFilteringNetwork_Fewshot/","text":"Dynamic Conditional Networks for Few-Shot Learning \u672c\u6587\u4f7f\u7528class\u7684\u6587\u5b57 embedding \u4f5c\u4e3a convolution \u7684 guidance \u6765\u5b8c\u6210few-shot learning\u7684\u4efb\u52a1\u3002 \u8fd9\u91cc\u4ec5\u4ecb\u7ecd\u5176\u5f00\u6e90\u7684\u90e8\u5206,\u4e5f\u5c31\u662f dynamic filtering network\u90e8\u5206. Dynamic Filtering Network \u7b97\u6cd5\u4e0a\u4e3a\u521d\u59cb\u5316 N \u4e2a\u6743\u91cd\u7ec4\uff0cembedding\u8f93\u51fa\u4e00\u4e2a N \u7ef4\u7684\u5206\u7c7b\u77e2\u91cf\uff0c\u5c06 N \u4e2a\u6743\u91cd\u52a0\u6743\u6c42\u548c\uff0c\u4f5c\u4e3a\u4e3b\u7ebf\u4e0aConv2D\u7684\u6743\u91cd\u3002","title":"Dynamic Conditional Networks for Few-Shot Learning"},{"location":"Building_Blocks/DynamicFilteringNetwork_Fewshot/#dynamic-conditional-networks-for-few-shot-learning","text":"\u672c\u6587\u4f7f\u7528class\u7684\u6587\u5b57 embedding \u4f5c\u4e3a convolution \u7684 guidance \u6765\u5b8c\u6210few-shot learning\u7684\u4efb\u52a1\u3002 \u8fd9\u91cc\u4ec5\u4ecb\u7ecd\u5176\u5f00\u6e90\u7684\u90e8\u5206,\u4e5f\u5c31\u662f dynamic filtering network\u90e8\u5206.","title":"Dynamic Conditional Networks for Few-Shot Learning"},{"location":"Building_Blocks/DynamicFilteringNetwork_Fewshot/#dynamic-filtering-network","text":"\u7b97\u6cd5\u4e0a\u4e3a\u521d\u59cb\u5316 N \u4e2a\u6743\u91cd\u7ec4\uff0cembedding\u8f93\u51fa\u4e00\u4e2a N \u7ef4\u7684\u5206\u7c7b\u77e2\u91cf\uff0c\u5c06 N \u4e2a\u6743\u91cd\u52a0\u6743\u6c42\u548c\uff0c\u4f5c\u4e3a\u4e3b\u7ebf\u4e0aConv2D\u7684\u6743\u91cd\u3002","title":"Dynamic Filtering Network"},{"location":"Building_Blocks/DynanicFilteringNetwork/","text":"Dynamic Filter Networks \u8fd9\u7bc72016\u7684NIPS\u8bba\u6587\u63a2\u8ba8\u7684\u662f\u52a8\u6001\u5377\u79ef\u6838\u7684\u95ee\u9898.\u539f\u6587\u7684\u5b98\u65b9\u5e93\u662f\u57fa\u4e8eTheano\u7684\uff0c\u540e\u6765\u4e5f\u6709\u4e86 tensorflow implementation .\u6982\u5ff5\u4e0d\u7b97\u590d\u6742\u3002 \u7ed3\u6784\u4e0e\u6a21\u5757\u7ec4\u6210 \u7b80\u5355\u6765\u8bf4\u5c31\u662f\u8ba9\u8f93\u5165\u4e00\u65b9\u9762\u4f5c\u4e3a\u88ab\u5377\u79ef\u7684\u5e38\u89c4\u8f93\u5165\uff0c\u53e6\u4e00\u65b9\u9762\u53c8\u4f7f\u7528\u7f51\u7edc\u751f\u6210\u5377\u79ef\u6838\u3002 \u8fd9\u91cc\u6709\u4e24\u79cd\u5b9e\u73b0\u65b9\u5f0f\uff0c\u7b2c\u4e00\u79cd\u662f\u6574\u4e2aFeature map\u53ea\u8f93\u51fa\u4e00\u4e2asingle Convolution Kernel,\u7136\u540e\u5377\u79ef\u4e5f\u4f1a\u4e0e\u4e00\u822c\u7684\u5377\u79ef\u4e00\u81f4\u3002 \u7b2c\u4e8c\u79cd\u662f\u5728feature map\u6bcf\u4e00\u4e2a\u70b9\u4e0a\u7528\u5e38\u89c4\u5377\u79ef\u751f\u62109(3x3)\u4e2a\u7279\u5f81,\u7136\u540e\u6bcf\u4e00\u4e2a\u70b9\u4e0a\u7528\u5404\u81ea\u7684\u8ba1\u7b97\u5f97\u5230\u7684\u7279\u5f81\u518d\u8ba1\u7b97\u5377\u79ef\u3002 \u4f5c\u8005\u6307\u51fa\u8fd9\u4e2a\u7ed3\u6784\u4e0eResidual\u8fde\u63a5\u6709\u4e00\u5b9a\u76f8\u4f3c\u4e4b\u5904\u3002","title":"Dynamic Filter Networks"},{"location":"Building_Blocks/DynanicFilteringNetwork/#dynamic-filter-networks","text":"\u8fd9\u7bc72016\u7684NIPS\u8bba\u6587\u63a2\u8ba8\u7684\u662f\u52a8\u6001\u5377\u79ef\u6838\u7684\u95ee\u9898.\u539f\u6587\u7684\u5b98\u65b9\u5e93\u662f\u57fa\u4e8eTheano\u7684\uff0c\u540e\u6765\u4e5f\u6709\u4e86 tensorflow implementation .\u6982\u5ff5\u4e0d\u7b97\u590d\u6742\u3002","title":"Dynamic Filter Networks"},{"location":"Building_Blocks/DynanicFilteringNetwork/#_1","text":"\u7b80\u5355\u6765\u8bf4\u5c31\u662f\u8ba9\u8f93\u5165\u4e00\u65b9\u9762\u4f5c\u4e3a\u88ab\u5377\u79ef\u7684\u5e38\u89c4\u8f93\u5165\uff0c\u53e6\u4e00\u65b9\u9762\u53c8\u4f7f\u7528\u7f51\u7edc\u751f\u6210\u5377\u79ef\u6838\u3002 \u8fd9\u91cc\u6709\u4e24\u79cd\u5b9e\u73b0\u65b9\u5f0f\uff0c\u7b2c\u4e00\u79cd\u662f\u6574\u4e2aFeature map\u53ea\u8f93\u51fa\u4e00\u4e2asingle Convolution Kernel,\u7136\u540e\u5377\u79ef\u4e5f\u4f1a\u4e0e\u4e00\u822c\u7684\u5377\u79ef\u4e00\u81f4\u3002 \u7b2c\u4e8c\u79cd\u662f\u5728feature map\u6bcf\u4e00\u4e2a\u70b9\u4e0a\u7528\u5e38\u89c4\u5377\u79ef\u751f\u62109(3x3)\u4e2a\u7279\u5f81,\u7136\u540e\u6bcf\u4e00\u4e2a\u70b9\u4e0a\u7528\u5404\u81ea\u7684\u8ba1\u7b97\u5f97\u5230\u7684\u7279\u5f81\u518d\u8ba1\u7b97\u5377\u79ef\u3002 \u4f5c\u8005\u6307\u51fa\u8fd9\u4e2a\u7ed3\u6784\u4e0eResidual\u8fde\u63a5\u6709\u4e00\u5b9a\u76f8\u4f3c\u4e4b\u5904\u3002","title":"\u7ed3\u6784\u4e0e\u6a21\u5757\u7ec4\u6210"},{"location":"Building_Blocks/ELASTIC/","text":"ELASTIC Improving CNNs With Dynamic Scaling Policies \u8fd9\u7bc7\u8bba\u6587\u7684\u60f3\u6cd5\u975e\u5e38\u7b80\u5355\uff0c\u7b80\u5355\u6765\u8bf4\u5c31\u662f\u8981\u8ba9\u7f51\u7edc\u80fd\u591f\u81ea\u4e3b\u5b66\u4e60\u5728inference\u8fc7\u7a0b\u4e2d\u91c7\u53d6\u600e\u6837\u7684scale\uff0c\u505a\u6cd5\u662f\u5728Resnet\u7684\u4e00\u4e2a\u5206\u652f\u4e2d\uff0c\u5148downsample\uff0c\u505a\u5377\u79ef\uff0c\u518dupsample\u3002\u540c\u65f6\u4fdd\u7559\u539fscale\u7684\u901a\u8def\uff0c\u7531\u8bad\u7ec3\u8fc7\u7a0b\u6765\u8ba9\u7f51\u7edc\u51b3\u5b9a\u5e94\u8be5\u5982\u4f55\u914d\u7f6e\u4e24\u4e2a\u901a\u8def\u7684\u53c2\u6570\uff0c\u4ee5\u6b64\u51b3\u5b9ascale. \u672c\u6587\u8fdb\u4e00\u6b65\u8fd8\u7ed9\u51fa\u4e86\u76ee\u524d\u591a\u79cdmultiscale model\u7684\u7ed3\u6784\uff0c\u56fe\u975e\u5e38\u7684\u597d\u770b\u4e14inspiring","title":"ELASTIC Improving CNNs With Dynamic Scaling Policies"},{"location":"Building_Blocks/ELASTIC/#elastic-improving-cnns-with-dynamic-scaling-policies","text":"\u8fd9\u7bc7\u8bba\u6587\u7684\u60f3\u6cd5\u975e\u5e38\u7b80\u5355\uff0c\u7b80\u5355\u6765\u8bf4\u5c31\u662f\u8981\u8ba9\u7f51\u7edc\u80fd\u591f\u81ea\u4e3b\u5b66\u4e60\u5728inference\u8fc7\u7a0b\u4e2d\u91c7\u53d6\u600e\u6837\u7684scale\uff0c\u505a\u6cd5\u662f\u5728Resnet\u7684\u4e00\u4e2a\u5206\u652f\u4e2d\uff0c\u5148downsample\uff0c\u505a\u5377\u79ef\uff0c\u518dupsample\u3002\u540c\u65f6\u4fdd\u7559\u539fscale\u7684\u901a\u8def\uff0c\u7531\u8bad\u7ec3\u8fc7\u7a0b\u6765\u8ba9\u7f51\u7edc\u51b3\u5b9a\u5e94\u8be5\u5982\u4f55\u914d\u7f6e\u4e24\u4e2a\u901a\u8def\u7684\u53c2\u6570\uff0c\u4ee5\u6b64\u51b3\u5b9ascale. \u672c\u6587\u8fdb\u4e00\u6b65\u8fd8\u7ed9\u51fa\u4e86\u76ee\u524d\u591a\u79cdmultiscale model\u7684\u7ed3\u6784\uff0c\u56fe\u975e\u5e38\u7684\u597d\u770b\u4e14inspiring","title":"ELASTIC Improving CNNs With Dynamic Scaling Policies"},{"location":"Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/","text":"EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks \u8fd9\u7bc7\u6587\u7ae0\u5206\u6790\u4e86CNN Scale Up\u7684\u76f8\u5173\u539f\u7406\uff0c\u7136\u540e\u7ed9\u51faEfficientNet.\u80cc\u540e\u7684\u903b\u8f91\u662f\u8fd9\u6837\u7684\uff0c\u9996\u5148\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7Neural Architecture Search\u5f97\u5230\u4e00\u4e2a\u4f4e\u5206\u8fa8\u7387\u56fe\u7247\u7684\u9ad8\u6548\u7387\u6a21\u578b\uff0c\u7136\u540e\u4f9d\u636e\u5b83\u5bf9\u5e94\u7684Scale Up\u539f\u7406\u8fdb\u884c\u6269\u5c55\u3002\u672c\u6587\u7528\u5230\u7684baseline\u51fa\u81ea MnasNet(pdf) ,\u5c31\u662f\u901a\u8fc7NAS\u5f97\u5230\u7684\u3002\u672c\u6587\u7684 \u4ee3\u7801 \u63d0\u4f9b\u4e86keras\u4ee5\u53caTensorflow\u5b9e\u73b0\u3002\u8fd9\u4e2a repo\u63d0\u4f9b\u4e86pytorch\u5b9e\u73b0 \u8fd9\u91cc\u6dfb\u52a0\u4e00\u4e2a medium\u4e0a\u7684\u89e3\u8bfb . Model Scaling Observation \u4e0a\u56fe\u5c55\u793a\u7684\u662f\u4e0d\u540c\u7684scale up\u4e00\u4e2a\u6a21\u578b\u7684\u65b9\u5f0f\u3002\u672c\u6587\u7684\u7ed3\u8bba\u662f\u5e94\u8be5\u6839\u636e\u8f93\u5165\u56fe\u7247\u7684\u5206\u8fa8\u7387\u3001channel\u6570\u76ee\u4ee5\u53ca\u7f51\u7edc\u6df1\u5ea6\u7efc\u5408Scale Up\u5f97\u5230\u7684\u63d0\u5347\u624d\u662f\u6700\u660e\u663e\u7684\u3002 \u4e0a\u56fe\u5c55\u793a\u7684\u662f\u5206\u522b\u53eaScale Up\u7f51\u7edcchannel\u6570,\u7f51\u7edc\u6df1\u5ea6,\u4ee5\u53ca\u56fe\u7247\u5206\u8fa8\u7387\u5f97\u5230\u7684\u3002\u4f5c\u8005\u7684\u7ed3\u8bba\u662f\u53eaScale Up\u4e00\u4e2a\u56e0\u5b50\u5f88\u5bb9\u6613\u5f97\u5230Saturation\uff0c\u901a\u8fc7\u53e6\u4e00\u4e2a\u5b9e\u9a8c\u53d1\u73b0\u603b\u5408\u4e00\u8d77Scale Up\u80fd\u5f97\u5230\u66f4\u597d\u7684\u7ed3\u679c\u3002 Compound Scaling Method \\begin{aligned} \\text { depth: } d &=\\alpha^{\\phi} \\\\ \\text { width: } w &=\\beta^{\\phi} \\\\ \\text { resolution: } r &=\\gamma^{\\phi} \\\\ \\text { s.t. } \\alpha & \\cdot \\beta^{2} \\cdot \\gamma^{2} \\approx 2 \\\\ \\alpha & \\geq 1, \\beta \\geq 1, \\gamma \\geq 1 \\end{aligned} \u4f5c\u8005\u9009\u62e9\u8fd9\u4e2a\u7684\u539f\u56e0\u662f\u56e0\u4e3aFLOPS(\u6d6e\u70b9\u8fd0\u7b97\u6570)\u6b63\u6bd4\u4e8e\u6df1\u5ea6,\u9891\u9053\u6570\u7684\u5e73\u65b9\u4ee5\u53ca\u5206\u8fa8\u7387\u7684\u5e73\u65b9\u3002 Efficient Net EfficientNet\u7684\u57fa\u672c\u5355\u5143\u7531 Mobile Conv(pdf) \u7ec4\u6210\u3002 \u4ece\u8fd9\u4e2aBaseLine\u5f00\u59cb\uff0c\u901a\u8fc7\u5c0fgrid-search\u5f97\u5230 \\alpha, \\beta, \\gamma \u7684\u521d\u59cb\u503c,\u66f4\u6539 \\phi \u5f97\u5230\u4e0d\u540c\u7684channel\u6570,\u5206\u8fa8\u7387\u4ee5\u53ca\u7f51\u7edc\u5c42\u6570(\u7f51\u7edc\u5c42\u6570\u7684\u66f4\u6539\u662f\u901a\u8fc7\u66f4\u6539\u5806\u53e0MBConv\u7684\u4e00\u4e2afor\u5faa\u73af\u7684\u5faa\u73af\u6b21\u6570\u5b9e\u73b0\u7684) \u5b9e\u9a8c\u4e0e\u7ed3\u679c \u672c\u6587\u9996\u5148\u5c1d\u8bd5Scale Up\u4e86MobileNet\u4ee5\u53caResNet\u7684\u5230\u597d\u7684\u7ed3\u679c\uff0c\u7136\u540e\u5f00\u59cbScale Up EfficientNet, \u4ee5\u4e0b\u56fe\u4e2d\u5404\u4e2a\u7f51\u7edc\u7684\u51c6\u786e\u7387\u3001\u53c2\u6570\u4ee5\u53caFLOPS\u662f\u4e00\u4e2a\u5f88\u597d\u7684\u53c2\u8003.","title":"EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks"},{"location":"Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/#efficientnet-rethinking-model-scaling-for-convolutional-neural-networks","text":"\u8fd9\u7bc7\u6587\u7ae0\u5206\u6790\u4e86CNN Scale Up\u7684\u76f8\u5173\u539f\u7406\uff0c\u7136\u540e\u7ed9\u51faEfficientNet.\u80cc\u540e\u7684\u903b\u8f91\u662f\u8fd9\u6837\u7684\uff0c\u9996\u5148\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7Neural Architecture Search\u5f97\u5230\u4e00\u4e2a\u4f4e\u5206\u8fa8\u7387\u56fe\u7247\u7684\u9ad8\u6548\u7387\u6a21\u578b\uff0c\u7136\u540e\u4f9d\u636e\u5b83\u5bf9\u5e94\u7684Scale Up\u539f\u7406\u8fdb\u884c\u6269\u5c55\u3002\u672c\u6587\u7528\u5230\u7684baseline\u51fa\u81ea MnasNet(pdf) ,\u5c31\u662f\u901a\u8fc7NAS\u5f97\u5230\u7684\u3002\u672c\u6587\u7684 \u4ee3\u7801 \u63d0\u4f9b\u4e86keras\u4ee5\u53caTensorflow\u5b9e\u73b0\u3002\u8fd9\u4e2a repo\u63d0\u4f9b\u4e86pytorch\u5b9e\u73b0 \u8fd9\u91cc\u6dfb\u52a0\u4e00\u4e2a medium\u4e0a\u7684\u89e3\u8bfb .","title":"EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks"},{"location":"Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/#model-scaling-observation","text":"\u4e0a\u56fe\u5c55\u793a\u7684\u662f\u4e0d\u540c\u7684scale up\u4e00\u4e2a\u6a21\u578b\u7684\u65b9\u5f0f\u3002\u672c\u6587\u7684\u7ed3\u8bba\u662f\u5e94\u8be5\u6839\u636e\u8f93\u5165\u56fe\u7247\u7684\u5206\u8fa8\u7387\u3001channel\u6570\u76ee\u4ee5\u53ca\u7f51\u7edc\u6df1\u5ea6\u7efc\u5408Scale Up\u5f97\u5230\u7684\u63d0\u5347\u624d\u662f\u6700\u660e\u663e\u7684\u3002 \u4e0a\u56fe\u5c55\u793a\u7684\u662f\u5206\u522b\u53eaScale Up\u7f51\u7edcchannel\u6570,\u7f51\u7edc\u6df1\u5ea6,\u4ee5\u53ca\u56fe\u7247\u5206\u8fa8\u7387\u5f97\u5230\u7684\u3002\u4f5c\u8005\u7684\u7ed3\u8bba\u662f\u53eaScale Up\u4e00\u4e2a\u56e0\u5b50\u5f88\u5bb9\u6613\u5f97\u5230Saturation\uff0c\u901a\u8fc7\u53e6\u4e00\u4e2a\u5b9e\u9a8c\u53d1\u73b0\u603b\u5408\u4e00\u8d77Scale Up\u80fd\u5f97\u5230\u66f4\u597d\u7684\u7ed3\u679c\u3002","title":"Model Scaling Observation"},{"location":"Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/#compound-scaling-method","text":"\\begin{aligned} \\text { depth: } d &=\\alpha^{\\phi} \\\\ \\text { width: } w &=\\beta^{\\phi} \\\\ \\text { resolution: } r &=\\gamma^{\\phi} \\\\ \\text { s.t. } \\alpha & \\cdot \\beta^{2} \\cdot \\gamma^{2} \\approx 2 \\\\ \\alpha & \\geq 1, \\beta \\geq 1, \\gamma \\geq 1 \\end{aligned} \u4f5c\u8005\u9009\u62e9\u8fd9\u4e2a\u7684\u539f\u56e0\u662f\u56e0\u4e3aFLOPS(\u6d6e\u70b9\u8fd0\u7b97\u6570)\u6b63\u6bd4\u4e8e\u6df1\u5ea6,\u9891\u9053\u6570\u7684\u5e73\u65b9\u4ee5\u53ca\u5206\u8fa8\u7387\u7684\u5e73\u65b9\u3002","title":"Compound Scaling Method"},{"location":"Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/#efficient-net","text":"EfficientNet\u7684\u57fa\u672c\u5355\u5143\u7531 Mobile Conv(pdf) \u7ec4\u6210\u3002 \u4ece\u8fd9\u4e2aBaseLine\u5f00\u59cb\uff0c\u901a\u8fc7\u5c0fgrid-search\u5f97\u5230 \\alpha, \\beta, \\gamma \u7684\u521d\u59cb\u503c,\u66f4\u6539 \\phi \u5f97\u5230\u4e0d\u540c\u7684channel\u6570,\u5206\u8fa8\u7387\u4ee5\u53ca\u7f51\u7edc\u5c42\u6570(\u7f51\u7edc\u5c42\u6570\u7684\u66f4\u6539\u662f\u901a\u8fc7\u66f4\u6539\u5806\u53e0MBConv\u7684\u4e00\u4e2afor\u5faa\u73af\u7684\u5faa\u73af\u6b21\u6570\u5b9e\u73b0\u7684)","title":"Efficient Net"},{"location":"Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/#_1","text":"\u672c\u6587\u9996\u5148\u5c1d\u8bd5Scale Up\u4e86MobileNet\u4ee5\u53caResNet\u7684\u5230\u597d\u7684\u7ed3\u679c\uff0c\u7136\u540e\u5f00\u59cbScale Up EfficientNet, \u4ee5\u4e0b\u56fe\u4e2d\u5404\u4e2a\u7f51\u7edc\u7684\u51c6\u786e\u7387\u3001\u53c2\u6570\u4ee5\u53caFLOPS\u662f\u4e00\u4e2a\u5f88\u597d\u7684\u53c2\u8003.","title":"\u5b9e\u9a8c\u4e0e\u7ed3\u679c"},{"location":"Building_Blocks/GhostNet/","text":"GhostNet: More Features from Cheap Operations class GhostModule(nn.Module): def __init__(self, inp, oup, kernel_size=1, ratio=2, dw_size=3, stride=1, relu=True): super(GhostModule, self).__init__() self.oup = oup init_channels = math.ceil(oup / ratio) new_channels = init_channels*(ratio-1) self.primary_conv = nn.Sequential( nn.Conv2d(inp, init_channels, kernel_size, stride, kernel_size//2, bias=False), nn.BatchNorm2d(init_channels), nn.ReLU(inplace=True) if relu else nn.Sequential(), ) self.cheap_operation = nn.Sequential( nn.Conv2d(init_channels, new_channels, dw_size, 1, dw_size//2, groups=init_channels, bias=False), nn.BatchNorm2d(new_channels), nn.ReLU(inplace=True) if relu else nn.Sequential(), ) def forward(self, x): x1 = self.primary_conv(x) x2 = self.cheap_operation(x1) out = torch.cat([x1,x2], dim=1) return out[:,:self.oup,:,:] class GhostBottleneck(nn.Module): def __init__(self, inp, hidden_dim, oup, kernel_size, stride, use_se): super(GhostBottleneck, self).__init__() assert stride in [1, 2] self.conv = nn.Sequential( # pw GhostModule(inp, hidden_dim, kernel_size=1, relu=True), # dw depthwise_conv(hidden_dim, hidden_dim, kernel_size, stride, relu=False) if stride==2 else nn.Sequential(), # Squeeze-and-Excite SELayer(hidden_dim) if use_se else nn.Sequential(), # pw-linear GhostModule(hidden_dim, oup, kernel_size=1, relu=False), ) if stride == 1 and inp == oup: self.shortcut = nn.Sequential() else: self.shortcut = nn.Sequential( depthwise_conv(inp, inp, 3, stride, relu=True), nn.Conv2d(inp, oup, 1, 1, 0, bias=False), nn.BatchNorm2d(oup), ) def forward(self, x): return self.conv(x) + self.shortcut(x)","title":"GhostNet: More Features from Cheap Operations"},{"location":"Building_Blocks/GhostNet/#ghostnet-more-features-from-cheap-operations","text":"class GhostModule(nn.Module): def __init__(self, inp, oup, kernel_size=1, ratio=2, dw_size=3, stride=1, relu=True): super(GhostModule, self).__init__() self.oup = oup init_channels = math.ceil(oup / ratio) new_channels = init_channels*(ratio-1) self.primary_conv = nn.Sequential( nn.Conv2d(inp, init_channels, kernel_size, stride, kernel_size//2, bias=False), nn.BatchNorm2d(init_channels), nn.ReLU(inplace=True) if relu else nn.Sequential(), ) self.cheap_operation = nn.Sequential( nn.Conv2d(init_channels, new_channels, dw_size, 1, dw_size//2, groups=init_channels, bias=False), nn.BatchNorm2d(new_channels), nn.ReLU(inplace=True) if relu else nn.Sequential(), ) def forward(self, x): x1 = self.primary_conv(x) x2 = self.cheap_operation(x1) out = torch.cat([x1,x2], dim=1) return out[:,:self.oup,:,:] class GhostBottleneck(nn.Module): def __init__(self, inp, hidden_dim, oup, kernel_size, stride, use_se): super(GhostBottleneck, self).__init__() assert stride in [1, 2] self.conv = nn.Sequential( # pw GhostModule(inp, hidden_dim, kernel_size=1, relu=True), # dw depthwise_conv(hidden_dim, hidden_dim, kernel_size, stride, relu=False) if stride==2 else nn.Sequential(), # Squeeze-and-Excite SELayer(hidden_dim) if use_se else nn.Sequential(), # pw-linear GhostModule(hidden_dim, oup, kernel_size=1, relu=False), ) if stride == 1 and inp == oup: self.shortcut = nn.Sequential() else: self.shortcut = nn.Sequential( depthwise_conv(inp, inp, 3, stride, relu=True), nn.Conv2d(inp, oup, 1, 1, 0, bias=False), nn.BatchNorm2d(oup), ) def forward(self, x): return self.conv(x) + self.shortcut(x)","title":"GhostNet: More Features from Cheap Operations"},{"location":"Building_Blocks/GumbelSoftmax/","text":"Categorical Reparameterization with Gumbel-Softmax \u8fd9\u7bc7\u6587\u7ae0\u7684\u5185\u5bb9\u5df2\u7ecf\u56fa\u5316\u4e3a\u4e86pytorch\u7684\u4e00\u4e2a \u51fd\u6570 \u5176\u4f5c\u7528\u662f\u5141\u8bb8 Stochastic, Differentiable, Probabilistic Weighted, Indexing. \u5148\u7528\u4ee3\u7801\u89e3\u91ca gumbel\u91c7\u6837\u53ef\u4ee5\u5982\u4f55\u7528\u5747\u5300\u968f\u673a\u91c7\u6837\u8868\u8fbe\uff1a def gumbel(*shape): u = np.random.rand(*shape) return -np.log(-np.log(u)) def gumbelsoftmax(weights, lmbda=1, N=10000): d = len(weights) logits = np.log(weights.reshape(d, 1)) gumbel_noise = gumbel(d*N).reshape(d, N) return softmax(( logits + gumbel_noise)/ lmbda, axis=0) \u901a\u8fc7\u91c7\u6837 gumbelsoftmax,\u5f97\u5230\u7684\u5206\u5e03\u8fd1\u4f3c\u4e8e \\frac{weights}{\\sum(weights)} \u5176\u4e2d lmbda \u53d8\u91cf\u7406\u89e3\u4e3a\u6e29\u5ea6\u8d85\u53c2\uff0c\u5176\u4f5c\u7528\u5728\u4e8e\u63a7\u5236\u91c7\u6837\u7cfb\u7edf\u7684\u968f\u673a\u6027. pytorch functional \u7684\u4ee3\u7801\u5982\u4e0b(\u4e0d\u5fc5\u590d\u5236\u4f7f\u7528\uff0c\u8fd9\u5185\u7f6e\u4e8epytorch\u4e2d): def gumbel_softmax(logits, tau=1, hard=False, eps=1e-10, dim=-1): # type: (Tensor, float, bool, float, int) -> Tensor r\"\"\" Examples:: >>> logits = torch.randn(20, 32) >>> # Sample soft categorical using reparametrization trick: >>> F.gumbel_softmax(logits, tau=1, hard=False) >>> # Sample hard categorical using \"Straight-through\" trick: >>> F.gumbel_softmax(logits, tau=1, hard=True) \"\"\" if eps != 1e-10: warnings.warn(\"`eps` parameter is deprecated and has no effect.\") gumbels = -torch.empty_like(logits, memory_format=torch.legacy_contiguous_format).exponential_().log() # ~Gumbel(0,1) gumbels = (logits + gumbels) / tau # ~Gumbel(logits,tau) y_soft = gumbels.softmax(dim) if hard: # Straight through. index = y_soft.max(dim, keepdim=True)[1] y_hard = torch.zeros_like(logits, memory_format=torch.legacy_contiguous_format).scatter_(dim, index, 1.0) ret = y_hard - y_soft.detach() + y_soft else: # Reparametrization trick. ret = y_soft return ret \u503c\u5f97\u6ce8\u610f\u7684\u662f\u8fd9\u91cc\u4f7f\u7528\u4e86 ret = y_hard - y_soft.detach() + y_soft \u8fd9\u4e00\u4e2atrick\u4f7f\u5f97one-hot\u7684 y\\_hard \u5728forward\u65f6\u662findexing\u91cf\uff0c\u4f46\u662fbackward\u7684\u65f6\u5019\u7528\u7684\u662f y\\_soft \u7684\u68af\u5ea6\u3002 \u8fd9\u7bc7\u6587\u7ae0\u4e0e\u5f88\u591a\u5176\u4ed6\u5185\u5bb9\u76f8\u5173\uff0c\u6bd4\u5982\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u4f5c\u4e3a\u4e00\u4e2a\u53ef\u4ee5exploit\u53c8\u53ef\u4ee5explore\u7684hard indexing. \u5728\u7f51\u7edc\u526a\u679d\u4e2d\u53ef\u4ee5\u4f5c\u4e3a\u4e00\u4e2a\u53ef\u4ee5\u5b66\u4e60\u4f7f\u7528\u7684\u53c2\u91cf\u3002 \u6bd4\u8f83\u5947\u5999\u7684\u662f\u8fd9\u7bc7paper\u5728ICLR\u53d1\u5e03\u7684\u65f6\u5019\u53ea\u662fmarginally accepted,\u4e5f\u53ea\u662fposter,\u4e3b\u8981\u662f\u53ef\u80fd\u539f\u4f5c\u8005\u4ee5\u53caReviewer\u5f53\u65f6\u53ea\u662f\u5728\u8003\u8651\u4f7f\u7528\u5728Generative Model\u4e0a\uff0c\u63d0\u5347\u6ca1\u6709\u90a3\u4e48\u663e\u8457\uff0c\u800c\u6ca1\u6709\u9884\u77e5\u8fd9\u4e48\u591a\u540e\u6765\u7684\u5e94\u7528\u3002\u4e0d\u8fc7\u540e\u6765\u5927\u5bb6\u5bf9\u5b83\u7684\u5f15\u7528\u4ee5\u53ca\u53d1\u6325\u662f\u5f88\u5de8\u5927\u7684.","title":"Categorical Reparameterization with Gumbel-Softmax"},{"location":"Building_Blocks/GumbelSoftmax/#categorical-reparameterization-with-gumbel-softmax","text":"\u8fd9\u7bc7\u6587\u7ae0\u7684\u5185\u5bb9\u5df2\u7ecf\u56fa\u5316\u4e3a\u4e86pytorch\u7684\u4e00\u4e2a \u51fd\u6570 \u5176\u4f5c\u7528\u662f\u5141\u8bb8 Stochastic, Differentiable, Probabilistic Weighted, Indexing. \u5148\u7528\u4ee3\u7801\u89e3\u91ca gumbel\u91c7\u6837\u53ef\u4ee5\u5982\u4f55\u7528\u5747\u5300\u968f\u673a\u91c7\u6837\u8868\u8fbe\uff1a def gumbel(*shape): u = np.random.rand(*shape) return -np.log(-np.log(u)) def gumbelsoftmax(weights, lmbda=1, N=10000): d = len(weights) logits = np.log(weights.reshape(d, 1)) gumbel_noise = gumbel(d*N).reshape(d, N) return softmax(( logits + gumbel_noise)/ lmbda, axis=0) \u901a\u8fc7\u91c7\u6837 gumbelsoftmax,\u5f97\u5230\u7684\u5206\u5e03\u8fd1\u4f3c\u4e8e \\frac{weights}{\\sum(weights)} \u5176\u4e2d lmbda \u53d8\u91cf\u7406\u89e3\u4e3a\u6e29\u5ea6\u8d85\u53c2\uff0c\u5176\u4f5c\u7528\u5728\u4e8e\u63a7\u5236\u91c7\u6837\u7cfb\u7edf\u7684\u968f\u673a\u6027. pytorch functional \u7684\u4ee3\u7801\u5982\u4e0b(\u4e0d\u5fc5\u590d\u5236\u4f7f\u7528\uff0c\u8fd9\u5185\u7f6e\u4e8epytorch\u4e2d): def gumbel_softmax(logits, tau=1, hard=False, eps=1e-10, dim=-1): # type: (Tensor, float, bool, float, int) -> Tensor r\"\"\" Examples:: >>> logits = torch.randn(20, 32) >>> # Sample soft categorical using reparametrization trick: >>> F.gumbel_softmax(logits, tau=1, hard=False) >>> # Sample hard categorical using \"Straight-through\" trick: >>> F.gumbel_softmax(logits, tau=1, hard=True) \"\"\" if eps != 1e-10: warnings.warn(\"`eps` parameter is deprecated and has no effect.\") gumbels = -torch.empty_like(logits, memory_format=torch.legacy_contiguous_format).exponential_().log() # ~Gumbel(0,1) gumbels = (logits + gumbels) / tau # ~Gumbel(logits,tau) y_soft = gumbels.softmax(dim) if hard: # Straight through. index = y_soft.max(dim, keepdim=True)[1] y_hard = torch.zeros_like(logits, memory_format=torch.legacy_contiguous_format).scatter_(dim, index, 1.0) ret = y_hard - y_soft.detach() + y_soft else: # Reparametrization trick. ret = y_soft return ret \u503c\u5f97\u6ce8\u610f\u7684\u662f\u8fd9\u91cc\u4f7f\u7528\u4e86 ret = y_hard - y_soft.detach() + y_soft \u8fd9\u4e00\u4e2atrick\u4f7f\u5f97one-hot\u7684 y\\_hard \u5728forward\u65f6\u662findexing\u91cf\uff0c\u4f46\u662fbackward\u7684\u65f6\u5019\u7528\u7684\u662f y\\_soft \u7684\u68af\u5ea6\u3002 \u8fd9\u7bc7\u6587\u7ae0\u4e0e\u5f88\u591a\u5176\u4ed6\u5185\u5bb9\u76f8\u5173\uff0c\u6bd4\u5982\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u4f5c\u4e3a\u4e00\u4e2a\u53ef\u4ee5exploit\u53c8\u53ef\u4ee5explore\u7684hard indexing. \u5728\u7f51\u7edc\u526a\u679d\u4e2d\u53ef\u4ee5\u4f5c\u4e3a\u4e00\u4e2a\u53ef\u4ee5\u5b66\u4e60\u4f7f\u7528\u7684\u53c2\u91cf\u3002 \u6bd4\u8f83\u5947\u5999\u7684\u662f\u8fd9\u7bc7paper\u5728ICLR\u53d1\u5e03\u7684\u65f6\u5019\u53ea\u662fmarginally accepted,\u4e5f\u53ea\u662fposter,\u4e3b\u8981\u662f\u53ef\u80fd\u539f\u4f5c\u8005\u4ee5\u53caReviewer\u5f53\u65f6\u53ea\u662f\u5728\u8003\u8651\u4f7f\u7528\u5728Generative Model\u4e0a\uff0c\u63d0\u5347\u6ca1\u6709\u90a3\u4e48\u663e\u8457\uff0c\u800c\u6ca1\u6709\u9884\u77e5\u8fd9\u4e48\u591a\u540e\u6765\u7684\u5e94\u7528\u3002\u4e0d\u8fc7\u540e\u6765\u5927\u5bb6\u5bf9\u5b83\u7684\u5f15\u7528\u4ee5\u53ca\u53d1\u6325\u662f\u5f88\u5de8\u5927\u7684.","title":"Categorical Reparameterization with Gumbel-Softmax"},{"location":"Building_Blocks/HRNet/","text":"Deep High-Resolution Representation Learning for Human Pose Estimation \u8fd9\u7bc7\u8bba\u6587\u7ed9\u51faHRnet,\u601d\u8def\u5173\u952e\u662f\u4fdd\u7559\u4e0d\u540cscale\u7684feature map\uff0c\u5e76\u5728\u524d\u4f20\u8fc7\u7a0b\u4e2d\u8ba9\u4ed6\u4eec\u4ea4\u6362\u4fe1\u606f\u3002","title":"Deep High-Resolution Representation Learning for Human Pose Estimation"},{"location":"Building_Blocks/HRNet/#deep-high-resolution-representation-learning-for-human-pose-estimation","text":"\u8fd9\u7bc7\u8bba\u6587\u7ed9\u51faHRnet,\u601d\u8def\u5173\u952e\u662f\u4fdd\u7559\u4e0d\u540cscale\u7684feature map\uff0c\u5e76\u5728\u524d\u4f20\u8fc7\u7a0b\u4e2d\u8ba9\u4ed6\u4eec\u4ea4\u6362\u4fe1\u606f\u3002","title":"Deep High-Resolution Representation Learning for Human Pose Estimation"},{"location":"Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/","text":"New Optimizers \u76ee\u5f55\u4e2d\u7684\u4ee3\u7801\u4ed3\u5e93\u6307\u5411Ranger\u5e93\uff0c\u662f\u4e00\u4e2a\u878d\u5408\u4e86\u591a\u4e2aoptimizer\u7684\u5e93\u3002\u5404\u7bc7\u6587\u7ae0\u6709\u81ea\u5df1\u7684\u5b98\u65b9\u5f00\u6e90\u5e93\u3002 Lookahead Optimizer:ksteps forward, 1 step back pdf code \u672c\u8d28\u4e0a\u6765\u8bf4\u5c31\u662f\u4fdd\u5b58\u4e24\u5957\u53c2\u6570\u503c\uff0c\u7528\u4e00\u5957\u53c2\u6570\u503c\u6b63\u5e38\u66f4\u65b0N\u6b65(\u6bcf\u53d8\u4e00\u6b21\u9700\u8981\u4f7f\u7528\u65b0\u7684\u6743\u503c\u8ba1\u7b97\u66f4\u65b0\u65b9\u5411)\uff0c\u7136\u540e\u7528\u7b2c\u4e8c\u5957\u53c2\u6570\u5f80\u6700\u7ec8\u8fd9N\u6b65\u7684\u603b\u66f4\u65b0\u65b9\u5411\u8d70\u4e00\u6b65 Gradient Centralization: A New Optimization Technique for Deep Neural Networks pdf code \u8ba1\u7b97\u65b9\u5f0f\u5f88\u7b80\u5355\uff0c\u5c31\u662f\u8ba1\u7b97graident\u4e4b\u540e\uff0c\u5728update\u4e4b\u524dnormalize\u5168\u5c40gradient for p in group['params']: d_p = p.grad.data d_p.add_(-d_p.mean(dim = tuple(range(1,len(list(d_p.size())))), keepdim = True)) \u8fd9\u79cd\u8bad\u7ec3\u65b9\u5f0f\u8981\u6c42\u7f51\u7edc\u7684\u6743\u91cd\u7684\u5747\u503c\u4e00\u76f4\u662f\u4e0d\u53d8\u7684\uff0c\u4e5f\u5c31\u662fregularize\u4e86\u6743\u91cd\u7684\u641c\u7d22\u7a7a\u95f4.\u4f5c\u8005\u4e5f\u8bc1\u660e\u8fd9\u79cd\u5199\u6cd5\u4f1aregularize feature output space.\u4f46\u662fkaiming init \u8fd8\u6709imagenet pretrained\u7684\u6743\u91cd\u503c\u90fd\u4e0d\u4f1a\u592a\u5927\uff0c\u6240\u4ee5\u8fd9\u4e2aregularization\u4e0d\u4f1a\u8f7b\u6613\u5d29\u3002 Decoupled Weight Decay Regularization pdf code \u8fd9\u7bc7paper\u63d0\u51fa\u7684AdamW\u4e3b\u8981\u7684\u7279\u70b9\u662f\u5c06weight decay\u4ece\u4f18\u5316\u4e2d\u89e3\u8026\u5408\u3002 \u5b9a\u4e49: L2 Regularization\u6307\u7684\u662f\u5728Loss function\u4e0a\u52a0\u4e0aweight square. weight decay\u6307\u7684\u662f\u5728\u66f4\u65b0\u6743\u91cd\u7684\u65f6\u5019\u989d\u5916\u51cf\u53bbweight\u7684\u5927\u5c0f. \u7406\u8bba\u4e0e\u5b9e\u9a8c\u524d\u7f6e\u7ed3\u8bba: L2 Regularization \u4e0e weight decay\u6548\u679c\u6709\u4e0d\u540c\u3002 \u8fd9\u662f\u56e0\u4e3aAdam\u7684\u7279\u6027\u9020\u6210\u7684\uff0c\u5728SGD\u65f6\u662f\u4e00\u6837\u7684\u3002 \u4f7f\u7528Adam\u7684\u65f6\u5019\uff0cL2 regularization\u6548\u679c\u4e0d\u597d\u3002 \u5bf9\u4e8eSGD\u548cAdam, weight decay\u4e00\u6837\u597d.","title":"New Optimizers"},{"location":"Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/#new-optimizers","text":"\u76ee\u5f55\u4e2d\u7684\u4ee3\u7801\u4ed3\u5e93\u6307\u5411Ranger\u5e93\uff0c\u662f\u4e00\u4e2a\u878d\u5408\u4e86\u591a\u4e2aoptimizer\u7684\u5e93\u3002\u5404\u7bc7\u6587\u7ae0\u6709\u81ea\u5df1\u7684\u5b98\u65b9\u5f00\u6e90\u5e93\u3002","title":"New Optimizers"},{"location":"Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/#lookahead-optimizerksteps-forward-1-step-back","text":"pdf code \u672c\u8d28\u4e0a\u6765\u8bf4\u5c31\u662f\u4fdd\u5b58\u4e24\u5957\u53c2\u6570\u503c\uff0c\u7528\u4e00\u5957\u53c2\u6570\u503c\u6b63\u5e38\u66f4\u65b0N\u6b65(\u6bcf\u53d8\u4e00\u6b21\u9700\u8981\u4f7f\u7528\u65b0\u7684\u6743\u503c\u8ba1\u7b97\u66f4\u65b0\u65b9\u5411)\uff0c\u7136\u540e\u7528\u7b2c\u4e8c\u5957\u53c2\u6570\u5f80\u6700\u7ec8\u8fd9N\u6b65\u7684\u603b\u66f4\u65b0\u65b9\u5411\u8d70\u4e00\u6b65","title":"Lookahead Optimizer:ksteps forward, 1 step back"},{"location":"Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/#gradient-centralization-a-new-optimization-technique-for-deep-neural-networks","text":"pdf code \u8ba1\u7b97\u65b9\u5f0f\u5f88\u7b80\u5355\uff0c\u5c31\u662f\u8ba1\u7b97graident\u4e4b\u540e\uff0c\u5728update\u4e4b\u524dnormalize\u5168\u5c40gradient for p in group['params']: d_p = p.grad.data d_p.add_(-d_p.mean(dim = tuple(range(1,len(list(d_p.size())))), keepdim = True)) \u8fd9\u79cd\u8bad\u7ec3\u65b9\u5f0f\u8981\u6c42\u7f51\u7edc\u7684\u6743\u91cd\u7684\u5747\u503c\u4e00\u76f4\u662f\u4e0d\u53d8\u7684\uff0c\u4e5f\u5c31\u662fregularize\u4e86\u6743\u91cd\u7684\u641c\u7d22\u7a7a\u95f4.\u4f5c\u8005\u4e5f\u8bc1\u660e\u8fd9\u79cd\u5199\u6cd5\u4f1aregularize feature output space.\u4f46\u662fkaiming init \u8fd8\u6709imagenet pretrained\u7684\u6743\u91cd\u503c\u90fd\u4e0d\u4f1a\u592a\u5927\uff0c\u6240\u4ee5\u8fd9\u4e2aregularization\u4e0d\u4f1a\u8f7b\u6613\u5d29\u3002","title":"Gradient Centralization: A New Optimization Technique for Deep Neural Networks"},{"location":"Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/#decoupled-weight-decay-regularization","text":"pdf code \u8fd9\u7bc7paper\u63d0\u51fa\u7684AdamW\u4e3b\u8981\u7684\u7279\u70b9\u662f\u5c06weight decay\u4ece\u4f18\u5316\u4e2d\u89e3\u8026\u5408\u3002 \u5b9a\u4e49: L2 Regularization\u6307\u7684\u662f\u5728Loss function\u4e0a\u52a0\u4e0aweight square. weight decay\u6307\u7684\u662f\u5728\u66f4\u65b0\u6743\u91cd\u7684\u65f6\u5019\u989d\u5916\u51cf\u53bbweight\u7684\u5927\u5c0f. \u7406\u8bba\u4e0e\u5b9e\u9a8c\u524d\u7f6e\u7ed3\u8bba: L2 Regularization \u4e0e weight decay\u6548\u679c\u6709\u4e0d\u540c\u3002 \u8fd9\u662f\u56e0\u4e3aAdam\u7684\u7279\u6027\u9020\u6210\u7684\uff0c\u5728SGD\u65f6\u662f\u4e00\u6837\u7684\u3002 \u4f7f\u7528Adam\u7684\u65f6\u5019\uff0cL2 regularization\u6548\u679c\u4e0d\u597d\u3002 \u5bf9\u4e8eSGD\u548cAdam, weight decay\u4e00\u6837\u597d.","title":"Decoupled Weight Decay Regularization"},{"location":"Building_Blocks/MDEQ/","text":"Multiscale Deep Equilibrium Models \u6211\u5728\u8fd9\u7bc7paper\u91cc\u9762\u8bfb\u5230\u4e86\u4e00\u4e2a\u53eb\u505a Implicit Deep Learning \u7684\u6982\u5ff5\u3002\u76ee\u524d\u4e3b\u8981\u7684\u6df1\u5ea6\u5b66\u4e60\u7f51\u7edc\uff0c\u5b83\u4eec\u7684forward pass\u662f\u9884\u8bbe\u5b9a\u597d\u7684\uff0c\u800cbackward pass\u5219\u662f\u4e00\u53e5forward\u7684\u8fd0\u7b97\u56fe\u53cd\u5411\u8ba1\u7b97\u3002\u800cImplicit Deep Learning\u7684\u7279\u70b9\u5728\u4e8eforward pass\u7684\u8fd0\u7b97\u5e76\u4e0d\u662f\u5b8c\u5168\u9884\u5148\u5b9a\u4e49\u597d\u7684\uff0c\u800c\u662f\u4f1a\u6709\u4e00\u4e2a\u7ec8\u6001\u6807\u51c6\uff0c\u7f51\u7edc\u5728\u5b9e\u73b0\u8fd9\u4e00\u7ec8\u6001\u6807\u51c6\u4e4b\u540e\u5b8c\u6210\u8fd9\u4e00\u5c42\u7684\u8fd0\u7b97\uff0c\u800c\u7f51\u7edc\u7684backward pass\u5219\u662f\u6839\u636e\u8fd9\u4e00\u7ec8\u6001\u6807\u51c6\u83b7\u53d6\u7684\uff0c\u4e0eforward pass\u7684\u8fc7\u7a0b\u65e0\u5173\u3002 Deep Equilibrium \u6df1\u5ea6\u5747\u8861 Motivation: \u5bf9\u4e8e\u4e00\u4e2a L \u5c42\u7684\u6743\u91cd\u5171\u4eab\u7684\u7f51\u7edc\u6765\u8bf4\uff0c\u5b83\u7684\u8fd0\u7b97\u53ef\u4ee5\u8868\u8fbe\u4e3a z^{[i+1]} = f_\\theta(z^{[i]};x) \u5982\u679c L \u6570\u91cf\u8d8b\u8fd1\u4e8e\u65e0\u7a77\uff0c\u5219\u6700\u7ec8\u7ed3\u679c\u4f1a\u8d8b\u5411\u4e8e\u4e00\u4e2a\u5747\u8861\u6001 z^* = f_\\theta(z^*;x) .\u8fd9\u4e2a\u8bbe\u5b9a\u7684\u597d\u5904\u6709\u4e8c\uff0c\u9996\u5148\u662fForward\u7ed3\u679c\u8ba1\u7b97\u53ef\u4ee5\u88ab\u7406\u89e3\u4e3a\u4e00\u4e2a\u4e0d\u52a8\u70b9\u7684\u5bfb\u627e z^* = Rootfind(f_\\theta(z;x) - z) ,\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u725b\u987f\u6cd5\u903c\u8fd1\u4e0d\u52a8\u70b9\u7684\u6839\u3002\u5176\u6b21\uff0c\u5728backward\u7684\u65f6\u5019\u53ef\u4ee5\u76f4\u63a5\u5bf9\u7a33\u5b9a\u6001\u8fdb\u884c\u53cd\u4f20 \\frac{\\partial \\ell}{\\partial \\theta}=\\frac{\\partial \\ell}{\\partial \\mathbf{z}^{\\star}}\\left(-\\left.J_{g_{\\theta}}^{-1}\\right|_{\\mathbf{z}^{\\star}}\\right) \\frac{\\partial f_{\\theta}\\left(\\mathbf{z}^{\\star} ; \\mathbf{x}\\right)}{\\partial \\theta} \\quad \\frac{\\partial \\ell}{\\partial \\mathbf{x}}=\\frac{\\partial \\ell}{\\partial \\mathbf{z}^{\\star}}\\left(-\\left.J_{g_{\\theta}}^{-1}\\right|_{\\mathbf{z}^{\\star}}\\right) \\frac{\\partial f_{\\theta}\\left(\\mathbf{z}^{\\star} ; \\mathbf{x}\\right)}{\\partial \\mathbf{x}} \u7531\u4e8e\u96c5\u514b\u6bd4\u77e9\u9635\u8ba1\u7b97\u7684\u590d\u6742\u6027\uff0c\u524d\u6587\u4ec5\u5728\u5e8f\u5217\u6a21\u578b\u4e2d\u5c1d\u8bd5\uff0c\u672c\u6587\u5c1d\u8bd5\u5c06\u5176\u6269\u5c55\u5230\u56fe\u7247\u4e2d \u7ed3\u6784 \u4ee3\u7801\u5448\u73b0\u7684\u903b\u8f91\u662f\u628amulti-scale resolution\u7406\u89e3\u4e3a\u4e00\u4e2a\u65e0\u9650\u6b21\u7684convolution block\uff0c\u6c42\u89e3\u5668\u9700\u8981\u505a\u7684\u662f\u5c31\u662f\u6c42\u51fa\u56fe\u4e2d f_\\theta \u7684\u4e0d\u52a8\u70b9,\u5bf9\u4e8epytorch\u51fd\u6570g\u7684\u6838\u5fc3\u4ee3\u7801\u5728 broyden.py ,\u4ee3\u7801\u5b8c\u5168\u4e3apython/pytorch\u8ba1\u7b97\u3002","title":"Multiscale Deep Equilibrium Models"},{"location":"Building_Blocks/MDEQ/#multiscale-deep-equilibrium-models","text":"\u6211\u5728\u8fd9\u7bc7paper\u91cc\u9762\u8bfb\u5230\u4e86\u4e00\u4e2a\u53eb\u505a Implicit Deep Learning \u7684\u6982\u5ff5\u3002\u76ee\u524d\u4e3b\u8981\u7684\u6df1\u5ea6\u5b66\u4e60\u7f51\u7edc\uff0c\u5b83\u4eec\u7684forward pass\u662f\u9884\u8bbe\u5b9a\u597d\u7684\uff0c\u800cbackward pass\u5219\u662f\u4e00\u53e5forward\u7684\u8fd0\u7b97\u56fe\u53cd\u5411\u8ba1\u7b97\u3002\u800cImplicit Deep Learning\u7684\u7279\u70b9\u5728\u4e8eforward pass\u7684\u8fd0\u7b97\u5e76\u4e0d\u662f\u5b8c\u5168\u9884\u5148\u5b9a\u4e49\u597d\u7684\uff0c\u800c\u662f\u4f1a\u6709\u4e00\u4e2a\u7ec8\u6001\u6807\u51c6\uff0c\u7f51\u7edc\u5728\u5b9e\u73b0\u8fd9\u4e00\u7ec8\u6001\u6807\u51c6\u4e4b\u540e\u5b8c\u6210\u8fd9\u4e00\u5c42\u7684\u8fd0\u7b97\uff0c\u800c\u7f51\u7edc\u7684backward pass\u5219\u662f\u6839\u636e\u8fd9\u4e00\u7ec8\u6001\u6807\u51c6\u83b7\u53d6\u7684\uff0c\u4e0eforward pass\u7684\u8fc7\u7a0b\u65e0\u5173\u3002","title":"Multiscale Deep Equilibrium Models"},{"location":"Building_Blocks/MDEQ/#deep-equilibrium","text":"","title":"Deep Equilibrium \u6df1\u5ea6\u5747\u8861"},{"location":"Building_Blocks/MDEQ/#motivation","text":"\u5bf9\u4e8e\u4e00\u4e2a L \u5c42\u7684\u6743\u91cd\u5171\u4eab\u7684\u7f51\u7edc\u6765\u8bf4\uff0c\u5b83\u7684\u8fd0\u7b97\u53ef\u4ee5\u8868\u8fbe\u4e3a z^{[i+1]} = f_\\theta(z^{[i]};x) \u5982\u679c L \u6570\u91cf\u8d8b\u8fd1\u4e8e\u65e0\u7a77\uff0c\u5219\u6700\u7ec8\u7ed3\u679c\u4f1a\u8d8b\u5411\u4e8e\u4e00\u4e2a\u5747\u8861\u6001 z^* = f_\\theta(z^*;x) .\u8fd9\u4e2a\u8bbe\u5b9a\u7684\u597d\u5904\u6709\u4e8c\uff0c\u9996\u5148\u662fForward\u7ed3\u679c\u8ba1\u7b97\u53ef\u4ee5\u88ab\u7406\u89e3\u4e3a\u4e00\u4e2a\u4e0d\u52a8\u70b9\u7684\u5bfb\u627e z^* = Rootfind(f_\\theta(z;x) - z) ,\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u725b\u987f\u6cd5\u903c\u8fd1\u4e0d\u52a8\u70b9\u7684\u6839\u3002\u5176\u6b21\uff0c\u5728backward\u7684\u65f6\u5019\u53ef\u4ee5\u76f4\u63a5\u5bf9\u7a33\u5b9a\u6001\u8fdb\u884c\u53cd\u4f20 \\frac{\\partial \\ell}{\\partial \\theta}=\\frac{\\partial \\ell}{\\partial \\mathbf{z}^{\\star}}\\left(-\\left.J_{g_{\\theta}}^{-1}\\right|_{\\mathbf{z}^{\\star}}\\right) \\frac{\\partial f_{\\theta}\\left(\\mathbf{z}^{\\star} ; \\mathbf{x}\\right)}{\\partial \\theta} \\quad \\frac{\\partial \\ell}{\\partial \\mathbf{x}}=\\frac{\\partial \\ell}{\\partial \\mathbf{z}^{\\star}}\\left(-\\left.J_{g_{\\theta}}^{-1}\\right|_{\\mathbf{z}^{\\star}}\\right) \\frac{\\partial f_{\\theta}\\left(\\mathbf{z}^{\\star} ; \\mathbf{x}\\right)}{\\partial \\mathbf{x}} \u7531\u4e8e\u96c5\u514b\u6bd4\u77e9\u9635\u8ba1\u7b97\u7684\u590d\u6742\u6027\uff0c\u524d\u6587\u4ec5\u5728\u5e8f\u5217\u6a21\u578b\u4e2d\u5c1d\u8bd5\uff0c\u672c\u6587\u5c1d\u8bd5\u5c06\u5176\u6269\u5c55\u5230\u56fe\u7247\u4e2d","title":"Motivation:"},{"location":"Building_Blocks/MDEQ/#_1","text":"\u4ee3\u7801\u5448\u73b0\u7684\u903b\u8f91\u662f\u628amulti-scale resolution\u7406\u89e3\u4e3a\u4e00\u4e2a\u65e0\u9650\u6b21\u7684convolution block\uff0c\u6c42\u89e3\u5668\u9700\u8981\u505a\u7684\u662f\u5c31\u662f\u6c42\u51fa\u56fe\u4e2d f_\\theta \u7684\u4e0d\u52a8\u70b9,\u5bf9\u4e8epytorch\u51fd\u6570g\u7684\u6838\u5fc3\u4ee3\u7801\u5728 broyden.py ,\u4ee3\u7801\u5b8c\u5168\u4e3apython/pytorch\u8ba1\u7b97\u3002","title":"\u7ed3\u6784"},{"location":"Building_Blocks/MLP_image_classification/","text":"MLP in Image Classification \u4f7f\u7528\u5168\u8fde\u63a5\u5c42\u5b9e\u73b0Imagenet\u7ea7\u522b\u7684\u56fe\u7247\u5206\u7c7b\u3002\u8fd9\u4e2aIdea\u5728\u76f8\u8fd1\u7684\u65f6\u95f4\u5185\u88ab\u591a\u4e2a\u4e0d\u540c\u7684\u7ec4\u522b\u5b9e\u73b0\u5e76\u8bad\u7ec3\u51fa\u6210\u679c\uff0c\u4ed6\u4eec\u51e0\u4e4e\u662f\u540c\u65f6\u53d1\u5e03\u4e86\u6587\u7ae0. \u8981\u7406\u89e3\u8fd9\u4e00\u8d8b\u52bf\uff0c\u9996\u5148\u53d1\u73b0\u7684\u662f ICLR \u91cc\u4f7f\u7528Visual Transformer\u76f4\u63a5\u8fdb\u884c\u56fe\u7247\u5206\u7c7b\u7684work\uff0c\u8fd9\u4e2a\u65b9\u6cd5\u63a5\u8fd1\u4e8ebag of positional-encoded words\u8fdb\u884c\u5206\u7c7b\uff0c\u7ed3\u679c\u51fa\u6765\u7ed3\u679c\u8fd8\u4e0d\u9519\u3002 \u6709\u5145\u5206\u7b97\u529b\u7684\u51e0\u4e2a\u5927\u7ec4\uff0cFacebook/Google/\u725b\u6d25\u5728\u76f8\u8fd1\u7684\u65f6\u95f4\u5185\u5206\u5e03\u4e86\u4e09\u4e2a\u6587\u7ae0\uff0c\u90fd\u662f\u4f7f\u7528\u5168\u8fde\u63a5\u5c42\u76f4\u63a5\u5b9e\u73b0\u56fe\u7247\u5206\u7c7b\u3002 Do You Even Need Attention? A Stack of Feed-Forward Layers Does Surprisingly Well on ImageNet pdf code \u725b\u6d25\u7684\u65b9\u6848\u975e\u5e38\u7b80\u5355\uff0c\u5c06Transformer\u91cc\u9762\u7684Attention\u90e8\u5206\u76f4\u63a5\u53d8\u6210\u5168\u8fde\u63a5\u5c42\u5c31\u53ef\u4ee5\u4e86. ResMLP: Feedforward networks for image classification with data-efficient training pdf \u8fd9\u7bc7\u6587\u7ae0\u5e26\u6709\u66f4\u591a\u7684\u6b8b\u5dee\u94fe\u63a5\uff0c\u5168\u8fde\u63a5\u5c42\u4e5f\u66f4\u591a\u3002\u672c\u6587\u58f0\u79f0\u8bad\u7ec3\u8fc7\u7a0b\u6bd4ViT\u8981\u66f4\u7a33\u5b9a\u3002 MLP-Mixer: An all-MLP Architecture for Vision pdf code \u672c\u6587\u63d0\u51fa\u4e86\u4e24\u79cdMLP\u5c42\uff0c\u4e00\u4e2a\u662f channel-mixing MLP , \u4e00\u4e2a\u662f token-mixing MLP . channel\u6307\u7684\u662f\u6bcf\u4e00\u4e2apatch\u533a\u57df\u7684\u7279\u5f81\u7684\u878d\u5408\uff0ctoken\u6307\u7684\u662f\u4e0d\u540cpatch\u533a\u57df\u4e4b\u95f4\u7684\u76f8\u4e92\u4fe1\u606f\u878d\u5408\u3002","title":"MLP in Image Classification"},{"location":"Building_Blocks/MLP_image_classification/#mlp-in-image-classification","text":"\u4f7f\u7528\u5168\u8fde\u63a5\u5c42\u5b9e\u73b0Imagenet\u7ea7\u522b\u7684\u56fe\u7247\u5206\u7c7b\u3002\u8fd9\u4e2aIdea\u5728\u76f8\u8fd1\u7684\u65f6\u95f4\u5185\u88ab\u591a\u4e2a\u4e0d\u540c\u7684\u7ec4\u522b\u5b9e\u73b0\u5e76\u8bad\u7ec3\u51fa\u6210\u679c\uff0c\u4ed6\u4eec\u51e0\u4e4e\u662f\u540c\u65f6\u53d1\u5e03\u4e86\u6587\u7ae0. \u8981\u7406\u89e3\u8fd9\u4e00\u8d8b\u52bf\uff0c\u9996\u5148\u53d1\u73b0\u7684\u662f ICLR \u91cc\u4f7f\u7528Visual Transformer\u76f4\u63a5\u8fdb\u884c\u56fe\u7247\u5206\u7c7b\u7684work\uff0c\u8fd9\u4e2a\u65b9\u6cd5\u63a5\u8fd1\u4e8ebag of positional-encoded words\u8fdb\u884c\u5206\u7c7b\uff0c\u7ed3\u679c\u51fa\u6765\u7ed3\u679c\u8fd8\u4e0d\u9519\u3002 \u6709\u5145\u5206\u7b97\u529b\u7684\u51e0\u4e2a\u5927\u7ec4\uff0cFacebook/Google/\u725b\u6d25\u5728\u76f8\u8fd1\u7684\u65f6\u95f4\u5185\u5206\u5e03\u4e86\u4e09\u4e2a\u6587\u7ae0\uff0c\u90fd\u662f\u4f7f\u7528\u5168\u8fde\u63a5\u5c42\u76f4\u63a5\u5b9e\u73b0\u56fe\u7247\u5206\u7c7b\u3002","title":"MLP in Image Classification"},{"location":"Building_Blocks/MLP_image_classification/#do-you-even-need-attention-a-stack-of-feed-forward-layers-does-surprisingly-well-on-imagenet","text":"pdf code \u725b\u6d25\u7684\u65b9\u6848\u975e\u5e38\u7b80\u5355\uff0c\u5c06Transformer\u91cc\u9762\u7684Attention\u90e8\u5206\u76f4\u63a5\u53d8\u6210\u5168\u8fde\u63a5\u5c42\u5c31\u53ef\u4ee5\u4e86.","title":"Do You Even Need Attention? A Stack of Feed-Forward Layers Does Surprisingly Well on ImageNet"},{"location":"Building_Blocks/MLP_image_classification/#resmlp-feedforward-networks-for-image-classification-with-data-efficient-training","text":"pdf \u8fd9\u7bc7\u6587\u7ae0\u5e26\u6709\u66f4\u591a\u7684\u6b8b\u5dee\u94fe\u63a5\uff0c\u5168\u8fde\u63a5\u5c42\u4e5f\u66f4\u591a\u3002\u672c\u6587\u58f0\u79f0\u8bad\u7ec3\u8fc7\u7a0b\u6bd4ViT\u8981\u66f4\u7a33\u5b9a\u3002","title":"ResMLP: Feedforward networks for image classification with data-efficient training"},{"location":"Building_Blocks/MLP_image_classification/#mlp-mixer-an-all-mlp-architecture-for-vision","text":"pdf code \u672c\u6587\u63d0\u51fa\u4e86\u4e24\u79cdMLP\u5c42\uff0c\u4e00\u4e2a\u662f channel-mixing MLP , \u4e00\u4e2a\u662f token-mixing MLP . channel\u6307\u7684\u662f\u6bcf\u4e00\u4e2apatch\u533a\u57df\u7684\u7279\u5f81\u7684\u878d\u5408\uff0ctoken\u6307\u7684\u662f\u4e0d\u540cpatch\u533a\u57df\u4e4b\u95f4\u7684\u76f8\u4e92\u4fe1\u606f\u878d\u5408\u3002","title":"MLP-Mixer: An all-MLP Architecture for Vision"},{"location":"Building_Blocks/NRI/","text":"Neural Relational Inference for Interacting Systems \u8fd9\u7bc7paper\u7684\u4f5c\u8005\u5199\u4e86\u4e00\u4e2a tutorial \u8fd9\u4e2a\u6a21\u578b\u7684\u76ee\u7684\u5c31\u662f\u62df\u5408\u4e00\u4e2a\u7cfb\u7edf\u7684\u7269\u7406\u6a21\u578b\uff0c \u540c\u65f6\u8981\u6c42\u663e\u793a\u5730\u5b66\u4e60\u4e0d\u540c\u8282\u70b9(states)\u4e4b\u95f4\u7684\u76f8\u4e92\u7ea6\u675f.\u6b63\u5982\u4e0a\u65b9\u7684\u89c6\u9891. Neural Relational Inference Model \u7f51\u7edc\u7ed3\u6784\u5982\u56fe\uff0c\u6574\u4e2a\u7cfb\u7edf\u91c7\u7528\u7684\u662f\u4e00\u4e2avariational autoencoder\u7684\u8bbe\u8ba1. \u5176\u4e2d v\\rightarrow e \u7684\u601d\u8def\u662f\u8bf4\u5c06 N \u4e2a\u8282\u70b9\u62fc\u63a5\u8d77\u6765\uff0c\u5f62\u6210\u4e00\u4e2a N\\times N \u7684\u77e9\u9635 (\u4e3b\u5bf9\u89d2\u7ebf\u90e8\u5206\u4e3a\u96f6). e\\rightarrow v \u7684\u601d\u8def\u662f\u5c06\u884c\u6c42\u548c. \u672c\u6587\u8fd9\u91cc\u5bf9 q_\\phi(z_{ij}|x) \u8bbe\u5b9a\u4e3a q_\\phi(z_{ij}|x) = \\text{softmax}(f_{enc,\\phi}(x)_{ij, 1:K})","title":"Neural Relational Inference for Interacting Systems"},{"location":"Building_Blocks/NRI/#neural-relational-inference-for-interacting-systems","text":"\u8fd9\u7bc7paper\u7684\u4f5c\u8005\u5199\u4e86\u4e00\u4e2a tutorial \u8fd9\u4e2a\u6a21\u578b\u7684\u76ee\u7684\u5c31\u662f\u62df\u5408\u4e00\u4e2a\u7cfb\u7edf\u7684\u7269\u7406\u6a21\u578b\uff0c \u540c\u65f6\u8981\u6c42\u663e\u793a\u5730\u5b66\u4e60\u4e0d\u540c\u8282\u70b9(states)\u4e4b\u95f4\u7684\u76f8\u4e92\u7ea6\u675f.\u6b63\u5982\u4e0a\u65b9\u7684\u89c6\u9891.","title":"Neural Relational Inference for Interacting Systems"},{"location":"Building_Blocks/NRI/#neural-relational-inference-model","text":"\u7f51\u7edc\u7ed3\u6784\u5982\u56fe\uff0c\u6574\u4e2a\u7cfb\u7edf\u91c7\u7528\u7684\u662f\u4e00\u4e2avariational autoencoder\u7684\u8bbe\u8ba1. \u5176\u4e2d v\\rightarrow e \u7684\u601d\u8def\u662f\u8bf4\u5c06 N \u4e2a\u8282\u70b9\u62fc\u63a5\u8d77\u6765\uff0c\u5f62\u6210\u4e00\u4e2a N\\times N \u7684\u77e9\u9635 (\u4e3b\u5bf9\u89d2\u7ebf\u90e8\u5206\u4e3a\u96f6). e\\rightarrow v \u7684\u601d\u8def\u662f\u5c06\u884c\u6c42\u548c. \u672c\u6587\u8fd9\u91cc\u5bf9 q_\\phi(z_{ij}|x) \u8bbe\u5b9a\u4e3a q_\\phi(z_{ij}|x) = \\text{softmax}(f_{enc,\\phi}(x)_{ij, 1:K})","title":"Neural Relational Inference Model"},{"location":"Building_Blocks/Non-local_Neural_Networks/","text":"Non-local Neural Networks non-local\u5355\u5143\u662f\u53e6\u4e00\u4e2a\u53ef\u4ee5\u663e\u8457\u63d0\u9ad83D,2D\u5377\u79ef\u7f51\u7edc\u7684\u611f\u53d7\u91ce \u6a21\u5757\u7ed3\u6784 \u56fe\u4e2d\u7ed9\u51fa\u7684\u662f\u7b80\u6d01\u660e\u4e86\u7684non-local\u6a21\u5757\u7684\u5177\u4f53\u7b97\u6cd5\u3002\u6bcf\u4e00\u4e2a\u84dd\u8272\u7684[1x1x1]\u8868\u660e\u4e00\u6b21channel-wise Conv\u3002 \u4ee5\u4e0b\u4e3a\u51e0\u4e2a\u76f4\u89c9\u4ee5\u53ca\u5f15\u7533 1. \u5728\u56fe\u4e2d\u5220\u9664T\u7ef4\u5ea6\u4e0d\u5f71\u54cd\u5168\u56fe\u7ed3\u6784\u4e0e\u53ef\u884c\u6027\uff0c\u4e5f\u4e0d\u5f71\u54cd\u6548\u679c\uff0c\u56e0\u800cnon-local\u4e5f\u53ef\u4ee5\u7528\u4e8e\u56fe\u7247\u3002 2. non_local\u6838\u5fc3\u662f f(x_i,x_j) = e^{x_i^T x_j} ,\u76f4\u89c9\u4e0a\u6765\u8bf4\uff0c\u4f1a\u8ba9\u4e0d\u540c\u65f6\u7a7a\u4f4d\u7f6e\u4e0a\u76f8\u4f3c\u7684\u7279\u5f81\u5f97\u5230\u8f83\u9ad8\u7684\u6fc0\u6d3b\u503c\uff0c\u5728\u540e\u9762\u4e5f\u5c31\u4f7f\u5f97\u56fe\u50cf\u4e0a\u4e00\u4e2a\u70b9\u7684feature map\u53ef\u4ee5\u878d\u5408\u4e0e\u5b83\u76f8\u4f3c\u7684\u5730\u65b9\u7684\u4fe1\u606f\u3002\u5728\u89c6\u9891\u5904\u7406\u4e2d\u6709\u4e00\u4e2a\u8f85\u52a9\u7269\u4f53\u8ddf\u8e2a\u7684\u6548\u679c\u3002 3. \u5f53feature map\u957f\u5bbd\u6bd4\u8f83\u5c0f\u7684\u65f6\u5019\u6548\u679c\u597d\u4e14\u8fd0\u7b97\u901f\u5ea6\u5feb\uff0c\u4f46\u662f\u5f53\u957f\u5bbd\u5f88\u5927\u7684\u65f6\u5019\u8fd0\u7b97\u4f1a\u5f88\u6162\uff0c\u56e0\u4e3a\u4f1a\u6709\u6570\u4e07\u7ef4\u7684\u77e9\u9635\u76f8\u4e58\uff0c\u8fd0\u7b97\u56fe\u4e5f\u4f1a\u590d\u6742\u3002 4. \u53ef\u4ee5\u9002\u5f53\u5220\u6539\u91cc\u9762\u51e0\u4e2a\u5377\u79ef\u64cd\u4f5c\u4ee5\u53casoftmax\u64cd\u4f5c\u3002","title":"Non-local Neural Networks"},{"location":"Building_Blocks/Non-local_Neural_Networks/#non-local-neural-networks","text":"non-local\u5355\u5143\u662f\u53e6\u4e00\u4e2a\u53ef\u4ee5\u663e\u8457\u63d0\u9ad83D,2D\u5377\u79ef\u7f51\u7edc\u7684\u611f\u53d7\u91ce","title":"Non-local Neural Networks"},{"location":"Building_Blocks/Non-local_Neural_Networks/#_1","text":"\u56fe\u4e2d\u7ed9\u51fa\u7684\u662f\u7b80\u6d01\u660e\u4e86\u7684non-local\u6a21\u5757\u7684\u5177\u4f53\u7b97\u6cd5\u3002\u6bcf\u4e00\u4e2a\u84dd\u8272\u7684[1x1x1]\u8868\u660e\u4e00\u6b21channel-wise Conv\u3002 \u4ee5\u4e0b\u4e3a\u51e0\u4e2a\u76f4\u89c9\u4ee5\u53ca\u5f15\u7533 1. \u5728\u56fe\u4e2d\u5220\u9664T\u7ef4\u5ea6\u4e0d\u5f71\u54cd\u5168\u56fe\u7ed3\u6784\u4e0e\u53ef\u884c\u6027\uff0c\u4e5f\u4e0d\u5f71\u54cd\u6548\u679c\uff0c\u56e0\u800cnon-local\u4e5f\u53ef\u4ee5\u7528\u4e8e\u56fe\u7247\u3002 2. non_local\u6838\u5fc3\u662f f(x_i,x_j) = e^{x_i^T x_j} ,\u76f4\u89c9\u4e0a\u6765\u8bf4\uff0c\u4f1a\u8ba9\u4e0d\u540c\u65f6\u7a7a\u4f4d\u7f6e\u4e0a\u76f8\u4f3c\u7684\u7279\u5f81\u5f97\u5230\u8f83\u9ad8\u7684\u6fc0\u6d3b\u503c\uff0c\u5728\u540e\u9762\u4e5f\u5c31\u4f7f\u5f97\u56fe\u50cf\u4e0a\u4e00\u4e2a\u70b9\u7684feature map\u53ef\u4ee5\u878d\u5408\u4e0e\u5b83\u76f8\u4f3c\u7684\u5730\u65b9\u7684\u4fe1\u606f\u3002\u5728\u89c6\u9891\u5904\u7406\u4e2d\u6709\u4e00\u4e2a\u8f85\u52a9\u7269\u4f53\u8ddf\u8e2a\u7684\u6548\u679c\u3002 3. \u5f53feature map\u957f\u5bbd\u6bd4\u8f83\u5c0f\u7684\u65f6\u5019\u6548\u679c\u597d\u4e14\u8fd0\u7b97\u901f\u5ea6\u5feb\uff0c\u4f46\u662f\u5f53\u957f\u5bbd\u5f88\u5927\u7684\u65f6\u5019\u8fd0\u7b97\u4f1a\u5f88\u6162\uff0c\u56e0\u4e3a\u4f1a\u6709\u6570\u4e07\u7ef4\u7684\u77e9\u9635\u76f8\u4e58\uff0c\u8fd0\u7b97\u56fe\u4e5f\u4f1a\u590d\u6742\u3002 4. \u53ef\u4ee5\u9002\u5f53\u5220\u6539\u91cc\u9762\u51e0\u4e2a\u5377\u79ef\u64cd\u4f5c\u4ee5\u53casoftmax\u64cd\u4f5c\u3002","title":"\u6a21\u5757\u7ed3\u6784"},{"location":"Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/","text":"On Multiplicative Integration with Recurrent Neural Networks \u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e00\u4e2a\u5728CAPs\u6a21\u578b\u63d0\u51fa\u7684 \u6587\u7ae0 \u4e2d\u4f7f\u7528\u7684Multiplicative Integration RNN(LSTM, GRU) \u672c\u8d28\u516c\u5f0f\u5c31\u662f \\phi\\left(\\boldsymbol{\\alpha} \\odot \\mathbf{W} \\boldsymbol{x} \\odot \\mathbf{U} \\boldsymbol{z}+\\boldsymbol{\\beta}_{1} \\odot \\mathbf{U} \\boldsymbol{z}+\\boldsymbol{\\beta}_{2} \\odot \\mathbf{W} \\boldsymbol{x}+\\boldsymbol{b}\\right) \u5176\u4e2d \\odot \u8868\u793a\u7684\u662f\u76f8\u540c\u5f62\u72b6\u7684\u77e9\u9635\u4e4b\u95f4\u7684\u5143\u7d20\u5bf9\u5e94\u4e58\u79ef\uff0c\u79f0\u4e3aHadamard product\u3002 \u76f4\u89c9\u662f\u4fc3\u8fdb\u4e86\u4e24\u4e2a\u4fe1\u606f\u6e90 x, z \u7684\u4fe1\u606f\u4ea4\u6d41\uff0c\u540c\u65f6\u4f18\u5316 Uz \u7684\u68af\u5ea6\u6d41\u901a,\u8fd9\u4e2a\u5728\u65f6\u5e8f\u6a21\u578b\u4e2d\u8f83\u4e3a\u91cd\u8981\u3002 \u8fd9\u4e2a github repo\u63d0\u4f9b\u4e86keras\u7684\u6267\u884c\u3002 \u5177\u4f53\u516c\u5f0f\u7684\u66ff\u6362\u5982\u4e0b:","title":"On Multiplicative Integration with Recurrent Neural Networks"},{"location":"Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/#on-multiplicative-integration-with-recurrent-neural-networks","text":"\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e00\u4e2a\u5728CAPs\u6a21\u578b\u63d0\u51fa\u7684 \u6587\u7ae0 \u4e2d\u4f7f\u7528\u7684Multiplicative Integration RNN(LSTM, GRU) \u672c\u8d28\u516c\u5f0f\u5c31\u662f \\phi\\left(\\boldsymbol{\\alpha} \\odot \\mathbf{W} \\boldsymbol{x} \\odot \\mathbf{U} \\boldsymbol{z}+\\boldsymbol{\\beta}_{1} \\odot \\mathbf{U} \\boldsymbol{z}+\\boldsymbol{\\beta}_{2} \\odot \\mathbf{W} \\boldsymbol{x}+\\boldsymbol{b}\\right) \u5176\u4e2d \\odot \u8868\u793a\u7684\u662f\u76f8\u540c\u5f62\u72b6\u7684\u77e9\u9635\u4e4b\u95f4\u7684\u5143\u7d20\u5bf9\u5e94\u4e58\u79ef\uff0c\u79f0\u4e3aHadamard product\u3002 \u76f4\u89c9\u662f\u4fc3\u8fdb\u4e86\u4e24\u4e2a\u4fe1\u606f\u6e90 x, z \u7684\u4fe1\u606f\u4ea4\u6d41\uff0c\u540c\u65f6\u4f18\u5316 Uz \u7684\u68af\u5ea6\u6d41\u901a,\u8fd9\u4e2a\u5728\u65f6\u5e8f\u6a21\u578b\u4e2d\u8f83\u4e3a\u91cd\u8981\u3002 \u8fd9\u4e2a github repo\u63d0\u4f9b\u4e86keras\u7684\u6267\u884c\u3002 \u5177\u4f53\u516c\u5f0f\u7684\u66ff\u6362\u5982\u4e0b:","title":"On Multiplicative Integration with Recurrent Neural Networks"},{"location":"Building_Blocks/PointAtrousNet/","text":"PointAtrousNet: Point Atrous Convolution for Point Cloud Analysis \u8fd9\u7bc7\u8bba\u6587\u5c06\u70b9\u4e91\u7684\u7a7a\u6d1e\u5377\u79ef\u5f15\u5165\u70b9\u4e91\u5206\u6790\u4e2d\uff0c\u503c\u5f97\u7559\u610f\u7684\u662f\u7a7a\u6d1e\u5377\u79ef\u5728 \u8fd9\u7bc7\u6587\u7ae0 \u4e2d\u5df2\u7ecf\u6709\u63d0\u53ca\uff0c\u4e14\u672c\u6587\u5e76\u6ca1\u6709\u5f15\u7528\u5b83\uff0c\u8fd9\u91cc\u53ef\u80fd\u6d89\u53ca\u4e00\u4e9b\u65f6\u95f4\u4e0a\u7684\u5dee\u522b\uff0c\u4e24\u8005\u5728\u70b9\u4e91\u7a7a\u6d1e\u5377\u79ef\u4e0a\u7684\u8ba1\u7b97\u8f83\u4e3a\u76f8\u4f3c\uff0c\u4f46\u662f\u672c\u6587\u8fdb\u4e00\u6b65\u63d0\u51fa\u4e86\u70b9\u4e91\u4e2d\u7684\u7a7a\u6d1e\u6c60\u5316\uff0c\u53ef\u4ee5\u4e00\u770b\u3002 \u70b9\u4e91\u7a7a\u6d1e\u5377\u79ef \u4e0e \u8fd9\u7bc7\u6587\u7ae0 \u76f8\u4f3c\uff0c\u5728\u6bcf\u4e00\u4e2a\u70b9\u9644\u8fd1\u5bfb\u627e\u6700\u9760\u8fd1\u7684K\u4e2a\u70b9,\u8fd9\u91cc\u4e0e\u524d\u6587\u4e0d\u540c\u7684\u5730\u65b9\u5728\u4e8e\u8fd9\u91cc\u91c7\u7528\u7684\u662fedge convolution\u7684\u64cd\u4f5c\uff0c\u4e0e DGCNN.pdf \u7c7b\u4f3c\u7684\u8fd0\u7b97\uff0c \u7b80\u4ecb X_{p}^{\\prime}=g\\left(H_{\\Theta}\\left(X_{p}, X_{q_{r}}\\right), \\ldots, H_{\\Theta}\\left(X_{p}, X_{\\left.q_{(r, k)}\\right)}\\right)\\right. \u5176\u4e2d H_{\\Theta} = h_{\\theta}\\left(X_{p} \\oplus\\left(X_{p}-X_{q_{i}}\\right)\\right) \u5bfb\u627e\u4e34\u8fd1\u70b9\u7684\u65b9\u6cd5\u4e5f\u662f\u6784\u5efaKD\u6811. \u70b9\u4e91\u7a7a\u6d1e\u6c60\u5316 \\begin{aligned} X_{p}^{\\prime} &=X_{p 1}^{\\prime} \\oplus X_{p 2}^{\\prime} \\oplus X_{p 3}^{\\prime} \\oplus X_{p 4}^{\\prime} \\\\ X_{p i}^{\\prime}=& g\\left(H_{\\rho}\\left(X_{p}, X_{q_{r_{i}}}\\right), \\ldots, H_{\\Theta}\\left(X_{p}, X_{\\left.q_{\\left(r_{i}, k\\right)}\\right)}\\right)\\right.\\end{aligned} \u5b8c\u6574\u7f51\u7edc\u793a\u4f8b \u8ba1\u7b97\u7ec6\u8282\u5728\u56fe\u4e2d\u4ee5\u53ca\u4ee3\u7801\u4e2d\u3002","title":"PointAtrousNet: Point Atrous Convolution for Point Cloud Analysis"},{"location":"Building_Blocks/PointAtrousNet/#pointatrousnet-point-atrous-convolution-for-point-cloud-analysis","text":"\u8fd9\u7bc7\u8bba\u6587\u5c06\u70b9\u4e91\u7684\u7a7a\u6d1e\u5377\u79ef\u5f15\u5165\u70b9\u4e91\u5206\u6790\u4e2d\uff0c\u503c\u5f97\u7559\u610f\u7684\u662f\u7a7a\u6d1e\u5377\u79ef\u5728 \u8fd9\u7bc7\u6587\u7ae0 \u4e2d\u5df2\u7ecf\u6709\u63d0\u53ca\uff0c\u4e14\u672c\u6587\u5e76\u6ca1\u6709\u5f15\u7528\u5b83\uff0c\u8fd9\u91cc\u53ef\u80fd\u6d89\u53ca\u4e00\u4e9b\u65f6\u95f4\u4e0a\u7684\u5dee\u522b\uff0c\u4e24\u8005\u5728\u70b9\u4e91\u7a7a\u6d1e\u5377\u79ef\u4e0a\u7684\u8ba1\u7b97\u8f83\u4e3a\u76f8\u4f3c\uff0c\u4f46\u662f\u672c\u6587\u8fdb\u4e00\u6b65\u63d0\u51fa\u4e86\u70b9\u4e91\u4e2d\u7684\u7a7a\u6d1e\u6c60\u5316\uff0c\u53ef\u4ee5\u4e00\u770b\u3002","title":"PointAtrousNet: Point Atrous Convolution for Point Cloud Analysis"},{"location":"Building_Blocks/PointAtrousNet/#_1","text":"\u4e0e \u8fd9\u7bc7\u6587\u7ae0 \u76f8\u4f3c\uff0c\u5728\u6bcf\u4e00\u4e2a\u70b9\u9644\u8fd1\u5bfb\u627e\u6700\u9760\u8fd1\u7684K\u4e2a\u70b9,\u8fd9\u91cc\u4e0e\u524d\u6587\u4e0d\u540c\u7684\u5730\u65b9\u5728\u4e8e\u8fd9\u91cc\u91c7\u7528\u7684\u662fedge convolution\u7684\u64cd\u4f5c\uff0c\u4e0e DGCNN.pdf \u7c7b\u4f3c\u7684\u8fd0\u7b97\uff0c \u7b80\u4ecb X_{p}^{\\prime}=g\\left(H_{\\Theta}\\left(X_{p}, X_{q_{r}}\\right), \\ldots, H_{\\Theta}\\left(X_{p}, X_{\\left.q_{(r, k)}\\right)}\\right)\\right. \u5176\u4e2d H_{\\Theta} = h_{\\theta}\\left(X_{p} \\oplus\\left(X_{p}-X_{q_{i}}\\right)\\right) \u5bfb\u627e\u4e34\u8fd1\u70b9\u7684\u65b9\u6cd5\u4e5f\u662f\u6784\u5efaKD\u6811.","title":"\u70b9\u4e91\u7a7a\u6d1e\u5377\u79ef"},{"location":"Building_Blocks/PointAtrousNet/#_2","text":"\\begin{aligned} X_{p}^{\\prime} &=X_{p 1}^{\\prime} \\oplus X_{p 2}^{\\prime} \\oplus X_{p 3}^{\\prime} \\oplus X_{p 4}^{\\prime} \\\\ X_{p i}^{\\prime}=& g\\left(H_{\\rho}\\left(X_{p}, X_{q_{r_{i}}}\\right), \\ldots, H_{\\Theta}\\left(X_{p}, X_{\\left.q_{\\left(r_{i}, k\\right)}\\right)}\\right)\\right.\\end{aligned}","title":"\u70b9\u4e91\u7a7a\u6d1e\u6c60\u5316"},{"location":"Building_Blocks/PointAtrousNet/#_3","text":"\u8ba1\u7b97\u7ec6\u8282\u5728\u56fe\u4e2d\u4ee5\u53ca\u4ee3\u7801\u4e2d\u3002","title":"\u5b8c\u6574\u7f51\u7edc\u793a\u4f8b"},{"location":"Building_Blocks/PositionalNorm/","text":"Positional Normalization \u8fd9\u7bc7paper\u63d0\u4f9b\u4e86\u4e00\u4e2a\u968f\u7740\u4e0d\u540cposition\u4e0d\u540c\u7684normalization scheme Normalization Review and Positional Normalization \u4ee3\u7801 import torch # x is the features of shape [B, C, H, W] # In the Encoder def PONO(x, epsilon=1e-5): mean = x.mean(dim=1, keepdim=True) std = x.var(dim=1, keepdim=True).add(epsilon).sqrt() output = (x - mean) / std return output, mean, std # In the Decoder # one can call MS(x, mean, std) # with the mean and std are from a PONO in the encoder def MS(x, beta, gamma): return x * gamma + beta ShortCut in Generative Model \u7528\u4e8e\u5c06\u8f93\u5165\u4fe1\u606f\u4f20\u9012\u5230\u8f93\u51fablock\u4e2d\uff0c\u80fd\u63d0\u5347\u751f\u6210\u6a21\u578b\u6bd4\u5982GAN\u7684\u6027\u80fd\u3002","title":"Positional Normalization"},{"location":"Building_Blocks/PositionalNorm/#positional-normalization","text":"\u8fd9\u7bc7paper\u63d0\u4f9b\u4e86\u4e00\u4e2a\u968f\u7740\u4e0d\u540cposition\u4e0d\u540c\u7684normalization scheme","title":"Positional Normalization"},{"location":"Building_Blocks/PositionalNorm/#normalization-review-and-positional-normalization","text":"\u4ee3\u7801 import torch # x is the features of shape [B, C, H, W] # In the Encoder def PONO(x, epsilon=1e-5): mean = x.mean(dim=1, keepdim=True) std = x.var(dim=1, keepdim=True).add(epsilon).sqrt() output = (x - mean) / std return output, mean, std # In the Decoder # one can call MS(x, mean, std) # with the mean and std are from a PONO in the encoder def MS(x, beta, gamma): return x * gamma + beta","title":"Normalization Review and Positional Normalization"},{"location":"Building_Blocks/PositionalNorm/#shortcut-in-generative-model","text":"\u7528\u4e8e\u5c06\u8f93\u5165\u4fe1\u606f\u4f20\u9012\u5230\u8f93\u51fablock\u4e2d\uff0c\u80fd\u63d0\u5347\u751f\u6210\u6a21\u578b\u6bd4\u5982GAN\u7684\u6027\u80fd\u3002","title":"ShortCut in Generative Model"},{"location":"Building_Blocks/RepVGG/","text":"RepVGG: Making VGG-style ConvNets Great Again \u8fd9\u7bc7paper\u7684\u76f4\u89c9\u662f\u7b80\u5355\u800c\u5f3a\u5927\u7684\u3002\u5728\u63a8\u7406\u7684\u65f6\u5019 3x3 \u5377\u79ef\u662f\u6700\u5feb\u7684\u4e5f\u662f\u6700\u5e7f\u6cdb\u652f\u6301\u7684,\u4e5f\u662f\u8fd0\u7b97\u5728Winograd\u4e0b\u4f18\u5316\u6700\u5f3a\u5927\u7684\u3002\u6240\u4ee5\u6700\u597d\u7684\u63a8\u7406\u7f51\u7edc\u5e94\u5f53\u662f\u6ca1\u6709\u6b8b\u5dee\u8fde\u63a5\u800c\u7eaf\u7cb9\u4f7f\u7528 3x3 \u5377\u79ef\u7684\u3002 \u4f46\u662fVGG\u8fd9\u6837\u7684\u7eaf\u7cb9\u5377\u79ef\u7684\u7f51\u7edc\uff0c\u5f31\u70b9\u5728\u4e8e\u5728\u5c42\u6570\u6df1\u7684\u65f6\u5019\u96be\u4ee5\u8bad\u7ec3\u3002 \u672c\u6587\u7684\u63d0\u51fa\u7684\u4e00\u4e2a\u5173\u952etrick \u53ef\u80fd\u542f\u53d1\u81ea\u795e\u7ecf\u7f51\u7edc\u90e8\u7f72\u65f6\u7684\u7b97\u5b50\u878d\u5408\u3002\u4e5f\u5c31\u662f\u5728\u8bad\u7ec3\u7684\u65f6\u5019\u8ba9\u7f51\u7edc\u4f7f\u7528\u6b8b\u5dee\u8fde\u63a5\uff0c\u4f46\u662f\u5728\u63a8\u7406\u7684\u65f6\u5019\u5168\u90e8\u878d\u5408\u4e3a\u5355\u4e00CNN. \u5e26\u4e0aBN\u540e\u7684\u5408\u6210\u6548\u679c:","title":"RepVGG: Making VGG-style ConvNets Great Again"},{"location":"Building_Blocks/RepVGG/#repvgg-making-vgg-style-convnets-great-again","text":"\u8fd9\u7bc7paper\u7684\u76f4\u89c9\u662f\u7b80\u5355\u800c\u5f3a\u5927\u7684\u3002\u5728\u63a8\u7406\u7684\u65f6\u5019 3x3 \u5377\u79ef\u662f\u6700\u5feb\u7684\u4e5f\u662f\u6700\u5e7f\u6cdb\u652f\u6301\u7684,\u4e5f\u662f\u8fd0\u7b97\u5728Winograd\u4e0b\u4f18\u5316\u6700\u5f3a\u5927\u7684\u3002\u6240\u4ee5\u6700\u597d\u7684\u63a8\u7406\u7f51\u7edc\u5e94\u5f53\u662f\u6ca1\u6709\u6b8b\u5dee\u8fde\u63a5\u800c\u7eaf\u7cb9\u4f7f\u7528 3x3 \u5377\u79ef\u7684\u3002 \u4f46\u662fVGG\u8fd9\u6837\u7684\u7eaf\u7cb9\u5377\u79ef\u7684\u7f51\u7edc\uff0c\u5f31\u70b9\u5728\u4e8e\u5728\u5c42\u6570\u6df1\u7684\u65f6\u5019\u96be\u4ee5\u8bad\u7ec3\u3002 \u672c\u6587\u7684\u63d0\u51fa\u7684\u4e00\u4e2a\u5173\u952etrick \u53ef\u80fd\u542f\u53d1\u81ea\u795e\u7ecf\u7f51\u7edc\u90e8\u7f72\u65f6\u7684\u7b97\u5b50\u878d\u5408\u3002\u4e5f\u5c31\u662f\u5728\u8bad\u7ec3\u7684\u65f6\u5019\u8ba9\u7f51\u7edc\u4f7f\u7528\u6b8b\u5dee\u8fde\u63a5\uff0c\u4f46\u662f\u5728\u63a8\u7406\u7684\u65f6\u5019\u5168\u90e8\u878d\u5408\u4e3a\u5355\u4e00CNN. \u5e26\u4e0aBN\u540e\u7684\u5408\u6210\u6548\u679c:","title":"RepVGG: Making VGG-style ConvNets Great Again"},{"location":"Building_Blocks/SPN_CSPN/","text":"Spatial Propagation Network, Convolutional SPN and More \u8fd9\u7bc7\u6587\u662fSPN\u4ee5\u53caCSPN\uff0cCSPN++\u4e09\u7bc7paper\u7684\u7efc\u8ff0\u3002\u5728\u4f20\u7edf\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\uff0c Diffusion \u662f\u4e00\u4e2a\u542f\u53d1\u4e8e\u81ea\u7136\u6269\u6563/\u6e17\u900f\u5b9a\u5f8b\u7684\u5904\u7406\u7b97\u6cd5\uff0c\u8fed\u4ee3\u5730\u4f7f\u7528\u4e0d\u540c\u4f4d\u7f6e\u4e0d\u540c\u7684kernel\u5c06\u67d0\u4e00\u70b9\u7684\u4fe1\u606f\u6269\u6563\u5230\u9644\u8fd1\u7684\u70b9\u4e0a\u3002 Spatial Propagation Network pdf code \u8fd9\u7bc7NIPS paper\uff0c\u662f\u7b2c\u4e00\u4e2a\u5c06Diffusion \u6df1\u5ea6\u5b66\u4e60\u5316\u7684paper\u3002 Spatial Propagation Basic \u5bf9\u4e8e\u6c34\u5e73\u65b9\u5411propagate\u7684\u4f8b\u5b50,\u8bbe x_t\uff0c h_t \u4e3a n\\times n \u7684\u7279\u5f81\u56fe\uff0c w_t \u662f n\\times n \u7684kernel\u77e9\u9635(\u53ef\u4ee5\u7406\u89e3\u4e3a\u4e00\u4e2a\u53c2\u6570\u77e9\u9635), d_t \u662f\u4e00\u4e2a\u5bf9\u89d2\u77e9\u9635\uff0c d_{t}(i, i)=\\sum_{j=1, j \\neq i}^{n} w_{t}(i, j) \u90a3\u4e48\u8fed\u4ee3\u516c\u5f0f\u5c31\u662f h_{t}=\\left(I-d_{t}\\right) x_{t}+w_{t} h_{t-1}, \\quad t \\in[2, n] \u5c06\u6240\u6709 t \u6b21\u8fed\u4ee3\u5199\u5728\u4e00\u4e2a t\\dot n \\times t\\dot n \u77e9\u9635\u91cc\u9762\uff0c H_{v}=\\left[\\begin{array}{ccccc} I & 0 & \\cdots & \\cdots & 0 \\\\ w_{2} & \\lambda_{2} & 0 & \\cdots & \\cdots \\\\ w_{3} w_{2} & w_{3} \\lambda_{2} & \\lambda_{3} & 0 & \\cdots \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\vdots & \\vdots & \\cdots & \\cdots & \\lambda_{n} \\end{array}\\right] X_{v}=G X_{v} Learning based Implementation \u672c\u6587\u63d0\u51fa\"three-way connection\"\uff0c\u4e5f\u5c31\u662f\u6bcf\u6b21\u4f20\u64ad\u4f1a\u5411\u4e0a\u4e0b\u5de6\u53f33\u4e2apixel\uff0c\u5206\u4e3a\u56db\u4e2a\u5206\u652f\uff0c\u7136\u540e\u5206\u522b\u8fed\u4ee3\u6267\u884c\u3002 \u5f00\u6e90\u4ee3\u7801\u53ea\u80fd\u652f\u6301\u65b9\u5f62\u77e9\u9635(\u9884\u8bbeH==W, \u6bd4\u8f83\u7684\u50f5\u786c,\u8fd9\u4e5f\u5bfc\u81f4\u4e86\u4ee3\u7801\u7684\u6ce8\u91ca\u4e00\u81f4\u6027\u6bd4\u8f83\u5dee)\uff0c\u56e0\u800c\u5177\u4f53implementation \u6b64\u5904\u7565\u8fc7 Convolutional SPN pdf code \u8fd9\u7bc7paper\u5728\u53cc\u76ee\u4ee5\u53caStereo\u4e2d\u662f\u4e00\u4e2a\u5f88\u5bcc\u88d5\u7684\u6587\u7ae0\uff0c\u5176\u4e2d\u7406\u8bba\u6bd4\u8f83\u65b0\u9896\u7684\u5730\u65b9\u662f\u52a0\u5165\u4e86CSPN\u6a21\u5757\u4ee5\u53ca\u5b83\u76843D\u5f62\u5f0f\u3002 code","title":"Spatial Propagation Network, Convolutional SPN and More"},{"location":"Building_Blocks/SPN_CSPN/#spatial-propagation-network-convolutional-spn-and-more","text":"\u8fd9\u7bc7\u6587\u662fSPN\u4ee5\u53caCSPN\uff0cCSPN++\u4e09\u7bc7paper\u7684\u7efc\u8ff0\u3002\u5728\u4f20\u7edf\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\uff0c Diffusion \u662f\u4e00\u4e2a\u542f\u53d1\u4e8e\u81ea\u7136\u6269\u6563/\u6e17\u900f\u5b9a\u5f8b\u7684\u5904\u7406\u7b97\u6cd5\uff0c\u8fed\u4ee3\u5730\u4f7f\u7528\u4e0d\u540c\u4f4d\u7f6e\u4e0d\u540c\u7684kernel\u5c06\u67d0\u4e00\u70b9\u7684\u4fe1\u606f\u6269\u6563\u5230\u9644\u8fd1\u7684\u70b9\u4e0a\u3002","title":"Spatial Propagation Network, Convolutional SPN and More"},{"location":"Building_Blocks/SPN_CSPN/#spatial-propagation-network","text":"pdf code \u8fd9\u7bc7NIPS paper\uff0c\u662f\u7b2c\u4e00\u4e2a\u5c06Diffusion \u6df1\u5ea6\u5b66\u4e60\u5316\u7684paper\u3002","title":"Spatial Propagation Network"},{"location":"Building_Blocks/SPN_CSPN/#spatial-propagation-basic","text":"\u5bf9\u4e8e\u6c34\u5e73\u65b9\u5411propagate\u7684\u4f8b\u5b50,\u8bbe x_t\uff0c h_t \u4e3a n\\times n \u7684\u7279\u5f81\u56fe\uff0c w_t \u662f n\\times n \u7684kernel\u77e9\u9635(\u53ef\u4ee5\u7406\u89e3\u4e3a\u4e00\u4e2a\u53c2\u6570\u77e9\u9635), d_t \u662f\u4e00\u4e2a\u5bf9\u89d2\u77e9\u9635\uff0c d_{t}(i, i)=\\sum_{j=1, j \\neq i}^{n} w_{t}(i, j) \u90a3\u4e48\u8fed\u4ee3\u516c\u5f0f\u5c31\u662f h_{t}=\\left(I-d_{t}\\right) x_{t}+w_{t} h_{t-1}, \\quad t \\in[2, n] \u5c06\u6240\u6709 t \u6b21\u8fed\u4ee3\u5199\u5728\u4e00\u4e2a t\\dot n \\times t\\dot n \u77e9\u9635\u91cc\u9762\uff0c H_{v}=\\left[\\begin{array}{ccccc} I & 0 & \\cdots & \\cdots & 0 \\\\ w_{2} & \\lambda_{2} & 0 & \\cdots & \\cdots \\\\ w_{3} w_{2} & w_{3} \\lambda_{2} & \\lambda_{3} & 0 & \\cdots \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\vdots & \\vdots & \\cdots & \\cdots & \\lambda_{n} \\end{array}\\right] X_{v}=G X_{v}","title":"Spatial Propagation Basic"},{"location":"Building_Blocks/SPN_CSPN/#learning-based-implementation","text":"\u672c\u6587\u63d0\u51fa\"three-way connection\"\uff0c\u4e5f\u5c31\u662f\u6bcf\u6b21\u4f20\u64ad\u4f1a\u5411\u4e0a\u4e0b\u5de6\u53f33\u4e2apixel\uff0c\u5206\u4e3a\u56db\u4e2a\u5206\u652f\uff0c\u7136\u540e\u5206\u522b\u8fed\u4ee3\u6267\u884c\u3002 \u5f00\u6e90\u4ee3\u7801\u53ea\u80fd\u652f\u6301\u65b9\u5f62\u77e9\u9635(\u9884\u8bbeH==W, \u6bd4\u8f83\u7684\u50f5\u786c,\u8fd9\u4e5f\u5bfc\u81f4\u4e86\u4ee3\u7801\u7684\u6ce8\u91ca\u4e00\u81f4\u6027\u6bd4\u8f83\u5dee)\uff0c\u56e0\u800c\u5177\u4f53implementation \u6b64\u5904\u7565\u8fc7","title":"Learning based Implementation"},{"location":"Building_Blocks/SPN_CSPN/#convolutional-spn","text":"pdf code \u8fd9\u7bc7paper\u5728\u53cc\u76ee\u4ee5\u53caStereo\u4e2d\u662f\u4e00\u4e2a\u5f88\u5bcc\u88d5\u7684\u6587\u7ae0\uff0c\u5176\u4e2d\u7406\u8bba\u6bd4\u8f83\u65b0\u9896\u7684\u5730\u65b9\u662f\u52a0\u5165\u4e86CSPN\u6a21\u5757\u4ee5\u53ca\u5b83\u76843D\u5f62\u5f0f\u3002 code","title":"Convolutional SPN"},{"location":"Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/","text":"SQUEEZENET: ALEXNET-LEVEL ACCURACY WITH 50X FEWER PARAMETERS AND <0.5MB MODEL SIZE \u672c\u6587\u63d0\u51fa\u4e86\u4f53\u79ef\u5c0f\u7684\u7f51\u7edc\u6709\u4ec0\u4e48\u597d\u5904\uff0c\u5e76\u4e14\u7ed9\u51fa\u4e86SqueezeNet\u7684\u5177\u4f53\u7ed3\u6784\u4ee5\u53ca\u76f8\u5173\u8ba8\u8bba \u7f51\u7edc\u7ed3\u6784\u8bbe\u8ba1\u7b56\u7565 \u5c06 3\\times 3 \u5377\u79ef\u6838\u8f6c\u6362\u4e3a 1\\times 1 \u964d\u4f4e\u8f93\u5165\u5230 3\\times 3 \u5377\u79ef\u7684\u7f51\u7edc\u8f93\u5165channel\u6570 \u5728\u8f83\u540e\u7684\u4f4d\u7f6e\u518d\u8fdb\u884c\u4e0b\u91c7\u6837\uff0c\u4ee5\u6b64\u63d0\u5347\u5377\u79ef\u5c42\u7684FOV Fire Module \u4e00\u4e2afire module\u7531: 1. squeeze layer(\u4ec5\u6709 1\\times 1 \u5377\u79ef\u7ec4\u6210) 2. expand layer(\u6df7\u5408 1\\times 1 \u5377\u79ef\u4e0e 3\\times 3 \u5377\u79ef) \u7b80\u5355\u6765\u8bf4\u5c31\u662f\u5148\u7528 1\\times 1 \u5377\u79ef\u964d\u4f4e\u8f93\u5165\u7684channel\u6570\uff0c\u7136\u540e\u5c06\u4e3b\u8f93\u51fa\u7684\u5c42\u90e8\u5206\u7528 1\\times 1 \u66ff\u4ee3\u3002 SqueezeNet\u7ed3\u6784 \u672c\u6587\u7b2c5\u7ae0\u8fd8\u8fdb\u4e00\u6b65\u8ba8\u8bba\u4e86Fire Module\u8fd8\u6709\u5b8f\u89c2\u7ed3\u6784\u4e2d\u7684\u4e00\u7cfb\u5217\u5177\u4f53\u53c2\u6570\u3002","title":"SQUEEZENET: ALEXNET-LEVEL ACCURACY WITH 50X FEWER PARAMETERS AND &lt;0.5MB MODEL SIZE"},{"location":"Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/#squeezenet-alexnet-level-accuracy-with-50x-fewer-parameters-and-05mb-model-size","text":"\u672c\u6587\u63d0\u51fa\u4e86\u4f53\u79ef\u5c0f\u7684\u7f51\u7edc\u6709\u4ec0\u4e48\u597d\u5904\uff0c\u5e76\u4e14\u7ed9\u51fa\u4e86SqueezeNet\u7684\u5177\u4f53\u7ed3\u6784\u4ee5\u53ca\u76f8\u5173\u8ba8\u8bba","title":"SQUEEZENET: ALEXNET-LEVEL ACCURACY WITH 50X FEWER PARAMETERS AND &lt;0.5MB MODEL SIZE"},{"location":"Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/#_1","text":"\u5c06 3\\times 3 \u5377\u79ef\u6838\u8f6c\u6362\u4e3a 1\\times 1 \u964d\u4f4e\u8f93\u5165\u5230 3\\times 3 \u5377\u79ef\u7684\u7f51\u7edc\u8f93\u5165channel\u6570 \u5728\u8f83\u540e\u7684\u4f4d\u7f6e\u518d\u8fdb\u884c\u4e0b\u91c7\u6837\uff0c\u4ee5\u6b64\u63d0\u5347\u5377\u79ef\u5c42\u7684FOV","title":"\u7f51\u7edc\u7ed3\u6784\u8bbe\u8ba1\u7b56\u7565"},{"location":"Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/#fire-module","text":"\u4e00\u4e2afire module\u7531: 1. squeeze layer(\u4ec5\u6709 1\\times 1 \u5377\u79ef\u7ec4\u6210) 2. expand layer(\u6df7\u5408 1\\times 1 \u5377\u79ef\u4e0e 3\\times 3 \u5377\u79ef) \u7b80\u5355\u6765\u8bf4\u5c31\u662f\u5148\u7528 1\\times 1 \u5377\u79ef\u964d\u4f4e\u8f93\u5165\u7684channel\u6570\uff0c\u7136\u540e\u5c06\u4e3b\u8f93\u51fa\u7684\u5c42\u90e8\u5206\u7528 1\\times 1 \u66ff\u4ee3\u3002","title":"Fire Module"},{"location":"Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/#squeezenet","text":"\u672c\u6587\u7b2c5\u7ae0\u8fd8\u8fdb\u4e00\u6b65\u8ba8\u8bba\u4e86Fire Module\u8fd8\u6709\u5b8f\u89c2\u7ed3\u6784\u4e2d\u7684\u4e00\u7cfb\u5217\u5177\u4f53\u53c2\u6570\u3002","title":"SqueezeNet\u7ed3\u6784"},{"location":"Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/","text":"Receptive Field Block Net for Accurate and Fast Object Detection \u8fd9\u7bc7\u6587\u7ae0\u7684\u4f5c\u8005\u57fa\u4e8e\u4eba\u773c\u5b9e\u9645\u7ec6\u80de\u7684\u611f\u53d7\u91ce,\u8bbe\u8ba1\u4e86RFB\u6a21\u5757\u7528\u4e8e\u5206\u7c7b\u4e0e\u68c0\u6d4b\u3002 RFB \u6a21\u5757\u611f\u53d7\u91ce\u53ef\u89c6\u5316 \u4e0b\u56fe\u8868\u793a\u4e86,RFB\u6a21\u5757\u7684\u611f\u53d7\u91ce\u4e0e\u4eba\u7c7bhV4\u773c\u7ec6\u80de\u3002 RFB \u6267\u884c\u7ed3\u6784 \u4e0b\u56fe\u8868\u8fbe\u4e24\u79cdRFB\u6a21\u5757\u7684\u5177\u4f53\u7ed3\u6784\uff0c\u5b83\u4eec\u5206\u522b\u8fd1\u4f3c\u4eba\u7c7b\u6df1\u5c42\u4e0e\u6d45\u5c42\u89c6\u89c9\u7ec6\u80de\u7684\u611f\u53d7\u91ce\u7279\u6027\u3002\u5177\u4f53\u5b9e\u73b0\u7684\u4f7f\u7528\u4f7f\u7528\u4e24\u4e2a\u4e32\u63a5\u7684 3\\times 3 \u5377\u79ef\u66ff\u4ee3 5\\times 5 \u5377\u79ef\u3002 RFB SSD RFB\u7ed3\u6784\u5728SSD\u4e2d\u4f7f\u7528\uff0c\u4f5c\u8005\u63d0\u5347\u4e86\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b\u7684\u6027\u80fd\u3002","title":"Receptive Field Block Net for Accurate and Fast Object Detection"},{"location":"Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/#receptive-field-block-net-for-accurate-and-fast-object-detection","text":"\u8fd9\u7bc7\u6587\u7ae0\u7684\u4f5c\u8005\u57fa\u4e8e\u4eba\u773c\u5b9e\u9645\u7ec6\u80de\u7684\u611f\u53d7\u91ce,\u8bbe\u8ba1\u4e86RFB\u6a21\u5757\u7528\u4e8e\u5206\u7c7b\u4e0e\u68c0\u6d4b\u3002","title":"Receptive Field Block Net for Accurate and Fast Object Detection"},{"location":"Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/#rfb","text":"\u4e0b\u56fe\u8868\u793a\u4e86,RFB\u6a21\u5757\u7684\u611f\u53d7\u91ce\u4e0e\u4eba\u7c7bhV4\u773c\u7ec6\u80de\u3002","title":"RFB \u6a21\u5757\u611f\u53d7\u91ce\u53ef\u89c6\u5316"},{"location":"Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/#rfb_1","text":"\u4e0b\u56fe\u8868\u8fbe\u4e24\u79cdRFB\u6a21\u5757\u7684\u5177\u4f53\u7ed3\u6784\uff0c\u5b83\u4eec\u5206\u522b\u8fd1\u4f3c\u4eba\u7c7b\u6df1\u5c42\u4e0e\u6d45\u5c42\u89c6\u89c9\u7ec6\u80de\u7684\u611f\u53d7\u91ce\u7279\u6027\u3002\u5177\u4f53\u5b9e\u73b0\u7684\u4f7f\u7528\u4f7f\u7528\u4e24\u4e2a\u4e32\u63a5\u7684 3\\times 3 \u5377\u79ef\u66ff\u4ee3 5\\times 5 \u5377\u79ef\u3002","title":"RFB \u6267\u884c\u7ed3\u6784"},{"location":"Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/#rfb-ssd","text":"RFB\u7ed3\u6784\u5728SSD\u4e2d\u4f7f\u7528\uff0c\u4f5c\u8005\u63d0\u5347\u4e86\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b\u7684\u6027\u80fd\u3002","title":"RFB SSD"},{"location":"Building_Blocks/Squeeze-and-Excitation_Networks/","text":"Squeeze-and-Excitation Networks \u8fd9\u7bc7\u8bba\u6587\u7ed9\u51fa\u4e86squeeze-and-excitation\u65b9\u6cd5\u6765\u63d0\u9ad8\u5377\u79ef\u7684\u611f\u53d7\u573a\u5927\u5c0f\u3002 \u8ba1\u7b97\u5355\u5143\u7ed3\u6784 F_{tr} \u4e3a\u57fa\u7840\u5377\u79ef\u64cd\u4f5c\uff0c F_{sq} \u4e3a\u4e00\u4e2a\u5168\u56fe(H*W)\u7684average pool. F_{ex} = \\sigma(W_2\\sigma(W_1z)) \uff0c\u5c31\u662f\u4e00\u4e2aencoder\u3001decoder\u7684\u7ed3\u6784\uff0c F_{scale} \u5219\u662f\u4e00\u4e2achannel-wise\u7684\u70b9\u4e58(scalar and a H\\times W feature map) \u4f5c\u8005\u8868\u793a\u8fd9\u53ef\u4ee5\u7406\u89e3\u4e3a\u4e00\u4e2a\u81ea\u6ce8\u610f\u529b\u51fd\u6570","title":"Squeeze-and-Excitation Networks"},{"location":"Building_Blocks/Squeeze-and-Excitation_Networks/#squeeze-and-excitation-networks","text":"\u8fd9\u7bc7\u8bba\u6587\u7ed9\u51fa\u4e86squeeze-and-excitation\u65b9\u6cd5\u6765\u63d0\u9ad8\u5377\u79ef\u7684\u611f\u53d7\u573a\u5927\u5c0f\u3002","title":"Squeeze-and-Excitation Networks"},{"location":"Building_Blocks/Squeeze-and-Excitation_Networks/#_1","text":"F_{tr} \u4e3a\u57fa\u7840\u5377\u79ef\u64cd\u4f5c\uff0c F_{sq} \u4e3a\u4e00\u4e2a\u5168\u56fe(H*W)\u7684average pool. F_{ex} = \\sigma(W_2\\sigma(W_1z)) \uff0c\u5c31\u662f\u4e00\u4e2aencoder\u3001decoder\u7684\u7ed3\u6784\uff0c F_{scale} \u5219\u662f\u4e00\u4e2achannel-wise\u7684\u70b9\u4e58(scalar and a H\\times W feature map) \u4f5c\u8005\u8868\u793a\u8fd9\u53ef\u4ee5\u7406\u89e3\u4e3a\u4e00\u4e2a\u81ea\u6ce8\u610f\u529b\u51fd\u6570","title":"\u8ba1\u7b97\u5355\u5143\u7ed3\u6784"},{"location":"Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/","text":"Stacked Hourglass Networks for Human Pose Estimation \u8fd9\u7bc7\u6587\u7ae0\u76ee\u524d\u5bf9\u4e8e\u672c\u4eba\u6765\u8bf4\u662f \u8fd9\u7bc7\u6587\u7ae0 \u7684\u524d\u7f6e. \u76ee\u524d(2019-10-13)\uff0c\u6211\u4eec\u4e3b\u8981\u8ba8\u8bba\u672c\u6587\u63d0\u5230\u7684HourGlass \u6a21\u5757 HourGlass \u8fd9\u4e2a\u6a21\u5757\u7684\u8bbe\u8ba1\u8d77\u6e90\u4e8e\u5bf9\u591ascale\u7279\u5f81\u8f93\u51fa\u63d0\u53d6\u7684\u9700\u6c42\u3002\u7b80\u5355\u800c\u8a00\u5c31\u662f\u5728\u6bcf\u4e00\u4e2ascale\uff0c\u8fdb\u884c\u5206\u652f\uff0c\u4e00\u90e8\u5206\u6267\u884c\u4e00\u6b21\u5377\u79ef\u7b49\u5f85\u8fdb\u4e00\u6b65\u4f7f\u7528\u3002\u53e6\u4e00\u90e8\u5206\u6267\u884cmax-pooling\u4e0b\u91c7\u6837\u3002\u53bb\u5230\u6700\u5c0ffeatrue maps\u540e\uff0c\u4f7f\u7528Neareast pooling\u4e0a\u91c7\u6837\uff0c\u4e0e\u540cscale\u7684\u5148\u524d\u7559\u4e0b\u7684\u6b8b\u5dee\u8fdb\u884celement-wise\u76f8\u52a0 \u672c\u6587\u540e\u9762\u63d0\u5230\u8fd9\u91cc\u7684\u5377\u79ef\u53ef\u4ee5\u4f7f\u7528\u4e0d\u540c\u7684\u65b9\u5f0f\u5b9e\u73b0\uff0c\u6bd4\u5982\u672c\u6587\u4f3c\u4e4e\u5c31\u662f\u4f7f\u7528\u4e00\u4e2a\u590d\u6742\u7684Res\u7ed3\u6784\u5b9e\u73b0\u7684\u3002\u5177\u4f53\u8fd8\u9700\u8981\u770b\u4ee3\u7801\u3002","title":"Stacked Hourglass Networks for Human Pose Estimation"},{"location":"Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/#stacked-hourglass-networks-for-human-pose-estimation","text":"\u8fd9\u7bc7\u6587\u7ae0\u76ee\u524d\u5bf9\u4e8e\u672c\u4eba\u6765\u8bf4\u662f \u8fd9\u7bc7\u6587\u7ae0 \u7684\u524d\u7f6e. \u76ee\u524d(2019-10-13)\uff0c\u6211\u4eec\u4e3b\u8981\u8ba8\u8bba\u672c\u6587\u63d0\u5230\u7684HourGlass \u6a21\u5757","title":"Stacked Hourglass Networks for Human Pose Estimation"},{"location":"Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/#hourglass","text":"\u8fd9\u4e2a\u6a21\u5757\u7684\u8bbe\u8ba1\u8d77\u6e90\u4e8e\u5bf9\u591ascale\u7279\u5f81\u8f93\u51fa\u63d0\u53d6\u7684\u9700\u6c42\u3002\u7b80\u5355\u800c\u8a00\u5c31\u662f\u5728\u6bcf\u4e00\u4e2ascale\uff0c\u8fdb\u884c\u5206\u652f\uff0c\u4e00\u90e8\u5206\u6267\u884c\u4e00\u6b21\u5377\u79ef\u7b49\u5f85\u8fdb\u4e00\u6b65\u4f7f\u7528\u3002\u53e6\u4e00\u90e8\u5206\u6267\u884cmax-pooling\u4e0b\u91c7\u6837\u3002\u53bb\u5230\u6700\u5c0ffeatrue maps\u540e\uff0c\u4f7f\u7528Neareast pooling\u4e0a\u91c7\u6837\uff0c\u4e0e\u540cscale\u7684\u5148\u524d\u7559\u4e0b\u7684\u6b8b\u5dee\u8fdb\u884celement-wise\u76f8\u52a0 \u672c\u6587\u540e\u9762\u63d0\u5230\u8fd9\u91cc\u7684\u5377\u79ef\u53ef\u4ee5\u4f7f\u7528\u4e0d\u540c\u7684\u65b9\u5f0f\u5b9e\u73b0\uff0c\u6bd4\u5982\u672c\u6587\u4f3c\u4e4e\u5c31\u662f\u4f7f\u7528\u4e00\u4e2a\u590d\u6742\u7684Res\u7ed3\u6784\u5b9e\u73b0\u7684\u3002\u5177\u4f53\u8fd8\u9700\u8981\u770b\u4ee3\u7801\u3002","title":"HourGlass"},{"location":"Building_Blocks/convNext/","text":"A ConvNet for the 2020s (ConvNeXt) \u8fd9\u7bc7paper\u7684\u5de5\u4f5c\u5728\u4e8e\u901a\u8fc7\u4fee\u6539ResNet\u7684\u4e00\u4e9b\u57fa\u7840\u8bbe\u5b9a\u4f7f\u5f97\u5b83\u53ef\u4ee5\u8d85\u8d8aTransformer. \u5176\u5b9e\u5c31\u662f\u4e00\u4e2a\u5f88\u7b80\u5355\u7684intuitition, \u8fd1\u5e74\u6765ViT\u65b9\u9762\u5f88\u591a\u7684\u8fdb\u6b65\u5728\u4e8e \u5229\u7528\u5c40\u90e8\u7279\u5f81 , \u5c24\u5176\u662fSwinTransformer,\u5f88\u591a\u7684\u63d0\u5347\u5c31\u662f\u5728\u5c1d\u8bd5\u7528\u56deconv\u7684\u7279\u70b9\u4f46\u662f\u53c8\u4e0d\u663e\u5f0f\u5730\u4f7f\u7528Conv, \u672c\u6587\u5c31\u53cd\u8fc7\u6765,\u8ba9Conv\u5b66\u4e60Transformer\u7684\u4e00\u4e9b\u7279\u6027,\u4f7f\u5f97\u6211\u4eec\u80fd\u591f\u7528Conv\u5f97\u5230Transformer\u7684\u6027\u80fd. \u63d0\u5347\u8def\u7ebf\u56fe: \u8bad\u7ec3\u65b9\u6cd5\u63d0\u5347 \u4f7f\u7528AdamW\u4f18\u5316\u5668 \u63d0\u5347\u8bad\u7ec3epoch\u6570 \u6570\u636e\u589e\u5f3a\u5305\u542b Mixup Cutmix RandAugment, RandomErasing. Stochastic Depth Label Smoothing Stochastic Depth (\u6307\u968f\u673a\u4e0d\u8fd0\u884cresidual block) \u6a21\u5757\u63d0\u5347 \u6a21\u5757\u603b\u7ed3\u800c\u8a00\u4e3a\u4ee5\u4e0b\u56fe:","title":"A ConvNet for the 2020s (ConvNeXt)"},{"location":"Building_Blocks/convNext/#a-convnet-for-the-2020s-convnext","text":"\u8fd9\u7bc7paper\u7684\u5de5\u4f5c\u5728\u4e8e\u901a\u8fc7\u4fee\u6539ResNet\u7684\u4e00\u4e9b\u57fa\u7840\u8bbe\u5b9a\u4f7f\u5f97\u5b83\u53ef\u4ee5\u8d85\u8d8aTransformer. \u5176\u5b9e\u5c31\u662f\u4e00\u4e2a\u5f88\u7b80\u5355\u7684intuitition, \u8fd1\u5e74\u6765ViT\u65b9\u9762\u5f88\u591a\u7684\u8fdb\u6b65\u5728\u4e8e \u5229\u7528\u5c40\u90e8\u7279\u5f81 , \u5c24\u5176\u662fSwinTransformer,\u5f88\u591a\u7684\u63d0\u5347\u5c31\u662f\u5728\u5c1d\u8bd5\u7528\u56deconv\u7684\u7279\u70b9\u4f46\u662f\u53c8\u4e0d\u663e\u5f0f\u5730\u4f7f\u7528Conv, \u672c\u6587\u5c31\u53cd\u8fc7\u6765,\u8ba9Conv\u5b66\u4e60Transformer\u7684\u4e00\u4e9b\u7279\u6027,\u4f7f\u5f97\u6211\u4eec\u80fd\u591f\u7528Conv\u5f97\u5230Transformer\u7684\u6027\u80fd.","title":"A ConvNet for the 2020s (ConvNeXt)"},{"location":"Building_Blocks/convNext/#_1","text":"","title":"\u63d0\u5347\u8def\u7ebf\u56fe:"},{"location":"Building_Blocks/convNext/#_2","text":"\u4f7f\u7528AdamW\u4f18\u5316\u5668 \u63d0\u5347\u8bad\u7ec3epoch\u6570 \u6570\u636e\u589e\u5f3a\u5305\u542b Mixup Cutmix RandAugment, RandomErasing. Stochastic Depth Label Smoothing Stochastic Depth (\u6307\u968f\u673a\u4e0d\u8fd0\u884cresidual block)","title":"\u8bad\u7ec3\u65b9\u6cd5\u63d0\u5347"},{"location":"Building_Blocks/convNext/#_3","text":"\u6a21\u5757\u603b\u7ed3\u800c\u8a00\u4e3a\u4ee5\u4e0b\u56fe:","title":"\u6a21\u5757\u63d0\u5347"},{"location":"Building_Blocks/crossBatchNormalization/","text":"Cross-Iteration Batch Normalization \u8fd9\u7bc7\u6587\u7ae0\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u7684\u6a21\u5757\uff0c\u79f0\u4e3a\u8de8\u8fed\u4ee3\u7684batchnorm, \u8fd9\u4e2a\u6a21\u5757\u7684\u6e90\u7801\u5728\u5176github\u9879\u76ee\u7684 ./mmdet/models/utils/CBN.py \u627e\u5f97\u5230. Motivation BatchNormalization\u7684\u4e00\u4e2a\u95ee\u9898\u5c31\u662f\u5728batchsize\u6bd4\u8f83\u5c0f\u7684\u65f6\u5019\u6548\u679c\u6bd4\u8f83\u5dee. \u540e\u6765\u53d1\u660e\u4e86 SyncBatchnorm \u6765\u8fdb\u884c\u8de8GPU\u7684batchnorm\u8ba1\u7b97\u3002\u672c\u6587\u8fdb\u4e00\u6b65\u63d0\u51fa\uff0c\u4f7f\u7528\u591a\u6b21\u8fed\u4ee3\u7684\u7ed3\u679c\u5e2e\u52a9batchnorm\u3002 \u76f4\u89c9\u4e0a\u6765\u8bf4,\u5c3d\u7ba1\u7531\u4e8e\u7f51\u7edc\u6743\u91cd\u53d8\u5316\u901f\u5ea6\u4e0d\u5feb\uff0c\u4f46\u662f\u76f8\u90bb\u7684\u8fed\u4ee3\u8fc7\u7a0b\u4e2d\uff0cexpected mean and variance\u8fd8\u662f\u5728\u53d8\u5316\u7684\u3002\u672c\u6587\u501f\u52a9\u4e00\u4e9b\u4f4e\u9636\u8fd1\u4f3c\uff0c\u5bf9\u6743\u91cd\u53d8\u5316\u505a\u4e86\u7b80\u8981\u7684\u5206\u6790\uff0c\u7ed9\u51fa\u4e86CBN\u6a21\u5757\u3002 CBN Algorithm \u672c\u8d28\u4e0a\u6765\u8bf4\uff0cbatch response\u7684\u5747\u503c\uff0c\u5e73\u65b9\u548c\u4ee5\u53ca\u65b9\u5dee\u7b49\u6570\u636e\u503c\u662f\u5173\u4e8e\u6743\u91cd\u7684\u4e00\u4e2adeterministic\u51fd\u6570\uff0c \u5728\u5b9e\u9645\u8ba1\u7b97\u4e2d\u7531\u67d0\u6b21\u8fed\u4ee3\u5f97\u5230\u7684\u7ed3\u679c\u8fdb\u884cMonte-Carlo\u4eff\u771f\u5f97\u5230\u4e00\u4e2a\u53c2\u8003\u503c\u3002\u8fc7\u53bb\u5f97\u5230\u7684\u5747\u503c\u53c2\u8003\u503c\u4e0e\u5f53\u524d\u503c\u5dee\u503c\u5728\u8fd9\u91cc\u7528\u6cf0\u52d2\u516c\u5f0f\u8fd1\u4f3c\u5f97\u5230.\u800c\u8fd9\u4e2aweight \u4ee3\u7801\u7684\u5b9e\u73b0\u662f\u5bfb\u627eBN\u524d\u7d27\u8ddf\u7740\u7684Conv\u7684weight(\u65e0bias)\u4f5c\u4e3a\u8fd9\u4e2a\u7b97\u6cd5\u4e2d\u7684 \\theta \\begin{aligned} \\mu_{t-\\tau}\\left(\\theta_{t}\\right)=& \\mu_{t-\\tau}\\left(\\theta_{t-\\tau}\\right)+\\frac{\\partial \\mu_{t-\\tau}\\left(\\theta_{t-\\tau}\\right)}{\\partial \\theta_{t-\\tau}}\\left(\\theta_{t}-\\theta_{t-\\tau}\\right) \\\\ &+\\mathbf{O}\\left(\\left\\|\\theta_{t}-\\theta_{t-\\tau}\\right\\|^{2}\\right) \\\\ \\nu_{t-\\tau}\\left(\\theta_{t}\\right)=& \\nu_{t-\\tau}\\left(\\theta_{t-\\tau}\\right)+\\frac{\\partial \\nu_{t-\\tau}\\left(\\theta_{t-\\tau}\\right)}{\\partial \\theta_{t-\\tau}}\\left(\\theta_{t}-\\theta_{t-\\tau}\\right) \\\\ &+\\mathbf{O}\\left(\\left\\|\\theta_{t}-\\theta_{t-\\tau}\\right\\|^{2}\\right) \\end{aligned} \u7701\u7565\u9ad8\u9636\u9879\uff0c\u540c\u65f6\u4f5c\u8005\u6307\u51fa\uff0c\u4e0d\u540cnormalization\u5c42\u4e4b\u95f4\u56e0\u4e3a\u524d\u4e00\u5c42\u6743\u91cd\u53d8\u5316\u5f15\u8d77\u7684\u540e\u4e00\u5c42\u7684\u5747\u503c\u4e0e\u65b9\u5dee\u53d8\u5316\u662f\u6709\u9650\u7684\u3002\u8fd9\u4e2a\u539f\u56e0\u6765\u8bf4\uff0c\u4f5c\u8005\u7684\u7406\u89e3\u662f\u8fd9\u4e2aBatchnorm\u4e2d\u76f4\u63a5\u5b66\u4e60\u7684 scale and shift\u5bf9\u540e\u4e00\u5c42\u5747\u503c\u4e0e\u65b9\u5dee\u7684\u5f71\u54cd\u603b\u4f53\u6765\u8bf4\u662f\u66f4\u4e3a\u663e\u8457\u7684\u3002\u8fdb\u800c\u5f97\u5230\u8fd9\u4e2a\u516c\u5f0f \\begin{array}{l} {\\boldsymbol{\\mu}_{t-\\tau}^{l}\\left(\\theta_{t}\\right) \\approx \\mu_{t-\\tau}^{l}\\left(\\theta_{t-\\tau}\\right)+\\frac{\\partial \\mu_{t-\\tau}^{l}\\left(\\theta_{t-\\tau}\\right)}{\\partial \\theta_{t-\\tau}^{l}}\\left(\\theta_{t}^{l}-\\theta_{t-\\tau}^{l}\\right)} \\\\ {\\nu_{t-\\tau}^{l}\\left(\\theta_{t}\\right) \\approx \\nu_{t-\\tau}^{l}\\left(\\theta_{t-\\tau}\\right)+\\frac{\\partial \\nu_{t-\\tau}^{l}\\left(\\theta_{t-\\tau}\\right)}{\\partial \\theta_{t-\\tau}^{l}}\\left(\\theta_{t}^{l}-\\theta_{t-\\tau}^{l}\\right)} \\end{array} \u672c\u6587\u540e\u9762\u7ed9\u51fa\u4e86\u4e00\u4e2aefficient\u7684\u68af\u5ea6\u7b97\u6cd5\uff0c\u4f46\u662f\u4ee3\u7801\u91cc\u76f4\u63a5\u4f7f\u7528\u7684\u662ftorch.autograd.grad\u7684API\u3002","title":"Cross-Iteration Batch Normalization"},{"location":"Building_Blocks/crossBatchNormalization/#cross-iteration-batch-normalization","text":"\u8fd9\u7bc7\u6587\u7ae0\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u7684\u6a21\u5757\uff0c\u79f0\u4e3a\u8de8\u8fed\u4ee3\u7684batchnorm, \u8fd9\u4e2a\u6a21\u5757\u7684\u6e90\u7801\u5728\u5176github\u9879\u76ee\u7684 ./mmdet/models/utils/CBN.py \u627e\u5f97\u5230.","title":"Cross-Iteration Batch Normalization"},{"location":"Building_Blocks/crossBatchNormalization/#motivation","text":"BatchNormalization\u7684\u4e00\u4e2a\u95ee\u9898\u5c31\u662f\u5728batchsize\u6bd4\u8f83\u5c0f\u7684\u65f6\u5019\u6548\u679c\u6bd4\u8f83\u5dee. \u540e\u6765\u53d1\u660e\u4e86 SyncBatchnorm \u6765\u8fdb\u884c\u8de8GPU\u7684batchnorm\u8ba1\u7b97\u3002\u672c\u6587\u8fdb\u4e00\u6b65\u63d0\u51fa\uff0c\u4f7f\u7528\u591a\u6b21\u8fed\u4ee3\u7684\u7ed3\u679c\u5e2e\u52a9batchnorm\u3002 \u76f4\u89c9\u4e0a\u6765\u8bf4,\u5c3d\u7ba1\u7531\u4e8e\u7f51\u7edc\u6743\u91cd\u53d8\u5316\u901f\u5ea6\u4e0d\u5feb\uff0c\u4f46\u662f\u76f8\u90bb\u7684\u8fed\u4ee3\u8fc7\u7a0b\u4e2d\uff0cexpected mean and variance\u8fd8\u662f\u5728\u53d8\u5316\u7684\u3002\u672c\u6587\u501f\u52a9\u4e00\u4e9b\u4f4e\u9636\u8fd1\u4f3c\uff0c\u5bf9\u6743\u91cd\u53d8\u5316\u505a\u4e86\u7b80\u8981\u7684\u5206\u6790\uff0c\u7ed9\u51fa\u4e86CBN\u6a21\u5757\u3002","title":"Motivation"},{"location":"Building_Blocks/crossBatchNormalization/#cbn-algorithm","text":"\u672c\u8d28\u4e0a\u6765\u8bf4\uff0cbatch response\u7684\u5747\u503c\uff0c\u5e73\u65b9\u548c\u4ee5\u53ca\u65b9\u5dee\u7b49\u6570\u636e\u503c\u662f\u5173\u4e8e\u6743\u91cd\u7684\u4e00\u4e2adeterministic\u51fd\u6570\uff0c \u5728\u5b9e\u9645\u8ba1\u7b97\u4e2d\u7531\u67d0\u6b21\u8fed\u4ee3\u5f97\u5230\u7684\u7ed3\u679c\u8fdb\u884cMonte-Carlo\u4eff\u771f\u5f97\u5230\u4e00\u4e2a\u53c2\u8003\u503c\u3002\u8fc7\u53bb\u5f97\u5230\u7684\u5747\u503c\u53c2\u8003\u503c\u4e0e\u5f53\u524d\u503c\u5dee\u503c\u5728\u8fd9\u91cc\u7528\u6cf0\u52d2\u516c\u5f0f\u8fd1\u4f3c\u5f97\u5230.\u800c\u8fd9\u4e2aweight \u4ee3\u7801\u7684\u5b9e\u73b0\u662f\u5bfb\u627eBN\u524d\u7d27\u8ddf\u7740\u7684Conv\u7684weight(\u65e0bias)\u4f5c\u4e3a\u8fd9\u4e2a\u7b97\u6cd5\u4e2d\u7684 \\theta \\begin{aligned} \\mu_{t-\\tau}\\left(\\theta_{t}\\right)=& \\mu_{t-\\tau}\\left(\\theta_{t-\\tau}\\right)+\\frac{\\partial \\mu_{t-\\tau}\\left(\\theta_{t-\\tau}\\right)}{\\partial \\theta_{t-\\tau}}\\left(\\theta_{t}-\\theta_{t-\\tau}\\right) \\\\ &+\\mathbf{O}\\left(\\left\\|\\theta_{t}-\\theta_{t-\\tau}\\right\\|^{2}\\right) \\\\ \\nu_{t-\\tau}\\left(\\theta_{t}\\right)=& \\nu_{t-\\tau}\\left(\\theta_{t-\\tau}\\right)+\\frac{\\partial \\nu_{t-\\tau}\\left(\\theta_{t-\\tau}\\right)}{\\partial \\theta_{t-\\tau}}\\left(\\theta_{t}-\\theta_{t-\\tau}\\right) \\\\ &+\\mathbf{O}\\left(\\left\\|\\theta_{t}-\\theta_{t-\\tau}\\right\\|^{2}\\right) \\end{aligned} \u7701\u7565\u9ad8\u9636\u9879\uff0c\u540c\u65f6\u4f5c\u8005\u6307\u51fa\uff0c\u4e0d\u540cnormalization\u5c42\u4e4b\u95f4\u56e0\u4e3a\u524d\u4e00\u5c42\u6743\u91cd\u53d8\u5316\u5f15\u8d77\u7684\u540e\u4e00\u5c42\u7684\u5747\u503c\u4e0e\u65b9\u5dee\u53d8\u5316\u662f\u6709\u9650\u7684\u3002\u8fd9\u4e2a\u539f\u56e0\u6765\u8bf4\uff0c\u4f5c\u8005\u7684\u7406\u89e3\u662f\u8fd9\u4e2aBatchnorm\u4e2d\u76f4\u63a5\u5b66\u4e60\u7684 scale and shift\u5bf9\u540e\u4e00\u5c42\u5747\u503c\u4e0e\u65b9\u5dee\u7684\u5f71\u54cd\u603b\u4f53\u6765\u8bf4\u662f\u66f4\u4e3a\u663e\u8457\u7684\u3002\u8fdb\u800c\u5f97\u5230\u8fd9\u4e2a\u516c\u5f0f \\begin{array}{l} {\\boldsymbol{\\mu}_{t-\\tau}^{l}\\left(\\theta_{t}\\right) \\approx \\mu_{t-\\tau}^{l}\\left(\\theta_{t-\\tau}\\right)+\\frac{\\partial \\mu_{t-\\tau}^{l}\\left(\\theta_{t-\\tau}\\right)}{\\partial \\theta_{t-\\tau}^{l}}\\left(\\theta_{t}^{l}-\\theta_{t-\\tau}^{l}\\right)} \\\\ {\\nu_{t-\\tau}^{l}\\left(\\theta_{t}\\right) \\approx \\nu_{t-\\tau}^{l}\\left(\\theta_{t-\\tau}\\right)+\\frac{\\partial \\nu_{t-\\tau}^{l}\\left(\\theta_{t-\\tau}\\right)}{\\partial \\theta_{t-\\tau}^{l}}\\left(\\theta_{t}^{l}-\\theta_{t-\\tau}^{l}\\right)} \\end{array} \u672c\u6587\u540e\u9762\u7ed9\u51fa\u4e86\u4e00\u4e2aefficient\u7684\u68af\u5ea6\u7b97\u6cd5\uff0c\u4f46\u662f\u4ee3\u7801\u91cc\u76f4\u63a5\u4f7f\u7528\u7684\u662ftorch.autograd.grad\u7684API\u3002","title":"CBN Algorithm"},{"location":"Building_Blocks/deepPruner/","text":"DeepPruner: Learning Efficient Stereo Matching via Differentiable PatchMatch \u4e0d\u540c\u4e8e\u4f7f\u7528Cost Volomn\u5bc6\u96c6\u9884\u6d4bdisparity\uff0c\u8fd9\u7bc7\u8bba\u6587\u4f7f\u7528Differentiable Patch Match\u8fdb\u884c\u4e00\u4e2a\u7c97\u7565\u7684disparity\u4f30\u8ba1\uff0c\u6700\u7ec8\u4f7f\u5f97\u9700\u8981densely predict\u7684disparity\u503c\u8fdc\u8fdc\u6bd4\u4e4b\u524d\u5c11\u3002\u6240\u4ee5\u79f0\u4e4b\u4e3apruning\u3002\u4f5c\u8005\u79f0\u5b9e\u73b0\u4e86\u5b9e\u65f6\u7684Scencflow\u4f30\u8ba1 Architecture \u7279\u5f81\u63d0\u53d6\u90e8\u5206\u4f7f\u7528\u7c7b\u4f3c\u4e8e PSMNet \u7684\u7ed3\u6784 Patch Match\u5728 \u540e\u6587 Cost Aggregation\u4e0e PSMNet \u4e00\u81f4\uff0c\u4e0d\u8fc7\u7531\u4e8e\u7ecf\u8fc7\u4e86pruning, \u641c\u7d22\u8303\u56f4\u66f4\u5c0f Refinement, \u4f7f\u7528\u5de6\u56fe\u7684image-feature\u901a\u8fc7Conv2D\u540e\u4e0eCost Softmax\u6c42expectation\u540e\u7684\u56fe\u7247residual\u94fe\u63a5 Patch Match Patch Match\u5728\u672c\u6587\u7684\u5b98\u65b9\u5f00\u6e90\u4ed3\u5e93\u91cc\u9762\u53ef\u4ee5\u88ab\u7406\u89e3\u4e3a\u662f\u4e00\u4e2a\u5355\u72ec\u7684\u5927\u6a21\u5757\u3002 \u8bbe\u8ba1\u4e09\u4e2a\u4e3b\u8981\u7f51\u7edc\u5c42\uff1a Sampler Layer \u968f\u673a\u62bd\u6837\u5c42\uff0c\u5728\u56fe\u7247\u4e0a\u7684\u6bcf\u4e2a\u70b9\u91cc\u9762\uff0c \u5728\u5f53\u524d\u641c\u7d22\u7a97\u53e3\u5185\uff0c\u751f\u6210 k \u4e2ax\u65b9\u5411\u7684\u6270\u52a8\u503c\u4e0e k \u4e2ay\u65b9\u5411\u7684\u6270\u52a8\u503c\uff0c\u6700\u540e\u5f62\u6210\u5728\u56fe\u7247\u7684\u6bcf\u4e00\u4e2afeature\u4f4d\u7f6e\u4e0a\uff0c\u4ea7\u751f k*k \u4e2adisparity estimation\u3002 Propagation Layer \u4f20\u64ad\u5c42\uff0c\u8fd9\u91cc\u6709\u4e24\u4e2a\u7248\u672c\uff0c\u8fd4\u56de\u503c\u4e2d\uff0c\u6bcf\u4e00\u4e2afeature map\u4e0a\u70b9\u7684\u7279\u5f81\u5305\u542b\u4e86\u5f53\u524d\u4f4d\u7f6e\u7684disparity estimation\u4ee5\u53ca\u76f8\u90bb\u4f4d\u7f6e\u7684disparity estimation. \u4f7f\u7528\u57fa\u7840\u7248\u672c\u7684\u8bdd\uff0c\u4f5c\u8005\u4f7f\u7528\u4e86\u4e00\u4e2a\u9884\u8bbe\u7684Conv3D,\u53bb\u63d0\u53d6\u7d27\u90bb\u4e0a\u4e0b\u5de6\u53f3\u7684feature\u3002 \u4f7f\u7528\u5feb\u901f\u7248\u672c\u7684\u8bdd\u4f5c\u8005\u5206\u4e24\u6b21propagate & evaluate, \u5206\u522b\u5bf9\u6c34\u5e73\u65b9\u5411\u4e0e\u7ad6\u76f4\u65b9\u5411\u5206\u5f00\u5904\u7406\u3002 Evaluation Layer \u8bc4\u4ef7\u5c42\uff0c \u5c06\u53f3\u56fe\u7279\u5f81\u6839\u636erandomly select\u7684disparity estimation warp\u5230\u5de6\u56fe\u4e0a\u3002\u7136\u540e\u7531\u4e8e\u5728\u4f20\u64ad\u5c42\u7684\u65f6\u5019\u5c31\u5df2\u7ecf\u5c06\u76f8\u90bb\u7684\u7279\u5f81\u90fd\u805a\u96c6\u8d77\u6765\u4e86\uff0c\u5728\u8bc4\u4ef7\u5c42\u5bf9\u6bcf\u4e00\u4e2apixel\u8ba1\u7b97\u5de6\u53f3\u56fe\u76f8\u90bb\u7279\u5f81\u7684\u76f8\u4f3c\u5ea6\u3002\u6700\u540e\u4f7f\u7528Softmax \u6c42\u51fa\u5404\u4e2adisparity estimation\u7684\u6982\u7387\uff0c\u5e76\u8ba1\u7b97\u5176expectation. \u5168\u5c40\u7b97\u6cd5 \u8fed\u4ee3\u5904\u7406\uff0csampler layer-> propagation layer-> evaluation layer. \u4ece\u5168\u5c40\u5e73\u5747\u91c7\u6837\u5f00\u59cb\uff0c\u5728\u4e4b\u540e\u7684\u6bcf\u6b21\u8fed\u4ee3\u4e2d\uff0c\u4f7f\u7528evaluation layer\u4f30\u8ba1\u7684expectation\u9644\u8fd1\u4e00\u5b9a\u5927\u5c0f\u7684\u7a97\u53e3\u4f5c\u4e3a\u65b0\u7684\u91c7\u6837\u8303\u56f4\u3002\u6700\u7ec8\u6bcf\u4e00\u4e2a\u70b9\u7684\u91c7\u6837\u8303\u56f4\u5c31\u88ab\u8fdb\u4e00\u6b65\u5730\u7f29\u5c0f\u3002","title":"DeepPruner: Learning Efficient Stereo Matching via Differentiable PatchMatch"},{"location":"Building_Blocks/deepPruner/#deeppruner-learning-efficient-stereo-matching-via-differentiable-patchmatch","text":"\u4e0d\u540c\u4e8e\u4f7f\u7528Cost Volomn\u5bc6\u96c6\u9884\u6d4bdisparity\uff0c\u8fd9\u7bc7\u8bba\u6587\u4f7f\u7528Differentiable Patch Match\u8fdb\u884c\u4e00\u4e2a\u7c97\u7565\u7684disparity\u4f30\u8ba1\uff0c\u6700\u7ec8\u4f7f\u5f97\u9700\u8981densely predict\u7684disparity\u503c\u8fdc\u8fdc\u6bd4\u4e4b\u524d\u5c11\u3002\u6240\u4ee5\u79f0\u4e4b\u4e3apruning\u3002\u4f5c\u8005\u79f0\u5b9e\u73b0\u4e86\u5b9e\u65f6\u7684Scencflow\u4f30\u8ba1","title":"DeepPruner: Learning Efficient Stereo Matching via Differentiable PatchMatch"},{"location":"Building_Blocks/deepPruner/#architecture","text":"\u7279\u5f81\u63d0\u53d6\u90e8\u5206\u4f7f\u7528\u7c7b\u4f3c\u4e8e PSMNet \u7684\u7ed3\u6784 Patch Match\u5728 \u540e\u6587 Cost Aggregation\u4e0e PSMNet \u4e00\u81f4\uff0c\u4e0d\u8fc7\u7531\u4e8e\u7ecf\u8fc7\u4e86pruning, \u641c\u7d22\u8303\u56f4\u66f4\u5c0f Refinement, \u4f7f\u7528\u5de6\u56fe\u7684image-feature\u901a\u8fc7Conv2D\u540e\u4e0eCost Softmax\u6c42expectation\u540e\u7684\u56fe\u7247residual\u94fe\u63a5","title":"Architecture"},{"location":"Building_Blocks/deepPruner/#patch-match","text":"Patch Match\u5728\u672c\u6587\u7684\u5b98\u65b9\u5f00\u6e90\u4ed3\u5e93\u91cc\u9762\u53ef\u4ee5\u88ab\u7406\u89e3\u4e3a\u662f\u4e00\u4e2a\u5355\u72ec\u7684\u5927\u6a21\u5757\u3002 \u8bbe\u8ba1\u4e09\u4e2a\u4e3b\u8981\u7f51\u7edc\u5c42\uff1a","title":"Patch Match"},{"location":"Building_Blocks/deepPruner/#sampler-layer","text":"\u968f\u673a\u62bd\u6837\u5c42\uff0c\u5728\u56fe\u7247\u4e0a\u7684\u6bcf\u4e2a\u70b9\u91cc\u9762\uff0c \u5728\u5f53\u524d\u641c\u7d22\u7a97\u53e3\u5185\uff0c\u751f\u6210 k \u4e2ax\u65b9\u5411\u7684\u6270\u52a8\u503c\u4e0e k \u4e2ay\u65b9\u5411\u7684\u6270\u52a8\u503c\uff0c\u6700\u540e\u5f62\u6210\u5728\u56fe\u7247\u7684\u6bcf\u4e00\u4e2afeature\u4f4d\u7f6e\u4e0a\uff0c\u4ea7\u751f k*k \u4e2adisparity estimation\u3002","title":"Sampler Layer"},{"location":"Building_Blocks/deepPruner/#propagation-layer","text":"\u4f20\u64ad\u5c42\uff0c\u8fd9\u91cc\u6709\u4e24\u4e2a\u7248\u672c\uff0c\u8fd4\u56de\u503c\u4e2d\uff0c\u6bcf\u4e00\u4e2afeature map\u4e0a\u70b9\u7684\u7279\u5f81\u5305\u542b\u4e86\u5f53\u524d\u4f4d\u7f6e\u7684disparity estimation\u4ee5\u53ca\u76f8\u90bb\u4f4d\u7f6e\u7684disparity estimation. \u4f7f\u7528\u57fa\u7840\u7248\u672c\u7684\u8bdd\uff0c\u4f5c\u8005\u4f7f\u7528\u4e86\u4e00\u4e2a\u9884\u8bbe\u7684Conv3D,\u53bb\u63d0\u53d6\u7d27\u90bb\u4e0a\u4e0b\u5de6\u53f3\u7684feature\u3002 \u4f7f\u7528\u5feb\u901f\u7248\u672c\u7684\u8bdd\u4f5c\u8005\u5206\u4e24\u6b21propagate & evaluate, \u5206\u522b\u5bf9\u6c34\u5e73\u65b9\u5411\u4e0e\u7ad6\u76f4\u65b9\u5411\u5206\u5f00\u5904\u7406\u3002","title":"Propagation Layer"},{"location":"Building_Blocks/deepPruner/#evaluation-layer","text":"\u8bc4\u4ef7\u5c42\uff0c \u5c06\u53f3\u56fe\u7279\u5f81\u6839\u636erandomly select\u7684disparity estimation warp\u5230\u5de6\u56fe\u4e0a\u3002\u7136\u540e\u7531\u4e8e\u5728\u4f20\u64ad\u5c42\u7684\u65f6\u5019\u5c31\u5df2\u7ecf\u5c06\u76f8\u90bb\u7684\u7279\u5f81\u90fd\u805a\u96c6\u8d77\u6765\u4e86\uff0c\u5728\u8bc4\u4ef7\u5c42\u5bf9\u6bcf\u4e00\u4e2apixel\u8ba1\u7b97\u5de6\u53f3\u56fe\u76f8\u90bb\u7279\u5f81\u7684\u76f8\u4f3c\u5ea6\u3002\u6700\u540e\u4f7f\u7528Softmax \u6c42\u51fa\u5404\u4e2adisparity estimation\u7684\u6982\u7387\uff0c\u5e76\u8ba1\u7b97\u5176expectation.","title":"Evaluation Layer"},{"location":"Building_Blocks/deepPruner/#_1","text":"\u8fed\u4ee3\u5904\u7406\uff0csampler layer-> propagation layer-> evaluation layer. \u4ece\u5168\u5c40\u5e73\u5747\u91c7\u6837\u5f00\u59cb\uff0c\u5728\u4e4b\u540e\u7684\u6bcf\u6b21\u8fed\u4ee3\u4e2d\uff0c\u4f7f\u7528evaluation layer\u4f30\u8ba1\u7684expectation\u9644\u8fd1\u4e00\u5b9a\u5927\u5c0f\u7684\u7a97\u53e3\u4f5c\u4e3a\u65b0\u7684\u91c7\u6837\u8303\u56f4\u3002\u6700\u7ec8\u6bcf\u4e00\u4e2a\u70b9\u7684\u91c7\u6837\u8303\u56f4\u5c31\u88ab\u8fdb\u4e00\u6b65\u5730\u7f29\u5c0f\u3002","title":"\u5168\u5c40\u7b97\u6cd5"},{"location":"Building_Blocks/deformable_convnet_v2/","text":"Deformable ConvNets V2: More Deformable, Better Results \u8fd9\u7bc7\u8bba\u6587\u4e00\u65b9\u9762\u8ba8\u8bba\u4e86\u4e00\u4e9b\u8bc4\u4ef7deformable convolution\u6a21\u5757\u6027\u80fd\u7684metric,\u5e76\u7528\u4e00\u4e9b\u5b9e\u9a8c\u5f15\u5165\u3002\u5728\u6a21\u5757\u8bbe\u8ba1\u4e0a\u7ed9\u51fadeformable convolution\u7684\u7ec6\u8282. \u7ed3\u6784\u89e3\u8bfb \u672c\u6587\u7684\u5173\u952e\u60f3\u6cd5\u662f\u5141\u8bb8deformable conv\u6a21\u5757\u5728\u540c\u4e00\u5f20\u56fe\u7247\u540c\u4e00\u4e2achannel\u3001\u4e0d\u540c\u5730\u65b9\u6709\u4e0d\u540c\u7684behavior,\u800c\u539f\u6765\u7684conv\u4ee5\u53ca\u521d\u59cb\u7684deformable conv\u90fd\u662f\u5904\u5904\u76f8\u540c\u7684\u7279\u5f81\u3002\u8ba1\u7b97\u516c\u5f0f y(p) = \\sum^K_{k=1}w_k\\dot x(p + p_k + \\Delta p_k) \\dot \\Delta m_k \u5176\u4e2d p, x(p), y(p) \u8868\u793a\u4f4d\u7f6e p \u4ee5\u53ca\u5f53\u524d\u4f4d\u7f6e\u4e0a\u7684\u8f93\u5165\u3001\u8f93\u51fafeature map\u3002 K \u4e3a\u5377\u79ef\u6838\u7684\u6570\u91cf\uff0c p_k\uff0c w_k \u4e3a\u666e\u901a\u5377\u79ef\u5b66\u4e60\u5230\u7684offset\u4ee5\u53ca\u6743\u91cd(\u5728\u540c\u4e00\u56fe\u7247\u540c\u4e00channel\u5904\u5904\u76f8\u540c), \\Delta p_k, \\Delta m_k \u4e3a\u65b0\u7248\u589e\u52a0\u7684\uff0c\u4e0e\u5f53\u524d\u533a\u57df\u76f8\u5173\u7684offset\u4ee5\u53ca\u6743\u91cd (offset \u5f62\u72b6\u4e3a [B, kernel_size**2, H, W]) \u800c \\Delta p_k, \\Delta m_k \u7531\u5f53\u524d\u4f4d\u7f6e\u5377\u79ef\u4ea7\u751f\uff0c\u6bd4\u5982\u4f7f\u7528K=9\uff0c\u5bf9\u5f53\u524d\u4f4d\u7f6e\u4f7f\u7528\u6b63\u5e38\u5377\u79ef\uff0c\u5f97\u523027\u4e2achannel\uff0c\u524d18\u4e2achannel\u5206\u522b\u5bf9\u5e94 \\Delta p_k \u7684\u4e24\u4e2a\u5750\u6807,\u540e9\u4e2achannel\u7ecf\u8fc7sigmoid\u6fc0\u6d3b\u540e\u53d8\u4e3a \\Delta m_k \u7ec6\u8282 \u53ef\u4ee5\u5b66\u4e60\u672c\u6587github\u4e2d\u4f7f\u7528cuda\u8f85\u52a9\u5e2e\u5fd9\u5f00\u53d1pytorch\u5b50\u6a21\u5757\uff0c\u5982\u679c\u53ef\u80fd\u53ef\u4ee5\u9002\u5f53\u5b66\u4e60\u3002\u6709\u52a0\u901f\u4ee3\u7801\u8fd0\u884c\u7684\u6f5c\u80fd\u3002","title":"Deformable ConvNets V2: More Deformable, Better Results"},{"location":"Building_Blocks/deformable_convnet_v2/#deformable-convnets-v2-more-deformable-better-results","text":"\u8fd9\u7bc7\u8bba\u6587\u4e00\u65b9\u9762\u8ba8\u8bba\u4e86\u4e00\u4e9b\u8bc4\u4ef7deformable convolution\u6a21\u5757\u6027\u80fd\u7684metric,\u5e76\u7528\u4e00\u4e9b\u5b9e\u9a8c\u5f15\u5165\u3002\u5728\u6a21\u5757\u8bbe\u8ba1\u4e0a\u7ed9\u51fadeformable convolution\u7684\u7ec6\u8282.","title":"Deformable ConvNets V2: More Deformable, Better Results"},{"location":"Building_Blocks/deformable_convnet_v2/#_1","text":"\u672c\u6587\u7684\u5173\u952e\u60f3\u6cd5\u662f\u5141\u8bb8deformable conv\u6a21\u5757\u5728\u540c\u4e00\u5f20\u56fe\u7247\u540c\u4e00\u4e2achannel\u3001\u4e0d\u540c\u5730\u65b9\u6709\u4e0d\u540c\u7684behavior,\u800c\u539f\u6765\u7684conv\u4ee5\u53ca\u521d\u59cb\u7684deformable conv\u90fd\u662f\u5904\u5904\u76f8\u540c\u7684\u7279\u5f81\u3002\u8ba1\u7b97\u516c\u5f0f y(p) = \\sum^K_{k=1}w_k\\dot x(p + p_k + \\Delta p_k) \\dot \\Delta m_k \u5176\u4e2d p, x(p), y(p) \u8868\u793a\u4f4d\u7f6e p \u4ee5\u53ca\u5f53\u524d\u4f4d\u7f6e\u4e0a\u7684\u8f93\u5165\u3001\u8f93\u51fafeature map\u3002 K \u4e3a\u5377\u79ef\u6838\u7684\u6570\u91cf\uff0c p_k\uff0c w_k \u4e3a\u666e\u901a\u5377\u79ef\u5b66\u4e60\u5230\u7684offset\u4ee5\u53ca\u6743\u91cd(\u5728\u540c\u4e00\u56fe\u7247\u540c\u4e00channel\u5904\u5904\u76f8\u540c), \\Delta p_k, \\Delta m_k \u4e3a\u65b0\u7248\u589e\u52a0\u7684\uff0c\u4e0e\u5f53\u524d\u533a\u57df\u76f8\u5173\u7684offset\u4ee5\u53ca\u6743\u91cd (offset \u5f62\u72b6\u4e3a [B, kernel_size**2, H, W]) \u800c \\Delta p_k, \\Delta m_k \u7531\u5f53\u524d\u4f4d\u7f6e\u5377\u79ef\u4ea7\u751f\uff0c\u6bd4\u5982\u4f7f\u7528K=9\uff0c\u5bf9\u5f53\u524d\u4f4d\u7f6e\u4f7f\u7528\u6b63\u5e38\u5377\u79ef\uff0c\u5f97\u523027\u4e2achannel\uff0c\u524d18\u4e2achannel\u5206\u522b\u5bf9\u5e94 \\Delta p_k \u7684\u4e24\u4e2a\u5750\u6807,\u540e9\u4e2achannel\u7ecf\u8fc7sigmoid\u6fc0\u6d3b\u540e\u53d8\u4e3a \\Delta m_k","title":"\u7ed3\u6784\u89e3\u8bfb"},{"location":"Building_Blocks/deformable_convnet_v2/#_2","text":"\u53ef\u4ee5\u5b66\u4e60\u672c\u6587github\u4e2d\u4f7f\u7528cuda\u8f85\u52a9\u5e2e\u5fd9\u5f00\u53d1pytorch\u5b50\u6a21\u5757\uff0c\u5982\u679c\u53ef\u80fd\u53ef\u4ee5\u9002\u5f53\u5b66\u4e60\u3002\u6709\u52a0\u901f\u4ee3\u7801\u8fd0\u884c\u7684\u6f5c\u80fd\u3002","title":"\u7ec6\u8282"},{"location":"Building_Blocks/fcanet/","text":"FcaNet: Frequency Channel Attention Networks \u8fd9\u7bc7paper\u662f\u57fa\u4e8e SENet \u4ee5\u53ca CBAM \u7684\u5173\u4e8e\u9891\u9053\u4e0a\u7684\u6ce8\u610f\u529b\u5206\u914d\u7684\u6a21\u5757\u8bbe\u8ba1\u3002 \u5728 SENet \u4e2dSE\u6a21\u5757\u5c06\u7279\u5f81\u56fe\u505a\u4e00\u4e2aglobal average pooling\uff0c\u5f97\u5230\u7684\u7279\u5f81\u77e2\u91cf\u3002\u7279\u5f81\u77e2\u91cf\u518d\u901a\u8fc7\u5168\u8fde\u63a5\u5c42\u6620\u5c04\u5230\u6ce8\u610f\u529b\u77e2\u91cf: \\text{att} = \\text{sigmoid} (\\text{fc}( \\text{GAP}(X))) CBAM \u5219\u540c\u65f6\u4f7f\u7528max-pooling\u4ee5\u53camean-pooling. \u672c\u6587\u6307\u51fa\uff0c\u5b83\u4eec\uff0c\u5c24\u5176\u662f SENet \uff0c\u4ece\u9891\u57df\u4e0a\u770b\u4ec5\u4f7f\u7528\u4e86\u4fe1\u606f\u4e2d\u6700\u4f4e\u9891\u7684\u4e00\u7c07\uff0c\u56e0\u800c\u4fe1\u606f\u6d41\u5931\u6bd4\u8f83\u5927\u3002\u672c\u6587\u7684\u601d\u8def\u662f\u589e\u52a0\u4f7f\u7528\u4e0d\u540c\u9891\u57df\u7684\u4fe1\u606f. 2D DCT \u5728\u56fe\u7247\u57df\u4e0a\uff0c DCT\u7684\u9891\u57df\u8ba1\u7b97: \\begin{array}{l} f_{h, w}^{2 d}=\\sum_{i=0}^{H-1} \\sum_{j=0}^{W-1} x_{i, j}^{2 d} \\underbrace{\\cos \\left(\\frac{\\pi h}{H}\\left(i+\\frac{1}{2}\\right)\\right) \\cos \\left(\\frac{\\pi w}{W}\\left(j+\\frac{1}{2}\\right)\\right)}_{\\text {DCT weights }} \\\\ \\text { s.t. } h \\in\\{0,1, \\cdots, H-1\\}, w \\in\\{0,1, \\cdots, W-1\\} \\end{array} \u9006\u53d8\u6362: \\begin{array}{l} x_{i, j}^{2 d}=\\sum_{h=0}^{H-1} \\sum_{w=0}^{W-1} f_{h, w}^{2 d} \\underbrace{\\cos \\left(\\frac{\\pi h}{H}\\left(i+\\frac{1}{2}\\right)\\right) \\cos \\left(\\frac{\\pi w}{W}\\left(j+\\frac{1}{2}\\right)\\right)}_{\\text {DCT weights }}, \\\\ \\text { s.t. } i \\in\\{0,1, \\cdots, H-1\\}, j \\in\\{0,1, \\cdots, W-1\\} . \\end{array} \u663e\u7136\uff0c\u5168\u5c40\u5e73\u5747\u6c60\u5316\u662f2D DCT\u7684\u4e00\u4e2a\u7279\u6b8acase, FCA Net: \u7b97\u6cd5: \u8bbe\u5b9a K \u4e2a\u57fa\u9891\uff0c\u8ba1\u7b97\u6bd4\u4f8b\u5e38\u6570 B_{h,w}^{i,j} = \\cos(\\frac{\\pi h}{H} (i + \\frac{1}{2})) \\cos(\\frac{\\pi w}{W}(j+\\frac{1}{2})) \u5c06\u8f93\u5165 X \u6cbf\u7740channel\u65b9\u5411\u5206\u4e3a [X^0, X^1, ..., X^{n-1}] . \u5bf9\u4e0d\u540c\u90e8\u5206\u8ba1\u7b972D DCT: \\begin{aligned} \\text { Freq }^{i} &=2 \\mathrm{DDCT}^{u, v}\\left(X^{i}\\right) \\\\ &=\\sum_{h=0}^{H-1} \\sum_{w=0}^{W-1} X_{:, h, w}^{i} B_{h, w}^{u, v} \\\\ & \\text { s.t. } i \\in\\{0,1, \\cdots, n-1\\}, \\end{aligned} concat\u8d77\u6765\u5e76\u4e14\u6267\u884c\u5168\u8fde\u63a5\u4ee5\u53ca sigmoid. \u4ee3\u7801\u4e0a\uff0c\u9664\u4e86\u8ba1\u7b97DCT\u7684\u5e38\u6570\u4e4b\u5916\uff0c\u8fd0\u884c\u65f6\u4ee3\u7801\u53d8\u52a8\u4e0d\u5927.","title":"FcaNet: Frequency Channel Attention Networks"},{"location":"Building_Blocks/fcanet/#fcanet-frequency-channel-attention-networks","text":"\u8fd9\u7bc7paper\u662f\u57fa\u4e8e SENet \u4ee5\u53ca CBAM \u7684\u5173\u4e8e\u9891\u9053\u4e0a\u7684\u6ce8\u610f\u529b\u5206\u914d\u7684\u6a21\u5757\u8bbe\u8ba1\u3002 \u5728 SENet \u4e2dSE\u6a21\u5757\u5c06\u7279\u5f81\u56fe\u505a\u4e00\u4e2aglobal average pooling\uff0c\u5f97\u5230\u7684\u7279\u5f81\u77e2\u91cf\u3002\u7279\u5f81\u77e2\u91cf\u518d\u901a\u8fc7\u5168\u8fde\u63a5\u5c42\u6620\u5c04\u5230\u6ce8\u610f\u529b\u77e2\u91cf: \\text{att} = \\text{sigmoid} (\\text{fc}( \\text{GAP}(X))) CBAM \u5219\u540c\u65f6\u4f7f\u7528max-pooling\u4ee5\u53camean-pooling. \u672c\u6587\u6307\u51fa\uff0c\u5b83\u4eec\uff0c\u5c24\u5176\u662f SENet \uff0c\u4ece\u9891\u57df\u4e0a\u770b\u4ec5\u4f7f\u7528\u4e86\u4fe1\u606f\u4e2d\u6700\u4f4e\u9891\u7684\u4e00\u7c07\uff0c\u56e0\u800c\u4fe1\u606f\u6d41\u5931\u6bd4\u8f83\u5927\u3002\u672c\u6587\u7684\u601d\u8def\u662f\u589e\u52a0\u4f7f\u7528\u4e0d\u540c\u9891\u57df\u7684\u4fe1\u606f.","title":"FcaNet: Frequency Channel Attention Networks"},{"location":"Building_Blocks/fcanet/#2d-dct","text":"\u5728\u56fe\u7247\u57df\u4e0a\uff0c DCT\u7684\u9891\u57df\u8ba1\u7b97: \\begin{array}{l} f_{h, w}^{2 d}=\\sum_{i=0}^{H-1} \\sum_{j=0}^{W-1} x_{i, j}^{2 d} \\underbrace{\\cos \\left(\\frac{\\pi h}{H}\\left(i+\\frac{1}{2}\\right)\\right) \\cos \\left(\\frac{\\pi w}{W}\\left(j+\\frac{1}{2}\\right)\\right)}_{\\text {DCT weights }} \\\\ \\text { s.t. } h \\in\\{0,1, \\cdots, H-1\\}, w \\in\\{0,1, \\cdots, W-1\\} \\end{array} \u9006\u53d8\u6362: \\begin{array}{l} x_{i, j}^{2 d}=\\sum_{h=0}^{H-1} \\sum_{w=0}^{W-1} f_{h, w}^{2 d} \\underbrace{\\cos \\left(\\frac{\\pi h}{H}\\left(i+\\frac{1}{2}\\right)\\right) \\cos \\left(\\frac{\\pi w}{W}\\left(j+\\frac{1}{2}\\right)\\right)}_{\\text {DCT weights }}, \\\\ \\text { s.t. } i \\in\\{0,1, \\cdots, H-1\\}, j \\in\\{0,1, \\cdots, W-1\\} . \\end{array} \u663e\u7136\uff0c\u5168\u5c40\u5e73\u5747\u6c60\u5316\u662f2D DCT\u7684\u4e00\u4e2a\u7279\u6b8acase,","title":"2D DCT"},{"location":"Building_Blocks/fcanet/#fca-net","text":"\u7b97\u6cd5: \u8bbe\u5b9a K \u4e2a\u57fa\u9891\uff0c\u8ba1\u7b97\u6bd4\u4f8b\u5e38\u6570 B_{h,w}^{i,j} = \\cos(\\frac{\\pi h}{H} (i + \\frac{1}{2})) \\cos(\\frac{\\pi w}{W}(j+\\frac{1}{2})) \u5c06\u8f93\u5165 X \u6cbf\u7740channel\u65b9\u5411\u5206\u4e3a [X^0, X^1, ..., X^{n-1}] . \u5bf9\u4e0d\u540c\u90e8\u5206\u8ba1\u7b972D DCT: \\begin{aligned} \\text { Freq }^{i} &=2 \\mathrm{DDCT}^{u, v}\\left(X^{i}\\right) \\\\ &=\\sum_{h=0}^{H-1} \\sum_{w=0}^{W-1} X_{:, h, w}^{i} B_{h, w}^{u, v} \\\\ & \\text { s.t. } i \\in\\{0,1, \\cdots, n-1\\}, \\end{aligned} concat\u8d77\u6765\u5e76\u4e14\u6267\u884c\u5168\u8fde\u63a5\u4ee5\u53ca sigmoid. \u4ee3\u7801\u4e0a\uff0c\u9664\u4e86\u8ba1\u7b97DCT\u7684\u5e38\u6570\u4e4b\u5916\uff0c\u8fd0\u884c\u65f6\u4ee3\u7801\u53d8\u52a8\u4e0d\u5927.","title":"FCA Net:"},{"location":"Building_Blocks/gabor_layer/","text":"Gabor Layers Enhance Network Robustness \u8fd9\u7bc7paper\u63d0\u51fa\u5c06gabor filter \u7528\u4e8e\u66ff\u4ee3\u5377\u79ef\u5c42.\u80fd\u5f97\u5230\u66f4\u9c81\u90a6\u66f4\u4e30\u5bcc\u7684features. Gabor Filter Gabor Filter \u662f\u4e00\u4e2a\u4eff\u751f\u5730\u7ebf\u6027\u6ee4\u6ce2\u5668,\u53ef\u4ee5\u7406\u89e3\u4e3a\u4e00\u4e2a\u88ab\u9ad8\u65af\u8c03\u5236\u8fc7\u7684\u6b63\u5f26\u51fd\u6570. \u6709\u4e00\u4e2a\u6bd4\u8f83\u597d\u7684 medium\u6559\u7a0b \\begin{array}{c} G_{\\theta}\\left(x^{\\prime}, y^{\\prime} ; \\sigma, \\gamma, \\lambda, \\psi\\right):=e^{-\\sigma^{2}\\left(x^{\\prime 2}+\\gamma^{2} y^{\\prime 2}\\right)} \\cos \\left(\\lambda x^{\\prime}+\\psi\\right) \\\\ x^{\\prime}=x \\cos \\theta-y \\sin \\theta \\quad y^{\\prime}=x \\sin \\theta+y \\cos \\theta \\end{array} \\sigma :\u9ad8\u65af\u65b9\u5dee \\gamma :spatial aspect ratio \\theta :\u5e73\u884c\u6761\u7eb9\u7684\u6cd5\u5411 \\psi :\u76f8\u4f4d \\lambda : \u6b63\u5f26\u6ce2\u957f Gabor Layer \u4e0a\u6587\u4e2d\u7684\u63d0\u5230\u7684\u4e94\u4e2a\u53c2\u6570\u9664\u4e86 \\theta \u7686\u4e3a\u7cfb\u7edf\u53ef\u76f4\u63a5\u5b66\u4e60\u53c2\u6570\u3002 \\theta \u5728 (0, 2\\pi) \u95f4\u5747\u5300\u91c7\u6837\u7684\u89d2\u5ea6. \u5b9e\u73b0\u4e0a\uff0c\u7531depth-wise gabor filter convolution \u4e0e \u666e\u901a 1\\times 1 conv\u7ec4\u6210 Regularization \u4f5c\u8005\u628a\u524d\u5411\u51fd\u6570 Lipschitz\u8fde\u7eed\u6027 \u4e0erobustness\u8054\u7cfb\u8d77\u6765\uff0c\u800cgabor filter\u7684lipschitz\u5e38\u6570\u4e3a: L=\\left(1+\\left|X^{\\prime}\\right| e^{-\\sigma^{2} m_{*}^{2}}\\right)\\left(1+\\left|Y^{\\prime}\\right| e^{-\\sigma^{2} \\gamma^{2} n_{*}^{2}}\\right) \u5728\u635f\u5931\u51fd\u6570\u4e0a\u7684\u4fee\u6539\u4e3a: \\mathcal{L}=\\mathcal{L}_{\\mathrm{ce}}-\\beta \\sum_{i} \\sigma_{i}^{2} \u4ee3\u7801\u5b9e\u73b0 import torch import numpy as np import torch.nn as nn import torch.nn.functional as F class GaborLayer(nn.Module): def __init__(self, in_channels, out_channels, kernel_size, stride, padding, kernels, extra_kernels=0, orientations=8, bias1=False, bias2=False, relu=True, use_alphas=True): super(GaborLayer, self).__init__() self.in_channels = in_channels self.out_channels = out_channels self.stride = stride self.padding = padding self.kernels = kernels self.orientations = orientations self.extra_kernels = extra_kernels self.total_kernels = self.kernels + self.extra_kernels self.responses = self.kernels * self.orientations + self.extra_kernels self.kernel_size = kernel_size self.relu = relu self.use_alphas = use_alphas # Replicate parameters so that broadcasting is possible self.Lambdas = nn.Parameter(torch.randn(self.total_kernels).unsqueeze( dim=1).unsqueeze(dim=2).unsqueeze(dim=3)) self.psis = nn.Parameter(torch.randn(self.total_kernels).unsqueeze( dim=1).unsqueeze(dim=2).unsqueeze(dim=3)) self.sigmas = nn.Parameter(torch.randn(self.total_kernels).unsqueeze( dim=1).unsqueeze(dim=2).unsqueeze(dim=3)) self.gammas_y = nn.Parameter(torch.randn(self.total_kernels).unsqueeze( dim=1).unsqueeze(dim=2).unsqueeze(dim=3)) if self.use_alphas: self.alphas = nn.Parameter(torch.randn(self.responses).unsqueeze( dim=1).unsqueeze(dim=2)) # Bias parameters start in zeros self.bias = nn.Parameter(torch.zeros(self.responses)) if bias1 else None # # # # # # # # # # # # # # The meshgrids # The orientations (NOT learnt!) thetas = torch.arange(0., self.orientations) * 2 * np.pi / self.orientations self.thetas = nn.Parameter(thetas, requires_grad=False) # The original meshgrid # Bounding box xmin, xmax = -1, 1 ymin, ymax = -1, 1 x_space = torch.linspace(xmin, xmax, kernel_size) y_space = torch.linspace(ymin, ymax, kernel_size) (y, x) = torch.meshgrid(y_space, x_space) # Unsqueeze for all orientations x, y = x.unsqueeze(dim=0), y.unsqueeze(dim=0) # Cosines and sines cosines = torch.cos(self.thetas).unsqueeze(dim=1).unsqueeze(dim=1) sines = torch.sin(self.thetas).unsqueeze(dim=1).unsqueeze(dim=1) self.x = x * cosines - y * sines self.y = x * sines + y * cosines # Expand for the number of kernels self.x, self.y = self.x.unsqueeze(dim=0), self.y.unsqueeze(dim=0) # Add as parameters self.x = nn.Parameter(self.x, requires_grad=False) self.y = nn.Parameter(self.y, requires_grad=False) # Some precomputed values self.x_sq = nn.Parameter(self.x**2, requires_grad=False) self.y_sq = nn.Parameter(self.y**2, requires_grad=False) # Conv1x1 channels self.channels1x1 = self.responses * self.in_channels self.conv1x1 = nn.Conv2d( in_channels=self.channels1x1, out_channels=out_channels, kernel_size=1, bias=bias2) def forward(self, x): # Generate the Gabor kernels if not hasattr(self, 'gabor_kernels'): self.gabor_kernels = self.generate_gabor_kernels() if self.training: self.gabor_kernels = self.generate_gabor_kernels() # kernels are of shape # [self.kernels*self.orientations + self.extra_kernels, 1, self.kernel_size, self.kernel_size] # Reshape the input: x is of size # [batch_size, in_channels, H, W] # and we need to merge the batch size and the input channels for the # depthwise convolution (and include a '1' channel for the convolution) bs, _, H, W = x.size() x = x.view(bs*self.in_channels, H, W).unsqueeze(dim=1) # Perform convolution out = F.conv2d(input=x, weight=self.gabor_kernels, bias=self.bias, stride=self.stride, padding=self.padding) if self.relu: out = torch.relu(out) # 'out' is of size [batch_size*in_channels, newH, newW] _, _, newH, newW = out.size() # reshape to: out = out.view(bs, self.in_channels, self.responses, newH, newW) # now reshape to out = out.view(bs, self.channels1x1, newH, newW) return self.conv1x1(out) ''' Inspired by https://en.wikipedia.org/wiki/Gabor_filter ''' def generate_gabor_kernels(self): # Precompute some squared terms gammas_y_sq = self.gammas_y**2 sigmas_sq = self.sigmas**2 # ORIENTED KERNELS # Compute Gaussian term # gaussian_term = torch.exp(-.5 * ( (gamma_x**2 * x_t**2 + gamma_y**2 * y_t**2)/ sigma**2 )) ori_y_term = gammas_y_sq[:self.kernels] * self.y_sq exponent_ori = (self.x_sq + ori_y_term) * sigmas_sq[:self.kernels] gaussian_term_ori = torch.exp(-exponent_ori) # Compute Cosine term # cosine_term = torch.cos(2 * np.pi * x_t / Lambda + psi) cosine_term_ori = torch.cos(self.x * self.Lambdas[:self.kernels] + self.psis[:self.kernels]) # 'ori_gb' has shape [self.kernels, self.orientations, kernel_size, kernel_size] ori_gb = gaussian_term_ori * cosine_term_ori # Combine the first two dimensions ori_gb = ori_gb.view(self.kernels * self.orientations, self.kernel_size, self.kernel_size) if self.extra_kernels > 0: # NON-ORIENTED KERNELS (take the first channel of x and y) # Compute Gaussian term # gaussian_term = torch.exp(-.5 * ( (gamma_x**2 * x_t**2 + gamma_y**2 * y_t**2)/ sigma**2 )) y_term = gammas_y_sq[self.kernels:] * self.y_sq[:, :1] exponent = (self.x_sq[:, :1] + y_term) * sigmas_sq[self.kernels:] gaussian_term = torch.exp(-exponent) # Compute Cosine term # cosine_term = torch.cos(2 * np.pi * x_t / Lambda + psi) cosine_term = torch.cos(self.x[:, :1] * self.Lambdas[self.kernels:] + self.psis[self.kernels:]) # 'gb' has shape [self.extra_kernels, 1, kernel_size, kernel_size] gb = gaussian_term * cosine_term # Collapse singleton dimension gb = gb.squeeze(dim=1) # Join oriented and non-oriented filters ori_gb = torch.cat([ori_gb, gb], dim=0) if self.use_alphas: ori_gb = self.alphas * ori_gb return ori_gb.unsqueeze(dim=1)","title":"Gabor Layers Enhance Network Robustness"},{"location":"Building_Blocks/gabor_layer/#gabor-layers-enhance-network-robustness","text":"\u8fd9\u7bc7paper\u63d0\u51fa\u5c06gabor filter \u7528\u4e8e\u66ff\u4ee3\u5377\u79ef\u5c42.\u80fd\u5f97\u5230\u66f4\u9c81\u90a6\u66f4\u4e30\u5bcc\u7684features.","title":"Gabor Layers Enhance Network Robustness"},{"location":"Building_Blocks/gabor_layer/#gabor-filter","text":"Gabor Filter \u662f\u4e00\u4e2a\u4eff\u751f\u5730\u7ebf\u6027\u6ee4\u6ce2\u5668,\u53ef\u4ee5\u7406\u89e3\u4e3a\u4e00\u4e2a\u88ab\u9ad8\u65af\u8c03\u5236\u8fc7\u7684\u6b63\u5f26\u51fd\u6570. \u6709\u4e00\u4e2a\u6bd4\u8f83\u597d\u7684 medium\u6559\u7a0b \\begin{array}{c} G_{\\theta}\\left(x^{\\prime}, y^{\\prime} ; \\sigma, \\gamma, \\lambda, \\psi\\right):=e^{-\\sigma^{2}\\left(x^{\\prime 2}+\\gamma^{2} y^{\\prime 2}\\right)} \\cos \\left(\\lambda x^{\\prime}+\\psi\\right) \\\\ x^{\\prime}=x \\cos \\theta-y \\sin \\theta \\quad y^{\\prime}=x \\sin \\theta+y \\cos \\theta \\end{array} \\sigma :\u9ad8\u65af\u65b9\u5dee \\gamma :spatial aspect ratio \\theta :\u5e73\u884c\u6761\u7eb9\u7684\u6cd5\u5411 \\psi :\u76f8\u4f4d \\lambda : \u6b63\u5f26\u6ce2\u957f","title":"Gabor Filter"},{"location":"Building_Blocks/gabor_layer/#gabor-layer","text":"\u4e0a\u6587\u4e2d\u7684\u63d0\u5230\u7684\u4e94\u4e2a\u53c2\u6570\u9664\u4e86 \\theta \u7686\u4e3a\u7cfb\u7edf\u53ef\u76f4\u63a5\u5b66\u4e60\u53c2\u6570\u3002 \\theta \u5728 (0, 2\\pi) \u95f4\u5747\u5300\u91c7\u6837\u7684\u89d2\u5ea6. \u5b9e\u73b0\u4e0a\uff0c\u7531depth-wise gabor filter convolution \u4e0e \u666e\u901a 1\\times 1 conv\u7ec4\u6210","title":"Gabor Layer"},{"location":"Building_Blocks/gabor_layer/#regularization","text":"\u4f5c\u8005\u628a\u524d\u5411\u51fd\u6570 Lipschitz\u8fde\u7eed\u6027 \u4e0erobustness\u8054\u7cfb\u8d77\u6765\uff0c\u800cgabor filter\u7684lipschitz\u5e38\u6570\u4e3a: L=\\left(1+\\left|X^{\\prime}\\right| e^{-\\sigma^{2} m_{*}^{2}}\\right)\\left(1+\\left|Y^{\\prime}\\right| e^{-\\sigma^{2} \\gamma^{2} n_{*}^{2}}\\right) \u5728\u635f\u5931\u51fd\u6570\u4e0a\u7684\u4fee\u6539\u4e3a: \\mathcal{L}=\\mathcal{L}_{\\mathrm{ce}}-\\beta \\sum_{i} \\sigma_{i}^{2}","title":"Regularization"},{"location":"Building_Blocks/gabor_layer/#_1","text":"import torch import numpy as np import torch.nn as nn import torch.nn.functional as F class GaborLayer(nn.Module): def __init__(self, in_channels, out_channels, kernel_size, stride, padding, kernels, extra_kernels=0, orientations=8, bias1=False, bias2=False, relu=True, use_alphas=True): super(GaborLayer, self).__init__() self.in_channels = in_channels self.out_channels = out_channels self.stride = stride self.padding = padding self.kernels = kernels self.orientations = orientations self.extra_kernels = extra_kernels self.total_kernels = self.kernels + self.extra_kernels self.responses = self.kernels * self.orientations + self.extra_kernels self.kernel_size = kernel_size self.relu = relu self.use_alphas = use_alphas # Replicate parameters so that broadcasting is possible self.Lambdas = nn.Parameter(torch.randn(self.total_kernels).unsqueeze( dim=1).unsqueeze(dim=2).unsqueeze(dim=3)) self.psis = nn.Parameter(torch.randn(self.total_kernels).unsqueeze( dim=1).unsqueeze(dim=2).unsqueeze(dim=3)) self.sigmas = nn.Parameter(torch.randn(self.total_kernels).unsqueeze( dim=1).unsqueeze(dim=2).unsqueeze(dim=3)) self.gammas_y = nn.Parameter(torch.randn(self.total_kernels).unsqueeze( dim=1).unsqueeze(dim=2).unsqueeze(dim=3)) if self.use_alphas: self.alphas = nn.Parameter(torch.randn(self.responses).unsqueeze( dim=1).unsqueeze(dim=2)) # Bias parameters start in zeros self.bias = nn.Parameter(torch.zeros(self.responses)) if bias1 else None # # # # # # # # # # # # # # The meshgrids # The orientations (NOT learnt!) thetas = torch.arange(0., self.orientations) * 2 * np.pi / self.orientations self.thetas = nn.Parameter(thetas, requires_grad=False) # The original meshgrid # Bounding box xmin, xmax = -1, 1 ymin, ymax = -1, 1 x_space = torch.linspace(xmin, xmax, kernel_size) y_space = torch.linspace(ymin, ymax, kernel_size) (y, x) = torch.meshgrid(y_space, x_space) # Unsqueeze for all orientations x, y = x.unsqueeze(dim=0), y.unsqueeze(dim=0) # Cosines and sines cosines = torch.cos(self.thetas).unsqueeze(dim=1).unsqueeze(dim=1) sines = torch.sin(self.thetas).unsqueeze(dim=1).unsqueeze(dim=1) self.x = x * cosines - y * sines self.y = x * sines + y * cosines # Expand for the number of kernels self.x, self.y = self.x.unsqueeze(dim=0), self.y.unsqueeze(dim=0) # Add as parameters self.x = nn.Parameter(self.x, requires_grad=False) self.y = nn.Parameter(self.y, requires_grad=False) # Some precomputed values self.x_sq = nn.Parameter(self.x**2, requires_grad=False) self.y_sq = nn.Parameter(self.y**2, requires_grad=False) # Conv1x1 channels self.channels1x1 = self.responses * self.in_channels self.conv1x1 = nn.Conv2d( in_channels=self.channels1x1, out_channels=out_channels, kernel_size=1, bias=bias2) def forward(self, x): # Generate the Gabor kernels if not hasattr(self, 'gabor_kernels'): self.gabor_kernels = self.generate_gabor_kernels() if self.training: self.gabor_kernels = self.generate_gabor_kernels() # kernels are of shape # [self.kernels*self.orientations + self.extra_kernels, 1, self.kernel_size, self.kernel_size] # Reshape the input: x is of size # [batch_size, in_channels, H, W] # and we need to merge the batch size and the input channels for the # depthwise convolution (and include a '1' channel for the convolution) bs, _, H, W = x.size() x = x.view(bs*self.in_channels, H, W).unsqueeze(dim=1) # Perform convolution out = F.conv2d(input=x, weight=self.gabor_kernels, bias=self.bias, stride=self.stride, padding=self.padding) if self.relu: out = torch.relu(out) # 'out' is of size [batch_size*in_channels, newH, newW] _, _, newH, newW = out.size() # reshape to: out = out.view(bs, self.in_channels, self.responses, newH, newW) # now reshape to out = out.view(bs, self.channels1x1, newH, newW) return self.conv1x1(out) ''' Inspired by https://en.wikipedia.org/wiki/Gabor_filter ''' def generate_gabor_kernels(self): # Precompute some squared terms gammas_y_sq = self.gammas_y**2 sigmas_sq = self.sigmas**2 # ORIENTED KERNELS # Compute Gaussian term # gaussian_term = torch.exp(-.5 * ( (gamma_x**2 * x_t**2 + gamma_y**2 * y_t**2)/ sigma**2 )) ori_y_term = gammas_y_sq[:self.kernels] * self.y_sq exponent_ori = (self.x_sq + ori_y_term) * sigmas_sq[:self.kernels] gaussian_term_ori = torch.exp(-exponent_ori) # Compute Cosine term # cosine_term = torch.cos(2 * np.pi * x_t / Lambda + psi) cosine_term_ori = torch.cos(self.x * self.Lambdas[:self.kernels] + self.psis[:self.kernels]) # 'ori_gb' has shape [self.kernels, self.orientations, kernel_size, kernel_size] ori_gb = gaussian_term_ori * cosine_term_ori # Combine the first two dimensions ori_gb = ori_gb.view(self.kernels * self.orientations, self.kernel_size, self.kernel_size) if self.extra_kernels > 0: # NON-ORIENTED KERNELS (take the first channel of x and y) # Compute Gaussian term # gaussian_term = torch.exp(-.5 * ( (gamma_x**2 * x_t**2 + gamma_y**2 * y_t**2)/ sigma**2 )) y_term = gammas_y_sq[self.kernels:] * self.y_sq[:, :1] exponent = (self.x_sq[:, :1] + y_term) * sigmas_sq[self.kernels:] gaussian_term = torch.exp(-exponent) # Compute Cosine term # cosine_term = torch.cos(2 * np.pi * x_t / Lambda + psi) cosine_term = torch.cos(self.x[:, :1] * self.Lambdas[self.kernels:] + self.psis[self.kernels:]) # 'gb' has shape [self.extra_kernels, 1, kernel_size, kernel_size] gb = gaussian_term * cosine_term # Collapse singleton dimension gb = gb.squeeze(dim=1) # Join oriented and non-oriented filters ori_gb = torch.cat([ori_gb, gb], dim=0) if self.use_alphas: ori_gb = self.alphas * ori_gb return ori_gb.unsqueeze(dim=1)","title":"\u4ee3\u7801\u5b9e\u73b0"},{"location":"Building_Blocks/involution/","text":"Involution: Inverting the Inherence of Convolution for Visual Recognition \u8fd9\u7bc7CVPR2021 paper\u6709\u4e00\u4efdinsightful\u7684 \u77e5\u4e4e\u4ecb\u7ecd","title":"Involution: Inverting the Inherence of Convolution for Visual Recognition"},{"location":"Building_Blocks/involution/#involution-inverting-the-inherence-of-convolution-for-visual-recognition","text":"\u8fd9\u7bc7CVPR2021 paper\u6709\u4e00\u4efdinsightful\u7684 \u77e5\u4e4e\u4ecb\u7ecd","title":"Involution: Inverting the Inherence of Convolution for Visual Recognition"},{"location":"Building_Blocks/mpvit/","text":"MonoViT: Self-Supervised Monocular Depth Estimation with a Vision Transformer \u8fd9\u7bc7paper\u5c1d\u8bd5\u7528vision transformer \u4f5c\u4e3abackbone\u8f93\u51famonodepth. \u6838\u5fc3\u7684\u8fd9\u4e2abackbone\u7684\u6539\u8fdb\u9700\u8981\u5173\u6ce8 MPViT \u4ee5\u53ca CoaT. Decoder\u91c7\u7528\u7684\u65f6 HR-Depth\u7684decoder. Co-Scale Conv-Attentional Image Transformers pdf code \u77e5\u4e4e\u8fde\u63a5 Factorized Attention attention \u539f\u6765\u7684\u6838\u5fc3\u516c\u5f0f\u662f: \\text{Att}(X) = \\text{softmax}(\\frac{QK^T}{\\sqrt{C}})V \u8fd9\u91cc\u7684 QK^T \u6709\u7740 \\mathbf{O}(N^2C) \u7684\u590d\u6742\u5ea6\uff0c\u5bf9\u56fe\u7247\u7684\u5206\u8fa8\u7387\u5f88\u5927\u7684\u65f6\u5019\u6709\u5f88\u5927\u7684\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\uff0c \u672c\u6587\u8ddf\u968fLambdaNets\u91c7\u7528 \\text{FactorAtt}(X) = \\frac{Q}{\\sqrt{C}} (\\text{softmax}(K)^T V) \u628a\u590d\u6742\u5ea6\u53d8\u4e3a \\mathbf{O}(NC^2) , \u5728\u5927\u90e8\u5206\u7684\u7f51\u7edc\u4e2d\u8fd9\u4e2a\u8ba1\u7b97\u91cf\u66f4\u52a0\u53ef\u63a7\u5236\u3002 Convolutional Positional Encoding \u6839\u636e\u524d\u6587\u516c\u5f0f\u53ef\u77e5\uff0c\u4ee5FactorAtt\u7684\u516c\u5f0f\u6765\u8bf4\uff0cQ\u7279\u5f81\u56fe\u4e2d\u4e24\u4e2a\u50cf\u7d20\u4e0a\u7684\u5411\u91cf\u5982\u679c\u6570\u503c\u4e00\u6837\uff0c\u5b83\u4eec\u8f93\u51fa\u7684\u6570\u503c\u5c31\u662f\u4e00\u6837\u7684\uff0c\u548c\u56fe\u7247\u4e2d\u7684\u50cf\u7d20\u4f4d\u7f6e\u4ee5\u53ca\u4ed6\u4eec\u7684\u90bb\u57df\u65e0\u5173\u3002 \u4e00\u90e8\u5206\u7684ViT\u5de5\u4f5c\u4f1a\u628a\u7edd\u5bf9\u4f4d\u7f6e\u4f5c\u4e3aembedding \u653e\u5230token\u91cc\u9762\uff0c\u672c\u6587\u7684\u505a\u6cd5\u662f\u52a0\u5165relative attention map EV \\in \\mathbb{R}^{N\\times C} , E\\in \\mathbb{R}^{N\\times N} . \\text{RelFactorAtt}(X) = \\frac{Q}{\\sqrt{C}}(\\text{softmax}(K)^TV) + EV E_{ij} = \\mathbb{I}(i,j) q_i p_{j-i} , \\mathbb{I} \u662f\u4e00\u4e2a\u6839\u636e\u4f4d\u7f6e\u7684indicator\u51fd\u6570\uff0c\u786e\u4fdd\u53eaattend to \u90bb\u57df\u7684\u90e8\u5206.\u8fd9\u4e2a\u8ba1\u7b97\u590d\u6742\u5ea6\u8fd8\u662f\u6bd4\u8f83\u5927\uff0c\u6240\u4ee5\u672c\u6587\u7684\u65b9\u6848\u662f: \\begin{aligned} \\hat{EV} &= Q \\circ \\text{DepthwiseConv2D(V)} \\end{aligned} \u8fd9\u91cc\u5bf9\u4e0d\u540c\u7684head\u5206\u7ec4\u91c7\u7528\u4e09\u4e2a\u4e0d\u540c\u7684\u5377\u79ef\u6838 (3,5,7), \u7ed3\u679cconcat \u56de\u53bb\u8f93\u51fa\u3002 \u6ce8\u610fMonoViT\u4e2d\u6ca1\u6709\u4f7f\u7528class token. \u5b9e\u9645\u6709\u4e00\u5b9a\u7684\u533a\u522b\u3002 MPViT : Multi-Path Vision Transformer for Dense Prediction pdf code \u77e5\u4e4e\u8fde\u63a5 \u8fd9\u7bc7paper\u5219\u4ee5 \u591a\u5c3a\u5ea6\u878d\u5408\u4e3a\u91cd\u70b9 \u5728\u4e0d\u540c\u7684scale\u5c3a\u5ea6\u4e0a\uff0c\u7528DepthWiseConv\u5728patch embedding\u6a21\u5757\u8fdb\u884c\u4e0b\u91c7\u6837\uff0c\u5e76\u7528\u4e0d\u540c\u7684\u5377\u79ef\u6df1\u5ea6\u5f97\u5230\u4e0d\u540c\u7684\u57fa\u7840\u7279\u5f81\uff0c\u7136\u540e\u8f93\u5165\u5230\u4e00\u4e2aMHCABlock\u4e2d\uff0c\u5176\u4e2d\u4e00\u6761\u8def\u7684\u5206\u652f\u4f7f\u7528\u4e00\u4e2a inverseResNet (conv1x1_bn_act+dwconv3x3_bn_act_conv1x1_bn)\u6b8b\u5dee\u94fe\u63a5\uff0c\u4f5c\u4e3aLocal Feature, \u7136\u540e\u6bcf\u4e2a\u4e0d\u540c\u611f\u53d7\u91ce\u7684\u7279\u5f81\u5206\u522b\u8fdb\u5165\u5e76\u884c\u7684\u5e8f\u5217Transformer, \u8fd9\u91cc\u91c7\u7528\u7684\u5c31\u662f\u524d\u6587\u63d0\u5230\u7684factorized attention; with shared \u5377\u79ef\u4f4d\u7f6eembedding \u548c shared \u5377\u79ef\u76f8\u5bf9\u4f4d\u7f6eembedding. \u5728MonoVit\u4e0d\u4f7f\u7528cls token\u7684\u65f6\u5019\uff0c\u6838\u5fc3\u5c31\u662fdwconv\u3002 \u51e0\u4e2a\u5206\u652f\u4e2d\u5206\u522b\u8ba1\u7b97\u540e\uff0cconcat\u8d77\u6765\u7528conv1x1\u538b\u7f29\u5230\u76ee\u6807\u7279\u5f81\u6570\u4f5c\u4e3a\u5f53\u524d\u5c3a\u5ea6\u7684\u7279\u5f81\u8f93\u51fa\u3002 High Resolution Self-Supervised Monocular Depth Estimation pdf code \u8fd9\u7bc7paper \u501f\u52a9HRNet\u76f8\u5173\u7684\u7814\u7a76\uff0c\u63d0\u51fa\u4e86\u4e0a\u56fe\u7684decoder\u6a21\u5f0f\uff0c\u5e76\u4e14\u63d0\u51fa\u4e86 feature fusion Squeeze-Excitation (fSE) \u6a21\u5757\u6765\u63d0\u5347\u5c42\u7ea7\u878d\u5408\u65f6\u7684\u6027\u80fd\uff0c","title":"MonoViT: Self-Supervised Monocular Depth Estimation with a Vision Transformer"},{"location":"Building_Blocks/mpvit/#monovit-self-supervised-monocular-depth-estimation-with-a-vision-transformer","text":"\u8fd9\u7bc7paper\u5c1d\u8bd5\u7528vision transformer \u4f5c\u4e3abackbone\u8f93\u51famonodepth. \u6838\u5fc3\u7684\u8fd9\u4e2abackbone\u7684\u6539\u8fdb\u9700\u8981\u5173\u6ce8 MPViT \u4ee5\u53ca CoaT. Decoder\u91c7\u7528\u7684\u65f6 HR-Depth\u7684decoder.","title":"MonoViT: Self-Supervised Monocular Depth Estimation with a Vision Transformer"},{"location":"Building_Blocks/mpvit/#co-scale-conv-attentional-image-transformers","text":"pdf code \u77e5\u4e4e\u8fde\u63a5","title":"Co-Scale Conv-Attentional Image Transformers"},{"location":"Building_Blocks/mpvit/#factorized-attention","text":"attention \u539f\u6765\u7684\u6838\u5fc3\u516c\u5f0f\u662f: \\text{Att}(X) = \\text{softmax}(\\frac{QK^T}{\\sqrt{C}})V \u8fd9\u91cc\u7684 QK^T \u6709\u7740 \\mathbf{O}(N^2C) \u7684\u590d\u6742\u5ea6\uff0c\u5bf9\u56fe\u7247\u7684\u5206\u8fa8\u7387\u5f88\u5927\u7684\u65f6\u5019\u6709\u5f88\u5927\u7684\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\uff0c \u672c\u6587\u8ddf\u968fLambdaNets\u91c7\u7528 \\text{FactorAtt}(X) = \\frac{Q}{\\sqrt{C}} (\\text{softmax}(K)^T V) \u628a\u590d\u6742\u5ea6\u53d8\u4e3a \\mathbf{O}(NC^2) , \u5728\u5927\u90e8\u5206\u7684\u7f51\u7edc\u4e2d\u8fd9\u4e2a\u8ba1\u7b97\u91cf\u66f4\u52a0\u53ef\u63a7\u5236\u3002","title":"Factorized Attention"},{"location":"Building_Blocks/mpvit/#convolutional-positional-encoding","text":"\u6839\u636e\u524d\u6587\u516c\u5f0f\u53ef\u77e5\uff0c\u4ee5FactorAtt\u7684\u516c\u5f0f\u6765\u8bf4\uff0cQ\u7279\u5f81\u56fe\u4e2d\u4e24\u4e2a\u50cf\u7d20\u4e0a\u7684\u5411\u91cf\u5982\u679c\u6570\u503c\u4e00\u6837\uff0c\u5b83\u4eec\u8f93\u51fa\u7684\u6570\u503c\u5c31\u662f\u4e00\u6837\u7684\uff0c\u548c\u56fe\u7247\u4e2d\u7684\u50cf\u7d20\u4f4d\u7f6e\u4ee5\u53ca\u4ed6\u4eec\u7684\u90bb\u57df\u65e0\u5173\u3002 \u4e00\u90e8\u5206\u7684ViT\u5de5\u4f5c\u4f1a\u628a\u7edd\u5bf9\u4f4d\u7f6e\u4f5c\u4e3aembedding \u653e\u5230token\u91cc\u9762\uff0c\u672c\u6587\u7684\u505a\u6cd5\u662f\u52a0\u5165relative attention map EV \\in \\mathbb{R}^{N\\times C} , E\\in \\mathbb{R}^{N\\times N} . \\text{RelFactorAtt}(X) = \\frac{Q}{\\sqrt{C}}(\\text{softmax}(K)^TV) + EV E_{ij} = \\mathbb{I}(i,j) q_i p_{j-i} , \\mathbb{I} \u662f\u4e00\u4e2a\u6839\u636e\u4f4d\u7f6e\u7684indicator\u51fd\u6570\uff0c\u786e\u4fdd\u53eaattend to \u90bb\u57df\u7684\u90e8\u5206.\u8fd9\u4e2a\u8ba1\u7b97\u590d\u6742\u5ea6\u8fd8\u662f\u6bd4\u8f83\u5927\uff0c\u6240\u4ee5\u672c\u6587\u7684\u65b9\u6848\u662f: \\begin{aligned} \\hat{EV} &= Q \\circ \\text{DepthwiseConv2D(V)} \\end{aligned} \u8fd9\u91cc\u5bf9\u4e0d\u540c\u7684head\u5206\u7ec4\u91c7\u7528\u4e09\u4e2a\u4e0d\u540c\u7684\u5377\u79ef\u6838 (3,5,7), \u7ed3\u679cconcat \u56de\u53bb\u8f93\u51fa\u3002 \u6ce8\u610fMonoViT\u4e2d\u6ca1\u6709\u4f7f\u7528class token. \u5b9e\u9645\u6709\u4e00\u5b9a\u7684\u533a\u522b\u3002","title":"Convolutional Positional Encoding"},{"location":"Building_Blocks/mpvit/#mpvit-multi-path-vision-transformer-for-dense-prediction","text":"pdf code \u77e5\u4e4e\u8fde\u63a5 \u8fd9\u7bc7paper\u5219\u4ee5 \u591a\u5c3a\u5ea6\u878d\u5408\u4e3a\u91cd\u70b9 \u5728\u4e0d\u540c\u7684scale\u5c3a\u5ea6\u4e0a\uff0c\u7528DepthWiseConv\u5728patch embedding\u6a21\u5757\u8fdb\u884c\u4e0b\u91c7\u6837\uff0c\u5e76\u7528\u4e0d\u540c\u7684\u5377\u79ef\u6df1\u5ea6\u5f97\u5230\u4e0d\u540c\u7684\u57fa\u7840\u7279\u5f81\uff0c\u7136\u540e\u8f93\u5165\u5230\u4e00\u4e2aMHCABlock\u4e2d\uff0c\u5176\u4e2d\u4e00\u6761\u8def\u7684\u5206\u652f\u4f7f\u7528\u4e00\u4e2a inverseResNet (conv1x1_bn_act+dwconv3x3_bn_act_conv1x1_bn)\u6b8b\u5dee\u94fe\u63a5\uff0c\u4f5c\u4e3aLocal Feature, \u7136\u540e\u6bcf\u4e2a\u4e0d\u540c\u611f\u53d7\u91ce\u7684\u7279\u5f81\u5206\u522b\u8fdb\u5165\u5e76\u884c\u7684\u5e8f\u5217Transformer, \u8fd9\u91cc\u91c7\u7528\u7684\u5c31\u662f\u524d\u6587\u63d0\u5230\u7684factorized attention; with shared \u5377\u79ef\u4f4d\u7f6eembedding \u548c shared \u5377\u79ef\u76f8\u5bf9\u4f4d\u7f6eembedding. \u5728MonoVit\u4e0d\u4f7f\u7528cls token\u7684\u65f6\u5019\uff0c\u6838\u5fc3\u5c31\u662fdwconv\u3002 \u51e0\u4e2a\u5206\u652f\u4e2d\u5206\u522b\u8ba1\u7b97\u540e\uff0cconcat\u8d77\u6765\u7528conv1x1\u538b\u7f29\u5230\u76ee\u6807\u7279\u5f81\u6570\u4f5c\u4e3a\u5f53\u524d\u5c3a\u5ea6\u7684\u7279\u5f81\u8f93\u51fa\u3002","title":"MPViT : Multi-Path Vision Transformer for Dense Prediction"},{"location":"Building_Blocks/mpvit/#high-resolution-self-supervised-monocular-depth-estimation","text":"pdf code \u8fd9\u7bc7paper \u501f\u52a9HRNet\u76f8\u5173\u7684\u7814\u7a76\uff0c\u63d0\u51fa\u4e86\u4e0a\u56fe\u7684decoder\u6a21\u5f0f\uff0c\u5e76\u4e14\u63d0\u51fa\u4e86 feature fusion Squeeze-Excitation (fSE) \u6a21\u5757\u6765\u63d0\u5347\u5c42\u7ea7\u878d\u5408\u65f6\u7684\u6027\u80fd\uff0c","title":"High Resolution Self-Supervised Monocular Depth Estimation"},{"location":"Building_Blocks/mutualNet/","text":"MutualNet: Adaptive ConvNet via Mutual Learning from Network Width and Resolution \u8fd9\u7bc7paper\u57fa\u4e8e EfficientNet \u7684\u601d\u8def\uff0c\u5728\u6b64\u4e4b\u4e0a\u7ee7\u7eed\u53d1\u6563\u3002EfficientNet\u63d0\u51fainput scale\u4e0e\u7f51\u7edc\u7684\u5927\u5c0f\u8981\u4e00\u540cScale up. Methods Sandwich Rule \u4e2d\u95f4\u4e24\u4e2a\u7f51\u7edc\u7684\u8f93\u5165\u9700\u8981\u662f\u5728 0.25 \\times, 1.0\\times \u4e4b\u95f4\u9009\u53d6\u8f93\u5165\u5927\u5c0f\u3002 Inplace Distillation \u4e2d\u95f4\u7684\u7f51\u7edc\u4f7f\u7528\u7684\u6743\u91cd\u90fd\u662f\u4e00\u81f4\u7684\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u6700\u5927\u5c42\u4f5c\u4e3aTeacher network,\u4e2d\u95f4\u5c42\u4f5c\u4e3aStudent network,\u4f7f\u7528KL Divergence\u4f5c\u8bad\u7ec3\u3002 Post-statistics of BN \u5728\u8bad\u7ec3\u5e76\u786e\u5b9a\u4e86\u9009\u62e9\u7684\u8f93\u5165\u5c3a\u5ea6\u4e4b\u540e\uff0c\u9700\u8981\u91cd\u65b0\u4e3asubnetwork\u6536\u96c6BN\u6570\u636e\u3002 Mutual Learning of different resolution \u68af\u5ea6\u9610\u8ff0: \\begin{aligned} \\frac{\\partial L}{\\partial W} &=\\frac{\\partial l_{W_{0: 0.4}, I_{R=128}}}{\\partial W_{0: 0.4}}+\\frac{\\partial l_{W_{0: 0.8}, I_{R=192}}}{\\partial W_{0: 0.8}} \\\\ &=\\frac{\\partial l_{W_{0: 0.4}, I_{R=128}}}{\\partial W_{0: 0.4}}+\\left(\\frac{\\partial l_{W_{0: 0.8}, I_{R=192}}}{\\partial W_{0: 0.4}} \\oplus \\frac{\\partial l_{W_{0: 0.8}, I_{R=192}}}{\\partial W_{0.4: 0.8}}\\right) \\\\ &=\\frac{\\partial l_{W_{0: 0.4}, I_{R=128}+\\partial l_{W_{0: 0.8}, I_{R=192}}} \\oplus \\frac{\\partial l_{W_{0: 0.8}, I_{R=192}}}{\\partial W_{0: 0.4}}}{\\partial W_{0.4: 0.8}} \\end{aligned}","title":"MutualNet: Adaptive ConvNet via Mutual Learning from Network Width and Resolution"},{"location":"Building_Blocks/mutualNet/#mutualnet-adaptive-convnet-via-mutual-learning-from-network-width-and-resolution","text":"\u8fd9\u7bc7paper\u57fa\u4e8e EfficientNet \u7684\u601d\u8def\uff0c\u5728\u6b64\u4e4b\u4e0a\u7ee7\u7eed\u53d1\u6563\u3002EfficientNet\u63d0\u51fainput scale\u4e0e\u7f51\u7edc\u7684\u5927\u5c0f\u8981\u4e00\u540cScale up.","title":"MutualNet: Adaptive ConvNet via Mutual Learning from Network Width and Resolution"},{"location":"Building_Blocks/mutualNet/#methods","text":"","title":"Methods"},{"location":"Building_Blocks/mutualNet/#sandwich-rule","text":"\u4e2d\u95f4\u4e24\u4e2a\u7f51\u7edc\u7684\u8f93\u5165\u9700\u8981\u662f\u5728 0.25 \\times, 1.0\\times \u4e4b\u95f4\u9009\u53d6\u8f93\u5165\u5927\u5c0f\u3002","title":"Sandwich Rule"},{"location":"Building_Blocks/mutualNet/#inplace-distillation","text":"\u4e2d\u95f4\u7684\u7f51\u7edc\u4f7f\u7528\u7684\u6743\u91cd\u90fd\u662f\u4e00\u81f4\u7684\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u6700\u5927\u5c42\u4f5c\u4e3aTeacher network,\u4e2d\u95f4\u5c42\u4f5c\u4e3aStudent network,\u4f7f\u7528KL Divergence\u4f5c\u8bad\u7ec3\u3002","title":"Inplace Distillation"},{"location":"Building_Blocks/mutualNet/#post-statistics-of-bn","text":"\u5728\u8bad\u7ec3\u5e76\u786e\u5b9a\u4e86\u9009\u62e9\u7684\u8f93\u5165\u5c3a\u5ea6\u4e4b\u540e\uff0c\u9700\u8981\u91cd\u65b0\u4e3asubnetwork\u6536\u96c6BN\u6570\u636e\u3002","title":"Post-statistics of BN"},{"location":"Building_Blocks/mutualNet/#mutual-learning-of-different-resolution","text":"\u68af\u5ea6\u9610\u8ff0: \\begin{aligned} \\frac{\\partial L}{\\partial W} &=\\frac{\\partial l_{W_{0: 0.4}, I_{R=128}}}{\\partial W_{0: 0.4}}+\\frac{\\partial l_{W_{0: 0.8}, I_{R=192}}}{\\partial W_{0: 0.8}} \\\\ &=\\frac{\\partial l_{W_{0: 0.4}, I_{R=128}}}{\\partial W_{0: 0.4}}+\\left(\\frac{\\partial l_{W_{0: 0.8}, I_{R=192}}}{\\partial W_{0: 0.4}} \\oplus \\frac{\\partial l_{W_{0: 0.8}, I_{R=192}}}{\\partial W_{0.4: 0.8}}\\right) \\\\ &=\\frac{\\partial l_{W_{0: 0.4}, I_{R=128}+\\partial l_{W_{0: 0.8}, I_{R=192}}} \\oplus \\frac{\\partial l_{W_{0: 0.8}, I_{R=192}}}{\\partial W_{0: 0.4}}}{\\partial W_{0.4: 0.8}} \\end{aligned}","title":"Mutual Learning of different resolution"},{"location":"Building_Blocks/omnivore/","text":"OMNIVORE: A Single Model for Many Visual Modalities \u8fd9\u7bc7\u8bba\u6587\u4ecb\u7ecd\u4e86Omnvore\u6a21\u578b\uff0c\u57fa\u672c\u662f\u57fa\u4e8e Swin Transformer \u7684\u590d\u5408\u6a21\u6001\u8f93\u5165\u6a21\u578b\u3002 \u4e0a\u56fe\u5bfb\u627e\u4e0d\u540c\u6570\u636e\u96c6\u4e2dImageNet\u91cc\u7684Nearest neighbour. \u5b9e\u9645\u505a\u6cd5\u4e0a\u662f\u7528\u540c\u4e00\u4e2a\u6a21\u578b\uff0c\u76f4\u63a5\u4f7f\u7528\u4e0d\u540c\u7684\u6a21\u6001\u8fdb\u884c\u8bad\u7ec3\u3002\u53ea\u662f\u9700\u8981\u4e0d\u540c\u7684patch stem\u9002\u5e94\u4e0d\u540c\u7684\u8f93\u5165\u65b9\u6848\u3002\u4e2d\u95f4\u7684Swin Model\u4e0e\u56fe\u7247\u5927\u5c0f\u4ee5\u53ca\u89c6\u9891\u957f\u5ea6\u5173\u7cfb\u4e0d\u5927\u3002 \u540c\u65f6\u8de8\u6570\u636e\u6a21\u578b\uff0c\u7531\u4e8e\u589e\u52a0\u4e86\u9884\u8bad\u7ec3\u4ee5\u53ca\u8bad\u7ec3\u7684\u6570\u636e\u6444\u5165\u91cf\uff0c\u5728RGBD\u4ee5\u53ca\u89c6\u9891\u6570\u636e\u96c6\u4e2d\u8868\u73b0\u51fa\u66f4\u597d\u7684\u6027\u80fd\u3002","title":"OMNIVORE: A Single Model for Many Visual Modalities"},{"location":"Building_Blocks/omnivore/#omnivore-a-single-model-for-many-visual-modalities","text":"\u8fd9\u7bc7\u8bba\u6587\u4ecb\u7ecd\u4e86Omnvore\u6a21\u578b\uff0c\u57fa\u672c\u662f\u57fa\u4e8e Swin Transformer \u7684\u590d\u5408\u6a21\u6001\u8f93\u5165\u6a21\u578b\u3002 \u4e0a\u56fe\u5bfb\u627e\u4e0d\u540c\u6570\u636e\u96c6\u4e2dImageNet\u91cc\u7684Nearest neighbour. \u5b9e\u9645\u505a\u6cd5\u4e0a\u662f\u7528\u540c\u4e00\u4e2a\u6a21\u578b\uff0c\u76f4\u63a5\u4f7f\u7528\u4e0d\u540c\u7684\u6a21\u6001\u8fdb\u884c\u8bad\u7ec3\u3002\u53ea\u662f\u9700\u8981\u4e0d\u540c\u7684patch stem\u9002\u5e94\u4e0d\u540c\u7684\u8f93\u5165\u65b9\u6848\u3002\u4e2d\u95f4\u7684Swin Model\u4e0e\u56fe\u7247\u5927\u5c0f\u4ee5\u53ca\u89c6\u9891\u957f\u5ea6\u5173\u7cfb\u4e0d\u5927\u3002 \u540c\u65f6\u8de8\u6570\u636e\u6a21\u578b\uff0c\u7531\u4e8e\u589e\u52a0\u4e86\u9884\u8bad\u7ec3\u4ee5\u53ca\u8bad\u7ec3\u7684\u6570\u636e\u6444\u5165\u91cf\uff0c\u5728RGBD\u4ee5\u53ca\u89c6\u9891\u6570\u636e\u96c6\u4e2d\u8868\u73b0\u51fa\u66f4\u597d\u7684\u6027\u80fd\u3002","title":"OMNIVORE: A Single Model for Many Visual Modalities"},{"location":"Building_Blocks/pyramid_vit/","text":"Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions \u8fd9\u7bc7paper\u7ed9\u51fa\u4e86\u4e00\u4e2a\u5728\u9ad8\u5206\u8fa8\u7387\u65f6\u4ecd\u80fd\u8fd0\u884c\u7684transformer\u7684backbone,\u5e76\u4e14\u80fd\u8f93\u51fadense head. \u5728stage i , \u7279\u5f81\u56fe\u4f1a\u53d8\u6210 \\frac{H_{i-1}W_{i-1}}{P_i^2} \u4e2apatches \u5bf9\u4e8e\u9ad8\u5206\u8fa8\u7387\u7684\u56fe\uff0c\u53ef\u4ee5\u91c7\u7528\u5982\u56fe\u7684\u65b9\u5f0f\uff0cspatially reduce key\u548cvalue\u7684\u7ef4\u5ea6\uff0c\u4ee5\u63a7\u5236\u5728\u9ad8\u5206\u8fa8\u7387\u65f6\u7684\u7ef4\u5ea6. PVTv2: Improved Baselines with Pyramid Vision Transformer pdf \u8fd9\u7bc7paper \u7ed9\u51fa\u4e09\u4e2a\u65b0\u7684PVT\u6539\u826f: overlapping patch embedding Zero-padding and depth-wise convolution aided feed forward network Linear SRA","title":"Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions"},{"location":"Building_Blocks/pyramid_vit/#pyramid-vision-transformer-a-versatile-backbone-for-dense-prediction-without-convolutions","text":"\u8fd9\u7bc7paper\u7ed9\u51fa\u4e86\u4e00\u4e2a\u5728\u9ad8\u5206\u8fa8\u7387\u65f6\u4ecd\u80fd\u8fd0\u884c\u7684transformer\u7684backbone,\u5e76\u4e14\u80fd\u8f93\u51fadense head. \u5728stage i , \u7279\u5f81\u56fe\u4f1a\u53d8\u6210 \\frac{H_{i-1}W_{i-1}}{P_i^2} \u4e2apatches \u5bf9\u4e8e\u9ad8\u5206\u8fa8\u7387\u7684\u56fe\uff0c\u53ef\u4ee5\u91c7\u7528\u5982\u56fe\u7684\u65b9\u5f0f\uff0cspatially reduce key\u548cvalue\u7684\u7ef4\u5ea6\uff0c\u4ee5\u63a7\u5236\u5728\u9ad8\u5206\u8fa8\u7387\u65f6\u7684\u7ef4\u5ea6.","title":"Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions"},{"location":"Building_Blocks/pyramid_vit/#pvtv2-improved-baselines-with-pyramid-vision-transformer","text":"pdf \u8fd9\u7bc7paper \u7ed9\u51fa\u4e09\u4e2a\u65b0\u7684PVT\u6539\u826f: overlapping patch embedding Zero-padding and depth-wise convolution aided feed forward network Linear SRA","title":"PVTv2: Improved Baselines with Pyramid Vision Transformer"},{"location":"Building_Blocks/replknet/","text":"Scaling Up Your Kernels to 31x31: Revisiting Large Kernel Design in CNNs \u672c\u6587\u4f5c\u8005\u6709\u76f8\u5f53\u5b8c\u6574\u7684 \u4ecb\u7ecd , MegEngine Code . \u57fa\u672c\u903b\u8f91: \u4f7f\u7528\u5927\u5377\u79ef\u6838\u76f4\u63a5\u589e\u5927\u611f\u53d7\u91ce\u4ee5\u53ca\u6709\u6548\u611f\u53d7\u91ce(effective receptive field ERF), \u540c\u65f6\u4f7f\u7528depthwise conv\u63a7\u5236FLOPS\u590d\u6742\u5ea6\u3002 \u66f4\u6539\u5377\u79ef\u7684CUDA\u52a0\u901f\u4ee3\u7801\u4f7f\u5f97\u5377\u79ef\u7b97\u6cd5\u5145\u5206\u5229\u7528\u5927\u5377\u79ef\u6838\u540e\u5728\u5377\u79ef\u6838\u7ef4\u5ea6\u4e0a\u7684\u5e76\u884c\u5ea6. \u6a21\u4eff RepVGG \u7684\u53c2\u6570\u91cd\u6574\u5316\u65b9\u5f0f\uff0c\u589e\u52a0identity connection\u4ee5\u53ca\u5c0f\u5377\u79ef\u6838\u5206\u652f\u3002\u5728\u63a8\u7406\u7684\u65f6\u5019\u878d\u5408\u6210VGG\u5f62\u6001\u7684\u7b80\u5355\u7ed3\u6784\u3002","title":"Scaling Up Your Kernels to 31x31: Revisiting Large Kernel Design in CNNs"},{"location":"Building_Blocks/replknet/#scaling-up-your-kernels-to-31x31-revisiting-large-kernel-design-in-cnns","text":"\u672c\u6587\u4f5c\u8005\u6709\u76f8\u5f53\u5b8c\u6574\u7684 \u4ecb\u7ecd , MegEngine Code . \u57fa\u672c\u903b\u8f91: \u4f7f\u7528\u5927\u5377\u79ef\u6838\u76f4\u63a5\u589e\u5927\u611f\u53d7\u91ce\u4ee5\u53ca\u6709\u6548\u611f\u53d7\u91ce(effective receptive field ERF), \u540c\u65f6\u4f7f\u7528depthwise conv\u63a7\u5236FLOPS\u590d\u6742\u5ea6\u3002 \u66f4\u6539\u5377\u79ef\u7684CUDA\u52a0\u901f\u4ee3\u7801\u4f7f\u5f97\u5377\u79ef\u7b97\u6cd5\u5145\u5206\u5229\u7528\u5927\u5377\u79ef\u6838\u540e\u5728\u5377\u79ef\u6838\u7ef4\u5ea6\u4e0a\u7684\u5e76\u884c\u5ea6. \u6a21\u4eff RepVGG \u7684\u53c2\u6570\u91cd\u6574\u5316\u65b9\u5f0f\uff0c\u589e\u52a0identity connection\u4ee5\u53ca\u5c0f\u5377\u79ef\u6838\u5206\u652f\u3002\u5728\u63a8\u7406\u7684\u65f6\u5019\u878d\u5408\u6210VGG\u5f62\u6001\u7684\u7b80\u5355\u7ed3\u6784\u3002","title":"Scaling Up Your Kernels to 31x31: Revisiting Large Kernel Design in CNNs"},{"location":"Building_Blocks/shape_adaptor/","text":"Shape Adaptor: A Learnable Resizing Module def ShapeAdaptor(input1, input2, alpha, residual=False, r1=0.5, r2=1.0): # sigmoid_alpha = sigmoid(alpha) if having penalty, i.e. penalty = 1; # the penalty value will be defined/computed in the model_training file sigmoid_alpha = torch.sigmoid(alpha) * ShapeAdaptor.penalty + r1 / (r2 - r1) * (ShapeAdaptor.penalty - 1) s_alpha = (r2 - r1) * sigmoid_alpha.item() + r1 # total no. of shape adaptors ShapeAdaptor.counter += 1 # the true current dim without any penalty (will be used for computing the correct penalty value) ShapeAdaptor.current_dim_true *= ((r2 - r1) * torch.sigmoid(alpha).item() + r1) if ShapeAdaptor.type == 'local': # a shape adaptor will drop at least 1 dimension (local structure), used in standard or AutoTL mode ShapeAdaptor.current_dim = int(ShapeAdaptor.current_dim * s_alpha) dim = 1 if ShapeAdaptor.current_dim < 1 else ShapeAdaptor.current_dim # output dim should be at least 1 elif ShapeAdaptor.type == 'global': # a shape adaptor could maintain the same dimension (global structure), used in AutoSC mode ShapeAdaptor.current_dim = ShapeAdaptor.current_dim * s_alpha dim = 1 if ShapeAdaptor.current_dim < 1 else round(ShapeAdaptor.current_dim) # output dim should be at least 1 ''' input1 = resizing(x, scale=r1); input2 = resizing(x, scale=r2) It's important to debug/confirm your model design using these two different implementations. Implementation A: input2_rs = F.interpolate(input2, scale_factor=(1/r2)*s_alpha, mode='bilinear', align_corners=True) input1_rs = F.interpolate(input1, size=input2_rs.shape[-2:], mode='bilinear', align_corners=True) Implementation B: input1_rs = F.interpolate(input1, scale_factor=(1/r1)*s_alpha, mode='bilinear', align_corners=True) input2_rs = F.interpolate(input2, size=input1_rs.shape[-2:], mode='bilinear', align_corners=True) Those two implementations (along with an additional version below) should produce the same shape. Note: +- 1 dim change in intermediate layers is expected due to different rounding methods. ''' input1_rs = F.interpolate(input1, size=dim, mode='bilinear', align_corners=True) input2_rs = F.interpolate(input2, size=dim, mode='bilinear', align_corners=True) if residual: # to keep gradient magnitude consistent with standard residuals: f(x) + x return 2 * (1 - sigmoid_alpha) * input1_rs + 2 * sigmoid_alpha * input2_rs else: return (1 - sigmoid_alpha) * input1_rs + sigmoid_alpha * input2_rs","title":"Shape Adaptor: A Learnable Resizing Module"},{"location":"Building_Blocks/shape_adaptor/#shape-adaptor-a-learnable-resizing-module","text":"def ShapeAdaptor(input1, input2, alpha, residual=False, r1=0.5, r2=1.0): # sigmoid_alpha = sigmoid(alpha) if having penalty, i.e. penalty = 1; # the penalty value will be defined/computed in the model_training file sigmoid_alpha = torch.sigmoid(alpha) * ShapeAdaptor.penalty + r1 / (r2 - r1) * (ShapeAdaptor.penalty - 1) s_alpha = (r2 - r1) * sigmoid_alpha.item() + r1 # total no. of shape adaptors ShapeAdaptor.counter += 1 # the true current dim without any penalty (will be used for computing the correct penalty value) ShapeAdaptor.current_dim_true *= ((r2 - r1) * torch.sigmoid(alpha).item() + r1) if ShapeAdaptor.type == 'local': # a shape adaptor will drop at least 1 dimension (local structure), used in standard or AutoTL mode ShapeAdaptor.current_dim = int(ShapeAdaptor.current_dim * s_alpha) dim = 1 if ShapeAdaptor.current_dim < 1 else ShapeAdaptor.current_dim # output dim should be at least 1 elif ShapeAdaptor.type == 'global': # a shape adaptor could maintain the same dimension (global structure), used in AutoSC mode ShapeAdaptor.current_dim = ShapeAdaptor.current_dim * s_alpha dim = 1 if ShapeAdaptor.current_dim < 1 else round(ShapeAdaptor.current_dim) # output dim should be at least 1 ''' input1 = resizing(x, scale=r1); input2 = resizing(x, scale=r2) It's important to debug/confirm your model design using these two different implementations. Implementation A: input2_rs = F.interpolate(input2, scale_factor=(1/r2)*s_alpha, mode='bilinear', align_corners=True) input1_rs = F.interpolate(input1, size=input2_rs.shape[-2:], mode='bilinear', align_corners=True) Implementation B: input1_rs = F.interpolate(input1, scale_factor=(1/r1)*s_alpha, mode='bilinear', align_corners=True) input2_rs = F.interpolate(input2, size=input1_rs.shape[-2:], mode='bilinear', align_corners=True) Those two implementations (along with an additional version below) should produce the same shape. Note: +- 1 dim change in intermediate layers is expected due to different rounding methods. ''' input1_rs = F.interpolate(input1, size=dim, mode='bilinear', align_corners=True) input2_rs = F.interpolate(input2, size=dim, mode='bilinear', align_corners=True) if residual: # to keep gradient magnitude consistent with standard residuals: f(x) + x return 2 * (1 - sigmoid_alpha) * input1_rs + 2 * sigmoid_alpha * input2_rs else: return (1 - sigmoid_alpha) * input1_rs + sigmoid_alpha * input2_rs","title":"Shape Adaptor: A Learnable Resizing Module"},{"location":"Building_Blocks/swinV2/","text":"Swin Transformer V2: Scaling Up Capacity and Resolution \u8fd9\u7bc7paper\u628aswin transformer\u5c3a\u5ea6\u8fdb\u884c\u4e86\u63d0\u5347\u3002\u5e76\u4e14\u4f7f\u7528\u4e86\u591a\u4e2atrick\u63d0\u5347transformer block\u7684\u6027\u80fd\u3002 \u5728\u628aswin transformer\u4ece\u56fe\u7247\u5206\u7c7b\u8f6c\u5230\u4e0b\u6e38\u4efb\u52a1\u65f6\uff0c\u5e38\u5e38\u9700\u8981\u63d0\u5347\u5206\u8fa8\u7387\u3002\u63d0\u5347\u5206\u8fa8\u7387\u65f6\uff0c\u4e3b\u8981\u9047\u5230\u7684\u95ee\u9898\uff1a \u6a21\u578b\u63d0\u5347\u5206\u8fa8\u7387\u65f6\u539f\u6709\u6a21\u578b\u7684\u6027\u80fd\u53ef\u80fd\u4f1a\u9000\u5316\u3002 swin-transformer\u7684\u5f88\u591a\u57fa\u7840\u64cd\u4f5c\u53ef\u80fd\u548c\u5c3a\u5ea6\u6709\u76f8\u5173\u6027\u3002\u9700\u8981\u66f4\u53ef\u9760\u7684\u8f6c\u79fb\u624b\u6bb5\uff0c\u63d0\u5347\u6a21\u578b\u8fc1\u79fb\u65f6\u7684\u6548\u679c\u3002 \u672c\u6587\u63d0\u51fa\u4e86\u4e00\u7cfb\u5217\u7684\u6280\u5de7\uff0c\u51cf\u5c11\u4e86GPU\u5b58\u50a8\u7684\u4f7f\u7528\uff0c\u4f7f\u5f97\u6a21\u578b\u5728\u66f4\u5927\u7684\u5206\u8fa8\u7387\u65f6\u53ef\u8bad\u7ec3\u3002\u5e76\u63d0\u5347\u4e86\u9884\u8bad\u7ec3\u6548\u679c\u3002 Basic Review of Swin Transformer \u9996\u5148\u56de\u987e\u4e00\u4e0b Swin . Normalization\u662f\u5728MLP\u4e4b\u524d\u7684\uff0c\u79f0\u4e3a pre-normalization. Position Bias\u91c7\u7528\u7684\u662frelative position bias. \u539f\u6765\u7684\u65b9\u6848\u662f\u52a0\u5165\u4e00\u4e2a B\\in R^{M^2 \\times M^2} \u7684\u77e9\u9635\uff0c\u4f7f\u5f97attention\u7684\u8f93\u51fa\u4e3a \\text{Attention}(Q, K, V) = \\text{SoftMax}(QK^T / \\sqrt{d} + B)V \u628a\u6a21\u578b\u8f93\u5165\u63d0\u5347\u7684\u65f6\u5019\u53d1\u73b0\u7684\u95ee\u9898\u3002 \u6a21\u578b\u7684\u5bb9\u91cf\u63d0\u5347\u7684\u8fc7\u7a0b\u4e2d\u7ed3\u679c\u4e0d\u7a33\u5b9a\u3002\u751a\u81f3\u4f1a\u65e0\u6cd5\u8bad\u7ec3\u3002 \u968f\u7740window size\u7684\u53d8\u5316\uff0c\u6a21\u578b\u8fc1\u79fb\u7684\u65f6\u5019\u6027\u80fd\u4f1a\u53d8\u5dee\uff0c\u51e0\u4e4e\u65e0\u6cd5\u8fc1\u79fb\u3002 \u540e\u5f52\u4e00\u5316 \u5206\u6790\u524d\u5f52\u4e00\u5316\u4e0d\u80fd\u4fdd\u8bc1\u8f93\u51fa\u7684\u7a33\u5b9a\u6027\uff0c\u5728\u6b8b\u5dee\u94fe\u63a5\u7684\u65f6\u5019\u53ef\u80fd\u4f1a\u6570\u503c\u4e0d\u7a33\u5b9a\uff0c\u6240\u4ee5\u8fd9\u91cc\u63d0\u51fa\u91c7\u7528post-normalization. \u5728MLP\u4e4b\u540e\u518d\u8fdb\u884cnormalization\uff0c\u8fd9\u6837\u53ef\u4ee5\u4fdd\u8bc1\u7d2f\u52a0\u4e4b\u540e\u7684\u6570\u503c\u4e0d\u4f1a\u592a\u75c5\u6001\u3002 Scaled Cosine Attention \u539f\u6765\u7684Attention\u6a21\u5757\u91cc\u9762\uff0c\u4f7f\u7528\u70b9\u4e58\uff0c\u73b0\u5728\u8fd9\u91cc\u4f7f\u7528scaled cosine \u6765\u8ba1\u7b97\u4e24\u4e2a\u50cf\u7d20\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\u3002 \\text{Sim}(q_i, k_j) = \\text{cos}(q_i,k_j)/\\tau + B_{i,j} \u5176\u4e2d \\tau \u662f\u53ef\u4ee5\u5b66\u4e60\u7684\u5c3a\u5ea6\u53c2\u6570\u3002 Scaling Window Resolution \u5f15\u5165\u4e86log-spaced \u8fde\u7eed\u4f4d\u7f6e\u3002\u4f7f\u5f97\u76f8\u5bf9\u4f4d\u7f6ebias\u53ef\u4ee5\u8ddf\u968f\u7740\u7a97\u53e3\u5927\u5c0f\u8fdb\u884c\u8f6c\u79fb\u3002 \u4e0e\u76f4\u63a5\u4f18\u5316\u4f4d\u7f6e\u53c2\u6570\u4e0d\u540c\uff0c\u672c\u6587\u63d0\u51fa\u8bad\u7ec3\u4e00\u4e2a\u5c0f\u7684meta\u7f51\u7edc(\u4e24\u5c42MLP)\u5b66\u4e60\u4ece\u76f8\u5bf9\u4f4d\u7f6e\u5230bias\u7684\u6620\u5c04\u3002 B(\\Delta x, \\Delta y) = \\mathcal{G}(\\Delta x, \\Delta y) \u4e3a\u4e86\u53ef\u4ee5\u5ef6\u5c55\u5230\u66f4\u5927\u7684\u7a97\u53e3\uff0c\u66f4\u5927\u7684 \\Delta x , \u672c\u6587\u63d0\u51fa\u5728\u8ba1\u7b97positional bias\u7684\u65f6\u5019\u4e0d\u8981\u4f7f\u7528\u539f\u6765\u7684\u5750\u6807\u503c\uff0c\u800c\u662f\u91c7\u7528log\u503c\u3002 \\hat{\\Delta x} = \\text{sign}(x) \\dot \\text{log}(1 + |\\Delta x|) \u7528\u8fd9\u4e2a\u65b9\u6cd5\uff0c\u5728\u62d3\u5c55\u5230\u66f4\u5927\u7684\u7a97\u53e3\u7684\u65f6\u5019\uff0c\u9700\u8981\u5ef6\u62d3\u7684\u500d\u6570\u5927\u5927\u7f29\u5c0f\uff0c\u63d0\u5347\u4e86\u5ef6\u5c55\u65f6\u7684\u7a33\u5b9a\u6027\u3002","title":"Swin Transformer V2: Scaling Up Capacity and Resolution"},{"location":"Building_Blocks/swinV2/#swin-transformer-v2-scaling-up-capacity-and-resolution","text":"\u8fd9\u7bc7paper\u628aswin transformer\u5c3a\u5ea6\u8fdb\u884c\u4e86\u63d0\u5347\u3002\u5e76\u4e14\u4f7f\u7528\u4e86\u591a\u4e2atrick\u63d0\u5347transformer block\u7684\u6027\u80fd\u3002 \u5728\u628aswin transformer\u4ece\u56fe\u7247\u5206\u7c7b\u8f6c\u5230\u4e0b\u6e38\u4efb\u52a1\u65f6\uff0c\u5e38\u5e38\u9700\u8981\u63d0\u5347\u5206\u8fa8\u7387\u3002\u63d0\u5347\u5206\u8fa8\u7387\u65f6\uff0c\u4e3b\u8981\u9047\u5230\u7684\u95ee\u9898\uff1a \u6a21\u578b\u63d0\u5347\u5206\u8fa8\u7387\u65f6\u539f\u6709\u6a21\u578b\u7684\u6027\u80fd\u53ef\u80fd\u4f1a\u9000\u5316\u3002 swin-transformer\u7684\u5f88\u591a\u57fa\u7840\u64cd\u4f5c\u53ef\u80fd\u548c\u5c3a\u5ea6\u6709\u76f8\u5173\u6027\u3002\u9700\u8981\u66f4\u53ef\u9760\u7684\u8f6c\u79fb\u624b\u6bb5\uff0c\u63d0\u5347\u6a21\u578b\u8fc1\u79fb\u65f6\u7684\u6548\u679c\u3002 \u672c\u6587\u63d0\u51fa\u4e86\u4e00\u7cfb\u5217\u7684\u6280\u5de7\uff0c\u51cf\u5c11\u4e86GPU\u5b58\u50a8\u7684\u4f7f\u7528\uff0c\u4f7f\u5f97\u6a21\u578b\u5728\u66f4\u5927\u7684\u5206\u8fa8\u7387\u65f6\u53ef\u8bad\u7ec3\u3002\u5e76\u63d0\u5347\u4e86\u9884\u8bad\u7ec3\u6548\u679c\u3002","title":"Swin Transformer V2: Scaling Up Capacity and Resolution"},{"location":"Building_Blocks/swinV2/#basic-review-of-swin-transformer","text":"\u9996\u5148\u56de\u987e\u4e00\u4e0b Swin . Normalization\u662f\u5728MLP\u4e4b\u524d\u7684\uff0c\u79f0\u4e3a pre-normalization. Position Bias\u91c7\u7528\u7684\u662frelative position bias. \u539f\u6765\u7684\u65b9\u6848\u662f\u52a0\u5165\u4e00\u4e2a B\\in R^{M^2 \\times M^2} \u7684\u77e9\u9635\uff0c\u4f7f\u5f97attention\u7684\u8f93\u51fa\u4e3a \\text{Attention}(Q, K, V) = \\text{SoftMax}(QK^T / \\sqrt{d} + B)V \u628a\u6a21\u578b\u8f93\u5165\u63d0\u5347\u7684\u65f6\u5019\u53d1\u73b0\u7684\u95ee\u9898\u3002 \u6a21\u578b\u7684\u5bb9\u91cf\u63d0\u5347\u7684\u8fc7\u7a0b\u4e2d\u7ed3\u679c\u4e0d\u7a33\u5b9a\u3002\u751a\u81f3\u4f1a\u65e0\u6cd5\u8bad\u7ec3\u3002 \u968f\u7740window size\u7684\u53d8\u5316\uff0c\u6a21\u578b\u8fc1\u79fb\u7684\u65f6\u5019\u6027\u80fd\u4f1a\u53d8\u5dee\uff0c\u51e0\u4e4e\u65e0\u6cd5\u8fc1\u79fb\u3002","title":"Basic Review of Swin Transformer"},{"location":"Building_Blocks/swinV2/#_1","text":"\u5206\u6790\u524d\u5f52\u4e00\u5316\u4e0d\u80fd\u4fdd\u8bc1\u8f93\u51fa\u7684\u7a33\u5b9a\u6027\uff0c\u5728\u6b8b\u5dee\u94fe\u63a5\u7684\u65f6\u5019\u53ef\u80fd\u4f1a\u6570\u503c\u4e0d\u7a33\u5b9a\uff0c\u6240\u4ee5\u8fd9\u91cc\u63d0\u51fa\u91c7\u7528post-normalization. \u5728MLP\u4e4b\u540e\u518d\u8fdb\u884cnormalization\uff0c\u8fd9\u6837\u53ef\u4ee5\u4fdd\u8bc1\u7d2f\u52a0\u4e4b\u540e\u7684\u6570\u503c\u4e0d\u4f1a\u592a\u75c5\u6001\u3002","title":"\u540e\u5f52\u4e00\u5316"},{"location":"Building_Blocks/swinV2/#scaled-cosine-attention","text":"\u539f\u6765\u7684Attention\u6a21\u5757\u91cc\u9762\uff0c\u4f7f\u7528\u70b9\u4e58\uff0c\u73b0\u5728\u8fd9\u91cc\u4f7f\u7528scaled cosine \u6765\u8ba1\u7b97\u4e24\u4e2a\u50cf\u7d20\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\u3002 \\text{Sim}(q_i, k_j) = \\text{cos}(q_i,k_j)/\\tau + B_{i,j} \u5176\u4e2d \\tau \u662f\u53ef\u4ee5\u5b66\u4e60\u7684\u5c3a\u5ea6\u53c2\u6570\u3002","title":"Scaled Cosine Attention"},{"location":"Building_Blocks/swinV2/#scaling-window-resolution","text":"\u5f15\u5165\u4e86log-spaced \u8fde\u7eed\u4f4d\u7f6e\u3002\u4f7f\u5f97\u76f8\u5bf9\u4f4d\u7f6ebias\u53ef\u4ee5\u8ddf\u968f\u7740\u7a97\u53e3\u5927\u5c0f\u8fdb\u884c\u8f6c\u79fb\u3002 \u4e0e\u76f4\u63a5\u4f18\u5316\u4f4d\u7f6e\u53c2\u6570\u4e0d\u540c\uff0c\u672c\u6587\u63d0\u51fa\u8bad\u7ec3\u4e00\u4e2a\u5c0f\u7684meta\u7f51\u7edc(\u4e24\u5c42MLP)\u5b66\u4e60\u4ece\u76f8\u5bf9\u4f4d\u7f6e\u5230bias\u7684\u6620\u5c04\u3002 B(\\Delta x, \\Delta y) = \\mathcal{G}(\\Delta x, \\Delta y) \u4e3a\u4e86\u53ef\u4ee5\u5ef6\u5c55\u5230\u66f4\u5927\u7684\u7a97\u53e3\uff0c\u66f4\u5927\u7684 \\Delta x , \u672c\u6587\u63d0\u51fa\u5728\u8ba1\u7b97positional bias\u7684\u65f6\u5019\u4e0d\u8981\u4f7f\u7528\u539f\u6765\u7684\u5750\u6807\u503c\uff0c\u800c\u662f\u91c7\u7528log\u503c\u3002 \\hat{\\Delta x} = \\text{sign}(x) \\dot \\text{log}(1 + |\\Delta x|) \u7528\u8fd9\u4e2a\u65b9\u6cd5\uff0c\u5728\u62d3\u5c55\u5230\u66f4\u5927\u7684\u7a97\u53e3\u7684\u65f6\u5019\uff0c\u9700\u8981\u5ef6\u62d3\u7684\u500d\u6570\u5927\u5927\u7f29\u5c0f\uff0c\u63d0\u5347\u4e86\u5ef6\u5c55\u65f6\u7684\u7a33\u5b9a\u6027\u3002","title":"Scaling Window Resolution"},{"location":"Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/","text":"Aggressive Driving with Model Predictive Path Integral Control \u672c\u7bc7\u8bba\u6587\u5c06Path integral Control\u4f7f\u7528\u5728\u81ea\u52a8\u9a7e\u9a76\u4e4b\u4e2d\uff0cPath integral Control\u5c5e\u4e8e\u968f\u673a\u6700\u4f18\u63a7\u5236\u7684\u7b97\u6cd5\uff0c\u6982\u5ff5\u5927\u6982\u662f\u8fd9\u6837\u7684\uff0c\u4f7f\u7528\u968f\u673a\u63a7\u5236\u503c\u8fdb\u884c\u63a2\u7d22\u4e0e\u9884\u6d4b\uff0c\u7136\u540e\u4e0e\u5176\u9009\u62e9\u5176\u4e2d\u6700\u4f18\u7684\uff0c\u7efc\u5408\u5404\u4e2a\u70b9\u7684\u6210\u672c\u4ee5\u53ca\u884c\u52a8\u8f68\u8ff9\uff0c\u5f97\u5230\u4e00\u4e2a\u7ecf\u9a8c\u4e0a\u7684\u6700\u4f18\u63a7\u5236\u7ed3\u679c\u3002 \u5728\u6bcf\u4e00\u4e2a\u65f6\u523b\uff1a \u786e\u5b9a\u53c2\u6570\uff0c\u968f\u673a\u91c7\u6837\u6570K\uff0c\u9884\u6d4b\u6b65\u957fN\uff0c\u521d\u59cb\u5316N\u4e2a\u521d\u59cb\u7684\u63a7\u5236\u5e8f\u5217\u3002 \u5bf9\u6bcf\u4e00\u4e2a\u91c7\u6837\u70b9(\u5e76\u884c\u8fdb\u884c)\uff0c\u52a0\u5165\u968f\u673a\u6270\u52a8\u5e8f\u5217\u5e76\u5f80\u524d\u9884\u6d4bN\u6b65\uff0c\u5f97\u5230\u5206\u522b\u7684\u603bcost \u8fd9\u91cc\u6ce8\u610f\u56fe\u4e2d\u7684 HG^{-1} = \\gamma \u662f\u4e00\u4e2a\u66f4\u65b0\u63a7\u5236\u53c2\u6570 \u4f7f\u7528\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u7684\u601d\u8def\uff0c\u53ea\u6267\u884c\u7b2c\u4e00\u4e2a\u63a7\u5236\u7ed3\u679c \u5b9e\u73b0\u7ec6\u8282\uff1a 1. \u6bcf\u4e00\u4e2a\u91c7\u6837\u70b9\u7684\u9884\u6d4b\u66f4\u65b0\u4ee5\u53ca\u6c42\u548c\u53ef\u4ee5\u7528GPU\u5e76\u884c\u5b8c\u6210(\u7528\u4e0a\u5343\u4e2a\u968f\u673a\u91c7\u6837\u6837\u672c) 2. \u5728\u5b9e\u65f6\u6267\u884c\u7684\u65f6\u5019\uff0c\u6bcf\u4e2a\u65f6\u523b\u53ea\u9700\u8981\u968f\u673a\u4f18\u5316\u6574\u4e2a\u5e8f\u5217\u4e00\u6b65\uff0c\u5e8f\u5217\u540e\u9762\u5730\u65b9\u53ef\u4ee5\u4f7f\u7528\u524d\u4e00\u4e2a\u65f6\u95f4\u6b65\u65f6\u7684\u4f18\u5316\u7ed3\u679c\u3002 3. \u672c\u6587\u63a7\u5236\u7684\u65f6\u5019\u8fd8\u662f\u5bf9\u63a7\u5236\u7ed3\u679c\u8fdb\u884c\u4e86\u5e73\u6ed1\u3002 \u7406\u8bba\u7ec6\u8282\uff1a 1. \u8981\u5b9e\u73b0\u4e0a\u6587\u56fe\u4e2d\u7684\u7b97\u6cd5\u8fd9\u4e00\u5316\u7b80\uff0c\u8981\u6c42\u7684\u662f\u968f\u673a\u566a\u58f0\u7684\u4f5c\u7528\u77e9\u9635 B \u7684\u4e0d\u53ef\u63a7\u5236\u4e0e\u53ef\u63a7\u5236\u90e8\u5206\uff0c\u4e0d\u53ef\u4ee5\u6709\u76f8\u5173\u9879 2. HG^{-1} = G_c^{-1} B_c \uff0c G_c \u5c31\u662f\u7cfb\u7edf\u52a8\u6001\u65b9\u7a0b\u4e2d\u7684\u53ef\u63a7\u90e8\u5206, B_c \u662f\u5e03\u6717\u6f02\u79fb\u5bf9\u5e94\u7684\u53ef\u63a7\u90e8\u5206\uff0c\u5728\u6211\u4eec\u4f18\u5316\u7684\u65f6\u5019\u5f80\u5f80\u4e24\u4e2a\u77e9\u9635\u662f\u76f8\u540c\u7684(\u5047\u8bbe\u63a7\u5236\u7ed3\u679c\u4ee5\u53ca\u6210\u672c\u4e0a\u662f\u4eff\u5c04\u7684)\uff0c\u90a3\u4e48\u8fd9\u4e2a \\gamma = 1 \u4e3a\u5e38\u6001 3. \\lambda \u662f\u6e29\u5ea6\uff0c\u4f1a\u63a7\u5236\u5141\u8bb8\u6ce2\u52a8\u7684\u81ea\u7531\u80fd\u7684\u5927\u5c0f,\u76f4\u89c9\u7ed3\u679c\u662f\u5f53 \\lambda \u8d8a\u5c0f\u7684\u65f6\u5019\uff0c\u7ed3\u679c\u66f4\u8d8b\u5411\u76f4\u63a5\u53d6\u6700\u4f18\uff0c\u5f53 \\lambda \u8d8a\u5927\u7684\u65f6\u5019\uff0c\u7ed3\u679c\u66f4\u8d8b\u5411\u4e8e\u968f\u673a\u9009\u62e9\u3002","title":"Aggressive Driving with Model Predictive Path Integral Control"},{"location":"Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/#aggressive-driving-with-model-predictive-path-integral-control","text":"\u672c\u7bc7\u8bba\u6587\u5c06Path integral Control\u4f7f\u7528\u5728\u81ea\u52a8\u9a7e\u9a76\u4e4b\u4e2d\uff0cPath integral Control\u5c5e\u4e8e\u968f\u673a\u6700\u4f18\u63a7\u5236\u7684\u7b97\u6cd5\uff0c\u6982\u5ff5\u5927\u6982\u662f\u8fd9\u6837\u7684\uff0c\u4f7f\u7528\u968f\u673a\u63a7\u5236\u503c\u8fdb\u884c\u63a2\u7d22\u4e0e\u9884\u6d4b\uff0c\u7136\u540e\u4e0e\u5176\u9009\u62e9\u5176\u4e2d\u6700\u4f18\u7684\uff0c\u7efc\u5408\u5404\u4e2a\u70b9\u7684\u6210\u672c\u4ee5\u53ca\u884c\u52a8\u8f68\u8ff9\uff0c\u5f97\u5230\u4e00\u4e2a\u7ecf\u9a8c\u4e0a\u7684\u6700\u4f18\u63a7\u5236\u7ed3\u679c\u3002 \u5728\u6bcf\u4e00\u4e2a\u65f6\u523b\uff1a \u786e\u5b9a\u53c2\u6570\uff0c\u968f\u673a\u91c7\u6837\u6570K\uff0c\u9884\u6d4b\u6b65\u957fN\uff0c\u521d\u59cb\u5316N\u4e2a\u521d\u59cb\u7684\u63a7\u5236\u5e8f\u5217\u3002 \u5bf9\u6bcf\u4e00\u4e2a\u91c7\u6837\u70b9(\u5e76\u884c\u8fdb\u884c)\uff0c\u52a0\u5165\u968f\u673a\u6270\u52a8\u5e8f\u5217\u5e76\u5f80\u524d\u9884\u6d4bN\u6b65\uff0c\u5f97\u5230\u5206\u522b\u7684\u603bcost \u8fd9\u91cc\u6ce8\u610f\u56fe\u4e2d\u7684 HG^{-1} = \\gamma \u662f\u4e00\u4e2a\u66f4\u65b0\u63a7\u5236\u53c2\u6570 \u4f7f\u7528\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u7684\u601d\u8def\uff0c\u53ea\u6267\u884c\u7b2c\u4e00\u4e2a\u63a7\u5236\u7ed3\u679c \u5b9e\u73b0\u7ec6\u8282\uff1a 1. \u6bcf\u4e00\u4e2a\u91c7\u6837\u70b9\u7684\u9884\u6d4b\u66f4\u65b0\u4ee5\u53ca\u6c42\u548c\u53ef\u4ee5\u7528GPU\u5e76\u884c\u5b8c\u6210(\u7528\u4e0a\u5343\u4e2a\u968f\u673a\u91c7\u6837\u6837\u672c) 2. \u5728\u5b9e\u65f6\u6267\u884c\u7684\u65f6\u5019\uff0c\u6bcf\u4e2a\u65f6\u523b\u53ea\u9700\u8981\u968f\u673a\u4f18\u5316\u6574\u4e2a\u5e8f\u5217\u4e00\u6b65\uff0c\u5e8f\u5217\u540e\u9762\u5730\u65b9\u53ef\u4ee5\u4f7f\u7528\u524d\u4e00\u4e2a\u65f6\u95f4\u6b65\u65f6\u7684\u4f18\u5316\u7ed3\u679c\u3002 3. \u672c\u6587\u63a7\u5236\u7684\u65f6\u5019\u8fd8\u662f\u5bf9\u63a7\u5236\u7ed3\u679c\u8fdb\u884c\u4e86\u5e73\u6ed1\u3002 \u7406\u8bba\u7ec6\u8282\uff1a 1. \u8981\u5b9e\u73b0\u4e0a\u6587\u56fe\u4e2d\u7684\u7b97\u6cd5\u8fd9\u4e00\u5316\u7b80\uff0c\u8981\u6c42\u7684\u662f\u968f\u673a\u566a\u58f0\u7684\u4f5c\u7528\u77e9\u9635 B \u7684\u4e0d\u53ef\u63a7\u5236\u4e0e\u53ef\u63a7\u5236\u90e8\u5206\uff0c\u4e0d\u53ef\u4ee5\u6709\u76f8\u5173\u9879 2. HG^{-1} = G_c^{-1} B_c \uff0c G_c \u5c31\u662f\u7cfb\u7edf\u52a8\u6001\u65b9\u7a0b\u4e2d\u7684\u53ef\u63a7\u90e8\u5206, B_c \u662f\u5e03\u6717\u6f02\u79fb\u5bf9\u5e94\u7684\u53ef\u63a7\u90e8\u5206\uff0c\u5728\u6211\u4eec\u4f18\u5316\u7684\u65f6\u5019\u5f80\u5f80\u4e24\u4e2a\u77e9\u9635\u662f\u76f8\u540c\u7684(\u5047\u8bbe\u63a7\u5236\u7ed3\u679c\u4ee5\u53ca\u6210\u672c\u4e0a\u662f\u4eff\u5c04\u7684)\uff0c\u90a3\u4e48\u8fd9\u4e2a \\gamma = 1 \u4e3a\u5e38\u6001 3. \\lambda \u662f\u6e29\u5ea6\uff0c\u4f1a\u63a7\u5236\u5141\u8bb8\u6ce2\u52a8\u7684\u81ea\u7531\u80fd\u7684\u5927\u5c0f,\u76f4\u89c9\u7ed3\u679c\u662f\u5f53 \\lambda \u8d8a\u5c0f\u7684\u65f6\u5019\uff0c\u7ed3\u679c\u66f4\u8d8b\u5411\u76f4\u63a5\u53d6\u6700\u4f18\uff0c\u5f53 \\lambda \u8d8a\u5927\u7684\u65f6\u5019\uff0c\u7ed3\u679c\u66f4\u8d8b\u5411\u4e8e\u968f\u673a\u9009\u62e9\u3002","title":"Aggressive Driving with Model Predictive Path Integral Control"},{"location":"Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/","text":"Backprop KF: Learning Discriminative DeterministicState Estimators \u8fd9\u662f\u4e00\u7bc7\u6bd4\u8f83\u65e9\u671f\u7684\u8bba\u6587\uff0c\u4ecb\u7ecd\u7684\u662f\u53ef\u5fae\u5206\u5361\u5c14\u66fc\u6ee4\u6ce2\u3002 \u6838\u5fc3\u7ed3\u6784 \u7528CNN\u8f93\u51fa\u4e00\u7ef4\u7684\u89c2\u6d4b\u77e2\u91cf\uff0c\u4ee5\u53ca\u89c2\u6d4b\u7684covariance\u77e9\u9635\u3002 \u89c2\u6d4b\u7684covariance\u77e9\u9635\u7684\u751f\u6210\u65b9\u5f0f\uff1a Relu->diag->square \u5176\u4f59\u4e2d\u95f4\u516c\u5f0f\u4e0e\u5361\u5c14\u66fc\u6ee4\u6ce2\u4e00\u81f4\uff0c\u6a21\u578b\u52a8\u6001\u65b9\u7a0b\u4e0e\u89c2\u6d4b\u65b9\u7a0b\u7686(\u72b6\u6001\u7a7a\u95f4ABCD\uff0cQ\u77e9\u9635)\u7686\u4e3a\u53ef\u5b66\u4e60\u7684\u53c2\u6570","title":"Backprop KF: Learning Discriminative DeterministicState Estimators"},{"location":"Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/#backprop-kf-learning-discriminative-deterministicstate-estimators","text":"\u8fd9\u662f\u4e00\u7bc7\u6bd4\u8f83\u65e9\u671f\u7684\u8bba\u6587\uff0c\u4ecb\u7ecd\u7684\u662f\u53ef\u5fae\u5206\u5361\u5c14\u66fc\u6ee4\u6ce2\u3002","title":"Backprop KF: Learning Discriminative DeterministicState Estimators"},{"location":"Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/#_1","text":"\u7528CNN\u8f93\u51fa\u4e00\u7ef4\u7684\u89c2\u6d4b\u77e2\u91cf\uff0c\u4ee5\u53ca\u89c2\u6d4b\u7684covariance\u77e9\u9635\u3002 \u89c2\u6d4b\u7684covariance\u77e9\u9635\u7684\u751f\u6210\u65b9\u5f0f\uff1a Relu->diag->square \u5176\u4f59\u4e2d\u95f4\u516c\u5f0f\u4e0e\u5361\u5c14\u66fc\u6ee4\u6ce2\u4e00\u81f4\uff0c\u6a21\u578b\u52a8\u6001\u65b9\u7a0b\u4e0e\u89c2\u6d4b\u65b9\u7a0b\u7686(\u72b6\u6001\u7a7a\u95f4ABCD\uff0cQ\u77e9\u9635)\u7686\u4e3a\u53ef\u5b66\u4e60\u7684\u53c2\u6570","title":"\u6838\u5fc3\u7ed3\u6784"},{"location":"Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/","text":"Cognitive Mapping and Planning for Visual Navigation \u8fd9\u7bc7\u8bba\u6587\u57282017\u5e74\u63d0\u51fa\u4e86\u76f4\u63a5\u5229\u7528\u89c6\u89c9\u8fdb\u884c\u5bfc\u822a\u7684\u601d\u8def \u603b\u4f53\u7ed3\u6784: \u8f93\u5165\u4e3aEgomotion, Goal, \u89c6\u89c9\u56fe\u50cf\u8f93\u5165, \u8f93\u51fa\u65f6action\uff0c\u4e2d\u95f4\u4f7f\u7528\u53ef\u5fae\u5206Mapper\u548c\u53ef\u5fae\u5206Planner Mapper \u8fd9\u91cc\u7ed9\u51fa\u7684mapper\u7ed3\u6784\u5982\u4e0a\u56fe\uff0c\u5df2\u77e5\u4e0a\u4e00\u65f6\u523b\u5bf9\u73af\u5883\u5efa\u56fe\u7ed3\u679c\u4ee5\u53ca\u672c\u4f53\u8fd0\u52a8\uff0c\u901a\u8fc7\u53ef\u5fae\u5206\u7684\u56fe\u7247\u5e73\u79fb\u548c\u65cb\u8f6c\uff0c\u5f97\u5230\u5bf9\u73b0\u5728\u5730\u56fe\u7684\u4f30\u8ba1\uff0c\u4e0e\u65b0\u56fe\u7247\u7ecf\u8fc7encoder\u3001decoder\u4e4b\u540e\u7684\u8f93\u51fa\u8fdb\u884c\u5408\u5e76\uff0c\u5f97\u5230\u5f53\u524d\u4e16\u754c\u7684\u4f30\u8ba1\u3002 Planner \u8fd9\u91cc\u7ed9\u51fa\u7684\u591a\u5c42\u7ea7Planner\u7ed3\u6784\u5982\u4e0a\u56fe\uff0c\u7b97\u6cd5\u5982\u4e0b \u5728\u6700\u5927\u7684\u5730\u56fe\u6700\u5c0f\u5b9e\u9645\u8ddd\u79bb\u5206\u8fa8\u7387\u7684costmap(\u6765\u81eamapper)\u4e0a\u5e26\u7740goal\u4fe1\u606f(\u5377\u79ef\u5408\u6210)\uff0c\u8fdb\u884cvalue-iteration(\u65b9\u6cd5\u540cQMDP\u4e2d\u63d0\u5230\u7684planner\u7684\u505a\u6cd5) \u5206\u5272\u51faValue Map\u4e2d\u5fc3\u90e8\u5206\uff0c\u4e0b\u91c7\u6837,\u4e0eGoal\u548cCostmap\u5377\u79ef\u5408\u6210\uff0c\u518d\u8fdb\u884cvalue-iteration \u91cd\u590d2\uff0c\u76f4\u5230\u5206\u8fa8\u7387\u8db3\u591f\u5c0f\u4e3a\u6b62\uff0c\u6700\u7ec8\u5168\u8fde\u63a5\u8f93\u5165\u884c\u52a8\u503c\u3002 \u663e\u7136\u8fd9\u4e2aplanner\u4e0eMapper\u90fd\u662f\u53ef\u5bfc\u7684\u8fd0\u7b97\u8fc7\u7a0b\uff0c\u53ef\u4ee5\u7aef\u5230\u7aef\u5b66\u4e60\u3002\u7f3a\u70b9\u5728\u4e8eValue-iteration\u7528\u4e8eplanner\u8fd9\u4e2a\u76ee\u524d\u53ea\u57282D\u5e73\u9762\u5bfc\u822a\u4e2d\u89c2\u5bdf\u8fc7\uff0c\u6709\u5f85\u8003\u8651\u3002","title":"Cognitive Mapping and Planning for Visual Navigation"},{"location":"Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/#cognitive-mapping-and-planning-for-visual-navigation","text":"\u8fd9\u7bc7\u8bba\u6587\u57282017\u5e74\u63d0\u51fa\u4e86\u76f4\u63a5\u5229\u7528\u89c6\u89c9\u8fdb\u884c\u5bfc\u822a\u7684\u601d\u8def \u603b\u4f53\u7ed3\u6784: \u8f93\u5165\u4e3aEgomotion, Goal, \u89c6\u89c9\u56fe\u50cf\u8f93\u5165, \u8f93\u51fa\u65f6action\uff0c\u4e2d\u95f4\u4f7f\u7528\u53ef\u5fae\u5206Mapper\u548c\u53ef\u5fae\u5206Planner","title":"Cognitive Mapping and Planning for Visual Navigation"},{"location":"Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/#mapper","text":"\u8fd9\u91cc\u7ed9\u51fa\u7684mapper\u7ed3\u6784\u5982\u4e0a\u56fe\uff0c\u5df2\u77e5\u4e0a\u4e00\u65f6\u523b\u5bf9\u73af\u5883\u5efa\u56fe\u7ed3\u679c\u4ee5\u53ca\u672c\u4f53\u8fd0\u52a8\uff0c\u901a\u8fc7\u53ef\u5fae\u5206\u7684\u56fe\u7247\u5e73\u79fb\u548c\u65cb\u8f6c\uff0c\u5f97\u5230\u5bf9\u73b0\u5728\u5730\u56fe\u7684\u4f30\u8ba1\uff0c\u4e0e\u65b0\u56fe\u7247\u7ecf\u8fc7encoder\u3001decoder\u4e4b\u540e\u7684\u8f93\u51fa\u8fdb\u884c\u5408\u5e76\uff0c\u5f97\u5230\u5f53\u524d\u4e16\u754c\u7684\u4f30\u8ba1\u3002","title":"Mapper"},{"location":"Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/#planner","text":"\u8fd9\u91cc\u7ed9\u51fa\u7684\u591a\u5c42\u7ea7Planner\u7ed3\u6784\u5982\u4e0a\u56fe\uff0c\u7b97\u6cd5\u5982\u4e0b \u5728\u6700\u5927\u7684\u5730\u56fe\u6700\u5c0f\u5b9e\u9645\u8ddd\u79bb\u5206\u8fa8\u7387\u7684costmap(\u6765\u81eamapper)\u4e0a\u5e26\u7740goal\u4fe1\u606f(\u5377\u79ef\u5408\u6210)\uff0c\u8fdb\u884cvalue-iteration(\u65b9\u6cd5\u540cQMDP\u4e2d\u63d0\u5230\u7684planner\u7684\u505a\u6cd5) \u5206\u5272\u51faValue Map\u4e2d\u5fc3\u90e8\u5206\uff0c\u4e0b\u91c7\u6837,\u4e0eGoal\u548cCostmap\u5377\u79ef\u5408\u6210\uff0c\u518d\u8fdb\u884cvalue-iteration \u91cd\u590d2\uff0c\u76f4\u5230\u5206\u8fa8\u7387\u8db3\u591f\u5c0f\u4e3a\u6b62\uff0c\u6700\u7ec8\u5168\u8fde\u63a5\u8f93\u5165\u884c\u52a8\u503c\u3002 \u663e\u7136\u8fd9\u4e2aplanner\u4e0eMapper\u90fd\u662f\u53ef\u5bfc\u7684\u8fd0\u7b97\u8fc7\u7a0b\uff0c\u53ef\u4ee5\u7aef\u5230\u7aef\u5b66\u4e60\u3002\u7f3a\u70b9\u5728\u4e8eValue-iteration\u7528\u4e8eplanner\u8fd9\u4e2a\u76ee\u524d\u53ea\u57282D\u5e73\u9762\u5bfc\u822a\u4e2d\u89c2\u5bdf\u8fc7\uff0c\u6709\u5f85\u8003\u8651\u3002","title":"Planner"},{"location":"Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/","text":"Composable Action-Conditioned Predictors: Flexible Off-Policy Learning for Robot Navigation \u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7075\u6d3b\u7684\u3001\u5c11\u76d1\u7763\u7684\u589e\u5f3a\u5b66\u4e60\u5bfc\u822a\u6846\u67b6\u3002\u6a21\u578b\u7684\u5b66\u4e60\u8fc7\u7a0b\u4f7f\u7528\u8f83\u4e3a\u6709\u9650\u7684\u76d1\u7763\uff0c\u540c\u4e00\u4e2a\u6a21\u578b\u53ef\u5728deploy\u65f6\u901a\u8fc7\u4fee\u6539reward\u51fd\u6570\u662f\u7684\u673a\u5668\u4eba\u8fbe\u5230\u6211\u4eec\u60f3\u8981\u7684\u5bfc\u822a\u6548\u679c CAPs\u6a21\u578b\u4ecb\u7ecd CAPs\u7ed3\u6784\u7684\u76ee\u7684\u662f\u901a\u8fc7\u8f93\u5165\u7684\u56fe\u7247\u3001\u72b6\u6001\u5e8f\u5217\uff0c\u8f93\u51fa\u5f53\u524d\u72b6\u6001\u4e0b\u51e0\u4e2a\u91cd\u8981\u7684encoded event\u7684\u76f8\u5173\u53c2\u6570\u3002\u672c\u8bba\u6587\u4f7f\u7528\u7684\u662fcollision, heading\uff0c or road lanes and doorways(\u95e8\u5728\u56fe\u7247\u4e2d\u7684\u6bd4\u4f8b)\u3002\u8fc7\u7a0b\u662foff-policy\u7684\uff0c\u6a21\u578b\u53d6\u51b3\u4e8e\u4e00\u4e2a\u957f\u5ea6\u4e3aN\u7684\u672a\u6765\u7684action list\uff0c\u6570\u636e\u91c7\u96c6\u8fc7\u7a0b\u91c7\u7528\u7684policy\u4e0e\u6700\u7ec8deploy\u6ca1\u6709\u5fc5\u7136\u8054\u7cfb\u3002 \u5728deploy\u7684\u65f6\u5019\uff0c\u7528\u6237\u57fa\u4e8eencoded event\u5b9a\u4e49\u65b0\u7684reward function\uff0c \u7cfb\u7edf\u901a\u8fc7MPC\u4f18\u5316\u5b9e\u73b0\u63a7\u5236\u3002 \u6df1\u5ea6\u5b66\u4e60\u7ed3\u6784 \u5377\u79ef\u63d0\u7279\u5f81->concat\u72b6\u6001\u5411\u91cf->\u5168\u8fde\u63a5-> multiplicative integration LSTM \u7684\u521d\u59cb\u72b6\u6001\u503c->\u8f93\u5165\u5e8f\u5217action->\u901a\u8fc7FC\u5206\u652f\u8f93\u51fa\u5404\u4e2aencoded event\u7684\u53c2\u6570 encoded events (event cues) \u8fd9\u91cc\u9009\u62e9\u7684collision\u4ee5\u53caheading\u4ee5\u53ca\u901f\u5ea6\u7b49\u53ef\u4ee5\u901a\u8fc7\u8f66\u8eab\u4f20\u611f\u5668\u6d4b\u5f97\u81ea\u52a8\u6807\u6ce8\uff0croad lanes\u548cdoorways\u8fd9\u91cc\u4f7f\u7528\u4e86\u4e00\u4e2apretrained FCN\u8fdb\u884c\u8bed\u4e49\u5206\u5272\u76f4\u63a5\u5f97\u5230\u7ed3\u679c \u4e09\u4e2a\u5b9e\u9a8c\u9879\u76ee \u4eff\u771f\u68ee\u6797\u8d70\u52a8 \u5956\u52b1\u51fd\u6570\u4e3a\uff1a R\\left(\\hat{E}_{t}^{(H, I)}\\right)=\\sum_{t^{\\prime}=t}^{t+H-1} 500 \\cdot\\left(1-\\hat{e}_{t^{\\prime}}^{(c o l l)}\\right)+\\left(\\cos \\left(\\hat{e}_{t^{\\prime}}^{(h e a d i n g)}-\\mathrm{GOAL}_{-} \\mathrm{HEADING}\\right)-1\\right) Carla\u4eff\u771f\u8fd0\u884c \u5956\u52b1\u51fd\u6570\u4e3a \\begin{aligned} R\\left(\\hat{E}_{t}^{(H, I)}\\right)=& \\sum_{t^{\\prime}=t}^{t+H-1} 50 \\cdot\\left(1-\\hat{e}_{t^{\\prime}}^{(c o l l)}\\right)-3 \\cdot \\frac{\\left|\\hat{e}_{t^{\\prime}}^{(s p e e d)}-\\operatorname{GOAL}_{-} \\operatorname{SPEB}\\right|}{\\operatorname{GOAL}_{-} \\operatorname{SPEED}}+5 \\cdot \\hat{e}_{t^{\\prime}}^{(l a n e-s e e n)}\\left(1-\\left|\\hat{e}_{t^{\\prime}}^{\\left(l a n e_{-} d i f f\\right)}\\right|\\right) \\\\ &-\\frac{5}{\\pi} \\cdot\\left|\\hat{e}_{t^{\\prime}}^{(h e a d i n g)}-\\operatorname{GOAL}_{-} \\operatorname{HEADING}\\right|-0.15 \\cdot\\left\\|\\mathbf{a}_{t^{\\prime}}^{(s t e e r)}\\right\\|_{2}^{2} \\end{aligned} \u9700\u8981\u9884\u6d4b\u7684\u5185\u5bb9\u5305\u62ec\uff1a\u78b0\u649e\u3001\u901f\u5ea6\u3001\u53ef\u89c1\u9053\u8def\u6bd4\u7387\u3001\u53ef\u89c1\u9053\u8def\u6bd4\u7387\u53d8\u5316\u7387\u3001\u671d\u5411 \u5b9e\u9645\u8f66\u8f86\u8fd0\u884c R\\left(\\hat{E}_{t}^{(H, I)}\\right)=\\sum_{t^{\\prime}=t}^{t+H-1}\\left(1-\\hat{e}_{t^{\\prime}}^{(c o l l)}\\right) \\cdot\\left[1-\\frac{0.1}{\\pi} \\cdot\\left|\\hat{e}_{t^{\\prime}}^{(h e a d i n g)}-\\operatorname{coth}_{-} \\operatorname{HEADING}\\right|+0.05 \\cdot \\hat{e}_{t^{\\prime}}^{(d o o r-f r a c)}\\right]-0.01 \\cdot\\left\\|\\mathbf{a}_{t^{\\prime}}\\right\\|_{2}^{2} \u9700\u8981\u9884\u6d4b\u7684\u5185\u5bb9\u5305\u62ec\uff1a\u78b0\u649e\u3001\u671d\u5411\u3001\u623f\u95f4\u95e8\u5360\u753b\u9762\u6bd4\u7387 \u5b83\u4eec\u4e3a\u4e86\u8bad\u7ec3Segmentation\u7f51\u7edc\u6807\u6ce8\u4e86\u8bad\u7ec3\u8fc7\u7a0b\u4e2d0.2%\u7684\u56fe\u50cf\u6570\u636e\u3002","title":"Composable Action-Conditioned Predictors: Flexible Off-Policy Learning for Robot Navigation"},{"location":"Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/#composable-action-conditioned-predictors-flexible-off-policy-learning-for-robot-navigation","text":"\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7075\u6d3b\u7684\u3001\u5c11\u76d1\u7763\u7684\u589e\u5f3a\u5b66\u4e60\u5bfc\u822a\u6846\u67b6\u3002\u6a21\u578b\u7684\u5b66\u4e60\u8fc7\u7a0b\u4f7f\u7528\u8f83\u4e3a\u6709\u9650\u7684\u76d1\u7763\uff0c\u540c\u4e00\u4e2a\u6a21\u578b\u53ef\u5728deploy\u65f6\u901a\u8fc7\u4fee\u6539reward\u51fd\u6570\u662f\u7684\u673a\u5668\u4eba\u8fbe\u5230\u6211\u4eec\u60f3\u8981\u7684\u5bfc\u822a\u6548\u679c","title":"Composable Action-Conditioned Predictors: Flexible Off-Policy Learning for Robot Navigation"},{"location":"Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/#caps","text":"CAPs\u7ed3\u6784\u7684\u76ee\u7684\u662f\u901a\u8fc7\u8f93\u5165\u7684\u56fe\u7247\u3001\u72b6\u6001\u5e8f\u5217\uff0c\u8f93\u51fa\u5f53\u524d\u72b6\u6001\u4e0b\u51e0\u4e2a\u91cd\u8981\u7684encoded event\u7684\u76f8\u5173\u53c2\u6570\u3002\u672c\u8bba\u6587\u4f7f\u7528\u7684\u662fcollision, heading\uff0c or road lanes and doorways(\u95e8\u5728\u56fe\u7247\u4e2d\u7684\u6bd4\u4f8b)\u3002\u8fc7\u7a0b\u662foff-policy\u7684\uff0c\u6a21\u578b\u53d6\u51b3\u4e8e\u4e00\u4e2a\u957f\u5ea6\u4e3aN\u7684\u672a\u6765\u7684action list\uff0c\u6570\u636e\u91c7\u96c6\u8fc7\u7a0b\u91c7\u7528\u7684policy\u4e0e\u6700\u7ec8deploy\u6ca1\u6709\u5fc5\u7136\u8054\u7cfb\u3002 \u5728deploy\u7684\u65f6\u5019\uff0c\u7528\u6237\u57fa\u4e8eencoded event\u5b9a\u4e49\u65b0\u7684reward function\uff0c \u7cfb\u7edf\u901a\u8fc7MPC\u4f18\u5316\u5b9e\u73b0\u63a7\u5236\u3002","title":"CAPs\u6a21\u578b\u4ecb\u7ecd"},{"location":"Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/#_1","text":"\u5377\u79ef\u63d0\u7279\u5f81->concat\u72b6\u6001\u5411\u91cf->\u5168\u8fde\u63a5-> multiplicative integration LSTM \u7684\u521d\u59cb\u72b6\u6001\u503c->\u8f93\u5165\u5e8f\u5217action->\u901a\u8fc7FC\u5206\u652f\u8f93\u51fa\u5404\u4e2aencoded event\u7684\u53c2\u6570","title":"\u6df1\u5ea6\u5b66\u4e60\u7ed3\u6784"},{"location":"Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/#encoded-events-event-cues","text":"\u8fd9\u91cc\u9009\u62e9\u7684collision\u4ee5\u53caheading\u4ee5\u53ca\u901f\u5ea6\u7b49\u53ef\u4ee5\u901a\u8fc7\u8f66\u8eab\u4f20\u611f\u5668\u6d4b\u5f97\u81ea\u52a8\u6807\u6ce8\uff0croad lanes\u548cdoorways\u8fd9\u91cc\u4f7f\u7528\u4e86\u4e00\u4e2apretrained FCN\u8fdb\u884c\u8bed\u4e49\u5206\u5272\u76f4\u63a5\u5f97\u5230\u7ed3\u679c","title":"encoded events (event cues)"},{"location":"Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/#_2","text":"","title":"\u4e09\u4e2a\u5b9e\u9a8c\u9879\u76ee"},{"location":"Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/#_3","text":"\u5956\u52b1\u51fd\u6570\u4e3a\uff1a R\\left(\\hat{E}_{t}^{(H, I)}\\right)=\\sum_{t^{\\prime}=t}^{t+H-1} 500 \\cdot\\left(1-\\hat{e}_{t^{\\prime}}^{(c o l l)}\\right)+\\left(\\cos \\left(\\hat{e}_{t^{\\prime}}^{(h e a d i n g)}-\\mathrm{GOAL}_{-} \\mathrm{HEADING}\\right)-1\\right)","title":"\u4eff\u771f\u68ee\u6797\u8d70\u52a8"},{"location":"Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/#carla","text":"\u5956\u52b1\u51fd\u6570\u4e3a \\begin{aligned} R\\left(\\hat{E}_{t}^{(H, I)}\\right)=& \\sum_{t^{\\prime}=t}^{t+H-1} 50 \\cdot\\left(1-\\hat{e}_{t^{\\prime}}^{(c o l l)}\\right)-3 \\cdot \\frac{\\left|\\hat{e}_{t^{\\prime}}^{(s p e e d)}-\\operatorname{GOAL}_{-} \\operatorname{SPEB}\\right|}{\\operatorname{GOAL}_{-} \\operatorname{SPEED}}+5 \\cdot \\hat{e}_{t^{\\prime}}^{(l a n e-s e e n)}\\left(1-\\left|\\hat{e}_{t^{\\prime}}^{\\left(l a n e_{-} d i f f\\right)}\\right|\\right) \\\\ &-\\frac{5}{\\pi} \\cdot\\left|\\hat{e}_{t^{\\prime}}^{(h e a d i n g)}-\\operatorname{GOAL}_{-} \\operatorname{HEADING}\\right|-0.15 \\cdot\\left\\|\\mathbf{a}_{t^{\\prime}}^{(s t e e r)}\\right\\|_{2}^{2} \\end{aligned} \u9700\u8981\u9884\u6d4b\u7684\u5185\u5bb9\u5305\u62ec\uff1a\u78b0\u649e\u3001\u901f\u5ea6\u3001\u53ef\u89c1\u9053\u8def\u6bd4\u7387\u3001\u53ef\u89c1\u9053\u8def\u6bd4\u7387\u53d8\u5316\u7387\u3001\u671d\u5411","title":"Carla\u4eff\u771f\u8fd0\u884c"},{"location":"Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/#_4","text":"R\\left(\\hat{E}_{t}^{(H, I)}\\right)=\\sum_{t^{\\prime}=t}^{t+H-1}\\left(1-\\hat{e}_{t^{\\prime}}^{(c o l l)}\\right) \\cdot\\left[1-\\frac{0.1}{\\pi} \\cdot\\left|\\hat{e}_{t^{\\prime}}^{(h e a d i n g)}-\\operatorname{coth}_{-} \\operatorname{HEADING}\\right|+0.05 \\cdot \\hat{e}_{t^{\\prime}}^{(d o o r-f r a c)}\\right]-0.01 \\cdot\\left\\|\\mathbf{a}_{t^{\\prime}}\\right\\|_{2}^{2} \u9700\u8981\u9884\u6d4b\u7684\u5185\u5bb9\u5305\u62ec\uff1a\u78b0\u649e\u3001\u671d\u5411\u3001\u623f\u95f4\u95e8\u5360\u753b\u9762\u6bd4\u7387 \u5b83\u4eec\u4e3a\u4e86\u8bad\u7ec3Segmentation\u7f51\u7edc\u6807\u6ce8\u4e86\u8bad\u7ec3\u8fc7\u7a0b\u4e2d0.2%\u7684\u56fe\u50cf\u6570\u636e\u3002","title":"\u5b9e\u9645\u8f66\u8f86\u8fd0\u884c"},{"location":"Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/","text":"DESPOT-\u03b1: Online POMDP Planning With Large State And Observation Spaces Under Construction \u5bf9\u8fd9\u7bc7\u6587\u7ae0\u7684\u7406\u89e3\u8fd8\u6ca1\u5b8c\u5168\uff0c\u8fd9\u7bc7\u6587\u7ae0\u6709\u4e00\u4e2a\u91cd\u8981\u7684 \u524d\u7f6e \u4e5f\u6ca1\u8bfb\u5b8c\u3002\u6570\u5b66\u96be\u5ea6\u8f83\u9ad8\u3002\u8fd9\u91cc\u5c1d\u8bd5\u540c\u65f6\u5199\u4e24\u4e2a\u6587\u7ae0\u7684review\u3002\u8fd9\u4e24\u7bc7\u6587\u7ae0\u89e3\u7b54\u7684\u90fd\u662f\u6c42\u89e3POMDP(partially observable markov decision process)\u7684\u95ee\u9898\uff0c\u57fa\u672c\u601d\u8def\u662f\u8499\u7279\u5361\u6d1b\u7b97\u6cd5(\u7136\u800c\u8fd8\u6709\u5927\u91cf\u7684\u6570\u5b66\u7406\u8bba\u4ee5\u53ca\u5206\u6790\uff0c\u76ee\u524d\u5e76\u672a\u5b66\u8d2f\u901a) \u524d\u7f6e\u8bba\u6587\u7b97\u6cd5:","title":"DESPOT-\u03b1: Online POMDP Planning With Large State And Observation Spaces"},{"location":"Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/#despot-online-pomdp-planning-with-large-state-and-observation-spaces","text":"","title":"DESPOT-\u03b1: Online POMDP Planning With Large State And Observation Spaces"},{"location":"Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/#under-construction","text":"\u5bf9\u8fd9\u7bc7\u6587\u7ae0\u7684\u7406\u89e3\u8fd8\u6ca1\u5b8c\u5168\uff0c\u8fd9\u7bc7\u6587\u7ae0\u6709\u4e00\u4e2a\u91cd\u8981\u7684 \u524d\u7f6e \u4e5f\u6ca1\u8bfb\u5b8c\u3002\u6570\u5b66\u96be\u5ea6\u8f83\u9ad8\u3002\u8fd9\u91cc\u5c1d\u8bd5\u540c\u65f6\u5199\u4e24\u4e2a\u6587\u7ae0\u7684review\u3002\u8fd9\u4e24\u7bc7\u6587\u7ae0\u89e3\u7b54\u7684\u90fd\u662f\u6c42\u89e3POMDP(partially observable markov decision process)\u7684\u95ee\u9898\uff0c\u57fa\u672c\u601d\u8def\u662f\u8499\u7279\u5361\u6d1b\u7b97\u6cd5(\u7136\u800c\u8fd8\u6709\u5927\u91cf\u7684\u6570\u5b66\u7406\u8bba\u4ee5\u53ca\u5206\u6790\uff0c\u76ee\u524d\u5e76\u672a\u5b66\u8d2f\u901a) \u524d\u7f6e\u8bba\u6587\u7b97\u6cd5:","title":"Under Construction"},{"location":"Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/","text":"Differentiable Algorithm Networks for Composable Robot Learning \u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u53ef\u5fae\u5206\u7b97\u6cd5\u7f51\u7edc\uff0c\u5176\u5b9e\u4e5f\u5c31\u662f\u5206\u6a21\u5757\u5730\u5b9e\u73b0\u53ef\u5fae\u5206\u7aef\u5230\u7aef\u8bad\u7ec3\u3002 \u57fa\u672c\u601d\u8def\u548cQMDP-Net\u4e00\u81f4\u3002","title":"Differentiable Algorithm Networks for Composable Robot Learning"},{"location":"Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/#differentiable-algorithm-networks-for-composable-robot-learning","text":"\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u53ef\u5fae\u5206\u7b97\u6cd5\u7f51\u7edc\uff0c\u5176\u5b9e\u4e5f\u5c31\u662f\u5206\u6a21\u5757\u5730\u5b9e\u73b0\u53ef\u5fae\u5206\u7aef\u5230\u7aef\u8bad\u7ec3\u3002 \u57fa\u672c\u601d\u8def\u548cQMDP-Net\u4e00\u81f4\u3002","title":"Differentiable Algorithm Networks for Composable Robot Learning"},{"location":"Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/","text":"Differentiable MPC for End-to-end Planning and Control \u8fd9\u7bc7\u8bba\u6587\u5ef6\u7eed\u4e86 OptNet \u7684\u601d\u8def\uff0c\u4e5f\u5c31\u662f\u901a\u8fc7\u6c42\u89e3\u53ef\u5b66\u4e60\u7684\u4e8c\u6b21\u6700\u4f18\u6765\u5f97\u5230\u6700\u4f18\u63a7\u5236\u7684\u6548\u679c","title":"Differentiable MPC for End-to-end Planning and Control"},{"location":"Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/#differentiable-mpc-for-end-to-end-planning-and-control","text":"\u8fd9\u7bc7\u8bba\u6587\u5ef6\u7eed\u4e86 OptNet \u7684\u601d\u8def\uff0c\u4e5f\u5c31\u662f\u901a\u8fc7\u6c42\u89e3\u53ef\u5b66\u4e60\u7684\u4e8c\u6b21\u6700\u4f18\u6765\u5f97\u5230\u6700\u4f18\u63a7\u5236\u7684\u6548\u679c","title":"Differentiable MPC for End-to-end Planning and Control"},{"location":"Planning_Control_DL/Diffusion_planner/","text":"Diffusion-Based Planning for Autonomous Driving with Flexible Guidance Project Page \u4f7f\u7528Diffusion model \u751f\u6210\u65e0\u4eba\u8f66Planner\u8f68\u8ff9 \u4e8c\u6bb5\u5f0fEnd to End\u5f62\u6001\uff0c\u8f93\u5165\u7684\u4e0d\u662f\u4f20\u611f\u5668\u6570\u636e\uff0c\u800c\u662f\u7a20\u5bc6\u6570\u636e Diffusion Model Fundamental \u524d\u5411\u7684\u52a0\u566a\u8fc7\u7a0b\uff1a q_{t 0}\\left(\\boldsymbol{x}^{(t)} \\mid \\boldsymbol{x}^{(0)}\\right)=\\mathcal{N}\\left(\\boldsymbol{x}^{(t)} \\mid \\alpha_t \\boldsymbol{x}^{(0)}, \\sigma_t^2 \\mathbf{I}\\right), t \\in[0,1], \u53cd\u5411\u8fc7\u7a0b\u53ef\u4ee5\u7528\u4e00\u4e0b\u7684diffusion ODE\u89e3 \\text { (Diffusion ODE) } \\mathrm{d} \\boldsymbol{x}^{(t)}=\\left[f(t) \\boldsymbol{x}^{(t)}-\\frac{1}{2} g^2(t) \\nabla_{\\boldsymbol{x}^{(t)}} \\log q_t\\left(\\boldsymbol{x}^{(t)}\\right)\\right] \\mathrm{d} t, \u5176\u4e2d\u7684 f(t), g(t) \u4e0enoise\u7684\u89c4\u5212 \\alpha_t, \\sigma_t \u76f8\u5173\u3002 f(t)=\\frac{\\mathrm{d} \\log \\alpha_t}{\\mathrm{~d} t}, g^2(t)=\\frac{\\mathrm{d} \\sigma_t^2}{\\mathrm{~d} t}-2 \\frac{\\mathrm{~d} \\log \\alpha_t}{\\mathrm{~d} t} \\sigma_t^2 Diffusion\u5c31\u662f\u7528score\u7f51\u7edc s_\\theta(x^{(t)}, t) \u6765\u62df\u5408 \\nabla_{x^{(t)}} \\log q_t(x(t)) Classifier Guidance \u4f7f\u7528\u4e00\u4e2aclassifier \\mathcal{E}_\\phi(x^{(t)}, t) , \u4f7f\u7528\u5206\u7c7bscore\u7684\u68af\u5ea6\u53bb\u4fee\u6539Diffusion score \\tilde{\\boldsymbol{s}}_\\theta\\left(\\boldsymbol{x}^{(t)}, t\\right)=\\boldsymbol{s}_\\theta\\left(\\boldsymbol{x}^{(t)}, t\\right)-\\nabla_{\\boldsymbol{x}^{(t)}} \\mathcal{E}_\\phi\\left(\\boldsymbol{x}^{(t)}, t\\right) Diffusion Planner Methodology Tasks Statement \u4f5c\u8005\u628a\u4efb\u52a1\u58f0\u660e\u4e3aFuture Generation Task. \u8f93\u5165\u6761\u4ef6 C , \u5305\u542b\u5f53\u524d\u8f66\u8f86\u72b6\u6001(current vehicle state), \u5386\u53f2\u6570\u636e(historical data), \u8f66\u9053\u7ebf\u4fe1\u606f(lane information), \u5bfc\u822a\u4fe1\u606f(navigation information), \u8f93\u51fa\u6240\u6709\u5173\u952e\u53c2\u4e0e\u8005\u7684\u672a\u6765\u8f68\u8ff9\uff0c\u4ece\u800c\u5141\u8bb8\u5bf9\u5408\u4f5c\u8005\u7684\u884c\u4e3a\u8fdb\u884c\u5efa\u6a21, \\boldsymbol{x}^{(0)}=\\left[\\begin{array}{c} x_{\\text {ego }}^{(0)} \\\\ x_{\\text {neighbor }_1}^{(0)} \\\\ \\vdots \\\\ x_{\\text {neighbor }_M}^{(0)} \\end{array}\\right]=\\left[\\begin{array}{cccc} x_{\\text {ego }^1}^1 & x_{\\text {ego }_2^2} & \\ldots & x_{\\text {ego }^\\tau} \\\\ x_{\\text {neighbor }_1}^1 & x_{\\text {neighbor }_1}^2 & \\ldots & x_{\\text {neighbor }_1}^\\tau \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ x_{\\text {neighbor }_M}^1 & x_{\\text {neighbor }_M}^2 & \\ldots & x_{\\text {neighbor }_M}^\\tau \\end{array}\\right] \u672c\u6587\u4f7f\u7528\u5e26\u6709\u62ec\u53f7\u7684\u4e0a\u6807\u8868\u8fbediffusion\u7684\u65f6\u95f4\u6b65\uff0c\u666e\u901a\u7684\u4e0a\u6807\u8868\u8fbe\u8f68\u8ff9\u7684\u65f6\u95f4\u6b65. \u9009\u62e9\u9644\u8fd1 M \u4e2a\u6700\u8fd1\u7684\u8f66\u8f86\uff0c\u5e76\u9884\u6d4b\u4ed6\u4eec\u53ef\u80fd\u7684\u672a\u6765\u8f68\u8ff9\uff0cDiffusion Planner\u7684\u8bad\u7ec3\u76ee\u6807\u4e3a \\mathcal{L}_\\theta=\\mathbb{E}_{\\boldsymbol{x}^{(0)}, t \\sim \\mathbb{U}(0,1), \\boldsymbol{x}^{(t)} \\sim q_{t 0}\\left(\\boldsymbol{x}^{(t)} \\mid \\boldsymbol{x}^{(0)}\\right)}\\left[\\left\\|\\mu_\\theta\\left(\\boldsymbol{x}^{(t)}, t, \\boldsymbol{C}\\right)-\\boldsymbol{x}^{(0)}\\right\\|^2\\right], Diffusion Score\u4e3a s_\\theta = (\\alpha_t \\mu_\\theta - x^{(t)}) / \\sigma_t^2 \u7528\u4f5cdenoising. Network \u57fa\u4e8eDiT\u67b6\u6784\uff0c \u5728\u7b2c\u4e00\u4e2astep, \u57fa\u7840\u8f93\u51fa\u4e3a\u6240\u6709\u8f66\u7684\u5f53\u524d\u8f68\u8ff9\u4f4d\u7f6e(\u4e0d\u5305\u542b\u901f\u5ea6\u4e0e\u52a0\u901f\u5ea6\u4ee5\u63d0\u5347\u95ed\u73af\u6027\u80fd)\u3002 \u4f7f\u7528 MLP-Mixer \u63d0\u53d6\u5e76\u878d\u5408\u591a\u79cd\u7c7b\u522b\uff08\u8f66\u8f86\u7684\u4f4d\u7f6e\u65b9\u5411\uff0c\u901f\u5ea6\uff0c\u5927\u5c0f\uff0c\u79cd\u7c7b\uff1b \u8f66\u9053\u7ebf\u7684\u5750\u6807\u4ea4\u901a\u706f\u901f\u5ea6\u9650\u5236\uff09\u7684\u4fe1\u606f\u3002","title":"Diffusion-Based Planning for Autonomous Driving with Flexible Guidance"},{"location":"Planning_Control_DL/Diffusion_planner/#diffusion-based-planning-for-autonomous-driving-with-flexible-guidance","text":"Project Page \u4f7f\u7528Diffusion model \u751f\u6210\u65e0\u4eba\u8f66Planner\u8f68\u8ff9 \u4e8c\u6bb5\u5f0fEnd to End\u5f62\u6001\uff0c\u8f93\u5165\u7684\u4e0d\u662f\u4f20\u611f\u5668\u6570\u636e\uff0c\u800c\u662f\u7a20\u5bc6\u6570\u636e","title":"Diffusion-Based Planning for Autonomous Driving with Flexible Guidance"},{"location":"Planning_Control_DL/Diffusion_planner/#diffusion-model-fundamental","text":"\u524d\u5411\u7684\u52a0\u566a\u8fc7\u7a0b\uff1a q_{t 0}\\left(\\boldsymbol{x}^{(t)} \\mid \\boldsymbol{x}^{(0)}\\right)=\\mathcal{N}\\left(\\boldsymbol{x}^{(t)} \\mid \\alpha_t \\boldsymbol{x}^{(0)}, \\sigma_t^2 \\mathbf{I}\\right), t \\in[0,1], \u53cd\u5411\u8fc7\u7a0b\u53ef\u4ee5\u7528\u4e00\u4e0b\u7684diffusion ODE\u89e3 \\text { (Diffusion ODE) } \\mathrm{d} \\boldsymbol{x}^{(t)}=\\left[f(t) \\boldsymbol{x}^{(t)}-\\frac{1}{2} g^2(t) \\nabla_{\\boldsymbol{x}^{(t)}} \\log q_t\\left(\\boldsymbol{x}^{(t)}\\right)\\right] \\mathrm{d} t, \u5176\u4e2d\u7684 f(t), g(t) \u4e0enoise\u7684\u89c4\u5212 \\alpha_t, \\sigma_t \u76f8\u5173\u3002 f(t)=\\frac{\\mathrm{d} \\log \\alpha_t}{\\mathrm{~d} t}, g^2(t)=\\frac{\\mathrm{d} \\sigma_t^2}{\\mathrm{~d} t}-2 \\frac{\\mathrm{~d} \\log \\alpha_t}{\\mathrm{~d} t} \\sigma_t^2 Diffusion\u5c31\u662f\u7528score\u7f51\u7edc s_\\theta(x^{(t)}, t) \u6765\u62df\u5408 \\nabla_{x^{(t)}} \\log q_t(x(t))","title":"Diffusion Model Fundamental"},{"location":"Planning_Control_DL/Diffusion_planner/#classifier-guidance","text":"\u4f7f\u7528\u4e00\u4e2aclassifier \\mathcal{E}_\\phi(x^{(t)}, t) , \u4f7f\u7528\u5206\u7c7bscore\u7684\u68af\u5ea6\u53bb\u4fee\u6539Diffusion score \\tilde{\\boldsymbol{s}}_\\theta\\left(\\boldsymbol{x}^{(t)}, t\\right)=\\boldsymbol{s}_\\theta\\left(\\boldsymbol{x}^{(t)}, t\\right)-\\nabla_{\\boldsymbol{x}^{(t)}} \\mathcal{E}_\\phi\\left(\\boldsymbol{x}^{(t)}, t\\right)","title":"Classifier Guidance"},{"location":"Planning_Control_DL/Diffusion_planner/#diffusion-planner-methodology","text":"","title":"Diffusion Planner Methodology"},{"location":"Planning_Control_DL/Diffusion_planner/#tasks-statement","text":"\u4f5c\u8005\u628a\u4efb\u52a1\u58f0\u660e\u4e3aFuture Generation Task. \u8f93\u5165\u6761\u4ef6 C , \u5305\u542b\u5f53\u524d\u8f66\u8f86\u72b6\u6001(current vehicle state), \u5386\u53f2\u6570\u636e(historical data), \u8f66\u9053\u7ebf\u4fe1\u606f(lane information), \u5bfc\u822a\u4fe1\u606f(navigation information), \u8f93\u51fa\u6240\u6709\u5173\u952e\u53c2\u4e0e\u8005\u7684\u672a\u6765\u8f68\u8ff9\uff0c\u4ece\u800c\u5141\u8bb8\u5bf9\u5408\u4f5c\u8005\u7684\u884c\u4e3a\u8fdb\u884c\u5efa\u6a21, \\boldsymbol{x}^{(0)}=\\left[\\begin{array}{c} x_{\\text {ego }}^{(0)} \\\\ x_{\\text {neighbor }_1}^{(0)} \\\\ \\vdots \\\\ x_{\\text {neighbor }_M}^{(0)} \\end{array}\\right]=\\left[\\begin{array}{cccc} x_{\\text {ego }^1}^1 & x_{\\text {ego }_2^2} & \\ldots & x_{\\text {ego }^\\tau} \\\\ x_{\\text {neighbor }_1}^1 & x_{\\text {neighbor }_1}^2 & \\ldots & x_{\\text {neighbor }_1}^\\tau \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ x_{\\text {neighbor }_M}^1 & x_{\\text {neighbor }_M}^2 & \\ldots & x_{\\text {neighbor }_M}^\\tau \\end{array}\\right] \u672c\u6587\u4f7f\u7528\u5e26\u6709\u62ec\u53f7\u7684\u4e0a\u6807\u8868\u8fbediffusion\u7684\u65f6\u95f4\u6b65\uff0c\u666e\u901a\u7684\u4e0a\u6807\u8868\u8fbe\u8f68\u8ff9\u7684\u65f6\u95f4\u6b65. \u9009\u62e9\u9644\u8fd1 M \u4e2a\u6700\u8fd1\u7684\u8f66\u8f86\uff0c\u5e76\u9884\u6d4b\u4ed6\u4eec\u53ef\u80fd\u7684\u672a\u6765\u8f68\u8ff9\uff0cDiffusion Planner\u7684\u8bad\u7ec3\u76ee\u6807\u4e3a \\mathcal{L}_\\theta=\\mathbb{E}_{\\boldsymbol{x}^{(0)}, t \\sim \\mathbb{U}(0,1), \\boldsymbol{x}^{(t)} \\sim q_{t 0}\\left(\\boldsymbol{x}^{(t)} \\mid \\boldsymbol{x}^{(0)}\\right)}\\left[\\left\\|\\mu_\\theta\\left(\\boldsymbol{x}^{(t)}, t, \\boldsymbol{C}\\right)-\\boldsymbol{x}^{(0)}\\right\\|^2\\right], Diffusion Score\u4e3a s_\\theta = (\\alpha_t \\mu_\\theta - x^{(t)}) / \\sigma_t^2 \u7528\u4f5cdenoising.","title":"Tasks Statement"},{"location":"Planning_Control_DL/Diffusion_planner/#network","text":"\u57fa\u4e8eDiT\u67b6\u6784\uff0c \u5728\u7b2c\u4e00\u4e2astep, \u57fa\u7840\u8f93\u51fa\u4e3a\u6240\u6709\u8f66\u7684\u5f53\u524d\u8f68\u8ff9\u4f4d\u7f6e(\u4e0d\u5305\u542b\u901f\u5ea6\u4e0e\u52a0\u901f\u5ea6\u4ee5\u63d0\u5347\u95ed\u73af\u6027\u80fd)\u3002 \u4f7f\u7528 MLP-Mixer \u63d0\u53d6\u5e76\u878d\u5408\u591a\u79cd\u7c7b\u522b\uff08\u8f66\u8f86\u7684\u4f4d\u7f6e\u65b9\u5411\uff0c\u901f\u5ea6\uff0c\u5927\u5c0f\uff0c\u79cd\u7c7b\uff1b \u8f66\u9053\u7ebf\u7684\u5750\u6807\u4ea4\u901a\u706f\u901f\u5ea6\u9650\u5236\uff09\u7684\u4fe1\u606f\u3002","title":"Network"},{"location":"Planning_Control_DL/EUDM/","text":"Efficient Uncertainty-aware Decision-making for Automated Driving Using Guided Branching \u8fd9\u7bc7paper\u662f\u6c88\u6559\u6388\u7ec4 Wenchao Ding \u5728\u8f66\u8f86\u610f\u56fe\u9884\u6d4b\uff0c\u51b3\u7b56\u4ee5\u53ca\u8def\u5f84\u89c4\u5212\u7684\u51e0\u4e2a\u5de5\u4f5c\u4e4b\u4e00\u3002\u8be5\u4f5c\u8005\u5728\u5b9e\u8f66\u4e0a\u5b9e\u73b0\u4e86\u6027\u80fd\u8f83\u597d\u7684demo\u3002 \u8fd9\u7bc7paper\u57fa\u672c\u6ca1\u6709\u4f7f\u7528\u5b66\u4e60\u7b97\u6cd5\uff0c\u91cd\u70b9\u662f\u57fa\u4e8e\u7ecf\u9a8c\u7684\u62bd\u8c61\u4e0e\u526a\u679d\uff0c\u89e3\u91ca\u6027\u4ee5\u53ca\u6269\u5c55\u6027\u90fd\u5f88\u5f3a\u3002 \u8bbe\u8ba1\u6846\u67b6 \u84dd\u8272\u6846\u90e8\u5206\u4e3b\u8981\u662f\u4e00\u4e2a\u90e8\u5206\u53ef\u89c2\u6d4b\u7684\u9a6c\u5c14\u79d1\u592b\u6a21\u578b(POMDP), \u6838\u5fc3\u5355\u5143\u6709\u4e09\uff0c\u7b2c\u4e00\u4e2a\u8868\u5f81\u662f\u672c\u4f53\u51b3\u7b56\u5e8f\u5217\u7684DCP-Tree, \u7b2c\u4e8c\u4e2a\u662f\u5bf9\u91cd\u70b9\u8f66\u8f86\u7684\u884c\u4e3a\u8fdb\u884c\u7a77\u4e3e\u7684CFB\uff0c \u7b2c\u4e09\u4e2a\u662f\u6bcf\u4e2a\u88ab\u7a77\u4e3e\u573a\u666f\u7684\u4eff\u771f Domain-specific Closed-loop Policy Tree (DCP-Tree) \u4f5c\u8005\u8bbe\u8ba1\u4e86\u4e00\u4e2a8s\u7684planning horizon,\u6bcf2s\u4e3a\u4e00\u4e2apolicy\u66f4\u65b0\u7684\u8282\u70b9\uff0c\u6240\u4ee5DCP\u6811\u7684\u6df1\u5ea6\u4e3a4\u3002\u4f5c\u8005\u8fdb\u4e00\u6b65\u8003\u8651\u4e86\u4e00\u4e2a\u526a\u679d\u65b9\u6cd5\uff0c\u4f5c\u8005\u8ba4\u4e3a\u6bcf\u4e2a\u89c4\u5212\u5468\u671f\u4e2d\uff0c\u53ea\u4f1a\u53d1\u751f\u4e00\u6b21\u7b56\u7565\u66f4\u65b0\uff0c\u5982\u679c\u9700\u8981\u590d\u6742\u7684\u89c4\u5219\u53d8\u5316(lane-keep -> lane-change -> lane-keep),\u5219\u53ef\u4ee5\u901a\u8fc7\u9ad8\u9891\u7387\u7684\u91cd\u65b0\u89c4\u5212\u5b9e\u73b0\u3002 Conditional Focused Branching (CFB) \u4f5c\u8005\u7684\u8bbe\u8ba1\u662f\u7a77\u4e3e\u76f8\u5173\u8f66\u8f86\u7684\u6240\u6709intention\uff0c \u4f46\u662f\u8fd9\u4e2a\"\u76f8\u5173\"\u8f66\u8f86\u662f\u7531\u672c\u8f66\u7684\u51b3\u7b56\u51b3\u5b9a\u7684\u3002\u6bd4\u5982\u5de6\u8f6c\u7684\u65f6\u5019\u4f1a\u4ec5\u8003\u8651\u672c\u7ebf\u4ee5\u53ca\u5de6\u4fa7\u7684\u8f66\u8f86\uff0c\u9650\u5236\u4e86\u641c\u7d22\u7684agent\u6570\u91cf\u3002 \u5185\u90e8\u4eff\u771f \u4f5c\u8005\u5728\u6cd5\u5411\u65b9\u5411\u5bf9\u5176\u4ed6\u8f66\u8f86\u7684\u63a7\u5236\u7b97\u6cd5\u662fpure pursuit controller, \u6cbf\u7ebf\u65b9\u5411\u7684\u63a7\u5236\u903b\u8f91\u4e3a IDM","title":"Efficient Uncertainty-aware Decision-making for Automated Driving Using Guided Branching"},{"location":"Planning_Control_DL/EUDM/#efficient-uncertainty-aware-decision-making-for-automated-driving-using-guided-branching","text":"\u8fd9\u7bc7paper\u662f\u6c88\u6559\u6388\u7ec4 Wenchao Ding \u5728\u8f66\u8f86\u610f\u56fe\u9884\u6d4b\uff0c\u51b3\u7b56\u4ee5\u53ca\u8def\u5f84\u89c4\u5212\u7684\u51e0\u4e2a\u5de5\u4f5c\u4e4b\u4e00\u3002\u8be5\u4f5c\u8005\u5728\u5b9e\u8f66\u4e0a\u5b9e\u73b0\u4e86\u6027\u80fd\u8f83\u597d\u7684demo\u3002 \u8fd9\u7bc7paper\u57fa\u672c\u6ca1\u6709\u4f7f\u7528\u5b66\u4e60\u7b97\u6cd5\uff0c\u91cd\u70b9\u662f\u57fa\u4e8e\u7ecf\u9a8c\u7684\u62bd\u8c61\u4e0e\u526a\u679d\uff0c\u89e3\u91ca\u6027\u4ee5\u53ca\u6269\u5c55\u6027\u90fd\u5f88\u5f3a\u3002","title":"Efficient Uncertainty-aware Decision-making for Automated Driving Using Guided Branching"},{"location":"Planning_Control_DL/EUDM/#_1","text":"\u84dd\u8272\u6846\u90e8\u5206\u4e3b\u8981\u662f\u4e00\u4e2a\u90e8\u5206\u53ef\u89c2\u6d4b\u7684\u9a6c\u5c14\u79d1\u592b\u6a21\u578b(POMDP), \u6838\u5fc3\u5355\u5143\u6709\u4e09\uff0c\u7b2c\u4e00\u4e2a\u8868\u5f81\u662f\u672c\u4f53\u51b3\u7b56\u5e8f\u5217\u7684DCP-Tree, \u7b2c\u4e8c\u4e2a\u662f\u5bf9\u91cd\u70b9\u8f66\u8f86\u7684\u884c\u4e3a\u8fdb\u884c\u7a77\u4e3e\u7684CFB\uff0c \u7b2c\u4e09\u4e2a\u662f\u6bcf\u4e2a\u88ab\u7a77\u4e3e\u573a\u666f\u7684\u4eff\u771f Domain-specific Closed-loop Policy Tree (DCP-Tree) \u4f5c\u8005\u8bbe\u8ba1\u4e86\u4e00\u4e2a8s\u7684planning horizon,\u6bcf2s\u4e3a\u4e00\u4e2apolicy\u66f4\u65b0\u7684\u8282\u70b9\uff0c\u6240\u4ee5DCP\u6811\u7684\u6df1\u5ea6\u4e3a4\u3002\u4f5c\u8005\u8fdb\u4e00\u6b65\u8003\u8651\u4e86\u4e00\u4e2a\u526a\u679d\u65b9\u6cd5\uff0c\u4f5c\u8005\u8ba4\u4e3a\u6bcf\u4e2a\u89c4\u5212\u5468\u671f\u4e2d\uff0c\u53ea\u4f1a\u53d1\u751f\u4e00\u6b21\u7b56\u7565\u66f4\u65b0\uff0c\u5982\u679c\u9700\u8981\u590d\u6742\u7684\u89c4\u5219\u53d8\u5316(lane-keep -> lane-change -> lane-keep),\u5219\u53ef\u4ee5\u901a\u8fc7\u9ad8\u9891\u7387\u7684\u91cd\u65b0\u89c4\u5212\u5b9e\u73b0\u3002 Conditional Focused Branching (CFB) \u4f5c\u8005\u7684\u8bbe\u8ba1\u662f\u7a77\u4e3e\u76f8\u5173\u8f66\u8f86\u7684\u6240\u6709intention\uff0c \u4f46\u662f\u8fd9\u4e2a\"\u76f8\u5173\"\u8f66\u8f86\u662f\u7531\u672c\u8f66\u7684\u51b3\u7b56\u51b3\u5b9a\u7684\u3002\u6bd4\u5982\u5de6\u8f6c\u7684\u65f6\u5019\u4f1a\u4ec5\u8003\u8651\u672c\u7ebf\u4ee5\u53ca\u5de6\u4fa7\u7684\u8f66\u8f86\uff0c\u9650\u5236\u4e86\u641c\u7d22\u7684agent\u6570\u91cf\u3002 \u5185\u90e8\u4eff\u771f \u4f5c\u8005\u5728\u6cd5\u5411\u65b9\u5411\u5bf9\u5176\u4ed6\u8f66\u8f86\u7684\u63a7\u5236\u7b97\u6cd5\u662fpure pursuit controller, \u6cbf\u7ebf\u65b9\u5411\u7684\u63a7\u5236\u903b\u8f91\u4e3a IDM","title":"\u8bbe\u8ba1\u6846\u67b6"},{"location":"Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/","text":"Hierarchical Imitation and Reinforcement Learning \u672c\u6587 \u5b98\u65b9\u4e3b\u9875 \u8fd9\u7bc7\u6587\u7ae0\u7684\u4e3b\u8981\u601d\u8def\u662f\u5c06\u5f3a\u5316\u5b66\u4e60\u5c42\u7ea7\u5316\u3002\u5229\u7528\u4e13\u5bb6\u7ecf\u9a8c\uff0c\u5c06\u4efb\u52a1\u5206\u89e3\u4e3a\u591a\u4e2a\u5c0f\u4efb\u52a1\uff0c\u7f51\u7edc\u4f1a\u540c\u65f6\u5c1d\u8bd5\u6a21\u4eff\u4e13\u5bb6\u5728\u5927\u3001\u5c0f\u4efb\u52a1\u4e0a\u7684\u7ecf\u9a8c\uff0c\u4e5f\u4f1a\u7528\u5f3a\u5316\u5b66\u4e60\u589e\u5f3a\u76f8\u5173\u7684\u7ecf\u9a8c\u3002 Formalization \u5bf9\u4e8e\u4e00\u4e2a\u4efb\u52a1\uff0c\u672c\u6587\u4f1a\u5c06\u5b83\u5206\u4e3a H_{HI} \u4e2a\u5e8f\u5217\u7684\u5b50\u4efb\u52a1\uff0c\u4ee5\u4e0a\u56fe\u7684\u8ff7\u5bab\u4e3e\u4f8b\uff0c\u91cc\u9762\u6bcf\u4e00\u4e2a 3x3 \u7684\u65b9\u683c\u5757\u5c31\u53ef\u4ee5\u7406\u89e3\u4e3a\u4e00\u4e2a\u5b50\u4efb\u52a1\u3002\u800c\u5728 3x3 \u65b9\u683c\u5757\u5185\u4e00\u683c\u4e00\u683c\u5730\u8fd0\u52a8\u5219\u662f\u7406\u89e3\u4e3a\u5b50\u4efb\u52a1\u7684\u6267\u884c\u3002 \u56e0\u4e3a\u4efb\u52a1\u5c31\u53d8\u4e3a\u4e86\u4e24\u79cd \u9ad8\u7ea7\u76ee\u6807\u7684\u8f6c\u79fb\u8f68\u8ff9 \\sigma^* = (s_1^*, g_1^*, \\tau_1^* ....) \u5e95\u5c42\u4efb\u52a1\u7684\u8f6c\u79fb\u8f68\u8ff9 \\tau^* = (s_1^*, a_1^*, ....) \u8499\u7279\u7956\u739b\u590d\u4ec7\u7684\u4efb\u52a1\u5206\u89e3:","title":"Hierarchical Imitation and Reinforcement Learning"},{"location":"Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/#hierarchical-imitation-and-reinforcement-learning","text":"\u672c\u6587 \u5b98\u65b9\u4e3b\u9875 \u8fd9\u7bc7\u6587\u7ae0\u7684\u4e3b\u8981\u601d\u8def\u662f\u5c06\u5f3a\u5316\u5b66\u4e60\u5c42\u7ea7\u5316\u3002\u5229\u7528\u4e13\u5bb6\u7ecf\u9a8c\uff0c\u5c06\u4efb\u52a1\u5206\u89e3\u4e3a\u591a\u4e2a\u5c0f\u4efb\u52a1\uff0c\u7f51\u7edc\u4f1a\u540c\u65f6\u5c1d\u8bd5\u6a21\u4eff\u4e13\u5bb6\u5728\u5927\u3001\u5c0f\u4efb\u52a1\u4e0a\u7684\u7ecf\u9a8c\uff0c\u4e5f\u4f1a\u7528\u5f3a\u5316\u5b66\u4e60\u589e\u5f3a\u76f8\u5173\u7684\u7ecf\u9a8c\u3002","title":"Hierarchical Imitation and Reinforcement Learning"},{"location":"Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/#formalization","text":"\u5bf9\u4e8e\u4e00\u4e2a\u4efb\u52a1\uff0c\u672c\u6587\u4f1a\u5c06\u5b83\u5206\u4e3a H_{HI} \u4e2a\u5e8f\u5217\u7684\u5b50\u4efb\u52a1\uff0c\u4ee5\u4e0a\u56fe\u7684\u8ff7\u5bab\u4e3e\u4f8b\uff0c\u91cc\u9762\u6bcf\u4e00\u4e2a 3x3 \u7684\u65b9\u683c\u5757\u5c31\u53ef\u4ee5\u7406\u89e3\u4e3a\u4e00\u4e2a\u5b50\u4efb\u52a1\u3002\u800c\u5728 3x3 \u65b9\u683c\u5757\u5185\u4e00\u683c\u4e00\u683c\u5730\u8fd0\u52a8\u5219\u662f\u7406\u89e3\u4e3a\u5b50\u4efb\u52a1\u7684\u6267\u884c\u3002 \u56e0\u4e3a\u4efb\u52a1\u5c31\u53d8\u4e3a\u4e86\u4e24\u79cd \u9ad8\u7ea7\u76ee\u6807\u7684\u8f6c\u79fb\u8f68\u8ff9 \\sigma^* = (s_1^*, g_1^*, \\tau_1^* ....) \u5e95\u5c42\u4efb\u52a1\u7684\u8f6c\u79fb\u8f68\u8ff9 \\tau^* = (s_1^*, a_1^*, ....) \u8499\u7279\u7956\u739b\u590d\u4ec7\u7684\u4efb\u52a1\u5206\u89e3:","title":"Formalization"},{"location":"Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/","text":"Intention-Net: Integrating Planning and DeepLearning for Goal-Directed Autonomous Navigation \u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86Intention Net\u3002\u7ed9\u51fa\u4e86\u4e24\u79cd\u6982\u5ff5\u4ee5\u53ca\uff0c\u4e00\u4e2a\u662fDLM(Discrete Local Motion)\u53ca\u5176\u7f51\u7edc\uff0c\u4e00\u4e2a\u662fLPE(local path and environment)\u53ca\u5176\u7f51\u7edc DLM\uff1a\u79bb\u6563\u5c40\u90e8\u8fd0\u52a8 \u672c\u6587\u7684\u79bb\u6563\u5c40\u90e8\u8fd0\u52a8\u4e5f\u5c31\u662f\u6307\u5f53\u524d\u5efa\u8bae\uff08\u547d\u4ee4\uff09\u7684\u5c40\u90e8\u8fd0\u52a8\uff0c\u6bd4\u5982\u8bf4\u5de6\u53f3\u8f6c\u3001\u76f4\u884c\u4ee5\u53ca\u505c\u8f66\uff0c\u8bba\u6587\u5b9e\u73b0\u7684\u662f\u901a\u8fc7\u5f53\u524d\u8def\u5f84\u7684\u66f2\u7387\u6765\u5224\u5b9a\u5c40\u90e8\u8fd0\u52a8\u7684\u3002 \u4f46\u662f\u53ef\u4ee5\u7531\u5b9a\u4e49\u770b\u5230\uff0c\u8fd9\u4e2a\u662f\u4e00\u4e2a\u4e0d\u5b8c\u6574\u7684\u63cf\u8ff0\uff0c\u6bd4\u5982\u8bf4\u524d\u65b9\u6709\u591a\u4e2a\u5206\u5c94\u53e3\u65f6\u5de6\u53f3\u8f6c\u4fe1\u606f\u5e76\u4e0d\u5145\u5206\u3002 LPE\uff1a\u5c40\u90e8\u8def\u5f84\u4e0e\u73af\u5883 \u672c\u6587\u7684\u5c40\u90e8\u8def\u5f84\u4e0e\u73af\u5883\uff0c\u7528\u4e00\u4e2a224*224\u7684\u9e1f\u77b0\u89c6\u89d2\u7684\u56fe\u7247\uff0c\u56fe\u4e2d\u5305\u542b\u5730\u56fe\u4e2d\u7684\u969c\u788d\u7269\u4fe1\u606f(costmap)\u3001\u6700\u8fd1\u8d70\u8fc7\u7684\u8f68\u8ff9\u3001\u4ee5\u53ca\u5c06\u6765\u89c4\u5212\u7684\u8f68\u8ff9 \u7f51\u7edc\u7ed3\u6784 \u5206\u522b\u7528\u8fd9\u4e24\u4e2a\u7ed3\u6784\uff0c\u5728\u63a5\u53d7\u8def\u5f84\u89c4\u5212\u7684\u5c40\u90e8\u8def\u5f84\u8f93\u5165\u4ee5\u53ca\u5730\u56fe\u4fe1\u606f\u4e4b\u540e\u8f93\u51fa\u63a7\u5236\u6307\u4ee4\uff0c\u53ef\u4ee5\u7aef\u5230\u7aef\u5b66\u4e60\u3002","title":"Intention-Net: Integrating Planning and DeepLearning for Goal-Directed Autonomous Navigation"},{"location":"Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/#intention-net-integrating-planning-and-deeplearning-for-goal-directed-autonomous-navigation","text":"\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86Intention Net\u3002\u7ed9\u51fa\u4e86\u4e24\u79cd\u6982\u5ff5\u4ee5\u53ca\uff0c\u4e00\u4e2a\u662fDLM(Discrete Local Motion)\u53ca\u5176\u7f51\u7edc\uff0c\u4e00\u4e2a\u662fLPE(local path and environment)\u53ca\u5176\u7f51\u7edc","title":"Intention-Net: Integrating Planning and DeepLearning for Goal-Directed Autonomous Navigation"},{"location":"Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/#dlm","text":"\u672c\u6587\u7684\u79bb\u6563\u5c40\u90e8\u8fd0\u52a8\u4e5f\u5c31\u662f\u6307\u5f53\u524d\u5efa\u8bae\uff08\u547d\u4ee4\uff09\u7684\u5c40\u90e8\u8fd0\u52a8\uff0c\u6bd4\u5982\u8bf4\u5de6\u53f3\u8f6c\u3001\u76f4\u884c\u4ee5\u53ca\u505c\u8f66\uff0c\u8bba\u6587\u5b9e\u73b0\u7684\u662f\u901a\u8fc7\u5f53\u524d\u8def\u5f84\u7684\u66f2\u7387\u6765\u5224\u5b9a\u5c40\u90e8\u8fd0\u52a8\u7684\u3002 \u4f46\u662f\u53ef\u4ee5\u7531\u5b9a\u4e49\u770b\u5230\uff0c\u8fd9\u4e2a\u662f\u4e00\u4e2a\u4e0d\u5b8c\u6574\u7684\u63cf\u8ff0\uff0c\u6bd4\u5982\u8bf4\u524d\u65b9\u6709\u591a\u4e2a\u5206\u5c94\u53e3\u65f6\u5de6\u53f3\u8f6c\u4fe1\u606f\u5e76\u4e0d\u5145\u5206\u3002","title":"DLM\uff1a\u79bb\u6563\u5c40\u90e8\u8fd0\u52a8"},{"location":"Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/#lpe","text":"\u672c\u6587\u7684\u5c40\u90e8\u8def\u5f84\u4e0e\u73af\u5883\uff0c\u7528\u4e00\u4e2a224*224\u7684\u9e1f\u77b0\u89c6\u89d2\u7684\u56fe\u7247\uff0c\u56fe\u4e2d\u5305\u542b\u5730\u56fe\u4e2d\u7684\u969c\u788d\u7269\u4fe1\u606f(costmap)\u3001\u6700\u8fd1\u8d70\u8fc7\u7684\u8f68\u8ff9\u3001\u4ee5\u53ca\u5c06\u6765\u89c4\u5212\u7684\u8f68\u8ff9","title":"LPE\uff1a\u5c40\u90e8\u8def\u5f84\u4e0e\u73af\u5883"},{"location":"Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/#_1","text":"\u5206\u522b\u7528\u8fd9\u4e24\u4e2a\u7ed3\u6784\uff0c\u5728\u63a5\u53d7\u8def\u5f84\u89c4\u5212\u7684\u5c40\u90e8\u8def\u5f84\u8f93\u5165\u4ee5\u53ca\u5730\u56fe\u4fe1\u606f\u4e4b\u540e\u8f93\u51fa\u63a7\u5236\u6307\u4ee4\uff0c\u53ef\u4ee5\u7aef\u5230\u7aef\u5b66\u4e60\u3002","title":"\u7f51\u7edc\u7ed3\u6784"},{"location":"Planning_Control_DL/Learning-based_MPC/","text":"Learning-based Model Predictive Control for Autonomous Racing \u8fd9\u7bc7\u6765\u81eaETH\u7684\u8bba\u6587\u8bb2\u8ff0\u4e86\u4e00\u4e2adata-driven MPC for racing\u7684\u7b97\u6cd5\u7cfb\u7edf\u6784\u5efa\u3002\u6838\u5fc3\u601d\u8def\u662f\u4f7f\u7528GP(\u9ad8\u65af\u8fc7\u7a0b)\u8865\u507f\u52a8\u529b\u5b66\u6a21\u578b\u7684\u4e0d\u786e\u5b9a\u6027\u4e0e\u5efa\u6a21\u8bef\u5dee\uff0c\u7136\u540e\u7528\u6839\u636e\u6b64\u4e0d\u786e\u5b9a\u6027\u6784\u5efa\u975e\u7ebf\u6027\u4f18\u5316\u95ee\u9898\u4f5c\u4e3aMPC\u7684\u6846\u67b6\u8fdb\u884c\u6c42\u89e3\u3002\u6587\u7ae0\u7b2c\u4e8c\u7ae0\u4ecb\u7ecd\u4e86GP\u6a21\u578b\u4ee5\u53ca\u7a00\u758fGP\u56de\u5f52\u7b97\u6cd5\u3002\u4ece\u7b2c\u4e09\u8282\u5f00\u59cb\u6309\u987a\u5e8f\u8bf4\u660e\u4e86\u540d\u4e49\u52a8\u529b\u5b66\u6a21\u578b\u3001GP\u8865\u507f\u95ee\u9898\u8868\u8ff0\u3001MPC\u635f\u5931\u51fd\u6570\u8868\u8ff0\u3001\u5e26\u6709\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u7684MPC\u7ea6\u675f\u51fd\u6570\u8868\u8ff0\u3001\u89e3\u8026MPC\u4e0eGP\u5e76\u7b80\u5316MPC\u8ba1\u7b97\u7684\u65b9\u6cd5\u3001\u7b80\u5316GP\u8fed\u4ee3\u8ba1\u7b97\u7684\u65b9\u6cd5\u3002 \u540d\u5b57\u52a8\u529b\u5b66\u6a21\u578b \u5bf9\u4e8eRacing Car,\u8fd9\u91cc\u9009\u62e9\u7684\u52a8\u529b\u5b66\u5efa\u6a21\u662f\u8f83\u4e3a\u7cbe\u786e\u7684\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u5e76\u4e14\u8f6e\u80ce\u6a21\u578b\u9009\u62e9\u7684\u662fPacejka\u6a21\u578b(\u5177\u4f53\u770b\u516c\u5f0f) \\dot{\\mathbf{x}}=\\left[\\begin{array}{c}{v_{x} \\cos \\varphi-v_{y} \\sin \\varphi} \\\\ {v_{x} \\sin \\varphi+v_{y} \\cos \\varphi} \\\\ {r} \\\\ {\\frac{1}{m}\\left(F_{R, y}+F_{F, y} \\cos \\delta-m v_{x} r\\right)} \\\\ {\\frac{1}{I_{z}}\\left(F_{F, y} l_{F} \\cos \\delta-F_{R, y} l_{R}+\\tau_{\\mathrm{TV}}\\right)} \\\\ {\\Delta \\delta} \\\\ {\\Delta T}\\end{array}\\right] \\begin{aligned} r_{\\text {target }} &=\\delta \\frac{v_{x}}{l_{F}+l_{R}} \\\\ \\tau_{\\mathrm{TV}} &=\\left(r_{\\text {target }}-r\\right) P_{\\mathrm{TV}} \\\\ \\alpha_{R} &=\\arctan \\left(\\frac{v_{y}-l_{R} r}{v_{x}}\\right) \\\\ \\alpha_{F} &=\\arctan \\left(\\frac{v_{y}+l_{F} r}{v_{x}}\\right)-\\delta \\\\ F_{R, y} &=D_{R} \\sin \\left(C_{R} \\arctan \\left(B_{R} \\alpha_{R}\\right)\\right) \\\\ F_{F, y} &=D_{F} \\sin \\left(C_{F} \\arctan \\left(B_{F} \\alpha_{F}\\right)\\right) \\end{aligned} \u79ef\u5206\u4f7f\u7528\u7684\u662fRK4, T_s = 50ms GP\u6a21\u578b\u56de\u5f52\u95ee\u9898\u63cf\u8ff0 GP\u6a21\u578b\u7684\u8f93\u5165\u4e3a z = [v_x;v_y; r;\\delta + \\frac{1}{2}\\Delta\\delta; T+ \\frac{1}{2}\\Delta T] \u540d\u4e49\u8fd0\u52a8\u5b66\u7684\u9884\u6d4b\u7ed3\u679c\u4e0e\u5b9e\u9645\u6d4b\u91cf\u7ed3\u679c\u7684\u5dee\u503c\u5c31\u662fGP\u7684\u76ee\u6807\u8f93\u51fa\u3002 \\mathbf{y}_{k}=\\mathbf{B}_{d}^{\\dagger}\\left(\\mathbf{x}_{k+1}-\\mathbf{f}\\left(\\mathbf{x}_{k}, \\mathbf{u}_{k}\\right)\\right)=\\mathbf{d}_{\\mathrm{true}}\\left(\\mathbf{z}_{k}\\right)+\\mathbf{w}_{k} \u5176\u4e2d B_d = [0_{3\\times 3}; I{3\\times 3}; 0_{2\\times}3] \u8bf4\u660e\u53ea\u6709\u4e00\u90e8\u5206\u503c\u9700\u8981\u8865\u507f(\u5176\u5b9e\u53ea\u6709xy\u65b9\u5411\u52a0\u901f\u5ea6\u8fd8\u6709\u89d2\u52a0\u901f\u5ea6\u9700\u8981\u8865\u507f), B_d^\\dagger \u4e3a\u4f2a\u9006. \u5747\u503c\u4e0e\u65b9\u5dee\u7684\u4f20\u9012 \\begin{aligned} \\boldsymbol{\\mu}_{k+1}^{\\mathrm{x}}=&\\mathbf{f}\\left(\\boldsymbol{\\mu}_{k}^{\\mathrm{x}}, \\mathbf{u}_{k}\\right)+\\mathbf{B}_{d} \\boldsymbol{\\mu}^{d}\\left(\\boldsymbol{\\mu}_{k}^{\\mathrm{z}}\\right) \\\\ \\mathbf{\\Sigma}_{k+1}^{\\mathrm{x}}=&\\left[\\nabla_{x} \\mathbf{f}\\left(\\boldsymbol{\\mu}_{k}^{\\mathrm{x}}, \\mathbf{u}_{k}\\right) \\quad \\mathbf{B}_{d}\\right] \\\\ &\\left[\\begin{array}{cc}{\\mathbf{\\Sigma}_{k}^{\\mathbf{x}}} & {\\mathbf{\\Sigma}^{d}\\left(\\boldsymbol{\\mu}_{k}^{\\mathrm{z}}\\right)+\\mathbf{\\Sigma}^{\\mathrm{w}}}\\end{array}\\right] \\\\ &\\left[\\nabla_{x} \\mathbf{f}\\left(\\boldsymbol{\\mu}_{k}^{\\mathbf{x}}, \\mathbf{u}_{k}\\right) \\quad \\mathbf{B}_{d}\\right]^{T} \\end{aligned} \u635f\u5931\u51fd\u6570 \u8fd9\u91cc\u4e0d\u8a8a\u6284\u5176\u516c\u5f0f\uff0c\u539f\u56e0\u662f\u7531\u4e8eRacing\u7684\u6027\u8d28\u4f7f\u5f97\u5b83\u53ea\u9700\u8981\u7ed5\u5708\uff0c\u635f\u5931\u51fd\u6570\u4e0e\u57fa\u7840\u7684\u975e\u7ebf\u6027MPC\u6ca1\u6709\u672c\u8d28\u533a\u522b\u3002 \u5e26\u6709\u4e0d\u786e\u5b9a\u6027\u7684\u7ea6\u675f \u8ddd\u79bb\u8f68\u8ff9\u70b9\u7684\u504f\u79bb\u6982\u7387\u5927\u4e8e p \u5bf9\u5e94\u7684\u8ddd\u79bb\u504f\u5dee\u503c\u4e3a R_{GP}(\\sum^{XY}_k) = \\sqrt{\\chi^2_2(p) \\lambda_{max}(\\sum_k^{XY})} \u5176\u4e2d \\sum^{XY} \u8868\u8fbe XY \u4f4d\u79fb\u5206\u91cf\u7684covariance\u77e9\u9635\u3002 \\lambda \u4e3a\u53d6\u7279\u5f81\u503c\u64cd\u4f5c\uff0c \\chi^2_2 \u4e3a\u8868\u8fbe\u5361\u65b9\u5206\u5e03\u3002\u6839\u53f7\u91cc\u9762\u7684\u610f\u601d\u662f\uff0c\u5148\u53d6\u6700\u5927\u7279\u5f81\u503c\uff0c\u4e5f\u5c31\u662f XY \u504f\u5dee\u77e9\u9635\u4e2d\u53d6\u51fa\u4e3b\u65b9\u5411\u7684\u65b9\u5dee\u503c\uff0c\u7136\u540e\u5361\u65b9\u5206\u5e03\u8868\u8fbe\u7684\u662f\u6b63\u6001\u5206\u5e03\u5e73\u65b9\u503c\u7684\u6982\u7387\u5206\u5e03\u3002 \u6700\u540e\u7ea6\u675f\u53ef\u4ee5\u8868\u8fbe\u4e3a \\left\\|\\left[\\begin{array}{c}{\\mu_{k}^{X}} \\\\ {\\mu_{k}^{X}}\\end{array}\\right]-\\left[\\begin{array}{c}{X_{c}\\left(\\theta_{k}\\right)} \\\\ {Y_{c}\\left(\\theta_{k}\\right)}\\end{array}\\right]\\right\\|^{2} \\leq\\left\\|R\\left(\\theta_{k}\\right)-R_{\\mathrm{GP}}\\left(\\Sigma_{k}^{X Y}\\right)\\right\\|^{2} \u5b9e\u9645\u8fd0\u7b97\u65f6\u53ea\u5bf9\u524d\u51e0\u6b65\u6709\u6548\u3002 \u5176\u4ed6\u5173\u4e8e\u529b\u3001\u8f93\u5165\u3001\u8f93\u5165\u53d8\u5316\u7387\u7684\u56fa\u5b9a\u7ea6\u675f\u8fd9\u91cc\u4e0d\u518d\u8a8a\u5199\u3002 \u8ba1\u7b97\u8003\u8651 \u7531\u4e8e\u7ea6\u675f\u4e2d\u5e26\u6709\u65b9\u5dee\uff0c\u6240\u4ee5\u4f1a\u548c\u5b9e\u9645\u91c7\u53d6\u7684\u63a7\u5236\u7ed3\u679c\u8026\u5408\uff0c\u8fd9\u91cc\u91c7\u53d6\u4e86\u4e00\u4e2a\u5b9e\u65f6\u8fd0\u7b97\u7684\u7b80\u5316\u3002\u7531\u4e8e\u4e0a\u4e00\u65f6\u523b\u7684\u4f18\u5316\u7ed3\u679c\u4f1a\u4fdd\u5b58\u5230\u8fd9\u4e00\u65f6\u523b\uff0c\u56e0\u800c\u53ef\u4ee5\u8fd1\u4f3c\u8ba4\u4e3a\u4f18\u5316\u524d\u540e\u7684\u63a7\u5236\u8f93\u5165\u5dee\u4e0d\u4f1a\u592a\u5927\uff0c\u8fdb\u800c\u65b9\u5dee\u7684\u4f20\u9012\u53ea\u4ee5\u672c\u65f6\u523b\u7b2c\u4e00\u6b21\u7684\u7ed3\u679c\u4e3a\u51c6\uff0c\u672c\u65f6\u523b\u540e\u7eed\u8fed\u4ee3\u4f18\u5316\u4e0d\u518d\u6539\u53d8\u65b9\u5dee\u3002 \u5728\u7ebf\u5b66\u4e60GP\u8865\u507f\u53c2\u6570 \u4f5c\u8005\u5f00\u53d1\u4e86\u4e00\u5957\u5728\u7ebf\u9009\u62e9\u6570\u636e\u6dfb\u52a0\u3001\u9009\u62e9\u6570\u636e\u66ff\u6362\u3001\u53bb\u9664outlier\u7684\u7b97\u6cd5\uff0c\u8fd9\u91cc\u4e0d\u590d\u8ff0\u3002","title":"Learning-based Model Predictive Control for Autonomous Racing"},{"location":"Planning_Control_DL/Learning-based_MPC/#learning-based-model-predictive-control-for-autonomous-racing","text":"\u8fd9\u7bc7\u6765\u81eaETH\u7684\u8bba\u6587\u8bb2\u8ff0\u4e86\u4e00\u4e2adata-driven MPC for racing\u7684\u7b97\u6cd5\u7cfb\u7edf\u6784\u5efa\u3002\u6838\u5fc3\u601d\u8def\u662f\u4f7f\u7528GP(\u9ad8\u65af\u8fc7\u7a0b)\u8865\u507f\u52a8\u529b\u5b66\u6a21\u578b\u7684\u4e0d\u786e\u5b9a\u6027\u4e0e\u5efa\u6a21\u8bef\u5dee\uff0c\u7136\u540e\u7528\u6839\u636e\u6b64\u4e0d\u786e\u5b9a\u6027\u6784\u5efa\u975e\u7ebf\u6027\u4f18\u5316\u95ee\u9898\u4f5c\u4e3aMPC\u7684\u6846\u67b6\u8fdb\u884c\u6c42\u89e3\u3002\u6587\u7ae0\u7b2c\u4e8c\u7ae0\u4ecb\u7ecd\u4e86GP\u6a21\u578b\u4ee5\u53ca\u7a00\u758fGP\u56de\u5f52\u7b97\u6cd5\u3002\u4ece\u7b2c\u4e09\u8282\u5f00\u59cb\u6309\u987a\u5e8f\u8bf4\u660e\u4e86\u540d\u4e49\u52a8\u529b\u5b66\u6a21\u578b\u3001GP\u8865\u507f\u95ee\u9898\u8868\u8ff0\u3001MPC\u635f\u5931\u51fd\u6570\u8868\u8ff0\u3001\u5e26\u6709\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u7684MPC\u7ea6\u675f\u51fd\u6570\u8868\u8ff0\u3001\u89e3\u8026MPC\u4e0eGP\u5e76\u7b80\u5316MPC\u8ba1\u7b97\u7684\u65b9\u6cd5\u3001\u7b80\u5316GP\u8fed\u4ee3\u8ba1\u7b97\u7684\u65b9\u6cd5\u3002","title":"Learning-based Model Predictive Control for Autonomous Racing"},{"location":"Planning_Control_DL/Learning-based_MPC/#_1","text":"\u5bf9\u4e8eRacing Car,\u8fd9\u91cc\u9009\u62e9\u7684\u52a8\u529b\u5b66\u5efa\u6a21\u662f\u8f83\u4e3a\u7cbe\u786e\u7684\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u5e76\u4e14\u8f6e\u80ce\u6a21\u578b\u9009\u62e9\u7684\u662fPacejka\u6a21\u578b(\u5177\u4f53\u770b\u516c\u5f0f) \\dot{\\mathbf{x}}=\\left[\\begin{array}{c}{v_{x} \\cos \\varphi-v_{y} \\sin \\varphi} \\\\ {v_{x} \\sin \\varphi+v_{y} \\cos \\varphi} \\\\ {r} \\\\ {\\frac{1}{m}\\left(F_{R, y}+F_{F, y} \\cos \\delta-m v_{x} r\\right)} \\\\ {\\frac{1}{I_{z}}\\left(F_{F, y} l_{F} \\cos \\delta-F_{R, y} l_{R}+\\tau_{\\mathrm{TV}}\\right)} \\\\ {\\Delta \\delta} \\\\ {\\Delta T}\\end{array}\\right] \\begin{aligned} r_{\\text {target }} &=\\delta \\frac{v_{x}}{l_{F}+l_{R}} \\\\ \\tau_{\\mathrm{TV}} &=\\left(r_{\\text {target }}-r\\right) P_{\\mathrm{TV}} \\\\ \\alpha_{R} &=\\arctan \\left(\\frac{v_{y}-l_{R} r}{v_{x}}\\right) \\\\ \\alpha_{F} &=\\arctan \\left(\\frac{v_{y}+l_{F} r}{v_{x}}\\right)-\\delta \\\\ F_{R, y} &=D_{R} \\sin \\left(C_{R} \\arctan \\left(B_{R} \\alpha_{R}\\right)\\right) \\\\ F_{F, y} &=D_{F} \\sin \\left(C_{F} \\arctan \\left(B_{F} \\alpha_{F}\\right)\\right) \\end{aligned} \u79ef\u5206\u4f7f\u7528\u7684\u662fRK4, T_s = 50ms","title":"\u540d\u5b57\u52a8\u529b\u5b66\u6a21\u578b"},{"location":"Planning_Control_DL/Learning-based_MPC/#gp","text":"GP\u6a21\u578b\u7684\u8f93\u5165\u4e3a z = [v_x;v_y; r;\\delta + \\frac{1}{2}\\Delta\\delta; T+ \\frac{1}{2}\\Delta T] \u540d\u4e49\u8fd0\u52a8\u5b66\u7684\u9884\u6d4b\u7ed3\u679c\u4e0e\u5b9e\u9645\u6d4b\u91cf\u7ed3\u679c\u7684\u5dee\u503c\u5c31\u662fGP\u7684\u76ee\u6807\u8f93\u51fa\u3002 \\mathbf{y}_{k}=\\mathbf{B}_{d}^{\\dagger}\\left(\\mathbf{x}_{k+1}-\\mathbf{f}\\left(\\mathbf{x}_{k}, \\mathbf{u}_{k}\\right)\\right)=\\mathbf{d}_{\\mathrm{true}}\\left(\\mathbf{z}_{k}\\right)+\\mathbf{w}_{k} \u5176\u4e2d B_d = [0_{3\\times 3}; I{3\\times 3}; 0_{2\\times}3] \u8bf4\u660e\u53ea\u6709\u4e00\u90e8\u5206\u503c\u9700\u8981\u8865\u507f(\u5176\u5b9e\u53ea\u6709xy\u65b9\u5411\u52a0\u901f\u5ea6\u8fd8\u6709\u89d2\u52a0\u901f\u5ea6\u9700\u8981\u8865\u507f), B_d^\\dagger \u4e3a\u4f2a\u9006.","title":"GP\u6a21\u578b\u56de\u5f52\u95ee\u9898\u63cf\u8ff0"},{"location":"Planning_Control_DL/Learning-based_MPC/#_2","text":"\\begin{aligned} \\boldsymbol{\\mu}_{k+1}^{\\mathrm{x}}=&\\mathbf{f}\\left(\\boldsymbol{\\mu}_{k}^{\\mathrm{x}}, \\mathbf{u}_{k}\\right)+\\mathbf{B}_{d} \\boldsymbol{\\mu}^{d}\\left(\\boldsymbol{\\mu}_{k}^{\\mathrm{z}}\\right) \\\\ \\mathbf{\\Sigma}_{k+1}^{\\mathrm{x}}=&\\left[\\nabla_{x} \\mathbf{f}\\left(\\boldsymbol{\\mu}_{k}^{\\mathrm{x}}, \\mathbf{u}_{k}\\right) \\quad \\mathbf{B}_{d}\\right] \\\\ &\\left[\\begin{array}{cc}{\\mathbf{\\Sigma}_{k}^{\\mathbf{x}}} & {\\mathbf{\\Sigma}^{d}\\left(\\boldsymbol{\\mu}_{k}^{\\mathrm{z}}\\right)+\\mathbf{\\Sigma}^{\\mathrm{w}}}\\end{array}\\right] \\\\ &\\left[\\nabla_{x} \\mathbf{f}\\left(\\boldsymbol{\\mu}_{k}^{\\mathbf{x}}, \\mathbf{u}_{k}\\right) \\quad \\mathbf{B}_{d}\\right]^{T} \\end{aligned}","title":"\u5747\u503c\u4e0e\u65b9\u5dee\u7684\u4f20\u9012"},{"location":"Planning_Control_DL/Learning-based_MPC/#_3","text":"\u8fd9\u91cc\u4e0d\u8a8a\u6284\u5176\u516c\u5f0f\uff0c\u539f\u56e0\u662f\u7531\u4e8eRacing\u7684\u6027\u8d28\u4f7f\u5f97\u5b83\u53ea\u9700\u8981\u7ed5\u5708\uff0c\u635f\u5931\u51fd\u6570\u4e0e\u57fa\u7840\u7684\u975e\u7ebf\u6027MPC\u6ca1\u6709\u672c\u8d28\u533a\u522b\u3002","title":"\u635f\u5931\u51fd\u6570"},{"location":"Planning_Control_DL/Learning-based_MPC/#_4","text":"\u8ddd\u79bb\u8f68\u8ff9\u70b9\u7684\u504f\u79bb\u6982\u7387\u5927\u4e8e p \u5bf9\u5e94\u7684\u8ddd\u79bb\u504f\u5dee\u503c\u4e3a R_{GP}(\\sum^{XY}_k) = \\sqrt{\\chi^2_2(p) \\lambda_{max}(\\sum_k^{XY})} \u5176\u4e2d \\sum^{XY} \u8868\u8fbe XY \u4f4d\u79fb\u5206\u91cf\u7684covariance\u77e9\u9635\u3002 \\lambda \u4e3a\u53d6\u7279\u5f81\u503c\u64cd\u4f5c\uff0c \\chi^2_2 \u4e3a\u8868\u8fbe\u5361\u65b9\u5206\u5e03\u3002\u6839\u53f7\u91cc\u9762\u7684\u610f\u601d\u662f\uff0c\u5148\u53d6\u6700\u5927\u7279\u5f81\u503c\uff0c\u4e5f\u5c31\u662f XY \u504f\u5dee\u77e9\u9635\u4e2d\u53d6\u51fa\u4e3b\u65b9\u5411\u7684\u65b9\u5dee\u503c\uff0c\u7136\u540e\u5361\u65b9\u5206\u5e03\u8868\u8fbe\u7684\u662f\u6b63\u6001\u5206\u5e03\u5e73\u65b9\u503c\u7684\u6982\u7387\u5206\u5e03\u3002 \u6700\u540e\u7ea6\u675f\u53ef\u4ee5\u8868\u8fbe\u4e3a \\left\\|\\left[\\begin{array}{c}{\\mu_{k}^{X}} \\\\ {\\mu_{k}^{X}}\\end{array}\\right]-\\left[\\begin{array}{c}{X_{c}\\left(\\theta_{k}\\right)} \\\\ {Y_{c}\\left(\\theta_{k}\\right)}\\end{array}\\right]\\right\\|^{2} \\leq\\left\\|R\\left(\\theta_{k}\\right)-R_{\\mathrm{GP}}\\left(\\Sigma_{k}^{X Y}\\right)\\right\\|^{2} \u5b9e\u9645\u8fd0\u7b97\u65f6\u53ea\u5bf9\u524d\u51e0\u6b65\u6709\u6548\u3002 \u5176\u4ed6\u5173\u4e8e\u529b\u3001\u8f93\u5165\u3001\u8f93\u5165\u53d8\u5316\u7387\u7684\u56fa\u5b9a\u7ea6\u675f\u8fd9\u91cc\u4e0d\u518d\u8a8a\u5199\u3002","title":"\u5e26\u6709\u4e0d\u786e\u5b9a\u6027\u7684\u7ea6\u675f"},{"location":"Planning_Control_DL/Learning-based_MPC/#_5","text":"\u7531\u4e8e\u7ea6\u675f\u4e2d\u5e26\u6709\u65b9\u5dee\uff0c\u6240\u4ee5\u4f1a\u548c\u5b9e\u9645\u91c7\u53d6\u7684\u63a7\u5236\u7ed3\u679c\u8026\u5408\uff0c\u8fd9\u91cc\u91c7\u53d6\u4e86\u4e00\u4e2a\u5b9e\u65f6\u8fd0\u7b97\u7684\u7b80\u5316\u3002\u7531\u4e8e\u4e0a\u4e00\u65f6\u523b\u7684\u4f18\u5316\u7ed3\u679c\u4f1a\u4fdd\u5b58\u5230\u8fd9\u4e00\u65f6\u523b\uff0c\u56e0\u800c\u53ef\u4ee5\u8fd1\u4f3c\u8ba4\u4e3a\u4f18\u5316\u524d\u540e\u7684\u63a7\u5236\u8f93\u5165\u5dee\u4e0d\u4f1a\u592a\u5927\uff0c\u8fdb\u800c\u65b9\u5dee\u7684\u4f20\u9012\u53ea\u4ee5\u672c\u65f6\u523b\u7b2c\u4e00\u6b21\u7684\u7ed3\u679c\u4e3a\u51c6\uff0c\u672c\u65f6\u523b\u540e\u7eed\u8fed\u4ee3\u4f18\u5316\u4e0d\u518d\u6539\u53d8\u65b9\u5dee\u3002","title":"\u8ba1\u7b97\u8003\u8651"},{"location":"Planning_Control_DL/Learning-based_MPC/#gp_1","text":"\u4f5c\u8005\u5f00\u53d1\u4e86\u4e00\u5957\u5728\u7ebf\u9009\u62e9\u6570\u636e\u6dfb\u52a0\u3001\u9009\u62e9\u6570\u636e\u66ff\u6362\u3001\u53bb\u9664outlier\u7684\u7b97\u6cd5\uff0c\u8fd9\u91cc\u4e0d\u590d\u8ff0\u3002","title":"\u5728\u7ebf\u5b66\u4e60GP\u8865\u507f\u53c2\u6570"},{"location":"Planning_Control_DL/PMPNet/","text":"Probabilistic End-to-End Vehicle Navigation in Complex Dynamic Environments with Multimodal Sensor Fusion \u8fd9\u7bc7paper\u4e5f\u662f\u5b9e\u9a8c\u5ba4\u5b66\u957f\u7684\u5de5\u4f5c. \u6709 \u89c6\u9891 : Network structure Route\u6765\u81ea\u4e8eCARLA\u7684\u8f93\u5165\uff0c\u8fd9\u91cc\u91c7\u7528\u6700\u8fd1\u7684130\u4e2awaypoints(CARLA\u5730\u56fe\u4e0a\u7684\u5c0f\u70b9)\u3002 attention\u662f\u7528\u4e8e\u8868\u8fbe\u4e0d\u540c\u4f20\u611f\u5668\u7684\u7279\u5f81\u7684\u6743\u91cd. a_1 \u8868\u8fbe\u7684\u662f\u6a21\u4eff\u5b66\u4e60\u7ed9\u51fa\u7684throttle, steering and brake GMM\u7684\u56de\u5f52\u4f7f\u7528\u6765\u81ea\u4e8e \u8fd9\u4e00\u7bc7paper.pdf \u3002\u8f93\u51fa\u7684\u662f\u672a\u6765\u6570\u79d2\u5185\u8f66\u5b50\u7684\u8fd0\u52a8\u89c4\u5212\u503c(\u901f\u5ea6\u4e0e\u89d2\u5ea6\u5206\u5e03\u56fe)\uff0c\u5bf9\u5176\u8fdb\u884c\u79ef\u5206\u5f97\u5230\u672a\u67655m\u7684\u76ee\u6807\u4f4d\u7f6e\uff0c\u7528PID\u63a7\u5236\u5668\u8ddf\u8e2a\u8fd9\u4e2a\u76ee\u6807\u70b9\u5f97\u5230 a_2 ,\u4e24\u4e2a\u63a7\u5236\u6307\u4ee4\u7684\u878d\u5408\u5219\u662f\u4f9d\u636e\u5bf9\u7d2f\u8ba1\u65b9\u5dee\u503c\u7684\u4f30\u8ba1\u3002\u4e0b\u56fe\u6765\u81ea\u4e8e\u8fd9\u7bc7\u5f15\u7528\u7684\u8bba\u6587\uff0c\u5177\u4f53\u529f\u80fd\u4e0e\u672c\u6587\u65e0\u5173\uff1a \\boldsymbol{a}_{f}=(1-\\lambda) \\boldsymbol{a}_{1}+\\lambda \\boldsymbol{a}_{2}, \\lambda=e^{-c_{1} \\cdot \\max \\left(0, \\Sigma_{i}^{k} \\sigma^{2}-c_{2}\\right)} \u878d\u5408\u4e86GMM\u6a21\u4eff\u5b66\u4e60\u7684\u601d\u60f3\uff0c\u5229\u7528GMM\u66ff\u4ee3\u4f20\u7edfplanning\uff0c\u7ed9\u51fa\u5e26\u65b9\u5dee\u4f30\u8ba1\u7684learning\u7ed3\u679c\uff0c\u7528\u6a21\u4eff\u5b66\u4e60\u63d0\u9ad8\u6027\u80fd\uff0c\u914d\u5408\u65b9\u5dee\u4f30\u8ba1\u7ed9\u51fa\u63a7\u5236\u7ed3\u679c\u3002","title":"Probabilistic End-to-End Vehicle Navigation in Complex Dynamic Environments with Multimodal Sensor Fusion"},{"location":"Planning_Control_DL/PMPNet/#probabilistic-end-to-end-vehicle-navigation-in-complex-dynamic-environments-with-multimodal-sensor-fusion","text":"\u8fd9\u7bc7paper\u4e5f\u662f\u5b9e\u9a8c\u5ba4\u5b66\u957f\u7684\u5de5\u4f5c. \u6709 \u89c6\u9891 :","title":"Probabilistic End-to-End Vehicle Navigation in Complex Dynamic Environments with Multimodal Sensor Fusion"},{"location":"Planning_Control_DL/PMPNet/#network-structure","text":"Route\u6765\u81ea\u4e8eCARLA\u7684\u8f93\u5165\uff0c\u8fd9\u91cc\u91c7\u7528\u6700\u8fd1\u7684130\u4e2awaypoints(CARLA\u5730\u56fe\u4e0a\u7684\u5c0f\u70b9)\u3002 attention\u662f\u7528\u4e8e\u8868\u8fbe\u4e0d\u540c\u4f20\u611f\u5668\u7684\u7279\u5f81\u7684\u6743\u91cd. a_1 \u8868\u8fbe\u7684\u662f\u6a21\u4eff\u5b66\u4e60\u7ed9\u51fa\u7684throttle, steering and brake GMM\u7684\u56de\u5f52\u4f7f\u7528\u6765\u81ea\u4e8e \u8fd9\u4e00\u7bc7paper.pdf \u3002\u8f93\u51fa\u7684\u662f\u672a\u6765\u6570\u79d2\u5185\u8f66\u5b50\u7684\u8fd0\u52a8\u89c4\u5212\u503c(\u901f\u5ea6\u4e0e\u89d2\u5ea6\u5206\u5e03\u56fe)\uff0c\u5bf9\u5176\u8fdb\u884c\u79ef\u5206\u5f97\u5230\u672a\u67655m\u7684\u76ee\u6807\u4f4d\u7f6e\uff0c\u7528PID\u63a7\u5236\u5668\u8ddf\u8e2a\u8fd9\u4e2a\u76ee\u6807\u70b9\u5f97\u5230 a_2 ,\u4e24\u4e2a\u63a7\u5236\u6307\u4ee4\u7684\u878d\u5408\u5219\u662f\u4f9d\u636e\u5bf9\u7d2f\u8ba1\u65b9\u5dee\u503c\u7684\u4f30\u8ba1\u3002\u4e0b\u56fe\u6765\u81ea\u4e8e\u8fd9\u7bc7\u5f15\u7528\u7684\u8bba\u6587\uff0c\u5177\u4f53\u529f\u80fd\u4e0e\u672c\u6587\u65e0\u5173\uff1a \\boldsymbol{a}_{f}=(1-\\lambda) \\boldsymbol{a}_{1}+\\lambda \\boldsymbol{a}_{2}, \\lambda=e^{-c_{1} \\cdot \\max \\left(0, \\Sigma_{i}^{k} \\sigma^{2}-c_{2}\\right)} \u878d\u5408\u4e86GMM\u6a21\u4eff\u5b66\u4e60\u7684\u601d\u60f3\uff0c\u5229\u7528GMM\u66ff\u4ee3\u4f20\u7edfplanning\uff0c\u7ed9\u51fa\u5e26\u65b9\u5dee\u4f30\u8ba1\u7684learning\u7ed3\u679c\uff0c\u7528\u6a21\u4eff\u5b66\u4e60\u63d0\u9ad8\u6027\u80fd\uff0c\u914d\u5408\u65b9\u5dee\u4f30\u8ba1\u7ed9\u51fa\u63a7\u5236\u7ed3\u679c\u3002","title":"Network structure"},{"location":"Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/","text":"Path Integral Networks: End-to-End Differentiable Optimal Control \u8fd9\u7bc7\u8bba\u6587\u5c06\u8def\u5f84\u79ef\u5206\u63a7\u5236\u7528\u5728\u4e86\u7aef\u5230\u7aef\u7684\u53ef\u5fae\u5206\u6700\u4f18\u63a7\u5236\u4e2d\uff0cPath Integral Control \u53ef\u4ee5\u53c2\u8003 \u8fd9\u7bc7 \u57fa\u672cPath Integral\u7b97\u6cd5 \u7c7b\u4f3c\u4e8ePath Integal \u63a7\u5236\u8bba\u6587\u4e2d\u7ed9\u51fa\u7684\u7b97\u6cd5\uff0c\u6ce8\u610f\u7cfb\u7edf\u5728\u6a21\u578b\u9884\u6d4b\u4ee5\u53careward\u9884\u6d4b\u7684\u65f6\u5019\u4f7f\u7528\u7684\u51fd\u6570\u4e3a\u795e\u7ecf\u7f51\u7edc\u5c42\u3002\u7531\u6b64\u53ef\u4ee5\u5f15\u51fa\u4ee5\u4e0b\u7684\u7ed3\u6784\u56fe \u5728\u6709\u4e13\u5bb6\u8f93\u5165\u53c2\u8003\u7684\u60c5\u51b5\u4e0b\uff0c\u6a21\u578b\u9884\u6d4b\u51fd\u6570\u4ee5\u53careward\u7684\u9884\u6d4b\u51fd\u6570\u53ef\u4ee5\u7aef\u5230\u7aef\u5b66\u4e60\u3002","title":"Path Integral Networks: End-to-End Differentiable Optimal Control"},{"location":"Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/#path-integral-networks-end-to-end-differentiable-optimal-control","text":"\u8fd9\u7bc7\u8bba\u6587\u5c06\u8def\u5f84\u79ef\u5206\u63a7\u5236\u7528\u5728\u4e86\u7aef\u5230\u7aef\u7684\u53ef\u5fae\u5206\u6700\u4f18\u63a7\u5236\u4e2d\uff0cPath Integral Control \u53ef\u4ee5\u53c2\u8003 \u8fd9\u7bc7","title":"Path Integral Networks: End-to-End Differentiable Optimal Control"},{"location":"Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/#path-integral","text":"\u7c7b\u4f3c\u4e8ePath Integal \u63a7\u5236\u8bba\u6587\u4e2d\u7ed9\u51fa\u7684\u7b97\u6cd5\uff0c\u6ce8\u610f\u7cfb\u7edf\u5728\u6a21\u578b\u9884\u6d4b\u4ee5\u53careward\u9884\u6d4b\u7684\u65f6\u5019\u4f7f\u7528\u7684\u51fd\u6570\u4e3a\u795e\u7ecf\u7f51\u7edc\u5c42\u3002\u7531\u6b64\u53ef\u4ee5\u5f15\u51fa\u4ee5\u4e0b\u7684\u7ed3\u6784\u56fe","title":"\u57fa\u672cPath Integral\u7b97\u6cd5"},{"location":"Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/#_1","text":"\u5728\u6709\u4e13\u5bb6\u8f93\u5165\u53c2\u8003\u7684\u60c5\u51b5\u4e0b\uff0c\u6a21\u578b\u9884\u6d4b\u51fd\u6570\u4ee5\u53careward\u7684\u9884\u6d4b\u51fd\u6570\u53ef\u4ee5\u7aef\u5230\u7aef\u5b66\u4e60\u3002","title":""},{"location":"Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/","text":"QMDP-Net: Deep Learning for Planning under Partial Observability \u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u7684QMDP\u7f51\u7edc\uff0c\u76ee\u7684\u662f\u5f53\u673a\u5668\u4eba\u53ea\u80fd\u89c2\u6d4b\u5230\u573a\u666f\u7684\u4e00\u90e8\u5206\u4e14\u4e0d\u80fd\u786e\u5b9a\u81ea\u5df1\u7684\u72b6\u6001\u7684\u65f6\u5019\uff0c\u5982\u679c\u5728\u4e0d\u786e\u5b9a\u6027\u4e2d\u4e00\u8fb9\u63a2\u7d22\u4e00\u8fb9\u8fdb\u884c\u89c4\u5212\u3002 \u672c\u6587\u7684\u56fe\u4e00\u8bf4\u660e\u4e86\u672c\u6587\u53ef\u4ee5\u4f7f\u7528\u6a21\u4eff\u5b66\u4e60\uff0c\u5b66\u4f1a\u4e00\u4e9b\u63a2\u7d22\u7684\u65b9\u5f0f\uff0c\u7136\u540e\u673a\u5668\u4eba\u80fd\u591f\u65b0\u7684\u8ff7\u5bab\u4e2d\u6210\u529f\u5bfc\u822a\u3002 \u4e3b\u8981\u8d21\u732e\u4e0e\u7f51\u7edc\u7ed3\u6784 \u5b8f\u89c2\u6765\u770b\uff0c\u4e00\u4e2aPolicy\u7684\u8f93\u5165\u662f\u5386\u53f2\u4e00\u7cfb\u5217\u7684actions\u548cobservations\uff0c\u8f93\u51fa\u7684\u662f\u4e00\u4e2a\u4e0b\u4e00\u4e2aaction\u3002\u800cQMDP\u7f51\u7edc\u7531\u4e24\u90e8\u5206\u7ec4\u6210\uff0c\u4e00\u4e2a\u662fRNN\u7ed3\u6784\u7684\u8d1d\u53f6\u65af\u6ee4\u6ce2\u5668\uff08\u7528\u4e8e\u7f16\u7801\u4fe1\u606f\uff09\uff0c\u4e00\u4e2a\u662fQMDP planner\u7f51\u7edc\u3002 \u6ee4\u6ce2\u6a21\u5757 \u6ee4\u6ce2\u6a21\u5757\uff0c\u5c06\u4e00\u4e2abelief\u3001\u884c\u52a8\u4ee5\u53ca\u89c2\u6d4b\u6620\u5c04\u5230\u4e0b\u4e00\u4e2abelief b_{t+1} = f(b_t|a_t, o_t) \u3002\u5b9e\u9645\u4e0a\u4f1a\u5206\u4e24\u6b65\uff0c\u7b2c\u4e00\u6b65\u8003\u8651action\uff0c\u7b2c\u4e8c\u6b65\u8003\u8651\u89c2\u6d4b\u3002 \u5bf9\u4e8e\u672c\u6587\u7684\u4e00\u4e2a N*N \u7f51\u683c\u7684\u5bfc\u822a\u4efb\u52a1\uff0cbelief\u662f\u4e00\u4e2aN*N\u7684\u5f20\u91cf\uff0c\u5927\u81f4\u8868\u8fbe\u7684\u662f\u5bf9\u673a\u5668\u4eba\u5f53\u524d\u4f4d\u7f6e\u7684\u7f6e\u4fe1\u5ea6\u3002 \u56fe\u4e2d\u7684 f_T \u662f\u4e00\u4e2a\u5e26\u6709|A|\u7ec4\u6ee4\u6ce2\u5668\u7684\u5377\u79ef\uff0c\u8f93\u51fa\u7684 b_t^\\prime \u662f\u4e0d\u540c\u8fd0\u52a8\u6761\u4ef6\u4e0b\uff0c\u7269\u4f53\u5bf9\u81ea\u8eab\u65b0\u7684\u4f4d\u7f6e\u7684\u4e00\u4e2a\u4f30\u8ba1\u3002 \u7406\u8bba\u4e0a\u6765\u8bf4\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528hard indexing\u53d6\u51fa\u5f53\u524d\u884c\u52a8\u5bf9\u5e94\u7684\u8fd0\u52a8\u540e\u7684\u4f30\u8ba1\uff0c\u4f46\u662f\u8fd9\u91cc\u8bba\u6587\u91c7\u7528\u4e86soft index\uff0c\u7528\u5168\u8fde\u63a5\u5c42\u5c06action\u6620\u5c04\u5230 f_A(a) (\u5f62\u72b6\u4e3aA\u7684\u5f20\u91cf)\u4e2d\uff0c\u518d\u7528\u6c42\u548c(\u70b9\u4e58\uff0c\u5173\u4e8eA\u8fd9\u4e2a\u7ef4\u5ea6\u6c42\u548c)\u7684\u5f62\u5f0f\u6267\u884csoft indexing\u3002 \u89c2\u6d4b\u6a21\u578b\u8f93\u51fa\u4e00\u4e2a N*N*O \u7684\u5f20\u91cf\uff0c\u5176\u4e2d\u6bcf\u4e00\u4e2a\u6570\u4ee3\u8868\u7684\u662f\u7f51\u683c\u5f53\u524d\u70b9\u5f97\u5230\u67d0\u4e00\u4e2a\u6d4b\u91cf\u503c\u7684\u6982\u7387\u3002\u540c\u6837\u91c7\u7528\u4e00\u4e2a\u5168\u8fde\u63a5\u6620\u5c04\u4ee5\u53casoft indexing\uff0c\u5f97\u5230\u4e00\u4e2aN*N\u7684\u6839\u636e\u89c2\u6d4b\u5f97\u5230\u7684\u4f30\u8ba1\u3002 \u5c06\u8fd0\u52a8\u4e0e\u89c2\u6d4b\u5f97\u5230\u7684\u4f30\u8ba1\u4f7f\u7528element-wise multiplication\u878d\u5408 planner \u6a21\u5757 \u4f7f\u7528Value-iteration\u5c42\u8ba1\u7b97\u51fa\u4e00\u4e2aN*N\u7684\u4ef7\u503c\u56fe\u3002\u8fd9\u4e2a\u5c42\u7684\u7b97\u6cd5\u662f 1. \u5c06\u521d\u59cb \\theta \u7528\u4e00\u4e2a\u5377\u79ef\u5c42\u6620\u5c04\u5230Reward\u56fe( N*N*A )\uff0c\u521d\u59cb\u5316Q\u51fd\u6570\u56fe 2. \u5bf9Q\u51fd\u6570\u56fe\u7684Action\u7ef4\u4f5cmax pool (1*1) \u5f97\u5230 V_k 3. \u5377\u79ef V_k \u4ee3\u8868state transition\u518d\u52a0\u4e0areward\u56fe\u5f97\u5230\u65b0\u7684Q\u51fd\u6570\u56fe 4. \u8fed\u4ee32\u30013\u591a\u6b21\uff0c\u5f97\u5230\u6700\u7ec8\u7684Q\u51fd\u6570 \u51b3\u7b56\u7684\u65f6\u5019\uff0c\u5c06Q\u51fd\u6570 (N*N*A) \u4e0e b_t (N*N) \u76f8\u4e58\uff0c\u5e76\u6c42\u548c\u5f97\u5230 q(a) (\u5f62\u72b6\u4e3aA\u7684\u5f20\u91cf) \u518d\u7528\u5168\u8fde\u63a5\u5c06\u8fd9\u4e2aq\u6620\u5c04\u5230action\u4e2d\u3002 \u6a21\u4eff\u8bad\u7ec3\u4e0e\u4f7f\u7528 \u6574\u4e2a\u8fd0\u7b97\u662f\u53ef\u5bfc\uff0c\u56e0\u800c\u53ef\u4ee5\u4f7f\u7528\u6a21\u4eff\u5b66\u4e60\u540c\u65f6\u8bad\u7ec3\u6ee4\u6ce2\u6a21\u5757\u4ee5\u53ca\u89c4\u5212\u6a21\u5757\u3002 \u5b9e\u9a8c\u76f4\u89c9 \u53efgeneralize\u5230\u65b0\u73af\u5883 \u7528soft index\u4ee5\u53ca\u53ef\u5b66\u4e60\u7684transition\u6709\u6548\u7684\u89e3\u51b3\u4e86\u4e0d\u786e\u5b9a\u6027\u95ee\u9898 QMDP-net\u5b66\u4e60\u5230\u7684\u6a21\u578b\u4e0d\u6b63\u786e\uff0c\u4f46\u662f\u6709\u7528 \u5728N\u6bd4\u8f83\u5c0f\u7684\u73af\u5883\u4e2d\u5f97\u5230\u7684\u7f51\u7edc\u53ef\u4ee5\u5feb\u901f\u5b66\u4e60\u5230N\u6bd4\u8f83\u5927\u7684\u73af\u5883 \u53ef\u4ee5\u7528CNN-LSTM\u7ed3\u6784\u66ff\u4ee3\u672c\u6587\u7684filter \u6a21\u5757\uff0c\u4f46\u662f\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u672c\u6587\u7684\u6548\u679c\u66f4\u597d(\u66f4\u591aregularization)","title":"QMDP-Net: Deep Learning for Planning under Partial Observability"},{"location":"Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/#qmdp-net-deep-learning-for-planning-under-partial-observability","text":"\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u7684QMDP\u7f51\u7edc\uff0c\u76ee\u7684\u662f\u5f53\u673a\u5668\u4eba\u53ea\u80fd\u89c2\u6d4b\u5230\u573a\u666f\u7684\u4e00\u90e8\u5206\u4e14\u4e0d\u80fd\u786e\u5b9a\u81ea\u5df1\u7684\u72b6\u6001\u7684\u65f6\u5019\uff0c\u5982\u679c\u5728\u4e0d\u786e\u5b9a\u6027\u4e2d\u4e00\u8fb9\u63a2\u7d22\u4e00\u8fb9\u8fdb\u884c\u89c4\u5212\u3002 \u672c\u6587\u7684\u56fe\u4e00\u8bf4\u660e\u4e86\u672c\u6587\u53ef\u4ee5\u4f7f\u7528\u6a21\u4eff\u5b66\u4e60\uff0c\u5b66\u4f1a\u4e00\u4e9b\u63a2\u7d22\u7684\u65b9\u5f0f\uff0c\u7136\u540e\u673a\u5668\u4eba\u80fd\u591f\u65b0\u7684\u8ff7\u5bab\u4e2d\u6210\u529f\u5bfc\u822a\u3002","title":"QMDP-Net: Deep Learning for Planning under Partial Observability"},{"location":"Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/#_1","text":"\u5b8f\u89c2\u6765\u770b\uff0c\u4e00\u4e2aPolicy\u7684\u8f93\u5165\u662f\u5386\u53f2\u4e00\u7cfb\u5217\u7684actions\u548cobservations\uff0c\u8f93\u51fa\u7684\u662f\u4e00\u4e2a\u4e0b\u4e00\u4e2aaction\u3002\u800cQMDP\u7f51\u7edc\u7531\u4e24\u90e8\u5206\u7ec4\u6210\uff0c\u4e00\u4e2a\u662fRNN\u7ed3\u6784\u7684\u8d1d\u53f6\u65af\u6ee4\u6ce2\u5668\uff08\u7528\u4e8e\u7f16\u7801\u4fe1\u606f\uff09\uff0c\u4e00\u4e2a\u662fQMDP planner\u7f51\u7edc\u3002","title":"\u4e3b\u8981\u8d21\u732e\u4e0e\u7f51\u7edc\u7ed3\u6784"},{"location":"Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/#_2","text":"\u6ee4\u6ce2\u6a21\u5757\uff0c\u5c06\u4e00\u4e2abelief\u3001\u884c\u52a8\u4ee5\u53ca\u89c2\u6d4b\u6620\u5c04\u5230\u4e0b\u4e00\u4e2abelief b_{t+1} = f(b_t|a_t, o_t) \u3002\u5b9e\u9645\u4e0a\u4f1a\u5206\u4e24\u6b65\uff0c\u7b2c\u4e00\u6b65\u8003\u8651action\uff0c\u7b2c\u4e8c\u6b65\u8003\u8651\u89c2\u6d4b\u3002 \u5bf9\u4e8e\u672c\u6587\u7684\u4e00\u4e2a N*N \u7f51\u683c\u7684\u5bfc\u822a\u4efb\u52a1\uff0cbelief\u662f\u4e00\u4e2aN*N\u7684\u5f20\u91cf\uff0c\u5927\u81f4\u8868\u8fbe\u7684\u662f\u5bf9\u673a\u5668\u4eba\u5f53\u524d\u4f4d\u7f6e\u7684\u7f6e\u4fe1\u5ea6\u3002 \u56fe\u4e2d\u7684 f_T \u662f\u4e00\u4e2a\u5e26\u6709|A|\u7ec4\u6ee4\u6ce2\u5668\u7684\u5377\u79ef\uff0c\u8f93\u51fa\u7684 b_t^\\prime \u662f\u4e0d\u540c\u8fd0\u52a8\u6761\u4ef6\u4e0b\uff0c\u7269\u4f53\u5bf9\u81ea\u8eab\u65b0\u7684\u4f4d\u7f6e\u7684\u4e00\u4e2a\u4f30\u8ba1\u3002 \u7406\u8bba\u4e0a\u6765\u8bf4\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528hard indexing\u53d6\u51fa\u5f53\u524d\u884c\u52a8\u5bf9\u5e94\u7684\u8fd0\u52a8\u540e\u7684\u4f30\u8ba1\uff0c\u4f46\u662f\u8fd9\u91cc\u8bba\u6587\u91c7\u7528\u4e86soft index\uff0c\u7528\u5168\u8fde\u63a5\u5c42\u5c06action\u6620\u5c04\u5230 f_A(a) (\u5f62\u72b6\u4e3aA\u7684\u5f20\u91cf)\u4e2d\uff0c\u518d\u7528\u6c42\u548c(\u70b9\u4e58\uff0c\u5173\u4e8eA\u8fd9\u4e2a\u7ef4\u5ea6\u6c42\u548c)\u7684\u5f62\u5f0f\u6267\u884csoft indexing\u3002 \u89c2\u6d4b\u6a21\u578b\u8f93\u51fa\u4e00\u4e2a N*N*O \u7684\u5f20\u91cf\uff0c\u5176\u4e2d\u6bcf\u4e00\u4e2a\u6570\u4ee3\u8868\u7684\u662f\u7f51\u683c\u5f53\u524d\u70b9\u5f97\u5230\u67d0\u4e00\u4e2a\u6d4b\u91cf\u503c\u7684\u6982\u7387\u3002\u540c\u6837\u91c7\u7528\u4e00\u4e2a\u5168\u8fde\u63a5\u6620\u5c04\u4ee5\u53casoft indexing\uff0c\u5f97\u5230\u4e00\u4e2aN*N\u7684\u6839\u636e\u89c2\u6d4b\u5f97\u5230\u7684\u4f30\u8ba1\u3002 \u5c06\u8fd0\u52a8\u4e0e\u89c2\u6d4b\u5f97\u5230\u7684\u4f30\u8ba1\u4f7f\u7528element-wise multiplication\u878d\u5408","title":"\u6ee4\u6ce2\u6a21\u5757"},{"location":"Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/#planner","text":"\u4f7f\u7528Value-iteration\u5c42\u8ba1\u7b97\u51fa\u4e00\u4e2aN*N\u7684\u4ef7\u503c\u56fe\u3002\u8fd9\u4e2a\u5c42\u7684\u7b97\u6cd5\u662f 1. \u5c06\u521d\u59cb \\theta \u7528\u4e00\u4e2a\u5377\u79ef\u5c42\u6620\u5c04\u5230Reward\u56fe( N*N*A )\uff0c\u521d\u59cb\u5316Q\u51fd\u6570\u56fe 2. \u5bf9Q\u51fd\u6570\u56fe\u7684Action\u7ef4\u4f5cmax pool (1*1) \u5f97\u5230 V_k 3. \u5377\u79ef V_k \u4ee3\u8868state transition\u518d\u52a0\u4e0areward\u56fe\u5f97\u5230\u65b0\u7684Q\u51fd\u6570\u56fe 4. \u8fed\u4ee32\u30013\u591a\u6b21\uff0c\u5f97\u5230\u6700\u7ec8\u7684Q\u51fd\u6570 \u51b3\u7b56\u7684\u65f6\u5019\uff0c\u5c06Q\u51fd\u6570 (N*N*A) \u4e0e b_t (N*N) \u76f8\u4e58\uff0c\u5e76\u6c42\u548c\u5f97\u5230 q(a) (\u5f62\u72b6\u4e3aA\u7684\u5f20\u91cf) \u518d\u7528\u5168\u8fde\u63a5\u5c06\u8fd9\u4e2aq\u6620\u5c04\u5230action\u4e2d\u3002","title":"planner \u6a21\u5757"},{"location":"Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/#_3","text":"\u6574\u4e2a\u8fd0\u7b97\u662f\u53ef\u5bfc\uff0c\u56e0\u800c\u53ef\u4ee5\u4f7f\u7528\u6a21\u4eff\u5b66\u4e60\u540c\u65f6\u8bad\u7ec3\u6ee4\u6ce2\u6a21\u5757\u4ee5\u53ca\u89c4\u5212\u6a21\u5757\u3002","title":"\u6a21\u4eff\u8bad\u7ec3\u4e0e\u4f7f\u7528"},{"location":"Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/#_4","text":"\u53efgeneralize\u5230\u65b0\u73af\u5883 \u7528soft index\u4ee5\u53ca\u53ef\u5b66\u4e60\u7684transition\u6709\u6548\u7684\u89e3\u51b3\u4e86\u4e0d\u786e\u5b9a\u6027\u95ee\u9898 QMDP-net\u5b66\u4e60\u5230\u7684\u6a21\u578b\u4e0d\u6b63\u786e\uff0c\u4f46\u662f\u6709\u7528 \u5728N\u6bd4\u8f83\u5c0f\u7684\u73af\u5883\u4e2d\u5f97\u5230\u7684\u7f51\u7edc\u53ef\u4ee5\u5feb\u901f\u5b66\u4e60\u5230N\u6bd4\u8f83\u5927\u7684\u73af\u5883 \u53ef\u4ee5\u7528CNN-LSTM\u7ed3\u6784\u66ff\u4ee3\u672c\u6587\u7684filter \u6a21\u5757\uff0c\u4f46\u662f\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u672c\u6587\u7684\u6548\u679c\u66f4\u597d(\u66f4\u591aregularization)","title":"\u5b9e\u9a8c\u76f4\u89c9"},{"location":"Planning_Control_DL/SUNRISE_for_Ensemble_RL/","text":"SUNRISE: A Simple Unified Framework for Ensemble Learning in Deep Reinforcement Learning \u8fd9\u7bc7\u8bba\u6587\u662f\u57fa\u4e8eSAC\u7684\u4e00\u4e2a\u6269\u5c55\u7b97\u6cd5\uff0c\u6709\u6bd4\u8f83\u597d\u7684\u4e00\u4e2a \u77e5\u4e4e\u6587\u7ae0\u4ecb\u7ecd . Preview - SAC Soft-Actor-Critic (SAC)\u662f\u4e00\u4e2aSOTA\u7684 model-free \u8fde\u7eedRL\u7b97\u6cd5. \u77e5\u4e4e\u6587\u7ae0\u4ecb\u7ecd \u5173\u952e\u7684\u4e00\u4e9b\u7279\u5f81: Off-policy, \u4f7f\u7528replay buffer. actor \u7f51\u7edc\u8f93\u51famean, variance\u3002 \u91cd\u6574\u5316\u91c7\u6837\u540e\uff0c\u5728\u6536\u96c6replay buffer\u7684\u8fc7\u7a0b\u4e2d\u8f93\u51fa\u7684\u662f\u968f\u673a\u7684\u7b56\u7565\u3002 \u5728\u6700\u5927\u5316\u5956\u52b1\u7684\u540c\u65f6\u6700\u5927\u5316\u7b56\u7565\u7684\u71b5, Q function\u7684\u8bad\u7ec3\u635f\u5931\u4e3a: \\mathcal{L}_{Q}\\left(\\tau_{t}, \\theta\\right)=\\left(Q_{\\theta}\\left(s_{t}, a_{t}\\right)-r_{t}-\\gamma \\mathbb{E}_{a_{t+1} \\sim \\pi_{\\phi}}\\left[Q_{\\bar{\\theta}}\\left(s_{t+1}, a_{t+1}\\right)-\\alpha \\log \\pi_{\\phi}\\left(a_{t+1} \\mid s_{t+1}\\right)\\right]\\right)^{2} Actor \u7684\u8bad\u7ec3\u76ee\u6807\u4e3a: \\mathcal{L}_{\\text {actor }}^{\\text {SAC }}(\\phi)=\\mathbb{E}_{s_{t} \\sim \\mathcal{B}}\\left[\\mathcal{L}_{\\pi}\\left(s_{t}, \\phi\\right)\\right], \\text { where } \\mathcal{L}_{\\pi}\\left(s_{t}, \\phi\\right)=\\mathbb{E}_{a_{t} \\sim \\pi_{\\phi}}\\left[\\alpha \\log \\pi_{\\phi}\\left(a_{t} \\mid s_{t}\\right)-Q_{\\theta}\\left(s_{t}, a_{t}\\right)\\right] double-Q trick. \u4f1a\u8bad\u7ec3\u4e24\u4e2aQ\u7f51\u7edc\uff0cactor\u7684\u8bad\u7ec3\u76ee\u6807\u4e2d Q \u53d6\u4e24\u4e2aQ\u7f51\u7edc\u4e2d\u7684\u6700\u5c0f\u503c\u3002 Sunrise \u5728SAC\u7684\u57fa\u7840\u4e0a\u878d\u5408\u4e86\u4e09\u4e2a\u5173\u952e\u70b9. \u968f\u673a\u521d\u59cb\u5316,\u540c\u65f6\u8bad\u7ec3 N \u4e2aSAC agent( N \u4e2aactor \u548c critics). \u4e14\u5728\u91c7\u96c6replay buffer\u7684\u65f6\u5019\u4f1a\u968f\u673a\u751f\u6210\u4e00\u4e2a N \u7ef4\u7684binary mask, \u5728\u8bad\u7ec3\u8fd9\u4e2a\u6837\u672c\u7684\u65f6\u5019\uff0c\u6839\u636ebinary mask\u9009\u62e9\u4e0d\u540c\u7684agent\u6765\u8bad\u7ec3\uff0c\u8fd9\u4e2a\u6280\u5de7\u53ef\u4ee5\u4fdd\u8bc1\u4e0d\u540cagent\u7684diversity (\u6ce8\u610f\u5728\u51b3\u5b9a\u91c7\u96c6\u7684action\u7684\u65f6\u5019\u4f1a\u7efc\u5408\u8003\u8651\u5168\u90e8\u7684agent\u7684\u8f93\u51fa\uff0c binary mask\u53ea\u5f71\u54cd\u8fd9\u4e2a\u6837\u672c\u7528\u4e8e\u8bad\u7ec3\u7684\u65f6\u5019\u7684\u6548\u679c). \u5e26\u6709\u6743\u91cd\u7684Bellman\u66f4\u65b0\u3002 \u5728\u8bad\u7ec3Q\u51fd\u6570\u7684\u65f6\u5019\uff0c\u5bf9\u4e8e\u6bcf\u4e00\u4e2a\"target Q\"\u6837\u672c\u8f93\u5165 (s_{t+1}, a_{t+1}) , \u8ba1\u7b97\u4e00\u4e2a\u6743\u91cd w(s, a) = \\text{sigmoid}(-\\bar Q_{\\text{std}}(s, a) * T) + 0.5 . \u90a3\u4e48\u7528\u4e8e\u8bad\u7ec3critics\u7684\u635f\u5931\u51fd\u6570\u90fd\u4e58\u4e0a\u8fd9\u4e2a\u6743\u91cd L_{WQ}(\\tau_t, \\theta_i) = w(s_{t+1}, a_{t+1}) L_Q(\\tau_t, \\theta_i) . Upper Confidence Bound(UCB, \u7f6e\u4fe1\u4e0a\u9650) \u63a2\u7d22\u3002\u5728\u63a2\u7d22\u5e76\u6536\u96c6replay buffer\u7684\u65f6\u5019\uff0c\u5bfb\u627e a_t = \\underset{a}{\\text{max}}\\{Q_{\\text{mean}(s_t, a)} + \\lambda Q_{\\text{std}} (s_t, a)\\} . \u8fd9\u4e2a\u6700\u5927\u5316\u7684\u4e25\u8c28\u89e3\u662f\u5f88\u96be\u6c42\u7684\uff0c\u672c\u6587\u7684\u8fd1\u4f3c\u505a\u6cd5\u662f\u8ba9 N \u4e2aactor\u751f\u6210 N \u4e2aaction \u5019\u9009\uff0c\u7136\u540e\u5206\u522b\u5728 N \u4e2acritics\u4e0a\u8ba1\u7b97\u8fd9\u4e2a N \u4e2a action\u7684 Q \u7684\u5747\u503c\u4e0e\u65b9\u5dee\u3002","title":"SUNRISE: A Simple Unified Framework for Ensemble Learning in Deep Reinforcement Learning"},{"location":"Planning_Control_DL/SUNRISE_for_Ensemble_RL/#sunrise-a-simple-unified-framework-for-ensemble-learning-in-deep-reinforcement-learning","text":"\u8fd9\u7bc7\u8bba\u6587\u662f\u57fa\u4e8eSAC\u7684\u4e00\u4e2a\u6269\u5c55\u7b97\u6cd5\uff0c\u6709\u6bd4\u8f83\u597d\u7684\u4e00\u4e2a \u77e5\u4e4e\u6587\u7ae0\u4ecb\u7ecd .","title":"SUNRISE: A Simple Unified Framework for Ensemble Learning in Deep Reinforcement Learning"},{"location":"Planning_Control_DL/SUNRISE_for_Ensemble_RL/#preview-sac","text":"Soft-Actor-Critic (SAC)\u662f\u4e00\u4e2aSOTA\u7684 model-free \u8fde\u7eedRL\u7b97\u6cd5. \u77e5\u4e4e\u6587\u7ae0\u4ecb\u7ecd \u5173\u952e\u7684\u4e00\u4e9b\u7279\u5f81: Off-policy, \u4f7f\u7528replay buffer. actor \u7f51\u7edc\u8f93\u51famean, variance\u3002 \u91cd\u6574\u5316\u91c7\u6837\u540e\uff0c\u5728\u6536\u96c6replay buffer\u7684\u8fc7\u7a0b\u4e2d\u8f93\u51fa\u7684\u662f\u968f\u673a\u7684\u7b56\u7565\u3002 \u5728\u6700\u5927\u5316\u5956\u52b1\u7684\u540c\u65f6\u6700\u5927\u5316\u7b56\u7565\u7684\u71b5, Q function\u7684\u8bad\u7ec3\u635f\u5931\u4e3a: \\mathcal{L}_{Q}\\left(\\tau_{t}, \\theta\\right)=\\left(Q_{\\theta}\\left(s_{t}, a_{t}\\right)-r_{t}-\\gamma \\mathbb{E}_{a_{t+1} \\sim \\pi_{\\phi}}\\left[Q_{\\bar{\\theta}}\\left(s_{t+1}, a_{t+1}\\right)-\\alpha \\log \\pi_{\\phi}\\left(a_{t+1} \\mid s_{t+1}\\right)\\right]\\right)^{2} Actor \u7684\u8bad\u7ec3\u76ee\u6807\u4e3a: \\mathcal{L}_{\\text {actor }}^{\\text {SAC }}(\\phi)=\\mathbb{E}_{s_{t} \\sim \\mathcal{B}}\\left[\\mathcal{L}_{\\pi}\\left(s_{t}, \\phi\\right)\\right], \\text { where } \\mathcal{L}_{\\pi}\\left(s_{t}, \\phi\\right)=\\mathbb{E}_{a_{t} \\sim \\pi_{\\phi}}\\left[\\alpha \\log \\pi_{\\phi}\\left(a_{t} \\mid s_{t}\\right)-Q_{\\theta}\\left(s_{t}, a_{t}\\right)\\right] double-Q trick. \u4f1a\u8bad\u7ec3\u4e24\u4e2aQ\u7f51\u7edc\uff0cactor\u7684\u8bad\u7ec3\u76ee\u6807\u4e2d Q \u53d6\u4e24\u4e2aQ\u7f51\u7edc\u4e2d\u7684\u6700\u5c0f\u503c\u3002","title":"Preview - SAC"},{"location":"Planning_Control_DL/SUNRISE_for_Ensemble_RL/#sunrise","text":"\u5728SAC\u7684\u57fa\u7840\u4e0a\u878d\u5408\u4e86\u4e09\u4e2a\u5173\u952e\u70b9. \u968f\u673a\u521d\u59cb\u5316,\u540c\u65f6\u8bad\u7ec3 N \u4e2aSAC agent( N \u4e2aactor \u548c critics). \u4e14\u5728\u91c7\u96c6replay buffer\u7684\u65f6\u5019\u4f1a\u968f\u673a\u751f\u6210\u4e00\u4e2a N \u7ef4\u7684binary mask, \u5728\u8bad\u7ec3\u8fd9\u4e2a\u6837\u672c\u7684\u65f6\u5019\uff0c\u6839\u636ebinary mask\u9009\u62e9\u4e0d\u540c\u7684agent\u6765\u8bad\u7ec3\uff0c\u8fd9\u4e2a\u6280\u5de7\u53ef\u4ee5\u4fdd\u8bc1\u4e0d\u540cagent\u7684diversity (\u6ce8\u610f\u5728\u51b3\u5b9a\u91c7\u96c6\u7684action\u7684\u65f6\u5019\u4f1a\u7efc\u5408\u8003\u8651\u5168\u90e8\u7684agent\u7684\u8f93\u51fa\uff0c binary mask\u53ea\u5f71\u54cd\u8fd9\u4e2a\u6837\u672c\u7528\u4e8e\u8bad\u7ec3\u7684\u65f6\u5019\u7684\u6548\u679c). \u5e26\u6709\u6743\u91cd\u7684Bellman\u66f4\u65b0\u3002 \u5728\u8bad\u7ec3Q\u51fd\u6570\u7684\u65f6\u5019\uff0c\u5bf9\u4e8e\u6bcf\u4e00\u4e2a\"target Q\"\u6837\u672c\u8f93\u5165 (s_{t+1}, a_{t+1}) , \u8ba1\u7b97\u4e00\u4e2a\u6743\u91cd w(s, a) = \\text{sigmoid}(-\\bar Q_{\\text{std}}(s, a) * T) + 0.5 . \u90a3\u4e48\u7528\u4e8e\u8bad\u7ec3critics\u7684\u635f\u5931\u51fd\u6570\u90fd\u4e58\u4e0a\u8fd9\u4e2a\u6743\u91cd L_{WQ}(\\tau_t, \\theta_i) = w(s_{t+1}, a_{t+1}) L_Q(\\tau_t, \\theta_i) . Upper Confidence Bound(UCB, \u7f6e\u4fe1\u4e0a\u9650) \u63a2\u7d22\u3002\u5728\u63a2\u7d22\u5e76\u6536\u96c6replay buffer\u7684\u65f6\u5019\uff0c\u5bfb\u627e a_t = \\underset{a}{\\text{max}}\\{Q_{\\text{mean}(s_t, a)} + \\lambda Q_{\\text{std}} (s_t, a)\\} . \u8fd9\u4e2a\u6700\u5927\u5316\u7684\u4e25\u8c28\u89e3\u662f\u5f88\u96be\u6c42\u7684\uff0c\u672c\u6587\u7684\u8fd1\u4f3c\u505a\u6cd5\u662f\u8ba9 N \u4e2aactor\u751f\u6210 N \u4e2aaction \u5019\u9009\uff0c\u7136\u540e\u5206\u522b\u5728 N \u4e2acritics\u4e0a\u8ba1\u7b97\u8fd9\u4e2a N \u4e2a action\u7684 Q \u7684\u5747\u503c\u4e0e\u65b9\u5dee\u3002","title":"Sunrise"},{"location":"Planning_Control_DL/Universal%20Planning%20Networks/","text":"Universal Planning Networks \u8fd9\u7bc7\u8bba\u6587\u7ed9\u51fa\u4e86\u4e00\u4e2a\u901a\u7528\u7684\u7528\u4e8eplanning\u7684\u7f51\u7edc\u7ed3\u6784\uff0c\u76ee\u7684\u662f\u8f93\u51fa\u4e00\u7cfb\u5217\u7684state\u4ee5\u63a5\u8fd1goal state\u3002\u5b83\u4e00\u65b9\u9762\u53ef\u4ee5\u7528\u4e8eplanning\uff0c\u4e5f\u53ef\u4ee5\u6839\u636e\u5b66\u4e60\u5230\u7684\u53c2\u6570\u7ed9\u51fa\u5f3a\u5316\u5b66\u4e60\u4f7f\u7528\u7684\u6210\u672c\u51fd\u6570 \u4e3b\u8981\u8d21\u732e 1. \u7f51\u7edc\u7ed3\u6784\u4e0e\u7b97\u6cd5 \u7ed9\u51fa\u5f53\u524d\u89c2\u6d4b o_t \u4ee5\u53ca\u76ee\u6807\u89c2\u6d4b o_g \u4f5c\u4e3a\u8f93\u5165\u56fe\u50cf\uff0c \\hat a_t \u4e3a t \u65f6\u523b\u9884\u6d4b\u7684\u884c\u52a8 \u5728GDP(Gradien Descent Planner)\u4e2d\uff0c\u7528\u53ef\u5b66\u4e60\u7684\u7f16\u7801\u5668 f_\\phi \u5c06\u89c2\u6d4b o_t \u8f6c\u6362\u4e3alatent space x_t ,\u518d\u7528\u53ef\u5b66\u4e60\u7684\u6a21\u578b\u8f6c\u6362\u53c2\u6570 \\hat x_{t+1} = g_\\theta(x_t, a_t) \u5b66\u4e60 n_p \u6b65\u3002\u5176\u4e2d\u7684action a_t \u4e3a\u968f\u673a\u521d\u59cb\u5316\uff0c\u5176\u540e\u4f7f\u7528\u68af\u5ea6\u4e0b\u964d\u8fdb\u884c\u4f18\u5316\u66f4\u65b0\u3002 \u4e2a\u4eba\u7406\u89e3\uff0c\u5176\u5b9estate transition\u7ed3\u6784\u53ef\u4ee5\u4f7f\u7528res\u6b8b\u5dee\u8fde\u63a5\u66f4\u79d1\u5b66\u3002 \u89c4\u5212\u8fc7\u7a0b\u4e2d\u7684\u635f\u5931\u51fd\u6570 L_{plan}^{(i)} \u4e3a\u6700\u7ec8\u9884\u6d4b\u7684 x_{t+T+1} \u4e0e x_g \u7684\u8ddd\u79bb\uff08\u53ef\u5bfc\uff09\u3002 \u5bf9planner\u7684\u8bad\u7ec3\u53ef\u4ee5\u7528Imitation\uff0c\u56e0\u4e3aGDP\u7ed9\u51faaction\u7684\u7b97\u6cd5\u5c3d\u7ba1\u6709\u68af\u5ea6\u7684\u4f7f\u7528\uff0c\u4f46\u662f\u662f\u5b8c\u5168\u53ef\u5bfc\u7684\u3002\u5728\u8fd9\u4e2a\u57fa\u7840\u4e0a\u7ed9\u51fa\u76f8\u5bf9\u53c2\u8003\u884c\u52a8\u7684\u635f\u5931\u51fd\u6570 L_{imitate} = ||\\hat a_{t:T} - a^{*}_{t:t+T}||_2^2 ,\u7528\u68af\u5ea6\u66f4\u65b0\u7f16\u7801\u5668\u4ee5\u53ca\u8fc7\u7a0b\u7f51\u7edc\u7684\u53c2\u6570\u3002 \u672c\u6587\u8fdb\u4e00\u6b65\u4ecb\u7ecd\u4e86\u4f7f\u7528\u8fd9\u4e2a\u5b66\u4e60\u5230\u7684\u7f16\u7801\u5668\u4e0e\u7cfb\u7edf\u6a21\u578b\u6765\u8f85\u52a9\u5f3a\u5316\u5b66\u4e60\u7684\u8fd0\u52a8\u751f\u6210\u3002 \u4e2a\u4eba\u7406\u89e3\uff0c\u8fd9\u4e2a\u6587\u7ae0\u7684\u4ee3\u7801\u96be\u5ea6\u5728\u4e8e\u6a21\u4eff\u5b66\u4e60\u7684\u65f6\u5019\uff0c\u4e00\u65b9\u9762\u9700\u8981\u7528\u68af\u5ea6\u66f4\u65b0\u8fd0\u52a8\uff0c\u53e6\u4e00\u65b9\u9762\u8981\u8fdb\u4e00\u6b65\u7684\u7528\u68af\u5ea6\u66f4\u65b0\u7cfb\u7edf\u53c2\u6570\u3002pytorch\u4ee5\u53caTensorflow\u9ed8\u8ba4\u60c5\u51b5\u4e0b\u53ea\u4f1a\u8bb0\u4f4f\u4e00\u6b21\u8fd0\u7b97\u8fc7\u7a0b\u4e2d\u7684\u68af\u5ea6\u4fe1\u606f\uff0c\u7136\u540e\u5728\u68af\u5ea6\u66f4\u65b0\u7684\u65f6\u5019\u9700\u8981\u5220\u9664\u6b64\u524d\u8bb0\u5f55\u7684\u6240\u6709\u8fd0\u7b97\u56fe\u3002\u4f46\u662f\u8fd9\u91cc\u6a21\u4eff\u7684\u65f6\u5019\u4f3c\u4e4e\u9700\u8981\u6211\u4eec\u8bb0\u4f4f\u68af\u5ea6\u66f4\u65b0\u7684\u65f6\u5019\u4f7f\u7528\u7684\u6574\u4e2a\u8fd0\u7b97\u56fe(\u4e5f\u53ef\u80fd\u53ea\u9700\u8981\u6700\u540e\u4e00\u6b65\uff0c\u5e76\u4fdd\u7559\u6700\u540e\u4e00\u6b65\u7684\u68af\u5ea6)","title":"Universal Planning Networks"},{"location":"Planning_Control_DL/Universal%20Planning%20Networks/#universal-planning-networks","text":"\u8fd9\u7bc7\u8bba\u6587\u7ed9\u51fa\u4e86\u4e00\u4e2a\u901a\u7528\u7684\u7528\u4e8eplanning\u7684\u7f51\u7edc\u7ed3\u6784\uff0c\u76ee\u7684\u662f\u8f93\u51fa\u4e00\u7cfb\u5217\u7684state\u4ee5\u63a5\u8fd1goal state\u3002\u5b83\u4e00\u65b9\u9762\u53ef\u4ee5\u7528\u4e8eplanning\uff0c\u4e5f\u53ef\u4ee5\u6839\u636e\u5b66\u4e60\u5230\u7684\u53c2\u6570\u7ed9\u51fa\u5f3a\u5316\u5b66\u4e60\u4f7f\u7528\u7684\u6210\u672c\u51fd\u6570","title":"Universal Planning Networks"},{"location":"Planning_Control_DL/Universal%20Planning%20Networks/#_1","text":"","title":"\u4e3b\u8981\u8d21\u732e"},{"location":"Planning_Control_DL/Universal%20Planning%20Networks/#1","text":"\u7ed9\u51fa\u5f53\u524d\u89c2\u6d4b o_t \u4ee5\u53ca\u76ee\u6807\u89c2\u6d4b o_g \u4f5c\u4e3a\u8f93\u5165\u56fe\u50cf\uff0c \\hat a_t \u4e3a t \u65f6\u523b\u9884\u6d4b\u7684\u884c\u52a8 \u5728GDP(Gradien Descent Planner)\u4e2d\uff0c\u7528\u53ef\u5b66\u4e60\u7684\u7f16\u7801\u5668 f_\\phi \u5c06\u89c2\u6d4b o_t \u8f6c\u6362\u4e3alatent space x_t ,\u518d\u7528\u53ef\u5b66\u4e60\u7684\u6a21\u578b\u8f6c\u6362\u53c2\u6570 \\hat x_{t+1} = g_\\theta(x_t, a_t) \u5b66\u4e60 n_p \u6b65\u3002\u5176\u4e2d\u7684action a_t \u4e3a\u968f\u673a\u521d\u59cb\u5316\uff0c\u5176\u540e\u4f7f\u7528\u68af\u5ea6\u4e0b\u964d\u8fdb\u884c\u4f18\u5316\u66f4\u65b0\u3002 \u4e2a\u4eba\u7406\u89e3\uff0c\u5176\u5b9estate transition\u7ed3\u6784\u53ef\u4ee5\u4f7f\u7528res\u6b8b\u5dee\u8fde\u63a5\u66f4\u79d1\u5b66\u3002 \u89c4\u5212\u8fc7\u7a0b\u4e2d\u7684\u635f\u5931\u51fd\u6570 L_{plan}^{(i)} \u4e3a\u6700\u7ec8\u9884\u6d4b\u7684 x_{t+T+1} \u4e0e x_g \u7684\u8ddd\u79bb\uff08\u53ef\u5bfc\uff09\u3002 \u5bf9planner\u7684\u8bad\u7ec3\u53ef\u4ee5\u7528Imitation\uff0c\u56e0\u4e3aGDP\u7ed9\u51faaction\u7684\u7b97\u6cd5\u5c3d\u7ba1\u6709\u68af\u5ea6\u7684\u4f7f\u7528\uff0c\u4f46\u662f\u662f\u5b8c\u5168\u53ef\u5bfc\u7684\u3002\u5728\u8fd9\u4e2a\u57fa\u7840\u4e0a\u7ed9\u51fa\u76f8\u5bf9\u53c2\u8003\u884c\u52a8\u7684\u635f\u5931\u51fd\u6570 L_{imitate} = ||\\hat a_{t:T} - a^{*}_{t:t+T}||_2^2 ,\u7528\u68af\u5ea6\u66f4\u65b0\u7f16\u7801\u5668\u4ee5\u53ca\u8fc7\u7a0b\u7f51\u7edc\u7684\u53c2\u6570\u3002 \u672c\u6587\u8fdb\u4e00\u6b65\u4ecb\u7ecd\u4e86\u4f7f\u7528\u8fd9\u4e2a\u5b66\u4e60\u5230\u7684\u7f16\u7801\u5668\u4e0e\u7cfb\u7edf\u6a21\u578b\u6765\u8f85\u52a9\u5f3a\u5316\u5b66\u4e60\u7684\u8fd0\u52a8\u751f\u6210\u3002 \u4e2a\u4eba\u7406\u89e3\uff0c\u8fd9\u4e2a\u6587\u7ae0\u7684\u4ee3\u7801\u96be\u5ea6\u5728\u4e8e\u6a21\u4eff\u5b66\u4e60\u7684\u65f6\u5019\uff0c\u4e00\u65b9\u9762\u9700\u8981\u7528\u68af\u5ea6\u66f4\u65b0\u8fd0\u52a8\uff0c\u53e6\u4e00\u65b9\u9762\u8981\u8fdb\u4e00\u6b65\u7684\u7528\u68af\u5ea6\u66f4\u65b0\u7cfb\u7edf\u53c2\u6570\u3002pytorch\u4ee5\u53caTensorflow\u9ed8\u8ba4\u60c5\u51b5\u4e0b\u53ea\u4f1a\u8bb0\u4f4f\u4e00\u6b21\u8fd0\u7b97\u8fc7\u7a0b\u4e2d\u7684\u68af\u5ea6\u4fe1\u606f\uff0c\u7136\u540e\u5728\u68af\u5ea6\u66f4\u65b0\u7684\u65f6\u5019\u9700\u8981\u5220\u9664\u6b64\u524d\u8bb0\u5f55\u7684\u6240\u6709\u8fd0\u7b97\u56fe\u3002\u4f46\u662f\u8fd9\u91cc\u6a21\u4eff\u7684\u65f6\u5019\u4f3c\u4e4e\u9700\u8981\u6211\u4eec\u8bb0\u4f4f\u68af\u5ea6\u66f4\u65b0\u7684\u65f6\u5019\u4f7f\u7528\u7684\u6574\u4e2a\u8fd0\u7b97\u56fe(\u4e5f\u53ef\u80fd\u53ea\u9700\u8981\u6700\u540e\u4e00\u6b65\uff0c\u5e76\u4fdd\u7559\u6700\u540e\u4e00\u6b65\u7684\u68af\u5ea6)","title":"1. \u7f51\u7edc\u7ed3\u6784\u4e0e\u7b97\u6cd5"},{"location":"Planning_Control_DL/diff_MPC_ARE/","text":"Infinite-Horizon Differentiable Model Predictive Control \u8fd9\u7bc7paper\u4e0e OptNet \u6709\u5f88\u5f3a\u7684\u5173\u8054\u6027. OptNet \u5229\u7528KKT condition\u5bf9QP\u95ee\u9898\u7684\u63a8\u7406\u7ed3\u679c\u8fdb\u884c\u53cd\u5411\u6c42\u5bfc\uff0c\u4f7f\u5f97\u4e8c\u6b21\u89c4\u5212\u4f18\u5316\u95ee\u9898\u7684backward\u9636\u6bb5\u4e0d\u9700\u8981\u88abforward\u9636\u6bb5\u7684\u8fed\u4ee3\u6240\u5f71\u54cd\u3002\u6548\u7387\u663e\u8457\u63d0\u9ad8\u3002 \u8fd9\u7bc7paper\u5219\u4f7f\u7528\u4e86\u63a7\u5236\u9886\u57df\u7684\u4e00\u4e2a\u5e38\u89c1\u7684trick\uff0c\u5728\u4f7f\u7528\u975e\u7ebf\u6027\u4f18\u5316\u5668/\u590d\u6742\u7b97\u6cd5\u65f6\uff0c\u5148\u4f7f\u7528\u4e00\u4e2a\u53ef\u9760\u7684\u63a7\u5236\u5668\u4f7f\u539f\u6765\u7684\u7cfb\u7edf\u57fa\u672c\u7a33\u5b9a\uff0c\u518d\u8ba9\u989d\u5916\u6dfb\u52a0\u7684\u4f18\u5316\u5668\u5bf9\u6b8b\u5dee\u7ed3\u679c\u8fdb\u884c\u56de\u5f52\u3002 \u672c\u6587\u4f7f\u7528\u7684\u9884\u7a33\u5b9a\u63a7\u5236\u5668\u662f\u4e00\u4e2aLQR\u63a7\u5236\u5668\u3002\u800cLQR\u4e2d\u5b58\u5728\u6709Q\uff0cR\u4e24\u4e2a\u4ee3\u8868\u5404\u4e2astate\u4e0einput\u7684\u635f\u5931\u8d85\u53c2\uff0c\u4f5c\u8005\u7684\u4e00\u4e2a\u4e3b\u8981\u8d21\u732e\u5c31\u662f\u8ba9\u8fd9\u4e24\u4e2a\u635f\u5931\u8d85\u53c2\u4e5f\u53d8\u6210\u53ef\u4ee5\u88ab\u5b66\u4e60\u7684\u53c2\u6570\uff0c\u7ed9\u51fa\u4e86\u5bf9Riccati\u65b9\u7a0b\u7684\u5bfc\u6570\u8fdb\u884c\u4f20\u64ad\u7684\u516c\u5f0f\u3002 \u6587\u7ae0\u7684\u7b97\u6cd5\u6846\u67b6\u5982\u56fe: MPC Review \u5bf9\u4e8e\u4e00\u4e2a\u7ebf\u6027\u7cfb\u7edf\uff0c\u6b65\u957f\u4e3a N \u7684MPC\u63a7\u5236\uff0c\u53ef\u4ee5\u5212\u5f52\u4e3a\u4ee5\u4e0b\u5e26\u6709\u677e\u5f1b\u53d8\u91cf(slack variables)\u7684\u4e8c\u6b21\u4f18\u5316\u95ee\u9898.\u4f7f\u5f97\u95ee\u9898\u603b\u662f\u6709\u89e3\u7684\u3002 \\begin{aligned} \\hat{u}_{0: N}^{\\star}=\\underset{\\hat{u}}{\\operatorname{argmin}} &\\frac{1}{2} \\sum_{k=0}^{N-1} \\hat{u}_{k}^{\\top} R \\hat{u}_{k}+\\frac{1}{2} \\sum_{k=1}^{N-1} \\hat{x}_{k}^{\\top} Q \\hat{x}_{k}+\\frac{1}{2} \\hat{x}_{N}^{\\top} Q_{N} \\hat{x}_{N}+k_{u} \\sum_{k=0}^{N-1} \\mathbf{1}_{m}^{\\top} r_{k}+k_{x} \\sum_{k=1}^{N} \\mathbf{1}_{n}^{\\top} s_{k} \\\\ \\text { s.t. } &\\hat{x}_{0}=x_{t} \\\\ &\\hat{x}_{k+1}=A \\hat{x}_{k}+B \\hat{u}_{k}, \\quad k \\in\\{0, \\ldots, N-1\\} \\\\ &\\underline{u}-r_{k} \\leq \\hat{u}_{k} \\leq \\bar{u}+r_{k} \\quad \\text { and } \\quad r_{k} \\geq 0, \\quad k \\in\\{0, \\cdots N-1\\} \\\\ &\\underline{x}-s_{k} \\leq \\hat{x}_{k} \\leq \\bar{x}+s_{k} \\quad \\text { and } \\quad s_{k} \\geq 0, \\quad k \\in\\{1, \\ldots, N\\} \\end{aligned} \u5982\u679c\u5c06\u8f93\u5165\u63a7\u5236\u53d8\u91cf\u5206\u89e3\u4e3a u_t = Kx_t + \\delta u_t ,\u5176\u4e2d K \u4e3a\u7ebf\u6027\u72b6\u6001\u53cd\u9988\u77e9\u9635\u3002\u4e0a\u8ff0\u95ee\u9898\u53d8\u4e3a \\begin{aligned} \\delta \\hat{u}_{0: N}^{\\star}=\\underset{\\delta \\hat{u}}{\\operatorname{argmin}} & \\frac{1}{2} \\sum_{k=0}^{N-1}\\left(K \\hat{x}_{k}+\\delta \\hat{u}_{k}\\right)^{\\top} R\\left(K \\hat{x}_{k}+\\delta \\hat{u}_{k}\\right)+\\frac{1}{2} \\sum_{k=1}^{N-1} \\hat{x}_{k}^{\\top} Q \\hat{x}_{k}+\\frac{1}{2} \\hat{x}_{N}^{\\top} Q_{N} \\hat{x}_{N} \\\\ &+k_{u} \\sum_{k=0}^{N-1} \\mathbf{1}_{m}^{\\top} r_{k}+k_{x} \\sum_{k=1}^{N} \\mathbf{1}_{n}^{\\top} s_{k} \\\\ \\text { s.t. } & \\hat{x}_{0}=x_{t} \\\\ &\\hat{x}_{k+1}=(A+B K) \\hat{x}_{k}+B \\delta \\hat{u}_{k}, \\quad k \\in\\{0, \\ldots, N-1\\} \\\\ & \\underline{u}-r_{k} \\leq K \\hat{x}_{k}+\\delta \\hat{u}_{k} \\leq \\bar{u}+r_{k} \\quad \\text { and } \\quad r_{k} \\geq 0, \\quad k \\in\\{0, \\ldots, N-1\\} \\\\ &\\underline{x}-s_{k} \\leq \\hat{x}_{k} \\leq \\bar{x}+s_{k} \\text { and } \\quad s_{k} \\geq 0, \\quad k \\in\\{1, \\ldots, N\\} \\end{aligned} \u5c06\u5b83\u4eec\u91cd\u6574\u4e3aQP\u95ee\u9898\u540e\uff0c\u53ef\u4ee5\u4f7f\u7528 OptNet \u7684\u6a21\u5757\u5f97\u5230\u4f18\u5316\u7ed3\u679c\u4ee5\u53ca\u5bf9 Q, R, A, B, \\underline{u}, \\bar{u}, \\underline{x}, \\bar{x}, k_{x} \u548c k_{u} \u7684\u5bfc\u6570\u3002 \u6a21\u4eff\u5b66\u4e60\u5219\u53ef\u4ee5\u6839\u636e\u4f18\u5316\u7ed3\u679c\u4ee5\u53ca\u4e13\u5bb6\u6807\u6ce8\u7ed9\u51fa\u635f\u5931\u3002 Infinite Horizon Terminal Cost LQR\u6700\u4f18\u63a7\u5236\u5668\u5f97\u5230\u7684\u63a7\u5236\u7ed3\u679c\u662f\u9488\u5bf9\u65e0\u9650\u9884\u6d4b\u6b65\u957f\u7684LTI\u7cfb\u7edfMPC\u95ee\u9898\u3002\u5176 K \u4e3a K=-\\left(R+B^{\\top} P B\\right)^{-1} B^{\\top} P A \u5176\u4e2d P \u4e3a\u79bb\u6563Riccati\u65b9\u7a0b\u7684\u89e3: P=A^{\\top} P A-A^{\\top} P B\\left(R+B^{\\top} P B\\right)^{-1} B^{\\top} P A+Q \u6ce8\u610f P \u7684 \u6570\u5b66\u610f\u4e49 ,Final cost J = x_0^TPx_0 .\u672c\u6587\u63d0\u51fa\u524d\u6587\u7684MPC\u4e2d\uff0c\u7528\u4e8e\u8868\u8fbe\u7a33\u6001cost\u7684\u77e9\u9635 Q_N = P . \u5bf9\u4e8e\u9009\u62e9 Q_N=P \u7684MPC\uff0c\u4f5c\u8005\u8bc1\u660e\u4e86\u4e00\u7cfb\u5217\u597d\u7684\u6027\u8d28\uff0c\u5305\u62ec\u5b58\u5728\u6709\u9650\u89c6\u754cMPC\u4f7f\u5f97\u65e0\u9650\u89c6\u754c\u6700\u4f18\uff0cMPC\u53ef\u884c\u6027\u3001\u6e10\u8fdb\u7a33\u5b9a\u6027\uff0c\u9c81\u68d2\u6027\u7b49\u3002 \u4f5c\u8005\u63d0\u51fa\u4e86 P \u4e0e A,B,Q,R \u7684\u77e2\u91cf\u5173\u7cfb\u3002 \\frac{\\partial \\operatorname{vec} P}{\\partial \\operatorname{vec} A}=Z_{1}^{-1} Z_{2}, \\quad \\frac{\\partial \\operatorname{vec} P}{\\partial \\operatorname{vec} B}=Z_{1}^{-1} Z_{3}, \\quad \\frac{\\partial \\operatorname{vec} P}{\\partial \\operatorname{vec} Q}=Z_{1}^{-1} Z_{4}, \\quad \\frac{\\partial \\operatorname{vec} P}{\\partial \\operatorname{vec} R}=Z_{1}^{-1} Z_{5} \\begin{aligned} Z_{1}&:=\\mathbf{I}_{n^{2}}-\\left(A^{\\top} \\otimes A^{\\top}\\right)\\left[\\mathbf{I}_{n^{2}}-\\left(P B M_{2} B^{\\top} \\otimes \\mathbf{I}_{n}\\right)-\\left(\\mathbf{I}_{n} \\otimes P B M_{2} B^{\\top}\\right)\\right. \\\\ &\\left.+(P B \\otimes P B)\\left(M_{2} \\otimes M_{2}\\right)\\left(B^{\\top} \\otimes B^{\\top}\\right)\\right] \\\\ Z_{2}&:=\\left(\\mathbf{V}_{n, n}+\\mathbf{I}_{n^{2}}\\right)\\left(\\mathbf{I}_{n} \\otimes A^{\\top} M_{1}\\right) \\\\ Z_{3}&:=\\left(A^{\\top} \\otimes A^{\\top}\\right)\\left[(P B \\otimes P B)\\left(M_{2} \\otimes M_{2}\\right)\\left(\\mathbf{I}_{m^{2}}+\\mathbf{V}_{m, m}\\right)\\left(\\mathbf{I}_{m} \\otimes B^{\\top} P\\right)\\right. \\\\ &\\left.-\\left(\\mathbf{I}_{n^{2}}+\\mathbf{V}_{n, n}\\right)\\left(P B M_{2} \\otimes P\\right)\\right] \\\\ Z_{4}&:=\\mathbf{I}_{n^{2}} \\\\ Z_{5}&:=\\left(A^{\\top} \\otimes A^{\\top}\\right)(P B \\otimes P B)\\left(M_{2} \\otimes M_{2}\\right) \\end{aligned} \\\\ \u5176\u4e2d \\otimes \u4e3a kronecker product","title":"Infinite-Horizon Differentiable Model Predictive Control"},{"location":"Planning_Control_DL/diff_MPC_ARE/#infinite-horizon-differentiable-model-predictive-control","text":"\u8fd9\u7bc7paper\u4e0e OptNet \u6709\u5f88\u5f3a\u7684\u5173\u8054\u6027. OptNet \u5229\u7528KKT condition\u5bf9QP\u95ee\u9898\u7684\u63a8\u7406\u7ed3\u679c\u8fdb\u884c\u53cd\u5411\u6c42\u5bfc\uff0c\u4f7f\u5f97\u4e8c\u6b21\u89c4\u5212\u4f18\u5316\u95ee\u9898\u7684backward\u9636\u6bb5\u4e0d\u9700\u8981\u88abforward\u9636\u6bb5\u7684\u8fed\u4ee3\u6240\u5f71\u54cd\u3002\u6548\u7387\u663e\u8457\u63d0\u9ad8\u3002 \u8fd9\u7bc7paper\u5219\u4f7f\u7528\u4e86\u63a7\u5236\u9886\u57df\u7684\u4e00\u4e2a\u5e38\u89c1\u7684trick\uff0c\u5728\u4f7f\u7528\u975e\u7ebf\u6027\u4f18\u5316\u5668/\u590d\u6742\u7b97\u6cd5\u65f6\uff0c\u5148\u4f7f\u7528\u4e00\u4e2a\u53ef\u9760\u7684\u63a7\u5236\u5668\u4f7f\u539f\u6765\u7684\u7cfb\u7edf\u57fa\u672c\u7a33\u5b9a\uff0c\u518d\u8ba9\u989d\u5916\u6dfb\u52a0\u7684\u4f18\u5316\u5668\u5bf9\u6b8b\u5dee\u7ed3\u679c\u8fdb\u884c\u56de\u5f52\u3002 \u672c\u6587\u4f7f\u7528\u7684\u9884\u7a33\u5b9a\u63a7\u5236\u5668\u662f\u4e00\u4e2aLQR\u63a7\u5236\u5668\u3002\u800cLQR\u4e2d\u5b58\u5728\u6709Q\uff0cR\u4e24\u4e2a\u4ee3\u8868\u5404\u4e2astate\u4e0einput\u7684\u635f\u5931\u8d85\u53c2\uff0c\u4f5c\u8005\u7684\u4e00\u4e2a\u4e3b\u8981\u8d21\u732e\u5c31\u662f\u8ba9\u8fd9\u4e24\u4e2a\u635f\u5931\u8d85\u53c2\u4e5f\u53d8\u6210\u53ef\u4ee5\u88ab\u5b66\u4e60\u7684\u53c2\u6570\uff0c\u7ed9\u51fa\u4e86\u5bf9Riccati\u65b9\u7a0b\u7684\u5bfc\u6570\u8fdb\u884c\u4f20\u64ad\u7684\u516c\u5f0f\u3002 \u6587\u7ae0\u7684\u7b97\u6cd5\u6846\u67b6\u5982\u56fe:","title":"Infinite-Horizon Differentiable Model Predictive Control"},{"location":"Planning_Control_DL/diff_MPC_ARE/#mpc-review","text":"\u5bf9\u4e8e\u4e00\u4e2a\u7ebf\u6027\u7cfb\u7edf\uff0c\u6b65\u957f\u4e3a N \u7684MPC\u63a7\u5236\uff0c\u53ef\u4ee5\u5212\u5f52\u4e3a\u4ee5\u4e0b\u5e26\u6709\u677e\u5f1b\u53d8\u91cf(slack variables)\u7684\u4e8c\u6b21\u4f18\u5316\u95ee\u9898.\u4f7f\u5f97\u95ee\u9898\u603b\u662f\u6709\u89e3\u7684\u3002 \\begin{aligned} \\hat{u}_{0: N}^{\\star}=\\underset{\\hat{u}}{\\operatorname{argmin}} &\\frac{1}{2} \\sum_{k=0}^{N-1} \\hat{u}_{k}^{\\top} R \\hat{u}_{k}+\\frac{1}{2} \\sum_{k=1}^{N-1} \\hat{x}_{k}^{\\top} Q \\hat{x}_{k}+\\frac{1}{2} \\hat{x}_{N}^{\\top} Q_{N} \\hat{x}_{N}+k_{u} \\sum_{k=0}^{N-1} \\mathbf{1}_{m}^{\\top} r_{k}+k_{x} \\sum_{k=1}^{N} \\mathbf{1}_{n}^{\\top} s_{k} \\\\ \\text { s.t. } &\\hat{x}_{0}=x_{t} \\\\ &\\hat{x}_{k+1}=A \\hat{x}_{k}+B \\hat{u}_{k}, \\quad k \\in\\{0, \\ldots, N-1\\} \\\\ &\\underline{u}-r_{k} \\leq \\hat{u}_{k} \\leq \\bar{u}+r_{k} \\quad \\text { and } \\quad r_{k} \\geq 0, \\quad k \\in\\{0, \\cdots N-1\\} \\\\ &\\underline{x}-s_{k} \\leq \\hat{x}_{k} \\leq \\bar{x}+s_{k} \\quad \\text { and } \\quad s_{k} \\geq 0, \\quad k \\in\\{1, \\ldots, N\\} \\end{aligned} \u5982\u679c\u5c06\u8f93\u5165\u63a7\u5236\u53d8\u91cf\u5206\u89e3\u4e3a u_t = Kx_t + \\delta u_t ,\u5176\u4e2d K \u4e3a\u7ebf\u6027\u72b6\u6001\u53cd\u9988\u77e9\u9635\u3002\u4e0a\u8ff0\u95ee\u9898\u53d8\u4e3a \\begin{aligned} \\delta \\hat{u}_{0: N}^{\\star}=\\underset{\\delta \\hat{u}}{\\operatorname{argmin}} & \\frac{1}{2} \\sum_{k=0}^{N-1}\\left(K \\hat{x}_{k}+\\delta \\hat{u}_{k}\\right)^{\\top} R\\left(K \\hat{x}_{k}+\\delta \\hat{u}_{k}\\right)+\\frac{1}{2} \\sum_{k=1}^{N-1} \\hat{x}_{k}^{\\top} Q \\hat{x}_{k}+\\frac{1}{2} \\hat{x}_{N}^{\\top} Q_{N} \\hat{x}_{N} \\\\ &+k_{u} \\sum_{k=0}^{N-1} \\mathbf{1}_{m}^{\\top} r_{k}+k_{x} \\sum_{k=1}^{N} \\mathbf{1}_{n}^{\\top} s_{k} \\\\ \\text { s.t. } & \\hat{x}_{0}=x_{t} \\\\ &\\hat{x}_{k+1}=(A+B K) \\hat{x}_{k}+B \\delta \\hat{u}_{k}, \\quad k \\in\\{0, \\ldots, N-1\\} \\\\ & \\underline{u}-r_{k} \\leq K \\hat{x}_{k}+\\delta \\hat{u}_{k} \\leq \\bar{u}+r_{k} \\quad \\text { and } \\quad r_{k} \\geq 0, \\quad k \\in\\{0, \\ldots, N-1\\} \\\\ &\\underline{x}-s_{k} \\leq \\hat{x}_{k} \\leq \\bar{x}+s_{k} \\text { and } \\quad s_{k} \\geq 0, \\quad k \\in\\{1, \\ldots, N\\} \\end{aligned} \u5c06\u5b83\u4eec\u91cd\u6574\u4e3aQP\u95ee\u9898\u540e\uff0c\u53ef\u4ee5\u4f7f\u7528 OptNet \u7684\u6a21\u5757\u5f97\u5230\u4f18\u5316\u7ed3\u679c\u4ee5\u53ca\u5bf9 Q, R, A, B, \\underline{u}, \\bar{u}, \\underline{x}, \\bar{x}, k_{x} \u548c k_{u} \u7684\u5bfc\u6570\u3002 \u6a21\u4eff\u5b66\u4e60\u5219\u53ef\u4ee5\u6839\u636e\u4f18\u5316\u7ed3\u679c\u4ee5\u53ca\u4e13\u5bb6\u6807\u6ce8\u7ed9\u51fa\u635f\u5931\u3002","title":"MPC Review"},{"location":"Planning_Control_DL/diff_MPC_ARE/#infinite-horizon-terminal-cost","text":"LQR\u6700\u4f18\u63a7\u5236\u5668\u5f97\u5230\u7684\u63a7\u5236\u7ed3\u679c\u662f\u9488\u5bf9\u65e0\u9650\u9884\u6d4b\u6b65\u957f\u7684LTI\u7cfb\u7edfMPC\u95ee\u9898\u3002\u5176 K \u4e3a K=-\\left(R+B^{\\top} P B\\right)^{-1} B^{\\top} P A \u5176\u4e2d P \u4e3a\u79bb\u6563Riccati\u65b9\u7a0b\u7684\u89e3: P=A^{\\top} P A-A^{\\top} P B\\left(R+B^{\\top} P B\\right)^{-1} B^{\\top} P A+Q \u6ce8\u610f P \u7684 \u6570\u5b66\u610f\u4e49 ,Final cost J = x_0^TPx_0 .\u672c\u6587\u63d0\u51fa\u524d\u6587\u7684MPC\u4e2d\uff0c\u7528\u4e8e\u8868\u8fbe\u7a33\u6001cost\u7684\u77e9\u9635 Q_N = P . \u5bf9\u4e8e\u9009\u62e9 Q_N=P \u7684MPC\uff0c\u4f5c\u8005\u8bc1\u660e\u4e86\u4e00\u7cfb\u5217\u597d\u7684\u6027\u8d28\uff0c\u5305\u62ec\u5b58\u5728\u6709\u9650\u89c6\u754cMPC\u4f7f\u5f97\u65e0\u9650\u89c6\u754c\u6700\u4f18\uff0cMPC\u53ef\u884c\u6027\u3001\u6e10\u8fdb\u7a33\u5b9a\u6027\uff0c\u9c81\u68d2\u6027\u7b49\u3002 \u4f5c\u8005\u63d0\u51fa\u4e86 P \u4e0e A,B,Q,R \u7684\u77e2\u91cf\u5173\u7cfb\u3002 \\frac{\\partial \\operatorname{vec} P}{\\partial \\operatorname{vec} A}=Z_{1}^{-1} Z_{2}, \\quad \\frac{\\partial \\operatorname{vec} P}{\\partial \\operatorname{vec} B}=Z_{1}^{-1} Z_{3}, \\quad \\frac{\\partial \\operatorname{vec} P}{\\partial \\operatorname{vec} Q}=Z_{1}^{-1} Z_{4}, \\quad \\frac{\\partial \\operatorname{vec} P}{\\partial \\operatorname{vec} R}=Z_{1}^{-1} Z_{5} \\begin{aligned} Z_{1}&:=\\mathbf{I}_{n^{2}}-\\left(A^{\\top} \\otimes A^{\\top}\\right)\\left[\\mathbf{I}_{n^{2}}-\\left(P B M_{2} B^{\\top} \\otimes \\mathbf{I}_{n}\\right)-\\left(\\mathbf{I}_{n} \\otimes P B M_{2} B^{\\top}\\right)\\right. \\\\ &\\left.+(P B \\otimes P B)\\left(M_{2} \\otimes M_{2}\\right)\\left(B^{\\top} \\otimes B^{\\top}\\right)\\right] \\\\ Z_{2}&:=\\left(\\mathbf{V}_{n, n}+\\mathbf{I}_{n^{2}}\\right)\\left(\\mathbf{I}_{n} \\otimes A^{\\top} M_{1}\\right) \\\\ Z_{3}&:=\\left(A^{\\top} \\otimes A^{\\top}\\right)\\left[(P B \\otimes P B)\\left(M_{2} \\otimes M_{2}\\right)\\left(\\mathbf{I}_{m^{2}}+\\mathbf{V}_{m, m}\\right)\\left(\\mathbf{I}_{m} \\otimes B^{\\top} P\\right)\\right. \\\\ &\\left.-\\left(\\mathbf{I}_{n^{2}}+\\mathbf{V}_{n, n}\\right)\\left(P B M_{2} \\otimes P\\right)\\right] \\\\ Z_{4}&:=\\mathbf{I}_{n^{2}} \\\\ Z_{5}&:=\\left(A^{\\top} \\otimes A^{\\top}\\right)(P B \\otimes P B)\\left(M_{2} \\otimes M_{2}\\right) \\end{aligned} \\\\ \u5176\u4e2d \\otimes \u4e3a kronecker product","title":"Infinite Horizon Terminal Cost"},{"location":"Planning_Control_DL/end2end_challenge/","text":"End-to-end Autonomous Driving: Challenges and Frontiers CVPR2023 \u7684 best paper Planning-oriented Autonomous Driving , \u6574\u5408\u4e86\u8fd1\u5e74\u6765BEV/Occupancy \u9884\u6d4b\u7684\u8bbe\u8ba1\u54f2\u5b66\uff0c\u4ee5\u53ca\u6570\u636e\u9a71\u52a8 planning prediction\u7684\u8fdb\u5c55\uff0c\u4f7f\u7528\u591a\u4e2atransformer \u7f51\u7edc\uff0c\u7528 DETR embedding\u7684\u68af\u5ea6\u4f20\u9012\u5f62\u5f0f\uff0c\u5728\u4fdd\u7559\u7f51\u7edc\u4e2d\u95f4\u9884\u6d4b\u8fc7\u7a0b\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u65e0\u4eba\u9a7e\u9a76\u7cfb\u7edf\u6574\u4f53\u7684\u7aef\u5230\u7aef\u8bad\u7ec3\u63a8\u7406\u3002 \u672c\u6587\u4ee5\u8fd1\u671f\u7684\u53d1\u5c55\u4e3a\u951a\u70b9\uff0c\u9610\u8ff0\u4e86\u7aef\u5230\u7aef\u9a7e\u9a76\u7684\u524d\u6cbf\u4e0e\u96be\u70b9\uff0c\u7531UniAD\u6240\u5c5e \u5b9e\u9a8c\u5ba4\u548cKITTI\u7ec4\u8054\u5408\u53d1\u5e03\u3002 Summary from ChatGPT The survey paper on end-to-end autonomous driving provides a comprehensive analysis of the current research status, future challenges, and related considerations in the field. Here is a summary: Current Research Status: - The field of autonomous driving has seen a rapid growth in approaches that utilize an end-to-end algorithm framework. This framework uses raw sensor input to generate vehicle motion plans, instead of focusing on individual tasks such as detection and motion prediction. - End-to-end systems, compared to modular pipelines, benefit from joint feature optimization for perception and planning. This has been made possible due to the availability of large-scale datasets, closed-loop evaluation, and the increasing need for autonomous driving algorithms to perform effectively in challenging scenarios. - The end-to-end approaches can be broadly classified into imitation learning and reinforcement learning. Significant progress has been made with the development of deep neural networks in both these areas. - A significant turning point occurred in 2021 for end-to-end autonomous driving. Attention was focused on incorporating more modalities and advanced architectures to capture global context and representative features. Future Challenges and Considerations: - Data Engine: The development of a data engine that can automatically collect, clean, and label data is a critical future challenge. This engine should also support mining hard/corner cases, scene generation, and editing to facilitate data-driven evaluations and promote diversity of data and the generalization ability of models. - Foundation Model: The utilization of large-scale data and model capacity has unleashed the immense potential of AI in high-level reasoning tasks. However, the direct adoption of large language models (LLMs) for autonomous driving might seem misaligned due to the different goals of the two targets. A feasible solution to develop a large autonomous driving model is to train a video predictor that can forecast long-term predictions of the environment, either in 2D or 3D. - Vehicle-to-everything (V2X): V2X systems offer promising solutions to address challenges such as occlusion and obstacles beyond the perceptual range. These systems could provide a solution to realize advanced decision intelligence among autonomous vehicles. The paper concludes that end-to-end autonomous driving faces great opportunities and challenges simultaneously, with the ultimate goal of building generalist agents. Future endeavors to embrace the rapidly developed foundation models and data engines are discussed.","title":"End-to-end Autonomous Driving: Challenges and Frontiers"},{"location":"Planning_Control_DL/end2end_challenge/#end-to-end-autonomous-driving-challenges-and-frontiers","text":"CVPR2023 \u7684 best paper Planning-oriented Autonomous Driving , \u6574\u5408\u4e86\u8fd1\u5e74\u6765BEV/Occupancy \u9884\u6d4b\u7684\u8bbe\u8ba1\u54f2\u5b66\uff0c\u4ee5\u53ca\u6570\u636e\u9a71\u52a8 planning prediction\u7684\u8fdb\u5c55\uff0c\u4f7f\u7528\u591a\u4e2atransformer \u7f51\u7edc\uff0c\u7528 DETR embedding\u7684\u68af\u5ea6\u4f20\u9012\u5f62\u5f0f\uff0c\u5728\u4fdd\u7559\u7f51\u7edc\u4e2d\u95f4\u9884\u6d4b\u8fc7\u7a0b\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u65e0\u4eba\u9a7e\u9a76\u7cfb\u7edf\u6574\u4f53\u7684\u7aef\u5230\u7aef\u8bad\u7ec3\u63a8\u7406\u3002 \u672c\u6587\u4ee5\u8fd1\u671f\u7684\u53d1\u5c55\u4e3a\u951a\u70b9\uff0c\u9610\u8ff0\u4e86\u7aef\u5230\u7aef\u9a7e\u9a76\u7684\u524d\u6cbf\u4e0e\u96be\u70b9\uff0c\u7531UniAD\u6240\u5c5e \u5b9e\u9a8c\u5ba4\u548cKITTI\u7ec4\u8054\u5408\u53d1\u5e03\u3002","title":"End-to-end Autonomous Driving: Challenges and Frontiers"},{"location":"Planning_Control_DL/end2end_challenge/#summary-from-chatgpt","text":"The survey paper on end-to-end autonomous driving provides a comprehensive analysis of the current research status, future challenges, and related considerations in the field. Here is a summary: Current Research Status: - The field of autonomous driving has seen a rapid growth in approaches that utilize an end-to-end algorithm framework. This framework uses raw sensor input to generate vehicle motion plans, instead of focusing on individual tasks such as detection and motion prediction. - End-to-end systems, compared to modular pipelines, benefit from joint feature optimization for perception and planning. This has been made possible due to the availability of large-scale datasets, closed-loop evaluation, and the increasing need for autonomous driving algorithms to perform effectively in challenging scenarios. - The end-to-end approaches can be broadly classified into imitation learning and reinforcement learning. Significant progress has been made with the development of deep neural networks in both these areas. - A significant turning point occurred in 2021 for end-to-end autonomous driving. Attention was focused on incorporating more modalities and advanced architectures to capture global context and representative features. Future Challenges and Considerations: - Data Engine: The development of a data engine that can automatically collect, clean, and label data is a critical future challenge. This engine should also support mining hard/corner cases, scene generation, and editing to facilitate data-driven evaluations and promote diversity of data and the generalization ability of models. - Foundation Model: The utilization of large-scale data and model capacity has unleashed the immense potential of AI in high-level reasoning tasks. However, the direct adoption of large language models (LLMs) for autonomous driving might seem misaligned due to the different goals of the two targets. A feasible solution to develop a large autonomous driving model is to train a video predictor that can forecast long-term predictions of the environment, either in 2D or 3D. - Vehicle-to-everything (V2X): V2X systems offer promising solutions to address challenges such as occlusion and obstacles beyond the perceptual range. These systems could provide a solution to realize advanced decision intelligence among autonomous vehicles. The paper concludes that end-to-end autonomous driving faces great opportunities and challenges simultaneously, with the ultimate goal of building generalist agents. Future endeavors to embrace the rapidly developed foundation models and data engines are discussed.","title":"Summary from ChatGPT"},{"location":"The_theory/CMA_ES/","text":"Covariance matrix adaptation evolution strategy: CMA-ES wiki blog CMA-ES\u662f\u4e00\u79cd\u8fdb\u5316\u7b97\u6cd5\uff0c\u4e5f\u5c31\u662f\u4e00\u79cd\u9ed1\u7bb1\u4f18\u5316\u7b97\u6cd5\u3002 Background \u8fdb\u5316\u7b97\u6cd5 \u57fa\u4e8e\u79cd\u7fa4\u7684\u7b97\u6cd5\u57fa\u672c\u601d\u8def: \u4ea7\u751f\u521d\u59cb\u91c7\u6837\u79cd\u7fa4, D=\\left\\{\\left(x_{i}, f\\left(x_{i}\\right)\\right)\\right\\}, x_{i} \\sim p_{\\theta}(x) \u8bc4\u4f30\u79cd\u7fa4\u7684\u9002\u5e94\u5ea6 \u9009\u62e9\u6700\u4f18\u5b50\u96c6\u6765\u66f4\u65b0\u53c2\u6570 \\theta . \u7b80\u5355\u9ad8\u65af\u8fdb\u5316\u7b56\u7565 \u79cd\u7fa4\u7684\u6982\u7387\u5206\u5e03 p_\\theta(x) \u88ab\u5efa\u6a21\u4e3a\u4e00\u4e2a n \u7ef4\u7684\u9ad8\u65af\u5206\u5e03\uff0c \\theta=(\\mu, \\sigma), p_{\\theta}(x) \\sim \\mathcal{N}\\left(\\mu, \\sigma^{2} I\\right)=\\mu+\\sigma \\mathcal{N}(0, I) \u57fa\u672c\u7b97\u6cd5: \u4ea7\u751f\u521d\u59cb\u5316\u53c2\u6570 \\theta=\\theta_0 \u4ece\u9ad8\u65af\u5206\u5e03\u4e2d\u91c7\u96c6\u540e\u4ee3\u79cd\u7fa4 D^{(t+1)}=\\left\\{x_{i}^{(t+1)} \\mid x_{i}^{(t+1)}=\\mu^{(t)}+\\sigma^{(t)} y_{i}^{(t+1)}\\right. where \\left.y_{i}^{(t+1)} \\sim \\mathcal{N}(x \\mid 0, \\mathbf{I}), i=1, \\ldots, \\Lambda\\right\\} \u9009\u62e9\u9002\u5e94\u5ea6\u6700\u597d\u7684 k \u4e2a\u6837\u672c\uff0c \u91cd\u65b0\u8ba1\u7b97\u65b0\u7684\u5747\u503c\u548c\u65b9\u5dee D_{\\text {elite }}^{(t+1)}=\\left\\{x_{i}^{(t+1)} \\mid x_{i}^{(t+1)} \\in D^{(t+1)}, i=1, \\ldots, \\lambda, \\lambda \\leq \\Lambda\\right\\} \\mu^{(t+1)}=\\operatorname{avg}\\left(D_{\\text {elite }}^{(t+1)}\\right)=\\frac{1}{\\lambda} \\sum_{i=1}^{\\wedge} x_{i}^{(t+1)} \\sigma^{(t+1)^{2}}=\\operatorname{var}\\left(D_{\\text {elite }}^{(t+1)}\\right)=\\frac{1}{\\lambda} \\sum_{i=1}^{\\lambda}\\left(x_{i}^{(t+1)}-\\mu^{(t)}\\right)^{2} \u534f\u65b9\u5dee\u81ea\u9002\u5e94\u8fdb\u5316\u7b56\u7565 CMA-ES CMA-ES\u6709\u4e24\u4e2a\u5173\u952e\u7684\u65b0\u6982\u5ff5: - \u4f7f\u7528\u4e00\u4e2a\u534f\u65b9\u5dee\u77e9\u9635 C \u8ddf\u8e2a\u4e24\u4e2a\u6837\u672c\u4e4b\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\u3002\u8fd9\u4e2a\u534f\u65b9\u5dee\u77e9\u9635\u4f1a\u662f\u4e00\u4e2a\u5bf9\u89d2\u77e9\u9635\uff0c\u4e14\u53ef\u4ee5\u7531\u4e00\u7ec4\u6807\u51c6\u6b63\u4ea4\u57fa\u548c\u4e00\u7ec4\u7279\u5f81\u503c \\lambda \u5b8c\u5168\u5b9a\u4e49 - \u4f7f\u7528 \\sigma \u63a7\u5236\u66f4\u65b0\u6b65\u957f\u3002 - \u5747\u503c\u7684\u66f4\u65b0\u65b9\u5411\u662f\u6700\u5927\u5316\u4e0a\u4e00\u6b21\u6210\u529f\u7684candidates\u7684\u6982\u7387\u503c\uff1b\u534f\u65b9\u5dee\u7684\u66f4\u65b0\u65b9\u5411\u662f\u6700\u5927\u5316\u4e0a\u4e00\u6b21\u66f4\u65b0\u7684\u641c\u7d22\u65b9\u5411\u3002\u53ef\u4ee5\u7406\u89e3\u4e3a\u662fnatural gradient descent. \u53e6\u5916\u8fd8\u4f1a\u5bf9\u8fc7\u53bb\u7684\u6570\u4e2a\u641c\u7d22\u65b9\u5411\u4e2d\uff0c\u7528PCA\u63d0\u53d6\u5176\u4e3b\u65b9\u5411\u3002 - \u8bb0\u5f55\u4e24\u4e2apath. search path \u4e3b\u8981\u662f\u5b58\u50a8\u8fde\u7eed\u6b65\u957f\u4e4b\u95f4\u7684\u76f8\u5173\u6027, \u662f\u4e3a\u4e86\u8ba1\u7b97\u597d\u7684\u76f8\u5173\u6027\u77e9\u9635\uff0c\u7b2c\u4e8c\u4e2aevolution path\u662f\u4e3a\u4e86\u63a7\u5236\u6b65\u957f\u3002 \u91c7\u6837\u65b9\u6cd5: \\begin{aligned} x_{i} & \\sim \\mathcal{N}\\left(m_{k}, \\sigma_{k}^{2} C_{k}\\right) \\\\ & \\sim m_{k}+\\sigma_{k} \\times \\mathcal{N}\\left(0, C_{k}\\right) \\end{aligned} \u4e3a\u6bcf\u4e00\u4e2a\u6837\u672c\u8ba1\u7b97\u9002\u5e94\u5ea6\u51fd\u6570 f(x_{i:\\lambda}) , \u5e76\u6392\u5e8f\u9009\u62e9\u524d \\mu \u4e2a\u3002 \u5747\u503c\u66f4\u65b0: \\mu^{(t+1)}=\\mu^{(t)}+\\alpha_{\\mu} \\frac{1}{\\lambda} \\sum_{i=1}^{\\lambda}\\left(x_{i}^{(t+1)}-\\mu^{(t)}\\right) \u6b65\u957f\u7684\u76f4\u89c9\u5982\u4e0b: \u7ef4\u62a4\u4e00\u4e2a\u6f14\u5316\u8def\u5f84(evolution path) p_{\\sigma} , \u8fd9\u91cc\u7684 m \u662f\u5747\u503c\u3002 \\begin{array}{l} p_{c} \\leftarrow \\underbrace{\\left(1-c_{c}\\right)}_{\\text {discount factor }} p_{c}+\\underbrace{\\mathbf{1}_{[0, \\alpha \\sqrt{n}]}\\left(\\left\\|p_{\\sigma}\\right\\|\\right)}_{\\text {indicator function }} \\overbrace{\\sqrt{1-\\left(1-c_{c}\\right)^{2}}}^{\\text { complements for discounted variance } \\sqrt{\\mu_{w}} \\frac{m_{k+1}-m_{k}}{\\sigma_{k}}}_{\\text {distributed as } \\mathcal{N}\\left(0, C_{k}\\right) \\text { under neutral selection }}\\\\ C_{k+1}=\\underbrace{\\left(1-c_{1}-c_{\\mu}+c_{s}\\right)}_{\\text {discount factor }} C_{k}+c_{1} \\underbrace{p_{c} p_{c}^{T}}_{\\text {rank one matrix }}+\\underbrace{\\sum_{i=1}^{\\mu} w_{i} \\frac{x_{i: \\lambda}-m_{k}}{\\sigma_{k}}\\left(\\frac{x_{i: \\lambda}-m_{k}}{\\sigma_{k}}\\right)^{T}}_{\\operatorname{rank} \\min (\\mu, n) \\text { matrix }} \\end{array}","title":"Covariance matrix adaptation evolution strategy: CMA-ES"},{"location":"The_theory/CMA_ES/#covariance-matrix-adaptation-evolution-strategy-cma-es","text":"wiki blog CMA-ES\u662f\u4e00\u79cd\u8fdb\u5316\u7b97\u6cd5\uff0c\u4e5f\u5c31\u662f\u4e00\u79cd\u9ed1\u7bb1\u4f18\u5316\u7b97\u6cd5\u3002","title":"Covariance matrix adaptation evolution strategy: CMA-ES"},{"location":"The_theory/CMA_ES/#background","text":"","title":"Background"},{"location":"The_theory/CMA_ES/#_1","text":"\u57fa\u4e8e\u79cd\u7fa4\u7684\u7b97\u6cd5\u57fa\u672c\u601d\u8def: \u4ea7\u751f\u521d\u59cb\u91c7\u6837\u79cd\u7fa4, D=\\left\\{\\left(x_{i}, f\\left(x_{i}\\right)\\right)\\right\\}, x_{i} \\sim p_{\\theta}(x) \u8bc4\u4f30\u79cd\u7fa4\u7684\u9002\u5e94\u5ea6 \u9009\u62e9\u6700\u4f18\u5b50\u96c6\u6765\u66f4\u65b0\u53c2\u6570 \\theta .","title":"\u8fdb\u5316\u7b97\u6cd5"},{"location":"The_theory/CMA_ES/#_2","text":"\u79cd\u7fa4\u7684\u6982\u7387\u5206\u5e03 p_\\theta(x) \u88ab\u5efa\u6a21\u4e3a\u4e00\u4e2a n \u7ef4\u7684\u9ad8\u65af\u5206\u5e03\uff0c \\theta=(\\mu, \\sigma), p_{\\theta}(x) \\sim \\mathcal{N}\\left(\\mu, \\sigma^{2} I\\right)=\\mu+\\sigma \\mathcal{N}(0, I) \u57fa\u672c\u7b97\u6cd5: \u4ea7\u751f\u521d\u59cb\u5316\u53c2\u6570 \\theta=\\theta_0 \u4ece\u9ad8\u65af\u5206\u5e03\u4e2d\u91c7\u96c6\u540e\u4ee3\u79cd\u7fa4 D^{(t+1)}=\\left\\{x_{i}^{(t+1)} \\mid x_{i}^{(t+1)}=\\mu^{(t)}+\\sigma^{(t)} y_{i}^{(t+1)}\\right. where \\left.y_{i}^{(t+1)} \\sim \\mathcal{N}(x \\mid 0, \\mathbf{I}), i=1, \\ldots, \\Lambda\\right\\} \u9009\u62e9\u9002\u5e94\u5ea6\u6700\u597d\u7684 k \u4e2a\u6837\u672c\uff0c \u91cd\u65b0\u8ba1\u7b97\u65b0\u7684\u5747\u503c\u548c\u65b9\u5dee D_{\\text {elite }}^{(t+1)}=\\left\\{x_{i}^{(t+1)} \\mid x_{i}^{(t+1)} \\in D^{(t+1)}, i=1, \\ldots, \\lambda, \\lambda \\leq \\Lambda\\right\\} \\mu^{(t+1)}=\\operatorname{avg}\\left(D_{\\text {elite }}^{(t+1)}\\right)=\\frac{1}{\\lambda} \\sum_{i=1}^{\\wedge} x_{i}^{(t+1)} \\sigma^{(t+1)^{2}}=\\operatorname{var}\\left(D_{\\text {elite }}^{(t+1)}\\right)=\\frac{1}{\\lambda} \\sum_{i=1}^{\\lambda}\\left(x_{i}^{(t+1)}-\\mu^{(t)}\\right)^{2}","title":"\u7b80\u5355\u9ad8\u65af\u8fdb\u5316\u7b56\u7565"},{"location":"The_theory/CMA_ES/#cma-es","text":"CMA-ES\u6709\u4e24\u4e2a\u5173\u952e\u7684\u65b0\u6982\u5ff5: - \u4f7f\u7528\u4e00\u4e2a\u534f\u65b9\u5dee\u77e9\u9635 C \u8ddf\u8e2a\u4e24\u4e2a\u6837\u672c\u4e4b\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\u3002\u8fd9\u4e2a\u534f\u65b9\u5dee\u77e9\u9635\u4f1a\u662f\u4e00\u4e2a\u5bf9\u89d2\u77e9\u9635\uff0c\u4e14\u53ef\u4ee5\u7531\u4e00\u7ec4\u6807\u51c6\u6b63\u4ea4\u57fa\u548c\u4e00\u7ec4\u7279\u5f81\u503c \\lambda \u5b8c\u5168\u5b9a\u4e49 - \u4f7f\u7528 \\sigma \u63a7\u5236\u66f4\u65b0\u6b65\u957f\u3002 - \u5747\u503c\u7684\u66f4\u65b0\u65b9\u5411\u662f\u6700\u5927\u5316\u4e0a\u4e00\u6b21\u6210\u529f\u7684candidates\u7684\u6982\u7387\u503c\uff1b\u534f\u65b9\u5dee\u7684\u66f4\u65b0\u65b9\u5411\u662f\u6700\u5927\u5316\u4e0a\u4e00\u6b21\u66f4\u65b0\u7684\u641c\u7d22\u65b9\u5411\u3002\u53ef\u4ee5\u7406\u89e3\u4e3a\u662fnatural gradient descent. \u53e6\u5916\u8fd8\u4f1a\u5bf9\u8fc7\u53bb\u7684\u6570\u4e2a\u641c\u7d22\u65b9\u5411\u4e2d\uff0c\u7528PCA\u63d0\u53d6\u5176\u4e3b\u65b9\u5411\u3002 - \u8bb0\u5f55\u4e24\u4e2apath. search path \u4e3b\u8981\u662f\u5b58\u50a8\u8fde\u7eed\u6b65\u957f\u4e4b\u95f4\u7684\u76f8\u5173\u6027, \u662f\u4e3a\u4e86\u8ba1\u7b97\u597d\u7684\u76f8\u5173\u6027\u77e9\u9635\uff0c\u7b2c\u4e8c\u4e2aevolution path\u662f\u4e3a\u4e86\u63a7\u5236\u6b65\u957f\u3002 \u91c7\u6837\u65b9\u6cd5: \\begin{aligned} x_{i} & \\sim \\mathcal{N}\\left(m_{k}, \\sigma_{k}^{2} C_{k}\\right) \\\\ & \\sim m_{k}+\\sigma_{k} \\times \\mathcal{N}\\left(0, C_{k}\\right) \\end{aligned} \u4e3a\u6bcf\u4e00\u4e2a\u6837\u672c\u8ba1\u7b97\u9002\u5e94\u5ea6\u51fd\u6570 f(x_{i:\\lambda}) , \u5e76\u6392\u5e8f\u9009\u62e9\u524d \\mu \u4e2a\u3002 \u5747\u503c\u66f4\u65b0: \\mu^{(t+1)}=\\mu^{(t)}+\\alpha_{\\mu} \\frac{1}{\\lambda} \\sum_{i=1}^{\\lambda}\\left(x_{i}^{(t+1)}-\\mu^{(t)}\\right) \u6b65\u957f\u7684\u76f4\u89c9\u5982\u4e0b: \u7ef4\u62a4\u4e00\u4e2a\u6f14\u5316\u8def\u5f84(evolution path) p_{\\sigma} , \u8fd9\u91cc\u7684 m \u662f\u5747\u503c\u3002 \\begin{array}{l} p_{c} \\leftarrow \\underbrace{\\left(1-c_{c}\\right)}_{\\text {discount factor }} p_{c}+\\underbrace{\\mathbf{1}_{[0, \\alpha \\sqrt{n}]}\\left(\\left\\|p_{\\sigma}\\right\\|\\right)}_{\\text {indicator function }} \\overbrace{\\sqrt{1-\\left(1-c_{c}\\right)^{2}}}^{\\text { complements for discounted variance } \\sqrt{\\mu_{w}} \\frac{m_{k+1}-m_{k}}{\\sigma_{k}}}_{\\text {distributed as } \\mathcal{N}\\left(0, C_{k}\\right) \\text { under neutral selection }}\\\\ C_{k+1}=\\underbrace{\\left(1-c_{1}-c_{\\mu}+c_{s}\\right)}_{\\text {discount factor }} C_{k}+c_{1} \\underbrace{p_{c} p_{c}^{T}}_{\\text {rank one matrix }}+\\underbrace{\\sum_{i=1}^{\\mu} w_{i} \\frac{x_{i: \\lambda}-m_{k}}{\\sigma_{k}}\\left(\\frac{x_{i: \\lambda}-m_{k}}{\\sigma_{k}}\\right)^{T}}_{\\operatorname{rank} \\min (\\mu, n) \\text { matrix }} \\end{array}","title":"\u534f\u65b9\u5dee\u81ea\u9002\u5e94\u8fdb\u5316\u7b56\u7565 CMA-ES"},{"location":"The_theory/CNN_position_information/","text":"How much Position Information Do Convolutional Neural Networks Encode? \u8fd9\u7bc7\u8bba\u6587\u8ba8\u8bba\u7684\u662f\u5173\u4e8eCNN\u5982\u4f55\u5b58\u50a8\u7edd\u5bf9\u4f4d\u7f6e\u4fe1\u606f\u7684\u3002\u8fd9\u7bc7\u6587\u7ae0\u6700\u7ec8\u7ed3\u8bba\u662fCNN\u901a\u8fc7zero-padding\u5f97\u5230\u8fb9\u754c\u4fe1\u606f\uff0c\u5e76\u4e14\u901a\u8fc7\u5927\u611f\u53d7\u91ce\u7684\u795e\u7ecf\u5143\u5c06\u7edd\u5bf9\u4f4d\u7f6e\u4fe1\u606f\u4f20\u9012\u3002\u672c\u6587\u66f4\u503c\u5f97\u4e00\u8bfb\u7684\u662f\u9762\u5bf9CNN\u65f6\u5176\u4e2d\u8fde\u8d2f\u7684\u903b\u8f91\u63a8\u7406\u601d\u8def\u4ee5\u53ca\u5b9e\u9a8c\u8bbe\u8ba1\u3002\u4f5c\u8005\u4f7f\u7528\u5b9e\u9a8c\u8fde\u7eed\u5730\u8bf4\u660e\u51e0\u4e2a\u95ee\u9898\uff0c\u9996\u5148\u662f\u4ecesegmentation\u5b9e\u9a8c\u4e2d\uff0cmotivated\u5f97\u77e5CNN\u4f3c\u4e4e\u786e\u5b9e\u5bf9\u7edd\u5bf9\u4f4d\u7f6e\u6709\u611f\u77e5\u80fd\u529b,\u5176\u6b21\u8bbe\u8ba1\u4e86\u4e00\u4e2arandom test\u8bf4\u660e\u76ee\u524d\u5e38\u89c1\u7684CNN\u786e\u5b9e\u6709\u7edd\u5bf9\u4f4d\u7f6e\u63a8\u7406\u80fd\u529b\uff0c\u518d\u63a5\u7740\u8bbe\u8ba1\u591a\u7ec4\u5bf9\u6bd4\u5b9e\u9a8c\u4ee5\u53caabelation study,\u53d1\u73b0\u4e86\u8d8a\u6df1\u7684feature map\u5bf9\u7edd\u5bf9\u4f4d\u7f6e\u611f\u77e5\u529b\u8d8a\u5f3a\uff0c\u540c\u65f6\u53d1\u73b0padding\u7684\u6709\u65e0\u5bf9\u6574\u4e2a\u7f51\u7edc\u7edd\u5bf9\u4f4d\u7f6e\u611f\u77e5\u80fd\u529b\u7684\u5de8\u5927\u5f71\u54cd\u3002 Motivation \u56fe\u4e2d\u53d1\u73b0\u7684\u662fsalient detection\u5bf9cropped\u7684\u56fe\u7247inference\u7ed3\u679c\u4e0e\u539f\u56fe\u6709\u5dee\u522b\uff0c\u5982\u679cCNN\u4ec5\u4f9d\u8d56\u50cf\u7d20\u7279\u5f81\u4e0d\u5e94\u6709\u5982\u6b64\u5927\u7684\u5dee\u522b\uff0c\u8fd9\u91cc\u63a8\u6d4b\u7269\u4f53\u4f4d\u7f6e\u4e5f\u4f1a\u5f71\u54cddetection\u7ed3\u679c\uff0c\u4e5f\u5c31\u662fCNN\u6709\u5b58\u50a8\u7a7a\u95f4\u4f4d\u7f6e\u7684\u80fd\u529b\u3002 Random Test random test\u7684\u4efb\u52a1: 1. \u5c06\u56fe\u7247\u9001\u5165freezed \u7684 f_{enc} \u7f51\u7edc\u8f93\u51famulti-scale feature maps 2. \u5bf9multi-scale feature maps\u9001\u5165\u4e00\u4e2a\u5c0f\u7684\u7f51\u7edc\u4e2d\uff0c\u8f93\u51faposition maps position maps\u7684groud truth\u7ed3\u679c\u4e3a\u9884\u5148\u5b9a\u4e49\u7684\uff0c\u4e0e\u56fe\u7247\u5185\u5bb9\u65e0\u5173\u7684\u6761\u7eb9\u56fe\uff0c\u8fd9\u4e9b\u6761\u7eb9\u56fe\u4e0a\u70b9\u7684\u50cf\u7d20\u503c\u4ec5\u4e0e\u5176\u4f4d\u7f6e\u76f8\u5173\u3002\u4f5c\u8005\u5728training \u4e0evalidation\u7684\u65f6\u5019\u4f7f\u7528\u5b8c\u5168\u4e0d\u540c\u5185\u5bb9\u7684\u56fe\u7247\uff0c\u7406\u8bba\u4e0a\u6765\u8bf4\uff0c\u7531\u4e8e\u4f4d\u7f6e\u4fe1\u606f\u4e0e\u8f93\u5165\u56fe\u7247\u5b8c\u5168\u6ca1\u6709\u76f8\u5173\u6027\uff0c\u56e0\u800c\u5982\u679cCNN\u80fd\u5b58\u50a8\u4f4d\u7f6e\u4fe1\u606f\uff0c\u5c31\u5927\u6982\u7387\u80fd\u5728test set\u4e0a\u590d\u73b0\u8fd9\u4e9b\u6761\u7eb9 \u5b9e\u9a8c\u7ed3\u679c\u4e0e\u4f5c\u8005\u9884\u8ba1\u76f8\u4f3c\uff0c\u53d1\u73b0\u7684\u60c5\u51b5\u662f\u6d45\u5c42\u7684posEnet\u65e0\u6cd5\u8fd8\u539f\u4f4d\u7f6e\u4fe1\u606f\uff0c\u800cVGG\u548cResnet\u6709\u76f8\u5f53\u7684\u5b58\u50a8\u4f4d\u7f6e\u4fe1\u606f\u7684\u80fd\u529b\uff0c\u5728test set\u4e0a\u4ecd\u7136\u53ef\u4ee5\u590d\u73b0\u4f4d\u7f6e\u6761\u7eb9\u56fe\u3002 Abelation Study \u63a5\u7740\u4f5c\u8005\u8fdb\u884c\u4e86\u51e0\u4e2a\u7b80\u5355\u7684\u5bf9\u6bd4\u5b9e\u9a8c\u3002\u5f97\u5230\u7684\u7ecf\u9a8c\u7ed3\u8bba\u662f:feature map\u8d8a\u6df1\uff0c\u4f4d\u7f6e\u4fe1\u606f\u8fd8\u539f\u8d8a\u597d;kernel_size\u8d8a\u5927\uff0c\u4f4d\u7f6e\u4fe1\u606f\u8fd8\u539f\u8d8a\u597d\u3002\u8fd9\u91cc\u4f5c\u8005\u70b9\u51fa\u8fd9\u4e24\u70b9\u6307\u5411\u540c\u4e00\u4e2a\u7ed3\u8bba\u5c31\u662f\u7edd\u5bf9\u4f4d\u7f6e\u7684\u611f\u77e5\u4e0e\u611f\u53d7\u91ce\u6b63\u76f8\u5173\u3002 \u4e0a\u56fe\u53f3\u4fa7\u8868\u683c\u4ee3\u8868\u4f5c\u8005\u7684\u53e6\u4e00\u4e2a\u5b9e\u9a8c\uff0c\u5728\u53bb\u9664VGG\u7684 padding\u5c42\u540e\uff0c\u4f1a\u53d1\u73b0\u5176\u5bf9\u7edd\u5bf9\u4f4d\u7f6e\u7684\u611f\u53d7\u80fd\u529b\u8fd1\u4e4e\u51cf\u534a\uff0c\u800c\u5bf9\u6d45\u5c42\u7f51\u7edc\u9644\u52a0padding\u7684\u7ed3\u679c\u53d1\u73b0\u7edd\u5bf9\u4f4d\u7f6e\u611f\u77e5\u80fd\u529b\u4e0epadding\u7684\u6570\u91cf\u6709\u5f88\u5f3a\u7684\u6b63\u76f8\u5173\u6027\u3002 \u4f5c\u8005\u8fdb\u800c\u70b9\u51fa\uff0c\u5f88\u6709\u53ef\u80fd\u662fzero-padding\u5728\u7f51\u7edc\u4e2d\u63d0\u4f9b\u4e86\u4e00\u4e2a\u56fe\u7247\u5185\u5bb9\u7684\u8fb9\u754c\u4fe1\u606f\uff0c\u7ecf\u8fc7CNN\u590d\u6742\u7684\u5904\u7406\u4ee5\u53ca\u591a\u5c42\u7684\u611f\u53d7\u91ce\u4f20\u9012\u540e\uff0c\u5728\u6df1\u5c42\u7684\u7f51\u7edc\u4e2d\u5b58\u50a8\u4e86\u56fe\u7247\u4e2d\u5404\u4e2a\u533a\u57df\u7684\u7edd\u5bf9\u4f4d\u7f6e\u4fe1\u606f\u3002 Pretrained on other tasks \u4f5c\u8005\u8fdb\u4e00\u6b65\u6307\u51fa\uff0c\u76f4\u89c9\u6765\u8bb2\uff0csalient object detection\u4e0esemantic segmentation\u4f1a\u6bd4\u5206\u7c7b\u95ee\u9898\u9700\u8981\u66f4\u591a\u7684\u4f4d\u7f6e\u4fe1\u606f\uff0c\u56e0\u800c\u5728\u8fd9\u4e24\u4e2a\u4efb\u52a1\u4e0apretrained\u7684\u7f51\u7edc\u4f1a\u63d0\u4f9b\u66f4\u591a\u7684\u4f4d\u7f6e\u4fe1\u606f\u3002 \u4e0a\u56fe\u5bf9\u6bd4\u6307\u51fa\uff0c\u5728\u5176\u4ed6\u4efb\u52a1\u4e0afine-tuned\u7684VGG backbone\u76f8\u6bd4\u4e8e\u5728imagenet\u4e0apretrained\u7684\u4f1a\u663e\u8457\u5730\u591a\u5b58\u50a8\u50cf\u7d20\u7684\u7edd\u5bf9\u5750\u6807\u4fe1\u606f\u3002","title":"How much Position Information Do Convolutional Neural Networks Encode?"},{"location":"The_theory/CNN_position_information/#how-much-position-information-do-convolutional-neural-networks-encode","text":"\u8fd9\u7bc7\u8bba\u6587\u8ba8\u8bba\u7684\u662f\u5173\u4e8eCNN\u5982\u4f55\u5b58\u50a8\u7edd\u5bf9\u4f4d\u7f6e\u4fe1\u606f\u7684\u3002\u8fd9\u7bc7\u6587\u7ae0\u6700\u7ec8\u7ed3\u8bba\u662fCNN\u901a\u8fc7zero-padding\u5f97\u5230\u8fb9\u754c\u4fe1\u606f\uff0c\u5e76\u4e14\u901a\u8fc7\u5927\u611f\u53d7\u91ce\u7684\u795e\u7ecf\u5143\u5c06\u7edd\u5bf9\u4f4d\u7f6e\u4fe1\u606f\u4f20\u9012\u3002\u672c\u6587\u66f4\u503c\u5f97\u4e00\u8bfb\u7684\u662f\u9762\u5bf9CNN\u65f6\u5176\u4e2d\u8fde\u8d2f\u7684\u903b\u8f91\u63a8\u7406\u601d\u8def\u4ee5\u53ca\u5b9e\u9a8c\u8bbe\u8ba1\u3002\u4f5c\u8005\u4f7f\u7528\u5b9e\u9a8c\u8fde\u7eed\u5730\u8bf4\u660e\u51e0\u4e2a\u95ee\u9898\uff0c\u9996\u5148\u662f\u4ecesegmentation\u5b9e\u9a8c\u4e2d\uff0cmotivated\u5f97\u77e5CNN\u4f3c\u4e4e\u786e\u5b9e\u5bf9\u7edd\u5bf9\u4f4d\u7f6e\u6709\u611f\u77e5\u80fd\u529b,\u5176\u6b21\u8bbe\u8ba1\u4e86\u4e00\u4e2arandom test\u8bf4\u660e\u76ee\u524d\u5e38\u89c1\u7684CNN\u786e\u5b9e\u6709\u7edd\u5bf9\u4f4d\u7f6e\u63a8\u7406\u80fd\u529b\uff0c\u518d\u63a5\u7740\u8bbe\u8ba1\u591a\u7ec4\u5bf9\u6bd4\u5b9e\u9a8c\u4ee5\u53caabelation study,\u53d1\u73b0\u4e86\u8d8a\u6df1\u7684feature map\u5bf9\u7edd\u5bf9\u4f4d\u7f6e\u611f\u77e5\u529b\u8d8a\u5f3a\uff0c\u540c\u65f6\u53d1\u73b0padding\u7684\u6709\u65e0\u5bf9\u6574\u4e2a\u7f51\u7edc\u7edd\u5bf9\u4f4d\u7f6e\u611f\u77e5\u80fd\u529b\u7684\u5de8\u5927\u5f71\u54cd\u3002","title":"How much Position Information Do Convolutional Neural Networks Encode?"},{"location":"The_theory/CNN_position_information/#motivation","text":"\u56fe\u4e2d\u53d1\u73b0\u7684\u662fsalient detection\u5bf9cropped\u7684\u56fe\u7247inference\u7ed3\u679c\u4e0e\u539f\u56fe\u6709\u5dee\u522b\uff0c\u5982\u679cCNN\u4ec5\u4f9d\u8d56\u50cf\u7d20\u7279\u5f81\u4e0d\u5e94\u6709\u5982\u6b64\u5927\u7684\u5dee\u522b\uff0c\u8fd9\u91cc\u63a8\u6d4b\u7269\u4f53\u4f4d\u7f6e\u4e5f\u4f1a\u5f71\u54cddetection\u7ed3\u679c\uff0c\u4e5f\u5c31\u662fCNN\u6709\u5b58\u50a8\u7a7a\u95f4\u4f4d\u7f6e\u7684\u80fd\u529b\u3002","title":"Motivation"},{"location":"The_theory/CNN_position_information/#random-test","text":"random test\u7684\u4efb\u52a1: 1. \u5c06\u56fe\u7247\u9001\u5165freezed \u7684 f_{enc} \u7f51\u7edc\u8f93\u51famulti-scale feature maps 2. \u5bf9multi-scale feature maps\u9001\u5165\u4e00\u4e2a\u5c0f\u7684\u7f51\u7edc\u4e2d\uff0c\u8f93\u51faposition maps position maps\u7684groud truth\u7ed3\u679c\u4e3a\u9884\u5148\u5b9a\u4e49\u7684\uff0c\u4e0e\u56fe\u7247\u5185\u5bb9\u65e0\u5173\u7684\u6761\u7eb9\u56fe\uff0c\u8fd9\u4e9b\u6761\u7eb9\u56fe\u4e0a\u70b9\u7684\u50cf\u7d20\u503c\u4ec5\u4e0e\u5176\u4f4d\u7f6e\u76f8\u5173\u3002\u4f5c\u8005\u5728training \u4e0evalidation\u7684\u65f6\u5019\u4f7f\u7528\u5b8c\u5168\u4e0d\u540c\u5185\u5bb9\u7684\u56fe\u7247\uff0c\u7406\u8bba\u4e0a\u6765\u8bf4\uff0c\u7531\u4e8e\u4f4d\u7f6e\u4fe1\u606f\u4e0e\u8f93\u5165\u56fe\u7247\u5b8c\u5168\u6ca1\u6709\u76f8\u5173\u6027\uff0c\u56e0\u800c\u5982\u679cCNN\u80fd\u5b58\u50a8\u4f4d\u7f6e\u4fe1\u606f\uff0c\u5c31\u5927\u6982\u7387\u80fd\u5728test set\u4e0a\u590d\u73b0\u8fd9\u4e9b\u6761\u7eb9 \u5b9e\u9a8c\u7ed3\u679c\u4e0e\u4f5c\u8005\u9884\u8ba1\u76f8\u4f3c\uff0c\u53d1\u73b0\u7684\u60c5\u51b5\u662f\u6d45\u5c42\u7684posEnet\u65e0\u6cd5\u8fd8\u539f\u4f4d\u7f6e\u4fe1\u606f\uff0c\u800cVGG\u548cResnet\u6709\u76f8\u5f53\u7684\u5b58\u50a8\u4f4d\u7f6e\u4fe1\u606f\u7684\u80fd\u529b\uff0c\u5728test set\u4e0a\u4ecd\u7136\u53ef\u4ee5\u590d\u73b0\u4f4d\u7f6e\u6761\u7eb9\u56fe\u3002","title":"Random Test"},{"location":"The_theory/CNN_position_information/#abelation-study","text":"\u63a5\u7740\u4f5c\u8005\u8fdb\u884c\u4e86\u51e0\u4e2a\u7b80\u5355\u7684\u5bf9\u6bd4\u5b9e\u9a8c\u3002\u5f97\u5230\u7684\u7ecf\u9a8c\u7ed3\u8bba\u662f:feature map\u8d8a\u6df1\uff0c\u4f4d\u7f6e\u4fe1\u606f\u8fd8\u539f\u8d8a\u597d;kernel_size\u8d8a\u5927\uff0c\u4f4d\u7f6e\u4fe1\u606f\u8fd8\u539f\u8d8a\u597d\u3002\u8fd9\u91cc\u4f5c\u8005\u70b9\u51fa\u8fd9\u4e24\u70b9\u6307\u5411\u540c\u4e00\u4e2a\u7ed3\u8bba\u5c31\u662f\u7edd\u5bf9\u4f4d\u7f6e\u7684\u611f\u77e5\u4e0e\u611f\u53d7\u91ce\u6b63\u76f8\u5173\u3002 \u4e0a\u56fe\u53f3\u4fa7\u8868\u683c\u4ee3\u8868\u4f5c\u8005\u7684\u53e6\u4e00\u4e2a\u5b9e\u9a8c\uff0c\u5728\u53bb\u9664VGG\u7684 padding\u5c42\u540e\uff0c\u4f1a\u53d1\u73b0\u5176\u5bf9\u7edd\u5bf9\u4f4d\u7f6e\u7684\u611f\u53d7\u80fd\u529b\u8fd1\u4e4e\u51cf\u534a\uff0c\u800c\u5bf9\u6d45\u5c42\u7f51\u7edc\u9644\u52a0padding\u7684\u7ed3\u679c\u53d1\u73b0\u7edd\u5bf9\u4f4d\u7f6e\u611f\u77e5\u80fd\u529b\u4e0epadding\u7684\u6570\u91cf\u6709\u5f88\u5f3a\u7684\u6b63\u76f8\u5173\u6027\u3002 \u4f5c\u8005\u8fdb\u800c\u70b9\u51fa\uff0c\u5f88\u6709\u53ef\u80fd\u662fzero-padding\u5728\u7f51\u7edc\u4e2d\u63d0\u4f9b\u4e86\u4e00\u4e2a\u56fe\u7247\u5185\u5bb9\u7684\u8fb9\u754c\u4fe1\u606f\uff0c\u7ecf\u8fc7CNN\u590d\u6742\u7684\u5904\u7406\u4ee5\u53ca\u591a\u5c42\u7684\u611f\u53d7\u91ce\u4f20\u9012\u540e\uff0c\u5728\u6df1\u5c42\u7684\u7f51\u7edc\u4e2d\u5b58\u50a8\u4e86\u56fe\u7247\u4e2d\u5404\u4e2a\u533a\u57df\u7684\u7edd\u5bf9\u4f4d\u7f6e\u4fe1\u606f\u3002","title":"Abelation Study"},{"location":"The_theory/CNN_position_information/#pretrained-on-other-tasks","text":"\u4f5c\u8005\u8fdb\u4e00\u6b65\u6307\u51fa\uff0c\u76f4\u89c9\u6765\u8bb2\uff0csalient object detection\u4e0esemantic segmentation\u4f1a\u6bd4\u5206\u7c7b\u95ee\u9898\u9700\u8981\u66f4\u591a\u7684\u4f4d\u7f6e\u4fe1\u606f\uff0c\u56e0\u800c\u5728\u8fd9\u4e24\u4e2a\u4efb\u52a1\u4e0apretrained\u7684\u7f51\u7edc\u4f1a\u63d0\u4f9b\u66f4\u591a\u7684\u4f4d\u7f6e\u4fe1\u606f\u3002 \u4e0a\u56fe\u5bf9\u6bd4\u6307\u51fa\uff0c\u5728\u5176\u4ed6\u4efb\u52a1\u4e0afine-tuned\u7684VGG backbone\u76f8\u6bd4\u4e8e\u5728imagenet\u4e0apretrained\u7684\u4f1a\u663e\u8457\u5730\u591a\u5b58\u50a8\u50cf\u7d20\u7684\u7edd\u5bf9\u5750\u6807\u4fe1\u606f\u3002","title":"Pretrained on other tasks"},{"location":"The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/","text":"Channel Pruning for Accelerating Very Deep Neural Networks \u8fd9\u7bc7\u6587\u7ae0\u4ecb\u7ecd\u4e86\u4e00\u79cd\u5b9e\u73b0channel pruning\u7684\u7b97\u6cd5\u3002\u5176\u6838\u5fc3\u7b97\u6cd5\u4ee3\u7801\u5728\u8be5\u5e93\u7684./lib/net.py -> R3\u65b9\u6cd5\u4e2d\u3002 \u5bf9pretrained\u7684\u6a21\u578b\uff0c\u8981\u8fdb\u884c\u6a21\u578b\u7684\u4fee\u526a\uff0c\u672c\u6587\u63d0\u5230\u6709\u4e09\u79cd\u65b9\u6cd5\uff0c\u7b2c\u4e00\u79cd\u662fsparse connection, \u7b2c\u4e8c\u79cd\u662ftensor factorization,\u7b2c\u4e09\u79cd\u662fchannel pruning. sparse connection\u7531\u4e8e\u5f15\u5165\u4e86\u4e0d\u89c4\u5219\u7684\u6a21\u578b\u7ed3\u6784\uff0c\u4f1a\u5bfc\u81f4\u5728GPU\u4e0a\u7684\u6267\u884c\u6548\u7387\u4e0b\u964d(\u5c3d\u7ba1flops\u4e0b\u964d\u4e86)\u3002tensor factorization\u672c\u8d28\u4e0a\u662f\u5bf9\u6743\u91cd\u77e9\u9635\u7684\u5206\u89e3\uff0c\u5bf9\u4e8e\u73b0\u4ee3\u7684Res-Connect\u6548\u679c\u4e0d\u4f73\u3002channel pruning\u4e0d\u6539\u53d8\u7ed3\u6784\uff0c\u4e0d\u6539\u53d8\u5e76\u884c\u8fd0\u884c\u60c5\u51b5\uff0c\u4ec5\u4ec5\u6539\u53d8channel\u6570\u91cf\u3002\u4e09\u79cd\u65b9\u5f0f\u7684\u56fe\u793a\u5982\u4e0b\uff1a \u7b97\u6cd5\u4ecb\u7ecd \u8fd9\u4e2a\u7b97\u6cd5\u5206\u4e3a\u4e24\u6b65\u8fed\u4ee3\uff0c\u5206\u522b\u4e3achannel selection \u4ee5\u53ca reconstruction.\u7b2c\u4e00\u6b65\u627e\u5230\u6700\u6709\u4fe1\u606f\u91cf\u7684channel\uff0c\u4fee\u526a\u5197\u4f59\u7684channel\uff0c\u8fd9\u91cc\u4f7f\u7528\u7684\u662fLasso regression, \u7b2c\u4e8c\u6b65reconstruct with Linear least square. \u4ee4\u8f93\u5165\u7684feature map\u7684channel\u6570\u4e3a c , \u5377\u79ef\u6838 W \u7684\u6743\u91cd\u4e3a n\\times c \\times k_h \\times k_w ,\u5377\u79ef\u6838\u6bcf\u6b21\u5377\u79ef\u4f1a\u5728\u4e00\u4e2a\u50cf\u7d20\u70b9\u4e0a\u751f\u6210\u4e00\u4e2a N\\times n \u7684\u8f93\u51fa\u77e9\u9635 Y ,\u5176\u4e2d N \u4e3abatch_num\uff0c\u8fd9\u91cc\u6682\u65f6\u4e0d\u8003\u8651bias\u9879\u3002\u8981\u5c06 c \u4fee\u526a\u4e3a c' .\u540c\u65f6\u6700\u5c0f\u5316reconstruction error\uff0c\u8fd9\u4e2a\u4f18\u5316\u95ee\u9898\u662f \\begin{array}{c}{\\underset{\\boldsymbol{\\beta}, \\mathbf{W}}{\\arg \\min } \\frac{1}{2 N}\\left\\|\\mathbf{Y}-\\sum_{i=1}^{c} \\beta_{i} \\mathbf{X}_{\\mathbf{i}} \\mathbf{W}_{\\mathbf{i}}^{\\top}\\right\\|_{F}^{2}} \\\\ {\\text { subject to }\\|\\boldsymbol{\\beta}\\|_{0} \\leq c^{\\prime}}\\end{array} \u5176\u4e2d ||\\cdot||_F \u4e3a\u4e8c\u9636\u8303\u6570, X_i \u662f\u4e00\u4e2a N\\times k_h k_w \u7684\u77e9\u9635\u88c1\u526a\u81ea\u8f93\u5165 X , \\beta \u662f\u957f\u5ea6\u4e3a c \u7684\u77e2\u91cf\u53c2\u6570\u3002 \u6c42\u89e3\u8fd9\u4e2a\u95ee\u9898\u662fNP\u96be\u7684,\u8fd9\u91cc\u9996\u5148\u5c06\u95ee\u9898\u7528l1\u8303\u6570\u677e\u5f1b\u4e3a \\begin{array}{l}{\\underset{\\boldsymbol{\\beta}, \\mathbf{W}}{\\arg \\min } \\frac{1}{2 N}\\left\\|\\mathrm{Y}-\\sum_{i=1}^{c} \\beta_{i} \\mathrm{X}_{\\mathrm{i}} \\mathrm{W}_{\\mathrm{i}}^{\\top}\\right\\|_{F}^{2}+\\lambda\\|\\boldsymbol{\\beta}\\|_{1}} \\\\ {\\text { subject to }\\|\\boldsymbol{\\beta}\\|_{0} \\leq c^{\\prime}, \\forall i\\left\\|\\mathrm{W}_{\\mathrm{i}}\\right\\|_{F}=1}\\end{array} \u540c\u65f6\u9650\u5236 ||W_i||_F = 1 \u7136\u540e\u5728\u4ee5\u4e0b\u4e24\u4e2a\u6b65\u9aa4\u4e2d\u8fed\u4ee3 \\beta \u5b50\u95ee\u9898 \u9996\u5148\u9501\u5b9a W ,\u6c42\u89e3 \\beta \u4f5c\u4e3achannel selection\u95ee\u9898,\u8fd9\u53d8\u6210\u4e86\u96f6\u8303\u6570\u7684LASSO regression,\u4ee3\u7801\u4e2d\u53ef\u4ee5\u77e5\u9053\u4f5c\u8005\u662f\u4f7f\u7528sklearn\u7684Lasso regression\u51fd\u6570\u505a\u7684\u3002 W \u5b50\u95ee\u9898 \u9501\u5b9a \\beta ,\u95ee\u9898\u53d8\u4e3a argmin_{W'} ||Y - X'(W')^T||^2_F ,\u8fd9\u91cc\u7684 X' = [\\beta_1X_1, \\beta_2 X_2 ...] ( N\\times ck_hk_w ), W' \u5f62\u72b6\u4e3a n\\times c k_hk_w , W' = [W_1, W_2...] ,\u4e4b\u540e\u518d\u4ee4 \\beta_i \\leftarrow \\beta_i ||W_i||_F, W_i \\leftarrow W_i/||W_i||_F ,\u8fd9\u91cc\u672c\u8d28\u4e0a\u662f\u4e00\u4e2a\u7ebf\u6027\u56de\u5f52\u3002 \u4f18\u5316\u8fc7\u7a0b \u4ece\u5df2\u7ecf\u8bad\u7ec3\u597d\u7684\u6a21\u578b\u4e2d\u521d\u59cb\u5316W, \\lambda=0, ||\\beta||_0 = c .\u9010\u6e10\u589e\u52a0 \\lambda ,\u5bf9\u4e8e\u6bcf\u4e00\u4e2a \\lambda \uff0c\u8fed\u4ee3\u4e24\u4e2a\u5b50\u95ee\u9898\u7684\u6c42\u89e3\u76f4\u5230 ||\\beta||_0 \u7a33\u5b9a,\u9010\u6e10\u589e\u52a0 \\lambda \u76f4\u5230\u6ee1\u8db3 ||\\beta||_0 \\leq c' .\u540e\u6765\u53c8\u53d1\u73b0\u8fd9\u6837\u592a\u6162\uff0c\u4e8e\u662f\u6267\u884c\u5b50\u95ee\u9898(1)\u591a\u6b21\uff0c\u76f4\u5230 \\beta \u6ee1\u8db3\uff0c\u7136\u540e\u4ec5\u6267\u884c\u4e00\u6b21\u5b50\u95ee\u9898(2) \u5168\u6a21\u578b\u4fee\u5efa \u5728\u4fee\u526a\u65f6\uff0c\u6bcf\u4e00\u4e2a\u5c42\u5355\u72ec\u5904\u7406,\u9700\u8981\u6ce8\u610f\u7684\u662f\u6700\u4f18\u5316\u95ee\u9898\u4e2d\u7684 Y \u9700\u8981\u4f7f\u7528\u7684\u662f\u539f\u6a21\u578b\u7684\u8f93\u51fa\u4f5c\u4e3a\u76ee\u6807 Y \uff0c\u8f93\u5165 X \u5219\u662f\u4fee\u526a\u540e\u7684\u6a21\u578b\u7684\uff0c\u8fd9\u6837\u53ef\u4ee5\u907f\u514d\u8bef\u5dee\u7684\u7d2f\u79ef\u3002 \u5904\u7406\u591a\u5206\u652f\u6a21\u578b \u8fd9\u91cc\u4e3b\u8981\u8ba8\u8bbaResNet. \u8fd9\u91cc\u4e00\u5171\u6709\u4e09\u4e2a\u5377\u79ef\u5c42\uff0c\u7531\u4e8eResNet\u9700\u8981\u4fdd\u6301\u4e0d\u540c\u5c42\u4e4b\u95f4channel\u6570\u7684\u7a33\u5b9a\uff0c\u6240\u4ee5\u53ea\u6709\u4e2d\u95f4\u7684bottlenect\u4e5f\u5c31\u662f c1 \u7684\u8f93\u5165\u5c42\u53ef\u4ee5\u5982\u524d\u6587\u4e00\u6837\u6b63\u5e38\u4fee\u526a\u3002 \u6700\u540e\u4e00\u5c42\u7684\u4fee\u526a \u8fd9\u91cc\u5c06\u4f18\u5316\u76ee\u6807\u4ece Y_2 \u6539\u4e3a Y_1 - Y_1'+Y_2 ,\u8fd9\u91cc Y_1' \u662f\u524d\u9762\u5c42\u4fee\u526a\u540e\u8f93\u51fa\u7684feature map, \u7b2c\u4e00\u5c42\u7684\u4fee\u526a \u589e\u52a0\u4e00\u4e2asampler\u5c42\uff0c\u6b63\u5e38\u51cf\u5c11 c_0 \u5377\u79ef\u5c42\u7684\u8f93\u5165channel\u6570\uff0c\u6ce8\u610f\u8fd9\u4e2asampler\u4e0d\u6539\u53d8short-connection\u8def\u5f84\u4e0a\u7684 Y_1","title":"Channel Pruning for Accelerating Very Deep Neural Networks"},{"location":"The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/#channel-pruning-for-accelerating-very-deep-neural-networks","text":"\u8fd9\u7bc7\u6587\u7ae0\u4ecb\u7ecd\u4e86\u4e00\u79cd\u5b9e\u73b0channel pruning\u7684\u7b97\u6cd5\u3002\u5176\u6838\u5fc3\u7b97\u6cd5\u4ee3\u7801\u5728\u8be5\u5e93\u7684./lib/net.py -> R3\u65b9\u6cd5\u4e2d\u3002 \u5bf9pretrained\u7684\u6a21\u578b\uff0c\u8981\u8fdb\u884c\u6a21\u578b\u7684\u4fee\u526a\uff0c\u672c\u6587\u63d0\u5230\u6709\u4e09\u79cd\u65b9\u6cd5\uff0c\u7b2c\u4e00\u79cd\u662fsparse connection, \u7b2c\u4e8c\u79cd\u662ftensor factorization,\u7b2c\u4e09\u79cd\u662fchannel pruning. sparse connection\u7531\u4e8e\u5f15\u5165\u4e86\u4e0d\u89c4\u5219\u7684\u6a21\u578b\u7ed3\u6784\uff0c\u4f1a\u5bfc\u81f4\u5728GPU\u4e0a\u7684\u6267\u884c\u6548\u7387\u4e0b\u964d(\u5c3d\u7ba1flops\u4e0b\u964d\u4e86)\u3002tensor factorization\u672c\u8d28\u4e0a\u662f\u5bf9\u6743\u91cd\u77e9\u9635\u7684\u5206\u89e3\uff0c\u5bf9\u4e8e\u73b0\u4ee3\u7684Res-Connect\u6548\u679c\u4e0d\u4f73\u3002channel pruning\u4e0d\u6539\u53d8\u7ed3\u6784\uff0c\u4e0d\u6539\u53d8\u5e76\u884c\u8fd0\u884c\u60c5\u51b5\uff0c\u4ec5\u4ec5\u6539\u53d8channel\u6570\u91cf\u3002\u4e09\u79cd\u65b9\u5f0f\u7684\u56fe\u793a\u5982\u4e0b\uff1a","title":"Channel Pruning for Accelerating Very Deep Neural Networks"},{"location":"The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/#_1","text":"\u8fd9\u4e2a\u7b97\u6cd5\u5206\u4e3a\u4e24\u6b65\u8fed\u4ee3\uff0c\u5206\u522b\u4e3achannel selection \u4ee5\u53ca reconstruction.\u7b2c\u4e00\u6b65\u627e\u5230\u6700\u6709\u4fe1\u606f\u91cf\u7684channel\uff0c\u4fee\u526a\u5197\u4f59\u7684channel\uff0c\u8fd9\u91cc\u4f7f\u7528\u7684\u662fLasso regression, \u7b2c\u4e8c\u6b65reconstruct with Linear least square. \u4ee4\u8f93\u5165\u7684feature map\u7684channel\u6570\u4e3a c , \u5377\u79ef\u6838 W \u7684\u6743\u91cd\u4e3a n\\times c \\times k_h \\times k_w ,\u5377\u79ef\u6838\u6bcf\u6b21\u5377\u79ef\u4f1a\u5728\u4e00\u4e2a\u50cf\u7d20\u70b9\u4e0a\u751f\u6210\u4e00\u4e2a N\\times n \u7684\u8f93\u51fa\u77e9\u9635 Y ,\u5176\u4e2d N \u4e3abatch_num\uff0c\u8fd9\u91cc\u6682\u65f6\u4e0d\u8003\u8651bias\u9879\u3002\u8981\u5c06 c \u4fee\u526a\u4e3a c' .\u540c\u65f6\u6700\u5c0f\u5316reconstruction error\uff0c\u8fd9\u4e2a\u4f18\u5316\u95ee\u9898\u662f \\begin{array}{c}{\\underset{\\boldsymbol{\\beta}, \\mathbf{W}}{\\arg \\min } \\frac{1}{2 N}\\left\\|\\mathbf{Y}-\\sum_{i=1}^{c} \\beta_{i} \\mathbf{X}_{\\mathbf{i}} \\mathbf{W}_{\\mathbf{i}}^{\\top}\\right\\|_{F}^{2}} \\\\ {\\text { subject to }\\|\\boldsymbol{\\beta}\\|_{0} \\leq c^{\\prime}}\\end{array} \u5176\u4e2d ||\\cdot||_F \u4e3a\u4e8c\u9636\u8303\u6570, X_i \u662f\u4e00\u4e2a N\\times k_h k_w \u7684\u77e9\u9635\u88c1\u526a\u81ea\u8f93\u5165 X , \\beta \u662f\u957f\u5ea6\u4e3a c \u7684\u77e2\u91cf\u53c2\u6570\u3002 \u6c42\u89e3\u8fd9\u4e2a\u95ee\u9898\u662fNP\u96be\u7684,\u8fd9\u91cc\u9996\u5148\u5c06\u95ee\u9898\u7528l1\u8303\u6570\u677e\u5f1b\u4e3a \\begin{array}{l}{\\underset{\\boldsymbol{\\beta}, \\mathbf{W}}{\\arg \\min } \\frac{1}{2 N}\\left\\|\\mathrm{Y}-\\sum_{i=1}^{c} \\beta_{i} \\mathrm{X}_{\\mathrm{i}} \\mathrm{W}_{\\mathrm{i}}^{\\top}\\right\\|_{F}^{2}+\\lambda\\|\\boldsymbol{\\beta}\\|_{1}} \\\\ {\\text { subject to }\\|\\boldsymbol{\\beta}\\|_{0} \\leq c^{\\prime}, \\forall i\\left\\|\\mathrm{W}_{\\mathrm{i}}\\right\\|_{F}=1}\\end{array} \u540c\u65f6\u9650\u5236 ||W_i||_F = 1 \u7136\u540e\u5728\u4ee5\u4e0b\u4e24\u4e2a\u6b65\u9aa4\u4e2d\u8fed\u4ee3","title":"\u7b97\u6cd5\u4ecb\u7ecd"},{"location":"The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/#beta","text":"\u9996\u5148\u9501\u5b9a W ,\u6c42\u89e3 \\beta \u4f5c\u4e3achannel selection\u95ee\u9898,\u8fd9\u53d8\u6210\u4e86\u96f6\u8303\u6570\u7684LASSO regression,\u4ee3\u7801\u4e2d\u53ef\u4ee5\u77e5\u9053\u4f5c\u8005\u662f\u4f7f\u7528sklearn\u7684Lasso regression\u51fd\u6570\u505a\u7684\u3002","title":"\\beta \u5b50\u95ee\u9898"},{"location":"The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/#w","text":"\u9501\u5b9a \\beta ,\u95ee\u9898\u53d8\u4e3a argmin_{W'} ||Y - X'(W')^T||^2_F ,\u8fd9\u91cc\u7684 X' = [\\beta_1X_1, \\beta_2 X_2 ...] ( N\\times ck_hk_w ), W' \u5f62\u72b6\u4e3a n\\times c k_hk_w , W' = [W_1, W_2...] ,\u4e4b\u540e\u518d\u4ee4 \\beta_i \\leftarrow \\beta_i ||W_i||_F, W_i \\leftarrow W_i/||W_i||_F ,\u8fd9\u91cc\u672c\u8d28\u4e0a\u662f\u4e00\u4e2a\u7ebf\u6027\u56de\u5f52\u3002","title":"W \u5b50\u95ee\u9898"},{"location":"The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/#_2","text":"\u4ece\u5df2\u7ecf\u8bad\u7ec3\u597d\u7684\u6a21\u578b\u4e2d\u521d\u59cb\u5316W, \\lambda=0, ||\\beta||_0 = c .\u9010\u6e10\u589e\u52a0 \\lambda ,\u5bf9\u4e8e\u6bcf\u4e00\u4e2a \\lambda \uff0c\u8fed\u4ee3\u4e24\u4e2a\u5b50\u95ee\u9898\u7684\u6c42\u89e3\u76f4\u5230 ||\\beta||_0 \u7a33\u5b9a,\u9010\u6e10\u589e\u52a0 \\lambda \u76f4\u5230\u6ee1\u8db3 ||\\beta||_0 \\leq c' .\u540e\u6765\u53c8\u53d1\u73b0\u8fd9\u6837\u592a\u6162\uff0c\u4e8e\u662f\u6267\u884c\u5b50\u95ee\u9898(1)\u591a\u6b21\uff0c\u76f4\u5230 \\beta \u6ee1\u8db3\uff0c\u7136\u540e\u4ec5\u6267\u884c\u4e00\u6b21\u5b50\u95ee\u9898(2)","title":"\u4f18\u5316\u8fc7\u7a0b"},{"location":"The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/#_3","text":"\u5728\u4fee\u526a\u65f6\uff0c\u6bcf\u4e00\u4e2a\u5c42\u5355\u72ec\u5904\u7406,\u9700\u8981\u6ce8\u610f\u7684\u662f\u6700\u4f18\u5316\u95ee\u9898\u4e2d\u7684 Y \u9700\u8981\u4f7f\u7528\u7684\u662f\u539f\u6a21\u578b\u7684\u8f93\u51fa\u4f5c\u4e3a\u76ee\u6807 Y \uff0c\u8f93\u5165 X \u5219\u662f\u4fee\u526a\u540e\u7684\u6a21\u578b\u7684\uff0c\u8fd9\u6837\u53ef\u4ee5\u907f\u514d\u8bef\u5dee\u7684\u7d2f\u79ef\u3002","title":"\u5168\u6a21\u578b\u4fee\u5efa"},{"location":"The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/#_4","text":"\u8fd9\u91cc\u4e3b\u8981\u8ba8\u8bbaResNet. \u8fd9\u91cc\u4e00\u5171\u6709\u4e09\u4e2a\u5377\u79ef\u5c42\uff0c\u7531\u4e8eResNet\u9700\u8981\u4fdd\u6301\u4e0d\u540c\u5c42\u4e4b\u95f4channel\u6570\u7684\u7a33\u5b9a\uff0c\u6240\u4ee5\u53ea\u6709\u4e2d\u95f4\u7684bottlenect\u4e5f\u5c31\u662f c1 \u7684\u8f93\u5165\u5c42\u53ef\u4ee5\u5982\u524d\u6587\u4e00\u6837\u6b63\u5e38\u4fee\u526a\u3002","title":"\u5904\u7406\u591a\u5206\u652f\u6a21\u578b"},{"location":"The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/#_5","text":"\u8fd9\u91cc\u5c06\u4f18\u5316\u76ee\u6807\u4ece Y_2 \u6539\u4e3a Y_1 - Y_1'+Y_2 ,\u8fd9\u91cc Y_1' \u662f\u524d\u9762\u5c42\u4fee\u526a\u540e\u8f93\u51fa\u7684feature map,","title":"\u6700\u540e\u4e00\u5c42\u7684\u4fee\u526a"},{"location":"The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/#_6","text":"\u589e\u52a0\u4e00\u4e2asampler\u5c42\uff0c\u6b63\u5e38\u51cf\u5c11 c_0 \u5377\u79ef\u5c42\u7684\u8f93\u5165channel\u6570\uff0c\u6ce8\u610f\u8fd9\u4e2asampler\u4e0d\u6539\u53d8short-connection\u8def\u5f84\u4e0a\u7684 Y_1","title":"\u7b2c\u4e00\u5c42\u7684\u4fee\u526a"},{"location":"The_theory/CheckerBoardDeconv/","text":"Deconvolution and Checkerboard Artifacts \u8fd9\u7bc7paper\u7684\"pdf\u94fe\u63a5\"\u6307\u5411\u7684\u5c31\u662f\u8fd9\u7bc7paper\u7684\u5b98\u7f51\uff0c\u5f88\u5f3a\u7684\u53ef\u89c6\u5316\u6548\u679c\uff0c\u8ba8\u8bba\u7684\u95ee\u9898\u662fdeconvolution\u4ee5\u53ca\u5b83\u9020\u6210\u7684\u68cb\u76d8\u683c\u5b50\u6548\u5e94\u3002 \u5177\u4f53\u539f\u7406\u4e0e\u53ef\u89c6\u5316\u5728\u5b98\u65b9\u7684\u52a8\u56fe\u4e2d\u5f88\u6e05\u6670\u3002\u8fd9\u91cc\u7efc\u5408\u4e00\u4e0b\u672c\u6587\u7684\u7ed3\u8bba \u5728upsampling\u7684\u65f6\u5019\u7528deconv\u4f1a\u5f62\u6210\u68cb\u76d8\u683c\u5b50,\u5c24\u5176\u662fkernel_size\u4e0d\u80fd\u6574\u9664stride\u7684\u65f6\u5019(\u5e38\u89c1kernel_size=3, stride=2\u5c31\u662f\u5982\u6b64) \u591a\u5c42\u6df1\u5ea6\u7f51\u7edc\u5728\u7406\u8bba\u4e0a\u53ef\u4ee5\u5b66\u4e60\u6743\u91cd\u6d88\u9664\u8fd9\u4e2a\u68cb\u76d8\u6837\u5f0f\uff0c\u4f46\u662f\u8fd9\u4e2a\u5f88\u96be\uff0c\u81ea\u7136\u60c5\u51b5\u4e0b\u591a\u5c42\u7f51\u7edc\uff0c\u4e8c\u7ef4\u5377\u79ef\u53ea\u4f1a\u52a0\u5f3a\u8fd9\u4e2aartifact\u3002 \u4f7f\u7528resizing \u52a0\u4e0aconvolution\u80fd\u66f4\u597d\u7684\u6d88\u9664artifact deconv\u7684artifact\u4e5f\u4f1a\u8bf4\u660e\u4e8c\u7ef4Downsample\u7684\u65f6\u5019\uff0c\u5982\u679c\u4f7f\u7528convolution\u76f4\u63a5\u4e0b\u91c7\u6837\uff0c\u4e5f\u4f1a\u5728\u68af\u5ea6\u4e0a\u5f62\u6210artifact\uff0c\u4e5f\u5c31\u662f\u4f1a\u5f62\u6210\u566a\u97f3\uff0c\u6709\u4e9b\u50cf\u7d20\u5f97\u5230\u7684\u68af\u5ea6\u66f4\u591a\u6709\u7684\u4f1a\u5c11\u3002\u8fd9\u4f1a\u5f71\u54cdGAN\u7684\u8bad\u7ec3\u4ee5\u53ca\u4e00\u822c\u5377\u79ef\u7f51\u7edc\u7684\u8bad\u7ec3\u3002","title":"Deconvolution and Checkerboard Artifacts"},{"location":"The_theory/CheckerBoardDeconv/#deconvolution-and-checkerboard-artifacts","text":"\u8fd9\u7bc7paper\u7684\"pdf\u94fe\u63a5\"\u6307\u5411\u7684\u5c31\u662f\u8fd9\u7bc7paper\u7684\u5b98\u7f51\uff0c\u5f88\u5f3a\u7684\u53ef\u89c6\u5316\u6548\u679c\uff0c\u8ba8\u8bba\u7684\u95ee\u9898\u662fdeconvolution\u4ee5\u53ca\u5b83\u9020\u6210\u7684\u68cb\u76d8\u683c\u5b50\u6548\u5e94\u3002 \u5177\u4f53\u539f\u7406\u4e0e\u53ef\u89c6\u5316\u5728\u5b98\u65b9\u7684\u52a8\u56fe\u4e2d\u5f88\u6e05\u6670\u3002\u8fd9\u91cc\u7efc\u5408\u4e00\u4e0b\u672c\u6587\u7684\u7ed3\u8bba \u5728upsampling\u7684\u65f6\u5019\u7528deconv\u4f1a\u5f62\u6210\u68cb\u76d8\u683c\u5b50,\u5c24\u5176\u662fkernel_size\u4e0d\u80fd\u6574\u9664stride\u7684\u65f6\u5019(\u5e38\u89c1kernel_size=3, stride=2\u5c31\u662f\u5982\u6b64) \u591a\u5c42\u6df1\u5ea6\u7f51\u7edc\u5728\u7406\u8bba\u4e0a\u53ef\u4ee5\u5b66\u4e60\u6743\u91cd\u6d88\u9664\u8fd9\u4e2a\u68cb\u76d8\u6837\u5f0f\uff0c\u4f46\u662f\u8fd9\u4e2a\u5f88\u96be\uff0c\u81ea\u7136\u60c5\u51b5\u4e0b\u591a\u5c42\u7f51\u7edc\uff0c\u4e8c\u7ef4\u5377\u79ef\u53ea\u4f1a\u52a0\u5f3a\u8fd9\u4e2aartifact\u3002 \u4f7f\u7528resizing \u52a0\u4e0aconvolution\u80fd\u66f4\u597d\u7684\u6d88\u9664artifact deconv\u7684artifact\u4e5f\u4f1a\u8bf4\u660e\u4e8c\u7ef4Downsample\u7684\u65f6\u5019\uff0c\u5982\u679c\u4f7f\u7528convolution\u76f4\u63a5\u4e0b\u91c7\u6837\uff0c\u4e5f\u4f1a\u5728\u68af\u5ea6\u4e0a\u5f62\u6210artifact\uff0c\u4e5f\u5c31\u662f\u4f1a\u5f62\u6210\u566a\u97f3\uff0c\u6709\u4e9b\u50cf\u7d20\u5f97\u5230\u7684\u68af\u5ea6\u66f4\u591a\u6709\u7684\u4f1a\u5c11\u3002\u8fd9\u4f1a\u5f71\u54cdGAN\u7684\u8bad\u7ec3\u4ee5\u53ca\u4e00\u822c\u5377\u79ef\u7f51\u7edc\u7684\u8bad\u7ec3\u3002","title":"Deconvolution and Checkerboard Artifacts"},{"location":"The_theory/ClosingGap/","text":"Train longer, generalize better: closing the generalization gap in large batch training of neural networks \u8fd9\u7bc7paper\u4ece\u7406\u8bba\u4ee5\u53ca\u5b9e\u9a8c\u4e0a\u5bf9\u8bad\u7ec3\u8fc7\u7a0b\u4ee5\u53caGeneralization\u8fdb\u884c\u4e86\u5206\u6790\u3002\u7ed9\u51fa\u4e86\u4e00\u4e9b\u6709\u8da3\u7684\u7ed3\u8bba\u4ee5\u53ca\u6709\u8da3\u7684module\u3002 \u4ee5\u524d\u7684paper\u7684\u4e00\u4e9b\u7ed3\u8bba \u7528\u5927batch\u8bad\u7ec3\uff0cgeneralization error\u4f1a\u63d0\u5347 \u5373\u4f7f\u7f51\u7edc\u8bad\u7ec3\u5f88\u957f\u65f6\u95f4\uff0c\u8fd9\u4e2a\u635f\u5931\u4e5f\u4e0d\u4f1a\u53d8\u5316 \u597d\u7684generalization\u5bf9\u5e94\u5e73\u6ed1\u7684minima \u5c0fbatch\u80fd\u8ba9\u6743\u91cd\u66f4\u8fdc\u79bb\u521d\u59cb\u503c\u3002 \u5927batch\u4e2d\u6a21\u4eff\u5c0fbatch\u7684\u7edf\u8ba1\u6570\u636e \u8bad\u7ec3\u65f6\u957f \u4f5c\u8005\u5206\u6790\u8ba4\u4e3a\uff0c\u5728\u8bad\u7ec3\u7684\u521d\u671f\uff0c\u5728\u7f51\u7edc\u4e0d\u53d1\u6563\u7684\u60c5\u51b5\u4e0b\uff0c\u7f51\u7edc\u7684random-walk\u4f1a\u6781\u6162\u5730\u8fdc\u79bb\u521d\u59cb\u503c\uff0c\u5f97\u5230\u66f4\u597d\u7684\u7ed3\u679c\u3002\u5efa\u8bae\u7684\u5b9e\u73b0\u65b9\u6cd5\u662f\u5c3d\u53ef\u80fd\u5927\u7684\u4e0d\u53d1\u6563\u7684\u5b66\u4e60\u7387\uff0c\u8db3\u591f\u5927\u7684\u5b66\u4e60\u6b65\u957f\u3002 \u5b66\u4e60\u7387\u5e94\u6b63\u6bd4\u4e8ebatchsize\u7684\u5e73\u65b9\u6839 \u8fd9\u6837\u7684\u539f\u7531\u662f\u5bf9\u5e94\u76f8\u540c\u7684\u53c2\u6570\u5347\u7ea7\u534f\u65b9\u5dee \\operatorname{cov}(\\Delta \\mathbf{w}, \\Delta \\mathbf{w}) \\approx \\frac{\\eta^{2}}{M}\\left(\\frac{1}{N} \\sum_{n=1}^{N} \\mathbf{g}_{n} \\mathbf{g}_{n}^{\\top}\\right) \u4f7f\u7528Ghost BatchNorm\u6a21\u4eff\u5c0fbatch\u65f6\u6570\u636e\u7684\u7edf\u8ba1\u7279\u6027\u3002 \u6839\u636eTensorflow Batchnorm\u7684 \u5b98\u65b9API\u6587\u6863 ,\u8bbe\u5b9a layer = tf.keras.layers.BatchNormalization(*args, virtual_batch_size=<any_number>) \u53ef\u4ee5\u8ba9batchnorm\u5bf9\u5927batch\u62c6\u5206\uff0c\u5206\u6210\u5927\u5c0f\u4e3a <any\\_number> \u7684\u5c0fbatch\u8fd0\u884cbatchnorm \u8bad\u7ec3loss\u8d8b\u4e8e\u5e73\u6ed1\u540e\u4e0d\u8981\u754f\u60e7overfitting\uff0c \u7ee7\u7eedtrain \u4f5c\u8005\u5b9e\u9a8c\u5224\u65ad\u7684\u8ba4\u4e3a\u662f\uff0c\u5927batch\u6027\u80fd\u5dee\u7684\u4e3b\u8981\u539f\u56e0\u662fupdate\u7684\u6b21\u6570\u4e0d\u591f\u591a(\u65c1\u767d:\u8fd9\u4e2a\u6709\u4e9b\u65f6\u5019\u5f88\u771f\u5b9e)\u3002","title":"Train longer, generalize better: closing the generalization gap in large batch training of neural networks"},{"location":"The_theory/ClosingGap/#train-longer-generalize-better-closing-the-generalization-gap-in-large-batch-training-of-neural-networks","text":"\u8fd9\u7bc7paper\u4ece\u7406\u8bba\u4ee5\u53ca\u5b9e\u9a8c\u4e0a\u5bf9\u8bad\u7ec3\u8fc7\u7a0b\u4ee5\u53caGeneralization\u8fdb\u884c\u4e86\u5206\u6790\u3002\u7ed9\u51fa\u4e86\u4e00\u4e9b\u6709\u8da3\u7684\u7ed3\u8bba\u4ee5\u53ca\u6709\u8da3\u7684module\u3002","title":"Train longer, generalize better: closing the generalization gap in large batch training of neural networks"},{"location":"The_theory/ClosingGap/#paper","text":"\u7528\u5927batch\u8bad\u7ec3\uff0cgeneralization error\u4f1a\u63d0\u5347 \u5373\u4f7f\u7f51\u7edc\u8bad\u7ec3\u5f88\u957f\u65f6\u95f4\uff0c\u8fd9\u4e2a\u635f\u5931\u4e5f\u4e0d\u4f1a\u53d8\u5316 \u597d\u7684generalization\u5bf9\u5e94\u5e73\u6ed1\u7684minima \u5c0fbatch\u80fd\u8ba9\u6743\u91cd\u66f4\u8fdc\u79bb\u521d\u59cb\u503c\u3002","title":"\u4ee5\u524d\u7684paper\u7684\u4e00\u4e9b\u7ed3\u8bba"},{"location":"The_theory/ClosingGap/#batchbatch","text":"","title":"\u5927batch\u4e2d\u6a21\u4eff\u5c0fbatch\u7684\u7edf\u8ba1\u6570\u636e"},{"location":"The_theory/ClosingGap/#_1","text":"\u4f5c\u8005\u5206\u6790\u8ba4\u4e3a\uff0c\u5728\u8bad\u7ec3\u7684\u521d\u671f\uff0c\u5728\u7f51\u7edc\u4e0d\u53d1\u6563\u7684\u60c5\u51b5\u4e0b\uff0c\u7f51\u7edc\u7684random-walk\u4f1a\u6781\u6162\u5730\u8fdc\u79bb\u521d\u59cb\u503c\uff0c\u5f97\u5230\u66f4\u597d\u7684\u7ed3\u679c\u3002\u5efa\u8bae\u7684\u5b9e\u73b0\u65b9\u6cd5\u662f\u5c3d\u53ef\u80fd\u5927\u7684\u4e0d\u53d1\u6563\u7684\u5b66\u4e60\u7387\uff0c\u8db3\u591f\u5927\u7684\u5b66\u4e60\u6b65\u957f\u3002","title":"\u8bad\u7ec3\u65f6\u957f"},{"location":"The_theory/ClosingGap/#batchsize","text":"\u8fd9\u6837\u7684\u539f\u7531\u662f\u5bf9\u5e94\u76f8\u540c\u7684\u53c2\u6570\u5347\u7ea7\u534f\u65b9\u5dee \\operatorname{cov}(\\Delta \\mathbf{w}, \\Delta \\mathbf{w}) \\approx \\frac{\\eta^{2}}{M}\\left(\\frac{1}{N} \\sum_{n=1}^{N} \\mathbf{g}_{n} \\mathbf{g}_{n}^{\\top}\\right)","title":"\u5b66\u4e60\u7387\u5e94\u6b63\u6bd4\u4e8ebatchsize\u7684\u5e73\u65b9\u6839"},{"location":"The_theory/ClosingGap/#ghost-batchnormbatch","text":"\u6839\u636eTensorflow Batchnorm\u7684 \u5b98\u65b9API\u6587\u6863 ,\u8bbe\u5b9a layer = tf.keras.layers.BatchNormalization(*args, virtual_batch_size=<any_number>) \u53ef\u4ee5\u8ba9batchnorm\u5bf9\u5927batch\u62c6\u5206\uff0c\u5206\u6210\u5927\u5c0f\u4e3a <any\\_number> \u7684\u5c0fbatch\u8fd0\u884cbatchnorm","title":"\u4f7f\u7528Ghost BatchNorm\u6a21\u4eff\u5c0fbatch\u65f6\u6570\u636e\u7684\u7edf\u8ba1\u7279\u6027\u3002"},{"location":"The_theory/ClosingGap/#lossoverfitting-train","text":"\u4f5c\u8005\u5b9e\u9a8c\u5224\u65ad\u7684\u8ba4\u4e3a\u662f\uff0c\u5927batch\u6027\u80fd\u5dee\u7684\u4e3b\u8981\u539f\u56e0\u662fupdate\u7684\u6b21\u6570\u4e0d\u591f\u591a(\u65c1\u767d:\u8fd9\u4e2a\u6709\u4e9b\u65f6\u5019\u5f88\u771f\u5b9e)\u3002","title":"\u8bad\u7ec3loss\u8d8b\u4e8e\u5e73\u6ed1\u540e\u4e0d\u8981\u754f\u60e7overfitting\uff0c \u7ee7\u7eedtrain"},{"location":"The_theory/Continuous_learning/","text":"Continuous Learning \u8fd9\u4e2a\u9875\u9762\u4ecb\u7ecd continuous learning \u7b49\u76f8\u5173\u6587\u7ae0\uff0c\u8fd9\u4e2a\u5b9e\u73b0\u7684task\u662f\u8bf4: \u73b0\u6709\u4e00\u4e2a\u6a21\u578b\uff0c\u5728\u6570\u636e\u96c6 D_A \u4e0a\u4f7f\u7528\u4e00\u4e2abackbone \\Theta_A , \u4e00\u4e2ahead \\theta_A \u5b8c\u6210\u4e86\u5bf9\u7b2c\u4e00\u4e2a\u4efb\u52a1 A \u7684\u8bad\u7ec3\u3002\u7136\u540e\u73b0\u5728\u6211\u4eec\u5931\u53bb\u4e86\u6570\u636e\u96c6 D_A , \u4ee5 backbone \\Theta_A \u4e3a\u57fa\u7840\uff0c\u4e00\u4e2a\u65b0\u7684head \\theta_B \u8fdb\u884c\u8bad\u7ec3\uff0c\u5b9e\u73b0\u5bf9\u4efb\u52a1 B \u7684\u8bad\u7ec3\uff0c\u4f9d\u6b21\u8fed\u4ee3\u3002 \u76f4\u89c9\u53ef\u4ee5\u5224\u65ad\uff0c\u5982\u679c\u76f4\u63a5\u8bad\u7ec3 \\theta_B \u56fa\u5b9a \\Theta_A , \u5219\u539f\u6765\u7684\u4efb\u52a1\u4e0d\u4f1a\u5fd8\u8bb0\uff0c\u4f46\u662f\u7f51\u7edc\u5728 B \u4e0a\u7684\u6027\u80fd\u53ef\u80fd\u4f1a\u6709\u9650\u3002\u5982\u679c \\Theta_A \u4e0e \\theta_B \u90fd\u5728 B \u4e0a\u91cd\u65b0\u8bad\u7ec3\uff0c\u5219\u65e2\u662f\u662f\u4f4e\u5b66\u4e60\u7387\u7684fine-tuning \u90fd\u53ef\u80fd\u4f1a\u4f7f\u5f97\u7f51\u7edc\u5b8c\u5168\u7834\u574f\u4efb\u52a1 A \u7684\u7ed3\u679c\u3002 Continuous Learning\u7684\u4efb\u52a1\u5219\u662f\u8bbe\u8ba1\u5bf9\u7b56\uff0c\u4f7f\u5f97\u5728 B ,\u4e43\u81f3 C, D ... \u8bad\u7ec3\u65f6\u53ef\u4ee5\u540c\u65f6\u63d0\u5347\u65b0\u4efb\u52a1\u7684\u6027\u80fd\u4e14\u5c3d\u53ef\u80fd\u51cf\u5c11\u5bf9\u65e7\u4efb\u52a1\u7684\u6027\u80fd. Overcoming catastrophic forgetting in neural networks pdf \u672c\u6587\u4e3a\u4e00\u4e2a\u5f00\u5c71\u9f3b\u7956\u3002\u9996\u5148\u4ece\u76f4\u89c9\u6765\u8bf4\uff0c\u5728\u6ca1\u6709\u65e7\u4efb\u52a1\u6570\u636e\u7684\u524d\u63d0\u4e0b\u8981\u540c\u65f6\u5b9e\u73b0\u65e7\u4efb\u52a1\uff0c\u53c8\u80fd\u8bad\u7ec3\u65b0\u4efb\u52a1\uff0c\u6700naive\u7684\u65b9\u6cd5\u5c31\u662f\u8bbe\u8ba1\u635f\u5931\u51fd\u6570 \\mathcal{L} = \\mathcal{L}_B + \\lambda \\mathcal{L}_{dist_{\\theta_A}} \u5176\u4e2d\u524d\u8005\u4e3a\u5f53\u524d\u4efb\u52a1\u8bad\u7ec3\u65f6\u7684\u4e00\u4e2a\u635f\u5931\u51fd\u6570\uff0c\u540e\u8005\u4e3a\u4e0e\u539f\u6765\u8bad\u7ec3\u597d\u7684\u53c2\u6570 \\theta_A \u7684\u67d0\u4e00\u4e2a\u8ddd\u79bb\u51fd\u6570\uff0c\u7ea6\u675f\u7f51\u7edc\u4e0d\u8981\u79bb\u5f00\u539f\u6765\u7684\u6a21\u578b\u592a\u8fdc\u3002 \u672c\u6587\u9996\u5148\u81ea\u7136\u5730\u8fdb\u884c\u7b2c\u4e00\u4e2a\u5ef6\u4f38: \u5728 \\theta_A \u4e2d\u4e0d\u540c\u7684\u53c2\u6570\u9700\u8981\u7ea6\u675f\u7684\u6743\u91cd\u53ef\u4ee5\u662f\u4e0d\u540c\u7684 \uff0c\u56e0\u4e3a\u6709\u91cd\u8981\u7684\u53c2\u6570\u4e5f\u6709\u6ca1\u90a3\u4e48\u91cd\u8981\u7684\u53c2\u6570. \u4e3a\u4e86\u8bc4\u4ef7\u7684\u6743\u91cd\u53c2\u6570\uff0c\u672c\u6587\u7528\u6982\u7387\u8bba\u5bf9\u8fd9\u4e2a\u95ee\u9898\u8fdb\u884c\u4e86\u95ee\u9898, \u9996\u5148: \\begin{aligned} \\log p(\\theta | D_A, D_B) &= \\log p(D_B | \\theta, D_A) + \\log p(\\theta | D_A) - \\log p(D_B | D_A) \\\\ &= \\log p(D_B | \\theta) + \\log p (\\theta | D_A) + \\text{const} \\\\ & = \\log p (D_B | \\theta) + \\log p(D_A | \\theta) + \\log p(\\theta) - \\log p(D_A) + \\text{const} \\\\ & = \\log p (D_B | \\theta) + \\log p(D_A | \\theta) + \\text{const}\\\\ \\end{aligned} \u7b2c\u4e00\u9879: \u5bf9\u4e8e\u57fa\u4e8e\u72ec\u7acb\u540c\u5206\u5e03\u7684\u5206\u7c7b\u95ee\u9898\u6765\u8bf4\uff0c\u7b2c\u4e00\u9879\u7b49\u540c\u4e8e \u5bf9\u6bcf\u4e00\u4e2a\u6570\u636e\uff0c \u7528\u7f51\u7edc\u8ba1\u7b97\u5176\u5f97\u5230\u5f53\u524d\u6807\u7b7e\u7c7b\u7684\u6982\u7387. \u8fd9\u4e2a\u4e0e\u5e38\u7528\u7684\u635f\u5931\u51fd\u6570Entropy\u662f\u4e00\u81f4\u7684; \u5bf9\u4e8e\u57fa\u4e8e\u72ec\u7acb\u5206\u5e03\u4e14\u5e26\u6709\u9ad8\u65af\u566a\u58f0\u7684\u56de\u5f52\u95ee\u9898\u6765\u8bf4, \u7b2c\u4e00\u9879\u5219\u7b49\u540c\u4e8e \u52a0\u4e0a\u56fa\u5b9a\u6743\u91cd\u7684MSE \u635f\u5931. \u7b2c\u4e8c\u9879: \u7531\u4e8e\u5728\u8bad\u7ec3\u7b2c\u4e8c\u4e2a\u4efb\u52a1\u7684\u65f6\u5019\u5931\u53bb\u4e86\u7b2c\u4e00\u4e2a\u4efb\u52a1\uff0c\u8fd9\u4e2a\u51fd\u6570\u53d8\u5f97\u4e0d\u53ef\u8ba1\u7b97\u4e86\uff0c\u4f5c\u8005\u63d0\u51fa\u4f7f\u7528\u6cf0\u52d2\u5c55\u5f00\u516c\u5f0f\u5728\u6b64\u524d\u4f18\u5316\u7ed3\u679c\u7684\u6700\u4f18\u503c \\theta_A^* \u9644\u8fd1\u5c55\u5f00, \u4f7f\u7528\u4e00\u4e2a\u4e8c\u6b21\u51fd\u6570\u5bf9\u5176\u8fdb\u884c\u8fd1\u4f3c. \u8fd9\u6709\u70b9\u50cf\u51f8\u4f18\u5316/\u4e00\u6b21\u4f18\u5316\u91cc\u9762\u7684 Proximal Gradient Method . $$ \\begin{aligned} \\log p(D_A | \\theta) &= \\log p(D_A | \\theta^ A) + \\frac{\\partial}{\\partial \\theta} \\log p(D_A | \\theta) | {\\theta=\\theta_A^ } (\\theta - \\theta_A^ ) + \\frac{\\partial^2}{\\partial\\theta^2} \\log p(D_A|\\theta)(\\theta - \\theta_A^ )^2\\ &= \\frac{\\partial^2}{\\partial\\theta^2} \\log p(D_A|\\theta)(\\theta - \\theta_A^*)^2 + \\text{const}\\\\ &= -\\mathbb{F}(D_A, \\theta_A^*)(\\theta - \\theta_A^*)^2 \\end{aligned} $$ \u53ef\u4ee5\u6ce8\u610f\u5230: \u7531\u4e8e \\theta_A^* \u662f\u6700\u4f18\u89e3\u4e86\uff0c\u6240\u4ee5\u4e00\u9636\u5bfc\u9879\u4e3a\u96f6 \u5de5\u7a0b\u6280\u5de7\uff0c\u8fd9\u91cc\u7b80\u5316\u4e86Hessian\u77e9\u9635\u4e3a\u5bf9\u89d2\u77e9\u9635,\u4e0d\u8003\u8651\u76f8\u5173\u9879. Fisher Infomation Matrix \u8fd1\u4f3c wiki \\begin{aligned} \\mathbb{F} &= - \\mathbb{E}_x \\left[ \\frac{\\partial^2}{\\partial \\theta^2} \\log p(x|\\theta)\\right] = \\frac{\\partial^2}{\\partial \\theta^2} \\log p(D|\\theta)\\\\ &= -\\mathbb{E}_x \\left[\\frac{\\frac{\\partial^2}{\\partial\\theta^2}p(x|\\theta)}{p(x|\\theta)} - \\left( \\frac{\\frac{\\partial}{\\partial\\theta} p(x|\\theta)}{p(x|\\theta)}\\right)^2\\right] \\\\ &= -\\mathbb{E}_x\\left[\\frac{\\frac{\\partial^2}{\\partial\\theta^2}p(x|\\theta)}{p(x|\\theta)} \\right] + \\mathbb{E}_x\\left[\\left(\\frac{\\partial}{\\partial\\theta} \\log p(x|\\theta)\\right)^2 \\right] \\\\ &= \\frac{\\partial^2}{\\partial\\theta^2}\\int p(x|\\theta) dx + \\left(\\frac{\\partial}{\\partial\\theta}\\log p(D|\\theta) \\right)^2 \\\\ & = \\left(\\frac{\\partial}{\\partial\\theta}\\log p(D|\\theta) \\right)^2 \\end{aligned} \u8fd9\u4e2a\u7b49\u5f0f\u7684\u5173\u952e\u610f\u601d\u5728\u4e8e\u53ef\u4ee5\u4f7f\u7528\u4ece\u6570\u636e\u91c7\u6837\u4e00\u9636\u5bfc\u7684\u5e73\u65b9\u53bb\u8fd1\u4f3c\u4e8c\u9636\u5bfc\u7684\u503c\u3002 EWC: Elastic weight consolidation \u56e0\u800c\u672c\u6587\u63d0\u51fa\u7684 EWC \u7b97\u6cd5: \u5728\u5b8c\u6210\u4e00\u4e2a\u4efb\u52a1\u7684\u5b66\u4e60\u540e\uff0c\u8bb0\u5f55\u4e0b\u5b83\u7684Fisher information matrix\u4ee5\u53ca\u5f53\u65f6\u7684\u6a21\u578b\uff0c\u5728\u9762\u5bf9\u65b0\u4efb\u52a1\u7684\u65f6\u5019\uff0c\u4f7f\u7528\u635f\u5931\u51fd\u6570: \\mathcal{L}(\\theta) = \\mathcal{L}_B(\\theta) + \\sum_i\\frac{\\lambda}{2} F_i(\\theta_i - \\theta_{A,i}^*)^2 On Quadratic Penalties in Elastic Weight Consolidation pdf \u8fd9\u7bc7paper\u63a5\u7740\u524d\u6587\u7684\u6982\u5ff5\uff0c\u8fdb\u4e00\u6b65\u7ed9\u51fa\u4e86\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u8fed\u4ee3\u7684\u516c\u5f0f\uff0c\u4e5f\u5c31\u662f\u5f53\u8bad\u7ec3\u5b8c\u7b2c\u4e8c\u4e2a\u4efb\u52a1\u4e4b\u540e\u7ee7\u7eed\u5f80\u7b2c\u4e09\u3001\u56db\u3001...\u3001 t \u4e2a\u4efb\u52a1\u65f6\u7684\u4f7f\u7528\u7684\u516c\u5f0f. \u8fd9\u91cc\u9996\u5148\u4ee5\u4e09\u4e2a\u4efb\u52a1\u4e3a\u4f8b\u5b50: \\log p\\left(\\theta \\mid \\mathcal{D}_{A}, \\mathcal{D}_{B}, \\mathcal{D}_{C}\\right)=\\log p\\left(\\mathcal{D}_{C} \\mid \\theta\\right)+\\log p\\left(\\theta \\mid \\mathcal{D}_{A}, \\mathcal{D}_{B}\\right)+\\text { constant. } \u5176\u4e2d $$ \\begin{aligned} \\log p\\left(\\theta \\mid \\mathcal{D}_{A}, \\mathcal{D}_{B}\\right) \\approx &\\frac{\\partial}{\\partial \\theta}\\left[\\log p\\left(\\mathcal{D}_{B} \\mid \\boldsymbol{\\theta}\\right)+\\log p\\left(\\boldsymbol{\\theta} \\mid \\mathcal{D}_{A}\\right)\\right]|_{\\boldsymbol{\\theta}_{A B}^{*}}\\left(\\boldsymbol{\\theta}-\\boldsymbol{\\theta}_{A B}^{*}\\right) \\\\ &+\\frac{1}{2}\\left\\{\\left(\\boldsymbol{\\theta}-\\boldsymbol{\\theta}_{A B}^{*}\\right)^{T} \\frac{\\partial^{2}}{\\partial^{2} \\boldsymbol{\\theta}}\\left[\\log p\\left(\\mathcal{D}_{B} \\mid \\boldsymbol{\\theta}\\right)+\\log p\\left(\\boldsymbol{\\theta} \\mid \\mathcal{D}_{A}\\right)\\right] \\mid \\boldsymbol{\\theta}_{A B}^{*}\\right.\\\\ &\\left.\\times\\left(\\boldsymbol{\\theta}-\\boldsymbol{\\theta}_{A B}^{*}\\right)\\right\\}+\\mathrm{const.} \\end{aligned} $$ \u6ce8\u610f\u8fd9\u662f\u5728 \\theta_{AB^*} \u5c55\u5f00\u7684. \u7559\u610f\u5230: \u7b2c\u4e00\u9879\u4e00\u9636\u5bfc\u4f9d\u7136\u4e3a\u96f6\uff0c\u7b2c\u4e8c\u9879\u4e8c\u9636\u5bfc\u91cc\u9762\u5728\u5220\u53bb\u5e38\u6570(\u8fd9\u4e2a\u5e38\u6570\u4f1a\u88ab\u4e8c\u9636\u5bfc\u6e05\u9664)\u540e\u53ef\u7406\u89e3\u4e3a\u4e24\u4e2a Fisher Information Matrix\u7684\u76f8\u52a0. \u4ee3\u5165\u5f97\u5230 \\log p\\left(\\theta \\mid \\mathcal{D}_{A}, \\mathcal{D}_{B}, \\mathcal{D}_{C}\\right) \\approx \\log p\\left(\\mathcal{D}_{C} \\mid \\theta\\right)-\\frac{1}{2} \\sum\\left(\\lambda_{A} F_{A, i}+\\lambda_{B} F_{B, i}\\right)\\left(\\theta_{i}-\\theta_{AB, i}^{*}\\right)^{2}+\\text { constant. } \u4f5c\u8005\u7ecf\u8fc7\u8fed\u4ee3\u540e\u7ed9\u51fa\u901a\u9879\u516c\u5f0f: \\theta_{T}^{*}=\\operatorname{argmin}_{\\theta}\\left\\{-\\log p\\left(\\mathcal{D}_{T} \\mid \\theta\\right)+\\frac{1}{2} \\sum_{i}\\left(\\sum_{t<T} \\lambda_{t} F_{t, i}+\\lambda_{\\text {prior }}\\right)\\left(\\theta_{i}-\\theta_{S, i}^{*}\\right)^{2}\\right\\} \u7ed9\u51fa\u4e00\u4e9b\u5173\u952e\u7684\u76f4\u89c9: \u6bcf\u6b21\u7684\u5c55\u5f00\u90fd\u662f\u57fa\u4e8e\u6700\u8fd1\u4e00\u6b21\u4efb\u52a1\u5f97\u5230\u7684\u6700\u4f18\u89e3\uff0c\u56e0\u800c\u53ea\u9700\u8981\u5b58\u4e0b\u4e00\u4e2a\u5148\u524dmodel. \u968f\u7740\u4efb\u52a1\u7684\u589e\u52a0,Fisher matrix\u4f1a\u53d8\u5f97\u8d8a\u6765\u8d8a\u5927\uff0c\u6a21\u578b\u4e5f\u4f1a\u8d8a\u6765\u8d8a\u96be\u5b66\u65b0\u7684\u7ed3\u679c\u3002\u8fd9\u4e5f\u662f\u4e0d\u53ef\u907f\u514d\u7684. \u524d\u9762\u4efb\u52a1\u5728\u540e\u9762\u7684\u4efb\u52a1\u7684head\u4e0a\u6ca1\u6709\u68af\u5ea6\uff0c\u56e0\u800c F_t t\\in [0,... \\tau-1] \u4e5f\u4e0d\u4f1a\u5f71\u54cd\u65b0\u7684head.\u6bcf\u4e2a\u4efb\u52a1\u81ea\u5df1\u7684head\u53ea\u4f1a\u6709\u4e00\u4e2a\u7ed3\u679c Memory Aware Synapses: Learning what (not) to forget pdf \u8fd9\u7bc7paper\u7684\u610f\u89c1\u66f4\u4e3a\u76f4\u63a5\uff0c\u5c31\u662f\u8bf4\u4e0d\u5e94\u8be5\u8003\u8651\u90a3\u4e2a\u6743\u91cd\u91cd\u8981\u90a3\u4e2a\u6743\u91cd\u4e0d\u91cd\u8981\uff0c\u5173\u952e\u5f97\u770b\u6700\u7ec8\u7684\u8f93\u51fa\u662f\u5426\u76f8\u4f3c. \u8bbe\u539f\u6765\u7684\u7f51\u7edc\u7684\u8f93\u51fa\u4e3a F(x_k;\\theta) , \u7ecf\u8fc7\u6270\u52a8\u540e\u7684\u8f93\u51fa\u53d8\u5316\u663e\u7136\u53ef\u4ee5\u7531\u8f93\u51fa\u503c\u5bf9\u6743\u91cd\u7684\u68af\u5ea6\uff0c\u4e00\u9636\u8fd1\u4f3c. \u6240\u4ee5\u6bcf\u4e00\u4e2a\u6743\u91cd\u7684 importance weight \u5c31\u662f \\Omega_{i j}=\\frac{1}{N} \\sum_{k=1}^{N}\\left\\|g_{i j}\\left(x_{k}\\right)\\right\\| \u4e0e\u4e4b\u524d\u7684\u8bbe\u5b9a\u4e0d\u540c\uff0c\u8fd9\u91cc\u7684importance weights. \u8fd9\u91cc\u91c7\u7528\u66f4\u4e3a\u7b80\u5355\u7684\u65b9\u6848: g_{ij}(x_k) = \\frac{\\partial l_2^2(F(x_k;\\theta))}{\\partial \\theta_{ij}} \u5177\u4f53\u5b9e\u73b0\u4e0a\uff0c\u5c31\u662f\u5728\u6bcf\u4e2a\u4efb\u52a1\u5b8c\u6210\u540e\uff0c\u8ba1\u7b97\u6bcf\u4e00\u4e2a\u6743\u91cd\u7684importance weights. \u5b66\u4e60\u65b0\u7684\u4efb\u52a1\u65f6: L(\\theta)=L_{n}(\\theta)+\\lambda \\sum_{i, j} \\Omega_{i j}\\left(\\theta_{i j}-\\theta_{i j}^{*}\\right)^{2} \u5c3d\u7ba1\u7406\u8bba\u6bd4\u8f83\u76f4\u767d\u6548\u679c\u751a\u81f3\u6bd4\u524d\u6587\u63d0\u5230\u7684\u66f4\u597d.","title":"Continuous Learning"},{"location":"The_theory/Continuous_learning/#continuous-learning","text":"\u8fd9\u4e2a\u9875\u9762\u4ecb\u7ecd continuous learning \u7b49\u76f8\u5173\u6587\u7ae0\uff0c\u8fd9\u4e2a\u5b9e\u73b0\u7684task\u662f\u8bf4: \u73b0\u6709\u4e00\u4e2a\u6a21\u578b\uff0c\u5728\u6570\u636e\u96c6 D_A \u4e0a\u4f7f\u7528\u4e00\u4e2abackbone \\Theta_A , \u4e00\u4e2ahead \\theta_A \u5b8c\u6210\u4e86\u5bf9\u7b2c\u4e00\u4e2a\u4efb\u52a1 A \u7684\u8bad\u7ec3\u3002\u7136\u540e\u73b0\u5728\u6211\u4eec\u5931\u53bb\u4e86\u6570\u636e\u96c6 D_A , \u4ee5 backbone \\Theta_A \u4e3a\u57fa\u7840\uff0c\u4e00\u4e2a\u65b0\u7684head \\theta_B \u8fdb\u884c\u8bad\u7ec3\uff0c\u5b9e\u73b0\u5bf9\u4efb\u52a1 B \u7684\u8bad\u7ec3\uff0c\u4f9d\u6b21\u8fed\u4ee3\u3002 \u76f4\u89c9\u53ef\u4ee5\u5224\u65ad\uff0c\u5982\u679c\u76f4\u63a5\u8bad\u7ec3 \\theta_B \u56fa\u5b9a \\Theta_A , \u5219\u539f\u6765\u7684\u4efb\u52a1\u4e0d\u4f1a\u5fd8\u8bb0\uff0c\u4f46\u662f\u7f51\u7edc\u5728 B \u4e0a\u7684\u6027\u80fd\u53ef\u80fd\u4f1a\u6709\u9650\u3002\u5982\u679c \\Theta_A \u4e0e \\theta_B \u90fd\u5728 B \u4e0a\u91cd\u65b0\u8bad\u7ec3\uff0c\u5219\u65e2\u662f\u662f\u4f4e\u5b66\u4e60\u7387\u7684fine-tuning \u90fd\u53ef\u80fd\u4f1a\u4f7f\u5f97\u7f51\u7edc\u5b8c\u5168\u7834\u574f\u4efb\u52a1 A \u7684\u7ed3\u679c\u3002 Continuous Learning\u7684\u4efb\u52a1\u5219\u662f\u8bbe\u8ba1\u5bf9\u7b56\uff0c\u4f7f\u5f97\u5728 B ,\u4e43\u81f3 C, D ... \u8bad\u7ec3\u65f6\u53ef\u4ee5\u540c\u65f6\u63d0\u5347\u65b0\u4efb\u52a1\u7684\u6027\u80fd\u4e14\u5c3d\u53ef\u80fd\u51cf\u5c11\u5bf9\u65e7\u4efb\u52a1\u7684\u6027\u80fd.","title":"Continuous Learning"},{"location":"The_theory/Continuous_learning/#overcoming-catastrophic-forgetting-in-neural-networks","text":"pdf \u672c\u6587\u4e3a\u4e00\u4e2a\u5f00\u5c71\u9f3b\u7956\u3002\u9996\u5148\u4ece\u76f4\u89c9\u6765\u8bf4\uff0c\u5728\u6ca1\u6709\u65e7\u4efb\u52a1\u6570\u636e\u7684\u524d\u63d0\u4e0b\u8981\u540c\u65f6\u5b9e\u73b0\u65e7\u4efb\u52a1\uff0c\u53c8\u80fd\u8bad\u7ec3\u65b0\u4efb\u52a1\uff0c\u6700naive\u7684\u65b9\u6cd5\u5c31\u662f\u8bbe\u8ba1\u635f\u5931\u51fd\u6570 \\mathcal{L} = \\mathcal{L}_B + \\lambda \\mathcal{L}_{dist_{\\theta_A}} \u5176\u4e2d\u524d\u8005\u4e3a\u5f53\u524d\u4efb\u52a1\u8bad\u7ec3\u65f6\u7684\u4e00\u4e2a\u635f\u5931\u51fd\u6570\uff0c\u540e\u8005\u4e3a\u4e0e\u539f\u6765\u8bad\u7ec3\u597d\u7684\u53c2\u6570 \\theta_A \u7684\u67d0\u4e00\u4e2a\u8ddd\u79bb\u51fd\u6570\uff0c\u7ea6\u675f\u7f51\u7edc\u4e0d\u8981\u79bb\u5f00\u539f\u6765\u7684\u6a21\u578b\u592a\u8fdc\u3002 \u672c\u6587\u9996\u5148\u81ea\u7136\u5730\u8fdb\u884c\u7b2c\u4e00\u4e2a\u5ef6\u4f38: \u5728 \\theta_A \u4e2d\u4e0d\u540c\u7684\u53c2\u6570\u9700\u8981\u7ea6\u675f\u7684\u6743\u91cd\u53ef\u4ee5\u662f\u4e0d\u540c\u7684 \uff0c\u56e0\u4e3a\u6709\u91cd\u8981\u7684\u53c2\u6570\u4e5f\u6709\u6ca1\u90a3\u4e48\u91cd\u8981\u7684\u53c2\u6570. \u4e3a\u4e86\u8bc4\u4ef7\u7684\u6743\u91cd\u53c2\u6570\uff0c\u672c\u6587\u7528\u6982\u7387\u8bba\u5bf9\u8fd9\u4e2a\u95ee\u9898\u8fdb\u884c\u4e86\u95ee\u9898, \u9996\u5148: \\begin{aligned} \\log p(\\theta | D_A, D_B) &= \\log p(D_B | \\theta, D_A) + \\log p(\\theta | D_A) - \\log p(D_B | D_A) \\\\ &= \\log p(D_B | \\theta) + \\log p (\\theta | D_A) + \\text{const} \\\\ & = \\log p (D_B | \\theta) + \\log p(D_A | \\theta) + \\log p(\\theta) - \\log p(D_A) + \\text{const} \\\\ & = \\log p (D_B | \\theta) + \\log p(D_A | \\theta) + \\text{const}\\\\ \\end{aligned} \u7b2c\u4e00\u9879: \u5bf9\u4e8e\u57fa\u4e8e\u72ec\u7acb\u540c\u5206\u5e03\u7684\u5206\u7c7b\u95ee\u9898\u6765\u8bf4\uff0c\u7b2c\u4e00\u9879\u7b49\u540c\u4e8e \u5bf9\u6bcf\u4e00\u4e2a\u6570\u636e\uff0c \u7528\u7f51\u7edc\u8ba1\u7b97\u5176\u5f97\u5230\u5f53\u524d\u6807\u7b7e\u7c7b\u7684\u6982\u7387. \u8fd9\u4e2a\u4e0e\u5e38\u7528\u7684\u635f\u5931\u51fd\u6570Entropy\u662f\u4e00\u81f4\u7684; \u5bf9\u4e8e\u57fa\u4e8e\u72ec\u7acb\u5206\u5e03\u4e14\u5e26\u6709\u9ad8\u65af\u566a\u58f0\u7684\u56de\u5f52\u95ee\u9898\u6765\u8bf4, \u7b2c\u4e00\u9879\u5219\u7b49\u540c\u4e8e \u52a0\u4e0a\u56fa\u5b9a\u6743\u91cd\u7684MSE \u635f\u5931. \u7b2c\u4e8c\u9879: \u7531\u4e8e\u5728\u8bad\u7ec3\u7b2c\u4e8c\u4e2a\u4efb\u52a1\u7684\u65f6\u5019\u5931\u53bb\u4e86\u7b2c\u4e00\u4e2a\u4efb\u52a1\uff0c\u8fd9\u4e2a\u51fd\u6570\u53d8\u5f97\u4e0d\u53ef\u8ba1\u7b97\u4e86\uff0c\u4f5c\u8005\u63d0\u51fa\u4f7f\u7528\u6cf0\u52d2\u5c55\u5f00\u516c\u5f0f\u5728\u6b64\u524d\u4f18\u5316\u7ed3\u679c\u7684\u6700\u4f18\u503c \\theta_A^* \u9644\u8fd1\u5c55\u5f00, \u4f7f\u7528\u4e00\u4e2a\u4e8c\u6b21\u51fd\u6570\u5bf9\u5176\u8fdb\u884c\u8fd1\u4f3c. \u8fd9\u6709\u70b9\u50cf\u51f8\u4f18\u5316/\u4e00\u6b21\u4f18\u5316\u91cc\u9762\u7684 Proximal Gradient Method . $$ \\begin{aligned} \\log p(D_A | \\theta) &= \\log p(D_A | \\theta^ A) + \\frac{\\partial}{\\partial \\theta} \\log p(D_A | \\theta) | {\\theta=\\theta_A^ } (\\theta - \\theta_A^ ) + \\frac{\\partial^2}{\\partial\\theta^2} \\log p(D_A|\\theta)(\\theta - \\theta_A^ )^2\\ &= \\frac{\\partial^2}{\\partial\\theta^2} \\log p(D_A|\\theta)(\\theta - \\theta_A^*)^2 + \\text{const}\\\\ &= -\\mathbb{F}(D_A, \\theta_A^*)(\\theta - \\theta_A^*)^2 \\end{aligned} $$ \u53ef\u4ee5\u6ce8\u610f\u5230: \u7531\u4e8e \\theta_A^* \u662f\u6700\u4f18\u89e3\u4e86\uff0c\u6240\u4ee5\u4e00\u9636\u5bfc\u9879\u4e3a\u96f6 \u5de5\u7a0b\u6280\u5de7\uff0c\u8fd9\u91cc\u7b80\u5316\u4e86Hessian\u77e9\u9635\u4e3a\u5bf9\u89d2\u77e9\u9635,\u4e0d\u8003\u8651\u76f8\u5173\u9879. Fisher Infomation Matrix \u8fd1\u4f3c wiki \\begin{aligned} \\mathbb{F} &= - \\mathbb{E}_x \\left[ \\frac{\\partial^2}{\\partial \\theta^2} \\log p(x|\\theta)\\right] = \\frac{\\partial^2}{\\partial \\theta^2} \\log p(D|\\theta)\\\\ &= -\\mathbb{E}_x \\left[\\frac{\\frac{\\partial^2}{\\partial\\theta^2}p(x|\\theta)}{p(x|\\theta)} - \\left( \\frac{\\frac{\\partial}{\\partial\\theta} p(x|\\theta)}{p(x|\\theta)}\\right)^2\\right] \\\\ &= -\\mathbb{E}_x\\left[\\frac{\\frac{\\partial^2}{\\partial\\theta^2}p(x|\\theta)}{p(x|\\theta)} \\right] + \\mathbb{E}_x\\left[\\left(\\frac{\\partial}{\\partial\\theta} \\log p(x|\\theta)\\right)^2 \\right] \\\\ &= \\frac{\\partial^2}{\\partial\\theta^2}\\int p(x|\\theta) dx + \\left(\\frac{\\partial}{\\partial\\theta}\\log p(D|\\theta) \\right)^2 \\\\ & = \\left(\\frac{\\partial}{\\partial\\theta}\\log p(D|\\theta) \\right)^2 \\end{aligned} \u8fd9\u4e2a\u7b49\u5f0f\u7684\u5173\u952e\u610f\u601d\u5728\u4e8e\u53ef\u4ee5\u4f7f\u7528\u4ece\u6570\u636e\u91c7\u6837\u4e00\u9636\u5bfc\u7684\u5e73\u65b9\u53bb\u8fd1\u4f3c\u4e8c\u9636\u5bfc\u7684\u503c\u3002","title":"Overcoming catastrophic forgetting in neural networks"},{"location":"The_theory/Continuous_learning/#ewc-elastic-weight-consolidation","text":"\u56e0\u800c\u672c\u6587\u63d0\u51fa\u7684 EWC \u7b97\u6cd5: \u5728\u5b8c\u6210\u4e00\u4e2a\u4efb\u52a1\u7684\u5b66\u4e60\u540e\uff0c\u8bb0\u5f55\u4e0b\u5b83\u7684Fisher information matrix\u4ee5\u53ca\u5f53\u65f6\u7684\u6a21\u578b\uff0c\u5728\u9762\u5bf9\u65b0\u4efb\u52a1\u7684\u65f6\u5019\uff0c\u4f7f\u7528\u635f\u5931\u51fd\u6570: \\mathcal{L}(\\theta) = \\mathcal{L}_B(\\theta) + \\sum_i\\frac{\\lambda}{2} F_i(\\theta_i - \\theta_{A,i}^*)^2","title":"EWC: Elastic weight consolidation"},{"location":"The_theory/Continuous_learning/#on-quadratic-penalties-in-elastic-weight-consolidation","text":"pdf \u8fd9\u7bc7paper\u63a5\u7740\u524d\u6587\u7684\u6982\u5ff5\uff0c\u8fdb\u4e00\u6b65\u7ed9\u51fa\u4e86\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u8fed\u4ee3\u7684\u516c\u5f0f\uff0c\u4e5f\u5c31\u662f\u5f53\u8bad\u7ec3\u5b8c\u7b2c\u4e8c\u4e2a\u4efb\u52a1\u4e4b\u540e\u7ee7\u7eed\u5f80\u7b2c\u4e09\u3001\u56db\u3001...\u3001 t \u4e2a\u4efb\u52a1\u65f6\u7684\u4f7f\u7528\u7684\u516c\u5f0f. \u8fd9\u91cc\u9996\u5148\u4ee5\u4e09\u4e2a\u4efb\u52a1\u4e3a\u4f8b\u5b50: \\log p\\left(\\theta \\mid \\mathcal{D}_{A}, \\mathcal{D}_{B}, \\mathcal{D}_{C}\\right)=\\log p\\left(\\mathcal{D}_{C} \\mid \\theta\\right)+\\log p\\left(\\theta \\mid \\mathcal{D}_{A}, \\mathcal{D}_{B}\\right)+\\text { constant. } \u5176\u4e2d $$ \\begin{aligned} \\log p\\left(\\theta \\mid \\mathcal{D}_{A}, \\mathcal{D}_{B}\\right) \\approx &\\frac{\\partial}{\\partial \\theta}\\left[\\log p\\left(\\mathcal{D}_{B} \\mid \\boldsymbol{\\theta}\\right)+\\log p\\left(\\boldsymbol{\\theta} \\mid \\mathcal{D}_{A}\\right)\\right]|_{\\boldsymbol{\\theta}_{A B}^{*}}\\left(\\boldsymbol{\\theta}-\\boldsymbol{\\theta}_{A B}^{*}\\right) \\\\ &+\\frac{1}{2}\\left\\{\\left(\\boldsymbol{\\theta}-\\boldsymbol{\\theta}_{A B}^{*}\\right)^{T} \\frac{\\partial^{2}}{\\partial^{2} \\boldsymbol{\\theta}}\\left[\\log p\\left(\\mathcal{D}_{B} \\mid \\boldsymbol{\\theta}\\right)+\\log p\\left(\\boldsymbol{\\theta} \\mid \\mathcal{D}_{A}\\right)\\right] \\mid \\boldsymbol{\\theta}_{A B}^{*}\\right.\\\\ &\\left.\\times\\left(\\boldsymbol{\\theta}-\\boldsymbol{\\theta}_{A B}^{*}\\right)\\right\\}+\\mathrm{const.} \\end{aligned} $$ \u6ce8\u610f\u8fd9\u662f\u5728 \\theta_{AB^*} \u5c55\u5f00\u7684. \u7559\u610f\u5230: \u7b2c\u4e00\u9879\u4e00\u9636\u5bfc\u4f9d\u7136\u4e3a\u96f6\uff0c\u7b2c\u4e8c\u9879\u4e8c\u9636\u5bfc\u91cc\u9762\u5728\u5220\u53bb\u5e38\u6570(\u8fd9\u4e2a\u5e38\u6570\u4f1a\u88ab\u4e8c\u9636\u5bfc\u6e05\u9664)\u540e\u53ef\u7406\u89e3\u4e3a\u4e24\u4e2a Fisher Information Matrix\u7684\u76f8\u52a0. \u4ee3\u5165\u5f97\u5230 \\log p\\left(\\theta \\mid \\mathcal{D}_{A}, \\mathcal{D}_{B}, \\mathcal{D}_{C}\\right) \\approx \\log p\\left(\\mathcal{D}_{C} \\mid \\theta\\right)-\\frac{1}{2} \\sum\\left(\\lambda_{A} F_{A, i}+\\lambda_{B} F_{B, i}\\right)\\left(\\theta_{i}-\\theta_{AB, i}^{*}\\right)^{2}+\\text { constant. } \u4f5c\u8005\u7ecf\u8fc7\u8fed\u4ee3\u540e\u7ed9\u51fa\u901a\u9879\u516c\u5f0f: \\theta_{T}^{*}=\\operatorname{argmin}_{\\theta}\\left\\{-\\log p\\left(\\mathcal{D}_{T} \\mid \\theta\\right)+\\frac{1}{2} \\sum_{i}\\left(\\sum_{t<T} \\lambda_{t} F_{t, i}+\\lambda_{\\text {prior }}\\right)\\left(\\theta_{i}-\\theta_{S, i}^{*}\\right)^{2}\\right\\} \u7ed9\u51fa\u4e00\u4e9b\u5173\u952e\u7684\u76f4\u89c9: \u6bcf\u6b21\u7684\u5c55\u5f00\u90fd\u662f\u57fa\u4e8e\u6700\u8fd1\u4e00\u6b21\u4efb\u52a1\u5f97\u5230\u7684\u6700\u4f18\u89e3\uff0c\u56e0\u800c\u53ea\u9700\u8981\u5b58\u4e0b\u4e00\u4e2a\u5148\u524dmodel. \u968f\u7740\u4efb\u52a1\u7684\u589e\u52a0,Fisher matrix\u4f1a\u53d8\u5f97\u8d8a\u6765\u8d8a\u5927\uff0c\u6a21\u578b\u4e5f\u4f1a\u8d8a\u6765\u8d8a\u96be\u5b66\u65b0\u7684\u7ed3\u679c\u3002\u8fd9\u4e5f\u662f\u4e0d\u53ef\u907f\u514d\u7684. \u524d\u9762\u4efb\u52a1\u5728\u540e\u9762\u7684\u4efb\u52a1\u7684head\u4e0a\u6ca1\u6709\u68af\u5ea6\uff0c\u56e0\u800c F_t t\\in [0,... \\tau-1] \u4e5f\u4e0d\u4f1a\u5f71\u54cd\u65b0\u7684head.\u6bcf\u4e2a\u4efb\u52a1\u81ea\u5df1\u7684head\u53ea\u4f1a\u6709\u4e00\u4e2a\u7ed3\u679c","title":"On Quadratic Penalties in Elastic Weight Consolidation"},{"location":"The_theory/Continuous_learning/#memory-aware-synapses-learning-what-not-to-forget","text":"pdf \u8fd9\u7bc7paper\u7684\u610f\u89c1\u66f4\u4e3a\u76f4\u63a5\uff0c\u5c31\u662f\u8bf4\u4e0d\u5e94\u8be5\u8003\u8651\u90a3\u4e2a\u6743\u91cd\u91cd\u8981\u90a3\u4e2a\u6743\u91cd\u4e0d\u91cd\u8981\uff0c\u5173\u952e\u5f97\u770b\u6700\u7ec8\u7684\u8f93\u51fa\u662f\u5426\u76f8\u4f3c. \u8bbe\u539f\u6765\u7684\u7f51\u7edc\u7684\u8f93\u51fa\u4e3a F(x_k;\\theta) , \u7ecf\u8fc7\u6270\u52a8\u540e\u7684\u8f93\u51fa\u53d8\u5316\u663e\u7136\u53ef\u4ee5\u7531\u8f93\u51fa\u503c\u5bf9\u6743\u91cd\u7684\u68af\u5ea6\uff0c\u4e00\u9636\u8fd1\u4f3c. \u6240\u4ee5\u6bcf\u4e00\u4e2a\u6743\u91cd\u7684 importance weight \u5c31\u662f \\Omega_{i j}=\\frac{1}{N} \\sum_{k=1}^{N}\\left\\|g_{i j}\\left(x_{k}\\right)\\right\\| \u4e0e\u4e4b\u524d\u7684\u8bbe\u5b9a\u4e0d\u540c\uff0c\u8fd9\u91cc\u7684importance weights. \u8fd9\u91cc\u91c7\u7528\u66f4\u4e3a\u7b80\u5355\u7684\u65b9\u6848: g_{ij}(x_k) = \\frac{\\partial l_2^2(F(x_k;\\theta))}{\\partial \\theta_{ij}} \u5177\u4f53\u5b9e\u73b0\u4e0a\uff0c\u5c31\u662f\u5728\u6bcf\u4e2a\u4efb\u52a1\u5b8c\u6210\u540e\uff0c\u8ba1\u7b97\u6bcf\u4e00\u4e2a\u6743\u91cd\u7684importance weights. \u5b66\u4e60\u65b0\u7684\u4efb\u52a1\u65f6: L(\\theta)=L_{n}(\\theta)+\\lambda \\sum_{i, j} \\Omega_{i j}\\left(\\theta_{i j}-\\theta_{i j}^{*}\\right)^{2} \u5c3d\u7ba1\u7406\u8bba\u6bd4\u8f83\u76f4\u767d\u6548\u679c\u751a\u81f3\u6bd4\u524d\u6587\u63d0\u5230\u7684\u66f4\u597d.","title":"Memory Aware Synapses: Learning what (not) to forget"},{"location":"The_theory/ConvexOptimization/","text":"Convex Optimization boyd's book \u672c\u6587\u4e3b\u8981\u6536\u96c6\u5728\u51f8\u4f18\u5316\u8bfe\u7a0b\u4e2d\u8bb0\u4e0b\u7684\u57fa\u7840\u6982\u5ff5\u4e0e\u7ed3\u8bba\u3002 Convex Optimization \u51f8\u51fd\u6570\u4e0e\u51f8\u96c6 \u57fa\u672c\u5b9a\u4e49\u4e0e\u91cd\u8981\u5224\u5b9a \u5e38\u89c1\u800c\u91cd\u8981\u7684\u51f9\u51f8\u51fd\u6570\u51fd\u6570 \u4fdd\u7559\u51f8\u7279\u6027\u7684\u5e38\u89c1\u64cd\u4f5c \u51f8\u4f18\u5316\u95ee\u9898 \u5c06 L_1 , L_{\\infty} \u8f6c\u6362\u4e3a\u51f8\u4f18\u5316\u95ee\u9898 \u5bf9\u5076\u4e0eKKT\u6761\u4ef6 \u5bf9\u5076\u51fd\u6570 \u5f3a\u5f31\u5bf9\u5076\u6027 KKT \u6761\u4ef6 CVX\u5e93 \u6700\u4f18\u5316\u7b97\u6cd5 \u725b\u987f\u6cd5 \u5185\u70b9\u6cd5 Block Coordinate Descent (BCD) Majorization-Minimization Algorithm (MM Algo.) \u5bfb\u627e surrogate function\u7684\u6280\u5de7 Convexity: Taylor, upper bound with quadratic terms \u4e8c\u6b21\u77e9\u9635\u5f0f (Lecturer \u7684\u6210\u679c) Geometric programming \u6ee4\u6ce2\u5668\u8bbe\u8ba1 \u5207\u6bd4\u96ea\u592b\u6ee4\u6ce2\u5668\u8bbe\u8ba1 magnitude \u4f18\u5316\u95ee\u9898 Log-\u5207\u6bd4\u96ea\u592b \u5e45\u503c\u8bbe\u8ba1 Markowitz Modern Portfolio Theory \u57fa\u7840\u95ee\u9898\u4e00: mean-variance portfolio (MVP) \u57fa\u7840\u95ee\u9898\u4e8c: Global minimum variance portfolio \u56f0\u96be\u95ee\u9898: Maximum Sharpe ratio portfolio (MSRP) \u4e8c\u5206\u6c42\u89e3 Dinkelbach Transform Charnes-Cooper Transform / Schaible transform Constrained Problem Optimized for Sparsity Iterative Reweighted l1-Norm Heuristic \u76f4\u89c9: \u7406\u8bba\u5206\u6790 Sparse Index Tracking Sparse Regression \u51f8\u51fd\u6570\u4e0e\u51f8\u96c6 \u57fa\u672c\u5b9a\u4e49\u4e0e\u91cd\u8981\u5224\u5b9a \u51f8\u51fd\u6570\u8981\u6c42\u51fd\u6570\u4e0a\uff0c\u4efb\u610f\u4e24\u70b9\u95f4\u7684\u51fd\u6570\u503c\u7684\u7ebf\u6027\u63d2\u503c\u8981\u4e0d\u5c0f\u4e8e\u7ebf\u6027\u63d2\u503c\u7684\u51fd\u6570\u503c. \\forall \\theta\\in[0,1] \\quad f(\\theta x + (1-\\theta)y) \\le \\theta f(x) + (1-\\theta) f(y) \u51f8\u96c6\u8981\u6c42\u96c6\u5408\u5185\u4e24\u70b9\u8fde\u7ebf\u4e0a\u7684\u4efb\u610f\u70b9\u90fd\u5728\u8be5\u96c6\u5408\u5185 \u51f8\u51fd\u6570\u7684\u4e00\u4e2a\u5145\u5206\u6761\u4ef6\u662f\u4e8c\u6b21\u5bfc\u4e0d\u5c0f\u4e8e\u96f6\uff0c\u4e25\u683c\u6765\u8bf4\u662f\u5bf9\u5404\u53d8\u91cf\u7684Hessian Matrix\u4e3a\u534a\u6b63\u5b9a. \u5e38\u89c1\u800c\u91cd\u8981\u7684\u51f9\u51f8\u51fd\u6570\u51fd\u6570 \u7ebf\u6027\u51fd\u6570\u540c\u65f6\u4e3a\u51f9\u3001\u51f8\u51fd\u6570 \u6240\u6709\u7684 norm (\u6ee1\u8db3\u4e09\u89d2\u5f62\u4e0d\u7b49\u5f0f)\u90fd\u662f\u51f8\u51fd\u6570 \u4e8c\u6b21\u51fd\u6570 f(x) = x^TPx + 2q^Tx + r , if P \\ge 0 \u662f\u51f8\u51fd\u6570. \u51e0\u4f55\u5e73\u5747 f(x) = (\\Pi^n_{i=1} x_i)^{1/n} \u662f\u51f9\u51fd\u6570 log-sum-exp f(x) = \\log{\\sum_i e^{x_i}} \u4e3a\u51f8\u51fd\u6570 quadratic over linear f(x,y) = x^2 / y \u4e3a\u51f8\u51fd\u6570 log-det f(X) = logdet(X) \u4e3a\u51f9\u51fd\u6570 (\u8bc1\u660e\u6bd4\u8f83\u53d6\u5de7\uff0c\u53d6\u5b9a\u4e49\u57df\u5185\u4efb\u610f\u4e24\u70b9\uff0c\u8bc1\u660e\u76f4\u7ebf\u4e0a\u63d2\u503c\u6bd4\u4f8b\u4e3a\u53d8\u91cf\u7684\u51fd\u6570\u90fd\u4e3a\u51f9\u51fd\u6570\uff0c\u5219\u6574\u4e2a\u51fd\u6570\u4e3a\u51f9\u51fd\u6570) \u77e9\u9635\u7684\u6700\u5927\u7279\u5f81\u503c f(X) = \\lambda_{max}(X) = \\underset{||y||_2=1}{max}y^TXy \u4e3a\u51f8\u51fd\u6570 (\u7531\u6700\u5927\u503c\u6027\u8d28) \u4fdd\u7559\u51f8\u7279\u6027\u7684\u5e38\u89c1\u64cd\u4f5c \u51f8\u51fd\u6570\u7684\u975e\u8d1f\u6743\u91cd\u6c42\u548c \u4e0e\u7ebf\u6027\u51fd\u6570\u7684\u5d4c\u5957 f(Ax + b) \u591a\u4e2a\u51f8\u51fd\u6570\u7684point-wise maximum. \u51f8\u51fd\u6570\u4e2d\u90e8\u5206\u53d8\u91cf\u7684maximum g(x) = \\underset{y\\in \\mathcal{A}}{sup}f(x, y) \u51f8\u4f18\u5316\u95ee\u9898 \\begin{array}{lll} \\underset{x}{\\operatorname{minimize}} & f_{0}(x) \\\\ \\text { subject to } & f_{i}(x) \\leq 0 & i=1, \\ldots, m \\\\ & h_{i}(x)=0 & i=1, \\ldots, p \\end{array} \u8981\u6c42: f_0(x) \u4e3a\u51f8\u51fd\u6570 f_i(x) \u4e3a\u51f8\u51fd\u6570 h_i(x) \u4e3a\u7ebf\u6027\u51fd\u6570\uff0c\u4e5f\u5c31\u662f\u53ea\u5141\u8bb8 Ax = b \u4e3b\u8981\u6027\u8d28: \u51f8\u4f18\u5316\u7684\u5c40\u90e8\u6700\u4f18\u7b49\u4e8e\u5168\u5c40\u6700\u4f18\uff0c\u4e14\u552f\u4e00. \u5c06 L_1 , L_{\\infty} \u8f6c\u6362\u4e3a\u51f8\u4f18\u5316\u95ee\u9898 L_{\\infty} \u95ee\u9898\u53ef\u4ee5\u5982\u6b64\u8f6c\u6362: \\begin{array}{ll} \\underset{x}{\\operatorname{minimize}} & \\|x\\|_{\\infty} \\\\ \\text { subject to } & G x \\leq h \\\\ & A x=b \\end{array} \u53d8\u6210 \\begin{array}{ll} \\underset{t, x}{\\operatorname{minimize}} & t \\\\ \\text { subject to } & -t \\mathbf{1} \\leq x \\leq t \\mathbf{1} \\\\ & G x \\leq h \\\\ & A x=b \\end{array} L_1 : \\begin{array}{ll} \\underset{x}{\\operatorname{minimize}} & \\|x\\|_{1} \\\\ \\text { subject to } & G x \\leq h \\\\ & A x=b \\end{array} \\begin{array}{ll} \\underset{t, x}{\\operatorname{minimize}} & \\sum_{i} t_{i} \\\\ \\text { subject to } & -t \\leq x \\leq t \\\\ & G x \\leq h \\\\ & A x=b \\end{array} \u5bf9\u5076\u4e0eKKT\u6761\u4ef6 \u5bf9\u4e8e\u4e00\u822c\u6700\u4f18\u5316\u95ee\u9898: \\begin{array}{lll} \\underset{x}{\\operatorname{minimize}} & f_{0}(x) \\\\ \\text { subject to } & f_{i}(x) \\leq 0 & i=1, \\ldots, m \\\\ & h_{i}(x)=0 & i=1, \\ldots, p \\end{array} Lagrangian\u4e3a: L(x, \\lambda, v) = f_0(x) + \\sum_i \\lambda_i f_i(x) + \\sum_i v_ih_i(x) \u5bf9\u5076\u51fd\u6570 g(\\lambda, v) = \\underset{x\\in D}{inf} L(x, \\lambda, v) \u65e0\u8bba\u539f\u6765\u7684\u95ee\u9898\u662f\u5426\u662f\u51f8\u4f18\u5316\u95ee\u9898\uff0c\u5bf9\u5076\u51fd\u6570 g \u4e00\u5b9a\u4e3a\u51f9\u51fd\u6570 (\u7ebf\u6027\u51fd\u6570\u7684inf\u4e0b\u754c\u7ec4\u5408)\uff0c\u56e0\u800c\u6700\u5927\u5316 g \u7ecf\u5e38\u662f\u4e00\u4e2a\u51f8\u4f18\u5316\u95ee\u9898. \u540c\u65f6 g(\\lambda, v) \\le p^* , \u5bf9\u5076\u51fd\u6570\u53ef\u4ee5\u8868\u5f81\u539f\u6700\u4f18\u5316\u95ee\u9898\u7684\u4f18\u5316\u4e0b\u754c\u3002 \u5bf9\u5076\u95ee\u9898\u5c31\u662f\u901a\u8fc7\u8ba1\u7b97 g \u7684\u6700\u5927\u503c(\u540c\u65f6\u6709\u7ea6\u675f \\lambda \\ge 0 )\uff0c\u6765\u5206\u6790\u539f\u95ee\u9898\u7684\u6700\u4f18\u4e0b\u754c\u3002 \u5f3a\u5f31\u5bf9\u5076\u6027 \u5bf9\u5076\u95ee\u9898\u5f97\u5230\u7684\u6700\u4f18\u89e3 d^* \\le p^* . \u5bf9\u4e8e\u5f3a\u5bf9\u5076\u95ee\u9898\uff0c\u5219\u6709 d^* = p^* \u6700\u7b80\u5355\u5e38\u7528\u7684\u7684\u5224\u65ad\u539f\u95ee\u9898\u4e3a\u5f3a\u5bf9\u5076\u7684\u6761\u4ef6: \u539f\u95ee\u9898\u662f\u51f8\u4f18\u5316\u95ee\u9898 \u4e14\u5c06\u4e0d\u7b49\u5f0f \\le \u7ea6\u675f\u6539\u4e3a\u66f4\u5f3a\u7684 < \u7ea6\u675f\uff0c\u4ecd\u5b58\u5728\u53ef\u884c\u57df. \u7ebf\u6027\u4e0d\u7b49\u5f0f\u7ea6\u675f\u4e0d\u9700\u8981\u6ee1\u8db3\u7b2c\u4e8c\u6761\u6761\u4ef6\u3002\u8fd9\u4e24\u70b9\u5728\u5927\u591a\u6570\u73b0\u5b9e\u51f8\u4f18\u5316\u95ee\u9898\u4e2d\u90fd\u6210\u7acb\uff0c\u5927\u591a\u6570\u7684\u51f8\u4f18\u5316\u95ee\u9898\u90fd\u662f\u5f3a\u5bf9\u5076\u7684\u3002 KKT \u6761\u4ef6 \u4e3b\u53ef\u884c\u6027, f_i(x) \\le 0 , h_i(x) = 0 \u5bf9\u5076\u53ef\u884c\u6027, \\lambda \\ge 0 Complementary slackness \\lambda_i^*f_i(x^*) = 0 \u62c9\u683c\u6717\u65e5\u51fd\u6570\u5bf9\u6bcf\u4e00\u4e2a\u53d8\u91cf\u7684\u5bfc\u6570(\u6216\u8005\u8bf4\u68af\u5ea6\u77e2\u91cf)\u4e3a\u96f6: \\nabla f_0(x) + \\sum\\lambda_i \\nabla f_i(x) + \\sum v_i \\nabla h_i(x) = 0 \u4e3b\u8981\u6027\u8d28: KKT\u6761\u4ef6\u662f\u4efb\u4f55\u4e00\u4e2a\u53ef\u5bfc\u7684\u6700\u4f18\u5316\u95ee\u9898\u7684\u5fc5\u8981\u6761\u4ef6\uff0c\u4e5f\u5373\u662f\u8bf4\u6700\u4f18\u89e3\u4e00\u5b9a\u6ee1\u8db3KKT\u6761\u4ef6 KKT\u6761\u4ef6\u5bf9\u51f8\u4f18\u5316\u95ee\u9898\u6765\u8bf4\u662f\u6700\u4f18\u89e3\u7684\u5145\u8981\u6761\u4ef6\uff0c\u4e5f\u5373\u662f\u8bf4\u53ef\u4ee5\u7528KKT\u6761\u4ef6\u6c42\u51fa\u6765\u7684\u89e3\u4e5f\u4e00\u5b9a\u662f\u5168\u5c40\u6700\u4f18\u89e3\u3002 \u8ba1\u7b97\u4e0e\u89e3\u9898: \u5229\u7528KKT\u6761\u4ef6\u76f4\u63a5\u6c42\u89e3\u7684\u65f6\u5019\u7ecf\u5e38\u4f1a\u9047\u5230\u5206\u7c7b\u8ba8\u8bba\uff0c\u4e3b\u8981\u95ee\u9898\u5728\u4e8e \\lambda_i \u662f\u5426\u4e3a\u96f6\uff0c\u7269\u7406\u610f\u4e49\u4e0a\u6765\u8bf4\u5c31\u662f\u89e3\u662f\u5728\u8fb9\u754c\u4e0a\u8fd8\u662f\u53ef\u884c\u57df\u5185\u3002\u53ef\u4ee5\u7528\u8fd9\u4e00\u4e2a\u53cd\u63a8\u56de\u5404\u79cd\u60c5\u51b5\u4e0b\u5bf9\u7cfb\u7edf\u53c2\u6570\u7684\u8981\u6c42\u3002 \u540c\u65f6\u8bb0\u5f55\u5e38\u7528\u7684\u77e9\u9635\u6c42\u5bfc\u7ed3\u679c ||Y - AX||^2_2 \\underset{\\text{grad w.r.t x}}{\\rightarrow} 2(A^TA)X - 2A^TY CVX\u5e93 python\u4e2d\u53ef\u4ee5\u4f7f\u7528cvxpy\u5e93\u5bf9\u51f8\u4f18\u5316\u95ee\u9898\u8fdb\u884c\u5efa\u6a21\u4e0e\u6c42\u89e3\u3002\u7406\u8bba\u4e0a\u6765\u8bf4cvxpy\u5e93\u53ef\u4ee5\u63a5\u53d7\u6bd4\u8f83\u590d\u6742\u7684\u95ee\u9898\u63cf\u8ff0\uff0c\u7136\u540e\u5e93\u4f1a\u5c1d\u8bd5\u5c06\u95ee\u9898\u8f6c\u6362\u4e3a\u6807\u51c6\u51f8\u4f18\u5316\u95ee\u9898\u5e76\u8c03\u7528\u6c42\u89e3\u5668\u6c42\u89e3. \u8001\u5e08\u7684\u5efa\u8bae\u662f\u7528\u6765\u5feb\u901f\u9a8c\u8bc1\u4e00\u4e2a\u95ee\u9898\u662f\u4e0d\u662f\u51f8\u4f18\u5316\u95ee\u9898. \u6700\u4f18\u5316\u7b97\u6cd5 \u4ee5\u6700\u5c0f\u5316\u4e3a\u4f8b\u5b50 \u725b\u987f\u6cd5 \u627f\u63a5\u68af\u5ea6\u4e0b\u964d\u6cd5, \u725b\u987f\u6cd5\u901a\u8fc7\u4e8c\u9636\u5bfc\u8f85\u52a9\u786e\u5b9a\u6b65\u957f: \\Delta \\mathbf{x}_{\\mathrm{nt}}=-\\nabla^{2} f(\\mathbf{x})^{-1} \\nabla f(\\mathbf{x}) \u5185\u70b9\u6cd5 \u5bf9\u4e8e\u4e0d\u7b49\u5f0f\u7ea6\u675f\uff0c\u53ef\u4ee5\u7406\u89e3\u4e3a\u9636\u8dc3\u5230\u65e0\u7a77\u7684\u4e00\u4e2a\u635f\u5931\u9879\uff0c\u4f7f\u7528Log\u60e9\u7f5a\u51fd\u6570\u8f6f\u5316\u6b64\u7ea6\u675f: \\underset{\\mathbf{x}}{\\operatorname{minimize}} \\quad f_{0}(\\mathbf{x})-(1 / t) \\sum_{i=1}^{m} \\log \\left(-f_{i}(\\mathbf{x})\\right) \u5f53t\u5f88\u5c0f\u7684\u65f6\u5019\uff0c\u6700\u4f18\u5316\u51e0\u4e4e\u53ea\u4f18\u5316\u4e0d\u7b49\u5f0f\u7ea6\u675f\u3002 \u5f53t\u5f88\u5927\u7684\u65f6\u5019\uff0c\u6700\u4f18\u5316\u51e0\u4e4e\u53ea\u4f18\u5316\u539f\u76ee\u6807\u51fd\u6570\uff0c\u4f46\u662f\u5728\u8fb9\u754c\u9644\u8fd1\u63a5\u8fd1\u4e8e\u9636\u8dc3\u3002 \u76f4\u63a5\u6765\u8bf4\uff0c\u5f53t\u5f88\u5927\u7684\u65f6\u5019\uff0c\u51fd\u6570\u4e0d\u597d\u4f18\u5316\uff0c\u56e0\u800c\u9700\u8981\u8fed\u4ee3\u4f18\u5316. \u4f5c\u4e1a\u4e00\u9053\u9898\u5c31\u662f\u8981\u6c42\u4f7f\u7528barrier method \u5904\u7406Lasso regression. Block Coordinate Descent (BCD) \\mathbf{x}_{i}^{k+1}=\\arg \\min _{\\mathbf{x}_{i} \\in \\mathcal{X}_{i}} f\\left(\\mathbf{x}_{1}^{k+1}, \\ldots, \\mathbf{x}_{i-1}^{k+1}, \\mathbf{x}_{i}, \\mathbf{x}_{i+1}^{k} \\ldots, \\mathbf{x}_{N+1}^{k}\\right) \u56fa\u5b9a\u4e00\u90e8\u5206\u89e3\uff0c\u6bcf\u4e00\u6b65\u53ea\u5bf9\u5176\u4e2d\u4e00\u90e8\u5206\u89e3\u8fdb\u884c\u6700\u4f18\u5316\u8fed\u4ee3\u3002 Majorization-Minimization Algorithm (MM Algo.) \u5bf9\u4e8e\u901a\u7528\u4f18\u5316\u95ee\u9898 \\begin{array}{lll} \\underset{x}{\\operatorname{minimize}} & f_{0}(x) \\\\ \\text { subject to } & f_{i}(x) \\leq 0 & i=1, \\ldots, m \\\\ & h_{i}(x)=0 & i=1, \\ldots, p \\end{array} \u8fed\u4ee3\u7684\u4f18\u5316\u4e00\u4e2asurrogate function. x^{k+1} = \\underset{x\\in\\mathcal{X}}{\\text{argmin}} u(x, x^k) \u8981\u6c42: u \u5728 f(x^k) \u5904\u76f8\u7b49\u4e14\u76f8\u5207 u(x, y) \\ge f(x) \u4f5c\u4e1a\u6709\u4e00\u9053\u7f16\u7801\u9898\u8981\u6c42\u4f7f\u7528MM\u6c42\u89e3 Expectation-Maximization EM \u7b97\u6cd5\u5c5e\u4e8e MM. \u5bfb\u627e surrogate function\u7684\u6280\u5de7 Convexity: \u51f8\u51fd\u6570: \\kappa(\\sum_i \\alpha_it_i) \\le \\sum_i\\alpha_i\\kappa(t_i) \u4f8b\u5b50: \\begin{aligned} \\kappa\\left(\\mathbf{w}^{T} \\mathbf{x}\\right) &=\\kappa\\left(\\mathbf{w}^{T}\\left(\\mathbf{x}-\\mathbf{x}^{k}\\right)+\\mathbf{w}^{T} \\mathbf{x}^{k}\\right) \\\\ &=\\kappa\\left(\\sum_{i} \\alpha_{i}\\left(\\frac{w_{i}\\left(x_{i}-x_{i}^{k}\\right)}{\\alpha_{i}}+\\mathbf{w}^{T} \\mathbf{x}^{k}\\right)\\right) \\\\ & \\leq \\sum_{i} \\alpha_{i} \\kappa\\left(\\frac{w_{i}\\left(x_{i}-x_{i}^{k}\\right)}{\\alpha_{i}}+\\mathbf{w}^{T} \\mathbf{x}^{k}\\right) \\\\ & \\leq \\sum_{i} \\frac{w_{i} x_{i}^{k}}{\\mathbf{w}^{T} \\mathbf{x}^{k}} \\kappa\\left(\\frac{\\mathbf{w}^{T} \\mathbf{x}^{k}}{x_{i}^{k}} x_{i}\\right) \\end{aligned} \u5176\u4e2d \\alpha_i = \\frac{w_ix_i^k}{w^Tx^k}, \\sum_i \\alpha_i=1 Taylor, upper bound with quadratic terms \\begin{aligned} \\kappa(\\mathbf{x}) &\\leq \\kappa\\left(\\mathbf{x}^{k}\\right)+\\nabla \\kappa\\left(\\mathbf{x}^{k}\\right)^{T}\\left(\\mathbf{x}-\\mathbf{x}^{k}\\right)+\\frac{1}{2}\\left(\\mathbf{x}-\\mathbf{x}^{k}\\right)^{T} \\mathbf{M}\\left(\\mathbf{x}-\\mathbf{x}^{k}\\right) \\\\ M - \\nabla^2\\kappa(x) &\\ge 0 \\end{aligned} \u8fd9\u4e2a M \u53ef\u4ee5\u7528 \\lambda_{max} \u4e5f\u53ef\u4ee5\u7528 \u4e8c\u6b21\u77e9\u9635\u5f0f (Lecturer \u7684\u6210\u679c) \u5bf9\u4e8e\u5bf9\u79f0\u77e9\u9635 L , \u6709\u5bf9\u79f0\u77e9\u9635 M \\ge L ,\u4f7f\u5f97 w^T L w \\le w^T M w + 2 w^{(k)T}(L-M)w - w^{(k)T}(L-M)w^{(k)} \u5176\u4e2d M \u5f80\u5f80\u53ef\u53d6 \\lambda_{max}(L) I Geometric programming \u6982\u5ff5\u5b9a\u4e49 monomial: c x_1^{a_1}x_2^{a_2}... , \u8981\u6c42 c > 0 Posynomial: \\sum c_k x_1^{a_{1k}}x_2^{a_{2k}}... GP \u5b9a\u4e49: \\begin{array}{lll} \\underset{x}{\\operatorname{minimize}} & f_{0}(x) \\\\ \\text { subject to } & f_{i}(x) \\leq 1 & i=1, \\ldots, m \\\\ & h_{i}(x)=1 & i=1, \\ldots, p \\end{array} \u5176\u4e2d f_i \u4e3aposynomials, g_i \u4e3a monomials. \u95ee\u9898\u6765\u6e90: \u6700\u5927\u5316\u5bb9\u5668\u4f53\u79ef \u7ea6\u675f\u6240\u6709\u5899(\u5730\u9762\u4e0e\u4fa7\u8fb9\u5899\u4f53)\u7684\u8868\u9762\u79ef \u7ea6\u675f\u957f\u5bbd\u6bd4,\u9ad8\u5bbd\u6bd4\u7b49 \\begin{array}{ll} \\text { maximize } & h w d \\\\ \\text { subject to } & 2(h w+h d) \\leq A_{\\text {wall }}, \\quad w d \\leq A_{\\text {floor }} \\\\ & \\alpha \\leq h / w \\leq \\beta, \\quad \\gamma \\leq w / d \\leq \\delta \\end{array} \u6c42\u89e3\u65b9\u6848: \u5c06\u6240\u6709\u53d8\u91cf x \u7528 e^{\\tilde{x}} \u66ff\u6362, \u5bf9\u76ee\u6807\u51fd\u6570\uff0c\u7ea6\u675f\u53d6 log \\begin{array}{lll} \\underset{x}{\\operatorname{minimize}} & \\log{ f_{0}(e^{\\tilde{x}})} \\\\ \\text { subject to } & \\text{log}f_{i}(e^{\\tilde{x}}) \\leq 0 & i=1, \\ldots, m \\\\ & \\text{log} h_{i}(e^{\\tilde{x}})=0 & i=1, \\ldots, p \\end{array} \u76ee\u6807\u51fd\u6570\u90fd\u53d8\u4e3a log-exp-sum \u7684\u5f62\u5f0f\uff0c\u56e0\u800c\u4e3a\u51f8\u51fd\u6570. \u6ee4\u6ce2\u5668\u8bbe\u8ba1 \u5207\u6bd4\u96ea\u592b\u6ee4\u6ce2\u5668\u8bbe\u8ba1 \u95ee\u9898\u5b9a\u4e49: \u8bbe\u8ba1\u6ee4\u6ce2\u5668\u7684\u7ebf\u6027\u53c2\u6570 \\vec h \u4f7f\u5f97\u7ed3\u679c\u7684\u6ee4\u6ce2\u5668\u4e0e\u76ee\u6807\u6ee4\u6ce2\u5668\u5728\u76ee\u6807\u9891\u7387\u8303\u56f4\u5185\u6700\u5927\u504f\u5dee\u6700\u5c0f\u5316 \\underset{\\vec h}{\\text{minimize}} \\underset{\\omega \\in [0, \\pi]}{max} |H(\\omega) - H_{des}(\\omega)| \u7531\u4e8e\u6ee4\u6ce2\u5668\u7684\u7279\u6027\u662f\u4e00\u4e2a\u865a\u6570\u51fd\u6570\uff0c\u56e0\u800c \"| |\"\u7b26\u53f7\u5e76\u4e0d\u662f\u7edd\u5bf9\u503c\u800c\u662f\u6a21\u957f,\u5bf9 \\omega \u79bb\u6563\u91c7\u6837,\u4f7f\u7528epigraph\u8f6c\u6362\u95ee\u9898\uff0c\u6700\u7ec8\u53ef\u4ee5\u5f97\u5230\u8fd9\u4e2aSOCP \\begin{array}{ll} \\underset{t, \\mathbf{h}}{\\operatorname{minimize}} & t \\\\ \\text { subject to } & \\left\\|\\mathbf{A}_{k} \\mathbf{h}-\\mathbf{b}_{k}\\right\\| \\leq t \\quad k=1, \\cdots, m \\end{array} \\begin{aligned} \\mathbf{h} &=\\left[\\begin{array}{cccc} h_{0} & \\cdots & h_{n-1} \\end{array}\\right]^{T} & \\\\ \\mathbf{A}_{k} &=\\left[\\begin{array}{ccc} 1 & \\cos \\omega_{k} & \\cdots & \\cos (n-1) \\omega_{k} \\\\ 0 & -\\sin \\omega_{k} & \\cdots & -\\sin (n-1) \\omega_{k} \\end{array}\\right] \\\\ \\mathbf{b}_{k} &=\\left[\\begin{array}{c} \\operatorname{Re} H_{\\operatorname{des}}\\left(\\omega_{k}\\right) \\\\ \\operatorname{Im} H_{\\operatorname{des}}\\left(\\omega_{k}\\right) \\end{array}\\right] &\\left(\\text { note: } \\mathbf{A}_{k} \\mathbf{h}=\\left[\\begin{array}{c} \\operatorname{Re} H\\left(\\omega_{k}\\right) \\\\ \\operatorname{Im} H\\left(\\omega_{k}\\right) \\end{array}\\right]\\right) \\end{aligned} magnitude \u4f18\u5316\u95ee\u9898 \u7ea6\u675f\u662f L(\\omega) < |H(\\omega)| < U(\\omega) \u3002 \u663e\u7136\u5de6\u4fa7\u7684\u5927\u4e8e\u53f7\u662f\u975e\u51f8\u7684\u3002\u8fd9\u91cc\u4f7f\u7528\u81ea\u76f8\u5173\u7cfb\u6570\u7684\u6982\u5ff5\u3002\u81ea\u76f8\u5173\u7cfb\u6570 r_t = \\sum_\\tau h_\\tau h_{\\tau + t} \u81ea\u76f8\u5173\u7cfb\u6570\u7684\u5085\u91cc\u53f6\u53d8\u6362\u7ed3\u679c R(\\omega) = \\sum_\\tau e^{-j\\omega\\tau}r_\\tau = |H(\\omega)|^2 (\u529f\u7387\u8c31\u51fd\u6570)\uff0c\u53ef\u4ee5\u89c2\u5bdf\u5230, R \u4e0e r \u662f\u7ebf\u6027\u5173\u7cfb. \u5c06\u539f\u6765\u7684\u7ea6\u675f\u8f6c\u6362\u4e3a L(\\omega)^2 \\le R(\\omega) \\le U(\\omega)^2 , \u8fd9\u4e2a\u4e0d\u7b49\u5f0f\u5bf9 r \u662f\u7ebf\u6027\uff0c\u51f8\u7684\u3002\u56e0\u800c\u5e45\u503c\u4f18\u5316\u95ee\u9898\u4e00\u822c\u89e3\u51b3\u65b9\u6cd5\u662f\u5c06\u95ee\u9898\u8f6c\u6362\u4e3a\u5173\u4e8e r \u7684\u51fd\u6570\uff0c\u7136\u540e\u7528spectral factorization\u5c06 r \u91cd\u65b0\u8f6c\u6362\u4e3a h Log-\u5207\u6bd4\u96ea\u592b \u5e45\u503c\u8bbe\u8ba1 \u4f18\u5316\u95ee\u9898\u4e13\u6ce8\u4e8e\u5e45\u503c\u7684\u5206\u8d1d\u503c\uff0c\u4f18\u5316\u662f\u8981\u6c42\u6700\u5c0f\u5316\u4e0e\u76ee\u6807\u8c31\u7684\u504f\u5dee \\text{minimize} \\underset{\\omega \\in [0,\\pi]}{max} |20\\log_{10}|H(\\omega)| - 20 \\log_{10}D(\\omega)| \u5176\u4e2d D \u6307\u7684\u662f\u76ee\u6807\u7684\u51fd\u6570\u7684\u5e45\u503c\u51fd\u6570. \u89e3\u51b3\u65b9\u6848: \u5f15\u5165\u65b0\u53d8\u91cf t , epigraph\u91cd\u5199max\u51fd\u6570 \u5c06\u95ee\u9898\u8f6c\u6362\u4e3a\u529f\u7387\u8c31\u51fd\u6570\u95ee\u9898 -t< 10\\log_{10}R(\\omega) - 10\\log_{10}D^2(\\omega) < t .\u6ce8\u610f R \u662f\u5173\u4e8e r \u7ebf\u6027\u7684. \u5229\u7528\u5bf9\u6570\u4e0e\u6307\u6570\u7684\u5355\u8c03\u6027\uff0c\u4e0d\u7b49\u5f0f\u4e24\u8fb9\u540c\u65f6\u53d6 10^x ,\u6362\u5143 \\tilde{t} = 10^t , \u540c\u65f6\u5c06\u4f18\u5316\u95ee\u9898\u4ece\u4f18\u5316 t \u76f4\u63a5\u6539\u4e3a\u4f18\u5316 \\tilde{t} 1/\\tilde{t} \\le R(\\omega) / D^2(\\omega) \\le \\tilde{t} \u53cd\u6bd4\u4f8b\u51fd\u6570\u4e0e\u7ebf\u6027\u51fd\u6570\u90fd\u6ee1\u8db3\u51f8\u4f18\u5316\u6761\u4ef6\uff0c\u95ee\u9898\u5212\u5f52\u4e3a\u51f8\u4f18\u5316\u95ee\u9898. Markowitz Modern Portfolio Theory \u8fd9\u4e2a \u91d1\u878d\u5b66\u95ee\u9898 \u662f\u4e00\u4e2a\u8ba8\u8bba\u6295\u8d44\u51b3\u7b56\u7684\u7b80\u5355\u6a21\u578b\u3002\u5176\u5c06\u6bcf\u4e00\u9879\u6295\u8d44\u7684\u62a5\u916c\u7406\u89e3\u4e3a\u4e00\u4e2a\u9ad8\u65af\u5206\u5e03;\u4e0d\u540c\u7684\u6295\u8d44\u7ec4\u6210\u7684\u8054\u5408\u5206\u5e03\u662f\u4e00\u4e2a(\u53ef\u80fd)\u6709\u76f8\u5173\u6027\u7684\u591a\u5143\u9ad8\u65af\u5206\u5e03;\u6295\u8d44\u8005\u7684\u51b3\u7b56\uff0c\u4e5f\u5c31\u662f\u6295\u8d44\u7ec4\u5408\u662f\u8d44\u4ea7\u7684\u52a0\u6743\u7ec4\u5408;\u6295\u8d44\u7ec4\u5408\u7684\u62a5\u916c\u4e5f\u5c06\u4f1a\u662f\u4e00\u4e2a\u9ad8\u65af\u5206\u5e03. \u6295\u8d44\u8005\u7684\u51b3\u7b56\u5c06\u4f1a\u662f\u6700\u5927\u5316\u6295\u8d44\u7ec4\u5408\u7684\u671f\u671b\u6536\u5165\uff0c\u540c\u65f6\u5e0c\u671b\u51cf\u5c11\u6295\u8d44\u62a5\u916c\u7684\u4e0d\u786e\u5b9a\u6027. \u57fa\u7840\u95ee\u9898\u4e00: mean-variance portfolio (MVP) \\begin{aligned} \\underset{w}{maximize} \\quad &w^T\\mu - \\lambda w^T \\Sigma w \\\\ \\text{subject to} \\quad&1^T w = 1 \\end{aligned} \u4f7f\u7528KKT\u53ef\u4ee5\u76f4\u63a5\u5f97\u5230\u7ed3\u679c\u3002 \\begin{aligned} w_{MVP} &= \\frac{1}{\\lambda} \\Sigma^{-1} (\\mu + v \\cdot 1)\\\\ v &= \\frac{2\\lambda - 1^T\\Sigma^{-1}\\mu}{1^T\\Sigma^{-1}\\cdot 1} \\end{aligned} \u62d3\u5c55\u601d\u8003\uff0c\u8fdb\u4e00\u6b65\u8003\u8651\u5230 \\mu \u4ee5\u53ca \\Sigma \u90fd\u662f\u4ece\u6570\u636e\u6837\u672c\u7684\u5747\u503c\u4e0e\u65b9\u5dee\u4e2d\u4f30\u8ba1\u51fa\u6765\u7684\uff0c\u56e0\u800c\u8fd9\u4e2a\u4f18\u5316\u95ee\u9898\u53ef\u4ee5\u7406\u89e3\u4e3a\u5bf9\u6837\u672c\u6570\u636e\u7684 l_2 -regularized\u7684\u56de\u5f52\u95ee\u9898. \u901a\u8fc7\u6dfb\u52a0\u4e00\u4e9b\u65b0\u7684\u7b80\u5355\u7ea6\u675f\uff0c\u53ef\u4ee5\u5728\u4e0d\u5f71\u54cd\u51f8\u4f18\u5316\u6027\u8d28\u7684\u57fa\u7840\u4e0a\u4f7f\u7ed3\u679c\u66f4\u9c81\u68d2. \u57fa\u7840\u95ee\u9898\u4e8c: Global minimum variance portfolio \\begin{aligned} \\underset{w}{\\text{minimize}} \\quad &w^T\\Sigma w\\\\ \\text{subject to} \\quad & w^T\\mu \\ge \\beta\\\\ &1^T w = 1 \\end{aligned} \u540c\u6837\u53ef\u4ee5\u901a\u8fc7KKT\u6761\u4ef6\u76f4\u63a5\u5f97\u5230\u7ed3\u679c w_{GMVP} = \\frac{1}{1^T\\Sigma^{-1}\\cdot 1} \\Sigma^{-1} \\cdot 1 \u56f0\u96be\u95ee\u9898: Maximum Sharpe ratio portfolio (MSRP) \\begin{aligned} \\underset{w}{\\text{maximize}} \\quad &\\frac{w^T\\mu - r_f}{\\sqrt{w^T\\Sigma w}} \\\\ \\text{subject to}\\quad & 1^T w = 1 \\end{aligned} \u8fd9\u4e2a\u95ee\u9898\u5e76\u4e0d\u662f\u51f8\u4f18\u5316\u95ee\u9898\uff0c\u4f46\u662f\u53ef\u4ee5\u88ab\u5f52\u7eb3\u4e3a concave-convex single-ratio fractional programming. \u4e5f\u5c31\u662f\u6700\u5927\u5316\u4e00\u4e2a \u5206\u5b50\u4e3a\u975e\u8d1f\u51f9\u51fd\u6570;\u5206\u6bcd\u4e3a\u975e\u8d1f\u51f8\u51fd\u6570\u7684\u51fd\u6570. \u5176\u4e00\u822c\u5f62\u5f0f\u4e3a \\underset{w}{\\text{maximize}} \\quad \\frac{f(x)}{g(x)} \u8fd9\u91cc\u4ecb\u7ecd\u4e86\u4e09\u79cd\u89e3\u6cd5 \u4e8c\u5206\u6c42\u89e3 \u5199\u6210epigraph\u5f62\u5f0f\uff0c \\begin{aligned} \\underset{x, t}{\\text{maximize}} \\quad &t \\\\ \\text{subject to} \\quad & t \\le \\frac{f(x)}{g(x)} \\end{aligned} \u5982\u679c\u5c06 t \u8bbe\u4e3a\u5e38\u91cf\uff0c\u8fd9\u4e2a\u95ee\u9898\u4f1a\u53d8\u6210\u4e00\u4e2a\u51f8\u4f18\u5316\u95ee\u9898\uff0c\u800c\u4e14\u662f\u53ef\u884c\u6027\u95ee\u9898. \\begin{aligned} \\underset{x}{\\text{maximize}} \\quad & 0 \\\\ \\text{subject to} \\quad & tg(x) \\le f(x) \\end{aligned} \u4f7f\u7528\u4e8c\u5206\u6cd5\u4e0d\u65ad\u6536\u655b t \u7684\u53d6\u503c\u8303\u56f4\uff0c\u627e\u5230\u6700\u5927\u7684\u5b58\u5728\u89e3\u7684 t ,\u5bf9\u5e94\u7684\u4f18\u5316\u7ed3\u679c\u4e3a\u76ee\u7684\u7ed3\u679c\u3002 Dinkelbach Transform \u5c06\u95ee\u9898\u8f6c\u6362\u4e3a \\begin{aligned} \\underset{x}{\\text{maximize}} \\quad & f(x) - yg(x) \\\\ \\end{aligned} \u5176\u4e2d\u65b0\u53d8\u91cf y \u4f1a\u88ab\u8fed\u4ee3\u5730\u66f4\u65b0\uff0c y^{(k)} = \\frac{f(x^{(k)})}{g(x^{(k)})} Dinkelbach\u8bc1\u660e\u4e86 y^{(k)} \u662f\u5355\u8c03\u9012\u589e\u7684\uff0c\u4e14\u7ed3\u679c\u6700\u7ec8\u4f1a\u6536\u655b\u5230 concave-convex FP\u7684\u6700\u4f18\u89e3. Charnes-Cooper Transform / Schaible transform \u5bf9\u4e8e\u7ebf\u6027\u7684 f(x), g(x) , \u5206\u6bcd\u90fd\u5927\u4e8e\u96f6\u7684\u60c5\u51b5. \u53ef\u4ee5\u91c7\u7528Charnes-Cooper\u8f6c\u6362\u3002 \u7ebf\u6027FP: \\begin{aligned} \\underset{x}{\\text{maximize}} \\quad & \\frac{c^Tx + \\alpha}{d^Tx+\\beta} \\\\ \\text{subject to} \\quad & Ax \\le b \\end{aligned} \u6362\u5143 y = \\frac{x}{d^Tx+\\beta} , t = \\frac{1}{d^Tx + \\beta} \u95ee\u9898\u53ef\u4ee5\u53d8\u4e3a \\begin{aligned} \\underset{y,t}{\\text{maximize}} \\quad & c^Ty + \\alpha t \\\\ \\text{subject to} \\quad & Ay \\le bt\\\\ & d^Ty + \\beta t = 1\\\\ &t \\ge 0 \\end{aligned} \u95ee\u9898\u53d8\u6210\u4e86\u4e00\u4e2aLinear programming. Shaible Transform\u662f\u4ee5\u4e0a\u8fd0\u7b97\u7684\u63a8\u5e7f\uff0c\u901a\u7528\u5730\u6765\u8bf4\uff0c \u53ef\u4ee5\u8bbe y=\\frac{x}{g(x)} , t = \\frac{1}{g(x)} , \u7ea6\u675f\u95ee\u9898\u53d8\u4e3a $$ \\begin{aligned} \\underset{y,t}{\\text{maximize}} \\quad & tf(\\frac{y}{t}) \\\\ \\text{subject to} \\quad & tg(y/t) \\le 1\\\\ &t \\ge 0 \\\\ & y/t \\in \\mathcal{X} \\quad \\text{\u4e5f\u5c31\u8bf4<script type=\"math/tex\">y/t \u8981\u6ee1\u8db3\u539f\u6765\u7684\u7ea6\u675f} \\end{aligned} $$ \u8fd9\u4e09\u79cd\u65b9\u6cd5\u90fd\u53ef\u4ee5\u7528\u4e8e\u6c42\u89e3\u539f\u6765\u7684MSRP\u95ee\u9898. Constrained Problem Optimized for Sparsity Iterative Reweighted l1-Norm Heuristic set w=1 repeat: \\text{minimize}_x ||Diag(w)x||_1 subject to constrained. w_i = 1 / (\\epsilon + |x_i|) Convergence \u76f4\u89c9: \u7b2c\u4e00\u6b21\u8fed\u4ee3\u4e3a\u76f4\u63a5\u4f18\u5316 l_1 -norm \u4e4b\u540e\u6bcf\u6b21\u8fed\u4ee3 \u5982\u679c x_i \u6bd4\u8f83\u5c0f\uff0c\u60e9\u7f5a\u6743\u91cd\u6bd4\u8f83\u5927\uff0c\u9a71\u4f7f\u5b83\u66f4\u5c0f \u5982\u679c x_i \u6bd4\u8f83\u5927\uff0c\u60e9\u7f5a\u6743\u91cd\u6bd4\u8f83\u5c0f\uff0c\u4e14\u68af\u5ea6\u4e5f\u4e0d\u5927\uff0c\u5141\u8bb8\u5b83\u53d8\u5f97\u66f4\u5927. \u7406\u8bba\u5206\u6790 \u4f7f\u7528 \\log \u800c\u4e0d\u662f ||x||_1 \u6765\u5bf9 ||x||_0 \u8fdb\u884c\u62df\u5408. \u4f18\u5316\u95ee\u9898\u53d8\u4e3a \\begin{array}{ll}\\underset{\\mathbf{x}}{\\operatorname{minimize}} & \\sum_{i=1}^{n} \\log \\left(1+x_{i} / \\varepsilon\\right) \\\\ \\text { subject to } & \\mathbf{x} \\in \\mathscr{C}, \\quad \\mathbf{x} \\geq \\mathbf{0}\\end{array} \u5bf9\u4e8e\u8fd9\u4e2a\u95ee\u9898\uff0c \u601d\u8def\u662f\u4f7f\u7528 MM\u7b97\u6cd5\uff0c\u4ee3\u7406\u51fd\u6570\u9009\u62e9\u5207\u7ebf\u62df\u5408: \\sum^n_{i=1}\\log(1 + x_i / \\varepsilon) \\approx \\sum^n_{i=1} \\log(1 + x_i^{(k)} / \\varepsilon) + \\sum_{i=1}^n \\frac{x_i - x_i^{(k)}}{\\varepsilon + x_i^{(k)}} \u95ee\u9898\u6700\u7ec8\u53d8\u4e3a\u6c42\u89e3\u4e00\u4e2a\u5e26\u7ea6\u675f\u7684\u7ebf\u6027\u76ee\u6807\u51fd\u6570 \\begin{aligned} &\\underset{x}{\\text{minimize}} \\quad \\sum^n_{i=1}w_ix_i\\\\ &\\text{subject to} \\quad x\\in\\mathscr{C}, \\quad x \\ge 0 \\end{aligned} \u5176\u4e2d w_i = 1 / (\\varepsilon + x_i^{(k)}) Sparse Index Tracking SIT\u7684\u610f\u601d\u662f\u91d1\u878d\u4e0a\u7528\u5c11\u6570\u4e2a\u80a1\u7968\u7684\u80a1\u4ef7\u7ebf\u6027\u52a0\u6743\u62df\u5408\u80a1\u6307(\u6052\u751f\u6307\u6570\uff0c\u4e0a\u8bc1\u6307\u6570\u7b49), \u4e3b\u8981\u96be\u70b9\u5728\u4e8e\u8fd9\u662f\u4e2a\u4e00\u4e2a\u7a00\u758f\u95ee\u9898\uff0c\u6211\u4eec\u53ea\u80fd\u9009\u62e9\u5176\u4e2d\u4e00\u90e8\u5206\u7684\u80a1\u7968\u8fdb\u884c\u62df\u5408\u3002\u8fd9\u4e2a\u9009\u62e9\u8fc7\u7a0b\u4e25\u8c28\u6765\u8bf4\u662f\u4e00\u4e2a NP\u96be\u7684\u95ee\u9898\u3002 Sparse Regression \\underset{w}{\\text{minimize}} \\quad ||r - Xw||_2 + \\lambda ||w||_0 lecture\u4e2d\u7ed9\u51fa\u4e86\u4f7f\u7528MM\u7b97\u6cd5\u8fdb\u884c\u6c42\u89e3\u7684\u65b9\u6848. \u7c7b\u4f3c\u4e8e\u524d\u6587\uff0c\u5b9a\u4e49\u5bf9 l_0 -norm\u7684\u8fd1\u4f3c \\rho_{p,\\gamma}(w) = \\frac{\\log{1 + |w|/p}}{\\log(1 + \\gamma/p)} \\gamma \u503c\u8d8a\u5c0f\uff0c\u5219\u5728\u903c\u8fd1\u96f6\u7684\u5c0f\u533a\u95f4\u5185\u8fd1\u4f3c\u8d8a\u51c6,\u4f46\u662f\u5728\u8f83\u5927\u7684\u533a\u95f4\u5185\u5219\u4e0d\u592a\u51c6\uff0c\u56e0\u800c\u662f\u4e00\u4e2a\u8d85\u53c2\u6570.\u4e14\u8fd9\u4e2a\u51fd\u6570\u5bf9 w\\ge0 \u662f\u4e00\u4e2a\u51f9\u51fd\u6570. \u65b0\u7684\u4f18\u5316\u76ee\u6807\u51fd\u6570: \\underset{w}{\\text{minimize}} \\quad \\frac{1}{T}||Xw-r^b||_2 + \\lambda 1^T \\rho_{p,u}(w) \u8fd9\u4e2a\u51fd\u6570\u8fd8\u4e0d\u662f\u51f8\u51fd\u6570\uff0c\u8fd9\u91cc\u91c7\u7528 MM \u7b97\u6cd5\u8fdb\u884c\u5904\u7406,\u4e0e\u524d\u4e00\u7ae0\u4e00\u81f4\uff0c\u4f7f\u7528\u5f53\u524d\u70b9\u7684\u5207\u7ebf\u4f5c\u4e3a\u4ee3\u7406\u51fd\u6570 \\approx \\frac{1}{\\log{1+\\gamma/p}} \\left\\{\\log(1 + w_i^{(k)} / \\varepsilon) + \\frac{w_i - w_i^{(k)}}{\\varepsilon + w_i^{(k)}}\\right\\} \u5728 MM \u4e2d\u9700\u8981\u8fed\u4ee3\u591a\u6b21\u6c42\u89e3\u7684\u76ee\u6807\u51fd\u6570: \\underset{w}{\\text{minimize}} \\quad \\frac{1}{T}||Xw-r^b||_2 + \\lambda d_{p,u}^{(k)T}(w) \\begin{array}{ll} \\text { subject to } & \\left.\\begin{array}{l} \\mathbf{w}^{\\top} \\mathbf{1}=1 \\\\ \\mathbf{0} \\leq \\mathbf{w} \\leq \\mathbf{1}, \\end{array}\\right\\} \\mathcal{W} \\end{array} \u8fd9\u4e2a\u51fd\u6570\u662f\u4e00\u4e2aQP,\u4ecd\u9700\u8981\u8fed\u4ee3\u3002 \u5bf9\u8fd9\u4e2aQP\u4f7f\u7528 MM ,\u4ee4 L = \\frac{1}{T}X^TX, M=\\lambda_{max}(L) I \u4ee5\u53ca\u516c\u5f0f w^T L w \\le w^T M w + 2 w^{(k)T}(L-M)w - w^{(k)T}(L-M)w^{(k)} \\begin{array}{l} \\mathbf{w}^{\\top} \\mathbf{L}_{1} \\mathbf{w}+\\left(\\lambda \\mathbf{d}_{p, u}^{(k)}-\\frac{2}{T} \\mathbf{X}^{\\top} \\mathbf{r}^{b}\\right)^{\\top} \\mathbf{w} \\\\ \\leq \\mathbf{w}^{\\top} \\mathbf{M}_{1} \\mathbf{w}+2 \\mathbf{w}^{(k)^{\\top}}\\left(\\mathbf{L}_{1}-\\mathbf{M}_{1}\\right) \\mathbf{w}-\\mathbf{w}^{(k)^{\\top}}\\left(\\mathbf{L}_{1}-\\mathbf{M}_{1}\\right) \\mathbf{w}^{(k)} \\\\ \\quad+\\left(\\lambda \\mathbf{d}_{p, u}^{(k)}-\\frac{2}{T} \\mathbf{X}^{\\top} \\mathbf{r}^{b}\\right)^{\\top} \\mathbf{w} \\\\ =\\lambda_{\\max }^{\\left(\\mathbf{L}_{1}\\right)} \\mathbf{w}^{\\top} \\mathbf{w}+\\left(2\\left(\\mathbf{L}_{1}-\\lambda_{\\max }^{\\left(\\mathbf{L}_{1}\\right)} \\mathbf{I}\\right) \\mathbf{w}^{(k)}+\\lambda \\mathbf{d}_{p, u}^{(k)}-\\frac{2}{T} \\mathbf{X}^{\\top} \\mathbf{r}^{b}\\right)^{\\top} \\mathbf{w}+\\text { const. } \\end{array} \u76ee\u6807\u51fd\u6570\u53d8\u4e3a: \\underset{w}{\\text{minimize}} \\quad w^Tw + q_1^{(k)T}w \\begin{array}{ll} \\text { subject to } & \\left.\\begin{array}{l} \\mathbf{w}^{\\top} \\mathbf{1}=1 \\\\ \\mathbf{0} \\leq \\mathbf{w} \\leq \\mathbf{1}, \\end{array}\\right\\} \\mathcal{W} \\end{array} \u5176\u4e2d \\mathbf{q}_{1}^{(k)}=\\frac{1}{\\lambda_{\\max }^{\\left(\\mathbf{L}_{1}\\right)}}\\left(2\\left(\\mathbf{L}_{1}-\\lambda_{\\max }^{\\left(\\mathbf{L}_{1}\\right)} \\mathbf{I}\\right) \\mathbf{w}^{(k)}+\\lambda \\mathbf{d}_{p, u}^{(k)}-\\frac{2}{T} \\mathbf{X}^{\\top} \\mathbf{r}^{b}\\right) \u5bf9\u8fd9\u4e2a\u95ee\u9898\u53ef\u4ee5\u7528KKT\u76f4\u63a5\u6c42\u51fa\u6700\u4f18\u89e3. \\mathcal{L}=w^Tw + q^Tw + \\mu(w^T1 -1) - \\sum h_iw + \\sum h'(w-1) . \u4f1a\u6c42\u51fa h' = 0 w_i = \\left\\{\\begin{array}{l}-\\frac{q_i+\\mu}{2}\\\\ 0\\end{array}\\right. h_i = \\left\\{\\begin{array}{ll}0 & w_i \\neq 0 \\\\ -q_i - v & w_i=0\\end{array} \\right. \u4ece h_i > 0 ,\u5f97\u5230\u5bf9 w_i \u7684\u5224\u522b\u6761\u4ef6 \\left\\{\\begin{array}{lll}v_i + q_i < 0 & w_i=0 & h_i = -v_i - q_i\\\\v_i + q_i > 0 & w_i=-\\frac{q_i+\\mu}{2} & h_i = 0 \\end{array}\\right. \u4ece\u7b49\u5f0f\u7ea6\u675f\uff0c\u5f97\u5230 \\mu = - \\frac{2 + \\sum_{i | w_i > 0}q_i}{\\text{num}\\_\\text{positive}}","title":"Convex Optimization"},{"location":"The_theory/ConvexOptimization/#convex-optimization","text":"boyd's book \u672c\u6587\u4e3b\u8981\u6536\u96c6\u5728\u51f8\u4f18\u5316\u8bfe\u7a0b\u4e2d\u8bb0\u4e0b\u7684\u57fa\u7840\u6982\u5ff5\u4e0e\u7ed3\u8bba\u3002 Convex Optimization \u51f8\u51fd\u6570\u4e0e\u51f8\u96c6 \u57fa\u672c\u5b9a\u4e49\u4e0e\u91cd\u8981\u5224\u5b9a \u5e38\u89c1\u800c\u91cd\u8981\u7684\u51f9\u51f8\u51fd\u6570\u51fd\u6570 \u4fdd\u7559\u51f8\u7279\u6027\u7684\u5e38\u89c1\u64cd\u4f5c \u51f8\u4f18\u5316\u95ee\u9898 \u5c06 L_1 , L_{\\infty} \u8f6c\u6362\u4e3a\u51f8\u4f18\u5316\u95ee\u9898 \u5bf9\u5076\u4e0eKKT\u6761\u4ef6 \u5bf9\u5076\u51fd\u6570 \u5f3a\u5f31\u5bf9\u5076\u6027 KKT \u6761\u4ef6 CVX\u5e93 \u6700\u4f18\u5316\u7b97\u6cd5 \u725b\u987f\u6cd5 \u5185\u70b9\u6cd5 Block Coordinate Descent (BCD) Majorization-Minimization Algorithm (MM Algo.) \u5bfb\u627e surrogate function\u7684\u6280\u5de7 Convexity: Taylor, upper bound with quadratic terms \u4e8c\u6b21\u77e9\u9635\u5f0f (Lecturer \u7684\u6210\u679c) Geometric programming \u6ee4\u6ce2\u5668\u8bbe\u8ba1 \u5207\u6bd4\u96ea\u592b\u6ee4\u6ce2\u5668\u8bbe\u8ba1 magnitude \u4f18\u5316\u95ee\u9898 Log-\u5207\u6bd4\u96ea\u592b \u5e45\u503c\u8bbe\u8ba1 Markowitz Modern Portfolio Theory \u57fa\u7840\u95ee\u9898\u4e00: mean-variance portfolio (MVP) \u57fa\u7840\u95ee\u9898\u4e8c: Global minimum variance portfolio \u56f0\u96be\u95ee\u9898: Maximum Sharpe ratio portfolio (MSRP) \u4e8c\u5206\u6c42\u89e3 Dinkelbach Transform Charnes-Cooper Transform / Schaible transform Constrained Problem Optimized for Sparsity Iterative Reweighted l1-Norm Heuristic \u76f4\u89c9: \u7406\u8bba\u5206\u6790 Sparse Index Tracking Sparse Regression","title":"Convex Optimization"},{"location":"The_theory/ConvexOptimization/#_1","text":"","title":"\u51f8\u51fd\u6570\u4e0e\u51f8\u96c6"},{"location":"The_theory/ConvexOptimization/#_2","text":"\u51f8\u51fd\u6570\u8981\u6c42\u51fd\u6570\u4e0a\uff0c\u4efb\u610f\u4e24\u70b9\u95f4\u7684\u51fd\u6570\u503c\u7684\u7ebf\u6027\u63d2\u503c\u8981\u4e0d\u5c0f\u4e8e\u7ebf\u6027\u63d2\u503c\u7684\u51fd\u6570\u503c. \\forall \\theta\\in[0,1] \\quad f(\\theta x + (1-\\theta)y) \\le \\theta f(x) + (1-\\theta) f(y) \u51f8\u96c6\u8981\u6c42\u96c6\u5408\u5185\u4e24\u70b9\u8fde\u7ebf\u4e0a\u7684\u4efb\u610f\u70b9\u90fd\u5728\u8be5\u96c6\u5408\u5185 \u51f8\u51fd\u6570\u7684\u4e00\u4e2a\u5145\u5206\u6761\u4ef6\u662f\u4e8c\u6b21\u5bfc\u4e0d\u5c0f\u4e8e\u96f6\uff0c\u4e25\u683c\u6765\u8bf4\u662f\u5bf9\u5404\u53d8\u91cf\u7684Hessian Matrix\u4e3a\u534a\u6b63\u5b9a.","title":"\u57fa\u672c\u5b9a\u4e49\u4e0e\u91cd\u8981\u5224\u5b9a"},{"location":"The_theory/ConvexOptimization/#_3","text":"\u7ebf\u6027\u51fd\u6570\u540c\u65f6\u4e3a\u51f9\u3001\u51f8\u51fd\u6570 \u6240\u6709\u7684 norm (\u6ee1\u8db3\u4e09\u89d2\u5f62\u4e0d\u7b49\u5f0f)\u90fd\u662f\u51f8\u51fd\u6570 \u4e8c\u6b21\u51fd\u6570 f(x) = x^TPx + 2q^Tx + r , if P \\ge 0 \u662f\u51f8\u51fd\u6570. \u51e0\u4f55\u5e73\u5747 f(x) = (\\Pi^n_{i=1} x_i)^{1/n} \u662f\u51f9\u51fd\u6570 log-sum-exp f(x) = \\log{\\sum_i e^{x_i}} \u4e3a\u51f8\u51fd\u6570 quadratic over linear f(x,y) = x^2 / y \u4e3a\u51f8\u51fd\u6570 log-det f(X) = logdet(X) \u4e3a\u51f9\u51fd\u6570 (\u8bc1\u660e\u6bd4\u8f83\u53d6\u5de7\uff0c\u53d6\u5b9a\u4e49\u57df\u5185\u4efb\u610f\u4e24\u70b9\uff0c\u8bc1\u660e\u76f4\u7ebf\u4e0a\u63d2\u503c\u6bd4\u4f8b\u4e3a\u53d8\u91cf\u7684\u51fd\u6570\u90fd\u4e3a\u51f9\u51fd\u6570\uff0c\u5219\u6574\u4e2a\u51fd\u6570\u4e3a\u51f9\u51fd\u6570) \u77e9\u9635\u7684\u6700\u5927\u7279\u5f81\u503c f(X) = \\lambda_{max}(X) = \\underset{||y||_2=1}{max}y^TXy \u4e3a\u51f8\u51fd\u6570 (\u7531\u6700\u5927\u503c\u6027\u8d28)","title":"\u5e38\u89c1\u800c\u91cd\u8981\u7684\u51f9\u51f8\u51fd\u6570\u51fd\u6570"},{"location":"The_theory/ConvexOptimization/#_4","text":"\u51f8\u51fd\u6570\u7684\u975e\u8d1f\u6743\u91cd\u6c42\u548c \u4e0e\u7ebf\u6027\u51fd\u6570\u7684\u5d4c\u5957 f(Ax + b) \u591a\u4e2a\u51f8\u51fd\u6570\u7684point-wise maximum. \u51f8\u51fd\u6570\u4e2d\u90e8\u5206\u53d8\u91cf\u7684maximum g(x) = \\underset{y\\in \\mathcal{A}}{sup}f(x, y)","title":"\u4fdd\u7559\u51f8\u7279\u6027\u7684\u5e38\u89c1\u64cd\u4f5c"},{"location":"The_theory/ConvexOptimization/#_5","text":"\\begin{array}{lll} \\underset{x}{\\operatorname{minimize}} & f_{0}(x) \\\\ \\text { subject to } & f_{i}(x) \\leq 0 & i=1, \\ldots, m \\\\ & h_{i}(x)=0 & i=1, \\ldots, p \\end{array} \u8981\u6c42: f_0(x) \u4e3a\u51f8\u51fd\u6570 f_i(x) \u4e3a\u51f8\u51fd\u6570 h_i(x) \u4e3a\u7ebf\u6027\u51fd\u6570\uff0c\u4e5f\u5c31\u662f\u53ea\u5141\u8bb8 Ax = b \u4e3b\u8981\u6027\u8d28: \u51f8\u4f18\u5316\u7684\u5c40\u90e8\u6700\u4f18\u7b49\u4e8e\u5168\u5c40\u6700\u4f18\uff0c\u4e14\u552f\u4e00.","title":"\u51f8\u4f18\u5316\u95ee\u9898"},{"location":"The_theory/ConvexOptimization/#l_1-l_infty","text":"L_{\\infty} \u95ee\u9898\u53ef\u4ee5\u5982\u6b64\u8f6c\u6362: \\begin{array}{ll} \\underset{x}{\\operatorname{minimize}} & \\|x\\|_{\\infty} \\\\ \\text { subject to } & G x \\leq h \\\\ & A x=b \\end{array} \u53d8\u6210 \\begin{array}{ll} \\underset{t, x}{\\operatorname{minimize}} & t \\\\ \\text { subject to } & -t \\mathbf{1} \\leq x \\leq t \\mathbf{1} \\\\ & G x \\leq h \\\\ & A x=b \\end{array} L_1 : \\begin{array}{ll} \\underset{x}{\\operatorname{minimize}} & \\|x\\|_{1} \\\\ \\text { subject to } & G x \\leq h \\\\ & A x=b \\end{array} \\begin{array}{ll} \\underset{t, x}{\\operatorname{minimize}} & \\sum_{i} t_{i} \\\\ \\text { subject to } & -t \\leq x \\leq t \\\\ & G x \\leq h \\\\ & A x=b \\end{array}","title":"\u5c06 L_1, L_{\\infty}\u8f6c\u6362\u4e3a\u51f8\u4f18\u5316\u95ee\u9898"},{"location":"The_theory/ConvexOptimization/#kkt","text":"\u5bf9\u4e8e\u4e00\u822c\u6700\u4f18\u5316\u95ee\u9898: \\begin{array}{lll} \\underset{x}{\\operatorname{minimize}} & f_{0}(x) \\\\ \\text { subject to } & f_{i}(x) \\leq 0 & i=1, \\ldots, m \\\\ & h_{i}(x)=0 & i=1, \\ldots, p \\end{array} Lagrangian\u4e3a: L(x, \\lambda, v) = f_0(x) + \\sum_i \\lambda_i f_i(x) + \\sum_i v_ih_i(x)","title":"\u5bf9\u5076\u4e0eKKT\u6761\u4ef6"},{"location":"The_theory/ConvexOptimization/#_6","text":"g(\\lambda, v) = \\underset{x\\in D}{inf} L(x, \\lambda, v) \u65e0\u8bba\u539f\u6765\u7684\u95ee\u9898\u662f\u5426\u662f\u51f8\u4f18\u5316\u95ee\u9898\uff0c\u5bf9\u5076\u51fd\u6570 g \u4e00\u5b9a\u4e3a\u51f9\u51fd\u6570 (\u7ebf\u6027\u51fd\u6570\u7684inf\u4e0b\u754c\u7ec4\u5408)\uff0c\u56e0\u800c\u6700\u5927\u5316 g \u7ecf\u5e38\u662f\u4e00\u4e2a\u51f8\u4f18\u5316\u95ee\u9898. \u540c\u65f6 g(\\lambda, v) \\le p^* , \u5bf9\u5076\u51fd\u6570\u53ef\u4ee5\u8868\u5f81\u539f\u6700\u4f18\u5316\u95ee\u9898\u7684\u4f18\u5316\u4e0b\u754c\u3002 \u5bf9\u5076\u95ee\u9898\u5c31\u662f\u901a\u8fc7\u8ba1\u7b97 g \u7684\u6700\u5927\u503c(\u540c\u65f6\u6709\u7ea6\u675f \\lambda \\ge 0 )\uff0c\u6765\u5206\u6790\u539f\u95ee\u9898\u7684\u6700\u4f18\u4e0b\u754c\u3002","title":"\u5bf9\u5076\u51fd\u6570"},{"location":"The_theory/ConvexOptimization/#_7","text":"\u5bf9\u5076\u95ee\u9898\u5f97\u5230\u7684\u6700\u4f18\u89e3 d^* \\le p^* . \u5bf9\u4e8e\u5f3a\u5bf9\u5076\u95ee\u9898\uff0c\u5219\u6709 d^* = p^* \u6700\u7b80\u5355\u5e38\u7528\u7684\u7684\u5224\u65ad\u539f\u95ee\u9898\u4e3a\u5f3a\u5bf9\u5076\u7684\u6761\u4ef6: \u539f\u95ee\u9898\u662f\u51f8\u4f18\u5316\u95ee\u9898 \u4e14\u5c06\u4e0d\u7b49\u5f0f \\le \u7ea6\u675f\u6539\u4e3a\u66f4\u5f3a\u7684 < \u7ea6\u675f\uff0c\u4ecd\u5b58\u5728\u53ef\u884c\u57df. \u7ebf\u6027\u4e0d\u7b49\u5f0f\u7ea6\u675f\u4e0d\u9700\u8981\u6ee1\u8db3\u7b2c\u4e8c\u6761\u6761\u4ef6\u3002\u8fd9\u4e24\u70b9\u5728\u5927\u591a\u6570\u73b0\u5b9e\u51f8\u4f18\u5316\u95ee\u9898\u4e2d\u90fd\u6210\u7acb\uff0c\u5927\u591a\u6570\u7684\u51f8\u4f18\u5316\u95ee\u9898\u90fd\u662f\u5f3a\u5bf9\u5076\u7684\u3002","title":"\u5f3a\u5f31\u5bf9\u5076\u6027"},{"location":"The_theory/ConvexOptimization/#kkt_1","text":"\u4e3b\u53ef\u884c\u6027, f_i(x) \\le 0 , h_i(x) = 0 \u5bf9\u5076\u53ef\u884c\u6027, \\lambda \\ge 0 Complementary slackness \\lambda_i^*f_i(x^*) = 0 \u62c9\u683c\u6717\u65e5\u51fd\u6570\u5bf9\u6bcf\u4e00\u4e2a\u53d8\u91cf\u7684\u5bfc\u6570(\u6216\u8005\u8bf4\u68af\u5ea6\u77e2\u91cf)\u4e3a\u96f6: \\nabla f_0(x) + \\sum\\lambda_i \\nabla f_i(x) + \\sum v_i \\nabla h_i(x) = 0 \u4e3b\u8981\u6027\u8d28: KKT\u6761\u4ef6\u662f\u4efb\u4f55\u4e00\u4e2a\u53ef\u5bfc\u7684\u6700\u4f18\u5316\u95ee\u9898\u7684\u5fc5\u8981\u6761\u4ef6\uff0c\u4e5f\u5373\u662f\u8bf4\u6700\u4f18\u89e3\u4e00\u5b9a\u6ee1\u8db3KKT\u6761\u4ef6 KKT\u6761\u4ef6\u5bf9\u51f8\u4f18\u5316\u95ee\u9898\u6765\u8bf4\u662f\u6700\u4f18\u89e3\u7684\u5145\u8981\u6761\u4ef6\uff0c\u4e5f\u5373\u662f\u8bf4\u53ef\u4ee5\u7528KKT\u6761\u4ef6\u6c42\u51fa\u6765\u7684\u89e3\u4e5f\u4e00\u5b9a\u662f\u5168\u5c40\u6700\u4f18\u89e3\u3002 \u8ba1\u7b97\u4e0e\u89e3\u9898: \u5229\u7528KKT\u6761\u4ef6\u76f4\u63a5\u6c42\u89e3\u7684\u65f6\u5019\u7ecf\u5e38\u4f1a\u9047\u5230\u5206\u7c7b\u8ba8\u8bba\uff0c\u4e3b\u8981\u95ee\u9898\u5728\u4e8e \\lambda_i \u662f\u5426\u4e3a\u96f6\uff0c\u7269\u7406\u610f\u4e49\u4e0a\u6765\u8bf4\u5c31\u662f\u89e3\u662f\u5728\u8fb9\u754c\u4e0a\u8fd8\u662f\u53ef\u884c\u57df\u5185\u3002\u53ef\u4ee5\u7528\u8fd9\u4e00\u4e2a\u53cd\u63a8\u56de\u5404\u79cd\u60c5\u51b5\u4e0b\u5bf9\u7cfb\u7edf\u53c2\u6570\u7684\u8981\u6c42\u3002 \u540c\u65f6\u8bb0\u5f55\u5e38\u7528\u7684\u77e9\u9635\u6c42\u5bfc\u7ed3\u679c ||Y - AX||^2_2 \\underset{\\text{grad w.r.t x}}{\\rightarrow} 2(A^TA)X - 2A^TY","title":"KKT \u6761\u4ef6"},{"location":"The_theory/ConvexOptimization/#cvx","text":"python\u4e2d\u53ef\u4ee5\u4f7f\u7528cvxpy\u5e93\u5bf9\u51f8\u4f18\u5316\u95ee\u9898\u8fdb\u884c\u5efa\u6a21\u4e0e\u6c42\u89e3\u3002\u7406\u8bba\u4e0a\u6765\u8bf4cvxpy\u5e93\u53ef\u4ee5\u63a5\u53d7\u6bd4\u8f83\u590d\u6742\u7684\u95ee\u9898\u63cf\u8ff0\uff0c\u7136\u540e\u5e93\u4f1a\u5c1d\u8bd5\u5c06\u95ee\u9898\u8f6c\u6362\u4e3a\u6807\u51c6\u51f8\u4f18\u5316\u95ee\u9898\u5e76\u8c03\u7528\u6c42\u89e3\u5668\u6c42\u89e3. \u8001\u5e08\u7684\u5efa\u8bae\u662f\u7528\u6765\u5feb\u901f\u9a8c\u8bc1\u4e00\u4e2a\u95ee\u9898\u662f\u4e0d\u662f\u51f8\u4f18\u5316\u95ee\u9898.","title":"CVX\u5e93"},{"location":"The_theory/ConvexOptimization/#_8","text":"\u4ee5\u6700\u5c0f\u5316\u4e3a\u4f8b\u5b50","title":"\u6700\u4f18\u5316\u7b97\u6cd5"},{"location":"The_theory/ConvexOptimization/#_9","text":"\u627f\u63a5\u68af\u5ea6\u4e0b\u964d\u6cd5, \u725b\u987f\u6cd5\u901a\u8fc7\u4e8c\u9636\u5bfc\u8f85\u52a9\u786e\u5b9a\u6b65\u957f: \\Delta \\mathbf{x}_{\\mathrm{nt}}=-\\nabla^{2} f(\\mathbf{x})^{-1} \\nabla f(\\mathbf{x})","title":"\u725b\u987f\u6cd5"},{"location":"The_theory/ConvexOptimization/#_10","text":"\u5bf9\u4e8e\u4e0d\u7b49\u5f0f\u7ea6\u675f\uff0c\u53ef\u4ee5\u7406\u89e3\u4e3a\u9636\u8dc3\u5230\u65e0\u7a77\u7684\u4e00\u4e2a\u635f\u5931\u9879\uff0c\u4f7f\u7528Log\u60e9\u7f5a\u51fd\u6570\u8f6f\u5316\u6b64\u7ea6\u675f: \\underset{\\mathbf{x}}{\\operatorname{minimize}} \\quad f_{0}(\\mathbf{x})-(1 / t) \\sum_{i=1}^{m} \\log \\left(-f_{i}(\\mathbf{x})\\right) \u5f53t\u5f88\u5c0f\u7684\u65f6\u5019\uff0c\u6700\u4f18\u5316\u51e0\u4e4e\u53ea\u4f18\u5316\u4e0d\u7b49\u5f0f\u7ea6\u675f\u3002 \u5f53t\u5f88\u5927\u7684\u65f6\u5019\uff0c\u6700\u4f18\u5316\u51e0\u4e4e\u53ea\u4f18\u5316\u539f\u76ee\u6807\u51fd\u6570\uff0c\u4f46\u662f\u5728\u8fb9\u754c\u9644\u8fd1\u63a5\u8fd1\u4e8e\u9636\u8dc3\u3002 \u76f4\u63a5\u6765\u8bf4\uff0c\u5f53t\u5f88\u5927\u7684\u65f6\u5019\uff0c\u51fd\u6570\u4e0d\u597d\u4f18\u5316\uff0c\u56e0\u800c\u9700\u8981\u8fed\u4ee3\u4f18\u5316. \u4f5c\u4e1a\u4e00\u9053\u9898\u5c31\u662f\u8981\u6c42\u4f7f\u7528barrier method \u5904\u7406Lasso regression.","title":"\u5185\u70b9\u6cd5"},{"location":"The_theory/ConvexOptimization/#block-coordinate-descent-bcd","text":"\\mathbf{x}_{i}^{k+1}=\\arg \\min _{\\mathbf{x}_{i} \\in \\mathcal{X}_{i}} f\\left(\\mathbf{x}_{1}^{k+1}, \\ldots, \\mathbf{x}_{i-1}^{k+1}, \\mathbf{x}_{i}, \\mathbf{x}_{i+1}^{k} \\ldots, \\mathbf{x}_{N+1}^{k}\\right) \u56fa\u5b9a\u4e00\u90e8\u5206\u89e3\uff0c\u6bcf\u4e00\u6b65\u53ea\u5bf9\u5176\u4e2d\u4e00\u90e8\u5206\u89e3\u8fdb\u884c\u6700\u4f18\u5316\u8fed\u4ee3\u3002","title":"Block Coordinate Descent (BCD)"},{"location":"The_theory/ConvexOptimization/#majorization-minimization-algorithm-mm-algo","text":"\u5bf9\u4e8e\u901a\u7528\u4f18\u5316\u95ee\u9898 \\begin{array}{lll} \\underset{x}{\\operatorname{minimize}} & f_{0}(x) \\\\ \\text { subject to } & f_{i}(x) \\leq 0 & i=1, \\ldots, m \\\\ & h_{i}(x)=0 & i=1, \\ldots, p \\end{array} \u8fed\u4ee3\u7684\u4f18\u5316\u4e00\u4e2asurrogate function. x^{k+1} = \\underset{x\\in\\mathcal{X}}{\\text{argmin}} u(x, x^k) \u8981\u6c42: u \u5728 f(x^k) \u5904\u76f8\u7b49\u4e14\u76f8\u5207 u(x, y) \\ge f(x) \u4f5c\u4e1a\u6709\u4e00\u9053\u7f16\u7801\u9898\u8981\u6c42\u4f7f\u7528MM\u6c42\u89e3 Expectation-Maximization EM \u7b97\u6cd5\u5c5e\u4e8e MM.","title":"Majorization-Minimization Algorithm (MM Algo.)"},{"location":"The_theory/ConvexOptimization/#surrogate-function","text":"","title":"\u5bfb\u627e surrogate function\u7684\u6280\u5de7"},{"location":"The_theory/ConvexOptimization/#convexity","text":"\u51f8\u51fd\u6570: \\kappa(\\sum_i \\alpha_it_i) \\le \\sum_i\\alpha_i\\kappa(t_i) \u4f8b\u5b50: \\begin{aligned} \\kappa\\left(\\mathbf{w}^{T} \\mathbf{x}\\right) &=\\kappa\\left(\\mathbf{w}^{T}\\left(\\mathbf{x}-\\mathbf{x}^{k}\\right)+\\mathbf{w}^{T} \\mathbf{x}^{k}\\right) \\\\ &=\\kappa\\left(\\sum_{i} \\alpha_{i}\\left(\\frac{w_{i}\\left(x_{i}-x_{i}^{k}\\right)}{\\alpha_{i}}+\\mathbf{w}^{T} \\mathbf{x}^{k}\\right)\\right) \\\\ & \\leq \\sum_{i} \\alpha_{i} \\kappa\\left(\\frac{w_{i}\\left(x_{i}-x_{i}^{k}\\right)}{\\alpha_{i}}+\\mathbf{w}^{T} \\mathbf{x}^{k}\\right) \\\\ & \\leq \\sum_{i} \\frac{w_{i} x_{i}^{k}}{\\mathbf{w}^{T} \\mathbf{x}^{k}} \\kappa\\left(\\frac{\\mathbf{w}^{T} \\mathbf{x}^{k}}{x_{i}^{k}} x_{i}\\right) \\end{aligned} \u5176\u4e2d \\alpha_i = \\frac{w_ix_i^k}{w^Tx^k}, \\sum_i \\alpha_i=1","title":"Convexity:"},{"location":"The_theory/ConvexOptimization/#taylor-upper-bound-with-quadratic-terms","text":"\\begin{aligned} \\kappa(\\mathbf{x}) &\\leq \\kappa\\left(\\mathbf{x}^{k}\\right)+\\nabla \\kappa\\left(\\mathbf{x}^{k}\\right)^{T}\\left(\\mathbf{x}-\\mathbf{x}^{k}\\right)+\\frac{1}{2}\\left(\\mathbf{x}-\\mathbf{x}^{k}\\right)^{T} \\mathbf{M}\\left(\\mathbf{x}-\\mathbf{x}^{k}\\right) \\\\ M - \\nabla^2\\kappa(x) &\\ge 0 \\end{aligned} \u8fd9\u4e2a M \u53ef\u4ee5\u7528 \\lambda_{max} \u4e5f\u53ef\u4ee5\u7528","title":"Taylor, upper bound with quadratic terms"},{"location":"The_theory/ConvexOptimization/#lecturer","text":"\u5bf9\u4e8e\u5bf9\u79f0\u77e9\u9635 L , \u6709\u5bf9\u79f0\u77e9\u9635 M \\ge L ,\u4f7f\u5f97 w^T L w \\le w^T M w + 2 w^{(k)T}(L-M)w - w^{(k)T}(L-M)w^{(k)} \u5176\u4e2d M \u5f80\u5f80\u53ef\u53d6 \\lambda_{max}(L) I","title":"\u4e8c\u6b21\u77e9\u9635\u5f0f (Lecturer \u7684\u6210\u679c)"},{"location":"The_theory/ConvexOptimization/#geometric-programming","text":"\u6982\u5ff5\u5b9a\u4e49 monomial: c x_1^{a_1}x_2^{a_2}... , \u8981\u6c42 c > 0 Posynomial: \\sum c_k x_1^{a_{1k}}x_2^{a_{2k}}... GP \u5b9a\u4e49: \\begin{array}{lll} \\underset{x}{\\operatorname{minimize}} & f_{0}(x) \\\\ \\text { subject to } & f_{i}(x) \\leq 1 & i=1, \\ldots, m \\\\ & h_{i}(x)=1 & i=1, \\ldots, p \\end{array} \u5176\u4e2d f_i \u4e3aposynomials, g_i \u4e3a monomials. \u95ee\u9898\u6765\u6e90: \u6700\u5927\u5316\u5bb9\u5668\u4f53\u79ef \u7ea6\u675f\u6240\u6709\u5899(\u5730\u9762\u4e0e\u4fa7\u8fb9\u5899\u4f53)\u7684\u8868\u9762\u79ef \u7ea6\u675f\u957f\u5bbd\u6bd4,\u9ad8\u5bbd\u6bd4\u7b49 \\begin{array}{ll} \\text { maximize } & h w d \\\\ \\text { subject to } & 2(h w+h d) \\leq A_{\\text {wall }}, \\quad w d \\leq A_{\\text {floor }} \\\\ & \\alpha \\leq h / w \\leq \\beta, \\quad \\gamma \\leq w / d \\leq \\delta \\end{array} \u6c42\u89e3\u65b9\u6848: \u5c06\u6240\u6709\u53d8\u91cf x \u7528 e^{\\tilde{x}} \u66ff\u6362, \u5bf9\u76ee\u6807\u51fd\u6570\uff0c\u7ea6\u675f\u53d6 log \\begin{array}{lll} \\underset{x}{\\operatorname{minimize}} & \\log{ f_{0}(e^{\\tilde{x}})} \\\\ \\text { subject to } & \\text{log}f_{i}(e^{\\tilde{x}}) \\leq 0 & i=1, \\ldots, m \\\\ & \\text{log} h_{i}(e^{\\tilde{x}})=0 & i=1, \\ldots, p \\end{array} \u76ee\u6807\u51fd\u6570\u90fd\u53d8\u4e3a log-exp-sum \u7684\u5f62\u5f0f\uff0c\u56e0\u800c\u4e3a\u51f8\u51fd\u6570.","title":"Geometric programming"},{"location":"The_theory/ConvexOptimization/#_11","text":"","title":"\u6ee4\u6ce2\u5668\u8bbe\u8ba1"},{"location":"The_theory/ConvexOptimization/#_12","text":"\u95ee\u9898\u5b9a\u4e49: \u8bbe\u8ba1\u6ee4\u6ce2\u5668\u7684\u7ebf\u6027\u53c2\u6570 \\vec h \u4f7f\u5f97\u7ed3\u679c\u7684\u6ee4\u6ce2\u5668\u4e0e\u76ee\u6807\u6ee4\u6ce2\u5668\u5728\u76ee\u6807\u9891\u7387\u8303\u56f4\u5185\u6700\u5927\u504f\u5dee\u6700\u5c0f\u5316 \\underset{\\vec h}{\\text{minimize}} \\underset{\\omega \\in [0, \\pi]}{max} |H(\\omega) - H_{des}(\\omega)| \u7531\u4e8e\u6ee4\u6ce2\u5668\u7684\u7279\u6027\u662f\u4e00\u4e2a\u865a\u6570\u51fd\u6570\uff0c\u56e0\u800c \"| |\"\u7b26\u53f7\u5e76\u4e0d\u662f\u7edd\u5bf9\u503c\u800c\u662f\u6a21\u957f,\u5bf9 \\omega \u79bb\u6563\u91c7\u6837,\u4f7f\u7528epigraph\u8f6c\u6362\u95ee\u9898\uff0c\u6700\u7ec8\u53ef\u4ee5\u5f97\u5230\u8fd9\u4e2aSOCP \\begin{array}{ll} \\underset{t, \\mathbf{h}}{\\operatorname{minimize}} & t \\\\ \\text { subject to } & \\left\\|\\mathbf{A}_{k} \\mathbf{h}-\\mathbf{b}_{k}\\right\\| \\leq t \\quad k=1, \\cdots, m \\end{array} \\begin{aligned} \\mathbf{h} &=\\left[\\begin{array}{cccc} h_{0} & \\cdots & h_{n-1} \\end{array}\\right]^{T} & \\\\ \\mathbf{A}_{k} &=\\left[\\begin{array}{ccc} 1 & \\cos \\omega_{k} & \\cdots & \\cos (n-1) \\omega_{k} \\\\ 0 & -\\sin \\omega_{k} & \\cdots & -\\sin (n-1) \\omega_{k} \\end{array}\\right] \\\\ \\mathbf{b}_{k} &=\\left[\\begin{array}{c} \\operatorname{Re} H_{\\operatorname{des}}\\left(\\omega_{k}\\right) \\\\ \\operatorname{Im} H_{\\operatorname{des}}\\left(\\omega_{k}\\right) \\end{array}\\right] &\\left(\\text { note: } \\mathbf{A}_{k} \\mathbf{h}=\\left[\\begin{array}{c} \\operatorname{Re} H\\left(\\omega_{k}\\right) \\\\ \\operatorname{Im} H\\left(\\omega_{k}\\right) \\end{array}\\right]\\right) \\end{aligned}","title":"\u5207\u6bd4\u96ea\u592b\u6ee4\u6ce2\u5668\u8bbe\u8ba1"},{"location":"The_theory/ConvexOptimization/#magnitude","text":"\u7ea6\u675f\u662f L(\\omega) < |H(\\omega)| < U(\\omega) \u3002 \u663e\u7136\u5de6\u4fa7\u7684\u5927\u4e8e\u53f7\u662f\u975e\u51f8\u7684\u3002\u8fd9\u91cc\u4f7f\u7528\u81ea\u76f8\u5173\u7cfb\u6570\u7684\u6982\u5ff5\u3002\u81ea\u76f8\u5173\u7cfb\u6570 r_t = \\sum_\\tau h_\\tau h_{\\tau + t} \u81ea\u76f8\u5173\u7cfb\u6570\u7684\u5085\u91cc\u53f6\u53d8\u6362\u7ed3\u679c R(\\omega) = \\sum_\\tau e^{-j\\omega\\tau}r_\\tau = |H(\\omega)|^2 (\u529f\u7387\u8c31\u51fd\u6570)\uff0c\u53ef\u4ee5\u89c2\u5bdf\u5230, R \u4e0e r \u662f\u7ebf\u6027\u5173\u7cfb. \u5c06\u539f\u6765\u7684\u7ea6\u675f\u8f6c\u6362\u4e3a L(\\omega)^2 \\le R(\\omega) \\le U(\\omega)^2 , \u8fd9\u4e2a\u4e0d\u7b49\u5f0f\u5bf9 r \u662f\u7ebf\u6027\uff0c\u51f8\u7684\u3002\u56e0\u800c\u5e45\u503c\u4f18\u5316\u95ee\u9898\u4e00\u822c\u89e3\u51b3\u65b9\u6cd5\u662f\u5c06\u95ee\u9898\u8f6c\u6362\u4e3a\u5173\u4e8e r \u7684\u51fd\u6570\uff0c\u7136\u540e\u7528spectral factorization\u5c06 r \u91cd\u65b0\u8f6c\u6362\u4e3a h","title":"magnitude \u4f18\u5316\u95ee\u9898"},{"location":"The_theory/ConvexOptimization/#log-","text":"\u4f18\u5316\u95ee\u9898\u4e13\u6ce8\u4e8e\u5e45\u503c\u7684\u5206\u8d1d\u503c\uff0c\u4f18\u5316\u662f\u8981\u6c42\u6700\u5c0f\u5316\u4e0e\u76ee\u6807\u8c31\u7684\u504f\u5dee \\text{minimize} \\underset{\\omega \\in [0,\\pi]}{max} |20\\log_{10}|H(\\omega)| - 20 \\log_{10}D(\\omega)| \u5176\u4e2d D \u6307\u7684\u662f\u76ee\u6807\u7684\u51fd\u6570\u7684\u5e45\u503c\u51fd\u6570. \u89e3\u51b3\u65b9\u6848: \u5f15\u5165\u65b0\u53d8\u91cf t , epigraph\u91cd\u5199max\u51fd\u6570 \u5c06\u95ee\u9898\u8f6c\u6362\u4e3a\u529f\u7387\u8c31\u51fd\u6570\u95ee\u9898 -t< 10\\log_{10}R(\\omega) - 10\\log_{10}D^2(\\omega) < t .\u6ce8\u610f R \u662f\u5173\u4e8e r \u7ebf\u6027\u7684. \u5229\u7528\u5bf9\u6570\u4e0e\u6307\u6570\u7684\u5355\u8c03\u6027\uff0c\u4e0d\u7b49\u5f0f\u4e24\u8fb9\u540c\u65f6\u53d6 10^x ,\u6362\u5143 \\tilde{t} = 10^t , \u540c\u65f6\u5c06\u4f18\u5316\u95ee\u9898\u4ece\u4f18\u5316 t \u76f4\u63a5\u6539\u4e3a\u4f18\u5316 \\tilde{t} 1/\\tilde{t} \\le R(\\omega) / D^2(\\omega) \\le \\tilde{t} \u53cd\u6bd4\u4f8b\u51fd\u6570\u4e0e\u7ebf\u6027\u51fd\u6570\u90fd\u6ee1\u8db3\u51f8\u4f18\u5316\u6761\u4ef6\uff0c\u95ee\u9898\u5212\u5f52\u4e3a\u51f8\u4f18\u5316\u95ee\u9898.","title":"Log-\u5207\u6bd4\u96ea\u592b \u5e45\u503c\u8bbe\u8ba1"},{"location":"The_theory/ConvexOptimization/#markowitz-modern-portfolio-theory","text":"\u8fd9\u4e2a \u91d1\u878d\u5b66\u95ee\u9898 \u662f\u4e00\u4e2a\u8ba8\u8bba\u6295\u8d44\u51b3\u7b56\u7684\u7b80\u5355\u6a21\u578b\u3002\u5176\u5c06\u6bcf\u4e00\u9879\u6295\u8d44\u7684\u62a5\u916c\u7406\u89e3\u4e3a\u4e00\u4e2a\u9ad8\u65af\u5206\u5e03;\u4e0d\u540c\u7684\u6295\u8d44\u7ec4\u6210\u7684\u8054\u5408\u5206\u5e03\u662f\u4e00\u4e2a(\u53ef\u80fd)\u6709\u76f8\u5173\u6027\u7684\u591a\u5143\u9ad8\u65af\u5206\u5e03;\u6295\u8d44\u8005\u7684\u51b3\u7b56\uff0c\u4e5f\u5c31\u662f\u6295\u8d44\u7ec4\u5408\u662f\u8d44\u4ea7\u7684\u52a0\u6743\u7ec4\u5408;\u6295\u8d44\u7ec4\u5408\u7684\u62a5\u916c\u4e5f\u5c06\u4f1a\u662f\u4e00\u4e2a\u9ad8\u65af\u5206\u5e03. \u6295\u8d44\u8005\u7684\u51b3\u7b56\u5c06\u4f1a\u662f\u6700\u5927\u5316\u6295\u8d44\u7ec4\u5408\u7684\u671f\u671b\u6536\u5165\uff0c\u540c\u65f6\u5e0c\u671b\u51cf\u5c11\u6295\u8d44\u62a5\u916c\u7684\u4e0d\u786e\u5b9a\u6027.","title":"Markowitz Modern Portfolio Theory"},{"location":"The_theory/ConvexOptimization/#mean-variance-portfolio-mvp","text":"\\begin{aligned} \\underset{w}{maximize} \\quad &w^T\\mu - \\lambda w^T \\Sigma w \\\\ \\text{subject to} \\quad&1^T w = 1 \\end{aligned} \u4f7f\u7528KKT\u53ef\u4ee5\u76f4\u63a5\u5f97\u5230\u7ed3\u679c\u3002 \\begin{aligned} w_{MVP} &= \\frac{1}{\\lambda} \\Sigma^{-1} (\\mu + v \\cdot 1)\\\\ v &= \\frac{2\\lambda - 1^T\\Sigma^{-1}\\mu}{1^T\\Sigma^{-1}\\cdot 1} \\end{aligned} \u62d3\u5c55\u601d\u8003\uff0c\u8fdb\u4e00\u6b65\u8003\u8651\u5230 \\mu \u4ee5\u53ca \\Sigma \u90fd\u662f\u4ece\u6570\u636e\u6837\u672c\u7684\u5747\u503c\u4e0e\u65b9\u5dee\u4e2d\u4f30\u8ba1\u51fa\u6765\u7684\uff0c\u56e0\u800c\u8fd9\u4e2a\u4f18\u5316\u95ee\u9898\u53ef\u4ee5\u7406\u89e3\u4e3a\u5bf9\u6837\u672c\u6570\u636e\u7684 l_2 -regularized\u7684\u56de\u5f52\u95ee\u9898. \u901a\u8fc7\u6dfb\u52a0\u4e00\u4e9b\u65b0\u7684\u7b80\u5355\u7ea6\u675f\uff0c\u53ef\u4ee5\u5728\u4e0d\u5f71\u54cd\u51f8\u4f18\u5316\u6027\u8d28\u7684\u57fa\u7840\u4e0a\u4f7f\u7ed3\u679c\u66f4\u9c81\u68d2.","title":"\u57fa\u7840\u95ee\u9898\u4e00: mean-variance portfolio (MVP)"},{"location":"The_theory/ConvexOptimization/#global-minimum-variance-portfolio","text":"\\begin{aligned} \\underset{w}{\\text{minimize}} \\quad &w^T\\Sigma w\\\\ \\text{subject to} \\quad & w^T\\mu \\ge \\beta\\\\ &1^T w = 1 \\end{aligned} \u540c\u6837\u53ef\u4ee5\u901a\u8fc7KKT\u6761\u4ef6\u76f4\u63a5\u5f97\u5230\u7ed3\u679c w_{GMVP} = \\frac{1}{1^T\\Sigma^{-1}\\cdot 1} \\Sigma^{-1} \\cdot 1","title":"\u57fa\u7840\u95ee\u9898\u4e8c: Global minimum variance portfolio"},{"location":"The_theory/ConvexOptimization/#maximum-sharpe-ratio-portfolio-msrp","text":"\\begin{aligned} \\underset{w}{\\text{maximize}} \\quad &\\frac{w^T\\mu - r_f}{\\sqrt{w^T\\Sigma w}} \\\\ \\text{subject to}\\quad & 1^T w = 1 \\end{aligned} \u8fd9\u4e2a\u95ee\u9898\u5e76\u4e0d\u662f\u51f8\u4f18\u5316\u95ee\u9898\uff0c\u4f46\u662f\u53ef\u4ee5\u88ab\u5f52\u7eb3\u4e3a concave-convex single-ratio fractional programming. \u4e5f\u5c31\u662f\u6700\u5927\u5316\u4e00\u4e2a \u5206\u5b50\u4e3a\u975e\u8d1f\u51f9\u51fd\u6570;\u5206\u6bcd\u4e3a\u975e\u8d1f\u51f8\u51fd\u6570\u7684\u51fd\u6570. \u5176\u4e00\u822c\u5f62\u5f0f\u4e3a \\underset{w}{\\text{maximize}} \\quad \\frac{f(x)}{g(x)} \u8fd9\u91cc\u4ecb\u7ecd\u4e86\u4e09\u79cd\u89e3\u6cd5","title":"\u56f0\u96be\u95ee\u9898: Maximum Sharpe ratio portfolio (MSRP)"},{"location":"The_theory/ConvexOptimization/#_13","text":"\u5199\u6210epigraph\u5f62\u5f0f\uff0c \\begin{aligned} \\underset{x, t}{\\text{maximize}} \\quad &t \\\\ \\text{subject to} \\quad & t \\le \\frac{f(x)}{g(x)} \\end{aligned} \u5982\u679c\u5c06 t \u8bbe\u4e3a\u5e38\u91cf\uff0c\u8fd9\u4e2a\u95ee\u9898\u4f1a\u53d8\u6210\u4e00\u4e2a\u51f8\u4f18\u5316\u95ee\u9898\uff0c\u800c\u4e14\u662f\u53ef\u884c\u6027\u95ee\u9898. \\begin{aligned} \\underset{x}{\\text{maximize}} \\quad & 0 \\\\ \\text{subject to} \\quad & tg(x) \\le f(x) \\end{aligned} \u4f7f\u7528\u4e8c\u5206\u6cd5\u4e0d\u65ad\u6536\u655b t \u7684\u53d6\u503c\u8303\u56f4\uff0c\u627e\u5230\u6700\u5927\u7684\u5b58\u5728\u89e3\u7684 t ,\u5bf9\u5e94\u7684\u4f18\u5316\u7ed3\u679c\u4e3a\u76ee\u7684\u7ed3\u679c\u3002","title":"\u4e8c\u5206\u6c42\u89e3"},{"location":"The_theory/ConvexOptimization/#dinkelbach-transform","text":"\u5c06\u95ee\u9898\u8f6c\u6362\u4e3a \\begin{aligned} \\underset{x}{\\text{maximize}} \\quad & f(x) - yg(x) \\\\ \\end{aligned} \u5176\u4e2d\u65b0\u53d8\u91cf y \u4f1a\u88ab\u8fed\u4ee3\u5730\u66f4\u65b0\uff0c y^{(k)} = \\frac{f(x^{(k)})}{g(x^{(k)})} Dinkelbach\u8bc1\u660e\u4e86 y^{(k)} \u662f\u5355\u8c03\u9012\u589e\u7684\uff0c\u4e14\u7ed3\u679c\u6700\u7ec8\u4f1a\u6536\u655b\u5230 concave-convex FP\u7684\u6700\u4f18\u89e3.","title":"Dinkelbach Transform"},{"location":"The_theory/ConvexOptimization/#charnes-cooper-transform-schaible-transform","text":"\u5bf9\u4e8e\u7ebf\u6027\u7684 f(x), g(x) , \u5206\u6bcd\u90fd\u5927\u4e8e\u96f6\u7684\u60c5\u51b5. \u53ef\u4ee5\u91c7\u7528Charnes-Cooper\u8f6c\u6362\u3002 \u7ebf\u6027FP: \\begin{aligned} \\underset{x}{\\text{maximize}} \\quad & \\frac{c^Tx + \\alpha}{d^Tx+\\beta} \\\\ \\text{subject to} \\quad & Ax \\le b \\end{aligned} \u6362\u5143 y = \\frac{x}{d^Tx+\\beta} , t = \\frac{1}{d^Tx + \\beta} \u95ee\u9898\u53ef\u4ee5\u53d8\u4e3a \\begin{aligned} \\underset{y,t}{\\text{maximize}} \\quad & c^Ty + \\alpha t \\\\ \\text{subject to} \\quad & Ay \\le bt\\\\ & d^Ty + \\beta t = 1\\\\ &t \\ge 0 \\end{aligned} \u95ee\u9898\u53d8\u6210\u4e86\u4e00\u4e2aLinear programming. Shaible Transform\u662f\u4ee5\u4e0a\u8fd0\u7b97\u7684\u63a8\u5e7f\uff0c\u901a\u7528\u5730\u6765\u8bf4\uff0c \u53ef\u4ee5\u8bbe y=\\frac{x}{g(x)} , t = \\frac{1}{g(x)} , \u7ea6\u675f\u95ee\u9898\u53d8\u4e3a $$ \\begin{aligned} \\underset{y,t}{\\text{maximize}} \\quad & tf(\\frac{y}{t}) \\\\ \\text{subject to} \\quad & tg(y/t) \\le 1\\\\ &t \\ge 0 \\\\ & y/t \\in \\mathcal{X} \\quad \\text{\u4e5f\u5c31\u8bf4<script type=\"math/tex\">y/t \u8981\u6ee1\u8db3\u539f\u6765\u7684\u7ea6\u675f} \\end{aligned} $$ \u8fd9\u4e09\u79cd\u65b9\u6cd5\u90fd\u53ef\u4ee5\u7528\u4e8e\u6c42\u89e3\u539f\u6765\u7684MSRP\u95ee\u9898.","title":"Charnes-Cooper Transform / Schaible transform"},{"location":"The_theory/ConvexOptimization/#constrained-problem-optimized-for-sparsity","text":"","title":"Constrained Problem Optimized for Sparsity"},{"location":"The_theory/ConvexOptimization/#iterative-reweighted-l1-norm-heuristic","text":"set w=1 repeat: \\text{minimize}_x ||Diag(w)x||_1 subject to constrained. w_i = 1 / (\\epsilon + |x_i|) Convergence","title":"Iterative Reweighted l1-Norm Heuristic"},{"location":"The_theory/ConvexOptimization/#_14","text":"\u7b2c\u4e00\u6b21\u8fed\u4ee3\u4e3a\u76f4\u63a5\u4f18\u5316 l_1 -norm \u4e4b\u540e\u6bcf\u6b21\u8fed\u4ee3 \u5982\u679c x_i \u6bd4\u8f83\u5c0f\uff0c\u60e9\u7f5a\u6743\u91cd\u6bd4\u8f83\u5927\uff0c\u9a71\u4f7f\u5b83\u66f4\u5c0f \u5982\u679c x_i \u6bd4\u8f83\u5927\uff0c\u60e9\u7f5a\u6743\u91cd\u6bd4\u8f83\u5c0f\uff0c\u4e14\u68af\u5ea6\u4e5f\u4e0d\u5927\uff0c\u5141\u8bb8\u5b83\u53d8\u5f97\u66f4\u5927.","title":"\u76f4\u89c9:"},{"location":"The_theory/ConvexOptimization/#_15","text":"\u4f7f\u7528 \\log \u800c\u4e0d\u662f ||x||_1 \u6765\u5bf9 ||x||_0 \u8fdb\u884c\u62df\u5408. \u4f18\u5316\u95ee\u9898\u53d8\u4e3a \\begin{array}{ll}\\underset{\\mathbf{x}}{\\operatorname{minimize}} & \\sum_{i=1}^{n} \\log \\left(1+x_{i} / \\varepsilon\\right) \\\\ \\text { subject to } & \\mathbf{x} \\in \\mathscr{C}, \\quad \\mathbf{x} \\geq \\mathbf{0}\\end{array} \u5bf9\u4e8e\u8fd9\u4e2a\u95ee\u9898\uff0c \u601d\u8def\u662f\u4f7f\u7528 MM\u7b97\u6cd5\uff0c\u4ee3\u7406\u51fd\u6570\u9009\u62e9\u5207\u7ebf\u62df\u5408: \\sum^n_{i=1}\\log(1 + x_i / \\varepsilon) \\approx \\sum^n_{i=1} \\log(1 + x_i^{(k)} / \\varepsilon) + \\sum_{i=1}^n \\frac{x_i - x_i^{(k)}}{\\varepsilon + x_i^{(k)}} \u95ee\u9898\u6700\u7ec8\u53d8\u4e3a\u6c42\u89e3\u4e00\u4e2a\u5e26\u7ea6\u675f\u7684\u7ebf\u6027\u76ee\u6807\u51fd\u6570 \\begin{aligned} &\\underset{x}{\\text{minimize}} \\quad \\sum^n_{i=1}w_ix_i\\\\ &\\text{subject to} \\quad x\\in\\mathscr{C}, \\quad x \\ge 0 \\end{aligned} \u5176\u4e2d w_i = 1 / (\\varepsilon + x_i^{(k)})","title":"\u7406\u8bba\u5206\u6790"},{"location":"The_theory/ConvexOptimization/#sparse-index-tracking","text":"SIT\u7684\u610f\u601d\u662f\u91d1\u878d\u4e0a\u7528\u5c11\u6570\u4e2a\u80a1\u7968\u7684\u80a1\u4ef7\u7ebf\u6027\u52a0\u6743\u62df\u5408\u80a1\u6307(\u6052\u751f\u6307\u6570\uff0c\u4e0a\u8bc1\u6307\u6570\u7b49), \u4e3b\u8981\u96be\u70b9\u5728\u4e8e\u8fd9\u662f\u4e2a\u4e00\u4e2a\u7a00\u758f\u95ee\u9898\uff0c\u6211\u4eec\u53ea\u80fd\u9009\u62e9\u5176\u4e2d\u4e00\u90e8\u5206\u7684\u80a1\u7968\u8fdb\u884c\u62df\u5408\u3002\u8fd9\u4e2a\u9009\u62e9\u8fc7\u7a0b\u4e25\u8c28\u6765\u8bf4\u662f\u4e00\u4e2a NP\u96be\u7684\u95ee\u9898\u3002","title":"Sparse Index Tracking"},{"location":"The_theory/ConvexOptimization/#sparse-regression","text":"\\underset{w}{\\text{minimize}} \\quad ||r - Xw||_2 + \\lambda ||w||_0 lecture\u4e2d\u7ed9\u51fa\u4e86\u4f7f\u7528MM\u7b97\u6cd5\u8fdb\u884c\u6c42\u89e3\u7684\u65b9\u6848. \u7c7b\u4f3c\u4e8e\u524d\u6587\uff0c\u5b9a\u4e49\u5bf9 l_0 -norm\u7684\u8fd1\u4f3c \\rho_{p,\\gamma}(w) = \\frac{\\log{1 + |w|/p}}{\\log(1 + \\gamma/p)} \\gamma \u503c\u8d8a\u5c0f\uff0c\u5219\u5728\u903c\u8fd1\u96f6\u7684\u5c0f\u533a\u95f4\u5185\u8fd1\u4f3c\u8d8a\u51c6,\u4f46\u662f\u5728\u8f83\u5927\u7684\u533a\u95f4\u5185\u5219\u4e0d\u592a\u51c6\uff0c\u56e0\u800c\u662f\u4e00\u4e2a\u8d85\u53c2\u6570.\u4e14\u8fd9\u4e2a\u51fd\u6570\u5bf9 w\\ge0 \u662f\u4e00\u4e2a\u51f9\u51fd\u6570. \u65b0\u7684\u4f18\u5316\u76ee\u6807\u51fd\u6570: \\underset{w}{\\text{minimize}} \\quad \\frac{1}{T}||Xw-r^b||_2 + \\lambda 1^T \\rho_{p,u}(w) \u8fd9\u4e2a\u51fd\u6570\u8fd8\u4e0d\u662f\u51f8\u51fd\u6570\uff0c\u8fd9\u91cc\u91c7\u7528 MM \u7b97\u6cd5\u8fdb\u884c\u5904\u7406,\u4e0e\u524d\u4e00\u7ae0\u4e00\u81f4\uff0c\u4f7f\u7528\u5f53\u524d\u70b9\u7684\u5207\u7ebf\u4f5c\u4e3a\u4ee3\u7406\u51fd\u6570 \\approx \\frac{1}{\\log{1+\\gamma/p}} \\left\\{\\log(1 + w_i^{(k)} / \\varepsilon) + \\frac{w_i - w_i^{(k)}}{\\varepsilon + w_i^{(k)}}\\right\\} \u5728 MM \u4e2d\u9700\u8981\u8fed\u4ee3\u591a\u6b21\u6c42\u89e3\u7684\u76ee\u6807\u51fd\u6570: \\underset{w}{\\text{minimize}} \\quad \\frac{1}{T}||Xw-r^b||_2 + \\lambda d_{p,u}^{(k)T}(w) \\begin{array}{ll} \\text { subject to } & \\left.\\begin{array}{l} \\mathbf{w}^{\\top} \\mathbf{1}=1 \\\\ \\mathbf{0} \\leq \\mathbf{w} \\leq \\mathbf{1}, \\end{array}\\right\\} \\mathcal{W} \\end{array} \u8fd9\u4e2a\u51fd\u6570\u662f\u4e00\u4e2aQP,\u4ecd\u9700\u8981\u8fed\u4ee3\u3002 \u5bf9\u8fd9\u4e2aQP\u4f7f\u7528 MM ,\u4ee4 L = \\frac{1}{T}X^TX, M=\\lambda_{max}(L) I \u4ee5\u53ca\u516c\u5f0f w^T L w \\le w^T M w + 2 w^{(k)T}(L-M)w - w^{(k)T}(L-M)w^{(k)} \\begin{array}{l} \\mathbf{w}^{\\top} \\mathbf{L}_{1} \\mathbf{w}+\\left(\\lambda \\mathbf{d}_{p, u}^{(k)}-\\frac{2}{T} \\mathbf{X}^{\\top} \\mathbf{r}^{b}\\right)^{\\top} \\mathbf{w} \\\\ \\leq \\mathbf{w}^{\\top} \\mathbf{M}_{1} \\mathbf{w}+2 \\mathbf{w}^{(k)^{\\top}}\\left(\\mathbf{L}_{1}-\\mathbf{M}_{1}\\right) \\mathbf{w}-\\mathbf{w}^{(k)^{\\top}}\\left(\\mathbf{L}_{1}-\\mathbf{M}_{1}\\right) \\mathbf{w}^{(k)} \\\\ \\quad+\\left(\\lambda \\mathbf{d}_{p, u}^{(k)}-\\frac{2}{T} \\mathbf{X}^{\\top} \\mathbf{r}^{b}\\right)^{\\top} \\mathbf{w} \\\\ =\\lambda_{\\max }^{\\left(\\mathbf{L}_{1}\\right)} \\mathbf{w}^{\\top} \\mathbf{w}+\\left(2\\left(\\mathbf{L}_{1}-\\lambda_{\\max }^{\\left(\\mathbf{L}_{1}\\right)} \\mathbf{I}\\right) \\mathbf{w}^{(k)}+\\lambda \\mathbf{d}_{p, u}^{(k)}-\\frac{2}{T} \\mathbf{X}^{\\top} \\mathbf{r}^{b}\\right)^{\\top} \\mathbf{w}+\\text { const. } \\end{array} \u76ee\u6807\u51fd\u6570\u53d8\u4e3a: \\underset{w}{\\text{minimize}} \\quad w^Tw + q_1^{(k)T}w \\begin{array}{ll} \\text { subject to } & \\left.\\begin{array}{l} \\mathbf{w}^{\\top} \\mathbf{1}=1 \\\\ \\mathbf{0} \\leq \\mathbf{w} \\leq \\mathbf{1}, \\end{array}\\right\\} \\mathcal{W} \\end{array} \u5176\u4e2d \\mathbf{q}_{1}^{(k)}=\\frac{1}{\\lambda_{\\max }^{\\left(\\mathbf{L}_{1}\\right)}}\\left(2\\left(\\mathbf{L}_{1}-\\lambda_{\\max }^{\\left(\\mathbf{L}_{1}\\right)} \\mathbf{I}\\right) \\mathbf{w}^{(k)}+\\lambda \\mathbf{d}_{p, u}^{(k)}-\\frac{2}{T} \\mathbf{X}^{\\top} \\mathbf{r}^{b}\\right) \u5bf9\u8fd9\u4e2a\u95ee\u9898\u53ef\u4ee5\u7528KKT\u76f4\u63a5\u6c42\u51fa\u6700\u4f18\u89e3. \\mathcal{L}=w^Tw + q^Tw + \\mu(w^T1 -1) - \\sum h_iw + \\sum h'(w-1) . \u4f1a\u6c42\u51fa h' = 0 w_i = \\left\\{\\begin{array}{l}-\\frac{q_i+\\mu}{2}\\\\ 0\\end{array}\\right. h_i = \\left\\{\\begin{array}{ll}0 & w_i \\neq 0 \\\\ -q_i - v & w_i=0\\end{array} \\right. \u4ece h_i > 0 ,\u5f97\u5230\u5bf9 w_i \u7684\u5224\u522b\u6761\u4ef6 \\left\\{\\begin{array}{lll}v_i + q_i < 0 & w_i=0 & h_i = -v_i - q_i\\\\v_i + q_i > 0 & w_i=-\\frac{q_i+\\mu}{2} & h_i = 0 \\end{array}\\right. \u4ece\u7b49\u5f0f\u7ea6\u675f\uff0c\u5f97\u5230 \\mu = - \\frac{2 + \\sum_{i | w_i > 0}q_i}{\\text{num}\\_\\text{positive}}","title":"Sparse Regression"},{"location":"The_theory/Designing_Network_Design_Spaces/","text":"Designing Network Design Spaces \u8fd9\u4e00\u7bc7\u662f\u4f55\u51ef\u660e\u7ec4\u8d85\u8d8aNAS\u8fdb\u884c\u601d\u8003\u7684\u4e00\u9879\u5de5\u4f5c\uff0c\u4e2d\u6587\u6e90\u6709\u76f8\u5f53\u591a\u7684\u4ecb\u7ecd\uff0c \u77e5\u4e4e , CSDN NAS for image classification\uff0c\u5305\u62ec\u6700\u8fd1\u7684 EfficientNet \uff0c\u6838\u5fc3\u601d\u8def\u662f\u901a\u8fc7\u641c\u7d22\u6216\u8005\u5b66\u4e60\uff0c\u5f97\u5230\u5355\u4e00\u4e00\u4e2abest model\uff0c\u6587\u7ae0\u8ba8\u8bba\u7684\u91cd\u70b9\u4e5f\u662f\u5728\u4e8e\u5982\u4f55\u7f29\u5c0f\u65e0\u7a77\u7684\u641c\u7d22\u7a7a\u95f4\u4ee5\u53ca\u5982\u4f55\u5b66\u4e60\u7684\u65b9\u6cd5\u3002 \u672c\u6587\u7684\u6838\u5fc3\u601d\u8def\u8d21\u732e\u5728\u4e8e\u5bf9\u641c\u7d22\u7a7a\u95f4\u8fdb\u884c\u5927\u5e45\u5ea6\u91c7\u6837\u3001\u7edf\u8ba1\u5206\u6790\uff0c\u518d\u8fdb\u884c\u526a\u679d\u5206\u6790\uff0c\u5f97\u5230\u4e00\u4e9b\u66f4\u9c81\u68d2\u7684\u5206\u6790\u7ed3\u8bba\uff0c\u6240\u4ee5\u79f0\u4e3a\"designing network design spaces\"\u3002 \u641c\u7d22\u7a7a\u95f4\u63a2\u7d22 \u4f5c\u8005\u7684\u601d\u8def\u662f\u642d\u5efa\u63a7\u5236\u6a21\u578b\u7684FLOPS\u5728400MFLOPS\u7ea7\u522b\uff0c\u5728\u5206\u6790\u67d0\u4e00\u4e2a\u641c\u7d22\u7a7a\u95f4\u7684\u65f6\u5019\uff0c\u5728\u5176\u4e2d\u91c7\u6837500\u4e2a\u6a21\u578b\uff0c\u5feb\u901f\u5728imagenet\u4e0a\u8bad\u7ec3(\u4f7f\u7528\u6700\u7b80\u5355\u7684\u8bbe\u5b9a\uff0c\u6bd4\u5982\u51cf\u5c11\u6570\u636e\u589e\u5f3a\uff0cepoch\u4ec5\u4e3a10) \u7edf\u8ba1\u5206\u6790\u8fd9\u4e9b\u6a21\u578b\u7684\u6027\u80fd\uff0c\u7528\u6765\u8bc4\u4f30\u8fd9\u4e2a\u8bbe\u8ba1\u7a7a\u95f4\u7684\u6027\u80fd\uff0c\u540c\u65f6\u6839\u636e\u8bbe\u5b9a\u7684\u4e00\u4e9b\u53d8\u91cf\u4e0e\u6027\u80fd\u7684\u76f8\u5173\u6027\u627e\u51fa\u4fee\u526a\u6216\u8005\u6539\u8fdb\u641c\u7d22\u7a7a\u95f4\u7684\u65b9\u5411\u3002 \u4f5c\u8005\u4eceAnyNetXa\u9010\u6b65\u7f29\u5c0f\u7a7a\u95f4\u65e9AnyNetXe,\u6700\u540e\u5230RegNet\u7684\u641c\u7d22\u7a7a\u95f4\u3002\u8fd9\u4e2a\u8fc7\u7a0b\u4e2d\u6709\u4e00\u7cfb\u5217\u7684\u7ed3\u8bba \u7f51\u7edc\u6df1\u5ea6\u6700\u597d\u572820\u4e2ablock\uff0c\u4e5f\u5c31\u662f60\u5c42\u9644\u8fd1\uff0c bottleneck ratio\u5e94\u8be5\u53d61\uff0c\u4e5f\u5c31\u662f\u4e0d\u4f7f\u7528Bottleneck \u6bcf\u6b21\u4e0b\u91c7\u6837\uff0cchannel\u5bbd\u5ea6\u7684\u4e58\u79ef\u5e94\u5f53\u4e3a2.5\u5de6\u53f3\uff0c\u800c\u4e0d\u662f\u666e\u904d\u4f7f\u7528\u76842(\u4e5f\u76f8\u8fd1) \"activation\"\u4e5f\u5c31\u662f\u5377\u79ef\u5c42\u8f93\u51fatensor\u7684\u5927\u5c0f\u4e0e\u8fd0\u7b97\u65f6\u95f4\u7684\u76f8\u5173\u6027\u8d85\u8d8aFLOPS inverted bottleneck\u4ee5\u53ca depthwise_conv(\u6bcf\u4e00\u4e2achannel\u4e00\u7ec4\u5377\u79ef\u53c2\u6570)\u5bf9\u6027\u80fd\u662f\u6709\u635f\u5bb3\u7684\u3002 squeeze and excitation\u662f\u6709\u6548\u7684 \u6700\u540e\u5c06 EfficientNet \u7684\u70b9\u6570\u9524\u4e86\u4e00\u4e0b,\u4e14\u66f4\u52a0\u8f7b\u91cf.","title":"Designing Network Design Spaces"},{"location":"The_theory/Designing_Network_Design_Spaces/#designing-network-design-spaces","text":"\u8fd9\u4e00\u7bc7\u662f\u4f55\u51ef\u660e\u7ec4\u8d85\u8d8aNAS\u8fdb\u884c\u601d\u8003\u7684\u4e00\u9879\u5de5\u4f5c\uff0c\u4e2d\u6587\u6e90\u6709\u76f8\u5f53\u591a\u7684\u4ecb\u7ecd\uff0c \u77e5\u4e4e , CSDN NAS for image classification\uff0c\u5305\u62ec\u6700\u8fd1\u7684 EfficientNet \uff0c\u6838\u5fc3\u601d\u8def\u662f\u901a\u8fc7\u641c\u7d22\u6216\u8005\u5b66\u4e60\uff0c\u5f97\u5230\u5355\u4e00\u4e00\u4e2abest model\uff0c\u6587\u7ae0\u8ba8\u8bba\u7684\u91cd\u70b9\u4e5f\u662f\u5728\u4e8e\u5982\u4f55\u7f29\u5c0f\u65e0\u7a77\u7684\u641c\u7d22\u7a7a\u95f4\u4ee5\u53ca\u5982\u4f55\u5b66\u4e60\u7684\u65b9\u6cd5\u3002 \u672c\u6587\u7684\u6838\u5fc3\u601d\u8def\u8d21\u732e\u5728\u4e8e\u5bf9\u641c\u7d22\u7a7a\u95f4\u8fdb\u884c\u5927\u5e45\u5ea6\u91c7\u6837\u3001\u7edf\u8ba1\u5206\u6790\uff0c\u518d\u8fdb\u884c\u526a\u679d\u5206\u6790\uff0c\u5f97\u5230\u4e00\u4e9b\u66f4\u9c81\u68d2\u7684\u5206\u6790\u7ed3\u8bba\uff0c\u6240\u4ee5\u79f0\u4e3a\"designing network design spaces\"\u3002","title":"Designing Network Design Spaces"},{"location":"The_theory/Designing_Network_Design_Spaces/#_1","text":"\u4f5c\u8005\u7684\u601d\u8def\u662f\u642d\u5efa\u63a7\u5236\u6a21\u578b\u7684FLOPS\u5728400MFLOPS\u7ea7\u522b\uff0c\u5728\u5206\u6790\u67d0\u4e00\u4e2a\u641c\u7d22\u7a7a\u95f4\u7684\u65f6\u5019\uff0c\u5728\u5176\u4e2d\u91c7\u6837500\u4e2a\u6a21\u578b\uff0c\u5feb\u901f\u5728imagenet\u4e0a\u8bad\u7ec3(\u4f7f\u7528\u6700\u7b80\u5355\u7684\u8bbe\u5b9a\uff0c\u6bd4\u5982\u51cf\u5c11\u6570\u636e\u589e\u5f3a\uff0cepoch\u4ec5\u4e3a10) \u7edf\u8ba1\u5206\u6790\u8fd9\u4e9b\u6a21\u578b\u7684\u6027\u80fd\uff0c\u7528\u6765\u8bc4\u4f30\u8fd9\u4e2a\u8bbe\u8ba1\u7a7a\u95f4\u7684\u6027\u80fd\uff0c\u540c\u65f6\u6839\u636e\u8bbe\u5b9a\u7684\u4e00\u4e9b\u53d8\u91cf\u4e0e\u6027\u80fd\u7684\u76f8\u5173\u6027\u627e\u51fa\u4fee\u526a\u6216\u8005\u6539\u8fdb\u641c\u7d22\u7a7a\u95f4\u7684\u65b9\u5411\u3002 \u4f5c\u8005\u4eceAnyNetXa\u9010\u6b65\u7f29\u5c0f\u7a7a\u95f4\u65e9AnyNetXe,\u6700\u540e\u5230RegNet\u7684\u641c\u7d22\u7a7a\u95f4\u3002\u8fd9\u4e2a\u8fc7\u7a0b\u4e2d\u6709\u4e00\u7cfb\u5217\u7684\u7ed3\u8bba \u7f51\u7edc\u6df1\u5ea6\u6700\u597d\u572820\u4e2ablock\uff0c\u4e5f\u5c31\u662f60\u5c42\u9644\u8fd1\uff0c bottleneck ratio\u5e94\u8be5\u53d61\uff0c\u4e5f\u5c31\u662f\u4e0d\u4f7f\u7528Bottleneck \u6bcf\u6b21\u4e0b\u91c7\u6837\uff0cchannel\u5bbd\u5ea6\u7684\u4e58\u79ef\u5e94\u5f53\u4e3a2.5\u5de6\u53f3\uff0c\u800c\u4e0d\u662f\u666e\u904d\u4f7f\u7528\u76842(\u4e5f\u76f8\u8fd1) \"activation\"\u4e5f\u5c31\u662f\u5377\u79ef\u5c42\u8f93\u51fatensor\u7684\u5927\u5c0f\u4e0e\u8fd0\u7b97\u65f6\u95f4\u7684\u76f8\u5173\u6027\u8d85\u8d8aFLOPS inverted bottleneck\u4ee5\u53ca depthwise_conv(\u6bcf\u4e00\u4e2achannel\u4e00\u7ec4\u5377\u79ef\u53c2\u6570)\u5bf9\u6027\u80fd\u662f\u6709\u635f\u5bb3\u7684\u3002 squeeze and excitation\u662f\u6709\u6548\u7684 \u6700\u540e\u5c06 EfficientNet \u7684\u70b9\u6570\u9524\u4e86\u4e00\u4e0b,\u4e14\u66f4\u52a0\u8f7b\u91cf.","title":"\u641c\u7d22\u7a7a\u95f4\u63a2\u7d22"},{"location":"The_theory/DirectLossMinimization/","text":"Direct Loss Minimization \u672c\u6587\u96c6\u4e2d\u5206\u6790\u51e0\u7bc7\u4e0edirect loss minimization\u76f8\u5173\u7684\u6587\u7ae0\u3002\u5176\u6700\u7ec8\u53cd\u5e94\u7684\u76f4\u89c2\u89e3\u91ca\u662f\u5728task-based loss\u6216\u8005reward\u7684\u5f15\u5bfc\u4e0b\u8ba9\u7f51\u7edc\u5411\u66f4\u9ad8\u7684\u8f93\u51fa\u6743\u91cd\u9760\u62e2\u3002 \u901a\u5e38\u4f18\u5316\u95ee\u9898\u7684\u76ee\u6807\u662f\u627e\u5230 w^* , \u4f7f\u5f97 w^* = \\underset{w}{argmin}E[L(y, y_w(x))] ,\u4e5f\u5c31\u662f\u76ee\u6807\u8f93\u51fa\u4e0e\u57fa\u4e8e\u6743\u91cd\u7684\u8f93\u51fa\u5dee\u8ddd\u6700\u5c0f. \u5f53 L \u76f8\u5bf9\u4e8e y \u662f\u4e0d\u53ef\u5bfc\u7684\u65f6\u5019\uff0c\u65e0\u6cd5\u53cd\u4f20\u3002 \u4e0e\u5f97\u5230task-related loss\u76f4\u63a5\u56de\u4f20\u4e0d\u540c,\u5b83\u7684\u601d\u8def\u662f\u6839\u636etask-related loss\u4ee5\u53ca\u7f51\u7edc\u672c\u8eab\u7684\u8f93\u51fa w_0 \uff0c\u641c\u7d22\u51fa\u53ef\u80fd\u66f4\u597d\u7684\u4e00\u7ec4\u8f93\u51fa w_1 , \u7136\u540e\u5b66\u4e60\u7b97\u6cd5\u7684\u8bad\u7ec3\u51fd\u6570\u5219\u662f\u5f15\u5bfc\u6a21\u578b\u7684\u8f93\u51fa\u8fdc\u79bb w_0 , \u9760\u8fd1 w_1 . \u6838\u5fc3 Motivation \u4e0e\u5960\u57fa: Direct Loss Minimization for Structured Prediction pdf \u8fd9\u7bc72010\u7684NIPS\u4ece\u4e24\u4e2a\u89d2\u5ea6\u5f15\u5165\u4e86 direct loss minimization. Perceptron Training Algorithm Structural SVM Perceptron\u7684\u7b97\u6cd5\u672c\u7ad9\u5728 \u6b64\u5904 \u7531\u7b80\u4ecb.\u672c\u6587\u91cd\u8ff0\u4e3a w^{t+1} = w^{t} + \\phi(x_t, y_i) - \\phi(x_t, y_{w^t}(x_t)) \u5176\u4e2d w \u4e3a\u7ebf\u6027\u6743\u91cd\uff0c \\phi \u4e3a\u7279\u5f81\u77e2\u91cf.\u8fd9\u4e2a\u57fa\u7840\u7684\u66f4\u65b0\u6cd5\u5219\u662f\u4e0eLoss\u65e0\u5173\u7684. Structural SVM\u7684\u5e7f\u4e49\u4f18\u5316\u95ee\u9898\u4e3a: w^* = \\underset{w}{argmin} E[\\underset{\\bar y \\in\\mathcal{Y}}{max}(L(y, \\bar y) - w^T (\\phi(x, y) - \\phi(x, \\bar y))) ] \u4e00\u4e2a\u89e3\u6cd5\u662f\u5148\u8ba1\u7b97\u51fahinge loss\u91cc\u9762 max \u5185\u7684 y \u7684\u53d6\u503c y^t_{hinge} . \u6743\u91cd\u66f4\u65b0\u53ef\u4ee5\u5199\u4e3a w^{t+1} = w^t + \\eta^t(\\phi(x_t, y_t) - \\phi(x_t, y_{hinge}^t)) \u4f5c\u8005\u8fdb\u4e00\u6b65\u52a0\u5165\u4e86\u53c2\u6570 \\epsilon \\begin{array}{c} w^{t+1}=w^{t}+\\eta^{t}\\left(\\phi\\left(x_{t}, y_{w^{t}}\\left(x_{t}\\right)\\right)-\\phi\\left(x_{t}, y_{\\text {direct }}^{t}\\right)\\right) \\\\ y_{\\text {direct }}^{t}=\\underset{\\tilde{y} \\in \\mathcal{Y}}{\\operatorname{argmax}}\\left(w^{t}\\right)^{\\top} \\phi\\left(x_{t}, \\tilde{y}\\right)+\\epsilon^{t} L(y, \\tilde{y}) \\end{array} \u4f5c\u8005\u4e3b\u8981\u7ed3\u679c\u5c31\u662f\u8bc1\u660e\u4e86\uff0c\u5f53 \\epsilon \u903c\u8fd1\u96f6\u7684\u65f6\u5019\uff0c\u66f4\u65b0\u65b9\u5411\u903c\u8fd1\u4e8e \\nabla_w E[y, y_m(x)] \u4e5f\u5c31\u662f\u8bf4, \\nabla_w E[L(y, y_m(x))] = \\underset{\\epsilon \\rightarrow 0}{lim}\\frac{1}{\\epsilon} E[\\phi(x, y_{direct}) - \\phi(x, y_m(x))] \u5176\u4e2d y_{direct} = \\underset{\\bar y \\in \\mathcal{Y}}{argmax} w^T\\phi(x, \\bar y)+ \\epsilon L(y, \\bar y) Training Deep Neural Networks via Direct Loss Minimization pdf \u8fd9\u7bc7paper\u662f AP-Loss \u7684\u5f15\u6587\u4e4b\u4e00. \u63d0\u51fa\u4e86\u4f7f\u7528 Direct Loss minimization\uff0c\u7528AP\u4f5c\u4e3a\u5f15\u5bfc\u8bad\u7ec3\u5206\u7c7b\u7f51\u7edc\u3002 \u53d6\u524d\u6587\u7684\u4f18\u5316\u95ee\u9898\u7684\u5904\u7406\uff0c\u8bbe\u7f6eTask Loss L = L_{AP} \u3002 y_w \u662f\u7f51\u7edc\u76f4\u63a5\u7684\u8f93\u51fa logits. \u68af\u5ea6 \\begin{aligned} & \\nabla_{w} \\mathbb{E}\\left[L\\left(y, y_{w}\\right)\\right] \\\\ =& \\pm \\lim _{\\epsilon \\rightarrow 0} \\frac{1}{\\epsilon} \\mathbb{E}\\left[\\nabla_{w} F\\left(x, y_{\\text {direct }}, w\\right)-\\nabla_{w} F\\left(x, y_{w}, w\\right)\\right] \\end{aligned} \u91cd\u70b9\u96be\u70b9\u5728\u4e8e\u8ba1\u7b97 y_{direct} = \\underset{\\hat y \\in\\mathcal{Y}}{argmax} F(x, \\hat y, w) + \\epsilon L_{AP}(y, \\hat y) \u5176\u6280\u5de7\u5e76\u4e0d\u901a\u7528(\u76ee\u524d\u60c5\u51b5\u6765\u8bf4\u663e\u8457\u5730\u6bd4\u4e0d\u4e0a AP-Loss \u7684\u7b97\u6cd5)\u3002\u4e0d\u5728\u672c\u7ad9\u5c55\u5f00. Direct Policy Gradients: Direct Optimization of Policies in Discrete Action Spaces pdf code \u8fd9\u7bc7NIPS2020 paper\u5219\u5c06Direct Loss minimization\u7528\u5230\u4e86policy gradient\u4e0a\uff0c\u5904\u7406\u79bb\u6563\u7684action spaces\u3002\u5de7\u5999\u7684\u5730\u65b9\u5728\u4e8e\u4f7f\u5f97A*\u7684\u526a\u679d-\u52a0\u901f\u641c\u7d22\u7684\u65b9\u6cd5\u5e94\u7528\u5230\u4e86\u5f3a\u5316\u5b66\u4e60\u4e2d\u3002 \u7531\u4e8e\u9700\u8981\u5bf9\u884c\u52a8\u8fdb\u884c\u91c7\u6837\uff0c\u53c8\u8fdb\u4e00\u6b65\u5f15\u5165\u4e86 Gumbel-reparametrization Policy Gradient \\begin{aligned} \\nabla_{\\theta} J\\left(\\pi_{\\theta}\\right) &=\\nabla_{\\theta} \\underset{\\tau \\sim \\pi_{\\theta}}{\\mathrm{E}}[R(\\tau)] \\\\ &=\\nabla_{\\theta} \\int_{\\tau} P(\\tau \\mid \\theta) R(\\tau) \\\\ &=\\int_{\\tau} \\nabla_{\\theta} P(\\tau \\mid \\theta) R(\\tau) \\\\ &=\\int_{\\tau} P(\\tau \\mid \\theta) \\nabla_{\\theta} \\log P(\\tau \\mid \\theta) R(\\tau) \\\\ &=\\underset{\\tau \\sim \\pi_{\\theta}}{\\mathrm{E}}\\left[\\nabla_{\\theta} \\log P(\\tau \\mid \\theta) R(\\tau)\\right] \\\\ &=\\underset{\\tau \\sim \\pi_{\\theta}}{\\mathrm{E}}\\left[\\nabla_{\\theta}(\\log \\rho_0(s_0) + \\sum_{t=0}^T(\\log P(s_{t+1}|s_t, a_t) + \\log \\pi_\\theta(a_t| s_t))) R(\\tau)\\right] \\\\ \\therefore \\nabla_{\\theta} J\\left(\\pi_{\\theta}\\right) &=\\underset{\\tau \\sim \\pi_{\\theta}}{\\mathrm{E}}\\left[\\sum_{t=0}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta}\\left(a_{t} \\mid s_{t}\\right) R(\\tau)\\right] \\end{aligned} Direct Policy Gradient \u4e0e\u57fa\u7840\u7684policy gradient\u76f4\u63a5\u4ece \\pi_\\theta \u91c7\u6837\u4e0d\u540c\uff0c\u672c\u6587\u7684\u8f93\u51faaction\u5b9a\u4e49\u4e3a \\begin{aligned} a^*(\\epsilon) &= \\underset{a}{argmax}D_\\theta(a; \\Gamma, S, \\epsilon) \\\\ D_\\theta(a; \\Gamma, S, \\epsilon) &= \\log \\Pi_\\theta(a|S) + \\Gamma(a) + \\epsilon R(a, S) \\end{aligned} \u5176\u4e2d \\Gamma \u662fgumbel\u968f\u673a\u91c7\u6837\u503c \u8bbeaction a \u7684\u91c7\u6837\u662f\u57fa\u4e8e\u4e8c\u5143\u51fd\u6570 D_\\theta(S, \\epsilon) \uff0c \u8fdb\u800c\u4fee\u6539\u5f97\u5230\u68af\u5ea6\u7ed3\u679c \\nabla_{\\theta} \\underset{\\tau \\sim \\pi_{\\theta}}{\\mathrm{E}}[R(\\tau)] = \\frac{1}{\\epsilon} \\mathbb{E}_{\\boldsymbol{S} \\sim P, \\Gamma}\\left[\\nabla_{\\theta} \\log \\Pi_{\\theta}\\left(\\boldsymbol{a}^{*}(\\epsilon) \\mid \\boldsymbol{S}\\right)-\\nabla_{\\theta} \\log \\Pi_{\\theta}\\left(\\boldsymbol{a}^{*}(0) \\mid \\boldsymbol{S}\\right)\\right] \u5176\u4e2d a^*(0) \u662f\u4e0d\u8003\u8651\u68af\u5ea6\u7ed3\u679c,\u7f51\u7edc\u6a21\u578b\u539f\u6765\u7684\u8f93\u51fa\u91c7\u6837\u51fa\u6765\u7684\u7ed3\u679c\uff0c\u8fd9\u91cc\u6587\u7ae0\u79f0\u4e4b\u4e3a a_{opt} . a^*(\\epsilon) \u662f\u5e26\u6709reward\u65f6\u63a8\u7406\u7684\u7ed3\u679c\uff0c\u4e0edirect loss minimization \u8303\u5f0f\u5bf9\u7167\uff0c\u8fd9\u91cc\u79f0\u4e4b\u4e3a a_{direct} \u641c\u7d22\u7b97\u6cd5: \u4f5c\u8005\u8fdb\u4e00\u6b65\u8003\u8651\u5230\u53ef\u4ee5\u4f7f\u7528 heuristic function H = G_\\theta(R) + \\epsilon(reward\\_so\\_far + bound\\_reward\\_togo) \u5728\u641c\u7d22\u6700\u4f73\u7684 d_{direct} \u7684\u8fc7\u7a0b\u4e2d\u4f7f\u7528 A*\u4fee\u526a\u6389expected results\u6bd4\u8f83\u4f4e\u7684\u90e8\u5206\uff0c\u52a0\u5feb\u641c\u7d22\u8fdb\u5ea6.","title":"Direct Loss Minimization"},{"location":"The_theory/DirectLossMinimization/#direct-loss-minimization","text":"\u672c\u6587\u96c6\u4e2d\u5206\u6790\u51e0\u7bc7\u4e0edirect loss minimization\u76f8\u5173\u7684\u6587\u7ae0\u3002\u5176\u6700\u7ec8\u53cd\u5e94\u7684\u76f4\u89c2\u89e3\u91ca\u662f\u5728task-based loss\u6216\u8005reward\u7684\u5f15\u5bfc\u4e0b\u8ba9\u7f51\u7edc\u5411\u66f4\u9ad8\u7684\u8f93\u51fa\u6743\u91cd\u9760\u62e2\u3002 \u901a\u5e38\u4f18\u5316\u95ee\u9898\u7684\u76ee\u6807\u662f\u627e\u5230 w^* , \u4f7f\u5f97 w^* = \\underset{w}{argmin}E[L(y, y_w(x))] ,\u4e5f\u5c31\u662f\u76ee\u6807\u8f93\u51fa\u4e0e\u57fa\u4e8e\u6743\u91cd\u7684\u8f93\u51fa\u5dee\u8ddd\u6700\u5c0f. \u5f53 L \u76f8\u5bf9\u4e8e y \u662f\u4e0d\u53ef\u5bfc\u7684\u65f6\u5019\uff0c\u65e0\u6cd5\u53cd\u4f20\u3002 \u4e0e\u5f97\u5230task-related loss\u76f4\u63a5\u56de\u4f20\u4e0d\u540c,\u5b83\u7684\u601d\u8def\u662f\u6839\u636etask-related loss\u4ee5\u53ca\u7f51\u7edc\u672c\u8eab\u7684\u8f93\u51fa w_0 \uff0c\u641c\u7d22\u51fa\u53ef\u80fd\u66f4\u597d\u7684\u4e00\u7ec4\u8f93\u51fa w_1 , \u7136\u540e\u5b66\u4e60\u7b97\u6cd5\u7684\u8bad\u7ec3\u51fd\u6570\u5219\u662f\u5f15\u5bfc\u6a21\u578b\u7684\u8f93\u51fa\u8fdc\u79bb w_0 , \u9760\u8fd1 w_1 .","title":"Direct Loss Minimization"},{"location":"The_theory/DirectLossMinimization/#motivation-direct-loss-minimization-for-structured-prediction","text":"pdf \u8fd9\u7bc72010\u7684NIPS\u4ece\u4e24\u4e2a\u89d2\u5ea6\u5f15\u5165\u4e86 direct loss minimization. Perceptron Training Algorithm Structural SVM Perceptron\u7684\u7b97\u6cd5\u672c\u7ad9\u5728 \u6b64\u5904 \u7531\u7b80\u4ecb.\u672c\u6587\u91cd\u8ff0\u4e3a w^{t+1} = w^{t} + \\phi(x_t, y_i) - \\phi(x_t, y_{w^t}(x_t)) \u5176\u4e2d w \u4e3a\u7ebf\u6027\u6743\u91cd\uff0c \\phi \u4e3a\u7279\u5f81\u77e2\u91cf.\u8fd9\u4e2a\u57fa\u7840\u7684\u66f4\u65b0\u6cd5\u5219\u662f\u4e0eLoss\u65e0\u5173\u7684. Structural SVM\u7684\u5e7f\u4e49\u4f18\u5316\u95ee\u9898\u4e3a: w^* = \\underset{w}{argmin} E[\\underset{\\bar y \\in\\mathcal{Y}}{max}(L(y, \\bar y) - w^T (\\phi(x, y) - \\phi(x, \\bar y))) ] \u4e00\u4e2a\u89e3\u6cd5\u662f\u5148\u8ba1\u7b97\u51fahinge loss\u91cc\u9762 max \u5185\u7684 y \u7684\u53d6\u503c y^t_{hinge} . \u6743\u91cd\u66f4\u65b0\u53ef\u4ee5\u5199\u4e3a w^{t+1} = w^t + \\eta^t(\\phi(x_t, y_t) - \\phi(x_t, y_{hinge}^t)) \u4f5c\u8005\u8fdb\u4e00\u6b65\u52a0\u5165\u4e86\u53c2\u6570 \\epsilon \\begin{array}{c} w^{t+1}=w^{t}+\\eta^{t}\\left(\\phi\\left(x_{t}, y_{w^{t}}\\left(x_{t}\\right)\\right)-\\phi\\left(x_{t}, y_{\\text {direct }}^{t}\\right)\\right) \\\\ y_{\\text {direct }}^{t}=\\underset{\\tilde{y} \\in \\mathcal{Y}}{\\operatorname{argmax}}\\left(w^{t}\\right)^{\\top} \\phi\\left(x_{t}, \\tilde{y}\\right)+\\epsilon^{t} L(y, \\tilde{y}) \\end{array} \u4f5c\u8005\u4e3b\u8981\u7ed3\u679c\u5c31\u662f\u8bc1\u660e\u4e86\uff0c\u5f53 \\epsilon \u903c\u8fd1\u96f6\u7684\u65f6\u5019\uff0c\u66f4\u65b0\u65b9\u5411\u903c\u8fd1\u4e8e \\nabla_w E[y, y_m(x)] \u4e5f\u5c31\u662f\u8bf4, \\nabla_w E[L(y, y_m(x))] = \\underset{\\epsilon \\rightarrow 0}{lim}\\frac{1}{\\epsilon} E[\\phi(x, y_{direct}) - \\phi(x, y_m(x))] \u5176\u4e2d y_{direct} = \\underset{\\bar y \\in \\mathcal{Y}}{argmax} w^T\\phi(x, \\bar y)+ \\epsilon L(y, \\bar y)","title":"\u6838\u5fc3 Motivation \u4e0e\u5960\u57fa: Direct Loss Minimization for Structured Prediction"},{"location":"The_theory/DirectLossMinimization/#training-deep-neural-networks-via-direct-loss-minimization","text":"pdf \u8fd9\u7bc7paper\u662f AP-Loss \u7684\u5f15\u6587\u4e4b\u4e00. \u63d0\u51fa\u4e86\u4f7f\u7528 Direct Loss minimization\uff0c\u7528AP\u4f5c\u4e3a\u5f15\u5bfc\u8bad\u7ec3\u5206\u7c7b\u7f51\u7edc\u3002 \u53d6\u524d\u6587\u7684\u4f18\u5316\u95ee\u9898\u7684\u5904\u7406\uff0c\u8bbe\u7f6eTask Loss L = L_{AP} \u3002 y_w \u662f\u7f51\u7edc\u76f4\u63a5\u7684\u8f93\u51fa logits. \u68af\u5ea6 \\begin{aligned} & \\nabla_{w} \\mathbb{E}\\left[L\\left(y, y_{w}\\right)\\right] \\\\ =& \\pm \\lim _{\\epsilon \\rightarrow 0} \\frac{1}{\\epsilon} \\mathbb{E}\\left[\\nabla_{w} F\\left(x, y_{\\text {direct }}, w\\right)-\\nabla_{w} F\\left(x, y_{w}, w\\right)\\right] \\end{aligned} \u91cd\u70b9\u96be\u70b9\u5728\u4e8e\u8ba1\u7b97 y_{direct} = \\underset{\\hat y \\in\\mathcal{Y}}{argmax} F(x, \\hat y, w) + \\epsilon L_{AP}(y, \\hat y) \u5176\u6280\u5de7\u5e76\u4e0d\u901a\u7528(\u76ee\u524d\u60c5\u51b5\u6765\u8bf4\u663e\u8457\u5730\u6bd4\u4e0d\u4e0a AP-Loss \u7684\u7b97\u6cd5)\u3002\u4e0d\u5728\u672c\u7ad9\u5c55\u5f00.","title":"Training Deep Neural Networks via Direct Loss Minimization"},{"location":"The_theory/DirectLossMinimization/#direct-policy-gradients-direct-optimization-of-policies-in-discrete-action-spaces","text":"pdf code \u8fd9\u7bc7NIPS2020 paper\u5219\u5c06Direct Loss minimization\u7528\u5230\u4e86policy gradient\u4e0a\uff0c\u5904\u7406\u79bb\u6563\u7684action spaces\u3002\u5de7\u5999\u7684\u5730\u65b9\u5728\u4e8e\u4f7f\u5f97A*\u7684\u526a\u679d-\u52a0\u901f\u641c\u7d22\u7684\u65b9\u6cd5\u5e94\u7528\u5230\u4e86\u5f3a\u5316\u5b66\u4e60\u4e2d\u3002 \u7531\u4e8e\u9700\u8981\u5bf9\u884c\u52a8\u8fdb\u884c\u91c7\u6837\uff0c\u53c8\u8fdb\u4e00\u6b65\u5f15\u5165\u4e86 Gumbel-reparametrization","title":"Direct Policy Gradients: Direct Optimization of Policies in Discrete Action Spaces"},{"location":"The_theory/DirectLossMinimization/#policy-gradient","text":"\\begin{aligned} \\nabla_{\\theta} J\\left(\\pi_{\\theta}\\right) &=\\nabla_{\\theta} \\underset{\\tau \\sim \\pi_{\\theta}}{\\mathrm{E}}[R(\\tau)] \\\\ &=\\nabla_{\\theta} \\int_{\\tau} P(\\tau \\mid \\theta) R(\\tau) \\\\ &=\\int_{\\tau} \\nabla_{\\theta} P(\\tau \\mid \\theta) R(\\tau) \\\\ &=\\int_{\\tau} P(\\tau \\mid \\theta) \\nabla_{\\theta} \\log P(\\tau \\mid \\theta) R(\\tau) \\\\ &=\\underset{\\tau \\sim \\pi_{\\theta}}{\\mathrm{E}}\\left[\\nabla_{\\theta} \\log P(\\tau \\mid \\theta) R(\\tau)\\right] \\\\ &=\\underset{\\tau \\sim \\pi_{\\theta}}{\\mathrm{E}}\\left[\\nabla_{\\theta}(\\log \\rho_0(s_0) + \\sum_{t=0}^T(\\log P(s_{t+1}|s_t, a_t) + \\log \\pi_\\theta(a_t| s_t))) R(\\tau)\\right] \\\\ \\therefore \\nabla_{\\theta} J\\left(\\pi_{\\theta}\\right) &=\\underset{\\tau \\sim \\pi_{\\theta}}{\\mathrm{E}}\\left[\\sum_{t=0}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta}\\left(a_{t} \\mid s_{t}\\right) R(\\tau)\\right] \\end{aligned}","title":"Policy Gradient"},{"location":"The_theory/DirectLossMinimization/#direct-policy-gradient","text":"\u4e0e\u57fa\u7840\u7684policy gradient\u76f4\u63a5\u4ece \\pi_\\theta \u91c7\u6837\u4e0d\u540c\uff0c\u672c\u6587\u7684\u8f93\u51faaction\u5b9a\u4e49\u4e3a \\begin{aligned} a^*(\\epsilon) &= \\underset{a}{argmax}D_\\theta(a; \\Gamma, S, \\epsilon) \\\\ D_\\theta(a; \\Gamma, S, \\epsilon) &= \\log \\Pi_\\theta(a|S) + \\Gamma(a) + \\epsilon R(a, S) \\end{aligned} \u5176\u4e2d \\Gamma \u662fgumbel\u968f\u673a\u91c7\u6837\u503c \u8bbeaction a \u7684\u91c7\u6837\u662f\u57fa\u4e8e\u4e8c\u5143\u51fd\u6570 D_\\theta(S, \\epsilon) \uff0c \u8fdb\u800c\u4fee\u6539\u5f97\u5230\u68af\u5ea6\u7ed3\u679c \\nabla_{\\theta} \\underset{\\tau \\sim \\pi_{\\theta}}{\\mathrm{E}}[R(\\tau)] = \\frac{1}{\\epsilon} \\mathbb{E}_{\\boldsymbol{S} \\sim P, \\Gamma}\\left[\\nabla_{\\theta} \\log \\Pi_{\\theta}\\left(\\boldsymbol{a}^{*}(\\epsilon) \\mid \\boldsymbol{S}\\right)-\\nabla_{\\theta} \\log \\Pi_{\\theta}\\left(\\boldsymbol{a}^{*}(0) \\mid \\boldsymbol{S}\\right)\\right] \u5176\u4e2d a^*(0) \u662f\u4e0d\u8003\u8651\u68af\u5ea6\u7ed3\u679c,\u7f51\u7edc\u6a21\u578b\u539f\u6765\u7684\u8f93\u51fa\u91c7\u6837\u51fa\u6765\u7684\u7ed3\u679c\uff0c\u8fd9\u91cc\u6587\u7ae0\u79f0\u4e4b\u4e3a a_{opt} . a^*(\\epsilon) \u662f\u5e26\u6709reward\u65f6\u63a8\u7406\u7684\u7ed3\u679c\uff0c\u4e0edirect loss minimization \u8303\u5f0f\u5bf9\u7167\uff0c\u8fd9\u91cc\u79f0\u4e4b\u4e3a a_{direct} \u641c\u7d22\u7b97\u6cd5: \u4f5c\u8005\u8fdb\u4e00\u6b65\u8003\u8651\u5230\u53ef\u4ee5\u4f7f\u7528 heuristic function H = G_\\theta(R) + \\epsilon(reward\\_so\\_far + bound\\_reward\\_togo) \u5728\u641c\u7d22\u6700\u4f73\u7684 d_{direct} \u7684\u8fc7\u7a0b\u4e2d\u4f7f\u7528 A*\u4fee\u526a\u6389expected results\u6bd4\u8f83\u4f4e\u7684\u90e8\u5206\uff0c\u52a0\u5feb\u641c\u7d22\u8fdb\u5ea6.","title":"Direct Policy Gradient"},{"location":"The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/","text":"Do Better ImageNet Models Transfer Better? \u8fd9\u7bc7\u8bba\u6587\u8ba4\u771f\u5730\u6d4b\u8bd5\u4e86\u4e0d\u540cImageNet Models\u7684transfer learning\u80fd\u529b\u3002\u521d\u6b65\u5b9e\u9a8c\u65b9\u5f0f\u662f\u572812\u4e2a\u4e0d\u540c\u79cd\u7c7b\u4e0d\u540csize\u7684\u8bad\u7ec3\u96c6\u4e2d\uff0c\u752816\u4e2apretrained \u7f51\u7edc\u5206\u522b\u8fdb\u884cfix-feature-extractor logistic regression\u3001fine-tuning\u3001re-train. \u90fd\u7528grid-search \u5bfb\u627e\u6700\u4f18\u5b66\u4e60\u7387\u7b49\u8d85\u53c2\u6570\u3002 metrics in comparison \u7ecf\u9a8c\u7ed3\u8bba\uff1a 1. ImageNet \u7684\u51c6\u786e\u7387\u80fd\u9884\u6d4blogistic regression\u7684\u51c6\u786e\u7387\uff0c\u4e24\u8005\u6709\u8f83\u5f3a\u7684\u6b63\u76f8\u5173\u6027\uff0c\u4f46\u662fregularization setting\u4f1a\u6709\u5f71\u54cd\u3002 Google\u4f7f\u7528\u76f8\u540c\u7684training setting\u65f6\uff0c\u7ed3\u8bba\u524d\u534a\u53e5\u6210\u7acb\uff0c\u5728imageNet\u8868\u73b0\u6700\u597d\u7684Inception-ResNet v2 and NASNet Large\u7a33\u5750\u524d\u4e24\u540d\u3002\u4f46\u662f\u4ece\u516c\u5f00\u80fd\u83b7\u5f97\u7684checkpoint\u4e2d\u5f00\u59cbtrain\u7684\u8bdd,ResNet\u548cDenseNet\u51e0\u4e4e\u603b\u662f\u6700\u597d\u7684\uff0c\u4e14transfer and basic accuracy\u7684\u76f8\u5173\u6027\u5f88\u5dee\uff0c\u8fdb\u4e00\u6b65\u7814\u7a76\u53d1\u73b0\u8fd9\u4e2a\u662f\u56e0\u4e3aregularization\u3002 \u627e\u5230\u56db\u4e2a\u635f\u5bb3Inception Net transfer accuracy\u7684\u64cd\u4f5c\uff0c\u5206\u522b\u662f \u5728batchnorm\u5c42\u4e0d\u4f7f\u7528scale parameter(\u4e0d\u5b66\u4e60\u65b9\u5dee) label smoothing(\u6e90\u81eainception-v3,\u5728\u8bad\u7ec3\u65f6\u6709\u4e00\u5b9a\u6982\u7387\u4e0d\u91c7\u53d6\u539flabel\uff0c\u800c\u5747\u5300\u968f\u673a\u9009\u62e9\u53e6\u4e00\u4e2aclass\u4f5c\u4e3aground truth) dropout \u989d\u5916\u7684\u5206\u7c7b\u8f93\u51fa(\u5728\u65e9\u671f\u5c42\u63d0\u524d\u8f93\u51fa\u7ed3\u679c) \u8fd9\u4e9b\u65b9\u6cd5\u5bf9top-1\u51c6\u786e\u7387\u63d0\u5347\u4e0d\u52301%\uff0c\u4f46\u662f\u5728transfer\u4e0a\u9020\u6210\u635f\u5bb3\uff0c\u5728\u76f8\u5bf9log\u6bd4\u4f8b\u4e0a\u6765\u8bf4\u76f8\u5f53\u4e8e\u628a\u6700\u597d\u7684backbone\u6362\u6210\u6700\u5f31\u7684backbone\uff0c\u8fd9\u4e2a\u4e0d\u4f46\u80fd\u5728transfer learning\u7684\u7ed3\u679c\u4e0a\u770b\u5230\u5dee\u522b\uff0c\u5728t-SNE\u7684\u53ef\u89c6\u5316\u7ed3\u679c\u4e2d\u4e5f\u53ef\u89c1\u5230\u3002 2. ImageNet\u51c6\u786e\u7387\u80fd\u9884\u6d4bfine-tunning\u7684\u8868\u73b0 regularizaion \u8fd8\u6709training setting\u7684\u6548\u679c\u5728fine-tuning\u7684\u7ed3\u679c\u4e0a\u5f71\u54cd\u4e0d\u5927(\u6709\u4e00\u5b9a\u635f\u4f24\uff0c\u6709\u4e9b\u8fd8\u6ca1\u6709\u635f\u4f24\uff0c\u6839\u636e\u56fe5\uff0c\u53ef\u4ee5\u8bf4\u662f\u6570\u636e\u4e0a\u4e0d\u663e\u8457)\u3002 \u4f46\u662f\u8fd9\u4e2a\u76f8\u5173\u6027\u5728\u4e0d\u540cdataset\u4e0a\u6709\u4e0d\u540c\uff0c\u5728\u5c0f\u7684\u6570\u636e\u96c6\u4e0a\u76f8\u5173\u6027\u6700\u660e\u663e 3. ImageNet\u51c6\u786e\u7387\u80fd\u9884\u6d4b\u968f\u673a\u521d\u59cb\u5316\u65f6\u7684\u8868\u73b0 \u8fd9\u4e2a\u76f8\u5173\u6027\u76f8\u5bf9\u5c0f\u4e00\u4e9b\uff0c\u800c\u4e14\u66f4\u7ec6\u81f4\u5730\u770b\uff0c\u6570\u636e\u96c6\u5927\u5c0f\u8f83\u5c0f\u65f6\u76f8\u5173\u6027\u4e0d\u663e\u8457\uff0c\u4f46\u662f\u5bf9\u4e8e\u5927\u6570\u636e\u96c6\uff0c\u76f8\u5173\u6027\u4f1a\u5f88\u663e\u8457\u3002 4. \u9009\u62e9\u66f4\u597d\u7684backbone\u6a21\u578b\u6027\u80fd\u63d0\u5347\u6bd4\u5f97\u4e0a\u4e3atransfer learning\u4e13\u95e8\u8bbe\u8ba1\u7684\u7279\u6b8a\u65b9\u6cd5 \u7ed3\u679c\u8868\u660epretrained model\u5bf9transfer\u7ed3\u679c\u5f88\u91cd\u8981\uff0c\u672c\u6587\u58f0\u79f0\u5728\u76ee\u524d\u4f7f\u7528\u7684dataset\u4e2d\uff0c\u5b83\u4eec\u901a\u8fc7\u66f4\u6362\u5e76fine-tuning backbone\u8d85\u8fc7\u4e86\u6587\u4e2d\u63d0\u5230\u7684N\u79cdtransfer learning\u7b97\u6cd5\u3002 \u672c\u6587\u6700\u540e\u8868\u793atransfer tricks\u4e0e\u66f4\u6362backbone\u5e76\u4e0d\u77db\u76fe\uff0c\u751a\u81f3\u53ef\u4ee5\u76f8\u4e92\u53e0\u52a0\uff0c\u4f46\u662f\u8fd9\u4e2a\u63d0\u5347\u5e45\u5ea6\u503c\u5f97\u5173\u6ce8 5. \u5bf9\u4e8e\u5206\u7c7b\u66f4\u7ec6\u81f4\u7684\u5982\u8f66\u5206\u7c7b\u4ee5\u53ca\u98de\u884c\u5668\u5206\u7c7b\uff0cImageNet Pretraining\u4e0e\u968f\u673a\u521d\u59cb\u5316\u76f8\u6bd4\u5dee\u8ddd\u4e0d\u5927 \u5176\u4f59\u5927\u90e8\u5206\u6570\u636e\u4e4b\u4e2d\uff0cfine_tune > pretrained > random\u662f\u663e\u8457\u5730 6. ImageNet\u52a0\u901f\u6536\u655b 7. \u5982\u4f55\u9009\u62e9\u662f\u5426pretrained \u7ed3\u8bba\uff1a \u6570\u636e\u6700\u5c11\u662flogistic regression\u53ef\u80fd\u662f\u6700\u597d\u7684(\u901f\u5ea6\u5feb\uff0c\u6027\u80fd\u597d)\uff0c\u5927\u4e00\u4e9b\u7684dataset,fine-tuning\u6700\u597d\uff0c\u66f4\u5927\u7684\u6570\u636e\u96c6\u4e2d\uff0c\u968f\u673a\u521d\u59cb\u5316\u7684\u6700\u7ec8\u6027\u80fd\u4f1a\u903c\u8fd1pretrained","title":"Do Better ImageNet Models Transfer Better?"},{"location":"The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/#do-better-imagenet-models-transfer-better","text":"\u8fd9\u7bc7\u8bba\u6587\u8ba4\u771f\u5730\u6d4b\u8bd5\u4e86\u4e0d\u540cImageNet Models\u7684transfer learning\u80fd\u529b\u3002\u521d\u6b65\u5b9e\u9a8c\u65b9\u5f0f\u662f\u572812\u4e2a\u4e0d\u540c\u79cd\u7c7b\u4e0d\u540csize\u7684\u8bad\u7ec3\u96c6\u4e2d\uff0c\u752816\u4e2apretrained \u7f51\u7edc\u5206\u522b\u8fdb\u884cfix-feature-extractor logistic regression\u3001fine-tuning\u3001re-train. \u90fd\u7528grid-search \u5bfb\u627e\u6700\u4f18\u5b66\u4e60\u7387\u7b49\u8d85\u53c2\u6570\u3002","title":"Do Better ImageNet Models Transfer Better?"},{"location":"The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/#metrics-in-comparison","text":"","title":"metrics in comparison"},{"location":"The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/#_1","text":"","title":"\u7ecf\u9a8c\u7ed3\u8bba\uff1a"},{"location":"The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/#1-imagenet-logistic-regressionregularization-setting","text":"Google\u4f7f\u7528\u76f8\u540c\u7684training setting\u65f6\uff0c\u7ed3\u8bba\u524d\u534a\u53e5\u6210\u7acb\uff0c\u5728imageNet\u8868\u73b0\u6700\u597d\u7684Inception-ResNet v2 and NASNet Large\u7a33\u5750\u524d\u4e24\u540d\u3002\u4f46\u662f\u4ece\u516c\u5f00\u80fd\u83b7\u5f97\u7684checkpoint\u4e2d\u5f00\u59cbtrain\u7684\u8bdd,ResNet\u548cDenseNet\u51e0\u4e4e\u603b\u662f\u6700\u597d\u7684\uff0c\u4e14transfer and basic accuracy\u7684\u76f8\u5173\u6027\u5f88\u5dee\uff0c\u8fdb\u4e00\u6b65\u7814\u7a76\u53d1\u73b0\u8fd9\u4e2a\u662f\u56e0\u4e3aregularization\u3002 \u627e\u5230\u56db\u4e2a\u635f\u5bb3Inception Net transfer accuracy\u7684\u64cd\u4f5c\uff0c\u5206\u522b\u662f \u5728batchnorm\u5c42\u4e0d\u4f7f\u7528scale parameter(\u4e0d\u5b66\u4e60\u65b9\u5dee) label smoothing(\u6e90\u81eainception-v3,\u5728\u8bad\u7ec3\u65f6\u6709\u4e00\u5b9a\u6982\u7387\u4e0d\u91c7\u53d6\u539flabel\uff0c\u800c\u5747\u5300\u968f\u673a\u9009\u62e9\u53e6\u4e00\u4e2aclass\u4f5c\u4e3aground truth) dropout \u989d\u5916\u7684\u5206\u7c7b\u8f93\u51fa(\u5728\u65e9\u671f\u5c42\u63d0\u524d\u8f93\u51fa\u7ed3\u679c) \u8fd9\u4e9b\u65b9\u6cd5\u5bf9top-1\u51c6\u786e\u7387\u63d0\u5347\u4e0d\u52301%\uff0c\u4f46\u662f\u5728transfer\u4e0a\u9020\u6210\u635f\u5bb3\uff0c\u5728\u76f8\u5bf9log\u6bd4\u4f8b\u4e0a\u6765\u8bf4\u76f8\u5f53\u4e8e\u628a\u6700\u597d\u7684backbone\u6362\u6210\u6700\u5f31\u7684backbone\uff0c\u8fd9\u4e2a\u4e0d\u4f46\u80fd\u5728transfer learning\u7684\u7ed3\u679c\u4e0a\u770b\u5230\u5dee\u522b\uff0c\u5728t-SNE\u7684\u53ef\u89c6\u5316\u7ed3\u679c\u4e2d\u4e5f\u53ef\u89c1\u5230\u3002","title":"1. ImageNet \u7684\u51c6\u786e\u7387\u80fd\u9884\u6d4blogistic regression\u7684\u51c6\u786e\u7387\uff0c\u4e24\u8005\u6709\u8f83\u5f3a\u7684\u6b63\u76f8\u5173\u6027\uff0c\u4f46\u662fregularization setting\u4f1a\u6709\u5f71\u54cd\u3002"},{"location":"The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/#2-imagenetfine-tunning","text":"regularizaion \u8fd8\u6709training setting\u7684\u6548\u679c\u5728fine-tuning\u7684\u7ed3\u679c\u4e0a\u5f71\u54cd\u4e0d\u5927(\u6709\u4e00\u5b9a\u635f\u4f24\uff0c\u6709\u4e9b\u8fd8\u6ca1\u6709\u635f\u4f24\uff0c\u6839\u636e\u56fe5\uff0c\u53ef\u4ee5\u8bf4\u662f\u6570\u636e\u4e0a\u4e0d\u663e\u8457)\u3002 \u4f46\u662f\u8fd9\u4e2a\u76f8\u5173\u6027\u5728\u4e0d\u540cdataset\u4e0a\u6709\u4e0d\u540c\uff0c\u5728\u5c0f\u7684\u6570\u636e\u96c6\u4e0a\u76f8\u5173\u6027\u6700\u660e\u663e","title":"2. ImageNet\u51c6\u786e\u7387\u80fd\u9884\u6d4bfine-tunning\u7684\u8868\u73b0"},{"location":"The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/#3-imagenet","text":"\u8fd9\u4e2a\u76f8\u5173\u6027\u76f8\u5bf9\u5c0f\u4e00\u4e9b\uff0c\u800c\u4e14\u66f4\u7ec6\u81f4\u5730\u770b\uff0c\u6570\u636e\u96c6\u5927\u5c0f\u8f83\u5c0f\u65f6\u76f8\u5173\u6027\u4e0d\u663e\u8457\uff0c\u4f46\u662f\u5bf9\u4e8e\u5927\u6570\u636e\u96c6\uff0c\u76f8\u5173\u6027\u4f1a\u5f88\u663e\u8457\u3002","title":"3. ImageNet\u51c6\u786e\u7387\u80fd\u9884\u6d4b\u968f\u673a\u521d\u59cb\u5316\u65f6\u7684\u8868\u73b0"},{"location":"The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/#4-backbonetransfer-learning","text":"\u7ed3\u679c\u8868\u660epretrained model\u5bf9transfer\u7ed3\u679c\u5f88\u91cd\u8981\uff0c\u672c\u6587\u58f0\u79f0\u5728\u76ee\u524d\u4f7f\u7528\u7684dataset\u4e2d\uff0c\u5b83\u4eec\u901a\u8fc7\u66f4\u6362\u5e76fine-tuning backbone\u8d85\u8fc7\u4e86\u6587\u4e2d\u63d0\u5230\u7684N\u79cdtransfer learning\u7b97\u6cd5\u3002 \u672c\u6587\u6700\u540e\u8868\u793atransfer tricks\u4e0e\u66f4\u6362backbone\u5e76\u4e0d\u77db\u76fe\uff0c\u751a\u81f3\u53ef\u4ee5\u76f8\u4e92\u53e0\u52a0\uff0c\u4f46\u662f\u8fd9\u4e2a\u63d0\u5347\u5e45\u5ea6\u503c\u5f97\u5173\u6ce8","title":"4. \u9009\u62e9\u66f4\u597d\u7684backbone\u6a21\u578b\u6027\u80fd\u63d0\u5347\u6bd4\u5f97\u4e0a\u4e3atransfer learning\u4e13\u95e8\u8bbe\u8ba1\u7684\u7279\u6b8a\u65b9\u6cd5"},{"location":"The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/#5-imagenet-pretraining","text":"\u5176\u4f59\u5927\u90e8\u5206\u6570\u636e\u4e4b\u4e2d\uff0cfine_tune > pretrained > random\u662f\u663e\u8457\u5730","title":"5. \u5bf9\u4e8e\u5206\u7c7b\u66f4\u7ec6\u81f4\u7684\u5982\u8f66\u5206\u7c7b\u4ee5\u53ca\u98de\u884c\u5668\u5206\u7c7b\uff0cImageNet Pretraining\u4e0e\u968f\u673a\u521d\u59cb\u5316\u76f8\u6bd4\u5dee\u8ddd\u4e0d\u5927"},{"location":"The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/#6-imagenet","text":"","title":"6. ImageNet\u52a0\u901f\u6536\u655b"},{"location":"The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/#7-pretrained","text":"\u7ed3\u8bba\uff1a \u6570\u636e\u6700\u5c11\u662flogistic regression\u53ef\u80fd\u662f\u6700\u597d\u7684(\u901f\u5ea6\u5feb\uff0c\u6027\u80fd\u597d)\uff0c\u5927\u4e00\u4e9b\u7684dataset,fine-tuning\u6700\u597d\uff0c\u66f4\u5927\u7684\u6570\u636e\u96c6\u4e2d\uff0c\u968f\u673a\u521d\u59cb\u5316\u7684\u6700\u7ec8\u6027\u80fd\u4f1a\u903c\u8fd1pretrained","title":"7. \u5982\u4f55\u9009\u62e9\u662f\u5426pretrained"},{"location":"The_theory/First_order_methods_review_and_updates/","text":"Firt Order Optimizers: Reviews and Updates \u8fd9\u7bc7\u5185\u5bb9\u4f1a\u4e13\u6ce8\u4e8e\u4e00\u9636\u4f18\u5316\u5668\u7684\u76f4\u89c9\u4e0e\u7b97\u6cd5\u4ecb\u7ecd\uff0c\u5e76\u4ece\u51f8\u4f18\u5316\u5f15\u5165\u76f8\u5173\u7684\u57fa\u7840\u6982\u5ff5 Firt Order Optimizers: Reviews and Updates \u8bba\u6587\u4e2d\u7ecf\u5e38\u51fa\u73b0\u7684\u57fa\u7840\u6982\u5ff5: Gradient-Descent and Basic Variants \u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5 Proximal Gradient Method Projected Gradient Descent Nesterov Accelerated Gradient Descent Triple Momentum Gradient Descent \u4e2a\u4eba\u5206\u6790 Boosting First-Order Methods by Shifting Objective: New Schemes with Faster Worst Case Rates Generalized Triple Momentum \u8bba\u6587\u4e2d\u7ecf\u5e38\u51fa\u73b0\u7684\u57fa\u7840\u6982\u5ff5: Lipschitz Continuity & L-Smooth \u6307: (\\nabla f(x) - \\nabla f(y))^T \\le L ||x-y|| \u76f4\u89c9\u6765\u8bf4\u662f\u4e8c\u9636\u5bfc\u6709\u4e0a\u9650 L \\mu -strongly convex \u6307: (\\nabla f(x) - \\nabla f(y))^T (x-y) \\ge m ||x-y||^2 \u76f4\u89c9\u6765\u8bf4\u662f\u4e8c\u9636\u5bfc\u6709\u6b63\u4e0b\u9650 m . \u5728\u51f8\u4f18\u5316\u95ee\u9898\u4e2d\uff0c\u8fde\u7eed\u51fd\u6570\u4e8c\u9636\u5bfc\u4e3a\u975e\u8d1f\u6570\uff0c\u8fd9\u91cc\u5f3a\u51f8\u51fd\u6570\u7684\u5b9a\u4e49\u5219\u662f\u8981\u6c42\u4e8c\u9636\u5bfc\u4e3a\u6b63\u6570. \u6587\u7ae0\u4e2d\u4f1a\u51fa\u73b0 \"L-smooth and \\mu -strongly convex\"\u7684\u8bf4\u6cd5\uff0c\u76f4\u89c9\u5730\u6765\u8bf4\u6307\u51fd\u6570\u7684\u4e8c\u9636\u5bfc\u5728\u6b63\u6570\u533a\u95f4 [\\mu, L] . co-coercivity \u77eb\u987d\u6027\u53ef\u4ee5\u7531convexity \u4e0e L-smooth\u4e00\u540c\u63a8\u5bfc, \u5176\u6570\u5b66\u8868\u8ff0\u4e3a \\frac{1}{2L} ||\\nabla f(x) - \\nabla f(y)||^2 \\le f(x) - f(y) - \\langle\\nabla(y), x-y\\rangle Gradient-Descent and Basic Variants \u5173\u4e8e\u68af\u5ea6\u4e0b\u964d\u76f8\u5173\u7b97\u6cd5\uff0c\u6709\u4e00\u4e2a\u6bd4\u8f83\u597d\u7684 \u535a\u5ba2 \u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5 x_{k+1} = x_{k} - \\alpha \\nabla f(x_{k}) \u4ecesurrogate function\u7684\u89d2\u5ea6\u6765\u89e3\u6790\uff0c\u5728\u6bcf\u4e00\u6b65 x_k \u4e0a\uff0c\u53ef\u4ee5\u7406\u89e3\u6784\u9020\u4e00\u4e2a\u4e0e\u539f\u51fd\u6570\u5728 x=x_k \u76f8\u5207\u4e14\u4e8c\u9636\u5bfc\u4e3a 1/\\alpha \u7684\u4e8c\u6b21\u51fd\u6570 y = \\frac{1}{2\\alpha}(x-x_k)^2 + \\nabla f(x_k)(x-x_k) + f(x_k) , \u6bcf\u4e00\u6b65 x \u4f1a\u66f4\u65b0\u5230\u8fd9\u4e2a\u4e8c\u6b21\u4ee3\u7406\u51fd\u6570\u7684\u6700\u5c0f\u503c\u4e0a. \u8fd9\u8981\u6c42\u6574\u4e2a\u51fd\u6570\u53ef\u5bfc\uff0c\u5982\u679c\u4e0d\u53ef\u5bfc\uff0c\u5219\u9700\u8981\u4f7f\u7528 subgradient descent. subgradient \u51fd\u6570 g \u5b9a\u4e49\u4e3a\u4efb\u610f\u68af\u5ea6\u51fd\u6570(subgradient oracle)\uff0c\u4f7f\u5f97 f(y) \\ge f(x) + \\langle g(x), y-x\\rangle Proximal Gradient Method \u5982\u679c\u51f8\u51fd\u6570\u5176\u4e2d\u6709\u4e00\u4e2a\u90e8\u5206\u4e0d\u53ef\u5bfc\uff0c\u4f46\u662f\u5904\u7406\u6bd4\u8f83\u7b80\u5355\uff0c\u90a3\u4e48\u53ef\u4ee5\u4f7f\u7528proximal optimization algorithms, \u5c06\u51fd\u6570\u5206\u89e3\u4e3a f(x) = g(x) + h(x) , \u5176\u4e2d h(x) \u4e0d\u4e00\u5b9a\u53ef\u5bfc. \u53ef\u4ee5\u7406\u89e3\u4e3a\u5206\u4e24\u6b65: \u5c06 g(x) \u7528\u5f53\u524d\u4f4d\u7f6e\u7684 x_k \u7684\u51fd\u6570\u503c\u6784\u9020\u4e00\u4e2a\u4e0e\u539f\u51fd\u6570\u76f8\u5207(\u66f2\u7387\u4e0d\u5fc5\u4e00\u81f4\u7684)\u7684\u51fd\u6570 g(x_k) + \\nabla g(x_k)^T(u-x_k) + \\frac{\\mu}{2} ||u-x_k||_2^2 \uff0c \u628a h(x) \u76f4\u63a5\u4e0e\u65b0\u4e8c\u6b21\u4ee3\u7406\u51fd\u6570\u7684\u6c42\u548c\u6c42global minimum. x^+ = \\underset{u}{\\text{argmin}}(h(u) + g(x_k) + \\nabla g(x_k)^T (u-x_k) + \\frac{\\mu}{2} ||u-x_k||_2^2) = \\underset{u}{\\text{argmin}}(h(u) + \\nabla g(x_k)^Tu + \\frac{\\mu}{2}||u-x_k||^2_2) \u91cd\u70b9: \u5982\u679c h(x) \u4e3aL2-norm L1-norm, \u8fd9\u4e9b\u5e38\u89c1\u7684regularizer, \u90a3\u4e48\u6700\u540e\u7684\u6c42\u4f18\u5316\u662f\u4e00\u4e2a\u5178\u578b\u7684\u4e8c\u6b21\u51fd\u6570 + \u4e8c\u6b21\u51fd\u6570\u6216\u8005 \u4e8c\u6b21\u51fd\u6570 + l_1 ,\u90fd\u662f\u6709closed-form solution\u7684\u3002 \u5982\u679c\u8bb0 x_k^* \u4e3a\u4e8c\u6b21\u4ee3\u7406\u51fd\u6570\u7684\u6781\u503c\u70b9 x_k - \\frac{\\nabla g(x_k)}{\\mu} , \u5c06 \\nabla g(x_k) \u4ee3\u5165\u4e0a\u6587\u7684\u6700\u4f18\u5316\u516c\u5f0f\uff0c \u90a3\u4e48\u6700\u540e\u7684\u6c42\u89e3\u51fd\u6570\u5e38\u5e38\u4f1a\u5199\u6210 proximal mapping (prox-operator), \\text{prox}_h^\\mu(x_k^*) = \\underset{u}{\\text{argmin}}(h(u) + \\frac{\\mu}{2}||u-x_k^*||_2^2) .\u8bf4\u660e\u6700\u7ec8\u7684\u7ed3\u679c\u65e2\u9700\u8981\u8003\u8651 g \u7684\u5c40\u90e8\u6700\u4f18\u70b9\uff0c\u4e5f\u9700\u8981\u8003\u8651 h(x) . \u7279\u6b8a\u60c5\u51b5\u4e2d\uff0c h \u53ef\u4ee5\u662f\u4e00\u4e2a indicator function,\u6307\u5b9a\u51fd\u6570\u7684\u5b9a\u4e49\u57df, \u5982\u679c x \u5728\u5b9a\u4e49\u57df C \u5185\u5219 h(x) = 0 ,\u5426\u5219 h(x) = \\infty . \u8fd9\u6837\u7684\u60c5\u51b5\u4e0b prox_h \u51fd\u6570\u53d8\u6210\u4e86\u5728\u6709\u9650\u5b9a\u4e49\u57df\u5185\u5bfb\u627e\u6700\u4f18\u89e3\u7684\u95ee\u9898\uff0c\u5f97\u5230\u7684\u7ed3\u679c\u662f\u6700\u4f18\u89e3\u5728\u8fb9\u754c\u4e0a\u7684\u6295\u5f71\u3002\u56e0\u800c wiki \u8bf4 prox\u51fd\u6570\u662fprojection\u64cd\u4f5c\u7684\u63a8\u5e7f\u3002 Projected Gradient Descent \u5982\u679c\u4f7f\u7528\u68af\u5ea6\u4e0b\u964d\u7684\u65f6\u5019\u8981\u6c42 x \u5728\u5b9a\u4e49\u57df C \u4e2d\uff0c\u90a3\u4e48\u4e00\u4e2aprojected\u68af\u5ea6\u4e0b\u964d\u7684\u505a\u6cd5\u662f\uff0c\u9996\u5148\u8fdb\u884c\u65e0\u7ea6\u675f\u68af\u5ea6\u4e0b\u964d\uff0c\u7136\u540e\u5c06\u7ed3\u679c\u6295\u5f71\u5230\u8fb9\u754c\u4e0a. \u8bb0\u4e3a x_{k+1} = \\pi_C(x_k - \\alpha_k\\nabla f(x_k)) , \u5176\u4e2d \\pi_C \u4e3a\u6b27\u51e0\u91cc\u5f97\u6295\u5f71\uff0c\u53ef\u4ee5\u5199\u6210 x_{k+1} = \\underset{x\\in C}{\\text{argmin}}||x_k - \\alpha_k \\nabla f(x_k) - x||_2^2 \u5c55\u5f00\u540e\u53d8\u4e3a x_{k+1} = \\underset{x\\in C}{\\text{argmin}}\\{\\langle x,\\nabla f(x_k) \\rangle + \\frac{1}{\\alpha_k}\\frac{||x-x_k||^2_2}{2} \\} General Projected Gradient Descent \u5219\u4f1a\u5141\u8bb8\u4e0d\u540c\u7684\u8ddd\u79bb\u91cf\u53d8\u4e3a: x_{k+1}=\\arg \\min _{x \\in C}\\left\\{\\left\\langle x, \\nabla f\\left(x_{k}\\right)\\right\\rangle+\\frac{1}{\\alpha_{k}} d\\left(x, x_{k}\\right)\\right\\} \u5e38\u7528\u7684\u8ddd\u79bb\u91cf Bregman Divergence D_f(x, y) = f(x) - f(y) - \\langle \\nabla f(y), x-y\\rangle \u7531 f(x) = \\sum x\\log x \u53ef\u4ee5\u4ea7\u51fa KL divergence. mirror gradient descent\u5e38\u5e38\u7528\u6765\u5206\u6790 Nesterov Accelerated Gradient Descent Nesterov Acceleration\u6709\u5f88\u591a\u4e2a\u4e0d\u540c\u7684\u7248\u672c\u3002 Nesterov \u5728 1983\u5e74\u7684paper: \\begin{aligned} y_t = x_t + \\frac{t}{t+3}(x^{t} - x^{t-1}) \\\\ x_{t+1} = y_t - \\eta_t \\nabla f(y^t) \\\\ \\end{aligned} \u5176\u4e2d \\frac{t}{t+3} \u53ef\u88ab\u6cdb\u5316\u4e3a\u4e00\u822c\u8d85\u53c2\u6570 \\beta . \u53ef\u4ee5\u7406\u89e3\u4e3a y_t \u4e3a\u6cbf\u7740\u4e4b\u524d\u7684\u66f4\u65b0\u65b9\u5411look-ahead\u7684\u76ee\u7684\u4f4d\u7f6e, \u7136\u540e\u5728look-ahead\u7684\u4f4d\u7f6e\u8fdb\u884c\u68af\u5ea6\u66f4\u65b0. \u8fd9\u7bc7Medium\u535a\u5ba2 \u63d0\u4f9b\u4e86\u57fa\u4e8e\"look-ahead\"\u7684\u9610\u8ff0. pytorch\u7684\u7248\u672c \u91c7\u7528\u7684\u662f\u66f4\u7740\u91cd\u4e8emomentum\u7684\u7b97\u6cd5\u63cf\u8ff0\uff0c\u5bf9\u6807\u7684\u662fmomentum gradient descent. \\begin{aligned} v_{t+1} &= \\mu \\cdot v_t + g_{t+1} \\\\ p_{t+1} &= p_t - lr \\cdot v_{t+1} \\end{aligned} Triple Momentum Gradient Descent pdf \u8fd9\u7bc7paper\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u56db\u4e2a\u53c2\u6570\u7684 triple momentum (TM)\u6846\u67b6. \u8fd9\u4e2a\u6846\u67b6\u4e0b\u7684\u4f18\u5316\u5668\u7b97\u6cd5: \\begin{aligned} \\xi_{k+1} &= (1+\\beta) \\xi_k - \\beta \\xi_{k-1} - \\alpha \\nabla f(y_k)\\\\ y_k &= (1 + \\gamma)\\xi_k - \\gamma\\xi_{k-1}\\\\ x_k &= (1 + \\delta)\\xi_k - \\delta\\xi_{k-1} \\end{aligned} | Method | Parameters | | :---------------- | :-------------------------------: | | Gradient descent | (\\alpha, 0, 0, 0) | | Momentum Gradient | (\\alpha, \\beta, 0, 0) | | Nesterov AG | (\\alpha, \\beta, \\beta, 0) | | General TM Algo. | (\\alpha, \\beta, \\gamma, \\delta) | \u76f4\u89c9\u4e0a\u6765\u8bf4\uff0c\u5982\u679c \\delta=0 , \u5219 x_k = \\xi_k , \u4ee3\u5165\u540e\u5c31\u80fd\u5f97\u5230\u524d\u9762\u63d0\u5230\u7684\u51e0\u4e2a\u7b97\u6cd5\u516c\u53f8. \u4f5c\u8005\u7ed9\u51fa\u4e86\u53c2\u6570, \u5982\u679c\u4e00\u76f4\u9700\u8981\u4f18\u5316\u7684\u51fd\u6570\u662f L-\\text{smooth}, \\mu-\\text{strongly convex} \u7684\u5219\u4ee4 \\kappa = L / \\mu, \\rho = 1 - 1 / \\sqrt{\\kappa} , \u8bbe\u5b9a\u4e00\u4e0b\u53c2\u6570\uff0c (\\alpha, \\beta, \\gamma, \\delta) = (\\frac{1+\\rho}{L}, \\frac{\\rho^2}{2-\\rho}, \\frac{\\rho^2}{(1+\\rho)(2-\\rho)}, \\frac{\\rho^2}{1 - \\rho^2}) \u4e2a\u4eba\u5206\u6790 \u8fd9\u4e2apaper\u662f\u6709\u5c0f\u95ee\u9898\u7684\uff0c\u5bf9\u524d\u9762TM\u6846\u67b6\u8fdb\u884c\u5206\u6790\uff0c\u53ef\u4ee5\u901a\u8fc7\u7528 x, y \u8868\u8fbe (\\xi_k - \\xi_{k-1}) \u53ef\u4ee5\u6d88\u53bb \\xi .\u540c\u65f6 \\gamma, \\delta \u4e2d\u6709\u4e00\u4e2a\u53c2\u6570\u662f\u591a\u4f59\u7684.\u8fd9\u4e2a\u516c\u5f0f\u6700\u7ec8\u53ef\u4ee5\u5316\u4e3a x_{k+1} - y_{k+1} = \\beta(x_k - y_k) - \\alpha (\\delta - \\gamma) \\nabla f(y_k) \u8fd9\u91cc\u5f15\u51fa\u4e86\u4e0b\u4e00\u7bc7paper\u63d0\u5230\u7684\u53c2\u6570\u5197\u4f59\u95ee\u9898\u3002 Boosting First-Order Methods by Shifting Objective: New Schemes with Faster Worst Case Rates pdf \u5bf9\u4e8e\u539f\u6765\u7684\u76ee\u6807\u51fd\u6570\uff0c\u53ef\u4ee5\u8f6c\u800c\u6c42\u89e3\u4e00\u4e2a\u65b0\u7684shifted objective. \\underset{x\\in\\mathcal{R}^d}{\\text{min}}h(x) = \\frac{1}{n}\\sum^n_{i=1}h_i(x), \\quad \\text{where}\\quad \\ h_i(x) = f_i(x) - f_i(x^*) - \\left\\langle\\nabla f_{i}\\left(x^{\\star}\\right), x-x^{\\star}\\right\\rangle - \\frac{\\mu}{2}||x - x^*||^2 \u53ef\u4ee5\u7b80\u5355\u7406\u89e3\u4e3a\u539f\u51fd\u6570\u4e0e\u6700\u503c\u70b9\u9644\u8fd1\u7684\u4e0b\u754c\u4e8c\u6b21\u51fd\u6570\u7684\u6b8b\u5dee.\u5bb9\u6613\u8bc1\u660e h(x)\u662f\u4e00\u4e2a\u51f8\u51fd\u6570 h(x)\u7684\u6781\u503c\u70b9\u4e0e \\sum f(x) \u76f8\u540c\uff0c\u8fdb\u800c\u53ef\u4ee5\u8bf4 h(x) \u7684\u6700\u4f18\u503c\u70b9\u4e0e f(x) \u7684\u76f8\u540c\u3002 h(x) (L-\\mu) -smooth \u77eb\u987d\u6027: \\forall x, y \\in \\mathbb{R}^{d}, h(x)-h(y)-\\langle\\nabla h(y), x-y\\rangle \\geq \\frac{1}{2(L-\\mu)}\\|\\nabla h(x)-\\nabla h(y)\\|^{2} Generalized Triple Momentum \u5176\u4e2d z_{k+1} \u7684\u8868\u8fbe\u5f0f\u7b49\u4ef7\u4e8e z_{k+1} = \\text{argmin}_x\\left\\{ (\\mu/2) ||x - y_k^*||_2^2 + (\\alpha_k / 2)||x - z_k||_2^2\\right\\} \u5176\u4e2d y_k^* \u4e3a\u4e8c\u6b21surrogate function g(y_k) + \\nabla g(y_k)^T(u-y_k) + \\frac{\\mu}{2} ||u-y_k||_2^2 \u7684\u6700\u4f18\u89e3 y_k-\\frac{\\nabla f(y_k)}{\\mu} . \u53ef\u5f97 z_{k+1} = \\frac{\\mu y_k^* + \\alpha z_k}{\\mu + \\alpha} = z_k - \\frac{\\mu}{\\mu+\\alpha}(y_k^* - z_k) , \u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u53ef\u4ee5\u8868\u8fbe\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5. \u800c\u5f53 \\tau_k^z = 0 \u65f6\uff0c\u53ef\u4ee5\u7b80\u5355\u5730\u5c06\u8fd9\u4e24\u6761\u5f0f\u5b50\u5316\u4e3a Nesterov\u7684\u6807\u51c6\u5f62\u5f0f\u3002 \u8fdb\u4e00\u6b65\u5730\u89c2\u5bdf y_k \u7684\u8868\u8fbe\u5f0f\u7684\u7b2c\u4e8c\u9879,\u5c06 z_{k} \u4ee3\u5165,\u5f97\u5230 \\frac{2\\mu + \\alpha}{\\mu + \\alpha}[\\mu(y_{k-1} - z_{k-1}) - \\nabla f(y_{k-1})] , \u7531\u4e8e\u5f3a\u51f8\u6027, \u7b2c\u4e8c\u9879\u4e0e y_{k-1} - z_{k-1} \u7b26\u53f7\u76f8\u53cd,\u8fd9\u662f\u7528\u4e8e\u8865\u507fstrongly-convex\u51fd\u6570\u7684\u4e8c\u9636\u5bfc\u7684,\u5f53\u6211\u4eec\u7528\u66f2\u7387\u4e3a \\mu \u7684\u66f4\u4e3a\u6241\u5e73\u7684\u4e8c\u6b21\u4ee3\u7406\u51fd\u6570\u6c42\u89e3\u65f6\uff0c\u6211\u4eec\u4f1a\u9ad8\u4f30\u5408\u9002\u7684\u68af\u5ea6\u4e0b\u964d\u6b65\u957f\u7684\u5927\u5c0f\uff0c\u56e0\u800c\u5e26\u7740\u52a8\u91cf\u5f80\u524d\u7684 y_k \u5728\u9884\u6d4blook-ahead \u65f6\u4f1a\u51cf\u53bb\u4e4b\u524d\u9ad8\u4f30\u7684\u4e00\u90e8\u5206\u6b65\u957f. \u53c2\u8003\u53c2\u6570 \\alpha=\\sqrt{L\\mu} - \\mu, \\tau_x=\\frac{2\\sqrt{\\kappa}-1}{\\kappa}, \\tau_z = \\frac{\\sqrt{\\kappa} - 1}{L(\\sqrt{\\kappa} - 1)} \u6587\u7ae0\u66f4\u4e25\u8c28\u4e14\u901a\u7528\u5730\u7528\u674e\u96c5\u666e\u8bfa\u592b\u51fd\u6570 T_{k}=h\\left(y_{k-1}\\right)-\\frac{1}{2(L-\\mu)}\\left\\|\\nabla h\\left(y_{k-1}\\right)\\right\\|^{2}+\\frac{\\lambda}{2}\\left\\|z_{k}-x^{\\star}\\right\\|^{2}, where \\lambda>0 \u8fdb\u884c\u4e86\u8bc1\u660e\uff0c \u8bf4\u660e\u5176\u6536\u655b\u901f\u5ea6\u4e0a\u9650\u6bd4\u8f83\u5c0f\u3002 \u4f5c\u8005\u4e4b\u540e\u5c06\u8fd9\u4e2a\u7b97\u6cd5\u9644\u52a0\u5728\u591a\u4e2a\u4e00\u9636\u7b97\u6cd5\u4e0a(SVRG, SAGA)\u3002\u7531\u4e8e\u4e2a\u4eba\u8ba4\u77e5\u6709\u9650\uff0c\u6682\u8fd8\u6ca1\u6709\u80fd\u529b\u5206\u6790\u8fd9\u4e9b\u7b97\u6cd5\u53ca\u5176\u5e94\u7528.","title":"Firt Order Optimizers: Reviews and Updates"},{"location":"The_theory/First_order_methods_review_and_updates/#firt-order-optimizers-reviews-and-updates","text":"\u8fd9\u7bc7\u5185\u5bb9\u4f1a\u4e13\u6ce8\u4e8e\u4e00\u9636\u4f18\u5316\u5668\u7684\u76f4\u89c9\u4e0e\u7b97\u6cd5\u4ecb\u7ecd\uff0c\u5e76\u4ece\u51f8\u4f18\u5316\u5f15\u5165\u76f8\u5173\u7684\u57fa\u7840\u6982\u5ff5 Firt Order Optimizers: Reviews and Updates \u8bba\u6587\u4e2d\u7ecf\u5e38\u51fa\u73b0\u7684\u57fa\u7840\u6982\u5ff5: Gradient-Descent and Basic Variants \u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5 Proximal Gradient Method Projected Gradient Descent Nesterov Accelerated Gradient Descent Triple Momentum Gradient Descent \u4e2a\u4eba\u5206\u6790 Boosting First-Order Methods by Shifting Objective: New Schemes with Faster Worst Case Rates Generalized Triple Momentum","title":"Firt Order Optimizers: Reviews and Updates"},{"location":"The_theory/First_order_methods_review_and_updates/#_1","text":"Lipschitz Continuity & L-Smooth \u6307: (\\nabla f(x) - \\nabla f(y))^T \\le L ||x-y|| \u76f4\u89c9\u6765\u8bf4\u662f\u4e8c\u9636\u5bfc\u6709\u4e0a\u9650 L \\mu -strongly convex \u6307: (\\nabla f(x) - \\nabla f(y))^T (x-y) \\ge m ||x-y||^2 \u76f4\u89c9\u6765\u8bf4\u662f\u4e8c\u9636\u5bfc\u6709\u6b63\u4e0b\u9650 m . \u5728\u51f8\u4f18\u5316\u95ee\u9898\u4e2d\uff0c\u8fde\u7eed\u51fd\u6570\u4e8c\u9636\u5bfc\u4e3a\u975e\u8d1f\u6570\uff0c\u8fd9\u91cc\u5f3a\u51f8\u51fd\u6570\u7684\u5b9a\u4e49\u5219\u662f\u8981\u6c42\u4e8c\u9636\u5bfc\u4e3a\u6b63\u6570. \u6587\u7ae0\u4e2d\u4f1a\u51fa\u73b0 \"L-smooth and \\mu -strongly convex\"\u7684\u8bf4\u6cd5\uff0c\u76f4\u89c9\u5730\u6765\u8bf4\u6307\u51fd\u6570\u7684\u4e8c\u9636\u5bfc\u5728\u6b63\u6570\u533a\u95f4 [\\mu, L] . co-coercivity \u77eb\u987d\u6027\u53ef\u4ee5\u7531convexity \u4e0e L-smooth\u4e00\u540c\u63a8\u5bfc, \u5176\u6570\u5b66\u8868\u8ff0\u4e3a \\frac{1}{2L} ||\\nabla f(x) - \\nabla f(y)||^2 \\le f(x) - f(y) - \\langle\\nabla(y), x-y\\rangle","title":"\u8bba\u6587\u4e2d\u7ecf\u5e38\u51fa\u73b0\u7684\u57fa\u7840\u6982\u5ff5:"},{"location":"The_theory/First_order_methods_review_and_updates/#gradient-descent-and-basic-variants","text":"\u5173\u4e8e\u68af\u5ea6\u4e0b\u964d\u76f8\u5173\u7b97\u6cd5\uff0c\u6709\u4e00\u4e2a\u6bd4\u8f83\u597d\u7684 \u535a\u5ba2","title":"Gradient-Descent and Basic Variants"},{"location":"The_theory/First_order_methods_review_and_updates/#_2","text":"x_{k+1} = x_{k} - \\alpha \\nabla f(x_{k}) \u4ecesurrogate function\u7684\u89d2\u5ea6\u6765\u89e3\u6790\uff0c\u5728\u6bcf\u4e00\u6b65 x_k \u4e0a\uff0c\u53ef\u4ee5\u7406\u89e3\u6784\u9020\u4e00\u4e2a\u4e0e\u539f\u51fd\u6570\u5728 x=x_k \u76f8\u5207\u4e14\u4e8c\u9636\u5bfc\u4e3a 1/\\alpha \u7684\u4e8c\u6b21\u51fd\u6570 y = \\frac{1}{2\\alpha}(x-x_k)^2 + \\nabla f(x_k)(x-x_k) + f(x_k) , \u6bcf\u4e00\u6b65 x \u4f1a\u66f4\u65b0\u5230\u8fd9\u4e2a\u4e8c\u6b21\u4ee3\u7406\u51fd\u6570\u7684\u6700\u5c0f\u503c\u4e0a. \u8fd9\u8981\u6c42\u6574\u4e2a\u51fd\u6570\u53ef\u5bfc\uff0c\u5982\u679c\u4e0d\u53ef\u5bfc\uff0c\u5219\u9700\u8981\u4f7f\u7528 subgradient descent. subgradient \u51fd\u6570 g \u5b9a\u4e49\u4e3a\u4efb\u610f\u68af\u5ea6\u51fd\u6570(subgradient oracle)\uff0c\u4f7f\u5f97 f(y) \\ge f(x) + \\langle g(x), y-x\\rangle","title":"\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5"},{"location":"The_theory/First_order_methods_review_and_updates/#proximal-gradient-method","text":"\u5982\u679c\u51f8\u51fd\u6570\u5176\u4e2d\u6709\u4e00\u4e2a\u90e8\u5206\u4e0d\u53ef\u5bfc\uff0c\u4f46\u662f\u5904\u7406\u6bd4\u8f83\u7b80\u5355\uff0c\u90a3\u4e48\u53ef\u4ee5\u4f7f\u7528proximal optimization algorithms, \u5c06\u51fd\u6570\u5206\u89e3\u4e3a f(x) = g(x) + h(x) , \u5176\u4e2d h(x) \u4e0d\u4e00\u5b9a\u53ef\u5bfc. \u53ef\u4ee5\u7406\u89e3\u4e3a\u5206\u4e24\u6b65: \u5c06 g(x) \u7528\u5f53\u524d\u4f4d\u7f6e\u7684 x_k \u7684\u51fd\u6570\u503c\u6784\u9020\u4e00\u4e2a\u4e0e\u539f\u51fd\u6570\u76f8\u5207(\u66f2\u7387\u4e0d\u5fc5\u4e00\u81f4\u7684)\u7684\u51fd\u6570 g(x_k) + \\nabla g(x_k)^T(u-x_k) + \\frac{\\mu}{2} ||u-x_k||_2^2 \uff0c \u628a h(x) \u76f4\u63a5\u4e0e\u65b0\u4e8c\u6b21\u4ee3\u7406\u51fd\u6570\u7684\u6c42\u548c\u6c42global minimum. x^+ = \\underset{u}{\\text{argmin}}(h(u) + g(x_k) + \\nabla g(x_k)^T (u-x_k) + \\frac{\\mu}{2} ||u-x_k||_2^2) = \\underset{u}{\\text{argmin}}(h(u) + \\nabla g(x_k)^Tu + \\frac{\\mu}{2}||u-x_k||^2_2) \u91cd\u70b9: \u5982\u679c h(x) \u4e3aL2-norm L1-norm, \u8fd9\u4e9b\u5e38\u89c1\u7684regularizer, \u90a3\u4e48\u6700\u540e\u7684\u6c42\u4f18\u5316\u662f\u4e00\u4e2a\u5178\u578b\u7684\u4e8c\u6b21\u51fd\u6570 + \u4e8c\u6b21\u51fd\u6570\u6216\u8005 \u4e8c\u6b21\u51fd\u6570 + l_1 ,\u90fd\u662f\u6709closed-form solution\u7684\u3002 \u5982\u679c\u8bb0 x_k^* \u4e3a\u4e8c\u6b21\u4ee3\u7406\u51fd\u6570\u7684\u6781\u503c\u70b9 x_k - \\frac{\\nabla g(x_k)}{\\mu} , \u5c06 \\nabla g(x_k) \u4ee3\u5165\u4e0a\u6587\u7684\u6700\u4f18\u5316\u516c\u5f0f\uff0c \u90a3\u4e48\u6700\u540e\u7684\u6c42\u89e3\u51fd\u6570\u5e38\u5e38\u4f1a\u5199\u6210 proximal mapping (prox-operator), \\text{prox}_h^\\mu(x_k^*) = \\underset{u}{\\text{argmin}}(h(u) + \\frac{\\mu}{2}||u-x_k^*||_2^2) .\u8bf4\u660e\u6700\u7ec8\u7684\u7ed3\u679c\u65e2\u9700\u8981\u8003\u8651 g \u7684\u5c40\u90e8\u6700\u4f18\u70b9\uff0c\u4e5f\u9700\u8981\u8003\u8651 h(x) . \u7279\u6b8a\u60c5\u51b5\u4e2d\uff0c h \u53ef\u4ee5\u662f\u4e00\u4e2a indicator function,\u6307\u5b9a\u51fd\u6570\u7684\u5b9a\u4e49\u57df, \u5982\u679c x \u5728\u5b9a\u4e49\u57df C \u5185\u5219 h(x) = 0 ,\u5426\u5219 h(x) = \\infty . \u8fd9\u6837\u7684\u60c5\u51b5\u4e0b prox_h \u51fd\u6570\u53d8\u6210\u4e86\u5728\u6709\u9650\u5b9a\u4e49\u57df\u5185\u5bfb\u627e\u6700\u4f18\u89e3\u7684\u95ee\u9898\uff0c\u5f97\u5230\u7684\u7ed3\u679c\u662f\u6700\u4f18\u89e3\u5728\u8fb9\u754c\u4e0a\u7684\u6295\u5f71\u3002\u56e0\u800c wiki \u8bf4 prox\u51fd\u6570\u662fprojection\u64cd\u4f5c\u7684\u63a8\u5e7f\u3002","title":"Proximal Gradient Method"},{"location":"The_theory/First_order_methods_review_and_updates/#projected-gradient-descent","text":"\u5982\u679c\u4f7f\u7528\u68af\u5ea6\u4e0b\u964d\u7684\u65f6\u5019\u8981\u6c42 x \u5728\u5b9a\u4e49\u57df C \u4e2d\uff0c\u90a3\u4e48\u4e00\u4e2aprojected\u68af\u5ea6\u4e0b\u964d\u7684\u505a\u6cd5\u662f\uff0c\u9996\u5148\u8fdb\u884c\u65e0\u7ea6\u675f\u68af\u5ea6\u4e0b\u964d\uff0c\u7136\u540e\u5c06\u7ed3\u679c\u6295\u5f71\u5230\u8fb9\u754c\u4e0a. \u8bb0\u4e3a x_{k+1} = \\pi_C(x_k - \\alpha_k\\nabla f(x_k)) , \u5176\u4e2d \\pi_C \u4e3a\u6b27\u51e0\u91cc\u5f97\u6295\u5f71\uff0c\u53ef\u4ee5\u5199\u6210 x_{k+1} = \\underset{x\\in C}{\\text{argmin}}||x_k - \\alpha_k \\nabla f(x_k) - x||_2^2 \u5c55\u5f00\u540e\u53d8\u4e3a x_{k+1} = \\underset{x\\in C}{\\text{argmin}}\\{\\langle x,\\nabla f(x_k) \\rangle + \\frac{1}{\\alpha_k}\\frac{||x-x_k||^2_2}{2} \\} General Projected Gradient Descent \u5219\u4f1a\u5141\u8bb8\u4e0d\u540c\u7684\u8ddd\u79bb\u91cf\u53d8\u4e3a: x_{k+1}=\\arg \\min _{x \\in C}\\left\\{\\left\\langle x, \\nabla f\\left(x_{k}\\right)\\right\\rangle+\\frac{1}{\\alpha_{k}} d\\left(x, x_{k}\\right)\\right\\} \u5e38\u7528\u7684\u8ddd\u79bb\u91cf Bregman Divergence D_f(x, y) = f(x) - f(y) - \\langle \\nabla f(y), x-y\\rangle \u7531 f(x) = \\sum x\\log x \u53ef\u4ee5\u4ea7\u51fa KL divergence. mirror gradient descent\u5e38\u5e38\u7528\u6765\u5206\u6790","title":"Projected Gradient Descent"},{"location":"The_theory/First_order_methods_review_and_updates/#nesterov-accelerated-gradient-descent","text":"Nesterov Acceleration\u6709\u5f88\u591a\u4e2a\u4e0d\u540c\u7684\u7248\u672c\u3002 Nesterov \u5728 1983\u5e74\u7684paper: \\begin{aligned} y_t = x_t + \\frac{t}{t+3}(x^{t} - x^{t-1}) \\\\ x_{t+1} = y_t - \\eta_t \\nabla f(y^t) \\\\ \\end{aligned} \u5176\u4e2d \\frac{t}{t+3} \u53ef\u88ab\u6cdb\u5316\u4e3a\u4e00\u822c\u8d85\u53c2\u6570 \\beta . \u53ef\u4ee5\u7406\u89e3\u4e3a y_t \u4e3a\u6cbf\u7740\u4e4b\u524d\u7684\u66f4\u65b0\u65b9\u5411look-ahead\u7684\u76ee\u7684\u4f4d\u7f6e, \u7136\u540e\u5728look-ahead\u7684\u4f4d\u7f6e\u8fdb\u884c\u68af\u5ea6\u66f4\u65b0. \u8fd9\u7bc7Medium\u535a\u5ba2 \u63d0\u4f9b\u4e86\u57fa\u4e8e\"look-ahead\"\u7684\u9610\u8ff0. pytorch\u7684\u7248\u672c \u91c7\u7528\u7684\u662f\u66f4\u7740\u91cd\u4e8emomentum\u7684\u7b97\u6cd5\u63cf\u8ff0\uff0c\u5bf9\u6807\u7684\u662fmomentum gradient descent. \\begin{aligned} v_{t+1} &= \\mu \\cdot v_t + g_{t+1} \\\\ p_{t+1} &= p_t - lr \\cdot v_{t+1} \\end{aligned}","title":"Nesterov Accelerated Gradient Descent"},{"location":"The_theory/First_order_methods_review_and_updates/#triple-momentum-gradient-descent","text":"pdf \u8fd9\u7bc7paper\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u56db\u4e2a\u53c2\u6570\u7684 triple momentum (TM)\u6846\u67b6. \u8fd9\u4e2a\u6846\u67b6\u4e0b\u7684\u4f18\u5316\u5668\u7b97\u6cd5: \\begin{aligned} \\xi_{k+1} &= (1+\\beta) \\xi_k - \\beta \\xi_{k-1} - \\alpha \\nabla f(y_k)\\\\ y_k &= (1 + \\gamma)\\xi_k - \\gamma\\xi_{k-1}\\\\ x_k &= (1 + \\delta)\\xi_k - \\delta\\xi_{k-1} \\end{aligned} | Method | Parameters | | :---------------- | :-------------------------------: | | Gradient descent | (\\alpha, 0, 0, 0) | | Momentum Gradient | (\\alpha, \\beta, 0, 0) | | Nesterov AG | (\\alpha, \\beta, \\beta, 0) | | General TM Algo. | (\\alpha, \\beta, \\gamma, \\delta) | \u76f4\u89c9\u4e0a\u6765\u8bf4\uff0c\u5982\u679c \\delta=0 , \u5219 x_k = \\xi_k , \u4ee3\u5165\u540e\u5c31\u80fd\u5f97\u5230\u524d\u9762\u63d0\u5230\u7684\u51e0\u4e2a\u7b97\u6cd5\u516c\u53f8. \u4f5c\u8005\u7ed9\u51fa\u4e86\u53c2\u6570, \u5982\u679c\u4e00\u76f4\u9700\u8981\u4f18\u5316\u7684\u51fd\u6570\u662f L-\\text{smooth}, \\mu-\\text{strongly convex} \u7684\u5219\u4ee4 \\kappa = L / \\mu, \\rho = 1 - 1 / \\sqrt{\\kappa} , \u8bbe\u5b9a\u4e00\u4e0b\u53c2\u6570\uff0c (\\alpha, \\beta, \\gamma, \\delta) = (\\frac{1+\\rho}{L}, \\frac{\\rho^2}{2-\\rho}, \\frac{\\rho^2}{(1+\\rho)(2-\\rho)}, \\frac{\\rho^2}{1 - \\rho^2})","title":"Triple Momentum Gradient Descent"},{"location":"The_theory/First_order_methods_review_and_updates/#_3","text":"\u8fd9\u4e2apaper\u662f\u6709\u5c0f\u95ee\u9898\u7684\uff0c\u5bf9\u524d\u9762TM\u6846\u67b6\u8fdb\u884c\u5206\u6790\uff0c\u53ef\u4ee5\u901a\u8fc7\u7528 x, y \u8868\u8fbe (\\xi_k - \\xi_{k-1}) \u53ef\u4ee5\u6d88\u53bb \\xi .\u540c\u65f6 \\gamma, \\delta \u4e2d\u6709\u4e00\u4e2a\u53c2\u6570\u662f\u591a\u4f59\u7684.\u8fd9\u4e2a\u516c\u5f0f\u6700\u7ec8\u53ef\u4ee5\u5316\u4e3a x_{k+1} - y_{k+1} = \\beta(x_k - y_k) - \\alpha (\\delta - \\gamma) \\nabla f(y_k) \u8fd9\u91cc\u5f15\u51fa\u4e86\u4e0b\u4e00\u7bc7paper\u63d0\u5230\u7684\u53c2\u6570\u5197\u4f59\u95ee\u9898\u3002","title":"\u4e2a\u4eba\u5206\u6790"},{"location":"The_theory/First_order_methods_review_and_updates/#boosting-first-order-methods-by-shifting-objective-new-schemes-with-faster-worst-case-rates","text":"pdf \u5bf9\u4e8e\u539f\u6765\u7684\u76ee\u6807\u51fd\u6570\uff0c\u53ef\u4ee5\u8f6c\u800c\u6c42\u89e3\u4e00\u4e2a\u65b0\u7684shifted objective. \\underset{x\\in\\mathcal{R}^d}{\\text{min}}h(x) = \\frac{1}{n}\\sum^n_{i=1}h_i(x), \\quad \\text{where}\\quad \\ h_i(x) = f_i(x) - f_i(x^*) - \\left\\langle\\nabla f_{i}\\left(x^{\\star}\\right), x-x^{\\star}\\right\\rangle - \\frac{\\mu}{2}||x - x^*||^2 \u53ef\u4ee5\u7b80\u5355\u7406\u89e3\u4e3a\u539f\u51fd\u6570\u4e0e\u6700\u503c\u70b9\u9644\u8fd1\u7684\u4e0b\u754c\u4e8c\u6b21\u51fd\u6570\u7684\u6b8b\u5dee.\u5bb9\u6613\u8bc1\u660e h(x)\u662f\u4e00\u4e2a\u51f8\u51fd\u6570 h(x)\u7684\u6781\u503c\u70b9\u4e0e \\sum f(x) \u76f8\u540c\uff0c\u8fdb\u800c\u53ef\u4ee5\u8bf4 h(x) \u7684\u6700\u4f18\u503c\u70b9\u4e0e f(x) \u7684\u76f8\u540c\u3002 h(x) (L-\\mu) -smooth \u77eb\u987d\u6027: \\forall x, y \\in \\mathbb{R}^{d}, h(x)-h(y)-\\langle\\nabla h(y), x-y\\rangle \\geq \\frac{1}{2(L-\\mu)}\\|\\nabla h(x)-\\nabla h(y)\\|^{2}","title":"Boosting First-Order Methods by Shifting Objective: New Schemes with Faster Worst Case Rates"},{"location":"The_theory/First_order_methods_review_and_updates/#generalized-triple-momentum","text":"\u5176\u4e2d z_{k+1} \u7684\u8868\u8fbe\u5f0f\u7b49\u4ef7\u4e8e z_{k+1} = \\text{argmin}_x\\left\\{ (\\mu/2) ||x - y_k^*||_2^2 + (\\alpha_k / 2)||x - z_k||_2^2\\right\\} \u5176\u4e2d y_k^* \u4e3a\u4e8c\u6b21surrogate function g(y_k) + \\nabla g(y_k)^T(u-y_k) + \\frac{\\mu}{2} ||u-y_k||_2^2 \u7684\u6700\u4f18\u89e3 y_k-\\frac{\\nabla f(y_k)}{\\mu} . \u53ef\u5f97 z_{k+1} = \\frac{\\mu y_k^* + \\alpha z_k}{\\mu + \\alpha} = z_k - \\frac{\\mu}{\\mu+\\alpha}(y_k^* - z_k) , \u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u53ef\u4ee5\u8868\u8fbe\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5. \u800c\u5f53 \\tau_k^z = 0 \u65f6\uff0c\u53ef\u4ee5\u7b80\u5355\u5730\u5c06\u8fd9\u4e24\u6761\u5f0f\u5b50\u5316\u4e3a Nesterov\u7684\u6807\u51c6\u5f62\u5f0f\u3002 \u8fdb\u4e00\u6b65\u5730\u89c2\u5bdf y_k \u7684\u8868\u8fbe\u5f0f\u7684\u7b2c\u4e8c\u9879,\u5c06 z_{k} \u4ee3\u5165,\u5f97\u5230 \\frac{2\\mu + \\alpha}{\\mu + \\alpha}[\\mu(y_{k-1} - z_{k-1}) - \\nabla f(y_{k-1})] , \u7531\u4e8e\u5f3a\u51f8\u6027, \u7b2c\u4e8c\u9879\u4e0e y_{k-1} - z_{k-1} \u7b26\u53f7\u76f8\u53cd,\u8fd9\u662f\u7528\u4e8e\u8865\u507fstrongly-convex\u51fd\u6570\u7684\u4e8c\u9636\u5bfc\u7684,\u5f53\u6211\u4eec\u7528\u66f2\u7387\u4e3a \\mu \u7684\u66f4\u4e3a\u6241\u5e73\u7684\u4e8c\u6b21\u4ee3\u7406\u51fd\u6570\u6c42\u89e3\u65f6\uff0c\u6211\u4eec\u4f1a\u9ad8\u4f30\u5408\u9002\u7684\u68af\u5ea6\u4e0b\u964d\u6b65\u957f\u7684\u5927\u5c0f\uff0c\u56e0\u800c\u5e26\u7740\u52a8\u91cf\u5f80\u524d\u7684 y_k \u5728\u9884\u6d4blook-ahead \u65f6\u4f1a\u51cf\u53bb\u4e4b\u524d\u9ad8\u4f30\u7684\u4e00\u90e8\u5206\u6b65\u957f. \u53c2\u8003\u53c2\u6570 \\alpha=\\sqrt{L\\mu} - \\mu, \\tau_x=\\frac{2\\sqrt{\\kappa}-1}{\\kappa}, \\tau_z = \\frac{\\sqrt{\\kappa} - 1}{L(\\sqrt{\\kappa} - 1)} \u6587\u7ae0\u66f4\u4e25\u8c28\u4e14\u901a\u7528\u5730\u7528\u674e\u96c5\u666e\u8bfa\u592b\u51fd\u6570 T_{k}=h\\left(y_{k-1}\\right)-\\frac{1}{2(L-\\mu)}\\left\\|\\nabla h\\left(y_{k-1}\\right)\\right\\|^{2}+\\frac{\\lambda}{2}\\left\\|z_{k}-x^{\\star}\\right\\|^{2}, where \\lambda>0 \u8fdb\u884c\u4e86\u8bc1\u660e\uff0c \u8bf4\u660e\u5176\u6536\u655b\u901f\u5ea6\u4e0a\u9650\u6bd4\u8f83\u5c0f\u3002 \u4f5c\u8005\u4e4b\u540e\u5c06\u8fd9\u4e2a\u7b97\u6cd5\u9644\u52a0\u5728\u591a\u4e2a\u4e00\u9636\u7b97\u6cd5\u4e0a(SVRG, SAGA)\u3002\u7531\u4e8e\u4e2a\u4eba\u8ba4\u77e5\u6709\u9650\uff0c\u6682\u8fd8\u6ca1\u6709\u80fd\u529b\u5206\u6790\u8fd9\u4e9b\u7b97\u6cd5\u53ca\u5176\u5e94\u7528.","title":"Generalized Triple Momentum"},{"location":"The_theory/Framework_Uncertainty_Propagation/","text":"Uncertainty Propagation in Neural Network \u672c\u6587\u4e3b\u8981\u8ba8\u8bba\u4e0d\u786e\u5b9a\u6027\u5728\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u4f20\u64ad\u7684\u4e00\u4e2a\u65b9\u6cd5\u3002 \u5728Bayesian Neural Network\u4e2d\uff0c\u6709\u4e24\u4e2a\u6d41\u6d3e\uff0c\u7b2c\u4e00\u4e2a\u662f\u628a\u6bcf\u4e00\u4e2aweight\u5efa\u6a21\u4e3a\u4e00\u4e2a\u9ad8\u65af\u5206\u5e03\uff0c\u53e6\u4e00\u4e2a\u662f\u628a\u8f93\u5165\u4e0e\u6fc0\u6d3b\u5efa\u6a21\u4e3a\u6982\u7387\u5206\u5e03\uff0c\u800cweight\u53ea\u662f\u5e38\u6570\u3002 \u8fd9\u91cc\u5206\u4eab\u4e00\u7bc7\u5728Robotics\u4e2d\u5e94\u7528\u7b2c\u4e8c\u79cdBNN\u7684paper\uff0c\u5728\u8fd9\u4e4b\u524d\u5206\u4eab\u672c\u6587\u7684\u4e3b\u8981\u6570\u5b66\u524d\u7f6e\uff0c Lightweight Probabilistic Deep Networks pdf \u672c\u6587\u76f4\u63a5\u4ecb\u7ecdADF\uff0c\u5c06\u4e2d\u95f4\u6240\u6709\u7684\u6fc0\u6d3b\u5c42\u90fd\u7406\u89e3\u4e3a\u6982\u7387\u5206\u5e03. \u5bf9\u4e8e\u4e00\u4e2a\u6807\u51c6\u7684\u795e\u7ecf\u7f51\u7edc\u524d\u5411\u4f20\u9012\uff0c\u53ef\u4ee5\u7528\u6982\u7387\u89e3\u8bfb\uff0c\u5199\u4e3a: \\begin{aligned} p\\left(\\mathbf{z}^{(0: l)}\\right) &=p\\left(\\mathbf{z}^{(0)}\\right) \\prod_{i=1}^{l} p\\left(\\mathbf{z}^{(i)} | \\mathbf{z}^{(i-1)}\\right) \\\\ p\\left(\\mathbf{z}^{(i)} | \\mathbf{z}^{(i-1)}\\right) &=\\delta\\left[\\mathbf{z}^{(i)}-\\mathbf{f}^{(i)}\\left(\\mathbf{z}^{(i-1)}\\right)\\right] \\end{aligned} \u6211\u4eec\u5f00\u59cb\u5047\u8bbe\uff0c\u7f51\u7edc\u7684\u8f93\u5165\u5c42\u6709\u566a\u97f3\uff0c\u53ef\u4ee5\u7528element-wise\u7684\u9ad8\u65af\u5206\u5e03\u5efa\u6a21(\u6ce8\u610f\u8fd9\u91cc\u5c06\u8f93\u5165\u566a\u97f3\u8bbe\u5b9a\u4e3a\u5404\u4e2a\u5143\u7d20\u72ec\u7acb\u7684\u65e0\u5473\u7684\u8bef\u5dee, aleatoric uncertaint)\u3002 p\\left(\\mathbf{z}^{(0)}\\right)=\\prod_{j} \\mathcal{N}\\left(z_{j}^{(0)} | x_{j}, \\sigma_{n}^{2}\\right) \u9ad8\u65af\u5206\u5e03\u5728\u7f51\u7edc\u4f20\u64ad\u4e2d\uff0c\u5c24\u5176\u662f\u975e\u7ebf\u6027\u5c42\u4f20\u64ad\u540e\u663e\u7136\u4e0d\u518d\u662f\u9ad8\u65af\u5206\u5e03\u3002\u4f5c\u8005\u6307\u51fa\uff0c\u4e3a\u4e86\u66f4\u597d\u7684\u5efa\u6a21\u4f20\u64ad\u540e\u7684\u5747\u503c\u4e0e\u65b9\u5dee\uff0c\u8981\u6c42\u7528\u4e00\u4e2a\u4e0e\u5b9e\u9645\u5206\u5e03\u7684 KL divergence \u5c3d\u53ef\u80fd\u5c0f\u7684\u5206\u5e03\uff0c \u8fd9\u7bc7paper \u6307\u51fa\u8fd9\u7b49\u4ef7\u4e8e\u8ba9\u6982\u7387\u5206\u5e03\u7684\u77e9\u76f8\u7b49\uff0c\u5728\u9ad8\u65af\u5047\u8bbe\u4e0b\uff0c\u5c31\u662f\u5747\u503c\u4e0e\u65b9\u5dee\u76f8\u7b49(\u9ad8\u65af\u53ea\u6709\u4e24\u4e2a\u53c2\u6570\uff0c\u53ea\u80fd\u63a7\u5236\u524d\u4e24\u9636\u7684\u77e9)\u3002 \u5bf9\u4e8e\u5377\u79ef\u5c42\u4ee5\u53ca\u666e\u901a\u5168\u8fde\u63a5\u5c42\uff0c\u672c\u8d28\u4e0a\u6765\u8bf4\u5b83\u4eec\u90fd\u662f\u7ebf\u6027\u5c42\uff0c\u65b9\u5dee\u4e0e\u5747\u503c\u7684\u8ba1\u7b97\u662f\u89e3\u8026\u7684\u3002 \\mathbb{E}[\\mathbf{f}(\\mathbf{z})]=W \\boldsymbol{\\mu}_{z}+\\mathbf{b} \\mathbb{V}[\\mathbf{f}(\\mathbf{z})]=(W \\circ W) \\boldsymbol{v}_{z} \\circ \u6307element-wise\u76f8\u4e58\uff0c\u5728implementation\u65f6\uff0c\u53ef\u4ee5\u53d1\u73b0\u8fd0\u7b97\u89c4\u5219\u4e0e\u539f\u6765\u7684Forward\u662f\u4e00\u81f4\u7684\uff0c\u7528\u4e0d\u540c\u7684weight\u548cbias\u5c31\u53ef\u4ee5\u4e86\u3002 \u5bf9\u4e8eReLU, \u8fd9\u7bc7paper \u8bc1\u660e\u4e86: \\begin{aligned} \\mu_{\\text {relu }}(\\mu, v) &=\\mu \\cdot \\Phi\\left(\\frac{\\mu}{\\sigma}\\right)+\\sigma \\cdot \\phi\\left(\\frac{\\mu}{\\sigma}\\right) \\\\ v_{\\text {relu }}(\\mu, v) &=(\\mu+v) \\cdot \\Phi\\left(\\frac{\\mu}{\\sigma}\\right)+\\mu \\sigma \\cdot \\phi\\left(\\frac{\\mu}{\\sigma}\\right)-\\mu_{\\text {relu }}^{2}(\\mu, v) \\end{aligned} \u5176\u4e2d \\sigma = \\sqrt{v} \u4e14 \\Phi \u662f\u6807\u51c6\u6b63\u592a\u7684\u7d2f\u8ba1\u51fd\u6570, \\phi \u662f\u6807\u51c6\u6b63\u6001\u51fd\u6570\u5bc6\u5ea6\u51fd\u6570 \u5bf9\u4e8eMaxPooling, \u8fd9\u7bc7paper \u6307\u51fa,\u5bf9\u4e8e\u4e24\u4e2a\u9ad8\u65af\u51fd\u6570: \\begin{aligned} E[Y] &=\\mu_{X_{i}} \\Phi(\\alpha)+\\mu_{X_{j}} \\Phi(-\\alpha)+\\theta \\phi(\\alpha) \\\\ \\operatorname{Var}[Y] &=\\left(\\sigma_{X_{i}}^{2}+\\mu_{X_{i}}^{2}\\right) \\Phi(\\alpha)+\\left(\\sigma_{X_{j}}^{2}+\\mu_{X_{j}}^{2}\\right) \\Phi(-\\alpha)+\\left(\\mu_{X_{i}}+\\mu_{X_{j}}\\right) \\theta \\phi(\\alpha)-E[Y]^{2} \\end{aligned} \\alpha=\\frac{\\left(\\mu_{X_{i}}-\\mu_{X_{j}}\\right)}{\\theta}, \\theta=\\sqrt{\\sigma_{X_{i}}^{2}+\\sigma_{X_{j}}^{2}} \u8fed\u4ee3\u5c06\u6240\u6709\u9700\u8981\u5904\u7406\u7684\u5143\u7d20\u9010\u4e2aMax\u8fd0\u7b97\u3002\u987a\u5e8f\u662f\u6709\u76f8\u5173\u7684\uff0c\u4f46\u662f\u672c\u6587\u5efa\u8bae\u4e3a\u4e86\u63d0\u901f\u53ef\u4ee5\u5c3d\u53ef\u80fd\u5e76\u884c\uff0c\u5148\u6c34\u5e73\u518d\u7ad6\u76f4\u65b9\u5411\uff0c\u76f4\u63a5\u5206\u5f00\u8ba1\u7b97\u3002 A General Framework for Uncertainty Estimation in Deep Learning \u672c\u6587\u5728end-to-end learning\u4e0a\u7efc\u5408\u4e24\u79cdbayesian uncertainty,\u7b2c\u4e00\u4e2a\u662f\u7531\u8f93\u5165\u6570\u636e\u4e0d\u786e\u5b9a\u6027\u5411\u540e\u4f20\u64ad\u7684\u7ed3\u679c\u3002\u7b2c\u4e8c\u4e2a\u662f T \u6b21\u91c7\u6837Dropout,\u5f97\u5230 T \u4e2a\u4e0d\u540c\u7684\u7ed3\u679c\u5982\u56fe\u878d\u5408\u3002","title":"Uncertainty Propagation in Neural Network"},{"location":"The_theory/Framework_Uncertainty_Propagation/#uncertainty-propagation-in-neural-network","text":"\u672c\u6587\u4e3b\u8981\u8ba8\u8bba\u4e0d\u786e\u5b9a\u6027\u5728\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u4f20\u64ad\u7684\u4e00\u4e2a\u65b9\u6cd5\u3002 \u5728Bayesian Neural Network\u4e2d\uff0c\u6709\u4e24\u4e2a\u6d41\u6d3e\uff0c\u7b2c\u4e00\u4e2a\u662f\u628a\u6bcf\u4e00\u4e2aweight\u5efa\u6a21\u4e3a\u4e00\u4e2a\u9ad8\u65af\u5206\u5e03\uff0c\u53e6\u4e00\u4e2a\u662f\u628a\u8f93\u5165\u4e0e\u6fc0\u6d3b\u5efa\u6a21\u4e3a\u6982\u7387\u5206\u5e03\uff0c\u800cweight\u53ea\u662f\u5e38\u6570\u3002 \u8fd9\u91cc\u5206\u4eab\u4e00\u7bc7\u5728Robotics\u4e2d\u5e94\u7528\u7b2c\u4e8c\u79cdBNN\u7684paper\uff0c\u5728\u8fd9\u4e4b\u524d\u5206\u4eab\u672c\u6587\u7684\u4e3b\u8981\u6570\u5b66\u524d\u7f6e\uff0c","title":"Uncertainty Propagation in Neural Network"},{"location":"The_theory/Framework_Uncertainty_Propagation/#lightweight-probabilistic-deep-networks","text":"pdf \u672c\u6587\u76f4\u63a5\u4ecb\u7ecdADF\uff0c\u5c06\u4e2d\u95f4\u6240\u6709\u7684\u6fc0\u6d3b\u5c42\u90fd\u7406\u89e3\u4e3a\u6982\u7387\u5206\u5e03. \u5bf9\u4e8e\u4e00\u4e2a\u6807\u51c6\u7684\u795e\u7ecf\u7f51\u7edc\u524d\u5411\u4f20\u9012\uff0c\u53ef\u4ee5\u7528\u6982\u7387\u89e3\u8bfb\uff0c\u5199\u4e3a: \\begin{aligned} p\\left(\\mathbf{z}^{(0: l)}\\right) &=p\\left(\\mathbf{z}^{(0)}\\right) \\prod_{i=1}^{l} p\\left(\\mathbf{z}^{(i)} | \\mathbf{z}^{(i-1)}\\right) \\\\ p\\left(\\mathbf{z}^{(i)} | \\mathbf{z}^{(i-1)}\\right) &=\\delta\\left[\\mathbf{z}^{(i)}-\\mathbf{f}^{(i)}\\left(\\mathbf{z}^{(i-1)}\\right)\\right] \\end{aligned} \u6211\u4eec\u5f00\u59cb\u5047\u8bbe\uff0c\u7f51\u7edc\u7684\u8f93\u5165\u5c42\u6709\u566a\u97f3\uff0c\u53ef\u4ee5\u7528element-wise\u7684\u9ad8\u65af\u5206\u5e03\u5efa\u6a21(\u6ce8\u610f\u8fd9\u91cc\u5c06\u8f93\u5165\u566a\u97f3\u8bbe\u5b9a\u4e3a\u5404\u4e2a\u5143\u7d20\u72ec\u7acb\u7684\u65e0\u5473\u7684\u8bef\u5dee, aleatoric uncertaint)\u3002 p\\left(\\mathbf{z}^{(0)}\\right)=\\prod_{j} \\mathcal{N}\\left(z_{j}^{(0)} | x_{j}, \\sigma_{n}^{2}\\right) \u9ad8\u65af\u5206\u5e03\u5728\u7f51\u7edc\u4f20\u64ad\u4e2d\uff0c\u5c24\u5176\u662f\u975e\u7ebf\u6027\u5c42\u4f20\u64ad\u540e\u663e\u7136\u4e0d\u518d\u662f\u9ad8\u65af\u5206\u5e03\u3002\u4f5c\u8005\u6307\u51fa\uff0c\u4e3a\u4e86\u66f4\u597d\u7684\u5efa\u6a21\u4f20\u64ad\u540e\u7684\u5747\u503c\u4e0e\u65b9\u5dee\uff0c\u8981\u6c42\u7528\u4e00\u4e2a\u4e0e\u5b9e\u9645\u5206\u5e03\u7684 KL divergence \u5c3d\u53ef\u80fd\u5c0f\u7684\u5206\u5e03\uff0c \u8fd9\u7bc7paper \u6307\u51fa\u8fd9\u7b49\u4ef7\u4e8e\u8ba9\u6982\u7387\u5206\u5e03\u7684\u77e9\u76f8\u7b49\uff0c\u5728\u9ad8\u65af\u5047\u8bbe\u4e0b\uff0c\u5c31\u662f\u5747\u503c\u4e0e\u65b9\u5dee\u76f8\u7b49(\u9ad8\u65af\u53ea\u6709\u4e24\u4e2a\u53c2\u6570\uff0c\u53ea\u80fd\u63a7\u5236\u524d\u4e24\u9636\u7684\u77e9)\u3002 \u5bf9\u4e8e\u5377\u79ef\u5c42\u4ee5\u53ca\u666e\u901a\u5168\u8fde\u63a5\u5c42\uff0c\u672c\u8d28\u4e0a\u6765\u8bf4\u5b83\u4eec\u90fd\u662f\u7ebf\u6027\u5c42\uff0c\u65b9\u5dee\u4e0e\u5747\u503c\u7684\u8ba1\u7b97\u662f\u89e3\u8026\u7684\u3002 \\mathbb{E}[\\mathbf{f}(\\mathbf{z})]=W \\boldsymbol{\\mu}_{z}+\\mathbf{b} \\mathbb{V}[\\mathbf{f}(\\mathbf{z})]=(W \\circ W) \\boldsymbol{v}_{z} \\circ \u6307element-wise\u76f8\u4e58\uff0c\u5728implementation\u65f6\uff0c\u53ef\u4ee5\u53d1\u73b0\u8fd0\u7b97\u89c4\u5219\u4e0e\u539f\u6765\u7684Forward\u662f\u4e00\u81f4\u7684\uff0c\u7528\u4e0d\u540c\u7684weight\u548cbias\u5c31\u53ef\u4ee5\u4e86\u3002 \u5bf9\u4e8eReLU, \u8fd9\u7bc7paper \u8bc1\u660e\u4e86: \\begin{aligned} \\mu_{\\text {relu }}(\\mu, v) &=\\mu \\cdot \\Phi\\left(\\frac{\\mu}{\\sigma}\\right)+\\sigma \\cdot \\phi\\left(\\frac{\\mu}{\\sigma}\\right) \\\\ v_{\\text {relu }}(\\mu, v) &=(\\mu+v) \\cdot \\Phi\\left(\\frac{\\mu}{\\sigma}\\right)+\\mu \\sigma \\cdot \\phi\\left(\\frac{\\mu}{\\sigma}\\right)-\\mu_{\\text {relu }}^{2}(\\mu, v) \\end{aligned} \u5176\u4e2d \\sigma = \\sqrt{v} \u4e14 \\Phi \u662f\u6807\u51c6\u6b63\u592a\u7684\u7d2f\u8ba1\u51fd\u6570, \\phi \u662f\u6807\u51c6\u6b63\u6001\u51fd\u6570\u5bc6\u5ea6\u51fd\u6570 \u5bf9\u4e8eMaxPooling, \u8fd9\u7bc7paper \u6307\u51fa,\u5bf9\u4e8e\u4e24\u4e2a\u9ad8\u65af\u51fd\u6570: \\begin{aligned} E[Y] &=\\mu_{X_{i}} \\Phi(\\alpha)+\\mu_{X_{j}} \\Phi(-\\alpha)+\\theta \\phi(\\alpha) \\\\ \\operatorname{Var}[Y] &=\\left(\\sigma_{X_{i}}^{2}+\\mu_{X_{i}}^{2}\\right) \\Phi(\\alpha)+\\left(\\sigma_{X_{j}}^{2}+\\mu_{X_{j}}^{2}\\right) \\Phi(-\\alpha)+\\left(\\mu_{X_{i}}+\\mu_{X_{j}}\\right) \\theta \\phi(\\alpha)-E[Y]^{2} \\end{aligned} \\alpha=\\frac{\\left(\\mu_{X_{i}}-\\mu_{X_{j}}\\right)}{\\theta}, \\theta=\\sqrt{\\sigma_{X_{i}}^{2}+\\sigma_{X_{j}}^{2}} \u8fed\u4ee3\u5c06\u6240\u6709\u9700\u8981\u5904\u7406\u7684\u5143\u7d20\u9010\u4e2aMax\u8fd0\u7b97\u3002\u987a\u5e8f\u662f\u6709\u76f8\u5173\u7684\uff0c\u4f46\u662f\u672c\u6587\u5efa\u8bae\u4e3a\u4e86\u63d0\u901f\u53ef\u4ee5\u5c3d\u53ef\u80fd\u5e76\u884c\uff0c\u5148\u6c34\u5e73\u518d\u7ad6\u76f4\u65b9\u5411\uff0c\u76f4\u63a5\u5206\u5f00\u8ba1\u7b97\u3002","title":"Lightweight Probabilistic Deep Networks"},{"location":"The_theory/Framework_Uncertainty_Propagation/#a-general-framework-for-uncertainty-estimation-in-deep-learning","text":"\u672c\u6587\u5728end-to-end learning\u4e0a\u7efc\u5408\u4e24\u79cdbayesian uncertainty,\u7b2c\u4e00\u4e2a\u662f\u7531\u8f93\u5165\u6570\u636e\u4e0d\u786e\u5b9a\u6027\u5411\u540e\u4f20\u64ad\u7684\u7ed3\u679c\u3002\u7b2c\u4e8c\u4e2a\u662f T \u6b21\u91c7\u6837Dropout,\u5f97\u5230 T \u4e2a\u4e0d\u540c\u7684\u7ed3\u679c\u5982\u56fe\u878d\u5408\u3002","title":"A General Framework for Uncertainty Estimation in Deep Learning"},{"location":"The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/","text":"FreeAnchor: Learning to Match Anchors for Visual Object Detection \u8fd9\u7bc7\u8bba\u6587\u6765\u81ea\u4e8eNIPS2019,\u4ecb\u7ecd\u4e86\u65b0\u7684\u65b9\u5f0f\u6765\u5b9e\u73b02D object detection\u4e2d\u7684matching. \u603b\u4f53\u601d\u8def\u6765\u8bf4\uff0c\u4f7f\u7528\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u7684\u6846\u67b6\u5728Object Detection\u7684\u8fc7\u7a0b\u4e2d\uff0c\u91c7\u7528\u4e00\u4e2a\u5355\u4e00\u7684loss\uff0c\u540c\u65f6\u5b8c\u6210proposal\u4e0eobject\u7684\u5339\u914d\u4ee5\u53ca\u56de\u5f52\u8bad\u7ec3\uff0c\u5c06\u4e2d\u95f4matching\u6b65\u9aa4\u81ea\u52a8\u5316\u4e86 \u6838\u5fc3\u7b97\u6cd5 \u7f51\u7edc\u6b63\u5e38\u524d\u4f20\u7ed9\u51fa\u4e00\u7cfb\u5217\u7684anchor box,\u6bcf\u4e00\u4e2aanchor box\u6709\u5206\u7c7b\u4ee5\u53calocalization\u4fe1\u606f \u5bf9\u6bcf\u4e00\u4e2a\u969c\u788d\u7269\uff0c\u9009\u62e9\u4e0e\u5b83IOU\u6700\u5927\u7684n\u4e2aanchor\uff0c\u5e26\u6709\u4e00\u4e2athreshold\uff0c\u5982\u679c\u4e00\u4e2aanchor\u4e0e\u591a\u4e2a\u7269\u4f53\u5339\u914d\uff0c \u5219\u4f7f\u8fd9\u4e2aanchor\u53ea\u4e0eIOU\u6700\u5927\u90a3\u4e2a\u7269\u4f53\u5339\u914d(\u662f\u5426\u6709\u8fd9\u4e2a\u9650\u5236\u9700\u8981\u770b\u5177\u4f53\u4ee3\u7801\u5b9e\u73b0) \u5bf9\u5f53\u524d\u7684\u5339\u914d\u5206\u914d\u8ba1\u7b97\u4e00\u4e2aLoss \u53cd\u4f20\u8bad\u7ec3 \u57fa\u4e8e\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u7684\u5339\u914dLoss \u5148\u7ed9\u51fa\u6700\u7ec8\u7684\u516c\u5f0f \\mathcal{L}^{\\prime}(\\theta)=-w_{1} \\sum_{i} \\log \\left(\\operatorname{Mean}-\\max \\left(X_{i}\\right)\\right)+w_{2} \\sum_{j} F L_{-}\\left(P\\left\\{a_{j} \\in A_{-}\\right\\}\\left(1-\\mathcal{P}(\\theta)_{j}^{b g}\\right)\\right) \u5176\u4e2d X_{i}=\\left\\{\\mathcal{P}(\\theta)_{i j}^{c l s} \\mathcal{P}(\\theta)_{i j}^{l o c} | a_{j} \\in A_{i}\\right\\} \u4ee3\u8868\u6bcf\u4e00\u4e2a\u7269\u4f53\u5bf9\u5e94\u7684anchor bag\u7684Likelihood \uff0c\u7b2c\u4e8c\u9879\u5219\u4ee3\u8868\u6ca1\u6709\u88ab\u9009\u5165anchor bag\u7684anchor\u4f5c\u4e3abackground\u7684Focal Loss\uff0cFocal Loss\u6e90\u81ea\u4e0e \u8fd9\u7bc7\u6587\u7ae0 ,\u76f4\u89c9\u5c31\u662f\u5bf9\u4e8e\u628a\u63e1\u5df2\u7ecf\u5f88\u5927\u7684\u6b63\u786e\u7684\u70b9\u964d\u4f4e\u6743\u91cd\uff0c \u5bf9\u96be\u5ea6\u8f83\u5927\u7684\u90e8\u5206\u589e\u52a0\u5b83\u4eec\u5728cost\u4e2d\u7684\u6743\u91cd\u3002 \u7b2c\u4e00\u9879\u3001Mean-max\u4ee5\u53ca\u6982\u7387\u5b9a\u4e49 Mean-max\u51fd\u6570\u8868\u8fbe\u5f0f\u4e3a: \\operatorname{Mean}-\\max (X)=\\frac{\\sum_{x_{j} \\in X} \\frac{x_{j}}{1-x_{j}}}{\\sum_{x_{j} \\in X} \\frac{1}{1-x_{j}}} \u5176\u5bf9\u5e94\u7684\u51fd\u6570\u56fe\u50cf\u4e3a \u76f4\u89c9\u6765\u8bf4\uff0c\u5c31\u662f\u5f53likelihood\u51fd\u6570\u503c\u8f83\u5c0f\uff0c\u7f51\u7edc\u521a\u521d\u59cb\u5316\u7684\u65f6\u5019\uff0c\u51fd\u6570\u7ed3\u679c\u8fd1\u4f3c\u4e8e\u5404\u4e2a\u6570\u7684\u5e73\u5747\u503c\uff0c \u5f53\u6709\u90e8\u5206likelihood\u51fd\u6570\u8f83\u5927\uff0c\u7f51\u7edc\u8bad\u7ec3\u6210\u719f\u4e4b\u540e\uff0c\u51fd\u6570\u7ed3\u679c\u8fd1\u4f3c\u4e8e\u5404\u4e2a\u6570\u7684max. \u6839\u636e\u4f5c\u8005\u7684\u8bbe\u8ba1 P(\\theta)^{cls}_{ij} = e^{-L(\\theta)^{cls}_{ij}} , Loc\u540c\u7406\uff0c\u7686\u4e3a\u5bf9\u5e94\u57fa\u7840cost\u51fd\u6570\u503c\u7684\u8d1f\u6307\u6570 Background\u9879\u5b9a\u4e49 P\\left\\{a_{j} \\in A_{-}\\right\\} = 1-\\max _{i} P\\left\\{a_{j} \\rightarrow b_{i}\\right\\} \u6307proposal\u6846 a_j \u4e0d\u4e0e\u6240\u6709\u7269\u4f53\u91cd\u5408\u7684\u6982\u7387 \u5176\u4e2d P\\left\\{a_{j} \\rightarrow b_{i}\\right\\}=\\text { Saturated linear }\\left(I o U_{i j}^{l o c}, t, \\max _{j}\\left(I o U_{i j}^{l o c}\\right)\\right) \\text { Saturated linear }\\left(x, t_{1}, t_{2}\\right)=\\left\\{\\begin{array}{ll}{0,} & {x \\leq t_{1}} \\\\ {\\frac{x-t_{1}}{t_{2}-t_{1}},} & {t_{1}<x<t_{2}} \\\\ {1,} & {x \\geq t_{2}}\\end{array}\\right.","title":"FreeAnchor: Learning to Match Anchors for Visual Object Detection"},{"location":"The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/#freeanchor-learning-to-match-anchors-for-visual-object-detection","text":"\u8fd9\u7bc7\u8bba\u6587\u6765\u81ea\u4e8eNIPS2019,\u4ecb\u7ecd\u4e86\u65b0\u7684\u65b9\u5f0f\u6765\u5b9e\u73b02D object detection\u4e2d\u7684matching. \u603b\u4f53\u601d\u8def\u6765\u8bf4\uff0c\u4f7f\u7528\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u7684\u6846\u67b6\u5728Object Detection\u7684\u8fc7\u7a0b\u4e2d\uff0c\u91c7\u7528\u4e00\u4e2a\u5355\u4e00\u7684loss\uff0c\u540c\u65f6\u5b8c\u6210proposal\u4e0eobject\u7684\u5339\u914d\u4ee5\u53ca\u56de\u5f52\u8bad\u7ec3\uff0c\u5c06\u4e2d\u95f4matching\u6b65\u9aa4\u81ea\u52a8\u5316\u4e86","title":"FreeAnchor: Learning to Match Anchors for Visual Object Detection"},{"location":"The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/#_1","text":"\u7f51\u7edc\u6b63\u5e38\u524d\u4f20\u7ed9\u51fa\u4e00\u7cfb\u5217\u7684anchor box,\u6bcf\u4e00\u4e2aanchor box\u6709\u5206\u7c7b\u4ee5\u53calocalization\u4fe1\u606f \u5bf9\u6bcf\u4e00\u4e2a\u969c\u788d\u7269\uff0c\u9009\u62e9\u4e0e\u5b83IOU\u6700\u5927\u7684n\u4e2aanchor\uff0c\u5e26\u6709\u4e00\u4e2athreshold\uff0c\u5982\u679c\u4e00\u4e2aanchor\u4e0e\u591a\u4e2a\u7269\u4f53\u5339\u914d\uff0c \u5219\u4f7f\u8fd9\u4e2aanchor\u53ea\u4e0eIOU\u6700\u5927\u90a3\u4e2a\u7269\u4f53\u5339\u914d(\u662f\u5426\u6709\u8fd9\u4e2a\u9650\u5236\u9700\u8981\u770b\u5177\u4f53\u4ee3\u7801\u5b9e\u73b0) \u5bf9\u5f53\u524d\u7684\u5339\u914d\u5206\u914d\u8ba1\u7b97\u4e00\u4e2aLoss \u53cd\u4f20\u8bad\u7ec3","title":"\u6838\u5fc3\u7b97\u6cd5"},{"location":"The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/#loss","text":"\u5148\u7ed9\u51fa\u6700\u7ec8\u7684\u516c\u5f0f \\mathcal{L}^{\\prime}(\\theta)=-w_{1} \\sum_{i} \\log \\left(\\operatorname{Mean}-\\max \\left(X_{i}\\right)\\right)+w_{2} \\sum_{j} F L_{-}\\left(P\\left\\{a_{j} \\in A_{-}\\right\\}\\left(1-\\mathcal{P}(\\theta)_{j}^{b g}\\right)\\right) \u5176\u4e2d X_{i}=\\left\\{\\mathcal{P}(\\theta)_{i j}^{c l s} \\mathcal{P}(\\theta)_{i j}^{l o c} | a_{j} \\in A_{i}\\right\\} \u4ee3\u8868\u6bcf\u4e00\u4e2a\u7269\u4f53\u5bf9\u5e94\u7684anchor bag\u7684Likelihood \uff0c\u7b2c\u4e8c\u9879\u5219\u4ee3\u8868\u6ca1\u6709\u88ab\u9009\u5165anchor bag\u7684anchor\u4f5c\u4e3abackground\u7684Focal Loss\uff0cFocal Loss\u6e90\u81ea\u4e0e \u8fd9\u7bc7\u6587\u7ae0 ,\u76f4\u89c9\u5c31\u662f\u5bf9\u4e8e\u628a\u63e1\u5df2\u7ecf\u5f88\u5927\u7684\u6b63\u786e\u7684\u70b9\u964d\u4f4e\u6743\u91cd\uff0c \u5bf9\u96be\u5ea6\u8f83\u5927\u7684\u90e8\u5206\u589e\u52a0\u5b83\u4eec\u5728cost\u4e2d\u7684\u6743\u91cd\u3002","title":"\u57fa\u4e8e\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u7684\u5339\u914dLoss"},{"location":"The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/#mean-max","text":"Mean-max\u51fd\u6570\u8868\u8fbe\u5f0f\u4e3a: \\operatorname{Mean}-\\max (X)=\\frac{\\sum_{x_{j} \\in X} \\frac{x_{j}}{1-x_{j}}}{\\sum_{x_{j} \\in X} \\frac{1}{1-x_{j}}} \u5176\u5bf9\u5e94\u7684\u51fd\u6570\u56fe\u50cf\u4e3a \u76f4\u89c9\u6765\u8bf4\uff0c\u5c31\u662f\u5f53likelihood\u51fd\u6570\u503c\u8f83\u5c0f\uff0c\u7f51\u7edc\u521a\u521d\u59cb\u5316\u7684\u65f6\u5019\uff0c\u51fd\u6570\u7ed3\u679c\u8fd1\u4f3c\u4e8e\u5404\u4e2a\u6570\u7684\u5e73\u5747\u503c\uff0c \u5f53\u6709\u90e8\u5206likelihood\u51fd\u6570\u8f83\u5927\uff0c\u7f51\u7edc\u8bad\u7ec3\u6210\u719f\u4e4b\u540e\uff0c\u51fd\u6570\u7ed3\u679c\u8fd1\u4f3c\u4e8e\u5404\u4e2a\u6570\u7684max. \u6839\u636e\u4f5c\u8005\u7684\u8bbe\u8ba1 P(\\theta)^{cls}_{ij} = e^{-L(\\theta)^{cls}_{ij}} , Loc\u540c\u7406\uff0c\u7686\u4e3a\u5bf9\u5e94\u57fa\u7840cost\u51fd\u6570\u503c\u7684\u8d1f\u6307\u6570","title":"\u7b2c\u4e00\u9879\u3001Mean-max\u4ee5\u53ca\u6982\u7387\u5b9a\u4e49"},{"location":"The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/#background","text":"P\\left\\{a_{j} \\in A_{-}\\right\\} = 1-\\max _{i} P\\left\\{a_{j} \\rightarrow b_{i}\\right\\} \u6307proposal\u6846 a_j \u4e0d\u4e0e\u6240\u6709\u7269\u4f53\u91cd\u5408\u7684\u6982\u7387 \u5176\u4e2d P\\left\\{a_{j} \\rightarrow b_{i}\\right\\}=\\text { Saturated linear }\\left(I o U_{i j}^{l o c}, t, \\max _{j}\\left(I o U_{i j}^{l o c}\\right)\\right) \\text { Saturated linear }\\left(x, t_{1}, t_{2}\\right)=\\left\\{\\begin{array}{ll}{0,} & {x \\leq t_{1}} \\\\ {\\frac{x-t_{1}}{t_{2}-t_{1}},} & {t_{1}<x<t_{2}} \\\\ {1,} & {x \\geq t_{2}}\\end{array}\\right.","title":"Background\u9879\u5b9a\u4e49"},{"location":"The_theory/LQF/","text":"LQF: Linear Quadratic Fine-Tuning \u8fd9\u7bc7paper\u63d0\u51fa\u4e00\u4e2a\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u7684\u7ebf\u6027\u8fd1\u4f3c\u6a21\u578b\u8fdb\u884cfine-tunning\u7684\u601d\u8def, \u4f7f\u5f97fine-tunnning\u7684\u8fc7\u7a0b\u4e00\u5b9a\u80fd\u6709\"\u6700\u4f18\u89e3\"\uff0c\u4e14\u4f7f\u5f97\u5404\u4e2a\u8d85\u53c2\u6570\u7684\u5f71\u54cd\u53ef\u4ee5\u89e3\u91ca. \u6a21\u578b\u7684\u7ebf\u6027\u5316 f^{lin}_w(x) = f_{w_0}(x) + \\nabla_w f_{w_0}(x) \\cdot (w-w_0) \u5c06\u5206\u7c7b\u95ee\u9898\u6539\u4e3a\u4e00\u4e2a\u56de\u5f52\u95ee\u9898\uff0c\u5373\u56de\u5f52\u4e00\u4e2aone-hot vector. \u5728\u8fd9\u4e2a\u6761\u4ef6\u4e0b\uff0c\u53ef\u4ee5\u5199\u51fa\u6743\u91cd\u7684\u95ed\u5f0f\u89e3: w^* = (J^TJ + \\lambda I)^{-1} J (Y - f_0(X))s \u7531\u4e8e\u77e9\u9635\u5927\u5c0f\u5f88\u5927\uff0c\u56e0\u800c\u6211\u4eec\u4e0d\u80fd\u76f4\u63a5\u8ba1\u7b97\u8fd9\u4e2a\u6700\u4f18\u89e3\u7684\uff0c\u6240\u4ee5\u8fd8\u662f\u9700\u8981\u4f7f\u7528\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u5904\u7406\u3002 \u4f46\u662f\u6211\u4eec\u53ef\u4ee5\u4ece\u8fd9\u4e2a\u516c\u5f0f\u4e2d\u76f4\u89c2\u7684\u4f53\u73b0\u51fa\u6bcf\u4e00\u4e2a\u8bad\u7ec3\u6837\u672c\u4ee5\u53ca\u8d85\u53c2\u6570\u5728Fine-tunning\u8fc7\u7a0b\u4e2d\u7684contribution. pre-conditioning \u7531\u4e0a\u5f0f\uff0c\u6574\u4e2a\u95ee\u9898\u53d8\u6210\u4e86\u4e00\u4e2a\u51f8\u4f18\u5316\u95ee\u9898\uff0c\u4f7f\u7528SGD\u6211\u4eec\u80fd\u786e\u4fdd\u7cfb\u7edf\u6700\u7ec8\u80fd\u6536\u655b\u5230\u5168\u5c40\u6700\u4f18\uff0c\u4f46\u662f\u7531\u4e8e\u66f2\u7387\u7684\u53d8\u5316\uff0c\u6536\u655b\u901f\u5ea6\u662f\u4e0d\u7a33\u5b9a\u7684\u3002\u8fd9\u4e2a\u6536\u655b\u901f\u5ea6\u53d6\u51b3\u4e8ehessian \u77e9\u9635\u7684\u5927\u5c0f\u7279\u5f81\u503c\u7684\u6bd4\u4f8b\u3002 \u4f7f\u7528\u9ad8\u65af\u725b\u987f\u7b97\u6cd5\uff0c\u76f4\u63a5\u8003\u8651\u4e8c\u9636\u4fe1\u606f\u80fd\u5927\u5927\u63d0\u5347\u6536\u655b\u901f\u5ea6\uff0c\u9ad8\u65af\u725b\u987f\u7b97\u6cd5\u7684\u516c\u5f0f w_{t+1} = w_t - H^{-1}(X) G(X) \u672c\u6587\u4f7f\u7528\"pre-conditioning\"\u8fd9\u4e2a\u672f\u8bed\uff0c\u6280\u672f\u4e0a\u4f7f\u7528 Fisher Information Matrix \u8fd1\u4f3c\u4e8c\u9636\u5bfc\u3002","title":"LQF: Linear Quadratic Fine-Tuning"},{"location":"The_theory/LQF/#lqf-linear-quadratic-fine-tuning","text":"\u8fd9\u7bc7paper\u63d0\u51fa\u4e00\u4e2a\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u7684\u7ebf\u6027\u8fd1\u4f3c\u6a21\u578b\u8fdb\u884cfine-tunning\u7684\u601d\u8def, \u4f7f\u5f97fine-tunnning\u7684\u8fc7\u7a0b\u4e00\u5b9a\u80fd\u6709\"\u6700\u4f18\u89e3\"\uff0c\u4e14\u4f7f\u5f97\u5404\u4e2a\u8d85\u53c2\u6570\u7684\u5f71\u54cd\u53ef\u4ee5\u89e3\u91ca.","title":"LQF: Linear Quadratic Fine-Tuning"},{"location":"The_theory/LQF/#_1","text":"f^{lin}_w(x) = f_{w_0}(x) + \\nabla_w f_{w_0}(x) \\cdot (w-w_0) \u5c06\u5206\u7c7b\u95ee\u9898\u6539\u4e3a\u4e00\u4e2a\u56de\u5f52\u95ee\u9898\uff0c\u5373\u56de\u5f52\u4e00\u4e2aone-hot vector. \u5728\u8fd9\u4e2a\u6761\u4ef6\u4e0b\uff0c\u53ef\u4ee5\u5199\u51fa\u6743\u91cd\u7684\u95ed\u5f0f\u89e3: w^* = (J^TJ + \\lambda I)^{-1} J (Y - f_0(X))s \u7531\u4e8e\u77e9\u9635\u5927\u5c0f\u5f88\u5927\uff0c\u56e0\u800c\u6211\u4eec\u4e0d\u80fd\u76f4\u63a5\u8ba1\u7b97\u8fd9\u4e2a\u6700\u4f18\u89e3\u7684\uff0c\u6240\u4ee5\u8fd8\u662f\u9700\u8981\u4f7f\u7528\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u5904\u7406\u3002 \u4f46\u662f\u6211\u4eec\u53ef\u4ee5\u4ece\u8fd9\u4e2a\u516c\u5f0f\u4e2d\u76f4\u89c2\u7684\u4f53\u73b0\u51fa\u6bcf\u4e00\u4e2a\u8bad\u7ec3\u6837\u672c\u4ee5\u53ca\u8d85\u53c2\u6570\u5728Fine-tunning\u8fc7\u7a0b\u4e2d\u7684contribution.","title":"\u6a21\u578b\u7684\u7ebf\u6027\u5316"},{"location":"The_theory/LQF/#pre-conditioning","text":"\u7531\u4e0a\u5f0f\uff0c\u6574\u4e2a\u95ee\u9898\u53d8\u6210\u4e86\u4e00\u4e2a\u51f8\u4f18\u5316\u95ee\u9898\uff0c\u4f7f\u7528SGD\u6211\u4eec\u80fd\u786e\u4fdd\u7cfb\u7edf\u6700\u7ec8\u80fd\u6536\u655b\u5230\u5168\u5c40\u6700\u4f18\uff0c\u4f46\u662f\u7531\u4e8e\u66f2\u7387\u7684\u53d8\u5316\uff0c\u6536\u655b\u901f\u5ea6\u662f\u4e0d\u7a33\u5b9a\u7684\u3002\u8fd9\u4e2a\u6536\u655b\u901f\u5ea6\u53d6\u51b3\u4e8ehessian \u77e9\u9635\u7684\u5927\u5c0f\u7279\u5f81\u503c\u7684\u6bd4\u4f8b\u3002 \u4f7f\u7528\u9ad8\u65af\u725b\u987f\u7b97\u6cd5\uff0c\u76f4\u63a5\u8003\u8651\u4e8c\u9636\u4fe1\u606f\u80fd\u5927\u5927\u63d0\u5347\u6536\u655b\u901f\u5ea6\uff0c\u9ad8\u65af\u725b\u987f\u7b97\u6cd5\u7684\u516c\u5f0f w_{t+1} = w_t - H^{-1}(X) G(X) \u672c\u6587\u4f7f\u7528\"pre-conditioning\"\u8fd9\u4e2a\u672f\u8bed\uff0c\u6280\u672f\u4e0a\u4f7f\u7528 Fisher Information Matrix \u8fd1\u4f3c\u4e8c\u9636\u5bfc\u3002","title":"pre-conditioning"},{"location":"The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/","text":"Localization-aware Channel Pruning for Object Detection \u8fd9\u7bc7\u8bba\u6587\u5c1d\u8bd5\u89e3\u51b3\u7684\u662f\u9488\u5bf92D\u7269\u4f53\u68c0\u6d4b\u7684 channel pruning \u95ee\u9898.localization-aware\u7684\u52a8\u673a\u8d77\u6e90\u4e8e DCP.pdf , \u539fchannel prunning\u7684\u95ee\u9898 \u7684\u505a\u6cd5\u662f\u9009\u62e9channel\u5e76\u7ebf\u6027\u91cd\u5efa\uff0c\u51cf\u5c11\u8f93\u51fa\u7684l2\u53d8\u5316\uff0c\u95ee\u9898\u5212\u5f52\u4e3alasso regression.\u4f46\u662fchannel\u4e2d\u6709\u5f88\u591a\u662f\u5197\u4f59\u7684\uff0c\u8f93\u51fa\u4e5f\u662f\u5197\u4f59\u7684\uff0c\u5b8c\u6574\u7684\u590d\u539f\u5e76\u4e0d\u4e00\u5b9a\u662f\u6700\u6709\u6548\u7684\uff0cDCP\u90a3\u7bc7\u5219\u662f\u57fa\u4e8e\u8fd9\u4e2a\u95ee\u9898\u5bf9\u5206\u7c7b\u95ee\u9898\u8fdb\u884c\u4f18\u5316\uff0c\u672c\u6587\u5219\u662f\u57fa\u4e8e\u8fd9\u4e2a\u95ee\u9898\uff0c\u5728object detection(\u4e3b\u8981\u57fa\u4e8eSSD\u7b97\u6cd5)\u4e2d\uff0c\u9488\u5bf9\u4f4d\u7f6e\u8f93\u51fa\u8fdb\u884c\u4f18\u5316\u3002\u8fc7\u7a0b\u4e2d\u63d0\u51fa\u4e86 contextual ROIAlign\u5c42. \u672c\u6587\u5efa\u8bae\u5728\u4f7f\u7528\u8fd9\u7bc7\u6587\u7ae0\u4e4b\u524d\u5148\u8865\u5145 DCP.pdf \u603b\u4f53pipeline Contextual ROIAlign Layer default bounding box\u4e0d\u4e00\u5b9a\u5305\u542b\u8db3\u591f\u7684\u8bed\u4e49\u4fe1\u606f\uff0c\u6240\u4ee5\u9700\u8981\u66f4\u5927\u7684\u6846\u6846. \\mathcal{F_O} = ROIAlign(\\mathcal{F_B}) + ROIAlign(\\mathcal{F_C}) \u5176\u4e2d A, B \u5206\u522b\u4e3aGroudTruth/Proposal box, C \u4e0e A, B \u7684\u5173\u7cfb\u5982\u56fe Channel Pruning Loss \u4f5c\u8005\u7684\u601d\u8def\u662f\u5f62\u6210\u4e00\u4e2a\u4e3a\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u7684loss\uff0c\u4ece\u800c\u53ef\u4ee5\u7528\u8fd9\u4e2a\u9644\u52a0\u7f51\u7edc\u7684\u68af\u5ea6\u8f85\u52a9\u6a21\u578b\u4fee\u526a\u3002 \u8fd9\u91cc\u7528 G \u6307\u4ee3 Generalized IoU \\begin{aligned} GIoU_{AB} &= IoU_{AB} - \\frac{C-U}{C} \\\\ U &= A + B - IoU_{AB} \\end{aligned} \u63a5\u7740\u5b9a\u4e49 E \u4e3a\u4ea4\u53c9\u71b5\uff0c \\mathcal{L_{ac}} \u4e3a\u9644\u52a0\u7f51\u7edc\u7684\u5206\u7c7b\u635f\u5931, \\mathcal{L_{ar}} \u4e3a\u9644\u52a0\u7f51\u8def\u7684\u5b9a\u4f4d\u635f\u5931\u3002 \\begin{aligned} \\mathcal{L}_{ac} &= \\sum_i E_i \\\\ \\mathcal{L}_{ar} &= \\sum_i m(1 - G_i) \\\\ \\mathcal{L_a} = \\mathcal{L}_{ac} + \\mathcal{L}_{ar} \\end{aligned} \u5176\u4e2d m \u4e3a\u6743\u91cd\u53c2\u6570\u3002 Localization-aware Channel Pruning \u603b\u4f53\u7b97\u6cd5\u6982\u8ff0 \u7b2c\u4e09\u884c \\mathcal{L_f} = \\mathcal{L_a} + \\mathcal{L_c} + \\mathcal{L_r} \u6307\u4ee3\u622a\u6b62\u5230\u73b0\u5728\u4fee\u526a\u540e\u7684loss\uff0c \u7b2c\u56db\u884c\u6307\u5bf9\u9644\u52a0\u7f51\u7edc\u548c\u4fee\u526a\u5230\u7b2ci\u5c42\u7684model\u8fdb\u884c\u8bad\u7ec3 \u7b2c\u4e94\u884c: \\begin{aligned} \\mathcal{L_{re}} &= \\frac{1}{2Q} ||F - X * W_C||^2_2 \\\\ \\mathcal{L}(W_C) &= \\mathcal{L}_{re}(W_C) + \\alpha\\mathcal{L_a}(W_C) \\\\ ||C||_0 &\\le K \\end{aligned} \\mathcal{L}_{re} \u4e3a\u91cd\u5efa\u8bef\u5dee \u7b2c\u516d\u884c\u6307\uff0c\u6c42\u51fa \\mathcal{L} \u5173\u4e8e\u5f53\u524d\u5c42W\u7684\u68af\u5ea6\u3002 S_k = \\sum^H_{i=1} \\sum^W_{j=1} ||\\frac{\\partial\\mathcal{L}}{\\partial W_{k,i,j}}||^2_2 \u6307\u4ee3\u7b2c k \u8f93\u51fachannel\u7684\u68af\u5ea6\u5747\u65b9\u548c\uff0c\u53ea\u4fdd\u7559\u68af\u5ea6\u6700\u5927\u7684\u51e0\u4e2achannel \u7b2c\u4e03\u884c\u6307 W_C = W_C - \\gamma\\frac{\\partial\\mathcal{L}}{\\partial W_C}","title":"Localization-aware Channel Pruning for Object Detection"},{"location":"The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/#localization-aware-channel-pruning-for-object-detection","text":"\u8fd9\u7bc7\u8bba\u6587\u5c1d\u8bd5\u89e3\u51b3\u7684\u662f\u9488\u5bf92D\u7269\u4f53\u68c0\u6d4b\u7684 channel pruning \u95ee\u9898.localization-aware\u7684\u52a8\u673a\u8d77\u6e90\u4e8e DCP.pdf , \u539fchannel prunning\u7684\u95ee\u9898 \u7684\u505a\u6cd5\u662f\u9009\u62e9channel\u5e76\u7ebf\u6027\u91cd\u5efa\uff0c\u51cf\u5c11\u8f93\u51fa\u7684l2\u53d8\u5316\uff0c\u95ee\u9898\u5212\u5f52\u4e3alasso regression.\u4f46\u662fchannel\u4e2d\u6709\u5f88\u591a\u662f\u5197\u4f59\u7684\uff0c\u8f93\u51fa\u4e5f\u662f\u5197\u4f59\u7684\uff0c\u5b8c\u6574\u7684\u590d\u539f\u5e76\u4e0d\u4e00\u5b9a\u662f\u6700\u6709\u6548\u7684\uff0cDCP\u90a3\u7bc7\u5219\u662f\u57fa\u4e8e\u8fd9\u4e2a\u95ee\u9898\u5bf9\u5206\u7c7b\u95ee\u9898\u8fdb\u884c\u4f18\u5316\uff0c\u672c\u6587\u5219\u662f\u57fa\u4e8e\u8fd9\u4e2a\u95ee\u9898\uff0c\u5728object detection(\u4e3b\u8981\u57fa\u4e8eSSD\u7b97\u6cd5)\u4e2d\uff0c\u9488\u5bf9\u4f4d\u7f6e\u8f93\u51fa\u8fdb\u884c\u4f18\u5316\u3002\u8fc7\u7a0b\u4e2d\u63d0\u51fa\u4e86 contextual ROIAlign\u5c42. \u672c\u6587\u5efa\u8bae\u5728\u4f7f\u7528\u8fd9\u7bc7\u6587\u7ae0\u4e4b\u524d\u5148\u8865\u5145 DCP.pdf","title":"Localization-aware Channel Pruning for Object Detection"},{"location":"The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/#pipeline","text":"","title":"\u603b\u4f53pipeline"},{"location":"The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/#contextual-roialign-layer","text":"default bounding box\u4e0d\u4e00\u5b9a\u5305\u542b\u8db3\u591f\u7684\u8bed\u4e49\u4fe1\u606f\uff0c\u6240\u4ee5\u9700\u8981\u66f4\u5927\u7684\u6846\u6846. \\mathcal{F_O} = ROIAlign(\\mathcal{F_B}) + ROIAlign(\\mathcal{F_C}) \u5176\u4e2d A, B \u5206\u522b\u4e3aGroudTruth/Proposal box, C \u4e0e A, B \u7684\u5173\u7cfb\u5982\u56fe","title":"Contextual ROIAlign Layer"},{"location":"The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/#channel-pruning-loss","text":"\u4f5c\u8005\u7684\u601d\u8def\u662f\u5f62\u6210\u4e00\u4e2a\u4e3a\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u7684loss\uff0c\u4ece\u800c\u53ef\u4ee5\u7528\u8fd9\u4e2a\u9644\u52a0\u7f51\u7edc\u7684\u68af\u5ea6\u8f85\u52a9\u6a21\u578b\u4fee\u526a\u3002 \u8fd9\u91cc\u7528 G \u6307\u4ee3 Generalized IoU \\begin{aligned} GIoU_{AB} &= IoU_{AB} - \\frac{C-U}{C} \\\\ U &= A + B - IoU_{AB} \\end{aligned} \u63a5\u7740\u5b9a\u4e49 E \u4e3a\u4ea4\u53c9\u71b5\uff0c \\mathcal{L_{ac}} \u4e3a\u9644\u52a0\u7f51\u7edc\u7684\u5206\u7c7b\u635f\u5931, \\mathcal{L_{ar}} \u4e3a\u9644\u52a0\u7f51\u8def\u7684\u5b9a\u4f4d\u635f\u5931\u3002 \\begin{aligned} \\mathcal{L}_{ac} &= \\sum_i E_i \\\\ \\mathcal{L}_{ar} &= \\sum_i m(1 - G_i) \\\\ \\mathcal{L_a} = \\mathcal{L}_{ac} + \\mathcal{L}_{ar} \\end{aligned} \u5176\u4e2d m \u4e3a\u6743\u91cd\u53c2\u6570\u3002","title":"Channel Pruning Loss"},{"location":"The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/#localization-aware-channel-pruning","text":"\u603b\u4f53\u7b97\u6cd5\u6982\u8ff0 \u7b2c\u4e09\u884c \\mathcal{L_f} = \\mathcal{L_a} + \\mathcal{L_c} + \\mathcal{L_r} \u6307\u4ee3\u622a\u6b62\u5230\u73b0\u5728\u4fee\u526a\u540e\u7684loss\uff0c \u7b2c\u56db\u884c\u6307\u5bf9\u9644\u52a0\u7f51\u7edc\u548c\u4fee\u526a\u5230\u7b2ci\u5c42\u7684model\u8fdb\u884c\u8bad\u7ec3 \u7b2c\u4e94\u884c: \\begin{aligned} \\mathcal{L_{re}} &= \\frac{1}{2Q} ||F - X * W_C||^2_2 \\\\ \\mathcal{L}(W_C) &= \\mathcal{L}_{re}(W_C) + \\alpha\\mathcal{L_a}(W_C) \\\\ ||C||_0 &\\le K \\end{aligned} \\mathcal{L}_{re} \u4e3a\u91cd\u5efa\u8bef\u5dee \u7b2c\u516d\u884c\u6307\uff0c\u6c42\u51fa \\mathcal{L} \u5173\u4e8e\u5f53\u524d\u5c42W\u7684\u68af\u5ea6\u3002 S_k = \\sum^H_{i=1} \\sum^W_{j=1} ||\\frac{\\partial\\mathcal{L}}{\\partial W_{k,i,j}}||^2_2 \u6307\u4ee3\u7b2c k \u8f93\u51fachannel\u7684\u68af\u5ea6\u5747\u65b9\u548c\uff0c\u53ea\u4fdd\u7559\u68af\u5ea6\u6700\u5927\u7684\u51e0\u4e2achannel \u7b2c\u4e03\u884c\u6307 W_C = W_C - \\gamma\\frac{\\partial\\mathcal{L}}{\\partial W_C}","title":"Localization-aware Channel Pruning"},{"location":"The_theory/NIPS2020_networks/","text":"NIPS 2020 for Experimental NN \u672c\u6587\u6536\u96c6\u51e0\u7bc7NIPS2020\u4e2d\u5927\u5b9e\u9a8c\u5ba4\u4e2d\u901a\u8fc7\u5b9e\u9a8c\u7814\u7a76\u795e\u7ecf\u7f51\u7edc\u76f8\u5173\u6027\u8d28\u7684\u6587\u7ae0\u3002\u76f8\u5173\u5b9e\u9a8c\u4e0d\u4e00\u5b9a\u6709\u4e25\u683c\u7684\u7406\u8bba\u63a8\u7406\u6216\u662f\u5b9e\u7528\u6027\u7684\u521b\u65b0\uff0c\u4f46\u662f\u53ef\u4ee5\u4e86\u89e3\u76f8\u5f53\u6709\u610f\u4e49\u7684\u7ecf\u9a8c\u7ed3\u8bba. NIPS 2020 for Experimental NN Batch Normalization Biases Residual Blocks Towards the Identity Function in Deep Networks Neural Networks Fail to Learn Periodic Functions and How to Fix It \u4e0eSIREN \u5bf9\u6bd4 On Warm-Starting Neural Network Training Rational Neural Network What do Neural Network Learn when Trained with Random Labels \u5bf9\u4e8e\u9ad8\u65af\u8f93\u5165\u7684Alignment \u5b9e\u9a8c An Analysis of SVD for Deep Rotation Estimation Is normalization indispensable for training deep neural networks Batch Normalization Biases Residual Blocks Towards the Identity Function in Deep Networks pdf \u8fd9\u7bc7DeepMind\u7684paper \u63a2\u7d22\u4e86BatchNorm\u4e0eResNet\u4e4b\u95f4\u7684\u4e00\u4e9b\u5173\u7cfb\u3002 \u4e0d\u96be\u8bc1\u660e\u7684\u6570\u5b66\u7ed3\u8bba: resnet\u7684variance\u516c\u5f0f, Var(x_i^{l+1}) = Var(x_i^l) + Var(f^l_i(x^l)) \u5bf9\u4e8e\u6ca1\u6709batchnorm\u7684\u7f51\u7edc\uff0c\u5176variance\u4e3a Var(x_i^{l+1}) = 2 * Var(x_i^l) = 2^l (\u4e24\u4e2a\u76f8\u4e92\u72ec\u7acb\u7684\u9ad8\u65af\u5206\u5e03\u8f93\u51fa variance\u4e3a\u4e24\u8005\u7684\u76f4\u63a5\u52a0\u548c), \u521d\u59cbvariance\u4f1a\u968f\u7740resnet\u6df1\u5ea6\u53d1\u6563\u3002\u5982\u679c\u5bf9\u52a0\u548c\u8f93\u51fascale down 1 / \\sqrt{2} , \u5219\u53ef\u4ee5\u4fdd\u6301variance\u4e0d\u53d8\u3002 \u5bf9\u4e8e\u8f93\u51fa\u524d\u6709batchnorm\u7684\u7f51\u7edc\uff0c\u5176variance \u4e3a Var(x_i^{l+1}) = Var(x_i^l) + 1 \\approx l . \u53ef\u4ee5\u53d1\u73b0\u7684\u662f\uff0c\u968f\u7740\u7f51\u7edc\u5c42\u6570 l \u7684\u589e\u5927\uff0c\u4e3b\u5e72\u7f51\u7edc\u4e0e\u6b8b\u5dee\u5206\u652f\u4e4b\u95f4\u7684variance \u6bd4\u4f8b\u4f1a\u8d8a\u6765\u8d8a\u5927\uff0c\u6b8b\u5dee\u5206\u652f\u4e0a\u7684batchnorm\u4f1a\u538b\u5236\u5176\u6570\u503c\u3002\u56e0\u800c\u4ece\u6570\u5b66\u4e0a\u53ef\u4ee5\u53d1\u73b0\u968f\u7740resnet\u6df1\u5ea6\u7684\u63d0\u5347\uff0c\u7f51\u7edc\u4f1a\u8d8b\u8fd1\u4e8eidentity function\u3002\u8fd9\u4f1a\u4f7f\u5f97\u7f51\u7edc\u521d\u59cb\u5316\u540e\u66f4\u5bb9\u6613\u8bad\u7ec3\u3002 \u4f5c\u8005\u63d0\u51fa\u4e86SkipInit \u6a21\u4eff\u4e86batchnorm\u7684\u7279\u6027 \u5b9e\u9a8c\u4e0a\u8bc1\u660e\u4e86\u5176variance\u7279\u6027\uff0c\u4ee5\u53caskipinit\u540c\u6837\u80fd\u8ba9\u6df1\u5ea6\u7684\u7f51\u7edc\u66f4\u5bb9\u6613\u8bad\u7ec3\u4e86\u3002 \u63a5\u7740\u4f5c\u8005\u4ece\u5b9e\u9a8c\u7684\u89d2\u5ea6\u5206\u6790\u4e86\u4e24\u4e2aBN\u76f8\u5173\u7684\u7ed3\u8bba. BN\u5141\u8bb8\u7f51\u7edc\u4f7f\u7528\u66f4\u5927\u7684\u5b66\u4e60\u7387\uff0c\u5b9e\u9a8c\u4e2d\u53d1\u73b0BN\u7f51\u7edc\u7684\u6700\u5927\u7a33\u5b9a\u5b66\u4e60\u7387\u6bd4skipinit\u66f4\u5927\u3002\u4f46\u662f\u5982\u679cbatchsize\u6bd4\u8f83\u5c0f\u7684\u8bdd\uff0c\u7f51\u7edc\u7684\u6027\u80fd\u4e5f\u5e76\u4e0d\u4f1a\u56e0\u6b64\u53d7\u76ca\u3002\u5982\u679c\u56fa\u5b9a\u4e0b\u8bad\u7ec3\u7684epoch\u6570(\u56fa\u5b9a\u8bad\u7ec3FLOPS\uff0c\u6539\u53d8batch_size\u4f1a\u6539\u53d8\u8fed\u4ee3\u6b21\u6570;\u5982\u679c\u56fa\u5b9a\u7684\u662fSteps\u5219\u5728\u5927batch\u7684\u60c5\u51b5\u4e0b\u8bad\u7ec3\u8fd0\u7b97\u91cf\u4e5f\u4f1a\u5927\u5e45\u63d0\u5347), \u90a3\u4e48\u6700\u4f18\u7684\u7ed3\u679c\u5176\u5b9e\u8fd8\u662f\u4f7f\u7528\u5c0fbatch size \u5c0flearning rate.\u56e0\u6b64\u4f5c\u8005\u8ba4\u4e3aBN\u5141\u8bb8\u7f51\u7edc\u4f7f\u7528\u66f4\u5927\u7684\u5b66\u4e60\u7387\u53ea\u662f\u8ba9\u5b66\u4e60\u7387\u53c2\u6570\u76f8\u5bf9\u66f4\u5bb9\u6613\u8c03\u4e86\uff0c\u800c\u5e76\u4e0d\u4f1a\u63d0\u5347\u7f51\u7edc\u6700\u7ec8\u7684\u6027\u80fd. BN\u4f1a\u63d0\u4f9b\u4e00\u4e2aregularization\u3002\u5b9e\u9a8c\u53d1\u73b0\u8fd9\u662f\u4e8b\u5b9e\u5b58\u5728\u7684;\u901a\u8fc7skipInit + dropout\u6216\u8005\u5176\u4ed6regularization\u624b\u6bb5\uff0c\u53ef\u4ee5\u5728\u4f4ebatch\u7684\u60c5\u51b5\u4e0b\u5f97\u5230\u6bd4BN\u66f4\u597d\u7684\u7ed3\u679c(BN\u5728\u4f4ebatch size\u7684\u60c5\u51b5\u4e0b\u53d1\u6325\u4e0d\u597d). \u603b\u7ed3\u8d77\u6765 BN\u4f1a\u8ba9ResNet\u66f4\u50cfidentity function\u6765\u4f7f\u5f97\u6df1\u7f51\u7edc\u66f4\u53ef\u8bad\u7ec3\u3002\u8fd9\u53ef\u4ee5\u7528skipInit\u6a21\u62df\u3002 BN\u8ba9\u7f51\u7edc\u7528\u66f4\u5927\u7684\u5b66\u4e60\u7387\u8bad\u7ec3\uff0c\u4e14\u5728\u5927batch\u60c5\u51b5\u4e0b\u66f4\u4e3a\u660e\u663e\u3002\u4f46\u662f\u8fd9\u5e76\u4e0d\u662f\u63d0\u5347\u6700\u4f18\u6027\u80fd\u7684\u6839\u672c\u539f\u56e0\u3002 BN\u8ba9\u7f51\u7edc\u83b7\u5f97\u4e00\u4e2a\u5185\u5728\u7684regularization. \u5728\u5927batch\u7684\u60c5\u51b5\u4e0b\u5f97\u5230\u5f88\u597d\u7684performance. \u800c\u4f5c\u8005\u6307\u51fa\u4f7f\u7528skipinit + \u5408\u9002\u7684regularization\u53ef\u4ee5\u5728\u5c0fbatch\u7684\u60c5\u51b5\u540c\u65f6\u8ba9\u7f51\u7edctrainable\u3001regularized\u3001\u4e14well performing. Neural Networks Fail to Learn Periodic Functions and How to Fix It pdf \u672c\u6587\u6307\u51fa\u4f7f\u7528\"ReLU\"\u662f\u8f93\u51fa\u5206\u6bb5\u7ebf\u6027\u51fd\u6570\u662f\u65e0\u6cd5\u62df\u5408\u5468\u671f\u51fd\u6570\u7684, \u4f5c\u8005\u63d0\u51fa\u4f7f\u7528 x + sin^2(x) \u4f5c\u4e3a\u6fc0\u6d3b\u51fd\u6570\uff0c\u8fd9\u4e2a\u51fd\u6570\u53ef\u4ee5\u6269\u5c55\u4e3a x + \\frac{1}{a}sin^2(ax) \u7531\u56fe\u7247\uff0c\u4f5c\u8005\u79f0\u8fd9\u4e2a\u4e3a Snake activation. \u5b9e\u9a8c\u6765\u8bf4\u8fd9\u4e2a\u51fd\u6570\u53ef\u4ee5\u5728\u7ecf\u5178\u95ee\u9898\u4e0a\u5f97\u5230\u4e0eReLU\u7b49\u6fc0\u6d3b\u51fd\u6570\u4e00\u81f4\u7684\u6027\u80fd\u3002\u4f46\u662f\u8fd9\u4e2a\u51fd\u6570\u6709\u66f4\u5f3a\u7684\u5ef6\u62d3\u6027\u3002\u4f7f\u7528\u6709\u9650\u7684\u51fd\u6570\u62df\u5408 y = sin(x) \u51fd\u6570\uff0c\u5982\u679c\u4f7f\u7528ReLU\u6216\u8005Tanh\uff0c\u5219\u5f53\u8f93\u5165\u51fd\u6570\u8d85\u51fa\u8bad\u7ec3\u8303\u56f4\u65f6\uff0cReLU\u4e0eTanh\u7406\u6240\u5e94\u5f53\u5730\u4e0d\u80fd\u590d\u73b0\u76ee\u6807\u51fd\u6570\u5468\u671f\u6027\u7684\u7279\u70b9\uff0c\u800c\u4f5c\u8005\u63d0\u51fa\u7684\u51fd\u6570 x + sin^2(x) \u53ef\u4ee5\u3002 \u4f5c\u8005\u8bc1\u660e\u4e86\uff0c\u5bf9\u4e8e\u7531ReLU\u7ec4\u6210\u7684(\u591a\u5c42)\u524d\u9988\u7f51\u7edc\uff0c\u5f53\u8f93\u5165\u5f3a\u5ea6(norm)\u65e0\u9650\u903c\u8fd1\u65e0\u7a77\u5927\u65f6\uff0c\u51fd\u6570\u7684\u8f93\u51fa\u503c\u4f1a\u903c\u8fd1\u4e8e\u8f93\u5165\u7684\u7ebf\u6027\u63d2\u503c. \u5bf9\u4e8eTanh\u7f51\u7edc\uff0c\u51fd\u6570\u8f93\u51fa\u503c\u5219\u4f1a\u903c\u8fd1\u4e8e\u5e38\u6570. \\underset{z\\rightarrow\\infty}{lim} ||f_{ReLU}(zu) - zW_u u - b_u||_2 = 0 \\underset{z\\rightarrow\\infty}{lim} ||f_{tanh}(zu) - v_u||_2 = 0 \u76f4\u89c9\u4e0a\u6765\u8bf4\uff0cReLU\u7f51\u7edc\u7684\u8f93\u51fa\u6700\u7ec8\u662f\u5206\u6bb5\u7ebf\u6027\u63d2\u503c\uff0c\u56e0\u800c\u5f53\u65e0\u7a77\u5916\u62d3\u7684\u65f6\u5019\uff0c\u6700\u7ec8\u4f1a\u6536\u655b\u4e8e\u4e00\u4e2a\u7ebf\u6027\u51fd\u6570. Tanh\u5f53\u65e0\u7a77\u5916\u62d3\u7684\u65f6\u5019\uff0c\u7531\u4e8e\u6fc0\u6d3b\u51fd\u6570\u7684\u503c\u7684\u9650\u5236\uff0c\u7b2c\u4e00\u6b21\u6fc0\u6d3b\u540e\u7684\u8f93\u51fa\u503c\u4f1a\u8d8b\u8fd1\u4e8e\u5e38\u6570, \u591a\u5c42\u8fd0\u7b97\u7684\u7ed3\u679c\u4e5f\u90fd\u4f1a\u8d8b\u8fd1\u4e8e\u4e00\u4e2a\u5e38\u6570\u3002 \u7136\u540e\u4f5c\u8005\u5f15\u5165\u4e86\u4e00\u4e2a\u8d85\u8d8a \u4e07\u80fd\u8fd1\u4f3c(universal approximation, UA)\u7684\u6027\u8d28, \u4e07\u80fd\u5ef6\u62d3(universal extrapolation, UE). UA\u5728\u795e\u7ecf\u7f51\u7edc\u9886\u57df\u6307\u7684\u662f:\u5df2\u77e5\u5b9a\u4e49\u5728\u6709\u754c\u533a\u95f4\u7684\u4efb\u610f\u8fde\u7eed\u51fd\u6570 f(x) , \u4e00\u4e2a\u5355\u9690\u85cf\u5c42\u7684\u795e\u7ecf\u7f51\u7edc\uff0c\u5728\u5bbd\u5ea6\u8d8b\u8fd1\u4e8e\u65e0\u7a77\u5927\u7684\u65f6\u5019\uff0c\u603b\u53ef\u4ee5\u627e\u5230\u4e00\u7ec4\u6743\u91cd w_N , \u4f7f\u5f97\u7f51\u7edc\u7684\u8f93\u51fa\u5728\u6709\u754c\u8303\u56f4\u5185\u5904\u5904\u903c\u8fd1\u8fd9\u4e00\u51fd\u6570. \u672c\u6587UE\u7684\u5219\u662f: \u5df2\u77e5\u5b9a\u4e49\u5728\u65e0\u7a77\u8303\u56f4\u5185\u7684\u4efb\u610f\u5468\u671f\u51fd\u6570 f(x) .\u4e00\u4e2a\u4ee5 x+sin^2(x) \u4e3a\u6fc0\u6d3b\u51fd\u6570\u7684\u5355\u9690\u85cf\u5c42\u795e\u7ecf\u7f51\u7edc\uff0c\u5728\u5bbd\u5ea6\u8d8b\u8fd1\u4e8e\u65e0\u7a77\u5927\u65f6\uff0c\u603b\u53ef\u4ee5\u627e\u5230\u4e00\u7ec4\u6743\u91cd w_N ,\u4f7f\u5f97\u7f51\u7edc\u7684\u8f93\u51fa\u5728\u6574\u4e2a\u5b9a\u4e49\u57df\u5185\u903c\u8fd1\u8fd9\u4e00\u4e2a\u51fd\u6570 f(x) . \u4e14\u5982\u679c\u76ee\u6807\u51fd\u6570\u662f\u8fde\u7eed\u7684\uff0c\u90a3\u4e48\u53ef\u4ee5\u505a\u5230\u5904\u5904\u903c\u8fd1. UE\u662f\u6bd4UA\u66f4\u5f3a\u7684\u7ed3\u8bba\uff0c\u663e\u7136UE\u53ef\u4ee5\u5305\u542bUA. \u4f5c\u8005\u7684\u8bc1\u660e\u601d\u8def\u662f\u9996\u5148\u4f7f\u7528\u5085\u91cc\u53f6\u53d8\u6362\uff0c\u8bc1\u660e\u4e86\u4f7f\u7528 sin(x) \u4f5c\u4e3a\u6fc0\u6d3b\u51fd\u6570\uff0c\u4e24\u5c42\u7f51\u7edc\u53ef\u4ee5\u62df\u5408\u4efb\u610f\u5468\u671f\u51fd\u6570\u3002\u7136\u540e\u53c8\u8bc1\u660e\u4e86\u4e24\u4e2a x + sin^2(x) \u6fc0\u6d3b\u7684\u795e\u7ecf\u5143\u53ef\u4ee5\u8868\u8fbe\u4e00\u4e2a sin(x) \u6216 cos(x) \u51fd\u6570. \u4e24\u8005\u7ed3\u5408\u5c31\u8bc1\u660e\u4e86 UE . \u5728\u521d\u59cb\u5316\u95ee\u9898\u4e0a\uff0c\u7531\u4e8e Snake \u6fc0\u6d3b\u51fd\u6570\u5728\u96f6\u9644\u8fd1\u8fd1\u4f3c\u4e8eidentity matrix. \u8981\u8ba9variance expectation\u4e3a 1\uff0c \u4f5c\u8005\u7ed9\u51fa\u7684\u521d\u59cb\u5316\u65b9\u6848\u4e3a Uniform(-\\sqrt{3/d}, +\\sqrt{3/d}). \u4f5c\u8005\u5728\u8003\u8651\u591a\u4e24\u9879\u9ad8\u9636\u91cf\u7684\u60c5\u51b5\u4e0b\u4e5f\u7ed9\u51fa\u4e86\u5bf9\u5e94 Var=1$\u65f6\u7684\u89e3\u6790\u89e3\uff0c\u6027\u80fd\u4f1a\u6709\u4e00\u5b9a\u7684\u63d0\u5347\uff0c\u4f46\u662f\u4f5c\u8005\u4e5f\u6307\u51fa\u5b9e\u7528\u4e0a\u6ca1\u6709\u5fc5\u8981. \u5728a\u7684\u9009\u62e9\u4e0a\uff0c\u5982\u679c\u95ee\u9898\u6ca1\u6709\u663e\u7136\u7684\u5468\u671f\u6027\uff0c\u5219 a = 0.5 \u6ca1\u6709\u95ee\u9898\uff0c\u5982\u679c\u95ee\u9898\u6709\u663e\u7136\u7684\u5468\u671f\u6027\uff0c\u5219\u53ef\u4ee5\u8bbe\u7f6e a \\in [5, 50] . \u4f5c\u8005\u4e5f\u8ba8\u8bba\u4e86\u628a a \u8bbe\u7f6e\u4e3a\u53ef\u5b66\u4e60\u7684\u53c2\u6570\u7684\u53ef\u80fd\u6027.\u6027\u80fd\u4e5f\u662f\u6ca1\u6709\u95ee\u9898\u7684. \u5df2\u7ecf\u6709\u8fd9\u4e2a\u51fd\u6570\u7684 tensorflow addon implementation on github \u4e0eSIREN \u5bf9\u6bd4 SIREN SIREN\u7684\u95ee\u9898\u5728\u4e8e\u5b83\u662f\u6709\u754c\u7684, \u7f3a\u5c11\u4e86 x \u8fd9\u4e00\u9879\u3002 Snake Activation\u7684\u4f5c\u8005\u4e5f\u8fdb\u4e00\u6b65\u63d0\u5230\u4e86 x + sin^2(x) \u6bd4 x + sin(x) \u597d\u7684\u5730\u65b9\u5728\u4e8e\u9ad8\u6b21\u9879\u51fa\u73b0\u5728 x^2 \u5904\uff0c\u62df\u5408\u4e0a\u66f4\u6709\u5229\u3002 On Warm-Starting Neural Network Training pdf \u8fd9\u7bc7paper\u8ba8\u8bba\u7684\u95ee\u9898\u4e0e catestrophic forgetting \u7684\u6709\u4e00\u5b9a\u533a\u522b\u3002\u672c\u6587\u7684\u8bdd\u9898\u662f\u5bf9\u4e00\u4e2a\u90e8\u7f72\u5728\u751f\u4ea7\u73af\u5883\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u5728\u7ebf\u6301\u7eed\u6709\u65b0\u7684\u6570\u636e\u4ea7\u751f\uff0c\u6570\u636e\u96c6\u4f1a\u6301\u7eed\u66f4\u65b0\uff0c\u5728\u8fd9\u4e2a\u60c5\u51b5\u4e0b\u9700\u8981\u5feb\u901f\u5730\u5bf9\u65b0\u6570\u636e\u6216\u8005\u65b0\u6570\u636e\u96c6\u8fdb\u884c\u5b66\u4e60\u3002\u4f46\u662f\u90e8\u7f72\u73af\u5883\u4e2d\u5f80\u5f80\u4f1a\u53d1\u73b0\uff0c\u5982\u679c\u7528\u4e4b\u524d\u73b0\u6709\u7684\u6a21\u578b\u4f5c\u4e3a\u57fa\u7840\u8fdb\u884c\u8bad\u7ec3\uff0c\u4e5f\u5c31\u662f warm-starting\u3002 \u5bf9\u4e8e\u51f8\u4f18\u5316\u95ee\u9898\uff0cwarm-starting \u4e00\u76f4\u662f\u4e00\u4e2a\u91cd\u8981\u7684\u6280\u5de7\u4e14\u975e\u5e38\u6709\u7528\uff0c\u4f46\u662f\u5bf9\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7f51\u7edc\u6765\u8bf4\uff0c\u51fa\u6765\u7684\u6a21\u578b\u6027\u80fd\u4f1a\u6bd4\u8f83\u5dee\uff0c\u4e14training \u96c6\u4e0a\u6027\u80fd\u663e\u73b0\u4e0d\u51fa\u6765\uff0c\u53cd\u800c\u662fgeneralization\u80fd\u529b\u5728\u4e0b\u964d\u3002\u56e0\u800c\u65f6\u81f3\u4eca\u65e5\u5f88\u591a\u76f8\u5173\u7684\u95ee\u9898\u4e2d\u6211\u4eec\u90fd\u4f1a\u5728\u6bcf\u4e2a\u8fed\u4ee3\u5faa\u73af\u5c06\u6a21\u578b\u521d\u59cb\u5316\u91cd\u65b0\u8bad\u7ec3\u3002 \u5728\u7b2c\u4e00\u4e2a\u5b9e\u9a8c\u4e2d\uff0c\u4f5c\u8005\u4f7f\u7528ResNet, MLP, Logistic regression\u5206\u522b\u5728CIFAR, SVHN\u4e0a \u76f4\u63a5Train\u6240\u6709\u6570\u636e / \u5148train \u524d50%, \u518dtrain \u6240\u6709\u6570\u636e\u3002 \u53d1\u73b0 ResNet\u4e0eMLP\u7684\u6027\u80fd\u4e0d\u8bba\u91c7\u7528SGD \u8fd8\u662fADAM\uff0c \u7b2c\u4e8c\u6b21\u5b9e\u9a8c\u90fd\u6bd4\u7b2c\u4e00\u4e2a\u5b9e\u9a8c\u5dee\u51e0\u4e2a\u70b9\uff0c\u5728\u6bd4\u8f83\u56f0\u96be\u7684CiFAR\u4e0a\u66f4\u4e3a\u660e\u663e\u3002\u800cLogistic Regression\u7684\u6027\u80fd\u5dee\u5219\u4e0d\u5927\uff0c\u56e0\u4e3a\u5b83\u7406\u5e94\u662f\u4e00\u4e2a\u51f8\u4f18\u5316\u8bbe\u7f6e\u3002\u53ef\u4ee5\u7b80\u5355\u5f97\u5230\u51e0\u4e2a\u521d\u6b65\u7684\u7ed3\u8bba: \u6570\u636e\u96c6\u8d8a\u56f0\u96be\uff0cwarm starting\u5e26\u6765\u7684\u6027\u80fd\u4e0b\u964d\u8d8a\u660e\u663e \u51f8\u4f18\u5316\u95ee\u9898LR\u51e0\u4e4e\u4e0d\u53d7warm-start\u5f71\u54cd\u3002 \u6211\u4eec\u5e38\u5e38\u8ba4\u4e3a\u57fa\u4e8e\u90e8\u5206data\u7684warm starting\u53ef\u4ee5\u7ed9\u7f51\u7edc\u4e00\u4e2a\u66f4\u597d\u7684 prior, \u4e14\u5e94\u5f53\u662f\u6709\u7528\u7684\uff0c\u8fd9\u4e5f\u7b26\u5408transfer learning\u7684\u903b\u8f91\u3002\u4f46\u662f\u5b9e\u9a8c\u7ed3\u679c\u4e0e\u8fd9\u4e2a\u9884\u671f\u5e76\u4e0d\u76f8\u7b26\u3002 \u800c\u73b0\u6210\u7684\u8c03\u53c2\u5de5\u5177\u53ef\u4ee5\u5904\u7406\u8fd9\u4e2a\u95ee\u9898\u5417? \u4f5c\u8005\u7f51\u683c\u641c\u7d22\u4e86 batch_size, learning rate, \u7b49\u53c2\u6570\uff0c\u53d1\u73b0\u662f\u80fd\u627e\u5230\u6027\u80fd\u4e0e\u539f\u6765\u6a21\u578b\u76f8\u5f53\u7684\u53c2\u6570\uff0c\u4f46\u662f\u5176\u8bad\u7ec3\u65f6\u957f\u5f80\u5f80\u4e5f\u8981\u5f88\u957f\u624d\u80fd\u5b9e\u73b0\uff0c\u6ca1\u6709\u6bd4\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\u8282\u7701\u8fd0\u7b97\u91cf\u3002\u4f5c\u8005\u6307\u51fa\u8fd9\u80cc\u540e\u5176\u5b9e\u5f88\u53ef\u80fd\u662f\u5fd8\u6389\u4e86\u539f\u6765\u7684\u53c2\u6570\u7279\u6027\uff0c\u51e0\u4e4e\u662f\u91cd\u65b0\u8bad\u7ec3\u4e86\u3002\u4e3a\u4e86\u8bc1\u5b9e\u8fd9\u4e00\u70b9\uff0c\u4f5c\u8005\u8ba1\u7b97\u8fd9\u4e9b\u6027\u80fd\u597d\u7684\u6a21\u578b\u5728\u4e8c\u6b21\u8bad\u7ec3\u524d\u540e\u7684\u6743\u91cd\u7684\u76f8\u5173\u6027\uff0c\u53d1\u73b0\u6027\u80fd\u597d\u7684\u6a21\u578b\u524d\u540e\u6743\u91cd\u51e0\u4e4e\u6ca1\u6709\u76f8\u5173\u6027\u3002\u4e14\u76f8\u5173\u6027\u8d8a\u5927\uff0c\u6027\u80fd\u8d8a\u5dee\uff0c\u4e5f\u5c31\u662f\u7f51\u7edc\u8d8a\u5dee\u8d8a\u597d\u3002\u4f46\u662fLogistic Regression\u5219\u4e0d\u4f1a\u6709\u8fd9\u6837\u7684\u60c5\u51b5\u3002 \u4f5c\u8005\u518d\u505a\u4e86\u4e00\u7ec4\u5b9e\u9a8c\uff0c\u5c31\u662f\u7b2c\u4e00\u8f6e\u8bad\u7ec3\u591a\u5c11\u6b21\u5c31\u4f1a\u5f00\u59cb\u635f\u5bb3generalization\u5462? \u4f5c\u8005\u5728CIFAR-10\u4e0a\u7528ResNet\u8fdb\u884c\u5b9e\u9a8c\uff0c\u6570\u636e\u663e\u793a\u7b2c\u4e00\u8f6e\u5927\u7ea620\u4e2aepochs\u4e4b\u540e\uff0c\u6700\u7ec8\u7684\u6027\u80fd\u5c31\u5df2\u7ecf\u5f00\u59cb\u663e\u8457\u5f80\u4e0b\u964d\u4e86\uff0c \u4f5c\u8005\u91c7\u7528L2 regularization, confidence-penalized training, adversarial training\u53bb\u89c4\u8303\u5316\u4e24\u4e2a\u9636\u6bb5\u7684\u8bad\u7ec3\u6d41\u7a0b\uff0c\u8fd9\u4e9b\u89c4\u8303\u5316\u65b9\u6cd5\u662f\u6709\u6548\u7684\uff0c\u4f46\u662f\u8fdc\u4e0d\u80fd\u5b8c\u5168\u6d88\u706d warm-starting\u5e26\u6765\u7684\u6027\u80fd\u4e0b\u964d\u3002 \u4f5c\u8005\u6700\u7ec8\u63d0\u51fa Shrink, Perturb, Repeat \u7684\u7b97\u6cd5\u3002\u6bcf\u5f53\u65b0\u6570\u636e\u52a0\u5165\u5230\u6570\u636e\u96c6\u65f6\uff0c\u5bf9\u6a21\u578b\u4e2d\u6240\u6709\u53c2\u6570\u4fee\u6b63\u4e3a \\theta_i^t \\leftarrow \\lambda\\theta_i^{t-1} + p^t \u5176\u4e2d p^t \\sim \\mathcal{N}(0, \\sigma^2) \u4e14 0 < \\lambda < 1 . \u76f4\u89c9: \u5c06\u6240\u6709\u6743\u91cd\u7f29\u5c0f\u4e00\u5b9a\u6bd4\u4f8b\u51e0\u4e4e\u4f1a\u4fdd\u7559ReLU\u5206\u7c7b\u6a21\u578b\u7684\u6240\u6709\u8bb0\u5fc6\uff0c\u5bf9\u4e8e\u590d\u6742\u7684\u5e26\u6709BatchNorm\u7684ResNet, \\lambda \u53bb\u5230\u4f4e\u4e8e 0.1 \u540e\u624d\u5f00\u59cb\u635f\u5931\u51c6\u786e\u7387\uff0c\u5bf9\u4e8e\u7b80\u5355\u7684\u5e26\u6709bias\u7684MLP\uff0c\u6743\u91cd\u7f29\u5c0f\u5230 \\lambda < 0.6 \u65f6\u624d\u5f00\u59cb\u635f\u4f24\u51c6\u786e\u7387\u3002 Warm-starting\u4e00\u5927\u95ee\u9898\u5728\u4e8e\u65b0\u6570\u636e\u7684gradients\u4f1a\u8fdc\u8fdc\u5927\u4e8e\u65e7\u6570\u636e\u7684gradient, \u4f7f\u7528shrinking\u7f29\u5c0f\u8f93\u51fa\u7f6e\u4fe1\u5ea6\uff0c\u52a0\u4e0a\u4e00\u70b9\u6270\u52a8\u540e\u5c31\u53ef\u4ee5\u8fdb\u4e00\u6b65\u5747\u8861\u6a21\u578b\u5bf9\u4e0d\u540c\u6570\u636e\u7684\u68af\u5ea6\u3002 \u5b9e\u9a8c\u4e0a \\sigma=0.01, \\lambda=0.6 \u5c31\u53ef\u4ee5\u5f97\u5230\u597d\u7684\u6027\u80fd\u3002 \u4ece\u6570\u503c\u66f4\u65b0\u4e0a\uff0c\u8fd9\u5176\u5b9e\u5f88\u63a5\u8fd1\u4e8e \\theta_i \\leftarrow \\lambda(\\theta_i + \\eta\\frac{\\partial L}{\\partial \\theta_i}) + p , \u5728\u4e00\u5b9a\u60c5\u51b5\u4e0b\u5f88\u63a5\u8fd1\u4e8e L_2 regularization. \u5728\u5b9e\u9645\u5b9e\u9a8c\u4e2d\uff0c\u4f5c\u8005\u53d1\u73b0 shrink-and-perturb \u80fd\u5728\u9759\u6001\u6570\u636e\u96c6\u4e2d\u540c\u6837\u5730\u5b9e\u73b0\u4e0e L_2 \u76f8\u4f3c\u7684regularization,\u4f46\u662f L_2 \u5e76\u4e0d\u80fd\u89e3\u51b3warm start\u95ee\u9898 Rational Neural Network pdf code \u8fd9\u7bc7paper\u63d0\u51fa\u4f7f\u7528\u5206\u5f0f\u591a\u9879\u5f0f\u4f5c\u4e3a\u6fc0\u6d3b\u51fd\u6570 F(x) = \\frac{P(x)}{Q(x)} = \\frac{\\sum^{r_P}_{i=0} a_ix^i}{\\sum^{r_Q}_{j=0}b_jx^j} \u8fd9\u5176\u5b9e\u5e76\u975e\u5b8c\u5168\u9996\u521b\uff0c\u672c\u6587\u8bc1\u660e\u4e86\u4e00\u4e9b\u76f8\u5173\u7684\u6027\u8d28\uff0c\u5e76\u4e14\u63d0\u51fa\u4e86\u4f7f\u7528\u5b83\u7684\u65b9\u6cd5\u3002\u672c\u6587\u63d0\u51fa\u7684\u601d\u8def\u662f r_P = 3, r_Q=2 \uff0c\u56e0\u800c\u4e00\u5171\u662f7\u4e2a\u53c2\u6570\uff0c\u540c\u65f6\u5f53x\u8d8b\u5411\u4e8e\u6781\u9650\u7684\u65f6\u5019\uff0c\u6fc0\u6d3b\u51fd\u6570\u4f1a\u8d8b\u5411\u4e8e\u6b63\u6bd4\u4f8b\u51fd\u6570( y=kx ) \u7406\u8bba\u5206\u6790\u7ed3\u8bba(\u672c\u6587\u7684\u4e3b\u8981\u5207\u5165\u89d2\u5ea6\u662f\u5bf9\u62df\u5408\u80fd\u529b\u7684\u63a2\u7d22,\u4e5f\u5373\u662f\u4f7f\u7528\u591a\u5c11\u53c2\u6570\u80fd\u8ba9\u8f93\u51fa\u51fd\u6570\u5c3d\u53ef\u80fd\u62df\u5408\u76ee\u6807\u51fd\u6570): - \u8981\u62df\u5408\u4e00\u4e2aReLU Network\u9700\u8981\u7684Rational Network\u53c2\u6570\u91cf \u6bd4 \u7528ReLU Network\u62df\u5408Rational Network\u7684\u53c2\u6570\u91cf\u8981\u6307\u6570\u7ea7\u7684\u5c11. - \u8981\u62df\u5408\u4e00\u4e2a\u8fde\u7eed\u51fd\u6570\uff0c\u9700\u8981\u7684Rational Network\u7684\u53c2\u6570\u91cf\u6bd4\u7528ReLU Network\u7684\u53c2\u6570\u91cf\u8981\u6307\u6570\u7ea7\u5730\u5c11\u3002 \u80cc\u666f\u77e5\u8bc6\uff0c\u5176\u8bc1\u660e\u65b9\u6cd5\u7c7b\u4f3c\u4e8e Why Deep Neural Networks for Function Approximation? . \u8fd9\u7bc7\u6587\u7ae0\u8bc1\u660e\u4e86\u6df1\u5c42ReLU\u795e\u7ecf\u7f51\u7edc\u62df\u5408\u4e00\u4e2a\u8fde\u7eed\u51fd\u6570\u6240\u9700\u8981\u7684\u53c2\u6570\u91cf\u6bd4\u6d45\u5c42\u7f51\u7edc\u7684\u53c2\u6570\u91cf\u8981\u6307\u6570\u7ea7\u5730\u5c11. What do Neural Network Learn when Trained with Random Labels pdf \u8fd9\u7bc7paper\u7814\u7a76\u4e86\u4f7f\u7528\u968f\u673a\u6807\u7b7e\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u4e4b\u540e\uff0c\u7f51\u7edc\u7a76\u7adf\u5b66\u5230\u4e86\u4ec0\u4e48\uff0c\u5bf9transfer learning\u7a76\u7adf\u6709\u4ec0\u4e48\u7528\u3002 \u5b83\u63a2\u8ba8\u4e86\u4e00\u4e2a\u73b0\u8c61\uff0c\u4e5f\u5c31\u662f\u5728\u4e00\u5b9a\u8d85\u53c2\u7684\u60c5\u51b5\u4e0b\uff0c\u5373\u4f7f\u4f7f\u7528\u968f\u673a\u53d8\u91cf\u8fdb\u884c\u8bad\u7ec3\uff0c\u7f51\u7edc\u8fd8\u662f\u6709\u53ef\u80fd\u80fd\u52a0\u901f\u6536\u655b(\u5c3d\u7ba1 genralization\u7684\u6027\u80fd\u6bd4\u8f83\u5dee) \u4f5c\u8005\u89e3\u91ca\u5e76\u5370\u8bc1\u4e86\u4e00\u4e2a\u91cd\u8981\u7684\u73b0\u8c61\uff0c\u4e5f\u5c31\u662f\u5373\u4f7f\u6ca1\u6709label\uff0c\u8bad\u7ec3\u7684\u8fc7\u7a0b\u4e2d\u5e95\u5c42\u7f51\u7edc\u7684\u6743\u91cd\u4e5f\u4f1a\u5bf9\u8f93\u5165\u6570\u636e\u7684\u7edf\u8ba1\u7279\u5f81\u8fdb\u884c\u590d\u523b(Alignment, defined as same eigenvectors)\uff0c\u53cd\u5e94\u5728\u6743\u91cd\u77e9\u9635\u7684\u4e8c\u9636\u7279\u6027\u4e0a\u3002\u5982\u679c\u6211\u4eec\u6a21\u4eff\u6570\u636e\u7684\u8fd9\u4e2a\u6027\u8d28\u9010\u5c42\u521d\u59cb\u5316\u7f51\u7edc\uff0c\u6211\u4eec\u4e5f\u53ef\u4ee5\u5b9e\u73b0\u5bf9\u7f51\u8def\u7684\u52a0\u901f\u8bad\u7ec3\u3002 \u5bf9\u4e8e\u9ad8\u65af\u8f93\u5165\u7684Alignment \u5047\u8bbe d \u7ef4\u8f93\u5165 x \u4ece \\mathcal{N}(0, \\Sigma_x) \u9ad8\u65af\u4e2d\u91c7\u6837\uff0c\u76ee\u6807\u6807\u7b7e\u4e3a\u968f\u673a\u503c\uff0c\u4e14\u521d\u59cb\u6743\u91cd\u4ece\u9ad8\u65af\u4e2d\u63d0\u53d6\uff0c\u90a3\u4e48\u6700\u540e\u5f97\u5230\u7684 d \u7ef4\u6743\u91cd\u6ee1\u8db3 \\mathbb{E}[\\boldsymbol{w}]=0 , \u4e14 \\Sigma_\\omega=\\mathbb{E}[\\omega\\cdot\\omega^T] \u7684\u7279\u5f81\u503c\u4e0e \\Sigma_x \u7684\u7279\u5f81\u503c\u4e00\u81f4\u3002 \u4f5c\u8005\u8bc1\u660e\u4e86\u8fd9\u4e2a\u7ed3\u679c\u4e0e\u6570\u636e\u91cf\uff0c\u7f51\u7edc\u662f\u5168\u8fde\u63a5\u8fd8\u662fconv\uff0c\u5b66\u4e60\u7387\u4e0e\u4f18\u5316\u5668\u7b49\u90fd\u65e0\u5173\u3002 \u5b9e\u9a8c \u5b9e\u9a8c\u7528\u771f\u5b9e\u6570\u636e\u96c6\u771f\u5b9e\u56fe\u7247\u968f\u673a\u6807\u7b7e\u4ee5\u53ca\u771f\u5b9e\u6807\u7b7e\u8fdb\u884c\u8bad\u7ec3\uff0c\u4f5c\u8005\u5728\u5b9e\u9a8c\u4e0a\u540c\u6837\u80fd\u5728\u7b2c\u4e00\u5c42\u5377\u79ef\u5c42\u4e0a\u8bc1\u660e\u5b9e\u9a8c\u7684\u73b0\u8c61\u3002\u80cc\u540e\u7684\u76f4\u89c9\u662f\u8bf4\u6700\u5e95\u5c42\u7684 3\\times 3 \u65b9\u5757\u4e0e\u771f\u5b9e\u6807\u7b7e\u51e0\u4e4e\u6ca1\u6709\u76f8\u5173\u6027\uff0c\u56e0\u800c\u7edf\u8ba1\u4e0a\u771f\u5b9e\u6807\u7b7e\u8bad\u7ec3\u5bf9\u4e8e\u7b2c\u4e00\u5c42\u4e5f\u76f8\u5f53\u4e8e\u5c31\u662f\u968f\u673a\u6807\u7b7e\u7684\u7ed3\u679c\u3002 \u5b9e\u9a8c\u4e2d misalignment\u8fdc\u4f4e\u4e8e\u968f\u673a\u6807\u7b7e\u7684\u7ed3\u679c, throughout training process. \u63a5\u7740\u4f5c\u8005\u5bf9\u6d45\u5c42\u7f51\u7edc\u7684\u6743\u91cd\u5728\u521d\u59cb\u5316\u7684\u65f6\u5019\u5148\u7528\u968f\u673asample\u7684\u7ed3\u679c\u8fdb\u884c\u91c7\u6837(\u4e5f\u5c31\u662f\u4e0d\u662f\u7528\u8bad\u7ec3\u7684\u6743\u91cd\uff0c\u800c\u662f\u91c7\u6837\u7684\u6743\u91cd)\uff0c\u53d1\u73b0\u540c\u6837\u80fd\u63d0\u5347\u7f51\u7edc\u7684\u8bad\u7ec3\u901f\u5ea6\u3002\u8bf4\u660e\u7f51\u7edc\u88ab\u968f\u673a\u6807\u7b7e\u9884\u8bad\u7ec3\u540e\u80fd\u63d0\u5347\u901f\u5ea6\u4e3b\u8981\u662f\u56e0\u4e3a\u5bf9\u6570\u636e\u4e8c\u9636\u7279\u6027\u7684alignment. \u5bf9\u4e8e\u66f4\u6df1\u7684\u7f51\u7edc\uff0c\u4f5c\u8005\u6307\u51fa\uff0c\u53ef\u4ee5\u4e00\u5c42\u4e00\u5c42\u5730\u521d\u59cb\u5316\uff0c\u4e5f\u5373\u662f\u5148\u4f7f\u7528\u91c7\u6837\u7684\u65b9\u6cd5\u5f97\u5230\u7b2c\u4e00\u5c42\u7f51\u7edc\u7684\u6743\u91cd\uff0c\u7136\u540e\u628a\u5b83\u4f5c\u4e3a\u8f93\u5165\u91c7\u6837\u7b2c\u4e8c\u5c42\u7684\u6743\u91cd\u3002\u80fd\u5f97\u5230\u4e0epre-train\u540c\u6837\u7684\u52a0\u901f. An Analysis of SVD for Deep Rotation Estimation pdf \u8fd9\u7bc7paper\u5206\u6790\u4e86 SVD\u7528\u4e8e\u6df1\u5ea6\u5b66\u4e60\u4e2d\u65cb\u8f6c\u7684\u4f30\u8ba1\u7684\u7279\u6027\u3002 \u4e0e keypointnet \u5bf9\u65cb\u8f6c\u7684\u63a8\u7406\u8fc7\u7a0b\u76f8\u4f3c\uff0c\u7f51\u7edc\u8f93\u51fa 9 \u7ef4\u7684\u6570\u503c\u8f93\u51fa\uff0c\u5806\u780c\u4e3a\u77e9\u9635 M M \u7684SVD\u5206\u89e3\u7ed3\u679c\u4e3a U\\Sigma V^T , \u5219\u5728\u65cb\u8f6c\u96c6 SO(3) \u4e0a\u7684\u6295\u5f71\u4e3a SVDO^+(M) = U\\Sigma'V^T ,\u5176\u4e2d \\Sigma' = diag(1, ..., 1, det(UV^T)) \u635f\u5931\u51fd\u6570 L(M,R) = ||SVDO^+(M) - R||_F^2 ,\u65cb\u8f6c\u77e9\u9635\u5dee\u7684\u4e8c\u8303\u6570\u5b9e\u8d28\u4e0a\u7b49\u4e8e\u4e24\u4e2a\u65cb\u8f6c\u7684\u5dee(\u4e5f\u662f\u4e00\u4e2a\u65cb\u8f6c\u77e9\u9635)\u7684\u4e3b\u65cb\u8f6c\u89d2\u5ea6. \u4f5c\u8005\u901a\u8fc7\u5206\u6790\u8fd9\u4e00\u635f\u5931\u51fd\u6570\u5bf9SVD\u7684\u53cd\u4f20\u7ed3\u679c\u8bc1\u660e\u4e86\u8fd9\u79cd\u8868\u793a\u65b9\u6cd5\u662f\u5728\u6700\u5c0f\u4e8c\u4e58\u6216\u662f\u9ad8\u65af\u6982\u7387\u566a\u58f0\u4e24\u79cd\u8868\u8ff0\u4e0b\u90fd\u662f\u6700\u4f18\u7684\uff0cSVD\u7684\u53cd\u4f20\u6bd4\u8f83\u590d\u6742. Derivative of SVD Is normalization indispensable for training deep neural networks pdf code \u8fd9\u7bc7paper\u63d0\u51fa\u7684\u5185\u5bb9\u662f\u53ef\u4ee5\u901a\u8fc7rescaling \u5904\u7406\u53ef\u4ee5\u5728\u4e0d\u4f7f\u7528batchnorm\u7684\u60c5\u51b5\u4e0b\u7ef4\u6301\u7f51\u7edc\u7684variance. \u8fd9\u7bc7paper\u540c\u65f6\u901a\u8fc7\u5b9e\u9a8c\u548c\u6570\u5b66\u4e0b\u4e86\u4e00\u4e2a\u5224\u65ad\uff0c\u8ba4\u4e3a\u5728\u6df1\u5ea6\u7f51\u7edc\u4e2d Dead ReLU \u662f\u7531\u7cfb\u7edf\u7684variance\u5f15\u8d77\u7684\uff0c\u6311\u9009\u597d\u7684\u521d\u59cb\u5316\u4e5f\u6ca1\u7528\uff0c\u8bad\u7ec3\u540e\u8fd8\u662f\u4f1a\u6709\u5f88\u591a\u7684 dead relu\uff0c\u66f4\u591a\u7684\u8bba\u8bc1\u5728\u9644\u5f55\u4f46\u5c1a\u672a\u653e\u51fa. \u4f5c\u8005\u63d0\u51fa rescalenet\uff0c\u6838\u5fc3\u601d\u8def\u662f\u60f3\u7f51\u7edc\u7684\u65b9\u5dee\u4e0d\u8981\u53d1\u6563\uff0c\u540c\u65f6\u7ecf\u8fc7\u591a\u5c42\u53e0\u52a0\u653e\u7f6e\u540e\uff0c\u6bcf\u4e00\u4e2a resblock\u7684\u5377\u79ef\u5c42\u8f93\u51fa\u5728\u6700\u540e\u8f93\u51fa\u4e0a\u7684\u6743\u91cd\u662f\u4e00\u81f4\u7684\u3002 \u6c42\u89e3\u8fd9\u4e24\u4e2a\u6761\u4ef6(\u5177\u4f53\u89e3\u6cd5\u5c1a\u672a\u653e\u51fa): \\operatorname{Var}\\left[\\boldsymbol{x}_{k}\\right]=\\alpha_{k}^{2} \\operatorname{Var}\\left[\\boldsymbol{x}_{k-1}\\right]+\\beta_{k}^{2} \\operatorname{Var}\\left[\\mathcal{F}_{k}\\left(\\boldsymbol{x}_{k-1}\\right)\\right]=\\operatorname{Var}\\left[\\boldsymbol{x}_{k-1}\\right] \\boldsymbol{x}_{L}=\\left(\\prod_{i=1}^{L} \\alpha_{i}\\right) \\boldsymbol{x}_{0}+\\sum_{k=1}^{L} \\beta_{k} \\prod_{i=k+1}^{L} \\alpha_{i} \\mathcal{F}_{k}\\left(\\boldsymbol{x}_{k-1}\\right) \u7ed9\u51fa: \\boldsymbol{x}_{k}=\\sqrt{\\frac{k-1+L}{k+L}} \\boldsymbol{x}_{k-1}+\\sqrt{\\frac{1}{k+L}} \\mathcal{F}_{k}\\left(\\boldsymbol{x}_{k-1}, \\Theta_{k}\\right) \u5176\u4ed6\u4e24\u4e2a\u6280\u5de7: \u4e8b\u5b9e\u4e0a\u5728\u8bad\u7ec3\u7684\u65f6\u5019\u4e0d\u4e00\u5b9a\u8981\u8ba9\u4e0d\u540cresblock\u5c42\u6570\u4e0a\u7684\u6743\u91cd\u4e00\u81f4\uff0c\u56e0\u4e3a\u7ecf\u9a8c\u4e0a\u6765\u8bf4\u4e0d\u540c\u7f51\u7edc\u5c42\u6210\u719f\u901f\u5ea6\u5c31\u662f\u4e0d\u4e00\u6837\u7684\uff0c\u5728\u4e0d\u540c\u7684\u8bad\u7ec3\u9636\u6bb5\u6709\u4e0d\u540c\u7684\u9700\u6c42\uff0c\u6240\u4ee5\u672c\u6587\u63d0\u51fa\u7ed9\u8fd9\u4e2a\u6bd4\u4f8b\u53c2\u6570\u4e00\u4e2a\u53ef\u5b66\u4e60\u7684\u4e58\u5b50: \\boldsymbol{x}_{k}=\\sqrt{\\frac{k-1+L}{k+L}} \\boldsymbol{x}_{k-1}+\\frac{m_{k}}{\\sqrt{L}} \\mathcal{F}_{k}\\left(\\boldsymbol{x}_{k-1}\\right) Bias\u7684\u521d\u59cb\u5316\u4f5c\u8005\u4e5f\u63d0\u51fa\u65b0\u7684\u89c2\u70b9\uff0c\u9996\u5148\u5c06\u8fd0\u7b97 y=Wx + b \u91cd\u65b0\u7406\u89e3\u4e3a y = W(x + b) \u800c\u8fd9\u4e2a b\u7684\u521d\u59cb\u5316\u4e3a\u7b2c\u4e00\u4e2a\u8f93\u5165\u7684mini-batch\u7684\u5747\u503c\u7684\u8d1f\u503c\uff0c\u4e5f\u5c31\u662f\u8ba9\u7b2c\u4e00\u4e2abatch\u5728\u4e0e\u6743\u91cd\u76f8\u4e58\u4e4b\u524d\u63a5\u8fd1\u662f normalized\u7684 (\u5728\u4ee3\u7801\u5b9e\u73b0\u4e0a\uff0c\u662f\u5b9a\u4e0b\u4e86\u4e00\u4e2athreshold, \u524dn=8\u4e2abatch\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u90fd\u4f1a\u6c42\u7d2f\u79ef\u5747\u503c)\uff0c\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u5b9e\u73b0\u4e86BN\u7684\u6548\u679c. \u540c\u65f6\u4f5c\u8005\u6307\u51fa\u8fd9\u4e2a\u6280\u5de7\u53ef\u4ee5\u663e\u8457\u51cf\u8f7brelu\u7684\u95ee\u9898.","title":"NIPS 2020 for Experimental NN"},{"location":"The_theory/NIPS2020_networks/#nips-2020-for-experimental-nn","text":"\u672c\u6587\u6536\u96c6\u51e0\u7bc7NIPS2020\u4e2d\u5927\u5b9e\u9a8c\u5ba4\u4e2d\u901a\u8fc7\u5b9e\u9a8c\u7814\u7a76\u795e\u7ecf\u7f51\u7edc\u76f8\u5173\u6027\u8d28\u7684\u6587\u7ae0\u3002\u76f8\u5173\u5b9e\u9a8c\u4e0d\u4e00\u5b9a\u6709\u4e25\u683c\u7684\u7406\u8bba\u63a8\u7406\u6216\u662f\u5b9e\u7528\u6027\u7684\u521b\u65b0\uff0c\u4f46\u662f\u53ef\u4ee5\u4e86\u89e3\u76f8\u5f53\u6709\u610f\u4e49\u7684\u7ecf\u9a8c\u7ed3\u8bba. NIPS 2020 for Experimental NN Batch Normalization Biases Residual Blocks Towards the Identity Function in Deep Networks Neural Networks Fail to Learn Periodic Functions and How to Fix It \u4e0eSIREN \u5bf9\u6bd4 On Warm-Starting Neural Network Training Rational Neural Network What do Neural Network Learn when Trained with Random Labels \u5bf9\u4e8e\u9ad8\u65af\u8f93\u5165\u7684Alignment \u5b9e\u9a8c An Analysis of SVD for Deep Rotation Estimation Is normalization indispensable for training deep neural networks","title":"NIPS 2020 for Experimental NN"},{"location":"The_theory/NIPS2020_networks/#batch-normalization-biases-residual-blocks-towards-the-identity-function-in-deep-networks","text":"pdf \u8fd9\u7bc7DeepMind\u7684paper \u63a2\u7d22\u4e86BatchNorm\u4e0eResNet\u4e4b\u95f4\u7684\u4e00\u4e9b\u5173\u7cfb\u3002 \u4e0d\u96be\u8bc1\u660e\u7684\u6570\u5b66\u7ed3\u8bba: resnet\u7684variance\u516c\u5f0f, Var(x_i^{l+1}) = Var(x_i^l) + Var(f^l_i(x^l)) \u5bf9\u4e8e\u6ca1\u6709batchnorm\u7684\u7f51\u7edc\uff0c\u5176variance\u4e3a Var(x_i^{l+1}) = 2 * Var(x_i^l) = 2^l (\u4e24\u4e2a\u76f8\u4e92\u72ec\u7acb\u7684\u9ad8\u65af\u5206\u5e03\u8f93\u51fa variance\u4e3a\u4e24\u8005\u7684\u76f4\u63a5\u52a0\u548c), \u521d\u59cbvariance\u4f1a\u968f\u7740resnet\u6df1\u5ea6\u53d1\u6563\u3002\u5982\u679c\u5bf9\u52a0\u548c\u8f93\u51fascale down 1 / \\sqrt{2} , \u5219\u53ef\u4ee5\u4fdd\u6301variance\u4e0d\u53d8\u3002 \u5bf9\u4e8e\u8f93\u51fa\u524d\u6709batchnorm\u7684\u7f51\u7edc\uff0c\u5176variance \u4e3a Var(x_i^{l+1}) = Var(x_i^l) + 1 \\approx l . \u53ef\u4ee5\u53d1\u73b0\u7684\u662f\uff0c\u968f\u7740\u7f51\u7edc\u5c42\u6570 l \u7684\u589e\u5927\uff0c\u4e3b\u5e72\u7f51\u7edc\u4e0e\u6b8b\u5dee\u5206\u652f\u4e4b\u95f4\u7684variance \u6bd4\u4f8b\u4f1a\u8d8a\u6765\u8d8a\u5927\uff0c\u6b8b\u5dee\u5206\u652f\u4e0a\u7684batchnorm\u4f1a\u538b\u5236\u5176\u6570\u503c\u3002\u56e0\u800c\u4ece\u6570\u5b66\u4e0a\u53ef\u4ee5\u53d1\u73b0\u968f\u7740resnet\u6df1\u5ea6\u7684\u63d0\u5347\uff0c\u7f51\u7edc\u4f1a\u8d8b\u8fd1\u4e8eidentity function\u3002\u8fd9\u4f1a\u4f7f\u5f97\u7f51\u7edc\u521d\u59cb\u5316\u540e\u66f4\u5bb9\u6613\u8bad\u7ec3\u3002 \u4f5c\u8005\u63d0\u51fa\u4e86SkipInit \u6a21\u4eff\u4e86batchnorm\u7684\u7279\u6027 \u5b9e\u9a8c\u4e0a\u8bc1\u660e\u4e86\u5176variance\u7279\u6027\uff0c\u4ee5\u53caskipinit\u540c\u6837\u80fd\u8ba9\u6df1\u5ea6\u7684\u7f51\u7edc\u66f4\u5bb9\u6613\u8bad\u7ec3\u4e86\u3002 \u63a5\u7740\u4f5c\u8005\u4ece\u5b9e\u9a8c\u7684\u89d2\u5ea6\u5206\u6790\u4e86\u4e24\u4e2aBN\u76f8\u5173\u7684\u7ed3\u8bba. BN\u5141\u8bb8\u7f51\u7edc\u4f7f\u7528\u66f4\u5927\u7684\u5b66\u4e60\u7387\uff0c\u5b9e\u9a8c\u4e2d\u53d1\u73b0BN\u7f51\u7edc\u7684\u6700\u5927\u7a33\u5b9a\u5b66\u4e60\u7387\u6bd4skipinit\u66f4\u5927\u3002\u4f46\u662f\u5982\u679cbatchsize\u6bd4\u8f83\u5c0f\u7684\u8bdd\uff0c\u7f51\u7edc\u7684\u6027\u80fd\u4e5f\u5e76\u4e0d\u4f1a\u56e0\u6b64\u53d7\u76ca\u3002\u5982\u679c\u56fa\u5b9a\u4e0b\u8bad\u7ec3\u7684epoch\u6570(\u56fa\u5b9a\u8bad\u7ec3FLOPS\uff0c\u6539\u53d8batch_size\u4f1a\u6539\u53d8\u8fed\u4ee3\u6b21\u6570;\u5982\u679c\u56fa\u5b9a\u7684\u662fSteps\u5219\u5728\u5927batch\u7684\u60c5\u51b5\u4e0b\u8bad\u7ec3\u8fd0\u7b97\u91cf\u4e5f\u4f1a\u5927\u5e45\u63d0\u5347), \u90a3\u4e48\u6700\u4f18\u7684\u7ed3\u679c\u5176\u5b9e\u8fd8\u662f\u4f7f\u7528\u5c0fbatch size \u5c0flearning rate.\u56e0\u6b64\u4f5c\u8005\u8ba4\u4e3aBN\u5141\u8bb8\u7f51\u7edc\u4f7f\u7528\u66f4\u5927\u7684\u5b66\u4e60\u7387\u53ea\u662f\u8ba9\u5b66\u4e60\u7387\u53c2\u6570\u76f8\u5bf9\u66f4\u5bb9\u6613\u8c03\u4e86\uff0c\u800c\u5e76\u4e0d\u4f1a\u63d0\u5347\u7f51\u7edc\u6700\u7ec8\u7684\u6027\u80fd. BN\u4f1a\u63d0\u4f9b\u4e00\u4e2aregularization\u3002\u5b9e\u9a8c\u53d1\u73b0\u8fd9\u662f\u4e8b\u5b9e\u5b58\u5728\u7684;\u901a\u8fc7skipInit + dropout\u6216\u8005\u5176\u4ed6regularization\u624b\u6bb5\uff0c\u53ef\u4ee5\u5728\u4f4ebatch\u7684\u60c5\u51b5\u4e0b\u5f97\u5230\u6bd4BN\u66f4\u597d\u7684\u7ed3\u679c(BN\u5728\u4f4ebatch size\u7684\u60c5\u51b5\u4e0b\u53d1\u6325\u4e0d\u597d). \u603b\u7ed3\u8d77\u6765 BN\u4f1a\u8ba9ResNet\u66f4\u50cfidentity function\u6765\u4f7f\u5f97\u6df1\u7f51\u7edc\u66f4\u53ef\u8bad\u7ec3\u3002\u8fd9\u53ef\u4ee5\u7528skipInit\u6a21\u62df\u3002 BN\u8ba9\u7f51\u7edc\u7528\u66f4\u5927\u7684\u5b66\u4e60\u7387\u8bad\u7ec3\uff0c\u4e14\u5728\u5927batch\u60c5\u51b5\u4e0b\u66f4\u4e3a\u660e\u663e\u3002\u4f46\u662f\u8fd9\u5e76\u4e0d\u662f\u63d0\u5347\u6700\u4f18\u6027\u80fd\u7684\u6839\u672c\u539f\u56e0\u3002 BN\u8ba9\u7f51\u7edc\u83b7\u5f97\u4e00\u4e2a\u5185\u5728\u7684regularization. \u5728\u5927batch\u7684\u60c5\u51b5\u4e0b\u5f97\u5230\u5f88\u597d\u7684performance. \u800c\u4f5c\u8005\u6307\u51fa\u4f7f\u7528skipinit + \u5408\u9002\u7684regularization\u53ef\u4ee5\u5728\u5c0fbatch\u7684\u60c5\u51b5\u540c\u65f6\u8ba9\u7f51\u7edctrainable\u3001regularized\u3001\u4e14well performing.","title":"Batch Normalization Biases Residual Blocks Towards the Identity Function in Deep Networks"},{"location":"The_theory/NIPS2020_networks/#neural-networks-fail-to-learn-periodic-functions-and-how-to-fix-it","text":"pdf \u672c\u6587\u6307\u51fa\u4f7f\u7528\"ReLU\"\u662f\u8f93\u51fa\u5206\u6bb5\u7ebf\u6027\u51fd\u6570\u662f\u65e0\u6cd5\u62df\u5408\u5468\u671f\u51fd\u6570\u7684, \u4f5c\u8005\u63d0\u51fa\u4f7f\u7528 x + sin^2(x) \u4f5c\u4e3a\u6fc0\u6d3b\u51fd\u6570\uff0c\u8fd9\u4e2a\u51fd\u6570\u53ef\u4ee5\u6269\u5c55\u4e3a x + \\frac{1}{a}sin^2(ax) \u7531\u56fe\u7247\uff0c\u4f5c\u8005\u79f0\u8fd9\u4e2a\u4e3a Snake activation. \u5b9e\u9a8c\u6765\u8bf4\u8fd9\u4e2a\u51fd\u6570\u53ef\u4ee5\u5728\u7ecf\u5178\u95ee\u9898\u4e0a\u5f97\u5230\u4e0eReLU\u7b49\u6fc0\u6d3b\u51fd\u6570\u4e00\u81f4\u7684\u6027\u80fd\u3002\u4f46\u662f\u8fd9\u4e2a\u51fd\u6570\u6709\u66f4\u5f3a\u7684\u5ef6\u62d3\u6027\u3002\u4f7f\u7528\u6709\u9650\u7684\u51fd\u6570\u62df\u5408 y = sin(x) \u51fd\u6570\uff0c\u5982\u679c\u4f7f\u7528ReLU\u6216\u8005Tanh\uff0c\u5219\u5f53\u8f93\u5165\u51fd\u6570\u8d85\u51fa\u8bad\u7ec3\u8303\u56f4\u65f6\uff0cReLU\u4e0eTanh\u7406\u6240\u5e94\u5f53\u5730\u4e0d\u80fd\u590d\u73b0\u76ee\u6807\u51fd\u6570\u5468\u671f\u6027\u7684\u7279\u70b9\uff0c\u800c\u4f5c\u8005\u63d0\u51fa\u7684\u51fd\u6570 x + sin^2(x) \u53ef\u4ee5\u3002 \u4f5c\u8005\u8bc1\u660e\u4e86\uff0c\u5bf9\u4e8e\u7531ReLU\u7ec4\u6210\u7684(\u591a\u5c42)\u524d\u9988\u7f51\u7edc\uff0c\u5f53\u8f93\u5165\u5f3a\u5ea6(norm)\u65e0\u9650\u903c\u8fd1\u65e0\u7a77\u5927\u65f6\uff0c\u51fd\u6570\u7684\u8f93\u51fa\u503c\u4f1a\u903c\u8fd1\u4e8e\u8f93\u5165\u7684\u7ebf\u6027\u63d2\u503c. \u5bf9\u4e8eTanh\u7f51\u7edc\uff0c\u51fd\u6570\u8f93\u51fa\u503c\u5219\u4f1a\u903c\u8fd1\u4e8e\u5e38\u6570. \\underset{z\\rightarrow\\infty}{lim} ||f_{ReLU}(zu) - zW_u u - b_u||_2 = 0 \\underset{z\\rightarrow\\infty}{lim} ||f_{tanh}(zu) - v_u||_2 = 0 \u76f4\u89c9\u4e0a\u6765\u8bf4\uff0cReLU\u7f51\u7edc\u7684\u8f93\u51fa\u6700\u7ec8\u662f\u5206\u6bb5\u7ebf\u6027\u63d2\u503c\uff0c\u56e0\u800c\u5f53\u65e0\u7a77\u5916\u62d3\u7684\u65f6\u5019\uff0c\u6700\u7ec8\u4f1a\u6536\u655b\u4e8e\u4e00\u4e2a\u7ebf\u6027\u51fd\u6570. Tanh\u5f53\u65e0\u7a77\u5916\u62d3\u7684\u65f6\u5019\uff0c\u7531\u4e8e\u6fc0\u6d3b\u51fd\u6570\u7684\u503c\u7684\u9650\u5236\uff0c\u7b2c\u4e00\u6b21\u6fc0\u6d3b\u540e\u7684\u8f93\u51fa\u503c\u4f1a\u8d8b\u8fd1\u4e8e\u5e38\u6570, \u591a\u5c42\u8fd0\u7b97\u7684\u7ed3\u679c\u4e5f\u90fd\u4f1a\u8d8b\u8fd1\u4e8e\u4e00\u4e2a\u5e38\u6570\u3002 \u7136\u540e\u4f5c\u8005\u5f15\u5165\u4e86\u4e00\u4e2a\u8d85\u8d8a \u4e07\u80fd\u8fd1\u4f3c(universal approximation, UA)\u7684\u6027\u8d28, \u4e07\u80fd\u5ef6\u62d3(universal extrapolation, UE). UA\u5728\u795e\u7ecf\u7f51\u7edc\u9886\u57df\u6307\u7684\u662f:\u5df2\u77e5\u5b9a\u4e49\u5728\u6709\u754c\u533a\u95f4\u7684\u4efb\u610f\u8fde\u7eed\u51fd\u6570 f(x) , \u4e00\u4e2a\u5355\u9690\u85cf\u5c42\u7684\u795e\u7ecf\u7f51\u7edc\uff0c\u5728\u5bbd\u5ea6\u8d8b\u8fd1\u4e8e\u65e0\u7a77\u5927\u7684\u65f6\u5019\uff0c\u603b\u53ef\u4ee5\u627e\u5230\u4e00\u7ec4\u6743\u91cd w_N , \u4f7f\u5f97\u7f51\u7edc\u7684\u8f93\u51fa\u5728\u6709\u754c\u8303\u56f4\u5185\u5904\u5904\u903c\u8fd1\u8fd9\u4e00\u51fd\u6570. \u672c\u6587UE\u7684\u5219\u662f: \u5df2\u77e5\u5b9a\u4e49\u5728\u65e0\u7a77\u8303\u56f4\u5185\u7684\u4efb\u610f\u5468\u671f\u51fd\u6570 f(x) .\u4e00\u4e2a\u4ee5 x+sin^2(x) \u4e3a\u6fc0\u6d3b\u51fd\u6570\u7684\u5355\u9690\u85cf\u5c42\u795e\u7ecf\u7f51\u7edc\uff0c\u5728\u5bbd\u5ea6\u8d8b\u8fd1\u4e8e\u65e0\u7a77\u5927\u65f6\uff0c\u603b\u53ef\u4ee5\u627e\u5230\u4e00\u7ec4\u6743\u91cd w_N ,\u4f7f\u5f97\u7f51\u7edc\u7684\u8f93\u51fa\u5728\u6574\u4e2a\u5b9a\u4e49\u57df\u5185\u903c\u8fd1\u8fd9\u4e00\u4e2a\u51fd\u6570 f(x) . \u4e14\u5982\u679c\u76ee\u6807\u51fd\u6570\u662f\u8fde\u7eed\u7684\uff0c\u90a3\u4e48\u53ef\u4ee5\u505a\u5230\u5904\u5904\u903c\u8fd1. UE\u662f\u6bd4UA\u66f4\u5f3a\u7684\u7ed3\u8bba\uff0c\u663e\u7136UE\u53ef\u4ee5\u5305\u542bUA. \u4f5c\u8005\u7684\u8bc1\u660e\u601d\u8def\u662f\u9996\u5148\u4f7f\u7528\u5085\u91cc\u53f6\u53d8\u6362\uff0c\u8bc1\u660e\u4e86\u4f7f\u7528 sin(x) \u4f5c\u4e3a\u6fc0\u6d3b\u51fd\u6570\uff0c\u4e24\u5c42\u7f51\u7edc\u53ef\u4ee5\u62df\u5408\u4efb\u610f\u5468\u671f\u51fd\u6570\u3002\u7136\u540e\u53c8\u8bc1\u660e\u4e86\u4e24\u4e2a x + sin^2(x) \u6fc0\u6d3b\u7684\u795e\u7ecf\u5143\u53ef\u4ee5\u8868\u8fbe\u4e00\u4e2a sin(x) \u6216 cos(x) \u51fd\u6570. \u4e24\u8005\u7ed3\u5408\u5c31\u8bc1\u660e\u4e86 UE . \u5728\u521d\u59cb\u5316\u95ee\u9898\u4e0a\uff0c\u7531\u4e8e Snake \u6fc0\u6d3b\u51fd\u6570\u5728\u96f6\u9644\u8fd1\u8fd1\u4f3c\u4e8eidentity matrix. \u8981\u8ba9variance expectation\u4e3a 1\uff0c \u4f5c\u8005\u7ed9\u51fa\u7684\u521d\u59cb\u5316\u65b9\u6848\u4e3a Uniform(-\\sqrt{3/d}, +\\sqrt{3/d}). \u4f5c\u8005\u5728\u8003\u8651\u591a\u4e24\u9879\u9ad8\u9636\u91cf\u7684\u60c5\u51b5\u4e0b\u4e5f\u7ed9\u51fa\u4e86\u5bf9\u5e94 Var=1$\u65f6\u7684\u89e3\u6790\u89e3\uff0c\u6027\u80fd\u4f1a\u6709\u4e00\u5b9a\u7684\u63d0\u5347\uff0c\u4f46\u662f\u4f5c\u8005\u4e5f\u6307\u51fa\u5b9e\u7528\u4e0a\u6ca1\u6709\u5fc5\u8981. \u5728a\u7684\u9009\u62e9\u4e0a\uff0c\u5982\u679c\u95ee\u9898\u6ca1\u6709\u663e\u7136\u7684\u5468\u671f\u6027\uff0c\u5219 a = 0.5 \u6ca1\u6709\u95ee\u9898\uff0c\u5982\u679c\u95ee\u9898\u6709\u663e\u7136\u7684\u5468\u671f\u6027\uff0c\u5219\u53ef\u4ee5\u8bbe\u7f6e a \\in [5, 50] . \u4f5c\u8005\u4e5f\u8ba8\u8bba\u4e86\u628a a \u8bbe\u7f6e\u4e3a\u53ef\u5b66\u4e60\u7684\u53c2\u6570\u7684\u53ef\u80fd\u6027.\u6027\u80fd\u4e5f\u662f\u6ca1\u6709\u95ee\u9898\u7684. \u5df2\u7ecf\u6709\u8fd9\u4e2a\u51fd\u6570\u7684 tensorflow addon implementation on github","title":"Neural Networks Fail to Learn Periodic Functions and How to Fix It"},{"location":"The_theory/NIPS2020_networks/#siren","text":"SIREN SIREN\u7684\u95ee\u9898\u5728\u4e8e\u5b83\u662f\u6709\u754c\u7684, \u7f3a\u5c11\u4e86 x \u8fd9\u4e00\u9879\u3002 Snake Activation\u7684\u4f5c\u8005\u4e5f\u8fdb\u4e00\u6b65\u63d0\u5230\u4e86 x + sin^2(x) \u6bd4 x + sin(x) \u597d\u7684\u5730\u65b9\u5728\u4e8e\u9ad8\u6b21\u9879\u51fa\u73b0\u5728 x^2 \u5904\uff0c\u62df\u5408\u4e0a\u66f4\u6709\u5229\u3002","title":"\u4e0eSIREN \u5bf9\u6bd4"},{"location":"The_theory/NIPS2020_networks/#on-warm-starting-neural-network-training","text":"pdf \u8fd9\u7bc7paper\u8ba8\u8bba\u7684\u95ee\u9898\u4e0e catestrophic forgetting \u7684\u6709\u4e00\u5b9a\u533a\u522b\u3002\u672c\u6587\u7684\u8bdd\u9898\u662f\u5bf9\u4e00\u4e2a\u90e8\u7f72\u5728\u751f\u4ea7\u73af\u5883\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u5728\u7ebf\u6301\u7eed\u6709\u65b0\u7684\u6570\u636e\u4ea7\u751f\uff0c\u6570\u636e\u96c6\u4f1a\u6301\u7eed\u66f4\u65b0\uff0c\u5728\u8fd9\u4e2a\u60c5\u51b5\u4e0b\u9700\u8981\u5feb\u901f\u5730\u5bf9\u65b0\u6570\u636e\u6216\u8005\u65b0\u6570\u636e\u96c6\u8fdb\u884c\u5b66\u4e60\u3002\u4f46\u662f\u90e8\u7f72\u73af\u5883\u4e2d\u5f80\u5f80\u4f1a\u53d1\u73b0\uff0c\u5982\u679c\u7528\u4e4b\u524d\u73b0\u6709\u7684\u6a21\u578b\u4f5c\u4e3a\u57fa\u7840\u8fdb\u884c\u8bad\u7ec3\uff0c\u4e5f\u5c31\u662f warm-starting\u3002 \u5bf9\u4e8e\u51f8\u4f18\u5316\u95ee\u9898\uff0cwarm-starting \u4e00\u76f4\u662f\u4e00\u4e2a\u91cd\u8981\u7684\u6280\u5de7\u4e14\u975e\u5e38\u6709\u7528\uff0c\u4f46\u662f\u5bf9\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7f51\u7edc\u6765\u8bf4\uff0c\u51fa\u6765\u7684\u6a21\u578b\u6027\u80fd\u4f1a\u6bd4\u8f83\u5dee\uff0c\u4e14training \u96c6\u4e0a\u6027\u80fd\u663e\u73b0\u4e0d\u51fa\u6765\uff0c\u53cd\u800c\u662fgeneralization\u80fd\u529b\u5728\u4e0b\u964d\u3002\u56e0\u800c\u65f6\u81f3\u4eca\u65e5\u5f88\u591a\u76f8\u5173\u7684\u95ee\u9898\u4e2d\u6211\u4eec\u90fd\u4f1a\u5728\u6bcf\u4e2a\u8fed\u4ee3\u5faa\u73af\u5c06\u6a21\u578b\u521d\u59cb\u5316\u91cd\u65b0\u8bad\u7ec3\u3002 \u5728\u7b2c\u4e00\u4e2a\u5b9e\u9a8c\u4e2d\uff0c\u4f5c\u8005\u4f7f\u7528ResNet, MLP, Logistic regression\u5206\u522b\u5728CIFAR, SVHN\u4e0a \u76f4\u63a5Train\u6240\u6709\u6570\u636e / \u5148train \u524d50%, \u518dtrain \u6240\u6709\u6570\u636e\u3002 \u53d1\u73b0 ResNet\u4e0eMLP\u7684\u6027\u80fd\u4e0d\u8bba\u91c7\u7528SGD \u8fd8\u662fADAM\uff0c \u7b2c\u4e8c\u6b21\u5b9e\u9a8c\u90fd\u6bd4\u7b2c\u4e00\u4e2a\u5b9e\u9a8c\u5dee\u51e0\u4e2a\u70b9\uff0c\u5728\u6bd4\u8f83\u56f0\u96be\u7684CiFAR\u4e0a\u66f4\u4e3a\u660e\u663e\u3002\u800cLogistic Regression\u7684\u6027\u80fd\u5dee\u5219\u4e0d\u5927\uff0c\u56e0\u4e3a\u5b83\u7406\u5e94\u662f\u4e00\u4e2a\u51f8\u4f18\u5316\u8bbe\u7f6e\u3002\u53ef\u4ee5\u7b80\u5355\u5f97\u5230\u51e0\u4e2a\u521d\u6b65\u7684\u7ed3\u8bba: \u6570\u636e\u96c6\u8d8a\u56f0\u96be\uff0cwarm starting\u5e26\u6765\u7684\u6027\u80fd\u4e0b\u964d\u8d8a\u660e\u663e \u51f8\u4f18\u5316\u95ee\u9898LR\u51e0\u4e4e\u4e0d\u53d7warm-start\u5f71\u54cd\u3002 \u6211\u4eec\u5e38\u5e38\u8ba4\u4e3a\u57fa\u4e8e\u90e8\u5206data\u7684warm starting\u53ef\u4ee5\u7ed9\u7f51\u7edc\u4e00\u4e2a\u66f4\u597d\u7684 prior, \u4e14\u5e94\u5f53\u662f\u6709\u7528\u7684\uff0c\u8fd9\u4e5f\u7b26\u5408transfer learning\u7684\u903b\u8f91\u3002\u4f46\u662f\u5b9e\u9a8c\u7ed3\u679c\u4e0e\u8fd9\u4e2a\u9884\u671f\u5e76\u4e0d\u76f8\u7b26\u3002 \u800c\u73b0\u6210\u7684\u8c03\u53c2\u5de5\u5177\u53ef\u4ee5\u5904\u7406\u8fd9\u4e2a\u95ee\u9898\u5417? \u4f5c\u8005\u7f51\u683c\u641c\u7d22\u4e86 batch_size, learning rate, \u7b49\u53c2\u6570\uff0c\u53d1\u73b0\u662f\u80fd\u627e\u5230\u6027\u80fd\u4e0e\u539f\u6765\u6a21\u578b\u76f8\u5f53\u7684\u53c2\u6570\uff0c\u4f46\u662f\u5176\u8bad\u7ec3\u65f6\u957f\u5f80\u5f80\u4e5f\u8981\u5f88\u957f\u624d\u80fd\u5b9e\u73b0\uff0c\u6ca1\u6709\u6bd4\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\u8282\u7701\u8fd0\u7b97\u91cf\u3002\u4f5c\u8005\u6307\u51fa\u8fd9\u80cc\u540e\u5176\u5b9e\u5f88\u53ef\u80fd\u662f\u5fd8\u6389\u4e86\u539f\u6765\u7684\u53c2\u6570\u7279\u6027\uff0c\u51e0\u4e4e\u662f\u91cd\u65b0\u8bad\u7ec3\u4e86\u3002\u4e3a\u4e86\u8bc1\u5b9e\u8fd9\u4e00\u70b9\uff0c\u4f5c\u8005\u8ba1\u7b97\u8fd9\u4e9b\u6027\u80fd\u597d\u7684\u6a21\u578b\u5728\u4e8c\u6b21\u8bad\u7ec3\u524d\u540e\u7684\u6743\u91cd\u7684\u76f8\u5173\u6027\uff0c\u53d1\u73b0\u6027\u80fd\u597d\u7684\u6a21\u578b\u524d\u540e\u6743\u91cd\u51e0\u4e4e\u6ca1\u6709\u76f8\u5173\u6027\u3002\u4e14\u76f8\u5173\u6027\u8d8a\u5927\uff0c\u6027\u80fd\u8d8a\u5dee\uff0c\u4e5f\u5c31\u662f\u7f51\u7edc\u8d8a\u5dee\u8d8a\u597d\u3002\u4f46\u662fLogistic Regression\u5219\u4e0d\u4f1a\u6709\u8fd9\u6837\u7684\u60c5\u51b5\u3002 \u4f5c\u8005\u518d\u505a\u4e86\u4e00\u7ec4\u5b9e\u9a8c\uff0c\u5c31\u662f\u7b2c\u4e00\u8f6e\u8bad\u7ec3\u591a\u5c11\u6b21\u5c31\u4f1a\u5f00\u59cb\u635f\u5bb3generalization\u5462? \u4f5c\u8005\u5728CIFAR-10\u4e0a\u7528ResNet\u8fdb\u884c\u5b9e\u9a8c\uff0c\u6570\u636e\u663e\u793a\u7b2c\u4e00\u8f6e\u5927\u7ea620\u4e2aepochs\u4e4b\u540e\uff0c\u6700\u7ec8\u7684\u6027\u80fd\u5c31\u5df2\u7ecf\u5f00\u59cb\u663e\u8457\u5f80\u4e0b\u964d\u4e86\uff0c \u4f5c\u8005\u91c7\u7528L2 regularization, confidence-penalized training, adversarial training\u53bb\u89c4\u8303\u5316\u4e24\u4e2a\u9636\u6bb5\u7684\u8bad\u7ec3\u6d41\u7a0b\uff0c\u8fd9\u4e9b\u89c4\u8303\u5316\u65b9\u6cd5\u662f\u6709\u6548\u7684\uff0c\u4f46\u662f\u8fdc\u4e0d\u80fd\u5b8c\u5168\u6d88\u706d warm-starting\u5e26\u6765\u7684\u6027\u80fd\u4e0b\u964d\u3002 \u4f5c\u8005\u6700\u7ec8\u63d0\u51fa Shrink, Perturb, Repeat \u7684\u7b97\u6cd5\u3002\u6bcf\u5f53\u65b0\u6570\u636e\u52a0\u5165\u5230\u6570\u636e\u96c6\u65f6\uff0c\u5bf9\u6a21\u578b\u4e2d\u6240\u6709\u53c2\u6570\u4fee\u6b63\u4e3a \\theta_i^t \\leftarrow \\lambda\\theta_i^{t-1} + p^t \u5176\u4e2d p^t \\sim \\mathcal{N}(0, \\sigma^2) \u4e14 0 < \\lambda < 1 . \u76f4\u89c9: \u5c06\u6240\u6709\u6743\u91cd\u7f29\u5c0f\u4e00\u5b9a\u6bd4\u4f8b\u51e0\u4e4e\u4f1a\u4fdd\u7559ReLU\u5206\u7c7b\u6a21\u578b\u7684\u6240\u6709\u8bb0\u5fc6\uff0c\u5bf9\u4e8e\u590d\u6742\u7684\u5e26\u6709BatchNorm\u7684ResNet, \\lambda \u53bb\u5230\u4f4e\u4e8e 0.1 \u540e\u624d\u5f00\u59cb\u635f\u5931\u51c6\u786e\u7387\uff0c\u5bf9\u4e8e\u7b80\u5355\u7684\u5e26\u6709bias\u7684MLP\uff0c\u6743\u91cd\u7f29\u5c0f\u5230 \\lambda < 0.6 \u65f6\u624d\u5f00\u59cb\u635f\u4f24\u51c6\u786e\u7387\u3002 Warm-starting\u4e00\u5927\u95ee\u9898\u5728\u4e8e\u65b0\u6570\u636e\u7684gradients\u4f1a\u8fdc\u8fdc\u5927\u4e8e\u65e7\u6570\u636e\u7684gradient, \u4f7f\u7528shrinking\u7f29\u5c0f\u8f93\u51fa\u7f6e\u4fe1\u5ea6\uff0c\u52a0\u4e0a\u4e00\u70b9\u6270\u52a8\u540e\u5c31\u53ef\u4ee5\u8fdb\u4e00\u6b65\u5747\u8861\u6a21\u578b\u5bf9\u4e0d\u540c\u6570\u636e\u7684\u68af\u5ea6\u3002 \u5b9e\u9a8c\u4e0a \\sigma=0.01, \\lambda=0.6 \u5c31\u53ef\u4ee5\u5f97\u5230\u597d\u7684\u6027\u80fd\u3002 \u4ece\u6570\u503c\u66f4\u65b0\u4e0a\uff0c\u8fd9\u5176\u5b9e\u5f88\u63a5\u8fd1\u4e8e \\theta_i \\leftarrow \\lambda(\\theta_i + \\eta\\frac{\\partial L}{\\partial \\theta_i}) + p , \u5728\u4e00\u5b9a\u60c5\u51b5\u4e0b\u5f88\u63a5\u8fd1\u4e8e L_2 regularization. \u5728\u5b9e\u9645\u5b9e\u9a8c\u4e2d\uff0c\u4f5c\u8005\u53d1\u73b0 shrink-and-perturb \u80fd\u5728\u9759\u6001\u6570\u636e\u96c6\u4e2d\u540c\u6837\u5730\u5b9e\u73b0\u4e0e L_2 \u76f8\u4f3c\u7684regularization,\u4f46\u662f L_2 \u5e76\u4e0d\u80fd\u89e3\u51b3warm start\u95ee\u9898","title":"On Warm-Starting Neural Network Training"},{"location":"The_theory/NIPS2020_networks/#rational-neural-network","text":"pdf code \u8fd9\u7bc7paper\u63d0\u51fa\u4f7f\u7528\u5206\u5f0f\u591a\u9879\u5f0f\u4f5c\u4e3a\u6fc0\u6d3b\u51fd\u6570 F(x) = \\frac{P(x)}{Q(x)} = \\frac{\\sum^{r_P}_{i=0} a_ix^i}{\\sum^{r_Q}_{j=0}b_jx^j} \u8fd9\u5176\u5b9e\u5e76\u975e\u5b8c\u5168\u9996\u521b\uff0c\u672c\u6587\u8bc1\u660e\u4e86\u4e00\u4e9b\u76f8\u5173\u7684\u6027\u8d28\uff0c\u5e76\u4e14\u63d0\u51fa\u4e86\u4f7f\u7528\u5b83\u7684\u65b9\u6cd5\u3002\u672c\u6587\u63d0\u51fa\u7684\u601d\u8def\u662f r_P = 3, r_Q=2 \uff0c\u56e0\u800c\u4e00\u5171\u662f7\u4e2a\u53c2\u6570\uff0c\u540c\u65f6\u5f53x\u8d8b\u5411\u4e8e\u6781\u9650\u7684\u65f6\u5019\uff0c\u6fc0\u6d3b\u51fd\u6570\u4f1a\u8d8b\u5411\u4e8e\u6b63\u6bd4\u4f8b\u51fd\u6570( y=kx ) \u7406\u8bba\u5206\u6790\u7ed3\u8bba(\u672c\u6587\u7684\u4e3b\u8981\u5207\u5165\u89d2\u5ea6\u662f\u5bf9\u62df\u5408\u80fd\u529b\u7684\u63a2\u7d22,\u4e5f\u5373\u662f\u4f7f\u7528\u591a\u5c11\u53c2\u6570\u80fd\u8ba9\u8f93\u51fa\u51fd\u6570\u5c3d\u53ef\u80fd\u62df\u5408\u76ee\u6807\u51fd\u6570): - \u8981\u62df\u5408\u4e00\u4e2aReLU Network\u9700\u8981\u7684Rational Network\u53c2\u6570\u91cf \u6bd4 \u7528ReLU Network\u62df\u5408Rational Network\u7684\u53c2\u6570\u91cf\u8981\u6307\u6570\u7ea7\u7684\u5c11. - \u8981\u62df\u5408\u4e00\u4e2a\u8fde\u7eed\u51fd\u6570\uff0c\u9700\u8981\u7684Rational Network\u7684\u53c2\u6570\u91cf\u6bd4\u7528ReLU Network\u7684\u53c2\u6570\u91cf\u8981\u6307\u6570\u7ea7\u5730\u5c11\u3002 \u80cc\u666f\u77e5\u8bc6\uff0c\u5176\u8bc1\u660e\u65b9\u6cd5\u7c7b\u4f3c\u4e8e Why Deep Neural Networks for Function Approximation? . \u8fd9\u7bc7\u6587\u7ae0\u8bc1\u660e\u4e86\u6df1\u5c42ReLU\u795e\u7ecf\u7f51\u7edc\u62df\u5408\u4e00\u4e2a\u8fde\u7eed\u51fd\u6570\u6240\u9700\u8981\u7684\u53c2\u6570\u91cf\u6bd4\u6d45\u5c42\u7f51\u7edc\u7684\u53c2\u6570\u91cf\u8981\u6307\u6570\u7ea7\u5730\u5c11.","title":"Rational Neural Network"},{"location":"The_theory/NIPS2020_networks/#what-do-neural-network-learn-when-trained-with-random-labels","text":"pdf \u8fd9\u7bc7paper\u7814\u7a76\u4e86\u4f7f\u7528\u968f\u673a\u6807\u7b7e\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u4e4b\u540e\uff0c\u7f51\u7edc\u7a76\u7adf\u5b66\u5230\u4e86\u4ec0\u4e48\uff0c\u5bf9transfer learning\u7a76\u7adf\u6709\u4ec0\u4e48\u7528\u3002 \u5b83\u63a2\u8ba8\u4e86\u4e00\u4e2a\u73b0\u8c61\uff0c\u4e5f\u5c31\u662f\u5728\u4e00\u5b9a\u8d85\u53c2\u7684\u60c5\u51b5\u4e0b\uff0c\u5373\u4f7f\u4f7f\u7528\u968f\u673a\u53d8\u91cf\u8fdb\u884c\u8bad\u7ec3\uff0c\u7f51\u7edc\u8fd8\u662f\u6709\u53ef\u80fd\u80fd\u52a0\u901f\u6536\u655b(\u5c3d\u7ba1 genralization\u7684\u6027\u80fd\u6bd4\u8f83\u5dee) \u4f5c\u8005\u89e3\u91ca\u5e76\u5370\u8bc1\u4e86\u4e00\u4e2a\u91cd\u8981\u7684\u73b0\u8c61\uff0c\u4e5f\u5c31\u662f\u5373\u4f7f\u6ca1\u6709label\uff0c\u8bad\u7ec3\u7684\u8fc7\u7a0b\u4e2d\u5e95\u5c42\u7f51\u7edc\u7684\u6743\u91cd\u4e5f\u4f1a\u5bf9\u8f93\u5165\u6570\u636e\u7684\u7edf\u8ba1\u7279\u5f81\u8fdb\u884c\u590d\u523b(Alignment, defined as same eigenvectors)\uff0c\u53cd\u5e94\u5728\u6743\u91cd\u77e9\u9635\u7684\u4e8c\u9636\u7279\u6027\u4e0a\u3002\u5982\u679c\u6211\u4eec\u6a21\u4eff\u6570\u636e\u7684\u8fd9\u4e2a\u6027\u8d28\u9010\u5c42\u521d\u59cb\u5316\u7f51\u7edc\uff0c\u6211\u4eec\u4e5f\u53ef\u4ee5\u5b9e\u73b0\u5bf9\u7f51\u8def\u7684\u52a0\u901f\u8bad\u7ec3\u3002","title":"What do Neural Network Learn when Trained with Random Labels"},{"location":"The_theory/NIPS2020_networks/#alignment","text":"\u5047\u8bbe d \u7ef4\u8f93\u5165 x \u4ece \\mathcal{N}(0, \\Sigma_x) \u9ad8\u65af\u4e2d\u91c7\u6837\uff0c\u76ee\u6807\u6807\u7b7e\u4e3a\u968f\u673a\u503c\uff0c\u4e14\u521d\u59cb\u6743\u91cd\u4ece\u9ad8\u65af\u4e2d\u63d0\u53d6\uff0c\u90a3\u4e48\u6700\u540e\u5f97\u5230\u7684 d \u7ef4\u6743\u91cd\u6ee1\u8db3 \\mathbb{E}[\\boldsymbol{w}]=0 , \u4e14 \\Sigma_\\omega=\\mathbb{E}[\\omega\\cdot\\omega^T] \u7684\u7279\u5f81\u503c\u4e0e \\Sigma_x \u7684\u7279\u5f81\u503c\u4e00\u81f4\u3002 \u4f5c\u8005\u8bc1\u660e\u4e86\u8fd9\u4e2a\u7ed3\u679c\u4e0e\u6570\u636e\u91cf\uff0c\u7f51\u7edc\u662f\u5168\u8fde\u63a5\u8fd8\u662fconv\uff0c\u5b66\u4e60\u7387\u4e0e\u4f18\u5316\u5668\u7b49\u90fd\u65e0\u5173\u3002","title":"\u5bf9\u4e8e\u9ad8\u65af\u8f93\u5165\u7684Alignment"},{"location":"The_theory/NIPS2020_networks/#_1","text":"\u5b9e\u9a8c\u7528\u771f\u5b9e\u6570\u636e\u96c6\u771f\u5b9e\u56fe\u7247\u968f\u673a\u6807\u7b7e\u4ee5\u53ca\u771f\u5b9e\u6807\u7b7e\u8fdb\u884c\u8bad\u7ec3\uff0c\u4f5c\u8005\u5728\u5b9e\u9a8c\u4e0a\u540c\u6837\u80fd\u5728\u7b2c\u4e00\u5c42\u5377\u79ef\u5c42\u4e0a\u8bc1\u660e\u5b9e\u9a8c\u7684\u73b0\u8c61\u3002\u80cc\u540e\u7684\u76f4\u89c9\u662f\u8bf4\u6700\u5e95\u5c42\u7684 3\\times 3 \u65b9\u5757\u4e0e\u771f\u5b9e\u6807\u7b7e\u51e0\u4e4e\u6ca1\u6709\u76f8\u5173\u6027\uff0c\u56e0\u800c\u7edf\u8ba1\u4e0a\u771f\u5b9e\u6807\u7b7e\u8bad\u7ec3\u5bf9\u4e8e\u7b2c\u4e00\u5c42\u4e5f\u76f8\u5f53\u4e8e\u5c31\u662f\u968f\u673a\u6807\u7b7e\u7684\u7ed3\u679c\u3002 \u5b9e\u9a8c\u4e2d misalignment\u8fdc\u4f4e\u4e8e\u968f\u673a\u6807\u7b7e\u7684\u7ed3\u679c, throughout training process. \u63a5\u7740\u4f5c\u8005\u5bf9\u6d45\u5c42\u7f51\u7edc\u7684\u6743\u91cd\u5728\u521d\u59cb\u5316\u7684\u65f6\u5019\u5148\u7528\u968f\u673asample\u7684\u7ed3\u679c\u8fdb\u884c\u91c7\u6837(\u4e5f\u5c31\u662f\u4e0d\u662f\u7528\u8bad\u7ec3\u7684\u6743\u91cd\uff0c\u800c\u662f\u91c7\u6837\u7684\u6743\u91cd)\uff0c\u53d1\u73b0\u540c\u6837\u80fd\u63d0\u5347\u7f51\u7edc\u7684\u8bad\u7ec3\u901f\u5ea6\u3002\u8bf4\u660e\u7f51\u7edc\u88ab\u968f\u673a\u6807\u7b7e\u9884\u8bad\u7ec3\u540e\u80fd\u63d0\u5347\u901f\u5ea6\u4e3b\u8981\u662f\u56e0\u4e3a\u5bf9\u6570\u636e\u4e8c\u9636\u7279\u6027\u7684alignment. \u5bf9\u4e8e\u66f4\u6df1\u7684\u7f51\u7edc\uff0c\u4f5c\u8005\u6307\u51fa\uff0c\u53ef\u4ee5\u4e00\u5c42\u4e00\u5c42\u5730\u521d\u59cb\u5316\uff0c\u4e5f\u5373\u662f\u5148\u4f7f\u7528\u91c7\u6837\u7684\u65b9\u6cd5\u5f97\u5230\u7b2c\u4e00\u5c42\u7f51\u7edc\u7684\u6743\u91cd\uff0c\u7136\u540e\u628a\u5b83\u4f5c\u4e3a\u8f93\u5165\u91c7\u6837\u7b2c\u4e8c\u5c42\u7684\u6743\u91cd\u3002\u80fd\u5f97\u5230\u4e0epre-train\u540c\u6837\u7684\u52a0\u901f.","title":"\u5b9e\u9a8c"},{"location":"The_theory/NIPS2020_networks/#an-analysis-of-svd-for-deep-rotation-estimation","text":"pdf \u8fd9\u7bc7paper\u5206\u6790\u4e86 SVD\u7528\u4e8e\u6df1\u5ea6\u5b66\u4e60\u4e2d\u65cb\u8f6c\u7684\u4f30\u8ba1\u7684\u7279\u6027\u3002 \u4e0e keypointnet \u5bf9\u65cb\u8f6c\u7684\u63a8\u7406\u8fc7\u7a0b\u76f8\u4f3c\uff0c\u7f51\u7edc\u8f93\u51fa 9 \u7ef4\u7684\u6570\u503c\u8f93\u51fa\uff0c\u5806\u780c\u4e3a\u77e9\u9635 M M \u7684SVD\u5206\u89e3\u7ed3\u679c\u4e3a U\\Sigma V^T , \u5219\u5728\u65cb\u8f6c\u96c6 SO(3) \u4e0a\u7684\u6295\u5f71\u4e3a SVDO^+(M) = U\\Sigma'V^T ,\u5176\u4e2d \\Sigma' = diag(1, ..., 1, det(UV^T)) \u635f\u5931\u51fd\u6570 L(M,R) = ||SVDO^+(M) - R||_F^2 ,\u65cb\u8f6c\u77e9\u9635\u5dee\u7684\u4e8c\u8303\u6570\u5b9e\u8d28\u4e0a\u7b49\u4e8e\u4e24\u4e2a\u65cb\u8f6c\u7684\u5dee(\u4e5f\u662f\u4e00\u4e2a\u65cb\u8f6c\u77e9\u9635)\u7684\u4e3b\u65cb\u8f6c\u89d2\u5ea6. \u4f5c\u8005\u901a\u8fc7\u5206\u6790\u8fd9\u4e00\u635f\u5931\u51fd\u6570\u5bf9SVD\u7684\u53cd\u4f20\u7ed3\u679c\u8bc1\u660e\u4e86\u8fd9\u79cd\u8868\u793a\u65b9\u6cd5\u662f\u5728\u6700\u5c0f\u4e8c\u4e58\u6216\u662f\u9ad8\u65af\u6982\u7387\u566a\u58f0\u4e24\u79cd\u8868\u8ff0\u4e0b\u90fd\u662f\u6700\u4f18\u7684\uff0cSVD\u7684\u53cd\u4f20\u6bd4\u8f83\u590d\u6742. Derivative of SVD","title":"An Analysis of SVD for Deep Rotation Estimation"},{"location":"The_theory/NIPS2020_networks/#is-normalization-indispensable-for-training-deep-neural-networks","text":"pdf code \u8fd9\u7bc7paper\u63d0\u51fa\u7684\u5185\u5bb9\u662f\u53ef\u4ee5\u901a\u8fc7rescaling \u5904\u7406\u53ef\u4ee5\u5728\u4e0d\u4f7f\u7528batchnorm\u7684\u60c5\u51b5\u4e0b\u7ef4\u6301\u7f51\u7edc\u7684variance. \u8fd9\u7bc7paper\u540c\u65f6\u901a\u8fc7\u5b9e\u9a8c\u548c\u6570\u5b66\u4e0b\u4e86\u4e00\u4e2a\u5224\u65ad\uff0c\u8ba4\u4e3a\u5728\u6df1\u5ea6\u7f51\u7edc\u4e2d Dead ReLU \u662f\u7531\u7cfb\u7edf\u7684variance\u5f15\u8d77\u7684\uff0c\u6311\u9009\u597d\u7684\u521d\u59cb\u5316\u4e5f\u6ca1\u7528\uff0c\u8bad\u7ec3\u540e\u8fd8\u662f\u4f1a\u6709\u5f88\u591a\u7684 dead relu\uff0c\u66f4\u591a\u7684\u8bba\u8bc1\u5728\u9644\u5f55\u4f46\u5c1a\u672a\u653e\u51fa. \u4f5c\u8005\u63d0\u51fa rescalenet\uff0c\u6838\u5fc3\u601d\u8def\u662f\u60f3\u7f51\u7edc\u7684\u65b9\u5dee\u4e0d\u8981\u53d1\u6563\uff0c\u540c\u65f6\u7ecf\u8fc7\u591a\u5c42\u53e0\u52a0\u653e\u7f6e\u540e\uff0c\u6bcf\u4e00\u4e2a resblock\u7684\u5377\u79ef\u5c42\u8f93\u51fa\u5728\u6700\u540e\u8f93\u51fa\u4e0a\u7684\u6743\u91cd\u662f\u4e00\u81f4\u7684\u3002 \u6c42\u89e3\u8fd9\u4e24\u4e2a\u6761\u4ef6(\u5177\u4f53\u89e3\u6cd5\u5c1a\u672a\u653e\u51fa): \\operatorname{Var}\\left[\\boldsymbol{x}_{k}\\right]=\\alpha_{k}^{2} \\operatorname{Var}\\left[\\boldsymbol{x}_{k-1}\\right]+\\beta_{k}^{2} \\operatorname{Var}\\left[\\mathcal{F}_{k}\\left(\\boldsymbol{x}_{k-1}\\right)\\right]=\\operatorname{Var}\\left[\\boldsymbol{x}_{k-1}\\right] \\boldsymbol{x}_{L}=\\left(\\prod_{i=1}^{L} \\alpha_{i}\\right) \\boldsymbol{x}_{0}+\\sum_{k=1}^{L} \\beta_{k} \\prod_{i=k+1}^{L} \\alpha_{i} \\mathcal{F}_{k}\\left(\\boldsymbol{x}_{k-1}\\right) \u7ed9\u51fa: \\boldsymbol{x}_{k}=\\sqrt{\\frac{k-1+L}{k+L}} \\boldsymbol{x}_{k-1}+\\sqrt{\\frac{1}{k+L}} \\mathcal{F}_{k}\\left(\\boldsymbol{x}_{k-1}, \\Theta_{k}\\right) \u5176\u4ed6\u4e24\u4e2a\u6280\u5de7: \u4e8b\u5b9e\u4e0a\u5728\u8bad\u7ec3\u7684\u65f6\u5019\u4e0d\u4e00\u5b9a\u8981\u8ba9\u4e0d\u540cresblock\u5c42\u6570\u4e0a\u7684\u6743\u91cd\u4e00\u81f4\uff0c\u56e0\u4e3a\u7ecf\u9a8c\u4e0a\u6765\u8bf4\u4e0d\u540c\u7f51\u7edc\u5c42\u6210\u719f\u901f\u5ea6\u5c31\u662f\u4e0d\u4e00\u6837\u7684\uff0c\u5728\u4e0d\u540c\u7684\u8bad\u7ec3\u9636\u6bb5\u6709\u4e0d\u540c\u7684\u9700\u6c42\uff0c\u6240\u4ee5\u672c\u6587\u63d0\u51fa\u7ed9\u8fd9\u4e2a\u6bd4\u4f8b\u53c2\u6570\u4e00\u4e2a\u53ef\u5b66\u4e60\u7684\u4e58\u5b50: \\boldsymbol{x}_{k}=\\sqrt{\\frac{k-1+L}{k+L}} \\boldsymbol{x}_{k-1}+\\frac{m_{k}}{\\sqrt{L}} \\mathcal{F}_{k}\\left(\\boldsymbol{x}_{k-1}\\right) Bias\u7684\u521d\u59cb\u5316\u4f5c\u8005\u4e5f\u63d0\u51fa\u65b0\u7684\u89c2\u70b9\uff0c\u9996\u5148\u5c06\u8fd0\u7b97 y=Wx + b \u91cd\u65b0\u7406\u89e3\u4e3a y = W(x + b) \u800c\u8fd9\u4e2a b\u7684\u521d\u59cb\u5316\u4e3a\u7b2c\u4e00\u4e2a\u8f93\u5165\u7684mini-batch\u7684\u5747\u503c\u7684\u8d1f\u503c\uff0c\u4e5f\u5c31\u662f\u8ba9\u7b2c\u4e00\u4e2abatch\u5728\u4e0e\u6743\u91cd\u76f8\u4e58\u4e4b\u524d\u63a5\u8fd1\u662f normalized\u7684 (\u5728\u4ee3\u7801\u5b9e\u73b0\u4e0a\uff0c\u662f\u5b9a\u4e0b\u4e86\u4e00\u4e2athreshold, \u524dn=8\u4e2abatch\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u90fd\u4f1a\u6c42\u7d2f\u79ef\u5747\u503c)\uff0c\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u5b9e\u73b0\u4e86BN\u7684\u6548\u679c. \u540c\u65f6\u4f5c\u8005\u6307\u51fa\u8fd9\u4e2a\u6280\u5de7\u53ef\u4ee5\u663e\u8457\u51cf\u8f7brelu\u7684\u95ee\u9898.","title":"Is normalization indispensable for training deep neural networks"},{"location":"The_theory/NTK/","text":"Neural Tangent Kernel: Convergence and Generalization in Neural Networks \u8fd9\u7bc7paper\u662f 2018 NIPS\u4e00\u7bc7\u6bd4\u8f83\u6709\u5f71\u54cd\u529b\u7684paper youtube tutorial Great Code demonstration Prior Experiments \u8fd9\u91cc\u5148\u6839\u636e\u8fd9\u4e2acode demonstration \u8865\u5145\u8bf4\u660e\u51e0\u4e2a\u91cd\u8981\u7684\u76f4\u89c9\u6982\u5ff5. \u968f\u673a\u521d\u59cb\u5316100\u4e2a\u5355\u8f93\u5165\u5355\u8f93\u51fa100\u4e2a\u9690\u85cf\u5355\u5143\u7684\u4e24\u5c42\u795e\u7ecf\u7f51\u7edc\uff0c\u8fd9\u4e9b\u7f51\u7edc\u7684\u8f93\u51fa\u63a5\u8fd1\u4e8e\u4e00\u4e2a\u57280\u9644\u8fd1\u7684\u9ad8\u65af\u8fc7\u7a0b\u7684\u8f93\u51fa\u5206\u5e03. \u5bf9\u4e8e\u4e00\u4e2a\u53ea\u6709(0, 0)\u6570\u636e\u7684\u9ad8\u65af\u8fc7\u7a0b \u5728\u4e00\u4e2a\u5c0f\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u8bad\u7ec3\u8fc7\u7a0b \u9ad8\u65af\u8fc7\u7a0b\u7684\u7ed3\u679c \u4e00\u4e2a\u6bd4\u8f83\u62bd\u8c61\u7684\u7ed3\u8bba\u5c31\u662f\u65e0\u9650\u5bbd\u7684\u795e\u7ecf\u7f51\u7edc\u7b49\u4ef7\u4e8e\u9ad8\u65af\u8fc7\u7a0b\u3002 \u7b2c\u4e8c\u4e2a\u7ed3\u8bba\u662f\u8bad\u7ec3\u8fc7\u7a0b\u6743\u91cd\u7684\u76f8\u5bf9\u53d8\u5316\u662f\u5f88\u5c0f\u7684 NTK \u5bf9\u7f51\u7edc\u8fdb\u884c\u6cf0\u52d2\u5c55\u5f00 f(x, w) = f(x, w_0) + \\nabla_w f(x, w_0)^T (w - w_0) \u53ef\u4ee5\u8bc1\u660e\u968f\u7740\u7f51\u7edc\u5bbd\u5ea6\u589e\u5927\uff0c \\text{relative change in model Jacobian} \\approx \\frac{d \\cdot \\text{rate of change of Jacobian}}{\\text{norm of Jacobian}} = \\frac{d \\cdot \\Vert \\nabla_w^2 \\boldsymbol y (\\boldsymbol{w_0})\\Vert}{\\Vert \\nabla_w \\boldsymbol y(\\boldsymbol{w_0})\\Vert} = {\\color{blue} \\Vert( \\boldsymbol y (\\boldsymbol w_0) - \\boldsymbol{\\bar y})\\Vert \\frac{\\Vert \\nabla_w^2 \\boldsymbol {y(w_0)} \\Vert}{\\Vert \\nabla_w \\boldsymbol {y(w_0)} \\Vert^2}}{\\color{red} \\ll 1} \u7f51\u7edc\u7684\u6743\u91cd\u7684\u53d8\u5316\u4e3a w_{k+1} = w_{k} - \\eta \\nabla_w L(w_k) , \u5176\u5173\u4e8e\u65f6\u95f4\u7684\u5fae\u5206\u4e3a \\dot w(t) = -\\nabla L(w(t)) = - \\nabla y(w)(y(w) - \\bar y) \u5219\u8f93\u51fa\u503c\u7684\u53d8\u5316\u7387, \\dot{\\boldsymbol{y}}(\\boldsymbol{w}) = \\nabla \\boldsymbol{y}(\\boldsymbol{w})^T \\boldsymbol{\\dot w} = - {\\color{red}\\nabla \\boldsymbol{y}(\\boldsymbol{w})^T \\nabla \\boldsymbol{y}(\\boldsymbol{w})} (\\boldsymbol{y}(\\boldsymbol{w}) - \\boldsymbol{\\bar y}) \u795e\u7ecf\u7f51\u7edc\u7684\u68af\u5ea6\u96c5\u514b\u6bd4\u77e9\u9635 J = \\nabla y(w) \uff0c NTK\u6307\u7684\u5c31\u662f J^T J . \u53ef\u4ee5\u8bc1\u660e\u8fc7\u53c2\u6570\u7f51\u7edc\u4f1a\u4ee5\u6307\u6570\u901f\u5ea6\u6536\u655b\u4e8e\u8bad\u7ec3\u635f\u5931\u4e3a\u96f6\u7684\u4f4d\u7f6e\u3002\u3000 \u89e3\u91ca\u7f51\u7edc\u4e3a\u4ec0\u4e48\u53ef\u4ee5\u8bad\u7ec3 \u540c\u65f6\u53ef\u4ee5\u53d1\u73b0\u8fd9\u4e2aNTK\u7531\u4e8e\u68af\u5ea6\u4e0b\u964d\u7684\u539f\u56e0\uff0c\u4f1a\u6709\u4e00\u4e2aimplicit l2-norm (\u76f8\u5f53\u4e8e\u6709\u4e00\u4e2a0-0\u7684\u6570\u636e\u70b9). \u89e3\u91ca\u7f51\u7edc\u4e3a\u4ec0\u4e48\u53ef\u4ee5generalize","title":"Neural Tangent Kernel: Convergence and Generalization in Neural Networks"},{"location":"The_theory/NTK/#neural-tangent-kernel-convergence-and-generalization-in-neural-networks","text":"\u8fd9\u7bc7paper\u662f 2018 NIPS\u4e00\u7bc7\u6bd4\u8f83\u6709\u5f71\u54cd\u529b\u7684paper youtube tutorial Great Code demonstration","title":"Neural Tangent Kernel: Convergence and Generalization in Neural Networks"},{"location":"The_theory/NTK/#prior-experiments","text":"\u8fd9\u91cc\u5148\u6839\u636e\u8fd9\u4e2acode demonstration \u8865\u5145\u8bf4\u660e\u51e0\u4e2a\u91cd\u8981\u7684\u76f4\u89c9\u6982\u5ff5. \u968f\u673a\u521d\u59cb\u5316100\u4e2a\u5355\u8f93\u5165\u5355\u8f93\u51fa100\u4e2a\u9690\u85cf\u5355\u5143\u7684\u4e24\u5c42\u795e\u7ecf\u7f51\u7edc\uff0c\u8fd9\u4e9b\u7f51\u7edc\u7684\u8f93\u51fa\u63a5\u8fd1\u4e8e\u4e00\u4e2a\u57280\u9644\u8fd1\u7684\u9ad8\u65af\u8fc7\u7a0b\u7684\u8f93\u51fa\u5206\u5e03. \u5bf9\u4e8e\u4e00\u4e2a\u53ea\u6709(0, 0)\u6570\u636e\u7684\u9ad8\u65af\u8fc7\u7a0b \u5728\u4e00\u4e2a\u5c0f\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u8bad\u7ec3\u8fc7\u7a0b \u9ad8\u65af\u8fc7\u7a0b\u7684\u7ed3\u679c \u4e00\u4e2a\u6bd4\u8f83\u62bd\u8c61\u7684\u7ed3\u8bba\u5c31\u662f\u65e0\u9650\u5bbd\u7684\u795e\u7ecf\u7f51\u7edc\u7b49\u4ef7\u4e8e\u9ad8\u65af\u8fc7\u7a0b\u3002 \u7b2c\u4e8c\u4e2a\u7ed3\u8bba\u662f\u8bad\u7ec3\u8fc7\u7a0b\u6743\u91cd\u7684\u76f8\u5bf9\u53d8\u5316\u662f\u5f88\u5c0f\u7684","title":"Prior Experiments"},{"location":"The_theory/NTK/#ntk","text":"\u5bf9\u7f51\u7edc\u8fdb\u884c\u6cf0\u52d2\u5c55\u5f00 f(x, w) = f(x, w_0) + \\nabla_w f(x, w_0)^T (w - w_0) \u53ef\u4ee5\u8bc1\u660e\u968f\u7740\u7f51\u7edc\u5bbd\u5ea6\u589e\u5927\uff0c \\text{relative change in model Jacobian} \\approx \\frac{d \\cdot \\text{rate of change of Jacobian}}{\\text{norm of Jacobian}} = \\frac{d \\cdot \\Vert \\nabla_w^2 \\boldsymbol y (\\boldsymbol{w_0})\\Vert}{\\Vert \\nabla_w \\boldsymbol y(\\boldsymbol{w_0})\\Vert} = {\\color{blue} \\Vert( \\boldsymbol y (\\boldsymbol w_0) - \\boldsymbol{\\bar y})\\Vert \\frac{\\Vert \\nabla_w^2 \\boldsymbol {y(w_0)} \\Vert}{\\Vert \\nabla_w \\boldsymbol {y(w_0)} \\Vert^2}}{\\color{red} \\ll 1} \u7f51\u7edc\u7684\u6743\u91cd\u7684\u53d8\u5316\u4e3a w_{k+1} = w_{k} - \\eta \\nabla_w L(w_k) , \u5176\u5173\u4e8e\u65f6\u95f4\u7684\u5fae\u5206\u4e3a \\dot w(t) = -\\nabla L(w(t)) = - \\nabla y(w)(y(w) - \\bar y) \u5219\u8f93\u51fa\u503c\u7684\u53d8\u5316\u7387, \\dot{\\boldsymbol{y}}(\\boldsymbol{w}) = \\nabla \\boldsymbol{y}(\\boldsymbol{w})^T \\boldsymbol{\\dot w} = - {\\color{red}\\nabla \\boldsymbol{y}(\\boldsymbol{w})^T \\nabla \\boldsymbol{y}(\\boldsymbol{w})} (\\boldsymbol{y}(\\boldsymbol{w}) - \\boldsymbol{\\bar y}) \u795e\u7ecf\u7f51\u7edc\u7684\u68af\u5ea6\u96c5\u514b\u6bd4\u77e9\u9635 J = \\nabla y(w) \uff0c NTK\u6307\u7684\u5c31\u662f J^T J . \u53ef\u4ee5\u8bc1\u660e\u8fc7\u53c2\u6570\u7f51\u7edc\u4f1a\u4ee5\u6307\u6570\u901f\u5ea6\u6536\u655b\u4e8e\u8bad\u7ec3\u635f\u5931\u4e3a\u96f6\u7684\u4f4d\u7f6e\u3002\u3000 \u89e3\u91ca\u7f51\u7edc\u4e3a\u4ec0\u4e48\u53ef\u4ee5\u8bad\u7ec3 \u540c\u65f6\u53ef\u4ee5\u53d1\u73b0\u8fd9\u4e2aNTK\u7531\u4e8e\u68af\u5ea6\u4e0b\u964d\u7684\u539f\u56e0\uff0c\u4f1a\u6709\u4e00\u4e2aimplicit l2-norm (\u76f8\u5f53\u4e8e\u6709\u4e00\u4e2a0-0\u7684\u6570\u636e\u70b9). \u89e3\u91ca\u7f51\u7edc\u4e3a\u4ec0\u4e48\u53ef\u4ee5generalize","title":"NTK"},{"location":"The_theory/Rethinking%20ImageNet%20Pre-training/","text":"Rethinking ImageNet Pre-training \u8fd9\u7bc7\u8bba\u6587\u6765\u81ea\u4f55\u51ef\u660e\u7684\u8bba\u6587\u8ba8\u8bba\u4e86pretraining\u5bf9detection task\u7684\u5f71\u54cd\uff0c\u5206\u6790\u4e86\u6570\u4e2a\u8981\u7d20 \u4e3b\u8981\u7ed3\u8bba pretrained\u52a0\u901f\u6536\u655b imagenet pretrained\u4e0d\u4e00\u5b9a\u63d0\u5347regularization\uff0c\u9664\u975e\u539f\u6765\u6570\u636e\u96c6\u91cf\u771f\u7684\u5f88\u5c0f \u5f53\u8bad\u7ec3\u4efb\u52a1\u5bf9\u4f4d\u7f6e\u4fe1\u606f\u975e\u5e38\u654f\u611f\u65f6\uff0c\u6bd4\u5982key-point\u68c0\u6d4b\uff0cimagenet pretrained\u7528\u5904\u4e0d\u5927 \u5176\u4ed6\u6280\u672f\u7ec6\u8282 Normalization\u5fc5\u4e0d\u53ef\u5c11\uff0c\u4f46\u662f\u7531\u4e8eDetection\u9ad8\u6e05\u56fe\u8981\u6c42\u9ad8\uff0c\u663e\u5b58\u4e0d\u591f\uff0c\u6240\u4ee5\u5982\u679c\u9700\u8981\u4ece\u5934\u5f00\u59cbtrain batch normalization\u4f1a\u56e0\u4e3abatch\u592a\u5c0f\u5f71\u54cd\u6548\u679c\uff0c\u6240\u4ee5\u5c1d\u8bd5GroupNorm\u7b49\u3002 \u5bf9\u4e8e\u6570\u636e\u91cf\u8db3\u591f\u5927\u7684detection task\u6765\u8bf4\uff0cpretrain\u53ef\u4ee5\u4f7f\u7ed3\u679c\u66f4\u5feb\u6536\u655b\uff0c\u4f46\u662frandom-initialization\u8db3\u591f\u957fepoch\u540e\u5f97\u5230\u7684\u7ed3\u679c\u4e00\u822c\u4e0d\u4f1a\u5dee\u4e8epretrain\uff0c\u5f53\u7136\u8981\u6c42\u6709GN \u4f7f\u7528\u521d\u59cb\u5b66\u4e60\u7387(\u8f83\u5927\u7684\u5b66\u4e60\u7387)\uff0c\u8bad\u7ec3\u66f4\u957f\u7684\u65f6\u95f4\u662f\u6709\u7528\u7684\uff0c\u957f\u65f6\u95f4\u4f7f\u7528\u4f4e\u5b66\u4e60\u7387\u63d0\u9ad8\u51c6\u786e\u7387\u7ecf\u5e38\u4f1a\u5bfc\u81f4overfitting","title":"Rethinking ImageNet Pre-training"},{"location":"The_theory/Rethinking%20ImageNet%20Pre-training/#rethinking-imagenet-pre-training","text":"\u8fd9\u7bc7\u8bba\u6587\u6765\u81ea\u4f55\u51ef\u660e\u7684\u8bba\u6587\u8ba8\u8bba\u4e86pretraining\u5bf9detection task\u7684\u5f71\u54cd\uff0c\u5206\u6790\u4e86\u6570\u4e2a\u8981\u7d20","title":"Rethinking ImageNet Pre-training"},{"location":"The_theory/Rethinking%20ImageNet%20Pre-training/#_1","text":"pretrained\u52a0\u901f\u6536\u655b imagenet pretrained\u4e0d\u4e00\u5b9a\u63d0\u5347regularization\uff0c\u9664\u975e\u539f\u6765\u6570\u636e\u96c6\u91cf\u771f\u7684\u5f88\u5c0f \u5f53\u8bad\u7ec3\u4efb\u52a1\u5bf9\u4f4d\u7f6e\u4fe1\u606f\u975e\u5e38\u654f\u611f\u65f6\uff0c\u6bd4\u5982key-point\u68c0\u6d4b\uff0cimagenet pretrained\u7528\u5904\u4e0d\u5927","title":"\u4e3b\u8981\u7ed3\u8bba"},{"location":"The_theory/Rethinking%20ImageNet%20Pre-training/#_2","text":"Normalization\u5fc5\u4e0d\u53ef\u5c11\uff0c\u4f46\u662f\u7531\u4e8eDetection\u9ad8\u6e05\u56fe\u8981\u6c42\u9ad8\uff0c\u663e\u5b58\u4e0d\u591f\uff0c\u6240\u4ee5\u5982\u679c\u9700\u8981\u4ece\u5934\u5f00\u59cbtrain batch normalization\u4f1a\u56e0\u4e3abatch\u592a\u5c0f\u5f71\u54cd\u6548\u679c\uff0c\u6240\u4ee5\u5c1d\u8bd5GroupNorm\u7b49\u3002 \u5bf9\u4e8e\u6570\u636e\u91cf\u8db3\u591f\u5927\u7684detection task\u6765\u8bf4\uff0cpretrain\u53ef\u4ee5\u4f7f\u7ed3\u679c\u66f4\u5feb\u6536\u655b\uff0c\u4f46\u662frandom-initialization\u8db3\u591f\u957fepoch\u540e\u5f97\u5230\u7684\u7ed3\u679c\u4e00\u822c\u4e0d\u4f1a\u5dee\u4e8epretrain\uff0c\u5f53\u7136\u8981\u6c42\u6709GN \u4f7f\u7528\u521d\u59cb\u5b66\u4e60\u7387(\u8f83\u5927\u7684\u5b66\u4e60\u7387)\uff0c\u8bad\u7ec3\u66f4\u957f\u7684\u65f6\u95f4\u662f\u6709\u7528\u7684\uff0c\u957f\u65f6\u95f4\u4f7f\u7528\u4f4e\u5b66\u4e60\u7387\u63d0\u9ad8\u51c6\u786e\u7387\u7ecf\u5e38\u4f1a\u5bfc\u81f4overfitting","title":"\u5176\u4ed6\u6280\u672f\u7ec6\u8282"},{"location":"The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/","text":"ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design \u8fd9\u7bc7\u6587\u7ae0\u4e0d\u4ec5\u5f15\u8fdb\u4e86ShuffleNet V2,\u66f4\u91cd\u8981\u7684\u662f\u63d0\u4f9b\u4e86\u5927\u91cf\u4f18\u5316\u7f51\u7edc\u7ed3\u6784\u3001\u52a0\u901f\u8fd0\u7b97\u3001\u63d0\u5347\u6548\u7387\u7684\u7f51\u7edc\u642d\u5efa\u5efa\u8bae\u3002 \u4e3a\u4ec0\u4e48FLOPs\u6307\u6807\u8fd8\u4e0d\u5145\u5206 FLOPs\u6307\u7684\u662f\u7f51\u7edc\u8fd0\u7b97\u8fc7\u7a0b\u4e2d\u9700\u8981\u6267\u884c\u7684\u4e58\u52a0\u6b65\u9aa4\u6b21\u6570\u3002 \u4f17\u6240\u5468\u77e5\uff0c\u5927\u91cf\u7f51\u7edc\u5982MobileNet,\u91c7\u7528group convolution \u4e0e depth-wise convolution,\u4e0edense convolution\u7f51\u7edc\u76f8\u6bd4\uff0c\u51e0\u4f55\u7ea7\u5730\u964d\u4f4e\u4e86\u6bcf\u4e00\u5c42\u7684FLOPs.\u4f46\u662fFLOPs\u5e76\u4e0d\u662f\u4e00\u4e2a\u5bf9\u8fd0\u7b97\u901f\u5ea6\u4e0e\u8fd0\u7b97\u590d\u6742\u5ea6\u6700\u76f4\u63a5\u7684\u4f30\u91cf\u6307\u6807\uff0c\u5b83\u53ea\u662f\u4e00\u4e2a\u8fd1\u4f3c\u3002\u4e0d\u540c\u7684\u7f51\u7edc\uff0c\u5c3d\u7ba1\u6709\u76f8\u4f3c\u7684FLOPs\uff0c\u5176\u901f\u5ea6\u4e5f\u4f1a\u6709\u5f88\u5927\u4e0d\u540c\u3002 FLOPs\u4e0e\u5b9e\u9645\u5ef6\u8fdf\u4e4b\u95f4\u4e00\u5927\u5dee\u522b\u5728\u4e8e\u6ca1\u6709\u8003\u8651Memory access cost(MAC),\u7b2c\u4e8c\u5927\u533a\u522b\u5728\u4e8e\u5e73\u884c\u8fd0\u7b97\u5ea6(degree of parallelism).\u540c\u65f6\u76f8\u540c\u7684\u8fd0\u7b97\u7ed3\u679c\u4f1a\u56e0\u5e73\u53f0\u800c\u5f02\uff0c \u524d\u6587 \u63d0\u51fa\u7684\u901a\u8fc7matrix decomposition\u964d\u7ef4\u52a0\u901f\uff0c\u7406\u8bba\u4e0a\u6765\u8bf4\u80fd\u6709\u66f4\u4f4e\u7684FLOPs,\u4f46\u662f\u5728GPU\u4e0a\u7684\u6267\u884c\u901f\u5ea6\u5374\u66f4\u6162\uff0c\u539f\u56e0\u662f\u67d0\u4e00\u4e2a\u7248\u672c\u7684CUDNN\u4e3a 3\\times 3 \u5377\u79ef\u7279\u6b8a\u4f18\u5316\u8fc7\u3002 \u56e0\u6b64\u63d0\u51fa\u5efa\u8bae\u5e94\u8be5\u7528\u901f\u5ea6\u800c\u975eFLOPs\u8fdb\u884c\u8ba8\u8bba\uff0c\u5e76\u4e14\u9700\u8981\u8bf4\u660e\u5e73\u53f0\u3002 \u672c\u6587\u7684\u8d21\u732e\u5c31\u662f\u5148\u63d0\u51fa\u4e86\u8bbe\u8ba1\u9ad8\u6548\u7f51\u7edc\u7684\u4e00\u4e9b\u5efa\u8bae\uff0c\u5e76\u63d0\u51fashuffleNet V2 \u5b9e\u7528\u5efa\u8bae \u5efa\u8bae\u4e00,\u4f7f\u7528\u7b49channel\u5bbd\u5ea6\u4ee5\u51cf\u5c11MAC 1 \\times 1 \u5377\u79ef\u7684FLOPs\u4e3a B = hwc_1c_2 ,\u5047\u8bbe\u5185\u5b58\u8db3\u591f\u5927\uff0cMAC\u4e3a MAC=hw(c_1+c_2)+c_1c_2 \u8fdb\u4e00\u6b65\u63a8\u5f97 MAC \\ge 2\\sqrt{hwB} + \\frac{B}{hw} \u56e0\u6b64\u5bf9\u4e8e\u76f8\u7b49\u7684FLOPs\u4ee5\u53ca\u76f8\u540c\u7684feature map\u5927\u5c0f\uff0c\u8f93\u5165\u8f93\u51fachannel\u6570\u4e00\u81f4\u65f6\uff0cMAC\u6700\u5c0f\u3002\u5c3d\u7ba1\u8fd9\u53ea\u662f\u7406\u8bba\u503c\uff0c\u4f46\u662f\u5b9e\u9a8c\u53ef\u4ee5\u8bc1\u660e\u8fd9\u4e00\u70b9(\u672c\u6587\u5bf9\u4e0d\u540c\u7684 1\\times 1 \u5377\u79ef\u7684c1,c2\u914d\u6bd4\u8fdb\u884c\u4e86\u5b9e\u9a8c\u5bf9\u6bd4\uff0c\u8bc1\u660e\u4e86\u8fd9\u70b9)\u3002 \u5efa\u8bae\u4e8c\uff0c\u8fc7\u591a\u7684group convolution\u63d0\u5347\u4e86MAC \u7406\u8bba\u5206\u6790 \\begin{aligned} \\mathrm{MAC} &=h w\\left(c_{1}+c_{2}\\right)+\\frac{c_{1} c_{2}}{g} \\\\ &=h w c_{1}+\\frac{B g}{c_{1}}+\\frac{B}{h w} \\end{aligned} MAC\u4e0e g the number of groups \u6b63\u76f8\u5173\u3002 \u5728\u540c\u7b49FLOPs\u7684\u60c5\u51b5\u4e0b\uff0cgroup number\u8d8a\u5927\u901f\u5ea6\u8d8a\u6162\uff0c\u4f46\u662f\u503c\u5f97\u6ce8\u610f\u7684\u662f\u4e3a\u4e86\u4fdd\u8bc1\u540c\u7b49FLOPs,\u589e\u5927group number\u65f6channel\u6570\u4e5f\u4f1a\u63d0\u5347\u3002\u6548\u679c\u5728GPU\u4e0a\u6bd4\u8f83\u660e\u663e\u3002 \u5efa\u8bae\u4e09\uff0c\u7f51\u7edc\u7684\u788e\u7247\u5316\u964d\u4f4e\u4e86\u5e76\u884c\u5ea6 \u8fd9\u91cc\u6307\u7684\u662f\u4e00\u4e2ablock\u91cc\u9762\u5e76\u884c\u7684\u5377\u79ef\u4e0epooling\u5c42\uff0c\u8fd9\u4e9b\u5e73\u884c\u4f46\u4e0d\u5e76\u884c\u7684\u8fd0\u7b97\u4f1a\u591a\u6b21\u89e6\u53d1GPU\u7684\u542f\u52a8\u4e0e\u540c\u6b65\u3002\u788e\u7247\u5316\u8fd0\u884c\u5728GPU\u4e0a\u5f71\u54cd\u8f83\u5927\uff0c\u5728CPU\u4e0a\u5f71\u54cd\u4e0d\u5927 \u5efa\u8bae\u56db\uff0cReLU,\u5143\u7d20\u95f4\u76f8\u52a0\u7b49element-wise operators\u540c\u6837\u4e0d\u53ef\u5ffd\u7565 \u5220\u9664ResNet\u4e2d\u7684ReLU\u4e0eshortcut\u4f1a\u770b\u5230\u7ea620%\u7684\u52a0\u901f\uff0c\u5c3d\u7ba1\u5b83\u4eec\u5f88\u5fc5\u8981\uff0c\u4f46\u662f\u5374\u662f\u4e8b\u5b9e\u4e0a\u5f71\u54cd\u901f\u5ea6\u7684\u3002 ShuffleNet V2: \u57fa\u672c\u5355\u5143\u7ed3\u6784\u53ca\u5176\u4e0eShuffleNet V1\u7684\u6bd4\u8f83\u5982\u56fe \u76f4\u89c9\uff1a \u7528 1\\times 1 \u5377\u79ef\u66ff\u4ee3Group Convolution+ channel shuffle \u4f7f\u7528Concat + Channel Shuffle\u66ff\u4ee3Add downsampling\u65f6\u4e0d\u4f7f\u7528avg pooling Channel split\u5c31\u662f\u7b80\u5355\u5730torch.split(dim=1) \u8fde\u7eed\u51fa\u73b0\u7684Concat, Channel Shuffle, Channel Split\u53ef\u4ee5\u653e\u5728\u4e00\u8d77\u4f5c\u4e3a\u4e00\u4e2a\u7edf\u4e00\u7684element-wise operation\u3002","title":"ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design"},{"location":"The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/#shufflenet-v2-practical-guidelines-for-efficient-cnn-architecture-design","text":"\u8fd9\u7bc7\u6587\u7ae0\u4e0d\u4ec5\u5f15\u8fdb\u4e86ShuffleNet V2,\u66f4\u91cd\u8981\u7684\u662f\u63d0\u4f9b\u4e86\u5927\u91cf\u4f18\u5316\u7f51\u7edc\u7ed3\u6784\u3001\u52a0\u901f\u8fd0\u7b97\u3001\u63d0\u5347\u6548\u7387\u7684\u7f51\u7edc\u642d\u5efa\u5efa\u8bae\u3002","title":"ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design"},{"location":"The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/#flops","text":"FLOPs\u6307\u7684\u662f\u7f51\u7edc\u8fd0\u7b97\u8fc7\u7a0b\u4e2d\u9700\u8981\u6267\u884c\u7684\u4e58\u52a0\u6b65\u9aa4\u6b21\u6570\u3002 \u4f17\u6240\u5468\u77e5\uff0c\u5927\u91cf\u7f51\u7edc\u5982MobileNet,\u91c7\u7528group convolution \u4e0e depth-wise convolution,\u4e0edense convolution\u7f51\u7edc\u76f8\u6bd4\uff0c\u51e0\u4f55\u7ea7\u5730\u964d\u4f4e\u4e86\u6bcf\u4e00\u5c42\u7684FLOPs.\u4f46\u662fFLOPs\u5e76\u4e0d\u662f\u4e00\u4e2a\u5bf9\u8fd0\u7b97\u901f\u5ea6\u4e0e\u8fd0\u7b97\u590d\u6742\u5ea6\u6700\u76f4\u63a5\u7684\u4f30\u91cf\u6307\u6807\uff0c\u5b83\u53ea\u662f\u4e00\u4e2a\u8fd1\u4f3c\u3002\u4e0d\u540c\u7684\u7f51\u7edc\uff0c\u5c3d\u7ba1\u6709\u76f8\u4f3c\u7684FLOPs\uff0c\u5176\u901f\u5ea6\u4e5f\u4f1a\u6709\u5f88\u5927\u4e0d\u540c\u3002 FLOPs\u4e0e\u5b9e\u9645\u5ef6\u8fdf\u4e4b\u95f4\u4e00\u5927\u5dee\u522b\u5728\u4e8e\u6ca1\u6709\u8003\u8651Memory access cost(MAC),\u7b2c\u4e8c\u5927\u533a\u522b\u5728\u4e8e\u5e73\u884c\u8fd0\u7b97\u5ea6(degree of parallelism).\u540c\u65f6\u76f8\u540c\u7684\u8fd0\u7b97\u7ed3\u679c\u4f1a\u56e0\u5e73\u53f0\u800c\u5f02\uff0c \u524d\u6587 \u63d0\u51fa\u7684\u901a\u8fc7matrix decomposition\u964d\u7ef4\u52a0\u901f\uff0c\u7406\u8bba\u4e0a\u6765\u8bf4\u80fd\u6709\u66f4\u4f4e\u7684FLOPs,\u4f46\u662f\u5728GPU\u4e0a\u7684\u6267\u884c\u901f\u5ea6\u5374\u66f4\u6162\uff0c\u539f\u56e0\u662f\u67d0\u4e00\u4e2a\u7248\u672c\u7684CUDNN\u4e3a 3\\times 3 \u5377\u79ef\u7279\u6b8a\u4f18\u5316\u8fc7\u3002 \u56e0\u6b64\u63d0\u51fa\u5efa\u8bae\u5e94\u8be5\u7528\u901f\u5ea6\u800c\u975eFLOPs\u8fdb\u884c\u8ba8\u8bba\uff0c\u5e76\u4e14\u9700\u8981\u8bf4\u660e\u5e73\u53f0\u3002 \u672c\u6587\u7684\u8d21\u732e\u5c31\u662f\u5148\u63d0\u51fa\u4e86\u8bbe\u8ba1\u9ad8\u6548\u7f51\u7edc\u7684\u4e00\u4e9b\u5efa\u8bae\uff0c\u5e76\u63d0\u51fashuffleNet V2","title":"\u4e3a\u4ec0\u4e48FLOPs\u6307\u6807\u8fd8\u4e0d\u5145\u5206"},{"location":"The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/#_1","text":"","title":"\u5b9e\u7528\u5efa\u8bae"},{"location":"The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/#channelmac","text":"1 \\times 1 \u5377\u79ef\u7684FLOPs\u4e3a B = hwc_1c_2 ,\u5047\u8bbe\u5185\u5b58\u8db3\u591f\u5927\uff0cMAC\u4e3a MAC=hw(c_1+c_2)+c_1c_2 \u8fdb\u4e00\u6b65\u63a8\u5f97 MAC \\ge 2\\sqrt{hwB} + \\frac{B}{hw} \u56e0\u6b64\u5bf9\u4e8e\u76f8\u7b49\u7684FLOPs\u4ee5\u53ca\u76f8\u540c\u7684feature map\u5927\u5c0f\uff0c\u8f93\u5165\u8f93\u51fachannel\u6570\u4e00\u81f4\u65f6\uff0cMAC\u6700\u5c0f\u3002\u5c3d\u7ba1\u8fd9\u53ea\u662f\u7406\u8bba\u503c\uff0c\u4f46\u662f\u5b9e\u9a8c\u53ef\u4ee5\u8bc1\u660e\u8fd9\u4e00\u70b9(\u672c\u6587\u5bf9\u4e0d\u540c\u7684 1\\times 1 \u5377\u79ef\u7684c1,c2\u914d\u6bd4\u8fdb\u884c\u4e86\u5b9e\u9a8c\u5bf9\u6bd4\uff0c\u8bc1\u660e\u4e86\u8fd9\u70b9)\u3002","title":"\u5efa\u8bae\u4e00,\u4f7f\u7528\u7b49channel\u5bbd\u5ea6\u4ee5\u51cf\u5c11MAC"},{"location":"The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/#group-convolutionmac","text":"\u7406\u8bba\u5206\u6790 \\begin{aligned} \\mathrm{MAC} &=h w\\left(c_{1}+c_{2}\\right)+\\frac{c_{1} c_{2}}{g} \\\\ &=h w c_{1}+\\frac{B g}{c_{1}}+\\frac{B}{h w} \\end{aligned} MAC\u4e0e g the number of groups \u6b63\u76f8\u5173\u3002 \u5728\u540c\u7b49FLOPs\u7684\u60c5\u51b5\u4e0b\uff0cgroup number\u8d8a\u5927\u901f\u5ea6\u8d8a\u6162\uff0c\u4f46\u662f\u503c\u5f97\u6ce8\u610f\u7684\u662f\u4e3a\u4e86\u4fdd\u8bc1\u540c\u7b49FLOPs,\u589e\u5927group number\u65f6channel\u6570\u4e5f\u4f1a\u63d0\u5347\u3002\u6548\u679c\u5728GPU\u4e0a\u6bd4\u8f83\u660e\u663e\u3002","title":"\u5efa\u8bae\u4e8c\uff0c\u8fc7\u591a\u7684group convolution\u63d0\u5347\u4e86MAC"},{"location":"The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/#_2","text":"\u8fd9\u91cc\u6307\u7684\u662f\u4e00\u4e2ablock\u91cc\u9762\u5e76\u884c\u7684\u5377\u79ef\u4e0epooling\u5c42\uff0c\u8fd9\u4e9b\u5e73\u884c\u4f46\u4e0d\u5e76\u884c\u7684\u8fd0\u7b97\u4f1a\u591a\u6b21\u89e6\u53d1GPU\u7684\u542f\u52a8\u4e0e\u540c\u6b65\u3002\u788e\u7247\u5316\u8fd0\u884c\u5728GPU\u4e0a\u5f71\u54cd\u8f83\u5927\uff0c\u5728CPU\u4e0a\u5f71\u54cd\u4e0d\u5927","title":"\u5efa\u8bae\u4e09\uff0c\u7f51\u7edc\u7684\u788e\u7247\u5316\u964d\u4f4e\u4e86\u5e76\u884c\u5ea6"},{"location":"The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/#reluelement-wise-operators","text":"\u5220\u9664ResNet\u4e2d\u7684ReLU\u4e0eshortcut\u4f1a\u770b\u5230\u7ea620%\u7684\u52a0\u901f\uff0c\u5c3d\u7ba1\u5b83\u4eec\u5f88\u5fc5\u8981\uff0c\u4f46\u662f\u5374\u662f\u4e8b\u5b9e\u4e0a\u5f71\u54cd\u901f\u5ea6\u7684\u3002","title":"\u5efa\u8bae\u56db\uff0cReLU,\u5143\u7d20\u95f4\u76f8\u52a0\u7b49element-wise operators\u540c\u6837\u4e0d\u53ef\u5ffd\u7565"},{"location":"The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/#shufflenet-v2","text":"\u57fa\u672c\u5355\u5143\u7ed3\u6784\u53ca\u5176\u4e0eShuffleNet V1\u7684\u6bd4\u8f83\u5982\u56fe \u76f4\u89c9\uff1a \u7528 1\\times 1 \u5377\u79ef\u66ff\u4ee3Group Convolution+ channel shuffle \u4f7f\u7528Concat + Channel Shuffle\u66ff\u4ee3Add downsampling\u65f6\u4e0d\u4f7f\u7528avg pooling Channel split\u5c31\u662f\u7b80\u5355\u5730torch.split(dim=1) \u8fde\u7eed\u51fa\u73b0\u7684Concat, Channel Shuffle, Channel Split\u53ef\u4ee5\u653e\u5728\u4e00\u8d77\u4f5c\u4e3a\u4e00\u4e2a\u7edf\u4e00\u7684element-wise operation\u3002","title":"ShuffleNet V2:"},{"location":"The_theory/Style_and_normalization/","text":"Style and Normalization \u56fe\u50cf\u6570\u636e\u7684\u98ce\u683c\u5e38\u5e38\u88ab\u8ba4\u4e3a\u4e0e\u56fe\u7247\u6570\u636e\u7684\u7edf\u8ba1\u7279\u5f81\u76f4\u63a5\u76f8\u5173\uff0c\u5bf9\u7f51\u7edc\u7684generalization\u80fd\u529b\u6709\u4e00\u5b9a\u5173\u7cfb\uff0c\u8fd9\u91cc\u5bf9\u76f8\u5173\u7684\u4e00\u4e9bpaper\u8fdb\u884c\u5206\u6790. Adaptive Batch Normalization for practical domain adaptation pdf \u8fd9\u7bc7paper\u89c2\u5bdf\u5230: \u6d45\u5c42\u4e0e\u9ad8\u5c42\u7684\u7279\u5f81\u90fd\u4f1a\u53d7\u5230domain shift\u7684\u5f71\u54cd BN \u5c42\u7684\u53c2\u6570\u5305\u542b\u6709\u6570\u636edomain\u7684\u7279\u5f81 \u63d0\u51fa\u4e86\u5982\u4e0b\u7684\u8fc1\u79fb\u7b97\u6cd5 Batch-Instance Normalization for Adaptively Style-Invariant Neural Networks pdf code \u8fd9\u7bc7paper\u6307\u51fa\uff0cInstance Normalization\u5728 style transfer\u548cGAN \u4e0a\u5e94\u7528\u6bd4\u8f83\u591a, \u5982 AdaIn . \u4f46\u662f IN\u5374\u88ab\u5b9e\u9a8c\u8bc1\u660e\u4f1a\u4f7f\u5206\u7c7b\u7f51\u7edc\u6027\u80fd\u4e0b\u964d,\u4f5c\u8005\u63a8\u6d4b\u8fd9\u662f\u56e0\u4e3a\u5206\u7c7b\u6709\u65f6\u5019\u786e\u5b9e\u9700\u8981\u56fe\u50cf\u7684\u98ce\u683c\u4fe1\u606f\u3002\u6307\u51fa\u4e0d\u540c\u4efb\u52a1\u53ef\u80fd\u9700\u8981BN\u4e5f\u53ef\u80fd\u9700\u8981IN\u3002\u56e0\u800c\u63d0\u51fa\u4e86BN-IN Gated by parameter\u7684\u8bbe\u8ba1. \u5176\u5b98\u65b9\u5b9e\u73b0: class _BatchInstanceNorm(_BatchNorm): def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True): super(_BatchInstanceNorm, self).__init__(num_features, eps, momentum, affine) self.gate = Parameter(torch.Tensor(num_features)) self.gate.data.fill_(1) setattr(self.gate, 'bin_gate', True) def forward(self, input): self._check_input_dim(input) # Batch norm if self.affine: bn_w = self.weight * self.gate else: bn_w = self.gate out_bn = F.batch_norm( input, self.running_mean, self.running_var, bn_w, self.bias, self.training, self.momentum, self.eps) # Instance norm b, c = input.size(0), input.size(1) if self.affine: in_w = self.weight * (1 - self.gate) else: in_w = 1 - self.gate input = input.view(1, b * c, *input.size()[2:]) out_in = F.batch_norm( input, None, None, None, None, True, self.momentum, self.eps) out_in = out_in.view(b, c, *input.size()[2:]) out_in.mul_(in_w[None, :, None, None]) return out_bn + out_in Two at Once: Enhancing Learning and Generalization Capacities via IBN-Net pdf code \u8fd9\u7bc7paper\u6709\u4e00\u53e5\u8bdd\u8868\u8fbe\u4e00\u4e2a\u7ecf\u5e38\u88ab\u5f15\u7528\u7684\u610f\u601d \"IN allows to filter out instance-specific contrast information from the content\". \u63d0\u51fa\u4e86\u6570\u4e2ablock\u7efc\u5408BN\u4e0eIN\u7684\u4f7f\u7528\u3002 Domain-invariant Stereo Matching Networks pdf code \u8fd9\u7bc7paper\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e24\u6b21normalize\u7684\u7b97\u6cd5\uff0c\u7b97\u6cd5\u7531\u4e24\u6b21normalization \u6784\u6210 \u4f5c\u8005\u5728\u8bba\u6587\u4e2d\u7684\u56fe\u4e2d\u63d0\u5230\uff0c\u4f7f\u7528Domain Norm\u8ba1\u7b97\u7684\u7279\u5f81\u56fe\uff0c\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u5f97\u5230\u7684norm distribution\u57fa\u672c\u662f\u4e00\u81f4\u7684\u3002 class DomainNorm(nn.Module): def __init__(self, channel, l2=True): super(DomainNorm, self).__init__() self.normalize = nn.InstanceNorm2d(num_features=channel, affine=False) self.l2 = l2 self.weight = nn.Parameter(torch.ones(1,channel,1,1)) self.bias = nn.Parameter(torch.zeros(1,channel,1,1)) self.weight.requires_grad = True self.bias.requires_grad = True def forward(self, x): x = self.normalize(x) if self.l2: return F.normalize(x, p=2, dim=1) return x * self.weight + self.bias","title":"Style and Normalization"},{"location":"The_theory/Style_and_normalization/#style-and-normalization","text":"\u56fe\u50cf\u6570\u636e\u7684\u98ce\u683c\u5e38\u5e38\u88ab\u8ba4\u4e3a\u4e0e\u56fe\u7247\u6570\u636e\u7684\u7edf\u8ba1\u7279\u5f81\u76f4\u63a5\u76f8\u5173\uff0c\u5bf9\u7f51\u7edc\u7684generalization\u80fd\u529b\u6709\u4e00\u5b9a\u5173\u7cfb\uff0c\u8fd9\u91cc\u5bf9\u76f8\u5173\u7684\u4e00\u4e9bpaper\u8fdb\u884c\u5206\u6790.","title":"Style and Normalization"},{"location":"The_theory/Style_and_normalization/#adaptive-batch-normalization-for-practical-domain-adaptation","text":"pdf \u8fd9\u7bc7paper\u89c2\u5bdf\u5230: \u6d45\u5c42\u4e0e\u9ad8\u5c42\u7684\u7279\u5f81\u90fd\u4f1a\u53d7\u5230domain shift\u7684\u5f71\u54cd BN \u5c42\u7684\u53c2\u6570\u5305\u542b\u6709\u6570\u636edomain\u7684\u7279\u5f81 \u63d0\u51fa\u4e86\u5982\u4e0b\u7684\u8fc1\u79fb\u7b97\u6cd5","title":"Adaptive Batch Normalization for practical domain adaptation"},{"location":"The_theory/Style_and_normalization/#batch-instance-normalization-for-adaptively-style-invariant-neural-networks","text":"pdf code \u8fd9\u7bc7paper\u6307\u51fa\uff0cInstance Normalization\u5728 style transfer\u548cGAN \u4e0a\u5e94\u7528\u6bd4\u8f83\u591a, \u5982 AdaIn . \u4f46\u662f IN\u5374\u88ab\u5b9e\u9a8c\u8bc1\u660e\u4f1a\u4f7f\u5206\u7c7b\u7f51\u7edc\u6027\u80fd\u4e0b\u964d,\u4f5c\u8005\u63a8\u6d4b\u8fd9\u662f\u56e0\u4e3a\u5206\u7c7b\u6709\u65f6\u5019\u786e\u5b9e\u9700\u8981\u56fe\u50cf\u7684\u98ce\u683c\u4fe1\u606f\u3002\u6307\u51fa\u4e0d\u540c\u4efb\u52a1\u53ef\u80fd\u9700\u8981BN\u4e5f\u53ef\u80fd\u9700\u8981IN\u3002\u56e0\u800c\u63d0\u51fa\u4e86BN-IN Gated by parameter\u7684\u8bbe\u8ba1. \u5176\u5b98\u65b9\u5b9e\u73b0: class _BatchInstanceNorm(_BatchNorm): def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True): super(_BatchInstanceNorm, self).__init__(num_features, eps, momentum, affine) self.gate = Parameter(torch.Tensor(num_features)) self.gate.data.fill_(1) setattr(self.gate, 'bin_gate', True) def forward(self, input): self._check_input_dim(input) # Batch norm if self.affine: bn_w = self.weight * self.gate else: bn_w = self.gate out_bn = F.batch_norm( input, self.running_mean, self.running_var, bn_w, self.bias, self.training, self.momentum, self.eps) # Instance norm b, c = input.size(0), input.size(1) if self.affine: in_w = self.weight * (1 - self.gate) else: in_w = 1 - self.gate input = input.view(1, b * c, *input.size()[2:]) out_in = F.batch_norm( input, None, None, None, None, True, self.momentum, self.eps) out_in = out_in.view(b, c, *input.size()[2:]) out_in.mul_(in_w[None, :, None, None]) return out_bn + out_in","title":"Batch-Instance Normalization for Adaptively Style-Invariant Neural Networks"},{"location":"The_theory/Style_and_normalization/#two-at-once-enhancing-learning-and-generalization-capacities-via-ibn-net","text":"pdf code \u8fd9\u7bc7paper\u6709\u4e00\u53e5\u8bdd\u8868\u8fbe\u4e00\u4e2a\u7ecf\u5e38\u88ab\u5f15\u7528\u7684\u610f\u601d \"IN allows to filter out instance-specific contrast information from the content\". \u63d0\u51fa\u4e86\u6570\u4e2ablock\u7efc\u5408BN\u4e0eIN\u7684\u4f7f\u7528\u3002","title":"Two at Once: Enhancing Learning and Generalization Capacities via IBN-Net"},{"location":"The_theory/Style_and_normalization/#domain-invariant-stereo-matching-networks","text":"pdf code \u8fd9\u7bc7paper\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e24\u6b21normalize\u7684\u7b97\u6cd5\uff0c\u7b97\u6cd5\u7531\u4e24\u6b21normalization \u6784\u6210 \u4f5c\u8005\u5728\u8bba\u6587\u4e2d\u7684\u56fe\u4e2d\u63d0\u5230\uff0c\u4f7f\u7528Domain Norm\u8ba1\u7b97\u7684\u7279\u5f81\u56fe\uff0c\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u5f97\u5230\u7684norm distribution\u57fa\u672c\u662f\u4e00\u81f4\u7684\u3002 class DomainNorm(nn.Module): def __init__(self, channel, l2=True): super(DomainNorm, self).__init__() self.normalize = nn.InstanceNorm2d(num_features=channel, affine=False) self.l2 = l2 self.weight = nn.Parameter(torch.ones(1,channel,1,1)) self.bias = nn.Parameter(torch.zeros(1,channel,1,1)) self.weight.requires_grad = True self.bias.requires_grad = True def forward(self, x): x = self.normalize(x) if self.l2: return F.normalize(x, p=2, dim=1) return x * self.weight + self.bias","title":"Domain-invariant Stereo Matching Networks"},{"location":"The_theory/TranslationInvarianceinCNN/","text":"On Translation Invariance in CNNs: Convolutional Layers can Exploit Absolute Spatial Location \u8fd9\u4e0d\u662f\u672c\u7ad9\u7b2c\u4e00\u7bc7\u8ba8\u8bba\u5377\u79ef\u7f51\u7edc\u7684\u4f4d\u7f6e\u4fe1\u606f,\u53e6\u4e00\u7bc7\u662f How much Position Information Do Convolutional Neural Networks Encode? \u8fd9\u7bc7\u6587\u7ae0\u5206\u6790\u7684\u89d2\u5ea6\u4e0e\u6700\u540e\u7684\u7ed3\u8bba\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u4e0e\u524d\u6587\u6709\u533a\u522b\uff0c\u524d\u9762\u90a3\u7bc7\u6587\u7ae0\u7684\u7ed3\u8bba\u662f\u591a\u5c42Zero-Padding\u52a0\u5927\u611f\u53d7\u91ce\u4e5f\u80fd\u5f97\u5230\u5145\u5206\u7684\u4f4d\u7f6e\u4fe1\u606f\uff0c\u4e3b\u8981\u5728\u7528\u5728\u8bed\u4e49\u5206\u5272\uff0c\u800c\u672c\u6587\u5219\u6709\u6240\u4e0d\u540c\uff0c\u4e3b\u8981\u8ba8\u8bba\u5206\u7c7b\u95ee\u9898\u3002 Inspiration \u7b2c\u4e00\u4e2a\u5b9e\u9a8c\u8ba8\u8bba\u7684\u662f\u540c\u4e00\u5f20\u56fe\u51fa\u73b0\u5728\u4e0d\u540c\u89d2\u843d\uff0c\u7528\u5355\u5c42CNN\u52a0global pooling\u5c31\u80fd\u6210\u529f\u533a\u5206\u3002 Case Study \u4f5c\u8005\u540c\u6837\u8003\u8651\u5230\u4e86CNN\u8fb9\u754c\u5904\u7406\u7684\u95ee\u9898\uff0c\u4ee5\u4e0b\u9762\u8fd9\u4e2acase\u8ba8\u8bba\u4e0d\u540cpadding\u8bbe\u7f6e\u5bf9\u8f93\u51fa\u7684\u5f71\u54cd\u3002 \u5206\u522b\u89c2\u5bdf: No padding. same zero padding same circular padding. Full padding. \u4ece\u5168\u5c40global pool\u7684\u7ed3\u679c\u6765\u770b\uff0c3\u4e0e4\u90fd\u4f1a\u8f93\u51fa\u4e00\u6837\u7684\u7ed3\u679c\uff0c\u800c1\u4e0e2\u8f93\u51fa\u7684\u7ed3\u679c\u4f1a\u56e0\u7edd\u5bf9\u4f4d\u7f6e\u800c\u5f02\u3002 Receptive Field \u8fd9\u91cc\u7684\u5b9e\u9a8c\u4e0e\u524d\u6587\u540c\u6837\u8bc1\u660e\u4e86\u7f51\u7edc\u611f\u53d7\u91ce\u8d8a\u5927\uff0c\u7f51\u7edc\u53ef\u4ee5\u5224\u65ad\u7edd\u5bf9\u4f4d\u7f6e\u7684border size\u5c31\u8d8a\u5927(\u5982\u679c\u56fe\u7247\u6709\u6548\u90e8\u5206\u8ddd\u79bb\u8fb9\u754c\u8fc7\u8fdc\uff0c\u8d85\u51fa\u611f\u53d7\u91ce\u8303\u56f4\uff0cCNN\u5c31\u4e0d\u53ef\u80fd\u5b9e\u73b0\u5206\u7c7b\u4e86)\u3002 \u7edd\u5bf9\u4f4d\u7f6e\u4e0e\u76f8\u5bf9\u4f4d\u7f6e \u4f5c\u8005\u8fdb\u4e00\u6b65\u505a\u4e86\u4e00\u4e2a\u5b9e\u9a8c\uff0c\u8981\u6c42\u7f51\u7edctrain\u4ee5\u4e0a\u7684\u8fd9\u4e2a\u6570\u636e\u96c6\uff0c\u5206\u7c7b\u7684\u6807\u51c6\u662f\u76f8\u5bf9\u4f4d\u7f6e\u3002\uff08\u5e94\u8be5\u7559\u610f\u5230\uff0c\u53ea\u770btraining set\u7684\u8bdd\uff0cdissimilar Testset\u8fd9\u91cc\u7684\u7ed3\u679c\u53ea\u662f\u4e00\u4e2a\u53ef\u80fd\u7684\u5206\u7c7b\u7b54\u6848\uff0c\u53ea\u662f\u4f5c\u8005\u5f3a\u8c03\u76f8\u5bf9\u4f4d\u7f6e\u624d\u6709\u6548\u6240\u4ee5\u5f97\u5230\u8fd9\u4e2a\u7ed3\u679c\uff09 \u7406\u8bba\u4e0a\u6765\u8bf4\uff0c\u4e00\u4e2a\u7f51\u7edc\u53ef\u4ee5\u638c\u63e1\u7ea2\u4e0e\u7eff\u7684\u7edd\u5bf9\u4f4d\u7f6e\uff0c\u4e5f\u5c31\u80fd\u63a8\u7406\u5176\u76f8\u5bf9\u4f4d\u7f6e\uff0c\u4f46\u662f\u8fd9\u6837\u7684\u505a\u6cd5\u5bb9\u6613\u5bf9\u7edd\u5bf9\u4f4d\u7f6eoverfit,\u5bf9\u4e8e\u65b0\u7684\u6ca1\u89c1\u8fc7\u7684\u7edd\u5bf9\u4f4d\u7f6e\u4f1a\u6709\u6027\u80fd\u4e0b\u964d\u3002 \u5bf9\u4e8e\u672c\u6570\u636e\uff0c1/2\u90fd\u80fd\u5bf9similar testset\u6b63\u786e\u5224\u65ad\uff0c\u4f46\u662f\u96be\u4ee5\u6b63\u786e\u5224\u65addissimilar testset. \u800c3/4\u5bf9\u4e24\u4e2atest set\u7684\u4f30\u8ba1\u51c6\u786e\u7387\u662f\u4e00\u81f4\u7684\u3002","title":"On Translation Invariance in CNNs: Convolutional Layers can Exploit Absolute Spatial Location"},{"location":"The_theory/TranslationInvarianceinCNN/#on-translation-invariance-in-cnns-convolutional-layers-can-exploit-absolute-spatial-location","text":"\u8fd9\u4e0d\u662f\u672c\u7ad9\u7b2c\u4e00\u7bc7\u8ba8\u8bba\u5377\u79ef\u7f51\u7edc\u7684\u4f4d\u7f6e\u4fe1\u606f,\u53e6\u4e00\u7bc7\u662f How much Position Information Do Convolutional Neural Networks Encode? \u8fd9\u7bc7\u6587\u7ae0\u5206\u6790\u7684\u89d2\u5ea6\u4e0e\u6700\u540e\u7684\u7ed3\u8bba\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u4e0e\u524d\u6587\u6709\u533a\u522b\uff0c\u524d\u9762\u90a3\u7bc7\u6587\u7ae0\u7684\u7ed3\u8bba\u662f\u591a\u5c42Zero-Padding\u52a0\u5927\u611f\u53d7\u91ce\u4e5f\u80fd\u5f97\u5230\u5145\u5206\u7684\u4f4d\u7f6e\u4fe1\u606f\uff0c\u4e3b\u8981\u5728\u7528\u5728\u8bed\u4e49\u5206\u5272\uff0c\u800c\u672c\u6587\u5219\u6709\u6240\u4e0d\u540c\uff0c\u4e3b\u8981\u8ba8\u8bba\u5206\u7c7b\u95ee\u9898\u3002","title":"On Translation Invariance in CNNs: Convolutional Layers can Exploit Absolute Spatial Location"},{"location":"The_theory/TranslationInvarianceinCNN/#inspiration","text":"\u7b2c\u4e00\u4e2a\u5b9e\u9a8c\u8ba8\u8bba\u7684\u662f\u540c\u4e00\u5f20\u56fe\u51fa\u73b0\u5728\u4e0d\u540c\u89d2\u843d\uff0c\u7528\u5355\u5c42CNN\u52a0global pooling\u5c31\u80fd\u6210\u529f\u533a\u5206\u3002","title":"Inspiration"},{"location":"The_theory/TranslationInvarianceinCNN/#case-study","text":"\u4f5c\u8005\u540c\u6837\u8003\u8651\u5230\u4e86CNN\u8fb9\u754c\u5904\u7406\u7684\u95ee\u9898\uff0c\u4ee5\u4e0b\u9762\u8fd9\u4e2acase\u8ba8\u8bba\u4e0d\u540cpadding\u8bbe\u7f6e\u5bf9\u8f93\u51fa\u7684\u5f71\u54cd\u3002 \u5206\u522b\u89c2\u5bdf: No padding. same zero padding same circular padding. Full padding. \u4ece\u5168\u5c40global pool\u7684\u7ed3\u679c\u6765\u770b\uff0c3\u4e0e4\u90fd\u4f1a\u8f93\u51fa\u4e00\u6837\u7684\u7ed3\u679c\uff0c\u800c1\u4e0e2\u8f93\u51fa\u7684\u7ed3\u679c\u4f1a\u56e0\u7edd\u5bf9\u4f4d\u7f6e\u800c\u5f02\u3002","title":"Case Study"},{"location":"The_theory/TranslationInvarianceinCNN/#receptive-field","text":"\u8fd9\u91cc\u7684\u5b9e\u9a8c\u4e0e\u524d\u6587\u540c\u6837\u8bc1\u660e\u4e86\u7f51\u7edc\u611f\u53d7\u91ce\u8d8a\u5927\uff0c\u7f51\u7edc\u53ef\u4ee5\u5224\u65ad\u7edd\u5bf9\u4f4d\u7f6e\u7684border size\u5c31\u8d8a\u5927(\u5982\u679c\u56fe\u7247\u6709\u6548\u90e8\u5206\u8ddd\u79bb\u8fb9\u754c\u8fc7\u8fdc\uff0c\u8d85\u51fa\u611f\u53d7\u91ce\u8303\u56f4\uff0cCNN\u5c31\u4e0d\u53ef\u80fd\u5b9e\u73b0\u5206\u7c7b\u4e86)\u3002","title":"Receptive Field"},{"location":"The_theory/TranslationInvarianceinCNN/#_1","text":"\u4f5c\u8005\u8fdb\u4e00\u6b65\u505a\u4e86\u4e00\u4e2a\u5b9e\u9a8c\uff0c\u8981\u6c42\u7f51\u7edctrain\u4ee5\u4e0a\u7684\u8fd9\u4e2a\u6570\u636e\u96c6\uff0c\u5206\u7c7b\u7684\u6807\u51c6\u662f\u76f8\u5bf9\u4f4d\u7f6e\u3002\uff08\u5e94\u8be5\u7559\u610f\u5230\uff0c\u53ea\u770btraining set\u7684\u8bdd\uff0cdissimilar Testset\u8fd9\u91cc\u7684\u7ed3\u679c\u53ea\u662f\u4e00\u4e2a\u53ef\u80fd\u7684\u5206\u7c7b\u7b54\u6848\uff0c\u53ea\u662f\u4f5c\u8005\u5f3a\u8c03\u76f8\u5bf9\u4f4d\u7f6e\u624d\u6709\u6548\u6240\u4ee5\u5f97\u5230\u8fd9\u4e2a\u7ed3\u679c\uff09 \u7406\u8bba\u4e0a\u6765\u8bf4\uff0c\u4e00\u4e2a\u7f51\u7edc\u53ef\u4ee5\u638c\u63e1\u7ea2\u4e0e\u7eff\u7684\u7edd\u5bf9\u4f4d\u7f6e\uff0c\u4e5f\u5c31\u80fd\u63a8\u7406\u5176\u76f8\u5bf9\u4f4d\u7f6e\uff0c\u4f46\u662f\u8fd9\u6837\u7684\u505a\u6cd5\u5bb9\u6613\u5bf9\u7edd\u5bf9\u4f4d\u7f6eoverfit,\u5bf9\u4e8e\u65b0\u7684\u6ca1\u89c1\u8fc7\u7684\u7edd\u5bf9\u4f4d\u7f6e\u4f1a\u6709\u6027\u80fd\u4e0b\u964d\u3002 \u5bf9\u4e8e\u672c\u6570\u636e\uff0c1/2\u90fd\u80fd\u5bf9similar testset\u6b63\u786e\u5224\u65ad\uff0c\u4f46\u662f\u96be\u4ee5\u6b63\u786e\u5224\u65addissimilar testset. \u800c3/4\u5bf9\u4e24\u4e2atest set\u7684\u4f30\u8ba1\u51c6\u786e\u7387\u662f\u4e00\u81f4\u7684\u3002","title":"\u7edd\u5bf9\u4f4d\u7f6e\u4e0e\u76f8\u5bf9\u4f4d\u7f6e"},{"location":"The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/","text":"Understanding Deep Learning Requires Rethinking Generalization \u8fd9\u7bc7\u6587\u7ae0\u8ba8\u8bba\u7f51\u7edc\u7684generalization\u80fd\u529b\u3002 \u4e00\u4e2a\u7279\u6b8a\u7684\u5b9e\u9a8c\uff1a \u5728\u4e00\u4e2adataset\u4e2d\uff0c\u5c06label\u6539\u4e3a\u5b8c\u5168\u968f\u673a\u62bd\u53d6\uff0c\u5b9e\u9a8c\u8bc1\u660e\uff0c\u76ee\u524d\u7684\u795e\u7ecf\u7f51\u7edc\u80fd\u591f\u5b8c\u5168\u8bb0\u5fc6\u6240\u6709\u7684\u968f\u673alabel\uff0c\u4f18\u5316\u96be\u5ea6\u5e76\u6ca1\u6709\u663e\u8457\u63d0\u5347\uff0c\u4f46\u662f\u660e\u786e\u53ef\u4ee5\u77e5\u9053\uff0cgeneralization\u7684\u7ed3\u679c\u5fc5\u7136\u7b49\u540c\u4e8e\u968f\u673a(training \u4e0e testing\u5b8c\u5168\u4e0d\u76f8\u5173)\u3002 \u6240\u4ee5\u9996\u5148\u53ef\u4ee5\u77e5\u9053\u4f18\u5316\u662f\u5426\u987a\u5229\u4e0e\u80fd\u5426generalize\u6ca1\u6709\u76f4\u63a5\u663e\u8457\u7684\u5173\u7cfb \u5173\u4e8eregularization\u6280\u5de7\uff1a \u6570\u636e\u589e\u5f3a\u3001weight delay( l_2 regularizer)\u3001dropout\uff0c\u7ecf\u9a8c\u4e0a\u90fd\u80fd\u63d0\u5347\u7f51\u7edc\u7684test\u51c6\u786e\u7387\uff0c\u4f46\u662f\u5728fitting random labels\u7684\u65f6\u5019\u90fd\u662f\u80fd\u8fbe\u5230training accuracy = 100%,test accuracy\u4ecd\u7136\u662f10%. BatchNorm\u548cEarlyStop\u90fd\u662f\u6709\u6548\u7684\u3002","title":"Understanding Deep Learning Requires Rethinking Generalization"},{"location":"The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/#understanding-deep-learning-requires-rethinking-generalization","text":"\u8fd9\u7bc7\u6587\u7ae0\u8ba8\u8bba\u7f51\u7edc\u7684generalization\u80fd\u529b\u3002 \u4e00\u4e2a\u7279\u6b8a\u7684\u5b9e\u9a8c\uff1a \u5728\u4e00\u4e2adataset\u4e2d\uff0c\u5c06label\u6539\u4e3a\u5b8c\u5168\u968f\u673a\u62bd\u53d6\uff0c\u5b9e\u9a8c\u8bc1\u660e\uff0c\u76ee\u524d\u7684\u795e\u7ecf\u7f51\u7edc\u80fd\u591f\u5b8c\u5168\u8bb0\u5fc6\u6240\u6709\u7684\u968f\u673alabel\uff0c\u4f18\u5316\u96be\u5ea6\u5e76\u6ca1\u6709\u663e\u8457\u63d0\u5347\uff0c\u4f46\u662f\u660e\u786e\u53ef\u4ee5\u77e5\u9053\uff0cgeneralization\u7684\u7ed3\u679c\u5fc5\u7136\u7b49\u540c\u4e8e\u968f\u673a(training \u4e0e testing\u5b8c\u5168\u4e0d\u76f8\u5173)\u3002 \u6240\u4ee5\u9996\u5148\u53ef\u4ee5\u77e5\u9053\u4f18\u5316\u662f\u5426\u987a\u5229\u4e0e\u80fd\u5426generalize\u6ca1\u6709\u76f4\u63a5\u663e\u8457\u7684\u5173\u7cfb \u5173\u4e8eregularization\u6280\u5de7\uff1a \u6570\u636e\u589e\u5f3a\u3001weight delay( l_2 regularizer)\u3001dropout\uff0c\u7ecf\u9a8c\u4e0a\u90fd\u80fd\u63d0\u5347\u7f51\u7edc\u7684test\u51c6\u786e\u7387\uff0c\u4f46\u662f\u5728fitting random labels\u7684\u65f6\u5019\u90fd\u662f\u80fd\u8fbe\u5230training accuracy = 100%,test accuracy\u4ecd\u7136\u662f10%. BatchNorm\u548cEarlyStop\u90fd\u662f\u6709\u6548\u7684\u3002","title":"Understanding Deep Learning Requires Rethinking Generalization"},{"location":"The_theory/VAE/","text":"Auto-Encoding Variational Bayes Variational Auto Encoder (VAE) \u662f\u7ecf\u5178\u7684\u751f\u6210\u6a21\u578b\u3002\u5176\u8bba\u6587\u7684\u8ba8\u8bba\u601d\u8def\u548c\u76ee\u524d\u5927\u5bb6\u6574\u7406\u7684\u5e38\u7528\u5206\u6790\u601d\u8def\u76f8\u6bd4\u6709\u4e00\u5b9a\u7684\u533a\u522b\u3002\u8fd9\u91cc\u6839\u636e\u5404\u5904\u7684\u8d44\u6599\u6574\u7406\u4e00\u4efd\u5b8c\u6574\u7684\u7406\u8bba\u89e3\u7b54. \u5df2\u77e5\u6570\u636e\u96c6 X , \u5176\u6570\u636e\u70b9 x_i , \u53d7\u9690\u53d8\u91cf z_i \u5f71\u54cd\u3002\u6211\u4eec\u7684\u76ee\u6807\u662f\u8bbe\u8ba1\u4e00\u4e2a\u89e3\u7801\u5668decoder, \u53c2\u6570\u8bb0\u4e3a \\theta , \u6211\u4eec\u7684\u76ee\u6807\u662f\u4f7f\u7528\u8fd9\u4e2a\u89e3\u7801\u5668\u5bf9\u6570\u636e\u7684\u5206\u5e03\u8fdb\u884c\u5efa\u6a21\uff0c\u6210\u4e3a p_\\theta(x) . \u8fd9\u4e2a\u6620\u5c04\u53d7\u9690\u53d8\u91cf z \u7684\u5f71\u54cd\uff0c\u8fdb\u800c\u8bb0\u8fd9\u4e2a\u5206\u5e03\u4e3a p_\\theta(x) = \\int_z{p_\\theta(x|z) p_\\theta(z) dz} \u76ee\u524d\u8fd9\u4e2a\u65b9\u7a0b\u7684\u5de6\u8fb9\u7684\u8ba1\u7b97\u4ee5\u53ca\u4f18\u5316\uff0c\u4ece\u5404\u4e2a\u89d2\u5ea6\u6765\u770b\u90fdintractable\uff0c\u800c\u4e14\u6211\u4eec\u8fd8\u9700\u8981\u5bf9\u9690\u53d8\u91cf\u8fdb\u884c\u63d0\u53d6\u548c\u5206\u6790. \u8fd9\u65f6\u5019\u6211\u4eec\u5f15\u5165\u7f16\u7801\u5668encoder, \u5176\u53c2\u6570\u8bb0\u4e3a \\phi \uff0c \u8fd9\u4e2a\u7f16\u7801\u5668\u4f1a\u5efa\u6a21\u9690\u53d8\u91cf\u7684\u540e\u9a8c\u5206\u5e03 q_\\phi(z|x) . \u6211\u4eec\u5e0c\u671b\u8fd9\u4e2a\u540e\u9a8c\u5206\u5e03\u4e0e\u4e0a\u5f0f\u5c55\u5f00\u5f0f\u65f6\u6240\u9700\u8981\u7684\u540e\u9a8c\u5206\u5e03 p_\\theta(z|x) \u51e0\u4e4e\u4e00\u81f4\uff0c\u4e5f\u5c31\u662f q_\\phi(z|x) \\approx p_\\theta(z|x) \u9996\u5148\u6211\u4eec\u5c1d\u8bd5\u66f4\u6570\u5b66\u5730\u63cf\u8ff0\u4e0a\u8ff0\u7684 \\approx . \u8fd9\u91cc\u6211\u4eec\u91c7\u7528KL Divergence, \u4f5c\u4e3a\u4e24\u4e2a\u6982\u7387\u5206\u5e03\u4e4b\u95f4\u7684\u5dee\u7684\u63cf\u8ff0. D_{KL}(p||q) = H(p, q) - H(p) = \\sum_{x\\in X} P(x) \\text{log}\\left( \\frac{P(x)}{Q(x)}\\right) = \\int{p(x)\\text{log}(\\frac{p(x)}{q(x)})}dx \u524d\u9762\u7684\u4e24\u4e2a\u5206\u5e03\u7684KL Divergence\u5c55\u5f00\u5982\u4e0b \\begin{aligned} D_{K L}\\left(q_{\\Phi}(\\mathbf{z} \\mid \\mathbf{x}) \\| p_{\\theta}(\\mathbf{z} \\mid \\mathbf{x})\\right) &=\\int q_{\\Phi}(\\mathbf{z} \\mid \\mathbf{x}) \\log \\frac{q_{\\Phi}(\\mathbf{z} \\mid \\mathbf{x})}{p_{\\theta}(\\mathbf{z} \\mid \\mathbf{x})} d \\mathbf{z} \\\\ &=\\int q_{\\Phi}(\\mathbf{z} \\mid \\mathbf{x}) \\log \\frac{q_{\\Phi}(\\mathbf{z} \\mid \\mathbf{x}) p_{\\theta}(\\mathbf{x})}{p_{\\theta}(\\mathbf{z}, \\mathbf{x})} d \\mathbf{z} \\\\ &=\\int q_{\\Phi}(\\mathbf{z} \\mid \\mathbf{x})\\left(\\log \\left(p_{\\theta}(\\mathbf{x})\\right)+\\log \\frac{q_{\\Phi}(\\mathbf{z} \\mid \\mathbf{x})}{p_{\\theta}(\\mathbf{z}, \\mathbf{x})}\\right) d \\mathbf{z} \\\\ &=\\log \\left(p_{\\theta}(\\mathbf{x})\\right)+\\int q_{\\Phi}(\\mathbf{z} \\mid \\mathbf{x}) \\log \\frac{q_{\\Phi}(\\mathbf{z} \\mid \\mathbf{x})}{p_{\\theta}(\\mathbf{z}, \\mathbf{x})} d \\mathbf{z} \\\\ &=\\log \\left(p_{\\theta}(\\mathbf{x})\\right)+\\int q_{\\Phi}(\\mathbf{z} \\mid \\mathbf{x}) \\log \\frac{q_{\\Phi}(\\mathbf{z} \\mid \\mathbf{x})}{p_{\\theta}(\\mathbf{x} \\mid \\mathbf{z}) p_{\\theta}(\\mathbf{z})} d \\mathbf{z} \\\\ &=\\log \\left(p_{\\theta}(\\mathbf{x})\\right)+E_{\\mathbf{z} \\sim q_{\\Phi}(\\mathbf{z} \\mid \\mathbf{x})}\\left(\\log \\frac{q_{\\Phi}(\\mathbf{z} \\mid \\mathbf{x})}{p_{\\theta}(\\mathbf{z})}-\\log \\left(p_{\\theta}(\\mathbf{x} \\mid \\mathbf{z})\\right)\\right) \\\\ &=\\log \\left(p_{\\theta}(\\mathbf{x})\\right)+D_{K L}\\left(q_{\\Phi}(\\mathbf{z} \\mid \\mathbf{x}) \\| p_{\\theta}(\\mathbf{z})\\right)-E_{\\mathbf{z} \\sim q_{\\Phi}(\\mathbf{z} \\mid \\mathbf{x})}\\left(\\log \\left(p_{\\theta}(\\mathbf{x} \\mid \\mathbf{z})\\right)\\right) \\end{aligned} \u7a0d\u5fae\u53d8\u5f62\uff0c\u5c06\u6211\u4eec\u60f3\u4f18\u5316\u7684\u5185\u5bb9\u653e\u4e00\u8fb9\uff0c\u53ef\u4ee5\u8ba1\u7b97\u7684\u5185\u5bb9\u653e\u53e6\u4e00\u8fb9: \\log \\left(p_{\\theta}(\\mathbf{x})\\right)-D_{K L}\\left(q_{\\Phi}(\\mathbf{z} \\mid \\mathbf{x}) \\| p_{\\theta}(\\mathbf{z} \\mid \\mathbf{x})\\right)=E_{\\mathbf{z} \\sim q_{\\Phi}(\\mathbf{z} \\mid \\mathbf{x})}\\left(\\log \\left(p_{\\theta}(\\mathbf{x} \\mid \\mathbf{z})\\right)\\right)-D_{K L}\\left(q_{\\Phi}(\\mathbf{z} \\mid \\mathbf{x}) \\| p_{\\theta}(\\mathbf{z})\\right) \u5de6\u8fb9\u7b2c\u4e00\u9879\u662f\u6211\u4eec\u60f3\u8981\u6700\u5927\u5316\u7684\u8868\u5f81\u89e3\u7801\u5668\u5bf9\u6570\u636e\u5206\u5e03\u7684\u5efa\u6a21\u6b63\u786e\u5ea6\u7684\u4f3c\u7136\u503c\uff0c\u7b2c\u4e8c\u9879\u662f\u7f16\u7801\u5668\u5bf9\u9690\u53d8\u91cf\u7684\u5efa\u6a21\u4e0e\u5b9e\u9645\u9690\u53d8\u91cf\u540e\u9a8c\u6982\u7387\u7684\u6982\u7387\u6a21\u578b\u5dee\u5f02\uff0c\u662f\u6211\u4eec\u60f3\u8981\u6700\u5c0f\u5316\u7684\u5185\u5bb9. \u53f3\u8fb9\u7b2c\u4e00\u9879\u662f\u7531\u7f16\u7801\u5668\u91c7\u6837\u7684\u9690\u53d8\u91cf\u8fdb\u884c\u89e3\u7801\u5f97\u5230\u7684\u6570\u636e\u7684\u4f3c\u7136\u503c\uff0c\u5b9e\u73b0\u8d77\u6765\u5c31\u662freconstruction loss, \u7b2c\u4e8c\u9879\u6307\u7f16\u7801\u5668\u8f93\u51fa\u4e0e\u9690\u53d8\u91cf\u672c\u8eab\u7684\u5148\u9a8c\u4e4b\u95f4\u7684\u6982\u7387\u5dee\u5f02\uff0c\u5b9e\u73b0\u8d77\u6765\u5c31\u662f\u4e00\u4e2a\u9650\u5236\u7f16\u7801\u5668\u8f93\u51fa\u5e45\u503c\u7684regularizer. \u6211\u4eec\u4ee4\u635f\u5931\u51fd\u6570 L_{\\theta, \\Phi}=-\\log \\left(p_{\\theta}(\\mathbf{x})\\right)+D_{K L}\\left(q_{\\Phi}(\\mathbf{z} \\mid \\mathbf{x}) \\| p_{\\theta}(\\mathbf{z} \\mid \\mathbf{x})\\right)=-E_{\\mathbf{z} \\sim q_{\\Phi}(\\mathbf{z} \\mid \\mathbf{x})}\\left(\\log \\left(p_{\\theta}(\\mathbf{x} \\mid \\mathbf{z})\\right)\\right)+D_{K L}\\left(q_{\\Phi}(\\mathbf{z} \\mid \\mathbf{x}) \\| p_{\\theta}(\\mathbf{z})\\right) \u7531KL divergence\u7684\u975e\u8d1f\u6027\uff0c\u53ef\u4ee5\u5f97\u5230 -L_{\\theta, \\Phi}=\\log \\left(p_{\\theta}(\\mathbf{x})\\right)-D_{K L}\\left(q_{\\Phi}(\\mathbf{z} \\mid \\mathbf{x}) \\| p_{\\theta}(\\mathbf{z} \\mid \\mathbf{x})\\right) \\leq \\log \\left(p_{\\theta}(\\mathbf{x})\\right) \u56e0\u800c\u635f\u5931\u51fd\u6570\u4f18\u5316\u7684\u5185\u5bb9\u53c8\u53ef\u4ee5\u7406\u89e3\u4e3a \\log{p_\\theta(x)} \u7684\u4e0b\u9650\uff0c\u56e0\u800c\u88ab\u6210\u4e3a evidence lower bound (ELBO).","title":"Auto-Encoding Variational Bayes"},{"location":"The_theory/VAE/#auto-encoding-variational-bayes","text":"Variational Auto Encoder (VAE) \u662f\u7ecf\u5178\u7684\u751f\u6210\u6a21\u578b\u3002\u5176\u8bba\u6587\u7684\u8ba8\u8bba\u601d\u8def\u548c\u76ee\u524d\u5927\u5bb6\u6574\u7406\u7684\u5e38\u7528\u5206\u6790\u601d\u8def\u76f8\u6bd4\u6709\u4e00\u5b9a\u7684\u533a\u522b\u3002\u8fd9\u91cc\u6839\u636e\u5404\u5904\u7684\u8d44\u6599\u6574\u7406\u4e00\u4efd\u5b8c\u6574\u7684\u7406\u8bba\u89e3\u7b54. \u5df2\u77e5\u6570\u636e\u96c6 X , \u5176\u6570\u636e\u70b9 x_i , \u53d7\u9690\u53d8\u91cf z_i \u5f71\u54cd\u3002\u6211\u4eec\u7684\u76ee\u6807\u662f\u8bbe\u8ba1\u4e00\u4e2a\u89e3\u7801\u5668decoder, \u53c2\u6570\u8bb0\u4e3a \\theta , \u6211\u4eec\u7684\u76ee\u6807\u662f\u4f7f\u7528\u8fd9\u4e2a\u89e3\u7801\u5668\u5bf9\u6570\u636e\u7684\u5206\u5e03\u8fdb\u884c\u5efa\u6a21\uff0c\u6210\u4e3a p_\\theta(x) . \u8fd9\u4e2a\u6620\u5c04\u53d7\u9690\u53d8\u91cf z \u7684\u5f71\u54cd\uff0c\u8fdb\u800c\u8bb0\u8fd9\u4e2a\u5206\u5e03\u4e3a p_\\theta(x) = \\int_z{p_\\theta(x|z) p_\\theta(z) dz} \u76ee\u524d\u8fd9\u4e2a\u65b9\u7a0b\u7684\u5de6\u8fb9\u7684\u8ba1\u7b97\u4ee5\u53ca\u4f18\u5316\uff0c\u4ece\u5404\u4e2a\u89d2\u5ea6\u6765\u770b\u90fdintractable\uff0c\u800c\u4e14\u6211\u4eec\u8fd8\u9700\u8981\u5bf9\u9690\u53d8\u91cf\u8fdb\u884c\u63d0\u53d6\u548c\u5206\u6790. \u8fd9\u65f6\u5019\u6211\u4eec\u5f15\u5165\u7f16\u7801\u5668encoder, \u5176\u53c2\u6570\u8bb0\u4e3a \\phi \uff0c \u8fd9\u4e2a\u7f16\u7801\u5668\u4f1a\u5efa\u6a21\u9690\u53d8\u91cf\u7684\u540e\u9a8c\u5206\u5e03 q_\\phi(z|x) . \u6211\u4eec\u5e0c\u671b\u8fd9\u4e2a\u540e\u9a8c\u5206\u5e03\u4e0e\u4e0a\u5f0f\u5c55\u5f00\u5f0f\u65f6\u6240\u9700\u8981\u7684\u540e\u9a8c\u5206\u5e03 p_\\theta(z|x) \u51e0\u4e4e\u4e00\u81f4\uff0c\u4e5f\u5c31\u662f q_\\phi(z|x) \\approx p_\\theta(z|x) \u9996\u5148\u6211\u4eec\u5c1d\u8bd5\u66f4\u6570\u5b66\u5730\u63cf\u8ff0\u4e0a\u8ff0\u7684 \\approx . \u8fd9\u91cc\u6211\u4eec\u91c7\u7528KL Divergence, \u4f5c\u4e3a\u4e24\u4e2a\u6982\u7387\u5206\u5e03\u4e4b\u95f4\u7684\u5dee\u7684\u63cf\u8ff0. D_{KL}(p||q) = H(p, q) - H(p) = \\sum_{x\\in X} P(x) \\text{log}\\left( \\frac{P(x)}{Q(x)}\\right) = \\int{p(x)\\text{log}(\\frac{p(x)}{q(x)})}dx \u524d\u9762\u7684\u4e24\u4e2a\u5206\u5e03\u7684KL Divergence\u5c55\u5f00\u5982\u4e0b \\begin{aligned} D_{K L}\\left(q_{\\Phi}(\\mathbf{z} \\mid \\mathbf{x}) \\| p_{\\theta}(\\mathbf{z} \\mid \\mathbf{x})\\right) &=\\int q_{\\Phi}(\\mathbf{z} \\mid \\mathbf{x}) \\log \\frac{q_{\\Phi}(\\mathbf{z} \\mid \\mathbf{x})}{p_{\\theta}(\\mathbf{z} \\mid \\mathbf{x})} d \\mathbf{z} \\\\ &=\\int q_{\\Phi}(\\mathbf{z} \\mid \\mathbf{x}) \\log \\frac{q_{\\Phi}(\\mathbf{z} \\mid \\mathbf{x}) p_{\\theta}(\\mathbf{x})}{p_{\\theta}(\\mathbf{z}, \\mathbf{x})} d \\mathbf{z} \\\\ &=\\int q_{\\Phi}(\\mathbf{z} \\mid \\mathbf{x})\\left(\\log \\left(p_{\\theta}(\\mathbf{x})\\right)+\\log \\frac{q_{\\Phi}(\\mathbf{z} \\mid \\mathbf{x})}{p_{\\theta}(\\mathbf{z}, \\mathbf{x})}\\right) d \\mathbf{z} \\\\ &=\\log \\left(p_{\\theta}(\\mathbf{x})\\right)+\\int q_{\\Phi}(\\mathbf{z} \\mid \\mathbf{x}) \\log \\frac{q_{\\Phi}(\\mathbf{z} \\mid \\mathbf{x})}{p_{\\theta}(\\mathbf{z}, \\mathbf{x})} d \\mathbf{z} \\\\ &=\\log \\left(p_{\\theta}(\\mathbf{x})\\right)+\\int q_{\\Phi}(\\mathbf{z} \\mid \\mathbf{x}) \\log \\frac{q_{\\Phi}(\\mathbf{z} \\mid \\mathbf{x})}{p_{\\theta}(\\mathbf{x} \\mid \\mathbf{z}) p_{\\theta}(\\mathbf{z})} d \\mathbf{z} \\\\ &=\\log \\left(p_{\\theta}(\\mathbf{x})\\right)+E_{\\mathbf{z} \\sim q_{\\Phi}(\\mathbf{z} \\mid \\mathbf{x})}\\left(\\log \\frac{q_{\\Phi}(\\mathbf{z} \\mid \\mathbf{x})}{p_{\\theta}(\\mathbf{z})}-\\log \\left(p_{\\theta}(\\mathbf{x} \\mid \\mathbf{z})\\right)\\right) \\\\ &=\\log \\left(p_{\\theta}(\\mathbf{x})\\right)+D_{K L}\\left(q_{\\Phi}(\\mathbf{z} \\mid \\mathbf{x}) \\| p_{\\theta}(\\mathbf{z})\\right)-E_{\\mathbf{z} \\sim q_{\\Phi}(\\mathbf{z} \\mid \\mathbf{x})}\\left(\\log \\left(p_{\\theta}(\\mathbf{x} \\mid \\mathbf{z})\\right)\\right) \\end{aligned} \u7a0d\u5fae\u53d8\u5f62\uff0c\u5c06\u6211\u4eec\u60f3\u4f18\u5316\u7684\u5185\u5bb9\u653e\u4e00\u8fb9\uff0c\u53ef\u4ee5\u8ba1\u7b97\u7684\u5185\u5bb9\u653e\u53e6\u4e00\u8fb9: \\log \\left(p_{\\theta}(\\mathbf{x})\\right)-D_{K L}\\left(q_{\\Phi}(\\mathbf{z} \\mid \\mathbf{x}) \\| p_{\\theta}(\\mathbf{z} \\mid \\mathbf{x})\\right)=E_{\\mathbf{z} \\sim q_{\\Phi}(\\mathbf{z} \\mid \\mathbf{x})}\\left(\\log \\left(p_{\\theta}(\\mathbf{x} \\mid \\mathbf{z})\\right)\\right)-D_{K L}\\left(q_{\\Phi}(\\mathbf{z} \\mid \\mathbf{x}) \\| p_{\\theta}(\\mathbf{z})\\right) \u5de6\u8fb9\u7b2c\u4e00\u9879\u662f\u6211\u4eec\u60f3\u8981\u6700\u5927\u5316\u7684\u8868\u5f81\u89e3\u7801\u5668\u5bf9\u6570\u636e\u5206\u5e03\u7684\u5efa\u6a21\u6b63\u786e\u5ea6\u7684\u4f3c\u7136\u503c\uff0c\u7b2c\u4e8c\u9879\u662f\u7f16\u7801\u5668\u5bf9\u9690\u53d8\u91cf\u7684\u5efa\u6a21\u4e0e\u5b9e\u9645\u9690\u53d8\u91cf\u540e\u9a8c\u6982\u7387\u7684\u6982\u7387\u6a21\u578b\u5dee\u5f02\uff0c\u662f\u6211\u4eec\u60f3\u8981\u6700\u5c0f\u5316\u7684\u5185\u5bb9. \u53f3\u8fb9\u7b2c\u4e00\u9879\u662f\u7531\u7f16\u7801\u5668\u91c7\u6837\u7684\u9690\u53d8\u91cf\u8fdb\u884c\u89e3\u7801\u5f97\u5230\u7684\u6570\u636e\u7684\u4f3c\u7136\u503c\uff0c\u5b9e\u73b0\u8d77\u6765\u5c31\u662freconstruction loss, \u7b2c\u4e8c\u9879\u6307\u7f16\u7801\u5668\u8f93\u51fa\u4e0e\u9690\u53d8\u91cf\u672c\u8eab\u7684\u5148\u9a8c\u4e4b\u95f4\u7684\u6982\u7387\u5dee\u5f02\uff0c\u5b9e\u73b0\u8d77\u6765\u5c31\u662f\u4e00\u4e2a\u9650\u5236\u7f16\u7801\u5668\u8f93\u51fa\u5e45\u503c\u7684regularizer. \u6211\u4eec\u4ee4\u635f\u5931\u51fd\u6570 L_{\\theta, \\Phi}=-\\log \\left(p_{\\theta}(\\mathbf{x})\\right)+D_{K L}\\left(q_{\\Phi}(\\mathbf{z} \\mid \\mathbf{x}) \\| p_{\\theta}(\\mathbf{z} \\mid \\mathbf{x})\\right)=-E_{\\mathbf{z} \\sim q_{\\Phi}(\\mathbf{z} \\mid \\mathbf{x})}\\left(\\log \\left(p_{\\theta}(\\mathbf{x} \\mid \\mathbf{z})\\right)\\right)+D_{K L}\\left(q_{\\Phi}(\\mathbf{z} \\mid \\mathbf{x}) \\| p_{\\theta}(\\mathbf{z})\\right) \u7531KL divergence\u7684\u975e\u8d1f\u6027\uff0c\u53ef\u4ee5\u5f97\u5230 -L_{\\theta, \\Phi}=\\log \\left(p_{\\theta}(\\mathbf{x})\\right)-D_{K L}\\left(q_{\\Phi}(\\mathbf{z} \\mid \\mathbf{x}) \\| p_{\\theta}(\\mathbf{z} \\mid \\mathbf{x})\\right) \\leq \\log \\left(p_{\\theta}(\\mathbf{x})\\right) \u56e0\u800c\u635f\u5931\u51fd\u6570\u4f18\u5316\u7684\u5185\u5bb9\u53c8\u53ef\u4ee5\u7406\u89e3\u4e3a \\log{p_\\theta(x)} \u7684\u4e0b\u9650\uff0c\u56e0\u800c\u88ab\u6210\u4e3a evidence lower bound (ELBO).","title":"Auto-Encoding Variational Bayes"},{"location":"The_theory/VovNet/","text":"An Energy and GPU-Computation Efficient Backbone Network for Real-Time Object Detection \u8fd9\u7bc7paper\u4ece DenseNet\u51fa\u53d1\uff0c\u7ed3\u5408\u8fd0\u7b97\u901f\u7387\u4e0e\u80fd\u91cf\u6548\u7387\u8c03\u6574\u66f4\u597d\u7684backbone\u7ed3\u6784\u3002 VovNet OSA\u6a21\u5757\u7ed3\u6784 \u5f71\u54cd\u8fd0\u7b97\u901f\u5ea6\u4e0e\u80fd\u91cf\u6d88\u8017\u7684\u5173\u952e\u56e0\u7d20 \u672c\u6587\u7ed3\u5408 ShuffleNet_V2 \u7684\u7ed3\u8bba\uff0c\u5e76\u7ee7\u7eed\u5ef6\u4f38\u3002 \u672c\u6587\u6307\u51fa\u5185\u5b58\u7684\u8bfb\u53d6\u4f7f\u7528(MAC)\u65f6\u95f4\u4ee5\u53ca\u80fd\u91cf\u6d88\u8017\u5f80\u5f80\u4f1a\u591a\u4e8e\u8ba1\u7b97\u7684\u82b1\u8d39\u3002 \u5bf9\u4e8e\u5377\u79ef\u5c42 MAC = hw(c_i + c_i) + k^2 c_i c_o \u7531 ShuffleNet_V2 ,\u540c\u6837\u7684\u7ed3\u8bba\uff0c\u5e94\u8be5\u5c3d\u53ef\u80fd\u4fdd\u8bc1\u8f93\u5165\u8f93\u51fafeature\u4e00\u6837\u591a\uff0c\u56e0\u800cDenseNet\u7684\u7ed3\u6784\u6548\u7387\u4e0d\u591f\u597d\u3002 DenseNet\u7684\u95ee\u9898: DenseNet\u7684\u6743\u91cdnorm\u53cd\u6620\u4e86\u5b83\u7684\u5197\u4f59\uff0c\u8981\u4e48Intermediate \u6743\u91cd\u5f88\u4f4e\uff0c\u8981\u4e48\u662f\u524d\u9762\u7684\u6743\u91cd\u5f88\u4f4e\u3002\u56e0\u800c\u7528OSA\u66ff\u4ee3Dense Connection.","title":"An Energy and GPU-Computation Efficient Backbone Network for Real-Time Object Detection"},{"location":"The_theory/VovNet/#an-energy-and-gpu-computation-efficient-backbone-network-for-real-time-object-detection","text":"\u8fd9\u7bc7paper\u4ece DenseNet\u51fa\u53d1\uff0c\u7ed3\u5408\u8fd0\u7b97\u901f\u7387\u4e0e\u80fd\u91cf\u6548\u7387\u8c03\u6574\u66f4\u597d\u7684backbone\u7ed3\u6784\u3002","title":"An Energy and GPU-Computation Efficient Backbone Network for Real-Time Object Detection"},{"location":"The_theory/VovNet/#vovnet-osa","text":"","title":"VovNet OSA\u6a21\u5757\u7ed3\u6784"},{"location":"The_theory/VovNet/#_1","text":"\u672c\u6587\u7ed3\u5408 ShuffleNet_V2 \u7684\u7ed3\u8bba\uff0c\u5e76\u7ee7\u7eed\u5ef6\u4f38\u3002 \u672c\u6587\u6307\u51fa\u5185\u5b58\u7684\u8bfb\u53d6\u4f7f\u7528(MAC)\u65f6\u95f4\u4ee5\u53ca\u80fd\u91cf\u6d88\u8017\u5f80\u5f80\u4f1a\u591a\u4e8e\u8ba1\u7b97\u7684\u82b1\u8d39\u3002 \u5bf9\u4e8e\u5377\u79ef\u5c42 MAC = hw(c_i + c_i) + k^2 c_i c_o \u7531 ShuffleNet_V2 ,\u540c\u6837\u7684\u7ed3\u8bba\uff0c\u5e94\u8be5\u5c3d\u53ef\u80fd\u4fdd\u8bc1\u8f93\u5165\u8f93\u51fafeature\u4e00\u6837\u591a\uff0c\u56e0\u800cDenseNet\u7684\u7ed3\u6784\u6548\u7387\u4e0d\u591f\u597d\u3002","title":"\u5f71\u54cd\u8fd0\u7b97\u901f\u5ea6\u4e0e\u80fd\u91cf\u6d88\u8017\u7684\u5173\u952e\u56e0\u7d20"},{"location":"The_theory/VovNet/#densenet","text":"DenseNet\u7684\u6743\u91cdnorm\u53cd\u6620\u4e86\u5b83\u7684\u5197\u4f59\uff0c\u8981\u4e48Intermediate \u6743\u91cd\u5f88\u4f4e\uff0c\u8981\u4e48\u662f\u524d\u9762\u7684\u6743\u91cd\u5f88\u4f4e\u3002\u56e0\u800c\u7528OSA\u66ff\u4ee3Dense Connection.","title":"DenseNet\u7684\u95ee\u9898:"},{"location":"The_theory/Why_gradien_clip_norm/","text":"WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY \u8fd9\u7bc7paper\u662f2020ICLR\u7684\u6ee1\u5206\u8bba\u6587\uff0c\u4f5c\u8005\u4ece\u7406\u8bba\u4e0e\u5b9e\u8df5\u4e0a\u53bb\u8bba\u8bc1\u4e86\u4e3a\u4ec0\u4e48\u5efa\u8bae clip gradient norm \u68af\u5ea6\u7684\u674e\u666e\u5e0c\u5179\u8fde\u7eed(Lipschitz Continuity) \u674e\u666e\u5e0c\u5179\u8fde\u7eed \u539f\u6307\u68af\u5ea6\u7684\u7edd\u5bf9\u503c\u6709\u6709\u9650\u4e0a\u9650\u3002 \u4f18\u5316\u4e2d\uff0c\u5b9a\u4e49\u4e00\u4e2a\u51fd\u6570\u4e3a L -\u5149\u6ed1( L-smooth )\u5982\u679c\u5bf9\u4efb\u610fx,y \\|\\nabla f(x)-\\nabla f(y)\\| \\leq L\\|x-y\\| \u53ef\u4ee5\u7406\u89e3\u4e3a\u51fd\u6570\u7684\u4e8c\u9636\u5bfc\u6709\u9650\u3002\u5bf9\u4e8e\u8fd9\u79cd\u6210\u672c\u51fd\u6570\uff0c\u68af\u5ea6\u4e0b\u964d\u4e2d\u9009\u62e9\u5b66\u4e60\u7387 h = 1/L \u4e3a\u7406\u8bba\u6700\u4f18\u5b66\u4e60\u7387\u9009\u62e9\u3002 \u4f46\u662f\u4e00\u4e2a\u7b80\u5355\u7684 y = x^3 \u51fd\u6570\u5c31\u5df2\u7ecf\u6253\u7834\u8fd9\u4e2a L -\u5149\u6ed1\u7684\u7406\u8bba\u9650\u5236\uff0c\u66f4\u4e0d\u7528\u8bf4\u9ad8\u7ef4\u7684\u7f51\u7edc\u51fd\u6570\u3002\u5bf9\u4e8e\u8fd9\u6837\u7684\u51fd\u6570\uff0c\u4f20\u7edf\u7684\u65b9\u6cd5\u662f\u8bbe\u7f6e\u4e00\u4e2a\u5b9a\u4e49\u57df\u7684\u8f93\u5165\u8303\u56f4\uff0c\u5f97\u5230\u4e00\u4e2a\u6709\u9650\u7684 L \u503c,\u4e3a\u4e86\u80fd\u6ee1\u8db3\u8db3\u591f\u591a\u7684\u8f93\u5165\u8303\u56f4\uff0c\u8fd9\u4e2aL\u503c\u5f80\u5f80\u4f1a\u8f83\u5927\uff0c\u4f7f\u5f97\u5b66\u4e60\u7387\u66f4\u4e3a\u4fdd\u5b88\u3002 \u53e6\u5916\u4f5c\u8005\u5728\u5b9e\u9a8c\u4e2d\u53d1\u73b0\u4e86gradient norm\u4e0e\u5149\u6ed1\u5ea6 L \u4e4b\u95f4\u7684\u5173\u7cfb (L_0-L_1) \u5149\u6ed1 \u5b9a\u4e49 \\left\\|\\nabla^{2} f(x)\\right\\| \\leq L_{0}+L_{1}\\|\\nabla f(x)\\| \u5373\u6ee1\u8db3\u4e8c\u9636\u5bfc\u77e9\u9635\u7684\u6a21\u4e0e\u4e00\u9636\u5bfc\u77e9\u9635\u7684\u6a21\u7684\u6bd4\u503c\u6709\u9650\u3002\u8fd9\u4e2a\u662f\u4e00\u4e2a\u6bd4\u8f83\u677e\u7684\u5149\u6ed1\u5b9a\u4e49\uff0c\u5bb9\u6613\u77e5\u9053\uff0c\u4efb\u610f\u9ad8\u9636\u7684\u591a\u9879\u5f0f\u51fd\u6570\u90fd\u4f1a\u5728\u6574\u4e2a\u5b9e\u6570\u96c6\u4e0a\u6ee1\u8db3\u8fd9\u4e00\u6761\u4ef6 \u4e4b\u540e\u4f5c\u8005\u6570\u5b66\u8bc1\u660e\u4e86\u5f53\u7b26\u5408\u6b64\u5149\u6ed1\u6761\u4ef6\u65f6\uff0c\u5982\u679c\u521d\u59cb\u5316\u4e0e\u5b9e\u9645\u7ed3\u679c\u6709\u4e00\u5b9a\u5dee\u8ddd\uff0cclip-GD\u7684\u6536\u655b\u901f\u5ea6\u4f1a\u8fdc\u8fdc\u5feb\u4e8efix GD Remark 5 . Theorem 1 of Carmon et al. (2017) and Theorem 4 together show that gradient descent with a fixed step size cannot converge to an \\epsilon -stationary point faster than \\Omega\\left(\\left(L_{1} M / \\log (M)+L_{0}\\right)\\left(f\\left(x_{0}\\right)-f^{*}\\right) \\epsilon^{-2}\\right) . Recall that clipped GD algorithm converges as \\mathcal{O}\\left(L_{0}\\left(f\\left(x_{0}\\right)-f^{*}\\right) \\epsilon^{-2}+L_{1}^{2}\\left(f\\left(x_{0}\\right)-f^{*}\\right) L_{0}^{-1}\\right) . Therefore, clipped GD can be arbitrarily faster than GD when L_{1} M is large, or in other words, when the problem has a poor initialization.","title":"WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY"},{"location":"The_theory/Why_gradien_clip_norm/#why-gradient-clipping-accelerates-training-a-theoretical-justification-for-adaptivity","text":"\u8fd9\u7bc7paper\u662f2020ICLR\u7684\u6ee1\u5206\u8bba\u6587\uff0c\u4f5c\u8005\u4ece\u7406\u8bba\u4e0e\u5b9e\u8df5\u4e0a\u53bb\u8bba\u8bc1\u4e86\u4e3a\u4ec0\u4e48\u5efa\u8bae clip gradient norm","title":"WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY"},{"location":"The_theory/Why_gradien_clip_norm/#lipschitz-continuity","text":"\u674e\u666e\u5e0c\u5179\u8fde\u7eed \u539f\u6307\u68af\u5ea6\u7684\u7edd\u5bf9\u503c\u6709\u6709\u9650\u4e0a\u9650\u3002 \u4f18\u5316\u4e2d\uff0c\u5b9a\u4e49\u4e00\u4e2a\u51fd\u6570\u4e3a L -\u5149\u6ed1( L-smooth )\u5982\u679c\u5bf9\u4efb\u610fx,y \\|\\nabla f(x)-\\nabla f(y)\\| \\leq L\\|x-y\\| \u53ef\u4ee5\u7406\u89e3\u4e3a\u51fd\u6570\u7684\u4e8c\u9636\u5bfc\u6709\u9650\u3002\u5bf9\u4e8e\u8fd9\u79cd\u6210\u672c\u51fd\u6570\uff0c\u68af\u5ea6\u4e0b\u964d\u4e2d\u9009\u62e9\u5b66\u4e60\u7387 h = 1/L \u4e3a\u7406\u8bba\u6700\u4f18\u5b66\u4e60\u7387\u9009\u62e9\u3002 \u4f46\u662f\u4e00\u4e2a\u7b80\u5355\u7684 y = x^3 \u51fd\u6570\u5c31\u5df2\u7ecf\u6253\u7834\u8fd9\u4e2a L -\u5149\u6ed1\u7684\u7406\u8bba\u9650\u5236\uff0c\u66f4\u4e0d\u7528\u8bf4\u9ad8\u7ef4\u7684\u7f51\u7edc\u51fd\u6570\u3002\u5bf9\u4e8e\u8fd9\u6837\u7684\u51fd\u6570\uff0c\u4f20\u7edf\u7684\u65b9\u6cd5\u662f\u8bbe\u7f6e\u4e00\u4e2a\u5b9a\u4e49\u57df\u7684\u8f93\u5165\u8303\u56f4\uff0c\u5f97\u5230\u4e00\u4e2a\u6709\u9650\u7684 L \u503c,\u4e3a\u4e86\u80fd\u6ee1\u8db3\u8db3\u591f\u591a\u7684\u8f93\u5165\u8303\u56f4\uff0c\u8fd9\u4e2aL\u503c\u5f80\u5f80\u4f1a\u8f83\u5927\uff0c\u4f7f\u5f97\u5b66\u4e60\u7387\u66f4\u4e3a\u4fdd\u5b88\u3002 \u53e6\u5916\u4f5c\u8005\u5728\u5b9e\u9a8c\u4e2d\u53d1\u73b0\u4e86gradient norm\u4e0e\u5149\u6ed1\u5ea6 L \u4e4b\u95f4\u7684\u5173\u7cfb","title":"\u68af\u5ea6\u7684\u674e\u666e\u5e0c\u5179\u8fde\u7eed(Lipschitz Continuity)"},{"location":"The_theory/Why_gradien_clip_norm/#l_0-l_1","text":"\u5b9a\u4e49 \\left\\|\\nabla^{2} f(x)\\right\\| \\leq L_{0}+L_{1}\\|\\nabla f(x)\\| \u5373\u6ee1\u8db3\u4e8c\u9636\u5bfc\u77e9\u9635\u7684\u6a21\u4e0e\u4e00\u9636\u5bfc\u77e9\u9635\u7684\u6a21\u7684\u6bd4\u503c\u6709\u9650\u3002\u8fd9\u4e2a\u662f\u4e00\u4e2a\u6bd4\u8f83\u677e\u7684\u5149\u6ed1\u5b9a\u4e49\uff0c\u5bb9\u6613\u77e5\u9053\uff0c\u4efb\u610f\u9ad8\u9636\u7684\u591a\u9879\u5f0f\u51fd\u6570\u90fd\u4f1a\u5728\u6574\u4e2a\u5b9e\u6570\u96c6\u4e0a\u6ee1\u8db3\u8fd9\u4e00\u6761\u4ef6 \u4e4b\u540e\u4f5c\u8005\u6570\u5b66\u8bc1\u660e\u4e86\u5f53\u7b26\u5408\u6b64\u5149\u6ed1\u6761\u4ef6\u65f6\uff0c\u5982\u679c\u521d\u59cb\u5316\u4e0e\u5b9e\u9645\u7ed3\u679c\u6709\u4e00\u5b9a\u5dee\u8ddd\uff0cclip-GD\u7684\u6536\u655b\u901f\u5ea6\u4f1a\u8fdc\u8fdc\u5feb\u4e8efix GD Remark 5 . Theorem 1 of Carmon et al. (2017) and Theorem 4 together show that gradient descent with a fixed step size cannot converge to an \\epsilon -stationary point faster than \\Omega\\left(\\left(L_{1} M / \\log (M)+L_{0}\\right)\\left(f\\left(x_{0}\\right)-f^{*}\\right) \\epsilon^{-2}\\right) . Recall that clipped GD algorithm converges as \\mathcal{O}\\left(L_{0}\\left(f\\left(x_{0}\\right)-f^{*}\\right) \\epsilon^{-2}+L_{1}^{2}\\left(f\\left(x_{0}\\right)-f^{*}\\right) L_{0}^{-1}\\right) . Therefore, clipped GD can be arbitrarily faster than GD when L_{1} M is large, or in other words, when the problem has a poor initialization.","title":"(L_0-L_1)\u5149\u6ed1"},{"location":"The_theory/bayesianNetwork/","text":"A comprehensive guide to bayesian convolutional neural network with variational inference \u8fd9\u7bc7\u6587\u7ae0\u662f\u6574\u5408\u4e86\u4e00\u4e2a\u9636\u6bb5\u7684BNN\u7f51\u7edc\u7684\u7814\u7a76\uff0c\u5e76\u7528\u5728\u4e86CNN\u4e0a\u3002 \u76f8\u5173\u7684tutorial BNN\u7684\u57fa\u672c\u65b9\u6cd5\u8bba\u57fa\u672c\u4e0a\u662f: - \u8bad\u7ec3\u7684\u65f6\u5019\uff0c\u9700\u8981\u5f97\u5230\u7f51\u7edc\u6743\u91cd\u7684\u5206\u5e03\u800c\u4e0d\u662f\u4e00\u4e2a\u70b9\u4f30\u8ba1, \u76ee\u524d\u4e3b\u6d41\u7684\u5b9e\u9645\u64cd\u4f5c\u662f\u5c06\u6bcf\u4e00\u4e2a\u6743\u91cd\u503c\u7406\u89e3\u4e3a\u4e00\u4e2a\u72ec\u7acb\u7684\u9ad8\u65af\u5206\u5e03 - \u63a8\u7406\u7684\u65f6\u5019\uff0c\u53ef\u4ee5\u8ba9\u7f51\u7edc\u53ea\u53d6mean\u503c\uff0c\u53ea\u8f93\u51fa\u4e00\u4e2a\u70b9\u4f30\u8ba1\u3002\u4e5f\u53ef\u4ee5\u5bf9\u5168\u7f51\u7edc\u591a\u6b21\u91c7\u6837\uff0c\u7528\u8499\u7279\u5361\u6d1b\u7684\u65b9\u5f0f\u8f93\u51fa\u591a\u4e2a\u70b9\u4f30\u8ba1\uff0c\u53ef\u4ee5\u4f7f\u7528bagging\u878d\u5408\uff0c\u4e5f\u53ef\u4ee5\u4f30\u8ba1\u4e0d\u786e\u5b9a\u6027\u3002 - \u6743\u91cd\u7684\u9ad8\u65af\u5206\u5e03\uff0c\u5728\u8bad\u7ec3\u65f6\u4ee5re-parametrization\u7684\u5f62\u5f0f\u524d\u4f20\u5e76\u8bad\u7ec3\uff0c\u5bf9\u4e8e\u5168\u8fde\u63a5\u5c42\u4ee5\u53ca\u5377\u79ef\u5c42\uff0c\u6709local-reparametrization\u7684\u5f62\u5f0f\u524d\u4f20\uff0c\u53ef\u4ee5\u63d0\u9ad8\u6548\u7387. \u7406\u8bba \u5df2\u77e5\u8bad\u7ec3\u96c6 D , \u6d4b\u8bd5\u96c6\u6570\u636e x_{test} , \u76ee\u6807\u662f\u8f93\u51fa y_{test} \u7684\u6982\u7387\u5206\u5e03\u3002 \u63cf\u8ff0\u4e3a p(y_{test}|x_{test}, D) \u7528\u5168\u6982\u7387\u516c\u5f0f\u5173\u4e8e\u6743\u91cd\u5c55\u5f00\u3002 p(y_{test} | x_{test}, D) = \\int_{\\theta} p(y_{test}| x_{test}, \\theta') p(\\theta'|D) d \\theta' p(\\theta'|D) \u53ef\u4ee5\u7406\u89e3\u4e3a\u5404\u7ec4\u70b9\u4f30\u8ba1\u7684\u7f51\u7edc\u6743\u91cd\u5728\u8bad\u7ec3\u6570\u636e\u96c6\u4e2d\u7684likelihood. \u8fd9\u4e2alikelihood\u53ef\u4ee5\u8fdb\u4e00\u6b65\u7528\u8d1d\u53f6\u65af\u516c\u5f0f\u62c6\u89e3: p(\\theta | D) = \\frac{p(D_y | D_x, \\theta) p(\\theta)}{\\int_\\theta p(D_y | D_x, \\theta') p(\\theta') d \\theta'} \\propto p(D_y | D_x, \\theta) p(\\theta) \u5206\u6bcd\u662f\u5f52\u4e00\u5316\uff0c\u5206\u5b50\u7b2c\u4e00\u9879\u662f\u8bad\u7ec3\u96c6\u4e0a\u7684likelihood,\u5b9e\u73b0\u4e0a\u5e38\u5e38\u53ef\u4ee5\u7406\u89e3\u4e3ascore\u6216\u8005loss\u3002 \u7b2c\u4e8c\u9879\u662f\u5148\u9a8c\u5206\u5e03\uff0c\u53ef\u4ee5\u7406\u89e3\u4e3aregularization.","title":"A comprehensive guide to bayesian convolutional neural network with variational inference"},{"location":"The_theory/bayesianNetwork/#a-comprehensive-guide-to-bayesian-convolutional-neural-network-with-variational-inference","text":"\u8fd9\u7bc7\u6587\u7ae0\u662f\u6574\u5408\u4e86\u4e00\u4e2a\u9636\u6bb5\u7684BNN\u7f51\u7edc\u7684\u7814\u7a76\uff0c\u5e76\u7528\u5728\u4e86CNN\u4e0a\u3002 \u76f8\u5173\u7684tutorial BNN\u7684\u57fa\u672c\u65b9\u6cd5\u8bba\u57fa\u672c\u4e0a\u662f: - \u8bad\u7ec3\u7684\u65f6\u5019\uff0c\u9700\u8981\u5f97\u5230\u7f51\u7edc\u6743\u91cd\u7684\u5206\u5e03\u800c\u4e0d\u662f\u4e00\u4e2a\u70b9\u4f30\u8ba1, \u76ee\u524d\u4e3b\u6d41\u7684\u5b9e\u9645\u64cd\u4f5c\u662f\u5c06\u6bcf\u4e00\u4e2a\u6743\u91cd\u503c\u7406\u89e3\u4e3a\u4e00\u4e2a\u72ec\u7acb\u7684\u9ad8\u65af\u5206\u5e03 - \u63a8\u7406\u7684\u65f6\u5019\uff0c\u53ef\u4ee5\u8ba9\u7f51\u7edc\u53ea\u53d6mean\u503c\uff0c\u53ea\u8f93\u51fa\u4e00\u4e2a\u70b9\u4f30\u8ba1\u3002\u4e5f\u53ef\u4ee5\u5bf9\u5168\u7f51\u7edc\u591a\u6b21\u91c7\u6837\uff0c\u7528\u8499\u7279\u5361\u6d1b\u7684\u65b9\u5f0f\u8f93\u51fa\u591a\u4e2a\u70b9\u4f30\u8ba1\uff0c\u53ef\u4ee5\u4f7f\u7528bagging\u878d\u5408\uff0c\u4e5f\u53ef\u4ee5\u4f30\u8ba1\u4e0d\u786e\u5b9a\u6027\u3002 - \u6743\u91cd\u7684\u9ad8\u65af\u5206\u5e03\uff0c\u5728\u8bad\u7ec3\u65f6\u4ee5re-parametrization\u7684\u5f62\u5f0f\u524d\u4f20\u5e76\u8bad\u7ec3\uff0c\u5bf9\u4e8e\u5168\u8fde\u63a5\u5c42\u4ee5\u53ca\u5377\u79ef\u5c42\uff0c\u6709local-reparametrization\u7684\u5f62\u5f0f\u524d\u4f20\uff0c\u53ef\u4ee5\u63d0\u9ad8\u6548\u7387.","title":"A comprehensive guide to bayesian convolutional neural network with variational inference"},{"location":"The_theory/bayesianNetwork/#_1","text":"\u5df2\u77e5\u8bad\u7ec3\u96c6 D , \u6d4b\u8bd5\u96c6\u6570\u636e x_{test} , \u76ee\u6807\u662f\u8f93\u51fa y_{test} \u7684\u6982\u7387\u5206\u5e03\u3002 \u63cf\u8ff0\u4e3a p(y_{test}|x_{test}, D) \u7528\u5168\u6982\u7387\u516c\u5f0f\u5173\u4e8e\u6743\u91cd\u5c55\u5f00\u3002 p(y_{test} | x_{test}, D) = \\int_{\\theta} p(y_{test}| x_{test}, \\theta') p(\\theta'|D) d \\theta' p(\\theta'|D) \u53ef\u4ee5\u7406\u89e3\u4e3a\u5404\u7ec4\u70b9\u4f30\u8ba1\u7684\u7f51\u7edc\u6743\u91cd\u5728\u8bad\u7ec3\u6570\u636e\u96c6\u4e2d\u7684likelihood. \u8fd9\u4e2alikelihood\u53ef\u4ee5\u8fdb\u4e00\u6b65\u7528\u8d1d\u53f6\u65af\u516c\u5f0f\u62c6\u89e3: p(\\theta | D) = \\frac{p(D_y | D_x, \\theta) p(\\theta)}{\\int_\\theta p(D_y | D_x, \\theta') p(\\theta') d \\theta'} \\propto p(D_y | D_x, \\theta) p(\\theta) \u5206\u6bcd\u662f\u5f52\u4e00\u5316\uff0c\u5206\u5b50\u7b2c\u4e00\u9879\u662f\u8bad\u7ec3\u96c6\u4e0a\u7684likelihood,\u5b9e\u73b0\u4e0a\u5e38\u5e38\u53ef\u4ee5\u7406\u89e3\u4e3ascore\u6216\u8005loss\u3002 \u7b2c\u4e8c\u9879\u662f\u5148\u9a8c\u5206\u5e03\uff0c\u53ef\u4ee5\u7406\u89e3\u4e3aregularization.","title":"\u7406\u8bba"},{"location":"The_theory/compondingTechforCNN/","text":"Compounding the Performance Improvements of Assembled Techniques in a Convolutional Neural Network This paper is essentially a summary for a bag of training techniques for training CNN. Neural Network Model ResNet-D Channel-Attention (SE & SK) Squeeze and Excitation (SE) module has been introduced by this paper Selective Kernel can be described in: A light-weight SK implementation in pytorch can be found here Anti-Alias Downsampling AA Downsampling is first proposed in this paper.pdf . This is also called Blur-Pool implemented in keras here Big Little Network (BL) Big Little Network is first proposed in This paper.pdf Regularization AutoAugment Auto Augmentation is proposed in This paper.pdf which applies reinforcement learning to train a agent to do image augmentation. Tensorflow open source code can be found here Mixup Mix up has been introduced in Bag of Freebies DropBlock Dropblock is first proposed in This paper.pdf dropblock has been implemented in Pytorch here Label Smoothing label smoothing \u6e90\u81eainception-v3,\u5728\u8bad\u7ec3\u65f6\u6709\u4e00\u5b9a\u6982\u7387\u4e0d\u91c7\u53d6\u539flabel\uff0c\u800c\u5747\u5300\u968f\u673a\u9009\u62e9\u53e6\u4e00\u4e2aclass\u4f5c\u4e3aground truth. The paper indeed shows that almost all methods induce improvement in ImageNet results and transfer learning result","title":"Compounding the Performance Improvements of Assembled Techniques in a Convolutional Neural Network"},{"location":"The_theory/compondingTechforCNN/#compounding-the-performance-improvements-of-assembled-techniques-in-a-convolutional-neural-network","text":"This paper is essentially a summary for a bag of training techniques for training CNN.","title":"Compounding the Performance Improvements of Assembled Techniques in a Convolutional Neural Network"},{"location":"The_theory/compondingTechforCNN/#neural-network-model","text":"","title":"Neural Network Model"},{"location":"The_theory/compondingTechforCNN/#resnet-d","text":"","title":"ResNet-D"},{"location":"The_theory/compondingTechforCNN/#channel-attention-se-sk","text":"Squeeze and Excitation (SE) module has been introduced by this paper Selective Kernel can be described in: A light-weight SK implementation in pytorch can be found here","title":"Channel-Attention (SE &amp; SK)"},{"location":"The_theory/compondingTechforCNN/#anti-alias-downsampling","text":"AA Downsampling is first proposed in this paper.pdf . This is also called Blur-Pool implemented in keras here","title":"Anti-Alias Downsampling"},{"location":"The_theory/compondingTechforCNN/#big-little-network-bl","text":"Big Little Network is first proposed in This paper.pdf","title":"Big Little Network (BL)"},{"location":"The_theory/compondingTechforCNN/#regularization","text":"","title":"Regularization"},{"location":"The_theory/compondingTechforCNN/#autoaugment","text":"Auto Augmentation is proposed in This paper.pdf which applies reinforcement learning to train a agent to do image augmentation. Tensorflow open source code can be found here","title":"AutoAugment"},{"location":"The_theory/compondingTechforCNN/#mixup","text":"Mix up has been introduced in Bag of Freebies","title":"Mixup"},{"location":"The_theory/compondingTechforCNN/#dropblock","text":"Dropblock is first proposed in This paper.pdf dropblock has been implemented in Pytorch here","title":"DropBlock"},{"location":"The_theory/compondingTechforCNN/#label-smoothing","text":"label smoothing \u6e90\u81eainception-v3,\u5728\u8bad\u7ec3\u65f6\u6709\u4e00\u5b9a\u6982\u7387\u4e0d\u91c7\u53d6\u539flabel\uff0c\u800c\u5747\u5300\u968f\u673a\u9009\u62e9\u53e6\u4e00\u4e2aclass\u4f5c\u4e3aground truth. The paper indeed shows that almost all methods induce improvement in ImageNet results and transfer learning result","title":"Label Smoothing"},{"location":"The_theory/ddn/","text":"Deep Declarative Networks: A New Hope \u8fd9\u4e2a\u6982\u5ff5\u7c7b\u4f3c\u4e8e\u6df1\u5ea6\u5747\u8861paper\u91cc\u9762\u7684 implicit deep learning \uff0c\u4e5f\u5c31\u662f\u7f51\u7edc\u6a21\u5757\u7684\u5b9a\u4e49\u7684\u662f\u7531\u5bf9\u8f93\u51fa\u7ed3\u679c\u7684\u5b9a\u4e49\u6765\u51b3\u5b9a\u7684\u3002 MDEQ \u8003\u8651\u7684\u662f\u5b9e\u73b0\u4e00\u4e2a\u65e0\u7a77\u6df1\u7684\u7f51\u7edc\uff0c\u66f4\u591a\u60c5\u51b5\u4e0b\u8fd9\u7c7b\u7f51\u7edc\u5c42\u4f7f\u7528\u7684\u662f\u4f18\u5316\u95ee\u9898\uff0c\u5982 EPnP , OptNet , SS3D \u8fd9\u7bc7paper\u5219\u7ed9\u51fa\u4e00\u7c7b\u57fa\u4e8e\u4f18\u5316\u7684 declarative networks \u7684\u6c42\u5bfc\u8bad\u7ec3\u65b9\u6cd5\u3002 \u4ee5\u65e0\u7ea6\u675f\u7684\u4f18\u5316\u95ee\u9898\u4e3a\u4f8b\uff0c\u8f93\u5165\u4e3a x , \u8f93\u51fa\u4e3a y , \\begin{array}{ll} \\operatorname{minimize} & J(x, y) \\\\ \\text { subject to } & y \\in \\arg \\min _{u \\in C} f(x, u) \\end{array} \u5728\u6700\u4f18\u7684\u70b9\u4e0a\uff0c \\frac{\\partial f}{\\partial y} = 0 , \u4e14\u8fd9\u4e2a\u4e0d\u7531 x \u53d8\u5316\uff0c\u518d\u6b21\u6c42\u5bfc\u53ef\u5f97 \\begin{aligned} 0_{m \\times n} &=\\mathrm{D}\\left(\\mathrm{D}_{Y} f(x, y)\\right)^{\\top} \\\\ &=\\mathrm{D}_{X Y}^{2} f(x, y)+\\mathrm{D}_{Y Y}^{2} f(x, y) \\mathrm{D} y(x) \\end{aligned} \\mathrm{D} y(x)=-\\left(\\mathrm{D}_{Y Y}^{2} f(x, y)\\right)^{-1} \\mathrm{D}_{X Y}^{2} f(x, y) \u5bf9\u4e8e\u5e26\u7b49\u5f0f\u4e0e\u4e0d\u7b49\u5f0f\u7684\u7ea6\u675f\uff0c\u672c\u6587\u5206\u522b\u7ed9\u51fa\u4e86\u6c42\u5bfc\u7684\u8ba1\u7b97\u65b9\u5f0f\u3002 \u4ee3\u7801 \u672c\u6587\u5f00\u6e90\u4e86\u524d\u6587\u6240\u6709\u7c7b\u578b\u7684\u8282\u70b9\u7684\u6c42\u5bfc\u65b9\u6cd5\u3002\u4ee3\u7801\u4e0a\u53ea\u8981\u5b9e\u73b0\u4e3b\u51fd\u6570\uff0c\u7ea6\u675f\u51fd\u6570\u4ee5\u53ca\u4f18\u5316\u8fc7\u7a0b(\u4e0e\u53cd\u5411\u65e0\u5173)\u5373\u53ef\u3002 \u5176\u6b21\u5728\u672c\u6587\u7684\u4ee3\u7801\u4e2d\u770b\u5230\u4e86pytorch\u81ea\u5e26\u7684\u4e00\u4e2a\u5b8c\u6574\u7684\u4f18\u5316\u51fd\u6570 torch.optim.LBFGS \u3002\u672c\u6587\u5229\u7528\u8fd9\u4e2a\u51fd\u6570\u5b9e\u73b0\u4e86\u4e00\u4e2a\u5f88\u901a\u7528\u7684PnP\u975e\u7ebf\u6027\u4f18\u5316","title":"Deep Declarative Networks: A New Hope"},{"location":"The_theory/ddn/#deep-declarative-networks-a-new-hope","text":"\u8fd9\u4e2a\u6982\u5ff5\u7c7b\u4f3c\u4e8e\u6df1\u5ea6\u5747\u8861paper\u91cc\u9762\u7684 implicit deep learning \uff0c\u4e5f\u5c31\u662f\u7f51\u7edc\u6a21\u5757\u7684\u5b9a\u4e49\u7684\u662f\u7531\u5bf9\u8f93\u51fa\u7ed3\u679c\u7684\u5b9a\u4e49\u6765\u51b3\u5b9a\u7684\u3002 MDEQ \u8003\u8651\u7684\u662f\u5b9e\u73b0\u4e00\u4e2a\u65e0\u7a77\u6df1\u7684\u7f51\u7edc\uff0c\u66f4\u591a\u60c5\u51b5\u4e0b\u8fd9\u7c7b\u7f51\u7edc\u5c42\u4f7f\u7528\u7684\u662f\u4f18\u5316\u95ee\u9898\uff0c\u5982 EPnP , OptNet , SS3D \u8fd9\u7bc7paper\u5219\u7ed9\u51fa\u4e00\u7c7b\u57fa\u4e8e\u4f18\u5316\u7684 declarative networks \u7684\u6c42\u5bfc\u8bad\u7ec3\u65b9\u6cd5\u3002 \u4ee5\u65e0\u7ea6\u675f\u7684\u4f18\u5316\u95ee\u9898\u4e3a\u4f8b\uff0c\u8f93\u5165\u4e3a x , \u8f93\u51fa\u4e3a y , \\begin{array}{ll} \\operatorname{minimize} & J(x, y) \\\\ \\text { subject to } & y \\in \\arg \\min _{u \\in C} f(x, u) \\end{array} \u5728\u6700\u4f18\u7684\u70b9\u4e0a\uff0c \\frac{\\partial f}{\\partial y} = 0 , \u4e14\u8fd9\u4e2a\u4e0d\u7531 x \u53d8\u5316\uff0c\u518d\u6b21\u6c42\u5bfc\u53ef\u5f97 \\begin{aligned} 0_{m \\times n} &=\\mathrm{D}\\left(\\mathrm{D}_{Y} f(x, y)\\right)^{\\top} \\\\ &=\\mathrm{D}_{X Y}^{2} f(x, y)+\\mathrm{D}_{Y Y}^{2} f(x, y) \\mathrm{D} y(x) \\end{aligned} \\mathrm{D} y(x)=-\\left(\\mathrm{D}_{Y Y}^{2} f(x, y)\\right)^{-1} \\mathrm{D}_{X Y}^{2} f(x, y) \u5bf9\u4e8e\u5e26\u7b49\u5f0f\u4e0e\u4e0d\u7b49\u5f0f\u7684\u7ea6\u675f\uff0c\u672c\u6587\u5206\u522b\u7ed9\u51fa\u4e86\u6c42\u5bfc\u7684\u8ba1\u7b97\u65b9\u5f0f\u3002","title":"Deep Declarative Networks: A New Hope"},{"location":"The_theory/ddn/#_1","text":"\u672c\u6587\u5f00\u6e90\u4e86\u524d\u6587\u6240\u6709\u7c7b\u578b\u7684\u8282\u70b9\u7684\u6c42\u5bfc\u65b9\u6cd5\u3002\u4ee3\u7801\u4e0a\u53ea\u8981\u5b9e\u73b0\u4e3b\u51fd\u6570\uff0c\u7ea6\u675f\u51fd\u6570\u4ee5\u53ca\u4f18\u5316\u8fc7\u7a0b(\u4e0e\u53cd\u5411\u65e0\u5173)\u5373\u53ef\u3002 \u5176\u6b21\u5728\u672c\u6587\u7684\u4ee3\u7801\u4e2d\u770b\u5230\u4e86pytorch\u81ea\u5e26\u7684\u4e00\u4e2a\u5b8c\u6574\u7684\u4f18\u5316\u51fd\u6570 torch.optim.LBFGS \u3002\u672c\u6587\u5229\u7528\u8fd9\u4e2a\u51fd\u6570\u5b9e\u73b0\u4e86\u4e00\u4e2a\u5f88\u901a\u7528\u7684PnP\u975e\u7ebf\u6027\u4f18\u5316","title":"\u4ee3\u7801"},{"location":"The_theory/deeplearning_foundation/","text":"Deep Learning \"Foundations\" \u5173\u4e8e\u795e\u7ecf\u7f51\u7edc\u4e3a\u4ec0\u4e48\u6709\u5f3a\u5927\u7684\u62df\u5408\u80fd\u529b\uff0c\u8fd1\u5e74\u6765\u6709\u5f88\u591a\u91cd\u8981\u7684\u6570\u5b66paper\u9010\u6e10\u6784\u7b51\u4e86\u4e00\u4e2a\u5177\u6709\u4e00\u5b9a\u89e3\u91ca\u6027\u7684\u5206\u6790\u6846\u67b6\uff0c\u672c\u6587\u5c1d\u8bd5\u4ee5 Feizi \u516c\u5f00\u7684 \u8bfe\u7a0b \u4e3a\u57fa\u7840\uff0c\u7ed3\u5408\u51e0\u7bc7paper\u7ed9\u51fa\u4e00\u4e2a\u5927\u81f4\u7684\u56de\u7b54\u3002 \u6574\u4f53\u601d\u8def \u5bf9\u4e8e\u201c\u795e\u7ecf\u7f51\u7edc\u4e3a\u4ec0\u4e48\u6709\u5f3a\u5927\u7684\u62df\u5408\u80fd\u529b\u201d\u8fd9\u4e2a\u95ee\u9898\uff0cFeizi\u5728\u8bfe\u7a0b\u8bbe\u7f6e\u7684\u65f6\u5019\u5c31\u5c06\u5b83\u62c6\u6210\u4e86\u4e24\u4e2a\u5b50\u95ee\u9898 \u4e3a\u4ec0\u4e48\u795e\u7ecf\u7f51\u7edc\u80fd\u8bad\u7ec3 (Optimization) \u4e3a\u4ec0\u4e48\u795e\u7ecf\u7f51\u7edc\u80fd\u6cdb\u5316 (Generalization) \u5982\u679c\u80fd\u56de\u7b54\u8fd9\u4e24\u4e2a\u95ee\u9898\uff0c\u4e5f\u5c31\u80fd\u56de\u7b54\u4e3a\u4ec0\u4e48\u795e\u7ecf\u7f51\u7edc\u80fd\u6210\u529f\u4e86\u3002 \u5728\u4e4b\u524d\uff0c\u5f88\u591a\u4eba\u6216\u8005\u8bba\u6587\u4f1a\u5c1d\u8bd5\u4ece\u53ef\u89e3\u91ca\u6027(interpretability)\u7b49\u65b9\u5411\u6765\u8003\u8651\uff0c\u6bd4\u5982\u5bf9\u7f51\u7edc\u8fdb\u884c\u53ef\u89c6\u5316\uff0c\u4e86\u89e3\u7f51\u7edc\u5b66\u4e60\u5230\u4e86\u4ec0\u4e48\u7279\u5f81,\u7136\u540e\u8f83\u4e3a\u62bd\u8c61\u5730\u7528\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u7684\u7279\u5f81\u63d0\u53d6\u52a0\u5206\u7c7b\u56de\u5f52\u8fdb\u884c\u89e3\u91ca\u3002 \u8fd9\u91ccFeizi\u5728\u8bfe\u7a0b\u4e2d\u7edf\u4e00\u5730\u4f7f\u7528\u6570\u5b66\u8303\u5f0f\uff0c\u4ece\u4ee5\u4e0a\u63d0\u51fa\u7684\u4e24\u4e2a\u5b50\u95ee\u9898\u89d2\u5ea6\u8fdb\u884c\u89e3\u7b54\u3002 Deep Learning Optimization - \u4e3a\u4ec0\u4e48\u795e\u7ecf\u7f51\u7edc\u80fd\u8bad\u7ec3 pdf_1 \u795e\u7ecf\u7f51\u7edc\u662f\u4e00\u4e2a\u9ad8\u5ea6\u975e\u7ebf\u6027\u3001\u975e\u51f8\u7684\u6620\u5c04\u51fd\u6570\uff0c\u4f46\u662f\u6211\u4eec\u5f88\u795e\u5947\u5730\u53ef\u4ee5\u4f7f\u7528\u7b80\u5355\u7684\u68af\u5ea6\u4e0b\u964d\u5f97\u5230\u597d\u7684\u89e3\u3002\u6240\u4ee5\u7b2c\u4e00\u4e2a\u95ee\u9898\u662f\u4e3a\u4ec0\u4e48\u795e\u7ecf\u7f51\u7edc\u80fd\u591f\u5728\u8bad\u7ec3\u96c6\u4e0a\u8bad\u7ec3\uff0c\u4e3a\u4ec0\u4e48\u68af\u5ea6\u4e0b\u964d\u53ef\u4ee5\u4f18\u5316\u4e00\u4e2a\u9ad8\u5ea6\u975e\u7ebf\u6027\u975e\u51f8\u7684\u51fd\u6570\u3002 Feizi\u5728\u7b2c\u4e8c\u8282\u8bfe\u4e0a\uff0c\u6839\u636e\u8fd9\u7bc7 Toward a theory of optimization for over-parameterized systems of non-linear equations: the lessons of deep learning ,\u6307\u51fa\u5e76\u8bba\u8bc1\u4e86over-parameterization\u662f\u5982\u4f55\u5e2e\u52a9\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u7684\u3002 Prior \u9884\u77e5\u8bc6 - NTK \u795e\u7ecf\u6b63\u5207\u6838 \u8bba\u6587 Neural Tangent Kernel \u5206\u6790\u4e86\u65e0\u9650\u5bbd\u5168\u8fde\u63a5\u7f51\u7edc\u7684\u7279\u6027\uff0c\u5b83\u7684\u76f8\u5173\u5b9e\u9a8c\u4ee5\u53ca\u7406\u8bba\u5206\u6790\u7ed9\u51fa\u4e86\u4ee5\u4e0b\u51e0\u4e2a\u5173\u952e\u7684\u524d\u7f6e\u77e5\u8bc6: \u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u65e0\u9650\u5bbd\u795e\u7ecf\u7f51\u7edc\u7684\u6743\u91cd\u53d8\u5316\u7387\u5e76\u4e0d\u5927\uff0c\u6743\u91cd\u6574\u4f53\u6765\u8bf4\u4f1a\u5f88\u63a5\u8fd1\u4e8e\u521d\u59cb\u5316\u7684\u60c5\u51b5\uff0c\u4e5f\u5c31\u662f\u6240\u8c13lazy-training\u7684\u73b0\u8c61\u3002 \u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u65e0\u9650\u5bbd\u7f51\u7edc\u7684\u96c5\u514b\u6bd4\u77e9\u9635\u51e0\u4e4e\u662f\u4e00\u4e2a\u5e38\u77e9\u9635\uff0c\u5176\u76f8\u5bf9\u53d8\u5316\u7387\u5f88\u4f4e\uff0c\u7f51\u7edc\u62df\u5408\u65f6\u8868\u73b0\u5f97\u5f88\u50cf\u4e00\u4e2a\u56fa\u5b9a\u7684\u6838\u51fd\u6570\u5728\u8fdb\u884c\u51fd\u6570\u62df\u5408,\u8fd9\u4e2a\u6838\u51fd\u6570\u7531\u521d\u59cb\u5316\u65f6\u7684\u6743\u91cd\u4ee5\u53ca\u5176\u96c5\u514b\u6bd4\u77e9\u9635\u76f4\u63a5\u6307\u5b9a\uff0c\u4e5f\u5c31\u662f\u6587\u7ae0\u63d0\u51fa\u7684 Neural Tangent Kernel. \u7531\u4e0a\u8ff0\u7684\u73b0\u8c61\uff0c\u5982\u679c\u6211\u4eec\u5047\u8bbe\u4e0a\u8ff0NTK\u8fd1\u4f3c\u662f\u6b63\u786e\u7684\uff0c\u5c31\u4f1a\u53d1\u73b0\u7f51\u7edc\u5728\u8bad\u7ec3\u7684\u65f6\u5019\u8868\u73b0\u5f97\u5f88\u63a5\u8fd1\u4e8e\u4e00\u4e2a\u7ebf\u6027\u6a21\u578b\uff0c\u7528\u68af\u5ea6\u4e0b\u964d\u80fd\u6307\u6570\u7ea7\u5730\u903c\u8fd1label\u3002 \u901a\u8fc7\u5bf9\u521d\u59cb\u5316\u65f6\u7684\u60c5\u51b5\u5206\u6790\uff0c\u53ef\u4ee5\u53d1\u73b0\u7f51\u7edc\u5728\u521d\u59cb\u5316\u65f6\u8fd1\u4f3c\u6709\u6709\u4e00\u4e2aL2-regularized\uff0c\u6216\u8005\u8bf4\u6838\u51fd\u6570\u6709\u4e00\u4e2a\u5fae\u5f31\u7684\u5728 (0,0) \u4f4d\u7f6e\u7684\u5148\u9a8c\u6837\u672c\u3002\u8fd9\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u89e3\u91ca\u4e86\u7f51\u7edc\u7684\u53ef\u6cdb\u5316\u6027\u3002 NTK \u672c\u8eab\u5df2\u7ecf\u8fd1\u4e4e\u5b8c\u6574\u7684\u56de\u7b54\u4e86\u7f51\u7edc\u4e3a\u4ec0\u4e48\u80fd\u8bad\u7ec3\uff0c\u4e3a\u4ec0\u4e48\u80fd\u6cdb\u5316\uff0c\u4f46\u662fNTK\u6709\u6bd4\u8f83\u5f3a\u7684\u8fd1\u4f3c\u7ea6\u675f\uff0c\u5b9e\u9645\u7f51\u7edc\u8bad\u7ec3\u80fd\u53d1\u73b0\u5f88\u591a\u7f51\u7edc\u7684\u8868\u73b0\u5e76\u4e0d\u4e0eNTK\u7684\u8bf4\u6cd5\u5b8c\u5168\u4e00\u81f4\u3002 Prior \u9884\u77e5\u8bc6 - Over-parameterization \u8fc7\u53c2\u6570\u7cfb\u7edf pdf_1 \u4ece\u8fc7\u53c2\u6570\u7cfb\u7edf\u8fdb\u884c\u5206\u6790\uff0c\u5176\u4e2d\u4e00\u4e2a\u5173\u952e\u7684\u9700\u8981\u63d0\u524d\u7406\u89e3\u7684\u70b9\u5728\u4e8e\u8fc7\u53c2\u6570\u7cfb\u7edf-\u6b20\u5b9a\u65b9\u7a0b\u7ec4\u7684\u6700\u4f18\u89e3\u7684\u6d41\u5f62(manifold). \u8fc7\u53c2\u6570\u7cfb\u7edf\u53ef\u4ee5\u5b9e\u73b0\u5b8c\u5168\u62df\u5408\uff0c\u5728\u5408\u9002\u7684\u635f\u5931\u51fd\u6570\u4e0b\u80fd\u505a\u5230\u635f\u5931\u4e3a\u96f6\uff0c \u6240\u4ee5\u53ef\u4ee5\u7406\u89e3\u4e3a\u7f51\u7edc\u5728\u8fdb\u884c\u63d2\u503c\u3002\u4f46\u662f\u63d2\u503c\u65b9\u7a0b\u6570\u91cf\u5c0f\u4e8e\u53c2\u6570\u6570\u91cf \u5173\u4e8e\u6b20\u5b9a\u65b9\u7a0b\u89e3\u7684\u6d41\u5f62\u7684\u76f4\u89c9\uff1a\u82e5\u6570\u636e\u6709\u4e00\u4e2a\u4e00\u7ef4\u70b9\uff0c\u800c\u6211\u4eec\u6709\u4e24\u4e2a\u53c2\u6570\uff0c\u6211\u4eec\u53ef\u4ee5\u5f97\u5230\u65e0\u6570\u6761\u7ebf\u7a7f\u8fc7\u8fd9\u4e2a\u70b9\uff0c\u5728\u53c2\u6570\u7a7a\u95f4( w_1, w_2 )\u4e0a\u4f1a\u662f\u4e00\u6761\u76f4\u7ebf\uff0c\u8fd9\u6761\u76f4\u7ebf\u5c31\u662f\u8fd9\u4e2a\u89e3\u7684\u6d41\u5f62\uff1b\u82e5\u6570\u636e\u6709\u4e24\u4e2a\u7a7a\u95f4\u70b9\uff0c\u800c\u6211\u4eec\u6709\u4e09\u4e2a\u53c2\u6570\uff0c\u7a7f\u8fc7\u4e00\u4e2a\u70b9\u7684\u6240\u6709\u5e73\u9762\u5728\u53c2\u6570\u7a7a\u95f4\u4e2d\u90fd\u662f\u4e00\u4e2a\u5e73\u9762\uff0c\u8fd9\u4e2a\u6b20\u5b9a\u65b9\u7a0b\u7ec4\u89e3\u7684\u6d41\u5f62\u5c31\u662f\u53c2\u6570\u7a7a\u95f4\u4e2d\u4e24\u4e2a\u6d41\u5f62\u7684\u4ea4\u7ebf\u3002 \u795e\u7ecf\u7f51\u7edc\u4e5f\u662f\u4e00\u4e2a\u5178\u578b\u7684\u8fc7\u53c2\u6570\u7cfb\u7edf \u4e3a\u4ec0\u4e48\u795e\u7ecf\u7f51\u7edc\u80fd\u8bad\u7ec3 pdf_1 \u7684\u4f5c\u8005\u5728\u8bba\u8bc1\u8fc7\u7a0b\u6307\u51fa\u4ee5\u4e0b\u51e0\u4e2a\u7ed3\u8bba: \u8fc7\u53c2\u6570\u7cfb\u7edf\uff0c\u5982\u795e\u7ecf\u7f51\u7edc\uff0c\u4e0e\u6b20\u53c2\u6570\u7cfb\u7edf\u4e0d\u540c\uff0c\u5176\u635f\u5931 \u5fc5\u5b9a \u662f\u4e00\u4e2a\u975e\u51f8\u51fd\u6570\uff0c\u4e0b\u9762\u7684\u56fe\u62bd\u8c61\u5730\u8868\u793a\u8fc7\u53c2\u6570\u7cfb\u7edf\u7684loss\u7684\u56fe\u50cf\uff0c\u548c\u524d\u6587prior\u4e2d\u7684\u76f4\u89c9\u60c5\u666f\u662f\u76f8\u4f3c\u7684\u3002 \u795e\u7ecf\u7f51\u7edc\u6ee1\u8db3 Polyak-Lojasiewicz (PL) \u6761\u4ef6\uff1b\u795e\u7ecf\u7f51\u7edc\u7cfb\u7edf\u7684\u6761\u4ef6\u6570\uff0c\u5b9a\u4e49\u4e3a \u6b63\u5207\u6838 \u7684\u6700\u5c0f\u3001\u6700\u5927\u7279\u5f81\u503c\u7684\u6bd4\u503c\uff1b\u5728\u6743\u91cd\u53d8\u5316\u5728\u4e00\u4e2a\u7403\u8303\u56f4\u65f6\uff0c\u795e\u7ecf\u7f51\u7edc\u7cfb\u7edf\u5c06\u80fd\u88ab\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u4f18\u5316\uff0c\u901f\u5ea6\u7531\u6761\u4ef6\u6570\u51b3\u5b9a\u3002 \u6761\u4ef6\u6570\u7684\u4e0a\u754c\u7531Hession\u77e9\u9635\u7684\u6a21\u51b3\u5b9a\uff1b\u800c\u5bbd\u7f51\u7edc\uff0c\u8fc7\u53c2\u6570\u53ef\u4ee5\u6539\u5584\u6761\u4ef6\u6570\u4f18\u5316\u6548\u679c\u3002 \u5177\u4f53\u7684\u63a8\u7406\u8fd9\u91cc\u7701\u7565\uff0c\u4f46\u662f\u63d0\u4f9b\u5173\u952e\u7684\u5b9a\u4e49\u4e0e\u63a8\u7406\u6b65\u9aa4\u4f5c\u4e3a\u53c2\u8003: PL \u6761\u4ef6 \u5b9a\u4e49\uff1a\u4e00\u4e2a\u635f\u5931\u51fd\u6570 \\mathcal{L}(w) \u662f \\mu -PL\u7684\uff0c\u5982\u679c: \\frac{1}{2}\\|\\nabla \\mathcal{L}(\\mathbf{w})\\|^{2} \\geq \\mu\\left(\\mathcal{L}(\\mathbf{w})-\\mathcal{L}\\left(w^{*}\\right)\\right), \\forall \\mathbf{w} \\in \\mathcal{S} Uniform Conditioning \u5b9a\u4e49: \u4e00\u4e2a\u6620\u5c04 F(w) \u662f \\mu -uniformly conditioned, \u90a3\u4e48\u5176\u6b63\u5207\u6838 K(w) \u7684\u6700\u5c0f\u7279\u5f81\u503c\u6ee1\u8db3: \\lambda_{min}(K(w)) \\ge \\mu, \\forall w \\in \\mathcal{S} Uniform Condition -> PL condition : \u53ef\u4ee5\u7b80\u5355\u8bc1\u660e\uff0c\u5982\u679c F(w) \u6ee1\u8db3uniform conditioning\uff0c\u4e14\u635f\u5931\u662f\u5e73\u65b9\u635f\u5931 \\mathcal{L}(w) = \\frac{1}{2}||F(w)- y ||^2 ,\u5219 L \u6ee1\u8db3PL\u6761\u4ef6\u3002 PL condition -> Solution Exist and Fast Convergence : \u8fd9\u4e2a\u8bc1\u660e\u96be\u5ea6\u6bd4\u8f83\u5927. \u603b\u7ed3\u8d77\u6765\uff0c\u5c31\u662f uniform conditioning \u52a0\u4e0a \u6761\u4ef6\u6570\u6709\u4e0a\u754c\uff0c\u53ef\u4ee5\u63a8\u5bfc\u5230\u5728\u4e00\u5b9a\u534a\u5f84\u7684\u7403\u5185\u89e3\u7684\u5b58\u5728\u6027\u4ee5\u53caGD\u7684\u5feb\u901f\u6536\u655b\u3002 Deep Learning Generalization - \u4e3a\u4ec0\u4e48\u795e\u7ecf\u7f51\u7edc\u80fd\u6cdb\u5316 \u8fd9\u65b9\u9762\u516c\u8ba4\u7684\u6709\u8bf4\u670d\u529b\u7684\u7406\u8bba\u7ed3\u679c\u76f8\u5bf9\u4f18\u5316\u4f1a\u5c11\u4e00\u4e9b\u3002\u6838\u5fc3\u95ee\u9898\u5728\u4e8e\u8981\u627e\u5230\u795e\u7ecf\u7f51\u7edc\u8fc7\u53c2\u6570\u3001\u68af\u5ea6\u4e0b\u964d\u7684implicit regularization. \u4e00\u4e2a\u6bd4\u8f83\u539f\u59cb\u7684\u60f3\u6cd5\u662f\u8ba4\u4e3a\u795e\u7ecf\u7f51\u7edc\u4ece\u4e00\u4e2a\u9ad8\u65af\u4e2d\u521d\u59cb\u5316\uff0c\u68af\u5ea6\u4e0b\u964d\u8fd0\u884c\uff0c\u6240\u4ee5\u6709implicit regularization, \u8fd9\u4e2a\u7406\u7531\u4e5f\u5728 NTK \u4e2d\u6709\u4f53\u73b0\uff0c\u4f46\u662f rethinking \u4e00\u6587\u4e2d\u6307\u51fa\u8fd9\u5e76\u4e0d\u5145\u5206\uff0c\u751a\u81f3\u5728\u5b9e\u9645\u8bad\u7ec3\u65f6\u5e76\u4e0d\u6210\u7acb(\u5728\u56fe\u7247\u5206\u7c7b\u4efb\u52a1\u4e2dnorm\u6700\u7ec8\u53ef\u4ee5\u53d8\u5f97\u5f88\u5927)\u3002 \u6709\u4e00\u7bc7 paper \u4ece\u5b9e\u9a8c\u4e0a\u8bf4\u660e\u4e86test error\u4e0e\u6a21\u578b\u590d\u6742\u5ea6\u4e4b\u95f4\u7684\u5173\u7cfb\u5e76\u4e0d\u662f\u4e00\u4e2a\u7b80\u5355\u7684U\u578b\u66f2\u7ebf\uff0c\u800c\u662f\u4e00\u4e2adouble descent \u66f2\u7ebf\uff1a The Implicit Bias of Gradient Descent on Separable Data pdf_2 \u8fd9\u7bc7paper\u5bf9\u5206\u7c7b\u95ee\u9898\u7ed9\u51fa\u4e86\u4e00\u4e2a\u5f88\u5f3a\u7684\u7406\u8bba\u8bc1\u660e\u4ee5\u53ca\u4f8b\u5b50. \u5bf9\u4e8e\u7ebf\u6027\u53ef\u5206\u7684\u6570\u636e\uff0c\u7528logistic\u56de\u5f52\uff0c\u635f\u5931\u9009\u62e9entropy loss\u6216\u8005\u5176\u4ed6\u6307\u6570\u8870\u51cf\u7684loss\uff0c\u5728\u8bad\u7ec3\u635f\u5931\u903c\u8fd1\u96f6\u7684\u65f6\u5019\u7ee7\u7eed\u8bad\u7ec3\uff0c\u7f51\u7edc\u6700\u7ec8\u4f1a\u6536\u655b\u4e8emax-margin SVM\u7684\u89e3\u3002 \u5728\u76f4\u89c9\u4e0a\u8fd9\u4e2a\u5176\u5b9e\u6bd4\u8f83\u597d\u7406\u89e3\uff0c\u5bf9\u4e8eentropy loss\u8fd9\u6837\u7684\u6307\u6570\u8870\u51cf\u7684loss, \u7f51\u7edc\u5176\u5b9e\u662f\u6ca1\u6709\u6700\u4f18\u70b9\u7684(\u68af\u5ea6\u4e3a\u96f6\u7684\u70b9)\uff0c\u968f\u7740\u6fc0\u6d3b\u503c\u7684\u6301\u7eed\u589e\u52a0\uff0c\u8ba9\u7f51\u7edc\u5bf9\u505a\u51fa\u7684prediction\u83b7\u5f97\u66f4\u9ad8\u7684\u7f6e\u4fe1\u5ea6\uff0c\u635f\u5931\u5c31\u4f1a\u6301\u7eed\u4e0b\u964d\u3002 \u5728\u6781\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u7531\u4e8e\u635f\u5931\u662f\u968f\u7740\u6b63\u786e\u7684\u7f6e\u4fe1\u5ea6\u63d0\u5347\u800c\u6307\u6570\u8870\u51cf\u7684\uff0c\u6240\u4ee5\u6700\u7ec8\u635f\u5931\u4f1a\u7531\u7f6e\u4fe1\u5ea6\u6700\u4f4e\u7684\u70b9dominate\u3002\u6240\u4ee5\u6700\u7ec8\u7ed3\u679c\u4f1a\u8d8b\u5411\u4e8eSVM\u7684\u7ed3\u679c\u3002","title":"Deep Learning \"Foundations\""},{"location":"The_theory/deeplearning_foundation/#deep-learning-foundations","text":"\u5173\u4e8e\u795e\u7ecf\u7f51\u7edc\u4e3a\u4ec0\u4e48\u6709\u5f3a\u5927\u7684\u62df\u5408\u80fd\u529b\uff0c\u8fd1\u5e74\u6765\u6709\u5f88\u591a\u91cd\u8981\u7684\u6570\u5b66paper\u9010\u6e10\u6784\u7b51\u4e86\u4e00\u4e2a\u5177\u6709\u4e00\u5b9a\u89e3\u91ca\u6027\u7684\u5206\u6790\u6846\u67b6\uff0c\u672c\u6587\u5c1d\u8bd5\u4ee5 Feizi \u516c\u5f00\u7684 \u8bfe\u7a0b \u4e3a\u57fa\u7840\uff0c\u7ed3\u5408\u51e0\u7bc7paper\u7ed9\u51fa\u4e00\u4e2a\u5927\u81f4\u7684\u56de\u7b54\u3002","title":"Deep Learning \"Foundations\""},{"location":"The_theory/deeplearning_foundation/#_1","text":"\u5bf9\u4e8e\u201c\u795e\u7ecf\u7f51\u7edc\u4e3a\u4ec0\u4e48\u6709\u5f3a\u5927\u7684\u62df\u5408\u80fd\u529b\u201d\u8fd9\u4e2a\u95ee\u9898\uff0cFeizi\u5728\u8bfe\u7a0b\u8bbe\u7f6e\u7684\u65f6\u5019\u5c31\u5c06\u5b83\u62c6\u6210\u4e86\u4e24\u4e2a\u5b50\u95ee\u9898 \u4e3a\u4ec0\u4e48\u795e\u7ecf\u7f51\u7edc\u80fd\u8bad\u7ec3 (Optimization) \u4e3a\u4ec0\u4e48\u795e\u7ecf\u7f51\u7edc\u80fd\u6cdb\u5316 (Generalization) \u5982\u679c\u80fd\u56de\u7b54\u8fd9\u4e24\u4e2a\u95ee\u9898\uff0c\u4e5f\u5c31\u80fd\u56de\u7b54\u4e3a\u4ec0\u4e48\u795e\u7ecf\u7f51\u7edc\u80fd\u6210\u529f\u4e86\u3002 \u5728\u4e4b\u524d\uff0c\u5f88\u591a\u4eba\u6216\u8005\u8bba\u6587\u4f1a\u5c1d\u8bd5\u4ece\u53ef\u89e3\u91ca\u6027(interpretability)\u7b49\u65b9\u5411\u6765\u8003\u8651\uff0c\u6bd4\u5982\u5bf9\u7f51\u7edc\u8fdb\u884c\u53ef\u89c6\u5316\uff0c\u4e86\u89e3\u7f51\u7edc\u5b66\u4e60\u5230\u4e86\u4ec0\u4e48\u7279\u5f81,\u7136\u540e\u8f83\u4e3a\u62bd\u8c61\u5730\u7528\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u7684\u7279\u5f81\u63d0\u53d6\u52a0\u5206\u7c7b\u56de\u5f52\u8fdb\u884c\u89e3\u91ca\u3002 \u8fd9\u91ccFeizi\u5728\u8bfe\u7a0b\u4e2d\u7edf\u4e00\u5730\u4f7f\u7528\u6570\u5b66\u8303\u5f0f\uff0c\u4ece\u4ee5\u4e0a\u63d0\u51fa\u7684\u4e24\u4e2a\u5b50\u95ee\u9898\u89d2\u5ea6\u8fdb\u884c\u89e3\u7b54\u3002","title":"\u6574\u4f53\u601d\u8def"},{"location":"The_theory/deeplearning_foundation/#deep-learning-optimization-","text":"pdf_1 \u795e\u7ecf\u7f51\u7edc\u662f\u4e00\u4e2a\u9ad8\u5ea6\u975e\u7ebf\u6027\u3001\u975e\u51f8\u7684\u6620\u5c04\u51fd\u6570\uff0c\u4f46\u662f\u6211\u4eec\u5f88\u795e\u5947\u5730\u53ef\u4ee5\u4f7f\u7528\u7b80\u5355\u7684\u68af\u5ea6\u4e0b\u964d\u5f97\u5230\u597d\u7684\u89e3\u3002\u6240\u4ee5\u7b2c\u4e00\u4e2a\u95ee\u9898\u662f\u4e3a\u4ec0\u4e48\u795e\u7ecf\u7f51\u7edc\u80fd\u591f\u5728\u8bad\u7ec3\u96c6\u4e0a\u8bad\u7ec3\uff0c\u4e3a\u4ec0\u4e48\u68af\u5ea6\u4e0b\u964d\u53ef\u4ee5\u4f18\u5316\u4e00\u4e2a\u9ad8\u5ea6\u975e\u7ebf\u6027\u975e\u51f8\u7684\u51fd\u6570\u3002 Feizi\u5728\u7b2c\u4e8c\u8282\u8bfe\u4e0a\uff0c\u6839\u636e\u8fd9\u7bc7 Toward a theory of optimization for over-parameterized systems of non-linear equations: the lessons of deep learning ,\u6307\u51fa\u5e76\u8bba\u8bc1\u4e86over-parameterization\u662f\u5982\u4f55\u5e2e\u52a9\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u7684\u3002","title":"Deep Learning Optimization - \u4e3a\u4ec0\u4e48\u795e\u7ecf\u7f51\u7edc\u80fd\u8bad\u7ec3"},{"location":"The_theory/deeplearning_foundation/#prior-ntk","text":"\u8bba\u6587 Neural Tangent Kernel \u5206\u6790\u4e86\u65e0\u9650\u5bbd\u5168\u8fde\u63a5\u7f51\u7edc\u7684\u7279\u6027\uff0c\u5b83\u7684\u76f8\u5173\u5b9e\u9a8c\u4ee5\u53ca\u7406\u8bba\u5206\u6790\u7ed9\u51fa\u4e86\u4ee5\u4e0b\u51e0\u4e2a\u5173\u952e\u7684\u524d\u7f6e\u77e5\u8bc6: \u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u65e0\u9650\u5bbd\u795e\u7ecf\u7f51\u7edc\u7684\u6743\u91cd\u53d8\u5316\u7387\u5e76\u4e0d\u5927\uff0c\u6743\u91cd\u6574\u4f53\u6765\u8bf4\u4f1a\u5f88\u63a5\u8fd1\u4e8e\u521d\u59cb\u5316\u7684\u60c5\u51b5\uff0c\u4e5f\u5c31\u662f\u6240\u8c13lazy-training\u7684\u73b0\u8c61\u3002 \u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u65e0\u9650\u5bbd\u7f51\u7edc\u7684\u96c5\u514b\u6bd4\u77e9\u9635\u51e0\u4e4e\u662f\u4e00\u4e2a\u5e38\u77e9\u9635\uff0c\u5176\u76f8\u5bf9\u53d8\u5316\u7387\u5f88\u4f4e\uff0c\u7f51\u7edc\u62df\u5408\u65f6\u8868\u73b0\u5f97\u5f88\u50cf\u4e00\u4e2a\u56fa\u5b9a\u7684\u6838\u51fd\u6570\u5728\u8fdb\u884c\u51fd\u6570\u62df\u5408,\u8fd9\u4e2a\u6838\u51fd\u6570\u7531\u521d\u59cb\u5316\u65f6\u7684\u6743\u91cd\u4ee5\u53ca\u5176\u96c5\u514b\u6bd4\u77e9\u9635\u76f4\u63a5\u6307\u5b9a\uff0c\u4e5f\u5c31\u662f\u6587\u7ae0\u63d0\u51fa\u7684 Neural Tangent Kernel. \u7531\u4e0a\u8ff0\u7684\u73b0\u8c61\uff0c\u5982\u679c\u6211\u4eec\u5047\u8bbe\u4e0a\u8ff0NTK\u8fd1\u4f3c\u662f\u6b63\u786e\u7684\uff0c\u5c31\u4f1a\u53d1\u73b0\u7f51\u7edc\u5728\u8bad\u7ec3\u7684\u65f6\u5019\u8868\u73b0\u5f97\u5f88\u63a5\u8fd1\u4e8e\u4e00\u4e2a\u7ebf\u6027\u6a21\u578b\uff0c\u7528\u68af\u5ea6\u4e0b\u964d\u80fd\u6307\u6570\u7ea7\u5730\u903c\u8fd1label\u3002 \u901a\u8fc7\u5bf9\u521d\u59cb\u5316\u65f6\u7684\u60c5\u51b5\u5206\u6790\uff0c\u53ef\u4ee5\u53d1\u73b0\u7f51\u7edc\u5728\u521d\u59cb\u5316\u65f6\u8fd1\u4f3c\u6709\u6709\u4e00\u4e2aL2-regularized\uff0c\u6216\u8005\u8bf4\u6838\u51fd\u6570\u6709\u4e00\u4e2a\u5fae\u5f31\u7684\u5728 (0,0) \u4f4d\u7f6e\u7684\u5148\u9a8c\u6837\u672c\u3002\u8fd9\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u89e3\u91ca\u4e86\u7f51\u7edc\u7684\u53ef\u6cdb\u5316\u6027\u3002 NTK \u672c\u8eab\u5df2\u7ecf\u8fd1\u4e4e\u5b8c\u6574\u7684\u56de\u7b54\u4e86\u7f51\u7edc\u4e3a\u4ec0\u4e48\u80fd\u8bad\u7ec3\uff0c\u4e3a\u4ec0\u4e48\u80fd\u6cdb\u5316\uff0c\u4f46\u662fNTK\u6709\u6bd4\u8f83\u5f3a\u7684\u8fd1\u4f3c\u7ea6\u675f\uff0c\u5b9e\u9645\u7f51\u7edc\u8bad\u7ec3\u80fd\u53d1\u73b0\u5f88\u591a\u7f51\u7edc\u7684\u8868\u73b0\u5e76\u4e0d\u4e0eNTK\u7684\u8bf4\u6cd5\u5b8c\u5168\u4e00\u81f4\u3002","title":"Prior \u9884\u77e5\u8bc6 - NTK \u795e\u7ecf\u6b63\u5207\u6838"},{"location":"The_theory/deeplearning_foundation/#prior-over-parameterization","text":"pdf_1 \u4ece\u8fc7\u53c2\u6570\u7cfb\u7edf\u8fdb\u884c\u5206\u6790\uff0c\u5176\u4e2d\u4e00\u4e2a\u5173\u952e\u7684\u9700\u8981\u63d0\u524d\u7406\u89e3\u7684\u70b9\u5728\u4e8e\u8fc7\u53c2\u6570\u7cfb\u7edf-\u6b20\u5b9a\u65b9\u7a0b\u7ec4\u7684\u6700\u4f18\u89e3\u7684\u6d41\u5f62(manifold). \u8fc7\u53c2\u6570\u7cfb\u7edf\u53ef\u4ee5\u5b9e\u73b0\u5b8c\u5168\u62df\u5408\uff0c\u5728\u5408\u9002\u7684\u635f\u5931\u51fd\u6570\u4e0b\u80fd\u505a\u5230\u635f\u5931\u4e3a\u96f6\uff0c \u6240\u4ee5\u53ef\u4ee5\u7406\u89e3\u4e3a\u7f51\u7edc\u5728\u8fdb\u884c\u63d2\u503c\u3002\u4f46\u662f\u63d2\u503c\u65b9\u7a0b\u6570\u91cf\u5c0f\u4e8e\u53c2\u6570\u6570\u91cf \u5173\u4e8e\u6b20\u5b9a\u65b9\u7a0b\u89e3\u7684\u6d41\u5f62\u7684\u76f4\u89c9\uff1a\u82e5\u6570\u636e\u6709\u4e00\u4e2a\u4e00\u7ef4\u70b9\uff0c\u800c\u6211\u4eec\u6709\u4e24\u4e2a\u53c2\u6570\uff0c\u6211\u4eec\u53ef\u4ee5\u5f97\u5230\u65e0\u6570\u6761\u7ebf\u7a7f\u8fc7\u8fd9\u4e2a\u70b9\uff0c\u5728\u53c2\u6570\u7a7a\u95f4( w_1, w_2 )\u4e0a\u4f1a\u662f\u4e00\u6761\u76f4\u7ebf\uff0c\u8fd9\u6761\u76f4\u7ebf\u5c31\u662f\u8fd9\u4e2a\u89e3\u7684\u6d41\u5f62\uff1b\u82e5\u6570\u636e\u6709\u4e24\u4e2a\u7a7a\u95f4\u70b9\uff0c\u800c\u6211\u4eec\u6709\u4e09\u4e2a\u53c2\u6570\uff0c\u7a7f\u8fc7\u4e00\u4e2a\u70b9\u7684\u6240\u6709\u5e73\u9762\u5728\u53c2\u6570\u7a7a\u95f4\u4e2d\u90fd\u662f\u4e00\u4e2a\u5e73\u9762\uff0c\u8fd9\u4e2a\u6b20\u5b9a\u65b9\u7a0b\u7ec4\u89e3\u7684\u6d41\u5f62\u5c31\u662f\u53c2\u6570\u7a7a\u95f4\u4e2d\u4e24\u4e2a\u6d41\u5f62\u7684\u4ea4\u7ebf\u3002 \u795e\u7ecf\u7f51\u7edc\u4e5f\u662f\u4e00\u4e2a\u5178\u578b\u7684\u8fc7\u53c2\u6570\u7cfb\u7edf","title":"Prior \u9884\u77e5\u8bc6 - Over-parameterization \u8fc7\u53c2\u6570\u7cfb\u7edf"},{"location":"The_theory/deeplearning_foundation/#_2","text":"pdf_1 \u7684\u4f5c\u8005\u5728\u8bba\u8bc1\u8fc7\u7a0b\u6307\u51fa\u4ee5\u4e0b\u51e0\u4e2a\u7ed3\u8bba: \u8fc7\u53c2\u6570\u7cfb\u7edf\uff0c\u5982\u795e\u7ecf\u7f51\u7edc\uff0c\u4e0e\u6b20\u53c2\u6570\u7cfb\u7edf\u4e0d\u540c\uff0c\u5176\u635f\u5931 \u5fc5\u5b9a \u662f\u4e00\u4e2a\u975e\u51f8\u51fd\u6570\uff0c\u4e0b\u9762\u7684\u56fe\u62bd\u8c61\u5730\u8868\u793a\u8fc7\u53c2\u6570\u7cfb\u7edf\u7684loss\u7684\u56fe\u50cf\uff0c\u548c\u524d\u6587prior\u4e2d\u7684\u76f4\u89c9\u60c5\u666f\u662f\u76f8\u4f3c\u7684\u3002 \u795e\u7ecf\u7f51\u7edc\u6ee1\u8db3 Polyak-Lojasiewicz (PL) \u6761\u4ef6\uff1b\u795e\u7ecf\u7f51\u7edc\u7cfb\u7edf\u7684\u6761\u4ef6\u6570\uff0c\u5b9a\u4e49\u4e3a \u6b63\u5207\u6838 \u7684\u6700\u5c0f\u3001\u6700\u5927\u7279\u5f81\u503c\u7684\u6bd4\u503c\uff1b\u5728\u6743\u91cd\u53d8\u5316\u5728\u4e00\u4e2a\u7403\u8303\u56f4\u65f6\uff0c\u795e\u7ecf\u7f51\u7edc\u7cfb\u7edf\u5c06\u80fd\u88ab\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u4f18\u5316\uff0c\u901f\u5ea6\u7531\u6761\u4ef6\u6570\u51b3\u5b9a\u3002 \u6761\u4ef6\u6570\u7684\u4e0a\u754c\u7531Hession\u77e9\u9635\u7684\u6a21\u51b3\u5b9a\uff1b\u800c\u5bbd\u7f51\u7edc\uff0c\u8fc7\u53c2\u6570\u53ef\u4ee5\u6539\u5584\u6761\u4ef6\u6570\u4f18\u5316\u6548\u679c\u3002 \u5177\u4f53\u7684\u63a8\u7406\u8fd9\u91cc\u7701\u7565\uff0c\u4f46\u662f\u63d0\u4f9b\u5173\u952e\u7684\u5b9a\u4e49\u4e0e\u63a8\u7406\u6b65\u9aa4\u4f5c\u4e3a\u53c2\u8003: PL \u6761\u4ef6 \u5b9a\u4e49\uff1a\u4e00\u4e2a\u635f\u5931\u51fd\u6570 \\mathcal{L}(w) \u662f \\mu -PL\u7684\uff0c\u5982\u679c: \\frac{1}{2}\\|\\nabla \\mathcal{L}(\\mathbf{w})\\|^{2} \\geq \\mu\\left(\\mathcal{L}(\\mathbf{w})-\\mathcal{L}\\left(w^{*}\\right)\\right), \\forall \\mathbf{w} \\in \\mathcal{S} Uniform Conditioning \u5b9a\u4e49: \u4e00\u4e2a\u6620\u5c04 F(w) \u662f \\mu -uniformly conditioned, \u90a3\u4e48\u5176\u6b63\u5207\u6838 K(w) \u7684\u6700\u5c0f\u7279\u5f81\u503c\u6ee1\u8db3: \\lambda_{min}(K(w)) \\ge \\mu, \\forall w \\in \\mathcal{S} Uniform Condition -> PL condition : \u53ef\u4ee5\u7b80\u5355\u8bc1\u660e\uff0c\u5982\u679c F(w) \u6ee1\u8db3uniform conditioning\uff0c\u4e14\u635f\u5931\u662f\u5e73\u65b9\u635f\u5931 \\mathcal{L}(w) = \\frac{1}{2}||F(w)- y ||^2 ,\u5219 L \u6ee1\u8db3PL\u6761\u4ef6\u3002 PL condition -> Solution Exist and Fast Convergence : \u8fd9\u4e2a\u8bc1\u660e\u96be\u5ea6\u6bd4\u8f83\u5927. \u603b\u7ed3\u8d77\u6765\uff0c\u5c31\u662f uniform conditioning \u52a0\u4e0a \u6761\u4ef6\u6570\u6709\u4e0a\u754c\uff0c\u53ef\u4ee5\u63a8\u5bfc\u5230\u5728\u4e00\u5b9a\u534a\u5f84\u7684\u7403\u5185\u89e3\u7684\u5b58\u5728\u6027\u4ee5\u53caGD\u7684\u5feb\u901f\u6536\u655b\u3002","title":"\u4e3a\u4ec0\u4e48\u795e\u7ecf\u7f51\u7edc\u80fd\u8bad\u7ec3"},{"location":"The_theory/deeplearning_foundation/#deep-learning-generalization-","text":"\u8fd9\u65b9\u9762\u516c\u8ba4\u7684\u6709\u8bf4\u670d\u529b\u7684\u7406\u8bba\u7ed3\u679c\u76f8\u5bf9\u4f18\u5316\u4f1a\u5c11\u4e00\u4e9b\u3002\u6838\u5fc3\u95ee\u9898\u5728\u4e8e\u8981\u627e\u5230\u795e\u7ecf\u7f51\u7edc\u8fc7\u53c2\u6570\u3001\u68af\u5ea6\u4e0b\u964d\u7684implicit regularization. \u4e00\u4e2a\u6bd4\u8f83\u539f\u59cb\u7684\u60f3\u6cd5\u662f\u8ba4\u4e3a\u795e\u7ecf\u7f51\u7edc\u4ece\u4e00\u4e2a\u9ad8\u65af\u4e2d\u521d\u59cb\u5316\uff0c\u68af\u5ea6\u4e0b\u964d\u8fd0\u884c\uff0c\u6240\u4ee5\u6709implicit regularization, \u8fd9\u4e2a\u7406\u7531\u4e5f\u5728 NTK \u4e2d\u6709\u4f53\u73b0\uff0c\u4f46\u662f rethinking \u4e00\u6587\u4e2d\u6307\u51fa\u8fd9\u5e76\u4e0d\u5145\u5206\uff0c\u751a\u81f3\u5728\u5b9e\u9645\u8bad\u7ec3\u65f6\u5e76\u4e0d\u6210\u7acb(\u5728\u56fe\u7247\u5206\u7c7b\u4efb\u52a1\u4e2dnorm\u6700\u7ec8\u53ef\u4ee5\u53d8\u5f97\u5f88\u5927)\u3002 \u6709\u4e00\u7bc7 paper \u4ece\u5b9e\u9a8c\u4e0a\u8bf4\u660e\u4e86test error\u4e0e\u6a21\u578b\u590d\u6742\u5ea6\u4e4b\u95f4\u7684\u5173\u7cfb\u5e76\u4e0d\u662f\u4e00\u4e2a\u7b80\u5355\u7684U\u578b\u66f2\u7ebf\uff0c\u800c\u662f\u4e00\u4e2adouble descent \u66f2\u7ebf\uff1a","title":"Deep Learning Generalization - \u4e3a\u4ec0\u4e48\u795e\u7ecf\u7f51\u7edc\u80fd\u6cdb\u5316"},{"location":"The_theory/deeplearning_foundation/#the-implicit-bias-of-gradient-descent-on-separable-data","text":"pdf_2 \u8fd9\u7bc7paper\u5bf9\u5206\u7c7b\u95ee\u9898\u7ed9\u51fa\u4e86\u4e00\u4e2a\u5f88\u5f3a\u7684\u7406\u8bba\u8bc1\u660e\u4ee5\u53ca\u4f8b\u5b50. \u5bf9\u4e8e\u7ebf\u6027\u53ef\u5206\u7684\u6570\u636e\uff0c\u7528logistic\u56de\u5f52\uff0c\u635f\u5931\u9009\u62e9entropy loss\u6216\u8005\u5176\u4ed6\u6307\u6570\u8870\u51cf\u7684loss\uff0c\u5728\u8bad\u7ec3\u635f\u5931\u903c\u8fd1\u96f6\u7684\u65f6\u5019\u7ee7\u7eed\u8bad\u7ec3\uff0c\u7f51\u7edc\u6700\u7ec8\u4f1a\u6536\u655b\u4e8emax-margin SVM\u7684\u89e3\u3002 \u5728\u76f4\u89c9\u4e0a\u8fd9\u4e2a\u5176\u5b9e\u6bd4\u8f83\u597d\u7406\u89e3\uff0c\u5bf9\u4e8eentropy loss\u8fd9\u6837\u7684\u6307\u6570\u8870\u51cf\u7684loss, \u7f51\u7edc\u5176\u5b9e\u662f\u6ca1\u6709\u6700\u4f18\u70b9\u7684(\u68af\u5ea6\u4e3a\u96f6\u7684\u70b9)\uff0c\u968f\u7740\u6fc0\u6d3b\u503c\u7684\u6301\u7eed\u589e\u52a0\uff0c\u8ba9\u7f51\u7edc\u5bf9\u505a\u51fa\u7684prediction\u83b7\u5f97\u66f4\u9ad8\u7684\u7f6e\u4fe1\u5ea6\uff0c\u635f\u5931\u5c31\u4f1a\u6301\u7eed\u4e0b\u964d\u3002 \u5728\u6781\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u7531\u4e8e\u635f\u5931\u662f\u968f\u7740\u6b63\u786e\u7684\u7f6e\u4fe1\u5ea6\u63d0\u5347\u800c\u6307\u6570\u8870\u51cf\u7684\uff0c\u6240\u4ee5\u6700\u7ec8\u635f\u5931\u4f1a\u7531\u7f6e\u4fe1\u5ea6\u6700\u4f4e\u7684\u70b9dominate\u3002\u6240\u4ee5\u6700\u7ec8\u7ed3\u679c\u4f1a\u8d8b\u5411\u4e8eSVM\u7684\u7ed3\u679c\u3002","title":"The Implicit Bias of Gradient Descent on Separable Data"},{"location":"The_theory/diffussion_model/","text":"Denoising Diffusion Probabilistic Models \u8fd9\u7bc7paper\u662fdiffusion model\u5728\u751f\u6210\u6a21\u578b\u4e2d\u7684\u5e94\u7528\u3002\u8fd9\u7bc7\u6587\u7ae0\u6709\u4e00\u4e2a\u63a8\u5bfc\u516c\u5f0f\u6bd4\u8f83\u7ec6\u81f4\uff0c\u8bb2\u89e3\u6bd4\u8f83\u6e05\u6670\u7684 \u535a\u5ba2 . Diffusion Model \u5728\u751f\u6210\u6a21\u578b\u4e2d\u7684\u7279\u70b9 \u8be5\u535a\u5ba2\u4e2d\u6709\u5982\u6b64\u4e00\u56fe\uff0c\u5927\u81f4\u603b\u7ed3\u4e86\u5f53\u524d\u751f\u6210\u6a21\u578b\u7684\u56db\u5927\u7c7b\u522b\u3002\u5bf9\u6297\u7f51\u7edc, VAE , flow-based model, \u4ee5\u53ca\u672c\u6587\u7684\u6269\u6563\u6a21\u578b\u3002 \u8fd9\u51e0\u79cd\u65b9\u5f0f\u7684\u5bf9\u6bd4\u6765\u770b\uff0cVAE\u548cFlow Model\u53ef\u4ee5\u5feb\u901f\u5730\u91c7\u6837\u51fa\u79cd\u7c7b\u8303\u56f4\u6bd4\u8f83\u5e7f\u7684\u6837\u672c\u3002GAN\u53ef\u4ee5\u5feb\u901f\u91c7\u6837\u51fa\u8d28\u91cf\u9ad8\u7684\u6837\u672c\uff0c\u800cDiffsion\u5219\u91c7\u6837\u901f\u5ea6\u6162\uff0c\u4f46\u662f\u91c7\u6837\u8303\u56f4\u79cd\u7c7b\u5e7f\u4e14\u8d28\u91cf\u5f88\u9ad8\u3002 Denoising Diffusion Model \u4e3b\u8981\u5de5\u4f5c\u6d41\u4e0e\u7ec4\u4ef6\u5206\u7c7b Diffusion \u64cd\u4f5c\u4e0a\u8868\u8fbe\u7684\u662f\u5bf9\u56fe\u7247\u9010\u6b65\u589e\u52a0\u9ad8\u65af\u566a\u58f0\uff0c\u6700\u7ec8\u628a\u56fe\u7247\u5b8c\u5168corrupt \u6210\u566a\u97f3\u7684\u8fc7\u7a0b\uff0c\u800c\u53cd\u5411\u5730\uff0c\u4ece\u566a\u97f3\u4e2d\u8fd8\u539f\u51fa\u56fe\u7247\u5730pattern\u5219\u662f\u751f\u6210\u7684\u8fc7\u7a0b\u3002 \u6570\u5b66\u4e0a\u6765\u8bf4\u5bf9\u56fe\u7247/\u6570\u636e\u9010\u6b65\u589e\u52a0\u9ad8\u65af\u566a\u58f0\u7684\u8fc7\u7a0b\u88ab\u8bbe\u8ba1\u6210\u4e00\u4e2a\u9a6c\u5c14\u53ef\u592b\u94fe\uff0c\u6bcf\u4e00\u4e2a\u65f6\u523b\u7684\u72b6\u6001\u53ea\u7531\u4e0a\u4e00\u4e2a\u65f6\u523b\u7684\u5747\u503c\u4ee5\u53ca\u989d\u5916\u7684\u566a\u58f0\u5f71\u54cd\u3002\u6982\u7387\u5730\u5199\u51fa\u516c\u5f0f q(x_t | x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1 - \\beta_t} x_{t-1}, \\beta_t \\mathbf{I}) , \u767d\u8bdd\u800c\u8a00\u5c31\u662f\u628a\u524d\u4e00\u4e2a\u65f6\u523b\u7684\u56fe\u50cf/\u6570\u636e/\u5206\u5e03scale down, \u518d\u4e0e\u4e00\u4e2a\u989d\u5916\u7684\u9ad8\u65af\u5206\u5e03\u52a0\u548c\u3002 \u53cd\u5411\u7684\u65f6\u5019\u6211\u4eec\u5df2\u77e5 x_T ,\u9700\u8981\u9010\u6b65\u4f30\u8ba1\u7b2c T \u6b65\u7684\u566a\u97f3 z_t \uff0c\u7136\u540e\u8ba9\u6570\u636e\"\u51cf\u53bb\"\u8fd9\u4e2a\u566a\u97f3\u3002\u672c\u6587\u63d0\u51fa\u7684\u5c31\u662f\u7528\u795e\u7ecf\u7f51\u7edc\uff0c\u8f93\u5165 x_T \u548c T \u65f6\u523b\u7684time_encoding, \u4f30\u8ba1\u8fd9\u4e2a\u5206\u8fa8\u7387\u4e0e\u539f\u56fe\u4e00\u6837\u7684\u566a\u97f3,\u5e76\u9010\u6b65\u53bb\u9664\u566a\u97f3\u5f97\u5230\u539f\u56fe\u3002\u56e0\u800c\u79f0\u4e3a\"Denoising Diffusion Model\". \u56e0\u800c\u5728\u529f\u80fd\u6a21\u7ec4\u4e0a\uff0c\u8fd9\u7c7b\u6a21\u578b\u5206\u4e3a\u4e00\u4e0b\u51e0\u4e2a\u90e8\u5206\uff0c\u4e0d\u540c\u7684\u6587\u7ae0\u4f1a\u6709\u4e0d\u540c\u7684\u5047\u8bbe\u4ee5\u53ca\u9009\u62e9\uff0c\u540e\u9762\u4f1a\u518d\u6570\u5b66\u4e0e\u4ee3\u7801\u4e0a\u7ed9\u51fa\u5b9e\u4f8b. \u524d\u5411\u91c7\u6837,\u8ba1\u7b97\u4ece\u56fe\u7247\u5230\u7b2c T \u65f6\u523b\u88abcorrupt\u7684\u72b6\u6001. \u8fd9\u91cc\u7684\u4e3b\u8981\u53d8\u91cf\u5728\u4e8e\u91c7\u6837\u65f6\u95f4\u53ef\u53d8\uff0c\u4e14 \\beta(t) \u5173\u4e8e\u65f6\u95f4\u7684\u51fd\u6570\uff0c\u79f0\u4e3anoise scheduler\u662f\u53ef\u63a7\u5236\u7684\uff0c\u5982\u7ebf\u6027\u589e\u52a0\u566a\u58f0\u7b49\u65b9\u6848\u3002 \u566a\u58f0\u4f30\u8ba1\u7f51\u7edc \\Theta , \u8f93\u5165 x_t \u65f6\u523b\u56fe\u7247\u4ee5\u53catime encoding\u8f93\u51fa\u540c\u5206\u8fa8\u7387\u566a\u58f0 z_t , \u4ee5UNet\u4e3a\u4e3b\u67b6\u6784\uff0c\u914d\u5408Attention\u7b49\u6a21\u5757\u9009\u62e9 (\u6bd4\u5982\u540e\u6765\u4e00\u4e9b\u52a0\u5165\u8bed\u8a00\u6216\u5916\u90e8\u4fe1\u606f\u7684\u751f\u6210\u6a21\u578b\u591a\u4f7f\u7528attention). \u635f\u5931\u51fd\u6570\uff0c\u5982\u4f55\u8bad\u7ec3\u566a\u58f0\u4f30\u8ba1(\u56fe\u7247\u4f30\u8ba1)\u7f51\u7edc\uff0c\u53ef\u4ee5\u6709\u5f88\u5f3a\u7684\u6982\u7387\u5b66\u652f\u6491\uff0c\u4e5f\u53ef\u4ee5\u50cf\u672c\u6587\u6700\u540e\u7684baseline implementation\u4e00\u6837\u7b80\u5355\u66b4\u529b\u3002 \u91c7\u6837\u63a8\u7406\uff0c\u6240\u8c13\u7684\"\u51cf\u53bb\u566a\u97f3\"\u6b65\u9aa4\uff0c\u8fd9\u91cc\u53ef\u4ee5\u6709\u66f4\u4e25\u8c28\u7684\u63a8\u5bfc\u8ba1\u7b97\u5f97\u5230\u66f4\u51c6\u786e\u7684\u6570\u503c\u3002 \u524d\u5411\u91c7\u6837 \u5982\u679c\u8f93\u5165\u56fe\u7247\u4e5f\u662f\u9ad8\u65af\u5206\u5e03\uff0c\u9ad8\u65af\u5206\u5e03\u4e0e\u9ad8\u65af\u5206\u5e03\u7684\u53e0\u52a0\u662f\u95ed\u5f0f\u7684\u9ad8\u65af\uff0c\u7528\u9012\u63a8\u7684\u516c\u5f0f\u8ba1\u7b97\u91c7\u6837\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u53ef\u4ee5\u5f88\u5feb\u53d1\u73b0\u6211\u4eec\u4e0d\u9700\u8981\u591a\u6b65\u8fed\u4ee3\u6765\u5b9e\u73b0\u52a0\u566a\u97f3\u91c7\u6837\uff0c\u800c\u662f\u53ef\u4ee5\u6839\u636enoise scheduler\u4ee5\u53ca\u968f\u673a\u6570\u751f\u6210\u5668\u76f4\u63a5\u5f97\u5230\u4efb\u610f\u65f6\u95f4\u70b9\u7684\u566a\u97f3\u548c\u56fe\u50cf\u6570\u636e\u3002 \u4ee4 \\alpha_t = 1 - \\beta_t , \\bar{\\alpha}_{t}=\\prod_{i=1}^{T} \\alpha_{i} , z_t \u662f\u4ee3\u7801\u4ece\u9ad8\u65af\u5206\u5e03\u4e2d\u91c7\u6837\u7684\u566a\u58f0\u3002 \\begin{aligned} x_t &= \\sqrt{\\alpha_t} x_{t-1} + \\sqrt{1 - \\alpha_t} z_{t-1} \\\\ &= \\sqrt{\\alpha_t \\alpha_{t-1}} x_{t-1} + \\sqrt{1 - \\alpha_t \\alpha_{t-1}} \\bar{z}_{t-2} \\\\ &= \\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\bar{z_0} \\end{aligned} \u6ce8\u610f\u4e24\u4e2a\u9ad8\u65af\u566a\u58f0\u65b9\u5dee\u4e3a \\sigma^2_1, \\sigma^2_2 \u7684 z \u7684\u878d\u5408\uff0c\u5f97\u5230\u7684\u662f\u4e00\u4e2a\u65b9\u5dee\u4e3a \\sigma_1^2 + \\sigma^2_2 \u7684\u9ad8\u65af\u5206\u5e03\u3002\u6240\u4ee5\u4e0a\u6587\u7684\u8ba1\u7b97\u5b9e\u9645\u4e0a\u662f \u65b9\u5dee\u4e3a 1-\\alpha_t \u7684\u9ad8\u65af\u5206\u5e03\u548c \\alpha_t * (1-\\alpha_{t-1}) \u7684\u9ad8\u65af\u5206\u5e03\u7684\u878d\u5408\uff0c\u5f97\u5230\u7684\u5c31\u662f\u65b9\u5dee\u4e3a 1 - \\alpha_t\\alpha_{t-1} \u7684\u9ad8\u65af\u5206\u5e03\uff0c\u7d2f\u79ef\u7ed3\u679c\u540c\u6837\u3002\u81f3\u6b64\uff0c\u6211\u4eec\u53ef\u4ee5\u6839\u636e\u539f\u6570\u636e x_0 , \u566a\u58f0\u89c4\u5212\u5668\u5f97\u5230\u7684 \\bar\\alpha_t , \u4ee5\u53ca\u91c7\u6837\u7684\u9ad8\u65af\u566a\u58f0\u5f97\u5230\u4efb\u610f\u65f6\u523b\u7684\u56fe\u7247 x_t \u91c7\u6837\u63a8\u7406 \u4f5c\u8005\u6307\u51fa\uff0c\u5728\u5df2\u77e5(conditioned on) x_t, x_0 \u65f6\uff0c\u53cd\u5411\u6982\u7387\u4e5f\u7b26\u5408\u4e00\u4e2a\u9ad8\u65af\u5206\u5e03 q(x_{t-1} | x_{t}, x_0) = \\mathcal{N}(x_{t-1}; \\tilde\\mu(x_t, x_0), \\tilde\\beta_t \\mathbf{I}) \u7531\u4e8e\u524d\u5411\u7684\u6982\u7387\u5206\u5e03\u662f\u5df2\u77e5\u7684\uff0c\u8fd9\u91cc\u7684\u540e\u9a8c\u5206\u5e03\u5c31\u7528\u8d1d\u53f6\u65af\u516c\u5f0f\u8f6c\u6362\u4e3a\u524d\u5411 (\u5206\u5b50\u662f x_t, x_{t-1} \u7684\u4e58\u79ef\u6982\u7387\uff0c\u66f4\u6539\u6761\u4ef6\u6982\u7387\u7684\u6761\u4ef6 conditioned on x_{t-1} , \u53ef\u5c06\u5168\u90e8\u53d8\u4e3a\u524d\u5411)\uff1b\u5e76\u91cd\u70b9\u5173\u6ce8\u6982\u7387\u5206\u5e03\u51fd\u6570\u7684\u6307\u6570\u90e8\u5206, \u7136\u540e\u628a\u5176\u4e2d\u7684 x_{t-1} \u63d0\u51fa\u6765\u3002 \\begin{aligned} q(x_{t-1} | x_t, x_0) &= q(x_t | x_{t-1}, x_0) \\frac{q(x_{t-1} | x_0)}{q(x_t | x_0)} = q(x_t | x_{t-1}) \\frac{q(x_{t-1} | x_0)}{q(x_t | x_0)} \\\\ & \\propto \\text{exp} (-\\frac{1}{2} (\\frac{(x_t - \\sqrt{\\alpha_t} x_{t-1})^2}{1-\\alpha_t} + \\frac{(x_{t-1} - \\sqrt{\\bar\\alpha_{t-1}}x_0)^2}{1 - \\bar\\alpha_{t-1}} - \\frac{(x_t - \\sqrt{\\bar\\alpha_t}x_0)^2}{1 - \\bar\\alpha_t})) \\\\ &=\\text{exp}(-\\frac{1}{2}[ (\\frac{\\alpha_t}{1 - \\alpha_t} + \\frac{1}{1-\\bar\\alpha_{t-1}}) x_{t-1}^2 - \\\\ &(\\frac{2\\sqrt{\\alpha_t} x_t}{1 - \\alpha_t}+ \\frac{2\\sqrt{\\bar\\alpha_{t-1}}x_0}{1 - \\bar\\alpha_{t-1}} )x_{t-1} + C(x_t, x_0) ] ) \\end{aligned} \u901a\u5206\u5e76\u51d1\u5e73\u65b9\uff0c\u627e\u51fa x_{t-1} \u7684\u65b9\u5dee\u548c\u5747\u503c. \u7531 Ax^2 - 2Bx = \\frac{(x - B/A)^2}{1/A} , \u5176\u4e2d A = \\frac{\\alpha_t}{1 - \\alpha_t} + \\frac{1}{1-\\bar\\alpha_{t-1}} = \\frac{1 - \\bar\\alpha_t}{\\beta_t (1 - \\bar\\alpha_{t-1})} \u65b9\u5dee \\sigma^2 = 1 / A = \\frac{\\beta_t (1 - \\bar\\alpha_{t-1})}{1 - \\bar\\alpha_t} . \u628a x_0 = \\frac{x_t - \\sqrt{1 - \\bar\\alpha_t} \\bar z_0}{\\sqrt{\\bar\\alpha_t}} \u4ee3\u5165 B \u4e2d\uff0c \\begin{aligned} B &= \\frac{\\sqrt{\\alpha_t}}{1 - \\alpha_t} x_t + \\frac{x_t}{(1-\\bar\\alpha_{t-1})\\sqrt{\\alpha_t}} - \\frac{\\sqrt{1-\\bar\\alpha_t}z_0}{(1-\\bar\\alpha_{t-1})\\sqrt{\\alpha_t}} \\\\ &= \\frac{(1-\\bar\\alpha_t)x_t}{(1-\\alpha_t)(1-\\bar\\alpha_{t-1})\\sqrt{\\alpha_t}} - \\frac{\\sqrt{1-\\bar\\alpha_t}z_0}{(1-\\bar\\alpha_{t-1})\\sqrt{\\alpha_t}} \\end{aligned} \u5747\u503c \\mu = B * 1/A = \\frac{1}{\\sqrt{\\alpha_t}} x_t - \\frac{1 - \\alpha_t}{\\sqrt{\\alpha_t} \\sqrt{1 - \\bar\\alpha_t}} z_t \u4ece\u6b64\u53ef\u8ba1\u7b97\u51fa q(x_{t-1}|x_t, x_0) \u662f\u4e00\u4e2a\u4ec5\u4e0e x_t, z_t \u6709\u5173\u7684\uff0c\u4ee5 \\mu, \\sigma^2 \u4e3a\u7279\u5f81\u7684\u9ad8\u65af\u5206\u5e03\u3002\u8fd9\u4e2a\u5747\u503c\u7684\u8ba1\u7b97\u4e5f\u53ef\u4ee5\u76f4\u89c2\u5730\u7406\u89e3\u4e3a\u589e\u5f3a\u8f93\u5165\u6570\u636e\u5e76\u51cf\u53bb\u4e00\u4e2a\u4f30\u8ba1\u7684\u566a\u58f0 z_t , \u5c31\u662f\u524d\u5411\u91c7\u6837\u7684\u4e00\u4e2a\u9006\u8fd0\u7b97\u3002\u8fd9\u4e2a\u5f0f\u5b50\u4e5f\u8bf4\u660e\u4e86\uff0c\u4e3a\u4ec0\u4e48\u6211\u4eec\u8bf4\u53ef\u4ee5\u901a\u8fc7\u4f30\u8ba1\u566a\u58f0\u51e0\u4e4e\u7b49\u4ef7\u4e8e\u76f4\u63a5\u4f30\u8ba1 x_{t-1} \u635f\u5931\u51fd\u6570 \u76f4\u89c2\u7684\u8bbe\u8ba1\u662f\u8bf4\u6839\u636e\u524d\u6587\uff0c\u8ba1\u7b97\u91cd\u5efa\u6bcf\u4e00\u4e2a\u65f6\u95f4 t \u7684\u56fe\u50cf\uff0c\u6bd4\u8f83\u53cd\u5411\u548c\u6b63\u5411\u7684\u56fe\u7247\u7684\u76f8\u4f3c\u5ea6\u4f5c\u4e3a\u635f\u5931\u51fd\u6570\uff0c\u672c\u6587\u5219\u8fdb\u4e00\u6b65\u7b80\u5316\u8fd9\u4e2a\u7ed3\u679c\uff0c\u6700\u7ec8\u91c7\u7528\u7684\u662f\u4e00\u4e2a L_2 \u51fd\u6570\uff0c\u6bd4\u8f83\u9884\u6d4b\u51fa\u6765\u7684\u566a\u58f0\u548c\u5b9e\u9645\u7684\u566a\u58f0\u7684\u76f8\u4f3c\u6027\u3002\u5982\u679c\u80fd\u901a\u8fc7\u88ab\u5e72\u6270\u7684\u56fe\u7247\u51c6\u786e\u5730\u9884\u6d4b\u566a\u58f0\uff0c\u663e\u7136\u6211\u4eec\u5c31\u80fd\u53cd\u63a8\u5f97\u5230\u539f\u6765\u7684\u8f93\u5165\u56fe\u7247\u3002\u56e0\u800c\u8fd9\u4e2a\u635f\u5931\u662f\u76f4\u89c2\u5408\u7406\u7684\u3002 \u4f5c\u8005\u8fdb\u4e00\u6b65\u4ece\u6570\u5b66\u89d2\u5ea6\u6709\u5206\u6790,\u601d\u8def\u4e0e VAE \u5f88\u63a5\u8fd1\u3002\u9996\u5148\u660e\u786e\u9700\u8981\u4f18\u5316\u95ee\u9898\u662f \\argmax_\\theta{p_\\theta(x_0)} , \u4e5f\u5c31\u662f\u9009\u62e9\u53c2\u6570 \\theta \uff0c \u6700\u5927\u5316\u7f51\u7edc\u9884\u6d4b\u7684\u6620\u5c04\u51fd\u6570 p_\\theta \u5728\u6570\u636e\u96c6\u6570\u636e x_0 \u4e2d\u6982\u7387\u3002 \u6a21\u4effVAE\u7684\u63a8\u7406\uff0c\u53ef\u4ee5\u8ba1\u7b97\u5f97\u5230 D_{KL}(q(x_{T:1} | x_0) || p_\\theta(x_{T:1} | x_0)) = \\log{p(x_0)} + \\mathbb{E}_{x_{T:1} \\sim q(x_{T:1} | x_0)}[\\log\\frac{q(x_{T:1} | x_0)}{p(x_{T:0})}] > 0 \\log{p(x_0)} > -\\mathbb{E}_{x_{T:1} \\sim q(x_{T:1} | x_0)}[\\log\\frac{q(x_{T:1} | x_0)}{p(x_{T:0})}] \u4ece\u800c\u786e\u5b9a\u4e86 \\log p(x_0) \u7684\u4e0b\u9650(ELBO),\u6ce8\u610f\u53f3\u9879\u8fd9\u91cc q \u662f\u524d\u63a8\u51fd\u6570\u5bb9\u6613\u8868\u8fbe\uff0c\u4e5f\u5c31\u662f\u9ad8\u65af\uff0c\u800c p \u662f\u540e\u63a8\u6761\u4ef6\u51fd\u6570\u5bb9\u6613\u8868\u8fbe\uff0c\u4e5f\u5c31\u662f\u7f51\u7edc\u7684\u9884\u6d4b(\u566a\u58f0). \\begin{aligned} L_{\\mathrm{VLB}} &=\\mathbb{E}_{q\\left(\\mathbf{x}_{0: T}\\right)}\\left[\\log \\frac{q\\left(\\mathbf{x}_{1: T} \\mid \\mathbf{x}_{0}\\right)}{p_{\\theta}\\left(\\mathbf{x}_{0: T}\\right)}\\right] \\\\ &=\\mathbb{E}_{q}\\left[\\log \\frac{\\prod_{t=1}^{T} q\\left(\\mathbf{x}_{t} \\mid \\mathbf{x}_{t-1}\\right)}{p_{\\theta}\\left(\\mathbf{x}_{T}\\right) \\prod_{t=1}^{T} p_{\\theta}\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_{t}\\right)}\\right] \\\\ &=\\mathbb{E}_{q}\\left[-\\log p_{\\theta}\\left(\\mathbf{x}_{T}\\right)+\\sum_{t=1}^{T} \\log \\frac{q\\left(\\mathbf{x}_{t} \\mid \\mathbf{x}_{t-1}\\right)}{p_{\\theta}\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_{t}\\right)}\\right] \\\\ &=\\mathbb{E}_{q}\\left[-\\log p_{\\theta}\\left(\\mathbf{x}_{T}\\right)+\\sum_{t=2}^{T} \\log \\frac{q\\left(\\mathbf{x}_{t} \\mid \\mathbf{x}_{t-1}\\right)}{p_{\\theta}\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_{t}\\right)}+\\log \\frac{q\\left(\\mathbf{x}_{1} \\mid \\mathbf{x}_{0}\\right)}{p_{\\theta}\\left(\\mathbf{x}_{0} \\mid \\mathbf{x}_{1}\\right)}\\right] \\\\ &=\\mathbb{E}_{q}\\left[-\\log p_{\\theta}\\left(\\mathbf{x}_{T}\\right)+\\sum_{t=2}^{T} \\log \\left(\\frac{q\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_{t}, \\mathbf{x}_{0}\\right)}{p_{\\theta}\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_{t}\\right)} \\cdot \\frac{q\\left(\\mathbf{x}_{t} \\mid \\mathbf{x}_{0}\\right)}{q\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_{0}\\right)}\\right)+\\log \\frac{q\\left(\\mathbf{x}_{1} \\mid \\mathbf{x}_{0}\\right)}{p_{\\theta}\\left(\\mathbf{x}_{0} \\mid \\mathbf{x}_{1}\\right)}\\right] \\\\ &=\\mathbb{E}_{q}\\left[-\\log p_{\\theta}\\left(\\mathbf{x}_{T}\\right)+\\sum_{t=2}^{T} \\log \\frac{q\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_{t}, \\mathbf{x}_{0}\\right)}{p_{\\theta}\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_{t}\\right)}+\\sum_{t=2}^{T} \\log \\frac{q\\left(\\mathbf{x}_{t} \\mid \\mathbf{x}_{0}\\right)}{q\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_{0}\\right)}+\\log \\frac{q\\left(\\mathbf{x}_{1} \\mid \\mathbf{x}_{0}\\right)}{p_{\\theta}\\left(\\mathbf{x}_{0} \\mid \\mathbf{x}_{1}\\right)}\\right] \\\\ &=\\mathbb{E}_{q}\\left[-\\log p_{\\theta}\\left(\\mathbf{x}_{T}\\right)+\\sum_{t=2}^{T} \\log \\frac{q\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_{t}, \\mathbf{x}_{0}\\right)}{p_{\\theta}\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_{t}\\right)}+\\log \\frac{q\\left(\\mathbf{x}_{T} \\mid \\mathbf{x}_{0}\\right)}{q\\left(\\mathbf{x}_{1} \\mid \\mathbf{x}_{0}\\right)}+\\log \\frac{q\\left(\\mathbf{x}_{1} \\mid \\mathbf{x}_{0}\\right)}{p_{\\theta}\\left(\\mathbf{x}_{0} \\mid \\mathbf{x}_{1}\\right)}\\right] \\\\ &=\\mathbb{E}_{q}\\left[\\log \\frac{q\\left(\\mathbf{x}_{T} \\mid \\mathbf{x}_{0}\\right)}{p_{\\theta}\\left(\\mathbf{x}_{T}\\right)}+\\sum_{t=2}^{T} \\log \\frac{q\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_{t}, \\mathbf{x}_{0}\\right)}{p_{\\theta}\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_{t}\\right)}-\\log p_{\\theta}\\left(\\mathbf{x}_{0} \\mid \\mathbf{x}_{1}\\right)\\right] \\\\ &=\\mathbb{E}_{q}\\left[D_{\\mathrm{KL}}\\left(q\\left(\\mathbf{x}_{T} \\mid \\mathbf{x}_{0}\\right) \\| p_{\\theta}\\left(\\mathbf{x}_{T}\\right)\\right)+\\sum^{\\mathrm{T}}_{t=2} D_{\\mathrm{KL}}\\left(q\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_{t}, \\mathbf{x}_{0}\\right) \\| p_{\\theta}\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_{t}\\right)\\right)-\\log p_{\\theta}\\left(\\mathbf{x}_{0} | x_1\\right)\\right]\\end{aligned} \u4e0a\u6587\u7b2c\u4e00\u9879\u4e0e \\theta \u65e0\u5173\uff0c\u56e0\u4e3a p(x_T) \u5c31\u662f\u566a\u58f0\uff0c\u6b63\u592a\u9ad8\u65af\u5206\u5e03\u3002\u6700\u540e\u4e00\u9879\u6839\u636e\u7684\u662f\u6700\u540e\u4e00\u6b65\u7684\u9006\u63a8\u516c\u5f0f\u505a\u3002\u800c\u4e2d\u95f4\u90e8\u5206\u5c31\u662f\u4e24\u4e2a\u9ad8\u65af\u4e4b\u95f4\u7684\u76f8\u4f3c\u5ea6(KL\u8ddd\u79bb)\uff0c \u9ad8\u65af\u4e4b\u95f4\u7684\u76f8\u4f3c\u5ea6\u4e25\u683c\u6765\u8bf4\u5e94\u8be5\u5982\u4e0b: \\begin{aligned} K L(p, q) &=-\\int p(x) \\log q(x) d x+\\int p(x) \\log p(x) d x \\\\ &=\\frac{1}{2} \\log \\left(2 \\pi \\sigma_{2}^{2}\\right)+\\frac{\\sigma_{1}^{2}+\\left(\\mu_{1}-\\mu_{2}\\right)^{2}}{2 \\sigma_{2}^{2}}-\\frac{1}{2}\\left(1+\\log 2 \\pi \\sigma_{1}^{2}\\right) \\\\ &=\\log \\frac{\\sigma_{2}}{\\sigma_{1}}+\\frac{\\sigma_{1}^{2}+\\left(\\mu_{1}-\\mu_{2}\\right)^{2}}{2 \\sigma_{2}^{2}}-\\frac{1}{2} \\end{aligned} \u4f46\u662f\u53ef\u4ee5\u901a\u8fc7\u8bad\u7ec3\u566a\u97f3\u6216\u8005\u91cd\u5efa\u56fe\u7247\u7684\u76f8\u4f3c\u6027\u76f4\u63a5\u7ed5\u8fc7\u8fd9\u4e2a\u635f\u5931\u51fd\u6570\u7684\u9009\u62e9\u3002\u4e0d\u540c\u70b9\u53ea\u662f\u5728\u53c2\u6570\u7684\u6743\u91cd\u4e0a\uff0c\u56e0\u800c\u4e0d\u662f\u91cd\u70b9\u3002\u4f46\u8fd9\u4e2a\u63a8\u7406\u8fc7\u7a0b\u8bf4\u660e\u4e86\u6b64\u524d\u7b80\u5355\u635f\u5931\u7684\u5145\u5206\u6027\u3002 youtube \u4e0a\u7684\u4e0a\u624b\u89c6\u9891","title":"Denoising Diffusion Probabilistic Models"},{"location":"The_theory/diffussion_model/#denoising-diffusion-probabilistic-models","text":"\u8fd9\u7bc7paper\u662fdiffusion model\u5728\u751f\u6210\u6a21\u578b\u4e2d\u7684\u5e94\u7528\u3002\u8fd9\u7bc7\u6587\u7ae0\u6709\u4e00\u4e2a\u63a8\u5bfc\u516c\u5f0f\u6bd4\u8f83\u7ec6\u81f4\uff0c\u8bb2\u89e3\u6bd4\u8f83\u6e05\u6670\u7684 \u535a\u5ba2 .","title":"Denoising Diffusion Probabilistic Models"},{"location":"The_theory/diffussion_model/#diffusion-model","text":"\u8be5\u535a\u5ba2\u4e2d\u6709\u5982\u6b64\u4e00\u56fe\uff0c\u5927\u81f4\u603b\u7ed3\u4e86\u5f53\u524d\u751f\u6210\u6a21\u578b\u7684\u56db\u5927\u7c7b\u522b\u3002\u5bf9\u6297\u7f51\u7edc, VAE , flow-based model, \u4ee5\u53ca\u672c\u6587\u7684\u6269\u6563\u6a21\u578b\u3002 \u8fd9\u51e0\u79cd\u65b9\u5f0f\u7684\u5bf9\u6bd4\u6765\u770b\uff0cVAE\u548cFlow Model\u53ef\u4ee5\u5feb\u901f\u5730\u91c7\u6837\u51fa\u79cd\u7c7b\u8303\u56f4\u6bd4\u8f83\u5e7f\u7684\u6837\u672c\u3002GAN\u53ef\u4ee5\u5feb\u901f\u91c7\u6837\u51fa\u8d28\u91cf\u9ad8\u7684\u6837\u672c\uff0c\u800cDiffsion\u5219\u91c7\u6837\u901f\u5ea6\u6162\uff0c\u4f46\u662f\u91c7\u6837\u8303\u56f4\u79cd\u7c7b\u5e7f\u4e14\u8d28\u91cf\u5f88\u9ad8\u3002","title":"Diffusion Model \u5728\u751f\u6210\u6a21\u578b\u4e2d\u7684\u7279\u70b9"},{"location":"The_theory/diffussion_model/#denoising-diffusion-model","text":"Diffusion \u64cd\u4f5c\u4e0a\u8868\u8fbe\u7684\u662f\u5bf9\u56fe\u7247\u9010\u6b65\u589e\u52a0\u9ad8\u65af\u566a\u58f0\uff0c\u6700\u7ec8\u628a\u56fe\u7247\u5b8c\u5168corrupt \u6210\u566a\u97f3\u7684\u8fc7\u7a0b\uff0c\u800c\u53cd\u5411\u5730\uff0c\u4ece\u566a\u97f3\u4e2d\u8fd8\u539f\u51fa\u56fe\u7247\u5730pattern\u5219\u662f\u751f\u6210\u7684\u8fc7\u7a0b\u3002 \u6570\u5b66\u4e0a\u6765\u8bf4\u5bf9\u56fe\u7247/\u6570\u636e\u9010\u6b65\u589e\u52a0\u9ad8\u65af\u566a\u58f0\u7684\u8fc7\u7a0b\u88ab\u8bbe\u8ba1\u6210\u4e00\u4e2a\u9a6c\u5c14\u53ef\u592b\u94fe\uff0c\u6bcf\u4e00\u4e2a\u65f6\u523b\u7684\u72b6\u6001\u53ea\u7531\u4e0a\u4e00\u4e2a\u65f6\u523b\u7684\u5747\u503c\u4ee5\u53ca\u989d\u5916\u7684\u566a\u58f0\u5f71\u54cd\u3002\u6982\u7387\u5730\u5199\u51fa\u516c\u5f0f q(x_t | x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1 - \\beta_t} x_{t-1}, \\beta_t \\mathbf{I}) , \u767d\u8bdd\u800c\u8a00\u5c31\u662f\u628a\u524d\u4e00\u4e2a\u65f6\u523b\u7684\u56fe\u50cf/\u6570\u636e/\u5206\u5e03scale down, \u518d\u4e0e\u4e00\u4e2a\u989d\u5916\u7684\u9ad8\u65af\u5206\u5e03\u52a0\u548c\u3002 \u53cd\u5411\u7684\u65f6\u5019\u6211\u4eec\u5df2\u77e5 x_T ,\u9700\u8981\u9010\u6b65\u4f30\u8ba1\u7b2c T \u6b65\u7684\u566a\u97f3 z_t \uff0c\u7136\u540e\u8ba9\u6570\u636e\"\u51cf\u53bb\"\u8fd9\u4e2a\u566a\u97f3\u3002\u672c\u6587\u63d0\u51fa\u7684\u5c31\u662f\u7528\u795e\u7ecf\u7f51\u7edc\uff0c\u8f93\u5165 x_T \u548c T \u65f6\u523b\u7684time_encoding, \u4f30\u8ba1\u8fd9\u4e2a\u5206\u8fa8\u7387\u4e0e\u539f\u56fe\u4e00\u6837\u7684\u566a\u97f3,\u5e76\u9010\u6b65\u53bb\u9664\u566a\u97f3\u5f97\u5230\u539f\u56fe\u3002\u56e0\u800c\u79f0\u4e3a\"Denoising Diffusion Model\". \u56e0\u800c\u5728\u529f\u80fd\u6a21\u7ec4\u4e0a\uff0c\u8fd9\u7c7b\u6a21\u578b\u5206\u4e3a\u4e00\u4e0b\u51e0\u4e2a\u90e8\u5206\uff0c\u4e0d\u540c\u7684\u6587\u7ae0\u4f1a\u6709\u4e0d\u540c\u7684\u5047\u8bbe\u4ee5\u53ca\u9009\u62e9\uff0c\u540e\u9762\u4f1a\u518d\u6570\u5b66\u4e0e\u4ee3\u7801\u4e0a\u7ed9\u51fa\u5b9e\u4f8b. \u524d\u5411\u91c7\u6837,\u8ba1\u7b97\u4ece\u56fe\u7247\u5230\u7b2c T \u65f6\u523b\u88abcorrupt\u7684\u72b6\u6001. \u8fd9\u91cc\u7684\u4e3b\u8981\u53d8\u91cf\u5728\u4e8e\u91c7\u6837\u65f6\u95f4\u53ef\u53d8\uff0c\u4e14 \\beta(t) \u5173\u4e8e\u65f6\u95f4\u7684\u51fd\u6570\uff0c\u79f0\u4e3anoise scheduler\u662f\u53ef\u63a7\u5236\u7684\uff0c\u5982\u7ebf\u6027\u589e\u52a0\u566a\u58f0\u7b49\u65b9\u6848\u3002 \u566a\u58f0\u4f30\u8ba1\u7f51\u7edc \\Theta , \u8f93\u5165 x_t \u65f6\u523b\u56fe\u7247\u4ee5\u53catime encoding\u8f93\u51fa\u540c\u5206\u8fa8\u7387\u566a\u58f0 z_t , \u4ee5UNet\u4e3a\u4e3b\u67b6\u6784\uff0c\u914d\u5408Attention\u7b49\u6a21\u5757\u9009\u62e9 (\u6bd4\u5982\u540e\u6765\u4e00\u4e9b\u52a0\u5165\u8bed\u8a00\u6216\u5916\u90e8\u4fe1\u606f\u7684\u751f\u6210\u6a21\u578b\u591a\u4f7f\u7528attention). \u635f\u5931\u51fd\u6570\uff0c\u5982\u4f55\u8bad\u7ec3\u566a\u58f0\u4f30\u8ba1(\u56fe\u7247\u4f30\u8ba1)\u7f51\u7edc\uff0c\u53ef\u4ee5\u6709\u5f88\u5f3a\u7684\u6982\u7387\u5b66\u652f\u6491\uff0c\u4e5f\u53ef\u4ee5\u50cf\u672c\u6587\u6700\u540e\u7684baseline implementation\u4e00\u6837\u7b80\u5355\u66b4\u529b\u3002 \u91c7\u6837\u63a8\u7406\uff0c\u6240\u8c13\u7684\"\u51cf\u53bb\u566a\u97f3\"\u6b65\u9aa4\uff0c\u8fd9\u91cc\u53ef\u4ee5\u6709\u66f4\u4e25\u8c28\u7684\u63a8\u5bfc\u8ba1\u7b97\u5f97\u5230\u66f4\u51c6\u786e\u7684\u6570\u503c\u3002","title":"Denoising Diffusion Model \u4e3b\u8981\u5de5\u4f5c\u6d41\u4e0e\u7ec4\u4ef6\u5206\u7c7b"},{"location":"The_theory/diffussion_model/#_1","text":"\u5982\u679c\u8f93\u5165\u56fe\u7247\u4e5f\u662f\u9ad8\u65af\u5206\u5e03\uff0c\u9ad8\u65af\u5206\u5e03\u4e0e\u9ad8\u65af\u5206\u5e03\u7684\u53e0\u52a0\u662f\u95ed\u5f0f\u7684\u9ad8\u65af\uff0c\u7528\u9012\u63a8\u7684\u516c\u5f0f\u8ba1\u7b97\u91c7\u6837\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u53ef\u4ee5\u5f88\u5feb\u53d1\u73b0\u6211\u4eec\u4e0d\u9700\u8981\u591a\u6b65\u8fed\u4ee3\u6765\u5b9e\u73b0\u52a0\u566a\u97f3\u91c7\u6837\uff0c\u800c\u662f\u53ef\u4ee5\u6839\u636enoise scheduler\u4ee5\u53ca\u968f\u673a\u6570\u751f\u6210\u5668\u76f4\u63a5\u5f97\u5230\u4efb\u610f\u65f6\u95f4\u70b9\u7684\u566a\u97f3\u548c\u56fe\u50cf\u6570\u636e\u3002 \u4ee4 \\alpha_t = 1 - \\beta_t , \\bar{\\alpha}_{t}=\\prod_{i=1}^{T} \\alpha_{i} , z_t \u662f\u4ee3\u7801\u4ece\u9ad8\u65af\u5206\u5e03\u4e2d\u91c7\u6837\u7684\u566a\u58f0\u3002 \\begin{aligned} x_t &= \\sqrt{\\alpha_t} x_{t-1} + \\sqrt{1 - \\alpha_t} z_{t-1} \\\\ &= \\sqrt{\\alpha_t \\alpha_{t-1}} x_{t-1} + \\sqrt{1 - \\alpha_t \\alpha_{t-1}} \\bar{z}_{t-2} \\\\ &= \\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\bar{z_0} \\end{aligned} \u6ce8\u610f\u4e24\u4e2a\u9ad8\u65af\u566a\u58f0\u65b9\u5dee\u4e3a \\sigma^2_1, \\sigma^2_2 \u7684 z \u7684\u878d\u5408\uff0c\u5f97\u5230\u7684\u662f\u4e00\u4e2a\u65b9\u5dee\u4e3a \\sigma_1^2 + \\sigma^2_2 \u7684\u9ad8\u65af\u5206\u5e03\u3002\u6240\u4ee5\u4e0a\u6587\u7684\u8ba1\u7b97\u5b9e\u9645\u4e0a\u662f \u65b9\u5dee\u4e3a 1-\\alpha_t \u7684\u9ad8\u65af\u5206\u5e03\u548c \\alpha_t * (1-\\alpha_{t-1}) \u7684\u9ad8\u65af\u5206\u5e03\u7684\u878d\u5408\uff0c\u5f97\u5230\u7684\u5c31\u662f\u65b9\u5dee\u4e3a 1 - \\alpha_t\\alpha_{t-1} \u7684\u9ad8\u65af\u5206\u5e03\uff0c\u7d2f\u79ef\u7ed3\u679c\u540c\u6837\u3002\u81f3\u6b64\uff0c\u6211\u4eec\u53ef\u4ee5\u6839\u636e\u539f\u6570\u636e x_0 , \u566a\u58f0\u89c4\u5212\u5668\u5f97\u5230\u7684 \\bar\\alpha_t , \u4ee5\u53ca\u91c7\u6837\u7684\u9ad8\u65af\u566a\u58f0\u5f97\u5230\u4efb\u610f\u65f6\u523b\u7684\u56fe\u7247 x_t","title":"\u524d\u5411\u91c7\u6837"},{"location":"The_theory/diffussion_model/#_2","text":"\u4f5c\u8005\u6307\u51fa\uff0c\u5728\u5df2\u77e5(conditioned on) x_t, x_0 \u65f6\uff0c\u53cd\u5411\u6982\u7387\u4e5f\u7b26\u5408\u4e00\u4e2a\u9ad8\u65af\u5206\u5e03 q(x_{t-1} | x_{t}, x_0) = \\mathcal{N}(x_{t-1}; \\tilde\\mu(x_t, x_0), \\tilde\\beta_t \\mathbf{I}) \u7531\u4e8e\u524d\u5411\u7684\u6982\u7387\u5206\u5e03\u662f\u5df2\u77e5\u7684\uff0c\u8fd9\u91cc\u7684\u540e\u9a8c\u5206\u5e03\u5c31\u7528\u8d1d\u53f6\u65af\u516c\u5f0f\u8f6c\u6362\u4e3a\u524d\u5411 (\u5206\u5b50\u662f x_t, x_{t-1} \u7684\u4e58\u79ef\u6982\u7387\uff0c\u66f4\u6539\u6761\u4ef6\u6982\u7387\u7684\u6761\u4ef6 conditioned on x_{t-1} , \u53ef\u5c06\u5168\u90e8\u53d8\u4e3a\u524d\u5411)\uff1b\u5e76\u91cd\u70b9\u5173\u6ce8\u6982\u7387\u5206\u5e03\u51fd\u6570\u7684\u6307\u6570\u90e8\u5206, \u7136\u540e\u628a\u5176\u4e2d\u7684 x_{t-1} \u63d0\u51fa\u6765\u3002 \\begin{aligned} q(x_{t-1} | x_t, x_0) &= q(x_t | x_{t-1}, x_0) \\frac{q(x_{t-1} | x_0)}{q(x_t | x_0)} = q(x_t | x_{t-1}) \\frac{q(x_{t-1} | x_0)}{q(x_t | x_0)} \\\\ & \\propto \\text{exp} (-\\frac{1}{2} (\\frac{(x_t - \\sqrt{\\alpha_t} x_{t-1})^2}{1-\\alpha_t} + \\frac{(x_{t-1} - \\sqrt{\\bar\\alpha_{t-1}}x_0)^2}{1 - \\bar\\alpha_{t-1}} - \\frac{(x_t - \\sqrt{\\bar\\alpha_t}x_0)^2}{1 - \\bar\\alpha_t})) \\\\ &=\\text{exp}(-\\frac{1}{2}[ (\\frac{\\alpha_t}{1 - \\alpha_t} + \\frac{1}{1-\\bar\\alpha_{t-1}}) x_{t-1}^2 - \\\\ &(\\frac{2\\sqrt{\\alpha_t} x_t}{1 - \\alpha_t}+ \\frac{2\\sqrt{\\bar\\alpha_{t-1}}x_0}{1 - \\bar\\alpha_{t-1}} )x_{t-1} + C(x_t, x_0) ] ) \\end{aligned} \u901a\u5206\u5e76\u51d1\u5e73\u65b9\uff0c\u627e\u51fa x_{t-1} \u7684\u65b9\u5dee\u548c\u5747\u503c. \u7531 Ax^2 - 2Bx = \\frac{(x - B/A)^2}{1/A} , \u5176\u4e2d A = \\frac{\\alpha_t}{1 - \\alpha_t} + \\frac{1}{1-\\bar\\alpha_{t-1}} = \\frac{1 - \\bar\\alpha_t}{\\beta_t (1 - \\bar\\alpha_{t-1})} \u65b9\u5dee \\sigma^2 = 1 / A = \\frac{\\beta_t (1 - \\bar\\alpha_{t-1})}{1 - \\bar\\alpha_t} . \u628a x_0 = \\frac{x_t - \\sqrt{1 - \\bar\\alpha_t} \\bar z_0}{\\sqrt{\\bar\\alpha_t}} \u4ee3\u5165 B \u4e2d\uff0c \\begin{aligned} B &= \\frac{\\sqrt{\\alpha_t}}{1 - \\alpha_t} x_t + \\frac{x_t}{(1-\\bar\\alpha_{t-1})\\sqrt{\\alpha_t}} - \\frac{\\sqrt{1-\\bar\\alpha_t}z_0}{(1-\\bar\\alpha_{t-1})\\sqrt{\\alpha_t}} \\\\ &= \\frac{(1-\\bar\\alpha_t)x_t}{(1-\\alpha_t)(1-\\bar\\alpha_{t-1})\\sqrt{\\alpha_t}} - \\frac{\\sqrt{1-\\bar\\alpha_t}z_0}{(1-\\bar\\alpha_{t-1})\\sqrt{\\alpha_t}} \\end{aligned} \u5747\u503c \\mu = B * 1/A = \\frac{1}{\\sqrt{\\alpha_t}} x_t - \\frac{1 - \\alpha_t}{\\sqrt{\\alpha_t} \\sqrt{1 - \\bar\\alpha_t}} z_t \u4ece\u6b64\u53ef\u8ba1\u7b97\u51fa q(x_{t-1}|x_t, x_0) \u662f\u4e00\u4e2a\u4ec5\u4e0e x_t, z_t \u6709\u5173\u7684\uff0c\u4ee5 \\mu, \\sigma^2 \u4e3a\u7279\u5f81\u7684\u9ad8\u65af\u5206\u5e03\u3002\u8fd9\u4e2a\u5747\u503c\u7684\u8ba1\u7b97\u4e5f\u53ef\u4ee5\u76f4\u89c2\u5730\u7406\u89e3\u4e3a\u589e\u5f3a\u8f93\u5165\u6570\u636e\u5e76\u51cf\u53bb\u4e00\u4e2a\u4f30\u8ba1\u7684\u566a\u58f0 z_t , \u5c31\u662f\u524d\u5411\u91c7\u6837\u7684\u4e00\u4e2a\u9006\u8fd0\u7b97\u3002\u8fd9\u4e2a\u5f0f\u5b50\u4e5f\u8bf4\u660e\u4e86\uff0c\u4e3a\u4ec0\u4e48\u6211\u4eec\u8bf4\u53ef\u4ee5\u901a\u8fc7\u4f30\u8ba1\u566a\u58f0\u51e0\u4e4e\u7b49\u4ef7\u4e8e\u76f4\u63a5\u4f30\u8ba1 x_{t-1}","title":"\u91c7\u6837\u63a8\u7406"},{"location":"The_theory/diffussion_model/#_3","text":"\u76f4\u89c2\u7684\u8bbe\u8ba1\u662f\u8bf4\u6839\u636e\u524d\u6587\uff0c\u8ba1\u7b97\u91cd\u5efa\u6bcf\u4e00\u4e2a\u65f6\u95f4 t \u7684\u56fe\u50cf\uff0c\u6bd4\u8f83\u53cd\u5411\u548c\u6b63\u5411\u7684\u56fe\u7247\u7684\u76f8\u4f3c\u5ea6\u4f5c\u4e3a\u635f\u5931\u51fd\u6570\uff0c\u672c\u6587\u5219\u8fdb\u4e00\u6b65\u7b80\u5316\u8fd9\u4e2a\u7ed3\u679c\uff0c\u6700\u7ec8\u91c7\u7528\u7684\u662f\u4e00\u4e2a L_2 \u51fd\u6570\uff0c\u6bd4\u8f83\u9884\u6d4b\u51fa\u6765\u7684\u566a\u58f0\u548c\u5b9e\u9645\u7684\u566a\u58f0\u7684\u76f8\u4f3c\u6027\u3002\u5982\u679c\u80fd\u901a\u8fc7\u88ab\u5e72\u6270\u7684\u56fe\u7247\u51c6\u786e\u5730\u9884\u6d4b\u566a\u58f0\uff0c\u663e\u7136\u6211\u4eec\u5c31\u80fd\u53cd\u63a8\u5f97\u5230\u539f\u6765\u7684\u8f93\u5165\u56fe\u7247\u3002\u56e0\u800c\u8fd9\u4e2a\u635f\u5931\u662f\u76f4\u89c2\u5408\u7406\u7684\u3002 \u4f5c\u8005\u8fdb\u4e00\u6b65\u4ece\u6570\u5b66\u89d2\u5ea6\u6709\u5206\u6790,\u601d\u8def\u4e0e VAE \u5f88\u63a5\u8fd1\u3002\u9996\u5148\u660e\u786e\u9700\u8981\u4f18\u5316\u95ee\u9898\u662f \\argmax_\\theta{p_\\theta(x_0)} , \u4e5f\u5c31\u662f\u9009\u62e9\u53c2\u6570 \\theta \uff0c \u6700\u5927\u5316\u7f51\u7edc\u9884\u6d4b\u7684\u6620\u5c04\u51fd\u6570 p_\\theta \u5728\u6570\u636e\u96c6\u6570\u636e x_0 \u4e2d\u6982\u7387\u3002 \u6a21\u4effVAE\u7684\u63a8\u7406\uff0c\u53ef\u4ee5\u8ba1\u7b97\u5f97\u5230 D_{KL}(q(x_{T:1} | x_0) || p_\\theta(x_{T:1} | x_0)) = \\log{p(x_0)} + \\mathbb{E}_{x_{T:1} \\sim q(x_{T:1} | x_0)}[\\log\\frac{q(x_{T:1} | x_0)}{p(x_{T:0})}] > 0 \\log{p(x_0)} > -\\mathbb{E}_{x_{T:1} \\sim q(x_{T:1} | x_0)}[\\log\\frac{q(x_{T:1} | x_0)}{p(x_{T:0})}] \u4ece\u800c\u786e\u5b9a\u4e86 \\log p(x_0) \u7684\u4e0b\u9650(ELBO),\u6ce8\u610f\u53f3\u9879\u8fd9\u91cc q \u662f\u524d\u63a8\u51fd\u6570\u5bb9\u6613\u8868\u8fbe\uff0c\u4e5f\u5c31\u662f\u9ad8\u65af\uff0c\u800c p \u662f\u540e\u63a8\u6761\u4ef6\u51fd\u6570\u5bb9\u6613\u8868\u8fbe\uff0c\u4e5f\u5c31\u662f\u7f51\u7edc\u7684\u9884\u6d4b(\u566a\u58f0). \\begin{aligned} L_{\\mathrm{VLB}} &=\\mathbb{E}_{q\\left(\\mathbf{x}_{0: T}\\right)}\\left[\\log \\frac{q\\left(\\mathbf{x}_{1: T} \\mid \\mathbf{x}_{0}\\right)}{p_{\\theta}\\left(\\mathbf{x}_{0: T}\\right)}\\right] \\\\ &=\\mathbb{E}_{q}\\left[\\log \\frac{\\prod_{t=1}^{T} q\\left(\\mathbf{x}_{t} \\mid \\mathbf{x}_{t-1}\\right)}{p_{\\theta}\\left(\\mathbf{x}_{T}\\right) \\prod_{t=1}^{T} p_{\\theta}\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_{t}\\right)}\\right] \\\\ &=\\mathbb{E}_{q}\\left[-\\log p_{\\theta}\\left(\\mathbf{x}_{T}\\right)+\\sum_{t=1}^{T} \\log \\frac{q\\left(\\mathbf{x}_{t} \\mid \\mathbf{x}_{t-1}\\right)}{p_{\\theta}\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_{t}\\right)}\\right] \\\\ &=\\mathbb{E}_{q}\\left[-\\log p_{\\theta}\\left(\\mathbf{x}_{T}\\right)+\\sum_{t=2}^{T} \\log \\frac{q\\left(\\mathbf{x}_{t} \\mid \\mathbf{x}_{t-1}\\right)}{p_{\\theta}\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_{t}\\right)}+\\log \\frac{q\\left(\\mathbf{x}_{1} \\mid \\mathbf{x}_{0}\\right)}{p_{\\theta}\\left(\\mathbf{x}_{0} \\mid \\mathbf{x}_{1}\\right)}\\right] \\\\ &=\\mathbb{E}_{q}\\left[-\\log p_{\\theta}\\left(\\mathbf{x}_{T}\\right)+\\sum_{t=2}^{T} \\log \\left(\\frac{q\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_{t}, \\mathbf{x}_{0}\\right)}{p_{\\theta}\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_{t}\\right)} \\cdot \\frac{q\\left(\\mathbf{x}_{t} \\mid \\mathbf{x}_{0}\\right)}{q\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_{0}\\right)}\\right)+\\log \\frac{q\\left(\\mathbf{x}_{1} \\mid \\mathbf{x}_{0}\\right)}{p_{\\theta}\\left(\\mathbf{x}_{0} \\mid \\mathbf{x}_{1}\\right)}\\right] \\\\ &=\\mathbb{E}_{q}\\left[-\\log p_{\\theta}\\left(\\mathbf{x}_{T}\\right)+\\sum_{t=2}^{T} \\log \\frac{q\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_{t}, \\mathbf{x}_{0}\\right)}{p_{\\theta}\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_{t}\\right)}+\\sum_{t=2}^{T} \\log \\frac{q\\left(\\mathbf{x}_{t} \\mid \\mathbf{x}_{0}\\right)}{q\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_{0}\\right)}+\\log \\frac{q\\left(\\mathbf{x}_{1} \\mid \\mathbf{x}_{0}\\right)}{p_{\\theta}\\left(\\mathbf{x}_{0} \\mid \\mathbf{x}_{1}\\right)}\\right] \\\\ &=\\mathbb{E}_{q}\\left[-\\log p_{\\theta}\\left(\\mathbf{x}_{T}\\right)+\\sum_{t=2}^{T} \\log \\frac{q\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_{t}, \\mathbf{x}_{0}\\right)}{p_{\\theta}\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_{t}\\right)}+\\log \\frac{q\\left(\\mathbf{x}_{T} \\mid \\mathbf{x}_{0}\\right)}{q\\left(\\mathbf{x}_{1} \\mid \\mathbf{x}_{0}\\right)}+\\log \\frac{q\\left(\\mathbf{x}_{1} \\mid \\mathbf{x}_{0}\\right)}{p_{\\theta}\\left(\\mathbf{x}_{0} \\mid \\mathbf{x}_{1}\\right)}\\right] \\\\ &=\\mathbb{E}_{q}\\left[\\log \\frac{q\\left(\\mathbf{x}_{T} \\mid \\mathbf{x}_{0}\\right)}{p_{\\theta}\\left(\\mathbf{x}_{T}\\right)}+\\sum_{t=2}^{T} \\log \\frac{q\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_{t}, \\mathbf{x}_{0}\\right)}{p_{\\theta}\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_{t}\\right)}-\\log p_{\\theta}\\left(\\mathbf{x}_{0} \\mid \\mathbf{x}_{1}\\right)\\right] \\\\ &=\\mathbb{E}_{q}\\left[D_{\\mathrm{KL}}\\left(q\\left(\\mathbf{x}_{T} \\mid \\mathbf{x}_{0}\\right) \\| p_{\\theta}\\left(\\mathbf{x}_{T}\\right)\\right)+\\sum^{\\mathrm{T}}_{t=2} D_{\\mathrm{KL}}\\left(q\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_{t}, \\mathbf{x}_{0}\\right) \\| p_{\\theta}\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_{t}\\right)\\right)-\\log p_{\\theta}\\left(\\mathbf{x}_{0} | x_1\\right)\\right]\\end{aligned} \u4e0a\u6587\u7b2c\u4e00\u9879\u4e0e \\theta \u65e0\u5173\uff0c\u56e0\u4e3a p(x_T) \u5c31\u662f\u566a\u58f0\uff0c\u6b63\u592a\u9ad8\u65af\u5206\u5e03\u3002\u6700\u540e\u4e00\u9879\u6839\u636e\u7684\u662f\u6700\u540e\u4e00\u6b65\u7684\u9006\u63a8\u516c\u5f0f\u505a\u3002\u800c\u4e2d\u95f4\u90e8\u5206\u5c31\u662f\u4e24\u4e2a\u9ad8\u65af\u4e4b\u95f4\u7684\u76f8\u4f3c\u5ea6(KL\u8ddd\u79bb)\uff0c \u9ad8\u65af\u4e4b\u95f4\u7684\u76f8\u4f3c\u5ea6\u4e25\u683c\u6765\u8bf4\u5e94\u8be5\u5982\u4e0b: \\begin{aligned} K L(p, q) &=-\\int p(x) \\log q(x) d x+\\int p(x) \\log p(x) d x \\\\ &=\\frac{1}{2} \\log \\left(2 \\pi \\sigma_{2}^{2}\\right)+\\frac{\\sigma_{1}^{2}+\\left(\\mu_{1}-\\mu_{2}\\right)^{2}}{2 \\sigma_{2}^{2}}-\\frac{1}{2}\\left(1+\\log 2 \\pi \\sigma_{1}^{2}\\right) \\\\ &=\\log \\frac{\\sigma_{2}}{\\sigma_{1}}+\\frac{\\sigma_{1}^{2}+\\left(\\mu_{1}-\\mu_{2}\\right)^{2}}{2 \\sigma_{2}^{2}}-\\frac{1}{2} \\end{aligned} \u4f46\u662f\u53ef\u4ee5\u901a\u8fc7\u8bad\u7ec3\u566a\u97f3\u6216\u8005\u91cd\u5efa\u56fe\u7247\u7684\u76f8\u4f3c\u6027\u76f4\u63a5\u7ed5\u8fc7\u8fd9\u4e2a\u635f\u5931\u51fd\u6570\u7684\u9009\u62e9\u3002\u4e0d\u540c\u70b9\u53ea\u662f\u5728\u53c2\u6570\u7684\u6743\u91cd\u4e0a\uff0c\u56e0\u800c\u4e0d\u662f\u91cd\u70b9\u3002\u4f46\u8fd9\u4e2a\u63a8\u7406\u8fc7\u7a0b\u8bf4\u660e\u4e86\u6b64\u524d\u7b80\u5355\u635f\u5931\u7684\u5145\u5206\u6027\u3002","title":"\u635f\u5931\u51fd\u6570"},{"location":"The_theory/diffussion_model/#youtube","text":"","title":"youtube \u4e0a\u7684\u4e0a\u624b\u89c6\u9891"},{"location":"The_theory/dino/","text":"Emerging Properties in Self-Supervised Vision Transformers \u5b98\u65b9\u535a\u5ba2\u4e3b\u9875 \u8fd9\u7bc7paper\u63d0\u51fa\u4e86\u81ea\u76d1\u7763\u8bad\u7ec3ViT, \u7ed9\u51fa\u7684\u6027\u80fd\u5f88\u9ad8\uff0c\u63a5\u8fd1\u4e8e\u6709\u76d1\u7763\u7684\u6570\u636e\uff0c\u4e14\u5176\u8f93\u51fa\u7684feature map\u6027\u80fd\u5f88\u9ad8\u3002 Related works Moco pdf code Moco \u662f\u4e00\u4e2a\u81ea\u76d1\u7763\u5b66\u4e60\u56fe\u7247\u5206\u7c7b\u7684\u6846\u67b6\uff0c\u5176\u7b97\u6cd5\u5982\u56fe: ViT ViT\u5177\u4f53\u53c2\u8003 \u8fd9\u7bc7\u6587\u7ae0 Method \u8fd9\u7bc7paper\u7684\u65b9\u6cd5\u548cMoCo\u6709\u4e00\u5b9a\u7684\u76f8\u4f3c\u6027\uff0c\u5f88\u53d7Moco\u7684\u542f\u53d1\uff0c\u4f46\u662f\u5b83\u628a\u4ed6\u7528\u5728transformer\u4e0a.\u4f5c\u8005\u53d1\u73b0\u5176transformer\u91cc\u9762\u7684\u5206\u7c7btoken\u53ef\u4ee5\u76f4\u63a5\u7528\u4e8e\u524d\u666f\u7684segmentation.","title":"Emerging Properties in Self-Supervised Vision Transformers"},{"location":"The_theory/dino/#emerging-properties-in-self-supervised-vision-transformers","text":"\u5b98\u65b9\u535a\u5ba2\u4e3b\u9875 \u8fd9\u7bc7paper\u63d0\u51fa\u4e86\u81ea\u76d1\u7763\u8bad\u7ec3ViT, \u7ed9\u51fa\u7684\u6027\u80fd\u5f88\u9ad8\uff0c\u63a5\u8fd1\u4e8e\u6709\u76d1\u7763\u7684\u6570\u636e\uff0c\u4e14\u5176\u8f93\u51fa\u7684feature map\u6027\u80fd\u5f88\u9ad8\u3002","title":"Emerging Properties in Self-Supervised Vision Transformers"},{"location":"The_theory/dino/#related-works","text":"","title":"Related works"},{"location":"The_theory/dino/#moco","text":"pdf code Moco \u662f\u4e00\u4e2a\u81ea\u76d1\u7763\u5b66\u4e60\u56fe\u7247\u5206\u7c7b\u7684\u6846\u67b6\uff0c\u5176\u7b97\u6cd5\u5982\u56fe:","title":"Moco"},{"location":"The_theory/dino/#vit","text":"ViT\u5177\u4f53\u53c2\u8003 \u8fd9\u7bc7\u6587\u7ae0","title":"ViT"},{"location":"The_theory/dino/#method","text":"\u8fd9\u7bc7paper\u7684\u65b9\u6cd5\u548cMoCo\u6709\u4e00\u5b9a\u7684\u76f8\u4f3c\u6027\uff0c\u5f88\u53d7Moco\u7684\u542f\u53d1\uff0c\u4f46\u662f\u5b83\u628a\u4ed6\u7528\u5728transformer\u4e0a.\u4f5c\u8005\u53d1\u73b0\u5176transformer\u91cc\u9762\u7684\u5206\u7c7btoken\u53ef\u4ee5\u76f4\u63a5\u7528\u4e8e\u524d\u666f\u7684segmentation.","title":"Method"},{"location":"The_theory/lle/","text":"An Introduction to Locally Linear Embedding \u8fd1\u671f\u5728\u56de\u987e PLiDAR++ \u4e2d\u4f7f\u7528\u96f7\u8fbe\u70b9\u4e91\u5bf9\u53cc\u76ee\u5339\u914d\u7ed3\u679c\u8fdb\u884c\u4f18\u5316\u7684\u65b9\u6cd5\u3002\u8be5\u4f5c\u8005\u63d0\u5230\uff0c\u5176\u4f18\u5316\u65b9\u6cd5\u7b49\u540c\u4e8e manifold learning\u4e2d\u7684 locally linear embedding (LLE), \u540c\u65f6\u6709fixed landmark\u4f5c\u4e3aguidance. \u9996\u5148\u629b\u5f00PLiDAR\u8bba\u6587\u7684\u6982\u5ff5\uff0c\u5148\u5f15\u5165\u4ee5\u4e0b\u6982\u5ff5: Manifold Learning \u5b9e\u8d28\u4e0a\u662f\u4e00\u7c7b\u9ad8\u7ef4\u6570\u636e\u964d\u7ef4\u7b97\u6cd5\u3002\u5b83\u4eec\u5047\u8bbe\u6570\u636e\u96c6\u4e2d\u6570\u636e\u70b9\u7684\u6574\u4f53\u5206\u5e03\u6709\u4e00\u5b9a\u7684\u51e0\u4f55/\u7edf\u8ba1\u89c4\u5219 (\u50cf\u662f\u4e0a\u56fe\u4e2d\u7684\u4e00\u4e2aS\u578b\u6d41\u578b)\u3002 scikit-learn \u5176\u4e2dLocally Linear Embedding\u5c5e\u4e8eManifold learning\u7684\u4e00\u79cd\u65b9\u6cd5\uff0c\u5b83\u5047\u8bbe\u6bcf\u4e00\u4e2a\u6570\u636e\u70b9\u4ee5\u53ca\u5b83\u7684\u8fd1\u90bb\u4f1a\u8fd1\u4f3c\u5730\u5b58\u5728\u4e8e\u6d41\u578b\u7684\u4e00\u4e2a\u5c40\u90e8\u7ebf\u6027\u9762\u5143\u4e0a\u3002 \u7b2c\u4e00\u6b65\uff0c\u9700\u8981\u8ba1\u7b97\u6bcf\u4e00\u4e2a\u6570\u636e\u70b9\u4e0a\u7684\u9762\u5143\u7279\u5f81(\u4e5f\u5c31\u662f\u9ad8\u7ef4\u9762\u5143\u7684\u7ebf\u6027\u65b9\u7a0b\u53c2\u6570)\u3002\u8fd9\u4e2a\u95ee\u9898\u8f6c\u6362\u4e3a\u7528\u6700\u8fd1\u90bb\u7279\u5f81\u7ebf\u6027\u6c42\u548c\u91cd\u5efa\u51fa\u539f\u7279\u5f81\u3002\u4e25\u8c28\u6765\u8bf4\u662f\u4e00\u4e2a \u4eff\u5c04(affine) \u8f6c\u6362, \u4e5f\u5c31\u662f\u6743\u91cd\u4e4b\u548c\u4e3a1\u3002\u91cd\u5efa\u7684\u7ed3\u679c W \u662f\u63cf\u8ff0\u6bcf\u4e00\u4e2a\u6570\u636e\u70b9\u9762\u5143\u7684\u53c2\u6570\u77e2\u91cf\u3002 \u5728\u6570\u636e\u964d\u7ef4\u7684LLE\u4e2d\uff0c\u6700\u8fd1\u90bb\u70b9\u7684\u6570\u91cf k \u8fdc\u8fdc\u5c0f\u4e8e\u6570\u636e\u7684\u7ef4\u5ea6\u3002 \\mathcal{E}(W)=\\sum_{i}\\left|\\vec{X}_{i}-\\sum_{j} W_{i j} \\vec{X}_{j}\\right|^{2} \u7b2c\u4e8c\u6b65\uff0c\u9700\u8981\u7528\u4ee5\u4e2ak\u4f4e\u7eac\u5ea6\u7684\u77e2\u91cf Y \u91cd\u5efa\u6570\u636e, \\Phi(Y)=\\sum_{i}\\left|\\vec{Y}_{i}-\\sum_{j} W_{i j} \\vec{Y}_{j}\\right|^{2} \u56de\u5230 PLiDAR ++, \u505a\u6cd5\u6709\u4e00\u5b9a\u533a\u522b\u3002\u9996\u5148\u5728\u7b2c\u4e00\u6b65\u9762\u5143\u7279\u5f81\u8ba1\u7b97\u65f6\uff0c\u6bcf\u4e00\u4e2a\u70b9\u7684\u4e09\u7ef4\u7279\u5f81\u7ef4\u5ea6\u5373\u4e3a3\uff0c \u800c\u6700\u8fd1\u90bb\u9ed8\u8ba4\u91c7\u53d610\u4e2a\u70b9\u3002\u540c\u4e00\u4e2a\u70b9\u6709\u65e0\u9650\u591a\u4e2a\u4eff\u5c04\u53d8\u6362\u8868\u793a\u65b9\u6cd5\uff0c\u6240\u4ee5PLiDAR++\u63d0\u51fa\u9700\u8981\u8fdb\u4e00\u6b65\u7ea6\u675f L_2 norm \u4ee5\u627e\u51fa\u4f18\u5316\u95ee\u9898\u552f\u4e00\u89e3\u3002 W=\\arg \\min _{W}\\|Z-W Z\\|_{2}^{2}, \\quad \\text { s.t. } W \\mathbf{1}=\\mathbf{1} \\text { and } W_{i j}=0 \\text { if } j \\notin \\mathcal{N}_{i} \u7b2c\u4e8c\u6b65\u91cd\u5efa\u65f6\uff0c\u5229\u7528\u70b9\u4e91\u70b9\u4f5c\u4e3alandmark. Z^{\\prime}=\\arg \\min _{Z^{\\prime}}\\left\\|Z^{\\prime}-W Z^{\\prime}\\right\\|^{2}, \\quad \\text { s.t. } Z_{1: n}^{\\prime}=G \\text {. }","title":"An Introduction to Locally Linear Embedding"},{"location":"The_theory/lle/#an-introduction-to-locally-linear-embedding","text":"\u8fd1\u671f\u5728\u56de\u987e PLiDAR++ \u4e2d\u4f7f\u7528\u96f7\u8fbe\u70b9\u4e91\u5bf9\u53cc\u76ee\u5339\u914d\u7ed3\u679c\u8fdb\u884c\u4f18\u5316\u7684\u65b9\u6cd5\u3002\u8be5\u4f5c\u8005\u63d0\u5230\uff0c\u5176\u4f18\u5316\u65b9\u6cd5\u7b49\u540c\u4e8e manifold learning\u4e2d\u7684 locally linear embedding (LLE), \u540c\u65f6\u6709fixed landmark\u4f5c\u4e3aguidance. \u9996\u5148\u629b\u5f00PLiDAR\u8bba\u6587\u7684\u6982\u5ff5\uff0c\u5148\u5f15\u5165\u4ee5\u4e0b\u6982\u5ff5: Manifold Learning \u5b9e\u8d28\u4e0a\u662f\u4e00\u7c7b\u9ad8\u7ef4\u6570\u636e\u964d\u7ef4\u7b97\u6cd5\u3002\u5b83\u4eec\u5047\u8bbe\u6570\u636e\u96c6\u4e2d\u6570\u636e\u70b9\u7684\u6574\u4f53\u5206\u5e03\u6709\u4e00\u5b9a\u7684\u51e0\u4f55/\u7edf\u8ba1\u89c4\u5219 (\u50cf\u662f\u4e0a\u56fe\u4e2d\u7684\u4e00\u4e2aS\u578b\u6d41\u578b)\u3002 scikit-learn \u5176\u4e2dLocally Linear Embedding\u5c5e\u4e8eManifold learning\u7684\u4e00\u79cd\u65b9\u6cd5\uff0c\u5b83\u5047\u8bbe\u6bcf\u4e00\u4e2a\u6570\u636e\u70b9\u4ee5\u53ca\u5b83\u7684\u8fd1\u90bb\u4f1a\u8fd1\u4f3c\u5730\u5b58\u5728\u4e8e\u6d41\u578b\u7684\u4e00\u4e2a\u5c40\u90e8\u7ebf\u6027\u9762\u5143\u4e0a\u3002 \u7b2c\u4e00\u6b65\uff0c\u9700\u8981\u8ba1\u7b97\u6bcf\u4e00\u4e2a\u6570\u636e\u70b9\u4e0a\u7684\u9762\u5143\u7279\u5f81(\u4e5f\u5c31\u662f\u9ad8\u7ef4\u9762\u5143\u7684\u7ebf\u6027\u65b9\u7a0b\u53c2\u6570)\u3002\u8fd9\u4e2a\u95ee\u9898\u8f6c\u6362\u4e3a\u7528\u6700\u8fd1\u90bb\u7279\u5f81\u7ebf\u6027\u6c42\u548c\u91cd\u5efa\u51fa\u539f\u7279\u5f81\u3002\u4e25\u8c28\u6765\u8bf4\u662f\u4e00\u4e2a \u4eff\u5c04(affine) \u8f6c\u6362, \u4e5f\u5c31\u662f\u6743\u91cd\u4e4b\u548c\u4e3a1\u3002\u91cd\u5efa\u7684\u7ed3\u679c W \u662f\u63cf\u8ff0\u6bcf\u4e00\u4e2a\u6570\u636e\u70b9\u9762\u5143\u7684\u53c2\u6570\u77e2\u91cf\u3002 \u5728\u6570\u636e\u964d\u7ef4\u7684LLE\u4e2d\uff0c\u6700\u8fd1\u90bb\u70b9\u7684\u6570\u91cf k \u8fdc\u8fdc\u5c0f\u4e8e\u6570\u636e\u7684\u7ef4\u5ea6\u3002 \\mathcal{E}(W)=\\sum_{i}\\left|\\vec{X}_{i}-\\sum_{j} W_{i j} \\vec{X}_{j}\\right|^{2} \u7b2c\u4e8c\u6b65\uff0c\u9700\u8981\u7528\u4ee5\u4e2ak\u4f4e\u7eac\u5ea6\u7684\u77e2\u91cf Y \u91cd\u5efa\u6570\u636e, \\Phi(Y)=\\sum_{i}\\left|\\vec{Y}_{i}-\\sum_{j} W_{i j} \\vec{Y}_{j}\\right|^{2} \u56de\u5230 PLiDAR ++, \u505a\u6cd5\u6709\u4e00\u5b9a\u533a\u522b\u3002\u9996\u5148\u5728\u7b2c\u4e00\u6b65\u9762\u5143\u7279\u5f81\u8ba1\u7b97\u65f6\uff0c\u6bcf\u4e00\u4e2a\u70b9\u7684\u4e09\u7ef4\u7279\u5f81\u7ef4\u5ea6\u5373\u4e3a3\uff0c \u800c\u6700\u8fd1\u90bb\u9ed8\u8ba4\u91c7\u53d610\u4e2a\u70b9\u3002\u540c\u4e00\u4e2a\u70b9\u6709\u65e0\u9650\u591a\u4e2a\u4eff\u5c04\u53d8\u6362\u8868\u793a\u65b9\u6cd5\uff0c\u6240\u4ee5PLiDAR++\u63d0\u51fa\u9700\u8981\u8fdb\u4e00\u6b65\u7ea6\u675f L_2 norm \u4ee5\u627e\u51fa\u4f18\u5316\u95ee\u9898\u552f\u4e00\u89e3\u3002 W=\\arg \\min _{W}\\|Z-W Z\\|_{2}^{2}, \\quad \\text { s.t. } W \\mathbf{1}=\\mathbf{1} \\text { and } W_{i j}=0 \\text { if } j \\notin \\mathcal{N}_{i} \u7b2c\u4e8c\u6b65\u91cd\u5efa\u65f6\uff0c\u5229\u7528\u70b9\u4e91\u70b9\u4f5c\u4e3alandmark. Z^{\\prime}=\\arg \\min _{Z^{\\prime}}\\left\\|Z^{\\prime}-W Z^{\\prime}\\right\\|^{2}, \\quad \\text { s.t. } Z_{1: n}^{\\prime}=G \\text {. }","title":"An Introduction to Locally Linear Embedding"},{"location":"The_theory/mean_field_theory/","text":"Mean Field Theory in Deep Learning \u5e73\u5747\u573a\u7406\u8bba\u5728\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u7406\u8bba\u5206\u6790\u662f\u4e00\u4e2a\u6df1\u5ea6\u6bd4\u8f83\u5927\u7684\u95ee\u9898\uff0c\u7531\u4e8e\u4e2a\u4eba\u7684\u9605\u8bfb\u80fd\u529b\u95ee\u9898\uff0c\u4e3b\u8981\u505a\u7684\u662f\u4e00\u4e9b\u7b80\u5355\u7684\u4ecb\u7ecd\u4e0e\u603b\u7ed3. \u5e73\u5747\u573a\u7406\u8bba\u5728\u6df1\u5ea6\u5b66\u4e60\u4e2d\u5b9e\u8d28\u4e0a\u5c31\u662f\u7528\u7edf\u8ba1\u65b9\u6cd5\u5206\u6790\u7f51\u7edc\u524d\u4f20\u4e0e\u53cd\u4f20\u65f6\u7684\u7edf\u8ba1\u503c\u52a8\u529b\u5b66\uff0c\u7136\u540e\u4f9d\u636e\u6b64\u5206\u6790\u7f51\u7edc\u7684\u8bad\u7ec3\u80fd\u529b\u3001\u62df\u5408\u80fd\u529b\uff0c\u5e76\u6311\u9009\u7f51\u7edc\u8bbe\u8ba1\u51c6\u5219\u3002 \u7531\u4e8e\u540c\u6837\u5730\u8981\u6c42\u7f51\u7edc\u7684\u5bbd\u5ea6\u8db3\u591f\u5bbd\u4ee5\u81f3\u4e8e\u65e0\u7a77\u5bbd\uff0c\u6240\u4ee5\u5e73\u5747\u573a\u7406\u8bba(MFT)\u7684\u5206\u6790\u7406\u5e94\u4e0e\u795e\u7ecf\u6b63\u5207\u6838(Neural Tangent Kernel, NTK)\u7ed9\u51fa\u76f8\u4f3c\u7684\u7ed3\u8bba. NTK \u6307\u51fa\u795e\u7ecf\u7f51\u7edc\u5728\u65e0\u9650\u5bbd\u7684\u65f6\u5019\u4f1a\u903c\u8fd1\u4e00\u4e2a\u7ebf\u6027\u6a21\u578b\uff0c\u4e8c\u9636\u5bfc\u6570\u9664\u4ee5\u4e00\u9636\u5bfc\u6570 (\u4e00\u9636\u5bfc\u6570\u7684\u76f8\u5bf9\u53d8\u5316\u7387)\u4f1a\u63a5\u8fd1\u4e8e\u96f6\uff0c\u56e0\u800c\u795e\u7ecf\u7f51\u7edc\u4f1a\u5f88\u597d\u8bad\u7ec3\u3002 Deep Information Propagation pdf \u77e5\u4e4e\u53c2\u8003 \u8fd9\u7bc7paper\u7684\u5047\u8bbe\u662f\u65e0\u7a77\u5bbd\u5168\u8fde\u63a5\u7f51\u7edc\uff0c\u65e0BN. \u5728\u521d\u59cb\u5316\u7684\u65f6\u5019\uff0c W_{i j}^{l} \\sim N\\left(0, \\sigma_{w}^{2} / N_{l}\\right) \u800c b_i^l \\sim N(0, \\sigma_b^2) . \u503c\u5f97\u6ce8\u610f\u7684\u662f\u76ee\u524d\u7684pytorch\u7684\u7ebf\u6027\u5c42\u4f7f\u7528\u7684\u662f\u5747\u5300\u91c7\u6837\u800c\u4e0d\u662f\u9ad8\u65af\u91c7\u6837\uff0c\u6240\u4ee5\u6570\u503c\u7ed3\u8bba\u53ef\u80fd\u4f1a\u6709\u51fa\u5165. \u6b64\u65f6\u7f51\u7edc\u4f20\u9012\u516c\u5f0f\u4e3a: z_{i}^{l}=\\sum_{j} W_{i j}^{l} y_{j}^{l}+b_{i}^{l} \\quad y_{i}^{l+1}=\\phi\\left(z_{i}^{l}\\right) \u4ece\u7edf\u8ba1\u4e0a\u53ef\u4ee5\u77e5\u9053 pre-activation\u7684\u671f\u671b \\mathbb{E}[z_{i;a}^l] = 0 , \u800c\u5176\u534f\u65b9\u5dee\u77e9\u9635\u4e3a\u4e00\u4e2a\u5bf9\u89d2\u7ebf\u77e9\u9635\uff0c\u4f5c\u8005\u8fd9\u91cc\u7528 Kronecker delta\u8fd0\u7b97\u7b26 \\delta_{ij} = [i==j] \u8868\u8fbe\uff0c \\mathbb{E}[z^l_{i;a} z^{l}_{j;a}] = q^l_{aa}\\delta_{ij} \u3002 \u6ce8\u610f z \u5e76\u4e0d\u7b26\u5408\u9ad8\u65af\u5206\u5e03\uff0c\u539f\u56e0\u662f\u975e\u7ebf\u6027\u6fc0\u6d3b\u6539\u53d8\u4e86\u5b83\u7684\u5f62\u6001\uff0c\u56e0\u6b64\u4f5c\u8005\u5f3a\u8c03\u4e86\u5e73\u5747\u573a\u7406\u8bba\u7684\u91cd\u8981\u8fd1\u4f3c\u5c31\u662f\u7528\u4e00\u4e2a Gaussian whose first two moments match those of z \u53bb\u66ff\u4ee3 z . \u56e0\u6b64\u4e3b\u5bf9\u89d2\u7ebf\u4e0a\u7684\u534f\u65b9\u5dee\u7406\u8bba\u4e0a\u9700\u8981\u79ef\u5206\u8fed\u4ee3\u8ba1\u7b97: q_{aa}^l = \\sigma^2_{w} \\int{Dz\\phi^2\\left( \\sqrt{q^{l-1}_{aa} z} \\right)} + \\sigma_b^2 \u5176\u4e2d D \u662f\u6807\u51c6\u9ad8\u65af\u5206\u5e03\u3002 \u5206\u6790\u4e24\u4e2a\u8f93\u5165 x \u5728\u8f93\u5165\u7f51\u7edc\u540e\u5b83\u4eec\u4e4b\u95f4\u76f8\u5173\u6027\u968f\u7740\u7f51\u7edc\u4f20\u64ad\u7684\u53d8\u5316\uff0c\u901a\u8fc7\u6bd4\u8f83\u590d\u6742\u7684\u5fae\u79ef\u5206\u8fd0\u7b97\uff0c\u53ef\u4ee5\u5f97\u5230(\u53c2\u8003 \u524d\u4e00\u7bc7\u6587\u7ae0 \u7684\u6570\u5b66\u63a8\u5bfc). \u5f97\u5230\u4e24\u4e2a\u8f93\u5165\u5728\u7f51\u7edc\u4e2d\u7684pre-activation\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u968f\u7740\u7f51\u7edc\u4f20\u9012\u7684\u53d8\u5316\u7279\u5f81\u3002 \\chi_{1}=\\frac{\\partial c_{a b}^{l}}{\\partial c_{a b}^{l-1}}=\\sigma_{w}^{2} \\int \\mathcal{D} z\\left[\\phi^{\\prime}\\left(\\sqrt{q^{*}} z\\right)\\right]^{2} \u5f53 \\chi_1 < 1 \u65f6\u6240\u6709\u7684\u8f93\u5165\u90fd\u4f1a\u6e10\u8fdb\u7684\u53d8\u6210\u5e26\u6709\u76f8\u5173\u6027\uff0c\u5f53 \\chi_1 > 1 \u65f6\u6240\u6709\u7684\u8f93\u5165\u90fd\u4f1a\u53d8\u6210\u4e0d\u76f8\u5173\uff0c\u53ea\u6709 \\chi_1 = 1 \u65f6\u662f\u7a33\u5b9a\u7684\uff0c\u8fd9\u5bf9\u5e94\u4e86\u7279\u5b9a\u6fc0\u6d3b\u51fd\u6570\u7684\u7279\u5b9a\u521d\u59cb\u5316\u6743\u91cd\u3002 \u5982\u679c\u662f tanh \u6fc0\u6d3b\u7684\u8bdd\uff0c\u5176\u76f8\u56fe\u5982\u4e0b: \u7ed3\u8bba\u4e0a\u6765\u8bf4\uff0c\u5982\u679c\u53c2\u6570\u5728\u8fd9\u6761\u4e34\u754c\u7ebf\u4e0a\uff0c\u7f51\u7edc\u5c31\u4f1a\u597d\u8bad\u7ec3\u3002\u4ece\u8868\u8fbe\u6027\u7684\u89d2\u5ea6\u6765\u770b\uff0c\u5bf9\u4e8e\u5728\u4e34\u754c\u7ebf\u4e0a\u7684\u7f51\u7edc\uff0c\u4e24\u4e2a\u8f93\u5165\u4fe1\u53f7\u7684\u76f8\u5173\u4fe1\u53f7\u5728\u7f51\u7edc\u4e2d\u4e0d\u5bb9\u6613\u88ab\u5fd8\u8bb0\uff0c\u6240\u4ee5\u53ef\u4ee5\u5728\u7f51\u7edc\u5f88\u6df1\u7684\u65f6\u5019\u4fe1\u606f\u7684\u4f20\u9012\u4e5f\u80fd\u6709\u4fdd\u8bc1\u3002 \u5982\u679c\u7814\u7a76\u4fe1\u53f7\u7684\u540e\u5411\u4f20\u64ad\uff0c\u4ee5\u5e73\u5747\u573a\u7406\u8bba\u8ba1\u7b97\uff0c\u53ef\u4ee5\u5f97\u5230\u4e0e\u4e0a\u56fe\u7c7b\u4f3c\u7684\u7ed3\u8bba\uff0c\u4e14\u4e00\u81f4\u7684\u53c2\u6570\u9009\u62e9\u7ebf\u3002\u4e5f\u5c31\u662f\u5f53\u7f51\u7edc\u53c2\u6570\u5904\u5728\u4e34\u754c\u7ebf\u4e0a\u65f6\uff0c\u68af\u5ea6\u7684\u4fe1\u53f7\u4f20\u9012\u4f1a\u6bd4\u8f83\u7a33\u5b9a\uff0c\u56e0\u6b64\u66f4\u6df1\u7684\u7f51\u7edc\u4e5f\u80fd\u8bad\u7ec3\u3002","title":"Mean Field Theory in Deep Learning"},{"location":"The_theory/mean_field_theory/#mean-field-theory-in-deep-learning","text":"\u5e73\u5747\u573a\u7406\u8bba\u5728\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u7406\u8bba\u5206\u6790\u662f\u4e00\u4e2a\u6df1\u5ea6\u6bd4\u8f83\u5927\u7684\u95ee\u9898\uff0c\u7531\u4e8e\u4e2a\u4eba\u7684\u9605\u8bfb\u80fd\u529b\u95ee\u9898\uff0c\u4e3b\u8981\u505a\u7684\u662f\u4e00\u4e9b\u7b80\u5355\u7684\u4ecb\u7ecd\u4e0e\u603b\u7ed3. \u5e73\u5747\u573a\u7406\u8bba\u5728\u6df1\u5ea6\u5b66\u4e60\u4e2d\u5b9e\u8d28\u4e0a\u5c31\u662f\u7528\u7edf\u8ba1\u65b9\u6cd5\u5206\u6790\u7f51\u7edc\u524d\u4f20\u4e0e\u53cd\u4f20\u65f6\u7684\u7edf\u8ba1\u503c\u52a8\u529b\u5b66\uff0c\u7136\u540e\u4f9d\u636e\u6b64\u5206\u6790\u7f51\u7edc\u7684\u8bad\u7ec3\u80fd\u529b\u3001\u62df\u5408\u80fd\u529b\uff0c\u5e76\u6311\u9009\u7f51\u7edc\u8bbe\u8ba1\u51c6\u5219\u3002 \u7531\u4e8e\u540c\u6837\u5730\u8981\u6c42\u7f51\u7edc\u7684\u5bbd\u5ea6\u8db3\u591f\u5bbd\u4ee5\u81f3\u4e8e\u65e0\u7a77\u5bbd\uff0c\u6240\u4ee5\u5e73\u5747\u573a\u7406\u8bba(MFT)\u7684\u5206\u6790\u7406\u5e94\u4e0e\u795e\u7ecf\u6b63\u5207\u6838(Neural Tangent Kernel, NTK)\u7ed9\u51fa\u76f8\u4f3c\u7684\u7ed3\u8bba. NTK \u6307\u51fa\u795e\u7ecf\u7f51\u7edc\u5728\u65e0\u9650\u5bbd\u7684\u65f6\u5019\u4f1a\u903c\u8fd1\u4e00\u4e2a\u7ebf\u6027\u6a21\u578b\uff0c\u4e8c\u9636\u5bfc\u6570\u9664\u4ee5\u4e00\u9636\u5bfc\u6570 (\u4e00\u9636\u5bfc\u6570\u7684\u76f8\u5bf9\u53d8\u5316\u7387)\u4f1a\u63a5\u8fd1\u4e8e\u96f6\uff0c\u56e0\u800c\u795e\u7ecf\u7f51\u7edc\u4f1a\u5f88\u597d\u8bad\u7ec3\u3002","title":"Mean Field Theory in Deep Learning"},{"location":"The_theory/mean_field_theory/#deep-information-propagation","text":"pdf \u77e5\u4e4e\u53c2\u8003 \u8fd9\u7bc7paper\u7684\u5047\u8bbe\u662f\u65e0\u7a77\u5bbd\u5168\u8fde\u63a5\u7f51\u7edc\uff0c\u65e0BN. \u5728\u521d\u59cb\u5316\u7684\u65f6\u5019\uff0c W_{i j}^{l} \\sim N\\left(0, \\sigma_{w}^{2} / N_{l}\\right) \u800c b_i^l \\sim N(0, \\sigma_b^2) . \u503c\u5f97\u6ce8\u610f\u7684\u662f\u76ee\u524d\u7684pytorch\u7684\u7ebf\u6027\u5c42\u4f7f\u7528\u7684\u662f\u5747\u5300\u91c7\u6837\u800c\u4e0d\u662f\u9ad8\u65af\u91c7\u6837\uff0c\u6240\u4ee5\u6570\u503c\u7ed3\u8bba\u53ef\u80fd\u4f1a\u6709\u51fa\u5165. \u6b64\u65f6\u7f51\u7edc\u4f20\u9012\u516c\u5f0f\u4e3a: z_{i}^{l}=\\sum_{j} W_{i j}^{l} y_{j}^{l}+b_{i}^{l} \\quad y_{i}^{l+1}=\\phi\\left(z_{i}^{l}\\right) \u4ece\u7edf\u8ba1\u4e0a\u53ef\u4ee5\u77e5\u9053 pre-activation\u7684\u671f\u671b \\mathbb{E}[z_{i;a}^l] = 0 , \u800c\u5176\u534f\u65b9\u5dee\u77e9\u9635\u4e3a\u4e00\u4e2a\u5bf9\u89d2\u7ebf\u77e9\u9635\uff0c\u4f5c\u8005\u8fd9\u91cc\u7528 Kronecker delta\u8fd0\u7b97\u7b26 \\delta_{ij} = [i==j] \u8868\u8fbe\uff0c \\mathbb{E}[z^l_{i;a} z^{l}_{j;a}] = q^l_{aa}\\delta_{ij} \u3002 \u6ce8\u610f z \u5e76\u4e0d\u7b26\u5408\u9ad8\u65af\u5206\u5e03\uff0c\u539f\u56e0\u662f\u975e\u7ebf\u6027\u6fc0\u6d3b\u6539\u53d8\u4e86\u5b83\u7684\u5f62\u6001\uff0c\u56e0\u6b64\u4f5c\u8005\u5f3a\u8c03\u4e86\u5e73\u5747\u573a\u7406\u8bba\u7684\u91cd\u8981\u8fd1\u4f3c\u5c31\u662f\u7528\u4e00\u4e2a Gaussian whose first two moments match those of z \u53bb\u66ff\u4ee3 z . \u56e0\u6b64\u4e3b\u5bf9\u89d2\u7ebf\u4e0a\u7684\u534f\u65b9\u5dee\u7406\u8bba\u4e0a\u9700\u8981\u79ef\u5206\u8fed\u4ee3\u8ba1\u7b97: q_{aa}^l = \\sigma^2_{w} \\int{Dz\\phi^2\\left( \\sqrt{q^{l-1}_{aa} z} \\right)} + \\sigma_b^2 \u5176\u4e2d D \u662f\u6807\u51c6\u9ad8\u65af\u5206\u5e03\u3002 \u5206\u6790\u4e24\u4e2a\u8f93\u5165 x \u5728\u8f93\u5165\u7f51\u7edc\u540e\u5b83\u4eec\u4e4b\u95f4\u76f8\u5173\u6027\u968f\u7740\u7f51\u7edc\u4f20\u64ad\u7684\u53d8\u5316\uff0c\u901a\u8fc7\u6bd4\u8f83\u590d\u6742\u7684\u5fae\u79ef\u5206\u8fd0\u7b97\uff0c\u53ef\u4ee5\u5f97\u5230(\u53c2\u8003 \u524d\u4e00\u7bc7\u6587\u7ae0 \u7684\u6570\u5b66\u63a8\u5bfc). \u5f97\u5230\u4e24\u4e2a\u8f93\u5165\u5728\u7f51\u7edc\u4e2d\u7684pre-activation\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u968f\u7740\u7f51\u7edc\u4f20\u9012\u7684\u53d8\u5316\u7279\u5f81\u3002 \\chi_{1}=\\frac{\\partial c_{a b}^{l}}{\\partial c_{a b}^{l-1}}=\\sigma_{w}^{2} \\int \\mathcal{D} z\\left[\\phi^{\\prime}\\left(\\sqrt{q^{*}} z\\right)\\right]^{2} \u5f53 \\chi_1 < 1 \u65f6\u6240\u6709\u7684\u8f93\u5165\u90fd\u4f1a\u6e10\u8fdb\u7684\u53d8\u6210\u5e26\u6709\u76f8\u5173\u6027\uff0c\u5f53 \\chi_1 > 1 \u65f6\u6240\u6709\u7684\u8f93\u5165\u90fd\u4f1a\u53d8\u6210\u4e0d\u76f8\u5173\uff0c\u53ea\u6709 \\chi_1 = 1 \u65f6\u662f\u7a33\u5b9a\u7684\uff0c\u8fd9\u5bf9\u5e94\u4e86\u7279\u5b9a\u6fc0\u6d3b\u51fd\u6570\u7684\u7279\u5b9a\u521d\u59cb\u5316\u6743\u91cd\u3002 \u5982\u679c\u662f tanh \u6fc0\u6d3b\u7684\u8bdd\uff0c\u5176\u76f8\u56fe\u5982\u4e0b: \u7ed3\u8bba\u4e0a\u6765\u8bf4\uff0c\u5982\u679c\u53c2\u6570\u5728\u8fd9\u6761\u4e34\u754c\u7ebf\u4e0a\uff0c\u7f51\u7edc\u5c31\u4f1a\u597d\u8bad\u7ec3\u3002\u4ece\u8868\u8fbe\u6027\u7684\u89d2\u5ea6\u6765\u770b\uff0c\u5bf9\u4e8e\u5728\u4e34\u754c\u7ebf\u4e0a\u7684\u7f51\u7edc\uff0c\u4e24\u4e2a\u8f93\u5165\u4fe1\u53f7\u7684\u76f8\u5173\u4fe1\u53f7\u5728\u7f51\u7edc\u4e2d\u4e0d\u5bb9\u6613\u88ab\u5fd8\u8bb0\uff0c\u6240\u4ee5\u53ef\u4ee5\u5728\u7f51\u7edc\u5f88\u6df1\u7684\u65f6\u5019\u4fe1\u606f\u7684\u4f20\u9012\u4e5f\u80fd\u6709\u4fdd\u8bc1\u3002 \u5982\u679c\u7814\u7a76\u4fe1\u53f7\u7684\u540e\u5411\u4f20\u64ad\uff0c\u4ee5\u5e73\u5747\u573a\u7406\u8bba\u8ba1\u7b97\uff0c\u53ef\u4ee5\u5f97\u5230\u4e0e\u4e0a\u56fe\u7c7b\u4f3c\u7684\u7ed3\u8bba\uff0c\u4e14\u4e00\u81f4\u7684\u53c2\u6570\u9009\u62e9\u7ebf\u3002\u4e5f\u5c31\u662f\u5f53\u7f51\u7edc\u53c2\u6570\u5904\u5728\u4e34\u754c\u7ebf\u4e0a\u65f6\uff0c\u68af\u5ea6\u7684\u4fe1\u53f7\u4f20\u9012\u4f1a\u6bd4\u8f83\u7a33\u5b9a\uff0c\u56e0\u6b64\u66f4\u6df1\u7684\u7f51\u7edc\u4e5f\u80fd\u8bad\u7ec3\u3002","title":"Deep Information Propagation"},{"location":"The_theory/mindthepad/","text":"Mind The Pad - CNNs Can Develop Blind Spots supplementary can be downloaded and it contains codes to reproduce most of the examples. \u8fd9\u7bc7paper\u662f\u8ba8\u8bba\u5728CNN\u91cc\u9762\u4f7f\u7528Zero Padding\u7684\u95ee\u9898\u7684\u3002\u76ee\u524d(2020.Oct.08)\u5728ICRL2021 underview. \u4e0e\u8fd9\u7bc7paper\u76f8\u4f3c\u7684\u8ba8\u8bbazero padding\u7684\u672c\u7ad9\u8fd8\u6536\u5f55\u4e86 How much Position Information Do Convolutional Neural Networks Encode On Translation Invariance in CNNs: Convolutional Layers can Exploit Absolute Spatial Location \u524d\u9762\u8fd9\u4e24\u7bc7\u6587\u7ae0\u5206\u522b\u8ba8\u8bba\u7684\u662f\u8bed\u4e49\u5206\u5272\u4e2dCNN\u5982\u4f55\u611f\u77e5\u5230\u4f4d\u7f6e\u4fe1\u606f\uff0c\u4ee5\u53ca\u5206\u7c7b\u95ee\u9898\u4e2dpadding\u4ee5\u53ca\u7834\u7f3a\u7684\u5e73\u79fb\u4e0d\u53d8\u5f62\u4f1a\u5982\u4f55\u8bf1\u5bfc\u795e\u7ecf\u7f51\u7edc\u5173\u6ce8\u76f8\u5bf9\u4f4d\u7f6e\u4ea6\u6216\u662f\u7edd\u5bf9\u4f4d\u7f6e \u8fd9\u7bc7paper\u7684motivation\u662f\u4f7f\u7528object detection model\u68c0\u6d4b\u4ea4\u901a\u706f\u7684\u65f6\u5019\uff0c\u4f1a\u53d1\u73b0\u6709\u65f6\u5019\u524d\u540e\u4e24\u5f20\u56fe\u4e2d\u4ea4\u901a\u706f\u4ec5\u4ec5\u662f\u6709\u5fae\u5c0f\u7684\u5e73\u79fb\u53d8\u6362\uff0c\u9884\u6d4b\u7684confidence\u5c31\u53ef\u80fd\u4f1a\u5dee\u8ddd\u5f88\u5927\u3002\u4f5c\u8005\u521d\u59cb\u7814\u7a76\u53d1\u73b0\u8fd9\u548cCNN\u8f93\u51fa\u6709\u76f2\u533a\u6709\u5173\u3002 \u4f5c\u8005\u4ece\u6b64\u51fa\u53d1\u8fdb\u4e00\u6b65\u5206\u4eab\u4e86\u5173\u4e8e Padding \u9020\u6210\u7684\u5f71\u54cd\u7684\u5176\u4ed6\u73b0\u8c61\u4ee5\u53ca\u89e3\u51b3\u65b9\u6848\u3002 Zero Padding \u9020\u6210\u7684 Spatial Bias \u8fd9\u5f20\u56fe\u5c55\u793a\u4e86zero padding\u5f62\u6210\u7684feature map\u5728\u8fb9\u7f18\u5730\u5e26\u9020\u6210\u7684\u6a2a\u7ad6\u4e24\u4e2a\u65b9\u5411\u7684\u7279\u6b8a\u6761\u7eb9\u3002 \u4f5c\u8005\u53e6\u5916\u505a\u4e86\u7b2c\u4e8c\u4e2a\u5b9e\u9a8c\uff0c\u8f93\u5165\u4e00\u4e2a\u7531\u96f6\u7ec4\u6210\u7684\u56fe\u7247\uff0c\u53ef\u4ee5\u5728\u7279\u5f81\u56fe\u7684\u8fb9\u7f18\u4e2d\u89c2\u5bdf\u5230\u66f4\u663e\u8457\u7684artifacts\u3002 \u8fd9\u4e2aartifacts\u7684\u7279\u70b9 \u7b2c\u4e00\u5c42\u6ca1\u6709 artifacts, \u7b2c\u4e8c\u6b21\u5377\u79ef\u540e\u4ea7\u751f\u4e00\u4e2apixel\u5927\u5c0f\u7684artifacts\u3002\u8fb9\u754c\u4e0a\u7684\u7279\u5f81\u95ee\u9898\u968f\u7740\u5c42\u6570\u7684\u589e\u591a\u800c\u53d8\u5f97\u66f4\u52a0\u660e\u663e. \u56db\u4e2a\u89d2\u843d\u7684aritfacts\u662f\u4e0d\u4e00\u6837\u7684\uff0c\u8fd9\u662f\u7531\u4e8e\u5377\u79ef\u6743\u91cd\u7684\u4e0d\u5bf9\u79f0\u9020\u6210\u7684\\ (\u4e2a\u4eba\u7406\u89e3\uff0c\u4e2a\u4eba\u57283D\u68c0\u6d4b\u7684\u7279\u5f81\u56fe\u4e2d\u4e5f\u80fd\u89c2\u5bdf\u5230\u76f8\u540c\u7684\u7279\u5f81\uff0c) Spatial Bias \u5bf9\u68c0\u6d4b\u4efb\u52a1\u7684\u5f71\u54cd \u5f53\u4ea4\u901a\u706f\u5728\u8fb9\u754c\u8303\u56f4\u7684\u65f6\u5019\uff0c\u5fae\u5c0f\u7684\u5e73\u79fb\u4f1a\u663e\u8457\u5730\u5f71\u54cd\u5206\u7c7bscore. Eliminating Uneven padding \u8fd9\u91cc\u63d2\u64ad\u63d0\u5230\u4e86\u5728stride=2\u7684conv\u4e2d\uff0cpadding\u7684\u5f71\u54cd\u3002 \u6211\u4eec\u5df2\u7ecf\u77e5\u9053padding\u4f1a\u9020\u6210 forward checkerboard \u3002\u4f5c\u8005\u53c8\u63d0\u51fa\u8fd9\u4e2apadding\u4f1a\u4ec5\u5728\u5de6\u4e0a\u65b9\u88ab\u4f7f\u7528\uff0cpadding\u5728\u540e\u7eedconv\u4e0a\u7684\u5730\u4f4d\u4f1a\u6709\u504f\u5dee\u3002\u4e5f\u4f1a\u5f15\u8d77conv kernel\u7684\u4e0d\u5bf9\u79f0 \u4f5c\u8005\u6307\u51fa\u9700\u8981\u4fdd\u8bc1\u56fe\u7247\u7684\u957f\u5bbd\u4e3a 2^d + 1 \u624d\u80fd\u62b5\u6297\u8fd9\u4e2apadding\u7684\u5f71\u54cd\u3002 \u4f5c\u8005\u7528ImageNet\u5b9e\u9a8c\u53d1\u73b0\u8f93\u5165\u56fe\u7247\u4ece 224\\times 224 \u53d8\u4e3a 225 \\times 225 \u4f1a\u5e73\u5747\u5e26\u6765 0.4\\% \u7684Top-1\u63d0\u5347 Foveation \u6709\u6548\u611f\u53d7\u91ce\u5206\u6790\uff0c\u8ba1\u7b97\u6bcf\u4e00\u4e2a\u8f93\u5165feature\u4f1a\u5f71\u54cd\u591a\u5c11\u4e2a\u8f93\u51fa\u50cf\u7d20\u3002 \u5728Same padding\u7684setting\u4e0b\uff0c\u4f5c\u8005\u53d1\u73b0Symmetric\u7684padding mode\u53ef\u4ee5\u5904\u7406\u6709\u6548\u611f\u53d7\u91ce\u4e0d\u5e73\u8861\u7684\u95ee\u9898(Pytorch \u4e0e\u4e4b\u5bf9\u5e94\uff0c\u5728padding=1\u7684\u60c5\u51b5\u4e0b\u76f8\u4f3c\u7684\u66ff\u4ee3\u4e3a ReplicationPad) \u4f5c\u8005\u540e\u9762\u7528Mirror Padding \u4ee3\u66ffZero Padding\uff0c\u5728SSD \u8def\u706f\u68c0\u6d4b\u5668\u4e0a\u5f97\u5230\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u4e14\u6d88\u9664\u4e86feature map\u4e0a\u7684artifacts","title":"Mind The Pad - CNNs Can Develop Blind Spots"},{"location":"The_theory/mindthepad/#mind-the-pad-cnns-can-develop-blind-spots","text":"supplementary can be downloaded and it contains codes to reproduce most of the examples. \u8fd9\u7bc7paper\u662f\u8ba8\u8bba\u5728CNN\u91cc\u9762\u4f7f\u7528Zero Padding\u7684\u95ee\u9898\u7684\u3002\u76ee\u524d(2020.Oct.08)\u5728ICRL2021 underview. \u4e0e\u8fd9\u7bc7paper\u76f8\u4f3c\u7684\u8ba8\u8bbazero padding\u7684\u672c\u7ad9\u8fd8\u6536\u5f55\u4e86 How much Position Information Do Convolutional Neural Networks Encode On Translation Invariance in CNNs: Convolutional Layers can Exploit Absolute Spatial Location \u524d\u9762\u8fd9\u4e24\u7bc7\u6587\u7ae0\u5206\u522b\u8ba8\u8bba\u7684\u662f\u8bed\u4e49\u5206\u5272\u4e2dCNN\u5982\u4f55\u611f\u77e5\u5230\u4f4d\u7f6e\u4fe1\u606f\uff0c\u4ee5\u53ca\u5206\u7c7b\u95ee\u9898\u4e2dpadding\u4ee5\u53ca\u7834\u7f3a\u7684\u5e73\u79fb\u4e0d\u53d8\u5f62\u4f1a\u5982\u4f55\u8bf1\u5bfc\u795e\u7ecf\u7f51\u7edc\u5173\u6ce8\u76f8\u5bf9\u4f4d\u7f6e\u4ea6\u6216\u662f\u7edd\u5bf9\u4f4d\u7f6e \u8fd9\u7bc7paper\u7684motivation\u662f\u4f7f\u7528object detection model\u68c0\u6d4b\u4ea4\u901a\u706f\u7684\u65f6\u5019\uff0c\u4f1a\u53d1\u73b0\u6709\u65f6\u5019\u524d\u540e\u4e24\u5f20\u56fe\u4e2d\u4ea4\u901a\u706f\u4ec5\u4ec5\u662f\u6709\u5fae\u5c0f\u7684\u5e73\u79fb\u53d8\u6362\uff0c\u9884\u6d4b\u7684confidence\u5c31\u53ef\u80fd\u4f1a\u5dee\u8ddd\u5f88\u5927\u3002\u4f5c\u8005\u521d\u59cb\u7814\u7a76\u53d1\u73b0\u8fd9\u548cCNN\u8f93\u51fa\u6709\u76f2\u533a\u6709\u5173\u3002 \u4f5c\u8005\u4ece\u6b64\u51fa\u53d1\u8fdb\u4e00\u6b65\u5206\u4eab\u4e86\u5173\u4e8e Padding \u9020\u6210\u7684\u5f71\u54cd\u7684\u5176\u4ed6\u73b0\u8c61\u4ee5\u53ca\u89e3\u51b3\u65b9\u6848\u3002","title":"Mind The Pad - CNNs Can Develop Blind Spots"},{"location":"The_theory/mindthepad/#zero-padding-spatial-bias","text":"\u8fd9\u5f20\u56fe\u5c55\u793a\u4e86zero padding\u5f62\u6210\u7684feature map\u5728\u8fb9\u7f18\u5730\u5e26\u9020\u6210\u7684\u6a2a\u7ad6\u4e24\u4e2a\u65b9\u5411\u7684\u7279\u6b8a\u6761\u7eb9\u3002 \u4f5c\u8005\u53e6\u5916\u505a\u4e86\u7b2c\u4e8c\u4e2a\u5b9e\u9a8c\uff0c\u8f93\u5165\u4e00\u4e2a\u7531\u96f6\u7ec4\u6210\u7684\u56fe\u7247\uff0c\u53ef\u4ee5\u5728\u7279\u5f81\u56fe\u7684\u8fb9\u7f18\u4e2d\u89c2\u5bdf\u5230\u66f4\u663e\u8457\u7684artifacts\u3002 \u8fd9\u4e2aartifacts\u7684\u7279\u70b9 \u7b2c\u4e00\u5c42\u6ca1\u6709 artifacts, \u7b2c\u4e8c\u6b21\u5377\u79ef\u540e\u4ea7\u751f\u4e00\u4e2apixel\u5927\u5c0f\u7684artifacts\u3002\u8fb9\u754c\u4e0a\u7684\u7279\u5f81\u95ee\u9898\u968f\u7740\u5c42\u6570\u7684\u589e\u591a\u800c\u53d8\u5f97\u66f4\u52a0\u660e\u663e. \u56db\u4e2a\u89d2\u843d\u7684aritfacts\u662f\u4e0d\u4e00\u6837\u7684\uff0c\u8fd9\u662f\u7531\u4e8e\u5377\u79ef\u6743\u91cd\u7684\u4e0d\u5bf9\u79f0\u9020\u6210\u7684\\ (\u4e2a\u4eba\u7406\u89e3\uff0c\u4e2a\u4eba\u57283D\u68c0\u6d4b\u7684\u7279\u5f81\u56fe\u4e2d\u4e5f\u80fd\u89c2\u5bdf\u5230\u76f8\u540c\u7684\u7279\u5f81\uff0c)","title":"Zero Padding \u9020\u6210\u7684 Spatial Bias"},{"location":"The_theory/mindthepad/#spatial-bias","text":"\u5f53\u4ea4\u901a\u706f\u5728\u8fb9\u754c\u8303\u56f4\u7684\u65f6\u5019\uff0c\u5fae\u5c0f\u7684\u5e73\u79fb\u4f1a\u663e\u8457\u5730\u5f71\u54cd\u5206\u7c7bscore.","title":"Spatial Bias \u5bf9\u68c0\u6d4b\u4efb\u52a1\u7684\u5f71\u54cd"},{"location":"The_theory/mindthepad/#eliminating-uneven-padding","text":"\u8fd9\u91cc\u63d2\u64ad\u63d0\u5230\u4e86\u5728stride=2\u7684conv\u4e2d\uff0cpadding\u7684\u5f71\u54cd\u3002 \u6211\u4eec\u5df2\u7ecf\u77e5\u9053padding\u4f1a\u9020\u6210 forward checkerboard \u3002\u4f5c\u8005\u53c8\u63d0\u51fa\u8fd9\u4e2apadding\u4f1a\u4ec5\u5728\u5de6\u4e0a\u65b9\u88ab\u4f7f\u7528\uff0cpadding\u5728\u540e\u7eedconv\u4e0a\u7684\u5730\u4f4d\u4f1a\u6709\u504f\u5dee\u3002\u4e5f\u4f1a\u5f15\u8d77conv kernel\u7684\u4e0d\u5bf9\u79f0 \u4f5c\u8005\u6307\u51fa\u9700\u8981\u4fdd\u8bc1\u56fe\u7247\u7684\u957f\u5bbd\u4e3a 2^d + 1 \u624d\u80fd\u62b5\u6297\u8fd9\u4e2apadding\u7684\u5f71\u54cd\u3002 \u4f5c\u8005\u7528ImageNet\u5b9e\u9a8c\u53d1\u73b0\u8f93\u5165\u56fe\u7247\u4ece 224\\times 224 \u53d8\u4e3a 225 \\times 225 \u4f1a\u5e73\u5747\u5e26\u6765 0.4\\% \u7684Top-1\u63d0\u5347","title":"Eliminating Uneven padding"},{"location":"The_theory/mindthepad/#foveation","text":"\u6709\u6548\u611f\u53d7\u91ce\u5206\u6790\uff0c\u8ba1\u7b97\u6bcf\u4e00\u4e2a\u8f93\u5165feature\u4f1a\u5f71\u54cd\u591a\u5c11\u4e2a\u8f93\u51fa\u50cf\u7d20\u3002 \u5728Same padding\u7684setting\u4e0b\uff0c\u4f5c\u8005\u53d1\u73b0Symmetric\u7684padding mode\u53ef\u4ee5\u5904\u7406\u6709\u6548\u611f\u53d7\u91ce\u4e0d\u5e73\u8861\u7684\u95ee\u9898(Pytorch \u4e0e\u4e4b\u5bf9\u5e94\uff0c\u5728padding=1\u7684\u60c5\u51b5\u4e0b\u76f8\u4f3c\u7684\u66ff\u4ee3\u4e3a ReplicationPad) \u4f5c\u8005\u540e\u9762\u7528Mirror Padding \u4ee3\u66ffZero Padding\uff0c\u5728SSD \u8def\u706f\u68c0\u6d4b\u5668\u4e0a\u5f97\u5230\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u4e14\u6d88\u9664\u4e86feature map\u4e0a\u7684artifacts","title":"Foveation"},{"location":"The_theory/momentum_BN/","text":"Momentum Batch Normalization for Deep Learning with Small Batch Size \u8fd9\u7bc7paper\u5c1d\u8bd5\u63a8\u5bfc\u5730\u8bc1\u660e Batch Normalization \u5728\u8bad\u7ec3\u65f6\u4f1a\u5f15\u5165\u4e00\u5b9a\u7684\u566a\u58f0\uff0c\u4e14\u8fd9\u4e2a\u566a\u58f0\u7684\u65b9\u5dee\u4e0e Batch Size \u6210\u53cd\u6bd4.\u7136\u540e\u672c\u6587\u63d0\u51fa\u4e86Momentum BN\u6765\u5e73\u8861\u3002 \u76f8\u4f3c\u7684\u5e0c\u671b\u5728\u4f4e Batch\u4e0b\u964d\u4f4e\u566a\u58f0\u7684\u65b9\u6848\u672c\u7ad9\u6536\u5f55\u6709 Cross Iteration BN .\u672c\u6587\u63d0\u51fa\u7684 Momentum BN\u76f8\u8f83\u800c\u8a00\u66f4\u7b80\u5355\uff0c\u7406\u8bba\u5206\u6790\u4e0a\u5404\u6709\u4f18\u52a3\u3002 Noise in BN \u5047\u8bbe\u8f93\u5165\u4fe1\u53f7\u7b26\u5408\u67d0\u4e00\u6b63\u592a\u5206\u5e03 ~ \\mathcal{N}(\\mu, \\sigma^2) \uff0c\u90a3\u4e48\u5927\u5c0f\u4e3a m \u7684mini-batch\u5219\u53ef\u4ee5\u7406\u89e3\u4e3a\u5bf9\u8fd9\u4e00\u6b63\u592a\u5206\u5e03\u8fdb\u884c\u91c7\u6837\u3002\u91c7\u6837\u6570\u636e\u5bf9\u539f\u6b63\u592a\u6570\u636e\u7684\u7edf\u8ba1\u7279\u5f81\u8fdb\u884c\u4f30\u8ba1\u65f6\uff0c\u5176\u7ed3\u679c\u7684\u5747\u503c \\mu_B = \\frac{1}{m} \\sum^m_{i=1} x_i \u4e5f\u6ee1\u8db3\u4e00\u4e2a\u6b63\u592a\u5206\u5e03\uff0c\u800c\u5176\u7ed3\u679c\u7684\u65b9\u5dee \\sigma_B^2 = \\frac{1}{m}\\sum^m_{i=1}(x_i - \\mu_B)^2 \u4e5f\u6ee1\u8db3\u4e00\u4e2a\u5361\u65b9\u5206\u5e03. \u53ef\u4ee5\u5f97\u5230\u4ee5\u4e0b\u4e24\u4e2a\u968f\u673a\u53d8\u91cf \\xi_{\\mu}=\\frac{\\mu-\\mu_{B}}{\\sigma} \\sim \\mathcal{N}\\left(0, \\frac{1}{m}\\right), \\quad \\xi_{\\sigma}=\\frac{\\sigma_{B}^{2}}{\\sigma^{2}} \\sim \\frac{1}{m} \\chi^{2}(m-1) \u90a3\u4e48\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0cBN\u7684\u5f52\u4e00\u5316\u8fd0\u7b97\u53ef\u4ee5\u91cd\u65b0\u5199\u4e3a: \\widehat{x}=\\frac{x-\\mu_{B}}{\\sigma_{B}}=\\frac{x-\\mu+\\left(\\mu-\\mu_{B}\\right)}{\\sigma \\frac{\\sigma_{B}}{\\sigma}}=\\frac{\\frac{x-\\mu}{\\sigma}+\\xi_{\\mu}}{\\sqrt{\\xi_{\\sigma}}}=\\frac{\\widetilde{x}+\\xi_{\\mu}}{\\sqrt{\\xi_{\\sigma}}} \u5728\u6982\u5ff5\u4e0a\uff0c \\tilde{x} \u4e3a\u539f\u6570\u636e\u7684\u771f\u5b9e\u5f52\u4e00\u5316\u503c\uff0c \\xi_{\\mu} \u4e3a\u52a0\u6cd5\u9879\u7684\u9ad8\u65af\u566a\u58f0\uff0c\u800c \\xi_{\\sigma} \u4e3a\u4e58\u6cd5\u9879\u7684\u9ad8\u65af\u566a\u58f0\u3002 Reduce Noise for BN with Small Batches \\mu_{M}^{(n)}=\\lambda \\mu_{M}^{(n-1)}+(1-\\lambda) \\mu_{B}, \\quad\\left(\\sigma_{M}^{(n)}\\right)^{2}=\\lambda\\left(\\sigma_{M}^{(n-1)}\\right)^{2}+(1-\\lambda) \\sigma_{B}^{2} \u7b97\u6cd5\u4e0a\u6bd4\u8f83\u76f4\u767d\uff0c\u5bf9\u5747\u503c\u4e0e\u65b9\u5dee\u8ba1\u7b97\u4e00\u4e2a\u6ed1\u52a8\u5e73\u5747\u3002\u4e0d\u96be\u8ba1\u7b97\u6ed1\u52a8\u5e73\u5747\u540e\u5f52\u4e00\u5316\u8fd0\u7b97\u7684\u9ad8\u65af\u566a\u58f0\u4e5f\u4f1a\u5e73\u6ed1\u800c\u4e0b\u964d\u3002 \\xi_{\\mu}=\\frac{\\mu-\\mu_{M}}{\\sigma} \\sim \\mathcal{N}\\left(0, \\frac{1-\\lambda}{m}\\right) \u8fdb\u4e00\u6b65\u5730\uff0c\u4f5c\u8005\u6307\u51fa\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u8c03\u6574 \\lambda \uff0c\u4f7fBN\u7684\u566a\u58f0\u7b49\u6548\u4e8e\u6307\u5b9a\u7684Batch Size\u65f6\u7684\u566a\u58f0\u3002 \u672c\u6587\u4e0e Cross Iteration BN \u7684\u533a\u522b: \u4f18\u70b9\uff1a - \u53ef\u4ee5\u901a\u8fc7\u8c03\u8282 \\lambda \u6307\u5b9a\u5bf9\u5e94\u7684batch size - \u8ba1\u7b97\u8fc7\u7a0b\u7b80\u5355,\u5feb\u901f. \u7f3a\u70b9: - \u6ca1\u6709\u8003\u8651\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7531\u4e8e\u6743\u91cd\u53d8\u5316\u5e26\u6765\u7684\u7edf\u8ba1\u7279\u5f81\u6f02\u79fb","title":"Momentum Batch Normalization for Deep Learning with Small Batch Size"},{"location":"The_theory/momentum_BN/#momentum-batch-normalization-for-deep-learning-with-small-batch-size","text":"\u8fd9\u7bc7paper\u5c1d\u8bd5\u63a8\u5bfc\u5730\u8bc1\u660e Batch Normalization \u5728\u8bad\u7ec3\u65f6\u4f1a\u5f15\u5165\u4e00\u5b9a\u7684\u566a\u58f0\uff0c\u4e14\u8fd9\u4e2a\u566a\u58f0\u7684\u65b9\u5dee\u4e0e Batch Size \u6210\u53cd\u6bd4.\u7136\u540e\u672c\u6587\u63d0\u51fa\u4e86Momentum BN\u6765\u5e73\u8861\u3002 \u76f8\u4f3c\u7684\u5e0c\u671b\u5728\u4f4e Batch\u4e0b\u964d\u4f4e\u566a\u58f0\u7684\u65b9\u6848\u672c\u7ad9\u6536\u5f55\u6709 Cross Iteration BN .\u672c\u6587\u63d0\u51fa\u7684 Momentum BN\u76f8\u8f83\u800c\u8a00\u66f4\u7b80\u5355\uff0c\u7406\u8bba\u5206\u6790\u4e0a\u5404\u6709\u4f18\u52a3\u3002","title":"Momentum Batch Normalization for Deep Learning with Small Batch Size"},{"location":"The_theory/momentum_BN/#noise-in-bn","text":"\u5047\u8bbe\u8f93\u5165\u4fe1\u53f7\u7b26\u5408\u67d0\u4e00\u6b63\u592a\u5206\u5e03 ~ \\mathcal{N}(\\mu, \\sigma^2) \uff0c\u90a3\u4e48\u5927\u5c0f\u4e3a m \u7684mini-batch\u5219\u53ef\u4ee5\u7406\u89e3\u4e3a\u5bf9\u8fd9\u4e00\u6b63\u592a\u5206\u5e03\u8fdb\u884c\u91c7\u6837\u3002\u91c7\u6837\u6570\u636e\u5bf9\u539f\u6b63\u592a\u6570\u636e\u7684\u7edf\u8ba1\u7279\u5f81\u8fdb\u884c\u4f30\u8ba1\u65f6\uff0c\u5176\u7ed3\u679c\u7684\u5747\u503c \\mu_B = \\frac{1}{m} \\sum^m_{i=1} x_i \u4e5f\u6ee1\u8db3\u4e00\u4e2a\u6b63\u592a\u5206\u5e03\uff0c\u800c\u5176\u7ed3\u679c\u7684\u65b9\u5dee \\sigma_B^2 = \\frac{1}{m}\\sum^m_{i=1}(x_i - \\mu_B)^2 \u4e5f\u6ee1\u8db3\u4e00\u4e2a\u5361\u65b9\u5206\u5e03. \u53ef\u4ee5\u5f97\u5230\u4ee5\u4e0b\u4e24\u4e2a\u968f\u673a\u53d8\u91cf \\xi_{\\mu}=\\frac{\\mu-\\mu_{B}}{\\sigma} \\sim \\mathcal{N}\\left(0, \\frac{1}{m}\\right), \\quad \\xi_{\\sigma}=\\frac{\\sigma_{B}^{2}}{\\sigma^{2}} \\sim \\frac{1}{m} \\chi^{2}(m-1) \u90a3\u4e48\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0cBN\u7684\u5f52\u4e00\u5316\u8fd0\u7b97\u53ef\u4ee5\u91cd\u65b0\u5199\u4e3a: \\widehat{x}=\\frac{x-\\mu_{B}}{\\sigma_{B}}=\\frac{x-\\mu+\\left(\\mu-\\mu_{B}\\right)}{\\sigma \\frac{\\sigma_{B}}{\\sigma}}=\\frac{\\frac{x-\\mu}{\\sigma}+\\xi_{\\mu}}{\\sqrt{\\xi_{\\sigma}}}=\\frac{\\widetilde{x}+\\xi_{\\mu}}{\\sqrt{\\xi_{\\sigma}}} \u5728\u6982\u5ff5\u4e0a\uff0c \\tilde{x} \u4e3a\u539f\u6570\u636e\u7684\u771f\u5b9e\u5f52\u4e00\u5316\u503c\uff0c \\xi_{\\mu} \u4e3a\u52a0\u6cd5\u9879\u7684\u9ad8\u65af\u566a\u58f0\uff0c\u800c \\xi_{\\sigma} \u4e3a\u4e58\u6cd5\u9879\u7684\u9ad8\u65af\u566a\u58f0\u3002","title":"Noise in BN"},{"location":"The_theory/momentum_BN/#reduce-noise-for-bn-with-small-batches","text":"\\mu_{M}^{(n)}=\\lambda \\mu_{M}^{(n-1)}+(1-\\lambda) \\mu_{B}, \\quad\\left(\\sigma_{M}^{(n)}\\right)^{2}=\\lambda\\left(\\sigma_{M}^{(n-1)}\\right)^{2}+(1-\\lambda) \\sigma_{B}^{2} \u7b97\u6cd5\u4e0a\u6bd4\u8f83\u76f4\u767d\uff0c\u5bf9\u5747\u503c\u4e0e\u65b9\u5dee\u8ba1\u7b97\u4e00\u4e2a\u6ed1\u52a8\u5e73\u5747\u3002\u4e0d\u96be\u8ba1\u7b97\u6ed1\u52a8\u5e73\u5747\u540e\u5f52\u4e00\u5316\u8fd0\u7b97\u7684\u9ad8\u65af\u566a\u58f0\u4e5f\u4f1a\u5e73\u6ed1\u800c\u4e0b\u964d\u3002 \\xi_{\\mu}=\\frac{\\mu-\\mu_{M}}{\\sigma} \\sim \\mathcal{N}\\left(0, \\frac{1-\\lambda}{m}\\right) \u8fdb\u4e00\u6b65\u5730\uff0c\u4f5c\u8005\u6307\u51fa\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u8c03\u6574 \\lambda \uff0c\u4f7fBN\u7684\u566a\u58f0\u7b49\u6548\u4e8e\u6307\u5b9a\u7684Batch Size\u65f6\u7684\u566a\u58f0\u3002 \u672c\u6587\u4e0e Cross Iteration BN \u7684\u533a\u522b: \u4f18\u70b9\uff1a - \u53ef\u4ee5\u901a\u8fc7\u8c03\u8282 \\lambda \u6307\u5b9a\u5bf9\u5e94\u7684batch size - \u8ba1\u7b97\u8fc7\u7a0b\u7b80\u5355,\u5feb\u901f. \u7f3a\u70b9: - \u6ca1\u6709\u8003\u8651\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7531\u4e8e\u6743\u91cd\u53d8\u5316\u5e26\u6765\u7684\u7edf\u8ba1\u7279\u5f81\u6f02\u79fb","title":"Reduce Noise for BN with Small Batches"},{"location":"The_theory/understandingKD/","text":"Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning \u8fd9\u7bc7paper\u6709\u5fae\u8f6f\u7814\u7a76\u9662\u7684\u4e00\u4e2a blog \u4f5c\u4e3a\u4ecb\u7ecd. \u7814\u7a76\u7684\u4e3b\u8981\u95ee\u9898\u4ee5\u53ca\u5207\u5165\u70b9\u662f\u4e3a\u4ec0\u4e48\u628a\u7f51\u7edcensemble\u8d77\u6765\uff0c\u751a\u81f3\u4f7f\u7528\u81ea\u84b8\u998f\uff0c\u5c31\u80fd\u63d0\u5347\u7f51\u7edc\u7684\u6d4b\u8bd5\u6027\u80fd\u3002 \u57fa\u7840\u7ed3\u8bba \u6587\u7ae0\u901a\u8fc7\u7cbe\u5de7\u5730\u8bbe\u8ba1\u5b9e\u9a8c\u5f97\u5230\u4e86\u6570\u4e2a\u6709\u610f\u601d\u5730\u7ed3\u8bba\u3002 Conclusions Simple Ideas \u5728\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u6a21\u578b\u7ec4\u5408\u4e0e\u4f20\u7edf\u65b9\u6cd5(\u7c7b\u4f3c NTK \u65b9\u6cd5)\u5f88\u4e0d\u540c NTK\u8fd9\u79cd\u7c7b\u4f3c\u7ebf\u6027\u7684\u6a21\u578b\u5728\u6a21\u578bensemble\u65f6\u4e0d\u4f1a\u6709\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u56e0\u800c\u7f51\u7edc\u7684\u5b66\u4e60\u673a\u5236\u548cNTK\u8fd8\u662f\u6709\u5f88\u5927\u533a\u522b,\u7f51\u7edc\u4e0d\u53ea\u662f\u5728\u968f\u673a\u6743\u91cd\u4e2d\u5bf9\u7279\u5f81\u8fdb\u884c\u9009\u62e9\uff0c\u800c\u662f\u5728\u8bad\u7ec3\u4e2d\u771f\u7684\u6709\u4e3a\u6570\u636e\u63d0\u53d6\u65b0\u7684\u7279\u5f81 \u7f51\u7edc\u7684ensemble\u4e0d\u53ea\u662f\u964d\u4f4e\u4e86\u9884\u6d4b\u7684variance\u63d0\u9ad8\u4fe1\u5fc3 \u7f51\u7edc\u5728\u56fe\u50cf\u5206\u7c7btraining set\u90fd\u662f\u51e0\u4e4e\u65e0\u635f\u5931\u7684\uff0c\u4f46\u662f\u4ecd\u7136\u80fd\u6709\u6d4b\u8bd5\u51c6\u786e\u5ea6\u63d0\u5347\u3002 \u4f46\u662f\u5982\u679c\u6570\u636e\u96c6\u662f\u9ad8\u65af\u91c7\u6837\u7684\u57fa\u7840\u6570\u636e\u96c6\uff0c\u5219\u6ca1\u6709\u8fd9\u6837\u7684\u6548\u679c\u3002\u662f\u56fe\u50cf\u6570\u636e\u96c6\u4e2d\u67d0\u4e9b\u7279\u5b9a\u7684\u5206\u5e03\u7279\u6027\u9020\u6210\u7684\u3002 hard label\u5bf9ensemble\u5f88\u91cd\u8981\uff0c\u751a\u81f3\u662f\u5fc5\u8981 \u5982\u679c\u4f7f\u7528KD \u4e5f\u5c31\u662f\u8f6flabel\u6765\u8bad\u7ec3\u5355\u4e2a\u7f51\u7edc\uff0c\u90a3\u4e48\u8fd9\u4e9b\u7f51\u7edc\u7684\u7ec4\u5408\u4e0d\u4f1a\u5e26\u6765\u6d4b\u8bd5\u51c6\u786e\u5ea6\u63d0\u5347 Multi view Data \u672c\u6587\u63d0\u51famulti-view data\u8fd9\u4e2a\u8bf4\u6cd5\u6765justify\u4e3a\u4ec0\u4e48ensemble\u4ee5\u53caKD\u6709\u7528.\u5728\u56fe\u7247\u4e0a\u6765\u8bf4\u5c31\u662f\u540c\u4e00\u4e2a\u5206\u7c7b\u6211\u4eec\u53ef\u4ee5\u6709\u591a\u79cd\u7279\u5f81\uff0c\u591a\u79cd\u89c6\u89d2\u6765\u6210\u529f\u5206\u7c7b\u3002Ensemble\u4e4b\u540e\u7684\u6a21\u578b\u5c31\u6709\u4e86\u4ece\u591a\u4e2a\u89c6\u89d2\uff0c\u591a\u4e2a\u9014\u5f84\u8fdb\u884c\u5206\u7c7b\u7684\u80fd\u529b\uff0c\u6240\u4ee5\u63d0\u5347\u4e86\u51c6\u786e\u5ea6\u3002 \u8fd9\u4e2a\u8bf4\u6cd5\u521d\u542c\u4f3c\u4e4e\u662f\u5728\u6df1\u5ea6\u5b66\u4e60\u6bd4\u8f83\u65e9\u671f\u5c31\u5df2\u7ecf\u6709\u7684\u8bf4\u6cd5\uff0c\u4f46\u662f\u672c\u6587\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7b80\u5355\u7684\u6570\u636e\u96c6\uff0c\u5177\u4f53\u5730\u523b\u753b\u4e86\u8fd9\u4e2amulti-view data\u7684\u8868\u73b0\uff0c\u5e76\u7528\u7b80\u5355\u7684\u7f51\u7edc\u5728\u8fd9\u4e2a\u7b80\u5355\u7684\u6570\u636e\u96c6\u4e0a\u590d\u73b0\u4e86\u4e0a\u8ff0\u7684\u8868\u73b0\u3002 Multi-view Data \u7684\u4eba\u5de5\u6784\u5efa\u3002 \u6570\u636e\u96c6\u5305\u542b\u56db\u4e2a\u7279\u5f81 v_1, v_2, v_3, v_4 \uff0c\u662f\u4e00\u4e2a\u4e8c\u5206\u7c7b\u95ee\u9898\u3002 label == 1 - 80%: v_1, v_2 \u90fd\u662f\u63a5\u8fd11\uff0c\u800c v_3, v_4 \u5176\u4e2d\u4e4b\u4e00\u662f0.1 - 10%: v_1 \u90fd\u662f\u63a5\u8fd11\uff0c\u800c v_3, v_4 \u5176\u4e2d\u4e4b\u4e00\u662f0.1 - 10%: v_2 \u90fd\u662f\u63a5\u8fd11\uff0c\u800c v_3, v_4 \u5176\u4e2d\u4e4b\u4e00\u662f0.1 label == 2 - 80%: v_3, v_4 \u90fd\u662f\u63a5\u8fd11\uff0c\u800c v_1, v_2 \u5176\u4e2d\u4e4b\u4e00\u662f0.1 - 10%: v_3 \u90fd\u662f\u63a5\u8fd11\uff0c\u800c v_1, v_2 \u5176\u4e2d\u4e4b\u4e00\u662f0.1 - 10%: v_4 \u90fd\u662f\u63a5\u8fd11\uff0c\u800c v_1, v_2 \u5176\u4e2d\u4e4b\u4e00\u662f0.1 \u5176\u4e2d 80%\u7684\u6570\u636e\u88ab\u79f0\u4e3a multi-view data. \u56e0\u4e3a\u4ed6\u4eec\u6709\u4e0d\u6b62\u4e00\u79cd\u65b9\u5f0f\u5206\u7c7b\u3002\u800c \u5269\u4e0b\u7684\u662f single-view data. \u6bcf\u4e2a\u7f51\u7edc\u7684\u5b66\u4e60\u7279\u6027, \u878d\u5408\uff0c \u84b8\u998f \u4f5c\u8005\u53d1\u73b0\uff0c\u7f51\u7edc\u4f1a\u5728 v_1, v_2 \u4e2d\u9009\u62e9\u4e00\u4e2a\u7279\u5f81\u6765\u6b63\u786e\u5206\u7c7blabel==1\uff0c\u4ee5\u53ca v_3, v_4 \u4e2d\u9009\u62e9\u4e00\u4e2a\u6765\u6b63\u786e\u5206\u7c7blabel==2. \u56e0\u6b6490%\u7684\u6570\u636e\u4f1a\u5f88\u5feb\u5730\u5206\u7c7b\u6210\u529f\u3002\u8bad\u7ec3\u4e00\u6bb5\u65f6\u95f4\u540e\uff0c\u8fd990%\u7684\u6570\u636e\u5c31\u4e0d\u518d\u63d0\u4f9b\u68af\u5ea6\u4e86\u3002 \u63a5\u7740\u7f51\u7edc\u4f1a\u5c1d\u8bd5\u8bb0\u5fc6\u5269\u4f59\u768410%\u8bad\u7ec3\u6837\u672c\uff0c\u4f46\u662f\u662f\u4f9d\u9760\u7f51\u7edc\u7684\u566a\u97f3\u8bb0\u5fc6\u80fd\u529b\u800c\u4e0d\u662f\u771f\u6b63\u7684\u627e\u5230\u4e86\u65b0\u7684\u7279\u5f81\u4f9d\u636e\u3002 test set\u7684\u51c6\u786e\u7387\u5c31\u5361\u572890\u5de6\u53f3 \u6a21\u578bensemble\u4e4b\u540e\uff0c\u7531\u4e8e\u968f\u673a\u521d\u59cb\u5316\u7684\u7f51\u7edc\u662f\u968f\u673a\u9009\u62e9\u4e3b\u7279\u5f81\u7684\uff0c\u56e0\u800c\u6709\u9650\u4e2a\u6a21\u578b\u7684\u878d\u5408\uff0c\u5c31\u53ef\u4ee5\u51d1\u9f50\u6240\u4ee5\u9700\u8981\u7684\u7279\u5f81\uff0ctest set\u51c6\u786e\u7387\u4e0a\u5230100%\u3002 \u800cKD\u7684\u65f6\u5019\u7531\u4e8e\u6211\u4eec\u63d0\u4f9b\u7684\u662f\u8f6flabel\uff0c\u6240\u4ee5\u7f51\u7edc\u5b9e\u9645\u4e0a\u662f\u88ab\u5f3a\u884c\u8981\u6c42\u5b66\u4e60multi-view.","title":"Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning"},{"location":"The_theory/understandingKD/#towards-understanding-ensemble-knowledge-distillation-and-self-distillation-in-deep-learning","text":"\u8fd9\u7bc7paper\u6709\u5fae\u8f6f\u7814\u7a76\u9662\u7684\u4e00\u4e2a blog \u4f5c\u4e3a\u4ecb\u7ecd. \u7814\u7a76\u7684\u4e3b\u8981\u95ee\u9898\u4ee5\u53ca\u5207\u5165\u70b9\u662f\u4e3a\u4ec0\u4e48\u628a\u7f51\u7edcensemble\u8d77\u6765\uff0c\u751a\u81f3\u4f7f\u7528\u81ea\u84b8\u998f\uff0c\u5c31\u80fd\u63d0\u5347\u7f51\u7edc\u7684\u6d4b\u8bd5\u6027\u80fd\u3002","title":"Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning"},{"location":"The_theory/understandingKD/#_1","text":"\u6587\u7ae0\u901a\u8fc7\u7cbe\u5de7\u5730\u8bbe\u8ba1\u5b9e\u9a8c\u5f97\u5230\u4e86\u6570\u4e2a\u6709\u610f\u601d\u5730\u7ed3\u8bba\u3002 Conclusions Simple Ideas \u5728\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u6a21\u578b\u7ec4\u5408\u4e0e\u4f20\u7edf\u65b9\u6cd5(\u7c7b\u4f3c NTK \u65b9\u6cd5)\u5f88\u4e0d\u540c NTK\u8fd9\u79cd\u7c7b\u4f3c\u7ebf\u6027\u7684\u6a21\u578b\u5728\u6a21\u578bensemble\u65f6\u4e0d\u4f1a\u6709\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u56e0\u800c\u7f51\u7edc\u7684\u5b66\u4e60\u673a\u5236\u548cNTK\u8fd8\u662f\u6709\u5f88\u5927\u533a\u522b,\u7f51\u7edc\u4e0d\u53ea\u662f\u5728\u968f\u673a\u6743\u91cd\u4e2d\u5bf9\u7279\u5f81\u8fdb\u884c\u9009\u62e9\uff0c\u800c\u662f\u5728\u8bad\u7ec3\u4e2d\u771f\u7684\u6709\u4e3a\u6570\u636e\u63d0\u53d6\u65b0\u7684\u7279\u5f81 \u7f51\u7edc\u7684ensemble\u4e0d\u53ea\u662f\u964d\u4f4e\u4e86\u9884\u6d4b\u7684variance\u63d0\u9ad8\u4fe1\u5fc3 \u7f51\u7edc\u5728\u56fe\u50cf\u5206\u7c7btraining set\u90fd\u662f\u51e0\u4e4e\u65e0\u635f\u5931\u7684\uff0c\u4f46\u662f\u4ecd\u7136\u80fd\u6709\u6d4b\u8bd5\u51c6\u786e\u5ea6\u63d0\u5347\u3002 \u4f46\u662f\u5982\u679c\u6570\u636e\u96c6\u662f\u9ad8\u65af\u91c7\u6837\u7684\u57fa\u7840\u6570\u636e\u96c6\uff0c\u5219\u6ca1\u6709\u8fd9\u6837\u7684\u6548\u679c\u3002\u662f\u56fe\u50cf\u6570\u636e\u96c6\u4e2d\u67d0\u4e9b\u7279\u5b9a\u7684\u5206\u5e03\u7279\u6027\u9020\u6210\u7684\u3002 hard label\u5bf9ensemble\u5f88\u91cd\u8981\uff0c\u751a\u81f3\u662f\u5fc5\u8981 \u5982\u679c\u4f7f\u7528KD \u4e5f\u5c31\u662f\u8f6flabel\u6765\u8bad\u7ec3\u5355\u4e2a\u7f51\u7edc\uff0c\u90a3\u4e48\u8fd9\u4e9b\u7f51\u7edc\u7684\u7ec4\u5408\u4e0d\u4f1a\u5e26\u6765\u6d4b\u8bd5\u51c6\u786e\u5ea6\u63d0\u5347","title":"\u57fa\u7840\u7ed3\u8bba"},{"location":"The_theory/understandingKD/#multi-view-data","text":"\u672c\u6587\u63d0\u51famulti-view data\u8fd9\u4e2a\u8bf4\u6cd5\u6765justify\u4e3a\u4ec0\u4e48ensemble\u4ee5\u53caKD\u6709\u7528.\u5728\u56fe\u7247\u4e0a\u6765\u8bf4\u5c31\u662f\u540c\u4e00\u4e2a\u5206\u7c7b\u6211\u4eec\u53ef\u4ee5\u6709\u591a\u79cd\u7279\u5f81\uff0c\u591a\u79cd\u89c6\u89d2\u6765\u6210\u529f\u5206\u7c7b\u3002Ensemble\u4e4b\u540e\u7684\u6a21\u578b\u5c31\u6709\u4e86\u4ece\u591a\u4e2a\u89c6\u89d2\uff0c\u591a\u4e2a\u9014\u5f84\u8fdb\u884c\u5206\u7c7b\u7684\u80fd\u529b\uff0c\u6240\u4ee5\u63d0\u5347\u4e86\u51c6\u786e\u5ea6\u3002 \u8fd9\u4e2a\u8bf4\u6cd5\u521d\u542c\u4f3c\u4e4e\u662f\u5728\u6df1\u5ea6\u5b66\u4e60\u6bd4\u8f83\u65e9\u671f\u5c31\u5df2\u7ecf\u6709\u7684\u8bf4\u6cd5\uff0c\u4f46\u662f\u672c\u6587\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7b80\u5355\u7684\u6570\u636e\u96c6\uff0c\u5177\u4f53\u5730\u523b\u753b\u4e86\u8fd9\u4e2amulti-view data\u7684\u8868\u73b0\uff0c\u5e76\u7528\u7b80\u5355\u7684\u7f51\u7edc\u5728\u8fd9\u4e2a\u7b80\u5355\u7684\u6570\u636e\u96c6\u4e0a\u590d\u73b0\u4e86\u4e0a\u8ff0\u7684\u8868\u73b0\u3002","title":"Multi view Data"},{"location":"The_theory/understandingKD/#multi-view-data_1","text":"\u6570\u636e\u96c6\u5305\u542b\u56db\u4e2a\u7279\u5f81 v_1, v_2, v_3, v_4 \uff0c\u662f\u4e00\u4e2a\u4e8c\u5206\u7c7b\u95ee\u9898\u3002 label == 1 - 80%: v_1, v_2 \u90fd\u662f\u63a5\u8fd11\uff0c\u800c v_3, v_4 \u5176\u4e2d\u4e4b\u4e00\u662f0.1 - 10%: v_1 \u90fd\u662f\u63a5\u8fd11\uff0c\u800c v_3, v_4 \u5176\u4e2d\u4e4b\u4e00\u662f0.1 - 10%: v_2 \u90fd\u662f\u63a5\u8fd11\uff0c\u800c v_3, v_4 \u5176\u4e2d\u4e4b\u4e00\u662f0.1 label == 2 - 80%: v_3, v_4 \u90fd\u662f\u63a5\u8fd11\uff0c\u800c v_1, v_2 \u5176\u4e2d\u4e4b\u4e00\u662f0.1 - 10%: v_3 \u90fd\u662f\u63a5\u8fd11\uff0c\u800c v_1, v_2 \u5176\u4e2d\u4e4b\u4e00\u662f0.1 - 10%: v_4 \u90fd\u662f\u63a5\u8fd11\uff0c\u800c v_1, v_2 \u5176\u4e2d\u4e4b\u4e00\u662f0.1 \u5176\u4e2d 80%\u7684\u6570\u636e\u88ab\u79f0\u4e3a multi-view data. \u56e0\u4e3a\u4ed6\u4eec\u6709\u4e0d\u6b62\u4e00\u79cd\u65b9\u5f0f\u5206\u7c7b\u3002\u800c \u5269\u4e0b\u7684\u662f single-view data.","title":"Multi-view Data \u7684\u4eba\u5de5\u6784\u5efa\u3002"},{"location":"The_theory/understandingKD/#_2","text":"\u4f5c\u8005\u53d1\u73b0\uff0c\u7f51\u7edc\u4f1a\u5728 v_1, v_2 \u4e2d\u9009\u62e9\u4e00\u4e2a\u7279\u5f81\u6765\u6b63\u786e\u5206\u7c7blabel==1\uff0c\u4ee5\u53ca v_3, v_4 \u4e2d\u9009\u62e9\u4e00\u4e2a\u6765\u6b63\u786e\u5206\u7c7blabel==2. \u56e0\u6b6490%\u7684\u6570\u636e\u4f1a\u5f88\u5feb\u5730\u5206\u7c7b\u6210\u529f\u3002\u8bad\u7ec3\u4e00\u6bb5\u65f6\u95f4\u540e\uff0c\u8fd990%\u7684\u6570\u636e\u5c31\u4e0d\u518d\u63d0\u4f9b\u68af\u5ea6\u4e86\u3002 \u63a5\u7740\u7f51\u7edc\u4f1a\u5c1d\u8bd5\u8bb0\u5fc6\u5269\u4f59\u768410%\u8bad\u7ec3\u6837\u672c\uff0c\u4f46\u662f\u662f\u4f9d\u9760\u7f51\u7edc\u7684\u566a\u97f3\u8bb0\u5fc6\u80fd\u529b\u800c\u4e0d\u662f\u771f\u6b63\u7684\u627e\u5230\u4e86\u65b0\u7684\u7279\u5f81\u4f9d\u636e\u3002 test set\u7684\u51c6\u786e\u7387\u5c31\u5361\u572890\u5de6\u53f3 \u6a21\u578bensemble\u4e4b\u540e\uff0c\u7531\u4e8e\u968f\u673a\u521d\u59cb\u5316\u7684\u7f51\u7edc\u662f\u968f\u673a\u9009\u62e9\u4e3b\u7279\u5f81\u7684\uff0c\u56e0\u800c\u6709\u9650\u4e2a\u6a21\u578b\u7684\u878d\u5408\uff0c\u5c31\u53ef\u4ee5\u51d1\u9f50\u6240\u4ee5\u9700\u8981\u7684\u7279\u5f81\uff0ctest set\u51c6\u786e\u7387\u4e0a\u5230100%\u3002 \u800cKD\u7684\u65f6\u5019\u7531\u4e8e\u6211\u4eec\u63d0\u4f9b\u7684\u662f\u8f6flabel\uff0c\u6240\u4ee5\u7f51\u7edc\u5b9e\u9645\u4e0a\u662f\u88ab\u5f3a\u884c\u8981\u6c42\u5b66\u4e60multi-view.","title":"\u6bcf\u4e2a\u7f51\u7edc\u7684\u5b66\u4e60\u7279\u6027, \u878d\u5408\uff0c \u84b8\u998f"},{"location":"The_theory/visualizing_landscape/","text":"Visualizing the Loss Landscape of Neural Nets \u8fd9\u7bc7paper\u5f15\u5165\u4e86\u5bf9\u795e\u7ecf\u7f51\u7edc\u635f\u5931\u7684\u53ef\u89c6\u5316\u65b9\u6cd5\uff0c\u5e76\u6839\u636e\u5176\u5206\u6790\u65b9\u6848\u7ed9\u51fa\u4e86\u4e00\u4e9b\u4e0e\u635f\u5931\u51fd\u6570\u56fe\u50cf\u76f8\u5173\u7684\u7ed3\u8bba\u3002 \u53ef\u89c6\u5316\u57fa\u7840 \u672c\u6587\u56de\u987e\u4ecb\u7ecd\u4e86\u4e00\u7ef4\u63d2\u503c\u4e0e\u4e8c\u7ef4\u7ed8\u56fe\u7684\u65b9\u6cd5,\u8fd9\u4e9b\u65b9\u6cd5\u6700\u65e9\u662f Ian GoodFellow \u63d0\u51fa\u6765\u7684,\u7528\u4e8e\u5206\u6790\u4e0d\u540c\u4f18\u5316\u5668\u7684\u4f18\u5316\u60c5\u51b5\u3002 \u4e00\u7ef4\u63d2\u503c \u8bbe\u5b9a\u4e24\u7ec4\u7f51\u7edc\u53c2\u6570\uff0c\u5176\u52a0\u6743\u5e73\u5747 \\theta(\\alpha) = (1-\\alpha)\\theta + \\alpha\\theta_1 ,\u7136\u540e\u53ef\u4ee5\u753b\u51fa\u635f\u5931\u51fd\u6570\u5173\u4e8e \\alpha \u7684\u51fd\u6570\u56fe\u50cf\u3002\u5176\u4e3b\u8981\u4f5c\u7528\u5728\u4e8e\u7814\u7a76\u7f51\u7edc\u5728\u4e0d\u540c\u6700\u4f18\u70b9\u9644\u8fd1\u7684\u5e73\u6ed1\u5ea6\u4e0e\u5c16\u9510\u5ea6\u3002\u4f46\u662f\u8fd9\u6837\u65b9\u6848\u5f97\u5230\u7684\u56fe\u50cf\u5f80\u5f80\u4e24\u4e2a\u6700\u503c\u4e2d\u95f4\u6ca1\u6709\u5176\u4ed6local minimum,\u4e14\u6ca1\u6709\u8003\u8651\u7f51\u7edc\u7684\u4e00\u4e9b\u7279\u6b8a\u7684\u6027\u8d28. \u4e8c\u7ef4\u7ed8\u56fe \u8bbe\u5b9a\u4e2d\u5fc3\u70b9 \\theta^* , \u4e24\u4e2a\u65b9\u5411\u77e2\u91cf \\delta, \\eta \uff0c2D\u8868\u9762\u56fe\u53ef\u4ee5\u8868\u8fbe\u4e3a: f(\\alpha, \\beta) = L(\\theta^* + \\alpha\\delta + \\beta\\eta) \u8fd9\u79cd\u65b9\u6cd5\u5728\u5b9e\u9645\u4f7f\u7528\u4e2d\u53ef\u4ee5\u663e\u793a\u51fa\u4e00\u822c\u7f51\u7edc\u635f\u5931\u5e73\u9762\u7684\u975e\u51f8\u6027\u3002 \u5178\u578b\u7684\u53ef\u89c6\u5316\u7ed3\u679c\u53ef\u4ee5\u5728\u8fd9\u4e2a \u4ea4\u4e92\u5f0f\u7f51\u7ad9 \u4e0a\u67e5\u770b Filter-Wise Normalization \u672c\u6587\u4e2d\u7684 \\delta, \\eta \u90fd\u662f\u53d6\u81ea\u4e00\u5b9ascale\u7684\u968f\u673a\u9ad8\u65af\u5206\u5e03\uff0c\u4f5c\u8005\u8868\u793a\u8fd9\u79cd\u65b9\u6848\u7531\u4e8e\u6ca1\u6709\u8003\u8651\u5230\u7f51\u7edc\u7684scale-invariance\u3002\u5bf9\u4e8e\u4e00\u4e2aReLU\u7f51\u7edc\uff0c\u589e\u5927 i \u5c4210\u500d\uff0c\u7f29\u5c0f i+1 \u5c4210\u500d,\u8f93\u51fa\u503c\u662f\u4e0d\u4f1a\u53d8\u7684\u3002\u5982\u679c\u6709normalization\u5c42\uff0c\u5219\u66f4\u663e\u7136\u3002 \u5982\u679c\u6211\u4eec\u5bf9\u7f51\u7edc\u7684\u6270\u52a8\u90fd\u662f\u4e00\u4e2a\u56fa\u5b9a\u7684\u5355\u5143\u503c\uff0c\u5219\u4e0d\u540cscale\u7684\u7f51\u7edc\u53d7\u6270\u52a8\u7684\u5f71\u54cd\u4f1a\u4e0d\u540c. \u4f5c\u8005\u9996\u5148\u901a\u8fc7\u9ad8\u65af\u5206\u5e03\u5f97\u5230\u968f\u673a\u77e2\u91cf d , \u7136\u540e\u5c06 d \u7684scale\u7edf\u4e00\u5230 \\theta \u4e0a d_{i,j} = \\frac{d_{i,j}}{|d_{i,j}|} |\\theta_{i,j}| \u5176\u4e2d\u8fd9\u8868\u8fbe\u7684\u662f\u7b2c i \u5c42\u7b2c j \u4e2afilter for d, w in zip(direction, weights): # First channel -> output filter num d.mul_(w.norm()/(d.norm() + 1e-10)) \u5bf9\u4e8e\u53ea\u6709\u4e00\u7ef4\u7684\u77e2\u91cf\u5219\u75280(default)\u6216\u8005\u590d\u5236\u4ee3\u66ff\u3002 \u5b9e\u9a8c - \u5206\u6790\u73b0\u6709\u6a21\u578b \u4f5c\u8005\u6839\u636e\u4ee5\u4e0a\u5185\u5bb9\u505a\u4e86\u4e00\u4e9b\u5b9e\u9a8c \u5728\u57fa\u7840\u76841D\u63d2\u503c\u7684\u6761\u4ef6\u4e0b\uff0c\u51fd\u6570\u7684non-convexity\u4e0d\u660e\u663e\uff0c\u53e6\u5916\u53d1\u73b0\u635f\u5931\u8868\u9762\u7684flatness\u4e0e\u6a21\u578b\u7684\u9c81\u68d2\u6027\u5173\u7cfb\u4e0d\u5927\u3002 \u4f7f\u7528filter-wise normalization\u4ee5\u53ca\u5355\u5411\u9ad8\u65af\u6270\u52a8\uff0c\u53d1\u73b0\u635f\u5931\u8868\u9762\u7684flatness\u4e0e\u6a21\u578b\u7684test error\u6709\u76f4\u63a5\u76f8\u5173\u6027\u3002 \u4f7f\u7528filter-wise normalization\u7684\u4e8c\u7ef4\u56fe\uff0c\u53d1\u73b0resnet\u7684\u635f\u5931\u8868\u9762\u5728convexity\u4e0a\u66f4\u52a0\u663e\u8457\uff0c\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u8868\u660e\u5b83\u66f4\u597d\u8bad\u7ec3\u3002 \u5b9e\u9a8c - \u5206\u6790\u4f18\u5316\u8f68\u8ff9 \u4f5c\u8005\u53d1\u73b0\u4f7f\u7528\u4e24\u4e2a\u968f\u673a\u9ad8\u65af\u77e2\u91cf(\u4e24\u4e2a\u9ad8\u65af\u968f\u673a\u77e2\u91cf\u7684cosine\u76f8\u4f3c\u5ea6\u7684\u671f\u671b\u4e3a \\sqrt{2 / \\pi n} \uff0c\u56e0\u800c\u51e0\u4e4e\u4e00\u5b9a\u662f\u5782\u76f4\u7684)\uff0c\u4f46\u662f\u5728\u5bf9\u4f18\u5316\u8f68\u8ff9\u8fdb\u884c\u53ef\u89c6\u5316\u7684\u65f6\u5019\uff0c\u5f88\u5927\u6982\u7387\u4f18\u5316\u7684\u65b9\u5411\u4f1a\u4e0e\u968f\u673a\u53ef\u89c6\u5316\u65b9\u5411\u77e2\u91cf\u5782\u76f4\u3002\u8fd9\u91cc\u63d0\u51fa\u7684\u662f\u4f7f\u7528PCA directions \u5bf9\u4e8e\u4ece \\theta_0 \u4f18\u5316 n \u6b65\u5230\u8fbe \\theta_n \u7684\u6a21\u578b\uff0c\u8bbe\u77e9\u9635 M = [\\theta_0 - \\theta_n; ...; \\theta_{n-1} - \\theta_n] \u4f7f\u7528\u4e3b\u6210\u5206\u5206\u6790\uff0c\u627e\u51fa\u5947\u5f02\u503c\u6700\u5927\u7684\u4e24\u4e2a\u65b9\u5411\uff0c\u6839\u636e\u6b64\u65b9\u5411\u6765\u8fdb\u884c\u53ef\u89c6\u5316\u3002","title":"Visualizing the Loss Landscape of Neural Nets"},{"location":"The_theory/visualizing_landscape/#visualizing-the-loss-landscape-of-neural-nets","text":"\u8fd9\u7bc7paper\u5f15\u5165\u4e86\u5bf9\u795e\u7ecf\u7f51\u7edc\u635f\u5931\u7684\u53ef\u89c6\u5316\u65b9\u6cd5\uff0c\u5e76\u6839\u636e\u5176\u5206\u6790\u65b9\u6848\u7ed9\u51fa\u4e86\u4e00\u4e9b\u4e0e\u635f\u5931\u51fd\u6570\u56fe\u50cf\u76f8\u5173\u7684\u7ed3\u8bba\u3002","title":"Visualizing the Loss Landscape of Neural Nets"},{"location":"The_theory/visualizing_landscape/#_1","text":"\u672c\u6587\u56de\u987e\u4ecb\u7ecd\u4e86\u4e00\u7ef4\u63d2\u503c\u4e0e\u4e8c\u7ef4\u7ed8\u56fe\u7684\u65b9\u6cd5,\u8fd9\u4e9b\u65b9\u6cd5\u6700\u65e9\u662f Ian GoodFellow \u63d0\u51fa\u6765\u7684,\u7528\u4e8e\u5206\u6790\u4e0d\u540c\u4f18\u5316\u5668\u7684\u4f18\u5316\u60c5\u51b5\u3002","title":"\u53ef\u89c6\u5316\u57fa\u7840"},{"location":"The_theory/visualizing_landscape/#_2","text":"\u8bbe\u5b9a\u4e24\u7ec4\u7f51\u7edc\u53c2\u6570\uff0c\u5176\u52a0\u6743\u5e73\u5747 \\theta(\\alpha) = (1-\\alpha)\\theta + \\alpha\\theta_1 ,\u7136\u540e\u53ef\u4ee5\u753b\u51fa\u635f\u5931\u51fd\u6570\u5173\u4e8e \\alpha \u7684\u51fd\u6570\u56fe\u50cf\u3002\u5176\u4e3b\u8981\u4f5c\u7528\u5728\u4e8e\u7814\u7a76\u7f51\u7edc\u5728\u4e0d\u540c\u6700\u4f18\u70b9\u9644\u8fd1\u7684\u5e73\u6ed1\u5ea6\u4e0e\u5c16\u9510\u5ea6\u3002\u4f46\u662f\u8fd9\u6837\u65b9\u6848\u5f97\u5230\u7684\u56fe\u50cf\u5f80\u5f80\u4e24\u4e2a\u6700\u503c\u4e2d\u95f4\u6ca1\u6709\u5176\u4ed6local minimum,\u4e14\u6ca1\u6709\u8003\u8651\u7f51\u7edc\u7684\u4e00\u4e9b\u7279\u6b8a\u7684\u6027\u8d28.","title":"\u4e00\u7ef4\u63d2\u503c"},{"location":"The_theory/visualizing_landscape/#_3","text":"\u8bbe\u5b9a\u4e2d\u5fc3\u70b9 \\theta^* , \u4e24\u4e2a\u65b9\u5411\u77e2\u91cf \\delta, \\eta \uff0c2D\u8868\u9762\u56fe\u53ef\u4ee5\u8868\u8fbe\u4e3a: f(\\alpha, \\beta) = L(\\theta^* + \\alpha\\delta + \\beta\\eta) \u8fd9\u79cd\u65b9\u6cd5\u5728\u5b9e\u9645\u4f7f\u7528\u4e2d\u53ef\u4ee5\u663e\u793a\u51fa\u4e00\u822c\u7f51\u7edc\u635f\u5931\u5e73\u9762\u7684\u975e\u51f8\u6027\u3002 \u5178\u578b\u7684\u53ef\u89c6\u5316\u7ed3\u679c\u53ef\u4ee5\u5728\u8fd9\u4e2a \u4ea4\u4e92\u5f0f\u7f51\u7ad9 \u4e0a\u67e5\u770b","title":"\u4e8c\u7ef4\u7ed8\u56fe"},{"location":"The_theory/visualizing_landscape/#filter-wise-normalization","text":"\u672c\u6587\u4e2d\u7684 \\delta, \\eta \u90fd\u662f\u53d6\u81ea\u4e00\u5b9ascale\u7684\u968f\u673a\u9ad8\u65af\u5206\u5e03\uff0c\u4f5c\u8005\u8868\u793a\u8fd9\u79cd\u65b9\u6848\u7531\u4e8e\u6ca1\u6709\u8003\u8651\u5230\u7f51\u7edc\u7684scale-invariance\u3002\u5bf9\u4e8e\u4e00\u4e2aReLU\u7f51\u7edc\uff0c\u589e\u5927 i \u5c4210\u500d\uff0c\u7f29\u5c0f i+1 \u5c4210\u500d,\u8f93\u51fa\u503c\u662f\u4e0d\u4f1a\u53d8\u7684\u3002\u5982\u679c\u6709normalization\u5c42\uff0c\u5219\u66f4\u663e\u7136\u3002 \u5982\u679c\u6211\u4eec\u5bf9\u7f51\u7edc\u7684\u6270\u52a8\u90fd\u662f\u4e00\u4e2a\u56fa\u5b9a\u7684\u5355\u5143\u503c\uff0c\u5219\u4e0d\u540cscale\u7684\u7f51\u7edc\u53d7\u6270\u52a8\u7684\u5f71\u54cd\u4f1a\u4e0d\u540c. \u4f5c\u8005\u9996\u5148\u901a\u8fc7\u9ad8\u65af\u5206\u5e03\u5f97\u5230\u968f\u673a\u77e2\u91cf d , \u7136\u540e\u5c06 d \u7684scale\u7edf\u4e00\u5230 \\theta \u4e0a d_{i,j} = \\frac{d_{i,j}}{|d_{i,j}|} |\\theta_{i,j}| \u5176\u4e2d\u8fd9\u8868\u8fbe\u7684\u662f\u7b2c i \u5c42\u7b2c j \u4e2afilter for d, w in zip(direction, weights): # First channel -> output filter num d.mul_(w.norm()/(d.norm() + 1e-10)) \u5bf9\u4e8e\u53ea\u6709\u4e00\u7ef4\u7684\u77e2\u91cf\u5219\u75280(default)\u6216\u8005\u590d\u5236\u4ee3\u66ff\u3002","title":"Filter-Wise Normalization"},{"location":"The_theory/visualizing_landscape/#-","text":"\u4f5c\u8005\u6839\u636e\u4ee5\u4e0a\u5185\u5bb9\u505a\u4e86\u4e00\u4e9b\u5b9e\u9a8c \u5728\u57fa\u7840\u76841D\u63d2\u503c\u7684\u6761\u4ef6\u4e0b\uff0c\u51fd\u6570\u7684non-convexity\u4e0d\u660e\u663e\uff0c\u53e6\u5916\u53d1\u73b0\u635f\u5931\u8868\u9762\u7684flatness\u4e0e\u6a21\u578b\u7684\u9c81\u68d2\u6027\u5173\u7cfb\u4e0d\u5927\u3002 \u4f7f\u7528filter-wise normalization\u4ee5\u53ca\u5355\u5411\u9ad8\u65af\u6270\u52a8\uff0c\u53d1\u73b0\u635f\u5931\u8868\u9762\u7684flatness\u4e0e\u6a21\u578b\u7684test error\u6709\u76f4\u63a5\u76f8\u5173\u6027\u3002 \u4f7f\u7528filter-wise normalization\u7684\u4e8c\u7ef4\u56fe\uff0c\u53d1\u73b0resnet\u7684\u635f\u5931\u8868\u9762\u5728convexity\u4e0a\u66f4\u52a0\u663e\u8457\uff0c\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u8868\u660e\u5b83\u66f4\u597d\u8bad\u7ec3\u3002","title":"\u5b9e\u9a8c - \u5206\u6790\u73b0\u6709\u6a21\u578b"},{"location":"The_theory/visualizing_landscape/#-_1","text":"\u4f5c\u8005\u53d1\u73b0\u4f7f\u7528\u4e24\u4e2a\u968f\u673a\u9ad8\u65af\u77e2\u91cf(\u4e24\u4e2a\u9ad8\u65af\u968f\u673a\u77e2\u91cf\u7684cosine\u76f8\u4f3c\u5ea6\u7684\u671f\u671b\u4e3a \\sqrt{2 / \\pi n} \uff0c\u56e0\u800c\u51e0\u4e4e\u4e00\u5b9a\u662f\u5782\u76f4\u7684)\uff0c\u4f46\u662f\u5728\u5bf9\u4f18\u5316\u8f68\u8ff9\u8fdb\u884c\u53ef\u89c6\u5316\u7684\u65f6\u5019\uff0c\u5f88\u5927\u6982\u7387\u4f18\u5316\u7684\u65b9\u5411\u4f1a\u4e0e\u968f\u673a\u53ef\u89c6\u5316\u65b9\u5411\u77e2\u91cf\u5782\u76f4\u3002\u8fd9\u91cc\u63d0\u51fa\u7684\u662f\u4f7f\u7528PCA directions \u5bf9\u4e8e\u4ece \\theta_0 \u4f18\u5316 n \u6b65\u5230\u8fbe \\theta_n \u7684\u6a21\u578b\uff0c\u8bbe\u77e9\u9635 M = [\\theta_0 - \\theta_n; ...; \\theta_{n-1} - \\theta_n] \u4f7f\u7528\u4e3b\u6210\u5206\u5206\u6790\uff0c\u627e\u51fa\u5947\u5f02\u503c\u6700\u5927\u7684\u4e24\u4e2a\u65b9\u5411\uff0c\u6839\u636e\u6b64\u65b9\u5411\u6765\u8fdb\u884c\u53ef\u89c6\u5316\u3002","title":"\u5b9e\u9a8c - \u5206\u6790\u4f18\u5316\u8f68\u8ff9"},{"location":"other_categories/Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/","text":"ChauffeurNet: Learning to Drive by Imitating the Best and Synthesizing the Worst \u8fd9\u7bc7RSS\u8bba\u6587\u6765\u81ea\u4e8eWaymo,\u82f1\u6587\u540d\u5b57\u7684\u7ffb\u8bd1\u610f\u601d\u662f\"\u53f8\u673a\u7f51\",\u7ed9\u51fa\u4e86\u4e00\u4e2aimitation learning\u7cfb\u7edf\uff0c\u5e76\u5b9e\u73b0\u4e86\u4e0a\u8f66\u6d4b\u8bd5\uff0c\u4e3a\u4e86\u4fdd\u8bc1\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u672c\u6587\u505a\u4e86\u5f88\u591a\u7684\u9009\u62e9\u4e0e\u52a0\u5f3a\uff0c\u503c\u5f97\u4e86\u89e3\u3002\u8fd9\u7bc7\u8bba\u6587\u6709 \u6765\u81eawaymo\u7684\u5b98\u65b9medium\u82f1\u6587\u89e3\u8bfb Imitation Learning\u7684\u5e38\u89c1\u95ee\u9898 \u4eceraw data\u5230\u5e95\u5c42 control\uff0cgeneralization\u96be\u5ea6\u5f88\u5927\uff0c\u4e0d\u9002\u7528\u4e8e\u4e0d\u540c\u7684\u4f20\u611f\u4e0e\u5e95\u5c42\u7269\u7406\u6a21\u578b\uff0c\u7f51\u7edc\u88ab\u8feb\u5f00\u73af\u5730\u5b66\u4e60\u5e95\u5c42\u7269\u7406\u6a21\u578b\uff0c\u6d6a\u8d39\u7f51\u7edc\u80fd\u529b\u3002 \u5373\u65f6\u8f6c\u6362\u4e3a\u672c\u6587\u7ed9\u51fa\u7684\u4e2d\u5c42\u8f93\u5165\u8f93\u51fa(processed percepetion and planning and map information -> target poses sequences),\u964d\u4f4e\u4e86\u7f51\u7edc\u7684\u4eff\u771f\u96be\u5ea6\uff0c30M\u7684\u6570\u636e\u70b9\u4ecd\u4e0d\u8db3\u4ee5\u4f7f\u4f20\u7edf\u7684Cloning\u7b97\u6cd5\u5f97\u5230\u597d\u7684\u7ed3\u679c\u3002 \u672c\u6587\u7f51\u7edc\u7684\u8f93\u5165\u8f93\u51fadata representation \u516b\u5f20\u56fe\u5206\u522b\u4e3a: 1. \u57ce\u5e02\u9053\u8def\u5730\u56fe 2. traffic light\u53ef\u901a\u884c\u4ee5\u53ca\u65f6\u95f4\u4fe1\u606f(\u5b9e\u9645\u4e0a\u662f\u7ed9\u4e86\u8fde\u7eed\u51e0\u5f20\u56fe\u7684) 3. \u901f\u5ea6\u9650\u5236 4. \u76ee\u6807\u8def\u5f84(\u7c7b\u4f3c\u5730\u56feAPP\u7684\u6307\u793a) 5. \u5f53\u524dagent\u4f4d\u7f6e 6. \u6700\u8fd1\u4e00\u7cfb\u5217\u7684\u52a8\u6001\u969c\u788d\u7269\u7684\u4f4d\u7f6e 7. \u8fc7\u53bb\u4e00\u6bb5\u65f6\u95f4\u7684agent poses 8. \u672a\u6765\u7684agent poses(\u8f93\u51fa) \u6240\u6709\u8f93\u5165\u56fe\u4ee5\u8f66\u8f86\u5f53\u524d\u5750\u6807\u7cfb\u7ed9\u51fa\uff0c\u8f66\u8f86\u5f53\u524dpose\u4f1a\u56fa\u5b9a\u5728\u4e00\u4e2a (u_0, v_0) \u70b9,\u8bad\u7ec3\u7684\u65f6\u5019\u4f1a\u989d\u5916\u5bf9\u6570\u636e\u4e2d\u7684\u8f66\u5b50\u7684heading\u52a0\u4e00\u4e2a\u6270\u52a8\uff0c\u76f8\u5f53\u4e8e\u8bad\u7ec3\u65f6\u7684\u8fd9\u4e9bfeature map\u4f1a\u76f8\u5bf9\u6709\u989d\u5916\u7684\u65cb\u8f6c\u3002 \u7f51\u7edc\u7ed3\u6784 \u6574\u4f53\u7ed3\u6784\u5982\u56fe\uff0c\u6838\u5fc3\u90e8\u5206\u4e3a\"\u8f93\u5165->\u7279\u5f81\u63d0\u53d6->RNN->\u671d\u5411\u3001\u901f\u5ea6\u3001\u672a\u6765\u76ee\u6807\u70b9\u3001\u5916\u6765heat map\"\u3002additional target \u5305\u62ecroad mask\u4ee5\u53ca\u4e00\u4e2adynamic object prediction \u4ee5\u4e0b\u52a8\u56fe\u8868\u8fbe\u4e86Agent RNN\u7684RNN\u7279\u5f81(\u5305\u62ecmemory) \u5411\u4e13\u5bb6\u6a21\u4eff\u5b66\u4e60 \u51e0\u4e2acost\u662f\u5e38\u89c1\u7684\uff0c\u8fd9\u91cc\u4e3b\u8981\u8bb0\u5f55\u4e00\u4e2atrick Past Motion Dropout \u8bad\u7ec3\u7684\u65f6\u5019\uff0c\u7531\u4e8e\u4e13\u5bb6\u7684\u8def\u5f84\u592a\u5e73\u6ed1\uff0c\u6709\u65f6\u5019\u53ea\u9700\u8981\u5bf9\u5148\u524d\u51e0\u4e2a\u65f6\u95f4\u70b9\u7684\u8def\u5f84\u70b9\u8fdb\u884c\u63d2\u503c\u5c31\u80fd\u987a\u5229\u5f97\u5230\u540e\u9762\u7684\u76ee\u6807\u70b9\uff0c\u4e14\u8bef\u5dee\u5f88\u5c0f\uff0c\u8fd9\u91cc\u4e3a\u4e86\u51cf\u8f7b\u8fd9\u4e2a\u95ee\u9898\uff0c\u5728\u8bad\u7ec3\u7684\u8fc7\u7a0b\u4e2d\u4f1a\u968f\u673adrop\u6389\u4e00\u4e9b\u5386\u53f2\u7684poses\uff0c\u5177\u4f53\u6765\u8bf4\u5c31\u662f50%\u7684training data\u91cc\u9762\u5386\u53f2poses\u4e3a\u7a7a\uff0c\u53ea\u5269\u4e0b\u5f53\u524dpose \u63d0\u5347\u96be\u5ea6 \u5408\u6210\u5e72\u6270 \u5c06\u4e00\u4e9b\u5e73\u6ed1\u7684\u4e13\u5bb6path\u4e2d\u95f4\uff0c\u968f\u673a\u9009\u4e00\u4e9b\u70b9\u6c34\u5e73\u6270\u52a8\uff0c\u7136\u540e\u7528\u5e73\u6ed1\u7684\u63d2\u503c\u91cd\u65b0\u751f\u6210\u5047\u7684\u4e13\u5bb6\u8f68\u8ff9\uff0c \u8fd9\u4e9b\u65b0\u7684\u8f68\u8ff9\u53ef\u80fd\u4f1a\u78b0\u649e\uff0c\u6240\u4ee5\u8fd9\u7c7bdata\u7684weight\u53ea\u6709\u6b63\u786edata\u76840.1\uff0c\u4e0d\u8fc7\u53ef\u4ee5\u5f88\u597d\u7684\u589e\u52a0\u6b63\u5e38\u53f8\u673a\u4e0d\u4f1a\u5230\u8fbe\u7684\u5371\u9669\u60c5\u666f \u8f85\u52a9loss \u78b0\u649e loss\uff0c\u4e3b\u8981\u5728\u4e8e\u60e9\u7f5aperturbation\u7684\u65f6\u5019\u7684\u4e00\u4e9b\u78b0\u649e on road loss: \u51e0\u4f55loss\uff0c\u52a0\u5f3a\u4e0e\u539f\u8f68\u8ff9\u7684\u91cd\u5408\u5ea6 road masking \u4e0e prediction \u6a21\u4effdropout \u6709\u4e00\u5b9a\u7684\u6982\u7387\u4e0d\u9700\u8981\u7f51\u7edc\u5b9e\u73b0imitation\uff0c\u8ba9imitation\u90e8\u5206loss\u4e3a0\uff0c\u53ea\u7559\u4e0b\u524d\u4e00\u6bb5\u5199\u5230\u7684\u9644\u52a0loss","title":"ChauffeurNet: Learning to Drive by Imitating the Best and Synthesizing the Worst"},{"location":"other_categories/Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/#chauffeurnet-learning-to-drive-by-imitating-the-best-and-synthesizing-the-worst","text":"\u8fd9\u7bc7RSS\u8bba\u6587\u6765\u81ea\u4e8eWaymo,\u82f1\u6587\u540d\u5b57\u7684\u7ffb\u8bd1\u610f\u601d\u662f\"\u53f8\u673a\u7f51\",\u7ed9\u51fa\u4e86\u4e00\u4e2aimitation learning\u7cfb\u7edf\uff0c\u5e76\u5b9e\u73b0\u4e86\u4e0a\u8f66\u6d4b\u8bd5\uff0c\u4e3a\u4e86\u4fdd\u8bc1\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u672c\u6587\u505a\u4e86\u5f88\u591a\u7684\u9009\u62e9\u4e0e\u52a0\u5f3a\uff0c\u503c\u5f97\u4e86\u89e3\u3002\u8fd9\u7bc7\u8bba\u6587\u6709 \u6765\u81eawaymo\u7684\u5b98\u65b9medium\u82f1\u6587\u89e3\u8bfb","title":"ChauffeurNet: Learning to Drive by Imitating the Best and Synthesizing the Worst"},{"location":"other_categories/Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/#imitation-learning","text":"\u4eceraw data\u5230\u5e95\u5c42 control\uff0cgeneralization\u96be\u5ea6\u5f88\u5927\uff0c\u4e0d\u9002\u7528\u4e8e\u4e0d\u540c\u7684\u4f20\u611f\u4e0e\u5e95\u5c42\u7269\u7406\u6a21\u578b\uff0c\u7f51\u7edc\u88ab\u8feb\u5f00\u73af\u5730\u5b66\u4e60\u5e95\u5c42\u7269\u7406\u6a21\u578b\uff0c\u6d6a\u8d39\u7f51\u7edc\u80fd\u529b\u3002 \u5373\u65f6\u8f6c\u6362\u4e3a\u672c\u6587\u7ed9\u51fa\u7684\u4e2d\u5c42\u8f93\u5165\u8f93\u51fa(processed percepetion and planning and map information -> target poses sequences),\u964d\u4f4e\u4e86\u7f51\u7edc\u7684\u4eff\u771f\u96be\u5ea6\uff0c30M\u7684\u6570\u636e\u70b9\u4ecd\u4e0d\u8db3\u4ee5\u4f7f\u4f20\u7edf\u7684Cloning\u7b97\u6cd5\u5f97\u5230\u597d\u7684\u7ed3\u679c\u3002","title":"Imitation Learning\u7684\u5e38\u89c1\u95ee\u9898"},{"location":"other_categories/Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/#data-representation","text":"\u516b\u5f20\u56fe\u5206\u522b\u4e3a: 1. \u57ce\u5e02\u9053\u8def\u5730\u56fe 2. traffic light\u53ef\u901a\u884c\u4ee5\u53ca\u65f6\u95f4\u4fe1\u606f(\u5b9e\u9645\u4e0a\u662f\u7ed9\u4e86\u8fde\u7eed\u51e0\u5f20\u56fe\u7684) 3. \u901f\u5ea6\u9650\u5236 4. \u76ee\u6807\u8def\u5f84(\u7c7b\u4f3c\u5730\u56feAPP\u7684\u6307\u793a) 5. \u5f53\u524dagent\u4f4d\u7f6e 6. \u6700\u8fd1\u4e00\u7cfb\u5217\u7684\u52a8\u6001\u969c\u788d\u7269\u7684\u4f4d\u7f6e 7. \u8fc7\u53bb\u4e00\u6bb5\u65f6\u95f4\u7684agent poses 8. \u672a\u6765\u7684agent poses(\u8f93\u51fa) \u6240\u6709\u8f93\u5165\u56fe\u4ee5\u8f66\u8f86\u5f53\u524d\u5750\u6807\u7cfb\u7ed9\u51fa\uff0c\u8f66\u8f86\u5f53\u524dpose\u4f1a\u56fa\u5b9a\u5728\u4e00\u4e2a (u_0, v_0) \u70b9,\u8bad\u7ec3\u7684\u65f6\u5019\u4f1a\u989d\u5916\u5bf9\u6570\u636e\u4e2d\u7684\u8f66\u5b50\u7684heading\u52a0\u4e00\u4e2a\u6270\u52a8\uff0c\u76f8\u5f53\u4e8e\u8bad\u7ec3\u65f6\u7684\u8fd9\u4e9bfeature map\u4f1a\u76f8\u5bf9\u6709\u989d\u5916\u7684\u65cb\u8f6c\u3002","title":"\u672c\u6587\u7f51\u7edc\u7684\u8f93\u5165\u8f93\u51fadata representation"},{"location":"other_categories/Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/#_1","text":"\u6574\u4f53\u7ed3\u6784\u5982\u56fe\uff0c\u6838\u5fc3\u90e8\u5206\u4e3a\"\u8f93\u5165->\u7279\u5f81\u63d0\u53d6->RNN->\u671d\u5411\u3001\u901f\u5ea6\u3001\u672a\u6765\u76ee\u6807\u70b9\u3001\u5916\u6765heat map\"\u3002additional target \u5305\u62ecroad mask\u4ee5\u53ca\u4e00\u4e2adynamic object prediction \u4ee5\u4e0b\u52a8\u56fe\u8868\u8fbe\u4e86Agent RNN\u7684RNN\u7279\u5f81(\u5305\u62ecmemory)","title":"\u7f51\u7edc\u7ed3\u6784"},{"location":"other_categories/Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/#_2","text":"\u51e0\u4e2acost\u662f\u5e38\u89c1\u7684\uff0c\u8fd9\u91cc\u4e3b\u8981\u8bb0\u5f55\u4e00\u4e2atrick","title":"\u5411\u4e13\u5bb6\u6a21\u4eff\u5b66\u4e60"},{"location":"other_categories/Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/#past-motion-dropout","text":"\u8bad\u7ec3\u7684\u65f6\u5019\uff0c\u7531\u4e8e\u4e13\u5bb6\u7684\u8def\u5f84\u592a\u5e73\u6ed1\uff0c\u6709\u65f6\u5019\u53ea\u9700\u8981\u5bf9\u5148\u524d\u51e0\u4e2a\u65f6\u95f4\u70b9\u7684\u8def\u5f84\u70b9\u8fdb\u884c\u63d2\u503c\u5c31\u80fd\u987a\u5229\u5f97\u5230\u540e\u9762\u7684\u76ee\u6807\u70b9\uff0c\u4e14\u8bef\u5dee\u5f88\u5c0f\uff0c\u8fd9\u91cc\u4e3a\u4e86\u51cf\u8f7b\u8fd9\u4e2a\u95ee\u9898\uff0c\u5728\u8bad\u7ec3\u7684\u8fc7\u7a0b\u4e2d\u4f1a\u968f\u673adrop\u6389\u4e00\u4e9b\u5386\u53f2\u7684poses\uff0c\u5177\u4f53\u6765\u8bf4\u5c31\u662f50%\u7684training data\u91cc\u9762\u5386\u53f2poses\u4e3a\u7a7a\uff0c\u53ea\u5269\u4e0b\u5f53\u524dpose","title":"Past Motion Dropout"},{"location":"other_categories/Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/#_3","text":"","title":"\u63d0\u5347\u96be\u5ea6"},{"location":"other_categories/Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/#_4","text":"\u5c06\u4e00\u4e9b\u5e73\u6ed1\u7684\u4e13\u5bb6path\u4e2d\u95f4\uff0c\u968f\u673a\u9009\u4e00\u4e9b\u70b9\u6c34\u5e73\u6270\u52a8\uff0c\u7136\u540e\u7528\u5e73\u6ed1\u7684\u63d2\u503c\u91cd\u65b0\u751f\u6210\u5047\u7684\u4e13\u5bb6\u8f68\u8ff9\uff0c \u8fd9\u4e9b\u65b0\u7684\u8f68\u8ff9\u53ef\u80fd\u4f1a\u78b0\u649e\uff0c\u6240\u4ee5\u8fd9\u7c7bdata\u7684weight\u53ea\u6709\u6b63\u786edata\u76840.1\uff0c\u4e0d\u8fc7\u53ef\u4ee5\u5f88\u597d\u7684\u589e\u52a0\u6b63\u5e38\u53f8\u673a\u4e0d\u4f1a\u5230\u8fbe\u7684\u5371\u9669\u60c5\u666f","title":"\u5408\u6210\u5e72\u6270"},{"location":"other_categories/Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/#loss","text":"\u78b0\u649e loss\uff0c\u4e3b\u8981\u5728\u4e8e\u60e9\u7f5aperturbation\u7684\u65f6\u5019\u7684\u4e00\u4e9b\u78b0\u649e on road loss: \u51e0\u4f55loss\uff0c\u52a0\u5f3a\u4e0e\u539f\u8f68\u8ff9\u7684\u91cd\u5408\u5ea6 road masking \u4e0e prediction","title":"\u8f85\u52a9loss"},{"location":"other_categories/Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/#dropout","text":"\u6709\u4e00\u5b9a\u7684\u6982\u7387\u4e0d\u9700\u8981\u7f51\u7edc\u5b9e\u73b0imitation\uff0c\u8ba9imitation\u90e8\u5206loss\u4e3a0\uff0c\u53ea\u7559\u4e0b\u524d\u4e00\u6bb5\u5199\u5230\u7684\u9644\u52a0loss","title":"\u6a21\u4effdropout"},{"location":"other_categories/Deep-Navigation/DriftingCaiRal/","text":"High-speed Autonomous Drifting with Deep Reinforcement Learning \u8fd9\u7bc7\u8bba\u6587\u662f\u5b9e\u9a8c\u5ba4\u5b66\u957f\u4eec\u7684\u4e00\u7bc7\u5408\u4f5c\u8bba\u6587\uff0c\u5b8c\u6210\u7684\u4efb\u52a1\u662f\u7528\u5f3a\u5316\u5b66\u4e60\u6559\u4f1a\u8f66\u5b50\u5728Carla\u4e2d\u6f02\u79fb,\u6210\u679c\u662f\u6210\u529f\u8ba9\u8f66\u5b50\u5728Carla\u8fbe\u5230\u8fd1100km/h\u7684\u8fc7\u5f2f\u901f\u5ea6\u3002\u672c\u6587\u6709\u4e00\u4e2a \u5b98\u65b9\u4e3b\u9875 \uff0c\u4ee5\u4e0b\u4e3a\u5176\u5728bilibili\u4e0a\u53d1\u5e03\u7684 \u89c6\u9891 \u5f3a\u5316\u5b66\u4e60\u5b9a\u4e49 \u5730\u56fe\u4e0e\u76ee\u6807\u8f68\u8ff9 \u5730\u56fe\u662f\u6839\u636e\u67d0\u77e5\u540d\u5361\u4e01\u8f66\u6e38\u620f\u7684\u5730\u56fe\u6539\u7f16\u7684\uff0c\u4f7f\u7528RoadRunner\u753b\u51fa\u6765\u5e76\u52a0\u8f7d\u5230Carla\u4e2d\u3002\u53c2\u8003\u8f68\u8ff9\u7531\u67d0\u53f8\u673a\u5728\u573a\u666f\u4e2d\u6a21\u62df\u9a7e\u9a76\u5f62\u6210\uff0c\u8981\u6c42\u662f\u5c3d\u53ef\u80fd\u5f00\u5f97\u5feb\u5e76\u4e14\u4f7f\u7528\u6f02\u79fb\u5f62\u6210\u5c16\u9510\u7684\u8f6c\u89d2\u3002 \u72b6\u6001\u53d8\u91cf\u4e0e\u884c\u52a8\u7a7a\u95f4 \u72b6\u6001\u53d8\u91cf\u5305\u542b:\u5f53\u524d\u8235\u89d2\uff0c\u5f53\u524d\u6cb9\u95e8\uff0c\u6b63\u5411\u3001\u6cd5\u5411\u3001\u603b\u901f\u5ea6\uff0c\u4fa7\u504f\u89d2\uff0c\u671d\u5411\u89d2\u3002 \u671d\u5411\u89d2\u7684Ground truth\u7531Vector Field Guidance(VFG)\u8ba1\u7b97\uff0c\u8fd9\u91cc\u5efa\u8bae\u53c2\u8003\u539f\u6587\u3002 \u72b6\u6001\u7a7a\u95f4\u4e3a: \\mathcal{S}=\\left\\{\\delta, \\tau, e_{y}, \\dot{e}_{y}, e_{\\psi}, \\dot{e}_{\\psi}, e_{\\beta}, \\dot{e}_{\\beta}, e_{v x}, \\dot{e}_{v x}, e_{v y}, \\dot{e}_{v y}, \\mathcal{T}\\right\\} \u5176\u4e2d \\mathcal{T} \u5305\u542b\u5341\u4e2a\u672a\u6765\u7684{ x, y, \\beta }\u76ee\u6807\u3002 \u884c\u52a8\u7a7a\u95f4\u4e3a\u4e3a\u5f52\u4e00\u5316\u5230[-1,1]\u7684\u8235\u89d2.Carla\u4e2dthrottle\u4e3a[0, 1],\u4f46\u662f\u4e3a\u4e86\u4f7f\u5f97\u8f66\u5b50\u5f00\u5f97\u5feb\uff0c\u52a0\u5feb\u8bad\u7ec3\uff0c\u8fd9\u91cc\u6709\u6548\u7684throttle\u8303\u56f4\u4e3a[0.6, 1]. \u8f93\u51fa\u503c\u7ecf\u4e00\u9636\u6570\u5b57\u6ee4\u6ce2\u5f97\u5230\u5b9e\u9645\u8f93\u5165\u5230Carla\u7684\u63a7\u5236\u503c\u3002 Reward\u51fd\u6570 \\begin{array}{ll}{r_{e_{y}}=e^{-k_{1} e_{y}}} \\\\ {r_{e_{\\psi}}, r_{e_{\\beta}}=f(x)=} & {\\left\\{\\begin{array}{cc}{e^{-k_{2}|x|}} & {|x|<90^{\\circ}} \\\\ {-e^{-k_{2}\\left(180^{\\circ}-x\\right)}} & {x \\geq 90^{\\circ}} \\\\ {-e^{-k_{2}\\left(180^{\\circ}+x\\right)}} & {x \\leq-90^{\\circ}}\\end{array}\\right.}\\end{array} r=v\\left(k_{e_{y}} r_{e_{y}}+k_{e_{\\psi}} r_{e_{\\psi}}+k_{e_{\\beta}} r_{e_{\\beta}}\\right) SAC \u7b97\u6cd5 \u8fd8\u6709\u4e00\u4e9b\u66f4\u4e3a\u5177\u4f53\u7684Trick\u5728\u8bba\u6587\u4e2d\u63d0\u5230\uff0c\u5efa\u8bae\u53c2\u8003\u5f00\u6e90\u4ee3\u7801\u3002 \u5b9e\u9a8c\u6548\u679c\u5efa\u8bae\u67e5\u8be2\u5176 \u5b98\u65b9\u4e3b\u9875 ,\u4e0e \u5b98\u65b9\u89c6\u9891","title":"High-speed Autonomous Drifting with Deep Reinforcement Learning"},{"location":"other_categories/Deep-Navigation/DriftingCaiRal/#high-speed-autonomous-drifting-with-deep-reinforcement-learning","text":"\u8fd9\u7bc7\u8bba\u6587\u662f\u5b9e\u9a8c\u5ba4\u5b66\u957f\u4eec\u7684\u4e00\u7bc7\u5408\u4f5c\u8bba\u6587\uff0c\u5b8c\u6210\u7684\u4efb\u52a1\u662f\u7528\u5f3a\u5316\u5b66\u4e60\u6559\u4f1a\u8f66\u5b50\u5728Carla\u4e2d\u6f02\u79fb,\u6210\u679c\u662f\u6210\u529f\u8ba9\u8f66\u5b50\u5728Carla\u8fbe\u5230\u8fd1100km/h\u7684\u8fc7\u5f2f\u901f\u5ea6\u3002\u672c\u6587\u6709\u4e00\u4e2a \u5b98\u65b9\u4e3b\u9875 \uff0c\u4ee5\u4e0b\u4e3a\u5176\u5728bilibili\u4e0a\u53d1\u5e03\u7684 \u89c6\u9891","title":"High-speed Autonomous Drifting with Deep Reinforcement Learning"},{"location":"other_categories/Deep-Navigation/DriftingCaiRal/#_1","text":"","title":"\u5f3a\u5316\u5b66\u4e60\u5b9a\u4e49"},{"location":"other_categories/Deep-Navigation/DriftingCaiRal/#_2","text":"\u5730\u56fe\u662f\u6839\u636e\u67d0\u77e5\u540d\u5361\u4e01\u8f66\u6e38\u620f\u7684\u5730\u56fe\u6539\u7f16\u7684\uff0c\u4f7f\u7528RoadRunner\u753b\u51fa\u6765\u5e76\u52a0\u8f7d\u5230Carla\u4e2d\u3002\u53c2\u8003\u8f68\u8ff9\u7531\u67d0\u53f8\u673a\u5728\u573a\u666f\u4e2d\u6a21\u62df\u9a7e\u9a76\u5f62\u6210\uff0c\u8981\u6c42\u662f\u5c3d\u53ef\u80fd\u5f00\u5f97\u5feb\u5e76\u4e14\u4f7f\u7528\u6f02\u79fb\u5f62\u6210\u5c16\u9510\u7684\u8f6c\u89d2\u3002","title":"\u5730\u56fe\u4e0e\u76ee\u6807\u8f68\u8ff9"},{"location":"other_categories/Deep-Navigation/DriftingCaiRal/#_3","text":"\u72b6\u6001\u53d8\u91cf\u5305\u542b:\u5f53\u524d\u8235\u89d2\uff0c\u5f53\u524d\u6cb9\u95e8\uff0c\u6b63\u5411\u3001\u6cd5\u5411\u3001\u603b\u901f\u5ea6\uff0c\u4fa7\u504f\u89d2\uff0c\u671d\u5411\u89d2\u3002 \u671d\u5411\u89d2\u7684Ground truth\u7531Vector Field Guidance(VFG)\u8ba1\u7b97\uff0c\u8fd9\u91cc\u5efa\u8bae\u53c2\u8003\u539f\u6587\u3002 \u72b6\u6001\u7a7a\u95f4\u4e3a: \\mathcal{S}=\\left\\{\\delta, \\tau, e_{y}, \\dot{e}_{y}, e_{\\psi}, \\dot{e}_{\\psi}, e_{\\beta}, \\dot{e}_{\\beta}, e_{v x}, \\dot{e}_{v x}, e_{v y}, \\dot{e}_{v y}, \\mathcal{T}\\right\\} \u5176\u4e2d \\mathcal{T} \u5305\u542b\u5341\u4e2a\u672a\u6765\u7684{ x, y, \\beta }\u76ee\u6807\u3002 \u884c\u52a8\u7a7a\u95f4\u4e3a\u4e3a\u5f52\u4e00\u5316\u5230[-1,1]\u7684\u8235\u89d2.Carla\u4e2dthrottle\u4e3a[0, 1],\u4f46\u662f\u4e3a\u4e86\u4f7f\u5f97\u8f66\u5b50\u5f00\u5f97\u5feb\uff0c\u52a0\u5feb\u8bad\u7ec3\uff0c\u8fd9\u91cc\u6709\u6548\u7684throttle\u8303\u56f4\u4e3a[0.6, 1]. \u8f93\u51fa\u503c\u7ecf\u4e00\u9636\u6570\u5b57\u6ee4\u6ce2\u5f97\u5230\u5b9e\u9645\u8f93\u5165\u5230Carla\u7684\u63a7\u5236\u503c\u3002","title":"\u72b6\u6001\u53d8\u91cf\u4e0e\u884c\u52a8\u7a7a\u95f4"},{"location":"other_categories/Deep-Navigation/DriftingCaiRal/#reward","text":"\\begin{array}{ll}{r_{e_{y}}=e^{-k_{1} e_{y}}} \\\\ {r_{e_{\\psi}}, r_{e_{\\beta}}=f(x)=} & {\\left\\{\\begin{array}{cc}{e^{-k_{2}|x|}} & {|x|<90^{\\circ}} \\\\ {-e^{-k_{2}\\left(180^{\\circ}-x\\right)}} & {x \\geq 90^{\\circ}} \\\\ {-e^{-k_{2}\\left(180^{\\circ}+x\\right)}} & {x \\leq-90^{\\circ}}\\end{array}\\right.}\\end{array} r=v\\left(k_{e_{y}} r_{e_{y}}+k_{e_{\\psi}} r_{e_{\\psi}}+k_{e_{\\beta}} r_{e_{\\beta}}\\right)","title":"Reward\u51fd\u6570"},{"location":"other_categories/Deep-Navigation/DriftingCaiRal/#sac","text":"\u8fd8\u6709\u4e00\u4e9b\u66f4\u4e3a\u5177\u4f53\u7684Trick\u5728\u8bba\u6587\u4e2d\u63d0\u5230\uff0c\u5efa\u8bae\u53c2\u8003\u5f00\u6e90\u4ee3\u7801\u3002 \u5b9e\u9a8c\u6548\u679c\u5efa\u8bae\u67e5\u8be2\u5176 \u5b98\u65b9\u4e3b\u9875 ,\u4e0e \u5b98\u65b9\u89c6\u9891","title":"SAC \u7b97\u6cd5"},{"location":"other_categories/Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/","text":"DroNet: Learning to Fly by Driving \u6838\u5fc3\u662f\u4e00\u4e2a\u6a21\u4eff\u5b66\u4e60\uff0c\u8f93\u51fa\u8235\u89d2\u4ee5\u53ca\u78b0\u649e\u6982\u7387\uff0c\u78b0\u649e\u6982\u7387\u6765\u81ea\u4e8e\u4f5c\u8005\u989d\u5916\u624b\u5de5\u6807\u6ce8\u7684\u6570\u636e\u96c6\u3002 \u7a81\u51fa\u7684\u7ed3\u679c\u662f\u7528\u65e0\u4eba\u8f66\u6536\u96c6\u7684\u6570\u636e\u80fd\u591f\u6269\u5c55\u5230\u65e0\u4eba\u673a\u4f7f\u7528\u3002","title":"DroNet: Learning to Fly by Driving"},{"location":"other_categories/Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/#dronet-learning-to-fly-by-driving","text":"\u6838\u5fc3\u662f\u4e00\u4e2a\u6a21\u4eff\u5b66\u4e60\uff0c\u8f93\u51fa\u8235\u89d2\u4ee5\u53ca\u78b0\u649e\u6982\u7387\uff0c\u78b0\u649e\u6982\u7387\u6765\u81ea\u4e8e\u4f5c\u8005\u989d\u5916\u624b\u5de5\u6807\u6ce8\u7684\u6570\u636e\u96c6\u3002 \u7a81\u51fa\u7684\u7ed3\u679c\u662f\u7528\u65e0\u4eba\u8f66\u6536\u96c6\u7684\u6570\u636e\u80fd\u591f\u6269\u5c55\u5230\u65e0\u4eba\u673a\u4f7f\u7528\u3002","title":"DroNet: Learning to Fly by Driving"},{"location":"other_categories/Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/","text":"End-to-end Driving Deploying through Uncertainty-Aware Imitation Learning and Stochastic Visual Domain Adaptation \u8fd9\u662f\u4e00\u7bc7\u672c\u5b9e\u9a8c\u5ba4\u5e08\u5144\u7684\u4e00\u7bc7\u6587\u7ae0\u3002\u8bb2\u7684\u662f\u7aef\u5230\u7aef\u7684\u5e26\u6709\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u7684\u6a21\u4eff\u5b66\u4e60\u4ee5\u53catransfer learning\u7684\u4f5c\u7528 \u5e26\u6709\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u7684\u6a21\u4eff\u5b66\u4e60 \u5c06\u56de\u5f52\u4efb\u52a1\u8f6c\u5316\u4e3a\u4ee5\u4e0b\u65b9\u7a0b: [y, \\sigma] = f^\\theta(x) L(\\theta)=\\frac{1}{2} \\frac{||y-\\hat y||^2}{\\sigma^2} + \\frac{1}{2}log \\sigma^2 \u65b9\u7a0b\u4e00\u662f\u795e\u7ecf\u7f51\u7edc\u7684forward pass\uff0c\u65b9\u7a0b\u4e8c\u662f\u56de\u5f52\u7684cost function\u3002 real-to-sim \u8f6c\u6362 \u5e08\u5144\u7684\u53e6\u4e00\u7bc7\u8bba\u6587\u53c8\u63d0\u5230\u4f7f\u7528Real-to-sim\u7684\u5b9e\u65f6\u8f6c\u6362\uff0c\u53ef\u4ee5\u8ba9\u5728simulation\u4e0b\u8bad\u7ec3\u7684agent\u5728\u9762\u5bf9\u73b0\u5b9e\u5f97\u5230\u7684\u56fe\u7247\u65f6\u6709\u66f4\u597d\u7684\u8868\u73b0\u3002\u672c\u6587\u7684\u601d\u8def\u662f\u5c06\u6d4b\u8bd5domain\u8f6c\u6362\u5230\u8bad\u7ec3\u7684\u4e09\u4e2adomain\u4e2d\uff0c\u5e76\u5206\u522b\u7ed9\u51fa\u8f93\u51fa\u3002","title":"End-to-end Driving Deploying through Uncertainty-Aware Imitation Learning and Stochastic Visual Domain Adaptation"},{"location":"other_categories/Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/#end-to-end-driving-deploying-through-uncertainty-aware-imitation-learning-and-stochastic-visual-domain-adaptation","text":"\u8fd9\u662f\u4e00\u7bc7\u672c\u5b9e\u9a8c\u5ba4\u5e08\u5144\u7684\u4e00\u7bc7\u6587\u7ae0\u3002\u8bb2\u7684\u662f\u7aef\u5230\u7aef\u7684\u5e26\u6709\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u7684\u6a21\u4eff\u5b66\u4e60\u4ee5\u53catransfer learning\u7684\u4f5c\u7528","title":"End-to-end Driving Deploying through Uncertainty-Aware Imitation Learning and Stochastic Visual Domain Adaptation"},{"location":"other_categories/Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/#_1","text":"\u5c06\u56de\u5f52\u4efb\u52a1\u8f6c\u5316\u4e3a\u4ee5\u4e0b\u65b9\u7a0b: [y, \\sigma] = f^\\theta(x) L(\\theta)=\\frac{1}{2} \\frac{||y-\\hat y||^2}{\\sigma^2} + \\frac{1}{2}log \\sigma^2 \u65b9\u7a0b\u4e00\u662f\u795e\u7ecf\u7f51\u7edc\u7684forward pass\uff0c\u65b9\u7a0b\u4e8c\u662f\u56de\u5f52\u7684cost function\u3002","title":"\u5e26\u6709\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u7684\u6a21\u4eff\u5b66\u4e60"},{"location":"other_categories/Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/#real-to-sim","text":"\u5e08\u5144\u7684\u53e6\u4e00\u7bc7\u8bba\u6587\u53c8\u63d0\u5230\u4f7f\u7528Real-to-sim\u7684\u5b9e\u65f6\u8f6c\u6362\uff0c\u53ef\u4ee5\u8ba9\u5728simulation\u4e0b\u8bad\u7ec3\u7684agent\u5728\u9762\u5bf9\u73b0\u5b9e\u5f97\u5230\u7684\u56fe\u7247\u65f6\u6709\u66f4\u597d\u7684\u8868\u73b0\u3002\u672c\u6587\u7684\u601d\u8def\u662f\u5c06\u6d4b\u8bd5domain\u8f6c\u6362\u5230\u8bad\u7ec3\u7684\u4e09\u4e2adomain\u4e2d\uff0c\u5e76\u5206\u522b\u7ed9\u51fa\u8f93\u51fa\u3002","title":"real-to-sim \u8f6c\u6362"},{"location":"other_categories/Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/","text":"Gaze Training by Modulated Dropout Improves Imitation Learning \u8fd9\u7bc7\u8bba\u6587\u6e90\u81ea\u4e8e\u5b9e\u9a8c\u5ba4\u5e08\u59d0\u3002 \u6838\u5fc3\u8d21\u732e\uff0c\u7528encoder-decoder\u8bad\u7ec3\u4e00\u4e2aGaze_map\u751f\u6210\u7f51\u7edc(\u6570\u636e\u6765\u81ea\u4e8e\u4eba\u5de5\u6807\u6ce8),\u7136\u540e\u5728\u6a21\u4eff\u5b66\u4e60\u7684\u65f6\u5019\u4f7f\u7528Gaze-modulated Dropout,\u8fd9\u4e2a\u6a21\u5757\u7684\u601d\u8def\u662f\u5728\u4f7f\u7528dropout\u7684\u65f6\u5019\uff0c\u51cf\u5c11gaze_map\u76f8\u5173\u90e8\u5206\u7684dropout\uff0c\u5c31\u50cf\u4eba\u773c\u6ce8\u89c6\u5355\u4e00\u533a\u57df\u4e00\u6837\u3002\u6ce8\u610f\u8fd9\u91cc\u4f7f\u7528\u7684dropout\u662ftensorflow\u9ed8\u8ba4\u7248\u672c\u7684dropout\u800c\u4e0d\u662fpytorch\u9ed8\u8ba4\u7684dropout2d(spatial-dropout).","title":"Gaze Training by Modulated Dropout Improves Imitation Learning"},{"location":"other_categories/Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/#gaze-training-by-modulated-dropout-improves-imitation-learning","text":"\u8fd9\u7bc7\u8bba\u6587\u6e90\u81ea\u4e8e\u5b9e\u9a8c\u5ba4\u5e08\u59d0\u3002 \u6838\u5fc3\u8d21\u732e\uff0c\u7528encoder-decoder\u8bad\u7ec3\u4e00\u4e2aGaze_map\u751f\u6210\u7f51\u7edc(\u6570\u636e\u6765\u81ea\u4e8e\u4eba\u5de5\u6807\u6ce8),\u7136\u540e\u5728\u6a21\u4eff\u5b66\u4e60\u7684\u65f6\u5019\u4f7f\u7528Gaze-modulated Dropout,\u8fd9\u4e2a\u6a21\u5757\u7684\u601d\u8def\u662f\u5728\u4f7f\u7528dropout\u7684\u65f6\u5019\uff0c\u51cf\u5c11gaze_map\u76f8\u5173\u90e8\u5206\u7684dropout\uff0c\u5c31\u50cf\u4eba\u773c\u6ce8\u89c6\u5355\u4e00\u533a\u57df\u4e00\u6837\u3002\u6ce8\u610f\u8fd9\u91cc\u4f7f\u7528\u7684dropout\u662ftensorflow\u9ed8\u8ba4\u7248\u672c\u7684dropout\u800c\u4e0d\u662fpytorch\u9ed8\u8ba4\u7684dropout2d(spatial-dropout).","title":"Gaze Training by Modulated Dropout Improves Imitation Learning"},{"location":"other_categories/Deep-Navigation/ModEL/","text":"ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving \u672c\u6587\u7684\u4e00\u4e2a\u601d\u8def\u5728\u4e8e\u901a\u8fc7\u628a\u5f3a\u5316\u5b66\u4e60\u7684\u8f93\u5165\u8f93\u51fa\u6a21\u5757\u5316\u5904\u7406,\u4f7f\u5f97\u6a21\u578b\u8fc1\u79fb\u6027\u66f4\u5f3a. \u6a21\u5757\u5982\u56fe,\u8f93\u5165\u901a\u8fc7\u8bed\u4e49\u5206\u5272\u5212\u5f52\u4e3a\u7a33\u5b9a\u7684\u57fa\u4e8e\u7c7b\u522b\u7684\u5206\u7c7b\u6a21,\u8f93\u51fa\u5219\u7531\u73b0\u5b9e\u4e2d\u7684PID\u63a7\u5236\u5668\u8fdb\u884c\u9694\u79bb.","title":"ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving"},{"location":"other_categories/Deep-Navigation/ModEL/#model-a-modularized-end-to-end-reinforcement-learning-framework-for-autonomous-driving","text":"\u672c\u6587\u7684\u4e00\u4e2a\u601d\u8def\u5728\u4e8e\u901a\u8fc7\u628a\u5f3a\u5316\u5b66\u4e60\u7684\u8f93\u5165\u8f93\u51fa\u6a21\u5757\u5316\u5904\u7406,\u4f7f\u5f97\u6a21\u578b\u8fc1\u79fb\u6027\u66f4\u5f3a. \u6a21\u5757\u5982\u56fe,\u8f93\u5165\u901a\u8fc7\u8bed\u4e49\u5206\u5272\u5212\u5f52\u4e3a\u7a33\u5b9a\u7684\u57fa\u4e8e\u7c7b\u522b\u7684\u5206\u7c7b\u6a21,\u8f93\u51fa\u5219\u7531\u73b0\u5b9e\u4e2d\u7684PID\u63a7\u5236\u5668\u8fdb\u884c\u9694\u79bb.","title":"ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving"},{"location":"other_categories/Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/","text":"Robustness to Out-of-Distribution Inputs via Task-Aware Generative Uncertainty \u8d1d\u53f6\u65af\u7406\u8bba\u80cc\u666f \u7f51\u7edc\u8bad\u7ec3\u7684\u65f6\u5019\u76f8\u5f53\u4e8e\u5bfb\u627e\u53c2\u6570 w \u6ee1\u8db3 w_{MAP} =argmax_{w}\\sum_ip(y_i|x_i,A_i,w) \u5176\u4e2d y,x,A,w \u5206\u522b\u4e3a\u89c2\u6d4b\u503c\uff0c\u884c\u52a8\u5e8f\u5217\u4ee5\u53ca\u53c2\u6570\u77e2\u91cf\u3002\u8fd9\u4e2a\u65b9\u6cd5\u53ef\u4ee5\u4f30\u6d4b\u8f93\u51fa\u7ed3\u679c\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u4f46\u662f\u4e0d\u80fd\u4f30\u8ba1\u6a21\u578b\u672c\u8eab\u53c2\u6570\u9009\u62e9\u7684\u4e0d\u786e\u5b9a\u6027\u3002 p(y|x,A) = \\int p(y|x,A,w)p(w|D_{train})dw \u7406\u8bba\u4e0a\u6765\u8bf4\u8fd9\u4e2a\u79ef\u5206\u662f\u65e0\u6cd5\u6c42\u89e3\u7684\u3002 \u5c3d\u7ba1\u8d1d\u53f6\u65af\u65b9\u6cd5\u53ef\u4ee5\u5904\u7406\u7531\u4e8e\u6570\u636e\u91cf\u4e0d\u8db3\u5f15\u8d77\u7684\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\uff0c\u4f46\u662f\u5bf9\u4e8e\u4e0etraining data\u5206\u5e03\u4e0d\u540c\u7684\u8f93\u5165\uff0c\u5176\u4f30\u8ba1\u503c\u4e0d\u591f\u51c6\u786e\u3002\u8fd9\u4e9b\u60c5\u51b5\u8bb0\u4e3a p(y|x^*,A) \u7f51\u7edc\u4e0e\u5b9e\u9645\u7b97\u6cd5 \u8bad\u7ec3\u65f6\uff1a \u6839\u636e\u8f93\u5165\u56fe\u7247\uff0c\u8bad\u7ec3\u4e00\u4e2a VAE \uff0c\u53e6\u5916\u6b63\u5e38\u5730\u4f7f\u7528\u539f\u6765\u9700\u8981\u7684\u6a21\u578b(\u672c\u6587\u4e3a\u4e00\u4e2a\u5377\u79ef+LSTM\u6a21\u578b)\u5f97\u5230 p(y|x,A) \u6d4b\u8bd5\u65f6: \u5c06\u6d4b\u8bd5\u8f93\u5165\u653e\u5230VAE\u4e2d\uff0c\u5728\u4e2d\u95f4\u9690\u5c42\u4e2d\u591a\u6b21\u91c7\u6837\u5f97\u5230\u591a\u4e2a\u8f93\u51fa\u56fe\u7247\uff0c\u6b63\u5e38\u5730\u4f7f\u7528\u539f\u6765\u7684\u6a21\u578b\u5f97\u5230\u591a\u4e2a\u8f93\u51fa\u7ed3\u679c\uff0c\u5bf9\u5176\u6c42\u5747\u503c\u4e0e\u65b9\u5dee\u5f97\u5230\u5bf9\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u7684\u4f30\u8ba1 \u80cc\u540e\u7684\u4e00\u4e9b\u5206\u6790 \u672c\u6587\u4e00\u4e2a\u4e0e\u5176\u4ed6\u6587\u7ae0\u4e0d\u540c\u7684\u7406\u7531\u662f\uff0c\u4e0d\u5e94\u8be5\u5355\u7eaf\u5730\u56e0\u4e3a\u6d4b\u8bd5\u65f6\u8f93\u5165\u56fe\u7247\u4e0e\u8bad\u7ec3\u65f6\u4e0d\u540c\u5c31\u5224\u65ad\u7ed3\u679c\u4f1a\u6709\u5f88\u5927\u7684\u4e0d\u786e\u5b9a\u6027\u3002\u7b80\u5355\u7684\u4f8b\u5b50\u6bd4\u5982\u8bf4\u5929\u82b1\u677f\u7684\u989c\u8272\uff0c\u5982\u679c\u6d4b\u8bd5\u65f6\u4e0e\u8bad\u7ec3\u65f6\u4e0d\u540c\uff0c\u4e00\u4e2a\u6b63\u5e38\u597d\u7684\u673a\u5668\u4eba\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4e0d\u4f1a\u56e0\u6b64\u5f97\u5230\u4e0d\u540c\u7684\u884c\u52a8\u8f93\u51fa\uff0c\"\u8f93\u51fa\u7ed3\u679c\u7684\u4e0d\u786e\u5b9a\u6027\u4e0d\u4f1a\u56e0\u65e0\u5173\u90e8\u5206\u7684\u533a\u522b\u800c\u63d0\u5347\"\u3002\u8fd9\u4e2a\u60c5\u51b5\u5728\u672c\u6587\u7684\u6a21\u578b\u4e2d\u4e5f\u6709\u6240\u4f53\u73b0\uff0c\u4f5c\u8005\u5b9e\u9a8c\u5ba4\u80fd\u53d1\u73b0VAE\u6d4b\u8bd5\u65f6\u8f93\u51fa\u7684\u56fe\u7247\u7684\u5929\u82b1\u677f\u989c\u8272\u4e0e\u5b9e\u9645\u7684\u8f93\u5165\u4e0d\u7b26\u5408\uff0c\u4f46\u662f\u6700\u7ec8\u8f93\u51fa\u7684\u884c\u52a8\u5dee\u522b\u4e0d\u5927\uff0c\u4e5f\u5c31\u4e0d\u4f1a\u63d0\u5347\u5bf9\u5e94\u7684\u4e0d\u786e\u5b9a\u6027\u3002\u8fd9\u4e5f\u662f\u4f5c\u8005\u5728\u4f30\u8ba1\u4e0d\u786e\u5b9a\u6027\u7684\u65f6\u5019\uff0c\u575a\u6301\u8981\u53bb\u5230\u6700\u540e\u884c\u52a8\u8f93\u51fa\u65f6\u518d\u4f30\u8ba1\u4e0d\u786e\u5b9a\u6027\uff0c\u800c\u4e0d\u662f\u4ece\u56fe\u50cf\u7684\u8f93\u5165\u5c31\u5f00\u59cb\u4f30\u8ba1\u3002","title":"Robustness to Out-of-Distribution Inputs via Task-Aware Generative Uncertainty"},{"location":"other_categories/Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/#robustness-to-out-of-distribution-inputs-via-task-aware-generative-uncertainty","text":"","title":"Robustness to Out-of-Distribution Inputs via Task-Aware Generative Uncertainty"},{"location":"other_categories/Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/#_1","text":"\u7f51\u7edc\u8bad\u7ec3\u7684\u65f6\u5019\u76f8\u5f53\u4e8e\u5bfb\u627e\u53c2\u6570 w \u6ee1\u8db3 w_{MAP} =argmax_{w}\\sum_ip(y_i|x_i,A_i,w) \u5176\u4e2d y,x,A,w \u5206\u522b\u4e3a\u89c2\u6d4b\u503c\uff0c\u884c\u52a8\u5e8f\u5217\u4ee5\u53ca\u53c2\u6570\u77e2\u91cf\u3002\u8fd9\u4e2a\u65b9\u6cd5\u53ef\u4ee5\u4f30\u6d4b\u8f93\u51fa\u7ed3\u679c\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u4f46\u662f\u4e0d\u80fd\u4f30\u8ba1\u6a21\u578b\u672c\u8eab\u53c2\u6570\u9009\u62e9\u7684\u4e0d\u786e\u5b9a\u6027\u3002 p(y|x,A) = \\int p(y|x,A,w)p(w|D_{train})dw \u7406\u8bba\u4e0a\u6765\u8bf4\u8fd9\u4e2a\u79ef\u5206\u662f\u65e0\u6cd5\u6c42\u89e3\u7684\u3002 \u5c3d\u7ba1\u8d1d\u53f6\u65af\u65b9\u6cd5\u53ef\u4ee5\u5904\u7406\u7531\u4e8e\u6570\u636e\u91cf\u4e0d\u8db3\u5f15\u8d77\u7684\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\uff0c\u4f46\u662f\u5bf9\u4e8e\u4e0etraining data\u5206\u5e03\u4e0d\u540c\u7684\u8f93\u5165\uff0c\u5176\u4f30\u8ba1\u503c\u4e0d\u591f\u51c6\u786e\u3002\u8fd9\u4e9b\u60c5\u51b5\u8bb0\u4e3a p(y|x^*,A)","title":"\u8d1d\u53f6\u65af\u7406\u8bba\u80cc\u666f"},{"location":"other_categories/Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/#_2","text":"\u8bad\u7ec3\u65f6\uff1a \u6839\u636e\u8f93\u5165\u56fe\u7247\uff0c\u8bad\u7ec3\u4e00\u4e2a VAE \uff0c\u53e6\u5916\u6b63\u5e38\u5730\u4f7f\u7528\u539f\u6765\u9700\u8981\u7684\u6a21\u578b(\u672c\u6587\u4e3a\u4e00\u4e2a\u5377\u79ef+LSTM\u6a21\u578b)\u5f97\u5230 p(y|x,A) \u6d4b\u8bd5\u65f6: \u5c06\u6d4b\u8bd5\u8f93\u5165\u653e\u5230VAE\u4e2d\uff0c\u5728\u4e2d\u95f4\u9690\u5c42\u4e2d\u591a\u6b21\u91c7\u6837\u5f97\u5230\u591a\u4e2a\u8f93\u51fa\u56fe\u7247\uff0c\u6b63\u5e38\u5730\u4f7f\u7528\u539f\u6765\u7684\u6a21\u578b\u5f97\u5230\u591a\u4e2a\u8f93\u51fa\u7ed3\u679c\uff0c\u5bf9\u5176\u6c42\u5747\u503c\u4e0e\u65b9\u5dee\u5f97\u5230\u5bf9\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u7684\u4f30\u8ba1","title":"\u7f51\u7edc\u4e0e\u5b9e\u9645\u7b97\u6cd5"},{"location":"other_categories/Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/#_3","text":"\u672c\u6587\u4e00\u4e2a\u4e0e\u5176\u4ed6\u6587\u7ae0\u4e0d\u540c\u7684\u7406\u7531\u662f\uff0c\u4e0d\u5e94\u8be5\u5355\u7eaf\u5730\u56e0\u4e3a\u6d4b\u8bd5\u65f6\u8f93\u5165\u56fe\u7247\u4e0e\u8bad\u7ec3\u65f6\u4e0d\u540c\u5c31\u5224\u65ad\u7ed3\u679c\u4f1a\u6709\u5f88\u5927\u7684\u4e0d\u786e\u5b9a\u6027\u3002\u7b80\u5355\u7684\u4f8b\u5b50\u6bd4\u5982\u8bf4\u5929\u82b1\u677f\u7684\u989c\u8272\uff0c\u5982\u679c\u6d4b\u8bd5\u65f6\u4e0e\u8bad\u7ec3\u65f6\u4e0d\u540c\uff0c\u4e00\u4e2a\u6b63\u5e38\u597d\u7684\u673a\u5668\u4eba\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4e0d\u4f1a\u56e0\u6b64\u5f97\u5230\u4e0d\u540c\u7684\u884c\u52a8\u8f93\u51fa\uff0c\"\u8f93\u51fa\u7ed3\u679c\u7684\u4e0d\u786e\u5b9a\u6027\u4e0d\u4f1a\u56e0\u65e0\u5173\u90e8\u5206\u7684\u533a\u522b\u800c\u63d0\u5347\"\u3002\u8fd9\u4e2a\u60c5\u51b5\u5728\u672c\u6587\u7684\u6a21\u578b\u4e2d\u4e5f\u6709\u6240\u4f53\u73b0\uff0c\u4f5c\u8005\u5b9e\u9a8c\u5ba4\u80fd\u53d1\u73b0VAE\u6d4b\u8bd5\u65f6\u8f93\u51fa\u7684\u56fe\u7247\u7684\u5929\u82b1\u677f\u989c\u8272\u4e0e\u5b9e\u9645\u7684\u8f93\u5165\u4e0d\u7b26\u5408\uff0c\u4f46\u662f\u6700\u7ec8\u8f93\u51fa\u7684\u884c\u52a8\u5dee\u522b\u4e0d\u5927\uff0c\u4e5f\u5c31\u4e0d\u4f1a\u63d0\u5347\u5bf9\u5e94\u7684\u4e0d\u786e\u5b9a\u6027\u3002\u8fd9\u4e5f\u662f\u4f5c\u8005\u5728\u4f30\u8ba1\u4e0d\u786e\u5b9a\u6027\u7684\u65f6\u5019\uff0c\u575a\u6301\u8981\u53bb\u5230\u6700\u540e\u884c\u52a8\u8f93\u51fa\u65f6\u518d\u4f30\u8ba1\u4e0d\u786e\u5b9a\u6027\uff0c\u800c\u4e0d\u662f\u4ece\u56fe\u50cf\u7684\u8f93\u5165\u5c31\u5f00\u59cb\u4f30\u8ba1\u3002","title":"\u80cc\u540e\u7684\u4e00\u4e9b\u5206\u6790"},{"location":"other_categories/SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/","text":"Interval-Based Visual-LiDAR Sensor Fusion \u8fd9\u7bc7paper\u662f2021 ICRA Best Paper Award in Robot Vision. Visual-Lidar Fusion Motivation \u89c6\u89c9\u7684\u7279\u5f81\u597d\u5904\u662f\u5bb9\u6613\u91cd\u8bc6\u522b\uff0c\u6240\u4ee5\u53ef\u4ee5\u5728sequence\u4e0a\u5bf9\u540c\u4e00\u70b9\u7b80\u5355\u7684\u91cd\u8bc6\u522b\u4e0e\u5339\u914d(\u70b9\u4e91\u7684\u5339\u914d\u6bd4\u8f83\u56f0\u96be).\u800c\u70b9\u4e91\u7684\u597d\u5904\u662f\u53ef\u4ee5\u7ed9\u6df1\u5ea6\u3002\u5982\u679c\u662f\u5c06\u6df1\u5ea6assocciate\u5230\u53ef\u4ee5\u88ab\u91cd\u8bc6\u522b\u7684feature\u4e0a\uff0c\u505aVO\u5c31\u53ef\u4ee5\u6709scale\u4e86. Interval Analysis \u8fd9\u7bc7paper\u63d0\u51fa\u4e00\u4e2a\u65b0\u7684\u8bef\u5dee\u5206\u6790\uff0c\u4ee5\u524d\u662f\u7528\u9ad8\u65af\u53bb\u5efa\u6a21\u7684\uff0c\u73b0\u5728\u7528boundary\u6765\u5206\u6790\uff0c\u6307\u771f\u503c\u5e94\u5f53\u662f\u5728\u4e00\u4e2a\u533a\u95f4\u5185. \u800c\u533a\u95f4\u4e4b\u95f4\u7684\u5408\u5e76\u8fd0\u7b97\u9700\u8981interval analysis [-4,3]-[1,5]=[-4-5,3-1]=[-9,2] Sensor Error Model LiDAR\u7684\u70b9\u8bef\u5dee\u53ef\u80fd\u6709\u4e24\u4e2a\u89d2\u5ea6\u8bef\u5dee\u4ee5\u53ca\u8ddd\u79bb\u6d4b\u91cf\u8bef\u5dee.\u6211\u4eec\u53ef\u4ee5\u7b97\u51fa\u5728\u6b27\u51e0\u91cc\u5f97\u7684\u5750\u6807\u7cfb\u4e0b\u7684\u4e00\u4e2a\u7acb\u65b9\u4f53bounding box, Camera\u7684\u8bef\u5dee\u6765\u81ea\u4e8e\u50cf\u7d20\u5339\u914d\u7684\u65f6\u5019\u7684\u8ddd\u79bb\uff0c\u8fd9\u91cc\u7528\u4e00\u4e2a\u65b9\u5f62\u7684bounding box\u5efa\u6a21","title":"Interval-Based Visual-LiDAR Sensor Fusion"},{"location":"other_categories/SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/#interval-based-visual-lidar-sensor-fusion","text":"\u8fd9\u7bc7paper\u662f2021 ICRA Best Paper Award in Robot Vision.","title":"Interval-Based Visual-LiDAR Sensor Fusion"},{"location":"other_categories/SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/#visual-lidar-fusion-motivation","text":"\u89c6\u89c9\u7684\u7279\u5f81\u597d\u5904\u662f\u5bb9\u6613\u91cd\u8bc6\u522b\uff0c\u6240\u4ee5\u53ef\u4ee5\u5728sequence\u4e0a\u5bf9\u540c\u4e00\u70b9\u7b80\u5355\u7684\u91cd\u8bc6\u522b\u4e0e\u5339\u914d(\u70b9\u4e91\u7684\u5339\u914d\u6bd4\u8f83\u56f0\u96be).\u800c\u70b9\u4e91\u7684\u597d\u5904\u662f\u53ef\u4ee5\u7ed9\u6df1\u5ea6\u3002\u5982\u679c\u662f\u5c06\u6df1\u5ea6assocciate\u5230\u53ef\u4ee5\u88ab\u91cd\u8bc6\u522b\u7684feature\u4e0a\uff0c\u505aVO\u5c31\u53ef\u4ee5\u6709scale\u4e86.","title":"Visual-Lidar Fusion Motivation"},{"location":"other_categories/SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/#interval-analysis","text":"\u8fd9\u7bc7paper\u63d0\u51fa\u4e00\u4e2a\u65b0\u7684\u8bef\u5dee\u5206\u6790\uff0c\u4ee5\u524d\u662f\u7528\u9ad8\u65af\u53bb\u5efa\u6a21\u7684\uff0c\u73b0\u5728\u7528boundary\u6765\u5206\u6790\uff0c\u6307\u771f\u503c\u5e94\u5f53\u662f\u5728\u4e00\u4e2a\u533a\u95f4\u5185. \u800c\u533a\u95f4\u4e4b\u95f4\u7684\u5408\u5e76\u8fd0\u7b97\u9700\u8981interval analysis [-4,3]-[1,5]=[-4-5,3-1]=[-9,2]","title":"Interval Analysis"},{"location":"other_categories/SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/#sensor-error-model","text":"LiDAR\u7684\u70b9\u8bef\u5dee\u53ef\u80fd\u6709\u4e24\u4e2a\u89d2\u5ea6\u8bef\u5dee\u4ee5\u53ca\u8ddd\u79bb\u6d4b\u91cf\u8bef\u5dee.\u6211\u4eec\u53ef\u4ee5\u7b97\u51fa\u5728\u6b27\u51e0\u91cc\u5f97\u7684\u5750\u6807\u7cfb\u4e0b\u7684\u4e00\u4e2a\u7acb\u65b9\u4f53bounding box, Camera\u7684\u8bef\u5dee\u6765\u81ea\u4e8e\u50cf\u7d20\u5339\u914d\u7684\u65f6\u5019\u7684\u8ddd\u79bb\uff0c\u8fd9\u91cc\u7528\u4e00\u4e2a\u65b9\u5f62\u7684bounding box\u5efa\u6a21","title":"Sensor Error Model"},{"location":"other_categories/SLAM/BAD-SLAM/","text":"BAD SLAM: Bundle Adjusted Direct RGB-D SLAM \u77e5\u4e4e\u89e3\u8bfb CSDN\u89e3\u8bfb \u8fd9\u7bc7paper\u628abundle adjustment\u7528\u5728\u7a20\u5bc6\u7684RGBD\u9884\u6d4b\u4e4b\u4e2d\uff0c\u5728\u540e\u7aef\u540c\u65f6\u4f18\u5316\u91cd\u5efa\u7684\u6a21\u578b\u548c\u76f8\u673a\u7684\u6240\u6709\u53c2\u6570(GPU\u7248\u672c)\u3002 Front end \u524d\u7aef\u4e0d\u662f\u672c\u6587\u7ae0\u7684\u4e3b\u8981\u5185\u5bb9\uff0c\u53ea\u6709\u7b80\u5355\u7684\u4ecb\u7ecd \u9884\u5904\u7406\uff0c\u4f7f\u7528\u53cc\u8fb9\u6ee4\u6ce2 keyframe\u7684\u9009\u62e9\uff0c\u6bcf\u5341\u5e27\u9009\u62e9\u4e00\u5e27\u4f5c\u4e3akeyframe.\u6bcf\u4e00\u5e27\u4f1a\u4f30\u8ba1\u76f8\u5bf9\u4e0a\u4e00\u4e2a\u5173\u952e\u5e27\u7684\u4f4d\u59ff\uff0calignment\u6bd4\u5bf9\u65f6\u91c7\u7528\u7684\u662fRGB\u7684\u68af\u5ea6\u800c\u4e0d\u662fRGB\u6570\u503c(\u76f8\u5f53\u4e8e\u662fedge\u7684\u5bf9\u9f50)\u3002 \u56de\u73af\u68c0\u6d4b,\u91c7\u7528bag-of-words\u7684 \u65b9\u6cd5 , code code2 Back end Surfel surfel \u672c\u6587\u7684\u5b9a\u4e49\u6765\u8bf4\uff0c\u5b9e\u8d28\u4e0a\u662f\u6709\u65b9\u5411\u7684\u5706\u76d8. \u9996\u5148\u628akeyframe\u5206\u6210\u4e00\u4e2a4x4\u7684\u5355\u5143\uff0c\u5982\u679c\u5355\u5143\u683c\u4e2d\u6ca1\u6709\u50cf\u7d20\u5bf9\u5e94\u73b0\u6709\u7684surfel\u3002surfel \u75313D\u4e2d\u5fc3\u70b9 p_s (\u9009\u4e2d\u70b9\u6295\u5f71\u5230\u8f6c\u6362\u5230\u4e16\u754c\u5750\u6807)\uff0c\u8868\u9762\u6cd5\u5411\u91cf (n_s) (\u6df1\u5ea6\u56fe\u7684\u9644\u8fd1\u7684\u6709\u9650\u5dee\u5206)\uff0c\u534a\u5f84 r_s (\u5f53\u524d\u70b9\u4e0e\u56db\u4e2a\u90bb\u57df\u50cf\u7d20\u76843D\u70b9\u4e4b\u95f4\u7684\u6700\u5c0f\u8ddd\u79bb)\uff0c\u6807\u91cf\u89c6\u89c9\u63cf\u8ff0\u5b50\u6784\u6210 d_s \u3002\u6784\u6210 surfel\u7684\u4f4d\u7f6e\u70b9\u5728\u540e\u7aef\u4f18\u5316\u7684\u65f6\u5019\u53ea\u53ef\u4ee5\u6cbf\u7740\u6cd5\u5411\u91cf\u66f4\u65b0\u3002\u56e0\u6b64\u76f8\u5f53\u4e8e\u53ea\u4f18\u5316\u4e00\u4e2a\u53c2\u6570 p_s + t * n_s \u91cc\u9762\u7684t. \u63cf\u8ff0\u5b50\u968f\u7740\u4f18\u5316\u6b63\u5e38\u4f18\u5316\u3002 surfel\u5408\u5e76\uff0c\u5728\u6bcf\u4e00\u6b21\u8fed\u4ee3\u4f18\u5316\u4e4b\u540e\uff0c\u9700\u8981\u5c06\u76f8\u4f3c\u7684surfel\u5408\u5e76\u3002\u5c06surfel\u6295\u5f71\u5230\u6240\u6709\u5173\u952e\u5e27\u4e2d\uff0c\u6295\u5f71\u5230\u540c\u4e00\u4e2a\u5355\u5143\u683c\u7684surfel\u8fdb\u884c\u5408\u5e76\u3002\u534a\u5f84\u4e3a\u76f8\u5e94\u6d4b\u91cf\u503c\u7684\u6700\u5c0f\u534a\u5f84\u3002\u8fc7\u6ee4\u5f02\u5e38\u534a\u5f84\u3002 Back-end Cost Function C(K, S)=\\sum_{k \\in K} \\sum_{s \\in S_{k}}\\left(\\rho_{\\text {Tukey }}(\\sigma_{D}^{-1} r_{\\text {geom }}(s, k)\\right)+ w_{photo} \\rho_{Huber}(\\sigma^{-1}_p r_{photo}(s,k)) r_{geom} \u51e0\u4f55\u7ed3\u6784\u8ba1\u7b97\u7684\u662f\u6d4b\u91cf\u4e0esurfel\u6cd5\u5411\u7684\u8ddd\u79bb\u3002 r_{\\text {geom }}(s, k)=\\left(\\mathbf{T}_{G}^{k} \\mathbf{n}_{s}\\right)^{T}\\left(\\pi_{D, k}^{-1}\\left(\\hat{\\pi}_{D, k}\\left(\\mathbf{T}_{G}^{k} \\mathbf{p}_{s}\\right)\\right)-\\mathbf{T}_{G}^{k} \\mathbf{p}_{s}\\right) r_{photo} \u8ba1\u7b97\u63cf\u8ff0\u5b50\u7684\u5dee\u8ddd\u3002 r_{\\text {photo }}(s, k)=\\left\\|\\left(\\begin{array}{l} I\\left(\\pi_{I, k}\\left(\\mathbf{s}_{1}\\right)\\right)-I\\left(\\pi_{I, k}\\left(\\mathbf{p}_{s}\\right)\\right) \\\\ I\\left(\\pi_{I, k}\\left(\\mathbf{s}_{2}\\right)\\right)-I\\left(\\pi_{I, k}\\left(\\mathbf{p}_{s}\\right)\\right) \\end{array}\\right)\\right\\|_{2}-d_{s}","title":"BAD SLAM: Bundle Adjusted Direct RGB-D SLAM"},{"location":"other_categories/SLAM/BAD-SLAM/#bad-slam-bundle-adjusted-direct-rgb-d-slam","text":"\u77e5\u4e4e\u89e3\u8bfb CSDN\u89e3\u8bfb \u8fd9\u7bc7paper\u628abundle adjustment\u7528\u5728\u7a20\u5bc6\u7684RGBD\u9884\u6d4b\u4e4b\u4e2d\uff0c\u5728\u540e\u7aef\u540c\u65f6\u4f18\u5316\u91cd\u5efa\u7684\u6a21\u578b\u548c\u76f8\u673a\u7684\u6240\u6709\u53c2\u6570(GPU\u7248\u672c)\u3002","title":"BAD SLAM: Bundle Adjusted Direct RGB-D SLAM"},{"location":"other_categories/SLAM/BAD-SLAM/#front-end","text":"\u524d\u7aef\u4e0d\u662f\u672c\u6587\u7ae0\u7684\u4e3b\u8981\u5185\u5bb9\uff0c\u53ea\u6709\u7b80\u5355\u7684\u4ecb\u7ecd \u9884\u5904\u7406\uff0c\u4f7f\u7528\u53cc\u8fb9\u6ee4\u6ce2 keyframe\u7684\u9009\u62e9\uff0c\u6bcf\u5341\u5e27\u9009\u62e9\u4e00\u5e27\u4f5c\u4e3akeyframe.\u6bcf\u4e00\u5e27\u4f1a\u4f30\u8ba1\u76f8\u5bf9\u4e0a\u4e00\u4e2a\u5173\u952e\u5e27\u7684\u4f4d\u59ff\uff0calignment\u6bd4\u5bf9\u65f6\u91c7\u7528\u7684\u662fRGB\u7684\u68af\u5ea6\u800c\u4e0d\u662fRGB\u6570\u503c(\u76f8\u5f53\u4e8e\u662fedge\u7684\u5bf9\u9f50)\u3002 \u56de\u73af\u68c0\u6d4b,\u91c7\u7528bag-of-words\u7684 \u65b9\u6cd5 , code code2","title":"Front end"},{"location":"other_categories/SLAM/BAD-SLAM/#back-end","text":"","title":"Back end"},{"location":"other_categories/SLAM/BAD-SLAM/#surfel","text":"surfel \u672c\u6587\u7684\u5b9a\u4e49\u6765\u8bf4\uff0c\u5b9e\u8d28\u4e0a\u662f\u6709\u65b9\u5411\u7684\u5706\u76d8. \u9996\u5148\u628akeyframe\u5206\u6210\u4e00\u4e2a4x4\u7684\u5355\u5143\uff0c\u5982\u679c\u5355\u5143\u683c\u4e2d\u6ca1\u6709\u50cf\u7d20\u5bf9\u5e94\u73b0\u6709\u7684surfel\u3002surfel \u75313D\u4e2d\u5fc3\u70b9 p_s (\u9009\u4e2d\u70b9\u6295\u5f71\u5230\u8f6c\u6362\u5230\u4e16\u754c\u5750\u6807)\uff0c\u8868\u9762\u6cd5\u5411\u91cf (n_s) (\u6df1\u5ea6\u56fe\u7684\u9644\u8fd1\u7684\u6709\u9650\u5dee\u5206)\uff0c\u534a\u5f84 r_s (\u5f53\u524d\u70b9\u4e0e\u56db\u4e2a\u90bb\u57df\u50cf\u7d20\u76843D\u70b9\u4e4b\u95f4\u7684\u6700\u5c0f\u8ddd\u79bb)\uff0c\u6807\u91cf\u89c6\u89c9\u63cf\u8ff0\u5b50\u6784\u6210 d_s \u3002\u6784\u6210 surfel\u7684\u4f4d\u7f6e\u70b9\u5728\u540e\u7aef\u4f18\u5316\u7684\u65f6\u5019\u53ea\u53ef\u4ee5\u6cbf\u7740\u6cd5\u5411\u91cf\u66f4\u65b0\u3002\u56e0\u6b64\u76f8\u5f53\u4e8e\u53ea\u4f18\u5316\u4e00\u4e2a\u53c2\u6570 p_s + t * n_s \u91cc\u9762\u7684t. \u63cf\u8ff0\u5b50\u968f\u7740\u4f18\u5316\u6b63\u5e38\u4f18\u5316\u3002 surfel\u5408\u5e76\uff0c\u5728\u6bcf\u4e00\u6b21\u8fed\u4ee3\u4f18\u5316\u4e4b\u540e\uff0c\u9700\u8981\u5c06\u76f8\u4f3c\u7684surfel\u5408\u5e76\u3002\u5c06surfel\u6295\u5f71\u5230\u6240\u6709\u5173\u952e\u5e27\u4e2d\uff0c\u6295\u5f71\u5230\u540c\u4e00\u4e2a\u5355\u5143\u683c\u7684surfel\u8fdb\u884c\u5408\u5e76\u3002\u534a\u5f84\u4e3a\u76f8\u5e94\u6d4b\u91cf\u503c\u7684\u6700\u5c0f\u534a\u5f84\u3002\u8fc7\u6ee4\u5f02\u5e38\u534a\u5f84\u3002","title":"Surfel"},{"location":"other_categories/SLAM/BAD-SLAM/#back-end-cost-function","text":"C(K, S)=\\sum_{k \\in K} \\sum_{s \\in S_{k}}\\left(\\rho_{\\text {Tukey }}(\\sigma_{D}^{-1} r_{\\text {geom }}(s, k)\\right)+ w_{photo} \\rho_{Huber}(\\sigma^{-1}_p r_{photo}(s,k)) r_{geom} \u51e0\u4f55\u7ed3\u6784\u8ba1\u7b97\u7684\u662f\u6d4b\u91cf\u4e0esurfel\u6cd5\u5411\u7684\u8ddd\u79bb\u3002 r_{\\text {geom }}(s, k)=\\left(\\mathbf{T}_{G}^{k} \\mathbf{n}_{s}\\right)^{T}\\left(\\pi_{D, k}^{-1}\\left(\\hat{\\pi}_{D, k}\\left(\\mathbf{T}_{G}^{k} \\mathbf{p}_{s}\\right)\\right)-\\mathbf{T}_{G}^{k} \\mathbf{p}_{s}\\right) r_{photo} \u8ba1\u7b97\u63cf\u8ff0\u5b50\u7684\u5dee\u8ddd\u3002 r_{\\text {photo }}(s, k)=\\left\\|\\left(\\begin{array}{l} I\\left(\\pi_{I, k}\\left(\\mathbf{s}_{1}\\right)\\right)-I\\left(\\pi_{I, k}\\left(\\mathbf{p}_{s}\\right)\\right) \\\\ I\\left(\\pi_{I, k}\\left(\\mathbf{s}_{2}\\right)\\right)-I\\left(\\pi_{I, k}\\left(\\mathbf{p}_{s}\\right)\\right) \\end{array}\\right)\\right\\|_{2}-d_{s}","title":"Back-end Cost Function"},{"location":"other_categories/SLAM/CubeSLAM/","text":"CubeSLAM: Monocular 3D Object SLAM \u8fd9\u7bc7\u6587\u7ae0\u5c06\u5355\u76ee3D\u7269\u4f53\u68c0\u6d4b\u4ee5\u53ca\u5355\u76eeSLAM\u653e\u5728\u540c\u4e00\u4e2a\u6846\u67b6\u91cc\u9762\u4f18\u5316\uff0c\u5f97\u5230\u7684\u7ed3\u679c\u662f\u540c\u65f6\u66f4\u4f18\u5316\u7684SLAM\u7ed3\u679c\u4ee5\u53ca\u68c0\u6d4b\u7ed3\u679c\u3002\u8fd9\u7bc7\u6587\u7ae0\u5c5e\u4e8e\u6240\u8c13\u7269\u4f53\u7ea7\u7684SLAM\u3002 \u5355\u76ee\u56fe\u7247\u7406\u89e3 \u5355\u76ee3D proposal\u751f\u6210 \u672c\u6587\u7684\u601d\u8def\u662f\u901a\u8fc7\u4e00\u4e2a\u89d2\u70b9\u52a0\u4e0a\u6d88\u5931\u70b9&\u65cb\u8f6c\u77e9\u9635\u4ee5\u53ca2D \u68c0\u6d4b\u6846\u8fd8\u539f\u4e09\u7ef4\u7ed3\u679c\u3002 \u7531\u65cb\u8f6c\u77e9\u9635\u5f97\u5230\u957f\u65b9\u5f62\u6846\u4e09\u4e2a\u6d88\u5931\u70b9\u7684\u516c\u5f0f\u4e3a: VP_i = KR_{col(i)}, i\\in \\{1,2,3\\} \u7531\u4e0a\u56fe\u53ef\u77e5,\u4ee5a\u5c0f\u56fe\u4e3a\u4f8b,\u5728\u5df2\u77e5\u6d88\u5931\u70b9\u4ee5\u53ca p_1 \u7684\u60c5\u51b5\u4e0b,\u5176\u4f59\u7684\u70b9\u53ef\u4ee5\u7528\u6c42\u4ea4\u70b9\u7684\u65b9\u5f0f\u5f97\u5230 p_2 = (VP_1, p1) \\times (B, C), p_4 = (VP_2, p1) \\times (A, D), p_3 = (VP_1, p_4)\\times(VP_2, p_2), p_5 = (VP_3, p_3) \\times (C, D), p_6 = (VP_2, p_5) \\times(VP_2, p_5), p_7=(VP_3, p_1)\\times(VP_1, p_6), p_8 = (VP_3, p_4)\\times(VP_2, p_7) \u4e8b\u5b9e\u4e0a\u540e\u9762\u7684\u70b9\u9009\u62e9\u7a7a\u95f4\u5f88\u5927. \u5f97\u5230\u89d2\u70b9\u540e\u8fdb\u4e00\u6b65\u6536\u7f29\u81ea\u7531\u5ea6 \u5bf9\u4e8e\u4efb\u610f\u59ff\u6001\u7684\u7269\u4f53\uff0c\u9009\u53d6\u4e0d\u5171\u9762\u7684\u56db\u4e2a\u89d2\u70b9\u59821,2,4,7\u8fdb\u884cPnP\u6c42\u89e3 \u5bf9\u4e8e\u5730\u9762\u4e0a\u7684\u7269\u4f53\uff0c\u5047\u8bbe\u5176roll,pitch\u89d2\u5ea6\u90fd\u4e3a0\u5ea6\uff0c\u5c31\u53ef\u4ee5\u4e0d\u4f7f\u7528PnP\u6c42\u89e3\u4e86\uff0c\u53ef\u4ee5\u6839\u636e\u76f8\u673a\u9ad8\u5ea6\u5c06\u5e95\u9762\u7684\u89d2\u70b9\u76f4\u63a5\u6295\u5f71\u5230\u4e16\u754c\u5750\u6807\u4e2d \u4e00\u822c\u6765\u8bf4\uff0c\u6211\u4eec\u53ef\u4ee5\u7528\u795e\u7ecf\u7f51\u7edc\u76f4\u63a5\u8f93\u51fa\u89d2\u5ea6\uff0c\u4f46\u662f\u8fd9\u91cc\u4e3a\u4e86\u63d0\u9ad8\u6cdb\u5316\u6027\u80fd\uff0c\u9009\u62e9\u91c7\u6837\u5e76\u5404\u81ea\u8bc4\u5206\u4ee5\u8ba1\u7b97\u65cb\u8f6c\u77e9\u9635\u3002 Proposal \u8bc4\u5206 \u603b\u4f53\u635f\u5931\u51fd\u6570\u4e3a: E(O|I) = \\phi_{dist}(O, I) + w_1\\phi_{angle}(O,I) + w_2 \\phi_{shape}(O) \u5176\u4e2d O \u4e3a\u635f\u5931\u51fd\u6570, w \u4e3a\u6743\u91cd\u8d85\u53c2\u6570 \\phi_{dist} ,\u5bf9\u56fe\u7247\u8fd0\u884ccanny edge\uff0c\u57fa\u4e8e\u6b64\u5efa\u7acbdistance map\uff0c\u5bf9\u6bcf\u4e00\u4e2a\u7ad6\u7ebf\uff0c\u91c7\u683710\u4e2a\u70b9\uff0c\u8ba1\u7b97\u8ddd\u79bb\u56fe\u4e2d\u7684\u8ddd\u79bb\u503c\u603b\u548c\uff0c\u6700\u540e\u5f97\u5206\u9664\u4ee52D\u6846\u7684\u5bf9\u89d2\u7ebf\u957f\u5ea6 \\phi_{angle} \u524d\u4e00\u4e2a\u51fd\u6570\u5bf9false positive\u7684\u5c0f\u76f4\u7ebf\u6bb5\u975e\u5e38\u654f\u611f\uff0c\u9996\u5148\u627e\u51fa\u4e0a\u4e0b\u9762\u7684\u957f\u8fb9\uff0c\u627e\u51fa\u76f8\u5bf9\u6d88\u5931\u70b9\u503e\u659c\u89d2\u6700\u5927\u4e0e\u6700\u5c0f\u7684. \\left\\langle a, b \\right\\rangle \u4e3a\u70b9 a,b \u76f4\u7ebf\u7684\u503e\u659c\u89d2\uff0c \\begin{aligned} \\phi_{\\text {angle}}(O, I)=& \\sum_{i=1: 3}\\left\\|\\left\\langle l_{i_{-} m s}, l_{i_{-} m t}\\right\\rangle-\\left\\langle\\mathrm{VP}_{i}, l_{i_{-} m t}\\right\\rangle\\right\\|+\\\\ &\\left\\|\\left\\langle l_{i_{-} n s}, l_{i_{-} n t}\\right\\rangle-\\left\\langle\\mathrm{VP}_{i}, l_{i_{-} n t}\\right\\rangle\\right\\| \\end{aligned} \u672c\u8d28\u4e0a\u662f\u8981\u6c42\u957f\u8fb9\u7ecf\u8fc7\u6d88\u5931\u70b9 \\phi_{shape} \u5982\u679c\u957f\u5bbd\u6bd4\u5f88\u5927\u7684\uff0c\u7ed9\u4e88\u4e00\u4e2a\u60e9\u7f5a\u3002 SLAM bundle adjustment\u95ee\u9898 \u8bb0\u76f8\u673a\u59ff\u6001\u30013D\u7269\u4f53\u3001\u7279\u5f81\u70b9\u5206\u522b\u4e3a C = \\{C_i\\}, O = \\{O_j\\}, P = \\{P_k\\} , BA\u5c31\u63cf\u8ff0\u4e3a\u4ee5\u4e0b\u7684\u6700\u5c0f\u4e8c\u4e58\u95ee\u9898 \\begin{aligned} C^{*}, O^{*}, P^{*}=& \\underset{\\{C, O, P\\}}{\\arg \\min } \\sum_{C_{i}, O_{j}, P_{k}}\\left\\|\\mathbf{e}\\left(c_{i}, o_{j}\\right)\\right\\|_{\\Sigma_{i j}}^{2}+\\\\ &\\left\\|\\mathbf{e}\\left(c_{i}, p_{k}\\right)\\right\\|_{\\Sigma_{i k}}^{2}+\\left\\|\\mathbf{e}\\left(o_{j}, p_{k}\\right)\\right\\|_{\\Sigma_{j k}}^{2} \\end{aligned} \u53f3\u4e0b\u89d2\u7684 \\sum \u8868\u8fbe\u7684\u662f\u4e0d\u540c\u8bef\u5dee\u4e4b\u95f4\u7684\u534f\u65b9\u5dee \u8bef\u5dee\u9879 \u76f8\u673a\u3001\u7269\u4f53\u4e0e\u4e16\u754c\u5750\u6807\u7cfb\u7684\u8f6c\u6362\u5e94\u4e0e\u76f8\u673a\u4e2d\u7269\u4f53\u7684\u76f8\u5bf9\u8f6c\u6362\u6709\u5bf9\u5e94 e_{c o_{-} 3 D}=\\left[\\log \\left(\\left(T_{c}^{-1} T_{o}\\right) T_{o m}^{-1}\\right)_{\\mathrm{se}_{3}}^{\\vee} \\quad \\mathbf{d}-\\mathbf{d}_{m}\\right] \u5176\u4e2d T = \\{R |t\\}, d \u4e3adimension 2. 3D\u7269\u4f53\u5728\u6295\u5f71\u5230\u76f8\u673a\u4e2d\uff0c\u4e0e2D\u6846\u7684\u4e2d\u5fc3\u3001\u957f\u5bbd\u7684\u8bef\u5dee e_{co\\_2D} = [c, s] - [c_m, s_m] 3. \u7269\u4f53\u4e0e\u70b9\u7684\u5bf9\u5e94\uff0c\u5df2\u77e5\u70b9P\u5728\u7269\u4f53O\u4e2d,\u90a3\u4e48\u5982\u679c\u70b9P\u4e0d\u5728\u8fd9\u4e2a3D\u6846\u91cc\u9762\u5c31\u7ed9\u4e88\u60e9\u7f5a. e_{op} = max(|T_o^{-1}P| - d_m, 0) 4. \u70b9\u4e0e\u76f8\u673a\u7684\u5bf9\u5e94\uff0c\u4e0e\u4f20\u7edf\u7684feature-based SLAM\u4e00\u81f4\u3002 e_{cp} = \\pi(T_c^{-1}P) - z_m \u5176\u4e2d z_m \u4e3a\u70b9P\u539f\u672c\u88ab\u89c2\u5bdf\u5230\u7684\u76f8\u673a\u5750\u6807\u3002 \u4f5c\u8005\u8fdb\u4e00\u6b65\u8ba8\u8bba\u4e86\u52a8\u6001\u573a\u666f\u4e2d\u7684SLAM,\u8fd9\u91cc\u7701\u7565","title":"CubeSLAM: Monocular 3D Object SLAM"},{"location":"other_categories/SLAM/CubeSLAM/#cubeslam-monocular-3d-object-slam","text":"\u8fd9\u7bc7\u6587\u7ae0\u5c06\u5355\u76ee3D\u7269\u4f53\u68c0\u6d4b\u4ee5\u53ca\u5355\u76eeSLAM\u653e\u5728\u540c\u4e00\u4e2a\u6846\u67b6\u91cc\u9762\u4f18\u5316\uff0c\u5f97\u5230\u7684\u7ed3\u679c\u662f\u540c\u65f6\u66f4\u4f18\u5316\u7684SLAM\u7ed3\u679c\u4ee5\u53ca\u68c0\u6d4b\u7ed3\u679c\u3002\u8fd9\u7bc7\u6587\u7ae0\u5c5e\u4e8e\u6240\u8c13\u7269\u4f53\u7ea7\u7684SLAM\u3002","title":"CubeSLAM: Monocular 3D Object SLAM"},{"location":"other_categories/SLAM/CubeSLAM/#_1","text":"","title":"\u5355\u76ee\u56fe\u7247\u7406\u89e3"},{"location":"other_categories/SLAM/CubeSLAM/#3d-proposal","text":"\u672c\u6587\u7684\u601d\u8def\u662f\u901a\u8fc7\u4e00\u4e2a\u89d2\u70b9\u52a0\u4e0a\u6d88\u5931\u70b9&\u65cb\u8f6c\u77e9\u9635\u4ee5\u53ca2D \u68c0\u6d4b\u6846\u8fd8\u539f\u4e09\u7ef4\u7ed3\u679c\u3002 \u7531\u65cb\u8f6c\u77e9\u9635\u5f97\u5230\u957f\u65b9\u5f62\u6846\u4e09\u4e2a\u6d88\u5931\u70b9\u7684\u516c\u5f0f\u4e3a: VP_i = KR_{col(i)}, i\\in \\{1,2,3\\} \u7531\u4e0a\u56fe\u53ef\u77e5,\u4ee5a\u5c0f\u56fe\u4e3a\u4f8b,\u5728\u5df2\u77e5\u6d88\u5931\u70b9\u4ee5\u53ca p_1 \u7684\u60c5\u51b5\u4e0b,\u5176\u4f59\u7684\u70b9\u53ef\u4ee5\u7528\u6c42\u4ea4\u70b9\u7684\u65b9\u5f0f\u5f97\u5230 p_2 = (VP_1, p1) \\times (B, C), p_4 = (VP_2, p1) \\times (A, D), p_3 = (VP_1, p_4)\\times(VP_2, p_2), p_5 = (VP_3, p_3) \\times (C, D), p_6 = (VP_2, p_5) \\times(VP_2, p_5), p_7=(VP_3, p_1)\\times(VP_1, p_6), p_8 = (VP_3, p_4)\\times(VP_2, p_7) \u4e8b\u5b9e\u4e0a\u540e\u9762\u7684\u70b9\u9009\u62e9\u7a7a\u95f4\u5f88\u5927. \u5f97\u5230\u89d2\u70b9\u540e\u8fdb\u4e00\u6b65\u6536\u7f29\u81ea\u7531\u5ea6 \u5bf9\u4e8e\u4efb\u610f\u59ff\u6001\u7684\u7269\u4f53\uff0c\u9009\u53d6\u4e0d\u5171\u9762\u7684\u56db\u4e2a\u89d2\u70b9\u59821,2,4,7\u8fdb\u884cPnP\u6c42\u89e3 \u5bf9\u4e8e\u5730\u9762\u4e0a\u7684\u7269\u4f53\uff0c\u5047\u8bbe\u5176roll,pitch\u89d2\u5ea6\u90fd\u4e3a0\u5ea6\uff0c\u5c31\u53ef\u4ee5\u4e0d\u4f7f\u7528PnP\u6c42\u89e3\u4e86\uff0c\u53ef\u4ee5\u6839\u636e\u76f8\u673a\u9ad8\u5ea6\u5c06\u5e95\u9762\u7684\u89d2\u70b9\u76f4\u63a5\u6295\u5f71\u5230\u4e16\u754c\u5750\u6807\u4e2d \u4e00\u822c\u6765\u8bf4\uff0c\u6211\u4eec\u53ef\u4ee5\u7528\u795e\u7ecf\u7f51\u7edc\u76f4\u63a5\u8f93\u51fa\u89d2\u5ea6\uff0c\u4f46\u662f\u8fd9\u91cc\u4e3a\u4e86\u63d0\u9ad8\u6cdb\u5316\u6027\u80fd\uff0c\u9009\u62e9\u91c7\u6837\u5e76\u5404\u81ea\u8bc4\u5206\u4ee5\u8ba1\u7b97\u65cb\u8f6c\u77e9\u9635\u3002","title":"\u5355\u76ee3D proposal\u751f\u6210"},{"location":"other_categories/SLAM/CubeSLAM/#proposal","text":"\u603b\u4f53\u635f\u5931\u51fd\u6570\u4e3a: E(O|I) = \\phi_{dist}(O, I) + w_1\\phi_{angle}(O,I) + w_2 \\phi_{shape}(O) \u5176\u4e2d O \u4e3a\u635f\u5931\u51fd\u6570, w \u4e3a\u6743\u91cd\u8d85\u53c2\u6570 \\phi_{dist} ,\u5bf9\u56fe\u7247\u8fd0\u884ccanny edge\uff0c\u57fa\u4e8e\u6b64\u5efa\u7acbdistance map\uff0c\u5bf9\u6bcf\u4e00\u4e2a\u7ad6\u7ebf\uff0c\u91c7\u683710\u4e2a\u70b9\uff0c\u8ba1\u7b97\u8ddd\u79bb\u56fe\u4e2d\u7684\u8ddd\u79bb\u503c\u603b\u548c\uff0c\u6700\u540e\u5f97\u5206\u9664\u4ee52D\u6846\u7684\u5bf9\u89d2\u7ebf\u957f\u5ea6 \\phi_{angle} \u524d\u4e00\u4e2a\u51fd\u6570\u5bf9false positive\u7684\u5c0f\u76f4\u7ebf\u6bb5\u975e\u5e38\u654f\u611f\uff0c\u9996\u5148\u627e\u51fa\u4e0a\u4e0b\u9762\u7684\u957f\u8fb9\uff0c\u627e\u51fa\u76f8\u5bf9\u6d88\u5931\u70b9\u503e\u659c\u89d2\u6700\u5927\u4e0e\u6700\u5c0f\u7684. \\left\\langle a, b \\right\\rangle \u4e3a\u70b9 a,b \u76f4\u7ebf\u7684\u503e\u659c\u89d2\uff0c \\begin{aligned} \\phi_{\\text {angle}}(O, I)=& \\sum_{i=1: 3}\\left\\|\\left\\langle l_{i_{-} m s}, l_{i_{-} m t}\\right\\rangle-\\left\\langle\\mathrm{VP}_{i}, l_{i_{-} m t}\\right\\rangle\\right\\|+\\\\ &\\left\\|\\left\\langle l_{i_{-} n s}, l_{i_{-} n t}\\right\\rangle-\\left\\langle\\mathrm{VP}_{i}, l_{i_{-} n t}\\right\\rangle\\right\\| \\end{aligned} \u672c\u8d28\u4e0a\u662f\u8981\u6c42\u957f\u8fb9\u7ecf\u8fc7\u6d88\u5931\u70b9 \\phi_{shape} \u5982\u679c\u957f\u5bbd\u6bd4\u5f88\u5927\u7684\uff0c\u7ed9\u4e88\u4e00\u4e2a\u60e9\u7f5a\u3002","title":"Proposal \u8bc4\u5206"},{"location":"other_categories/SLAM/CubeSLAM/#slam","text":"","title":"SLAM"},{"location":"other_categories/SLAM/CubeSLAM/#bundle-adjustment","text":"\u8bb0\u76f8\u673a\u59ff\u6001\u30013D\u7269\u4f53\u3001\u7279\u5f81\u70b9\u5206\u522b\u4e3a C = \\{C_i\\}, O = \\{O_j\\}, P = \\{P_k\\} , BA\u5c31\u63cf\u8ff0\u4e3a\u4ee5\u4e0b\u7684\u6700\u5c0f\u4e8c\u4e58\u95ee\u9898 \\begin{aligned} C^{*}, O^{*}, P^{*}=& \\underset{\\{C, O, P\\}}{\\arg \\min } \\sum_{C_{i}, O_{j}, P_{k}}\\left\\|\\mathbf{e}\\left(c_{i}, o_{j}\\right)\\right\\|_{\\Sigma_{i j}}^{2}+\\\\ &\\left\\|\\mathbf{e}\\left(c_{i}, p_{k}\\right)\\right\\|_{\\Sigma_{i k}}^{2}+\\left\\|\\mathbf{e}\\left(o_{j}, p_{k}\\right)\\right\\|_{\\Sigma_{j k}}^{2} \\end{aligned} \u53f3\u4e0b\u89d2\u7684 \\sum \u8868\u8fbe\u7684\u662f\u4e0d\u540c\u8bef\u5dee\u4e4b\u95f4\u7684\u534f\u65b9\u5dee","title":"bundle adjustment\u95ee\u9898"},{"location":"other_categories/SLAM/CubeSLAM/#_2","text":"\u76f8\u673a\u3001\u7269\u4f53\u4e0e\u4e16\u754c\u5750\u6807\u7cfb\u7684\u8f6c\u6362\u5e94\u4e0e\u76f8\u673a\u4e2d\u7269\u4f53\u7684\u76f8\u5bf9\u8f6c\u6362\u6709\u5bf9\u5e94 e_{c o_{-} 3 D}=\\left[\\log \\left(\\left(T_{c}^{-1} T_{o}\\right) T_{o m}^{-1}\\right)_{\\mathrm{se}_{3}}^{\\vee} \\quad \\mathbf{d}-\\mathbf{d}_{m}\\right] \u5176\u4e2d T = \\{R |t\\}, d \u4e3adimension 2. 3D\u7269\u4f53\u5728\u6295\u5f71\u5230\u76f8\u673a\u4e2d\uff0c\u4e0e2D\u6846\u7684\u4e2d\u5fc3\u3001\u957f\u5bbd\u7684\u8bef\u5dee e_{co\\_2D} = [c, s] - [c_m, s_m] 3. \u7269\u4f53\u4e0e\u70b9\u7684\u5bf9\u5e94\uff0c\u5df2\u77e5\u70b9P\u5728\u7269\u4f53O\u4e2d,\u90a3\u4e48\u5982\u679c\u70b9P\u4e0d\u5728\u8fd9\u4e2a3D\u6846\u91cc\u9762\u5c31\u7ed9\u4e88\u60e9\u7f5a. e_{op} = max(|T_o^{-1}P| - d_m, 0) 4. \u70b9\u4e0e\u76f8\u673a\u7684\u5bf9\u5e94\uff0c\u4e0e\u4f20\u7edf\u7684feature-based SLAM\u4e00\u81f4\u3002 e_{cp} = \\pi(T_c^{-1}P) - z_m \u5176\u4e2d z_m \u4e3a\u70b9P\u539f\u672c\u88ab\u89c2\u5bdf\u5230\u7684\u76f8\u673a\u5750\u6807\u3002 \u4f5c\u8005\u8fdb\u4e00\u6b65\u8ba8\u8bba\u4e86\u52a8\u6001\u573a\u666f\u4e2d\u7684SLAM,\u8fd9\u91cc\u7701\u7565","title":"\u8bef\u5dee\u9879"},{"location":"other_categories/SLAM/DISK/","text":"DISK: Learning local features with policy gradient \u8fd9\u7bc7paper\u5f15\u7528\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u5173\u952e\u70b9\u7684\u68c0\u6d4b\u3002\u662f2020NIPS paper. \u52a8\u673a\u4e0a\u6765\u8bf4\u76f4\u63a5\u4f18\u5316\u6210\u529f\u7684\u5339\u914d\u70b9\u6570\u91cf\u3002 \u6d41\u7a0b \u7279\u5f81\u63d0\u53d6 P(F_I | I, \\theta_F) \u4f7f\u7528UNet\u4ece\u56fe\u7247 I \u63d0\u53d6\u8f93\u51faheatmap\u4ee5\u53ca\u7279\u5f81\u77e2\u91cf(128\u7ef4)\u3002\u8fd9\u7bc7paper\u6a21\u4eff superPoint \u7684\u505a\u6cd5\uff0c\u5c06\u56fe\u7247\u5206\u4e3a\u7531 h\\times h \u65b9\u5757\u7ec4\u6210\u7684\u7f51\u683c\uff0c\u7136\u540e\u6bcf\u4e00\u4e2a\u7f51\u683c\u4e2d\u6700\u591a\u53ea\u4f1a\u6709\u4e00\u4e2akeypoint. \u800c\u6bcf\u4e00\u4e2a\u7f51\u683c\u5185\u5f97\u5206\u6700\u9ad8\u7684\u70b9\u662f\u5426\u4e3akeypoint\u9664\u4e86softmax\u7684\u76f8\u5bf9score\u5916\u8fd8\u6709\u4e00\u4e2asigmoid\u7684\u7edd\u5bf9score\u3002 P(p|K^u) = softmax(K^u)_p \\cdot \\sigma(K_p^u) \u56fe\u7247\u7684\u7279\u5f81\u96c6 F_I = \\{(p_1, D(p_1)), (p_2, D(p_2)), ...\\} \u5339\u914d\u5206\u5e03 P(M_{A\\leftrightarrow B} | F_A, F_B, \\theta_M) \u5df2\u77e5\u4e24\u4e2a\u7279\u5f81\u7684\u7279\u5f81\u96c6\uff0c\u4f5c\u8005\u8ba1\u7b97\u6bcf\u4e24\u4e2adescriptor\u4e4b\u95f4\u7684 l_2 \u8ddd\u79bb\u3002\u4ece\u800c\u5f97\u5230\u8ddd\u79bb\u77e9\u9635 d .\u5728\u8bad\u7ec3\u7684\u65f6\u5019\uff0c\u4f5c\u8005\u6839\u636e d \u7684\u884c\u6216\u5217\u7684softmax\u503c\u6765\u8ba1\u7b97\u4ece A \\rightarrow B \u6216\u8005 B \\rightarrow A \u7684\u5339\u914d\u6982\u7387\u3002 \u4f5c\u8005\u6307\u51fa\u4e3a\u4e86\u907f\u514d\u5b66\u51fa\u96be\u4ee5\u627e\u51fa\u4e00\u4e00\u5bf9\u5e94\u7684\u7279\u5f81\u70b9,\u4e00\u822c\u91c7\u7528\u5faa\u73af\u4e00\u81f4\u5339\u914d(Cycle-consistent matching)\u4ee5\u53ca\u6bd4\u7387\u6d4b\u8bd5(ratio test)\u4e24\u4e2a\u6280\u5de7\u3002 \u5faa\u73af\u4e00\u81f4\u5339\u914d\u8981\u6c42\u4e24\u4e2akeypoint\u5fc5\u987b\u662f\u76f8\u4e92\u95f4\u90fd\u662fnearest neighbours\u624d\u80fd\u7b97\u662f\u4e00\u4e2a\u4e00\u4e2a\u5b8c\u6574\u7684match \u6bd4\u7387\u6d4b\u8bd5\u662fSIFT\u5f00\u59cb\u5f15\u5165\u7684\u6280\u5de7\uff0c\u5bf9\u4e8e\u4e00\u4e2akeypoint\uff0c\u5b83\u4e0e\u6700\u8fd1\u70b9\u7684\u8ddd\u79bb\u5fc5\u987b\u4e0e\u6b21\u8fd1\u70b9\u7684\u7279\u5f81\u8ddd\u79bb\u5927\u4e00\u5b9a\u7684\u500d\u7387\uff0c\u624d\u80fd\u7b97\u662f\u6b63\u786e\u7684match. \u4f46\u662f\u8fd9\u4e24\u4e2a\u6280\u5de7\u90fd\u662f\u4e0d\u53ef\u5bfc\u7684\u3002\u4f5c\u8005\u63d0\u51fa\u7684\u65b9\u6848\u662f\u653e\u677e\u5faa\u73af\u4e00\u76f4\u5339\u914d\u7684\u8981\u6c42\u3002\u8fd9\u91cc\u91c7\u53d6\u7684\u65b9\u6848\u662f\u53ea\u8981\u4e24\u4e2akeypoint\u5728softmax\u4e2d\u4e92\u76f8\u91c7\u6837\u5230\u5c31\u7b97\u662fconsistent. \u800c\u4e24\u4e2a\u7279\u5f81\u6210\u529f\u5339\u914d\u4e0a\u7684\u6982\u7387\u53ef\u4ee5\u8ba1\u7b97\u662f P(i \\leftrightarrow j) = P_{A\\rightarrow B}(i | d, j) \\cdot P_{A \\leftarrow B} (j|d,i) . \u56e0\u800c\u91c7\u6837\u672c\u8eab\u4e0d\u5f71\u54cd\u68af\u5ea6\u7684\u8ba1\u7b97. Reward R(M_{A \\leftrightarrow B}) \u5bf9\u4e8e\u6b63\u786e\u5339\u914d\u7684\u70b9\u4e0e\u4e0d\u6b63\u786e\u7684\u70b9\u5206\u522b\u7ed9\u4e00\u5b9a\u91cf\u7684reward\u3002 \u5982\u679c\u4e24\u4e2a\u70b9\u662f\u6709\u6df1\u5ea6\u6807\u6ce8\u4e14\u91cd\u6295\u5f71\u8ddd\u79bb\u8db3\u591f\u5c0f\uff0c\u5219\u8bb0\u5f55\u4e3a\u6b63\u786e\u7684\u5339\u914d\u3002\u5982\u679c\u4e24\u4e2a\u70b9\u6ca1\u6709\u6df1\u5ea6\u6807\u6ce8\uff0c\u4e14\u8ddd\u79bb\u4e0d\u5927\uff0c\u5219\u4e0d\u8ba1\u7b97reward\uff0c\u5176\u4ed6\u70b9\u8bb0\u5f55\u4e3a\u4e0d\u6b63\u786e\u7684\u5339\u914d\u70b9\u3002 \u68af\u5ea6 \u8fd9\u91cc\u91c7\u7528policy gradient\u7684\u7b97\u6cd5. \u76ee\u6807: \u8ba1\u7b97 \\nabla_{\\theta} \\underset{M_{A} \\rightarrow B}{\\mathbb{E}} R\\left(M_{A \\leftrightarrow B}\\right) \u671f\u671b\u8868\u8fbe: \\underset{i,j}{\\sum}[P(i\\leftrightarrow j|F_A, F_B, \\theta_M) \\cdot r(i \\leftrightarrow j)] Log-Gradient Trick: \\nabla_\\theta P = P\\cdot \\nabla{\\log{P}} Factorizing \\nabla_\\theta P : \\nabla_\\theta P = \\nabla_{\\theta_M}\\log{P(i\\leftrightarrow j | F_A, F_B, \\theta_M)} + \\nabla_{\\theta_F}\\log{P(F_{A,i} | A, \\theta_F) + \\nabla_{\\theta_F}\\log{P(F_{B,j} | B, \\theta_F)}} \u3002\u8fd9\u4e2a\u7b49\u5f0f\u8bb2\u7684\u5c31\u662f i,j \u4e24\u4e2a\u70b9\u6210\u529f\u5339\u914d\u7684\u6982\u7387\u662f\u7531\u8ddd\u79bb\u5339\u914d\u4ee5\u53ca\u4e24\u5f20\u56fe\u5404\u81ea\u7684\u5206\u7c7b\u7ed3\u679c\u7d2f\u4e58\u800c\u5f97\u7684\u3002 \u7531\u6b64\uff0c\u4ec5\u4ec5\u901a\u8fc7\u5339\u914d\u7684reward\uff0c\u5c31\u53ef\u4ee5\u4f7f\u6574\u4e2a\u7f51\u7edc\u7aef\u5230\u7aef\u8bad\u7ec3\u3002","title":"DISK: Learning local features with policy gradient"},{"location":"other_categories/SLAM/DISK/#disk-learning-local-features-with-policy-gradient","text":"\u8fd9\u7bc7paper\u5f15\u7528\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u5173\u952e\u70b9\u7684\u68c0\u6d4b\u3002\u662f2020NIPS paper. \u52a8\u673a\u4e0a\u6765\u8bf4\u76f4\u63a5\u4f18\u5316\u6210\u529f\u7684\u5339\u914d\u70b9\u6570\u91cf\u3002","title":"DISK: Learning local features with policy gradient"},{"location":"other_categories/SLAM/DISK/#_1","text":"","title":"\u6d41\u7a0b"},{"location":"other_categories/SLAM/DISK/#pf_i-i-theta_f","text":"\u4f7f\u7528UNet\u4ece\u56fe\u7247 I \u63d0\u53d6\u8f93\u51faheatmap\u4ee5\u53ca\u7279\u5f81\u77e2\u91cf(128\u7ef4)\u3002\u8fd9\u7bc7paper\u6a21\u4eff superPoint \u7684\u505a\u6cd5\uff0c\u5c06\u56fe\u7247\u5206\u4e3a\u7531 h\\times h \u65b9\u5757\u7ec4\u6210\u7684\u7f51\u683c\uff0c\u7136\u540e\u6bcf\u4e00\u4e2a\u7f51\u683c\u4e2d\u6700\u591a\u53ea\u4f1a\u6709\u4e00\u4e2akeypoint. \u800c\u6bcf\u4e00\u4e2a\u7f51\u683c\u5185\u5f97\u5206\u6700\u9ad8\u7684\u70b9\u662f\u5426\u4e3akeypoint\u9664\u4e86softmax\u7684\u76f8\u5bf9score\u5916\u8fd8\u6709\u4e00\u4e2asigmoid\u7684\u7edd\u5bf9score\u3002 P(p|K^u) = softmax(K^u)_p \\cdot \\sigma(K_p^u) \u56fe\u7247\u7684\u7279\u5f81\u96c6 F_I = \\{(p_1, D(p_1)), (p_2, D(p_2)), ...\\}","title":"\u7279\u5f81\u63d0\u53d6 P(F_I | I, \\theta_F)"},{"location":"other_categories/SLAM/DISK/#pm_aleftrightarrow-b-f_a-f_b-theta_m","text":"\u5df2\u77e5\u4e24\u4e2a\u7279\u5f81\u7684\u7279\u5f81\u96c6\uff0c\u4f5c\u8005\u8ba1\u7b97\u6bcf\u4e24\u4e2adescriptor\u4e4b\u95f4\u7684 l_2 \u8ddd\u79bb\u3002\u4ece\u800c\u5f97\u5230\u8ddd\u79bb\u77e9\u9635 d .\u5728\u8bad\u7ec3\u7684\u65f6\u5019\uff0c\u4f5c\u8005\u6839\u636e d \u7684\u884c\u6216\u5217\u7684softmax\u503c\u6765\u8ba1\u7b97\u4ece A \\rightarrow B \u6216\u8005 B \\rightarrow A \u7684\u5339\u914d\u6982\u7387\u3002 \u4f5c\u8005\u6307\u51fa\u4e3a\u4e86\u907f\u514d\u5b66\u51fa\u96be\u4ee5\u627e\u51fa\u4e00\u4e00\u5bf9\u5e94\u7684\u7279\u5f81\u70b9,\u4e00\u822c\u91c7\u7528\u5faa\u73af\u4e00\u81f4\u5339\u914d(Cycle-consistent matching)\u4ee5\u53ca\u6bd4\u7387\u6d4b\u8bd5(ratio test)\u4e24\u4e2a\u6280\u5de7\u3002 \u5faa\u73af\u4e00\u81f4\u5339\u914d\u8981\u6c42\u4e24\u4e2akeypoint\u5fc5\u987b\u662f\u76f8\u4e92\u95f4\u90fd\u662fnearest neighbours\u624d\u80fd\u7b97\u662f\u4e00\u4e2a\u4e00\u4e2a\u5b8c\u6574\u7684match \u6bd4\u7387\u6d4b\u8bd5\u662fSIFT\u5f00\u59cb\u5f15\u5165\u7684\u6280\u5de7\uff0c\u5bf9\u4e8e\u4e00\u4e2akeypoint\uff0c\u5b83\u4e0e\u6700\u8fd1\u70b9\u7684\u8ddd\u79bb\u5fc5\u987b\u4e0e\u6b21\u8fd1\u70b9\u7684\u7279\u5f81\u8ddd\u79bb\u5927\u4e00\u5b9a\u7684\u500d\u7387\uff0c\u624d\u80fd\u7b97\u662f\u6b63\u786e\u7684match. \u4f46\u662f\u8fd9\u4e24\u4e2a\u6280\u5de7\u90fd\u662f\u4e0d\u53ef\u5bfc\u7684\u3002\u4f5c\u8005\u63d0\u51fa\u7684\u65b9\u6848\u662f\u653e\u677e\u5faa\u73af\u4e00\u76f4\u5339\u914d\u7684\u8981\u6c42\u3002\u8fd9\u91cc\u91c7\u53d6\u7684\u65b9\u6848\u662f\u53ea\u8981\u4e24\u4e2akeypoint\u5728softmax\u4e2d\u4e92\u76f8\u91c7\u6837\u5230\u5c31\u7b97\u662fconsistent. \u800c\u4e24\u4e2a\u7279\u5f81\u6210\u529f\u5339\u914d\u4e0a\u7684\u6982\u7387\u53ef\u4ee5\u8ba1\u7b97\u662f P(i \\leftrightarrow j) = P_{A\\rightarrow B}(i | d, j) \\cdot P_{A \\leftarrow B} (j|d,i) . \u56e0\u800c\u91c7\u6837\u672c\u8eab\u4e0d\u5f71\u54cd\u68af\u5ea6\u7684\u8ba1\u7b97.","title":"\u5339\u914d\u5206\u5e03 P(M_{A\\leftrightarrow B} | F_A, F_B, \\theta_M)"},{"location":"other_categories/SLAM/DISK/#reward-rm_a-leftrightarrow-b","text":"\u5bf9\u4e8e\u6b63\u786e\u5339\u914d\u7684\u70b9\u4e0e\u4e0d\u6b63\u786e\u7684\u70b9\u5206\u522b\u7ed9\u4e00\u5b9a\u91cf\u7684reward\u3002 \u5982\u679c\u4e24\u4e2a\u70b9\u662f\u6709\u6df1\u5ea6\u6807\u6ce8\u4e14\u91cd\u6295\u5f71\u8ddd\u79bb\u8db3\u591f\u5c0f\uff0c\u5219\u8bb0\u5f55\u4e3a\u6b63\u786e\u7684\u5339\u914d\u3002\u5982\u679c\u4e24\u4e2a\u70b9\u6ca1\u6709\u6df1\u5ea6\u6807\u6ce8\uff0c\u4e14\u8ddd\u79bb\u4e0d\u5927\uff0c\u5219\u4e0d\u8ba1\u7b97reward\uff0c\u5176\u4ed6\u70b9\u8bb0\u5f55\u4e3a\u4e0d\u6b63\u786e\u7684\u5339\u914d\u70b9\u3002","title":"Reward R(M_{A \\leftrightarrow B})"},{"location":"other_categories/SLAM/DISK/#_2","text":"\u8fd9\u91cc\u91c7\u7528policy gradient\u7684\u7b97\u6cd5. \u76ee\u6807: \u8ba1\u7b97 \\nabla_{\\theta} \\underset{M_{A} \\rightarrow B}{\\mathbb{E}} R\\left(M_{A \\leftrightarrow B}\\right) \u671f\u671b\u8868\u8fbe: \\underset{i,j}{\\sum}[P(i\\leftrightarrow j|F_A, F_B, \\theta_M) \\cdot r(i \\leftrightarrow j)] Log-Gradient Trick: \\nabla_\\theta P = P\\cdot \\nabla{\\log{P}} Factorizing \\nabla_\\theta P : \\nabla_\\theta P = \\nabla_{\\theta_M}\\log{P(i\\leftrightarrow j | F_A, F_B, \\theta_M)} + \\nabla_{\\theta_F}\\log{P(F_{A,i} | A, \\theta_F) + \\nabla_{\\theta_F}\\log{P(F_{B,j} | B, \\theta_F)}} \u3002\u8fd9\u4e2a\u7b49\u5f0f\u8bb2\u7684\u5c31\u662f i,j \u4e24\u4e2a\u70b9\u6210\u529f\u5339\u914d\u7684\u6982\u7387\u662f\u7531\u8ddd\u79bb\u5339\u914d\u4ee5\u53ca\u4e24\u5f20\u56fe\u5404\u81ea\u7684\u5206\u7c7b\u7ed3\u679c\u7d2f\u4e58\u800c\u5f97\u7684\u3002 \u7531\u6b64\uff0c\u4ec5\u4ec5\u901a\u8fc7\u5339\u914d\u7684reward\uff0c\u5c31\u53ef\u4ee5\u4f7f\u6574\u4e2a\u7f51\u7edc\u7aef\u5230\u7aef\u8bad\u7ec3\u3002","title":"\u68af\u5ea6"},{"location":"other_categories/SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/","text":"DeepICP: An End-to-End Deep Neural Network for 3D Point Cloud Registration \u7aef\u5230\u7aef\u65b9\u6cd5\u5b9e\u73b0\u57fa\u4e8ekeypoint\u7684\u70b9\u4e91ICP,\u8f93\u5165\u662f\u4e24\u5e27\u76f8\u90bb\u7684\u70b9\u4e91\uff0c\u4e00\u4e2a\u5bf9\u4e8e\u76f8\u5bf9\u8fd0\u52a8\u7684\u521d\u59cb\u4f30\u8ba1\u3002 \u6574\u4f53\u7ed3\u6784 \u7279\u5f81\u63d0\u53d6\u5c42 \u4e24\u5e27\u70b9\u4e91\u9996\u5148\u8f93\u5165\u5230\u4e00\u4e2a\u5171\u7528\u7684\u7279\u5f81\u63d0\u53d6\u7f51\u7edc\u3002\u4f7f\u7528\u7684\u7f51\u7edc\u4e0ePointNet++\u4e00\u81f4\u3002\u4f5c\u8005\u7684\u4e00\u4e2a\u7406\u8bba\u662f\u8bf4\u8fd9\u91cc\u4f1a\u5e26\u6709\u4e00\u4e9b\u8bed\u4e49\u4fe1\u606f\uff0c\u8fd9\u4e9b\u8bed\u4e49\u4fe1\u606f\u53ef\u4ee5\u6709\u6548\u5730\u5e2e\u52a9\u907f\u514d\u52a8\u6001\u7269\u4f53\u3002 \u70b9\u52a0\u6743 \u7406\u8bba\u4e0a\u6765\u8bf4\uff0c\u5177\u6709\u5f3a\u7279\u5f81\u7684\u70b9\u5e94\u8be5\u4f1a\u5206\u914d\u66f4\u5927\u7684\u6743\u91cd\u3002\u8fd9\u91cc\u7684\u505a\u6cd5\u662f\u5bf9\u6bcf\u4e00\u4e2a\u70b9\u8fdb\u884c\u4e09\u5c42\u5168\u8fde\u63a5\uff0c\u524d\u9762\u4e24\u5c42\u5e26\u6709batchnorm\u4ee5\u53caReLU\uff0c\u6700\u540e\u4e00\u5c42softplus\u6fc0\u6d3b: y = ln(1 + e^x) ,\u6700\u540e\u9009\u51fa\u8f93\u51fa\u6743\u91cd\u6700\u5927\u7684N\u4e2a\u70b9\uff0c\u8fd9\u4e9b\u70b9\u4ee5\u53ca\u5bf9\u5e94\u7684\u6743\u91cd\u5728\u540e\u7eed\u7ee7\u7eed\u4f7f\u7528\u3002 \u6df1\u5ea6\u7279\u5f81\u63d0\u53d6(embedding) \u4f7f\u7528\u4e00\u4e2amini-PointNet(\u6709\u5f85\u5f15\u7528\u8bf4\u660e), \u5bf9\u524d\u9762\u63d0\u51fa\u7684N\u4e2akeypoint\uff0c\u5728\u534a\u5f84\u4e3a d \u7684\u8303\u56f4\u5185\uff0c\u6536\u96c6K\u4e2a\u4e34\u8fd1\u70b9(\u53ef\u91cd\u590d),\u5bf9\u8fd9\u4e9b\u70b9\uff0c\u628a\u4ed6\u4eec\u7684\u76f8\u5bf9\u5750\u6807normalized by d \uff0c\u518d\u52a0\u4e0alidar\u5f3a\u5ea6\uff0c\u8fd9\u4e9b\u7279\u5f81\u4e0e\u7279\u5f81\u63d0\u53d6\u5c42\u7684\u7279\u5f81concat\uff0c mini-pointNet \u7531\u4e09\u5c42\u5168\u8fde\u63a5\u548cmax-pooling\u7ec4\u6210\uff0c\u8f93\u5165\u662f N\\times K \\times 36 \u7684\u77e2\u91cf\uff0c\u8f93\u51fa\u662f\u4e00\u4e2a,\u8f93\u51fa\u4ecd\u7136\u662f N\\times 32 \u7ef4\u7684\u5411\u91cf \u5bf9\u5e94\u70b9\u751f\u6210 \u4f20\u7edfICP\u76f4\u63a5\u9009\u62e9\u6700\u9760\u8fd1\u7684\u70b9\u6700\u4e3a\u5bf9\u5e94\u70b9\uff0c\u8fd9\u4f7f\u5f97\u53cd\u5411\u4f20\u64ad\u65e0\u6cd5\u8fdb\u884c\uff0c\u800c\u4e14\u7531\u4e8e\u70b9\u4e91\u7684\u7a00\u758f\u7279\u6027\uff0c\u5f88\u591a\u65f6\u5019\u6839\u672c\u4e0d\u5b58\u5728\u5bf9\u5e94\u70b9\uff0c\u8fd9\u91cc\u63d0\u51fa\u4e86\u4f7f\u7528CPG\u5c42. \u9996\u5148\u5c06N\u4e2a\u5728\u6e90\u70b9\u4e91\u7684keypoint\u901a\u8fc7\u9884\u4f30\u8ba1\u7684\u8f6c\u79fb\u77e9\u9635\u8fdb\u884c\u4e00\u6b21\u5750\u6807\u53d8\u6362\u3002\u5728\u8f6c\u6362\u540e\u7684\u4f4d\u7f6e\u4e0a\uff0c\u5c06\u5b83\u7684\u4e34\u8fd1\u7a7a\u95f4\u5206\u4e3a (\\frac{2r}{s} + 1, \\frac{2r}{s} + 1, \\frac{2r}{s} + 1) 3D\u7f51\u683c\uff0c\u6bcf\u4e00\u4e2a\u7f51\u683c\u7684\u4e2d\u5fc3{ y^{'}_j, j=1,...,C }\u53ef\u7406\u89e3\u4e3a\u53ef\u80fd\u7684\u5bf9\u5e94\u70b9\uff0c\u4e0e\u524d\u6587\u6df1\u5ea6\u7279\u5f81\u63d0\u53d6\u4e00\u81f4\uff0c\u4f7f\u7528\u7279\u5f81\u63d0\u53d6\u5f97\u5230 N\\times C \\times 32 \u7684\u77e2\u91cf\u3002\u6765\u81ea\u6e90\u57df\u4e0e\u76ee\u6807\u57df\u7684\u7279\u5f81\u9001\u52303D CNN + softmax\u4e2d\uff0c\u76ee\u6807\u662f\u5b66\u4e60\u4e00\u4e2asimilarity distance metric. Loss \u51fd\u6570 \u5bf9\u4e8e\u6bcf\u4e00\u4e2akeypoint\uff0c\u6211\u4eec\u901a\u8fc7GT\u53ef\u4ee5\u77e5\u9053\u5b83\u6700\u540e\u7684cooresponding points\uff0c\u7528GT\u7684\u5bf9\u5e94\u70b9\u4e0e\u9884\u6d4b\u7684\u5bf9\u5e94\u70b9\u4e4b\u95f4\u7684\u8bef\u5dee\u53ef\u4ee5\u7ed9\u51fa\u4e00\u4e2aLoss \u5c06\u6240\u6709\u7684\u5bf9\u5e94\u70b9\u878d\u5408\u8d77\u6765\uff0c\u53ef\u4ee5\u7528SVD\u6c42\u51faR\uff0cT\uff0c\u7528RT\u4ee3\u66ff\u9884\u6d4b\u7684\u5bf9\u5e94\u70b9\u6c42\u51fa\u7b2c\u4e8c\u4e2aloss\uff0cTensorflow\u63d0\u4f9b\u4e86\u53ef\u5fae\u5206\u7684SVD\u5b9e\u73b0\uff0c \u6700\u7ec8loss\u4e3a\u4e24\u8005\u7684\u878d\u5408","title":"DeepICP: An End-to-End Deep Neural Network for 3D Point Cloud Registration"},{"location":"other_categories/SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/#deepicp-an-end-to-end-deep-neural-network-for-3d-point-cloud-registration","text":"\u7aef\u5230\u7aef\u65b9\u6cd5\u5b9e\u73b0\u57fa\u4e8ekeypoint\u7684\u70b9\u4e91ICP,\u8f93\u5165\u662f\u4e24\u5e27\u76f8\u90bb\u7684\u70b9\u4e91\uff0c\u4e00\u4e2a\u5bf9\u4e8e\u76f8\u5bf9\u8fd0\u52a8\u7684\u521d\u59cb\u4f30\u8ba1\u3002","title":"DeepICP: An End-to-End Deep Neural Network for 3D Point Cloud Registration"},{"location":"other_categories/SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/#_1","text":"","title":"\u6574\u4f53\u7ed3\u6784"},{"location":"other_categories/SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/#_2","text":"\u4e24\u5e27\u70b9\u4e91\u9996\u5148\u8f93\u5165\u5230\u4e00\u4e2a\u5171\u7528\u7684\u7279\u5f81\u63d0\u53d6\u7f51\u7edc\u3002\u4f7f\u7528\u7684\u7f51\u7edc\u4e0ePointNet++\u4e00\u81f4\u3002\u4f5c\u8005\u7684\u4e00\u4e2a\u7406\u8bba\u662f\u8bf4\u8fd9\u91cc\u4f1a\u5e26\u6709\u4e00\u4e9b\u8bed\u4e49\u4fe1\u606f\uff0c\u8fd9\u4e9b\u8bed\u4e49\u4fe1\u606f\u53ef\u4ee5\u6709\u6548\u5730\u5e2e\u52a9\u907f\u514d\u52a8\u6001\u7269\u4f53\u3002","title":"\u7279\u5f81\u63d0\u53d6\u5c42"},{"location":"other_categories/SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/#_3","text":"\u7406\u8bba\u4e0a\u6765\u8bf4\uff0c\u5177\u6709\u5f3a\u7279\u5f81\u7684\u70b9\u5e94\u8be5\u4f1a\u5206\u914d\u66f4\u5927\u7684\u6743\u91cd\u3002\u8fd9\u91cc\u7684\u505a\u6cd5\u662f\u5bf9\u6bcf\u4e00\u4e2a\u70b9\u8fdb\u884c\u4e09\u5c42\u5168\u8fde\u63a5\uff0c\u524d\u9762\u4e24\u5c42\u5e26\u6709batchnorm\u4ee5\u53caReLU\uff0c\u6700\u540e\u4e00\u5c42softplus\u6fc0\u6d3b: y = ln(1 + e^x) ,\u6700\u540e\u9009\u51fa\u8f93\u51fa\u6743\u91cd\u6700\u5927\u7684N\u4e2a\u70b9\uff0c\u8fd9\u4e9b\u70b9\u4ee5\u53ca\u5bf9\u5e94\u7684\u6743\u91cd\u5728\u540e\u7eed\u7ee7\u7eed\u4f7f\u7528\u3002","title":"\u70b9\u52a0\u6743"},{"location":"other_categories/SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/#embedding","text":"\u4f7f\u7528\u4e00\u4e2amini-PointNet(\u6709\u5f85\u5f15\u7528\u8bf4\u660e), \u5bf9\u524d\u9762\u63d0\u51fa\u7684N\u4e2akeypoint\uff0c\u5728\u534a\u5f84\u4e3a d \u7684\u8303\u56f4\u5185\uff0c\u6536\u96c6K\u4e2a\u4e34\u8fd1\u70b9(\u53ef\u91cd\u590d),\u5bf9\u8fd9\u4e9b\u70b9\uff0c\u628a\u4ed6\u4eec\u7684\u76f8\u5bf9\u5750\u6807normalized by d \uff0c\u518d\u52a0\u4e0alidar\u5f3a\u5ea6\uff0c\u8fd9\u4e9b\u7279\u5f81\u4e0e\u7279\u5f81\u63d0\u53d6\u5c42\u7684\u7279\u5f81concat\uff0c mini-pointNet \u7531\u4e09\u5c42\u5168\u8fde\u63a5\u548cmax-pooling\u7ec4\u6210\uff0c\u8f93\u5165\u662f N\\times K \\times 36 \u7684\u77e2\u91cf\uff0c\u8f93\u51fa\u662f\u4e00\u4e2a,\u8f93\u51fa\u4ecd\u7136\u662f N\\times 32 \u7ef4\u7684\u5411\u91cf","title":"\u6df1\u5ea6\u7279\u5f81\u63d0\u53d6(embedding)"},{"location":"other_categories/SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/#_4","text":"\u4f20\u7edfICP\u76f4\u63a5\u9009\u62e9\u6700\u9760\u8fd1\u7684\u70b9\u6700\u4e3a\u5bf9\u5e94\u70b9\uff0c\u8fd9\u4f7f\u5f97\u53cd\u5411\u4f20\u64ad\u65e0\u6cd5\u8fdb\u884c\uff0c\u800c\u4e14\u7531\u4e8e\u70b9\u4e91\u7684\u7a00\u758f\u7279\u6027\uff0c\u5f88\u591a\u65f6\u5019\u6839\u672c\u4e0d\u5b58\u5728\u5bf9\u5e94\u70b9\uff0c\u8fd9\u91cc\u63d0\u51fa\u4e86\u4f7f\u7528CPG\u5c42. \u9996\u5148\u5c06N\u4e2a\u5728\u6e90\u70b9\u4e91\u7684keypoint\u901a\u8fc7\u9884\u4f30\u8ba1\u7684\u8f6c\u79fb\u77e9\u9635\u8fdb\u884c\u4e00\u6b21\u5750\u6807\u53d8\u6362\u3002\u5728\u8f6c\u6362\u540e\u7684\u4f4d\u7f6e\u4e0a\uff0c\u5c06\u5b83\u7684\u4e34\u8fd1\u7a7a\u95f4\u5206\u4e3a (\\frac{2r}{s} + 1, \\frac{2r}{s} + 1, \\frac{2r}{s} + 1) 3D\u7f51\u683c\uff0c\u6bcf\u4e00\u4e2a\u7f51\u683c\u7684\u4e2d\u5fc3{ y^{'}_j, j=1,...,C }\u53ef\u7406\u89e3\u4e3a\u53ef\u80fd\u7684\u5bf9\u5e94\u70b9\uff0c\u4e0e\u524d\u6587\u6df1\u5ea6\u7279\u5f81\u63d0\u53d6\u4e00\u81f4\uff0c\u4f7f\u7528\u7279\u5f81\u63d0\u53d6\u5f97\u5230 N\\times C \\times 32 \u7684\u77e2\u91cf\u3002\u6765\u81ea\u6e90\u57df\u4e0e\u76ee\u6807\u57df\u7684\u7279\u5f81\u9001\u52303D CNN + softmax\u4e2d\uff0c\u76ee\u6807\u662f\u5b66\u4e60\u4e00\u4e2asimilarity distance metric.","title":"\u5bf9\u5e94\u70b9\u751f\u6210"},{"location":"other_categories/SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/#loss","text":"\u5bf9\u4e8e\u6bcf\u4e00\u4e2akeypoint\uff0c\u6211\u4eec\u901a\u8fc7GT\u53ef\u4ee5\u77e5\u9053\u5b83\u6700\u540e\u7684cooresponding points\uff0c\u7528GT\u7684\u5bf9\u5e94\u70b9\u4e0e\u9884\u6d4b\u7684\u5bf9\u5e94\u70b9\u4e4b\u95f4\u7684\u8bef\u5dee\u53ef\u4ee5\u7ed9\u51fa\u4e00\u4e2aLoss \u5c06\u6240\u6709\u7684\u5bf9\u5e94\u70b9\u878d\u5408\u8d77\u6765\uff0c\u53ef\u4ee5\u7528SVD\u6c42\u51faR\uff0cT\uff0c\u7528RT\u4ee3\u66ff\u9884\u6d4b\u7684\u5bf9\u5e94\u70b9\u6c42\u51fa\u7b2c\u4e8c\u4e2aloss\uff0cTensorflow\u63d0\u4f9b\u4e86\u53ef\u5fae\u5206\u7684SVD\u5b9e\u73b0\uff0c \u6700\u7ec8loss\u4e3a\u4e24\u8005\u7684\u878d\u5408","title":"Loss \u51fd\u6570"},{"location":"other_categories/SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/","text":"Joint Unsupervised Learning of Optical Flow and Depth by Watching Stereo Videos \u6b63\u5982\u9898\u76ee\u8868\u793a\uff0c\u672c\u6587\u7684\u76ee\u6807\u662f\u4f7f\u7528\u8fde\u7eed\u7684\u53cc\u76ee\u89c6\u9891\uff0c\u540c\u65f6\u9884\u6d4b\u5149\u6d41\u3001\u6df1\u5ea6\u4ee5\u53ca\u76f8\u673a\u8fd0\u52a8\u3002\u5927\u5e45\u5ea6\u91cd\u7528\u524d\u4eba\u7684\u7814\u7a76\uff0c\u7279\u70b9\u662f\u6ce8\u91cd\u5bf9\u8fd0\u52a8\u4ee5\u53ca\u906e\u6321\u7269\u4f53\u7684\u53bb\u9664 \u4e3b\u4f53\u6d41\u7a0b\u56fe \u8fd9\u4e2apipeline\u5206\u4e3a\u597d\u51e0\u4e2astages \u9996\u5148\uff0c\u7528PWC-Flow\u9884\u6d4b\u5de6\u76ee\u76f8\u673a\u7684\u4e24\u5f20\u56fe\u4e4b\u95f4\u7684\u5149\u6d41 F_{12}^{opt} \uff0c\u7528MotionNet\u9884\u6d4b\u5de6\u76ee\u76f8\u673a\u4e24\u5f20\u56fe\u4e4b\u95f4\u7684\u8fd0\u52a8 T_{12} , \u7528PWD-Disp\u4f30\u8ba1\u53cc\u76ee\u76f8\u673a\u4e4b\u95f4\u7684\u89c6\u5dee\uff0c\u7528\u89c6\u5dee\u53ef\u4ee5\u8f6c\u6362\u4e3a\u6df1\u5ea6 D = B f_x / d \u3002 \u7b2c\u4e8c\uff0c\u7ed3\u5408 D_1, T_{12} \uff0c\u6211\u4eec\u53ef\u4ee5\u8ba1\u7b97\u51fa\u56e0\u76f8\u673a\u521a\u4f53\u8fd0\u52a8\u800c\u4ea7\u751f\u7684\u5149\u6d41\uff0c\u8bb0\u4e3a F^{rig}_{12} \uff0c\u56fe\u4e2d\u672a\u5448\u73b0\u3002\u7136\u540erigid-alignment module\u5c06\u76f8\u673a\u8fd0\u52a8\u4ece T_{12} \u7cbe\u4fee\u4e3a T_{12}' ,\u5e76\u8fdb\u4e00\u6b65\u5f97\u5230\u7cbe\u4fee\u7684 F^{rig'}_{12} \u7b2c\u4e09\uff0cConsistency check\u53bb\u9664\u8fd0\u52a8\u533a\u57df\u3002 \u7b2c\u4e00\u3001\u7f51\u7edc\u7ed3\u6784 PWC-Flow\u6309\u7167 \u8fd9\u7bc7\u6587\u7ae0 \u7684 \u4ecb\u7ecd MotionNet\u6309\u7167 \u8fd9\u7bc7\u6587\u7ae0 \u7684 \u4ecb\u7ecd PWC-Disp\u4ecePWC-Flow\u66f4\u6539\uff0c\u5728Cost volumn\u8ba1\u7b97\u7684\u65f6\u5019\u5f3a\u8feb\u5b83\u53ea\u5728\u6c34\u5e73\u65b9\u5411\u4e0a\u641c\u7d22\uff0c\u8f93\u51fa\u5f62\u72b6\u81ea\u7136\u53d8\u6210\u4e86 d\\times H \\times W \u7b2c\u4e8c\u3001Rigid Alighment Module \u8fd9\u4e2a\u6a21\u5757\u76ee\u6807\u662f\u7b2c\u4e00\u6b65\u7cbe\u4fee \u9996\u5148\u901a\u8fc7 Q_t(i,j) = D_t(i,j) K^{-1P_t(i,j)} \u5c06\u5f53\u524d\u56fe\u50cf\u8f6c\u6362\u4e3a\u76f8\u673a\u5750\u6807\u7cfb\uff0c3D\u7a7a\u95f4\u4e2d\u7684\u70b9\uff0c\u7136\u540e\u4f7f\u7528 T_{12} \u8f6c\u6362\uff0c\u5f97\u5230\u5bf9\u5e94\u70b9\u5728\u7b2c\u4e8c\u65f6\u523b\u76f8\u673a\u5750\u6807\u7cfb\u4e2d\u7684\u70b9 \\hat Q_1 \u3002 \\widetilde Q_1 \u5219\u8868\u793a Q_2 \u901a\u8fc7\u5149\u6d41 F^{opt}_{12} \u56de\u5230\u7b2c\u4e00\u65f6\u523b\u7684\u5750\u6807\uff0c \u901a\u8fc7\u6c42\u89e3\u4ee5\u4e0a\u4e24\u4e2a\u6b65\u9aa4\uff0c\u53ef\u4ee5\u5f97\u5230\u7cbe\u4fee\u7684T \u7b2c\u4e09\u3001Consistent Check \u7cbe\u4fee\u7684rigid\u5149\u6d41\u4e0e\u7f51\u7edc\u5149\u6d41\u7684\u5dee\u4e2d\uff0c\u503c\u8fc7\u5927\u6216\u8005\u88ab\u906e\u6321\u7684\u90e8\u5206\u4f1a\u88ab\u8fc7\u6ee4\u6389\uff0c\u4f7f\u7528thresholding\u7ed9\u51fa\u4e00\u4e2amask\uff0c\u53ea\u6709mask\u4e2d\u8ba4\u4e3a\u662f\u9759\u6b62\u7269\u4f53\u7684\u624d\u4f1a\u8fdb\u884closs\u8ba1\u7b97\u3002","title":"Joint Unsupervised Learning of Optical Flow and Depth by Watching Stereo Videos"},{"location":"other_categories/SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/#joint-unsupervised-learning-of-optical-flow-and-depth-by-watching-stereo-videos","text":"\u6b63\u5982\u9898\u76ee\u8868\u793a\uff0c\u672c\u6587\u7684\u76ee\u6807\u662f\u4f7f\u7528\u8fde\u7eed\u7684\u53cc\u76ee\u89c6\u9891\uff0c\u540c\u65f6\u9884\u6d4b\u5149\u6d41\u3001\u6df1\u5ea6\u4ee5\u53ca\u76f8\u673a\u8fd0\u52a8\u3002\u5927\u5e45\u5ea6\u91cd\u7528\u524d\u4eba\u7684\u7814\u7a76\uff0c\u7279\u70b9\u662f\u6ce8\u91cd\u5bf9\u8fd0\u52a8\u4ee5\u53ca\u906e\u6321\u7269\u4f53\u7684\u53bb\u9664","title":"Joint Unsupervised Learning of Optical Flow and Depth by Watching Stereo Videos"},{"location":"other_categories/SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/#_1","text":"\u8fd9\u4e2apipeline\u5206\u4e3a\u597d\u51e0\u4e2astages \u9996\u5148\uff0c\u7528PWC-Flow\u9884\u6d4b\u5de6\u76ee\u76f8\u673a\u7684\u4e24\u5f20\u56fe\u4e4b\u95f4\u7684\u5149\u6d41 F_{12}^{opt} \uff0c\u7528MotionNet\u9884\u6d4b\u5de6\u76ee\u76f8\u673a\u4e24\u5f20\u56fe\u4e4b\u95f4\u7684\u8fd0\u52a8 T_{12} , \u7528PWD-Disp\u4f30\u8ba1\u53cc\u76ee\u76f8\u673a\u4e4b\u95f4\u7684\u89c6\u5dee\uff0c\u7528\u89c6\u5dee\u53ef\u4ee5\u8f6c\u6362\u4e3a\u6df1\u5ea6 D = B f_x / d \u3002 \u7b2c\u4e8c\uff0c\u7ed3\u5408 D_1, T_{12} \uff0c\u6211\u4eec\u53ef\u4ee5\u8ba1\u7b97\u51fa\u56e0\u76f8\u673a\u521a\u4f53\u8fd0\u52a8\u800c\u4ea7\u751f\u7684\u5149\u6d41\uff0c\u8bb0\u4e3a F^{rig}_{12} \uff0c\u56fe\u4e2d\u672a\u5448\u73b0\u3002\u7136\u540erigid-alignment module\u5c06\u76f8\u673a\u8fd0\u52a8\u4ece T_{12} \u7cbe\u4fee\u4e3a T_{12}' ,\u5e76\u8fdb\u4e00\u6b65\u5f97\u5230\u7cbe\u4fee\u7684 F^{rig'}_{12} \u7b2c\u4e09\uff0cConsistency check\u53bb\u9664\u8fd0\u52a8\u533a\u57df\u3002","title":"\u4e3b\u4f53\u6d41\u7a0b\u56fe"},{"location":"other_categories/SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/#_2","text":"PWC-Flow\u6309\u7167 \u8fd9\u7bc7\u6587\u7ae0 \u7684 \u4ecb\u7ecd MotionNet\u6309\u7167 \u8fd9\u7bc7\u6587\u7ae0 \u7684 \u4ecb\u7ecd PWC-Disp\u4ecePWC-Flow\u66f4\u6539\uff0c\u5728Cost volumn\u8ba1\u7b97\u7684\u65f6\u5019\u5f3a\u8feb\u5b83\u53ea\u5728\u6c34\u5e73\u65b9\u5411\u4e0a\u641c\u7d22\uff0c\u8f93\u51fa\u5f62\u72b6\u81ea\u7136\u53d8\u6210\u4e86 d\\times H \\times W","title":"\u7b2c\u4e00\u3001\u7f51\u7edc\u7ed3\u6784"},{"location":"other_categories/SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/#rigid-alighment-module","text":"\u8fd9\u4e2a\u6a21\u5757\u76ee\u6807\u662f\u7b2c\u4e00\u6b65\u7cbe\u4fee \u9996\u5148\u901a\u8fc7 Q_t(i,j) = D_t(i,j) K^{-1P_t(i,j)} \u5c06\u5f53\u524d\u56fe\u50cf\u8f6c\u6362\u4e3a\u76f8\u673a\u5750\u6807\u7cfb\uff0c3D\u7a7a\u95f4\u4e2d\u7684\u70b9\uff0c\u7136\u540e\u4f7f\u7528 T_{12} \u8f6c\u6362\uff0c\u5f97\u5230\u5bf9\u5e94\u70b9\u5728\u7b2c\u4e8c\u65f6\u523b\u76f8\u673a\u5750\u6807\u7cfb\u4e2d\u7684\u70b9 \\hat Q_1 \u3002 \\widetilde Q_1 \u5219\u8868\u793a Q_2 \u901a\u8fc7\u5149\u6d41 F^{opt}_{12} \u56de\u5230\u7b2c\u4e00\u65f6\u523b\u7684\u5750\u6807\uff0c \u901a\u8fc7\u6c42\u89e3\u4ee5\u4e0a\u4e24\u4e2a\u6b65\u9aa4\uff0c\u53ef\u4ee5\u5f97\u5230\u7cbe\u4fee\u7684T","title":"\u7b2c\u4e8c\u3001Rigid Alighment Module"},{"location":"other_categories/SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/#consistent-check","text":"\u7cbe\u4fee\u7684rigid\u5149\u6d41\u4e0e\u7f51\u7edc\u5149\u6d41\u7684\u5dee\u4e2d\uff0c\u503c\u8fc7\u5927\u6216\u8005\u88ab\u906e\u6321\u7684\u90e8\u5206\u4f1a\u88ab\u8fc7\u6ee4\u6389\uff0c\u4f7f\u7528thresholding\u7ed9\u51fa\u4e00\u4e2amask\uff0c\u53ea\u6709mask\u4e2d\u8ba4\u4e3a\u662f\u9759\u6b62\u7269\u4f53\u7684\u624d\u4f1a\u8fdb\u884closs\u8ba1\u7b97\u3002","title":"\u7b2c\u4e09\u3001Consistent Check"},{"location":"other_categories/SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/","text":"LO-Net: Deep Real-time Lidar Odometry \u8fd9\u7bc7\u6587\u7ae0\u4e3b\u8981\u6709\u4e09\u4e2a\u8d21\u732e\uff0c\u7b2c\u4e00\u63d0\u51fa\u4e86scan-to-scan lidar odometry\u7f51\u7edc\uff0c\u540c\u65f6\u4f30\u8ba1\u9762\u7684\u6cd5\u5411\u4ee5\u53camask for dynamic regions\u3002\u7b2c\u4e8c\u878d\u5408\u76f8\u90bb\u4e24\u5e27\u7f51\u7edc\u8fdb\u884c\u4f30\u8ba1\uff0c\u7b2c\u4e09\uff0c\u878d\u5408\u4e00\u4e2amapping module. \u6ce8\u610fGithub \u94fe\u63a5\u4ee3\u7801\u5c1a\u672a\u516c\u5e03\u3002 \u7f51\u7edc\u4e3b\u8981\u7ed3\u6784 \u6574\u4f53\u6765\u8bf4\uff0c\u7f51\u7edc\u7531\u4e09\u4e2a\u7f51\u7edc\u6784\u6210\uff0c\u5206\u522b\u662f\u6cd5\u5411\u4f30\u8ba1\u7f51\u7edc(point wise)\uff0cmask \u4f30\u8ba1\u7f51\u7edc\u4ee5\u53ca\u4e00\u4e2a\u5171\u7528\u53c2\u6570\u7684\u53cc\u751f\u59ff\u6001\u56de\u5f52\u4e3b\u7f51\u8def\u3002\u5b83\u4ee5\u4e24\u4e2a\u76f8\u90bb\u7684lidar\u70b9\u4f5c\u4e3a\u8f93\u5165\uff0c\u4f30\u8ba1\u51fa6\u81ea\u7531\u5ea6\u7684\u76f8\u5bf9\u8fd0\u52a8\u3001\u70b9\u4e91\u5404\u70b9\u7684\u9762\u6cd5\u5411\u4ee5\u53ca\u52a8\u6001\u533a\u57dfmask\u3002odometry\u7684\u8f93\u51fa\u4f1a\u901a\u8fc7mapping\u6a21\u5757\u8fdb\u4e00\u6b65\u63d0\u9ad8\uff0c\u6700\u7ec8\u7684\u8f93\u51fa\u4f1a\u662f\u76f8\u5bf9\u4e8e\u521d\u59cb\u4f4d\u7f6e\u7684\u504f\u79fb \u8f93\u5165\u7f16\u7801 \u4e3a\u4e86\u8ba9\u7f51\u7edc\u7684\u6570\u636e\u7f16\u6392\u53d8\u5f97\u7d27\u51d1\uff0c\u8fd9\u91cc\u4f7f\u7528\u5706\u67f1\u5750\u6807\u7cfb \\alpha = arctan(y/x)/\\Delta \\alpha \\beta = arcsin(z/\\sqrt{x^2+y^2 + z^2} / \u0394\u03b2) \u5982\u679c\u540c\u4e00\u4e2a \\alpha, \\beta \u5750\u6807\u6709\u4e0d\u6b62\u4e00\u4e2a\u70b9\uff0c\u5219\u4ee5\u6700\u8fd1\u7684\u70b9\u4e3a\u51c6\uff0c\u6bcf\u4e00\u4e2a\u70b9\u7684\u7279\u5f81\u5305\u62ec\u5f3a\u5ea6\u503c\u4ee5\u53ca\u8ddd\u79bb\u503c\u3002 \u51e0\u4f55\u7ea6\u675f \u6cd5\u5411\u4f30\u8ba1 \u4e25\u8c28\u6765\u8bf4\uff0c\u4ee5\u4e0a\u56fe\u4e3a\u4f8b\u5b50\uff0c\u70b9\u4e91\u7684\u6cd5\u5411\u5e94\u8be5\u7531\u70b9 X^i \u4ee5\u53ca\u5176 k \u4e2a\u76f8\u90bb\u7684\u70b9\uff0c\u7531\u4e0b\u5f0f\u5b9a\u4e49 \\argmin_{\\mathcal{N}(X^i)} ||[w_{i1}(X^{i_1}-X^i),...]^T\\mathcal{N}(X^i)||_2 \u4e5f\u5c31\u662f\u5bfb\u627e\u4e00\u4e2a\u77e2\u91cf\uff0c\u4f7f\u5f97\u8fd9\u4e2a\u77e2\u91cf\u4e0ek\u4e2a\u76f8\u90bb\u70b9\u77e2\u91cf\u7684\u70b9\u4e58\u7684\u52a0\u6743\u6c42\u548c\u503c(\u6216\u8005\u662f\u52a0\u6743\u8303\u6570)\u6700\u5c0f.\u4e00\u822c\u6765\u8bf4\u8ddd\u79bb\u8d8a\u8fd1\u6743\u91cd\u8d8a\u5927\uff0c\u8ddd\u79bb\u8d8a\u8fdc\u6743\u91cd\u8d8a\u5c0f\u3002 \u672c\u6587\u4e3a\u4e86\u7b80\u5316\u8fd9\u4e00\u8ba1\u7b97\uff0c\u4f7f\u7528\u4ee5\u4e0b\u65b9\u7a0b \\mathcal{N}(X^i) = \\sum_{X^{i_k}, X^{i_j} in \\mathcal{P}} (w_{ik}(X^{i_k} - X^i) \\times w_{ij}(X^{i_j} - X^i)) \u5176\u4e2d \\mathcal{P} \u4e3a\u5f53\u524d\u70b9\u7684\u4e34\u8fd1\u70b9\u3002 \u76f8\u90bb\u4e24\u7c07\u70b9\u4e91\u4e4b\u95f4\u6709\u4e00\u5b9a\u7684\u5bf9\u5e94\u5173\u7cfb\uff0c\u4ee4 P \u4e3a\u6295\u5f71\u8fc7\u7a0b\u800c T_t \u4e3a\u76f8\u5bf9\u4f4d\u79fb\uff0c\u53ef\u4ee5\u627e\u5230\u70b9 X^{\\alpha \\beta}_{t-1} \u7684\u5bf9\u5e94\u70b9 \\hat X^{\\alpha \\beta}_t = P T_t P^{-1} X^{\\alpha\\beta}_{t-1} \u7531\u4e8e\u76f8\u5bf9\u5e94\u7684\u70b9\u6cd5\u5411\u4f30\u8ba1\u7406\u5e94\u6bd4\u8f83\u76f8\u4f3c\uff0c\u6240\u4ee5\u4e00\u4e2a\u7ea6\u675f\u662f \\mathcal{L}_n = \\sum_{\\alpha \\beta}||\\mathcal{N}(\\hat X^{\\alpha\\beta}_t) - \\mathcal{N}(X^{\\alpha\\beta}_t) ||_1 \\dot e^{|\\nabla_r(\\hat X^{\\alpha \\beta}_t)|} \u5176\u4e2d \\nabla_r(\\hat X^{\\alpha \\beta}_t) \u662f\u8ddd\u79bb\u5173\u4e8e \\alpha \\beta \u7684\u5fae\u5206\uff0c\u610f\u601d\u662f\u53d8\u5316\u8d8a\u5267\u70c8\u7684\u5730\u65b9\u8d8a\u91cd\u8981 \u91cc\u7a0b\u56de\u5f52 \u8fd9\u91cc\u8ba9\u7f51\u7edc\u5728\u5b8c\u5168\u8fde\u63a5\u5c42\u8f93\u51fa7\u4e2a\u6570\u503c\uff0c\u524d\u4e09\u4e2a\u662f\u5e73\u79fb\u5411\u91cf\uff0c\u540e\u9762\u56db\u4e2a\u662f\u56db\u5143\u6570\u3002\u5927\u90e8\u5206\u7f51\u7edc\u5c42\u4f7f\u7528\u7684\u662f fireConv \u7531\u4e8e\u70b9\u4e91\u7684\u7279\u6027\uff0cfeature map\u7684\u9ad8\u5ea6\u8fdc\u5c0f\u4e8e\u5bbd\u5ea6(360\u00b0\u70b9\u4e91)\uff0c\u6240\u4ee5\u5728\u4e0b\u91c7\u6837\u7684\u65f6\u5019\u53ea\u5bf9\u5bbd\u5ea6\u8fdb\u884cmax pooling \u5728\u5b66\u4e60\u65f6\u7531\u4e8e\u65cb\u8f6c\u4e0e\u5e73\u79fb\u7684\u5355\u4f4d\u4e0d\u540c\uff0c\u540c\u65f6\u4e3a\u4e86\u907f\u514d\u8c03\u8282\u8d85\u53c2\uff0c\u4f7f\u7528\u81ea\u52a8\u5b66\u4e60\u7684\u53c2\u6570(\u4e2a\u4eba\u6ce8\u89e3:\u5c3d\u7ba1\u516c\u5f0f\u4e0d\u540c\uff0c\u5f15\u7528\u7684\u6587\u7ae0\u4e5f\u4e0d\u4e00\u81f4\uff0c\u4f46\u662f\u57fa\u672c\u53ef\u4ee5\u786e\u8ba4\u7406\u8bba\u672c\u8d28\u6765\u81ea\u4e8e multi-loss ) \\begin{aligned} \\mathcal{L}_{o} &=\\mathcal{L}_{x}\\left(S_{t-1} ; S_{t}\\right) \\exp \\left(-s_{x}\\right)+s_{x} \\\\ &+\\mathcal{L}_{q}\\left(S_{t-1} ; S_{t}\\right) \\exp \\left(-s_{q}\\right)+s_{q} \\end{aligned} Mask\u4f30\u8ba1 \u8f93\u51fa\u7684mask\u4f1a\u5f71\u54cd\u5230\u51e0\u4f55\u7ea6\u675f\u7684cost function\uff0c\u88ab\u6539\u9020\u4e3a \\mathcal{L}_{n}=\\sum_{\\alpha \\beta} \\mathcal{M}\\left(X_{t}^{\\alpha \\beta}\\right)\\left\\|\\mathcal{N}\\left(\\hat{X}_{t}^{\\alpha \\beta}\\right)-\\mathcal{N}\\left(X_{t}^{\\alpha \\beta}\\right)\\right\\|_{1} \\cdot e^{\\left|\\nabla r\\left(\\hat{X}_{t}^{\\alpha \\beta}\\right)\\right|} \u6ce8\u610f\u5230\u7531\u4e8emask prediction\u6ca1\u6709ground truth \u6240\u4ee5\u5c06\u6240\u6709mask\u8bbe\u7f6e\u4e3a0\u53ef\u4ee5\u8ba9cost\u53d8\u5f97\u6700\u5c0f\uff0c\u6240\u4ee5\u9644\u52a0\u4ee5\u4e0b\u7684cost\uff0c\u76ee\u6807\u662f\u8ba9\u7f51\u7edc\u80fd\u591f\u6743\u8861\u3002 \\mathcal{L}_{r}=-\\sum_{\\alpha \\beta} \\log P\\left(\\mathcal{M}\\left(X_{t}^{\\alpha \\beta}\\right)=1\\right) Mapping refinement * \u8868\u793a\u7684\u662f\u5bf9\u6cd5\u5411\u4f30\u8ba1\u7684\u4e00\u4e2a\u9884\u8bbe\u7684 3\\times 5 \u5377\u79ef\uff0c\u4e2d\u592e\u4e3a-14.\u5176\u4ed6\u503c\u4e3a1\uff0c\u662f\u4e00\u4e2a\u9ad8\u901a\u6ee4\u6ce2\u5668\u3002feature map\u4e0a\u503c\u6700\u5c0f\u7684 n_c \u4e2amask\u5916\u7684\u70b9\u9009\u51fa\u6765\uff0c\u8ba4\u4e3a\u662f\u5e73\u9762\u533a\u57df\u3002 \\mathbf{\\Pi} \u6307\u7684\u662f\u8ba1\u7b97lidar pose\u7684\u5148\u9a8c\u8ba1\u7b97(\u5047\u8bbe\u4e0a\u4e00\u65f6\u523b\u8f6c\u6362\u77e9\u9635\u4e0d\u53d8) M_{init} = M_{t-1}M^{-1}_{t-2}M_{t-1} \\mathbf{\\Psi} \u9996\u5148\u5229\u7528\u7f51\u7edc\u9884\u6d4b\u7684\u4e24\u5e27\u95f4\u4f4d\u79fb\u7ebf\u6027\u63d2\u503c\u8865\u507f\u8fd0\u52a8\u7578\u53d8\uff0c\u7136\u540e\u7528 M_{init} \u5c06\u65b0\u7684\u70b9\u4e91\u8f6c\u79fb\u5230\u4e16\u754c\u5750\u6807\u7cfb\u4e0b\u3002 \u5047\u8bbe p_i \u662f\u5f53\u524dscan\u7684\u70b9\uff0c m_i \u662f\u5bf9\u5e94\u70b9\uff0c\u800c n_i \u662f\u5bf9\u5e94\u70b9\u7684\u6cd5\u5411\u3002\u5168\u5c40mapping\u7684\u76ee\u6807\u5c31\u662f\u8981\u627e\u5230\u4e00\u4e2a\u6700\u4f18\u7684 M \u4f7f\u5f97 \\hat{\\mathbf{M}}_{o p t}=\\underset{\\hat{\\mathbf{M}}}{\\arg \\min } \\sum_{i}\\left(\\left(\\hat{\\mathbf{M}} \\cdot \\boldsymbol{p}_{i}-\\boldsymbol{m}_{i}\\right) \\cdot \\boldsymbol{n}_{i}\\right)^{2} \\Theta :\u8fed\u4ee3\u5730\u6c42\u89e3\u4e0a\u6587\u63d0\u5230\u7684\u65b9\u7a0b\uff0c \\mathbf{M}_{t}=\\prod_{k=1}^{n_{i t e r}} \\hat{\\mathbf{M}}_{k} \\mathbf{M}_{i n i t} \\Phi \u6839\u636e\u4f18\u5316\u540e\u7684\u4f4d\u79fb\u7ed3\u679c\u751f\u6210\u6700\u7ec8\u7684\u70b9\u4e91\u7ed3\u679c\u3002 \\sum,N \u5c06\u65b0\u7684\u70b9\u4e91\u52a0\u5230\u5730\u56fe\u4e2d\uff0c\u7136\u540e\u6e05\u9664\u6700\u65e7\u7684\u70b9\u4e91\uff0c\u53ea\u4fdd\u5b58\u6700\u65e7\u7684 n_m \u4e2a\u70b9\u4e91","title":"LO-Net: Deep Real-time Lidar Odometry"},{"location":"other_categories/SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/#lo-net-deep-real-time-lidar-odometry","text":"\u8fd9\u7bc7\u6587\u7ae0\u4e3b\u8981\u6709\u4e09\u4e2a\u8d21\u732e\uff0c\u7b2c\u4e00\u63d0\u51fa\u4e86scan-to-scan lidar odometry\u7f51\u7edc\uff0c\u540c\u65f6\u4f30\u8ba1\u9762\u7684\u6cd5\u5411\u4ee5\u53camask for dynamic regions\u3002\u7b2c\u4e8c\u878d\u5408\u76f8\u90bb\u4e24\u5e27\u7f51\u7edc\u8fdb\u884c\u4f30\u8ba1\uff0c\u7b2c\u4e09\uff0c\u878d\u5408\u4e00\u4e2amapping module. \u6ce8\u610fGithub \u94fe\u63a5\u4ee3\u7801\u5c1a\u672a\u516c\u5e03\u3002","title":"LO-Net: Deep Real-time Lidar Odometry"},{"location":"other_categories/SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/#_1","text":"\u6574\u4f53\u6765\u8bf4\uff0c\u7f51\u7edc\u7531\u4e09\u4e2a\u7f51\u7edc\u6784\u6210\uff0c\u5206\u522b\u662f\u6cd5\u5411\u4f30\u8ba1\u7f51\u7edc(point wise)\uff0cmask \u4f30\u8ba1\u7f51\u7edc\u4ee5\u53ca\u4e00\u4e2a\u5171\u7528\u53c2\u6570\u7684\u53cc\u751f\u59ff\u6001\u56de\u5f52\u4e3b\u7f51\u8def\u3002\u5b83\u4ee5\u4e24\u4e2a\u76f8\u90bb\u7684lidar\u70b9\u4f5c\u4e3a\u8f93\u5165\uff0c\u4f30\u8ba1\u51fa6\u81ea\u7531\u5ea6\u7684\u76f8\u5bf9\u8fd0\u52a8\u3001\u70b9\u4e91\u5404\u70b9\u7684\u9762\u6cd5\u5411\u4ee5\u53ca\u52a8\u6001\u533a\u57dfmask\u3002odometry\u7684\u8f93\u51fa\u4f1a\u901a\u8fc7mapping\u6a21\u5757\u8fdb\u4e00\u6b65\u63d0\u9ad8\uff0c\u6700\u7ec8\u7684\u8f93\u51fa\u4f1a\u662f\u76f8\u5bf9\u4e8e\u521d\u59cb\u4f4d\u7f6e\u7684\u504f\u79fb","title":"\u7f51\u7edc\u4e3b\u8981\u7ed3\u6784"},{"location":"other_categories/SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/#_2","text":"\u4e3a\u4e86\u8ba9\u7f51\u7edc\u7684\u6570\u636e\u7f16\u6392\u53d8\u5f97\u7d27\u51d1\uff0c\u8fd9\u91cc\u4f7f\u7528\u5706\u67f1\u5750\u6807\u7cfb \\alpha = arctan(y/x)/\\Delta \\alpha \\beta = arcsin(z/\\sqrt{x^2+y^2 + z^2} / \u0394\u03b2) \u5982\u679c\u540c\u4e00\u4e2a \\alpha, \\beta \u5750\u6807\u6709\u4e0d\u6b62\u4e00\u4e2a\u70b9\uff0c\u5219\u4ee5\u6700\u8fd1\u7684\u70b9\u4e3a\u51c6\uff0c\u6bcf\u4e00\u4e2a\u70b9\u7684\u7279\u5f81\u5305\u62ec\u5f3a\u5ea6\u503c\u4ee5\u53ca\u8ddd\u79bb\u503c\u3002","title":"\u8f93\u5165\u7f16\u7801"},{"location":"other_categories/SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/#_3","text":"","title":"\u51e0\u4f55\u7ea6\u675f"},{"location":"other_categories/SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/#_4","text":"\u4e25\u8c28\u6765\u8bf4\uff0c\u4ee5\u4e0a\u56fe\u4e3a\u4f8b\u5b50\uff0c\u70b9\u4e91\u7684\u6cd5\u5411\u5e94\u8be5\u7531\u70b9 X^i \u4ee5\u53ca\u5176 k \u4e2a\u76f8\u90bb\u7684\u70b9\uff0c\u7531\u4e0b\u5f0f\u5b9a\u4e49 \\argmin_{\\mathcal{N}(X^i)} ||[w_{i1}(X^{i_1}-X^i),...]^T\\mathcal{N}(X^i)||_2 \u4e5f\u5c31\u662f\u5bfb\u627e\u4e00\u4e2a\u77e2\u91cf\uff0c\u4f7f\u5f97\u8fd9\u4e2a\u77e2\u91cf\u4e0ek\u4e2a\u76f8\u90bb\u70b9\u77e2\u91cf\u7684\u70b9\u4e58\u7684\u52a0\u6743\u6c42\u548c\u503c(\u6216\u8005\u662f\u52a0\u6743\u8303\u6570)\u6700\u5c0f.\u4e00\u822c\u6765\u8bf4\u8ddd\u79bb\u8d8a\u8fd1\u6743\u91cd\u8d8a\u5927\uff0c\u8ddd\u79bb\u8d8a\u8fdc\u6743\u91cd\u8d8a\u5c0f\u3002 \u672c\u6587\u4e3a\u4e86\u7b80\u5316\u8fd9\u4e00\u8ba1\u7b97\uff0c\u4f7f\u7528\u4ee5\u4e0b\u65b9\u7a0b \\mathcal{N}(X^i) = \\sum_{X^{i_k}, X^{i_j} in \\mathcal{P}} (w_{ik}(X^{i_k} - X^i) \\times w_{ij}(X^{i_j} - X^i)) \u5176\u4e2d \\mathcal{P} \u4e3a\u5f53\u524d\u70b9\u7684\u4e34\u8fd1\u70b9\u3002 \u76f8\u90bb\u4e24\u7c07\u70b9\u4e91\u4e4b\u95f4\u6709\u4e00\u5b9a\u7684\u5bf9\u5e94\u5173\u7cfb\uff0c\u4ee4 P \u4e3a\u6295\u5f71\u8fc7\u7a0b\u800c T_t \u4e3a\u76f8\u5bf9\u4f4d\u79fb\uff0c\u53ef\u4ee5\u627e\u5230\u70b9 X^{\\alpha \\beta}_{t-1} \u7684\u5bf9\u5e94\u70b9 \\hat X^{\\alpha \\beta}_t = P T_t P^{-1} X^{\\alpha\\beta}_{t-1} \u7531\u4e8e\u76f8\u5bf9\u5e94\u7684\u70b9\u6cd5\u5411\u4f30\u8ba1\u7406\u5e94\u6bd4\u8f83\u76f8\u4f3c\uff0c\u6240\u4ee5\u4e00\u4e2a\u7ea6\u675f\u662f \\mathcal{L}_n = \\sum_{\\alpha \\beta}||\\mathcal{N}(\\hat X^{\\alpha\\beta}_t) - \\mathcal{N}(X^{\\alpha\\beta}_t) ||_1 \\dot e^{|\\nabla_r(\\hat X^{\\alpha \\beta}_t)|} \u5176\u4e2d \\nabla_r(\\hat X^{\\alpha \\beta}_t) \u662f\u8ddd\u79bb\u5173\u4e8e \\alpha \\beta \u7684\u5fae\u5206\uff0c\u610f\u601d\u662f\u53d8\u5316\u8d8a\u5267\u70c8\u7684\u5730\u65b9\u8d8a\u91cd\u8981","title":"\u6cd5\u5411\u4f30\u8ba1"},{"location":"other_categories/SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/#_5","text":"\u8fd9\u91cc\u8ba9\u7f51\u7edc\u5728\u5b8c\u5168\u8fde\u63a5\u5c42\u8f93\u51fa7\u4e2a\u6570\u503c\uff0c\u524d\u4e09\u4e2a\u662f\u5e73\u79fb\u5411\u91cf\uff0c\u540e\u9762\u56db\u4e2a\u662f\u56db\u5143\u6570\u3002\u5927\u90e8\u5206\u7f51\u7edc\u5c42\u4f7f\u7528\u7684\u662f fireConv \u7531\u4e8e\u70b9\u4e91\u7684\u7279\u6027\uff0cfeature map\u7684\u9ad8\u5ea6\u8fdc\u5c0f\u4e8e\u5bbd\u5ea6(360\u00b0\u70b9\u4e91)\uff0c\u6240\u4ee5\u5728\u4e0b\u91c7\u6837\u7684\u65f6\u5019\u53ea\u5bf9\u5bbd\u5ea6\u8fdb\u884cmax pooling \u5728\u5b66\u4e60\u65f6\u7531\u4e8e\u65cb\u8f6c\u4e0e\u5e73\u79fb\u7684\u5355\u4f4d\u4e0d\u540c\uff0c\u540c\u65f6\u4e3a\u4e86\u907f\u514d\u8c03\u8282\u8d85\u53c2\uff0c\u4f7f\u7528\u81ea\u52a8\u5b66\u4e60\u7684\u53c2\u6570(\u4e2a\u4eba\u6ce8\u89e3:\u5c3d\u7ba1\u516c\u5f0f\u4e0d\u540c\uff0c\u5f15\u7528\u7684\u6587\u7ae0\u4e5f\u4e0d\u4e00\u81f4\uff0c\u4f46\u662f\u57fa\u672c\u53ef\u4ee5\u786e\u8ba4\u7406\u8bba\u672c\u8d28\u6765\u81ea\u4e8e multi-loss ) \\begin{aligned} \\mathcal{L}_{o} &=\\mathcal{L}_{x}\\left(S_{t-1} ; S_{t}\\right) \\exp \\left(-s_{x}\\right)+s_{x} \\\\ &+\\mathcal{L}_{q}\\left(S_{t-1} ; S_{t}\\right) \\exp \\left(-s_{q}\\right)+s_{q} \\end{aligned}","title":"\u91cc\u7a0b\u56de\u5f52"},{"location":"other_categories/SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/#mask","text":"\u8f93\u51fa\u7684mask\u4f1a\u5f71\u54cd\u5230\u51e0\u4f55\u7ea6\u675f\u7684cost function\uff0c\u88ab\u6539\u9020\u4e3a \\mathcal{L}_{n}=\\sum_{\\alpha \\beta} \\mathcal{M}\\left(X_{t}^{\\alpha \\beta}\\right)\\left\\|\\mathcal{N}\\left(\\hat{X}_{t}^{\\alpha \\beta}\\right)-\\mathcal{N}\\left(X_{t}^{\\alpha \\beta}\\right)\\right\\|_{1} \\cdot e^{\\left|\\nabla r\\left(\\hat{X}_{t}^{\\alpha \\beta}\\right)\\right|} \u6ce8\u610f\u5230\u7531\u4e8emask prediction\u6ca1\u6709ground truth \u6240\u4ee5\u5c06\u6240\u6709mask\u8bbe\u7f6e\u4e3a0\u53ef\u4ee5\u8ba9cost\u53d8\u5f97\u6700\u5c0f\uff0c\u6240\u4ee5\u9644\u52a0\u4ee5\u4e0b\u7684cost\uff0c\u76ee\u6807\u662f\u8ba9\u7f51\u7edc\u80fd\u591f\u6743\u8861\u3002 \\mathcal{L}_{r}=-\\sum_{\\alpha \\beta} \\log P\\left(\\mathcal{M}\\left(X_{t}^{\\alpha \\beta}\\right)=1\\right)","title":"Mask\u4f30\u8ba1"},{"location":"other_categories/SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/#mapping-refinement","text":"* \u8868\u793a\u7684\u662f\u5bf9\u6cd5\u5411\u4f30\u8ba1\u7684\u4e00\u4e2a\u9884\u8bbe\u7684 3\\times 5 \u5377\u79ef\uff0c\u4e2d\u592e\u4e3a-14.\u5176\u4ed6\u503c\u4e3a1\uff0c\u662f\u4e00\u4e2a\u9ad8\u901a\u6ee4\u6ce2\u5668\u3002feature map\u4e0a\u503c\u6700\u5c0f\u7684 n_c \u4e2amask\u5916\u7684\u70b9\u9009\u51fa\u6765\uff0c\u8ba4\u4e3a\u662f\u5e73\u9762\u533a\u57df\u3002 \\mathbf{\\Pi} \u6307\u7684\u662f\u8ba1\u7b97lidar pose\u7684\u5148\u9a8c\u8ba1\u7b97(\u5047\u8bbe\u4e0a\u4e00\u65f6\u523b\u8f6c\u6362\u77e9\u9635\u4e0d\u53d8) M_{init} = M_{t-1}M^{-1}_{t-2}M_{t-1} \\mathbf{\\Psi} \u9996\u5148\u5229\u7528\u7f51\u7edc\u9884\u6d4b\u7684\u4e24\u5e27\u95f4\u4f4d\u79fb\u7ebf\u6027\u63d2\u503c\u8865\u507f\u8fd0\u52a8\u7578\u53d8\uff0c\u7136\u540e\u7528 M_{init} \u5c06\u65b0\u7684\u70b9\u4e91\u8f6c\u79fb\u5230\u4e16\u754c\u5750\u6807\u7cfb\u4e0b\u3002 \u5047\u8bbe p_i \u662f\u5f53\u524dscan\u7684\u70b9\uff0c m_i \u662f\u5bf9\u5e94\u70b9\uff0c\u800c n_i \u662f\u5bf9\u5e94\u70b9\u7684\u6cd5\u5411\u3002\u5168\u5c40mapping\u7684\u76ee\u6807\u5c31\u662f\u8981\u627e\u5230\u4e00\u4e2a\u6700\u4f18\u7684 M \u4f7f\u5f97 \\hat{\\mathbf{M}}_{o p t}=\\underset{\\hat{\\mathbf{M}}}{\\arg \\min } \\sum_{i}\\left(\\left(\\hat{\\mathbf{M}} \\cdot \\boldsymbol{p}_{i}-\\boldsymbol{m}_{i}\\right) \\cdot \\boldsymbol{n}_{i}\\right)^{2} \\Theta :\u8fed\u4ee3\u5730\u6c42\u89e3\u4e0a\u6587\u63d0\u5230\u7684\u65b9\u7a0b\uff0c \\mathbf{M}_{t}=\\prod_{k=1}^{n_{i t e r}} \\hat{\\mathbf{M}}_{k} \\mathbf{M}_{i n i t} \\Phi \u6839\u636e\u4f18\u5316\u540e\u7684\u4f4d\u79fb\u7ed3\u679c\u751f\u6210\u6700\u7ec8\u7684\u70b9\u4e91\u7ed3\u679c\u3002 \\sum,N \u5c06\u65b0\u7684\u70b9\u4e91\u52a0\u5230\u5730\u56fe\u4e2d\uff0c\u7136\u540e\u6e05\u9664\u6700\u65e7\u7684\u70b9\u4e91\uff0c\u53ea\u4fdd\u5b58\u6700\u65e7\u7684 n_m \u4e2a\u70b9\u4e91","title":"Mapping refinement"},{"location":"other_categories/SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/","text":"PWC-Net: CNNs for Optical Flow Using Pyramid, Warping, and Cost Volume \u8fd9\u7bc7\u8bba\u6587\u662f Joint Unsupervised Learning of Optical Flow and Depth by Watching Stereo Videos \u7684\u524d\u7f6e \u7f51\u7edc\u7ed3\u6784 \u8f93\u5165\u7684\u7279\u5f81\u5206\u4e3a6\u5c42\uff0c\u7b2c\u4e00\u5c42\u4e3a\u8f93\u5165\u56fe\u7247\uff0c\u7b2c\u4e8c\u5c42\u5f00\u59cb\u4e3aConvNet\u4e0b\u91c7\u6837\u7684\u8f93\u51fa, \u8f93\u51fa\u5149\u6d41 Warping Layer \u5728\u7b2cL\u5c42\uff0c\u5c06\u7b2c\u4e8c\u5f20\u56fe\u7b2cl+1\u5c42\u7684feature\u901a\u8fc7\u4e0a\u91c7\u6837\u8f6c\u6362\u5230\u7b2c\u4e00\u5f20\u56fe c^l_w(x) = c^l_2(x + up_2(w^{l+1})(x)) \u91c7\u7528\u7684\u662fbilinear interpolation Cost volume layer \u8868\u793a\u7684\u662f\u4e00\u4e2apixel\u4e0e\u4e0b\u4e00\u65f6\u523b\u5bf9\u5e94pixel match\u7684cost\u3002\u4f7f\u7528\u7279\u5f81\u7684coorelation\u6765\u8868\u793a cv^l(x1, x2) = \\frac{1}{N} (c^l_1(x_1))^T c^l_w(x_2) \u5177\u4f53\uff1a\u8f93\u51fa\u662f d^2\\times H^l \\times W^l \u5176\u5b9e\u5c31\u662f\u5de6\u89c6\u89d2\u6bcf\u4e00\u4e2a\u70b9 x_1 \u4e0ewarp\u7ed3\u679c\u5bf9\u5e94\u5468\u56f4 d\\times d \u4e2apixels\u7684\u7279\u5f81\u5411\u91cf\u6c42\u76f8\u5173\u6027 Optical flow estimator \u8fd9\u662f\u4e00\u4e2a\u591a\u5c42CNN\uff0c\u8f93\u5165\u662fCost Volumn, \u7b2c\u4e00\u56fe\u7684\u7279\u5f81\u4ee5\u53ca\u4e0a\u91c7\u6837\u7684\u5149\u6d41\uff0c\u5b83\u7684\u8f93\u51fa\u662f\u7b2c l \u5c42\u7684\u5149\u6d41 w^l .\u4e0d\u540c\u5c42\u4e4b\u95f4\u7684CNN\u7279\u5f81\u4e0d\u5171\u4eab\uff0c Context Network \u4f7f\u7528\u6700\u7ec8\u8f93\u51fa\u7684\u5149\u6d41\u4ee5\u53ca\u524d\u4e00\u5c42\u7684\u7279\u5f81\u4f5c\u4e3a\u8f93\u5165\uff0c\u518d\u8f93\u51fa\u66f4\u7cbe\u786e\u7684\u5149\u6d41\u503c\uff0c\u591a\u4f7f\u7528dilated Conv\u53bb\u63d0\u5347\u611f\u53d7\u91ce","title":"PWC-Net: CNNs for Optical Flow Using Pyramid, Warping, and Cost Volume"},{"location":"other_categories/SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/#pwc-net-cnns-for-optical-flow-using-pyramid-warping-and-cost-volume","text":"\u8fd9\u7bc7\u8bba\u6587\u662f Joint Unsupervised Learning of Optical Flow and Depth by Watching Stereo Videos \u7684\u524d\u7f6e","title":"PWC-Net: CNNs for Optical Flow Using Pyramid, Warping, and Cost Volume"},{"location":"other_categories/SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/#_1","text":"\u8f93\u5165\u7684\u7279\u5f81\u5206\u4e3a6\u5c42\uff0c\u7b2c\u4e00\u5c42\u4e3a\u8f93\u5165\u56fe\u7247\uff0c\u7b2c\u4e8c\u5c42\u5f00\u59cb\u4e3aConvNet\u4e0b\u91c7\u6837\u7684\u8f93\u51fa, \u8f93\u51fa\u5149\u6d41","title":"\u7f51\u7edc\u7ed3\u6784"},{"location":"other_categories/SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/#warping-layer","text":"\u5728\u7b2cL\u5c42\uff0c\u5c06\u7b2c\u4e8c\u5f20\u56fe\u7b2cl+1\u5c42\u7684feature\u901a\u8fc7\u4e0a\u91c7\u6837\u8f6c\u6362\u5230\u7b2c\u4e00\u5f20\u56fe c^l_w(x) = c^l_2(x + up_2(w^{l+1})(x)) \u91c7\u7528\u7684\u662fbilinear interpolation","title":"Warping Layer"},{"location":"other_categories/SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/#cost-volume-layer","text":"\u8868\u793a\u7684\u662f\u4e00\u4e2apixel\u4e0e\u4e0b\u4e00\u65f6\u523b\u5bf9\u5e94pixel match\u7684cost\u3002\u4f7f\u7528\u7279\u5f81\u7684coorelation\u6765\u8868\u793a cv^l(x1, x2) = \\frac{1}{N} (c^l_1(x_1))^T c^l_w(x_2) \u5177\u4f53\uff1a\u8f93\u51fa\u662f d^2\\times H^l \\times W^l \u5176\u5b9e\u5c31\u662f\u5de6\u89c6\u89d2\u6bcf\u4e00\u4e2a\u70b9 x_1 \u4e0ewarp\u7ed3\u679c\u5bf9\u5e94\u5468\u56f4 d\\times d \u4e2apixels\u7684\u7279\u5f81\u5411\u91cf\u6c42\u76f8\u5173\u6027","title":"Cost volume layer"},{"location":"other_categories/SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/#optical-flow-estimator","text":"\u8fd9\u662f\u4e00\u4e2a\u591a\u5c42CNN\uff0c\u8f93\u5165\u662fCost Volumn, \u7b2c\u4e00\u56fe\u7684\u7279\u5f81\u4ee5\u53ca\u4e0a\u91c7\u6837\u7684\u5149\u6d41\uff0c\u5b83\u7684\u8f93\u51fa\u662f\u7b2c l \u5c42\u7684\u5149\u6d41 w^l .\u4e0d\u540c\u5c42\u4e4b\u95f4\u7684CNN\u7279\u5f81\u4e0d\u5171\u4eab\uff0c","title":"Optical flow estimator"},{"location":"other_categories/SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/#context-network","text":"\u4f7f\u7528\u6700\u7ec8\u8f93\u51fa\u7684\u5149\u6d41\u4ee5\u53ca\u524d\u4e00\u5c42\u7684\u7279\u5f81\u4f5c\u4e3a\u8f93\u5165\uff0c\u518d\u8f93\u51fa\u66f4\u7cbe\u786e\u7684\u5149\u6d41\u503c\uff0c\u591a\u4f7f\u7528dilated Conv\u53bb\u63d0\u5347\u611f\u53d7\u91ce","title":"Context Network"},{"location":"other_categories/SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/","text":"SuperPoint: Self-Supervised Interest Point Detection and Description \u8fd9\u7bc7\u8bba\u6587\u5b8c\u6210\u7684\u4efb\u52a1\u662f\u4ece\u4e00\u5f20\u56fe\u4e2d\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u63d0\u53d6\u51fakeypoints\uff0c\u8fd9\u4e9bkeypoints\u7684\u5173\u952e\u662f\u8981\u6c42\u5bf9\u65cb\u8f6c\uff0cscale\uff0c\u5e73\u79fb\u9c81\u68d2\u3002 Training procedure \u8bad\u7ec3\u8fc7\u7a0b\u9700\u8981\u4e09\u4e2a\u9636\u6bb5\uff0c\u7b2c\u4e00\u4e2a\u9636\u6bb5\uff0c\u4f7f\u7528\u4e00\u4e9b\u7b80\u5355\u7684\u51e0\u4f55\u56fe\u7247,\u7528\u4e00\u4e9b\u6ca1\u6709\u6b67\u4e49\u7684\u56fe\u7247\u548ckeypoint\uff0c\u8fdb\u884c\u521d\u59cb\u8bad\u7ec3\u3002 \u7b2c\u4e8c\u9636\u6bb5\uff0c\u4f7f\u7528\u4e00\u4e9b\u6ca1\u6709label\u7684\u771f\u5b9e\u56fe\u7247\uff0c\u5bf9\u540c\u4e00\u5f20\u56fe\uff0c\u4f7f\u7528Homographic transform(\u65cb\u8f6c\u5e73\u79fb\u7b49\u7684\u7ec4\u5408),\u8f6c\u6362\u6210\u4e00\u4e2abatch\u7684\u56fe.\u7528\u524d\u6587\u7684detector\uff0c\u751f\u6210keypoint\uff0c\u518d\u8f6c\u6362\u4e3a\u539f\u56fe\u5750\u6807\u7cfb\u4e2d\u3002\u8fd9\u4e2abatch\u4f1a\u5728\u5f53\u524d\u56fe\u5f62\u6210\u4e00\u4e2akeypoint\u7684heatmap. \\mathbf{x}=\\mathcal{H}^{-1} f_{\\theta}(\\mathcal{H}(I)) \\hat{F}\\left(I ; f_{\\theta}\\right)=\\frac{1}{N_{h}} \\sum_{i=1}^{N_{h}} \\mathcal{H}_{i}^{-1} f_{\\theta}\\left(\\mathcal{H}_{i}(I)\\right) \u7b2c\u4e09\u9636\u6bb5\uff0c\u4f7f\u7528joint training\u3002\u8fdb\u884c\u8bad\u7ec3 \u7f51\u7edc\u6a21\u578b \u7b2c\u4e00\u9636\u6bb5\u53ea\u4f7f\u7528\u4e0a\u90e8\u5206\u90a3\u4e00\u652f\uff0c\u7b2c\u4e09\u9636\u6bb5\u4f1a\u6709descriptor\u90e8\u5206\u3002 Encoder\u7c7b\u4f3c\u4e8eVGG, Interest Point Decoder\uff0c65\u4e2aChannels,\u610f\u601d\u662f\u5468\u8fb9 8\\times 8 \u4e2a\u65b9\u5757\u6709keypoint\u7684\u6982\u7387\uff0c\u8fd8\u6709\u4e00\u4e2a\u662f\u7a7a\u7c7b. \u8fd9\u91cc\u7684 reshape \u5bf9\u5e94\u7684\u662fpytorch.nn.pixelshuffel\u64cd\u4f5c\uff0c\u5c06channel\u7684\u5185\u5bb9\u7ffb\u5230feature map\u7a7a\u95f4\u7684\u533a\u57df\u3002 \u5982\u679cground truth\u91cc\u9762 8\\times 8 \u533a\u57df\u91cc\u9762\u6709\u4e0d\u6b62\u4e00\u4e2a\u70b9\u5219\u968f\u673a\u9009\u53d6\u4e00\u4e2a\u4f5c\u4e3a\u8be5\u533a\u57df\u91cc\u9762\u6709\u6548\u7684ground truth\u3002 \u8fd9\u91cc\u4f7f\u7528cross-entropy,\u8ba1\u7b97 \\mathcal{L}_p \u8fdb\u884c\u8bad\u7ec3\u3002 descriptor\u7684\u8bad\u7ec3\uff0c\u4f9d\u9760\u7684\u662f\u540c\u4e00\u5f20\u56fe\uff0c\u5c06\u56fehomographic transform\u5230\u53e6\u4e00\u5f20\u56fe\u53bb\uff0c\u8981\u6c42keypoint\u5bf9\u5e94\u7684\u4f4d\u7f6e\u63cf\u8ff0\u76f8\u540c\u3002 s_{h w h^{\\prime} w^{\\prime}} \u6307\u4ee3\u5bf9\u5e94\u5173\u7cfb s_{h w h^{\\prime} w^{\\prime}}=\\left\\{\\begin{array}{ll}{1,} & {\\text { if } \\| \\widehat{\\mathcal{H} \\mathbf{p}_{h w}}-\\mathbf{p}_{h^{\\prime} w^{\\prime}}|| \\leq 8} \\\\ {0,} & {\\text { otherwise }}\\end{array}\\right. \\begin{aligned} l_{d}\\left(\\mathbf{d}, \\mathbf{d}^{\\prime} ; s\\right) &=\\lambda_{d} * s * \\max \\left(0, m_{p}-\\mathbf{d}^{T} \\mathbf{d}^{\\prime}\\right) \\\\ &+(1-s) * \\max \\left(0, \\mathbf{d}^{T} \\mathbf{d}^{\\prime}-m_{n}\\right) \\end{aligned} \\begin{aligned} \\mathcal{L}_{d}\\left(\\mathcal{D}, \\mathcal{D}^{\\prime}, S\\right) &=\\\\ \\frac{1}{\\left(H_{c} W_{c}\\right)^{2}} &\\sum_{h=1}^{H_{c}, W_{c} H_{c}, W_{c}} l_{d}\\left(\\mathbf{d}_{h w}, \\mathbf{d}_{h^{\\prime} w^{\\prime}}^{\\prime} ; s_{h w h^{\\prime} w^{\\prime}}\\right) \\end{aligned} \\begin{aligned} \\mathcal{L}\\left(\\mathcal{X}, \\mathcal{X}^{\\prime}, \\mathcal{D}, \\mathcal{D}^{\\prime} ; Y, Y^{\\prime}, S\\right) &=\\\\ \\mathcal{L}_{p}(\\mathcal{X}, Y)&+\\mathcal{L}_{p}\\left(\\mathcal{X}^{\\prime}, Y^{\\prime}\\right)+\\lambda \\mathcal{L}_{d}\\left(\\mathcal{D}, \\mathcal{D}^{\\prime}, S\\right) \\end{aligned}","title":"SuperPoint: Self-Supervised Interest Point Detection and Description"},{"location":"other_categories/SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/#superpoint-self-supervised-interest-point-detection-and-description","text":"\u8fd9\u7bc7\u8bba\u6587\u5b8c\u6210\u7684\u4efb\u52a1\u662f\u4ece\u4e00\u5f20\u56fe\u4e2d\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u63d0\u53d6\u51fakeypoints\uff0c\u8fd9\u4e9bkeypoints\u7684\u5173\u952e\u662f\u8981\u6c42\u5bf9\u65cb\u8f6c\uff0cscale\uff0c\u5e73\u79fb\u9c81\u68d2\u3002","title":"SuperPoint: Self-Supervised Interest Point Detection and Description"},{"location":"other_categories/SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/#training-procedure","text":"\u8bad\u7ec3\u8fc7\u7a0b\u9700\u8981\u4e09\u4e2a\u9636\u6bb5\uff0c\u7b2c\u4e00\u4e2a\u9636\u6bb5\uff0c\u4f7f\u7528\u4e00\u4e9b\u7b80\u5355\u7684\u51e0\u4f55\u56fe\u7247,\u7528\u4e00\u4e9b\u6ca1\u6709\u6b67\u4e49\u7684\u56fe\u7247\u548ckeypoint\uff0c\u8fdb\u884c\u521d\u59cb\u8bad\u7ec3\u3002 \u7b2c\u4e8c\u9636\u6bb5\uff0c\u4f7f\u7528\u4e00\u4e9b\u6ca1\u6709label\u7684\u771f\u5b9e\u56fe\u7247\uff0c\u5bf9\u540c\u4e00\u5f20\u56fe\uff0c\u4f7f\u7528Homographic transform(\u65cb\u8f6c\u5e73\u79fb\u7b49\u7684\u7ec4\u5408),\u8f6c\u6362\u6210\u4e00\u4e2abatch\u7684\u56fe.\u7528\u524d\u6587\u7684detector\uff0c\u751f\u6210keypoint\uff0c\u518d\u8f6c\u6362\u4e3a\u539f\u56fe\u5750\u6807\u7cfb\u4e2d\u3002\u8fd9\u4e2abatch\u4f1a\u5728\u5f53\u524d\u56fe\u5f62\u6210\u4e00\u4e2akeypoint\u7684heatmap. \\mathbf{x}=\\mathcal{H}^{-1} f_{\\theta}(\\mathcal{H}(I)) \\hat{F}\\left(I ; f_{\\theta}\\right)=\\frac{1}{N_{h}} \\sum_{i=1}^{N_{h}} \\mathcal{H}_{i}^{-1} f_{\\theta}\\left(\\mathcal{H}_{i}(I)\\right) \u7b2c\u4e09\u9636\u6bb5\uff0c\u4f7f\u7528joint training\u3002\u8fdb\u884c\u8bad\u7ec3","title":"Training procedure"},{"location":"other_categories/SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/#_1","text":"\u7b2c\u4e00\u9636\u6bb5\u53ea\u4f7f\u7528\u4e0a\u90e8\u5206\u90a3\u4e00\u652f\uff0c\u7b2c\u4e09\u9636\u6bb5\u4f1a\u6709descriptor\u90e8\u5206\u3002 Encoder\u7c7b\u4f3c\u4e8eVGG, Interest Point Decoder\uff0c65\u4e2aChannels,\u610f\u601d\u662f\u5468\u8fb9 8\\times 8 \u4e2a\u65b9\u5757\u6709keypoint\u7684\u6982\u7387\uff0c\u8fd8\u6709\u4e00\u4e2a\u662f\u7a7a\u7c7b. \u8fd9\u91cc\u7684 reshape \u5bf9\u5e94\u7684\u662fpytorch.nn.pixelshuffel\u64cd\u4f5c\uff0c\u5c06channel\u7684\u5185\u5bb9\u7ffb\u5230feature map\u7a7a\u95f4\u7684\u533a\u57df\u3002 \u5982\u679cground truth\u91cc\u9762 8\\times 8 \u533a\u57df\u91cc\u9762\u6709\u4e0d\u6b62\u4e00\u4e2a\u70b9\u5219\u968f\u673a\u9009\u53d6\u4e00\u4e2a\u4f5c\u4e3a\u8be5\u533a\u57df\u91cc\u9762\u6709\u6548\u7684ground truth\u3002 \u8fd9\u91cc\u4f7f\u7528cross-entropy,\u8ba1\u7b97 \\mathcal{L}_p \u8fdb\u884c\u8bad\u7ec3\u3002 descriptor\u7684\u8bad\u7ec3\uff0c\u4f9d\u9760\u7684\u662f\u540c\u4e00\u5f20\u56fe\uff0c\u5c06\u56fehomographic transform\u5230\u53e6\u4e00\u5f20\u56fe\u53bb\uff0c\u8981\u6c42keypoint\u5bf9\u5e94\u7684\u4f4d\u7f6e\u63cf\u8ff0\u76f8\u540c\u3002 s_{h w h^{\\prime} w^{\\prime}} \u6307\u4ee3\u5bf9\u5e94\u5173\u7cfb s_{h w h^{\\prime} w^{\\prime}}=\\left\\{\\begin{array}{ll}{1,} & {\\text { if } \\| \\widehat{\\mathcal{H} \\mathbf{p}_{h w}}-\\mathbf{p}_{h^{\\prime} w^{\\prime}}|| \\leq 8} \\\\ {0,} & {\\text { otherwise }}\\end{array}\\right. \\begin{aligned} l_{d}\\left(\\mathbf{d}, \\mathbf{d}^{\\prime} ; s\\right) &=\\lambda_{d} * s * \\max \\left(0, m_{p}-\\mathbf{d}^{T} \\mathbf{d}^{\\prime}\\right) \\\\ &+(1-s) * \\max \\left(0, \\mathbf{d}^{T} \\mathbf{d}^{\\prime}-m_{n}\\right) \\end{aligned} \\begin{aligned} \\mathcal{L}_{d}\\left(\\mathcal{D}, \\mathcal{D}^{\\prime}, S\\right) &=\\\\ \\frac{1}{\\left(H_{c} W_{c}\\right)^{2}} &\\sum_{h=1}^{H_{c}, W_{c} H_{c}, W_{c}} l_{d}\\left(\\mathbf{d}_{h w}, \\mathbf{d}_{h^{\\prime} w^{\\prime}}^{\\prime} ; s_{h w h^{\\prime} w^{\\prime}}\\right) \\end{aligned} \\begin{aligned} \\mathcal{L}\\left(\\mathcal{X}, \\mathcal{X}^{\\prime}, \\mathcal{D}, \\mathcal{D}^{\\prime} ; Y, Y^{\\prime}, S\\right) &=\\\\ \\mathcal{L}_{p}(\\mathcal{X}, Y)&+\\mathcal{L}_{p}\\left(\\mathcal{X}^{\\prime}, Y^{\\prime}\\right)+\\lambda \\mathcal{L}_{d}\\left(\\mathcal{D}, \\mathcal{D}^{\\prime}, S\\right) \\end{aligned}","title":"\u7f51\u7edc\u6a21\u578b"},{"location":"other_categories/SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/","text":"Unsupervised Learning of Depth and Ego-Motion from Video \u8fd9\u7bc7\u6587\u7ae0\u5bf9\u6211\u6765\u8bf4\u8bf4\u662f \u8fd9\u7bc7\u6587\u7ae0 \u7684\u524d\u7f6e \u8fd9\u7bc7\u6587\u7ae0\u89e3\u51b3\u7684\u95ee\u9898\u4e5f\u662f\u8f93\u5165\u89c6\u9891\uff0c\u8f93\u51fa\u6df1\u5ea6\u4e0e\u76f8\u673a\u8fd0\u52a8\u7684\u4f30\u8ba1 \u4e3bpipeline \u4f7f\u7528\u76f8\u90bb\u4e09\u5f20\u56fe\uff0c\u5c06\u5f53\u524d I_t \u8f93\u5165\u5230Depth CNN\u8f93\u51fa\u6df1\u5ea6\u56fe\uff0c\u5c06\u76f8\u90bb\u7684 I_{t-1}, I_{t+1} \u5171\u4e09\u5f20\u56fe\u8f93\u5165\u5230pose CNN(CNN\u76f4\u63a5\u8f93\u51fa\u516d\u81ea\u7531\u5ea6\u8fd0\u52a8)\u4e2d\u8f93\u51fa\u524d\u540e\u4e24\u4e2a\u76f8\u5bf9pose\uff0c\u5728\u5df2\u77e5\u5f53\u524d\u56fe\u7684\u6df1\u5ea6\u4ee5\u53ca\u8fd0\u52a8\uff0c\u5c06\u5f53\u524d\u56fe\u8f6c\u6362\u5230\u524d\u540e\u65f6\u523b\u7684\u56fe\u7247\u4e0a(\u4f7f\u7528bilinear intepolation\u8fdb\u884c\u53ef\u5bfc\u7684\u8f6c\u6362) \u672c\u6587\u8fd8\u6709\u66f4\u591a\u5185\u5bb9\u5982(Explainability mask)\uff0c\u6b64\u5904\u8ba8\u8bba\u5230\u6b64\u4e3a\u6b62\u3002","title":"Unsupervised Learning of Depth and Ego-Motion from Video"},{"location":"other_categories/SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/#unsupervised-learning-of-depth-and-ego-motion-from-video","text":"\u8fd9\u7bc7\u6587\u7ae0\u5bf9\u6211\u6765\u8bf4\u8bf4\u662f \u8fd9\u7bc7\u6587\u7ae0 \u7684\u524d\u7f6e \u8fd9\u7bc7\u6587\u7ae0\u89e3\u51b3\u7684\u95ee\u9898\u4e5f\u662f\u8f93\u5165\u89c6\u9891\uff0c\u8f93\u51fa\u6df1\u5ea6\u4e0e\u76f8\u673a\u8fd0\u52a8\u7684\u4f30\u8ba1","title":"Unsupervised Learning of Depth and Ego-Motion from Video"},{"location":"other_categories/SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/#pipeline","text":"\u4f7f\u7528\u76f8\u90bb\u4e09\u5f20\u56fe\uff0c\u5c06\u5f53\u524d I_t \u8f93\u5165\u5230Depth CNN\u8f93\u51fa\u6df1\u5ea6\u56fe\uff0c\u5c06\u76f8\u90bb\u7684 I_{t-1}, I_{t+1} \u5171\u4e09\u5f20\u56fe\u8f93\u5165\u5230pose CNN(CNN\u76f4\u63a5\u8f93\u51fa\u516d\u81ea\u7531\u5ea6\u8fd0\u52a8)\u4e2d\u8f93\u51fa\u524d\u540e\u4e24\u4e2a\u76f8\u5bf9pose\uff0c\u5728\u5df2\u77e5\u5f53\u524d\u56fe\u7684\u6df1\u5ea6\u4ee5\u53ca\u8fd0\u52a8\uff0c\u5c06\u5f53\u524d\u56fe\u8f6c\u6362\u5230\u524d\u540e\u65f6\u523b\u7684\u56fe\u7247\u4e0a(\u4f7f\u7528bilinear intepolation\u8fdb\u884c\u53ef\u5bfc\u7684\u8f6c\u6362) \u672c\u6587\u8fd8\u6709\u66f4\u591a\u5185\u5bb9\u5982(Explainability mask)\uff0c\u6b64\u5904\u8ba8\u8bba\u5230\u6b64\u4e3a\u6b62\u3002","title":"\u4e3bpipeline"},{"location":"other_categories/SLAM/VOLDOR/","text":"VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals \u8fd9\u7bc7paper\u8ba8\u8bba\u7684\u662f\u89c6\u89c9SLAM\u95ee\u9898\u7684\u89e3\u51b3\uff0c\u91cd\u70b9\u533a\u522b\u70b9: \u7a20\u5bc6\u5149\u6d41\u66ff\u4ee3\u7a00\u758f\u7279\u5f81\u70b9\u5339\u914d \u628arigidness map\u4f5c\u4e3a\u6982\u7387\u63a8\u7406\u7684\u4e00\u90e8\u5206\uff0c\u7528\u4e8e\u533a\u5206\u51fa\u8fd0\u52a8\u4e2d\u7684\u7269\u4f53\u50cf\u7d20 \u63d0\u51fa\u4e86\u65b0\u7684\u6982\u7387\u5206\u5e03\u62df\u5408\u4ee5\u53ca\u63a8\u7406\u65b9\u6cd5. \u6574\u4e2a\u6846\u67b6\u9ad8\u5ea6\u5e76\u884c\u5316\uff0c\u9002\u5408\u5728GPU\u4e0a\u76f4\u63a5\u5b8c\u6210SLAM\u4efb\u52a1 \u65b9\u6cd5 Notations \\mathcal{I} : \u56fe\u7247 \\mathcal{X} : \u89c2\u6d4b\u5230\u7684\u5149\u6d41\u573a \\mathcal{T} : \u76f8\u673a\u59ff\u6001. \\mathcal{\\theta} : \u6df1\u5ea6\u56fe \\mathcal{W} : Rigidness \u6982\u7387\u56fe\uff0c\u82e5 W(i,j)==0 \u8bf4\u660e\u662f\u52a8\u6001\u7269\u4f53\u50cf\u7d20. \\boldsymbol{\\pi} : \u6295\u5f71\u70b9\u7684\u5750\u6807\uff0c\u5df2\u77e5\u6df1\u5ea6\uff0c\u76f8\u673a\u59ff\u6001\u4ee5\u53ca\u76f8\u673a\u5185\u53c2\uff0c\u53ef\u4ee5\u5f97\u5230\u9759\u6001\u70b9\u7684\u6295\u5f71\u5173\u7cfb \\boldsymbol{\\pi}_{\\boldsymbol{t}}\\left(\\theta^{j}\\right)=\\boldsymbol{K}\\left(\\prod_{i=0}^{t} \\boldsymbol{T}_{\\boldsymbol{i}}\\right) \\theta^{j} \\boldsymbol{K}^{-1}\\left[x_{j} y_{j} 1\\right]^{T} \\boldsymbol{\\xi} : \u7531\u8fd0\u52a8\u4ea7\u751f\u7684\u5149\u6d41 \\boldsymbol{\\xi}_{\\boldsymbol{t}}\\left(\\theta^{j}\\right)=\\boldsymbol{\\pi}_{\\boldsymbol{t}}\\left(\\theta^{j}\\right)-\\boldsymbol{\\pi}_{\\boldsymbol{t - 1}}\\left(\\theta^{j}\\right) \u5bf9\u56fe\u7247\u89c2\u6d4b\u5230\u7684\u5149\u6d41\u573a\uff0c\u6211\u4eec\u53ef\u4ee5\u8868\u8fbe\u5176\u6982\u7387\u5206\u5e03, \u5176\u4e2d\u5206\u4e3a\u4e24\u90e8\u5206\u3002 - \u5bf9\u4e8e\u9759\u6001\u50cf\u7d20\uff0c\u5176\u6982\u7387\u4e0e\u8fd0\u52a8/\u6df1\u5ea6\u5f97\u5230\u7684\u5149\u6d41\u4e4b\u95f4\u7684\u6b8b\u5dee\u6709\u5173 - \u5bf9\u4e8e\u52a8\u6001\u50cf\u7d20\uff0c\u672c\u6587\u5047\u8bbe\u5176\u4e3a\u4e00\u4e2a\u5747\u5300\u5206\u5e03. \\begin{aligned} P\\left(\\boldsymbol{X}_{t}^{\\boldsymbol{\\pi}_{t-1}\\left(\\theta^{j}\\right)}\\right.&\\left.\\mid \\theta^{j}, \\boldsymbol{T}_{\\boldsymbol{t}}, W_{t}^{j} ; \\boldsymbol{T}_{\\mathbf{1}} \\cdots \\boldsymbol{T}_{\\boldsymbol{t - 1}}\\right) \\\\ &=\\left\\{\\begin{array}{ll} \\rho\\left(\\boldsymbol{\\xi}_{\\boldsymbol{t}}\\left(\\theta^{j}\\right) \\| \\boldsymbol{X}_{t}^{\\boldsymbol{\\pi}_{t-1}\\left(\\theta^{j}\\right)}\\right) & \\text { if } W_{t}^{j}=1 \\\\ \\mu\\left(\\boldsymbol{X}_{t}^{\\boldsymbol{\\pi}_{t-1}\\left(\\theta^{j}\\right)}\\right) & \\text { if } W_{t}^{j}=0 \\end{array}\\right. \\end{aligned} VO\u95ee\u9898\u5c31\u88ab\u5efa\u6a21\u6210\u6700\u5927\u5316\u4ee5\u4e0alikelihood\u7684\u6c42\u89e3\u95ee\u9898. \\begin{array}{c} \\underset{\\boldsymbol{\\theta}, \\mathbb{T}, \\mathrm{W}}{\\operatorname{argmax}} P(\\mathbb{X} \\mid \\boldsymbol{\\theta}, \\mathbb{T}, \\mathbb{W}) \\\\ =\\underset{\\boldsymbol{\\theta}, \\mathrm{T}, \\mathrm{W}}{\\operatorname{argmax}} \\prod_{t} \\prod_{j} P\\left(\\boldsymbol{X}_{t}^{\\boldsymbol{\\pi}_{t-1}\\left(\\theta^{j}\\right)} \\mid \\theta^{j}, \\boldsymbol{T}_{t}, W_{t}^{j}\\right) \\end{array} Fisk Residual Model \u7528\u4ec0\u4e48\u6982\u7387\u6a21\u578b\u5bf9\u6b8b\u5dee\u8fdb\u884c\u5efa\u6a21\u662f\u672c\u6587\u4e2d\u4e00\u4e2a\u5f88\u91cd\u8981\u7684\u9009\u62e9\u9879.\u672c\u6587\u505a\u4e86\u4ee5\u4e0b\u7684\u89c2\u5bdf hierarchical\u7684\u5149\u6d41\u65b9\u6cd5\uff0c\u5149\u6d41\u4f30\u8ba1\u7684\u8bef\u5dee\u4f1a\u4e0e\u5149\u6d41\u672c\u8eab\u7684\u503c\u6210\u6b63\u6bd4(\u5149\u6d41\u50cf\u7d20\u503c\u8d8a\u5927, \u7edd\u5bf9\u8bef\u5dee\u503c\u5c31\u8d8a\u5927). \u4f5c\u8005\u5c1d\u8bd5\u5bf9\u73b0\u6709\u7684\u591a\u4e2a\u5149\u6d41\u6a21\u578b\u7684\u8bef\u5dee\u7279\u6027\u8fdb\u884c\u6982\u7387\u62df\u5408\uff0c\u53d1\u73b0\u662f fisk distribution \u662f\u6700\u597d\u7684. Fisk distribution: \u4e5f\u79f0\u4e3aLog-logistic distribution, \u6307\u7684\u662f\u4e00\u4e2a\u968f\u673a\u53d8\u91cf\u7684\u5bf9\u6570\u503c\u670d\u4ece logistic distribution . logistic distribution\u5728\u9ed8\u8ba4\u7684\u53c2\u6570\u4e0b\u7684\u7d2f\u79ef\u51fd\u6570\u5c31\u662f\u6211\u4eec\u5e38\u7528\u7684sigmoid. \u800c fisk\u7684\u6982\u7387\u5bc6\u5ea6\u51fd\u6570\u4e3a: f(x ; \\alpha, \\beta)=\\frac{(\\beta / \\alpha)(x / \\alpha)^{\\beta-1}}{\\left(1+(x / \\alpha)^{\\beta}\\right)^{2}} \u63a8\u7406\u65b9\u6cd5 \u6df1\u5ea6\u4e0erigidness\u7684\u66f4\u65b0 \\theta_{\\mathrm{MIE}}^{j}=\\operatorname{argmax}_{\\theta^{j *}} \\sum_{t} q\\left(W_{t}^{j}=1\\right) \\log \\frac{P\\left(\\boldsymbol{X}_{t}^{\\pi_{t-1}\\left(\\theta^{j}\\right)} \\mid \\theta^{j}=\\theta^{j *}, W_{t}^{j}=1\\right)}{\\sum_{W_{t}^{j}} P\\left(\\boldsymbol{X}_{t}^{\\left.\\pi_{t-1}^{(\\theta j}\\right)} \\mid \\theta^{\\left.j=\\theta^{j *}, W_{t}^{j}\\right)}\\right.} \u7b97\u6cd5\u4e0a\u9996\u5148\u5c06 W \u521d\u59cb\u5316\u4e3a1, T \u4e0e \\theta \u7531\u7a20\u5bc6\u5149\u6d41\u6700\u5c0f\u4e8c\u4e58\u6cd5\u5f97\u5230\u521d\u59cb\u89e3. \u7136\u540e\u5faa\u73af\u5730\u4f18\u5316\u76f8\u673a\u59ff\u6001\u3001\u6df1\u5ea6\u4ee5\u53ca\u521a\u5ea6\u56fe. VOLDOR+SLAM: For the times when feature-based or direct methods are not good enough pdf \u8fd9\u7bc7paper\u5c06\u524d\u9762\u7684VOLDOR\u6269\u5c55\u4e3a\u4e86\u4e00\u4e2a\u5b8c\u6574\u7684SLAM\u7cfb\u7edf,","title":"VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals"},{"location":"other_categories/SLAM/VOLDOR/#voldor-visual-odometry-from-log-logistic-dense-optical-flow-residuals","text":"\u8fd9\u7bc7paper\u8ba8\u8bba\u7684\u662f\u89c6\u89c9SLAM\u95ee\u9898\u7684\u89e3\u51b3\uff0c\u91cd\u70b9\u533a\u522b\u70b9: \u7a20\u5bc6\u5149\u6d41\u66ff\u4ee3\u7a00\u758f\u7279\u5f81\u70b9\u5339\u914d \u628arigidness map\u4f5c\u4e3a\u6982\u7387\u63a8\u7406\u7684\u4e00\u90e8\u5206\uff0c\u7528\u4e8e\u533a\u5206\u51fa\u8fd0\u52a8\u4e2d\u7684\u7269\u4f53\u50cf\u7d20 \u63d0\u51fa\u4e86\u65b0\u7684\u6982\u7387\u5206\u5e03\u62df\u5408\u4ee5\u53ca\u63a8\u7406\u65b9\u6cd5. \u6574\u4e2a\u6846\u67b6\u9ad8\u5ea6\u5e76\u884c\u5316\uff0c\u9002\u5408\u5728GPU\u4e0a\u76f4\u63a5\u5b8c\u6210SLAM\u4efb\u52a1","title":"VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals"},{"location":"other_categories/SLAM/VOLDOR/#_1","text":"","title":"\u65b9\u6cd5"},{"location":"other_categories/SLAM/VOLDOR/#notations","text":"\\mathcal{I} : \u56fe\u7247 \\mathcal{X} : \u89c2\u6d4b\u5230\u7684\u5149\u6d41\u573a \\mathcal{T} : \u76f8\u673a\u59ff\u6001. \\mathcal{\\theta} : \u6df1\u5ea6\u56fe \\mathcal{W} : Rigidness \u6982\u7387\u56fe\uff0c\u82e5 W(i,j)==0 \u8bf4\u660e\u662f\u52a8\u6001\u7269\u4f53\u50cf\u7d20. \\boldsymbol{\\pi} : \u6295\u5f71\u70b9\u7684\u5750\u6807\uff0c\u5df2\u77e5\u6df1\u5ea6\uff0c\u76f8\u673a\u59ff\u6001\u4ee5\u53ca\u76f8\u673a\u5185\u53c2\uff0c\u53ef\u4ee5\u5f97\u5230\u9759\u6001\u70b9\u7684\u6295\u5f71\u5173\u7cfb \\boldsymbol{\\pi}_{\\boldsymbol{t}}\\left(\\theta^{j}\\right)=\\boldsymbol{K}\\left(\\prod_{i=0}^{t} \\boldsymbol{T}_{\\boldsymbol{i}}\\right) \\theta^{j} \\boldsymbol{K}^{-1}\\left[x_{j} y_{j} 1\\right]^{T} \\boldsymbol{\\xi} : \u7531\u8fd0\u52a8\u4ea7\u751f\u7684\u5149\u6d41 \\boldsymbol{\\xi}_{\\boldsymbol{t}}\\left(\\theta^{j}\\right)=\\boldsymbol{\\pi}_{\\boldsymbol{t}}\\left(\\theta^{j}\\right)-\\boldsymbol{\\pi}_{\\boldsymbol{t - 1}}\\left(\\theta^{j}\\right) \u5bf9\u56fe\u7247\u89c2\u6d4b\u5230\u7684\u5149\u6d41\u573a\uff0c\u6211\u4eec\u53ef\u4ee5\u8868\u8fbe\u5176\u6982\u7387\u5206\u5e03, \u5176\u4e2d\u5206\u4e3a\u4e24\u90e8\u5206\u3002 - \u5bf9\u4e8e\u9759\u6001\u50cf\u7d20\uff0c\u5176\u6982\u7387\u4e0e\u8fd0\u52a8/\u6df1\u5ea6\u5f97\u5230\u7684\u5149\u6d41\u4e4b\u95f4\u7684\u6b8b\u5dee\u6709\u5173 - \u5bf9\u4e8e\u52a8\u6001\u50cf\u7d20\uff0c\u672c\u6587\u5047\u8bbe\u5176\u4e3a\u4e00\u4e2a\u5747\u5300\u5206\u5e03. \\begin{aligned} P\\left(\\boldsymbol{X}_{t}^{\\boldsymbol{\\pi}_{t-1}\\left(\\theta^{j}\\right)}\\right.&\\left.\\mid \\theta^{j}, \\boldsymbol{T}_{\\boldsymbol{t}}, W_{t}^{j} ; \\boldsymbol{T}_{\\mathbf{1}} \\cdots \\boldsymbol{T}_{\\boldsymbol{t - 1}}\\right) \\\\ &=\\left\\{\\begin{array}{ll} \\rho\\left(\\boldsymbol{\\xi}_{\\boldsymbol{t}}\\left(\\theta^{j}\\right) \\| \\boldsymbol{X}_{t}^{\\boldsymbol{\\pi}_{t-1}\\left(\\theta^{j}\\right)}\\right) & \\text { if } W_{t}^{j}=1 \\\\ \\mu\\left(\\boldsymbol{X}_{t}^{\\boldsymbol{\\pi}_{t-1}\\left(\\theta^{j}\\right)}\\right) & \\text { if } W_{t}^{j}=0 \\end{array}\\right. \\end{aligned} VO\u95ee\u9898\u5c31\u88ab\u5efa\u6a21\u6210\u6700\u5927\u5316\u4ee5\u4e0alikelihood\u7684\u6c42\u89e3\u95ee\u9898. \\begin{array}{c} \\underset{\\boldsymbol{\\theta}, \\mathbb{T}, \\mathrm{W}}{\\operatorname{argmax}} P(\\mathbb{X} \\mid \\boldsymbol{\\theta}, \\mathbb{T}, \\mathbb{W}) \\\\ =\\underset{\\boldsymbol{\\theta}, \\mathrm{T}, \\mathrm{W}}{\\operatorname{argmax}} \\prod_{t} \\prod_{j} P\\left(\\boldsymbol{X}_{t}^{\\boldsymbol{\\pi}_{t-1}\\left(\\theta^{j}\\right)} \\mid \\theta^{j}, \\boldsymbol{T}_{t}, W_{t}^{j}\\right) \\end{array}","title":"Notations"},{"location":"other_categories/SLAM/VOLDOR/#fisk-residual-model","text":"\u7528\u4ec0\u4e48\u6982\u7387\u6a21\u578b\u5bf9\u6b8b\u5dee\u8fdb\u884c\u5efa\u6a21\u662f\u672c\u6587\u4e2d\u4e00\u4e2a\u5f88\u91cd\u8981\u7684\u9009\u62e9\u9879.\u672c\u6587\u505a\u4e86\u4ee5\u4e0b\u7684\u89c2\u5bdf hierarchical\u7684\u5149\u6d41\u65b9\u6cd5\uff0c\u5149\u6d41\u4f30\u8ba1\u7684\u8bef\u5dee\u4f1a\u4e0e\u5149\u6d41\u672c\u8eab\u7684\u503c\u6210\u6b63\u6bd4(\u5149\u6d41\u50cf\u7d20\u503c\u8d8a\u5927, \u7edd\u5bf9\u8bef\u5dee\u503c\u5c31\u8d8a\u5927). \u4f5c\u8005\u5c1d\u8bd5\u5bf9\u73b0\u6709\u7684\u591a\u4e2a\u5149\u6d41\u6a21\u578b\u7684\u8bef\u5dee\u7279\u6027\u8fdb\u884c\u6982\u7387\u62df\u5408\uff0c\u53d1\u73b0\u662f fisk distribution \u662f\u6700\u597d\u7684.","title":"Fisk Residual Model"},{"location":"other_categories/SLAM/VOLDOR/#fisk-distribution","text":"\u4e5f\u79f0\u4e3aLog-logistic distribution, \u6307\u7684\u662f\u4e00\u4e2a\u968f\u673a\u53d8\u91cf\u7684\u5bf9\u6570\u503c\u670d\u4ece logistic distribution . logistic distribution\u5728\u9ed8\u8ba4\u7684\u53c2\u6570\u4e0b\u7684\u7d2f\u79ef\u51fd\u6570\u5c31\u662f\u6211\u4eec\u5e38\u7528\u7684sigmoid. \u800c fisk\u7684\u6982\u7387\u5bc6\u5ea6\u51fd\u6570\u4e3a: f(x ; \\alpha, \\beta)=\\frac{(\\beta / \\alpha)(x / \\alpha)^{\\beta-1}}{\\left(1+(x / \\alpha)^{\\beta}\\right)^{2}}","title":"Fisk distribution:"},{"location":"other_categories/SLAM/VOLDOR/#_2","text":"","title":"\u63a8\u7406\u65b9\u6cd5"},{"location":"other_categories/SLAM/VOLDOR/#rigidness","text":"\\theta_{\\mathrm{MIE}}^{j}=\\operatorname{argmax}_{\\theta^{j *}} \\sum_{t} q\\left(W_{t}^{j}=1\\right) \\log \\frac{P\\left(\\boldsymbol{X}_{t}^{\\pi_{t-1}\\left(\\theta^{j}\\right)} \\mid \\theta^{j}=\\theta^{j *}, W_{t}^{j}=1\\right)}{\\sum_{W_{t}^{j}} P\\left(\\boldsymbol{X}_{t}^{\\left.\\pi_{t-1}^{(\\theta j}\\right)} \\mid \\theta^{\\left.j=\\theta^{j *}, W_{t}^{j}\\right)}\\right.} \u7b97\u6cd5\u4e0a\u9996\u5148\u5c06 W \u521d\u59cb\u5316\u4e3a1, T \u4e0e \\theta \u7531\u7a20\u5bc6\u5149\u6d41\u6700\u5c0f\u4e8c\u4e58\u6cd5\u5f97\u5230\u521d\u59cb\u89e3. \u7136\u540e\u5faa\u73af\u5730\u4f18\u5316\u76f8\u673a\u59ff\u6001\u3001\u6df1\u5ea6\u4ee5\u53ca\u521a\u5ea6\u56fe.","title":"\u6df1\u5ea6\u4e0erigidness\u7684\u66f4\u65b0"},{"location":"other_categories/SLAM/VOLDOR/#voldorslam-for-the-times-when-feature-based-or-direct-methods-are-not-good-enough","text":"pdf \u8fd9\u7bc7paper\u5c06\u524d\u9762\u7684VOLDOR\u6269\u5c55\u4e3a\u4e86\u4e00\u4e2a\u5b8c\u6574\u7684SLAM\u7cfb\u7edf,","title":"VOLDOR+SLAM: For the times when feature-based or direct methods are not good enough"},{"location":"other_categories/Segmentation/Actor-Critic%20Instance%20Segmentation/","text":"Actor-Critic Instance Segmentation \u8fd9\u7bc7\u8bba\u6587\u4f7f\u7528actor critic \u5f3a\u5316\u5b66\u4e60\uff0c\u4fbf\u4e8e\u9884\u6d4b\u4e00\u7cfb\u5217\u7684segmentation result \u6574\u4f53\u7ed3\u6784 \u5f3a\u5316\u5b66\u4e60\u5b9a\u4e49 \u72b6\u6001\u7a7a\u95f4 s_t = (I, M_t) \u5176\u4e2d I \u4e3a\u8f93\u5165\u56fe\u7247\uff0c M_t \u4e3a\u7b2c t \u65f6\u523b\u7684\u7efc\u5408mask \u884c\u52a8 a_t \u662fdecoder\u7684\u8f93\u5165\uff0c\u662f\u4e00\u4e2a\u8f83\u4e3a\u4f4e\u7ef4\u7684\u8fde\u7eed\u77e2\u91cf \u72b6\u6001\u8f6c\u79fb T = (I, max(M_t, D(a_t))) \uff0c\u76f8\u5f53\u4e8e\u5c06\u65b0decode\u7684mask\u52a0\u548c\u5728\u539f\u6765\u7684\u7d2f\u52a0mask\u4e0a reward\uff0c\u5148\u5b9a\u4e49 \\phi_t = max(\\sum_{i=1}^t F(S_i, T_{ki})) \u610f\u601d\u662f\u5bfb\u627e\u6700\u4f18\u7684predicted instance-ground truth\u642d\u914d\uff0c\u5f97\u5230\u7684\u6700\u5927\u5956\u52b1\uff0c\u7136\u540ereward\u5c31\u662f r_t = \\phi(s_{t+1}) - \\phi(s_t) \u4f7f\u7528\u91cd\u70b9\uff1a 1. decoder\u9700\u8981\u63d0\u524dtrain\u597d\uff0c\u6700\u597d\u4e0d\u8981\u6539\u53d8,\u9700\u8981\u7684\u662f\u4e00\u4e2aconditional variational encoder(cVAE) 2. \u9700\u8981\u5141\u8bb8critics warm-up AC training","title":"Actor-Critic Instance Segmentation"},{"location":"other_categories/Segmentation/Actor-Critic%20Instance%20Segmentation/#actor-critic-instance-segmentation","text":"\u8fd9\u7bc7\u8bba\u6587\u4f7f\u7528actor critic \u5f3a\u5316\u5b66\u4e60\uff0c\u4fbf\u4e8e\u9884\u6d4b\u4e00\u7cfb\u5217\u7684segmentation result","title":"Actor-Critic Instance Segmentation"},{"location":"other_categories/Segmentation/Actor-Critic%20Instance%20Segmentation/#_1","text":"","title":"\u6574\u4f53\u7ed3\u6784"},{"location":"other_categories/Segmentation/Actor-Critic%20Instance%20Segmentation/#_2","text":"\u72b6\u6001\u7a7a\u95f4 s_t = (I, M_t) \u5176\u4e2d I \u4e3a\u8f93\u5165\u56fe\u7247\uff0c M_t \u4e3a\u7b2c t \u65f6\u523b\u7684\u7efc\u5408mask \u884c\u52a8 a_t \u662fdecoder\u7684\u8f93\u5165\uff0c\u662f\u4e00\u4e2a\u8f83\u4e3a\u4f4e\u7ef4\u7684\u8fde\u7eed\u77e2\u91cf \u72b6\u6001\u8f6c\u79fb T = (I, max(M_t, D(a_t))) \uff0c\u76f8\u5f53\u4e8e\u5c06\u65b0decode\u7684mask\u52a0\u548c\u5728\u539f\u6765\u7684\u7d2f\u52a0mask\u4e0a reward\uff0c\u5148\u5b9a\u4e49 \\phi_t = max(\\sum_{i=1}^t F(S_i, T_{ki})) \u610f\u601d\u662f\u5bfb\u627e\u6700\u4f18\u7684predicted instance-ground truth\u642d\u914d\uff0c\u5f97\u5230\u7684\u6700\u5927\u5956\u52b1\uff0c\u7136\u540ereward\u5c31\u662f r_t = \\phi(s_{t+1}) - \\phi(s_t) \u4f7f\u7528\u91cd\u70b9\uff1a 1. decoder\u9700\u8981\u63d0\u524dtrain\u597d\uff0c\u6700\u597d\u4e0d\u8981\u6539\u53d8,\u9700\u8981\u7684\u662f\u4e00\u4e2aconditional variational encoder(cVAE) 2. \u9700\u8981\u5141\u8bb8critics warm-up AC training","title":"\u5f3a\u5316\u5b66\u4e60\u5b9a\u4e49"},{"location":"other_categories/Segmentation/BEV_segmentation/","text":"Summary of Multiple Papers on BEV Fusion \u5728\u7279\u65af\u62c9\u516c\u5f00\u65e5\u5ba3\u4f20\u4e86\u73af\u89c6\u6444\u50cf\u5934\u5230BEV\u8bed\u4e49\u5206\u5272\u7684\u76f4\u63a5\u9884\u6d4b\u65b9\u6848\u524d\u540e(\u5f88\u591a\u7814\u7a76\u5728Tesla\u524d\u5df2\u7ecf\u6709\u5728\u505a)\uff0c\u51fa\u73b0\u4e86\u5927\u91cf\u7684\u8bba\u6587\u5b9e\u73b0\u76f8\u5173\u7684\u529f\u80fd\u65b9\u6848\u3002\u5f88\u591a\u7684\u5de5\u4f5c\u4e5f\u4e0eBEV Fusion\u7684\u5b9e\u73b0\u6709\u5173\u3002\u5176\u6838\u5fc3\u7684\u89c2\u70b9\u4e0e\u76f8\u5173\u6280\u672f\u8981\u7d20\u53ef\u4ee5\u603b\u7ed3\u5982\u4e0b\uff1a \u4ee5\u4fef\u89c6\u56fe\uff0cBEV\u7684\u7a20\u5bc6\u7279\u5f81\u56fe\u4f5c\u4e3a\u591a\u76f8\u673a/\u591a\u4f20\u611f\u5668\u7684\u878d\u5408\u4f4d\u70b9\uff0c\u4e14BEV\u76f4\u63a5\u8fdb\u884c\u7a20\u5bc6\u8f93\u51fa(\u5982\u8bed\u4e49\u5206\u5272\u4ee5\u53ca\u7269\u4f53\u68c0\u6d4b)\u3002 \u628a\u76f8\u673a\u7279\u5f81\u8f6c\u6362\u5230BEV\u7a20\u5bc6\u7279\u5f81\u56fe\u4e2d\uff0c\u91cc\u9762\u53ef\u80fd\u7275\u6d89\u5230\u76f8\u673a\u7684\u6295\u5f71\u51e0\u4f55\u3001\u5916\u53c2\u3001\u50cf\u7d20\u6df1\u5ea6\u7b49\u95ee\u9898\u3002\u76ee\u524d\u4e5f\u6709\u591a\u79cd\u5b9e\u73b0\u65b9\u6848\u3002 \u628a\u70b9\u4e91\u7279\u5f81\u8f6c\u6362\u5230BEV\u7a20\u5bc6\u7279\u5f81\u56fe\u4e2d\uff0c\u76ee\u524d\u5e38\u89c1\u7684\u65b9\u6848\u662fVoxelization\u4ee5\u53caPointPillar\u65b9\u6848, \u5176\u4e2dPointPillar\u65b9\u6848\uff0c\u6700\u7ec8\u8f93\u51fa\u76f4\u63a5\u662fBEV 2D\u7279\u5f81\u56fe\uff0c\u53d7\u5230\u7684\u5173\u6ce8\u6bd4\u8f83\u591a\u3002 \u8f93\u51fa\u7684\u5185\u5bb9\u53ef\u4ee5\u975e\u5e38\u4e30\u5bcc\uff0c\u73b0\u6709\u5df2\u7ecf\u6709\u5927\u91cf\u7684\u5229\u7528\u7a20\u5bc6\u7684BEV\u7279\u5f81\u8fdb\u884c\u8f93\u51fa\u7684\u5404\u79cd\u5de5\u4f5c\uff0c\u4e14Nuscenes\u7b49\u6570\u636e\u96c6\u63d0\u4f9b\u5927\u91cf\u7684\u6807\u6ce8\u6570\u636e\u3002\u7269\u4f53\u68c0\u6d4b\u3001\u8f66\u9053\u68c0\u6d4b\u3001\u9ad8\u7cbe\u5730\u56fe\u5efa\u9020\u3001\u751a\u81f3\u8f68\u8ff9\u9884\u6d4b\u3001\u8def\u5f84\u89c4\u5212\u3002 \u5173\u4e8eBEV\u5185\u7684\u76ee\u6807\u68c0\u6d4b\uff0c\u7531CenterNet, CenterPoint\u76f8\u5173\u7684\u5de5\u4f5c\u7ed3\u6784\u7b80\u5355\u800c\u4e14\u53ef\u4ee5\u8f7b\u677e\u6269\u5c55\u51fa\u590d\u6742\u7684\u8f93\u51fa\u5934\uff0cAnchorLess\u7684\u8bbe\u8ba1\u5728BEV\u68c0\u6d4b\u4e5f\u6bd4\u8f83\u5403\u9999\uff0c\u56e0\u800c\u662f\u591a\u4efb\u52a1\u65b9\u6848\u4e2d\u7684\u5e38\u7528\u9009\u62e9\u3002 \u5173\u4e8e\u76f8\u673a\u7279\u5f81\u5230BEV\u7a20\u5bc6\u7279\u5f81\u56fe\u7684\u8f6c\u6362\uff0c\u76ee\u524d\u89c2\u5bdf\u5230\u7684\u51e0\u79cd\u65b9\u6848: \u4f7f\u7528\u5355\u76ee\u6df1\u5ea6\uff0c\u9884\u6d4b\u6bcf\u4e00\u4e2a\u50cf\u7d20\u7684\u6df1\u5ea6\u503c/\u6df1\u5ea6\u5206\u5e03\uff0c\u628a\u56fe\u50cf\u7279\u5f81\u8f6c\u6362\u4e3a\u79bb\u6563\u7684\u70b9\u4e91\u7279\u5f81/\u7a20\u5bc6\u7684Voxelized 3D\u7279\u5f81\uff0c \u518d\u8f6c\u6362\u5230BEV 2D. \u4f7f\u7528Inverse Perspective Mapping, \u5047\u8bbe\u56fe\u7247\u6240\u6709\u7684\u70b9\u90fd\u5728\u5730\u9762(\u53ef\u4ee5\u7406\u89e3\u4e3a\u540c\u4e00\u4e2a\u5e73\u9762\u7684\u70b9\u5728\u76f8\u673a\u5e73\u9762\u7684\u6295\u5f71)\uff0cBEV\u7279\u5f81\u56fe\u53ef\u4ee5\u7406\u89e3\u662f\u5730\u9762\u7279\u5f81\u5728\u4e00\u4e2a\u4fef\u77b0\u76f8\u673a\u4e2d\u7684\u6295\u5f71\uff0c\u540c\u4e00\u4e2a\u5e73\u9762\u5728\u4e0d\u540c\u76f8\u673a\u4e2d\u7684\u6295\u5f71\u7684\u5404\u70b9\u5750\u6807\u53ef\u4ee5\u7531 Homography Matrix \u8054\u7cfb\u8d77\u6765\uff0c \u8fd9\u4e2a\u77e9\u9635\u53ef\u4ee5\u7528\u5730\u9762\u4e0a\u4e00\u4e2a\u65b9\u5f62\u56db\u4e2a\u9876\u70b9\u5728\u4e24\u4e2a\u76f8\u673a\u4e2d\u7684\u5750\u6807\u6c42\u89e3\uff0c\u4e5f\u53ef\u4ee5\u7528\u76f8\u673a\u7684\u5185\u5916\u53c2\u76f4\u63a5\u8ba1\u7b97\u83b7\u5f97\u3002 \u4f7f\u7528\u5168\u8fde\u63a5\u5c42\uff0c\u8ba9\u7f51\u7edc\u51b3\u5b9aBEV\u4e2d\u7684\u6bcf\u4e00\u4e2a\u50cf\u7d20\u662f\u7531\u524d\u7f6e\u7279\u5f81\u7684\u54ea\u4e9b\u4f4d\u7f6e\u7684\u50cf\u7d20\u70b9\u52a0\u6743\u8ba1\u7b97\u7684\u3002\u8fd9\u79cd\u505a\u6cd5\u5e38\u5e38\u8981\u4e3a\u4e0d\u540c\u7684\u76f8\u673a\u505a\u4e0d\u540c\u7684\u5168\u8fde\u63a5\u5c42\uff0c\u4e14\u524d\u7f6e\u56fe\u4e2d\u70b9\u7684\u8bed\u4e49\u4fe1\u606f\u4e0d\u5f71\u54cd\u5176\u4e0eBEV\u7684\u8fde\u63a5;\u4e5f\u5c31\u662f\u65e0\u6cd5\u901a\u8fc7\u5224\u65ad\u56fe\u7247\u70b9\u662f\u4e0d\u662f\u5730\u9762\u70b9\uff0c\u628a\u5b83\u7684\u7279\u5f81\u52a0\u5230BEV\u7684\u4e0d\u540c\u4f4d\u7f6e\u4e0a\u3002 \u4f7f\u7528Attention\u5c42\uff0c\u8ba9\u7f51\u7edc\u6839\u636e\u5b66\u4e60\u5230\u7684\u5185\u5bb9\u4ee5\u53ca\u56fe\u7247\u7684\u8bed\u4e49\u4fe1\u606f\uff0c\u5224\u65adBEV\u4e0a\u7684\u6bcf\u4e00\u4e2a\u7279\u5f81\u5e94\u8be5\u5982\u4f55\u7531\u524d\u7f6e\u76f8\u673a\u7684\u7279\u5f81\u6784\u6210\u3002\u5728\u6570\u636e\u91cf\u6781\u5927\u7684\u60c5\u51b5\u4e0b\uff0c\u53ef\u80fd\u53ef\u4ee5\u8ba9\u7f51\u7edc\u5b66\u4e60\u5230\u5982\u4f55\u5b9e\u73b0\u6839\u636e\u8bed\u4e49\u4ee5\u53ca\u4f4d\u7f6e\u5206\u914d\u6c42\u548c\u6743\u91cd\u3002 \u672c\u7ad9\u6b64\u524d\u4e5f\u6709\u4e00\u5b9a\u7684\u6536\u96c6: Lift-Splat-Shoot : \u4efb\u52a1: BEV\u9884\u6d4b\u8f66\u8f86\u8bed\u4e49\u5206\u5272\uff0c\u8def\u5f84\u89c4\u5212 \u76f8\u673a: \u9884\u6d4b\u6df1\u5ea6\u7684\u5206\u5e03\uff0c\u6839\u636e\u5206\u5e03\u5c06\u524d\u666f\u56fe\u6295\u5c04\u5230\u76f8\u673a\u5750\u6807\u7cfb\u4e0b\u7684\u4e00\u4e2aVoxel\u4e2d\u3002\u518d\u6839\u636e\u76f8\u673a\u7684\u5916\u53c2\uff0c\u628a\u5168\u5c40\u7528\u7684BEV\u5e73\u9762\u8303\u56f4\u5185\u7684Voxel\u70b9\u7559\u4e0b\uff0c\u96c6\u5408\u8fd9\u4e9b\u70b9\u4f7f\u7528PointPillar\u5904\u7406\u5e76\u538b\u5230BEV 2D.\u4e2d. PanopticBEV : \u4efb\u52a1: \u524d\u5411\u6444\u50cf\u5934\u7684\u5168\u666f\u5206\u5272 \u76f8\u673a\u8f6c\u6362\u65b9\u6848: \u878d\u5408\u4e86\u4e24\u4e2a\u505a\u6cd5\uff0c\u4e00\u4e2a\u5206\u652f\u91c7\u7528IPM\u8f6c\u6362\uff0c\u53e6\u4e00\u4e2a\u5206\u652f\u5bf9\u6bcf\u4e00\u4e2a\u50cf\u7d20\u70b9\u9884\u6d4b\u5176\u6df1\u5ea6\uff0c\u4f7f\u7528\u76f8\u673a\u5185\u53c2\u6269\u5c55\u6210\u76f8\u673a\u5750\u6807\u7cfb\u76843D\u7279\u5f81\u56fe\uff0c 3D grid_sample\u8f6c\u5230BEV\u4e0a\u3002 HDMapNet : \u4efb\u52a1: BEV\u9884\u6d4b\u9ad8\u7cbe\u5730\u56fe/\u5730\u56fe\u5143\u7d20\u8bed\u4e49\u5206\u5272. \u76f8\u673a\u8f6c\u6362\u65b9\u6848: Neural View Transformer \\rightarrow \u4e0d\u540c\u76f8\u673a\u4e0d\u540c\u5168\u8fde\u63a5\u5c42\u5148\u8f6c\u5230\u5404\u81ea\u7684\u524d\u5411BEV\u4e2d\uff0c\u518d\u6839\u636e\u5916\u53c2\u8f6c\u5230\u5168\u5c40BEV \u70b9\u4e91\u4f7f\u7528: PointPillar BEVFusion: Multi-Task Multi-Sensor Fusion with Unified Bird\u2019s-Eye View Representation pdf code \u4efb\u52a1: \u4e09\u7ef4\u76ee\u6807\u68c0\u6d4b & \u5730\u56fe\u5206\u5272\u3002 \u76f8\u673a\u8f6c\u6362\u65b9\u6848: \u9884\u6d4b\u6df1\u5ea6\u5206\u5e03\uff0c\u5c06\u56fe\u50cf\u7279\u5f81\u6269\u5c55\u4e3a (N, H, W, D) \u7684\u70b9\u4e91\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u52a0\u901f\u7684BEV Pooling \u6a21\u5757\uff0c\u5feb\u901f\u5c06\u843d\u5728BEV\u4e0a\u6bcf\u4e2abin\u7684\u70b9\u878d\u5408\u8d77\u6765(mean/max)\u3002 BEV Pooling \u65b9\u6848: \u5728\u5185\u53c2\u5916\u53c2\u56fa\u5b9a\u7684\u60c5\u51b5\u4e0b\uff0c\u6bcf\u4e2a\u76f8\u673a\u9884\u6d4b\u7684\u70b9\u4e91\u65b9\u9635\uff0c\u5728\u8f66\u8eab\u5750\u6807\u4e0a\u90fd\u662f\u56fa\u5b9a\u7684\u3002\u8bb0\u5f55\u70b9\u4e91\u65b9\u9635\u6bcf\u4e00\u4e2a\u70b9\u4f1a\u843d\u5230BEV\u4e0a\u7684\u54ea\u4e00\u4e2a\u70b9\uff0c\u5e76\u6839\u636e\u843d\u70b9\u5728BEV\u56fe\u4e2d\u7684index\u8fdb\u884c\u6392\u5e8f (\u8fd9\u4e9b\u90fd\u53ef\u4ee5\u63d0\u524d\u8ba1\u7b97)\u3002 \u8ba1\u7b97\u65f6\uff0c\u6839\u636e\u5df2\u7ecf\u63d0\u524d\u8ba1\u7b97\u597d\u7684\u6392\u5e8f\u5bf9\u70b9\u4e91\u65b9\u9635\u8fdb\u884c\u91cd\u6392\uff0c\u53ef\u4ee5\u5f97\u5230\u4e0a\u56fe(b).i \u7684\u5f62\u6001\u3002\u6bcf\u4e00\u4e2aBEV\u70b9\u5bf9\u5e94\u7684\u6240\u6709cam features\u4f1a\u5728\u5185\u5b58\u4e0a\u8fde\u7eed\uff0c\u5e76\u4e14\u6211\u4eec\u9884\u8ba1\u7b97\u65f6\uff0c\u4e5f\u53ef\u4ee5\u7f13\u5b58\u6bcf\u4e00\u4e2aBEV\u70b9\u5bf9\u5e94\u7684\u6570\u7ec4\u7684\u8d77\u59cb\u4e0e\u7ec8\u6b62\u5750\u6807\u3002 \u4f5c\u8005\u5b9e\u73b0\u4e86\u4e00\u4e2a\u4e13\u7528\u7684GPU\u6838\uff0c\u8ba9GPU\u7684\u6bcf\u4e00\u4e2a\u7ebf\u7a0b\u5206\u522b\u8d1f\u8d23\u4e00\u4e2aBEV\u70b9\u5185\u6240\u6709\u70b9\u7684\u7d2f\u52a0\u8ba1\u7b97\u3002 \u6781\u5927\u5730\u52a0\u901f\u4e86BEV pooling\u8fd9\u4e00\u6b65\u7684\u8fd0\u7b97\u901f\u5ea6\u3002\u4f7f\u5f97\u76f4\u63a5\u9884\u6d4b\u6df1\u5ea6\u5206\u5e03\u53d8\u6210\u4e00\u4e2a\u8fd0\u7b97\u538b\u529b\u5c1a\u53ef\u7684\u6a21\u5757\u3002 BEVFusion: A Simple and Robust LiDAR-Camera Fusion Framework pdf code \u4efb\u52a1: \u4e09\u7ef4\u76ee\u6807\u68c0\u6d4b \u76f8\u673a\u8f6c\u6362\u65b9\u6848: \u4e0eLSS\u4e00\u81f4\u3002 \u52a0\u5165\u4e86LiDAR. \u76f8\u673a\u4e0eBEV\u7279\u5f81\u8f93\u51fa\u4e4b\u95f4\u7684\u8f6c\u6362\u7684\u8fdb\u4e00\u6b65\u6570\u5b66\u5316\u6574\u7406: \u76ee\u524d\u7684\u65b9\u6848\u6765\u8bf4\uff0c\u90fd\u662f\u5bf9BEV\u4e0a\u6bcf\u4e00\u4e2a\u50cf\u7d20\u70b9\uff0c\u53bb\"\u5bfb\u627e\"\uff0c\u6216\u8005\u878d\u5408\u76f8\u673a\u56fe\u50cf\u4e0a\u7684\u7279\u5f81\u3002\u540c\u4e00\u6570\u5b66\u7b26\u53f7\uff0cBEV\u4e0a\u7b2c i \u4e2a\u70b9\u7684\u7279\u5f81\u4e3a F_{BEV}(x_i, z_i) , \u56fe\u7247\u4e0a\u7b2c j \u4e2a\u70b9\u7684\u5750\u6807\u662f F_{IMG}(Cx_j, Cy_j) , \u6269\u5c55\u8d77\u89c1\uff0c\u5982\u679c Cx_j, Cy_j \u662f\u5c0f\u6570\uff0c\u5219\u4ee3\u8868\u6211\u4eec\u4f7f\u7528\u63d2\u503c\u5f97\u5230\u7684\u7279\u5f81\u503c\u3002\u8fd9\u91cc\u7740\u91cd\u8003\u8651\u7684\u8f93\u51fa\u662f\u5728\u76f8\u673a\u5750\u6807\u7cfb\u4e0b\u7684BEV\u7279\u5f81\uff0c\u73af\u89c6\u7cfb\u7edf\u7684\u8f6c\u6362\u7684\u989d\u5916\u7684\u5916\u53c2\u4e0d\u8003\u8651\u3002 \u57fa\u4e8e\u4f20\u7edfCV\u7684 IPM(Inverse Perspective Mapping), \u5bf9\u5730\u9762\u9ad8\u5ea6\u7684BEV\u4e0a\u7684\u6bcf\u4e00\u4e2a\u70b9\u6839\u636e\u5750\u6807\u627e\u5230\u5bf9\u5e94\u56fe\u7247\u4e0a\u7684\u90a3\u4e2a\u70b9\u7684\u7279\u5f81\uff0c\u8fd9\u91cc\u4e3e\u65e0\u4eba\u8f66\u4e2d\u5e38\u89c1\u7684\u5df2\u77e5\u8ddd\u79bb\u5730\u9762\u9ad8\u5ea6 L \u7684\u5e73\u89c6(\u4fef\u4ef0\u89d2\u4e3a0)\u76f8\u673a\u4e3a\u4f8b\uff0c\u628a\u5750\u6807 (x_i, L, z_i) \u6295\u5f71\u5230\u76f8\u673a\u4e2d\uff0c\u6211\u4eec\u53ef\u4ee5\u5f97\u5230\u4ee5\u4e0b\u5173\u7cfb: F_{BEV}(x_i, z_i) = F_{img}(\\frac{f_x x_i}{z_i} + cx, \\frac{f_y L}{z_i} + cy) Neural Transformer (HDMapNet): \u5bf9\u8f93\u5165\u56fe\u7247\u4f7f\u7528\u5168\u8fde\u63a5\u5c42\u8f93\u51fa\u76f4\u63a5\u662fBEV\uff0c\u5c3d\u7ba1\u5b9e\u9645\u64cd\u4f5c\u4e0a\u662f\u591a\u5c42\u7ebf\u6027\u7f51\u7edc\uff0c\u6211\u4eec\u89c2\u5bdf\u5230\u5b9e\u8d28\u4e0a\u56fe\u7247\u6bcf\u4e2a\u50cf\u7d20\u70b9\u4e0e\u8f93\u51fa\u7279\u5f81\u4e4b\u95f4\u7684\u8fde\u63a5\u90fd\u662f\u5b66\u4e60\u540e\u56fa\u5b9a\u4e0b\u6765\u7684\u53c2\u6570\u3002 F_{BEV}(x_i, z_i) = MLP[F_{IMG}(Cx_j, Cy_j)] = \\sum_j W_{ij} F_{img}(Cx_j, Cy_j) Lift-Splat-Shoot (LSS): \u8ba9\u8f93\u5165\u56fe\u7247\u8f93\u51fa\u4e00\u4e2a\u5b8c\u6574\u7684\u6df1\u5ea6\u6982\u7387\u5206\u5e03\uff0c\u7136\u540e\u6295\u5c04\u5230BEV\u4e0a\uff0c\u7531\u4e8e\u9ed8\u8ba4\u65b9\u6848\u662f\u6ca1\u6709\u76d1\u7763\u7684\uff0c\u6240\u4ee5\u8fd9\u4e2a\u6df1\u5ea6\u6982\u7387\u672c\u8d28\u4e0a\u5c31\u662f\u4e00\u4e2a\u7531CNN\u4ece\u8bed\u4e49\u4e2d\u63d0\u53d6\u51fa\u6765\u7684\u4e00\u4e2a\u6743\u91cd. \u800c\u4e14\u7279\u5f81\u7684\u878d\u5408\u90fd\u5728\u56fe\u7247\u7684\u4e00\u4e2a\u7ad6\u7ebf\u4e0a CX_i = \\frac{f_x x_i}{z_i} + cx F_{BEV}(x_i, z_i) = \\sum_{j}F_{IMG}(CX_i, Cy_i) *P(Z(CX_i, Cy_i) == z_i) F_{BEV}(x_i, z_i) = \\sum_{j}F_{IMG}(CX_i, Cy_i) *CNN(F_{IMG}(CX_i, Cy_j)) \u672c\u8d28\u4e0a\u5c31\u662f\u4e00\u4e2a dynamic kernel Cross View Transformer\u5b9e\u8d28\u4e0a\u662f\u4e00\u4e2aBEV\u56fa\u5b9aembedding Q , \u4e0eimage feature+image embedding\u4ea7\u751f\u7684K,V\u4e4b\u95f4\u7684Cross-attention F_{BEV}(x_i, z_i) = \\sum_jw_{i,j} V(Cx_j, Cy_j) = \\sum_{j}[K(Cx_j, Cy_j), Q(x_i, z_i)]V(Cx_j, Cy_j) \u4e3b\u8981\u7684\u96be\u70b9\u5728\u4e8eQKV\u540c\u65f6\u9700\u8981\u5b66\u4e60\uff0cQ\u751a\u81f3\u662f\u4ece\u96f6\u5f00\u59cb\u5b66\u4e60\uff0c\u91cd\u8981\u7684image embedding\u4e5f\u662f\u4ece\u96f6\u5f00\u59cb\u5b66\u4e60\u3002\u5728\u6570\u636e\u91cf\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u6027\u80fd\u4e0d\u4e00\u5b9a\u662f\u6700\u4f73\u7684\u3002","title":"Summary of Multiple Papers on BEV Fusion"},{"location":"other_categories/Segmentation/BEV_segmentation/#summary-of-multiple-papers-on-bev-fusion","text":"\u5728\u7279\u65af\u62c9\u516c\u5f00\u65e5\u5ba3\u4f20\u4e86\u73af\u89c6\u6444\u50cf\u5934\u5230BEV\u8bed\u4e49\u5206\u5272\u7684\u76f4\u63a5\u9884\u6d4b\u65b9\u6848\u524d\u540e(\u5f88\u591a\u7814\u7a76\u5728Tesla\u524d\u5df2\u7ecf\u6709\u5728\u505a)\uff0c\u51fa\u73b0\u4e86\u5927\u91cf\u7684\u8bba\u6587\u5b9e\u73b0\u76f8\u5173\u7684\u529f\u80fd\u65b9\u6848\u3002\u5f88\u591a\u7684\u5de5\u4f5c\u4e5f\u4e0eBEV Fusion\u7684\u5b9e\u73b0\u6709\u5173\u3002\u5176\u6838\u5fc3\u7684\u89c2\u70b9\u4e0e\u76f8\u5173\u6280\u672f\u8981\u7d20\u53ef\u4ee5\u603b\u7ed3\u5982\u4e0b\uff1a \u4ee5\u4fef\u89c6\u56fe\uff0cBEV\u7684\u7a20\u5bc6\u7279\u5f81\u56fe\u4f5c\u4e3a\u591a\u76f8\u673a/\u591a\u4f20\u611f\u5668\u7684\u878d\u5408\u4f4d\u70b9\uff0c\u4e14BEV\u76f4\u63a5\u8fdb\u884c\u7a20\u5bc6\u8f93\u51fa(\u5982\u8bed\u4e49\u5206\u5272\u4ee5\u53ca\u7269\u4f53\u68c0\u6d4b)\u3002 \u628a\u76f8\u673a\u7279\u5f81\u8f6c\u6362\u5230BEV\u7a20\u5bc6\u7279\u5f81\u56fe\u4e2d\uff0c\u91cc\u9762\u53ef\u80fd\u7275\u6d89\u5230\u76f8\u673a\u7684\u6295\u5f71\u51e0\u4f55\u3001\u5916\u53c2\u3001\u50cf\u7d20\u6df1\u5ea6\u7b49\u95ee\u9898\u3002\u76ee\u524d\u4e5f\u6709\u591a\u79cd\u5b9e\u73b0\u65b9\u6848\u3002 \u628a\u70b9\u4e91\u7279\u5f81\u8f6c\u6362\u5230BEV\u7a20\u5bc6\u7279\u5f81\u56fe\u4e2d\uff0c\u76ee\u524d\u5e38\u89c1\u7684\u65b9\u6848\u662fVoxelization\u4ee5\u53caPointPillar\u65b9\u6848, \u5176\u4e2dPointPillar\u65b9\u6848\uff0c\u6700\u7ec8\u8f93\u51fa\u76f4\u63a5\u662fBEV 2D\u7279\u5f81\u56fe\uff0c\u53d7\u5230\u7684\u5173\u6ce8\u6bd4\u8f83\u591a\u3002 \u8f93\u51fa\u7684\u5185\u5bb9\u53ef\u4ee5\u975e\u5e38\u4e30\u5bcc\uff0c\u73b0\u6709\u5df2\u7ecf\u6709\u5927\u91cf\u7684\u5229\u7528\u7a20\u5bc6\u7684BEV\u7279\u5f81\u8fdb\u884c\u8f93\u51fa\u7684\u5404\u79cd\u5de5\u4f5c\uff0c\u4e14Nuscenes\u7b49\u6570\u636e\u96c6\u63d0\u4f9b\u5927\u91cf\u7684\u6807\u6ce8\u6570\u636e\u3002\u7269\u4f53\u68c0\u6d4b\u3001\u8f66\u9053\u68c0\u6d4b\u3001\u9ad8\u7cbe\u5730\u56fe\u5efa\u9020\u3001\u751a\u81f3\u8f68\u8ff9\u9884\u6d4b\u3001\u8def\u5f84\u89c4\u5212\u3002 \u5173\u4e8eBEV\u5185\u7684\u76ee\u6807\u68c0\u6d4b\uff0c\u7531CenterNet, CenterPoint\u76f8\u5173\u7684\u5de5\u4f5c\u7ed3\u6784\u7b80\u5355\u800c\u4e14\u53ef\u4ee5\u8f7b\u677e\u6269\u5c55\u51fa\u590d\u6742\u7684\u8f93\u51fa\u5934\uff0cAnchorLess\u7684\u8bbe\u8ba1\u5728BEV\u68c0\u6d4b\u4e5f\u6bd4\u8f83\u5403\u9999\uff0c\u56e0\u800c\u662f\u591a\u4efb\u52a1\u65b9\u6848\u4e2d\u7684\u5e38\u7528\u9009\u62e9\u3002 \u5173\u4e8e\u76f8\u673a\u7279\u5f81\u5230BEV\u7a20\u5bc6\u7279\u5f81\u56fe\u7684\u8f6c\u6362\uff0c\u76ee\u524d\u89c2\u5bdf\u5230\u7684\u51e0\u79cd\u65b9\u6848: \u4f7f\u7528\u5355\u76ee\u6df1\u5ea6\uff0c\u9884\u6d4b\u6bcf\u4e00\u4e2a\u50cf\u7d20\u7684\u6df1\u5ea6\u503c/\u6df1\u5ea6\u5206\u5e03\uff0c\u628a\u56fe\u50cf\u7279\u5f81\u8f6c\u6362\u4e3a\u79bb\u6563\u7684\u70b9\u4e91\u7279\u5f81/\u7a20\u5bc6\u7684Voxelized 3D\u7279\u5f81\uff0c \u518d\u8f6c\u6362\u5230BEV 2D. \u4f7f\u7528Inverse Perspective Mapping, \u5047\u8bbe\u56fe\u7247\u6240\u6709\u7684\u70b9\u90fd\u5728\u5730\u9762(\u53ef\u4ee5\u7406\u89e3\u4e3a\u540c\u4e00\u4e2a\u5e73\u9762\u7684\u70b9\u5728\u76f8\u673a\u5e73\u9762\u7684\u6295\u5f71)\uff0cBEV\u7279\u5f81\u56fe\u53ef\u4ee5\u7406\u89e3\u662f\u5730\u9762\u7279\u5f81\u5728\u4e00\u4e2a\u4fef\u77b0\u76f8\u673a\u4e2d\u7684\u6295\u5f71\uff0c\u540c\u4e00\u4e2a\u5e73\u9762\u5728\u4e0d\u540c\u76f8\u673a\u4e2d\u7684\u6295\u5f71\u7684\u5404\u70b9\u5750\u6807\u53ef\u4ee5\u7531 Homography Matrix \u8054\u7cfb\u8d77\u6765\uff0c \u8fd9\u4e2a\u77e9\u9635\u53ef\u4ee5\u7528\u5730\u9762\u4e0a\u4e00\u4e2a\u65b9\u5f62\u56db\u4e2a\u9876\u70b9\u5728\u4e24\u4e2a\u76f8\u673a\u4e2d\u7684\u5750\u6807\u6c42\u89e3\uff0c\u4e5f\u53ef\u4ee5\u7528\u76f8\u673a\u7684\u5185\u5916\u53c2\u76f4\u63a5\u8ba1\u7b97\u83b7\u5f97\u3002 \u4f7f\u7528\u5168\u8fde\u63a5\u5c42\uff0c\u8ba9\u7f51\u7edc\u51b3\u5b9aBEV\u4e2d\u7684\u6bcf\u4e00\u4e2a\u50cf\u7d20\u662f\u7531\u524d\u7f6e\u7279\u5f81\u7684\u54ea\u4e9b\u4f4d\u7f6e\u7684\u50cf\u7d20\u70b9\u52a0\u6743\u8ba1\u7b97\u7684\u3002\u8fd9\u79cd\u505a\u6cd5\u5e38\u5e38\u8981\u4e3a\u4e0d\u540c\u7684\u76f8\u673a\u505a\u4e0d\u540c\u7684\u5168\u8fde\u63a5\u5c42\uff0c\u4e14\u524d\u7f6e\u56fe\u4e2d\u70b9\u7684\u8bed\u4e49\u4fe1\u606f\u4e0d\u5f71\u54cd\u5176\u4e0eBEV\u7684\u8fde\u63a5;\u4e5f\u5c31\u662f\u65e0\u6cd5\u901a\u8fc7\u5224\u65ad\u56fe\u7247\u70b9\u662f\u4e0d\u662f\u5730\u9762\u70b9\uff0c\u628a\u5b83\u7684\u7279\u5f81\u52a0\u5230BEV\u7684\u4e0d\u540c\u4f4d\u7f6e\u4e0a\u3002 \u4f7f\u7528Attention\u5c42\uff0c\u8ba9\u7f51\u7edc\u6839\u636e\u5b66\u4e60\u5230\u7684\u5185\u5bb9\u4ee5\u53ca\u56fe\u7247\u7684\u8bed\u4e49\u4fe1\u606f\uff0c\u5224\u65adBEV\u4e0a\u7684\u6bcf\u4e00\u4e2a\u7279\u5f81\u5e94\u8be5\u5982\u4f55\u7531\u524d\u7f6e\u76f8\u673a\u7684\u7279\u5f81\u6784\u6210\u3002\u5728\u6570\u636e\u91cf\u6781\u5927\u7684\u60c5\u51b5\u4e0b\uff0c\u53ef\u80fd\u53ef\u4ee5\u8ba9\u7f51\u7edc\u5b66\u4e60\u5230\u5982\u4f55\u5b9e\u73b0\u6839\u636e\u8bed\u4e49\u4ee5\u53ca\u4f4d\u7f6e\u5206\u914d\u6c42\u548c\u6743\u91cd\u3002 \u672c\u7ad9\u6b64\u524d\u4e5f\u6709\u4e00\u5b9a\u7684\u6536\u96c6:","title":"Summary of Multiple Papers on BEV Fusion"},{"location":"other_categories/Segmentation/BEV_segmentation/#lift-splat-shoot","text":"\u4efb\u52a1: BEV\u9884\u6d4b\u8f66\u8f86\u8bed\u4e49\u5206\u5272\uff0c\u8def\u5f84\u89c4\u5212 \u76f8\u673a: \u9884\u6d4b\u6df1\u5ea6\u7684\u5206\u5e03\uff0c\u6839\u636e\u5206\u5e03\u5c06\u524d\u666f\u56fe\u6295\u5c04\u5230\u76f8\u673a\u5750\u6807\u7cfb\u4e0b\u7684\u4e00\u4e2aVoxel\u4e2d\u3002\u518d\u6839\u636e\u76f8\u673a\u7684\u5916\u53c2\uff0c\u628a\u5168\u5c40\u7528\u7684BEV\u5e73\u9762\u8303\u56f4\u5185\u7684Voxel\u70b9\u7559\u4e0b\uff0c\u96c6\u5408\u8fd9\u4e9b\u70b9\u4f7f\u7528PointPillar\u5904\u7406\u5e76\u538b\u5230BEV 2D.\u4e2d.","title":"Lift-Splat-Shoot:"},{"location":"other_categories/Segmentation/BEV_segmentation/#panopticbev","text":"\u4efb\u52a1: \u524d\u5411\u6444\u50cf\u5934\u7684\u5168\u666f\u5206\u5272 \u76f8\u673a\u8f6c\u6362\u65b9\u6848: \u878d\u5408\u4e86\u4e24\u4e2a\u505a\u6cd5\uff0c\u4e00\u4e2a\u5206\u652f\u91c7\u7528IPM\u8f6c\u6362\uff0c\u53e6\u4e00\u4e2a\u5206\u652f\u5bf9\u6bcf\u4e00\u4e2a\u50cf\u7d20\u70b9\u9884\u6d4b\u5176\u6df1\u5ea6\uff0c\u4f7f\u7528\u76f8\u673a\u5185\u53c2\u6269\u5c55\u6210\u76f8\u673a\u5750\u6807\u7cfb\u76843D\u7279\u5f81\u56fe\uff0c 3D grid_sample\u8f6c\u5230BEV\u4e0a\u3002","title":"PanopticBEV:"},{"location":"other_categories/Segmentation/BEV_segmentation/#hdmapnet","text":"\u4efb\u52a1: BEV\u9884\u6d4b\u9ad8\u7cbe\u5730\u56fe/\u5730\u56fe\u5143\u7d20\u8bed\u4e49\u5206\u5272. \u76f8\u673a\u8f6c\u6362\u65b9\u6848: Neural View Transformer \\rightarrow \u4e0d\u540c\u76f8\u673a\u4e0d\u540c\u5168\u8fde\u63a5\u5c42\u5148\u8f6c\u5230\u5404\u81ea\u7684\u524d\u5411BEV\u4e2d\uff0c\u518d\u6839\u636e\u5916\u53c2\u8f6c\u5230\u5168\u5c40BEV \u70b9\u4e91\u4f7f\u7528: PointPillar","title":"HDMapNet:"},{"location":"other_categories/Segmentation/BEV_segmentation/#bevfusion-multi-task-multi-sensor-fusion-with-unified-birds-eye-view-representation","text":"pdf code \u4efb\u52a1: \u4e09\u7ef4\u76ee\u6807\u68c0\u6d4b & \u5730\u56fe\u5206\u5272\u3002 \u76f8\u673a\u8f6c\u6362\u65b9\u6848: \u9884\u6d4b\u6df1\u5ea6\u5206\u5e03\uff0c\u5c06\u56fe\u50cf\u7279\u5f81\u6269\u5c55\u4e3a (N, H, W, D) \u7684\u70b9\u4e91\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u52a0\u901f\u7684BEV Pooling \u6a21\u5757\uff0c\u5feb\u901f\u5c06\u843d\u5728BEV\u4e0a\u6bcf\u4e2abin\u7684\u70b9\u878d\u5408\u8d77\u6765(mean/max)\u3002","title":"BEVFusion: Multi-Task Multi-Sensor Fusion with Unified Bird\u2019s-Eye View Representation"},{"location":"other_categories/Segmentation/BEV_segmentation/#bev-pooling","text":"\u5728\u5185\u53c2\u5916\u53c2\u56fa\u5b9a\u7684\u60c5\u51b5\u4e0b\uff0c\u6bcf\u4e2a\u76f8\u673a\u9884\u6d4b\u7684\u70b9\u4e91\u65b9\u9635\uff0c\u5728\u8f66\u8eab\u5750\u6807\u4e0a\u90fd\u662f\u56fa\u5b9a\u7684\u3002\u8bb0\u5f55\u70b9\u4e91\u65b9\u9635\u6bcf\u4e00\u4e2a\u70b9\u4f1a\u843d\u5230BEV\u4e0a\u7684\u54ea\u4e00\u4e2a\u70b9\uff0c\u5e76\u6839\u636e\u843d\u70b9\u5728BEV\u56fe\u4e2d\u7684index\u8fdb\u884c\u6392\u5e8f (\u8fd9\u4e9b\u90fd\u53ef\u4ee5\u63d0\u524d\u8ba1\u7b97)\u3002 \u8ba1\u7b97\u65f6\uff0c\u6839\u636e\u5df2\u7ecf\u63d0\u524d\u8ba1\u7b97\u597d\u7684\u6392\u5e8f\u5bf9\u70b9\u4e91\u65b9\u9635\u8fdb\u884c\u91cd\u6392\uff0c\u53ef\u4ee5\u5f97\u5230\u4e0a\u56fe(b).i \u7684\u5f62\u6001\u3002\u6bcf\u4e00\u4e2aBEV\u70b9\u5bf9\u5e94\u7684\u6240\u6709cam features\u4f1a\u5728\u5185\u5b58\u4e0a\u8fde\u7eed\uff0c\u5e76\u4e14\u6211\u4eec\u9884\u8ba1\u7b97\u65f6\uff0c\u4e5f\u53ef\u4ee5\u7f13\u5b58\u6bcf\u4e00\u4e2aBEV\u70b9\u5bf9\u5e94\u7684\u6570\u7ec4\u7684\u8d77\u59cb\u4e0e\u7ec8\u6b62\u5750\u6807\u3002 \u4f5c\u8005\u5b9e\u73b0\u4e86\u4e00\u4e2a\u4e13\u7528\u7684GPU\u6838\uff0c\u8ba9GPU\u7684\u6bcf\u4e00\u4e2a\u7ebf\u7a0b\u5206\u522b\u8d1f\u8d23\u4e00\u4e2aBEV\u70b9\u5185\u6240\u6709\u70b9\u7684\u7d2f\u52a0\u8ba1\u7b97\u3002 \u6781\u5927\u5730\u52a0\u901f\u4e86BEV pooling\u8fd9\u4e00\u6b65\u7684\u8fd0\u7b97\u901f\u5ea6\u3002\u4f7f\u5f97\u76f4\u63a5\u9884\u6d4b\u6df1\u5ea6\u5206\u5e03\u53d8\u6210\u4e00\u4e2a\u8fd0\u7b97\u538b\u529b\u5c1a\u53ef\u7684\u6a21\u5757\u3002","title":"BEV Pooling \u65b9\u6848:"},{"location":"other_categories/Segmentation/BEV_segmentation/#bevfusion-a-simple-and-robust-lidar-camera-fusion-framework","text":"pdf code \u4efb\u52a1: \u4e09\u7ef4\u76ee\u6807\u68c0\u6d4b \u76f8\u673a\u8f6c\u6362\u65b9\u6848: \u4e0eLSS\u4e00\u81f4\u3002 \u52a0\u5165\u4e86LiDAR.","title":"BEVFusion: A Simple and Robust LiDAR-Camera Fusion Framework"},{"location":"other_categories/Segmentation/BEV_segmentation/#bev","text":"\u76ee\u524d\u7684\u65b9\u6848\u6765\u8bf4\uff0c\u90fd\u662f\u5bf9BEV\u4e0a\u6bcf\u4e00\u4e2a\u50cf\u7d20\u70b9\uff0c\u53bb\"\u5bfb\u627e\"\uff0c\u6216\u8005\u878d\u5408\u76f8\u673a\u56fe\u50cf\u4e0a\u7684\u7279\u5f81\u3002\u540c\u4e00\u6570\u5b66\u7b26\u53f7\uff0cBEV\u4e0a\u7b2c i \u4e2a\u70b9\u7684\u7279\u5f81\u4e3a F_{BEV}(x_i, z_i) , \u56fe\u7247\u4e0a\u7b2c j \u4e2a\u70b9\u7684\u5750\u6807\u662f F_{IMG}(Cx_j, Cy_j) , \u6269\u5c55\u8d77\u89c1\uff0c\u5982\u679c Cx_j, Cy_j \u662f\u5c0f\u6570\uff0c\u5219\u4ee3\u8868\u6211\u4eec\u4f7f\u7528\u63d2\u503c\u5f97\u5230\u7684\u7279\u5f81\u503c\u3002\u8fd9\u91cc\u7740\u91cd\u8003\u8651\u7684\u8f93\u51fa\u662f\u5728\u76f8\u673a\u5750\u6807\u7cfb\u4e0b\u7684BEV\u7279\u5f81\uff0c\u73af\u89c6\u7cfb\u7edf\u7684\u8f6c\u6362\u7684\u989d\u5916\u7684\u5916\u53c2\u4e0d\u8003\u8651\u3002 \u57fa\u4e8e\u4f20\u7edfCV\u7684 IPM(Inverse Perspective Mapping), \u5bf9\u5730\u9762\u9ad8\u5ea6\u7684BEV\u4e0a\u7684\u6bcf\u4e00\u4e2a\u70b9\u6839\u636e\u5750\u6807\u627e\u5230\u5bf9\u5e94\u56fe\u7247\u4e0a\u7684\u90a3\u4e2a\u70b9\u7684\u7279\u5f81\uff0c\u8fd9\u91cc\u4e3e\u65e0\u4eba\u8f66\u4e2d\u5e38\u89c1\u7684\u5df2\u77e5\u8ddd\u79bb\u5730\u9762\u9ad8\u5ea6 L \u7684\u5e73\u89c6(\u4fef\u4ef0\u89d2\u4e3a0)\u76f8\u673a\u4e3a\u4f8b\uff0c\u628a\u5750\u6807 (x_i, L, z_i) \u6295\u5f71\u5230\u76f8\u673a\u4e2d\uff0c\u6211\u4eec\u53ef\u4ee5\u5f97\u5230\u4ee5\u4e0b\u5173\u7cfb: F_{BEV}(x_i, z_i) = F_{img}(\\frac{f_x x_i}{z_i} + cx, \\frac{f_y L}{z_i} + cy) Neural Transformer (HDMapNet): \u5bf9\u8f93\u5165\u56fe\u7247\u4f7f\u7528\u5168\u8fde\u63a5\u5c42\u8f93\u51fa\u76f4\u63a5\u662fBEV\uff0c\u5c3d\u7ba1\u5b9e\u9645\u64cd\u4f5c\u4e0a\u662f\u591a\u5c42\u7ebf\u6027\u7f51\u7edc\uff0c\u6211\u4eec\u89c2\u5bdf\u5230\u5b9e\u8d28\u4e0a\u56fe\u7247\u6bcf\u4e2a\u50cf\u7d20\u70b9\u4e0e\u8f93\u51fa\u7279\u5f81\u4e4b\u95f4\u7684\u8fde\u63a5\u90fd\u662f\u5b66\u4e60\u540e\u56fa\u5b9a\u4e0b\u6765\u7684\u53c2\u6570\u3002 F_{BEV}(x_i, z_i) = MLP[F_{IMG}(Cx_j, Cy_j)] = \\sum_j W_{ij} F_{img}(Cx_j, Cy_j) Lift-Splat-Shoot (LSS): \u8ba9\u8f93\u5165\u56fe\u7247\u8f93\u51fa\u4e00\u4e2a\u5b8c\u6574\u7684\u6df1\u5ea6\u6982\u7387\u5206\u5e03\uff0c\u7136\u540e\u6295\u5c04\u5230BEV\u4e0a\uff0c\u7531\u4e8e\u9ed8\u8ba4\u65b9\u6848\u662f\u6ca1\u6709\u76d1\u7763\u7684\uff0c\u6240\u4ee5\u8fd9\u4e2a\u6df1\u5ea6\u6982\u7387\u672c\u8d28\u4e0a\u5c31\u662f\u4e00\u4e2a\u7531CNN\u4ece\u8bed\u4e49\u4e2d\u63d0\u53d6\u51fa\u6765\u7684\u4e00\u4e2a\u6743\u91cd. \u800c\u4e14\u7279\u5f81\u7684\u878d\u5408\u90fd\u5728\u56fe\u7247\u7684\u4e00\u4e2a\u7ad6\u7ebf\u4e0a CX_i = \\frac{f_x x_i}{z_i} + cx F_{BEV}(x_i, z_i) = \\sum_{j}F_{IMG}(CX_i, Cy_i) *P(Z(CX_i, Cy_i) == z_i) F_{BEV}(x_i, z_i) = \\sum_{j}F_{IMG}(CX_i, Cy_i) *CNN(F_{IMG}(CX_i, Cy_j)) \u672c\u8d28\u4e0a\u5c31\u662f\u4e00\u4e2a dynamic kernel Cross View Transformer\u5b9e\u8d28\u4e0a\u662f\u4e00\u4e2aBEV\u56fa\u5b9aembedding Q , \u4e0eimage feature+image embedding\u4ea7\u751f\u7684K,V\u4e4b\u95f4\u7684Cross-attention F_{BEV}(x_i, z_i) = \\sum_jw_{i,j} V(Cx_j, Cy_j) = \\sum_{j}[K(Cx_j, Cy_j), Q(x_i, z_i)]V(Cx_j, Cy_j) \u4e3b\u8981\u7684\u96be\u70b9\u5728\u4e8eQKV\u540c\u65f6\u9700\u8981\u5b66\u4e60\uff0cQ\u751a\u81f3\u662f\u4ece\u96f6\u5f00\u59cb\u5b66\u4e60\uff0c\u91cd\u8981\u7684image embedding\u4e5f\u662f\u4ece\u96f6\u5f00\u59cb\u5b66\u4e60\u3002\u5728\u6570\u636e\u91cf\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u6027\u80fd\u4e0d\u4e00\u5b9a\u662f\u6700\u4f73\u7684\u3002","title":"\u76f8\u673a\u4e0eBEV\u7279\u5f81\u8f93\u51fa\u4e4b\u95f4\u7684\u8f6c\u6362\u7684\u8fdb\u4e00\u6b65\u6570\u5b66\u5316\u6574\u7406:"},{"location":"other_categories/Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/","text":"Convolutional CRFs for Semantic Segmentation \u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4f7f\u7528\u5377\u79ef\u7248\u7684Conditional Random Field(CRF)\u7528\u4e8e\u4f18\u5316\u8bed\u4e49\u5206\u5272\u7684\u7ed3\u679c\u3002\u9996\u5148\u56de\u987e(\u5bf9\u5199review\u7684\u6211\u6b64\u65f6\u662f\u65b0\u5b66\u7684) FullCRF\u7684\u7b97\u6cd5\uff0c\u7136\u540e\u63d0\u51fa\u4e86ConvCRF\u7684\u7b97\u6cd5\u4ee5\u53caimplementation, \u4f5c\u8005\u4ee3\u7801\u5df2\u5f00\u6e90 FullCRF CRF\u7684\u539f\u610f\u5728\u4e8e\u8ba9\u7279\u5f81\u76f8\u4f3c\u7684\u70b9\u8f93\u51fa\u76f8\u4f3c\u7684\u503c\uff0c\u6700\u540e\u8f6c\u6362\u4e3a\u4f18\u5316\u4e00\u4e0b\u8fd9\u4e2a\u6761\u4ef6\u6982\u7387: E(\\hat{x} | I)=\\sum_{i \\leq N} \\psi_{u}\\left(\\hat{x}_{i} | I\\right)+\\sum_{i \\neq j \\leq N} \\psi_{p}\\left(\\hat{x}_{i}, \\hat{x}_{j} | I\\right) \u7b2c\u4e00\u9879\u4e3a\u57fa\u7840\u5168\u8bed\u4e49\u5206\u5272\u7f51\u7edc\u8f93\u51fa\u7684\u503c\uff0c\u7b2c\u4e8c\u9879\u4f53\u73b0\u56fe\u7247\u4e2d\u4e0d\u540c\u4f4d\u7f6e\u7684\u76f8\u4e92\u5f71\u54cd\u3002FullCRF\u4e2d\uff0c\u7b2c\u4e8c\u9879\u8ba1\u7b97\u516c\u5f0f\u4e3a \\psi_{p}\\left(x_{i}, x_{j} | I\\right) :=\\mu\\left(x_{i}, x_{j}\\right) \\sum_{m=1}^{M} w^{(m)} k_{G}^{(m)}\\left(f_{i}^{I}, f_{j}^{I}\\right) \u5176\u4e2d \\mu(x_i,x_j) = |x_i \\neq x_j| \u4e5f\u5c31\u662f\u56fe\u4e2d\u6bcf\u4e00\u4e2a\u70b9(\u9664\u4e86\u70b9i\u4e4b\u5916).\u5e38\u7528\u7684\u6838\u51fd\u6570 k \u6709\u5982\u4ee5\u4e0b\u7684\u9ad8\u65af\u51fd\u6570 k\\left(f_{i}^{I}, f_{j}^{I}\\right) :=w^{(1)} \\exp \\left(-\\frac{\\left|p_{i}-p_{j}\\right|^{2}}{2 \\theta_{\\alpha}^{2}}-\\frac{\\left|I_{i}-I_{j}\\right|^{2}}{2 \\theta_{\\beta}^{2}}\\right)+w^{(2)} \\exp \\left(-\\frac{\\left|p_{i}-p_{j}\\right|^{2}}{2 \\theta_{\\gamma}^{2}}\\right) \u5176\u4e2d w^{(1)},\\theta \u7b49\u662f\u4ec5\u6709\u7684\u53ef\u5b66\u4e60\u53c2\u6570\uff0c\u76f4\u89c9\u800c\u8a00\uff0c\u5c31\u662f\u7279\u5f81\u76f8\u4f3c\u8005\u76f8\u4e92\u5f71\u54cd\u5927\uff0c\u8ddd\u79bb\u8fd1\u8005\u76f8\u4e92\u5f71\u54cd\u5927\u3002 \u6700\u7ec8\u5b9e\u73b0\u7684\u8fed\u4ee3\u7b97\u6cd5: ConvCRF ConvCRF\u5148\u5047\u8bbe\u4e24\u4e2a\u66fc\u54c8\u987f\u8ddd\u79bb\u5927\u4e8e\u4e00\u5b9a\u9608\u503c k \u7684\u70b9\u76f8\u4e92\u72ec\u7acb\uff0c\u8fd9\u4e2a k \u79f0\u4e3aConvCRF\u7684filter size.\u8fd9\u4e5f\u5c31\u662fConvCRF\u5bf9\u524d\u6587 \\mu(x_i,x_j) \u7684\u9884\u8bbe\u65b9\u5f0f \u5bf9\u4e8e\u4f4d\u4e8e x,y \u7684\u70b9\u5b83\u5bf9\u5e94\u7684\u5377\u79ef\u6838/CRF\u6838\u4e3a k_{g}[b, d x, d y, x, y] :=\\exp \\left(-\\sum_{i=1}^{d} \\frac{\\left|f_{i}^{(d)}[b, x, y]-f_{i}^{(d)}[b, x-d x, y-d y]\\right|^{2}}{2 \\dot{\\theta}_{i}^{2}}\\right) \u5176\u4e2d \\theta_i \u4e3a\u53ef\u5b66\u4e60\u7684\u53d8\u91cf f_i \u4e3a\u7279\u5f81\u5411\u91cf,\u5377\u79ef\u8303\u56f4\u5185\u7684\u6bcf\u4e00\u4e2apair\u6709\u4e00\u4e2a\u5bf9\u5e94\u7684, K=\\sum^s_{i=1}w_i g_i \u7ed3\u679cQ,combined message\u5219\u7531\u6b64\u5f0f\u5b50\u7ed9\u51fa Q[b, c, x, y]=\\sum_{d x, d y \\leq k} K[b, d x, d y, x, y] \\cdot P[b, c, x+d x, y+d y] \u4f5c\u8005\u63d0\u5230\uff0c\u8fd9\u4e2a\u8fd0\u7b97\u64cd\u4f5c\u53ef\u4ee5\u8bf4\u7c7b\u4f3c\u4e8elocally connected layers(every pixel has its own filter),\u533a\u522b\u5728\u4e8e\u6bcf\u4e00\u4e2akernel\u5728channel\u65b9\u5411\u4e0a\u662f\u4e00\u4e2a\u5e38\u6570(\u53ea\u8d1f\u8d23\u52a0\u6743\u6c42\u548c\u6574\u4e2afeature vector\u800c\u4e0d\u9700\u8981\u91cd\u6574feature)\u3002 (\u9898\u5916\u8bdd\uff0clocally connected layer\u76ee\u524d\u6709keras implementation\u4f46\u662f\u8fd8\u6ca1\u6709officail pytorch implementation\uff0c \u53c2\u8003 )","title":"Convolutional CRFs for Semantic Segmentation"},{"location":"other_categories/Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/#convolutional-crfs-for-semantic-segmentation","text":"\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4f7f\u7528\u5377\u79ef\u7248\u7684Conditional Random Field(CRF)\u7528\u4e8e\u4f18\u5316\u8bed\u4e49\u5206\u5272\u7684\u7ed3\u679c\u3002\u9996\u5148\u56de\u987e(\u5bf9\u5199review\u7684\u6211\u6b64\u65f6\u662f\u65b0\u5b66\u7684) FullCRF\u7684\u7b97\u6cd5\uff0c\u7136\u540e\u63d0\u51fa\u4e86ConvCRF\u7684\u7b97\u6cd5\u4ee5\u53caimplementation, \u4f5c\u8005\u4ee3\u7801\u5df2\u5f00\u6e90","title":"Convolutional CRFs for Semantic Segmentation"},{"location":"other_categories/Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/#fullcrf","text":"CRF\u7684\u539f\u610f\u5728\u4e8e\u8ba9\u7279\u5f81\u76f8\u4f3c\u7684\u70b9\u8f93\u51fa\u76f8\u4f3c\u7684\u503c\uff0c\u6700\u540e\u8f6c\u6362\u4e3a\u4f18\u5316\u4e00\u4e0b\u8fd9\u4e2a\u6761\u4ef6\u6982\u7387: E(\\hat{x} | I)=\\sum_{i \\leq N} \\psi_{u}\\left(\\hat{x}_{i} | I\\right)+\\sum_{i \\neq j \\leq N} \\psi_{p}\\left(\\hat{x}_{i}, \\hat{x}_{j} | I\\right) \u7b2c\u4e00\u9879\u4e3a\u57fa\u7840\u5168\u8bed\u4e49\u5206\u5272\u7f51\u7edc\u8f93\u51fa\u7684\u503c\uff0c\u7b2c\u4e8c\u9879\u4f53\u73b0\u56fe\u7247\u4e2d\u4e0d\u540c\u4f4d\u7f6e\u7684\u76f8\u4e92\u5f71\u54cd\u3002FullCRF\u4e2d\uff0c\u7b2c\u4e8c\u9879\u8ba1\u7b97\u516c\u5f0f\u4e3a \\psi_{p}\\left(x_{i}, x_{j} | I\\right) :=\\mu\\left(x_{i}, x_{j}\\right) \\sum_{m=1}^{M} w^{(m)} k_{G}^{(m)}\\left(f_{i}^{I}, f_{j}^{I}\\right) \u5176\u4e2d \\mu(x_i,x_j) = |x_i \\neq x_j| \u4e5f\u5c31\u662f\u56fe\u4e2d\u6bcf\u4e00\u4e2a\u70b9(\u9664\u4e86\u70b9i\u4e4b\u5916).\u5e38\u7528\u7684\u6838\u51fd\u6570 k \u6709\u5982\u4ee5\u4e0b\u7684\u9ad8\u65af\u51fd\u6570 k\\left(f_{i}^{I}, f_{j}^{I}\\right) :=w^{(1)} \\exp \\left(-\\frac{\\left|p_{i}-p_{j}\\right|^{2}}{2 \\theta_{\\alpha}^{2}}-\\frac{\\left|I_{i}-I_{j}\\right|^{2}}{2 \\theta_{\\beta}^{2}}\\right)+w^{(2)} \\exp \\left(-\\frac{\\left|p_{i}-p_{j}\\right|^{2}}{2 \\theta_{\\gamma}^{2}}\\right) \u5176\u4e2d w^{(1)},\\theta \u7b49\u662f\u4ec5\u6709\u7684\u53ef\u5b66\u4e60\u53c2\u6570\uff0c\u76f4\u89c9\u800c\u8a00\uff0c\u5c31\u662f\u7279\u5f81\u76f8\u4f3c\u8005\u76f8\u4e92\u5f71\u54cd\u5927\uff0c\u8ddd\u79bb\u8fd1\u8005\u76f8\u4e92\u5f71\u54cd\u5927\u3002 \u6700\u7ec8\u5b9e\u73b0\u7684\u8fed\u4ee3\u7b97\u6cd5:","title":"FullCRF"},{"location":"other_categories/Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/#convcrf","text":"ConvCRF\u5148\u5047\u8bbe\u4e24\u4e2a\u66fc\u54c8\u987f\u8ddd\u79bb\u5927\u4e8e\u4e00\u5b9a\u9608\u503c k \u7684\u70b9\u76f8\u4e92\u72ec\u7acb\uff0c\u8fd9\u4e2a k \u79f0\u4e3aConvCRF\u7684filter size.\u8fd9\u4e5f\u5c31\u662fConvCRF\u5bf9\u524d\u6587 \\mu(x_i,x_j) \u7684\u9884\u8bbe\u65b9\u5f0f \u5bf9\u4e8e\u4f4d\u4e8e x,y \u7684\u70b9\u5b83\u5bf9\u5e94\u7684\u5377\u79ef\u6838/CRF\u6838\u4e3a k_{g}[b, d x, d y, x, y] :=\\exp \\left(-\\sum_{i=1}^{d} \\frac{\\left|f_{i}^{(d)}[b, x, y]-f_{i}^{(d)}[b, x-d x, y-d y]\\right|^{2}}{2 \\dot{\\theta}_{i}^{2}}\\right) \u5176\u4e2d \\theta_i \u4e3a\u53ef\u5b66\u4e60\u7684\u53d8\u91cf f_i \u4e3a\u7279\u5f81\u5411\u91cf,\u5377\u79ef\u8303\u56f4\u5185\u7684\u6bcf\u4e00\u4e2apair\u6709\u4e00\u4e2a\u5bf9\u5e94\u7684, K=\\sum^s_{i=1}w_i g_i \u7ed3\u679cQ,combined message\u5219\u7531\u6b64\u5f0f\u5b50\u7ed9\u51fa Q[b, c, x, y]=\\sum_{d x, d y \\leq k} K[b, d x, d y, x, y] \\cdot P[b, c, x+d x, y+d y] \u4f5c\u8005\u63d0\u5230\uff0c\u8fd9\u4e2a\u8fd0\u7b97\u64cd\u4f5c\u53ef\u4ee5\u8bf4\u7c7b\u4f3c\u4e8elocally connected layers(every pixel has its own filter),\u533a\u522b\u5728\u4e8e\u6bcf\u4e00\u4e2akernel\u5728channel\u65b9\u5411\u4e0a\u662f\u4e00\u4e2a\u5e38\u6570(\u53ea\u8d1f\u8d23\u52a0\u6743\u6c42\u548c\u6574\u4e2afeature vector\u800c\u4e0d\u9700\u8981\u91cd\u6574feature)\u3002 (\u9898\u5916\u8bdd\uff0clocally connected layer\u76ee\u524d\u6709keras implementation\u4f46\u662f\u8fd8\u6ca1\u6709officail pytorch implementation\uff0c \u53c2\u8003 )","title":"ConvCRF"},{"location":"other_categories/Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/","text":"Deep Multi-Sensor Lane Detection \u8fd9\u7bc7\u8bba\u6587\u7ed9\u51fa\u7528lidar\u548c\u56fe\u7247\u4e24\u8005\u5171\u540c\u8fdb\u884c\u5b66\u4e60\u4fa6\u6d4b\u9053\u8def\u7ebf\u7684\u7b97\u6cd5 \u5173\u952e\u70b9\uff1a \u4f5c\u8005\u7528\u56fe\u7247\u793a\u4f8b\u8bf4\u660e\uff0c\u5373\u4f7f\u5728\u76f8\u673a\u5750\u6807\u7cfb(\u56fe\u7247)\u4e2d\u5b9e\u73b0\u4e86\u8fd1\u4e4e\u5b8c\u7f8e\u7cbe\u786e\u7684road-segmentation,\u8f6c\u6362\u5230\u4fef\u89c6\u56fe\u7684\u65f6\u5019\u7cbe\u786e\u5ea6\u4ecd\u7136\u5f88\u4f4e\uff0c\u6240\u4ee5\u9700\u8981\u7528\u4fef\u89c6\u56fe\u76f4\u63a5\u5904\u7406\u3002 \u8f93\u51fa\u7684\u4fef\u89c6\u56fe\u4e2d\u6bcf\u4e00\u4e2a\u50cf\u7d20\u4ee3\u8868\u7684\u662f\u8be5\u70b9\u8ddd\u79bb\u6700\u8fd1\u9053\u8def\u7ebf\u7684\u8ddd\u79bb\u3002\u4e0e\u76f4\u63a5segmentation\u76f8\u6bd4\uff0c\u53ef\u4ee5\u7f13\u89e3\u8f93\u51fa\u7ed3\u679c\u4ec5\u4e0e\u7ed3\u679c\u7a0d\u7a0doffset\u65f6\u7684loss\u8fc7\u5927\u7b49\u7684\u95ee\u9898\u3002 \u70b9\u4e91\u8bc6\u522b\u8def\u5f84\u65f6\uff0c\u5730\u9762\u70b9\u8fc7\u4e8e\u7a00\u758f\uff0c\u6240\u4ee5\u9700\u8981\u6570\u4e2a\u70b9\u4e91\u6d4b\u91cf\u7ed3\u679c\u5408\u5e76\u4e00\u8d77\u4f7f\u7528\uff0c\u7136\u540e\u538b\u5728\u5e73\u9762\u56fe\u4e2d\u8f93\u5165CNN\u3002","title":"Deep Multi-Sensor Lane Detection"},{"location":"other_categories/Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/#deep-multi-sensor-lane-detection","text":"\u8fd9\u7bc7\u8bba\u6587\u7ed9\u51fa\u7528lidar\u548c\u56fe\u7247\u4e24\u8005\u5171\u540c\u8fdb\u884c\u5b66\u4e60\u4fa6\u6d4b\u9053\u8def\u7ebf\u7684\u7b97\u6cd5 \u5173\u952e\u70b9\uff1a \u4f5c\u8005\u7528\u56fe\u7247\u793a\u4f8b\u8bf4\u660e\uff0c\u5373\u4f7f\u5728\u76f8\u673a\u5750\u6807\u7cfb(\u56fe\u7247)\u4e2d\u5b9e\u73b0\u4e86\u8fd1\u4e4e\u5b8c\u7f8e\u7cbe\u786e\u7684road-segmentation,\u8f6c\u6362\u5230\u4fef\u89c6\u56fe\u7684\u65f6\u5019\u7cbe\u786e\u5ea6\u4ecd\u7136\u5f88\u4f4e\uff0c\u6240\u4ee5\u9700\u8981\u7528\u4fef\u89c6\u56fe\u76f4\u63a5\u5904\u7406\u3002 \u8f93\u51fa\u7684\u4fef\u89c6\u56fe\u4e2d\u6bcf\u4e00\u4e2a\u50cf\u7d20\u4ee3\u8868\u7684\u662f\u8be5\u70b9\u8ddd\u79bb\u6700\u8fd1\u9053\u8def\u7ebf\u7684\u8ddd\u79bb\u3002\u4e0e\u76f4\u63a5segmentation\u76f8\u6bd4\uff0c\u53ef\u4ee5\u7f13\u89e3\u8f93\u51fa\u7ed3\u679c\u4ec5\u4e0e\u7ed3\u679c\u7a0d\u7a0doffset\u65f6\u7684loss\u8fc7\u5927\u7b49\u7684\u95ee\u9898\u3002 \u70b9\u4e91\u8bc6\u522b\u8def\u5f84\u65f6\uff0c\u5730\u9762\u70b9\u8fc7\u4e8e\u7a00\u758f\uff0c\u6240\u4ee5\u9700\u8981\u6570\u4e2a\u70b9\u4e91\u6d4b\u91cf\u7ed3\u679c\u5408\u5e76\u4e00\u8d77\u4f7f\u7528\uff0c\u7136\u540e\u538b\u5728\u5e73\u9762\u56fe\u4e2d\u8f93\u5165CNN\u3002","title":"Deep Multi-Sensor Lane Detection"},{"location":"other_categories/Segmentation/DeepSnake/","text":"Deep Snake for Real-Time Instance Segmentation \u8fd9\u7bc7paper\u5f15\u5165\u4e86\u6df1\u5ea6\u5b66\u4e60\u5316\u7684snake\u7b97\u6cd5\u3002snake\u7b97\u6cd5\u7684\u601d\u8def\u662f\u7ed9\u5b9a\u4e00\u4e2a\u521d\u59cb\u7684\u56de\u73afcontour, \u6bcf\u4e00\u4e2a\u8fb9\u7f18\u70b9\u6839\u636e\u81ea\u5df1\u9644\u8fd1\u56fe\u7247\u6027\u8d28\u7684energy function\u4ee5\u53ca\u5176\u4ed6\u4fe1\u606f\uff0c\u9010\u6b65\u574d\u7f29\u5230\u6240\u5173\u6ce8\u7684\u7269\u4f53\u7684\u8fb9\u7f18\u4e0a\uff0c\u7279\u70b9\u5728\u4e8e\u5bf9\u6210\u529f\u8fa8\u8bc6\u7684\u7269\u4f53\u8fd9\u4e2a\u7b97\u6cd5\u8fb9\u7f18\u4e0a\u7684\u7cbe\u786e\u5ea6\u76f8\u5bf9\u6709\u4fdd\u8bc1\u3002 Deep snake intuition \u521d\u59cbcontour \\rightarrow circular conv \\rightarrow conv \\rightarrow \u6bcf\u4e00\u4e2a\u70b9\u7684\u504f\u79fb\u3002 Circular Conv \u672c\u6587\u4ee3\u7801\u91cc\u9762\u4f7f\u7528\u4e00\u4e2a\u4e8c\u7ef4tensor\u6309\u5706\u5468\u987a\u5e8f\u5b58\u50a8contour\u70b9\uff0c\u5e76\u6ca1\u6709\u4f7f\u7528\u8fc7\u5ea6\u7684\u6280\u5de7\u3002 \u603b\u4f53\u7ed3\u6784 \u56feb\u8bf4\u660e\u6574\u4e2a\u56fe\u7684\u7ed3\u6784\uff0c\u9996\u5148\u4f7f\u75282D detector\u5f97\u5230\u57fa\u7840bounding box\uff0c\u7136\u540e\u8f6c\u5316\u4e3acoutour, \u7136\u540e\u4f7f\u7528\u7b2c\u4e00\u5c42deep snake(\u5927\u611f\u53d7\u91ce) \u8ba1\u7b97offset \\rightarrow upsampling\u523040\u4e2a\u70b9\uff0c\u518d\u7ecf\u8fc7deep snake(\u5c0f\u611f\u53d7\u91ce)\u5f97\u5230\u6700\u540e\u7684deformation output\u3002","title":"Deep Snake for Real-Time Instance Segmentation"},{"location":"other_categories/Segmentation/DeepSnake/#deep-snake-for-real-time-instance-segmentation","text":"\u8fd9\u7bc7paper\u5f15\u5165\u4e86\u6df1\u5ea6\u5b66\u4e60\u5316\u7684snake\u7b97\u6cd5\u3002snake\u7b97\u6cd5\u7684\u601d\u8def\u662f\u7ed9\u5b9a\u4e00\u4e2a\u521d\u59cb\u7684\u56de\u73afcontour, \u6bcf\u4e00\u4e2a\u8fb9\u7f18\u70b9\u6839\u636e\u81ea\u5df1\u9644\u8fd1\u56fe\u7247\u6027\u8d28\u7684energy function\u4ee5\u53ca\u5176\u4ed6\u4fe1\u606f\uff0c\u9010\u6b65\u574d\u7f29\u5230\u6240\u5173\u6ce8\u7684\u7269\u4f53\u7684\u8fb9\u7f18\u4e0a\uff0c\u7279\u70b9\u5728\u4e8e\u5bf9\u6210\u529f\u8fa8\u8bc6\u7684\u7269\u4f53\u8fd9\u4e2a\u7b97\u6cd5\u8fb9\u7f18\u4e0a\u7684\u7cbe\u786e\u5ea6\u76f8\u5bf9\u6709\u4fdd\u8bc1\u3002","title":"Deep Snake for Real-Time Instance Segmentation"},{"location":"other_categories/Segmentation/DeepSnake/#deep-snake-intuition","text":"\u521d\u59cbcontour \\rightarrow circular conv \\rightarrow conv \\rightarrow \u6bcf\u4e00\u4e2a\u70b9\u7684\u504f\u79fb\u3002","title":"Deep snake intuition"},{"location":"other_categories/Segmentation/DeepSnake/#circular-conv","text":"\u672c\u6587\u4ee3\u7801\u91cc\u9762\u4f7f\u7528\u4e00\u4e2a\u4e8c\u7ef4tensor\u6309\u5706\u5468\u987a\u5e8f\u5b58\u50a8contour\u70b9\uff0c\u5e76\u6ca1\u6709\u4f7f\u7528\u8fc7\u5ea6\u7684\u6280\u5de7\u3002","title":"Circular Conv"},{"location":"other_categories/Segmentation/DeepSnake/#_1","text":"\u56feb\u8bf4\u660e\u6574\u4e2a\u56fe\u7684\u7ed3\u6784\uff0c\u9996\u5148\u4f7f\u75282D detector\u5f97\u5230\u57fa\u7840bounding box\uff0c\u7136\u540e\u8f6c\u5316\u4e3acoutour, \u7136\u540e\u4f7f\u7528\u7b2c\u4e00\u5c42deep snake(\u5927\u611f\u53d7\u91ce) \u8ba1\u7b97offset \\rightarrow upsampling\u523040\u4e2a\u70b9\uff0c\u518d\u7ecf\u8fc7deep snake(\u5c0f\u611f\u53d7\u91ce)\u5f97\u5230\u6700\u540e\u7684deformation output\u3002","title":"\u603b\u4f53\u7ed3\u6784"},{"location":"other_categories/Segmentation/FCN_panoptic_seg/","text":"Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN) \u8fd9\u7bc7paper\u4e3b\u8981idea\u662f\u76ee\u6807\u662f\u4f7f\u7528FCN\u7684\u7ed3\u6784\u7edf\u4e00\u5730\u8868\u8fbe\u5168\u666f\u5206\u5272\u91cc\u9762\u7684\u524d\u666f\u7269\u4f53\u4ee5\u53ca\u80cc\u666f\u3002 Structures \u6574\u4f53\u7ed3\u6784\u5982\u56fe\uff0c Position Head : \u5206\u522b\u7528\u5377\u79ef\u8f93\u51fa\u524d\u666f\u4e2d\u5fc3\u7684\u70ed\u56fe(\u9ad8\u65af\u6743\u91cd)\u4ee5\u53ca\u80cc\u666f\u7684\u70ed\u56fe\uff0c \u53ef\u4ee5\u7528Ground truth\u8fdb\u884c\u76d1\u7763\u3002\u63a8\u7406\u7684\u65f6\u5019\uff0c\u4ece\u4e2d\u9009\u62e9\u6700\u9ad8\u7684 k \u4e2a\u4e2d\u5fc3\u70b9(\u5bf9\u4e8e\u80cc\u666f\u7c7b\u522b\uff0c\u5219 k=1 ), \u5f97\u5230\u4e2d\u5fc3\u70b9\u96c6\u5408 D^{st}, D^{th} . Kernel Head : \u7528coordConv(\u4e0e SOLO \u4e00\u81f4)\u4ee5\u53ca\u666e\u901aConv\u8f93\u51fa G_{in}\\in\\mathbb{R}^{C_{\\mathrm{e}} \\times W_{i} \\times H_{i}} . \u5728\u63a8\u7406\u7684\u65f6\u5019\uff0c\u5bf9\u6bcf\u4e00\u4e2a\u524d\u6587\u5f97\u5230\u7684\u4e2d\u5fc3\u70b9,\u5982 (x_c, y_c) \uff0c \u53d6\u5bf9\u5e94\u5750\u6807\u7684\u7279\u5f81 G_{i,:,x_c, y_c} \\in \\mathbb{R}^{C_e \\times 1 \\times 1} \u76f8\u5f53\u4e8e\u6bcf\u4e00\u4e2a\u4e2d\u5fc3\u70b9\u4f1a\u6709\u4e00\u4e2akernel ( 3\\times 3 )\u8868\u8fbe\u5b83\u7684\u7279\u5f81. Kernel Fusion : \u5982\u679c\u4e24\u4e2a\u540c\u7c7b\u522b\u7684\u7279\u5f81\u4f59\u5f26\u76f8\u4f3c\u5ea6\u8d85\u8fc7\u4e00\u4e2a\u9608\u503c\uff0c\u5219\u4f1a\u88ab\u7406\u89e3\u4e3a\u5177\u6709\u540c\u4e00\u4e2aID. K \u4e2d\u62e5\u6709\u76f8\u540cID\u7684\u6838\u7279\u5f81\u4f1a\u53d6\u5747\u503c\u3002\u6570\u5b66\u4e86\u6765\u8bf4 K_j = \\text{AvgPool}(G'_{j}) . G'j = \\{G_m : \\text{ID}(G_m) = \\text{ID}(G_j) \\} . \u8fd9\u6837\u5f97\u5230\u7684\u76f8\u5f53\u4e8e\u6bcf\u4e00\u4e2aID\u6709\u4e00\u4e2a\u81ea\u5df1\u7684kernel. Feature Encoder : \u7528 CoordConv\u548c\u4e00\u822c\u5377\u79ef\u5728\u9ad8\u5206\u8fa8\u7387\u76f4\u63a5\u5904\u7406\u7279\u5f81\uff0c\u7136\u540e\u6bcf\u4e00\u4e2aID\u7684\u8f93\u51fa\u5c31\u7528\u524d\u9762\u4e0d\u540c\u7684\u5377\u79ef\u6838\u5206\u522b\u5bf9feature\u8fdb\u884c\u5377\u79ef\u8f93\u51fa\u3002","title":"Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)"},{"location":"other_categories/Segmentation/FCN_panoptic_seg/#fully-convolutional-networks-for-panoptic-segmentation-panoptic-fcn","text":"\u8fd9\u7bc7paper\u4e3b\u8981idea\u662f\u76ee\u6807\u662f\u4f7f\u7528FCN\u7684\u7ed3\u6784\u7edf\u4e00\u5730\u8868\u8fbe\u5168\u666f\u5206\u5272\u91cc\u9762\u7684\u524d\u666f\u7269\u4f53\u4ee5\u53ca\u80cc\u666f\u3002","title":"Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)"},{"location":"other_categories/Segmentation/FCN_panoptic_seg/#structures","text":"\u6574\u4f53\u7ed3\u6784\u5982\u56fe\uff0c Position Head : \u5206\u522b\u7528\u5377\u79ef\u8f93\u51fa\u524d\u666f\u4e2d\u5fc3\u7684\u70ed\u56fe(\u9ad8\u65af\u6743\u91cd)\u4ee5\u53ca\u80cc\u666f\u7684\u70ed\u56fe\uff0c \u53ef\u4ee5\u7528Ground truth\u8fdb\u884c\u76d1\u7763\u3002\u63a8\u7406\u7684\u65f6\u5019\uff0c\u4ece\u4e2d\u9009\u62e9\u6700\u9ad8\u7684 k \u4e2a\u4e2d\u5fc3\u70b9(\u5bf9\u4e8e\u80cc\u666f\u7c7b\u522b\uff0c\u5219 k=1 ), \u5f97\u5230\u4e2d\u5fc3\u70b9\u96c6\u5408 D^{st}, D^{th} . Kernel Head : \u7528coordConv(\u4e0e SOLO \u4e00\u81f4)\u4ee5\u53ca\u666e\u901aConv\u8f93\u51fa G_{in}\\in\\mathbb{R}^{C_{\\mathrm{e}} \\times W_{i} \\times H_{i}} . \u5728\u63a8\u7406\u7684\u65f6\u5019\uff0c\u5bf9\u6bcf\u4e00\u4e2a\u524d\u6587\u5f97\u5230\u7684\u4e2d\u5fc3\u70b9,\u5982 (x_c, y_c) \uff0c \u53d6\u5bf9\u5e94\u5750\u6807\u7684\u7279\u5f81 G_{i,:,x_c, y_c} \\in \\mathbb{R}^{C_e \\times 1 \\times 1} \u76f8\u5f53\u4e8e\u6bcf\u4e00\u4e2a\u4e2d\u5fc3\u70b9\u4f1a\u6709\u4e00\u4e2akernel ( 3\\times 3 )\u8868\u8fbe\u5b83\u7684\u7279\u5f81. Kernel Fusion : \u5982\u679c\u4e24\u4e2a\u540c\u7c7b\u522b\u7684\u7279\u5f81\u4f59\u5f26\u76f8\u4f3c\u5ea6\u8d85\u8fc7\u4e00\u4e2a\u9608\u503c\uff0c\u5219\u4f1a\u88ab\u7406\u89e3\u4e3a\u5177\u6709\u540c\u4e00\u4e2aID. K \u4e2d\u62e5\u6709\u76f8\u540cID\u7684\u6838\u7279\u5f81\u4f1a\u53d6\u5747\u503c\u3002\u6570\u5b66\u4e86\u6765\u8bf4 K_j = \\text{AvgPool}(G'_{j}) . G'j = \\{G_m : \\text{ID}(G_m) = \\text{ID}(G_j) \\} . \u8fd9\u6837\u5f97\u5230\u7684\u76f8\u5f53\u4e8e\u6bcf\u4e00\u4e2aID\u6709\u4e00\u4e2a\u81ea\u5df1\u7684kernel. Feature Encoder : \u7528 CoordConv\u548c\u4e00\u822c\u5377\u79ef\u5728\u9ad8\u5206\u8fa8\u7387\u76f4\u63a5\u5904\u7406\u7279\u5f81\uff0c\u7136\u540e\u6bcf\u4e00\u4e2aID\u7684\u8f93\u51fa\u5c31\u7528\u524d\u9762\u4e0d\u540c\u7684\u5377\u79ef\u6838\u5206\u522b\u5bf9feature\u8fdb\u884c\u5377\u79ef\u8f93\u51fa\u3002","title":"Structures"},{"location":"other_categories/Segmentation/LRNNET/","text":"LRNNET: A LIGHT-WEIGHTED NETWORK WITH EFFICIENT REDUCED NON-LOCAL OPERATION FOR REAL-TIME SEMANTIC SEGMENTATION \u8fd9\u7bc7paper\u7684\u4e3b\u8981\u8d21\u732e\u5728\u4e8e\u63d0\u51fa\u4e86\u4e00\u4e2a\u7b80\u5316\u7248\u7684non-local\u7b97\u6cd5\u3002 \u5177\u4f53\u7684\u505a\u6cd5\u662f\u628a C\\times H \\times W \u7684\u5f20\u91cf\u4ee5 S \u4e3a\u91cf\u5316\u5355\u4f4d\uff0c\u5206\u5272\u4e3a S=\\frac{H\\times W}{H'\\times W'} \u4e2a\u8d85\u50cf\u7d20\uff0cnon-local\u7684\u9762\u79ef\u5927\u5c0f\u53d8\u4e3a H', W' \uff0c\u5927\u5e45\u5ea6\u964d\u4f4e\u8fd0\u7b97\u91cf\u3002 \u5bf9\u4e8e\u5404\u4e2a\u65b9\u5757\uff0c C \\times H' \\times W' , \u672c\u6587\u63d0\u51fa\u4e86\u4f7f\u7528normalized\u5de6\u5947\u5f02\u4e3b\u5411\u91cf\u4f5c\u4e3a\u5bf9\u6574\u4e2a\u8d85\u50cf\u7d20\u7684\u66ff\u4ee3\uff0c\u5c06\u5b83\u5e73\u6574\u4e3a C\\times (H'\\times W') \u77e9\u9635\uff0c\u7136\u540e\u4f7f\u7528power iteration\u7b97\u6cd5\u8ba1\u7b97\u5de6\u4e3b\u5947\u5f02\u77e2\u91cf C'\\times 1 .","title":"LRNNET: A LIGHT-WEIGHTED NETWORK WITH EFFICIENT REDUCED NON-LOCAL OPERATION FOR REAL-TIME SEMANTIC SEGMENTATION"},{"location":"other_categories/Segmentation/LRNNET/#lrnnet-a-light-weighted-network-with-efficient-reduced-non-local-operation-for-real-time-semantic-segmentation","text":"\u8fd9\u7bc7paper\u7684\u4e3b\u8981\u8d21\u732e\u5728\u4e8e\u63d0\u51fa\u4e86\u4e00\u4e2a\u7b80\u5316\u7248\u7684non-local\u7b97\u6cd5\u3002 \u5177\u4f53\u7684\u505a\u6cd5\u662f\u628a C\\times H \\times W \u7684\u5f20\u91cf\u4ee5 S \u4e3a\u91cf\u5316\u5355\u4f4d\uff0c\u5206\u5272\u4e3a S=\\frac{H\\times W}{H'\\times W'} \u4e2a\u8d85\u50cf\u7d20\uff0cnon-local\u7684\u9762\u79ef\u5927\u5c0f\u53d8\u4e3a H', W' \uff0c\u5927\u5e45\u5ea6\u964d\u4f4e\u8fd0\u7b97\u91cf\u3002 \u5bf9\u4e8e\u5404\u4e2a\u65b9\u5757\uff0c C \\times H' \\times W' , \u672c\u6587\u63d0\u51fa\u4e86\u4f7f\u7528normalized\u5de6\u5947\u5f02\u4e3b\u5411\u91cf\u4f5c\u4e3a\u5bf9\u6574\u4e2a\u8d85\u50cf\u7d20\u7684\u66ff\u4ee3\uff0c\u5c06\u5b83\u5e73\u6574\u4e3a C\\times (H'\\times W') \u77e9\u9635\uff0c\u7136\u540e\u4f7f\u7528power iteration\u7b97\u6cd5\u8ba1\u7b97\u5de6\u4e3b\u5947\u5f02\u77e2\u91cf C'\\times 1 .","title":"LRNNET: A LIGHT-WEIGHTED NETWORK WITH EFFICIENT REDUCED NON-LOCAL OPERATION FOR REAL-TIME SEMANTIC SEGMENTATION"},{"location":"other_categories/Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/","text":"Lane Detection and Classification using Cascaded CNNs \u8fd9\u7bc7\u8bba\u6587\u7ed9\u51fa\u7684\u662f\u7aef\u5230\u7aef\u8def\u7ebf\u70b9\u7684\u5206\u5272\u3001\u805a\u7c7b\u4ee5\u53ca\u5206\u7c7b\u7684\u7b97\u6cd5\u3002 \u6574\u4f53\u7ed3\u6784\u5982\u56fe 1. Instance Segmentation \u7528\u7269\u4f53\u5206\u5272\u505a\u7b2c\u4e00\u6b65\uff0c\u53ea\u63a2\u6d4b\u6700\u591a4\u6761\u7ebf\u3002\u4e2d\u5fc3\u4e24\u6761\u4ee5\u53ca\u5de6\u53f3\u4e00\u6761\u3002 2. \u5206\u7c7b \u6570\u636e\u4e0a\u4f7f\u7528\u989d\u5916\u7684\u4eba\u5de5\u6807\u6ce8\uff0c\u5f97\u5230\u56fe\u68ee\u6570\u636e\u96c6\u7684\u9053\u8def\u7ebf\u7c7b\u522b\uff0c github \u4eceSegmentation\u7684\u70b9\u4e2d\u627e\u5230\u539f\u56fe\u4e0a\u7684\u70b9\uff0c\u4ece\u4e2d\u91c7\u6837\u56fa\u5b9a\u6570\u91cf\u7684\u70b9(\u4f5c\u4e3a\u7a33\u5b9a\u7684\u8f93\u5165\u5927\u5c0f)\u3002 \u628a\u8fd9\u4e9bdescriptor(\u56fa\u5b9a\u6570\u91cf)\u8f93\u5165\u5230\u53e6\u4e00\u4e2a\u5355\u72ec\u8bad\u7ec3\u7684CNN\u4e2d\u5b8c\u6210\u5206\u7c7b\u3002","title":"Lane Detection and Classification using Cascaded CNNs"},{"location":"other_categories/Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/#lane-detection-and-classification-using-cascaded-cnns","text":"\u8fd9\u7bc7\u8bba\u6587\u7ed9\u51fa\u7684\u662f\u7aef\u5230\u7aef\u8def\u7ebf\u70b9\u7684\u5206\u5272\u3001\u805a\u7c7b\u4ee5\u53ca\u5206\u7c7b\u7684\u7b97\u6cd5\u3002 \u6574\u4f53\u7ed3\u6784\u5982\u56fe","title":"Lane Detection and Classification using Cascaded CNNs"},{"location":"other_categories/Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/#1-instance-segmentation","text":"\u7528\u7269\u4f53\u5206\u5272\u505a\u7b2c\u4e00\u6b65\uff0c\u53ea\u63a2\u6d4b\u6700\u591a4\u6761\u7ebf\u3002\u4e2d\u5fc3\u4e24\u6761\u4ee5\u53ca\u5de6\u53f3\u4e00\u6761\u3002","title":"1. Instance Segmentation"},{"location":"other_categories/Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/#2","text":"\u6570\u636e\u4e0a\u4f7f\u7528\u989d\u5916\u7684\u4eba\u5de5\u6807\u6ce8\uff0c\u5f97\u5230\u56fe\u68ee\u6570\u636e\u96c6\u7684\u9053\u8def\u7ebf\u7c7b\u522b\uff0c github \u4eceSegmentation\u7684\u70b9\u4e2d\u627e\u5230\u539f\u56fe\u4e0a\u7684\u70b9\uff0c\u4ece\u4e2d\u91c7\u6837\u56fa\u5b9a\u6570\u91cf\u7684\u70b9(\u4f5c\u4e3a\u7a33\u5b9a\u7684\u8f93\u5165\u5927\u5c0f)\u3002 \u628a\u8fd9\u4e9bdescriptor(\u56fa\u5b9a\u6570\u91cf)\u8f93\u5165\u5230\u53e6\u4e00\u4e2a\u5355\u72ec\u8bad\u7ec3\u7684CNN\u4e2d\u5b8c\u6210\u5206\u7c7b\u3002","title":"2. \u5206\u7c7b"},{"location":"other_categories/Segmentation/PointRend/","text":"PointRend: Image Segmentation as Rendering \u8fd9\u7bc7\u6765\u81eaFAIR\u7684\u8bba\u6587\u5c06\u8bed\u4e49\u5206\u5272\u7684\u540e\u5904\u7406\u7406\u89e3\u4e3a\u8ba1\u7b97\u673a\u56fe\u5f62\u5b66\u7684\u6e32\u67d3\u95ee\u9898\uff0c\u76ee\u6807\u5c31\u662f\u8981\u63d0\u5347\u8bed\u4e49\u5206\u5272\u5728\u7269\u4f53\u8fb9\u7f18\u7684\u6027\u80fd\uff0c\u540c\u65f6\u63a7\u5236\u8fd0\u7b97\u91cf\u3002\u91c7\u53d6\u7684\u65b9\u6cd5\u662f\u5148\u8f93\u51fa\u4e00\u4e2a\u4f4e\u5206\u8fa8\u7387\u7684\u5206\u5272\u56fe\uff0c\u518d\u9010\u6b65\u4e0a\u91c7\u6837\uff0c\u5728\u4e0a\u91c7\u6837\u7684\u8fc7\u7a0b\u4e2d\u5bf9\u8fb9\u7f18\u8fdb\u884c\u5904\u7406\uff0c\u6548\u679c\u50cf\u662f\u6e32\u67d3\u4e2d\u9010\u6b65\u7cbe\u4fee\u8fb9\u7f18\u7684\u89c6\u89c9\u6548\u679c\u3002 PointRend\u56fe\u793a \u5bf9\u4e8einstance segmentation\uff0c\u5148\u8f93\u51fa\u4e00\u4e2a\u7c97\u7cd9\u7684\u8bed\u4e49\u5206\u5272\u7ed3\u679c( 7\\times 7 )\uff0c\u7136\u540e\u8fed\u4ee3\u4e0a\u91c7\u6837\u8fc7\u7a0b\uff0c\u6bcf\u4e00\u4e2a\u4e0a\u91c7\u6837\u8fc7\u7a0b\u5305\u542b \u53cc\u7ebf\u6027\u63d2\u503c\u4e0a\u91c7\u6837 \u6267\u884cPointRend\u6a21\u5757\uff0c\u9009\u53d6\u4e0d\u786e\u5b9a\u6027\u9ad8\u7684\u70b9\uff0c\u63d0\u53d6\u51fa\u5176\u7279\u5f81\u77e2\u91cf\uff0c\u7528\u4e00\u4e2ashared\u7684MLP\u5f97\u5230\u5176\u8f93\u51fa\u7ed3\u679c\u3002 \u70b9\u7684\u9009\u62e9, \u63a8\u7406\u65f6 \u9009\u62e9\u7684\u8981\u6c42\u662f\u5bfb\u627eprobability\u6700\u63a5\u8fd10.5\u7684\u70b9\u8fdb\u884c\u5904\u7406\u3002 \u5bf9\u4e8e\u4e00\u4e2a\u76ee\u6807\u4e3a M\\times M \u7684\u5206\u5272\u56fe\uff0cPointRend\u53ea\u9700\u8981 N log_2\\frac{M}{M_0} \u6b21\u8f93\u51fa. \u5bf9\u4e8e M = 224, M_0 = 7 \u672c\u6587\u9009\u62e9 N = 28^2 ,\u662f\u539f\u6765\u7684 1/16 . \u70b9\u7684\u9009\u62e9\uff0c\u8bad\u7ec3\u65f6 \u672c\u6587\u91c7\u7528\u7684\u662f\u4e00\u4e2a\u5e26\u504f\u89c1\u7684\u968f\u673a\u91c7\u6837\uff0c\u7b97\u6cd5\u5982\u4e0b\uff1a \u8fc7\u91c7\u6837\uff0c\u603b\u5171\u4f1a\u968f\u673a\u91c7 kN (k > 1) \u4e2a\u70b9 \u4ece\u8fd9 kN \u4e2a\u70b9\u4e2d\u9009\u62e9 (\\beta N) \u4e2a\u6700\u4e0d\u786e\u5b9a\u7684\u70b9 \u4ece\u5269\u4e0b\u70b9\u4e2d\u5b8c\u5168\u968f\u673a\u91c7\u6837 (1-\\beta) N \u4e2a\u70b9, \u8bad\u7ec3\u8fc7\u7a0b\u4e0e\u7c97\u7cd9\u7684\u8bed\u4e49\u5206\u5272\u521d\u59cb\u4f30\u8ba1\u662f\u5e73\u884c\u7684\u3001\u5206\u79bb\u7684 \u6297\u952f\u9f7f\u6548\u679c\u56fe:","title":"PointRend: Image Segmentation as Rendering"},{"location":"other_categories/Segmentation/PointRend/#pointrend-image-segmentation-as-rendering","text":"\u8fd9\u7bc7\u6765\u81eaFAIR\u7684\u8bba\u6587\u5c06\u8bed\u4e49\u5206\u5272\u7684\u540e\u5904\u7406\u7406\u89e3\u4e3a\u8ba1\u7b97\u673a\u56fe\u5f62\u5b66\u7684\u6e32\u67d3\u95ee\u9898\uff0c\u76ee\u6807\u5c31\u662f\u8981\u63d0\u5347\u8bed\u4e49\u5206\u5272\u5728\u7269\u4f53\u8fb9\u7f18\u7684\u6027\u80fd\uff0c\u540c\u65f6\u63a7\u5236\u8fd0\u7b97\u91cf\u3002\u91c7\u53d6\u7684\u65b9\u6cd5\u662f\u5148\u8f93\u51fa\u4e00\u4e2a\u4f4e\u5206\u8fa8\u7387\u7684\u5206\u5272\u56fe\uff0c\u518d\u9010\u6b65\u4e0a\u91c7\u6837\uff0c\u5728\u4e0a\u91c7\u6837\u7684\u8fc7\u7a0b\u4e2d\u5bf9\u8fb9\u7f18\u8fdb\u884c\u5904\u7406\uff0c\u6548\u679c\u50cf\u662f\u6e32\u67d3\u4e2d\u9010\u6b65\u7cbe\u4fee\u8fb9\u7f18\u7684\u89c6\u89c9\u6548\u679c\u3002","title":"PointRend: Image Segmentation as Rendering"},{"location":"other_categories/Segmentation/PointRend/#pointrend","text":"\u5bf9\u4e8einstance segmentation\uff0c\u5148\u8f93\u51fa\u4e00\u4e2a\u7c97\u7cd9\u7684\u8bed\u4e49\u5206\u5272\u7ed3\u679c( 7\\times 7 )\uff0c\u7136\u540e\u8fed\u4ee3\u4e0a\u91c7\u6837\u8fc7\u7a0b\uff0c\u6bcf\u4e00\u4e2a\u4e0a\u91c7\u6837\u8fc7\u7a0b\u5305\u542b \u53cc\u7ebf\u6027\u63d2\u503c\u4e0a\u91c7\u6837 \u6267\u884cPointRend\u6a21\u5757\uff0c\u9009\u53d6\u4e0d\u786e\u5b9a\u6027\u9ad8\u7684\u70b9\uff0c\u63d0\u53d6\u51fa\u5176\u7279\u5f81\u77e2\u91cf\uff0c\u7528\u4e00\u4e2ashared\u7684MLP\u5f97\u5230\u5176\u8f93\u51fa\u7ed3\u679c\u3002","title":"PointRend\u56fe\u793a"},{"location":"other_categories/Segmentation/PointRend/#_1","text":"\u9009\u62e9\u7684\u8981\u6c42\u662f\u5bfb\u627eprobability\u6700\u63a5\u8fd10.5\u7684\u70b9\u8fdb\u884c\u5904\u7406\u3002 \u5bf9\u4e8e\u4e00\u4e2a\u76ee\u6807\u4e3a M\\times M \u7684\u5206\u5272\u56fe\uff0cPointRend\u53ea\u9700\u8981 N log_2\\frac{M}{M_0} \u6b21\u8f93\u51fa. \u5bf9\u4e8e M = 224, M_0 = 7 \u672c\u6587\u9009\u62e9 N = 28^2 ,\u662f\u539f\u6765\u7684 1/16 .","title":"\u70b9\u7684\u9009\u62e9, \u63a8\u7406\u65f6"},{"location":"other_categories/Segmentation/PointRend/#_2","text":"\u672c\u6587\u91c7\u7528\u7684\u662f\u4e00\u4e2a\u5e26\u504f\u89c1\u7684\u968f\u673a\u91c7\u6837\uff0c\u7b97\u6cd5\u5982\u4e0b\uff1a \u8fc7\u91c7\u6837\uff0c\u603b\u5171\u4f1a\u968f\u673a\u91c7 kN (k > 1) \u4e2a\u70b9 \u4ece\u8fd9 kN \u4e2a\u70b9\u4e2d\u9009\u62e9 (\\beta N) \u4e2a\u6700\u4e0d\u786e\u5b9a\u7684\u70b9 \u4ece\u5269\u4e0b\u70b9\u4e2d\u5b8c\u5168\u968f\u673a\u91c7\u6837 (1-\\beta) N \u4e2a\u70b9, \u8bad\u7ec3\u8fc7\u7a0b\u4e0e\u7c97\u7cd9\u7684\u8bed\u4e49\u5206\u5272\u521d\u59cb\u4f30\u8ba1\u662f\u5e73\u884c\u7684\u3001\u5206\u79bb\u7684 \u6297\u952f\u9f7f\u6548\u679c\u56fe:","title":"\u70b9\u7684\u9009\u62e9\uff0c\u8bad\u7ec3\u65f6"},{"location":"other_categories/Segmentation/PolorMask/","text":"PolarMask: Single Shot Instance Segmentation with Polar Representation \u8fd9\u7bc7paper\u53d7\u542f\u53d1\u4e8e FCOS \uff0c\u4f7f\u7528\u65b0\u7684\u6570\u636e\u8868\u8fbe\u65b9\u5f0f\uff0c\u5c06Instance Segmentation\u5728network inference\u4e0a\u5212\u5f52\u4e3a\u4e0e2D detection\u5b8c\u5168\u4e00\u81f4\u7684\u590d\u6742\u5ea6\u3002\u6709\u673a\u4f1a\u4e0e YOLOACT \u7ade\u4e89\u6210\u4e3aInstance Seg\u5de5\u4e1a\u4f7f\u7528\u7684\u6807\u51c6\u3002 \u603b\u4f53\u7ed3\u6784 \u5728\u7f51\u7edc\u7ed3\u6784\u4e0a\u9762\u8fd9\u7bc7paper\u9009\u62e9\u7684\u662f\u4e0e FCOS \u4e00\u81f4\u7684\u7ed3\u6784\uff0c\u4ec5\u4ec5\u5728 regression\u5206\u652f\u4e0a\u6539\u4e3a\u5bf9 n=36 \u4e2a\u5c04\u7ebf\u65b9\u5411\u7684\u8ddd\u79bb\u8fdb\u884c\u56de\u5f52\u3002 \u6bcf\u4e00\u4e2ainstance\u88ab\u5efa\u6a21\u4e3a\u4e2d\u5fc3\u70b9+\u5bc6\u96c6\u7684\u4ece\u4e2d\u5fc3\u70b9\u5f80\u5916\u7684\u5c04\u7ebf\u5f62\u6210\u7684contour. \"\u4e2d\u5fc3\"\u5b9a\u4e49 \u4e0e FCOS \u7c7b\u4f3c\uff0c\u8fd9\u91cc\u9700\u8981\u5b9a\u4e49\u4e00\u4e2a\u4e2d\u5fc3\u70b9\uff0c\u540c\u65f6\u9700\u8981\u5b9a\u4e49\u4e00\u4e2a\u4e2d\u5fc3\u5ea6\u3002\u4f5c\u8005\u6bd4\u8f83\u8fc7\u4f7f\u7528bounding box\u7684\u4e2d\u70b9\u4e0e\u8d28\u5fc3\uff0c\u53d1\u73b0\u4f7f\u7528\u8d28\u5fc3\u7406\u8bba\u6027\u80fd\u4e0a\u9650\u66f4\u597d\uff0c\u6240\u4ee5\u672c\u6587\u7684ground truth center\u7684\u4f4d\u7f6e\u4e2d\u5fc3\u5728\u8d28\u5fc3\u4e0a\uff0c\u4f5c\u8005\u5141\u8bb8\u51e0\u4e2apixel\u7ea7\u522b\u7684\u6270\u52a8\uff0c\u589e\u52a0\u6b63\u6837\u672c\u6570\u91cf\u3002 \u800c\u4e2d\u5fc3\u5ea6\u4e0e FCOS \u7684centerness\u6709\u4e00\u5b9a\u533a\u522b\uff0c\u4f46\u662f\u601d\u8def\u4e00\u81f4\uff0c\u516c\u5f0f\u5982\u4e0b\uff1a \\text { Polar Centerness }=\\sqrt{\\frac{\\min \\left(\\left\\{d_{1}, d_{2}, \\ldots, d_{n}\\right\\}\\right)}{\\max \\left(\\left\\{d_{1}, d_{2}, \\ldots, d_{n}\\right\\}\\right)}} \u8fd9\u4e2a\u6570\u503c\u9700\u8981 polar centerness\u5206\u652f\u8fdb\u884c\u76d1\u7763\u5b66\u4e60\uff0c\u5728inference\u7684\u65f6\u5019\u4f1a\u4e0eclassification score\u76f8\u4e58\u4f5c\u4e3aNMS\u7684\u5224\u636e\u3002 Polar IoU Loss \u4f5c\u8005\u8ba4\u4e3a\u76f4\u63a5\u7528SmoothL1 Loss\u8bad\u7ec3\u662f\u5355\u72ec\u5730\u8bad\u7ec3\u65b9\u5f0f\uff0c\u4f5c\u8005\u8ba4\u4e3a\u5e94\u5f53n\u4e2a\u5c04\u7ebf\u8ddd\u79bb\u4e00\u8d77train\u3002\u5e76\u63d0\u51fa\u4e86Polar IoU Loss \u5047\u8bbe\u4e2d\u5fc3\u70b9\u6b63\u786e\uff0cIoU\u5373\u662f\u5982\u56fe(\u8fd9\u4e2a\u56fe\u7247\u53ef\u89c6\u5316\u77ac\u95f4\u5c06\u770b\u4f3c\u6bd4\u8f83\u590d\u6742\u7684\u95ee\u9898\u89e3\u91ca\u5f97\u5f88\u597d)\uff0c\u663e\u7136\u6211\u4eec\u4f1a\u7528\u79bb\u6563\u5316\u8868\u8fbe\u66ff\u4ee3\u79ef\u5206\uff0c\u516c\u5f0f\u5982\u4e0b\uff1a \\text{IoU} = \\lim_{N\\rightarrow\\infty}\\frac{\\sum^N_{i=1} \\frac{1}{2} d^2_{min} \\Delta \\theta_i}{\\sum^N_{i=1} \\frac{1}{2} d^2_{max} \\Delta \\theta_i} \u4f5c\u8005\u53d1\u73b0\u5e73\u65b9\u5bf9\u6027\u80fd\u63d0\u5347\u6709\u9650\uff0c\u9009\u62e9\u4e86\u76f4\u63a5\u4f7f\u7528\u4e00\u6b21\u65b9\u8fd1\u4f3c\u4ee3\u66ff\uff0c\u5e76\u53d6log\u5f97\u5230loss \\text { Polar IoU }=\\frac{\\sum_{i=1}^{n} d_{\\min }}{\\sum_{i=1}^{n} d_{\\max }} \\text { Polar IoU Loss}= -\\text{log} (\\text{Polar IoU}) distance labeling \u6700\u540e\u4e00\u4e2a\u95ee\u9898\u5c31\u662f\u5982\u4f55\u83b7\u5f97\u6bcf\u4e00\u4e2ainstance\u4e2d\u6bcf\u4e00\u4e2acenter point\u7684\u5c04\u7ebfground truth. \u7b2c\u4e00\u6b65\u662f\u9884\u5904\u7406\u7684\u65f6\u5019\u4f7f\u7528 cv2.findCoutours \u5f97\u5230\u4e00\u7cfb\u5217\u8fb9\u7f18\u70b9\u3002\u7136\u540e\u5bf9\u6bcf\u4e00\u4e2a\u9700\u8981\u8ba1\u7b97distance groud truth\u7684center\u70b9\u8fd0\u884c\uff1a \u7ffb\u8bd1\u89e3\u91ca \u8ba1\u7b97\u6bcf\u4e00\u4e2a\u8fb9\u7f18\u70b9\u5230\"center\"\u70b9\u7684\u8ddd\u79bb\u4e0e\u76f8\u5bf9\u5939\u89d2\u3002 \u5bf9\u6bcf\u4e00\u4e2a\u9700\u8981ground truth\u7684\u5939\u89d2\uff1a \u5982\u679c\u57281\u4e2d\u6709\u8ba1\u7b97\u5230\uff0c\u5219\u5bf9\u5e94\u6700\u5927\u7684d(\u53ef\u80fd\u4e00\u4e2a\u89d2\u5ea6\u4e0d\u6b62\u4e00\u4e2a\u6700\u8fdc\u70b9\uff0c\u8981\u7a0d\u5fae\u8003\u8651\u51f9\u5f62\u72b6\uff0c\u8fd9\u79cd\u6570\u91cf\u4e0d\u591a)\u3002 \u5982\u679c\u6ca1\u6709\uff0c\u5219\u53d6\u6700\u90bb\u8fd1\u7684\u5df2\u6709\u7684theta\u5bf9\u5e94\u7684\u8ddd\u79bb\u6700\u4e3aground truth\uff0c\u5426\u5219\u53d6\u4e00\u4e2a\u6781\u5c0f\u503c\u3002","title":"PolarMask: Single Shot Instance Segmentation with Polar Representation"},{"location":"other_categories/Segmentation/PolorMask/#polarmask-single-shot-instance-segmentation-with-polar-representation","text":"\u8fd9\u7bc7paper\u53d7\u542f\u53d1\u4e8e FCOS \uff0c\u4f7f\u7528\u65b0\u7684\u6570\u636e\u8868\u8fbe\u65b9\u5f0f\uff0c\u5c06Instance Segmentation\u5728network inference\u4e0a\u5212\u5f52\u4e3a\u4e0e2D detection\u5b8c\u5168\u4e00\u81f4\u7684\u590d\u6742\u5ea6\u3002\u6709\u673a\u4f1a\u4e0e YOLOACT \u7ade\u4e89\u6210\u4e3aInstance Seg\u5de5\u4e1a\u4f7f\u7528\u7684\u6807\u51c6\u3002","title":"PolarMask: Single Shot Instance Segmentation with Polar Representation"},{"location":"other_categories/Segmentation/PolorMask/#_1","text":"\u5728\u7f51\u7edc\u7ed3\u6784\u4e0a\u9762\u8fd9\u7bc7paper\u9009\u62e9\u7684\u662f\u4e0e FCOS \u4e00\u81f4\u7684\u7ed3\u6784\uff0c\u4ec5\u4ec5\u5728 regression\u5206\u652f\u4e0a\u6539\u4e3a\u5bf9 n=36 \u4e2a\u5c04\u7ebf\u65b9\u5411\u7684\u8ddd\u79bb\u8fdb\u884c\u56de\u5f52\u3002 \u6bcf\u4e00\u4e2ainstance\u88ab\u5efa\u6a21\u4e3a\u4e2d\u5fc3\u70b9+\u5bc6\u96c6\u7684\u4ece\u4e2d\u5fc3\u70b9\u5f80\u5916\u7684\u5c04\u7ebf\u5f62\u6210\u7684contour.","title":"\u603b\u4f53\u7ed3\u6784"},{"location":"other_categories/Segmentation/PolorMask/#_2","text":"\u4e0e FCOS \u7c7b\u4f3c\uff0c\u8fd9\u91cc\u9700\u8981\u5b9a\u4e49\u4e00\u4e2a\u4e2d\u5fc3\u70b9\uff0c\u540c\u65f6\u9700\u8981\u5b9a\u4e49\u4e00\u4e2a\u4e2d\u5fc3\u5ea6\u3002\u4f5c\u8005\u6bd4\u8f83\u8fc7\u4f7f\u7528bounding box\u7684\u4e2d\u70b9\u4e0e\u8d28\u5fc3\uff0c\u53d1\u73b0\u4f7f\u7528\u8d28\u5fc3\u7406\u8bba\u6027\u80fd\u4e0a\u9650\u66f4\u597d\uff0c\u6240\u4ee5\u672c\u6587\u7684ground truth center\u7684\u4f4d\u7f6e\u4e2d\u5fc3\u5728\u8d28\u5fc3\u4e0a\uff0c\u4f5c\u8005\u5141\u8bb8\u51e0\u4e2apixel\u7ea7\u522b\u7684\u6270\u52a8\uff0c\u589e\u52a0\u6b63\u6837\u672c\u6570\u91cf\u3002 \u800c\u4e2d\u5fc3\u5ea6\u4e0e FCOS \u7684centerness\u6709\u4e00\u5b9a\u533a\u522b\uff0c\u4f46\u662f\u601d\u8def\u4e00\u81f4\uff0c\u516c\u5f0f\u5982\u4e0b\uff1a \\text { Polar Centerness }=\\sqrt{\\frac{\\min \\left(\\left\\{d_{1}, d_{2}, \\ldots, d_{n}\\right\\}\\right)}{\\max \\left(\\left\\{d_{1}, d_{2}, \\ldots, d_{n}\\right\\}\\right)}} \u8fd9\u4e2a\u6570\u503c\u9700\u8981 polar centerness\u5206\u652f\u8fdb\u884c\u76d1\u7763\u5b66\u4e60\uff0c\u5728inference\u7684\u65f6\u5019\u4f1a\u4e0eclassification score\u76f8\u4e58\u4f5c\u4e3aNMS\u7684\u5224\u636e\u3002","title":"\"\u4e2d\u5fc3\"\u5b9a\u4e49"},{"location":"other_categories/Segmentation/PolorMask/#polar-iou-loss","text":"\u4f5c\u8005\u8ba4\u4e3a\u76f4\u63a5\u7528SmoothL1 Loss\u8bad\u7ec3\u662f\u5355\u72ec\u5730\u8bad\u7ec3\u65b9\u5f0f\uff0c\u4f5c\u8005\u8ba4\u4e3a\u5e94\u5f53n\u4e2a\u5c04\u7ebf\u8ddd\u79bb\u4e00\u8d77train\u3002\u5e76\u63d0\u51fa\u4e86Polar IoU Loss \u5047\u8bbe\u4e2d\u5fc3\u70b9\u6b63\u786e\uff0cIoU\u5373\u662f\u5982\u56fe(\u8fd9\u4e2a\u56fe\u7247\u53ef\u89c6\u5316\u77ac\u95f4\u5c06\u770b\u4f3c\u6bd4\u8f83\u590d\u6742\u7684\u95ee\u9898\u89e3\u91ca\u5f97\u5f88\u597d)\uff0c\u663e\u7136\u6211\u4eec\u4f1a\u7528\u79bb\u6563\u5316\u8868\u8fbe\u66ff\u4ee3\u79ef\u5206\uff0c\u516c\u5f0f\u5982\u4e0b\uff1a \\text{IoU} = \\lim_{N\\rightarrow\\infty}\\frac{\\sum^N_{i=1} \\frac{1}{2} d^2_{min} \\Delta \\theta_i}{\\sum^N_{i=1} \\frac{1}{2} d^2_{max} \\Delta \\theta_i} \u4f5c\u8005\u53d1\u73b0\u5e73\u65b9\u5bf9\u6027\u80fd\u63d0\u5347\u6709\u9650\uff0c\u9009\u62e9\u4e86\u76f4\u63a5\u4f7f\u7528\u4e00\u6b21\u65b9\u8fd1\u4f3c\u4ee3\u66ff\uff0c\u5e76\u53d6log\u5f97\u5230loss \\text { Polar IoU }=\\frac{\\sum_{i=1}^{n} d_{\\min }}{\\sum_{i=1}^{n} d_{\\max }} \\text { Polar IoU Loss}= -\\text{log} (\\text{Polar IoU})","title":"Polar IoU Loss"},{"location":"other_categories/Segmentation/PolorMask/#distance-labeling","text":"\u6700\u540e\u4e00\u4e2a\u95ee\u9898\u5c31\u662f\u5982\u4f55\u83b7\u5f97\u6bcf\u4e00\u4e2ainstance\u4e2d\u6bcf\u4e00\u4e2acenter point\u7684\u5c04\u7ebfground truth. \u7b2c\u4e00\u6b65\u662f\u9884\u5904\u7406\u7684\u65f6\u5019\u4f7f\u7528 cv2.findCoutours \u5f97\u5230\u4e00\u7cfb\u5217\u8fb9\u7f18\u70b9\u3002\u7136\u540e\u5bf9\u6bcf\u4e00\u4e2a\u9700\u8981\u8ba1\u7b97distance groud truth\u7684center\u70b9\u8fd0\u884c\uff1a \u7ffb\u8bd1\u89e3\u91ca \u8ba1\u7b97\u6bcf\u4e00\u4e2a\u8fb9\u7f18\u70b9\u5230\"center\"\u70b9\u7684\u8ddd\u79bb\u4e0e\u76f8\u5bf9\u5939\u89d2\u3002 \u5bf9\u6bcf\u4e00\u4e2a\u9700\u8981ground truth\u7684\u5939\u89d2\uff1a \u5982\u679c\u57281\u4e2d\u6709\u8ba1\u7b97\u5230\uff0c\u5219\u5bf9\u5e94\u6700\u5927\u7684d(\u53ef\u80fd\u4e00\u4e2a\u89d2\u5ea6\u4e0d\u6b62\u4e00\u4e2a\u6700\u8fdc\u70b9\uff0c\u8981\u7a0d\u5fae\u8003\u8651\u51f9\u5f62\u72b6\uff0c\u8fd9\u79cd\u6570\u91cf\u4e0d\u591a)\u3002 \u5982\u679c\u6ca1\u6709\uff0c\u5219\u53d6\u6700\u90bb\u8fd1\u7684\u5df2\u6709\u7684theta\u5bf9\u5e94\u7684\u8ddd\u79bb\u6700\u4e3aground truth\uff0c\u5426\u5219\u53d6\u4e00\u4e2a\u6781\u5c0f\u503c\u3002","title":"distance labeling"},{"location":"other_categories/Segmentation/RDSNet/","text":"RDSNet: A New Deep Architecture for Reciprocal Object Detection and Instance Segmentation \u8fd9\u7bc7paper\u63d0\u51fa\u7684RDSNet,\u6838\u5fc3\u7684\u601d\u60f3\u5728\u4e8e\u8ba92D object detection\u4e0einstance segmentation\u76f8\u4e92\u5e2e\u52a9,\u5f00\u6e90\u7684\u4ee3\u7801\u57fa\u4e8emmdetection. \u7ed3\u6784\u4e0e\u65b9\u6cd5 object stream\u4e0epixel stream\u5206\u522b\u4e0e YOLOACT \u76f8\u4f3c\uff0c\u4e0d\u540c\u7684\u5730\u65b9\u5728\u4e8eRDSNet\u4f1a\u989d\u5916\u5bf9\u6bcf\u4e00\u4e2aanchor\u9884\u6d4b\u4e00\u4e2a 2\\times d \u7ef4\u7684representation. Instance Aware Pixel Stream \u8fd9\u4e00\u6b65\u7684\u8003\u8651\u662f\u8ba9pixel stream\u5f97\u5230object \u7279\u5f81\u4fe1\u606f\u3002 \u5bf9\u4e8e\u6bcf\u4e00\u4e2aobject o , \u6709\u5b83\u7684 2\\times d\\times 1\\times 1 \u7684representation \\phi(v_o) \u3002\u6574\u5f20\u56fe\u5728pixel stream\u4f1a\u5f62\u6210\u4e00\u4e2a 1\\times d\\times h_f\\times w_f \u7684\u7279\u5f81\uff0c\u62bd\u51fa\u4e0e\u76ee\u6807object\u76f8\u4f3c\u7684features M_{o}=\\operatorname{softmax}\\left(\\Psi(U) \\star \\phi\\left(v_{o}\\right)\\right) \u5176\u4e2d M_o \u7684\u7ef4\u5ea6\u662f 2\\times 1\\times h_f\\times w_f ,\u4f7f\u7528cross-entropy\u6765\u8bad\u7ec3\u3002\u6709\u70b9\u50cf\u662fmetric learning Cropping to Translation-variant \u524d\u4e00\u6b65\u7684\u4e00\u4e2a\u5173\u952e\u95ee\u9898\u5728\u4e8e\u540c\u7c7bobject \u4e0d\u540cinstance\u5728\u4e0a\u4e00\u6b65\u8fd8\u662f\u4f1a\u5f97\u5230\u9ad8\u7684\u76f8\u5173\u6027\uff0c\u5728cropping\u8fd9\u4e00\u6b65\u9700\u8981\u6253\u7834\u8fd9\u4e2a\u76f8\u5173\u6027\u3002\u4f5c\u8005\u7684\u505a\u6cd5\u6bd4\u8f83\u76f4\u767d\uff0c\u5c06\u73b0\u6709\u7684object \u8fb9\u7f18expand\u4e00\u4e2a\u9884\u8bbe\u7684\u6bd4\u4f8b\u4e4b\u540e(expand\u7684\u539f\u56e0\u662f\u907f\u514d\u56e0\u4e3a\u4f30\u8ba1\u7684\u4e0d\u51c6\u786e\u5ea6)\uff0c\u5c06\u5bf9\u5e94pixel stream\u4e2d\u5728\u8fb9\u7f18\u5916\u7684\u70b9\u8bbe\u4e3a\u80cc\u666f\u3002 MBRM mask assisted detection \u8fd9\u4e00\u6b65\u7684\u8003\u8651\u662f\u8ba9instance segmentation\u53bb\u63d0\u5347object detection\u7684\u51c6\u786e\u5ea6,\u4e2a\u4eba\u7684\u611f\u89c9\u4e0e\u76f4\u89c9\u5e76\u4e0d\u592a\u76f8\u7b26\u5408, \u4ee3\u7801","title":"RDSNet: A New Deep Architecture for Reciprocal Object Detection and Instance Segmentation"},{"location":"other_categories/Segmentation/RDSNet/#rdsnet-a-new-deep-architecture-for-reciprocal-object-detection-and-instance-segmentation","text":"\u8fd9\u7bc7paper\u63d0\u51fa\u7684RDSNet,\u6838\u5fc3\u7684\u601d\u60f3\u5728\u4e8e\u8ba92D object detection\u4e0einstance segmentation\u76f8\u4e92\u5e2e\u52a9,\u5f00\u6e90\u7684\u4ee3\u7801\u57fa\u4e8emmdetection.","title":"RDSNet: A New Deep Architecture for Reciprocal Object Detection and Instance Segmentation"},{"location":"other_categories/Segmentation/RDSNet/#_1","text":"object stream\u4e0epixel stream\u5206\u522b\u4e0e YOLOACT \u76f8\u4f3c\uff0c\u4e0d\u540c\u7684\u5730\u65b9\u5728\u4e8eRDSNet\u4f1a\u989d\u5916\u5bf9\u6bcf\u4e00\u4e2aanchor\u9884\u6d4b\u4e00\u4e2a 2\\times d \u7ef4\u7684representation.","title":"\u7ed3\u6784\u4e0e\u65b9\u6cd5"},{"location":"other_categories/Segmentation/RDSNet/#instance-aware-pixel-stream","text":"\u8fd9\u4e00\u6b65\u7684\u8003\u8651\u662f\u8ba9pixel stream\u5f97\u5230object \u7279\u5f81\u4fe1\u606f\u3002 \u5bf9\u4e8e\u6bcf\u4e00\u4e2aobject o , \u6709\u5b83\u7684 2\\times d\\times 1\\times 1 \u7684representation \\phi(v_o) \u3002\u6574\u5f20\u56fe\u5728pixel stream\u4f1a\u5f62\u6210\u4e00\u4e2a 1\\times d\\times h_f\\times w_f \u7684\u7279\u5f81\uff0c\u62bd\u51fa\u4e0e\u76ee\u6807object\u76f8\u4f3c\u7684features M_{o}=\\operatorname{softmax}\\left(\\Psi(U) \\star \\phi\\left(v_{o}\\right)\\right) \u5176\u4e2d M_o \u7684\u7ef4\u5ea6\u662f 2\\times 1\\times h_f\\times w_f ,\u4f7f\u7528cross-entropy\u6765\u8bad\u7ec3\u3002\u6709\u70b9\u50cf\u662fmetric learning","title":"Instance Aware Pixel Stream"},{"location":"other_categories/Segmentation/RDSNet/#cropping-to-translation-variant","text":"\u524d\u4e00\u6b65\u7684\u4e00\u4e2a\u5173\u952e\u95ee\u9898\u5728\u4e8e\u540c\u7c7bobject \u4e0d\u540cinstance\u5728\u4e0a\u4e00\u6b65\u8fd8\u662f\u4f1a\u5f97\u5230\u9ad8\u7684\u76f8\u5173\u6027\uff0c\u5728cropping\u8fd9\u4e00\u6b65\u9700\u8981\u6253\u7834\u8fd9\u4e2a\u76f8\u5173\u6027\u3002\u4f5c\u8005\u7684\u505a\u6cd5\u6bd4\u8f83\u76f4\u767d\uff0c\u5c06\u73b0\u6709\u7684object \u8fb9\u7f18expand\u4e00\u4e2a\u9884\u8bbe\u7684\u6bd4\u4f8b\u4e4b\u540e(expand\u7684\u539f\u56e0\u662f\u907f\u514d\u56e0\u4e3a\u4f30\u8ba1\u7684\u4e0d\u51c6\u786e\u5ea6)\uff0c\u5c06\u5bf9\u5e94pixel stream\u4e2d\u5728\u8fb9\u7f18\u5916\u7684\u70b9\u8bbe\u4e3a\u80cc\u666f\u3002","title":"Cropping to Translation-variant"},{"location":"other_categories/Segmentation/RDSNet/#mbrm-mask-assisted-detection","text":"\u8fd9\u4e00\u6b65\u7684\u8003\u8651\u662f\u8ba9instance segmentation\u53bb\u63d0\u5347object detection\u7684\u51c6\u786e\u5ea6,\u4e2a\u4eba\u7684\u611f\u89c9\u4e0e\u76f4\u89c9\u5e76\u4e0d\u592a\u76f8\u7b26\u5408, \u4ee3\u7801","title":"MBRM mask assisted detection"},{"location":"other_categories/Segmentation/SAUNet/","text":"SAUNet: Shape Attentive U-Net for Interpretable Medical Image Segmentation Structure \u672c\u6587\u5c06\u7f51\u7edc\u5206\u4e3atexture branch\u4e0eshape branch\u3002\u6ce8\u91cd\u4f7f\u7528Attention\u5f62\u6210\u53ef\u89e3\u91ca\u6027\u3002Loss\u4f7f\u7528\u57fa\u672c\u7684CE loss\u4ee5\u53ca Dice Loss","title":"SAUNet: Shape Attentive U-Net for Interpretable Medical Image Segmentation"},{"location":"other_categories/Segmentation/SAUNet/#saunet-shape-attentive-u-net-for-interpretable-medical-image-segmentation","text":"","title":"SAUNet: Shape Attentive U-Net for Interpretable Medical Image Segmentation"},{"location":"other_categories/Segmentation/SAUNet/#structure","text":"\u672c\u6587\u5c06\u7f51\u7edc\u5206\u4e3atexture branch\u4e0eshape branch\u3002\u6ce8\u91cd\u4f7f\u7528Attention\u5f62\u6210\u53ef\u89e3\u91ca\u6027\u3002Loss\u4f7f\u7528\u57fa\u672c\u7684CE loss\u4ee5\u53ca Dice Loss","title":"Structure"},{"location":"other_categories/Segmentation/SOLO/","text":"SOLO: Segmenting Objects by Locations \u8fd9\u7bc7\u6587\u7ae0\u7528\u4e00\u4e2a\u6bd4\u8f83\u7b80\u5355\u7684\u65b9\u6cd5\u5b9e\u73b0instance segmentation\uff0c\u601d\u8def\u5728\u4e8e\u5c06\u56fe\u7247\u5206\u4e3a S\\times S \u4e2a\u5c0fgrid\uff0c\u7136\u540e\u5047\u8bbe\u6bcf\u4e00\u4e2agrid\u91cc\u9762\u540c\u7c7b\u7269\u4f53\u53ea\u6709\u4e00\u4e2ainstance\uff0c\u7136\u540e\u8981\u6c42\u5206\u7c7b\u4efb\u52a1\u8f93\u51fa\u7684\u7ed3\u679c\u91cc\u9762\u5e26\u6709\"\u4f4d\u7f6e\u7c7b\".\u4ece\u800c\u5c06\u4e0d\u540c\u4f4d\u7f6e\u7684\u7ed3\u679c\u5206\u79bb\u51fa\u6765 Update: 2020/03/26: \u4f5c\u8005\u7ec4\u63a8\u51fa SOLOv2 : pdf Framework \u672c\u6587\u4f7f\u7528retinanet\u7684\u601d\u8def\uff0c\u4f7f\u7528multi-scale\u8f93\u51fa\uff0c\u4e0d\u540c\u7684scale\u5206\u89e3\u7684\u56fe\u50cf\u533a\u5757 S \u6570\u76ee\u662f\u4e0d\u540c\u7684 Breaking Translational Invariance \u4f46\u662f\u7f51\u7edc\u4e2d\u9700\u8981\u6ce8\u610f\u7684\u5730\u65b9\u5728\u4e8e\u4e00\u822c\u7684\u5b8c\u5168\u5377\u79ef\u7f51\u7edc\u662f\u5e73\u79fb\u4e0d\u53d8\u7684\uff0c\u4e5f\u5c31\u662f\u5728\u4e0d\u540c\u4f4d\u7f6e\u7684\u76f8\u4f3c\u7684\u7269\u4f53\uff0c\u5728\u7f51\u7edc\u4e2d\u5f62\u6210\u7684feature\u5e94\u8be5\u662f\u76f8\u4f3c\u7684\uff0c\u4f46\u662f\u5728\u672c\u6587\u4e2d\u8fd9\u662f\u4e0d\u88ab\u5141\u8bb8\u7684\uff0c\u56e0\u4e3a\u8f93\u51fa\u7684mask branch\u4e2d,\u6211\u4eec\u5e0c\u671b\u540c\u6837\u7684\u753b\u9762\u5185\u5bb9\uff0c\u5728\u4e0d\u540c\u4f4d\u7f6e\u8f93\u51fa\u7684feature\u5b8c\u5168\u4e0d\u540c\uff0c\u56e0\u800c\u5728mask branch\u4e2d\u4f7f\u7528 Coord Conv \u800c\u4e0d\u662f\u4e0d\u540c\u666e\u901a\u7684\u5377\u79ef\u6838\uff0c\u4f5c\u8005\u4e2d\u95f4\u63d0\u5230\u4e86 Semi Conv \u8fd9\u4e5f\u662f\u4e00\u79cd\u9009\u62e9\uff0c\u4e0d\u8fc7\u6700\u7ec8\u9009\u62e9\u7684\u662f Coord Conv Loss Function \u4f5c\u8005\u9762\u5bf9class imbalance problem,\u9009\u62e9\u4e86 Dice Loss \u800c\u4e0d\u662ffocal loss. \u5176\u516c\u5f0f\u4e3a: \\begin{aligned} L_{\\text {Dice}}&=1-D(\\mathbf{p}, \\mathbf{q})\\\\ D(\\mathbf{p}, \\mathbf{q})&=\\frac{2 \\sum_{x, y}\\left(\\mathbf{p}_{x, y} \\cdot \\mathbf{q}_{x, y}\\right)}{\\sum_{x, y} \\mathbf{p}_{x, y}^{2}+\\sum_{x, y} \\mathbf{q}_{x, y}^{2}} \\end{aligned} \u4ed4\u7ec6\u89c2\u5bdf D(p, q) \u7684\u516c\u5f0f\uff0c\u53ef\u4ee5\u4e86\u89e3\u5230\u8fd9\u662f\u4e00\u4e2a\u7c7b\u4f3c\u4e8eIoU\u7684metric, Decoupled SOLO \u7531\u4e8e\u8981\u5206\u7c7b S^2 \uff0c\u8fd9\u4e2achannel\u6570\u592a\u5927\u4e86\uff0c\u540c\u65f6\u6709\u5f88\u5927\u5197\u4f59\uff0c\u56e0\u800c\u8f6c\u800c\u8ba9\u7f51\u7edc\u5c06 X, Y \u4e0a\u7684bin\u5206\u522b\u5206\u7c7b\uff0cinference\u7684\u65f6\u5019\u8ba9\u5bf9\u5e94\u884c\u5217\u7684sigmoid\u76f8\u4e58 SOLOv2 performance: \u63d0\u5347\u5728\u4e8e\u4e09\u4e2a\u95ee\u9898 \u5bf9\u7a00\u758f\u7684Mask prediction \u4f7f\u7528dynamic head\u8fdb\u884c\u4f18\u5316 \u5145\u5206\u5229\u7528FPN\u878d\u5408\u591a\u5c3a\u5ea6 Matrix NMS Dynamic head Dynamic head\u7684\u7b97\u6cd5\u6765\u8bf4\uff0c\u5c31\u662f\u8ba9\u7f51\u7edc\u5355\u72ec\u8bad\u7ec3\u4e00\u4e2a\u5377\u79ef\u6838\u7528\u4e8e\u6700\u540e\u4e00\u6b65\u7684 1\\times 1 \u5377\u79ef\u3002 \u76f4\u89c9\u5982\u6b64\uff1a\u89c2\u5bdf\u539f\u6765\u7684SOLO\uff0c S\\times S \u4e2aMask\u8fd9\u6837\u7684\u9884\u6d4b\u662f\u5f88\u7a00\u758f\u7684\uff0c\u53ea\u6709\u90e8\u5206\u6709\u7528\uff0c\u4e0d\u5982train\u4e00\u4e2a\u52a8\u6001\u7684 1\\times 1 or 3\\times 3 \u5377\u79ef\u6838\u5b66\u4f1a\u4ece\u91cc\u9762\u7f51\u7edc\u4e2d\u5206\u5f00\u4e0d\u540c\u4f4d\u7f6e\u7684\u7ed3\u679c(\u6ce8\u610f\u8fd9\u4e2a\u5377\u79ef\u6838\u7684\u751f\u6210\u8fc7\u7a0b\u7684\u611f\u53d7\u91ce\u662f\u5168\u56fe)\u3002(\u7a0d\u7a0d\u6709\u4e00\u70b9\u8be1\u5f02\u4e0d\u8fc7\u53ef\u4ee5\u63a5\u53d7)\u3002 Mask Feature prediction \u7b80\u5355\u6765\u8bf4\u5c31\u662f\u7528 Coord Conv \u8ba1\u7b97\u540e\u4e0a\u91c7\u6837\u6240\u6709scale\u7684\u7279\u5f81\uff0c\u6c42\u548c\uff0c\u7136\u540e 1\\times 1 \u5377\u79ef\u5f97\u5230\u6700\u7ec8\u8f93\u51fa Matrix NMS \u91c7\u7528\u7684\u6982\u5ff5\u662f SoftNMS\u7684\u6982\u5ff5\uff0c\u4e5f\u5c31\u662f\u6839\u636e\u91cd\u5408\u7684\u6846\u7684IOU\u4e0e\u6982\u7387 \u8fdb\u4e00\u6b65\u964d\u4f4e \u4f4escore box\u7684score\u7684\u505a\u6cd5\u3002\u8fd9\u91cc\u5728\u540e\u9762NMS\u65f6\uff0c\u7528IoU\u7684\u51fd\u6570\u66ff\u4ee3\u6982\u7387\u503c\uff0c\u56e0\u800cdecay scale\u53ea\u4e0eIoU\u6709\u5173","title":"SOLO: Segmenting Objects by Locations"},{"location":"other_categories/Segmentation/SOLO/#solo-segmenting-objects-by-locations","text":"\u8fd9\u7bc7\u6587\u7ae0\u7528\u4e00\u4e2a\u6bd4\u8f83\u7b80\u5355\u7684\u65b9\u6cd5\u5b9e\u73b0instance segmentation\uff0c\u601d\u8def\u5728\u4e8e\u5c06\u56fe\u7247\u5206\u4e3a S\\times S \u4e2a\u5c0fgrid\uff0c\u7136\u540e\u5047\u8bbe\u6bcf\u4e00\u4e2agrid\u91cc\u9762\u540c\u7c7b\u7269\u4f53\u53ea\u6709\u4e00\u4e2ainstance\uff0c\u7136\u540e\u8981\u6c42\u5206\u7c7b\u4efb\u52a1\u8f93\u51fa\u7684\u7ed3\u679c\u91cc\u9762\u5e26\u6709\"\u4f4d\u7f6e\u7c7b\".\u4ece\u800c\u5c06\u4e0d\u540c\u4f4d\u7f6e\u7684\u7ed3\u679c\u5206\u79bb\u51fa\u6765 Update: 2020/03/26: \u4f5c\u8005\u7ec4\u63a8\u51fa SOLOv2 : pdf","title":"SOLO: Segmenting Objects by Locations"},{"location":"other_categories/Segmentation/SOLO/#framework","text":"\u672c\u6587\u4f7f\u7528retinanet\u7684\u601d\u8def\uff0c\u4f7f\u7528multi-scale\u8f93\u51fa\uff0c\u4e0d\u540c\u7684scale\u5206\u89e3\u7684\u56fe\u50cf\u533a\u5757 S \u6570\u76ee\u662f\u4e0d\u540c\u7684","title":"Framework"},{"location":"other_categories/Segmentation/SOLO/#breaking-translational-invariance","text":"\u4f46\u662f\u7f51\u7edc\u4e2d\u9700\u8981\u6ce8\u610f\u7684\u5730\u65b9\u5728\u4e8e\u4e00\u822c\u7684\u5b8c\u5168\u5377\u79ef\u7f51\u7edc\u662f\u5e73\u79fb\u4e0d\u53d8\u7684\uff0c\u4e5f\u5c31\u662f\u5728\u4e0d\u540c\u4f4d\u7f6e\u7684\u76f8\u4f3c\u7684\u7269\u4f53\uff0c\u5728\u7f51\u7edc\u4e2d\u5f62\u6210\u7684feature\u5e94\u8be5\u662f\u76f8\u4f3c\u7684\uff0c\u4f46\u662f\u5728\u672c\u6587\u4e2d\u8fd9\u662f\u4e0d\u88ab\u5141\u8bb8\u7684\uff0c\u56e0\u4e3a\u8f93\u51fa\u7684mask branch\u4e2d,\u6211\u4eec\u5e0c\u671b\u540c\u6837\u7684\u753b\u9762\u5185\u5bb9\uff0c\u5728\u4e0d\u540c\u4f4d\u7f6e\u8f93\u51fa\u7684feature\u5b8c\u5168\u4e0d\u540c\uff0c\u56e0\u800c\u5728mask branch\u4e2d\u4f7f\u7528 Coord Conv \u800c\u4e0d\u662f\u4e0d\u540c\u666e\u901a\u7684\u5377\u79ef\u6838\uff0c\u4f5c\u8005\u4e2d\u95f4\u63d0\u5230\u4e86 Semi Conv \u8fd9\u4e5f\u662f\u4e00\u79cd\u9009\u62e9\uff0c\u4e0d\u8fc7\u6700\u7ec8\u9009\u62e9\u7684\u662f Coord Conv","title":"Breaking Translational Invariance"},{"location":"other_categories/Segmentation/SOLO/#loss-function","text":"\u4f5c\u8005\u9762\u5bf9class imbalance problem,\u9009\u62e9\u4e86 Dice Loss \u800c\u4e0d\u662ffocal loss. \u5176\u516c\u5f0f\u4e3a: \\begin{aligned} L_{\\text {Dice}}&=1-D(\\mathbf{p}, \\mathbf{q})\\\\ D(\\mathbf{p}, \\mathbf{q})&=\\frac{2 \\sum_{x, y}\\left(\\mathbf{p}_{x, y} \\cdot \\mathbf{q}_{x, y}\\right)}{\\sum_{x, y} \\mathbf{p}_{x, y}^{2}+\\sum_{x, y} \\mathbf{q}_{x, y}^{2}} \\end{aligned} \u4ed4\u7ec6\u89c2\u5bdf D(p, q) \u7684\u516c\u5f0f\uff0c\u53ef\u4ee5\u4e86\u89e3\u5230\u8fd9\u662f\u4e00\u4e2a\u7c7b\u4f3c\u4e8eIoU\u7684metric,","title":"Loss Function"},{"location":"other_categories/Segmentation/SOLO/#decoupled-solo","text":"\u7531\u4e8e\u8981\u5206\u7c7b S^2 \uff0c\u8fd9\u4e2achannel\u6570\u592a\u5927\u4e86\uff0c\u540c\u65f6\u6709\u5f88\u5927\u5197\u4f59\uff0c\u56e0\u800c\u8f6c\u800c\u8ba9\u7f51\u7edc\u5c06 X, Y \u4e0a\u7684bin\u5206\u522b\u5206\u7c7b\uff0cinference\u7684\u65f6\u5019\u8ba9\u5bf9\u5e94\u884c\u5217\u7684sigmoid\u76f8\u4e58","title":"Decoupled SOLO"},{"location":"other_categories/Segmentation/SOLO/#solov2","text":"performance: \u63d0\u5347\u5728\u4e8e\u4e09\u4e2a\u95ee\u9898 \u5bf9\u7a00\u758f\u7684Mask prediction \u4f7f\u7528dynamic head\u8fdb\u884c\u4f18\u5316 \u5145\u5206\u5229\u7528FPN\u878d\u5408\u591a\u5c3a\u5ea6 Matrix NMS","title":"SOLOv2"},{"location":"other_categories/Segmentation/SOLO/#dynamic-head","text":"Dynamic head\u7684\u7b97\u6cd5\u6765\u8bf4\uff0c\u5c31\u662f\u8ba9\u7f51\u7edc\u5355\u72ec\u8bad\u7ec3\u4e00\u4e2a\u5377\u79ef\u6838\u7528\u4e8e\u6700\u540e\u4e00\u6b65\u7684 1\\times 1 \u5377\u79ef\u3002 \u76f4\u89c9\u5982\u6b64\uff1a\u89c2\u5bdf\u539f\u6765\u7684SOLO\uff0c S\\times S \u4e2aMask\u8fd9\u6837\u7684\u9884\u6d4b\u662f\u5f88\u7a00\u758f\u7684\uff0c\u53ea\u6709\u90e8\u5206\u6709\u7528\uff0c\u4e0d\u5982train\u4e00\u4e2a\u52a8\u6001\u7684 1\\times 1 or 3\\times 3 \u5377\u79ef\u6838\u5b66\u4f1a\u4ece\u91cc\u9762\u7f51\u7edc\u4e2d\u5206\u5f00\u4e0d\u540c\u4f4d\u7f6e\u7684\u7ed3\u679c(\u6ce8\u610f\u8fd9\u4e2a\u5377\u79ef\u6838\u7684\u751f\u6210\u8fc7\u7a0b\u7684\u611f\u53d7\u91ce\u662f\u5168\u56fe)\u3002(\u7a0d\u7a0d\u6709\u4e00\u70b9\u8be1\u5f02\u4e0d\u8fc7\u53ef\u4ee5\u63a5\u53d7)\u3002","title":"Dynamic head"},{"location":"other_categories/Segmentation/SOLO/#mask-feature-prediction","text":"\u7b80\u5355\u6765\u8bf4\u5c31\u662f\u7528 Coord Conv \u8ba1\u7b97\u540e\u4e0a\u91c7\u6837\u6240\u6709scale\u7684\u7279\u5f81\uff0c\u6c42\u548c\uff0c\u7136\u540e 1\\times 1 \u5377\u79ef\u5f97\u5230\u6700\u7ec8\u8f93\u51fa","title":"Mask Feature prediction"},{"location":"other_categories/Segmentation/SOLO/#matrix-nms","text":"\u91c7\u7528\u7684\u6982\u5ff5\u662f SoftNMS\u7684\u6982\u5ff5\uff0c\u4e5f\u5c31\u662f\u6839\u636e\u91cd\u5408\u7684\u6846\u7684IOU\u4e0e\u6982\u7387 \u8fdb\u4e00\u6b65\u964d\u4f4e \u4f4escore box\u7684score\u7684\u505a\u6cd5\u3002\u8fd9\u91cc\u5728\u540e\u9762NMS\u65f6\uff0c\u7528IoU\u7684\u51fd\u6570\u66ff\u4ee3\u6982\u7387\u503c\uff0c\u56e0\u800cdecay scale\u53ea\u4e0eIoU\u6709\u5173","title":"Matrix NMS"},{"location":"other_categories/Segmentation/TPV/","text":"Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction \u8fd9\u7bc7paper\u5c1d\u8bd5\u89e3\u51b3tesla\u5728AI day\u4e0a\u63cf\u8ff0\u7684\u4e00\u4e2a3D semantic occupancy map prediction. \u5728\u5b66\u754c\u5df2\u7ecf\u94fa\u5f00BEV\u65b9\u6848\u7684\u4eca\u5929\uff0c\u8fd9\u4e2a\u4efb\u52a1\u7684\u96be\u70b9\u5728\u4e8e3D voxelized prediction\u4e0e image frames\u4e4b\u95f4\u7684attention\u6216\u8005feature transformation\u5360\u636e\u7684\u663e\u5b58\u4e0e\u8fd0\u7b97\u7a7a\u95f4\u8fc7\u5927\u3002\u672c\u6587\u5c1d\u8bd5\u5229\u75283D\u4fe1\u606f\u7684\u5197\u4f59\uff0c\u4ece\u9884\u6d4b\u4e09\u7ef4\u7acb\u4f53feature\u964d\u7ef4\u81f3\u9884\u6d4b\u4e09\u4e2a\u76f8\u4e92\u5782\u76f4\u7684\u4e09\u89c6\u56feFeatures\u4ee5\u63a7\u5236\u6a21\u578b\u590d\u6742\u5ea6\u3002 Basic Framework \u57fa\u672c\u6a21\u578b\u63cf\u8ff0: \u7f51\u7edc\u8f93\u5165\u516d\u5f20\u56fe\uff0c\u4f7f\u7528cnn backbone\u8fdb\u884c\u5904\u7406\u3002 \u4e09\u4e2aTPV \u5e73\u9762\u7684 query(embedding) \u4f5c\u4e3a\u8d77\u59cb\u8f93\u5165\uff0c\u7ecf\u8fc7\u4e09\u5c42 TPV\u5e73\u9762\u4e0eImages\u7684cross attention, \u518d\u7ecf\u8fc7\u4e24\u5c42 TPV\u5e73\u9762\u4e4b\u95f4\u7684cross attention\u3002\u5f62\u6210\u5bf9\u73af\u5883\u573a\u666f\u7684\u4e09\u89c6\u56fe\u7279\u5f81\u63cf\u8ff0\u3002 \u4e0e\u56fe\u7247\u7684cross attention\u3002\u6a21\u5757\u6574\u4f53\u662fdeformable attention. \u5bf9\u4e8e\u4e00\u4e2aplane query\u4e0a\u7684\u6bcf\u4e00\u4e2a\u70b9\uff0c\u5728\u5782\u76f4\u4e8e\u5e73\u9762\u65b9\u5411\u91c7\u6837\u6570\u4e2a\u4e09\u7ef4\u70b9\uff0c\u6295\u5f71\u5230\u516d\u4e2a\u76f8\u673a\u4e0a\uff0c\u627e\u5230\u76f8\u673a\u4e0a\u7684base reference points. \u7136\u540e\u6839\u636eplane query -> MLP mapping \u8f93\u51fa\u6bcf\u4e2a\u70b9\u7684\u504f\u79fb\u503c\u3002\u5728\u56fe\u7247\u4e0a\u91c7\u6837\uff0c\u5e76\u4f7f\u7528attention\u6c42\u548c\u3002 \u5e73\u9762\u4e4b\u95f4\u7684hybrid attention. \u6a21\u5757\u6574\u4f53\u5f0fDeformable attention. \u5bf9\u4e8e\u4e00\u4e2aplane query\u4e0a\u7684\u6bcf\u4e00\u4e2a\u70b9\uff0c\u5728\u5782\u76f4\u4e8e\u5e73\u9762\u65b9\u5411\u91c7\u6837\u6570\u4e2a\u70b9\uff0c\u7136\u540e\u6295\u5f71\u5230\u5176\u4ed6\u4e24\u4e2a\u5e73\u9762\u7279\u5f81\u4e0a\uff0c\u5176\u4ed6\u4e0e\u56fe\u7247\u7684\u76f8\u4f3c\u3002 \u7f51\u7edc\u8f93\u51fa\u7684\u5f62\u6001\uff0c\u5982\u679c\u8981\u67e5\u8be2\u5355\u4e2a\u70b9\u7684\u7ed3\u679c\uff0c\u5c31\u628a\u5355\u4e2a\u70b9\u6295\u5f71\u5230\u4e09\u4e2aTPV\u5e73\u9762\u4e0a\uff0c\u53d6\u7279\u5f81\uff0c\u7d2f\u52a0\u4e4b\u540eMLP mapping\u5230\u8bed\u4e49\u5206\u5272\u8f93\u51fa\u3002\u5982\u679c\u8981\u540c\u65f6\u8f93\u51fa\u7f51\u683c\u4e2d\u6bcf\u4e00\u4e2a\u70b9\uff0c\u5219\u53ef\u4ee5\u628a\u4e09\u4e2a\u5e73\u9762broadcast\u5230voxel\u5f62\u6001\uff0c\u7136\u540e\u6574\u4f53MLP mapping\u540c\u65f6\u8f93\u51fa\uff08\u4e00\u822c\u7528\u4e8e\u53ef\u89c6\u5316\uff09\u3002 \u5bf9\u4e8eLidar Segmentation works,\u5bf9\u6bcf\u4e00\u4e2a\u6709\u6807\u6ce8\u7684\u96f7\u8fbe\u70b9\uff0c\u5355\u72ec\u91c7\u6837\u5176\u8bed\u4e49\u5206\u5272\u7ed3\u679c\u8fdb\u884c\u76d1\u7763\u8bad\u7ec3\u3002","title":"Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction"},{"location":"other_categories/Segmentation/TPV/#tri-perspective-view-for-vision-based-3d-semantic-occupancy-prediction","text":"\u8fd9\u7bc7paper\u5c1d\u8bd5\u89e3\u51b3tesla\u5728AI day\u4e0a\u63cf\u8ff0\u7684\u4e00\u4e2a3D semantic occupancy map prediction. \u5728\u5b66\u754c\u5df2\u7ecf\u94fa\u5f00BEV\u65b9\u6848\u7684\u4eca\u5929\uff0c\u8fd9\u4e2a\u4efb\u52a1\u7684\u96be\u70b9\u5728\u4e8e3D voxelized prediction\u4e0e image frames\u4e4b\u95f4\u7684attention\u6216\u8005feature transformation\u5360\u636e\u7684\u663e\u5b58\u4e0e\u8fd0\u7b97\u7a7a\u95f4\u8fc7\u5927\u3002\u672c\u6587\u5c1d\u8bd5\u5229\u75283D\u4fe1\u606f\u7684\u5197\u4f59\uff0c\u4ece\u9884\u6d4b\u4e09\u7ef4\u7acb\u4f53feature\u964d\u7ef4\u81f3\u9884\u6d4b\u4e09\u4e2a\u76f8\u4e92\u5782\u76f4\u7684\u4e09\u89c6\u56feFeatures\u4ee5\u63a7\u5236\u6a21\u578b\u590d\u6742\u5ea6\u3002","title":"Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction"},{"location":"other_categories/Segmentation/TPV/#basic-framework","text":"\u57fa\u672c\u6a21\u578b\u63cf\u8ff0: \u7f51\u7edc\u8f93\u5165\u516d\u5f20\u56fe\uff0c\u4f7f\u7528cnn backbone\u8fdb\u884c\u5904\u7406\u3002 \u4e09\u4e2aTPV \u5e73\u9762\u7684 query(embedding) \u4f5c\u4e3a\u8d77\u59cb\u8f93\u5165\uff0c\u7ecf\u8fc7\u4e09\u5c42 TPV\u5e73\u9762\u4e0eImages\u7684cross attention, \u518d\u7ecf\u8fc7\u4e24\u5c42 TPV\u5e73\u9762\u4e4b\u95f4\u7684cross attention\u3002\u5f62\u6210\u5bf9\u73af\u5883\u573a\u666f\u7684\u4e09\u89c6\u56fe\u7279\u5f81\u63cf\u8ff0\u3002 \u4e0e\u56fe\u7247\u7684cross attention\u3002\u6a21\u5757\u6574\u4f53\u662fdeformable attention. \u5bf9\u4e8e\u4e00\u4e2aplane query\u4e0a\u7684\u6bcf\u4e00\u4e2a\u70b9\uff0c\u5728\u5782\u76f4\u4e8e\u5e73\u9762\u65b9\u5411\u91c7\u6837\u6570\u4e2a\u4e09\u7ef4\u70b9\uff0c\u6295\u5f71\u5230\u516d\u4e2a\u76f8\u673a\u4e0a\uff0c\u627e\u5230\u76f8\u673a\u4e0a\u7684base reference points. \u7136\u540e\u6839\u636eplane query -> MLP mapping \u8f93\u51fa\u6bcf\u4e2a\u70b9\u7684\u504f\u79fb\u503c\u3002\u5728\u56fe\u7247\u4e0a\u91c7\u6837\uff0c\u5e76\u4f7f\u7528attention\u6c42\u548c\u3002 \u5e73\u9762\u4e4b\u95f4\u7684hybrid attention. \u6a21\u5757\u6574\u4f53\u5f0fDeformable attention. \u5bf9\u4e8e\u4e00\u4e2aplane query\u4e0a\u7684\u6bcf\u4e00\u4e2a\u70b9\uff0c\u5728\u5782\u76f4\u4e8e\u5e73\u9762\u65b9\u5411\u91c7\u6837\u6570\u4e2a\u70b9\uff0c\u7136\u540e\u6295\u5f71\u5230\u5176\u4ed6\u4e24\u4e2a\u5e73\u9762\u7279\u5f81\u4e0a\uff0c\u5176\u4ed6\u4e0e\u56fe\u7247\u7684\u76f8\u4f3c\u3002 \u7f51\u7edc\u8f93\u51fa\u7684\u5f62\u6001\uff0c\u5982\u679c\u8981\u67e5\u8be2\u5355\u4e2a\u70b9\u7684\u7ed3\u679c\uff0c\u5c31\u628a\u5355\u4e2a\u70b9\u6295\u5f71\u5230\u4e09\u4e2aTPV\u5e73\u9762\u4e0a\uff0c\u53d6\u7279\u5f81\uff0c\u7d2f\u52a0\u4e4b\u540eMLP mapping\u5230\u8bed\u4e49\u5206\u5272\u8f93\u51fa\u3002\u5982\u679c\u8981\u540c\u65f6\u8f93\u51fa\u7f51\u683c\u4e2d\u6bcf\u4e00\u4e2a\u70b9\uff0c\u5219\u53ef\u4ee5\u628a\u4e09\u4e2a\u5e73\u9762broadcast\u5230voxel\u5f62\u6001\uff0c\u7136\u540e\u6574\u4f53MLP mapping\u540c\u65f6\u8f93\u51fa\uff08\u4e00\u822c\u7528\u4e8e\u53ef\u89c6\u5316\uff09\u3002 \u5bf9\u4e8eLidar Segmentation works,\u5bf9\u6bcf\u4e00\u4e2a\u6709\u6807\u6ce8\u7684\u96f7\u8fbe\u70b9\uff0c\u5355\u72ec\u91c7\u6837\u5176\u8bed\u4e49\u5206\u5272\u7ed3\u679c\u8fdb\u884c\u76d1\u7763\u8bad\u7ec3\u3002","title":"Basic Framework"},{"location":"other_categories/Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/","text":"TensorMask: A Foundation for Dense Object Segmentation \u8fd9\u7bc7\u6587\u7ae0\u63d0\u51fa\u4e86one-stage instance segmentation\u7684\u7b97\u6cd5\u3002\u4f20\u7edf\u6765\u8bf4\uff0cstate of the art \u7684instance segmentation\u7684\u505a\u6cd5\u57fa\u672c\u4e0a\u662f\u5148\u8fdb\u884cobject detection\u5f97\u52302D\u6846\uff0c\u7136\u540e\u5728\u6846\u5185\u8fdb\u884cSemantic Segmentation\u3002\u8fd9\u6837\u7684two-stage\u751a\u81f3\u662fMulti-stage\u7684\u505a\u6cd5(object detection\u53ef\u80fd\u5c31two-stage).\u8fd8\u6709\u4e00\u79cd\u505a\u6cd5\u662f\u5148\u751f\u6210label pixel\u7136\u540e\u8fdb\u884c\u805a\u7c7b\u3002 \u672c\u6587\u6838\u5fc3\u601d\u8def\u5c31\u662f\u5c06\u6574\u4e2a\u95ee\u9898\u8f6c\u6362\u4e3a\u4e00\u4e2a\u56db\u7ef4\u5f20\u91cf (V, U, H, W) \u7684\u56de\u5f52or\u5206\u7c7b\u95ee\u9898\u3002\u5bf9\u6bcf\u4e00\u4e2a\u5750\u6807\u70b9 (h, w) \u5bf9\u5e94\u4e00\u4e2a\u77e9\u9635 (V, U) \uff0c\u8bbe \\alpha \u4e3a\u5355\u4f4d\u8f6c\u6362\u6bd4\u4f8b\uff0c\u5219\u77e9\u9635\u4e2d\u7684\u5143\u7d20 (v, u) \u6307\u4ee3\u539f\u56fe (h + \\alpha v, w + \\alpha u) \u662fmask\u7684\u6982\u7387\uff0c\u6216\u5176\u4ed6\u53c2\u6570\u3002\u8fd9\u6837\u6574\u4e2a\u7f51\u7edc\u7684\u8bad\u7ec3\u76ee\u6807\u5c31\u548c\u4e00\u4e2a SSD \u6216\u8005\u8bf4Yolo\u5dee\u4e0d\u591a\u4e86,\u8fd9\u540c\u65f6\u53c8\u548cDeepMask\u4e0d\u540c\uff0c\u663e\u5f0f\u5730\u8868\u8fbe U, V \u5750\u6807\uff0c\u5e76\u4e3a\u6b64\u9002\u914d\u66f4\u591a\u7684\u8fd0\u7b97\u65b9\u5f0f. \u4e3b\u8981\u8868\u8fbe\u65b9\u5f0f\u7684\u5b9a\u4e49 Natural Representation \u8868\u8fbe\u4e3a (V, U, H, W) ,\u5bf9\u4e8e\u4e00\u4e2a4D (V,U,H,W) \u7684\u5f20\u91cf\uff0c\u5b83\u5728 (v, u, y, x) \u7684\u503c\u4ee3\u8868\u5728\u4e00\u4e2a\u4e2d\u5fc3\u5728 (y,x) \u7684\u5927\u5c0f\u4e3a \\alpha V \\times \\alpha U \u7a97\u53e3\u7684\u70b9 (y + \\alpha v, x + \\alpha u) \u7684mask\u503c\u3002 Aligned Representation \u5bf9\u4e00\u4e2a4D (\\hat V, \\hat U, \\hat H, \\hat W) \u7684\u5f20\u91cf\uff0c\u5b83\u5728 (\\hat v, \\hat u, \\hat y, \\hat x) \u7684\u503c\u4ee3\u8868\u5728\u4e00\u4e2a\u4e2d\u5fc3\u5728 (\\hat y, \\hat x) \u7684\u5927\u5c0f\u4e3a \\hat\\alpha \\hat V \\times \\hat\\alpha \\hat U \u7a97\u53e3\u7684\u70b9 (\\hat y - \\hat\\alpha \\hat v, \\hat x - \\hat\\alpha \\hat u) \u7684mask\u503c\u3002 \u5173\u952e\u7684\u7406\u89e3\u662f\u5728\u5750\u6807 (\\hat y, \\hat x) \u4e0a\u7684\u5b50\u77e9\u9635 (\\hat V, \\hat U) \uff0c\u4e0a\u7684\u6240\u6709\u503c\u90fd\u662f\u5728\u63cf\u8ff0\u8fd9\u4e2a\u5750\u6807 (\\hat y, \\hat x) \u7684\uff0c\u6240\u4ee5\u79f0\u4e3a\u4e3a aligned \u4e24\u8005\u7684\u5b9a\u4e49\u53ef\u4ee5\u7531\u8fd9\u5f20\u56fe\u663e\u793a \u4e24\u8005\u7684\u8f6c\u6362\uff1a \\begin{aligned} \\mathcal{F}(v, u, y, x) &= \\hat\\mathcal{F}(v, u, y+\\alpha v, x + \\alpha u) \\\\ \\hat\\mathcal{F}(\\hat v, \\hat u, \\hat y, \\hat x) &= \\mathcal{F}(\\hat v, \\hat u, \\hat y - \\alpha\\hat v, \\hat x- \\alpha\\hat u) \\end{aligned} \u7f51\u7edc\u7ed3\u6784, \u8f93\u51faHead, \u7ec6\u8282\u7ed3\u6784, \u8bad\u7ec3\u7ec6\u8282 \u7f51\u7edc\u91c7\u7528FPN\u8f93\u51fa\u591a\u4e2a\u4e0d\u540c\u5c3a\u5ea6\u7684feature maps,\u5f62\u72b6 (C, \\frac{1}{2^k} H, \\frac{1}{2^k}W \u8f93\u51faHead \u672c\u6587\u6bd4\u8f83\u4e865\u79cd\u8f93\u51faHead. 4\u79cd\u662fbaseline, \u533a\u522b\u5728\u4e8e\u4e0d\u540cScale\u4e0a\u7684\u56fe\uff0c\u7b2c\u4e94\u4e2ahead\u4f1a\u8f93\u51fa\u76f8\u540c\u7cbe\u786e\u5ea6\u7684\u7f51\u683c \u5176\u4e2d\u7684\u7ec6\u8282\u8fd0\u7b97\u5982\u56fe \u8fd9\u4e9b\u7ec6\u8282\u8fd0\u7b97\u672c\u8d28\u4e0a\u90fd\u662f\u5750\u6807\u53d8\u6362\u4ee5\u53ca\u91c7\u6837 \u8bad\u7ec3\u7ec6\u8282 \u5bf9FPN\u7684\u5fae\u8c03 Label\u5206\u914d Fully Contain center of m is close to center of windows unique","title":"TensorMask: A Foundation for Dense Object Segmentation"},{"location":"other_categories/Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/#tensormask-a-foundation-for-dense-object-segmentation","text":"\u8fd9\u7bc7\u6587\u7ae0\u63d0\u51fa\u4e86one-stage instance segmentation\u7684\u7b97\u6cd5\u3002\u4f20\u7edf\u6765\u8bf4\uff0cstate of the art \u7684instance segmentation\u7684\u505a\u6cd5\u57fa\u672c\u4e0a\u662f\u5148\u8fdb\u884cobject detection\u5f97\u52302D\u6846\uff0c\u7136\u540e\u5728\u6846\u5185\u8fdb\u884cSemantic Segmentation\u3002\u8fd9\u6837\u7684two-stage\u751a\u81f3\u662fMulti-stage\u7684\u505a\u6cd5(object detection\u53ef\u80fd\u5c31two-stage).\u8fd8\u6709\u4e00\u79cd\u505a\u6cd5\u662f\u5148\u751f\u6210label pixel\u7136\u540e\u8fdb\u884c\u805a\u7c7b\u3002 \u672c\u6587\u6838\u5fc3\u601d\u8def\u5c31\u662f\u5c06\u6574\u4e2a\u95ee\u9898\u8f6c\u6362\u4e3a\u4e00\u4e2a\u56db\u7ef4\u5f20\u91cf (V, U, H, W) \u7684\u56de\u5f52or\u5206\u7c7b\u95ee\u9898\u3002\u5bf9\u6bcf\u4e00\u4e2a\u5750\u6807\u70b9 (h, w) \u5bf9\u5e94\u4e00\u4e2a\u77e9\u9635 (V, U) \uff0c\u8bbe \\alpha \u4e3a\u5355\u4f4d\u8f6c\u6362\u6bd4\u4f8b\uff0c\u5219\u77e9\u9635\u4e2d\u7684\u5143\u7d20 (v, u) \u6307\u4ee3\u539f\u56fe (h + \\alpha v, w + \\alpha u) \u662fmask\u7684\u6982\u7387\uff0c\u6216\u5176\u4ed6\u53c2\u6570\u3002\u8fd9\u6837\u6574\u4e2a\u7f51\u7edc\u7684\u8bad\u7ec3\u76ee\u6807\u5c31\u548c\u4e00\u4e2a SSD \u6216\u8005\u8bf4Yolo\u5dee\u4e0d\u591a\u4e86,\u8fd9\u540c\u65f6\u53c8\u548cDeepMask\u4e0d\u540c\uff0c\u663e\u5f0f\u5730\u8868\u8fbe U, V \u5750\u6807\uff0c\u5e76\u4e3a\u6b64\u9002\u914d\u66f4\u591a\u7684\u8fd0\u7b97\u65b9\u5f0f.","title":"TensorMask: A Foundation for Dense Object Segmentation"},{"location":"other_categories/Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/#_1","text":"","title":"\u4e3b\u8981\u8868\u8fbe\u65b9\u5f0f\u7684\u5b9a\u4e49"},{"location":"other_categories/Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/#natural-representation","text":"\u8868\u8fbe\u4e3a (V, U, H, W) ,\u5bf9\u4e8e\u4e00\u4e2a4D (V,U,H,W) \u7684\u5f20\u91cf\uff0c\u5b83\u5728 (v, u, y, x) \u7684\u503c\u4ee3\u8868\u5728\u4e00\u4e2a\u4e2d\u5fc3\u5728 (y,x) \u7684\u5927\u5c0f\u4e3a \\alpha V \\times \\alpha U \u7a97\u53e3\u7684\u70b9 (y + \\alpha v, x + \\alpha u) \u7684mask\u503c\u3002","title":"Natural Representation"},{"location":"other_categories/Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/#aligned-representation","text":"\u5bf9\u4e00\u4e2a4D (\\hat V, \\hat U, \\hat H, \\hat W) \u7684\u5f20\u91cf\uff0c\u5b83\u5728 (\\hat v, \\hat u, \\hat y, \\hat x) \u7684\u503c\u4ee3\u8868\u5728\u4e00\u4e2a\u4e2d\u5fc3\u5728 (\\hat y, \\hat x) \u7684\u5927\u5c0f\u4e3a \\hat\\alpha \\hat V \\times \\hat\\alpha \\hat U \u7a97\u53e3\u7684\u70b9 (\\hat y - \\hat\\alpha \\hat v, \\hat x - \\hat\\alpha \\hat u) \u7684mask\u503c\u3002 \u5173\u952e\u7684\u7406\u89e3\u662f\u5728\u5750\u6807 (\\hat y, \\hat x) \u4e0a\u7684\u5b50\u77e9\u9635 (\\hat V, \\hat U) \uff0c\u4e0a\u7684\u6240\u6709\u503c\u90fd\u662f\u5728\u63cf\u8ff0\u8fd9\u4e2a\u5750\u6807 (\\hat y, \\hat x) \u7684\uff0c\u6240\u4ee5\u79f0\u4e3a\u4e3a aligned \u4e24\u8005\u7684\u5b9a\u4e49\u53ef\u4ee5\u7531\u8fd9\u5f20\u56fe\u663e\u793a \u4e24\u8005\u7684\u8f6c\u6362\uff1a \\begin{aligned} \\mathcal{F}(v, u, y, x) &= \\hat\\mathcal{F}(v, u, y+\\alpha v, x + \\alpha u) \\\\ \\hat\\mathcal{F}(\\hat v, \\hat u, \\hat y, \\hat x) &= \\mathcal{F}(\\hat v, \\hat u, \\hat y - \\alpha\\hat v, \\hat x- \\alpha\\hat u) \\end{aligned}","title":"Aligned Representation"},{"location":"other_categories/Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/#head","text":"\u7f51\u7edc\u91c7\u7528FPN\u8f93\u51fa\u591a\u4e2a\u4e0d\u540c\u5c3a\u5ea6\u7684feature maps,\u5f62\u72b6 (C, \\frac{1}{2^k} H, \\frac{1}{2^k}W","title":"\u7f51\u7edc\u7ed3\u6784, \u8f93\u51faHead, \u7ec6\u8282\u7ed3\u6784, \u8bad\u7ec3\u7ec6\u8282"},{"location":"other_categories/Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/#head_1","text":"\u672c\u6587\u6bd4\u8f83\u4e865\u79cd\u8f93\u51faHead. 4\u79cd\u662fbaseline, \u533a\u522b\u5728\u4e8e\u4e0d\u540cScale\u4e0a\u7684\u56fe\uff0c\u7b2c\u4e94\u4e2ahead\u4f1a\u8f93\u51fa\u76f8\u540c\u7cbe\u786e\u5ea6\u7684\u7f51\u683c \u5176\u4e2d\u7684\u7ec6\u8282\u8fd0\u7b97\u5982\u56fe \u8fd9\u4e9b\u7ec6\u8282\u8fd0\u7b97\u672c\u8d28\u4e0a\u90fd\u662f\u5750\u6807\u53d8\u6362\u4ee5\u53ca\u91c7\u6837","title":"\u8f93\u51faHead"},{"location":"other_categories/Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/#_2","text":"\u5bf9FPN\u7684\u5fae\u8c03 Label\u5206\u914d Fully Contain center of m is close to center of windows unique","title":"\u8bad\u7ec3\u7ec6\u8282"},{"location":"other_categories/Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/","text":"Ultra Fast Structure-aware Deep Lane Detection \u8fd9\u7bc7paper\u4f7f\u7528\u65b0\u7684\u601d\u8def\u8fdb\u884c\u9053\u8def\u7ebf\u7684\u68c0\u6d4b\uff0c\u8fbe\u5230\u4e86300+FPS. \u9053\u8def\u68c0\u6d4b\u95ee\u9898\u5b9a\u4e49 \u672c\u6587\u5bf9\u8def\u7ebf\u68c0\u6d4b\u7684\u5b9a\u4e49\u662f\u6709\u6240\u4e0d\u540c\u7684\uff0c\u5b9a\u4e49\u4e3a\uff0c\"\u57fa\u4e8e\u5168\u5c40\u56fe\u7247\u7279\u5f81\u7684row-based selection\".\u6216\u8005\u8bf4\u5728\u6bcf\u4e00\u4e2a\u9884\u8bbe\u5b9a\u7684\u884c\u4e0a\u9009\u62e9\u884c\u5355\u5143\u3002 \u56fe\u4e2d w \u4e3a\u4e00\u884c\u5185\u5217anchor\u7684\u6570\u91cf\u3002 P_{i, j,:}=f^{i j}(X), \\text { s.t. } i \\in[1, C], j \\in[1, h] \u5176\u4e2d C \u4e3a\u6700\u5927\u9053\u8def\u7ebf\u6570\uff0c h \u4e3arow anchor\u7684\u6570\u91cf\uff0c\u6bcf\u4e00\u4e2alane\u5728\u4e00\u4e2arow-anchor\u4e0a\u53ea\u6709\u4e00\u4e2a\u5206\u7c7b\u7ed3\u679c\uff0c\u6240\u4ee5\u635f\u5931loss\u4e3a\uff1a L_{c l s}=\\sum_{i=1}^{C} \\sum_{j=1}^{h} L_{C E}\\left(P_{i, j,:}, T_{i, j, i}\\right) \u8f85\u52a9\u635f\u5931 \u76f8\u90bb\u884c\u4e4b\u95f4\u9053\u8def\u6709\u8fde\u7eed\u6027: L_{s i m}=\\sum_{i=1}^{C} \\sum_{j=1}^{h-1}\\left\\|P_{i, j,:}-P_{i, j+1,:}\\right\\|_{1} \u76f8\u90bb\u9053\u8def\u659c\u7387\u5177\u6709\u8fde\u7eed\u6027: \\begin{aligned} L_{s h p}=\\sum_{i=1}^{C} \\sum_{j=1}^{h-2} \\| &\\left(\\operatorname{Loc}_{i, j}-\\operatorname{Loc}_{i, j+1}\\right) \\\\ &-\\left(\\operatorname{Loc}_{i, j+1}-\\operatorname{Loc}_{i, j+2}\\right) \\|_{1} \\end{aligned} \u7531\u4e8elocation\u4f4d\u7f6e\u5982\u679c\u4f7f\u7528\u4e00\u884c\u4e0a\u7684argmax\u6765\u53d6\u7684\u8bdd\u4e0d\u53ef\u5bfc\uff0c\u6240\u4ee5\u91c7\u7528 softmax\u5f97\u5230\u4e00\u884c\u6bcf\u4e2a\u70b9\u7684\u6982\u7387\u540e\u52a0\u6743\u5e73\u5747\uff0c\u8fd9\u6837\u635f\u5931\u51fd\u6570\u5c31\u53ef\u5bfc\u4e86\u3002 \u7f51\u7edc\u7ed3\u6784","title":"Ultra Fast Structure-aware Deep Lane Detection"},{"location":"other_categories/Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/#ultra-fast-structure-aware-deep-lane-detection","text":"\u8fd9\u7bc7paper\u4f7f\u7528\u65b0\u7684\u601d\u8def\u8fdb\u884c\u9053\u8def\u7ebf\u7684\u68c0\u6d4b\uff0c\u8fbe\u5230\u4e86300+FPS.","title":"Ultra Fast Structure-aware Deep Lane Detection"},{"location":"other_categories/Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/#_1","text":"\u672c\u6587\u5bf9\u8def\u7ebf\u68c0\u6d4b\u7684\u5b9a\u4e49\u662f\u6709\u6240\u4e0d\u540c\u7684\uff0c\u5b9a\u4e49\u4e3a\uff0c\"\u57fa\u4e8e\u5168\u5c40\u56fe\u7247\u7279\u5f81\u7684row-based selection\".\u6216\u8005\u8bf4\u5728\u6bcf\u4e00\u4e2a\u9884\u8bbe\u5b9a\u7684\u884c\u4e0a\u9009\u62e9\u884c\u5355\u5143\u3002 \u56fe\u4e2d w \u4e3a\u4e00\u884c\u5185\u5217anchor\u7684\u6570\u91cf\u3002 P_{i, j,:}=f^{i j}(X), \\text { s.t. } i \\in[1, C], j \\in[1, h] \u5176\u4e2d C \u4e3a\u6700\u5927\u9053\u8def\u7ebf\u6570\uff0c h \u4e3arow anchor\u7684\u6570\u91cf\uff0c\u6bcf\u4e00\u4e2alane\u5728\u4e00\u4e2arow-anchor\u4e0a\u53ea\u6709\u4e00\u4e2a\u5206\u7c7b\u7ed3\u679c\uff0c\u6240\u4ee5\u635f\u5931loss\u4e3a\uff1a L_{c l s}=\\sum_{i=1}^{C} \\sum_{j=1}^{h} L_{C E}\\left(P_{i, j,:}, T_{i, j, i}\\right)","title":"\u9053\u8def\u68c0\u6d4b\u95ee\u9898\u5b9a\u4e49"},{"location":"other_categories/Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/#_2","text":"\u76f8\u90bb\u884c\u4e4b\u95f4\u9053\u8def\u6709\u8fde\u7eed\u6027: L_{s i m}=\\sum_{i=1}^{C} \\sum_{j=1}^{h-1}\\left\\|P_{i, j,:}-P_{i, j+1,:}\\right\\|_{1} \u76f8\u90bb\u9053\u8def\u659c\u7387\u5177\u6709\u8fde\u7eed\u6027: \\begin{aligned} L_{s h p}=\\sum_{i=1}^{C} \\sum_{j=1}^{h-2} \\| &\\left(\\operatorname{Loc}_{i, j}-\\operatorname{Loc}_{i, j+1}\\right) \\\\ &-\\left(\\operatorname{Loc}_{i, j+1}-\\operatorname{Loc}_{i, j+2}\\right) \\|_{1} \\end{aligned} \u7531\u4e8elocation\u4f4d\u7f6e\u5982\u679c\u4f7f\u7528\u4e00\u884c\u4e0a\u7684argmax\u6765\u53d6\u7684\u8bdd\u4e0d\u53ef\u5bfc\uff0c\u6240\u4ee5\u91c7\u7528 softmax\u5f97\u5230\u4e00\u884c\u6bcf\u4e2a\u70b9\u7684\u6982\u7387\u540e\u52a0\u6743\u5e73\u5747\uff0c\u8fd9\u6837\u635f\u5931\u51fd\u6570\u5c31\u53ef\u5bfc\u4e86\u3002","title":"\u8f85\u52a9\u635f\u5931"},{"location":"other_categories/Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/#_3","text":"","title":"\u7f51\u7edc\u7ed3\u6784"},{"location":"other_categories/Segmentation/YOLACT/","text":"YOLACT Real-time Instance Segmentation \u8fd9\u7bc7\u8bba\u6587\u5c1d\u8bd5\u89e3\u51b3\u7684\u662fone stage instance segmentation\u7684\u95ee\u9898\u3002\u672c\u6587\u6709\u6210\u719f\u7684\u5f00\u6e90\u4ee3\u7801\u4ee5\u53ca\u8f83\u5feb\u7684inference\u901f\u5ea6\uff0c\u503c\u5f97\u4e86\u89e3\u5b66\u4e60\u3002\u672c\u6587\u4e3b\u8981\u7684\u4e24\u4e2a\u8d21\u732e\uff0c\u4e00\u4e2a\u662f\u63d0\u51fa\u4e86YOLOACT\uff0c\u57fa\u4e8eprototype mask\u4ee5\u53cacoefficient\u7684\u89e3\u51b3\u65b9\u5f0f\uff0c\u53e6\u4e00\u4e2a\u662fFastNMS \u603b\u4f53\u6d41\u7a0b \u672c\u6587\u7684backbone\u9009\u62e9\u4e86\u57fa\u4e8eResNet\u7684 Feature Pyramid network,\u8fd9\u91cc\u6709\u4e00\u4e2a\u57fa\u4e8eretina\u7684\u7f51\u7edc pytorch\u5b9e\u73b0 \uff0c\u4e5f\u6709\u4e00\u4e2a \u9b54\u6539\u8fc7\u7684 . Prediction head \u6bcf\u4e00\u4e2ascale\u5305\u542b\u4e09\u4e2a\u90e8\u5206 \u5206\u522b\u4e3a\u5206\u7c7b\u30012D\u6846\u56de\u5f52\u4ee5\u53camask \u53c2\u6570 Protonet \u8fd9\u91cc\u4eceP3\u5f15\u51fa\uff0c\u91c7\u7528\u4e86FCN\u7684\u65b9\u5f0f\uff0c\u4f5c\u8005\u7684\u7406\u5ff5\u662f\uff0cCNN\u53ef\u4ee5\u4ea7\u751f spatially coherent\u7684\u7279\u5f81\uff0c\u800cFully connected layer\u4fbf\u4e8e\u4ea7\u751f\u8bed\u4e49\u7279\u5f81\uff0c\u56e0\u800c\u8ba9FC layers(conv 1*1)\u4ea7\u751fmask\uff0c\u7528FCN\u4ea7\u751fmask \u878d\u5408 M=\\sigma\\left(P C^{T}\\right) \u5176\u4e2d P: h\\times w \\times k, C:n\\times k \uff0cP\u4ee3\u8868k\u4e2aprototype\uff0cC\u4ee3\u8868n\u4e2a\u957f\u5ea6\u4e3ak\u7684nms\u540e\u7684key vector. \u6574\u4e2a\u7f51\u7edc\u7528\u5206\u7c7b\u4e0e\u56de\u5f52loss\u7aef\u5230\u7aef\u8bad\u7ec3 Fast NMS \u601d\u8def\u5c31\u662f\u5141\u8bb8\u88ab\u5220\u9664\u7684bounding box\u53bb\u6291\u5236\u5176\u4ed6\u7f51\u683c\uff0c\u5728\u539f\u7248\u7684nms\u4e2d\uff0cbox\u7684\u6291\u5236\u4e0eiou\u7684\u8ba1\u7b97\u9700\u8981\u4e00\u5b9a\u8fed\u4ee3\u8ba1\u7b97\uff0c\u8fd9\u91cc\u53ef\u4ee5\u76f4\u63a5\u5e76\u884c\u8fd0\u7b97\u5e76\u4e14\u7528 pytorch API\u76f4\u63a5\u5feb\u901f\u5b9e\u73b0 \uff0c\u53ea\u4e0d\u8fc7\u4f1a\u591asuppress\u4e00\u4e9bbbox\u3002","title":"YOLACT Real-time Instance Segmentation"},{"location":"other_categories/Segmentation/YOLACT/#yolact-real-time-instance-segmentation","text":"\u8fd9\u7bc7\u8bba\u6587\u5c1d\u8bd5\u89e3\u51b3\u7684\u662fone stage instance segmentation\u7684\u95ee\u9898\u3002\u672c\u6587\u6709\u6210\u719f\u7684\u5f00\u6e90\u4ee3\u7801\u4ee5\u53ca\u8f83\u5feb\u7684inference\u901f\u5ea6\uff0c\u503c\u5f97\u4e86\u89e3\u5b66\u4e60\u3002\u672c\u6587\u4e3b\u8981\u7684\u4e24\u4e2a\u8d21\u732e\uff0c\u4e00\u4e2a\u662f\u63d0\u51fa\u4e86YOLOACT\uff0c\u57fa\u4e8eprototype mask\u4ee5\u53cacoefficient\u7684\u89e3\u51b3\u65b9\u5f0f\uff0c\u53e6\u4e00\u4e2a\u662fFastNMS","title":"YOLACT Real-time Instance Segmentation"},{"location":"other_categories/Segmentation/YOLACT/#_1","text":"\u672c\u6587\u7684backbone\u9009\u62e9\u4e86\u57fa\u4e8eResNet\u7684 Feature Pyramid network,\u8fd9\u91cc\u6709\u4e00\u4e2a\u57fa\u4e8eretina\u7684\u7f51\u7edc pytorch\u5b9e\u73b0 \uff0c\u4e5f\u6709\u4e00\u4e2a \u9b54\u6539\u8fc7\u7684 .","title":"\u603b\u4f53\u6d41\u7a0b"},{"location":"other_categories/Segmentation/YOLACT/#prediction-head","text":"\u6bcf\u4e00\u4e2ascale\u5305\u542b\u4e09\u4e2a\u90e8\u5206 \u5206\u522b\u4e3a\u5206\u7c7b\u30012D\u6846\u56de\u5f52\u4ee5\u53camask \u53c2\u6570","title":"Prediction head"},{"location":"other_categories/Segmentation/YOLACT/#protonet","text":"\u8fd9\u91cc\u4eceP3\u5f15\u51fa\uff0c\u91c7\u7528\u4e86FCN\u7684\u65b9\u5f0f\uff0c\u4f5c\u8005\u7684\u7406\u5ff5\u662f\uff0cCNN\u53ef\u4ee5\u4ea7\u751f spatially coherent\u7684\u7279\u5f81\uff0c\u800cFully connected layer\u4fbf\u4e8e\u4ea7\u751f\u8bed\u4e49\u7279\u5f81\uff0c\u56e0\u800c\u8ba9FC layers(conv 1*1)\u4ea7\u751fmask\uff0c\u7528FCN\u4ea7\u751fmask","title":"Protonet"},{"location":"other_categories/Segmentation/YOLACT/#_2","text":"M=\\sigma\\left(P C^{T}\\right) \u5176\u4e2d P: h\\times w \\times k, C:n\\times k \uff0cP\u4ee3\u8868k\u4e2aprototype\uff0cC\u4ee3\u8868n\u4e2a\u957f\u5ea6\u4e3ak\u7684nms\u540e\u7684key vector. \u6574\u4e2a\u7f51\u7edc\u7528\u5206\u7c7b\u4e0e\u56de\u5f52loss\u7aef\u5230\u7aef\u8bad\u7ec3","title":"\u878d\u5408"},{"location":"other_categories/Segmentation/YOLACT/#fast-nms","text":"\u601d\u8def\u5c31\u662f\u5141\u8bb8\u88ab\u5220\u9664\u7684bounding box\u53bb\u6291\u5236\u5176\u4ed6\u7f51\u683c\uff0c\u5728\u539f\u7248\u7684nms\u4e2d\uff0cbox\u7684\u6291\u5236\u4e0eiou\u7684\u8ba1\u7b97\u9700\u8981\u4e00\u5b9a\u8fed\u4ee3\u8ba1\u7b97\uff0c\u8fd9\u91cc\u53ef\u4ee5\u76f4\u63a5\u5e76\u884c\u8fd0\u7b97\u5e76\u4e14\u7528 pytorch API\u76f4\u63a5\u5feb\u901f\u5b9e\u73b0 \uff0c\u53ea\u4e0d\u8fc7\u4f1a\u591asuppress\u4e00\u4e9bbbox\u3002","title":"Fast NMS"},{"location":"other_categories/Segmentation/blenderMask/","text":"BlendMask: Top-Down Meets Bottom-Up for Instance Segmentation \u8fd9\u7bc7paper\u4ece YOLOACT \u51fa\u53d1\uff0c\u8fdb\u884c\u4e86\u591a\u4e2a\u63d0\u5347\u7ec6\u8282\u3002 \u603b\u4f53pipeline \u5728 YOLOACT \u4e2d\uff0c\u6240\u8c13\"\u9876\u5c42\"detection\u7ed3\u679c\u662f\u4e00\u7cfb\u52172D detection\u7684\u7ed3\u679c\uff0c\u7531\u4e8e\u9700\u8981\u8f93\u51fa a \u4e2aanchors, \u56e0\u800c\u8f93\u51fachannel\u91cc\u9762\u6709 k\\times a \u4e2achannels\u8d1f\u8d23\u7ed9 a \u4e2aanchors\u5206\u522b\u9884\u6d4b k \u4e2a\u53c2\u6570\uff0c\u8fd9\u4e9b\u53c2\u6570\u7528\u6765\u7ed9 k \u4e2aattention mask\u4f5c\u7ebf\u6027\u7ec4\u5408\u3002\u56e0\u800c\u6bcf\u4e00\u4e2amask\u53ea\u80fd\u6709\u4e00\u4e2a\u5e38\u6570\u3002 \u672c\u6587\u91c7\u7528\u4e86 FCOS \u7684 anchor-free detection\u65b9\u6848\uff0c\u5728 FCOS \u4e2d\uff0c\u6bcf\u4e00\u4e2atower\u53ea\u9700\u8981\u8f93\u51fa6\u4e2achannel\u6765\u5b8c\u62102D detection\uff0c\u800c\u6709\u4e86\u66f4\u591a\u7684channel\u7528\u4e86\u9884\u6d4b\u65b0\u7684\u4fe1\u606f\uff0c\u4f5c\u8005\u8fd9\u91cc\u8bbe\u7f6e K \\times M \\times M \u4e2a\u53c2\u6570\uff0c\u7528\u6765\u4f5cblending\uff0c\u8fd9\u91ccK\u6307\u7684\u5c31\u662fmask\u6570\u91cf\uff0cM\u6307\u7684\u662f\u53c2\u6570\u5206\u8fa8\u7387\u3002 Bottom Module\uff0c \u9009\u62e9DeepLabV3+\u7684decoder.\u8f93\u51fa K \u4e2amask. Blender Module \u4f5c\u8005\u5c06Bottom Module\u7684\u8f93\u51fa\uff0c\u6839\u636e\u9876\u5c42detection\u7ed3\u679c\uff0c\u7528RoiPooling\u53d6\u5f97 K \u4e2a R\\times R \u7684\u7279\u5f81\u56fe\uff0c\u5c06\u9876\u5c42\u8f93\u51fa\u7684 M\\times M \u4e2a\u53c2\u6570interpolate \u5230 R\\times R \uff0c \\mathbf{a}_{d}^{\\prime}=\\text { interpolate }_{M \\times M \\rightarrow R \\times R}\\left(\\mathbf{a}_{d}\\right), \\quad \\forall d \\in\\{1 \\ldots D\\} \\mathbf{s}_{d}=\\operatorname{softmax}\\left(\\mathbf{a}_{d}^{\\prime}\\right), \\quad \\forall d \\in\\{1 \\ldots D\\} \u6700\u540e\u7528\u70b9\u4e58\u4e0e\u7d2f\u52a0\uff0c\u5982\u4e0a\u56fe\u4e00\u6837\u53e0\u52a0\u8d77\u6765 \\mathbf{m}_{d}=\\sum_{k=1}^{K} \\mathbf{s}_{d}^{k} \\circ \\mathbf{r}_{d}^{k}, \\quad \\forall d \\in\\{1 \\ldots D\\}","title":"BlendMask: Top-Down Meets Bottom-Up for Instance Segmentation"},{"location":"other_categories/Segmentation/blenderMask/#blendmask-top-down-meets-bottom-up-for-instance-segmentation","text":"\u8fd9\u7bc7paper\u4ece YOLOACT \u51fa\u53d1\uff0c\u8fdb\u884c\u4e86\u591a\u4e2a\u63d0\u5347\u7ec6\u8282\u3002","title":"BlendMask: Top-Down Meets Bottom-Up for Instance Segmentation"},{"location":"other_categories/Segmentation/blenderMask/#pipeline","text":"\u5728 YOLOACT \u4e2d\uff0c\u6240\u8c13\"\u9876\u5c42\"detection\u7ed3\u679c\u662f\u4e00\u7cfb\u52172D detection\u7684\u7ed3\u679c\uff0c\u7531\u4e8e\u9700\u8981\u8f93\u51fa a \u4e2aanchors, \u56e0\u800c\u8f93\u51fachannel\u91cc\u9762\u6709 k\\times a \u4e2achannels\u8d1f\u8d23\u7ed9 a \u4e2aanchors\u5206\u522b\u9884\u6d4b k \u4e2a\u53c2\u6570\uff0c\u8fd9\u4e9b\u53c2\u6570\u7528\u6765\u7ed9 k \u4e2aattention mask\u4f5c\u7ebf\u6027\u7ec4\u5408\u3002\u56e0\u800c\u6bcf\u4e00\u4e2amask\u53ea\u80fd\u6709\u4e00\u4e2a\u5e38\u6570\u3002 \u672c\u6587\u91c7\u7528\u4e86 FCOS \u7684 anchor-free detection\u65b9\u6848\uff0c\u5728 FCOS \u4e2d\uff0c\u6bcf\u4e00\u4e2atower\u53ea\u9700\u8981\u8f93\u51fa6\u4e2achannel\u6765\u5b8c\u62102D detection\uff0c\u800c\u6709\u4e86\u66f4\u591a\u7684channel\u7528\u4e86\u9884\u6d4b\u65b0\u7684\u4fe1\u606f\uff0c\u4f5c\u8005\u8fd9\u91cc\u8bbe\u7f6e K \\times M \\times M \u4e2a\u53c2\u6570\uff0c\u7528\u6765\u4f5cblending\uff0c\u8fd9\u91ccK\u6307\u7684\u5c31\u662fmask\u6570\u91cf\uff0cM\u6307\u7684\u662f\u53c2\u6570\u5206\u8fa8\u7387\u3002 Bottom Module\uff0c \u9009\u62e9DeepLabV3+\u7684decoder.\u8f93\u51fa K \u4e2amask.","title":"\u603b\u4f53pipeline"},{"location":"other_categories/Segmentation/blenderMask/#blender-module","text":"\u4f5c\u8005\u5c06Bottom Module\u7684\u8f93\u51fa\uff0c\u6839\u636e\u9876\u5c42detection\u7ed3\u679c\uff0c\u7528RoiPooling\u53d6\u5f97 K \u4e2a R\\times R \u7684\u7279\u5f81\u56fe\uff0c\u5c06\u9876\u5c42\u8f93\u51fa\u7684 M\\times M \u4e2a\u53c2\u6570interpolate \u5230 R\\times R \uff0c \\mathbf{a}_{d}^{\\prime}=\\text { interpolate }_{M \\times M \\rightarrow R \\times R}\\left(\\mathbf{a}_{d}\\right), \\quad \\forall d \\in\\{1 \\ldots D\\} \\mathbf{s}_{d}=\\operatorname{softmax}\\left(\\mathbf{a}_{d}^{\\prime}\\right), \\quad \\forall d \\in\\{1 \\ldots D\\} \u6700\u540e\u7528\u70b9\u4e58\u4e0e\u7d2f\u52a0\uff0c\u5982\u4e0a\u56fe\u4e00\u6837\u53e0\u52a0\u8d77\u6765 \\mathbf{m}_{d}=\\sum_{k=1}^{K} \\mathbf{s}_{d}^{k} \\circ \\mathbf{r}_{d}^{k}, \\quad \\forall d \\in\\{1 \\ldots D\\}","title":"Blender Module"},{"location":"other_categories/Segmentation/condInst/","text":"Conditional Convolutions for Instance Segmentation \u8fd9\u7bc7paper\u7ed9\u51fa\u4e86\u4e00\u70b9\u6027\u80fd\u4e0d\u9519\u4e14\u4ee3\u7801\u76f8\u5bf9\u89c4\u8303\u7b80\u6d01\u7684One-Stage Instance Segmentation\u7b97\u6cd5\u3002 \u7f51\u7edc\u7ed3\u6784 \u57fa\u4e8e FCOS \u7684\u68c0\u6d4b\u67b6\u6784\uff0c\u6bcf\u4e00\u4e2ascale\u7684Shared head\u4f1a\u9884\u6d4b\u5206\u7c7b\uff0ccenterness\u7b49\u503c\u3002\u6bcf\u4e00\u4e2a\u88ab\u68c0\u6d4b\u51fa\u6765\u7684\u7269\u4f53\u4f1a\u4f7f\u7528\u8ddf\u968fHead\u4e00\u8d77\u4f30\u8ba1\u7684Convolution filter\uff0c\u5bf9mask\u8fdb\u884c\u878d\u5408. Dynamic Mask Head\u4ee3\u7801","title":"Conditional Convolutions for Instance Segmentation"},{"location":"other_categories/Segmentation/condInst/#conditional-convolutions-for-instance-segmentation","text":"\u8fd9\u7bc7paper\u7ed9\u51fa\u4e86\u4e00\u70b9\u6027\u80fd\u4e0d\u9519\u4e14\u4ee3\u7801\u76f8\u5bf9\u89c4\u8303\u7b80\u6d01\u7684One-Stage Instance Segmentation\u7b97\u6cd5\u3002","title":"Conditional Convolutions for Instance Segmentation"},{"location":"other_categories/Segmentation/condInst/#_1","text":"\u57fa\u4e8e FCOS \u7684\u68c0\u6d4b\u67b6\u6784\uff0c\u6bcf\u4e00\u4e2ascale\u7684Shared head\u4f1a\u9884\u6d4b\u5206\u7c7b\uff0ccenterness\u7b49\u503c\u3002\u6bcf\u4e00\u4e2a\u88ab\u68c0\u6d4b\u51fa\u6765\u7684\u7269\u4f53\u4f1a\u4f7f\u7528\u8ddf\u968fHead\u4e00\u8d77\u4f30\u8ba1\u7684Convolution filter\uff0c\u5bf9mask\u8fdb\u884c\u878d\u5408. Dynamic Mask Head\u4ee3\u7801","title":"\u7f51\u7edc\u7ed3\u6784"},{"location":"other_categories/Segmentation/focus_on_local/","text":"Focus on Local: Detecting Lane Marker from Bottom Up via Key Point \u8fd9\u7bc7paper\u7684\u7279\u70b9\u5728\u4e8e\u5c06object detection\u4ee5\u53capose estimation\u7684\u4e00\u4e9b\u601d\u8def\u5f15\u5165\u4e86\u8f66\u9053\u7ebf\u7684\u68c0\u6d4b\u3002","title":"Focus on Local: Detecting Lane Marker from Bottom Up via Key Point"},{"location":"other_categories/Segmentation/focus_on_local/#focus-on-local-detecting-lane-marker-from-bottom-up-via-key-point","text":"\u8fd9\u7bc7paper\u7684\u7279\u70b9\u5728\u4e8e\u5c06object detection\u4ee5\u53capose estimation\u7684\u4e00\u4e9b\u601d\u8def\u5f15\u5165\u4e86\u8f66\u9053\u7ebf\u7684\u68c0\u6d4b\u3002","title":"Focus on Local: Detecting Lane Marker from Bottom Up via Key Point"},{"location":"other_categories/Segmentation/freeSOLO/","text":"FreeSOLO \u8fd9\u7bc7paper\u8ba8\u8bba\u7684\u4efb\u52a1\u662f\u4f7f\u7528\u5b8c\u5168\u65e0label\u7684\u56fe\u7247\u6570\u636e\u96c6\uff0c\u81ea\u76d1\u7763\u5730\u8fdb\u884cinstance segmentation\u3002\u672c\u6587\u6709\u4e00\u4e2a\u6bd4\u8f83\u6e05\u6670\u7684 \u77e5\u4e4e\u4ecb\u7ecd . \u8fd9\u7bc7paper\u7684\u4e3b\u8981\u601d\u8def\u6765\u6e90\u4e8e\u4e09\u7bc7\u6587\u7ae0\u3002 SOLO DenseCL BoxInst SOLO \u8fd9\u7bc7paper\u5728\u672c\u7ad9\u6709\u6536\u5f55 SOLO . SOLO\u662f\u4e00\u4e2a\u7a20\u5bc6\u9884\u6d4b\u3001\u7a20\u5bc6\u76d1\u7763\u7684instance seg\u7f51\u7edc\uff0c\u5176\u7279\u5f81\u5728\u4e8e\u628a\u56fe\u50cf\u7a7a\u95f4\u5206\u89e3\u4e3a S_x, S_y \u4e2a\u5c0f\u5757\uff0c\u6bcf\u4e00\u4e2a\u5c0f\u5757\u76ee\u6807\u9884\u6d4b\u4e00\u4e2amask\u5e76\u5206\u7c7b\uff0c\u8fd9\u4e00\u90e8\u5206\u6709\u5206\u652fCategory Branch\u9884\u6d4b\u4e00\u4e2a (S_x, S_y, C) \u7684\u77e9\u9635\u5b8c\u6210\uff0c\u53e6\u4e00\u4e2a\u5206\u652fMask Branch\u9884\u6d4b\u4e00\u4e2a (H, W, S_x\\cdot S_y) \u7684\u77e9\u9635\uff0c\u4e3a\u6bcf\u4e00\u4e2a\u5c0f\u5757\u9884\u6d4b (H,W) \u7684\u5168\u5c40mask. \u540e\u6765\u4f5c\u8005\u63d0\u51fa\u4e86\u5f88\u591a\u6539\u8fdb\u7684\u70b9\uff0c\u4e3b\u8981\u96c6\u4e2d\u5728\u5904\u7406Mask Branch\u7684\u7a00\u758f\u6027\uff0c DenseCL pdf \u77e5\u4e4e DenseCL\u662f\u57fa\u4e8eMoCo\u7684\uff0c\u89e3\u51b3\u7684\u662f\u4e3a\u4e86\u7a20\u5bc6\u4f30\u8ba1\u7684\u89c6\u89c9\u9884\u8bad\u7ec3\u3002MoCo\u7684\u505a\u6cd5\u662f\u628a\u6574\u4e2a\u56fe\u7247\u538b\u7f29\u6210\u4e00\u4e2a\u5355\u4e00\u7279\u5f81\uff1bcontrastive loss\u4e2d\u6b63\u6837\u672c\u7684\u6765\u6e90\u662f\u540c\u4e00\u5f20\u56fe\u4e0d\u540c\u7684\u6570\u636e\u589e\u5f3a\u540e\u63d0\u53d6\u7684\u7279\u5f81\uff0c\u8d1f\u6837\u672c\u7684\u6765\u6e90\u5219\u662f\u4e0d\u540c\u56fe\u7247\u63d0\u53d6\u7684\u7279\u5f81\u3002 DenseCL\u9488\u5bf9\u7684\u5219\u662f\u7a20\u5bc6\u9884\u6d4b\uff0c\u4f7f\u7528\u7eafCNN\u7f51\u8def\uff0c\u8f93\u51fa\u4e00\u4e2a\u7a20\u5bc6\u7684 (H', W', C) \u7684\u7279\u5f81\u56fe\u3002contrastive loss\u4e2d\u7684\u8d1f\u6837\u672c\u540c\u6765\u662f\u4e0d\u540c\u56fe\u7247\u63d0\u53d6\u7684\u7279\u5f81\uff0c\u4f46\u662f\u6b63\u6837\u672c\u4e0d\u53ea\u662f\u4e0d\u540c\u6570\u636e\u589e\u5f3a\u540e\u7684\u7279\u5f81\u5168\u4f53\uff0c\u4f5c\u8005\u63d0\u51fa\u7684\u65b9\u6848\u662f\u8ba9\u540c\u4e00\u5f20\u56fe\u4e0d\u540c\u589e\u5f3a\u7ed3\u679c x^{q1},x^{q2} \u5206\u522b\u8f93\u51fa\u4e24\u7ec4 S \\cdot S \u7a20\u5bc6\u7279\u5f81\u3002 \u7136\u540e\u8ba1\u7b97\u5b83\u4eec\u7684\u76f8\u4f3c\u5ea6\u77e9\u9635 S^2 \\times S^2 \uff0c\u628a\u6bcf\u4e00\u884c\u548c\u6bcf\u4e00\u5217\u76f8\u4f3c\u5ea6\u6700\u5927\u7684\u90a3\u4e2a\u503c\u5bf9\u5e94\u7684\u7279\u5f81\u4f5c\u4e3a\u6b63\u6837\u672c\u5bf9\u3002 BoxInst pdf code \u8fd9\u7bc7paper\u89e3\u51b3\u7684\u95ee\u9898\u662f\u901a\u8fc72D bounding boxes\u7684\u6570\u636e\u5f31\u76d1\u7763\uff0c\u8f93\u51fainstance segmentation. BoxInst\u8bbe\u8ba1\u7684\u4e24\u4e2a\u635f\u5931\u3002 \u7b2c\u4e00\u4e2a\u662f\u8981\u6c42bounding box\u7684\u8fb9\u7f18\u548cmask\u7684\u8fb9\u7f18\u8981\u4e00\u81f4\u3002\u8fd9\u91cc\u7684\u505a\u6cd5\u8ba1\u7b97mask\u5728 x/y\u8f74\u4e0a\u7684max\u6295\u5f71\uff0c\u548cbounding box\u7684\u8fb9\u7f18\u76f4\u63a5\u8ba1\u7b97loss. \u8fd9\u4e2a\u635f\u5931\u7528\u4e8e\u63a7\u5236mask\u7684\u8fb9\u754c\u3002 \u7b2c\u4e8c\u4e2a\u662f\u5219\u662f\u5229\u7528\u989c\u8272\u76f8\u8fd1\u7684\u70b9\u5e94\u5f53\u6709\u76f8\u540c\u7684\u7279\u5f81\uff0c\u4e24\u4e2a\u70b9\u6839\u636e\u9884\u6d4b\u7ed3\u679c\uff0c\u4ed6\u4eec\u5728\u540c\u4e00\u7c7b\u7684\u6982\u7387\u4e3a P(y_e = 1) = m_{i,j} \\cdot m_{k,l} + (1 - m_{i,j}) \\cdot (1 - m_{k,l}) \u6846\u5916\u7684\u4e00\u5b9a\u4e0d\u662f\u76ee\u6807\u70b9\u3002\u6846\u5185\u989c\u8272\u5dee\u5728\u9608\u503c\u5185\u7684\u4e24\u4e2a\u50cf\u7d20\u8981\u4e48\u540c\u65f6\u662fmask\uff0c\u8981\u4e48\u540c\u65f6\u4e0d\u662fmask. \u4ece\u800c\u5f31\u76d1\u7763\u5730\u627e\u51famask. Free SOLO\u672c\u6587\u5219\u7531Free Mask \u548c self-supervised SOLO\u7ec4\u6210\u3002 Free Mask\u91c7\u53d6\u4e0eSOLO\u76f8\u5f53\u7edf\u4e00\u7684\u64cd\u4f5c\u3002 \u4ece\u56fe\u7247\u4e2d\u63d0\u53d6\u7279\u5f81 I , \u628a I \u4e0b\u91c7\u6837 1/2, 1/4, 1/8 \u5230 Q:(H' \\times W' \\times E) , \u8fd9\u4e2aQ\u4e0eK\u8ba1\u7b97\u76f8\u4f3c\u6027\u77e9\u9635 H\\times W \\times (H' \\times W') \u7c7b\u4f3c\u4e8eSOLO\u4e2d\u7684mask branch. \u8fd9\u91cc\u91c7\u7528\u7684backbone\u5219\u662f\u7ecf\u8fc7DenseCL\u8bad\u7ec3\u7684backbone\uff0cDenseCL backbone\u6b63\u662f\u7528\u76f8\u4f3c\u6027\u77e9\u9635\u9009\u6b63\u6837\u672c\u7684\uff0c\u5728\u8fd9\u65b9\u9762\u7684\u5c5e\u6027\u6bd4\u8f83\u597d\uff0c\u53ef\u4ee5\u5f97\u5230\u6bd4\u8f83\u9ad8\u7684\u9884\u8bad\u7ec3\u8d28\u91cf\u3002 \u8f93\u51facourase Mask label\u4e4b\u540e\u8bad\u7ec3\u4e00\u4e2aSOLO.\u91c7\u7528\u7684\u662fBoxInst\u7684\u635f\u5931\uff0c\u800c\u6295\u5f71\u635f\u5931\u6539\u4e3aAvg\u800c\u4e0d\u662fMean.","title":"FreeSOLO"},{"location":"other_categories/Segmentation/freeSOLO/#freesolo","text":"\u8fd9\u7bc7paper\u8ba8\u8bba\u7684\u4efb\u52a1\u662f\u4f7f\u7528\u5b8c\u5168\u65e0label\u7684\u56fe\u7247\u6570\u636e\u96c6\uff0c\u81ea\u76d1\u7763\u5730\u8fdb\u884cinstance segmentation\u3002\u672c\u6587\u6709\u4e00\u4e2a\u6bd4\u8f83\u6e05\u6670\u7684 \u77e5\u4e4e\u4ecb\u7ecd . \u8fd9\u7bc7paper\u7684\u4e3b\u8981\u601d\u8def\u6765\u6e90\u4e8e\u4e09\u7bc7\u6587\u7ae0\u3002 SOLO DenseCL BoxInst","title":"FreeSOLO"},{"location":"other_categories/Segmentation/freeSOLO/#solo","text":"\u8fd9\u7bc7paper\u5728\u672c\u7ad9\u6709\u6536\u5f55 SOLO . SOLO\u662f\u4e00\u4e2a\u7a20\u5bc6\u9884\u6d4b\u3001\u7a20\u5bc6\u76d1\u7763\u7684instance seg\u7f51\u7edc\uff0c\u5176\u7279\u5f81\u5728\u4e8e\u628a\u56fe\u50cf\u7a7a\u95f4\u5206\u89e3\u4e3a S_x, S_y \u4e2a\u5c0f\u5757\uff0c\u6bcf\u4e00\u4e2a\u5c0f\u5757\u76ee\u6807\u9884\u6d4b\u4e00\u4e2amask\u5e76\u5206\u7c7b\uff0c\u8fd9\u4e00\u90e8\u5206\u6709\u5206\u652fCategory Branch\u9884\u6d4b\u4e00\u4e2a (S_x, S_y, C) \u7684\u77e9\u9635\u5b8c\u6210\uff0c\u53e6\u4e00\u4e2a\u5206\u652fMask Branch\u9884\u6d4b\u4e00\u4e2a (H, W, S_x\\cdot S_y) \u7684\u77e9\u9635\uff0c\u4e3a\u6bcf\u4e00\u4e2a\u5c0f\u5757\u9884\u6d4b (H,W) \u7684\u5168\u5c40mask. \u540e\u6765\u4f5c\u8005\u63d0\u51fa\u4e86\u5f88\u591a\u6539\u8fdb\u7684\u70b9\uff0c\u4e3b\u8981\u96c6\u4e2d\u5728\u5904\u7406Mask Branch\u7684\u7a00\u758f\u6027\uff0c","title":"SOLO"},{"location":"other_categories/Segmentation/freeSOLO/#densecl","text":"pdf \u77e5\u4e4e DenseCL\u662f\u57fa\u4e8eMoCo\u7684\uff0c\u89e3\u51b3\u7684\u662f\u4e3a\u4e86\u7a20\u5bc6\u4f30\u8ba1\u7684\u89c6\u89c9\u9884\u8bad\u7ec3\u3002MoCo\u7684\u505a\u6cd5\u662f\u628a\u6574\u4e2a\u56fe\u7247\u538b\u7f29\u6210\u4e00\u4e2a\u5355\u4e00\u7279\u5f81\uff1bcontrastive loss\u4e2d\u6b63\u6837\u672c\u7684\u6765\u6e90\u662f\u540c\u4e00\u5f20\u56fe\u4e0d\u540c\u7684\u6570\u636e\u589e\u5f3a\u540e\u63d0\u53d6\u7684\u7279\u5f81\uff0c\u8d1f\u6837\u672c\u7684\u6765\u6e90\u5219\u662f\u4e0d\u540c\u56fe\u7247\u63d0\u53d6\u7684\u7279\u5f81\u3002 DenseCL\u9488\u5bf9\u7684\u5219\u662f\u7a20\u5bc6\u9884\u6d4b\uff0c\u4f7f\u7528\u7eafCNN\u7f51\u8def\uff0c\u8f93\u51fa\u4e00\u4e2a\u7a20\u5bc6\u7684 (H', W', C) \u7684\u7279\u5f81\u56fe\u3002contrastive loss\u4e2d\u7684\u8d1f\u6837\u672c\u540c\u6765\u662f\u4e0d\u540c\u56fe\u7247\u63d0\u53d6\u7684\u7279\u5f81\uff0c\u4f46\u662f\u6b63\u6837\u672c\u4e0d\u53ea\u662f\u4e0d\u540c\u6570\u636e\u589e\u5f3a\u540e\u7684\u7279\u5f81\u5168\u4f53\uff0c\u4f5c\u8005\u63d0\u51fa\u7684\u65b9\u6848\u662f\u8ba9\u540c\u4e00\u5f20\u56fe\u4e0d\u540c\u589e\u5f3a\u7ed3\u679c x^{q1},x^{q2} \u5206\u522b\u8f93\u51fa\u4e24\u7ec4 S \\cdot S \u7a20\u5bc6\u7279\u5f81\u3002 \u7136\u540e\u8ba1\u7b97\u5b83\u4eec\u7684\u76f8\u4f3c\u5ea6\u77e9\u9635 S^2 \\times S^2 \uff0c\u628a\u6bcf\u4e00\u884c\u548c\u6bcf\u4e00\u5217\u76f8\u4f3c\u5ea6\u6700\u5927\u7684\u90a3\u4e2a\u503c\u5bf9\u5e94\u7684\u7279\u5f81\u4f5c\u4e3a\u6b63\u6837\u672c\u5bf9\u3002","title":"DenseCL"},{"location":"other_categories/Segmentation/freeSOLO/#boxinst","text":"pdf code \u8fd9\u7bc7paper\u89e3\u51b3\u7684\u95ee\u9898\u662f\u901a\u8fc72D bounding boxes\u7684\u6570\u636e\u5f31\u76d1\u7763\uff0c\u8f93\u51fainstance segmentation. BoxInst\u8bbe\u8ba1\u7684\u4e24\u4e2a\u635f\u5931\u3002 \u7b2c\u4e00\u4e2a\u662f\u8981\u6c42bounding box\u7684\u8fb9\u7f18\u548cmask\u7684\u8fb9\u7f18\u8981\u4e00\u81f4\u3002\u8fd9\u91cc\u7684\u505a\u6cd5\u8ba1\u7b97mask\u5728 x/y\u8f74\u4e0a\u7684max\u6295\u5f71\uff0c\u548cbounding box\u7684\u8fb9\u7f18\u76f4\u63a5\u8ba1\u7b97loss. \u8fd9\u4e2a\u635f\u5931\u7528\u4e8e\u63a7\u5236mask\u7684\u8fb9\u754c\u3002 \u7b2c\u4e8c\u4e2a\u662f\u5219\u662f\u5229\u7528\u989c\u8272\u76f8\u8fd1\u7684\u70b9\u5e94\u5f53\u6709\u76f8\u540c\u7684\u7279\u5f81\uff0c\u4e24\u4e2a\u70b9\u6839\u636e\u9884\u6d4b\u7ed3\u679c\uff0c\u4ed6\u4eec\u5728\u540c\u4e00\u7c7b\u7684\u6982\u7387\u4e3a P(y_e = 1) = m_{i,j} \\cdot m_{k,l} + (1 - m_{i,j}) \\cdot (1 - m_{k,l}) \u6846\u5916\u7684\u4e00\u5b9a\u4e0d\u662f\u76ee\u6807\u70b9\u3002\u6846\u5185\u989c\u8272\u5dee\u5728\u9608\u503c\u5185\u7684\u4e24\u4e2a\u50cf\u7d20\u8981\u4e48\u540c\u65f6\u662fmask\uff0c\u8981\u4e48\u540c\u65f6\u4e0d\u662fmask. \u4ece\u800c\u5f31\u76d1\u7763\u5730\u627e\u51famask. Free SOLO\u672c\u6587\u5219\u7531Free Mask \u548c self-supervised SOLO\u7ec4\u6210\u3002 Free Mask\u91c7\u53d6\u4e0eSOLO\u76f8\u5f53\u7edf\u4e00\u7684\u64cd\u4f5c\u3002 \u4ece\u56fe\u7247\u4e2d\u63d0\u53d6\u7279\u5f81 I , \u628a I \u4e0b\u91c7\u6837 1/2, 1/4, 1/8 \u5230 Q:(H' \\times W' \\times E) , \u8fd9\u4e2aQ\u4e0eK\u8ba1\u7b97\u76f8\u4f3c\u6027\u77e9\u9635 H\\times W \\times (H' \\times W') \u7c7b\u4f3c\u4e8eSOLO\u4e2d\u7684mask branch. \u8fd9\u91cc\u91c7\u7528\u7684backbone\u5219\u662f\u7ecf\u8fc7DenseCL\u8bad\u7ec3\u7684backbone\uff0cDenseCL backbone\u6b63\u662f\u7528\u76f8\u4f3c\u6027\u77e9\u9635\u9009\u6b63\u6837\u672c\u7684\uff0c\u5728\u8fd9\u65b9\u9762\u7684\u5c5e\u6027\u6bd4\u8f83\u597d\uff0c\u53ef\u4ee5\u5f97\u5230\u6bd4\u8f83\u9ad8\u7684\u9884\u8bad\u7ec3\u8d28\u91cf\u3002 \u8f93\u51facourase Mask label\u4e4b\u540e\u8bad\u7ec3\u4e00\u4e2aSOLO.\u91c7\u7528\u7684\u662fBoxInst\u7684\u635f\u5931\uff0c\u800c\u6295\u5f71\u635f\u5931\u6539\u4e3aAvg\u800c\u4e0d\u662fMean.","title":"BoxInst"},{"location":"other_categories/Segmentation/hdmapnet/","text":"HDMapNet: An Online HD Map Construction and Evaluation Framework \u8fd9\u7bc7\u8bba\u6587\u5c1d\u8bd5\u89e3\u51b3\u7684\u662f\u5c40\u90e8HDMap\u7684\u6784\u5efa\u95ee\u9898.\u8f93\u5165\u662f\u73af\u7ed5\u7684\u516d\u4e2a(\u591a\u4e2a)\u6444\u50cf\u5934,\u8f93\u51fa\u662fBEV\u4e2d\u77e2\u91cf\u5316\u7684\u9ad8\u7cbe\u5730\u56fe. \u8bba\u6587\u4e0e\u4ee3\u7801\u4e2d\u5404\u4e2a\u90e8\u5206\u7684\u505a\u6cd5: PV image encoder \u91c7\u7528\u540c\u4e00\u4e2a efficientnet backbone Neural View Transformer \u9996\u5148\u7528\u4e00\u4e2a\u4e0d\u540c\u6444\u50cf\u5934\u4e4b\u95f4\u4e0d\u5171\u4eab\u7684\u5168\u8fde\u63a5\u7f51\u7edc\u5c06FV\u6620\u5c04\u5230\u6bcf\u4e2a\u76f8\u673a\u7684\u524d\u9762\u533a\u57df\u7684\u5c0fBEV\u4e2d.\u7136\u540e\u6839\u636e\u76f8\u673a\u7684\u5916\u53c2\u5c06\u76f8\u673a\u524d\u7684\u5c0fBEV\u533a\u57df\u7684\u7279\u5f81,\u901a\u8fc7\u65cb\u8f6c\u548c\u5e73\u79fb\u8f6c\u79fb\u5230\u4ee5\u8f66\u8eab\u4e3a\u4e2d\u5fc3\u7684\u5927BEV\u4e0a. \u5bf9\u4e0d\u540c\u6444\u50cf\u673a\u5728\u5927BEV\u4e0a\u7684\u7279\u5f81\u56fe\u53d6maxpooling. \u6fc0\u5149\u96f7\u8fbe\u6570\u636e\u91c7\u7528PointPillar \u7f51\u7edc\u63d0\u53d6\u7279\u5f81\u5e76\u538b\u7f29\u5230BEV\u4e0a.\u4f7f\u7528torch_scatter\u540e\u672c\u6587\u7684\u4ee3\u7801\u66f4\u4e3a\u5e72\u51c0. instance embedding\u5bf9\u6bcf\u4e00\u7ec4\u8f66\u9053\u7ebf\u50cf\u7d20\u4f4d\u7f6e\u4e0a\u7684\u7279\u5f81\u8fdb\u884c\u8ba1\u7b97.\u8ba1\u7b97\u6bcf\u4e00\u6761\u8f66\u9053\u7ebf\u7684\u7279\u5f81\u5747\u503c,\u635f\u5931\u5219\u662f\u81ea\u76d1\u7763\u7684\u5bf9\u6297\u6027\u635f\u5931. direction\u6307\u7684\u662f\u8f66\u9053\u7ebf\u5728\u5404\u4e2a\u70b9\u4e0a\u7684\u65b9\u5411,\u7531\u4e8e\u524d\u8fdb\u4e0e\u540e\u9000\u4e0d\u533a\u5206,\u6240\u4ee5\u6bcf\u4e2a\u70b9\u9884\u6d4b\u7684\u90fd\u662f\u4e00\u4e2a N_d \u7684\u77e2\u91cf,\u800c\u76d1\u7763\u4ed6\u4eec\u7684\u662f\u4e00\u4e2a\u6709\u4e24\u4e2a 1 label\u7684\u77e2\u91cf.","title":"HDMapNet: An Online HD Map Construction and Evaluation Framework"},{"location":"other_categories/Segmentation/hdmapnet/#hdmapnet-an-online-hd-map-construction-and-evaluation-framework","text":"\u8fd9\u7bc7\u8bba\u6587\u5c1d\u8bd5\u89e3\u51b3\u7684\u662f\u5c40\u90e8HDMap\u7684\u6784\u5efa\u95ee\u9898.\u8f93\u5165\u662f\u73af\u7ed5\u7684\u516d\u4e2a(\u591a\u4e2a)\u6444\u50cf\u5934,\u8f93\u51fa\u662fBEV\u4e2d\u77e2\u91cf\u5316\u7684\u9ad8\u7cbe\u5730\u56fe. \u8bba\u6587\u4e0e\u4ee3\u7801\u4e2d\u5404\u4e2a\u90e8\u5206\u7684\u505a\u6cd5: PV image encoder \u91c7\u7528\u540c\u4e00\u4e2a efficientnet backbone Neural View Transformer \u9996\u5148\u7528\u4e00\u4e2a\u4e0d\u540c\u6444\u50cf\u5934\u4e4b\u95f4\u4e0d\u5171\u4eab\u7684\u5168\u8fde\u63a5\u7f51\u7edc\u5c06FV\u6620\u5c04\u5230\u6bcf\u4e2a\u76f8\u673a\u7684\u524d\u9762\u533a\u57df\u7684\u5c0fBEV\u4e2d.\u7136\u540e\u6839\u636e\u76f8\u673a\u7684\u5916\u53c2\u5c06\u76f8\u673a\u524d\u7684\u5c0fBEV\u533a\u57df\u7684\u7279\u5f81,\u901a\u8fc7\u65cb\u8f6c\u548c\u5e73\u79fb\u8f6c\u79fb\u5230\u4ee5\u8f66\u8eab\u4e3a\u4e2d\u5fc3\u7684\u5927BEV\u4e0a. \u5bf9\u4e0d\u540c\u6444\u50cf\u673a\u5728\u5927BEV\u4e0a\u7684\u7279\u5f81\u56fe\u53d6maxpooling. \u6fc0\u5149\u96f7\u8fbe\u6570\u636e\u91c7\u7528PointPillar \u7f51\u7edc\u63d0\u53d6\u7279\u5f81\u5e76\u538b\u7f29\u5230BEV\u4e0a.\u4f7f\u7528torch_scatter\u540e\u672c\u6587\u7684\u4ee3\u7801\u66f4\u4e3a\u5e72\u51c0. instance embedding\u5bf9\u6bcf\u4e00\u7ec4\u8f66\u9053\u7ebf\u50cf\u7d20\u4f4d\u7f6e\u4e0a\u7684\u7279\u5f81\u8fdb\u884c\u8ba1\u7b97.\u8ba1\u7b97\u6bcf\u4e00\u6761\u8f66\u9053\u7ebf\u7684\u7279\u5f81\u5747\u503c,\u635f\u5931\u5219\u662f\u81ea\u76d1\u7763\u7684\u5bf9\u6297\u6027\u635f\u5931. direction\u6307\u7684\u662f\u8f66\u9053\u7ebf\u5728\u5404\u4e2a\u70b9\u4e0a\u7684\u65b9\u5411,\u7531\u4e8e\u524d\u8fdb\u4e0e\u540e\u9000\u4e0d\u533a\u5206,\u6240\u4ee5\u6bcf\u4e2a\u70b9\u9884\u6d4b\u7684\u90fd\u662f\u4e00\u4e2a N_d \u7684\u77e2\u91cf,\u800c\u76d1\u7763\u4ed6\u4eec\u7684\u662f\u4e00\u4e2a\u6709\u4e24\u4e2a 1 label\u7684\u77e2\u91cf.","title":"HDMapNet: An Online HD Map Construction and Evaluation Framework"},{"location":"other_categories/Segmentation/lift_splat_shoot/","text":"Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D \u8fd9\u7bc7paper\u7684 \u5b98\u65b9\u7f51\u7ad9 , youtube \u89c6\u9891,\u89c6\u9891\u4e2d\u63d0\u4f9b\u4e86\u4e0d\u5c11paper\u91cc\u9762\u6ca1\u6709\u63d0\u53ca\u7684motivation\u4ee5\u53ca\u66f4\u5177\u4f53\u7684\u7b97\u6cd5: \u8fd9\u7bc7paper\u5b8c\u6210\u7684\u4efb\u52a1\u662f\u57fa\u4e8e\u73af\u89c6\u6444\u50cf\u5934\u7684\u8bed\u4e49\u5206\u5272+\u6295\u5f71\u5230\u5730\u9762+\u8def\u5f84\u89c4\u5212\u3002 \u7ed3\u6784 \u4e3b\u8981\u601d\u8def \u5bf9\u6bcf\u4e00\u5f20\u56fe\u7247\u4f7f\u7528CNN\u63d0\u53d6\u7279\u5f81 \u7528lift \u5c06\u56fe\u7247\u7279\u5f81\u63d0\u5347\u4e3a\u70b9\u4e91 splat \u7ed3\u5408\u4e86point-pillar\u4ee5\u53ca OFT \u7684\u601d\u8def\u5c06\u591a\u56fe\u7684\u70b9\u4e91\u878d\u5408\u5230BEV\u4e0a shoot \u8fdb\u884c\u7aef\u5230\u7aefcost-map learning\u4ee5\u53ca\u8def\u5f84\u89c4\u5212\u3002 Lift \u8f93\u51fa\u7684\u7ed3\u679c\u4e3a D\\cdot H \\cdot W \u7684\u77e9\u9635\uff0c\u4e14\u6df1\u5ea6\u4e5f\u6709\u4e00\u4e2a\u9884\u6d4b\uff0c\u53ef\u4ee5\u7406\u89e3\u4e3aOFT\u8f6c\u6362\u4e2d\u5728\u6df1\u5ea6\u65b9\u5411\u6709\u4e00\u4e2a\u52a0\u6743\u3002 Splat \u8fd9\u91cc\u91c7\u7528\u7684\u662f\u7c7b\u4f3c\u4e8e point-pillar\u7684\u65b9\u6848\uff0c\u5728\u6bcf\u4e00\u4e2a\u5e73\u9762voxel\u4e0a\u7684\u6240\u6709feature\u4f1a\u88ab\u878d\u5408\u6210\u5f53\u524d\u5355\u5143\u7684\u7279\u5f81\u3002\u8fd9\u91cc\u63d0\u5230\u4e86\u4e00\u4e9b\u6a21\u4effOFT\u7684coding \u6280\u5de7(\u672c\u4eba\u6ca1\u80fd\u770b\u61c2\uff0c\u7b49\u5f85\u5176code) Shoot: Motion Planning \u5f80\u524d\u91c7\u6837K\u6761\u8f68\u8ff9,\u6839\u636e costmap\u4ee5\u53ca Neural Motion Planner \u7684\u505a\u6cd5\u5b9e\u73b0\u3002\u662f\u4e00\u4e2a\u6a21\u4eff\u5b66\u4e60\u7684\u601d\u8def\u3002","title":"Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D"},{"location":"other_categories/Segmentation/lift_splat_shoot/#lift-splat-shoot-encoding-images-from-arbitrary-camera-rigs-by-implicitly-unprojecting-to-3d","text":"\u8fd9\u7bc7paper\u7684 \u5b98\u65b9\u7f51\u7ad9 , youtube \u89c6\u9891,\u89c6\u9891\u4e2d\u63d0\u4f9b\u4e86\u4e0d\u5c11paper\u91cc\u9762\u6ca1\u6709\u63d0\u53ca\u7684motivation\u4ee5\u53ca\u66f4\u5177\u4f53\u7684\u7b97\u6cd5: \u8fd9\u7bc7paper\u5b8c\u6210\u7684\u4efb\u52a1\u662f\u57fa\u4e8e\u73af\u89c6\u6444\u50cf\u5934\u7684\u8bed\u4e49\u5206\u5272+\u6295\u5f71\u5230\u5730\u9762+\u8def\u5f84\u89c4\u5212\u3002","title":"Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D"},{"location":"other_categories/Segmentation/lift_splat_shoot/#_1","text":"\u4e3b\u8981\u601d\u8def \u5bf9\u6bcf\u4e00\u5f20\u56fe\u7247\u4f7f\u7528CNN\u63d0\u53d6\u7279\u5f81 \u7528lift \u5c06\u56fe\u7247\u7279\u5f81\u63d0\u5347\u4e3a\u70b9\u4e91 splat \u7ed3\u5408\u4e86point-pillar\u4ee5\u53ca OFT \u7684\u601d\u8def\u5c06\u591a\u56fe\u7684\u70b9\u4e91\u878d\u5408\u5230BEV\u4e0a shoot \u8fdb\u884c\u7aef\u5230\u7aefcost-map learning\u4ee5\u53ca\u8def\u5f84\u89c4\u5212\u3002","title":"\u7ed3\u6784"},{"location":"other_categories/Segmentation/lift_splat_shoot/#lift","text":"\u8f93\u51fa\u7684\u7ed3\u679c\u4e3a D\\cdot H \\cdot W \u7684\u77e9\u9635\uff0c\u4e14\u6df1\u5ea6\u4e5f\u6709\u4e00\u4e2a\u9884\u6d4b\uff0c\u53ef\u4ee5\u7406\u89e3\u4e3aOFT\u8f6c\u6362\u4e2d\u5728\u6df1\u5ea6\u65b9\u5411\u6709\u4e00\u4e2a\u52a0\u6743\u3002","title":"Lift"},{"location":"other_categories/Segmentation/lift_splat_shoot/#splat","text":"\u8fd9\u91cc\u91c7\u7528\u7684\u662f\u7c7b\u4f3c\u4e8e point-pillar\u7684\u65b9\u6848\uff0c\u5728\u6bcf\u4e00\u4e2a\u5e73\u9762voxel\u4e0a\u7684\u6240\u6709feature\u4f1a\u88ab\u878d\u5408\u6210\u5f53\u524d\u5355\u5143\u7684\u7279\u5f81\u3002\u8fd9\u91cc\u63d0\u5230\u4e86\u4e00\u4e9b\u6a21\u4effOFT\u7684coding \u6280\u5de7(\u672c\u4eba\u6ca1\u80fd\u770b\u61c2\uff0c\u7b49\u5f85\u5176code)","title":"Splat"},{"location":"other_categories/Segmentation/lift_splat_shoot/#shoot-motion-planning","text":"\u5f80\u524d\u91c7\u6837K\u6761\u8f68\u8ff9,\u6839\u636e costmap\u4ee5\u53ca Neural Motion Planner \u7684\u505a\u6cd5\u5b9e\u73b0\u3002\u662f\u4e00\u4e2a\u6a21\u4eff\u5b66\u4e60\u7684\u601d\u8def\u3002","title":"Shoot: Motion Planning"},{"location":"other_categories/Segmentation/mmsegmentation/","text":"Some Collections Around MMSegmentation \u672c\u6587\u6536\u96c6\u4e00\u7cfb\u5217 MMSegmentation \u6536\u5f55\u7684\u7f51\u7edc\u7ed3\u6784. \u4ee5Res50-D8 512 \\times 1024 \u5728cityscapes\u4e0a\u7684\u6027\u80fd Methods mIoU FPS DeepLabV3+ 79.61 3.94 DNLNet 78.61 2.56 NonLocal 78.24 2.72 PSPNet 77.85 4.07 GCNet 77.69 3.93 ANN 77.40 3.71 OCRNet 74.30 10.45 FCN 72.25 4.17 Pyramid Scene Parsing Network (PSPNetwork) pdf code Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation pdf code GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond pdf code Asymmetric Non-local Neural Networks for Semantic Segmentation (ANN) pdf code Object-Contextual Representations for Semantic Segmentation pdf code Disentangled Non-Local Neural Networks (DNL) pdf code \u56fe\u4e2dwhiten\u7684\u64cd\u4f5c\u6307\u7684\u662f\u51cf\u53bb\u5173\u4e8echannel\u901a\u9053\u7684\u5747\u503c.","title":"Some Collections Around MMSegmentation"},{"location":"other_categories/Segmentation/mmsegmentation/#some-collections-around-mmsegmentation","text":"\u672c\u6587\u6536\u96c6\u4e00\u7cfb\u5217 MMSegmentation \u6536\u5f55\u7684\u7f51\u7edc\u7ed3\u6784. \u4ee5Res50-D8 512 \\times 1024 \u5728cityscapes\u4e0a\u7684\u6027\u80fd Methods mIoU FPS DeepLabV3+ 79.61 3.94 DNLNet 78.61 2.56 NonLocal 78.24 2.72 PSPNet 77.85 4.07 GCNet 77.69 3.93 ANN 77.40 3.71 OCRNet 74.30 10.45 FCN 72.25 4.17","title":"Some Collections Around MMSegmentation"},{"location":"other_categories/Segmentation/mmsegmentation/#pyramid-scene-parsing-network-pspnetwork","text":"pdf code","title":"Pyramid Scene Parsing Network (PSPNetwork)"},{"location":"other_categories/Segmentation/mmsegmentation/#encoder-decoder-with-atrous-separable-convolution-for-semantic-image-segmentation","text":"pdf code","title":"Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation"},{"location":"other_categories/Segmentation/mmsegmentation/#gcnet-non-local-networks-meet-squeeze-excitation-networks-and-beyond","text":"pdf code","title":"GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond"},{"location":"other_categories/Segmentation/mmsegmentation/#asymmetric-non-local-neural-networks-for-semantic-segmentation-ann","text":"pdf code","title":"Asymmetric Non-local Neural Networks for Semantic Segmentation (ANN)"},{"location":"other_categories/Segmentation/mmsegmentation/#object-contextual-representations-for-semantic-segmentation","text":"pdf code","title":"Object-Contextual Representations for Semantic Segmentation"},{"location":"other_categories/Segmentation/mmsegmentation/#disentangled-non-local-neural-networks-dnl","text":"pdf code \u56fe\u4e2dwhiten\u7684\u64cd\u4f5c\u6307\u7684\u662f\u51cf\u53bb\u5173\u4e8echannel\u901a\u9053\u7684\u5747\u503c.","title":"Disentangled Non-Local Neural Networks (DNL)"},{"location":"other_categories/Segmentation/mmsegmentation/#_1","text":"","title":""},{"location":"other_categories/Segmentation/nvidia-multi-scale-seg/","text":"Hierarchical Multi-scale Attention for Semantic Segmentation \u8fd9\u7bc7paper\u7684\u601d\u8def\u662f\u4f7f\u7528auto-labelling \u53bb\u5229\u7528\u597d\u672a\u88ab\u6807\u6ce8\u7684pixels.","title":"Hierarchical Multi-scale Attention for Semantic Segmentation"},{"location":"other_categories/Segmentation/nvidia-multi-scale-seg/#hierarchical-multi-scale-attention-for-semantic-segmentation","text":"\u8fd9\u7bc7paper\u7684\u601d\u8def\u662f\u4f7f\u7528auto-labelling \u53bb\u5229\u7528\u597d\u672a\u88ab\u6807\u6ce8\u7684pixels.","title":"Hierarchical Multi-scale Attention for Semantic Segmentation"},{"location":"other_categories/Segmentation/openOccupancy/","text":"OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception \u8fd9\u7bc7paper\u4ecb\u7ecd\u4e86\u57fa\u4e8enuscenes + \u4eba\u5de5\u4f18\u5316\u5f62\u6210\u7684occupancy dataset benchmark. \u5e76\u7ed9\u51fa\u4e86\u5404\u4e2amodal\u7684\u4e00\u4e2abaseline. \u73b0\u6709\u76f8\u5173\u6570\u636e\u96c6\u4e4b\u95f4\u7684\u76f8\u4e92\u6bd4\u8f83\uff1a Augmenting and Purifying (AAP) \u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u4e2aAugmenting and Purifying (AAP) pipeline \u4f5c\u4e3a\u771f\u503c\u7684\u751f\u6210\u65b9\u6848\uff1a \u5bf9\u4e8e\u591a\u5e27\u5e26\u6807\u6ce8\u7684\u96f7\u8fbe\u6570\u636e\uff0c\u9759\u6001\u70b9\u76f4\u63a5\u901a\u8fc7 related pose \u8f6c\u79fb\u5230\u5f53\u524d\u5e27\u3002\u52a8\u6001\u70b9\u6839\u636e associated instance ID \u5173\u8054\uff0c\u627e\u5230\u4e24\u5e27 target object\u5728\u4e16\u754c\u5750\u6807\u4e0b\u7684\u7edd\u5bf9\u4f4d\u79fb\uff0c \u628a3D bounding box\u5185\u7684\u70b9\u4e91\u90fd\u6309\u7167dynamic object\u7684\u8fd9\u4e2a\u7edd\u5bf9\u4f4d\u79fbalign\u5230\u5f53\u524d\u5e27\u3002 \u4f7f\u7528\u96f7\u8fbe\u5728\u6570\u636e\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u8865\u5145\u88ab\u906e\u6321\u5f97\u90e8\u5206\u3002\u751f\u6210pseudo label\u8fdb\u884caugment \u4eba\u5de5\u6e05\u7406\u589e\u5f3a\u540e\u7684label\u5f62\u6210\u6700\u7ec8\u7684ground truth. Comparison \u4e0d\u540c\u6a21\u6001\u7b97\u6cd5\u5728open occupancy\u4e0a\u7684\u6a2a\u5411\u6bd4\u8f83:","title":"OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception"},{"location":"other_categories/Segmentation/openOccupancy/#openoccupancy-a-large-scale-benchmark-for-surrounding-semantic-occupancy-perception","text":"\u8fd9\u7bc7paper\u4ecb\u7ecd\u4e86\u57fa\u4e8enuscenes + \u4eba\u5de5\u4f18\u5316\u5f62\u6210\u7684occupancy dataset benchmark. \u5e76\u7ed9\u51fa\u4e86\u5404\u4e2amodal\u7684\u4e00\u4e2abaseline. \u73b0\u6709\u76f8\u5173\u6570\u636e\u96c6\u4e4b\u95f4\u7684\u76f8\u4e92\u6bd4\u8f83\uff1a","title":"OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception"},{"location":"other_categories/Segmentation/openOccupancy/#augmenting-and-purifying-aap","text":"\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u4e2aAugmenting and Purifying (AAP) pipeline \u4f5c\u4e3a\u771f\u503c\u7684\u751f\u6210\u65b9\u6848\uff1a \u5bf9\u4e8e\u591a\u5e27\u5e26\u6807\u6ce8\u7684\u96f7\u8fbe\u6570\u636e\uff0c\u9759\u6001\u70b9\u76f4\u63a5\u901a\u8fc7 related pose \u8f6c\u79fb\u5230\u5f53\u524d\u5e27\u3002\u52a8\u6001\u70b9\u6839\u636e associated instance ID \u5173\u8054\uff0c\u627e\u5230\u4e24\u5e27 target object\u5728\u4e16\u754c\u5750\u6807\u4e0b\u7684\u7edd\u5bf9\u4f4d\u79fb\uff0c \u628a3D bounding box\u5185\u7684\u70b9\u4e91\u90fd\u6309\u7167dynamic object\u7684\u8fd9\u4e2a\u7edd\u5bf9\u4f4d\u79fbalign\u5230\u5f53\u524d\u5e27\u3002 \u4f7f\u7528\u96f7\u8fbe\u5728\u6570\u636e\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u8865\u5145\u88ab\u906e\u6321\u5f97\u90e8\u5206\u3002\u751f\u6210pseudo label\u8fdb\u884caugment \u4eba\u5de5\u6e05\u7406\u589e\u5f3a\u540e\u7684label\u5f62\u6210\u6700\u7ec8\u7684ground truth.","title":"Augmenting and Purifying (AAP)"},{"location":"other_categories/Segmentation/openOccupancy/#comparison","text":"\u4e0d\u540c\u6a21\u6001\u7b97\u6cd5\u5728open occupancy\u4e0a\u7684\u6a2a\u5411\u6bd4\u8f83:","title":"Comparison"},{"location":"other_categories/Segmentation/panopticBEV/","text":"Bird's-Eye-View Panoptic Segmentation Using Monocular Frontal View Images \u5b98\u65b9\u8d44\u6e90\u7f51\u7ad9 \u8fd9\u7bc7\u8bba\u6587\u5b9e\u73b0\u7684\u662f\u7aef\u5230\u7aef\u7684\u524d\u5411\u6444\u50cf\u5934\u56fe\u50cf\u5230BEV\u5168\u666f\u5206\u5272\u3002\u4f5c\u8005\u5728KITTI-360\u548cnuscenes\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u8bbe\u8ba1\u4e86\u5b8c\u6574\u7684label\u6570\u636e\u96c6\u3002 \u4e3b\u8981\u7f51\u7edc\u7ed3\u6784\u5982\u4e0b\u56fe: \u4f7f\u7528efficientDet\u4f5c\u4e3a backbone & neck. \u8bbe\u8ba1\u4e86Dense Transformer\u8f6c\u79fbFront View -> BEV \u4f7f\u7528anchor based \u57fa\u7840\u7684panoptic seg \u89c6\u89d2\u8f6c\u6362\u8fd9\u4e2a\u6838\u5fc3\u95ee\u9898\u4e0a\uff0c\u4f5c\u8005\u63d0\u4f9b\u4e86\u4e24\u79cd\u8f6c\u6362\u65b9\u6cd5\uff0c\u8ba9\u7f51\u7edc\u9009\u62e9(softmax)\u4e00\u79cd\u4f7f\u7528\u3002 \u7b2c\u4e00\u79cd\u4e5f\u5c31\u662f vertical transformer, \u5bf9\u6bcf\u4e00\u4e2a\u50cf\u7d20\u70b9\u9884\u6d4b\u5176\u6df1\u5ea6(multi bin classification)\uff0c\u4f7f\u7528\u76f8\u673a\u5185\u53c2\u5c06\u7279\u5f81\u6269\u5c55\u6210\u76f8\u673a\u5750\u6807\u7cfb\u4e0b\u7684\u7279\u5f81\u56fe\uff0c\u5e76\u4f7f\u75283D grid_sample\u8f6c\u5230BEV\u4e0a. \u7b2c\u4e8c\u79cd\u5219\u662fIPM, \u5047\u8bbe\u6bcf\u4e00\u4e2a\u50cf\u7d20\u90fd\u5728\u5730\u9762\u4e0a,\u5229\u7528\u76f8\u673a\u7684\u5185\u53c2\u4ee5\u53ca\u5916\u53c2(\u79bb\u5730\u9762\u9ad8\u5ea6\u4ee5\u53ca\u51e0\u4f55\u65cb\u8f6c),\u8ba1\u7b97\u56fe\u7247\u5750\u6807\u4ee5\u53caBEV\u5750\u6807\u4e4b\u95f4\u7684Homography Matrix, \u628a\u56fe\u7247\u5750\u6807\u76f4\u63a5\u8f6c\u5230BEV\u4e0a.","title":"Bird's-Eye-View Panoptic Segmentation Using Monocular Frontal View Images"},{"location":"other_categories/Segmentation/panopticBEV/#birds-eye-view-panoptic-segmentation-using-monocular-frontal-view-images","text":"\u5b98\u65b9\u8d44\u6e90\u7f51\u7ad9 \u8fd9\u7bc7\u8bba\u6587\u5b9e\u73b0\u7684\u662f\u7aef\u5230\u7aef\u7684\u524d\u5411\u6444\u50cf\u5934\u56fe\u50cf\u5230BEV\u5168\u666f\u5206\u5272\u3002\u4f5c\u8005\u5728KITTI-360\u548cnuscenes\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u8bbe\u8ba1\u4e86\u5b8c\u6574\u7684label\u6570\u636e\u96c6\u3002 \u4e3b\u8981\u7f51\u7edc\u7ed3\u6784\u5982\u4e0b\u56fe: \u4f7f\u7528efficientDet\u4f5c\u4e3a backbone & neck. \u8bbe\u8ba1\u4e86Dense Transformer\u8f6c\u79fbFront View -> BEV \u4f7f\u7528anchor based \u57fa\u7840\u7684panoptic seg \u89c6\u89d2\u8f6c\u6362\u8fd9\u4e2a\u6838\u5fc3\u95ee\u9898\u4e0a\uff0c\u4f5c\u8005\u63d0\u4f9b\u4e86\u4e24\u79cd\u8f6c\u6362\u65b9\u6cd5\uff0c\u8ba9\u7f51\u7edc\u9009\u62e9(softmax)\u4e00\u79cd\u4f7f\u7528\u3002 \u7b2c\u4e00\u79cd\u4e5f\u5c31\u662f vertical transformer, \u5bf9\u6bcf\u4e00\u4e2a\u50cf\u7d20\u70b9\u9884\u6d4b\u5176\u6df1\u5ea6(multi bin classification)\uff0c\u4f7f\u7528\u76f8\u673a\u5185\u53c2\u5c06\u7279\u5f81\u6269\u5c55\u6210\u76f8\u673a\u5750\u6807\u7cfb\u4e0b\u7684\u7279\u5f81\u56fe\uff0c\u5e76\u4f7f\u75283D grid_sample\u8f6c\u5230BEV\u4e0a. \u7b2c\u4e8c\u79cd\u5219\u662fIPM, \u5047\u8bbe\u6bcf\u4e00\u4e2a\u50cf\u7d20\u90fd\u5728\u5730\u9762\u4e0a,\u5229\u7528\u76f8\u673a\u7684\u5185\u53c2\u4ee5\u53ca\u5916\u53c2(\u79bb\u5730\u9762\u9ad8\u5ea6\u4ee5\u53ca\u51e0\u4f55\u65cb\u8f6c),\u8ba1\u7b97\u56fe\u7247\u5750\u6807\u4ee5\u53caBEV\u5750\u6807\u4e4b\u95f4\u7684Homography Matrix, \u628a\u56fe\u7247\u5750\u6807\u76f4\u63a5\u8f6c\u5230BEV\u4e0a.","title":"Bird's-Eye-View Panoptic Segmentation Using Monocular Frontal View Images"},{"location":"other_categories/Segmentation/pyorcc/","text":"Predicting Semantic Map Representations from Images using Pyramid Occupancy Networks \u8fd9\u7bc7paper\u662f\u4e0e OfT \u540c\u4e00\u4e00\u4f5c\u7684\u4f5c\u54c1\uff0c\u4e24\u8005\u6709\u5f88\u5f3a\u7684\u76f8\u4f3c\u6027\uff0c\u8fd9\u7bc7paper\u7684\u76ee\u6807\u662f\u8f93\u5165\u73af\u89c6\u5355\u76ee\u6444\u50cf\u5934\uff0c\u8f93\u51fa\u5468\u56f4360\u00b0\u7684BEV\u8bed\u4e49\u5206\u5272\u5730\u56fe: \u7f51\u7edc\u7ed3\u6784 \u8fd9\u7bc7paper\u4e0e oft \u5728\u4f7f\u7528OFT\u540c\u6b3e\u7684feature transformation\u4e0a\u6709\u5f88\u5927\u7684\u8fdb\u6b65\uff0c \u4f7f\u7528FPN\u7f51\u7edc\u5f97\u5230multi-scale\u7684\u7279\u5f81\uff0c\u5bf9\u6bcf\u4e00\u4e2a\u7279\u5f81\u90fd\u8f6c\u6210BEV \u4e0d\u540cscale\u7684\u7279\u5f81\u5bf9\u5e94\u4e0d\u540c\u7684BEV\u4e0a\u7684\u8ddd\u79bb(\u8fd9\u70b9\u5f88intuitive\u53c8\u5f88\u806a\u660e\uff0c\u5728OFT\u5b9e\u9645\u8bad\u7ec3\u7684\u65f6\u5019\u4f1a\u53d1\u73b0\u53d7\u5236\u4e8e\u5206\u8fa8\u7387\uff0cOFT\u6cbf\u7740\u4e00\u6761\u5c04\u7ebf\u4e0a\u7684\u683c\u70b9\u51e0\u4e4e\u5c31\u662f\u5f88\u957f\u7684\u533a\u57df\u90fd\u662f\u540c\u4e00\u4e2a\u503c\uff0c\u4f7f\u7528\u4e0d\u540c\u7279\u5f81\u5bf9\u5e94\u4e0d\u540c\u7684\u8ddd\u79bb\uff0c\u4e00\u65b9\u9762\u53ef\u4ee5\u6253\u7834\u8fd9\u4e2a\u4e0d\u826f\u7684\u7279\u6027\uff0c\u589e\u5f3aBEV\u4e0a\u7684\u4fe1\u606f\u88d5\u5ea6\uff0c\u53e6\u4e00\u65b9\u9762\u4e5f\u5f88\u7b26\u5408\u4e0d\u540cscale\u4e0a\u611f\u53d7\u91ce\u4e0e\u7269\u4f53\u5927\u5c0f\u7684\u7279\u6027). \u7531\u4e8e\u8f6c\u5230\u4e86BEV\u4e0a\uff0c\u4f5c\u8005\u81ea\u7136\u5730\u8fdb\u4e00\u6b65\u60f3\u5230\u4e86\u5bf9\u65f6\u5e8f\u4fe1\u606f\u4e0a\u8fdb\u884c\u878d\u5408\uff0c\u8fd9\u6837\u9884\u6d4b\u51fa\u6765\u7684BEV\u4e0a\u7684occupancy map\u5c31\u53ef\u4ee5\u7528\u8d1d\u53f6\u65af\u65b9\u6cd5\u76f4\u63a5\u66f4\u65b0\u3002 \u6570\u636e\u751f\u6210 \u4f5c\u8005\u4f7f\u7528 Nuscenes\u8fd8\u6709 Argoverse\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u4f5c\u8005\u79f0\u4ed6\u5c06\u6807\u6ce8\u597d\u7684\u5730\u56fe\u4ee5\u53ca3D bounding box\u6295\u5f71\u5230\u76f8\u673a\u5750\u6807\u7cfb\u7684xz\u5e73\u9762\u4e0a\uff0c\u540c\u65f6\u4f5c\u8005\u8fd8\u6807\u6ce8\u4e86\u4e00\u4e2avisibility mask,\u65b9\u6cd5\u662f\u5bf9\u56fe\u4e0a\u9762\u7684\u6bcf\u4e00\u70b9\uff0c\u5982\u679c\u4ed6\u5728FOV\u4e2d\u4e14\u81f3\u5c11\u6709\u4e00\u4e2alidar \u5c04\u7ebf\u7a7f\u8fc7\u5b83\u3002 \u6700\u7ec8\u6a21\u578b\u7684\u6027\u80fd\u7684\u7edd\u5bf9\u503c\u8fd8\u662f\u6709\u9650\u7684\uff0c\u4f46\u662fvisualization\u6548\u679c\u5f88\u597d\u3002","title":"Predicting Semantic Map Representations from Images using Pyramid Occupancy Networks"},{"location":"other_categories/Segmentation/pyorcc/#predicting-semantic-map-representations-from-images-using-pyramid-occupancy-networks","text":"\u8fd9\u7bc7paper\u662f\u4e0e OfT \u540c\u4e00\u4e00\u4f5c\u7684\u4f5c\u54c1\uff0c\u4e24\u8005\u6709\u5f88\u5f3a\u7684\u76f8\u4f3c\u6027\uff0c\u8fd9\u7bc7paper\u7684\u76ee\u6807\u662f\u8f93\u5165\u73af\u89c6\u5355\u76ee\u6444\u50cf\u5934\uff0c\u8f93\u51fa\u5468\u56f4360\u00b0\u7684BEV\u8bed\u4e49\u5206\u5272\u5730\u56fe:","title":"Predicting Semantic Map Representations from Images using Pyramid Occupancy Networks"},{"location":"other_categories/Segmentation/pyorcc/#_1","text":"\u8fd9\u7bc7paper\u4e0e oft \u5728\u4f7f\u7528OFT\u540c\u6b3e\u7684feature transformation\u4e0a\u6709\u5f88\u5927\u7684\u8fdb\u6b65\uff0c \u4f7f\u7528FPN\u7f51\u7edc\u5f97\u5230multi-scale\u7684\u7279\u5f81\uff0c\u5bf9\u6bcf\u4e00\u4e2a\u7279\u5f81\u90fd\u8f6c\u6210BEV \u4e0d\u540cscale\u7684\u7279\u5f81\u5bf9\u5e94\u4e0d\u540c\u7684BEV\u4e0a\u7684\u8ddd\u79bb(\u8fd9\u70b9\u5f88intuitive\u53c8\u5f88\u806a\u660e\uff0c\u5728OFT\u5b9e\u9645\u8bad\u7ec3\u7684\u65f6\u5019\u4f1a\u53d1\u73b0\u53d7\u5236\u4e8e\u5206\u8fa8\u7387\uff0cOFT\u6cbf\u7740\u4e00\u6761\u5c04\u7ebf\u4e0a\u7684\u683c\u70b9\u51e0\u4e4e\u5c31\u662f\u5f88\u957f\u7684\u533a\u57df\u90fd\u662f\u540c\u4e00\u4e2a\u503c\uff0c\u4f7f\u7528\u4e0d\u540c\u7279\u5f81\u5bf9\u5e94\u4e0d\u540c\u7684\u8ddd\u79bb\uff0c\u4e00\u65b9\u9762\u53ef\u4ee5\u6253\u7834\u8fd9\u4e2a\u4e0d\u826f\u7684\u7279\u6027\uff0c\u589e\u5f3aBEV\u4e0a\u7684\u4fe1\u606f\u88d5\u5ea6\uff0c\u53e6\u4e00\u65b9\u9762\u4e5f\u5f88\u7b26\u5408\u4e0d\u540cscale\u4e0a\u611f\u53d7\u91ce\u4e0e\u7269\u4f53\u5927\u5c0f\u7684\u7279\u6027). \u7531\u4e8e\u8f6c\u5230\u4e86BEV\u4e0a\uff0c\u4f5c\u8005\u81ea\u7136\u5730\u8fdb\u4e00\u6b65\u60f3\u5230\u4e86\u5bf9\u65f6\u5e8f\u4fe1\u606f\u4e0a\u8fdb\u884c\u878d\u5408\uff0c\u8fd9\u6837\u9884\u6d4b\u51fa\u6765\u7684BEV\u4e0a\u7684occupancy map\u5c31\u53ef\u4ee5\u7528\u8d1d\u53f6\u65af\u65b9\u6cd5\u76f4\u63a5\u66f4\u65b0\u3002","title":"\u7f51\u7edc\u7ed3\u6784"},{"location":"other_categories/Segmentation/pyorcc/#_2","text":"\u4f5c\u8005\u4f7f\u7528 Nuscenes\u8fd8\u6709 Argoverse\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u4f5c\u8005\u79f0\u4ed6\u5c06\u6807\u6ce8\u597d\u7684\u5730\u56fe\u4ee5\u53ca3D bounding box\u6295\u5f71\u5230\u76f8\u673a\u5750\u6807\u7cfb\u7684xz\u5e73\u9762\u4e0a\uff0c\u540c\u65f6\u4f5c\u8005\u8fd8\u6807\u6ce8\u4e86\u4e00\u4e2avisibility mask,\u65b9\u6cd5\u662f\u5bf9\u56fe\u4e0a\u9762\u7684\u6bcf\u4e00\u70b9\uff0c\u5982\u679c\u4ed6\u5728FOV\u4e2d\u4e14\u81f3\u5c11\u6709\u4e00\u4e2alidar \u5c04\u7ebf\u7a7f\u8fc7\u5b83\u3002 \u6700\u7ec8\u6a21\u578b\u7684\u6027\u80fd\u7684\u7edd\u5bf9\u503c\u8fd8\u662f\u6709\u9650\u7684\uff0c\u4f46\u662fvisualization\u6548\u679c\u5f88\u597d\u3002","title":"\u6570\u636e\u751f\u6210"},{"location":"other_categories/Segmentation/skyeye/","text":"SkyEye: Self-Supervised Bird\u2019s-Eye-View Semantic Mapping Using Monocular Frontal View Images \u8fd9\u7bc7paper\u63a2\u7d22\u4e86\u5f31\u76d1\u7763\u7684BEV\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u3002\u5047\u8bbe\u6211\u4eec\u53ea\u6709\u56fe\u7247\u5e8f\u5217\u4ee5\u53ca\u5728\u56fe\u7247\u4e0a\u7684\u8bed\u4e49\u5206\u5272\u6807\u6ce8\uff0c\u5982\u4f55\u8bad\u7ec3\u4e00\u4e2a\u7f51\u7edc\u4ece\u4e00\u4e2a\u76f8\u673a\u8f93\u51faBEV\u4e0a\u7684\u8bed\u4e49\u5206\u5272\u7ed3\u679c\u3002 \u8bba\u6587\u4e2d\u8d34\u4e86 \u5b98\u65b9\u9875\u9762 \uff0c\u4e0d\u8fc7\u5728\u53d1\u672c\u6587\u4e4b\u524d\u5c1a\u672a\u5b8c\u6210\u5efa\u8bbe\u3002 Basic Framework \u5de5\u4f5c\u6d41\u7a0b: \u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u7f51\u7edc(\u91c7\u7528\u7684\u662f\u5728kitti360\u4e0a\u53cc\u76ee\u8bad\u7ec3\u7684) \u5728\u6709gt label\u7684\u56fe\u7247\u4e0a\u9762\uff0c\u628aFront view \u8f6c\u5230BEV\u4e0a\uff0c\u7ecf\u8fc7densification (\u56fe\u5f62\u5b66\u64cd\u4f5cerode) + bounding box fitting (\u586b\u6ee1object). \u751f\u6210BEV\u4e0a\u7684pseudo label. \u4e3b\u7f51\u7edc\u7ed3\u6784\u5305\u542b encoder, lifting module (LSS)\uff0c \u5f62\u6210voxel, \u5206\u522b\u6295\u5f71\u5230\u4e0d\u540c\u76f8\u673a\u5750\u6807\u7cfb\u4e0b\uff0c\u9884\u6d4b\u5bf9\u5e94\u7684 image frame \u8bed\u4e49\u5206\u5272\u5f62\u6001,\u6dfb\u52a0multi-frame consistency\u7684loss\u3002\u540c\u65f6\u6295\u5f71\u5230BEV\u4e0a\uff0c\u5728BEV\u4e0a\u8f93\u51fa\u7ed3\u679c\uff0c\u5e76\u4f7f\u7528psuedolabel \u8fdb\u884c\u76d1\u7763\u3002","title":"SkyEye: Self-Supervised Bird\u2019s-Eye-View Semantic Mapping Using Monocular Frontal View Images"},{"location":"other_categories/Segmentation/skyeye/#skyeye-self-supervised-birds-eye-view-semantic-mapping-using-monocular-frontal-view-images","text":"\u8fd9\u7bc7paper\u63a2\u7d22\u4e86\u5f31\u76d1\u7763\u7684BEV\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u3002\u5047\u8bbe\u6211\u4eec\u53ea\u6709\u56fe\u7247\u5e8f\u5217\u4ee5\u53ca\u5728\u56fe\u7247\u4e0a\u7684\u8bed\u4e49\u5206\u5272\u6807\u6ce8\uff0c\u5982\u4f55\u8bad\u7ec3\u4e00\u4e2a\u7f51\u7edc\u4ece\u4e00\u4e2a\u76f8\u673a\u8f93\u51faBEV\u4e0a\u7684\u8bed\u4e49\u5206\u5272\u7ed3\u679c\u3002 \u8bba\u6587\u4e2d\u8d34\u4e86 \u5b98\u65b9\u9875\u9762 \uff0c\u4e0d\u8fc7\u5728\u53d1\u672c\u6587\u4e4b\u524d\u5c1a\u672a\u5b8c\u6210\u5efa\u8bbe\u3002","title":"SkyEye: Self-Supervised Bird\u2019s-Eye-View Semantic Mapping Using Monocular Frontal View Images"},{"location":"other_categories/Segmentation/skyeye/#basic-framework","text":"\u5de5\u4f5c\u6d41\u7a0b: \u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u7f51\u7edc(\u91c7\u7528\u7684\u662f\u5728kitti360\u4e0a\u53cc\u76ee\u8bad\u7ec3\u7684) \u5728\u6709gt label\u7684\u56fe\u7247\u4e0a\u9762\uff0c\u628aFront view \u8f6c\u5230BEV\u4e0a\uff0c\u7ecf\u8fc7densification (\u56fe\u5f62\u5b66\u64cd\u4f5cerode) + bounding box fitting (\u586b\u6ee1object). \u751f\u6210BEV\u4e0a\u7684pseudo label. \u4e3b\u7f51\u7edc\u7ed3\u6784\u5305\u542b encoder, lifting module (LSS)\uff0c \u5f62\u6210voxel, \u5206\u522b\u6295\u5f71\u5230\u4e0d\u540c\u76f8\u673a\u5750\u6807\u7cfb\u4e0b\uff0c\u9884\u6d4b\u5bf9\u5e94\u7684 image frame \u8bed\u4e49\u5206\u5272\u5f62\u6001,\u6dfb\u52a0multi-frame consistency\u7684loss\u3002\u540c\u65f6\u6295\u5f71\u5230BEV\u4e0a\uff0c\u5728BEV\u4e0a\u8f93\u51fa\u7ed3\u679c\uff0c\u5e76\u4f7f\u7528psuedolabel \u8fdb\u884c\u76d1\u7763\u3002","title":"Basic Framework"},{"location":"other_categories/Segmentation/uniocc/","text":"UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering \u8fd9\u7bc7paper\u63d0\u51fa\u4e86\u4e0d\u4f7f\u75283D occupancy labeling \u5b8c\u6210 3D occupancy Prediction\u8bad\u7ec3\u7684\u65b9\u6848, \u57fa\u672c\u601d\u8def\u662f\u7528NeRF\u5bf9\u76f8\u90bb\u5e27\u8fdb\u884c\u8bed\u4e49\u5206\u5272\u4e0e\u6df1\u5ea6\u6e32\u67d3\u3002\u7136\u540e\u4f7f\u7528\u96f7\u8fbe\u70b9\u5728\u56fe\u7247\u4e0a\u7684\u6295\u5f71\u5b8c\u6210\u76d1\u7763\u3002 Rendering Supervision \u4f7f\u7528LSS\u8f93\u51fa\u4e00\u4e2a\u4e09\u7ef4\u4f53\u7d20 V^{x,y,z} , \u4f7f\u7528\u4e24\u4e2a\u5206\u5f00\u7684MLP\u7f51\u7edc\u5bf9\u6bcf\u4e00\u4e2a\u70b9\u9884\u6d4boccupancy prediction \u548c Segmentation label. \u6e32\u67d3\u91c7\u7528NeRF\u4e2d\u5e38\u7528\u7684\u5149\u7ebf\u91c7\u6837\u65b9\u6848\uff1a \\begin{aligned} T\\left(z_k\\right) & =\\exp \\left(-\\sum_{t=1}^{k-1} \\sigma\\left(z_t\\right) \\beta_t\\right) \\\\ \\alpha\\left(z_k\\right) & =1-\\exp \\left(-\\sigma\\left(z_k\\right) \\beta_k\\right) \\\\ D & =\\sum_{k=1}^N T\\left(z_k\\right) \\alpha\\left(z_k\\right) z_k \\\\ S & =\\sum_{k=1}^N T\\left(z_k\\right) \\alpha\\left(z_k\\right) s\\left(z_k\\right) \\\\ \\beta_k &= z_{k+1} - z_{k} \\end{aligned} \u4f7f\u7528SILoss \u548c CELoss\u5206\u522b\u76d1\u7763\u6df1\u5ea6\u6e32\u67d3\u548c\u8bed\u4e49\u5206\u5272\u6e32\u67d3\u3002 Depth Aware Teacher Student \u5bf9\u4e8e\u6ca1\u6709\u6807\u6ce8\u7684\u6570\u636e\uff0c\u4f7f\u7528\u4e00\u4e2a\u81ea\u76d1\u7763\u65b9\u6848\u8865\u5145\u76d1\u7763\u5982\u56fe: Other Settings Using Semantic label\uff0c\u53bb\u9664\u65f6\u5e8f\u8fc7\u7a0b\u4e2d\u53ef\u80fd\u7684\u52a8\u6001\u7269\u4f53(\u8f66\u548c\u4eba) \u4f7f\u7528 visibility mask, \u5224\u65ad\u5f53\u524d\u8981\u63a8\u65ad\u7684\u70b9\u662f\u5426\u8d85\u51fa\u4e86\u73b0\u6709Voxel\u7684\u8303\u56f4\uff0c\u5e76\u51cf\u5c11\u5bf9\u8fd9\u90e8\u5206\u70b9\u7684\u76d1\u7763\u3002\u53ef\u4ee5\u63d0\u5347\u70b9\u6570\uff0c\u4f46\u662f\u4f1a\u635f\u5931\u53ef\u89c6\u5316\u6548\u679c(\u76f8\u5f53\u4e8e\u6ca1\u6709\u5bf9\u5929\u7a7a\u90e8\u5206\u7684\u70b9\u4f5c\u6b63\u786e\u76d1\u7763)\u3002","title":"UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering"},{"location":"other_categories/Segmentation/uniocc/#uniocc-unifying-vision-centric-3d-occupancy-prediction-with-geometric-and-semantic-rendering","text":"\u8fd9\u7bc7paper\u63d0\u51fa\u4e86\u4e0d\u4f7f\u75283D occupancy labeling \u5b8c\u6210 3D occupancy Prediction\u8bad\u7ec3\u7684\u65b9\u6848, \u57fa\u672c\u601d\u8def\u662f\u7528NeRF\u5bf9\u76f8\u90bb\u5e27\u8fdb\u884c\u8bed\u4e49\u5206\u5272\u4e0e\u6df1\u5ea6\u6e32\u67d3\u3002\u7136\u540e\u4f7f\u7528\u96f7\u8fbe\u70b9\u5728\u56fe\u7247\u4e0a\u7684\u6295\u5f71\u5b8c\u6210\u76d1\u7763\u3002","title":"UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering"},{"location":"other_categories/Segmentation/uniocc/#rendering-supervision","text":"\u4f7f\u7528LSS\u8f93\u51fa\u4e00\u4e2a\u4e09\u7ef4\u4f53\u7d20 V^{x,y,z} , \u4f7f\u7528\u4e24\u4e2a\u5206\u5f00\u7684MLP\u7f51\u7edc\u5bf9\u6bcf\u4e00\u4e2a\u70b9\u9884\u6d4boccupancy prediction \u548c Segmentation label. \u6e32\u67d3\u91c7\u7528NeRF\u4e2d\u5e38\u7528\u7684\u5149\u7ebf\u91c7\u6837\u65b9\u6848\uff1a \\begin{aligned} T\\left(z_k\\right) & =\\exp \\left(-\\sum_{t=1}^{k-1} \\sigma\\left(z_t\\right) \\beta_t\\right) \\\\ \\alpha\\left(z_k\\right) & =1-\\exp \\left(-\\sigma\\left(z_k\\right) \\beta_k\\right) \\\\ D & =\\sum_{k=1}^N T\\left(z_k\\right) \\alpha\\left(z_k\\right) z_k \\\\ S & =\\sum_{k=1}^N T\\left(z_k\\right) \\alpha\\left(z_k\\right) s\\left(z_k\\right) \\\\ \\beta_k &= z_{k+1} - z_{k} \\end{aligned} \u4f7f\u7528SILoss \u548c CELoss\u5206\u522b\u76d1\u7763\u6df1\u5ea6\u6e32\u67d3\u548c\u8bed\u4e49\u5206\u5272\u6e32\u67d3\u3002","title":"Rendering Supervision"},{"location":"other_categories/Segmentation/uniocc/#depth-aware-teacher-student","text":"\u5bf9\u4e8e\u6ca1\u6709\u6807\u6ce8\u7684\u6570\u636e\uff0c\u4f7f\u7528\u4e00\u4e2a\u81ea\u76d1\u7763\u65b9\u6848\u8865\u5145\u76d1\u7763\u5982\u56fe:","title":"Depth Aware Teacher Student"},{"location":"other_categories/Segmentation/uniocc/#other-settings","text":"Using Semantic label\uff0c\u53bb\u9664\u65f6\u5e8f\u8fc7\u7a0b\u4e2d\u53ef\u80fd\u7684\u52a8\u6001\u7269\u4f53(\u8f66\u548c\u4eba) \u4f7f\u7528 visibility mask, \u5224\u65ad\u5f53\u524d\u8981\u63a8\u65ad\u7684\u70b9\u662f\u5426\u8d85\u51fa\u4e86\u73b0\u6709Voxel\u7684\u8303\u56f4\uff0c\u5e76\u51cf\u5c11\u5bf9\u8fd9\u90e8\u5206\u70b9\u7684\u76d1\u7763\u3002\u53ef\u4ee5\u63d0\u5347\u70b9\u6570\uff0c\u4f46\u662f\u4f1a\u635f\u5931\u53ef\u89c6\u5316\u6548\u679c(\u76f8\u5f53\u4e8e\u6ca1\u6709\u5bf9\u5929\u7a7a\u90e8\u5206\u7684\u70b9\u4f5c\u6b63\u786e\u76d1\u7763)\u3002","title":"Other Settings"},{"location":"other_categories/Summaries/Collections_StereoMatching_KITTI/","text":"Collections of Stereo Matching from KITTI \u672c\u6587\u8bb0\u5f55\u4e86 Stereo Matching \u6709\u6587\u7ae0/\u6709code\u5b9e\u73b0\u7684\u4e3b\u8981paper.\u5c06\u4f1a\u6301\u7eedupdate Methods D1-all D1-bg D1-fg Time CSPN 1.74 1.51 2.88 1.0 GANet-deep 1.81 1.48 3.46 1.8 AcfNet 1.89 1.51 3.80 0.48 CDN-GANet 1.92 1.66 3.20 0.40 AANet+ 2.03 1.65 3.96 0.06 DecomposeNet 2.37 2.07 3.87 0.05 DeepPruner 2.15 1.87 3.56 0.18 PSMNet 2.32 1.86 4.62 0.21 FADNet 2.82 2.68 3.50 0.05 NVStereoNet 3.13 2.62 5.69 0.6 RTS2Net 3.56 3.09 5.91 0.02 SsSMnet 3.40 2.70 6.92 0.8 STTR 3.73 3.23 6.2 0.6 \u5176\u4e2d\u672c\u7ad9\u5176\u4ed6\u9875\u9762\u5df2\u6709\u7684\u6587\u7ae0\u4e3a CSPN , AcfNet , CDN-GANet , DeepPruner , PSMNet , FADNet , SsSMnet , RTS2Net . \u76ee\u5f55: Collections of Stereo Matching from KITTI GANet AANet DecomposeNet NVStereoNet STTR: Revisiting Stereo Depth Estimation From a Sequence-to-Sequence Perspective with Transformers \u6574\u4f53\u7ed3\u6784 Optimal Transport GANet pdf code Feature Extraction\u4f7f\u7528\u7684\u662fstacked hourglass network. Cost Volume\u7684\u5f62\u6210\u4e0e PSMNet \u4e00\u81f4\u3002\u7136\u540e\u63a5\u6570\u4e2aSGA\u6a21\u5757\uff0c\u4ee5\u53caLGA\u6a21\u5757\u3002\u5de6\u56fe\u4f1a\u63a5\u4e0a\"guidance subnet\"\u4f7f\u7528\u6570\u4e2a\u7b80\u5355\u5377\u79ef\u751f\u6210\u6743\u91cd\u77e9\u9635\u63d0\u4f9b\u5230GA\u6a21\u5757\u4e2d\u3002 GA\u5c42\u5bf9\u5e94scanline optimization\u65b9\u6cd5 ref1 ref2 ,\u53ef\u4ee5\u7406\u89e3\u4e3a\u4e00\u4e2a\u52a8\u6001\u89c4\u5212\u7b97\u6cd5,\u5176\u4e2d\u672c\u6587\u7684 \\mathbf{r} \u4e3a\u56db\u4e2a\u65b9\u5411\u7684\u77e2\u91cf\u3002\u91cc\u9762\u7684\u6743\u91cd\u662f\u6bcf\u4e00\u4e2a\u50cf\u7d20\u4e0d\u4e00\u81f4\u7684\uff0c\u901a\u8fc7subnet\u63d0\u4f9bguidance. Semi-Global Guided Aggregation(SGA) \u9700\u8981\u7684guidence\u6743\u91cd\u5927\u5c0f\u4e3a H\\times W \\times K\\times F(K=5) \uff0c\u4e0d\u540cdisparity\u4f7f\u7528\u7684\u6743\u91cd\u4e00\u81f4: C_{\\mathbf{r}}^{A}(\\mathbf{p}, d)=\\operatorname{sum}\\left\\{\\begin{array}{l} \\mathbf{w}_{0}(\\mathbf{p}, \\mathbf{r}) \\cdot C(\\mathbf{p}, d) \\\\ \\mathbf{w}_{1}(\\mathbf{p}, \\mathbf{r}) \\cdot C_{\\mathbf{r}}^{A}(\\mathbf{p}-\\mathbf{r}, d) \\\\ \\mathbf{w}_{2}(\\mathbf{p}, \\mathbf{r}) \\cdot C_{\\mathbf{r}}^{A}(\\mathbf{p}-\\mathbf{r}, d-1) \\\\ \\mathbf{w}_{3}(\\mathbf{p}, \\mathbf{r}) \\cdot C_{\\mathbf{r}}^{A}(\\mathbf{p}-\\mathbf{r}, d+1) \\\\ \\mathbf{w}_{4}(\\mathbf{p}, \\mathbf{r}) \\cdot \\max _{i} C_{\\mathbf{r}}^{A}(\\mathbf{p}-\\mathbf{r}, i) \\end{array}\\right. \\text {s.t.} \\quad \\sum_{i=0,1,2,3,4} \\mathbf{w}_{i}(\\mathbf{p}, \\mathbf{r})=1 C^{A}(\\mathbf{p}, d)=\\max _{\\mathbf{r}} C_{\\mathbf{r}}^{A}(\\mathbf{p}, d) Local Aggregation(LGA),\u9700\u8981\u7684guidence\u6743\u91cd\u5927\u5c0f\u4e3a H\\times W\\times 3K^2 \\times F : \\begin{array}{c} C^{A}(\\mathbf{p}, d)=\\operatorname{sum}\\left\\{\\begin{array}{l} \\sum_{\\mathbf{q} \\in N_{\\mathrm{p}}} \\omega_{0}(\\mathbf{p}, \\mathbf{q}) \\cdot C(\\mathbf{q}, d) \\\\ \\sum_{\\mathbf{q} \\in N_{\\mathrm{p}}} \\omega_{1}(\\mathbf{p}, \\mathbf{q}) \\cdot C(\\mathbf{q}, d-1) \\\\ \\sum_{\\mathbf{q} \\in N_{\\mathrm{p}}} \\omega_{2}(\\mathbf{p}, \\mathbf{q}) \\cdot C(\\mathbf{q}, d+1) \\end{array}\\right. \\\\ \\text { s.t. } \\sum_{\\mathbf{q} \\in N_{\\mathrm{p}}} \\omega_{0}(\\mathbf{p}, \\mathbf{q})+\\omega_{1}(\\mathbf{p}, \\mathbf{q})+\\omega_{2}(\\mathbf{p}, \\mathbf{q})=1 \\end{array} AANet pdf code \u672c\u6587\u4f7f\u7528coorelation\u7684\u65b9\u5f0f\u751f\u62103D Cost Volume. Adaptive Intra-Scale Aggregation\u672c\u8d28\u4e0a\u662f\u5206\u7ec4\u7684\u53ef\u53d8\u5377\u79ef\uff1a \\tilde{\\boldsymbol{C}}(d, \\boldsymbol{p})=\\sum_{k=1}^{K^{2}} w_{k} \\cdot \\boldsymbol{C}\\left(d, \\boldsymbol{p}+\\boldsymbol{p}_{k}+\\Delta \\boldsymbol{p}_{k}\\right) \\cdot m_{k} \u591ascale\u878d\u5408\uff0c\u8fd9\u91cc\u91c7\u7528\u7684\u662f HRNet \u7684\u65b9\u6cd5 \\hat{\\boldsymbol{C}}^{s}=\\sum_{k=1}^{S} f_{k}\\left(\\tilde{\\boldsymbol{C}}^{k}\\right), \\quad s=1,2, \\cdots, S f_{k}=\\left\\{\\begin{array}{l} \\mathcal{I}, \\quad k=s \\\\ (s-k) \\text { stride }-2\\oplus 3 \\times 3 \\text { convs, } \\quad k<s \\\\ \\text { upsampling } \\oplus 1 \\times 1 \\text { conv, } \\quad k>s \\end{array}\\right. DecomposeNet pdf \u8fd9\u7bc7paper\u7684\u601d\u8def\u8fd8\u662f\u964d\u4f4e\u590d\u6742\u5ea6\uff0c\u5728\u6700\u4f4e\u7684\u5206\u8fa8\u7387\u4e0b\u8ba1\u7b97 Full Stereo Matching \u7684 Cost Volume. \u5728\u9ad8\u5206\u8fa8\u7387\u4e0b\u8fdb\u884c Sparse Matching. \u4e00\u5f20\u56fe\u6982\u5ff5\u4e0a\u53ef\u4ee5\u5206\u4e3a\u4e24\u79cd\u533a\u57df\uff0c\u4e00\u79cd\u662f\u7c97\u5206\u7c7b\u533a\u57df CA \uff0c\u8fd9\u91cc\u7684\u53cc\u76ee\u5339\u914d\u7ed3\u679c\u53ef\u4ee5\u4ece\u4f4e\u5206\u8fa8\u7387\u4e0a\u91c7\u6837\u540erefine\u51fa\u6765\uff1b\u4e00\u79cd\u662f\u7ec6\u5206\u7c7b\u533a\u57df FA \uff0c\u8fd9\u91cc\u7684\u53cc\u76ee\u7531\u9ad8\u5206\u8fa8\u7387\u7684sparse matching\u8fd8\u539f. \\begin{array}{c} \\hat{D}_{L}=\\widehat{\\mathcal{F}}\\left(\\mathrm{FA}_{L}, \\mathrm{FA}_{L}\\right) \\\\ \\vdots \\\\ \\hat{D}_{1}=\\widehat{\\mathcal{F}}\\left(\\mathrm{FA}_{1}, \\mathrm{FA}_{1}\\right) \\\\ D_{0}=\\mathcal{F}\\left(\\dot{\\mathrm{A}}_{0}, \\dot{\\mathrm{A}}_{0}\\right) \\\\ \\tilde{D}=\\hat{D}_{L} \\cup \\cdots \\cup \\hat{D}_{1} \\cup D_{0} \\end{array} \u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u81ea\u76d1\u7763\u7684\u65b9\u6848\u8bad\u7ec3\u4e00\u4e2a\u5c0f\u7f51\u7edc\u8bc6\u522b\u56fe\u4e2d\u88abdownsampling\u7834\u574f\u7684\u7279\u5f81, \u5176\u8f93\u5165\u662f\u672c\u5c42\u7684\u7279\u5f81 F_l \u4ee5\u53ca\u4e0b\u5c42\u4e0a\u91c7\u6837\u7684 F'_{l-1} \u7684\u7279\u5f81\u5dee F_l - F'_{l-1} , \u8f93\u51fa\u7684\u7f51\u7edc\u53ef\u4ee5\u88ab\u8bad\u7ec3\uff0c\u8bad\u7ec3\u76ee\u6807\u662f\u589e\u52a0 FA \u533a\u57df\u7684\u7279\u5f81\u5dee\uff0c\u540c\u65f6\u8981\u6c42\u5176\u7a00\u758f. \\mathcal{L}_{l}^{\\mathrm{DLD}}=\\left|F A_{l}\\right|-\\alpha \\frac{\\sum_{(h, w) \\in F A_{l}}\\left\\|F_{l}(h, w)-F_{l-1}^{\\prime}(h, w)\\right\\|_{2}}{\\left|F A_{l}\\right|} \u5728\u7a00\u758fMask\u4e0a\u76f4\u63a5\u8ba1\u7b97disparity: $$ \\begin{aligned} C_{l}(h, w, d)&=<{F} {l}^{left}(h, w), {F} {l}^{right}(h, w-d)> \\ P_{l}(h, w, d) &=\\frac{\\mathrm{e}^{C_{l}(h, w, d)-C_{l}^{\\max }(h, w)}}{\\sum_{d=0} \\mathrm{e}^{C_{l}(h, w, d)-C_{l}^{\\max }(h, w)}} \\ C_{l}^{\\max }(h, w) &=\\max {d} C {l}(h, w, d) \\ \\hat{D} {l}(h, w)&=\\sum {d=0} P_{l}(h, w, d) * d \\end{aligned} $$ NVStereoNet pdf code \u635f\u5931\u4e0e monodepth \u76f8\u4f3c L=\\lambda_{1} E_{\\text {image}}+\\lambda_{2} E_{\\text {lidar}}+\\lambda_{3} E_{l r}+\\lambda_{4} E_{d s} \\begin{aligned} E_{\\text {image}} &=E_{\\text {image}}^{l}+E_{\\text {image}}^{r} \\\\ E_{\\text {lidar}} &=\\left|d_{l}-\\bar{d}_{l}\\right|+\\left|d_{r}-\\bar{d}_{r}\\right| \\\\ E_{l r} &=\\frac{1}{n} \\sum_{i j}\\left|d_{i j}^{l}-\\tilde{d}_{i j}^{l}\\right|+\\frac{1}{n} \\sum_{i j}\\left|d_{i j}^{r}-\\tilde{d}_{i j}^{r}\\right| \\\\ E_{d s} &=E_{d s}^{l}+E_{d s}^{r} \\end{aligned} \\begin{aligned} E_{\\text {image}}^{l} &=\\frac{1}{n} \\sum_{i, j} \\alpha \\frac{1-\\operatorname{SSIM}\\left(I_{i j}^{l}, \\tilde{I}_{i j}^{l}\\right)}{2}+(1-\\alpha) | I_{i j}^{l}-\\tilde{I}_{i j}^{l} \\\\ E_{d s}^{l} &=\\frac{1}{n} \\sum_{i, j}\\left|\\partial_{x} d_{i j}^{l}\\right| e^{-\\left\\|\\partial_{x} I_{i, j}^{l}\\right\\|}+\\left|\\partial_{y} d_{i j}^{l}\\right| e^{-\\| \\partial_{y} I_{i, j}^{l}} \\| \\end{aligned} STTR: Revisiting Stereo Depth Estimation From a Sequence-to-Sequence Perspective with Transformers pdf code \u8fd9\u7bc7paper\u7528Sequence/Transformer\u7684\u89d2\u5ea6\u91cd\u65b0\u7406\u89e3\u53cc\u76eepixel-wise matching. \u4f5c\u8005\u6307\u51fa\u4e3b\u8981\u6709\u4e09\u5927\u4f18\u52bf: \u4e0d\u518d\u5f3a\u7ea6\u675f\u8981\u6c42\u4e00\u4e2a\u56fa\u5b9a\u7684disparity\u4e0a\u9650 \u80fd\u8bc6\u522b\u906e\u6321\u533a\u57df\uff0c\u7ed9\u51faconfidence \u7ed9\u51fauniqueness\u7ea6\u675f (\u8fd9\u4e2a\u662f\u901a\u8fc7\u6c42\u89e3\u4e00\u4e2a\u6700\u4f18\u5316\u95ee\u9898\u5b9e\u73b0\u7684) \u6574\u4f53\u7ed3\u6784 \u672c\u6587\u7684\u7f51\u7edc\u7ed3\u6784\u7684\u6bcf\u4e00\u4e2a\u90e8\u4ef6\u90fd\u662f\u57fa\u7840\u90e8\u4ef6\uff0c\u8bbe\u8ba1\u4e0a\u6216\u8005\u4ee3\u7801\u4e0a\u4e3b\u8981\u7684\u4e0d\u540c\u70b9: backbone\u7f51\u7edc\u91c7\u7528\u7684\u662f\u7c7b\u4f3c\u4e8eU-Net\u7684\u7ed3\u6784,\u4f46\u662f\u5728\u6700\u540e\u4e00\u5c42\u91c7\u6837\u540e\u52a0\u4e86\u4e00\u4e2a DeepLabv3 \u7684ASPP multiheader transformer\u91cc\u9762\u4ea4\u9519\u4f7f\u7528self-attention\u4e0ecross attention.\u800c\u4ee3\u7801\u4e0a\u4e24\u8005\u5206\u522b\u53ea\u6709\u4e00\u4e2amulti-head\u7684ModuleList, \u4e5f\u5c31\u662f\u56fe\u4e2d\u91cd\u590d\u7684\u6a21\u5757\u4f7f\u7528\u7684\u662f\u76f8\u540c\u7684\u53c2\u6570. attention\u662f\u5148\u8c03\u6574\u5f62\u72b6 [1, C, H, W] \\rightarrow [H, W, C] , \u8f93\u51fa\u76f8\u5f53\u4e8e\u662f\u5728 W \u4e0a\u5c55\u5f00\u4e00\u4e2a\u4e2a\u50cf\u7d20\u5339\u914d\uff0c\u4e5f\u5c31\u662f\u7406\u89e3\u4e3a\u53ea\u5728\u6c34\u5e73 epipolar line\u4e0a\u8fdb\u884c\u5339\u914d, \u8f93\u51fa\u5f62\u72b6 [H, W, W] Position Embedding \u91c7\u7528\u7684\u662f\u4f5c\u8005\u63d0\u51fa\u7684\u76f8\u5bf9\u4f4d\u7f6eencode, \u8fd9\u4e2aencoding\u662f\u4e00\u4e2a [B, C, H, \\times (2W-1)] , \u7684encoder. \u7531\u4e8e\u591a\u5c42multihead\u7684transformer\u8fd0\u7b97\u91cf\u548c\u663e\u5b58\u5360\u7528\u5f88\u5927\uff0c\u4ee3\u7801\u5b9e\u73b0\u4e0a\u9700\u8981\u7528 checkpoint \u6280\u5de7\uff0c\u4e5f\u5c31\u662f\u6682\u65f6\u4e0d\u5b58\u50a8\u4e2d\u95f4\u8fd0\u7b97\u53d8\u91cf\uff0c\u5728backward\u7684\u65f6\u5019\u518d\u91cd\u65b0\u524d\u4f20\u8ba1\u7b97\u4e2d\u95f4\u8fd0\u7b97\u53d8\u91cf. \u53bb\u5230\u6700\u540e\u4e00\u5c42\u7684\u65f6\u5019\u4f5c\u8005\u6307\u51fa\u5e94\u8be5\u8003\u8651\u76f8\u673a\u5de6\u53f3\u653e\u7f6e\u7684\u5173\u7cfb\uff0c\u53f3feature\u4e0e\u5de6feature\u8ba1\u7b97attention\u7684\u65f6\u5019\u5e94\u8be5\u53ea\u8ba1\u7b97\u90e8\u5206\u7684attention\u503c (\u4e0a\u534a\u4e09\u89d2). Optimal Transport \u4ece\u7f51\u7edc\u4e2d\u5f97\u5230 W \\times W \u5339\u914d\u635f\u5931\u77e9\u9635\u540e\uff0c\u6211\u4eec\u5e0c\u671b\u5f97\u5230\u6bcf\u4e00\u4e2a\u50cf\u7d20\u7684\u6700\u4f18\u7684, unique\u7684\u7ed3\u679c. \u8fd9\u4e2a\u95ee\u9898\u53ef\u4ee5\u88abformulate\u4e3a\u4e00\u4e2a optimal transport\u7684\u95ee\u9898, \u6709\u4e00\u4e2a\u4ece\u5206\u751c\u54c1motivate\u7684\u6bd4\u8f83\u597d\u7684\u535a\u5ba2 . \u7528STTR\u8fd9\u7bc7paper\u7684\u8bed\u8a00\u8fdb\u884c\u95ee\u9898\u8868\u8ff0: \\begin{aligned} \\underset{\\vec P}{\\text{minimize}} \\quad &\\underset{i,j}{\\sum}P_{ij} M_{ij} + \\frac{1}{\\lambda} \\sum_{i,j} P_{i,j} \\log P_{i,j} \\\\ \\text{subject to} \\quad & \\sum_i P_{i,j} = c_j\\\\ & \\sum_j P_{i,j} = r_i\\\\ \\end{aligned} \u76ee\u6807\u51fd\u6570\u7684\u7b2c\u4e00\u9879\u4e3a\u7531\u635f\u5931\u51fd\u6570\u77e9\u9635\u51b3\u5b9a\u7684\u7ebf\u6027\u51fd\u6570\uff0c\u7b2c\u4e8c\u9879\u4e3a\u4ea4\u53c9\u71b5\u3002\u7ea6\u675f\u51fd\u6570\u8981\u6c42\u6bcf\u4e00\u4e2a\u5de6\u56fe\u7684\u50cf\u7d20\u53ea\u4f1a\u5bf9\u5e94\u4e00\u4e2a\u53f3\u56fe\u7684\u50cf\u7d20(\u6982\u7387\u548c\u4e3a1)\uff0c \u6bcf\u4e00\u4e2a\u53f3\u56fe\u7684\u50cf\u7d20\u4e5f\u53ea\u4f1a\u5bf9\u5e94\u4e00\u4e2a\u5de6\u56fe\u7684\u50cf\u7d20. Optimal Transport\u662f\u4e00\u4e2a \u51f8\u4f18\u5316\u4e2d\u7684LP\u95ee\u9898\uff0c\u4f7f\u7528 KKT \u6761\u4ef6\u4e2d\u5bf9 P \u7684\u5bfc\u6570\u4e3a\u96f6\u53ef\u4ee5\u5f97\u5230: P_{ij} = \\alpha_i \\beta_j e^{-\\lambda M} \u5176\u4e2d \\alpha \u548c \\beta \u4e0e\u62c9\u683c\u6717\u65e5\u4e58\u5b50\u548c\u8d85\u53c2\u6570 \\lambda \u6709\u5173\uff0c\u4ece\u7ea6\u675f\u4e2d\u6c42\u89e3\u83b7\u5f97. Sinkhorn Distances\u7684\u539f\u59cb\u4ee3\u7801\u5219\u662f: def compute_optimal_transport(M, r, c, lam, epsilon=1e-8): \"\"\" Computes the optimal transport matrix and Slinkhorn distance using the Sinkhorn-Knopp algorithm. From https://michielstock.github.io/OptimalTransport/ Inputs: - M : cost matrix (n x m) - r : vector of marginals (n, ) - c : vector of marginals (m, ) - lam : strength of the entropic regularization - epsilon : convergence parameter Outputs: - P : optimal transport matrix (n x m) - dist : Sinkhorn distance \"\"\" n, m = M.shape P = np.exp(- lam * M) P /= P.sum() u = np.zeros(n) # normalize this matrix while np.max(np.abs(u - P.sum(1))) > epsilon: u = P.sum(1) P *= (r / u).reshape((-1, 1)) # normalize in first dimension P *= (c / P.sum(0)).reshape((1, -1)) #normalize in second dimension return P, np.sum(P * M) \u672c\u6587\u7684\u4ee3\u7801\u5219\u662f\u9ed8\u8ba4 \\lambda \u4e3a1\uff0c\u5728 \\log \u7a7a\u95f4\u4e2d\u8fdb\u884c\u8fed\u4ee3\u8ba1\u7b97\u3002","title":"Collections of Stereo Matching from KITTI"},{"location":"other_categories/Summaries/Collections_StereoMatching_KITTI/#collections-of-stereo-matching-from-kitti","text":"\u672c\u6587\u8bb0\u5f55\u4e86 Stereo Matching \u6709\u6587\u7ae0/\u6709code\u5b9e\u73b0\u7684\u4e3b\u8981paper.\u5c06\u4f1a\u6301\u7eedupdate Methods D1-all D1-bg D1-fg Time CSPN 1.74 1.51 2.88 1.0 GANet-deep 1.81 1.48 3.46 1.8 AcfNet 1.89 1.51 3.80 0.48 CDN-GANet 1.92 1.66 3.20 0.40 AANet+ 2.03 1.65 3.96 0.06 DecomposeNet 2.37 2.07 3.87 0.05 DeepPruner 2.15 1.87 3.56 0.18 PSMNet 2.32 1.86 4.62 0.21 FADNet 2.82 2.68 3.50 0.05 NVStereoNet 3.13 2.62 5.69 0.6 RTS2Net 3.56 3.09 5.91 0.02 SsSMnet 3.40 2.70 6.92 0.8 STTR 3.73 3.23 6.2 0.6 \u5176\u4e2d\u672c\u7ad9\u5176\u4ed6\u9875\u9762\u5df2\u6709\u7684\u6587\u7ae0\u4e3a CSPN , AcfNet , CDN-GANet , DeepPruner , PSMNet , FADNet , SsSMnet , RTS2Net . \u76ee\u5f55: Collections of Stereo Matching from KITTI GANet AANet DecomposeNet NVStereoNet STTR: Revisiting Stereo Depth Estimation From a Sequence-to-Sequence Perspective with Transformers \u6574\u4f53\u7ed3\u6784 Optimal Transport","title":"Collections of Stereo Matching from KITTI"},{"location":"other_categories/Summaries/Collections_StereoMatching_KITTI/#ganet","text":"pdf code Feature Extraction\u4f7f\u7528\u7684\u662fstacked hourglass network. Cost Volume\u7684\u5f62\u6210\u4e0e PSMNet \u4e00\u81f4\u3002\u7136\u540e\u63a5\u6570\u4e2aSGA\u6a21\u5757\uff0c\u4ee5\u53caLGA\u6a21\u5757\u3002\u5de6\u56fe\u4f1a\u63a5\u4e0a\"guidance subnet\"\u4f7f\u7528\u6570\u4e2a\u7b80\u5355\u5377\u79ef\u751f\u6210\u6743\u91cd\u77e9\u9635\u63d0\u4f9b\u5230GA\u6a21\u5757\u4e2d\u3002 GA\u5c42\u5bf9\u5e94scanline optimization\u65b9\u6cd5 ref1 ref2 ,\u53ef\u4ee5\u7406\u89e3\u4e3a\u4e00\u4e2a\u52a8\u6001\u89c4\u5212\u7b97\u6cd5,\u5176\u4e2d\u672c\u6587\u7684 \\mathbf{r} \u4e3a\u56db\u4e2a\u65b9\u5411\u7684\u77e2\u91cf\u3002\u91cc\u9762\u7684\u6743\u91cd\u662f\u6bcf\u4e00\u4e2a\u50cf\u7d20\u4e0d\u4e00\u81f4\u7684\uff0c\u901a\u8fc7subnet\u63d0\u4f9bguidance. Semi-Global Guided Aggregation(SGA) \u9700\u8981\u7684guidence\u6743\u91cd\u5927\u5c0f\u4e3a H\\times W \\times K\\times F(K=5) \uff0c\u4e0d\u540cdisparity\u4f7f\u7528\u7684\u6743\u91cd\u4e00\u81f4: C_{\\mathbf{r}}^{A}(\\mathbf{p}, d)=\\operatorname{sum}\\left\\{\\begin{array}{l} \\mathbf{w}_{0}(\\mathbf{p}, \\mathbf{r}) \\cdot C(\\mathbf{p}, d) \\\\ \\mathbf{w}_{1}(\\mathbf{p}, \\mathbf{r}) \\cdot C_{\\mathbf{r}}^{A}(\\mathbf{p}-\\mathbf{r}, d) \\\\ \\mathbf{w}_{2}(\\mathbf{p}, \\mathbf{r}) \\cdot C_{\\mathbf{r}}^{A}(\\mathbf{p}-\\mathbf{r}, d-1) \\\\ \\mathbf{w}_{3}(\\mathbf{p}, \\mathbf{r}) \\cdot C_{\\mathbf{r}}^{A}(\\mathbf{p}-\\mathbf{r}, d+1) \\\\ \\mathbf{w}_{4}(\\mathbf{p}, \\mathbf{r}) \\cdot \\max _{i} C_{\\mathbf{r}}^{A}(\\mathbf{p}-\\mathbf{r}, i) \\end{array}\\right. \\text {s.t.} \\quad \\sum_{i=0,1,2,3,4} \\mathbf{w}_{i}(\\mathbf{p}, \\mathbf{r})=1 C^{A}(\\mathbf{p}, d)=\\max _{\\mathbf{r}} C_{\\mathbf{r}}^{A}(\\mathbf{p}, d) Local Aggregation(LGA),\u9700\u8981\u7684guidence\u6743\u91cd\u5927\u5c0f\u4e3a H\\times W\\times 3K^2 \\times F : \\begin{array}{c} C^{A}(\\mathbf{p}, d)=\\operatorname{sum}\\left\\{\\begin{array}{l} \\sum_{\\mathbf{q} \\in N_{\\mathrm{p}}} \\omega_{0}(\\mathbf{p}, \\mathbf{q}) \\cdot C(\\mathbf{q}, d) \\\\ \\sum_{\\mathbf{q} \\in N_{\\mathrm{p}}} \\omega_{1}(\\mathbf{p}, \\mathbf{q}) \\cdot C(\\mathbf{q}, d-1) \\\\ \\sum_{\\mathbf{q} \\in N_{\\mathrm{p}}} \\omega_{2}(\\mathbf{p}, \\mathbf{q}) \\cdot C(\\mathbf{q}, d+1) \\end{array}\\right. \\\\ \\text { s.t. } \\sum_{\\mathbf{q} \\in N_{\\mathrm{p}}} \\omega_{0}(\\mathbf{p}, \\mathbf{q})+\\omega_{1}(\\mathbf{p}, \\mathbf{q})+\\omega_{2}(\\mathbf{p}, \\mathbf{q})=1 \\end{array}","title":"GANet"},{"location":"other_categories/Summaries/Collections_StereoMatching_KITTI/#aanet","text":"pdf code \u672c\u6587\u4f7f\u7528coorelation\u7684\u65b9\u5f0f\u751f\u62103D Cost Volume. Adaptive Intra-Scale Aggregation\u672c\u8d28\u4e0a\u662f\u5206\u7ec4\u7684\u53ef\u53d8\u5377\u79ef\uff1a \\tilde{\\boldsymbol{C}}(d, \\boldsymbol{p})=\\sum_{k=1}^{K^{2}} w_{k} \\cdot \\boldsymbol{C}\\left(d, \\boldsymbol{p}+\\boldsymbol{p}_{k}+\\Delta \\boldsymbol{p}_{k}\\right) \\cdot m_{k} \u591ascale\u878d\u5408\uff0c\u8fd9\u91cc\u91c7\u7528\u7684\u662f HRNet \u7684\u65b9\u6cd5 \\hat{\\boldsymbol{C}}^{s}=\\sum_{k=1}^{S} f_{k}\\left(\\tilde{\\boldsymbol{C}}^{k}\\right), \\quad s=1,2, \\cdots, S f_{k}=\\left\\{\\begin{array}{l} \\mathcal{I}, \\quad k=s \\\\ (s-k) \\text { stride }-2\\oplus 3 \\times 3 \\text { convs, } \\quad k<s \\\\ \\text { upsampling } \\oplus 1 \\times 1 \\text { conv, } \\quad k>s \\end{array}\\right.","title":"AANet"},{"location":"other_categories/Summaries/Collections_StereoMatching_KITTI/#decomposenet","text":"pdf \u8fd9\u7bc7paper\u7684\u601d\u8def\u8fd8\u662f\u964d\u4f4e\u590d\u6742\u5ea6\uff0c\u5728\u6700\u4f4e\u7684\u5206\u8fa8\u7387\u4e0b\u8ba1\u7b97 Full Stereo Matching \u7684 Cost Volume. \u5728\u9ad8\u5206\u8fa8\u7387\u4e0b\u8fdb\u884c Sparse Matching. \u4e00\u5f20\u56fe\u6982\u5ff5\u4e0a\u53ef\u4ee5\u5206\u4e3a\u4e24\u79cd\u533a\u57df\uff0c\u4e00\u79cd\u662f\u7c97\u5206\u7c7b\u533a\u57df CA \uff0c\u8fd9\u91cc\u7684\u53cc\u76ee\u5339\u914d\u7ed3\u679c\u53ef\u4ee5\u4ece\u4f4e\u5206\u8fa8\u7387\u4e0a\u91c7\u6837\u540erefine\u51fa\u6765\uff1b\u4e00\u79cd\u662f\u7ec6\u5206\u7c7b\u533a\u57df FA \uff0c\u8fd9\u91cc\u7684\u53cc\u76ee\u7531\u9ad8\u5206\u8fa8\u7387\u7684sparse matching\u8fd8\u539f. \\begin{array}{c} \\hat{D}_{L}=\\widehat{\\mathcal{F}}\\left(\\mathrm{FA}_{L}, \\mathrm{FA}_{L}\\right) \\\\ \\vdots \\\\ \\hat{D}_{1}=\\widehat{\\mathcal{F}}\\left(\\mathrm{FA}_{1}, \\mathrm{FA}_{1}\\right) \\\\ D_{0}=\\mathcal{F}\\left(\\dot{\\mathrm{A}}_{0}, \\dot{\\mathrm{A}}_{0}\\right) \\\\ \\tilde{D}=\\hat{D}_{L} \\cup \\cdots \\cup \\hat{D}_{1} \\cup D_{0} \\end{array} \u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u81ea\u76d1\u7763\u7684\u65b9\u6848\u8bad\u7ec3\u4e00\u4e2a\u5c0f\u7f51\u7edc\u8bc6\u522b\u56fe\u4e2d\u88abdownsampling\u7834\u574f\u7684\u7279\u5f81, \u5176\u8f93\u5165\u662f\u672c\u5c42\u7684\u7279\u5f81 F_l \u4ee5\u53ca\u4e0b\u5c42\u4e0a\u91c7\u6837\u7684 F'_{l-1} \u7684\u7279\u5f81\u5dee F_l - F'_{l-1} , \u8f93\u51fa\u7684\u7f51\u7edc\u53ef\u4ee5\u88ab\u8bad\u7ec3\uff0c\u8bad\u7ec3\u76ee\u6807\u662f\u589e\u52a0 FA \u533a\u57df\u7684\u7279\u5f81\u5dee\uff0c\u540c\u65f6\u8981\u6c42\u5176\u7a00\u758f. \\mathcal{L}_{l}^{\\mathrm{DLD}}=\\left|F A_{l}\\right|-\\alpha \\frac{\\sum_{(h, w) \\in F A_{l}}\\left\\|F_{l}(h, w)-F_{l-1}^{\\prime}(h, w)\\right\\|_{2}}{\\left|F A_{l}\\right|} \u5728\u7a00\u758fMask\u4e0a\u76f4\u63a5\u8ba1\u7b97disparity: $$ \\begin{aligned} C_{l}(h, w, d)&=<{F} {l}^{left}(h, w), {F} {l}^{right}(h, w-d)> \\ P_{l}(h, w, d) &=\\frac{\\mathrm{e}^{C_{l}(h, w, d)-C_{l}^{\\max }(h, w)}}{\\sum_{d=0} \\mathrm{e}^{C_{l}(h, w, d)-C_{l}^{\\max }(h, w)}} \\ C_{l}^{\\max }(h, w) &=\\max {d} C {l}(h, w, d) \\ \\hat{D} {l}(h, w)&=\\sum {d=0} P_{l}(h, w, d) * d \\end{aligned} $$","title":"DecomposeNet"},{"location":"other_categories/Summaries/Collections_StereoMatching_KITTI/#nvstereonet","text":"pdf code \u635f\u5931\u4e0e monodepth \u76f8\u4f3c L=\\lambda_{1} E_{\\text {image}}+\\lambda_{2} E_{\\text {lidar}}+\\lambda_{3} E_{l r}+\\lambda_{4} E_{d s} \\begin{aligned} E_{\\text {image}} &=E_{\\text {image}}^{l}+E_{\\text {image}}^{r} \\\\ E_{\\text {lidar}} &=\\left|d_{l}-\\bar{d}_{l}\\right|+\\left|d_{r}-\\bar{d}_{r}\\right| \\\\ E_{l r} &=\\frac{1}{n} \\sum_{i j}\\left|d_{i j}^{l}-\\tilde{d}_{i j}^{l}\\right|+\\frac{1}{n} \\sum_{i j}\\left|d_{i j}^{r}-\\tilde{d}_{i j}^{r}\\right| \\\\ E_{d s} &=E_{d s}^{l}+E_{d s}^{r} \\end{aligned} \\begin{aligned} E_{\\text {image}}^{l} &=\\frac{1}{n} \\sum_{i, j} \\alpha \\frac{1-\\operatorname{SSIM}\\left(I_{i j}^{l}, \\tilde{I}_{i j}^{l}\\right)}{2}+(1-\\alpha) | I_{i j}^{l}-\\tilde{I}_{i j}^{l} \\\\ E_{d s}^{l} &=\\frac{1}{n} \\sum_{i, j}\\left|\\partial_{x} d_{i j}^{l}\\right| e^{-\\left\\|\\partial_{x} I_{i, j}^{l}\\right\\|}+\\left|\\partial_{y} d_{i j}^{l}\\right| e^{-\\| \\partial_{y} I_{i, j}^{l}} \\| \\end{aligned}","title":"NVStereoNet"},{"location":"other_categories/Summaries/Collections_StereoMatching_KITTI/#sttr-revisiting-stereo-depth-estimation-from-a-sequence-to-sequence-perspective-with-transformers","text":"pdf code \u8fd9\u7bc7paper\u7528Sequence/Transformer\u7684\u89d2\u5ea6\u91cd\u65b0\u7406\u89e3\u53cc\u76eepixel-wise matching. \u4f5c\u8005\u6307\u51fa\u4e3b\u8981\u6709\u4e09\u5927\u4f18\u52bf: \u4e0d\u518d\u5f3a\u7ea6\u675f\u8981\u6c42\u4e00\u4e2a\u56fa\u5b9a\u7684disparity\u4e0a\u9650 \u80fd\u8bc6\u522b\u906e\u6321\u533a\u57df\uff0c\u7ed9\u51faconfidence \u7ed9\u51fauniqueness\u7ea6\u675f (\u8fd9\u4e2a\u662f\u901a\u8fc7\u6c42\u89e3\u4e00\u4e2a\u6700\u4f18\u5316\u95ee\u9898\u5b9e\u73b0\u7684)","title":"STTR: Revisiting Stereo Depth Estimation From a Sequence-to-Sequence Perspective with Transformers"},{"location":"other_categories/Summaries/Collections_StereoMatching_KITTI/#_1","text":"\u672c\u6587\u7684\u7f51\u7edc\u7ed3\u6784\u7684\u6bcf\u4e00\u4e2a\u90e8\u4ef6\u90fd\u662f\u57fa\u7840\u90e8\u4ef6\uff0c\u8bbe\u8ba1\u4e0a\u6216\u8005\u4ee3\u7801\u4e0a\u4e3b\u8981\u7684\u4e0d\u540c\u70b9: backbone\u7f51\u7edc\u91c7\u7528\u7684\u662f\u7c7b\u4f3c\u4e8eU-Net\u7684\u7ed3\u6784,\u4f46\u662f\u5728\u6700\u540e\u4e00\u5c42\u91c7\u6837\u540e\u52a0\u4e86\u4e00\u4e2a DeepLabv3 \u7684ASPP multiheader transformer\u91cc\u9762\u4ea4\u9519\u4f7f\u7528self-attention\u4e0ecross attention.\u800c\u4ee3\u7801\u4e0a\u4e24\u8005\u5206\u522b\u53ea\u6709\u4e00\u4e2amulti-head\u7684ModuleList, \u4e5f\u5c31\u662f\u56fe\u4e2d\u91cd\u590d\u7684\u6a21\u5757\u4f7f\u7528\u7684\u662f\u76f8\u540c\u7684\u53c2\u6570. attention\u662f\u5148\u8c03\u6574\u5f62\u72b6 [1, C, H, W] \\rightarrow [H, W, C] , \u8f93\u51fa\u76f8\u5f53\u4e8e\u662f\u5728 W \u4e0a\u5c55\u5f00\u4e00\u4e2a\u4e2a\u50cf\u7d20\u5339\u914d\uff0c\u4e5f\u5c31\u662f\u7406\u89e3\u4e3a\u53ea\u5728\u6c34\u5e73 epipolar line\u4e0a\u8fdb\u884c\u5339\u914d, \u8f93\u51fa\u5f62\u72b6 [H, W, W] Position Embedding \u91c7\u7528\u7684\u662f\u4f5c\u8005\u63d0\u51fa\u7684\u76f8\u5bf9\u4f4d\u7f6eencode, \u8fd9\u4e2aencoding\u662f\u4e00\u4e2a [B, C, H, \\times (2W-1)] , \u7684encoder. \u7531\u4e8e\u591a\u5c42multihead\u7684transformer\u8fd0\u7b97\u91cf\u548c\u663e\u5b58\u5360\u7528\u5f88\u5927\uff0c\u4ee3\u7801\u5b9e\u73b0\u4e0a\u9700\u8981\u7528 checkpoint \u6280\u5de7\uff0c\u4e5f\u5c31\u662f\u6682\u65f6\u4e0d\u5b58\u50a8\u4e2d\u95f4\u8fd0\u7b97\u53d8\u91cf\uff0c\u5728backward\u7684\u65f6\u5019\u518d\u91cd\u65b0\u524d\u4f20\u8ba1\u7b97\u4e2d\u95f4\u8fd0\u7b97\u53d8\u91cf. \u53bb\u5230\u6700\u540e\u4e00\u5c42\u7684\u65f6\u5019\u4f5c\u8005\u6307\u51fa\u5e94\u8be5\u8003\u8651\u76f8\u673a\u5de6\u53f3\u653e\u7f6e\u7684\u5173\u7cfb\uff0c\u53f3feature\u4e0e\u5de6feature\u8ba1\u7b97attention\u7684\u65f6\u5019\u5e94\u8be5\u53ea\u8ba1\u7b97\u90e8\u5206\u7684attention\u503c (\u4e0a\u534a\u4e09\u89d2).","title":"\u6574\u4f53\u7ed3\u6784"},{"location":"other_categories/Summaries/Collections_StereoMatching_KITTI/#optimal-transport","text":"\u4ece\u7f51\u7edc\u4e2d\u5f97\u5230 W \\times W \u5339\u914d\u635f\u5931\u77e9\u9635\u540e\uff0c\u6211\u4eec\u5e0c\u671b\u5f97\u5230\u6bcf\u4e00\u4e2a\u50cf\u7d20\u7684\u6700\u4f18\u7684, unique\u7684\u7ed3\u679c. \u8fd9\u4e2a\u95ee\u9898\u53ef\u4ee5\u88abformulate\u4e3a\u4e00\u4e2a optimal transport\u7684\u95ee\u9898, \u6709\u4e00\u4e2a\u4ece\u5206\u751c\u54c1motivate\u7684\u6bd4\u8f83\u597d\u7684\u535a\u5ba2 . \u7528STTR\u8fd9\u7bc7paper\u7684\u8bed\u8a00\u8fdb\u884c\u95ee\u9898\u8868\u8ff0: \\begin{aligned} \\underset{\\vec P}{\\text{minimize}} \\quad &\\underset{i,j}{\\sum}P_{ij} M_{ij} + \\frac{1}{\\lambda} \\sum_{i,j} P_{i,j} \\log P_{i,j} \\\\ \\text{subject to} \\quad & \\sum_i P_{i,j} = c_j\\\\ & \\sum_j P_{i,j} = r_i\\\\ \\end{aligned} \u76ee\u6807\u51fd\u6570\u7684\u7b2c\u4e00\u9879\u4e3a\u7531\u635f\u5931\u51fd\u6570\u77e9\u9635\u51b3\u5b9a\u7684\u7ebf\u6027\u51fd\u6570\uff0c\u7b2c\u4e8c\u9879\u4e3a\u4ea4\u53c9\u71b5\u3002\u7ea6\u675f\u51fd\u6570\u8981\u6c42\u6bcf\u4e00\u4e2a\u5de6\u56fe\u7684\u50cf\u7d20\u53ea\u4f1a\u5bf9\u5e94\u4e00\u4e2a\u53f3\u56fe\u7684\u50cf\u7d20(\u6982\u7387\u548c\u4e3a1)\uff0c \u6bcf\u4e00\u4e2a\u53f3\u56fe\u7684\u50cf\u7d20\u4e5f\u53ea\u4f1a\u5bf9\u5e94\u4e00\u4e2a\u5de6\u56fe\u7684\u50cf\u7d20. Optimal Transport\u662f\u4e00\u4e2a \u51f8\u4f18\u5316\u4e2d\u7684LP\u95ee\u9898\uff0c\u4f7f\u7528 KKT \u6761\u4ef6\u4e2d\u5bf9 P \u7684\u5bfc\u6570\u4e3a\u96f6\u53ef\u4ee5\u5f97\u5230: P_{ij} = \\alpha_i \\beta_j e^{-\\lambda M} \u5176\u4e2d \\alpha \u548c \\beta \u4e0e\u62c9\u683c\u6717\u65e5\u4e58\u5b50\u548c\u8d85\u53c2\u6570 \\lambda \u6709\u5173\uff0c\u4ece\u7ea6\u675f\u4e2d\u6c42\u89e3\u83b7\u5f97. Sinkhorn Distances\u7684\u539f\u59cb\u4ee3\u7801\u5219\u662f: def compute_optimal_transport(M, r, c, lam, epsilon=1e-8): \"\"\" Computes the optimal transport matrix and Slinkhorn distance using the Sinkhorn-Knopp algorithm. From https://michielstock.github.io/OptimalTransport/ Inputs: - M : cost matrix (n x m) - r : vector of marginals (n, ) - c : vector of marginals (m, ) - lam : strength of the entropic regularization - epsilon : convergence parameter Outputs: - P : optimal transport matrix (n x m) - dist : Sinkhorn distance \"\"\" n, m = M.shape P = np.exp(- lam * M) P /= P.sum() u = np.zeros(n) # normalize this matrix while np.max(np.abs(u - P.sum(1))) > epsilon: u = P.sum(1) P *= (r / u).reshape((-1, 1)) # normalize in first dimension P *= (c / P.sum(0)).reshape((1, -1)) #normalize in second dimension return P, np.sum(P * M) \u672c\u6587\u7684\u4ee3\u7801\u5219\u662f\u9ed8\u8ba4 \\lambda \u4e3a1\uff0c\u5728 \\log \u7a7a\u95f4\u4e2d\u8fdb\u884c\u8fed\u4ee3\u8ba1\u7b97\u3002","title":"Optimal Transport"},{"location":"other_categories/Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/","text":"Machine Learning for Robot Planning and Control from Byron Boots Georgia Tech Robot Learning Lab \u672c\u6587\u5148\u8ba8\u8bba\u5f00\u8f66\u8fd9\u4e2a\u95ee\u9898\uff0c\u5e76\u5f15\u51fa\u6df1\u5ea6\u5b66\u4e60\u5728\u673a\u5668\u4eba\u63a7\u5236\u4e0e\u89c4\u5212\u4e2d\u7684\u5229\u7528\u7684\u4e00\u4e2a\u4e09\u89d2\u56fe\u3002 MPC \u7b2c\u4e00\u90e8\u5206\u5148\u96c6\u4e2d\u8ba8\u8bbaMPC\uff0cMPC\u662f\u4e00\u4e2a\u6210\u529f\u7684\u7b97\u6cd5\u4ee5\u53ca\u89e3\u51b3\u95ee\u9898\u7684\u65b9\u5f0f\uff0c\u4f46\u662f\u5927\u5bb6\u666e\u904d\u8ba4\u4e3a\u6210\u529f\u7684\u4f20\u7edfMPC\u6709\u4e00\u4e9b\u95ee\u9898\u3002\u603b\u7ed3\u8d77\u6765\uff0cMPC\u95ee\u9898\u4e00\u662f\u975e\u7ebf\u6027(\u52a8\u529b\u5b66\u4e0e\u7ea6\u675f)\uff0c\u95ee\u9898\u4e8c\u662f\u6a21\u578b\u4e0d\u51c6\u786e\u3002 \u7136\u540e\u4ecb\u7ecd\u4e86 MPPI\u7b97\u6cd5 \u8fd9\u91cc\u63cf\u8ff0\u7684\u53e6\u4e00\u4e2a\u95ee\u9898\u662f\u5728\u4f7f\u7528MPPI\u7b97\u6cd5\u5b9e\u65f6\u5b66\u4e60\u7684\u65f6\u5019\uff0c\u5b58\u5728\u4e00\u4e2a\u63a2\u7d22\u7684\u95ee\u9898\uff0c\u8f93\u5165\u9700\u8981\u4e30\u5bcc\uff0c\u540c\u65f6\u4e0e\u5b9e\u9645\u7684\u8f93\u5165\u5206\u5e03\u76f8\u4f3c\u3002 \u8981\u7f13\u89e3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6f14\u8bb2\u8005\u4f7f\u7528\u7684\u5176\u4e2d\u4e00\u4e2a\u65b9\u6cd5\u662f\u5148\u7528human driver\u5f97\u5230\u521d\u59cb\u6570\u636e\uff0c\u5bf9\u6a21\u578b\u8fdb\u884c\u521d\u59cb\u8bad\u7ec3(bootstrapping)\u3002 Imitation \u63a5\u7740\u6211\u4eec\u8ba8\u8bba\u6a21\u4eff\u5b66\u4e60\u4ee5\u53ca\u5728\u4e13\u5bb6\u5f15\u5bfc\u4e0b\u7684\u63a2\u7d22\u95ee\u9898\u3002 \u8fd9\u91cc\u6307\u5411\u4e86 AggreVaTeD (Aggregate Values to Imitate) [Sun, Venkatraman, Gordon, Boots, Bagnell; ICML 2017]\u3002 \u6682\u672a\u6536\u5f55\u6b64\u8bba\u6587 Parameterized a Robot \u8fd9\u91cc\u6307\u5411\u4e86differentiable MPC(\u8fd9\u7bc7\u8bba\u6587\u53c8\u5f88\u91cd\u8981\u5730\u53c2\u8003\u4e86OptNet)\uff0c","title":"Machine Learning for Robot Planning and Control from Byron Boots Georgia Tech Robot Learning Lab"},{"location":"other_categories/Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/#machine-learning-for-robot-planning-and-control-from-byron-boots-georgia-tech-robot-learning-lab","text":"\u672c\u6587\u5148\u8ba8\u8bba\u5f00\u8f66\u8fd9\u4e2a\u95ee\u9898\uff0c\u5e76\u5f15\u51fa\u6df1\u5ea6\u5b66\u4e60\u5728\u673a\u5668\u4eba\u63a7\u5236\u4e0e\u89c4\u5212\u4e2d\u7684\u5229\u7528\u7684\u4e00\u4e2a\u4e09\u89d2\u56fe\u3002","title":"Machine Learning for Robot Planning and Control from Byron Boots Georgia Tech Robot Learning Lab"},{"location":"other_categories/Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/#mpc","text":"\u7b2c\u4e00\u90e8\u5206\u5148\u96c6\u4e2d\u8ba8\u8bbaMPC\uff0cMPC\u662f\u4e00\u4e2a\u6210\u529f\u7684\u7b97\u6cd5\u4ee5\u53ca\u89e3\u51b3\u95ee\u9898\u7684\u65b9\u5f0f\uff0c\u4f46\u662f\u5927\u5bb6\u666e\u904d\u8ba4\u4e3a\u6210\u529f\u7684\u4f20\u7edfMPC\u6709\u4e00\u4e9b\u95ee\u9898\u3002\u603b\u7ed3\u8d77\u6765\uff0cMPC\u95ee\u9898\u4e00\u662f\u975e\u7ebf\u6027(\u52a8\u529b\u5b66\u4e0e\u7ea6\u675f)\uff0c\u95ee\u9898\u4e8c\u662f\u6a21\u578b\u4e0d\u51c6\u786e\u3002 \u7136\u540e\u4ecb\u7ecd\u4e86 MPPI\u7b97\u6cd5 \u8fd9\u91cc\u63cf\u8ff0\u7684\u53e6\u4e00\u4e2a\u95ee\u9898\u662f\u5728\u4f7f\u7528MPPI\u7b97\u6cd5\u5b9e\u65f6\u5b66\u4e60\u7684\u65f6\u5019\uff0c\u5b58\u5728\u4e00\u4e2a\u63a2\u7d22\u7684\u95ee\u9898\uff0c\u8f93\u5165\u9700\u8981\u4e30\u5bcc\uff0c\u540c\u65f6\u4e0e\u5b9e\u9645\u7684\u8f93\u5165\u5206\u5e03\u76f8\u4f3c\u3002 \u8981\u7f13\u89e3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6f14\u8bb2\u8005\u4f7f\u7528\u7684\u5176\u4e2d\u4e00\u4e2a\u65b9\u6cd5\u662f\u5148\u7528human driver\u5f97\u5230\u521d\u59cb\u6570\u636e\uff0c\u5bf9\u6a21\u578b\u8fdb\u884c\u521d\u59cb\u8bad\u7ec3(bootstrapping)\u3002","title":"MPC"},{"location":"other_categories/Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/#imitation","text":"\u63a5\u7740\u6211\u4eec\u8ba8\u8bba\u6a21\u4eff\u5b66\u4e60\u4ee5\u53ca\u5728\u4e13\u5bb6\u5f15\u5bfc\u4e0b\u7684\u63a2\u7d22\u95ee\u9898\u3002 \u8fd9\u91cc\u6307\u5411\u4e86 AggreVaTeD (Aggregate Values to Imitate) [Sun, Venkatraman, Gordon, Boots, Bagnell; ICML 2017]\u3002 \u6682\u672a\u6536\u5f55\u6b64\u8bba\u6587","title":"Imitation"},{"location":"other_categories/Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/#parameterized-a-robot","text":"\u8fd9\u91cc\u6307\u5411\u4e86differentiable MPC(\u8fd9\u7bc7\u8bba\u6587\u53c8\u5f88\u91cd\u8981\u5730\u53c2\u8003\u4e86OptNet)\uff0c","title":"Parameterized a Robot"},{"location":"other_categories/Summaries/SelfAttentionandCNN/","text":"Summary of Self Attention / Transformer in Vision System (Last update 2021-07-14) \u8fd9\u662f\u4e00\u4efd\u63cf\u8ff0\u6570\u7bc7\u5173\u4e8e\u5728CNN\u7cfb\u7edf\u4e2d\u4f7f\u7528self-attention\u7684\u5c0f\u7edf\u8ba1. \u5bf9\u4e8e\u5728NLP\u53d6\u5f97\u5f88\u597d\u8868\u73b0\u7684self-attention\u673a\u5236\uff0c\u672c\u7f51\u7ad9\u5728\u8bba\u6587 Attention is all you need \u6709\u8be6\u7ec6\u7684\u4ecb\u7ecd\u3002 2019\u5e74\u4e0a\u534a\u5e74\uff0cGoogle \u63d0\u51fa\u4e86 Attention Augmented Convolution , \u8fd9\u662f\u4e00\u4e2a\u7c7b\u4f3c Non-local \u6a21\u5757\u7684\u601d\u8def\uff0c\u501f\u7528self-attention\u7684\u673a\u5236\u52a0\u4e0apositional-encoding\uff0c\u8bbe\u8ba1\u51fa\u4e00\u4e2a\u63d0\u4f9b\u5168\u5c40attention\u7684\u6a21\u5757\uff0c\u8fd9\u4e2a\u6a21\u5757\u7684\u7f3a\u70b9\u5728\u4e8e\u5728\u56fe\u7247\u5f88\u5927\u7684\u65f6\u5019\u4f1a\u9700\u8981\u4e00\u4e2a\u5f88\u5927\u7684\u77e9\u9635\u4e58\u6cd5\uff0c\u6240\u4ee5\u8fd9\u4e2a\u6a21\u5757\u5fc5\u987b\u53ea\u80fd\u5728\u591a\u6b21\u4e0b\u91c7\u6837\u540e\u4f7f\u7528\uff0c\u4e14\u8fd8\u9700\u8981\u6ce8\u91cd\u663e\u5b58\u7ba1\u7406\uff0c\u53ef\u4ee5\u7406\u89e3\u4e3a\u662f\u4e00\u4e2a\u96be\u4ee5scale up\u7684\u65b9\u6848\u3002 \u672c\u6587\u63a5\u4e0b\u6765\u4f1a\u4ecb\u7ecd\u4e24\u7bc7paper\uff0c\u4e00\u7bc7paper\u63d0\u51fa\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u56fe\u7247\u5c40\u90e8attention\u7684\u6a21\u5757\uff0c\u7528\u7565\u5c11\u4e8e\u4f20\u7edfConv\u7684\u8fd0\u7b97\u4e0e\u53c2\u6570\uff0c\u5728imagenet\u548cCoco\u5206\u522b\u5f97\u5230\u4e86\u4e0e\u4f20\u7edfCNN\u51e0\u4e4e\u4e00\u81f4\u7684\u6027\u80fd\u3002\u53e6\u4e00\u7bc7\u9610\u8ff0\u4e86\u5c40\u90e8Attention\u4e0eConvolution\u7684\u5173\u7cfb\uff0c\u8868\u660eMulti-head \u5c40\u90e8Attention\u53ef\u4ee5\u5b9e\u73b0\u4f20\u7edfConvolution\u7684\u6027\u80fd\u3002 Stand-Alone Self-Attention in Vision Models pdf code y_{i j}=\\sum_{a, b \\in \\mathcal{N}_{k}(i, j)} \\operatorname{softmax}_{a b}\\left(q_{i j}^{\\top} k_{a b}+q_{i j}^{\\top} r_{a-i, b-j}\\right) v_{a b} \u8fd9\u4e2a\u6a21\u5757\u7684\u8bbe\u8ba1\u53ef\u4ee5\u76f4\u63a5CNN\uff0c\u540c\u65f6\u8fd0\u7b97\u91cf\u5e76\u6ca1\u6709\u663e\u7136\u63d0\u5347\uff0c\u6027\u80fd\u5219\u76f8\u8fd1\u3002 \u4f5c\u8005\u901a\u8fc7\u5b9e\u9a8c\u53d1\u73b0\u9664\u4e86\u7b2c\u4e00\u4e2aConv\u5efa\u8bae\u7528Convolution\uff0c\u6a21\u578b\u7684\u5176\u4f59\u5377\u79ef\u6a21\u5757\u5c31\u53ef\u4ee5\u7528Attention\u66ff\u4ee3\u3002\u4f5c\u8005\u8fd8\u505a\u4e86\u66f4\u591a\u7684\u5176\u4ed6\u5b9e\u9a8c\uff0c\u8bc1\u660eattention\u66ff\u4ee3CNN\u662f\u5927\u6709\u53ef\u4e3a\u7684\u3002 On The Relationship Between Self-Attention and Convolution Layers pdf code \u672c\u6587\u8fd8\u6709\u4e00\u4e2a \u5b98\u65b9\u7f51\u7ad9 \u4ee5\u53ca \u5b98\u65b9\u82f1\u6587\u535a\u5ba2 \u7406\u8bba\u7ed3\u8bba\u662f\u5c40\u90e8Attention\u662fCNN\u7684\u6269\u5c55\uff0c\u5177\u4f53implementation\u6709\u533a\u522b\u3002 Swin Transformer: Hierarchical Vision Transformer using Shifted Windows pdf code \u8fd9\u7bc7paper\u5176\u5b9e\u6709\u70b9\u56de\u5230local\u4e86\uff0c\u4f46\u662f\u66f4\u52a0\u53ef\u9760\u4e86\uff0c\u6bcf\u6b21\u53ea\u5bf9\u7a97\u53e3\u5185\u7684\u8dd1transformer, \u4f9d\u9760\u591a\u5c42\u7ea7\u9010\u6e10\u63d0\u5347\u611f\u53d7\u91ce. Scaling Vision Transformers pdf \u8fd9\u7bc7google\u7684paper\u5728JFT-3B\u4ee5\u53ca\u6570\u5343TPU\u7684\u52a0\u6301\u4e0b\u8bad\u7ec3\u4e86\u4e00\u4e2aSOTA\u7684Transformer.\u6539\u8fdb\u4e86ViT\u7684\u67b6\u6784\u548c\u8bad\u7ec3\uff0c\u51cf\u5c11\u4e86\u5185\u5b58\u6d88\u8017\u5e76\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u51c6\u786e\u6027. BLOG \u65b9\u6848 Decouple weight decay for the head. \u8ba9\u8f93\u51fa\u5934\u7684weight decay\u66f4\u5927\u3002 Save memory by removing the [class] token Scale up data Memory-efficient optimizers: adafactor optimizer Learning-rate schedule: Reciprocal-square root VOLO: Vision Outlooker for Visual Recognition PDF code \u8fd9\u7bc7paper\u5728\u4e0d\u4f7f\u7528\u989d\u5916\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u9996\u6b21\u7a81\u7834\u4e8687%\u7684ImageNet\u51c6\u786e\u7387. \u4e0e\u5176\u4ed6paper\u76f8\u6bd4\uff0c\u672c\u6587\u66f4\u4e13\u6ce8\u4e8e\u5bf9local feature\u7684\u5f62\u6210\u3002 outlook\u7684\u65b9\u6cd5\u5c31\u662f\u5c06 K\\times K \u7684\u6838\u5c55\u5f00\uff0c\u5bf9\u6bcf\u4e00\u4e2a\u70b9\uff0c\u8ba1\u7b97\u5176\u9644\u8fd1 K\\times K \u4e2a\u50cf\u7d20\u7684\u81ea\u76f8\u4f3c\u77e9\u9635 (K\\times K) \\times (K\\times K) , \u7136\u540e\u878d\u5408\u8fd9\u4e9b\u8fd1\u90bb\u7684features. A Survey on Visual Transformer pdf A Comprehensive Study of Vision Transformers on Dense Prediction Tasks pdf \u8fd9\u7bc7\u8bba\u6587\u6bd4\u8f83\u4e86CNN\u4e0eTransformer\u7684\u6027\u80fd\u4ee5\u53ca\u6269\u5c55\u6027\u533a\u522b\uff0c\u5173\u6ce8\u4e8e2D\u68c0\u6d4b\u4ee5\u53ca\u8bed\u4e49\u5206\u5272\u3002 \u7ed3\u8bba\uff1a ViT\u4e0eCNN\u76f8\u6bd4\uff0c\u5728\u539f\u6570\u636e\u96c6\u4e0a\u7cbe\u5ea6\u66f4\u4f4e\uff0c\u7406\u8bba\u8fd0\u7b97\u590d\u6742\u5ea6\u66f4\u4f4e\uff0c\u4f46\u662f\u901f\u5ea6\u66f4\u6162\u3002 \u4f5c\u8005\u8ba4\u4e3a\u5982\u679cGPU\u4e3aTransformer\u4f18\u5316\uff0c\u5219Transformer\u6709\u5728\u901f\u5ea6\u4e0a\u8d85\u8d8aCNN\u7684\u6f5c\u529b\u3002 ViT\u5728\u8de8\u6570\u636e\u96c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u66f4\u597d\uff0c\u8bed\u4e49\u5206\u5272\u4e0a\u6bd4\u76ee\u6807\u68c0\u6d4b\u66f4\u660e\u663e\u3002\u5206\u6790\u5176\u6536\u655b\u7684Local minima\u635f\u5931landscape, \u8ba4\u4e3aViT\u6536\u655b\u5230\u7684local minima\u66f4\u5e73\u6ed1\u3002(\u505a\u6cd5\u4e0a\u6270\u52a8\u7f51\u7edc\u7684\u6743\u91cd\uff0c\u635f\u5931\u5728\u8bad\u7ec3\u96c6\u4e0a\u6d4b\u8bd5) ViT\u624d\u6d4b\u8bd5\u96c6\u66f4well-calibrated\u3002 ViT\u5bf9\u566a\u58f0\u4ee5\u53ca\u5bf9\u6297\u653b\u51fb\u7684\u62b5\u6297\u6027\u66f4\u5f3a\u3002 \u5bf9\u4e8e\u9ad8\u5206\u8fa8\u8f93\u5165\uff0cCNN\u7684\u6027\u80fd\u4fdd\u5b58\u6bd4ViT\u66f4\u597d\uff0c\u5c3d\u7ba1ViT\u7406\u8bba\u4e0a\u6709\u66f4\u5927\u7684\u611f\u53d7\u91ce\uff0c\u4f46\u662f\u6027\u80fd\u53cd\u800c\u4e0d\u884c\uff0c\u4f5c\u8005\u8ba4\u4e3a\u548cPositional Embedding\u6709\u5173\u7cfb\u3002","title":"Summary of Self Attention / Transformer in Vision System (Last update 2021-07-14)"},{"location":"other_categories/Summaries/SelfAttentionandCNN/#summary-of-self-attention-transformer-in-vision-system-last-update-2021-07-14","text":"\u8fd9\u662f\u4e00\u4efd\u63cf\u8ff0\u6570\u7bc7\u5173\u4e8e\u5728CNN\u7cfb\u7edf\u4e2d\u4f7f\u7528self-attention\u7684\u5c0f\u7edf\u8ba1. \u5bf9\u4e8e\u5728NLP\u53d6\u5f97\u5f88\u597d\u8868\u73b0\u7684self-attention\u673a\u5236\uff0c\u672c\u7f51\u7ad9\u5728\u8bba\u6587 Attention is all you need \u6709\u8be6\u7ec6\u7684\u4ecb\u7ecd\u3002 2019\u5e74\u4e0a\u534a\u5e74\uff0cGoogle \u63d0\u51fa\u4e86 Attention Augmented Convolution , \u8fd9\u662f\u4e00\u4e2a\u7c7b\u4f3c Non-local \u6a21\u5757\u7684\u601d\u8def\uff0c\u501f\u7528self-attention\u7684\u673a\u5236\u52a0\u4e0apositional-encoding\uff0c\u8bbe\u8ba1\u51fa\u4e00\u4e2a\u63d0\u4f9b\u5168\u5c40attention\u7684\u6a21\u5757\uff0c\u8fd9\u4e2a\u6a21\u5757\u7684\u7f3a\u70b9\u5728\u4e8e\u5728\u56fe\u7247\u5f88\u5927\u7684\u65f6\u5019\u4f1a\u9700\u8981\u4e00\u4e2a\u5f88\u5927\u7684\u77e9\u9635\u4e58\u6cd5\uff0c\u6240\u4ee5\u8fd9\u4e2a\u6a21\u5757\u5fc5\u987b\u53ea\u80fd\u5728\u591a\u6b21\u4e0b\u91c7\u6837\u540e\u4f7f\u7528\uff0c\u4e14\u8fd8\u9700\u8981\u6ce8\u91cd\u663e\u5b58\u7ba1\u7406\uff0c\u53ef\u4ee5\u7406\u89e3\u4e3a\u662f\u4e00\u4e2a\u96be\u4ee5scale up\u7684\u65b9\u6848\u3002 \u672c\u6587\u63a5\u4e0b\u6765\u4f1a\u4ecb\u7ecd\u4e24\u7bc7paper\uff0c\u4e00\u7bc7paper\u63d0\u51fa\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u56fe\u7247\u5c40\u90e8attention\u7684\u6a21\u5757\uff0c\u7528\u7565\u5c11\u4e8e\u4f20\u7edfConv\u7684\u8fd0\u7b97\u4e0e\u53c2\u6570\uff0c\u5728imagenet\u548cCoco\u5206\u522b\u5f97\u5230\u4e86\u4e0e\u4f20\u7edfCNN\u51e0\u4e4e\u4e00\u81f4\u7684\u6027\u80fd\u3002\u53e6\u4e00\u7bc7\u9610\u8ff0\u4e86\u5c40\u90e8Attention\u4e0eConvolution\u7684\u5173\u7cfb\uff0c\u8868\u660eMulti-head \u5c40\u90e8Attention\u53ef\u4ee5\u5b9e\u73b0\u4f20\u7edfConvolution\u7684\u6027\u80fd\u3002","title":"Summary of Self Attention / Transformer in Vision System (Last update 2021-07-14)"},{"location":"other_categories/Summaries/SelfAttentionandCNN/#stand-alone-self-attention-in-vision-models","text":"pdf code y_{i j}=\\sum_{a, b \\in \\mathcal{N}_{k}(i, j)} \\operatorname{softmax}_{a b}\\left(q_{i j}^{\\top} k_{a b}+q_{i j}^{\\top} r_{a-i, b-j}\\right) v_{a b} \u8fd9\u4e2a\u6a21\u5757\u7684\u8bbe\u8ba1\u53ef\u4ee5\u76f4\u63a5CNN\uff0c\u540c\u65f6\u8fd0\u7b97\u91cf\u5e76\u6ca1\u6709\u663e\u7136\u63d0\u5347\uff0c\u6027\u80fd\u5219\u76f8\u8fd1\u3002 \u4f5c\u8005\u901a\u8fc7\u5b9e\u9a8c\u53d1\u73b0\u9664\u4e86\u7b2c\u4e00\u4e2aConv\u5efa\u8bae\u7528Convolution\uff0c\u6a21\u578b\u7684\u5176\u4f59\u5377\u79ef\u6a21\u5757\u5c31\u53ef\u4ee5\u7528Attention\u66ff\u4ee3\u3002\u4f5c\u8005\u8fd8\u505a\u4e86\u66f4\u591a\u7684\u5176\u4ed6\u5b9e\u9a8c\uff0c\u8bc1\u660eattention\u66ff\u4ee3CNN\u662f\u5927\u6709\u53ef\u4e3a\u7684\u3002","title":"Stand-Alone Self-Attention in Vision Models"},{"location":"other_categories/Summaries/SelfAttentionandCNN/#on-the-relationship-between-self-attention-and-convolution-layers","text":"pdf code \u672c\u6587\u8fd8\u6709\u4e00\u4e2a \u5b98\u65b9\u7f51\u7ad9 \u4ee5\u53ca \u5b98\u65b9\u82f1\u6587\u535a\u5ba2 \u7406\u8bba\u7ed3\u8bba\u662f\u5c40\u90e8Attention\u662fCNN\u7684\u6269\u5c55\uff0c\u5177\u4f53implementation\u6709\u533a\u522b\u3002","title":"On The Relationship Between Self-Attention and Convolution Layers"},{"location":"other_categories/Summaries/SelfAttentionandCNN/#swin-transformer-hierarchical-vision-transformer-using-shifted-windows","text":"pdf code \u8fd9\u7bc7paper\u5176\u5b9e\u6709\u70b9\u56de\u5230local\u4e86\uff0c\u4f46\u662f\u66f4\u52a0\u53ef\u9760\u4e86\uff0c\u6bcf\u6b21\u53ea\u5bf9\u7a97\u53e3\u5185\u7684\u8dd1transformer, \u4f9d\u9760\u591a\u5c42\u7ea7\u9010\u6e10\u63d0\u5347\u611f\u53d7\u91ce.","title":"Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"},{"location":"other_categories/Summaries/SelfAttentionandCNN/#scaling-vision-transformers","text":"pdf \u8fd9\u7bc7google\u7684paper\u5728JFT-3B\u4ee5\u53ca\u6570\u5343TPU\u7684\u52a0\u6301\u4e0b\u8bad\u7ec3\u4e86\u4e00\u4e2aSOTA\u7684Transformer.\u6539\u8fdb\u4e86ViT\u7684\u67b6\u6784\u548c\u8bad\u7ec3\uff0c\u51cf\u5c11\u4e86\u5185\u5b58\u6d88\u8017\u5e76\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u51c6\u786e\u6027. BLOG \u65b9\u6848 Decouple weight decay for the head. \u8ba9\u8f93\u51fa\u5934\u7684weight decay\u66f4\u5927\u3002 Save memory by removing the [class] token Scale up data Memory-efficient optimizers: adafactor optimizer Learning-rate schedule: Reciprocal-square root","title":"Scaling Vision Transformers"},{"location":"other_categories/Summaries/SelfAttentionandCNN/#volo-vision-outlooker-for-visual-recognition","text":"PDF code \u8fd9\u7bc7paper\u5728\u4e0d\u4f7f\u7528\u989d\u5916\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u9996\u6b21\u7a81\u7834\u4e8687%\u7684ImageNet\u51c6\u786e\u7387. \u4e0e\u5176\u4ed6paper\u76f8\u6bd4\uff0c\u672c\u6587\u66f4\u4e13\u6ce8\u4e8e\u5bf9local feature\u7684\u5f62\u6210\u3002 outlook\u7684\u65b9\u6cd5\u5c31\u662f\u5c06 K\\times K \u7684\u6838\u5c55\u5f00\uff0c\u5bf9\u6bcf\u4e00\u4e2a\u70b9\uff0c\u8ba1\u7b97\u5176\u9644\u8fd1 K\\times K \u4e2a\u50cf\u7d20\u7684\u81ea\u76f8\u4f3c\u77e9\u9635 (K\\times K) \\times (K\\times K) , \u7136\u540e\u878d\u5408\u8fd9\u4e9b\u8fd1\u90bb\u7684features.","title":"VOLO: Vision Outlooker for Visual Recognition"},{"location":"other_categories/Summaries/SelfAttentionandCNN/#a-survey-on-visual-transformer","text":"pdf","title":"A Survey on Visual Transformer"},{"location":"other_categories/Summaries/SelfAttentionandCNN/#a-comprehensive-study-of-vision-transformers-on-dense-prediction-tasks","text":"pdf \u8fd9\u7bc7\u8bba\u6587\u6bd4\u8f83\u4e86CNN\u4e0eTransformer\u7684\u6027\u80fd\u4ee5\u53ca\u6269\u5c55\u6027\u533a\u522b\uff0c\u5173\u6ce8\u4e8e2D\u68c0\u6d4b\u4ee5\u53ca\u8bed\u4e49\u5206\u5272\u3002 \u7ed3\u8bba\uff1a ViT\u4e0eCNN\u76f8\u6bd4\uff0c\u5728\u539f\u6570\u636e\u96c6\u4e0a\u7cbe\u5ea6\u66f4\u4f4e\uff0c\u7406\u8bba\u8fd0\u7b97\u590d\u6742\u5ea6\u66f4\u4f4e\uff0c\u4f46\u662f\u901f\u5ea6\u66f4\u6162\u3002 \u4f5c\u8005\u8ba4\u4e3a\u5982\u679cGPU\u4e3aTransformer\u4f18\u5316\uff0c\u5219Transformer\u6709\u5728\u901f\u5ea6\u4e0a\u8d85\u8d8aCNN\u7684\u6f5c\u529b\u3002 ViT\u5728\u8de8\u6570\u636e\u96c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u66f4\u597d\uff0c\u8bed\u4e49\u5206\u5272\u4e0a\u6bd4\u76ee\u6807\u68c0\u6d4b\u66f4\u660e\u663e\u3002\u5206\u6790\u5176\u6536\u655b\u7684Local minima\u635f\u5931landscape, \u8ba4\u4e3aViT\u6536\u655b\u5230\u7684local minima\u66f4\u5e73\u6ed1\u3002(\u505a\u6cd5\u4e0a\u6270\u52a8\u7f51\u7edc\u7684\u6743\u91cd\uff0c\u635f\u5931\u5728\u8bad\u7ec3\u96c6\u4e0a\u6d4b\u8bd5) ViT\u624d\u6d4b\u8bd5\u96c6\u66f4well-calibrated\u3002 ViT\u5bf9\u566a\u58f0\u4ee5\u53ca\u5bf9\u6297\u653b\u51fb\u7684\u62b5\u6297\u6027\u66f4\u5f3a\u3002 \u5bf9\u4e8e\u9ad8\u5206\u8fa8\u8f93\u5165\uff0cCNN\u7684\u6027\u80fd\u4fdd\u5b58\u6bd4ViT\u66f4\u597d\uff0c\u5c3d\u7ba1ViT\u7406\u8bba\u4e0a\u6709\u66f4\u5927\u7684\u611f\u53d7\u91ce\uff0c\u4f46\u662f\u6027\u80fd\u53cd\u800c\u4e0d\u884c\uff0c\u4f5c\u8005\u8ba4\u4e3a\u548cPositional Embedding\u6709\u5173\u7cfb\u3002","title":"A Comprehensive Study of Vision Transformers on Dense Prediction Tasks"},{"location":"other_categories/Summaries/SummaryOfMono3DDetection_in2019/","text":"Monocular 3D Object Detection in Autonomous Driving \u2014 A Review \u8fd9\u662f\u4e00\u7bc7Medium\u4e0a\u7684\u4e00\u4e2a\u4f5c\u8005\u5bf92019\u5355\u76ee3D\u68c0\u6d4b\u7684\u4e00\u4e2a\u603b\u7ed3\uff0c \u539f\u6587\u8fde\u63a5 . \u5e76\u5e26\u6709\u4e00\u5f20\u5f88\u7cfb\u7edf\u7684 \u603b\u7ed3\u8868 \u4f5c\u8005\u5c063D\u68c0\u6d4b\u5212\u5206\u4e3a4\u7c7b: Representation transformation; (BEV transform or Pseudo-lidar) Keypoints and shape; Distance from Constraints; Direct 3D proposal;","title":"Monocular 3D Object Detection in Autonomous Driving \u2014 A Review"},{"location":"other_categories/Summaries/SummaryOfMono3DDetection_in2019/#monocular-3d-object-detection-in-autonomous-driving-a-review","text":"\u8fd9\u662f\u4e00\u7bc7Medium\u4e0a\u7684\u4e00\u4e2a\u4f5c\u8005\u5bf92019\u5355\u76ee3D\u68c0\u6d4b\u7684\u4e00\u4e2a\u603b\u7ed3\uff0c \u539f\u6587\u8fde\u63a5 . \u5e76\u5e26\u6709\u4e00\u5f20\u5f88\u7cfb\u7edf\u7684 \u603b\u7ed3\u8868 \u4f5c\u8005\u5c063D\u68c0\u6d4b\u5212\u5206\u4e3a4\u7c7b: Representation transformation; (BEV transform or Pseudo-lidar) Keypoints and shape; Distance from Constraints; Direct 3D proposal;","title":"Monocular 3D Object Detection in Autonomous Driving \u2014 A Review"},{"location":"other_categories/Summaries/SummaryOfSingleStageInstanceSeg/","text":"Summary of Single Stage Instance Segmentation \u8fd9\u7bc7\u6587\u7ae0\u4e3b\u8981\u662f\u6839\u636eMedium\u4e0a\u4e00\u4e2a\u535a\u4e3b\u7684 review . \u8fd9\u4e2areview\u5728\u5206\u7c7b\u4e0a\u5c06\u5355\u9636\u6bb5\u5b9e\u4f8b\u5206\u5272\u5206\u4e3a\u57fa\u4e8elocal mask\u4e0eglobal mask Local Mask local mask\u6307\u8f93\u51fa\u4e00\u4e2a\u4e2abounding box\uff0c\u5e76\u4e14\u5bf9\u5c40\u90e8instance\u7684contour\u6216\u8005mask\u8fdb\u884c\u7f16\u7801\u4e0e\u9884\u6d4b\u3002\u6587\u4e2d\u63d0\u5230\u4e86\u591a\u7bc7paper\uff0c\u5176\u4e2d TensorMask , PolarMask \u662f\u672c\u7f51\u7ad9\u5df2\u7ecf\u6709\u7b80\u4ecb\u7684paper.\u8fd9\u91cc\u8fdb\u884c\u4e00\u4e9b\u8865\u5145 Fourier Net pdf code \u8fd9\u7bc7paper\u4ee3\u7801\u4ee5\u53ca\u601d\u60f3\u4e0a\u57fa\u4e8ePolarMask,\u5bf9\u4e8e\u8fd9\u4e00\u5468\u5c04\u7ebf\u7684\u957f\u5ea6\uff0c\u6211\u4eec\u53ea\u9700\u8981\u9884\u6d4b\u5176\u5085\u91cc\u53f6\u53d8\u6362\u503c\u7684\u524d M \u9879\uff0c\u8fd9\u4e2a M \u51b3\u5b9a\u7f51\u7edc\u8bad\u7ec3\u7684\u8d85\u53c2\uff0c\u8fd9\u79cd\u65b9\u6cd5\u5f97\u5230\u7684\u8f93\u51fa\u4e00\u822c\u6765\u8bf4\u6bd4PolarMask\u540c\u7b49\u53c2\u6570\u91cf\u7684\u7ed3\u679c\u66f4\u52a0\u7cbe\u7ec6\u3002\u4f46\u662f\u8fd9\u7bc7paper\u7684\u4f5c\u8005\u6ca1\u6709train\u51fa\u4e00\u4e2a\u4ee4\u4eba\u4fe1\u670d\u7684\u70b9\u6570\u3002 p_{n, i}=\\frac{1}{N} \\sum_{k=0}^{N-1} x_{k, i} e^{\\frac{j 2 \\pi k n}{N}} Global Mask global mask\u6307\u7f51\u7edc\u76f4\u63a5\u8f93\u51fa\u6574\u4e2a\u56fe\u5927\u5c0f\u7684mask\uff0c\u5e76\u540e\u671f\u8fdb\u884c\u6574\u5408\u6216\u8005\u7ec4\u5408\u4ee5\u5f62\u6210\u6bcf\u4e00\u4e2aobject\u7684mask\u3002\u6587\u4e2d\u63d0\u5230\u4e86\u591a\u7bc7paper\uff0c\u6700\u8fd1SOTA\u6c34\u5e73\u7684\u4e3b\u8981\u4e3a\u5176\u4e2d\u7684 YOLOACT , SOLO \uff0c\u672c\u7f51\u7ad9\u4e5f\u6709\u76f8\u5173\u7684\u4ecb\u7ecd\u3002","title":"Summary of Single Stage Instance Segmentation"},{"location":"other_categories/Summaries/SummaryOfSingleStageInstanceSeg/#summary-of-single-stage-instance-segmentation","text":"\u8fd9\u7bc7\u6587\u7ae0\u4e3b\u8981\u662f\u6839\u636eMedium\u4e0a\u4e00\u4e2a\u535a\u4e3b\u7684 review . \u8fd9\u4e2areview\u5728\u5206\u7c7b\u4e0a\u5c06\u5355\u9636\u6bb5\u5b9e\u4f8b\u5206\u5272\u5206\u4e3a\u57fa\u4e8elocal mask\u4e0eglobal mask","title":"Summary of Single Stage Instance Segmentation"},{"location":"other_categories/Summaries/SummaryOfSingleStageInstanceSeg/#local-mask","text":"local mask\u6307\u8f93\u51fa\u4e00\u4e2a\u4e2abounding box\uff0c\u5e76\u4e14\u5bf9\u5c40\u90e8instance\u7684contour\u6216\u8005mask\u8fdb\u884c\u7f16\u7801\u4e0e\u9884\u6d4b\u3002\u6587\u4e2d\u63d0\u5230\u4e86\u591a\u7bc7paper\uff0c\u5176\u4e2d TensorMask , PolarMask \u662f\u672c\u7f51\u7ad9\u5df2\u7ecf\u6709\u7b80\u4ecb\u7684paper.\u8fd9\u91cc\u8fdb\u884c\u4e00\u4e9b\u8865\u5145","title":"Local Mask"},{"location":"other_categories/Summaries/SummaryOfSingleStageInstanceSeg/#fourier-net","text":"pdf code \u8fd9\u7bc7paper\u4ee3\u7801\u4ee5\u53ca\u601d\u60f3\u4e0a\u57fa\u4e8ePolarMask,\u5bf9\u4e8e\u8fd9\u4e00\u5468\u5c04\u7ebf\u7684\u957f\u5ea6\uff0c\u6211\u4eec\u53ea\u9700\u8981\u9884\u6d4b\u5176\u5085\u91cc\u53f6\u53d8\u6362\u503c\u7684\u524d M \u9879\uff0c\u8fd9\u4e2a M \u51b3\u5b9a\u7f51\u7edc\u8bad\u7ec3\u7684\u8d85\u53c2\uff0c\u8fd9\u79cd\u65b9\u6cd5\u5f97\u5230\u7684\u8f93\u51fa\u4e00\u822c\u6765\u8bf4\u6bd4PolarMask\u540c\u7b49\u53c2\u6570\u91cf\u7684\u7ed3\u679c\u66f4\u52a0\u7cbe\u7ec6\u3002\u4f46\u662f\u8fd9\u7bc7paper\u7684\u4f5c\u8005\u6ca1\u6709train\u51fa\u4e00\u4e2a\u4ee4\u4eba\u4fe1\u670d\u7684\u70b9\u6570\u3002 p_{n, i}=\\frac{1}{N} \\sum_{k=0}^{N-1} x_{k, i} e^{\\frac{j 2 \\pi k n}{N}}","title":"Fourier Net"},{"location":"other_categories/Summaries/SummaryOfSingleStageInstanceSeg/#global-mask","text":"global mask\u6307\u7f51\u7edc\u76f4\u63a5\u8f93\u51fa\u6574\u4e2a\u56fe\u5927\u5c0f\u7684mask\uff0c\u5e76\u540e\u671f\u8fdb\u884c\u6574\u5408\u6216\u8005\u7ec4\u5408\u4ee5\u5f62\u6210\u6bcf\u4e00\u4e2aobject\u7684mask\u3002\u6587\u4e2d\u63d0\u5230\u4e86\u591a\u7bc7paper\uff0c\u6700\u8fd1SOTA\u6c34\u5e73\u7684\u4e3b\u8981\u4e3a\u5176\u4e2d\u7684 YOLOACT , SOLO \uff0c\u672c\u7f51\u7ad9\u4e5f\u6709\u76f8\u5173\u7684\u4ecb\u7ecd\u3002","title":"Global Mask"},{"location":"other_categories/Summaries/SummaryOfTemperalBEV/","text":"Summary of Temperal BEV \u8fd9\u91cc\u9075\u4ece \u516c\u4f17\u53f7 \u63d0\u4f9b\u7684\u65b9\u5411\u8fdb\u884c\u7814\u7a76\u3002\u7531\u4e8e\u5fae\u4fe1\u516c\u4f17\u53f7\u7684\u5386\u53f2\u5185\u5bb9\u94fe\u63a5\u5e76\u4e0d\u7a33\u5b9a\uff0c\u8fd9\u91cc\u6211\u4eec\u5c06\u539f\u9875\u9762 \u6253\u5370\u51fa\u6765 \u3002 \u76f4\u89c9\u4e0a\u6765\u8bf4\uff0c\u5982\u679c\u6211\u4eec\u60f3\u8981\u5bf9\u591a\u5e27\u7684BEV feature\u8fdb\u884c\u878d\u5408\uff0c\u6700\u7b80\u5355\u7684\u60f3\u6cd5\u662f\u9996\u5148\u9700\u8981\u6839\u636e\u524d\u5e27\u4e0e\u5f53\u524d\u5e27\u4e4b\u95f4\u7684\u76f8\u5bf9\u4f4d\u79fb\u5c06\u7279\u5f81\u901a\u8fc7grid_sampling\u63d0\u53d6\u5230\u5f53\u524d\u5e27\u4e0b\uff0c\u5b9e\u73b0\u5750\u6807\u7684\u5bf9\u9f50(alignment)\u3002\u7136\u540e\u5bf9\u591a\u5e27\u7684BEV feature concat\u8d77\u6765conv,\u6216\u8005\u662f\u8dd1attention\u878d\u5408\u3002 \u4f46\u662f\u5982 deformable attention \u7684\u64cd\u4f5c\uff0c\u6211\u4eec\u53ef\u4ee5\u53ea\u628areference points\u8fdb\u884c\u5750\u6807\u8f6c\u6362\uff0c\u800c\u4e0d\u9700\u8981\u5bf9\u5168\u7279\u5f81\u8fdb\u884c\u5904\u7406\u5c31\u53ef\u4ee5\u62bd\u8c61\u5730\u5b9e\u73b0\u524d\u6587\u7684alignment\u7684\u6548\u679c\u3002 \u53e6\u5916\u4e5f\u9700\u8981\u6ce8\u610f\uff0c\u5982\u679c\u76f4\u63a5\u4f7f\u7528\u5e27\u4e4b\u95f4\u5730temperal attention\uff0c\u7531\u4e8eattention\u672c\u8d28\u4e0a\u7684\u65e0\u5e8f\u6027, \u7406\u8bba\u4e0a\u53ef\u4ee5\u901a\u8fc7\u5904\u7406positional embedding\u4e5f\u90fd\u80fd\u62bd\u8c61\u5730\u5b9e\u73b0alignment\u6548\u679c\u3002 \u603b\u800c\u8a00\u4e4bAttention\u673a\u5236\u7684\u7279\u6b8a\u6027\u7ed9\u5e27\u95f4\u4e0d\u540c\u5750\u6807\u7684BEV feature\u7684\u878d\u5408\u5e26\u6765\u4e86\u66f4\u591a\u65b0\u7684\u53ef\u80fd\u3002 BEVFormer: Learning Bird\u2019s-Eye-View Representation from Multi-Camera Images via Spatiotemporal Transformers pdf code \u8fd9\u7bc7paper\u4e3b\u8981\u7684\u6846\u67b6\u662f\u4f7f\u7528Transformer\u878d\u5408\u4e0d\u540c\u89c6\u89d2\u7684\u7279\u5f81\uff0c\u5f62\u6210BEV feature\u3002 \u5e76\u4e14\u5728\u65f6\u5e8f\u4e0a\uff0c\u8bfb\u53d6\u4e0a\u4e00\u4e2a\u65f6\u523b\u7684\u878d\u5408\u540e\u7684BEV feature\u4f7f\u7528Attention\u673a\u5236\u8fdb\u884c\u65f6\u5e8f\u4e0a\u7684\u878d\u5408(\u7c7b\u4f3c\u4e8eRNN\u7684\u5f62\u5f0f)\u3002 \u5177\u4f53\u800c\u8a00: \u4f7f\u7528ResNet101 \u63d0\u53d6\u6bcf\u4e2a\u76f8\u673a\u7684\u7279\u5f81\u3002 Spatial Cross-Attention\u7684\u601d\u8def\u662f\u5728BEV\u4e0a\u6bcf\u4e2aPillar\u751f\u6210 N_{ref} \u4e2a\u53c2\u8003\u70b9(3D reference point)\uff0c\u628a\u53c2\u8003\u70b9\u6295\u5f71\u5230\u6bcf\u4e2a\u76f8\u673a\u4e0a\uff0c\u83b7\u5f97 \u4e8c\u7ef4\u53c2\u8003\u70b9 (2D reference points). \u7136\u540e\u7528BEV query\u751f\u6210offset, \\operatorname{SCA}\\left(Q_p, F_t\\right)=\\frac{1}{\\left|\\mathcal{V}_{\\text {hit }}\\right|} \\sum_{i \\in \\mathcal{V}_{\\text {hit }}} \\sum_{j=1}^{N_{\\text {ref }}} \\operatorname{DeformAttn}\\left(Q_p, \\mathcal{P}(p, i, j), F_t^i\\right) Temperal Cross-Attention\u5219\u8981\u878d\u5408\u524d\u5e27\u7684BEV feature,\u5982\u679c\u662f\u9996\u5e27\uff0c\u5219\u9000\u5316\u4e3aself-attention, \u5426\u5219\u662f\u6b63\u5e38\u7684Temperal self-attention. \u800cdeformable attention\u4e2d\u7684\u504f\u79fb\u91cf \\Delta p \u6539\u4e3a\u7531 Q, B \u7684concat\u9884\u6d4b\uff0c\u800c\u4e0d\u662f\u53ea\u7528 Q \u9884\u6d4b\u3002 \\operatorname{TSA}\\left(Q_p,\\left\\{Q, B_{t-1}^{\\prime}\\right\\}\\right)=\\sum_{V \\in\\left\\{Q, B_{t-1}^{\\prime}\\right\\}} \\operatorname{DeformAttn}\\left(Q_p, p, V\\right) \u672c\u6587\u6700\u540e\u8f93\u51fa\u76f4\u63a5\u91c7\u7528DETR\u7684\u8f93\u51fa\u5f62\u6001\uff0c\u4f5c\u4e3a\u4e09\u7ef4\u68c0\u6d4b\u7684\u8f93\u51fa\u3002 \u8bad\u7ec3\u7684\u65f6\u5019\u662f\u4ece\u4e4b\u524d\u4e24\u79d2\u7684\u6570\u636e\u4e2d\u968f\u673a\u9009\u62e9\u4e09\u5e27\uff0c\u4f5c\u4e3a\u524d\u5e27\u3002\u524d\u9762\u4e09\u5e27\u7684BEV\u7279\u5f81\u751f\u6210\u4e0d\u4ea7\u751f\u68af\u5ea6\u3002 Polar Parametrization for Vision-based Surround-View 3D Detection pdf code \u672c\u6587\u6307\u51fa\u4e00\u70b9\u5728\u4e8e\u4f7f\u7528\u6781\u5750\u6807\u6765\u8868\u8fbe\u7269\u4f53\uff0c\u5bf9\u4e8e\u516d\u4e2a\u76f8\u673a\u7684\u8bbe\u8ba1\u60c5\u51b5\u6765\u8bf4\u66f4\u4e3a\u5bf9\u79f0\u3002 \u5728\u77e9\u5f62\u5750\u6807\u7cfb\u4e0b\uff0c\u4e24\u8f86\u8ddd\u79bb\u76f8\u7b49\u7684\u8f66\uff0c\u4e00\u4e2a\u5728\u6b63\u524d\u65b9\uff0c\u4e00\u4e2a\u5728\u4fa7\u524d\u65b9\uff0c\u53ef\u80fd\u5728\u6b63\u524d\u65b9\u7684\u8f66\u5b50\u56e0\u4e3a\u8ddd\u79bb d > z_{max} \u5c31\u88ab\u8fc7\u6ee4\u6389\uff0c\u4f46\u662f\u5728\u4fa7\u524d\u65b9\u7684\u8f66\u5b50\u56e0\u4e3a d < \\sqrt{2}z_{max} \u5374\u88ab\u4fdd\u7559\uff0c\u4f46\u662f\u7531\u4e8e\u4e24\u4e2a\u8f66\u5b50\u5230\u76f8\u673a\u8ddd\u79bb\u4e00\u81f4\uff0c\u4ed6\u4eec\u5728\u4e24\u4e2a\u76f8\u673a\u4e0a\u7684\u6295\u5f71\u662f\u4e00\u81f4\u7684\u3002\u5982\u679c\u7ed9\u51fa\u7684label\u5374\u4e0d\u4e00\u81f4\uff0c\u4f1a\u5f71\u54cd\u6a21\u578b\u7684\u6536\u655b\u3002 \u672c\u6587\u628a\u524d\u6587\u7684BEV query\uff0c BEV label assignment\u5168\u90e8\u6362\u5230\u4e86\u6781\u5750\u6807\u7cfb\u4e0b\uff0c\u800c\u7f51\u7edc\u7ed3\u6784\u4e0a\u4ec5\u4ec5\u9700\u8981\u6539\u53d8 polar box encoding (\u8fd9\u4e2a\u5728\u5b9e\u9645coding\u4e2d\u6ca1\u6709\u6539\u53d8\uff0c\u53ea\u662f\u903b\u8f91\u4e0a\u53d1\u751f\u4e86\u53d8\u5316)\u3002 \u5728\u65f6\u5e8f\u878d\u5408\u7684\u65f6\u5019\uff0c\u8fd9\u91cc\u7684\u91c7\u6837\u662f\u6295\u5f71\u70b9 + \u53ef\u5b66\u4e60\u504f\u79fb\uff0c\u6027\u8d28\u4e0a\u7c7b\u4f3c\u4e8e\u524d\u6587\u7684deformable attention.\u53ef\u5b66\u4e60\u504f\u79fb\u7531\u5f53\u524d\u70b9\u7279\u5f81\u4e0equery concat + linear\u8f93\u51fa\u3002 \u878d\u5408\u591a\u5e27\u6570\u636e\u7684\u65f6\u5019\uff0c\u672c\u6587\u7684\u65b9\u6848\u662f\u628a\u5f53\u524d\u5e27\u7684 polar pillar \u70b9\u6295\u5f71\u5230\u8fc7\u53bb\u7684 image frame\u4e0a\u9762\uff0c\u7528\u4e0a\u6587\u7684\u65b9\u6cd5\u8fdb\u884c\u4fe1\u606f\u878d\u5408\u3002","title":"Summary of Temperal BEV"},{"location":"other_categories/Summaries/SummaryOfTemperalBEV/#summary-of-temperal-bev","text":"\u8fd9\u91cc\u9075\u4ece \u516c\u4f17\u53f7 \u63d0\u4f9b\u7684\u65b9\u5411\u8fdb\u884c\u7814\u7a76\u3002\u7531\u4e8e\u5fae\u4fe1\u516c\u4f17\u53f7\u7684\u5386\u53f2\u5185\u5bb9\u94fe\u63a5\u5e76\u4e0d\u7a33\u5b9a\uff0c\u8fd9\u91cc\u6211\u4eec\u5c06\u539f\u9875\u9762 \u6253\u5370\u51fa\u6765 \u3002 \u76f4\u89c9\u4e0a\u6765\u8bf4\uff0c\u5982\u679c\u6211\u4eec\u60f3\u8981\u5bf9\u591a\u5e27\u7684BEV feature\u8fdb\u884c\u878d\u5408\uff0c\u6700\u7b80\u5355\u7684\u60f3\u6cd5\u662f\u9996\u5148\u9700\u8981\u6839\u636e\u524d\u5e27\u4e0e\u5f53\u524d\u5e27\u4e4b\u95f4\u7684\u76f8\u5bf9\u4f4d\u79fb\u5c06\u7279\u5f81\u901a\u8fc7grid_sampling\u63d0\u53d6\u5230\u5f53\u524d\u5e27\u4e0b\uff0c\u5b9e\u73b0\u5750\u6807\u7684\u5bf9\u9f50(alignment)\u3002\u7136\u540e\u5bf9\u591a\u5e27\u7684BEV feature concat\u8d77\u6765conv,\u6216\u8005\u662f\u8dd1attention\u878d\u5408\u3002 \u4f46\u662f\u5982 deformable attention \u7684\u64cd\u4f5c\uff0c\u6211\u4eec\u53ef\u4ee5\u53ea\u628areference points\u8fdb\u884c\u5750\u6807\u8f6c\u6362\uff0c\u800c\u4e0d\u9700\u8981\u5bf9\u5168\u7279\u5f81\u8fdb\u884c\u5904\u7406\u5c31\u53ef\u4ee5\u62bd\u8c61\u5730\u5b9e\u73b0\u524d\u6587\u7684alignment\u7684\u6548\u679c\u3002 \u53e6\u5916\u4e5f\u9700\u8981\u6ce8\u610f\uff0c\u5982\u679c\u76f4\u63a5\u4f7f\u7528\u5e27\u4e4b\u95f4\u5730temperal attention\uff0c\u7531\u4e8eattention\u672c\u8d28\u4e0a\u7684\u65e0\u5e8f\u6027, \u7406\u8bba\u4e0a\u53ef\u4ee5\u901a\u8fc7\u5904\u7406positional embedding\u4e5f\u90fd\u80fd\u62bd\u8c61\u5730\u5b9e\u73b0alignment\u6548\u679c\u3002 \u603b\u800c\u8a00\u4e4bAttention\u673a\u5236\u7684\u7279\u6b8a\u6027\u7ed9\u5e27\u95f4\u4e0d\u540c\u5750\u6807\u7684BEV feature\u7684\u878d\u5408\u5e26\u6765\u4e86\u66f4\u591a\u65b0\u7684\u53ef\u80fd\u3002","title":"Summary of Temperal BEV"},{"location":"other_categories/Summaries/SummaryOfTemperalBEV/#bevformer-learning-birds-eye-view-representation-from-multi-camera-images-via-spatiotemporal-transformers","text":"pdf code \u8fd9\u7bc7paper\u4e3b\u8981\u7684\u6846\u67b6\u662f\u4f7f\u7528Transformer\u878d\u5408\u4e0d\u540c\u89c6\u89d2\u7684\u7279\u5f81\uff0c\u5f62\u6210BEV feature\u3002 \u5e76\u4e14\u5728\u65f6\u5e8f\u4e0a\uff0c\u8bfb\u53d6\u4e0a\u4e00\u4e2a\u65f6\u523b\u7684\u878d\u5408\u540e\u7684BEV feature\u4f7f\u7528Attention\u673a\u5236\u8fdb\u884c\u65f6\u5e8f\u4e0a\u7684\u878d\u5408(\u7c7b\u4f3c\u4e8eRNN\u7684\u5f62\u5f0f)\u3002 \u5177\u4f53\u800c\u8a00: \u4f7f\u7528ResNet101 \u63d0\u53d6\u6bcf\u4e2a\u76f8\u673a\u7684\u7279\u5f81\u3002 Spatial Cross-Attention\u7684\u601d\u8def\u662f\u5728BEV\u4e0a\u6bcf\u4e2aPillar\u751f\u6210 N_{ref} \u4e2a\u53c2\u8003\u70b9(3D reference point)\uff0c\u628a\u53c2\u8003\u70b9\u6295\u5f71\u5230\u6bcf\u4e2a\u76f8\u673a\u4e0a\uff0c\u83b7\u5f97 \u4e8c\u7ef4\u53c2\u8003\u70b9 (2D reference points). \u7136\u540e\u7528BEV query\u751f\u6210offset, \\operatorname{SCA}\\left(Q_p, F_t\\right)=\\frac{1}{\\left|\\mathcal{V}_{\\text {hit }}\\right|} \\sum_{i \\in \\mathcal{V}_{\\text {hit }}} \\sum_{j=1}^{N_{\\text {ref }}} \\operatorname{DeformAttn}\\left(Q_p, \\mathcal{P}(p, i, j), F_t^i\\right) Temperal Cross-Attention\u5219\u8981\u878d\u5408\u524d\u5e27\u7684BEV feature,\u5982\u679c\u662f\u9996\u5e27\uff0c\u5219\u9000\u5316\u4e3aself-attention, \u5426\u5219\u662f\u6b63\u5e38\u7684Temperal self-attention. \u800cdeformable attention\u4e2d\u7684\u504f\u79fb\u91cf \\Delta p \u6539\u4e3a\u7531 Q, B \u7684concat\u9884\u6d4b\uff0c\u800c\u4e0d\u662f\u53ea\u7528 Q \u9884\u6d4b\u3002 \\operatorname{TSA}\\left(Q_p,\\left\\{Q, B_{t-1}^{\\prime}\\right\\}\\right)=\\sum_{V \\in\\left\\{Q, B_{t-1}^{\\prime}\\right\\}} \\operatorname{DeformAttn}\\left(Q_p, p, V\\right) \u672c\u6587\u6700\u540e\u8f93\u51fa\u76f4\u63a5\u91c7\u7528DETR\u7684\u8f93\u51fa\u5f62\u6001\uff0c\u4f5c\u4e3a\u4e09\u7ef4\u68c0\u6d4b\u7684\u8f93\u51fa\u3002 \u8bad\u7ec3\u7684\u65f6\u5019\u662f\u4ece\u4e4b\u524d\u4e24\u79d2\u7684\u6570\u636e\u4e2d\u968f\u673a\u9009\u62e9\u4e09\u5e27\uff0c\u4f5c\u4e3a\u524d\u5e27\u3002\u524d\u9762\u4e09\u5e27\u7684BEV\u7279\u5f81\u751f\u6210\u4e0d\u4ea7\u751f\u68af\u5ea6\u3002","title":"BEVFormer: Learning Bird\u2019s-Eye-View Representation from Multi-Camera Images via Spatiotemporal Transformers"},{"location":"other_categories/Summaries/SummaryOfTemperalBEV/#polar-parametrization-for-vision-based-surround-view-3d-detection","text":"pdf code \u672c\u6587\u6307\u51fa\u4e00\u70b9\u5728\u4e8e\u4f7f\u7528\u6781\u5750\u6807\u6765\u8868\u8fbe\u7269\u4f53\uff0c\u5bf9\u4e8e\u516d\u4e2a\u76f8\u673a\u7684\u8bbe\u8ba1\u60c5\u51b5\u6765\u8bf4\u66f4\u4e3a\u5bf9\u79f0\u3002 \u5728\u77e9\u5f62\u5750\u6807\u7cfb\u4e0b\uff0c\u4e24\u8f86\u8ddd\u79bb\u76f8\u7b49\u7684\u8f66\uff0c\u4e00\u4e2a\u5728\u6b63\u524d\u65b9\uff0c\u4e00\u4e2a\u5728\u4fa7\u524d\u65b9\uff0c\u53ef\u80fd\u5728\u6b63\u524d\u65b9\u7684\u8f66\u5b50\u56e0\u4e3a\u8ddd\u79bb d > z_{max} \u5c31\u88ab\u8fc7\u6ee4\u6389\uff0c\u4f46\u662f\u5728\u4fa7\u524d\u65b9\u7684\u8f66\u5b50\u56e0\u4e3a d < \\sqrt{2}z_{max} \u5374\u88ab\u4fdd\u7559\uff0c\u4f46\u662f\u7531\u4e8e\u4e24\u4e2a\u8f66\u5b50\u5230\u76f8\u673a\u8ddd\u79bb\u4e00\u81f4\uff0c\u4ed6\u4eec\u5728\u4e24\u4e2a\u76f8\u673a\u4e0a\u7684\u6295\u5f71\u662f\u4e00\u81f4\u7684\u3002\u5982\u679c\u7ed9\u51fa\u7684label\u5374\u4e0d\u4e00\u81f4\uff0c\u4f1a\u5f71\u54cd\u6a21\u578b\u7684\u6536\u655b\u3002 \u672c\u6587\u628a\u524d\u6587\u7684BEV query\uff0c BEV label assignment\u5168\u90e8\u6362\u5230\u4e86\u6781\u5750\u6807\u7cfb\u4e0b\uff0c\u800c\u7f51\u7edc\u7ed3\u6784\u4e0a\u4ec5\u4ec5\u9700\u8981\u6539\u53d8 polar box encoding (\u8fd9\u4e2a\u5728\u5b9e\u9645coding\u4e2d\u6ca1\u6709\u6539\u53d8\uff0c\u53ea\u662f\u903b\u8f91\u4e0a\u53d1\u751f\u4e86\u53d8\u5316)\u3002 \u5728\u65f6\u5e8f\u878d\u5408\u7684\u65f6\u5019\uff0c\u8fd9\u91cc\u7684\u91c7\u6837\u662f\u6295\u5f71\u70b9 + \u53ef\u5b66\u4e60\u504f\u79fb\uff0c\u6027\u8d28\u4e0a\u7c7b\u4f3c\u4e8e\u524d\u6587\u7684deformable attention.\u53ef\u5b66\u4e60\u504f\u79fb\u7531\u5f53\u524d\u70b9\u7279\u5f81\u4e0equery concat + linear\u8f93\u51fa\u3002 \u878d\u5408\u591a\u5e27\u6570\u636e\u7684\u65f6\u5019\uff0c\u672c\u6587\u7684\u65b9\u6848\u662f\u628a\u5f53\u524d\u5e27\u7684 polar pillar \u70b9\u6295\u5f71\u5230\u8fc7\u53bb\u7684 image frame\u4e0a\u9762\uff0c\u7528\u4e0a\u6587\u7684\u65b9\u6cd5\u8fdb\u884c\u4fe1\u606f\u878d\u5408\u3002","title":"Polar Parametrization for Vision-based Surround-View 3D Detection"},{"location":"other_categories/Summaries/Summary_CVPR_2021/","text":"Summaries for several CVPR 2021 papers OTA: Optimal Transport Assignment for Object Detection pdf code OTA \u8fd9\u7bc7paper\u63d0\u51fa\u4f7f\u7528optimal transport assignment \u505a ground truth label assignment. \u524d\u4e00\u5e74\u7684CVPR\u4e00\u7bc7paper ATSS \u8bf4\u660eRetinaNet\u548cFCOS\u4e4b\u95f4\u7684\u5dee\u8ddd\u53ea\u8981\u5728\u4e8elabel\u7684\u5206\u914d\u4e0a. \u8fd9\u7bc7paper\u4f7f\u7528 \u6700\u4f18\u8fd0\u8f93 . \u7406\u89e3\u4e0a\u6765\u8bf4\uff0c\u6bcf\u4e00\u4e2aground truth\u4f5c\u4e3a\u4e00\u4e2a optimal transport \u7684 supplier \u63d0\u4f9b k \u4e2a\u6b63\u6837\u672c, \u5269\u4e0b\u7684\u7531background\u63d0\u4f9b\u8d1f\u6837\u672c. \u7136\u540e\u6bcf\u4e00\u4e2aanchor \u4f5c\u4e3ademander. \u5206\u914d\u6210\u672c\u7531\u635f\u5931\u51fd\u6570\u51b3\u5b9a \\begin{aligned} c_{i j}^{f g}=& L_{c l s}\\left(P_{j}^{c l s}(\\theta), G_{i}^{c l s}\\right)+\\\\ & \\alpha L_{r e g}\\left(P_{j}^{b o x}(\\theta), G_{i}^{b o x}\\right) \\end{aligned} \u8fd9\u4e2aassign \u8fc7\u7a0b\u4ee3\u7801\u4e0a\u662f\u5c4f\u853d\u53cd\u4f20\u7684\u3002 Boundary IoU: Improving Object-Centric Image Segmentation Evaluation pdf code G_d, P_d \u5b9a\u4e49\u4e3a\u8ddd\u79bbground truth\u4e0e\u9884\u6d4b\u7684contours\u8ddd\u79bb\u5728 d \u4ee5\u5185\u7684\u50cf\u7d20. \u5b9e\u73b0\u4e0a\u662f\u4f7f\u7528cv2.copyMakeBorder\u4ee5\u53cacv2.erode\u3002 \u6838\u5fc3\u4ee3\u7801 GrooMeD-NMS: Grouped Mathematically Differentiable NMS for Monocular 3D Object Detection pdf code \u8fd9\u7bc7paper\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684NMS\u7b97\u6cd5\uff0c\u4f46\u662f\u4ec5\u4ec5\u4f7f\u7528\u4e862D\u7b49\u4fe1\u606f\uff0c\u4f46\u662f\u4ec5evaluate\u57283D\u7b97\u6cd5\u4e0a.\u601d\u8def\u662f\u8ba9NMS\u53d8\u5f97\u53ef\u4ee5\u8bad\u7ec3 Learning Optical Flow From a Few Matches pdf \u8fd9\u7bc7\u6587\u7ae0\u7684\u4f5c\u8005\u6307\u51fa\u65b0\u7684\u5149\u6d41\u6846\u67b6\uff0c\u6bd4\u5982 RAFT , \u4f1a\u8ba1\u7b97\u6bcf\u4e00\u4e2a\u50cf\u7d20\u4e0e\u6574\u4e2a\u7279\u5f81\u56fe\u8ba1\u7b97\u5339\u914d\u4ee5\u53ca\u5149\u6d41\u3002\u4f46\u662f\u4f5c\u8005\u8ba4\u4e3a\u8fd9\u4e2a\u5360\u7528\u5b58\u50a8\u592a\u591a\uff0c\u4e0d\u597d\uff0c\u4f46\u662f\u539f\u6765\u7684\u90bb\u57df\u5339\u914d\u4e5f\u8fd8\u662f\u4e0d\u5c3d\u5982\u4eba\u610f\u3002\u4f5c\u8005\u63d0\u51fa\u5728RAFT\u7684\u57fa\u7840\u4e0a\uff0c\u53ea\u5b58\u50a8\u6bcf\u4e2a\u50cf\u7d20\u7684tok-k\u4e2a\u5339\u914d\u3002\u6700\u7ec8\u5f62\u6210\u4e00\u4e2a\u79bb\u6563\u7684\u5339\u914d\u3002\u5b58\u50a8\u65b9\u6cd5\u4e5f\u4ece\u5bc6\u96c6\u5b58\u50a8\u53d8\u4e3a{\u503c\uff0c\u5750\u6807}\u5b58\u50a8 UPFlow: Upsampling Pyramid for Unsupervised Optical Flow Learning pdf code \u8fd9\u7bc7paper\u6539\u5584\u4e86\u65e0\u76d1\u7763\u7684\u5149\u6d41\u3002 self-guided upsample. Loss guidance at pyramid levels. \u5176\u4ed6\u65e0\u76d1\u7763\u5149\u6d41\u7684\u4f4e\u5206\u8fa8\u7387\u8f93\u51fa\u5c42\u662f\u7528GT\u8bad\u7ec3\u7684\u3002\u672c\u6587\u5219\u662f\u7528\u9ad8\u5206\u8fa8\u7387\u7684\u8f93\u51fa\u4e0b\u91c7\u6837\u8fdb\u884c\u8bad\u7ec3\uff0c AutoFlow: Learning a Better Training Set for Optical Flow pdf project-page \u8fd9\u7bc7paper\u7684\u4efb\u52a1\u662f\u6839\u636e\u5c11\u91cf\u975e\u8fde\u7eed\u56fe\u7247\u6570\u636e\uff0c\u5408\u6210\u5149\u6d41\u8bad\u7ec3\u7528\u7684\u6570\u636e\u3002","title":"Summaries for several CVPR 2021 papers"},{"location":"other_categories/Summaries/Summary_CVPR_2021/#summaries-for-several-cvpr-2021-papers","text":"","title":"Summaries for several CVPR 2021 papers"},{"location":"other_categories/Summaries/Summary_CVPR_2021/#ota-optimal-transport-assignment-for-object-detection","text":"pdf code OTA \u8fd9\u7bc7paper\u63d0\u51fa\u4f7f\u7528optimal transport assignment \u505a ground truth label assignment. \u524d\u4e00\u5e74\u7684CVPR\u4e00\u7bc7paper ATSS \u8bf4\u660eRetinaNet\u548cFCOS\u4e4b\u95f4\u7684\u5dee\u8ddd\u53ea\u8981\u5728\u4e8elabel\u7684\u5206\u914d\u4e0a. \u8fd9\u7bc7paper\u4f7f\u7528 \u6700\u4f18\u8fd0\u8f93 . \u7406\u89e3\u4e0a\u6765\u8bf4\uff0c\u6bcf\u4e00\u4e2aground truth\u4f5c\u4e3a\u4e00\u4e2a optimal transport \u7684 supplier \u63d0\u4f9b k \u4e2a\u6b63\u6837\u672c, \u5269\u4e0b\u7684\u7531background\u63d0\u4f9b\u8d1f\u6837\u672c. \u7136\u540e\u6bcf\u4e00\u4e2aanchor \u4f5c\u4e3ademander. \u5206\u914d\u6210\u672c\u7531\u635f\u5931\u51fd\u6570\u51b3\u5b9a \\begin{aligned} c_{i j}^{f g}=& L_{c l s}\\left(P_{j}^{c l s}(\\theta), G_{i}^{c l s}\\right)+\\\\ & \\alpha L_{r e g}\\left(P_{j}^{b o x}(\\theta), G_{i}^{b o x}\\right) \\end{aligned} \u8fd9\u4e2aassign \u8fc7\u7a0b\u4ee3\u7801\u4e0a\u662f\u5c4f\u853d\u53cd\u4f20\u7684\u3002","title":"OTA: Optimal Transport Assignment for Object Detection"},{"location":"other_categories/Summaries/Summary_CVPR_2021/#boundary-iou-improving-object-centric-image-segmentation-evaluation","text":"pdf code G_d, P_d \u5b9a\u4e49\u4e3a\u8ddd\u79bbground truth\u4e0e\u9884\u6d4b\u7684contours\u8ddd\u79bb\u5728 d \u4ee5\u5185\u7684\u50cf\u7d20. \u5b9e\u73b0\u4e0a\u662f\u4f7f\u7528cv2.copyMakeBorder\u4ee5\u53cacv2.erode\u3002 \u6838\u5fc3\u4ee3\u7801","title":"Boundary IoU: Improving Object-Centric Image Segmentation Evaluation"},{"location":"other_categories/Summaries/Summary_CVPR_2021/#groomed-nms-grouped-mathematically-differentiable-nms-for-monocular-3d-object-detection","text":"pdf code \u8fd9\u7bc7paper\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684NMS\u7b97\u6cd5\uff0c\u4f46\u662f\u4ec5\u4ec5\u4f7f\u7528\u4e862D\u7b49\u4fe1\u606f\uff0c\u4f46\u662f\u4ec5evaluate\u57283D\u7b97\u6cd5\u4e0a.\u601d\u8def\u662f\u8ba9NMS\u53d8\u5f97\u53ef\u4ee5\u8bad\u7ec3","title":"GrooMeD-NMS: Grouped Mathematically Differentiable NMS for Monocular 3D Object Detection"},{"location":"other_categories/Summaries/Summary_CVPR_2021/#learning-optical-flow-from-a-few-matches","text":"pdf \u8fd9\u7bc7\u6587\u7ae0\u7684\u4f5c\u8005\u6307\u51fa\u65b0\u7684\u5149\u6d41\u6846\u67b6\uff0c\u6bd4\u5982 RAFT , \u4f1a\u8ba1\u7b97\u6bcf\u4e00\u4e2a\u50cf\u7d20\u4e0e\u6574\u4e2a\u7279\u5f81\u56fe\u8ba1\u7b97\u5339\u914d\u4ee5\u53ca\u5149\u6d41\u3002\u4f46\u662f\u4f5c\u8005\u8ba4\u4e3a\u8fd9\u4e2a\u5360\u7528\u5b58\u50a8\u592a\u591a\uff0c\u4e0d\u597d\uff0c\u4f46\u662f\u539f\u6765\u7684\u90bb\u57df\u5339\u914d\u4e5f\u8fd8\u662f\u4e0d\u5c3d\u5982\u4eba\u610f\u3002\u4f5c\u8005\u63d0\u51fa\u5728RAFT\u7684\u57fa\u7840\u4e0a\uff0c\u53ea\u5b58\u50a8\u6bcf\u4e2a\u50cf\u7d20\u7684tok-k\u4e2a\u5339\u914d\u3002\u6700\u7ec8\u5f62\u6210\u4e00\u4e2a\u79bb\u6563\u7684\u5339\u914d\u3002\u5b58\u50a8\u65b9\u6cd5\u4e5f\u4ece\u5bc6\u96c6\u5b58\u50a8\u53d8\u4e3a{\u503c\uff0c\u5750\u6807}\u5b58\u50a8","title":"Learning Optical Flow From a Few Matches"},{"location":"other_categories/Summaries/Summary_CVPR_2021/#upflow-upsampling-pyramid-for-unsupervised-optical-flow-learning","text":"pdf code \u8fd9\u7bc7paper\u6539\u5584\u4e86\u65e0\u76d1\u7763\u7684\u5149\u6d41\u3002 self-guided upsample. Loss guidance at pyramid levels. \u5176\u4ed6\u65e0\u76d1\u7763\u5149\u6d41\u7684\u4f4e\u5206\u8fa8\u7387\u8f93\u51fa\u5c42\u662f\u7528GT\u8bad\u7ec3\u7684\u3002\u672c\u6587\u5219\u662f\u7528\u9ad8\u5206\u8fa8\u7387\u7684\u8f93\u51fa\u4e0b\u91c7\u6837\u8fdb\u884c\u8bad\u7ec3\uff0c","title":"UPFlow: Upsampling Pyramid for Unsupervised Optical Flow Learning"},{"location":"other_categories/Summaries/Summary_CVPR_2021/#autoflow-learning-a-better-training-set-for-optical-flow","text":"pdf project-page \u8fd9\u7bc7paper\u7684\u4efb\u52a1\u662f\u6839\u636e\u5c11\u91cf\u975e\u8fde\u7eed\u56fe\u7247\u6570\u636e\uff0c\u5408\u6210\u5149\u6d41\u8bad\u7ec3\u7528\u7684\u6570\u636e\u3002","title":"AutoFlow: Learning a Better Training Set for Optical Flow"},{"location":"other_categories/Summaries/Summary_ICCV_2021/","text":"Summaries for several ICCV 2021 papers Rank & Sort Loss for Object Detection and Instance Segmentation pdf code \u8fd9\u7bc7paper\u7684\u4f5c\u8005\u4e0e aLRPLoss \u662f\u540c\u4e00\u4e2a\u4f5c\u8005\uff0c\u8fd9\u7bc7paper\u5728\u7406\u8bba\u4ee5\u53ca\u65b9\u6cd5\u4e0a\u4e5f\u662f\u4f1a\u6709\u4e00\u5b9a\u7684\u8fde\u7eed\u6027\u3002 \u524d\u6587\u7684\u8bbe\u5b9a\u6ca1\u6709\u8003\u8651\u5728 L* \\neq 0 \u7684\u65f6\u5019\u65e0\u6cd5\u89e3\u91ca\u3002\u4e14\u6ca1\u6709\u53ea\u6709 i\\in \\mathcal{P} \\& j \\in \\mathcal{N} \u7684\u65f6\u5019\u5b58\u5728\u635f\u5931\uff0c\u4e5f\u5ffd\u7565\u4e86\u7c7b\u4e4b\u95f4\u7684\u635f\u5931\u3002 \u635f\u5931\u7684\u5b9a\u4e49: \\mathcal{L}=\\frac{1}{Z} \\sum_{i \\in \\mathcal{P} \\cup \\mathcal{N}}\\left(\\ell(i)-\\ell^{*}(i)\\right) \u8ba1\u7b97\u65b9\u6cd5\u8f6c\u6362\u4e3a: L_{ij} = (l(i) - l*(i))p(j|i) FaPN: Feature-aligned Pyramid Network for Dense Image Prediction pdf code \u76f4\u89c9\u4e0a\u6765\u8bf4\u89e3\u51b3\u7684\u662f\u4f4e\u5206\u8fa8\u7387\u7279\u5f81\u4e0a\u91c7\u6837\u65f6\u5019\u7684alignment\u95ee\u9898\u3002\u65b9\u6cd5\u4e0a\u4e3b\u8981\u662fdeformable conv and attention. AutoShape: Real-Time Shape-Aware Monocular 3D Object Detection pdf code \u8fd9\u7bc7paper\u5728 [MonoFlex]\u7b49\u6587\u7ae0\u7684\u57fa\u7840\u4e0a\uff0c\u63d0\u51fa\u57fa\u4e8e\u8f66\u8f86\u7684CAD\u7684\u6a21\u578b\u63d0\u53d6\u5bf9\u5e94\u7684keypoints, \u8fd9\u6837\u5c31\u4e0d\u9700\u8981\u5bf9\u8fd9\u7a7a\u767d\u7684\u80cc\u666f\u63d0\u53d6\u8f66\u8f86\u5305\u7edc\u6846\u7684\u5173\u952e\u70b9\u4e86\u3002 \u672c\u6587\u5b9e\u73b0\u4e86\u76ee\u524dSOTA\u7684\u6027\u80fd\u3002 \u53c2\u8003 \u8868\u683c SOTR: Segmenting Objects with Transformers pdf code \u8fd9\u7bc7paper\u4f7f\u7528Transformer \u5904\u7406 Instance Segmentation \u5982 YOLOACT \u4e2d\u7684\u5206\u7c7b\u5934\uff0c\u5e76\u63d0\u4f9b\u52a8\u6001\u5377\u79ef\u6838\u8f93\u51fa\u4e0d\u540c\u5f97instances. Are we Missing Confidence in Pseudo-LiDAR Methods for Monocular 3D Object Detection? pdf \u8fd9\u7bc7\u6587\u7ae0\u6b63\u9762\u5206\u6790\u4e86Pseudo LiDAR\u5728validation set\u4e0a\u4e0e\u5728test set\u4e0a\u5de8\u5927\u7684\u6027\u80fd\u504f\u5dee. \u6bd4\u8f83\u663e\u7136\u7684\u662f,depth prediction\u7684\u8bad\u7ec3\u96c6\u548c\u68c0\u6d4b\u7684\u8bad\u7ec3\u96c6\u548c\u9a8c\u8bc1\u96c6\u4e4b\u95f4\u6709\u5f88\u5927\u7684\u4ea4\u96c6.\u8fd9\u4f7f\u5f97\u5728\u4e00\u90e8\u5206\u7684validation set\u4e0a,\u9884\u8bad\u7ec3\u7684\u6df1\u5ea6\u9884\u6d4b\u7f51\u7edc\u80fd\u9884\u6d4b\u51fa\u4e0d\u771f\u5b9e\u7684\u9ad8\u7cbe\u786e\u5ea6.\u5927\u5e45\u5ea6\u5730\u63d0\u5347\u4e86validation\u7684\u70b9\u6570. \u4e0a\u56fe\u8bf4\u660e\u4e86\u73b0\u5728PLiDAR\u8def\u5f84\u5f88\u5f3a\u7684validation\u7ed3\u679c\u4f46\u662f\u4e0d\u662f\u7279\u522b\u5f3a\u7684test set\u7ed3\u679c. \u901a\u8fc7\u63cf\u753bdetection sequence\u548cdepth prediction sequence\u7684GIS\u91cd\u53e0\u5ea6.\u53ef\u4ee5\u5f97\u5230\u4e0b\u56fe \u672c\u6587\u63d0\u51fa\u589e\u52a0\u9884\u6d4b3D bounding box\u7684\u7f6e\u4fe1\u5ea6.\u540c\u65f6\u63a7\u5236\u5728validation training\u7684\u65f6\u5019\u4f7f\u7528\u7684depth prediction split. \u5f97\u5230\u4e86\u66f4\u52a0\u5e73\u8861\u7684PLiDAR\u7ed3\u679c. \u6027\u80fd\u548c\u7f51\u7edc\u7ed3\u6784\u4e0a\u7684\u521b\u65b0\u5e76\u4e0d\u662f\u5f88\u5de8\u5927, \u4f46\u662f\u7cfb\u7edf\u5730\u6b63\u9762\u8bf4\u660e\u4e86PLiDAR\u76ee\u524d\u5728validation set\u4e0a\u5931\u771f\u7684\u4e8b\u5b9e.","title":"Summaries for several ICCV 2021 papers"},{"location":"other_categories/Summaries/Summary_ICCV_2021/#summaries-for-several-iccv-2021-papers","text":"","title":"Summaries for several ICCV 2021 papers"},{"location":"other_categories/Summaries/Summary_ICCV_2021/#rank-sort-loss-for-object-detection-and-instance-segmentation","text":"pdf code \u8fd9\u7bc7paper\u7684\u4f5c\u8005\u4e0e aLRPLoss \u662f\u540c\u4e00\u4e2a\u4f5c\u8005\uff0c\u8fd9\u7bc7paper\u5728\u7406\u8bba\u4ee5\u53ca\u65b9\u6cd5\u4e0a\u4e5f\u662f\u4f1a\u6709\u4e00\u5b9a\u7684\u8fde\u7eed\u6027\u3002 \u524d\u6587\u7684\u8bbe\u5b9a\u6ca1\u6709\u8003\u8651\u5728 L* \\neq 0 \u7684\u65f6\u5019\u65e0\u6cd5\u89e3\u91ca\u3002\u4e14\u6ca1\u6709\u53ea\u6709 i\\in \\mathcal{P} \\& j \\in \\mathcal{N} \u7684\u65f6\u5019\u5b58\u5728\u635f\u5931\uff0c\u4e5f\u5ffd\u7565\u4e86\u7c7b\u4e4b\u95f4\u7684\u635f\u5931\u3002 \u635f\u5931\u7684\u5b9a\u4e49: \\mathcal{L}=\\frac{1}{Z} \\sum_{i \\in \\mathcal{P} \\cup \\mathcal{N}}\\left(\\ell(i)-\\ell^{*}(i)\\right) \u8ba1\u7b97\u65b9\u6cd5\u8f6c\u6362\u4e3a: L_{ij} = (l(i) - l*(i))p(j|i)","title":"Rank &amp; Sort Loss for Object Detection and Instance Segmentation"},{"location":"other_categories/Summaries/Summary_ICCV_2021/#fapn-feature-aligned-pyramid-network-for-dense-image-prediction","text":"pdf code \u76f4\u89c9\u4e0a\u6765\u8bf4\u89e3\u51b3\u7684\u662f\u4f4e\u5206\u8fa8\u7387\u7279\u5f81\u4e0a\u91c7\u6837\u65f6\u5019\u7684alignment\u95ee\u9898\u3002\u65b9\u6cd5\u4e0a\u4e3b\u8981\u662fdeformable conv and attention.","title":"FaPN: Feature-aligned Pyramid Network for Dense Image Prediction"},{"location":"other_categories/Summaries/Summary_ICCV_2021/#autoshape-real-time-shape-aware-monocular-3d-object-detection","text":"pdf code \u8fd9\u7bc7paper\u5728 [MonoFlex]\u7b49\u6587\u7ae0\u7684\u57fa\u7840\u4e0a\uff0c\u63d0\u51fa\u57fa\u4e8e\u8f66\u8f86\u7684CAD\u7684\u6a21\u578b\u63d0\u53d6\u5bf9\u5e94\u7684keypoints, \u8fd9\u6837\u5c31\u4e0d\u9700\u8981\u5bf9\u8fd9\u7a7a\u767d\u7684\u80cc\u666f\u63d0\u53d6\u8f66\u8f86\u5305\u7edc\u6846\u7684\u5173\u952e\u70b9\u4e86\u3002 \u672c\u6587\u5b9e\u73b0\u4e86\u76ee\u524dSOTA\u7684\u6027\u80fd\u3002 \u53c2\u8003 \u8868\u683c","title":"AutoShape: Real-Time Shape-Aware Monocular 3D Object Detection"},{"location":"other_categories/Summaries/Summary_ICCV_2021/#sotr-segmenting-objects-with-transformers","text":"pdf code \u8fd9\u7bc7paper\u4f7f\u7528Transformer \u5904\u7406 Instance Segmentation \u5982 YOLOACT \u4e2d\u7684\u5206\u7c7b\u5934\uff0c\u5e76\u63d0\u4f9b\u52a8\u6001\u5377\u79ef\u6838\u8f93\u51fa\u4e0d\u540c\u5f97instances.","title":"SOTR: Segmenting Objects with Transformers"},{"location":"other_categories/Summaries/Summary_ICCV_2021/#are-we-missing-confidence-in-pseudo-lidar-methods-for-monocular-3d-object-detection","text":"pdf \u8fd9\u7bc7\u6587\u7ae0\u6b63\u9762\u5206\u6790\u4e86Pseudo LiDAR\u5728validation set\u4e0a\u4e0e\u5728test set\u4e0a\u5de8\u5927\u7684\u6027\u80fd\u504f\u5dee. \u6bd4\u8f83\u663e\u7136\u7684\u662f,depth prediction\u7684\u8bad\u7ec3\u96c6\u548c\u68c0\u6d4b\u7684\u8bad\u7ec3\u96c6\u548c\u9a8c\u8bc1\u96c6\u4e4b\u95f4\u6709\u5f88\u5927\u7684\u4ea4\u96c6.\u8fd9\u4f7f\u5f97\u5728\u4e00\u90e8\u5206\u7684validation set\u4e0a,\u9884\u8bad\u7ec3\u7684\u6df1\u5ea6\u9884\u6d4b\u7f51\u7edc\u80fd\u9884\u6d4b\u51fa\u4e0d\u771f\u5b9e\u7684\u9ad8\u7cbe\u786e\u5ea6.\u5927\u5e45\u5ea6\u5730\u63d0\u5347\u4e86validation\u7684\u70b9\u6570. \u4e0a\u56fe\u8bf4\u660e\u4e86\u73b0\u5728PLiDAR\u8def\u5f84\u5f88\u5f3a\u7684validation\u7ed3\u679c\u4f46\u662f\u4e0d\u662f\u7279\u522b\u5f3a\u7684test set\u7ed3\u679c. \u901a\u8fc7\u63cf\u753bdetection sequence\u548cdepth prediction sequence\u7684GIS\u91cd\u53e0\u5ea6.\u53ef\u4ee5\u5f97\u5230\u4e0b\u56fe \u672c\u6587\u63d0\u51fa\u589e\u52a0\u9884\u6d4b3D bounding box\u7684\u7f6e\u4fe1\u5ea6.\u540c\u65f6\u63a7\u5236\u5728validation training\u7684\u65f6\u5019\u4f7f\u7528\u7684depth prediction split. \u5f97\u5230\u4e86\u66f4\u52a0\u5e73\u8861\u7684PLiDAR\u7ed3\u679c. \u6027\u80fd\u548c\u7f51\u7edc\u7ed3\u6784\u4e0a\u7684\u521b\u65b0\u5e76\u4e0d\u662f\u5f88\u5de8\u5927, \u4f46\u662f\u7cfb\u7edf\u5730\u6b63\u9762\u8bf4\u660e\u4e86PLiDAR\u76ee\u524d\u5728validation set\u4e0a\u5931\u771f\u7684\u4e8b\u5b9e.","title":"Are we Missing Confidence in Pseudo-LiDAR Methods for Monocular 3D Object Detection?"},{"location":"other_categories/Summaries/Summary_ICLR_2021/","text":"Summaries for several ICLR 2021 papers An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale pdf code pytorch-code \u8fd9\u7bc7paper\u4f7f\u7528transformer\u8fdb\u884c\u89c6\u89c9\u5206\u7c7b\u3002 \u5177\u4f53\u505a\u6cd5\u4e0a\uff0c\u5bf9\u4e8e224x224\u7684\u56fe\u7247\u53ef\u4ee5\u5206\u621016x16\u7684\u4e00\u7cfb\u5217\u5c0f\u5757\uff0c\u6bcf\u4e00\u4e2a\u5c0f\u5757\u5e26\u4e0a\u4e00\u7ef4\u7684positional encoding\u653e\u5165transformer\u4e2d\uff0c\u5355\u5c42\u8fde\u63a5: \\begin{aligned} \\mathbf{z}_{0} &=\\left[\\mathbf{x}_{\\text {class }} ; \\mathbf{x}_{p}^{1} \\mathbf{E} ; \\mathbf{x}_{p}^{2} \\mathbf{E} ; \\cdots ; \\mathbf{x}_{p}^{N} \\mathbf{E}\\right]+\\mathbf{E}_{\\text {pos }}, & \\mathbf{E} \\in \\mathbb{R}^{\\left(P^{2} \\cdot C\\right) \\times D}, \\mathbf{E}_{\\text {pos }} \\in \\mathbb{R}^{(N+1) \\times D} \\\\ \\mathbf{z}_{\\ell}^{\\prime} &=\\operatorname{MSA}\\left(\\operatorname{LN}\\left(\\mathbf{z}_{\\ell-1}\\right)\\right)+\\mathbf{z}_{\\ell-1}, & \\ell=1 \\ldots L \\\\ \\mathbf{z}_{\\ell} &=\\operatorname{MLP}\\left(\\operatorname{LN}\\left(\\mathbf{z}_{\\ell}^{\\prime}\\right)\\right)+\\mathbf{z}_{\\ell}^{\\prime}, & \\ell=1 \\ldots L \\\\ \\mathbf{y} &=\\mathrm{LN}\\left(\\mathbf{z}_{L}^{0}\\right) & \\end{aligned} \u6574\u4e2a\u7f51\u7edc\u4e0d\u4f7f\u7528\u5377\u79ef\uff0c\u4f46\u662f\u80fd\u5728\u5927\u5927\u5c0f\u5c0f\u7684\u5206\u7c7b\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u5377\u79ef\u7684\u6c34\u5e73. What Matters for On-Policy Deep Actor-Critic Methods? A Large-Scale Study pdf \u8fd9\u7bc7\u6587\u7ae0\u7efc\u5408\u505a\u4e86\u5f88\u7ec6\u81f4\u5f88\u5927\u89c4\u6a21\u7684\u5b9e\u9a8c\uff0c\u8ba4\u771f\u5730\u5206\u6790\u4e86\u7a76\u7adf\u662f\u54ea\u4e9b\u56e0\u7d20\u4fc3\u8fdb\u4e86on-policy \u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60 AC\u7b97\u6cd5\u7684\u6027\u80fd\u3002\u672c\u6587\u89c4\u8303\u5316\u5730\u5b9e\u73b0\u4e8650\u591a\u4e2a\u8bbe\u8ba1\u56e0\u7d20\uff0c\u7136\u540e\u518d\u4e94\u4e2a\u4e0d\u540c\u7684\u73af\u5883\u4e0b\u8fdb\u884c\u4e86\u5927\u91cf\u7684\u5b9e\u9a8c\u3002 \u4e00\u4e2a \u77e5\u4e4e\u535a\u5ba2 \u6536\u96c6\u4e86\u5176\u4e2d\u7684\u5b9e\u9a8c\u7ed3\u8bba How Neural Networks Extrapolate: From Feedforward to Graph Neural Networks pdf \u8fd9\u7bc7paper\u5c1d\u8bd5\u89e3\u7b54\u4e86\u5168\u8fde\u63a5ReLU\u7f51\u7edc\u4ee5\u53caGNN\u662f\u5982\u4f55extrapolate\u7684\uff0c\u4e5f\u5c31\u662f\u5728\u4ec0\u4e48\u60c5\u51b5\u4e0b\uff0c\u5168\u8fde\u63a5\u4ee5\u53caGNN\u80fd\u5b66\u4e60\u5230training distribution\u4e4b\u5916\u7684\u6b63\u786e\u7ed3\u679c\uff0c\u5e76\u7ed9\u51fa\u4e86\u7406\u8bba\u8bc1\u660e. Image Augmentation Is All You Need: Regularizing Deep Reinforcement Learning from Pixels pdf \u8fd9\u7bc7paper\u53d1\u73b0\u7b80\u5355\u7684\u6570\u636e\u589e\u5f3a\u4e5f\u80fd\u5b9e\u73b0\u589e\u5f3a\u5b66\u4e60\u7684SOTA.","title":"Summaries for several ICLR 2021 papers"},{"location":"other_categories/Summaries/Summary_ICLR_2021/#summaries-for-several-iclr-2021-papers","text":"","title":"Summaries for several ICLR 2021 papers"},{"location":"other_categories/Summaries/Summary_ICLR_2021/#an-image-is-worth-16x16-words-transformers-for-image-recognition-at-scale","text":"pdf code pytorch-code \u8fd9\u7bc7paper\u4f7f\u7528transformer\u8fdb\u884c\u89c6\u89c9\u5206\u7c7b\u3002 \u5177\u4f53\u505a\u6cd5\u4e0a\uff0c\u5bf9\u4e8e224x224\u7684\u56fe\u7247\u53ef\u4ee5\u5206\u621016x16\u7684\u4e00\u7cfb\u5217\u5c0f\u5757\uff0c\u6bcf\u4e00\u4e2a\u5c0f\u5757\u5e26\u4e0a\u4e00\u7ef4\u7684positional encoding\u653e\u5165transformer\u4e2d\uff0c\u5355\u5c42\u8fde\u63a5: \\begin{aligned} \\mathbf{z}_{0} &=\\left[\\mathbf{x}_{\\text {class }} ; \\mathbf{x}_{p}^{1} \\mathbf{E} ; \\mathbf{x}_{p}^{2} \\mathbf{E} ; \\cdots ; \\mathbf{x}_{p}^{N} \\mathbf{E}\\right]+\\mathbf{E}_{\\text {pos }}, & \\mathbf{E} \\in \\mathbb{R}^{\\left(P^{2} \\cdot C\\right) \\times D}, \\mathbf{E}_{\\text {pos }} \\in \\mathbb{R}^{(N+1) \\times D} \\\\ \\mathbf{z}_{\\ell}^{\\prime} &=\\operatorname{MSA}\\left(\\operatorname{LN}\\left(\\mathbf{z}_{\\ell-1}\\right)\\right)+\\mathbf{z}_{\\ell-1}, & \\ell=1 \\ldots L \\\\ \\mathbf{z}_{\\ell} &=\\operatorname{MLP}\\left(\\operatorname{LN}\\left(\\mathbf{z}_{\\ell}^{\\prime}\\right)\\right)+\\mathbf{z}_{\\ell}^{\\prime}, & \\ell=1 \\ldots L \\\\ \\mathbf{y} &=\\mathrm{LN}\\left(\\mathbf{z}_{L}^{0}\\right) & \\end{aligned} \u6574\u4e2a\u7f51\u7edc\u4e0d\u4f7f\u7528\u5377\u79ef\uff0c\u4f46\u662f\u80fd\u5728\u5927\u5927\u5c0f\u5c0f\u7684\u5206\u7c7b\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u5377\u79ef\u7684\u6c34\u5e73.","title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"},{"location":"other_categories/Summaries/Summary_ICLR_2021/#what-matters-for-on-policy-deep-actor-critic-methods-a-large-scale-study","text":"pdf \u8fd9\u7bc7\u6587\u7ae0\u7efc\u5408\u505a\u4e86\u5f88\u7ec6\u81f4\u5f88\u5927\u89c4\u6a21\u7684\u5b9e\u9a8c\uff0c\u8ba4\u771f\u5730\u5206\u6790\u4e86\u7a76\u7adf\u662f\u54ea\u4e9b\u56e0\u7d20\u4fc3\u8fdb\u4e86on-policy \u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60 AC\u7b97\u6cd5\u7684\u6027\u80fd\u3002\u672c\u6587\u89c4\u8303\u5316\u5730\u5b9e\u73b0\u4e8650\u591a\u4e2a\u8bbe\u8ba1\u56e0\u7d20\uff0c\u7136\u540e\u518d\u4e94\u4e2a\u4e0d\u540c\u7684\u73af\u5883\u4e0b\u8fdb\u884c\u4e86\u5927\u91cf\u7684\u5b9e\u9a8c\u3002 \u4e00\u4e2a \u77e5\u4e4e\u535a\u5ba2 \u6536\u96c6\u4e86\u5176\u4e2d\u7684\u5b9e\u9a8c\u7ed3\u8bba","title":"What Matters for On-Policy Deep Actor-Critic Methods? A Large-Scale Study"},{"location":"other_categories/Summaries/Summary_ICLR_2021/#how-neural-networks-extrapolate-from-feedforward-to-graph-neural-networks","text":"pdf \u8fd9\u7bc7paper\u5c1d\u8bd5\u89e3\u7b54\u4e86\u5168\u8fde\u63a5ReLU\u7f51\u7edc\u4ee5\u53caGNN\u662f\u5982\u4f55extrapolate\u7684\uff0c\u4e5f\u5c31\u662f\u5728\u4ec0\u4e48\u60c5\u51b5\u4e0b\uff0c\u5168\u8fde\u63a5\u4ee5\u53caGNN\u80fd\u5b66\u4e60\u5230training distribution\u4e4b\u5916\u7684\u6b63\u786e\u7ed3\u679c\uff0c\u5e76\u7ed9\u51fa\u4e86\u7406\u8bba\u8bc1\u660e.","title":"How Neural Networks Extrapolate: From Feedforward to Graph Neural Networks"},{"location":"other_categories/Summaries/Summary_ICLR_2021/#image-augmentation-is-all-you-need-regularizing-deep-reinforcement-learning-from-pixels","text":"pdf \u8fd9\u7bc7paper\u53d1\u73b0\u7b80\u5355\u7684\u6570\u636e\u589e\u5f3a\u4e5f\u80fd\u5b9e\u73b0\u589e\u5f3a\u5b66\u4e60\u7684SOTA.","title":"Image Augmentation Is All You Need: Regularizing Deep Reinforcement Learning from Pixels"},{"location":"other_categories/Summaries/Summary_ICML_2021/","text":"Summaries for several ICML 2021 papers ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision pdf \u672c\u6587\u662f\u7b2c\u4e00\u7bc7\u4f7f\u7528Transformer\uff0c\u4e0d\u4f7f\u7528CNN feature \u5b9e\u73b0\u7684VLP\u6a21\u578b Relative Positional Encoding for Transformers with Linear Complexity pdf code \u8fd9\u7bc7paper\u63d0\u51fa\u4e86 Stochastic Positional Encoding (SPE)\uff0c\u91cd\u70b9\u7684\u601d\u8def\u5728\u4e8e\u5c06\u51c6\u786e\u7684positional Attention\u7406\u89e3\u4e3a\u4e00\u4e2acovariance(\u56e0\u4e3a\u5b83\u8bbe\u8ba1\u4e24\u4e2afeature\u4ee5\u53capositional encoding\u7684\u70b9\u4e58)\u56e0\u800c\u53ef\u4ee5\u7528\u91c7\u6837\u53bb\u5b66\u4e60\u3002 Accelerate CNNs from Three Dimensions: A Comprehensive Pruning Framework pdf \u672c\u6587\u6709\u4e00\u4e2a\u5f88\u597d\u7684 CSDN blog \u8fd9\u7bc7paper\u63d0\u51fa\u4e86\u4e00\u4e2a\u5f88\u5168\u9762\u7684\u526a\u679d\u6846\u67b6\uff0c\u4ece\u4e09\u4e2a\u7ef4\u5ea6\u5bf9CNN\u8fdb\u884c\u52a0\u901f\u3002\u6709\u4e00\u70b9\u50cf\u662fEfficient Net\u5bf9\u6df1\u5ea6\uff0c\u5bbd\u5ea6\uff0c\u5206\u8fa8\u7387\u4e00\u7ec4\u7cfb\u6570\u4e3a\u57fa\u51c6\u7136\u540e\u53bb\u653e\u5927\u7f51\u7edc\uff0c\u8fd9\u7bc7paper\u5219\u662f\u786e\u5b9a\u4e00\u7ec4\u53c2\u6570\u53bbprune\u7f51\u8def. \u672c\u6587\u4e3b\u8981\u7684\u89c2\u70b9: \u4ee5\u5f80\u7684\u65b9\u6cd5\uff0c\u8981\u4e48\u662f\u5bf9layer\u8fdb\u884c\u526a\u679d\uff0c\u8981\u4e48\u662f\u5bf9filter\u8fdb\u884c\u526a\u679d\uff0c\u751a\u81f3\u662f\u7f29\u5c0fresolution. \u5982\u679c\u540c\u65f6\u7740\u773c\u591a\u4e2a\u7ef4\u5ea6\uff0c\u53ef\u4ee5\u6709\u66f4\u9ad8\u7684\u6027\u80fd\u6548\u679c. \u540c\u65f6\u7684pruning\u53ef\u4ee5\u7406\u89e3\u4e3a\u4e00\u4e2a\u4f18\u5316\u95ee\u9898\uff0c\u800c\u7cbe\u5ea6\u53ef\u4ee5\u88ab\u5efa\u6a21\u6210\u6df1\u5ea6\u5bbd\u5ea6\u4ee5\u53ca\u5206\u8fa8\u7387\u7684\u4e00\u4e2a\u591a\u9879\u5f0f. \\begin{aligned} d^{\\star}, w^{\\star}, r^{\\star}=& \\underset{d, w, r}{\\arg \\max } \\mathcal{F}(d, w, r ; \\Lambda) \\\\ \\text { s.t. } \\mathcal{C}(d, w, r)&=T \\times \\mathcal{C}\\left(d_{0}, w_{0}, r_{0}\\right), \\\\ \\mathcal{C}(d,w,r) &= dw^2r^2 \\\\ \\mathcal{F}(d, w, r, \\Lambda) &= \\sum^\\mathcal{K}_{i,j,k=0}{\\lambda_{i,j,k}d^iw^jr^k} \\end{aligned} Differentiable Spatial Planning using Transformers pdf \u8fd9\u7bc7paper\u63d0\u51fa\u7528transformer\u8fdb\u884cvalue iteration\u6765\u52a0\u901fplanning\u7684value\u6269\u6563.","title":"Summaries for several ICML 2021 papers"},{"location":"other_categories/Summaries/Summary_ICML_2021/#summaries-for-several-icml-2021-papers","text":"","title":"Summaries for several ICML 2021 papers"},{"location":"other_categories/Summaries/Summary_ICML_2021/#vilt-vision-and-language-transformer-without-convolution-or-region-supervision","text":"pdf \u672c\u6587\u662f\u7b2c\u4e00\u7bc7\u4f7f\u7528Transformer\uff0c\u4e0d\u4f7f\u7528CNN feature \u5b9e\u73b0\u7684VLP\u6a21\u578b","title":"ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision"},{"location":"other_categories/Summaries/Summary_ICML_2021/#relative-positional-encoding-for-transformers-with-linear-complexity","text":"pdf code \u8fd9\u7bc7paper\u63d0\u51fa\u4e86 Stochastic Positional Encoding (SPE)\uff0c\u91cd\u70b9\u7684\u601d\u8def\u5728\u4e8e\u5c06\u51c6\u786e\u7684positional Attention\u7406\u89e3\u4e3a\u4e00\u4e2acovariance(\u56e0\u4e3a\u5b83\u8bbe\u8ba1\u4e24\u4e2afeature\u4ee5\u53capositional encoding\u7684\u70b9\u4e58)\u56e0\u800c\u53ef\u4ee5\u7528\u91c7\u6837\u53bb\u5b66\u4e60\u3002","title":"Relative Positional Encoding for Transformers with Linear Complexity"},{"location":"other_categories/Summaries/Summary_ICML_2021/#accelerate-cnns-from-three-dimensions-a-comprehensive-pruning-framework","text":"pdf \u672c\u6587\u6709\u4e00\u4e2a\u5f88\u597d\u7684 CSDN blog \u8fd9\u7bc7paper\u63d0\u51fa\u4e86\u4e00\u4e2a\u5f88\u5168\u9762\u7684\u526a\u679d\u6846\u67b6\uff0c\u4ece\u4e09\u4e2a\u7ef4\u5ea6\u5bf9CNN\u8fdb\u884c\u52a0\u901f\u3002\u6709\u4e00\u70b9\u50cf\u662fEfficient Net\u5bf9\u6df1\u5ea6\uff0c\u5bbd\u5ea6\uff0c\u5206\u8fa8\u7387\u4e00\u7ec4\u7cfb\u6570\u4e3a\u57fa\u51c6\u7136\u540e\u53bb\u653e\u5927\u7f51\u7edc\uff0c\u8fd9\u7bc7paper\u5219\u662f\u786e\u5b9a\u4e00\u7ec4\u53c2\u6570\u53bbprune\u7f51\u8def. \u672c\u6587\u4e3b\u8981\u7684\u89c2\u70b9: \u4ee5\u5f80\u7684\u65b9\u6cd5\uff0c\u8981\u4e48\u662f\u5bf9layer\u8fdb\u884c\u526a\u679d\uff0c\u8981\u4e48\u662f\u5bf9filter\u8fdb\u884c\u526a\u679d\uff0c\u751a\u81f3\u662f\u7f29\u5c0fresolution. \u5982\u679c\u540c\u65f6\u7740\u773c\u591a\u4e2a\u7ef4\u5ea6\uff0c\u53ef\u4ee5\u6709\u66f4\u9ad8\u7684\u6027\u80fd\u6548\u679c. \u540c\u65f6\u7684pruning\u53ef\u4ee5\u7406\u89e3\u4e3a\u4e00\u4e2a\u4f18\u5316\u95ee\u9898\uff0c\u800c\u7cbe\u5ea6\u53ef\u4ee5\u88ab\u5efa\u6a21\u6210\u6df1\u5ea6\u5bbd\u5ea6\u4ee5\u53ca\u5206\u8fa8\u7387\u7684\u4e00\u4e2a\u591a\u9879\u5f0f. \\begin{aligned} d^{\\star}, w^{\\star}, r^{\\star}=& \\underset{d, w, r}{\\arg \\max } \\mathcal{F}(d, w, r ; \\Lambda) \\\\ \\text { s.t. } \\mathcal{C}(d, w, r)&=T \\times \\mathcal{C}\\left(d_{0}, w_{0}, r_{0}\\right), \\\\ \\mathcal{C}(d,w,r) &= dw^2r^2 \\\\ \\mathcal{F}(d, w, r, \\Lambda) &= \\sum^\\mathcal{K}_{i,j,k=0}{\\lambda_{i,j,k}d^iw^jr^k} \\end{aligned}","title":"Accelerate CNNs from Three Dimensions: A Comprehensive Pruning Framework"},{"location":"other_categories/Summaries/Summary_ICML_2021/#differentiable-spatial-planning-using-transformers","text":"pdf \u8fd9\u7bc7paper\u63d0\u51fa\u7528transformer\u8fdb\u884cvalue iteration\u6765\u52a0\u901fplanning\u7684value\u6269\u6563.","title":"Differentiable Spatial Planning using Transformers"},{"location":"other_categories/Summaries/Summary_MapExtraction/","text":"Summary of Several Map Extraction Papers \u672cblog\u6536\u96c6\u4e00\u7cfb\u5217\u5173\u4e8e\u4ece\u7a7a\u4e2d\u56fe\u7247\u63d0\u53d6\u5730\u9762\u5730\u56fe\u7684paper.\u672c\u8d28\u4e0a\u6765\u8bf4\u5c5e\u4e8e\u4ece\u56fe\u7247\u63d0\u53d6\u7531\u8def\u7ebf(edge)\u4e0e\u4ea4\u53c9\u70b9(node)\u7ec4\u6210\u7684\u56fe(graph) \u4f20\u7edf\u7684\u601d\u8def: 1. \u8bed\u4e49\u5206\u5272\u5206\u5272\u51fa\u8def\u9762\u548c\u975e\u8def\u9762\u7684binary map 2. \u50cf\u7d20\u6536\u7f29,\u4f7f\u5f97binary map\u53ea\u5269\u4e0b\u5355\u7ebf\u7684\u8def\u9762. 3. \u6c42\u8fde\u901a\u57df,\u5e76\u4fee\u526a\u5730\u56fe. RoadTracer: Automatic Extraction of Road Networks from Aerial Images pdf code \u8fd9\u7bc7\u8bba\u6587\u5f00\u542f\u4e86\u4ee5\u8fed\u4ee3\u4e3abaseline\u65b9\u6cd5\u7684\u56fe\u751f\u6210. \u5728\u7ed9\u51fa\u8d77\u70b9\u4e4b\u540e,\u7f51\u7edc\u5728\u6bcf\u4e00\u4e2astep, crop\u51fa\u9644\u8fd1\u7684RGB\u56fe,\u9884\u6d4b\u4e0b\u4e00\u6b65edge\u7684\u751f\u957f\u65b9\u5411. \u672c\u6587\u7684\u751f\u957f\u8ddd\u79bb\u662f\u56fa\u5b9a\u957f\u5ea6(fix-step)\u7684.\u7f51\u7edc\u7684\u9884\u6d4b\u662f\u662f\u5426\u505c\u6b62\u4ee5\u53ca\u9884\u6d4b\u89d2\u5ea6. \u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0b\u53ef\u4ee5\u7406\u89e3\u4e3a\u6a21\u4eff\u5b66\u4e60\u7684\u8bad\u7ec3\u6a21\u5f0f.\u5982\u679c\u4f7f\u7528GT\u5730\u56fe\u7684\u8def\u7ebf\u751f\u6210\u751f\u957f\u65b9\u5411,\u8ba9\u7f51\u7edc\u5728\u8bad\u7ec3\u7684\u65f6\u5019\u6a21\u4effGT\u5730\u56fe\u7684\u751f\u957f\u65b9\u5f0f,\u7f51\u7edc\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u4f1a\u56e0\u4e3a\u566a\u97f3\u504f\u79bb\u5f00\u6b63\u786e\u7684\u8def\u7ebf\u5e76\u65e0\u6cd5\u590d\u539f. \u4eba\u9020\u7684\u566a\u58f0\u4e5f\u65e0\u6cd5\u5b9e\u9645\u5730\u6a21\u4eff\u7f51\u7edc\u7684\u566a\u58f0\u5206\u5e03.\u672c\u6587\u63d0\u53d6\u4f7f\u7528\u52a8\u6001\u751f\u6210label\u7684\u65b9\u5f0f. \u5bf9\u6bcf\u4e00\u4e2acropped\u51fa\u6765\u7684\u5b50\u56fe\u4ee5\u53ca\u73b0\u6709\u7684\u5e8f\u5217,\u751f\u6210\u6807\u7b7e\u7684\u65b9\u6cd5. \u8def\u7ebf\u5206\u914d \u7528map-matching\u7b97\u6cd5\u5224\u65ad\u76ee\u524d\u7684\"\u5e8f\u5217\"\u5c5e\u4e8e\u5730\u56fe\u4e0a\u7684\u54ea\u4e00\u6761\u8def\u5f84(\u4e0d\u80fd\u7528\u5f53\u524d\u70b9\u7684\u6700\u8fd1\u8def\u7ebf\u5c31\u786e\u5b9a, \u5426\u5219\u56e0\u4e3a\u566a\u97f3\u4f1a\u504f\u822a). \u8fd9\u4e2a\u95ee\u9898\u5728GPS/\u624b\u673a\u4fe1\u53f7\u5206\u6790\u65f6\u6709\u7528\u5230,\u5982 \u6b64\u6587 . \u8fd9\u7bc7\u6587\u7ae0\u91cc\u9762\u7814\u7a76\u7684\u662f\u624b\u673a\u4fe1\u53f7\u70b9(GPS)\u4f4d\u7f6e\u5e8f\u5217\u4e0e\u5730\u56fe\u8def\u7ebf\u7684\u5339\u914d\u95ee\u9898.\u88ab\u8bbe\u8ba1\u4e3a\u4e00\u4e2a HMM \u6a21\u578b,\u6bcf\u4e00\u4e2a\u65f6\u523b\u89c2\u6d4b\u503c\u662fGPS\u4f4d\u7f6e,\u9690\u53d8\u91cf\u662f\u6240\u5904\u7684\u8def\u7ebfedge. \u72b6\u6001\u8f6c\u79fb\u77e9\u9635\u7684\u7279\u70b9\u5728\u4e8e\u4e24\u6b21\u6d4b\u91cf\u8981\u4e48\u5728\u540c\u4e00\u4e2a\u8def\u7ebfedge\u4e0a,\u8981\u4e48\u5728\u4e00\u4e2a\u76f8\u4e92\u8fde\u63a5\u7684\u8def\u7ebfedge\u4e0a.\u4ece\u800c\u5f62\u6210\u9a6c\u5c14\u79d1\u592b\u94fe. \u8fd9\u91cc\u7684\u4efb\u52a1\u76f8\u5f53\u4e8e\u5df2\u77e5\u5e8f\u5217\u89c2\u6d4b,\u6c42\u89e3\u6700\u5927\u4f3c\u7136\u7684\u9690\u53d8\u91cf\u5e8f\u5217. \u4f7f\u7528\u7684 Viterbi \u7b97\u6cd5. \u672c\u6587\u7684\u6807\u7b7e\u5c31\u662f\u5728\u5df2\u77e5\u8def\u7ebf\u7684\u5206\u914d\u7684\u60c5\u51b5\u4e0b,\u6295\u5f71\u5230\u6700\u8fd1\u7684\u70b9\u4e0a,\u7136\u540e\u5f80\u524d\u4e00\u4e2a\u56fa\u5b9a\u7684\u6b65\u957f\u4f5c\u4e3a\u76ee\u6807. VecRoad: Point-based Iterative Graph Exploration for Road Graphs Extraction pdf code \u8fd9\u7bc7\u8bba\u6587\u5728\u4e0a\u6587\u7684\u57fa\u7840\u4e0a\u63d0\u51fa\u65b0\u7684\u63d0\u5347. \u4f7f\u7528\u9884\u6d4b\u4e0b\u4e00\u4e2a\u70b9\u7684\u65b9\u6cd5,\u66ff\u4ee3\u65b9\u5411.\u4ece\u800c\u5b9e\u73b0\u5728\u8def\u53e3\u4f4d\u7f6e\u7684\u7075\u6d3b\u5730\u63a7\u5236\u6b65\u957f\uff0c\u4ece\u800c\u7f51\u7edc\u53ef\u4ee5\u5728\u4ea4\u70b9\u5904\u5f97\u5230\u6bd4\u8f83\u51c6\u786e\u3002 \u4f7f\u7528\u8bed\u4e49\u5206\u5272+\u4ea4\u70b9\u8bed\u4e49\u5206\u5272\u4f5c\u4e3a\u51b3\u7b56\u5c0f\u7f51\u7edc\u7684\u8f93\u5165\u3002\u8fd9\u4e9b\u5173\u952e\u7684\u6570\u636ecues\u63d0\u5347\u4e86\u7f51\u7edc\u7684\u51c6\u786e\u5ea6\u3002 iCurb: Imitation Learning-based Detection of Road Curbs using Aerial Images for Autonomous Driving pdf code","title":"Summary of Several Map Extraction Papers"},{"location":"other_categories/Summaries/Summary_MapExtraction/#summary-of-several-map-extraction-papers","text":"\u672cblog\u6536\u96c6\u4e00\u7cfb\u5217\u5173\u4e8e\u4ece\u7a7a\u4e2d\u56fe\u7247\u63d0\u53d6\u5730\u9762\u5730\u56fe\u7684paper.\u672c\u8d28\u4e0a\u6765\u8bf4\u5c5e\u4e8e\u4ece\u56fe\u7247\u63d0\u53d6\u7531\u8def\u7ebf(edge)\u4e0e\u4ea4\u53c9\u70b9(node)\u7ec4\u6210\u7684\u56fe(graph) \u4f20\u7edf\u7684\u601d\u8def: 1. \u8bed\u4e49\u5206\u5272\u5206\u5272\u51fa\u8def\u9762\u548c\u975e\u8def\u9762\u7684binary map 2. \u50cf\u7d20\u6536\u7f29,\u4f7f\u5f97binary map\u53ea\u5269\u4e0b\u5355\u7ebf\u7684\u8def\u9762. 3. \u6c42\u8fde\u901a\u57df,\u5e76\u4fee\u526a\u5730\u56fe.","title":"Summary of Several Map Extraction Papers"},{"location":"other_categories/Summaries/Summary_MapExtraction/#roadtracer-automatic-extraction-of-road-networks-from-aerial-images","text":"pdf code \u8fd9\u7bc7\u8bba\u6587\u5f00\u542f\u4e86\u4ee5\u8fed\u4ee3\u4e3abaseline\u65b9\u6cd5\u7684\u56fe\u751f\u6210. \u5728\u7ed9\u51fa\u8d77\u70b9\u4e4b\u540e,\u7f51\u7edc\u5728\u6bcf\u4e00\u4e2astep, crop\u51fa\u9644\u8fd1\u7684RGB\u56fe,\u9884\u6d4b\u4e0b\u4e00\u6b65edge\u7684\u751f\u957f\u65b9\u5411. \u672c\u6587\u7684\u751f\u957f\u8ddd\u79bb\u662f\u56fa\u5b9a\u957f\u5ea6(fix-step)\u7684.\u7f51\u7edc\u7684\u9884\u6d4b\u662f\u662f\u5426\u505c\u6b62\u4ee5\u53ca\u9884\u6d4b\u89d2\u5ea6. \u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0b\u53ef\u4ee5\u7406\u89e3\u4e3a\u6a21\u4eff\u5b66\u4e60\u7684\u8bad\u7ec3\u6a21\u5f0f.\u5982\u679c\u4f7f\u7528GT\u5730\u56fe\u7684\u8def\u7ebf\u751f\u6210\u751f\u957f\u65b9\u5411,\u8ba9\u7f51\u7edc\u5728\u8bad\u7ec3\u7684\u65f6\u5019\u6a21\u4effGT\u5730\u56fe\u7684\u751f\u957f\u65b9\u5f0f,\u7f51\u7edc\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u4f1a\u56e0\u4e3a\u566a\u97f3\u504f\u79bb\u5f00\u6b63\u786e\u7684\u8def\u7ebf\u5e76\u65e0\u6cd5\u590d\u539f. \u4eba\u9020\u7684\u566a\u58f0\u4e5f\u65e0\u6cd5\u5b9e\u9645\u5730\u6a21\u4eff\u7f51\u7edc\u7684\u566a\u58f0\u5206\u5e03.\u672c\u6587\u63d0\u53d6\u4f7f\u7528\u52a8\u6001\u751f\u6210label\u7684\u65b9\u5f0f. \u5bf9\u6bcf\u4e00\u4e2acropped\u51fa\u6765\u7684\u5b50\u56fe\u4ee5\u53ca\u73b0\u6709\u7684\u5e8f\u5217,\u751f\u6210\u6807\u7b7e\u7684\u65b9\u6cd5.","title":"RoadTracer: Automatic Extraction of Road Networks from Aerial Images"},{"location":"other_categories/Summaries/Summary_MapExtraction/#_1","text":"\u7528map-matching\u7b97\u6cd5\u5224\u65ad\u76ee\u524d\u7684\"\u5e8f\u5217\"\u5c5e\u4e8e\u5730\u56fe\u4e0a\u7684\u54ea\u4e00\u6761\u8def\u5f84(\u4e0d\u80fd\u7528\u5f53\u524d\u70b9\u7684\u6700\u8fd1\u8def\u7ebf\u5c31\u786e\u5b9a, \u5426\u5219\u56e0\u4e3a\u566a\u97f3\u4f1a\u504f\u822a). \u8fd9\u4e2a\u95ee\u9898\u5728GPS/\u624b\u673a\u4fe1\u53f7\u5206\u6790\u65f6\u6709\u7528\u5230,\u5982 \u6b64\u6587 . \u8fd9\u7bc7\u6587\u7ae0\u91cc\u9762\u7814\u7a76\u7684\u662f\u624b\u673a\u4fe1\u53f7\u70b9(GPS)\u4f4d\u7f6e\u5e8f\u5217\u4e0e\u5730\u56fe\u8def\u7ebf\u7684\u5339\u914d\u95ee\u9898.\u88ab\u8bbe\u8ba1\u4e3a\u4e00\u4e2a HMM \u6a21\u578b,\u6bcf\u4e00\u4e2a\u65f6\u523b\u89c2\u6d4b\u503c\u662fGPS\u4f4d\u7f6e,\u9690\u53d8\u91cf\u662f\u6240\u5904\u7684\u8def\u7ebfedge. \u72b6\u6001\u8f6c\u79fb\u77e9\u9635\u7684\u7279\u70b9\u5728\u4e8e\u4e24\u6b21\u6d4b\u91cf\u8981\u4e48\u5728\u540c\u4e00\u4e2a\u8def\u7ebfedge\u4e0a,\u8981\u4e48\u5728\u4e00\u4e2a\u76f8\u4e92\u8fde\u63a5\u7684\u8def\u7ebfedge\u4e0a.\u4ece\u800c\u5f62\u6210\u9a6c\u5c14\u79d1\u592b\u94fe. \u8fd9\u91cc\u7684\u4efb\u52a1\u76f8\u5f53\u4e8e\u5df2\u77e5\u5e8f\u5217\u89c2\u6d4b,\u6c42\u89e3\u6700\u5927\u4f3c\u7136\u7684\u9690\u53d8\u91cf\u5e8f\u5217. \u4f7f\u7528\u7684 Viterbi \u7b97\u6cd5. \u672c\u6587\u7684\u6807\u7b7e\u5c31\u662f\u5728\u5df2\u77e5\u8def\u7ebf\u7684\u5206\u914d\u7684\u60c5\u51b5\u4e0b,\u6295\u5f71\u5230\u6700\u8fd1\u7684\u70b9\u4e0a,\u7136\u540e\u5f80\u524d\u4e00\u4e2a\u56fa\u5b9a\u7684\u6b65\u957f\u4f5c\u4e3a\u76ee\u6807.","title":"\u8def\u7ebf\u5206\u914d"},{"location":"other_categories/Summaries/Summary_MapExtraction/#vecroad-point-based-iterative-graph-exploration-for-road-graphs-extraction","text":"pdf code \u8fd9\u7bc7\u8bba\u6587\u5728\u4e0a\u6587\u7684\u57fa\u7840\u4e0a\u63d0\u51fa\u65b0\u7684\u63d0\u5347. \u4f7f\u7528\u9884\u6d4b\u4e0b\u4e00\u4e2a\u70b9\u7684\u65b9\u6cd5,\u66ff\u4ee3\u65b9\u5411.\u4ece\u800c\u5b9e\u73b0\u5728\u8def\u53e3\u4f4d\u7f6e\u7684\u7075\u6d3b\u5730\u63a7\u5236\u6b65\u957f\uff0c\u4ece\u800c\u7f51\u7edc\u53ef\u4ee5\u5728\u4ea4\u70b9\u5904\u5f97\u5230\u6bd4\u8f83\u51c6\u786e\u3002 \u4f7f\u7528\u8bed\u4e49\u5206\u5272+\u4ea4\u70b9\u8bed\u4e49\u5206\u5272\u4f5c\u4e3a\u51b3\u7b56\u5c0f\u7f51\u7edc\u7684\u8f93\u5165\u3002\u8fd9\u4e9b\u5173\u952e\u7684\u6570\u636ecues\u63d0\u5347\u4e86\u7f51\u7edc\u7684\u51c6\u786e\u5ea6\u3002","title":"VecRoad: Point-based Iterative Graph Exploration for Road Graphs Extraction"},{"location":"other_categories/Summaries/Summary_MapExtraction/#icurb-imitation-learning-based-detection-of-road-curbs-using-aerial-images-for-autonomous-driving","text":"pdf code","title":"iCurb: Imitation Learning-based Detection of Road Curbs using Aerial Images for Autonomous Driving"},{"location":"other_categories/Summaries/Summary_NIPS_2020_app/","text":"Summary of NIPS 2020 for application \u8fd9\u4e2a\u9875\u9762\u548c NIPS 2020 for NN \u76f8\u6bd4\uff0c\u770b\u7684\u66f4\u591a\u662f\u7f51\u7edc\u5177\u4f53\u5e94\u7528\u4e0a\u7684\u5b9e\u9a8c\uff0c\u800c\u4e0d\u662f\u7406\u8bba. Displacement-Invariant Matching Cost Learning for Accurate Optical Flow Estimation (DICL) pdf code \u8fd9\u7bc7paper\u63d0\u51fa\u7684\u662f\u4f7f\u7528concat\u7684\u7ed3\u6784\u53bb\u8fdb\u884c\u5149\u6d41\u7684\u4f30\u8ba1. \u8fd9\u4e2a\u65b9\u6cd5\u5728 \u53cc\u76ee \u4e2d\u5f88\u5e38\u89c1\uff0c\u4f46\u662f\u5728\u5149\u6d41\u4e2d\u4e0d\u5e38\u89c1\u3002\u4e3b\u8981\u539f\u56e0\u662f\u5149\u6d41\u4e2d\u9700\u8981\u5728\u4e8c\u7ef4\u5e73\u9762\u5185\u8fdb\u884c\u5927\u89c4\u6a21\u7684\u641c\u7d22\uff0c\u56e0\u800c\u7528concat-based\u7684cost volume\u5185\u5b58\u5360\u7528\u5f88\u5927\uff0c\u4e5f\u4f1a\u5f88\u6162\u3002 \u8fd9\u7bc7paper\u4f7f\u7528\u7684\u662f concat-based\u7684cost volume. \u8fd9\u4e2acost volume\u7684\u8f93\u5165\u4e3a\u4e24\u5f20 (B, C, H, W) \u7684\u56fe\u7247\u7279\u5f81, \u5c06target\u56fe\u591a\u6b21\u5e73\u79fb(x, y, \u6b63\u8d1f)\u5e76\u4e0ereference\u56feconcat \u8f93\u51fa\u7684\u5927\u5c0f\u4e3a [B, 2C, 2U+1, 2V+1, H, W] \u5176\u4e2d U, V \u5206\u522b\u4e3a\u7ad6\u76f4\u4e0e\u6c34\u5e73\u7684\u641c\u7d22\u8303\u56f4. \u7136\u540e\u5c06 (B, 2U+1, 2V+1) \u5168\u90e8merge\u5230\u4e00\u4e2a\u7ef4\u5ea6 B' \u4e0a\uff0c\u5bf9 [B', 2D, H, W] \u4f5c\u5377\u79ef\uff0c\u8f93\u51facost [B', 1, H, W] , \u518d\u590d\u539f\u5230 [B, 2U+1, 2V+1, H, W] . An End to End Network Architecture for Fundamental Matrix Estimation pdf \u7acb\u4f53\u89c6\u89c9\u4e2d\u4e24\u4e2a\u76f8\u673a\u62cd\u5230\u7684\u56fe\u7247\u4e2d\u7684\u5bf9\u5e94\u70b9\u6ee1\u8db3 x'Fx = 0 , \u5176\u4e2d Fx \u4e3aepipolar line. F=K_2^{-T}\\hat tRK_1^{-1} \u5c31\u662f fundamental matrix ( \\hat t \u6307\u4e09\u7ef4\u77e2\u91cf\u53c9\u4e58\u8f6c\u70b9\u4e58). \u4f20\u7edf\u6765\u8bf4\u9700\u8981keypoint detection, matching, eight-point estimation\u8fd9\u6837\u7684\u6d41\u7a0b. \u8fd9\u7bc7paper\u91c7\u53d6\u7684\u662f\u6bd4\u8f83\u66b4\u529b\u8fd0\u7b97\u7684\u65b9\u6848(\u5e76\u975e\u5b8c\u5168\u65b0\uff0c\u4f46\u662f\u6709\u65b0\u610f). contractive learning, \u7528CNN\u5b66\u4e60\uff0c\u8ba9\u4e24\u5f20\u56fe\u5bf9\u5e94keypoint\u8ddd\u79bb\u8fd1\uff0c\u4e0d\u5bf9\u5e94keypoint\u8ddd\u79bb\u8fdc\uff0c\u53c2\u8003 UCN .LAP\u662f Location Aware Pooling ,\u8fd9\u7bc7\u4e5f\u662f\u91cd\u8981\u7684\u524d\u7f6e\u8bba\u6587. \u5bf9\u6bcf\u4e00\u4e2apixel\u7b97\u5168\u8fde\u63a5\uff0c\u8fd9\u8fb9\u79f0\u4e3apointnet\u7ed3\u6784,\u5f97\u5230\u6bcf\u4e2apixel\u6210\u4e3a\u53ef\u9760keypoint\u7684\u6982\u7387, \u7528\u4e8e\u6240\u8c13\u53bb\u9664out-lier \u6700\u540e\u7684\u4f30\u8ba1\u91c7\u7528\u7684\u662f\u5168\u8fde\u63a5\uff0c\u56de\u5f52\u7684\u53c2\u6570\u6709\u516b\u4e2a [f_1, f_2, \\alpha, \\beta] ,F\u7684\u9884\u6d4b\u503c\u662f [f_1; f_2; \\alpha f_1 + \\beta f_2] . \u8bad\u7ec3loss, \u4e00\u4e2a\u662f\u5bf9 F \u7684 l_1, l_2 loss\uff0c\u53e6\u4e00\u4e2a\u662f\u6240\u6709keypoint\u7684\u6b8b\u5dee l_e = \\gamma \\frac{1}{N} \\sum^N_i|m'^T_i\\hat F m_i| Fine-Grained Dynamic Head for Object Detection pdf code Motivation\u5c31\u662f\u60f3\u5728FPN\u7684\u57fa\u7840\u4e0a\u4e0d\u540cScale\u7684\u7279\u5f81\u80fd\u591f\u4e92\u76f8\u878d\u5408: \u7ed3\u6784\u4e0a\u5c31\u662f\u591a\u5c42\u5730\u7ed9\u4f7f\u5f97\u76f8\u90bbscale\u4e4b\u95f4feature\u80fd\u591f\u4e92\u901a\uff0c\u4f5c\u8005\u989d\u5916\u6dfb\u52a0\u4e86 \u4e00\u4e2arouting\u6a21\u5757;\u8bbe\u8ba1\u4e86\u4e00\u4e0b\u5404\u4e2apath;\u6dfb\u52a0\u4e00\u4e2agate. \u56fe\u4e2dgate factor m \u5728\u7406\u60f3\u60c5\u51b5\u4e0b\u5e94\u8be5\u4f7f\u7528\u79bb\u6563\u503c\uff0c\u4e14\u7406\u8bba\u4e0a\u53ef\u4ee5\u7528policy gradient\u7684\u65b9\u6cd5\u4f18\u5316[0, 1]\u8bbe\u7f6e\uff0c\u4f46\u662f\u8fd9\u91cc\u6309\u7167\u6df1\u5ea6\u5b66\u4e60\u7684\u4f20\u7edf\u5c06\u79bb\u6563\u503c\u8fde\u7eed\u5316, \\delta(v)=\\max \\left(0, \\frac{\\tanh (v-\\tau)+\\tanh (\\tau)}{1+\\tanh (\\tau)}\\right) \\in[0,1], \\forall v \\in \\mathbb{R}","title":"Summary of NIPS 2020 for application"},{"location":"other_categories/Summaries/Summary_NIPS_2020_app/#summary-of-nips-2020-for-application","text":"\u8fd9\u4e2a\u9875\u9762\u548c NIPS 2020 for NN \u76f8\u6bd4\uff0c\u770b\u7684\u66f4\u591a\u662f\u7f51\u7edc\u5177\u4f53\u5e94\u7528\u4e0a\u7684\u5b9e\u9a8c\uff0c\u800c\u4e0d\u662f\u7406\u8bba.","title":"Summary of NIPS 2020 for application"},{"location":"other_categories/Summaries/Summary_NIPS_2020_app/#displacement-invariant-matching-cost-learning-for-accurate-optical-flow-estimation-dicl","text":"pdf code \u8fd9\u7bc7paper\u63d0\u51fa\u7684\u662f\u4f7f\u7528concat\u7684\u7ed3\u6784\u53bb\u8fdb\u884c\u5149\u6d41\u7684\u4f30\u8ba1. \u8fd9\u4e2a\u65b9\u6cd5\u5728 \u53cc\u76ee \u4e2d\u5f88\u5e38\u89c1\uff0c\u4f46\u662f\u5728\u5149\u6d41\u4e2d\u4e0d\u5e38\u89c1\u3002\u4e3b\u8981\u539f\u56e0\u662f\u5149\u6d41\u4e2d\u9700\u8981\u5728\u4e8c\u7ef4\u5e73\u9762\u5185\u8fdb\u884c\u5927\u89c4\u6a21\u7684\u641c\u7d22\uff0c\u56e0\u800c\u7528concat-based\u7684cost volume\u5185\u5b58\u5360\u7528\u5f88\u5927\uff0c\u4e5f\u4f1a\u5f88\u6162\u3002 \u8fd9\u7bc7paper\u4f7f\u7528\u7684\u662f concat-based\u7684cost volume. \u8fd9\u4e2acost volume\u7684\u8f93\u5165\u4e3a\u4e24\u5f20 (B, C, H, W) \u7684\u56fe\u7247\u7279\u5f81, \u5c06target\u56fe\u591a\u6b21\u5e73\u79fb(x, y, \u6b63\u8d1f)\u5e76\u4e0ereference\u56feconcat \u8f93\u51fa\u7684\u5927\u5c0f\u4e3a [B, 2C, 2U+1, 2V+1, H, W] \u5176\u4e2d U, V \u5206\u522b\u4e3a\u7ad6\u76f4\u4e0e\u6c34\u5e73\u7684\u641c\u7d22\u8303\u56f4. \u7136\u540e\u5c06 (B, 2U+1, 2V+1) \u5168\u90e8merge\u5230\u4e00\u4e2a\u7ef4\u5ea6 B' \u4e0a\uff0c\u5bf9 [B', 2D, H, W] \u4f5c\u5377\u79ef\uff0c\u8f93\u51facost [B', 1, H, W] , \u518d\u590d\u539f\u5230 [B, 2U+1, 2V+1, H, W] .","title":"Displacement-Invariant Matching Cost Learning for Accurate Optical Flow Estimation (DICL)"},{"location":"other_categories/Summaries/Summary_NIPS_2020_app/#an-end-to-end-network-architecture-for-fundamental-matrix-estimation","text":"pdf \u7acb\u4f53\u89c6\u89c9\u4e2d\u4e24\u4e2a\u76f8\u673a\u62cd\u5230\u7684\u56fe\u7247\u4e2d\u7684\u5bf9\u5e94\u70b9\u6ee1\u8db3 x'Fx = 0 , \u5176\u4e2d Fx \u4e3aepipolar line. F=K_2^{-T}\\hat tRK_1^{-1} \u5c31\u662f fundamental matrix ( \\hat t \u6307\u4e09\u7ef4\u77e2\u91cf\u53c9\u4e58\u8f6c\u70b9\u4e58). \u4f20\u7edf\u6765\u8bf4\u9700\u8981keypoint detection, matching, eight-point estimation\u8fd9\u6837\u7684\u6d41\u7a0b. \u8fd9\u7bc7paper\u91c7\u53d6\u7684\u662f\u6bd4\u8f83\u66b4\u529b\u8fd0\u7b97\u7684\u65b9\u6848(\u5e76\u975e\u5b8c\u5168\u65b0\uff0c\u4f46\u662f\u6709\u65b0\u610f). contractive learning, \u7528CNN\u5b66\u4e60\uff0c\u8ba9\u4e24\u5f20\u56fe\u5bf9\u5e94keypoint\u8ddd\u79bb\u8fd1\uff0c\u4e0d\u5bf9\u5e94keypoint\u8ddd\u79bb\u8fdc\uff0c\u53c2\u8003 UCN .LAP\u662f Location Aware Pooling ,\u8fd9\u7bc7\u4e5f\u662f\u91cd\u8981\u7684\u524d\u7f6e\u8bba\u6587. \u5bf9\u6bcf\u4e00\u4e2apixel\u7b97\u5168\u8fde\u63a5\uff0c\u8fd9\u8fb9\u79f0\u4e3apointnet\u7ed3\u6784,\u5f97\u5230\u6bcf\u4e2apixel\u6210\u4e3a\u53ef\u9760keypoint\u7684\u6982\u7387, \u7528\u4e8e\u6240\u8c13\u53bb\u9664out-lier \u6700\u540e\u7684\u4f30\u8ba1\u91c7\u7528\u7684\u662f\u5168\u8fde\u63a5\uff0c\u56de\u5f52\u7684\u53c2\u6570\u6709\u516b\u4e2a [f_1, f_2, \\alpha, \\beta] ,F\u7684\u9884\u6d4b\u503c\u662f [f_1; f_2; \\alpha f_1 + \\beta f_2] . \u8bad\u7ec3loss, \u4e00\u4e2a\u662f\u5bf9 F \u7684 l_1, l_2 loss\uff0c\u53e6\u4e00\u4e2a\u662f\u6240\u6709keypoint\u7684\u6b8b\u5dee l_e = \\gamma \\frac{1}{N} \\sum^N_i|m'^T_i\\hat F m_i|","title":"An End to End Network Architecture for Fundamental Matrix Estimation"},{"location":"other_categories/Summaries/Summary_NIPS_2020_app/#fine-grained-dynamic-head-for-object-detection","text":"pdf code Motivation\u5c31\u662f\u60f3\u5728FPN\u7684\u57fa\u7840\u4e0a\u4e0d\u540cScale\u7684\u7279\u5f81\u80fd\u591f\u4e92\u76f8\u878d\u5408: \u7ed3\u6784\u4e0a\u5c31\u662f\u591a\u5c42\u5730\u7ed9\u4f7f\u5f97\u76f8\u90bbscale\u4e4b\u95f4feature\u80fd\u591f\u4e92\u901a\uff0c\u4f5c\u8005\u989d\u5916\u6dfb\u52a0\u4e86 \u4e00\u4e2arouting\u6a21\u5757;\u8bbe\u8ba1\u4e86\u4e00\u4e0b\u5404\u4e2apath;\u6dfb\u52a0\u4e00\u4e2agate. \u56fe\u4e2dgate factor m \u5728\u7406\u60f3\u60c5\u51b5\u4e0b\u5e94\u8be5\u4f7f\u7528\u79bb\u6563\u503c\uff0c\u4e14\u7406\u8bba\u4e0a\u53ef\u4ee5\u7528policy gradient\u7684\u65b9\u6cd5\u4f18\u5316[0, 1]\u8bbe\u7f6e\uff0c\u4f46\u662f\u8fd9\u91cc\u6309\u7167\u6df1\u5ea6\u5b66\u4e60\u7684\u4f20\u7edf\u5c06\u79bb\u6563\u503c\u8fde\u7eed\u5316, \\delta(v)=\\max \\left(0, \\frac{\\tanh (v-\\tau)+\\tanh (\\tau)}{1+\\tanh (\\tau)}\\right) \\in[0,1], \\forall v \\in \\mathbb{R}","title":"Fine-Grained Dynamic Head for Object Detection"},{"location":"other_categories/Summaries/Summary_NIPS_2021/","text":"Summaries for several NeurIPS 2021 papers Do Vision Transformers See Like Convolutional Neural Networks? pdf \u8fd9\u7bc7paper,\u5c1d\u8bd5\u4eceViT\u4ee5\u53caCNN\u7684\u4e2d\u95f4\u7279\u5f81\u8fdb\u884c\u5206\u6790\uff0c\u5c1d\u8bd5\u7406\u89e3ViT\u5b66\u4e60\u5230\u7684\u7279\u5f81\uff0c\u5b66\u4e60\u7684\u65b9\u6cd5\u4e0eCNN\u76f8\u6bd4\u6709\u4ec0\u4e48\u4e0d\u540c\u3002 \u7ed3\u8bba\u6765\u8bf4: ViT\u7684\u6d45\u5c42\u4e0e\u6df1\u5c42\u4e4b\u95f4\u7684\"\u76f8\u4f3c\u6027\u66f4\u5927\"\uff0c\u4e00\u5927\u539f\u56e0\u662f\u8d2f\u7a7f\u6574\u4e2aViT\u7684\u6b8b\u5dee\u8fde\u63a5. ViT\u5728\u6d45\u5c42\u7684\u65f6\u5019\u5c31\u6709ResNet\u6df1\u5c42\u7684\u7279\u6027\uff0c\u4e14\u66f4\u65e9\u83b7\u53d6\u5168\u5c40\u7279\u5f81\u3002 \u4f4e\u5c42\u7f51\u7edc\u4e2d\u7684\u5c40\u90e8\u4fe1\u606f\u4f9d\u7136\u975e\u5e38\u91cd\u8981\uff0cViT\u9700\u8981\u5927\u91cf\u7684\u8bad\u7ec3\u6570\u636e\u6765\u5e2e\u52a9\u7f51\u7edc\u5b66\u4e60\u8fd9\u4e2a\uff0c\u8fd9\u4e5f\u662f\u5b83\u5bf9\u6570\u636e\u7684\u8981\u6c42\u66f4\u9ad8\u7684\u4e00\u4e2a\u8868\u73b0. \u6211\u4eec\u53d1\u73b0ViT\u540c\u6837\u80fd\u5f88\u597d\u7684\u4fdd\u5b58\u5c40\u90e8\u4f4d\u7f6e\u4fe1\u606f. \u65b9\u6cd5\u4e0a\u6765\u8bf4\uff0c\u672c\u6587\u91c7\u7528 Centred Kernel Alignment (CKA)\u6765\u5206\u6790\u795e\u7ecf\u7f51\u7edc\u7684\u7279\u5f81\u3002 code : \u5b9a\u4e49\u7531 m \u4e2a\u6570\u636e\u4ea7\u751f\u7684\u4e24\u5c42\u7f51\u7edc\u7684\u7279\u5f81 X\\in \\mathbb{R}^{m\\times p_1} Y\\in \\mathbb{R}^{m\\times p_2} . \u5b9a\u4e49\u4e24\u7ec4\u6570\u636e\u7684\u683c\u62c9\u59c6(Gram)\u77e9\u9635 K = XX^T, L = YY^T . \\operatorname{CKA}(\\boldsymbol{K}, \\boldsymbol{L})=\\frac{\\operatorname{HSIC}(\\boldsymbol{K}, \\boldsymbol{L})}{\\sqrt{\\operatorname{HSIC}(\\boldsymbol{K}, \\boldsymbol{K}) \\operatorname{HSIC}(\\boldsymbol{L}, \\boldsymbol{L})}} \u5176\u4e2d: HSIC \u662f Hilbert-Schmidt independence criterion H = I_n - \\frac{1}{n} \\bold{1} \\bold{1}^T, K' = HKH, L'=HLH , \\text{HSIC}(K,L) = \\text{vec}(K') \\dot \\text{vec}(L') / (m - 1)^2 . You Only Look One Sequence pdf code \u8fd9\u7bc7\u8bba\u6587\u7684\u6709\u51e0\u4e2a\u4e3b\u8981\u7684\u8bba\u70b9: Transformer\u5e94\u8be5\u91cd\u89c6\u9884\u8bad\u7ec3\u6548\u679c\u3002\u8fd9\u7bc7paper\u7684\u7f51\u7edc\u7ed3\u6784\u5c31\u5f88\u4f9d\u8d56\u9884\u8bad\u7ec3\uff0c\u4e14\u4e0eCNN\u4e0d\u540c, from scratch training\u901a\u8fc7\u589e\u957f\u8bad\u7ec3\u65f6\u95f4\u4e5f\u96be\u4ee5\u83b7\u5f97fine-tuning\u7684\u6c34\u5e73. \u7ed3\u6784\u4e0a\u672c\u6587\u63a5\u8fd1\u4e8e\u65e0CNN\u57fa\u7840\u7684DETR, \u4f5c\u8005\u8ba4\u4e3a\u9700\u8981\u5c06\u5e8f\u5217\u91cd\u6574\u56de2D\u7684\u90fd\u4e0d\u7b26\u5408Transformer\u53bb\u9664inductive bias\u7684\u7cbe\u795e\uff0c\u56e0\u800c\u4e0d\u63a8\u8350\u3002 \u4e3a\u4e86\u68c0\u6d4b\u8f93\u51fa\uff0c\u53bb\u9664\u4e86CLS Token,\u800c\u52a0\u5165\u91cd\u65b0\u968f\u673a\u521d\u59cb\u5316\u7684DET Token. Per-Pixel Classification is Not All You Need for Semantic Segmentation pdf code \u77e5\u4e4e \u8fd9\u7bc7paper\u4e3b\u8981\u7684\u65b0\u601d\u60f3\u5728\u4e8e\u4f7f\u7528instance segmentation\u7684mask prediction\u601d\u8def\u5c1d\u8bd5\u89e3\u51b3\u4e86\u8bed\u4e49\u5206\u5272\u7684\u95ee\u9898\u3002\u4f7f\u5f97\u8bed\u4e49\u5206\u5272\u4e0d\u4ec5\u4ec5\u662f\u4e00\u4e2a\u5bf9\u6bcf\u4e00\u4e2a\u50cf\u7d20\u8fdb\u884c\u5206\u7c7b\u7684\u65b9\u6cd5. \u8f93\u51fa\u7ed3\u6784\u4ece K \u4e2a\u5bc6\u96c6\u7684\u5206\u7c7b\u70ed\u56fe( K \u4e0e\u5206\u7c7b\u7c7b\u522b\u6570\u4e00\u81f4)\u3002 \u53d8\u4e3a N \u4e2a\u4e8c\u5206\u7c7b\u7684mask\u70ed\u56fe\uff0c \u4e0e N \u4e2a K+1 \u7ef4\u7684mask\u5206\u7c7b\u3002 N \u4e0e K \u53ef\u4ee5\u4e0d\u540c. \u53c8\u63d0\u51fa\u4e86mask-former\u7684\u7ed3\u6784\u3002 \u6709\u4eba\u6307\u51fa\u8fd9\u7bc7\u8bba\u6587\u4e0e max-deeplab \u5f88\u50cf,\u4e8b\u5b9e\u5982\u6b64\uff0cmax-deeplab\u7528\u7c7b\u4f3c\u7684\u65b9\u6cd5\u76f4\u63a5\u5b8c\u6210\u4e86\u5168\u666f\u5206\u5272\uff0c\u800c\u672c\u6587\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u53ef\u4ee5\u7406\u89e3\u4e3amax-deeplab\u7684\u5b50\u95ee\u9898\u3002","title":"Summaries for several NeurIPS 2021 papers"},{"location":"other_categories/Summaries/Summary_NIPS_2021/#summaries-for-several-neurips-2021-papers","text":"","title":"Summaries for several NeurIPS 2021 papers"},{"location":"other_categories/Summaries/Summary_NIPS_2021/#do-vision-transformers-see-like-convolutional-neural-networks","text":"pdf \u8fd9\u7bc7paper,\u5c1d\u8bd5\u4eceViT\u4ee5\u53caCNN\u7684\u4e2d\u95f4\u7279\u5f81\u8fdb\u884c\u5206\u6790\uff0c\u5c1d\u8bd5\u7406\u89e3ViT\u5b66\u4e60\u5230\u7684\u7279\u5f81\uff0c\u5b66\u4e60\u7684\u65b9\u6cd5\u4e0eCNN\u76f8\u6bd4\u6709\u4ec0\u4e48\u4e0d\u540c\u3002 \u7ed3\u8bba\u6765\u8bf4: ViT\u7684\u6d45\u5c42\u4e0e\u6df1\u5c42\u4e4b\u95f4\u7684\"\u76f8\u4f3c\u6027\u66f4\u5927\"\uff0c\u4e00\u5927\u539f\u56e0\u662f\u8d2f\u7a7f\u6574\u4e2aViT\u7684\u6b8b\u5dee\u8fde\u63a5. ViT\u5728\u6d45\u5c42\u7684\u65f6\u5019\u5c31\u6709ResNet\u6df1\u5c42\u7684\u7279\u6027\uff0c\u4e14\u66f4\u65e9\u83b7\u53d6\u5168\u5c40\u7279\u5f81\u3002 \u4f4e\u5c42\u7f51\u7edc\u4e2d\u7684\u5c40\u90e8\u4fe1\u606f\u4f9d\u7136\u975e\u5e38\u91cd\u8981\uff0cViT\u9700\u8981\u5927\u91cf\u7684\u8bad\u7ec3\u6570\u636e\u6765\u5e2e\u52a9\u7f51\u7edc\u5b66\u4e60\u8fd9\u4e2a\uff0c\u8fd9\u4e5f\u662f\u5b83\u5bf9\u6570\u636e\u7684\u8981\u6c42\u66f4\u9ad8\u7684\u4e00\u4e2a\u8868\u73b0. \u6211\u4eec\u53d1\u73b0ViT\u540c\u6837\u80fd\u5f88\u597d\u7684\u4fdd\u5b58\u5c40\u90e8\u4f4d\u7f6e\u4fe1\u606f. \u65b9\u6cd5\u4e0a\u6765\u8bf4\uff0c\u672c\u6587\u91c7\u7528 Centred Kernel Alignment (CKA)\u6765\u5206\u6790\u795e\u7ecf\u7f51\u7edc\u7684\u7279\u5f81\u3002 code : \u5b9a\u4e49\u7531 m \u4e2a\u6570\u636e\u4ea7\u751f\u7684\u4e24\u5c42\u7f51\u7edc\u7684\u7279\u5f81 X\\in \\mathbb{R}^{m\\times p_1} Y\\in \\mathbb{R}^{m\\times p_2} . \u5b9a\u4e49\u4e24\u7ec4\u6570\u636e\u7684\u683c\u62c9\u59c6(Gram)\u77e9\u9635 K = XX^T, L = YY^T . \\operatorname{CKA}(\\boldsymbol{K}, \\boldsymbol{L})=\\frac{\\operatorname{HSIC}(\\boldsymbol{K}, \\boldsymbol{L})}{\\sqrt{\\operatorname{HSIC}(\\boldsymbol{K}, \\boldsymbol{K}) \\operatorname{HSIC}(\\boldsymbol{L}, \\boldsymbol{L})}} \u5176\u4e2d: HSIC \u662f Hilbert-Schmidt independence criterion H = I_n - \\frac{1}{n} \\bold{1} \\bold{1}^T, K' = HKH, L'=HLH , \\text{HSIC}(K,L) = \\text{vec}(K') \\dot \\text{vec}(L') / (m - 1)^2 .","title":"Do Vision Transformers See Like Convolutional Neural Networks?"},{"location":"other_categories/Summaries/Summary_NIPS_2021/#you-only-look-one-sequence","text":"pdf code \u8fd9\u7bc7\u8bba\u6587\u7684\u6709\u51e0\u4e2a\u4e3b\u8981\u7684\u8bba\u70b9: Transformer\u5e94\u8be5\u91cd\u89c6\u9884\u8bad\u7ec3\u6548\u679c\u3002\u8fd9\u7bc7paper\u7684\u7f51\u7edc\u7ed3\u6784\u5c31\u5f88\u4f9d\u8d56\u9884\u8bad\u7ec3\uff0c\u4e14\u4e0eCNN\u4e0d\u540c, from scratch training\u901a\u8fc7\u589e\u957f\u8bad\u7ec3\u65f6\u95f4\u4e5f\u96be\u4ee5\u83b7\u5f97fine-tuning\u7684\u6c34\u5e73. \u7ed3\u6784\u4e0a\u672c\u6587\u63a5\u8fd1\u4e8e\u65e0CNN\u57fa\u7840\u7684DETR, \u4f5c\u8005\u8ba4\u4e3a\u9700\u8981\u5c06\u5e8f\u5217\u91cd\u6574\u56de2D\u7684\u90fd\u4e0d\u7b26\u5408Transformer\u53bb\u9664inductive bias\u7684\u7cbe\u795e\uff0c\u56e0\u800c\u4e0d\u63a8\u8350\u3002 \u4e3a\u4e86\u68c0\u6d4b\u8f93\u51fa\uff0c\u53bb\u9664\u4e86CLS Token,\u800c\u52a0\u5165\u91cd\u65b0\u968f\u673a\u521d\u59cb\u5316\u7684DET Token.","title":"You Only Look One Sequence"},{"location":"other_categories/Summaries/Summary_NIPS_2021/#per-pixel-classification-is-not-all-you-need-for-semantic-segmentation","text":"pdf code \u77e5\u4e4e \u8fd9\u7bc7paper\u4e3b\u8981\u7684\u65b0\u601d\u60f3\u5728\u4e8e\u4f7f\u7528instance segmentation\u7684mask prediction\u601d\u8def\u5c1d\u8bd5\u89e3\u51b3\u4e86\u8bed\u4e49\u5206\u5272\u7684\u95ee\u9898\u3002\u4f7f\u5f97\u8bed\u4e49\u5206\u5272\u4e0d\u4ec5\u4ec5\u662f\u4e00\u4e2a\u5bf9\u6bcf\u4e00\u4e2a\u50cf\u7d20\u8fdb\u884c\u5206\u7c7b\u7684\u65b9\u6cd5. \u8f93\u51fa\u7ed3\u6784\u4ece K \u4e2a\u5bc6\u96c6\u7684\u5206\u7c7b\u70ed\u56fe( K \u4e0e\u5206\u7c7b\u7c7b\u522b\u6570\u4e00\u81f4)\u3002 \u53d8\u4e3a N \u4e2a\u4e8c\u5206\u7c7b\u7684mask\u70ed\u56fe\uff0c \u4e0e N \u4e2a K+1 \u7ef4\u7684mask\u5206\u7c7b\u3002 N \u4e0e K \u53ef\u4ee5\u4e0d\u540c. \u53c8\u63d0\u51fa\u4e86mask-former\u7684\u7ed3\u6784\u3002 \u6709\u4eba\u6307\u51fa\u8fd9\u7bc7\u8bba\u6587\u4e0e max-deeplab \u5f88\u50cf,\u4e8b\u5b9e\u5982\u6b64\uff0cmax-deeplab\u7528\u7c7b\u4f3c\u7684\u65b9\u6cd5\u76f4\u63a5\u5b8c\u6210\u4e86\u5168\u666f\u5206\u5272\uff0c\u800c\u672c\u6587\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u53ef\u4ee5\u7406\u89e3\u4e3amax-deeplab\u7684\u5b50\u95ee\u9898\u3002","title":"Per-Pixel Classification is Not All You Need for Semantic Segmentation"},{"location":"other_categories/Summaries/Summary_of_ICRA_2020/","text":"Summaries for several ICRA 2020 papers \u672c\u5c4aICRA\u6709\u6570\u7bc7paper\u5728\u4e4b\u524d\u5df2\u7ecf\u6709review, \"A General Framework for Uncertainty Estimation in Deep Learning\" , \"FADNet: A Fast and Accurate Network for Disparity Estimation\" \"Object-Centric Stereo Matching for 3D Object Detection\" \u76ee\u5f55: Summaries for several ICRA 2020 papers Event-Based Angular Velocity Regression with Spiking Networks Pedestrian Planar LiDAR Pose (PPLP) Network for Oriented Pedestrian Detection Based on Planar LiDAR and Monocular Images CNN Based Road User Detection Using the 3D Radar Cube PST900: RGB-Thermal Calibration, Dataset and Segmentation Network Instance Segmentation of LiDAR Point Clouds SegVoxelNet: Exploring Semantic Context and Depth-aware Features for 3D Vehicle Detection from Point Cloud Radar as a Teacher: Weakly Supervised Vehicle Detection using Radar Labels Self-supervised linear motion deblurring Fast Panoptic Segmentation Network Real-Time Semantic Stereo Matching MultiDepth: Single-Image Depth Estimation via Multi-Task Regression and Classification MPC-Net: A First Principles Guided Policy Search MapLite: Autonomous Intersection Navigation Without a Detailed Prior Map \u8fd9\u91cc\u7ee7\u7eed\u641c\u96c6\u591a\u7bc7\u6709\u8da3\u7684ICRA 2020 papers. Event-Based Angular Velocity Regression with Spiking Networks pdf code \u8fd9\u7bc7paper\u5229\u7528\u4e862018NeurIPS\u7684\u4e00\u7bc7\u5173\u4e8e spiking neural network \u7684\u6587\u7ae0\uff0c\u8fd9\u7bc7\u6587\u7ae0\u63d0\u51fa\u4e86SNN\u7684\u4e00\u4e2a\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5e76\u4e14\u4ecb\u7ecd\u4e86\u76f8\u5173\u7684\u6982\u5ff5\uff0c\u540c\u65f6\u7ed9\u51fa\u4e86 pytorch\u5e93/cuda \u4ee3\u7801\u7528\u4e8e\u52a0\u901f\u8fd0\u7b97. \u672c\u6587\u5229\u7528\u4e86NIPS paper\u7684\u8fd9\u4e2a\u5e93\uff0c\u8f93\u5165\u4e3a\u5e8f\u5217\u7684image-like event sequence,\u8f93\u51fa\u4e3a\u5e8f\u5217\u7684\u4e09\u8f74\u89d2\u901f\u5ea6\uff0c \u4eff\u771f\u6570\u636e\u6765\u81ea\u4e8e esim\u4eff\u771f\u5668 Pedestrian Planar LiDAR Pose (PPLP) Network for Oriented Pedestrian Detection Based on Planar LiDAR and Monocular Images pdf code CNN Based Road User Detection Using the 3D Radar Cube pdf code \u8fd9\u7bc7paper\u8c03\u7528\u5e95\u5c42\u7684radar\u6570\u636e\uff0c\u540c\u65f6\u4f7f\u7528\u5e95\u5c42\u7684radar cube\u6570\u636e\u4ee5\u53caradar target\u6570\u636e\uff0c\u5728\u7eafradar\u7684\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4e86 3D object detection. PST900: RGB-Thermal Calibration, Dataset and Segmentation Network pdf code \u672c\u6587\u63d0\u51fa\u7684\u4e3b\u8981\u8d21\u732e\u662f RGB-Thermal\u7684\u6821\u6b63(\u5229\u7528\u4e00\u4e2a\u53cc\u76eeRGB\u76f8\u673a\u5f97\u5230\u6df1\u5ea6\u4f30\u8ba1\uff0c\u518d\u56de\u6295\u5230Thermal\u4e0a)\u4ee5\u53ca\u4e00\u4e2a\u8bed\u4e49\u5206\u5272\u6570\u636e\u96c6\uff0c Instance Segmentation of LiDAR Point Clouds pdf code SegVoxelNet: Exploring Semantic Context and Depth-aware Features for 3D Vehicle Detection from Point Cloud pdf \u8fd9\u7bc7paper\u6765\u81ea\u4e8e D4LCN \u7684\u7ec4\u3002 \u8bed\u4e49\u5206\u5272BEV Ground Truth\u6765\u81ea\u4e8ebbox\u76f4\u63a5\u7684\u6295\u5f71\u3002Depth Aware\u7684\u7406\u89e3\u662f\u8fd1\u5904\u3001\u8fdc\u5904\u7684\u70b9\u4e91\u5206\u5e03\u5bc6\u5ea6\u5dee\u8ddd\u8f83\u5927\uff0c\u5c06BEV\u6cbf\u7740\u6df1\u5ea6\u8f74\u5206\u6210\u5e26\u6709\u91cd\u53e0\u90e8\u5206\u7684\u51e0\u4e2a\u90e8\u5206\uff0c\u6267\u884c\u4e0d\u540c\u7684\u5377\u79ef\u64cd\u4f5c\u3002\u5728KITTI\u4e0a\u7684\u6027\u80fd\u4e0ePointPillars\u548cPointRCNN\u76f8\u8fd1\u3002 Radar as a Teacher: Weakly Supervised Vehicle Detection using Radar Labels pdf \u8fd9\u7bc7paper\u5efa\u8bae\u53c2\u8003\u6b64\u524dNIPS\u7684 co-teaching\u7684paper Self-supervised linear motion deblurring pdf code \u8fd9\u7bc7paper\u51fa\u81eaKITTI\u6570\u636e\u96c6\u7684\u5b9e\u9a8c\u5ba4\u3002\u8fd9\u7bc7paper\u7684\u4e3b\u8981idea\u662f\u4f7f\u7528\u4e00\u4e2areblur module\uff0c\u5728\u7ebf\u6027\u8fd0\u52a8\u7684\u5047\u8bbe\u4e0b\uff0c\u5229\u7528\u5149\u6d41\u4e0eblurring\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5c06\u4e00\u4e2adeblurred\u7684module\u91cd\u65b0\u53d8\u4e3ablurred\uff0c\u8fd9\u6837\u5c31\u53ef\u4ee5\u5f62\u6210\u4e00\u4e2a\u81ea\u76d1\u7763\u7684\u4f53\u7cfb\u3002 \u672c\u6587\u4f7f\u7528\u73b0\u6210\u7684deblur\u4ee5\u53ca\u5149\u6d41\u7f51\u7edc\u7ed3\u6784\uff0c\u5229\u7528\u524d\u540e\u5e27\u7684consistence\u8bad\u7ec3\u5149\u6d41\uff0c\u540c\u65f6\u5bf9deblur\u7ed3\u679c\u63d0\u51fa\u9690\u6027\u7684\u8981\u6c42\u3002\u524d\u9762\u63d0\u5230\u7684\u81ea\u76d1\u7763\u7f51\u7edcloss\u53ef\u4ee5\u8bad\u7ec3deblur\u7f51\u7edc\uff0c\u540c\u65f6\u5bf9\u5149\u6d41\u7684\u8ba1\u7b97\u63d0\u51fa\u9690\u6027\u7684\u8981\u6c42\uff0c\u672c\u6587\u7684reblur\u662f\u4e00\u4e2a\u975e\u5b66\u4e60\u53ef\u5fae\u5206\u6a21\u5757\uff0c\u56e0\u800c\u6574\u4e2a\u7f51\u7edc\u53ef\u5fae\u5206\uff0c\u53ef\u4ee5\u7aef\u5230\u7aef\u81ea\u76d1\u7763\u5b66\u4e60\u3002 reblur\u6a21\u5757\u65b9\u7a0b: \\mathbf{B}(\\mathbf{x}) \\approx \\frac{1}{2 N+1} \\sum_{i=-N}^{N}\\left(\\mathcal{W}_{0 \\rightarrow i} \\circ \\mathbf{I}_{0}\\right)(\\mathbf{x}) \u5176\u4e2d \\mathcal{W} \u6307\u7684\u662f\u5c06\u539f\u56fe\u6839\u636e\u5149\u6d41\u8fdb\u884cwarping, Fast Panoptic Segmentation Network pdf Real-Time Semantic Stereo Matching pdf \u5728\u8bed\u4e49\u5206\u5272\u548c\u53cc\u76ee\u4e0a\u90fd\u6709\u4e0d\u9519\u7684\u70b9\u6570(\u4e00\u822c\u822c)\uff0c\u4e3b\u8981\u662f\u901f\u5ea6\u6bd4\u8f83\u5feb MultiDepth: Single-Image Depth Estimation via Multi-Task Regression and Classification pdf \u6df1\u5ea6\u9884\u6d4b\u95ee\u9898\u540c\u65f6\u8d70\u5206\u7c7b\u4e0e\u56de\u5f52\u3002 MPC-Net: A First Principles Guided Policy Search pdf code \u8fd9\u7bc7paper\u4f7f\u7528\u7c7b\u6a21\u4eff\u5b66\u4e60\uff0c\u5f97\u5230\u4e00\u4e2a\u5feb\u901f\u7684MPC approximator.\u672c\u6587\u5229\u7528\u6700\u4f18\u63a7\u5236\u89e3\u7684\u5fc5\u8981\u6761\u4ef6\uff0cHJB\u65b9\u7a0b\uff0c\u8981\u6c42\u4e00\u4e0b\u54c8\u5bc6\u987f\u91cf\u6700\u5c0f\u5316, \\begin{aligned} \\boldsymbol{u}^{*}(t, \\boldsymbol{x}) &=\\arg \\min _{\\boldsymbol{u}} \\mathcal{H}(\\boldsymbol{x}, \\boldsymbol{u}, t) \\\\ \\mathcal{H}(\\boldsymbol{x}, \\boldsymbol{u}, t) &:=\\mathcal{L}(\\boldsymbol{x}, \\boldsymbol{u}, t)+\\partial_{\\boldsymbol{x}} V(t, \\boldsymbol{x}) \\boldsymbol{f}(\\boldsymbol{x}, \\boldsymbol{u}, t) \\end{aligned} \u672c\u6587\u63d0\u51fa\u4f7f\u7528SLQ(Sequential-Linear-Quadritic)\u6700\u4f18\u5316\u65b9\u6cd5\u8fdb\u884c\u6700\u4f18\u5316\u6c42\u89e3(\u8fd9\u4e2a\u7b97\u6cd5\u7c7b\u4f3c\u4e8e\u8fde\u7eed\u65f6\u95f4\u7684\u8fed\u4ee3LQR),\u901a\u8fc7\u8fd9\u4e2a\u6c42\u89e3\u5668\u53ef\u4ee5\u5f97\u5230MPC teacher\u7684\u63a7\u5236\u547d\u4ee4\uff0c\u4ee5\u53cavalue function\u5173\u4e8ex\u7684\u6c42\u5bfc \\partial_{\\boldsymbol{x}} V(t, \\boldsymbol{x}) \u5b66\u4e60\u65b9\u6cd5: 1. \u5728\u6bcf\u4e00\u6b21\u8fed\u4ee3\u4e2d\uff0c\u4f7f\u7528MPC\u4ee5\u53ca\u7f51\u7edc\u7684\u878d\u5408\u4fe1\u53f7\uff0c\u5bf9\u7cfb\u7edf\u5b8c\u6210\u4e00\u4e2a\u5e8f\u5217\u7684\u4eff\u771f\uff0c\u878d\u5408\u6bd4\u91cd\u968f\u7740\u8bad\u7ec3\u7684\u8fdb\u884c\u9010\u6e10\u7531\u7f51\u7edc\u8f93\u51fa\u4e3b\u5bfc\uff0c\u5728buffer\u4e2d\u5b58\u4e0b\u65f6\u95f4\u6233\uff0cstate\uff0cvalue\u51fd\u6570\u5bf9\u4e8estate\u7684\u5bfc\u6570\u77e2\u91cf,\u7f51\u7edc\u6700\u4f18\u8f93\u51fa\u503c\u7b49\u7ed3\u679c\u3002 2. \u4ecebuffer\u4e2d\u8fdb\u884c\u91c7\u6837\uff0c\u4f7f\u7528\u5b58\u50a8\u4e2d\u7684state, dvdx, t\u4ee5\u53ca\u7f51\u7edc\u8f93\u51fa\u7684u\uff0c\u8ba1\u7b97\u54c8\u5bc6\u987f\u91cf\u3002\u5bf9\u4e8e\u672c\u6587\u5b9e\u9a8c\u4e2d\u7ed9\u51fa\u4e8c\u6b21\u635f\u5931\u51fd\u6570\uff0c \\mathcal{L} \u4e0e u \u76f8\u5173\u7684\u53ea\u6709regularization\u9879 uRu^T ,dvdx\u4e3a\u5e38\u77e2\u91cf\uff0c f(x, u, t) \u4e3a\u6a21\u578b\u7684 \\dot x ,\u4e0e\u7cfb\u7edf\u6a21\u578b\u6709\u5173\uff0c\u4e5f\u5c31\u4f1a\u4e0e u \u6709\u5173\u3002 3. \u628a\u54c8\u5bc6\u987f\u91cf\u7406\u89e3\u4e3a\u635f\u5931\u51fd\u6570\uff0c\u4f7f\u7528\u68af\u5ea6\u4e0b\u964d\u4f18\u5316\u7f51\u7edc\u53c2\u6570\uff0c MapLite: Autonomous Intersection Navigation Without a Detailed Prior Map pdf \u8fd9\u7bc7paper\u63d0\u51fa\u4e86\u4e00\u4e2a\u7cfb\u7edf\uff0c\u4ec5\u5229\u7528\u62d3\u6251\u5730\u56fe\u5b8c\u6210\u65e0\u4eba\u8f66\u7684\u5b9a\u4f4d\u4e0e\u5bfc\u822a\u4efb\u52a1\u3002\u70b9\u4e91\u7684\u8def\u9762\u5206\u5272\u4f7f\u7528\u7684\u505a\u6cd5\u662f\u5bf9\u6bcf\u4e00\u4e2a\u70b9\u63d0\u53d6\u8bbe\u5b9a\u7684\u4e94\u4e2afeature\uff0c\u4f7f\u7528linear SVM\u5224\u65ad\u5b83\u662f\u5426\u5728\u5730\u9762\u4e0a\u3002\u5173\u952e\u5bf9\u4e8e\u62d3\u6251\u5730\u56fe\u7684\u5b9a\u4f4d\uff0c\u672c\u6587\u4f7f\u7528\u4f20\u7edf\u7684\u6982\u7387\u6ee4\u6ce2\u7b97\u6cd5,\u6709\u4e00\u5b9a\u5c40\u9650\u6027\uff0c\u4f46\u662f\u5728\u4e00\u5b9a\u8303\u56f4\u5185\u663e\u7136\u662f\u6709\u6548\u7684\u3002","title":"Summaries for several ICRA 2020 papers"},{"location":"other_categories/Summaries/Summary_of_ICRA_2020/#summaries-for-several-icra-2020-papers","text":"\u672c\u5c4aICRA\u6709\u6570\u7bc7paper\u5728\u4e4b\u524d\u5df2\u7ecf\u6709review, \"A General Framework for Uncertainty Estimation in Deep Learning\" , \"FADNet: A Fast and Accurate Network for Disparity Estimation\" \"Object-Centric Stereo Matching for 3D Object Detection\" \u76ee\u5f55: Summaries for several ICRA 2020 papers Event-Based Angular Velocity Regression with Spiking Networks Pedestrian Planar LiDAR Pose (PPLP) Network for Oriented Pedestrian Detection Based on Planar LiDAR and Monocular Images CNN Based Road User Detection Using the 3D Radar Cube PST900: RGB-Thermal Calibration, Dataset and Segmentation Network Instance Segmentation of LiDAR Point Clouds SegVoxelNet: Exploring Semantic Context and Depth-aware Features for 3D Vehicle Detection from Point Cloud Radar as a Teacher: Weakly Supervised Vehicle Detection using Radar Labels Self-supervised linear motion deblurring Fast Panoptic Segmentation Network Real-Time Semantic Stereo Matching MultiDepth: Single-Image Depth Estimation via Multi-Task Regression and Classification MPC-Net: A First Principles Guided Policy Search MapLite: Autonomous Intersection Navigation Without a Detailed Prior Map \u8fd9\u91cc\u7ee7\u7eed\u641c\u96c6\u591a\u7bc7\u6709\u8da3\u7684ICRA 2020 papers.","title":"Summaries for several ICRA 2020 papers"},{"location":"other_categories/Summaries/Summary_of_ICRA_2020/#event-based-angular-velocity-regression-with-spiking-networks","text":"pdf code \u8fd9\u7bc7paper\u5229\u7528\u4e862018NeurIPS\u7684\u4e00\u7bc7\u5173\u4e8e spiking neural network \u7684\u6587\u7ae0\uff0c\u8fd9\u7bc7\u6587\u7ae0\u63d0\u51fa\u4e86SNN\u7684\u4e00\u4e2a\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5e76\u4e14\u4ecb\u7ecd\u4e86\u76f8\u5173\u7684\u6982\u5ff5\uff0c\u540c\u65f6\u7ed9\u51fa\u4e86 pytorch\u5e93/cuda \u4ee3\u7801\u7528\u4e8e\u52a0\u901f\u8fd0\u7b97. \u672c\u6587\u5229\u7528\u4e86NIPS paper\u7684\u8fd9\u4e2a\u5e93\uff0c\u8f93\u5165\u4e3a\u5e8f\u5217\u7684image-like event sequence,\u8f93\u51fa\u4e3a\u5e8f\u5217\u7684\u4e09\u8f74\u89d2\u901f\u5ea6\uff0c \u4eff\u771f\u6570\u636e\u6765\u81ea\u4e8e esim\u4eff\u771f\u5668","title":"Event-Based Angular Velocity Regression with Spiking Networks"},{"location":"other_categories/Summaries/Summary_of_ICRA_2020/#pedestrian-planar-lidar-pose-pplp-network-for-oriented-pedestrian-detection-based-on-planar-lidar-and-monocular-images","text":"pdf code","title":"Pedestrian Planar LiDAR Pose (PPLP) Network for Oriented Pedestrian Detection Based on Planar LiDAR and Monocular Images"},{"location":"other_categories/Summaries/Summary_of_ICRA_2020/#cnn-based-road-user-detection-using-the-3d-radar-cube","text":"pdf code \u8fd9\u7bc7paper\u8c03\u7528\u5e95\u5c42\u7684radar\u6570\u636e\uff0c\u540c\u65f6\u4f7f\u7528\u5e95\u5c42\u7684radar cube\u6570\u636e\u4ee5\u53caradar target\u6570\u636e\uff0c\u5728\u7eafradar\u7684\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4e86 3D object detection.","title":"CNN Based Road User Detection Using the 3D Radar Cube"},{"location":"other_categories/Summaries/Summary_of_ICRA_2020/#pst900-rgb-thermal-calibration-dataset-and-segmentation-network","text":"pdf code \u672c\u6587\u63d0\u51fa\u7684\u4e3b\u8981\u8d21\u732e\u662f RGB-Thermal\u7684\u6821\u6b63(\u5229\u7528\u4e00\u4e2a\u53cc\u76eeRGB\u76f8\u673a\u5f97\u5230\u6df1\u5ea6\u4f30\u8ba1\uff0c\u518d\u56de\u6295\u5230Thermal\u4e0a)\u4ee5\u53ca\u4e00\u4e2a\u8bed\u4e49\u5206\u5272\u6570\u636e\u96c6\uff0c","title":"PST900: RGB-Thermal Calibration, Dataset and Segmentation Network"},{"location":"other_categories/Summaries/Summary_of_ICRA_2020/#instance-segmentation-of-lidar-point-clouds","text":"pdf code","title":"Instance Segmentation of LiDAR Point Clouds"},{"location":"other_categories/Summaries/Summary_of_ICRA_2020/#segvoxelnet-exploring-semantic-context-and-depth-aware-features-for-3d-vehicle-detection-from-point-cloud","text":"pdf \u8fd9\u7bc7paper\u6765\u81ea\u4e8e D4LCN \u7684\u7ec4\u3002 \u8bed\u4e49\u5206\u5272BEV Ground Truth\u6765\u81ea\u4e8ebbox\u76f4\u63a5\u7684\u6295\u5f71\u3002Depth Aware\u7684\u7406\u89e3\u662f\u8fd1\u5904\u3001\u8fdc\u5904\u7684\u70b9\u4e91\u5206\u5e03\u5bc6\u5ea6\u5dee\u8ddd\u8f83\u5927\uff0c\u5c06BEV\u6cbf\u7740\u6df1\u5ea6\u8f74\u5206\u6210\u5e26\u6709\u91cd\u53e0\u90e8\u5206\u7684\u51e0\u4e2a\u90e8\u5206\uff0c\u6267\u884c\u4e0d\u540c\u7684\u5377\u79ef\u64cd\u4f5c\u3002\u5728KITTI\u4e0a\u7684\u6027\u80fd\u4e0ePointPillars\u548cPointRCNN\u76f8\u8fd1\u3002","title":"SegVoxelNet: Exploring Semantic Context and Depth-aware Features for 3D Vehicle Detection from Point Cloud"},{"location":"other_categories/Summaries/Summary_of_ICRA_2020/#radar-as-a-teacher-weakly-supervised-vehicle-detection-using-radar-labels","text":"pdf \u8fd9\u7bc7paper\u5efa\u8bae\u53c2\u8003\u6b64\u524dNIPS\u7684 co-teaching\u7684paper","title":"Radar as a Teacher: Weakly Supervised Vehicle Detection using Radar Labels"},{"location":"other_categories/Summaries/Summary_of_ICRA_2020/#self-supervised-linear-motion-deblurring","text":"pdf code \u8fd9\u7bc7paper\u51fa\u81eaKITTI\u6570\u636e\u96c6\u7684\u5b9e\u9a8c\u5ba4\u3002\u8fd9\u7bc7paper\u7684\u4e3b\u8981idea\u662f\u4f7f\u7528\u4e00\u4e2areblur module\uff0c\u5728\u7ebf\u6027\u8fd0\u52a8\u7684\u5047\u8bbe\u4e0b\uff0c\u5229\u7528\u5149\u6d41\u4e0eblurring\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5c06\u4e00\u4e2adeblurred\u7684module\u91cd\u65b0\u53d8\u4e3ablurred\uff0c\u8fd9\u6837\u5c31\u53ef\u4ee5\u5f62\u6210\u4e00\u4e2a\u81ea\u76d1\u7763\u7684\u4f53\u7cfb\u3002 \u672c\u6587\u4f7f\u7528\u73b0\u6210\u7684deblur\u4ee5\u53ca\u5149\u6d41\u7f51\u7edc\u7ed3\u6784\uff0c\u5229\u7528\u524d\u540e\u5e27\u7684consistence\u8bad\u7ec3\u5149\u6d41\uff0c\u540c\u65f6\u5bf9deblur\u7ed3\u679c\u63d0\u51fa\u9690\u6027\u7684\u8981\u6c42\u3002\u524d\u9762\u63d0\u5230\u7684\u81ea\u76d1\u7763\u7f51\u7edcloss\u53ef\u4ee5\u8bad\u7ec3deblur\u7f51\u7edc\uff0c\u540c\u65f6\u5bf9\u5149\u6d41\u7684\u8ba1\u7b97\u63d0\u51fa\u9690\u6027\u7684\u8981\u6c42\uff0c\u672c\u6587\u7684reblur\u662f\u4e00\u4e2a\u975e\u5b66\u4e60\u53ef\u5fae\u5206\u6a21\u5757\uff0c\u56e0\u800c\u6574\u4e2a\u7f51\u7edc\u53ef\u5fae\u5206\uff0c\u53ef\u4ee5\u7aef\u5230\u7aef\u81ea\u76d1\u7763\u5b66\u4e60\u3002 reblur\u6a21\u5757\u65b9\u7a0b: \\mathbf{B}(\\mathbf{x}) \\approx \\frac{1}{2 N+1} \\sum_{i=-N}^{N}\\left(\\mathcal{W}_{0 \\rightarrow i} \\circ \\mathbf{I}_{0}\\right)(\\mathbf{x}) \u5176\u4e2d \\mathcal{W} \u6307\u7684\u662f\u5c06\u539f\u56fe\u6839\u636e\u5149\u6d41\u8fdb\u884cwarping,","title":"Self-supervised linear motion deblurring"},{"location":"other_categories/Summaries/Summary_of_ICRA_2020/#fast-panoptic-segmentation-network","text":"pdf","title":"Fast Panoptic Segmentation Network"},{"location":"other_categories/Summaries/Summary_of_ICRA_2020/#real-time-semantic-stereo-matching","text":"pdf \u5728\u8bed\u4e49\u5206\u5272\u548c\u53cc\u76ee\u4e0a\u90fd\u6709\u4e0d\u9519\u7684\u70b9\u6570(\u4e00\u822c\u822c)\uff0c\u4e3b\u8981\u662f\u901f\u5ea6\u6bd4\u8f83\u5feb","title":"Real-Time Semantic Stereo Matching"},{"location":"other_categories/Summaries/Summary_of_ICRA_2020/#multidepth-single-image-depth-estimation-via-multi-task-regression-and-classification","text":"pdf \u6df1\u5ea6\u9884\u6d4b\u95ee\u9898\u540c\u65f6\u8d70\u5206\u7c7b\u4e0e\u56de\u5f52\u3002","title":"MultiDepth: Single-Image Depth Estimation via Multi-Task Regression and Classification"},{"location":"other_categories/Summaries/Summary_of_ICRA_2020/#mpc-net-a-first-principles-guided-policy-search","text":"pdf code \u8fd9\u7bc7paper\u4f7f\u7528\u7c7b\u6a21\u4eff\u5b66\u4e60\uff0c\u5f97\u5230\u4e00\u4e2a\u5feb\u901f\u7684MPC approximator.\u672c\u6587\u5229\u7528\u6700\u4f18\u63a7\u5236\u89e3\u7684\u5fc5\u8981\u6761\u4ef6\uff0cHJB\u65b9\u7a0b\uff0c\u8981\u6c42\u4e00\u4e0b\u54c8\u5bc6\u987f\u91cf\u6700\u5c0f\u5316, \\begin{aligned} \\boldsymbol{u}^{*}(t, \\boldsymbol{x}) &=\\arg \\min _{\\boldsymbol{u}} \\mathcal{H}(\\boldsymbol{x}, \\boldsymbol{u}, t) \\\\ \\mathcal{H}(\\boldsymbol{x}, \\boldsymbol{u}, t) &:=\\mathcal{L}(\\boldsymbol{x}, \\boldsymbol{u}, t)+\\partial_{\\boldsymbol{x}} V(t, \\boldsymbol{x}) \\boldsymbol{f}(\\boldsymbol{x}, \\boldsymbol{u}, t) \\end{aligned} \u672c\u6587\u63d0\u51fa\u4f7f\u7528SLQ(Sequential-Linear-Quadritic)\u6700\u4f18\u5316\u65b9\u6cd5\u8fdb\u884c\u6700\u4f18\u5316\u6c42\u89e3(\u8fd9\u4e2a\u7b97\u6cd5\u7c7b\u4f3c\u4e8e\u8fde\u7eed\u65f6\u95f4\u7684\u8fed\u4ee3LQR),\u901a\u8fc7\u8fd9\u4e2a\u6c42\u89e3\u5668\u53ef\u4ee5\u5f97\u5230MPC teacher\u7684\u63a7\u5236\u547d\u4ee4\uff0c\u4ee5\u53cavalue function\u5173\u4e8ex\u7684\u6c42\u5bfc \\partial_{\\boldsymbol{x}} V(t, \\boldsymbol{x}) \u5b66\u4e60\u65b9\u6cd5: 1. \u5728\u6bcf\u4e00\u6b21\u8fed\u4ee3\u4e2d\uff0c\u4f7f\u7528MPC\u4ee5\u53ca\u7f51\u7edc\u7684\u878d\u5408\u4fe1\u53f7\uff0c\u5bf9\u7cfb\u7edf\u5b8c\u6210\u4e00\u4e2a\u5e8f\u5217\u7684\u4eff\u771f\uff0c\u878d\u5408\u6bd4\u91cd\u968f\u7740\u8bad\u7ec3\u7684\u8fdb\u884c\u9010\u6e10\u7531\u7f51\u7edc\u8f93\u51fa\u4e3b\u5bfc\uff0c\u5728buffer\u4e2d\u5b58\u4e0b\u65f6\u95f4\u6233\uff0cstate\uff0cvalue\u51fd\u6570\u5bf9\u4e8estate\u7684\u5bfc\u6570\u77e2\u91cf,\u7f51\u7edc\u6700\u4f18\u8f93\u51fa\u503c\u7b49\u7ed3\u679c\u3002 2. \u4ecebuffer\u4e2d\u8fdb\u884c\u91c7\u6837\uff0c\u4f7f\u7528\u5b58\u50a8\u4e2d\u7684state, dvdx, t\u4ee5\u53ca\u7f51\u7edc\u8f93\u51fa\u7684u\uff0c\u8ba1\u7b97\u54c8\u5bc6\u987f\u91cf\u3002\u5bf9\u4e8e\u672c\u6587\u5b9e\u9a8c\u4e2d\u7ed9\u51fa\u4e8c\u6b21\u635f\u5931\u51fd\u6570\uff0c \\mathcal{L} \u4e0e u \u76f8\u5173\u7684\u53ea\u6709regularization\u9879 uRu^T ,dvdx\u4e3a\u5e38\u77e2\u91cf\uff0c f(x, u, t) \u4e3a\u6a21\u578b\u7684 \\dot x ,\u4e0e\u7cfb\u7edf\u6a21\u578b\u6709\u5173\uff0c\u4e5f\u5c31\u4f1a\u4e0e u \u6709\u5173\u3002 3. \u628a\u54c8\u5bc6\u987f\u91cf\u7406\u89e3\u4e3a\u635f\u5931\u51fd\u6570\uff0c\u4f7f\u7528\u68af\u5ea6\u4e0b\u964d\u4f18\u5316\u7f51\u7edc\u53c2\u6570\uff0c","title":"MPC-Net: A First Principles Guided Policy Search"},{"location":"other_categories/Summaries/Summary_of_ICRA_2020/#maplite-autonomous-intersection-navigation-without-a-detailed-prior-map","text":"pdf \u8fd9\u7bc7paper\u63d0\u51fa\u4e86\u4e00\u4e2a\u7cfb\u7edf\uff0c\u4ec5\u5229\u7528\u62d3\u6251\u5730\u56fe\u5b8c\u6210\u65e0\u4eba\u8f66\u7684\u5b9a\u4f4d\u4e0e\u5bfc\u822a\u4efb\u52a1\u3002\u70b9\u4e91\u7684\u8def\u9762\u5206\u5272\u4f7f\u7528\u7684\u505a\u6cd5\u662f\u5bf9\u6bcf\u4e00\u4e2a\u70b9\u63d0\u53d6\u8bbe\u5b9a\u7684\u4e94\u4e2afeature\uff0c\u4f7f\u7528linear SVM\u5224\u65ad\u5b83\u662f\u5426\u5728\u5730\u9762\u4e0a\u3002\u5173\u952e\u5bf9\u4e8e\u62d3\u6251\u5730\u56fe\u7684\u5b9a\u4f4d\uff0c\u672c\u6587\u4f7f\u7528\u4f20\u7edf\u7684\u6982\u7387\u6ee4\u6ce2\u7b97\u6cd5,\u6709\u4e00\u5b9a\u5c40\u9650\u6027\uff0c\u4f46\u662f\u5728\u4e00\u5b9a\u8303\u56f4\u5185\u663e\u7136\u662f\u6709\u6548\u7684\u3002","title":"MapLite: Autonomous Intersection Navigation Without a Detailed Prior Map"},{"location":"other_categories/Summaries/Summary_of_SegLoss/","text":"Segmentation Loss Odyssey \u8fd9\u7bc7paper\u662f\u4e00\u4e2a\u6781\u4e3a\u7b80\u77ed\u7684\u5bf9\u4e8e\u8bed\u4e49\u5206\u5272Loss\u7684review. \u5f00\u6e90\u7684\u4ee3\u7801\u91cc\u9762\u7528pytorch\u7b80\u8981\u5730\u5b9e\u73b0\u4e86\u672c\u6587\u63d0\u5230\u7684\u6240\u6709loss\u51fd\u6570\u3002\u6587\u7ae0\u6beb\u4e0d\u62d6\u6ce5\u5e26\u6c34\uff0c\u8fd1\u4e4e\u4e8e\u7b14\u8bb0\u5c0f\u6284\u3002\u5176ReadME\u662f\u4e00\u4e2a\u5f88\u597d\u7684reference\u5e73\u53f0\u3002 \u539f\u6587README Date First Author Title Conference/Journal 202004 J. H. Moltz Contour Dice coefficient (CDC) Loss: Learning a Loss Function for Segmentation: A Feasibility Study ISBI 202003 Suprosanna Shit clDice -- a Topology-Preserving Loss Function for Tubular Structure Segmentation (pytorch) arXiv 202002 TBD Uncertainty-weighted Loss: Function for Medical Image Segmentation using Deep Convolutional Neural Network (paper) MIDL 2020 201912 Yuan Xue Shape-Aware Organ Segmentation by Predicting Signed Distance Maps (arxiv) AAAI 2020 201912 Xiaoling Hu Topology-Preserving Deep Image Segmentation (paper) NeurIPS 201912 JohannesC.Paetzold clDice-a Novel Connectivity-Preserving Loss Function for Vessel Segmentation (paper) MedNeurIPS2019 201910 Shuai Zhao Region Mutual Information Loss for Semantic Segmentation (paper) (pytorch) NeurIPS 2019 201910 Shuai Zhao Correlation Maximized Structural Similarity Loss for Semantic Segmentation (paper) arxiv 201908 Pierre-AntoineGanaye Removing Segmentation Inconsistencies with Semi-Supervised Non-Adjacency Constraint (paper) (official pytorch) Medical Image Analysis 201906 Xu Chen Learning Active Contour Models for Medical Image Segmentation (paper) (official-keras) CVPR 2019 20190422 Davood Karimi Reducing the Hausdorff Distance in Medical Image Segmentation with Convolutional Neural Networks (paper) TMI 201907 20190417 Francesco Caliva Distance Map Loss Penalty Term for Semantic Segmentation (paper) MIDL 2019 20190411 Su Yang Major Vessel Segmentation on X-ray Coronary Angiography using Deep Networks with a Novel Penalty Loss Function (paper) MIDL 2019 20190405 Boah Kim Multiphase Level-Set Loss for Semi-Supervised and Unsupervised Segmentation with Deep Learning (paper) arxiv 201901 Seyed Raein Hashemi Asymmetric Loss Functions and Deep Densely Connected Networks for Highly Imbalanced Medical Image Segmentation: Application to Multiple Sclerosis Lesion Detection (paper) IEEE Access 201812 Hoel Kervadec Boundary loss for highly unbalanced segmentation (paper) , (pytorch 1.0) MIDL 2019 201810 Nabila Abraham A Novel Focal Tversky loss function with improved Attention U-Net for lesion segmentation (paper) (keras) ISBI 2019 201809 Fabian Isensee CE+Dice: nnU-Net: Self-adapting Framework for U-Net-Based Medical Image Segmentation (paper) arxiv 20180831 Ken C. L. Wong 3D Segmentation with Exponential Logarithmic Loss for Highly Unbalanced Object Sizes (paper) MICCAI 2018 20180815 Wentao Zhu Dice+Focal: AnatomyNet: Deep Learning for Fast and Fully Automated Whole-volume Segmentation of Head and Neck Anatomy (arxiv) (pytorch) Medical Physics 201806 Javier Ribera Weighted Hausdorff Distance: Locating Objects Without Bounding Boxes (paper) , (pytorch) CVPR 2019 201805 Saeid Asgari Taghanaki Combo Loss: Handling Input and Output Imbalance in Multi-Organ Segmentation (arxiv) (keras) Computerized Medical Imaging and Graphics 201709 S M Masudur Rahman AL ARIF Shape-aware deep convolutional neural network for vertebrae segmentation (paper) MICCAI 2017 Workshop 201708 Tsung-Yi Lin Focal Loss for Dense Object Detection (paper) , (code) ICCV, TPAMI 20170711 Carole Sudre Generalised Dice overlap as a deep learning loss function for highly unbalanced segmentations (paper) DLMIA 2017 20170703 Lucas Fidon Generalised Wasserstein Dice Score for Imbalanced Multi-class Segmentation using Holistic Convolutional Networks (paper) MICCAI 2017 BrainLes 201705 Maxim Berman The Lov\u00e1sz-Softmax loss: A tractable surrogate for the optimization of the intersection-over-union measure in neural networks (paper) , (code) CVPR 2018 201701 Seyed Sadegh Mohseni Salehi Tversky loss function for image segmentation using 3D fully convolutional deep networks (paper) MICCAI 2017 MLMI 201612 Md Atiqur Rahman Optimizing Intersection-Over-Union in Deep Neural Networks for Image Segmentation (paper) 2016 International Symposium on Visual Computing 201606 Fausto Milletari \"Dice Loss\" V-net: Fully convolutional neural networks for volumetric medical image segmentation (paper) , (caffe code) International Conference on 3D Vision 201605 Zifeng Wu TopK loss Bridging Category-level and Instance-level Semantic Image Segmentation (paper) arxiv 201511 Tom Brosch \"Sensitivity-Specifity loss\" Deep Convolutional Encoder Networks for Multiple Sclerosis Lesion Segmentation (paper) (code) MICCAI 2015 201505 Olaf Ronneberger \"Weighted cross entropy\" U-Net: Convolutional Networks for Biomedical Image Segmentation (paper) MICCAI 2015 201309 Gabriela Csurka What is a good evaluation measure for semantic segmentation? (paper) BMVA 2013 Most of the corresponding tensorflow code can be found here . \u672c\u6587 citation @article{SegLossOdyssey, title={Segmentation Loss Odyssey}, author={Ma Jun}, journal={arXiv preprint arXiv:2005.13449}, year={2020} }","title":"Segmentation Loss Odyssey"},{"location":"other_categories/Summaries/Summary_of_SegLoss/#segmentation-loss-odyssey","text":"\u8fd9\u7bc7paper\u662f\u4e00\u4e2a\u6781\u4e3a\u7b80\u77ed\u7684\u5bf9\u4e8e\u8bed\u4e49\u5206\u5272Loss\u7684review. \u5f00\u6e90\u7684\u4ee3\u7801\u91cc\u9762\u7528pytorch\u7b80\u8981\u5730\u5b9e\u73b0\u4e86\u672c\u6587\u63d0\u5230\u7684\u6240\u6709loss\u51fd\u6570\u3002\u6587\u7ae0\u6beb\u4e0d\u62d6\u6ce5\u5e26\u6c34\uff0c\u8fd1\u4e4e\u4e8e\u7b14\u8bb0\u5c0f\u6284\u3002\u5176ReadME\u662f\u4e00\u4e2a\u5f88\u597d\u7684reference\u5e73\u53f0\u3002","title":"Segmentation Loss Odyssey"},{"location":"other_categories/Summaries/Summary_of_SegLoss/#readme","text":"Date First Author Title Conference/Journal 202004 J. H. Moltz Contour Dice coefficient (CDC) Loss: Learning a Loss Function for Segmentation: A Feasibility Study ISBI 202003 Suprosanna Shit clDice -- a Topology-Preserving Loss Function for Tubular Structure Segmentation (pytorch) arXiv 202002 TBD Uncertainty-weighted Loss: Function for Medical Image Segmentation using Deep Convolutional Neural Network (paper) MIDL 2020 201912 Yuan Xue Shape-Aware Organ Segmentation by Predicting Signed Distance Maps (arxiv) AAAI 2020 201912 Xiaoling Hu Topology-Preserving Deep Image Segmentation (paper) NeurIPS 201912 JohannesC.Paetzold clDice-a Novel Connectivity-Preserving Loss Function for Vessel Segmentation (paper) MedNeurIPS2019 201910 Shuai Zhao Region Mutual Information Loss for Semantic Segmentation (paper) (pytorch) NeurIPS 2019 201910 Shuai Zhao Correlation Maximized Structural Similarity Loss for Semantic Segmentation (paper) arxiv 201908 Pierre-AntoineGanaye Removing Segmentation Inconsistencies with Semi-Supervised Non-Adjacency Constraint (paper) (official pytorch) Medical Image Analysis 201906 Xu Chen Learning Active Contour Models for Medical Image Segmentation (paper) (official-keras) CVPR 2019 20190422 Davood Karimi Reducing the Hausdorff Distance in Medical Image Segmentation with Convolutional Neural Networks (paper) TMI 201907 20190417 Francesco Caliva Distance Map Loss Penalty Term for Semantic Segmentation (paper) MIDL 2019 20190411 Su Yang Major Vessel Segmentation on X-ray Coronary Angiography using Deep Networks with a Novel Penalty Loss Function (paper) MIDL 2019 20190405 Boah Kim Multiphase Level-Set Loss for Semi-Supervised and Unsupervised Segmentation with Deep Learning (paper) arxiv 201901 Seyed Raein Hashemi Asymmetric Loss Functions and Deep Densely Connected Networks for Highly Imbalanced Medical Image Segmentation: Application to Multiple Sclerosis Lesion Detection (paper) IEEE Access 201812 Hoel Kervadec Boundary loss for highly unbalanced segmentation (paper) , (pytorch 1.0) MIDL 2019 201810 Nabila Abraham A Novel Focal Tversky loss function with improved Attention U-Net for lesion segmentation (paper) (keras) ISBI 2019 201809 Fabian Isensee CE+Dice: nnU-Net: Self-adapting Framework for U-Net-Based Medical Image Segmentation (paper) arxiv 20180831 Ken C. L. Wong 3D Segmentation with Exponential Logarithmic Loss for Highly Unbalanced Object Sizes (paper) MICCAI 2018 20180815 Wentao Zhu Dice+Focal: AnatomyNet: Deep Learning for Fast and Fully Automated Whole-volume Segmentation of Head and Neck Anatomy (arxiv) (pytorch) Medical Physics 201806 Javier Ribera Weighted Hausdorff Distance: Locating Objects Without Bounding Boxes (paper) , (pytorch) CVPR 2019 201805 Saeid Asgari Taghanaki Combo Loss: Handling Input and Output Imbalance in Multi-Organ Segmentation (arxiv) (keras) Computerized Medical Imaging and Graphics 201709 S M Masudur Rahman AL ARIF Shape-aware deep convolutional neural network for vertebrae segmentation (paper) MICCAI 2017 Workshop 201708 Tsung-Yi Lin Focal Loss for Dense Object Detection (paper) , (code) ICCV, TPAMI 20170711 Carole Sudre Generalised Dice overlap as a deep learning loss function for highly unbalanced segmentations (paper) DLMIA 2017 20170703 Lucas Fidon Generalised Wasserstein Dice Score for Imbalanced Multi-class Segmentation using Holistic Convolutional Networks (paper) MICCAI 2017 BrainLes 201705 Maxim Berman The Lov\u00e1sz-Softmax loss: A tractable surrogate for the optimization of the intersection-over-union measure in neural networks (paper) , (code) CVPR 2018 201701 Seyed Sadegh Mohseni Salehi Tversky loss function for image segmentation using 3D fully convolutional deep networks (paper) MICCAI 2017 MLMI 201612 Md Atiqur Rahman Optimizing Intersection-Over-Union in Deep Neural Networks for Image Segmentation (paper) 2016 International Symposium on Visual Computing 201606 Fausto Milletari \"Dice Loss\" V-net: Fully convolutional neural networks for volumetric medical image segmentation (paper) , (caffe code) International Conference on 3D Vision 201605 Zifeng Wu TopK loss Bridging Category-level and Instance-level Semantic Image Segmentation (paper) arxiv 201511 Tom Brosch \"Sensitivity-Specifity loss\" Deep Convolutional Encoder Networks for Multiple Sclerosis Lesion Segmentation (paper) (code) MICCAI 2015 201505 Olaf Ronneberger \"Weighted cross entropy\" U-Net: Convolutional Networks for Biomedical Image Segmentation (paper) MICCAI 2015 201309 Gabriela Csurka What is a good evaluation measure for semantic segmentation? (paper) BMVA 2013 Most of the corresponding tensorflow code can be found here .","title":"\u539f\u6587README"},{"location":"other_categories/Summaries/Summary_of_SegLoss/#citation","text":"@article{SegLossOdyssey, title={Segmentation Loss Odyssey}, author={Ma Jun}, journal={arXiv preprint arXiv:2005.13449}, year={2020} }","title":"\u672c\u6587 citation"},{"location":"other_categories/Summaries/Summary_of_serveral_cvpr2020/","text":"Summaries for several CVPR 2020 papers \u76ee\u5f55: Summaries for several CVPR 2020 papers Bridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selection (ATSS) Depth Sensing Beyond LiDAR Range Motivation \u786c\u4ef6\u914d\u7f6e \u7b97\u6cd5\u6d41\u7a0b RetinaTrack MUXConv: Information Multiplexing in Convolutional Neural Networks Structure Aware Single-stage 3D Object Detection from Point Cloud Camouflaged Object Detection Lightweight Multi-View 3D Pose Estimation through Camera-Disentangled Representation Flow2Stereo: Effective Self-Supervised Learning of Optical Flow and Stereo Matching What You See is What You Get: Exploiting Visibility for 3D Object Detection Instance Shadow Detection A Model-driven Deep Neural Network for Single Image Rain Removal Single Image Optical Flow Estimation with an Event Camera \\Pi - nets: Deep Polynomial Neural Networks Bridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selection (ATSS) pdf code_head code_assigner/core \u8fd9\u7bc7paper\u505a\u4e86\u4e00\u4e2a\u76f8\u5f53\u7ec6\u81f4\u5bf9\u6bd4\u5b9e\u9a8c\u6765\u5206\u6790anchor based\u4e0eanchor free\u6a21\u5757\u7684\u533a\u522b\u7ed3\u679c\u3002\u53e6\u5916\u81ea\u5df1\u63d0\u51fa\u4e86\u66f4\u597d\u7684sampling\u65b9\u6cd5 \\rightarrow ATSS \u4f20\u7edf\u7684RetinaNet(32.5%)\u88abanchor free\u7684 FCOS (37.8%)\u6027\u80fd\u5927\u5e45\u8d85\u8d8a\uff0c\u662f\u4ec0\u4e48\u5f15\u8d77\u4e86\u8fd9\u4e9b\u5dee\u8ddd\u5462\uff0c\u662fanchor free\u4e0eanchor based\u672c\u8eab\u5417\uff1f\u4f5c\u8005\u5206\u6790\u6307\u51fa\uff0c\u5bf9\u4e8e\u4e00\u4e2a\u6bcf\u4e00\u4e2ascale\u53ea\u6709\u4e00\u4e2aanchor box\u7684 Retinanet\u6a21\u578b\u4e0eFCOS\u8fdb\u884c\u5bf9\u6bd4\uff0c\u8bad\u7ec3\u7ec6\u8282\u7684\u5dee\u522b:1. GroupNorm 2. GIoU Loss 3. Center point should be in the Box 4. Centerness branch (also helpful in NMS)5. additional trainable scalar, \u8fd9\u91cc\u9762\u6bd4\u8f83\u91cd\u8981\u7684\u662f(1),(2),(4). RetinaNet\u4e0eFCOS\u7684\u672c\u8d28\u533a\u522b\u6709\u4e8c\uff0c\u7b2c\u4e00\u4e2a\u662f\u5206\u7c7b\u65f6\u7ed9\u5b9a\u6b63\u6837\u672c\u7684\u65b9\u6cd5\u3002 \u7b2c\u4e8c\u4e2a\u662f\u56de\u5f52\u65f6\u6846\u5927\u5c0f\u7684\u56de\u5f52\u65b9\u6cd5.\u4f5c\u8005\u5728\u4e0b\u8868\u5b9e\u9a8c\u8bf4\u660e\u4e86\u6b63\u8d1f\u6837\u672c\u7684\u5206\u914d\u7b56\u7565\u624d\u662f\u5f71\u54cd\u70b9\u6570\u7684\u6700\u91cd\u8981\u56e0\u7d20\u3002 \u4f5c\u8005\u63d0\u51fa\u7684ATSS\u7b97\u6cd5. code_assigner/core Depth Sensing Beyond LiDAR Range pdf \u8fd9\u7bc7paper\u662f\u5c5e\u4e8e\u503c\u5f97\u7cbe\u8bfb\u7684\u6587\u7ae0\u4e4b\u4e00\u3002 Motivation LiDAR\u4e00\u76f4\u4ee5\u6765\u5b58\u5728\u7740\u65e0\u6cd5\u611f\u77e5\u8db3\u591f\u8fdc\u7684\u7269\u4f53\u7684\u95ee\u9898\uff0c\u5373\u4f7f\u7ebf\u6570\u5f88\u5927\uff0c\u6709\u6548\u7684\u8ddd\u79bb\u53ef\u80fd\u4e5f\u5c31\u53ea\u80fd\u53bb\u523080\u7c73\u3002\u800c\u8fd9\u4e2a\u5bf9\u4e8e\u4e2d\u901f\u884c\u9a76\u7684\u6c7d\u8f66\u6765\u8bf4\u4ec5\u4ec5\u662f\u51e0\u79d2\u5185\u7684\u91cc\u7a0b\uff0c\u56e0\u800c\u5f80\u5f80\u4e0d\u591f\u7528\u3002\u672c\u6587\u8981\u5b9e\u73b0\u8d85\u8fc7\u4e00\u767e\u7c73\u4e43\u81f3\u672c\u6587\u7ed9\u51fa\u7684\u8fd1300\u7c73\u8303\u56f4\u5185\u7684\u6df1\u5ea6\u4f30\u8ba1\uff0c\u4f7f\u7528\u4e86\u7279\u6b8a\u914d\u7f6e\u7684\u591a\u76ee\u6444\u50cf\u673a(\u4e09\u76ee)\u3002\u5bf9\u4e8e\u6444\u50cf\u673a\u53c2\u6570\u6765\u8bf4\uff0c\u5c31\u9700\u8981\u5927\u5206\u8fa8\u7387\uff0c\u5c0f\u611f\u53d7\u91ce\u7528\u4e8e\u4e13\u6ce8\u4e8e\u8fdc\u666f\uff0c\u4f46\u662f\u8fd9\u4e2a\u786c\u4ef6\u914d\u7f6e\u7684\u95ee\u9898\u5728\u4e8e\u5bf9\u5fae\u5c0f\u7684\u5916\u53c2\u6270\u52a8\u975e\u5e38\u654f\u611f\uff0c\u56e0\u800c\u9700\u8981\u9c81\u68d2\u7b97\u6cd5or\u5728\u7ebf\u8865\u507f(\u672c\u6587\u7684\u65b9\u6848)\u3002 \u786c\u4ef6\u914d\u7f6e \u4f5c\u8005\u6587\u4e2d\u4ee5\u53ca\u540e\u9762\u7684\u8ba1\u7b97\u540c\u65f6\u6307\u51fa\uff0c\u5de6\u53f3\u76ee\u7684\u8ddd\u79bb\u5e94\u8be5\u8db3\u591f\u8fdc\uff0c\u800c\u4e14\u524d\u540e\u76f8\u673a\u8ddd\u79bb\u4e5f\u5e94\u8be5\u5c3d\u53ef\u80fd\u8fdc\uff0c\u8fd9\u4e9b\u90fd\u4ec5\u4ec5\u88ab\u8f66\u8eab\u5927\u5c0f\u6240\u9650\u5236\u3002 \u7b97\u6cd5\u6d41\u7a0b \u7b2c\u4e00\u6b65\u8fdb\u884c\u7684\u662fretification,\u5f97\u5230\u4e24\u4e2a\u76f8\u673a\u4e4b\u95f4\u7684\u4e00\u4e2a\u76f8\u4e92\u8f6c\u6362\uff0c\u4f7f\u5f97Stereo matching\u7684epipolar line\u662f\u6c34\u5e73\u7684\u3002\u4f5c\u8005\u7b80\u5355\u5730\u8bc1\u660e\u4e86\uff0c\u5bf9\u4e8e\u5c0fFOV\u76f8\u673a\u4ee5\u53ca\u5c0f\u6270\u52a8\u6765\u8bf4\uff0c\u5173\u4e8ex/y\u8f74\u7684\u5fae\u5c0f\u65cb\u8f6c\u90fd\u8fd1\u4f3c\u7b49\u540c\u4e8e\u6574\u4e2a\u753b\u9762\u7684\u5e73\u79fb\u3002\u800c\u753b\u9762\u7684\u65cb\u8f6c\u672c\u8eab\u5c31\u662f\u4e00\u4e2aAffine transform.\u56e0\u800c\u5b9e\u9645\u4e0a\u53ef\u4ee5\u7528homography transform matrix(2x3)\uff0c\u5c31\u53ef\u4ee5\u5b8c\u6210\u5bf9\u539f\u6765\u56fe\u7247\u7684\u77eb\u6b63\u3002\u6574\u4e2a\u7b97\u6cd5\u7684\u601d\u8def\u5c31\u662f\u4f7f\u7528RANSAC\uff0c\u51b3\u5b9aH\u77e9\u9635\u5bf9\u5e94\u7684\u53c2\u6570\u3002\u8981\u6c42\u662f\u8ba9\u5c3d\u53ef\u80fd\u591a\u7684matched point\u90fd\u5728\u540c\u4e00\u4e2a\u6c34\u5e73\u7ebf\u4e0a\u3002\u53ef\u4ee5\u60f3\u8c61\u7684\u662f\uff0c\u8fd9\u6837\u7684retification\u65b9\u5f0f\u65e0\u6cd5\u51b3\u5b9a\u76f8\u673a\u5173\u4e8ey\u8f74\u7684\u65cb\u8f6c\uff0c\u56e0\u800cwarped\u540e\u7684\u56fe\u7247\u8ba1\u7b97\u5f97\u5230\u7684\u53cc\u76ee\u5339\u914ddisparity\u4e0e\u6b63\u786e\u7684\u503c\u4e4b\u95f4\u6709\u4e00\u4e2abias(\u53ef\u4ee5\u7406\u89e3\u4e3a\u5bf9\u65cb\u8f6c\u7684\u4f30\u8ba1\u7684bias/ambiguity) Disparity Estimation\u4f7f\u7528\u7684\u662f\u4e00\u4e2apretrained\u7684stereo matching\u7f51\u7edc. \\left\\langle\\mathbf{H}_{2,1: 3}^{(l)}, \\mathbf{x}^{(l)}\\right\\rangle-\\left\\langle\\mathbf{H}_{2,1: 3}^{(r)}, \\mathbf{x}^{(r)}\\right\\rangle=0 \u7b2c\u4e09\u6b65\u662f\u5229\u7528\u524d\u540e\u76f8\u673a\u6d88\u9664\u7b2c\u4e00\u90e8\u5206\u4ea7\u751f\u7684ambiguity.\u5bf9\u4e8e\u5de6\u76ee\u4e0a\u4efb\u610f\u4e24\u4e2a\u5177\u6709\u76f8\u540c\u6df1\u5ea6\u7684pixel x_1^{(l)}, x_2^{(l)} \uff0c\u8fd9\u4e24\u4e2a\u70b9\u57283D\u4e2d\uff0c\u76f8\u5bf9\u4e8e\u524d\u540e\u6444\u50cf\u673a\u7684 x , y \u5750\u6807\u7684\u5dee\u662f\u4e00\u81f4\u7684(\u8fd9\u91cc\u6307\u7684\u662f\u76f8\u673a\u53c2\u8003\u7cfb\u800c\u4e0d\u662f\u56fe\u7247\u53c2\u8003\u7cfb)\uff0c\u8fd9\u91cc\u5f15\u51fa\u4ee5\u4e0b\u63a8\u5bfc\u3002 \\begin{aligned} m_{l}=\\left\\|\\mathbf{x}_{1}^{(l)}-\\mathbf{x}_{2}^{(l)}\\right\\|&=\\frac{f}{z^{(l)}} \\cdot\\left\\|\\mathbf{X}_{1}^{(l)}-\\mathbf{X}_{2}^{(l)}\\right\\| \\\\ m_{b}&=\\frac{f}{z^{(b)}} \\cdot\\left\\|\\mathbf{X}_{1}^{(b)}-\\mathbf{X}_{2}^{(b)}\\right\\| \\\\ \\mathbf{X}_{1}^{(l)}-\\mathbf{X}_{2}^{(l)}&=\\mathbf{X}_{1}^{(b)}-\\mathbf{X}_{2}^{(b)}, z^{(b)}=z^{(l)}+C_{l b} \\\\ \\frac{m_{l}}{m_{b}}&=\\frac{z^{(l)}+C_{l b}}{z^{(l)}}\\\\ z=z^{(l)}&=\\frac{C_{l b}}{\\frac{m_{l}}{m_{b}}-1}\\\\ \\end{aligned} \u5bf9\u4e8e\u4e00\u7ec4\u7b26\u5408 m_l > m_b , m_l > \\delta , |d_1 - d_2| < \\eta \u7684\u70b9\uff0c\u7531\u5b83\u4eec\u7684\u6570\u503c\u53ef\u4ee5\u5f97\u5230\u7684\u5bf9disparity bias\u7684\u4f30\u8ba1\u4e3a: q=f \\cdot \\frac{C_{l r}}{C_{l b}} \\cdot\\left(\\frac{m_{l}}{m_{b}}-1\\right)-\\frac{d_{1}+d_{2}}{2} \u524d\u9762\u63d0\u5230\u7684\u8fd9\u4e09\u4e2a\u6761\u4ef6\u5206\u522b\u8868\u793a (1) \u8fd9\u4e2abatched\u662f\u5426\u4e8b\u5b9e\u4e0a\u771f\u5b9e;\u9760\u524d\u76f8\u673a\u770b\u5230\u7684\u4e24\u70b9\u8ddd\u79bb\u7406\u5e94\u66f4\u5927 (2) \u4e24\u4e2a\u70b9\u7684\u8ddd\u79bb\u5e94\u8be5\u8db3\u591f\u5927\uff0c\u6700\u597d\u4e0d\u8981\u8d34\u5728\u4e00\u8d77 (3) \u4e24\u4e2a\u50cf\u7d20\u4e4b\u95f4\u7684\u8ddd\u79bb\u51e0\u4e4e\u662f\u76f8\u7b49\u7684\u3002 \u4f5c\u8005\u540e\u9762\u7528\uff1a 1. \u7528\u4eff\u771f\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5f88\u5c0f\u7684\u8bef\u5dee\uff0c\u5728\u53cc\u76ee\u957f\u8ddd\u79bb\u5339\u914d\u4e2d\u5c31\u4f1a\u5f88\u5927\u7684\u4f7f\u6027\u80fd\u9000\u5316\u3002 2. \u4ed6\u4eec\u63d0\u51fa\u7684retification\u65b9\u6cd5\u4e0e\u6807\u51c68\u70b9\u6cd5\u5bf9\u6bd4\u5728\u8fd9\u4e2a\u573a\u666f\u4e0b\u6570\u503c\u66f4\u7a33\u5b9a\uff1b\u56e0\u4e3a\u4ed6\u4eec\u5229\u7528\u4e86\u8fd9\u4e2a\u573a\u666f\u4e0b\u7684\u5148\u9a8c\u77e5\u8bc6\u3002 3. \u5b9e\u9a8c\u8bf4\u660e\u4e86ambiguity removal\u7684\u8fc7\u7a0b\uff0c\u4ee5\u53ca\u6700\u540e\u8ba1\u7b97\u7684\u6709\u6548\u6027\u3002 RetinaTrack pdf \u7279\u70b9\uff0c\u4e0d\u540canchor\u5728\u66f4\u65e9\u671ffeatures\u5c31\u5f00\u59cb\u5206\u5f00\uff0c\u6bcf\u4e00\u4e2aanchor\u8f93\u51fa\u4e00\u4e2a256\u7ef4\u5ea6\u7684features. \u5bf9 \u5355\u4e00\u56fe\u7247 \u7528triplet loss \\mathcal{L}_{B H}(\\theta ; X)=\\sum_{j=1}^{A} \\text { SoftPlus }\\left(m+\\max _{p=1 \\rightarrow A \\atop t_{j}=t_{p}} D_{j p}-\\min _{\\ell=1 \\ldots A \\atop t_{j} \\neq t_{\\ell}} D_{j \\ell}\\right) \u6838\u5fc3\u601d\u8def\u5c31\u662f\u76f8\u540cinstance\u7684\u4e0d\u540canchor\u8f93\u51fa\u76f8\u4f3c\u7684embedding\uff0c\u4e0d\u540cinstance\u7684\u4e0d\u540canchor\u8f93\u51fa\u4e0d\u540c\u7684embedding\u3002\u672c\u6587\u7528\u57fa\u7840\u7684euclidean distance\u4f5c\u4e3aloss MUXConv: Information Multiplexing in Convolutional Neural Networks pdf code Structure Aware Single-stage 3D Object Detection from Point Cloud pdf code \u57fa\u4e8eMMdetection\u5f00\u53d1\u7684\u70b9\u4e913D \u68c0\u6d4b\uff0c\u6027\u80fd\u5f88\u9ad8\uff0c\u91cd\u70b9\u5728\u4e8e\u9644\u52a0task\u7684\u8bbe\u8ba1\u7684\uff0c\u80fd\u8ba9\u4e00\u4e2a\u63a5\u8fd1\u4e8eVoxelNet\u7684\u7ed3\u6784\u5f97\u5230\u5f88\u5927\u7684\u63d0\u5347 Camouflaged Object Detection pdf code \u672c\u95ee\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u95ee\u9898\u4ee5\u53ca\u65b0\u7684\u6570\u636e\u96c6\uff0c\u6709 \u4e2d\u6587paper . Lightweight Multi-View 3D Pose Estimation through Camera-Disentangled Representation pdf FTL\u5c42\u4e0d\u662f\u4e00\u4e2a\u5b66\u4e60\u5c42\uff0c\u7528\u4e00\u4e2a\u5168\u8fde\u63a5\u5c42\uff0c\u5c062D\u9884\u6d4b\u7528\u65cb\u8f6c\u53d8\u6362\u8f6c\u6362\u5230\u4e0ecamera\u65b9\u4f4d\u65e0\u5173\u7684\u7ed3\u679c \\mathbf{y}=F_{T_{1}, \\ldots, T_{n}}[\\mathbf{x}]=\\left[\\begin{array}{ccc} F_{T_{1}} & & \\\\ & \\ddots & \\\\ & & F_{T_{n}} \\end{array}\\right] 3D \u4f4d\u7f6e\u4f30\u8ba1\u91c7\u7528\u7684\u662fSFM\u7684formulation d_{i} u_{i}=p_{i}^{1 T} \\mathbf{x}, d_{i} v_{i}=p_{i}^{2 T} \\mathbf{x}, d_{i}=p_{i}^{3 T} \\mathbf{x} \\begin{array}{l} \\left(u_{i} p_{i}^{3 T}-p_{i}^{1 T}\\right) \\mathbf{x}=0 \\\\ \\left(v_{i} p_{i}^{3 T}-p_{i}^{2 T}\\right) \\mathbf{x}=0 \\end{array} \u628a\u540c\u4e00\u4e2a\u5173\u8282\u6240\u6709\u70b9\u653e\u5728\u4e00\u8d77\uff0c A \\mathbf{x}=\\mathbf{0} ,\u539f\u7406\u4e0a\u6765\u8bf4\u9700\u8981\u4f7f\u7528SVD\u627e\u51fa\u6700\u5c0f\u7279\u5f81\u503c\u5bf9\u5e94\u7684\u7279\u5f81\u5411\u91cf\u3002\u4f5c\u8005\u6307\u51fa\u5982\u679c\u53ea\u9700\u8981\u6c42\u6700\u5c0f\u7279\u5f81\u503c\uff0c\u4e0d\u9700\u8981\u4f7f\u7528SVD\uff0c\u4f7f\u7528\u4ee5\u4e0b\u8fed\u4ee3\u7b97\u6cd5\u5373\u53ef Flow2Stereo: Effective Self-Supervised Learning of Optical Flow and Stereo Matching pdf What You See is What You Get: Exploiting Visibility for 3D Object Detection pdf \u672c\u6587\u4e3b\u8981\u9488\u5bf9\u7684\u5c31\u662fNuscene\u573a\u666f\u4e2d\u88ab\u906e\u6321\u6bd4\u8f83\u4e25\u91cd\u7684\u7269\u4f53\u3002\u901a\u8fc7\u7528\u70b9\u4e91\u5efa\u7acb\u7684occupancy map,\u7f51\u7edc\u53ef\u4ee5infer\u4ec0\u4e48\u5730\u65b9\u662f\u53ef\u80fd\u88ab\u906e\u6321\u800c\u53ef\u80fd\u6709\u7269\u4f53\u7684\u3002 \u672c\u6587\u6709\u4e00\u4e2a\u6bd4\u8f83\u597d\u7684 \u4e2d\u6587\u535a\u5ba2 ,\u63d0\u5230\u4e86\u672c\u6587\u51e0\u4e2a\u91cd\u8981\u7684\u6709\u8da3\u7684\u7ec6\u8282\uff0c\u7b2c\u4e00\u4e2a\u662f3D\u4e16\u754c\u7684Fast Voxel Traversal\u751f\u6210occupancy map;\u7b2c\u4e8c\u4e2a\u662f\u6570\u636e\u589e\u5f3a\uff0c\u91c7\u53d6\u7684\u65b9\u6cd5\u4e0eoccupancy map\u8fdb\u884c\u589e\u5f3a\uff1b\u7b2c\u4e09\u4e2a\u662fonline bayesian grid mapping. Instance Shadow Detection pdf code \u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u4efb\u52a1\uff0c\u65b0\u7684dataset\u4ee5\u53cabaseline\u65b9\u6cd5\u3002\u4efb\u52a1\u662f\u7269\u4f53\u4e0e\u5f71\u5b50\u7684instance segmentation\u4ee5\u53ca\u4e00\u4e00\u5bf9\u5e94\u3002 A Model-driven Deep Neural Network for Single Image Rain Removal pdf code Single Image Optical Flow Estimation with an Event Camera pdf \u672c\u6587\u662f\u57fa\u4e8eDAVIS\u7684event + gray scale\u8bbe\u8ba1\u7684\u7b97\u6cd5\uff0c\u4e00\u4e2a\u91cd\u8981\u7684idea\u662fevent\u4fe1\u606f\u672c\u8eab\u53ef\u4ee5\u76f4\u63a5\u5b58\u50a8\u5149\u6d41\u76f8\u5173\u7684\u4fe1\u606f\uff0c\u800c\u7070\u5ea6\u56fe\u7684\u52a8\u6001\u6a21\u7cca\u4e5f\u53ef\u4ee5\u7528\u4e8e\u6307\u5f15\u5149\u6d41\u4f30\u8ba1\u3002\u56e0\u6b64\u672c\u6587\u63d0\u51fa\u540c\u65f6\u4f30\u8ba1\u5149\u6d41\u573a\u4ee5\u53calatent image \\mathcal{L} . \u80fd\u91cf\u51fd\u6570\u8bbe\u8ba1\u4e3a \\min _{\\mathbf{L} . \\mathbf{u}} \\mu_{1} \\phi_{\\mathrm{eve}}(\\mathbf{L}, \\mathbf{u})+\\mu_{2} \\phi_{\\mathrm{blur}}(\\mathbf{L}, \\mathbf{u})+\\phi_{\\mathrm{flow}}(\\nabla \\mathbf{u})+\\phi_{\\mathrm{im}}(\\nabla \\mathbf{L}) \u9996\u5148\u662f\u8003\u8651\u4e86\u5149\u7167\u53d8\u5316\u7684\u5149\u6d41-\u5149\u7167\u4e00\u81f4\u6761\u4ef6 \\begin{aligned} \\phi_{\\mathrm{eve}}(\\mathbf{L}, \\mathbf{u})=\\sum_{\\mathbf{x} \\in \\Omega} \\| &\\mathbf{L}(\\mathbf{x}, f)(\\exp (c \\mathbf{E}(\\mathbf{x}, t))-1) \\\\ &+\\left[u_{\\mathbf{x}}, v_{\\mathbf{x}}\\right]^{\\mathrm{T}} \\nabla \\mathbf{L}(\\mathbf{x}, f) \\|_{1} \\end{aligned} \u6a21\u7cca: \u6a21\u7cca\u540e\u7684\u56fe\u7247 B \u53ef\u4ee5\u7531\u6a21\u7cca\u6838 K \u4ee5\u53calatent \u56fe\u7247 L \u5377\u79ef\u8868\u8fbe: \\begin{aligned} \\mathbf{B}(\\mathbf{x}) &=\\sum_{\\mathbf{y} \\in \\Omega} \\mathbf{k}(\\mathbf{y}) \\mathbf{L}(\\mathbf{x}-\\mathbf{y}) \\\\ &=\\sum_{\\mathbf{y} \\in \\Omega} \\mathbf{k}_{\\mathbf{u}^{\\prime}(\\mathbf{x})}(\\mathbf{y}) \\mathbf{L}(\\mathbf{x}-\\mathbf{y}) \\end{aligned} k_{\\mathbf{u}^{\\prime}(\\mathbf{x})}(\\mathbf{y})=\\left\\{\\begin{array}{ll} \\frac{1}{\\left|\\mathbf{u}^{\\prime}(\\mathbf{x})\\right|}, & \\text { if } \\mathbf{y}=\\alpha \\mathbf{u}^{\\prime}(\\mathbf{x}),|\\alpha| \\leq \\frac{1}{2} \\\\ \\mathbf{0}, & \\text { otherwise } \\end{array}\\right. \u6a21\u7cca\u6761\u4ef6: \\phi_{\\text {blur }}(\\mathbf{L}, \\mathbf{u})=\\sum_{\\mathbf{x}, \\mathbf{y} \\in \\Omega}\\left\\|\\mathbf{k}_{\\mathbf{u}^{\\prime}(\\mathbf{x})}(\\mathbf{y}) \\mathbf{L}(\\mathbf{x}-\\mathbf{y})-\\mathbf{B}(\\mathbf{x})\\right\\|^{2} \u540e\u4e24\u9879\u4e3a\u8fde\u7eed\u6027\u8981\u6c42\uff0c\u6587\u4e2d\u7684\u8bbe\u8ba1\u6bd4\u8f83\u7cbe\u7ec6\u3002 \u4f18\u5316\u65b9\u6cd5\u4e0a\u672c\u6587\u8fed\u4ee3\u8fdb\u884c\u5149\u6d41\u4f30\u8ba1\u4ee5\u53ca\u56fe\u7247\u7684deblur \\Pi - nets: Deep Polynomial Neural Networks pdf \u53ef\u5b66\u4e60\u53c2\u6570\u7684\u591a\u9879\u5f0f\u8ba1\u7b97\u6a21\u5757(\u6700\u7ec8\u8f93\u51fa\u4e3a\u8f93\u5165\u7684\u591a\u9879\u5f0f\u8868\u8fbe)","title":"Summaries for several CVPR 2020 papers"},{"location":"other_categories/Summaries/Summary_of_serveral_cvpr2020/#summaries-for-several-cvpr-2020-papers","text":"\u76ee\u5f55: Summaries for several CVPR 2020 papers Bridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selection (ATSS) Depth Sensing Beyond LiDAR Range Motivation \u786c\u4ef6\u914d\u7f6e \u7b97\u6cd5\u6d41\u7a0b RetinaTrack MUXConv: Information Multiplexing in Convolutional Neural Networks Structure Aware Single-stage 3D Object Detection from Point Cloud Camouflaged Object Detection Lightweight Multi-View 3D Pose Estimation through Camera-Disentangled Representation Flow2Stereo: Effective Self-Supervised Learning of Optical Flow and Stereo Matching What You See is What You Get: Exploiting Visibility for 3D Object Detection Instance Shadow Detection A Model-driven Deep Neural Network for Single Image Rain Removal Single Image Optical Flow Estimation with an Event Camera \\Pi - nets: Deep Polynomial Neural Networks","title":"Summaries for several CVPR 2020 papers"},{"location":"other_categories/Summaries/Summary_of_serveral_cvpr2020/#bridging-the-gap-between-anchor-based-and-anchor-free-detection-via-adaptive-training-sample-selection-atss","text":"pdf code_head code_assigner/core \u8fd9\u7bc7paper\u505a\u4e86\u4e00\u4e2a\u76f8\u5f53\u7ec6\u81f4\u5bf9\u6bd4\u5b9e\u9a8c\u6765\u5206\u6790anchor based\u4e0eanchor free\u6a21\u5757\u7684\u533a\u522b\u7ed3\u679c\u3002\u53e6\u5916\u81ea\u5df1\u63d0\u51fa\u4e86\u66f4\u597d\u7684sampling\u65b9\u6cd5 \\rightarrow ATSS \u4f20\u7edf\u7684RetinaNet(32.5%)\u88abanchor free\u7684 FCOS (37.8%)\u6027\u80fd\u5927\u5e45\u8d85\u8d8a\uff0c\u662f\u4ec0\u4e48\u5f15\u8d77\u4e86\u8fd9\u4e9b\u5dee\u8ddd\u5462\uff0c\u662fanchor free\u4e0eanchor based\u672c\u8eab\u5417\uff1f\u4f5c\u8005\u5206\u6790\u6307\u51fa\uff0c\u5bf9\u4e8e\u4e00\u4e2a\u6bcf\u4e00\u4e2ascale\u53ea\u6709\u4e00\u4e2aanchor box\u7684 Retinanet\u6a21\u578b\u4e0eFCOS\u8fdb\u884c\u5bf9\u6bd4\uff0c\u8bad\u7ec3\u7ec6\u8282\u7684\u5dee\u522b:1. GroupNorm 2. GIoU Loss 3. Center point should be in the Box 4. Centerness branch (also helpful in NMS)5. additional trainable scalar, \u8fd9\u91cc\u9762\u6bd4\u8f83\u91cd\u8981\u7684\u662f(1),(2),(4). RetinaNet\u4e0eFCOS\u7684\u672c\u8d28\u533a\u522b\u6709\u4e8c\uff0c\u7b2c\u4e00\u4e2a\u662f\u5206\u7c7b\u65f6\u7ed9\u5b9a\u6b63\u6837\u672c\u7684\u65b9\u6cd5\u3002 \u7b2c\u4e8c\u4e2a\u662f\u56de\u5f52\u65f6\u6846\u5927\u5c0f\u7684\u56de\u5f52\u65b9\u6cd5.\u4f5c\u8005\u5728\u4e0b\u8868\u5b9e\u9a8c\u8bf4\u660e\u4e86\u6b63\u8d1f\u6837\u672c\u7684\u5206\u914d\u7b56\u7565\u624d\u662f\u5f71\u54cd\u70b9\u6570\u7684\u6700\u91cd\u8981\u56e0\u7d20\u3002 \u4f5c\u8005\u63d0\u51fa\u7684ATSS\u7b97\u6cd5. code_assigner/core","title":"Bridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selection (ATSS)"},{"location":"other_categories/Summaries/Summary_of_serveral_cvpr2020/#depth-sensing-beyond-lidar-range","text":"pdf \u8fd9\u7bc7paper\u662f\u5c5e\u4e8e\u503c\u5f97\u7cbe\u8bfb\u7684\u6587\u7ae0\u4e4b\u4e00\u3002","title":"Depth Sensing Beyond LiDAR Range"},{"location":"other_categories/Summaries/Summary_of_serveral_cvpr2020/#motivation","text":"LiDAR\u4e00\u76f4\u4ee5\u6765\u5b58\u5728\u7740\u65e0\u6cd5\u611f\u77e5\u8db3\u591f\u8fdc\u7684\u7269\u4f53\u7684\u95ee\u9898\uff0c\u5373\u4f7f\u7ebf\u6570\u5f88\u5927\uff0c\u6709\u6548\u7684\u8ddd\u79bb\u53ef\u80fd\u4e5f\u5c31\u53ea\u80fd\u53bb\u523080\u7c73\u3002\u800c\u8fd9\u4e2a\u5bf9\u4e8e\u4e2d\u901f\u884c\u9a76\u7684\u6c7d\u8f66\u6765\u8bf4\u4ec5\u4ec5\u662f\u51e0\u79d2\u5185\u7684\u91cc\u7a0b\uff0c\u56e0\u800c\u5f80\u5f80\u4e0d\u591f\u7528\u3002\u672c\u6587\u8981\u5b9e\u73b0\u8d85\u8fc7\u4e00\u767e\u7c73\u4e43\u81f3\u672c\u6587\u7ed9\u51fa\u7684\u8fd1300\u7c73\u8303\u56f4\u5185\u7684\u6df1\u5ea6\u4f30\u8ba1\uff0c\u4f7f\u7528\u4e86\u7279\u6b8a\u914d\u7f6e\u7684\u591a\u76ee\u6444\u50cf\u673a(\u4e09\u76ee)\u3002\u5bf9\u4e8e\u6444\u50cf\u673a\u53c2\u6570\u6765\u8bf4\uff0c\u5c31\u9700\u8981\u5927\u5206\u8fa8\u7387\uff0c\u5c0f\u611f\u53d7\u91ce\u7528\u4e8e\u4e13\u6ce8\u4e8e\u8fdc\u666f\uff0c\u4f46\u662f\u8fd9\u4e2a\u786c\u4ef6\u914d\u7f6e\u7684\u95ee\u9898\u5728\u4e8e\u5bf9\u5fae\u5c0f\u7684\u5916\u53c2\u6270\u52a8\u975e\u5e38\u654f\u611f\uff0c\u56e0\u800c\u9700\u8981\u9c81\u68d2\u7b97\u6cd5or\u5728\u7ebf\u8865\u507f(\u672c\u6587\u7684\u65b9\u6848)\u3002","title":"Motivation"},{"location":"other_categories/Summaries/Summary_of_serveral_cvpr2020/#_1","text":"\u4f5c\u8005\u6587\u4e2d\u4ee5\u53ca\u540e\u9762\u7684\u8ba1\u7b97\u540c\u65f6\u6307\u51fa\uff0c\u5de6\u53f3\u76ee\u7684\u8ddd\u79bb\u5e94\u8be5\u8db3\u591f\u8fdc\uff0c\u800c\u4e14\u524d\u540e\u76f8\u673a\u8ddd\u79bb\u4e5f\u5e94\u8be5\u5c3d\u53ef\u80fd\u8fdc\uff0c\u8fd9\u4e9b\u90fd\u4ec5\u4ec5\u88ab\u8f66\u8eab\u5927\u5c0f\u6240\u9650\u5236\u3002","title":"\u786c\u4ef6\u914d\u7f6e"},{"location":"other_categories/Summaries/Summary_of_serveral_cvpr2020/#_2","text":"\u7b2c\u4e00\u6b65\u8fdb\u884c\u7684\u662fretification,\u5f97\u5230\u4e24\u4e2a\u76f8\u673a\u4e4b\u95f4\u7684\u4e00\u4e2a\u76f8\u4e92\u8f6c\u6362\uff0c\u4f7f\u5f97Stereo matching\u7684epipolar line\u662f\u6c34\u5e73\u7684\u3002\u4f5c\u8005\u7b80\u5355\u5730\u8bc1\u660e\u4e86\uff0c\u5bf9\u4e8e\u5c0fFOV\u76f8\u673a\u4ee5\u53ca\u5c0f\u6270\u52a8\u6765\u8bf4\uff0c\u5173\u4e8ex/y\u8f74\u7684\u5fae\u5c0f\u65cb\u8f6c\u90fd\u8fd1\u4f3c\u7b49\u540c\u4e8e\u6574\u4e2a\u753b\u9762\u7684\u5e73\u79fb\u3002\u800c\u753b\u9762\u7684\u65cb\u8f6c\u672c\u8eab\u5c31\u662f\u4e00\u4e2aAffine transform.\u56e0\u800c\u5b9e\u9645\u4e0a\u53ef\u4ee5\u7528homography transform matrix(2x3)\uff0c\u5c31\u53ef\u4ee5\u5b8c\u6210\u5bf9\u539f\u6765\u56fe\u7247\u7684\u77eb\u6b63\u3002\u6574\u4e2a\u7b97\u6cd5\u7684\u601d\u8def\u5c31\u662f\u4f7f\u7528RANSAC\uff0c\u51b3\u5b9aH\u77e9\u9635\u5bf9\u5e94\u7684\u53c2\u6570\u3002\u8981\u6c42\u662f\u8ba9\u5c3d\u53ef\u80fd\u591a\u7684matched point\u90fd\u5728\u540c\u4e00\u4e2a\u6c34\u5e73\u7ebf\u4e0a\u3002\u53ef\u4ee5\u60f3\u8c61\u7684\u662f\uff0c\u8fd9\u6837\u7684retification\u65b9\u5f0f\u65e0\u6cd5\u51b3\u5b9a\u76f8\u673a\u5173\u4e8ey\u8f74\u7684\u65cb\u8f6c\uff0c\u56e0\u800cwarped\u540e\u7684\u56fe\u7247\u8ba1\u7b97\u5f97\u5230\u7684\u53cc\u76ee\u5339\u914ddisparity\u4e0e\u6b63\u786e\u7684\u503c\u4e4b\u95f4\u6709\u4e00\u4e2abias(\u53ef\u4ee5\u7406\u89e3\u4e3a\u5bf9\u65cb\u8f6c\u7684\u4f30\u8ba1\u7684bias/ambiguity) Disparity Estimation\u4f7f\u7528\u7684\u662f\u4e00\u4e2apretrained\u7684stereo matching\u7f51\u7edc. \\left\\langle\\mathbf{H}_{2,1: 3}^{(l)}, \\mathbf{x}^{(l)}\\right\\rangle-\\left\\langle\\mathbf{H}_{2,1: 3}^{(r)}, \\mathbf{x}^{(r)}\\right\\rangle=0 \u7b2c\u4e09\u6b65\u662f\u5229\u7528\u524d\u540e\u76f8\u673a\u6d88\u9664\u7b2c\u4e00\u90e8\u5206\u4ea7\u751f\u7684ambiguity.\u5bf9\u4e8e\u5de6\u76ee\u4e0a\u4efb\u610f\u4e24\u4e2a\u5177\u6709\u76f8\u540c\u6df1\u5ea6\u7684pixel x_1^{(l)}, x_2^{(l)} \uff0c\u8fd9\u4e24\u4e2a\u70b9\u57283D\u4e2d\uff0c\u76f8\u5bf9\u4e8e\u524d\u540e\u6444\u50cf\u673a\u7684 x , y \u5750\u6807\u7684\u5dee\u662f\u4e00\u81f4\u7684(\u8fd9\u91cc\u6307\u7684\u662f\u76f8\u673a\u53c2\u8003\u7cfb\u800c\u4e0d\u662f\u56fe\u7247\u53c2\u8003\u7cfb)\uff0c\u8fd9\u91cc\u5f15\u51fa\u4ee5\u4e0b\u63a8\u5bfc\u3002 \\begin{aligned} m_{l}=\\left\\|\\mathbf{x}_{1}^{(l)}-\\mathbf{x}_{2}^{(l)}\\right\\|&=\\frac{f}{z^{(l)}} \\cdot\\left\\|\\mathbf{X}_{1}^{(l)}-\\mathbf{X}_{2}^{(l)}\\right\\| \\\\ m_{b}&=\\frac{f}{z^{(b)}} \\cdot\\left\\|\\mathbf{X}_{1}^{(b)}-\\mathbf{X}_{2}^{(b)}\\right\\| \\\\ \\mathbf{X}_{1}^{(l)}-\\mathbf{X}_{2}^{(l)}&=\\mathbf{X}_{1}^{(b)}-\\mathbf{X}_{2}^{(b)}, z^{(b)}=z^{(l)}+C_{l b} \\\\ \\frac{m_{l}}{m_{b}}&=\\frac{z^{(l)}+C_{l b}}{z^{(l)}}\\\\ z=z^{(l)}&=\\frac{C_{l b}}{\\frac{m_{l}}{m_{b}}-1}\\\\ \\end{aligned} \u5bf9\u4e8e\u4e00\u7ec4\u7b26\u5408 m_l > m_b , m_l > \\delta , |d_1 - d_2| < \\eta \u7684\u70b9\uff0c\u7531\u5b83\u4eec\u7684\u6570\u503c\u53ef\u4ee5\u5f97\u5230\u7684\u5bf9disparity bias\u7684\u4f30\u8ba1\u4e3a: q=f \\cdot \\frac{C_{l r}}{C_{l b}} \\cdot\\left(\\frac{m_{l}}{m_{b}}-1\\right)-\\frac{d_{1}+d_{2}}{2} \u524d\u9762\u63d0\u5230\u7684\u8fd9\u4e09\u4e2a\u6761\u4ef6\u5206\u522b\u8868\u793a (1) \u8fd9\u4e2abatched\u662f\u5426\u4e8b\u5b9e\u4e0a\u771f\u5b9e;\u9760\u524d\u76f8\u673a\u770b\u5230\u7684\u4e24\u70b9\u8ddd\u79bb\u7406\u5e94\u66f4\u5927 (2) \u4e24\u4e2a\u70b9\u7684\u8ddd\u79bb\u5e94\u8be5\u8db3\u591f\u5927\uff0c\u6700\u597d\u4e0d\u8981\u8d34\u5728\u4e00\u8d77 (3) \u4e24\u4e2a\u50cf\u7d20\u4e4b\u95f4\u7684\u8ddd\u79bb\u51e0\u4e4e\u662f\u76f8\u7b49\u7684\u3002 \u4f5c\u8005\u540e\u9762\u7528\uff1a 1. \u7528\u4eff\u771f\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5f88\u5c0f\u7684\u8bef\u5dee\uff0c\u5728\u53cc\u76ee\u957f\u8ddd\u79bb\u5339\u914d\u4e2d\u5c31\u4f1a\u5f88\u5927\u7684\u4f7f\u6027\u80fd\u9000\u5316\u3002 2. \u4ed6\u4eec\u63d0\u51fa\u7684retification\u65b9\u6cd5\u4e0e\u6807\u51c68\u70b9\u6cd5\u5bf9\u6bd4\u5728\u8fd9\u4e2a\u573a\u666f\u4e0b\u6570\u503c\u66f4\u7a33\u5b9a\uff1b\u56e0\u4e3a\u4ed6\u4eec\u5229\u7528\u4e86\u8fd9\u4e2a\u573a\u666f\u4e0b\u7684\u5148\u9a8c\u77e5\u8bc6\u3002 3. \u5b9e\u9a8c\u8bf4\u660e\u4e86ambiguity removal\u7684\u8fc7\u7a0b\uff0c\u4ee5\u53ca\u6700\u540e\u8ba1\u7b97\u7684\u6709\u6548\u6027\u3002","title":"\u7b97\u6cd5\u6d41\u7a0b"},{"location":"other_categories/Summaries/Summary_of_serveral_cvpr2020/#retinatrack","text":"pdf \u7279\u70b9\uff0c\u4e0d\u540canchor\u5728\u66f4\u65e9\u671ffeatures\u5c31\u5f00\u59cb\u5206\u5f00\uff0c\u6bcf\u4e00\u4e2aanchor\u8f93\u51fa\u4e00\u4e2a256\u7ef4\u5ea6\u7684features. \u5bf9 \u5355\u4e00\u56fe\u7247 \u7528triplet loss \\mathcal{L}_{B H}(\\theta ; X)=\\sum_{j=1}^{A} \\text { SoftPlus }\\left(m+\\max _{p=1 \\rightarrow A \\atop t_{j}=t_{p}} D_{j p}-\\min _{\\ell=1 \\ldots A \\atop t_{j} \\neq t_{\\ell}} D_{j \\ell}\\right) \u6838\u5fc3\u601d\u8def\u5c31\u662f\u76f8\u540cinstance\u7684\u4e0d\u540canchor\u8f93\u51fa\u76f8\u4f3c\u7684embedding\uff0c\u4e0d\u540cinstance\u7684\u4e0d\u540canchor\u8f93\u51fa\u4e0d\u540c\u7684embedding\u3002\u672c\u6587\u7528\u57fa\u7840\u7684euclidean distance\u4f5c\u4e3aloss","title":"RetinaTrack"},{"location":"other_categories/Summaries/Summary_of_serveral_cvpr2020/#muxconv-information-multiplexing-in-convolutional-neural-networks","text":"pdf code","title":"MUXConv: Information Multiplexing in Convolutional Neural Networks"},{"location":"other_categories/Summaries/Summary_of_serveral_cvpr2020/#structure-aware-single-stage-3d-object-detection-from-point-cloud","text":"pdf code \u57fa\u4e8eMMdetection\u5f00\u53d1\u7684\u70b9\u4e913D \u68c0\u6d4b\uff0c\u6027\u80fd\u5f88\u9ad8\uff0c\u91cd\u70b9\u5728\u4e8e\u9644\u52a0task\u7684\u8bbe\u8ba1\u7684\uff0c\u80fd\u8ba9\u4e00\u4e2a\u63a5\u8fd1\u4e8eVoxelNet\u7684\u7ed3\u6784\u5f97\u5230\u5f88\u5927\u7684\u63d0\u5347","title":"Structure Aware Single-stage 3D Object Detection from Point Cloud"},{"location":"other_categories/Summaries/Summary_of_serveral_cvpr2020/#camouflaged-object-detection","text":"pdf code \u672c\u95ee\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u95ee\u9898\u4ee5\u53ca\u65b0\u7684\u6570\u636e\u96c6\uff0c\u6709 \u4e2d\u6587paper .","title":"Camouflaged Object Detection"},{"location":"other_categories/Summaries/Summary_of_serveral_cvpr2020/#lightweight-multi-view-3d-pose-estimation-through-camera-disentangled-representation","text":"pdf FTL\u5c42\u4e0d\u662f\u4e00\u4e2a\u5b66\u4e60\u5c42\uff0c\u7528\u4e00\u4e2a\u5168\u8fde\u63a5\u5c42\uff0c\u5c062D\u9884\u6d4b\u7528\u65cb\u8f6c\u53d8\u6362\u8f6c\u6362\u5230\u4e0ecamera\u65b9\u4f4d\u65e0\u5173\u7684\u7ed3\u679c \\mathbf{y}=F_{T_{1}, \\ldots, T_{n}}[\\mathbf{x}]=\\left[\\begin{array}{ccc} F_{T_{1}} & & \\\\ & \\ddots & \\\\ & & F_{T_{n}} \\end{array}\\right] 3D \u4f4d\u7f6e\u4f30\u8ba1\u91c7\u7528\u7684\u662fSFM\u7684formulation d_{i} u_{i}=p_{i}^{1 T} \\mathbf{x}, d_{i} v_{i}=p_{i}^{2 T} \\mathbf{x}, d_{i}=p_{i}^{3 T} \\mathbf{x} \\begin{array}{l} \\left(u_{i} p_{i}^{3 T}-p_{i}^{1 T}\\right) \\mathbf{x}=0 \\\\ \\left(v_{i} p_{i}^{3 T}-p_{i}^{2 T}\\right) \\mathbf{x}=0 \\end{array} \u628a\u540c\u4e00\u4e2a\u5173\u8282\u6240\u6709\u70b9\u653e\u5728\u4e00\u8d77\uff0c A \\mathbf{x}=\\mathbf{0} ,\u539f\u7406\u4e0a\u6765\u8bf4\u9700\u8981\u4f7f\u7528SVD\u627e\u51fa\u6700\u5c0f\u7279\u5f81\u503c\u5bf9\u5e94\u7684\u7279\u5f81\u5411\u91cf\u3002\u4f5c\u8005\u6307\u51fa\u5982\u679c\u53ea\u9700\u8981\u6c42\u6700\u5c0f\u7279\u5f81\u503c\uff0c\u4e0d\u9700\u8981\u4f7f\u7528SVD\uff0c\u4f7f\u7528\u4ee5\u4e0b\u8fed\u4ee3\u7b97\u6cd5\u5373\u53ef","title":"Lightweight Multi-View 3D Pose Estimation through Camera-Disentangled Representation"},{"location":"other_categories/Summaries/Summary_of_serveral_cvpr2020/#flow2stereo-effective-self-supervised-learning-of-optical-flow-and-stereo-matching","text":"pdf","title":"Flow2Stereo: Effective Self-Supervised Learning of Optical Flow and Stereo Matching"},{"location":"other_categories/Summaries/Summary_of_serveral_cvpr2020/#what-you-see-is-what-you-get-exploiting-visibility-for-3d-object-detection","text":"pdf \u672c\u6587\u4e3b\u8981\u9488\u5bf9\u7684\u5c31\u662fNuscene\u573a\u666f\u4e2d\u88ab\u906e\u6321\u6bd4\u8f83\u4e25\u91cd\u7684\u7269\u4f53\u3002\u901a\u8fc7\u7528\u70b9\u4e91\u5efa\u7acb\u7684occupancy map,\u7f51\u7edc\u53ef\u4ee5infer\u4ec0\u4e48\u5730\u65b9\u662f\u53ef\u80fd\u88ab\u906e\u6321\u800c\u53ef\u80fd\u6709\u7269\u4f53\u7684\u3002 \u672c\u6587\u6709\u4e00\u4e2a\u6bd4\u8f83\u597d\u7684 \u4e2d\u6587\u535a\u5ba2 ,\u63d0\u5230\u4e86\u672c\u6587\u51e0\u4e2a\u91cd\u8981\u7684\u6709\u8da3\u7684\u7ec6\u8282\uff0c\u7b2c\u4e00\u4e2a\u662f3D\u4e16\u754c\u7684Fast Voxel Traversal\u751f\u6210occupancy map;\u7b2c\u4e8c\u4e2a\u662f\u6570\u636e\u589e\u5f3a\uff0c\u91c7\u53d6\u7684\u65b9\u6cd5\u4e0eoccupancy map\u8fdb\u884c\u589e\u5f3a\uff1b\u7b2c\u4e09\u4e2a\u662fonline bayesian grid mapping.","title":"What You See is What You Get: Exploiting Visibility for 3D Object Detection"},{"location":"other_categories/Summaries/Summary_of_serveral_cvpr2020/#instance-shadow-detection","text":"pdf code \u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u4efb\u52a1\uff0c\u65b0\u7684dataset\u4ee5\u53cabaseline\u65b9\u6cd5\u3002\u4efb\u52a1\u662f\u7269\u4f53\u4e0e\u5f71\u5b50\u7684instance segmentation\u4ee5\u53ca\u4e00\u4e00\u5bf9\u5e94\u3002","title":"Instance Shadow Detection"},{"location":"other_categories/Summaries/Summary_of_serveral_cvpr2020/#a-model-driven-deep-neural-network-for-single-image-rain-removal","text":"pdf code","title":"A Model-driven Deep Neural Network for Single Image Rain Removal"},{"location":"other_categories/Summaries/Summary_of_serveral_cvpr2020/#single-image-optical-flow-estimation-with-an-event-camera","text":"pdf \u672c\u6587\u662f\u57fa\u4e8eDAVIS\u7684event + gray scale\u8bbe\u8ba1\u7684\u7b97\u6cd5\uff0c\u4e00\u4e2a\u91cd\u8981\u7684idea\u662fevent\u4fe1\u606f\u672c\u8eab\u53ef\u4ee5\u76f4\u63a5\u5b58\u50a8\u5149\u6d41\u76f8\u5173\u7684\u4fe1\u606f\uff0c\u800c\u7070\u5ea6\u56fe\u7684\u52a8\u6001\u6a21\u7cca\u4e5f\u53ef\u4ee5\u7528\u4e8e\u6307\u5f15\u5149\u6d41\u4f30\u8ba1\u3002\u56e0\u6b64\u672c\u6587\u63d0\u51fa\u540c\u65f6\u4f30\u8ba1\u5149\u6d41\u573a\u4ee5\u53calatent image \\mathcal{L} . \u80fd\u91cf\u51fd\u6570\u8bbe\u8ba1\u4e3a \\min _{\\mathbf{L} . \\mathbf{u}} \\mu_{1} \\phi_{\\mathrm{eve}}(\\mathbf{L}, \\mathbf{u})+\\mu_{2} \\phi_{\\mathrm{blur}}(\\mathbf{L}, \\mathbf{u})+\\phi_{\\mathrm{flow}}(\\nabla \\mathbf{u})+\\phi_{\\mathrm{im}}(\\nabla \\mathbf{L}) \u9996\u5148\u662f\u8003\u8651\u4e86\u5149\u7167\u53d8\u5316\u7684\u5149\u6d41-\u5149\u7167\u4e00\u81f4\u6761\u4ef6 \\begin{aligned} \\phi_{\\mathrm{eve}}(\\mathbf{L}, \\mathbf{u})=\\sum_{\\mathbf{x} \\in \\Omega} \\| &\\mathbf{L}(\\mathbf{x}, f)(\\exp (c \\mathbf{E}(\\mathbf{x}, t))-1) \\\\ &+\\left[u_{\\mathbf{x}}, v_{\\mathbf{x}}\\right]^{\\mathrm{T}} \\nabla \\mathbf{L}(\\mathbf{x}, f) \\|_{1} \\end{aligned} \u6a21\u7cca: \u6a21\u7cca\u540e\u7684\u56fe\u7247 B \u53ef\u4ee5\u7531\u6a21\u7cca\u6838 K \u4ee5\u53calatent \u56fe\u7247 L \u5377\u79ef\u8868\u8fbe: \\begin{aligned} \\mathbf{B}(\\mathbf{x}) &=\\sum_{\\mathbf{y} \\in \\Omega} \\mathbf{k}(\\mathbf{y}) \\mathbf{L}(\\mathbf{x}-\\mathbf{y}) \\\\ &=\\sum_{\\mathbf{y} \\in \\Omega} \\mathbf{k}_{\\mathbf{u}^{\\prime}(\\mathbf{x})}(\\mathbf{y}) \\mathbf{L}(\\mathbf{x}-\\mathbf{y}) \\end{aligned} k_{\\mathbf{u}^{\\prime}(\\mathbf{x})}(\\mathbf{y})=\\left\\{\\begin{array}{ll} \\frac{1}{\\left|\\mathbf{u}^{\\prime}(\\mathbf{x})\\right|}, & \\text { if } \\mathbf{y}=\\alpha \\mathbf{u}^{\\prime}(\\mathbf{x}),|\\alpha| \\leq \\frac{1}{2} \\\\ \\mathbf{0}, & \\text { otherwise } \\end{array}\\right. \u6a21\u7cca\u6761\u4ef6: \\phi_{\\text {blur }}(\\mathbf{L}, \\mathbf{u})=\\sum_{\\mathbf{x}, \\mathbf{y} \\in \\Omega}\\left\\|\\mathbf{k}_{\\mathbf{u}^{\\prime}(\\mathbf{x})}(\\mathbf{y}) \\mathbf{L}(\\mathbf{x}-\\mathbf{y})-\\mathbf{B}(\\mathbf{x})\\right\\|^{2} \u540e\u4e24\u9879\u4e3a\u8fde\u7eed\u6027\u8981\u6c42\uff0c\u6587\u4e2d\u7684\u8bbe\u8ba1\u6bd4\u8f83\u7cbe\u7ec6\u3002 \u4f18\u5316\u65b9\u6cd5\u4e0a\u672c\u6587\u8fed\u4ee3\u8fdb\u884c\u5149\u6d41\u4f30\u8ba1\u4ee5\u53ca\u56fe\u7247\u7684deblur","title":"Single Image Optical Flow Estimation with an Event Camera"},{"location":"other_categories/Summaries/Summary_of_serveral_cvpr2020/#pi-nets-deep-polynomial-neural-networks","text":"pdf \u53ef\u5b66\u4e60\u53c2\u6570\u7684\u591a\u9879\u5f0f\u8ba1\u7b97\u6a21\u5757(\u6700\u7ec8\u8f93\u51fa\u4e3a\u8f93\u5165\u7684\u591a\u9879\u5f0f\u8868\u8fbe)","title":"\\Pi - nets: Deep Polynomial Neural Networks"},{"location":"other_categories/Summaries/Summary_of_serveral_eccv2020/","text":"Summaries for several ECCV 2020 papers \u76ee\u5f55: Summaries for several ECCV 2020 papers Deep Hough Transform for Semantic Line Detection \u6574\u4f53\u63a8\u7406\u7ed3\u6784 DHT \u8fd0\u7b97\u793a\u610f\u56fe EA Score \\rightarrow Euclidean distance and Angular distance PIoU Loss: Towards Accurate Oriented Object Detection in Complex Environments Learning Stereo from Single Images Occlusion and Collisions Aware Forward Warping Depth Sharpening Attentive Normalization TIDE: A General Toolbox for Identifying Object Detection Errors Arbitrary-Oriented Object Detection with Circular Smooth Label RAFT: Recurrent All-Pairs Field Transforms for Optical Flow Deep Hough Transform for Semantic Line Detection pdf code \u672c\u6587\u7684\u4e3b\u8981\u8d21\u732e: DHT \u64cd\u4f5c\uff0c\u5bf9\u6574\u4e2afeature map\u64cd\u4f5c,\u4f5c\u8005\u63d0\u4f9b\u4e86cuda \u52a0\u901f\u7684 code .\u5c06\u76f4\u7ebf\u7279\u5f81\u8f6c\u6362\u4e3a\u70b9\u7279\u5f81 \u63d0\u51faEA score \u66ff\u4ee3IoU\u7528\u4e8e\u8bc4\u4ef7\u76f4\u7ebf\u7684detection. (Segmentation\u7684IoU\u5bf9\u76f4\u7ebf\u7279\u5f81\u7684\u8bc4\u4ef7\u6bd4\u8f83\u7c97\u7cd9) \u4e0eSegmentation\u68c0\u6d4b\u76f4\u7ebf\u7684\u533a\u522b: \u53ea\u80fd\u7ed9\u51fa\u7ebf\u7684\u65b9\u7a0b\uff0c\u82e5\u9700\u8981\u68c0\u6d4b\u7684\u662f\u7ebf\u6bb5\uff0c\u672c\u6587\u7ed9\u51fa\u7684\u65b9\u6848\u4f3c\u4e4e\u6ca1\u6709\u529e\u6cd5\u5904\u7406. \u6574\u4f53\u63a8\u7406\u7ed3\u6784 DHT \u8fd0\u7b97\u793a\u610f\u56fe EA Score \\rightarrow Euclidean distance and Angular distance \u89d2\u5ea6\u8ddd\u79bb,\u57fa\u4e8e\u4e0e\u6c34\u5e73\u65b9\u5411\u7684\u5939\u89d2\u7684\u5dee: \\mathcal{S}_\\theta = 1 - \\frac{\\theta(l_i, l_j)}{\\pi / 2} \u6b27\u6c0f\u8ddd\u79bb,\u57fa\u4e8e\u76f4\u7ebf\u5728\u56fe\u4e0a\u7684\u4e2d\u70b9\u7684 normalized \u8ddd\u79bb (\u5148\u5c06\u56fe\u7247\u5f52\u4e00\u5316\u4e3a\u4e00\u4e2a\u5355\u4f4d\u65b9\u5757) \\mathcal{S}_d = 1 - D(l_i, l_j) EA Score: \\mathcal{S} = (\\mathcal{S}_\\theta \\cdot \\mathcal{S}_d) ^ 2 PIoU Loss: Towards Accurate Oriented Object Detection in Complex Environments pdf code \u672c\u6587\u7684\u4e3b\u8981\u8d21\u732e\u662f\u63d0\u51fa\u4e86PIoU (pixel IoU) Loss,\u4e00\u4e2a\u57fa\u4e8eIoU\u7684\uff0c\u9488\u5bf9\u65cb\u8f6cbounding box (OBB)\u7684\uff0c \u53ef\u5fae\u5206\u7684\uff0c\u66f4\u9ad8\u6548\u7387\u7684\u635f\u5931\u51fd\u6570 \u4e0eRotated IoU Loss\u76f8\u6bd4\uff0c\u8fd0\u7b97\u901f\u5ea6\u66f4\u5feb \u4e0e\u76ee\u524d\u66f4\u5e38\u7528\u7684L1 Loss\u76f8\u6bd4\uff0c\u56de\u5f52\u65cb\u8f6c bounding box\u7684\u6027\u80fd\u66f4\u597d\uff0c\u4e5f\u4e0e\u6700\u7ec8\u8bc4\u5224\u6307\u6807IoU\u66f4\u76f8\u8fd1\u3002 \u63d0\u4f9b\u4e86piou\u7684cuda\u52a0\u901f \u4ee3\u7801 \u7ed3\u5408\u4e0a\u56fe\uff0c\u672c\u6587\u7684\u53d1\u5c55\u601d\u8def: PIoU \u5b9e\u8d28\u76f8\u5f53\u4e8e\u6570\u51fa\u4e24\u4e2a\u65cb\u8f6cbounding box\u76f8\u4ea4\u90e8\u5206\u6709\u591a\u5c11\u4e2apixel\uff0c\u7ed3\u5408box\u7684\u5927\u5c0f\u53ef\u4ee5\u5f97\u5230pixel \u4e0a\u7684IoU. \u5982\u4f55\u5feb\u901f\u5730\u5224\u65ad\u4e00\u4e2apixel\u662f\u5426\u5728bounding box\u5185? \\rightarrow piou\u7b97\u6cd5 \u5224\u65ad\u64cd\u4f5c\u662f\u4e0d\u53ef\u5bfc\u7684\uff0c\u5982\u4f55\u8f6f\u5316(soften)\u8fd9\u4e2a\u5224\u65ad\u64cd\u4f5c\u4f7f\u5f97PIoU \u5bf9\u9884\u6d4b\u6846\u7684\u4e2d\u5fc3\u70b9\u4f4d\u7f6e,\u957f\u5bbd\uff0c\u8f6c\u89d2\u53ef\u5bfc? \u7531 (a)\u53ef\u77e5\u5224\u65ad\u7b97\u6cd5\u4e3a: \\delta\\left(\\boldsymbol{p}_{i, j} \\mid \\boldsymbol{b}\\right)=\\left\\{\\begin{array}{ll} 1, & d_{i, j}^{w} \\leq \\frac{w}{2}, d_{i, j}^{h} \\leq \\frac{h}{2} \\\\ 0, & \\text { otherwise } \\end{array}\\right. \\begin{aligned} d_{i j} &=d(i, j)=\\sqrt{\\left(c_{x}-i\\right)^{2}+\\left(c_{y}-j\\right)^{2}} \\\\ d_{i j}^{w} &=\\left|d_{i j} \\cos \\beta\\right| \\\\ d_{i j}^{h} &=\\left|d_{i j} \\sin \\beta\\right| \\\\ \\beta &=\\left\\{\\begin{array}{ll} \\theta+\\arccos \\frac{c_{x}-i}{d_{i j}}, & c_{y}-j \\geq 0 \\\\ \\theta-\\arccos \\frac{c_{x}-i}{d_{i j}}, & c_{y}-j<0 \\end{array}\\right. \\end{aligned} \u4f7f\u7528\u4e00\u4e2a 1 - sigmoid \u7684\u51fd\u6570\u8f6f\u5316\u8fd9\u4e2a\u903b\u8f91\u5224\u65ad. K(d, s) = 1 - \\frac{1}{1 + e^{-k(d-s)}} F\\left(\\boldsymbol{p}_{i, j} \\mid \\boldsymbol{b}\\right)=K\\left(d_{i, j}^{w}, w\\right) K\\left(d_{i, j}^{h}, h\\right) S_{\\boldsymbol{b} \\cap \\boldsymbol{b}^{\\prime}} \\approx \\sum_{\\boldsymbol{p}_{i, j} \\in B_{\\boldsymbol{b}, b^{\\prime}}} F\\left(\\boldsymbol{p}_{i, j} \\mid \\boldsymbol{b}\\right) F\\left(\\boldsymbol{p}_{i, j} \\mid \\boldsymbol{b}^{\\prime}\\right) Learning Stereo from Single Images pdf code \u8fd9\u7bc7paper\u505a\u4e86\u4e00\u4e2a\u6bd4\u8f83\u795e\u5947\u7684\u4efb\u52a1\uff0c\u4e5f\u5c31\u662f\u4ece\u5355\u76ee\u56fe\u7247\u751f\u6210\u53cc\u76ee\u8bad\u7ec3\u96c6 \u7b80\u5355\u6765\u8bf4\u5c31\u662f \u5355\u76ee\u56fe\u7247 \\underrightarrow{CNN \u5355\u76ee\u6df1\u5ea6\u751f\u6210} RGBD\u56fe\u7247 \\underrightarrow{\u865a\u62dfbaseline} \u89c6\u5dee\u56fe \\underrightarrow{\u91c7\u6837\u4e0e\u56fe\u7247\u751f\u6210} \u53cc\u76ee\u6570\u636e\u3002 \u4f5c\u8005\u4f7f\u7528\u8fd9\u6837\u751f\u6210\u7684\u53cc\u76ee\u4f5c\u4e3a\u8bad\u7ec3\u6570\u636e\u5f97\u5230\u4e86\u76f8\u5f53\u597d\u7684\u53cc\u76ee\u5339\u914d\u6027\u80fd\u3002 \u6838\u5fc3\u65b0\u7684\u5185\u5bb9\u5728\u4e0e disparity sharpening\u8fd8\u6709forward warping\u8fd9\u4e24\u6b65 Occlusion and Collisions Aware Forward Warping \u76f4\u63a5\u5730\u91c7\u6837\u4f1a\u5bfc\u81f4\u4e24\u4e2a\u95ee\u9898\uff0c\u4e00\u4e2a\u662f\u56e0\u4e3a\u906e\u6321\u4ea7\u751f\u7684\u5b54(\u65b0\u89c6\u89d2\u53ef\u89c2\u5bdf\uff0c\u65e7\u89c6\u89d2\u4e0d\u53ef\u89c2\u5bdf\u7684\u5185\u5bb9)\uff0c\u4e00\u4e2a\u662f\u78b0\u649e(\u65e7\u89c6\u89d2\u53ef\u89c2\u5bdf\uff0c\u65b0\u89c6\u89d2\u4e0e\u5176\u4ed6\u50cf\u7d20\u91cd\u5408) \u591a\u4e2a\u50cf\u7d20\u5bf9\u5e94\u4e00\u4e2a\u50cf\u7d20(\u78b0\u649e)\u65f6\uff0c\u9009\u53d6\u4fdd\u7559\u89c6\u5dee\u8f83\u5927\u7684\u70b9. \u5bf9\u4e8e\u7a7a\u767d\u7684\u533a\u57df,\u672c\u6587\u63d0\u51fa\u4ece\u6570\u636e\u5e93\u4e2d\u968f\u673a\u9009\u53d6\u53e6\u4e00\u5f20\u56fe I_b \uff0c\u4f7f\u7528 color transfer \u7b97\u6cd5\u5c06 I_b \u7684\u989c\u8272\u98ce\u683c\u8f6c\u6362\u4e3a I_l \u7684\u3002\u7136\u540e\u586b\u8865\u4e0a\u7a7a\u767d\u7684\u90e8\u5206.\u586b\u8865\u7684\u65b9\u5f0f\u6309\u7167 Cut, Paste and Learn . color transfer \u7684\u505a\u6cd5\u662f\u5c06RGB\u56fe\u7247\u8f6c\u53d8\u4e3a CIE Lab \u683c\u5f0f\uff0c\u5c06Source \u56fe\u5404\u4e2aChannel\u7684\u5747\u503c\u4e0e\u65b9\u5dee\u8f6c\u6362\u4e3aTarget \u56fe\u7684\u5373\u53ef\u3002 Cut, Paste and Learn \u7684\u5728\u586b\u8865\u7684\u65f6\u5019\u4f7f\u7528\u7684\u662f Gaussian\u4ee5\u53caPoisson Blending. \u5176\u4e2d Poisson Blending , \u662f\u5c06\u586b\u8865\u8fc7\u7a0b\u7406\u89e3\u4e3a\u4e00\u4e2a\u5e26\u6709\u72c4\u5229\u514b\u96f7\u8fb9\u754c\u6761\u4ef6\u7684\u6cca\u677e\u504f\u5fae\u5206\u65b9\u7a0b\uff0c\u8fd9\u4e2a\u505a\u6cd5\u7684intuition\u4ee5\u53ca\u8bc1\u660e\u53ef\u4ee5\u5728 \u4e00\u4e2alecture ppt \u4e2d\u627e\u5230. Depth Sharpening \u4f5c\u8005\u4e5f\u6307\u51fa\u4e86\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u5f88\u5bb9\u6613\u5728\u7269\u4f53\u7684\u8fb9\u7f18\u751f\u6210\u8fc7\u6e21\u5f62\u6001\u7684\u70b9\uff0c\u548c CDN \u6307\u51fa\u7684\u73b0\u8c61\u76f8\u540c\u3002\u672c\u6587\u4f5c\u8005\u7684\u601d\u8def\u5219\u662f\u5bf9\u4e8e Sobel edge filter\u76f8\u5e94\u5927\u4e8e\u4e09\u7684\u70b9\u6807\u8bb0\u4e3a\"flying pixels\"\uff0c\u5c06\u5176disparity\u8d4b\u503c\u4e3a\u79bb\u5b83\u6700\u8fd1\u7684\"non-flying\" pixel. Attentive Normalization pdf code \u8fd9\u7bc7paper\u63d0\u51fa\u4e86\u4e00\u4e2aNormalization + SE\u7684\u878d\u5408 class hsigmoid(nn.Module): \"\"\" Hard Linear Sigmoid \"\"\" def forward(self, x): out = F.relu6(x + 3, inplace=True) / 6 return out class AttentionWeights(nn.Module): def __init__(self, num_channels, k, attention_mode=0): super(AttentionWeights, self).__init__() self.k = k self.avgpool = nn.AdaptiveAvgPool2d(1) layers = [] if attention_mode == 0: layers = [ nn.Conv2d(num_channels, k, 1), hsigmoid() ] elif attention_mode == 2: layers = [ nn.Conv2d(num_channels, k, 1, bias=False), hsigmoid() ] else: raise NotImplementedError(\"Unknow attention weight type\") self.attention = nn.Sequential(*layers) def forward(self, x): b, c, _, _ = x.size() y = self.avgpool(x)#.view(b, c) var = torch.var(x, dim=(2, 3)).view(b, c, 1, 1) y *= (var + 1e-3).rsqrt() #y = torch.cat((y, var), dim=1) return self.attention(y).view(b, self.k) class AttentiveGroupNorm(nn.GroupNorm): def __init__(self, num_groups, num_channels, k, eps=1e-5): super(AttentiveGroupNorm, self).__init__(num_groups, num_channels, eps=eps, affine=False) self.k = k self.weight_ = nn.Parameter(torch.Tensor(k, num_channels)) self.bias_ = nn.Parameter(torch.Tensor(k, num_channels)) self.attention_weights = AttentionWeights(num_channels, k) self._init_params() def _init_params(self): nn.init.normal_(self.weight_, 1, 0.1) nn.init.normal_(self.bias_, 0, 0.1) def forward(self, input): output = super(AttentiveGroupNorm, self).forward(input) size = output.size() y = self.attention_weights(input) weight = y @ self.weight_ bias = y @ self.bias_ weight = weight.unsqueeze(-1).unsqueeze(-1).expand(size) bias = bias.unsqueeze(-1).unsqueeze(-1).expand(size) return weight * output + bias TIDE: A General Toolbox for Identifying Object Detection Errors pdf code \u8fd9\u7bc7paper\u7ed9\u4e86\u4e00\u4e2aPython\u5de5\u5177\u7bb1, \u66f4\u4e3a\u8be6\u7ec6\u5730\u53bb\u5206\u6790object detection\u7684error, Arbitrary-Oriented Object Detection with Circular Smooth Label pdf code \u8fd9\u7bc7paper review\u4e86\u56de\u5f52\u65cb\u8f6c bounding boxes\u7684\u65b9\u6cd5\uff0c\u540c\u65f6\u6307\u51fa\u4e86\u76ee\u524d\u51e0\u4e2a\u56de\u5f52\u53c2\u6570\u5316\u65b9\u6cd5\u7684\u5f0a\u7aef\uff0c\u63d0\u51fa\u7684\u65b9\u6848\u4e3b\u8981\u662f\u89e3\u51b3\u4e86\u89d2\u5ea6\u7684\u4e0d\u8fde\u7eed\u6027\u95ee\u9898. \u4e09\u79cd\u5df2\u6709\u7684\u53c2\u6570\u5316\u56de\u5f52\u65cb\u8f6cbounding boxe\u7684\u65b9\u6cd5: \u5728\u67d0\u4e2a\u7279\u6b8a\u60c5\u51b5\u4e0b\uff0c\u4ee5\u4e0a\u4e09\u79cd\u53c2\u6570\u65b9\u6cd5\u75c5\u6001\u7684\u56de\u5f52\u8fc7\u7a0b: \u4f5c\u8005\u63d0\u51fa\u5728\u89d2\u5ea6\u4e0a\u91c7\u7528\u7a97\u51fd\u6570\u5e73\u6ed1\u540e\u7684\u3001\u57fa\u4e8e\u5206\u7c7b\u7684\u8bad\u7ec3\u65b9\u6848 RAFT: Recurrent All-Pairs Field Transforms for Optical Flow pdf code \u8fd9\u7bc7paper\u662feccv2020\u7684best paper.\u5bf9\u4e8e\u5149\u6d41\u7684\u8ba1\u7b97\u6709\u6bd4\u8f83\u5927\u7684\u521b\u65b0\u3002\u4e3b\u8981\u5b9e\u73b0\u7684\u662f\u5728\u9ad8\u5206\u8fa8\u7387\u4e0b\u5bf9\u5927\u5c3a\u5ea6\u5149\u6d41\u641c\u7d22\u7684\u4f18\u5316\u65b9\u6848\u3002 \u6838\u5fc3\u7684\u521b\u65b0\u5728\u4e8e\u5728scale 8 \u9884\u8ba1\u7b97\u4e24\u5e27\u4e4b\u95f4\u6bcf\u4e24\u4e2a\u50cf\u7d20\u4e4b\u95f4\u7684\u5339\u914dscore(dot product),\u5728\u8fed\u4ee3\u4f18\u5316\u7684\u8fc7\u7a0b\u4e2d\"LookUp\" Operator\u8d1f\u8d23\u5c06sample\u5149\u6d41\u5bf9\u5e94\u4f4d\u7f6e\u7684cost. \u907f\u514d\u4e86\u591a\u6b21\u91cd\u590d\u8ba1\u7b97cost volume. \u4f7f\u5f97\u5728scale 8\u4e0a\u641c\u7d22\u957f\u8ddd\u79bb\u7684\u5149\u6d41\u6210\u4e3a\u53ef\u80fd\uff0c\u5bf9\u5feb\u901f\u79fb\u52a8\u7684\u5c0f\u7269\u4f53\u7684\u68c0\u6d4b\u80fd\u529b\u53d8\u5f3a\u3002","title":"Summaries for several ECCV 2020 papers"},{"location":"other_categories/Summaries/Summary_of_serveral_eccv2020/#summaries-for-several-eccv-2020-papers","text":"\u76ee\u5f55: Summaries for several ECCV 2020 papers Deep Hough Transform for Semantic Line Detection \u6574\u4f53\u63a8\u7406\u7ed3\u6784 DHT \u8fd0\u7b97\u793a\u610f\u56fe EA Score \\rightarrow Euclidean distance and Angular distance PIoU Loss: Towards Accurate Oriented Object Detection in Complex Environments Learning Stereo from Single Images Occlusion and Collisions Aware Forward Warping Depth Sharpening Attentive Normalization TIDE: A General Toolbox for Identifying Object Detection Errors Arbitrary-Oriented Object Detection with Circular Smooth Label RAFT: Recurrent All-Pairs Field Transforms for Optical Flow","title":"Summaries for several ECCV 2020 papers"},{"location":"other_categories/Summaries/Summary_of_serveral_eccv2020/#deep-hough-transform-for-semantic-line-detection","text":"pdf code \u672c\u6587\u7684\u4e3b\u8981\u8d21\u732e: DHT \u64cd\u4f5c\uff0c\u5bf9\u6574\u4e2afeature map\u64cd\u4f5c,\u4f5c\u8005\u63d0\u4f9b\u4e86cuda \u52a0\u901f\u7684 code .\u5c06\u76f4\u7ebf\u7279\u5f81\u8f6c\u6362\u4e3a\u70b9\u7279\u5f81 \u63d0\u51faEA score \u66ff\u4ee3IoU\u7528\u4e8e\u8bc4\u4ef7\u76f4\u7ebf\u7684detection. (Segmentation\u7684IoU\u5bf9\u76f4\u7ebf\u7279\u5f81\u7684\u8bc4\u4ef7\u6bd4\u8f83\u7c97\u7cd9) \u4e0eSegmentation\u68c0\u6d4b\u76f4\u7ebf\u7684\u533a\u522b: \u53ea\u80fd\u7ed9\u51fa\u7ebf\u7684\u65b9\u7a0b\uff0c\u82e5\u9700\u8981\u68c0\u6d4b\u7684\u662f\u7ebf\u6bb5\uff0c\u672c\u6587\u7ed9\u51fa\u7684\u65b9\u6848\u4f3c\u4e4e\u6ca1\u6709\u529e\u6cd5\u5904\u7406.","title":"Deep Hough Transform for Semantic Line Detection"},{"location":"other_categories/Summaries/Summary_of_serveral_eccv2020/#_1","text":"","title":"\u6574\u4f53\u63a8\u7406\u7ed3\u6784"},{"location":"other_categories/Summaries/Summary_of_serveral_eccv2020/#dht","text":"","title":"DHT \u8fd0\u7b97\u793a\u610f\u56fe"},{"location":"other_categories/Summaries/Summary_of_serveral_eccv2020/#ea-score-rightarrow-euclidean-distance-and-angular-distance","text":"\u89d2\u5ea6\u8ddd\u79bb,\u57fa\u4e8e\u4e0e\u6c34\u5e73\u65b9\u5411\u7684\u5939\u89d2\u7684\u5dee: \\mathcal{S}_\\theta = 1 - \\frac{\\theta(l_i, l_j)}{\\pi / 2} \u6b27\u6c0f\u8ddd\u79bb,\u57fa\u4e8e\u76f4\u7ebf\u5728\u56fe\u4e0a\u7684\u4e2d\u70b9\u7684 normalized \u8ddd\u79bb (\u5148\u5c06\u56fe\u7247\u5f52\u4e00\u5316\u4e3a\u4e00\u4e2a\u5355\u4f4d\u65b9\u5757) \\mathcal{S}_d = 1 - D(l_i, l_j) EA Score: \\mathcal{S} = (\\mathcal{S}_\\theta \\cdot \\mathcal{S}_d) ^ 2","title":"EA Score \\rightarrow Euclidean distance and Angular distance"},{"location":"other_categories/Summaries/Summary_of_serveral_eccv2020/#piou-loss-towards-accurate-oriented-object-detection-in-complex-environments","text":"pdf code \u672c\u6587\u7684\u4e3b\u8981\u8d21\u732e\u662f\u63d0\u51fa\u4e86PIoU (pixel IoU) Loss,\u4e00\u4e2a\u57fa\u4e8eIoU\u7684\uff0c\u9488\u5bf9\u65cb\u8f6cbounding box (OBB)\u7684\uff0c \u53ef\u5fae\u5206\u7684\uff0c\u66f4\u9ad8\u6548\u7387\u7684\u635f\u5931\u51fd\u6570 \u4e0eRotated IoU Loss\u76f8\u6bd4\uff0c\u8fd0\u7b97\u901f\u5ea6\u66f4\u5feb \u4e0e\u76ee\u524d\u66f4\u5e38\u7528\u7684L1 Loss\u76f8\u6bd4\uff0c\u56de\u5f52\u65cb\u8f6c bounding box\u7684\u6027\u80fd\u66f4\u597d\uff0c\u4e5f\u4e0e\u6700\u7ec8\u8bc4\u5224\u6307\u6807IoU\u66f4\u76f8\u8fd1\u3002 \u63d0\u4f9b\u4e86piou\u7684cuda\u52a0\u901f \u4ee3\u7801 \u7ed3\u5408\u4e0a\u56fe\uff0c\u672c\u6587\u7684\u53d1\u5c55\u601d\u8def: PIoU \u5b9e\u8d28\u76f8\u5f53\u4e8e\u6570\u51fa\u4e24\u4e2a\u65cb\u8f6cbounding box\u76f8\u4ea4\u90e8\u5206\u6709\u591a\u5c11\u4e2apixel\uff0c\u7ed3\u5408box\u7684\u5927\u5c0f\u53ef\u4ee5\u5f97\u5230pixel \u4e0a\u7684IoU. \u5982\u4f55\u5feb\u901f\u5730\u5224\u65ad\u4e00\u4e2apixel\u662f\u5426\u5728bounding box\u5185? \\rightarrow piou\u7b97\u6cd5 \u5224\u65ad\u64cd\u4f5c\u662f\u4e0d\u53ef\u5bfc\u7684\uff0c\u5982\u4f55\u8f6f\u5316(soften)\u8fd9\u4e2a\u5224\u65ad\u64cd\u4f5c\u4f7f\u5f97PIoU \u5bf9\u9884\u6d4b\u6846\u7684\u4e2d\u5fc3\u70b9\u4f4d\u7f6e,\u957f\u5bbd\uff0c\u8f6c\u89d2\u53ef\u5bfc? \u7531 (a)\u53ef\u77e5\u5224\u65ad\u7b97\u6cd5\u4e3a: \\delta\\left(\\boldsymbol{p}_{i, j} \\mid \\boldsymbol{b}\\right)=\\left\\{\\begin{array}{ll} 1, & d_{i, j}^{w} \\leq \\frac{w}{2}, d_{i, j}^{h} \\leq \\frac{h}{2} \\\\ 0, & \\text { otherwise } \\end{array}\\right. \\begin{aligned} d_{i j} &=d(i, j)=\\sqrt{\\left(c_{x}-i\\right)^{2}+\\left(c_{y}-j\\right)^{2}} \\\\ d_{i j}^{w} &=\\left|d_{i j} \\cos \\beta\\right| \\\\ d_{i j}^{h} &=\\left|d_{i j} \\sin \\beta\\right| \\\\ \\beta &=\\left\\{\\begin{array}{ll} \\theta+\\arccos \\frac{c_{x}-i}{d_{i j}}, & c_{y}-j \\geq 0 \\\\ \\theta-\\arccos \\frac{c_{x}-i}{d_{i j}}, & c_{y}-j<0 \\end{array}\\right. \\end{aligned} \u4f7f\u7528\u4e00\u4e2a 1 - sigmoid \u7684\u51fd\u6570\u8f6f\u5316\u8fd9\u4e2a\u903b\u8f91\u5224\u65ad. K(d, s) = 1 - \\frac{1}{1 + e^{-k(d-s)}} F\\left(\\boldsymbol{p}_{i, j} \\mid \\boldsymbol{b}\\right)=K\\left(d_{i, j}^{w}, w\\right) K\\left(d_{i, j}^{h}, h\\right) S_{\\boldsymbol{b} \\cap \\boldsymbol{b}^{\\prime}} \\approx \\sum_{\\boldsymbol{p}_{i, j} \\in B_{\\boldsymbol{b}, b^{\\prime}}} F\\left(\\boldsymbol{p}_{i, j} \\mid \\boldsymbol{b}\\right) F\\left(\\boldsymbol{p}_{i, j} \\mid \\boldsymbol{b}^{\\prime}\\right)","title":"PIoU Loss: Towards Accurate Oriented Object Detection in Complex Environments"},{"location":"other_categories/Summaries/Summary_of_serveral_eccv2020/#learning-stereo-from-single-images","text":"pdf code \u8fd9\u7bc7paper\u505a\u4e86\u4e00\u4e2a\u6bd4\u8f83\u795e\u5947\u7684\u4efb\u52a1\uff0c\u4e5f\u5c31\u662f\u4ece\u5355\u76ee\u56fe\u7247\u751f\u6210\u53cc\u76ee\u8bad\u7ec3\u96c6 \u7b80\u5355\u6765\u8bf4\u5c31\u662f \u5355\u76ee\u56fe\u7247 \\underrightarrow{CNN \u5355\u76ee\u6df1\u5ea6\u751f\u6210} RGBD\u56fe\u7247 \\underrightarrow{\u865a\u62dfbaseline} \u89c6\u5dee\u56fe \\underrightarrow{\u91c7\u6837\u4e0e\u56fe\u7247\u751f\u6210} \u53cc\u76ee\u6570\u636e\u3002 \u4f5c\u8005\u4f7f\u7528\u8fd9\u6837\u751f\u6210\u7684\u53cc\u76ee\u4f5c\u4e3a\u8bad\u7ec3\u6570\u636e\u5f97\u5230\u4e86\u76f8\u5f53\u597d\u7684\u53cc\u76ee\u5339\u914d\u6027\u80fd\u3002 \u6838\u5fc3\u65b0\u7684\u5185\u5bb9\u5728\u4e0e disparity sharpening\u8fd8\u6709forward warping\u8fd9\u4e24\u6b65","title":"Learning Stereo from Single Images"},{"location":"other_categories/Summaries/Summary_of_serveral_eccv2020/#occlusion-and-collisions-aware-forward-warping","text":"\u76f4\u63a5\u5730\u91c7\u6837\u4f1a\u5bfc\u81f4\u4e24\u4e2a\u95ee\u9898\uff0c\u4e00\u4e2a\u662f\u56e0\u4e3a\u906e\u6321\u4ea7\u751f\u7684\u5b54(\u65b0\u89c6\u89d2\u53ef\u89c2\u5bdf\uff0c\u65e7\u89c6\u89d2\u4e0d\u53ef\u89c2\u5bdf\u7684\u5185\u5bb9)\uff0c\u4e00\u4e2a\u662f\u78b0\u649e(\u65e7\u89c6\u89d2\u53ef\u89c2\u5bdf\uff0c\u65b0\u89c6\u89d2\u4e0e\u5176\u4ed6\u50cf\u7d20\u91cd\u5408) \u591a\u4e2a\u50cf\u7d20\u5bf9\u5e94\u4e00\u4e2a\u50cf\u7d20(\u78b0\u649e)\u65f6\uff0c\u9009\u53d6\u4fdd\u7559\u89c6\u5dee\u8f83\u5927\u7684\u70b9. \u5bf9\u4e8e\u7a7a\u767d\u7684\u533a\u57df,\u672c\u6587\u63d0\u51fa\u4ece\u6570\u636e\u5e93\u4e2d\u968f\u673a\u9009\u53d6\u53e6\u4e00\u5f20\u56fe I_b \uff0c\u4f7f\u7528 color transfer \u7b97\u6cd5\u5c06 I_b \u7684\u989c\u8272\u98ce\u683c\u8f6c\u6362\u4e3a I_l \u7684\u3002\u7136\u540e\u586b\u8865\u4e0a\u7a7a\u767d\u7684\u90e8\u5206.\u586b\u8865\u7684\u65b9\u5f0f\u6309\u7167 Cut, Paste and Learn . color transfer \u7684\u505a\u6cd5\u662f\u5c06RGB\u56fe\u7247\u8f6c\u53d8\u4e3a CIE Lab \u683c\u5f0f\uff0c\u5c06Source \u56fe\u5404\u4e2aChannel\u7684\u5747\u503c\u4e0e\u65b9\u5dee\u8f6c\u6362\u4e3aTarget \u56fe\u7684\u5373\u53ef\u3002 Cut, Paste and Learn \u7684\u5728\u586b\u8865\u7684\u65f6\u5019\u4f7f\u7528\u7684\u662f Gaussian\u4ee5\u53caPoisson Blending. \u5176\u4e2d Poisson Blending , \u662f\u5c06\u586b\u8865\u8fc7\u7a0b\u7406\u89e3\u4e3a\u4e00\u4e2a\u5e26\u6709\u72c4\u5229\u514b\u96f7\u8fb9\u754c\u6761\u4ef6\u7684\u6cca\u677e\u504f\u5fae\u5206\u65b9\u7a0b\uff0c\u8fd9\u4e2a\u505a\u6cd5\u7684intuition\u4ee5\u53ca\u8bc1\u660e\u53ef\u4ee5\u5728 \u4e00\u4e2alecture ppt \u4e2d\u627e\u5230.","title":"Occlusion and Collisions Aware Forward Warping"},{"location":"other_categories/Summaries/Summary_of_serveral_eccv2020/#depth-sharpening","text":"\u4f5c\u8005\u4e5f\u6307\u51fa\u4e86\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u5f88\u5bb9\u6613\u5728\u7269\u4f53\u7684\u8fb9\u7f18\u751f\u6210\u8fc7\u6e21\u5f62\u6001\u7684\u70b9\uff0c\u548c CDN \u6307\u51fa\u7684\u73b0\u8c61\u76f8\u540c\u3002\u672c\u6587\u4f5c\u8005\u7684\u601d\u8def\u5219\u662f\u5bf9\u4e8e Sobel edge filter\u76f8\u5e94\u5927\u4e8e\u4e09\u7684\u70b9\u6807\u8bb0\u4e3a\"flying pixels\"\uff0c\u5c06\u5176disparity\u8d4b\u503c\u4e3a\u79bb\u5b83\u6700\u8fd1\u7684\"non-flying\" pixel.","title":"Depth Sharpening"},{"location":"other_categories/Summaries/Summary_of_serveral_eccv2020/#attentive-normalization","text":"pdf code \u8fd9\u7bc7paper\u63d0\u51fa\u4e86\u4e00\u4e2aNormalization + SE\u7684\u878d\u5408 class hsigmoid(nn.Module): \"\"\" Hard Linear Sigmoid \"\"\" def forward(self, x): out = F.relu6(x + 3, inplace=True) / 6 return out class AttentionWeights(nn.Module): def __init__(self, num_channels, k, attention_mode=0): super(AttentionWeights, self).__init__() self.k = k self.avgpool = nn.AdaptiveAvgPool2d(1) layers = [] if attention_mode == 0: layers = [ nn.Conv2d(num_channels, k, 1), hsigmoid() ] elif attention_mode == 2: layers = [ nn.Conv2d(num_channels, k, 1, bias=False), hsigmoid() ] else: raise NotImplementedError(\"Unknow attention weight type\") self.attention = nn.Sequential(*layers) def forward(self, x): b, c, _, _ = x.size() y = self.avgpool(x)#.view(b, c) var = torch.var(x, dim=(2, 3)).view(b, c, 1, 1) y *= (var + 1e-3).rsqrt() #y = torch.cat((y, var), dim=1) return self.attention(y).view(b, self.k) class AttentiveGroupNorm(nn.GroupNorm): def __init__(self, num_groups, num_channels, k, eps=1e-5): super(AttentiveGroupNorm, self).__init__(num_groups, num_channels, eps=eps, affine=False) self.k = k self.weight_ = nn.Parameter(torch.Tensor(k, num_channels)) self.bias_ = nn.Parameter(torch.Tensor(k, num_channels)) self.attention_weights = AttentionWeights(num_channels, k) self._init_params() def _init_params(self): nn.init.normal_(self.weight_, 1, 0.1) nn.init.normal_(self.bias_, 0, 0.1) def forward(self, input): output = super(AttentiveGroupNorm, self).forward(input) size = output.size() y = self.attention_weights(input) weight = y @ self.weight_ bias = y @ self.bias_ weight = weight.unsqueeze(-1).unsqueeze(-1).expand(size) bias = bias.unsqueeze(-1).unsqueeze(-1).expand(size) return weight * output + bias","title":"Attentive Normalization"},{"location":"other_categories/Summaries/Summary_of_serveral_eccv2020/#tide-a-general-toolbox-for-identifying-object-detection-errors","text":"pdf code \u8fd9\u7bc7paper\u7ed9\u4e86\u4e00\u4e2aPython\u5de5\u5177\u7bb1, \u66f4\u4e3a\u8be6\u7ec6\u5730\u53bb\u5206\u6790object detection\u7684error,","title":"TIDE: A General Toolbox for Identifying Object Detection Errors"},{"location":"other_categories/Summaries/Summary_of_serveral_eccv2020/#arbitrary-oriented-object-detection-with-circular-smooth-label","text":"pdf code \u8fd9\u7bc7paper review\u4e86\u56de\u5f52\u65cb\u8f6c bounding boxes\u7684\u65b9\u6cd5\uff0c\u540c\u65f6\u6307\u51fa\u4e86\u76ee\u524d\u51e0\u4e2a\u56de\u5f52\u53c2\u6570\u5316\u65b9\u6cd5\u7684\u5f0a\u7aef\uff0c\u63d0\u51fa\u7684\u65b9\u6848\u4e3b\u8981\u662f\u89e3\u51b3\u4e86\u89d2\u5ea6\u7684\u4e0d\u8fde\u7eed\u6027\u95ee\u9898. \u4e09\u79cd\u5df2\u6709\u7684\u53c2\u6570\u5316\u56de\u5f52\u65cb\u8f6cbounding boxe\u7684\u65b9\u6cd5: \u5728\u67d0\u4e2a\u7279\u6b8a\u60c5\u51b5\u4e0b\uff0c\u4ee5\u4e0a\u4e09\u79cd\u53c2\u6570\u65b9\u6cd5\u75c5\u6001\u7684\u56de\u5f52\u8fc7\u7a0b: \u4f5c\u8005\u63d0\u51fa\u5728\u89d2\u5ea6\u4e0a\u91c7\u7528\u7a97\u51fd\u6570\u5e73\u6ed1\u540e\u7684\u3001\u57fa\u4e8e\u5206\u7c7b\u7684\u8bad\u7ec3\u65b9\u6848","title":"Arbitrary-Oriented Object Detection with Circular Smooth Label"},{"location":"other_categories/Summaries/Summary_of_serveral_eccv2020/#raft-recurrent-all-pairs-field-transforms-for-optical-flow","text":"pdf code \u8fd9\u7bc7paper\u662feccv2020\u7684best paper.\u5bf9\u4e8e\u5149\u6d41\u7684\u8ba1\u7b97\u6709\u6bd4\u8f83\u5927\u7684\u521b\u65b0\u3002\u4e3b\u8981\u5b9e\u73b0\u7684\u662f\u5728\u9ad8\u5206\u8fa8\u7387\u4e0b\u5bf9\u5927\u5c3a\u5ea6\u5149\u6d41\u641c\u7d22\u7684\u4f18\u5316\u65b9\u6848\u3002 \u6838\u5fc3\u7684\u521b\u65b0\u5728\u4e8e\u5728scale 8 \u9884\u8ba1\u7b97\u4e24\u5e27\u4e4b\u95f4\u6bcf\u4e24\u4e2a\u50cf\u7d20\u4e4b\u95f4\u7684\u5339\u914dscore(dot product),\u5728\u8fed\u4ee3\u4f18\u5316\u7684\u8fc7\u7a0b\u4e2d\"LookUp\" Operator\u8d1f\u8d23\u5c06sample\u5149\u6d41\u5bf9\u5e94\u4f4d\u7f6e\u7684cost. \u907f\u514d\u4e86\u591a\u6b21\u91cd\u590d\u8ba1\u7b97cost volume. \u4f7f\u5f97\u5728scale 8\u4e0a\u641c\u7d22\u957f\u8ddd\u79bb\u7684\u5149\u6d41\u6210\u4e3a\u53ef\u80fd\uff0c\u5bf9\u5feb\u901f\u79fb\u52a8\u7684\u5c0f\u7269\u4f53\u7684\u68c0\u6d4b\u80fd\u529b\u53d8\u5f3a\u3002","title":"RAFT: Recurrent All-Pairs Field Transforms for Optical Flow"},{"location":"other_categories/Summaries/Summary_of_several_iccv2019/","text":"Summaries for several ICCV 2019 papers \u76ee\u5f55: Summaries for several ICCV 2019 papers Many Task Learning With Task Routing AdaTransform: Adaptive Data Transformation LIP: Local Importance-based Pooling Anchor Loss: Modulating Loss Scale based on Prediction Difficulty Many Task Learning With Task Routing pdf code \u8fd9\u7bc7\u6587\u7ae0\u89e3\u51b3\u591a\u4efb\u52a1\u5b66\u4e60\u7684\u95ee\u9898\uff0c\u89e3\u51b3\u7684\u65b9\u6848\u662f\u4e00\u6b21train\u53eaactivate\u6709\u9650\u4e2a\u4efb\u52a1\uff0c\u7136\u540e\u968f\u673a\u6fc0\u6d3b\u3002\u8fd9\u6269\u5c55\u6210\u4e00\u4e2a\u5e7f\u4e49\u7684\u5c42\uff0ctrain\u65f6\u968f\u673a\u6fc0\u6d3b\u6709\u9650\u4e2a\u8f93\u51fa\u3002 AdaTransform: Adaptive Data Transformation pdf \u8fd9\u7bc7\u6587\u7ae0\u4f5c\u8005\u6709\u7c7b\u4f3c\u4efb\u52a1\u7684 \u65e7repo .\u8fd9\u7bc7\u6587\u7ae0\u8bad\u7ec3\u4e00\u4e2a\u4e13\u6ce8\u4e8e\u6570\u636e\u589e\u5f3a\u7684agent\uff0c\u5728\u7ade\u4e89\u6a21\u5f0f\u4e2d\u80fd\u6311\u6218\u539f\u7f51\u7edc\uff0c\u5728\u5408\u4f5c\u6a21\u5f0f\u4e2d\u80fd\u63d0\u5347\u539f\u7f51\u7edc\u7684Generalization\u80fd\u529b\u3002\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u8fd9\u4e2aagent\uff0c\u53e6\u5916\u4e5f\u9700\u8981\u4e00\u4e2a\u5206\u7c7b\u5668\u7528\u4e8e\u5728\u7ade\u4e89\u6a21\u5f0f\u65f6\u9274\u522bagent\u7684\u7f51\u7edc\u662f\u5426\u8fc7\u4e8e\u504f\u79bb\u539f\u5206\u5e03\u3002 LIP: Local Importance-based Pooling pdf code \u5982\u56fe\u6a21\u5757 Anchor Loss: Modulating Loss Scale based on Prediction Difficulty pdf \\ell(p, q ; \\gamma)=-\\underbrace{(1+\\overbrace{q-q_{*}}^{\\text {predicion difficulty }})}_{\\text {modulator }} \\underbrace{(1-p) \\log (1-q)}_{\\text {cross entropy }} \u8fd9\u7bc7\u6587\u7ae0\u8ba8\u8bba\u4e86\u4e00\u4e2a\u57fa\u4e8eFocal Loss\u5f00\u53d1\u7684\u4e00\u4e2a\u635f\u5931\u51fd\u6570\uff0c\u5982\u4e0a\u516c\u5f0f\u3002\u672c\u6587\u5bf9 q_* \u8fdb\u884c\u4e86\u8ba8\u8bba\uff0c\u5728\u5206\u7c7b\u4ee5\u53ca\u59ff\u6001\u4f30\u8ba1(\u672c\u8d28\u4e0a\u90fd\u662f\u5206\u7c7b\u95ee\u9898)\u4e2d\u5f97\u5230\u4e86SOTA\u7684\u7ed3\u679c\u3002","title":"Summaries for several ICCV 2019 papers"},{"location":"other_categories/Summaries/Summary_of_several_iccv2019/#summaries-for-several-iccv-2019-papers","text":"\u76ee\u5f55: Summaries for several ICCV 2019 papers Many Task Learning With Task Routing AdaTransform: Adaptive Data Transformation LIP: Local Importance-based Pooling Anchor Loss: Modulating Loss Scale based on Prediction Difficulty","title":"Summaries for several ICCV 2019 papers"},{"location":"other_categories/Summaries/Summary_of_several_iccv2019/#many-task-learning-with-task-routing","text":"pdf code \u8fd9\u7bc7\u6587\u7ae0\u89e3\u51b3\u591a\u4efb\u52a1\u5b66\u4e60\u7684\u95ee\u9898\uff0c\u89e3\u51b3\u7684\u65b9\u6848\u662f\u4e00\u6b21train\u53eaactivate\u6709\u9650\u4e2a\u4efb\u52a1\uff0c\u7136\u540e\u968f\u673a\u6fc0\u6d3b\u3002\u8fd9\u6269\u5c55\u6210\u4e00\u4e2a\u5e7f\u4e49\u7684\u5c42\uff0ctrain\u65f6\u968f\u673a\u6fc0\u6d3b\u6709\u9650\u4e2a\u8f93\u51fa\u3002","title":"Many Task Learning With Task Routing"},{"location":"other_categories/Summaries/Summary_of_several_iccv2019/#adatransform-adaptive-data-transformation","text":"pdf \u8fd9\u7bc7\u6587\u7ae0\u4f5c\u8005\u6709\u7c7b\u4f3c\u4efb\u52a1\u7684 \u65e7repo .\u8fd9\u7bc7\u6587\u7ae0\u8bad\u7ec3\u4e00\u4e2a\u4e13\u6ce8\u4e8e\u6570\u636e\u589e\u5f3a\u7684agent\uff0c\u5728\u7ade\u4e89\u6a21\u5f0f\u4e2d\u80fd\u6311\u6218\u539f\u7f51\u7edc\uff0c\u5728\u5408\u4f5c\u6a21\u5f0f\u4e2d\u80fd\u63d0\u5347\u539f\u7f51\u7edc\u7684Generalization\u80fd\u529b\u3002\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u8fd9\u4e2aagent\uff0c\u53e6\u5916\u4e5f\u9700\u8981\u4e00\u4e2a\u5206\u7c7b\u5668\u7528\u4e8e\u5728\u7ade\u4e89\u6a21\u5f0f\u65f6\u9274\u522bagent\u7684\u7f51\u7edc\u662f\u5426\u8fc7\u4e8e\u504f\u79bb\u539f\u5206\u5e03\u3002","title":"AdaTransform: Adaptive Data Transformation"},{"location":"other_categories/Summaries/Summary_of_several_iccv2019/#lip-local-importance-based-pooling","text":"pdf code \u5982\u56fe\u6a21\u5757","title":"LIP: Local Importance-based Pooling"},{"location":"other_categories/Summaries/Summary_of_several_iccv2019/#anchor-loss-modulating-loss-scale-based-on-prediction-difficulty","text":"pdf \\ell(p, q ; \\gamma)=-\\underbrace{(1+\\overbrace{q-q_{*}}^{\\text {predicion difficulty }})}_{\\text {modulator }} \\underbrace{(1-p) \\log (1-q)}_{\\text {cross entropy }} \u8fd9\u7bc7\u6587\u7ae0\u8ba8\u8bba\u4e86\u4e00\u4e2a\u57fa\u4e8eFocal Loss\u5f00\u53d1\u7684\u4e00\u4e2a\u635f\u5931\u51fd\u6570\uff0c\u5982\u4e0a\u516c\u5f0f\u3002\u672c\u6587\u5bf9 q_* \u8fdb\u884c\u4e86\u8ba8\u8bba\uff0c\u5728\u5206\u7c7b\u4ee5\u53ca\u59ff\u6001\u4f30\u8ba1(\u672c\u8d28\u4e0a\u90fd\u662f\u5206\u7c7b\u95ee\u9898)\u4e2d\u5f97\u5230\u4e86SOTA\u7684\u7ed3\u679c\u3002","title":"Anchor Loss: Modulating Loss Scale based on Prediction Difficulty"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-27/","text":"Arxiv Computer Vision Papers - 2025-08-27 Executive Summary \u597d\u7684\uff0c\u8fd9\u662f\u4e00\u4efd\u9488\u5bf92025\u5e748\u670825\u65e5Arxiv\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u6700\u65b0\u8bba\u6587\u7684\u6267\u884c\u6458\u8981\u3002 Arxiv \u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u6700\u65b0\u8bba\u6587\u6267\u884c\u6458\u8981 (2025-08-25) \u62a5\u544a\u65e5\u671f: 2025\u5e748\u670825\u65e5 \u62a5\u544a\u4eba: [\u60a8\u7684\u59d3\u540d/\u7814\u7a76\u52a9\u7406] \u672c\u62a5\u544a\u65e8\u5728\u4e3a\u5fd9\u788c\u7684\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e00\u4efd\u5173\u4e8e2025\u5e748\u670825\u65e5Arxiv\u4e0a\u53d1\u5e03\u768410\u7bc7\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u6700\u65b0\u8bba\u6587\u7684\u7b80\u660e\u6982\u8ff0\uff0c\u4ee5\u5e2e\u52a9\u5feb\u901f\u4e86\u89e3\u8be5\u9886\u57df\u7684\u91cd\u8981\u53d1\u5c55\u3002 1. \u4e3b\u8981\u4e3b\u9898\u4e0e\u8d8b\u52bf\u6982\u8ff0 \u672c\u6b21\u53d1\u5e03\u7684\u8bba\u6587\u5c55\u73b0\u4e86\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u51e0\u4e2a\u663e\u8457\u7684\u8d8b\u52bf\uff1a Transformer\u67b6\u6784\u7684\u6301\u7eed\u4e3b\u5bfc\u4e0e\u6df1\u5316: \u51e0\u4e4e\u4e00\u534a\u7684\u8bba\u6587\uff082, 7, 8, 10\uff09\u76f4\u63a5\u6216\u95f4\u63a5\u4f7f\u7528\u4e86Transformer\u67b6\u6784\uff0c\u5e76\u5bf9\u5176\u8fdb\u884c\u521b\u65b0\u6027\u6539\u8fdb\uff08\u5982\u51e0\u4f55\u4f4d\u7f6e\u7f16\u7801\u3001MoE\uff09\u6216\u5e94\u7528\u4e8e\u7279\u5b9a\u4efb\u52a1\uff08\u8d85\u5206\u3001\u5206\u5272\u3001\u56fe\u50cf\u589e\u5f3a\uff09\u3002 \u7279\u5b9a\u5e94\u7528\u9886\u57df\u7684\u6df1\u5ea6\u63a2\u7d22: \u533b\u7597\u5f71\u50cf\uff081, 9\uff09\u3001\u81ea\u52a8\u9a7e\u9a76\uff084, 5\uff09\u3001\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\uff087\uff09\u3001\u4f4e\u5149\u7167\u56fe\u50cf\u589e\u5f3a\uff0810\uff09\u548c\u5c4b\u9876\u5206\u5272\uff088\uff09\u7b49\u9886\u57df\u6301\u7eed\u53d7\u5230\u5173\u6ce8\uff0c\u7814\u7a76\u4eba\u5458\u81f4\u529b\u4e8e\u5c06\u5148\u8fdb\u6a21\u578b\u843d\u5730\u5230\u5b9e\u9645\u95ee\u9898\u4e2d\u3002 \u6570\u636e\u4e3a\u4e2d\u5fc3\u7684\u4eba\u5de5\u667a\u80fd (Data-Centric AI): \u8bba\u6587\u5173\u6ce8\u6570\u636e\u8d28\u91cf\uff086\uff09\u548c\u6570\u636e\u589e\u5f3a\uff081\uff09\uff0c\u5f3a\u8c03\u4e86\u9ad8\u8d28\u91cf\u6570\u636e\u548c\u6709\u6548\u6570\u636e\u5904\u7406\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u91cd\u8981\u6027\u3002 \u57fa\u7840\u6a21\u578b (Foundation Models) \u7684\u6f14\u8fdb\u4e0e\u5e94\u7528: \u8bba\u6587\u63a2\u8ba8\u4e86\u57fa\u7840\u6a21\u578b\u7684\u672a\u6765\u53d1\u5c55\u65b9\u5411\uff083\uff09\u4ee5\u53ca\u5b83\u4eec\u5728\u7279\u5b9a\u3001\u6311\u6218\u6027\u9886\u57df\uff08\u5982\u5185\u7aa5\u955c\u6df1\u5ea6\u4f30\u8ba1\uff0c9\uff09\u7684\u5e94\u7528\u6f5c\u529b\u3002 \u81ea\u76d1\u7763\u5b66\u4e60\u4e0e\u751f\u6210\u6a21\u578b: \u81ea\u76d1\u7763\u5b66\u4e60\uff085\uff09\u548c\u6269\u6563\u6a21\u578b\uff081\uff09\u4f5c\u4e3a\u5f3a\u5927\u7684\u8303\u5f0f\uff0c\u88ab\u7528\u4e8e\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u548c\u590d\u6742\u573a\u666f\u7406\u89e3\u7b49\u95ee\u9898\u3002 2. \u7279\u522b\u663e\u8457\u6216\u521b\u65b0\u7684\u8bba\u6587 \u8bba\u65872: \"Beyond flattening: a geometrically principled positional encoding for vision transformers with Weierstrass elliptic functions\" \u521b\u65b0\u70b9: \u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u9b4f\u5c14\u65af\u7279\u62c9\u65af\u692d\u5706\u51fd\u6570\u7684\u51e0\u4f55\u539f\u7406\u4f4d\u7f6e\u7f16\u7801\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edfTransformer\u4e2d\u6241\u5e73\u5316\u7684\u4f4d\u7f6e\u7f16\u7801\u65b9\u5f0f\u3002\u5b83\u53ef\u80fd\u4e3aTransformer\u7684\u5e95\u5c42\u673a\u5236\u5e26\u6765\u66f4\u6df1\u523b\u7684\u7406\u8bba\u7406\u89e3\u548c\u6027\u80fd\u63d0\u5347\uff0c\u5177\u6709\u6f5c\u5728\u7684\u67b6\u6784\u7ea7\u5f71\u54cd\u3002 \u8bba\u65873: \"Why Relational Graphs Will Save the Next Generation of Vision Foundation Models?\" \u521b\u65b0\u70b9: \u8fd9\u662f\u4e00\u7bc7\u5177\u6709\u524d\u77bb\u6027\u548c\u6982\u5ff5\u6027\u7684\u8bba\u6587\uff0c\u63a2\u8ba8\u4e86\u5173\u7cfb\u56fe\u5728\u672a\u6765\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u3002\u5b83\u6311\u6218\u4e86\u5f53\u524d\u57fa\u7840\u6a21\u578b\u4ec5\u4f9d\u8d56\u5927\u89c4\u6a21\u6570\u636e\u548c\u53c2\u6570\u7684\u8303\u5f0f\uff0c\u63d0\u51fa\u901a\u8fc7\u5f15\u5165\u5173\u7cfb\u5f52\u7eb3\u504f\u7f6e\u6765\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5bf9\u9886\u57df\u53d1\u5c55\u65b9\u5411\u5177\u6709\u6307\u5bfc\u610f\u4e49\u3002 \u8bba\u65876: \"Learning to Detect Label Errors by Making Them: A Method for Segmentation and Object Detection Datasets\" \u521b\u65b0\u70b9: \u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u201c\u5236\u9020\u201d\u6807\u7b7e\u9519\u8bef\u6765\u5b66\u4e60\u68c0\u6d4b\u771f\u5b9e\u6807\u7b7e\u9519\u8bef\u7684\u65b0\u9896\u65b9\u6cd5\u3002\u8fd9\u76f4\u63a5\u89e3\u51b3\u4e86\u5b9e\u9645\u5e94\u7528\u4e2d\u6570\u636e\u96c6\u8d28\u91cf\u63a7\u5236\u7684\u75db\u70b9\uff0c\u5bf9\u4e8e\u63d0\u9ad8\u6a21\u578b\u9c81\u68d2\u6027\u548c\u51cf\u5c11\u4eba\u5de5\u6807\u6ce8\u6210\u672c\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002 \u8bba\u65879: \"EndoUFM: Utilizing Foundation Models for Monocular depth estimation of endoscopic images\" \u521b\u65b0\u70b9: \u5c06\u57fa\u7840\u6a21\u578b\u5e94\u7528\u4e8e\u6781\u5177\u6311\u6218\u6027\u7684\u5185\u7aa5\u955c\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u4efb\u52a1\uff0c\u5c55\u793a\u4e86\u57fa\u7840\u6a21\u578b\u5728\u7279\u5b9a\u3001\u6570\u636e\u7a00\u7f3a\u4e14\u5f62\u53d8\u4e25\u91cd\u7684\u533b\u7597\u573a\u666f\u4e2d\u7684\u5f3a\u5927\u9002\u5e94\u6027\u548c\u6f5c\u529b\u3002 3. \u65b0\u5174\u7814\u7a76\u65b9\u5411\u6216\u6280\u672f Transformer\u67b6\u6784\u7684\u51e0\u4f55\u5316\u4e0e\u7406\u8bba\u6df1\u5316: \u8bba\u65872\u9884\u793a\u7740\u5bf9Transformer\u6838\u5fc3\u7ec4\u4ef6\uff08\u5982\u4f4d\u7f6e\u7f16\u7801\uff09\u8fdb\u884c\u66f4\u6df1\u5c42\u6b21\u7684\u6570\u5b66\u548c\u51e0\u4f55\u539f\u7406\u63a2\u7d22\uff0c\u4ee5\u7a81\u7834\u73b0\u6709\u6027\u80fd\u74f6\u9888\u3002 \u5c06\u5173\u7cfb\u5f52\u7eb3\u504f\u7f6e\u878d\u5165\u57fa\u7840\u6a21\u578b: \u8bba\u65873\u5f3a\u8c03\u4e86\u5728\u57fa\u7840\u6a21\u578b\u4e2d\u5f15\u5165\u7ed3\u6784\u5316\u77e5\u8bc6\uff08\u5982\u5173\u7cfb\u56fe\uff09\u7684\u91cd\u8981\u6027\uff0c\u4ee5\u63d0\u5347\u5176\u63a8\u7406\u80fd\u529b\u548c\u6cdb\u5316\u6027\uff0c\u800c\u975e\u4ec5\u4ec5\u4f9d\u8d56\u89c4\u6a21\u3002 \u6570\u636e\u8d28\u91cf\u7ba1\u7406\u4f5c\u4e3a\u53ef\u5b66\u4e60\u4efb\u52a1: \u8bba\u65876\u63d0\u51fa\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u81ea\u52a8\u68c0\u6d4b\u548c\u7ea0\u6b63\u6570\u636e\u96c6\u4e2d\u7684\u6807\u7b7e\u9519\u8bef\uff0c\u5c06\u6570\u636e\u6e05\u6d17\u4ece\u4eba\u5de5\u5bc6\u96c6\u578b\u4efb\u52a1\u8f6c\u53d8\u4e3a\u81ea\u52a8\u5316\u6d41\u7a0b\u3002 \u6269\u6563\u6a21\u578b\u5728\u6570\u636e\u589e\u5f3a\u4e2d\u7684\u5e94\u7528: \u8bba\u65871\u5c55\u793a\u4e86\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u5408\u6210\u6570\u636e\u4ee5\u589e\u5f3a\u533b\u7597\u5f71\u50cf\u6570\u636e\u96c6\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u6570\u636e\u7a00\u7f3a\u7684\u9886\u57df\u3002 \u591a\u6a21\u6001\u81ea\u76d1\u7763\u5b66\u4e60\u5728\u590d\u6742\u573a\u666f\u7406\u89e3\u4e2d\u7684\u5e94\u7528: \u8bba\u65875\u5229\u7528\u591a\u6a21\u6001\u81ea\u76d1\u7763\u6846\u67b6\u8fdb\u884c\u573a\u666f\u65e0\u5173\u7684\u53ef\u901a\u884c\u6027\u4f30\u8ba1\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7b49\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002 4. \u5efa\u8bae\u9605\u8bfb\u7684\u8bba\u6587 \u6839\u636e\u7814\u7a76\u5174\u8da3\u548c\u6f5c\u5728\u5f71\u54cd\uff0c\u5efa\u8bae\u4f18\u5148\u9605\u8bfb\u4ee5\u4e0b\u8bba\u6587\uff1a \u5bf9\u4e8e\u5173\u6ce8Transformer\u5e95\u5c42\u673a\u5236\u548c\u67b6\u6784\u521b\u65b0\u7684\u7814\u7a76\u4eba\u5458: \u8bba\u65872: \"Beyond flattening: a geometrically principled positional encoding for vision transformers with Weierstrass elliptic functions\" \u5bf9\u4e8e\u5173\u6ce8\u57fa\u7840\u6a21\u578b\u672a\u6765\u53d1\u5c55\u548c\u7406\u8bba\u74f6\u9888\u7684\u7814\u7a76\u4eba\u5458: \u8bba\u65873: \"Why Relational Graphs Will Save the Next Generation of Vision Foundation Models?\" \u5bf9\u4e8e\u4ece\u4e8b\u6570\u636e\u96c6\u6784\u5efa\u3001\u8d28\u91cf\u63a7\u5236\u6216\u6a21\u578b\u5b9e\u9645\u90e8\u7f72\u7684\u7814\u7a76\u4eba\u5458: \u8bba\u65876: \"Learning to Detect Label Errors by Making Them: A Method for Segmentation and Object Detection Datasets\" \u5bf9\u4e8e\u533b\u7597\u5f71\u50cf\u9886\u57df\u6216\u5bf9\u57fa\u7840\u6a21\u578b\u5728\u7279\u5b9a\u9886\u57df\u843d\u5730\u611f\u5174\u8da3\u7684\u7814\u7a76\u4eba\u5458: \u8bba\u65879: \"EndoUFM: Utilizing Foundation Models for Monocular depth estimation of endoscopic images\" \u8bba\u65871: \"Diffusion-Based Data Augmentation for Medical Image Segmentation\" \u5176\u4ed6\u8bba\u6587\uff084, 5, 7, 8, 10\uff09\u5bf9\u5404\u81ea\u7684\u7279\u5b9a\u5e94\u7528\u9886\u57df\u5177\u6709\u76f4\u63a5\u4ef7\u503c\uff0c\u53ef\u6839\u636e\u4e2a\u4eba\u7814\u7a76\u65b9\u5411\u9009\u62e9\u6027\u9605\u8bfb\u3002 Table of Contents Diffusion-Based Data Augmentation for Medical Image Segmentation Beyond flattening: a geometrically principled positional encoding for vision transformers with Weierstrass elliptic functions Why Relational Graphs Will Save the Next Generation of Vision Foundation Models? Enhanced Drift-Aware Computer Vision Architecture for Autonomous Driving Scene-Agnostic Traversability Labeling and Estimation via a Multimodal Self-supervised Framework Learning to Detect Label Errors by Making Them: A Method for Segmentation and Object Detection Datasets CATformer: Contrastive Adversarial Transformer for Image Super-Resolution RoofSeg: An edge-aware transformer-based network for end-to-end roof plane segmentation EndoUFM: Utilizing Foundation Models for Monocular depth estimation of endoscopic images ISALux: Illumination and Segmentation Aware Transformer Employing Mixture of Experts for Low Light Image Enhancement Papers Diffusion-Based Data Augmentation for Medical Image Segmentation Authors: Maham Nazir, Muhammad Aqeel, Francesco Setti Published: 2025-08-25 Categories: cs.CV, cs.LG Abstract: Medical image segmentation models struggle with rare abnormalities due to scarce annotated pathological data. We propose DiffAug a novel framework that combines textguided diffusion-based generation with automatic segmentation validation to address this challenge. Our proposed approach uses latent diffusion models conditioned on medical text descriptions and spatial masks to synthesize abnormalities via inpainting on normal images. Generated samples undergo dynamic quality validation through a latentspace segmentation network that ensures accurate localization while enabling single-step inference. The text prompts, derived from medical literature, guide the generation of diverse abnormality types without requiring manual annotation. Our validation mechanism filters synthetic samples based on spatial accuracy, maintaining quality while operating efficiently through direct latent estimation. Evaluated on three medical imaging benchmarks (CVC-ClinicDB, Kvasir-SEG, REFUGE2), our framework achieves state-of-the-art performance with 8-10% Dice improvements over baselines and reduces false negative rates by up to 28% for challenging cases like small polyps and flat lesions critical for early detection in screening applications. Analysis: \u4f5c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u4e13\u5bb6\uff0c\u6211\u5bf9\u8fd9\u7bc7\u8bba\u6587\u6458\u8981\u8fdb\u884c\u5982\u4e0b\u5206\u6790\uff1a 1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u660e\u6458\u8981 (Concise Summary) \u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a DiffAug \u7684\u65b0\u9896\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u5408\u6210\u6570\u636e\u6765\u89e3\u51b3\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6a21\u578b\u5728\u5904\u7406\u7f55\u89c1\u5f02\u5e38\u65f6\u9762\u4e34\u7684\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002DiffAug \u7ed3\u5408\u4e86\u6587\u672c\u5f15\u5bfc\u7684\u6269\u6563\u6a21\u578b\u8fdb\u884c\u5f02\u5e38\u751f\u6210\uff08\u901a\u8fc7\u5728\u6b63\u5e38\u56fe\u50cf\u4e0a\u8fdb\u884c\u5185\u7ed8\uff09\uff0c\u5e76\u5229\u7528\u4e00\u4e2a\u9ad8\u6548\u7684\u6f5c\u5728\u7a7a\u95f4\u5206\u5272\u7f51\u7edc\u8fdb\u884c\u52a8\u6001\u8d28\u91cf\u9a8c\u8bc1\uff0c\u786e\u4fdd\u4e86\u751f\u6210\u6837\u672c\u7684\u7a7a\u95f4\u51c6\u786e\u6027\u3002\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u533b\u5b66\u56fe\u50cf\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u5c0f\u578b\u548c\u6241\u5e73\u75c5\u53d8\u7b49\u6311\u6218\u6027\u75c5\u4f8b\u65f6\u3002 2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u5b66\u65b9\u6cd5 (Key Innovation or Methodological Approach) \u8be5\u8bba\u6587\u7684\u5173\u952e\u521b\u65b0\u5728\u4e8e\u5176 \u96c6\u6210\u5316\u7684\u3001\u53cc\u9636\u6bb5\u65b9\u6cd5 \uff1a 1. \u6587\u672c\u5f15\u5bfc\u7684\u6f5c\u5728\u6269\u6563\u6a21\u578b\u8fdb\u884c\u6761\u4ef6\u751f\u6210\uff1a DiffAug \u5229\u7528\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u533b\u5b66\u6587\u672c\u63cf\u8ff0\u548c\u7a7a\u95f4\u63a9\u7801\u4f5c\u4e3a\u6761\u4ef6\uff0c\u5728\u6b63\u5e38\u56fe\u50cf\u4e0a\u8fdb\u884c\u5185\u7ed8\u6765\u5408\u6210\u5404\u79cd\u5f02\u5e38\u3002\u8fd9\u4f7f\u5f97\u80fd\u591f\u6839\u636e\u6587\u672c\u63d0\u793a\u751f\u6210\u591a\u6837\u5316\u7684\u5f02\u5e38\u7c7b\u578b\uff0c\u800c\u65e0\u9700\u624b\u52a8\u6807\u6ce8\u751f\u6210\u8fc7\u7a0b\u3002 2. \u9ad8\u6548\u7684\u6f5c\u5728\u7a7a\u95f4\u5206\u5272\u7f51\u7edc\u8fdb\u884c\u52a8\u6001\u8d28\u91cf\u9a8c\u8bc1\uff1a \u8bba\u6587\u5f15\u5165\u4e86\u4e00\u4e2a\u72ec\u7279\u7684\u9a8c\u8bc1\u673a\u5236\uff0c\u901a\u8fc7\u4e00\u4e2a\u5728\u6f5c\u5728\u7a7a\u95f4\u64cd\u4f5c\u7684\u5206\u5272\u7f51\u7edc\u6765\u52a8\u6001\u8bc4\u4f30\u751f\u6210\u6837\u672c\u7684\u8d28\u91cf\u548c\u5b9a\u4f4d\u51c6\u786e\u6027\u3002\u8fd9\u79cd\u65b9\u6cd5\u4e0d\u4ec5\u786e\u4fdd\u4e86\u5408\u6210\u6570\u636e\u7684\u5b9e\u7528\u6027\uff0c\u800c\u4e14\u901a\u8fc7\u5355\u6b65\u63a8\u7406\u548c\u76f4\u63a5\u6f5c\u5728\u4f30\u8ba1\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u8fc7\u6ee4\uff0c\u907f\u514d\u4e86\u751f\u6210\u4f4e\u8d28\u91cf\u6216\u4e0d\u51c6\u786e\u7684\u6837\u672c\u3002 \u8fd9\u79cd\u5c06\u667a\u80fd\u751f\u6210\u4e0e\u9ad8\u6548\u3001\u81ea\u52a8\u9a8c\u8bc1\u76f8\u7ed3\u5408\u7684\u7b56\u7565\uff0c\u662f\u5176\u533a\u522b\u4e8e\u4f20\u7edf\u6570\u636e\u589e\u5f3a\u548c\u4e00\u822c\u6269\u6563\u6a21\u578b\u5e94\u7528\u7684\u5173\u952e\u3002 3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd (Potential Impact on the Field) \u8fd9\u9879\u7814\u7a76\u5bf9\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u533b\u5b66\u56fe\u50cf\u5206\u6790\u9886\u57df\u5177\u6709\u6df1\u8fdc\u5f71\u54cd\uff1a * \u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u7684\u6838\u5fc3\u6311\u6218\uff1a \u5b83\u4e3a\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u957f\u671f\u5b58\u5728\u7684\u7f55\u89c1\u75be\u75c5\u548c\u5f02\u5e38\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ece\u800c\u80fd\u591f\u8bad\u7ec3\u51fa\u66f4\u9c81\u68d2\u3001\u66f4\u51c6\u786e\u7684\u5206\u5272\u6a21\u578b\u3002 * \u63d0\u5347\u8bca\u65ad\u51c6\u786e\u6027\u4e0e\u65e9\u671f\u68c0\u6d4b\uff1a \u663e\u8457\u964d\u4f4e\u4e86\u5c0f\u606f\u8089\u548c\u6241\u5e73\u75c5\u53d8\u7b49\u6311\u6218\u6027\u75c5\u4f8b\u7684\u5047\u9634\u6027\u7387\uff0c\u8fd9\u610f\u5473\u7740\u53ef\u4ee5\u66f4\u65e9\u3001\u66f4\u51c6\u786e\u5730\u68c0\u6d4b\u5230\u5173\u952e\u75be\u75c5\uff0c\u5bf9\u764c\u75c7\u7b5b\u67e5\u7b49\u5e94\u7528\u5177\u6709\u5de8\u5927\u7684\u4e34\u5e8a\u4ef7\u503c\u3002 * \u63a8\u52a8\u5408\u6210\u6570\u636e\u5728\u533b\u7597\u9886\u57df\u7684\u5e94\u7528\uff1a \u8bc1\u660e\u4e86\u9ad8\u8d28\u91cf\u3001\u9a8c\u8bc1\u8fc7\u7684\u5408\u6210\u6570\u636e\u5728\u533b\u7597\u9886\u57df\u4f5c\u4e3a\u771f\u5b9e\u6570\u636e\u6709\u6548\u8865\u5145\u7684\u6f5c\u529b\uff0c\u53ef\u80fd\u52a0\u901f\u65b0\u7b97\u6cd5\u7684\u5f00\u53d1\u548c\u90e8\u7f72\u3002 * \u542f\u53d1\u65b0\u7684\u6570\u636e\u589e\u5f3a\u8303\u5f0f\uff1a \u5176\u7ed3\u5408\u751f\u6210\u6a21\u578b\u4e0e\u667a\u80fd\u9a8c\u8bc1\u7684\u6846\u67b6\uff0c\u53ef\u4ee5\u4e3a\u5176\u4ed6\u9700\u8981\u9ad8\u8d28\u91cf\u5408\u6210\u6570\u636e\uff08\u5c24\u5176\u662f\u5728\u6570\u636e\u4e0d\u5e73\u8861\u6216\u7a00\u7f3a\u573a\u666f\u4e0b\uff09\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u63d0\u4f9b\u65b0\u7684\u601d\u8def\u3002 4. \u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 (Related Areas or Applications) \u9664\u4e86\u533b\u5b66\u56fe\u50cf\u5206\u5272\u672c\u8eab\uff0c\u8fd9\u9879\u7814\u7a76\u8fd8\u53ef\u4ee5\u60e0\u53ca\u4ee5\u4e0b\u9886\u57df\u548c\u5e94\u7528\uff1a * \u533b\u5b66\u56fe\u50cf\u68c0\u6d4b\u4e0e\u5206\u7c7b\uff1a \u7c7b\u4f3c\u7684\u65b9\u6cd5\u53ef\u4ee5\u7528\u4e8e\u751f\u6210\u7f55\u89c1\u75c5\u53d8\u7684\u68c0\u6d4b\u6216\u5206\u7c7b\u4efb\u52a1\u7684\u8bad\u7ec3\u6570\u636e\u3002 * \u5c11\u6837\u672c\u5b66\u4e60 (Few-Shot Learning) \u548c\u96f6\u6837\u672c\u5b66\u4e60 (Zero-Shot Learning)\uff1a \u901a\u8fc7\u751f\u6210\u5408\u6210\u6837\u672c\uff0c\u53ef\u4ee5\u6709\u6548\u6269\u5c55\u6709\u9650\u7684\u771f\u5b9e\u6570\u636e\u96c6\uff0c\u4ece\u800c\u6539\u5584\u5c11\u6837\u672c\u6216\u96f6\u6837\u672c\u573a\u666f\u4e0b\u7684\u6a21\u578b\u6027\u80fd\u3002 * \u9886\u57df\u9002\u5e94 (Domain Adaptation)\uff1a \u751f\u6210\u7279\u5b9a\u9886\u57df\u6216\u7279\u5b9a\u8bbe\u5907\u7279\u5f81\u7684\u5408\u6210\u6570\u636e\uff0c\u6709\u52a9\u4e8e\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u6e90\u4e4b\u95f4\u8fdb\u884c\u6cdb\u5316\u3002 * \u5de5\u4e1a\u7f3a\u9677\u68c0\u6d4b\uff1a \u5728\u5de5\u4e1a\u751f\u4ea7\u4e2d\uff0c\u7f55\u89c1\u7f3a\u9677\u7684\u56fe\u50cf\u6570\u636e\u901a\u5e38\u975e\u5e38\u7a00\u7f3a\uff0c\u8be5\u65b9\u6cd5\u53ef\u4ee5\u7528\u4e8e\u751f\u6210\u8fd9\u4e9b\u7f3a\u9677\u7684\u5408\u6210\u56fe\u50cf\uff0c\u4ee5\u8bad\u7ec3\u66f4\u51c6\u786e\u7684\u68c0\u6d4b\u7cfb\u7edf\u3002 * \u81ea\u52a8\u9a7e\u9a76\uff1a \u751f\u6210\u6781\u7aef\u6216\u7f55\u89c1\u4ea4\u901a\u573a\u666f\uff08\u5982\u4e8b\u6545\u3001\u5f02\u5e38\u5929\u6c14\u6761\u4ef6\uff09\u7684\u56fe\u50cf\u6570\u636e\uff0c\u4ee5\u63d0\u9ad8\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u3002 * \u8ba1\u7b97\u673a\u56fe\u5f62\u5b66\u4e0e\u865a\u62df\u73b0\u5b9e\uff1a \u751f\u6210\u5177\u6709\u7279\u5b9a\u5c5e\u6027\u6216\u6761\u4ef6\u7684\u56fe\u50cf\u5185\u5bb9\uff0c\u7528\u4e8e\u8bad\u7ec3\u6216\u6a21\u62df\u3002 5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u7684\u5c40\u9650\u6027 (Limitations Inferable from the Abstract) \u5c3d\u7ba1\u6458\u8981\u5c55\u793a\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u7ed3\u679c\uff0c\u4f46\u4ecd\u53ef\u63a8\u65ad\u51fa\u4e00\u4e9b\u6f5c\u5728\u5c40\u9650\u6027\uff1a * \u6587\u672c\u63d0\u793a\u7684\u8d28\u91cf\u548c\u8986\u76d6\u8303\u56f4\uff1a \u6458\u8981\u63d0\u5230\u6587\u672c\u63d0\u793a\u6765\u6e90\u4e8e\u533b\u5b66\u6587\u732e\uff0c\u4f46\u5176\u80fd\u5426\u5b8c\u5168\u6355\u6349\u6240\u6709\u7f55\u89c1\u5f02\u5e38\u7684\u7ec6\u5fae\u7279\u5f81\u548c\u591a\u6837\u6027\uff0c\u4ee5\u53ca\u5982\u4f55\u5904\u7406\u6587\u732e\u4e2d\u672a\u5145\u5206\u63cf\u8ff0\u7684\u5f02\u5e38\uff0c\u4ecd\u662f\u4e00\u4e2a\u95ee\u9898\u3002\u6587\u672c\u63d0\u793a\u7684\u8d28\u91cf\u76f4\u63a5\u5f71\u54cd\u751f\u6210\u6837\u672c\u7684\u51c6\u786e\u6027\u548c\u591a\u6837\u6027\u3002 * \u751f\u6210\u6837\u672c\u7684\u771f\u5b9e\u6027\u4e0e\u4e34\u5e8a\u53ef\u4fe1\u5ea6\uff1a \u5c3d\u7ba1\u901a\u8fc7\u6f5c\u5728\u7a7a\u95f4\u9a8c\u8bc1\u786e\u4fdd\u4e86\u7a7a\u95f4\u51c6\u786e\u6027\uff0c\u4f46\u751f\u6210\u6837\u672c\u7684\u89c6\u89c9\u771f\u5b9e\u611f\uff08\u5373\u662f\u5426\u80fd\u88ab\u4e34\u5e8a\u533b\u751f\u8ba4\u4e3a\u662f\u771f\u5b9e\u7684\uff09\u4ecd\u9700\u8fdb\u4e00\u6b65\u8bc4\u4f30\u3002\u6269\u6563\u6a21\u578b\u53ef\u80fd\u5f15\u5165\u5fae\u5999\u7684\u4f2a\u5f71\u6216\u4e0d\u81ea\u7136\u7684\u7eb9\u7406\uff0c\u8fd9\u5728\u4e34\u5e8a\u5e94\u7528\u4e2d\u53ef\u80fd\u662f\u654f\u611f\u7684\u3002 * \u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\uff1a \u6269\u6563\u6a21\u578b\u901a\u5e38\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u7ed3\u5408\u6f5c\u5728\u7a7a\u95f4\u5206\u5272\u7f51\u7edc\uff0c\u6574\u4e2a\u6846\u67b6\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u53ef\u80fd\u9700\u8981\u5927\u91cf\u7684\u8ba1\u7b97\u8d44\u6e90\u548c\u65f6\u95f4\u3002 * \u5bf9\u201c\u6b63\u5e38\u56fe\u50cf\u201d\u7684\u4f9d\u8d56\uff1a \u8be5\u65b9\u6cd5\u901a\u8fc7\u5728\u6b63\u5e38\u56fe\u50cf\u4e0a\u8fdb\u884c\u5185\u7ed8\u6765\u5408\u6210\u5f02\u5e38\u3002\u8fd9\u610f\u5473\u7740\u5b83\u9700\u8981\u4e00\u4e2a\u8db3\u591f\u5927\u4e14\u591a\u6837\u5316\u7684\u201c\u6b63\u5e38\u201d\u56fe\u50cf\u6570\u636e\u96c6\u4f5c\u4e3a\u57fa\u7840\u3002\u5982\u679c\u6b63\u5e38\u56fe\u50cf\u672c\u8eab\u4e5f\u7a00\u7f3a\u6216\u5177\u6709\u9ad8\u5ea6\u53d8\u5f02\u6027\uff0c\u5219\u53ef\u80fd\u4f1a\u9650\u5236\u5176\u5e94\u7528\u3002 * \u6f5c\u5728\u7a7a\u95f4\u9a8c\u8bc1\u7684\u5c40\u9650\u6027\uff1a \u5c3d\u7ba1\u9ad8\u6548\uff0c\u4f46\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u9a8c\u8bc1\u53ef\u80fd\u65e0\u6cd5\u6355\u6349\u5230\u50cf\u7d20\u7a7a\u95f4\u4e2d\u6240\u6709\u7ec6\u5fae\u7684\u9519\u8bef\u6216\u4e0d\u4e00\u81f4\u6027\u3002\u5b83\u53ef\u80fd\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u5bf9\u751f\u6210\u8d28\u91cf\u7684\u8bc4\u4f30\u4e0d\u591f\u5168\u9762\u3002 Key Findings: We propose DiffAug a novel framework that combines textguided diffusion-based generation with automatic segmentation validation to address this challenge. Evaluated on three medical imaging benchmarks (CVC-ClinicDB, Kvasir-SEG, REFUGE2), our framework achieves state-of-the-art performance with 8-10% Dice improvements over baselines and reduces false negative rates by up to 28% for challenging cases like small polyps and flat lesions critical for early detection in screening applications. Links: PDF arXiv Beyond flattening: a geometrically principled positional encoding for vision transformers with Weierstrass elliptic functions Authors: Zhihang Xin, Xitong Hu, Rui Wang Published: 2025-08-26 Categories: cs.CV Abstract: Vision Transformers have demonstrated remarkable success in computer vision tasks, yet their reliance on learnable one-dimensional positional embeddings fundamentally disrupts the inherent two-dimensional spatial structure of images through patch flattening procedures. Traditional positional encoding approaches lack geometric constraints and fail to establish monotonic correspondence between Euclidean spatial distances and sequential index distances, thereby limiting the model's capacity to leverage spatial proximity priors effectively. We propose Weierstrass Elliptic Function Positional Encoding (WEF-PE), a mathematically principled approach that directly addresses two-dimensional coordinates through natural complex domain representation, where the doubly periodic properties of elliptic functions align remarkably with translational invariance patterns commonly observed in visual data. Our method exploits the non-linear geometric nature of elliptic functions to encode spatial distance relationships naturally, while the algebraic addition formula enables direct derivation of relative positional information between arbitrary patch pairs from their absolute encodings. Comprehensive experiments demonstrate that WEF-PE achieves superior performance across diverse scenarios, including 63.78\\% accuracy on CIFAR-100 from-scratch training with ViT-Tiny architecture, 93.28\\% on CIFAR-100 fine-tuning with ViT-Base, and consistent improvements on VTAB-1k benchmark tasks. Theoretical analysis confirms the distance-decay property through rigorous mathematical proof, while attention visualization reveals enhanced geometric inductive bias and more coherent semantic focus compared to conventional approaches.The source code implementing the methods described in this paper is publicly available on GitHub. Analysis: \u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u3001\u57fa\u4e8e\u6570\u5b66\u539f\u7406\u7684\u89c6\u89c9Transformer\u4f4d\u7f6e\u7f16\u7801\u65b9\u6cd5\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u56fe\u50cf\u4e8c\u7ef4\u7a7a\u95f4\u7ed3\u6784\u65f6\u7684\u5c40\u9650\u6027\u3002 1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u6d01\u603b\u7ed3 (2-3 \u53e5\u8bdd) \u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aWeierstrass\u692d\u5706\u51fd\u6570\u4f4d\u7f6e\u7f16\u7801\uff08WEF-PE\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u65e8\u5728\u89e3\u51b3Vision Transformer\u4e2d\u901a\u8fc7\u8865\u4e01\u5c55\u5e73\u5bfc\u81f4\u7684\u4e8c\u7ef4\u7a7a\u95f4\u4fe1\u606f\u4e22\u5931\u95ee\u9898\u3002WEF-PE\u5229\u7528Weierstrass\u692d\u5706\u51fd\u6570\u5728\u590d\u6570\u57df\u4e2d\u76f4\u63a5\u7f16\u7801\u4e8c\u7ef4\u5750\u6807\uff0c\u5176\u53cc\u5468\u671f\u6027\u4e0e\u89c6\u89c9\u6570\u636e\u7684\u5e73\u79fb\u4e0d\u53d8\u6027\u76f8\u543b\u5408\uff0c\u5e76\u901a\u8fc7\u4ee3\u6570\u52a0\u6cd5\u516c\u5f0f\u76f4\u63a5\u63a8\u5bfc\u76f8\u5bf9\u4f4d\u7f6e\u4fe1\u606f\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0cWEF-PE\u663e\u8457\u63d0\u5347\u4e86ViT\u5728\u591a\u79cd\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u5e76\u589e\u5f3a\u4e86\u51e0\u4f55\u5f52\u7eb3\u504f\u7f6e\u3002 2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u8bba \u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u5c06 Weierstrass\u692d\u5706\u51fd\u6570 \u5f15\u5165\u5230Vision Transformer\u7684\u4e8c\u7ef4\u4f4d\u7f6e\u7f16\u7801\u4e2d\u3002\u5177\u4f53\u65b9\u6cd5\u8bba\u5305\u62ec\uff1a * \u76f4\u63a5\u5904\u7406\u4e8c\u7ef4\u5750\u6807\u4e0e\u590d\u6570\u57df\u8868\u793a\uff1a \u6452\u5f03\u4e86\u5c06\u4e8c\u7ef4\u56fe\u50cf\u5c55\u5e73\u4e3a\u4e00\u7ef4\u5e8f\u5217\u7684\u505a\u6cd5\uff0c\u800c\u662f\u76f4\u63a5\u5728\u590d\u6570\u57df\u4e2d\u5904\u7406\u56fe\u50cf\u7684\u4e8c\u7ef4\u5750\u6807\uff0c\u8fd9\u4e0e\u4f20\u7edf\u7684\u57fa\u4e8e\u6b63\u5f26/\u4f59\u5f26\u6216\u53ef\u5b66\u4e60\u7684\u4e00\u7ef4\u5d4c\u5165\u5f62\u6210\u9c9c\u660e\u5bf9\u6bd4\u3002 * \u5229\u7528\u692d\u5706\u51fd\u6570\u7684\u53cc\u5468\u671f\u6027\uff1a Weierstrass\u692d\u5706\u51fd\u6570\u7684\u53cc\u5468\u671f\u6027\u4e0e\u89c6\u89c9\u6570\u636e\u4e2d\u5e38\u89c1\u7684\u5e73\u79fb\u4e0d\u53d8\u6027\u6a21\u5f0f\u9ad8\u5ea6\u5951\u5408\uff0c\u4e3a\u4f4d\u7f6e\u7f16\u7801\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u51e0\u4f55\u7ea6\u675f\u3002 * \u975e\u7ebf\u6027\u51e0\u4f55\u7f16\u7801\u7a7a\u95f4\u8ddd\u79bb\uff1a \u692d\u5706\u51fd\u6570\u7684\u975e\u7ebf\u6027\u51e0\u4f55\u7279\u6027\u80fd\u591f\u66f4\u81ea\u7136\u5730\u7f16\u7801\u7a7a\u95f4\u8ddd\u79bb\u5173\u7cfb\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u65e0\u6cd5\u5728\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\u8ddd\u79bb\u548c\u5e8f\u5217\u7d22\u5f15\u8ddd\u79bb\u4e4b\u95f4\u5efa\u7acb\u5355\u8c03\u5bf9\u5e94\u5173\u7cfb\u7684\u95ee\u9898\u3002 * \u4ee3\u6570\u52a0\u6cd5\u516c\u5f0f\u63a8\u5bfc\u76f8\u5bf9\u4f4d\u7f6e\uff1a \u8bba\u6587\u6307\u51fa\uff0c\u692d\u5706\u51fd\u6570\u7684\u4ee3\u6570\u52a0\u6cd5\u516c\u5f0f\u53ef\u4ee5\u76f4\u63a5\u4ece\u7edd\u5bf9\u4f4d\u7f6e\u7f16\u7801\u4e2d\u63a8\u5bfc\u51fa\u4efb\u610f\u8865\u4e01\u5bf9\u4e4b\u95f4\u7684\u76f8\u5bf9\u4f4d\u7f6e\u4fe1\u606f\uff0c\u8fd9\u5bf9\u4e8eTransformer\u7684\u6ce8\u610f\u529b\u673a\u5236\u81f3\u5173\u91cd\u8981\u3002 3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd \u63d0\u5347Vision Transformer\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\uff1a \u901a\u8fc7\u5f15\u5165\u66f4\u5f3a\u7684\u51e0\u4f55\u5f52\u7eb3\u504f\u7f6e\uff0cWEF-PE\u6709\u671b\u663e\u8457\u63d0\u5347ViT\u5728\u5404\u79cd\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u7cbe\u7ec6\u7a7a\u95f4\u7406\u89e3\u7684\u4efb\u52a1\u4e2d\u3002 \u63a8\u52a8\u4f4d\u7f6e\u7f16\u7801\u7814\u7a76\u7684\u65b0\u8303\u5f0f\uff1a \u672c\u6587\u4e3a\u4f4d\u7f6e\u7f16\u7801\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u7684\u3001\u57fa\u4e8e\u6570\u5b66\u539f\u7406\u7684\u89c6\u89d2\uff0c\u53ef\u80fd\u4f1a\u542f\u53d1\u7814\u7a76\u8005\u63a2\u7d22\u66f4\u591a\u9ad8\u7ea7\u6570\u5b66\u5de5\u5177\u6765\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u7ed3\u6784\u5316\u6570\u636e\u7f16\u7801\u95ee\u9898\u3002 \u589e\u5f3a\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\uff1a \u7406\u8bba\u5206\u6790\u548c\u6ce8\u610f\u529b\u53ef\u89c6\u5316\u8868\u660e\uff0cWEF-PE\u80fd\u5e26\u6765\u66f4\u8fde\u8d2f\u7684\u8bed\u4e49\u7126\u70b9\u548c\u66f4\u5f3a\u7684\u51e0\u4f55\u5f52\u7eb3\u504f\u7f6e\uff0c\u6709\u52a9\u4e8e\u7406\u89e3ViT\u5982\u4f55\u5904\u7406\u7a7a\u95f4\u4fe1\u606f\u3002 \u4e3a\u5176\u4ed6\u7ed3\u6784\u5316\u6570\u636e\u7f16\u7801\u63d0\u4f9b\u501f\u9274\uff1a \u8fd9\u79cd\u5229\u7528\u7279\u5b9a\u6570\u5b66\u51fd\u6570\u7279\u6027\u6765\u7f16\u7801\u6570\u636e\u7ed3\u6784\u7684\u601d\u60f3\uff0c\u53ef\u80fd\u88ab\u63a8\u5e7f\u5230\u5176\u4ed6\u9700\u8981\u4fdd\u7559\u590d\u6742\u7ed3\u6784\u4fe1\u606f\u7684\u9886\u57df\uff0c\u4f8b\u5982\u56fe\u795e\u7ecf\u7f51\u7edc\u30013D\u70b9\u4e91\u5904\u7406\u7b49\u3002 4. \u53ef\u80fd\u53d7\u76ca\u7684\u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 \u56fe\u50cf\u5206\u7c7b\u3001\u76ee\u6807\u68c0\u6d4b\u3001\u8bed\u4e49\u5206\u5272\uff1a \u4f5c\u4e3aViT\u7684\u57fa\u7840\u7ec4\u4ef6\uff0cWEF-PE\u5c06\u76f4\u63a5\u63d0\u5347\u8fd9\u4e9b\u6838\u5fc3\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u7684\u6027\u80fd\u3002 \u533b\u5b66\u56fe\u50cf\u5206\u6790\uff1a \u5728\u533b\u5b66\u56fe\u50cf\u4e2d\uff0c\u7cbe\u786e\u7684\u7a7a\u95f4\u5173\u7cfb\u548c\u51e0\u4f55\u7ed3\u6784\u81f3\u5173\u91cd\u8981\uff0cWEF-PE\u6709\u671b\u63d0\u9ad8\u8bca\u65ad\u548c\u5206\u5272\u7684\u51c6\u786e\u6027\u3002 \u9065\u611f\u56fe\u50cf\u5904\u7406\uff1a \u9065\u611f\u56fe\u50cf\u901a\u5e38\u5177\u6709\u5927\u5c3a\u5ea6\u3001\u91cd\u590d\u6a21\u5f0f\u548c\u5e73\u79fb\u4e0d\u53d8\u6027\uff0cWEF-PE\u7684\u53cc\u5468\u671f\u6027\u7279\u6027\u53ef\u80fd\u5728\u6b64\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002 \u89c6\u9891\u7406\u89e3\uff1a \u5982\u679c\u80fd\u5c06\u4e8c\u7ef4\u7684WEF-PE\u6269\u5c55\u5230\u4e09\u7ef4\uff08\u7a7a\u95f4+\u65f6\u95f4\uff09\uff0c\u5219\u53ef\u80fd\u5bf9\u89c6\u9891Transformer\u4e2d\u7684\u65f6\u7a7a\u4f4d\u7f6e\u7f16\u7801\u4ea7\u751f\u79ef\u6781\u5f71\u54cd\u3002 \u56fe\u50cf\u751f\u6210\u4e0e\u7f16\u8f91\uff1a \u66f4\u597d\u7684\u7a7a\u95f4\u7406\u89e3\u6709\u52a9\u4e8e\u751f\u6210\u66f4\u5177\u51e0\u4f55\u4e00\u81f4\u6027\u548c\u771f\u5b9e\u611f\u7684\u56fe\u50cf\u3002 5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u7684\u5c40\u9650\u6027 \u8ba1\u7b97\u590d\u6742\u6027\uff1a Weierstrass\u692d\u5706\u51fd\u6570\u5728\u6570\u5b66\u4e0a\u8f83\u4e3a\u590d\u6742\uff0c\u5176\u8ba1\u7b97\u5f00\u9500\u53ef\u80fd\u9ad8\u4e8e\u7b80\u5355\u7684\u53ef\u5b66\u4e60\u5d4c\u5165\u6216\u6b63\u5f26/\u4f59\u5f26\u7f16\u7801\uff0c\u6458\u8981\u4e2d\u672a\u63d0\u53ca\u5177\u4f53\u7684\u8ba1\u7b97\u6548\u7387\u5bf9\u6bd4\u3002 \u53c2\u6570\u9009\u62e9\u4e0e\u5b66\u4e60\uff1a \u692d\u5706\u51fd\u6570\u901a\u5e38\u6d89\u53ca\u5468\u671f\u3001\u6a21\u6570\u7b49\u53c2\u6570\u3002\u6458\u8981\u4e2d\u672a\u8be6\u7ec6\u8bf4\u660e\u8fd9\u4e9b\u53c2\u6570\u662f\u5982\u4f55\u786e\u5b9a\uff08\u4f8b\u5982\uff0c\u56fa\u5b9a\u3001\u53ef\u5b66\u4e60\u6216\u901a\u8fc7\u67d0\u79cd\u4f18\u5316\u8fc7\u7a0b\u5f97\u5230\uff09\uff0c\u8fd9\u53ef\u80fd\u5f15\u5165\u989d\u5916\u7684\u590d\u6742\u6027\u6216\u8c03\u4f18\u96be\u5ea6\u3002 \u901a\u7528\u6027\u4e0e\u6269\u5c55\u6027\uff1a \u8bba\u6587\u5f3a\u8c03\u4e86\u201c\u53cc\u5468\u671f\u6027\u201d\u4e0e2D\u56fe\u50cf\u7684\u5951\u5408\uff0c\u4f46\u5176\u76f4\u63a5\u5e94\u7528\u4e8e3D\u6570\u636e\uff08\u5982\u70b9\u4e91\u3001\u4f53\u7d20\uff09\u6216\u975e\u7f51\u683c\u7ed3\u6784\u6570\u636e\u7684\u80fd\u529b\u548c\u65b9\u6cd5\u672a\u5728\u6458\u8981\u4e2d\u63d0\u53ca\uff0c\u53ef\u80fd\u9700\u8981\u8fdb\u4e00\u6b65\u7684\u7406\u8bba\u548c\u65b9\u6cd5\u6269\u5c55\u3002 \u7406\u8bba\u4e0e\u5b9e\u8df5\u7684\u5e73\u8861\uff1a \u5c3d\u7ba1\u5f3a\u8c03\u4e86\u201c\u6570\u5b66\u539f\u7406\u201d\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u5982\u4f55\u5e73\u8861\u7406\u8bba\u7684\u4e25\u8c28\u6027\u4e0e\u5de5\u7a0b\u5b9e\u73b0\u7684\u6548\u7387\u548c\u9c81\u68d2\u6027\uff0c\u662f\u6240\u6709\u590d\u6742\u6570\u5b66\u65b9\u6cd5\u9762\u4e34\u7684\u6311\u6218\u3002 Key Findings: We propose Weierstrass Elliptic Function Positional Encoding (WEF-PE), a mathematically principled approach that directly addresses two-dimensional coordinates through natural complex domain representation, where the doubly periodic properties of elliptic functions align remarkably with translational invariance patterns commonly observed in visual data. Our method exploits the non-linear geometric nature of elliptic functions to encode spatial distance relationships naturally, while the algebraic addition formula enables direct derivation of relative positional information between arbitrary patch pairs from their absolute encodings. Links: PDF arXiv Why Relational Graphs Will Save the Next Generation of Vision Foundation Models? Authors: Fatemeh Ziaeetabar Published: 2025-08-25 Categories: cs.CV Abstract: Vision foundation models (FMs) have become the predominant architecture in computer vision, providing highly transferable representations learned from large-scale, multimodal corpora. Nonetheless, they exhibit persistent limitations on tasks that require explicit reasoning over entities, roles, and spatio-temporal relations. Such relational competence is indispensable for fine-grained human activity recognition, egocentric video understanding, and multimodal medical image analysis, where spatial, temporal, and semantic dependencies are decisive for performance. We advance the position that next-generation FMs should incorporate explicit relational interfaces, instantiated as dynamic relational graphs (graphs whose topology and edge semantics are inferred from the input and task context). We illustrate this position with cross-domain evidence from recent systems in human manipulation action recognition and brain tumor segmentation, showing that augmenting FMs with lightweight, context-adaptive graph-reasoning modules improves fine-grained semantic fidelity, out of distribution robustness, interpretability, and computational efficiency relative to FM only baselines. Importantly, by reasoning sparsely over semantic nodes, such hybrids also achieve favorable memory and hardware efficiency, enabling deployment under practical resource constraints. We conclude with a targeted research agenda for FM graph hybrids, prioritizing learned dynamic graph construction, multi-level relational reasoning (e.g., part object scene in activity understanding, or region organ in medical imaging), cross-modal fusion, and evaluation protocols that directly probe relational competence in structured vision tasks. Analysis: \u8fd9\u7bc7\u8bba\u6587\u6458\u8981\u63d0\u51fa\u4e86\u4e00\u79cd\u5173\u4e8e\u4e0b\u4e00\u4ee3\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08Vision Foundation Models, FMs\uff09\u53d1\u5c55\u65b9\u5411\u7684\u6df1\u523b\u89c1\u89e3\u3002\u4f5c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u4e13\u5bb6\uff0c\u6211\u5bf9\u8be5\u6458\u8981\u7684\u5206\u6790\u5982\u4e0b\uff1a \u8bba\u6587\u6458\u8981\u5206\u6790\uff1aWhy Relational Graphs Will Save the Next Generation of Vision Foundation Models? 1. \u8bba\u6587\u6838\u5fc3\u8d21\u732e\u7684\u7b80\u660e\u603b\u7ed3 (Concise Summary of Main Contribution) \u8bba\u6587\u6307\u51fa\u5f53\u524d\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u5728\u9700\u8981\u5b9e\u4f53\u3001\u89d2\u8272\u548c\u65f6\u7a7a\u5173\u7cfb\u663e\u5f0f\u63a8\u7406\u7684\u4efb\u52a1\u4e0a\u5b58\u5728\u5c40\u9650\u3002\u4e3a\u89e3\u51b3\u6b64\u95ee\u9898\uff0c\u8bba\u6587\u63d0\u51fa\u4e0b\u4e00\u4ee3\u57fa\u7840\u6a21\u578b\u5e94\u6574\u5408\u52a8\u6001\u5173\u7cfb\u56fe\uff08dynamic relational graphs\uff09\u4f5c\u4e3a\u663e\u5f0f\u5173\u7cfb\u63a5\u53e3\u3002\u8fd9\u79cd\u6df7\u5408\u6a21\u578b\u80fd\u663e\u8457\u63d0\u5347\u7ec6\u7c92\u5ea6\u8bed\u4e49\u7406\u89e3\u3001\u6cdb\u5316\u80fd\u529b\u3001\u53ef\u89e3\u91ca\u6027\u53ca\u8ba1\u7b97\u6548\u7387\uff0c\u4ece\u800c\u514b\u670d\u73b0\u6709FMs\u7684\u4e0d\u8db3\u3002 2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u8bba (Key Innovation or Methodological Approach) \u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u63d0\u51fa\u5c06\u201c\u52a8\u6001\u5173\u7cfb\u56fe\u201d\uff08dynamic relational graphs\uff09\u4f5c\u4e3a\u663e\u5f0f\u5173\u7cfb\u63a5\u53e3\u6574\u5408\u5230\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u4e2d\u3002\u8fd9\u4e9b\u56fe\u7684\u62d3\u6251\u7ed3\u6784\u548c\u8fb9\u8bed\u4e49\u662f\u6839\u636e\u8f93\u5165\u548c\u4efb\u52a1\u4e0a\u4e0b\u6587\u52a8\u6001\u63a8\u65ad\u7684\u3002\u901a\u8fc7\u8fd9\u79cd\u8f7b\u91cf\u7ea7\u3001\u4e0a\u4e0b\u6587\u81ea\u9002\u5e94\u7684\u56fe\u63a8\u7406\u6a21\u5757\u589e\u5f3a\u57fa\u7840\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u5bf9\u5b9e\u4f53\u3001\u89d2\u8272\u548c\u65f6\u7a7a\u5173\u7cfb\u7684\u7a00\u758f\u4e14\u9ad8\u6548\u7684\u63a8\u7406\uff0c\u4ece\u800c\u5f25\u8865\u4e86FMs\u5728\u590d\u6742\u5173\u7cfb\u7406\u89e3\u4e0a\u7684\u4e0d\u8db3\u3002 3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd (Potential Impact on the Field) \u8fd9\u9879\u7814\u7a76\u6709\u671b\u63a8\u52a8\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u8303\u5f0f\u7684\u6f14\u8fdb\uff0c\u4f7f\u5176\u4ece\u4e3b\u8981\u4f9d\u8d56\u5927\u89c4\u6a21\u6570\u636e\u5b66\u4e60\u9690\u5f0f\u8868\u793a\uff0c\u8f6c\u5411\u80fd\u591f\u8fdb\u884c\u663e\u5f0f\u3001\u7ed3\u6784\u5316\u63a8\u7406\u3002\u8fd9\u5c06\u663e\u8457\u62d3\u5bbd\u57fa\u7840\u6a21\u578b\u5728\u9700\u8981\u590d\u6742\u5173\u7cfb\u7406\u89e3\uff08\u5982\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\u3001\u533b\u7597\u5f71\u50cf\u5206\u6790\uff09\u9886\u57df\u7684\u5e94\u7528\u8303\u56f4\u548c\u6027\u80fd\u4e0a\u9650\u3002\u540c\u65f6\uff0c\u63d0\u5347\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u8d44\u6e90\u6548\u7387\uff0c\u5bf9\u4e8e\u57fa\u7840\u6a21\u578b\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u90e8\u7f72\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u53ef\u80fd\u5f15\u9886\u4e0b\u4e00\u4ee3FMs\u7684\u8bbe\u8ba1\u65b9\u5411\u3002 4. \u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 (Related Areas or Applications that Might Benefit) \u7ec6\u7c92\u5ea6\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b (Fine-grained Human Activity Recognition): \u7406\u89e3\u590d\u6742\u52a8\u4f5c\u5e8f\u5217\u4e2d\u7684\u5b9e\u4f53\u3001\u5de5\u5177\u3001\u4ea4\u4e92\u548c\u65f6\u5e8f\u5173\u7cfb\u3002 \u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u89c6\u9891\u7406\u89e3 (Egocentric Video Understanding): \u5206\u6790\u4f69\u6234\u8005\u89c6\u89d2\u4e0b\u7684\u7269\u4f53\u4ea4\u4e92\u3001\u610f\u56fe\u548c\u73af\u5883\u5173\u7cfb\u3002 \u591a\u6a21\u6001\u533b\u5b66\u56fe\u50cf\u5206\u6790 (Multimodal Medical Image Analysis): \u6574\u5408\u4e0d\u540c\u6a21\u6001\uff08\u5982MRI\u3001CT\uff09\u4fe1\u606f\uff0c\u63a8\u7406\u75c5\u7076\u3001\u5668\u5b98\u4e4b\u95f4\u7684\u7a7a\u95f4\u3001\u8bed\u4e49\u5173\u7cfb\uff0c\u4f8b\u5982\u8111\u80bf\u7624\u5206\u5272\u548c\u75be\u75c5\u8bca\u65ad\u3002 \u673a\u5668\u4eba\u64cd\u4f5c\u4e0e\u5177\u8eab\u667a\u80fd (Robotic Manipulation and Embodied AI): \u89c4\u5212\u548c\u6267\u884c\u590d\u6742\u4efb\u52a1\uff0c\u9700\u8981\u7406\u89e3\u7269\u4f53\u5c5e\u6027\u3001\u73af\u5883\u7ea6\u675f\u548c\u64cd\u4f5c\u5e8f\u5217\u3002 \u573a\u666f\u56fe\u751f\u6210\u4e0e\u89c6\u89c9\u95ee\u7b54 (Scene Graph Generation and Visual Question Answering): \u66f4\u51c6\u786e\u5730\u6355\u6349\u56fe\u50cf\u4e2d\u7684\u5b9e\u4f53\u53ca\u5176\u5173\u7cfb\uff0c\u652f\u6301\u66f4\u6df1\u5c42\u6b21\u7684\u8bed\u4e49\u7406\u89e3\u548c\u63a8\u7406\u3002 5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u7684\u5c40\u9650\u6027 (Limitations that Can Be Inferred from the Abstract) \u6027\u8d28\u4e3a\u5c55\u671b\u6027\u6216\u7efc\u8ff0\u6027 (Prospective/Review Nature): \u9274\u4e8e\u53d1\u5e03\u65e5\u671f\u662f2025\u5e74\uff0c\u4e14\u6458\u8981\u4e2d\u4f7f\u7528\u4e86\u201cWe advance the position\u201d\u3001\u201cWe illustrate this position with cross-domain evidence\u201d\u548c\u201cWe conclude with a targeted research agenda\u201d\u7b49\u8868\u8ff0\uff0c\u8fd9\u7bc7\u8bba\u6587\u66f4\u50cf\u662f\u4e00\u7bc7\u63d0\u51fa\u7814\u7a76\u65b9\u5411\u3001\u603b\u7ed3\u73b0\u6709\u8bc1\u636e\u5e76\u89c4\u5212\u672a\u6765\u8def\u7ebf\u56fe\u7684\u5c55\u671b\u6027\u6216\u7efc\u8ff0\u6027\u6587\u7ae0\uff0c\u800c\u975e\u4e00\u7bc7\u63d0\u51fa\u5168\u65b0\u6a21\u578b\u67b6\u6784\u5e76\u63d0\u4f9b\u5927\u91cf\u65b0\u5b9e\u9a8c\u7ed3\u679c\u7684\u5b9e\u8bc1\u8bba\u6587\u3002\u5176\u201c\u8bc1\u636e\u201d\u53ef\u80fd\u6765\u81ea\u5bf9\u73b0\u6709\u5de5\u4f5c\u7684\u7efc\u5408\u5206\u6790\uff0c\u800c\u975e\u672c\u6587\u9996\u6b21\u63d0\u51fa\u7684\u65b0\u5b9e\u9a8c\u3002 \u52a8\u6001\u56fe\u6784\u5efa\u7684\u590d\u6742\u6027 (Complexity of Dynamic Graph Construction): \u6458\u8981\u4e2d\u5f3a\u8c03\u4e86\u201clearned dynamic graph construction\u201d\u662f\u672a\u6765\u7684\u7814\u7a76\u91cd\u70b9\uff0c\u8fd9\u6697\u793a\u4e86\u5982\u4f55\u9ad8\u6548\u3001\u51c6\u786e\u5730\u4ece\u539f\u59cb\u8f93\u5165\u548c\u4efb\u52a1\u4e0a\u4e0b\u6587\u4e2d\u52a8\u6001\u63a8\u65ad\u51fa\u6700\u4f18\u7684\u56fe\u62d3\u6251\u548c\u8fb9\u8bed\u4e49\uff0c\u672c\u8eab\u5c31\u662f\u4e00\u4e2a\u590d\u6742\u4e14\u5c1a\u672a\u5b8c\u5168\u89e3\u51b3\u7684\u6311\u6218\u3002 \u201c\u8f7b\u91cf\u7ea7\u201d\u7684\u5b9a\u4e49\u4e0e\u6743\u8861 (Definition and Trade-offs of \"Lightweight\"): \u5c3d\u7ba1\u58f0\u79f0\u201clightweight\u201d\u548c\u201cfavorable memory and hardware efficiency\u201d\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u56fe\u63a8\u7406\u6a21\u5757\u7684\u590d\u6742\u6027\u3001\u56fe\u7684\u5927\u5c0f\u4ee5\u53ca\u4e0e\u57fa\u7840\u6a21\u578b\u878d\u5408\u7684\u5f00\u9500\uff0c\u4ecd\u53ef\u80fd\u5e26\u6765\u989d\u5916\u7684\u8ba1\u7b97\u8d1f\u62c5\uff0c\u9700\u8981\u4ed4\u7ec6\u7684\u5de5\u7a0b\u4f18\u5316\u548c\u6743\u8861\u3002 \u8bc4\u4f30\u534f\u8bae\u7684\u7f3a\u5931 (Lack of Established Evaluation Protocols): \u6458\u8981\u4e2d\u63d0\u5230\u9700\u8981\u201cevaluation protocols that directly probe relational competence\u201d\uff0c\u8fd9\u8868\u660e\u76ee\u524d\u53ef\u80fd\u7f3a\u4e4f\u6807\u51c6\u5316\u7684\u3001\u80fd\u591f\u5145\u5206\u8861\u91cf\u6a21\u578b\u5173\u7cfb\u63a8\u7406\u80fd\u529b\u7684\u57fa\u51c6\u548c\u8bc4\u4f30\u65b9\u6cd5\uff0c\u8fd9\u4f1a\u7ed9\u7814\u7a76\u8fdb\u5c55\u5e26\u6765\u6311\u6218\u3002 Key Findings: Importantly, by reasoning sparsely over semantic nodes, such hybrids also achieve favorable memory and hardware efficiency, enabling deployment under practical resource constraints. We conclude with a targeted research agenda for FM graph hybrids, prioritizing learned dynamic graph construction, multi-level relational reasoning (e.g., part object scene in activity understanding, or region organ in medical imaging), cross-modal fusion, and evaluation protocols that directly probe relational competence in structured vision tasks. Links: PDF arXiv Enhanced Drift-Aware Computer Vision Architecture for Autonomous Driving Authors: Md Shahi Amran Hossain, Abu Shad Ahammed, Sayeri Mukherjee, Roman Obermaisser Published: 2025-08-25 Categories: cs.CV, math.LO Abstract: The use of computer vision in automotive is a trending research in which safety and security are a primary concern. In particular, for autonomous driving, preventing road accidents requires highly accurate object detection under diverse conditions. To address this issue, recently the International Organization for Standardization (ISO) released the 8800 norm, providing structured frameworks for managing associated AI relevant risks. However, challenging scenarios such as adverse weather or low lighting often introduce data drift, leading to degraded model performance and potential safety violations. In this work, we present a novel hybrid computer vision architecture trained with thousands of synthetic image data from the road environment to improve robustness in unseen drifted environments. Our dual mode framework utilized YOLO version 8 for swift detection and incorporated a five-layer CNN for verification. The system functioned in sequence and improved the detection accuracy by more than 90\\% when tested with drift-augmented road images. The focus was to demonstrate how such a hybrid model can provide better road safety when working together in a hybrid structure. Analysis: \u597d\u7684\uff0c\u4f5c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u4e13\u5bb6\uff0c\u6211\u5c06\u5bf9\u8fd9\u7bc7\u8bba\u6587\u6458\u8981\u8fdb\u884c\u6280\u672f\u6027\u5206\u6790\u3002 \u8bba\u6587\u6458\u8981\u5206\u6790\uff1aEnhanced Drift-Aware Computer Vision Architecture for Autonomous Driving 1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u660e\u603b\u7ed3 (2-3 \u53e5\u8bdd) \u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6df7\u5408\u8ba1\u7b97\u673a\u89c6\u89c9\u67b6\u6784\uff0c\u65e8\u5728\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u4e2d\u56e0\u6570\u636e\u6f02\u79fb\uff08\u5982\u6076\u52a3\u5929\u6c14\u3001\u4f4e\u5149\u7167\uff09\u5bfc\u81f4\u7684\u7269\u4f53\u68c0\u6d4b\u6027\u80fd\u4e0b\u964d\u548c\u6f5c\u5728\u5b89\u5168\u95ee\u9898\u3002\u8be5\u67b6\u6784\u7ed3\u5408\u4e86YOLOv8\u8fdb\u884c\u5feb\u901f\u68c0\u6d4b\u548c\u4e00\u4e2a\u4e94\u5c42CNN\u8fdb\u884c\u9a8c\u8bc1\uff0c\u901a\u8fc7\u5e8f\u5217\u5316\u5de5\u4f5c\u6d41\u7a0b\u548c\u5408\u6210\u6570\u636e\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u6f02\u79fb\u589e\u5f3a\u56fe\u50cf\u4e0a\u7684\u68c0\u6d4b\u51c6\u786e\u7387\uff0c\u4ece\u800c\u589e\u5f3a\u4e86\u9053\u8def\u5b89\u5168\u6027\u3002 2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u5b66\u65b9\u6cd5 \u8be5\u7814\u7a76\u7684\u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u5176 \u53cc\u6a21\u6001\u6df7\u5408\u67b6\u6784\uff08dual mode hybrid architecture\uff09 \u548c \u6f02\u79fb\u611f\u77e5\uff08drift-aware\uff09 \u7684\u8bbe\u8ba1\u7406\u5ff5\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\uff1a * \u6df7\u5408\u67b6\u6784\u4e0e\u5e8f\u5217\u5316\u9a8c\u8bc1\uff1a \u7cfb\u7edf\u91c7\u7528YOLOv8\u4f5c\u4e3a\u7b2c\u4e00\u9636\u6bb5\uff0c\u5b9e\u73b0\u5feb\u901f\u7269\u4f53\u68c0\u6d4b\uff1b\u968f\u540e\uff0c\u4e00\u4e2a\u4e13\u95e8\u7684\u4e94\u5c42\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u4f5c\u4e3a\u7b2c\u4e8c\u9636\u6bb5\uff0c\u5bf9YOLOv8\u7684\u68c0\u6d4b\u7ed3\u679c\u8fdb\u884c\u9a8c\u8bc1\u3002\u8fd9\u79cd\u201c\u5feb\u901f\u68c0\u6d4b + \u6df1\u5ea6\u9a8c\u8bc1\u201d\u7684\u5e8f\u5217\u5316\u5de5\u4f5c\u6d41\u662f\u5176\u72ec\u7279\u4e4b\u5904\uff0c\u65e8\u5728\u5e73\u8861\u901f\u5ea6\u4e0e\u9c81\u68d2\u6027\u3002 * \u5408\u6210\u6570\u636e\u8bad\u7ec3\uff1a \u4e3a\u4e86\u63d0\u9ad8\u6a21\u578b\u5728\u201c\u672a\u89c1\u8fc7\u7684\u6f02\u79fb\u73af\u5883\uff08unseen drifted environments\uff09\u201d\u4e2d\u7684\u9c81\u68d2\u6027\uff0c\u8be5\u67b6\u6784\u5229\u7528\u4e86\u6570\u5343\u5f20\u5408\u6210\u56fe\u50cf\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u3002\u8fd9\u6709\u52a9\u4e8e\u6a21\u578b\u5b66\u4e60\u5e76\u9002\u5e94\u5404\u79cd\u6f5c\u5728\u7684\u6f02\u79fb\u6761\u4ef6\uff0c\u800c\u65e0\u9700\u4f9d\u8d56\u96be\u4ee5\u83b7\u53d6\u7684\u771f\u5b9e\u4e16\u754c\u6f02\u79fb\u6570\u636e\u3002 * \u4e13\u6ce8\u4e8e\u6570\u636e\u6f02\u79fb\u9c81\u68d2\u6027\uff1a \u8bba\u6587\u660e\u786e\u5c06\u6570\u636e\u6f02\u79fb\uff08\u7531\u6076\u52a3\u5929\u6c14\u3001\u4f4e\u5149\u7167\u7b49\u5f15\u8d77\uff09\u4f5c\u4e3a\u6838\u5fc3\u6311\u6218\uff0c\u5e76\u8bbe\u8ba1\u67b6\u6784\u6765\u76f4\u63a5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u8fd9\u4e0eISO 8800\u7b49\u65b0\u5174\u6807\u51c6\u5bf9AI\u98ce\u9669\u7ba1\u7406\u7684\u5173\u6ce8\u76f8\u543b\u5408\u3002 3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd \u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u5b89\u5168\u6027\uff1a \u901a\u8fc7\u6709\u6548\u5e94\u5bf9\u6570\u636e\u6f02\u79fb\uff0c\u8be5\u7814\u7a76\u6709\u671b\u663e\u8457\u63d0\u9ad8\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5728\u590d\u6742\u548c\u6076\u52a3\u6761\u4ef6\u4e0b\u7684\u7269\u4f53\u68c0\u6d4b\u53ef\u9760\u6027\uff0c\u76f4\u63a5\u5173\u7cfb\u5230\u884c\u8f66\u5b89\u5168\uff0c\u5e76\u53ef\u80fd\u6210\u4e3a\u672a\u6765AD\u7cfb\u7edf\u8bbe\u8ba1\u4e2d\u7684\u4e00\u4e2a\u91cd\u8981\u8003\u91cf\u3002 \u63a8\u52a8\u6df7\u5408\u6a21\u578b\u8303\u5f0f\uff1a \u8fd9\u79cd\u201c\u5feb\u901f\u68c0\u6d4b\u5668 + \u9a8c\u8bc1\u5668\u201d\u7684\u6df7\u5408\u67b6\u6784\u4e3a\u5176\u4ed6\u5bf9\u5b9e\u65f6\u6027\u3001\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u90fd\u6709\u9ad8\u8981\u6c42\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u7684\u8bbe\u8ba1\u8303\u5f0f\u3002 \u5f3a\u8c03\u5408\u6210\u6570\u636e\u4ef7\u503c\uff1a \u8bba\u6587\u518d\u6b21\u8bc1\u660e\u4e86\u5408\u6210\u6570\u636e\u5728\u8bad\u7ec3\u9c81\u68d2\u6a21\u578b\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u7f55\u89c1\u6216\u96be\u4ee5\u83b7\u53d6\u7684\u6f02\u79fb\u6570\u636e\u65b9\u9762\u7684\u5de8\u5927\u6f5c\u529b\u3002 \u7b26\u5408\u884c\u4e1a\u6807\u51c6\uff1a \u63d0\u53caISO 8800\u8868\u660e\u8be5\u7814\u7a76\u5177\u6709\u5f88\u5f3a\u7684\u5b9e\u9645\u5e94\u7528\u5bfc\u5411\uff0c\u5176\u6210\u679c\u53ef\u80fd\u6709\u52a9\u4e8e\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u6ee1\u8db3\u672a\u6765\u7684\u5b89\u5168\u548c\u98ce\u9669\u7ba1\u7406\u6807\u51c6\u3002 4. \u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 \u9664\u4e86\u81ea\u52a8\u9a7e\u9a76\uff0c\u4ee5\u4e0b\u9886\u57df\u6216\u5e94\u7528\u4e5f\u53ef\u80fd\u4ece\u8fd9\u9879\u7814\u7a76\u4e2d\u53d7\u76ca\uff1a * \u5de5\u4e1a\u81ea\u52a8\u5316\u4e0e\u673a\u5668\u4eba\uff1a \u5728\u751f\u4ea7\u7ebf\u6216\u4ed3\u5e93\u4e2d\uff0c\u5149\u7167\u3001\u7070\u5c18\u3001\u78e8\u635f\u7b49\u73af\u5883\u53d8\u5316\u53ef\u80fd\u5bfc\u81f4\u6570\u636e\u6f02\u79fb\uff0c\u5f71\u54cd\u673a\u5668\u4eba\u7684\u89c6\u89c9\u611f\u77e5\u548c\u64cd\u4f5c\u7cbe\u5ea6\u3002 * \u667a\u80fd\u76d1\u63a7\u7cfb\u7edf\uff1a \u6237\u5916\u76d1\u63a7\u6444\u50cf\u5934\u5e38\u9762\u4e34\u5929\u6c14\u3001\u663c\u591c\u3001\u5b63\u8282\u53d8\u5316\u7b49\u6f02\u79fb\uff0c\u8be5\u65b9\u6cd5\u53ef\u7528\u4e8e\u63d0\u9ad8\u5f02\u5e38\u4e8b\u4ef6\u68c0\u6d4b\u7684\u9c81\u68d2\u6027\u3002 * \u533b\u7597\u5f71\u50cf\u5206\u6790\uff1a \u4e0d\u540c\u7684\u8bbe\u5907\u3001\u626b\u63cf\u53c2\u6570\u6216\u60a3\u8005\u751f\u7406\u53d8\u5316\u53ef\u80fd\u5f15\u5165\u6570\u636e\u6f02\u79fb\uff0c\u5f71\u54cd\u75be\u75c5\u8bca\u65ad\u7684\u51c6\u786e\u6027\u3002 * \u822a\u7a7a\u822a\u5929\u4e0e\u65e0\u4eba\u673a\uff1a \u5728\u590d\u6742\u5927\u6c14\u6761\u4ef6\u6216\u672a\u77e5\u73af\u5883\u4e2d\u6267\u884c\u4efb\u52a1\u65f6\uff0c\u89c6\u89c9\u7cfb\u7edf\u9700\u8981\u6781\u9ad8\u7684\u9c81\u68d2\u6027\u3002 * \u4efb\u4f55\u5728\u975e\u53d7\u63a7\u73af\u5883\u4e2d\u90e8\u7f72\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u7cfb\u7edf\uff1a \u51e1\u662f\u73af\u5883\u6761\u4ef6\u591a\u53d8\u3001\u6570\u636e\u5206\u5e03\u53ef\u80fd\u968f\u65f6\u95f4\u6216\u6761\u4ef6\u53d8\u5316\u7684\u573a\u666f\uff0c\u8fd9\u79cd\u6f02\u79fb\u611f\u77e5\u548c\u9a8c\u8bc1\u7684\u67b6\u6784\u90fd\u5177\u6709\u501f\u9274\u610f\u4e49\u3002 5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u7684\u5c40\u9650\u6027 \u5408\u6210\u6570\u636e\u4e0e\u771f\u5b9e\u4e16\u754c\u7684\u5dee\u8ddd\uff08Domain Gap\uff09\uff1a \u5c3d\u7ba1\u5408\u6210\u6570\u636e\u6709\u52a9\u4e8e\u89e3\u51b3\u6f02\u79fb\u95ee\u9898\uff0c\u4f46\u5408\u6210\u6570\u636e\u4e0e\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e4b\u95f4\u901a\u5e38\u5b58\u5728\u9886\u57df\u5dee\u8ddd\u3002\u6a21\u578b\u5728\u201c\u6f02\u79fb\u589e\u5f3a\u7684\u9053\u8def\u56fe\u50cf\u201d\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5728\u771f\u5b9e\u4e16\u754c\u3001\u672a\u89c1\u8fc7\u7684\u3001\u9ad8\u5ea6\u590d\u6742\u7684\u6f02\u79fb\u6761\u4ef6\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u4ecd\u9700\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u3002 \u6027\u80fd\u6307\u6807\u7684\u76f8\u5bf9\u6027\uff1a \u201c\u68c0\u6d4b\u51c6\u786e\u7387\u63d0\u9ad8\u4e8690%\u4ee5\u4e0a\u201d\u662f\u4e00\u4e2a\u76f8\u5bf9\u63d0\u5347\u3002\u6458\u8981\u672a\u63d0\u4f9b\u57fa\u7ebf\u6a21\u578b\u7684\u7edd\u5bf9\u51c6\u786e\u7387\uff0c\u4e5f\u672a\u8bf4\u660e\u63d0\u534790%\u662f\u57fa\u4e8e\u54ea\u4e2a\u521d\u59cb\u503c\u3002\u4f8b\u5982\uff0c\u4ece10%\u63d0\u5347\u523019%\u4e5f\u662f90%\u7684\u63d0\u5347\uff0c\u4f46\u7edd\u5bf9\u6027\u80fd\u53ef\u80fd\u4ecd\u4e0d\u8db3\u4ee5\u6ee1\u8db3\u81ea\u52a8\u9a7e\u9a76\u7684\u9700\u6c42\u3002 \u5e8f\u5217\u5316\u5904\u7406\u7684\u5b9e\u65f6\u6027\u8003\u91cf\uff1a \u5c3d\u7ba1YOLOv8\u4ee5\u201cswift detection\u201d\u8457\u79f0\uff0c\u4f46\u589e\u52a0\u4e00\u4e2a\u4e94\u5c42CNN\u7684\u201c\u9a8c\u8bc1\u201d\u6b65\u9aa4\uff0c\u5fc5\u7136\u4f1a\u5f15\u5165\u989d\u5916\u7684\u8ba1\u7b97\u5ef6\u8fdf\u3002\u5bf9\u4e8e\u81ea\u52a8\u9a7e\u9a76\u8fd9\u79cd\u5bf9\u5b9e\u65f6\u6027\u8981\u6c42\u6781\u9ad8\u7684\u5e94\u7528\uff0c\u8fd9\u79cd\u5ef6\u8fdf\u662f\u5426\u5728\u53ef\u63a5\u53d7\u8303\u56f4\u5185\uff0c\u4ee5\u53ca\u5982\u4f55\u4f18\u5316\u4ee5\u6ee1\u8db3\u5b9e\u65f6\u6027\u8981\u6c42\uff0c\u662f\u9700\u8981\u5173\u6ce8\u7684\u95ee\u9898\u3002 \u4e94\u5c42CNN\u7684\u9a8c\u8bc1\u80fd\u529b\uff1a \u6458\u8981\u4e2d\u5bf9\u201c\u4e94\u5c42CNN\u201d\u7684\u63cf\u8ff0\u76f8\u5bf9\u7b80\u5355\u3002\u5176\u9a8c\u8bc1\u673a\u5236\u3001\u590d\u6742\u5ea6\u548c\u5bf9\u5404\u79cd\u6f02\u79fb\u6a21\u5f0f\u7684\u9c81\u68d2\u6027\u5982\u4f55\uff0c\u4ee5\u53ca\u5b83\u662f\u5426\u8db3\u4ee5\u5e94\u5bf9\u81ea\u52a8\u9a7e\u9a76\u4e2d\u53ef\u80fd\u51fa\u73b0\u7684\u6781\u7aef\u548c\u591a\u6837\u5316\u7684\u6f02\u79fb\u60c5\u51b5\uff0c\u4ecd\u6709\u5f85\u8be6\u7ec6\u8bf4\u660e\u3002 \u6f02\u79fb\u7c7b\u578b\u7684\u8986\u76d6\u8303\u56f4\uff1a \u6458\u8981\u63d0\u5230\u4e86\u201c\u6076\u52a3\u5929\u6c14\u6216\u4f4e\u5149\u7167\u201d\u4f5c\u4e3a\u6f02\u79fb\u6765\u6e90\uff0c\u4f46\u672a\u8be6\u7ec6\u8bf4\u660e\u6a21\u578b\u80fd\u591f\u5e94\u5bf9\u7684\u6f02\u79fb\u7c7b\u578b\u548c\u7a0b\u5ea6\u3002\u4f8b\u5982\uff0c\u5bf9\u4e8e\u4f20\u611f\u5668\u6545\u969c\u3001\u906e\u6321\u3001\u5bf9\u6297\u6027\u653b\u51fb\u7b49\u5176\u4ed6\u5f62\u5f0f\u7684\u201c\u6f02\u79fb\u201d\u6216\u5f02\u5e38\u60c5\u51b5\uff0c\u8be5\u67b6\u6784\u7684\u6709\u6548\u6027\u5982\u4f55\uff1f Key Findings: In this work, we present a novel hybrid computer vision architecture trained with thousands of synthetic image data from the road environment to improve robustness in unseen drifted environments. Links: PDF arXiv Scene-Agnostic Traversability Labeling and Estimation via a Multimodal Self-supervised Framework Authors: Zipeng Fang, Yanbo Wang, Lei Zhao, Weidong Chen Published: 2025-08-25 Categories: cs.RO, cs.CV Abstract: Traversability estimation is critical for enabling robots to navigate across diverse terrains and environments. While recent self-supervised learning methods achieve promising results, they often fail to capture the characteristics of non-traversable regions. Moreover, most prior works concentrate on a single modality, overlooking the complementary strengths offered by integrating heterogeneous sensory modalities for more robust traversability estimation. To address these limitations, we propose a multimodal self-supervised framework for traversability labeling and estimation. First, our annotation pipeline integrates footprint, LiDAR, and camera data as prompts for a vision foundation model, generating traversability labels that account for both semantic and geometric cues. Then, leveraging these labels, we train a dual-stream network that jointly learns from different modalities in a decoupled manner, enhancing its capacity to recognize diverse traversability patterns. In addition, we incorporate sparse LiDAR-based supervision to mitigate the noise introduced by pseudo labels. Finally, extensive experiments conducted across urban, off-road, and campus environments demonstrate the effectiveness of our approach. The proposed automatic labeling method consistently achieves around 88% IoU across diverse datasets. Compared to existing self-supervised state-of-the-art methods, our multimodal traversability estimation network yields consistently higher IoU, improving by 1.6-3.5% on all evaluated datasets. Analysis: \u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6a21\u6001\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u53ef\u901a\u884c\u6027\u533a\u57df\u7684\u6807\u6ce8\u4e0e\u4f30\u8ba1\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u8bc6\u522b\u4e0d\u53ef\u901a\u884c\u533a\u57df\u548c\u6709\u6548\u5229\u7528\u591a\u6a21\u6001\u4fe1\u606f\u65b9\u9762\u7684\u4e0d\u8db3\u3002 1. \u8bba\u6587\u6838\u5fc3\u8d21\u732e\u603b\u7ed3 (Concise Summary) \u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u53ef\u901a\u884c\u6027\u533a\u57df\u7684\u6807\u6ce8\u4e0e\u4f30\u8ba1\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u8bc6\u522b\u4e0d\u53ef\u901a\u884c\u533a\u57df\u548c\u5229\u7528\u591a\u6a21\u6001\u4fe1\u606f\u65b9\u9762\u7684\u4e0d\u8db3\u3002\u5b83\u901a\u8fc7\u6574\u5408\u8db3\u8ff9\u3001LiDAR\u548c\u76f8\u673a\u6570\u636e\u4f5c\u4e3a\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u63d0\u793a\u6765\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u4f2a\u6807\u7b7e\uff0c\u5e76\u5229\u7528\u8fd9\u4e9b\u6807\u7b7e\u8bad\u7ec3\u4e00\u4e2a\u53cc\u6d41\u7f51\u7edc\uff0c\u8f85\u4ee5\u7a00\u758fLiDAR\u76d1\u7763\u4ee5\u63d0\u9ad8\u9c81\u68d2\u6027\uff0c\u6700\u7ec8\u5728\u591a\u6837\u5316\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002 2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u8bba (Key Innovation or Methodological Approach) \u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u5176\u591a\u6a21\u6001\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u7279\u522b\u662f\u5728\u4f2a\u6807\u7b7e\u751f\u6210\u548c\u7f51\u7edc\u8bad\u7ec3\u9636\u6bb5\u3002 1. \u521b\u65b0\u6027\u4f2a\u6807\u7b7e\u751f\u6210\uff1a \u8be5\u65b9\u6cd5\u521b\u65b0\u6027\u5730\u5229\u7528\u8db3\u8ff9\uff08footprint\uff09\u3001LiDAR\u548c\u76f8\u673a\u6570\u636e\u4f5c\u4e3a\u63d0\u793a\uff08prompts\uff09\uff0c\u9a71\u52a8\u4e00\u4e2a \u89c6\u89c9\u57fa\u7840\u6a21\u578b \u6765\u751f\u6210\u7ed3\u5408\u8bed\u4e49\u548c\u51e0\u4f55\u7ebf\u7d22\u7684\u53ef\u901a\u884c\u6027\u6807\u7b7e\u3002\u8fd9\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u573a\u666f\u65e0\u5173\u7684\u4f2a\u6807\u7b7e\u751f\u6210\u7b56\u7565\uff0c\u5c24\u5176\u89e3\u51b3\u4e86\u4f20\u7edf\u81ea\u76d1\u7763\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u4e0d\u53ef\u901a\u884c\u533a\u57df\u7279\u5f81\u7684\u95ee\u9898\u3002 2. \u89e3\u8026\u5f0f\u591a\u6a21\u6001\u5b66\u4e60\u7f51\u7edc\uff1a \u63d0\u51fa\u7684\u53cc\u6d41\u7f51\u7edc\u4ee5 \u89e3\u8026\u65b9\u5f0f \u8054\u5408\u5b66\u4e60\u4e0d\u540c\u6a21\u6001\u7684\u7279\u5f81\uff0c\u589e\u5f3a\u4e86\u5bf9\u591a\u6837\u5316\u53ef\u901a\u884c\u6027\u6a21\u5f0f\u7684\u8bc6\u522b\u80fd\u529b\uff0c\u5145\u5206\u5229\u7528\u4e86\u5f02\u6784\u4f20\u611f\u5668\u7684\u4e92\u8865\u4f18\u52bf\u3002 3. \u7a00\u758fLiDAR\u76d1\u7763\u7f13\u89e3\u566a\u58f0\uff1a \u5f15\u5165 \u7a00\u758fLiDAR\u76d1\u7763 \u6765\u6709\u6548\u7f13\u89e3\u4f2a\u6807\u7b7e\u5e26\u6765\u7684\u566a\u58f0\uff0c\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\uff0c\u5f25\u8865\u4e86\u7eaf\u81ea\u76d1\u7763\u65b9\u6cd5\u53ef\u80fd\u5b58\u5728\u7684\u8bef\u5dee\u3002 3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd (Potential Impact on the Field) \u8be5\u7814\u7a76\u5bf9\u673a\u5668\u4eba\u5bfc\u822a\u9886\u57df\u5177\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u5b83\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u9c81\u68d2\u3001\u66f4\u901a\u7528\u7684\u53ef\u901a\u884c\u6027\u4f30\u8ba1\u65b9\u6cd5\uff0c\u4f7f\u673a\u5668\u4eba\u5728\u672a\u77e5\u548c\u590d\u6742\u5730\u5f62\u4e2d\u81ea\u4e3b\u5bfc\u822a\u7684\u80fd\u529b\u5f97\u5230\u663e\u8457\u63d0\u5347\u3002\u5176\u81ea\u76d1\u7763\u7684\u4f2a\u6807\u7b7e\u751f\u6210\u7b56\u7565\uff0c\u7279\u522b\u662f\u7ed3\u5408\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u548c\u591a\u6a21\u6001\u63d0\u793a\uff0c\u4e3a\u51cf\u5c11\u6570\u636e\u6807\u6ce8\u6210\u672c\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u5bf9\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u4e2d\u5176\u4ed6\u9700\u8981\u5927\u91cf\u6807\u6ce8\u7684\u611f\u77e5\u4efb\u52a1\u5177\u6709\u501f\u9274\u610f\u4e49\u3002\u6b64\u5916\uff0c\u5b83\u4e5f\u4e3a\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u5728\u673a\u5668\u4eba\u611f\u77e5\u4e2d\u7684\u5e94\u7528\u6811\u7acb\u4e86\u65b0\u7684\u8303\u4f8b\u3002 4. \u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 (Related Areas or Applications) \u8fd9\u9879\u7814\u7a76\u7684\u6210\u679c\u53ef\u4ee5\u5e7f\u6cdb\u5e94\u7528\u4e8e\u4ee5\u4e0b\u9886\u57df\uff1a * \u81ea\u52a8\u9a7e\u9a76\u4e0e\u65e0\u4eba\u8f66\uff1a \u7279\u522b\u662f\u5728\u975e\u7ed3\u6784\u5316\u9053\u8def\u3001\u8d8a\u91ce\u73af\u5883\u6216\u590d\u6742\u57ce\u5e02\u533a\u57df\u7684\u8def\u5f84\u89c4\u5212\u548c\u51b3\u7b56\u3002 * \u641c\u6551\u673a\u5668\u4eba\uff1a \u5728\u707e\u540e\u5e9f\u589f\u3001\u5d0e\u5c96\u5730\u5f62\u4e2d\u8fdb\u884c\u641c\u7d22\u548c\u6551\u63f4\u4efb\u52a1\u3002 * \u7a7a\u95f4\u63a2\u7d22\u4e0e\u519b\u4e8b\u673a\u5668\u4eba\uff1a \u5728\u672a\u77e5\u6216\u6781\u7aef\u73af\u5883\u4e2d\u8fdb\u884c\u81ea\u4e3b\u63a2\u6d4b\u548c\u884c\u52a8\u3002 * \u519c\u4e1a\u673a\u5668\u4eba\uff1a \u5728\u519c\u7530\u4e2d\u8bc6\u522b\u53ef\u901a\u884c\u533a\u57df\uff0c\u8fdb\u884c\u4f5c\u7269\u76d1\u6d4b\u548c\u7ba1\u7406\u3002 * \u7269\u6d41\u4e0e\u914d\u9001\u673a\u5668\u4eba\uff1a \u63d0\u5347\u5728\u591a\u6837\u5316\u57ce\u5e02\u548c\u90ca\u533a\u73af\u5883\u4e2d\u201c\u6700\u540e\u4e00\u516c\u91cc\u201d\u914d\u9001\u7684\u9c81\u68d2\u6027\u3002 5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u7684\u5c40\u9650\u6027 (Inferred Limitations) \u5c3d\u7ba1\u8be5\u65b9\u6cd5\u53d6\u5f97\u4e86\u663e\u8457\u6210\u679c\uff0c\u4f46\u4ece\u6458\u8981\u4e2d\u4ecd\u53ef\u63a8\u65ad\u51fa\u4e00\u4e9b\u6f5c\u5728\u5c40\u9650\u6027\uff1a * \u4f2a\u6807\u7b7e\u8d28\u91cf\u7684\u4f9d\u8d56\u6027\uff1a \u4f2a\u6807\u7b7e\u7684\u751f\u6210\u8d28\u91cf\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u6240\u4f7f\u7528\u7684\u201c\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u201d\u7684\u6027\u80fd\u53ca\u5176\u5bf9\u591a\u6a21\u6001\u63d0\u793a\u7684\u7406\u89e3\u80fd\u529b\u3002\u5982\u679c\u57fa\u7840\u6a21\u578b\u5728\u7279\u5b9a\u573a\u666f\u6216\u7269\u4f53\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u751f\u6210\u7684\u4f2a\u6807\u7b7e\u53ef\u80fd\u4f1a\u5f15\u5165\u7cfb\u7edf\u6027\u8bef\u5dee\uff0c\u4ece\u800c\u5f71\u54cd\u540e\u7eed\u7f51\u7edc\u7684\u8bad\u7ec3\u6548\u679c\u3002 * \u201c\u89e3\u8026\u201d\u5b66\u4e60\u7684\u6743\u8861\uff1a \u6458\u8981\u63d0\u5230\u53cc\u6d41\u7f51\u7edc\u4ee5\u201c\u89e3\u8026\u65b9\u5f0f\u201d\u5b66\u4e60\u4e0d\u540c\u6a21\u6001\uff0c\u8fd9\u53ef\u80fd\u6709\u52a9\u4e8e\u63d0\u53d6\u6a21\u6001\u7279\u5b9a\u7279\u5f81\uff0c\u4f46\u4e5f\u53ef\u80fd\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u9650\u5236\u4e86\u4e0d\u540c\u6a21\u6001\u4e4b\u95f4\u66f4\u6df1\u5c42\u6b21\u3001\u66f4\u590d\u6742\u7684\u4ea4\u4e92\u548c\u4fe1\u606f\u878d\u5408\uff0c\u4ece\u800c\u5f71\u54cd\u6700\u7ec8\u7684\u51b3\u7b56\u9c81\u68d2\u6027\u3002 * \u7a00\u758fLiDAR\u76d1\u7763\u7684\u6027\u8d28\uff1a \u5c3d\u7ba1\u7a00\u758fLiDAR\u76d1\u7763\u7528\u4e8e\u7f13\u89e3\u4f2a\u6807\u7b7e\u566a\u58f0\uff0c\u4f46\u5176\u5177\u4f53\u6765\u6e90\uff08\u662f\u5b8c\u5168\u81ea\u52a8\u751f\u6210\u8fd8\u662f\u9700\u8981\u5c11\u91cf\u4eba\u5de5\u5e72\u9884\uff09\u548c\u7a00\u758f\u7a0b\u5ea6\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u4ee5\u53ca\u5176\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4ecd\u9700\u8fdb\u4e00\u6b65\u63a2\u8ba8\u3002 * \u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\uff1a \u6574\u5408\u591a\u6a21\u6001\u6570\u636e\u3001\u5229\u7528\u57fa\u7840\u6a21\u578b\u4ee5\u53ca\u8bad\u7ec3\u53cc\u6d41\u7f51\u7edc\uff0c\u53ef\u80fd\u5bf9\u8ba1\u7b97\u8d44\u6e90\uff08\u8bad\u7ec3\u548c\u63a8\u7406\uff09\u6709\u8f83\u9ad8\u8981\u6c42\uff0c\u8fd9\u5bf9\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u673a\u5668\u4eba\u5e73\u53f0\u53ef\u80fd\u662f\u4e00\u4e2a\u6311\u6218\u3002 Key Findings: To address these limitations, we propose a multimodal self-supervised framework for traversability labeling and estimation. Finally, extensive experiments conducted across urban, off-road, and campus environments demonstrate the effectiveness of our approach. Compared to existing self-supervised state-of-the-art methods, our multimodal traversability estimation network yields consistently higher IoU, improving by 1.6-3.5% on all evaluated datasets. Links: PDF arXiv Learning to Detect Label Errors by Making Them: A Method for Segmentation and Object Detection Datasets Authors: Sarina Penquitt, Tobias Riedlinger, Timo Heller, Markus Reischl, Matthias Rottmann Published: 2025-08-25 Categories: cs.LG, cs.CV Abstract: Recently, detection of label errors and improvement of label quality in datasets for supervised learning tasks has become an increasingly important goal in both research and industry. The consequences of incorrectly annotated data include reduced model performance, biased benchmark results, and lower overall accuracy. Current state-of-the-art label error detection methods often focus on a single computer vision task and, consequently, a specific type of dataset, containing, for example, either bounding boxes or pixel-wise annotations. Furthermore, previous methods are not learning-based. In this work, we overcome this research gap. We present a unified method for detecting label errors in object detection, semantic segmentation, and instance segmentation datasets. In a nutshell, our approach - learning to detect label errors by making them - works as follows: we inject different kinds of label errors into the ground truth. Then, the detection of label errors, across all mentioned primary tasks, is framed as an instance segmentation problem based on a composite input. In our experiments, we compare the label error detection performance of our method with various baselines and state-of-the-art approaches of each task's domain on simulated label errors across multiple tasks, datasets, and base models. This is complemented by a generalization study on real-world label errors. Additionally, we release 459 real label errors identified in the Cityscapes dataset and provide a benchmark for real label error detection in Cityscapes. Analysis: \u4f5c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u4e13\u5bb6\uff0c\u6211\u5bf9\u8fd9\u7bc7\u8bba\u6587\u6458\u8981\u8fdb\u884c\u5982\u4e0b\u5206\u6790\uff1a \u8bba\u6587\u6458\u8981\u5206\u6790\uff1aLearning to Detect Label Errors by Making Them: A Method for Segmentation and Object Detection Datasets 1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u6d01\u603b\u7ed3 (2-3 \u53e5\u8bdd) \u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u3001\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4b\u76ee\u6807\u68c0\u6d4b\u3001\u8bed\u4e49\u5206\u5272\u548c\u5b9e\u4f8b\u5206\u5272\u6570\u636e\u96c6\u4e2d\u5b58\u5728\u7684\u6807\u7b7e\u9519\u8bef\u3002\u5176\u6838\u5fc3\u601d\u60f3\u662f\u901a\u8fc7\u5411\u771f\u503c\u4e2d\u6ce8\u5165\u4e0d\u540c\u7c7b\u578b\u7684\u5408\u6210\u9519\u8bef\u6765\u8bad\u7ec3\u4e00\u4e2a\u9519\u8bef\u68c0\u6d4b\u6a21\u578b\uff0c\u5e76\u5c06\u9519\u8bef\u68c0\u6d4b\u4efb\u52a1\u5efa\u6a21\u4e3a\u4e00\u4e2a\u57fa\u4e8e\u590d\u5408\u8f93\u5165\u7684\u5b9e\u4f8b\u5206\u5272\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u65e8\u5728\u514b\u670d\u73b0\u6709\u65b9\u6cd5\u4ec5\u9650\u4e8e\u5355\u4e00\u4efb\u52a1\u4e14\u975e\u5b66\u4e60\u7684\u5c40\u9650\u6027\uff0c\u5e76\u901a\u8fc7\u771f\u5b9e\u4e16\u754c\u9519\u8bef\u548c\u57fa\u51c6\u6d4b\u8bd5\u6765\u9a8c\u8bc1\u5176\u6cdb\u5316\u80fd\u529b\u3002 2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u8bba \u7edf\u4e00\u7684\u3001\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\uff1a \u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4e13\u6ce8\u4e8e\u5355\u4e00\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\uff08\u5982\u4ec5\u8fb9\u754c\u6846\u6216\u50cf\u7d20\u7ea7\u6807\u6ce8\uff09\u4e14\u975e\u5b66\u4e60\u7684\u5c40\u9650\u6027\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u540c\u65f6\u5904\u7406\u76ee\u6807\u68c0\u6d4b\u3001\u8bed\u4e49\u5206\u5272\u548c\u5b9e\u4f8b\u5206\u5272\u7684\u901a\u7528\u6846\u67b6\u3002 \u201c\u901a\u8fc7\u5236\u9020\u9519\u8bef\u6765\u5b66\u4e60\u68c0\u6d4b\u9519\u8bef\u201d (Learning by Making Them)\uff1a \u8fd9\u662f\u4e00\u4e2a\u5de7\u5999\u7684\u81ea\u76d1\u7763\u6216\u6570\u636e\u589e\u5f3a\u7b56\u7565\u3002\u901a\u8fc7\u7cfb\u7edf\u5730\u5411\u771f\u503c\u4e2d\u6ce8\u5165\u4e0d\u540c\u7c7b\u578b\u7684\u6807\u7b7e\u9519\u8bef\uff0c\u4e3a\u9519\u8bef\u68c0\u6d4b\u6a21\u578b\u751f\u6210\u8bad\u7ec3\u6570\u636e\uff0c\u4ece\u800c\u907f\u514d\u4e86\u5bf9\u5927\u91cf\u771f\u5b9e\u9519\u8bef\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\u3002 \u5c06\u9519\u8bef\u68c0\u6d4b\u6846\u67b6\u4e3a\u5b9e\u4f8b\u5206\u5272\u95ee\u9898\uff1a \u8fd9\u79cd\u65b9\u6cd5\u80fd\u591f\u7cbe\u786e\u5730\u5b9a\u4f4d\u548c\u8bc6\u522b\u56fe\u50cf\u4e2d\u4e0d\u540c\u7c7b\u578b\u7684\u6807\u7b7e\u9519\u8bef\u533a\u57df\uff0c\u5c06\u5176\u89c6\u4e3a\u72ec\u7acb\u7684\u201c\u9519\u8bef\u5b9e\u4f8b\u201d\uff0c\u8fd9\u6bd4\u7b80\u5355\u7684\u5206\u7c7b\u6216\u68c0\u6d4b\u6846\u65b9\u6cd5\u66f4\u5177\u8868\u73b0\u529b\u3002 \u590d\u5408\u8f93\u5165 (Composite Input)\uff1a \u867d\u7136\u6458\u8981\u672a\u8be6\u7ec6\u8bf4\u660e\uff0c\u4f46\u6697\u793a\u4e86\u901a\u8fc7\u7ed3\u5408\u539f\u59cb\u56fe\u50cf\u4fe1\u606f\u548c\u53ef\u80fd\u5305\u542b\u9519\u8bef\u4fe1\u606f\u7684\u8868\u793a\u6765\u8bad\u7ec3\u6a21\u578b\uff0c\u4ee5\u63d0\u9ad8\u9519\u8bef\u68c0\u6d4b\u7684\u9c81\u68d2\u6027\u3002 3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd \u63d0\u5347\u6570\u636e\u96c6\u8d28\u91cf\u548c\u6a21\u578b\u6027\u80fd\uff1a \u81ea\u52a8\u5316\u3001\u7edf\u4e00\u7684\u6807\u7b7e\u9519\u8bef\u68c0\u6d4b\u5c06\u663e\u8457\u63d0\u9ad8\u8ba1\u7b97\u673a\u89c6\u89c9\u6570\u636e\u96c6\u7684\u8d28\u91cf\uff0c\u4ece\u800c\u8bad\u7ec3\u51fa\u66f4\u9c81\u68d2\u3001\u6027\u80fd\u66f4\u597d\u7684\u6a21\u578b\uff0c\u5e76\u51cf\u5c11\u56e0\u6570\u636e\u9519\u8bef\u5bfc\u81f4\u7684\u6a21\u578b\u504f\u5dee\u3002 \u66f4\u53ef\u9760\u7684\u57fa\u51c6\u6d4b\u8bd5\uff1a \u51cf\u5c11\u6570\u636e\u96c6\u4e2d\u7684\u9519\u8bef\u5c06\u4f7f\u57fa\u51c6\u6d4b\u8bd5\u7ed3\u679c\u66f4\u52a0\u516c\u5e73\u548c\u53ef\u4fe1\uff0c\u4fc3\u8fdb\u66f4\u6709\u6548\u7684\u6a21\u578b\u6bd4\u8f83\u548c\u7814\u7a76\u8fdb\u5c55\u3002 \u964d\u4f4e\u6570\u636e\u6807\u6ce8\u6210\u672c\u548c\u65f6\u95f4\uff1a \u81ea\u52a8\u5316\u9519\u8bef\u68c0\u6d4b\u53ef\u4ee5\u5927\u5e45\u51cf\u5c11\u4eba\u5de5\u8d28\u91cf\u63a7\u5236\u7684\u9700\u6c42\uff0c\u964d\u4f4e\u6570\u636e\u96c6\u521b\u5efa\u548c\u7ef4\u62a4\u7684\u6210\u672c\u4e0e\u65f6\u95f4\u3002 \u63a8\u52a8\u6570\u636e\u4e2d\u5fc3AI\u53d1\u5c55\uff1a \u8be5\u7814\u7a76\u4e0e\u5f53\u524d\u201c\u6570\u636e\u4e2d\u5fc3AI\u201d\u7684\u8d8b\u52bf\u9ad8\u5ea6\u5951\u5408\uff0c\u5f3a\u8c03\u4e86\u6570\u636e\u8d28\u91cf\u5728\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u6838\u5fc3\u4f5c\u7528\u3002 \u793e\u533a\u8d44\u6e90\u8d21\u732e\uff1a \u8bba\u6587\u53d1\u5e03\u4e86 Cityscapes \u6570\u636e\u96c6\u4e2d\u8bc6\u522b\u51fa\u7684 459 \u4e2a\u771f\u5b9e\u6807\u7b7e\u9519\u8bef\uff0c\u5e76\u63d0\u4f9b\u4e86\u771f\u5b9e\u6807\u7b7e\u9519\u8bef\u68c0\u6d4b\u7684\u57fa\u51c6\uff0c\u8fd9\u5c06\u4e3a\u672a\u6765\u7684\u7814\u7a76\u63d0\u4f9b\u5b9d\u8d35\u7684\u8d44\u6e90\u548c\u8bc4\u4f30\u6807\u51c6\u3002 4. \u53ef\u80fd\u53d7\u76ca\u7684\u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 \u81ea\u52a8\u9a7e\u9a76\uff1a \u81ea\u52a8\u9a7e\u9a76\u6570\u636e\u96c6\u5e9e\u5927\u4e14\u5bf9\u7cbe\u5ea6\u8981\u6c42\u6781\u9ad8\uff0c\u6807\u7b7e\u9519\u8bef\u53ef\u80fd\u5bfc\u81f4\u4e25\u91cd\u540e\u679c\u3002\u8be5\u65b9\u6cd5\u53ef\u7528\u4e8e\u63d0\u9ad8\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u6570\u636e\u96c6\u7684\u8d28\u91cf\u3002 \u533b\u5b66\u5f71\u50cf\u5206\u6790\uff1a \u533b\u5b66\u56fe\u50cf\u6807\u6ce8\u7684\u51c6\u786e\u6027\u76f4\u63a5\u5173\u7cfb\u5230\u8bca\u65ad\u548c\u6cbb\u7597\uff0c\u9519\u8bef\u68c0\u6d4b\u5bf9\u4e8e\u786e\u4fdd\u6a21\u578b\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\u3002 \u5927\u89c4\u6a21\u6570\u636e\u96c6\u63d0\u4f9b\u5546/\u6807\u6ce8\u670d\u52a1\uff1a \u4e13\u95e8\u4ece\u4e8b\u6570\u636e\u96c6\u521b\u5efa\u548c\u8d28\u91cf\u63a7\u5236\u7684\u516c\u53f8\u5c06\u76f4\u63a5\u53d7\u76ca\u4e8e\u8fd9\u79cd\u81ea\u52a8\u5316\u5de5\u5177\u3002 \u673a\u5668\u4eba\u611f\u77e5\uff1a \u673a\u5668\u4eba\u9700\u8981\u51c6\u786e\u7684\u73af\u5883\u611f\u77e5\u6765\u6267\u884c\u4efb\u52a1\uff0c\u9ad8\u8d28\u91cf\u7684\u8bad\u7ec3\u6570\u636e\u662f\u57fa\u7840\u3002 \u4efb\u4f55\u4f9d\u8d56\u76d1\u7763\u5b66\u4e60\u7684\u5de5\u4e1a\u5e94\u7528\uff1a \u51e1\u662f\u4f7f\u7528\u5927\u91cf\u6807\u6ce8\u6570\u636e\u8fdb\u884c\u6a21\u578b\u8bad\u7ec3\u7684\u884c\u4e1a\uff08\u5982\u96f6\u552e\u3001\u5b89\u9632\u3001\u519c\u4e1a\u7b49\uff09\u90fd\u5c06\u4ece\u66f4\u53ef\u9760\u7684\u6570\u636e\u8d28\u91cf\u4e2d\u83b7\u76ca\u3002 \u4e3b\u52a8\u5b66\u4e60 (Active Learning) \u548c\u6570\u636e\u7b56\u5c55 (Data Curation)\uff1a \u8bc6\u522b\u51fa\u9519\u8bef\u6837\u672c\u53ef\u4ee5\u6307\u5bfc\u91cd\u65b0\u6807\u6ce8\u6216\u4f18\u5148\u5904\u7406\uff0c\u4f18\u5316\u6570\u636e\u5229\u7528\u6548\u7387\u3002 5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u7684\u5c40\u9650\u6027 \u5408\u6210\u9519\u8bef\u4e0e\u771f\u5b9e\u9519\u8bef\u7684\u5dee\u8ddd\uff1a \u5c3d\u7ba1\u8bba\u6587\u63d0\u5230\u4e86\u5bf9\u771f\u5b9e\u4e16\u754c\u9519\u8bef\u7684\u6cdb\u5316\u7814\u7a76\uff0c\u4f46\u5408\u6210\u9519\u8bef\u662f\u5426\u80fd\u5b8c\u5168\u8986\u76d6\u6240\u6709\u7c7b\u578b\u3001\u6240\u6709\u7ec6\u5fae\u4e4b\u5904\u7684\u771f\u5b9e\u4e16\u754c\u9519\u8bef\uff0c\u4ee5\u53ca\u6a21\u578b\u5bf9\u672a\u89c1\u8fc7\u7684\u771f\u5b9e\u9519\u8bef\u7c7b\u578b\u7684\u68c0\u6d4b\u80fd\u529b\uff0c\u4ecd\u662f\u4e00\u4e2a\u6f5c\u5728\u7684\u6311\u6218\u3002 \u201c\u590d\u5408\u8f93\u5165\u201d\u7684\u7ec6\u8282\uff1a \u6458\u8981\u4e2d\u672a\u8be6\u7ec6\u8bf4\u660e\u201c\u590d\u5408\u8f93\u5165\u201d\u7684\u5177\u4f53\u6784\u6210\uff0c\u5176\u8bbe\u8ba1\u5bf9\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u6cdb\u5316\u80fd\u529b\u53ef\u80fd\u81f3\u5173\u91cd\u8981\u3002 \u8ba1\u7b97\u6210\u672c\uff1a \u8bad\u7ec3\u4e00\u4e2a\u57fa\u4e8e\u5b66\u4e60\u7684\u9519\u8bef\u68c0\u6d4b\u6a21\u578b\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u5927\u89c4\u6a21\u6570\u636e\u96c6\u548c\u590d\u6742\u7684\u5b9e\u4f8b\u5206\u5272\u4efb\u52a1\uff0c\u53ef\u80fd\u9700\u8981\u663e\u8457\u7684\u8ba1\u7b97\u8d44\u6e90\u548c\u65f6\u95f4\uff0c\u6458\u8981\u4e2d\u672a\u63d0\u53ca\u8fd9\u65b9\u9762\u7684\u8003\u91cf\u3002 \u9519\u8bef\u7c7b\u578b\u7684\u7c92\u5ea6\uff1a \u6458\u8981\u4e2d\u63d0\u5230\u201c\u4e0d\u540c\u79cd\u7c7b\u7684\u6807\u7b7e\u9519\u8bef\u201d\uff0c\u4f46\u672a\u5177\u4f53\u8bf4\u660e\u80fd\u68c0\u6d4b\u5230\u54ea\u4e9b\u7c92\u5ea6\u7684\u9519\u8bef\uff08\u4f8b\u5982\uff0c\u662f\u660e\u663e\u7684\u51e0\u4f55\u9519\u8bef\uff0c\u8fd8\u662f\u66f4\u7ec6\u5fae\u7684\u8bed\u4e49\u4e0d\u4e00\u81f4\u6027\uff09\u3002 \u9519\u8bef\u4fee\u6b63\u673a\u5236\uff1a \u8bba\u6587\u4e13\u6ce8\u4e8e\u9519\u8bef\u68c0\u6d4b\uff0c\u4f46\u672a\u63d0\u53ca\u5982\u4f55\u5c06\u68c0\u6d4b\u5230\u7684\u9519\u8bef\u6709\u6548\u5730\u53cd\u9988\u7ed9\u6807\u6ce8\u5458\u8fdb\u884c\u4fee\u6b63\uff0c\u6216\u662f\u5426\u63d0\u4f9b\u81ea\u52a8\u4fee\u6b63\u7684\u5efa\u8bae\u3002 Key Findings: Current state-of-the-art label error detection methods often focus on a single computer vision task and, consequently, a specific type of dataset, containing, for example, either bounding boxes or pixel-wise annotations. We present a unified method for detecting label errors in object detection, semantic segmentation, and instance segmentation datasets. In a nutshell, our approach - learning to detect label errors by making them - works as follows: we inject different kinds of label errors into the ground truth. In our experiments, we compare the label error detection performance of our method with various baselines and state-of-the-art approaches of each task's domain on simulated label errors across multiple tasks, datasets, and base models. Links: PDF arXiv CATformer: Contrastive Adversarial Transformer for Image Super-Resolution Authors: Qinyi Tian, Spence Cox, Laura E. Dalton Published: 2025-08-25 Categories: cs.CV Abstract: Super-resolution remains a promising technique to enhance the quality of low-resolution images. This study introduces CATformer (Contrastive Adversarial Transformer), a novel neural network integrating diffusion-inspired feature refinement with adversarial and contrastive learning. CATformer employs a dual-branch architecture combining a primary diffusion-inspired transformer, which progressively refines latent representations, with an auxiliary transformer branch designed to enhance robustness to noise through learned latent contrasts. These complementary representations are fused and decoded using deep Residual-in-Residual Dense Blocks for enhanced reconstruction quality. Extensive experiments on benchmark datasets demonstrate that CATformer outperforms recent transformer-based and diffusion-inspired methods both in efficiency and visual image quality. This work bridges the performance gap among transformer-, diffusion-, and GAN-based methods, laying a foundation for practical applications of diffusion-inspired transformers in super-resolution. Analysis: \u4f5c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u4e13\u5bb6\uff0c\u6211\u5bf9\u8fd9\u7bc7\u5173\u4e8eCATformer\u7684\u8bba\u6587\u6458\u8981\u8fdb\u884c\u5982\u4e0b\u5206\u6790\uff1a CATformer: Contrastive Adversarial Transformer for Image Super-Resolution 1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u6d01\u603b\u7ed3 (2-3 \u53e5\u8bdd) CATformer\u662f\u4e00\u79cd\u65b0\u9896\u7684\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u795e\u7ecf\u7f51\u7edc\uff0c\u5b83\u5de7\u5999\u5730\u5c06\u53d7\u6269\u6563\u6a21\u578b\u542f\u53d1\u7684\u7279\u5f81\u7ec6\u5316\u3001\u5bf9\u6297\u6027\u5b66\u4e60\u548c\u5bf9\u6bd4\u5b66\u4e60\u96c6\u6210\u5230\u4e00\u4e2a\u53cc\u5206\u652fTransformer\u67b6\u6784\u4e2d\u3002\u8be5\u6a21\u578b\u901a\u8fc7\u6e10\u8fdb\u5f0f\u6f5c\u5728\u8868\u793a\u7ec6\u5316\u548c\u589e\u5f3a\u5bf9\u566a\u58f0\u7684\u9c81\u68d2\u6027\uff0c\u5728\u6548\u7387\u548c\u89c6\u89c9\u8d28\u91cf\u4e0a\u5747\u8d85\u8d8a\u4e86\u73b0\u6709\u7684Transformer\u548c\u6269\u6563\u542f\u53d1\u65b9\u6cd5\u3002\u8fd9\u9879\u5de5\u4f5c\u4e3a\u8d85\u5206\u8fa8\u7387\u9886\u57df\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u6df7\u5408\u8303\u5f0f\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u4e3a\u6269\u6563\u542f\u53d1\u5f0fTransformer\u7684\u5b9e\u9645\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840\u3002 2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u8bba\u65b9\u6cd5 CATformer\u7684\u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u5176\u72ec\u7279\u7684 \u53cc\u5206\u652fTransformer\u67b6\u6784 \u4ee5\u53ca\u5bf9 \u591a\u8303\u5f0f\u5b66\u4e60\u7684\u6df1\u5ea6\u878d\u5408 \u3002\u5177\u4f53\u800c\u8a00\uff1a * \u53cc\u5206\u652f\u67b6\u6784\uff1a \u5305\u542b\u4e00\u4e2a \u4e3b\u6269\u6563\u542f\u53d1\u5f0fTransformer\u5206\u652f \uff0c\u8d1f\u8d23\u5bf9\u6f5c\u5728\u8868\u793a\u8fdb\u884c\u6e10\u8fdb\u5f0f\u7ec6\u5316\uff0c\u501f\u9274\u4e86\u6269\u6563\u6a21\u578b\u9010\u6b65\u53bb\u566a\u548c\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\u7684\u6838\u5fc3\u601d\u60f3\u3002 * \u8f85\u52a9\u5bf9\u6bd4\u5b66\u4e60\u5206\u652f\uff1a \u53e6\u4e00\u4e2a \u8f85\u52a9Transformer\u5206\u652f \u901a\u8fc7\u5b66\u4e60\u5230\u7684\u6f5c\u5728\u5bf9\u6bd4\u6765\u589e\u5f3a\u6a21\u578b\u5bf9\u566a\u58f0\u7684\u9c81\u68d2\u6027\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u5229\u7528\u5bf9\u6bd4\u5b66\u4e60\u6765\u89e3\u51b3\u8d85\u5206\u8fa8\u7387\u4e2d\u5e38\u89c1\u566a\u58f0\u95ee\u9898\u7684\u7b56\u7565\u3002 * \u591a\u8303\u5f0f\u878d\u5408\uff1a \u5c06 \u6269\u6563\u542f\u53d1 \uff08\u7528\u4e8e\u7279\u5f81\u7ec6\u5316\uff09\u3001 \u5bf9\u6297\u6027\u5b66\u4e60 \uff08\u7528\u4e8e\u751f\u6210\u771f\u5b9e\u611f\u56fe\u50cf\uff09\u548c \u5bf9\u6bd4\u5b66\u4e60 \uff08\u7528\u4e8e\u566a\u58f0\u9c81\u68d2\u6027\uff09\u8fd9\u4e09\u79cd\u5f3a\u5927\u7684\u5b66\u4e60\u8303\u5f0f\u5de7\u5999\u5730\u96c6\u6210\u5728\u4e00\u4e2aTransformer\u6846\u67b6\u5185\uff0c\u5e76\u901a\u8fc7Residual-in-Residual Dense Blocks\u8fdb\u884c\u878d\u5408\u548c\u89e3\u7801\uff0c\u4ee5\u5b9e\u73b0\u5353\u8d8a\u7684\u91cd\u5efa\u8d28\u91cf\u3002 3. \u5bf9\u8be5\u9886\u57df\u7684\u6f5c\u5728\u5f71\u54cd \u6027\u80fd\u65b0\u6807\u6746\uff1a CATformer\u58f0\u79f0\u5728\u6548\u7387\u548c\u89c6\u89c9\u8d28\u91cf\u4e0a\u5747\u8d85\u8d8a\u4e86\u73b0\u6709\u7684Transformer\u548c\u6269\u6563\u542f\u53d1\u65b9\u6cd5\uff0c\u8fd9\u53ef\u80fd\u4e3a\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u9886\u57df\u6811\u7acb\u65b0\u7684\u6027\u80fd\u57fa\u51c6\u3002 \u5f25\u5408\u65b9\u6cd5\u8bba\u9e3f\u6c9f\uff1a \u8be5\u7814\u7a76\u660e\u786e\u6307\u51fa\u5176\u5de5\u4f5c\u201c\u5f25\u5408\u4e86Transformer\u3001\u6269\u6563\u6a21\u578b\u548cGAN\u8fd9\u4e09\u79cd\u65b9\u6cd5\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\u201d\uff0c\u8fd9\u8868\u660e\u5b83\u6210\u529f\u5730\u7ed3\u5408\u4e86\u5404\u5bb6\u4e4b\u957f\uff0c\u514b\u670d\u4e86\u5355\u4e00\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u53ef\u80fd\u5f15\u9886\u672a\u6765\u8d85\u5206\u8fa8\u7387\u4e43\u81f3\u66f4\u5e7f\u6cdb\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u7684\u6df7\u5408\u67b6\u6784\u8bbe\u8ba1\u8d8b\u52bf\u3002 \u63a8\u52a8\u6269\u6563\u542f\u53d1\u6a21\u578b\u7684\u5b9e\u7528\u5316\uff1a \u6269\u6563\u6a21\u578b\u867d\u7136\u751f\u6210\u8d28\u91cf\u9ad8\uff0c\u4f46\u901a\u5e38\u63a8\u7406\u901f\u5ea6\u8f83\u6162\u3002CATformer\u901a\u8fc7\u201c\u6269\u6563\u542f\u53d1\u201d\u800c\u975e\u5b8c\u6574\u6269\u6563\u6a21\u578b\u7684\u65b9\u5f0f\uff0c\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u7684\u540c\u65f6\u63d0\u5347\u4e86\u6548\u7387\uff0c\u4e3a\u5c06\u6269\u6563\u6a21\u578b\u7684\u5f3a\u5927\u80fd\u529b\u5f15\u5165\u5b9e\u9645\u5e94\u7528\u573a\u666f\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002 \u589e\u5f3a\u6a21\u578b\u9c81\u68d2\u6027\uff1a \u5f15\u5165\u5bf9\u6bd4\u5b66\u4e60\u4ee5\u589e\u5f3a\u5bf9\u566a\u58f0\u7684\u9c81\u68d2\u6027\uff0c\u5bf9\u4e8e\u771f\u5b9e\u4e16\u754c\u4e2d\u4f4e\u8d28\u91cf\u3001\u542b\u566a\u56fe\u50cf\u7684\u8d85\u5206\u8fa8\u7387\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u7684\u5b9e\u7528\u4ef7\u503c\u3002 4. \u53ef\u80fd\u53d7\u76ca\u4e8e\u8fd9\u9879\u7814\u7a76\u7684\u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 \u901a\u7528\u56fe\u50cf\u6062\u590d\u4e0e\u589e\u5f3a\uff1a \u9664\u4e86\u8d85\u5206\u8fa8\u7387\uff0c\u5176\u5bf9\u566a\u58f0\u7684\u9c81\u68d2\u6027\u548c\u9ad8\u8d28\u91cf\u91cd\u5efa\u80fd\u529b\u53ef\u80fd\u76f4\u63a5\u5e94\u7528\u4e8e\u56fe\u50cf\u53bb\u566a\u3001\u53bb\u6a21\u7cca\u3001\u56fe\u50cf\u4fee\u590d\u7b49\u4efb\u52a1\u3002 \u533b\u5b66\u5f71\u50cf\uff1a \u63d0\u9ad8\u4f4e\u5206\u8fa8\u7387\u533b\u5b66\u626b\u63cf\u56fe\u50cf\uff08\u5982MRI\u3001CT\uff09\u7684\u7ec6\u8282\uff0c\u8f85\u52a9\u533b\u751f\u8fdb\u884c\u66f4\u7cbe\u786e\u7684\u8bca\u65ad\u3002 \u9065\u611f\u4e0e\u5b89\u9632\u76d1\u63a7\uff1a \u63d0\u5347\u536b\u661f\u56fe\u50cf\u3001\u65e0\u4eba\u673a\u822a\u62cd\u56fe\u6216\u76d1\u63a7\u5f55\u50cf\u7684\u6e05\u6670\u5ea6\uff0c\u4fbf\u4e8e\u76ee\u6807\u8bc6\u522b\u3001\u6001\u52bf\u611f\u77e5\u548c\u4e8b\u4ef6\u5206\u6790\u3002 \u8ba1\u7b97\u6444\u5f71\u4e0e\u89c6\u9891\u5904\u7406\uff1a \u6539\u5584\u6d88\u8d39\u7ea7\u8bbe\u5907\u62cd\u6444\u7684\u4f4e\u8d28\u91cf\u7167\u7247\u548c\u89c6\u9891\uff0c\u5b9e\u73b0\u66f4\u6e05\u6670\u7684\u653e\u5927\u548c\u7ec6\u8282\u6062\u590d\u3002 \u6570\u5b57\u5185\u5bb9\u521b\u4f5c\u4e0e\u5a31\u4e50\uff1a \u63d0\u5347\u8001\u65e7\u7167\u7247\u3001\u89c6\u9891\u7684\u753b\u8d28\uff0c\u6216\u5728\u6e38\u620f\u3001VR/AR\u7b49\u573a\u666f\u4e2d\u5b9e\u73b0\u5b9e\u65f6\u9ad8\u8d28\u91cf\u6e32\u67d3\u3002 \u5176\u4ed6\u6761\u4ef6\u56fe\u50cf\u751f\u6210\u4efb\u52a1\uff1a \u5176\u878d\u5408\u591a\u79cd\u5b66\u4e60\u8303\u5f0f\u548c\u67b6\u6784\u7684\u601d\u8def\uff0c\u53ef\u80fd\u4e3a\u5176\u4ed6\u6761\u4ef6\u56fe\u50cf\u751f\u6210\u4efb\u52a1\uff08\u5982\u6587\u672c\u5230\u56fe\u50cf\u3001\u56fe\u50cf\u5230\u56fe\u50cf\u8f6c\u6362\uff09\u63d0\u4f9b\u65b0\u7684\u8bbe\u8ba1\u7075\u611f\u3002 5. \u4ece\u6458\u8981\u4e2d\u53ef\u4ee5\u63a8\u65ad\u51fa\u7684\u4efb\u4f55\u5c40\u9650\u6027 \u67b6\u6784\u590d\u6742\u6027\u4e0e\u8bad\u7ec3\u6210\u672c\uff1a \u53cc\u5206\u652fTransformer\u67b6\u6784\u7ed3\u5408\u4e09\u79cd\u5b66\u4e60\u8303\u5f0f\uff08\u6269\u6563\u542f\u53d1\u3001\u5bf9\u6297\u3001\u5bf9\u6bd4\uff09\u4ee5\u53caRIRDB\uff0c\u610f\u5473\u7740\u6a21\u578b\u53ef\u80fd\u975e\u5e38\u590d\u6742\uff0c\u8bad\u7ec3\u96be\u5ea6\u5927\uff0c\u5bf9\u8ba1\u7b97\u8d44\u6e90\u7684\u9700\u6c42\u9ad8\uff0c\u4e14\u8d85\u53c2\u6570\u8c03\u4f18\u53ef\u80fd\u5177\u6709\u6311\u6218\u6027\u3002 \u201c\u6269\u6563\u542f\u53d1\u201d\u7684\u6df1\u5ea6\uff1a \u8bba\u6587\u5f3a\u8c03\u201c\u6269\u6563\u542f\u53d1\u201d\u7684\u7279\u5f81\u7ec6\u5316\uff0c\u800c\u975e\u5b8c\u6574\u7684\u6269\u6563\u6a21\u578b\u3002\u8fd9\u53ef\u80fd\u610f\u5473\u7740\u5b83\u5728\u751f\u6210\u591a\u6837\u6027\u6216\u5904\u7406\u6781\u7aef\u4f4e\u8d28\u91cf\u8f93\u5165\u65b9\u9762\u7684\u80fd\u529b\uff0c\u53ef\u80fd\u4e0d\u5982\u5b8c\u6574\u7684\u6269\u6563\u751f\u6210\u6a21\u578b\u3002\u5176\u201c\u6e10\u8fdb\u5f0f\u7ec6\u5316\u201d\u7684\u5177\u4f53\u673a\u5236\u548c\u6548\u679c\u4e0e\u5b8c\u6574\u6269\u6563\u6a21\u578b\u7684\u5dee\u5f02\uff0c\u6709\u5f85\u6b63\u6587\u63ed\u793a\u3002 \u566a\u58f0\u9c81\u68d2\u6027\u7684\u5177\u4f53\u8303\u56f4\uff1a \u6458\u8981\u63d0\u5230\u901a\u8fc7\u5b66\u4e60\u5230\u7684\u6f5c\u5728\u5bf9\u6bd4\u589e\u5f3a\u5bf9\u566a\u58f0\u7684\u9c81\u68d2\u6027\uff0c\u4f46\u672a\u5177\u4f53\u8bf4\u660e\u80fd\u5904\u7406\u7684\u566a\u58f0\u7c7b\u578b\uff08\u5982\u9ad8\u65af\u566a\u58f0\u3001\u771f\u5b9e\u4e16\u754c\u566a\u58f0\u3001\u4f20\u611f\u5668\u566a\u58f0\uff09\u548c\u5f3a\u5ea6\u8303\u56f4\u3002\u5728\u9762\u5bf9\u9ad8\u5ea6\u590d\u6742\u6216\u975e\u5178\u578b\u566a\u58f0\u65f6\uff0c\u5176\u8868\u73b0\u5982\u4f55\u4ecd\u662f\u672a\u77e5\u3002 \u201c\u6548\u7387\u201d\u7684\u91cf\u5316\uff1a \u5c3d\u7ba1\u58f0\u79f0\u5728\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4f46\u5177\u4f53\u7684\u91cf\u5316\u6570\u636e\uff08\u5982\u63a8\u7406\u65f6\u95f4\u3001\u53c2\u6570\u91cf\u3001FLOPs\uff09\u5728\u6458\u8981\u4e2d\u7f3a\u5931\uff0c\u65e0\u6cd5\u5224\u65ad\u5176\u201c\u6548\u7387\u201d\u7684\u7edd\u5bf9\u6c34\u5e73\uff0c\u5c24\u5176\u662f\u5728\u4e0e\u8f7b\u91cf\u7ea7\u6a21\u578b\u5bf9\u6bd4\u65f6\u3002 \u6cdb\u5316\u80fd\u529b\uff1a \u5c3d\u7ba1\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u9762\u5bf9\u672a\u89c1\u8fc7\u7684\u9ad8\u5ea6\u590d\u6742\u6216\u7279\u5b9a\u9886\u57df\u7684\u771f\u5b9e\u4e16\u754c\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\u65f6\uff0c\u5176\u6cdb\u5316\u80fd\u529b\u4ecd\u9700\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u3002\u4f8b\u5982\uff0c\u5bf9\u4e8e\u7279\u5b9a\u7eb9\u7406\u3001\u5149\u7167\u6761\u4ef6\u6216\u5185\u5bb9\uff08\u5982\u4eba\u8138\u3001\u6587\u672c\uff09\u7684\u8d85\u5206\u8fa8\u7387\u6548\u679c\u3002 Key Findings: This study introduces CATformer (Contrastive Adversarial Transformer), a novel neural network integrating diffusion-inspired feature refinement with adversarial and contrastive learning. Extensive experiments on benchmark datasets demonstrate that CATformer outperforms recent transformer-based and diffusion-inspired methods both in efficiency and visual image quality. Links: PDF arXiv RoofSeg: An edge-aware transformer-based network for end-to-end roof plane segmentation Authors: Siyuan You, Guozheng Xu, Pengwei Zhou, Qiwen Jin, Jian Yao, Li Li Published: 2025-08-26 Categories: cs.CV, cs.AI Abstract: Roof plane segmentation is one of the key procedures for reconstructing three-dimensional (3D) building models at levels of detail (LoD) 2 and 3 from airborne light detection and ranging (LiDAR) point clouds. The majority of current approaches for roof plane segmentation rely on the manually designed or learned features followed by some specifically designed geometric clustering strategies. Because the learned features are more powerful than the manually designed features, the deep learning-based approaches usually perform better than the traditional approaches. However, the current deep learning-based approaches have three unsolved problems. The first is that most of them are not truly end-to-end, the plane segmentation results may be not optimal. The second is that the point feature discriminability near the edges is relatively low, leading to inaccurate planar edges. The third is that the planar geometric characteristics are not sufficiently considered to constrain the network training. To solve these issues, a novel edge-aware transformer-based network, named RoofSeg, is developed for segmenting roof planes from LiDAR point clouds in a truly end-to-end manner. In the RoofSeg, we leverage a transformer encoder-decoder-based framework to hierarchically predict the plane instance masks with the use of a set of learnable plane queries. To further improve the segmentation accuracy of edge regions, we also design an Edge-Aware Mask Module (EAMM) that sufficiently incorporates planar geometric prior of edges to enhance its discriminability for plane instance mask refinement. In addition, we propose an adaptive weighting strategy in the mask loss to reduce the influence of misclassified points, and also propose a new plane geometric loss to constrain the network training. Analysis: \u4f5c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u4e13\u5bb6\uff0c\u6211\u5bf9\u8fd9\u7bc7\u5173\u4e8eRoofSeg\u7684\u8bba\u6587\u6458\u8981\u8fdb\u884c\u4e86\u6df1\u5165\u5206\u6790\uff1a 1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u603b\u7ed3 (Concise summary) \u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRoofSeg\u7684\u8fb9\u7f18\u611f\u77e5Transformer\u7f51\u7edc\uff0c\u65e8\u5728\u5b9e\u73b0\u4eceLiDAR\u70b9\u4e91\u4e2d\u7aef\u5230\u7aef\u7684\u5c4b\u9876\u5e73\u9762\u5206\u5272\u3002\u5b83\u901a\u8fc7\u7ed3\u5408Transformer\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\u3001\u4e13\u95e8\u8bbe\u8ba1\u7684\u8fb9\u7f18\u611f\u77e5\u63a9\u7801\u6a21\u5757\uff08EAMM\uff09\u4ee5\u53ca\u65b0\u7684\u51e0\u4f55\u635f\u5931\u51fd\u6570\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u975e\u7aef\u5230\u7aef\u3001\u8fb9\u7f18\u5206\u5272\u7cbe\u5ea6\u4f4e\u548c\u7f3a\u4e4f\u51e0\u4f55\u7ea6\u675f\u7b49\u65b9\u9762\u7684\u6311\u6218\u3002 2. \u5173\u952e\u521b\u65b0\u70b9\u6216\u65b9\u6cd5\u5b66 (Key innovation or methodological approach) \u7aef\u5230\u7aefTransformer\u67b6\u6784\uff1a \u9996\u6b21\u5c06Transformer\u7f16\u7801\u5668-\u89e3\u7801\u5668\u6846\u67b6\u5e94\u7528\u4e8e\u5c4b\u9876\u5e73\u9762\u5206\u5272\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u5e73\u9762\u67e5\u8be2\uff08learnable plane queries\uff09\u76f4\u63a5\u9884\u6d4b\u5e73\u9762\u5b9e\u4f8b\u63a9\u7801\uff0c\u5b9e\u73b0\u4e86\u771f\u6b63\u7684\u7aef\u5230\u7aef\uff08end-to-end\uff09\u5206\u5272\uff0c\u907f\u514d\u4e86\u540e\u5904\u7406\u7684\u6b21\u4f18\u6027\u3002 \u8fb9\u7f18\u611f\u77e5\u63a9\u7801\u6a21\u5757\uff08EAMM\uff09\uff1a \u9488\u5bf9\u8fb9\u7f18\u533a\u57df\u7279\u5f81\u5224\u522b\u529b\u4f4e\u7684\u95ee\u9898\uff0c\u8bbe\u8ba1\u4e86EAMM\uff0c\u8be5\u6a21\u5757\u5145\u5206\u878d\u5165\u4e86\u5e73\u9762\u7684\u51e0\u4f55\u5148\u9a8c\u77e5\u8bc6\uff0c\u4ee5\u663e\u8457\u63d0\u5347\u8fb9\u7f18\u533a\u57df\u7684\u5206\u5272\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002 \u7efc\u5408\u635f\u5931\u51fd\u6570\u8bbe\u8ba1\uff1a \u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u52a0\u6743\u7b56\u7565\u7684\u63a9\u7801\u635f\u5931\uff0c\u4ee5\u964d\u4f4e\u8bef\u5206\u7c7b\u70b9\u7684\u5f71\u54cd\uff1b\u540c\u65f6\u5f15\u5165\u4e86\u65b0\u7684\u5e73\u9762\u51e0\u4f55\u635f\u5931\uff0c\u7528\u4e8e\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u663e\u5f0f\u5730\u7ea6\u675f\u7f51\u7edc\u5b66\u4e60\uff0c\u786e\u4fdd\u5206\u5272\u7ed3\u679c\u7684\u51e0\u4f55\u4e00\u81f4\u6027\u3002 3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd (Potential impact on the field) \u63d0\u53473D\u5efa\u7b51\u6a21\u578b\u91cd\u5efa\u7cbe\u5ea6\uff1a \u901a\u8fc7\u63d0\u4f9b\u9ad8\u7cbe\u5ea6\u7684\u5c4b\u9876\u5e73\u9762\u5206\u5272\uff0c\u7279\u522b\u662f\u5bf9\u8fb9\u7f18\u533a\u57df\u7684\u7cbe\u786e\u5904\u7406\uff0c\u5c06\u76f4\u63a5\u63d0\u5347LoD 2\u548cLoD 3\u7ea7\u522b3D\u5efa\u7b51\u6a21\u578b\u7684\u91cd\u5efa\u8d28\u91cf\u548c\u81ea\u52a8\u5316\u6c34\u5e73\u3002 \u63a8\u52a8\u70b9\u4e91\u8bed\u4e49/\u5b9e\u4f8b\u5206\u5272\u53d1\u5c55\uff1a \u5f15\u5165Transformer\u67b6\u6784\u548c\u51e0\u4f55\u5148\u9a8c\u7ea6\u675f\uff0c\u4e3a\u70b9\u4e91\u6570\u636e\u4e2d\u590d\u6742\u51e0\u4f55\u4f53\u7684\u7aef\u5230\u7aef\u5206\u5272\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u7684\u8303\u5f0f\uff0c\u53ef\u80fd\u542f\u53d1\u5176\u4ed6\u7ed3\u6784\u5316\u573a\u666f\uff08\u5982\u5ba4\u5185\u3001\u5de5\u4e1a\u90e8\u4ef6\uff09\u7684\u5206\u5272\u65b9\u6cd5\u3002 \u7b80\u5316\u5de5\u4f5c\u6d41\u7a0b\uff1a \u771f\u6b63\u7684\u7aef\u5230\u7aef\u65b9\u6cd5\u51cf\u5c11\u4e86\u5bf9\u590d\u6742\u540e\u5904\u7406\u7684\u4f9d\u8d56\uff0c\u7b80\u5316\u4e86\u4ece\u539f\u59cbLiDAR\u70b9\u4e91\u5230\u7ed3\u6784\u5316\u5c4b\u9876\u6a21\u578b\u7684\u6574\u4e2a\u6d41\u7a0b\uff0c\u63d0\u9ad8\u4e86\u6548\u7387\u3002 4. \u76f8\u5173\u5e94\u7528\u9886\u57df (Related areas or applications that might benefit from this research) 3D\u57ce\u5e02\u5efa\u6a21\u4e0e\u6570\u5b57\u5b6a\u751f\uff1a \u9ad8\u7cbe\u5ea6\u5c4b\u9876\u6a21\u578b\u662f\u6784\u5efa\u7cbe\u7ec6\u5316\u57ce\u5e02\u6570\u5b57\u5b6a\u751f\u548c\u5730\u7406\u4fe1\u606f\u7cfb\u7edf\uff08GIS\uff09\u7684\u57fa\u7840\u3002 \u667a\u6167\u57ce\u5e02\u5e94\u7528\uff1a \u4f8b\u5982\uff0c\u5c4b\u9876\u592a\u9633\u80fd\u677f\u5b89\u88c5\u6f5c\u529b\u8bc4\u4f30\u3001\u57ce\u5e02\u70ed\u5c9b\u6548\u5e94\u5206\u6790\u3001\u5efa\u7b51\u80fd\u8017\u6a21\u62df\u7b49\u3002 \u707e\u5bb3\u8bc4\u4f30\u4e0e\u7ba1\u7406\uff1a \u5feb\u901f\u51c6\u786e\u5730\u8bc4\u4f30\u5c4b\u9876\u5728\u81ea\u7136\u707e\u5bb3\uff08\u5982\u5730\u9707\u3001\u98d3\u98ce\uff09\u540e\u7684\u635f\u574f\u60c5\u51b5\u3002 \u81ea\u52a8\u9a7e\u9a76\u4e0e\u673a\u5668\u4eba\u5bfc\u822a\uff1a \u5e2e\u52a9\u8f66\u8f86\u548c\u673a\u5668\u4eba\u66f4\u597d\u5730\u7406\u89e3\u548c\u611f\u77e5\u5468\u56f4\u7684\u5efa\u7b51\u73af\u5883\u3002 \u5efa\u7b51\u4fe1\u606f\u6a21\u578b\uff08BIM\uff09\u4e0e\u6d4b\u7ed8\uff1a \u4e3a\u5efa\u7b51\u8bbe\u8ba1\u3001\u65bd\u5de5\u548c\u7ef4\u62a4\u63d0\u4f9b\u7cbe\u786e\u7684\u51e0\u4f55\u6570\u636e\u3002 5. \u53ef\u63a8\u65ad\u7684\u5c40\u9650\u6027 (Any limitations that can be inferred from the abstract) \u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\uff1a Transformer\u6a21\u578b\u901a\u5e38\u5177\u6709\u8f83\u9ad8\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u5185\u5b58\u6d88\u8017\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u5927\u89c4\u6a21\u70b9\u4e91\u6570\u636e\u65f6\uff0c\u8bad\u7ec3\u548c\u63a8\u7406\u65f6\u95f4\u53ef\u80fd\u8f83\u957f\u3002 \u5bf9LiDAR\u70b9\u4e91\u8d28\u91cf\u7684\u4f9d\u8d56\uff1a \u5c3d\u7ba1\u58f0\u79f0\u201c\u8fb9\u7f18\u611f\u77e5\u201d\uff0c\u4f46LiDAR\u70b9\u4e91\u7684\u5bc6\u5ea6\u3001\u566a\u58f0\u548c\u626b\u63cf\u89d2\u5ea6\u4ecd\u53ef\u80fd\u5f71\u54cd\u8fb9\u7f18\u533a\u57df\u7684\u7cbe\u5ea6\uff0c\u7279\u522b\u662f\u5728\u70b9\u4e91\u7a00\u758f\u6216\u5b58\u5728\u906e\u6321\u7684\u533a\u57df\u3002 \u590d\u6742\u5c4b\u9876\u7ed3\u6784\u4e0e\u6cdb\u5316\u6027\uff1a \u62bd\u8c61\u4e2d\u672a\u63d0\u53ca\u6a21\u578b\u5bf9\u6781\u7aef\u590d\u6742\u3001\u975e\u6807\u51c6\u6216\u5177\u6709\u5927\u91cf\u9644\u5c5e\u7269\uff08\u5982\u70df\u56f1\u3001\u5929\u7a97\u3001\u690d\u88ab\u8986\u76d6\uff09\u7684\u5c4b\u9876\u7ed3\u6784\u7684\u6cdb\u5316\u80fd\u529b\u3002 \u51e0\u4f55\u5148\u9a8c\u7684\u5b9a\u4e49\u4e0e\u9002\u7528\u6027\uff1a \u201c\u5e73\u9762\u51e0\u4f55\u5148\u9a8c\u201d\u7684\u5177\u4f53\u5b9e\u73b0\u65b9\u5f0f\u548c\u5176\u5728\u5404\u79cd\u5c4b\u9876\u7c7b\u578b\uff08\u5982\u66f2\u9762\u5c4b\u9876\uff09\u4e0a\u7684\u9002\u7528\u6027\u5c1a\u4e0d\u660e\u786e\u3002\u5982\u679c\u5148\u9a8c\u8fc7\u4e8e\u521a\u6027\uff0c\u53ef\u80fd\u4f1a\u9650\u5236\u6a21\u578b\u7684\u7075\u6d3b\u6027\u3002 \u6570\u636e\u6807\u6ce8\u6210\u672c\uff1a \u7aef\u5230\u7aef\u8bad\u7ec3\u901a\u5e38\u9700\u8981\u5927\u91cf\u7684\u7cbe\u786e\u6807\u6ce8\u6570\u636e\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u8fb9\u7f18\u533a\u57df\u548c\u5e73\u9762\u5b9e\u4f8b\u7684\u6807\u6ce8\uff0c\u8fd9\u53ef\u80fd\u662f\u4e00\u4e2a\u6311\u6218\u3002 Key Findings: To solve these issues, a novel edge-aware transformer-based network, named RoofSeg, is developed for segmenting roof planes from LiDAR point clouds in a truly end-to-end manner. In addition, we propose an adaptive weighting strategy in the mask loss to reduce the influence of misclassified points, and also propose a new plane geometric loss to constrain the network training. Links: PDF arXiv EndoUFM: Utilizing Foundation Models for Monocular depth estimation of endoscopic images Authors: Xinning Yao, Bo Liu, Bojian Li, Jingjing Wang, Jinghua Yue, Fugen Zhou Published: 2025-08-25 Categories: cs.CV Abstract: Depth estimation is a foundational component for 3D reconstruction in minimally invasive endoscopic surgeries. However, existing monocular depth estimation techniques often exhibit limited performance to the varying illumination and complex textures of the surgical environment. While powerful visual foundation models offer a promising solution, their training on natural images leads to significant domain adaptability limitations and semantic perception deficiencies when applied to endoscopy. In this study, we introduce EndoUFM, an unsupervised monocular depth estimation framework that innovatively integrating dual foundation models for surgical scenes, which enhance the depth estimation performance by leveraging the powerful pre-learned priors. The framework features a novel adaptive fine-tuning strategy that incorporates Random Vector Low-Rank Adaptation (RVLoRA) to enhance model adaptability, and a Residual block based on Depthwise Separable Convolution (Res-DSC) to improve the capture of fine-grained local features. Furthermore, we design a mask-guided smoothness loss to enforce depth consistency within anatomical tissue structures. Extensive experiments on the SCARED, Hamlyn, SERV-CT, and EndoNeRF datasets confirm that our method achieves state-of-the-art performance while maintaining an efficient model size. This work contributes to augmenting surgeons' spatial perception during minimally invasive procedures, thereby enhancing surgical precision and safety, with crucial implications for augmented reality and navigation systems. Analysis: \u4f5c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u4e13\u5bb6\uff0c\u6211\u5bf9\u8fd9\u7bc7\u5173\u4e8eEndoUFM\u7684\u8bba\u6587\u6458\u8981\u8fdb\u884c\u4e86\u6df1\u5165\u5206\u6790\uff1a EndoUFM: Utilizing Foundation Models for Monocular depth estimation of endoscopic images 1. \u8bba\u6587\u6838\u5fc3\u8d21\u732e\u603b\u7ed3 (Concise Summary) \u672c\u6587\u63d0\u51fa\u4e86EndoUFM\uff0c\u4e00\u4e2a\u9488\u5bf9\u5185\u7aa5\u955c\u56fe\u50cf\u7684\u65e0\u76d1\u7763\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u7b97\u6cd5\u3002\u5b83\u521b\u65b0\u6027\u5730\u6574\u5408\u4e86\u53cc\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u81ea\u9002\u5e94\u5fae\u8c03\u7b56\u7565\uff08RVLoRA\uff09\u3001\u6b8b\u5dee\u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\u5757\uff08Res-DSC\uff09\u4ee5\u53ca\u63a9\u819c\u5f15\u5bfc\u5e73\u6ed1\u635f\u5931\uff0c\u6709\u6548\u514b\u670d\u4e86\u57fa\u7840\u6a21\u578b\u5728\u5185\u7aa5\u955c\u9886\u57df\u5b58\u5728\u7684\u57df\u9002\u5e94\u548c\u8bed\u4e49\u611f\u77e5\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6df1\u5ea6\u4f30\u8ba1\u6027\u80fd\u3002\u8be5\u65b9\u6cd5\u65e8\u5728\u589e\u5f3a\u5916\u79d1\u533b\u751f\u7684\u7a7a\u95f4\u611f\u77e5\u80fd\u529b\uff0c\u63d0\u9ad8\u5fae\u521b\u624b\u672f\u7684\u7cbe\u786e\u6027\u548c\u5b89\u5168\u6027\u3002 2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u5b66\u65b9\u6cd5 (Key Innovation or Methodological Approach) EndoUFM\u7684\u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u5176 \u53cc\u57fa\u7840\u6a21\u578b\u7684\u521b\u65b0\u6027\u96c6\u6210 \uff0c\u65e8\u5728\u5229\u7528\u9884\u8bad\u7ec3\u7684\u5f3a\u5927\u5148\u9a8c\u77e5\u8bc6\u6765\u89e3\u51b3\u5185\u7aa5\u955c\u56fe\u50cf\u6df1\u5ea6\u4f30\u8ba1\u7684\u6311\u6218\u3002\u4e3a\u89e3\u51b3\u57fa\u7840\u6a21\u578b\u5728\u5185\u7aa5\u955c\u9886\u57df\u5b58\u5728\u7684\u663e\u8457\u57df\u9002\u5e94\u6027\u9650\u5236\u548c\u8bed\u4e49\u611f\u77e5\u7f3a\u9677\uff0c\u5b83\u5f15\u5165\u4e86\u4ee5\u4e0b\u5173\u952e\u65b9\u6cd5\uff1a \u57fa\u4e8e\u968f\u673a\u5411\u91cf\u4f4e\u79e9\u9002\u5e94\uff08RVLoRA\uff09\u7684\u81ea\u9002\u5e94\u5fae\u8c03\u7b56\u7565 \uff1a\u8fd9\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u6a21\u578b\u9002\u5e94\u6280\u672f\uff0c\u5141\u8bb8\u5728\u4e0d\u5b8c\u5168\u91cd\u65b0\u8bad\u7ec3\u6574\u4e2a\u5927\u578b\u57fa\u7840\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u5c11\u91cf\u53ef\u8bad\u7ec3\u53c2\u6570\u6765\u589e\u5f3a\u6a21\u578b\u5bf9\u5185\u7aa5\u955c\u56fe\u50cf\u7684\u9002\u5e94\u6027\u3002 \u57fa\u4e8e\u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\u7684\u6b8b\u5dee\u5757\uff08Res-DSC\uff09 \uff1a\u8fd9\u79cd\u8bbe\u8ba1\u65e8\u5728\u66f4\u6709\u6548\u5730\u6355\u83b7\u5185\u7aa5\u955c\u56fe\u50cf\u4e2d\u7cbe\u7ec6\u7684\u5c40\u90e8\u7279\u5f81\uff0c\u8fd9\u5bf9\u4e8e\u533a\u5206\u590d\u6742\u7ec4\u7ec7\u7ed3\u6784\u548c\u5fae\u5c0f\u75c5\u53d8\u81f3\u5173\u91cd\u8981\u3002 \u63a9\u819c\u5f15\u5bfc\u7684\u5e73\u6ed1\u635f\u5931\uff08Mask-guided smoothness loss\uff09 \uff1a\u901a\u8fc7\u5f15\u5165\u89e3\u5256\u7ec4\u7ec7\u63a9\u819c\u6765\u6307\u5bfc\u6df1\u5ea6\u56fe\u7684\u5e73\u6ed1\u6027\uff0c\u786e\u4fdd\u5728\u540c\u4e00\u7ec4\u7ec7\u7ed3\u6784\u5185\u90e8\u7684\u6df1\u5ea6\u4e00\u81f4\u6027\uff0c\u907f\u514d\u8de8\u8fb9\u754c\u7684\u6a21\u7cca\u6216\u4e0d\u51c6\u786e\u3002 3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd (Potential Impact on the Field) \u8fd9\u9879\u7814\u7a76\u5bf9\u5fae\u521b\u624b\u672f\u9886\u57df\u5177\u6709\u663e\u8457\u5f71\u54cd\u3002\u901a\u8fc7\u63d0\u4f9b\u9ad8\u7cbe\u5ea6\u7684\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\uff0c\u5b83\u80fd \u663e\u8457\u589e\u5f3a\u5916\u79d1\u533b\u751f\u5728\u624b\u672f\u8fc7\u7a0b\u4e2d\u7684\u7a7a\u95f4\u611f\u77e5\u80fd\u529b \uff0c\u4ece\u800c \u63d0\u9ad8\u624b\u672f\u7684\u7cbe\u786e\u6027\u548c\u5b89\u5168\u6027 \u3002\u6b64\u5916\uff0c\u5176\u6210\u679c\u5bf9\u4e8e \u589e\u5f3a\u73b0\u5b9e\uff08AR\uff09\u624b\u672f\u5bfc\u822a\u7cfb\u7edf \u548c \u672f\u4e2d\u4e09\u7ef4\u91cd\u5efa \u81f3\u5173\u91cd\u8981\uff0c\u6709\u671b\u63a8\u52a8\u8fd9\u4e9b\u6280\u672f\u7684\u4e34\u5e8a\u5e94\u7528\u3002\u4ece\u66f4\u5e7f\u6cdb\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u89d2\u5ea6\u770b\uff0c\u5b83\u4e3a \u5c06\u901a\u7528\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u6210\u529f\u8fc1\u79fb\u5e76\u9002\u5e94\u5230\u7279\u5b9a\u3001\u590d\u6742\u4e14\u6570\u636e\u7a00\u7f3a\u7684\u533b\u7597\u56fe\u50cf\u9886\u57df \u63d0\u4f9b\u4e86\u6709\u6548\u8303\u5f0f\uff0c\u5c55\u793a\u4e86\u57fa\u7840\u6a21\u578b\u5728\u4e13\u4e1a\u9886\u57df\u5e94\u7528\u7684\u5de8\u5927\u6f5c\u529b\u3002 4. \u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 (Related Areas or Applications that might benefit from this research) \u5fae\u521b\u5185\u7aa5\u955c\u624b\u672f\uff08MIS\uff09 \uff1a\u76f4\u63a5\u5e94\u7528\uff0c\u7528\u4e8e\u672f\u4e2d\u4e09\u7ef4\u91cd\u5efa\u3001\u76ee\u6807\u5b9a\u4f4d\u3001\u75c5\u7076\u6d4b\u91cf\u548c\u624b\u672f\u5668\u68b0\u8ddf\u8e2a\u3002 \u589e\u5f3a\u73b0\u5b9e\uff08AR\uff09/\u6df7\u5408\u73b0\u5b9e\uff08MR\uff09\u624b\u672f\u5bfc\u822a \uff1a\u5c06\u865a\u62df\u4fe1\u606f\uff08\u5982\u89c4\u5212\u8def\u5f84\u3001\u75c5\u7076\u8fb9\u754c\u3001\u91cd\u8981\u7ed3\u6784\uff09\u7cbe\u786e\u53e0\u52a0\u5230\u771f\u5b9e\u624b\u672f\u89c6\u91ce\u4e2d\uff0c\u63d0\u4f9b\u5b9e\u65f6\u5f15\u5bfc\u3002 \u673a\u5668\u4eba\u8f85\u52a9\u624b\u672f \uff1a\u4e3a\u624b\u672f\u673a\u5668\u4eba\u63d0\u4f9b\u66f4\u51c6\u786e\u7684\u73af\u5883\u611f\u77e5\u548c\u907f\u969c\u80fd\u529b\uff0c\u63d0\u9ad8\u81ea\u52a8\u5316\u6c34\u5e73\u3002 \u624b\u672f\u6a21\u62df\u4e0e\u8bad\u7ec3 \uff1a\u521b\u5efa\u66f4\u771f\u5b9e\u7684\u4e09\u7ef4\u624b\u672f\u573a\u666f\uff0c\u63d0\u9ad8\u533b\u5b66\u751f\u548c\u5916\u79d1\u533b\u751f\u7684\u57f9\u8bad\u6548\u679c\u3002 \u533b\u7597\u56fe\u50cf\u5206\u6790\u4e0e\u8bca\u65ad \uff1a\u4e3a\u5176\u4ed6\u57fa\u4e8e\u4e09\u7ef4\u4fe1\u606f\u7684\u5206\u6790\u4efb\u52a1\uff08\u5982\u80bf\u7624\u4f53\u79ef\u6d4b\u91cf\u3001\u5668\u5b98\u5f62\u53d8\u5206\u6790\uff09\u63d0\u4f9b\u57fa\u7840\u6570\u636e\u3002 \u901a\u7528\u57fa\u7840\u6a21\u578b\u5728\u7279\u5b9a\u9886\u57df\u7684\u9002\u5e94\u6027\u7814\u7a76 \uff1a\u4e3a\u5176\u4ed6\u4e13\u4e1a\u9886\u57df\uff08\u5982\u5de5\u4e1a\u68c0\u6d4b\u3001\u9065\u611f\u3001\u81ea\u52a8\u9a7e\u9a76\uff09\u7684\u57fa\u7840\u6a21\u578b\u5e94\u7528\u63d0\u4f9b\u501f\u9274\uff0c\u5c24\u5176\u662f\u5728\u6570\u636e\u6807\u6ce8\u6210\u672c\u9ad8\u6602\u6216\u6570\u636e\u7a00\u7f3a\u7684\u573a\u666f\u3002 5. \u4ece\u6458\u8981\u4e2d\u63a8\u65ad\u51fa\u7684\u5c40\u9650\u6027 (Any limitations that can be inferred from the abstract) \u65e0\u76d1\u7763\u5b66\u4e60\u7684\u56fa\u6709\u6311\u6218 \uff1a\u5c3d\u7ba1\u58f0\u79f0\u8fbe\u5230\u4e86SOTA\uff0c\u4f46\u65e0\u76d1\u7763\u65b9\u6cd5\u5728\u7406\u8bba\u4e0a\u53ef\u80fd\u4ecd\u65e0\u6cd5\u5b8c\u5168\u8d85\u8d8a\u5728\u5927\u91cf\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\u4e0a\u8bad\u7ec3\u7684\u76d1\u7763\u65b9\u6cd5\uff08\u5982\u679c\u6b64\u7c7b\u6570\u636e\u53ef\u7528\u7684\u8bdd\uff09\u3002\u5176\u6027\u80fd\u4e0a\u9650\u53ef\u80fd\u53d7\u9650\u4e8e\u81ea\u76d1\u7763\u4fe1\u53f7\u7684\u8d28\u91cf\u548c\u5185\u7aa5\u955c\u56fe\u50cf\u7684\u590d\u6742\u6027\u3002 \u57fa\u7840\u6a21\u578b\u7684\u8ba1\u7b97\u6210\u672c \uff1a\u867d\u7136\u6458\u8981\u63d0\u5230\u201c\u4fdd\u6301\u9ad8\u6548\u7684\u6a21\u578b\u5c3a\u5bf8\u201d\uff0c\u4f46\u201c\u53cc\u57fa\u7840\u6a21\u578b\u201d\u7684\u96c6\u6210\u901a\u5e38\u610f\u5473\u7740\u76f8\u5bf9\u8f83\u5927\u7684\u6a21\u578b\u53c2\u6570\u91cf\u548c\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u3002\u8fd9\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u6216\u9700\u8981\u6781\u4f4e\u5ef6\u8fdf\u7684\u5b9e\u65f6\u624b\u672f\u573a\u666f\u4e2d\uff0c\u53ef\u80fd\u4ecd\u662f\u4e00\u4e2a\u9700\u8981\u4ed4\u7ec6\u6743\u8861\u7684\u56e0\u7d20\u3002 \u63a9\u819c\u751f\u6210\u4f9d\u8d56\u6027 \uff1a\u6458\u8981\u4e2d\u63d0\u5230\u7684\u201c\u63a9\u819c\u5f15\u5bfc\u7684\u5e73\u6ed1\u635f\u5931\u201d\u9700\u8981\u51c6\u786e\u7684\u89e3\u5256\u7ec4\u7ec7\u63a9\u819c\u3002\u5982\u679c\u8fd9\u4e9b\u63a9\u819c\u9700\u8981\u4eba\u5de5\u6807\u6ce8\uff0c\u5c06\u589e\u52a0\u6570\u636e\u51c6\u5907\u7684\u6210\u672c\uff1b\u5982\u679c\u901a\u8fc7\u81ea\u52a8\u5316\u65b9\u6cd5\u751f\u6210\uff0c\u90a3\u4e48\u63a9\u819c\u672c\u8eab\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u5c06\u76f4\u63a5\u5f71\u54cd\u6df1\u5ea6\u4f30\u8ba1\u7684\u6027\u80fd\uff0c\u4e14\u81ea\u52a8\u5316\u63a9\u819c\u751f\u6210\u672c\u8eab\u4e5f\u662f\u4e00\u4e2a\u6311\u6218\u3002 \u9886\u57df\u6cdb\u5316\u6027 \uff1a\u5c3d\u7ba1\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u5185\u7aa5\u955c\u56fe\u50cf\u7684\u590d\u6742\u6027\u548c\u591a\u6837\u6027\uff08\u5982\u4e0d\u540c\u75c5\u7406\u3001\u4e0d\u540c\u5668\u68b0\u3001\u4e0d\u540c\u533b\u751f\u64cd\u4f5c\u4e60\u60ef\u3001\u6781\u7aef\u5149\u7167\u53d8\u5316\u3001\u51fa\u8840\u3001\u70df\u96fe\u7b49\uff09\u53ef\u80fd\u8fdc\u8d85\u73b0\u6709\u6570\u636e\u96c6\u7684\u8986\u76d6\u8303\u56f4\u3002\u6a21\u578b\u5728\u672a\u89c1\u8fc7\u7684\u6781\u7aef\u4e34\u5e8a\u6761\u4ef6\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u4ecd\u9700\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u3002 \u5b9e\u65f6\u6027\u8981\u6c42 \uff1a\u5bf9\u4e8e\u624b\u672f\u5bfc\u822a\u548cAR\u7cfb\u7edf\uff0c\u5b9e\u65f6\u6027\u662f\u5173\u952e\u3002\u6458\u8981\u672a\u660e\u786e\u63d0\u53ca\u6a21\u578b\u7684\u63a8\u7406\u901f\u5ea6\uff0c\u8fd9\u5bf9\u4e8e\u5b9e\u9645\u4e34\u5e8a\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002\u9ad8\u6548\u7684\u6a21\u578b\u5c3a\u5bf8\u5e76\u4e0d\u7b49\u540c\u4e8e\u9ad8\u901f\u63a8\u7406\u3002 Key Findings: In this study, we introduce EndoUFM, an unsupervised monocular depth estimation framework that innovatively integrating dual foundation models for surgical scenes, which enhance the depth estimation performance by leveraging the powerful pre-learned priors. The framework features a novel adaptive fine-tuning strategy that incorporates Random Vector Low-Rank Adaptation (RVLoRA) to enhance model adaptability, and a Residual block based on Depthwise Separable Convolution (Res-DSC) to improve the capture of fine-grained local features. Extensive experiments on the SCARED, Hamlyn, SERV-CT, and EndoNeRF datasets confirm that our method achieves state-of-the-art performance while maintaining an efficient model size. Links: PDF arXiv ISALux: Illumination and Segmentation Aware Transformer Employing Mixture of Experts for Low Light Image Enhancement Authors: Raul Balmez, Alexandru Brateanu, Ciprian Orhei, Codruta Ancuti, Cosmin Ancuti Published: 2025-08-25 Categories: cs.CV Abstract: We introduce ISALux, a novel transformer-based approach for Low-Light Image Enhancement (LLIE) that seamlessly integrates illumination and semantic priors. Our architecture includes an original self-attention block, Hybrid Illumination and Semantics-Aware Multi-Headed Self- Attention (HISA-MSA), which integrates illumination and semantic segmentation maps for en- hanced feature extraction. ISALux employs two self-attention modules to independently process illumination and semantic features, selectively enriching each other to regulate luminance and high- light structural variations in real-world scenarios. A Mixture of Experts (MoE)-based Feed-Forward Network (FFN) enhances contextual learning, with a gating mechanism conditionally activating the top K experts for specialized processing. To address overfitting in LLIE methods caused by distinct light patterns in benchmarking datasets, we enhance the HISA-MSA module with low-rank matrix adaptations (LoRA). Extensive qualitative and quantitative evaluations across multiple specialized datasets demonstrate that ISALux is competitive with state-of-the-art (SOTA) methods. Addition- ally, an ablation study highlights the contribution of each component in the proposed model. Code will be released upon publication. Analysis: \u8fd9\u7bc7\u8bba\u6587\u4ecb\u7ecd\u7684ISALux\u662f\u4e00\u4e2a\u5728\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\uff08LLIE\uff09\u9886\u57df\u5177\u6709\u521b\u65b0\u6027\u7684\u5de5\u4f5c\u3002\u4f5c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u4e13\u5bb6\uff0c\u4ee5\u4e0b\u662f\u5bf9\u5176\u6458\u8981\u7684\u8be6\u7ec6\u5206\u6790\uff1a 1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u660e\u6458\u8981 (Concise Summary) ISALux\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8eTransformer\u7684\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\uff08LLIE\uff09\u65b9\u6cd5\uff0c\u5b83\u901a\u8fc7\u4e00\u4e2a\u6df7\u5408\u5149\u7167\u548c\u8bed\u4e49\u611f\u77e5\u591a\u5934\u81ea\u6ce8\u610f\u529b\uff08HISA-MSA\uff09\u6a21\u5757\uff0c\u65e0\u7f1d\u5730\u6574\u5408\u4e86\u5149\u7167\u548c\u8bed\u4e49\u5148\u9a8c\u4fe1\u606f\u3002\u8be5\u6a21\u578b\u5229\u7528\u4e24\u4e2a\u76f8\u4e92\u589e\u5f3a\u7684\u81ea\u6ce8\u610f\u529b\u6a21\u5757\u6765\u5904\u7406\u5149\u7167\u548c\u8bed\u4e49\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u57fa\u4e8e\u4e13\u5bb6\u6df7\u5408\uff08MoE\uff09\u7684\u524d\u9988\u7f51\u7edc\u589e\u5f3a\u4e0a\u4e0b\u6587\u5b66\u4e60\uff0c\u540c\u65f6\u5f15\u5165\u4f4e\u79e9\u77e9\u9635\u9002\u5e94\uff08LoRA\uff09\u6765\u89e3\u51b3LLIE\u65b9\u6cd5\u4e2d\u5e38\u89c1\u7684\u8fc7\u62df\u5408\u95ee\u9898\u3002 2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u5b66\u65b9\u6cd5 (Key Innovation or Methodological Approach) ISALux\u7684\u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u5176 Hybrid Illumination and Semantics-Aware Multi-Headed Self-Attention (HISA-MSA) \u6a21\u5757\u3002\u8fd9\u4e2a\u6a21\u5757\u72ec\u7279\u5730\u5c06\u5149\u7167\u56fe\u548c\u8bed\u4e49\u5206\u5272\u56fe\u76f4\u63a5\u6574\u5408\u5230\u81ea\u6ce8\u610f\u529b\u673a\u5236\u4e2d\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u5bf9\u5149\u7167\u6761\u4ef6\u548c\u7269\u4f53\u8bed\u4e49\u5185\u5bb9\u90fd\u654f\u611f\u7684\u7279\u5f81\u63d0\u53d6\u3002\u5177\u4f53\u6765\u8bf4\uff1a \u53cc\u91cd\u81ea\u6ce8\u610f\u529b\u673a\u5236\u4e0e\u76f8\u4e92\u589e\u5f3a\uff1a ISALux\u91c7\u7528\u4e24\u4e2a\u72ec\u7acb\u7684\u81ea\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u5206\u522b\u5904\u7406\u5149\u7167\u7279\u5f81\u548c\u8bed\u4e49\u7279\u5f81\uff0c\u4f46\u5b83\u4eec\u80fd\u591f\u201c\u9009\u62e9\u6027\u5730\u76f8\u4e92\u4e30\u5bcc\u201d\uff0c\u8fd9\u8868\u660e\u5b83\u4eec\u4e4b\u95f4\u5b58\u5728\u4e00\u79cd\u534f\u540c\u4f5c\u7528\uff0c\u800c\u975e\u7b80\u5355\u7684\u5e76\u884c\u5904\u7406\uff0c\u4ece\u800c\u80fd\u591f\u66f4\u7cbe\u7ec6\u5730\u8c03\u8282\u4eae\u5ea6\u548c\u9ad8\u5149\u7ed3\u6784\u53d8\u5316\u3002 \u57fa\u4e8e\u4e13\u5bb6\u6df7\u5408\uff08MoE\uff09\u7684\u524d\u9988\u7f51\u7edc\uff1a \u5f15\u5165MoE-based FFN\uff0c\u901a\u8fc7\u4e00\u4e2a\u95e8\u63a7\u673a\u5236\u6709\u6761\u4ef6\u5730\u6fc0\u6d3b\u9876\u90e8\u7684K\u4e2a\u4e13\u5bb6\uff0c\u4ee5\u5b9e\u73b0\u66f4\u4e13\u4e1a\u5316\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u5904\u7406\uff0c\u8fd9\u6709\u52a9\u4e8e\u6a21\u578b\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u8fdb\u884c\u81ea\u9002\u5e94\u5b66\u4e60\u3002 LoRA\u7528\u4e8e\u89e3\u51b3\u8fc7\u62df\u5408\uff1a \u9488\u5bf9LLIE\u65b9\u6cd5\u4e2d\u56e0\u57fa\u51c6\u6570\u636e\u96c6\u5149\u7167\u6a21\u5f0f\u5dee\u5f02\u5bfc\u81f4\u7684\u8fc7\u62df\u5408\u95ee\u9898\uff0cISALux\u5728HISA-MSA\u6a21\u5757\u4e2d\u878d\u5165\u4e86\u4f4e\u79e9\u77e9\u9635\u9002\u5e94\uff08LoRA\uff09\u3002\u8fd9\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u53c2\u6570\u5fae\u8c03\u6280\u672f\uff0c\u901a\u5e38\u7528\u4e8e\u5927\u578b\u6a21\u578b\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u6a21\u578b\u5728\u4e0d\u540c\u5149\u7167\u6761\u4ef6\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002 3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd (Potential Impact on the Field) LLIE\u6027\u80fd\u63d0\u5347\uff1a \u901a\u8fc7\u6df1\u5ea6\u6574\u5408\u8bed\u4e49\u548c\u5149\u7167\u5148\u9a8c\uff0c\u5e76\u7ed3\u5408MoE\u548cLoRA\uff0cISALux\u6709\u671b\u5728\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u4efb\u52a1\u4e0a\u8fbe\u5230\u6216\u8d85\u8d8a\u73b0\u6709SOTA\u65b9\u6cd5\uff0c\u63d0\u4f9b\u66f4\u81ea\u7136\u3001\u7ec6\u8282\u66f4\u4e30\u5bcc\u7684\u589e\u5f3a\u7ed3\u679c\u3002 \u591a\u6a21\u6001\u5148\u9a8c\u6574\u5408\u7684\u65b0\u8303\u5f0f\uff1a \u8be5\u7814\u7a76\u4e3aTransformer\u67b6\u6784\u4e2d\u5982\u4f55\u6709\u6548\u878d\u5408\u4e0d\u540c\u7c7b\u578b\u7684\uff08\u5982\u51e0\u4f55\u3001\u8bed\u4e49\u3001\u7269\u7406\uff09\u5148\u9a8c\u4fe1\u606f\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u6709\u529b\u7684\u8303\u4f8b\uff0c\u8fd9\u53ef\u80fd\u542f\u53d1\u5176\u4ed6\u4f4e\u7ea7\u89c6\u89c9\u4efb\u52a1\uff08\u5982\u53bb\u96fe\u3001\u53bb\u566a\u3001\u8d85\u5206\u8fa8\u7387\uff09\u7684\u8bbe\u8ba1\u3002 \u89e3\u51b3LLIE\u6cdb\u5316\u6027\u6311\u6218\uff1a LoRA\u7684\u5e94\u7528\u4e3a\u89e3\u51b3LLIE\u6a21\u578b\u5728\u4e0d\u540c\u5149\u7167\u6570\u636e\u96c6\u4e4b\u95f4\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u9014\u5f84\uff0c\u6709\u52a9\u4e8e\u5f00\u53d1\u51fa\u66f4\u9c81\u68d2\u3001\u66f4\u5177\u5b9e\u7528\u6027\u7684\u6a21\u578b\u3002 Transformer\u5728\u4f4e\u7ea7\u89c6\u89c9\u4e2d\u7684\u8fdb\u4e00\u6b65\u5e94\u7528\uff1a \u8fdb\u4e00\u6b65\u5de9\u56fa\u4e86Transformer\u5728\u56fe\u50cf\u589e\u5f3a\u7b49\u4f4e\u7ea7\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u5c55\u793a\u4e86\u5176\u5728\u6355\u6349\u957f\u8ddd\u79bb\u4f9d\u8d56\u548c\u590d\u6742\u4e0a\u4e0b\u6587\u4fe1\u606f\u65b9\u9762\u7684\u4f18\u52bf\u3002 4. \u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 (Related Areas or Applications) \u8ba1\u7b97\u673a\u89c6\u89c9\u5728\u6311\u6218\u6027\u73af\u5883\u4e0b\u7684\u5e94\u7528\uff1a \u81ea\u52a8\u9a7e\u9a76\u3001\u76d1\u63a7\u7cfb\u7edf\u3001\u673a\u5668\u4eba\u89c6\u89c9\u3001\u65e0\u4eba\u673a\u6210\u50cf\u7b49\uff0c\u8fd9\u4e9b\u573a\u666f\u5bf9\u591c\u95f4\u6216\u4f4e\u5149\u7167\u6761\u4ef6\u4e0b\u7684\u611f\u77e5\u80fd\u529b\u6709\u6781\u9ad8\u8981\u6c42\u3002 \u6d88\u8d39\u7ea7\u6444\u5f71\uff1a \u667a\u80fd\u624b\u673a\u548c\u5176\u4ed6\u76f8\u673a\u8bbe\u5907\u5728\u4f4e\u5149\u73af\u5883\u4e0b\u7684\u56fe\u50cf\u8d28\u91cf\u63d0\u5347\u3002 \u533b\u7597\u5f71\u50cf\uff1a \u589e\u5f3a\u4f4e\u5149\u663e\u5fae\u955c\u56fe\u50cf\u6216\u5185\u7aa5\u955c\u56fe\u50cf\u7684\u8d28\u91cf\uff0c\u4ee5\u8f85\u52a9\u8bca\u65ad\u3002 \u5176\u4ed6\u56fe\u50cf\u6062\u590d\u4efb\u52a1\uff1a \u8bba\u6587\u4e2d\u6574\u5408\u591a\u6a21\u6001\u5148\u9a8c\u7684\u65b9\u6cd5\u5b66\u601d\u60f3\u53ef\u4ee5\u63a8\u5e7f\u5230\u53bb\u96fe\u3001\u53bb\u566a\u3001\u56fe\u50cf\u53bb\u96e8\u7b49\u5176\u4ed6\u9700\u8981\u4e30\u5bcc\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u56fe\u50cf\u6062\u590d\u4efb\u52a1\u3002 \u591a\u6a21\u6001\u5b66\u4e60\uff1a \u4e3a\u5982\u4f55\u5c06\u56fe\u50cf\u7684\u50cf\u7d20\u7ea7\u4fe1\u606f\u4e0e\u9ad8\u7ea7\u8bed\u4e49\u4fe1\u606f\u6709\u6548\u7ed3\u5408\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7814\u7a76\u65b9\u5411\u3002 5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u7684\u5c40\u9650\u6027 (Limitations Inferred from the Abstract) \u5bf9\u8bed\u4e49\u5148\u9a8c\u7684\u4f9d\u8d56\uff1a \u6a21\u578b\u4f9d\u8d56\u4e8e\u8bed\u4e49\u5206\u5272\u56fe\u3002\u8fd9\u610f\u5473\u7740\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u8981\u4e48\u9700\u8981\u4e00\u4e2a\u989d\u5916\u7684\u3001\u9884\u8bad\u7ec3\u7684\u8bed\u4e49\u5206\u5272\u6a21\u578b\uff0c\u8fd9\u4f1a\u589e\u52a0\u8ba1\u7b97\u5f00\u9500\u548c\u6f5c\u5728\u7684\u9519\u8bef\u4f20\u64ad\uff08\u5982\u679c\u5206\u5272\u4e0d\u51c6\u786e\uff09\uff0c\u8981\u4e48\u6a21\u578b\u9700\u8981\u540c\u65f6\u5b66\u4e60\u5206\u5272\u548c\u589e\u5f3a\uff0c\u8fd9\u4f1a\u589e\u52a0\u6a21\u578b\u7684\u590d\u6742\u6027\u3002\u6458\u8981\u4e2d\u63d0\u5230\u201cintegrates...semantic priors\u201d\uff0c\u66f4\u503e\u5411\u4e8e\u524d\u8005\u3002 \u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\uff1a Transformer\u6a21\u578b\u672c\u8eab\u901a\u5e38\u8ba1\u7b97\u91cf\u8f83\u5927\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u65f6\u3002\u6b64\u5916\uff0cMoE\u7ed3\u6784\u867d\u7136\u5728\u53c2\u6570\u6548\u7387\u4e0a\u53ef\u80fd\u6709\u6240\u4f18\u52bf\uff0c\u4f46\u5728\u63a8\u7406\u65f6\u6fc0\u6d3b\u591a\u4e2a\u4e13\u5bb6\u4e5f\u53ef\u80fd\u589e\u52a0\u8ba1\u7b97\u8d1f\u62c5\u3002\u6458\u8981\u672a\u63d0\u53ca\u6a21\u578b\u7684\u6548\u7387\u6216\u5b9e\u65f6\u6027\u3002 \u8bad\u7ec3\u590d\u6742\u6027\uff1a \u7ed3\u5408\u4e86HISA-MSA\u3001MoE\u548cLoRA\u7684\u590d\u6742\u67b6\u6784\uff0c\u5176\u8bad\u7ec3\u8fc7\u7a0b\u53ef\u80fd\u9700\u8981\u5927\u91cf\u7684\u8ba1\u7b97\u8d44\u6e90\u548c\u7cbe\u7ec6\u7684\u8d85\u53c2\u6570\u8c03\u4f18\u3002 LoRA\u7684\u9002\u7528\u6027\uff1a \u5c3d\u7ba1LoRA\u6709\u52a9\u4e8e\u7f13\u89e3\u8fc7\u62df\u5408\uff0c\u4f46\u5176\u6548\u679c\u53ef\u80fd\u4ecd\u53d7\u9650\u4e8e\u8bad\u7ec3\u6570\u636e\u7684\u591a\u6837\u6027\u3002\u5728\u6781\u7aef\u6216\u672a\u89c1\u7684\u4f4e\u5149\u573a\u666f\u4e0b\uff0c\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u4ecd\u53ef\u80fd\u9762\u4e34\u6311\u6218\u3002 \u5b9e\u65f6\u6027\u4e0e\u90e8\u7f72\uff1a \u6458\u8981\u672a\u63d0\u4f9b\u5173\u4e8e\u6a21\u578b\u5927\u5c0f\u3001\u63a8\u7406\u901f\u5ea6\u6216\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72\u6f5c\u529b\u7684\u4fe1\u606f\uff0c\u8fd9\u5bf9\u4e8e\u5b9e\u9645\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002 Key Findings: We introduce ISALux, a novel transformer-based approach for Low-Light Image Enhancement (LLIE) that seamlessly integrates illumination and semantic priors. Extensive qualitative and quantitative evaluations across multiple specialized datasets demonstrate that ISALux is competitive with state-of-the-art (SOTA) methods. Addition- ally, an ablation study highlights the contribution of each component in the proposed model. Links: PDF arXiv","title":"Arxiv Computer Vision Papers - 2025-08-27"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-27/#arxiv-computer-vision-papers-2025-08-27","text":"","title":"Arxiv Computer Vision Papers - 2025-08-27"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-27/#executive-summary","text":"\u597d\u7684\uff0c\u8fd9\u662f\u4e00\u4efd\u9488\u5bf92025\u5e748\u670825\u65e5Arxiv\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u6700\u65b0\u8bba\u6587\u7684\u6267\u884c\u6458\u8981\u3002 Arxiv \u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u6700\u65b0\u8bba\u6587\u6267\u884c\u6458\u8981 (2025-08-25) \u62a5\u544a\u65e5\u671f: 2025\u5e748\u670825\u65e5 \u62a5\u544a\u4eba: [\u60a8\u7684\u59d3\u540d/\u7814\u7a76\u52a9\u7406] \u672c\u62a5\u544a\u65e8\u5728\u4e3a\u5fd9\u788c\u7684\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e00\u4efd\u5173\u4e8e2025\u5e748\u670825\u65e5Arxiv\u4e0a\u53d1\u5e03\u768410\u7bc7\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u6700\u65b0\u8bba\u6587\u7684\u7b80\u660e\u6982\u8ff0\uff0c\u4ee5\u5e2e\u52a9\u5feb\u901f\u4e86\u89e3\u8be5\u9886\u57df\u7684\u91cd\u8981\u53d1\u5c55\u3002 1. \u4e3b\u8981\u4e3b\u9898\u4e0e\u8d8b\u52bf\u6982\u8ff0 \u672c\u6b21\u53d1\u5e03\u7684\u8bba\u6587\u5c55\u73b0\u4e86\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u51e0\u4e2a\u663e\u8457\u7684\u8d8b\u52bf\uff1a Transformer\u67b6\u6784\u7684\u6301\u7eed\u4e3b\u5bfc\u4e0e\u6df1\u5316: \u51e0\u4e4e\u4e00\u534a\u7684\u8bba\u6587\uff082, 7, 8, 10\uff09\u76f4\u63a5\u6216\u95f4\u63a5\u4f7f\u7528\u4e86Transformer\u67b6\u6784\uff0c\u5e76\u5bf9\u5176\u8fdb\u884c\u521b\u65b0\u6027\u6539\u8fdb\uff08\u5982\u51e0\u4f55\u4f4d\u7f6e\u7f16\u7801\u3001MoE\uff09\u6216\u5e94\u7528\u4e8e\u7279\u5b9a\u4efb\u52a1\uff08\u8d85\u5206\u3001\u5206\u5272\u3001\u56fe\u50cf\u589e\u5f3a\uff09\u3002 \u7279\u5b9a\u5e94\u7528\u9886\u57df\u7684\u6df1\u5ea6\u63a2\u7d22: \u533b\u7597\u5f71\u50cf\uff081, 9\uff09\u3001\u81ea\u52a8\u9a7e\u9a76\uff084, 5\uff09\u3001\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\uff087\uff09\u3001\u4f4e\u5149\u7167\u56fe\u50cf\u589e\u5f3a\uff0810\uff09\u548c\u5c4b\u9876\u5206\u5272\uff088\uff09\u7b49\u9886\u57df\u6301\u7eed\u53d7\u5230\u5173\u6ce8\uff0c\u7814\u7a76\u4eba\u5458\u81f4\u529b\u4e8e\u5c06\u5148\u8fdb\u6a21\u578b\u843d\u5730\u5230\u5b9e\u9645\u95ee\u9898\u4e2d\u3002 \u6570\u636e\u4e3a\u4e2d\u5fc3\u7684\u4eba\u5de5\u667a\u80fd (Data-Centric AI): \u8bba\u6587\u5173\u6ce8\u6570\u636e\u8d28\u91cf\uff086\uff09\u548c\u6570\u636e\u589e\u5f3a\uff081\uff09\uff0c\u5f3a\u8c03\u4e86\u9ad8\u8d28\u91cf\u6570\u636e\u548c\u6709\u6548\u6570\u636e\u5904\u7406\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u91cd\u8981\u6027\u3002 \u57fa\u7840\u6a21\u578b (Foundation Models) \u7684\u6f14\u8fdb\u4e0e\u5e94\u7528: \u8bba\u6587\u63a2\u8ba8\u4e86\u57fa\u7840\u6a21\u578b\u7684\u672a\u6765\u53d1\u5c55\u65b9\u5411\uff083\uff09\u4ee5\u53ca\u5b83\u4eec\u5728\u7279\u5b9a\u3001\u6311\u6218\u6027\u9886\u57df\uff08\u5982\u5185\u7aa5\u955c\u6df1\u5ea6\u4f30\u8ba1\uff0c9\uff09\u7684\u5e94\u7528\u6f5c\u529b\u3002 \u81ea\u76d1\u7763\u5b66\u4e60\u4e0e\u751f\u6210\u6a21\u578b: \u81ea\u76d1\u7763\u5b66\u4e60\uff085\uff09\u548c\u6269\u6563\u6a21\u578b\uff081\uff09\u4f5c\u4e3a\u5f3a\u5927\u7684\u8303\u5f0f\uff0c\u88ab\u7528\u4e8e\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u548c\u590d\u6742\u573a\u666f\u7406\u89e3\u7b49\u95ee\u9898\u3002 2. \u7279\u522b\u663e\u8457\u6216\u521b\u65b0\u7684\u8bba\u6587 \u8bba\u65872: \"Beyond flattening: a geometrically principled positional encoding for vision transformers with Weierstrass elliptic functions\" \u521b\u65b0\u70b9: \u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u9b4f\u5c14\u65af\u7279\u62c9\u65af\u692d\u5706\u51fd\u6570\u7684\u51e0\u4f55\u539f\u7406\u4f4d\u7f6e\u7f16\u7801\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edfTransformer\u4e2d\u6241\u5e73\u5316\u7684\u4f4d\u7f6e\u7f16\u7801\u65b9\u5f0f\u3002\u5b83\u53ef\u80fd\u4e3aTransformer\u7684\u5e95\u5c42\u673a\u5236\u5e26\u6765\u66f4\u6df1\u523b\u7684\u7406\u8bba\u7406\u89e3\u548c\u6027\u80fd\u63d0\u5347\uff0c\u5177\u6709\u6f5c\u5728\u7684\u67b6\u6784\u7ea7\u5f71\u54cd\u3002 \u8bba\u65873: \"Why Relational Graphs Will Save the Next Generation of Vision Foundation Models?\" \u521b\u65b0\u70b9: \u8fd9\u662f\u4e00\u7bc7\u5177\u6709\u524d\u77bb\u6027\u548c\u6982\u5ff5\u6027\u7684\u8bba\u6587\uff0c\u63a2\u8ba8\u4e86\u5173\u7cfb\u56fe\u5728\u672a\u6765\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u3002\u5b83\u6311\u6218\u4e86\u5f53\u524d\u57fa\u7840\u6a21\u578b\u4ec5\u4f9d\u8d56\u5927\u89c4\u6a21\u6570\u636e\u548c\u53c2\u6570\u7684\u8303\u5f0f\uff0c\u63d0\u51fa\u901a\u8fc7\u5f15\u5165\u5173\u7cfb\u5f52\u7eb3\u504f\u7f6e\u6765\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5bf9\u9886\u57df\u53d1\u5c55\u65b9\u5411\u5177\u6709\u6307\u5bfc\u610f\u4e49\u3002 \u8bba\u65876: \"Learning to Detect Label Errors by Making Them: A Method for Segmentation and Object Detection Datasets\" \u521b\u65b0\u70b9: \u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u201c\u5236\u9020\u201d\u6807\u7b7e\u9519\u8bef\u6765\u5b66\u4e60\u68c0\u6d4b\u771f\u5b9e\u6807\u7b7e\u9519\u8bef\u7684\u65b0\u9896\u65b9\u6cd5\u3002\u8fd9\u76f4\u63a5\u89e3\u51b3\u4e86\u5b9e\u9645\u5e94\u7528\u4e2d\u6570\u636e\u96c6\u8d28\u91cf\u63a7\u5236\u7684\u75db\u70b9\uff0c\u5bf9\u4e8e\u63d0\u9ad8\u6a21\u578b\u9c81\u68d2\u6027\u548c\u51cf\u5c11\u4eba\u5de5\u6807\u6ce8\u6210\u672c\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002 \u8bba\u65879: \"EndoUFM: Utilizing Foundation Models for Monocular depth estimation of endoscopic images\" \u521b\u65b0\u70b9: \u5c06\u57fa\u7840\u6a21\u578b\u5e94\u7528\u4e8e\u6781\u5177\u6311\u6218\u6027\u7684\u5185\u7aa5\u955c\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u4efb\u52a1\uff0c\u5c55\u793a\u4e86\u57fa\u7840\u6a21\u578b\u5728\u7279\u5b9a\u3001\u6570\u636e\u7a00\u7f3a\u4e14\u5f62\u53d8\u4e25\u91cd\u7684\u533b\u7597\u573a\u666f\u4e2d\u7684\u5f3a\u5927\u9002\u5e94\u6027\u548c\u6f5c\u529b\u3002 3. \u65b0\u5174\u7814\u7a76\u65b9\u5411\u6216\u6280\u672f Transformer\u67b6\u6784\u7684\u51e0\u4f55\u5316\u4e0e\u7406\u8bba\u6df1\u5316: \u8bba\u65872\u9884\u793a\u7740\u5bf9Transformer\u6838\u5fc3\u7ec4\u4ef6\uff08\u5982\u4f4d\u7f6e\u7f16\u7801\uff09\u8fdb\u884c\u66f4\u6df1\u5c42\u6b21\u7684\u6570\u5b66\u548c\u51e0\u4f55\u539f\u7406\u63a2\u7d22\uff0c\u4ee5\u7a81\u7834\u73b0\u6709\u6027\u80fd\u74f6\u9888\u3002 \u5c06\u5173\u7cfb\u5f52\u7eb3\u504f\u7f6e\u878d\u5165\u57fa\u7840\u6a21\u578b: \u8bba\u65873\u5f3a\u8c03\u4e86\u5728\u57fa\u7840\u6a21\u578b\u4e2d\u5f15\u5165\u7ed3\u6784\u5316\u77e5\u8bc6\uff08\u5982\u5173\u7cfb\u56fe\uff09\u7684\u91cd\u8981\u6027\uff0c\u4ee5\u63d0\u5347\u5176\u63a8\u7406\u80fd\u529b\u548c\u6cdb\u5316\u6027\uff0c\u800c\u975e\u4ec5\u4ec5\u4f9d\u8d56\u89c4\u6a21\u3002 \u6570\u636e\u8d28\u91cf\u7ba1\u7406\u4f5c\u4e3a\u53ef\u5b66\u4e60\u4efb\u52a1: \u8bba\u65876\u63d0\u51fa\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u81ea\u52a8\u68c0\u6d4b\u548c\u7ea0\u6b63\u6570\u636e\u96c6\u4e2d\u7684\u6807\u7b7e\u9519\u8bef\uff0c\u5c06\u6570\u636e\u6e05\u6d17\u4ece\u4eba\u5de5\u5bc6\u96c6\u578b\u4efb\u52a1\u8f6c\u53d8\u4e3a\u81ea\u52a8\u5316\u6d41\u7a0b\u3002 \u6269\u6563\u6a21\u578b\u5728\u6570\u636e\u589e\u5f3a\u4e2d\u7684\u5e94\u7528: \u8bba\u65871\u5c55\u793a\u4e86\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u5408\u6210\u6570\u636e\u4ee5\u589e\u5f3a\u533b\u7597\u5f71\u50cf\u6570\u636e\u96c6\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u6570\u636e\u7a00\u7f3a\u7684\u9886\u57df\u3002 \u591a\u6a21\u6001\u81ea\u76d1\u7763\u5b66\u4e60\u5728\u590d\u6742\u573a\u666f\u7406\u89e3\u4e2d\u7684\u5e94\u7528: \u8bba\u65875\u5229\u7528\u591a\u6a21\u6001\u81ea\u76d1\u7763\u6846\u67b6\u8fdb\u884c\u573a\u666f\u65e0\u5173\u7684\u53ef\u901a\u884c\u6027\u4f30\u8ba1\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7b49\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002 4. \u5efa\u8bae\u9605\u8bfb\u7684\u8bba\u6587 \u6839\u636e\u7814\u7a76\u5174\u8da3\u548c\u6f5c\u5728\u5f71\u54cd\uff0c\u5efa\u8bae\u4f18\u5148\u9605\u8bfb\u4ee5\u4e0b\u8bba\u6587\uff1a \u5bf9\u4e8e\u5173\u6ce8Transformer\u5e95\u5c42\u673a\u5236\u548c\u67b6\u6784\u521b\u65b0\u7684\u7814\u7a76\u4eba\u5458: \u8bba\u65872: \"Beyond flattening: a geometrically principled positional encoding for vision transformers with Weierstrass elliptic functions\" \u5bf9\u4e8e\u5173\u6ce8\u57fa\u7840\u6a21\u578b\u672a\u6765\u53d1\u5c55\u548c\u7406\u8bba\u74f6\u9888\u7684\u7814\u7a76\u4eba\u5458: \u8bba\u65873: \"Why Relational Graphs Will Save the Next Generation of Vision Foundation Models?\" \u5bf9\u4e8e\u4ece\u4e8b\u6570\u636e\u96c6\u6784\u5efa\u3001\u8d28\u91cf\u63a7\u5236\u6216\u6a21\u578b\u5b9e\u9645\u90e8\u7f72\u7684\u7814\u7a76\u4eba\u5458: \u8bba\u65876: \"Learning to Detect Label Errors by Making Them: A Method for Segmentation and Object Detection Datasets\" \u5bf9\u4e8e\u533b\u7597\u5f71\u50cf\u9886\u57df\u6216\u5bf9\u57fa\u7840\u6a21\u578b\u5728\u7279\u5b9a\u9886\u57df\u843d\u5730\u611f\u5174\u8da3\u7684\u7814\u7a76\u4eba\u5458: \u8bba\u65879: \"EndoUFM: Utilizing Foundation Models for Monocular depth estimation of endoscopic images\" \u8bba\u65871: \"Diffusion-Based Data Augmentation for Medical Image Segmentation\" \u5176\u4ed6\u8bba\u6587\uff084, 5, 7, 8, 10\uff09\u5bf9\u5404\u81ea\u7684\u7279\u5b9a\u5e94\u7528\u9886\u57df\u5177\u6709\u76f4\u63a5\u4ef7\u503c\uff0c\u53ef\u6839\u636e\u4e2a\u4eba\u7814\u7a76\u65b9\u5411\u9009\u62e9\u6027\u9605\u8bfb\u3002","title":"Executive Summary"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-27/#table-of-contents","text":"Diffusion-Based Data Augmentation for Medical Image Segmentation Beyond flattening: a geometrically principled positional encoding for vision transformers with Weierstrass elliptic functions Why Relational Graphs Will Save the Next Generation of Vision Foundation Models? Enhanced Drift-Aware Computer Vision Architecture for Autonomous Driving Scene-Agnostic Traversability Labeling and Estimation via a Multimodal Self-supervised Framework Learning to Detect Label Errors by Making Them: A Method for Segmentation and Object Detection Datasets CATformer: Contrastive Adversarial Transformer for Image Super-Resolution RoofSeg: An edge-aware transformer-based network for end-to-end roof plane segmentation EndoUFM: Utilizing Foundation Models for Monocular depth estimation of endoscopic images ISALux: Illumination and Segmentation Aware Transformer Employing Mixture of Experts for Low Light Image Enhancement","title":"Table of Contents"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-27/#papers","text":"","title":"Papers"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-27/#diffusion-based-data-augmentation-for-medical-image-segmentation","text":"Authors: Maham Nazir, Muhammad Aqeel, Francesco Setti Published: 2025-08-25 Categories: cs.CV, cs.LG Abstract: Medical image segmentation models struggle with rare abnormalities due to scarce annotated pathological data. We propose DiffAug a novel framework that combines textguided diffusion-based generation with automatic segmentation validation to address this challenge. Our proposed approach uses latent diffusion models conditioned on medical text descriptions and spatial masks to synthesize abnormalities via inpainting on normal images. Generated samples undergo dynamic quality validation through a latentspace segmentation network that ensures accurate localization while enabling single-step inference. The text prompts, derived from medical literature, guide the generation of diverse abnormality types without requiring manual annotation. Our validation mechanism filters synthetic samples based on spatial accuracy, maintaining quality while operating efficiently through direct latent estimation. Evaluated on three medical imaging benchmarks (CVC-ClinicDB, Kvasir-SEG, REFUGE2), our framework achieves state-of-the-art performance with 8-10% Dice improvements over baselines and reduces false negative rates by up to 28% for challenging cases like small polyps and flat lesions critical for early detection in screening applications. Analysis: \u4f5c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u4e13\u5bb6\uff0c\u6211\u5bf9\u8fd9\u7bc7\u8bba\u6587\u6458\u8981\u8fdb\u884c\u5982\u4e0b\u5206\u6790\uff1a","title":"Diffusion-Based Data Augmentation for Medical Image Segmentation"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-27/#1-concise-summary","text":"\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a DiffAug \u7684\u65b0\u9896\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u5408\u6210\u6570\u636e\u6765\u89e3\u51b3\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6a21\u578b\u5728\u5904\u7406\u7f55\u89c1\u5f02\u5e38\u65f6\u9762\u4e34\u7684\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002DiffAug \u7ed3\u5408\u4e86\u6587\u672c\u5f15\u5bfc\u7684\u6269\u6563\u6a21\u578b\u8fdb\u884c\u5f02\u5e38\u751f\u6210\uff08\u901a\u8fc7\u5728\u6b63\u5e38\u56fe\u50cf\u4e0a\u8fdb\u884c\u5185\u7ed8\uff09\uff0c\u5e76\u5229\u7528\u4e00\u4e2a\u9ad8\u6548\u7684\u6f5c\u5728\u7a7a\u95f4\u5206\u5272\u7f51\u7edc\u8fdb\u884c\u52a8\u6001\u8d28\u91cf\u9a8c\u8bc1\uff0c\u786e\u4fdd\u4e86\u751f\u6210\u6837\u672c\u7684\u7a7a\u95f4\u51c6\u786e\u6027\u3002\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u533b\u5b66\u56fe\u50cf\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u5c0f\u578b\u548c\u6241\u5e73\u75c5\u53d8\u7b49\u6311\u6218\u6027\u75c5\u4f8b\u65f6\u3002","title":"1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u660e\u6458\u8981 (Concise Summary)"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-27/#2-key-innovation-or-methodological-approach","text":"\u8be5\u8bba\u6587\u7684\u5173\u952e\u521b\u65b0\u5728\u4e8e\u5176 \u96c6\u6210\u5316\u7684\u3001\u53cc\u9636\u6bb5\u65b9\u6cd5 \uff1a 1. \u6587\u672c\u5f15\u5bfc\u7684\u6f5c\u5728\u6269\u6563\u6a21\u578b\u8fdb\u884c\u6761\u4ef6\u751f\u6210\uff1a DiffAug \u5229\u7528\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u533b\u5b66\u6587\u672c\u63cf\u8ff0\u548c\u7a7a\u95f4\u63a9\u7801\u4f5c\u4e3a\u6761\u4ef6\uff0c\u5728\u6b63\u5e38\u56fe\u50cf\u4e0a\u8fdb\u884c\u5185\u7ed8\u6765\u5408\u6210\u5404\u79cd\u5f02\u5e38\u3002\u8fd9\u4f7f\u5f97\u80fd\u591f\u6839\u636e\u6587\u672c\u63d0\u793a\u751f\u6210\u591a\u6837\u5316\u7684\u5f02\u5e38\u7c7b\u578b\uff0c\u800c\u65e0\u9700\u624b\u52a8\u6807\u6ce8\u751f\u6210\u8fc7\u7a0b\u3002 2. \u9ad8\u6548\u7684\u6f5c\u5728\u7a7a\u95f4\u5206\u5272\u7f51\u7edc\u8fdb\u884c\u52a8\u6001\u8d28\u91cf\u9a8c\u8bc1\uff1a \u8bba\u6587\u5f15\u5165\u4e86\u4e00\u4e2a\u72ec\u7279\u7684\u9a8c\u8bc1\u673a\u5236\uff0c\u901a\u8fc7\u4e00\u4e2a\u5728\u6f5c\u5728\u7a7a\u95f4\u64cd\u4f5c\u7684\u5206\u5272\u7f51\u7edc\u6765\u52a8\u6001\u8bc4\u4f30\u751f\u6210\u6837\u672c\u7684\u8d28\u91cf\u548c\u5b9a\u4f4d\u51c6\u786e\u6027\u3002\u8fd9\u79cd\u65b9\u6cd5\u4e0d\u4ec5\u786e\u4fdd\u4e86\u5408\u6210\u6570\u636e\u7684\u5b9e\u7528\u6027\uff0c\u800c\u4e14\u901a\u8fc7\u5355\u6b65\u63a8\u7406\u548c\u76f4\u63a5\u6f5c\u5728\u4f30\u8ba1\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u8fc7\u6ee4\uff0c\u907f\u514d\u4e86\u751f\u6210\u4f4e\u8d28\u91cf\u6216\u4e0d\u51c6\u786e\u7684\u6837\u672c\u3002 \u8fd9\u79cd\u5c06\u667a\u80fd\u751f\u6210\u4e0e\u9ad8\u6548\u3001\u81ea\u52a8\u9a8c\u8bc1\u76f8\u7ed3\u5408\u7684\u7b56\u7565\uff0c\u662f\u5176\u533a\u522b\u4e8e\u4f20\u7edf\u6570\u636e\u589e\u5f3a\u548c\u4e00\u822c\u6269\u6563\u6a21\u578b\u5e94\u7528\u7684\u5173\u952e\u3002","title":"2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u5b66\u65b9\u6cd5 (Key Innovation or Methodological Approach)"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-27/#3-potential-impact-on-the-field","text":"\u8fd9\u9879\u7814\u7a76\u5bf9\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u533b\u5b66\u56fe\u50cf\u5206\u6790\u9886\u57df\u5177\u6709\u6df1\u8fdc\u5f71\u54cd\uff1a * \u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u7684\u6838\u5fc3\u6311\u6218\uff1a \u5b83\u4e3a\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u957f\u671f\u5b58\u5728\u7684\u7f55\u89c1\u75be\u75c5\u548c\u5f02\u5e38\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ece\u800c\u80fd\u591f\u8bad\u7ec3\u51fa\u66f4\u9c81\u68d2\u3001\u66f4\u51c6\u786e\u7684\u5206\u5272\u6a21\u578b\u3002 * \u63d0\u5347\u8bca\u65ad\u51c6\u786e\u6027\u4e0e\u65e9\u671f\u68c0\u6d4b\uff1a \u663e\u8457\u964d\u4f4e\u4e86\u5c0f\u606f\u8089\u548c\u6241\u5e73\u75c5\u53d8\u7b49\u6311\u6218\u6027\u75c5\u4f8b\u7684\u5047\u9634\u6027\u7387\uff0c\u8fd9\u610f\u5473\u7740\u53ef\u4ee5\u66f4\u65e9\u3001\u66f4\u51c6\u786e\u5730\u68c0\u6d4b\u5230\u5173\u952e\u75be\u75c5\uff0c\u5bf9\u764c\u75c7\u7b5b\u67e5\u7b49\u5e94\u7528\u5177\u6709\u5de8\u5927\u7684\u4e34\u5e8a\u4ef7\u503c\u3002 * \u63a8\u52a8\u5408\u6210\u6570\u636e\u5728\u533b\u7597\u9886\u57df\u7684\u5e94\u7528\uff1a \u8bc1\u660e\u4e86\u9ad8\u8d28\u91cf\u3001\u9a8c\u8bc1\u8fc7\u7684\u5408\u6210\u6570\u636e\u5728\u533b\u7597\u9886\u57df\u4f5c\u4e3a\u771f\u5b9e\u6570\u636e\u6709\u6548\u8865\u5145\u7684\u6f5c\u529b\uff0c\u53ef\u80fd\u52a0\u901f\u65b0\u7b97\u6cd5\u7684\u5f00\u53d1\u548c\u90e8\u7f72\u3002 * \u542f\u53d1\u65b0\u7684\u6570\u636e\u589e\u5f3a\u8303\u5f0f\uff1a \u5176\u7ed3\u5408\u751f\u6210\u6a21\u578b\u4e0e\u667a\u80fd\u9a8c\u8bc1\u7684\u6846\u67b6\uff0c\u53ef\u4ee5\u4e3a\u5176\u4ed6\u9700\u8981\u9ad8\u8d28\u91cf\u5408\u6210\u6570\u636e\uff08\u5c24\u5176\u662f\u5728\u6570\u636e\u4e0d\u5e73\u8861\u6216\u7a00\u7f3a\u573a\u666f\u4e0b\uff09\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u63d0\u4f9b\u65b0\u7684\u601d\u8def\u3002","title":"3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd (Potential Impact on the Field)"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-27/#4-related-areas-or-applications","text":"\u9664\u4e86\u533b\u5b66\u56fe\u50cf\u5206\u5272\u672c\u8eab\uff0c\u8fd9\u9879\u7814\u7a76\u8fd8\u53ef\u4ee5\u60e0\u53ca\u4ee5\u4e0b\u9886\u57df\u548c\u5e94\u7528\uff1a * \u533b\u5b66\u56fe\u50cf\u68c0\u6d4b\u4e0e\u5206\u7c7b\uff1a \u7c7b\u4f3c\u7684\u65b9\u6cd5\u53ef\u4ee5\u7528\u4e8e\u751f\u6210\u7f55\u89c1\u75c5\u53d8\u7684\u68c0\u6d4b\u6216\u5206\u7c7b\u4efb\u52a1\u7684\u8bad\u7ec3\u6570\u636e\u3002 * \u5c11\u6837\u672c\u5b66\u4e60 (Few-Shot Learning) \u548c\u96f6\u6837\u672c\u5b66\u4e60 (Zero-Shot Learning)\uff1a \u901a\u8fc7\u751f\u6210\u5408\u6210\u6837\u672c\uff0c\u53ef\u4ee5\u6709\u6548\u6269\u5c55\u6709\u9650\u7684\u771f\u5b9e\u6570\u636e\u96c6\uff0c\u4ece\u800c\u6539\u5584\u5c11\u6837\u672c\u6216\u96f6\u6837\u672c\u573a\u666f\u4e0b\u7684\u6a21\u578b\u6027\u80fd\u3002 * \u9886\u57df\u9002\u5e94 (Domain Adaptation)\uff1a \u751f\u6210\u7279\u5b9a\u9886\u57df\u6216\u7279\u5b9a\u8bbe\u5907\u7279\u5f81\u7684\u5408\u6210\u6570\u636e\uff0c\u6709\u52a9\u4e8e\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u6e90\u4e4b\u95f4\u8fdb\u884c\u6cdb\u5316\u3002 * \u5de5\u4e1a\u7f3a\u9677\u68c0\u6d4b\uff1a \u5728\u5de5\u4e1a\u751f\u4ea7\u4e2d\uff0c\u7f55\u89c1\u7f3a\u9677\u7684\u56fe\u50cf\u6570\u636e\u901a\u5e38\u975e\u5e38\u7a00\u7f3a\uff0c\u8be5\u65b9\u6cd5\u53ef\u4ee5\u7528\u4e8e\u751f\u6210\u8fd9\u4e9b\u7f3a\u9677\u7684\u5408\u6210\u56fe\u50cf\uff0c\u4ee5\u8bad\u7ec3\u66f4\u51c6\u786e\u7684\u68c0\u6d4b\u7cfb\u7edf\u3002 * \u81ea\u52a8\u9a7e\u9a76\uff1a \u751f\u6210\u6781\u7aef\u6216\u7f55\u89c1\u4ea4\u901a\u573a\u666f\uff08\u5982\u4e8b\u6545\u3001\u5f02\u5e38\u5929\u6c14\u6761\u4ef6\uff09\u7684\u56fe\u50cf\u6570\u636e\uff0c\u4ee5\u63d0\u9ad8\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u3002 * \u8ba1\u7b97\u673a\u56fe\u5f62\u5b66\u4e0e\u865a\u62df\u73b0\u5b9e\uff1a \u751f\u6210\u5177\u6709\u7279\u5b9a\u5c5e\u6027\u6216\u6761\u4ef6\u7684\u56fe\u50cf\u5185\u5bb9\uff0c\u7528\u4e8e\u8bad\u7ec3\u6216\u6a21\u62df\u3002","title":"4. \u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 (Related Areas or Applications)"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-27/#5-limitations-inferable-from-the-abstract","text":"\u5c3d\u7ba1\u6458\u8981\u5c55\u793a\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u7ed3\u679c\uff0c\u4f46\u4ecd\u53ef\u63a8\u65ad\u51fa\u4e00\u4e9b\u6f5c\u5728\u5c40\u9650\u6027\uff1a * \u6587\u672c\u63d0\u793a\u7684\u8d28\u91cf\u548c\u8986\u76d6\u8303\u56f4\uff1a \u6458\u8981\u63d0\u5230\u6587\u672c\u63d0\u793a\u6765\u6e90\u4e8e\u533b\u5b66\u6587\u732e\uff0c\u4f46\u5176\u80fd\u5426\u5b8c\u5168\u6355\u6349\u6240\u6709\u7f55\u89c1\u5f02\u5e38\u7684\u7ec6\u5fae\u7279\u5f81\u548c\u591a\u6837\u6027\uff0c\u4ee5\u53ca\u5982\u4f55\u5904\u7406\u6587\u732e\u4e2d\u672a\u5145\u5206\u63cf\u8ff0\u7684\u5f02\u5e38\uff0c\u4ecd\u662f\u4e00\u4e2a\u95ee\u9898\u3002\u6587\u672c\u63d0\u793a\u7684\u8d28\u91cf\u76f4\u63a5\u5f71\u54cd\u751f\u6210\u6837\u672c\u7684\u51c6\u786e\u6027\u548c\u591a\u6837\u6027\u3002 * \u751f\u6210\u6837\u672c\u7684\u771f\u5b9e\u6027\u4e0e\u4e34\u5e8a\u53ef\u4fe1\u5ea6\uff1a \u5c3d\u7ba1\u901a\u8fc7\u6f5c\u5728\u7a7a\u95f4\u9a8c\u8bc1\u786e\u4fdd\u4e86\u7a7a\u95f4\u51c6\u786e\u6027\uff0c\u4f46\u751f\u6210\u6837\u672c\u7684\u89c6\u89c9\u771f\u5b9e\u611f\uff08\u5373\u662f\u5426\u80fd\u88ab\u4e34\u5e8a\u533b\u751f\u8ba4\u4e3a\u662f\u771f\u5b9e\u7684\uff09\u4ecd\u9700\u8fdb\u4e00\u6b65\u8bc4\u4f30\u3002\u6269\u6563\u6a21\u578b\u53ef\u80fd\u5f15\u5165\u5fae\u5999\u7684\u4f2a\u5f71\u6216\u4e0d\u81ea\u7136\u7684\u7eb9\u7406\uff0c\u8fd9\u5728\u4e34\u5e8a\u5e94\u7528\u4e2d\u53ef\u80fd\u662f\u654f\u611f\u7684\u3002 * \u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\uff1a \u6269\u6563\u6a21\u578b\u901a\u5e38\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u7ed3\u5408\u6f5c\u5728\u7a7a\u95f4\u5206\u5272\u7f51\u7edc\uff0c\u6574\u4e2a\u6846\u67b6\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u53ef\u80fd\u9700\u8981\u5927\u91cf\u7684\u8ba1\u7b97\u8d44\u6e90\u548c\u65f6\u95f4\u3002 * \u5bf9\u201c\u6b63\u5e38\u56fe\u50cf\u201d\u7684\u4f9d\u8d56\uff1a \u8be5\u65b9\u6cd5\u901a\u8fc7\u5728\u6b63\u5e38\u56fe\u50cf\u4e0a\u8fdb\u884c\u5185\u7ed8\u6765\u5408\u6210\u5f02\u5e38\u3002\u8fd9\u610f\u5473\u7740\u5b83\u9700\u8981\u4e00\u4e2a\u8db3\u591f\u5927\u4e14\u591a\u6837\u5316\u7684\u201c\u6b63\u5e38\u201d\u56fe\u50cf\u6570\u636e\u96c6\u4f5c\u4e3a\u57fa\u7840\u3002\u5982\u679c\u6b63\u5e38\u56fe\u50cf\u672c\u8eab\u4e5f\u7a00\u7f3a\u6216\u5177\u6709\u9ad8\u5ea6\u53d8\u5f02\u6027\uff0c\u5219\u53ef\u80fd\u4f1a\u9650\u5236\u5176\u5e94\u7528\u3002 * \u6f5c\u5728\u7a7a\u95f4\u9a8c\u8bc1\u7684\u5c40\u9650\u6027\uff1a \u5c3d\u7ba1\u9ad8\u6548\uff0c\u4f46\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u9a8c\u8bc1\u53ef\u80fd\u65e0\u6cd5\u6355\u6349\u5230\u50cf\u7d20\u7a7a\u95f4\u4e2d\u6240\u6709\u7ec6\u5fae\u7684\u9519\u8bef\u6216\u4e0d\u4e00\u81f4\u6027\u3002\u5b83\u53ef\u80fd\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u5bf9\u751f\u6210\u8d28\u91cf\u7684\u8bc4\u4f30\u4e0d\u591f\u5168\u9762\u3002 Key Findings: We propose DiffAug a novel framework that combines textguided diffusion-based generation with automatic segmentation validation to address this challenge. Evaluated on three medical imaging benchmarks (CVC-ClinicDB, Kvasir-SEG, REFUGE2), our framework achieves state-of-the-art performance with 8-10% Dice improvements over baselines and reduces false negative rates by up to 28% for challenging cases like small polyps and flat lesions critical for early detection in screening applications. Links: PDF arXiv","title":"5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u7684\u5c40\u9650\u6027 (Limitations Inferable from the Abstract)"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-27/#beyond-flattening-a-geometrically-principled-positional-encoding-for-vision-transformers-with-weierstrass-elliptic-functions","text":"Authors: Zhihang Xin, Xitong Hu, Rui Wang Published: 2025-08-26 Categories: cs.CV Abstract: Vision Transformers have demonstrated remarkable success in computer vision tasks, yet their reliance on learnable one-dimensional positional embeddings fundamentally disrupts the inherent two-dimensional spatial structure of images through patch flattening procedures. Traditional positional encoding approaches lack geometric constraints and fail to establish monotonic correspondence between Euclidean spatial distances and sequential index distances, thereby limiting the model's capacity to leverage spatial proximity priors effectively. We propose Weierstrass Elliptic Function Positional Encoding (WEF-PE), a mathematically principled approach that directly addresses two-dimensional coordinates through natural complex domain representation, where the doubly periodic properties of elliptic functions align remarkably with translational invariance patterns commonly observed in visual data. Our method exploits the non-linear geometric nature of elliptic functions to encode spatial distance relationships naturally, while the algebraic addition formula enables direct derivation of relative positional information between arbitrary patch pairs from their absolute encodings. Comprehensive experiments demonstrate that WEF-PE achieves superior performance across diverse scenarios, including 63.78\\% accuracy on CIFAR-100 from-scratch training with ViT-Tiny architecture, 93.28\\% on CIFAR-100 fine-tuning with ViT-Base, and consistent improvements on VTAB-1k benchmark tasks. Theoretical analysis confirms the distance-decay property through rigorous mathematical proof, while attention visualization reveals enhanced geometric inductive bias and more coherent semantic focus compared to conventional approaches.The source code implementing the methods described in this paper is publicly available on GitHub. Analysis: \u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u3001\u57fa\u4e8e\u6570\u5b66\u539f\u7406\u7684\u89c6\u89c9Transformer\u4f4d\u7f6e\u7f16\u7801\u65b9\u6cd5\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u56fe\u50cf\u4e8c\u7ef4\u7a7a\u95f4\u7ed3\u6784\u65f6\u7684\u5c40\u9650\u6027\u3002 1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u6d01\u603b\u7ed3 (2-3 \u53e5\u8bdd) \u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aWeierstrass\u692d\u5706\u51fd\u6570\u4f4d\u7f6e\u7f16\u7801\uff08WEF-PE\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u65e8\u5728\u89e3\u51b3Vision Transformer\u4e2d\u901a\u8fc7\u8865\u4e01\u5c55\u5e73\u5bfc\u81f4\u7684\u4e8c\u7ef4\u7a7a\u95f4\u4fe1\u606f\u4e22\u5931\u95ee\u9898\u3002WEF-PE\u5229\u7528Weierstrass\u692d\u5706\u51fd\u6570\u5728\u590d\u6570\u57df\u4e2d\u76f4\u63a5\u7f16\u7801\u4e8c\u7ef4\u5750\u6807\uff0c\u5176\u53cc\u5468\u671f\u6027\u4e0e\u89c6\u89c9\u6570\u636e\u7684\u5e73\u79fb\u4e0d\u53d8\u6027\u76f8\u543b\u5408\uff0c\u5e76\u901a\u8fc7\u4ee3\u6570\u52a0\u6cd5\u516c\u5f0f\u76f4\u63a5\u63a8\u5bfc\u76f8\u5bf9\u4f4d\u7f6e\u4fe1\u606f\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0cWEF-PE\u663e\u8457\u63d0\u5347\u4e86ViT\u5728\u591a\u79cd\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u5e76\u589e\u5f3a\u4e86\u51e0\u4f55\u5f52\u7eb3\u504f\u7f6e\u3002 2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u8bba \u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u5c06 Weierstrass\u692d\u5706\u51fd\u6570 \u5f15\u5165\u5230Vision Transformer\u7684\u4e8c\u7ef4\u4f4d\u7f6e\u7f16\u7801\u4e2d\u3002\u5177\u4f53\u65b9\u6cd5\u8bba\u5305\u62ec\uff1a * \u76f4\u63a5\u5904\u7406\u4e8c\u7ef4\u5750\u6807\u4e0e\u590d\u6570\u57df\u8868\u793a\uff1a \u6452\u5f03\u4e86\u5c06\u4e8c\u7ef4\u56fe\u50cf\u5c55\u5e73\u4e3a\u4e00\u7ef4\u5e8f\u5217\u7684\u505a\u6cd5\uff0c\u800c\u662f\u76f4\u63a5\u5728\u590d\u6570\u57df\u4e2d\u5904\u7406\u56fe\u50cf\u7684\u4e8c\u7ef4\u5750\u6807\uff0c\u8fd9\u4e0e\u4f20\u7edf\u7684\u57fa\u4e8e\u6b63\u5f26/\u4f59\u5f26\u6216\u53ef\u5b66\u4e60\u7684\u4e00\u7ef4\u5d4c\u5165\u5f62\u6210\u9c9c\u660e\u5bf9\u6bd4\u3002 * \u5229\u7528\u692d\u5706\u51fd\u6570\u7684\u53cc\u5468\u671f\u6027\uff1a Weierstrass\u692d\u5706\u51fd\u6570\u7684\u53cc\u5468\u671f\u6027\u4e0e\u89c6\u89c9\u6570\u636e\u4e2d\u5e38\u89c1\u7684\u5e73\u79fb\u4e0d\u53d8\u6027\u6a21\u5f0f\u9ad8\u5ea6\u5951\u5408\uff0c\u4e3a\u4f4d\u7f6e\u7f16\u7801\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u51e0\u4f55\u7ea6\u675f\u3002 * \u975e\u7ebf\u6027\u51e0\u4f55\u7f16\u7801\u7a7a\u95f4\u8ddd\u79bb\uff1a \u692d\u5706\u51fd\u6570\u7684\u975e\u7ebf\u6027\u51e0\u4f55\u7279\u6027\u80fd\u591f\u66f4\u81ea\u7136\u5730\u7f16\u7801\u7a7a\u95f4\u8ddd\u79bb\u5173\u7cfb\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u65e0\u6cd5\u5728\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\u8ddd\u79bb\u548c\u5e8f\u5217\u7d22\u5f15\u8ddd\u79bb\u4e4b\u95f4\u5efa\u7acb\u5355\u8c03\u5bf9\u5e94\u5173\u7cfb\u7684\u95ee\u9898\u3002 * \u4ee3\u6570\u52a0\u6cd5\u516c\u5f0f\u63a8\u5bfc\u76f8\u5bf9\u4f4d\u7f6e\uff1a \u8bba\u6587\u6307\u51fa\uff0c\u692d\u5706\u51fd\u6570\u7684\u4ee3\u6570\u52a0\u6cd5\u516c\u5f0f\u53ef\u4ee5\u76f4\u63a5\u4ece\u7edd\u5bf9\u4f4d\u7f6e\u7f16\u7801\u4e2d\u63a8\u5bfc\u51fa\u4efb\u610f\u8865\u4e01\u5bf9\u4e4b\u95f4\u7684\u76f8\u5bf9\u4f4d\u7f6e\u4fe1\u606f\uff0c\u8fd9\u5bf9\u4e8eTransformer\u7684\u6ce8\u610f\u529b\u673a\u5236\u81f3\u5173\u91cd\u8981\u3002 3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd \u63d0\u5347Vision Transformer\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\uff1a \u901a\u8fc7\u5f15\u5165\u66f4\u5f3a\u7684\u51e0\u4f55\u5f52\u7eb3\u504f\u7f6e\uff0cWEF-PE\u6709\u671b\u663e\u8457\u63d0\u5347ViT\u5728\u5404\u79cd\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u7cbe\u7ec6\u7a7a\u95f4\u7406\u89e3\u7684\u4efb\u52a1\u4e2d\u3002 \u63a8\u52a8\u4f4d\u7f6e\u7f16\u7801\u7814\u7a76\u7684\u65b0\u8303\u5f0f\uff1a \u672c\u6587\u4e3a\u4f4d\u7f6e\u7f16\u7801\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u7684\u3001\u57fa\u4e8e\u6570\u5b66\u539f\u7406\u7684\u89c6\u89d2\uff0c\u53ef\u80fd\u4f1a\u542f\u53d1\u7814\u7a76\u8005\u63a2\u7d22\u66f4\u591a\u9ad8\u7ea7\u6570\u5b66\u5de5\u5177\u6765\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u7ed3\u6784\u5316\u6570\u636e\u7f16\u7801\u95ee\u9898\u3002 \u589e\u5f3a\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\uff1a \u7406\u8bba\u5206\u6790\u548c\u6ce8\u610f\u529b\u53ef\u89c6\u5316\u8868\u660e\uff0cWEF-PE\u80fd\u5e26\u6765\u66f4\u8fde\u8d2f\u7684\u8bed\u4e49\u7126\u70b9\u548c\u66f4\u5f3a\u7684\u51e0\u4f55\u5f52\u7eb3\u504f\u7f6e\uff0c\u6709\u52a9\u4e8e\u7406\u89e3ViT\u5982\u4f55\u5904\u7406\u7a7a\u95f4\u4fe1\u606f\u3002 \u4e3a\u5176\u4ed6\u7ed3\u6784\u5316\u6570\u636e\u7f16\u7801\u63d0\u4f9b\u501f\u9274\uff1a \u8fd9\u79cd\u5229\u7528\u7279\u5b9a\u6570\u5b66\u51fd\u6570\u7279\u6027\u6765\u7f16\u7801\u6570\u636e\u7ed3\u6784\u7684\u601d\u60f3\uff0c\u53ef\u80fd\u88ab\u63a8\u5e7f\u5230\u5176\u4ed6\u9700\u8981\u4fdd\u7559\u590d\u6742\u7ed3\u6784\u4fe1\u606f\u7684\u9886\u57df\uff0c\u4f8b\u5982\u56fe\u795e\u7ecf\u7f51\u7edc\u30013D\u70b9\u4e91\u5904\u7406\u7b49\u3002 4. \u53ef\u80fd\u53d7\u76ca\u7684\u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 \u56fe\u50cf\u5206\u7c7b\u3001\u76ee\u6807\u68c0\u6d4b\u3001\u8bed\u4e49\u5206\u5272\uff1a \u4f5c\u4e3aViT\u7684\u57fa\u7840\u7ec4\u4ef6\uff0cWEF-PE\u5c06\u76f4\u63a5\u63d0\u5347\u8fd9\u4e9b\u6838\u5fc3\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u7684\u6027\u80fd\u3002 \u533b\u5b66\u56fe\u50cf\u5206\u6790\uff1a \u5728\u533b\u5b66\u56fe\u50cf\u4e2d\uff0c\u7cbe\u786e\u7684\u7a7a\u95f4\u5173\u7cfb\u548c\u51e0\u4f55\u7ed3\u6784\u81f3\u5173\u91cd\u8981\uff0cWEF-PE\u6709\u671b\u63d0\u9ad8\u8bca\u65ad\u548c\u5206\u5272\u7684\u51c6\u786e\u6027\u3002 \u9065\u611f\u56fe\u50cf\u5904\u7406\uff1a \u9065\u611f\u56fe\u50cf\u901a\u5e38\u5177\u6709\u5927\u5c3a\u5ea6\u3001\u91cd\u590d\u6a21\u5f0f\u548c\u5e73\u79fb\u4e0d\u53d8\u6027\uff0cWEF-PE\u7684\u53cc\u5468\u671f\u6027\u7279\u6027\u53ef\u80fd\u5728\u6b64\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002 \u89c6\u9891\u7406\u89e3\uff1a \u5982\u679c\u80fd\u5c06\u4e8c\u7ef4\u7684WEF-PE\u6269\u5c55\u5230\u4e09\u7ef4\uff08\u7a7a\u95f4+\u65f6\u95f4\uff09\uff0c\u5219\u53ef\u80fd\u5bf9\u89c6\u9891Transformer\u4e2d\u7684\u65f6\u7a7a\u4f4d\u7f6e\u7f16\u7801\u4ea7\u751f\u79ef\u6781\u5f71\u54cd\u3002 \u56fe\u50cf\u751f\u6210\u4e0e\u7f16\u8f91\uff1a \u66f4\u597d\u7684\u7a7a\u95f4\u7406\u89e3\u6709\u52a9\u4e8e\u751f\u6210\u66f4\u5177\u51e0\u4f55\u4e00\u81f4\u6027\u548c\u771f\u5b9e\u611f\u7684\u56fe\u50cf\u3002 5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u7684\u5c40\u9650\u6027 \u8ba1\u7b97\u590d\u6742\u6027\uff1a Weierstrass\u692d\u5706\u51fd\u6570\u5728\u6570\u5b66\u4e0a\u8f83\u4e3a\u590d\u6742\uff0c\u5176\u8ba1\u7b97\u5f00\u9500\u53ef\u80fd\u9ad8\u4e8e\u7b80\u5355\u7684\u53ef\u5b66\u4e60\u5d4c\u5165\u6216\u6b63\u5f26/\u4f59\u5f26\u7f16\u7801\uff0c\u6458\u8981\u4e2d\u672a\u63d0\u53ca\u5177\u4f53\u7684\u8ba1\u7b97\u6548\u7387\u5bf9\u6bd4\u3002 \u53c2\u6570\u9009\u62e9\u4e0e\u5b66\u4e60\uff1a \u692d\u5706\u51fd\u6570\u901a\u5e38\u6d89\u53ca\u5468\u671f\u3001\u6a21\u6570\u7b49\u53c2\u6570\u3002\u6458\u8981\u4e2d\u672a\u8be6\u7ec6\u8bf4\u660e\u8fd9\u4e9b\u53c2\u6570\u662f\u5982\u4f55\u786e\u5b9a\uff08\u4f8b\u5982\uff0c\u56fa\u5b9a\u3001\u53ef\u5b66\u4e60\u6216\u901a\u8fc7\u67d0\u79cd\u4f18\u5316\u8fc7\u7a0b\u5f97\u5230\uff09\uff0c\u8fd9\u53ef\u80fd\u5f15\u5165\u989d\u5916\u7684\u590d\u6742\u6027\u6216\u8c03\u4f18\u96be\u5ea6\u3002 \u901a\u7528\u6027\u4e0e\u6269\u5c55\u6027\uff1a \u8bba\u6587\u5f3a\u8c03\u4e86\u201c\u53cc\u5468\u671f\u6027\u201d\u4e0e2D\u56fe\u50cf\u7684\u5951\u5408\uff0c\u4f46\u5176\u76f4\u63a5\u5e94\u7528\u4e8e3D\u6570\u636e\uff08\u5982\u70b9\u4e91\u3001\u4f53\u7d20\uff09\u6216\u975e\u7f51\u683c\u7ed3\u6784\u6570\u636e\u7684\u80fd\u529b\u548c\u65b9\u6cd5\u672a\u5728\u6458\u8981\u4e2d\u63d0\u53ca\uff0c\u53ef\u80fd\u9700\u8981\u8fdb\u4e00\u6b65\u7684\u7406\u8bba\u548c\u65b9\u6cd5\u6269\u5c55\u3002 \u7406\u8bba\u4e0e\u5b9e\u8df5\u7684\u5e73\u8861\uff1a \u5c3d\u7ba1\u5f3a\u8c03\u4e86\u201c\u6570\u5b66\u539f\u7406\u201d\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u5982\u4f55\u5e73\u8861\u7406\u8bba\u7684\u4e25\u8c28\u6027\u4e0e\u5de5\u7a0b\u5b9e\u73b0\u7684\u6548\u7387\u548c\u9c81\u68d2\u6027\uff0c\u662f\u6240\u6709\u590d\u6742\u6570\u5b66\u65b9\u6cd5\u9762\u4e34\u7684\u6311\u6218\u3002 Key Findings: We propose Weierstrass Elliptic Function Positional Encoding (WEF-PE), a mathematically principled approach that directly addresses two-dimensional coordinates through natural complex domain representation, where the doubly periodic properties of elliptic functions align remarkably with translational invariance patterns commonly observed in visual data. Our method exploits the non-linear geometric nature of elliptic functions to encode spatial distance relationships naturally, while the algebraic addition formula enables direct derivation of relative positional information between arbitrary patch pairs from their absolute encodings. Links: PDF arXiv","title":"Beyond flattening: a geometrically principled positional encoding for vision transformers with Weierstrass elliptic functions"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-27/#why-relational-graphs-will-save-the-next-generation-of-vision-foundation-models","text":"Authors: Fatemeh Ziaeetabar Published: 2025-08-25 Categories: cs.CV Abstract: Vision foundation models (FMs) have become the predominant architecture in computer vision, providing highly transferable representations learned from large-scale, multimodal corpora. Nonetheless, they exhibit persistent limitations on tasks that require explicit reasoning over entities, roles, and spatio-temporal relations. Such relational competence is indispensable for fine-grained human activity recognition, egocentric video understanding, and multimodal medical image analysis, where spatial, temporal, and semantic dependencies are decisive for performance. We advance the position that next-generation FMs should incorporate explicit relational interfaces, instantiated as dynamic relational graphs (graphs whose topology and edge semantics are inferred from the input and task context). We illustrate this position with cross-domain evidence from recent systems in human manipulation action recognition and brain tumor segmentation, showing that augmenting FMs with lightweight, context-adaptive graph-reasoning modules improves fine-grained semantic fidelity, out of distribution robustness, interpretability, and computational efficiency relative to FM only baselines. Importantly, by reasoning sparsely over semantic nodes, such hybrids also achieve favorable memory and hardware efficiency, enabling deployment under practical resource constraints. We conclude with a targeted research agenda for FM graph hybrids, prioritizing learned dynamic graph construction, multi-level relational reasoning (e.g., part object scene in activity understanding, or region organ in medical imaging), cross-modal fusion, and evaluation protocols that directly probe relational competence in structured vision tasks. Analysis: \u8fd9\u7bc7\u8bba\u6587\u6458\u8981\u63d0\u51fa\u4e86\u4e00\u79cd\u5173\u4e8e\u4e0b\u4e00\u4ee3\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08Vision Foundation Models, FMs\uff09\u53d1\u5c55\u65b9\u5411\u7684\u6df1\u523b\u89c1\u89e3\u3002\u4f5c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u4e13\u5bb6\uff0c\u6211\u5bf9\u8be5\u6458\u8981\u7684\u5206\u6790\u5982\u4e0b\uff1a","title":"Why Relational Graphs Will Save the Next Generation of Vision Foundation Models?"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-27/#why-relational-graphs-will-save-the-next-generation-of-vision-foundation-models_1","text":"1. \u8bba\u6587\u6838\u5fc3\u8d21\u732e\u7684\u7b80\u660e\u603b\u7ed3 (Concise Summary of Main Contribution) \u8bba\u6587\u6307\u51fa\u5f53\u524d\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u5728\u9700\u8981\u5b9e\u4f53\u3001\u89d2\u8272\u548c\u65f6\u7a7a\u5173\u7cfb\u663e\u5f0f\u63a8\u7406\u7684\u4efb\u52a1\u4e0a\u5b58\u5728\u5c40\u9650\u3002\u4e3a\u89e3\u51b3\u6b64\u95ee\u9898\uff0c\u8bba\u6587\u63d0\u51fa\u4e0b\u4e00\u4ee3\u57fa\u7840\u6a21\u578b\u5e94\u6574\u5408\u52a8\u6001\u5173\u7cfb\u56fe\uff08dynamic relational graphs\uff09\u4f5c\u4e3a\u663e\u5f0f\u5173\u7cfb\u63a5\u53e3\u3002\u8fd9\u79cd\u6df7\u5408\u6a21\u578b\u80fd\u663e\u8457\u63d0\u5347\u7ec6\u7c92\u5ea6\u8bed\u4e49\u7406\u89e3\u3001\u6cdb\u5316\u80fd\u529b\u3001\u53ef\u89e3\u91ca\u6027\u53ca\u8ba1\u7b97\u6548\u7387\uff0c\u4ece\u800c\u514b\u670d\u73b0\u6709FMs\u7684\u4e0d\u8db3\u3002 2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u8bba (Key Innovation or Methodological Approach) \u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u63d0\u51fa\u5c06\u201c\u52a8\u6001\u5173\u7cfb\u56fe\u201d\uff08dynamic relational graphs\uff09\u4f5c\u4e3a\u663e\u5f0f\u5173\u7cfb\u63a5\u53e3\u6574\u5408\u5230\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u4e2d\u3002\u8fd9\u4e9b\u56fe\u7684\u62d3\u6251\u7ed3\u6784\u548c\u8fb9\u8bed\u4e49\u662f\u6839\u636e\u8f93\u5165\u548c\u4efb\u52a1\u4e0a\u4e0b\u6587\u52a8\u6001\u63a8\u65ad\u7684\u3002\u901a\u8fc7\u8fd9\u79cd\u8f7b\u91cf\u7ea7\u3001\u4e0a\u4e0b\u6587\u81ea\u9002\u5e94\u7684\u56fe\u63a8\u7406\u6a21\u5757\u589e\u5f3a\u57fa\u7840\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u5bf9\u5b9e\u4f53\u3001\u89d2\u8272\u548c\u65f6\u7a7a\u5173\u7cfb\u7684\u7a00\u758f\u4e14\u9ad8\u6548\u7684\u63a8\u7406\uff0c\u4ece\u800c\u5f25\u8865\u4e86FMs\u5728\u590d\u6742\u5173\u7cfb\u7406\u89e3\u4e0a\u7684\u4e0d\u8db3\u3002 3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd (Potential Impact on the Field) \u8fd9\u9879\u7814\u7a76\u6709\u671b\u63a8\u52a8\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u8303\u5f0f\u7684\u6f14\u8fdb\uff0c\u4f7f\u5176\u4ece\u4e3b\u8981\u4f9d\u8d56\u5927\u89c4\u6a21\u6570\u636e\u5b66\u4e60\u9690\u5f0f\u8868\u793a\uff0c\u8f6c\u5411\u80fd\u591f\u8fdb\u884c\u663e\u5f0f\u3001\u7ed3\u6784\u5316\u63a8\u7406\u3002\u8fd9\u5c06\u663e\u8457\u62d3\u5bbd\u57fa\u7840\u6a21\u578b\u5728\u9700\u8981\u590d\u6742\u5173\u7cfb\u7406\u89e3\uff08\u5982\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\u3001\u533b\u7597\u5f71\u50cf\u5206\u6790\uff09\u9886\u57df\u7684\u5e94\u7528\u8303\u56f4\u548c\u6027\u80fd\u4e0a\u9650\u3002\u540c\u65f6\uff0c\u63d0\u5347\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u8d44\u6e90\u6548\u7387\uff0c\u5bf9\u4e8e\u57fa\u7840\u6a21\u578b\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u90e8\u7f72\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u53ef\u80fd\u5f15\u9886\u4e0b\u4e00\u4ee3FMs\u7684\u8bbe\u8ba1\u65b9\u5411\u3002 4. \u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 (Related Areas or Applications that Might Benefit) \u7ec6\u7c92\u5ea6\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b (Fine-grained Human Activity Recognition): \u7406\u89e3\u590d\u6742\u52a8\u4f5c\u5e8f\u5217\u4e2d\u7684\u5b9e\u4f53\u3001\u5de5\u5177\u3001\u4ea4\u4e92\u548c\u65f6\u5e8f\u5173\u7cfb\u3002 \u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u89c6\u9891\u7406\u89e3 (Egocentric Video Understanding): \u5206\u6790\u4f69\u6234\u8005\u89c6\u89d2\u4e0b\u7684\u7269\u4f53\u4ea4\u4e92\u3001\u610f\u56fe\u548c\u73af\u5883\u5173\u7cfb\u3002 \u591a\u6a21\u6001\u533b\u5b66\u56fe\u50cf\u5206\u6790 (Multimodal Medical Image Analysis): \u6574\u5408\u4e0d\u540c\u6a21\u6001\uff08\u5982MRI\u3001CT\uff09\u4fe1\u606f\uff0c\u63a8\u7406\u75c5\u7076\u3001\u5668\u5b98\u4e4b\u95f4\u7684\u7a7a\u95f4\u3001\u8bed\u4e49\u5173\u7cfb\uff0c\u4f8b\u5982\u8111\u80bf\u7624\u5206\u5272\u548c\u75be\u75c5\u8bca\u65ad\u3002 \u673a\u5668\u4eba\u64cd\u4f5c\u4e0e\u5177\u8eab\u667a\u80fd (Robotic Manipulation and Embodied AI): \u89c4\u5212\u548c\u6267\u884c\u590d\u6742\u4efb\u52a1\uff0c\u9700\u8981\u7406\u89e3\u7269\u4f53\u5c5e\u6027\u3001\u73af\u5883\u7ea6\u675f\u548c\u64cd\u4f5c\u5e8f\u5217\u3002 \u573a\u666f\u56fe\u751f\u6210\u4e0e\u89c6\u89c9\u95ee\u7b54 (Scene Graph Generation and Visual Question Answering): \u66f4\u51c6\u786e\u5730\u6355\u6349\u56fe\u50cf\u4e2d\u7684\u5b9e\u4f53\u53ca\u5176\u5173\u7cfb\uff0c\u652f\u6301\u66f4\u6df1\u5c42\u6b21\u7684\u8bed\u4e49\u7406\u89e3\u548c\u63a8\u7406\u3002 5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u7684\u5c40\u9650\u6027 (Limitations that Can Be Inferred from the Abstract) \u6027\u8d28\u4e3a\u5c55\u671b\u6027\u6216\u7efc\u8ff0\u6027 (Prospective/Review Nature): \u9274\u4e8e\u53d1\u5e03\u65e5\u671f\u662f2025\u5e74\uff0c\u4e14\u6458\u8981\u4e2d\u4f7f\u7528\u4e86\u201cWe advance the position\u201d\u3001\u201cWe illustrate this position with cross-domain evidence\u201d\u548c\u201cWe conclude with a targeted research agenda\u201d\u7b49\u8868\u8ff0\uff0c\u8fd9\u7bc7\u8bba\u6587\u66f4\u50cf\u662f\u4e00\u7bc7\u63d0\u51fa\u7814\u7a76\u65b9\u5411\u3001\u603b\u7ed3\u73b0\u6709\u8bc1\u636e\u5e76\u89c4\u5212\u672a\u6765\u8def\u7ebf\u56fe\u7684\u5c55\u671b\u6027\u6216\u7efc\u8ff0\u6027\u6587\u7ae0\uff0c\u800c\u975e\u4e00\u7bc7\u63d0\u51fa\u5168\u65b0\u6a21\u578b\u67b6\u6784\u5e76\u63d0\u4f9b\u5927\u91cf\u65b0\u5b9e\u9a8c\u7ed3\u679c\u7684\u5b9e\u8bc1\u8bba\u6587\u3002\u5176\u201c\u8bc1\u636e\u201d\u53ef\u80fd\u6765\u81ea\u5bf9\u73b0\u6709\u5de5\u4f5c\u7684\u7efc\u5408\u5206\u6790\uff0c\u800c\u975e\u672c\u6587\u9996\u6b21\u63d0\u51fa\u7684\u65b0\u5b9e\u9a8c\u3002 \u52a8\u6001\u56fe\u6784\u5efa\u7684\u590d\u6742\u6027 (Complexity of Dynamic Graph Construction): \u6458\u8981\u4e2d\u5f3a\u8c03\u4e86\u201clearned dynamic graph construction\u201d\u662f\u672a\u6765\u7684\u7814\u7a76\u91cd\u70b9\uff0c\u8fd9\u6697\u793a\u4e86\u5982\u4f55\u9ad8\u6548\u3001\u51c6\u786e\u5730\u4ece\u539f\u59cb\u8f93\u5165\u548c\u4efb\u52a1\u4e0a\u4e0b\u6587\u4e2d\u52a8\u6001\u63a8\u65ad\u51fa\u6700\u4f18\u7684\u56fe\u62d3\u6251\u548c\u8fb9\u8bed\u4e49\uff0c\u672c\u8eab\u5c31\u662f\u4e00\u4e2a\u590d\u6742\u4e14\u5c1a\u672a\u5b8c\u5168\u89e3\u51b3\u7684\u6311\u6218\u3002 \u201c\u8f7b\u91cf\u7ea7\u201d\u7684\u5b9a\u4e49\u4e0e\u6743\u8861 (Definition and Trade-offs of \"Lightweight\"): \u5c3d\u7ba1\u58f0\u79f0\u201clightweight\u201d\u548c\u201cfavorable memory and hardware efficiency\u201d\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u56fe\u63a8\u7406\u6a21\u5757\u7684\u590d\u6742\u6027\u3001\u56fe\u7684\u5927\u5c0f\u4ee5\u53ca\u4e0e\u57fa\u7840\u6a21\u578b\u878d\u5408\u7684\u5f00\u9500\uff0c\u4ecd\u53ef\u80fd\u5e26\u6765\u989d\u5916\u7684\u8ba1\u7b97\u8d1f\u62c5\uff0c\u9700\u8981\u4ed4\u7ec6\u7684\u5de5\u7a0b\u4f18\u5316\u548c\u6743\u8861\u3002 \u8bc4\u4f30\u534f\u8bae\u7684\u7f3a\u5931 (Lack of Established Evaluation Protocols): \u6458\u8981\u4e2d\u63d0\u5230\u9700\u8981\u201cevaluation protocols that directly probe relational competence\u201d\uff0c\u8fd9\u8868\u660e\u76ee\u524d\u53ef\u80fd\u7f3a\u4e4f\u6807\u51c6\u5316\u7684\u3001\u80fd\u591f\u5145\u5206\u8861\u91cf\u6a21\u578b\u5173\u7cfb\u63a8\u7406\u80fd\u529b\u7684\u57fa\u51c6\u548c\u8bc4\u4f30\u65b9\u6cd5\uff0c\u8fd9\u4f1a\u7ed9\u7814\u7a76\u8fdb\u5c55\u5e26\u6765\u6311\u6218\u3002 Key Findings: Importantly, by reasoning sparsely over semantic nodes, such hybrids also achieve favorable memory and hardware efficiency, enabling deployment under practical resource constraints. We conclude with a targeted research agenda for FM graph hybrids, prioritizing learned dynamic graph construction, multi-level relational reasoning (e.g., part object scene in activity understanding, or region organ in medical imaging), cross-modal fusion, and evaluation protocols that directly probe relational competence in structured vision tasks. Links: PDF arXiv","title":"\u8bba\u6587\u6458\u8981\u5206\u6790\uff1aWhy Relational Graphs Will Save the Next Generation of Vision Foundation Models?"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-27/#enhanced-drift-aware-computer-vision-architecture-for-autonomous-driving","text":"Authors: Md Shahi Amran Hossain, Abu Shad Ahammed, Sayeri Mukherjee, Roman Obermaisser Published: 2025-08-25 Categories: cs.CV, math.LO Abstract: The use of computer vision in automotive is a trending research in which safety and security are a primary concern. In particular, for autonomous driving, preventing road accidents requires highly accurate object detection under diverse conditions. To address this issue, recently the International Organization for Standardization (ISO) released the 8800 norm, providing structured frameworks for managing associated AI relevant risks. However, challenging scenarios such as adverse weather or low lighting often introduce data drift, leading to degraded model performance and potential safety violations. In this work, we present a novel hybrid computer vision architecture trained with thousands of synthetic image data from the road environment to improve robustness in unseen drifted environments. Our dual mode framework utilized YOLO version 8 for swift detection and incorporated a five-layer CNN for verification. The system functioned in sequence and improved the detection accuracy by more than 90\\% when tested with drift-augmented road images. The focus was to demonstrate how such a hybrid model can provide better road safety when working together in a hybrid structure. Analysis: \u597d\u7684\uff0c\u4f5c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u4e13\u5bb6\uff0c\u6211\u5c06\u5bf9\u8fd9\u7bc7\u8bba\u6587\u6458\u8981\u8fdb\u884c\u6280\u672f\u6027\u5206\u6790\u3002","title":"Enhanced Drift-Aware Computer Vision Architecture for Autonomous Driving"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-27/#enhanced-drift-aware-computer-vision-architecture-for-autonomous-driving_1","text":"1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u660e\u603b\u7ed3 (2-3 \u53e5\u8bdd) \u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6df7\u5408\u8ba1\u7b97\u673a\u89c6\u89c9\u67b6\u6784\uff0c\u65e8\u5728\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u4e2d\u56e0\u6570\u636e\u6f02\u79fb\uff08\u5982\u6076\u52a3\u5929\u6c14\u3001\u4f4e\u5149\u7167\uff09\u5bfc\u81f4\u7684\u7269\u4f53\u68c0\u6d4b\u6027\u80fd\u4e0b\u964d\u548c\u6f5c\u5728\u5b89\u5168\u95ee\u9898\u3002\u8be5\u67b6\u6784\u7ed3\u5408\u4e86YOLOv8\u8fdb\u884c\u5feb\u901f\u68c0\u6d4b\u548c\u4e00\u4e2a\u4e94\u5c42CNN\u8fdb\u884c\u9a8c\u8bc1\uff0c\u901a\u8fc7\u5e8f\u5217\u5316\u5de5\u4f5c\u6d41\u7a0b\u548c\u5408\u6210\u6570\u636e\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u6f02\u79fb\u589e\u5f3a\u56fe\u50cf\u4e0a\u7684\u68c0\u6d4b\u51c6\u786e\u7387\uff0c\u4ece\u800c\u589e\u5f3a\u4e86\u9053\u8def\u5b89\u5168\u6027\u3002 2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u5b66\u65b9\u6cd5 \u8be5\u7814\u7a76\u7684\u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u5176 \u53cc\u6a21\u6001\u6df7\u5408\u67b6\u6784\uff08dual mode hybrid architecture\uff09 \u548c \u6f02\u79fb\u611f\u77e5\uff08drift-aware\uff09 \u7684\u8bbe\u8ba1\u7406\u5ff5\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\uff1a * \u6df7\u5408\u67b6\u6784\u4e0e\u5e8f\u5217\u5316\u9a8c\u8bc1\uff1a \u7cfb\u7edf\u91c7\u7528YOLOv8\u4f5c\u4e3a\u7b2c\u4e00\u9636\u6bb5\uff0c\u5b9e\u73b0\u5feb\u901f\u7269\u4f53\u68c0\u6d4b\uff1b\u968f\u540e\uff0c\u4e00\u4e2a\u4e13\u95e8\u7684\u4e94\u5c42\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u4f5c\u4e3a\u7b2c\u4e8c\u9636\u6bb5\uff0c\u5bf9YOLOv8\u7684\u68c0\u6d4b\u7ed3\u679c\u8fdb\u884c\u9a8c\u8bc1\u3002\u8fd9\u79cd\u201c\u5feb\u901f\u68c0\u6d4b + \u6df1\u5ea6\u9a8c\u8bc1\u201d\u7684\u5e8f\u5217\u5316\u5de5\u4f5c\u6d41\u662f\u5176\u72ec\u7279\u4e4b\u5904\uff0c\u65e8\u5728\u5e73\u8861\u901f\u5ea6\u4e0e\u9c81\u68d2\u6027\u3002 * \u5408\u6210\u6570\u636e\u8bad\u7ec3\uff1a \u4e3a\u4e86\u63d0\u9ad8\u6a21\u578b\u5728\u201c\u672a\u89c1\u8fc7\u7684\u6f02\u79fb\u73af\u5883\uff08unseen drifted environments\uff09\u201d\u4e2d\u7684\u9c81\u68d2\u6027\uff0c\u8be5\u67b6\u6784\u5229\u7528\u4e86\u6570\u5343\u5f20\u5408\u6210\u56fe\u50cf\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u3002\u8fd9\u6709\u52a9\u4e8e\u6a21\u578b\u5b66\u4e60\u5e76\u9002\u5e94\u5404\u79cd\u6f5c\u5728\u7684\u6f02\u79fb\u6761\u4ef6\uff0c\u800c\u65e0\u9700\u4f9d\u8d56\u96be\u4ee5\u83b7\u53d6\u7684\u771f\u5b9e\u4e16\u754c\u6f02\u79fb\u6570\u636e\u3002 * \u4e13\u6ce8\u4e8e\u6570\u636e\u6f02\u79fb\u9c81\u68d2\u6027\uff1a \u8bba\u6587\u660e\u786e\u5c06\u6570\u636e\u6f02\u79fb\uff08\u7531\u6076\u52a3\u5929\u6c14\u3001\u4f4e\u5149\u7167\u7b49\u5f15\u8d77\uff09\u4f5c\u4e3a\u6838\u5fc3\u6311\u6218\uff0c\u5e76\u8bbe\u8ba1\u67b6\u6784\u6765\u76f4\u63a5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u8fd9\u4e0eISO 8800\u7b49\u65b0\u5174\u6807\u51c6\u5bf9AI\u98ce\u9669\u7ba1\u7406\u7684\u5173\u6ce8\u76f8\u543b\u5408\u3002 3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd \u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u5b89\u5168\u6027\uff1a \u901a\u8fc7\u6709\u6548\u5e94\u5bf9\u6570\u636e\u6f02\u79fb\uff0c\u8be5\u7814\u7a76\u6709\u671b\u663e\u8457\u63d0\u9ad8\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5728\u590d\u6742\u548c\u6076\u52a3\u6761\u4ef6\u4e0b\u7684\u7269\u4f53\u68c0\u6d4b\u53ef\u9760\u6027\uff0c\u76f4\u63a5\u5173\u7cfb\u5230\u884c\u8f66\u5b89\u5168\uff0c\u5e76\u53ef\u80fd\u6210\u4e3a\u672a\u6765AD\u7cfb\u7edf\u8bbe\u8ba1\u4e2d\u7684\u4e00\u4e2a\u91cd\u8981\u8003\u91cf\u3002 \u63a8\u52a8\u6df7\u5408\u6a21\u578b\u8303\u5f0f\uff1a \u8fd9\u79cd\u201c\u5feb\u901f\u68c0\u6d4b\u5668 + \u9a8c\u8bc1\u5668\u201d\u7684\u6df7\u5408\u67b6\u6784\u4e3a\u5176\u4ed6\u5bf9\u5b9e\u65f6\u6027\u3001\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u90fd\u6709\u9ad8\u8981\u6c42\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u7684\u8bbe\u8ba1\u8303\u5f0f\u3002 \u5f3a\u8c03\u5408\u6210\u6570\u636e\u4ef7\u503c\uff1a \u8bba\u6587\u518d\u6b21\u8bc1\u660e\u4e86\u5408\u6210\u6570\u636e\u5728\u8bad\u7ec3\u9c81\u68d2\u6a21\u578b\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u7f55\u89c1\u6216\u96be\u4ee5\u83b7\u53d6\u7684\u6f02\u79fb\u6570\u636e\u65b9\u9762\u7684\u5de8\u5927\u6f5c\u529b\u3002 \u7b26\u5408\u884c\u4e1a\u6807\u51c6\uff1a \u63d0\u53caISO 8800\u8868\u660e\u8be5\u7814\u7a76\u5177\u6709\u5f88\u5f3a\u7684\u5b9e\u9645\u5e94\u7528\u5bfc\u5411\uff0c\u5176\u6210\u679c\u53ef\u80fd\u6709\u52a9\u4e8e\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u6ee1\u8db3\u672a\u6765\u7684\u5b89\u5168\u548c\u98ce\u9669\u7ba1\u7406\u6807\u51c6\u3002 4. \u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 \u9664\u4e86\u81ea\u52a8\u9a7e\u9a76\uff0c\u4ee5\u4e0b\u9886\u57df\u6216\u5e94\u7528\u4e5f\u53ef\u80fd\u4ece\u8fd9\u9879\u7814\u7a76\u4e2d\u53d7\u76ca\uff1a * \u5de5\u4e1a\u81ea\u52a8\u5316\u4e0e\u673a\u5668\u4eba\uff1a \u5728\u751f\u4ea7\u7ebf\u6216\u4ed3\u5e93\u4e2d\uff0c\u5149\u7167\u3001\u7070\u5c18\u3001\u78e8\u635f\u7b49\u73af\u5883\u53d8\u5316\u53ef\u80fd\u5bfc\u81f4\u6570\u636e\u6f02\u79fb\uff0c\u5f71\u54cd\u673a\u5668\u4eba\u7684\u89c6\u89c9\u611f\u77e5\u548c\u64cd\u4f5c\u7cbe\u5ea6\u3002 * \u667a\u80fd\u76d1\u63a7\u7cfb\u7edf\uff1a \u6237\u5916\u76d1\u63a7\u6444\u50cf\u5934\u5e38\u9762\u4e34\u5929\u6c14\u3001\u663c\u591c\u3001\u5b63\u8282\u53d8\u5316\u7b49\u6f02\u79fb\uff0c\u8be5\u65b9\u6cd5\u53ef\u7528\u4e8e\u63d0\u9ad8\u5f02\u5e38\u4e8b\u4ef6\u68c0\u6d4b\u7684\u9c81\u68d2\u6027\u3002 * \u533b\u7597\u5f71\u50cf\u5206\u6790\uff1a \u4e0d\u540c\u7684\u8bbe\u5907\u3001\u626b\u63cf\u53c2\u6570\u6216\u60a3\u8005\u751f\u7406\u53d8\u5316\u53ef\u80fd\u5f15\u5165\u6570\u636e\u6f02\u79fb\uff0c\u5f71\u54cd\u75be\u75c5\u8bca\u65ad\u7684\u51c6\u786e\u6027\u3002 * \u822a\u7a7a\u822a\u5929\u4e0e\u65e0\u4eba\u673a\uff1a \u5728\u590d\u6742\u5927\u6c14\u6761\u4ef6\u6216\u672a\u77e5\u73af\u5883\u4e2d\u6267\u884c\u4efb\u52a1\u65f6\uff0c\u89c6\u89c9\u7cfb\u7edf\u9700\u8981\u6781\u9ad8\u7684\u9c81\u68d2\u6027\u3002 * \u4efb\u4f55\u5728\u975e\u53d7\u63a7\u73af\u5883\u4e2d\u90e8\u7f72\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u7cfb\u7edf\uff1a \u51e1\u662f\u73af\u5883\u6761\u4ef6\u591a\u53d8\u3001\u6570\u636e\u5206\u5e03\u53ef\u80fd\u968f\u65f6\u95f4\u6216\u6761\u4ef6\u53d8\u5316\u7684\u573a\u666f\uff0c\u8fd9\u79cd\u6f02\u79fb\u611f\u77e5\u548c\u9a8c\u8bc1\u7684\u67b6\u6784\u90fd\u5177\u6709\u501f\u9274\u610f\u4e49\u3002 5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u7684\u5c40\u9650\u6027 \u5408\u6210\u6570\u636e\u4e0e\u771f\u5b9e\u4e16\u754c\u7684\u5dee\u8ddd\uff08Domain Gap\uff09\uff1a \u5c3d\u7ba1\u5408\u6210\u6570\u636e\u6709\u52a9\u4e8e\u89e3\u51b3\u6f02\u79fb\u95ee\u9898\uff0c\u4f46\u5408\u6210\u6570\u636e\u4e0e\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e4b\u95f4\u901a\u5e38\u5b58\u5728\u9886\u57df\u5dee\u8ddd\u3002\u6a21\u578b\u5728\u201c\u6f02\u79fb\u589e\u5f3a\u7684\u9053\u8def\u56fe\u50cf\u201d\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5728\u771f\u5b9e\u4e16\u754c\u3001\u672a\u89c1\u8fc7\u7684\u3001\u9ad8\u5ea6\u590d\u6742\u7684\u6f02\u79fb\u6761\u4ef6\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u4ecd\u9700\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u3002 \u6027\u80fd\u6307\u6807\u7684\u76f8\u5bf9\u6027\uff1a \u201c\u68c0\u6d4b\u51c6\u786e\u7387\u63d0\u9ad8\u4e8690%\u4ee5\u4e0a\u201d\u662f\u4e00\u4e2a\u76f8\u5bf9\u63d0\u5347\u3002\u6458\u8981\u672a\u63d0\u4f9b\u57fa\u7ebf\u6a21\u578b\u7684\u7edd\u5bf9\u51c6\u786e\u7387\uff0c\u4e5f\u672a\u8bf4\u660e\u63d0\u534790%\u662f\u57fa\u4e8e\u54ea\u4e2a\u521d\u59cb\u503c\u3002\u4f8b\u5982\uff0c\u4ece10%\u63d0\u5347\u523019%\u4e5f\u662f90%\u7684\u63d0\u5347\uff0c\u4f46\u7edd\u5bf9\u6027\u80fd\u53ef\u80fd\u4ecd\u4e0d\u8db3\u4ee5\u6ee1\u8db3\u81ea\u52a8\u9a7e\u9a76\u7684\u9700\u6c42\u3002 \u5e8f\u5217\u5316\u5904\u7406\u7684\u5b9e\u65f6\u6027\u8003\u91cf\uff1a \u5c3d\u7ba1YOLOv8\u4ee5\u201cswift detection\u201d\u8457\u79f0\uff0c\u4f46\u589e\u52a0\u4e00\u4e2a\u4e94\u5c42CNN\u7684\u201c\u9a8c\u8bc1\u201d\u6b65\u9aa4\uff0c\u5fc5\u7136\u4f1a\u5f15\u5165\u989d\u5916\u7684\u8ba1\u7b97\u5ef6\u8fdf\u3002\u5bf9\u4e8e\u81ea\u52a8\u9a7e\u9a76\u8fd9\u79cd\u5bf9\u5b9e\u65f6\u6027\u8981\u6c42\u6781\u9ad8\u7684\u5e94\u7528\uff0c\u8fd9\u79cd\u5ef6\u8fdf\u662f\u5426\u5728\u53ef\u63a5\u53d7\u8303\u56f4\u5185\uff0c\u4ee5\u53ca\u5982\u4f55\u4f18\u5316\u4ee5\u6ee1\u8db3\u5b9e\u65f6\u6027\u8981\u6c42\uff0c\u662f\u9700\u8981\u5173\u6ce8\u7684\u95ee\u9898\u3002 \u4e94\u5c42CNN\u7684\u9a8c\u8bc1\u80fd\u529b\uff1a \u6458\u8981\u4e2d\u5bf9\u201c\u4e94\u5c42CNN\u201d\u7684\u63cf\u8ff0\u76f8\u5bf9\u7b80\u5355\u3002\u5176\u9a8c\u8bc1\u673a\u5236\u3001\u590d\u6742\u5ea6\u548c\u5bf9\u5404\u79cd\u6f02\u79fb\u6a21\u5f0f\u7684\u9c81\u68d2\u6027\u5982\u4f55\uff0c\u4ee5\u53ca\u5b83\u662f\u5426\u8db3\u4ee5\u5e94\u5bf9\u81ea\u52a8\u9a7e\u9a76\u4e2d\u53ef\u80fd\u51fa\u73b0\u7684\u6781\u7aef\u548c\u591a\u6837\u5316\u7684\u6f02\u79fb\u60c5\u51b5\uff0c\u4ecd\u6709\u5f85\u8be6\u7ec6\u8bf4\u660e\u3002 \u6f02\u79fb\u7c7b\u578b\u7684\u8986\u76d6\u8303\u56f4\uff1a \u6458\u8981\u63d0\u5230\u4e86\u201c\u6076\u52a3\u5929\u6c14\u6216\u4f4e\u5149\u7167\u201d\u4f5c\u4e3a\u6f02\u79fb\u6765\u6e90\uff0c\u4f46\u672a\u8be6\u7ec6\u8bf4\u660e\u6a21\u578b\u80fd\u591f\u5e94\u5bf9\u7684\u6f02\u79fb\u7c7b\u578b\u548c\u7a0b\u5ea6\u3002\u4f8b\u5982\uff0c\u5bf9\u4e8e\u4f20\u611f\u5668\u6545\u969c\u3001\u906e\u6321\u3001\u5bf9\u6297\u6027\u653b\u51fb\u7b49\u5176\u4ed6\u5f62\u5f0f\u7684\u201c\u6f02\u79fb\u201d\u6216\u5f02\u5e38\u60c5\u51b5\uff0c\u8be5\u67b6\u6784\u7684\u6709\u6548\u6027\u5982\u4f55\uff1f Key Findings: In this work, we present a novel hybrid computer vision architecture trained with thousands of synthetic image data from the road environment to improve robustness in unseen drifted environments. Links: PDF arXiv","title":"\u8bba\u6587\u6458\u8981\u5206\u6790\uff1aEnhanced Drift-Aware Computer Vision Architecture for Autonomous Driving"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-27/#scene-agnostic-traversability-labeling-and-estimation-via-a-multimodal-self-supervised-framework","text":"Authors: Zipeng Fang, Yanbo Wang, Lei Zhao, Weidong Chen Published: 2025-08-25 Categories: cs.RO, cs.CV Abstract: Traversability estimation is critical for enabling robots to navigate across diverse terrains and environments. While recent self-supervised learning methods achieve promising results, they often fail to capture the characteristics of non-traversable regions. Moreover, most prior works concentrate on a single modality, overlooking the complementary strengths offered by integrating heterogeneous sensory modalities for more robust traversability estimation. To address these limitations, we propose a multimodal self-supervised framework for traversability labeling and estimation. First, our annotation pipeline integrates footprint, LiDAR, and camera data as prompts for a vision foundation model, generating traversability labels that account for both semantic and geometric cues. Then, leveraging these labels, we train a dual-stream network that jointly learns from different modalities in a decoupled manner, enhancing its capacity to recognize diverse traversability patterns. In addition, we incorporate sparse LiDAR-based supervision to mitigate the noise introduced by pseudo labels. Finally, extensive experiments conducted across urban, off-road, and campus environments demonstrate the effectiveness of our approach. The proposed automatic labeling method consistently achieves around 88% IoU across diverse datasets. Compared to existing self-supervised state-of-the-art methods, our multimodal traversability estimation network yields consistently higher IoU, improving by 1.6-3.5% on all evaluated datasets. Analysis: \u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6a21\u6001\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u53ef\u901a\u884c\u6027\u533a\u57df\u7684\u6807\u6ce8\u4e0e\u4f30\u8ba1\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u8bc6\u522b\u4e0d\u53ef\u901a\u884c\u533a\u57df\u548c\u6709\u6548\u5229\u7528\u591a\u6a21\u6001\u4fe1\u606f\u65b9\u9762\u7684\u4e0d\u8db3\u3002","title":"Scene-Agnostic Traversability Labeling and Estimation via a Multimodal Self-supervised Framework"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-27/#1-concise-summary_1","text":"\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u53ef\u901a\u884c\u6027\u533a\u57df\u7684\u6807\u6ce8\u4e0e\u4f30\u8ba1\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u8bc6\u522b\u4e0d\u53ef\u901a\u884c\u533a\u57df\u548c\u5229\u7528\u591a\u6a21\u6001\u4fe1\u606f\u65b9\u9762\u7684\u4e0d\u8db3\u3002\u5b83\u901a\u8fc7\u6574\u5408\u8db3\u8ff9\u3001LiDAR\u548c\u76f8\u673a\u6570\u636e\u4f5c\u4e3a\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u63d0\u793a\u6765\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u4f2a\u6807\u7b7e\uff0c\u5e76\u5229\u7528\u8fd9\u4e9b\u6807\u7b7e\u8bad\u7ec3\u4e00\u4e2a\u53cc\u6d41\u7f51\u7edc\uff0c\u8f85\u4ee5\u7a00\u758fLiDAR\u76d1\u7763\u4ee5\u63d0\u9ad8\u9c81\u68d2\u6027\uff0c\u6700\u7ec8\u5728\u591a\u6837\u5316\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002","title":"1. \u8bba\u6587\u6838\u5fc3\u8d21\u732e\u603b\u7ed3 (Concise Summary)"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-27/#2-key-innovation-or-methodological-approach_1","text":"\u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u5176\u591a\u6a21\u6001\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u7279\u522b\u662f\u5728\u4f2a\u6807\u7b7e\u751f\u6210\u548c\u7f51\u7edc\u8bad\u7ec3\u9636\u6bb5\u3002 1. \u521b\u65b0\u6027\u4f2a\u6807\u7b7e\u751f\u6210\uff1a \u8be5\u65b9\u6cd5\u521b\u65b0\u6027\u5730\u5229\u7528\u8db3\u8ff9\uff08footprint\uff09\u3001LiDAR\u548c\u76f8\u673a\u6570\u636e\u4f5c\u4e3a\u63d0\u793a\uff08prompts\uff09\uff0c\u9a71\u52a8\u4e00\u4e2a \u89c6\u89c9\u57fa\u7840\u6a21\u578b \u6765\u751f\u6210\u7ed3\u5408\u8bed\u4e49\u548c\u51e0\u4f55\u7ebf\u7d22\u7684\u53ef\u901a\u884c\u6027\u6807\u7b7e\u3002\u8fd9\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u573a\u666f\u65e0\u5173\u7684\u4f2a\u6807\u7b7e\u751f\u6210\u7b56\u7565\uff0c\u5c24\u5176\u89e3\u51b3\u4e86\u4f20\u7edf\u81ea\u76d1\u7763\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u4e0d\u53ef\u901a\u884c\u533a\u57df\u7279\u5f81\u7684\u95ee\u9898\u3002 2. \u89e3\u8026\u5f0f\u591a\u6a21\u6001\u5b66\u4e60\u7f51\u7edc\uff1a \u63d0\u51fa\u7684\u53cc\u6d41\u7f51\u7edc\u4ee5 \u89e3\u8026\u65b9\u5f0f \u8054\u5408\u5b66\u4e60\u4e0d\u540c\u6a21\u6001\u7684\u7279\u5f81\uff0c\u589e\u5f3a\u4e86\u5bf9\u591a\u6837\u5316\u53ef\u901a\u884c\u6027\u6a21\u5f0f\u7684\u8bc6\u522b\u80fd\u529b\uff0c\u5145\u5206\u5229\u7528\u4e86\u5f02\u6784\u4f20\u611f\u5668\u7684\u4e92\u8865\u4f18\u52bf\u3002 3. \u7a00\u758fLiDAR\u76d1\u7763\u7f13\u89e3\u566a\u58f0\uff1a \u5f15\u5165 \u7a00\u758fLiDAR\u76d1\u7763 \u6765\u6709\u6548\u7f13\u89e3\u4f2a\u6807\u7b7e\u5e26\u6765\u7684\u566a\u58f0\uff0c\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\uff0c\u5f25\u8865\u4e86\u7eaf\u81ea\u76d1\u7763\u65b9\u6cd5\u53ef\u80fd\u5b58\u5728\u7684\u8bef\u5dee\u3002","title":"2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u8bba (Key Innovation or Methodological Approach)"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-27/#3-potential-impact-on-the-field_1","text":"\u8be5\u7814\u7a76\u5bf9\u673a\u5668\u4eba\u5bfc\u822a\u9886\u57df\u5177\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u5b83\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u9c81\u68d2\u3001\u66f4\u901a\u7528\u7684\u53ef\u901a\u884c\u6027\u4f30\u8ba1\u65b9\u6cd5\uff0c\u4f7f\u673a\u5668\u4eba\u5728\u672a\u77e5\u548c\u590d\u6742\u5730\u5f62\u4e2d\u81ea\u4e3b\u5bfc\u822a\u7684\u80fd\u529b\u5f97\u5230\u663e\u8457\u63d0\u5347\u3002\u5176\u81ea\u76d1\u7763\u7684\u4f2a\u6807\u7b7e\u751f\u6210\u7b56\u7565\uff0c\u7279\u522b\u662f\u7ed3\u5408\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u548c\u591a\u6a21\u6001\u63d0\u793a\uff0c\u4e3a\u51cf\u5c11\u6570\u636e\u6807\u6ce8\u6210\u672c\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u5bf9\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u4e2d\u5176\u4ed6\u9700\u8981\u5927\u91cf\u6807\u6ce8\u7684\u611f\u77e5\u4efb\u52a1\u5177\u6709\u501f\u9274\u610f\u4e49\u3002\u6b64\u5916\uff0c\u5b83\u4e5f\u4e3a\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u5728\u673a\u5668\u4eba\u611f\u77e5\u4e2d\u7684\u5e94\u7528\u6811\u7acb\u4e86\u65b0\u7684\u8303\u4f8b\u3002","title":"3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd (Potential Impact on the Field)"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-27/#4-related-areas-or-applications_1","text":"\u8fd9\u9879\u7814\u7a76\u7684\u6210\u679c\u53ef\u4ee5\u5e7f\u6cdb\u5e94\u7528\u4e8e\u4ee5\u4e0b\u9886\u57df\uff1a * \u81ea\u52a8\u9a7e\u9a76\u4e0e\u65e0\u4eba\u8f66\uff1a \u7279\u522b\u662f\u5728\u975e\u7ed3\u6784\u5316\u9053\u8def\u3001\u8d8a\u91ce\u73af\u5883\u6216\u590d\u6742\u57ce\u5e02\u533a\u57df\u7684\u8def\u5f84\u89c4\u5212\u548c\u51b3\u7b56\u3002 * \u641c\u6551\u673a\u5668\u4eba\uff1a \u5728\u707e\u540e\u5e9f\u589f\u3001\u5d0e\u5c96\u5730\u5f62\u4e2d\u8fdb\u884c\u641c\u7d22\u548c\u6551\u63f4\u4efb\u52a1\u3002 * \u7a7a\u95f4\u63a2\u7d22\u4e0e\u519b\u4e8b\u673a\u5668\u4eba\uff1a \u5728\u672a\u77e5\u6216\u6781\u7aef\u73af\u5883\u4e2d\u8fdb\u884c\u81ea\u4e3b\u63a2\u6d4b\u548c\u884c\u52a8\u3002 * \u519c\u4e1a\u673a\u5668\u4eba\uff1a \u5728\u519c\u7530\u4e2d\u8bc6\u522b\u53ef\u901a\u884c\u533a\u57df\uff0c\u8fdb\u884c\u4f5c\u7269\u76d1\u6d4b\u548c\u7ba1\u7406\u3002 * \u7269\u6d41\u4e0e\u914d\u9001\u673a\u5668\u4eba\uff1a \u63d0\u5347\u5728\u591a\u6837\u5316\u57ce\u5e02\u548c\u90ca\u533a\u73af\u5883\u4e2d\u201c\u6700\u540e\u4e00\u516c\u91cc\u201d\u914d\u9001\u7684\u9c81\u68d2\u6027\u3002","title":"4. \u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 (Related Areas or Applications)"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-27/#5-inferred-limitations","text":"\u5c3d\u7ba1\u8be5\u65b9\u6cd5\u53d6\u5f97\u4e86\u663e\u8457\u6210\u679c\uff0c\u4f46\u4ece\u6458\u8981\u4e2d\u4ecd\u53ef\u63a8\u65ad\u51fa\u4e00\u4e9b\u6f5c\u5728\u5c40\u9650\u6027\uff1a * \u4f2a\u6807\u7b7e\u8d28\u91cf\u7684\u4f9d\u8d56\u6027\uff1a \u4f2a\u6807\u7b7e\u7684\u751f\u6210\u8d28\u91cf\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u6240\u4f7f\u7528\u7684\u201c\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u201d\u7684\u6027\u80fd\u53ca\u5176\u5bf9\u591a\u6a21\u6001\u63d0\u793a\u7684\u7406\u89e3\u80fd\u529b\u3002\u5982\u679c\u57fa\u7840\u6a21\u578b\u5728\u7279\u5b9a\u573a\u666f\u6216\u7269\u4f53\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u751f\u6210\u7684\u4f2a\u6807\u7b7e\u53ef\u80fd\u4f1a\u5f15\u5165\u7cfb\u7edf\u6027\u8bef\u5dee\uff0c\u4ece\u800c\u5f71\u54cd\u540e\u7eed\u7f51\u7edc\u7684\u8bad\u7ec3\u6548\u679c\u3002 * \u201c\u89e3\u8026\u201d\u5b66\u4e60\u7684\u6743\u8861\uff1a \u6458\u8981\u63d0\u5230\u53cc\u6d41\u7f51\u7edc\u4ee5\u201c\u89e3\u8026\u65b9\u5f0f\u201d\u5b66\u4e60\u4e0d\u540c\u6a21\u6001\uff0c\u8fd9\u53ef\u80fd\u6709\u52a9\u4e8e\u63d0\u53d6\u6a21\u6001\u7279\u5b9a\u7279\u5f81\uff0c\u4f46\u4e5f\u53ef\u80fd\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u9650\u5236\u4e86\u4e0d\u540c\u6a21\u6001\u4e4b\u95f4\u66f4\u6df1\u5c42\u6b21\u3001\u66f4\u590d\u6742\u7684\u4ea4\u4e92\u548c\u4fe1\u606f\u878d\u5408\uff0c\u4ece\u800c\u5f71\u54cd\u6700\u7ec8\u7684\u51b3\u7b56\u9c81\u68d2\u6027\u3002 * \u7a00\u758fLiDAR\u76d1\u7763\u7684\u6027\u8d28\uff1a \u5c3d\u7ba1\u7a00\u758fLiDAR\u76d1\u7763\u7528\u4e8e\u7f13\u89e3\u4f2a\u6807\u7b7e\u566a\u58f0\uff0c\u4f46\u5176\u5177\u4f53\u6765\u6e90\uff08\u662f\u5b8c\u5168\u81ea\u52a8\u751f\u6210\u8fd8\u662f\u9700\u8981\u5c11\u91cf\u4eba\u5de5\u5e72\u9884\uff09\u548c\u7a00\u758f\u7a0b\u5ea6\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u4ee5\u53ca\u5176\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4ecd\u9700\u8fdb\u4e00\u6b65\u63a2\u8ba8\u3002 * \u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\uff1a \u6574\u5408\u591a\u6a21\u6001\u6570\u636e\u3001\u5229\u7528\u57fa\u7840\u6a21\u578b\u4ee5\u53ca\u8bad\u7ec3\u53cc\u6d41\u7f51\u7edc\uff0c\u53ef\u80fd\u5bf9\u8ba1\u7b97\u8d44\u6e90\uff08\u8bad\u7ec3\u548c\u63a8\u7406\uff09\u6709\u8f83\u9ad8\u8981\u6c42\uff0c\u8fd9\u5bf9\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u673a\u5668\u4eba\u5e73\u53f0\u53ef\u80fd\u662f\u4e00\u4e2a\u6311\u6218\u3002 Key Findings: To address these limitations, we propose a multimodal self-supervised framework for traversability labeling and estimation. Finally, extensive experiments conducted across urban, off-road, and campus environments demonstrate the effectiveness of our approach. Compared to existing self-supervised state-of-the-art methods, our multimodal traversability estimation network yields consistently higher IoU, improving by 1.6-3.5% on all evaluated datasets. Links: PDF arXiv","title":"5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u7684\u5c40\u9650\u6027 (Inferred Limitations)"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-27/#learning-to-detect-label-errors-by-making-them-a-method-for-segmentation-and-object-detection-datasets","text":"Authors: Sarina Penquitt, Tobias Riedlinger, Timo Heller, Markus Reischl, Matthias Rottmann Published: 2025-08-25 Categories: cs.LG, cs.CV Abstract: Recently, detection of label errors and improvement of label quality in datasets for supervised learning tasks has become an increasingly important goal in both research and industry. The consequences of incorrectly annotated data include reduced model performance, biased benchmark results, and lower overall accuracy. Current state-of-the-art label error detection methods often focus on a single computer vision task and, consequently, a specific type of dataset, containing, for example, either bounding boxes or pixel-wise annotations. Furthermore, previous methods are not learning-based. In this work, we overcome this research gap. We present a unified method for detecting label errors in object detection, semantic segmentation, and instance segmentation datasets. In a nutshell, our approach - learning to detect label errors by making them - works as follows: we inject different kinds of label errors into the ground truth. Then, the detection of label errors, across all mentioned primary tasks, is framed as an instance segmentation problem based on a composite input. In our experiments, we compare the label error detection performance of our method with various baselines and state-of-the-art approaches of each task's domain on simulated label errors across multiple tasks, datasets, and base models. This is complemented by a generalization study on real-world label errors. Additionally, we release 459 real label errors identified in the Cityscapes dataset and provide a benchmark for real label error detection in Cityscapes. Analysis: \u4f5c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u4e13\u5bb6\uff0c\u6211\u5bf9\u8fd9\u7bc7\u8bba\u6587\u6458\u8981\u8fdb\u884c\u5982\u4e0b\u5206\u6790\uff1a","title":"Learning to Detect Label Errors by Making Them: A Method for Segmentation and Object Detection Datasets"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-27/#learning-to-detect-label-errors-by-making-them-a-method-for-segmentation-and-object-detection-datasets_1","text":"1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u6d01\u603b\u7ed3 (2-3 \u53e5\u8bdd) \u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u3001\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4b\u76ee\u6807\u68c0\u6d4b\u3001\u8bed\u4e49\u5206\u5272\u548c\u5b9e\u4f8b\u5206\u5272\u6570\u636e\u96c6\u4e2d\u5b58\u5728\u7684\u6807\u7b7e\u9519\u8bef\u3002\u5176\u6838\u5fc3\u601d\u60f3\u662f\u901a\u8fc7\u5411\u771f\u503c\u4e2d\u6ce8\u5165\u4e0d\u540c\u7c7b\u578b\u7684\u5408\u6210\u9519\u8bef\u6765\u8bad\u7ec3\u4e00\u4e2a\u9519\u8bef\u68c0\u6d4b\u6a21\u578b\uff0c\u5e76\u5c06\u9519\u8bef\u68c0\u6d4b\u4efb\u52a1\u5efa\u6a21\u4e3a\u4e00\u4e2a\u57fa\u4e8e\u590d\u5408\u8f93\u5165\u7684\u5b9e\u4f8b\u5206\u5272\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u65e8\u5728\u514b\u670d\u73b0\u6709\u65b9\u6cd5\u4ec5\u9650\u4e8e\u5355\u4e00\u4efb\u52a1\u4e14\u975e\u5b66\u4e60\u7684\u5c40\u9650\u6027\uff0c\u5e76\u901a\u8fc7\u771f\u5b9e\u4e16\u754c\u9519\u8bef\u548c\u57fa\u51c6\u6d4b\u8bd5\u6765\u9a8c\u8bc1\u5176\u6cdb\u5316\u80fd\u529b\u3002 2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u8bba \u7edf\u4e00\u7684\u3001\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\uff1a \u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4e13\u6ce8\u4e8e\u5355\u4e00\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\uff08\u5982\u4ec5\u8fb9\u754c\u6846\u6216\u50cf\u7d20\u7ea7\u6807\u6ce8\uff09\u4e14\u975e\u5b66\u4e60\u7684\u5c40\u9650\u6027\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u540c\u65f6\u5904\u7406\u76ee\u6807\u68c0\u6d4b\u3001\u8bed\u4e49\u5206\u5272\u548c\u5b9e\u4f8b\u5206\u5272\u7684\u901a\u7528\u6846\u67b6\u3002 \u201c\u901a\u8fc7\u5236\u9020\u9519\u8bef\u6765\u5b66\u4e60\u68c0\u6d4b\u9519\u8bef\u201d (Learning by Making Them)\uff1a \u8fd9\u662f\u4e00\u4e2a\u5de7\u5999\u7684\u81ea\u76d1\u7763\u6216\u6570\u636e\u589e\u5f3a\u7b56\u7565\u3002\u901a\u8fc7\u7cfb\u7edf\u5730\u5411\u771f\u503c\u4e2d\u6ce8\u5165\u4e0d\u540c\u7c7b\u578b\u7684\u6807\u7b7e\u9519\u8bef\uff0c\u4e3a\u9519\u8bef\u68c0\u6d4b\u6a21\u578b\u751f\u6210\u8bad\u7ec3\u6570\u636e\uff0c\u4ece\u800c\u907f\u514d\u4e86\u5bf9\u5927\u91cf\u771f\u5b9e\u9519\u8bef\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\u3002 \u5c06\u9519\u8bef\u68c0\u6d4b\u6846\u67b6\u4e3a\u5b9e\u4f8b\u5206\u5272\u95ee\u9898\uff1a \u8fd9\u79cd\u65b9\u6cd5\u80fd\u591f\u7cbe\u786e\u5730\u5b9a\u4f4d\u548c\u8bc6\u522b\u56fe\u50cf\u4e2d\u4e0d\u540c\u7c7b\u578b\u7684\u6807\u7b7e\u9519\u8bef\u533a\u57df\uff0c\u5c06\u5176\u89c6\u4e3a\u72ec\u7acb\u7684\u201c\u9519\u8bef\u5b9e\u4f8b\u201d\uff0c\u8fd9\u6bd4\u7b80\u5355\u7684\u5206\u7c7b\u6216\u68c0\u6d4b\u6846\u65b9\u6cd5\u66f4\u5177\u8868\u73b0\u529b\u3002 \u590d\u5408\u8f93\u5165 (Composite Input)\uff1a \u867d\u7136\u6458\u8981\u672a\u8be6\u7ec6\u8bf4\u660e\uff0c\u4f46\u6697\u793a\u4e86\u901a\u8fc7\u7ed3\u5408\u539f\u59cb\u56fe\u50cf\u4fe1\u606f\u548c\u53ef\u80fd\u5305\u542b\u9519\u8bef\u4fe1\u606f\u7684\u8868\u793a\u6765\u8bad\u7ec3\u6a21\u578b\uff0c\u4ee5\u63d0\u9ad8\u9519\u8bef\u68c0\u6d4b\u7684\u9c81\u68d2\u6027\u3002 3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd \u63d0\u5347\u6570\u636e\u96c6\u8d28\u91cf\u548c\u6a21\u578b\u6027\u80fd\uff1a \u81ea\u52a8\u5316\u3001\u7edf\u4e00\u7684\u6807\u7b7e\u9519\u8bef\u68c0\u6d4b\u5c06\u663e\u8457\u63d0\u9ad8\u8ba1\u7b97\u673a\u89c6\u89c9\u6570\u636e\u96c6\u7684\u8d28\u91cf\uff0c\u4ece\u800c\u8bad\u7ec3\u51fa\u66f4\u9c81\u68d2\u3001\u6027\u80fd\u66f4\u597d\u7684\u6a21\u578b\uff0c\u5e76\u51cf\u5c11\u56e0\u6570\u636e\u9519\u8bef\u5bfc\u81f4\u7684\u6a21\u578b\u504f\u5dee\u3002 \u66f4\u53ef\u9760\u7684\u57fa\u51c6\u6d4b\u8bd5\uff1a \u51cf\u5c11\u6570\u636e\u96c6\u4e2d\u7684\u9519\u8bef\u5c06\u4f7f\u57fa\u51c6\u6d4b\u8bd5\u7ed3\u679c\u66f4\u52a0\u516c\u5e73\u548c\u53ef\u4fe1\uff0c\u4fc3\u8fdb\u66f4\u6709\u6548\u7684\u6a21\u578b\u6bd4\u8f83\u548c\u7814\u7a76\u8fdb\u5c55\u3002 \u964d\u4f4e\u6570\u636e\u6807\u6ce8\u6210\u672c\u548c\u65f6\u95f4\uff1a \u81ea\u52a8\u5316\u9519\u8bef\u68c0\u6d4b\u53ef\u4ee5\u5927\u5e45\u51cf\u5c11\u4eba\u5de5\u8d28\u91cf\u63a7\u5236\u7684\u9700\u6c42\uff0c\u964d\u4f4e\u6570\u636e\u96c6\u521b\u5efa\u548c\u7ef4\u62a4\u7684\u6210\u672c\u4e0e\u65f6\u95f4\u3002 \u63a8\u52a8\u6570\u636e\u4e2d\u5fc3AI\u53d1\u5c55\uff1a \u8be5\u7814\u7a76\u4e0e\u5f53\u524d\u201c\u6570\u636e\u4e2d\u5fc3AI\u201d\u7684\u8d8b\u52bf\u9ad8\u5ea6\u5951\u5408\uff0c\u5f3a\u8c03\u4e86\u6570\u636e\u8d28\u91cf\u5728\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u6838\u5fc3\u4f5c\u7528\u3002 \u793e\u533a\u8d44\u6e90\u8d21\u732e\uff1a \u8bba\u6587\u53d1\u5e03\u4e86 Cityscapes \u6570\u636e\u96c6\u4e2d\u8bc6\u522b\u51fa\u7684 459 \u4e2a\u771f\u5b9e\u6807\u7b7e\u9519\u8bef\uff0c\u5e76\u63d0\u4f9b\u4e86\u771f\u5b9e\u6807\u7b7e\u9519\u8bef\u68c0\u6d4b\u7684\u57fa\u51c6\uff0c\u8fd9\u5c06\u4e3a\u672a\u6765\u7684\u7814\u7a76\u63d0\u4f9b\u5b9d\u8d35\u7684\u8d44\u6e90\u548c\u8bc4\u4f30\u6807\u51c6\u3002 4. \u53ef\u80fd\u53d7\u76ca\u7684\u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 \u81ea\u52a8\u9a7e\u9a76\uff1a \u81ea\u52a8\u9a7e\u9a76\u6570\u636e\u96c6\u5e9e\u5927\u4e14\u5bf9\u7cbe\u5ea6\u8981\u6c42\u6781\u9ad8\uff0c\u6807\u7b7e\u9519\u8bef\u53ef\u80fd\u5bfc\u81f4\u4e25\u91cd\u540e\u679c\u3002\u8be5\u65b9\u6cd5\u53ef\u7528\u4e8e\u63d0\u9ad8\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u6570\u636e\u96c6\u7684\u8d28\u91cf\u3002 \u533b\u5b66\u5f71\u50cf\u5206\u6790\uff1a \u533b\u5b66\u56fe\u50cf\u6807\u6ce8\u7684\u51c6\u786e\u6027\u76f4\u63a5\u5173\u7cfb\u5230\u8bca\u65ad\u548c\u6cbb\u7597\uff0c\u9519\u8bef\u68c0\u6d4b\u5bf9\u4e8e\u786e\u4fdd\u6a21\u578b\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\u3002 \u5927\u89c4\u6a21\u6570\u636e\u96c6\u63d0\u4f9b\u5546/\u6807\u6ce8\u670d\u52a1\uff1a \u4e13\u95e8\u4ece\u4e8b\u6570\u636e\u96c6\u521b\u5efa\u548c\u8d28\u91cf\u63a7\u5236\u7684\u516c\u53f8\u5c06\u76f4\u63a5\u53d7\u76ca\u4e8e\u8fd9\u79cd\u81ea\u52a8\u5316\u5de5\u5177\u3002 \u673a\u5668\u4eba\u611f\u77e5\uff1a \u673a\u5668\u4eba\u9700\u8981\u51c6\u786e\u7684\u73af\u5883\u611f\u77e5\u6765\u6267\u884c\u4efb\u52a1\uff0c\u9ad8\u8d28\u91cf\u7684\u8bad\u7ec3\u6570\u636e\u662f\u57fa\u7840\u3002 \u4efb\u4f55\u4f9d\u8d56\u76d1\u7763\u5b66\u4e60\u7684\u5de5\u4e1a\u5e94\u7528\uff1a \u51e1\u662f\u4f7f\u7528\u5927\u91cf\u6807\u6ce8\u6570\u636e\u8fdb\u884c\u6a21\u578b\u8bad\u7ec3\u7684\u884c\u4e1a\uff08\u5982\u96f6\u552e\u3001\u5b89\u9632\u3001\u519c\u4e1a\u7b49\uff09\u90fd\u5c06\u4ece\u66f4\u53ef\u9760\u7684\u6570\u636e\u8d28\u91cf\u4e2d\u83b7\u76ca\u3002 \u4e3b\u52a8\u5b66\u4e60 (Active Learning) \u548c\u6570\u636e\u7b56\u5c55 (Data Curation)\uff1a \u8bc6\u522b\u51fa\u9519\u8bef\u6837\u672c\u53ef\u4ee5\u6307\u5bfc\u91cd\u65b0\u6807\u6ce8\u6216\u4f18\u5148\u5904\u7406\uff0c\u4f18\u5316\u6570\u636e\u5229\u7528\u6548\u7387\u3002 5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u7684\u5c40\u9650\u6027 \u5408\u6210\u9519\u8bef\u4e0e\u771f\u5b9e\u9519\u8bef\u7684\u5dee\u8ddd\uff1a \u5c3d\u7ba1\u8bba\u6587\u63d0\u5230\u4e86\u5bf9\u771f\u5b9e\u4e16\u754c\u9519\u8bef\u7684\u6cdb\u5316\u7814\u7a76\uff0c\u4f46\u5408\u6210\u9519\u8bef\u662f\u5426\u80fd\u5b8c\u5168\u8986\u76d6\u6240\u6709\u7c7b\u578b\u3001\u6240\u6709\u7ec6\u5fae\u4e4b\u5904\u7684\u771f\u5b9e\u4e16\u754c\u9519\u8bef\uff0c\u4ee5\u53ca\u6a21\u578b\u5bf9\u672a\u89c1\u8fc7\u7684\u771f\u5b9e\u9519\u8bef\u7c7b\u578b\u7684\u68c0\u6d4b\u80fd\u529b\uff0c\u4ecd\u662f\u4e00\u4e2a\u6f5c\u5728\u7684\u6311\u6218\u3002 \u201c\u590d\u5408\u8f93\u5165\u201d\u7684\u7ec6\u8282\uff1a \u6458\u8981\u4e2d\u672a\u8be6\u7ec6\u8bf4\u660e\u201c\u590d\u5408\u8f93\u5165\u201d\u7684\u5177\u4f53\u6784\u6210\uff0c\u5176\u8bbe\u8ba1\u5bf9\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u6cdb\u5316\u80fd\u529b\u53ef\u80fd\u81f3\u5173\u91cd\u8981\u3002 \u8ba1\u7b97\u6210\u672c\uff1a \u8bad\u7ec3\u4e00\u4e2a\u57fa\u4e8e\u5b66\u4e60\u7684\u9519\u8bef\u68c0\u6d4b\u6a21\u578b\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u5927\u89c4\u6a21\u6570\u636e\u96c6\u548c\u590d\u6742\u7684\u5b9e\u4f8b\u5206\u5272\u4efb\u52a1\uff0c\u53ef\u80fd\u9700\u8981\u663e\u8457\u7684\u8ba1\u7b97\u8d44\u6e90\u548c\u65f6\u95f4\uff0c\u6458\u8981\u4e2d\u672a\u63d0\u53ca\u8fd9\u65b9\u9762\u7684\u8003\u91cf\u3002 \u9519\u8bef\u7c7b\u578b\u7684\u7c92\u5ea6\uff1a \u6458\u8981\u4e2d\u63d0\u5230\u201c\u4e0d\u540c\u79cd\u7c7b\u7684\u6807\u7b7e\u9519\u8bef\u201d\uff0c\u4f46\u672a\u5177\u4f53\u8bf4\u660e\u80fd\u68c0\u6d4b\u5230\u54ea\u4e9b\u7c92\u5ea6\u7684\u9519\u8bef\uff08\u4f8b\u5982\uff0c\u662f\u660e\u663e\u7684\u51e0\u4f55\u9519\u8bef\uff0c\u8fd8\u662f\u66f4\u7ec6\u5fae\u7684\u8bed\u4e49\u4e0d\u4e00\u81f4\u6027\uff09\u3002 \u9519\u8bef\u4fee\u6b63\u673a\u5236\uff1a \u8bba\u6587\u4e13\u6ce8\u4e8e\u9519\u8bef\u68c0\u6d4b\uff0c\u4f46\u672a\u63d0\u53ca\u5982\u4f55\u5c06\u68c0\u6d4b\u5230\u7684\u9519\u8bef\u6709\u6548\u5730\u53cd\u9988\u7ed9\u6807\u6ce8\u5458\u8fdb\u884c\u4fee\u6b63\uff0c\u6216\u662f\u5426\u63d0\u4f9b\u81ea\u52a8\u4fee\u6b63\u7684\u5efa\u8bae\u3002 Key Findings: Current state-of-the-art label error detection methods often focus on a single computer vision task and, consequently, a specific type of dataset, containing, for example, either bounding boxes or pixel-wise annotations. We present a unified method for detecting label errors in object detection, semantic segmentation, and instance segmentation datasets. In a nutshell, our approach - learning to detect label errors by making them - works as follows: we inject different kinds of label errors into the ground truth. In our experiments, we compare the label error detection performance of our method with various baselines and state-of-the-art approaches of each task's domain on simulated label errors across multiple tasks, datasets, and base models. Links: PDF arXiv","title":"\u8bba\u6587\u6458\u8981\u5206\u6790\uff1aLearning to Detect Label Errors by Making Them: A Method for Segmentation and Object Detection Datasets"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-27/#catformer-contrastive-adversarial-transformer-for-image-super-resolution","text":"Authors: Qinyi Tian, Spence Cox, Laura E. Dalton Published: 2025-08-25 Categories: cs.CV Abstract: Super-resolution remains a promising technique to enhance the quality of low-resolution images. This study introduces CATformer (Contrastive Adversarial Transformer), a novel neural network integrating diffusion-inspired feature refinement with adversarial and contrastive learning. CATformer employs a dual-branch architecture combining a primary diffusion-inspired transformer, which progressively refines latent representations, with an auxiliary transformer branch designed to enhance robustness to noise through learned latent contrasts. These complementary representations are fused and decoded using deep Residual-in-Residual Dense Blocks for enhanced reconstruction quality. Extensive experiments on benchmark datasets demonstrate that CATformer outperforms recent transformer-based and diffusion-inspired methods both in efficiency and visual image quality. This work bridges the performance gap among transformer-, diffusion-, and GAN-based methods, laying a foundation for practical applications of diffusion-inspired transformers in super-resolution. Analysis: \u4f5c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u4e13\u5bb6\uff0c\u6211\u5bf9\u8fd9\u7bc7\u5173\u4e8eCATformer\u7684\u8bba\u6587\u6458\u8981\u8fdb\u884c\u5982\u4e0b\u5206\u6790\uff1a","title":"CATformer: Contrastive Adversarial Transformer for Image Super-Resolution"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-27/#catformer-contrastive-adversarial-transformer-for-image-super-resolution_1","text":"1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u6d01\u603b\u7ed3 (2-3 \u53e5\u8bdd) CATformer\u662f\u4e00\u79cd\u65b0\u9896\u7684\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u795e\u7ecf\u7f51\u7edc\uff0c\u5b83\u5de7\u5999\u5730\u5c06\u53d7\u6269\u6563\u6a21\u578b\u542f\u53d1\u7684\u7279\u5f81\u7ec6\u5316\u3001\u5bf9\u6297\u6027\u5b66\u4e60\u548c\u5bf9\u6bd4\u5b66\u4e60\u96c6\u6210\u5230\u4e00\u4e2a\u53cc\u5206\u652fTransformer\u67b6\u6784\u4e2d\u3002\u8be5\u6a21\u578b\u901a\u8fc7\u6e10\u8fdb\u5f0f\u6f5c\u5728\u8868\u793a\u7ec6\u5316\u548c\u589e\u5f3a\u5bf9\u566a\u58f0\u7684\u9c81\u68d2\u6027\uff0c\u5728\u6548\u7387\u548c\u89c6\u89c9\u8d28\u91cf\u4e0a\u5747\u8d85\u8d8a\u4e86\u73b0\u6709\u7684Transformer\u548c\u6269\u6563\u542f\u53d1\u65b9\u6cd5\u3002\u8fd9\u9879\u5de5\u4f5c\u4e3a\u8d85\u5206\u8fa8\u7387\u9886\u57df\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u6df7\u5408\u8303\u5f0f\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u4e3a\u6269\u6563\u542f\u53d1\u5f0fTransformer\u7684\u5b9e\u9645\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840\u3002 2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u8bba\u65b9\u6cd5 CATformer\u7684\u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u5176\u72ec\u7279\u7684 \u53cc\u5206\u652fTransformer\u67b6\u6784 \u4ee5\u53ca\u5bf9 \u591a\u8303\u5f0f\u5b66\u4e60\u7684\u6df1\u5ea6\u878d\u5408 \u3002\u5177\u4f53\u800c\u8a00\uff1a * \u53cc\u5206\u652f\u67b6\u6784\uff1a \u5305\u542b\u4e00\u4e2a \u4e3b\u6269\u6563\u542f\u53d1\u5f0fTransformer\u5206\u652f \uff0c\u8d1f\u8d23\u5bf9\u6f5c\u5728\u8868\u793a\u8fdb\u884c\u6e10\u8fdb\u5f0f\u7ec6\u5316\uff0c\u501f\u9274\u4e86\u6269\u6563\u6a21\u578b\u9010\u6b65\u53bb\u566a\u548c\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\u7684\u6838\u5fc3\u601d\u60f3\u3002 * \u8f85\u52a9\u5bf9\u6bd4\u5b66\u4e60\u5206\u652f\uff1a \u53e6\u4e00\u4e2a \u8f85\u52a9Transformer\u5206\u652f \u901a\u8fc7\u5b66\u4e60\u5230\u7684\u6f5c\u5728\u5bf9\u6bd4\u6765\u589e\u5f3a\u6a21\u578b\u5bf9\u566a\u58f0\u7684\u9c81\u68d2\u6027\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u5229\u7528\u5bf9\u6bd4\u5b66\u4e60\u6765\u89e3\u51b3\u8d85\u5206\u8fa8\u7387\u4e2d\u5e38\u89c1\u566a\u58f0\u95ee\u9898\u7684\u7b56\u7565\u3002 * \u591a\u8303\u5f0f\u878d\u5408\uff1a \u5c06 \u6269\u6563\u542f\u53d1 \uff08\u7528\u4e8e\u7279\u5f81\u7ec6\u5316\uff09\u3001 \u5bf9\u6297\u6027\u5b66\u4e60 \uff08\u7528\u4e8e\u751f\u6210\u771f\u5b9e\u611f\u56fe\u50cf\uff09\u548c \u5bf9\u6bd4\u5b66\u4e60 \uff08\u7528\u4e8e\u566a\u58f0\u9c81\u68d2\u6027\uff09\u8fd9\u4e09\u79cd\u5f3a\u5927\u7684\u5b66\u4e60\u8303\u5f0f\u5de7\u5999\u5730\u96c6\u6210\u5728\u4e00\u4e2aTransformer\u6846\u67b6\u5185\uff0c\u5e76\u901a\u8fc7Residual-in-Residual Dense Blocks\u8fdb\u884c\u878d\u5408\u548c\u89e3\u7801\uff0c\u4ee5\u5b9e\u73b0\u5353\u8d8a\u7684\u91cd\u5efa\u8d28\u91cf\u3002 3. \u5bf9\u8be5\u9886\u57df\u7684\u6f5c\u5728\u5f71\u54cd \u6027\u80fd\u65b0\u6807\u6746\uff1a CATformer\u58f0\u79f0\u5728\u6548\u7387\u548c\u89c6\u89c9\u8d28\u91cf\u4e0a\u5747\u8d85\u8d8a\u4e86\u73b0\u6709\u7684Transformer\u548c\u6269\u6563\u542f\u53d1\u65b9\u6cd5\uff0c\u8fd9\u53ef\u80fd\u4e3a\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u9886\u57df\u6811\u7acb\u65b0\u7684\u6027\u80fd\u57fa\u51c6\u3002 \u5f25\u5408\u65b9\u6cd5\u8bba\u9e3f\u6c9f\uff1a \u8be5\u7814\u7a76\u660e\u786e\u6307\u51fa\u5176\u5de5\u4f5c\u201c\u5f25\u5408\u4e86Transformer\u3001\u6269\u6563\u6a21\u578b\u548cGAN\u8fd9\u4e09\u79cd\u65b9\u6cd5\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\u201d\uff0c\u8fd9\u8868\u660e\u5b83\u6210\u529f\u5730\u7ed3\u5408\u4e86\u5404\u5bb6\u4e4b\u957f\uff0c\u514b\u670d\u4e86\u5355\u4e00\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u53ef\u80fd\u5f15\u9886\u672a\u6765\u8d85\u5206\u8fa8\u7387\u4e43\u81f3\u66f4\u5e7f\u6cdb\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u7684\u6df7\u5408\u67b6\u6784\u8bbe\u8ba1\u8d8b\u52bf\u3002 \u63a8\u52a8\u6269\u6563\u542f\u53d1\u6a21\u578b\u7684\u5b9e\u7528\u5316\uff1a \u6269\u6563\u6a21\u578b\u867d\u7136\u751f\u6210\u8d28\u91cf\u9ad8\uff0c\u4f46\u901a\u5e38\u63a8\u7406\u901f\u5ea6\u8f83\u6162\u3002CATformer\u901a\u8fc7\u201c\u6269\u6563\u542f\u53d1\u201d\u800c\u975e\u5b8c\u6574\u6269\u6563\u6a21\u578b\u7684\u65b9\u5f0f\uff0c\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u7684\u540c\u65f6\u63d0\u5347\u4e86\u6548\u7387\uff0c\u4e3a\u5c06\u6269\u6563\u6a21\u578b\u7684\u5f3a\u5927\u80fd\u529b\u5f15\u5165\u5b9e\u9645\u5e94\u7528\u573a\u666f\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002 \u589e\u5f3a\u6a21\u578b\u9c81\u68d2\u6027\uff1a \u5f15\u5165\u5bf9\u6bd4\u5b66\u4e60\u4ee5\u589e\u5f3a\u5bf9\u566a\u58f0\u7684\u9c81\u68d2\u6027\uff0c\u5bf9\u4e8e\u771f\u5b9e\u4e16\u754c\u4e2d\u4f4e\u8d28\u91cf\u3001\u542b\u566a\u56fe\u50cf\u7684\u8d85\u5206\u8fa8\u7387\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u7684\u5b9e\u7528\u4ef7\u503c\u3002 4. \u53ef\u80fd\u53d7\u76ca\u4e8e\u8fd9\u9879\u7814\u7a76\u7684\u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 \u901a\u7528\u56fe\u50cf\u6062\u590d\u4e0e\u589e\u5f3a\uff1a \u9664\u4e86\u8d85\u5206\u8fa8\u7387\uff0c\u5176\u5bf9\u566a\u58f0\u7684\u9c81\u68d2\u6027\u548c\u9ad8\u8d28\u91cf\u91cd\u5efa\u80fd\u529b\u53ef\u80fd\u76f4\u63a5\u5e94\u7528\u4e8e\u56fe\u50cf\u53bb\u566a\u3001\u53bb\u6a21\u7cca\u3001\u56fe\u50cf\u4fee\u590d\u7b49\u4efb\u52a1\u3002 \u533b\u5b66\u5f71\u50cf\uff1a \u63d0\u9ad8\u4f4e\u5206\u8fa8\u7387\u533b\u5b66\u626b\u63cf\u56fe\u50cf\uff08\u5982MRI\u3001CT\uff09\u7684\u7ec6\u8282\uff0c\u8f85\u52a9\u533b\u751f\u8fdb\u884c\u66f4\u7cbe\u786e\u7684\u8bca\u65ad\u3002 \u9065\u611f\u4e0e\u5b89\u9632\u76d1\u63a7\uff1a \u63d0\u5347\u536b\u661f\u56fe\u50cf\u3001\u65e0\u4eba\u673a\u822a\u62cd\u56fe\u6216\u76d1\u63a7\u5f55\u50cf\u7684\u6e05\u6670\u5ea6\uff0c\u4fbf\u4e8e\u76ee\u6807\u8bc6\u522b\u3001\u6001\u52bf\u611f\u77e5\u548c\u4e8b\u4ef6\u5206\u6790\u3002 \u8ba1\u7b97\u6444\u5f71\u4e0e\u89c6\u9891\u5904\u7406\uff1a \u6539\u5584\u6d88\u8d39\u7ea7\u8bbe\u5907\u62cd\u6444\u7684\u4f4e\u8d28\u91cf\u7167\u7247\u548c\u89c6\u9891\uff0c\u5b9e\u73b0\u66f4\u6e05\u6670\u7684\u653e\u5927\u548c\u7ec6\u8282\u6062\u590d\u3002 \u6570\u5b57\u5185\u5bb9\u521b\u4f5c\u4e0e\u5a31\u4e50\uff1a \u63d0\u5347\u8001\u65e7\u7167\u7247\u3001\u89c6\u9891\u7684\u753b\u8d28\uff0c\u6216\u5728\u6e38\u620f\u3001VR/AR\u7b49\u573a\u666f\u4e2d\u5b9e\u73b0\u5b9e\u65f6\u9ad8\u8d28\u91cf\u6e32\u67d3\u3002 \u5176\u4ed6\u6761\u4ef6\u56fe\u50cf\u751f\u6210\u4efb\u52a1\uff1a \u5176\u878d\u5408\u591a\u79cd\u5b66\u4e60\u8303\u5f0f\u548c\u67b6\u6784\u7684\u601d\u8def\uff0c\u53ef\u80fd\u4e3a\u5176\u4ed6\u6761\u4ef6\u56fe\u50cf\u751f\u6210\u4efb\u52a1\uff08\u5982\u6587\u672c\u5230\u56fe\u50cf\u3001\u56fe\u50cf\u5230\u56fe\u50cf\u8f6c\u6362\uff09\u63d0\u4f9b\u65b0\u7684\u8bbe\u8ba1\u7075\u611f\u3002 5. \u4ece\u6458\u8981\u4e2d\u53ef\u4ee5\u63a8\u65ad\u51fa\u7684\u4efb\u4f55\u5c40\u9650\u6027 \u67b6\u6784\u590d\u6742\u6027\u4e0e\u8bad\u7ec3\u6210\u672c\uff1a \u53cc\u5206\u652fTransformer\u67b6\u6784\u7ed3\u5408\u4e09\u79cd\u5b66\u4e60\u8303\u5f0f\uff08\u6269\u6563\u542f\u53d1\u3001\u5bf9\u6297\u3001\u5bf9\u6bd4\uff09\u4ee5\u53caRIRDB\uff0c\u610f\u5473\u7740\u6a21\u578b\u53ef\u80fd\u975e\u5e38\u590d\u6742\uff0c\u8bad\u7ec3\u96be\u5ea6\u5927\uff0c\u5bf9\u8ba1\u7b97\u8d44\u6e90\u7684\u9700\u6c42\u9ad8\uff0c\u4e14\u8d85\u53c2\u6570\u8c03\u4f18\u53ef\u80fd\u5177\u6709\u6311\u6218\u6027\u3002 \u201c\u6269\u6563\u542f\u53d1\u201d\u7684\u6df1\u5ea6\uff1a \u8bba\u6587\u5f3a\u8c03\u201c\u6269\u6563\u542f\u53d1\u201d\u7684\u7279\u5f81\u7ec6\u5316\uff0c\u800c\u975e\u5b8c\u6574\u7684\u6269\u6563\u6a21\u578b\u3002\u8fd9\u53ef\u80fd\u610f\u5473\u7740\u5b83\u5728\u751f\u6210\u591a\u6837\u6027\u6216\u5904\u7406\u6781\u7aef\u4f4e\u8d28\u91cf\u8f93\u5165\u65b9\u9762\u7684\u80fd\u529b\uff0c\u53ef\u80fd\u4e0d\u5982\u5b8c\u6574\u7684\u6269\u6563\u751f\u6210\u6a21\u578b\u3002\u5176\u201c\u6e10\u8fdb\u5f0f\u7ec6\u5316\u201d\u7684\u5177\u4f53\u673a\u5236\u548c\u6548\u679c\u4e0e\u5b8c\u6574\u6269\u6563\u6a21\u578b\u7684\u5dee\u5f02\uff0c\u6709\u5f85\u6b63\u6587\u63ed\u793a\u3002 \u566a\u58f0\u9c81\u68d2\u6027\u7684\u5177\u4f53\u8303\u56f4\uff1a \u6458\u8981\u63d0\u5230\u901a\u8fc7\u5b66\u4e60\u5230\u7684\u6f5c\u5728\u5bf9\u6bd4\u589e\u5f3a\u5bf9\u566a\u58f0\u7684\u9c81\u68d2\u6027\uff0c\u4f46\u672a\u5177\u4f53\u8bf4\u660e\u80fd\u5904\u7406\u7684\u566a\u58f0\u7c7b\u578b\uff08\u5982\u9ad8\u65af\u566a\u58f0\u3001\u771f\u5b9e\u4e16\u754c\u566a\u58f0\u3001\u4f20\u611f\u5668\u566a\u58f0\uff09\u548c\u5f3a\u5ea6\u8303\u56f4\u3002\u5728\u9762\u5bf9\u9ad8\u5ea6\u590d\u6742\u6216\u975e\u5178\u578b\u566a\u58f0\u65f6\uff0c\u5176\u8868\u73b0\u5982\u4f55\u4ecd\u662f\u672a\u77e5\u3002 \u201c\u6548\u7387\u201d\u7684\u91cf\u5316\uff1a \u5c3d\u7ba1\u58f0\u79f0\u5728\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4f46\u5177\u4f53\u7684\u91cf\u5316\u6570\u636e\uff08\u5982\u63a8\u7406\u65f6\u95f4\u3001\u53c2\u6570\u91cf\u3001FLOPs\uff09\u5728\u6458\u8981\u4e2d\u7f3a\u5931\uff0c\u65e0\u6cd5\u5224\u65ad\u5176\u201c\u6548\u7387\u201d\u7684\u7edd\u5bf9\u6c34\u5e73\uff0c\u5c24\u5176\u662f\u5728\u4e0e\u8f7b\u91cf\u7ea7\u6a21\u578b\u5bf9\u6bd4\u65f6\u3002 \u6cdb\u5316\u80fd\u529b\uff1a \u5c3d\u7ba1\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u9762\u5bf9\u672a\u89c1\u8fc7\u7684\u9ad8\u5ea6\u590d\u6742\u6216\u7279\u5b9a\u9886\u57df\u7684\u771f\u5b9e\u4e16\u754c\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\u65f6\uff0c\u5176\u6cdb\u5316\u80fd\u529b\u4ecd\u9700\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u3002\u4f8b\u5982\uff0c\u5bf9\u4e8e\u7279\u5b9a\u7eb9\u7406\u3001\u5149\u7167\u6761\u4ef6\u6216\u5185\u5bb9\uff08\u5982\u4eba\u8138\u3001\u6587\u672c\uff09\u7684\u8d85\u5206\u8fa8\u7387\u6548\u679c\u3002 Key Findings: This study introduces CATformer (Contrastive Adversarial Transformer), a novel neural network integrating diffusion-inspired feature refinement with adversarial and contrastive learning. Extensive experiments on benchmark datasets demonstrate that CATformer outperforms recent transformer-based and diffusion-inspired methods both in efficiency and visual image quality. Links: PDF arXiv","title":"CATformer: Contrastive Adversarial Transformer for Image Super-Resolution"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-27/#roofseg-an-edge-aware-transformer-based-network-for-end-to-end-roof-plane-segmentation","text":"Authors: Siyuan You, Guozheng Xu, Pengwei Zhou, Qiwen Jin, Jian Yao, Li Li Published: 2025-08-26 Categories: cs.CV, cs.AI Abstract: Roof plane segmentation is one of the key procedures for reconstructing three-dimensional (3D) building models at levels of detail (LoD) 2 and 3 from airborne light detection and ranging (LiDAR) point clouds. The majority of current approaches for roof plane segmentation rely on the manually designed or learned features followed by some specifically designed geometric clustering strategies. Because the learned features are more powerful than the manually designed features, the deep learning-based approaches usually perform better than the traditional approaches. However, the current deep learning-based approaches have three unsolved problems. The first is that most of them are not truly end-to-end, the plane segmentation results may be not optimal. The second is that the point feature discriminability near the edges is relatively low, leading to inaccurate planar edges. The third is that the planar geometric characteristics are not sufficiently considered to constrain the network training. To solve these issues, a novel edge-aware transformer-based network, named RoofSeg, is developed for segmenting roof planes from LiDAR point clouds in a truly end-to-end manner. In the RoofSeg, we leverage a transformer encoder-decoder-based framework to hierarchically predict the plane instance masks with the use of a set of learnable plane queries. To further improve the segmentation accuracy of edge regions, we also design an Edge-Aware Mask Module (EAMM) that sufficiently incorporates planar geometric prior of edges to enhance its discriminability for plane instance mask refinement. In addition, we propose an adaptive weighting strategy in the mask loss to reduce the influence of misclassified points, and also propose a new plane geometric loss to constrain the network training. Analysis: \u4f5c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u4e13\u5bb6\uff0c\u6211\u5bf9\u8fd9\u7bc7\u5173\u4e8eRoofSeg\u7684\u8bba\u6587\u6458\u8981\u8fdb\u884c\u4e86\u6df1\u5165\u5206\u6790\uff1a","title":"RoofSeg: An edge-aware transformer-based network for end-to-end roof plane segmentation"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-27/#1-concise-summary_2","text":"\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRoofSeg\u7684\u8fb9\u7f18\u611f\u77e5Transformer\u7f51\u7edc\uff0c\u65e8\u5728\u5b9e\u73b0\u4eceLiDAR\u70b9\u4e91\u4e2d\u7aef\u5230\u7aef\u7684\u5c4b\u9876\u5e73\u9762\u5206\u5272\u3002\u5b83\u901a\u8fc7\u7ed3\u5408Transformer\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\u3001\u4e13\u95e8\u8bbe\u8ba1\u7684\u8fb9\u7f18\u611f\u77e5\u63a9\u7801\u6a21\u5757\uff08EAMM\uff09\u4ee5\u53ca\u65b0\u7684\u51e0\u4f55\u635f\u5931\u51fd\u6570\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u975e\u7aef\u5230\u7aef\u3001\u8fb9\u7f18\u5206\u5272\u7cbe\u5ea6\u4f4e\u548c\u7f3a\u4e4f\u51e0\u4f55\u7ea6\u675f\u7b49\u65b9\u9762\u7684\u6311\u6218\u3002","title":"1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u603b\u7ed3 (Concise summary)"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-27/#2-key-innovation-or-methodological-approach_2","text":"\u7aef\u5230\u7aefTransformer\u67b6\u6784\uff1a \u9996\u6b21\u5c06Transformer\u7f16\u7801\u5668-\u89e3\u7801\u5668\u6846\u67b6\u5e94\u7528\u4e8e\u5c4b\u9876\u5e73\u9762\u5206\u5272\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u5e73\u9762\u67e5\u8be2\uff08learnable plane queries\uff09\u76f4\u63a5\u9884\u6d4b\u5e73\u9762\u5b9e\u4f8b\u63a9\u7801\uff0c\u5b9e\u73b0\u4e86\u771f\u6b63\u7684\u7aef\u5230\u7aef\uff08end-to-end\uff09\u5206\u5272\uff0c\u907f\u514d\u4e86\u540e\u5904\u7406\u7684\u6b21\u4f18\u6027\u3002 \u8fb9\u7f18\u611f\u77e5\u63a9\u7801\u6a21\u5757\uff08EAMM\uff09\uff1a \u9488\u5bf9\u8fb9\u7f18\u533a\u57df\u7279\u5f81\u5224\u522b\u529b\u4f4e\u7684\u95ee\u9898\uff0c\u8bbe\u8ba1\u4e86EAMM\uff0c\u8be5\u6a21\u5757\u5145\u5206\u878d\u5165\u4e86\u5e73\u9762\u7684\u51e0\u4f55\u5148\u9a8c\u77e5\u8bc6\uff0c\u4ee5\u663e\u8457\u63d0\u5347\u8fb9\u7f18\u533a\u57df\u7684\u5206\u5272\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002 \u7efc\u5408\u635f\u5931\u51fd\u6570\u8bbe\u8ba1\uff1a \u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u52a0\u6743\u7b56\u7565\u7684\u63a9\u7801\u635f\u5931\uff0c\u4ee5\u964d\u4f4e\u8bef\u5206\u7c7b\u70b9\u7684\u5f71\u54cd\uff1b\u540c\u65f6\u5f15\u5165\u4e86\u65b0\u7684\u5e73\u9762\u51e0\u4f55\u635f\u5931\uff0c\u7528\u4e8e\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u663e\u5f0f\u5730\u7ea6\u675f\u7f51\u7edc\u5b66\u4e60\uff0c\u786e\u4fdd\u5206\u5272\u7ed3\u679c\u7684\u51e0\u4f55\u4e00\u81f4\u6027\u3002","title":"2. \u5173\u952e\u521b\u65b0\u70b9\u6216\u65b9\u6cd5\u5b66 (Key innovation or methodological approach)"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-27/#3-potential-impact-on-the-field_2","text":"\u63d0\u53473D\u5efa\u7b51\u6a21\u578b\u91cd\u5efa\u7cbe\u5ea6\uff1a \u901a\u8fc7\u63d0\u4f9b\u9ad8\u7cbe\u5ea6\u7684\u5c4b\u9876\u5e73\u9762\u5206\u5272\uff0c\u7279\u522b\u662f\u5bf9\u8fb9\u7f18\u533a\u57df\u7684\u7cbe\u786e\u5904\u7406\uff0c\u5c06\u76f4\u63a5\u63d0\u5347LoD 2\u548cLoD 3\u7ea7\u522b3D\u5efa\u7b51\u6a21\u578b\u7684\u91cd\u5efa\u8d28\u91cf\u548c\u81ea\u52a8\u5316\u6c34\u5e73\u3002 \u63a8\u52a8\u70b9\u4e91\u8bed\u4e49/\u5b9e\u4f8b\u5206\u5272\u53d1\u5c55\uff1a \u5f15\u5165Transformer\u67b6\u6784\u548c\u51e0\u4f55\u5148\u9a8c\u7ea6\u675f\uff0c\u4e3a\u70b9\u4e91\u6570\u636e\u4e2d\u590d\u6742\u51e0\u4f55\u4f53\u7684\u7aef\u5230\u7aef\u5206\u5272\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u7684\u8303\u5f0f\uff0c\u53ef\u80fd\u542f\u53d1\u5176\u4ed6\u7ed3\u6784\u5316\u573a\u666f\uff08\u5982\u5ba4\u5185\u3001\u5de5\u4e1a\u90e8\u4ef6\uff09\u7684\u5206\u5272\u65b9\u6cd5\u3002 \u7b80\u5316\u5de5\u4f5c\u6d41\u7a0b\uff1a \u771f\u6b63\u7684\u7aef\u5230\u7aef\u65b9\u6cd5\u51cf\u5c11\u4e86\u5bf9\u590d\u6742\u540e\u5904\u7406\u7684\u4f9d\u8d56\uff0c\u7b80\u5316\u4e86\u4ece\u539f\u59cbLiDAR\u70b9\u4e91\u5230\u7ed3\u6784\u5316\u5c4b\u9876\u6a21\u578b\u7684\u6574\u4e2a\u6d41\u7a0b\uff0c\u63d0\u9ad8\u4e86\u6548\u7387\u3002","title":"3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd (Potential impact on the field)"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-27/#4-related-areas-or-applications-that-might-benefit-from-this-research","text":"3D\u57ce\u5e02\u5efa\u6a21\u4e0e\u6570\u5b57\u5b6a\u751f\uff1a \u9ad8\u7cbe\u5ea6\u5c4b\u9876\u6a21\u578b\u662f\u6784\u5efa\u7cbe\u7ec6\u5316\u57ce\u5e02\u6570\u5b57\u5b6a\u751f\u548c\u5730\u7406\u4fe1\u606f\u7cfb\u7edf\uff08GIS\uff09\u7684\u57fa\u7840\u3002 \u667a\u6167\u57ce\u5e02\u5e94\u7528\uff1a \u4f8b\u5982\uff0c\u5c4b\u9876\u592a\u9633\u80fd\u677f\u5b89\u88c5\u6f5c\u529b\u8bc4\u4f30\u3001\u57ce\u5e02\u70ed\u5c9b\u6548\u5e94\u5206\u6790\u3001\u5efa\u7b51\u80fd\u8017\u6a21\u62df\u7b49\u3002 \u707e\u5bb3\u8bc4\u4f30\u4e0e\u7ba1\u7406\uff1a \u5feb\u901f\u51c6\u786e\u5730\u8bc4\u4f30\u5c4b\u9876\u5728\u81ea\u7136\u707e\u5bb3\uff08\u5982\u5730\u9707\u3001\u98d3\u98ce\uff09\u540e\u7684\u635f\u574f\u60c5\u51b5\u3002 \u81ea\u52a8\u9a7e\u9a76\u4e0e\u673a\u5668\u4eba\u5bfc\u822a\uff1a \u5e2e\u52a9\u8f66\u8f86\u548c\u673a\u5668\u4eba\u66f4\u597d\u5730\u7406\u89e3\u548c\u611f\u77e5\u5468\u56f4\u7684\u5efa\u7b51\u73af\u5883\u3002 \u5efa\u7b51\u4fe1\u606f\u6a21\u578b\uff08BIM\uff09\u4e0e\u6d4b\u7ed8\uff1a \u4e3a\u5efa\u7b51\u8bbe\u8ba1\u3001\u65bd\u5de5\u548c\u7ef4\u62a4\u63d0\u4f9b\u7cbe\u786e\u7684\u51e0\u4f55\u6570\u636e\u3002","title":"4. \u76f8\u5173\u5e94\u7528\u9886\u57df (Related areas or applications that might benefit from this research)"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-27/#5-any-limitations-that-can-be-inferred-from-the-abstract","text":"\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\uff1a Transformer\u6a21\u578b\u901a\u5e38\u5177\u6709\u8f83\u9ad8\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u5185\u5b58\u6d88\u8017\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u5927\u89c4\u6a21\u70b9\u4e91\u6570\u636e\u65f6\uff0c\u8bad\u7ec3\u548c\u63a8\u7406\u65f6\u95f4\u53ef\u80fd\u8f83\u957f\u3002 \u5bf9LiDAR\u70b9\u4e91\u8d28\u91cf\u7684\u4f9d\u8d56\uff1a \u5c3d\u7ba1\u58f0\u79f0\u201c\u8fb9\u7f18\u611f\u77e5\u201d\uff0c\u4f46LiDAR\u70b9\u4e91\u7684\u5bc6\u5ea6\u3001\u566a\u58f0\u548c\u626b\u63cf\u89d2\u5ea6\u4ecd\u53ef\u80fd\u5f71\u54cd\u8fb9\u7f18\u533a\u57df\u7684\u7cbe\u5ea6\uff0c\u7279\u522b\u662f\u5728\u70b9\u4e91\u7a00\u758f\u6216\u5b58\u5728\u906e\u6321\u7684\u533a\u57df\u3002 \u590d\u6742\u5c4b\u9876\u7ed3\u6784\u4e0e\u6cdb\u5316\u6027\uff1a \u62bd\u8c61\u4e2d\u672a\u63d0\u53ca\u6a21\u578b\u5bf9\u6781\u7aef\u590d\u6742\u3001\u975e\u6807\u51c6\u6216\u5177\u6709\u5927\u91cf\u9644\u5c5e\u7269\uff08\u5982\u70df\u56f1\u3001\u5929\u7a97\u3001\u690d\u88ab\u8986\u76d6\uff09\u7684\u5c4b\u9876\u7ed3\u6784\u7684\u6cdb\u5316\u80fd\u529b\u3002 \u51e0\u4f55\u5148\u9a8c\u7684\u5b9a\u4e49\u4e0e\u9002\u7528\u6027\uff1a \u201c\u5e73\u9762\u51e0\u4f55\u5148\u9a8c\u201d\u7684\u5177\u4f53\u5b9e\u73b0\u65b9\u5f0f\u548c\u5176\u5728\u5404\u79cd\u5c4b\u9876\u7c7b\u578b\uff08\u5982\u66f2\u9762\u5c4b\u9876\uff09\u4e0a\u7684\u9002\u7528\u6027\u5c1a\u4e0d\u660e\u786e\u3002\u5982\u679c\u5148\u9a8c\u8fc7\u4e8e\u521a\u6027\uff0c\u53ef\u80fd\u4f1a\u9650\u5236\u6a21\u578b\u7684\u7075\u6d3b\u6027\u3002 \u6570\u636e\u6807\u6ce8\u6210\u672c\uff1a \u7aef\u5230\u7aef\u8bad\u7ec3\u901a\u5e38\u9700\u8981\u5927\u91cf\u7684\u7cbe\u786e\u6807\u6ce8\u6570\u636e\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u8fb9\u7f18\u533a\u57df\u548c\u5e73\u9762\u5b9e\u4f8b\u7684\u6807\u6ce8\uff0c\u8fd9\u53ef\u80fd\u662f\u4e00\u4e2a\u6311\u6218\u3002 Key Findings: To solve these issues, a novel edge-aware transformer-based network, named RoofSeg, is developed for segmenting roof planes from LiDAR point clouds in a truly end-to-end manner. In addition, we propose an adaptive weighting strategy in the mask loss to reduce the influence of misclassified points, and also propose a new plane geometric loss to constrain the network training. Links: PDF arXiv","title":"5. \u53ef\u63a8\u65ad\u7684\u5c40\u9650\u6027 (Any limitations that can be inferred from the abstract)"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-27/#endoufm-utilizing-foundation-models-for-monocular-depth-estimation-of-endoscopic-images","text":"Authors: Xinning Yao, Bo Liu, Bojian Li, Jingjing Wang, Jinghua Yue, Fugen Zhou Published: 2025-08-25 Categories: cs.CV Abstract: Depth estimation is a foundational component for 3D reconstruction in minimally invasive endoscopic surgeries. However, existing monocular depth estimation techniques often exhibit limited performance to the varying illumination and complex textures of the surgical environment. While powerful visual foundation models offer a promising solution, their training on natural images leads to significant domain adaptability limitations and semantic perception deficiencies when applied to endoscopy. In this study, we introduce EndoUFM, an unsupervised monocular depth estimation framework that innovatively integrating dual foundation models for surgical scenes, which enhance the depth estimation performance by leveraging the powerful pre-learned priors. The framework features a novel adaptive fine-tuning strategy that incorporates Random Vector Low-Rank Adaptation (RVLoRA) to enhance model adaptability, and a Residual block based on Depthwise Separable Convolution (Res-DSC) to improve the capture of fine-grained local features. Furthermore, we design a mask-guided smoothness loss to enforce depth consistency within anatomical tissue structures. Extensive experiments on the SCARED, Hamlyn, SERV-CT, and EndoNeRF datasets confirm that our method achieves state-of-the-art performance while maintaining an efficient model size. This work contributes to augmenting surgeons' spatial perception during minimally invasive procedures, thereby enhancing surgical precision and safety, with crucial implications for augmented reality and navigation systems. Analysis: \u4f5c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u4e13\u5bb6\uff0c\u6211\u5bf9\u8fd9\u7bc7\u5173\u4e8eEndoUFM\u7684\u8bba\u6587\u6458\u8981\u8fdb\u884c\u4e86\u6df1\u5165\u5206\u6790\uff1a","title":"EndoUFM: Utilizing Foundation Models for Monocular depth estimation of endoscopic images"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-27/#endoufm-utilizing-foundation-models-for-monocular-depth-estimation-of-endoscopic-images_1","text":"1. \u8bba\u6587\u6838\u5fc3\u8d21\u732e\u603b\u7ed3 (Concise Summary) \u672c\u6587\u63d0\u51fa\u4e86EndoUFM\uff0c\u4e00\u4e2a\u9488\u5bf9\u5185\u7aa5\u955c\u56fe\u50cf\u7684\u65e0\u76d1\u7763\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u7b97\u6cd5\u3002\u5b83\u521b\u65b0\u6027\u5730\u6574\u5408\u4e86\u53cc\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u81ea\u9002\u5e94\u5fae\u8c03\u7b56\u7565\uff08RVLoRA\uff09\u3001\u6b8b\u5dee\u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\u5757\uff08Res-DSC\uff09\u4ee5\u53ca\u63a9\u819c\u5f15\u5bfc\u5e73\u6ed1\u635f\u5931\uff0c\u6709\u6548\u514b\u670d\u4e86\u57fa\u7840\u6a21\u578b\u5728\u5185\u7aa5\u955c\u9886\u57df\u5b58\u5728\u7684\u57df\u9002\u5e94\u548c\u8bed\u4e49\u611f\u77e5\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6df1\u5ea6\u4f30\u8ba1\u6027\u80fd\u3002\u8be5\u65b9\u6cd5\u65e8\u5728\u589e\u5f3a\u5916\u79d1\u533b\u751f\u7684\u7a7a\u95f4\u611f\u77e5\u80fd\u529b\uff0c\u63d0\u9ad8\u5fae\u521b\u624b\u672f\u7684\u7cbe\u786e\u6027\u548c\u5b89\u5168\u6027\u3002 2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u5b66\u65b9\u6cd5 (Key Innovation or Methodological Approach) EndoUFM\u7684\u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u5176 \u53cc\u57fa\u7840\u6a21\u578b\u7684\u521b\u65b0\u6027\u96c6\u6210 \uff0c\u65e8\u5728\u5229\u7528\u9884\u8bad\u7ec3\u7684\u5f3a\u5927\u5148\u9a8c\u77e5\u8bc6\u6765\u89e3\u51b3\u5185\u7aa5\u955c\u56fe\u50cf\u6df1\u5ea6\u4f30\u8ba1\u7684\u6311\u6218\u3002\u4e3a\u89e3\u51b3\u57fa\u7840\u6a21\u578b\u5728\u5185\u7aa5\u955c\u9886\u57df\u5b58\u5728\u7684\u663e\u8457\u57df\u9002\u5e94\u6027\u9650\u5236\u548c\u8bed\u4e49\u611f\u77e5\u7f3a\u9677\uff0c\u5b83\u5f15\u5165\u4e86\u4ee5\u4e0b\u5173\u952e\u65b9\u6cd5\uff1a \u57fa\u4e8e\u968f\u673a\u5411\u91cf\u4f4e\u79e9\u9002\u5e94\uff08RVLoRA\uff09\u7684\u81ea\u9002\u5e94\u5fae\u8c03\u7b56\u7565 \uff1a\u8fd9\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u6a21\u578b\u9002\u5e94\u6280\u672f\uff0c\u5141\u8bb8\u5728\u4e0d\u5b8c\u5168\u91cd\u65b0\u8bad\u7ec3\u6574\u4e2a\u5927\u578b\u57fa\u7840\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u5c11\u91cf\u53ef\u8bad\u7ec3\u53c2\u6570\u6765\u589e\u5f3a\u6a21\u578b\u5bf9\u5185\u7aa5\u955c\u56fe\u50cf\u7684\u9002\u5e94\u6027\u3002 \u57fa\u4e8e\u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\u7684\u6b8b\u5dee\u5757\uff08Res-DSC\uff09 \uff1a\u8fd9\u79cd\u8bbe\u8ba1\u65e8\u5728\u66f4\u6709\u6548\u5730\u6355\u83b7\u5185\u7aa5\u955c\u56fe\u50cf\u4e2d\u7cbe\u7ec6\u7684\u5c40\u90e8\u7279\u5f81\uff0c\u8fd9\u5bf9\u4e8e\u533a\u5206\u590d\u6742\u7ec4\u7ec7\u7ed3\u6784\u548c\u5fae\u5c0f\u75c5\u53d8\u81f3\u5173\u91cd\u8981\u3002 \u63a9\u819c\u5f15\u5bfc\u7684\u5e73\u6ed1\u635f\u5931\uff08Mask-guided smoothness loss\uff09 \uff1a\u901a\u8fc7\u5f15\u5165\u89e3\u5256\u7ec4\u7ec7\u63a9\u819c\u6765\u6307\u5bfc\u6df1\u5ea6\u56fe\u7684\u5e73\u6ed1\u6027\uff0c\u786e\u4fdd\u5728\u540c\u4e00\u7ec4\u7ec7\u7ed3\u6784\u5185\u90e8\u7684\u6df1\u5ea6\u4e00\u81f4\u6027\uff0c\u907f\u514d\u8de8\u8fb9\u754c\u7684\u6a21\u7cca\u6216\u4e0d\u51c6\u786e\u3002 3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd (Potential Impact on the Field) \u8fd9\u9879\u7814\u7a76\u5bf9\u5fae\u521b\u624b\u672f\u9886\u57df\u5177\u6709\u663e\u8457\u5f71\u54cd\u3002\u901a\u8fc7\u63d0\u4f9b\u9ad8\u7cbe\u5ea6\u7684\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\uff0c\u5b83\u80fd \u663e\u8457\u589e\u5f3a\u5916\u79d1\u533b\u751f\u5728\u624b\u672f\u8fc7\u7a0b\u4e2d\u7684\u7a7a\u95f4\u611f\u77e5\u80fd\u529b \uff0c\u4ece\u800c \u63d0\u9ad8\u624b\u672f\u7684\u7cbe\u786e\u6027\u548c\u5b89\u5168\u6027 \u3002\u6b64\u5916\uff0c\u5176\u6210\u679c\u5bf9\u4e8e \u589e\u5f3a\u73b0\u5b9e\uff08AR\uff09\u624b\u672f\u5bfc\u822a\u7cfb\u7edf \u548c \u672f\u4e2d\u4e09\u7ef4\u91cd\u5efa \u81f3\u5173\u91cd\u8981\uff0c\u6709\u671b\u63a8\u52a8\u8fd9\u4e9b\u6280\u672f\u7684\u4e34\u5e8a\u5e94\u7528\u3002\u4ece\u66f4\u5e7f\u6cdb\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u89d2\u5ea6\u770b\uff0c\u5b83\u4e3a \u5c06\u901a\u7528\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u6210\u529f\u8fc1\u79fb\u5e76\u9002\u5e94\u5230\u7279\u5b9a\u3001\u590d\u6742\u4e14\u6570\u636e\u7a00\u7f3a\u7684\u533b\u7597\u56fe\u50cf\u9886\u57df \u63d0\u4f9b\u4e86\u6709\u6548\u8303\u5f0f\uff0c\u5c55\u793a\u4e86\u57fa\u7840\u6a21\u578b\u5728\u4e13\u4e1a\u9886\u57df\u5e94\u7528\u7684\u5de8\u5927\u6f5c\u529b\u3002 4. \u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 (Related Areas or Applications that might benefit from this research) \u5fae\u521b\u5185\u7aa5\u955c\u624b\u672f\uff08MIS\uff09 \uff1a\u76f4\u63a5\u5e94\u7528\uff0c\u7528\u4e8e\u672f\u4e2d\u4e09\u7ef4\u91cd\u5efa\u3001\u76ee\u6807\u5b9a\u4f4d\u3001\u75c5\u7076\u6d4b\u91cf\u548c\u624b\u672f\u5668\u68b0\u8ddf\u8e2a\u3002 \u589e\u5f3a\u73b0\u5b9e\uff08AR\uff09/\u6df7\u5408\u73b0\u5b9e\uff08MR\uff09\u624b\u672f\u5bfc\u822a \uff1a\u5c06\u865a\u62df\u4fe1\u606f\uff08\u5982\u89c4\u5212\u8def\u5f84\u3001\u75c5\u7076\u8fb9\u754c\u3001\u91cd\u8981\u7ed3\u6784\uff09\u7cbe\u786e\u53e0\u52a0\u5230\u771f\u5b9e\u624b\u672f\u89c6\u91ce\u4e2d\uff0c\u63d0\u4f9b\u5b9e\u65f6\u5f15\u5bfc\u3002 \u673a\u5668\u4eba\u8f85\u52a9\u624b\u672f \uff1a\u4e3a\u624b\u672f\u673a\u5668\u4eba\u63d0\u4f9b\u66f4\u51c6\u786e\u7684\u73af\u5883\u611f\u77e5\u548c\u907f\u969c\u80fd\u529b\uff0c\u63d0\u9ad8\u81ea\u52a8\u5316\u6c34\u5e73\u3002 \u624b\u672f\u6a21\u62df\u4e0e\u8bad\u7ec3 \uff1a\u521b\u5efa\u66f4\u771f\u5b9e\u7684\u4e09\u7ef4\u624b\u672f\u573a\u666f\uff0c\u63d0\u9ad8\u533b\u5b66\u751f\u548c\u5916\u79d1\u533b\u751f\u7684\u57f9\u8bad\u6548\u679c\u3002 \u533b\u7597\u56fe\u50cf\u5206\u6790\u4e0e\u8bca\u65ad \uff1a\u4e3a\u5176\u4ed6\u57fa\u4e8e\u4e09\u7ef4\u4fe1\u606f\u7684\u5206\u6790\u4efb\u52a1\uff08\u5982\u80bf\u7624\u4f53\u79ef\u6d4b\u91cf\u3001\u5668\u5b98\u5f62\u53d8\u5206\u6790\uff09\u63d0\u4f9b\u57fa\u7840\u6570\u636e\u3002 \u901a\u7528\u57fa\u7840\u6a21\u578b\u5728\u7279\u5b9a\u9886\u57df\u7684\u9002\u5e94\u6027\u7814\u7a76 \uff1a\u4e3a\u5176\u4ed6\u4e13\u4e1a\u9886\u57df\uff08\u5982\u5de5\u4e1a\u68c0\u6d4b\u3001\u9065\u611f\u3001\u81ea\u52a8\u9a7e\u9a76\uff09\u7684\u57fa\u7840\u6a21\u578b\u5e94\u7528\u63d0\u4f9b\u501f\u9274\uff0c\u5c24\u5176\u662f\u5728\u6570\u636e\u6807\u6ce8\u6210\u672c\u9ad8\u6602\u6216\u6570\u636e\u7a00\u7f3a\u7684\u573a\u666f\u3002 5. \u4ece\u6458\u8981\u4e2d\u63a8\u65ad\u51fa\u7684\u5c40\u9650\u6027 (Any limitations that can be inferred from the abstract) \u65e0\u76d1\u7763\u5b66\u4e60\u7684\u56fa\u6709\u6311\u6218 \uff1a\u5c3d\u7ba1\u58f0\u79f0\u8fbe\u5230\u4e86SOTA\uff0c\u4f46\u65e0\u76d1\u7763\u65b9\u6cd5\u5728\u7406\u8bba\u4e0a\u53ef\u80fd\u4ecd\u65e0\u6cd5\u5b8c\u5168\u8d85\u8d8a\u5728\u5927\u91cf\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\u4e0a\u8bad\u7ec3\u7684\u76d1\u7763\u65b9\u6cd5\uff08\u5982\u679c\u6b64\u7c7b\u6570\u636e\u53ef\u7528\u7684\u8bdd\uff09\u3002\u5176\u6027\u80fd\u4e0a\u9650\u53ef\u80fd\u53d7\u9650\u4e8e\u81ea\u76d1\u7763\u4fe1\u53f7\u7684\u8d28\u91cf\u548c\u5185\u7aa5\u955c\u56fe\u50cf\u7684\u590d\u6742\u6027\u3002 \u57fa\u7840\u6a21\u578b\u7684\u8ba1\u7b97\u6210\u672c \uff1a\u867d\u7136\u6458\u8981\u63d0\u5230\u201c\u4fdd\u6301\u9ad8\u6548\u7684\u6a21\u578b\u5c3a\u5bf8\u201d\uff0c\u4f46\u201c\u53cc\u57fa\u7840\u6a21\u578b\u201d\u7684\u96c6\u6210\u901a\u5e38\u610f\u5473\u7740\u76f8\u5bf9\u8f83\u5927\u7684\u6a21\u578b\u53c2\u6570\u91cf\u548c\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u3002\u8fd9\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u6216\u9700\u8981\u6781\u4f4e\u5ef6\u8fdf\u7684\u5b9e\u65f6\u624b\u672f\u573a\u666f\u4e2d\uff0c\u53ef\u80fd\u4ecd\u662f\u4e00\u4e2a\u9700\u8981\u4ed4\u7ec6\u6743\u8861\u7684\u56e0\u7d20\u3002 \u63a9\u819c\u751f\u6210\u4f9d\u8d56\u6027 \uff1a\u6458\u8981\u4e2d\u63d0\u5230\u7684\u201c\u63a9\u819c\u5f15\u5bfc\u7684\u5e73\u6ed1\u635f\u5931\u201d\u9700\u8981\u51c6\u786e\u7684\u89e3\u5256\u7ec4\u7ec7\u63a9\u819c\u3002\u5982\u679c\u8fd9\u4e9b\u63a9\u819c\u9700\u8981\u4eba\u5de5\u6807\u6ce8\uff0c\u5c06\u589e\u52a0\u6570\u636e\u51c6\u5907\u7684\u6210\u672c\uff1b\u5982\u679c\u901a\u8fc7\u81ea\u52a8\u5316\u65b9\u6cd5\u751f\u6210\uff0c\u90a3\u4e48\u63a9\u819c\u672c\u8eab\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u5c06\u76f4\u63a5\u5f71\u54cd\u6df1\u5ea6\u4f30\u8ba1\u7684\u6027\u80fd\uff0c\u4e14\u81ea\u52a8\u5316\u63a9\u819c\u751f\u6210\u672c\u8eab\u4e5f\u662f\u4e00\u4e2a\u6311\u6218\u3002 \u9886\u57df\u6cdb\u5316\u6027 \uff1a\u5c3d\u7ba1\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u5185\u7aa5\u955c\u56fe\u50cf\u7684\u590d\u6742\u6027\u548c\u591a\u6837\u6027\uff08\u5982\u4e0d\u540c\u75c5\u7406\u3001\u4e0d\u540c\u5668\u68b0\u3001\u4e0d\u540c\u533b\u751f\u64cd\u4f5c\u4e60\u60ef\u3001\u6781\u7aef\u5149\u7167\u53d8\u5316\u3001\u51fa\u8840\u3001\u70df\u96fe\u7b49\uff09\u53ef\u80fd\u8fdc\u8d85\u73b0\u6709\u6570\u636e\u96c6\u7684\u8986\u76d6\u8303\u56f4\u3002\u6a21\u578b\u5728\u672a\u89c1\u8fc7\u7684\u6781\u7aef\u4e34\u5e8a\u6761\u4ef6\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u4ecd\u9700\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u3002 \u5b9e\u65f6\u6027\u8981\u6c42 \uff1a\u5bf9\u4e8e\u624b\u672f\u5bfc\u822a\u548cAR\u7cfb\u7edf\uff0c\u5b9e\u65f6\u6027\u662f\u5173\u952e\u3002\u6458\u8981\u672a\u660e\u786e\u63d0\u53ca\u6a21\u578b\u7684\u63a8\u7406\u901f\u5ea6\uff0c\u8fd9\u5bf9\u4e8e\u5b9e\u9645\u4e34\u5e8a\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002\u9ad8\u6548\u7684\u6a21\u578b\u5c3a\u5bf8\u5e76\u4e0d\u7b49\u540c\u4e8e\u9ad8\u901f\u63a8\u7406\u3002 Key Findings: In this study, we introduce EndoUFM, an unsupervised monocular depth estimation framework that innovatively integrating dual foundation models for surgical scenes, which enhance the depth estimation performance by leveraging the powerful pre-learned priors. The framework features a novel adaptive fine-tuning strategy that incorporates Random Vector Low-Rank Adaptation (RVLoRA) to enhance model adaptability, and a Residual block based on Depthwise Separable Convolution (Res-DSC) to improve the capture of fine-grained local features. Extensive experiments on the SCARED, Hamlyn, SERV-CT, and EndoNeRF datasets confirm that our method achieves state-of-the-art performance while maintaining an efficient model size. Links: PDF arXiv","title":"EndoUFM: Utilizing Foundation Models for Monocular depth estimation of endoscopic images"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-27/#isalux-illumination-and-segmentation-aware-transformer-employing-mixture-of-experts-for-low-light-image-enhancement","text":"Authors: Raul Balmez, Alexandru Brateanu, Ciprian Orhei, Codruta Ancuti, Cosmin Ancuti Published: 2025-08-25 Categories: cs.CV Abstract: We introduce ISALux, a novel transformer-based approach for Low-Light Image Enhancement (LLIE) that seamlessly integrates illumination and semantic priors. Our architecture includes an original self-attention block, Hybrid Illumination and Semantics-Aware Multi-Headed Self- Attention (HISA-MSA), which integrates illumination and semantic segmentation maps for en- hanced feature extraction. ISALux employs two self-attention modules to independently process illumination and semantic features, selectively enriching each other to regulate luminance and high- light structural variations in real-world scenarios. A Mixture of Experts (MoE)-based Feed-Forward Network (FFN) enhances contextual learning, with a gating mechanism conditionally activating the top K experts for specialized processing. To address overfitting in LLIE methods caused by distinct light patterns in benchmarking datasets, we enhance the HISA-MSA module with low-rank matrix adaptations (LoRA). Extensive qualitative and quantitative evaluations across multiple specialized datasets demonstrate that ISALux is competitive with state-of-the-art (SOTA) methods. Addition- ally, an ablation study highlights the contribution of each component in the proposed model. Code will be released upon publication. Analysis: \u8fd9\u7bc7\u8bba\u6587\u4ecb\u7ecd\u7684ISALux\u662f\u4e00\u4e2a\u5728\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\uff08LLIE\uff09\u9886\u57df\u5177\u6709\u521b\u65b0\u6027\u7684\u5de5\u4f5c\u3002\u4f5c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u4e13\u5bb6\uff0c\u4ee5\u4e0b\u662f\u5bf9\u5176\u6458\u8981\u7684\u8be6\u7ec6\u5206\u6790\uff1a","title":"ISALux: Illumination and Segmentation Aware Transformer Employing Mixture of Experts for Low Light Image Enhancement"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-27/#1-concise-summary_3","text":"ISALux\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8eTransformer\u7684\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\uff08LLIE\uff09\u65b9\u6cd5\uff0c\u5b83\u901a\u8fc7\u4e00\u4e2a\u6df7\u5408\u5149\u7167\u548c\u8bed\u4e49\u611f\u77e5\u591a\u5934\u81ea\u6ce8\u610f\u529b\uff08HISA-MSA\uff09\u6a21\u5757\uff0c\u65e0\u7f1d\u5730\u6574\u5408\u4e86\u5149\u7167\u548c\u8bed\u4e49\u5148\u9a8c\u4fe1\u606f\u3002\u8be5\u6a21\u578b\u5229\u7528\u4e24\u4e2a\u76f8\u4e92\u589e\u5f3a\u7684\u81ea\u6ce8\u610f\u529b\u6a21\u5757\u6765\u5904\u7406\u5149\u7167\u548c\u8bed\u4e49\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u57fa\u4e8e\u4e13\u5bb6\u6df7\u5408\uff08MoE\uff09\u7684\u524d\u9988\u7f51\u7edc\u589e\u5f3a\u4e0a\u4e0b\u6587\u5b66\u4e60\uff0c\u540c\u65f6\u5f15\u5165\u4f4e\u79e9\u77e9\u9635\u9002\u5e94\uff08LoRA\uff09\u6765\u89e3\u51b3LLIE\u65b9\u6cd5\u4e2d\u5e38\u89c1\u7684\u8fc7\u62df\u5408\u95ee\u9898\u3002","title":"1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u660e\u6458\u8981 (Concise Summary)"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-27/#2-key-innovation-or-methodological-approach_3","text":"ISALux\u7684\u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u5176 Hybrid Illumination and Semantics-Aware Multi-Headed Self-Attention (HISA-MSA) \u6a21\u5757\u3002\u8fd9\u4e2a\u6a21\u5757\u72ec\u7279\u5730\u5c06\u5149\u7167\u56fe\u548c\u8bed\u4e49\u5206\u5272\u56fe\u76f4\u63a5\u6574\u5408\u5230\u81ea\u6ce8\u610f\u529b\u673a\u5236\u4e2d\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u5bf9\u5149\u7167\u6761\u4ef6\u548c\u7269\u4f53\u8bed\u4e49\u5185\u5bb9\u90fd\u654f\u611f\u7684\u7279\u5f81\u63d0\u53d6\u3002\u5177\u4f53\u6765\u8bf4\uff1a \u53cc\u91cd\u81ea\u6ce8\u610f\u529b\u673a\u5236\u4e0e\u76f8\u4e92\u589e\u5f3a\uff1a ISALux\u91c7\u7528\u4e24\u4e2a\u72ec\u7acb\u7684\u81ea\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u5206\u522b\u5904\u7406\u5149\u7167\u7279\u5f81\u548c\u8bed\u4e49\u7279\u5f81\uff0c\u4f46\u5b83\u4eec\u80fd\u591f\u201c\u9009\u62e9\u6027\u5730\u76f8\u4e92\u4e30\u5bcc\u201d\uff0c\u8fd9\u8868\u660e\u5b83\u4eec\u4e4b\u95f4\u5b58\u5728\u4e00\u79cd\u534f\u540c\u4f5c\u7528\uff0c\u800c\u975e\u7b80\u5355\u7684\u5e76\u884c\u5904\u7406\uff0c\u4ece\u800c\u80fd\u591f\u66f4\u7cbe\u7ec6\u5730\u8c03\u8282\u4eae\u5ea6\u548c\u9ad8\u5149\u7ed3\u6784\u53d8\u5316\u3002 \u57fa\u4e8e\u4e13\u5bb6\u6df7\u5408\uff08MoE\uff09\u7684\u524d\u9988\u7f51\u7edc\uff1a \u5f15\u5165MoE-based FFN\uff0c\u901a\u8fc7\u4e00\u4e2a\u95e8\u63a7\u673a\u5236\u6709\u6761\u4ef6\u5730\u6fc0\u6d3b\u9876\u90e8\u7684K\u4e2a\u4e13\u5bb6\uff0c\u4ee5\u5b9e\u73b0\u66f4\u4e13\u4e1a\u5316\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u5904\u7406\uff0c\u8fd9\u6709\u52a9\u4e8e\u6a21\u578b\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u8fdb\u884c\u81ea\u9002\u5e94\u5b66\u4e60\u3002 LoRA\u7528\u4e8e\u89e3\u51b3\u8fc7\u62df\u5408\uff1a \u9488\u5bf9LLIE\u65b9\u6cd5\u4e2d\u56e0\u57fa\u51c6\u6570\u636e\u96c6\u5149\u7167\u6a21\u5f0f\u5dee\u5f02\u5bfc\u81f4\u7684\u8fc7\u62df\u5408\u95ee\u9898\uff0cISALux\u5728HISA-MSA\u6a21\u5757\u4e2d\u878d\u5165\u4e86\u4f4e\u79e9\u77e9\u9635\u9002\u5e94\uff08LoRA\uff09\u3002\u8fd9\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u53c2\u6570\u5fae\u8c03\u6280\u672f\uff0c\u901a\u5e38\u7528\u4e8e\u5927\u578b\u6a21\u578b\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u6a21\u578b\u5728\u4e0d\u540c\u5149\u7167\u6761\u4ef6\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002","title":"2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u5b66\u65b9\u6cd5 (Key Innovation or Methodological Approach)"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-27/#3-potential-impact-on-the-field_3","text":"LLIE\u6027\u80fd\u63d0\u5347\uff1a \u901a\u8fc7\u6df1\u5ea6\u6574\u5408\u8bed\u4e49\u548c\u5149\u7167\u5148\u9a8c\uff0c\u5e76\u7ed3\u5408MoE\u548cLoRA\uff0cISALux\u6709\u671b\u5728\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u4efb\u52a1\u4e0a\u8fbe\u5230\u6216\u8d85\u8d8a\u73b0\u6709SOTA\u65b9\u6cd5\uff0c\u63d0\u4f9b\u66f4\u81ea\u7136\u3001\u7ec6\u8282\u66f4\u4e30\u5bcc\u7684\u589e\u5f3a\u7ed3\u679c\u3002 \u591a\u6a21\u6001\u5148\u9a8c\u6574\u5408\u7684\u65b0\u8303\u5f0f\uff1a \u8be5\u7814\u7a76\u4e3aTransformer\u67b6\u6784\u4e2d\u5982\u4f55\u6709\u6548\u878d\u5408\u4e0d\u540c\u7c7b\u578b\u7684\uff08\u5982\u51e0\u4f55\u3001\u8bed\u4e49\u3001\u7269\u7406\uff09\u5148\u9a8c\u4fe1\u606f\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u6709\u529b\u7684\u8303\u4f8b\uff0c\u8fd9\u53ef\u80fd\u542f\u53d1\u5176\u4ed6\u4f4e\u7ea7\u89c6\u89c9\u4efb\u52a1\uff08\u5982\u53bb\u96fe\u3001\u53bb\u566a\u3001\u8d85\u5206\u8fa8\u7387\uff09\u7684\u8bbe\u8ba1\u3002 \u89e3\u51b3LLIE\u6cdb\u5316\u6027\u6311\u6218\uff1a LoRA\u7684\u5e94\u7528\u4e3a\u89e3\u51b3LLIE\u6a21\u578b\u5728\u4e0d\u540c\u5149\u7167\u6570\u636e\u96c6\u4e4b\u95f4\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u9014\u5f84\uff0c\u6709\u52a9\u4e8e\u5f00\u53d1\u51fa\u66f4\u9c81\u68d2\u3001\u66f4\u5177\u5b9e\u7528\u6027\u7684\u6a21\u578b\u3002 Transformer\u5728\u4f4e\u7ea7\u89c6\u89c9\u4e2d\u7684\u8fdb\u4e00\u6b65\u5e94\u7528\uff1a \u8fdb\u4e00\u6b65\u5de9\u56fa\u4e86Transformer\u5728\u56fe\u50cf\u589e\u5f3a\u7b49\u4f4e\u7ea7\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u5c55\u793a\u4e86\u5176\u5728\u6355\u6349\u957f\u8ddd\u79bb\u4f9d\u8d56\u548c\u590d\u6742\u4e0a\u4e0b\u6587\u4fe1\u606f\u65b9\u9762\u7684\u4f18\u52bf\u3002","title":"3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd (Potential Impact on the Field)"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-27/#4-related-areas-or-applications_2","text":"\u8ba1\u7b97\u673a\u89c6\u89c9\u5728\u6311\u6218\u6027\u73af\u5883\u4e0b\u7684\u5e94\u7528\uff1a \u81ea\u52a8\u9a7e\u9a76\u3001\u76d1\u63a7\u7cfb\u7edf\u3001\u673a\u5668\u4eba\u89c6\u89c9\u3001\u65e0\u4eba\u673a\u6210\u50cf\u7b49\uff0c\u8fd9\u4e9b\u573a\u666f\u5bf9\u591c\u95f4\u6216\u4f4e\u5149\u7167\u6761\u4ef6\u4e0b\u7684\u611f\u77e5\u80fd\u529b\u6709\u6781\u9ad8\u8981\u6c42\u3002 \u6d88\u8d39\u7ea7\u6444\u5f71\uff1a \u667a\u80fd\u624b\u673a\u548c\u5176\u4ed6\u76f8\u673a\u8bbe\u5907\u5728\u4f4e\u5149\u73af\u5883\u4e0b\u7684\u56fe\u50cf\u8d28\u91cf\u63d0\u5347\u3002 \u533b\u7597\u5f71\u50cf\uff1a \u589e\u5f3a\u4f4e\u5149\u663e\u5fae\u955c\u56fe\u50cf\u6216\u5185\u7aa5\u955c\u56fe\u50cf\u7684\u8d28\u91cf\uff0c\u4ee5\u8f85\u52a9\u8bca\u65ad\u3002 \u5176\u4ed6\u56fe\u50cf\u6062\u590d\u4efb\u52a1\uff1a \u8bba\u6587\u4e2d\u6574\u5408\u591a\u6a21\u6001\u5148\u9a8c\u7684\u65b9\u6cd5\u5b66\u601d\u60f3\u53ef\u4ee5\u63a8\u5e7f\u5230\u53bb\u96fe\u3001\u53bb\u566a\u3001\u56fe\u50cf\u53bb\u96e8\u7b49\u5176\u4ed6\u9700\u8981\u4e30\u5bcc\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u56fe\u50cf\u6062\u590d\u4efb\u52a1\u3002 \u591a\u6a21\u6001\u5b66\u4e60\uff1a \u4e3a\u5982\u4f55\u5c06\u56fe\u50cf\u7684\u50cf\u7d20\u7ea7\u4fe1\u606f\u4e0e\u9ad8\u7ea7\u8bed\u4e49\u4fe1\u606f\u6709\u6548\u7ed3\u5408\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7814\u7a76\u65b9\u5411\u3002","title":"4. \u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 (Related Areas or Applications)"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-27/#5-limitations-inferred-from-the-abstract","text":"\u5bf9\u8bed\u4e49\u5148\u9a8c\u7684\u4f9d\u8d56\uff1a \u6a21\u578b\u4f9d\u8d56\u4e8e\u8bed\u4e49\u5206\u5272\u56fe\u3002\u8fd9\u610f\u5473\u7740\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u8981\u4e48\u9700\u8981\u4e00\u4e2a\u989d\u5916\u7684\u3001\u9884\u8bad\u7ec3\u7684\u8bed\u4e49\u5206\u5272\u6a21\u578b\uff0c\u8fd9\u4f1a\u589e\u52a0\u8ba1\u7b97\u5f00\u9500\u548c\u6f5c\u5728\u7684\u9519\u8bef\u4f20\u64ad\uff08\u5982\u679c\u5206\u5272\u4e0d\u51c6\u786e\uff09\uff0c\u8981\u4e48\u6a21\u578b\u9700\u8981\u540c\u65f6\u5b66\u4e60\u5206\u5272\u548c\u589e\u5f3a\uff0c\u8fd9\u4f1a\u589e\u52a0\u6a21\u578b\u7684\u590d\u6742\u6027\u3002\u6458\u8981\u4e2d\u63d0\u5230\u201cintegrates...semantic priors\u201d\uff0c\u66f4\u503e\u5411\u4e8e\u524d\u8005\u3002 \u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\uff1a Transformer\u6a21\u578b\u672c\u8eab\u901a\u5e38\u8ba1\u7b97\u91cf\u8f83\u5927\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u65f6\u3002\u6b64\u5916\uff0cMoE\u7ed3\u6784\u867d\u7136\u5728\u53c2\u6570\u6548\u7387\u4e0a\u53ef\u80fd\u6709\u6240\u4f18\u52bf\uff0c\u4f46\u5728\u63a8\u7406\u65f6\u6fc0\u6d3b\u591a\u4e2a\u4e13\u5bb6\u4e5f\u53ef\u80fd\u589e\u52a0\u8ba1\u7b97\u8d1f\u62c5\u3002\u6458\u8981\u672a\u63d0\u53ca\u6a21\u578b\u7684\u6548\u7387\u6216\u5b9e\u65f6\u6027\u3002 \u8bad\u7ec3\u590d\u6742\u6027\uff1a \u7ed3\u5408\u4e86HISA-MSA\u3001MoE\u548cLoRA\u7684\u590d\u6742\u67b6\u6784\uff0c\u5176\u8bad\u7ec3\u8fc7\u7a0b\u53ef\u80fd\u9700\u8981\u5927\u91cf\u7684\u8ba1\u7b97\u8d44\u6e90\u548c\u7cbe\u7ec6\u7684\u8d85\u53c2\u6570\u8c03\u4f18\u3002 LoRA\u7684\u9002\u7528\u6027\uff1a \u5c3d\u7ba1LoRA\u6709\u52a9\u4e8e\u7f13\u89e3\u8fc7\u62df\u5408\uff0c\u4f46\u5176\u6548\u679c\u53ef\u80fd\u4ecd\u53d7\u9650\u4e8e\u8bad\u7ec3\u6570\u636e\u7684\u591a\u6837\u6027\u3002\u5728\u6781\u7aef\u6216\u672a\u89c1\u7684\u4f4e\u5149\u573a\u666f\u4e0b\uff0c\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u4ecd\u53ef\u80fd\u9762\u4e34\u6311\u6218\u3002 \u5b9e\u65f6\u6027\u4e0e\u90e8\u7f72\uff1a \u6458\u8981\u672a\u63d0\u4f9b\u5173\u4e8e\u6a21\u578b\u5927\u5c0f\u3001\u63a8\u7406\u901f\u5ea6\u6216\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72\u6f5c\u529b\u7684\u4fe1\u606f\uff0c\u8fd9\u5bf9\u4e8e\u5b9e\u9645\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002 Key Findings: We introduce ISALux, a novel transformer-based approach for Low-Light Image Enhancement (LLIE) that seamlessly integrates illumination and semantic priors. Extensive qualitative and quantitative evaluations across multiple specialized datasets demonstrate that ISALux is competitive with state-of-the-art (SOTA) methods. Addition- ally, an ablation study highlights the contribution of each component in the proposed model. Links: PDF arXiv","title":"5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u7684\u5c40\u9650\u6027 (Limitations Inferred from the Abstract)"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-28/","text":"Arxiv Computer Vision Papers - 2025-08-28 Executive Summary \u597d\u7684\uff0c\u8fd9\u662f\u4e00\u4efd\u9488\u5bf92025\u5e748\u670827\u65e5Arxiv\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u6700\u65b0\u8bba\u6587\u7684\u7b80\u660e\u6267\u884c\u6458\u8981\u3002 Arxiv \u8ba1\u7b97\u673a\u89c6\u89c9\u6bcf\u65e5\u62a5\u544a\u6267\u884c\u6458\u8981 (2025-08-27) \u672c\u62a5\u544a\u6db5\u76d6\u4e86\u4eca\u65e5Arxiv\u4e0a\u53d1\u5e03\u768410\u7bc7\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u8bba\u6587\uff0c\u4e3b\u8981\u805a\u7126\u4e8e \u57fa\u7840\u6a21\u578b\uff08Foundation Models\uff09\u7684\u5e7f\u6cdb\u5e94\u7528\u30013D\u89c6\u89c9\u7684\u8fdb\u6b65\u3001\u5148\u8fdb\u7684\u5206\u5272\u6280\u672f\u4ee5\u53caTransformer\u67b6\u6784\u7684\u6301\u7eed\u521b\u65b0 \u3002 1. \u4e3b\u8981\u8d8b\u52bf\u4e0e\u4e3b\u9898\u6982\u89c8\uff1a \u57fa\u7840\u6a21\u578b\u4e0e\u6cdb\u5316\u80fd\u529b (Foundation Models & Generalization): \u591a\u7bc7\u8bba\u6587\u63a2\u7d22\u5982\u4f55\u5229\u7528\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\uff08\u5982Vision Foundation Models\uff09\u6765\u63d0\u5347\u4efb\u52a1\u7684\u6cdb\u5316\u80fd\u529b\u3001\u51cf\u5c11\u5bf9\u5927\u91cf\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u5e76\u5b9e\u73b0\u5f00\u653e\u8bcd\u6c47\uff08Open-Vocabulary\uff09\u80fd\u529b\u3002\u8fd9\u5728\u76ee\u6807\u68c0\u6d4b\u548c\u7ec6\u7c92\u5ea6\u5206\u7c7b\u4e2d\u8868\u73b0\u5c24\u4e3a\u7a81\u51fa\u3002 \u5148\u8fdb\u7684\u5206\u5272\u6280\u672f (Advanced Segmentation Techniques): \u5206\u5272\u4efb\u52a1\u6301\u7eed\u6f14\u8fdb\uff0c\u5f15\u5165\u4e86\u6269\u6563\u6a21\u578b\u8fdb\u884c\u751f\u6210\u5f0f\u5206\u5272\uff0c\u4ee5\u53ca\u7ed3\u5408\u591a\u6a21\u6001\u3001\u539f\u578b\u5bf9\u9f50\u548c\u8fb9\u7f18\u611f\u77e5\u7b49\u65b9\u6cd5\uff0c\u4ee5\u5e94\u5bf9\u534a\u76d1\u7763\u3001\u533b\u5b66\u56fe\u50cf\u548c\u7279\u5b9a\u573a\u666f\uff08\u5982\u5c4b\u9876\u5e73\u9762\uff09\u7684\u6311\u6218\u3002 \u9c81\u68d2\u76843D\u89c6\u89c9 (Robust 3D Vision): 3D\u76ee\u6807\u68c0\u6d4b\u548c3D\u91cd\u5efa\u662f\u70ed\u95e8\u65b9\u5411\u3002\u7814\u7a76\u4eba\u5458\u81f4\u529b\u4e8e\u63d0\u9ad8\u5355\u76ee\u3001\u591a\u89c6\u89d23D\u68c0\u6d4b\u7684\u6cdb\u5316\u6027\uff0c\u5e76\u5229\u7528\u6269\u6563\u6a21\u578b\u8fdb\u884c\u7a00\u758f\u89c6\u89d2\u4e0b\u76843D\u91cd\u5efa\uff0c\u751a\u81f3\u5b9e\u73b0\u65e0\u4eba\u5de5\u6807\u6ce8\u7684\u5f00\u653e\u8bcd\u6c473D\u68c0\u6d4b\u3002 Transformer\u67b6\u6784\u521b\u65b0\u4e0e\u6548\u7387 (Architectural Innovations & Efficiency): Transformer\u4f5c\u4e3a\u6838\u5fc3\u67b6\u6784\uff0c\u5176\u5185\u90e8\u673a\u5236\uff08\u5982\u4f4d\u7f6e\u7f16\u7801\uff09\u4ecd\u5728\u88ab\u6df1\u5165\u7814\u7a76\u548c\u4f18\u5316\uff0c\u4ee5\u63d0\u9ad8\u5176\u51e0\u4f55\u611f\u77e5\u80fd\u529b\u3002\u540c\u65f6\uff0c\u6df7\u5408\u67b6\u6784\u548c\u9ad8\u6548\u5fae\u8c03\u6280\u672f\uff08\u5982LoRA\uff09\u88ab\u7528\u4e8e\u7279\u5b9a\u5e94\u7528\uff0c\u4ee5\u5e73\u8861\u6027\u80fd\u4e0e\u8ba1\u7b97\u8d44\u6e90\u3002 \u5782\u76f4\u9886\u57df\u5e94\u7528 (Vertical Applications): \u8ba1\u7b97\u673a\u89c6\u89c9\u6280\u672f\u5728\u533b\u7597\u8bca\u65ad\uff08\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\uff09\u3001\u519c\u4e1a\uff08\u82f9\u679c\u6811\u91cd\u5efa\uff09\u3001\u6c7d\u8f66\u5185\u9970\u68c0\u6d4b\u548c\u6606\u866b\u5206\u7c7b\u7b49\u4e13\u4e1a\u9886\u57df\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u5e94\u7528\u6f5c\u529b\u3002 2. \u7279\u522b\u503c\u5f97\u5173\u6ce8\u7684\u8bba\u6587\uff1a OpenM3D: Open Vocabulary Multi-view Indoor 3D Object Detection without Human Annotations (Peng-Hao Hsu et al.) \u521b\u65b0\u70b9: \u5728\u5ba4\u51853D\u76ee\u6807\u68c0\u6d4b\u9886\u57df\u5b9e\u73b0\u4e86\u5f00\u653e\u8bcd\u6c47\u80fd\u529b\uff0c\u4e14 \u65e0\u9700\u4eba\u5de5\u6807\u6ce8 \u3002\u8fd9\u6781\u5927\u5730\u964d\u4f4e\u4e86\u6570\u636e\u6807\u6ce8\u6210\u672c\uff0c\u5e76\u63d0\u5347\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u6027\u548c\u5b9e\u7528\u6027\uff0c\u662f3D\u89c6\u89c9\u9886\u57df\u7684\u4e00\u4e2a\u91cd\u8981\u7a81\u7834\u3002 GS: Generative Segmentation via Label Diffusion (Yuhao Chen et al.) \u521b\u65b0\u70b9: \u5c06\u6269\u6563\u6a21\u578b\u5f15\u5165\u5230\u5206\u5272\u4efb\u52a1\u4e2d\uff0c\u5b9e\u73b0\u4e86\u751f\u6210\u5f0f\u5206\u5272\u3002\u8fd9\u4e3a\u7406\u89e3\u548c\u751f\u6210\u50cf\u7d20\u7ea7\u6807\u7b7e\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9896\u4e14\u5f3a\u5927\u7684\u8303\u5f0f\uff0c\u6709\u671b\u5728\u5404\u79cd\u5206\u5272\u4efb\u52a1\u4e2d\u53d6\u5f97\u4f18\u5f02\u8868\u73b0\u3002 Beyond flattening: a geometrically principled positional encoding for vision transformers with Weierstrass elliptic functions (Zhihang Xin, Xitong Hu, Rui Wang) \u521b\u65b0\u70b9: \u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9b4f\u5c14\u65af\u7279\u62c9\u65af\u692d\u5706\u51fd\u6570\u7684\u51e0\u4f55\u539f\u7406\u4f4d\u7f6e\u7f16\u7801\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u7684\u6241\u5e73\u5316\u5904\u7406\u3002\u8fd9\u4ece\u7406\u8bba\u5c42\u9762\u63d0\u5347\u4e86Vision Transformer\u5bf9\u7a7a\u95f4\u4fe1\u606f\u7684\u611f\u77e5\u548c\u5904\u7406\u80fd\u529b\uff0c\u5bf9Transformer\u67b6\u6784\u7684\u672a\u6765\u53d1\u5c55\u5177\u6709\u6df1\u8fdc\u5f71\u54cd\u3002 Scalable Object Detection in the Car Interior With Vision Foundation Models (B\u00e1lint M\u00e9sz\u00e1ros et al.) \u521b\u65b0\u70b9: \u5c55\u793a\u4e86\u5982\u4f55\u9ad8\u6548\u5229\u7528\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u89e3\u51b3\u6c7d\u8f66\u5185\u9970\u8fd9\u4e00\u590d\u6742\u573a\u666f\u4e0b\u7684\u53ef\u6269\u5c55\u76ee\u6807\u68c0\u6d4b\u95ee\u9898\u3002\u8fd9\u4f53\u73b0\u4e86\u57fa\u7840\u6a21\u578b\u5728\u5b9e\u9645\u5de5\u4e1a\u5e94\u7528\u4e2d\u7684\u5de8\u5927\u4ef7\u503c\u548c\u6f5c\u529b\u3002 Bridging Domain Gaps for Fine-Grained Moth Classification Through Expert-Informed Adaptation and Foundation Model Priors (Ross J Gardiner et al.) \u521b\u65b0\u70b9: \u7ed3\u5408\u4e86\u9886\u57df\u4e13\u5bb6\u77e5\u8bc6\u548c\u57fa\u7840\u6a21\u578b\u5148\u9a8c\uff0c\u6709\u6548\u5f25\u5408\u4e86\u7ec6\u7c92\u5ea6\u5206\u7c7b\u4e2d\u7684\u9886\u57df\u9e3f\u6c9f\u3002\u8fd9\u79cd\u4eba\u673a\u534f\u4f5c\u7684\u8303\u5f0f\u4e3a\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u548c\u9886\u57df\u9002\u5e94\u6027\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u3002 3. \u65b0\u5174\u7814\u7a76\u65b9\u5411\u4e0e\u6280\u672f\uff1a \u6269\u6563\u6a21\u578b\u7684\u591a\u529f\u80fd\u6027 (Versatility of Diffusion Models): \u6269\u6563\u6a21\u578b\u4e0d\u518d\u5c40\u9650\u4e8e\u56fe\u50cf\u751f\u6210\uff0c\u6b63\u88ab\u79ef\u6781\u63a2\u7d22\u7528\u4e8e\u66f4\u590d\u6742\u7684\u611f\u77e5\u4efb\u52a1\uff0c\u5982\u50cf\u7d20\u7ea7\u5206\u5272\u548c3D\u91cd\u5efa\u3002 \u5f00\u653e\u8bcd\u6c47\u4e0e\u65e0\u6807\u6ce8\u5b66\u4e60 (Open-Vocabulary & Annotation-Free Learning): \u7ed3\u5408\u57fa\u7840\u6a21\u578b\uff0c\u5b9e\u73b0\u65e0\u9700\u7279\u5b9a\u7c7b\u522b\u6807\u6ce8\u6216\u751a\u81f3\u65e0\u9700\u4efb\u4f55\u4eba\u5de5\u6807\u6ce8\u7684\u8bc6\u522b\u548c\u68c0\u6d4b\uff0c\u662f\u672a\u6765\u964d\u4f4eAI\u5e94\u7528\u95e8\u69db\u7684\u5173\u952e\u3002 Transformer\u7684\u51e0\u4f55\u611f\u77e5\u8bbe\u8ba1 (Geometrically-Aware Transformer Design): \u6df1\u5165\u7814\u7a76Transformer\u7684\u5185\u90e8\u673a\u5236\uff0c\u7279\u522b\u662f\u4f4d\u7f6e\u7f16\u7801\uff0c\u4ee5\u66f4\u597d\u5730\u878d\u5165\u51e0\u4f55\u5148\u9a8c\u77e5\u8bc6\uff0c\u63d0\u5347\u6a21\u578b\u5bf9\u590d\u6742\u7a7a\u95f4\u7ed3\u6784\u7684\u7406\u89e3\u3002 \u9ad8\u6548\u6df7\u5408\u67b6\u6784\u4e0e\u5fae\u8c03 (Efficient Hybrid Architectures & Fine-tuning): \u7ed3\u5408\u4e0d\u540c\u6a21\u578b\uff08\u5982CNN\u3001Transformer\uff09\u7684\u4f18\u52bf\uff0c\u5e76\u5229\u7528LoRA\u7b49\u9ad8\u6548\u5fae\u8c03\u6280\u672f\uff0c\u4ee5\u5728\u7279\u5b9a\u5e94\u7528\u4e2d\u5b9e\u73b0\u6027\u80fd\u4e0e\u6548\u7387\u7684\u6700\u4f73\u5e73\u8861\u3002 4. \u5efa\u8bae\u6df1\u5165\u9605\u8bfb\u7684\u8bba\u6587\uff1a \u4e3a\u4e86\u5feb\u901f\u628a\u63e1\u9886\u57df\u524d\u6cbf\u548c\u6f5c\u5728\u7684\u7a81\u7834\u6027\u8fdb\u5c55\uff0c\u5efa\u8bae\u4f18\u5148\u9605\u8bfb\u4ee5\u4e0b\u8bba\u6587\uff1a OpenM3D: Open Vocabulary Multi-view Indoor 3D Object Detection without Human Annotations (Peng-Hao Hsu et al.) \u7406\u7531: \u89e3\u51b3\u4e863D\u89c6\u89c9\u4e2d\u5f00\u653e\u8bcd\u6c47\u548c\u6807\u6ce8\u6210\u672c\u4e24\u5927\u6838\u5fc3\u96be\u9898\uff0c\u5177\u6709\u6781\u9ad8\u7684\u7814\u7a76\u4ef7\u503c\u548c\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002 GS: Generative Segmentation via Label Diffusion (Yuhao Chen et al.) \u7406\u7531: \u4ee3\u8868\u4e86\u6269\u6563\u6a21\u578b\u5728\u611f\u77e5\u4efb\u52a1\u4e2d\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u4e3a\u5206\u5272\u4efb\u52a1\u63d0\u4f9b\u4e86\u5168\u65b0\u7684\u89c6\u89d2\u548c\u65b9\u6cd5\u3002 Beyond flattening: a geometrically principled positional encoding for vision transformers with Weierstrass elliptic functions (Zhihang Xin, Xitong Hu, Rui Wang) \u7406\u7531: \u5bf9Transformer\u8fd9\u4e00\u6838\u5fc3\u67b6\u6784\u7684\u5e95\u5c42\u673a\u5236\u8fdb\u884c\u4e86\u7406\u8bba\u6027\u521b\u65b0\uff0c\u53ef\u80fd\u5bf9\u672a\u6765\u6240\u6709\u57fa\u4e8eTransformer\u7684\u6a21\u578b\u4ea7\u751f\u57fa\u7840\u6027\u5f71\u54cd\u3002 Scalable Object Detection in the Car Interior With Vision Foundation Models (B\u00e1lint M\u00e9sz\u00e1ros et al.) \u7406\u7531: \u63d0\u4f9b\u4e86\u4e00\u4e2a\u5c06\u57fa\u7840\u6a21\u578b\u5e94\u7528\u4e8e\u590d\u6742\u3001\u9ad8\u8981\u6c42\u5b9e\u9645\u573a\u666f\u7684\u4f18\u79c0\u6848\u4f8b\uff0c\u5bf9\u4e8e\u7406\u89e3\u57fa\u7840\u6a21\u578b\u7684\u5de5\u7a0b\u5316\u5e94\u7528\u6709\u91cd\u8981\u53c2\u8003\u4ef7\u503c\u3002 Bridging Domain Gaps for Fine-Grained Moth Classification Through Expert-Informed Adaptation and Foundation Model Priors (Ross J Gardiner et al.) \u7406\u7531: \u5c55\u793a\u4e86\u5982\u4f55\u5de7\u5999\u7ed3\u5408\u4eba\u7c7b\u4e13\u5bb6\u77e5\u8bc6\u548cAI\u6a21\u578b\uff0c\u89e3\u51b3\u7ec6\u7c92\u5ea6\u5206\u7c7b\u548c\u9886\u57df\u9002\u5e94\u6027\u6311\u6218\uff0c\u5bf9\u4e8e\u6570\u636e\u7a00\u7f3a\u6216\u4e13\u4e1a\u6027\u5f3a\u7684\u4efb\u52a1\u6709\u501f\u9274\u610f\u4e49\u3002 Table of Contents GS: Generative Segmentation via Label Diffusion Multimodal Prototype Alignment for Semi-supervised Pathology Image Segmentation Scalable Object Detection in the Car Interior With Vision Foundation Models Generalizing Monocular 3D Object Detection DATR: Diffusion-based 3D Apple Tree Reconstruction Framework with Sparse-View Beyond flattening: a geometrically principled positional encoding for vision transformers with Weierstrass elliptic functions Bridging Domain Gaps for Fine-Grained Moth Classification Through Expert-Informed Adaptation and Foundation Model Priors OpenM3D: Open Vocabulary Multi-view Indoor 3D Object Detection without Human Annotations EffNetViTLoRA: An Efficient Hybrid Deep Learning Approach for Alzheimer's Disease Diagnosis RoofSeg: An edge-aware transformer-based network for end-to-end roof plane segmentation Papers GS: Generative Segmentation via Label Diffusion Authors: Yuhao Chen, Shubin Chen, Liang Lin, Guangrun Wang Published: 2025-08-27 Categories: cs.CV Abstract: Language-driven image segmentation is a fundamental task in vision-language understanding, requiring models to segment regions of an image corresponding to natural language expressions. Traditional methods approach this as a discriminative problem, assigning each pixel to foreground or background based on semantic alignment. Recently, diffusion models have been introduced to this domain, but existing approaches remain image-centric: they either (i) use image diffusion models as visual feature extractors, (ii) synthesize segmentation data via image generation to train discriminative models, or (iii) perform diffusion inversion to extract attention cues from pre-trained image diffusion models-thereby treating segmentation as an auxiliary process. In this paper, we propose GS (Generative Segmentation), a novel framework that formulates segmentation itself as a generative task via label diffusion. Instead of generating images conditioned on label maps and text, GS reverses the generative process: it directly generates segmentation masks from noise, conditioned on both the input image and the accompanying language description. This paradigm makes label generation the primary modeling target, enabling end-to-end training with explicit control over spatial and semantic fidelity. To demonstrate the effectiveness of our approach, we evaluate GS on Panoptic Narrative Grounding (PNG), a representative and challenging benchmark for multimodal segmentation that requires panoptic-level reasoning guided by narrative captions. Experimental results show that GS significantly outperforms existing discriminative and diffusion-based methods, setting a new state-of-the-art for language-driven segmentation. Analysis: \u8fd9\u7bc7\u8bba\u6587\u300aGS: Generative Segmentation via Label Diffusion\u300b\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8bed\u8a00\u9a71\u52a8\u56fe\u50cf\u5206\u5272\u65b9\u6cd5\uff0c\u4ee5\u4e0b\u662f\u8be6\u7ec6\u5206\u6790\uff1a 1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u6d01\u603b\u7ed3 (2-3 \u53e5\u8bdd) \u672c\u6587\u63d0\u51fa\u4e86GS\uff08Generative Segmentation\uff09\u6846\u67b6\uff0c\u5c06\u8bed\u8a00\u9a71\u52a8\u7684\u56fe\u50cf\u5206\u5272\u4efb\u52a1\u91cd\u65b0\u5b9a\u4e49\u4e3a\u901a\u8fc7\u6807\u7b7e\u6269\u6563\uff08label diffusion\uff09\u8fdb\u884c\u7684\u751f\u6210\u5f0f\u4efb\u52a1\u3002\u4e0e\u73b0\u6709\u65b9\u6cd5\u5c06\u5206\u5272\u89c6\u4e3a\u5224\u522b\u5f0f\u95ee\u9898\u6216\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7684\u8f85\u52a9\u8fc7\u7a0b\u4e0d\u540c\uff0cGS\u76f4\u63a5\u4ece\u566a\u58f0\u4e2d\u751f\u6210\u5206\u5272\u63a9\u7801\uff0c\u5e76\u4ee5\u8f93\u5165\u56fe\u50cf\u548c\u8bed\u8a00\u63cf\u8ff0\u4e3a\u6761\u4ef6\uff0c\u4f7f\u6807\u7b7e\u751f\u6210\u6210\u4e3a\u6838\u5fc3\u5efa\u6a21\u76ee\u6807\u3002\u5b9e\u9a8c\u8bc1\u660eGS\u5728Panoptic Narrative Grounding\u57fa\u51c6\u4e0a\u663e\u8457\u8d85\u8d8a\u4e86\u73b0\u6709\u5224\u522b\u5f0f\u548c\u57fa\u4e8e\u6269\u6563\u7684\u65b9\u6cd5\uff0c\u8fbe\u5230\u4e86\u65b0\u7684SOTA\u3002 2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u8bba \u6838\u5fc3\u521b\u65b0\u5728\u4e8e \u8303\u5f0f\u8f6c\u53d8 \uff1a\u5c06\u56fe\u50cf\u5206\u5272\u4efb\u52a1\u672c\u8eab\u4ece\u4f20\u7edf\u7684\u5224\u522b\u5f0f\u95ee\u9898\uff08\u5c06\u50cf\u7d20\u5206\u7c7b\u4e3a\u524d\u666f/\u80cc\u666f\uff09\u6216\u5c06\u5206\u5272\u4f5c\u4e3a\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7684\u8f85\u52a9\u8fc7\u7a0b\uff0c\u8f6c\u53d8\u4e3a\u4e00\u4e2a \u7eaf\u7cb9\u7684\u751f\u6210\u5f0f\u4efb\u52a1 \u3002 \u5177\u4f53\u65b9\u6cd5\u662f\u5f15\u5165 \u201c\u6807\u7b7e\u6269\u6563\u201d\uff08label diffusion\uff09 \u8303\u5f0f\uff1a * \u9006\u8f6c\u751f\u6210\u8fc7\u7a0b\uff1a \u73b0\u6709\u6269\u6563\u6a21\u578b\u901a\u5e38\u662f\u6839\u636e\u6807\u7b7e\u56fe\u548c\u6587\u672c\u751f\u6210\u56fe\u50cf\uff0c\u800cGS\u5219\u53cd\u5176\u9053\u800c\u884c\u4e4b\uff0c\u76f4\u63a5\u4ece\u968f\u673a\u566a\u58f0\u4e2d\u751f\u6210\u5206\u5272\u63a9\u7801\u3002 * \u591a\u6a21\u6001\u6761\u4ef6\uff1a \u751f\u6210\u8fc7\u7a0b\u540c\u65f6\u4ee5\u8f93\u5165\u56fe\u50cf\u548c\u4f34\u968f\u7684\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u4e3a\u6761\u4ef6\u3002 * \u6807\u7b7e\u751f\u6210\u4e3a\u6838\u5fc3\uff1a \u8fd9\u79cd\u65b9\u6cd5\u4f7f\u5f97\u5206\u5272\u63a9\u7801\u7684\u751f\u6210\u6210\u4e3a\u6a21\u578b\u7684\u4e3b\u8981\u76ee\u6807\uff0c\u800c\u975e\u4ece\u56fe\u50cf\u7279\u5f81\u4e2d\u63a8\u65ad\u6216\u4f5c\u4e3a\u56fe\u50cf\u751f\u6210\u8fc7\u7a0b\u7684\u526f\u4ea7\u54c1\u3002 * \u7aef\u5230\u7aef\u8bad\u7ec3\u4e0e\u663e\u5f0f\u63a7\u5236\uff1a \u8fd9\u79cd\u8303\u5f0f\u5141\u8bb8\u7aef\u5230\u7aef\u8bad\u7ec3\uff0c\u5e76\u80fd\u5bf9\u751f\u6210\u7684\u5206\u5272\u63a9\u7801\u7684\u7a7a\u95f4\u548c\u8bed\u4e49\u4fdd\u771f\u5ea6\u8fdb\u884c\u663e\u5f0f\u63a7\u5236\u3002 3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd \u5f00\u8f9f\u65b0\u7684\u7814\u7a76\u65b9\u5411\uff1a \u5c06\u6269\u6563\u6a21\u578b\u5e94\u7528\u4e8e\u7ed3\u6784\u5316\u9884\u6d4b\u4efb\u52a1\uff08\u5982\u5206\u5272\uff09\u7684\u751f\u6210\u5f0f\u5efa\u6a21\uff0c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u5176\u4ed6\u7c7b\u4f3c\u4efb\u52a1\uff08\u5982\u6df1\u5ea6\u4f30\u8ba1\u3001\u59ff\u6001\u4f30\u8ba1\u3001\u573a\u666f\u56fe\u751f\u6210\u7b49\uff09\u63d0\u4f9b\u4e86\u65b0\u7684\u5efa\u6a21\u601d\u8def\u548c\u8303\u5f0f\u3002 \u63d0\u5347\u591a\u6a21\u6001\u7406\u89e3\u80fd\u529b\uff1a \u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u590d\u6742\u8bed\u8a00\u63cf\u8ff0\u4e0b\u5bf9\u56fe\u50cf\u5185\u5bb9\u8fdb\u884c\u7cbe\u7ec6\u5206\u5272\u7684\u80fd\u529b\uff0c\u63a8\u52a8\u4e86\u89c6\u89c9-\u8bed\u8a00\u7406\u89e3\u7684\u8fb9\u754c\u3002 \u66f4\u5f3a\u5927\u7684\u5206\u5272\u6a21\u578b\uff1a \u751f\u6210\u5f0f\u65b9\u6cd5\u53ef\u80fd\u6bd4\u5224\u522b\u5f0f\u65b9\u6cd5\u66f4\u80fd\u6355\u6349\u590d\u6742\u7684\u8bed\u4e49\u548c\u7a7a\u95f4\u5173\u7cfb\uff0c\u4ece\u800c\u4ea7\u751f\u66f4\u9c81\u68d2\u3001\u66f4\u7cbe\u7ec6\u7684\u5206\u5272\u7ed3\u679c\u3002 \u589e\u5f3a\u6a21\u578b\u53ef\u63a7\u6027\uff1a \u5f3a\u8c03\u5bf9\u7a7a\u95f4\u548c\u8bed\u4e49\u4fdd\u771f\u5ea6\u7684\u663e\u5f0f\u63a7\u5236\uff0c\u8fd9\u5bf9\u4e8e\u9700\u8981\u9ad8\u7cbe\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u7684\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002 4. \u53ef\u80fd\u53d7\u76ca\u7684\u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 \u591a\u6a21\u6001\u7406\u89e3\u4e0e\u4ea4\u4e92\uff1a \u667a\u80fd\u52a9\u624b\u3001\u56fe\u50cf\u641c\u7d22\u3001\u5185\u5bb9\u521b\u4f5c\u7b49\u9886\u57df\uff0c\u673a\u5668\u53ef\u4ee5\u66f4\u7cbe\u786e\u5730\u7406\u89e3\u7528\u6237\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63d0\u51fa\u7684\u56fe\u50cf\u7f16\u8f91\u6216\u67e5\u8be2\u9700\u6c42\u3002 \u673a\u5668\u4eba\u4e0e\u81ea\u52a8\u5316\uff1a \u673a\u5668\u4eba\u53ef\u4ee5\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u66f4\u7cbe\u786e\u5730\u8bc6\u522b\u548c\u64cd\u4f5c\u73af\u5883\u4e2d\u7684\u7279\u5b9a\u5bf9\u8c61\u6216\u533a\u57df\uff0c\u4f8b\u5982\u201c\u62ff\u8d77\u684c\u5b50\u4e0a\u90a3\u4e2a\u7ea2\u8272\u7684\u676f\u5b50\u65c1\u8fb9\u7684\u5c0f\u76d2\u5b50\u201d\u3002 \u56fe\u50cf\u7f16\u8f91\u4e0e\u5185\u5bb9\u751f\u6210\uff1a \u63d0\u4f9b\u66f4\u7cbe\u7ec6\u3001\u8bed\u8a00\u9a71\u52a8\u7684\u56fe\u50cf\u7f16\u8f91\u548c\u751f\u6210\u80fd\u529b\uff0c\u7528\u6237\u53ef\u4ee5\u901a\u8fc7\u63cf\u8ff0\u6765\u7cbe\u786e\u4fee\u6539\u56fe\u50cf\u4e2d\u7684\u7279\u5b9a\u90e8\u5206\uff0c\u4f8b\u5982\u201c\u628a\u8fd9\u4e2a\u4eba\u7684\u5934\u53d1\u67d3\u6210\u84dd\u8272\u201d\u3002 \u533b\u7597\u5f71\u50cf\u5206\u6790\uff1a \u7ed3\u5408\u533b\u751f\u5bf9\u75c5\u7076\u7684\u63cf\u8ff0\uff08\u5982\u201c\u5de6\u80ba\u4e0a\u53f6\u9760\u8fd1\u80f8\u58c1\u7684\u7ed3\u8282\u201d\uff09\uff0c\u5b9e\u73b0\u66f4\u7cbe\u51c6\u7684\u75c5\u7076\u5206\u5272\u548c\u5b9a\u4f4d\u3002 \u8f85\u52a9\u9a7e\u9a76\uff1a \u7406\u89e3\u590d\u6742\u7684\u573a\u666f\u63cf\u8ff0\uff0c\u5e2e\u52a9\u8f66\u8f86\u8bc6\u522b\u7279\u5b9a\u76ee\u6807\u6216\u5371\u9669\u533a\u57df\uff0c\u4f8b\u5982\u201c\u6ce8\u610f\u524d\u65b9\u53f3\u4fa7\u8f66\u9053\u4e0a\u90a3\u8f86\u767d\u8272\u5361\u8f66\u65c1\u8fb9\u7684\u884c\u4eba\u201d\u3002 5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u7684\u5c40\u9650\u6027 \u8ba1\u7b97\u6210\u672c\uff1a \u6269\u6563\u6a21\u578b\u901a\u5e38\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u9636\u6bb5\u8ba1\u7b97\u91cf\u8f83\u5927\uff0c\u5c24\u5176\u662f\u5728\u751f\u6210\u9ad8\u5206\u8fa8\u7387\u63a9\u7801\u65f6\uff0c\u8fd9\u53ef\u80fd\u9650\u5236\u5176\u5728\u8d44\u6e90\u53d7\u9650\u6216\u5b9e\u65f6\u6027\u8981\u6c42\u9ad8\u7684\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002 \u6570\u636e\u4f9d\u8d56\uff1a \u5c3d\u7ba1\u6458\u8981\u5f3a\u8c03\u4e86\u5176\u751f\u6210\u80fd\u529b\uff0c\u4f46\u8bad\u7ec3\u4e00\u4e2a\u5f3a\u5927\u7684\u6269\u6563\u6a21\u578b\u901a\u5e38\u9700\u8981\u5927\u91cf\u7684\u6807\u6ce8\u6570\u636e\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u590d\u6742\u7684\u591a\u6a21\u6001\u8f93\u5165\u65f6\u3002 \u6807\u7b7e\u7a7a\u95f4\u8868\u793a\u7684\u6311\u6218\uff1a \u6458\u8981\u4e2d\u672a\u8be6\u7ec6\u8bf4\u660e\u201c\u6807\u7b7e\u6269\u6563\u201d\u5982\u4f55\u5904\u7406\u79bb\u6563\u7684\u50cf\u7d20\u7ea7\u6807\u7b7e\u7a7a\u95f4\uff08\u4f8b\u5982\uff0c\u5982\u4f55\u5c06\u4e8c\u503c\u6216\u591a\u7c7b\u5206\u5272\u63a9\u7801\u878d\u5165\u8fde\u7eed\u7684\u6269\u6563\u8fc7\u7a0b\uff09\uff0c\u8fd9\u53ef\u80fd\u6d89\u53ca\u590d\u6742\u7684\u8fde\u7eed\u5316\u6216\u79bb\u6563\u5316\u7b56\u7565\uff0c\u5176\u8bbe\u8ba1\u548c\u4f18\u5316\u53ef\u80fd\u5177\u6709\u6311\u6218\u6027\u3002 \u6cdb\u5316\u80fd\u529b\uff1a \u8bba\u6587\u4e3b\u8981\u5728Panoptic Narrative Grounding (PNG) \u57fa\u51c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002\u5176\u5728\u5176\u4ed6\u7c7b\u578b\u7684\u5206\u5272\u4efb\u52a1\uff08\u5982\u7eaf\u8bed\u4e49\u5206\u5272\u3001\u5b9e\u4f8b\u5206\u5272\u6216\u66f4\u7b80\u5355\u7684\u8bed\u8a00\u5f15\u5bfc\u5206\u5272\uff09\u4e0a\u7684\u8868\u73b0\u548c\u6548\u7387\u5c1a\u4e0d\u660e\u786e\u3002 \u53ef\u63a7\u6027\u7c92\u5ea6\uff1a \u5c3d\u7ba1\u63d0\u5230\u201c\u663e\u5f0f\u63a7\u5236\u201d\uff0c\u4f46\u5177\u4f53\u5982\u4f55\u901a\u8fc7\u8bed\u8a00\u63cf\u8ff0\u5b9e\u73b0\u5bf9\u751f\u6210\u63a9\u7801\u7684\u7cbe\u7ec6\u3001\u591a\u5c42\u6b21\u63a7\u5236\uff0c\u4ee5\u53ca\u8fd9\u79cd\u63a7\u5236\u7684\u8fb9\u754c\u5728\u54ea\u91cc\uff0c\u4ecd\u9700\u8fdb\u4e00\u6b65\u63a2\u8ba8\u3002 Key Findings: In this paper, we propose GS (Generative Segmentation), a novel framework that formulates segmentation itself as a generative task via label diffusion. To demonstrate the effectiveness of our approach, we evaluate GS on Panoptic Narrative Grounding (PNG), a representative and challenging benchmark for multimodal segmentation that requires panoptic-level reasoning guided by narrative captions. Experimental results show that GS significantly outperforms existing discriminative and diffusion-based methods, setting a new state-of-the-art for language-driven segmentation. Links: PDF arXiv Multimodal Prototype Alignment for Semi-supervised Pathology Image Segmentation Authors: Mingxi Fu, Fanglei Fu, Xitong Ling, Huaitian Yuan, Tian Guan, Yonghong He, Lianghui Zhu Published: 2025-08-27 Categories: cs.CV, cs.AI Abstract: Pathological image segmentation faces numerous challenges, particularly due to ambiguous semantic boundaries and the high cost of pixel-level annotations. Although recent semi-supervised methods based on consistency regularization (e.g., UniMatch) have made notable progress, they mainly rely on perturbation-based consistency within the image modality, making it difficult to capture high-level semantic priors, especially in structurally complex pathology images. To address these limitations, we propose MPAMatch - a novel segmentation framework that performs pixel-level contrastive learning under a multimodal prototype-guided supervision paradigm. The core innovation of MPAMatch lies in the dual contrastive learning scheme between image prototypes and pixel labels, and between text prototypes and pixel labels, providing supervision at both structural and semantic levels. This coarse-to-fine supervisory strategy not only enhances the discriminative capability on unlabeled samples but also introduces the text prototype supervision into segmentation for the first time, significantly improving semantic boundary modeling. In addition, we reconstruct the classic segmentation architecture (TransUNet) by replacing its ViT backbone with a pathology-pretrained foundation model (Uni), enabling more effective extraction of pathology-relevant features. Extensive experiments on GLAS, EBHI-SEG-GLAND, EBHI-SEG-CANCER, and KPI show MPAMatch's superiority over state-of-the-art methods, validating its dual advantages in structural and semantic modeling. Analysis: \u8fd9\u7bc7\u8bba\u6587\u6458\u8981\u63cf\u8ff0\u4e86\u4e00\u9879\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\uff0c\u7279\u522b\u662f\u533b\u5b66\u56fe\u50cf\u5206\u6790\u65b9\u9762\u5177\u6709\u6f5c\u5728\u91cd\u8981\u6027\u7684\u5de5\u4f5c\u3002\u4ee5\u4e0b\u662f\u8be6\u7ec6\u5206\u6790\uff1a 1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u603b\u7ed3 (Main Contribution Summary) \u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMPAMatch\u7684\u65b0\u578b\u534a\u76d1\u7763\u5206\u5272\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u75c5\u7406\u56fe\u50cf\u5206\u5272\u4e2d\u8bed\u4e49\u8fb9\u754c\u6a21\u7cca\u548c\u50cf\u7d20\u7ea7\u6807\u6ce8\u6210\u672c\u9ad8\u6602\u7684\u95ee\u9898\u3002\u5176\u6838\u5fc3\u8d21\u732e\u5728\u4e8e\u5f15\u5165\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u539f\u578b\u5f15\u5bfc\u7684\u76d1\u7763\u8303\u5f0f\uff0c\u901a\u8fc7\u56fe\u50cf\u539f\u578b\u4e0e\u50cf\u7d20\u6807\u7b7e\u3001\u6587\u672c\u539f\u578b\u4e0e\u50cf\u7d20\u6807\u7b7e\u4e4b\u95f4\u7684\u53cc\u91cd\u5bf9\u6bd4\u5b66\u4e60\uff0c\u9996\u6b21\u5c06\u6587\u672c\u8bed\u4e49\u5148\u9a8c\u5f15\u5165\u5206\u5272\u4efb\u52a1\uff0c\u4ece\u800c\u5728\u7ed3\u6784\u548c\u8bed\u4e49\u5c42\u9762\u63d0\u4f9b\u76d1\u7763\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5bf9\u590d\u6742\u75c5\u7406\u56fe\u50cf\u7684\u5224\u522b\u80fd\u529b\u548c\u8bed\u4e49\u8fb9\u754c\u5efa\u6a21\u7cbe\u5ea6\u3002 2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u5b66 (Key Innovation or Methodological Approach) MPAMatch\u7684\u5173\u952e\u521b\u65b0\u5728\u4e8e\u5176 \u591a\u6a21\u6001\u539f\u578b\u5bf9\u9f50\uff08Multimodal Prototype Alignment\uff09 \u7b56\u7565\uff0c\u5177\u4f53\u4f53\u73b0\u5728\u4ee5\u4e0b\u51e0\u70b9\uff1a \u53cc\u91cd\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6848 (Dual Contrastive Learning Scheme)\uff1a \u8fd9\u662f\u6700\u6838\u5fc3\u7684\u521b\u65b0\u3002 \u56fe\u50cf\u539f\u578b\u4e0e\u50cf\u7d20\u6807\u7b7e\u7684\u5bf9\u6bd4\u5b66\u4e60\uff1a \u7528\u4e8e\u6355\u83b7\u56fe\u50cf\u7684\u7ed3\u6784\u4fe1\u606f\uff0c\u589e\u5f3a\u5bf9\u672a\u6807\u6ce8\u6837\u672c\u7684\u5224\u522b\u80fd\u529b\u3002 \u6587\u672c\u539f\u578b\u4e0e\u50cf\u7d20\u6807\u7b7e\u7684\u5bf9\u6bd4\u5b66\u4e60\uff1a \u9996\u6b21\u5c06\u9ad8\u5c42\u8bed\u4e49\u5148\u9a8c\uff08\u901a\u8fc7\u6587\u672c\u63cf\u8ff0\uff09\u5f15\u5165\u50cf\u7d20\u7ea7\u5206\u5272\u4efb\u52a1\u3002\u8fd9\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u7406\u89e3\u548c\u5229\u7528\u6587\u672c\u4e2d\u8574\u542b\u7684\u8bed\u4e49\u4fe1\u606f\uff0c\u4ece\u800c\u66f4\u597d\u5730\u5904\u7406\u6a21\u7cca\u7684\u8bed\u4e49\u8fb9\u754c\u3002 \u7c97\u5230\u7ec6\u7684\u76d1\u7763\u7b56\u7565 (Coarse-to-fine Supervisory Strategy)\uff1a \u7ed3\u5408\u7ed3\u6784\u548c\u8bed\u4e49\u5c42\u9762\u7684\u76d1\u7763\uff0c\u63d0\u4f9b\u66f4\u5168\u9762\u548c\u7cbe\u7ec6\u7684\u6307\u5bfc\u3002 \u67b6\u6784\u589e\u5f3a (Architectural Enhancement)\uff1a \u5c06\u7ecf\u5178\u7684TransUNet\u67b6\u6784\u4e2d\u7684ViT\u9aa8\u5e72\u7f51\u7edc\u66ff\u6362\u4e3a\u9884\u8bad\u7ec3\u7684\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b\uff08Uni\uff09\uff0c\u4ee5\u66f4\u6709\u6548\u5730\u63d0\u53d6\u4e0e\u75c5\u7406\u5b66\u76f8\u5173\u7684\u7279\u5f02\u6027\u7279\u5f81\uff0c\u4e3a\u540e\u7eed\u7684\u5bf9\u6bd4\u5b66\u4e60\u548c\u5206\u5272\u4efb\u52a1\u63d0\u4f9b\u9ad8\u8d28\u91cf\u7684\u8868\u793a\u3002 3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd (Potential Impact on the Field) \u964d\u4f4e\u6807\u6ce8\u6210\u672c\uff0c\u52a0\u901f\u533b\u5b66AI\u53d1\u5c55\uff1a \u901a\u8fc7\u9ad8\u6548\u7684\u534a\u76d1\u7763\u5b66\u4e60\uff0c\u663e\u8457\u51cf\u5c11\u5bf9\u6602\u8d35\u4e14\u8017\u65f6\u7684\u50cf\u7d20\u7ea7\u75c5\u7406\u56fe\u50cf\u6807\u6ce8\u7684\u4f9d\u8d56\uff0c\u4ece\u800c\u52a0\u901f\u75c5\u7406AI\u6a21\u578b\u7684\u5f00\u53d1\u548c\u90e8\u7f72\u3002 \u63d0\u5347\u75c5\u7406\u56fe\u50cf\u5206\u5272\u7cbe\u5ea6\uff1a \u5c24\u5176\u662f\u5728\u5904\u7406\u8bed\u4e49\u8fb9\u754c\u6a21\u7cca\u548c\u7ed3\u6784\u590d\u6742\u7684\u75c5\u7406\u56fe\u50cf\u65f6\uff0c\u7ed3\u5408\u6587\u672c\u8bed\u4e49\u4fe1\u606f\u6709\u671b\u5e26\u6765\u7a81\u7834\u6027\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5bf9\u75be\u75c5\u8bca\u65ad\u3001\u9884\u540e\u8bc4\u4f30\u548c\u6cbb\u7597\u89c4\u5212\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002 \u63a8\u52a8\u591a\u6a21\u6001\u5b66\u4e60\u5728\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u7684\u5e94\u7528\uff1a \u9996\u6b21\u5c06\u6587\u672c\u539f\u578b\u76d1\u7763\u5f15\u5165\u5206\u5272\u4efb\u52a1\uff0c\u4e3a\u672a\u6765\u5728\u533b\u5b66\u56fe\u50cf\u9886\u57df\u6574\u5408\u66f4\u591a\u6a21\u6001\uff08\u5982\u4e34\u5e8a\u62a5\u544a\u3001\u57fa\u56e0\u7ec4\u6570\u636e\uff09\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u548c\u8303\u5f0f\u3002 \u9a8c\u8bc1\u57fa\u7840\u6a21\u578b\u5728\u7279\u5b9a\u9886\u57df\uff08\u75c5\u7406\u5b66\uff09\u7684\u6709\u6548\u6027\uff1a \u5f3a\u8c03\u4e86\u5229\u7528\u9886\u57df\u7279\u5b9a\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\uff08\u5982Uni\uff09\u4f5c\u4e3a\u9aa8\u5e72\u7f51\u7edc\u7684\u91cd\u8981\u6027\uff0c\u8fd9\u5bf9\u4e8e\u5c06\u901a\u7528AI\u80fd\u529b\u9002\u914d\u5230\u4e13\u4e1a\u9886\u57df\u5177\u6709\u6307\u5bfc\u610f\u4e49\u3002 4. \u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 (Related Areas or Applications) \u6570\u5b57\u75c5\u7406\u5b66 (Digital Pathology)\uff1a \u80bf\u7624\u68c0\u6d4b\u3001\u7ec4\u7ec7\u5206\u578b\u3001\u75c5\u7406\u5206\u7ea7\u3001\u7ec6\u80de\u6838/\u817a\u4f53\u5206\u5272\u3001\u5b9a\u91cf\u5206\u6790\u7b49\u3002 \u533b\u5b66\u56fe\u50cf\u5206\u6790 (Medical Image Analysis)\uff1a \u4efb\u4f55\u9700\u8981\u9ad8\u7cbe\u5ea6\u5206\u5272\u4f46\u6807\u6ce8\u6210\u672c\u9ad8\u6602\u7684\u533b\u5b66\u56fe\u50cf\u4efb\u52a1\uff0c\u4f8b\u5982\u653e\u5c04\u5b66\u56fe\u50cf\uff08CT/MRI\uff09\u4e2d\u7684\u5668\u5b98\u6216\u75c5\u7076\u5206\u5272\u3001\u663e\u5fae\u955c\u56fe\u50cf\u5206\u6790\u7b49\u3002 \u5f31\u76d1\u7763/\u534a\u76d1\u7763\u5b66\u4e60 (Weakly/Semi-supervised Learning)\uff1a \u4e3a\u8fd9\u4e9b\u5b66\u4e60\u8303\u5f0f\u63d0\u4f9b\u4e86\u65b0\u7684\u591a\u6a21\u6001\u878d\u5408\u7b56\u7565\u3002 \u591a\u6a21\u6001\u4eba\u5de5\u667a\u80fd (Multimodal AI)\uff1a \u63a2\u7d22\u56fe\u50cf\u4e0e\u6587\u672c\u7b49\u4e0d\u540c\u6a21\u6001\u4fe1\u606f\u878d\u5408\u7684\u901a\u7528\u65b9\u6cd5\u3002 \u57fa\u7840\u6a21\u578b\u5728\u5782\u76f4\u9886\u57df\u7684\u5e94\u7528 (Foundation Models in Vertical Domains)\uff1a \u5982\u4f55\u6709\u6548\u5229\u7528\u548c\u9002\u914d\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\u5230\u7279\u5b9a\u4e13\u4e1a\u9886\u57df\u3002 5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u7684\u5c40\u9650\u6027 (Inferred Limitations from the Abstract) \u6587\u672c\u539f\u578b\u8d28\u91cf\u548c\u751f\u6210\uff1a \u6458\u8981\u4e2d\u672a\u8be6\u7ec6\u8bf4\u660e\u6587\u672c\u539f\u578b\u662f\u5982\u4f55\u751f\u6210\u6216\u83b7\u53d6\u7684\u3002\u6587\u672c\u539f\u578b\u7684\u8d28\u91cf\u3001\u7279\u5f02\u6027\u548c\u8986\u76d6\u8303\u56f4\u53ef\u80fd\u76f4\u63a5\u5f71\u54cd\u8bed\u4e49\u76d1\u7763\u7684\u6548\u679c\u3002\u5982\u679c\u6587\u672c\u63cf\u8ff0\u4e0d\u51c6\u786e\u6216\u4e0d\u5168\u9762\uff0c\u53ef\u80fd\u4f1a\u5f15\u5165\u566a\u58f0\u6216\u504f\u5dee\u3002 \u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\uff1a \u53cc\u91cd\u5bf9\u6bd4\u5b66\u4e60\u548c\u4f7f\u7528\u5927\u578b\u75c5\u7406\u5b66\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\uff08Uni\uff09\u4f5c\u4e3a\u9aa8\u5e72\u7f51\u7edc\uff0c\u53ef\u80fd\u9700\u8981\u8f83\u9ad8\u7684\u8ba1\u7b97\u8d44\u6e90\uff08GPU\u5185\u5b58\u548c\u8ba1\u7b97\u80fd\u529b\uff09\uff0c\u8fd9\u53ef\u80fd\u9650\u5236\u5176\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u5e94\u7528\u3002 \u5bf9\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u4f9d\u8d56\uff1a \u8be5\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u4e00\u4e2a\u201c\u75c5\u7406\u5b66\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\uff08Uni\uff09\u201d\u3002\u5982\u679c\u7279\u5b9a\u75c5\u7406\u4efb\u52a1\u6216\u6570\u636e\u96c6\u6ca1\u6709\u5408\u9002\u7684\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u6216\u8005\u8be5\u6a21\u578b\u672c\u8eab\u5b58\u5728\u5c40\u9650\u6027\uff0c\u53ef\u80fd\u4f1a\u5f71\u54cdMPAMatch\u7684\u6027\u80fd\u3002 \u6cdb\u5316\u80fd\u529b\uff1a \u5c3d\u7ba1\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86SOTA\u7ed3\u679c\uff0c\u4f46\u6587\u672c\u539f\u578b\u548c\u56fe\u50cf\u539f\u578b\u5bf9\u9f50\u7684\u673a\u5236\u5728\u9762\u5bf9\u5168\u65b0\u7684\u3001\u672a\u89c1\u8fc7\u7684\u75c5\u7406\u7c7b\u578b\u6216\u5177\u6709\u663e\u8457\u9886\u57df\u5dee\u5f02\u7684\u6570\u636e\u65f6\uff0c\u5176\u6cdb\u5316\u80fd\u529b\u4ecd\u9700\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u3002 \u53ef\u89e3\u91ca\u6027\uff1a \u591a\u6a21\u6001\u878d\u5408\u548c\u5bf9\u6bd4\u5b66\u4e60\u7684\u590d\u6742\u6027\u53ef\u80fd\u4f7f\u5f97\u6a21\u578b\u7684\u51b3\u7b56\u8fc7\u7a0b\u76f8\u5bf9\u4e0d\u900f\u660e\uff0c\u5bf9\u4e8e\u4e34\u5e8a\u5e94\u7528\u800c\u8a00\uff0c\u63d0\u9ad8\u53ef\u89e3\u91ca\u6027\u53ef\u80fd\u662f\u4e00\u4e2a\u6311\u6218\u3002 Key Findings: To address these limitations, we propose MPAMatch - a novel segmentation framework that performs pixel-level contrastive learning under a multimodal prototype-guided supervision paradigm. Extensive experiments on GLAS, EBHI-SEG-GLAND, EBHI-SEG-CANCER, and KPI show MPAMatch's superiority over state-of-the-art methods, validating its dual advantages in structural and semantic modeling. Links: PDF arXiv Scalable Object Detection in the Car Interior With Vision Foundation Models Authors: B\u00e1lint M\u00e9sz\u00e1ros, Ahmet Firintepe, Sebastian Schmidt, Stephan G\u00fcnnemann Published: 2025-08-27 Categories: cs.CV Abstract: AI tasks in the car interior like identifying and localizing externally introduced objects is crucial for response quality of personal assistants. However, computational resources of on-board systems remain highly constrained, restricting the deployment of such solutions directly within the vehicle. To address this limitation, we propose the novel Object Detection and Localization (ODAL) framework for interior scene understanding. Our approach leverages vision foundation models through a distributed architecture, splitting computational tasks between on-board and cloud. This design overcomes the resource constraints of running foundation models directly in the car. To benchmark model performance, we introduce ODALbench, a new metric for comprehensive assessment of detection and localization.Our analysis demonstrates the framework's potential to establish new standards in this domain. We compare the state-of-the-art GPT-4o vision foundation model with the lightweight LLaVA 1.5 7B model and explore how fine-tuning enhances the lightweight models performance. Remarkably, our fine-tuned ODAL-LLaVA model achieves an ODAL _{score} of 89%, representing a 71% improvement over its baseline performance and outperforming GPT-4o by nearly 20%. Furthermore, the fine-tuned model maintains high detection accuracy while significantly reducing hallucinations, achieving an ODAL _{SNR} three times higher than GPT-4o. Analysis: \u8fd9\u7bc7\u8bba\u6587\u6458\u8981\u5c55\u793a\u4e86\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u5728\u89e3\u51b3\u5b9e\u9645\u5de5\u7a0b\u6311\u6218\u65b9\u9762\u7684\u521b\u65b0\u65b9\u6cd5\u3002\u4ee5\u4e0b\u662f\u6839\u636e\u6458\u8981\u8fdb\u884c\u7684\u5206\u6790\uff1a 1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u660e\u6458\u8981 (Concise Summary) \u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aODAL\u7684\u5206\u5e03\u5f0f\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u8f66\u8f7d\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u4e0b\uff0c\u5229\u7528\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u5b9e\u73b0\u8f66\u5185\u7269\u4f53\u7684\u53ef\u6269\u5c55\u68c0\u6d4b\u4e0e\u5b9a\u4f4d\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u5c06\u8ba1\u7b97\u4efb\u52a1\u5206\u914d\u5230\u8f66\u8f7d\u548c\u4e91\u7aef\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u76f4\u63a5\u90e8\u7f72\u57fa\u7840\u6a21\u578b\u7684\u8d44\u6e90\u74f6\u9888\uff0c\u5e76\u901a\u8fc7\u5f15\u5165\u65b0\u5ea6\u91cfODALbench\uff0c\u5c55\u793a\u4e86\u5176\u5fae\u8c03\u7684\u8f7b\u91cf\u7ea7\u6a21\u578b\uff08ODAL-LLaVA\uff09\u5728\u6027\u80fd\u548c\u5e7b\u89c9\u6291\u5236\u65b9\u9762\u5747\u8d85\u8d8a\u4e86GPT-4o\u3002 2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u8bba (Key Innovation or Methodological Approach) \u5206\u5e03\u5f0f\u67b6\u6784\u5229\u7528\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff1a \u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u5176 \u5206\u5e03\u5f0f\u67b6\u6784 \uff0c\u5c06\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u8ba1\u7b97\u4efb\u52a1\u667a\u80fd\u5730\u5206\u914d\u5230\u8f66\u8f7d\u7cfb\u7edf\u548c\u4e91\u7aef\uff0c\u4ece\u800c\u89c4\u907f\u4e86\u8f66\u8f7d\u786c\u4ef6\u7684\u8d44\u6e90\u9650\u5236\u3002\u8fd9\u79cd\u6df7\u5408\u90e8\u7f72\u7b56\u7565\u662f\u89e3\u51b3\u8fb9\u7f18\u8bbe\u5907\u4e0a\u8fd0\u884c\u5927\u578bAI\u6a21\u578b\u6311\u6218\u7684\u5b9e\u7528\u65b9\u6848\u3002 \u8f7b\u91cf\u7ea7\u6a21\u578b\u5fae\u8c03\u8d85\u8d8aSOTA\u901a\u7528\u6a21\u578b\uff1a \u8bba\u6587\u5c55\u793a\u4e86\u901a\u8fc7 \u5bf9\u8f7b\u91cf\u7ea7\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08\u5982LLaVA\uff09\u8fdb\u884c\u9886\u57df\u7279\u5b9a\u5fae\u8c03 \uff0c\u53ef\u4ee5\u5728\u7279\u5b9a\u4efb\u52a1\uff08\u8f66\u5185\u7269\u4f53\u68c0\u6d4b\u4e0e\u5b9a\u4f4d\uff09\u4e0a\u663e\u8457\u8d85\u8d8a\u66f4\u5927\u578b\u3001\u901a\u7528\u6027\u66f4\u5f3a\u7684SOTA\u6a21\u578b\uff08\u5982GPT-4o\uff09\uff0c\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u5e7b\u89c9\uff08ODAL _{SNR} \u662fGPT-4o\u7684\u4e09\u500d\uff09\u3002\u8fd9\u5f3a\u8c03\u4e86\u9886\u57df\u9002\u5e94\u6027\u548c\u6a21\u578b\u6548\u7387\u7684\u91cd\u8981\u6027\u3002 \u5f15\u5165\u65b0\u7684\u8bc4\u4f30\u6307\u6807ODALbench\uff1a \u63d0\u51fa\u4e86 ODALbench \u8fd9\u4e00\u65b0\u7684\u7efc\u5408\u8bc4\u4f30\u6307\u6807\uff0c\u7528\u4e8e\u5168\u9762\u8bc4\u4f30\u68c0\u6d4b\u548c\u5b9a\u4f4d\u6027\u80fd\uff0c\u4e3a\u8be5\u7279\u5b9a\u9886\u57df\u63d0\u4f9b\u4e86\u66f4\u7cbe\u786e\u548c\u5168\u9762\u7684\u8861\u91cf\u6807\u51c6\u3002 3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd (Potential Impact on the Field) \u63a8\u52a8\u8fb9\u7f18AI\u548c\u8f66\u8f7dAI\u7684\u53d1\u5c55\uff1a \u672c\u7814\u7a76\u4e3a\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\uff08\u5982\u8f66\u8f7d\u7cfb\u7edf\uff09\u4e0a\u90e8\u7f72\u548c\u5229\u7528\u5f3a\u5927\u7684\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u6761 \u5207\u5b9e\u53ef\u884c\u7684\u8def\u5f84 \uff0c\u514b\u670d\u4e86\u5f53\u524d\u57fa\u7840\u6a21\u578b\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u7684\u74f6\u9888\u3002\u5b83\u6709\u671b \u63a8\u52a8\u8f66\u8f7dAI\u4efb\u52a1\uff08\u5982\u667a\u80fd\u5ea7\u8231\u3001\u4e58\u5ba2\u76d1\u63a7\u3001\u4e2a\u6027\u5316\u52a9\u624b\uff09\u7684\u667a\u80fd\u5316\u6c34\u5e73 \uff0c\u4f7f\u5176\u80fd\u591f\u66f4\u51c6\u786e\u3001\u66f4\u53ef\u9760\u5730\u7406\u89e3\u8f66\u5185\u73af\u5883\u3002 \u91cd\u65b0\u5b9a\u4e49\u6a21\u578b\u9009\u62e9\u548c\u4f18\u5316\u7b56\u7565\uff1a \u8bba\u6587\u6709\u529b\u5730\u8bc1\u660e\u4e86\uff0c\u5728\u7279\u5b9a\u5e94\u7528\u573a\u666f\u4e0b\uff0c\u901a\u8fc7\u5de7\u5999\u7684\u67b6\u6784\u8bbe\u8ba1\u548c\u9886\u57df\u7279\u5b9a\u5fae\u8c03\uff0c\u8f7b\u91cf\u7ea7\u6a21\u578b\u4e0d\u4ec5\u53ef\u4ee5\u8fbe\u5230\u751a\u81f3\u8d85\u8d8a\u5927\u578b\u901a\u7528\u6a21\u578b\u7684\u6027\u80fd\uff0c\u800c\u4e14\u5728\u8d44\u6e90\u6548\u7387\u548c\u53ef\u9760\u6027\uff08\u51cf\u5c11\u5e7b\u89c9\uff09\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002\u8fd9\u53ef\u80fd\u542f\u53d1\u7814\u7a76\u8005\u548c\u5de5\u7a0b\u5e08\u91cd\u65b0\u601d\u8003\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72AI\u65f6\u7684\u6a21\u578b\u9009\u62e9\u548c\u4f18\u5316\u7b56\u7565\u3002 \u5206\u5e03\u5f0f\u8ba1\u7b97\u8303\u5f0f\u7684\u63a2\u7d22\uff1a \u8be5\u5de5\u4f5c\u4e5f\u4e3a \u8fb9\u7f18AI\u7684\u5206\u5e03\u5f0f\u8ba1\u7b97\u8303\u5f0f \u63d0\u4f9b\u4e86\u6709\u76ca\u7684\u63a2\u7d22\uff0c\u53ef\u80fd\u542f\u53d1\u5176\u4ed6\u7c7b\u4f3c\u573a\u666f\u7684\u5e94\u7528\uff0c\u5176\u4e2d\u8ba1\u7b97\u5bc6\u96c6\u578b\u4efb\u52a1\u9700\u8981\u5728\u672c\u5730\u54cd\u5e94\u6027\u548c\u4e91\u7aef\u5f3a\u5927\u80fd\u529b\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002 4. \u53ef\u80fd\u53d7\u76ca\u7684\u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 (Related Areas or Applications) \u667a\u80fd\u9a7e\u9a76\u4e0e\u8f66\u8f7d\u7cfb\u7edf\uff1a \u76f4\u63a5\u5e94\u7528\u4e8e\u667a\u80fd\u5ea7\u8231\u3001\u4e58\u5ba2\u884c\u4e3a\u5206\u6790\u3001\u9057\u7559\u7269\u54c1\u68c0\u6d4b\u3001\u8f66\u5185\u5b89\u5168\u76d1\u63a7\u3001\u4e2a\u6027\u5316\u670d\u52a1\uff08\u5982\u6839\u636e\u8f66\u5185\u7269\u54c1\u8c03\u6574\u73af\u5883\uff09\u7b49\u3002 \u8fb9\u7f18\u8ba1\u7b97\u4e0e\u7269\u8054\u7f51\uff08IoT\uff09\uff1a \u51e1\u662f\u9700\u8981\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u8fd0\u884c\u590d\u6742AI\u6a21\u578b\uff0c\u5e76\u9700\u8981\u4e0e\u4e91\u7aef\u534f\u540c\u7684\u573a\u666f\uff0c\u5982\u667a\u80fd\u5bb6\u5c45\u3001\u5de5\u4e1a\u81ea\u52a8\u5316\u3001\u673a\u5668\u4eba\u89c6\u89c9\u3001\u667a\u80fd\u96f6\u552e\u7b49\u3002 \u4eba\u673a\u4ea4\u4e92\uff08HCI\uff09\uff1a \u63d0\u5347\u8f66\u8f7d\u4e2a\u4eba\u52a9\u624b\u7684\u73af\u5883\u611f\u77e5\u80fd\u529b\u548c\u54cd\u5e94\u8d28\u91cf\uff0c\u4f7f\u5176\u80fd\u66f4\u667a\u80fd\u5730\u7406\u89e3\u7528\u6237\u610f\u56fe\u548c\u73af\u5883\u4e0a\u4e0b\u6587\u3002 \u5b89\u5168\u4e0e\u5b89\u9632\uff1a \u8bc6\u522b\u8f66\u5185\u6f5c\u5728\u5371\u9669\u7269\u54c1\u6216\u5f02\u5e38\u60c5\u51b5\uff0c\u4f8b\u5982\u68c0\u6d4b\u6613\u71c3\u7269\u3001\u6b66\u5668\u6216\u88ab\u9057\u5f03\u7684\u513f\u7ae5/\u5ba0\u7269\u3002 5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u7684\u5c40\u9650\u6027 (Limitations Inferred from the Abstract) \u5bf9\u4e91\u7aef\u8fde\u63a5\u7684\u4f9d\u8d56\uff1a \u5206\u5e03\u5f0f\u67b6\u6784\u610f\u5473\u7740\u5728\u7f51\u7edc\u8fde\u63a5\u4e0d\u7a33\u5b9a\u6216\u65e0\u7f51\u7edc\u7684\u73af\u5883\u4e0b\uff0c\u7cfb\u7edf\u7684\u6027\u80fd\u53ef\u80fd\u4f1a\u53d7\u5230\u4e25\u91cd\u5f71\u54cd\uff0c\u751a\u81f3\u65e0\u6cd5\u5de5\u4f5c\u3002\u540c\u65f6\uff0c\u5c06\u8f66\u5185\u6570\u636e\u4f20\u8f93\u81f3\u4e91\u7aef\u53ef\u80fd\u5f15\u53d1 \u6570\u636e\u9690\u79c1\u548c\u5b89\u5168 \u65b9\u9762\u7684\u62c5\u5fe7\uff0c\u5c24\u5176\u662f\u5728\u6d89\u53ca\u4e2a\u4eba\u6216\u654f\u611f\u4fe1\u606f\u65f6\u3002 \u7279\u5b9a\u4efb\u52a1\u7684\u6cdb\u5316\u80fd\u529b\uff1a \u5c3d\u7ba1\u5fae\u8c03\u540e\u7684ODAL-LLaVA\u5728\u201c\u5916\u90e8\u5f15\u5165\u7269\u4f53\u201d\u7684\u68c0\u6d4b\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5728\u66f4\u5e7f\u6cdb\u7684\u8f66\u5185\u573a\u666f\uff08\u5982\u4e0d\u540c\u8f66\u578b\u3001\u5149\u7167\u6761\u4ef6\u3001\u4e58\u5ba2\u884c\u4e3a\u5206\u6790\u7b49\uff09\u6216\u8bc6\u522b\u5176\u4ed6\u7c7b\u578b\u7269\u4f53\u65f6\u7684 \u6cdb\u5316\u80fd\u529b \u5c1a\u4e0d\u660e\u786e\u3002 \u8f66\u8f7d\u7aef\u8ba1\u7b97\u8d1f\u8377\u7684\u8be6\u7ec6\u7a0b\u5ea6\uff1a \u6458\u8981\u4e2d\u63d0\u5230\u8f66\u8f7d\u7aef\u8d44\u6e90\u53d7\u9650\uff0c\u4f46\u672a\u8be6\u7ec6\u8bf4\u660e\u8f66\u8f7d\u7aef\u5177\u4f53\u627f\u62c5\u7684\u8ba1\u7b97\u4efb\u52a1\u53ca\u5176\u6240\u9700\u7684\u6700\u5c0f\u8d44\u6e90\uff0c\u8fd9\u53ef\u80fd\u5f71\u54cd\u5176\u5728\u6781\u5ea6\u53d7\u9650\u7cfb\u7edf\u4e0a\u7684\u90e8\u7f72\u3002 ODALbench\u7684\u666e\u9002\u6027\uff1a \u4f5c\u4e3a\u4e00\u4e2a\u65b0\u63d0\u51fa\u7684\u5ea6\u91cf\u6807\u51c6\uff0c\u5176\u5728\u884c\u4e1a\u5185\u7684\u63a5\u53d7\u5ea6\u3001\u4e0e\u5176\u4ed6\u73b0\u6709\u5ea6\u91cf\u7684\u517c\u5bb9\u6027\u4ee5\u53ca\u5728\u66f4\u5e7f\u6cdb\u573a\u666f\u4e0b\u7684\u6709\u6548\u6027\u4ecd\u9700\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u3002 \u5b9e\u65f6\u6027\u8981\u6c42\uff1a \u5206\u5e03\u5f0f\u67b6\u6784\u5f15\u5165\u4e86\u7f51\u7edc\u5ef6\u8fdf\uff0c\u5bf9\u4e8e\u67d0\u4e9b\u9700\u8981\u6781\u4f4e\u5ef6\u8fdf\u7684\u5b9e\u65f6\u5e94\u7528\uff08\u4f8b\u5982\u5b89\u5168\u5173\u952e\u578b\u4efb\u52a1\uff09\uff0c\u8fd9\u79cd\u5ef6\u8fdf\u53ef\u80fd\u662f\u4e00\u4e2a\u6311\u6218\u3002\u6458\u8981\u4e2d\u672a\u63d0\u53ca\u5177\u4f53\u7684\u5ef6\u8fdf\u6307\u6807\u3002 Key Findings: To address this limitation, we propose the novel Object Detection and Localization (ODAL) framework for interior scene understanding. Our approach leverages vision foundation models through a distributed architecture, splitting computational tasks between on-board and cloud. To benchmark model performance, we introduce ODALbench, a new metric for comprehensive assessment of detection and localization.Our analysis demonstrates the framework's potential to establish new standards in this domain. We compare the state-of-the-art GPT-4o vision foundation model with the lightweight LLaVA 1.5 7B model and explore how fine-tuning enhances the lightweight models performance. Links: PDF arXiv Generalizing Monocular 3D Object Detection Authors: Abhinav Kumar Published: 2025-08-27 Categories: cs.CV Abstract: Monocular 3D object detection (Mono3D) is a fundamental computer vision task that estimates an object's class, 3D position, dimensions, and orientation from a single image. Its applications, including autonomous driving, augmented reality, and robotics, critically rely on accurate 3D environmental understanding. This thesis addresses the challenge of generalizing Mono3D models to diverse scenarios, including occlusions, datasets, object sizes, and camera parameters. To enhance occlusion robustness, we propose a mathematically differentiable NMS (GrooMeD-NMS). To improve generalization to new datasets, we explore depth equivariant (DEVIANT) backbones. We address the issue of large object detection, demonstrating that it's not solely a data imbalance or receptive field problem but also a noise sensitivity issue. To mitigate this, we introduce a segmentation-based approach in bird's-eye view with dice loss (SeaBird). Finally, we mathematically analyze the extrapolation of Mono3D models to unseen camera heights and improve Mono3D generalization in such out-of-distribution settings. Analysis: \u8fd9\u7bc7\u8bba\u6587\u7684\u6458\u8981\u5c55\u793a\u4e86\u5728\u5355\u76ee3D\u76ee\u6807\u68c0\u6d4b\uff08Mono3D\uff09\u9886\u57df\u8fdb\u884c\u6df1\u5165\u4e14\u591a\u7ef4\u5ea6\u6cdb\u5316\u7814\u7a76\u7684\u52aa\u529b\uff0c\u5177\u6709\u663e\u8457\u7684\u6280\u672f\u8da3\u5473\u6027\u548c\u6f5c\u5728\u91cd\u8981\u6027\u3002 \u4ee5\u4e0b\u662f\u6839\u636e\u6458\u8981\u8fdb\u884c\u7684\u5206\u6790\uff1a \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u6d01\u603b\u7ed3 (Concise Summary of Main Contribution) \u8fd9\u7bc7\u8bba\u6587\u4e13\u6ce8\u4e8e\u63d0\u5347\u5355\u76ee3D\u76ee\u6807\u68c0\u6d4b\uff08Mono3D\uff09\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002\u5b83\u901a\u8fc7\u63d0\u51fa\u4e00\u7cfb\u5217\u521b\u65b0\u65b9\u6cd5\uff0c\u5206\u522b\u89e3\u51b3\u4e86\u6a21\u578b\u5728\u5904\u7406\u906e\u6321\u3001\u65b0\u6570\u636e\u96c6\u3001\u4e0d\u540c\u7269\u4f53\u5c3a\u5bf8\uff08\u7279\u522b\u662f\u5927\u578b\u7269\u4f53\uff09\u4ee5\u53ca\u672a\u77e5\u76f8\u673a\u53c2\u6570\u7b49\u591a\u6837\u5316\u573a\u666f\u65f6\u7684\u6311\u6218\uff0c\u65e8\u5728\u63d0\u9ad8Mono3D\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u9002\u7528\u6027\u3002 \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u8bba (Key Innovation or Methodological Approach) \u8bba\u6587\u7684\u5173\u952e\u521b\u65b0\u5728\u4e8e\u5176\u9488\u5bf9Mono3D\u6cdb\u5316\u6311\u6218\u7684 \u591a\u7ef4\u5ea6\u3001\u7cfb\u7edf\u6027\u89e3\u51b3\u65b9\u6848 \u3002\u5177\u4f53\u5305\u62ec\uff1a GrooMeD-NMS \uff1a\u63d0\u51fa\u4e00\u79cd\u6570\u5b66\u4e0a\u53ef\u5fae\u5206\u7684\u975e\u6781\u5927\u503c\u6291\u5236\uff08NMS\uff09\u65b9\u6cd5\uff0c\u4ee5\u589e\u5f3a\u6a21\u578b\u5bf9\u906e\u6321\u7684\u9c81\u68d2\u6027\uff0c\u8fd9\u5728\u4f20\u7edfNMS\u7684\u79bb\u6563\u6027\u9650\u5236\u4e0b\u662f\u4e00\u4e2a\u663e\u8457\u7a81\u7834\u3002 DEVIANT backbones \uff1a\u63a2\u7d22\u6df1\u5ea6\u7b49\u53d8\uff08depth equivariant\uff09\u9aa8\u5e72\u7f51\u7edc\uff0c\u4ee5\u63d0\u9ad8\u6a21\u578b\u5728\u65b0\u6570\u636e\u96c6\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5229\u7528\u4e86\u7b49\u53d8\u6027\u8fd9\u4e00\u5f3a\u5927\u7684\u5f52\u7eb3\u504f\u7f6e\u3002 SeaBird \uff1a\u9488\u5bf9\u5927\u578b\u7269\u4f53\u68c0\u6d4b\u4e2d\u7684\u566a\u58f0\u654f\u611f\u6027\u95ee\u9898\uff0c\u5f15\u5165\u4e00\u79cd\u57fa\u4e8e\u9e1f\u77b0\u56fe\uff08BEV\uff09\u7684\u5206\u5272\u65b9\u6cd5\uff0c\u5e76\u7ed3\u5408Dice\u635f\u5931\uff0c\u8fd9\u6539\u53d8\u4e86\u4f20\u7edf\u8fb9\u754c\u6846\u56de\u5f52\u7684\u8303\u5f0f\u3002 \u6570\u5b66\u5206\u6790 \uff1a\u5bf9Mono3D\u6a21\u578b\u5728\u672a\u89c1\u76f8\u673a\u9ad8\u5ea6\u4e0b\u7684\u5916\u63a8\u80fd\u529b\u8fdb\u884c\u6570\u5b66\u5206\u6790\uff0c\u5e76\u636e\u6b64\u6539\u8fdb\u4e86\u6a21\u578b\u5728\u5206\u5e03\u5916\uff08OOD\uff09\u8bbe\u7f6e\u4e0b\u7684\u6cdb\u5316\u6027\u80fd\u3002 \u5bf9\u9886\u57df\u7684\u6f5c\u5728\u5f71\u54cd (Potential Impact on the Field) \u8fd9\u9879\u7814\u7a76\u7684\u6f5c\u5728\u5f71\u54cd\u662f\u5de8\u5927\u7684\u3002\u901a\u8fc7\u663e\u8457\u63d0\u5347Mono3D\u6a21\u578b\u5728\u590d\u6742\u591a\u53d8\u73af\u5883\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\uff0c\u5b83\u5c06\u76f4\u63a5\u63a8\u52a8\u5355\u76ee3D\u611f\u77e5\u6280\u672f\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u90e8\u7f72\u548c\u53ef\u9760\u6027\u3002\u4f8b\u5982\uff0c\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\uff0c\u66f4\u51c6\u786e\u3001\u66f4\u5c11\u53d7\u906e\u6321\u548c\u73af\u5883\u53d8\u5316\u5f71\u54cd\u76843D\u611f\u77e5\u80fd\u63d0\u9ad8\u51b3\u7b56\u5b89\u5168\u6027\uff1b\u5728\u589e\u5f3a\u73b0\u5b9e\u548c\u673a\u5668\u4eba\u9886\u57df\uff0c\u5bf9\u7269\u4f533D\u59ff\u6001\u7684\u7cbe\u786e\u7406\u89e3\u662f\u5b9e\u73b0\u65e0\u7f1d\u4ea4\u4e92\u548c\u81ea\u4e3b\u64cd\u4f5c\u7684\u57fa\u7840\u3002\u8fd9\u6709\u52a9\u4e8e\u5c06Mono3D\u4ece\u5b9e\u9a8c\u5ba4\u63a8\u5411\u66f4\u5e7f\u9614\u7684\u5de5\u4e1a\u548c\u6d88\u8d39\u7ea7\u5e94\u7528\u3002 \u53ef\u80fd\u53d7\u76ca\u7684\u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 (Related Areas or Applications that Might Benefit) \u81ea\u52a8\u9a7e\u9a76 (Autonomous Driving) \uff1a\u5bf9\u8f66\u8f86\u3001\u884c\u4eba\u3001\u9a91\u884c\u8005\u7b49\u969c\u788d\u7269\u7684\u7cbe\u786e3D\u611f\u77e5\u662f\u5b89\u5168\u5bfc\u822a\u548c\u8def\u5f84\u89c4\u5212\u7684\u6838\u5fc3\u3002 \u589e\u5f3a\u73b0\u5b9e (Augmented Reality, AR) \uff1a\u9700\u8981\u51c6\u786e\u4f30\u8ba1\u771f\u5b9e\u4e16\u754c\u7269\u4f53\u76843D\u4f4d\u7f6e\u548c\u5c3a\u5bf8\uff0c\u4ee5\u4fbf\u5c06\u865a\u62df\u5185\u5bb9\u65e0\u7f1d\u53e0\u52a0\u3002 \u673a\u5668\u4eba\u5b66 (Robotics) \uff1a\u673a\u5668\u4eba\u9700\u8981\u7406\u89e3\u5176\u64cd\u4f5c\u73af\u5883\u4e2d\u7684\u7269\u4f533D\u4fe1\u606f\uff0c\u4ee5\u8fdb\u884c\u6293\u53d6\u3001\u5bfc\u822a\u548c\u4eba\u673a\u4ea4\u4e92\u3002 3D\u573a\u666f\u7406\u89e3 (3D Scene Understanding) \uff1a\u66f4\u5e7f\u6cdb\u5730\uff0c\u4efb\u4f55\u9700\u8981\u4ece\u5355\u76ee\u56fe\u50cf\u91cd\u5efa\u6216\u7406\u89e33D\u573a\u666f\u7684\u5e94\u7528\u90fd\u4f1a\u53d7\u76ca\u3002 \u9886\u57df\u9002\u5e94\u4e0e\u6cdb\u5316 (Domain Adaptation and Generalization) \uff1a\u8bba\u6587\u4e2d\u89e3\u51b3\u65b0\u6570\u636e\u96c6\u548cOOD\u76f8\u673a\u53c2\u6570\u7684\u95ee\u9898\uff0c\u4e0e\u8fd9\u4e9b\u7814\u7a76\u9886\u57df\u7d27\u5bc6\u76f8\u5173\u3002 \u9c81\u68d2\u89c6\u89c9\u7cfb\u7edf (Robust Vision Systems) \uff1a\u63d0\u5347\u6a21\u578b\u5728\u6076\u52a3\u6761\u4ef6\uff08\u5982\u906e\u6321\u3001\u566a\u58f0\uff09\u4e0b\u7684\u6027\u80fd\uff0c\u662f\u6784\u5efa\u53ef\u9760\u89c6\u89c9\u7cfb\u7edf\u7684\u5173\u952e\u3002 \u53ef\u4ece\u6458\u8981\u4e2d\u63a8\u65ad\u51fa\u7684\u5c40\u9650\u6027 (Limitations that Can Be Inferred from the Abstract) \u672a\u63d0\u53ca\u7684\u6cdb\u5316\u7ef4\u5ea6 \uff1a\u5c3d\u7ba1\u8bba\u6587\u89e3\u51b3\u4e86\u591a\u4e2a\u5173\u952e\u7684\u6cdb\u5316\u6311\u6218\uff0c\u4f46\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u6cdb\u5316\u95ee\u9898\u8fdc\u4e0d\u6b62\u8fd9\u4e9b\u3002\u4f8b\u5982\uff0c\u6a21\u578b\u5728\u6781\u7aef\u5929\u6c14\u6761\u4ef6\uff08\u96e8\u3001\u96ea\u3001\u96fe\uff09\u3001\u4e0d\u540c\u5149\u7167\u53d8\u5316\u3001\u4f20\u611f\u5668\u566a\u58f0\u6216\u4e0d\u540c\u7eb9\u7406\u73af\u5883\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u672a\u5728\u6458\u8981\u4e2d\u63d0\u53ca\u3002 \u8ba1\u7b97\u6548\u7387\u4e0e\u5b9e\u65f6\u6027 \uff1a\u6458\u8981\u4e2d\u63d0\u51fa\u7684\u591a\u79cd\u65b0\u65b9\u6cd5\uff08\u5982\u53ef\u5fae\u5206NMS\u3001\u6df1\u5ea6\u7b49\u53d8\u9aa8\u5e72\u7f51\u7edc\u3001BEV\u5206\u5272\uff09\u53ef\u80fd\u5f15\u5165\u989d\u5916\u7684\u8ba1\u7b97\u5f00\u9500\u3002\u8bba\u6587\u672a\u8bf4\u660e\u8fd9\u4e9b\u65b9\u6cd5\u5bf9\u6a21\u578b\u63a8\u7406\u901f\u5ea6\u548c\u5b9e\u65f6\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u8fd9\u5bf9\u4e8e\u81ea\u52a8\u9a7e\u9a76\u7b49\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002 \u5355\u76ee\u89c6\u89c9\u7684\u56fa\u6709\u5c40\u9650\u6027 \uff1a\u5c3d\u7ba1\u8bba\u6587\u81f4\u529b\u4e8e\u63d0\u5347Mono3D\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u5355\u76ee\u56fe\u50cf\u56fa\u6709\u7684\u6df1\u5ea6\u6a21\u7cca\u6027\u4ecd\u7136\u662f\u5176\u6839\u672c\u9650\u5236\u3002\u8fd9\u4e9b\u65b9\u6cd5\u662f\u5728\u5355\u76ee\u8303\u5f0f\u4e0b\u8fdb\u884c\u4f18\u5316\uff0c\u800c\u975e\u4ece\u6839\u672c\u4e0a\u6539\u53d8\u5176\u8f93\u5165\u4fe1\u606f\u6e90\u3002 \u7279\u5b9a\u573a\u666f\u7684\u9002\u7528\u6027 \uff1a\u4f8b\u5982\uff0cSeaBird\u65b9\u6cd5\u4e13\u95e8\u9488\u5bf9\u201c\u5927\u578b\u7269\u4f53\u201d\u548c\u201c\u566a\u58f0\u654f\u611f\u6027\u201d\u95ee\u9898\u3002\u5bf9\u4e8e\u5c0f\u578b\u7269\u4f53\u6216\u4e0d\u540c\u7c7b\u578b\u7684\u68c0\u6d4b\u6311\u6218\uff0c\u53ef\u80fd\u9700\u8981\u5176\u4ed6\u4e13\u95e8\u7684\u89e3\u51b3\u65b9\u6848\u3002 \u6570\u636e\u96c6\u4f9d\u8d56\u6027 \uff1a\u867d\u7136\u63d0\u51fa\u4e86DEVIANT\u6765\u6539\u5584\u5bf9\u65b0\u6570\u636e\u96c6\u7684\u6cdb\u5316\uff0c\u4f46\u5176\u8bad\u7ec3\u548c\u9a8c\u8bc1\u662f\u5426\u4ecd\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u4ee5\u53ca\u5728\u5b8c\u5168\u65e0\u76d1\u7763\u6216\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\u7684\u8868\u73b0\u5982\u4f55\uff0c\u6458\u8981\u4e2d\u672a\u8be6\u7ec6\u8bf4\u660e\u3002 Key Findings: To enhance occlusion robustness, we propose a mathematically differentiable NMS (GrooMeD-NMS). To improve generalization to new datasets, we explore depth equivariant (DEVIANT) backbones. To mitigate this, we introduce a segmentation-based approach in bird's-eye view with dice loss (SeaBird). Links: PDF arXiv DATR: Diffusion-based 3D Apple Tree Reconstruction Framework with Sparse-View Authors: Tian Qiu, Alan Zoubi, Yiyuan Lin, Ruiming Du, Lailiang Cheng, Yu Jiang Published: 2025-08-27 Categories: cs.RO, cs.CV Abstract: Digital twin applications offered transformative potential by enabling real-time monitoring and robotic simulation through accurate virtual replicas of physical assets. The key to these systems is 3D reconstruction with high geometrical fidelity. However, existing methods struggled under field conditions, especially with sparse and occluded views. This study developed a two-stage framework (DATR) for the reconstruction of apple trees from sparse views. The first stage leverages onboard sensors and foundation models to semi-automatically generate tree masks from complex field images. Tree masks are used to filter out background information in multi-modal data for the single-image-to-3D reconstruction at the second stage. This stage consists of a diffusion model and a large reconstruction model for respective multi view and implicit neural field generation. The training of the diffusion model and LRM was achieved by using realistic synthetic apple trees generated by a Real2Sim data generator. The framework was evaluated on both field and synthetic datasets. The field dataset includes six apple trees with field-measured ground truth, while the synthetic dataset featured structurally diverse trees. Evaluation results showed that our DATR framework outperformed existing 3D reconstruction methods across both datasets and achieved domain-trait estimation comparable to industrial-grade stationary laser scanners while improving the throughput by \\sim 360 times, demonstrating strong potential for scalable agricultural digital twin systems. Analysis: \u4f5c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u4e13\u5bb6\uff0c\u6211\u5bf9\u8fd9\u7bc7\u5173\u4e8eDATR\u7684\u8bba\u6587\u6458\u8981\u8fdb\u884c\u5982\u4e0b\u5206\u6790\uff1a DATR: Diffusion-based 3D Apple Tree Reconstruction Framework with Sparse-View 1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u6d01\u603b\u7ed3 (2-3 \u53e5\u8bdd) \u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDATR\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u5728\u91ce\u5916\u590d\u6742\u3001\u7a00\u758f\u548c\u906e\u6321\u89c6\u56fe\u6761\u4ef6\u4e0b\u9ad8\u7cbe\u5ea63D\u82f9\u679c\u6811\u91cd\u5efa\u7684\u96be\u9898\u3002\u8be5\u6846\u67b6\u9996\u5148\u5229\u7528\u57fa\u7840\u6a21\u578b\u534a\u81ea\u52a8\u751f\u6210\u6811\u6728\u63a9\u819c\u4ee5\u8fc7\u6ee4\u80cc\u666f\uff0c\u968f\u540e\u901a\u8fc7\u7ed3\u5408\u6269\u6563\u6a21\u578b\u548c\u5927\u578b\u91cd\u5efa\u6a21\u578b\uff08LRM\uff09\u5b9e\u73b0\u4ece\u5355\u5f20\u56fe\u50cf\u5230\u9690\u5f0f\u795e\u7ecf\u573a\u76843D\u91cd\u5efa\uff0c\u5e76\u5229\u7528Real2Sim\u6570\u636e\u751f\u6210\u5668\u4ea7\u751f\u7684\u5408\u6210\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u3002DATR\u5728\u7cbe\u5ea6\u4e0a\u53ef\u4e0e\u5de5\u4e1a\u7ea7\u6fc0\u5149\u626b\u63cf\u4eea\u5ab2\u7f8e\uff0c\u540c\u65f6\u5c06\u541e\u5410\u91cf\u63d0\u9ad8\u4e86\u7ea6360\u500d\uff0c\u4e3a\u53ef\u6269\u5c55\u7684\u519c\u4e1a\u6570\u5b57\u5b6a\u751f\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5f3a\u5927\u6f5c\u529b\u3002 2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u5b66\u65b9\u6cd5 \u8be5\u8bba\u6587\u7684\u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u5176 \u4e24\u9636\u6bb5\u7684\u6df7\u5408\u65b9\u6cd5 \uff0c\u7279\u522b\u662f\u5728\u7b2c\u4e8c\u9636\u6bb5\uff1a 1. \u524d\u666f\u5206\u5272\u4e0e\u80cc\u666f\u8fc7\u6ee4\uff1a \u5229\u7528 \u57fa\u7840\u6a21\u578b\uff08Foundation Models\uff09 \u5904\u7406\u590d\u6742\u91ce\u5916\u56fe\u50cf\uff0c\u534a\u81ea\u52a8\u751f\u6210\u6811\u6728\u63a9\u819c\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u80cc\u666f\u5e72\u6270\u95ee\u9898\uff0c\u8fd9\u662f\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u8fdb\u884c3D\u91cd\u5efa\u7684\u5173\u952e\u9884\u5904\u7406\u6b65\u9aa4\u3002 2. \u6269\u6563\u6a21\u578b\u4e0e\u5927\u578b\u91cd\u5efa\u6a21\u578b\uff08LRM\uff09\u7684\u7ed3\u5408\uff1a \u8fd9\u662f\u6280\u672f\u4e0a\u7684\u4eae\u70b9\u3002\u6269\u6563\u6a21\u578b\u901a\u5e38\u7528\u4e8e\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\u6216\u591a\u89c6\u56fe\u6570\u636e\uff0c\u800cLRM\uff08\u5982Google\u7684LRM\uff09\u5219\u64c5\u957f\u4ece\u5355\u5f20\u56fe\u50cf\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u9690\u5f0f\u795e\u7ecf\u573a\uff08Implicit Neural Fields\uff09\u8868\u793a\u76843D\u6a21\u578b\u3002\u5c06\u4e24\u8005\u7ed3\u5408\uff0c\u53ef\u80fd\u610f\u5473\u7740\u6269\u6563\u6a21\u578b\u8d1f\u8d23\u751f\u6210\u591a\u89c6\u56fe\u4e00\u81f4\u6027\u4fe1\u606f\u6216\u4f5c\u4e3aLRM\u7684\u6761\u4ef6\u8f93\u5165\uff0c\u4ee5\u589e\u5f3a\u5355\u89c6\u56fe\u91cd\u5efa\u7684\u9c81\u68d2\u6027\u548c\u7ec6\u8282\u3002 3. Real2Sim\u6570\u636e\u751f\u6210\u7b56\u7565\uff1a \u9488\u5bf9\u590d\u6742\u6709\u673a\u4f53\uff08\u5982\u6811\u6728\uff093D\u6570\u636e\u7a00\u7f3a\u7684\u6311\u6218\uff0c\u901a\u8fc7\u201cReal2Sim\u201d\u6570\u636e\u751f\u6210\u5668\u521b\u5efa\u903c\u771f\u7684\u5408\u6210\u82f9\u679c\u6811\u6570\u636e\u8fdb\u884c\u6a21\u578b\u8bad\u7ec3\uff0c\u6709\u6548\u5f25\u8865\u4e86\u771f\u5b9e\u4e16\u754c\u6570\u636e\u91c7\u96c6\u7684\u4e0d\u8db3\uff0c\u5e76\u53ef\u80fd\u6709\u52a9\u4e8e\u6a21\u578b\u5b66\u4e60\u66f4\u5e7f\u6cdb\u7684\u7ed3\u6784\u591a\u6837\u6027\u3002 4. \u7a00\u758f\u89c6\u56fe\u4e0b\u7684\u9ad8\u4fdd\u771f\u91cd\u5efa\uff1a \u660e\u786e\u9488\u5bf9\u201c\u7a00\u758f\u548c\u906e\u6321\u89c6\u56fe\u201d\u8fd9\u4e00\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u75db\u70b9\uff0c\u901a\u8fc7\u4e0a\u8ff0\u65b9\u6cd5\u5b9e\u73b0\u4e86\u9ad8\u51e0\u4f55\u4fdd\u771f\u5ea6\u76843D\u91cd\u5efa\u3002 3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd \u519c\u4e1a\u6570\u5b57\u5316\u8f6c\u578b\uff1a \u4e3a\u7cbe\u51c6\u519c\u4e1a\u548c\u667a\u6167\u519c\u4e1a\u63d0\u4f9b\u4e86\u9769\u547d\u6027\u7684\u5de5\u5177\u3002\u9ad8\u7cbe\u5ea6\u3001\u9ad8\u541e\u5410\u91cf\u7684\u679c\u68113D\u6a21\u578b\u5c06\u6781\u5927\u5730\u63a8\u52a8\u679c\u56ed\u7ba1\u7406\uff08\u5982\u751f\u957f\u76d1\u6d4b\u3001\u75c5\u866b\u5bb3\u9884\u8b66\u3001\u4ea7\u91cf\u9884\u6d4b\uff09\u3001\u81ea\u52a8\u5316\u4fee\u526a\u548c\u91c7\u6458\u673a\u5668\u4eba\u7684\u53d1\u5c55\uff0c\u5b9e\u73b0\u519c\u4e1a\u751f\u4ea7\u7684\u667a\u80fd\u5316\u548c\u9ad8\u6548\u5316\u3002 \u8ba1\u7b97\u673a\u89c6\u89c9\u4e0e3D\u91cd\u5efa\uff1a \u63a8\u52a8\u4e86\u4ece\u7a00\u758f/\u5355\u89c6\u56fe\u91cd\u5efa\u590d\u6742\u6709\u673a\u4f53\uff08\u5982\u6811\u6728\uff09\u7684\u6280\u672f\u8fb9\u754c\u3002\u5b83\u5c55\u793a\u4e86\u6269\u6563\u6a21\u578b\u548c\u5927\u578b\u91cd\u5efa\u6a21\u578b\u5728\u5904\u7406\u91ce\u5916\u975e\u7ed3\u6784\u5316\u73af\u5883\u3001\u514b\u670d\u6570\u636e\u7a00\u7f3a\u548c\u906e\u6321\u95ee\u9898\u65b9\u9762\u7684\u5f3a\u5927\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u5176\u4ed6\u590d\u6742\u573a\u666f\u76843D\u91cd\u5efa\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u548c\u8303\u5f0f\u3002 Real2Sim\u8303\u5f0f\u9a8c\u8bc1\uff1a \u6210\u529f\u5e94\u7528Real2Sim\u6570\u636e\u751f\u6210\u7b56\u7565\u6765\u8bad\u7ec3\u590d\u67423D\u91cd\u5efa\u6a21\u578b\uff0c\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u5408\u6210\u6570\u636e\u5728\u5f25\u8865\u771f\u5b9e\u6570\u636e\u4e0d\u8db3\u3001\u52a0\u901f\u6a21\u578b\u5f00\u53d1\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u5bf9\u5176\u4ed6\u6570\u636e\u53d7\u9650\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u5177\u6709\u501f\u9274\u610f\u4e49\u3002 \u673a\u5668\u4eba\u611f\u77e5\uff1a \u663e\u8457\u63d0\u5347\u4e86\u519c\u4e1a\u673a\u5668\u4eba\u5bf9\u590d\u6742\u81ea\u7136\u73af\u5883\u7684\u611f\u77e5\u80fd\u529b\uff0c\u4f7f\u5176\u80fd\u591f\u66f4\u7cbe\u786e\u5730\u7406\u89e3\u548c\u4ea4\u4e92\u5468\u56f4\u7684\u690d\u7269\u5bf9\u8c61\u3002 4. \u53ef\u80fd\u53d7\u76ca\u7684\u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 \u7cbe\u51c6\u519c\u4e1a\u4e0e\u667a\u6167\u519c\u4e1a\uff1a \u679c\u6811\u5065\u5eb7\u76d1\u6d4b\u3001\u751f\u957f\u5468\u671f\u9884\u6d4b\u3001\u4ea7\u91cf\u4f30\u7b97\u3001\u81ea\u52a8\u5316\u4fee\u526a\u4e0e\u91c7\u6458\u673a\u5668\u4eba\u5bfc\u822a\u3002 \u6797\u4e1a\u4e0e\u751f\u6001\u5b66\uff1a \u68ee\u6797\u8d44\u6e90\u666e\u67e5\u3001\u6811\u6728\u75c5\u866b\u5bb3\u76d1\u6d4b\u3001\u751f\u7269\u91cf\u4f30\u7b97\u3001\u751f\u6001\u7cfb\u7edf\u5efa\u6a21\u3002 \u57ce\u5e02\u89c4\u5212\u4e0e\u56ed\u6797\u8bbe\u8ba1\uff1a \u57ce\u5e02\u7eff\u5316\u7ba1\u7406\u3001\u666f\u89c2\u8bbe\u8ba1\u4e2d\u7684\u6811\u6728\u5efa\u6a21\u3002 \u73af\u5883\u76d1\u6d4b\uff1a \u690d\u7269\u751f\u957f\u52a8\u6001\u8ffd\u8e2a\u3001\u6c14\u5019\u53d8\u5316\u5bf9\u690d\u88ab\u5f71\u54cd\u7684\u7814\u7a76\u3002 \u673a\u5668\u4eba\u5b66\uff1a \u6237\u5916\u673a\u5668\u4eba\u5bf9\u590d\u6742\u81ea\u7136\u73af\u5883\u7684\u611f\u77e5\u4e0e\u4ea4\u4e92\uff0c\u4f8b\u5982\u5728\u975e\u7ed3\u6784\u5316\u5730\u5f62\u4e2d\u8fdb\u884c\u5bfc\u822a\u6216\u76ee\u6807\u8bc6\u522b\u3002 \u865a\u62df\u73b0\u5b9e/\u589e\u5f3a\u73b0\u5b9e\uff08VR/AR\uff09\u5185\u5bb9\u751f\u6210\uff1a \u4e3a\u865a\u62df\u73b0\u5b9e\u6216\u589e\u5f3a\u73b0\u5b9e\u5e94\u7528\u521b\u5efa\u903c\u771f\u7684\u81ea\u7136\u573a\u666f\u548c\u690d\u7269\u6a21\u578b\u3002 5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u7684\u5c40\u9650\u6027 \u534a\u81ea\u52a8\u5316\u63a9\u819c\u751f\u6210\uff1a \u6458\u8981\u4e2d\u63d0\u5230\u201c\u534a\u81ea\u52a8\u5316\u201d\u751f\u6210\u6811\u6728\u63a9\u819c\uff0c\u8fd9\u53ef\u80fd\u610f\u5473\u7740\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u4ecd\u9700\u8981\u4e00\u5b9a\u7a0b\u5ea6\u7684\u4eba\u5de5\u5e72\u9884\u6216\u76d1\u7763\uff0c\u9650\u5236\u4e86\u5176\u5b8c\u5168\u81ea\u52a8\u5316\u7684\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u5728\u5927\u89c4\u6a21\u90e8\u7f72\u65f6\u3002 \u9886\u57df\u7279\u5f02\u6027\uff1a \u8be5\u6846\u67b6\u4e13\u95e8\u9488\u5bf9\u201c\u82f9\u679c\u6811\u201d\u8fdb\u884c\u5f00\u53d1\u548c\u8bc4\u4f30\u3002\u867d\u7136\u65b9\u6cd5\u53ef\u80fd\u5177\u6709\u901a\u7528\u6027\uff0c\u4f46\u5176\u8bad\u7ec3\u6a21\u578b\u548c\u6027\u80fd\u53ef\u80fd\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u82f9\u679c\u6811\u7684\u7ed3\u6784\u7279\u5f81\uff0c\u63a8\u5e7f\u5230\u5176\u4ed6\u6811\u79cd\uff08\u5982\u677e\u6811\u3001\u6a61\u6811\u7b49\u5177\u6709\u4e0d\u540c\u5206\u652f\u7ed3\u6784\u548c\u53f6\u7247\u5bc6\u5ea6\u7684\u6811\u6728\uff09\u6216\u66f4\u5e7f\u6cdb\u7684\u6709\u673a\u7269\u4f53\u53ef\u80fd\u9700\u8981\u989d\u5916\u7684\u9002\u5e94\u6027\u5de5\u4f5c\u6216\u91cd\u65b0\u8bad\u7ec3\u3002 Sim-to-Real Gap\uff1a \u8bad\u7ec3\u4f9d\u8d56\u4e8e\u201cReal2Sim\u6570\u636e\u751f\u6210\u5668\u201d\u4ea7\u751f\u7684\u5408\u6210\u6570\u636e\u3002\u5c3d\u7ba1\u6458\u8981\u5f3a\u8c03\u5408\u6210\u6570\u636e\u903c\u771f\uff0c\u4f46\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u590d\u6742\u6027\u548c\u53d8\u5f02\u6027\uff08\u5982\u5149\u7167\u53d8\u5316\u3001\u5929\u6c14\u6761\u4ef6\u3001\u4e0d\u540c\u751f\u957f\u9636\u6bb5\u7684\u6811\u6728\u5f62\u6001\u3001\u75c5\u866b\u5bb3\u5f71\u54cd\u7b49\uff09\u53ef\u80fd\u4ecd\u672a\u5b8c\u5168\u8986\u76d6\uff0c\u53ef\u80fd\u5b58\u5728\u4ece\u6a21\u62df\u5230\u771f\u5b9e\u73af\u5883\u7684\u6cdb\u5316\u5dee\u8ddd\u3002 \u7a00\u758f\u89c6\u56fe\u7684\u6781\u9650\uff1a \u5c3d\u7ba1\u89e3\u51b3\u4e86\u7a00\u758f\u548c\u906e\u6321\u89c6\u56fe\u7684\u95ee\u9898\uff0c\u4f46\u6458\u8981\u5e76\u672a\u8bf4\u660e\u5176\u5bf9\u89c6\u56fe\u7a00\u758f\u7a0b\u5ea6\u7684\u9c81\u68d2\u6027\u4e0a\u9650\uff0c\u5728\u6781\u7aef\u7a00\u758f\u6216\u906e\u6321\u60c5\u51b5\u4e0b\uff08\u4f8b\u5982\uff0c\u4ec5\u6709\u6781\u5c11\u6570\u56fe\u50cf\u6216\u5927\u90e8\u5206\u88ab\u906e\u6321\uff09\u6027\u80fd\u5982\u4f55\u4ecd\u9700\u8fdb\u4e00\u6b65\u63a2\u8ba8\u3002 \u51e0\u4f55\u7cbe\u5ea6\u4e0e\u6fc0\u5149\u626b\u63cf\u4eea\u7684\u6743\u8861\uff1a \u5c3d\u7ba1\u5728\u201c\u9886\u57df\u7279\u5f81\u4f30\u8ba1\u201d\u4e0a\u4e0e\u5de5\u4e1a\u7ea7\u6fc0\u5149\u626b\u63cf\u4eea\u201c\u76f8\u5f53\u201d\uff0c\u5e76\u5927\u5e45\u63d0\u9ad8\u4e86\u541e\u5410\u91cf\uff0c\u4f46\u5bf9\u4e8e\u7eaf\u7cb9\u7684\u51e0\u4f55\u7ec6\u8282\u7cbe\u5ea6\uff0c\u4e0e\u6700\u9876\u7ea7\u7684\u3001\u8017\u65f6\u7684\u6fc0\u5149\u626b\u63cf\u4eea\u76f8\u6bd4\uff0c\u53ef\u80fd\u4ecd\u5b58\u5728\u7ec6\u5fae\u5dee\u8ddd\uff0c\u5c24\u5176\u662f\u5728\u5fae\u5c0f\u5206\u652f\u6216\u53f6\u7247\u7ea7\u522b\u7684\u7ec6\u8282\u4e0a\u3002 Key Findings: Evaluation results showed that our DATR framework outperformed existing 3D reconstruction methods across both datasets and achieved domain-trait estimation comparable to industrial-grade stationary laser scanners while improving the throughput by \\sim 360 times, demonstrating strong potential for scalable agricultural digital twin systems. Links: PDF arXiv Beyond flattening: a geometrically principled positional encoding for vision transformers with Weierstrass elliptic functions Authors: Zhihang Xin, Xitong Hu, Rui Wang Published: 2025-08-26 Categories: cs.CV Abstract: Vision Transformers have demonstrated remarkable success in computer vision tasks, yet their reliance on learnable one-dimensional positional embeddings fundamentally disrupts the inherent two-dimensional spatial structure of images through patch flattening procedures. Traditional positional encoding approaches lack geometric constraints and fail to establish monotonic correspondence between Euclidean spatial distances and sequential index distances, thereby limiting the model's capacity to leverage spatial proximity priors effectively. We propose Weierstrass Elliptic Function Positional Encoding (WEF-PE), a mathematically principled approach that directly addresses two-dimensional coordinates through natural complex domain representation, where the doubly periodic properties of elliptic functions align remarkably with translational invariance patterns commonly observed in visual data. Our method exploits the non-linear geometric nature of elliptic functions to encode spatial distance relationships naturally, while the algebraic addition formula enables direct derivation of relative positional information between arbitrary patch pairs from their absolute encodings. Comprehensive experiments demonstrate that WEF-PE achieves superior performance across diverse scenarios, including 63.78\\% accuracy on CIFAR-100 from-scratch training with ViT-Tiny architecture, 93.28\\% on CIFAR-100 fine-tuning with ViT-Base, and consistent improvements on VTAB-1k benchmark tasks. Theoretical analysis confirms the distance-decay property through rigorous mathematical proof, while attention visualization reveals enhanced geometric inductive bias and more coherent semantic focus compared to conventional approaches.The source code implementing the methods described in this paper is publicly available on GitHub. Analysis: \u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u3001\u57fa\u4e8e\u6570\u5b66\u539f\u7406\u7684\u4f4d\u7f6e\u7f16\u7801\u65b9\u6cd5\uff0c\u65e8\u5728\u89e3\u51b3Vision Transformers (ViT) \u4e2d\u56fe\u50cf\u5c55\u5e73\u5bfc\u81f4\u7684\u4e8c\u7ef4\u7a7a\u95f4\u7ed3\u6784\u4fe1\u606f\u4e22\u5931\u95ee\u9898\u3002\u4f5c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u4e13\u5bb6\uff0c\u6211\u5bf9\u8fd9\u7bc7\u8bba\u6587\u7684\u5206\u6790\u5982\u4e0b\uff1a \u8bba\u6587\u5206\u6790\uff1aWeierstrass Elliptic Function Positional Encoding (WEF-PE) 1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e (Concise Summary) \u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aWeierstrass\u692d\u5706\u51fd\u6570\u4f4d\u7f6e\u7f16\u7801\uff08WEF-PE\uff09\u7684\u65b0\u578b\u65b9\u6cd5\uff0c\u65e8\u5728\u89e3\u51b3Vision Transformers\u4e2d\u56fe\u50cf\u5c55\u5e73\u5bfc\u81f4\u7684\u4e8c\u7ef4\u7a7a\u95f4\u7ed3\u6784\u4e22\u5931\u95ee\u9898\u3002WEF-PE\u5229\u7528Weierstrass\u692d\u5706\u51fd\u6570\u5728\u590d\u6570\u57df\u7684\u51e0\u4f55\u7279\u6027\u548c\u53cc\u5468\u671f\u6027\uff0c\u76f4\u63a5\u7f16\u7801\u4e8c\u7ef4\u5750\u6807\uff0c\u4ece\u800c\u66f4\u597d\u5730\u6355\u6349\u7a7a\u95f4\u8ddd\u79bb\u5173\u7cfb\u548c\u7ffb\u8bd1\u4e0d\u53d8\u6027\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0cWEF-PE\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u589e\u5f3a\u4e86\u51e0\u4f55\u5f52\u7eb3\u504f\u7f6e\u3002 2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u5b66 (Key Innovation or Methodological Approach) \u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u9996\u6b21\u5c06 Weierstrass\u692d\u5706\u51fd\u6570 \u5f15\u5165Vision Transformers\u7684\u4f4d\u7f6e\u7f16\u7801\uff0c\u4ee5\u4e00\u79cd \u51e0\u4f55\u539f\u7406\u6027 \u7684\u65b9\u5f0f\u76f4\u63a5\u5904\u7406\u4e8c\u7ef4\u56fe\u50cf\u5750\u6807\u3002\u5177\u4f53\u65b9\u6cd5\u5b66\u4eae\u70b9\u5305\u62ec\uff1a \u76f4\u63a5\u4e8c\u7ef4\u5750\u6807\u7f16\u7801\uff1a \u6452\u5f03\u4e86\u4f20\u7edf\u7684\u5c06\u4e8c\u7ef4\u56fe\u50cf\u5c55\u5e73\u4e3a\u4e00\u7ef4\u5e8f\u5217\u7684\u505a\u6cd5\uff0c\u800c\u662f\u76f4\u63a5\u5728 \u590d\u6570\u57df \u4e2d\u8868\u793a\u548c\u7f16\u7801\u4e8c\u7ef4\u56fe\u50cf\u5750\u6807\uff0c\u4ece\u800c\u4fdd\u7559\u4e86\u56fe\u50cf\u56fa\u6709\u7684\u7a7a\u95f4\u7ed3\u6784\u3002 Weierstrass\u692d\u5706\u51fd\u6570\u7684\u5e94\u7528\uff1a \u5229\u7528\u8be5\u51fd\u6570\u7684\u4ee5\u4e0b\u7279\u6027\uff1a \u975e\u7ebf\u6027\u51e0\u4f55\u6027\u8d28\uff1a \u81ea\u7136\u5730\u7f16\u7801\u7a7a\u95f4\u8ddd\u79bb\u5173\u7cfb\uff0c\u5e76\u5efa\u7acb\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\u8ddd\u79bb\u4e0e\u7f16\u7801\u5e8f\u5217\u8ddd\u79bb\u4e4b\u95f4\u7684\u5355\u8c03\u5bf9\u5e94\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7f3a\u4e4f\u51e0\u4f55\u7ea6\u675f\u7684\u95ee\u9898\u3002 \u53cc\u5468\u671f\u6027\uff1a \u692d\u5706\u51fd\u6570\u7684\u53cc\u5468\u671f\u6027\u4e0e\u89c6\u89c9\u6570\u636e\u4e2d\u5e38\u89c1\u7684 \u5e73\u79fb\u4e0d\u53d8\u6027 \u6a21\u5f0f\u9ad8\u5ea6\u5951\u5408\uff0c\u6709\u52a9\u4e8e\u6a21\u578b\u66f4\u597d\u5730\u7406\u89e3\u548c\u5229\u7528\u8fd9\u79cd\u5148\u9a8c\u77e5\u8bc6\u3002 \u4ee3\u6570\u52a0\u6cd5\u516c\u5f0f\uff1a \u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u76f4\u63a5\u4ece\u4efb\u610f\u4e24\u4e2a\u8865\u4e01\u7684\u7edd\u5bf9\u4f4d\u7f6e\u7f16\u7801\u4e2d\u63a8\u5bfc\u51fa\u5b83\u4eec\u4e4b\u95f4\u7684 \u76f8\u5bf9\u4f4d\u7f6e\u4fe1\u606f \uff0c\u8fd9\u5bf9\u4e8eTransformer\u7684\u6ce8\u610f\u529b\u673a\u5236\u81f3\u5173\u91cd\u8981\u3002 \u7406\u8bba\u4e0e\u5b9e\u8df5\u7ed3\u5408\uff1a \u901a\u8fc7\u4e25\u683c\u7684\u6570\u5b66\u8bc1\u660e\u786e\u8ba4\u4e86 \u8ddd\u79bb\u8870\u51cf\uff08distance-decay\uff09 \u7279\u6027\uff0c\u5e76\u901a\u8fc7\u6ce8\u610f\u529b\u53ef\u89c6\u5316\u63ed\u793a\u4e86\u589e\u5f3a\u7684\u51e0\u4f55\u5f52\u7eb3\u504f\u7f6e\u548c\u66f4\u8fde\u8d2f\u7684\u8bed\u4e49\u7126\u70b9\u3002 3. \u5bf9\u9886\u57df\u7684\u6f5c\u5728\u5f71\u54cd (Potential Impact on the Field) \u63a8\u52a8\u4f4d\u7f6e\u7f16\u7801\u8303\u5f0f\u8f6c\u53d8\uff1a \u4ece\u7ecf\u9a8c\u6027\u6216\u53ef\u5b66\u4e60\u76841D\u7f16\u7801\u8f6c\u5411\u57fa\u4e8e\u6df1\u5c42\u6570\u5b66\u539f\u7406\u76842D\u51e0\u4f55\u7f16\u7801\uff0c\u4e3aViT\u8bbe\u8ba1\u63d0\u4f9b\u65b0\u7684\u7406\u8bba\u57fa\u7840\u548c\u65b9\u5411\u3002 \u63d0\u5347ViT\u7684\u51e0\u4f55\u7406\u89e3\u548c\u6027\u80fd\uff1a \u589e\u5f3a\u6a21\u578b\u5bf9\u7a7a\u95f4\u7ed3\u6784\u548c\u8ddd\u79bb\u5173\u7cfb\u7684\u611f\u77e5\u80fd\u529b\uff0c\u4ece\u800c\u5728\u5404\u79cd\u89c6\u89c9\u4efb\u52a1\u4e2d\u53d6\u5f97\u66f4\u4f18\u5f02\u7684\u8868\u73b0\u548c\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5c24\u5176\u662f\u5728\u5bf9\u7a7a\u95f4\u7ec6\u8282\u654f\u611f\u7684\u4efb\u52a1\u4e0a\u3002 \u542f\u53d1\u65b0\u7814\u7a76\u65b9\u5411\uff1a \u9f13\u52b1\u7814\u7a76\u8005\u63a2\u7d22\u66f4\u591a\u9ad8\u7ea7\u6570\u5b66\u5de5\u5177\uff08\u5982\u590d\u5206\u6790\u3001\u5fae\u5206\u51e0\u4f55\u3001\u62d3\u6251\u5b66\u7b49\uff09\u5728\u6df1\u5ea6\u5b66\u4e60\uff0c\u7279\u522b\u662fTransformer\u67b6\u6784\u4e2d\u7684\u5e94\u7528\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u6a21\u578b\u7684\u7ed3\u6784\u6027\u9650\u5236\u3002 \u589e\u5f3a\u6a21\u578b\u53ef\u89e3\u91ca\u6027\uff1a \u901a\u8fc7\u7406\u8bba\u8bc1\u660e\u548c\u6ce8\u610f\u529b\u53ef\u89c6\u5316\uff0c\u63ed\u793a\u4e86\u66f4\u5f3a\u7684\u51e0\u4f55\u5f52\u7eb3\u504f\u7f6e\u548c\u66f4\u8fde\u8d2f\u7684\u8bed\u4e49\u7126\u70b9\uff0c\u6709\u52a9\u4e8e\u7406\u89e3ViT\u7684\u5de5\u4f5c\u673a\u5236\uff0c\u5e76\u53ef\u80fd\u6307\u5bfc\u672a\u6765\u6a21\u578b\u7684\u8bbe\u8ba1\u3002 4. \u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 (Related Areas or Applications that Might Benefit) \u901a\u7528\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\uff1a \u56fe\u50cf\u5206\u7c7b\u3001\u76ee\u6807\u68c0\u6d4b\u3001\u8bed\u4e49\u5206\u5272\u3001\u5b9e\u4f8b\u5206\u5272\u3001\u59ff\u6001\u4f30\u8ba1\u7b49\uff0c\u4efb\u4f55\u9700\u8981\u7cbe\u786e\u7a7a\u95f4\u7406\u89e3\u7684\u4efb\u52a1\u3002 \u89c6\u9891\u5904\u7406\uff1a \u89c6\u9891Transformer\u4e2d\uff0c\u53ef\u4ee5\u6269\u5c55\u5230\u4e09\u7ef4\uff08\u7a7a\u95f4+\u65f6\u95f4\uff09\u4f4d\u7f6e\u7f16\u7801\uff0c\u66f4\u597d\u5730\u6355\u6349\u65f6\u7a7a\u5173\u7cfb\u3002 \u533b\u5b66\u5f71\u50cf\u5206\u6790\uff1a \u5bf9\u56fe\u50cf\u7684\u51e0\u4f55\u7ed3\u6784\u548c\u76f8\u5bf9\u4f4d\u7f6e\u654f\u611f\uff0cWEF-PE\u6709\u671b\u63d0\u9ad8\u8bca\u65ad\u51c6\u786e\u6027\uff0c\u4f8b\u5982\u80bf\u7624\u5b9a\u4f4d\u3001\u75c5\u7076\u5206\u5272\u7b49\u3002 \u9065\u611f\u56fe\u50cf\u5206\u6790\uff1a \u5927\u5c3a\u5ea6\u56fe\u50cf\u4e2d\u7684\u5730\u7269\u8bc6\u522b\u548c\u53d8\u5316\u68c0\u6d4b\uff0c\u5bf9\u7a7a\u95f4\u4e0a\u4e0b\u6587\u7684\u7406\u89e3\u81f3\u5173\u91cd\u8981\u3002 3D\u89c6\u89c9\uff1a \u5c3d\u7ba1\u672c\u6587\u662f2D\uff0c\u4f46\u5176\u51e0\u4f55\u539f\u7406\u6027\u53ef\u80fd\u4e3a3D\u70b9\u4e91\u6216\u4f53\u7d20\u6570\u636e\u7684Transformer\u67b6\u6784\u63d0\u4f9b\u542f\u53d1\uff0c\u4f8b\u59823D\u76ee\u6807\u68c0\u6d4b\u6216\u573a\u666f\u7406\u89e3\u3002 \u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\uff1a \u5982\u679c\u56fe\u8282\u70b9\u5177\u6709\u9690\u5f0f\u6216\u663e\u5f0f\u7684\u7a7a\u95f4\u5750\u6807\uff0c\u8fd9\u79cd\u51e0\u4f55\u7f16\u7801\u601d\u60f3\u4e5f\u53ef\u80fd\u9002\u7528\uff0c\u4ee5\u66f4\u597d\u5730\u5229\u7528\u8282\u70b9\u95f4\u7684\u7a7a\u95f4\u5173\u7cfb\u3002 5. \u53ef\u63a8\u65ad\u7684\u5c40\u9650\u6027 (Limitations that Can Be Inferred from the Abstract) \u6570\u5b66\u590d\u6742\u6027\u4e0e\u5b9e\u73b0\u96be\u5ea6\uff1a Weierstrass\u692d\u5706\u51fd\u6570\u53ca\u5176\u5728\u590d\u6570\u57df\u7684\u5e94\u7528\u5bf9\u4e0d\u719f\u6089\u8be5\u9886\u57df\u7684\u5f00\u53d1\u8005\u6765\u8bf4\u53ef\u80fd\u5177\u6709\u8f83\u9ad8\u7684\u7406\u89e3\u548c\u5b9e\u73b0\u95e8\u69db\u3002 \u8ba1\u7b97\u5f00\u9500\uff1a \u5c3d\u7ba1\u6458\u8981\u672a\u63d0\u53ca\uff0c\u4f46\u590d\u6742\u7684\u6570\u5b66\u51fd\u6570\u8ba1\u7b97\u53ef\u80fd\u4f1a\u5f15\u5165\u989d\u5916\u7684\u8ba1\u7b97\u5f00\u9500\uff0c\u5c24\u5176\u662f\u5728\u5927\u89c4\u6a21\u6a21\u578b\u6216\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u4e0a\uff0c\u8fd9\u53ef\u80fd\u5f71\u54cd\u63a8\u7406\u901f\u5ea6\u3002 \u201c\u53cc\u5468\u671f\u6027\u201d\u7684\u666e\u9002\u6027\uff1a \u5c3d\u7ba1\u6458\u8981\u6307\u51fa\u5176\u4e0e\u89c6\u89c9\u6570\u636e\u7684\u5e73\u79fb\u4e0d\u53d8\u6027\u6a21\u5f0f\u5bf9\u9f50\uff0c\u4f46\u5e76\u975e\u6240\u6709\u89c6\u89c9\u573a\u666f\u90fd\u4e25\u683c\u7b26\u5408\u53cc\u5468\u671f\u6027\u5047\u8bbe\uff08\u4f8b\u5982\uff0c\u56fe\u50cf\u8fb9\u754c\u6548\u5e94\u3001\u975e\u5468\u671f\u6027\u7eb9\u7406\u7b49\uff09\uff0c\u8fd9\u53ef\u80fd\u5728\u67d0\u4e9b\u7279\u5b9a\u6570\u636e\u96c6\u6216\u4efb\u52a1\u4e2d\u9650\u5236\u5176\u8868\u73b0\u3002 \u8d85\u53c2\u6570\u8c03\u4f18\uff1a \u692d\u5706\u51fd\u6570\u53ef\u80fd\u6d89\u53ca\u4e00\u4e9b\u53c2\u6570\uff08\u5982\u5468\u671f\u3001\u6a21\u6570\u7b49\uff09\uff0c\u8fd9\u4e9b\u53c2\u6570\u7684\u9009\u62e9\u548c\u8c03\u4f18\u53ef\u80fd\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\u6216\u989d\u5916\u7684\u5b9e\u9a8c\uff0c\u589e\u52a0\u4e86\u6a21\u578b\u7684\u590d\u6742\u6027\u3002 \u4e0e\u5176\u4ed6\u5148\u8fdb2D PE\u65b9\u6cd5\u7684\u6bd4\u8f83\uff1a \u6458\u8981\u4e2d\u4e3b\u8981\u4e0e\u201c\u4f20\u7edf\u201d\u65b9\u6cd5\u6bd4\u8f83\uff0c\u4f46\u672a\u660e\u786e\u63d0\u53ca\u4e0e\u6700\u8fd1\u7684SOTA 2D PE\u65b9\u6cd5\uff08\u5982RoPE\u3001xPos\u3001\u6216\u5176\u4ed6\u57fa\u4e8e\u5085\u91cc\u53f6\u7279\u5f81\u7684PE\uff09\u7684\u8be6\u7ec6\u5bf9\u6bd4\uff0c\u8fd9\u53ef\u80fd\u662f\u4e00\u4e2a\u6f5c\u5728\u7684\u5c40\u9650\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u76f8\u5bf9\u4f18\u52bf\u3002 Key Findings: We propose Weierstrass Elliptic Function Positional Encoding (WEF-PE), a mathematically principled approach that directly addresses two-dimensional coordinates through natural complex domain representation, where the doubly periodic properties of elliptic functions align remarkably with translational invariance patterns commonly observed in visual data. Our method exploits the non-linear geometric nature of elliptic functions to encode spatial distance relationships naturally, while the algebraic addition formula enables direct derivation of relative positional information between arbitrary patch pairs from their absolute encodings. Links: PDF arXiv Bridging Domain Gaps for Fine-Grained Moth Classification Through Expert-Informed Adaptation and Foundation Model Priors Authors: Ross J Gardiner, Guillaume Mougeot, Sareh Rowlands, Benno I Simmons, Flemming Helsing, Toke Thomas H\u00f8ye Published: 2025-08-27 Categories: cs.CV Abstract: Labelling images of Lepidoptera (moths) from automated camera systems is vital for understanding insect declines. However, accurate species identification is challenging due to domain shifts between curated images and noisy field imagery. We propose a lightweight classification approach, combining limited expert-labelled field data with knowledge distillation from the high-performance BioCLIP2 foundation model into a ConvNeXt-tiny architecture. Experiments on 101 Danish moth species from AMI camera systems demonstrate that BioCLIP2 substantially outperforms other methods and that our distilled lightweight model achieves comparable accuracy with significantly reduced computational cost. These insights offer practical guidelines for the development of efficient insect monitoring systems and bridging domain gaps for fine-grained classification. Analysis: \u4f5c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u4e13\u5bb6\uff0c\u6211\u5bf9\u8fd9\u7bc7\u8bba\u6587\u6458\u8981\u8fdb\u884c\u5982\u4e0b\u5206\u6790\uff1a 1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u6d01\u603b\u7ed3 (Concise Summary) \u672c\u6587\u9488\u5bf9\u81ea\u52a8\u5316\u76f8\u673a\u7cfb\u7edf\u6355\u83b7\u7684\u98de\u86fe\u56fe\u50cf\u5728\u7cbe\u7ec6\u7c92\u5ea6\u5206\u7c7b\u4e2d\u5b58\u5728\u7684\u9886\u57df\u6f02\u79fb\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u5206\u7c7b\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u5c06\u6709\u9650\u7684\u4e13\u5bb6\u6807\u6ce8\u91ce\u5916\u6570\u636e\u4e0e\u9ad8\u6027\u80fdBioCLIP2\u57fa\u7840\u6a21\u578b\u7684\u77e5\u8bc6\u84b8\u998f\u76f8\u7ed3\u5408\uff0c\u76ee\u6807\u662fConvNeXt-tiny\u67b6\u6784\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c\u84b8\u998f\u540e\u7684\u8f7b\u91cf\u7ea7\u6a21\u578b\u5728\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\uff0c\u80fd\u8fbe\u5230\u4e0eBioCLIP2\u76f8\u5f53\u7684\u51c6\u786e\u6027\uff0c\u4e3a\u9ad8\u6548\u6606\u866b\u76d1\u6d4b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002 2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u5b66\u65b9\u6cd5 (Key Innovation or Methodological Approach) \u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u5c06\u9ad8\u6027\u80fd\u57fa\u7840\u6a21\u578b\uff08BioCLIP2\uff09\u7684\u5f3a\u5927\u8868\u5f81\u80fd\u529b\uff0c\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u6280\u672f\u8fc1\u79fb\u5230\u4e00\u4e2a\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u7684\u8f7b\u91cf\u7ea7\u6a21\u578b\uff08ConvNeXt-tiny\uff09\u4e0a\u3002\u540c\u65f6\uff0c\u8be5\u65b9\u6cd5\u5de7\u5999\u5730\u5229\u7528\u4e86\u6709\u9650\u7684\u4e13\u5bb6\u6807\u6ce8\u91ce\u5916\u6570\u636e\u6765\u9002\u5e94\u771f\u5b9e\u4e16\u754c\u7684\u9886\u57df\u6f02\u79fb\uff0c\u4ece\u800c\u5728\u4fdd\u8bc1\u7cbe\u7ec6\u7c92\u5ea6\u5206\u7c7b\u51c6\u786e\u6027\u7684\u524d\u63d0\u4e0b\uff0c\u5927\u5e45\u964d\u4f4e\u4e86\u90e8\u7f72\u6210\u672c\u3002\u8fd9\u79cd\u7ed3\u5408\u4e86\u57fa\u7840\u6a21\u578b\u5148\u9a8c\u77e5\u8bc6\u3001\u77e5\u8bc6\u84b8\u998f\u548c\u9886\u57df\u9002\u5e94\u7684\u7b56\u7565\uff0c\u4e3a\u89e3\u51b3\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u548c\u8ba1\u7b97\u8d44\u6e90\u9650\u5236\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002 3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd (Potential Impact on the Field) \u8fd9\u9879\u7814\u7a76\u4e3a\u5f00\u53d1\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u6606\u866b\u76d1\u6d4b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c24\u5176\u662f\u5728\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u7684\u91ce\u5916\u90e8\u7f72\u573a\u666f\u3002\u5176\u63d0\u51fa\u7684\u7ed3\u5408\u57fa\u7840\u6a21\u578b\u5148\u9a8c\u77e5\u8bc6\u548c\u9886\u57df\u9002\u5e94\u7684\u77e5\u8bc6\u84b8\u998f\u7b56\u7565\uff0c\u5bf9\u5176\u4ed6\u9700\u8981\u5904\u7406\u9886\u57df\u6f02\u79fb\u548c\u7cbe\u7ec6\u7c92\u5ea6\u5206\u7c7b\u7684\u751f\u7269\u591a\u6837\u6027\u76d1\u6d4b\u3001\u519c\u4e1a\u75c5\u866b\u5bb3\u8bc6\u522b\u7b49\u9886\u57df\u5177\u6709\u91cd\u8981\u7684\u501f\u9274\u610f\u4e49\u3002\u5b83\u5c55\u793a\u4e86\u5982\u4f55\u5c06\u5927\u578b\u6a21\u578b\u7684\u5f3a\u5927\u80fd\u529b\u8f6c\u5316\u4e3a\u8fb9\u7f18\u8bbe\u5907\u53ef\u7528\u7684\u8f7b\u91cf\u7ea7\u5e94\u7528\uff0c\u63a8\u52a8\u4e86AI\u5728\u751f\u6001\u5b66\u548c\u73af\u5883\u79d1\u5b66\u9886\u57df\u7684\u5b9e\u9645\u843d\u5730\u3002 4. \u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 (Related Areas or Applications) \u751f\u7269\u591a\u6837\u6027\u76d1\u6d4b\u4e0e\u4fdd\u62a4: \u9664\u4e86\u98de\u86fe\uff0c\u8fd8\u53ef\u5e94\u7528\u4e8e\u5176\u4ed6\u6606\u866b\u3001\u9e1f\u7c7b\u3001\u690d\u7269\u7b49\u7269\u79cd\u7684\u81ea\u52a8\u5316\u8bc6\u522b\u548c\u79cd\u7fa4\u76d1\u6d4b\u3002 \u519c\u4e1a\u75c5\u866b\u5bb3\u8bc6\u522b: \u5e2e\u52a9\u519c\u6c11\u5feb\u901f\u51c6\u786e\u8bc6\u522b\u4f5c\u7269\u75c5\u5bb3\u6216\u5bb3\u866b\uff0c\u8fdb\u884c\u7cbe\u51c6\u9632\u6cbb\u3002 \u533b\u5b66\u5f71\u50cf\u5206\u6790: \u89e3\u51b3\u4e0d\u540c\u533b\u7597\u8bbe\u5907\u6216\u6570\u636e\u96c6\u4e4b\u95f4\u7684\u9886\u57df\u6f02\u79fb\uff0c\u5b9e\u73b0\u75be\u75c5\u7684\u7cbe\u7ec6\u5316\u8bca\u65ad\u3002 \u5de5\u4e1a\u7f3a\u9677\u68c0\u6d4b: \u9002\u5e94\u4e0d\u540c\u751f\u4ea7\u7ebf\u6216\u5149\u7167\u6761\u4ef6\u4e0b\u7684\u4ea7\u54c1\u7f3a\u9677\u8bc6\u522b\u3002 \u9065\u611f\u56fe\u50cf\u5206\u6790: \u5728\u4e0d\u540c\u5730\u7406\u533a\u57df\u6216\u5b63\u8282\u6761\u4ef6\u4e0b\uff0c\u5bf9\u5730\u7269\u8fdb\u884c\u7cbe\u7ec6\u5206\u7c7b\u3002 \u4efb\u4f55\u9700\u8981\u8fb9\u7f18\u90e8\u7f72\u7684\u7cbe\u7ec6\u7c92\u5ea6\u5206\u7c7b\u4efb\u52a1: \u5c24\u5176\u662f\u5728\u6570\u636e\u6807\u6ce8\u6210\u672c\u9ad8\u6602\u3001\u8ba1\u7b97\u8d44\u6e90\u53d7\u9650\u7684\u573a\u666f\u3002 5. \u53ef\u4ece\u6458\u8981\u4e2d\u63a8\u65ad\u51fa\u7684\u5c40\u9650\u6027 (Limitations Inferred from the Abstract) \u51c6\u786e\u6027\u5e76\u975e\u8d85\u8d8a\u57fa\u7840\u6a21\u578b: \u84b8\u998f\u540e\u7684\u8f7b\u91cf\u7ea7\u6a21\u578b\u5b9e\u73b0\u4e86\u201c\u53ef\u6bd4\uff08comparable\uff09\u201d\u7684\u51c6\u786e\u6027\uff0c\u800c\u975e\u201c\u8d85\u8d8a\uff08outperforms\uff09\u201d\u57fa\u7840\u6a21\u578bBioCLIP2\u3002\u8fd9\u610f\u5473\u7740\u5728\u8ffd\u6c42\u6781\u81f4\u51c6\u786e\u6027\u7684\u573a\u666f\u4e0b\uff0c\u53ef\u80fd\u4ecd\u9700\u6743\u8861\u8ba1\u7b97\u6210\u672c\u3002 \u5bf9\u4e13\u5bb6\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56: \u5c3d\u7ba1\u65b9\u6cd5\u65e8\u5728\u5229\u7528\u201c\u6709\u9650\u201d\u7684\u4e13\u5bb6\u6807\u6ce8\u6570\u636e\uff0c\u4f46\u5176\u6027\u80fd\u4ecd\u53ef\u80fd\u53d7\u9650\u4e8e\u8fd9\u4e9b\u6570\u636e\u7684\u8d28\u91cf\u548c\u6570\u91cf\u3002\u5728\u5b8c\u5168\u6ca1\u6709\u4e13\u5bb6\u6807\u6ce8\u7684\u9886\u57df\uff0c\u8be5\u65b9\u6cd5\u53ef\u80fd\u9762\u4e34\u6311\u6218\u3002 \u7279\u5b9a\u6570\u636e\u96c6\u7684\u6cdb\u5316\u6027: \u5b9e\u9a8c\u57fa\u4e8e\u201c101\u79cd\u4e39\u9ea6\u98de\u86fe\u201d\u7684\u7279\u5b9a\u6570\u636e\u96c6\u3002\u5176\u5728\u5176\u4ed6\u5730\u7406\u533a\u57df\u3001\u66f4\u591a\u7269\u79cd\u6216\u4e0d\u540c\u751f\u7269\u7c7b\u7fa4\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4ee5\u53ca\u5bf9\u4e0d\u540c\u7c7b\u578b\u91ce\u5916\u566a\u58f0\u7684\u9c81\u68d2\u6027\uff0c\u4ecd\u9700\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u3002 \u201c\u8f7b\u91cf\u7ea7\u201d\u7684\u7a0b\u5ea6: ConvNeXt-tiny\u867d\u7136\u76f8\u5bf9\u8f83\u5c0f\uff0c\u4f46\u5bf9\u4e8e\u67d0\u4e9b\u6781\u5ea6\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\uff08\u5982\u5fae\u63a7\u5236\u5668\uff09\u800c\u8a00\uff0c\u53ef\u80fd\u4ecd\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u3002\u62bd\u8c61\u4e2d\u672a\u8be6\u7ec6\u8bf4\u660e\u5177\u4f53\u7684\u8ba1\u7b97\u8d44\u6e90\u8282\u7701\u6bd4\u4f8b\u6216\u90e8\u7f72\u73af\u5883\u3002 Key Findings: We propose a lightweight classification approach, combining limited expert-labelled field data with knowledge distillation from the high-performance BioCLIP2 foundation model into a ConvNeXt-tiny architecture. Experiments on 101 Danish moth species from AMI camera systems demonstrate that BioCLIP2 substantially outperforms other methods and that our distilled lightweight model achieves comparable accuracy with significantly reduced computational cost. Links: PDF arXiv OpenM3D: Open Vocabulary Multi-view Indoor 3D Object Detection without Human Annotations Authors: Peng-Hao Hsu, Ke Zhang, Fu-En Wang, Tao Tu, Ming-Feng Li, Yu-Lun Liu, Albert Y. C. Chen, Min Sun, Cheng-Hao Kuo Published: 2025-08-27 Categories: cs.CV Abstract: Open-vocabulary (OV) 3D object detection is an emerging field, yet its exploration through image-based methods remains limited compared to 3D point cloud-based methods. We introduce OpenM3D, a novel open-vocabulary multi-view indoor 3D object detector trained without human annotations. In particular, OpenM3D is a single-stage detector adapting the 2D-induced voxel features from the ImGeoNet model. To support OV, it is jointly trained with a class-agnostic 3D localization loss requiring high-quality 3D pseudo boxes and a voxel-semantic alignment loss requiring diverse pre-trained CLIP features. We follow the training setting of OV-3DET where posed RGB-D images are given but no human annotations of 3D boxes or classes are available. We propose a 3D Pseudo Box Generation method using a graph embedding technique that combines 2D segments into coherent 3D structures. Our pseudo-boxes achieve higher precision and recall than other methods, including the method proposed in OV-3DET. We further sample diverse CLIP features from 2D segments associated with each coherent 3D structure to align with the corresponding voxel feature. The key to training a highly accurate single-stage detector requires both losses to be learned toward high-quality targets. At inference, OpenM3D, a highly efficient detector, requires only multi-view images for input and demonstrates superior accuracy and speed (0.3 sec. per scene) on ScanNet200 and ARKitScenes indoor benchmarks compared to existing methods. We outperform a strong two-stage method that leverages our class-agnostic detector with a ViT CLIP-based OV classifier and a baseline incorporating multi-view depth estimator on both accuracy and speed. Analysis: \u8fd9\u7bc7\u8bba\u6587\u300aOpenM3D: Open Vocabulary Multi-view Indoor 3D Object Detection without Human Annotations\u300b\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\uff0c\u7279\u522b\u662f3D\u76ee\u6807\u68c0\u6d4b\u65b9\u9762\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u5f15\u4eba\u6ce8\u76ee\u7684\u65b0\u65b9\u6cd5\u3002\u4ee5\u4e0b\u662f\u8be6\u7ec6\u5206\u6790\uff1a 1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u660e\u6458\u8981 (Concise Summary) OpenM3D\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5f00\u653e\u8bcd\u6c47\uff08OV\uff09\u591a\u89c6\u89d2\u5ba4\u51853D\u76ee\u6807\u68c0\u6d4b\u5668\uff0c\u5176\u6838\u5fc3\u521b\u65b0\u5728\u4e8e \u65e0\u9700\u4eba\u5de5\u6807\u6ce8 \u5373\u53ef\u8fdb\u884c\u8bad\u7ec3\u3002\u8be5\u65b9\u6cd5\u662f\u4e00\u4e2a\u5355\u9636\u6bb5\u68c0\u6d4b\u5668\uff0c\u901a\u8fc7\u7ed3\u5408\u56fe\u5d4c\u5165\u6280\u672f\u751f\u6210\u9ad8\u8d28\u91cf\u76843D\u4f2a\u6846\uff0c\u5e76\u5229\u7528\u591a\u6837\u5316\u7684CLIP\u7279\u5f81\u8fdb\u884c\u4f53\u7d20\u8bed\u4e49\u5bf9\u9f50\uff0c\u4ece\u800c\u5728ScanNet200\u548cARKitScenes\u7b49\u5ba4\u5185\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u51c6\u786e\u6027\u548c\u901f\u5ea6\u3002 2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u8bba (Key Innovation or Methodological Approach) OpenM3D\u7684\u5173\u952e\u521b\u65b0\u5728\u4e8e\u5176 \u65e0\u4eba\u5de5\u6807\u6ce8\u7684\u8bad\u7ec3\u8303\u5f0f \uff0c\u4ee5\u53ca\u5b9e\u73b0\u8fd9\u4e00\u8303\u5f0f\u7684\u4e24\u4e2a\u6838\u5fc3\u6280\u672f\uff1a \u65e0\u76d1\u7763/\u5f31\u76d1\u7763\u76843D\u4f2a\u6846\u751f\u6210 (3D Pseudo Box Generation without Human Annotations): \u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u5d4c\u5165\uff08graph embedding\uff09\u6280\u672f\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u5c062D\u56fe\u50cf\u4e2d\u7684\u5206\u5272\uff08segments\uff09\u7ec4\u5408\u6210\u8fde\u8d2f\u76843D\u7ed3\u6784\uff0c\u5e76\u4ece\u4e2d\u751f\u6210\u9ad8\u8d28\u91cf\u76843D\u4f2a\u6846\u3002\u8fd9\u4e9b\u4f2a\u6846\u4f5c\u4e3a\u7c7b\u65e0\u5173\u76843D\u5b9a\u4f4d\u635f\u5931\uff08class-agnostic 3D localization loss\uff09\u7684\u8bad\u7ec3\u76ee\u6807\uff0c\u6781\u5927\u5730\u7f13\u89e3\u4e86\u5bf9\u6602\u8d353D\u6846\u6807\u6ce8\u7684\u9700\u6c42\u3002 \u57fa\u4e8eCLIP\u7279\u5f81\u7684\u4f53\u7d20\u8bed\u4e49\u5bf9\u9f50 (Voxel-Semantic Alignment with Diverse CLIP Features): \u4e3a\u4e86\u652f\u6301\u5f00\u653e\u8bcd\u6c47\u68c0\u6d4b\uff0cOpenM3D\u5229\u7528\u9884\u8bad\u7ec3\u7684CLIP\u6a21\u578b\u63d0\u53d62D\u5206\u5272\u7684\u591a\u6837\u5316\u7279\u5f81\uff0c\u5e76\u5c06\u5176\u4e0e\u5bf9\u5e94\u76843D\u4f53\u7d20\u7279\u5f81\u8fdb\u884c\u5bf9\u9f50\u3002\u8fd9\u79cd\u4f53\u7d20-\u8bed\u4e49\u5bf9\u9f50\u635f\u5931\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u7406\u89e3\u548c\u68c0\u6d4b\u8bad\u7ec3\u4e2d\u672a\u89c1\u8fc7\u7684\u7c7b\u522b\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u5f00\u653e\u8bcd\u6c47\u80fd\u529b\u3002 \u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u662f\u4e00\u4e2a \u9ad8\u6548\u7684\u5355\u9636\u6bb5\u68c0\u6d4b\u5668 \uff0c\u901a\u8fc7\u5de7\u5999\u5730\u7ed3\u54082D\u8bf1\u5bfc\u7684\u4f53\u7d20\u7279\u5f81\uff08\u4eceImGeoNet\u6a21\u578b\uff09\u548c\u4e0a\u8ff0\u4e24\u79cd\u635f\u5931\uff0c\u5b9e\u73b0\u4e86\u5728\u51c6\u786e\u6027\u548c\u901f\u5ea6\u4e0a\u7684SOTA\u8868\u73b0\u3002 3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd (Potential Impact on the Field) \u964d\u4f4e3D\u68c0\u6d4b\u7684\u6807\u6ce8\u6210\u672c: 3D\u76ee\u6807\u68c0\u6d4b\uff0c\u5c24\u5176\u662f\u5ba4\u5185\u573a\u666f\uff0c\u5176\u4eba\u5de5\u6807\u6ce8\u6210\u672c\u6781\u9ad8\u3002OpenM3D\u901a\u8fc7\u5b8c\u5168\u6d88\u9664\u5bf93D\u6846\u548c\u7c7b\u522b\u7684\u4eba\u5de5\u6807\u6ce8\uff0c\u6781\u5927\u5730\u964d\u4f4e\u4e86\u7814\u7a76\u548c\u5e94\u75283D\u68c0\u6d4b\u7684\u95e8\u69db\uff0c\u6709\u671b\u52a0\u901f\u8be5\u9886\u57df\u7684\u53d1\u5c55\u548c\u5b9e\u9645\u90e8\u7f72\u3002 \u63a8\u52a8\u5f00\u653e\u8bcd\u6c473D\u68c0\u6d4b\u7684\u53d1\u5c55: \u5f00\u653e\u8bcd\u6c47\u80fd\u529b\u662f\u672a\u6765AI\u7cfb\u7edf\u7684\u91cd\u8981\u7279\u5f81\u3002OpenM3D\u5728\u56fe\u50cf\u57fa\uff08image-based\uff093D\u5f00\u653e\u8bcd\u6c47\u68c0\u6d4b\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u8bc1\u660e\u4e862D\u9884\u8bad\u7ec3\u6a21\u578b\uff08\u5982CLIP\uff09\u57283D\u4efb\u52a1\u4e2d\u7684\u5de8\u5927\u6f5c\u529b\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u3002 \u4fc3\u8fdb2D\u4e0e3D\u89c6\u89c9\u7684\u878d\u5408: \u8be5\u5de5\u4f5c\u6709\u6548\u5730\u5c06\u5f3a\u5927\u76842D\u5206\u5272\u548c\u8bed\u4e49\u7406\u89e3\u80fd\u529b\uff08\u901a\u8fc7CLIP\uff09\u8fc1\u79fb\u52303D\u7a7a\u95f4\uff0c\u8fdb\u4e00\u6b65\u5f25\u5408\u4e862D\u548c3D\u89c6\u89c9\u4efb\u52a1\u4e4b\u95f4\u7684\u9e3f\u6c9f\uff0c\u4e3a\u6784\u5efa\u66f4\u901a\u7528\u3001\u66f4\u5f3a\u5927\u7684\u89c6\u89c9\u611f\u77e5\u7cfb\u7edf\u63d0\u4f9b\u4e86\u8303\u4f8b\u3002 \u63d0\u5347\u5b9e\u65f63D\u611f\u77e5\u7684\u6548\u7387: \u4f5c\u4e3a\u5355\u9636\u6bb5\u68c0\u6d4b\u5668\uff0cOpenM3D\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e860.3\u79d2/\u573a\u666f\u7684\u63a8\u7406\u901f\u5ea6\uff0c\u8fd9\u5bf9\u4e8e\u673a\u5668\u4eba\u3001AR/VR\u7b49\u9700\u8981\u5b9e\u65f63D\u611f\u77e5\u7684\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002 4. \u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 (Related Areas or Applications) \u673a\u5668\u4eba\u5b66\u4e0e\u81ea\u4e3b\u5bfc\u822a: \u5ba4\u5185\u670d\u52a1\u673a\u5668\u4eba\u3001\u65e0\u4eba\u673a\u7b49\u9700\u8981\u5728\u590d\u6742\u5ba4\u5185\u73af\u5883\u4e2d\u8fdb\u884c\u7269\u4f53\u8bc6\u522b\u3001\u6293\u53d6\u548c\u907f\u969c\uff0cOpenM3D\u80fd\u63d0\u4f9b\u9ad8\u6548\u3001\u7075\u6d3b\u76843D\u611f\u77e5\u80fd\u529b\u3002 \u589e\u5f3a\u73b0\u5b9e (AR) / \u865a\u62df\u73b0\u5b9e (VR): \u5b9e\u65f6\u7406\u89e3\u7528\u6237\u5468\u56f4\u76843D\u73af\u5883\u548c\u7269\u4f53\uff0c\u5b9e\u73b0\u865a\u62df\u5185\u5bb9\u7684\u7cbe\u786e\u653e\u7f6e\u548c\u4ea4\u4e92\uff0c\u63d0\u5347\u6c89\u6d78\u611f\u3002 \u5ba4\u5185\u6d4b\u7ed8\u4e0e\u6570\u5b57\u5b6a\u751f (Digital Twins): \u81ea\u52a8\u6784\u5efa\u5177\u6709\u8bed\u4e49\u4fe1\u606f\u7684\u5ba4\u51853D\u6a21\u578b\uff0c\u7528\u4e8e\u8bbe\u65bd\u7ba1\u7406\u3001\u7a7a\u95f4\u89c4\u5212\u7b49\u3002 \u667a\u80fd\u5bb6\u5c45\u4e0e\u667a\u6167\u57ce\u5e02: \u5b9e\u73b0\u5bf9\u5ba4\u5185\u7269\u54c1\u7684\u667a\u80fd\u8bc6\u522b\u548c\u7ba1\u7406\uff0c\u6784\u5efa\u66f4\u667a\u80fd\u3001\u66f4\u4eba\u6027\u5316\u7684\u5c45\u4f4f\u548c\u5de5\u4f5c\u73af\u5883\u3002 \u5f31\u76d1\u7763/\u81ea\u76d1\u7763\u5b66\u4e60: \u4f5c\u4e3a\u65e0\u4eba\u5de5\u6807\u6ce8\u5b66\u4e60\u7684\u6210\u529f\u6848\u4f8b\uff0c\u8be5\u7814\u7a76\u5bf9\u66f4\u5e7f\u6cdb\u7684\u5f31\u76d1\u7763\u548c\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u5177\u6709\u501f\u9274\u610f\u4e49\u3002 5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u7684\u5c40\u9650\u6027 (Inferred Limitations) \u4ec5\u9650\u4e8e\u5ba4\u5185\u573a\u666f (Indoor-specific): \u6458\u8981\u660e\u786e\u6307\u51fa\u662f\u201c\u5ba4\u51853D\u76ee\u6807\u68c0\u6d4b\u201d\uff0c\u8fd9\u610f\u5473\u7740\u5176\u65b9\u6cd5\u53ef\u80fd\u9488\u5bf9\u5ba4\u5185\u73af\u5883\u7684\u7279\u6027\u8fdb\u884c\u4e86\u4f18\u5316\uff0c\u4e0d\u4e00\u5b9a\u80fd\u76f4\u63a5\u6cdb\u5316\u5230\u5ba4\u5916\u6216\u66f4\u590d\u6742\u7684\u5f00\u653e\u573a\u666f\uff0c\u56e0\u4e3a\u5ba4\u5916\u573a\u666f\u7684\u7269\u4f53\u7c7b\u578b\u3001\u5c3a\u5ea6\u3001\u5149\u7167\u548c\u906e\u6321\u6a21\u5f0f\u5dee\u5f02\u5de8\u5927\u3002 \u8bad\u7ec3\u9636\u6bb5\u5bf9RGB-D\u6570\u636e\u548c\u76f8\u673a\u59ff\u6001\u7684\u4f9d\u8d56 (Reliance on Posed RGB-D during Training): \u5c3d\u7ba1\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\uff0c\u4f46\u6458\u8981\u63d0\u5230\u8bad\u7ec3\u9075\u5faaOV-3DET\u7684\u8bbe\u7f6e\uff0c\u5373\u201cposed RGB-D images are given\u201d\u3002\u8fd9\u610f\u5473\u7740\u5728\u8bad\u7ec3\u65f6\uff0c\u6a21\u578b\u9700\u8981\u6df1\u5ea6\u4fe1\u606f\u548c\u7cbe\u786e\u7684\u76f8\u673a\u59ff\u6001\u3002\u867d\u7136\u63a8\u7406\u65f6\u53ea\u9700\u591a\u89c6\u89d2\u56fe\u50cf\uff0c\u4f46\u83b7\u53d6\u9ad8\u8d28\u91cf\u7684RGB-D\u6570\u636e\u548c\u59ff\u6001\u672c\u8eab\u4e5f\u53ef\u80fd\u662f\u4e00\u4e2a\u6311\u6218\u3002 \u5bf92D\u5206\u5272\u548cCLIP\u6a21\u578b\u8d28\u91cf\u7684\u4f9d\u8d56 (Dependency on 2D Segmentation and CLIP Quality): \u4f2a\u6846\u751f\u6210\u4f9d\u8d56\u4e8e2D\u5206\u5272\uff0c\u5f00\u653e\u8bcd\u6c47\u80fd\u529b\u4f9d\u8d56\u4e8e\u9884\u8bad\u7ec3CLIP\u6a21\u578b\u7684\u7279\u5f81\u3002\u5982\u679c\u5e95\u5c42\u76842D\u5206\u5272\u6216CLIP\u6a21\u578b\u5728\u7279\u5b9a\u573a\u666f\u6216\u7269\u4f53\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u53ef\u80fd\u4f1a\u76f4\u63a5\u5f71\u54cdOpenM3D\u7684\u6027\u80fd\u3002 \u201c\u8fde\u8d2f3D\u7ed3\u6784\u201d\u7684\u5047\u8bbe (Assumption of \"Coherent 3D Structures\"): \u4f2a\u6846\u751f\u6210\u65b9\u6cd5\u901a\u8fc7\u56fe\u5d4c\u5165\u5c062D\u5206\u5272\u7ec4\u5408\u6210\u201c\u8fde\u8d2f\u76843D\u7ed3\u6784\u201d\u3002\u5bf9\u4e8e\u9ad8\u5ea6\u906e\u6321\u3001\u788e\u7247\u5316\u6216\u5f62\u72b6\u4e0d\u89c4\u5219\u7684\u7269\u4f53\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u4f2a\u6846\u53ef\u80fd\u4ecd\u9762\u4e34\u6311\u6218\u3002 \u591a\u89c6\u89d2\u8f93\u5165\u8981\u6c42 (Multi-view Input Requirement): \u5c3d\u7ba1\u63a8\u7406\u901f\u5ea6\u5feb\uff0c\u4f46\u5b83\u9700\u8981\u201cmulti-view images for input\u201d\u3002\u5bf9\u4e8e\u5355\u89c6\u89d2\u6216\u6781\u5c11\u89c6\u89d2\u8f93\u5165\u7684\u573a\u666f\uff0c\u5176\u9002\u7528\u6027\u53ef\u80fd\u53d7\u9650\u3002 Key Findings: We introduce OpenM3D, a novel open-vocabulary multi-view indoor 3D object detector trained without human annotations. We propose a 3D Pseudo Box Generation method using a graph embedding technique that combines 2D segments into coherent 3D structures. Links: PDF arXiv EffNetViTLoRA: An Efficient Hybrid Deep Learning Approach for Alzheimer's Disease Diagnosis Authors: Mahdieh Behjat Khatooni, Mohsen Soryani Published: 2025-08-26 Categories: cs.CV Abstract: Alzheimer's disease (AD) is one of the most prevalent neurodegenerative disorders worldwide. As it progresses, it leads to the deterioration of cognitive functions. Since AD is irreversible, early diagnosis is crucial for managing its progression. Mild Cognitive Impairment (MCI) represents an intermediate stage between Cognitively Normal (CN) individuals and those with AD, and is considered a transitional phase from normal cognition to Alzheimer's disease. Diagnosing MCI is particularly challenging due to the subtle differences between adjacent diagnostic categories. In this study, we propose EffNetViTLoRA, a generalized end-to-end model for AD diagnosis using the whole Alzheimer's Disease Neuroimaging Initiative (ADNI) Magnetic Resonance Imaging (MRI) dataset. Our model integrates a Convolutional Neural Network (CNN) with a Vision Transformer (ViT) to capture both local and global features from MRI images. Unlike previous studies that rely on limited subsets of data, our approach is trained on the full T1-weighted MRI dataset from ADNI, resulting in a more robust and unbiased model. This comprehensive methodology enhances the model's clinical reliability. Furthermore, fine-tuning large pretrained models often yields suboptimal results when source and target dataset domains differ. To address this, we incorporate Low-Rank Adaptation (LoRA) to effectively adapt the pretrained ViT model to our target domain. This method enables efficient knowledge transfer and reduces the risk of overfitting. Our model achieves a classification accuracy of 92.52% and an F1-score of 92.76% across three diagnostic categories: AD, MCI, and CN for full ADNI dataset. Analysis: \u4f5c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u4e13\u5bb6\uff0c\u6211\u5bf9\u8fd9\u7bc7\u8bba\u6587\u6458\u8981\u8fdb\u884c\u5982\u4e0b\u5206\u6790\uff1a \u8bba\u6587\u6807\u9898\uff1a EffNetViTLoRA: An Efficient Hybrid Deep Learning Approach for Alzheimer's Disease Diagnosis \u4f5c\u8005\uff1a Mahdieh Behjat Khatooni, Mohsen Soryani \u7c7b\u522b\uff1a cs.CV \u53d1\u8868\u65e5\u671f\uff1a 2025-08-26 1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u6d01\u603b\u7ed3 (Concise Summary) \u672c\u6587\u63d0\u51faEffNetViTLoRA\uff0c\u4e00\u4e2a\u7528\u4e8e\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\uff08AD\uff09\u8bca\u65ad\u7684\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3002\u8be5\u6a21\u578b\u7ed3\u5408CNN\u548cVision Transformer\uff08ViT\uff09\u4ee5\u6355\u83b7MRI\u56fe\u50cf\u7684\u5c40\u90e8\u548c\u5168\u5c40\u7279\u5f81\uff0c\u5e76\u5229\u7528LoRA\u6280\u672f\u9ad8\u6548\u5730\u9002\u5e94\u9884\u8bad\u7ec3ViT\u3002\u5176\u4e3b\u8981\u8d21\u732e\u5728\u4e8e\u9996\u6b21\u5728\u5b8c\u6574\u7684ADNI T1\u52a0\u6743MRI\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u548c\u8bc4\u4f30\uff0c\u5b9e\u73b0\u4e86\u9ad8\u51c6\u786e\u7387\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u4e34\u5e8a\u53ef\u9760\u6027\u3002 2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u5b66\u65b9\u6cd5 (Key Innovation or Methodological Approach) \u8be5\u7814\u7a76\u7684\u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u5176\u201cEffNetViTLoRA\u201d\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u53ca\u5176\u8bad\u7ec3\u7b56\u7565\uff1a \u6df7\u5408CNN-ViT\u67b6\u6784\uff1a \u6a21\u578b\u7ed3\u5408\u4e86\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u548cVision Transformer\uff08ViT\uff09\u7684\u4f18\u52bf\u3002CNN\uff08\u540d\u79f0\u6697\u793a\u53ef\u80fd\u57fa\u4e8eEfficientNet\uff09\u64c5\u957f\u6355\u6349MRI\u56fe\u50cf\u7684\u5c40\u90e8\u7cbe\u7ec6\u7279\u5f81\uff0c\u800cViT\u5219\u80fd\u6709\u6548\u63d0\u53d6\u5168\u5c40\u4e0a\u4e0b\u6587\u4fe1\u606f\u548c\u957f\u8ddd\u79bb\u4f9d\u8d56\u5173\u7cfb\uff0c\u4ece\u800c\u5b9e\u73b0\u5bf9\u533b\u5b66\u56fe\u50cf\u591a\u5c3a\u5ea6\u7279\u5f81\u7684\u5168\u9762\u7406\u89e3\u3002 \u5168ADNI\u6570\u636e\u96c6\u8bad\u7ec3\uff1a \u4e0e\u4ee5\u5f80\u7814\u7a76\u901a\u5e38\u4f9d\u8d56\u6570\u636e\u5b50\u96c6\u4e0d\u540c\uff0c\u8be5\u65b9\u6cd5\u5728\u5b8c\u6574\u7684\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u795e\u7ecf\u5f71\u50cf\u5b66\u5021\u8bae\uff08ADNI\uff09T1\u52a0\u6743MRI\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002\u8fd9\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\uff0c\u51cf\u5c11\u4e86\u6f5c\u5728\u7684\u6570\u636e\u9009\u62e9\u504f\u5dee\uff0c\u4f7f\u5176\u66f4\u63a5\u8fd1\u771f\u5b9e\u4e16\u754c\u7684\u4e34\u5e8a\u5e94\u7528\u3002 \u5f15\u5165LoRA\u8fdb\u884c\u9ad8\u6548\u5fae\u8c03\uff1a \u9488\u5bf9\u5927\u578b\u9884\u8bad\u7ec3ViT\u6a21\u578b\u5728\u6e90\u57df\u548c\u76ee\u6807\u57df\uff08\u5982\u901a\u7528\u56fe\u50cf\u4e0e\u533b\u5b66\u56fe\u50cf\uff09\u5dee\u5f02\u8f83\u5927\u65f6\u5fae\u8c03\u6548\u679c\u4e0d\u4f73\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u8be5\u7814\u7a76\u521b\u9020\u6027\u5730\u5f15\u5165\u4e86\u4f4e\u79e9\u9002\u5e94\uff08Low-Rank Adaptation, LoRA\uff09\u6280\u672f\u3002LoRA\u901a\u8fc7\u5728\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u7279\u5b9a\u5c42\u6ce8\u5165\u5c11\u91cf\u53ef\u8bad\u7ec3\u7684\u4f4e\u79e9\u77e9\u9635\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u77e5\u8bc6\u8fc1\u79fb\uff0c\u540c\u65f6\u6709\u6548\u907f\u514d\u4e86\u8fc7\u62df\u5408\uff0c\u5e76\u5927\u5e45\u964d\u4f4e\u4e86\u5fae\u8c03\u6240\u9700\u7684\u8ba1\u7b97\u8d44\u6e90\u3002 3. \u5bf9\u8be5\u9886\u57df\u7684\u6f5c\u5728\u5f71\u54cd (Potential Impact on the Field) \u63d0\u5347AD\u65e9\u671f\u8bca\u65ad\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\uff1a \u5c24\u5176\u662f\u5728\u533a\u5206MCI\u8fd9\u4e00\u6311\u6218\u6027\u9636\u6bb5\uff0c\u9ad8\u51c6\u786e\u7387\uff0892.52%\uff09\u548cF1\u5206\u6570\uff0892.76%\uff09\u7684\u6a21\u578b\u80fd\u4e3a\u4e34\u5e8a\u533b\u751f\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u8f85\u52a9\u8bca\u65ad\u5de5\u5177\uff0c\u4ece\u800c\u5b9e\u73b0\u65e9\u671f\u5e72\u9884\u548c\u75be\u75c5\u7ba1\u7406\uff0c\u5ef6\u7f13\u75be\u75c5\u8fdb\u5c55\u3002 \u63a8\u52a8\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u6df7\u5408\u6a21\u578b\u548c\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u7684\u5e94\u7528\uff1a \u7ed3\u5408CNN\u548cViT\u7684\u4f18\u52bf\uff0c\u5e76\u5229\u7528LoRA\u7b49\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u6280\u672f\uff0c\u4e3a\u5904\u7406\u5927\u89c4\u6a21\u3001\u590d\u6742\u533b\u5b66\u56fe\u50cf\u6570\u636e\u63d0\u4f9b\u4e86\u65b0\u7684\u8303\u5f0f\uff0c\u5c24\u5176\u662f\u5728\u8d44\u6e90\u53d7\u9650\u6216\u9700\u8981\u5feb\u901f\u8fed\u4ee3\u7684\u573a\u666f\u3002 \u5efa\u7acb\u66f4\u5177\u6cdb\u5316\u80fd\u529b\u7684\u57fa\u51c6\u6a21\u578b\uff1a \u5728\u5b8c\u6574ADNI\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\uff0c\u5176\u9c81\u68d2\u6027\u548c\u65e0\u504f\u6027\u4f7f\u5176\u6709\u671b\u6210\u4e3a\u672a\u6765AD\u8bca\u65ad\u7814\u7a76\u7684\u6709\u529b\u57fa\u51c6\uff0c\u4fc3\u8fdb\u8be5\u9886\u57df\u7814\u7a76\u7684\u6807\u51c6\u5316\u548c\u8fdb\u6b65\u3002 4. \u53ef\u80fd\u53d7\u76ca\u4e8e\u8fd9\u9879\u7814\u7a76\u7684\u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 (Related Areas or Applications) \u5176\u4ed6\u795e\u7ecf\u9000\u884c\u6027\u75be\u75c5\u7684\u8bca\u65ad\uff1a \u5982\u5e15\u91d1\u68ee\u75c5\u3001\u591a\u53d1\u6027\u786c\u5316\u75c7\u7b49\uff0c\u8fd9\u4e9b\u75be\u75c5\u4e5f\u4f9d\u8d56MRI\u56fe\u50cf\u8fdb\u884c\u8bca\u65ad\uff0c\u4e14\u53ef\u80fd\u9762\u4e34\u7c7b\u4f3c\u7684\u65e9\u671f\u8bca\u65ad\u6311\u6218\u548c\u6570\u636e\u7279\u6027\u3002 \u5176\u4ed6\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4efb\u52a1\uff1a \u80bf\u7624\u68c0\u6d4b\u3001\u5668\u5b98\u5206\u5272\u3001\u75be\u75c5\u5206\u671f\u7b49\uff0c\u51e1\u662f\u9700\u8981\u540c\u65f6\u6355\u6349\u5c40\u90e8\u7cbe\u7ec6\u75c5\u7076\u548c\u5168\u5c40\u7ed3\u6784\u4fe1\u606f\uff0c\u5e76\u53ef\u80fd\u6d89\u53ca\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u6a21\u578b\u5fae\u8c03\u7684\u533b\u5b66\u56fe\u50cf\u4efb\u52a1\uff0c\u90fd\u53ef\u4ee5\u501f\u9274\u8fd9\u79cd\u6df7\u5408\u67b6\u6784\u548cLoRA\u7b56\u7565\u3002 \u901a\u7528\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u4e2d\u7684\u5c0f\u6837\u672c\u5b66\u4e60\u4e0e\u9886\u57df\u9002\u5e94\uff1a EffNetViTLoRA\u4e2dLoRA\u7684\u5e94\u7528\uff0c\u4e3a\u5728\u6570\u636e\u91cf\u6709\u9650\u6216\u5b58\u5728\u663e\u8457\u9886\u57df\u5dee\u5f02\u7684\u60c5\u51b5\u4e0b\uff0c\u9ad8\u6548\u5730\u5c06\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\u9002\u5e94\u5230\u7279\u5b9a\u4efb\u52a1\u63d0\u4f9b\u4e86\u901a\u7528\u89e3\u51b3\u65b9\u6848\u3002 5. \u4ece\u6458\u8981\u4e2d\u53ef\u4ee5\u63a8\u65ad\u51fa\u7684\u4efb\u4f55\u5c40\u9650\u6027 (Limitations that can be inferred from the abstract) \u7f3a\u4e4f\u5916\u90e8\u6570\u636e\u96c6\u9a8c\u8bc1\uff1a \u5c3d\u7ba1\u5728\u5b8c\u6574\u7684ADNI\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bad\u7ec3\uff0c\u4f46\u6a21\u578b\u5728\u6765\u81ea\u4e0d\u540c\u533b\u9662\u3001\u4e0d\u540c\u626b\u63cf\u4eea\u6216\u4e0d\u540c\u4eba\u7fa4\u7684\u72ec\u7acb\u5916\u90e8\u6570\u636e\u96c6\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u4ecd\u6709\u5f85\u9a8c\u8bc1\u3002\u8fd9\u5bf9\u4e8e\u8bc4\u4f30\u5176\u771f\u6b63\u7684\u4e34\u5e8a\u9002\u7528\u6027\u81f3\u5173\u91cd\u8981\u3002 \u6a21\u578b\u590d\u6742\u6027\u4e0e\u53ef\u89e3\u91ca\u6027\uff1a \u6df7\u5408CNN-ViT\u67b6\u6784\u867d\u7136\u5f3a\u5927\uff0c\u4f46\u4e5f\u53ef\u80fd\u589e\u52a0\u6a21\u578b\u7684\u590d\u6742\u6027\uff0c\u964d\u4f4e\u5176\u51b3\u7b56\u8fc7\u7a0b\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u8fd9\u5728\u4e34\u5e8a\u8bca\u65ad\u4e2d\u662f\u4e00\u4e2a\u91cd\u8981\u8003\u91cf\u3002\u6458\u8981\u4e2d\u672a\u63d0\u53ca\u4efb\u4f55\u5173\u4e8e\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u7684\u65b9\u6cd5\u3002 \u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\uff1a \u5c3d\u7ba1LoRA\u964d\u4f4e\u4e86\u5fae\u8c03\u6210\u672c\uff0c\u4f46\u8bad\u7ec3\u4e00\u4e2a\u5728\u5b8c\u6574ADNI\u6570\u636e\u96c6\u4e0a\u7684\u6df7\u5408CNN-ViT\u6a21\u578b\uff0c\u5176\u521d\u59cb\u8bad\u7ec3\u9636\u6bb5\u53ef\u80fd\u4ecd\u9700\u8981\u663e\u8457\u7684\u8ba1\u7b97\u8d44\u6e90\u3002 \u672a\u660e\u786e\u7684CNN\u7ec4\u4ef6\u7ec6\u8282\uff1a \u6458\u8981\u4e2d\u63d0\u5230\u201cEffNetViTLoRA\u201d\uff0c\u6697\u793aCNN\u90e8\u5206\u53ef\u80fd\u57fa\u4e8eEfficientNet\uff0c\u4f46\u672a\u8be6\u7ec6\u8bf4\u660e\u5177\u4f53\u662f\u54ea\u79cdEfficientNet\u53d8\u4f53\u4ee5\u53ca\u5176\u4e0eViT\u7684\u96c6\u6210\u65b9\u5f0f\uff08\u4f8b\u5982\uff0c\u662f\u4e32\u8054\u3001\u5e76\u884c\u8fd8\u662f\u7279\u5f81\u878d\u5408\uff09\u3002 \u7f3a\u4e4f\u4e0e\u73b0\u6709SOTA\u65b9\u6cd5\u7684\u76f4\u63a5\u91cf\u5316\u6bd4\u8f83\uff1a \u6458\u8981\u5f3a\u8c03\u4e86\u5728\u5b8c\u6574ADNI\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u4f18\u52bf\uff0c\u4f46\u672a\u63d0\u4f9b\u4e0e\u5f53\u524d\u5728\u8be5\u6570\u636e\u96c6\u4e0a\u6216\u7c7b\u4f3c\u4efb\u52a1\u4e0a\u7684\u6700\u5148\u8fdb\uff08SOTA\uff09\u65b9\u6cd5\u7684\u76f4\u63a5\u6027\u80fd\u5bf9\u6bd4\u6570\u636e\uff0c\u4f7f\u5f9792.52%\u7684\u51c6\u786e\u7387\u7f3a\u4e4f\u4e00\u4e2a\u660e\u786e\u7684\u53c2\u7167\u7cfb\u6765\u8bc4\u4f30\u5176\u76f8\u5bf9\u4f18\u8d8a\u6027\u3002 Key Findings: In this study, we propose EffNetViTLoRA, a generalized end-to-end model for AD diagnosis using the whole Alzheimer's Disease Neuroimaging Initiative (ADNI) Magnetic Resonance Imaging (MRI) dataset. Links: PDF arXiv RoofSeg: An edge-aware transformer-based network for end-to-end roof plane segmentation Authors: Siyuan You, Guozheng Xu, Pengwei Zhou, Qiwen Jin, Jian Yao, Li Li Published: 2025-08-26 Categories: cs.CV, cs.AI Abstract: Roof plane segmentation is one of the key procedures for reconstructing three-dimensional (3D) building models at levels of detail (LoD) 2 and 3 from airborne light detection and ranging (LiDAR) point clouds. The majority of current approaches for roof plane segmentation rely on the manually designed or learned features followed by some specifically designed geometric clustering strategies. Because the learned features are more powerful than the manually designed features, the deep learning-based approaches usually perform better than the traditional approaches. However, the current deep learning-based approaches have three unsolved problems. The first is that most of them are not truly end-to-end, the plane segmentation results may be not optimal. The second is that the point feature discriminability near the edges is relatively low, leading to inaccurate planar edges. The third is that the planar geometric characteristics are not sufficiently considered to constrain the network training. To solve these issues, a novel edge-aware transformer-based network, named RoofSeg, is developed for segmenting roof planes from LiDAR point clouds in a truly end-to-end manner. In the RoofSeg, we leverage a transformer encoder-decoder-based framework to hierarchically predict the plane instance masks with the use of a set of learnable plane queries. To further improve the segmentation accuracy of edge regions, we also design an Edge-Aware Mask Module (EAMM) that sufficiently incorporates planar geometric prior of edges to enhance its discriminability for plane instance mask refinement. In addition, we propose an adaptive weighting strategy in the mask loss to reduce the influence of misclassified points, and also propose a new plane geometric loss to constrain the network training. Analysis: \u4f5c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u4e13\u5bb6\uff0c\u6211\u5bf9\u8fd9\u7bc7\u5173\u4e8eRoofSeg\u7684\u8bba\u6587\u6458\u8981\u8fdb\u884c\u5982\u4e0b\u5206\u6790\uff1a \u8bba\u6587\u6458\u8981\u5206\u6790\uff1aRoofSeg: An edge-aware transformer-based network for end-to-end roof plane segmentation 1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u660e\u603b\u7ed3 (Concise Summary): \u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRoofSeg\u7684\u8fb9\u7f18\u611f\u77e5\u3001\u57fa\u4e8eTransformer\u7684\u7aef\u5230\u7aef\u7f51\u7edc\uff0c\u7528\u4e8e\u4eceLiDAR\u70b9\u4e91\u4e2d\u8fdb\u884c\u5c4b\u9876\u5e73\u9762\u5206\u5272\u3002\u5b83\u901a\u8fc7\u7ed3\u5408Transformer\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\u3001\u4e13\u95e8\u8bbe\u8ba1\u7684\u8fb9\u7f18\u611f\u77e5\u63a9\u7801\u6a21\u5757\uff08EAMM\uff09\u4ee5\u53ca\u521b\u65b0\u7684\u635f\u5931\u51fd\u6570\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u975e\u7aef\u5230\u7aef\u3001\u8fb9\u7f18\u7cbe\u5ea6\u4f4e\u548c\u51e0\u4f55\u7ea6\u675f\u4e0d\u8db3\u7b49\u65b9\u9762\u7684\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5c4b\u9876\u5e73\u9762\u5206\u5272\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002 2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u5b66\u65b9\u6cd5 (Key Innovation or Methodological Approach): RoofSeg\u7684\u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u5176 \u7aef\u5230\u7aef\u7684Transformer\u67b6\u6784 \u4e0e\u591a\u9879\u9488\u5bf9\u5c4b\u9876\u5e73\u9762\u5206\u5272\u7279\u5b9a\u6311\u6218\u7684\u8bbe\u8ba1\u76f8\u7ed3\u5408\uff1a \u7aef\u5230\u7aefTransformer\u7f16\u7801\u5668-\u89e3\u7801\u5668\u6846\u67b6\uff1a \u91c7\u7528\u7c7b\u4f3cDETR\u6216Mask2Former\u7684\u8303\u5f0f\uff0c\u5229\u7528\u53ef\u5b66\u4e60\u7684\u5e73\u9762\u67e5\u8be2\uff08learnable plane queries\uff09\u76f4\u63a5\u9884\u6d4b\u5e73\u9762\u5b9e\u4f8b\u63a9\u7801\uff0c\u5b9e\u73b0\u4e86\u771f\u6b63\u7684\u7aef\u5230\u7aef\u5206\u5272\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u540e\u5904\u7406\u5e26\u6765\u7684\u6b21\u4f18\u7ed3\u679c\u3002 \u8fb9\u7f18\u611f\u77e5\u63a9\u7801\u6a21\u5757\uff08Edge-Aware Mask Module, EAMM\uff09\uff1a \u8fd9\u662f\u89e3\u51b3\u8fb9\u7f18\u533a\u57df\u5224\u522b\u529b\u4f4e\u7684\u5173\u952e\u3002EAMM\u5145\u5206\u878d\u5165\u4e86\u5e73\u9762\u7684\u51e0\u4f55\u5148\u9a8c\u77e5\u8bc6\uff0c\u4ee5\u589e\u5f3a\u7f51\u7edc\u5bf9\u8fb9\u7f18\u533a\u57df\u7279\u5f81\u7684\u5224\u522b\u80fd\u529b\uff0c\u4ece\u800c\u63d0\u9ad8\u5e73\u9762\u8fb9\u7f18\u7684\u5206\u5272\u7cbe\u5ea6\u3002 \u521b\u65b0\u7684\u635f\u5931\u51fd\u6570\uff1a \u81ea\u9002\u5e94\u52a0\u6743\u63a9\u7801\u635f\u5931\uff08Adaptive weighting strategy in the mask loss\uff09\uff1a \u65e8\u5728\u51cf\u5c11\u8bef\u5206\u7c7b\u70b9\u5bf9\u635f\u5931\u8ba1\u7b97\u7684\u5f71\u54cd\uff0c\u63d0\u9ad8\u8bad\u7ec3\u7684\u9c81\u68d2\u6027\u3002 \u65b0\u7684\u5e73\u9762\u51e0\u4f55\u635f\u5931\uff08New plane geometric loss\uff09\uff1a \u7528\u4e8e\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u663e\u5f0f\u5730\u7ea6\u675f\u7f51\u7edc\u7684\u8f93\u51fa\uff0c\u4f7f\u5176\u66f4\u597d\u5730\u7b26\u5408\u5e73\u9762\u7684\u51e0\u4f55\u7279\u6027\uff0c\u8fdb\u4e00\u6b65\u63d0\u5347\u5206\u5272\u7ed3\u679c\u7684\u51e0\u4f55\u51c6\u786e\u6027\u3002 3. \u5bf9\u8be5\u9886\u57df\u7684\u6f5c\u5728\u5f71\u54cd (Potential Impact on the Field): \u8be5\u7814\u7a76\u5bf9\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\uff0c\u7279\u522b\u662f\u4e09\u7ef4\u91cd\u5efa\u548c\u70b9\u4e91\u5904\u7406\uff0c\u5177\u6709\u663e\u8457\u7684\u6f5c\u5728\u5f71\u54cd\uff1a \u63a8\u52a8\u9ad8\u7cbe\u5ea6\u4e09\u7ef4\u57ce\u5e02\u5efa\u6a21\uff1a \u4f5c\u4e3aLoD 2\u548cLoD 3\u7ea7\u522b\u4e09\u7ef4\u5efa\u7b51\u6a21\u578b\u91cd\u5efa\u7684\u5173\u952e\u6b65\u9aa4\uff0cRoofSeg\u7684\u51fa\u73b0\u5c06\u76f4\u63a5\u63d0\u5347\u5c4b\u9876\u5e73\u9762\u5206\u5272\u7684\u81ea\u52a8\u5316\u6c34\u5e73\u548c\u7cbe\u5ea6\uff0c\u4ece\u800c\u52a0\u901f\u548c\u4f18\u5316\u9ad8\u7cbe\u5ea6\u57ce\u5e02\u6570\u5b57\u5b6a\u751f\u548cBIM\u6a21\u578b\u7684\u6784\u5efa\u3002 \u4e3a\u70b9\u4e91\u5b9e\u4f8b\u5206\u5272\u63d0\u4f9b\u65b0\u8303\u5f0f\uff1a \u5176\u7ed3\u5408Transformer\u3001\u51e0\u4f55\u5148\u9a8c\u548c\u8fb9\u7f18\u611f\u77e5\u673a\u5236\u7684\u7aef\u5230\u7aef\u8bbe\u8ba1\u601d\u8def\uff0c\u53ef\u80fd\u4e3a\u5176\u4ed6\u70b9\u4e91\u4e2d\u7ed3\u6784\u5316\u5bf9\u8c61\uff08\u5982\u5899\u58c1\u3001\u9053\u8def\u3001\u7a97\u6237\u7b49\uff09\u7684\u5b9e\u4f8b\u5206\u5272\u4efb\u52a1\u63d0\u4f9b\u65b0\u7684\u7814\u7a76\u65b9\u5411\u548c\u501f\u9274\u3002 \u63d0\u5347\u70b9\u4e91\u5904\u7406\u7684\u9c81\u68d2\u6027\u4e0e\u51c6\u786e\u6027\uff1a \u901a\u8fc7\u89e3\u51b3\u8fb9\u7f18\u6a21\u7cca\u548c\u51e0\u4f55\u7ea6\u675f\u4e0d\u8db3\u7b49\u957f\u671f\u5b58\u5728\u7684\u75db\u70b9\uff0cRoofSeg\u6709\u671b\u6210\u4e3a\u5c4b\u9876\u5e73\u9762\u5206\u5272\u9886\u57df\u7684\u4e00\u4e2a\u91cd\u8981\u57fa\u51c6\uff0c\u5e76\u542f\u53d1\u66f4\u591a\u9488\u5bf9\u70b9\u4e91\u6570\u636e\u7279\u6027\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8bbe\u8ba1\u3002 4. \u53ef\u80fd\u53d7\u76ca\u4e8e\u8fd9\u9879\u7814\u7a76\u7684\u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 (Related Areas or Applications): \u4e09\u7ef4\u57ce\u5e02\u5efa\u6a21\u4e0e\u6570\u5b57\u5b6a\u751f (3D City Modeling and Digital Twins): \u76f4\u63a5\u53d7\u76ca\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u7cbe\u5ea6\u7684\u5efa\u7b51\u6a21\u578b\u3002 \u57ce\u5e02\u89c4\u5212\u4e0e\u7ba1\u7406 (Urban Planning and Management): \u7cbe\u786e\u7684\u5efa\u7b51\u51e0\u4f55\u4fe1\u606f\u5bf9\u4e8e\u57ce\u5e02\u89c4\u5212\u3001\u5bb9\u79ef\u7387\u8ba1\u7b97\u3001\u65e5\u7167\u5206\u6790\u7b49\u81f3\u5173\u91cd\u8981\u3002 \u707e\u5bb3\u8bc4\u4f30\u4e0e\u5e94\u6025\u54cd\u5e94 (Disaster Assessment and Emergency Response): \u5feb\u901f\u51c6\u786e\u5730\u91cd\u5efa\u53d7\u635f\u5efa\u7b51\u6a21\u578b\u6709\u52a9\u4e8e\u8bc4\u4f30\u707e\u60c5\u548c\u89c4\u5212\u6551\u63f4\u3002 \u80fd\u6e90\u6548\u7387\u5206\u6790\u4e0e\u592a\u9633\u80fd\u6f5c\u529b\u8bc4\u4f30 (Energy Efficiency Analysis and Solar Potential Assessment): \u5c4b\u9876\u7684\u51e0\u4f55\u5f62\u72b6\u548c\u671d\u5411\u662f\u8bc4\u4f30\u592a\u9633\u80fd\u7535\u6c60\u677f\u5b89\u88c5\u6f5c\u529b\u7684\u57fa\u7840\u3002 \u5efa\u7b51\u4fe1\u606f\u6a21\u578b\uff08BIM\uff09\u4e0e\u8d44\u4ea7\u7ba1\u7406 (Building Information Modeling (BIM) and Asset Management): \u5c06LiDAR\u6570\u636e\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u7684BIM\u6a21\u578b\uff0c\u4fbf\u4e8e\u5efa\u7b51\u5168\u751f\u547d\u5468\u671f\u7ba1\u7406\u3002 \u5730\u7406\u4fe1\u606f\u7cfb\u7edf\uff08GIS\uff09\u4e0e\u9065\u611f (Geographic Information Systems (GIS) and Remote Sensing): \u63d0\u4f9b\u66f4\u7cbe\u786e\u7684\u5730\u7406\u7a7a\u95f4\u6570\u636e\u3002 5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u51fa\u7684\u5c40\u9650\u6027 (Limitations that can be inferred from the abstract): \u8ba1\u7b97\u8d44\u6e90\u9700\u6c42 (Computational Resource Requirements): \u57fa\u4e8eTransformer\u7684\u7f51\u7edc\u901a\u5e38\u8ba1\u7b97\u91cf\u8f83\u5927\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u5927\u89c4\u6a21LiDAR\u70b9\u4e91\u65f6\uff0c\u8bad\u7ec3\u548c\u63a8\u7406\u53ef\u80fd\u9700\u8981\u663e\u8457\u7684\u8ba1\u7b97\u8d44\u6e90\u3002 \u6570\u636e\u4f9d\u8d56\u6027 (Data Dependency): Transformer\u6a21\u578b\u901a\u5e38\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u3002\u6458\u8981\u4e2d\u672a\u63d0\u53ca\u6570\u636e\u96c6\u7684\u89c4\u6a21\u548c\u591a\u6837\u6027\uff0c\u5176\u6027\u80fd\u53ef\u80fd\u53d7\u9650\u4e8e\u8bad\u7ec3\u6570\u636e\u7684\u8d28\u91cf\u548c\u8986\u76d6\u8303\u56f4\u3002 \u590d\u6742\u5c4b\u9876\u7ed3\u6784\u7684\u6cdb\u5316\u80fd\u529b (Generalization to Complex Roof Structures): \u8bba\u6587\u5f3a\u8c03\u201c\u5e73\u9762\u5206\u5272\u201d\u548c\u201c\u5e73\u9762\u51e0\u4f55\u7279\u6027\u201d\u3002\u5bf9\u4e8e\u9ad8\u5ea6\u590d\u6742\u3001\u975e\u5e73\u9762\uff08\u5982\u7a79\u9876\u3001\u66f2\u9762\uff09\u6216\u975e\u5e38\u89c4\u7684\u5c4b\u9876\u7ed3\u6784\uff0c\u5176\u201c\u5e73\u9762\u51e0\u4f55\u5148\u9a8c\u201d\u7684\u9002\u7528\u6027\u53ef\u80fd\u53d7\u9650\uff0c\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u6709\u5f85\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u3002 \u5b9e\u65f6\u6027 (Real-time Performance): \u6458\u8981\u672a\u63d0\u53ca\u6a21\u578b\u7684\u63a8\u7406\u901f\u5ea6\uff0c\u5bf9\u4e8e\u9700\u8981\u5b9e\u65f6\u6216\u8fd1\u5b9e\u65f6\u5904\u7406\u7684\u5e94\u7528\u573a\u666f\uff0c\u5176\u6027\u80fd\u53ef\u80fd\u662f\u4e00\u4e2a\u8003\u91cf\u56e0\u7d20\u3002 \u7279\u5b9a\u4e8e\u5c4b\u9876\u7684\u5c40\u9650\u6027 (Roof-Specific Limitations): EAMM\u548c\u5e73\u9762\u51e0\u4f55\u635f\u5931\u662f\u4e3a\u5c4b\u9876\u5e73\u9762\u7279\u6027\u8bbe\u8ba1\u7684\u3002\u5c06\u5176\u65b9\u6cd5\u63a8\u5e7f\u5230\u5176\u4ed6\u7c7b\u578b\u7684\u70b9\u4e91\u5bf9\u8c61\u5206\u5272\u53ef\u80fd\u9700\u8981\u989d\u5916\u7684\u4fee\u6539\u548c\u9002\u5e94\u3002 Key Findings: To solve these issues, a novel edge-aware transformer-based network, named RoofSeg, is developed for segmenting roof planes from LiDAR point clouds in a truly end-to-end manner. In addition, we propose an adaptive weighting strategy in the mask loss to reduce the influence of misclassified points, and also propose a new plane geometric loss to constrain the network training. Links: PDF arXiv","title":"Arxiv Computer Vision Papers - 2025-08-28"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-28/#arxiv-computer-vision-papers-2025-08-28","text":"","title":"Arxiv Computer Vision Papers - 2025-08-28"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-28/#executive-summary","text":"\u597d\u7684\uff0c\u8fd9\u662f\u4e00\u4efd\u9488\u5bf92025\u5e748\u670827\u65e5Arxiv\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u6700\u65b0\u8bba\u6587\u7684\u7b80\u660e\u6267\u884c\u6458\u8981\u3002 Arxiv \u8ba1\u7b97\u673a\u89c6\u89c9\u6bcf\u65e5\u62a5\u544a\u6267\u884c\u6458\u8981 (2025-08-27) \u672c\u62a5\u544a\u6db5\u76d6\u4e86\u4eca\u65e5Arxiv\u4e0a\u53d1\u5e03\u768410\u7bc7\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u8bba\u6587\uff0c\u4e3b\u8981\u805a\u7126\u4e8e \u57fa\u7840\u6a21\u578b\uff08Foundation Models\uff09\u7684\u5e7f\u6cdb\u5e94\u7528\u30013D\u89c6\u89c9\u7684\u8fdb\u6b65\u3001\u5148\u8fdb\u7684\u5206\u5272\u6280\u672f\u4ee5\u53caTransformer\u67b6\u6784\u7684\u6301\u7eed\u521b\u65b0 \u3002 1. \u4e3b\u8981\u8d8b\u52bf\u4e0e\u4e3b\u9898\u6982\u89c8\uff1a \u57fa\u7840\u6a21\u578b\u4e0e\u6cdb\u5316\u80fd\u529b (Foundation Models & Generalization): \u591a\u7bc7\u8bba\u6587\u63a2\u7d22\u5982\u4f55\u5229\u7528\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\uff08\u5982Vision Foundation Models\uff09\u6765\u63d0\u5347\u4efb\u52a1\u7684\u6cdb\u5316\u80fd\u529b\u3001\u51cf\u5c11\u5bf9\u5927\u91cf\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u5e76\u5b9e\u73b0\u5f00\u653e\u8bcd\u6c47\uff08Open-Vocabulary\uff09\u80fd\u529b\u3002\u8fd9\u5728\u76ee\u6807\u68c0\u6d4b\u548c\u7ec6\u7c92\u5ea6\u5206\u7c7b\u4e2d\u8868\u73b0\u5c24\u4e3a\u7a81\u51fa\u3002 \u5148\u8fdb\u7684\u5206\u5272\u6280\u672f (Advanced Segmentation Techniques): \u5206\u5272\u4efb\u52a1\u6301\u7eed\u6f14\u8fdb\uff0c\u5f15\u5165\u4e86\u6269\u6563\u6a21\u578b\u8fdb\u884c\u751f\u6210\u5f0f\u5206\u5272\uff0c\u4ee5\u53ca\u7ed3\u5408\u591a\u6a21\u6001\u3001\u539f\u578b\u5bf9\u9f50\u548c\u8fb9\u7f18\u611f\u77e5\u7b49\u65b9\u6cd5\uff0c\u4ee5\u5e94\u5bf9\u534a\u76d1\u7763\u3001\u533b\u5b66\u56fe\u50cf\u548c\u7279\u5b9a\u573a\u666f\uff08\u5982\u5c4b\u9876\u5e73\u9762\uff09\u7684\u6311\u6218\u3002 \u9c81\u68d2\u76843D\u89c6\u89c9 (Robust 3D Vision): 3D\u76ee\u6807\u68c0\u6d4b\u548c3D\u91cd\u5efa\u662f\u70ed\u95e8\u65b9\u5411\u3002\u7814\u7a76\u4eba\u5458\u81f4\u529b\u4e8e\u63d0\u9ad8\u5355\u76ee\u3001\u591a\u89c6\u89d23D\u68c0\u6d4b\u7684\u6cdb\u5316\u6027\uff0c\u5e76\u5229\u7528\u6269\u6563\u6a21\u578b\u8fdb\u884c\u7a00\u758f\u89c6\u89d2\u4e0b\u76843D\u91cd\u5efa\uff0c\u751a\u81f3\u5b9e\u73b0\u65e0\u4eba\u5de5\u6807\u6ce8\u7684\u5f00\u653e\u8bcd\u6c473D\u68c0\u6d4b\u3002 Transformer\u67b6\u6784\u521b\u65b0\u4e0e\u6548\u7387 (Architectural Innovations & Efficiency): Transformer\u4f5c\u4e3a\u6838\u5fc3\u67b6\u6784\uff0c\u5176\u5185\u90e8\u673a\u5236\uff08\u5982\u4f4d\u7f6e\u7f16\u7801\uff09\u4ecd\u5728\u88ab\u6df1\u5165\u7814\u7a76\u548c\u4f18\u5316\uff0c\u4ee5\u63d0\u9ad8\u5176\u51e0\u4f55\u611f\u77e5\u80fd\u529b\u3002\u540c\u65f6\uff0c\u6df7\u5408\u67b6\u6784\u548c\u9ad8\u6548\u5fae\u8c03\u6280\u672f\uff08\u5982LoRA\uff09\u88ab\u7528\u4e8e\u7279\u5b9a\u5e94\u7528\uff0c\u4ee5\u5e73\u8861\u6027\u80fd\u4e0e\u8ba1\u7b97\u8d44\u6e90\u3002 \u5782\u76f4\u9886\u57df\u5e94\u7528 (Vertical Applications): \u8ba1\u7b97\u673a\u89c6\u89c9\u6280\u672f\u5728\u533b\u7597\u8bca\u65ad\uff08\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\uff09\u3001\u519c\u4e1a\uff08\u82f9\u679c\u6811\u91cd\u5efa\uff09\u3001\u6c7d\u8f66\u5185\u9970\u68c0\u6d4b\u548c\u6606\u866b\u5206\u7c7b\u7b49\u4e13\u4e1a\u9886\u57df\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u5e94\u7528\u6f5c\u529b\u3002 2. \u7279\u522b\u503c\u5f97\u5173\u6ce8\u7684\u8bba\u6587\uff1a OpenM3D: Open Vocabulary Multi-view Indoor 3D Object Detection without Human Annotations (Peng-Hao Hsu et al.) \u521b\u65b0\u70b9: \u5728\u5ba4\u51853D\u76ee\u6807\u68c0\u6d4b\u9886\u57df\u5b9e\u73b0\u4e86\u5f00\u653e\u8bcd\u6c47\u80fd\u529b\uff0c\u4e14 \u65e0\u9700\u4eba\u5de5\u6807\u6ce8 \u3002\u8fd9\u6781\u5927\u5730\u964d\u4f4e\u4e86\u6570\u636e\u6807\u6ce8\u6210\u672c\uff0c\u5e76\u63d0\u5347\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u6027\u548c\u5b9e\u7528\u6027\uff0c\u662f3D\u89c6\u89c9\u9886\u57df\u7684\u4e00\u4e2a\u91cd\u8981\u7a81\u7834\u3002 GS: Generative Segmentation via Label Diffusion (Yuhao Chen et al.) \u521b\u65b0\u70b9: \u5c06\u6269\u6563\u6a21\u578b\u5f15\u5165\u5230\u5206\u5272\u4efb\u52a1\u4e2d\uff0c\u5b9e\u73b0\u4e86\u751f\u6210\u5f0f\u5206\u5272\u3002\u8fd9\u4e3a\u7406\u89e3\u548c\u751f\u6210\u50cf\u7d20\u7ea7\u6807\u7b7e\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9896\u4e14\u5f3a\u5927\u7684\u8303\u5f0f\uff0c\u6709\u671b\u5728\u5404\u79cd\u5206\u5272\u4efb\u52a1\u4e2d\u53d6\u5f97\u4f18\u5f02\u8868\u73b0\u3002 Beyond flattening: a geometrically principled positional encoding for vision transformers with Weierstrass elliptic functions (Zhihang Xin, Xitong Hu, Rui Wang) \u521b\u65b0\u70b9: \u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9b4f\u5c14\u65af\u7279\u62c9\u65af\u692d\u5706\u51fd\u6570\u7684\u51e0\u4f55\u539f\u7406\u4f4d\u7f6e\u7f16\u7801\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u7684\u6241\u5e73\u5316\u5904\u7406\u3002\u8fd9\u4ece\u7406\u8bba\u5c42\u9762\u63d0\u5347\u4e86Vision Transformer\u5bf9\u7a7a\u95f4\u4fe1\u606f\u7684\u611f\u77e5\u548c\u5904\u7406\u80fd\u529b\uff0c\u5bf9Transformer\u67b6\u6784\u7684\u672a\u6765\u53d1\u5c55\u5177\u6709\u6df1\u8fdc\u5f71\u54cd\u3002 Scalable Object Detection in the Car Interior With Vision Foundation Models (B\u00e1lint M\u00e9sz\u00e1ros et al.) \u521b\u65b0\u70b9: \u5c55\u793a\u4e86\u5982\u4f55\u9ad8\u6548\u5229\u7528\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u89e3\u51b3\u6c7d\u8f66\u5185\u9970\u8fd9\u4e00\u590d\u6742\u573a\u666f\u4e0b\u7684\u53ef\u6269\u5c55\u76ee\u6807\u68c0\u6d4b\u95ee\u9898\u3002\u8fd9\u4f53\u73b0\u4e86\u57fa\u7840\u6a21\u578b\u5728\u5b9e\u9645\u5de5\u4e1a\u5e94\u7528\u4e2d\u7684\u5de8\u5927\u4ef7\u503c\u548c\u6f5c\u529b\u3002 Bridging Domain Gaps for Fine-Grained Moth Classification Through Expert-Informed Adaptation and Foundation Model Priors (Ross J Gardiner et al.) \u521b\u65b0\u70b9: \u7ed3\u5408\u4e86\u9886\u57df\u4e13\u5bb6\u77e5\u8bc6\u548c\u57fa\u7840\u6a21\u578b\u5148\u9a8c\uff0c\u6709\u6548\u5f25\u5408\u4e86\u7ec6\u7c92\u5ea6\u5206\u7c7b\u4e2d\u7684\u9886\u57df\u9e3f\u6c9f\u3002\u8fd9\u79cd\u4eba\u673a\u534f\u4f5c\u7684\u8303\u5f0f\u4e3a\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u548c\u9886\u57df\u9002\u5e94\u6027\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u3002 3. \u65b0\u5174\u7814\u7a76\u65b9\u5411\u4e0e\u6280\u672f\uff1a \u6269\u6563\u6a21\u578b\u7684\u591a\u529f\u80fd\u6027 (Versatility of Diffusion Models): \u6269\u6563\u6a21\u578b\u4e0d\u518d\u5c40\u9650\u4e8e\u56fe\u50cf\u751f\u6210\uff0c\u6b63\u88ab\u79ef\u6781\u63a2\u7d22\u7528\u4e8e\u66f4\u590d\u6742\u7684\u611f\u77e5\u4efb\u52a1\uff0c\u5982\u50cf\u7d20\u7ea7\u5206\u5272\u548c3D\u91cd\u5efa\u3002 \u5f00\u653e\u8bcd\u6c47\u4e0e\u65e0\u6807\u6ce8\u5b66\u4e60 (Open-Vocabulary & Annotation-Free Learning): \u7ed3\u5408\u57fa\u7840\u6a21\u578b\uff0c\u5b9e\u73b0\u65e0\u9700\u7279\u5b9a\u7c7b\u522b\u6807\u6ce8\u6216\u751a\u81f3\u65e0\u9700\u4efb\u4f55\u4eba\u5de5\u6807\u6ce8\u7684\u8bc6\u522b\u548c\u68c0\u6d4b\uff0c\u662f\u672a\u6765\u964d\u4f4eAI\u5e94\u7528\u95e8\u69db\u7684\u5173\u952e\u3002 Transformer\u7684\u51e0\u4f55\u611f\u77e5\u8bbe\u8ba1 (Geometrically-Aware Transformer Design): \u6df1\u5165\u7814\u7a76Transformer\u7684\u5185\u90e8\u673a\u5236\uff0c\u7279\u522b\u662f\u4f4d\u7f6e\u7f16\u7801\uff0c\u4ee5\u66f4\u597d\u5730\u878d\u5165\u51e0\u4f55\u5148\u9a8c\u77e5\u8bc6\uff0c\u63d0\u5347\u6a21\u578b\u5bf9\u590d\u6742\u7a7a\u95f4\u7ed3\u6784\u7684\u7406\u89e3\u3002 \u9ad8\u6548\u6df7\u5408\u67b6\u6784\u4e0e\u5fae\u8c03 (Efficient Hybrid Architectures & Fine-tuning): \u7ed3\u5408\u4e0d\u540c\u6a21\u578b\uff08\u5982CNN\u3001Transformer\uff09\u7684\u4f18\u52bf\uff0c\u5e76\u5229\u7528LoRA\u7b49\u9ad8\u6548\u5fae\u8c03\u6280\u672f\uff0c\u4ee5\u5728\u7279\u5b9a\u5e94\u7528\u4e2d\u5b9e\u73b0\u6027\u80fd\u4e0e\u6548\u7387\u7684\u6700\u4f73\u5e73\u8861\u3002 4. \u5efa\u8bae\u6df1\u5165\u9605\u8bfb\u7684\u8bba\u6587\uff1a \u4e3a\u4e86\u5feb\u901f\u628a\u63e1\u9886\u57df\u524d\u6cbf\u548c\u6f5c\u5728\u7684\u7a81\u7834\u6027\u8fdb\u5c55\uff0c\u5efa\u8bae\u4f18\u5148\u9605\u8bfb\u4ee5\u4e0b\u8bba\u6587\uff1a OpenM3D: Open Vocabulary Multi-view Indoor 3D Object Detection without Human Annotations (Peng-Hao Hsu et al.) \u7406\u7531: \u89e3\u51b3\u4e863D\u89c6\u89c9\u4e2d\u5f00\u653e\u8bcd\u6c47\u548c\u6807\u6ce8\u6210\u672c\u4e24\u5927\u6838\u5fc3\u96be\u9898\uff0c\u5177\u6709\u6781\u9ad8\u7684\u7814\u7a76\u4ef7\u503c\u548c\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002 GS: Generative Segmentation via Label Diffusion (Yuhao Chen et al.) \u7406\u7531: \u4ee3\u8868\u4e86\u6269\u6563\u6a21\u578b\u5728\u611f\u77e5\u4efb\u52a1\u4e2d\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u4e3a\u5206\u5272\u4efb\u52a1\u63d0\u4f9b\u4e86\u5168\u65b0\u7684\u89c6\u89d2\u548c\u65b9\u6cd5\u3002 Beyond flattening: a geometrically principled positional encoding for vision transformers with Weierstrass elliptic functions (Zhihang Xin, Xitong Hu, Rui Wang) \u7406\u7531: \u5bf9Transformer\u8fd9\u4e00\u6838\u5fc3\u67b6\u6784\u7684\u5e95\u5c42\u673a\u5236\u8fdb\u884c\u4e86\u7406\u8bba\u6027\u521b\u65b0\uff0c\u53ef\u80fd\u5bf9\u672a\u6765\u6240\u6709\u57fa\u4e8eTransformer\u7684\u6a21\u578b\u4ea7\u751f\u57fa\u7840\u6027\u5f71\u54cd\u3002 Scalable Object Detection in the Car Interior With Vision Foundation Models (B\u00e1lint M\u00e9sz\u00e1ros et al.) \u7406\u7531: \u63d0\u4f9b\u4e86\u4e00\u4e2a\u5c06\u57fa\u7840\u6a21\u578b\u5e94\u7528\u4e8e\u590d\u6742\u3001\u9ad8\u8981\u6c42\u5b9e\u9645\u573a\u666f\u7684\u4f18\u79c0\u6848\u4f8b\uff0c\u5bf9\u4e8e\u7406\u89e3\u57fa\u7840\u6a21\u578b\u7684\u5de5\u7a0b\u5316\u5e94\u7528\u6709\u91cd\u8981\u53c2\u8003\u4ef7\u503c\u3002 Bridging Domain Gaps for Fine-Grained Moth Classification Through Expert-Informed Adaptation and Foundation Model Priors (Ross J Gardiner et al.) \u7406\u7531: \u5c55\u793a\u4e86\u5982\u4f55\u5de7\u5999\u7ed3\u5408\u4eba\u7c7b\u4e13\u5bb6\u77e5\u8bc6\u548cAI\u6a21\u578b\uff0c\u89e3\u51b3\u7ec6\u7c92\u5ea6\u5206\u7c7b\u548c\u9886\u57df\u9002\u5e94\u6027\u6311\u6218\uff0c\u5bf9\u4e8e\u6570\u636e\u7a00\u7f3a\u6216\u4e13\u4e1a\u6027\u5f3a\u7684\u4efb\u52a1\u6709\u501f\u9274\u610f\u4e49\u3002","title":"Executive Summary"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-28/#table-of-contents","text":"GS: Generative Segmentation via Label Diffusion Multimodal Prototype Alignment for Semi-supervised Pathology Image Segmentation Scalable Object Detection in the Car Interior With Vision Foundation Models Generalizing Monocular 3D Object Detection DATR: Diffusion-based 3D Apple Tree Reconstruction Framework with Sparse-View Beyond flattening: a geometrically principled positional encoding for vision transformers with Weierstrass elliptic functions Bridging Domain Gaps for Fine-Grained Moth Classification Through Expert-Informed Adaptation and Foundation Model Priors OpenM3D: Open Vocabulary Multi-view Indoor 3D Object Detection without Human Annotations EffNetViTLoRA: An Efficient Hybrid Deep Learning Approach for Alzheimer's Disease Diagnosis RoofSeg: An edge-aware transformer-based network for end-to-end roof plane segmentation","title":"Table of Contents"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-28/#papers","text":"","title":"Papers"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-28/#gs-generative-segmentation-via-label-diffusion","text":"Authors: Yuhao Chen, Shubin Chen, Liang Lin, Guangrun Wang Published: 2025-08-27 Categories: cs.CV Abstract: Language-driven image segmentation is a fundamental task in vision-language understanding, requiring models to segment regions of an image corresponding to natural language expressions. Traditional methods approach this as a discriminative problem, assigning each pixel to foreground or background based on semantic alignment. Recently, diffusion models have been introduced to this domain, but existing approaches remain image-centric: they either (i) use image diffusion models as visual feature extractors, (ii) synthesize segmentation data via image generation to train discriminative models, or (iii) perform diffusion inversion to extract attention cues from pre-trained image diffusion models-thereby treating segmentation as an auxiliary process. In this paper, we propose GS (Generative Segmentation), a novel framework that formulates segmentation itself as a generative task via label diffusion. Instead of generating images conditioned on label maps and text, GS reverses the generative process: it directly generates segmentation masks from noise, conditioned on both the input image and the accompanying language description. This paradigm makes label generation the primary modeling target, enabling end-to-end training with explicit control over spatial and semantic fidelity. To demonstrate the effectiveness of our approach, we evaluate GS on Panoptic Narrative Grounding (PNG), a representative and challenging benchmark for multimodal segmentation that requires panoptic-level reasoning guided by narrative captions. Experimental results show that GS significantly outperforms existing discriminative and diffusion-based methods, setting a new state-of-the-art for language-driven segmentation. Analysis: \u8fd9\u7bc7\u8bba\u6587\u300aGS: Generative Segmentation via Label Diffusion\u300b\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8bed\u8a00\u9a71\u52a8\u56fe\u50cf\u5206\u5272\u65b9\u6cd5\uff0c\u4ee5\u4e0b\u662f\u8be6\u7ec6\u5206\u6790\uff1a","title":"GS: Generative Segmentation via Label Diffusion"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-28/#1-2-3","text":"\u672c\u6587\u63d0\u51fa\u4e86GS\uff08Generative Segmentation\uff09\u6846\u67b6\uff0c\u5c06\u8bed\u8a00\u9a71\u52a8\u7684\u56fe\u50cf\u5206\u5272\u4efb\u52a1\u91cd\u65b0\u5b9a\u4e49\u4e3a\u901a\u8fc7\u6807\u7b7e\u6269\u6563\uff08label diffusion\uff09\u8fdb\u884c\u7684\u751f\u6210\u5f0f\u4efb\u52a1\u3002\u4e0e\u73b0\u6709\u65b9\u6cd5\u5c06\u5206\u5272\u89c6\u4e3a\u5224\u522b\u5f0f\u95ee\u9898\u6216\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7684\u8f85\u52a9\u8fc7\u7a0b\u4e0d\u540c\uff0cGS\u76f4\u63a5\u4ece\u566a\u58f0\u4e2d\u751f\u6210\u5206\u5272\u63a9\u7801\uff0c\u5e76\u4ee5\u8f93\u5165\u56fe\u50cf\u548c\u8bed\u8a00\u63cf\u8ff0\u4e3a\u6761\u4ef6\uff0c\u4f7f\u6807\u7b7e\u751f\u6210\u6210\u4e3a\u6838\u5fc3\u5efa\u6a21\u76ee\u6807\u3002\u5b9e\u9a8c\u8bc1\u660eGS\u5728Panoptic Narrative Grounding\u57fa\u51c6\u4e0a\u663e\u8457\u8d85\u8d8a\u4e86\u73b0\u6709\u5224\u522b\u5f0f\u548c\u57fa\u4e8e\u6269\u6563\u7684\u65b9\u6cd5\uff0c\u8fbe\u5230\u4e86\u65b0\u7684SOTA\u3002","title":"1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u6d01\u603b\u7ed3 (2-3 \u53e5\u8bdd)"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-28/#2","text":"\u6838\u5fc3\u521b\u65b0\u5728\u4e8e \u8303\u5f0f\u8f6c\u53d8 \uff1a\u5c06\u56fe\u50cf\u5206\u5272\u4efb\u52a1\u672c\u8eab\u4ece\u4f20\u7edf\u7684\u5224\u522b\u5f0f\u95ee\u9898\uff08\u5c06\u50cf\u7d20\u5206\u7c7b\u4e3a\u524d\u666f/\u80cc\u666f\uff09\u6216\u5c06\u5206\u5272\u4f5c\u4e3a\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7684\u8f85\u52a9\u8fc7\u7a0b\uff0c\u8f6c\u53d8\u4e3a\u4e00\u4e2a \u7eaf\u7cb9\u7684\u751f\u6210\u5f0f\u4efb\u52a1 \u3002 \u5177\u4f53\u65b9\u6cd5\u662f\u5f15\u5165 \u201c\u6807\u7b7e\u6269\u6563\u201d\uff08label diffusion\uff09 \u8303\u5f0f\uff1a * \u9006\u8f6c\u751f\u6210\u8fc7\u7a0b\uff1a \u73b0\u6709\u6269\u6563\u6a21\u578b\u901a\u5e38\u662f\u6839\u636e\u6807\u7b7e\u56fe\u548c\u6587\u672c\u751f\u6210\u56fe\u50cf\uff0c\u800cGS\u5219\u53cd\u5176\u9053\u800c\u884c\u4e4b\uff0c\u76f4\u63a5\u4ece\u968f\u673a\u566a\u58f0\u4e2d\u751f\u6210\u5206\u5272\u63a9\u7801\u3002 * \u591a\u6a21\u6001\u6761\u4ef6\uff1a \u751f\u6210\u8fc7\u7a0b\u540c\u65f6\u4ee5\u8f93\u5165\u56fe\u50cf\u548c\u4f34\u968f\u7684\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u4e3a\u6761\u4ef6\u3002 * \u6807\u7b7e\u751f\u6210\u4e3a\u6838\u5fc3\uff1a \u8fd9\u79cd\u65b9\u6cd5\u4f7f\u5f97\u5206\u5272\u63a9\u7801\u7684\u751f\u6210\u6210\u4e3a\u6a21\u578b\u7684\u4e3b\u8981\u76ee\u6807\uff0c\u800c\u975e\u4ece\u56fe\u50cf\u7279\u5f81\u4e2d\u63a8\u65ad\u6216\u4f5c\u4e3a\u56fe\u50cf\u751f\u6210\u8fc7\u7a0b\u7684\u526f\u4ea7\u54c1\u3002 * \u7aef\u5230\u7aef\u8bad\u7ec3\u4e0e\u663e\u5f0f\u63a7\u5236\uff1a \u8fd9\u79cd\u8303\u5f0f\u5141\u8bb8\u7aef\u5230\u7aef\u8bad\u7ec3\uff0c\u5e76\u80fd\u5bf9\u751f\u6210\u7684\u5206\u5272\u63a9\u7801\u7684\u7a7a\u95f4\u548c\u8bed\u4e49\u4fdd\u771f\u5ea6\u8fdb\u884c\u663e\u5f0f\u63a7\u5236\u3002","title":"2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u8bba"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-28/#3","text":"\u5f00\u8f9f\u65b0\u7684\u7814\u7a76\u65b9\u5411\uff1a \u5c06\u6269\u6563\u6a21\u578b\u5e94\u7528\u4e8e\u7ed3\u6784\u5316\u9884\u6d4b\u4efb\u52a1\uff08\u5982\u5206\u5272\uff09\u7684\u751f\u6210\u5f0f\u5efa\u6a21\uff0c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u5176\u4ed6\u7c7b\u4f3c\u4efb\u52a1\uff08\u5982\u6df1\u5ea6\u4f30\u8ba1\u3001\u59ff\u6001\u4f30\u8ba1\u3001\u573a\u666f\u56fe\u751f\u6210\u7b49\uff09\u63d0\u4f9b\u4e86\u65b0\u7684\u5efa\u6a21\u601d\u8def\u548c\u8303\u5f0f\u3002 \u63d0\u5347\u591a\u6a21\u6001\u7406\u89e3\u80fd\u529b\uff1a \u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u590d\u6742\u8bed\u8a00\u63cf\u8ff0\u4e0b\u5bf9\u56fe\u50cf\u5185\u5bb9\u8fdb\u884c\u7cbe\u7ec6\u5206\u5272\u7684\u80fd\u529b\uff0c\u63a8\u52a8\u4e86\u89c6\u89c9-\u8bed\u8a00\u7406\u89e3\u7684\u8fb9\u754c\u3002 \u66f4\u5f3a\u5927\u7684\u5206\u5272\u6a21\u578b\uff1a \u751f\u6210\u5f0f\u65b9\u6cd5\u53ef\u80fd\u6bd4\u5224\u522b\u5f0f\u65b9\u6cd5\u66f4\u80fd\u6355\u6349\u590d\u6742\u7684\u8bed\u4e49\u548c\u7a7a\u95f4\u5173\u7cfb\uff0c\u4ece\u800c\u4ea7\u751f\u66f4\u9c81\u68d2\u3001\u66f4\u7cbe\u7ec6\u7684\u5206\u5272\u7ed3\u679c\u3002 \u589e\u5f3a\u6a21\u578b\u53ef\u63a7\u6027\uff1a \u5f3a\u8c03\u5bf9\u7a7a\u95f4\u548c\u8bed\u4e49\u4fdd\u771f\u5ea6\u7684\u663e\u5f0f\u63a7\u5236\uff0c\u8fd9\u5bf9\u4e8e\u9700\u8981\u9ad8\u7cbe\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u7684\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002","title":"3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-28/#4","text":"\u591a\u6a21\u6001\u7406\u89e3\u4e0e\u4ea4\u4e92\uff1a \u667a\u80fd\u52a9\u624b\u3001\u56fe\u50cf\u641c\u7d22\u3001\u5185\u5bb9\u521b\u4f5c\u7b49\u9886\u57df\uff0c\u673a\u5668\u53ef\u4ee5\u66f4\u7cbe\u786e\u5730\u7406\u89e3\u7528\u6237\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63d0\u51fa\u7684\u56fe\u50cf\u7f16\u8f91\u6216\u67e5\u8be2\u9700\u6c42\u3002 \u673a\u5668\u4eba\u4e0e\u81ea\u52a8\u5316\uff1a \u673a\u5668\u4eba\u53ef\u4ee5\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u66f4\u7cbe\u786e\u5730\u8bc6\u522b\u548c\u64cd\u4f5c\u73af\u5883\u4e2d\u7684\u7279\u5b9a\u5bf9\u8c61\u6216\u533a\u57df\uff0c\u4f8b\u5982\u201c\u62ff\u8d77\u684c\u5b50\u4e0a\u90a3\u4e2a\u7ea2\u8272\u7684\u676f\u5b50\u65c1\u8fb9\u7684\u5c0f\u76d2\u5b50\u201d\u3002 \u56fe\u50cf\u7f16\u8f91\u4e0e\u5185\u5bb9\u751f\u6210\uff1a \u63d0\u4f9b\u66f4\u7cbe\u7ec6\u3001\u8bed\u8a00\u9a71\u52a8\u7684\u56fe\u50cf\u7f16\u8f91\u548c\u751f\u6210\u80fd\u529b\uff0c\u7528\u6237\u53ef\u4ee5\u901a\u8fc7\u63cf\u8ff0\u6765\u7cbe\u786e\u4fee\u6539\u56fe\u50cf\u4e2d\u7684\u7279\u5b9a\u90e8\u5206\uff0c\u4f8b\u5982\u201c\u628a\u8fd9\u4e2a\u4eba\u7684\u5934\u53d1\u67d3\u6210\u84dd\u8272\u201d\u3002 \u533b\u7597\u5f71\u50cf\u5206\u6790\uff1a \u7ed3\u5408\u533b\u751f\u5bf9\u75c5\u7076\u7684\u63cf\u8ff0\uff08\u5982\u201c\u5de6\u80ba\u4e0a\u53f6\u9760\u8fd1\u80f8\u58c1\u7684\u7ed3\u8282\u201d\uff09\uff0c\u5b9e\u73b0\u66f4\u7cbe\u51c6\u7684\u75c5\u7076\u5206\u5272\u548c\u5b9a\u4f4d\u3002 \u8f85\u52a9\u9a7e\u9a76\uff1a \u7406\u89e3\u590d\u6742\u7684\u573a\u666f\u63cf\u8ff0\uff0c\u5e2e\u52a9\u8f66\u8f86\u8bc6\u522b\u7279\u5b9a\u76ee\u6807\u6216\u5371\u9669\u533a\u57df\uff0c\u4f8b\u5982\u201c\u6ce8\u610f\u524d\u65b9\u53f3\u4fa7\u8f66\u9053\u4e0a\u90a3\u8f86\u767d\u8272\u5361\u8f66\u65c1\u8fb9\u7684\u884c\u4eba\u201d\u3002","title":"4. \u53ef\u80fd\u53d7\u76ca\u7684\u76f8\u5173\u9886\u57df\u6216\u5e94\u7528"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-28/#5","text":"\u8ba1\u7b97\u6210\u672c\uff1a \u6269\u6563\u6a21\u578b\u901a\u5e38\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u9636\u6bb5\u8ba1\u7b97\u91cf\u8f83\u5927\uff0c\u5c24\u5176\u662f\u5728\u751f\u6210\u9ad8\u5206\u8fa8\u7387\u63a9\u7801\u65f6\uff0c\u8fd9\u53ef\u80fd\u9650\u5236\u5176\u5728\u8d44\u6e90\u53d7\u9650\u6216\u5b9e\u65f6\u6027\u8981\u6c42\u9ad8\u7684\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002 \u6570\u636e\u4f9d\u8d56\uff1a \u5c3d\u7ba1\u6458\u8981\u5f3a\u8c03\u4e86\u5176\u751f\u6210\u80fd\u529b\uff0c\u4f46\u8bad\u7ec3\u4e00\u4e2a\u5f3a\u5927\u7684\u6269\u6563\u6a21\u578b\u901a\u5e38\u9700\u8981\u5927\u91cf\u7684\u6807\u6ce8\u6570\u636e\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u590d\u6742\u7684\u591a\u6a21\u6001\u8f93\u5165\u65f6\u3002 \u6807\u7b7e\u7a7a\u95f4\u8868\u793a\u7684\u6311\u6218\uff1a \u6458\u8981\u4e2d\u672a\u8be6\u7ec6\u8bf4\u660e\u201c\u6807\u7b7e\u6269\u6563\u201d\u5982\u4f55\u5904\u7406\u79bb\u6563\u7684\u50cf\u7d20\u7ea7\u6807\u7b7e\u7a7a\u95f4\uff08\u4f8b\u5982\uff0c\u5982\u4f55\u5c06\u4e8c\u503c\u6216\u591a\u7c7b\u5206\u5272\u63a9\u7801\u878d\u5165\u8fde\u7eed\u7684\u6269\u6563\u8fc7\u7a0b\uff09\uff0c\u8fd9\u53ef\u80fd\u6d89\u53ca\u590d\u6742\u7684\u8fde\u7eed\u5316\u6216\u79bb\u6563\u5316\u7b56\u7565\uff0c\u5176\u8bbe\u8ba1\u548c\u4f18\u5316\u53ef\u80fd\u5177\u6709\u6311\u6218\u6027\u3002 \u6cdb\u5316\u80fd\u529b\uff1a \u8bba\u6587\u4e3b\u8981\u5728Panoptic Narrative Grounding (PNG) \u57fa\u51c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002\u5176\u5728\u5176\u4ed6\u7c7b\u578b\u7684\u5206\u5272\u4efb\u52a1\uff08\u5982\u7eaf\u8bed\u4e49\u5206\u5272\u3001\u5b9e\u4f8b\u5206\u5272\u6216\u66f4\u7b80\u5355\u7684\u8bed\u8a00\u5f15\u5bfc\u5206\u5272\uff09\u4e0a\u7684\u8868\u73b0\u548c\u6548\u7387\u5c1a\u4e0d\u660e\u786e\u3002 \u53ef\u63a7\u6027\u7c92\u5ea6\uff1a \u5c3d\u7ba1\u63d0\u5230\u201c\u663e\u5f0f\u63a7\u5236\u201d\uff0c\u4f46\u5177\u4f53\u5982\u4f55\u901a\u8fc7\u8bed\u8a00\u63cf\u8ff0\u5b9e\u73b0\u5bf9\u751f\u6210\u63a9\u7801\u7684\u7cbe\u7ec6\u3001\u591a\u5c42\u6b21\u63a7\u5236\uff0c\u4ee5\u53ca\u8fd9\u79cd\u63a7\u5236\u7684\u8fb9\u754c\u5728\u54ea\u91cc\uff0c\u4ecd\u9700\u8fdb\u4e00\u6b65\u63a2\u8ba8\u3002 Key Findings: In this paper, we propose GS (Generative Segmentation), a novel framework that formulates segmentation itself as a generative task via label diffusion. To demonstrate the effectiveness of our approach, we evaluate GS on Panoptic Narrative Grounding (PNG), a representative and challenging benchmark for multimodal segmentation that requires panoptic-level reasoning guided by narrative captions. Experimental results show that GS significantly outperforms existing discriminative and diffusion-based methods, setting a new state-of-the-art for language-driven segmentation. Links: PDF arXiv","title":"5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u7684\u5c40\u9650\u6027"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-28/#multimodal-prototype-alignment-for-semi-supervised-pathology-image-segmentation","text":"Authors: Mingxi Fu, Fanglei Fu, Xitong Ling, Huaitian Yuan, Tian Guan, Yonghong He, Lianghui Zhu Published: 2025-08-27 Categories: cs.CV, cs.AI Abstract: Pathological image segmentation faces numerous challenges, particularly due to ambiguous semantic boundaries and the high cost of pixel-level annotations. Although recent semi-supervised methods based on consistency regularization (e.g., UniMatch) have made notable progress, they mainly rely on perturbation-based consistency within the image modality, making it difficult to capture high-level semantic priors, especially in structurally complex pathology images. To address these limitations, we propose MPAMatch - a novel segmentation framework that performs pixel-level contrastive learning under a multimodal prototype-guided supervision paradigm. The core innovation of MPAMatch lies in the dual contrastive learning scheme between image prototypes and pixel labels, and between text prototypes and pixel labels, providing supervision at both structural and semantic levels. This coarse-to-fine supervisory strategy not only enhances the discriminative capability on unlabeled samples but also introduces the text prototype supervision into segmentation for the first time, significantly improving semantic boundary modeling. In addition, we reconstruct the classic segmentation architecture (TransUNet) by replacing its ViT backbone with a pathology-pretrained foundation model (Uni), enabling more effective extraction of pathology-relevant features. Extensive experiments on GLAS, EBHI-SEG-GLAND, EBHI-SEG-CANCER, and KPI show MPAMatch's superiority over state-of-the-art methods, validating its dual advantages in structural and semantic modeling. Analysis: \u8fd9\u7bc7\u8bba\u6587\u6458\u8981\u63cf\u8ff0\u4e86\u4e00\u9879\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\uff0c\u7279\u522b\u662f\u533b\u5b66\u56fe\u50cf\u5206\u6790\u65b9\u9762\u5177\u6709\u6f5c\u5728\u91cd\u8981\u6027\u7684\u5de5\u4f5c\u3002\u4ee5\u4e0b\u662f\u8be6\u7ec6\u5206\u6790\uff1a","title":"Multimodal Prototype Alignment for Semi-supervised Pathology Image Segmentation"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-28/#1-main-contribution-summary","text":"\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMPAMatch\u7684\u65b0\u578b\u534a\u76d1\u7763\u5206\u5272\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u75c5\u7406\u56fe\u50cf\u5206\u5272\u4e2d\u8bed\u4e49\u8fb9\u754c\u6a21\u7cca\u548c\u50cf\u7d20\u7ea7\u6807\u6ce8\u6210\u672c\u9ad8\u6602\u7684\u95ee\u9898\u3002\u5176\u6838\u5fc3\u8d21\u732e\u5728\u4e8e\u5f15\u5165\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u539f\u578b\u5f15\u5bfc\u7684\u76d1\u7763\u8303\u5f0f\uff0c\u901a\u8fc7\u56fe\u50cf\u539f\u578b\u4e0e\u50cf\u7d20\u6807\u7b7e\u3001\u6587\u672c\u539f\u578b\u4e0e\u50cf\u7d20\u6807\u7b7e\u4e4b\u95f4\u7684\u53cc\u91cd\u5bf9\u6bd4\u5b66\u4e60\uff0c\u9996\u6b21\u5c06\u6587\u672c\u8bed\u4e49\u5148\u9a8c\u5f15\u5165\u5206\u5272\u4efb\u52a1\uff0c\u4ece\u800c\u5728\u7ed3\u6784\u548c\u8bed\u4e49\u5c42\u9762\u63d0\u4f9b\u76d1\u7763\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5bf9\u590d\u6742\u75c5\u7406\u56fe\u50cf\u7684\u5224\u522b\u80fd\u529b\u548c\u8bed\u4e49\u8fb9\u754c\u5efa\u6a21\u7cbe\u5ea6\u3002","title":"1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u603b\u7ed3 (Main Contribution Summary)"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-28/#2-key-innovation-or-methodological-approach","text":"MPAMatch\u7684\u5173\u952e\u521b\u65b0\u5728\u4e8e\u5176 \u591a\u6a21\u6001\u539f\u578b\u5bf9\u9f50\uff08Multimodal Prototype Alignment\uff09 \u7b56\u7565\uff0c\u5177\u4f53\u4f53\u73b0\u5728\u4ee5\u4e0b\u51e0\u70b9\uff1a \u53cc\u91cd\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6848 (Dual Contrastive Learning Scheme)\uff1a \u8fd9\u662f\u6700\u6838\u5fc3\u7684\u521b\u65b0\u3002 \u56fe\u50cf\u539f\u578b\u4e0e\u50cf\u7d20\u6807\u7b7e\u7684\u5bf9\u6bd4\u5b66\u4e60\uff1a \u7528\u4e8e\u6355\u83b7\u56fe\u50cf\u7684\u7ed3\u6784\u4fe1\u606f\uff0c\u589e\u5f3a\u5bf9\u672a\u6807\u6ce8\u6837\u672c\u7684\u5224\u522b\u80fd\u529b\u3002 \u6587\u672c\u539f\u578b\u4e0e\u50cf\u7d20\u6807\u7b7e\u7684\u5bf9\u6bd4\u5b66\u4e60\uff1a \u9996\u6b21\u5c06\u9ad8\u5c42\u8bed\u4e49\u5148\u9a8c\uff08\u901a\u8fc7\u6587\u672c\u63cf\u8ff0\uff09\u5f15\u5165\u50cf\u7d20\u7ea7\u5206\u5272\u4efb\u52a1\u3002\u8fd9\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u7406\u89e3\u548c\u5229\u7528\u6587\u672c\u4e2d\u8574\u542b\u7684\u8bed\u4e49\u4fe1\u606f\uff0c\u4ece\u800c\u66f4\u597d\u5730\u5904\u7406\u6a21\u7cca\u7684\u8bed\u4e49\u8fb9\u754c\u3002 \u7c97\u5230\u7ec6\u7684\u76d1\u7763\u7b56\u7565 (Coarse-to-fine Supervisory Strategy)\uff1a \u7ed3\u5408\u7ed3\u6784\u548c\u8bed\u4e49\u5c42\u9762\u7684\u76d1\u7763\uff0c\u63d0\u4f9b\u66f4\u5168\u9762\u548c\u7cbe\u7ec6\u7684\u6307\u5bfc\u3002 \u67b6\u6784\u589e\u5f3a (Architectural Enhancement)\uff1a \u5c06\u7ecf\u5178\u7684TransUNet\u67b6\u6784\u4e2d\u7684ViT\u9aa8\u5e72\u7f51\u7edc\u66ff\u6362\u4e3a\u9884\u8bad\u7ec3\u7684\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b\uff08Uni\uff09\uff0c\u4ee5\u66f4\u6709\u6548\u5730\u63d0\u53d6\u4e0e\u75c5\u7406\u5b66\u76f8\u5173\u7684\u7279\u5f02\u6027\u7279\u5f81\uff0c\u4e3a\u540e\u7eed\u7684\u5bf9\u6bd4\u5b66\u4e60\u548c\u5206\u5272\u4efb\u52a1\u63d0\u4f9b\u9ad8\u8d28\u91cf\u7684\u8868\u793a\u3002","title":"2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u5b66 (Key Innovation or Methodological Approach)"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-28/#3-potential-impact-on-the-field","text":"\u964d\u4f4e\u6807\u6ce8\u6210\u672c\uff0c\u52a0\u901f\u533b\u5b66AI\u53d1\u5c55\uff1a \u901a\u8fc7\u9ad8\u6548\u7684\u534a\u76d1\u7763\u5b66\u4e60\uff0c\u663e\u8457\u51cf\u5c11\u5bf9\u6602\u8d35\u4e14\u8017\u65f6\u7684\u50cf\u7d20\u7ea7\u75c5\u7406\u56fe\u50cf\u6807\u6ce8\u7684\u4f9d\u8d56\uff0c\u4ece\u800c\u52a0\u901f\u75c5\u7406AI\u6a21\u578b\u7684\u5f00\u53d1\u548c\u90e8\u7f72\u3002 \u63d0\u5347\u75c5\u7406\u56fe\u50cf\u5206\u5272\u7cbe\u5ea6\uff1a \u5c24\u5176\u662f\u5728\u5904\u7406\u8bed\u4e49\u8fb9\u754c\u6a21\u7cca\u548c\u7ed3\u6784\u590d\u6742\u7684\u75c5\u7406\u56fe\u50cf\u65f6\uff0c\u7ed3\u5408\u6587\u672c\u8bed\u4e49\u4fe1\u606f\u6709\u671b\u5e26\u6765\u7a81\u7834\u6027\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5bf9\u75be\u75c5\u8bca\u65ad\u3001\u9884\u540e\u8bc4\u4f30\u548c\u6cbb\u7597\u89c4\u5212\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002 \u63a8\u52a8\u591a\u6a21\u6001\u5b66\u4e60\u5728\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u7684\u5e94\u7528\uff1a \u9996\u6b21\u5c06\u6587\u672c\u539f\u578b\u76d1\u7763\u5f15\u5165\u5206\u5272\u4efb\u52a1\uff0c\u4e3a\u672a\u6765\u5728\u533b\u5b66\u56fe\u50cf\u9886\u57df\u6574\u5408\u66f4\u591a\u6a21\u6001\uff08\u5982\u4e34\u5e8a\u62a5\u544a\u3001\u57fa\u56e0\u7ec4\u6570\u636e\uff09\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u548c\u8303\u5f0f\u3002 \u9a8c\u8bc1\u57fa\u7840\u6a21\u578b\u5728\u7279\u5b9a\u9886\u57df\uff08\u75c5\u7406\u5b66\uff09\u7684\u6709\u6548\u6027\uff1a \u5f3a\u8c03\u4e86\u5229\u7528\u9886\u57df\u7279\u5b9a\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\uff08\u5982Uni\uff09\u4f5c\u4e3a\u9aa8\u5e72\u7f51\u7edc\u7684\u91cd\u8981\u6027\uff0c\u8fd9\u5bf9\u4e8e\u5c06\u901a\u7528AI\u80fd\u529b\u9002\u914d\u5230\u4e13\u4e1a\u9886\u57df\u5177\u6709\u6307\u5bfc\u610f\u4e49\u3002","title":"3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd (Potential Impact on the Field)"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-28/#4-related-areas-or-applications","text":"\u6570\u5b57\u75c5\u7406\u5b66 (Digital Pathology)\uff1a \u80bf\u7624\u68c0\u6d4b\u3001\u7ec4\u7ec7\u5206\u578b\u3001\u75c5\u7406\u5206\u7ea7\u3001\u7ec6\u80de\u6838/\u817a\u4f53\u5206\u5272\u3001\u5b9a\u91cf\u5206\u6790\u7b49\u3002 \u533b\u5b66\u56fe\u50cf\u5206\u6790 (Medical Image Analysis)\uff1a \u4efb\u4f55\u9700\u8981\u9ad8\u7cbe\u5ea6\u5206\u5272\u4f46\u6807\u6ce8\u6210\u672c\u9ad8\u6602\u7684\u533b\u5b66\u56fe\u50cf\u4efb\u52a1\uff0c\u4f8b\u5982\u653e\u5c04\u5b66\u56fe\u50cf\uff08CT/MRI\uff09\u4e2d\u7684\u5668\u5b98\u6216\u75c5\u7076\u5206\u5272\u3001\u663e\u5fae\u955c\u56fe\u50cf\u5206\u6790\u7b49\u3002 \u5f31\u76d1\u7763/\u534a\u76d1\u7763\u5b66\u4e60 (Weakly/Semi-supervised Learning)\uff1a \u4e3a\u8fd9\u4e9b\u5b66\u4e60\u8303\u5f0f\u63d0\u4f9b\u4e86\u65b0\u7684\u591a\u6a21\u6001\u878d\u5408\u7b56\u7565\u3002 \u591a\u6a21\u6001\u4eba\u5de5\u667a\u80fd (Multimodal AI)\uff1a \u63a2\u7d22\u56fe\u50cf\u4e0e\u6587\u672c\u7b49\u4e0d\u540c\u6a21\u6001\u4fe1\u606f\u878d\u5408\u7684\u901a\u7528\u65b9\u6cd5\u3002 \u57fa\u7840\u6a21\u578b\u5728\u5782\u76f4\u9886\u57df\u7684\u5e94\u7528 (Foundation Models in Vertical Domains)\uff1a \u5982\u4f55\u6709\u6548\u5229\u7528\u548c\u9002\u914d\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\u5230\u7279\u5b9a\u4e13\u4e1a\u9886\u57df\u3002","title":"4. \u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 (Related Areas or Applications)"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-28/#5-inferred-limitations-from-the-abstract","text":"\u6587\u672c\u539f\u578b\u8d28\u91cf\u548c\u751f\u6210\uff1a \u6458\u8981\u4e2d\u672a\u8be6\u7ec6\u8bf4\u660e\u6587\u672c\u539f\u578b\u662f\u5982\u4f55\u751f\u6210\u6216\u83b7\u53d6\u7684\u3002\u6587\u672c\u539f\u578b\u7684\u8d28\u91cf\u3001\u7279\u5f02\u6027\u548c\u8986\u76d6\u8303\u56f4\u53ef\u80fd\u76f4\u63a5\u5f71\u54cd\u8bed\u4e49\u76d1\u7763\u7684\u6548\u679c\u3002\u5982\u679c\u6587\u672c\u63cf\u8ff0\u4e0d\u51c6\u786e\u6216\u4e0d\u5168\u9762\uff0c\u53ef\u80fd\u4f1a\u5f15\u5165\u566a\u58f0\u6216\u504f\u5dee\u3002 \u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\uff1a \u53cc\u91cd\u5bf9\u6bd4\u5b66\u4e60\u548c\u4f7f\u7528\u5927\u578b\u75c5\u7406\u5b66\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\uff08Uni\uff09\u4f5c\u4e3a\u9aa8\u5e72\u7f51\u7edc\uff0c\u53ef\u80fd\u9700\u8981\u8f83\u9ad8\u7684\u8ba1\u7b97\u8d44\u6e90\uff08GPU\u5185\u5b58\u548c\u8ba1\u7b97\u80fd\u529b\uff09\uff0c\u8fd9\u53ef\u80fd\u9650\u5236\u5176\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u5e94\u7528\u3002 \u5bf9\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u4f9d\u8d56\uff1a \u8be5\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u4e00\u4e2a\u201c\u75c5\u7406\u5b66\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\uff08Uni\uff09\u201d\u3002\u5982\u679c\u7279\u5b9a\u75c5\u7406\u4efb\u52a1\u6216\u6570\u636e\u96c6\u6ca1\u6709\u5408\u9002\u7684\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u6216\u8005\u8be5\u6a21\u578b\u672c\u8eab\u5b58\u5728\u5c40\u9650\u6027\uff0c\u53ef\u80fd\u4f1a\u5f71\u54cdMPAMatch\u7684\u6027\u80fd\u3002 \u6cdb\u5316\u80fd\u529b\uff1a \u5c3d\u7ba1\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86SOTA\u7ed3\u679c\uff0c\u4f46\u6587\u672c\u539f\u578b\u548c\u56fe\u50cf\u539f\u578b\u5bf9\u9f50\u7684\u673a\u5236\u5728\u9762\u5bf9\u5168\u65b0\u7684\u3001\u672a\u89c1\u8fc7\u7684\u75c5\u7406\u7c7b\u578b\u6216\u5177\u6709\u663e\u8457\u9886\u57df\u5dee\u5f02\u7684\u6570\u636e\u65f6\uff0c\u5176\u6cdb\u5316\u80fd\u529b\u4ecd\u9700\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u3002 \u53ef\u89e3\u91ca\u6027\uff1a \u591a\u6a21\u6001\u878d\u5408\u548c\u5bf9\u6bd4\u5b66\u4e60\u7684\u590d\u6742\u6027\u53ef\u80fd\u4f7f\u5f97\u6a21\u578b\u7684\u51b3\u7b56\u8fc7\u7a0b\u76f8\u5bf9\u4e0d\u900f\u660e\uff0c\u5bf9\u4e8e\u4e34\u5e8a\u5e94\u7528\u800c\u8a00\uff0c\u63d0\u9ad8\u53ef\u89e3\u91ca\u6027\u53ef\u80fd\u662f\u4e00\u4e2a\u6311\u6218\u3002 Key Findings: To address these limitations, we propose MPAMatch - a novel segmentation framework that performs pixel-level contrastive learning under a multimodal prototype-guided supervision paradigm. Extensive experiments on GLAS, EBHI-SEG-GLAND, EBHI-SEG-CANCER, and KPI show MPAMatch's superiority over state-of-the-art methods, validating its dual advantages in structural and semantic modeling. Links: PDF arXiv","title":"5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u7684\u5c40\u9650\u6027 (Inferred Limitations from the Abstract)"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-28/#scalable-object-detection-in-the-car-interior-with-vision-foundation-models","text":"Authors: B\u00e1lint M\u00e9sz\u00e1ros, Ahmet Firintepe, Sebastian Schmidt, Stephan G\u00fcnnemann Published: 2025-08-27 Categories: cs.CV Abstract: AI tasks in the car interior like identifying and localizing externally introduced objects is crucial for response quality of personal assistants. However, computational resources of on-board systems remain highly constrained, restricting the deployment of such solutions directly within the vehicle. To address this limitation, we propose the novel Object Detection and Localization (ODAL) framework for interior scene understanding. Our approach leverages vision foundation models through a distributed architecture, splitting computational tasks between on-board and cloud. This design overcomes the resource constraints of running foundation models directly in the car. To benchmark model performance, we introduce ODALbench, a new metric for comprehensive assessment of detection and localization.Our analysis demonstrates the framework's potential to establish new standards in this domain. We compare the state-of-the-art GPT-4o vision foundation model with the lightweight LLaVA 1.5 7B model and explore how fine-tuning enhances the lightweight models performance. Remarkably, our fine-tuned ODAL-LLaVA model achieves an ODAL _{score} of 89%, representing a 71% improvement over its baseline performance and outperforming GPT-4o by nearly 20%. Furthermore, the fine-tuned model maintains high detection accuracy while significantly reducing hallucinations, achieving an ODAL _{SNR} three times higher than GPT-4o. Analysis: \u8fd9\u7bc7\u8bba\u6587\u6458\u8981\u5c55\u793a\u4e86\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u5728\u89e3\u51b3\u5b9e\u9645\u5de5\u7a0b\u6311\u6218\u65b9\u9762\u7684\u521b\u65b0\u65b9\u6cd5\u3002\u4ee5\u4e0b\u662f\u6839\u636e\u6458\u8981\u8fdb\u884c\u7684\u5206\u6790\uff1a","title":"Scalable Object Detection in the Car Interior With Vision Foundation Models"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-28/#1-concise-summary","text":"\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aODAL\u7684\u5206\u5e03\u5f0f\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u8f66\u8f7d\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u4e0b\uff0c\u5229\u7528\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u5b9e\u73b0\u8f66\u5185\u7269\u4f53\u7684\u53ef\u6269\u5c55\u68c0\u6d4b\u4e0e\u5b9a\u4f4d\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u5c06\u8ba1\u7b97\u4efb\u52a1\u5206\u914d\u5230\u8f66\u8f7d\u548c\u4e91\u7aef\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u76f4\u63a5\u90e8\u7f72\u57fa\u7840\u6a21\u578b\u7684\u8d44\u6e90\u74f6\u9888\uff0c\u5e76\u901a\u8fc7\u5f15\u5165\u65b0\u5ea6\u91cfODALbench\uff0c\u5c55\u793a\u4e86\u5176\u5fae\u8c03\u7684\u8f7b\u91cf\u7ea7\u6a21\u578b\uff08ODAL-LLaVA\uff09\u5728\u6027\u80fd\u548c\u5e7b\u89c9\u6291\u5236\u65b9\u9762\u5747\u8d85\u8d8a\u4e86GPT-4o\u3002","title":"1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u660e\u6458\u8981 (Concise Summary)"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-28/#2-key-innovation-or-methodological-approach_1","text":"\u5206\u5e03\u5f0f\u67b6\u6784\u5229\u7528\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff1a \u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u5176 \u5206\u5e03\u5f0f\u67b6\u6784 \uff0c\u5c06\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u8ba1\u7b97\u4efb\u52a1\u667a\u80fd\u5730\u5206\u914d\u5230\u8f66\u8f7d\u7cfb\u7edf\u548c\u4e91\u7aef\uff0c\u4ece\u800c\u89c4\u907f\u4e86\u8f66\u8f7d\u786c\u4ef6\u7684\u8d44\u6e90\u9650\u5236\u3002\u8fd9\u79cd\u6df7\u5408\u90e8\u7f72\u7b56\u7565\u662f\u89e3\u51b3\u8fb9\u7f18\u8bbe\u5907\u4e0a\u8fd0\u884c\u5927\u578bAI\u6a21\u578b\u6311\u6218\u7684\u5b9e\u7528\u65b9\u6848\u3002 \u8f7b\u91cf\u7ea7\u6a21\u578b\u5fae\u8c03\u8d85\u8d8aSOTA\u901a\u7528\u6a21\u578b\uff1a \u8bba\u6587\u5c55\u793a\u4e86\u901a\u8fc7 \u5bf9\u8f7b\u91cf\u7ea7\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08\u5982LLaVA\uff09\u8fdb\u884c\u9886\u57df\u7279\u5b9a\u5fae\u8c03 \uff0c\u53ef\u4ee5\u5728\u7279\u5b9a\u4efb\u52a1\uff08\u8f66\u5185\u7269\u4f53\u68c0\u6d4b\u4e0e\u5b9a\u4f4d\uff09\u4e0a\u663e\u8457\u8d85\u8d8a\u66f4\u5927\u578b\u3001\u901a\u7528\u6027\u66f4\u5f3a\u7684SOTA\u6a21\u578b\uff08\u5982GPT-4o\uff09\uff0c\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u5e7b\u89c9\uff08ODAL _{SNR} \u662fGPT-4o\u7684\u4e09\u500d\uff09\u3002\u8fd9\u5f3a\u8c03\u4e86\u9886\u57df\u9002\u5e94\u6027\u548c\u6a21\u578b\u6548\u7387\u7684\u91cd\u8981\u6027\u3002 \u5f15\u5165\u65b0\u7684\u8bc4\u4f30\u6307\u6807ODALbench\uff1a \u63d0\u51fa\u4e86 ODALbench \u8fd9\u4e00\u65b0\u7684\u7efc\u5408\u8bc4\u4f30\u6307\u6807\uff0c\u7528\u4e8e\u5168\u9762\u8bc4\u4f30\u68c0\u6d4b\u548c\u5b9a\u4f4d\u6027\u80fd\uff0c\u4e3a\u8be5\u7279\u5b9a\u9886\u57df\u63d0\u4f9b\u4e86\u66f4\u7cbe\u786e\u548c\u5168\u9762\u7684\u8861\u91cf\u6807\u51c6\u3002","title":"2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u8bba (Key Innovation or Methodological Approach)"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-28/#3-potential-impact-on-the-field_1","text":"\u63a8\u52a8\u8fb9\u7f18AI\u548c\u8f66\u8f7dAI\u7684\u53d1\u5c55\uff1a \u672c\u7814\u7a76\u4e3a\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\uff08\u5982\u8f66\u8f7d\u7cfb\u7edf\uff09\u4e0a\u90e8\u7f72\u548c\u5229\u7528\u5f3a\u5927\u7684\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u6761 \u5207\u5b9e\u53ef\u884c\u7684\u8def\u5f84 \uff0c\u514b\u670d\u4e86\u5f53\u524d\u57fa\u7840\u6a21\u578b\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u7684\u74f6\u9888\u3002\u5b83\u6709\u671b \u63a8\u52a8\u8f66\u8f7dAI\u4efb\u52a1\uff08\u5982\u667a\u80fd\u5ea7\u8231\u3001\u4e58\u5ba2\u76d1\u63a7\u3001\u4e2a\u6027\u5316\u52a9\u624b\uff09\u7684\u667a\u80fd\u5316\u6c34\u5e73 \uff0c\u4f7f\u5176\u80fd\u591f\u66f4\u51c6\u786e\u3001\u66f4\u53ef\u9760\u5730\u7406\u89e3\u8f66\u5185\u73af\u5883\u3002 \u91cd\u65b0\u5b9a\u4e49\u6a21\u578b\u9009\u62e9\u548c\u4f18\u5316\u7b56\u7565\uff1a \u8bba\u6587\u6709\u529b\u5730\u8bc1\u660e\u4e86\uff0c\u5728\u7279\u5b9a\u5e94\u7528\u573a\u666f\u4e0b\uff0c\u901a\u8fc7\u5de7\u5999\u7684\u67b6\u6784\u8bbe\u8ba1\u548c\u9886\u57df\u7279\u5b9a\u5fae\u8c03\uff0c\u8f7b\u91cf\u7ea7\u6a21\u578b\u4e0d\u4ec5\u53ef\u4ee5\u8fbe\u5230\u751a\u81f3\u8d85\u8d8a\u5927\u578b\u901a\u7528\u6a21\u578b\u7684\u6027\u80fd\uff0c\u800c\u4e14\u5728\u8d44\u6e90\u6548\u7387\u548c\u53ef\u9760\u6027\uff08\u51cf\u5c11\u5e7b\u89c9\uff09\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002\u8fd9\u53ef\u80fd\u542f\u53d1\u7814\u7a76\u8005\u548c\u5de5\u7a0b\u5e08\u91cd\u65b0\u601d\u8003\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72AI\u65f6\u7684\u6a21\u578b\u9009\u62e9\u548c\u4f18\u5316\u7b56\u7565\u3002 \u5206\u5e03\u5f0f\u8ba1\u7b97\u8303\u5f0f\u7684\u63a2\u7d22\uff1a \u8be5\u5de5\u4f5c\u4e5f\u4e3a \u8fb9\u7f18AI\u7684\u5206\u5e03\u5f0f\u8ba1\u7b97\u8303\u5f0f \u63d0\u4f9b\u4e86\u6709\u76ca\u7684\u63a2\u7d22\uff0c\u53ef\u80fd\u542f\u53d1\u5176\u4ed6\u7c7b\u4f3c\u573a\u666f\u7684\u5e94\u7528\uff0c\u5176\u4e2d\u8ba1\u7b97\u5bc6\u96c6\u578b\u4efb\u52a1\u9700\u8981\u5728\u672c\u5730\u54cd\u5e94\u6027\u548c\u4e91\u7aef\u5f3a\u5927\u80fd\u529b\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002","title":"3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd (Potential Impact on the Field)"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-28/#4-related-areas-or-applications_1","text":"\u667a\u80fd\u9a7e\u9a76\u4e0e\u8f66\u8f7d\u7cfb\u7edf\uff1a \u76f4\u63a5\u5e94\u7528\u4e8e\u667a\u80fd\u5ea7\u8231\u3001\u4e58\u5ba2\u884c\u4e3a\u5206\u6790\u3001\u9057\u7559\u7269\u54c1\u68c0\u6d4b\u3001\u8f66\u5185\u5b89\u5168\u76d1\u63a7\u3001\u4e2a\u6027\u5316\u670d\u52a1\uff08\u5982\u6839\u636e\u8f66\u5185\u7269\u54c1\u8c03\u6574\u73af\u5883\uff09\u7b49\u3002 \u8fb9\u7f18\u8ba1\u7b97\u4e0e\u7269\u8054\u7f51\uff08IoT\uff09\uff1a \u51e1\u662f\u9700\u8981\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u8fd0\u884c\u590d\u6742AI\u6a21\u578b\uff0c\u5e76\u9700\u8981\u4e0e\u4e91\u7aef\u534f\u540c\u7684\u573a\u666f\uff0c\u5982\u667a\u80fd\u5bb6\u5c45\u3001\u5de5\u4e1a\u81ea\u52a8\u5316\u3001\u673a\u5668\u4eba\u89c6\u89c9\u3001\u667a\u80fd\u96f6\u552e\u7b49\u3002 \u4eba\u673a\u4ea4\u4e92\uff08HCI\uff09\uff1a \u63d0\u5347\u8f66\u8f7d\u4e2a\u4eba\u52a9\u624b\u7684\u73af\u5883\u611f\u77e5\u80fd\u529b\u548c\u54cd\u5e94\u8d28\u91cf\uff0c\u4f7f\u5176\u80fd\u66f4\u667a\u80fd\u5730\u7406\u89e3\u7528\u6237\u610f\u56fe\u548c\u73af\u5883\u4e0a\u4e0b\u6587\u3002 \u5b89\u5168\u4e0e\u5b89\u9632\uff1a \u8bc6\u522b\u8f66\u5185\u6f5c\u5728\u5371\u9669\u7269\u54c1\u6216\u5f02\u5e38\u60c5\u51b5\uff0c\u4f8b\u5982\u68c0\u6d4b\u6613\u71c3\u7269\u3001\u6b66\u5668\u6216\u88ab\u9057\u5f03\u7684\u513f\u7ae5/\u5ba0\u7269\u3002","title":"4. \u53ef\u80fd\u53d7\u76ca\u7684\u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 (Related Areas or Applications)"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-28/#5-limitations-inferred-from-the-abstract","text":"\u5bf9\u4e91\u7aef\u8fde\u63a5\u7684\u4f9d\u8d56\uff1a \u5206\u5e03\u5f0f\u67b6\u6784\u610f\u5473\u7740\u5728\u7f51\u7edc\u8fde\u63a5\u4e0d\u7a33\u5b9a\u6216\u65e0\u7f51\u7edc\u7684\u73af\u5883\u4e0b\uff0c\u7cfb\u7edf\u7684\u6027\u80fd\u53ef\u80fd\u4f1a\u53d7\u5230\u4e25\u91cd\u5f71\u54cd\uff0c\u751a\u81f3\u65e0\u6cd5\u5de5\u4f5c\u3002\u540c\u65f6\uff0c\u5c06\u8f66\u5185\u6570\u636e\u4f20\u8f93\u81f3\u4e91\u7aef\u53ef\u80fd\u5f15\u53d1 \u6570\u636e\u9690\u79c1\u548c\u5b89\u5168 \u65b9\u9762\u7684\u62c5\u5fe7\uff0c\u5c24\u5176\u662f\u5728\u6d89\u53ca\u4e2a\u4eba\u6216\u654f\u611f\u4fe1\u606f\u65f6\u3002 \u7279\u5b9a\u4efb\u52a1\u7684\u6cdb\u5316\u80fd\u529b\uff1a \u5c3d\u7ba1\u5fae\u8c03\u540e\u7684ODAL-LLaVA\u5728\u201c\u5916\u90e8\u5f15\u5165\u7269\u4f53\u201d\u7684\u68c0\u6d4b\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5728\u66f4\u5e7f\u6cdb\u7684\u8f66\u5185\u573a\u666f\uff08\u5982\u4e0d\u540c\u8f66\u578b\u3001\u5149\u7167\u6761\u4ef6\u3001\u4e58\u5ba2\u884c\u4e3a\u5206\u6790\u7b49\uff09\u6216\u8bc6\u522b\u5176\u4ed6\u7c7b\u578b\u7269\u4f53\u65f6\u7684 \u6cdb\u5316\u80fd\u529b \u5c1a\u4e0d\u660e\u786e\u3002 \u8f66\u8f7d\u7aef\u8ba1\u7b97\u8d1f\u8377\u7684\u8be6\u7ec6\u7a0b\u5ea6\uff1a \u6458\u8981\u4e2d\u63d0\u5230\u8f66\u8f7d\u7aef\u8d44\u6e90\u53d7\u9650\uff0c\u4f46\u672a\u8be6\u7ec6\u8bf4\u660e\u8f66\u8f7d\u7aef\u5177\u4f53\u627f\u62c5\u7684\u8ba1\u7b97\u4efb\u52a1\u53ca\u5176\u6240\u9700\u7684\u6700\u5c0f\u8d44\u6e90\uff0c\u8fd9\u53ef\u80fd\u5f71\u54cd\u5176\u5728\u6781\u5ea6\u53d7\u9650\u7cfb\u7edf\u4e0a\u7684\u90e8\u7f72\u3002 ODALbench\u7684\u666e\u9002\u6027\uff1a \u4f5c\u4e3a\u4e00\u4e2a\u65b0\u63d0\u51fa\u7684\u5ea6\u91cf\u6807\u51c6\uff0c\u5176\u5728\u884c\u4e1a\u5185\u7684\u63a5\u53d7\u5ea6\u3001\u4e0e\u5176\u4ed6\u73b0\u6709\u5ea6\u91cf\u7684\u517c\u5bb9\u6027\u4ee5\u53ca\u5728\u66f4\u5e7f\u6cdb\u573a\u666f\u4e0b\u7684\u6709\u6548\u6027\u4ecd\u9700\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u3002 \u5b9e\u65f6\u6027\u8981\u6c42\uff1a \u5206\u5e03\u5f0f\u67b6\u6784\u5f15\u5165\u4e86\u7f51\u7edc\u5ef6\u8fdf\uff0c\u5bf9\u4e8e\u67d0\u4e9b\u9700\u8981\u6781\u4f4e\u5ef6\u8fdf\u7684\u5b9e\u65f6\u5e94\u7528\uff08\u4f8b\u5982\u5b89\u5168\u5173\u952e\u578b\u4efb\u52a1\uff09\uff0c\u8fd9\u79cd\u5ef6\u8fdf\u53ef\u80fd\u662f\u4e00\u4e2a\u6311\u6218\u3002\u6458\u8981\u4e2d\u672a\u63d0\u53ca\u5177\u4f53\u7684\u5ef6\u8fdf\u6307\u6807\u3002 Key Findings: To address this limitation, we propose the novel Object Detection and Localization (ODAL) framework for interior scene understanding. Our approach leverages vision foundation models through a distributed architecture, splitting computational tasks between on-board and cloud. To benchmark model performance, we introduce ODALbench, a new metric for comprehensive assessment of detection and localization.Our analysis demonstrates the framework's potential to establish new standards in this domain. We compare the state-of-the-art GPT-4o vision foundation model with the lightweight LLaVA 1.5 7B model and explore how fine-tuning enhances the lightweight models performance. Links: PDF arXiv","title":"5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u7684\u5c40\u9650\u6027 (Limitations Inferred from the Abstract)"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-28/#generalizing-monocular-3d-object-detection","text":"Authors: Abhinav Kumar Published: 2025-08-27 Categories: cs.CV Abstract: Monocular 3D object detection (Mono3D) is a fundamental computer vision task that estimates an object's class, 3D position, dimensions, and orientation from a single image. Its applications, including autonomous driving, augmented reality, and robotics, critically rely on accurate 3D environmental understanding. This thesis addresses the challenge of generalizing Mono3D models to diverse scenarios, including occlusions, datasets, object sizes, and camera parameters. To enhance occlusion robustness, we propose a mathematically differentiable NMS (GrooMeD-NMS). To improve generalization to new datasets, we explore depth equivariant (DEVIANT) backbones. We address the issue of large object detection, demonstrating that it's not solely a data imbalance or receptive field problem but also a noise sensitivity issue. To mitigate this, we introduce a segmentation-based approach in bird's-eye view with dice loss (SeaBird). Finally, we mathematically analyze the extrapolation of Mono3D models to unseen camera heights and improve Mono3D generalization in such out-of-distribution settings. Analysis: \u8fd9\u7bc7\u8bba\u6587\u7684\u6458\u8981\u5c55\u793a\u4e86\u5728\u5355\u76ee3D\u76ee\u6807\u68c0\u6d4b\uff08Mono3D\uff09\u9886\u57df\u8fdb\u884c\u6df1\u5165\u4e14\u591a\u7ef4\u5ea6\u6cdb\u5316\u7814\u7a76\u7684\u52aa\u529b\uff0c\u5177\u6709\u663e\u8457\u7684\u6280\u672f\u8da3\u5473\u6027\u548c\u6f5c\u5728\u91cd\u8981\u6027\u3002 \u4ee5\u4e0b\u662f\u6839\u636e\u6458\u8981\u8fdb\u884c\u7684\u5206\u6790\uff1a \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u6d01\u603b\u7ed3 (Concise Summary of Main Contribution) \u8fd9\u7bc7\u8bba\u6587\u4e13\u6ce8\u4e8e\u63d0\u5347\u5355\u76ee3D\u76ee\u6807\u68c0\u6d4b\uff08Mono3D\uff09\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002\u5b83\u901a\u8fc7\u63d0\u51fa\u4e00\u7cfb\u5217\u521b\u65b0\u65b9\u6cd5\uff0c\u5206\u522b\u89e3\u51b3\u4e86\u6a21\u578b\u5728\u5904\u7406\u906e\u6321\u3001\u65b0\u6570\u636e\u96c6\u3001\u4e0d\u540c\u7269\u4f53\u5c3a\u5bf8\uff08\u7279\u522b\u662f\u5927\u578b\u7269\u4f53\uff09\u4ee5\u53ca\u672a\u77e5\u76f8\u673a\u53c2\u6570\u7b49\u591a\u6837\u5316\u573a\u666f\u65f6\u7684\u6311\u6218\uff0c\u65e8\u5728\u63d0\u9ad8Mono3D\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u9002\u7528\u6027\u3002 \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u8bba (Key Innovation or Methodological Approach) \u8bba\u6587\u7684\u5173\u952e\u521b\u65b0\u5728\u4e8e\u5176\u9488\u5bf9Mono3D\u6cdb\u5316\u6311\u6218\u7684 \u591a\u7ef4\u5ea6\u3001\u7cfb\u7edf\u6027\u89e3\u51b3\u65b9\u6848 \u3002\u5177\u4f53\u5305\u62ec\uff1a GrooMeD-NMS \uff1a\u63d0\u51fa\u4e00\u79cd\u6570\u5b66\u4e0a\u53ef\u5fae\u5206\u7684\u975e\u6781\u5927\u503c\u6291\u5236\uff08NMS\uff09\u65b9\u6cd5\uff0c\u4ee5\u589e\u5f3a\u6a21\u578b\u5bf9\u906e\u6321\u7684\u9c81\u68d2\u6027\uff0c\u8fd9\u5728\u4f20\u7edfNMS\u7684\u79bb\u6563\u6027\u9650\u5236\u4e0b\u662f\u4e00\u4e2a\u663e\u8457\u7a81\u7834\u3002 DEVIANT backbones \uff1a\u63a2\u7d22\u6df1\u5ea6\u7b49\u53d8\uff08depth equivariant\uff09\u9aa8\u5e72\u7f51\u7edc\uff0c\u4ee5\u63d0\u9ad8\u6a21\u578b\u5728\u65b0\u6570\u636e\u96c6\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5229\u7528\u4e86\u7b49\u53d8\u6027\u8fd9\u4e00\u5f3a\u5927\u7684\u5f52\u7eb3\u504f\u7f6e\u3002 SeaBird \uff1a\u9488\u5bf9\u5927\u578b\u7269\u4f53\u68c0\u6d4b\u4e2d\u7684\u566a\u58f0\u654f\u611f\u6027\u95ee\u9898\uff0c\u5f15\u5165\u4e00\u79cd\u57fa\u4e8e\u9e1f\u77b0\u56fe\uff08BEV\uff09\u7684\u5206\u5272\u65b9\u6cd5\uff0c\u5e76\u7ed3\u5408Dice\u635f\u5931\uff0c\u8fd9\u6539\u53d8\u4e86\u4f20\u7edf\u8fb9\u754c\u6846\u56de\u5f52\u7684\u8303\u5f0f\u3002 \u6570\u5b66\u5206\u6790 \uff1a\u5bf9Mono3D\u6a21\u578b\u5728\u672a\u89c1\u76f8\u673a\u9ad8\u5ea6\u4e0b\u7684\u5916\u63a8\u80fd\u529b\u8fdb\u884c\u6570\u5b66\u5206\u6790\uff0c\u5e76\u636e\u6b64\u6539\u8fdb\u4e86\u6a21\u578b\u5728\u5206\u5e03\u5916\uff08OOD\uff09\u8bbe\u7f6e\u4e0b\u7684\u6cdb\u5316\u6027\u80fd\u3002 \u5bf9\u9886\u57df\u7684\u6f5c\u5728\u5f71\u54cd (Potential Impact on the Field) \u8fd9\u9879\u7814\u7a76\u7684\u6f5c\u5728\u5f71\u54cd\u662f\u5de8\u5927\u7684\u3002\u901a\u8fc7\u663e\u8457\u63d0\u5347Mono3D\u6a21\u578b\u5728\u590d\u6742\u591a\u53d8\u73af\u5883\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\uff0c\u5b83\u5c06\u76f4\u63a5\u63a8\u52a8\u5355\u76ee3D\u611f\u77e5\u6280\u672f\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u90e8\u7f72\u548c\u53ef\u9760\u6027\u3002\u4f8b\u5982\uff0c\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\uff0c\u66f4\u51c6\u786e\u3001\u66f4\u5c11\u53d7\u906e\u6321\u548c\u73af\u5883\u53d8\u5316\u5f71\u54cd\u76843D\u611f\u77e5\u80fd\u63d0\u9ad8\u51b3\u7b56\u5b89\u5168\u6027\uff1b\u5728\u589e\u5f3a\u73b0\u5b9e\u548c\u673a\u5668\u4eba\u9886\u57df\uff0c\u5bf9\u7269\u4f533D\u59ff\u6001\u7684\u7cbe\u786e\u7406\u89e3\u662f\u5b9e\u73b0\u65e0\u7f1d\u4ea4\u4e92\u548c\u81ea\u4e3b\u64cd\u4f5c\u7684\u57fa\u7840\u3002\u8fd9\u6709\u52a9\u4e8e\u5c06Mono3D\u4ece\u5b9e\u9a8c\u5ba4\u63a8\u5411\u66f4\u5e7f\u9614\u7684\u5de5\u4e1a\u548c\u6d88\u8d39\u7ea7\u5e94\u7528\u3002 \u53ef\u80fd\u53d7\u76ca\u7684\u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 (Related Areas or Applications that Might Benefit) \u81ea\u52a8\u9a7e\u9a76 (Autonomous Driving) \uff1a\u5bf9\u8f66\u8f86\u3001\u884c\u4eba\u3001\u9a91\u884c\u8005\u7b49\u969c\u788d\u7269\u7684\u7cbe\u786e3D\u611f\u77e5\u662f\u5b89\u5168\u5bfc\u822a\u548c\u8def\u5f84\u89c4\u5212\u7684\u6838\u5fc3\u3002 \u589e\u5f3a\u73b0\u5b9e (Augmented Reality, AR) \uff1a\u9700\u8981\u51c6\u786e\u4f30\u8ba1\u771f\u5b9e\u4e16\u754c\u7269\u4f53\u76843D\u4f4d\u7f6e\u548c\u5c3a\u5bf8\uff0c\u4ee5\u4fbf\u5c06\u865a\u62df\u5185\u5bb9\u65e0\u7f1d\u53e0\u52a0\u3002 \u673a\u5668\u4eba\u5b66 (Robotics) \uff1a\u673a\u5668\u4eba\u9700\u8981\u7406\u89e3\u5176\u64cd\u4f5c\u73af\u5883\u4e2d\u7684\u7269\u4f533D\u4fe1\u606f\uff0c\u4ee5\u8fdb\u884c\u6293\u53d6\u3001\u5bfc\u822a\u548c\u4eba\u673a\u4ea4\u4e92\u3002 3D\u573a\u666f\u7406\u89e3 (3D Scene Understanding) \uff1a\u66f4\u5e7f\u6cdb\u5730\uff0c\u4efb\u4f55\u9700\u8981\u4ece\u5355\u76ee\u56fe\u50cf\u91cd\u5efa\u6216\u7406\u89e33D\u573a\u666f\u7684\u5e94\u7528\u90fd\u4f1a\u53d7\u76ca\u3002 \u9886\u57df\u9002\u5e94\u4e0e\u6cdb\u5316 (Domain Adaptation and Generalization) \uff1a\u8bba\u6587\u4e2d\u89e3\u51b3\u65b0\u6570\u636e\u96c6\u548cOOD\u76f8\u673a\u53c2\u6570\u7684\u95ee\u9898\uff0c\u4e0e\u8fd9\u4e9b\u7814\u7a76\u9886\u57df\u7d27\u5bc6\u76f8\u5173\u3002 \u9c81\u68d2\u89c6\u89c9\u7cfb\u7edf (Robust Vision Systems) \uff1a\u63d0\u5347\u6a21\u578b\u5728\u6076\u52a3\u6761\u4ef6\uff08\u5982\u906e\u6321\u3001\u566a\u58f0\uff09\u4e0b\u7684\u6027\u80fd\uff0c\u662f\u6784\u5efa\u53ef\u9760\u89c6\u89c9\u7cfb\u7edf\u7684\u5173\u952e\u3002 \u53ef\u4ece\u6458\u8981\u4e2d\u63a8\u65ad\u51fa\u7684\u5c40\u9650\u6027 (Limitations that Can Be Inferred from the Abstract) \u672a\u63d0\u53ca\u7684\u6cdb\u5316\u7ef4\u5ea6 \uff1a\u5c3d\u7ba1\u8bba\u6587\u89e3\u51b3\u4e86\u591a\u4e2a\u5173\u952e\u7684\u6cdb\u5316\u6311\u6218\uff0c\u4f46\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u6cdb\u5316\u95ee\u9898\u8fdc\u4e0d\u6b62\u8fd9\u4e9b\u3002\u4f8b\u5982\uff0c\u6a21\u578b\u5728\u6781\u7aef\u5929\u6c14\u6761\u4ef6\uff08\u96e8\u3001\u96ea\u3001\u96fe\uff09\u3001\u4e0d\u540c\u5149\u7167\u53d8\u5316\u3001\u4f20\u611f\u5668\u566a\u58f0\u6216\u4e0d\u540c\u7eb9\u7406\u73af\u5883\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u672a\u5728\u6458\u8981\u4e2d\u63d0\u53ca\u3002 \u8ba1\u7b97\u6548\u7387\u4e0e\u5b9e\u65f6\u6027 \uff1a\u6458\u8981\u4e2d\u63d0\u51fa\u7684\u591a\u79cd\u65b0\u65b9\u6cd5\uff08\u5982\u53ef\u5fae\u5206NMS\u3001\u6df1\u5ea6\u7b49\u53d8\u9aa8\u5e72\u7f51\u7edc\u3001BEV\u5206\u5272\uff09\u53ef\u80fd\u5f15\u5165\u989d\u5916\u7684\u8ba1\u7b97\u5f00\u9500\u3002\u8bba\u6587\u672a\u8bf4\u660e\u8fd9\u4e9b\u65b9\u6cd5\u5bf9\u6a21\u578b\u63a8\u7406\u901f\u5ea6\u548c\u5b9e\u65f6\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u8fd9\u5bf9\u4e8e\u81ea\u52a8\u9a7e\u9a76\u7b49\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002 \u5355\u76ee\u89c6\u89c9\u7684\u56fa\u6709\u5c40\u9650\u6027 \uff1a\u5c3d\u7ba1\u8bba\u6587\u81f4\u529b\u4e8e\u63d0\u5347Mono3D\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u5355\u76ee\u56fe\u50cf\u56fa\u6709\u7684\u6df1\u5ea6\u6a21\u7cca\u6027\u4ecd\u7136\u662f\u5176\u6839\u672c\u9650\u5236\u3002\u8fd9\u4e9b\u65b9\u6cd5\u662f\u5728\u5355\u76ee\u8303\u5f0f\u4e0b\u8fdb\u884c\u4f18\u5316\uff0c\u800c\u975e\u4ece\u6839\u672c\u4e0a\u6539\u53d8\u5176\u8f93\u5165\u4fe1\u606f\u6e90\u3002 \u7279\u5b9a\u573a\u666f\u7684\u9002\u7528\u6027 \uff1a\u4f8b\u5982\uff0cSeaBird\u65b9\u6cd5\u4e13\u95e8\u9488\u5bf9\u201c\u5927\u578b\u7269\u4f53\u201d\u548c\u201c\u566a\u58f0\u654f\u611f\u6027\u201d\u95ee\u9898\u3002\u5bf9\u4e8e\u5c0f\u578b\u7269\u4f53\u6216\u4e0d\u540c\u7c7b\u578b\u7684\u68c0\u6d4b\u6311\u6218\uff0c\u53ef\u80fd\u9700\u8981\u5176\u4ed6\u4e13\u95e8\u7684\u89e3\u51b3\u65b9\u6848\u3002 \u6570\u636e\u96c6\u4f9d\u8d56\u6027 \uff1a\u867d\u7136\u63d0\u51fa\u4e86DEVIANT\u6765\u6539\u5584\u5bf9\u65b0\u6570\u636e\u96c6\u7684\u6cdb\u5316\uff0c\u4f46\u5176\u8bad\u7ec3\u548c\u9a8c\u8bc1\u662f\u5426\u4ecd\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u4ee5\u53ca\u5728\u5b8c\u5168\u65e0\u76d1\u7763\u6216\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\u7684\u8868\u73b0\u5982\u4f55\uff0c\u6458\u8981\u4e2d\u672a\u8be6\u7ec6\u8bf4\u660e\u3002 Key Findings: To enhance occlusion robustness, we propose a mathematically differentiable NMS (GrooMeD-NMS). To improve generalization to new datasets, we explore depth equivariant (DEVIANT) backbones. To mitigate this, we introduce a segmentation-based approach in bird's-eye view with dice loss (SeaBird). Links: PDF arXiv","title":"Generalizing Monocular 3D Object Detection"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-28/#datr-diffusion-based-3d-apple-tree-reconstruction-framework-with-sparse-view","text":"Authors: Tian Qiu, Alan Zoubi, Yiyuan Lin, Ruiming Du, Lailiang Cheng, Yu Jiang Published: 2025-08-27 Categories: cs.RO, cs.CV Abstract: Digital twin applications offered transformative potential by enabling real-time monitoring and robotic simulation through accurate virtual replicas of physical assets. The key to these systems is 3D reconstruction with high geometrical fidelity. However, existing methods struggled under field conditions, especially with sparse and occluded views. This study developed a two-stage framework (DATR) for the reconstruction of apple trees from sparse views. The first stage leverages onboard sensors and foundation models to semi-automatically generate tree masks from complex field images. Tree masks are used to filter out background information in multi-modal data for the single-image-to-3D reconstruction at the second stage. This stage consists of a diffusion model and a large reconstruction model for respective multi view and implicit neural field generation. The training of the diffusion model and LRM was achieved by using realistic synthetic apple trees generated by a Real2Sim data generator. The framework was evaluated on both field and synthetic datasets. The field dataset includes six apple trees with field-measured ground truth, while the synthetic dataset featured structurally diverse trees. Evaluation results showed that our DATR framework outperformed existing 3D reconstruction methods across both datasets and achieved domain-trait estimation comparable to industrial-grade stationary laser scanners while improving the throughput by \\sim 360 times, demonstrating strong potential for scalable agricultural digital twin systems. Analysis: \u4f5c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u4e13\u5bb6\uff0c\u6211\u5bf9\u8fd9\u7bc7\u5173\u4e8eDATR\u7684\u8bba\u6587\u6458\u8981\u8fdb\u884c\u5982\u4e0b\u5206\u6790\uff1a","title":"DATR: Diffusion-based 3D Apple Tree Reconstruction Framework with Sparse-View"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-28/#datr-diffusion-based-3d-apple-tree-reconstruction-framework-with-sparse-view_1","text":"","title":"DATR: Diffusion-based 3D Apple Tree Reconstruction Framework with Sparse-View"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-28/#1-2-3_1","text":"\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDATR\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u5728\u91ce\u5916\u590d\u6742\u3001\u7a00\u758f\u548c\u906e\u6321\u89c6\u56fe\u6761\u4ef6\u4e0b\u9ad8\u7cbe\u5ea63D\u82f9\u679c\u6811\u91cd\u5efa\u7684\u96be\u9898\u3002\u8be5\u6846\u67b6\u9996\u5148\u5229\u7528\u57fa\u7840\u6a21\u578b\u534a\u81ea\u52a8\u751f\u6210\u6811\u6728\u63a9\u819c\u4ee5\u8fc7\u6ee4\u80cc\u666f\uff0c\u968f\u540e\u901a\u8fc7\u7ed3\u5408\u6269\u6563\u6a21\u578b\u548c\u5927\u578b\u91cd\u5efa\u6a21\u578b\uff08LRM\uff09\u5b9e\u73b0\u4ece\u5355\u5f20\u56fe\u50cf\u5230\u9690\u5f0f\u795e\u7ecf\u573a\u76843D\u91cd\u5efa\uff0c\u5e76\u5229\u7528Real2Sim\u6570\u636e\u751f\u6210\u5668\u4ea7\u751f\u7684\u5408\u6210\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u3002DATR\u5728\u7cbe\u5ea6\u4e0a\u53ef\u4e0e\u5de5\u4e1a\u7ea7\u6fc0\u5149\u626b\u63cf\u4eea\u5ab2\u7f8e\uff0c\u540c\u65f6\u5c06\u541e\u5410\u91cf\u63d0\u9ad8\u4e86\u7ea6360\u500d\uff0c\u4e3a\u53ef\u6269\u5c55\u7684\u519c\u4e1a\u6570\u5b57\u5b6a\u751f\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5f3a\u5927\u6f5c\u529b\u3002","title":"1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u6d01\u603b\u7ed3 (2-3 \u53e5\u8bdd)"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-28/#2_1","text":"\u8be5\u8bba\u6587\u7684\u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u5176 \u4e24\u9636\u6bb5\u7684\u6df7\u5408\u65b9\u6cd5 \uff0c\u7279\u522b\u662f\u5728\u7b2c\u4e8c\u9636\u6bb5\uff1a 1. \u524d\u666f\u5206\u5272\u4e0e\u80cc\u666f\u8fc7\u6ee4\uff1a \u5229\u7528 \u57fa\u7840\u6a21\u578b\uff08Foundation Models\uff09 \u5904\u7406\u590d\u6742\u91ce\u5916\u56fe\u50cf\uff0c\u534a\u81ea\u52a8\u751f\u6210\u6811\u6728\u63a9\u819c\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u80cc\u666f\u5e72\u6270\u95ee\u9898\uff0c\u8fd9\u662f\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u8fdb\u884c3D\u91cd\u5efa\u7684\u5173\u952e\u9884\u5904\u7406\u6b65\u9aa4\u3002 2. \u6269\u6563\u6a21\u578b\u4e0e\u5927\u578b\u91cd\u5efa\u6a21\u578b\uff08LRM\uff09\u7684\u7ed3\u5408\uff1a \u8fd9\u662f\u6280\u672f\u4e0a\u7684\u4eae\u70b9\u3002\u6269\u6563\u6a21\u578b\u901a\u5e38\u7528\u4e8e\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\u6216\u591a\u89c6\u56fe\u6570\u636e\uff0c\u800cLRM\uff08\u5982Google\u7684LRM\uff09\u5219\u64c5\u957f\u4ece\u5355\u5f20\u56fe\u50cf\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u9690\u5f0f\u795e\u7ecf\u573a\uff08Implicit Neural Fields\uff09\u8868\u793a\u76843D\u6a21\u578b\u3002\u5c06\u4e24\u8005\u7ed3\u5408\uff0c\u53ef\u80fd\u610f\u5473\u7740\u6269\u6563\u6a21\u578b\u8d1f\u8d23\u751f\u6210\u591a\u89c6\u56fe\u4e00\u81f4\u6027\u4fe1\u606f\u6216\u4f5c\u4e3aLRM\u7684\u6761\u4ef6\u8f93\u5165\uff0c\u4ee5\u589e\u5f3a\u5355\u89c6\u56fe\u91cd\u5efa\u7684\u9c81\u68d2\u6027\u548c\u7ec6\u8282\u3002 3. Real2Sim\u6570\u636e\u751f\u6210\u7b56\u7565\uff1a \u9488\u5bf9\u590d\u6742\u6709\u673a\u4f53\uff08\u5982\u6811\u6728\uff093D\u6570\u636e\u7a00\u7f3a\u7684\u6311\u6218\uff0c\u901a\u8fc7\u201cReal2Sim\u201d\u6570\u636e\u751f\u6210\u5668\u521b\u5efa\u903c\u771f\u7684\u5408\u6210\u82f9\u679c\u6811\u6570\u636e\u8fdb\u884c\u6a21\u578b\u8bad\u7ec3\uff0c\u6709\u6548\u5f25\u8865\u4e86\u771f\u5b9e\u4e16\u754c\u6570\u636e\u91c7\u96c6\u7684\u4e0d\u8db3\uff0c\u5e76\u53ef\u80fd\u6709\u52a9\u4e8e\u6a21\u578b\u5b66\u4e60\u66f4\u5e7f\u6cdb\u7684\u7ed3\u6784\u591a\u6837\u6027\u3002 4. \u7a00\u758f\u89c6\u56fe\u4e0b\u7684\u9ad8\u4fdd\u771f\u91cd\u5efa\uff1a \u660e\u786e\u9488\u5bf9\u201c\u7a00\u758f\u548c\u906e\u6321\u89c6\u56fe\u201d\u8fd9\u4e00\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u75db\u70b9\uff0c\u901a\u8fc7\u4e0a\u8ff0\u65b9\u6cd5\u5b9e\u73b0\u4e86\u9ad8\u51e0\u4f55\u4fdd\u771f\u5ea6\u76843D\u91cd\u5efa\u3002","title":"2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u5b66\u65b9\u6cd5"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-28/#3_1","text":"\u519c\u4e1a\u6570\u5b57\u5316\u8f6c\u578b\uff1a \u4e3a\u7cbe\u51c6\u519c\u4e1a\u548c\u667a\u6167\u519c\u4e1a\u63d0\u4f9b\u4e86\u9769\u547d\u6027\u7684\u5de5\u5177\u3002\u9ad8\u7cbe\u5ea6\u3001\u9ad8\u541e\u5410\u91cf\u7684\u679c\u68113D\u6a21\u578b\u5c06\u6781\u5927\u5730\u63a8\u52a8\u679c\u56ed\u7ba1\u7406\uff08\u5982\u751f\u957f\u76d1\u6d4b\u3001\u75c5\u866b\u5bb3\u9884\u8b66\u3001\u4ea7\u91cf\u9884\u6d4b\uff09\u3001\u81ea\u52a8\u5316\u4fee\u526a\u548c\u91c7\u6458\u673a\u5668\u4eba\u7684\u53d1\u5c55\uff0c\u5b9e\u73b0\u519c\u4e1a\u751f\u4ea7\u7684\u667a\u80fd\u5316\u548c\u9ad8\u6548\u5316\u3002 \u8ba1\u7b97\u673a\u89c6\u89c9\u4e0e3D\u91cd\u5efa\uff1a \u63a8\u52a8\u4e86\u4ece\u7a00\u758f/\u5355\u89c6\u56fe\u91cd\u5efa\u590d\u6742\u6709\u673a\u4f53\uff08\u5982\u6811\u6728\uff09\u7684\u6280\u672f\u8fb9\u754c\u3002\u5b83\u5c55\u793a\u4e86\u6269\u6563\u6a21\u578b\u548c\u5927\u578b\u91cd\u5efa\u6a21\u578b\u5728\u5904\u7406\u91ce\u5916\u975e\u7ed3\u6784\u5316\u73af\u5883\u3001\u514b\u670d\u6570\u636e\u7a00\u7f3a\u548c\u906e\u6321\u95ee\u9898\u65b9\u9762\u7684\u5f3a\u5927\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u5176\u4ed6\u590d\u6742\u573a\u666f\u76843D\u91cd\u5efa\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u548c\u8303\u5f0f\u3002 Real2Sim\u8303\u5f0f\u9a8c\u8bc1\uff1a \u6210\u529f\u5e94\u7528Real2Sim\u6570\u636e\u751f\u6210\u7b56\u7565\u6765\u8bad\u7ec3\u590d\u67423D\u91cd\u5efa\u6a21\u578b\uff0c\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u5408\u6210\u6570\u636e\u5728\u5f25\u8865\u771f\u5b9e\u6570\u636e\u4e0d\u8db3\u3001\u52a0\u901f\u6a21\u578b\u5f00\u53d1\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u5bf9\u5176\u4ed6\u6570\u636e\u53d7\u9650\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u5177\u6709\u501f\u9274\u610f\u4e49\u3002 \u673a\u5668\u4eba\u611f\u77e5\uff1a \u663e\u8457\u63d0\u5347\u4e86\u519c\u4e1a\u673a\u5668\u4eba\u5bf9\u590d\u6742\u81ea\u7136\u73af\u5883\u7684\u611f\u77e5\u80fd\u529b\uff0c\u4f7f\u5176\u80fd\u591f\u66f4\u7cbe\u786e\u5730\u7406\u89e3\u548c\u4ea4\u4e92\u5468\u56f4\u7684\u690d\u7269\u5bf9\u8c61\u3002","title":"3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-28/#4_1","text":"\u7cbe\u51c6\u519c\u4e1a\u4e0e\u667a\u6167\u519c\u4e1a\uff1a \u679c\u6811\u5065\u5eb7\u76d1\u6d4b\u3001\u751f\u957f\u5468\u671f\u9884\u6d4b\u3001\u4ea7\u91cf\u4f30\u7b97\u3001\u81ea\u52a8\u5316\u4fee\u526a\u4e0e\u91c7\u6458\u673a\u5668\u4eba\u5bfc\u822a\u3002 \u6797\u4e1a\u4e0e\u751f\u6001\u5b66\uff1a \u68ee\u6797\u8d44\u6e90\u666e\u67e5\u3001\u6811\u6728\u75c5\u866b\u5bb3\u76d1\u6d4b\u3001\u751f\u7269\u91cf\u4f30\u7b97\u3001\u751f\u6001\u7cfb\u7edf\u5efa\u6a21\u3002 \u57ce\u5e02\u89c4\u5212\u4e0e\u56ed\u6797\u8bbe\u8ba1\uff1a \u57ce\u5e02\u7eff\u5316\u7ba1\u7406\u3001\u666f\u89c2\u8bbe\u8ba1\u4e2d\u7684\u6811\u6728\u5efa\u6a21\u3002 \u73af\u5883\u76d1\u6d4b\uff1a \u690d\u7269\u751f\u957f\u52a8\u6001\u8ffd\u8e2a\u3001\u6c14\u5019\u53d8\u5316\u5bf9\u690d\u88ab\u5f71\u54cd\u7684\u7814\u7a76\u3002 \u673a\u5668\u4eba\u5b66\uff1a \u6237\u5916\u673a\u5668\u4eba\u5bf9\u590d\u6742\u81ea\u7136\u73af\u5883\u7684\u611f\u77e5\u4e0e\u4ea4\u4e92\uff0c\u4f8b\u5982\u5728\u975e\u7ed3\u6784\u5316\u5730\u5f62\u4e2d\u8fdb\u884c\u5bfc\u822a\u6216\u76ee\u6807\u8bc6\u522b\u3002 \u865a\u62df\u73b0\u5b9e/\u589e\u5f3a\u73b0\u5b9e\uff08VR/AR\uff09\u5185\u5bb9\u751f\u6210\uff1a \u4e3a\u865a\u62df\u73b0\u5b9e\u6216\u589e\u5f3a\u73b0\u5b9e\u5e94\u7528\u521b\u5efa\u903c\u771f\u7684\u81ea\u7136\u573a\u666f\u548c\u690d\u7269\u6a21\u578b\u3002","title":"4. \u53ef\u80fd\u53d7\u76ca\u7684\u76f8\u5173\u9886\u57df\u6216\u5e94\u7528"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-28/#5_1","text":"\u534a\u81ea\u52a8\u5316\u63a9\u819c\u751f\u6210\uff1a \u6458\u8981\u4e2d\u63d0\u5230\u201c\u534a\u81ea\u52a8\u5316\u201d\u751f\u6210\u6811\u6728\u63a9\u819c\uff0c\u8fd9\u53ef\u80fd\u610f\u5473\u7740\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u4ecd\u9700\u8981\u4e00\u5b9a\u7a0b\u5ea6\u7684\u4eba\u5de5\u5e72\u9884\u6216\u76d1\u7763\uff0c\u9650\u5236\u4e86\u5176\u5b8c\u5168\u81ea\u52a8\u5316\u7684\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u5728\u5927\u89c4\u6a21\u90e8\u7f72\u65f6\u3002 \u9886\u57df\u7279\u5f02\u6027\uff1a \u8be5\u6846\u67b6\u4e13\u95e8\u9488\u5bf9\u201c\u82f9\u679c\u6811\u201d\u8fdb\u884c\u5f00\u53d1\u548c\u8bc4\u4f30\u3002\u867d\u7136\u65b9\u6cd5\u53ef\u80fd\u5177\u6709\u901a\u7528\u6027\uff0c\u4f46\u5176\u8bad\u7ec3\u6a21\u578b\u548c\u6027\u80fd\u53ef\u80fd\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u82f9\u679c\u6811\u7684\u7ed3\u6784\u7279\u5f81\uff0c\u63a8\u5e7f\u5230\u5176\u4ed6\u6811\u79cd\uff08\u5982\u677e\u6811\u3001\u6a61\u6811\u7b49\u5177\u6709\u4e0d\u540c\u5206\u652f\u7ed3\u6784\u548c\u53f6\u7247\u5bc6\u5ea6\u7684\u6811\u6728\uff09\u6216\u66f4\u5e7f\u6cdb\u7684\u6709\u673a\u7269\u4f53\u53ef\u80fd\u9700\u8981\u989d\u5916\u7684\u9002\u5e94\u6027\u5de5\u4f5c\u6216\u91cd\u65b0\u8bad\u7ec3\u3002 Sim-to-Real Gap\uff1a \u8bad\u7ec3\u4f9d\u8d56\u4e8e\u201cReal2Sim\u6570\u636e\u751f\u6210\u5668\u201d\u4ea7\u751f\u7684\u5408\u6210\u6570\u636e\u3002\u5c3d\u7ba1\u6458\u8981\u5f3a\u8c03\u5408\u6210\u6570\u636e\u903c\u771f\uff0c\u4f46\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u590d\u6742\u6027\u548c\u53d8\u5f02\u6027\uff08\u5982\u5149\u7167\u53d8\u5316\u3001\u5929\u6c14\u6761\u4ef6\u3001\u4e0d\u540c\u751f\u957f\u9636\u6bb5\u7684\u6811\u6728\u5f62\u6001\u3001\u75c5\u866b\u5bb3\u5f71\u54cd\u7b49\uff09\u53ef\u80fd\u4ecd\u672a\u5b8c\u5168\u8986\u76d6\uff0c\u53ef\u80fd\u5b58\u5728\u4ece\u6a21\u62df\u5230\u771f\u5b9e\u73af\u5883\u7684\u6cdb\u5316\u5dee\u8ddd\u3002 \u7a00\u758f\u89c6\u56fe\u7684\u6781\u9650\uff1a \u5c3d\u7ba1\u89e3\u51b3\u4e86\u7a00\u758f\u548c\u906e\u6321\u89c6\u56fe\u7684\u95ee\u9898\uff0c\u4f46\u6458\u8981\u5e76\u672a\u8bf4\u660e\u5176\u5bf9\u89c6\u56fe\u7a00\u758f\u7a0b\u5ea6\u7684\u9c81\u68d2\u6027\u4e0a\u9650\uff0c\u5728\u6781\u7aef\u7a00\u758f\u6216\u906e\u6321\u60c5\u51b5\u4e0b\uff08\u4f8b\u5982\uff0c\u4ec5\u6709\u6781\u5c11\u6570\u56fe\u50cf\u6216\u5927\u90e8\u5206\u88ab\u906e\u6321\uff09\u6027\u80fd\u5982\u4f55\u4ecd\u9700\u8fdb\u4e00\u6b65\u63a2\u8ba8\u3002 \u51e0\u4f55\u7cbe\u5ea6\u4e0e\u6fc0\u5149\u626b\u63cf\u4eea\u7684\u6743\u8861\uff1a \u5c3d\u7ba1\u5728\u201c\u9886\u57df\u7279\u5f81\u4f30\u8ba1\u201d\u4e0a\u4e0e\u5de5\u4e1a\u7ea7\u6fc0\u5149\u626b\u63cf\u4eea\u201c\u76f8\u5f53\u201d\uff0c\u5e76\u5927\u5e45\u63d0\u9ad8\u4e86\u541e\u5410\u91cf\uff0c\u4f46\u5bf9\u4e8e\u7eaf\u7cb9\u7684\u51e0\u4f55\u7ec6\u8282\u7cbe\u5ea6\uff0c\u4e0e\u6700\u9876\u7ea7\u7684\u3001\u8017\u65f6\u7684\u6fc0\u5149\u626b\u63cf\u4eea\u76f8\u6bd4\uff0c\u53ef\u80fd\u4ecd\u5b58\u5728\u7ec6\u5fae\u5dee\u8ddd\uff0c\u5c24\u5176\u662f\u5728\u5fae\u5c0f\u5206\u652f\u6216\u53f6\u7247\u7ea7\u522b\u7684\u7ec6\u8282\u4e0a\u3002 Key Findings: Evaluation results showed that our DATR framework outperformed existing 3D reconstruction methods across both datasets and achieved domain-trait estimation comparable to industrial-grade stationary laser scanners while improving the throughput by \\sim 360 times, demonstrating strong potential for scalable agricultural digital twin systems. Links: PDF arXiv","title":"5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u7684\u5c40\u9650\u6027"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-28/#beyond-flattening-a-geometrically-principled-positional-encoding-for-vision-transformers-with-weierstrass-elliptic-functions","text":"Authors: Zhihang Xin, Xitong Hu, Rui Wang Published: 2025-08-26 Categories: cs.CV Abstract: Vision Transformers have demonstrated remarkable success in computer vision tasks, yet their reliance on learnable one-dimensional positional embeddings fundamentally disrupts the inherent two-dimensional spatial structure of images through patch flattening procedures. Traditional positional encoding approaches lack geometric constraints and fail to establish monotonic correspondence between Euclidean spatial distances and sequential index distances, thereby limiting the model's capacity to leverage spatial proximity priors effectively. We propose Weierstrass Elliptic Function Positional Encoding (WEF-PE), a mathematically principled approach that directly addresses two-dimensional coordinates through natural complex domain representation, where the doubly periodic properties of elliptic functions align remarkably with translational invariance patterns commonly observed in visual data. Our method exploits the non-linear geometric nature of elliptic functions to encode spatial distance relationships naturally, while the algebraic addition formula enables direct derivation of relative positional information between arbitrary patch pairs from their absolute encodings. Comprehensive experiments demonstrate that WEF-PE achieves superior performance across diverse scenarios, including 63.78\\% accuracy on CIFAR-100 from-scratch training with ViT-Tiny architecture, 93.28\\% on CIFAR-100 fine-tuning with ViT-Base, and consistent improvements on VTAB-1k benchmark tasks. Theoretical analysis confirms the distance-decay property through rigorous mathematical proof, while attention visualization reveals enhanced geometric inductive bias and more coherent semantic focus compared to conventional approaches.The source code implementing the methods described in this paper is publicly available on GitHub. Analysis: \u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u3001\u57fa\u4e8e\u6570\u5b66\u539f\u7406\u7684\u4f4d\u7f6e\u7f16\u7801\u65b9\u6cd5\uff0c\u65e8\u5728\u89e3\u51b3Vision Transformers (ViT) \u4e2d\u56fe\u50cf\u5c55\u5e73\u5bfc\u81f4\u7684\u4e8c\u7ef4\u7a7a\u95f4\u7ed3\u6784\u4fe1\u606f\u4e22\u5931\u95ee\u9898\u3002\u4f5c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u4e13\u5bb6\uff0c\u6211\u5bf9\u8fd9\u7bc7\u8bba\u6587\u7684\u5206\u6790\u5982\u4e0b\uff1a","title":"Beyond flattening: a geometrically principled positional encoding for vision transformers with Weierstrass elliptic functions"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-28/#weierstrass-elliptic-function-positional-encoding-wef-pe","text":"1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e (Concise Summary) \u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aWeierstrass\u692d\u5706\u51fd\u6570\u4f4d\u7f6e\u7f16\u7801\uff08WEF-PE\uff09\u7684\u65b0\u578b\u65b9\u6cd5\uff0c\u65e8\u5728\u89e3\u51b3Vision Transformers\u4e2d\u56fe\u50cf\u5c55\u5e73\u5bfc\u81f4\u7684\u4e8c\u7ef4\u7a7a\u95f4\u7ed3\u6784\u4e22\u5931\u95ee\u9898\u3002WEF-PE\u5229\u7528Weierstrass\u692d\u5706\u51fd\u6570\u5728\u590d\u6570\u57df\u7684\u51e0\u4f55\u7279\u6027\u548c\u53cc\u5468\u671f\u6027\uff0c\u76f4\u63a5\u7f16\u7801\u4e8c\u7ef4\u5750\u6807\uff0c\u4ece\u800c\u66f4\u597d\u5730\u6355\u6349\u7a7a\u95f4\u8ddd\u79bb\u5173\u7cfb\u548c\u7ffb\u8bd1\u4e0d\u53d8\u6027\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0cWEF-PE\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u589e\u5f3a\u4e86\u51e0\u4f55\u5f52\u7eb3\u504f\u7f6e\u3002 2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u5b66 (Key Innovation or Methodological Approach) \u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u9996\u6b21\u5c06 Weierstrass\u692d\u5706\u51fd\u6570 \u5f15\u5165Vision Transformers\u7684\u4f4d\u7f6e\u7f16\u7801\uff0c\u4ee5\u4e00\u79cd \u51e0\u4f55\u539f\u7406\u6027 \u7684\u65b9\u5f0f\u76f4\u63a5\u5904\u7406\u4e8c\u7ef4\u56fe\u50cf\u5750\u6807\u3002\u5177\u4f53\u65b9\u6cd5\u5b66\u4eae\u70b9\u5305\u62ec\uff1a \u76f4\u63a5\u4e8c\u7ef4\u5750\u6807\u7f16\u7801\uff1a \u6452\u5f03\u4e86\u4f20\u7edf\u7684\u5c06\u4e8c\u7ef4\u56fe\u50cf\u5c55\u5e73\u4e3a\u4e00\u7ef4\u5e8f\u5217\u7684\u505a\u6cd5\uff0c\u800c\u662f\u76f4\u63a5\u5728 \u590d\u6570\u57df \u4e2d\u8868\u793a\u548c\u7f16\u7801\u4e8c\u7ef4\u56fe\u50cf\u5750\u6807\uff0c\u4ece\u800c\u4fdd\u7559\u4e86\u56fe\u50cf\u56fa\u6709\u7684\u7a7a\u95f4\u7ed3\u6784\u3002 Weierstrass\u692d\u5706\u51fd\u6570\u7684\u5e94\u7528\uff1a \u5229\u7528\u8be5\u51fd\u6570\u7684\u4ee5\u4e0b\u7279\u6027\uff1a \u975e\u7ebf\u6027\u51e0\u4f55\u6027\u8d28\uff1a \u81ea\u7136\u5730\u7f16\u7801\u7a7a\u95f4\u8ddd\u79bb\u5173\u7cfb\uff0c\u5e76\u5efa\u7acb\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\u8ddd\u79bb\u4e0e\u7f16\u7801\u5e8f\u5217\u8ddd\u79bb\u4e4b\u95f4\u7684\u5355\u8c03\u5bf9\u5e94\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7f3a\u4e4f\u51e0\u4f55\u7ea6\u675f\u7684\u95ee\u9898\u3002 \u53cc\u5468\u671f\u6027\uff1a \u692d\u5706\u51fd\u6570\u7684\u53cc\u5468\u671f\u6027\u4e0e\u89c6\u89c9\u6570\u636e\u4e2d\u5e38\u89c1\u7684 \u5e73\u79fb\u4e0d\u53d8\u6027 \u6a21\u5f0f\u9ad8\u5ea6\u5951\u5408\uff0c\u6709\u52a9\u4e8e\u6a21\u578b\u66f4\u597d\u5730\u7406\u89e3\u548c\u5229\u7528\u8fd9\u79cd\u5148\u9a8c\u77e5\u8bc6\u3002 \u4ee3\u6570\u52a0\u6cd5\u516c\u5f0f\uff1a \u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u76f4\u63a5\u4ece\u4efb\u610f\u4e24\u4e2a\u8865\u4e01\u7684\u7edd\u5bf9\u4f4d\u7f6e\u7f16\u7801\u4e2d\u63a8\u5bfc\u51fa\u5b83\u4eec\u4e4b\u95f4\u7684 \u76f8\u5bf9\u4f4d\u7f6e\u4fe1\u606f \uff0c\u8fd9\u5bf9\u4e8eTransformer\u7684\u6ce8\u610f\u529b\u673a\u5236\u81f3\u5173\u91cd\u8981\u3002 \u7406\u8bba\u4e0e\u5b9e\u8df5\u7ed3\u5408\uff1a \u901a\u8fc7\u4e25\u683c\u7684\u6570\u5b66\u8bc1\u660e\u786e\u8ba4\u4e86 \u8ddd\u79bb\u8870\u51cf\uff08distance-decay\uff09 \u7279\u6027\uff0c\u5e76\u901a\u8fc7\u6ce8\u610f\u529b\u53ef\u89c6\u5316\u63ed\u793a\u4e86\u589e\u5f3a\u7684\u51e0\u4f55\u5f52\u7eb3\u504f\u7f6e\u548c\u66f4\u8fde\u8d2f\u7684\u8bed\u4e49\u7126\u70b9\u3002 3. \u5bf9\u9886\u57df\u7684\u6f5c\u5728\u5f71\u54cd (Potential Impact on the Field) \u63a8\u52a8\u4f4d\u7f6e\u7f16\u7801\u8303\u5f0f\u8f6c\u53d8\uff1a \u4ece\u7ecf\u9a8c\u6027\u6216\u53ef\u5b66\u4e60\u76841D\u7f16\u7801\u8f6c\u5411\u57fa\u4e8e\u6df1\u5c42\u6570\u5b66\u539f\u7406\u76842D\u51e0\u4f55\u7f16\u7801\uff0c\u4e3aViT\u8bbe\u8ba1\u63d0\u4f9b\u65b0\u7684\u7406\u8bba\u57fa\u7840\u548c\u65b9\u5411\u3002 \u63d0\u5347ViT\u7684\u51e0\u4f55\u7406\u89e3\u548c\u6027\u80fd\uff1a \u589e\u5f3a\u6a21\u578b\u5bf9\u7a7a\u95f4\u7ed3\u6784\u548c\u8ddd\u79bb\u5173\u7cfb\u7684\u611f\u77e5\u80fd\u529b\uff0c\u4ece\u800c\u5728\u5404\u79cd\u89c6\u89c9\u4efb\u52a1\u4e2d\u53d6\u5f97\u66f4\u4f18\u5f02\u7684\u8868\u73b0\u548c\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5c24\u5176\u662f\u5728\u5bf9\u7a7a\u95f4\u7ec6\u8282\u654f\u611f\u7684\u4efb\u52a1\u4e0a\u3002 \u542f\u53d1\u65b0\u7814\u7a76\u65b9\u5411\uff1a \u9f13\u52b1\u7814\u7a76\u8005\u63a2\u7d22\u66f4\u591a\u9ad8\u7ea7\u6570\u5b66\u5de5\u5177\uff08\u5982\u590d\u5206\u6790\u3001\u5fae\u5206\u51e0\u4f55\u3001\u62d3\u6251\u5b66\u7b49\uff09\u5728\u6df1\u5ea6\u5b66\u4e60\uff0c\u7279\u522b\u662fTransformer\u67b6\u6784\u4e2d\u7684\u5e94\u7528\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u6a21\u578b\u7684\u7ed3\u6784\u6027\u9650\u5236\u3002 \u589e\u5f3a\u6a21\u578b\u53ef\u89e3\u91ca\u6027\uff1a \u901a\u8fc7\u7406\u8bba\u8bc1\u660e\u548c\u6ce8\u610f\u529b\u53ef\u89c6\u5316\uff0c\u63ed\u793a\u4e86\u66f4\u5f3a\u7684\u51e0\u4f55\u5f52\u7eb3\u504f\u7f6e\u548c\u66f4\u8fde\u8d2f\u7684\u8bed\u4e49\u7126\u70b9\uff0c\u6709\u52a9\u4e8e\u7406\u89e3ViT\u7684\u5de5\u4f5c\u673a\u5236\uff0c\u5e76\u53ef\u80fd\u6307\u5bfc\u672a\u6765\u6a21\u578b\u7684\u8bbe\u8ba1\u3002 4. \u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 (Related Areas or Applications that Might Benefit) \u901a\u7528\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\uff1a \u56fe\u50cf\u5206\u7c7b\u3001\u76ee\u6807\u68c0\u6d4b\u3001\u8bed\u4e49\u5206\u5272\u3001\u5b9e\u4f8b\u5206\u5272\u3001\u59ff\u6001\u4f30\u8ba1\u7b49\uff0c\u4efb\u4f55\u9700\u8981\u7cbe\u786e\u7a7a\u95f4\u7406\u89e3\u7684\u4efb\u52a1\u3002 \u89c6\u9891\u5904\u7406\uff1a \u89c6\u9891Transformer\u4e2d\uff0c\u53ef\u4ee5\u6269\u5c55\u5230\u4e09\u7ef4\uff08\u7a7a\u95f4+\u65f6\u95f4\uff09\u4f4d\u7f6e\u7f16\u7801\uff0c\u66f4\u597d\u5730\u6355\u6349\u65f6\u7a7a\u5173\u7cfb\u3002 \u533b\u5b66\u5f71\u50cf\u5206\u6790\uff1a \u5bf9\u56fe\u50cf\u7684\u51e0\u4f55\u7ed3\u6784\u548c\u76f8\u5bf9\u4f4d\u7f6e\u654f\u611f\uff0cWEF-PE\u6709\u671b\u63d0\u9ad8\u8bca\u65ad\u51c6\u786e\u6027\uff0c\u4f8b\u5982\u80bf\u7624\u5b9a\u4f4d\u3001\u75c5\u7076\u5206\u5272\u7b49\u3002 \u9065\u611f\u56fe\u50cf\u5206\u6790\uff1a \u5927\u5c3a\u5ea6\u56fe\u50cf\u4e2d\u7684\u5730\u7269\u8bc6\u522b\u548c\u53d8\u5316\u68c0\u6d4b\uff0c\u5bf9\u7a7a\u95f4\u4e0a\u4e0b\u6587\u7684\u7406\u89e3\u81f3\u5173\u91cd\u8981\u3002 3D\u89c6\u89c9\uff1a \u5c3d\u7ba1\u672c\u6587\u662f2D\uff0c\u4f46\u5176\u51e0\u4f55\u539f\u7406\u6027\u53ef\u80fd\u4e3a3D\u70b9\u4e91\u6216\u4f53\u7d20\u6570\u636e\u7684Transformer\u67b6\u6784\u63d0\u4f9b\u542f\u53d1\uff0c\u4f8b\u59823D\u76ee\u6807\u68c0\u6d4b\u6216\u573a\u666f\u7406\u89e3\u3002 \u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\uff1a \u5982\u679c\u56fe\u8282\u70b9\u5177\u6709\u9690\u5f0f\u6216\u663e\u5f0f\u7684\u7a7a\u95f4\u5750\u6807\uff0c\u8fd9\u79cd\u51e0\u4f55\u7f16\u7801\u601d\u60f3\u4e5f\u53ef\u80fd\u9002\u7528\uff0c\u4ee5\u66f4\u597d\u5730\u5229\u7528\u8282\u70b9\u95f4\u7684\u7a7a\u95f4\u5173\u7cfb\u3002 5. \u53ef\u63a8\u65ad\u7684\u5c40\u9650\u6027 (Limitations that Can Be Inferred from the Abstract) \u6570\u5b66\u590d\u6742\u6027\u4e0e\u5b9e\u73b0\u96be\u5ea6\uff1a Weierstrass\u692d\u5706\u51fd\u6570\u53ca\u5176\u5728\u590d\u6570\u57df\u7684\u5e94\u7528\u5bf9\u4e0d\u719f\u6089\u8be5\u9886\u57df\u7684\u5f00\u53d1\u8005\u6765\u8bf4\u53ef\u80fd\u5177\u6709\u8f83\u9ad8\u7684\u7406\u89e3\u548c\u5b9e\u73b0\u95e8\u69db\u3002 \u8ba1\u7b97\u5f00\u9500\uff1a \u5c3d\u7ba1\u6458\u8981\u672a\u63d0\u53ca\uff0c\u4f46\u590d\u6742\u7684\u6570\u5b66\u51fd\u6570\u8ba1\u7b97\u53ef\u80fd\u4f1a\u5f15\u5165\u989d\u5916\u7684\u8ba1\u7b97\u5f00\u9500\uff0c\u5c24\u5176\u662f\u5728\u5927\u89c4\u6a21\u6a21\u578b\u6216\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u4e0a\uff0c\u8fd9\u53ef\u80fd\u5f71\u54cd\u63a8\u7406\u901f\u5ea6\u3002 \u201c\u53cc\u5468\u671f\u6027\u201d\u7684\u666e\u9002\u6027\uff1a \u5c3d\u7ba1\u6458\u8981\u6307\u51fa\u5176\u4e0e\u89c6\u89c9\u6570\u636e\u7684\u5e73\u79fb\u4e0d\u53d8\u6027\u6a21\u5f0f\u5bf9\u9f50\uff0c\u4f46\u5e76\u975e\u6240\u6709\u89c6\u89c9\u573a\u666f\u90fd\u4e25\u683c\u7b26\u5408\u53cc\u5468\u671f\u6027\u5047\u8bbe\uff08\u4f8b\u5982\uff0c\u56fe\u50cf\u8fb9\u754c\u6548\u5e94\u3001\u975e\u5468\u671f\u6027\u7eb9\u7406\u7b49\uff09\uff0c\u8fd9\u53ef\u80fd\u5728\u67d0\u4e9b\u7279\u5b9a\u6570\u636e\u96c6\u6216\u4efb\u52a1\u4e2d\u9650\u5236\u5176\u8868\u73b0\u3002 \u8d85\u53c2\u6570\u8c03\u4f18\uff1a \u692d\u5706\u51fd\u6570\u53ef\u80fd\u6d89\u53ca\u4e00\u4e9b\u53c2\u6570\uff08\u5982\u5468\u671f\u3001\u6a21\u6570\u7b49\uff09\uff0c\u8fd9\u4e9b\u53c2\u6570\u7684\u9009\u62e9\u548c\u8c03\u4f18\u53ef\u80fd\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\u6216\u989d\u5916\u7684\u5b9e\u9a8c\uff0c\u589e\u52a0\u4e86\u6a21\u578b\u7684\u590d\u6742\u6027\u3002 \u4e0e\u5176\u4ed6\u5148\u8fdb2D PE\u65b9\u6cd5\u7684\u6bd4\u8f83\uff1a \u6458\u8981\u4e2d\u4e3b\u8981\u4e0e\u201c\u4f20\u7edf\u201d\u65b9\u6cd5\u6bd4\u8f83\uff0c\u4f46\u672a\u660e\u786e\u63d0\u53ca\u4e0e\u6700\u8fd1\u7684SOTA 2D PE\u65b9\u6cd5\uff08\u5982RoPE\u3001xPos\u3001\u6216\u5176\u4ed6\u57fa\u4e8e\u5085\u91cc\u53f6\u7279\u5f81\u7684PE\uff09\u7684\u8be6\u7ec6\u5bf9\u6bd4\uff0c\u8fd9\u53ef\u80fd\u662f\u4e00\u4e2a\u6f5c\u5728\u7684\u5c40\u9650\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u76f8\u5bf9\u4f18\u52bf\u3002 Key Findings: We propose Weierstrass Elliptic Function Positional Encoding (WEF-PE), a mathematically principled approach that directly addresses two-dimensional coordinates through natural complex domain representation, where the doubly periodic properties of elliptic functions align remarkably with translational invariance patterns commonly observed in visual data. Our method exploits the non-linear geometric nature of elliptic functions to encode spatial distance relationships naturally, while the algebraic addition formula enables direct derivation of relative positional information between arbitrary patch pairs from their absolute encodings. Links: PDF arXiv","title":"\u8bba\u6587\u5206\u6790\uff1aWeierstrass Elliptic Function Positional Encoding (WEF-PE)"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-28/#bridging-domain-gaps-for-fine-grained-moth-classification-through-expert-informed-adaptation-and-foundation-model-priors","text":"Authors: Ross J Gardiner, Guillaume Mougeot, Sareh Rowlands, Benno I Simmons, Flemming Helsing, Toke Thomas H\u00f8ye Published: 2025-08-27 Categories: cs.CV Abstract: Labelling images of Lepidoptera (moths) from automated camera systems is vital for understanding insect declines. However, accurate species identification is challenging due to domain shifts between curated images and noisy field imagery. We propose a lightweight classification approach, combining limited expert-labelled field data with knowledge distillation from the high-performance BioCLIP2 foundation model into a ConvNeXt-tiny architecture. Experiments on 101 Danish moth species from AMI camera systems demonstrate that BioCLIP2 substantially outperforms other methods and that our distilled lightweight model achieves comparable accuracy with significantly reduced computational cost. These insights offer practical guidelines for the development of efficient insect monitoring systems and bridging domain gaps for fine-grained classification. Analysis: \u4f5c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u4e13\u5bb6\uff0c\u6211\u5bf9\u8fd9\u7bc7\u8bba\u6587\u6458\u8981\u8fdb\u884c\u5982\u4e0b\u5206\u6790\uff1a","title":"Bridging Domain Gaps for Fine-Grained Moth Classification Through Expert-Informed Adaptation and Foundation Model Priors"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-28/#1-concise-summary_1","text":"\u672c\u6587\u9488\u5bf9\u81ea\u52a8\u5316\u76f8\u673a\u7cfb\u7edf\u6355\u83b7\u7684\u98de\u86fe\u56fe\u50cf\u5728\u7cbe\u7ec6\u7c92\u5ea6\u5206\u7c7b\u4e2d\u5b58\u5728\u7684\u9886\u57df\u6f02\u79fb\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u5206\u7c7b\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u5c06\u6709\u9650\u7684\u4e13\u5bb6\u6807\u6ce8\u91ce\u5916\u6570\u636e\u4e0e\u9ad8\u6027\u80fdBioCLIP2\u57fa\u7840\u6a21\u578b\u7684\u77e5\u8bc6\u84b8\u998f\u76f8\u7ed3\u5408\uff0c\u76ee\u6807\u662fConvNeXt-tiny\u67b6\u6784\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c\u84b8\u998f\u540e\u7684\u8f7b\u91cf\u7ea7\u6a21\u578b\u5728\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\uff0c\u80fd\u8fbe\u5230\u4e0eBioCLIP2\u76f8\u5f53\u7684\u51c6\u786e\u6027\uff0c\u4e3a\u9ad8\u6548\u6606\u866b\u76d1\u6d4b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002","title":"1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u6d01\u603b\u7ed3 (Concise Summary)"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-28/#2-key-innovation-or-methodological-approach_2","text":"\u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u5c06\u9ad8\u6027\u80fd\u57fa\u7840\u6a21\u578b\uff08BioCLIP2\uff09\u7684\u5f3a\u5927\u8868\u5f81\u80fd\u529b\uff0c\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u6280\u672f\u8fc1\u79fb\u5230\u4e00\u4e2a\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u7684\u8f7b\u91cf\u7ea7\u6a21\u578b\uff08ConvNeXt-tiny\uff09\u4e0a\u3002\u540c\u65f6\uff0c\u8be5\u65b9\u6cd5\u5de7\u5999\u5730\u5229\u7528\u4e86\u6709\u9650\u7684\u4e13\u5bb6\u6807\u6ce8\u91ce\u5916\u6570\u636e\u6765\u9002\u5e94\u771f\u5b9e\u4e16\u754c\u7684\u9886\u57df\u6f02\u79fb\uff0c\u4ece\u800c\u5728\u4fdd\u8bc1\u7cbe\u7ec6\u7c92\u5ea6\u5206\u7c7b\u51c6\u786e\u6027\u7684\u524d\u63d0\u4e0b\uff0c\u5927\u5e45\u964d\u4f4e\u4e86\u90e8\u7f72\u6210\u672c\u3002\u8fd9\u79cd\u7ed3\u5408\u4e86\u57fa\u7840\u6a21\u578b\u5148\u9a8c\u77e5\u8bc6\u3001\u77e5\u8bc6\u84b8\u998f\u548c\u9886\u57df\u9002\u5e94\u7684\u7b56\u7565\uff0c\u4e3a\u89e3\u51b3\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u548c\u8ba1\u7b97\u8d44\u6e90\u9650\u5236\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002","title":"2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u5b66\u65b9\u6cd5 (Key Innovation or Methodological Approach)"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-28/#3-potential-impact-on-the-field_2","text":"\u8fd9\u9879\u7814\u7a76\u4e3a\u5f00\u53d1\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u6606\u866b\u76d1\u6d4b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c24\u5176\u662f\u5728\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u7684\u91ce\u5916\u90e8\u7f72\u573a\u666f\u3002\u5176\u63d0\u51fa\u7684\u7ed3\u5408\u57fa\u7840\u6a21\u578b\u5148\u9a8c\u77e5\u8bc6\u548c\u9886\u57df\u9002\u5e94\u7684\u77e5\u8bc6\u84b8\u998f\u7b56\u7565\uff0c\u5bf9\u5176\u4ed6\u9700\u8981\u5904\u7406\u9886\u57df\u6f02\u79fb\u548c\u7cbe\u7ec6\u7c92\u5ea6\u5206\u7c7b\u7684\u751f\u7269\u591a\u6837\u6027\u76d1\u6d4b\u3001\u519c\u4e1a\u75c5\u866b\u5bb3\u8bc6\u522b\u7b49\u9886\u57df\u5177\u6709\u91cd\u8981\u7684\u501f\u9274\u610f\u4e49\u3002\u5b83\u5c55\u793a\u4e86\u5982\u4f55\u5c06\u5927\u578b\u6a21\u578b\u7684\u5f3a\u5927\u80fd\u529b\u8f6c\u5316\u4e3a\u8fb9\u7f18\u8bbe\u5907\u53ef\u7528\u7684\u8f7b\u91cf\u7ea7\u5e94\u7528\uff0c\u63a8\u52a8\u4e86AI\u5728\u751f\u6001\u5b66\u548c\u73af\u5883\u79d1\u5b66\u9886\u57df\u7684\u5b9e\u9645\u843d\u5730\u3002","title":"3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd (Potential Impact on the Field)"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-28/#4-related-areas-or-applications_2","text":"\u751f\u7269\u591a\u6837\u6027\u76d1\u6d4b\u4e0e\u4fdd\u62a4: \u9664\u4e86\u98de\u86fe\uff0c\u8fd8\u53ef\u5e94\u7528\u4e8e\u5176\u4ed6\u6606\u866b\u3001\u9e1f\u7c7b\u3001\u690d\u7269\u7b49\u7269\u79cd\u7684\u81ea\u52a8\u5316\u8bc6\u522b\u548c\u79cd\u7fa4\u76d1\u6d4b\u3002 \u519c\u4e1a\u75c5\u866b\u5bb3\u8bc6\u522b: \u5e2e\u52a9\u519c\u6c11\u5feb\u901f\u51c6\u786e\u8bc6\u522b\u4f5c\u7269\u75c5\u5bb3\u6216\u5bb3\u866b\uff0c\u8fdb\u884c\u7cbe\u51c6\u9632\u6cbb\u3002 \u533b\u5b66\u5f71\u50cf\u5206\u6790: \u89e3\u51b3\u4e0d\u540c\u533b\u7597\u8bbe\u5907\u6216\u6570\u636e\u96c6\u4e4b\u95f4\u7684\u9886\u57df\u6f02\u79fb\uff0c\u5b9e\u73b0\u75be\u75c5\u7684\u7cbe\u7ec6\u5316\u8bca\u65ad\u3002 \u5de5\u4e1a\u7f3a\u9677\u68c0\u6d4b: \u9002\u5e94\u4e0d\u540c\u751f\u4ea7\u7ebf\u6216\u5149\u7167\u6761\u4ef6\u4e0b\u7684\u4ea7\u54c1\u7f3a\u9677\u8bc6\u522b\u3002 \u9065\u611f\u56fe\u50cf\u5206\u6790: \u5728\u4e0d\u540c\u5730\u7406\u533a\u57df\u6216\u5b63\u8282\u6761\u4ef6\u4e0b\uff0c\u5bf9\u5730\u7269\u8fdb\u884c\u7cbe\u7ec6\u5206\u7c7b\u3002 \u4efb\u4f55\u9700\u8981\u8fb9\u7f18\u90e8\u7f72\u7684\u7cbe\u7ec6\u7c92\u5ea6\u5206\u7c7b\u4efb\u52a1: \u5c24\u5176\u662f\u5728\u6570\u636e\u6807\u6ce8\u6210\u672c\u9ad8\u6602\u3001\u8ba1\u7b97\u8d44\u6e90\u53d7\u9650\u7684\u573a\u666f\u3002","title":"4. \u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 (Related Areas or Applications)"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-28/#5-limitations-inferred-from-the-abstract_1","text":"\u51c6\u786e\u6027\u5e76\u975e\u8d85\u8d8a\u57fa\u7840\u6a21\u578b: \u84b8\u998f\u540e\u7684\u8f7b\u91cf\u7ea7\u6a21\u578b\u5b9e\u73b0\u4e86\u201c\u53ef\u6bd4\uff08comparable\uff09\u201d\u7684\u51c6\u786e\u6027\uff0c\u800c\u975e\u201c\u8d85\u8d8a\uff08outperforms\uff09\u201d\u57fa\u7840\u6a21\u578bBioCLIP2\u3002\u8fd9\u610f\u5473\u7740\u5728\u8ffd\u6c42\u6781\u81f4\u51c6\u786e\u6027\u7684\u573a\u666f\u4e0b\uff0c\u53ef\u80fd\u4ecd\u9700\u6743\u8861\u8ba1\u7b97\u6210\u672c\u3002 \u5bf9\u4e13\u5bb6\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56: \u5c3d\u7ba1\u65b9\u6cd5\u65e8\u5728\u5229\u7528\u201c\u6709\u9650\u201d\u7684\u4e13\u5bb6\u6807\u6ce8\u6570\u636e\uff0c\u4f46\u5176\u6027\u80fd\u4ecd\u53ef\u80fd\u53d7\u9650\u4e8e\u8fd9\u4e9b\u6570\u636e\u7684\u8d28\u91cf\u548c\u6570\u91cf\u3002\u5728\u5b8c\u5168\u6ca1\u6709\u4e13\u5bb6\u6807\u6ce8\u7684\u9886\u57df\uff0c\u8be5\u65b9\u6cd5\u53ef\u80fd\u9762\u4e34\u6311\u6218\u3002 \u7279\u5b9a\u6570\u636e\u96c6\u7684\u6cdb\u5316\u6027: \u5b9e\u9a8c\u57fa\u4e8e\u201c101\u79cd\u4e39\u9ea6\u98de\u86fe\u201d\u7684\u7279\u5b9a\u6570\u636e\u96c6\u3002\u5176\u5728\u5176\u4ed6\u5730\u7406\u533a\u57df\u3001\u66f4\u591a\u7269\u79cd\u6216\u4e0d\u540c\u751f\u7269\u7c7b\u7fa4\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4ee5\u53ca\u5bf9\u4e0d\u540c\u7c7b\u578b\u91ce\u5916\u566a\u58f0\u7684\u9c81\u68d2\u6027\uff0c\u4ecd\u9700\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u3002 \u201c\u8f7b\u91cf\u7ea7\u201d\u7684\u7a0b\u5ea6: ConvNeXt-tiny\u867d\u7136\u76f8\u5bf9\u8f83\u5c0f\uff0c\u4f46\u5bf9\u4e8e\u67d0\u4e9b\u6781\u5ea6\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\uff08\u5982\u5fae\u63a7\u5236\u5668\uff09\u800c\u8a00\uff0c\u53ef\u80fd\u4ecd\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u3002\u62bd\u8c61\u4e2d\u672a\u8be6\u7ec6\u8bf4\u660e\u5177\u4f53\u7684\u8ba1\u7b97\u8d44\u6e90\u8282\u7701\u6bd4\u4f8b\u6216\u90e8\u7f72\u73af\u5883\u3002 Key Findings: We propose a lightweight classification approach, combining limited expert-labelled field data with knowledge distillation from the high-performance BioCLIP2 foundation model into a ConvNeXt-tiny architecture. Experiments on 101 Danish moth species from AMI camera systems demonstrate that BioCLIP2 substantially outperforms other methods and that our distilled lightweight model achieves comparable accuracy with significantly reduced computational cost. Links: PDF arXiv","title":"5. \u53ef\u4ece\u6458\u8981\u4e2d\u63a8\u65ad\u51fa\u7684\u5c40\u9650\u6027 (Limitations Inferred from the Abstract)"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-28/#openm3d-open-vocabulary-multi-view-indoor-3d-object-detection-without-human-annotations","text":"Authors: Peng-Hao Hsu, Ke Zhang, Fu-En Wang, Tao Tu, Ming-Feng Li, Yu-Lun Liu, Albert Y. C. Chen, Min Sun, Cheng-Hao Kuo Published: 2025-08-27 Categories: cs.CV Abstract: Open-vocabulary (OV) 3D object detection is an emerging field, yet its exploration through image-based methods remains limited compared to 3D point cloud-based methods. We introduce OpenM3D, a novel open-vocabulary multi-view indoor 3D object detector trained without human annotations. In particular, OpenM3D is a single-stage detector adapting the 2D-induced voxel features from the ImGeoNet model. To support OV, it is jointly trained with a class-agnostic 3D localization loss requiring high-quality 3D pseudo boxes and a voxel-semantic alignment loss requiring diverse pre-trained CLIP features. We follow the training setting of OV-3DET where posed RGB-D images are given but no human annotations of 3D boxes or classes are available. We propose a 3D Pseudo Box Generation method using a graph embedding technique that combines 2D segments into coherent 3D structures. Our pseudo-boxes achieve higher precision and recall than other methods, including the method proposed in OV-3DET. We further sample diverse CLIP features from 2D segments associated with each coherent 3D structure to align with the corresponding voxel feature. The key to training a highly accurate single-stage detector requires both losses to be learned toward high-quality targets. At inference, OpenM3D, a highly efficient detector, requires only multi-view images for input and demonstrates superior accuracy and speed (0.3 sec. per scene) on ScanNet200 and ARKitScenes indoor benchmarks compared to existing methods. We outperform a strong two-stage method that leverages our class-agnostic detector with a ViT CLIP-based OV classifier and a baseline incorporating multi-view depth estimator on both accuracy and speed. Analysis: \u8fd9\u7bc7\u8bba\u6587\u300aOpenM3D: Open Vocabulary Multi-view Indoor 3D Object Detection without Human Annotations\u300b\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\uff0c\u7279\u522b\u662f3D\u76ee\u6807\u68c0\u6d4b\u65b9\u9762\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u5f15\u4eba\u6ce8\u76ee\u7684\u65b0\u65b9\u6cd5\u3002\u4ee5\u4e0b\u662f\u8be6\u7ec6\u5206\u6790\uff1a","title":"OpenM3D: Open Vocabulary Multi-view Indoor 3D Object Detection without Human Annotations"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-28/#1-concise-summary_2","text":"OpenM3D\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5f00\u653e\u8bcd\u6c47\uff08OV\uff09\u591a\u89c6\u89d2\u5ba4\u51853D\u76ee\u6807\u68c0\u6d4b\u5668\uff0c\u5176\u6838\u5fc3\u521b\u65b0\u5728\u4e8e \u65e0\u9700\u4eba\u5de5\u6807\u6ce8 \u5373\u53ef\u8fdb\u884c\u8bad\u7ec3\u3002\u8be5\u65b9\u6cd5\u662f\u4e00\u4e2a\u5355\u9636\u6bb5\u68c0\u6d4b\u5668\uff0c\u901a\u8fc7\u7ed3\u5408\u56fe\u5d4c\u5165\u6280\u672f\u751f\u6210\u9ad8\u8d28\u91cf\u76843D\u4f2a\u6846\uff0c\u5e76\u5229\u7528\u591a\u6837\u5316\u7684CLIP\u7279\u5f81\u8fdb\u884c\u4f53\u7d20\u8bed\u4e49\u5bf9\u9f50\uff0c\u4ece\u800c\u5728ScanNet200\u548cARKitScenes\u7b49\u5ba4\u5185\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u51c6\u786e\u6027\u548c\u901f\u5ea6\u3002","title":"1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u660e\u6458\u8981 (Concise Summary)"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-28/#2-key-innovation-or-methodological-approach_3","text":"OpenM3D\u7684\u5173\u952e\u521b\u65b0\u5728\u4e8e\u5176 \u65e0\u4eba\u5de5\u6807\u6ce8\u7684\u8bad\u7ec3\u8303\u5f0f \uff0c\u4ee5\u53ca\u5b9e\u73b0\u8fd9\u4e00\u8303\u5f0f\u7684\u4e24\u4e2a\u6838\u5fc3\u6280\u672f\uff1a \u65e0\u76d1\u7763/\u5f31\u76d1\u7763\u76843D\u4f2a\u6846\u751f\u6210 (3D Pseudo Box Generation without Human Annotations): \u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u5d4c\u5165\uff08graph embedding\uff09\u6280\u672f\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u5c062D\u56fe\u50cf\u4e2d\u7684\u5206\u5272\uff08segments\uff09\u7ec4\u5408\u6210\u8fde\u8d2f\u76843D\u7ed3\u6784\uff0c\u5e76\u4ece\u4e2d\u751f\u6210\u9ad8\u8d28\u91cf\u76843D\u4f2a\u6846\u3002\u8fd9\u4e9b\u4f2a\u6846\u4f5c\u4e3a\u7c7b\u65e0\u5173\u76843D\u5b9a\u4f4d\u635f\u5931\uff08class-agnostic 3D localization loss\uff09\u7684\u8bad\u7ec3\u76ee\u6807\uff0c\u6781\u5927\u5730\u7f13\u89e3\u4e86\u5bf9\u6602\u8d353D\u6846\u6807\u6ce8\u7684\u9700\u6c42\u3002 \u57fa\u4e8eCLIP\u7279\u5f81\u7684\u4f53\u7d20\u8bed\u4e49\u5bf9\u9f50 (Voxel-Semantic Alignment with Diverse CLIP Features): \u4e3a\u4e86\u652f\u6301\u5f00\u653e\u8bcd\u6c47\u68c0\u6d4b\uff0cOpenM3D\u5229\u7528\u9884\u8bad\u7ec3\u7684CLIP\u6a21\u578b\u63d0\u53d62D\u5206\u5272\u7684\u591a\u6837\u5316\u7279\u5f81\uff0c\u5e76\u5c06\u5176\u4e0e\u5bf9\u5e94\u76843D\u4f53\u7d20\u7279\u5f81\u8fdb\u884c\u5bf9\u9f50\u3002\u8fd9\u79cd\u4f53\u7d20-\u8bed\u4e49\u5bf9\u9f50\u635f\u5931\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u7406\u89e3\u548c\u68c0\u6d4b\u8bad\u7ec3\u4e2d\u672a\u89c1\u8fc7\u7684\u7c7b\u522b\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u5f00\u653e\u8bcd\u6c47\u80fd\u529b\u3002 \u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u662f\u4e00\u4e2a \u9ad8\u6548\u7684\u5355\u9636\u6bb5\u68c0\u6d4b\u5668 \uff0c\u901a\u8fc7\u5de7\u5999\u5730\u7ed3\u54082D\u8bf1\u5bfc\u7684\u4f53\u7d20\u7279\u5f81\uff08\u4eceImGeoNet\u6a21\u578b\uff09\u548c\u4e0a\u8ff0\u4e24\u79cd\u635f\u5931\uff0c\u5b9e\u73b0\u4e86\u5728\u51c6\u786e\u6027\u548c\u901f\u5ea6\u4e0a\u7684SOTA\u8868\u73b0\u3002","title":"2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u8bba (Key Innovation or Methodological Approach)"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-28/#3-potential-impact-on-the-field_3","text":"\u964d\u4f4e3D\u68c0\u6d4b\u7684\u6807\u6ce8\u6210\u672c: 3D\u76ee\u6807\u68c0\u6d4b\uff0c\u5c24\u5176\u662f\u5ba4\u5185\u573a\u666f\uff0c\u5176\u4eba\u5de5\u6807\u6ce8\u6210\u672c\u6781\u9ad8\u3002OpenM3D\u901a\u8fc7\u5b8c\u5168\u6d88\u9664\u5bf93D\u6846\u548c\u7c7b\u522b\u7684\u4eba\u5de5\u6807\u6ce8\uff0c\u6781\u5927\u5730\u964d\u4f4e\u4e86\u7814\u7a76\u548c\u5e94\u75283D\u68c0\u6d4b\u7684\u95e8\u69db\uff0c\u6709\u671b\u52a0\u901f\u8be5\u9886\u57df\u7684\u53d1\u5c55\u548c\u5b9e\u9645\u90e8\u7f72\u3002 \u63a8\u52a8\u5f00\u653e\u8bcd\u6c473D\u68c0\u6d4b\u7684\u53d1\u5c55: \u5f00\u653e\u8bcd\u6c47\u80fd\u529b\u662f\u672a\u6765AI\u7cfb\u7edf\u7684\u91cd\u8981\u7279\u5f81\u3002OpenM3D\u5728\u56fe\u50cf\u57fa\uff08image-based\uff093D\u5f00\u653e\u8bcd\u6c47\u68c0\u6d4b\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u8bc1\u660e\u4e862D\u9884\u8bad\u7ec3\u6a21\u578b\uff08\u5982CLIP\uff09\u57283D\u4efb\u52a1\u4e2d\u7684\u5de8\u5927\u6f5c\u529b\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u3002 \u4fc3\u8fdb2D\u4e0e3D\u89c6\u89c9\u7684\u878d\u5408: \u8be5\u5de5\u4f5c\u6709\u6548\u5730\u5c06\u5f3a\u5927\u76842D\u5206\u5272\u548c\u8bed\u4e49\u7406\u89e3\u80fd\u529b\uff08\u901a\u8fc7CLIP\uff09\u8fc1\u79fb\u52303D\u7a7a\u95f4\uff0c\u8fdb\u4e00\u6b65\u5f25\u5408\u4e862D\u548c3D\u89c6\u89c9\u4efb\u52a1\u4e4b\u95f4\u7684\u9e3f\u6c9f\uff0c\u4e3a\u6784\u5efa\u66f4\u901a\u7528\u3001\u66f4\u5f3a\u5927\u7684\u89c6\u89c9\u611f\u77e5\u7cfb\u7edf\u63d0\u4f9b\u4e86\u8303\u4f8b\u3002 \u63d0\u5347\u5b9e\u65f63D\u611f\u77e5\u7684\u6548\u7387: \u4f5c\u4e3a\u5355\u9636\u6bb5\u68c0\u6d4b\u5668\uff0cOpenM3D\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e860.3\u79d2/\u573a\u666f\u7684\u63a8\u7406\u901f\u5ea6\uff0c\u8fd9\u5bf9\u4e8e\u673a\u5668\u4eba\u3001AR/VR\u7b49\u9700\u8981\u5b9e\u65f63D\u611f\u77e5\u7684\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002","title":"3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd (Potential Impact on the Field)"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-28/#4-related-areas-or-applications_3","text":"\u673a\u5668\u4eba\u5b66\u4e0e\u81ea\u4e3b\u5bfc\u822a: \u5ba4\u5185\u670d\u52a1\u673a\u5668\u4eba\u3001\u65e0\u4eba\u673a\u7b49\u9700\u8981\u5728\u590d\u6742\u5ba4\u5185\u73af\u5883\u4e2d\u8fdb\u884c\u7269\u4f53\u8bc6\u522b\u3001\u6293\u53d6\u548c\u907f\u969c\uff0cOpenM3D\u80fd\u63d0\u4f9b\u9ad8\u6548\u3001\u7075\u6d3b\u76843D\u611f\u77e5\u80fd\u529b\u3002 \u589e\u5f3a\u73b0\u5b9e (AR) / \u865a\u62df\u73b0\u5b9e (VR): \u5b9e\u65f6\u7406\u89e3\u7528\u6237\u5468\u56f4\u76843D\u73af\u5883\u548c\u7269\u4f53\uff0c\u5b9e\u73b0\u865a\u62df\u5185\u5bb9\u7684\u7cbe\u786e\u653e\u7f6e\u548c\u4ea4\u4e92\uff0c\u63d0\u5347\u6c89\u6d78\u611f\u3002 \u5ba4\u5185\u6d4b\u7ed8\u4e0e\u6570\u5b57\u5b6a\u751f (Digital Twins): \u81ea\u52a8\u6784\u5efa\u5177\u6709\u8bed\u4e49\u4fe1\u606f\u7684\u5ba4\u51853D\u6a21\u578b\uff0c\u7528\u4e8e\u8bbe\u65bd\u7ba1\u7406\u3001\u7a7a\u95f4\u89c4\u5212\u7b49\u3002 \u667a\u80fd\u5bb6\u5c45\u4e0e\u667a\u6167\u57ce\u5e02: \u5b9e\u73b0\u5bf9\u5ba4\u5185\u7269\u54c1\u7684\u667a\u80fd\u8bc6\u522b\u548c\u7ba1\u7406\uff0c\u6784\u5efa\u66f4\u667a\u80fd\u3001\u66f4\u4eba\u6027\u5316\u7684\u5c45\u4f4f\u548c\u5de5\u4f5c\u73af\u5883\u3002 \u5f31\u76d1\u7763/\u81ea\u76d1\u7763\u5b66\u4e60: \u4f5c\u4e3a\u65e0\u4eba\u5de5\u6807\u6ce8\u5b66\u4e60\u7684\u6210\u529f\u6848\u4f8b\uff0c\u8be5\u7814\u7a76\u5bf9\u66f4\u5e7f\u6cdb\u7684\u5f31\u76d1\u7763\u548c\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u5177\u6709\u501f\u9274\u610f\u4e49\u3002","title":"4. \u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 (Related Areas or Applications)"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-28/#5-inferred-limitations","text":"\u4ec5\u9650\u4e8e\u5ba4\u5185\u573a\u666f (Indoor-specific): \u6458\u8981\u660e\u786e\u6307\u51fa\u662f\u201c\u5ba4\u51853D\u76ee\u6807\u68c0\u6d4b\u201d\uff0c\u8fd9\u610f\u5473\u7740\u5176\u65b9\u6cd5\u53ef\u80fd\u9488\u5bf9\u5ba4\u5185\u73af\u5883\u7684\u7279\u6027\u8fdb\u884c\u4e86\u4f18\u5316\uff0c\u4e0d\u4e00\u5b9a\u80fd\u76f4\u63a5\u6cdb\u5316\u5230\u5ba4\u5916\u6216\u66f4\u590d\u6742\u7684\u5f00\u653e\u573a\u666f\uff0c\u56e0\u4e3a\u5ba4\u5916\u573a\u666f\u7684\u7269\u4f53\u7c7b\u578b\u3001\u5c3a\u5ea6\u3001\u5149\u7167\u548c\u906e\u6321\u6a21\u5f0f\u5dee\u5f02\u5de8\u5927\u3002 \u8bad\u7ec3\u9636\u6bb5\u5bf9RGB-D\u6570\u636e\u548c\u76f8\u673a\u59ff\u6001\u7684\u4f9d\u8d56 (Reliance on Posed RGB-D during Training): \u5c3d\u7ba1\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\uff0c\u4f46\u6458\u8981\u63d0\u5230\u8bad\u7ec3\u9075\u5faaOV-3DET\u7684\u8bbe\u7f6e\uff0c\u5373\u201cposed RGB-D images are given\u201d\u3002\u8fd9\u610f\u5473\u7740\u5728\u8bad\u7ec3\u65f6\uff0c\u6a21\u578b\u9700\u8981\u6df1\u5ea6\u4fe1\u606f\u548c\u7cbe\u786e\u7684\u76f8\u673a\u59ff\u6001\u3002\u867d\u7136\u63a8\u7406\u65f6\u53ea\u9700\u591a\u89c6\u89d2\u56fe\u50cf\uff0c\u4f46\u83b7\u53d6\u9ad8\u8d28\u91cf\u7684RGB-D\u6570\u636e\u548c\u59ff\u6001\u672c\u8eab\u4e5f\u53ef\u80fd\u662f\u4e00\u4e2a\u6311\u6218\u3002 \u5bf92D\u5206\u5272\u548cCLIP\u6a21\u578b\u8d28\u91cf\u7684\u4f9d\u8d56 (Dependency on 2D Segmentation and CLIP Quality): \u4f2a\u6846\u751f\u6210\u4f9d\u8d56\u4e8e2D\u5206\u5272\uff0c\u5f00\u653e\u8bcd\u6c47\u80fd\u529b\u4f9d\u8d56\u4e8e\u9884\u8bad\u7ec3CLIP\u6a21\u578b\u7684\u7279\u5f81\u3002\u5982\u679c\u5e95\u5c42\u76842D\u5206\u5272\u6216CLIP\u6a21\u578b\u5728\u7279\u5b9a\u573a\u666f\u6216\u7269\u4f53\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u53ef\u80fd\u4f1a\u76f4\u63a5\u5f71\u54cdOpenM3D\u7684\u6027\u80fd\u3002 \u201c\u8fde\u8d2f3D\u7ed3\u6784\u201d\u7684\u5047\u8bbe (Assumption of \"Coherent 3D Structures\"): \u4f2a\u6846\u751f\u6210\u65b9\u6cd5\u901a\u8fc7\u56fe\u5d4c\u5165\u5c062D\u5206\u5272\u7ec4\u5408\u6210\u201c\u8fde\u8d2f\u76843D\u7ed3\u6784\u201d\u3002\u5bf9\u4e8e\u9ad8\u5ea6\u906e\u6321\u3001\u788e\u7247\u5316\u6216\u5f62\u72b6\u4e0d\u89c4\u5219\u7684\u7269\u4f53\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u4f2a\u6846\u53ef\u80fd\u4ecd\u9762\u4e34\u6311\u6218\u3002 \u591a\u89c6\u89d2\u8f93\u5165\u8981\u6c42 (Multi-view Input Requirement): \u5c3d\u7ba1\u63a8\u7406\u901f\u5ea6\u5feb\uff0c\u4f46\u5b83\u9700\u8981\u201cmulti-view images for input\u201d\u3002\u5bf9\u4e8e\u5355\u89c6\u89d2\u6216\u6781\u5c11\u89c6\u89d2\u8f93\u5165\u7684\u573a\u666f\uff0c\u5176\u9002\u7528\u6027\u53ef\u80fd\u53d7\u9650\u3002 Key Findings: We introduce OpenM3D, a novel open-vocabulary multi-view indoor 3D object detector trained without human annotations. We propose a 3D Pseudo Box Generation method using a graph embedding technique that combines 2D segments into coherent 3D structures. Links: PDF arXiv","title":"5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u7684\u5c40\u9650\u6027 (Inferred Limitations)"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-28/#effnetvitlora-an-efficient-hybrid-deep-learning-approach-for-alzheimers-disease-diagnosis","text":"Authors: Mahdieh Behjat Khatooni, Mohsen Soryani Published: 2025-08-26 Categories: cs.CV Abstract: Alzheimer's disease (AD) is one of the most prevalent neurodegenerative disorders worldwide. As it progresses, it leads to the deterioration of cognitive functions. Since AD is irreversible, early diagnosis is crucial for managing its progression. Mild Cognitive Impairment (MCI) represents an intermediate stage between Cognitively Normal (CN) individuals and those with AD, and is considered a transitional phase from normal cognition to Alzheimer's disease. Diagnosing MCI is particularly challenging due to the subtle differences between adjacent diagnostic categories. In this study, we propose EffNetViTLoRA, a generalized end-to-end model for AD diagnosis using the whole Alzheimer's Disease Neuroimaging Initiative (ADNI) Magnetic Resonance Imaging (MRI) dataset. Our model integrates a Convolutional Neural Network (CNN) with a Vision Transformer (ViT) to capture both local and global features from MRI images. Unlike previous studies that rely on limited subsets of data, our approach is trained on the full T1-weighted MRI dataset from ADNI, resulting in a more robust and unbiased model. This comprehensive methodology enhances the model's clinical reliability. Furthermore, fine-tuning large pretrained models often yields suboptimal results when source and target dataset domains differ. To address this, we incorporate Low-Rank Adaptation (LoRA) to effectively adapt the pretrained ViT model to our target domain. This method enables efficient knowledge transfer and reduces the risk of overfitting. Our model achieves a classification accuracy of 92.52% and an F1-score of 92.76% across three diagnostic categories: AD, MCI, and CN for full ADNI dataset. Analysis: \u4f5c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u4e13\u5bb6\uff0c\u6211\u5bf9\u8fd9\u7bc7\u8bba\u6587\u6458\u8981\u8fdb\u884c\u5982\u4e0b\u5206\u6790\uff1a \u8bba\u6587\u6807\u9898\uff1a EffNetViTLoRA: An Efficient Hybrid Deep Learning Approach for Alzheimer's Disease Diagnosis \u4f5c\u8005\uff1a Mahdieh Behjat Khatooni, Mohsen Soryani \u7c7b\u522b\uff1a cs.CV \u53d1\u8868\u65e5\u671f\uff1a 2025-08-26","title":"EffNetViTLoRA: An Efficient Hybrid Deep Learning Approach for Alzheimer's Disease Diagnosis"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-28/#1-concise-summary_3","text":"\u672c\u6587\u63d0\u51faEffNetViTLoRA\uff0c\u4e00\u4e2a\u7528\u4e8e\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\uff08AD\uff09\u8bca\u65ad\u7684\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3002\u8be5\u6a21\u578b\u7ed3\u5408CNN\u548cVision Transformer\uff08ViT\uff09\u4ee5\u6355\u83b7MRI\u56fe\u50cf\u7684\u5c40\u90e8\u548c\u5168\u5c40\u7279\u5f81\uff0c\u5e76\u5229\u7528LoRA\u6280\u672f\u9ad8\u6548\u5730\u9002\u5e94\u9884\u8bad\u7ec3ViT\u3002\u5176\u4e3b\u8981\u8d21\u732e\u5728\u4e8e\u9996\u6b21\u5728\u5b8c\u6574\u7684ADNI T1\u52a0\u6743MRI\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u548c\u8bc4\u4f30\uff0c\u5b9e\u73b0\u4e86\u9ad8\u51c6\u786e\u7387\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u4e34\u5e8a\u53ef\u9760\u6027\u3002","title":"1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u6d01\u603b\u7ed3 (Concise Summary)"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-28/#2-key-innovation-or-methodological-approach_4","text":"\u8be5\u7814\u7a76\u7684\u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u5176\u201cEffNetViTLoRA\u201d\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u53ca\u5176\u8bad\u7ec3\u7b56\u7565\uff1a \u6df7\u5408CNN-ViT\u67b6\u6784\uff1a \u6a21\u578b\u7ed3\u5408\u4e86\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u548cVision Transformer\uff08ViT\uff09\u7684\u4f18\u52bf\u3002CNN\uff08\u540d\u79f0\u6697\u793a\u53ef\u80fd\u57fa\u4e8eEfficientNet\uff09\u64c5\u957f\u6355\u6349MRI\u56fe\u50cf\u7684\u5c40\u90e8\u7cbe\u7ec6\u7279\u5f81\uff0c\u800cViT\u5219\u80fd\u6709\u6548\u63d0\u53d6\u5168\u5c40\u4e0a\u4e0b\u6587\u4fe1\u606f\u548c\u957f\u8ddd\u79bb\u4f9d\u8d56\u5173\u7cfb\uff0c\u4ece\u800c\u5b9e\u73b0\u5bf9\u533b\u5b66\u56fe\u50cf\u591a\u5c3a\u5ea6\u7279\u5f81\u7684\u5168\u9762\u7406\u89e3\u3002 \u5168ADNI\u6570\u636e\u96c6\u8bad\u7ec3\uff1a \u4e0e\u4ee5\u5f80\u7814\u7a76\u901a\u5e38\u4f9d\u8d56\u6570\u636e\u5b50\u96c6\u4e0d\u540c\uff0c\u8be5\u65b9\u6cd5\u5728\u5b8c\u6574\u7684\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u795e\u7ecf\u5f71\u50cf\u5b66\u5021\u8bae\uff08ADNI\uff09T1\u52a0\u6743MRI\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002\u8fd9\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\uff0c\u51cf\u5c11\u4e86\u6f5c\u5728\u7684\u6570\u636e\u9009\u62e9\u504f\u5dee\uff0c\u4f7f\u5176\u66f4\u63a5\u8fd1\u771f\u5b9e\u4e16\u754c\u7684\u4e34\u5e8a\u5e94\u7528\u3002 \u5f15\u5165LoRA\u8fdb\u884c\u9ad8\u6548\u5fae\u8c03\uff1a \u9488\u5bf9\u5927\u578b\u9884\u8bad\u7ec3ViT\u6a21\u578b\u5728\u6e90\u57df\u548c\u76ee\u6807\u57df\uff08\u5982\u901a\u7528\u56fe\u50cf\u4e0e\u533b\u5b66\u56fe\u50cf\uff09\u5dee\u5f02\u8f83\u5927\u65f6\u5fae\u8c03\u6548\u679c\u4e0d\u4f73\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u8be5\u7814\u7a76\u521b\u9020\u6027\u5730\u5f15\u5165\u4e86\u4f4e\u79e9\u9002\u5e94\uff08Low-Rank Adaptation, LoRA\uff09\u6280\u672f\u3002LoRA\u901a\u8fc7\u5728\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u7279\u5b9a\u5c42\u6ce8\u5165\u5c11\u91cf\u53ef\u8bad\u7ec3\u7684\u4f4e\u79e9\u77e9\u9635\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u77e5\u8bc6\u8fc1\u79fb\uff0c\u540c\u65f6\u6709\u6548\u907f\u514d\u4e86\u8fc7\u62df\u5408\uff0c\u5e76\u5927\u5e45\u964d\u4f4e\u4e86\u5fae\u8c03\u6240\u9700\u7684\u8ba1\u7b97\u8d44\u6e90\u3002","title":"2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u5b66\u65b9\u6cd5 (Key Innovation or Methodological Approach)"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-28/#3-potential-impact-on-the-field_4","text":"\u63d0\u5347AD\u65e9\u671f\u8bca\u65ad\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\uff1a \u5c24\u5176\u662f\u5728\u533a\u5206MCI\u8fd9\u4e00\u6311\u6218\u6027\u9636\u6bb5\uff0c\u9ad8\u51c6\u786e\u7387\uff0892.52%\uff09\u548cF1\u5206\u6570\uff0892.76%\uff09\u7684\u6a21\u578b\u80fd\u4e3a\u4e34\u5e8a\u533b\u751f\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u8f85\u52a9\u8bca\u65ad\u5de5\u5177\uff0c\u4ece\u800c\u5b9e\u73b0\u65e9\u671f\u5e72\u9884\u548c\u75be\u75c5\u7ba1\u7406\uff0c\u5ef6\u7f13\u75be\u75c5\u8fdb\u5c55\u3002 \u63a8\u52a8\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u6df7\u5408\u6a21\u578b\u548c\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u7684\u5e94\u7528\uff1a \u7ed3\u5408CNN\u548cViT\u7684\u4f18\u52bf\uff0c\u5e76\u5229\u7528LoRA\u7b49\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u6280\u672f\uff0c\u4e3a\u5904\u7406\u5927\u89c4\u6a21\u3001\u590d\u6742\u533b\u5b66\u56fe\u50cf\u6570\u636e\u63d0\u4f9b\u4e86\u65b0\u7684\u8303\u5f0f\uff0c\u5c24\u5176\u662f\u5728\u8d44\u6e90\u53d7\u9650\u6216\u9700\u8981\u5feb\u901f\u8fed\u4ee3\u7684\u573a\u666f\u3002 \u5efa\u7acb\u66f4\u5177\u6cdb\u5316\u80fd\u529b\u7684\u57fa\u51c6\u6a21\u578b\uff1a \u5728\u5b8c\u6574ADNI\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\uff0c\u5176\u9c81\u68d2\u6027\u548c\u65e0\u504f\u6027\u4f7f\u5176\u6709\u671b\u6210\u4e3a\u672a\u6765AD\u8bca\u65ad\u7814\u7a76\u7684\u6709\u529b\u57fa\u51c6\uff0c\u4fc3\u8fdb\u8be5\u9886\u57df\u7814\u7a76\u7684\u6807\u51c6\u5316\u548c\u8fdb\u6b65\u3002","title":"3. \u5bf9\u8be5\u9886\u57df\u7684\u6f5c\u5728\u5f71\u54cd (Potential Impact on the Field)"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-28/#4-related-areas-or-applications_4","text":"\u5176\u4ed6\u795e\u7ecf\u9000\u884c\u6027\u75be\u75c5\u7684\u8bca\u65ad\uff1a \u5982\u5e15\u91d1\u68ee\u75c5\u3001\u591a\u53d1\u6027\u786c\u5316\u75c7\u7b49\uff0c\u8fd9\u4e9b\u75be\u75c5\u4e5f\u4f9d\u8d56MRI\u56fe\u50cf\u8fdb\u884c\u8bca\u65ad\uff0c\u4e14\u53ef\u80fd\u9762\u4e34\u7c7b\u4f3c\u7684\u65e9\u671f\u8bca\u65ad\u6311\u6218\u548c\u6570\u636e\u7279\u6027\u3002 \u5176\u4ed6\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4efb\u52a1\uff1a \u80bf\u7624\u68c0\u6d4b\u3001\u5668\u5b98\u5206\u5272\u3001\u75be\u75c5\u5206\u671f\u7b49\uff0c\u51e1\u662f\u9700\u8981\u540c\u65f6\u6355\u6349\u5c40\u90e8\u7cbe\u7ec6\u75c5\u7076\u548c\u5168\u5c40\u7ed3\u6784\u4fe1\u606f\uff0c\u5e76\u53ef\u80fd\u6d89\u53ca\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u6a21\u578b\u5fae\u8c03\u7684\u533b\u5b66\u56fe\u50cf\u4efb\u52a1\uff0c\u90fd\u53ef\u4ee5\u501f\u9274\u8fd9\u79cd\u6df7\u5408\u67b6\u6784\u548cLoRA\u7b56\u7565\u3002 \u901a\u7528\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u4e2d\u7684\u5c0f\u6837\u672c\u5b66\u4e60\u4e0e\u9886\u57df\u9002\u5e94\uff1a EffNetViTLoRA\u4e2dLoRA\u7684\u5e94\u7528\uff0c\u4e3a\u5728\u6570\u636e\u91cf\u6709\u9650\u6216\u5b58\u5728\u663e\u8457\u9886\u57df\u5dee\u5f02\u7684\u60c5\u51b5\u4e0b\uff0c\u9ad8\u6548\u5730\u5c06\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\u9002\u5e94\u5230\u7279\u5b9a\u4efb\u52a1\u63d0\u4f9b\u4e86\u901a\u7528\u89e3\u51b3\u65b9\u6848\u3002","title":"4. \u53ef\u80fd\u53d7\u76ca\u4e8e\u8fd9\u9879\u7814\u7a76\u7684\u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 (Related Areas or Applications)"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-28/#5-limitations-that-can-be-inferred-from-the-abstract","text":"\u7f3a\u4e4f\u5916\u90e8\u6570\u636e\u96c6\u9a8c\u8bc1\uff1a \u5c3d\u7ba1\u5728\u5b8c\u6574\u7684ADNI\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bad\u7ec3\uff0c\u4f46\u6a21\u578b\u5728\u6765\u81ea\u4e0d\u540c\u533b\u9662\u3001\u4e0d\u540c\u626b\u63cf\u4eea\u6216\u4e0d\u540c\u4eba\u7fa4\u7684\u72ec\u7acb\u5916\u90e8\u6570\u636e\u96c6\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u4ecd\u6709\u5f85\u9a8c\u8bc1\u3002\u8fd9\u5bf9\u4e8e\u8bc4\u4f30\u5176\u771f\u6b63\u7684\u4e34\u5e8a\u9002\u7528\u6027\u81f3\u5173\u91cd\u8981\u3002 \u6a21\u578b\u590d\u6742\u6027\u4e0e\u53ef\u89e3\u91ca\u6027\uff1a \u6df7\u5408CNN-ViT\u67b6\u6784\u867d\u7136\u5f3a\u5927\uff0c\u4f46\u4e5f\u53ef\u80fd\u589e\u52a0\u6a21\u578b\u7684\u590d\u6742\u6027\uff0c\u964d\u4f4e\u5176\u51b3\u7b56\u8fc7\u7a0b\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u8fd9\u5728\u4e34\u5e8a\u8bca\u65ad\u4e2d\u662f\u4e00\u4e2a\u91cd\u8981\u8003\u91cf\u3002\u6458\u8981\u4e2d\u672a\u63d0\u53ca\u4efb\u4f55\u5173\u4e8e\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u7684\u65b9\u6cd5\u3002 \u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\uff1a \u5c3d\u7ba1LoRA\u964d\u4f4e\u4e86\u5fae\u8c03\u6210\u672c\uff0c\u4f46\u8bad\u7ec3\u4e00\u4e2a\u5728\u5b8c\u6574ADNI\u6570\u636e\u96c6\u4e0a\u7684\u6df7\u5408CNN-ViT\u6a21\u578b\uff0c\u5176\u521d\u59cb\u8bad\u7ec3\u9636\u6bb5\u53ef\u80fd\u4ecd\u9700\u8981\u663e\u8457\u7684\u8ba1\u7b97\u8d44\u6e90\u3002 \u672a\u660e\u786e\u7684CNN\u7ec4\u4ef6\u7ec6\u8282\uff1a \u6458\u8981\u4e2d\u63d0\u5230\u201cEffNetViTLoRA\u201d\uff0c\u6697\u793aCNN\u90e8\u5206\u53ef\u80fd\u57fa\u4e8eEfficientNet\uff0c\u4f46\u672a\u8be6\u7ec6\u8bf4\u660e\u5177\u4f53\u662f\u54ea\u79cdEfficientNet\u53d8\u4f53\u4ee5\u53ca\u5176\u4e0eViT\u7684\u96c6\u6210\u65b9\u5f0f\uff08\u4f8b\u5982\uff0c\u662f\u4e32\u8054\u3001\u5e76\u884c\u8fd8\u662f\u7279\u5f81\u878d\u5408\uff09\u3002 \u7f3a\u4e4f\u4e0e\u73b0\u6709SOTA\u65b9\u6cd5\u7684\u76f4\u63a5\u91cf\u5316\u6bd4\u8f83\uff1a \u6458\u8981\u5f3a\u8c03\u4e86\u5728\u5b8c\u6574ADNI\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u4f18\u52bf\uff0c\u4f46\u672a\u63d0\u4f9b\u4e0e\u5f53\u524d\u5728\u8be5\u6570\u636e\u96c6\u4e0a\u6216\u7c7b\u4f3c\u4efb\u52a1\u4e0a\u7684\u6700\u5148\u8fdb\uff08SOTA\uff09\u65b9\u6cd5\u7684\u76f4\u63a5\u6027\u80fd\u5bf9\u6bd4\u6570\u636e\uff0c\u4f7f\u5f9792.52%\u7684\u51c6\u786e\u7387\u7f3a\u4e4f\u4e00\u4e2a\u660e\u786e\u7684\u53c2\u7167\u7cfb\u6765\u8bc4\u4f30\u5176\u76f8\u5bf9\u4f18\u8d8a\u6027\u3002 Key Findings: In this study, we propose EffNetViTLoRA, a generalized end-to-end model for AD diagnosis using the whole Alzheimer's Disease Neuroimaging Initiative (ADNI) Magnetic Resonance Imaging (MRI) dataset. Links: PDF arXiv","title":"5. \u4ece\u6458\u8981\u4e2d\u53ef\u4ee5\u63a8\u65ad\u51fa\u7684\u4efb\u4f55\u5c40\u9650\u6027 (Limitations that can be inferred from the abstract)"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-28/#roofseg-an-edge-aware-transformer-based-network-for-end-to-end-roof-plane-segmentation","text":"Authors: Siyuan You, Guozheng Xu, Pengwei Zhou, Qiwen Jin, Jian Yao, Li Li Published: 2025-08-26 Categories: cs.CV, cs.AI Abstract: Roof plane segmentation is one of the key procedures for reconstructing three-dimensional (3D) building models at levels of detail (LoD) 2 and 3 from airborne light detection and ranging (LiDAR) point clouds. The majority of current approaches for roof plane segmentation rely on the manually designed or learned features followed by some specifically designed geometric clustering strategies. Because the learned features are more powerful than the manually designed features, the deep learning-based approaches usually perform better than the traditional approaches. However, the current deep learning-based approaches have three unsolved problems. The first is that most of them are not truly end-to-end, the plane segmentation results may be not optimal. The second is that the point feature discriminability near the edges is relatively low, leading to inaccurate planar edges. The third is that the planar geometric characteristics are not sufficiently considered to constrain the network training. To solve these issues, a novel edge-aware transformer-based network, named RoofSeg, is developed for segmenting roof planes from LiDAR point clouds in a truly end-to-end manner. In the RoofSeg, we leverage a transformer encoder-decoder-based framework to hierarchically predict the plane instance masks with the use of a set of learnable plane queries. To further improve the segmentation accuracy of edge regions, we also design an Edge-Aware Mask Module (EAMM) that sufficiently incorporates planar geometric prior of edges to enhance its discriminability for plane instance mask refinement. In addition, we propose an adaptive weighting strategy in the mask loss to reduce the influence of misclassified points, and also propose a new plane geometric loss to constrain the network training. Analysis: \u4f5c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u4e13\u5bb6\uff0c\u6211\u5bf9\u8fd9\u7bc7\u5173\u4e8eRoofSeg\u7684\u8bba\u6587\u6458\u8981\u8fdb\u884c\u5982\u4e0b\u5206\u6790\uff1a","title":"RoofSeg: An edge-aware transformer-based network for end-to-end roof plane segmentation"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-28/#roofseg-an-edge-aware-transformer-based-network-for-end-to-end-roof-plane-segmentation_1","text":"1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u660e\u603b\u7ed3 (Concise Summary): \u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRoofSeg\u7684\u8fb9\u7f18\u611f\u77e5\u3001\u57fa\u4e8eTransformer\u7684\u7aef\u5230\u7aef\u7f51\u7edc\uff0c\u7528\u4e8e\u4eceLiDAR\u70b9\u4e91\u4e2d\u8fdb\u884c\u5c4b\u9876\u5e73\u9762\u5206\u5272\u3002\u5b83\u901a\u8fc7\u7ed3\u5408Transformer\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\u3001\u4e13\u95e8\u8bbe\u8ba1\u7684\u8fb9\u7f18\u611f\u77e5\u63a9\u7801\u6a21\u5757\uff08EAMM\uff09\u4ee5\u53ca\u521b\u65b0\u7684\u635f\u5931\u51fd\u6570\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u975e\u7aef\u5230\u7aef\u3001\u8fb9\u7f18\u7cbe\u5ea6\u4f4e\u548c\u51e0\u4f55\u7ea6\u675f\u4e0d\u8db3\u7b49\u65b9\u9762\u7684\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5c4b\u9876\u5e73\u9762\u5206\u5272\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002 2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u5b66\u65b9\u6cd5 (Key Innovation or Methodological Approach): RoofSeg\u7684\u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u5176 \u7aef\u5230\u7aef\u7684Transformer\u67b6\u6784 \u4e0e\u591a\u9879\u9488\u5bf9\u5c4b\u9876\u5e73\u9762\u5206\u5272\u7279\u5b9a\u6311\u6218\u7684\u8bbe\u8ba1\u76f8\u7ed3\u5408\uff1a \u7aef\u5230\u7aefTransformer\u7f16\u7801\u5668-\u89e3\u7801\u5668\u6846\u67b6\uff1a \u91c7\u7528\u7c7b\u4f3cDETR\u6216Mask2Former\u7684\u8303\u5f0f\uff0c\u5229\u7528\u53ef\u5b66\u4e60\u7684\u5e73\u9762\u67e5\u8be2\uff08learnable plane queries\uff09\u76f4\u63a5\u9884\u6d4b\u5e73\u9762\u5b9e\u4f8b\u63a9\u7801\uff0c\u5b9e\u73b0\u4e86\u771f\u6b63\u7684\u7aef\u5230\u7aef\u5206\u5272\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u540e\u5904\u7406\u5e26\u6765\u7684\u6b21\u4f18\u7ed3\u679c\u3002 \u8fb9\u7f18\u611f\u77e5\u63a9\u7801\u6a21\u5757\uff08Edge-Aware Mask Module, EAMM\uff09\uff1a \u8fd9\u662f\u89e3\u51b3\u8fb9\u7f18\u533a\u57df\u5224\u522b\u529b\u4f4e\u7684\u5173\u952e\u3002EAMM\u5145\u5206\u878d\u5165\u4e86\u5e73\u9762\u7684\u51e0\u4f55\u5148\u9a8c\u77e5\u8bc6\uff0c\u4ee5\u589e\u5f3a\u7f51\u7edc\u5bf9\u8fb9\u7f18\u533a\u57df\u7279\u5f81\u7684\u5224\u522b\u80fd\u529b\uff0c\u4ece\u800c\u63d0\u9ad8\u5e73\u9762\u8fb9\u7f18\u7684\u5206\u5272\u7cbe\u5ea6\u3002 \u521b\u65b0\u7684\u635f\u5931\u51fd\u6570\uff1a \u81ea\u9002\u5e94\u52a0\u6743\u63a9\u7801\u635f\u5931\uff08Adaptive weighting strategy in the mask loss\uff09\uff1a \u65e8\u5728\u51cf\u5c11\u8bef\u5206\u7c7b\u70b9\u5bf9\u635f\u5931\u8ba1\u7b97\u7684\u5f71\u54cd\uff0c\u63d0\u9ad8\u8bad\u7ec3\u7684\u9c81\u68d2\u6027\u3002 \u65b0\u7684\u5e73\u9762\u51e0\u4f55\u635f\u5931\uff08New plane geometric loss\uff09\uff1a \u7528\u4e8e\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u663e\u5f0f\u5730\u7ea6\u675f\u7f51\u7edc\u7684\u8f93\u51fa\uff0c\u4f7f\u5176\u66f4\u597d\u5730\u7b26\u5408\u5e73\u9762\u7684\u51e0\u4f55\u7279\u6027\uff0c\u8fdb\u4e00\u6b65\u63d0\u5347\u5206\u5272\u7ed3\u679c\u7684\u51e0\u4f55\u51c6\u786e\u6027\u3002 3. \u5bf9\u8be5\u9886\u57df\u7684\u6f5c\u5728\u5f71\u54cd (Potential Impact on the Field): \u8be5\u7814\u7a76\u5bf9\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\uff0c\u7279\u522b\u662f\u4e09\u7ef4\u91cd\u5efa\u548c\u70b9\u4e91\u5904\u7406\uff0c\u5177\u6709\u663e\u8457\u7684\u6f5c\u5728\u5f71\u54cd\uff1a \u63a8\u52a8\u9ad8\u7cbe\u5ea6\u4e09\u7ef4\u57ce\u5e02\u5efa\u6a21\uff1a \u4f5c\u4e3aLoD 2\u548cLoD 3\u7ea7\u522b\u4e09\u7ef4\u5efa\u7b51\u6a21\u578b\u91cd\u5efa\u7684\u5173\u952e\u6b65\u9aa4\uff0cRoofSeg\u7684\u51fa\u73b0\u5c06\u76f4\u63a5\u63d0\u5347\u5c4b\u9876\u5e73\u9762\u5206\u5272\u7684\u81ea\u52a8\u5316\u6c34\u5e73\u548c\u7cbe\u5ea6\uff0c\u4ece\u800c\u52a0\u901f\u548c\u4f18\u5316\u9ad8\u7cbe\u5ea6\u57ce\u5e02\u6570\u5b57\u5b6a\u751f\u548cBIM\u6a21\u578b\u7684\u6784\u5efa\u3002 \u4e3a\u70b9\u4e91\u5b9e\u4f8b\u5206\u5272\u63d0\u4f9b\u65b0\u8303\u5f0f\uff1a \u5176\u7ed3\u5408Transformer\u3001\u51e0\u4f55\u5148\u9a8c\u548c\u8fb9\u7f18\u611f\u77e5\u673a\u5236\u7684\u7aef\u5230\u7aef\u8bbe\u8ba1\u601d\u8def\uff0c\u53ef\u80fd\u4e3a\u5176\u4ed6\u70b9\u4e91\u4e2d\u7ed3\u6784\u5316\u5bf9\u8c61\uff08\u5982\u5899\u58c1\u3001\u9053\u8def\u3001\u7a97\u6237\u7b49\uff09\u7684\u5b9e\u4f8b\u5206\u5272\u4efb\u52a1\u63d0\u4f9b\u65b0\u7684\u7814\u7a76\u65b9\u5411\u548c\u501f\u9274\u3002 \u63d0\u5347\u70b9\u4e91\u5904\u7406\u7684\u9c81\u68d2\u6027\u4e0e\u51c6\u786e\u6027\uff1a \u901a\u8fc7\u89e3\u51b3\u8fb9\u7f18\u6a21\u7cca\u548c\u51e0\u4f55\u7ea6\u675f\u4e0d\u8db3\u7b49\u957f\u671f\u5b58\u5728\u7684\u75db\u70b9\uff0cRoofSeg\u6709\u671b\u6210\u4e3a\u5c4b\u9876\u5e73\u9762\u5206\u5272\u9886\u57df\u7684\u4e00\u4e2a\u91cd\u8981\u57fa\u51c6\uff0c\u5e76\u542f\u53d1\u66f4\u591a\u9488\u5bf9\u70b9\u4e91\u6570\u636e\u7279\u6027\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8bbe\u8ba1\u3002 4. \u53ef\u80fd\u53d7\u76ca\u4e8e\u8fd9\u9879\u7814\u7a76\u7684\u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 (Related Areas or Applications): \u4e09\u7ef4\u57ce\u5e02\u5efa\u6a21\u4e0e\u6570\u5b57\u5b6a\u751f (3D City Modeling and Digital Twins): \u76f4\u63a5\u53d7\u76ca\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u7cbe\u5ea6\u7684\u5efa\u7b51\u6a21\u578b\u3002 \u57ce\u5e02\u89c4\u5212\u4e0e\u7ba1\u7406 (Urban Planning and Management): \u7cbe\u786e\u7684\u5efa\u7b51\u51e0\u4f55\u4fe1\u606f\u5bf9\u4e8e\u57ce\u5e02\u89c4\u5212\u3001\u5bb9\u79ef\u7387\u8ba1\u7b97\u3001\u65e5\u7167\u5206\u6790\u7b49\u81f3\u5173\u91cd\u8981\u3002 \u707e\u5bb3\u8bc4\u4f30\u4e0e\u5e94\u6025\u54cd\u5e94 (Disaster Assessment and Emergency Response): \u5feb\u901f\u51c6\u786e\u5730\u91cd\u5efa\u53d7\u635f\u5efa\u7b51\u6a21\u578b\u6709\u52a9\u4e8e\u8bc4\u4f30\u707e\u60c5\u548c\u89c4\u5212\u6551\u63f4\u3002 \u80fd\u6e90\u6548\u7387\u5206\u6790\u4e0e\u592a\u9633\u80fd\u6f5c\u529b\u8bc4\u4f30 (Energy Efficiency Analysis and Solar Potential Assessment): \u5c4b\u9876\u7684\u51e0\u4f55\u5f62\u72b6\u548c\u671d\u5411\u662f\u8bc4\u4f30\u592a\u9633\u80fd\u7535\u6c60\u677f\u5b89\u88c5\u6f5c\u529b\u7684\u57fa\u7840\u3002 \u5efa\u7b51\u4fe1\u606f\u6a21\u578b\uff08BIM\uff09\u4e0e\u8d44\u4ea7\u7ba1\u7406 (Building Information Modeling (BIM) and Asset Management): \u5c06LiDAR\u6570\u636e\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u7684BIM\u6a21\u578b\uff0c\u4fbf\u4e8e\u5efa\u7b51\u5168\u751f\u547d\u5468\u671f\u7ba1\u7406\u3002 \u5730\u7406\u4fe1\u606f\u7cfb\u7edf\uff08GIS\uff09\u4e0e\u9065\u611f (Geographic Information Systems (GIS) and Remote Sensing): \u63d0\u4f9b\u66f4\u7cbe\u786e\u7684\u5730\u7406\u7a7a\u95f4\u6570\u636e\u3002 5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u51fa\u7684\u5c40\u9650\u6027 (Limitations that can be inferred from the abstract): \u8ba1\u7b97\u8d44\u6e90\u9700\u6c42 (Computational Resource Requirements): \u57fa\u4e8eTransformer\u7684\u7f51\u7edc\u901a\u5e38\u8ba1\u7b97\u91cf\u8f83\u5927\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u5927\u89c4\u6a21LiDAR\u70b9\u4e91\u65f6\uff0c\u8bad\u7ec3\u548c\u63a8\u7406\u53ef\u80fd\u9700\u8981\u663e\u8457\u7684\u8ba1\u7b97\u8d44\u6e90\u3002 \u6570\u636e\u4f9d\u8d56\u6027 (Data Dependency): Transformer\u6a21\u578b\u901a\u5e38\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u3002\u6458\u8981\u4e2d\u672a\u63d0\u53ca\u6570\u636e\u96c6\u7684\u89c4\u6a21\u548c\u591a\u6837\u6027\uff0c\u5176\u6027\u80fd\u53ef\u80fd\u53d7\u9650\u4e8e\u8bad\u7ec3\u6570\u636e\u7684\u8d28\u91cf\u548c\u8986\u76d6\u8303\u56f4\u3002 \u590d\u6742\u5c4b\u9876\u7ed3\u6784\u7684\u6cdb\u5316\u80fd\u529b (Generalization to Complex Roof Structures): \u8bba\u6587\u5f3a\u8c03\u201c\u5e73\u9762\u5206\u5272\u201d\u548c\u201c\u5e73\u9762\u51e0\u4f55\u7279\u6027\u201d\u3002\u5bf9\u4e8e\u9ad8\u5ea6\u590d\u6742\u3001\u975e\u5e73\u9762\uff08\u5982\u7a79\u9876\u3001\u66f2\u9762\uff09\u6216\u975e\u5e38\u89c4\u7684\u5c4b\u9876\u7ed3\u6784\uff0c\u5176\u201c\u5e73\u9762\u51e0\u4f55\u5148\u9a8c\u201d\u7684\u9002\u7528\u6027\u53ef\u80fd\u53d7\u9650\uff0c\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u6709\u5f85\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u3002 \u5b9e\u65f6\u6027 (Real-time Performance): \u6458\u8981\u672a\u63d0\u53ca\u6a21\u578b\u7684\u63a8\u7406\u901f\u5ea6\uff0c\u5bf9\u4e8e\u9700\u8981\u5b9e\u65f6\u6216\u8fd1\u5b9e\u65f6\u5904\u7406\u7684\u5e94\u7528\u573a\u666f\uff0c\u5176\u6027\u80fd\u53ef\u80fd\u662f\u4e00\u4e2a\u8003\u91cf\u56e0\u7d20\u3002 \u7279\u5b9a\u4e8e\u5c4b\u9876\u7684\u5c40\u9650\u6027 (Roof-Specific Limitations): EAMM\u548c\u5e73\u9762\u51e0\u4f55\u635f\u5931\u662f\u4e3a\u5c4b\u9876\u5e73\u9762\u7279\u6027\u8bbe\u8ba1\u7684\u3002\u5c06\u5176\u65b9\u6cd5\u63a8\u5e7f\u5230\u5176\u4ed6\u7c7b\u578b\u7684\u70b9\u4e91\u5bf9\u8c61\u5206\u5272\u53ef\u80fd\u9700\u8981\u989d\u5916\u7684\u4fee\u6539\u548c\u9002\u5e94\u3002 Key Findings: To solve these issues, a novel edge-aware transformer-based network, named RoofSeg, is developed for segmenting roof planes from LiDAR point clouds in a truly end-to-end manner. In addition, we propose an adaptive weighting strategy in the mask loss to reduce the influence of misclassified points, and also propose a new plane geometric loss to constrain the network training. Links: PDF arXiv","title":"\u8bba\u6587\u6458\u8981\u5206\u6790\uff1aRoofSeg: An edge-aware transformer-based network for end-to-end roof plane segmentation"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-29/","text":"Arxiv Computer Vision Papers - 2025-08-29 Executive Summary \u597d\u7684\uff0c\u8fd9\u662f\u4e00\u4efd\u9488\u5bf92025\u5e748\u670828\u65e5Arxiv\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u8bba\u6587\u7684\u7b80\u660e\u6267\u884c\u6458\u8981\uff1a \u6267\u884c\u6458\u8981\uff1aArxiv\u8ba1\u7b97\u673a\u89c6\u89c9\u8bba\u6587\u901f\u89c8 (2025\u5e748\u670828\u65e5) \u672c\u6267\u884c\u6458\u8981\u65e8\u5728\u4e3a\u5fd9\u788c\u7684\u7814\u7a76\u4eba\u5458\u63d0\u4f9b2025\u5e748\u670828\u65e5Arxiv\u4e0a\u6700\u65b0\u53d1\u5e03\u768410\u7bc7\u8ba1\u7b97\u673a\u89c6\u89c9\u8bba\u6587\u7684\u5feb\u901f\u6982\u89c8\uff0c\u91cd\u70b9\u5173\u6ce8\u5176\u4e3b\u8981\u8d8b\u52bf\u3001\u521b\u65b0\u70b9\u53ca\u672a\u6765\u65b9\u5411\u3002 1. \u4e3b\u8981\u4e3b\u9898\u548c\u8d8b\u52bf\uff1a \u57fa\u7840\u6a21\u578b (Foundation Models) \u7684\u5e7f\u6cdb\u5e94\u7528\uff1a \u591a\u4e2a\u7814\u7a76\u5229\u7528\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\uff08\u5982Vision Foundation Models, CLIP, DINO\uff09\u4f5c\u4e3a\u5f3a\u5927\u7684\u7279\u5f81\u63d0\u53d6\u5668\u6216\u5148\u9a8c\u77e5\u8bc6\uff0c\u4ee5\u89e3\u51b3\u5404\u79cd\u4e0b\u6e38\u4efb\u52a1\uff0c\u5305\u62ec\u76ee\u6807\u68c0\u6d4b\u3001\u56fe\u50cf\u5206\u5272\u548c\u7ec6\u7c92\u5ea6\u5206\u7c7b\u3002\u8fd9\u8868\u660e\u57fa\u7840\u6a21\u578b\u5df2\u6210\u4e3a\u63a8\u52a8CV\u9886\u57df\u8fdb\u6b65\u7684\u6838\u5fc3\u9a71\u52a8\u529b\u3002 \u4f4e\u8d44\u6e90/\u65e0\u76d1\u7763/\u96f6\u6837\u672c\u5b66\u4e60\uff1a \u663e\u8457\u7684\u8d8b\u52bf\u662f\u51cf\u5c11\u5bf9\u5927\u91cf\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\u3002\u8bba\u6587\u63a2\u7d22\u4e86\u65e0\u76d1\u77633D\u76ee\u6807\u68c0\u6d4b\u3001\u65e0\u6807\u6ce8\u7684\u591a\u89c6\u89d23D\u68c0\u6d4b\u3001\u534a\u76d1\u7763\u5206\u5272\u4ee5\u53ca\u8bad\u7ec3\u65e0\u5173\u7684\u5f00\u653e\u8bcd\u6c47\u5206\u5272\u7b49\u65b9\u6cd5\uff0c\u65e8\u5728\u63d0\u9ad8\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u90e8\u7f72\u6548\u7387\u3002 3D \u89c6\u89c9\u7684\u6301\u7eed\u8fdb\u6b65\uff1a 3D\u76ee\u6807\u68c0\u6d4b\uff0c\u5305\u62ec\u5355\u76ee\u3001\u591a\u89c6\u89d2\u548c\u591a\u6a21\u6001\u878d\u5408\uff08LiDAR-Camera\uff09\u65b9\u6cd5\uff0c\u4ecd\u5728\u79ef\u6781\u53d1\u5c55\uff0c\u5e76\u7740\u91cd\u4e8e\u6cdb\u5316\u80fd\u529b\u548c\u65e0\u76d1\u7763\u5b66\u4e60\u3002 \u56fe\u50cf\u5206\u5272\u7684\u591a\u6837\u5316\u521b\u65b0\uff1a \u5206\u5272\u4efb\u52a1\u901a\u8fc7\u751f\u6210\u6a21\u578b\uff08\u6269\u6563\u6a21\u578b\uff09\u3001\u591a\u6a21\u6001\u539f\u578b\u5bf9\u9f50\u3001\u4ee5\u53ca\u5229\u7528\u57fa\u7840\u6a21\u578b\u7684\u9ad8\u4fdd\u771f\u7279\u5f81\u6216\u81ea\u9002\u5e94\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u5728\u901a\u7528\u3001\u533b\u5b66\u548c\u5f00\u653e\u8bcd\u6c47\u573a\u666f\u4e0b\u7684\u65b0\u7a81\u7834\u3002 \u9886\u57df\u9002\u5e94\u4e0e\u6cdb\u5316\uff1a \u9488\u5bf9\u7279\u5b9a\u9886\u57df\uff08\u5982\u8f66\u8f7d\u5185\u9970\u3001\u75c5\u7406\u56fe\u50cf\u3001\u6606\u866b\u5206\u7c7b\uff09\u7684\u6311\u6218\uff0c\u7814\u7a76\u4eba\u5458\u81f4\u529b\u4e8e\u63d0\u5347\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u5206\u5e03\u4e0b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002 2. \u7279\u522b\u91cd\u8981\u6216\u521b\u65b0\u7684\u8bba\u6587\uff1a [10] OpenM3D: Open Vocabulary Multi-view Indoor 3D Object Detection without Human Annotations\uff1a \u8fd9\u7bc7\u8bba\u6587\u57283D\u76ee\u6807\u68c0\u6d4b\u9886\u57df\u5b9e\u73b0\u4e86\u591a\u9879\u7a81\u7834\uff0c\u7ed3\u5408\u4e86\u5f00\u653e\u8bcd\u6c47\u3001\u591a\u89c6\u89d2\u548c\u65e0\u4eba\u5de5\u6807\u6ce8\u7684\u8303\u5f0f\uff0c\u6781\u5927\u5730\u964d\u4f4e\u4e863D\u573a\u666f\u7406\u89e3\u7684\u6807\u6ce8\u6210\u672c\uff0c\u5177\u6709\u5de8\u5927\u7684\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002 [2] GS: Generative Segmentation via Label Diffusion\uff1a \u5c06\u6269\u6563\u6a21\u578b\u5f15\u5165\u56fe\u50cf\u5206\u5272\u4efb\u52a1\uff0c\u901a\u8fc7\u6807\u7b7e\u6269\u6563\u5b9e\u73b0\u751f\u6210\u5f0f\u5206\u5272\uff0c\u4e3a\u5206\u5272\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u65b0\u7684\u89c6\u89d2\u548c\u65b9\u6cd5\uff0c\u6709\u671b\u5728\u590d\u6742\u573a\u666f\u548c\u6570\u636e\u7a00\u7f3a\u65f6\u5c55\u73b0\u4f18\u52bf\u3002 [8] Plug-in Feedback Self-adaptive Attention in CLIP for Training-free Open-Vocabulary Segmentation\uff1a \u8be5\u5de5\u4f5c\u901a\u8fc7\u5728CLIP\u4e2d\u5f15\u5165\u5373\u63d2\u5373\u7528\u7684\u53cd\u9988\u81ea\u9002\u5e94\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u8bad\u7ec3\u65e0\u5173\u7684\u5f00\u653e\u8bcd\u6c47\u5206\u5272\uff0c\u6781\u5927\u5730\u63d0\u9ad8\u4e86\u5f00\u653e\u8bcd\u6c47\u5206\u5272\u7684\u6548\u7387\u548c\u7075\u6d3b\u6027\u3002 [7] Dino U-Net: Exploiting High-Fidelity Dense Features from Foundation Models for Medical Image Segmentation\uff1a \u6709\u6548\u5730\u5c06\u5f3a\u5927\u7684Vision Transformer\uff08\u5982DINO\uff09\u7684\u5bc6\u96c6\u7279\u5f81\u4e0eU-Net\u67b6\u6784\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u6027\u80fd\uff0c\u5bf9\u533b\u7597AI\u9886\u57df\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002 3. \u65b0\u5174\u7814\u7a76\u65b9\u5411\u6216\u6280\u672f\uff1a \u6269\u6563\u6a21\u578b\u5728\u611f\u77e5\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\uff1a \u4e0d\u518d\u5c40\u9650\u4e8e\u56fe\u50cf\u751f\u6210\uff0c\u6269\u6563\u6a21\u578b\u6b63\u88ab\u63a2\u7d22\u7528\u4e8e\u66f4\u5e7f\u6cdb\u7684\u611f\u77e5\u4efb\u52a1\uff0c\u5982\u56fe\u50cf\u5206\u5272\u3002 \u57fa\u7840\u6a21\u578b\u4f5c\u4e3a\u201c\u5373\u63d2\u5373\u7528\u201d\u7ec4\u4ef6\uff1a \u57fa\u7840\u6a21\u578b\u4e0d\u4ec5\u7528\u4e8e\u5fae\u8c03\uff0c\u5176\u5185\u90e8\u673a\u5236\uff08\u5982\u5bc6\u96c6\u7279\u5f81\u3001\u6ce8\u610f\u529b\uff09\u6b63\u88ab\u5de7\u5999\u5730\u63d0\u53d6\u548c\u5229\u7528\uff0c\u4ee5\u5b9e\u73b0\u8bad\u7ec3\u65e0\u5173\u6216\u9ad8\u6548\u7684\u4e0b\u6e38\u4efb\u52a1\u3002 \u591a\u6a21\u6001\u4e0e\u591a\u89c6\u89d2\u65e0\u76d1\u7763/\u96f6\u6837\u672c\u5b66\u4e60\uff1a \u7ed3\u5408\u4e0d\u540c\u6a21\u6001\uff08LiDAR-Camera\uff09\u548c\u89c6\u89d2\u4fe1\u606f\uff0c\u5728\u65e0\u76d1\u7763\u6216\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u89e3\u51b33D\u611f\u77e5\u95ee\u9898\uff0c\u662f\u672a\u6765\u964d\u4f4e\u6210\u672c\u548c\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\u7684\u5173\u952e\u3002 \u4e13\u5bb6\u77e5\u8bc6\u4e0e\u57fa\u7840\u6a21\u578b\u7ed3\u5408\uff1a \u5c06\u9886\u57df\u4e13\u5bb6\u77e5\u8bc6\u878d\u5165\u57fa\u7840\u6a21\u578b\u7684\u9002\u5e94\u8fc7\u7a0b\uff0c\u4ee5\u89e3\u51b3\u7ec6\u7c92\u5ea6\u5206\u7c7b\u7b49\u7279\u5b9a\u9886\u57df\u7684\u6311\u6218\u3002 4. \u6700\u503c\u5f97\u6df1\u5165\u9605\u8bfb\u7684\u8bba\u6587\uff1a \u4e3a\u4e86\u5168\u9762\u4e86\u89e3\u5f53\u524d\u9886\u57df\u7684\u524d\u6cbf\u8fdb\u5c55\uff0c\u5efa\u8bae\u91cd\u70b9\u9605\u8bfb\u4ee5\u4e0b\u8bba\u6587\uff1a [10] OpenM3D: Open Vocabulary Multi-view Indoor 3D Object Detection without Human Annotations\uff1a \u4ee3\u8868\u4e863D\u89c6\u89c9\u548c\u65e0\u76d1\u7763/\u5f00\u653e\u8bcd\u6c47\u5b66\u4e60\u7684\u6700\u65b0\u878d\u5408\uff0c\u5177\u6709\u9ad8\u5f71\u54cd\u529b\u3002 [2] GS: Generative Segmentation via Label Diffusion\uff1a \u63a2\u7d22\u4e86\u6269\u6563\u6a21\u578b\u5728\u5206\u5272\u4efb\u52a1\u4e2d\u7684\u65b0\u8303\u5f0f\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002 [8] Plug-in Feedback Self-adaptive Attention in CLIP for Training-free Open-Vocabulary Segmentation\uff1a \u5c55\u793a\u4e86\u5982\u4f55\u9ad8\u6548\u5229\u7528\u73b0\u6709\u57fa\u7840\u6a21\u578b\u5b9e\u73b0\u8bad\u7ec3\u65e0\u5173\u7684\u5f00\u653e\u8bcd\u6c47\u80fd\u529b\uff0c\u5177\u6709\u5f88\u9ad8\u7684\u5b9e\u7528\u4ef7\u503c\u3002 [7] Dino U-Net: Exploiting High-Fidelity Dense Features from Foundation Models for Medical Image Segmentation\uff1a \u5bf9\u4e8e\u5173\u6ce8\u533b\u5b66\u56fe\u50cf\u5206\u6790\u548c\u57fa\u7840\u6a21\u578b\u5e94\u7528\u7684\u7814\u7a76\u4eba\u5458\uff0c\u63d0\u4f9b\u4e86\u5c06\u5148\u8fdb\u6a21\u578b\u5e94\u7528\u4e8e\u5173\u952e\u9886\u57df\u7684\u6709\u6548\u7b56\u7565\u3002 Table of Contents Enhancing Pseudo-Boxes via Data-Level LiDAR-Camera Fusion for Unsupervised 3D Object Detection GS: Generative Segmentation via Label Diffusion Multimodal Prototype Alignment for Semi-supervised Pathology Image Segmentation Scalable Object Detection in the Car Interior With Vision Foundation Models Generalizing Monocular 3D Object Detection CraftGraffiti: Exploring Human Identity with Custom Graffiti Art via Facial-Preserving Diffusion Models Dino U-Net: Exploiting High-Fidelity Dense Features from Foundation Models for Medical Image Segmentation Plug-in Feedback Self-adaptive Attention in CLIP for Training-free Open-Vocabulary Segmentation Bridging Domain Gaps for Fine-Grained Moth Classification Through Expert-Informed Adaptation and Foundation Model Priors OpenM3D: Open Vocabulary Multi-view Indoor 3D Object Detection without Human Annotations Papers Enhancing Pseudo-Boxes via Data-Level LiDAR-Camera Fusion for Unsupervised 3D Object Detection Authors: Mingqian Ji, Jian Yang, Shanshan Zhang Published: 2025-08-28 Categories: cs.CV Abstract: Existing LiDAR-based 3D object detectors typically rely on manually annotated labels for training to achieve good performance. However, obtaining high-quality 3D labels is time-consuming and labor-intensive. To address this issue, recent works explore unsupervised 3D object detection by introducing RGB images as an auxiliary modal to assist pseudo-box generation. However, these methods simply integrate pseudo-boxes generated by LiDAR point clouds and RGB images. Yet, such a label-level fusion strategy brings limited improvements to the quality of pseudo-boxes, as it overlooks the complementary nature in terms of LiDAR and RGB image data. To overcome the above limitations, we propose a novel data-level fusion framework that integrates RGB images and LiDAR data at an early stage. Specifically, we utilize vision foundation models for instance segmentation and depth estimation on images and introduce a bi-directional fusion method, where real points acquire category labels from the 2D space, while 2D pixels are projected onto 3D to enhance real point density. To mitigate noise from depth and segmentation estimations, we propose a local and global filtering method, which applies local radius filtering to suppress depth estimation errors and global statistical filtering to remove segmentation-induced outliers. Furthermore, we propose a data-level fusion based dynamic self-evolution strategy, which iteratively refines pseudo-boxes under a dense representation, significantly improving localization accuracy. Extensive experiments on the nuScenes dataset demonstrate that the detector trained by our method significantly outperforms that trained by previous state-of-the-art methods with 28.4 \\% mAP on the nuScenes validation benchmark. Analysis: \u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6570\u636e\u7ea7LiDAR-\u76f8\u673a\u878d\u5408\u6846\u67b6\uff0c\u7528\u4e8e\u65e0\u76d1\u77633D\u76ee\u6807\u68c0\u6d4b\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u4e2d\u4f2a\u6807\u7b7e\u8d28\u91cf\u53d7\u9650\u7684\u95ee\u9898\u3002\u4ee5\u4e0b\u662f\u8be6\u7ec6\u5206\u6790\uff1a 1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u660e\u603b\u7ed3 (Concise Summary) \u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6570\u636e\u7ea7LiDAR-\u76f8\u673a\u878d\u5408\u6846\u67b6\uff0c\u7528\u4e8e\u65e0\u76d1\u77633D\u76ee\u6807\u68c0\u6d4b\uff0c\u4ee5\u514b\u670d\u73b0\u6709\u6807\u7b7e\u7ea7\u878d\u5408\u7b56\u7565\u7684\u5c40\u9650\u6027\u3002\u901a\u8fc7\u5728\u65e9\u671f\u9636\u6bb5\u6574\u5408RGB\u56fe\u50cf\u548cLiDAR\u6570\u636e\uff0c\u5e76\u5229\u7528\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u53cc\u5411\u878d\u5408\u3001\u566a\u58f0\u8fc7\u6ee4\u548c\u52a8\u6001\u81ea\u6f14\u5316\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4f2a\u6807\u7b7e\u7684\u8d28\u91cf\u548c3D\u5b9a\u4f4d\u7cbe\u5ea6\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5176\u8bad\u7ec3\u51fa\u7684\u68c0\u6d4b\u5668\u5728nuScenes\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e8628.4% mAP\u7684\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002 2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u5b66\u65b9\u6cd5 (Key Innovation or Methodological Approach) \u8be5\u8bba\u6587\u7684\u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u63d0\u51fa\u4e86 \u6570\u636e\u7ea7\uff08data-level\uff09LiDAR-\u76f8\u673a\u878d\u5408\u6846\u67b6 \uff0c\u800c\u975e\u4f20\u7edf\u7684\u6807\u7b7e\u7ea7\u878d\u5408\uff0c\u4ee5\u5145\u5206\u5229\u7528LiDAR\u548cRGB\u56fe\u50cf\u4e4b\u95f4\u7684\u6570\u636e\u4e92\u8865\u6027\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\uff1a \u65e9\u671f\u6570\u636e\u878d\u5408 \uff1a\u4e0d\u540c\u4e8e\u5c06LiDAR\u548cRGB\u5355\u72ec\u751f\u6210\u7684\u4f2a\u6807\u7b7e\u8fdb\u884c\u7b80\u5355\u6574\u5408\uff0c\u8be5\u65b9\u6cd5\u5728\u539f\u59cb\u6570\u636e\u5c42\u9762\uff08\u65e9\u671f\u9636\u6bb5\uff09\u5c31\u5c06\u4e24\u79cd\u6a21\u6001\u7684\u6570\u636e\u8fdb\u884c\u878d\u5408\u3002 \u53cc\u5411\u878d\u5408\u673a\u5236 \uff1a \u5229\u7528\u9884\u8bad\u7ec3\u7684 \u89c6\u89c9\u57fa\u7840\u6a21\u578b \uff08Vision Foundation Models\uff09\u5bf9RGB\u56fe\u50cf\u8fdb\u884c\u5b9e\u4f8b\u5206\u5272\u548c\u6df1\u5ea6\u4f30\u8ba1\u3002 3D\u70b9\u83b7\u53d62D\u7c7b\u522b\u6807\u7b7e \uff1a\u5c062D\u56fe\u50cf\u4e2d\u7684\u7c7b\u522b\u4fe1\u606f\uff08\u6765\u81ea\u5b9e\u4f8b\u5206\u5272\uff09\u6620\u5c04\u52303D\u70b9\u4e91\u4e0a\u3002 2D\u50cf\u7d20\u589e\u5f3a3D\u70b9\u5bc6\u5ea6 \uff1a\u5c062D\u50cf\u7d20\u6295\u5f71\u52303D\u7a7a\u95f4\uff0c\u4ee5\u589e\u5f3aLiDAR\u70b9\u4e91\u7684\u5bc6\u5ea6\uff0c\u5c24\u5176\u662f\u5728\u7a00\u758f\u533a\u57df\u3002 \u9c81\u68d2\u6027\u566a\u58f0\u8fc7\u6ee4 \uff1a\u4e3a\u4e86\u7f13\u89e3\u6df1\u5ea6\u4f30\u8ba1\u548c\u5206\u5272\u7ed3\u679c\u4e2d\u7684\u566a\u58f0\uff0c\u63d0\u51fa\u4e86\uff1a \u5c40\u90e8\u534a\u5f84\u8fc7\u6ee4\uff08Local Radius Filtering\uff09 \uff1a\u7528\u4e8e\u6291\u5236\u6df1\u5ea6\u4f30\u8ba1\u8bef\u5dee\u3002 \u5168\u5c40\u7edf\u8ba1\u8fc7\u6ee4\uff08Global Statistical Filtering\uff09 \uff1a\u7528\u4e8e\u79fb\u9664\u7531\u5206\u5272\u9519\u8bef\u5f15\u8d77\u7684\u79bb\u7fa4\u70b9\u3002 \u6570\u636e\u7ea7\u878d\u5408\u7684\u52a8\u6001\u81ea\u6f14\u5316\u7b56\u7565\uff08Dynamic Self-Evolution Strategy\uff09 \uff1a\u57fa\u4e8e\u6570\u636e\u7ea7\u878d\u5408\u7684\u4f2a\u6807\u7b7e\uff0c\u901a\u8fc7\u8fed\u4ee3\u7ec6\u5316\u8fc7\u7a0b\uff0c\u5728\u5bc6\u96c6\u8868\u793a\u4e0b\u663e\u8457\u63d0\u5347\u4f2a\u6807\u7b7e\u7684\u5b9a\u4f4d\u7cbe\u5ea6\u3002 3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd (Potential Impact on the Field) \u63a8\u52a8\u65e0\u76d1\u77633D\u76ee\u6807\u68c0\u6d4b\u53d1\u5c55 \uff1a\u663e\u8457\u63d0\u5347\u4e86\u65e0\u76d1\u7763\u65b9\u6cd5\u5728\u590d\u6742\u573a\u666f\u4e0b\u7684\u6027\u80fd\uff0c\u4f7f\u5176\u66f4\u63a5\u8fd1\u751a\u81f3\u53ef\u80fd\u8d85\u8d8a\u90e8\u5206\u6709\u76d1\u7763\u65b9\u6cd5\uff0c\u964d\u4f4e\u4e86\u5bf9\u6602\u8d353D\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u4ece\u800c\u52a0\u901f\u4e86\u76f8\u5173\u7814\u7a76\u548c\u5e94\u7528\u3002 \u5f3a\u8c03\u6570\u636e\u7ea7\u878d\u5408\u7684\u6f5c\u529b \uff1a\u8bc1\u660e\u4e86\u5728\u65e9\u671f\u9636\u6bb5\u878d\u5408\u591a\u6a21\u6001\u6570\u636e\uff08LiDAR\u548cRGB\uff09\u7684\u4f18\u8d8a\u6027\uff0c\u4e3a\u672a\u6765\u591a\u6a21\u6001\u611f\u77e5\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u53ef\u80fd\u542f\u53d1\u66f4\u591a\u7814\u7a76\u63a2\u7d22\u4e0d\u540c\u6a21\u6001\u5728\u6570\u636e\u5c42\u9762\u7684\u6df1\u5ea6\u878d\u5408\u3002 \u4fc3\u8fdb\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u5e94\u7528 \uff1a\u5c55\u793a\u4e86\u5229\u7528\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08\u5982\u5b9e\u4f8b\u5206\u5272\u548c\u6df1\u5ea6\u4f30\u8ba1\uff09\u4f5c\u4e3a\u591a\u6a21\u6001\u878d\u5408\u7684\u5f3a\u5927\u524d\u7aef\u7684\u6709\u6548\u6027\uff0c\u4e3a\u8fd9\u4e9b\u6a21\u578b\u5728\u66f4\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u3002 \u5b9e\u7528\u6027\u63d0\u5347 \uff1a\u4e3a\u81ea\u52a8\u9a7e\u9a76\u3001\u673a\u5668\u4eba\u7b49\u9700\u8981\u9ad8\u7cbe\u5ea63D\u611f\u77e5\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u7ecf\u6d4e\u3001\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u964d\u4f4e\u4e86\u90e8\u7f72\u6210\u672c\u548c\u65f6\u95f4\u3002 4. \u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 (Related Areas or Applications) \u81ea\u52a8\u9a7e\u9a76 (Autonomous Driving) \uff1a\u76f4\u63a5\u53d7\u76ca\uff0c\u964d\u4f4e\u4e86\u8bad\u7ec33D\u611f\u77e5\u6a21\u578b\u7684\u6570\u636e\u6210\u672c\uff0c\u52a0\u901f\u4e86\u81ea\u52a8\u9a7e\u9a76\u6280\u672f\u7684\u7814\u53d1\u548c\u90e8\u7f72\u3002 \u673a\u5668\u4eba\u5b66 (Robotics) \uff1a\u7528\u4e8e\u673a\u5668\u4eba\u5bfc\u822a\u3001\u73af\u5883\u611f\u77e5\u3001\u7269\u4f53\u6293\u53d6\u548c\u4eba\u673a\u4ea4\u4e92\uff0c\u5c24\u5176\u662f\u5728\u672a\u77e5\u6216\u52a8\u6001\u73af\u5883\u4e2d\u3002 \u667a\u6167\u57ce\u5e02 (Smart Cities) \uff1a\u4ea4\u901a\u76d1\u63a7\u3001\u57fa\u7840\u8bbe\u65bd\u7ba1\u7406\u3001\u884c\u4eba\u6d41\u5206\u6790\u7b49\u573a\u666f\u4e2d\u76843D\u76ee\u6807\u8bc6\u522b\u548c\u8ddf\u8e2a\u3002 \u589e\u5f3a\u73b0\u5b9e/\u865a\u62df\u73b0\u5b9e (AR/VR) \uff1a3D\u573a\u666f\u7406\u89e3\u3001\u73af\u5883\u91cd\u5efa\u548c\u865a\u62df\u7269\u4f53\u4e0e\u771f\u5b9e\u4e16\u754c\u7684\u7cbe\u786e\u878d\u5408\u3002 \u5de5\u4e1a\u81ea\u52a8\u5316 (Industrial Automation) \uff1a\u4f8b\u5982\u4ed3\u5e93\u4e2d\u7684\u7269\u4f53\u8bc6\u522b\u3001\u5b9a\u4f4d\u548c\u5206\u62e3\uff0c\u63d0\u9ad8\u81ea\u52a8\u5316\u6c34\u5e73\u3002 5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u7684\u5c40\u9650\u6027 (Limitations Inferred from the Abstract) \u5bf92D\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u4f9d\u8d56 \uff1a\u65b9\u6cd5\u7684\u6027\u80fd\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u53d7\u9650\u4e8e\u6240\u4f7f\u7528\u76842D\u5b9e\u4f8b\u5206\u5272\u548c\u6df1\u5ea6\u4f30\u8ba1\u6a21\u578b\u7684\u51c6\u786e\u6027\u3002\u8fd9\u4e9b\u6a21\u578b\u7684\u8bef\u5dee\u4f1a\u76f4\u63a5\u5f71\u54cd3D\u4f2a\u6807\u7b7e\u7684\u8d28\u91cf\uff0c\u5c24\u5176\u662f\u5728\u590d\u6742\u6216\u4e0d\u7406\u60f3\u7684\u56fe\u50cf\u6761\u4ef6\u4e0b\u3002 \u566a\u58f0\u7f13\u89e3\u7684\u6311\u6218 \uff1a\u5c3d\u7ba1\u63d0\u51fa\u4e86\u8fc7\u6ee4\u65b9\u6cd5\uff0c\u4f46\u6df1\u5ea6\u4f30\u8ba1\u548c2D\u5206\u5272\u56fa\u6709\u7684\u566a\u58f0\u548c\u4e0d\u786e\u5b9a\u6027\u4ecd\u7136\u662f\u9700\u8981\u6301\u7eed\u5173\u6ce8\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u6076\u52a3\u73af\u5883\uff08\u5982\u96e8\u96fe\u3001\u4f4e\u5149\u7167\u3001\u5f3a\u53cd\u5149\uff09\u4e0b\uff0c\u8fd9\u4e9b\u566a\u58f0\u53ef\u80fd\u96be\u4ee5\u5b8c\u5168\u6d88\u9664\u3002 \u8ba1\u7b97\u8d44\u6e90\u6d88\u8017 \uff1a\u65e9\u671f\u6570\u636e\u7ea7\u878d\u5408\u3001\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u5e94\u7528\u4ee5\u53ca\u8fed\u4ee3\u81ea\u6f14\u5316\u7b56\u7565\uff0c\u53ef\u80fd\u9700\u8981\u8f83\u9ad8\u7684\u8ba1\u7b97\u8d44\u6e90\u548c\u65f6\u95f4\u6210\u672c\uff0c\u8fd9\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u6216\u5b9e\u65f6\u5e94\u7528\u4e2d\u53ef\u80fd\u662f\u4e00\u4e2a\u8003\u91cf\u3002 \u6cdb\u5316\u80fd\u529b \uff1a\u5b9e\u9a8c\u4e3b\u8981\u5728nuScenes\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u3002\u5176\u5728\u4e0d\u540c\u4f20\u611f\u5668\u914d\u7f6e\u3001\u4e0d\u540c\u573a\u666f\u7c7b\u578b\uff08\u5982\u8d8a\u91ce\u3001\u5ba4\u5185\uff09\u6216\u4e0d\u540c\u5730\u7406\u73af\u5883\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u5c1a\u5f85\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u3002 \u7279\u5b9a\u6a21\u6001\u9650\u5236 \uff1a\u76ee\u524d\u4ec5\u9650\u4e8eLiDAR\u548cRGB\u56fe\u50cf\u7684\u878d\u5408\u3002\u5bf9\u4e8e\u5176\u4ed6\u6a21\u6001\uff08\u5982\u96f7\u8fbe\u3001\u70ed\u6210\u50cf\uff09\u7684\u6574\u5408\u6f5c\u529b\u672a\u63d0\u53ca\uff0c\u8fd9\u4e9b\u6a21\u6001\u5728\u67d0\u4e9b\u7279\u5b9a\u573a\u666f\u4e0b\u53ef\u80fd\u63d0\u4f9b\u989d\u5916\u7684\u9c81\u68d2\u6027\u3002 Key Findings: To overcome the above limitations, we propose a novel data-level fusion framework that integrates RGB images and LiDAR data at an early stage. To mitigate noise from depth and segmentation estimations, we propose a local and global filtering method, which applies local radius filtering to suppress depth estimation errors and global statistical filtering to remove segmentation-induced outliers. Furthermore, we propose a data-level fusion based dynamic self-evolution strategy, which iteratively refines pseudo-boxes under a dense representation, significantly improving localization accuracy. Extensive experiments on the nuScenes dataset demonstrate that the detector trained by our method significantly outperforms that trained by previous state-of-the-art methods with 28.4 \\% mAP on the nuScenes validation benchmark. Links: PDF arXiv GS: Generative Segmentation via Label Diffusion Authors: Yuhao Chen, Shubin Chen, Liang Lin, Guangrun Wang Published: 2025-08-27 Categories: cs.CV Abstract: Language-driven image segmentation is a fundamental task in vision-language understanding, requiring models to segment regions of an image corresponding to natural language expressions. Traditional methods approach this as a discriminative problem, assigning each pixel to foreground or background based on semantic alignment. Recently, diffusion models have been introduced to this domain, but existing approaches remain image-centric: they either (i) use image diffusion models as visual feature extractors, (ii) synthesize segmentation data via image generation to train discriminative models, or (iii) perform diffusion inversion to extract attention cues from pre-trained image diffusion models-thereby treating segmentation as an auxiliary process. In this paper, we propose GS (Generative Segmentation), a novel framework that formulates segmentation itself as a generative task via label diffusion. Instead of generating images conditioned on label maps and text, GS reverses the generative process: it directly generates segmentation masks from noise, conditioned on both the input image and the accompanying language description. This paradigm makes label generation the primary modeling target, enabling end-to-end training with explicit control over spatial and semantic fidelity. To demonstrate the effectiveness of our approach, we evaluate GS on Panoptic Narrative Grounding (PNG), a representative and challenging benchmark for multimodal segmentation that requires panoptic-level reasoning guided by narrative captions. Experimental results show that GS significantly outperforms existing discriminative and diffusion-based methods, setting a new state-of-the-art for language-driven segmentation. Analysis: \u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\uff0c\u7279\u522b\u662f\u89c6\u89c9-\u8bed\u8a00\u7406\u89e3\uff08Vision-Language Understanding\uff09\u65b9\u5411\u4e0a\u5177\u6709\u6f5c\u5728\u91cd\u8981\u6027\u7684\u65b0\u65b9\u6cd5\u3002 1. \u8bba\u6587\u6838\u5fc3\u8d21\u732e\u7684\u7b80\u6d01\u603b\u7ed3 (2-3\u53e5\u8bdd) \u672c\u6587\u63d0\u51fa\u4e86GS\uff08Generative Segmentation\uff09\u6846\u67b6\uff0c\u5c06\u8bed\u8a00\u9a71\u52a8\u7684\u56fe\u50cf\u5206\u5272\u4efb\u52a1\u91cd\u65b0\u5b9a\u4e49\u4e3a\u901a\u8fc7\u6807\u7b7e\u6269\u6563\uff08label diffusion\uff09\u5b9e\u73b0\u7684\u751f\u6210\u5f0f\u4efb\u52a1\u3002\u4e0e\u73b0\u6709\u65b9\u6cd5\u5c06\u5206\u5272\u89c6\u4e3a\u8f85\u52a9\u8fc7\u7a0b\u4e0d\u540c\uff0cGS\u76f4\u63a5\u4ece\u566a\u58f0\u4e2d\u751f\u6210\u5206\u5272\u63a9\u7801\uff0c\u5e76\u4ee5\u8f93\u5165\u56fe\u50cf\u548c\u8bed\u8a00\u63cf\u8ff0\u4e3a\u6761\u4ef6\uff0c\u4f7f\u6807\u7b7e\u751f\u6210\u6210\u4e3a\u6838\u5fc3\u5efa\u6a21\u76ee\u6807\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cGS\u663e\u8457\u8d85\u8d8a\u4e86\u73b0\u6709\u5224\u522b\u5f0f\u548c\u57fa\u4e8e\u6269\u6563\u7684\u65b9\u6cd5\uff0c\u5728\u8bed\u8a00\u9a71\u52a8\u5206\u5272\u4efb\u52a1\u4e0a\u8fbe\u5230\u4e86\u65b0\u7684SOTA\u3002 2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u5b66\u65b9\u6cd5 GS\u7684\u6838\u5fc3\u521b\u65b0\u5728\u4e8e \u8303\u5f0f\u8f6c\u53d8 \uff1a\u5b83\u5c06\u56fe\u50cf\u5206\u5272\u4efb\u52a1\u672c\u8eab\u4ece\u4f20\u7edf\u7684\u5224\u522b\u5f0f\u95ee\u9898\uff08\u5c06\u50cf\u7d20\u5206\u7c7b\u4e3a\u524d\u666f\u6216\u80cc\u666f\uff09\u6216\u5c06\u5206\u5272\u4f5c\u4e3a\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7684\u8f85\u52a9\u8f93\u51fa\uff0c\u91cd\u65b0\u5b9a\u4e49\u4e3a \u751f\u6210\u5f0f\u4efb\u52a1 \u3002 \u5177\u4f53\u7684\u65b9\u6cd5\u5b66\u521b\u65b0\u662f\u5f15\u5165\u4e86 \u201c\u6807\u7b7e\u6269\u6563\u201d\uff08label diffusion\uff09 \u673a\u5236\u3002\u4e0e\u73b0\u6709\u6269\u6563\u6a21\u578b\u901a\u5e38\u751f\u6210\u56fe\u50cf\uff08\u4ee5\u6807\u7b7e\u6216\u6587\u672c\u4e3a\u6761\u4ef6\uff09\u4e0d\u540c\uff0cGS\u98a0\u8986\u4e86\u8fd9\u4e00\u8fc7\u7a0b\uff1a\u5b83\u76f4\u63a5\u4ece\u968f\u673a\u566a\u58f0\u4e2d\u9010\u6b65\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u5206\u5272\u63a9\u7801\uff0c\u5e76\u4ee5\u8f93\u5165\u56fe\u50cf\u548c\u5bf9\u5e94\u7684\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u4f5c\u4e3a\u751f\u6210\u8fc7\u7a0b\u7684\u6761\u4ef6\u3002\u8fd9\u79cd\u201c\u6807\u7b7e\u4f18\u5148\u201d\u7684\u751f\u6210\u8303\u5f0f\u4f7f\u5f97\u6807\u7b7e\u751f\u6210\u6210\u4e3a\u7aef\u5230\u7aef\u8bad\u7ec3\u7684\u4e3b\u8981\u76ee\u6807\uff0c\u4ece\u800c\u80fd\u591f\u5bf9\u751f\u6210\u63a9\u7801\u7684\u7a7a\u95f4\u7cbe\u5ea6\u548c\u8bed\u4e49\u4fdd\u771f\u5ea6\u8fdb\u884c\u663e\u5f0f\u4e14\u7cbe\u7ec6\u7684\u63a7\u5236\u3002 3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd \u5f00\u8f9f\u65b0\u8303\u5f0f\uff1a \u5c06\u5206\u5272\u4efb\u52a1\u4ece\u5224\u522b\u5f0f\u6216\u8f85\u52a9\u4efb\u52a1\u8f6c\u53d8\u4e3a\u6838\u5fc3\u751f\u6210\u5f0f\u4efb\u52a1\uff0c\u4e3a\u89c6\u89c9-\u8bed\u8a00\u7406\u89e3\u9886\u57df\u7684\u5206\u5272\u95ee\u9898\u5f00\u8f9f\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\uff0c\u53ef\u80fd\u542f\u53d1\u5176\u4ed6\u7ed3\u6784\u5316\u9884\u6d4b\u4efb\u52a1\uff08\u5982\u5173\u952e\u70b9\u68c0\u6d4b\u3001\u59ff\u6001\u4f30\u8ba1\uff09\u4e2d\u5e94\u7528\u6269\u6563\u6a21\u578b\u7684\u65b0\u601d\u8def\u3002 \u6027\u80fd\u7a81\u7834\uff1a \u5728Panoptic Narrative Grounding (PNG) \u7b49\u590d\u6742\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u591a\u6a21\u6001\u5206\u5272\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u53d6\u5f97SOTA\uff0c\u8bc1\u660e\u4e86\u751f\u6210\u5f0f\u5206\u5272\u5728\u5904\u7406\u590d\u6742\u53d9\u8ff0\u6027\u63cf\u8ff0\u548c\u5168\u666f\u7ea7\u63a8\u7406\u65b9\u9762\u7684\u4f18\u8d8a\u6027\u3002 \u63d0\u5347\u53ef\u63a7\u6027\u4e0e\u7cbe\u5ea6\uff1a \u5f3a\u8c03\u5bf9\u7a7a\u95f4\u548c\u8bed\u4e49\u4fdd\u771f\u5ea6\u7684\u663e\u5f0f\u63a7\u5236\uff0c\u9884\u793a\u7740\u672a\u6765\u6a21\u578b\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u9ad8\u7cbe\u5ea6\u5206\u5272\u7ed3\u679c\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u7cbe\u7ec6\u4ea4\u4e92\u548c\u7f16\u8f91\u7684\u573a\u666f\u3002 \u7edf\u4e00\u89c6\u89c9-\u8bed\u8a00\u4efb\u52a1\uff1a \u901a\u8fc7\u5c06\u5206\u5272\u878d\u5165\u751f\u6210\u6846\u67b6\uff0c\u53ef\u80fd\u4e3a\u66f4\u6df1\u5c42\u6b21\u7684\u89c6\u89c9-\u8bed\u8a00\u8054\u5408\u5efa\u6a21\u63d0\u4f9b\u65b0\u7684\u89c6\u89d2\uff0c\u4fc3\u8fdb\u6a21\u578b\u66f4\u597d\u5730\u7406\u89e3\u56fe\u50cf\u5185\u5bb9\u4e0e\u8bed\u8a00\u63cf\u8ff0\u4e4b\u95f4\u7684\u590d\u6742\u5bf9\u5e94\u5173\u7cfb\u3002 4. \u53ef\u80fd\u53d7\u76ca\u7684\u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 \u4eba\u673a\u4ea4\u4e92 (HCI)\uff1a \u5141\u8bb8\u7528\u6237\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u7cbe\u786e\u5730\u9009\u62e9\u3001\u7f16\u8f91\u6216\u64cd\u4f5c\u56fe\u50cf\u4e2d\u7684\u7279\u5b9a\u533a\u57df\uff0c\u4f8b\u5982\u667a\u80fd\u56fe\u50cf\u7f16\u8f91\u8f6f\u4ef6\u3001\u865a\u62df\u52a9\u624b\u3002 \u673a\u5668\u4eba\u5b66 (Robotics)\uff1a \u673a\u5668\u4eba\u53ef\u4ee5\u901a\u8fc7\u8bed\u8a00\u6307\u4ee4\u7406\u89e3\u73af\u5883\u5e76\u6267\u884c\u7cbe\u7ec6\u7684\u64cd\u4f5c\uff0c\u4f8b\u5982\u201c\u6293\u4f4f\u684c\u5b50\u4e0a\u7684\u90a3\u4e2a\u84dd\u8272\u676f\u5b50\u201d\u9700\u8981\u7cbe\u786e\u5730\u5206\u5272\u51fa\u676f\u5b50\u3002 \u533b\u5b66\u5f71\u50cf\u5206\u6790\uff1a \u533b\u751f\u6216\u7814\u7a76\u4eba\u5458\u53ef\u4ee5\u901a\u8fc7\u63cf\u8ff0\u6765\u8f85\u52a9\u75c5\u7076\u533a\u57df\u7684\u7cbe\u786e\u5206\u5272\u548c\u5206\u6790\uff0c\u63d0\u9ad8\u8bca\u65ad\u6548\u7387\u548c\u51c6\u786e\u6027\u3002 \u5185\u5bb9\u521b\u4f5c\u4e0e\u7f16\u8f91\uff1a \u5728\u7535\u5f71\u3001\u5e7f\u544a\u5236\u4f5c\u4e2d\uff0c\u57fa\u4e8e\u6587\u672c\u63cf\u8ff0\u8fdb\u884c\u7cbe\u786e\u7684\u5bf9\u8c61\u9009\u62e9\u3001\u62a0\u56fe\u548c\u4fee\u6539\uff0c\u5927\u5927\u63d0\u9ad8\u5de5\u4f5c\u6548\u7387\u3002 \u81ea\u52a8\u9a7e\u9a76\uff1a \u8f66\u8f86\u9700\u8981\u7406\u89e3\u590d\u6742\u7684\u573a\u666f\u63cf\u8ff0\u5e76\u8bc6\u522b\u7279\u5b9a\u76ee\u6807\uff08\u4f8b\u5982\u201c\u524d\u65b9\u5de6\u4fa7\u7684\u884c\u4eba\u201d\uff09\uff0c\u4ee5\u8fdb\u884c\u5b89\u5168\u51b3\u7b56\u3002 \u8f85\u52a9\u6280\u672f\uff1a \u5e2e\u52a9\u89c6\u969c\u4eba\u58eb\u901a\u8fc7\u8bed\u8a00\u63cf\u8ff0\u66f4\u6df1\u5165\u5730\u7406\u89e3\u56fe\u50cf\u5185\u5bb9\uff0c\u63d0\u5347\u53ef\u8bbf\u95ee\u6027\u3002 \u901a\u7528\u89c6\u89c9-\u8bed\u8a00\u7406\u89e3\uff1a \u4efb\u4f55\u9700\u8981\u6df1\u5ea6\u7ed3\u5408\u89c6\u89c9\u548c\u8bed\u8a00\u4fe1\u606f\u8fdb\u884c\u7cbe\u7ec6\u7a7a\u95f4\u63a8\u7406\u548c\u8f93\u51fa\u7684\u4efb\u52a1\u3002 5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u7684\u5c40\u9650\u6027 \u8ba1\u7b97\u6210\u672c\uff1a \u6269\u6563\u6a21\u578b\u901a\u5e38\u8ba1\u7b97\u91cf\u5927\uff0c\u751f\u6210\u8fc7\u7a0b\u6d89\u53ca\u591a\u6b65\u8fed\u4ee3\uff0c\u53ef\u80fd\u5bfc\u81f4\u63a8\u7406\u901f\u5ea6\u8f83\u6162\uff0c\u96be\u4ee5\u6ee1\u8db3\u5b9e\u65f6\u5e94\u7528\u9700\u6c42\u3002 \u6570\u636e\u4f9d\u8d56\uff1a \u8bad\u7ec3\u8fd9\u79cd\u590d\u6742\u7684\u751f\u6210\u5f0f\u6a21\u578b\uff0c\u7279\u522b\u662f\u9700\u8981\u56fe\u50cf\u3001\u8bed\u8a00\u548c\u7cbe\u786e\u6807\u7b7e\u4e09\u8005\u5bf9\u9f50\u7684\uff0c\u901a\u5e38\u9700\u8981\u5927\u89c4\u6a21\u3001\u9ad8\u8d28\u91cf\u4e14\u6807\u6ce8\u7cbe\u7ec6\u7684\u6570\u636e\u96c6\uff0c\u8fd9\u53ef\u80fd\u9650\u5236\u5176\u5728\u6570\u636e\u7a00\u7f3a\u9886\u57df\u7684\u5e94\u7528\u3002 \u6cdb\u5316\u80fd\u529b\uff1a \u5c3d\u7ba1\u662f\u8bed\u8a00\u9a71\u52a8\uff0c\u4f46\u6a21\u578b\u5bf9\u8bad\u7ec3\u6570\u636e\u4e2d\u672a\u51fa\u73b0\u8fc7\u7684\u5168\u65b0\u6982\u5ff5\u3001\u9ad8\u5ea6\u62bd\u8c61\u6216\u6b67\u4e49\u6027\u5f3a\u7684\u8bed\u8a00\u63cf\u8ff0\u7684\u6cdb\u5316\u80fd\u529b\u4ecd\u9700\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u3002 \u53ef\u63a7\u6027\u7c92\u5ea6\uff1a \u6458\u8981\u4e2d\u63d0\u5230\u201c\u663e\u5f0f\u63a7\u5236\u201d\uff0c\u4f46\u5b9e\u9645\u64cd\u4f5c\u4e2d\uff0c\u5982\u4f55\u901a\u8fc7\u8bed\u8a00\u63cf\u8ff0\u6765\u7cbe\u7ec6\u5730\u5fae\u8c03\u751f\u6210\u63a9\u7801\u7684\u6bcf\u4e00\u4e2a\u7ec6\u8282\uff0c\u4ee5\u53ca\u5904\u7406\u8bed\u8a00\u63cf\u8ff0\u4e2d\u7684\u6a21\u7cca\u6027\u6216\u51b2\u7a81\uff0c\u53ef\u80fd\u4ecd\u662f\u4e00\u4e2a\u6311\u6218\u3002 \u6a21\u578b\u590d\u6742\u6027\uff1a \u751f\u6210\u5f0f\u6a21\u578b\u901a\u5e38\u6bd4\u5224\u522b\u5f0f\u6a21\u578b\u66f4\u590d\u6742\uff0c\u8bad\u7ec3\u548c\u8c03\u8bd5\u53ef\u80fd\u66f4\u5177\u6311\u6218\u6027\u3002 Key Findings: In this paper, we propose GS (Generative Segmentation), a novel framework that formulates segmentation itself as a generative task via label diffusion. To demonstrate the effectiveness of our approach, we evaluate GS on Panoptic Narrative Grounding (PNG), a representative and challenging benchmark for multimodal segmentation that requires panoptic-level reasoning guided by narrative captions. Experimental results show that GS significantly outperforms existing discriminative and diffusion-based methods, setting a new state-of-the-art for language-driven segmentation. Links: PDF arXiv Multimodal Prototype Alignment for Semi-supervised Pathology Image Segmentation Authors: Mingxi Fu, Fanglei Fu, Xitong Ling, Huaitian Yuan, Tian Guan, Yonghong He, Lianghui Zhu Published: 2025-08-27 Categories: cs.CV, cs.AI Abstract: Pathological image segmentation faces numerous challenges, particularly due to ambiguous semantic boundaries and the high cost of pixel-level annotations. Although recent semi-supervised methods based on consistency regularization (e.g., UniMatch) have made notable progress, they mainly rely on perturbation-based consistency within the image modality, making it difficult to capture high-level semantic priors, especially in structurally complex pathology images. To address these limitations, we propose MPAMatch - a novel segmentation framework that performs pixel-level contrastive learning under a multimodal prototype-guided supervision paradigm. The core innovation of MPAMatch lies in the dual contrastive learning scheme between image prototypes and pixel labels, and between text prototypes and pixel labels, providing supervision at both structural and semantic levels. This coarse-to-fine supervisory strategy not only enhances the discriminative capability on unlabeled samples but also introduces the text prototype supervision into segmentation for the first time, significantly improving semantic boundary modeling. In addition, we reconstruct the classic segmentation architecture (TransUNet) by replacing its ViT backbone with a pathology-pretrained foundation model (Uni), enabling more effective extraction of pathology-relevant features. Extensive experiments on GLAS, EBHI-SEG-GLAND, EBHI-SEG-CANCER, and KPI show MPAMatch's superiority over state-of-the-art methods, validating its dual advantages in structural and semantic modeling. Analysis: \u8fd9\u662f\u4e00\u7bc7\u5173\u4e8e\u534a\u76d1\u7763\u75c5\u7406\u56fe\u50cf\u5206\u5272\u7684\u8bba\u6587\u6458\u8981\u5206\u6790\uff0c\u4ee5\u4e0b\u662f\u6211\u7684\u4e13\u4e1a\u89e3\u8bfb\uff1a \u8bba\u6587\u6458\u8981\u5206\u6790\uff1aMultimodal Prototype Alignment for Semi-supervised Pathology Image Segmentation 1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u603b\u7ed3 (2-3\u53e5\u8bdd) \u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMPAMatch\u7684\u65b0\u578b\u534a\u76d1\u7763\u5206\u5272\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u75c5\u7406\u56fe\u50cf\u5206\u5272\u4e2d\u6a21\u7cca\u8fb9\u754c\u548c\u9ad8\u6807\u6ce8\u6210\u672c\u7684\u6311\u6218\u3002\u5176\u6838\u5fc3\u8d21\u732e\u5728\u4e8e\u5f15\u5165\u4e86\u591a\u6a21\u6001\u539f\u578b\u5f15\u5bfc\u7684\u76d1\u7763\u8303\u5f0f\uff0c\u901a\u8fc7\u56fe\u50cf\u539f\u578b\u4e0e\u50cf\u7d20\u6807\u7b7e\u3001\u4ee5\u53ca\u6587\u672c\u539f\u578b\u4e0e\u50cf\u7d20\u6807\u7b7e\u4e4b\u95f4\u7684\u53cc\u91cd\u5bf9\u6bd4\u5b66\u4e60\uff0c\u9996\u6b21\u5c06\u6587\u672c\u539f\u578b\u76d1\u7763\u5f15\u5165\u5206\u5272\u4efb\u52a1\uff0c\u4ece\u800c\u5728\u7ed3\u6784\u548c\u8bed\u4e49\u5c42\u9762\u63d0\u4f9b\u66f4\u4e30\u5bcc\u7684\u76d1\u7763\u4fe1\u606f\u3002\u6b64\u5916\uff0c\u5b83\u8fd8\u901a\u8fc7\u6574\u5408\u75c5\u7406\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\uff08Uni\uff09\u6539\u8fdb\u4e86TransUNet\u67b6\u6784\uff0c\u4ee5\u66f4\u6709\u6548\u5730\u63d0\u53d6\u75c5\u7406\u76f8\u5173\u7279\u5f81\u3002 2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u5b66\u65b9\u6cd5 MPAMatch\u7684\u5173\u952e\u521b\u65b0\u5728\u4e8e\u5176 \u591a\u6a21\u6001\u539f\u578b\u5f15\u5bfc\u7684\u76d1\u7763\u8303\u5f0f \u548c \u53cc\u91cd\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6848 \uff1a * \u53cc\u91cd\u5bf9\u6bd4\u5b66\u4e60\uff1a * \u56fe\u50cf\u539f\u578b\u4e0e\u50cf\u7d20\u6807\u7b7e\u7684\u5bf9\u6bd4\u5b66\u4e60\uff1a \u63d0\u4f9b\u7ed3\u6784\u5c42\u9762\u7684\u76d1\u7763\uff0c\u589e\u5f3a\u5bf9\u56fe\u50cf\u5c40\u90e8\u7279\u5f81\u7684\u5224\u522b\u80fd\u529b\u3002 * \u6587\u672c\u539f\u578b\u4e0e\u50cf\u7d20\u6807\u7b7e\u7684\u5bf9\u6bd4\u5b66\u4e60\uff1a \u8fd9\u662f\u8be5\u65b9\u6cd5\u7684\u72ec\u7279\u4e4b\u5904\uff0c\u9996\u6b21\u5c06\u9ad8\u5c42\u8bed\u4e49\u4fe1\u606f\uff08\u901a\u8fc7\u6587\u672c\u63cf\u8ff0\u6355\u83b7\uff09\u5f15\u5165\u50cf\u7d20\u7ea7\u5206\u5272\u4efb\u52a1\uff0c\u663e\u8457\u6539\u5584\u4e86\u8bed\u4e49\u8fb9\u754c\u5efa\u6a21\uff0c\u5b9e\u73b0\u4e86\u4ece\u7c97\u5230\u7ec6\u7684\u76d1\u7763\u7b56\u7565\u3002 * \u67b6\u6784\u6539\u8fdb\uff1a \u5c06\u7ecf\u5178\u7684TransUNet\u67b6\u6784\u4e2d\u7684ViT\u9aa8\u5e72\u7f51\u7edc\u66ff\u6362\u4e3a\u7ecf\u8fc7\u75c5\u7406\u5b66\u9884\u8bad\u7ec3\u7684\u57fa\u7840\u6a21\u578b\uff08Uni\uff09\uff0c\u8fd9\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u63d0\u53d6\u66f4\u5177\u9886\u57df\u7279\u5f02\u6027\u548c\u9c81\u68d2\u6027\u7684\u75c5\u7406\u76f8\u5173\u7279\u5f81\u3002 * \u534a\u76d1\u7763\u5b66\u4e60\u8303\u5f0f\uff1a \u5728\u65e0\u6807\u7b7e\u6837\u672c\u4e0a\u901a\u8fc7\u8fd9\u79cd\u591a\u6a21\u6001\u539f\u578b\u5bf9\u9f50\u8fdb\u884c\u50cf\u7d20\u7ea7\u5bf9\u6bd4\u5b66\u4e60\uff0c\u6709\u6548\u5229\u7528\u4e86\u672a\u6807\u6ce8\u6570\u636e\u3002 3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd \u964d\u4f4e\u6807\u6ce8\u6210\u672c\uff1a \u901a\u8fc7\u9ad8\u6548\u7684\u534a\u76d1\u7763\u5b66\u4e60\uff0cMPAMatch\u6709\u671b\u5927\u5e45\u51cf\u5c11\u75c5\u7406\u56fe\u50cf\u5206\u5272\u6240\u9700\u7684\u50cf\u7d20\u7ea7\u6807\u6ce8\u5de5\u4f5c\u91cf\uff0c\u52a0\u901fAI\u5728\u6570\u5b57\u75c5\u7406\u9886\u57df\u7684\u5e94\u7528\u548c\u90e8\u7f72\u3002 \u63d0\u5347\u5206\u5272\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\uff1a \u5f15\u5165\u6587\u672c\u8bed\u4e49\u4fe1\u606f\uff0c\u7279\u522b\u662f\u5bf9\u6a21\u7cca\u8fb9\u754c\u7684\u5efa\u6a21\u80fd\u529b\uff0c\u5c06\u663e\u8457\u63d0\u9ad8\u75c5\u7406\u56fe\u50cf\u5206\u5272\u7684\u51c6\u786e\u6027\u548c\u5bf9\u590d\u6742\u7ed3\u6784\u7684\u7406\u89e3\u3002 \u5f00\u8f9f\u65b0\u7814\u7a76\u65b9\u5411\uff1a \u9996\u6b21\u5c06\u6587\u672c\u539f\u578b\u76d1\u7763\u5f15\u5165\u5206\u5272\u4efb\u52a1\uff0c\u4e3a\u591a\u6a21\u6001\u5b66\u4e60\u5728\u50cf\u7d20\u7ea7\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u5f00\u8f9f\u4e86\u65b0\u7684\u9014\u5f84\uff0c\u9f13\u52b1\u7814\u7a76\u8005\u63a2\u7d22\u66f4\u591a\u6a21\u6001\uff08\u5982\u57fa\u56e0\u7ec4\u6570\u636e\u3001\u4e34\u5e8a\u62a5\u544a\uff09\u4e0e\u56fe\u50cf\u5206\u5272\u7684\u7ed3\u5408\u3002 \u63a8\u52a8\u57fa\u7840\u6a21\u578b\u5728\u533b\u7597\u9886\u57df\u7684\u5e94\u7528\uff1a \u7ed3\u5408\u75c5\u7406\u9884\u8bad\u7ec3\u7684\u57fa\u7840\u6a21\u578b\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u7279\u5b9a\u9886\u57df\u4efb\u52a1\u4e2d\u7684\u5f3a\u5927\u7279\u5f81\u63d0\u53d6\u80fd\u529b\uff0c\u4e3a\u672a\u6765\u533b\u7597AI\u6a21\u578b\u7684\u5f00\u53d1\u63d0\u4f9b\u4e86\u8303\u4f8b\u3002 4. \u76f8\u5173\u9886\u57df\u6216\u5e94\u7528\u53d7\u76ca \u6570\u5b57\u75c5\u7406\u5b66\u548c\u8ba1\u7b97\u75c5\u7406\u5b66\uff1a \u80bf\u7624\u5206\u5272\u3001\u817a\u4f53\u5206\u5272\u3001\u7ec4\u7ec7\u7c7b\u578b\u8bc6\u522b\u3001\u75be\u75c5\u5206\u7ea7\u548c\u9884\u540e\u8bc4\u4f30\u7b49\u3002 \u533b\u5b66\u56fe\u50cf\u5206\u6790\uff1a \u5176\u4ed6\u9700\u8981\u7cbe\u7ec6\u50cf\u7d20\u7ea7\u5206\u5272\u4e14\u6807\u6ce8\u6210\u672c\u9ad8\u6602\u7684\u533b\u5b66\u5f71\u50cf\u4efb\u52a1\uff0c\u5982\u653e\u5c04\u5b66\u56fe\u50cf\uff08CT/MRI\uff09\u4e2d\u7684\u5668\u5b98\u6216\u75c5\u7076\u5206\u5272\u3002 \u534a\u76d1\u7763\u5b66\u4e60\uff1a \u4e3a\u901a\u7528\u534a\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u63d0\u4f9b\u4e86\u65b0\u7684\u591a\u6a21\u6001\u548c\u539f\u578b\u5f15\u5bfc\u7b56\u7565\u3002 \u591a\u6a21\u6001\u5b66\u4e60\uff1a \u63a8\u52a8\u4e86\u56fe\u50cf\u4e0e\u6587\u672c\u7b49\u591a\u6a21\u6001\u4fe1\u606f\u878d\u5408\u5728\u50cf\u7d20\u7ea7\u4efb\u52a1\u4e2d\u7684\u7814\u7a76\u3002 \u57fa\u7840\u6a21\u578b\u5e94\u7528\uff1a \u4e3a\u5982\u4f55\u5c06\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\uff08\u5982\u89c6\u89c9Transformer\uff09\u9002\u5e94\u5230\u7279\u5b9a\u9886\u57df\uff08\u5982\u533b\u7597\uff09\u5e76\u7ed3\u5408\u4e0b\u6e38\u4efb\u52a1\u63d0\u4f9b\u4e86\u5b9e\u8df5\u7ecf\u9a8c\u3002 5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u7684\u5c40\u9650\u6027 \u6587\u672c\u539f\u578b\u751f\u6210\u4e0e\u8d28\u91cf\uff1a \u6458\u8981\u4e2d\u672a\u8be6\u7ec6\u8bf4\u660e\u6587\u672c\u539f\u578b\u662f\u5982\u4f55\u751f\u6210\u6216\u83b7\u53d6\u7684\u3002\u9ad8\u8d28\u91cf\u3001\u5177\u6709\u4ee3\u8868\u6027\u7684\u6587\u672c\u539f\u578b\u5bf9\u4e8e\u8bed\u4e49\u76d1\u7763\u81f3\u5173\u91cd\u8981\uff0c\u5176\u751f\u6210\u8fc7\u7a0b\u53ef\u80fd\u590d\u6742\u4e14\u9700\u8981\u9886\u57df\u4e13\u5bb6\u77e5\u8bc6\uff0c\u751a\u81f3\u53ef\u80fd\u5f15\u5165\u4e3b\u89c2\u6027\u6216\u504f\u5dee\u3002 \u5bf9\u57fa\u7840\u6a21\u578b\u7684\u4f9d\u8d56\uff1a \u6027\u80fd\u53ef\u80fd\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u201c\u75c5\u7406\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\uff08Uni\uff09\u201d\u7684\u8d28\u91cf\u3001\u53ef\u7528\u6027\u53ca\u5176\u9884\u8bad\u7ec3\u6570\u636e\u7684\u89c4\u6a21\u548c\u591a\u6837\u6027\u3002\u5982\u679c\u8be5\u6a21\u578b\u4e0d\u516c\u5f00\u6216\u96be\u4ee5\u83b7\u53d6\uff0c\u5219\u65b9\u6cd5\u7684\u590d\u73b0\u548c\u63a8\u5e7f\u53ef\u80fd\u53d7\u9650\u3002 \u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\uff1a \u50cf\u7d20\u7ea7\u5bf9\u6bd4\u5b66\u4e60\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u5206\u8fa8\u7387\u75c5\u7406\u56fe\u50cf\u4e0a\uff0c\u901a\u5e38\u9700\u8981\u5927\u91cf\u7684\u8ba1\u7b97\u8d44\u6e90\uff08GPU\u5185\u5b58\u548c\u8ba1\u7b97\u65f6\u95f4\uff09\u3002 \u6cdb\u5316\u80fd\u529b\uff1a \u5c3d\u7ba1\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u4f46\u75c5\u7406\u56fe\u50cf\u7684\u590d\u6742\u6027\u548c\u591a\u6837\u6027\u6781\u9ad8\uff08\u4e0d\u540c\u67d3\u8272\u3001\u4e0d\u540c\u75be\u75c5\u7c7b\u578b\u3001\u4e0d\u540c\u626b\u63cf\u4eea\uff09\uff0c\u6587\u672c\u539f\u578b\u548c\u56fe\u50cf\u539f\u578b\u5728\u9762\u5bf9\u672a\u89c1\u8fc7\u7684\u65b0\u75c5\u7406\u7c7b\u578b\u6216\u7f55\u89c1\u75c5\u53d8\u65f6\u7684\u6cdb\u5316\u80fd\u529b\u4ecd\u9700\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u3002 \u53ef\u89e3\u91ca\u6027\uff1a \u591a\u6a21\u6001\u539f\u578b\u5bf9\u9f50\u548c\u5bf9\u6bd4\u5b66\u4e60\u7684\u5185\u90e8\u673a\u5236\u53ef\u80fd\u76f8\u5bf9\u590d\u6742\uff0c\u5176\u51b3\u7b56\u8fc7\u7a0b\u7684\u53ef\u89e3\u91ca\u6027\u53ef\u80fd\u4e0d\u5982\u4e00\u4e9b\u66f4\u7b80\u5355\u7684\u6a21\u578b\u3002 Key Findings: To address these limitations, we propose MPAMatch - a novel segmentation framework that performs pixel-level contrastive learning under a multimodal prototype-guided supervision paradigm. Extensive experiments on GLAS, EBHI-SEG-GLAND, EBHI-SEG-CANCER, and KPI show MPAMatch's superiority over state-of-the-art methods, validating its dual advantages in structural and semantic modeling. Links: PDF arXiv Scalable Object Detection in the Car Interior With Vision Foundation Models Authors: B\u00e1lint M\u00e9sz\u00e1ros, Ahmet Firintepe, Sebastian Schmidt, Stephan G\u00fcnnemann Published: 2025-08-27 Categories: cs.CV Abstract: AI tasks in the car interior like identifying and localizing externally introduced objects is crucial for response quality of personal assistants. However, computational resources of on-board systems remain highly constrained, restricting the deployment of such solutions directly within the vehicle. To address this limitation, we propose the novel Object Detection and Localization (ODAL) framework for interior scene understanding. Our approach leverages vision foundation models through a distributed architecture, splitting computational tasks between on-board and cloud. This design overcomes the resource constraints of running foundation models directly in the car. To benchmark model performance, we introduce ODALbench, a new metric for comprehensive assessment of detection and localization.Our analysis demonstrates the framework's potential to establish new standards in this domain. We compare the state-of-the-art GPT-4o vision foundation model with the lightweight LLaVA 1.5 7B model and explore how fine-tuning enhances the lightweight models performance. Remarkably, our fine-tuned ODAL-LLaVA model achieves an ODAL _{score} of 89%, representing a 71% improvement over its baseline performance and outperforming GPT-4o by nearly 20%. Furthermore, the fine-tuned model maintains high detection accuracy while significantly reducing hallucinations, achieving an ODAL _{SNR} three times higher than GPT-4o. Analysis: \u8fd9\u7bc7\u8bba\u6587\u6458\u8981\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5173\u4e8e\u5728\u8f66\u8f7d\u73af\u5883\u4e2d\u5e94\u7528\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u7269\u4f53\u68c0\u6d4b\u548c\u5b9a\u4f4d\u7684\u6709\u8da3\u89c6\u89d2\u3002\u4ee5\u4e0b\u662f\u6211\u7684\u5206\u6790\uff1a 1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u660e\u6458\u8981 (2-3 \u53e5\u8bdd) \u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aODAL\uff08Object Detection and Localization\uff09\u7684\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u8f66\u8f7d\u7cfb\u7edf\u8d44\u6e90\u53d7\u9650\u4e0b\uff0c\u5728\u6c7d\u8f66\u5185\u90e8\u7f72\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u7269\u4f53\u68c0\u6d4b\u548c\u5b9a\u4f4d\u7684\u6311\u6218\u3002\u901a\u8fc7\u91c7\u7528\u5206\u5e03\u5f0f\u67b6\u6784\uff0c\u5c06\u8ba1\u7b97\u4efb\u52a1\u5728\u8f66\u8f7d\u7aef\u548c\u4e91\u7aef\u4e4b\u95f4\u5206\u62c5\uff0cODAL\u6846\u67b6\u6210\u529f\u514b\u670d\u4e86\u76f4\u63a5\u5728\u8f66\u5185\u8fd0\u884c\u5927\u578b\u57fa\u7840\u6a21\u578b\u7684\u9650\u5236\u3002\u7814\u7a76\u8868\u660e\uff0c\u7ecf\u8fc7\u5fae\u8c03\u7684\u8f7b\u91cf\u7ea7ODAL-LLaVA\u6a21\u578b\u4e0d\u4ec5\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e86GPT-4o\uff0c\u8fd8\u663e\u8457\u964d\u4f4e\u4e86\u5e7b\u89c9\u73b0\u8c61\uff0c\u4e3a\u8f66\u8f7dAI\u4efb\u52a1\u6811\u7acb\u4e86\u65b0\u6807\u51c6\u3002 2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u8bba \u5206\u5e03\u5f0f\u67b6\u6784\u5229\u7528\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff1a \u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u5176 \u5206\u5e03\u5f0fODAL\u6846\u67b6 \uff0c\u5b83\u5de7\u5999\u5730\u5c06\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u8ba1\u7b97\u4efb\u52a1\u5206\u89e3\uff0c\u4e00\u90e8\u5206\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8f66\u8f7d\u7aef\u6267\u884c\uff0c\u53e6\u4e00\u90e8\u5206\u5219\u5378\u8f7d\u5230\u4e91\u7aef\u3002\u8fd9\u4f7f\u5f97\u5728\u8f66\u5185\u73af\u5883\u4e2d\u4f7f\u7528\u901a\u5e38\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\u7684\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u6210\u4e3a\u53ef\u80fd\u3002 \u8f7b\u91cf\u7ea7\u6a21\u578b\u5fae\u8c03\u8d85\u8d8aSOTA\uff1a \u8bba\u6587\u5c55\u793a\u4e86\u901a\u8fc7\u5bf9LLaVA 1.5 7B\u8fd9\u6837\u7684\u8f7b\u91cf\u7ea7\u6a21\u578b\u8fdb\u884c \u5fae\u8c03 \uff0c\u4e0d\u4ec5\u80fd\u5927\u5e45\u63d0\u5347\u5176\u5728\u7279\u5b9a\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff08ODAL _{score} \u63d0\u534771%\uff09\uff0c\u751a\u81f3\u80fd\u8d85\u8d8a\u5f53\u524d\u6700\u5148\u8fdb\u7684\u901a\u7528\u6a21\u578b\uff08GPT-4o\uff09\uff0c\u5e76\u5728\u51cf\u5c11\u5e7b\u89c9\u65b9\u9762\u8868\u73b0\u66f4\u4f18\uff08ODAL _{SNR} \u662fGPT-4o\u7684\u4e09\u500d\uff09\u3002 \u65b0\u578b\u8bc4\u4f30\u6307\u6807ODALbench\uff1a \u5f15\u5165\u4e86 ODALbench \u8fd9\u4e00\u65b0\u7684\u5ea6\u91cf\u6807\u51c6\uff0c\u7528\u4e8e\u5168\u9762\u8bc4\u4f30\u68c0\u6d4b\u548c\u5b9a\u4f4d\u6027\u80fd\uff0c\u8fd9\u6709\u52a9\u4e8e\u5728\u8be5\u7279\u5b9a\u9886\u57df\u5efa\u7acb\u7edf\u4e00\u7684\u6027\u80fd\u8bc4\u4f30\u57fa\u51c6\u3002 3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd \u63a8\u52a8\u8fb9\u7f18AI\u548c\u8f66\u8f7dAI\u53d1\u5c55\uff1a \u8be5\u7814\u7a76\u4e3a\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\uff08\u5982\u6c7d\u8f66\uff09\u4e0a\u90e8\u7f72\u590d\u6742AI\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u884c\u7684\u8303\u5f0f\u3002\u5b83\u53ef\u80fd\u52a0\u901f\u8f66\u8f7dAI\uff08\u5982\u667a\u80fd\u5ea7\u8231\u52a9\u624b\u3001\u9a7e\u9a76\u5458\u76d1\u63a7\u3001\u4e58\u5ba2\u5b89\u5168\uff09\u7684\u666e\u53ca\u548c\u80fd\u529b\u63d0\u5347\u3002 \u91cd\u65b0\u5b9a\u4e49\u57fa\u7840\u6a21\u578b\u90e8\u7f72\u7b56\u7565\uff1a \u8bba\u6587\u8bc1\u660e\u4e86\u5206\u5e03\u5f0f\u67b6\u6784\u5728\u5229\u7528\u5927\u578b\u57fa\u7840\u6a21\u578b\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u8fd9\u53ef\u80fd\u4fc3\u4f7f\u672a\u6765\u66f4\u591aAI\u5e94\u7528\u91c7\u7528\u6df7\u5408\uff08\u672c\u5730+\u4e91\u7aef\uff09\u90e8\u7f72\u7b56\u7565\uff0c\u4ee5\u5e73\u8861\u6027\u80fd\u3001\u8d44\u6e90\u548c\u6210\u672c\u3002 \u5f3a\u8c03\u8f7b\u91cf\u7ea7\u6a21\u578b\u5fae\u8c03\u7684\u91cd\u8981\u6027\uff1a \u7ed3\u679c\u8868\u660e\uff0c\u9488\u5bf9\u7279\u5b9a\u9886\u57df\u8fdb\u884c\u6df1\u5ea6\u5fae\u8c03\u7684\u8f7b\u91cf\u7ea7\u6a21\u578b\uff0c\u5176\u6027\u80fd\u53ef\u4ee5\u8d85\u8d8a\u901a\u7528\u578b\u5927\u578b\u6a21\u578b\uff0c\u8fd9\u53ef\u80fd\u4f1a\u9f13\u52b1\u7814\u7a76\u4eba\u5458\u548c\u5f00\u53d1\u8005\u66f4\u591a\u5730\u5173\u6ce8\u6a21\u578b\u6548\u7387\u548c\u9886\u57df\u9002\u5e94\u6027\uff0c\u800c\u975e\u4e00\u5473\u8ffd\u6c42\u6a21\u578b\u89c4\u6a21\u3002 \u5efa\u7acb\u65b0\u7684\u8bc4\u4f30\u6807\u51c6\uff1a ODALbench\u7684\u5f15\u5165\u6709\u671b\u6210\u4e3a\u8f66\u8f7d\u5185\u90e8\u7269\u4f53\u68c0\u6d4b\u548c\u5b9a\u4f4d\u4efb\u52a1\u7684\u884c\u4e1a\u6807\u51c6\uff0c\u4fc3\u8fdb\u66f4\u516c\u5e73\u3001\u66f4\u5168\u9762\u7684\u6a21\u578b\u6bd4\u8f83\u3002 4. \u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 \u667a\u80fd\u5ea7\u8231\u548c\u8f66\u8f7d\u52a9\u624b\uff1a \u76f4\u63a5\u53d7\u76ca\uff0c\u53ef\u5b9e\u73b0\u66f4\u7cbe\u51c6\u7684\u7269\u4f53\u8bc6\u522b\u548c\u5b9a\u4f4d\uff0c\u63d0\u5347\u8bed\u97f3\u52a9\u624b\u54cd\u5e94\u8d28\u91cf\u548c\u7528\u6237\u4f53\u9a8c\u3002 \u9a7e\u9a76\u5458\u548c\u4e58\u5ba2\u76d1\u63a7\uff1a \u7528\u4e8e\u68c0\u6d4b\u8f66\u5185\u5f02\u5e38\u7269\u54c1\u3001\u513f\u7ae5\u9057\u7559\u3001\u9a7e\u9a76\u5458\u75b2\u52b3\u6216\u5206\u5fc3\u7b49\uff0c\u63d0\u5347\u884c\u8f66\u5b89\u5168\u3002 \u8fb9\u7f18\u8ba1\u7b97\u548c\u7269\u8054\u7f51\uff08IoT\uff09\uff1a \u4efb\u4f55\u9700\u8981\u5f3a\u5927AI\u80fd\u529b\u4f46\u672c\u5730\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\uff08\u5982\u667a\u80fd\u5bb6\u5c45\u8bbe\u5907\u3001\u5de5\u4e1a\u673a\u5668\u4eba\u3001\u667a\u80fd\u6444\u50cf\u5934\uff09\u90fd\u53ef\u4ee5\u501f\u9274\u8fd9\u79cd\u5206\u5e03\u5f0f\u67b6\u6784\u3002 \u673a\u5668\u4eba\u5b66\uff1a \u673a\u5668\u4eba\u901a\u5e38\u5177\u6709\u6709\u9650\u7684\u677f\u8f7d\u8ba1\u7b97\u80fd\u529b\uff0c\u4f46\u53ef\u80fd\u9700\u8981\u590d\u6742\u7684\u89c6\u89c9\u7406\u89e3\u80fd\u529b\uff0c\u8be5\u65b9\u6cd5\u53ef\u7528\u4e8e\u5b9e\u73b0\u66f4\u9ad8\u7ea7\u7684\u611f\u77e5\u529f\u80fd\u3002 \u9690\u79c1\u8ba1\u7b97\uff1a \u867d\u7136\u6458\u8981\u672a\u63d0\u53ca\uff0c\u4f46\u5206\u5e03\u5f0f\u67b6\u6784\u5728\u672a\u6765\u53ef\u80fd\u4e0e\u8054\u90a6\u5b66\u4e60\u6216\u9690\u79c1\u4fdd\u62a4\u6280\u672f\u7ed3\u5408\uff0c\u4ee5\u5728\u4e91\u7aef\u5904\u7406\u6570\u636e\u65f6\u66f4\u597d\u5730\u4fdd\u62a4\u7528\u6237\u9690\u79c1\u3002 5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u7684\u5c40\u9650\u6027 \u7f51\u7edc\u8fde\u63a5\u4f9d\u8d56\u6027\uff1a \u5206\u5e03\u5f0f\u67b6\u6784\u9ad8\u5ea6\u4f9d\u8d56\u7a33\u5b9a\u3001\u4f4e\u5ef6\u8fdf\u7684\u4e91\u7aef\u7f51\u7edc\u8fde\u63a5\u3002\u5728\u7f51\u7edc\u4fe1\u53f7\u5dee\u6216\u65e0\u4fe1\u53f7\u7684\u533a\u57df\uff0c\u7cfb\u7edf\u7684\u6027\u80fd\u53ef\u80fd\u4f1a\u4e25\u91cd\u4e0b\u964d\u751a\u81f3\u65e0\u6cd5\u5de5\u4f5c\u3002 \u9690\u79c1\u548c\u5b89\u5168\u95ee\u9898\uff1a \u5c06\u8f66\u5185\u573a\u666f\u6570\u636e\u4f20\u8f93\u5230\u4e91\u7aef\u8fdb\u884c\u5904\u7406\uff0c\u53ef\u80fd\u4f1a\u5f15\u53d1\u7528\u6237\u9690\u79c1\u548c\u6570\u636e\u5b89\u5168\u65b9\u9762\u7684\u62c5\u5fe7\uff0c\u9700\u8981\u5f3a\u5927\u7684\u52a0\u5bc6\u548c\u9690\u79c1\u4fdd\u62a4\u673a\u5236\u3002 \u5b9e\u65f6\u6027\u6311\u6218\uff1a \u5c3d\u7ba1\u89e3\u51b3\u4e86\u8d44\u6e90\u9650\u5236\uff0c\u4f46\u4e91\u7aef\u901a\u4fe1\u7684\u5f80\u8fd4\u5ef6\u8fdf\uff08latency\uff09\u5bf9\u4e8e\u67d0\u4e9b\u5bf9\u5b9e\u65f6\u6027\u8981\u6c42\u6781\u9ad8\u7684\u8f66\u8f7d\u4efb\u52a1\uff08\u5982\u7d27\u6025\u5b89\u5168\u54cd\u5e94\uff09\u53ef\u80fd\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002\u6458\u8981\u4e2d\u672a\u63d0\u4f9b\u5177\u4f53\u7684\u5ef6\u8fdf\u6307\u6807\u3002 \u8fd0\u8425\u6210\u672c\uff1a \u6301\u7eed\u5229\u7528\u4e91\u7aef\u8ba1\u7b97\u8d44\u6e90\u4f1a\u4ea7\u751f\u76f8\u5e94\u7684\u8fd0\u8425\u6210\u672c\uff0c\u8fd9\u5bf9\u4e8e\u5927\u89c4\u6a21\u90e8\u7f72\u548c\u5546\u4e1a\u5316\u53ef\u80fd\u662f\u4e00\u4e2a\u9700\u8981\u8003\u8651\u7684\u56e0\u7d20\u3002 \u6cdb\u5316\u80fd\u529b\uff1a \u5c3d\u7ba1ODAL-LLaVA\u5728\u201c\u6c7d\u8f66\u5185\u90e8\u201d\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5728\u5176\u4ed6\u901a\u7528\u7269\u4f53\u68c0\u6d4b\u6216\u5b9a\u4f4d\u4efb\u52a1\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u5982\u4f55\uff0c\u6458\u8981\u4e2d\u5e76\u672a\u63d0\u53ca\u3002\u5176\u9ad8\u6027\u80fd\u53ef\u80fd\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u9488\u5bf9\u7279\u5b9a\u9886\u57df\u8fdb\u884c\u7684\u5fae\u8c03\u3002 Key Findings: To address this limitation, we propose the novel Object Detection and Localization (ODAL) framework for interior scene understanding. Our approach leverages vision foundation models through a distributed architecture, splitting computational tasks between on-board and cloud. To benchmark model performance, we introduce ODALbench, a new metric for comprehensive assessment of detection and localization.Our analysis demonstrates the framework's potential to establish new standards in this domain. We compare the state-of-the-art GPT-4o vision foundation model with the lightweight LLaVA 1.5 7B model and explore how fine-tuning enhances the lightweight models performance. Links: PDF arXiv Generalizing Monocular 3D Object Detection Authors: Abhinav Kumar Published: 2025-08-27 Categories: cs.CV Abstract: Monocular 3D object detection (Mono3D) is a fundamental computer vision task that estimates an object's class, 3D position, dimensions, and orientation from a single image. Its applications, including autonomous driving, augmented reality, and robotics, critically rely on accurate 3D environmental understanding. This thesis addresses the challenge of generalizing Mono3D models to diverse scenarios, including occlusions, datasets, object sizes, and camera parameters. To enhance occlusion robustness, we propose a mathematically differentiable NMS (GrooMeD-NMS). To improve generalization to new datasets, we explore depth equivariant (DEVIANT) backbones. We address the issue of large object detection, demonstrating that it's not solely a data imbalance or receptive field problem but also a noise sensitivity issue. To mitigate this, we introduce a segmentation-based approach in bird's-eye view with dice loss (SeaBird). Finally, we mathematically analyze the extrapolation of Mono3D models to unseen camera heights and improve Mono3D generalization in such out-of-distribution settings. Analysis: \u8fd9\u7bc7\u8bba\u6587\u7684\u6458\u8981\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5173\u4e8e\u5355\u76ee3D\u76ee\u6807\u68c0\u6d4b\uff08Mono3D\uff09\u9886\u57df\u91cd\u8981\u8fdb\u5c55\u7684\u6982\u89c8\uff0c\u7279\u522b\u5173\u6ce8\u5176\u6cdb\u5316\u80fd\u529b\u3002\u4ee5\u4e0b\u662f\u8be6\u7ec6\u5206\u6790\uff1a 1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u6d01\u603b\u7ed3 (Concise Summary) \u8fd9\u7bc7\u8bba\u6587\u7cfb\u7edf\u6027\u5730\u89e3\u51b3\u4e86\u5355\u76ee3D\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u5728\u590d\u6742\u548c\u591a\u6837\u5316\u573a\u666f\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u6311\u6218\u3002\u4f5c\u8005\u901a\u8fc7\u63d0\u51fa\u4e00\u7cfb\u5217\u521b\u65b0\u65b9\u6cd5\uff0c\u5206\u522b\u63d0\u5347\u4e86\u6a21\u578b\u5728\u5904\u7406\u906e\u6321\u3001\u9002\u5e94\u65b0\u6570\u636e\u96c6\u3001\u68c0\u6d4b\u5927\u578b\u7269\u4f53\u4ee5\u53ca\u5e94\u5bf9\u4e0d\u540c\u76f8\u673a\u53c2\u6570\uff08\u7279\u522b\u662f\u76f8\u673a\u9ad8\u5ea6\uff09\u65f6\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002\u5176\u6838\u5fc3\u8d21\u732e\u5728\u4e8e\u7cfb\u7edf\u6027\u5730\u589e\u5f3aMono3D\u6a21\u578b\u7684\u5b9e\u7528\u6027\u548c\u53ef\u9760\u6027\u3002 2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u5b66\u65b9\u6cd5 (Key Innovation or Methodological Approach) \u8be5\u8bba\u6587\u7684\u5173\u952e\u521b\u65b0\u5728\u4e8e\u5176 \u591a\u7ba1\u9f50\u4e0b\u3001\u7cfb\u7edf\u6027\u5730\u89e3\u51b3Mono3D\u6cdb\u5316\u95ee\u9898\u7684\u7b56\u7565 \u3002\u5b83\u5e76\u975e\u4f9d\u8d56\u5355\u4e00\u7684\u7a81\u7834\u6027\u7b97\u6cd5\uff0c\u800c\u662f\u9488\u5bf9\u6cdb\u5316\u80fd\u529b\u7684\u591a\u4e2a\u5177\u4f53\u7ef4\u5ea6\uff08\u906e\u6321\u3001\u6570\u636e\u96c6\u3001\u7269\u4f53\u5927\u5c0f\u3001\u76f8\u673a\u53c2\u6570\uff09\u63d0\u51fa\u4e86\u5b9a\u5236\u5316\u7684\u89e3\u51b3\u65b9\u6848\uff1a GrooMeD-NMS (Occlusion Robustness): \u63d0\u51fa\u4e86\u4e00\u79cd \u6570\u5b66\u53ef\u5fae\u5206\u7684\u975e\u6781\u5927\u503c\u6291\u5236\uff08NMS\uff09 \u65b9\u6cd5\uff0c\u65e8\u5728\u63d0\u9ad8\u6a21\u578b\u5728\u5b58\u5728\u906e\u6321\u60c5\u51b5\u4e0b\u7684\u68c0\u6d4b\u6027\u80fd\u3002\u53ef\u5fae\u5206NMS\u5141\u8bb8\u7aef\u5230\u7aef\u8bad\u7ec3\uff0c\u66f4\u597d\u5730\u4f18\u5316\u906e\u6321\u573a\u666f\u4e0b\u7684\u68c0\u6d4b\u7ed3\u679c\u3002 DEVIANT Backbones (Dataset Generalization): \u63a2\u7d22\u4e86 \u6df1\u5ea6\u7b49\u53d8\uff08depth equivariant\uff09\u9aa8\u5e72\u7f51\u7edc \uff0c\u4ee5\u589e\u5f3a\u6a21\u578b\u5bf9\u65b0\u6570\u636e\u96c6\u7684\u9002\u5e94\u6027\u3002\u6df1\u5ea6\u7b49\u53d8\u6027\u610f\u5473\u7740\u6a21\u578b\u5bf9\u8f93\u5165\u6df1\u5ea6\u56fe\u7684\u53d8\u6362\u5177\u6709\u4e00\u81f4\u7684\u54cd\u5e94\uff0c\u8fd9\u5bf9\u4e8e\u8de8\u6570\u636e\u96c6\u7684\u6cdb\u5316\u81f3\u5173\u91cd\u8981\u3002 SeaBird (Large Object Detection): \u9488\u5bf9\u5927\u578b\u7269\u4f53\u68c0\u6d4b\u4e2d\u7684\u566a\u58f0\u654f\u611f\u6027\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd \u57fa\u4e8e\u9e1f\u77b0\u56fe\uff08BEV\uff09\u7684\u5206\u5272\u65b9\u6cd5\uff0c\u5e76\u7ed3\u5408Dice\u635f\u5931 \u3002\u8fd9\u8868\u660e\u4f5c\u8005\u5c06\u5927\u578b\u7269\u4f53\u68c0\u6d4b\u91cd\u65b0\u5b9a\u4e49\u4e3a\u4e00\u4e2a\u5206\u5272\u4efb\u52a1\uff0c\u5e76\u5229\u7528BEV\u7684\u4f18\u52bf\u6765\u964d\u4f4e\u566a\u58f0\u5f71\u54cd\u3002 Mathematical Analysis for Camera Parameters: \u5bf9Mono3D\u6a21\u578b\u5728 \u672a\u89c1\u8fc7\u7684\u76f8\u673a\u9ad8\u5ea6 \u7b49\u5206\u5e03\u5916\uff08OOD\uff09\u8bbe\u7f6e\u4e0b\u7684\u5916\u63a8\u80fd\u529b\u8fdb\u884c\u4e86 \u6570\u5b66\u5206\u6790 \uff0c\u5e76\u636e\u6b64\u6539\u8fdb\u4e86\u6cdb\u5316\u6027\u80fd\u3002\u8fd9\u4e3a\u7406\u89e3\u548c\u63d0\u5347\u6a21\u578b\u5728\u4e0d\u540c\u76f8\u673a\u914d\u7f6e\u4e0b\u7684\u8868\u73b0\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002 3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd (Potential Impact on the Field) \u8fd9\u9879\u7814\u7a76\u5982\u679c\u6210\u529f\uff0c\u5c06\u5bf9\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u4ea7\u751f\u663e\u8457\u5f71\u54cd\uff0c\u5c24\u5176\u662f\u5728\u4f9d\u8d563D\u611f\u77e5\u7684\u5b9e\u9645\u5e94\u7528\u4e2d\uff1a \u63d0\u5347Mono3D\u7684\u5b9e\u7528\u6027\uff1a \u901a\u8fc7\u89e3\u51b3\u6cdb\u5316\u6027\u8fd9\u4e00\u6838\u5fc3\u6311\u6218\uff0c\u8be5\u7814\u7a76\u5c06\u4f7f\u5355\u76ee3D\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u590d\u6742\u591a\u53d8\u7684\u73af\u5883\u4e2d\u66f4\u52a0\u53ef\u9760\u548c\u5b9e\u7528\uff0c\u4ece\u800c\u52a0\u901f\u5176\u5728\u81ea\u52a8\u9a7e\u9a76\u3001\u589e\u5f3a\u73b0\u5b9e\u548c\u673a\u5668\u4eba\u7b49\u9886\u57df\u7684\u90e8\u7f72\u3002 \u63a8\u52a8\u6cdb\u5316\u6027\u7814\u7a76\uff1a \u8bba\u6587\u9488\u5bf9\u4e0d\u540c\u6cdb\u5316\u6311\u6218\u63d0\u51fa\u7684\u5177\u4f53\u89e3\u51b3\u65b9\u6848\uff08\u5982\u53ef\u5fae\u5206NMS\u3001\u6df1\u5ea6\u7b49\u53d8\u7f51\u7edc\u3001BEV\u5206\u5272\u65b9\u6cd5\uff09\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u548c\u5de5\u5177\uff0c\u9f13\u52b1\u7814\u7a76\u4eba\u5458\u66f4\u7ec6\u81f4\u5730\u5206\u6790\u548c\u89e3\u51b3\u6a21\u578b\u6cdb\u5316\u95ee\u9898\u3002 \u7406\u8bba\u4e0e\u5b9e\u8df5\u7ed3\u5408\uff1a \u5bf9\u76f8\u673a\u53c2\u6570\u7684\u6570\u5b66\u5206\u6790\uff0c\u7ed3\u5408\u5177\u4f53\u7684\u7b97\u6cd5\u6539\u8fdb\uff0c\u4f53\u73b0\u4e86\u7406\u8bba\u6307\u5bfc\u5b9e\u8df5\u7684\u4ef7\u503c\uff0c\u6709\u52a9\u4e8e\u6784\u5efa\u66f4\u5177\u9c81\u68d2\u6027\u7684\u6a21\u578b\u3002 4. \u76f8\u5173\u9886\u57df\u6216\u5e94\u7528\u53d7\u76ca (Related Areas or Applications that might benefit) \u9664\u4e86\u6458\u8981\u4e2d\u660e\u786e\u63d0\u5230\u7684 \u81ea\u52a8\u9a7e\u9a76\u3001\u589e\u5f3a\u73b0\u5b9e\u548c\u673a\u5668\u4eba\u6280\u672f \uff0c\u4ee5\u4e0b\u9886\u57df\u548c\u5e94\u7528\u4e5f\u53ef\u80fd\u4ece\u8fd9\u9879\u7814\u7a76\u4e2d\u53d7\u76ca\uff1a 3D\u573a\u666f\u7406\u89e3\uff1a \u4efb\u4f55\u9700\u8981\u4ece\u5355\u76ee\u56fe\u50cf\u8fdb\u884c\u7cbe\u786e3D\u573a\u666f\u91cd\u5efa\u548c\u7406\u89e3\u7684\u4efb\u52a1\uff0c\u4f8b\u5982\u667a\u80fd\u57ce\u5e02\u76d1\u63a7\u3001\u65e0\u4eba\u673a\u5de1\u68c0\u3002 \u4eba\u673a\u4ea4\u4e92\uff1a \u9700\u8981\u5b9e\u65f6\u611f\u77e5\u7528\u62373D\u59ff\u6001\u6216\u624b\u52bf\u7684\u5e94\u7528\uff0c\u5982\u865a\u62df\u73b0\u5b9e\uff08VR\uff09\u4e2d\u7684\u4ea4\u4e92\u3001\u667a\u80fd\u5bb6\u5c45\u63a7\u5236\u3002 \u5de5\u4e1a\u68c0\u6d4b\u4e0e\u6d4b\u91cf\uff1a \u5728\u751f\u4ea7\u7ebf\u4e0a\u8fdb\u884c\u7269\u4f53\u5c3a\u5bf8\u3001\u4f4d\u7f6e\u548c\u7f3a\u9677\u76843D\u68c0\u6d4b\uff0c\u5c24\u5176\u662f\u5728\u6210\u672c\u654f\u611f\u6216\u7a7a\u95f4\u53d7\u9650\u7684\u573a\u666f\u4e0b\u3002 \u533b\u5b66\u5f71\u50cf\u5206\u6790\uff1a \u5982\u679c\u80fd\u5c062D\u533b\u5b66\u56fe\u50cf\u8f6c\u5316\u4e3a3D\u7ed3\u6784\uff0c\u5c06\u6709\u52a9\u4e8e\u75be\u75c5\u8bca\u65ad\u548c\u6cbb\u7597\u89c4\u5212\u3002 \u8ba1\u7b97\u673a\u56fe\u5f62\u5b66\uff1a \u8f85\u52a93D\u6a21\u578b\u751f\u6210\u3001\u573a\u666f\u91cd\u5efa\u548c\u865a\u62df\u73af\u5883\u7684\u521b\u5efa\u3002 5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u7684\u5c40\u9650\u6027 (Limitations that can be inferred from the abstract) \u5c3d\u7ba1\u6458\u8981\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u8d21\u732e\uff0c\u4f46\u4ecd\u53ef\u63a8\u65ad\u51fa\u4e00\u4e9b\u6f5c\u5728\u7684\u5c40\u9650\u6027\uff1a \u6a21\u578b\u590d\u6742\u6027\u589e\u52a0\uff1a \u5f15\u5165\u591a\u79cd\u4e13\u95e8\u7684\u6a21\u5757\uff08GrooMeD-NMS\u3001DEVIANT\u9aa8\u5e72\u3001SeaBird\uff09\u53ef\u80fd\u4f1a\u663e\u8457\u589e\u52a0\u6a21\u578b\u7684\u6574\u4f53\u590d\u6742\u6027\u3001\u8ba1\u7b97\u6210\u672c\u548c\u5185\u5b58\u9700\u6c42\uff0c\u8fd9\u53ef\u80fd\u5bf9\u5b9e\u65f6\u5e94\u7528\u6216\u8d44\u6e90\u53d7\u9650\u7684\u8bbe\u5907\u6784\u6210\u6311\u6218\u3002 \u6a21\u5757\u95f4\u534f\u540c\u4e0e\u96c6\u6210\uff1a \u6458\u8981\u5c06\u8fd9\u4e9b\u89e3\u51b3\u65b9\u6848\u89c6\u4e3a\u72ec\u7acb\u7684\u95ee\u9898\u89e3\u51b3\u8005\u3002\u5982\u4f55\u6709\u6548\u5730\u5c06\u8fd9\u4e9b\u4e0d\u540c\u7684\u7ec4\u4ef6\u96c6\u6210\u5230\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\u4e2d\uff0c\u5e76\u786e\u4fdd\u5b83\u4eec\u4e4b\u95f4\u826f\u597d\u7684\u534f\u540c\u4f5c\u7528\u800c\u975e\u76f8\u4e92\u5e72\u6270\uff0c\u662f\u4e00\u4e2a\u6f5c\u5728\u7684\u6311\u6218\u3002 \u6cdb\u5316\u8303\u56f4\u7684\u5b8c\u6574\u6027\uff1a \u5c3d\u7ba1\u89e3\u51b3\u4e86\u906e\u6321\u3001\u6570\u636e\u96c6\u3001\u7269\u4f53\u5927\u5c0f\u548c\u76f8\u673a\u53c2\u6570\u7b49\u5173\u952e\u6cdb\u5316\u7ef4\u5ea6\uff0c\u4f46\u201c\u6cdb\u5316\u201d\u662f\u4e00\u4e2a\u975e\u5e38\u5e7f\u6cdb\u7684\u6982\u5ff5\u3002\u6458\u8981\u5e76\u672a\u63d0\u53ca\u5bf9\u5176\u4ed6\u91cd\u8981\u56e0\u7d20\uff08\u5982\u6781\u7aef\u5149\u7167\u53d8\u5316\u3001\u6076\u52a3\u5929\u6c14\u6761\u4ef6\u3001\u7269\u4f53\u6750\u8d28\u591a\u6837\u6027\u3001\u8fd0\u52a8\u6a21\u7cca\u7b49\uff09\u7684\u6cdb\u5316\u80fd\u529b\u3002 \u5355\u76ee\u89c6\u89c9\u7684\u56fa\u6709\u5c40\u9650\uff1a \u65e0\u8bba\u6a21\u578b\u5982\u4f55\u4f18\u5316\uff0c\u5355\u76ee3D\u68c0\u6d4b\u59cb\u7ec8\u9762\u4e34\u4ece2D\u56fe\u50cf\u63a8\u65ad3D\u6df1\u5ea6\u4fe1\u606f\u7684\u56fa\u6709\u6a21\u7cca\u6027\u3002\u8be5\u5de5\u4f5c\u65e8\u5728\u7f13\u89e3\u8fd9\u4e9b\u95ee\u9898\uff0c\u4f46\u65e0\u6cd5\u4ece\u6839\u672c\u4e0a\u6d88\u9664\u5355\u76ee\u8f93\u5165\u7684\u5c40\u9650\u6027\uff0c\u5176\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u53ef\u80fd\u4ecd\u96be\u4ee5\u4e0e\u591a\u76ee\u6216\u6df1\u5ea6\u4f20\u611f\u5668\u65b9\u6848\u5ab2\u7f8e\u3002 \u201c\u63a2\u7d22\u201d\u7684\u542b\u4e49\uff1a \u5bf9\u4e8eDEVIANT\u9aa8\u5e72\u7f51\u7edc\uff0c\u6458\u8981\u4f7f\u7528\u4e86\u201cwe explore\u201d\uff08\u6211\u4eec\u63a2\u7d22\uff09\uff0c\u8fd9\u53ef\u80fd\u6697\u793a\u8be5\u65b9\u5411\u4ecd\u5728\u7814\u7a76\u4e2d\uff0c\u5176\u6700\u7ec8\u7684\u6cdb\u5316\u6548\u679c\u548c\u7a33\u5b9a\u6027\u53ef\u80fd\u5c1a\u672a\u5b8c\u5168\u9a8c\u8bc1\u6216\u8fbe\u5230\u6700\u4f73\u72b6\u6001\u3002 Key Findings: To enhance occlusion robustness, we propose a mathematically differentiable NMS (GrooMeD-NMS). To improve generalization to new datasets, we explore depth equivariant (DEVIANT) backbones. To mitigate this, we introduce a segmentation-based approach in bird's-eye view with dice loss (SeaBird). Links: PDF arXiv CraftGraffiti: Exploring Human Identity with Custom Graffiti Art via Facial-Preserving Diffusion Models Authors: Ayan Banerjee, Fernando Vilari\u00f1o, Josep Llad\u00f3s Published: 2025-08-28 Categories: cs.CV Abstract: Preserving facial identity under extreme stylistic transformation remains a major challenge in generative art. In graffiti, a high-contrast, abstract medium, subtle distortions to the eyes, nose, or mouth can erase the subject's recognizability, undermining both personal and cultural authenticity. We present CraftGraffiti, an end-to-end text-guided graffiti generation framework designed with facial feature preservation as a primary objective. Given an input image and a style and pose descriptive prompt, CraftGraffiti first applies graffiti style transfer via LoRA-fine-tuned pretrained diffusion transformer, then enforces identity fidelity through a face-consistent self-attention mechanism that augments attention layers with explicit identity embeddings. Pose customization is achieved without keypoints, using CLIP-guided prompt extension to enable dynamic re-posing while retaining facial coherence. We formally justify and empirically validate the \"style-first, identity-after\" paradigm, showing it reduces attribute drift compared to the reverse order. Quantitative results demonstrate competitive facial feature consistency and state-of-the-art aesthetic and human preference scores, while qualitative analyses and a live deployment at the Cruilla Festival highlight the system's real-world creative impact. CraftGraffiti advances the goal of identity-respectful AI-assisted artistry, offering a principled approach for blending stylistic freedom with recognizability in creative AI applications. Analysis: \u8fd9\u7bc7\u8bba\u6587\u300aCraftGraffiti: Exploring Human Identity with Custom Graffiti Art via Facial-Preserving Diffusion Models\u300b\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u751f\u6210\u5f0fAI\u9886\u57df\u63d0\u51fa\u4e86\u4e00\u4e2a\u5f15\u4eba\u6ce8\u76ee\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u6781\u7aef\u98ce\u683c\u5316\u8f6c\u6362\u65f6\u7684\u4eba\u8138\u8eab\u4efd\u4fdd\u6301\u95ee\u9898\u3002 1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u660e\u603b\u7ed3 CraftGraffiti \u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u3001\u6587\u672c\u5f15\u5bfc\u7684\u6d82\u9e26\u751f\u6210\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u5728\u6781\u7aef\u98ce\u683c\u5316\uff08\u5982\u6d82\u9e26\uff09\u4e0b\u4eba\u8138\u8eab\u4efd\u96be\u4ee5\u4fdd\u6301\u7684\u6311\u6218\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u7ed3\u5408 LoRA \u5fae\u8c03\u7684\u6269\u6563\u6a21\u578b\u8fdb\u884c\u98ce\u683c\u8fc1\u79fb\uff0c\u5e76\u5f15\u5165\u4e00\u79cd\u5e26\u6709\u663e\u5f0f\u8eab\u4efd\u5d4c\u5165\u7684\u4eba\u8138\u4e00\u81f4\u81ea\u6ce8\u610f\u529b\u673a\u5236\u6765\u5f3a\u5236\u4fdd\u6301\u8eab\u4efd\uff0c\u540c\u65f6\u5229\u7528 CLIP \u5f15\u5bfc\u7684\u63d0\u793a\u6269\u5c55\u5b9e\u73b0\u65e0\u5173\u952e\u70b9\u7684\u59ff\u6001\u5b9a\u5236\u3002\u8bba\u6587\u7684\u6838\u5fc3\u8d21\u732e\u5728\u4e8e\u9a8c\u8bc1\u4e86\u201c\u5148\u98ce\u683c\uff0c\u540e\u8eab\u4efd\u201d\u7684\u5904\u7406\u8303\u5f0f\u80fd\u6709\u6548\u51cf\u5c11\u5c5e\u6027\u6f02\u79fb\uff0c\u5e76\u5728\u8eab\u4efd\u4e00\u81f4\u6027\u3001\u7f8e\u5b66\u548c\u7528\u6237\u504f\u597d\u65b9\u9762\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002 2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u8bba \u8be5\u8bba\u6587\u7684\u5173\u952e\u521b\u65b0\u548c\u65b9\u6cd5\u8bba\u4f53\u73b0\u5728\u4ee5\u4e0b\u51e0\u4e2a\u65b9\u9762\uff1a \u201c\u5148\u98ce\u683c\uff0c\u540e\u8eab\u4efd\u201d\u8303\u5f0f\uff1a \u8bba\u6587\u5f62\u5f0f\u5316\u5730\u8bba\u8bc1\u5e76\u7ecf\u9a8c\u6027\u5730\u9a8c\u8bc1\u4e86\u201c\u5148\u98ce\u683c\u8fc1\u79fb\uff0c\u540e\u8eab\u4efd\u4fdd\u6301\u201d\u7684\u5904\u7406\u987a\u5e8f\u4f18\u4e8e\u53cd\u5411\u987a\u5e8f\uff0c\u80fd\u6709\u6548\u51cf\u5c11\u751f\u6210\u8fc7\u7a0b\u4e2d\u5c5e\u6027\u7684\u6f02\u79fb\uff0c\u8fd9\u4e3a\u591a\u76ee\u6807\u751f\u6210\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u4e2a\u91cd\u8981\u7684\u8bbe\u8ba1\u539f\u5219\u3002 \u4eba\u8138\u4e00\u81f4\u81ea\u6ce8\u610f\u529b\u673a\u5236\u4e0e\u663e\u5f0f\u8eab\u4efd\u5d4c\u5165\uff1a \u4e3a\u4e86\u5728\u98ce\u683c\u5316\u540e\u5f3a\u5236\u4fdd\u6301\u8eab\u4efd\uff0cCraftGraffiti \u5728\u6269\u6563\u6a21\u578b\u7684\u6ce8\u610f\u529b\u5c42\u4e2d\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4eba\u8138\u4e00\u81f4\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5e76\u8f85\u4ee5\u663e\u5f0f\u8eab\u4efd\u5d4c\u5165\u3002\u8fd9\u79cd\u673a\u5236\u80fd\u591f\u5f15\u5bfc\u6a21\u578b\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u66f4\u5173\u6ce8\u5e76\u4fdd\u7559\u5173\u952e\u7684\u9762\u90e8\u7279\u5f81\u3002 \u65e0\u5173\u952e\u70b9\u7684\u59ff\u6001\u5b9a\u5236\uff1a \u4f20\u7edf\u7684\u59ff\u6001\u63a7\u5236\u901a\u5e38\u4f9d\u8d56\u4e8e\u5173\u952e\u70b9\u68c0\u6d4b\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u4f7f\u7528 CLIP \u5f15\u5bfc\u7684\u63d0\u793a\u6269\u5c55\u6765\u5b9e\u73b0\u52a8\u6001\u7684\u59ff\u6001\u91cd\u65b0\u5b9a\u4f4d\uff0c\u540c\u65f6\u4fdd\u6301\u9762\u90e8\u8fde\u8d2f\u6027\uff0c\u907f\u514d\u4e86\u5bf9\u663e\u5f0f\u5173\u952e\u70b9\u6807\u6ce8\u7684\u4f9d\u8d56\uff0c\u63d0\u9ad8\u4e86\u7075\u6d3b\u6027\u3002 LoRA \u5fae\u8c03\u7684\u6269\u6563\u6a21\u578b\uff1a \u5229\u7528 LoRA (Low-Rank Adaptation) \u5bf9\u9884\u8bad\u7ec3\u7684\u6269\u6563 Transformer \u8fdb\u884c\u5fae\u8c03\uff0c\u4ee5\u9ad8\u6548\u5730\u5b9e\u73b0\u6d82\u9e26\u98ce\u683c\u8fc1\u79fb\uff0c\u8fd9\u662f\u4e00\u79cd\u5728\u4fdd\u6301\u6a21\u578b\u5927\u90e8\u5206\u53c2\u6570\u4e0d\u53d8\u7684\u60c5\u51b5\u4e0b\uff0c\u6709\u6548\u9002\u5e94\u7279\u5b9a\u98ce\u683c\u7684\u7b56\u7565\u3002 3. \u5bf9\u9886\u57df\u7684\u6f5c\u5728\u5f71\u54cd \u63a8\u52a8\u8eab\u4efd\u4fdd\u6301\u751f\u6210\u6280\u672f\uff1a \u8be5\u7814\u7a76\u4e3a\u5728\u6781\u7aef\u98ce\u683c\u5316\u573a\u666f\u4e0b\u4fdd\u6301\u4eba\u8138\u8eab\u4efd\u63d0\u4f9b\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u7684\u65b9\u6cd5\uff0c\u8fd9\u5bf9\u4e8e\u751f\u6210\u5f0fAI\u9886\u57df\u662f\u4e00\u4e2a\u957f\u671f\u5b58\u5728\u7684\u6311\u6218\u3002\u5b83\u4e3a\u672a\u6765\u5f00\u53d1\u66f4\u9c81\u68d2\u3001\u66f4\u5177\u8eab\u4efd\u610f\u8bc6\u7684\u751f\u6210\u6a21\u578b\u5960\u5b9a\u4e86\u57fa\u7840\u3002 \u8d4b\u80fd\u201c\u8eab\u4efd\u5c0a\u91cd\u578bAI\u827a\u672f\u201d\uff1a \u8bba\u6587\u660e\u786e\u63d0\u51fa\u4e86\u201c\u8eab\u4efd\u5c0a\u91cd\u578bAI\u8f85\u52a9\u827a\u672f\u201d\u7684\u76ee\u6807\uff0c\u8fd9\u5728AI\u827a\u672f\u521b\u4f5c\u4e2d\u5177\u6709\u91cd\u8981\u7684\u4f26\u7406\u548c\u5b9e\u9645\u610f\u4e49\u3002\u5b83\u4f7f\u5f97\u7528\u6237\u80fd\u591f\u5728\u4eab\u53d7\u827a\u672f\u81ea\u7531\u7684\u540c\u65f6\uff0c\u786e\u4fdd\u5176\u4e2a\u4eba\u8eab\u4efd\uff08\u5c24\u5176\u662f\u9762\u90e8\u7279\u5f81\uff09\u5f97\u5230\u51c6\u786e\u7684\u4fdd\u7559\uff0c\u4ece\u800c\u589e\u5f3a\u7528\u6237\u4f53\u9a8c\u548c\u4fe1\u4efb\u3002 \u542f\u53d1\u591a\u76ee\u6807\u751f\u6210\u4efb\u52a1\u8bbe\u8ba1\uff1a \u201c\u5148\u98ce\u683c\uff0c\u540e\u8eab\u4efd\u201d\u7684\u8303\u5f0f\u9a8c\u8bc1\uff0c\u4e3a\u5176\u4ed6\u9700\u8981\u5e73\u8861\u591a\u4e2a\u76f8\u4e92\u51b2\u7a81\u7684\u751f\u6210\u76ee\u6807\uff08\u5982\u98ce\u683c\u3001\u5185\u5bb9\u3001\u8eab\u4efd\u3001\u59ff\u6001\u7b49\uff09\u7684\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u6307\u5bfc\u3002 \u6269\u5c55\u6269\u6563\u6a21\u578b\u7684\u5e94\u7528\uff1a \u901a\u8fc7\u7ed3\u5408 LoRA\u3001\u81ea\u5b9a\u4e49\u6ce8\u610f\u529b\u673a\u5236\u548c CLIP \u5f15\u5bfc\uff0c\u5c55\u793a\u4e86\u6269\u6563\u6a21\u578b\u5728\u590d\u6742\u3001\u9ad8\u4fdd\u771f\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u5f3a\u5927\u6f5c\u529b\u548c\u7075\u6d3b\u6027\u3002 4. \u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 \u8fd9\u9879\u7814\u7a76\u7684\u6210\u679c\u53ef\u4ee5\u5e7f\u6cdb\u5e94\u7528\u4e8e\u4ee5\u4e0b\u9886\u57df\uff1a \u4e2a\u6027\u5316\u5185\u5bb9\u521b\u4f5c\uff1a \u7528\u6237\u53ef\u4ee5\u751f\u6210\u9ad8\u5ea6\u98ce\u683c\u5316\u4f46\u4ecd\u80fd\u8bc6\u522b\u51fa\u81ea\u5df1\u9762\u90e8\u7684\u5934\u50cf\u3001\u793e\u4ea4\u5a92\u4f53\u56fe\u7247\u3001\u6570\u5b57\u827a\u672f\u4f5c\u54c1\u7b49\u3002 \u865a\u62df\u8bd5\u7a7f\u4e0e\u6570\u5b57\u65f6\u5c1a\uff1a \u5728\u865a\u62df\u73af\u5883\u4e2d\u5c55\u793a\u670d\u88c5\u6216\u914d\u9970\u65f6\uff0c\u786e\u4fdd\u6a21\u7279\u7684\u8138\u90e8\u7279\u5f81\u4fdd\u6301\u4e00\u81f4\uff0c\u589e\u5f3a\u771f\u5b9e\u611f\u548c\u4e2a\u6027\u5316\u4f53\u9a8c\u3002 \u6e38\u620f\u4e0e\u52a8\u753b\u89d2\u8272\u8bbe\u8ba1\uff1a \u4ece\u771f\u5b9e\u4eba\u7269\u7167\u7247\u751f\u6210\u98ce\u683c\u5316\u7684\u6e38\u620f\u6216\u52a8\u753b\u89d2\u8272\uff0c\u540c\u65f6\u4fdd\u7559\u5176\u6838\u5fc3\u9762\u90e8\u7279\u5f81\u3002 \u827a\u672f\u6ee4\u955c\u4e0e\u7279\u6548\uff1a \u4e3a\u7167\u7247\u548c\u89c6\u9891\u5e94\u7528\u5404\u79cd\u827a\u672f\u98ce\u683c\u6ee4\u955c\uff0c\u540c\u65f6\u786e\u4fdd\u4eba\u8138\u7684\u8bc6\u522b\u5ea6\uff0c\u63d0\u5347\u7528\u6237\u4f53\u9a8c\u3002 \u6570\u5b57\u8eab\u4efd\u4e0e\u5143\u5b87\u5b99\uff1a \u5728\u5143\u5b87\u5b99\u4e2d\u521b\u5efa\u9ad8\u5ea6\u4e2a\u6027\u5316\u4e14\u5177\u6709\u8fa8\u8bc6\u5ea6\u7684\u6570\u5b57\u5206\u8eab\u6216\u865a\u62df\u5f62\u8c61\u3002 \u9762\u90e8\u4fee\u590d\u4e0e\u589e\u5f3a\uff1a \u5728\u5bf9\u8001\u65e7\u7167\u7247\u8fdb\u884c\u98ce\u683c\u5316\u4fee\u590d\u6216\u589e\u5f3a\u65f6\uff0c\u786e\u4fdd\u9762\u90e8\u8eab\u4efd\u7684\u51c6\u786e\u6027\u3002 5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u7684\u5c40\u9650\u6027 \u5c3d\u7ba1\u8bba\u6587\u5c55\u793a\u4e86\u663e\u8457\u7684\u8fdb\u6b65\uff0c\u4f46\u4ece\u6458\u8981\u4e2d\u4ecd\u53ef\u63a8\u65ad\u51fa\u4e00\u4e9b\u6f5c\u5728\u7684\u5c40\u9650\u6027\uff1a \u98ce\u683c\u6cdb\u5316\u6027\uff1a \u8bba\u6587\u660e\u786e\u805a\u7126\u4e8e\u201c\u6d82\u9e26\u201d\u8fd9\u79cd\u201c\u9ad8\u5bf9\u6bd4\u5ea6\u3001\u62bd\u8c61\u201d\u7684\u5a92\u4ecb\u3002\u5176\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u5176\u4ed6\u6781\u7aef\u6216\u975e\u6781\u7aef\u98ce\u683c\uff08\u5982\u6cb9\u753b\u3001\u6c34\u5f69\u3001\u5361\u901a\u7b49\uff09\u4e0b\u7684\u8868\u73b0\u5982\u4f55\uff0c\u4ecd\u9700\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u3002 \u8eab\u4efd\u4fdd\u6301\u7684\u8303\u56f4\uff1a \u6458\u8981\u5f3a\u8c03\u7684\u662f\u201c\u9762\u90e8\u8eab\u4efd\u201d\u3001\u201c\u773c\u775b\u3001\u9f3b\u5b50\u6216\u5634\u5df4\u201d\u7684\u4fdd\u7559\u3002\u8fd9\u8868\u660e\u8be5\u6846\u67b6\u53ef\u80fd\u4e3b\u8981\u5173\u6ce8\u9762\u90e8\u7279\u5f81\uff0c\u5bf9\u4e8e\u5168\u8eab\u8eab\u4efd\u3001\u8eab\u4f53\u59ff\u6001\u7684\u7ec6\u5fae\u7279\u5f81\u6216\u975e\u9762\u90e8\u5c5e\u6027\u7684\u4fdd\u6301\u80fd\u529b\u53ef\u80fd\u6709\u9650\u3002 \u8ba1\u7b97\u6210\u672c\uff1a \u6269\u6563\u6a21\u578b\uff0c\u7279\u522b\u662f\u7ecf\u8fc7 LoRA \u5fae\u8c03\u5e76\u589e\u52a0\u4e86\u81ea\u5b9a\u4e49\u6ce8\u610f\u529b\u673a\u5236\u7684\u6a21\u578b\uff0c\u901a\u5e38\u8ba1\u7b97\u6210\u672c\u8f83\u9ad8\uff0c\u53ef\u80fd\u5bf9\u5b9e\u65f6\u5e94\u7528\u6216\u5927\u89c4\u6a21\u90e8\u7f72\u6784\u6210\u6311\u6218\u3002 \u201c\u5c5e\u6027\u6f02\u79fb\u201d\u7684\u7a0b\u5ea6\uff1a \u5c3d\u7ba1\u201c\u5148\u98ce\u683c\uff0c\u540e\u8eab\u4efd\u201d\u8303\u5f0f\u201c\u51cf\u5c11\u4e86\u5c5e\u6027\u6f02\u79fb\u201d\uff0c\u4f46\u8fd9\u5e76\u4e0d\u610f\u5473\u7740\u5b8c\u5168\u6d88\u9664\u4e86\u6f02\u79fb\u3002\u5728\u67d0\u4e9b\u6781\u7aef\u60c5\u51b5\u4e0b\uff0c\u9762\u90e8\u7279\u5f81\u7684\u7ec6\u5fae\u4e4b\u5904\u4ecd\u53ef\u80fd\u53d7\u5230\u5f71\u54cd\uff0c\u5bfc\u81f4\u8bc6\u522b\u5ea6\u4e0b\u964d\u3002 \u5bf9\u9884\u8bad\u7ec3\u6a21\u578b\u548c CLIP \u7684\u4f9d\u8d56\uff1a \u6a21\u578b\u7684\u6027\u80fd\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u4f9d\u8d56\u4e8e\u6240\u4f7f\u7528\u7684\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u548c CLIP \u6a21\u578b\u7684\u8d28\u91cf\u548c\u6f5c\u5728\u504f\u5dee\u3002\u8fd9\u4e9b\u57fa\u7840\u6a21\u578b\u7684\u5c40\u9650\u6027\u53ef\u80fd\u4f1a\u4f20\u9012\u5230 CraftGraffiti \u7684\u8f93\u51fa\u4e2d\u3002 \u201c\u7ade\u4e89\u6027\u201d\u7ed3\u679c\u7684\u542b\u4e49\uff1a \u6458\u8981\u4e2d\u63d0\u5230\u201c\u5b9a\u91cf\u7ed3\u679c\u5c55\u793a\u4e86\u7ade\u4e89\u6027\u7684\u4eba\u8138\u7279\u5f81\u4e00\u81f4\u6027\u201d\uff0c\u8fd9\u901a\u5e38\u610f\u5473\u7740\u8868\u73b0\u826f\u597d\uff0c\u4f46\u53ef\u80fd\u5e76\u975e\u5b8c\u7f8e\u65e0\u7455\u3002\u5728\u67d0\u4e9b\u7279\u5b9a\u6216\u590d\u6742\u7684\u4eba\u8138\u8868\u60c5\u3001\u5149\u7167\u6216\u906e\u6321\u6761\u4ef6\u4e0b\uff0c\u5176\u9c81\u68d2\u6027\u4ecd\u6709\u5f85\u6df1\u5165\u63a2\u8ba8\u3002 Key Findings: Quantitative results demonstrate competitive facial feature consistency and state-of-the-art aesthetic and human preference scores, while qualitative analyses and a live deployment at the Cruilla Festival highlight the system's real-world creative impact. Links: PDF arXiv Dino U-Net: Exploiting High-Fidelity Dense Features from Foundation Models for Medical Image Segmentation Authors: Yifan Gao, Haoyue Li, Feng Yuan, Xiaosong Wang, Xin Gao Published: 2025-08-28 Categories: cs.CV, eess.IV Abstract: Foundation models pre-trained on large-scale natural image datasets offer a powerful paradigm for medical image segmentation. However, effectively transferring their learned representations for precise clinical applications remains a challenge. In this work, we propose Dino U-Net, a novel encoder-decoder architecture designed to exploit the high-fidelity dense features of the DINOv3 vision foundation model. Our architecture introduces an encoder built upon a frozen DINOv3 backbone, which employs a specialized adapter to fuse the model's rich semantic features with low-level spatial details. To preserve the quality of these representations during dimensionality reduction, we design a new fidelity-aware projection module (FAPM) that effectively refines and projects the features for the decoder. We conducted extensive experiments on seven diverse public medical image segmentation datasets. Our results show that Dino U-Net achieves state-of-the-art performance, consistently outperforming previous methods across various imaging modalities. Our framework proves to be highly scalable, with segmentation accuracy consistently improving as the backbone model size increases up to the 7-billion-parameter variant. The findings demonstrate that leveraging the superior, dense-pretrained features from a general-purpose foundation model provides a highly effective and parameter-efficient approach to advance the accuracy of medical image segmentation. The code is available at https://github.com/yifangao112/DinoUNet. Analysis: \u4f5c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u4e13\u5bb6\uff0c\u6211\u5bf9\u8fd9\u7bc7\u8bba\u6587\u6458\u8981\u8fdb\u884c\u5982\u4e0b\u5206\u6790\uff1a Dino U-Net: Exploiting High-Fidelity Dense Features from Foundation Models for Medical Image Segmentation 1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u660e\u603b\u7ed3 (2-3 \u53e5\u8bdd) \u672c\u6587\u63d0\u51fa\u4e86Dino U-Net\uff0c\u4e00\u4e2a\u65b0\u9896\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\uff0c\u65e8\u5728\u5229\u7528DINOv3\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u9ad8\u4fdd\u771f\u5bc6\u96c6\u7279\u5f81\u8fdb\u884c\u533b\u5b66\u56fe\u50cf\u5206\u5272\u3002\u5b83\u901a\u8fc7\u4e00\u4e2a\u57fa\u4e8e\u51bb\u7ed3DINOv3\u9aa8\u5e72\u7684\u7f16\u7801\u5668\uff0c\u7ed3\u5408\u4e13\u95e8\u7684\u9002\u914d\u5668\u548c\u65b0\u9896\u7684\u4fdd\u771f\u5ea6\u611f\u77e5\u6295\u5f71\u6a21\u5757\uff08FAPM\uff09\uff0c\u6709\u6548\u5730\u5c06\u8bed\u4e49\u7279\u5f81\u4e0e\u7a7a\u95f4\u7ec6\u8282\u878d\u5408\uff0c\u5e76\u4fdd\u6301\u7279\u5f81\u8d28\u91cf\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0cDino U-Net\u5728\u591a\u4e2a\u533b\u5b66\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5c55\u73b0\u51fa\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\uff0c\u4e3a\u533b\u5b66\u56fe\u50cf\u5206\u5272\u63d0\u4f9b\u4e86\u4e00\u79cd\u53c2\u6570\u9ad8\u6548\u4e14\u9ad8\u6027\u80fd\u7684\u89e3\u51b3\u65b9\u6848\u3002 2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u5b66\u65b9\u6cd5 Dino U-Net\u7684\u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u5176\u72ec\u7279\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\uff0c\u7279\u522b\u662f\u5982\u4f55\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u5730\u5229\u7528DINOv3\u57fa\u7840\u6a21\u578b\u7684\u5bc6\u96c6\u7279\u5f81\u8fdb\u884c\u50cf\u7d20\u7ea7\u9884\u6d4b\u3002\u5177\u4f53\u5305\u62ec\uff1a \u57fa\u4e8e\u51bb\u7ed3DINOv3\u9aa8\u5e72\u7684\u7f16\u7801\u5668\u4e0e\u4e13\u95e8\u9002\u914d\u5668\uff1a \u8bba\u6587\u5229\u7528\u4e00\u4e2a\u51bb\u7ed3\u7684DINOv3\u9aa8\u5e72\u4f5c\u4e3a\u7f16\u7801\u5668\uff0c\u8fd9\u6781\u5927\u5730\u63d0\u9ad8\u4e86\u53c2\u6570\u6548\u7387\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u5f15\u5165\u4e86\u4e00\u4e2a\u201c\u4e13\u95e8\u9002\u914d\u5668\u201d\u6765\u878d\u5408DINOv3\u63d0\u53d6\u7684\u4e30\u5bcc\u8bed\u4e49\u7279\u5f81\u4e0e\u4f4e\u5c42\u7a7a\u95f4\u7ec6\u8282\u3002\u8fd9\u5bf9\u4e8e\u533b\u5b66\u56fe\u50cf\u5206\u5272\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u4e3a\u5206\u5272\u4efb\u52a1\u65e2\u9700\u8981\u9ad8\u5c42\u8bed\u4e49\u7406\u89e3\uff0c\u4e5f\u9700\u8981\u7cbe\u786e\u7684\u8fb9\u754c\u548c\u5c40\u90e8\u7ec6\u8282\u3002 \u4fdd\u771f\u5ea6\u611f\u77e5\u6295\u5f71\u6a21\u5757\uff08FAPM\uff09\uff1a \u4e3a\u4e86\u5728\u5c06\u9ad8\u7ef4\u7279\u5f81\u6295\u5f71\u5230\u89e3\u7801\u5668\u6240\u9700\u7ef4\u5ea6\u65f6\uff0c\u6700\u5927\u9650\u5ea6\u5730\u4fdd\u7559\u7279\u5f81\u7684\u8d28\u91cf\u548c\u4fe1\u606f\uff0c\u8bba\u6587\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u65b0\u7684\u201c\u4fdd\u771f\u5ea6\u611f\u77e5\u6295\u5f71\u6a21\u5757\uff08FAPM\uff09\u201d\u3002\u8fd9\u4e2a\u6a21\u5757\u80fd\u591f\u6709\u6548\u5730\u7cbe\u70bc\u548c\u6295\u5f71\u7279\u5f81\uff0c\u907f\u514d\u5728\u964d\u7ef4\u8fc7\u7a0b\u4e2d\u4e22\u5931\u5173\u952e\u4fe1\u606f\uff0c\u4ece\u800c\u786e\u4fdd\u89e3\u7801\u5668\u80fd\u591f\u63a5\u6536\u5230\u9ad8\u8d28\u91cf\u7684\u8868\u793a\u3002 \u5229\u7528\u901a\u7528\u57fa\u7840\u6a21\u578b\u7684\u5bc6\u96c6\u9884\u8bad\u7ec3\u7279\u5f81\uff1a \u8bba\u6587\u5f3a\u8c03\u4e86\u5229\u7528DINOv3\u8fd9\u79cd\u5728\u6d77\u91cf\u81ea\u7136\u56fe\u50cf\u4e0a\u8fdb\u884c\u201c\u5bc6\u96c6\u9884\u8bad\u7ec3\u201d\u7684\u57fa\u7840\u6a21\u578b\u6240\u5e26\u6765\u7684\u4f18\u52bf\u3002DINOv3\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\uff0c\u80fd\u591f\u5b66\u4e60\u5230\u975e\u5e38\u4e30\u5bcc\u7684\u3001\u5bf9\u56fe\u50cf\u5c40\u90e8\u7ed3\u6784\u548c\u5168\u5c40\u8bed\u4e49\u90fd\u654f\u611f\u7684\u7279\u5f81\uff0c\u8fd9\u4e9b\u7279\u5f81\u88ab\u8bc1\u660e\u53ef\u4ee5\u6709\u6548\u5730\u8fc1\u79fb\u5230\u533b\u5b66\u9886\u57df\u3002 3. \u5bf9\u8be5\u9886\u57df\u7684\u6f5c\u5728\u5f71\u54cd \u672c\u7814\u7a76\u7684\u6f5c\u5728\u5f71\u54cd\u662f\u591a\u65b9\u9762\u7684\uff1a \u63d0\u5347\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7cbe\u5ea6\uff1a Dino U-Net\u7684SOTA\u6027\u80fd\u5c06\u4e3a\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6811\u7acb\u65b0\u7684\u6807\u6746\uff0c\u6709\u671b\u63a8\u52a8\u8be5\u9886\u57df\u5728\u4e34\u5e8a\u8bca\u65ad\u548c\u6cbb\u7597\u89c4\u5212\u4e2d\u7684\u5e94\u7528\u3002 \u53c2\u6570\u9ad8\u6548\u7684\u8303\u5f0f\uff1a \u901a\u8fc7\u51bb\u7ed3\u5927\u578b\u57fa\u7840\u6a21\u578b\u9aa8\u5e72\u5e76\u5f15\u5165\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\uff0c\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u53c2\u6570\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002\u8fd9\u5bf9\u4e8e\u533b\u5b66\u9886\u57df\u5c24\u4e3a\u91cd\u8981\uff0c\u56e0\u4e3a\u533b\u5b66\u6570\u636e\u96c6\u901a\u5e38\u89c4\u6a21\u6709\u9650\uff0c\u4e14\u8bad\u7ec3\u6570\u5341\u4ebf\u53c2\u6570\u7684\u6a21\u578b\u6210\u672c\u9ad8\u6602\u3002\u8fd9\u79cd\u8303\u5f0f\u4f7f\u5f97\u5728\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u4e0b\u4e5f\u80fd\u5145\u5206\u5229\u7528\u5927\u578b\u6a21\u578b\u7684\u5f3a\u5927\u80fd\u529b\u3002 \u9a8c\u8bc1\u57fa\u7840\u6a21\u578b\u5728\u4e13\u4e1a\u9886\u57df\u7684\u6cdb\u5316\u80fd\u529b\uff1a \u672c\u6587\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u5728\u901a\u7528\u81ea\u7136\u56fe\u50cf\u4e0a\u9884\u8bad\u7ec3\u7684\u57fa\u7840\u6a21\u578b\uff08\u5982DINOv3\uff09\u5728\u9ad8\u5ea6\u4e13\u4e1a\u5316\u9886\u57df\uff08\u5982\u533b\u5b66\u56fe\u50cf\u5206\u6790\uff09\u7684\u5f3a\u5927\u8fc1\u79fb\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u7cbe\u7ec6\u50cf\u7d20\u7ea7\u7406\u89e3\u7684\u5bc6\u96c6\u9884\u6d4b\u4efb\u52a1\u4e0a\u3002 \u52a0\u901f\u533b\u5b66AI\u7814\u53d1\uff1a \u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6027\u80fd\u4e14\u6613\u4e8e\u4f7f\u7528\u7684\u6846\u67b6\uff08\u4ee3\u7801\u5df2\u5f00\u6e90\uff09\uff0c\u6709\u671b\u52a0\u901f\u7814\u7a76\u4eba\u5458\u5728\u533b\u5b66\u56fe\u50cf\u5206\u5272\u9886\u57df\u7684\u63a2\u7d22\u548c\u521b\u65b0\u3002 4. \u53ef\u80fd\u53d7\u76ca\u4e8e\u8fd9\u9879\u7814\u7a76\u7684\u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 \u5176\u4ed6\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4efb\u52a1\uff1a \u5982\u75c5\u7076\u68c0\u6d4b\u3001\u56fe\u50cf\u914d\u51c6\u3001\u75be\u75c5\u5206\u7c7b\uff08\u7279\u522b\u662f\u9700\u8981\u7cbe\u786e\u5b9a\u4f4d\u75c5\u7076\u7684\uff09\u30013D\u91cd\u5efa\u7b49\uff0c\u8fd9\u4e9b\u4efb\u52a1\u540c\u6837\u9700\u8981\u5f3a\u5927\u7684\u7279\u5f81\u63d0\u53d6\u548c\u50cf\u7d20\u7ea7\u7406\u89e3\u80fd\u529b\u3002 \u5176\u4ed6\u9700\u8981\u7cbe\u7ec6\u50cf\u7d20\u7ea7\u7406\u89e3\u7684\u4e13\u4e1a\u9886\u57df\uff1a \u5982\u5de5\u4e1a\u7f3a\u9677\u68c0\u6d4b\u3001\u9065\u611f\u56fe\u50cf\u5206\u6790\u3001\u751f\u7269\u663e\u5fae\u56fe\u50cf\u5904\u7406\u7b49\uff0c\u8fd9\u4e9b\u9886\u57df\u540c\u6837\u9762\u4e34\u6570\u636e\u7a00\u7f3a\u548c\u5bf9\u7ec6\u8282\u654f\u611f\u7684\u6311\u6218\uff0c\u53ef\u4ee5\u501f\u9274Dino U-Net\u5229\u7528\u57fa\u7840\u6a21\u578b\u7279\u5f81\u7684\u7b56\u7565\u3002 \u5c0f\u6837\u672c\uff08few-shot\uff09\u6216\u96f6\u6837\u672c\uff08zero-shot\uff09\u5b66\u4e60\uff1a DINOv3\u7684\u5f3a\u5927\u7279\u5f81\u63d0\u53d6\u80fd\u529b\u53ef\u80fd\u4e3a\u8fd9\u4e9b\u573a\u666f\u63d0\u4f9b\u66f4\u5f3a\u7684\u6cdb\u5316\u57fa\u7840\uff0c\u5c24\u5176\u662f\u5728\u533b\u5b66\u9886\u57df\uff0c\u65b0\u75be\u75c5\u6216\u7f55\u89c1\u75be\u75c5\u7684\u6570\u636e\u5f80\u5f80\u975e\u5e38\u7a00\u7f3a\u3002 \u6a21\u578b\u538b\u7f29\u4e0e\u90e8\u7f72\uff1a \u51bb\u7ed3\u9aa8\u5e72\u7684\u7b56\u7565\u6709\u52a9\u4e8e\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\uff0c\u964d\u4f4e\u6a21\u578b\u8bad\u7ec3\u548c\u5fae\u8c03\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u4e3a\u672a\u6765\u5728\u8fb9\u7f18\u8bbe\u5907\u6216\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u7684\u4e34\u5e8a\u73af\u5883\u4e2d\u90e8\u7f72\u9ad8\u6027\u80fd\u6a21\u578b\u63d0\u4f9b\u601d\u8def\u3002 5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u51fa\u7684\u5c40\u9650\u6027 \u5bf9DINOv3\u9aa8\u5e72\u7684\u4f9d\u8d56\u6027\uff1a \u5c3d\u7ba1\u51bb\u7ed3\u9aa8\u5e72\u63d0\u9ad8\u4e86\u53c2\u6570\u6548\u7387\uff0c\u4f46DINOv3\u672c\u8eab\u662f\u5728\u81ea\u7136\u56fe\u50cf\u4e0a\u9884\u8bad\u7ec3\u7684\u3002\u867d\u7136\u9002\u914d\u5668\u65e8\u5728\u5f25\u8865\uff0c\u4f46\u5176\u7279\u5f81\u8868\u793a\u53ef\u80fd\u65e0\u6cd5\u5b8c\u5168\u6355\u6349\u533b\u5b66\u56fe\u50cf\u7279\u6709\u7684\u3001\u4e0e\u81ea\u7136\u56fe\u50cf\u5dee\u5f02\u6781\u5927\u7684\u7ec6\u5fae\u75c5\u7406\u7279\u5f81\uff0c\u5c24\u5176\u662f\u5728\u67d0\u4e9b\u4e0e\u81ea\u7136\u56fe\u50cf\u89c6\u89c9\u7279\u6027\u5dee\u5f02\u5de8\u5927\u7684\u533b\u5b66\u6a21\u6001\u4e0a\u3002 \u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\uff1a \u5373\u4f7f\u9aa8\u5e72\u51bb\u7ed3\uff0c\u6458\u8981\u4e2d\u63d0\u53ca\u7684\u201c70\u4ebf\u53c2\u6570\u201d\u6a21\u578b\u5728\u63a8\u7406\u9636\u6bb5\u4ecd\u53ef\u80fd\u9700\u8981\u663e\u8457\u7684\u8ba1\u7b97\u8d44\u6e90\uff0c\u8fd9\u5bf9\u4e8e\u5b9e\u65f6\u6027\u8981\u6c42\u9ad8\u6216\u8d44\u6e90\u53d7\u9650\u7684\u4e34\u5e8a\u90e8\u7f72\u73af\u5883\u53ef\u80fd\u662f\u4e00\u4e2a\u6311\u6218\u3002 \u53ef\u89e3\u91ca\u6027\uff1a \u57fa\u7840\u6a21\u578b\u901a\u5e38\u88ab\u89c6\u4e3a\u9ed1\u7bb1\uff0c\u5f15\u5165\u989d\u5916\u7684\u9002\u914d\u5668\u548c\u6295\u5f71\u6a21\u5757\u53ef\u80fd\u4f1a\u8fdb\u4e00\u6b65\u589e\u52a0\u6a21\u578b\u51b3\u7b56\u8fc7\u7a0b\u7684\u590d\u6742\u6027\uff0c\u964d\u4f4e\u5176\u5728\u4e34\u5e8a\u5e94\u7528\u4e2d\u81f3\u5173\u91cd\u8981\u7684\u53ef\u89e3\u91ca\u6027\u3002 \u9002\u914d\u5668\u548cFAPM\u7684\u8bad\u7ec3\u6570\u636e\u9700\u6c42\uff1a \u5c3d\u7ba1\u9aa8\u5e72\u662f\u51bb\u7ed3\u7684\uff0c\u4f46\u9002\u914d\u5668\u548cFAPM\u4ecd\u9700\u8981\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\u8fdb\u884c\u8bad\u7ec3\u3002\u6458\u8981\u4e2d\u672a\u63d0\u53ca\u8fd9\u4e9b\u6a21\u5757\u5bf9\u8bad\u7ec3\u6570\u636e\u91cf\u7684\u654f\u611f\u6027\uff0c\u8fd9\u53ef\u80fd\u5728\u6781\u5ea6\u7a00\u7f3a\u7684\u533b\u5b66\u6570\u636e\u96c6\u4e0a\u6784\u6210\u6311\u6218\u3002 \u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u533b\u5b66\u6a21\u6001\u6216\u7f55\u89c1\u75c5\u53d8\uff1a \u5c3d\u7ba1\u5728\u201c\u4e03\u4e2a\u591a\u6837\u5316\u7684\u516c\u5171\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6570\u636e\u96c6\u201d\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u4f46\u8fd9\u4e9b\u6570\u636e\u96c6\u53ef\u80fd\u65e0\u6cd5\u5b8c\u5168\u8986\u76d6\u6240\u6709\u533b\u5b66\u6a21\u6001\u6216\u7f55\u89c1\u75c5\u53d8\u7684\u590d\u6742\u6027\u3002\u6a21\u578b\u5728\u9762\u5bf9\u4e0e\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u5dee\u5f02\u8f83\u5927\u7684\u65b0\u6a21\u6001\u6216\u75c5\u53d8\u65f6\u7684\u9c81\u68d2\u6027\u4ecd\u9700\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u3002 Key Findings: In this work, we propose Dino U-Net, a novel encoder-decoder architecture designed to exploit the high-fidelity dense features of the DINOv3 vision foundation model. To preserve the quality of these representations during dimensionality reduction, we design a new fidelity-aware projection module (FAPM) that effectively refines and projects the features for the decoder. Our results show that Dino U-Net achieves state-of-the-art performance, consistently outperforming previous methods across various imaging modalities. Links: PDF arXiv Plug-in Feedback Self-adaptive Attention in CLIP for Training-free Open-Vocabulary Segmentation Authors: Zhixiang Chi, Yanan Wu, Li Gu, Huan Liu, Ziqiang Wang, Yang Zhang, Yang Wang, Konstantinos N. Plataniotis Published: 2025-08-27 Categories: cs.CV, cs.LG Abstract: CLIP exhibits strong visual-textual alignment but struggle with open-vocabulary segmentation due to poor localization. Prior methods enhance spatial coherence by modifying intermediate attention. But, this coherence isn't consistently propagated to the final output due to subsequent operations such as projections. Additionally, intermediate attention lacks direct interaction with text representations, such semantic discrepancy limits the full potential of CLIP. In this work, we propose a training-free, feedback-driven self-adaptive framework that adapts output-based patch-level correspondences back to the intermediate attention. The output predictions, being the culmination of the model's processing, encapsulate the most comprehensive visual and textual semantics about each patch. Our approach enhances semantic consistency between internal representations and final predictions by leveraging the model's outputs as a stronger spatial coherence prior. We design key modules, including attention isolation, confidence-based pruning for sparse adaptation, and adaptation ensemble, to effectively feedback the output coherence cues. Our method functions as a plug-in module, seamlessly integrating into four state-of-the-art approaches with three backbones (ViT-B, ViT-L, ViT-H). We further validate our framework across multiple attention types (Q-K, self-self, and Proxy augmented with MAE, SAM, and DINO). Our approach consistently improves their performance across eight benchmarks. Analysis: \u8fd9\u7bc7\u8bba\u6587\u6458\u8981\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9CLIP\u6a21\u578b\u5728\u5f00\u653e\u8bcd\u6c47\u5206\u5272\uff08Open-Vocabulary Segmentation\uff09\u4efb\u52a1\u4e2d\u5b9a\u4f4d\u80fd\u529b\u4e0d\u8db3\u7684\u521b\u65b0\u89e3\u51b3\u65b9\u6848\u3002\u4ee5\u4e0b\u662f\u8be6\u7ec6\u5206\u6790\uff1a 1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u660e\u603b\u7ed3 (Concise Summary) \u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\uff08training-free\uff09\u7684\u53cd\u9988\u9a71\u52a8\u81ea\u9002\u5e94\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3CLIP\u6a21\u578b\u5728\u5f00\u653e\u8bcd\u6c47\u5206\u5272\u4e2d\u5b9a\u4f4d\u80fd\u529b\u5dee\u7684\u95ee\u9898\u3002\u5b83\u901a\u8fc7\u5c06\u6a21\u578b\u6700\u7ec8\u8f93\u51fa\u5c42\u9762\u7684\u8865\u4e01\u7ea7\uff08patch-level\uff09\u8bed\u4e49\u4e00\u81f4\u6027\u53cd\u9988\u56de\u4e2d\u95f4\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4ece\u800c\u589e\u5f3a\u5185\u90e8\u8868\u793a\u4e0e\u6700\u7ec8\u9884\u6d4b\u4e4b\u95f4\u7684\u8bed\u4e49\u4e00\u81f4\u6027\u3002\u4f5c\u4e3a\u4e00\u4e2a\u5373\u63d2\u5373\u7528\uff08plug-in\uff09\u6a21\u5757\uff0c\u8be5\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u5347\u73b0\u6709SOTA\u6a21\u578b\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u6027\u80fd\u3002 2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u8bba (Key Innovation or Methodological Approach) \u8be5\u8bba\u6587\u7684\u5173\u952e\u521b\u65b0\u5728\u4e8e\u5176 \u201c\u8f93\u51fa\u5230\u4e2d\u95f4\u6ce8\u610f\u529b\u201d\u7684\u53cd\u9988\u9a71\u52a8\u81ea\u9002\u5e94\u673a\u5236 \u3002 1. \u53cd\u9988\u56de\u8def\u8bbe\u8ba1\uff1a \u4f20\u7edf\u65b9\u6cd5\u901a\u5e38\u76f4\u63a5\u4fee\u6539\u4e2d\u95f4\u6ce8\u610f\u529b\uff0c\u4f46\u5b58\u5728\u8bed\u4e49\u4f20\u64ad\u4e0d\u4e00\u81f4\u548c\u7f3a\u4e4f\u4e0e\u6587\u672c\u8868\u793a\u76f4\u63a5\u4ea4\u4e92\u7684\u95ee\u9898\u3002\u672c\u6587\u5219\u521b\u65b0\u6027\u5730\u5229\u7528\u6a21\u578b\u7684 \u6700\u7ec8\u8f93\u51fa\u9884\u6d4b \u4f5c\u4e3a\u201c\u66f4\u5f3a\u7684\u7a7a\u95f4\u4e00\u81f4\u6027\u5148\u9a8c\u201d\uff08stronger spatial coherence prior\uff09\u3002\u8fd9\u4e9b\u8f93\u51fa\u88ab\u8ba4\u4e3a\u662f\u6a21\u578b\u5904\u7406\u7684\u201c\u9ad8\u6f6e\u201d\uff0c\u5c01\u88c5\u4e86\u6700\u5168\u9762\u7684\u89c6\u89c9\u548c\u6587\u672c\u8bed\u4e49\u3002 2. \u81ea\u9002\u5e94\u8c03\u6574\uff1a \u5c06\u8fd9\u4e9b\u8f93\u51fa\u5c42\u9762\u7684\u8865\u4e01\u7ea7\u5bf9\u5e94\u5173\u7cfb\uff08patch-level correspondences\uff09\u53cd\u9988\u5e76\u81ea\u9002\u5e94\u5730\u8c03\u6574\u4e2d\u95f4\u6ce8\u610f\u529b\uff0c\u4ece\u800c\u5728\u5185\u90e8\u8868\u793a\u548c\u6700\u7ec8\u9884\u6d4b\u4e4b\u95f4\u5efa\u7acb\u66f4\u5f3a\u7684\u8bed\u4e49\u4e00\u81f4\u6027\u3002 3. \u8bad\u7ec3\u65e0\u5173\u6027\u4e0e\u5373\u63d2\u5373\u7528\uff1a \u6574\u4e2a\u6846\u67b6\u662f\u201c\u8bad\u7ec3\u65e0\u5173\u201d\uff08training-free\uff09\u7684\uff0c\u8fd9\u610f\u5473\u7740\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3CLIP\u6a21\u578b\uff0c\u53ef\u4ee5\u76f4\u63a5\u4f5c\u4e3a\u201c\u5373\u63d2\u5373\u7528\u201d\u6a21\u5757\u96c6\u6210\u5230\u73b0\u6709\u65b9\u6cd5\u4e2d\uff0c\u5927\u5927\u964d\u4f4e\u4e86\u5e94\u7528\u6210\u672c\u548c\u590d\u6742\u6027\u3002 4. \u6838\u5fc3\u6a21\u5757\uff1a \u4e3a\u5b9e\u73b0\u8fd9\u4e00\u53cd\u9988\u673a\u5236\uff0c\u8bba\u6587\u8bbe\u8ba1\u4e86\u5173\u952e\u6a21\u5757\uff0c\u5305\u62ec \u6ce8\u610f\u529b\u9694\u79bb\uff08attention isolation\uff09 \u3001\u7528\u4e8e\u7a00\u758f\u81ea\u9002\u5e94\u7684 \u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u526a\u679d\uff08confidence-based pruning for sparse adaptation\uff09 \u4ee5\u53ca \u81ea\u9002\u5e94\u96c6\u6210\uff08adaptation ensemble\uff09 \u3002 3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd (Potential Impact on the Field) \u63d0\u5347CLIP\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u5b9e\u7528\u6027\uff1a \u663e\u8457\u589e\u5f3a\u4e86CLIP\u5728\u5f00\u653e\u8bcd\u6c47\u5206\u5272\u8fd9\u4e00\u5173\u952e\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u5b9a\u4f4d\u80fd\u529b\uff0c\u4f7f\u5176\u5728\u9700\u8981\u7cbe\u7ec6\u89c6\u89c9\u7406\u89e3\u7684\u5e94\u7528\u4e2d\u66f4\u5177\u7ade\u4e89\u529b\u3002 \u5f00\u521b\u65b0\u7684\u6a21\u578b\u6539\u8fdb\u8303\u5f0f\uff1a \u201c\u8f93\u51fa\u5230\u5185\u90e8\u8868\u793a\u201d\u7684\u53cd\u9988\u673a\u5236\u53ef\u80fd\u4e3a\u6539\u8fdb\u5176\u4ed6\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\uff08\u4e0d\u4ec5\u9650\u4e8eCLIP\uff09\u5728\u7279\u5b9a\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u63d0\u4f9b\u65b0\u7684\u601d\u8def\uff0c\u5c24\u5176\u662f\u5728\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7684\u573a\u666f\u4e0b\u3002 \u964d\u4f4e\u7814\u7a76\u548c\u5e94\u7528\u6210\u672c\uff1a \u201c\u8bad\u7ec3\u65e0\u5173\u201d\u548c\u201c\u5373\u63d2\u5373\u7528\u201d\u7684\u7279\u6027\u610f\u5473\u7740\u7814\u7a76\u4eba\u5458\u548c\u5f00\u53d1\u8005\u53ef\u4ee5\u4ee5\u8f83\u4f4e\u7684\u8ba1\u7b97\u6210\u672c\u548c\u65f6\u95f4\u6295\u5165\uff0c\u5feb\u901f\u63d0\u5347\u73b0\u6709\u6a21\u578b\u7684\u6027\u80fd\uff0c\u52a0\u901f\u4e86\u6280\u672f\u4ece\u7814\u7a76\u5230\u5e94\u7528\u7684\u8f6c\u5316\u3002 \u63a8\u52a8\u5f00\u653e\u8bcd\u6c47\u5b66\u4e60\u53d1\u5c55\uff1a \u63d0\u9ad8\u4e86\u6a21\u578b\u5904\u7406\u672a\u77e5\u7c7b\u522b\u5bf9\u8c61\u7684\u80fd\u529b\uff0c\u5bf9\u4e8e\u96f6\u6837\u672c\uff08zero-shot\uff09\u548c\u5c11\u6837\u672c\uff08few-shot\uff09\u5b66\u4e60\u7b49\u9886\u57df\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002 4. \u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 (Related Areas or Applications) \u5f00\u653e\u8bcd\u6c47\u5206\u5272\u4e0e\u68c0\u6d4b\uff1a \u76f4\u63a5\u53d7\u76ca\u9886\u57df\uff0c\u80fd\u591f\u66f4\u51c6\u786e\u5730\u8bc6\u522b\u548c\u5206\u5272\u56fe\u50cf\u4e2d\u4efb\u610f\u6587\u672c\u63cf\u8ff0\u7684\u5bf9\u8c61\u3002 \u96f6\u6837\u672c/\u5c11\u6837\u672c\u5b66\u4e60\uff1a \u589e\u5f3a\u4e86\u6a21\u578b\u5728\u9762\u5bf9\u672a\u89c1\u8fc7\u7c7b\u522b\u65f6\u7684\u6cdb\u5316\u80fd\u529b\u548c\u5b9a\u4f4d\u7cbe\u5ea6\u3002 \u56fe\u50cf\u7f16\u8f91\u4e0e\u5185\u5bb9\u751f\u6210\uff1a \u5141\u8bb8\u7528\u6237\u901a\u8fc7\u6587\u672c\u6307\u4ee4\u66f4\u7cbe\u786e\u5730\u9009\u62e9\u548c\u4fee\u6539\u56fe\u50cf\u4e2d\u7684\u7279\u5b9a\u533a\u57df\u3002 \u673a\u5668\u4eba\u4e0e\u81ea\u52a8\u9a7e\u9a76\uff1a \u63d0\u5347\u4e86\u7cfb\u7edf\u5bf9\u73af\u5883\u4e2d\u672a\u77e5\u7269\u4f53\u6216\u573a\u666f\u7684\u7406\u89e3\u548c\u4ea4\u4e92\u80fd\u529b\uff0c\u4f8b\u5982\u8bc6\u522b\u548c\u907f\u5f00\u65b0\u51fa\u73b0\u7684\u969c\u788d\u7269\u3002 \u533b\u5b66\u5f71\u50cf\u5206\u6790\uff1a \u8f85\u52a9\u533b\u751f\u6839\u636e\u6587\u672c\u63cf\u8ff0\uff0c\u66f4\u51c6\u786e\u5730\u5b9a\u4f4d\u548c\u5206\u5272\u75c5\u53d8\u533a\u57df\u6216\u7279\u5b9a\u7ec4\u7ec7\u7ed3\u6784\u3002 \u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u4e0e\u56fe\u50cf\u68c0\u7d22\uff1a \u66f4\u7cbe\u7ec6\u7684\u89c6\u89c9-\u6587\u672c\u5bf9\u9f50\u6709\u52a9\u4e8e\u63d0\u5347\u5bf9\u590d\u6742\u95ee\u9898\u7684\u7406\u89e3\u548c\u66f4\u51c6\u786e\u7684\u56fe\u50cf\u5185\u5bb9\u68c0\u7d22\u3002 5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u7684\u5c40\u9650\u6027 (Limitations Inferred from the Abstract) \u63a8\u7406\u65f6\u95f4\u5f00\u9500\uff1a \u5c3d\u7ba1\u662f\u201c\u8bad\u7ec3\u65e0\u5173\u201d\u4e14\u4f7f\u7528\u4e86\u201c\u7a00\u758f\u81ea\u9002\u5e94\u201d\uff0c\u4f46\u5f15\u5165\u53cd\u9988\u5faa\u73af\u548c\u989d\u5916\u7684\u6a21\u5757\uff08\u6ce8\u610f\u529b\u9694\u79bb\u3001\u526a\u679d\u3001\u96c6\u6210\uff09\u53ef\u80fd\u4f1a\u589e\u52a0\u6a21\u578b\u7684\u63a8\u7406\u65f6\u95f4\uff08inference latency\uff09\uff0c\u8fd9\u5bf9\u4e8e\u5b9e\u65f6\u5e94\u7528\u53ef\u80fd\u662f\u4e00\u4e2a\u8003\u91cf\u3002 \u5bf9\u521d\u59cb\u8f93\u51fa\u8d28\u91cf\u7684\u4f9d\u8d56\uff1a \u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u4f9d\u8d56\u4e8e\u6a21\u578b\u521d\u59cb\u8f93\u51fa\u9884\u6d4b\u7684\u8d28\u91cf\u3002\u5982\u679c\u521d\u59cb\u8f93\u51fa\u975e\u5e38\u5dee\uff0c\u53cd\u9988\u673a\u5236\u80fd\u5426\u6709\u6548\u5f15\u5bfc\u6539\u8fdb\u53ef\u80fd\u5b58\u5728\u7591\u95ee\u3002\u6458\u8981\u4e2d\u63d0\u5230\u8f93\u51fa\u201c\u5c01\u88c5\u4e86\u6700\u5168\u9762\u7684\u8bed\u4e49\u201d\uff0c\u6697\u793a\u5176\u8d28\u91cf\u8db3\u591f\u9ad8\uff0c\u4f46\u6781\u7aef\u60c5\u51b5\u4ecd\u9700\u9a8c\u8bc1\u3002 \u67b6\u6784\u7279\u5f02\u6027\uff1a \u8be5\u65b9\u6cd5\u662f\u4e3aCLIP\u7684Transformer\u67b6\u6784\u53ca\u5176\u6ce8\u610f\u529b\u673a\u5236\u8bbe\u8ba1\u7684\u3002\u5176\u5728\u5176\u4ed6\u975eTransformer\u6216\u4e0d\u540c\u6ce8\u610f\u529b\u673a\u5236\u7684\u89c6\u89c9\u6a21\u578b\u4e0a\u7684\u666e\u9002\u6027\u6216\u6709\u6548\u6027\u672a\u5728\u6458\u8981\u4e2d\u63d0\u53ca\u3002 \u201c\u65e0\u7f1d\u96c6\u6210\u201d\u7684\u5b9e\u9645\u590d\u6742\u6027\uff1a \u5c3d\u7ba1\u58f0\u79f0\u201c\u65e0\u7f1d\u96c6\u6210\u201d\uff0c\u4f46\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\uff0c\u4efb\u4f55\u5373\u63d2\u5373\u7528\u6a21\u5757\u90fd\u53ef\u80fd\u9700\u8981\u4e00\u5b9a\u7684\u5de5\u7a0b\u9002\u914d\u548c\u8c03\u8bd5\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u591a\u79cdSOTA\u65b9\u6cd5\u548c\u4e0d\u540c\u9aa8\u5e72\u7f51\u7edc\u65f6\u3002 Key Findings: In this work, we propose a training-free, feedback-driven self-adaptive framework that adapts output-based patch-level correspondences back to the intermediate attention. Our approach enhances semantic consistency between internal representations and final predictions by leveraging the model's outputs as a stronger spatial coherence prior. Our method functions as a plug-in module, seamlessly integrating into four state-of-the-art approaches with three backbones (ViT-B, ViT-L, ViT-H). Our approach consistently improves their performance across eight benchmarks. Links: PDF arXiv Bridging Domain Gaps for Fine-Grained Moth Classification Through Expert-Informed Adaptation and Foundation Model Priors Authors: Ross J Gardiner, Guillaume Mougeot, Sareh Rowlands, Benno I Simmons, Flemming Helsing, Toke Thomas H\u00f8ye Published: 2025-08-27 Categories: cs.CV Abstract: Labelling images of Lepidoptera (moths) from automated camera systems is vital for understanding insect declines. However, accurate species identification is challenging due to domain shifts between curated images and noisy field imagery. We propose a lightweight classification approach, combining limited expert-labelled field data with knowledge distillation from the high-performance BioCLIP2 foundation model into a ConvNeXt-tiny architecture. Experiments on 101 Danish moth species from AMI camera systems demonstrate that BioCLIP2 substantially outperforms other methods and that our distilled lightweight model achieves comparable accuracy with significantly reduced computational cost. These insights offer practical guidelines for the development of efficient insect monitoring systems and bridging domain gaps for fine-grained classification. Analysis: \u8fd9\u7bc7\u8bba\u6587\u7684\u6458\u8981\u5c55\u793a\u4e86\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u5728\u89e3\u51b3\u5b9e\u9645\u751f\u7269\u591a\u6837\u6027\u76d1\u6d4b\u6311\u6218\u65b9\u9762\u7684\u6700\u65b0\u8fdb\u5c55\u3002\u4ee5\u4e0b\u662f\u8be6\u7ec6\u5206\u6790\uff1a 1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u6d01\u603b\u7ed3 (2-3 \u53e5\u8bdd) \u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u5206\u7c7b\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u9ad8\u6027\u80fd\u7684BioCLIP2\u57fa\u7840\u6a21\u578b\u77e5\u8bc6\u84b8\u998f\u5230ConvNeXt-tiny\u67b6\u6784\u4e2d\uff0c\u5e76\u7ed3\u5408\u6709\u9650\u7684\u4e13\u5bb6\u6807\u6ce8\u91ce\u5916\u6570\u636e\uff0c\u4ee5\u89e3\u51b3\u81ea\u52a8\u76f8\u673a\u7cfb\u7edf\u6355\u83b7\u7684\u98de\u86fe\u56fe\u50cf\u5728\u7cbe\u7ec6\u7c92\u5ea6\u5206\u7c7b\u4e2d\u5b58\u5728\u7684\u9886\u57df\u6f02\u79fb\u95ee\u9898\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u4e0eBioCLIP2\u76f8\u5f53\u7684\u5206\u7c7b\u7cbe\u5ea6\uff0c\u4e3a\u9ad8\u6548\u7684\u6606\u866b\u76d1\u6d4b\u7cfb\u7edf\u548c\u8de8\u9886\u57df\u7cbe\u7ec6\u7c92\u5ea6\u5206\u7c7b\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002 2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u5b66\u65b9\u6cd5 \u8be5\u7814\u7a76\u7684\u5173\u952e\u521b\u65b0\u5728\u4e8e\u5176 \u534f\u540c\u7ec4\u5408 \u7684\u65b9\u6cd5\u8bba\uff0c\u6709\u6548\u5730\u878d\u5408\u4e86\u4ee5\u4e0b\u51e0\u4e2a\u6838\u5fc3\u601d\u60f3\uff1a \u5229\u7528\u5f3a\u5927\u7684\u57fa\u7840\u6a21\u578b\u5148\u9a8c\u77e5\u8bc6 (Foundation Model Priors): \u5f15\u5165\u4e86\u9ad8\u6027\u80fd\u7684BioCLIP2\u57fa\u7840\u6a21\u578b\u4f5c\u4e3a\u201c\u6559\u5e08\u201d\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u53ef\u80fd\u5728\u5927\u89c4\u6a21\u751f\u7269\u56fe\u50cf\u548c\u6587\u672c\u6570\u636e\u4e0a\u8fdb\u884c\u4e86\u9884\u8bad\u7ec3\uff0c\u4ece\u800c\u5177\u5907\u4e86\u5f3a\u5927\u7684\u7279\u5f81\u63d0\u53d6\u548c\u6cdb\u5316\u80fd\u529b\u3002 \u77e5\u8bc6\u84b8\u998f (Knowledge Distillation) \u5b9e\u73b0\u6a21\u578b\u8f7b\u91cf\u5316: \u5c06BioCLIP2\u7684\u4e30\u5bcc\u77e5\u8bc6\u84b8\u998f\u5230\u4e00\u4e2a\u8ba1\u7b97\u6210\u672c\u663e\u8457\u66f4\u4f4e\u7684ConvNeXt-tiny\u201c\u5b66\u751f\u201d\u6a21\u578b\u4e2d\u3002\u8fd9\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u5927\u5e45\u51cf\u5c11\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u90e8\u7f72\u73af\u5883\u3002 \u4e13\u5bb6\u4fe1\u606f\u5f15\u5bfc\u7684\u9886\u57df\u9002\u5e94 (Expert-Informed Adaptation): \u7ed3\u5408\u201c\u6709\u9650\u7684\u4e13\u5bb6\u6807\u6ce8\u91ce\u5916\u6570\u636e\u201d\u6765\u5fae\u8c03\u6216\u6307\u5bfc\u84b8\u998f\u8fc7\u7a0b\uff0c\u76f4\u63a5\u89e3\u51b3\u7b56\u5c55\u56fe\u50cf\u4e0e\u5608\u6742\u91ce\u5916\u56fe\u50cf\u4e4b\u95f4\u7684\u9886\u57df\u6f02\u79fb\u95ee\u9898\u3002\u8fd9\u79cd\u6709\u9488\u5bf9\u6027\u7684\u6570\u636e\u5229\u7528\u662f\u5f25\u5408\u9886\u57df\u5dee\u8ddd\u7684\u5173\u952e\u3002 \u9488\u5bf9\u7cbe\u7ec6\u7c92\u5ea6\u5206\u7c7b\u7684\u5b9e\u7528\u89e3\u51b3\u65b9\u6848: \u9488\u5bf9\u98de\u86fe\u8fd9\u79cd\u5177\u6709\u9ad8\u5ea6\u76f8\u4f3c\u7269\u79cd\u7684\u7cbe\u7ec6\u7c92\u5ea6\u5206\u7c7b\u4efb\u52a1\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5728\u771f\u5b9e\u4e16\u754c\u3001\u5608\u6742\u6570\u636e\u573a\u666f\u4e0b\u65e2\u51c6\u786e\u53c8\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002 3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd \u63a8\u52a8\u9ad8\u6548\u3001\u53ef\u90e8\u7f72\u7684CV\u7cfb\u7edf\u53d1\u5c55: \u8bc1\u660e\u4e86\u901a\u8fc7\u57fa\u7840\u6a21\u578b\u84b8\u998f\u53ef\u4ee5\u6709\u6548\u5730\u5c06\u5927\u578b\u6a21\u578b\u7684\u6027\u80fd\u4f18\u52bf\u8f6c\u79fb\u5230\u5c0f\u578b\u3001\u9ad8\u6548\u7684\u6a21\u578b\u4e0a\uff0c\u8fd9\u5bf9\u4e8e\u8fb9\u7f18\u8ba1\u7b97\u3001\u5b9e\u65f6\u76d1\u6d4b\u548c\u8d44\u6e90\u53d7\u9650\u7684\u5e94\u7528\u573a\u666f\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002 \u4e3a\u9886\u57df\u9002\u5e94\u63d0\u4f9b\u65b0\u8303\u5f0f: \u5f3a\u8c03\u4e86\u7ed3\u5408\u57fa\u7840\u6a21\u578b\u5148\u9a8c\u77e5\u8bc6\u548c\u5c11\u91cf\u76ee\u6807\u9886\u57df\u4e13\u5bb6\u6570\u636e\u8fdb\u884c\u77e5\u8bc6\u84b8\u998f\uff0c\u662f\u89e3\u51b3\u9886\u57df\u6f02\u79fb\u95ee\u9898\u7684\u4e00\u79cd\u5f3a\u5927\u4e14\u5b9e\u7528\u7684\u7b56\u7565\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u6570\u636e\u6807\u6ce8\u6210\u672c\u9ad8\u6602\u7684\u4e13\u4e1a\u9886\u57df\u3002 \u52a0\u901f\u751f\u7269\u591a\u6837\u6027\u76d1\u6d4b\u548c\u751f\u6001\u7814\u7a76: \u4e3a\u81ea\u52a8\u5316\u6606\u866b\uff08\u53ca\u5176\u4ed6\u751f\u7269\uff09\u8bc6\u522b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6280\u672f\u652f\u6491\uff0c\u6709\u52a9\u4e8e\u66f4\u51c6\u786e\u3001\u66f4\u9ad8\u6548\u5730\u76d1\u6d4b\u7269\u79cd\u6570\u91cf\u548c\u5206\u5e03\uff0c\u4ece\u800c\u66f4\u597d\u5730\u7406\u89e3\u548c\u5e94\u5bf9\u751f\u7269\u591a\u6837\u6027\u4e0b\u964d\u95ee\u9898\u3002 \u9a8c\u8bc1\u57fa\u7840\u6a21\u578b\u5728\u7279\u5b9a\u9886\u57df\u7684\u4ef7\u503c: \u8fdb\u4e00\u6b65\u5de9\u56fa\u4e86\u50cfCLIP\u8fd9\u7c7b\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u5728\u7279\u5b9a\u3001\u4e13\u4e1a\u9886\u57df\uff08\u5982\u751f\u7269\u56fe\u50cf\u5206\u6790\uff09\u7684\u5f3a\u5927\u8fc1\u79fb\u5b66\u4e60\u80fd\u529b\u548c\u4f5c\u4e3a\u5f3a\u5927\u7279\u5f81\u63d0\u53d6\u5668\u7684\u6f5c\u529b\u3002 4. \u53ef\u80fd\u53d7\u76ca\u4e8e\u6b64\u7814\u7a76\u7684\u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 \u5176\u4ed6\u7cbe\u7ec6\u7c92\u5ea6\u5206\u7c7b\u4efb\u52a1: \u4f8b\u5982\uff0c\u9e1f\u7c7b\u7269\u79cd\u8bc6\u522b\u3001\u690d\u7269\u75c5\u5bb3\u8bca\u65ad\u3001\u533b\u5b66\u5f71\u50cf\u4e2d\u5fae\u5c0f\u75c5\u7076\u5206\u7c7b\u3001\u5de5\u4e1a\u4ea7\u54c1\u7f3a\u9677\u68c0\u6d4b\u7b49\uff0c\u8fd9\u4e9b\u4efb\u52a1\u901a\u5e38\u9762\u4e34\u6570\u636e\u7a00\u7f3a\u3001\u9886\u57df\u6f02\u79fb\u548c\u5bf9\u9ad8\u7cbe\u5ea6\u8981\u6c42\u7684\u95ee\u9898\u3002 \u751f\u6001\u5b66\u548c\u751f\u7269\u591a\u6837\u6027\u76d1\u6d4b: \u9664\u4e86\u98de\u86fe\uff0c\u8be5\u65b9\u6cd5\u53ef\u63a8\u5e7f\u5230\u5176\u4ed6\u6606\u866b\u3001\u9c7c\u7c7b\u3001\u4e24\u6816\u52a8\u7269\u3001\u690d\u7269\u7b49\u7269\u79cd\u7684\u81ea\u52a8\u5316\u8bc6\u522b\u548c\u8ba1\u6570\uff0c\u5c24\u5176\u662f\u5728\u91ce\u5916\u590d\u6742\u73af\u5883\u4e0b\u3002 \u519c\u4e1a\u79d1\u6280: \u519c\u4f5c\u7269\u75c5\u866b\u5bb3\u7684\u65e9\u671f\u9884\u8b66\u548c\u8bc6\u522b\uff0c\u901a\u8fc7\u90e8\u7f72\u8f7b\u91cf\u7ea7\u6a21\u578b\u5728\u7530\u95f4\u8bbe\u5907\u4e0a\u8fdb\u884c\u5b9e\u65f6\u76d1\u6d4b\u3002 \u9065\u611f\u548c\u73af\u5883\u76d1\u6d4b: \u4ece\u536b\u661f\u6216\u65e0\u4eba\u673a\u56fe\u50cf\u4e2d\u8bc6\u522b\u7279\u5b9a\u5730\u7269\u3001\u690d\u88ab\u7c7b\u578b\u6216\u73af\u5883\u53d8\u5316\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u5feb\u901f\u5904\u7406\u548c\u90e8\u7f72\u7684\u573a\u666f\u3002 \u533b\u7597\u5f71\u50cf\u5206\u6790: \u5c06\u5728\u5927\u578b\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u9884\u8bad\u7ec3\u7684\u6a21\u578b\u9002\u5e94\u5230\u7279\u5b9a\u533b\u9662\u6216\u8bbe\u5907\u4ea7\u751f\u7684\u4e34\u5e8a\u6570\u636e\u4e0a\uff0c\u4ee5\u8f85\u52a9\u8bca\u65ad\uff0c\u5c24\u5176\u662f\u5728\u7f55\u89c1\u75be\u75c5\u6216\u9690\u79c1\u654f\u611f\u6570\u636e\u53d7\u9650\u7684\u60c5\u51b5\u4e0b\u3002 \u5de5\u4e1a\u81ea\u52a8\u5316\u548c\u8d28\u91cf\u63a7\u5236: \u5728\u751f\u4ea7\u7ebf\u4e0a\u90e8\u7f72\u8f7b\u91cf\u7ea7\u6a21\u578b\u8fdb\u884c\u5b9e\u65f6\u4ea7\u54c1\u8d28\u91cf\u68c0\u6d4b\uff0c\u8bc6\u522b\u5fae\u5c0f\u7f3a\u9677\u3002 5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u7684\u5c40\u9650\u6027 \u901a\u7528\u6027/\u6cdb\u5316\u80fd\u529b: \u5b9e\u9a8c\u4ec5\u5728\u201c101\u79cd\u4e39\u9ea6\u98de\u86fe\u201d\u548c\u201cAMI\u76f8\u673a\u7cfb\u7edf\u201d\u4e0a\u8fdb\u884c\u3002\u8be5\u65b9\u6cd5\u5728\u66f4\u5e7f\u6cdb\u7684\u98de\u86fe\u7269\u79cd\u3001\u4e0d\u540c\u5730\u7406\u533a\u57df\u3001\u4e0d\u540c\u6c14\u5019\u6761\u4ef6\u6216\u4e0d\u540c\u7c7b\u578b\u76f8\u673a\u7cfb\u7edf\uff08\u5206\u8fa8\u7387\u3001\u5149\u7167\u3001\u566a\u58f0\u7279\u6027\uff09\u4e0b\u7684\u8868\u73b0\u4ecd\u9700\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u3002 \u201c\u6709\u9650\u4e13\u5bb6\u6807\u6ce8\u91ce\u5916\u6570\u636e\u201d\u7684\u6570\u91cf\u672a\u660e\u786e: \u6458\u8981\u672a\u5177\u4f53\u8bf4\u660e\u201c\u6709\u9650\u201d\u6570\u636e\u7684\u91cf\u7ea7\u3002\u8be5\u65b9\u6cd5\u5bf9\u6570\u636e\u91cf\u7684\u654f\u611f\u6027\u5982\u4f55\uff1f\u5728\u6570\u636e\u6781\u5ea6\u7a00\u7f3a\uff08\u4f8b\u5982\uff0c\u53ea\u6709\u51e0\u5341\u5f20\u56fe\u50cf\uff09\u7684\u60c5\u51b5\u4e0b\u662f\u5426\u4f9d\u7136\u6709\u6548\uff1f \u5bf9\u7279\u5b9a\u57fa\u7840\u6a21\u578b\u7684\u4f9d\u8d56: \u6a21\u578b\u7684\u6027\u80fd\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4f9d\u8d56\u4e8eBioCLIP2\u7684\u5f3a\u5927\u80fd\u529b\u3002\u5982\u679cBioCLIP2\u672c\u8eab\u5b58\u5728\u5c40\u9650\u6027\uff0c\u6216\u8005\u672a\u6765\u51fa\u73b0\u66f4\u4f18\u7684\u57fa\u7840\u6a21\u578b\uff0c\u5219\u9700\u8981\u91cd\u65b0\u8bc4\u4f30\u6216\u66f4\u65b0\u3002 \u77e5\u8bc6\u84b8\u998f\u7684\u56fa\u6709\u5c40\u9650: \u77e5\u8bc6\u84b8\u998f\u901a\u5e38\u610f\u5473\u7740\u5b66\u751f\u6a21\u578b\u7684\u6027\u80fd\u4e0a\u9650\u7531\u6559\u5e08\u6a21\u578b\u51b3\u5b9a\uff0c\u5b66\u751f\u6a21\u578b\u5f88\u96be\u8d85\u8d8a\u6559\u5e08\u6a21\u578b\u3002\u8fd9\u610f\u5473\u7740BioCLIP2\u7684\u4efb\u4f55\u56fa\u6709\u504f\u5dee\u6216\u9519\u8bef\u90fd\u53ef\u80fd\u88ab\u4f20\u9012\u7ed9ConvNeXt-tiny\u3002 \u73af\u5883\u9c81\u68d2\u6027\u672a\u8be6\u8ff0: \u6458\u8981\u672a\u63d0\u53ca\u6a21\u578b\u5728\u6781\u7aef\u91ce\u5916\u73af\u5883\u6761\u4ef6\uff08\u5982\u6076\u52a3\u5929\u6c14\u3001\u5149\u7167\u5267\u70c8\u53d8\u5316\u3001\u906e\u6321\u3001\u90e8\u5206\u53ef\u89c1\u7b49\uff09\u4e0b\u7684\u9c81\u68d2\u6027\u8868\u73b0\u3002 \u8ba1\u7b97\u6210\u672c\u964d\u4f4e\u7684\u5177\u4f53\u91cf\u5316: \u867d\u7136\u63d0\u5230\u4e86\u201c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u201d\uff0c\u4f46\u6ca1\u6709\u7ed9\u51fa\u5177\u4f53\u7684\u91cf\u5316\u6307\u6807\uff08\u5982FLOPs\u3001\u53c2\u6570\u91cf\u3001\u63a8\u7406\u65f6\u95f4\u7b49\uff09\uff0c\u8fd9\u4f7f\u5f97\u96be\u4ee5\u76f4\u63a5\u6bd4\u8f83\u5176\u6548\u7387\u63d0\u5347\u7684\u7a0b\u5ea6\u3002 Key Findings: We propose a lightweight classification approach, combining limited expert-labelled field data with knowledge distillation from the high-performance BioCLIP2 foundation model into a ConvNeXt-tiny architecture. Experiments on 101 Danish moth species from AMI camera systems demonstrate that BioCLIP2 substantially outperforms other methods and that our distilled lightweight model achieves comparable accuracy with significantly reduced computational cost. Links: PDF arXiv OpenM3D: Open Vocabulary Multi-view Indoor 3D Object Detection without Human Annotations Authors: Peng-Hao Hsu, Ke Zhang, Fu-En Wang, Tao Tu, Ming-Feng Li, Yu-Lun Liu, Albert Y. C. Chen, Min Sun, Cheng-Hao Kuo Published: 2025-08-27 Categories: cs.CV Abstract: Open-vocabulary (OV) 3D object detection is an emerging field, yet its exploration through image-based methods remains limited compared to 3D point cloud-based methods. We introduce OpenM3D, a novel open-vocabulary multi-view indoor 3D object detector trained without human annotations. In particular, OpenM3D is a single-stage detector adapting the 2D-induced voxel features from the ImGeoNet model. To support OV, it is jointly trained with a class-agnostic 3D localization loss requiring high-quality 3D pseudo boxes and a voxel-semantic alignment loss requiring diverse pre-trained CLIP features. We follow the training setting of OV-3DET where posed RGB-D images are given but no human annotations of 3D boxes or classes are available. We propose a 3D Pseudo Box Generation method using a graph embedding technique that combines 2D segments into coherent 3D structures. Our pseudo-boxes achieve higher precision and recall than other methods, including the method proposed in OV-3DET. We further sample diverse CLIP features from 2D segments associated with each coherent 3D structure to align with the corresponding voxel feature. The key to training a highly accurate single-stage detector requires both losses to be learned toward high-quality targets. At inference, OpenM3D, a highly efficient detector, requires only multi-view images for input and demonstrates superior accuracy and speed (0.3 sec. per scene) on ScanNet200 and ARKitScenes indoor benchmarks compared to existing methods. We outperform a strong two-stage method that leverages our class-agnostic detector with a ViT CLIP-based OV classifier and a baseline incorporating multi-view depth estimator on both accuracy and speed. Analysis: \u8fd9\u7bc7\u8bba\u6587\u300aOpenM3D: Open Vocabulary Multi-view Indoor 3D Object Detection without Human Annotations\u300b\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\uff0c\u7279\u522b\u662f3D\u76ee\u6807\u68c0\u6d4b\u65b9\u9762\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u5f15\u4eba\u6ce8\u76ee\u7684\u65b0\u65b9\u6cd5\u3002\u4ee5\u4e0b\u662f\u8be6\u7ec6\u5206\u6790\uff1a 1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u6d01\u603b\u7ed3 (2-3\u53e5\u8bdd) OpenM3D\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5355\u9636\u6bb5\u3001\u5f00\u653e\u8bcd\u6c47\u3001\u591a\u89c6\u89d2\u5ba4\u51853D\u76ee\u6807\u68c0\u6d4b\u5668\uff0c\u5176\u6838\u5fc3\u521b\u65b0\u5728\u4e8e \u65e0\u9700\u4eba\u5de5\u6807\u6ce8 \u5373\u53ef\u8fdb\u884c\u8bad\u7ec3\u3002\u5b83\u901a\u8fc7\u7ed3\u5408\u9ad8\u8d28\u91cf\u76843D\u4f2a\u6846\u751f\u6210\u65b9\u6cd5\uff08\u57fa\u4e8e\u56fe\u5d4c\u5165\uff09\u548c\u5229\u7528\u591a\u6837\u5316CLIP\u7279\u5f81\u7684\u4f53\u7d20\u8bed\u4e49\u5bf9\u9f50\u635f\u5931\uff0c\u5b9e\u73b0\u4e86\u5728\u5ba4\u5185\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u7684\u51c6\u786e\u6027\u548c\u901f\u5ea6\u3002\u8fd9\u9879\u5de5\u4f5c\u663e\u8457\u964d\u4f4e\u4e863D\u68c0\u6d4b\u7684\u6807\u6ce8\u6210\u672c\uff0c\u5e76\u63a8\u52a8\u4e86\u56fe\u50cf\u57fa3D\u5f00\u653e\u8bcd\u6c47\u68c0\u6d4b\u7684\u53d1\u5c55\u3002 2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u8bba OpenM3D\u7684\u5173\u952e\u521b\u65b0\u5728\u4e8e\u5176 \u65e0\u4eba\u5de5\u6807\u6ce8\u7684\u8bad\u7ec3\u8303\u5f0f \u4ee5\u53ca\u5b9e\u73b0\u8fd9\u4e00\u8303\u5f0f\u6240\u91c7\u7528\u7684\u72ec\u7279\u65b9\u6cd5\uff1a \u65e0\u4eba\u5de5\u6807\u6ce8\u7684\u8bad\u7ec3\uff1a \u8fd9\u662f\u6700\u6838\u5fc3\u7684\u521b\u65b0\u70b9\u3002\u8bba\u6587\u660e\u786e\u6307\u51fa\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u4e0d\u4f7f\u7528\u4efb\u4f55\u4eba\u5de5\u6807\u6ce8\u76843D\u8fb9\u754c\u6846\u6216\u7c7b\u522b\u4fe1\u606f\u3002\u8fd9\u6781\u5927\u5730\u964d\u4f4e\u4e863D\u68c0\u6d4b\u7684\u6807\u6ce8\u6210\u672c\uff0c\u662f\u8be5\u9886\u57df\u7684\u4e00\u4e2a\u91cd\u8981\u7a81\u7834\u3002 \u9ad8\u8d28\u91cf3D\u4f2a\u6846\u751f\u6210\uff1a \u4e3a\u4e86\u5f25\u8865\u7f3a\u4e4f\u4eba\u5de5\u6807\u6ce8\u76843D\u8fb9\u754c\u6846\uff0cOpenM3D\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u76843D\u4f2a\u6846\u751f\u6210\u65b9\u6cd5\u3002\u5b83\u5229\u7528 \u56fe\u5d4c\u5165\u6280\u672f \u5c062D\u56fe\u50cf\u4e2d\u7684\u5206\u5272\u7ed3\u679c\u7ec4\u5408\u6210\u8fde\u8d2f\u76843D\u7ed3\u6784\uff0c\u4ece\u800c\u751f\u6210\u9ad8\u8d28\u91cf\u76843D\u4f2a\u6846\u3002\u8fd9\u4e9b\u4f2a\u6846\u7528\u4e8e\u8bad\u7ec3\u7c7b\u4e0d\u53ef\u77e5\u76843D\u5b9a\u4f4d\u635f\u5931\u3002 \u4f53\u7d20\u8bed\u4e49\u5bf9\u9f50\u635f\u5931\u4e0e\u591a\u6837\u5316CLIP\u7279\u5f81\uff1a \u4e3a\u4e86\u5b9e\u73b0\u5f00\u653e\u8bcd\u6c47\u80fd\u529b\uff0cOpenM3D\u5f15\u5165\u4e86\u4f53\u7d20\u8bed\u4e49\u5bf9\u9f50\u635f\u5931\u3002\u5b83\u4ece\u4e0e\u6bcf\u4e2a\u8fde\u8d2f3D\u7ed3\u6784\u76f8\u5173\u76842D\u5206\u5272\u4e2d\u91c7\u6837 \u591a\u6837\u5316\u7684\u9884\u8bad\u7ec3CLIP\u7279\u5f81 \uff0c\u5e76\u5c06\u5176\u4e0e\u5bf9\u5e94\u76843D\u4f53\u7d20\u7279\u5f81\u8fdb\u884c\u5bf9\u9f50\u3002\u8fd9\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u7406\u89e3\u548c\u8bc6\u522b\u8bad\u7ec3\u4e2d\u672a\u89c1\u7684\u7269\u4f53\u7c7b\u522b\u3002 \u5355\u9636\u6bb5\u9ad8\u6548\u68c0\u6d4b\u5668\uff1a OpenM3D\u662f\u4e00\u4e2a\u5355\u9636\u6bb5\u68c0\u6d4b\u5668\uff0c\u5b83\u5229\u7528ImGeoNet\u6a21\u578b\u5bfc\u51fa\u76842D\u8bf1\u5bfc\u4f53\u7d20\u7279\u5f81\u3002\u7ed3\u5408\u4e0a\u8ff0\u4e24\u79cd\u635f\u5931\uff0c\u5b83\u5728\u63a8\u7406\u65f6\u4ec5\u9700\u591a\u89c6\u89d2\u56fe\u50cf\u8f93\u5165\uff0c\u4fbf\u80fd\u5b9e\u73b0\u9ad8\u6548\u7387\uff080.3\u79d2/\u573a\u666f\uff09\u548c\u9ad8\u7cbe\u5ea6\u3002 3. \u5bf9\u8be5\u9886\u57df\u7684\u6f5c\u5728\u5f71\u54cd \u964d\u4f4e\u6807\u6ce8\u6210\u672c\uff0c\u52a0\u901f\u7814\u7a76\u4e0e\u5e94\u7528\uff1a 3D\u76ee\u6807\u68c0\u6d4b\uff0c\u7279\u522b\u662f\u5ba4\u5185\u573a\u666f\uff0c\u5176\u6570\u636e\u6807\u6ce8\u6210\u672c\u6781\u9ad8\u3002OpenM3D\u7684\u65e0\u4eba\u5de5\u6807\u6ce8\u8bad\u7ec3\u8303\u5f0f\u5c06\u6781\u5927\u5730\u964d\u4f4e\u8fd9\u4e00\u95e8\u69db\uff0c\u4f7f\u5f97\u7814\u7a76\u4eba\u5458\u548c\u5f00\u53d1\u8005\u80fd\u591f\u66f4\u5feb\u5730\u63a2\u7d22\u548c\u90e8\u7f723D\u89c6\u89c9\u7cfb\u7edf\uff0c\u800c\u65e0\u9700\u6295\u5165\u5927\u91cf\u8d44\u6e90\u8fdb\u884c\u6570\u636e\u6807\u6ce8\u3002 \u63a8\u52a8\u5f00\u653e\u8bcd\u6c473D\u68c0\u6d4b\u53d1\u5c55\uff1a \u76ee\u524d\u5f00\u653e\u8bcd\u6c473D\u68c0\u6d4b\u4ecd\u5904\u4e8e\u65e9\u671f\u9636\u6bb5\uff0c\u4e14\u56fe\u50cf\u57fa\u65b9\u6cd5\u76f8\u5bf9\u6ede\u540e\u3002OpenM3D\u7684\u6210\u529f\u8bc1\u660e\u4e86\u56fe\u50cf\u57fa\u65b9\u6cd5\u5728\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u5f00\u653e\u8bcd\u6c473D\u68c0\u6d4b\u7684\u6f5c\u529b\uff0c\u5c06\u6fc0\u52b1\u66f4\u591a\u7814\u7a76\u8005\u6295\u5165\u5230\u8fd9\u4e00\u65b9\u5411\u3002 \u63d0\u5347\u56fe\u50cf\u57fa3D\u68c0\u6d4b\u7684\u7ade\u4e89\u529b\uff1a \u4f20\u7edf\u4e0a\uff0c\u57fa\u4e8e\u70b9\u4e91\u76843D\u68c0\u6d4b\u65b9\u6cd5\u5728\u7cbe\u5ea6\u4e0a\u5f80\u5f80\u4f18\u4e8e\u56fe\u50cf\u57fa\u65b9\u6cd5\u3002OpenM3D\u5c55\u793a\u4e86\u56fe\u50cf\u57fa\u65b9\u6cd5\u5728\u65e0\u76d1\u7763\u8bbe\u7f6e\u4e0b\u4e5f\u80fd\u8fbe\u5230\u5353\u8d8a\u7684\u6027\u80fd\uff0c\u751a\u81f3\u8d85\u8d8a\u4e86\u67d0\u4e9b\u5f3a\u52b2\u7684\u57fa\u7ebf\u548c\u4e24\u9636\u6bb5\u65b9\u6cd5\uff0c\u8fd9\u6709\u52a9\u4e8e\u63d0\u5347\u56fe\u50cf\u57fa3D\u68c0\u6d4b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u7ade\u4e89\u529b\u3002 \u4fc3\u8fdb\u81ea\u76d1\u7763/\u5f31\u76d1\u77633D\u5b66\u4e60\uff1a \u8be5\u8bba\u6587\u4e3a3D\u89c6\u89c9\u9886\u57df\u7684\u81ea\u76d1\u7763\u548c\u5f31\u76d1\u7763\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u548c\u8303\u4f8b\uff0c\u7279\u522b\u662f\u5982\u4f55\u6709\u6548\u5730\u4ece2D\u4fe1\u606f\uff08\u59822D\u5206\u5272\u3001CLIP\u7279\u5f81\uff09\u4e2d\u63d0\u53d63D\u8bed\u4e49\u548c\u51e0\u4f55\u4fe1\u606f\u3002 4. \u53ef\u80fd\u53d7\u76ca\u4e8e\u8fd9\u9879\u7814\u7a76\u7684\u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 \u673a\u5668\u4eba\u5b66\u548c\u81ea\u4e3b\u7cfb\u7edf\uff1a \u673a\u5668\u4eba\u9700\u8981\u5728\u672a\u77e5\u6216\u4e0d\u65ad\u53d8\u5316\u7684\u73af\u5883\u4e2d\u8bc6\u522b\u548c\u64cd\u4f5c\u5404\u79cd\u7269\u4f53\u3002OpenM3D\u7684\u5f00\u653e\u8bcd\u6c47\u548c\u65e0\u6807\u6ce8\u7279\u6027\u4f7f\u5176\u975e\u5e38\u9002\u5408\u673a\u5668\u4eba\u5bfc\u822a\u3001\u6293\u53d6\u548c\u573a\u666f\u7406\u89e3\uff0c\u5c24\u5176\u662f\u5728\u5bb6\u5ead\u3001\u529e\u516c\u5ba4\u7b49\u5ba4\u5185\u73af\u5883\u4e2d\u3002 \u589e\u5f3a\u73b0\u5b9e (AR) / \u865a\u62df\u73b0\u5b9e (VR)\uff1a AR/VR\u5e94\u7528\u9700\u8981\u5b9e\u65f6\u3001\u51c6\u786e\u5730\u8bc6\u522b\u548c\u5b9a\u4f4d\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u7269\u4f53\uff0c\u4ee5\u4fbf\u8fdb\u884c\u865a\u62df\u5185\u5bb9\u7684\u53e0\u52a0\u548c\u4ea4\u4e92\u3002OpenM3D\u53ef\u4ee5\u5e2e\u52a9AR/VR\u7cfb\u7edf\u66f4\u597d\u5730\u7406\u89e3\u7528\u6237\u5468\u56f4\u7684\u73af\u5883\uff0c\u5b9e\u73b0\u66f4\u6c89\u6d78\u5f0f\u7684\u4f53\u9a8c\u3002 \u6570\u5b57\u5b6a\u751f\u548c\u5ba4\u5185\u6d4b\u7ed8\uff1a \u81ea\u52a8\u5316\u5730\u6784\u5efa\u5177\u6709\u8bed\u4e49\u4fe1\u606f\u7684\u5ba4\u51853D\u6a21\u578b\uff08\u6570\u5b57\u5b6a\u751f\uff09\u662f\u8bb8\u591a\u884c\u4e1a\u7684\u9700\u6c42\u3002OpenM3D\u53ef\u4ee5\u7528\u4e8e\u81ea\u52a8\u8bc6\u522b\u548c\u5206\u7c7b\u5ba4\u5185\u7269\u4f53\uff0c\u52a0\u901f3D\u6a21\u578b\u7684\u8bed\u4e49\u6807\u6ce8\u8fc7\u7a0b\u3002 \u667a\u80fd\u5bb6\u5c45\u548c\u7269\u8054\u7f51 (IoT)\uff1a \u667a\u80fd\u8bbe\u5907\u9700\u8981\u7406\u89e3\u5176\u6240\u5904\u7684\u73af\u5883\u548c\u5176\u4e2d\u7684\u7269\u4f53\u3002OpenM3D\u53ef\u4ee5\u8d4b\u80fd\u667a\u80fd\u5bb6\u5c45\u7cfb\u7edf\uff0c\u4f7f\u5176\u80fd\u591f\u8bc6\u522b\u66f4\u591a\u79cd\u7c7b\u7684\u7269\u4f53\uff0c\u4ece\u800c\u63d0\u4f9b\u66f4\u667a\u80fd\u3001\u66f4\u4e2a\u6027\u5316\u7684\u670d\u52a1\u3002 \u5b89\u5168\u76d1\u63a7\uff1a \u5728\u76d1\u63a7\u573a\u666f\u4e2d\uff0c\u8bc6\u522b\u7279\u5b9a\u7269\u4f53\u6216\u5f02\u5e38\u4e8b\u4ef6\u81f3\u5173\u91cd\u8981\u3002\u5f00\u653e\u8bcd\u6c47\u80fd\u529b\u53ef\u4ee5\u5e2e\u52a9\u7cfb\u7edf\u68c0\u6d4b\u9884\u5b9a\u4e49\u7c7b\u522b\u4e4b\u5916\u7684\u6f5c\u5728\u5a01\u80c1\u6216\u611f\u5174\u8da3\u7684\u7269\u4f53\u3002 5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u7684\u5c40\u9650\u6027 \u9886\u57df\u9650\u5236\uff1a \u8bba\u6587\u660e\u786e\u6307\u51fa\u662f\u9488\u5bf9\u201c \u5ba4\u5185 \u201d3D\u76ee\u6807\u68c0\u6d4b\u3002\u8fd9\u610f\u5473\u7740\u8be5\u65b9\u6cd5\u53ef\u80fd\u4e0d\u76f4\u63a5\u9002\u7528\u4e8e\u5ba4\u5916\u573a\u666f\uff0c\u56e0\u4e3a\u5ba4\u5916\u73af\u5883\u7684\u7269\u4f53\u7c7b\u578b\u3001\u5c3a\u5ea6\u3001\u5149\u7167\u6761\u4ef6\u548c\u906e\u6321\u6a21\u5f0f\u4e0e\u5ba4\u5185\u6709\u663e\u8457\u5dee\u5f02\u3002 \u8f93\u5165\u6570\u636e\u8981\u6c42\uff1a \u5c3d\u7ba1\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\uff0c\u4f46\u6a21\u578b\u4ecd\u9700\u8981\u201c \u591a\u89c6\u89d2RGB-D\u56fe\u50cf \u201d\u4f5c\u4e3a\u8f93\u5165\uff0c\u5e76\u4e14\u8fd9\u4e9b\u56fe\u50cf\u662f\u201c \u5df2\u59ff\u6001\u5316 (posed) \u201d\u7684\u3002\u8fd9\u610f\u5473\u7740\u5b83\u4f9d\u8d56\u4e8e\u6df1\u5ea6\u4f20\u611f\u5668\u548c\u51c6\u786e\u7684\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\uff0c\u8fd9\u5728\u67d0\u4e9b\u5e94\u7528\u573a\u666f\u4e2d\u53ef\u80fd\u662f\u4e00\u4e2a\u9650\u5236\uff0c\u4f8b\u5982\u7eafRGB\u8f93\u5165\u6216\u6ca1\u6709\u7cbe\u786e\u59ff\u6001\u4fe1\u606f\u7684\u573a\u666f\u3002 \u5bf9\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u4f9d\u8d56\uff1a \u6a21\u578b\u7684\u6027\u80fd\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4f9d\u8d56\u4e8e\u5176\u6240\u4f7f\u7528\u7684\u9884\u8bad\u7ec3\u6a21\u578b\uff08\u5982ImGeoNet\u7528\u4e8e2D\u8bf1\u5bfc\u4f53\u7d20\u7279\u5f81\uff0cCLIP\u7528\u4e8e\u8bed\u4e49\u7279\u5f81\uff09\u7684\u8d28\u91cf\u548c\u6cdb\u5316\u80fd\u529b\u3002\u5982\u679c\u8fd9\u4e9b\u57fa\u7840\u6a21\u578b\u5b58\u5728\u504f\u5dee\u6216\u5728\u7279\u5b9a\u9886\u57df\u8868\u73b0\u4e0d\u4f73\uff0c\u53ef\u80fd\u4f1a\u5f71\u54cdOpenM3D\u7684\u6700\u7ec8\u6027\u80fd\u3002 \u4f2a\u6807\u7b7e\u8d28\u91cf\u7684\u654f\u611f\u6027\uff1a \u6458\u8981\u4e2d\u5f3a\u8c03\u4e86\u201c\u9ad8\u8d28\u91cf3D\u4f2a\u6846\u201d\u5bf9\u4e8e\u8bad\u7ec3\u51c6\u786e\u7684\u5355\u9636\u6bb5\u68c0\u6d4b\u5668\u7684\u91cd\u8981\u6027\u3002\u8fd9\u610f\u5473\u7740\u4f2a\u6846\u751f\u6210\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\u81f3\u5173\u91cd\u8981\u3002\u5728\u6781\u7aef\u590d\u6742\u3001\u9ad8\u5ea6\u6742\u4e71\u6216\u5305\u542b\u5927\u91cf\u900f\u660e/\u53cd\u5c04\u7269\u4f53\u7684\u5ba4\u5185\u573a\u666f\u4e2d\uff0c\u4f2a\u6846\u7684\u751f\u6210\u8d28\u91cf\u53ef\u80fd\u4f1a\u4e0b\u964d\uff0c\u4ece\u800c\u5f71\u54cd\u6574\u4f53\u6027\u80fd\u3002 \u201c\u5f00\u653e\u8bcd\u6c47\u201d\u7684\u771f\u6b63\u6cdb\u5316\u80fd\u529b\uff1a \u5c3d\u7ba1\u4f7f\u7528\u4e86CLIP\u7279\u5f81\uff0c\u4f46CLIP\u672c\u8eab\u662f\u5728\u5927\u91cf2D\u56fe\u50cf\u4e0a\u8bad\u7ec3\u7684\u3002\u5b83\u5bf93D\u7269\u4f53\u5c5e\u6027\u7684\u7406\u89e3\uff0c\u4ee5\u53ca\u5728\u975e\u5e38\u89c4\u89c6\u89d2\u6216\u9ad8\u5ea6\u906e\u6321\u60c5\u51b5\u4e0b\u7684\u8bc6\u522b\u80fd\u529b\uff0c\u53ef\u80fd\u4ecd\u6709\u5c40\u9650\u6027\u3002\u6a21\u578b\u5bf9\u5b8c\u5168\u65b0\u9896\u3001\u4e0e2D\u8bad\u7ec3\u6570\u636e\u5dee\u5f02\u5f88\u5927\u76843D\u7269\u4f53\u7684\u6cdb\u5316\u80fd\u529b\u4ecd\u9700\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u3002 Key Findings: We introduce OpenM3D, a novel open-vocabulary multi-view indoor 3D object detector trained without human annotations. We propose a 3D Pseudo Box Generation method using a graph embedding technique that combines 2D segments into coherent 3D structures. Links: PDF arXiv","title":"Arxiv Computer Vision Papers - 2025-08-29"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-29/#arxiv-computer-vision-papers-2025-08-29","text":"","title":"Arxiv Computer Vision Papers - 2025-08-29"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-29/#executive-summary","text":"\u597d\u7684\uff0c\u8fd9\u662f\u4e00\u4efd\u9488\u5bf92025\u5e748\u670828\u65e5Arxiv\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u8bba\u6587\u7684\u7b80\u660e\u6267\u884c\u6458\u8981\uff1a \u6267\u884c\u6458\u8981\uff1aArxiv\u8ba1\u7b97\u673a\u89c6\u89c9\u8bba\u6587\u901f\u89c8 (2025\u5e748\u670828\u65e5) \u672c\u6267\u884c\u6458\u8981\u65e8\u5728\u4e3a\u5fd9\u788c\u7684\u7814\u7a76\u4eba\u5458\u63d0\u4f9b2025\u5e748\u670828\u65e5Arxiv\u4e0a\u6700\u65b0\u53d1\u5e03\u768410\u7bc7\u8ba1\u7b97\u673a\u89c6\u89c9\u8bba\u6587\u7684\u5feb\u901f\u6982\u89c8\uff0c\u91cd\u70b9\u5173\u6ce8\u5176\u4e3b\u8981\u8d8b\u52bf\u3001\u521b\u65b0\u70b9\u53ca\u672a\u6765\u65b9\u5411\u3002 1. \u4e3b\u8981\u4e3b\u9898\u548c\u8d8b\u52bf\uff1a \u57fa\u7840\u6a21\u578b (Foundation Models) \u7684\u5e7f\u6cdb\u5e94\u7528\uff1a \u591a\u4e2a\u7814\u7a76\u5229\u7528\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\uff08\u5982Vision Foundation Models, CLIP, DINO\uff09\u4f5c\u4e3a\u5f3a\u5927\u7684\u7279\u5f81\u63d0\u53d6\u5668\u6216\u5148\u9a8c\u77e5\u8bc6\uff0c\u4ee5\u89e3\u51b3\u5404\u79cd\u4e0b\u6e38\u4efb\u52a1\uff0c\u5305\u62ec\u76ee\u6807\u68c0\u6d4b\u3001\u56fe\u50cf\u5206\u5272\u548c\u7ec6\u7c92\u5ea6\u5206\u7c7b\u3002\u8fd9\u8868\u660e\u57fa\u7840\u6a21\u578b\u5df2\u6210\u4e3a\u63a8\u52a8CV\u9886\u57df\u8fdb\u6b65\u7684\u6838\u5fc3\u9a71\u52a8\u529b\u3002 \u4f4e\u8d44\u6e90/\u65e0\u76d1\u7763/\u96f6\u6837\u672c\u5b66\u4e60\uff1a \u663e\u8457\u7684\u8d8b\u52bf\u662f\u51cf\u5c11\u5bf9\u5927\u91cf\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\u3002\u8bba\u6587\u63a2\u7d22\u4e86\u65e0\u76d1\u77633D\u76ee\u6807\u68c0\u6d4b\u3001\u65e0\u6807\u6ce8\u7684\u591a\u89c6\u89d23D\u68c0\u6d4b\u3001\u534a\u76d1\u7763\u5206\u5272\u4ee5\u53ca\u8bad\u7ec3\u65e0\u5173\u7684\u5f00\u653e\u8bcd\u6c47\u5206\u5272\u7b49\u65b9\u6cd5\uff0c\u65e8\u5728\u63d0\u9ad8\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u90e8\u7f72\u6548\u7387\u3002 3D \u89c6\u89c9\u7684\u6301\u7eed\u8fdb\u6b65\uff1a 3D\u76ee\u6807\u68c0\u6d4b\uff0c\u5305\u62ec\u5355\u76ee\u3001\u591a\u89c6\u89d2\u548c\u591a\u6a21\u6001\u878d\u5408\uff08LiDAR-Camera\uff09\u65b9\u6cd5\uff0c\u4ecd\u5728\u79ef\u6781\u53d1\u5c55\uff0c\u5e76\u7740\u91cd\u4e8e\u6cdb\u5316\u80fd\u529b\u548c\u65e0\u76d1\u7763\u5b66\u4e60\u3002 \u56fe\u50cf\u5206\u5272\u7684\u591a\u6837\u5316\u521b\u65b0\uff1a \u5206\u5272\u4efb\u52a1\u901a\u8fc7\u751f\u6210\u6a21\u578b\uff08\u6269\u6563\u6a21\u578b\uff09\u3001\u591a\u6a21\u6001\u539f\u578b\u5bf9\u9f50\u3001\u4ee5\u53ca\u5229\u7528\u57fa\u7840\u6a21\u578b\u7684\u9ad8\u4fdd\u771f\u7279\u5f81\u6216\u81ea\u9002\u5e94\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u5728\u901a\u7528\u3001\u533b\u5b66\u548c\u5f00\u653e\u8bcd\u6c47\u573a\u666f\u4e0b\u7684\u65b0\u7a81\u7834\u3002 \u9886\u57df\u9002\u5e94\u4e0e\u6cdb\u5316\uff1a \u9488\u5bf9\u7279\u5b9a\u9886\u57df\uff08\u5982\u8f66\u8f7d\u5185\u9970\u3001\u75c5\u7406\u56fe\u50cf\u3001\u6606\u866b\u5206\u7c7b\uff09\u7684\u6311\u6218\uff0c\u7814\u7a76\u4eba\u5458\u81f4\u529b\u4e8e\u63d0\u5347\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u5206\u5e03\u4e0b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002 2. \u7279\u522b\u91cd\u8981\u6216\u521b\u65b0\u7684\u8bba\u6587\uff1a [10] OpenM3D: Open Vocabulary Multi-view Indoor 3D Object Detection without Human Annotations\uff1a \u8fd9\u7bc7\u8bba\u6587\u57283D\u76ee\u6807\u68c0\u6d4b\u9886\u57df\u5b9e\u73b0\u4e86\u591a\u9879\u7a81\u7834\uff0c\u7ed3\u5408\u4e86\u5f00\u653e\u8bcd\u6c47\u3001\u591a\u89c6\u89d2\u548c\u65e0\u4eba\u5de5\u6807\u6ce8\u7684\u8303\u5f0f\uff0c\u6781\u5927\u5730\u964d\u4f4e\u4e863D\u573a\u666f\u7406\u89e3\u7684\u6807\u6ce8\u6210\u672c\uff0c\u5177\u6709\u5de8\u5927\u7684\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002 [2] GS: Generative Segmentation via Label Diffusion\uff1a \u5c06\u6269\u6563\u6a21\u578b\u5f15\u5165\u56fe\u50cf\u5206\u5272\u4efb\u52a1\uff0c\u901a\u8fc7\u6807\u7b7e\u6269\u6563\u5b9e\u73b0\u751f\u6210\u5f0f\u5206\u5272\uff0c\u4e3a\u5206\u5272\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u65b0\u7684\u89c6\u89d2\u548c\u65b9\u6cd5\uff0c\u6709\u671b\u5728\u590d\u6742\u573a\u666f\u548c\u6570\u636e\u7a00\u7f3a\u65f6\u5c55\u73b0\u4f18\u52bf\u3002 [8] Plug-in Feedback Self-adaptive Attention in CLIP for Training-free Open-Vocabulary Segmentation\uff1a \u8be5\u5de5\u4f5c\u901a\u8fc7\u5728CLIP\u4e2d\u5f15\u5165\u5373\u63d2\u5373\u7528\u7684\u53cd\u9988\u81ea\u9002\u5e94\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u8bad\u7ec3\u65e0\u5173\u7684\u5f00\u653e\u8bcd\u6c47\u5206\u5272\uff0c\u6781\u5927\u5730\u63d0\u9ad8\u4e86\u5f00\u653e\u8bcd\u6c47\u5206\u5272\u7684\u6548\u7387\u548c\u7075\u6d3b\u6027\u3002 [7] Dino U-Net: Exploiting High-Fidelity Dense Features from Foundation Models for Medical Image Segmentation\uff1a \u6709\u6548\u5730\u5c06\u5f3a\u5927\u7684Vision Transformer\uff08\u5982DINO\uff09\u7684\u5bc6\u96c6\u7279\u5f81\u4e0eU-Net\u67b6\u6784\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u6027\u80fd\uff0c\u5bf9\u533b\u7597AI\u9886\u57df\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002 3. \u65b0\u5174\u7814\u7a76\u65b9\u5411\u6216\u6280\u672f\uff1a \u6269\u6563\u6a21\u578b\u5728\u611f\u77e5\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\uff1a \u4e0d\u518d\u5c40\u9650\u4e8e\u56fe\u50cf\u751f\u6210\uff0c\u6269\u6563\u6a21\u578b\u6b63\u88ab\u63a2\u7d22\u7528\u4e8e\u66f4\u5e7f\u6cdb\u7684\u611f\u77e5\u4efb\u52a1\uff0c\u5982\u56fe\u50cf\u5206\u5272\u3002 \u57fa\u7840\u6a21\u578b\u4f5c\u4e3a\u201c\u5373\u63d2\u5373\u7528\u201d\u7ec4\u4ef6\uff1a \u57fa\u7840\u6a21\u578b\u4e0d\u4ec5\u7528\u4e8e\u5fae\u8c03\uff0c\u5176\u5185\u90e8\u673a\u5236\uff08\u5982\u5bc6\u96c6\u7279\u5f81\u3001\u6ce8\u610f\u529b\uff09\u6b63\u88ab\u5de7\u5999\u5730\u63d0\u53d6\u548c\u5229\u7528\uff0c\u4ee5\u5b9e\u73b0\u8bad\u7ec3\u65e0\u5173\u6216\u9ad8\u6548\u7684\u4e0b\u6e38\u4efb\u52a1\u3002 \u591a\u6a21\u6001\u4e0e\u591a\u89c6\u89d2\u65e0\u76d1\u7763/\u96f6\u6837\u672c\u5b66\u4e60\uff1a \u7ed3\u5408\u4e0d\u540c\u6a21\u6001\uff08LiDAR-Camera\uff09\u548c\u89c6\u89d2\u4fe1\u606f\uff0c\u5728\u65e0\u76d1\u7763\u6216\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u89e3\u51b33D\u611f\u77e5\u95ee\u9898\uff0c\u662f\u672a\u6765\u964d\u4f4e\u6210\u672c\u548c\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\u7684\u5173\u952e\u3002 \u4e13\u5bb6\u77e5\u8bc6\u4e0e\u57fa\u7840\u6a21\u578b\u7ed3\u5408\uff1a \u5c06\u9886\u57df\u4e13\u5bb6\u77e5\u8bc6\u878d\u5165\u57fa\u7840\u6a21\u578b\u7684\u9002\u5e94\u8fc7\u7a0b\uff0c\u4ee5\u89e3\u51b3\u7ec6\u7c92\u5ea6\u5206\u7c7b\u7b49\u7279\u5b9a\u9886\u57df\u7684\u6311\u6218\u3002 4. \u6700\u503c\u5f97\u6df1\u5165\u9605\u8bfb\u7684\u8bba\u6587\uff1a \u4e3a\u4e86\u5168\u9762\u4e86\u89e3\u5f53\u524d\u9886\u57df\u7684\u524d\u6cbf\u8fdb\u5c55\uff0c\u5efa\u8bae\u91cd\u70b9\u9605\u8bfb\u4ee5\u4e0b\u8bba\u6587\uff1a [10] OpenM3D: Open Vocabulary Multi-view Indoor 3D Object Detection without Human Annotations\uff1a \u4ee3\u8868\u4e863D\u89c6\u89c9\u548c\u65e0\u76d1\u7763/\u5f00\u653e\u8bcd\u6c47\u5b66\u4e60\u7684\u6700\u65b0\u878d\u5408\uff0c\u5177\u6709\u9ad8\u5f71\u54cd\u529b\u3002 [2] GS: Generative Segmentation via Label Diffusion\uff1a \u63a2\u7d22\u4e86\u6269\u6563\u6a21\u578b\u5728\u5206\u5272\u4efb\u52a1\u4e2d\u7684\u65b0\u8303\u5f0f\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002 [8] Plug-in Feedback Self-adaptive Attention in CLIP for Training-free Open-Vocabulary Segmentation\uff1a \u5c55\u793a\u4e86\u5982\u4f55\u9ad8\u6548\u5229\u7528\u73b0\u6709\u57fa\u7840\u6a21\u578b\u5b9e\u73b0\u8bad\u7ec3\u65e0\u5173\u7684\u5f00\u653e\u8bcd\u6c47\u80fd\u529b\uff0c\u5177\u6709\u5f88\u9ad8\u7684\u5b9e\u7528\u4ef7\u503c\u3002 [7] Dino U-Net: Exploiting High-Fidelity Dense Features from Foundation Models for Medical Image Segmentation\uff1a \u5bf9\u4e8e\u5173\u6ce8\u533b\u5b66\u56fe\u50cf\u5206\u6790\u548c\u57fa\u7840\u6a21\u578b\u5e94\u7528\u7684\u7814\u7a76\u4eba\u5458\uff0c\u63d0\u4f9b\u4e86\u5c06\u5148\u8fdb\u6a21\u578b\u5e94\u7528\u4e8e\u5173\u952e\u9886\u57df\u7684\u6709\u6548\u7b56\u7565\u3002","title":"Executive Summary"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-29/#table-of-contents","text":"Enhancing Pseudo-Boxes via Data-Level LiDAR-Camera Fusion for Unsupervised 3D Object Detection GS: Generative Segmentation via Label Diffusion Multimodal Prototype Alignment for Semi-supervised Pathology Image Segmentation Scalable Object Detection in the Car Interior With Vision Foundation Models Generalizing Monocular 3D Object Detection CraftGraffiti: Exploring Human Identity with Custom Graffiti Art via Facial-Preserving Diffusion Models Dino U-Net: Exploiting High-Fidelity Dense Features from Foundation Models for Medical Image Segmentation Plug-in Feedback Self-adaptive Attention in CLIP for Training-free Open-Vocabulary Segmentation Bridging Domain Gaps for Fine-Grained Moth Classification Through Expert-Informed Adaptation and Foundation Model Priors OpenM3D: Open Vocabulary Multi-view Indoor 3D Object Detection without Human Annotations","title":"Table of Contents"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-29/#papers","text":"","title":"Papers"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-29/#enhancing-pseudo-boxes-via-data-level-lidar-camera-fusion-for-unsupervised-3d-object-detection","text":"Authors: Mingqian Ji, Jian Yang, Shanshan Zhang Published: 2025-08-28 Categories: cs.CV Abstract: Existing LiDAR-based 3D object detectors typically rely on manually annotated labels for training to achieve good performance. However, obtaining high-quality 3D labels is time-consuming and labor-intensive. To address this issue, recent works explore unsupervised 3D object detection by introducing RGB images as an auxiliary modal to assist pseudo-box generation. However, these methods simply integrate pseudo-boxes generated by LiDAR point clouds and RGB images. Yet, such a label-level fusion strategy brings limited improvements to the quality of pseudo-boxes, as it overlooks the complementary nature in terms of LiDAR and RGB image data. To overcome the above limitations, we propose a novel data-level fusion framework that integrates RGB images and LiDAR data at an early stage. Specifically, we utilize vision foundation models for instance segmentation and depth estimation on images and introduce a bi-directional fusion method, where real points acquire category labels from the 2D space, while 2D pixels are projected onto 3D to enhance real point density. To mitigate noise from depth and segmentation estimations, we propose a local and global filtering method, which applies local radius filtering to suppress depth estimation errors and global statistical filtering to remove segmentation-induced outliers. Furthermore, we propose a data-level fusion based dynamic self-evolution strategy, which iteratively refines pseudo-boxes under a dense representation, significantly improving localization accuracy. Extensive experiments on the nuScenes dataset demonstrate that the detector trained by our method significantly outperforms that trained by previous state-of-the-art methods with 28.4 \\% mAP on the nuScenes validation benchmark. Analysis: \u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6570\u636e\u7ea7LiDAR-\u76f8\u673a\u878d\u5408\u6846\u67b6\uff0c\u7528\u4e8e\u65e0\u76d1\u77633D\u76ee\u6807\u68c0\u6d4b\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u4e2d\u4f2a\u6807\u7b7e\u8d28\u91cf\u53d7\u9650\u7684\u95ee\u9898\u3002\u4ee5\u4e0b\u662f\u8be6\u7ec6\u5206\u6790\uff1a","title":"Enhancing Pseudo-Boxes via Data-Level LiDAR-Camera Fusion for Unsupervised 3D Object Detection"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-29/#1-concise-summary","text":"\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6570\u636e\u7ea7LiDAR-\u76f8\u673a\u878d\u5408\u6846\u67b6\uff0c\u7528\u4e8e\u65e0\u76d1\u77633D\u76ee\u6807\u68c0\u6d4b\uff0c\u4ee5\u514b\u670d\u73b0\u6709\u6807\u7b7e\u7ea7\u878d\u5408\u7b56\u7565\u7684\u5c40\u9650\u6027\u3002\u901a\u8fc7\u5728\u65e9\u671f\u9636\u6bb5\u6574\u5408RGB\u56fe\u50cf\u548cLiDAR\u6570\u636e\uff0c\u5e76\u5229\u7528\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u53cc\u5411\u878d\u5408\u3001\u566a\u58f0\u8fc7\u6ee4\u548c\u52a8\u6001\u81ea\u6f14\u5316\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4f2a\u6807\u7b7e\u7684\u8d28\u91cf\u548c3D\u5b9a\u4f4d\u7cbe\u5ea6\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5176\u8bad\u7ec3\u51fa\u7684\u68c0\u6d4b\u5668\u5728nuScenes\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e8628.4% mAP\u7684\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002","title":"1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u660e\u603b\u7ed3 (Concise Summary)"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-29/#2-key-innovation-or-methodological-approach","text":"\u8be5\u8bba\u6587\u7684\u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u63d0\u51fa\u4e86 \u6570\u636e\u7ea7\uff08data-level\uff09LiDAR-\u76f8\u673a\u878d\u5408\u6846\u67b6 \uff0c\u800c\u975e\u4f20\u7edf\u7684\u6807\u7b7e\u7ea7\u878d\u5408\uff0c\u4ee5\u5145\u5206\u5229\u7528LiDAR\u548cRGB\u56fe\u50cf\u4e4b\u95f4\u7684\u6570\u636e\u4e92\u8865\u6027\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\uff1a \u65e9\u671f\u6570\u636e\u878d\u5408 \uff1a\u4e0d\u540c\u4e8e\u5c06LiDAR\u548cRGB\u5355\u72ec\u751f\u6210\u7684\u4f2a\u6807\u7b7e\u8fdb\u884c\u7b80\u5355\u6574\u5408\uff0c\u8be5\u65b9\u6cd5\u5728\u539f\u59cb\u6570\u636e\u5c42\u9762\uff08\u65e9\u671f\u9636\u6bb5\uff09\u5c31\u5c06\u4e24\u79cd\u6a21\u6001\u7684\u6570\u636e\u8fdb\u884c\u878d\u5408\u3002 \u53cc\u5411\u878d\u5408\u673a\u5236 \uff1a \u5229\u7528\u9884\u8bad\u7ec3\u7684 \u89c6\u89c9\u57fa\u7840\u6a21\u578b \uff08Vision Foundation Models\uff09\u5bf9RGB\u56fe\u50cf\u8fdb\u884c\u5b9e\u4f8b\u5206\u5272\u548c\u6df1\u5ea6\u4f30\u8ba1\u3002 3D\u70b9\u83b7\u53d62D\u7c7b\u522b\u6807\u7b7e \uff1a\u5c062D\u56fe\u50cf\u4e2d\u7684\u7c7b\u522b\u4fe1\u606f\uff08\u6765\u81ea\u5b9e\u4f8b\u5206\u5272\uff09\u6620\u5c04\u52303D\u70b9\u4e91\u4e0a\u3002 2D\u50cf\u7d20\u589e\u5f3a3D\u70b9\u5bc6\u5ea6 \uff1a\u5c062D\u50cf\u7d20\u6295\u5f71\u52303D\u7a7a\u95f4\uff0c\u4ee5\u589e\u5f3aLiDAR\u70b9\u4e91\u7684\u5bc6\u5ea6\uff0c\u5c24\u5176\u662f\u5728\u7a00\u758f\u533a\u57df\u3002 \u9c81\u68d2\u6027\u566a\u58f0\u8fc7\u6ee4 \uff1a\u4e3a\u4e86\u7f13\u89e3\u6df1\u5ea6\u4f30\u8ba1\u548c\u5206\u5272\u7ed3\u679c\u4e2d\u7684\u566a\u58f0\uff0c\u63d0\u51fa\u4e86\uff1a \u5c40\u90e8\u534a\u5f84\u8fc7\u6ee4\uff08Local Radius Filtering\uff09 \uff1a\u7528\u4e8e\u6291\u5236\u6df1\u5ea6\u4f30\u8ba1\u8bef\u5dee\u3002 \u5168\u5c40\u7edf\u8ba1\u8fc7\u6ee4\uff08Global Statistical Filtering\uff09 \uff1a\u7528\u4e8e\u79fb\u9664\u7531\u5206\u5272\u9519\u8bef\u5f15\u8d77\u7684\u79bb\u7fa4\u70b9\u3002 \u6570\u636e\u7ea7\u878d\u5408\u7684\u52a8\u6001\u81ea\u6f14\u5316\u7b56\u7565\uff08Dynamic Self-Evolution Strategy\uff09 \uff1a\u57fa\u4e8e\u6570\u636e\u7ea7\u878d\u5408\u7684\u4f2a\u6807\u7b7e\uff0c\u901a\u8fc7\u8fed\u4ee3\u7ec6\u5316\u8fc7\u7a0b\uff0c\u5728\u5bc6\u96c6\u8868\u793a\u4e0b\u663e\u8457\u63d0\u5347\u4f2a\u6807\u7b7e\u7684\u5b9a\u4f4d\u7cbe\u5ea6\u3002","title":"2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u5b66\u65b9\u6cd5 (Key Innovation or Methodological Approach)"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-29/#3-potential-impact-on-the-field","text":"\u63a8\u52a8\u65e0\u76d1\u77633D\u76ee\u6807\u68c0\u6d4b\u53d1\u5c55 \uff1a\u663e\u8457\u63d0\u5347\u4e86\u65e0\u76d1\u7763\u65b9\u6cd5\u5728\u590d\u6742\u573a\u666f\u4e0b\u7684\u6027\u80fd\uff0c\u4f7f\u5176\u66f4\u63a5\u8fd1\u751a\u81f3\u53ef\u80fd\u8d85\u8d8a\u90e8\u5206\u6709\u76d1\u7763\u65b9\u6cd5\uff0c\u964d\u4f4e\u4e86\u5bf9\u6602\u8d353D\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u4ece\u800c\u52a0\u901f\u4e86\u76f8\u5173\u7814\u7a76\u548c\u5e94\u7528\u3002 \u5f3a\u8c03\u6570\u636e\u7ea7\u878d\u5408\u7684\u6f5c\u529b \uff1a\u8bc1\u660e\u4e86\u5728\u65e9\u671f\u9636\u6bb5\u878d\u5408\u591a\u6a21\u6001\u6570\u636e\uff08LiDAR\u548cRGB\uff09\u7684\u4f18\u8d8a\u6027\uff0c\u4e3a\u672a\u6765\u591a\u6a21\u6001\u611f\u77e5\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u53ef\u80fd\u542f\u53d1\u66f4\u591a\u7814\u7a76\u63a2\u7d22\u4e0d\u540c\u6a21\u6001\u5728\u6570\u636e\u5c42\u9762\u7684\u6df1\u5ea6\u878d\u5408\u3002 \u4fc3\u8fdb\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u5e94\u7528 \uff1a\u5c55\u793a\u4e86\u5229\u7528\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08\u5982\u5b9e\u4f8b\u5206\u5272\u548c\u6df1\u5ea6\u4f30\u8ba1\uff09\u4f5c\u4e3a\u591a\u6a21\u6001\u878d\u5408\u7684\u5f3a\u5927\u524d\u7aef\u7684\u6709\u6548\u6027\uff0c\u4e3a\u8fd9\u4e9b\u6a21\u578b\u5728\u66f4\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u3002 \u5b9e\u7528\u6027\u63d0\u5347 \uff1a\u4e3a\u81ea\u52a8\u9a7e\u9a76\u3001\u673a\u5668\u4eba\u7b49\u9700\u8981\u9ad8\u7cbe\u5ea63D\u611f\u77e5\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u7ecf\u6d4e\u3001\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u964d\u4f4e\u4e86\u90e8\u7f72\u6210\u672c\u548c\u65f6\u95f4\u3002","title":"3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd (Potential Impact on the Field)"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-29/#4-related-areas-or-applications","text":"\u81ea\u52a8\u9a7e\u9a76 (Autonomous Driving) \uff1a\u76f4\u63a5\u53d7\u76ca\uff0c\u964d\u4f4e\u4e86\u8bad\u7ec33D\u611f\u77e5\u6a21\u578b\u7684\u6570\u636e\u6210\u672c\uff0c\u52a0\u901f\u4e86\u81ea\u52a8\u9a7e\u9a76\u6280\u672f\u7684\u7814\u53d1\u548c\u90e8\u7f72\u3002 \u673a\u5668\u4eba\u5b66 (Robotics) \uff1a\u7528\u4e8e\u673a\u5668\u4eba\u5bfc\u822a\u3001\u73af\u5883\u611f\u77e5\u3001\u7269\u4f53\u6293\u53d6\u548c\u4eba\u673a\u4ea4\u4e92\uff0c\u5c24\u5176\u662f\u5728\u672a\u77e5\u6216\u52a8\u6001\u73af\u5883\u4e2d\u3002 \u667a\u6167\u57ce\u5e02 (Smart Cities) \uff1a\u4ea4\u901a\u76d1\u63a7\u3001\u57fa\u7840\u8bbe\u65bd\u7ba1\u7406\u3001\u884c\u4eba\u6d41\u5206\u6790\u7b49\u573a\u666f\u4e2d\u76843D\u76ee\u6807\u8bc6\u522b\u548c\u8ddf\u8e2a\u3002 \u589e\u5f3a\u73b0\u5b9e/\u865a\u62df\u73b0\u5b9e (AR/VR) \uff1a3D\u573a\u666f\u7406\u89e3\u3001\u73af\u5883\u91cd\u5efa\u548c\u865a\u62df\u7269\u4f53\u4e0e\u771f\u5b9e\u4e16\u754c\u7684\u7cbe\u786e\u878d\u5408\u3002 \u5de5\u4e1a\u81ea\u52a8\u5316 (Industrial Automation) \uff1a\u4f8b\u5982\u4ed3\u5e93\u4e2d\u7684\u7269\u4f53\u8bc6\u522b\u3001\u5b9a\u4f4d\u548c\u5206\u62e3\uff0c\u63d0\u9ad8\u81ea\u52a8\u5316\u6c34\u5e73\u3002","title":"4. \u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 (Related Areas or Applications)"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-29/#5-limitations-inferred-from-the-abstract","text":"\u5bf92D\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u4f9d\u8d56 \uff1a\u65b9\u6cd5\u7684\u6027\u80fd\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u53d7\u9650\u4e8e\u6240\u4f7f\u7528\u76842D\u5b9e\u4f8b\u5206\u5272\u548c\u6df1\u5ea6\u4f30\u8ba1\u6a21\u578b\u7684\u51c6\u786e\u6027\u3002\u8fd9\u4e9b\u6a21\u578b\u7684\u8bef\u5dee\u4f1a\u76f4\u63a5\u5f71\u54cd3D\u4f2a\u6807\u7b7e\u7684\u8d28\u91cf\uff0c\u5c24\u5176\u662f\u5728\u590d\u6742\u6216\u4e0d\u7406\u60f3\u7684\u56fe\u50cf\u6761\u4ef6\u4e0b\u3002 \u566a\u58f0\u7f13\u89e3\u7684\u6311\u6218 \uff1a\u5c3d\u7ba1\u63d0\u51fa\u4e86\u8fc7\u6ee4\u65b9\u6cd5\uff0c\u4f46\u6df1\u5ea6\u4f30\u8ba1\u548c2D\u5206\u5272\u56fa\u6709\u7684\u566a\u58f0\u548c\u4e0d\u786e\u5b9a\u6027\u4ecd\u7136\u662f\u9700\u8981\u6301\u7eed\u5173\u6ce8\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u6076\u52a3\u73af\u5883\uff08\u5982\u96e8\u96fe\u3001\u4f4e\u5149\u7167\u3001\u5f3a\u53cd\u5149\uff09\u4e0b\uff0c\u8fd9\u4e9b\u566a\u58f0\u53ef\u80fd\u96be\u4ee5\u5b8c\u5168\u6d88\u9664\u3002 \u8ba1\u7b97\u8d44\u6e90\u6d88\u8017 \uff1a\u65e9\u671f\u6570\u636e\u7ea7\u878d\u5408\u3001\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u5e94\u7528\u4ee5\u53ca\u8fed\u4ee3\u81ea\u6f14\u5316\u7b56\u7565\uff0c\u53ef\u80fd\u9700\u8981\u8f83\u9ad8\u7684\u8ba1\u7b97\u8d44\u6e90\u548c\u65f6\u95f4\u6210\u672c\uff0c\u8fd9\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u6216\u5b9e\u65f6\u5e94\u7528\u4e2d\u53ef\u80fd\u662f\u4e00\u4e2a\u8003\u91cf\u3002 \u6cdb\u5316\u80fd\u529b \uff1a\u5b9e\u9a8c\u4e3b\u8981\u5728nuScenes\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u3002\u5176\u5728\u4e0d\u540c\u4f20\u611f\u5668\u914d\u7f6e\u3001\u4e0d\u540c\u573a\u666f\u7c7b\u578b\uff08\u5982\u8d8a\u91ce\u3001\u5ba4\u5185\uff09\u6216\u4e0d\u540c\u5730\u7406\u73af\u5883\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u5c1a\u5f85\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u3002 \u7279\u5b9a\u6a21\u6001\u9650\u5236 \uff1a\u76ee\u524d\u4ec5\u9650\u4e8eLiDAR\u548cRGB\u56fe\u50cf\u7684\u878d\u5408\u3002\u5bf9\u4e8e\u5176\u4ed6\u6a21\u6001\uff08\u5982\u96f7\u8fbe\u3001\u70ed\u6210\u50cf\uff09\u7684\u6574\u5408\u6f5c\u529b\u672a\u63d0\u53ca\uff0c\u8fd9\u4e9b\u6a21\u6001\u5728\u67d0\u4e9b\u7279\u5b9a\u573a\u666f\u4e0b\u53ef\u80fd\u63d0\u4f9b\u989d\u5916\u7684\u9c81\u68d2\u6027\u3002 Key Findings: To overcome the above limitations, we propose a novel data-level fusion framework that integrates RGB images and LiDAR data at an early stage. To mitigate noise from depth and segmentation estimations, we propose a local and global filtering method, which applies local radius filtering to suppress depth estimation errors and global statistical filtering to remove segmentation-induced outliers. Furthermore, we propose a data-level fusion based dynamic self-evolution strategy, which iteratively refines pseudo-boxes under a dense representation, significantly improving localization accuracy. Extensive experiments on the nuScenes dataset demonstrate that the detector trained by our method significantly outperforms that trained by previous state-of-the-art methods with 28.4 \\% mAP on the nuScenes validation benchmark. Links: PDF arXiv","title":"5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u7684\u5c40\u9650\u6027 (Limitations Inferred from the Abstract)"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-29/#gs-generative-segmentation-via-label-diffusion","text":"Authors: Yuhao Chen, Shubin Chen, Liang Lin, Guangrun Wang Published: 2025-08-27 Categories: cs.CV Abstract: Language-driven image segmentation is a fundamental task in vision-language understanding, requiring models to segment regions of an image corresponding to natural language expressions. Traditional methods approach this as a discriminative problem, assigning each pixel to foreground or background based on semantic alignment. Recently, diffusion models have been introduced to this domain, but existing approaches remain image-centric: they either (i) use image diffusion models as visual feature extractors, (ii) synthesize segmentation data via image generation to train discriminative models, or (iii) perform diffusion inversion to extract attention cues from pre-trained image diffusion models-thereby treating segmentation as an auxiliary process. In this paper, we propose GS (Generative Segmentation), a novel framework that formulates segmentation itself as a generative task via label diffusion. Instead of generating images conditioned on label maps and text, GS reverses the generative process: it directly generates segmentation masks from noise, conditioned on both the input image and the accompanying language description. This paradigm makes label generation the primary modeling target, enabling end-to-end training with explicit control over spatial and semantic fidelity. To demonstrate the effectiveness of our approach, we evaluate GS on Panoptic Narrative Grounding (PNG), a representative and challenging benchmark for multimodal segmentation that requires panoptic-level reasoning guided by narrative captions. Experimental results show that GS significantly outperforms existing discriminative and diffusion-based methods, setting a new state-of-the-art for language-driven segmentation. Analysis: \u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\uff0c\u7279\u522b\u662f\u89c6\u89c9-\u8bed\u8a00\u7406\u89e3\uff08Vision-Language Understanding\uff09\u65b9\u5411\u4e0a\u5177\u6709\u6f5c\u5728\u91cd\u8981\u6027\u7684\u65b0\u65b9\u6cd5\u3002","title":"GS: Generative Segmentation via Label Diffusion"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-29/#1-2-3","text":"\u672c\u6587\u63d0\u51fa\u4e86GS\uff08Generative Segmentation\uff09\u6846\u67b6\uff0c\u5c06\u8bed\u8a00\u9a71\u52a8\u7684\u56fe\u50cf\u5206\u5272\u4efb\u52a1\u91cd\u65b0\u5b9a\u4e49\u4e3a\u901a\u8fc7\u6807\u7b7e\u6269\u6563\uff08label diffusion\uff09\u5b9e\u73b0\u7684\u751f\u6210\u5f0f\u4efb\u52a1\u3002\u4e0e\u73b0\u6709\u65b9\u6cd5\u5c06\u5206\u5272\u89c6\u4e3a\u8f85\u52a9\u8fc7\u7a0b\u4e0d\u540c\uff0cGS\u76f4\u63a5\u4ece\u566a\u58f0\u4e2d\u751f\u6210\u5206\u5272\u63a9\u7801\uff0c\u5e76\u4ee5\u8f93\u5165\u56fe\u50cf\u548c\u8bed\u8a00\u63cf\u8ff0\u4e3a\u6761\u4ef6\uff0c\u4f7f\u6807\u7b7e\u751f\u6210\u6210\u4e3a\u6838\u5fc3\u5efa\u6a21\u76ee\u6807\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cGS\u663e\u8457\u8d85\u8d8a\u4e86\u73b0\u6709\u5224\u522b\u5f0f\u548c\u57fa\u4e8e\u6269\u6563\u7684\u65b9\u6cd5\uff0c\u5728\u8bed\u8a00\u9a71\u52a8\u5206\u5272\u4efb\u52a1\u4e0a\u8fbe\u5230\u4e86\u65b0\u7684SOTA\u3002","title":"1. \u8bba\u6587\u6838\u5fc3\u8d21\u732e\u7684\u7b80\u6d01\u603b\u7ed3 (2-3\u53e5\u8bdd)"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-29/#2","text":"GS\u7684\u6838\u5fc3\u521b\u65b0\u5728\u4e8e \u8303\u5f0f\u8f6c\u53d8 \uff1a\u5b83\u5c06\u56fe\u50cf\u5206\u5272\u4efb\u52a1\u672c\u8eab\u4ece\u4f20\u7edf\u7684\u5224\u522b\u5f0f\u95ee\u9898\uff08\u5c06\u50cf\u7d20\u5206\u7c7b\u4e3a\u524d\u666f\u6216\u80cc\u666f\uff09\u6216\u5c06\u5206\u5272\u4f5c\u4e3a\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7684\u8f85\u52a9\u8f93\u51fa\uff0c\u91cd\u65b0\u5b9a\u4e49\u4e3a \u751f\u6210\u5f0f\u4efb\u52a1 \u3002 \u5177\u4f53\u7684\u65b9\u6cd5\u5b66\u521b\u65b0\u662f\u5f15\u5165\u4e86 \u201c\u6807\u7b7e\u6269\u6563\u201d\uff08label diffusion\uff09 \u673a\u5236\u3002\u4e0e\u73b0\u6709\u6269\u6563\u6a21\u578b\u901a\u5e38\u751f\u6210\u56fe\u50cf\uff08\u4ee5\u6807\u7b7e\u6216\u6587\u672c\u4e3a\u6761\u4ef6\uff09\u4e0d\u540c\uff0cGS\u98a0\u8986\u4e86\u8fd9\u4e00\u8fc7\u7a0b\uff1a\u5b83\u76f4\u63a5\u4ece\u968f\u673a\u566a\u58f0\u4e2d\u9010\u6b65\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u5206\u5272\u63a9\u7801\uff0c\u5e76\u4ee5\u8f93\u5165\u56fe\u50cf\u548c\u5bf9\u5e94\u7684\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u4f5c\u4e3a\u751f\u6210\u8fc7\u7a0b\u7684\u6761\u4ef6\u3002\u8fd9\u79cd\u201c\u6807\u7b7e\u4f18\u5148\u201d\u7684\u751f\u6210\u8303\u5f0f\u4f7f\u5f97\u6807\u7b7e\u751f\u6210\u6210\u4e3a\u7aef\u5230\u7aef\u8bad\u7ec3\u7684\u4e3b\u8981\u76ee\u6807\uff0c\u4ece\u800c\u80fd\u591f\u5bf9\u751f\u6210\u63a9\u7801\u7684\u7a7a\u95f4\u7cbe\u5ea6\u548c\u8bed\u4e49\u4fdd\u771f\u5ea6\u8fdb\u884c\u663e\u5f0f\u4e14\u7cbe\u7ec6\u7684\u63a7\u5236\u3002","title":"2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u5b66\u65b9\u6cd5"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-29/#3","text":"\u5f00\u8f9f\u65b0\u8303\u5f0f\uff1a \u5c06\u5206\u5272\u4efb\u52a1\u4ece\u5224\u522b\u5f0f\u6216\u8f85\u52a9\u4efb\u52a1\u8f6c\u53d8\u4e3a\u6838\u5fc3\u751f\u6210\u5f0f\u4efb\u52a1\uff0c\u4e3a\u89c6\u89c9-\u8bed\u8a00\u7406\u89e3\u9886\u57df\u7684\u5206\u5272\u95ee\u9898\u5f00\u8f9f\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\uff0c\u53ef\u80fd\u542f\u53d1\u5176\u4ed6\u7ed3\u6784\u5316\u9884\u6d4b\u4efb\u52a1\uff08\u5982\u5173\u952e\u70b9\u68c0\u6d4b\u3001\u59ff\u6001\u4f30\u8ba1\uff09\u4e2d\u5e94\u7528\u6269\u6563\u6a21\u578b\u7684\u65b0\u601d\u8def\u3002 \u6027\u80fd\u7a81\u7834\uff1a \u5728Panoptic Narrative Grounding (PNG) \u7b49\u590d\u6742\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u591a\u6a21\u6001\u5206\u5272\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u53d6\u5f97SOTA\uff0c\u8bc1\u660e\u4e86\u751f\u6210\u5f0f\u5206\u5272\u5728\u5904\u7406\u590d\u6742\u53d9\u8ff0\u6027\u63cf\u8ff0\u548c\u5168\u666f\u7ea7\u63a8\u7406\u65b9\u9762\u7684\u4f18\u8d8a\u6027\u3002 \u63d0\u5347\u53ef\u63a7\u6027\u4e0e\u7cbe\u5ea6\uff1a \u5f3a\u8c03\u5bf9\u7a7a\u95f4\u548c\u8bed\u4e49\u4fdd\u771f\u5ea6\u7684\u663e\u5f0f\u63a7\u5236\uff0c\u9884\u793a\u7740\u672a\u6765\u6a21\u578b\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u9ad8\u7cbe\u5ea6\u5206\u5272\u7ed3\u679c\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u7cbe\u7ec6\u4ea4\u4e92\u548c\u7f16\u8f91\u7684\u573a\u666f\u3002 \u7edf\u4e00\u89c6\u89c9-\u8bed\u8a00\u4efb\u52a1\uff1a \u901a\u8fc7\u5c06\u5206\u5272\u878d\u5165\u751f\u6210\u6846\u67b6\uff0c\u53ef\u80fd\u4e3a\u66f4\u6df1\u5c42\u6b21\u7684\u89c6\u89c9-\u8bed\u8a00\u8054\u5408\u5efa\u6a21\u63d0\u4f9b\u65b0\u7684\u89c6\u89d2\uff0c\u4fc3\u8fdb\u6a21\u578b\u66f4\u597d\u5730\u7406\u89e3\u56fe\u50cf\u5185\u5bb9\u4e0e\u8bed\u8a00\u63cf\u8ff0\u4e4b\u95f4\u7684\u590d\u6742\u5bf9\u5e94\u5173\u7cfb\u3002","title":"3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-29/#4","text":"\u4eba\u673a\u4ea4\u4e92 (HCI)\uff1a \u5141\u8bb8\u7528\u6237\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u7cbe\u786e\u5730\u9009\u62e9\u3001\u7f16\u8f91\u6216\u64cd\u4f5c\u56fe\u50cf\u4e2d\u7684\u7279\u5b9a\u533a\u57df\uff0c\u4f8b\u5982\u667a\u80fd\u56fe\u50cf\u7f16\u8f91\u8f6f\u4ef6\u3001\u865a\u62df\u52a9\u624b\u3002 \u673a\u5668\u4eba\u5b66 (Robotics)\uff1a \u673a\u5668\u4eba\u53ef\u4ee5\u901a\u8fc7\u8bed\u8a00\u6307\u4ee4\u7406\u89e3\u73af\u5883\u5e76\u6267\u884c\u7cbe\u7ec6\u7684\u64cd\u4f5c\uff0c\u4f8b\u5982\u201c\u6293\u4f4f\u684c\u5b50\u4e0a\u7684\u90a3\u4e2a\u84dd\u8272\u676f\u5b50\u201d\u9700\u8981\u7cbe\u786e\u5730\u5206\u5272\u51fa\u676f\u5b50\u3002 \u533b\u5b66\u5f71\u50cf\u5206\u6790\uff1a \u533b\u751f\u6216\u7814\u7a76\u4eba\u5458\u53ef\u4ee5\u901a\u8fc7\u63cf\u8ff0\u6765\u8f85\u52a9\u75c5\u7076\u533a\u57df\u7684\u7cbe\u786e\u5206\u5272\u548c\u5206\u6790\uff0c\u63d0\u9ad8\u8bca\u65ad\u6548\u7387\u548c\u51c6\u786e\u6027\u3002 \u5185\u5bb9\u521b\u4f5c\u4e0e\u7f16\u8f91\uff1a \u5728\u7535\u5f71\u3001\u5e7f\u544a\u5236\u4f5c\u4e2d\uff0c\u57fa\u4e8e\u6587\u672c\u63cf\u8ff0\u8fdb\u884c\u7cbe\u786e\u7684\u5bf9\u8c61\u9009\u62e9\u3001\u62a0\u56fe\u548c\u4fee\u6539\uff0c\u5927\u5927\u63d0\u9ad8\u5de5\u4f5c\u6548\u7387\u3002 \u81ea\u52a8\u9a7e\u9a76\uff1a \u8f66\u8f86\u9700\u8981\u7406\u89e3\u590d\u6742\u7684\u573a\u666f\u63cf\u8ff0\u5e76\u8bc6\u522b\u7279\u5b9a\u76ee\u6807\uff08\u4f8b\u5982\u201c\u524d\u65b9\u5de6\u4fa7\u7684\u884c\u4eba\u201d\uff09\uff0c\u4ee5\u8fdb\u884c\u5b89\u5168\u51b3\u7b56\u3002 \u8f85\u52a9\u6280\u672f\uff1a \u5e2e\u52a9\u89c6\u969c\u4eba\u58eb\u901a\u8fc7\u8bed\u8a00\u63cf\u8ff0\u66f4\u6df1\u5165\u5730\u7406\u89e3\u56fe\u50cf\u5185\u5bb9\uff0c\u63d0\u5347\u53ef\u8bbf\u95ee\u6027\u3002 \u901a\u7528\u89c6\u89c9-\u8bed\u8a00\u7406\u89e3\uff1a \u4efb\u4f55\u9700\u8981\u6df1\u5ea6\u7ed3\u5408\u89c6\u89c9\u548c\u8bed\u8a00\u4fe1\u606f\u8fdb\u884c\u7cbe\u7ec6\u7a7a\u95f4\u63a8\u7406\u548c\u8f93\u51fa\u7684\u4efb\u52a1\u3002","title":"4. \u53ef\u80fd\u53d7\u76ca\u7684\u76f8\u5173\u9886\u57df\u6216\u5e94\u7528"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-29/#5","text":"\u8ba1\u7b97\u6210\u672c\uff1a \u6269\u6563\u6a21\u578b\u901a\u5e38\u8ba1\u7b97\u91cf\u5927\uff0c\u751f\u6210\u8fc7\u7a0b\u6d89\u53ca\u591a\u6b65\u8fed\u4ee3\uff0c\u53ef\u80fd\u5bfc\u81f4\u63a8\u7406\u901f\u5ea6\u8f83\u6162\uff0c\u96be\u4ee5\u6ee1\u8db3\u5b9e\u65f6\u5e94\u7528\u9700\u6c42\u3002 \u6570\u636e\u4f9d\u8d56\uff1a \u8bad\u7ec3\u8fd9\u79cd\u590d\u6742\u7684\u751f\u6210\u5f0f\u6a21\u578b\uff0c\u7279\u522b\u662f\u9700\u8981\u56fe\u50cf\u3001\u8bed\u8a00\u548c\u7cbe\u786e\u6807\u7b7e\u4e09\u8005\u5bf9\u9f50\u7684\uff0c\u901a\u5e38\u9700\u8981\u5927\u89c4\u6a21\u3001\u9ad8\u8d28\u91cf\u4e14\u6807\u6ce8\u7cbe\u7ec6\u7684\u6570\u636e\u96c6\uff0c\u8fd9\u53ef\u80fd\u9650\u5236\u5176\u5728\u6570\u636e\u7a00\u7f3a\u9886\u57df\u7684\u5e94\u7528\u3002 \u6cdb\u5316\u80fd\u529b\uff1a \u5c3d\u7ba1\u662f\u8bed\u8a00\u9a71\u52a8\uff0c\u4f46\u6a21\u578b\u5bf9\u8bad\u7ec3\u6570\u636e\u4e2d\u672a\u51fa\u73b0\u8fc7\u7684\u5168\u65b0\u6982\u5ff5\u3001\u9ad8\u5ea6\u62bd\u8c61\u6216\u6b67\u4e49\u6027\u5f3a\u7684\u8bed\u8a00\u63cf\u8ff0\u7684\u6cdb\u5316\u80fd\u529b\u4ecd\u9700\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u3002 \u53ef\u63a7\u6027\u7c92\u5ea6\uff1a \u6458\u8981\u4e2d\u63d0\u5230\u201c\u663e\u5f0f\u63a7\u5236\u201d\uff0c\u4f46\u5b9e\u9645\u64cd\u4f5c\u4e2d\uff0c\u5982\u4f55\u901a\u8fc7\u8bed\u8a00\u63cf\u8ff0\u6765\u7cbe\u7ec6\u5730\u5fae\u8c03\u751f\u6210\u63a9\u7801\u7684\u6bcf\u4e00\u4e2a\u7ec6\u8282\uff0c\u4ee5\u53ca\u5904\u7406\u8bed\u8a00\u63cf\u8ff0\u4e2d\u7684\u6a21\u7cca\u6027\u6216\u51b2\u7a81\uff0c\u53ef\u80fd\u4ecd\u662f\u4e00\u4e2a\u6311\u6218\u3002 \u6a21\u578b\u590d\u6742\u6027\uff1a \u751f\u6210\u5f0f\u6a21\u578b\u901a\u5e38\u6bd4\u5224\u522b\u5f0f\u6a21\u578b\u66f4\u590d\u6742\uff0c\u8bad\u7ec3\u548c\u8c03\u8bd5\u53ef\u80fd\u66f4\u5177\u6311\u6218\u6027\u3002 Key Findings: In this paper, we propose GS (Generative Segmentation), a novel framework that formulates segmentation itself as a generative task via label diffusion. To demonstrate the effectiveness of our approach, we evaluate GS on Panoptic Narrative Grounding (PNG), a representative and challenging benchmark for multimodal segmentation that requires panoptic-level reasoning guided by narrative captions. Experimental results show that GS significantly outperforms existing discriminative and diffusion-based methods, setting a new state-of-the-art for language-driven segmentation. Links: PDF arXiv","title":"5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u7684\u5c40\u9650\u6027"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-29/#multimodal-prototype-alignment-for-semi-supervised-pathology-image-segmentation","text":"Authors: Mingxi Fu, Fanglei Fu, Xitong Ling, Huaitian Yuan, Tian Guan, Yonghong He, Lianghui Zhu Published: 2025-08-27 Categories: cs.CV, cs.AI Abstract: Pathological image segmentation faces numerous challenges, particularly due to ambiguous semantic boundaries and the high cost of pixel-level annotations. Although recent semi-supervised methods based on consistency regularization (e.g., UniMatch) have made notable progress, they mainly rely on perturbation-based consistency within the image modality, making it difficult to capture high-level semantic priors, especially in structurally complex pathology images. To address these limitations, we propose MPAMatch - a novel segmentation framework that performs pixel-level contrastive learning under a multimodal prototype-guided supervision paradigm. The core innovation of MPAMatch lies in the dual contrastive learning scheme between image prototypes and pixel labels, and between text prototypes and pixel labels, providing supervision at both structural and semantic levels. This coarse-to-fine supervisory strategy not only enhances the discriminative capability on unlabeled samples but also introduces the text prototype supervision into segmentation for the first time, significantly improving semantic boundary modeling. In addition, we reconstruct the classic segmentation architecture (TransUNet) by replacing its ViT backbone with a pathology-pretrained foundation model (Uni), enabling more effective extraction of pathology-relevant features. Extensive experiments on GLAS, EBHI-SEG-GLAND, EBHI-SEG-CANCER, and KPI show MPAMatch's superiority over state-of-the-art methods, validating its dual advantages in structural and semantic modeling. Analysis: \u8fd9\u662f\u4e00\u7bc7\u5173\u4e8e\u534a\u76d1\u7763\u75c5\u7406\u56fe\u50cf\u5206\u5272\u7684\u8bba\u6587\u6458\u8981\u5206\u6790\uff0c\u4ee5\u4e0b\u662f\u6211\u7684\u4e13\u4e1a\u89e3\u8bfb\uff1a","title":"Multimodal Prototype Alignment for Semi-supervised Pathology Image Segmentation"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-29/#multimodal-prototype-alignment-for-semi-supervised-pathology-image-segmentation_1","text":"1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u603b\u7ed3 (2-3\u53e5\u8bdd) \u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMPAMatch\u7684\u65b0\u578b\u534a\u76d1\u7763\u5206\u5272\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u75c5\u7406\u56fe\u50cf\u5206\u5272\u4e2d\u6a21\u7cca\u8fb9\u754c\u548c\u9ad8\u6807\u6ce8\u6210\u672c\u7684\u6311\u6218\u3002\u5176\u6838\u5fc3\u8d21\u732e\u5728\u4e8e\u5f15\u5165\u4e86\u591a\u6a21\u6001\u539f\u578b\u5f15\u5bfc\u7684\u76d1\u7763\u8303\u5f0f\uff0c\u901a\u8fc7\u56fe\u50cf\u539f\u578b\u4e0e\u50cf\u7d20\u6807\u7b7e\u3001\u4ee5\u53ca\u6587\u672c\u539f\u578b\u4e0e\u50cf\u7d20\u6807\u7b7e\u4e4b\u95f4\u7684\u53cc\u91cd\u5bf9\u6bd4\u5b66\u4e60\uff0c\u9996\u6b21\u5c06\u6587\u672c\u539f\u578b\u76d1\u7763\u5f15\u5165\u5206\u5272\u4efb\u52a1\uff0c\u4ece\u800c\u5728\u7ed3\u6784\u548c\u8bed\u4e49\u5c42\u9762\u63d0\u4f9b\u66f4\u4e30\u5bcc\u7684\u76d1\u7763\u4fe1\u606f\u3002\u6b64\u5916\uff0c\u5b83\u8fd8\u901a\u8fc7\u6574\u5408\u75c5\u7406\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\uff08Uni\uff09\u6539\u8fdb\u4e86TransUNet\u67b6\u6784\uff0c\u4ee5\u66f4\u6709\u6548\u5730\u63d0\u53d6\u75c5\u7406\u76f8\u5173\u7279\u5f81\u3002 2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u5b66\u65b9\u6cd5 MPAMatch\u7684\u5173\u952e\u521b\u65b0\u5728\u4e8e\u5176 \u591a\u6a21\u6001\u539f\u578b\u5f15\u5bfc\u7684\u76d1\u7763\u8303\u5f0f \u548c \u53cc\u91cd\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6848 \uff1a * \u53cc\u91cd\u5bf9\u6bd4\u5b66\u4e60\uff1a * \u56fe\u50cf\u539f\u578b\u4e0e\u50cf\u7d20\u6807\u7b7e\u7684\u5bf9\u6bd4\u5b66\u4e60\uff1a \u63d0\u4f9b\u7ed3\u6784\u5c42\u9762\u7684\u76d1\u7763\uff0c\u589e\u5f3a\u5bf9\u56fe\u50cf\u5c40\u90e8\u7279\u5f81\u7684\u5224\u522b\u80fd\u529b\u3002 * \u6587\u672c\u539f\u578b\u4e0e\u50cf\u7d20\u6807\u7b7e\u7684\u5bf9\u6bd4\u5b66\u4e60\uff1a \u8fd9\u662f\u8be5\u65b9\u6cd5\u7684\u72ec\u7279\u4e4b\u5904\uff0c\u9996\u6b21\u5c06\u9ad8\u5c42\u8bed\u4e49\u4fe1\u606f\uff08\u901a\u8fc7\u6587\u672c\u63cf\u8ff0\u6355\u83b7\uff09\u5f15\u5165\u50cf\u7d20\u7ea7\u5206\u5272\u4efb\u52a1\uff0c\u663e\u8457\u6539\u5584\u4e86\u8bed\u4e49\u8fb9\u754c\u5efa\u6a21\uff0c\u5b9e\u73b0\u4e86\u4ece\u7c97\u5230\u7ec6\u7684\u76d1\u7763\u7b56\u7565\u3002 * \u67b6\u6784\u6539\u8fdb\uff1a \u5c06\u7ecf\u5178\u7684TransUNet\u67b6\u6784\u4e2d\u7684ViT\u9aa8\u5e72\u7f51\u7edc\u66ff\u6362\u4e3a\u7ecf\u8fc7\u75c5\u7406\u5b66\u9884\u8bad\u7ec3\u7684\u57fa\u7840\u6a21\u578b\uff08Uni\uff09\uff0c\u8fd9\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u63d0\u53d6\u66f4\u5177\u9886\u57df\u7279\u5f02\u6027\u548c\u9c81\u68d2\u6027\u7684\u75c5\u7406\u76f8\u5173\u7279\u5f81\u3002 * \u534a\u76d1\u7763\u5b66\u4e60\u8303\u5f0f\uff1a \u5728\u65e0\u6807\u7b7e\u6837\u672c\u4e0a\u901a\u8fc7\u8fd9\u79cd\u591a\u6a21\u6001\u539f\u578b\u5bf9\u9f50\u8fdb\u884c\u50cf\u7d20\u7ea7\u5bf9\u6bd4\u5b66\u4e60\uff0c\u6709\u6548\u5229\u7528\u4e86\u672a\u6807\u6ce8\u6570\u636e\u3002 3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd \u964d\u4f4e\u6807\u6ce8\u6210\u672c\uff1a \u901a\u8fc7\u9ad8\u6548\u7684\u534a\u76d1\u7763\u5b66\u4e60\uff0cMPAMatch\u6709\u671b\u5927\u5e45\u51cf\u5c11\u75c5\u7406\u56fe\u50cf\u5206\u5272\u6240\u9700\u7684\u50cf\u7d20\u7ea7\u6807\u6ce8\u5de5\u4f5c\u91cf\uff0c\u52a0\u901fAI\u5728\u6570\u5b57\u75c5\u7406\u9886\u57df\u7684\u5e94\u7528\u548c\u90e8\u7f72\u3002 \u63d0\u5347\u5206\u5272\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\uff1a \u5f15\u5165\u6587\u672c\u8bed\u4e49\u4fe1\u606f\uff0c\u7279\u522b\u662f\u5bf9\u6a21\u7cca\u8fb9\u754c\u7684\u5efa\u6a21\u80fd\u529b\uff0c\u5c06\u663e\u8457\u63d0\u9ad8\u75c5\u7406\u56fe\u50cf\u5206\u5272\u7684\u51c6\u786e\u6027\u548c\u5bf9\u590d\u6742\u7ed3\u6784\u7684\u7406\u89e3\u3002 \u5f00\u8f9f\u65b0\u7814\u7a76\u65b9\u5411\uff1a \u9996\u6b21\u5c06\u6587\u672c\u539f\u578b\u76d1\u7763\u5f15\u5165\u5206\u5272\u4efb\u52a1\uff0c\u4e3a\u591a\u6a21\u6001\u5b66\u4e60\u5728\u50cf\u7d20\u7ea7\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u5f00\u8f9f\u4e86\u65b0\u7684\u9014\u5f84\uff0c\u9f13\u52b1\u7814\u7a76\u8005\u63a2\u7d22\u66f4\u591a\u6a21\u6001\uff08\u5982\u57fa\u56e0\u7ec4\u6570\u636e\u3001\u4e34\u5e8a\u62a5\u544a\uff09\u4e0e\u56fe\u50cf\u5206\u5272\u7684\u7ed3\u5408\u3002 \u63a8\u52a8\u57fa\u7840\u6a21\u578b\u5728\u533b\u7597\u9886\u57df\u7684\u5e94\u7528\uff1a \u7ed3\u5408\u75c5\u7406\u9884\u8bad\u7ec3\u7684\u57fa\u7840\u6a21\u578b\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u7279\u5b9a\u9886\u57df\u4efb\u52a1\u4e2d\u7684\u5f3a\u5927\u7279\u5f81\u63d0\u53d6\u80fd\u529b\uff0c\u4e3a\u672a\u6765\u533b\u7597AI\u6a21\u578b\u7684\u5f00\u53d1\u63d0\u4f9b\u4e86\u8303\u4f8b\u3002 4. \u76f8\u5173\u9886\u57df\u6216\u5e94\u7528\u53d7\u76ca \u6570\u5b57\u75c5\u7406\u5b66\u548c\u8ba1\u7b97\u75c5\u7406\u5b66\uff1a \u80bf\u7624\u5206\u5272\u3001\u817a\u4f53\u5206\u5272\u3001\u7ec4\u7ec7\u7c7b\u578b\u8bc6\u522b\u3001\u75be\u75c5\u5206\u7ea7\u548c\u9884\u540e\u8bc4\u4f30\u7b49\u3002 \u533b\u5b66\u56fe\u50cf\u5206\u6790\uff1a \u5176\u4ed6\u9700\u8981\u7cbe\u7ec6\u50cf\u7d20\u7ea7\u5206\u5272\u4e14\u6807\u6ce8\u6210\u672c\u9ad8\u6602\u7684\u533b\u5b66\u5f71\u50cf\u4efb\u52a1\uff0c\u5982\u653e\u5c04\u5b66\u56fe\u50cf\uff08CT/MRI\uff09\u4e2d\u7684\u5668\u5b98\u6216\u75c5\u7076\u5206\u5272\u3002 \u534a\u76d1\u7763\u5b66\u4e60\uff1a \u4e3a\u901a\u7528\u534a\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u63d0\u4f9b\u4e86\u65b0\u7684\u591a\u6a21\u6001\u548c\u539f\u578b\u5f15\u5bfc\u7b56\u7565\u3002 \u591a\u6a21\u6001\u5b66\u4e60\uff1a \u63a8\u52a8\u4e86\u56fe\u50cf\u4e0e\u6587\u672c\u7b49\u591a\u6a21\u6001\u4fe1\u606f\u878d\u5408\u5728\u50cf\u7d20\u7ea7\u4efb\u52a1\u4e2d\u7684\u7814\u7a76\u3002 \u57fa\u7840\u6a21\u578b\u5e94\u7528\uff1a \u4e3a\u5982\u4f55\u5c06\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\uff08\u5982\u89c6\u89c9Transformer\uff09\u9002\u5e94\u5230\u7279\u5b9a\u9886\u57df\uff08\u5982\u533b\u7597\uff09\u5e76\u7ed3\u5408\u4e0b\u6e38\u4efb\u52a1\u63d0\u4f9b\u4e86\u5b9e\u8df5\u7ecf\u9a8c\u3002 5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u7684\u5c40\u9650\u6027 \u6587\u672c\u539f\u578b\u751f\u6210\u4e0e\u8d28\u91cf\uff1a \u6458\u8981\u4e2d\u672a\u8be6\u7ec6\u8bf4\u660e\u6587\u672c\u539f\u578b\u662f\u5982\u4f55\u751f\u6210\u6216\u83b7\u53d6\u7684\u3002\u9ad8\u8d28\u91cf\u3001\u5177\u6709\u4ee3\u8868\u6027\u7684\u6587\u672c\u539f\u578b\u5bf9\u4e8e\u8bed\u4e49\u76d1\u7763\u81f3\u5173\u91cd\u8981\uff0c\u5176\u751f\u6210\u8fc7\u7a0b\u53ef\u80fd\u590d\u6742\u4e14\u9700\u8981\u9886\u57df\u4e13\u5bb6\u77e5\u8bc6\uff0c\u751a\u81f3\u53ef\u80fd\u5f15\u5165\u4e3b\u89c2\u6027\u6216\u504f\u5dee\u3002 \u5bf9\u57fa\u7840\u6a21\u578b\u7684\u4f9d\u8d56\uff1a \u6027\u80fd\u53ef\u80fd\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u201c\u75c5\u7406\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\uff08Uni\uff09\u201d\u7684\u8d28\u91cf\u3001\u53ef\u7528\u6027\u53ca\u5176\u9884\u8bad\u7ec3\u6570\u636e\u7684\u89c4\u6a21\u548c\u591a\u6837\u6027\u3002\u5982\u679c\u8be5\u6a21\u578b\u4e0d\u516c\u5f00\u6216\u96be\u4ee5\u83b7\u53d6\uff0c\u5219\u65b9\u6cd5\u7684\u590d\u73b0\u548c\u63a8\u5e7f\u53ef\u80fd\u53d7\u9650\u3002 \u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\uff1a \u50cf\u7d20\u7ea7\u5bf9\u6bd4\u5b66\u4e60\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u5206\u8fa8\u7387\u75c5\u7406\u56fe\u50cf\u4e0a\uff0c\u901a\u5e38\u9700\u8981\u5927\u91cf\u7684\u8ba1\u7b97\u8d44\u6e90\uff08GPU\u5185\u5b58\u548c\u8ba1\u7b97\u65f6\u95f4\uff09\u3002 \u6cdb\u5316\u80fd\u529b\uff1a \u5c3d\u7ba1\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u4f46\u75c5\u7406\u56fe\u50cf\u7684\u590d\u6742\u6027\u548c\u591a\u6837\u6027\u6781\u9ad8\uff08\u4e0d\u540c\u67d3\u8272\u3001\u4e0d\u540c\u75be\u75c5\u7c7b\u578b\u3001\u4e0d\u540c\u626b\u63cf\u4eea\uff09\uff0c\u6587\u672c\u539f\u578b\u548c\u56fe\u50cf\u539f\u578b\u5728\u9762\u5bf9\u672a\u89c1\u8fc7\u7684\u65b0\u75c5\u7406\u7c7b\u578b\u6216\u7f55\u89c1\u75c5\u53d8\u65f6\u7684\u6cdb\u5316\u80fd\u529b\u4ecd\u9700\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u3002 \u53ef\u89e3\u91ca\u6027\uff1a \u591a\u6a21\u6001\u539f\u578b\u5bf9\u9f50\u548c\u5bf9\u6bd4\u5b66\u4e60\u7684\u5185\u90e8\u673a\u5236\u53ef\u80fd\u76f8\u5bf9\u590d\u6742\uff0c\u5176\u51b3\u7b56\u8fc7\u7a0b\u7684\u53ef\u89e3\u91ca\u6027\u53ef\u80fd\u4e0d\u5982\u4e00\u4e9b\u66f4\u7b80\u5355\u7684\u6a21\u578b\u3002 Key Findings: To address these limitations, we propose MPAMatch - a novel segmentation framework that performs pixel-level contrastive learning under a multimodal prototype-guided supervision paradigm. Extensive experiments on GLAS, EBHI-SEG-GLAND, EBHI-SEG-CANCER, and KPI show MPAMatch's superiority over state-of-the-art methods, validating its dual advantages in structural and semantic modeling. Links: PDF arXiv","title":"\u8bba\u6587\u6458\u8981\u5206\u6790\uff1aMultimodal Prototype Alignment for Semi-supervised Pathology Image Segmentation"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-29/#scalable-object-detection-in-the-car-interior-with-vision-foundation-models","text":"Authors: B\u00e1lint M\u00e9sz\u00e1ros, Ahmet Firintepe, Sebastian Schmidt, Stephan G\u00fcnnemann Published: 2025-08-27 Categories: cs.CV Abstract: AI tasks in the car interior like identifying and localizing externally introduced objects is crucial for response quality of personal assistants. However, computational resources of on-board systems remain highly constrained, restricting the deployment of such solutions directly within the vehicle. To address this limitation, we propose the novel Object Detection and Localization (ODAL) framework for interior scene understanding. Our approach leverages vision foundation models through a distributed architecture, splitting computational tasks between on-board and cloud. This design overcomes the resource constraints of running foundation models directly in the car. To benchmark model performance, we introduce ODALbench, a new metric for comprehensive assessment of detection and localization.Our analysis demonstrates the framework's potential to establish new standards in this domain. We compare the state-of-the-art GPT-4o vision foundation model with the lightweight LLaVA 1.5 7B model and explore how fine-tuning enhances the lightweight models performance. Remarkably, our fine-tuned ODAL-LLaVA model achieves an ODAL _{score} of 89%, representing a 71% improvement over its baseline performance and outperforming GPT-4o by nearly 20%. Furthermore, the fine-tuned model maintains high detection accuracy while significantly reducing hallucinations, achieving an ODAL _{SNR} three times higher than GPT-4o. Analysis: \u8fd9\u7bc7\u8bba\u6587\u6458\u8981\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5173\u4e8e\u5728\u8f66\u8f7d\u73af\u5883\u4e2d\u5e94\u7528\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u7269\u4f53\u68c0\u6d4b\u548c\u5b9a\u4f4d\u7684\u6709\u8da3\u89c6\u89d2\u3002\u4ee5\u4e0b\u662f\u6211\u7684\u5206\u6790\uff1a","title":"Scalable Object Detection in the Car Interior With Vision Foundation Models"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-29/#1-2-3_1","text":"\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aODAL\uff08Object Detection and Localization\uff09\u7684\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u8f66\u8f7d\u7cfb\u7edf\u8d44\u6e90\u53d7\u9650\u4e0b\uff0c\u5728\u6c7d\u8f66\u5185\u90e8\u7f72\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u7269\u4f53\u68c0\u6d4b\u548c\u5b9a\u4f4d\u7684\u6311\u6218\u3002\u901a\u8fc7\u91c7\u7528\u5206\u5e03\u5f0f\u67b6\u6784\uff0c\u5c06\u8ba1\u7b97\u4efb\u52a1\u5728\u8f66\u8f7d\u7aef\u548c\u4e91\u7aef\u4e4b\u95f4\u5206\u62c5\uff0cODAL\u6846\u67b6\u6210\u529f\u514b\u670d\u4e86\u76f4\u63a5\u5728\u8f66\u5185\u8fd0\u884c\u5927\u578b\u57fa\u7840\u6a21\u578b\u7684\u9650\u5236\u3002\u7814\u7a76\u8868\u660e\uff0c\u7ecf\u8fc7\u5fae\u8c03\u7684\u8f7b\u91cf\u7ea7ODAL-LLaVA\u6a21\u578b\u4e0d\u4ec5\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e86GPT-4o\uff0c\u8fd8\u663e\u8457\u964d\u4f4e\u4e86\u5e7b\u89c9\u73b0\u8c61\uff0c\u4e3a\u8f66\u8f7dAI\u4efb\u52a1\u6811\u7acb\u4e86\u65b0\u6807\u51c6\u3002","title":"1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u660e\u6458\u8981 (2-3 \u53e5\u8bdd)"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-29/#2_1","text":"\u5206\u5e03\u5f0f\u67b6\u6784\u5229\u7528\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff1a \u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u5176 \u5206\u5e03\u5f0fODAL\u6846\u67b6 \uff0c\u5b83\u5de7\u5999\u5730\u5c06\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u8ba1\u7b97\u4efb\u52a1\u5206\u89e3\uff0c\u4e00\u90e8\u5206\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8f66\u8f7d\u7aef\u6267\u884c\uff0c\u53e6\u4e00\u90e8\u5206\u5219\u5378\u8f7d\u5230\u4e91\u7aef\u3002\u8fd9\u4f7f\u5f97\u5728\u8f66\u5185\u73af\u5883\u4e2d\u4f7f\u7528\u901a\u5e38\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\u7684\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u6210\u4e3a\u53ef\u80fd\u3002 \u8f7b\u91cf\u7ea7\u6a21\u578b\u5fae\u8c03\u8d85\u8d8aSOTA\uff1a \u8bba\u6587\u5c55\u793a\u4e86\u901a\u8fc7\u5bf9LLaVA 1.5 7B\u8fd9\u6837\u7684\u8f7b\u91cf\u7ea7\u6a21\u578b\u8fdb\u884c \u5fae\u8c03 \uff0c\u4e0d\u4ec5\u80fd\u5927\u5e45\u63d0\u5347\u5176\u5728\u7279\u5b9a\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff08ODAL _{score} \u63d0\u534771%\uff09\uff0c\u751a\u81f3\u80fd\u8d85\u8d8a\u5f53\u524d\u6700\u5148\u8fdb\u7684\u901a\u7528\u6a21\u578b\uff08GPT-4o\uff09\uff0c\u5e76\u5728\u51cf\u5c11\u5e7b\u89c9\u65b9\u9762\u8868\u73b0\u66f4\u4f18\uff08ODAL _{SNR} \u662fGPT-4o\u7684\u4e09\u500d\uff09\u3002 \u65b0\u578b\u8bc4\u4f30\u6307\u6807ODALbench\uff1a \u5f15\u5165\u4e86 ODALbench \u8fd9\u4e00\u65b0\u7684\u5ea6\u91cf\u6807\u51c6\uff0c\u7528\u4e8e\u5168\u9762\u8bc4\u4f30\u68c0\u6d4b\u548c\u5b9a\u4f4d\u6027\u80fd\uff0c\u8fd9\u6709\u52a9\u4e8e\u5728\u8be5\u7279\u5b9a\u9886\u57df\u5efa\u7acb\u7edf\u4e00\u7684\u6027\u80fd\u8bc4\u4f30\u57fa\u51c6\u3002","title":"2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u8bba"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-29/#3_1","text":"\u63a8\u52a8\u8fb9\u7f18AI\u548c\u8f66\u8f7dAI\u53d1\u5c55\uff1a \u8be5\u7814\u7a76\u4e3a\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\uff08\u5982\u6c7d\u8f66\uff09\u4e0a\u90e8\u7f72\u590d\u6742AI\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u884c\u7684\u8303\u5f0f\u3002\u5b83\u53ef\u80fd\u52a0\u901f\u8f66\u8f7dAI\uff08\u5982\u667a\u80fd\u5ea7\u8231\u52a9\u624b\u3001\u9a7e\u9a76\u5458\u76d1\u63a7\u3001\u4e58\u5ba2\u5b89\u5168\uff09\u7684\u666e\u53ca\u548c\u80fd\u529b\u63d0\u5347\u3002 \u91cd\u65b0\u5b9a\u4e49\u57fa\u7840\u6a21\u578b\u90e8\u7f72\u7b56\u7565\uff1a \u8bba\u6587\u8bc1\u660e\u4e86\u5206\u5e03\u5f0f\u67b6\u6784\u5728\u5229\u7528\u5927\u578b\u57fa\u7840\u6a21\u578b\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u8fd9\u53ef\u80fd\u4fc3\u4f7f\u672a\u6765\u66f4\u591aAI\u5e94\u7528\u91c7\u7528\u6df7\u5408\uff08\u672c\u5730+\u4e91\u7aef\uff09\u90e8\u7f72\u7b56\u7565\uff0c\u4ee5\u5e73\u8861\u6027\u80fd\u3001\u8d44\u6e90\u548c\u6210\u672c\u3002 \u5f3a\u8c03\u8f7b\u91cf\u7ea7\u6a21\u578b\u5fae\u8c03\u7684\u91cd\u8981\u6027\uff1a \u7ed3\u679c\u8868\u660e\uff0c\u9488\u5bf9\u7279\u5b9a\u9886\u57df\u8fdb\u884c\u6df1\u5ea6\u5fae\u8c03\u7684\u8f7b\u91cf\u7ea7\u6a21\u578b\uff0c\u5176\u6027\u80fd\u53ef\u4ee5\u8d85\u8d8a\u901a\u7528\u578b\u5927\u578b\u6a21\u578b\uff0c\u8fd9\u53ef\u80fd\u4f1a\u9f13\u52b1\u7814\u7a76\u4eba\u5458\u548c\u5f00\u53d1\u8005\u66f4\u591a\u5730\u5173\u6ce8\u6a21\u578b\u6548\u7387\u548c\u9886\u57df\u9002\u5e94\u6027\uff0c\u800c\u975e\u4e00\u5473\u8ffd\u6c42\u6a21\u578b\u89c4\u6a21\u3002 \u5efa\u7acb\u65b0\u7684\u8bc4\u4f30\u6807\u51c6\uff1a ODALbench\u7684\u5f15\u5165\u6709\u671b\u6210\u4e3a\u8f66\u8f7d\u5185\u90e8\u7269\u4f53\u68c0\u6d4b\u548c\u5b9a\u4f4d\u4efb\u52a1\u7684\u884c\u4e1a\u6807\u51c6\uff0c\u4fc3\u8fdb\u66f4\u516c\u5e73\u3001\u66f4\u5168\u9762\u7684\u6a21\u578b\u6bd4\u8f83\u3002","title":"3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-29/#4_1","text":"\u667a\u80fd\u5ea7\u8231\u548c\u8f66\u8f7d\u52a9\u624b\uff1a \u76f4\u63a5\u53d7\u76ca\uff0c\u53ef\u5b9e\u73b0\u66f4\u7cbe\u51c6\u7684\u7269\u4f53\u8bc6\u522b\u548c\u5b9a\u4f4d\uff0c\u63d0\u5347\u8bed\u97f3\u52a9\u624b\u54cd\u5e94\u8d28\u91cf\u548c\u7528\u6237\u4f53\u9a8c\u3002 \u9a7e\u9a76\u5458\u548c\u4e58\u5ba2\u76d1\u63a7\uff1a \u7528\u4e8e\u68c0\u6d4b\u8f66\u5185\u5f02\u5e38\u7269\u54c1\u3001\u513f\u7ae5\u9057\u7559\u3001\u9a7e\u9a76\u5458\u75b2\u52b3\u6216\u5206\u5fc3\u7b49\uff0c\u63d0\u5347\u884c\u8f66\u5b89\u5168\u3002 \u8fb9\u7f18\u8ba1\u7b97\u548c\u7269\u8054\u7f51\uff08IoT\uff09\uff1a \u4efb\u4f55\u9700\u8981\u5f3a\u5927AI\u80fd\u529b\u4f46\u672c\u5730\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\uff08\u5982\u667a\u80fd\u5bb6\u5c45\u8bbe\u5907\u3001\u5de5\u4e1a\u673a\u5668\u4eba\u3001\u667a\u80fd\u6444\u50cf\u5934\uff09\u90fd\u53ef\u4ee5\u501f\u9274\u8fd9\u79cd\u5206\u5e03\u5f0f\u67b6\u6784\u3002 \u673a\u5668\u4eba\u5b66\uff1a \u673a\u5668\u4eba\u901a\u5e38\u5177\u6709\u6709\u9650\u7684\u677f\u8f7d\u8ba1\u7b97\u80fd\u529b\uff0c\u4f46\u53ef\u80fd\u9700\u8981\u590d\u6742\u7684\u89c6\u89c9\u7406\u89e3\u80fd\u529b\uff0c\u8be5\u65b9\u6cd5\u53ef\u7528\u4e8e\u5b9e\u73b0\u66f4\u9ad8\u7ea7\u7684\u611f\u77e5\u529f\u80fd\u3002 \u9690\u79c1\u8ba1\u7b97\uff1a \u867d\u7136\u6458\u8981\u672a\u63d0\u53ca\uff0c\u4f46\u5206\u5e03\u5f0f\u67b6\u6784\u5728\u672a\u6765\u53ef\u80fd\u4e0e\u8054\u90a6\u5b66\u4e60\u6216\u9690\u79c1\u4fdd\u62a4\u6280\u672f\u7ed3\u5408\uff0c\u4ee5\u5728\u4e91\u7aef\u5904\u7406\u6570\u636e\u65f6\u66f4\u597d\u5730\u4fdd\u62a4\u7528\u6237\u9690\u79c1\u3002","title":"4. \u76f8\u5173\u9886\u57df\u6216\u5e94\u7528"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-29/#5_1","text":"\u7f51\u7edc\u8fde\u63a5\u4f9d\u8d56\u6027\uff1a \u5206\u5e03\u5f0f\u67b6\u6784\u9ad8\u5ea6\u4f9d\u8d56\u7a33\u5b9a\u3001\u4f4e\u5ef6\u8fdf\u7684\u4e91\u7aef\u7f51\u7edc\u8fde\u63a5\u3002\u5728\u7f51\u7edc\u4fe1\u53f7\u5dee\u6216\u65e0\u4fe1\u53f7\u7684\u533a\u57df\uff0c\u7cfb\u7edf\u7684\u6027\u80fd\u53ef\u80fd\u4f1a\u4e25\u91cd\u4e0b\u964d\u751a\u81f3\u65e0\u6cd5\u5de5\u4f5c\u3002 \u9690\u79c1\u548c\u5b89\u5168\u95ee\u9898\uff1a \u5c06\u8f66\u5185\u573a\u666f\u6570\u636e\u4f20\u8f93\u5230\u4e91\u7aef\u8fdb\u884c\u5904\u7406\uff0c\u53ef\u80fd\u4f1a\u5f15\u53d1\u7528\u6237\u9690\u79c1\u548c\u6570\u636e\u5b89\u5168\u65b9\u9762\u7684\u62c5\u5fe7\uff0c\u9700\u8981\u5f3a\u5927\u7684\u52a0\u5bc6\u548c\u9690\u79c1\u4fdd\u62a4\u673a\u5236\u3002 \u5b9e\u65f6\u6027\u6311\u6218\uff1a \u5c3d\u7ba1\u89e3\u51b3\u4e86\u8d44\u6e90\u9650\u5236\uff0c\u4f46\u4e91\u7aef\u901a\u4fe1\u7684\u5f80\u8fd4\u5ef6\u8fdf\uff08latency\uff09\u5bf9\u4e8e\u67d0\u4e9b\u5bf9\u5b9e\u65f6\u6027\u8981\u6c42\u6781\u9ad8\u7684\u8f66\u8f7d\u4efb\u52a1\uff08\u5982\u7d27\u6025\u5b89\u5168\u54cd\u5e94\uff09\u53ef\u80fd\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002\u6458\u8981\u4e2d\u672a\u63d0\u4f9b\u5177\u4f53\u7684\u5ef6\u8fdf\u6307\u6807\u3002 \u8fd0\u8425\u6210\u672c\uff1a \u6301\u7eed\u5229\u7528\u4e91\u7aef\u8ba1\u7b97\u8d44\u6e90\u4f1a\u4ea7\u751f\u76f8\u5e94\u7684\u8fd0\u8425\u6210\u672c\uff0c\u8fd9\u5bf9\u4e8e\u5927\u89c4\u6a21\u90e8\u7f72\u548c\u5546\u4e1a\u5316\u53ef\u80fd\u662f\u4e00\u4e2a\u9700\u8981\u8003\u8651\u7684\u56e0\u7d20\u3002 \u6cdb\u5316\u80fd\u529b\uff1a \u5c3d\u7ba1ODAL-LLaVA\u5728\u201c\u6c7d\u8f66\u5185\u90e8\u201d\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5728\u5176\u4ed6\u901a\u7528\u7269\u4f53\u68c0\u6d4b\u6216\u5b9a\u4f4d\u4efb\u52a1\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u5982\u4f55\uff0c\u6458\u8981\u4e2d\u5e76\u672a\u63d0\u53ca\u3002\u5176\u9ad8\u6027\u80fd\u53ef\u80fd\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u9488\u5bf9\u7279\u5b9a\u9886\u57df\u8fdb\u884c\u7684\u5fae\u8c03\u3002 Key Findings: To address this limitation, we propose the novel Object Detection and Localization (ODAL) framework for interior scene understanding. Our approach leverages vision foundation models through a distributed architecture, splitting computational tasks between on-board and cloud. To benchmark model performance, we introduce ODALbench, a new metric for comprehensive assessment of detection and localization.Our analysis demonstrates the framework's potential to establish new standards in this domain. We compare the state-of-the-art GPT-4o vision foundation model with the lightweight LLaVA 1.5 7B model and explore how fine-tuning enhances the lightweight models performance. Links: PDF arXiv","title":"5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u7684\u5c40\u9650\u6027"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-29/#generalizing-monocular-3d-object-detection","text":"Authors: Abhinav Kumar Published: 2025-08-27 Categories: cs.CV Abstract: Monocular 3D object detection (Mono3D) is a fundamental computer vision task that estimates an object's class, 3D position, dimensions, and orientation from a single image. Its applications, including autonomous driving, augmented reality, and robotics, critically rely on accurate 3D environmental understanding. This thesis addresses the challenge of generalizing Mono3D models to diverse scenarios, including occlusions, datasets, object sizes, and camera parameters. To enhance occlusion robustness, we propose a mathematically differentiable NMS (GrooMeD-NMS). To improve generalization to new datasets, we explore depth equivariant (DEVIANT) backbones. We address the issue of large object detection, demonstrating that it's not solely a data imbalance or receptive field problem but also a noise sensitivity issue. To mitigate this, we introduce a segmentation-based approach in bird's-eye view with dice loss (SeaBird). Finally, we mathematically analyze the extrapolation of Mono3D models to unseen camera heights and improve Mono3D generalization in such out-of-distribution settings. Analysis: \u8fd9\u7bc7\u8bba\u6587\u7684\u6458\u8981\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5173\u4e8e\u5355\u76ee3D\u76ee\u6807\u68c0\u6d4b\uff08Mono3D\uff09\u9886\u57df\u91cd\u8981\u8fdb\u5c55\u7684\u6982\u89c8\uff0c\u7279\u522b\u5173\u6ce8\u5176\u6cdb\u5316\u80fd\u529b\u3002\u4ee5\u4e0b\u662f\u8be6\u7ec6\u5206\u6790\uff1a","title":"Generalizing Monocular 3D Object Detection"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-29/#1-concise-summary_1","text":"\u8fd9\u7bc7\u8bba\u6587\u7cfb\u7edf\u6027\u5730\u89e3\u51b3\u4e86\u5355\u76ee3D\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u5728\u590d\u6742\u548c\u591a\u6837\u5316\u573a\u666f\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u6311\u6218\u3002\u4f5c\u8005\u901a\u8fc7\u63d0\u51fa\u4e00\u7cfb\u5217\u521b\u65b0\u65b9\u6cd5\uff0c\u5206\u522b\u63d0\u5347\u4e86\u6a21\u578b\u5728\u5904\u7406\u906e\u6321\u3001\u9002\u5e94\u65b0\u6570\u636e\u96c6\u3001\u68c0\u6d4b\u5927\u578b\u7269\u4f53\u4ee5\u53ca\u5e94\u5bf9\u4e0d\u540c\u76f8\u673a\u53c2\u6570\uff08\u7279\u522b\u662f\u76f8\u673a\u9ad8\u5ea6\uff09\u65f6\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002\u5176\u6838\u5fc3\u8d21\u732e\u5728\u4e8e\u7cfb\u7edf\u6027\u5730\u589e\u5f3aMono3D\u6a21\u578b\u7684\u5b9e\u7528\u6027\u548c\u53ef\u9760\u6027\u3002","title":"1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u6d01\u603b\u7ed3 (Concise Summary)"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-29/#2-key-innovation-or-methodological-approach_1","text":"\u8be5\u8bba\u6587\u7684\u5173\u952e\u521b\u65b0\u5728\u4e8e\u5176 \u591a\u7ba1\u9f50\u4e0b\u3001\u7cfb\u7edf\u6027\u5730\u89e3\u51b3Mono3D\u6cdb\u5316\u95ee\u9898\u7684\u7b56\u7565 \u3002\u5b83\u5e76\u975e\u4f9d\u8d56\u5355\u4e00\u7684\u7a81\u7834\u6027\u7b97\u6cd5\uff0c\u800c\u662f\u9488\u5bf9\u6cdb\u5316\u80fd\u529b\u7684\u591a\u4e2a\u5177\u4f53\u7ef4\u5ea6\uff08\u906e\u6321\u3001\u6570\u636e\u96c6\u3001\u7269\u4f53\u5927\u5c0f\u3001\u76f8\u673a\u53c2\u6570\uff09\u63d0\u51fa\u4e86\u5b9a\u5236\u5316\u7684\u89e3\u51b3\u65b9\u6848\uff1a GrooMeD-NMS (Occlusion Robustness): \u63d0\u51fa\u4e86\u4e00\u79cd \u6570\u5b66\u53ef\u5fae\u5206\u7684\u975e\u6781\u5927\u503c\u6291\u5236\uff08NMS\uff09 \u65b9\u6cd5\uff0c\u65e8\u5728\u63d0\u9ad8\u6a21\u578b\u5728\u5b58\u5728\u906e\u6321\u60c5\u51b5\u4e0b\u7684\u68c0\u6d4b\u6027\u80fd\u3002\u53ef\u5fae\u5206NMS\u5141\u8bb8\u7aef\u5230\u7aef\u8bad\u7ec3\uff0c\u66f4\u597d\u5730\u4f18\u5316\u906e\u6321\u573a\u666f\u4e0b\u7684\u68c0\u6d4b\u7ed3\u679c\u3002 DEVIANT Backbones (Dataset Generalization): \u63a2\u7d22\u4e86 \u6df1\u5ea6\u7b49\u53d8\uff08depth equivariant\uff09\u9aa8\u5e72\u7f51\u7edc \uff0c\u4ee5\u589e\u5f3a\u6a21\u578b\u5bf9\u65b0\u6570\u636e\u96c6\u7684\u9002\u5e94\u6027\u3002\u6df1\u5ea6\u7b49\u53d8\u6027\u610f\u5473\u7740\u6a21\u578b\u5bf9\u8f93\u5165\u6df1\u5ea6\u56fe\u7684\u53d8\u6362\u5177\u6709\u4e00\u81f4\u7684\u54cd\u5e94\uff0c\u8fd9\u5bf9\u4e8e\u8de8\u6570\u636e\u96c6\u7684\u6cdb\u5316\u81f3\u5173\u91cd\u8981\u3002 SeaBird (Large Object Detection): \u9488\u5bf9\u5927\u578b\u7269\u4f53\u68c0\u6d4b\u4e2d\u7684\u566a\u58f0\u654f\u611f\u6027\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd \u57fa\u4e8e\u9e1f\u77b0\u56fe\uff08BEV\uff09\u7684\u5206\u5272\u65b9\u6cd5\uff0c\u5e76\u7ed3\u5408Dice\u635f\u5931 \u3002\u8fd9\u8868\u660e\u4f5c\u8005\u5c06\u5927\u578b\u7269\u4f53\u68c0\u6d4b\u91cd\u65b0\u5b9a\u4e49\u4e3a\u4e00\u4e2a\u5206\u5272\u4efb\u52a1\uff0c\u5e76\u5229\u7528BEV\u7684\u4f18\u52bf\u6765\u964d\u4f4e\u566a\u58f0\u5f71\u54cd\u3002 Mathematical Analysis for Camera Parameters: \u5bf9Mono3D\u6a21\u578b\u5728 \u672a\u89c1\u8fc7\u7684\u76f8\u673a\u9ad8\u5ea6 \u7b49\u5206\u5e03\u5916\uff08OOD\uff09\u8bbe\u7f6e\u4e0b\u7684\u5916\u63a8\u80fd\u529b\u8fdb\u884c\u4e86 \u6570\u5b66\u5206\u6790 \uff0c\u5e76\u636e\u6b64\u6539\u8fdb\u4e86\u6cdb\u5316\u6027\u80fd\u3002\u8fd9\u4e3a\u7406\u89e3\u548c\u63d0\u5347\u6a21\u578b\u5728\u4e0d\u540c\u76f8\u673a\u914d\u7f6e\u4e0b\u7684\u8868\u73b0\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002","title":"2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u5b66\u65b9\u6cd5 (Key Innovation or Methodological Approach)"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-29/#3-potential-impact-on-the-field_1","text":"\u8fd9\u9879\u7814\u7a76\u5982\u679c\u6210\u529f\uff0c\u5c06\u5bf9\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u4ea7\u751f\u663e\u8457\u5f71\u54cd\uff0c\u5c24\u5176\u662f\u5728\u4f9d\u8d563D\u611f\u77e5\u7684\u5b9e\u9645\u5e94\u7528\u4e2d\uff1a \u63d0\u5347Mono3D\u7684\u5b9e\u7528\u6027\uff1a \u901a\u8fc7\u89e3\u51b3\u6cdb\u5316\u6027\u8fd9\u4e00\u6838\u5fc3\u6311\u6218\uff0c\u8be5\u7814\u7a76\u5c06\u4f7f\u5355\u76ee3D\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u590d\u6742\u591a\u53d8\u7684\u73af\u5883\u4e2d\u66f4\u52a0\u53ef\u9760\u548c\u5b9e\u7528\uff0c\u4ece\u800c\u52a0\u901f\u5176\u5728\u81ea\u52a8\u9a7e\u9a76\u3001\u589e\u5f3a\u73b0\u5b9e\u548c\u673a\u5668\u4eba\u7b49\u9886\u57df\u7684\u90e8\u7f72\u3002 \u63a8\u52a8\u6cdb\u5316\u6027\u7814\u7a76\uff1a \u8bba\u6587\u9488\u5bf9\u4e0d\u540c\u6cdb\u5316\u6311\u6218\u63d0\u51fa\u7684\u5177\u4f53\u89e3\u51b3\u65b9\u6848\uff08\u5982\u53ef\u5fae\u5206NMS\u3001\u6df1\u5ea6\u7b49\u53d8\u7f51\u7edc\u3001BEV\u5206\u5272\u65b9\u6cd5\uff09\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u548c\u5de5\u5177\uff0c\u9f13\u52b1\u7814\u7a76\u4eba\u5458\u66f4\u7ec6\u81f4\u5730\u5206\u6790\u548c\u89e3\u51b3\u6a21\u578b\u6cdb\u5316\u95ee\u9898\u3002 \u7406\u8bba\u4e0e\u5b9e\u8df5\u7ed3\u5408\uff1a \u5bf9\u76f8\u673a\u53c2\u6570\u7684\u6570\u5b66\u5206\u6790\uff0c\u7ed3\u5408\u5177\u4f53\u7684\u7b97\u6cd5\u6539\u8fdb\uff0c\u4f53\u73b0\u4e86\u7406\u8bba\u6307\u5bfc\u5b9e\u8df5\u7684\u4ef7\u503c\uff0c\u6709\u52a9\u4e8e\u6784\u5efa\u66f4\u5177\u9c81\u68d2\u6027\u7684\u6a21\u578b\u3002","title":"3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd (Potential Impact on the Field)"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-29/#4-related-areas-or-applications-that-might-benefit","text":"\u9664\u4e86\u6458\u8981\u4e2d\u660e\u786e\u63d0\u5230\u7684 \u81ea\u52a8\u9a7e\u9a76\u3001\u589e\u5f3a\u73b0\u5b9e\u548c\u673a\u5668\u4eba\u6280\u672f \uff0c\u4ee5\u4e0b\u9886\u57df\u548c\u5e94\u7528\u4e5f\u53ef\u80fd\u4ece\u8fd9\u9879\u7814\u7a76\u4e2d\u53d7\u76ca\uff1a 3D\u573a\u666f\u7406\u89e3\uff1a \u4efb\u4f55\u9700\u8981\u4ece\u5355\u76ee\u56fe\u50cf\u8fdb\u884c\u7cbe\u786e3D\u573a\u666f\u91cd\u5efa\u548c\u7406\u89e3\u7684\u4efb\u52a1\uff0c\u4f8b\u5982\u667a\u80fd\u57ce\u5e02\u76d1\u63a7\u3001\u65e0\u4eba\u673a\u5de1\u68c0\u3002 \u4eba\u673a\u4ea4\u4e92\uff1a \u9700\u8981\u5b9e\u65f6\u611f\u77e5\u7528\u62373D\u59ff\u6001\u6216\u624b\u52bf\u7684\u5e94\u7528\uff0c\u5982\u865a\u62df\u73b0\u5b9e\uff08VR\uff09\u4e2d\u7684\u4ea4\u4e92\u3001\u667a\u80fd\u5bb6\u5c45\u63a7\u5236\u3002 \u5de5\u4e1a\u68c0\u6d4b\u4e0e\u6d4b\u91cf\uff1a \u5728\u751f\u4ea7\u7ebf\u4e0a\u8fdb\u884c\u7269\u4f53\u5c3a\u5bf8\u3001\u4f4d\u7f6e\u548c\u7f3a\u9677\u76843D\u68c0\u6d4b\uff0c\u5c24\u5176\u662f\u5728\u6210\u672c\u654f\u611f\u6216\u7a7a\u95f4\u53d7\u9650\u7684\u573a\u666f\u4e0b\u3002 \u533b\u5b66\u5f71\u50cf\u5206\u6790\uff1a \u5982\u679c\u80fd\u5c062D\u533b\u5b66\u56fe\u50cf\u8f6c\u5316\u4e3a3D\u7ed3\u6784\uff0c\u5c06\u6709\u52a9\u4e8e\u75be\u75c5\u8bca\u65ad\u548c\u6cbb\u7597\u89c4\u5212\u3002 \u8ba1\u7b97\u673a\u56fe\u5f62\u5b66\uff1a \u8f85\u52a93D\u6a21\u578b\u751f\u6210\u3001\u573a\u666f\u91cd\u5efa\u548c\u865a\u62df\u73af\u5883\u7684\u521b\u5efa\u3002","title":"4. \u76f8\u5173\u9886\u57df\u6216\u5e94\u7528\u53d7\u76ca (Related Areas or Applications that might benefit)"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-29/#5-limitations-that-can-be-inferred-from-the-abstract","text":"\u5c3d\u7ba1\u6458\u8981\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u8d21\u732e\uff0c\u4f46\u4ecd\u53ef\u63a8\u65ad\u51fa\u4e00\u4e9b\u6f5c\u5728\u7684\u5c40\u9650\u6027\uff1a \u6a21\u578b\u590d\u6742\u6027\u589e\u52a0\uff1a \u5f15\u5165\u591a\u79cd\u4e13\u95e8\u7684\u6a21\u5757\uff08GrooMeD-NMS\u3001DEVIANT\u9aa8\u5e72\u3001SeaBird\uff09\u53ef\u80fd\u4f1a\u663e\u8457\u589e\u52a0\u6a21\u578b\u7684\u6574\u4f53\u590d\u6742\u6027\u3001\u8ba1\u7b97\u6210\u672c\u548c\u5185\u5b58\u9700\u6c42\uff0c\u8fd9\u53ef\u80fd\u5bf9\u5b9e\u65f6\u5e94\u7528\u6216\u8d44\u6e90\u53d7\u9650\u7684\u8bbe\u5907\u6784\u6210\u6311\u6218\u3002 \u6a21\u5757\u95f4\u534f\u540c\u4e0e\u96c6\u6210\uff1a \u6458\u8981\u5c06\u8fd9\u4e9b\u89e3\u51b3\u65b9\u6848\u89c6\u4e3a\u72ec\u7acb\u7684\u95ee\u9898\u89e3\u51b3\u8005\u3002\u5982\u4f55\u6709\u6548\u5730\u5c06\u8fd9\u4e9b\u4e0d\u540c\u7684\u7ec4\u4ef6\u96c6\u6210\u5230\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\u4e2d\uff0c\u5e76\u786e\u4fdd\u5b83\u4eec\u4e4b\u95f4\u826f\u597d\u7684\u534f\u540c\u4f5c\u7528\u800c\u975e\u76f8\u4e92\u5e72\u6270\uff0c\u662f\u4e00\u4e2a\u6f5c\u5728\u7684\u6311\u6218\u3002 \u6cdb\u5316\u8303\u56f4\u7684\u5b8c\u6574\u6027\uff1a \u5c3d\u7ba1\u89e3\u51b3\u4e86\u906e\u6321\u3001\u6570\u636e\u96c6\u3001\u7269\u4f53\u5927\u5c0f\u548c\u76f8\u673a\u53c2\u6570\u7b49\u5173\u952e\u6cdb\u5316\u7ef4\u5ea6\uff0c\u4f46\u201c\u6cdb\u5316\u201d\u662f\u4e00\u4e2a\u975e\u5e38\u5e7f\u6cdb\u7684\u6982\u5ff5\u3002\u6458\u8981\u5e76\u672a\u63d0\u53ca\u5bf9\u5176\u4ed6\u91cd\u8981\u56e0\u7d20\uff08\u5982\u6781\u7aef\u5149\u7167\u53d8\u5316\u3001\u6076\u52a3\u5929\u6c14\u6761\u4ef6\u3001\u7269\u4f53\u6750\u8d28\u591a\u6837\u6027\u3001\u8fd0\u52a8\u6a21\u7cca\u7b49\uff09\u7684\u6cdb\u5316\u80fd\u529b\u3002 \u5355\u76ee\u89c6\u89c9\u7684\u56fa\u6709\u5c40\u9650\uff1a \u65e0\u8bba\u6a21\u578b\u5982\u4f55\u4f18\u5316\uff0c\u5355\u76ee3D\u68c0\u6d4b\u59cb\u7ec8\u9762\u4e34\u4ece2D\u56fe\u50cf\u63a8\u65ad3D\u6df1\u5ea6\u4fe1\u606f\u7684\u56fa\u6709\u6a21\u7cca\u6027\u3002\u8be5\u5de5\u4f5c\u65e8\u5728\u7f13\u89e3\u8fd9\u4e9b\u95ee\u9898\uff0c\u4f46\u65e0\u6cd5\u4ece\u6839\u672c\u4e0a\u6d88\u9664\u5355\u76ee\u8f93\u5165\u7684\u5c40\u9650\u6027\uff0c\u5176\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u53ef\u80fd\u4ecd\u96be\u4ee5\u4e0e\u591a\u76ee\u6216\u6df1\u5ea6\u4f20\u611f\u5668\u65b9\u6848\u5ab2\u7f8e\u3002 \u201c\u63a2\u7d22\u201d\u7684\u542b\u4e49\uff1a \u5bf9\u4e8eDEVIANT\u9aa8\u5e72\u7f51\u7edc\uff0c\u6458\u8981\u4f7f\u7528\u4e86\u201cwe explore\u201d\uff08\u6211\u4eec\u63a2\u7d22\uff09\uff0c\u8fd9\u53ef\u80fd\u6697\u793a\u8be5\u65b9\u5411\u4ecd\u5728\u7814\u7a76\u4e2d\uff0c\u5176\u6700\u7ec8\u7684\u6cdb\u5316\u6548\u679c\u548c\u7a33\u5b9a\u6027\u53ef\u80fd\u5c1a\u672a\u5b8c\u5168\u9a8c\u8bc1\u6216\u8fbe\u5230\u6700\u4f73\u72b6\u6001\u3002 Key Findings: To enhance occlusion robustness, we propose a mathematically differentiable NMS (GrooMeD-NMS). To improve generalization to new datasets, we explore depth equivariant (DEVIANT) backbones. To mitigate this, we introduce a segmentation-based approach in bird's-eye view with dice loss (SeaBird). Links: PDF arXiv","title":"5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u7684\u5c40\u9650\u6027 (Limitations that can be inferred from the abstract)"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-29/#craftgraffiti-exploring-human-identity-with-custom-graffiti-art-via-facial-preserving-diffusion-models","text":"Authors: Ayan Banerjee, Fernando Vilari\u00f1o, Josep Llad\u00f3s Published: 2025-08-28 Categories: cs.CV Abstract: Preserving facial identity under extreme stylistic transformation remains a major challenge in generative art. In graffiti, a high-contrast, abstract medium, subtle distortions to the eyes, nose, or mouth can erase the subject's recognizability, undermining both personal and cultural authenticity. We present CraftGraffiti, an end-to-end text-guided graffiti generation framework designed with facial feature preservation as a primary objective. Given an input image and a style and pose descriptive prompt, CraftGraffiti first applies graffiti style transfer via LoRA-fine-tuned pretrained diffusion transformer, then enforces identity fidelity through a face-consistent self-attention mechanism that augments attention layers with explicit identity embeddings. Pose customization is achieved without keypoints, using CLIP-guided prompt extension to enable dynamic re-posing while retaining facial coherence. We formally justify and empirically validate the \"style-first, identity-after\" paradigm, showing it reduces attribute drift compared to the reverse order. Quantitative results demonstrate competitive facial feature consistency and state-of-the-art aesthetic and human preference scores, while qualitative analyses and a live deployment at the Cruilla Festival highlight the system's real-world creative impact. CraftGraffiti advances the goal of identity-respectful AI-assisted artistry, offering a principled approach for blending stylistic freedom with recognizability in creative AI applications. Analysis: \u8fd9\u7bc7\u8bba\u6587\u300aCraftGraffiti: Exploring Human Identity with Custom Graffiti Art via Facial-Preserving Diffusion Models\u300b\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u751f\u6210\u5f0fAI\u9886\u57df\u63d0\u51fa\u4e86\u4e00\u4e2a\u5f15\u4eba\u6ce8\u76ee\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u6781\u7aef\u98ce\u683c\u5316\u8f6c\u6362\u65f6\u7684\u4eba\u8138\u8eab\u4efd\u4fdd\u6301\u95ee\u9898\u3002","title":"CraftGraffiti: Exploring Human Identity with Custom Graffiti Art via Facial-Preserving Diffusion Models"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-29/#1","text":"CraftGraffiti \u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u3001\u6587\u672c\u5f15\u5bfc\u7684\u6d82\u9e26\u751f\u6210\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u5728\u6781\u7aef\u98ce\u683c\u5316\uff08\u5982\u6d82\u9e26\uff09\u4e0b\u4eba\u8138\u8eab\u4efd\u96be\u4ee5\u4fdd\u6301\u7684\u6311\u6218\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u7ed3\u5408 LoRA \u5fae\u8c03\u7684\u6269\u6563\u6a21\u578b\u8fdb\u884c\u98ce\u683c\u8fc1\u79fb\uff0c\u5e76\u5f15\u5165\u4e00\u79cd\u5e26\u6709\u663e\u5f0f\u8eab\u4efd\u5d4c\u5165\u7684\u4eba\u8138\u4e00\u81f4\u81ea\u6ce8\u610f\u529b\u673a\u5236\u6765\u5f3a\u5236\u4fdd\u6301\u8eab\u4efd\uff0c\u540c\u65f6\u5229\u7528 CLIP \u5f15\u5bfc\u7684\u63d0\u793a\u6269\u5c55\u5b9e\u73b0\u65e0\u5173\u952e\u70b9\u7684\u59ff\u6001\u5b9a\u5236\u3002\u8bba\u6587\u7684\u6838\u5fc3\u8d21\u732e\u5728\u4e8e\u9a8c\u8bc1\u4e86\u201c\u5148\u98ce\u683c\uff0c\u540e\u8eab\u4efd\u201d\u7684\u5904\u7406\u8303\u5f0f\u80fd\u6709\u6548\u51cf\u5c11\u5c5e\u6027\u6f02\u79fb\uff0c\u5e76\u5728\u8eab\u4efd\u4e00\u81f4\u6027\u3001\u7f8e\u5b66\u548c\u7528\u6237\u504f\u597d\u65b9\u9762\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002","title":"1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u660e\u603b\u7ed3"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-29/#2_2","text":"\u8be5\u8bba\u6587\u7684\u5173\u952e\u521b\u65b0\u548c\u65b9\u6cd5\u8bba\u4f53\u73b0\u5728\u4ee5\u4e0b\u51e0\u4e2a\u65b9\u9762\uff1a \u201c\u5148\u98ce\u683c\uff0c\u540e\u8eab\u4efd\u201d\u8303\u5f0f\uff1a \u8bba\u6587\u5f62\u5f0f\u5316\u5730\u8bba\u8bc1\u5e76\u7ecf\u9a8c\u6027\u5730\u9a8c\u8bc1\u4e86\u201c\u5148\u98ce\u683c\u8fc1\u79fb\uff0c\u540e\u8eab\u4efd\u4fdd\u6301\u201d\u7684\u5904\u7406\u987a\u5e8f\u4f18\u4e8e\u53cd\u5411\u987a\u5e8f\uff0c\u80fd\u6709\u6548\u51cf\u5c11\u751f\u6210\u8fc7\u7a0b\u4e2d\u5c5e\u6027\u7684\u6f02\u79fb\uff0c\u8fd9\u4e3a\u591a\u76ee\u6807\u751f\u6210\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u4e2a\u91cd\u8981\u7684\u8bbe\u8ba1\u539f\u5219\u3002 \u4eba\u8138\u4e00\u81f4\u81ea\u6ce8\u610f\u529b\u673a\u5236\u4e0e\u663e\u5f0f\u8eab\u4efd\u5d4c\u5165\uff1a \u4e3a\u4e86\u5728\u98ce\u683c\u5316\u540e\u5f3a\u5236\u4fdd\u6301\u8eab\u4efd\uff0cCraftGraffiti \u5728\u6269\u6563\u6a21\u578b\u7684\u6ce8\u610f\u529b\u5c42\u4e2d\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4eba\u8138\u4e00\u81f4\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5e76\u8f85\u4ee5\u663e\u5f0f\u8eab\u4efd\u5d4c\u5165\u3002\u8fd9\u79cd\u673a\u5236\u80fd\u591f\u5f15\u5bfc\u6a21\u578b\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u66f4\u5173\u6ce8\u5e76\u4fdd\u7559\u5173\u952e\u7684\u9762\u90e8\u7279\u5f81\u3002 \u65e0\u5173\u952e\u70b9\u7684\u59ff\u6001\u5b9a\u5236\uff1a \u4f20\u7edf\u7684\u59ff\u6001\u63a7\u5236\u901a\u5e38\u4f9d\u8d56\u4e8e\u5173\u952e\u70b9\u68c0\u6d4b\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u4f7f\u7528 CLIP \u5f15\u5bfc\u7684\u63d0\u793a\u6269\u5c55\u6765\u5b9e\u73b0\u52a8\u6001\u7684\u59ff\u6001\u91cd\u65b0\u5b9a\u4f4d\uff0c\u540c\u65f6\u4fdd\u6301\u9762\u90e8\u8fde\u8d2f\u6027\uff0c\u907f\u514d\u4e86\u5bf9\u663e\u5f0f\u5173\u952e\u70b9\u6807\u6ce8\u7684\u4f9d\u8d56\uff0c\u63d0\u9ad8\u4e86\u7075\u6d3b\u6027\u3002 LoRA \u5fae\u8c03\u7684\u6269\u6563\u6a21\u578b\uff1a \u5229\u7528 LoRA (Low-Rank Adaptation) \u5bf9\u9884\u8bad\u7ec3\u7684\u6269\u6563 Transformer \u8fdb\u884c\u5fae\u8c03\uff0c\u4ee5\u9ad8\u6548\u5730\u5b9e\u73b0\u6d82\u9e26\u98ce\u683c\u8fc1\u79fb\uff0c\u8fd9\u662f\u4e00\u79cd\u5728\u4fdd\u6301\u6a21\u578b\u5927\u90e8\u5206\u53c2\u6570\u4e0d\u53d8\u7684\u60c5\u51b5\u4e0b\uff0c\u6709\u6548\u9002\u5e94\u7279\u5b9a\u98ce\u683c\u7684\u7b56\u7565\u3002","title":"2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u8bba"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-29/#3_2","text":"\u63a8\u52a8\u8eab\u4efd\u4fdd\u6301\u751f\u6210\u6280\u672f\uff1a \u8be5\u7814\u7a76\u4e3a\u5728\u6781\u7aef\u98ce\u683c\u5316\u573a\u666f\u4e0b\u4fdd\u6301\u4eba\u8138\u8eab\u4efd\u63d0\u4f9b\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u7684\u65b9\u6cd5\uff0c\u8fd9\u5bf9\u4e8e\u751f\u6210\u5f0fAI\u9886\u57df\u662f\u4e00\u4e2a\u957f\u671f\u5b58\u5728\u7684\u6311\u6218\u3002\u5b83\u4e3a\u672a\u6765\u5f00\u53d1\u66f4\u9c81\u68d2\u3001\u66f4\u5177\u8eab\u4efd\u610f\u8bc6\u7684\u751f\u6210\u6a21\u578b\u5960\u5b9a\u4e86\u57fa\u7840\u3002 \u8d4b\u80fd\u201c\u8eab\u4efd\u5c0a\u91cd\u578bAI\u827a\u672f\u201d\uff1a \u8bba\u6587\u660e\u786e\u63d0\u51fa\u4e86\u201c\u8eab\u4efd\u5c0a\u91cd\u578bAI\u8f85\u52a9\u827a\u672f\u201d\u7684\u76ee\u6807\uff0c\u8fd9\u5728AI\u827a\u672f\u521b\u4f5c\u4e2d\u5177\u6709\u91cd\u8981\u7684\u4f26\u7406\u548c\u5b9e\u9645\u610f\u4e49\u3002\u5b83\u4f7f\u5f97\u7528\u6237\u80fd\u591f\u5728\u4eab\u53d7\u827a\u672f\u81ea\u7531\u7684\u540c\u65f6\uff0c\u786e\u4fdd\u5176\u4e2a\u4eba\u8eab\u4efd\uff08\u5c24\u5176\u662f\u9762\u90e8\u7279\u5f81\uff09\u5f97\u5230\u51c6\u786e\u7684\u4fdd\u7559\uff0c\u4ece\u800c\u589e\u5f3a\u7528\u6237\u4f53\u9a8c\u548c\u4fe1\u4efb\u3002 \u542f\u53d1\u591a\u76ee\u6807\u751f\u6210\u4efb\u52a1\u8bbe\u8ba1\uff1a \u201c\u5148\u98ce\u683c\uff0c\u540e\u8eab\u4efd\u201d\u7684\u8303\u5f0f\u9a8c\u8bc1\uff0c\u4e3a\u5176\u4ed6\u9700\u8981\u5e73\u8861\u591a\u4e2a\u76f8\u4e92\u51b2\u7a81\u7684\u751f\u6210\u76ee\u6807\uff08\u5982\u98ce\u683c\u3001\u5185\u5bb9\u3001\u8eab\u4efd\u3001\u59ff\u6001\u7b49\uff09\u7684\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u6307\u5bfc\u3002 \u6269\u5c55\u6269\u6563\u6a21\u578b\u7684\u5e94\u7528\uff1a \u901a\u8fc7\u7ed3\u5408 LoRA\u3001\u81ea\u5b9a\u4e49\u6ce8\u610f\u529b\u673a\u5236\u548c CLIP \u5f15\u5bfc\uff0c\u5c55\u793a\u4e86\u6269\u6563\u6a21\u578b\u5728\u590d\u6742\u3001\u9ad8\u4fdd\u771f\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u5f3a\u5927\u6f5c\u529b\u548c\u7075\u6d3b\u6027\u3002","title":"3. \u5bf9\u9886\u57df\u7684\u6f5c\u5728\u5f71\u54cd"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-29/#4_2","text":"\u8fd9\u9879\u7814\u7a76\u7684\u6210\u679c\u53ef\u4ee5\u5e7f\u6cdb\u5e94\u7528\u4e8e\u4ee5\u4e0b\u9886\u57df\uff1a \u4e2a\u6027\u5316\u5185\u5bb9\u521b\u4f5c\uff1a \u7528\u6237\u53ef\u4ee5\u751f\u6210\u9ad8\u5ea6\u98ce\u683c\u5316\u4f46\u4ecd\u80fd\u8bc6\u522b\u51fa\u81ea\u5df1\u9762\u90e8\u7684\u5934\u50cf\u3001\u793e\u4ea4\u5a92\u4f53\u56fe\u7247\u3001\u6570\u5b57\u827a\u672f\u4f5c\u54c1\u7b49\u3002 \u865a\u62df\u8bd5\u7a7f\u4e0e\u6570\u5b57\u65f6\u5c1a\uff1a \u5728\u865a\u62df\u73af\u5883\u4e2d\u5c55\u793a\u670d\u88c5\u6216\u914d\u9970\u65f6\uff0c\u786e\u4fdd\u6a21\u7279\u7684\u8138\u90e8\u7279\u5f81\u4fdd\u6301\u4e00\u81f4\uff0c\u589e\u5f3a\u771f\u5b9e\u611f\u548c\u4e2a\u6027\u5316\u4f53\u9a8c\u3002 \u6e38\u620f\u4e0e\u52a8\u753b\u89d2\u8272\u8bbe\u8ba1\uff1a \u4ece\u771f\u5b9e\u4eba\u7269\u7167\u7247\u751f\u6210\u98ce\u683c\u5316\u7684\u6e38\u620f\u6216\u52a8\u753b\u89d2\u8272\uff0c\u540c\u65f6\u4fdd\u7559\u5176\u6838\u5fc3\u9762\u90e8\u7279\u5f81\u3002 \u827a\u672f\u6ee4\u955c\u4e0e\u7279\u6548\uff1a \u4e3a\u7167\u7247\u548c\u89c6\u9891\u5e94\u7528\u5404\u79cd\u827a\u672f\u98ce\u683c\u6ee4\u955c\uff0c\u540c\u65f6\u786e\u4fdd\u4eba\u8138\u7684\u8bc6\u522b\u5ea6\uff0c\u63d0\u5347\u7528\u6237\u4f53\u9a8c\u3002 \u6570\u5b57\u8eab\u4efd\u4e0e\u5143\u5b87\u5b99\uff1a \u5728\u5143\u5b87\u5b99\u4e2d\u521b\u5efa\u9ad8\u5ea6\u4e2a\u6027\u5316\u4e14\u5177\u6709\u8fa8\u8bc6\u5ea6\u7684\u6570\u5b57\u5206\u8eab\u6216\u865a\u62df\u5f62\u8c61\u3002 \u9762\u90e8\u4fee\u590d\u4e0e\u589e\u5f3a\uff1a \u5728\u5bf9\u8001\u65e7\u7167\u7247\u8fdb\u884c\u98ce\u683c\u5316\u4fee\u590d\u6216\u589e\u5f3a\u65f6\uff0c\u786e\u4fdd\u9762\u90e8\u8eab\u4efd\u7684\u51c6\u786e\u6027\u3002","title":"4. \u76f8\u5173\u9886\u57df\u6216\u5e94\u7528"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-29/#5_2","text":"\u5c3d\u7ba1\u8bba\u6587\u5c55\u793a\u4e86\u663e\u8457\u7684\u8fdb\u6b65\uff0c\u4f46\u4ece\u6458\u8981\u4e2d\u4ecd\u53ef\u63a8\u65ad\u51fa\u4e00\u4e9b\u6f5c\u5728\u7684\u5c40\u9650\u6027\uff1a \u98ce\u683c\u6cdb\u5316\u6027\uff1a \u8bba\u6587\u660e\u786e\u805a\u7126\u4e8e\u201c\u6d82\u9e26\u201d\u8fd9\u79cd\u201c\u9ad8\u5bf9\u6bd4\u5ea6\u3001\u62bd\u8c61\u201d\u7684\u5a92\u4ecb\u3002\u5176\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u5176\u4ed6\u6781\u7aef\u6216\u975e\u6781\u7aef\u98ce\u683c\uff08\u5982\u6cb9\u753b\u3001\u6c34\u5f69\u3001\u5361\u901a\u7b49\uff09\u4e0b\u7684\u8868\u73b0\u5982\u4f55\uff0c\u4ecd\u9700\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u3002 \u8eab\u4efd\u4fdd\u6301\u7684\u8303\u56f4\uff1a \u6458\u8981\u5f3a\u8c03\u7684\u662f\u201c\u9762\u90e8\u8eab\u4efd\u201d\u3001\u201c\u773c\u775b\u3001\u9f3b\u5b50\u6216\u5634\u5df4\u201d\u7684\u4fdd\u7559\u3002\u8fd9\u8868\u660e\u8be5\u6846\u67b6\u53ef\u80fd\u4e3b\u8981\u5173\u6ce8\u9762\u90e8\u7279\u5f81\uff0c\u5bf9\u4e8e\u5168\u8eab\u8eab\u4efd\u3001\u8eab\u4f53\u59ff\u6001\u7684\u7ec6\u5fae\u7279\u5f81\u6216\u975e\u9762\u90e8\u5c5e\u6027\u7684\u4fdd\u6301\u80fd\u529b\u53ef\u80fd\u6709\u9650\u3002 \u8ba1\u7b97\u6210\u672c\uff1a \u6269\u6563\u6a21\u578b\uff0c\u7279\u522b\u662f\u7ecf\u8fc7 LoRA \u5fae\u8c03\u5e76\u589e\u52a0\u4e86\u81ea\u5b9a\u4e49\u6ce8\u610f\u529b\u673a\u5236\u7684\u6a21\u578b\uff0c\u901a\u5e38\u8ba1\u7b97\u6210\u672c\u8f83\u9ad8\uff0c\u53ef\u80fd\u5bf9\u5b9e\u65f6\u5e94\u7528\u6216\u5927\u89c4\u6a21\u90e8\u7f72\u6784\u6210\u6311\u6218\u3002 \u201c\u5c5e\u6027\u6f02\u79fb\u201d\u7684\u7a0b\u5ea6\uff1a \u5c3d\u7ba1\u201c\u5148\u98ce\u683c\uff0c\u540e\u8eab\u4efd\u201d\u8303\u5f0f\u201c\u51cf\u5c11\u4e86\u5c5e\u6027\u6f02\u79fb\u201d\uff0c\u4f46\u8fd9\u5e76\u4e0d\u610f\u5473\u7740\u5b8c\u5168\u6d88\u9664\u4e86\u6f02\u79fb\u3002\u5728\u67d0\u4e9b\u6781\u7aef\u60c5\u51b5\u4e0b\uff0c\u9762\u90e8\u7279\u5f81\u7684\u7ec6\u5fae\u4e4b\u5904\u4ecd\u53ef\u80fd\u53d7\u5230\u5f71\u54cd\uff0c\u5bfc\u81f4\u8bc6\u522b\u5ea6\u4e0b\u964d\u3002 \u5bf9\u9884\u8bad\u7ec3\u6a21\u578b\u548c CLIP \u7684\u4f9d\u8d56\uff1a \u6a21\u578b\u7684\u6027\u80fd\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u4f9d\u8d56\u4e8e\u6240\u4f7f\u7528\u7684\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u548c CLIP \u6a21\u578b\u7684\u8d28\u91cf\u548c\u6f5c\u5728\u504f\u5dee\u3002\u8fd9\u4e9b\u57fa\u7840\u6a21\u578b\u7684\u5c40\u9650\u6027\u53ef\u80fd\u4f1a\u4f20\u9012\u5230 CraftGraffiti \u7684\u8f93\u51fa\u4e2d\u3002 \u201c\u7ade\u4e89\u6027\u201d\u7ed3\u679c\u7684\u542b\u4e49\uff1a \u6458\u8981\u4e2d\u63d0\u5230\u201c\u5b9a\u91cf\u7ed3\u679c\u5c55\u793a\u4e86\u7ade\u4e89\u6027\u7684\u4eba\u8138\u7279\u5f81\u4e00\u81f4\u6027\u201d\uff0c\u8fd9\u901a\u5e38\u610f\u5473\u7740\u8868\u73b0\u826f\u597d\uff0c\u4f46\u53ef\u80fd\u5e76\u975e\u5b8c\u7f8e\u65e0\u7455\u3002\u5728\u67d0\u4e9b\u7279\u5b9a\u6216\u590d\u6742\u7684\u4eba\u8138\u8868\u60c5\u3001\u5149\u7167\u6216\u906e\u6321\u6761\u4ef6\u4e0b\uff0c\u5176\u9c81\u68d2\u6027\u4ecd\u6709\u5f85\u6df1\u5165\u63a2\u8ba8\u3002 Key Findings: Quantitative results demonstrate competitive facial feature consistency and state-of-the-art aesthetic and human preference scores, while qualitative analyses and a live deployment at the Cruilla Festival highlight the system's real-world creative impact. Links: PDF arXiv","title":"5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u7684\u5c40\u9650\u6027"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-29/#dino-u-net-exploiting-high-fidelity-dense-features-from-foundation-models-for-medical-image-segmentation","text":"Authors: Yifan Gao, Haoyue Li, Feng Yuan, Xiaosong Wang, Xin Gao Published: 2025-08-28 Categories: cs.CV, eess.IV Abstract: Foundation models pre-trained on large-scale natural image datasets offer a powerful paradigm for medical image segmentation. However, effectively transferring their learned representations for precise clinical applications remains a challenge. In this work, we propose Dino U-Net, a novel encoder-decoder architecture designed to exploit the high-fidelity dense features of the DINOv3 vision foundation model. Our architecture introduces an encoder built upon a frozen DINOv3 backbone, which employs a specialized adapter to fuse the model's rich semantic features with low-level spatial details. To preserve the quality of these representations during dimensionality reduction, we design a new fidelity-aware projection module (FAPM) that effectively refines and projects the features for the decoder. We conducted extensive experiments on seven diverse public medical image segmentation datasets. Our results show that Dino U-Net achieves state-of-the-art performance, consistently outperforming previous methods across various imaging modalities. Our framework proves to be highly scalable, with segmentation accuracy consistently improving as the backbone model size increases up to the 7-billion-parameter variant. The findings demonstrate that leveraging the superior, dense-pretrained features from a general-purpose foundation model provides a highly effective and parameter-efficient approach to advance the accuracy of medical image segmentation. The code is available at https://github.com/yifangao112/DinoUNet. Analysis: \u4f5c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u4e13\u5bb6\uff0c\u6211\u5bf9\u8fd9\u7bc7\u8bba\u6587\u6458\u8981\u8fdb\u884c\u5982\u4e0b\u5206\u6790\uff1a","title":"Dino U-Net: Exploiting High-Fidelity Dense Features from Foundation Models for Medical Image Segmentation"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-29/#dino-u-net-exploiting-high-fidelity-dense-features-from-foundation-models-for-medical-image-segmentation_1","text":"1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u660e\u603b\u7ed3 (2-3 \u53e5\u8bdd) \u672c\u6587\u63d0\u51fa\u4e86Dino U-Net\uff0c\u4e00\u4e2a\u65b0\u9896\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\uff0c\u65e8\u5728\u5229\u7528DINOv3\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u9ad8\u4fdd\u771f\u5bc6\u96c6\u7279\u5f81\u8fdb\u884c\u533b\u5b66\u56fe\u50cf\u5206\u5272\u3002\u5b83\u901a\u8fc7\u4e00\u4e2a\u57fa\u4e8e\u51bb\u7ed3DINOv3\u9aa8\u5e72\u7684\u7f16\u7801\u5668\uff0c\u7ed3\u5408\u4e13\u95e8\u7684\u9002\u914d\u5668\u548c\u65b0\u9896\u7684\u4fdd\u771f\u5ea6\u611f\u77e5\u6295\u5f71\u6a21\u5757\uff08FAPM\uff09\uff0c\u6709\u6548\u5730\u5c06\u8bed\u4e49\u7279\u5f81\u4e0e\u7a7a\u95f4\u7ec6\u8282\u878d\u5408\uff0c\u5e76\u4fdd\u6301\u7279\u5f81\u8d28\u91cf\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0cDino U-Net\u5728\u591a\u4e2a\u533b\u5b66\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5c55\u73b0\u51fa\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\uff0c\u4e3a\u533b\u5b66\u56fe\u50cf\u5206\u5272\u63d0\u4f9b\u4e86\u4e00\u79cd\u53c2\u6570\u9ad8\u6548\u4e14\u9ad8\u6027\u80fd\u7684\u89e3\u51b3\u65b9\u6848\u3002 2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u5b66\u65b9\u6cd5 Dino U-Net\u7684\u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u5176\u72ec\u7279\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\uff0c\u7279\u522b\u662f\u5982\u4f55\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u5730\u5229\u7528DINOv3\u57fa\u7840\u6a21\u578b\u7684\u5bc6\u96c6\u7279\u5f81\u8fdb\u884c\u50cf\u7d20\u7ea7\u9884\u6d4b\u3002\u5177\u4f53\u5305\u62ec\uff1a \u57fa\u4e8e\u51bb\u7ed3DINOv3\u9aa8\u5e72\u7684\u7f16\u7801\u5668\u4e0e\u4e13\u95e8\u9002\u914d\u5668\uff1a \u8bba\u6587\u5229\u7528\u4e00\u4e2a\u51bb\u7ed3\u7684DINOv3\u9aa8\u5e72\u4f5c\u4e3a\u7f16\u7801\u5668\uff0c\u8fd9\u6781\u5927\u5730\u63d0\u9ad8\u4e86\u53c2\u6570\u6548\u7387\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u5f15\u5165\u4e86\u4e00\u4e2a\u201c\u4e13\u95e8\u9002\u914d\u5668\u201d\u6765\u878d\u5408DINOv3\u63d0\u53d6\u7684\u4e30\u5bcc\u8bed\u4e49\u7279\u5f81\u4e0e\u4f4e\u5c42\u7a7a\u95f4\u7ec6\u8282\u3002\u8fd9\u5bf9\u4e8e\u533b\u5b66\u56fe\u50cf\u5206\u5272\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u4e3a\u5206\u5272\u4efb\u52a1\u65e2\u9700\u8981\u9ad8\u5c42\u8bed\u4e49\u7406\u89e3\uff0c\u4e5f\u9700\u8981\u7cbe\u786e\u7684\u8fb9\u754c\u548c\u5c40\u90e8\u7ec6\u8282\u3002 \u4fdd\u771f\u5ea6\u611f\u77e5\u6295\u5f71\u6a21\u5757\uff08FAPM\uff09\uff1a \u4e3a\u4e86\u5728\u5c06\u9ad8\u7ef4\u7279\u5f81\u6295\u5f71\u5230\u89e3\u7801\u5668\u6240\u9700\u7ef4\u5ea6\u65f6\uff0c\u6700\u5927\u9650\u5ea6\u5730\u4fdd\u7559\u7279\u5f81\u7684\u8d28\u91cf\u548c\u4fe1\u606f\uff0c\u8bba\u6587\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u65b0\u7684\u201c\u4fdd\u771f\u5ea6\u611f\u77e5\u6295\u5f71\u6a21\u5757\uff08FAPM\uff09\u201d\u3002\u8fd9\u4e2a\u6a21\u5757\u80fd\u591f\u6709\u6548\u5730\u7cbe\u70bc\u548c\u6295\u5f71\u7279\u5f81\uff0c\u907f\u514d\u5728\u964d\u7ef4\u8fc7\u7a0b\u4e2d\u4e22\u5931\u5173\u952e\u4fe1\u606f\uff0c\u4ece\u800c\u786e\u4fdd\u89e3\u7801\u5668\u80fd\u591f\u63a5\u6536\u5230\u9ad8\u8d28\u91cf\u7684\u8868\u793a\u3002 \u5229\u7528\u901a\u7528\u57fa\u7840\u6a21\u578b\u7684\u5bc6\u96c6\u9884\u8bad\u7ec3\u7279\u5f81\uff1a \u8bba\u6587\u5f3a\u8c03\u4e86\u5229\u7528DINOv3\u8fd9\u79cd\u5728\u6d77\u91cf\u81ea\u7136\u56fe\u50cf\u4e0a\u8fdb\u884c\u201c\u5bc6\u96c6\u9884\u8bad\u7ec3\u201d\u7684\u57fa\u7840\u6a21\u578b\u6240\u5e26\u6765\u7684\u4f18\u52bf\u3002DINOv3\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\uff0c\u80fd\u591f\u5b66\u4e60\u5230\u975e\u5e38\u4e30\u5bcc\u7684\u3001\u5bf9\u56fe\u50cf\u5c40\u90e8\u7ed3\u6784\u548c\u5168\u5c40\u8bed\u4e49\u90fd\u654f\u611f\u7684\u7279\u5f81\uff0c\u8fd9\u4e9b\u7279\u5f81\u88ab\u8bc1\u660e\u53ef\u4ee5\u6709\u6548\u5730\u8fc1\u79fb\u5230\u533b\u5b66\u9886\u57df\u3002 3. \u5bf9\u8be5\u9886\u57df\u7684\u6f5c\u5728\u5f71\u54cd \u672c\u7814\u7a76\u7684\u6f5c\u5728\u5f71\u54cd\u662f\u591a\u65b9\u9762\u7684\uff1a \u63d0\u5347\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7cbe\u5ea6\uff1a Dino U-Net\u7684SOTA\u6027\u80fd\u5c06\u4e3a\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6811\u7acb\u65b0\u7684\u6807\u6746\uff0c\u6709\u671b\u63a8\u52a8\u8be5\u9886\u57df\u5728\u4e34\u5e8a\u8bca\u65ad\u548c\u6cbb\u7597\u89c4\u5212\u4e2d\u7684\u5e94\u7528\u3002 \u53c2\u6570\u9ad8\u6548\u7684\u8303\u5f0f\uff1a \u901a\u8fc7\u51bb\u7ed3\u5927\u578b\u57fa\u7840\u6a21\u578b\u9aa8\u5e72\u5e76\u5f15\u5165\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\uff0c\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u53c2\u6570\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002\u8fd9\u5bf9\u4e8e\u533b\u5b66\u9886\u57df\u5c24\u4e3a\u91cd\u8981\uff0c\u56e0\u4e3a\u533b\u5b66\u6570\u636e\u96c6\u901a\u5e38\u89c4\u6a21\u6709\u9650\uff0c\u4e14\u8bad\u7ec3\u6570\u5341\u4ebf\u53c2\u6570\u7684\u6a21\u578b\u6210\u672c\u9ad8\u6602\u3002\u8fd9\u79cd\u8303\u5f0f\u4f7f\u5f97\u5728\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u4e0b\u4e5f\u80fd\u5145\u5206\u5229\u7528\u5927\u578b\u6a21\u578b\u7684\u5f3a\u5927\u80fd\u529b\u3002 \u9a8c\u8bc1\u57fa\u7840\u6a21\u578b\u5728\u4e13\u4e1a\u9886\u57df\u7684\u6cdb\u5316\u80fd\u529b\uff1a \u672c\u6587\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u5728\u901a\u7528\u81ea\u7136\u56fe\u50cf\u4e0a\u9884\u8bad\u7ec3\u7684\u57fa\u7840\u6a21\u578b\uff08\u5982DINOv3\uff09\u5728\u9ad8\u5ea6\u4e13\u4e1a\u5316\u9886\u57df\uff08\u5982\u533b\u5b66\u56fe\u50cf\u5206\u6790\uff09\u7684\u5f3a\u5927\u8fc1\u79fb\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u7cbe\u7ec6\u50cf\u7d20\u7ea7\u7406\u89e3\u7684\u5bc6\u96c6\u9884\u6d4b\u4efb\u52a1\u4e0a\u3002 \u52a0\u901f\u533b\u5b66AI\u7814\u53d1\uff1a \u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6027\u80fd\u4e14\u6613\u4e8e\u4f7f\u7528\u7684\u6846\u67b6\uff08\u4ee3\u7801\u5df2\u5f00\u6e90\uff09\uff0c\u6709\u671b\u52a0\u901f\u7814\u7a76\u4eba\u5458\u5728\u533b\u5b66\u56fe\u50cf\u5206\u5272\u9886\u57df\u7684\u63a2\u7d22\u548c\u521b\u65b0\u3002 4. \u53ef\u80fd\u53d7\u76ca\u4e8e\u8fd9\u9879\u7814\u7a76\u7684\u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 \u5176\u4ed6\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4efb\u52a1\uff1a \u5982\u75c5\u7076\u68c0\u6d4b\u3001\u56fe\u50cf\u914d\u51c6\u3001\u75be\u75c5\u5206\u7c7b\uff08\u7279\u522b\u662f\u9700\u8981\u7cbe\u786e\u5b9a\u4f4d\u75c5\u7076\u7684\uff09\u30013D\u91cd\u5efa\u7b49\uff0c\u8fd9\u4e9b\u4efb\u52a1\u540c\u6837\u9700\u8981\u5f3a\u5927\u7684\u7279\u5f81\u63d0\u53d6\u548c\u50cf\u7d20\u7ea7\u7406\u89e3\u80fd\u529b\u3002 \u5176\u4ed6\u9700\u8981\u7cbe\u7ec6\u50cf\u7d20\u7ea7\u7406\u89e3\u7684\u4e13\u4e1a\u9886\u57df\uff1a \u5982\u5de5\u4e1a\u7f3a\u9677\u68c0\u6d4b\u3001\u9065\u611f\u56fe\u50cf\u5206\u6790\u3001\u751f\u7269\u663e\u5fae\u56fe\u50cf\u5904\u7406\u7b49\uff0c\u8fd9\u4e9b\u9886\u57df\u540c\u6837\u9762\u4e34\u6570\u636e\u7a00\u7f3a\u548c\u5bf9\u7ec6\u8282\u654f\u611f\u7684\u6311\u6218\uff0c\u53ef\u4ee5\u501f\u9274Dino U-Net\u5229\u7528\u57fa\u7840\u6a21\u578b\u7279\u5f81\u7684\u7b56\u7565\u3002 \u5c0f\u6837\u672c\uff08few-shot\uff09\u6216\u96f6\u6837\u672c\uff08zero-shot\uff09\u5b66\u4e60\uff1a DINOv3\u7684\u5f3a\u5927\u7279\u5f81\u63d0\u53d6\u80fd\u529b\u53ef\u80fd\u4e3a\u8fd9\u4e9b\u573a\u666f\u63d0\u4f9b\u66f4\u5f3a\u7684\u6cdb\u5316\u57fa\u7840\uff0c\u5c24\u5176\u662f\u5728\u533b\u5b66\u9886\u57df\uff0c\u65b0\u75be\u75c5\u6216\u7f55\u89c1\u75be\u75c5\u7684\u6570\u636e\u5f80\u5f80\u975e\u5e38\u7a00\u7f3a\u3002 \u6a21\u578b\u538b\u7f29\u4e0e\u90e8\u7f72\uff1a \u51bb\u7ed3\u9aa8\u5e72\u7684\u7b56\u7565\u6709\u52a9\u4e8e\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\uff0c\u964d\u4f4e\u6a21\u578b\u8bad\u7ec3\u548c\u5fae\u8c03\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u4e3a\u672a\u6765\u5728\u8fb9\u7f18\u8bbe\u5907\u6216\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u7684\u4e34\u5e8a\u73af\u5883\u4e2d\u90e8\u7f72\u9ad8\u6027\u80fd\u6a21\u578b\u63d0\u4f9b\u601d\u8def\u3002 5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u51fa\u7684\u5c40\u9650\u6027 \u5bf9DINOv3\u9aa8\u5e72\u7684\u4f9d\u8d56\u6027\uff1a \u5c3d\u7ba1\u51bb\u7ed3\u9aa8\u5e72\u63d0\u9ad8\u4e86\u53c2\u6570\u6548\u7387\uff0c\u4f46DINOv3\u672c\u8eab\u662f\u5728\u81ea\u7136\u56fe\u50cf\u4e0a\u9884\u8bad\u7ec3\u7684\u3002\u867d\u7136\u9002\u914d\u5668\u65e8\u5728\u5f25\u8865\uff0c\u4f46\u5176\u7279\u5f81\u8868\u793a\u53ef\u80fd\u65e0\u6cd5\u5b8c\u5168\u6355\u6349\u533b\u5b66\u56fe\u50cf\u7279\u6709\u7684\u3001\u4e0e\u81ea\u7136\u56fe\u50cf\u5dee\u5f02\u6781\u5927\u7684\u7ec6\u5fae\u75c5\u7406\u7279\u5f81\uff0c\u5c24\u5176\u662f\u5728\u67d0\u4e9b\u4e0e\u81ea\u7136\u56fe\u50cf\u89c6\u89c9\u7279\u6027\u5dee\u5f02\u5de8\u5927\u7684\u533b\u5b66\u6a21\u6001\u4e0a\u3002 \u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\uff1a \u5373\u4f7f\u9aa8\u5e72\u51bb\u7ed3\uff0c\u6458\u8981\u4e2d\u63d0\u53ca\u7684\u201c70\u4ebf\u53c2\u6570\u201d\u6a21\u578b\u5728\u63a8\u7406\u9636\u6bb5\u4ecd\u53ef\u80fd\u9700\u8981\u663e\u8457\u7684\u8ba1\u7b97\u8d44\u6e90\uff0c\u8fd9\u5bf9\u4e8e\u5b9e\u65f6\u6027\u8981\u6c42\u9ad8\u6216\u8d44\u6e90\u53d7\u9650\u7684\u4e34\u5e8a\u90e8\u7f72\u73af\u5883\u53ef\u80fd\u662f\u4e00\u4e2a\u6311\u6218\u3002 \u53ef\u89e3\u91ca\u6027\uff1a \u57fa\u7840\u6a21\u578b\u901a\u5e38\u88ab\u89c6\u4e3a\u9ed1\u7bb1\uff0c\u5f15\u5165\u989d\u5916\u7684\u9002\u914d\u5668\u548c\u6295\u5f71\u6a21\u5757\u53ef\u80fd\u4f1a\u8fdb\u4e00\u6b65\u589e\u52a0\u6a21\u578b\u51b3\u7b56\u8fc7\u7a0b\u7684\u590d\u6742\u6027\uff0c\u964d\u4f4e\u5176\u5728\u4e34\u5e8a\u5e94\u7528\u4e2d\u81f3\u5173\u91cd\u8981\u7684\u53ef\u89e3\u91ca\u6027\u3002 \u9002\u914d\u5668\u548cFAPM\u7684\u8bad\u7ec3\u6570\u636e\u9700\u6c42\uff1a \u5c3d\u7ba1\u9aa8\u5e72\u662f\u51bb\u7ed3\u7684\uff0c\u4f46\u9002\u914d\u5668\u548cFAPM\u4ecd\u9700\u8981\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\u8fdb\u884c\u8bad\u7ec3\u3002\u6458\u8981\u4e2d\u672a\u63d0\u53ca\u8fd9\u4e9b\u6a21\u5757\u5bf9\u8bad\u7ec3\u6570\u636e\u91cf\u7684\u654f\u611f\u6027\uff0c\u8fd9\u53ef\u80fd\u5728\u6781\u5ea6\u7a00\u7f3a\u7684\u533b\u5b66\u6570\u636e\u96c6\u4e0a\u6784\u6210\u6311\u6218\u3002 \u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u533b\u5b66\u6a21\u6001\u6216\u7f55\u89c1\u75c5\u53d8\uff1a \u5c3d\u7ba1\u5728\u201c\u4e03\u4e2a\u591a\u6837\u5316\u7684\u516c\u5171\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6570\u636e\u96c6\u201d\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u4f46\u8fd9\u4e9b\u6570\u636e\u96c6\u53ef\u80fd\u65e0\u6cd5\u5b8c\u5168\u8986\u76d6\u6240\u6709\u533b\u5b66\u6a21\u6001\u6216\u7f55\u89c1\u75c5\u53d8\u7684\u590d\u6742\u6027\u3002\u6a21\u578b\u5728\u9762\u5bf9\u4e0e\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u5dee\u5f02\u8f83\u5927\u7684\u65b0\u6a21\u6001\u6216\u75c5\u53d8\u65f6\u7684\u9c81\u68d2\u6027\u4ecd\u9700\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u3002 Key Findings: In this work, we propose Dino U-Net, a novel encoder-decoder architecture designed to exploit the high-fidelity dense features of the DINOv3 vision foundation model. To preserve the quality of these representations during dimensionality reduction, we design a new fidelity-aware projection module (FAPM) that effectively refines and projects the features for the decoder. Our results show that Dino U-Net achieves state-of-the-art performance, consistently outperforming previous methods across various imaging modalities. Links: PDF arXiv","title":"Dino U-Net: Exploiting High-Fidelity Dense Features from Foundation Models for Medical Image Segmentation"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-29/#plug-in-feedback-self-adaptive-attention-in-clip-for-training-free-open-vocabulary-segmentation","text":"Authors: Zhixiang Chi, Yanan Wu, Li Gu, Huan Liu, Ziqiang Wang, Yang Zhang, Yang Wang, Konstantinos N. Plataniotis Published: 2025-08-27 Categories: cs.CV, cs.LG Abstract: CLIP exhibits strong visual-textual alignment but struggle with open-vocabulary segmentation due to poor localization. Prior methods enhance spatial coherence by modifying intermediate attention. But, this coherence isn't consistently propagated to the final output due to subsequent operations such as projections. Additionally, intermediate attention lacks direct interaction with text representations, such semantic discrepancy limits the full potential of CLIP. In this work, we propose a training-free, feedback-driven self-adaptive framework that adapts output-based patch-level correspondences back to the intermediate attention. The output predictions, being the culmination of the model's processing, encapsulate the most comprehensive visual and textual semantics about each patch. Our approach enhances semantic consistency between internal representations and final predictions by leveraging the model's outputs as a stronger spatial coherence prior. We design key modules, including attention isolation, confidence-based pruning for sparse adaptation, and adaptation ensemble, to effectively feedback the output coherence cues. Our method functions as a plug-in module, seamlessly integrating into four state-of-the-art approaches with three backbones (ViT-B, ViT-L, ViT-H). We further validate our framework across multiple attention types (Q-K, self-self, and Proxy augmented with MAE, SAM, and DINO). Our approach consistently improves their performance across eight benchmarks. Analysis: \u8fd9\u7bc7\u8bba\u6587\u6458\u8981\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9CLIP\u6a21\u578b\u5728\u5f00\u653e\u8bcd\u6c47\u5206\u5272\uff08Open-Vocabulary Segmentation\uff09\u4efb\u52a1\u4e2d\u5b9a\u4f4d\u80fd\u529b\u4e0d\u8db3\u7684\u521b\u65b0\u89e3\u51b3\u65b9\u6848\u3002\u4ee5\u4e0b\u662f\u8be6\u7ec6\u5206\u6790\uff1a","title":"Plug-in Feedback Self-adaptive Attention in CLIP for Training-free Open-Vocabulary Segmentation"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-29/#1-concise-summary_2","text":"\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\uff08training-free\uff09\u7684\u53cd\u9988\u9a71\u52a8\u81ea\u9002\u5e94\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3CLIP\u6a21\u578b\u5728\u5f00\u653e\u8bcd\u6c47\u5206\u5272\u4e2d\u5b9a\u4f4d\u80fd\u529b\u5dee\u7684\u95ee\u9898\u3002\u5b83\u901a\u8fc7\u5c06\u6a21\u578b\u6700\u7ec8\u8f93\u51fa\u5c42\u9762\u7684\u8865\u4e01\u7ea7\uff08patch-level\uff09\u8bed\u4e49\u4e00\u81f4\u6027\u53cd\u9988\u56de\u4e2d\u95f4\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4ece\u800c\u589e\u5f3a\u5185\u90e8\u8868\u793a\u4e0e\u6700\u7ec8\u9884\u6d4b\u4e4b\u95f4\u7684\u8bed\u4e49\u4e00\u81f4\u6027\u3002\u4f5c\u4e3a\u4e00\u4e2a\u5373\u63d2\u5373\u7528\uff08plug-in\uff09\u6a21\u5757\uff0c\u8be5\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u5347\u73b0\u6709SOTA\u6a21\u578b\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u6027\u80fd\u3002","title":"1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u660e\u603b\u7ed3 (Concise Summary)"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-29/#2-key-innovation-or-methodological-approach_2","text":"\u8be5\u8bba\u6587\u7684\u5173\u952e\u521b\u65b0\u5728\u4e8e\u5176 \u201c\u8f93\u51fa\u5230\u4e2d\u95f4\u6ce8\u610f\u529b\u201d\u7684\u53cd\u9988\u9a71\u52a8\u81ea\u9002\u5e94\u673a\u5236 \u3002 1. \u53cd\u9988\u56de\u8def\u8bbe\u8ba1\uff1a \u4f20\u7edf\u65b9\u6cd5\u901a\u5e38\u76f4\u63a5\u4fee\u6539\u4e2d\u95f4\u6ce8\u610f\u529b\uff0c\u4f46\u5b58\u5728\u8bed\u4e49\u4f20\u64ad\u4e0d\u4e00\u81f4\u548c\u7f3a\u4e4f\u4e0e\u6587\u672c\u8868\u793a\u76f4\u63a5\u4ea4\u4e92\u7684\u95ee\u9898\u3002\u672c\u6587\u5219\u521b\u65b0\u6027\u5730\u5229\u7528\u6a21\u578b\u7684 \u6700\u7ec8\u8f93\u51fa\u9884\u6d4b \u4f5c\u4e3a\u201c\u66f4\u5f3a\u7684\u7a7a\u95f4\u4e00\u81f4\u6027\u5148\u9a8c\u201d\uff08stronger spatial coherence prior\uff09\u3002\u8fd9\u4e9b\u8f93\u51fa\u88ab\u8ba4\u4e3a\u662f\u6a21\u578b\u5904\u7406\u7684\u201c\u9ad8\u6f6e\u201d\uff0c\u5c01\u88c5\u4e86\u6700\u5168\u9762\u7684\u89c6\u89c9\u548c\u6587\u672c\u8bed\u4e49\u3002 2. \u81ea\u9002\u5e94\u8c03\u6574\uff1a \u5c06\u8fd9\u4e9b\u8f93\u51fa\u5c42\u9762\u7684\u8865\u4e01\u7ea7\u5bf9\u5e94\u5173\u7cfb\uff08patch-level correspondences\uff09\u53cd\u9988\u5e76\u81ea\u9002\u5e94\u5730\u8c03\u6574\u4e2d\u95f4\u6ce8\u610f\u529b\uff0c\u4ece\u800c\u5728\u5185\u90e8\u8868\u793a\u548c\u6700\u7ec8\u9884\u6d4b\u4e4b\u95f4\u5efa\u7acb\u66f4\u5f3a\u7684\u8bed\u4e49\u4e00\u81f4\u6027\u3002 3. \u8bad\u7ec3\u65e0\u5173\u6027\u4e0e\u5373\u63d2\u5373\u7528\uff1a \u6574\u4e2a\u6846\u67b6\u662f\u201c\u8bad\u7ec3\u65e0\u5173\u201d\uff08training-free\uff09\u7684\uff0c\u8fd9\u610f\u5473\u7740\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3CLIP\u6a21\u578b\uff0c\u53ef\u4ee5\u76f4\u63a5\u4f5c\u4e3a\u201c\u5373\u63d2\u5373\u7528\u201d\u6a21\u5757\u96c6\u6210\u5230\u73b0\u6709\u65b9\u6cd5\u4e2d\uff0c\u5927\u5927\u964d\u4f4e\u4e86\u5e94\u7528\u6210\u672c\u548c\u590d\u6742\u6027\u3002 4. \u6838\u5fc3\u6a21\u5757\uff1a \u4e3a\u5b9e\u73b0\u8fd9\u4e00\u53cd\u9988\u673a\u5236\uff0c\u8bba\u6587\u8bbe\u8ba1\u4e86\u5173\u952e\u6a21\u5757\uff0c\u5305\u62ec \u6ce8\u610f\u529b\u9694\u79bb\uff08attention isolation\uff09 \u3001\u7528\u4e8e\u7a00\u758f\u81ea\u9002\u5e94\u7684 \u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u526a\u679d\uff08confidence-based pruning for sparse adaptation\uff09 \u4ee5\u53ca \u81ea\u9002\u5e94\u96c6\u6210\uff08adaptation ensemble\uff09 \u3002","title":"2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u8bba (Key Innovation or Methodological Approach)"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-29/#3-potential-impact-on-the-field_2","text":"\u63d0\u5347CLIP\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u5b9e\u7528\u6027\uff1a \u663e\u8457\u589e\u5f3a\u4e86CLIP\u5728\u5f00\u653e\u8bcd\u6c47\u5206\u5272\u8fd9\u4e00\u5173\u952e\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u5b9a\u4f4d\u80fd\u529b\uff0c\u4f7f\u5176\u5728\u9700\u8981\u7cbe\u7ec6\u89c6\u89c9\u7406\u89e3\u7684\u5e94\u7528\u4e2d\u66f4\u5177\u7ade\u4e89\u529b\u3002 \u5f00\u521b\u65b0\u7684\u6a21\u578b\u6539\u8fdb\u8303\u5f0f\uff1a \u201c\u8f93\u51fa\u5230\u5185\u90e8\u8868\u793a\u201d\u7684\u53cd\u9988\u673a\u5236\u53ef\u80fd\u4e3a\u6539\u8fdb\u5176\u4ed6\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\uff08\u4e0d\u4ec5\u9650\u4e8eCLIP\uff09\u5728\u7279\u5b9a\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u63d0\u4f9b\u65b0\u7684\u601d\u8def\uff0c\u5c24\u5176\u662f\u5728\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7684\u573a\u666f\u4e0b\u3002 \u964d\u4f4e\u7814\u7a76\u548c\u5e94\u7528\u6210\u672c\uff1a \u201c\u8bad\u7ec3\u65e0\u5173\u201d\u548c\u201c\u5373\u63d2\u5373\u7528\u201d\u7684\u7279\u6027\u610f\u5473\u7740\u7814\u7a76\u4eba\u5458\u548c\u5f00\u53d1\u8005\u53ef\u4ee5\u4ee5\u8f83\u4f4e\u7684\u8ba1\u7b97\u6210\u672c\u548c\u65f6\u95f4\u6295\u5165\uff0c\u5feb\u901f\u63d0\u5347\u73b0\u6709\u6a21\u578b\u7684\u6027\u80fd\uff0c\u52a0\u901f\u4e86\u6280\u672f\u4ece\u7814\u7a76\u5230\u5e94\u7528\u7684\u8f6c\u5316\u3002 \u63a8\u52a8\u5f00\u653e\u8bcd\u6c47\u5b66\u4e60\u53d1\u5c55\uff1a \u63d0\u9ad8\u4e86\u6a21\u578b\u5904\u7406\u672a\u77e5\u7c7b\u522b\u5bf9\u8c61\u7684\u80fd\u529b\uff0c\u5bf9\u4e8e\u96f6\u6837\u672c\uff08zero-shot\uff09\u548c\u5c11\u6837\u672c\uff08few-shot\uff09\u5b66\u4e60\u7b49\u9886\u57df\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002","title":"3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd (Potential Impact on the Field)"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-29/#4-related-areas-or-applications_1","text":"\u5f00\u653e\u8bcd\u6c47\u5206\u5272\u4e0e\u68c0\u6d4b\uff1a \u76f4\u63a5\u53d7\u76ca\u9886\u57df\uff0c\u80fd\u591f\u66f4\u51c6\u786e\u5730\u8bc6\u522b\u548c\u5206\u5272\u56fe\u50cf\u4e2d\u4efb\u610f\u6587\u672c\u63cf\u8ff0\u7684\u5bf9\u8c61\u3002 \u96f6\u6837\u672c/\u5c11\u6837\u672c\u5b66\u4e60\uff1a \u589e\u5f3a\u4e86\u6a21\u578b\u5728\u9762\u5bf9\u672a\u89c1\u8fc7\u7c7b\u522b\u65f6\u7684\u6cdb\u5316\u80fd\u529b\u548c\u5b9a\u4f4d\u7cbe\u5ea6\u3002 \u56fe\u50cf\u7f16\u8f91\u4e0e\u5185\u5bb9\u751f\u6210\uff1a \u5141\u8bb8\u7528\u6237\u901a\u8fc7\u6587\u672c\u6307\u4ee4\u66f4\u7cbe\u786e\u5730\u9009\u62e9\u548c\u4fee\u6539\u56fe\u50cf\u4e2d\u7684\u7279\u5b9a\u533a\u57df\u3002 \u673a\u5668\u4eba\u4e0e\u81ea\u52a8\u9a7e\u9a76\uff1a \u63d0\u5347\u4e86\u7cfb\u7edf\u5bf9\u73af\u5883\u4e2d\u672a\u77e5\u7269\u4f53\u6216\u573a\u666f\u7684\u7406\u89e3\u548c\u4ea4\u4e92\u80fd\u529b\uff0c\u4f8b\u5982\u8bc6\u522b\u548c\u907f\u5f00\u65b0\u51fa\u73b0\u7684\u969c\u788d\u7269\u3002 \u533b\u5b66\u5f71\u50cf\u5206\u6790\uff1a \u8f85\u52a9\u533b\u751f\u6839\u636e\u6587\u672c\u63cf\u8ff0\uff0c\u66f4\u51c6\u786e\u5730\u5b9a\u4f4d\u548c\u5206\u5272\u75c5\u53d8\u533a\u57df\u6216\u7279\u5b9a\u7ec4\u7ec7\u7ed3\u6784\u3002 \u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u4e0e\u56fe\u50cf\u68c0\u7d22\uff1a \u66f4\u7cbe\u7ec6\u7684\u89c6\u89c9-\u6587\u672c\u5bf9\u9f50\u6709\u52a9\u4e8e\u63d0\u5347\u5bf9\u590d\u6742\u95ee\u9898\u7684\u7406\u89e3\u548c\u66f4\u51c6\u786e\u7684\u56fe\u50cf\u5185\u5bb9\u68c0\u7d22\u3002","title":"4. \u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 (Related Areas or Applications)"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-29/#5-limitations-inferred-from-the-abstract_1","text":"\u63a8\u7406\u65f6\u95f4\u5f00\u9500\uff1a \u5c3d\u7ba1\u662f\u201c\u8bad\u7ec3\u65e0\u5173\u201d\u4e14\u4f7f\u7528\u4e86\u201c\u7a00\u758f\u81ea\u9002\u5e94\u201d\uff0c\u4f46\u5f15\u5165\u53cd\u9988\u5faa\u73af\u548c\u989d\u5916\u7684\u6a21\u5757\uff08\u6ce8\u610f\u529b\u9694\u79bb\u3001\u526a\u679d\u3001\u96c6\u6210\uff09\u53ef\u80fd\u4f1a\u589e\u52a0\u6a21\u578b\u7684\u63a8\u7406\u65f6\u95f4\uff08inference latency\uff09\uff0c\u8fd9\u5bf9\u4e8e\u5b9e\u65f6\u5e94\u7528\u53ef\u80fd\u662f\u4e00\u4e2a\u8003\u91cf\u3002 \u5bf9\u521d\u59cb\u8f93\u51fa\u8d28\u91cf\u7684\u4f9d\u8d56\uff1a \u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u4f9d\u8d56\u4e8e\u6a21\u578b\u521d\u59cb\u8f93\u51fa\u9884\u6d4b\u7684\u8d28\u91cf\u3002\u5982\u679c\u521d\u59cb\u8f93\u51fa\u975e\u5e38\u5dee\uff0c\u53cd\u9988\u673a\u5236\u80fd\u5426\u6709\u6548\u5f15\u5bfc\u6539\u8fdb\u53ef\u80fd\u5b58\u5728\u7591\u95ee\u3002\u6458\u8981\u4e2d\u63d0\u5230\u8f93\u51fa\u201c\u5c01\u88c5\u4e86\u6700\u5168\u9762\u7684\u8bed\u4e49\u201d\uff0c\u6697\u793a\u5176\u8d28\u91cf\u8db3\u591f\u9ad8\uff0c\u4f46\u6781\u7aef\u60c5\u51b5\u4ecd\u9700\u9a8c\u8bc1\u3002 \u67b6\u6784\u7279\u5f02\u6027\uff1a \u8be5\u65b9\u6cd5\u662f\u4e3aCLIP\u7684Transformer\u67b6\u6784\u53ca\u5176\u6ce8\u610f\u529b\u673a\u5236\u8bbe\u8ba1\u7684\u3002\u5176\u5728\u5176\u4ed6\u975eTransformer\u6216\u4e0d\u540c\u6ce8\u610f\u529b\u673a\u5236\u7684\u89c6\u89c9\u6a21\u578b\u4e0a\u7684\u666e\u9002\u6027\u6216\u6709\u6548\u6027\u672a\u5728\u6458\u8981\u4e2d\u63d0\u53ca\u3002 \u201c\u65e0\u7f1d\u96c6\u6210\u201d\u7684\u5b9e\u9645\u590d\u6742\u6027\uff1a \u5c3d\u7ba1\u58f0\u79f0\u201c\u65e0\u7f1d\u96c6\u6210\u201d\uff0c\u4f46\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\uff0c\u4efb\u4f55\u5373\u63d2\u5373\u7528\u6a21\u5757\u90fd\u53ef\u80fd\u9700\u8981\u4e00\u5b9a\u7684\u5de5\u7a0b\u9002\u914d\u548c\u8c03\u8bd5\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u591a\u79cdSOTA\u65b9\u6cd5\u548c\u4e0d\u540c\u9aa8\u5e72\u7f51\u7edc\u65f6\u3002 Key Findings: In this work, we propose a training-free, feedback-driven self-adaptive framework that adapts output-based patch-level correspondences back to the intermediate attention. Our approach enhances semantic consistency between internal representations and final predictions by leveraging the model's outputs as a stronger spatial coherence prior. Our method functions as a plug-in module, seamlessly integrating into four state-of-the-art approaches with three backbones (ViT-B, ViT-L, ViT-H). Our approach consistently improves their performance across eight benchmarks. Links: PDF arXiv","title":"5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u7684\u5c40\u9650\u6027 (Limitations Inferred from the Abstract)"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-29/#bridging-domain-gaps-for-fine-grained-moth-classification-through-expert-informed-adaptation-and-foundation-model-priors","text":"Authors: Ross J Gardiner, Guillaume Mougeot, Sareh Rowlands, Benno I Simmons, Flemming Helsing, Toke Thomas H\u00f8ye Published: 2025-08-27 Categories: cs.CV Abstract: Labelling images of Lepidoptera (moths) from automated camera systems is vital for understanding insect declines. However, accurate species identification is challenging due to domain shifts between curated images and noisy field imagery. We propose a lightweight classification approach, combining limited expert-labelled field data with knowledge distillation from the high-performance BioCLIP2 foundation model into a ConvNeXt-tiny architecture. Experiments on 101 Danish moth species from AMI camera systems demonstrate that BioCLIP2 substantially outperforms other methods and that our distilled lightweight model achieves comparable accuracy with significantly reduced computational cost. These insights offer practical guidelines for the development of efficient insect monitoring systems and bridging domain gaps for fine-grained classification. Analysis: \u8fd9\u7bc7\u8bba\u6587\u7684\u6458\u8981\u5c55\u793a\u4e86\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u5728\u89e3\u51b3\u5b9e\u9645\u751f\u7269\u591a\u6837\u6027\u76d1\u6d4b\u6311\u6218\u65b9\u9762\u7684\u6700\u65b0\u8fdb\u5c55\u3002\u4ee5\u4e0b\u662f\u8be6\u7ec6\u5206\u6790\uff1a","title":"Bridging Domain Gaps for Fine-Grained Moth Classification Through Expert-Informed Adaptation and Foundation Model Priors"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-29/#1-2-3_2","text":"\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u5206\u7c7b\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u9ad8\u6027\u80fd\u7684BioCLIP2\u57fa\u7840\u6a21\u578b\u77e5\u8bc6\u84b8\u998f\u5230ConvNeXt-tiny\u67b6\u6784\u4e2d\uff0c\u5e76\u7ed3\u5408\u6709\u9650\u7684\u4e13\u5bb6\u6807\u6ce8\u91ce\u5916\u6570\u636e\uff0c\u4ee5\u89e3\u51b3\u81ea\u52a8\u76f8\u673a\u7cfb\u7edf\u6355\u83b7\u7684\u98de\u86fe\u56fe\u50cf\u5728\u7cbe\u7ec6\u7c92\u5ea6\u5206\u7c7b\u4e2d\u5b58\u5728\u7684\u9886\u57df\u6f02\u79fb\u95ee\u9898\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u4e0eBioCLIP2\u76f8\u5f53\u7684\u5206\u7c7b\u7cbe\u5ea6\uff0c\u4e3a\u9ad8\u6548\u7684\u6606\u866b\u76d1\u6d4b\u7cfb\u7edf\u548c\u8de8\u9886\u57df\u7cbe\u7ec6\u7c92\u5ea6\u5206\u7c7b\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002","title":"1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u6d01\u603b\u7ed3 (2-3 \u53e5\u8bdd)"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-29/#2_3","text":"\u8be5\u7814\u7a76\u7684\u5173\u952e\u521b\u65b0\u5728\u4e8e\u5176 \u534f\u540c\u7ec4\u5408 \u7684\u65b9\u6cd5\u8bba\uff0c\u6709\u6548\u5730\u878d\u5408\u4e86\u4ee5\u4e0b\u51e0\u4e2a\u6838\u5fc3\u601d\u60f3\uff1a \u5229\u7528\u5f3a\u5927\u7684\u57fa\u7840\u6a21\u578b\u5148\u9a8c\u77e5\u8bc6 (Foundation Model Priors): \u5f15\u5165\u4e86\u9ad8\u6027\u80fd\u7684BioCLIP2\u57fa\u7840\u6a21\u578b\u4f5c\u4e3a\u201c\u6559\u5e08\u201d\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u53ef\u80fd\u5728\u5927\u89c4\u6a21\u751f\u7269\u56fe\u50cf\u548c\u6587\u672c\u6570\u636e\u4e0a\u8fdb\u884c\u4e86\u9884\u8bad\u7ec3\uff0c\u4ece\u800c\u5177\u5907\u4e86\u5f3a\u5927\u7684\u7279\u5f81\u63d0\u53d6\u548c\u6cdb\u5316\u80fd\u529b\u3002 \u77e5\u8bc6\u84b8\u998f (Knowledge Distillation) \u5b9e\u73b0\u6a21\u578b\u8f7b\u91cf\u5316: \u5c06BioCLIP2\u7684\u4e30\u5bcc\u77e5\u8bc6\u84b8\u998f\u5230\u4e00\u4e2a\u8ba1\u7b97\u6210\u672c\u663e\u8457\u66f4\u4f4e\u7684ConvNeXt-tiny\u201c\u5b66\u751f\u201d\u6a21\u578b\u4e2d\u3002\u8fd9\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u5927\u5e45\u51cf\u5c11\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u90e8\u7f72\u73af\u5883\u3002 \u4e13\u5bb6\u4fe1\u606f\u5f15\u5bfc\u7684\u9886\u57df\u9002\u5e94 (Expert-Informed Adaptation): \u7ed3\u5408\u201c\u6709\u9650\u7684\u4e13\u5bb6\u6807\u6ce8\u91ce\u5916\u6570\u636e\u201d\u6765\u5fae\u8c03\u6216\u6307\u5bfc\u84b8\u998f\u8fc7\u7a0b\uff0c\u76f4\u63a5\u89e3\u51b3\u7b56\u5c55\u56fe\u50cf\u4e0e\u5608\u6742\u91ce\u5916\u56fe\u50cf\u4e4b\u95f4\u7684\u9886\u57df\u6f02\u79fb\u95ee\u9898\u3002\u8fd9\u79cd\u6709\u9488\u5bf9\u6027\u7684\u6570\u636e\u5229\u7528\u662f\u5f25\u5408\u9886\u57df\u5dee\u8ddd\u7684\u5173\u952e\u3002 \u9488\u5bf9\u7cbe\u7ec6\u7c92\u5ea6\u5206\u7c7b\u7684\u5b9e\u7528\u89e3\u51b3\u65b9\u6848: \u9488\u5bf9\u98de\u86fe\u8fd9\u79cd\u5177\u6709\u9ad8\u5ea6\u76f8\u4f3c\u7269\u79cd\u7684\u7cbe\u7ec6\u7c92\u5ea6\u5206\u7c7b\u4efb\u52a1\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5728\u771f\u5b9e\u4e16\u754c\u3001\u5608\u6742\u6570\u636e\u573a\u666f\u4e0b\u65e2\u51c6\u786e\u53c8\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002","title":"2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u5b66\u65b9\u6cd5"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-29/#3_3","text":"\u63a8\u52a8\u9ad8\u6548\u3001\u53ef\u90e8\u7f72\u7684CV\u7cfb\u7edf\u53d1\u5c55: \u8bc1\u660e\u4e86\u901a\u8fc7\u57fa\u7840\u6a21\u578b\u84b8\u998f\u53ef\u4ee5\u6709\u6548\u5730\u5c06\u5927\u578b\u6a21\u578b\u7684\u6027\u80fd\u4f18\u52bf\u8f6c\u79fb\u5230\u5c0f\u578b\u3001\u9ad8\u6548\u7684\u6a21\u578b\u4e0a\uff0c\u8fd9\u5bf9\u4e8e\u8fb9\u7f18\u8ba1\u7b97\u3001\u5b9e\u65f6\u76d1\u6d4b\u548c\u8d44\u6e90\u53d7\u9650\u7684\u5e94\u7528\u573a\u666f\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002 \u4e3a\u9886\u57df\u9002\u5e94\u63d0\u4f9b\u65b0\u8303\u5f0f: \u5f3a\u8c03\u4e86\u7ed3\u5408\u57fa\u7840\u6a21\u578b\u5148\u9a8c\u77e5\u8bc6\u548c\u5c11\u91cf\u76ee\u6807\u9886\u57df\u4e13\u5bb6\u6570\u636e\u8fdb\u884c\u77e5\u8bc6\u84b8\u998f\uff0c\u662f\u89e3\u51b3\u9886\u57df\u6f02\u79fb\u95ee\u9898\u7684\u4e00\u79cd\u5f3a\u5927\u4e14\u5b9e\u7528\u7684\u7b56\u7565\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u6570\u636e\u6807\u6ce8\u6210\u672c\u9ad8\u6602\u7684\u4e13\u4e1a\u9886\u57df\u3002 \u52a0\u901f\u751f\u7269\u591a\u6837\u6027\u76d1\u6d4b\u548c\u751f\u6001\u7814\u7a76: \u4e3a\u81ea\u52a8\u5316\u6606\u866b\uff08\u53ca\u5176\u4ed6\u751f\u7269\uff09\u8bc6\u522b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6280\u672f\u652f\u6491\uff0c\u6709\u52a9\u4e8e\u66f4\u51c6\u786e\u3001\u66f4\u9ad8\u6548\u5730\u76d1\u6d4b\u7269\u79cd\u6570\u91cf\u548c\u5206\u5e03\uff0c\u4ece\u800c\u66f4\u597d\u5730\u7406\u89e3\u548c\u5e94\u5bf9\u751f\u7269\u591a\u6837\u6027\u4e0b\u964d\u95ee\u9898\u3002 \u9a8c\u8bc1\u57fa\u7840\u6a21\u578b\u5728\u7279\u5b9a\u9886\u57df\u7684\u4ef7\u503c: \u8fdb\u4e00\u6b65\u5de9\u56fa\u4e86\u50cfCLIP\u8fd9\u7c7b\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u5728\u7279\u5b9a\u3001\u4e13\u4e1a\u9886\u57df\uff08\u5982\u751f\u7269\u56fe\u50cf\u5206\u6790\uff09\u7684\u5f3a\u5927\u8fc1\u79fb\u5b66\u4e60\u80fd\u529b\u548c\u4f5c\u4e3a\u5f3a\u5927\u7279\u5f81\u63d0\u53d6\u5668\u7684\u6f5c\u529b\u3002","title":"3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-29/#4_3","text":"\u5176\u4ed6\u7cbe\u7ec6\u7c92\u5ea6\u5206\u7c7b\u4efb\u52a1: \u4f8b\u5982\uff0c\u9e1f\u7c7b\u7269\u79cd\u8bc6\u522b\u3001\u690d\u7269\u75c5\u5bb3\u8bca\u65ad\u3001\u533b\u5b66\u5f71\u50cf\u4e2d\u5fae\u5c0f\u75c5\u7076\u5206\u7c7b\u3001\u5de5\u4e1a\u4ea7\u54c1\u7f3a\u9677\u68c0\u6d4b\u7b49\uff0c\u8fd9\u4e9b\u4efb\u52a1\u901a\u5e38\u9762\u4e34\u6570\u636e\u7a00\u7f3a\u3001\u9886\u57df\u6f02\u79fb\u548c\u5bf9\u9ad8\u7cbe\u5ea6\u8981\u6c42\u7684\u95ee\u9898\u3002 \u751f\u6001\u5b66\u548c\u751f\u7269\u591a\u6837\u6027\u76d1\u6d4b: \u9664\u4e86\u98de\u86fe\uff0c\u8be5\u65b9\u6cd5\u53ef\u63a8\u5e7f\u5230\u5176\u4ed6\u6606\u866b\u3001\u9c7c\u7c7b\u3001\u4e24\u6816\u52a8\u7269\u3001\u690d\u7269\u7b49\u7269\u79cd\u7684\u81ea\u52a8\u5316\u8bc6\u522b\u548c\u8ba1\u6570\uff0c\u5c24\u5176\u662f\u5728\u91ce\u5916\u590d\u6742\u73af\u5883\u4e0b\u3002 \u519c\u4e1a\u79d1\u6280: \u519c\u4f5c\u7269\u75c5\u866b\u5bb3\u7684\u65e9\u671f\u9884\u8b66\u548c\u8bc6\u522b\uff0c\u901a\u8fc7\u90e8\u7f72\u8f7b\u91cf\u7ea7\u6a21\u578b\u5728\u7530\u95f4\u8bbe\u5907\u4e0a\u8fdb\u884c\u5b9e\u65f6\u76d1\u6d4b\u3002 \u9065\u611f\u548c\u73af\u5883\u76d1\u6d4b: \u4ece\u536b\u661f\u6216\u65e0\u4eba\u673a\u56fe\u50cf\u4e2d\u8bc6\u522b\u7279\u5b9a\u5730\u7269\u3001\u690d\u88ab\u7c7b\u578b\u6216\u73af\u5883\u53d8\u5316\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u5feb\u901f\u5904\u7406\u548c\u90e8\u7f72\u7684\u573a\u666f\u3002 \u533b\u7597\u5f71\u50cf\u5206\u6790: \u5c06\u5728\u5927\u578b\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u9884\u8bad\u7ec3\u7684\u6a21\u578b\u9002\u5e94\u5230\u7279\u5b9a\u533b\u9662\u6216\u8bbe\u5907\u4ea7\u751f\u7684\u4e34\u5e8a\u6570\u636e\u4e0a\uff0c\u4ee5\u8f85\u52a9\u8bca\u65ad\uff0c\u5c24\u5176\u662f\u5728\u7f55\u89c1\u75be\u75c5\u6216\u9690\u79c1\u654f\u611f\u6570\u636e\u53d7\u9650\u7684\u60c5\u51b5\u4e0b\u3002 \u5de5\u4e1a\u81ea\u52a8\u5316\u548c\u8d28\u91cf\u63a7\u5236: \u5728\u751f\u4ea7\u7ebf\u4e0a\u90e8\u7f72\u8f7b\u91cf\u7ea7\u6a21\u578b\u8fdb\u884c\u5b9e\u65f6\u4ea7\u54c1\u8d28\u91cf\u68c0\u6d4b\uff0c\u8bc6\u522b\u5fae\u5c0f\u7f3a\u9677\u3002","title":"4. \u53ef\u80fd\u53d7\u76ca\u4e8e\u6b64\u7814\u7a76\u7684\u76f8\u5173\u9886\u57df\u6216\u5e94\u7528"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-29/#5_3","text":"\u901a\u7528\u6027/\u6cdb\u5316\u80fd\u529b: \u5b9e\u9a8c\u4ec5\u5728\u201c101\u79cd\u4e39\u9ea6\u98de\u86fe\u201d\u548c\u201cAMI\u76f8\u673a\u7cfb\u7edf\u201d\u4e0a\u8fdb\u884c\u3002\u8be5\u65b9\u6cd5\u5728\u66f4\u5e7f\u6cdb\u7684\u98de\u86fe\u7269\u79cd\u3001\u4e0d\u540c\u5730\u7406\u533a\u57df\u3001\u4e0d\u540c\u6c14\u5019\u6761\u4ef6\u6216\u4e0d\u540c\u7c7b\u578b\u76f8\u673a\u7cfb\u7edf\uff08\u5206\u8fa8\u7387\u3001\u5149\u7167\u3001\u566a\u58f0\u7279\u6027\uff09\u4e0b\u7684\u8868\u73b0\u4ecd\u9700\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u3002 \u201c\u6709\u9650\u4e13\u5bb6\u6807\u6ce8\u91ce\u5916\u6570\u636e\u201d\u7684\u6570\u91cf\u672a\u660e\u786e: \u6458\u8981\u672a\u5177\u4f53\u8bf4\u660e\u201c\u6709\u9650\u201d\u6570\u636e\u7684\u91cf\u7ea7\u3002\u8be5\u65b9\u6cd5\u5bf9\u6570\u636e\u91cf\u7684\u654f\u611f\u6027\u5982\u4f55\uff1f\u5728\u6570\u636e\u6781\u5ea6\u7a00\u7f3a\uff08\u4f8b\u5982\uff0c\u53ea\u6709\u51e0\u5341\u5f20\u56fe\u50cf\uff09\u7684\u60c5\u51b5\u4e0b\u662f\u5426\u4f9d\u7136\u6709\u6548\uff1f \u5bf9\u7279\u5b9a\u57fa\u7840\u6a21\u578b\u7684\u4f9d\u8d56: \u6a21\u578b\u7684\u6027\u80fd\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4f9d\u8d56\u4e8eBioCLIP2\u7684\u5f3a\u5927\u80fd\u529b\u3002\u5982\u679cBioCLIP2\u672c\u8eab\u5b58\u5728\u5c40\u9650\u6027\uff0c\u6216\u8005\u672a\u6765\u51fa\u73b0\u66f4\u4f18\u7684\u57fa\u7840\u6a21\u578b\uff0c\u5219\u9700\u8981\u91cd\u65b0\u8bc4\u4f30\u6216\u66f4\u65b0\u3002 \u77e5\u8bc6\u84b8\u998f\u7684\u56fa\u6709\u5c40\u9650: \u77e5\u8bc6\u84b8\u998f\u901a\u5e38\u610f\u5473\u7740\u5b66\u751f\u6a21\u578b\u7684\u6027\u80fd\u4e0a\u9650\u7531\u6559\u5e08\u6a21\u578b\u51b3\u5b9a\uff0c\u5b66\u751f\u6a21\u578b\u5f88\u96be\u8d85\u8d8a\u6559\u5e08\u6a21\u578b\u3002\u8fd9\u610f\u5473\u7740BioCLIP2\u7684\u4efb\u4f55\u56fa\u6709\u504f\u5dee\u6216\u9519\u8bef\u90fd\u53ef\u80fd\u88ab\u4f20\u9012\u7ed9ConvNeXt-tiny\u3002 \u73af\u5883\u9c81\u68d2\u6027\u672a\u8be6\u8ff0: \u6458\u8981\u672a\u63d0\u53ca\u6a21\u578b\u5728\u6781\u7aef\u91ce\u5916\u73af\u5883\u6761\u4ef6\uff08\u5982\u6076\u52a3\u5929\u6c14\u3001\u5149\u7167\u5267\u70c8\u53d8\u5316\u3001\u906e\u6321\u3001\u90e8\u5206\u53ef\u89c1\u7b49\uff09\u4e0b\u7684\u9c81\u68d2\u6027\u8868\u73b0\u3002 \u8ba1\u7b97\u6210\u672c\u964d\u4f4e\u7684\u5177\u4f53\u91cf\u5316: \u867d\u7136\u63d0\u5230\u4e86\u201c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u201d\uff0c\u4f46\u6ca1\u6709\u7ed9\u51fa\u5177\u4f53\u7684\u91cf\u5316\u6307\u6807\uff08\u5982FLOPs\u3001\u53c2\u6570\u91cf\u3001\u63a8\u7406\u65f6\u95f4\u7b49\uff09\uff0c\u8fd9\u4f7f\u5f97\u96be\u4ee5\u76f4\u63a5\u6bd4\u8f83\u5176\u6548\u7387\u63d0\u5347\u7684\u7a0b\u5ea6\u3002 Key Findings: We propose a lightweight classification approach, combining limited expert-labelled field data with knowledge distillation from the high-performance BioCLIP2 foundation model into a ConvNeXt-tiny architecture. Experiments on 101 Danish moth species from AMI camera systems demonstrate that BioCLIP2 substantially outperforms other methods and that our distilled lightweight model achieves comparable accuracy with significantly reduced computational cost. Links: PDF arXiv","title":"5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u7684\u5c40\u9650\u6027"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-29/#openm3d-open-vocabulary-multi-view-indoor-3d-object-detection-without-human-annotations","text":"Authors: Peng-Hao Hsu, Ke Zhang, Fu-En Wang, Tao Tu, Ming-Feng Li, Yu-Lun Liu, Albert Y. C. Chen, Min Sun, Cheng-Hao Kuo Published: 2025-08-27 Categories: cs.CV Abstract: Open-vocabulary (OV) 3D object detection is an emerging field, yet its exploration through image-based methods remains limited compared to 3D point cloud-based methods. We introduce OpenM3D, a novel open-vocabulary multi-view indoor 3D object detector trained without human annotations. In particular, OpenM3D is a single-stage detector adapting the 2D-induced voxel features from the ImGeoNet model. To support OV, it is jointly trained with a class-agnostic 3D localization loss requiring high-quality 3D pseudo boxes and a voxel-semantic alignment loss requiring diverse pre-trained CLIP features. We follow the training setting of OV-3DET where posed RGB-D images are given but no human annotations of 3D boxes or classes are available. We propose a 3D Pseudo Box Generation method using a graph embedding technique that combines 2D segments into coherent 3D structures. Our pseudo-boxes achieve higher precision and recall than other methods, including the method proposed in OV-3DET. We further sample diverse CLIP features from 2D segments associated with each coherent 3D structure to align with the corresponding voxel feature. The key to training a highly accurate single-stage detector requires both losses to be learned toward high-quality targets. At inference, OpenM3D, a highly efficient detector, requires only multi-view images for input and demonstrates superior accuracy and speed (0.3 sec. per scene) on ScanNet200 and ARKitScenes indoor benchmarks compared to existing methods. We outperform a strong two-stage method that leverages our class-agnostic detector with a ViT CLIP-based OV classifier and a baseline incorporating multi-view depth estimator on both accuracy and speed. Analysis: \u8fd9\u7bc7\u8bba\u6587\u300aOpenM3D: Open Vocabulary Multi-view Indoor 3D Object Detection without Human Annotations\u300b\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\uff0c\u7279\u522b\u662f3D\u76ee\u6807\u68c0\u6d4b\u65b9\u9762\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u5f15\u4eba\u6ce8\u76ee\u7684\u65b0\u65b9\u6cd5\u3002\u4ee5\u4e0b\u662f\u8be6\u7ec6\u5206\u6790\uff1a","title":"OpenM3D: Open Vocabulary Multi-view Indoor 3D Object Detection without Human Annotations"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-29/#1-2-3_3","text":"OpenM3D\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5355\u9636\u6bb5\u3001\u5f00\u653e\u8bcd\u6c47\u3001\u591a\u89c6\u89d2\u5ba4\u51853D\u76ee\u6807\u68c0\u6d4b\u5668\uff0c\u5176\u6838\u5fc3\u521b\u65b0\u5728\u4e8e \u65e0\u9700\u4eba\u5de5\u6807\u6ce8 \u5373\u53ef\u8fdb\u884c\u8bad\u7ec3\u3002\u5b83\u901a\u8fc7\u7ed3\u5408\u9ad8\u8d28\u91cf\u76843D\u4f2a\u6846\u751f\u6210\u65b9\u6cd5\uff08\u57fa\u4e8e\u56fe\u5d4c\u5165\uff09\u548c\u5229\u7528\u591a\u6837\u5316CLIP\u7279\u5f81\u7684\u4f53\u7d20\u8bed\u4e49\u5bf9\u9f50\u635f\u5931\uff0c\u5b9e\u73b0\u4e86\u5728\u5ba4\u5185\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u7684\u51c6\u786e\u6027\u548c\u901f\u5ea6\u3002\u8fd9\u9879\u5de5\u4f5c\u663e\u8457\u964d\u4f4e\u4e863D\u68c0\u6d4b\u7684\u6807\u6ce8\u6210\u672c\uff0c\u5e76\u63a8\u52a8\u4e86\u56fe\u50cf\u57fa3D\u5f00\u653e\u8bcd\u6c47\u68c0\u6d4b\u7684\u53d1\u5c55\u3002","title":"1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u6d01\u603b\u7ed3 (2-3\u53e5\u8bdd)"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-29/#2_4","text":"OpenM3D\u7684\u5173\u952e\u521b\u65b0\u5728\u4e8e\u5176 \u65e0\u4eba\u5de5\u6807\u6ce8\u7684\u8bad\u7ec3\u8303\u5f0f \u4ee5\u53ca\u5b9e\u73b0\u8fd9\u4e00\u8303\u5f0f\u6240\u91c7\u7528\u7684\u72ec\u7279\u65b9\u6cd5\uff1a \u65e0\u4eba\u5de5\u6807\u6ce8\u7684\u8bad\u7ec3\uff1a \u8fd9\u662f\u6700\u6838\u5fc3\u7684\u521b\u65b0\u70b9\u3002\u8bba\u6587\u660e\u786e\u6307\u51fa\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u4e0d\u4f7f\u7528\u4efb\u4f55\u4eba\u5de5\u6807\u6ce8\u76843D\u8fb9\u754c\u6846\u6216\u7c7b\u522b\u4fe1\u606f\u3002\u8fd9\u6781\u5927\u5730\u964d\u4f4e\u4e863D\u68c0\u6d4b\u7684\u6807\u6ce8\u6210\u672c\uff0c\u662f\u8be5\u9886\u57df\u7684\u4e00\u4e2a\u91cd\u8981\u7a81\u7834\u3002 \u9ad8\u8d28\u91cf3D\u4f2a\u6846\u751f\u6210\uff1a \u4e3a\u4e86\u5f25\u8865\u7f3a\u4e4f\u4eba\u5de5\u6807\u6ce8\u76843D\u8fb9\u754c\u6846\uff0cOpenM3D\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u76843D\u4f2a\u6846\u751f\u6210\u65b9\u6cd5\u3002\u5b83\u5229\u7528 \u56fe\u5d4c\u5165\u6280\u672f \u5c062D\u56fe\u50cf\u4e2d\u7684\u5206\u5272\u7ed3\u679c\u7ec4\u5408\u6210\u8fde\u8d2f\u76843D\u7ed3\u6784\uff0c\u4ece\u800c\u751f\u6210\u9ad8\u8d28\u91cf\u76843D\u4f2a\u6846\u3002\u8fd9\u4e9b\u4f2a\u6846\u7528\u4e8e\u8bad\u7ec3\u7c7b\u4e0d\u53ef\u77e5\u76843D\u5b9a\u4f4d\u635f\u5931\u3002 \u4f53\u7d20\u8bed\u4e49\u5bf9\u9f50\u635f\u5931\u4e0e\u591a\u6837\u5316CLIP\u7279\u5f81\uff1a \u4e3a\u4e86\u5b9e\u73b0\u5f00\u653e\u8bcd\u6c47\u80fd\u529b\uff0cOpenM3D\u5f15\u5165\u4e86\u4f53\u7d20\u8bed\u4e49\u5bf9\u9f50\u635f\u5931\u3002\u5b83\u4ece\u4e0e\u6bcf\u4e2a\u8fde\u8d2f3D\u7ed3\u6784\u76f8\u5173\u76842D\u5206\u5272\u4e2d\u91c7\u6837 \u591a\u6837\u5316\u7684\u9884\u8bad\u7ec3CLIP\u7279\u5f81 \uff0c\u5e76\u5c06\u5176\u4e0e\u5bf9\u5e94\u76843D\u4f53\u7d20\u7279\u5f81\u8fdb\u884c\u5bf9\u9f50\u3002\u8fd9\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u7406\u89e3\u548c\u8bc6\u522b\u8bad\u7ec3\u4e2d\u672a\u89c1\u7684\u7269\u4f53\u7c7b\u522b\u3002 \u5355\u9636\u6bb5\u9ad8\u6548\u68c0\u6d4b\u5668\uff1a OpenM3D\u662f\u4e00\u4e2a\u5355\u9636\u6bb5\u68c0\u6d4b\u5668\uff0c\u5b83\u5229\u7528ImGeoNet\u6a21\u578b\u5bfc\u51fa\u76842D\u8bf1\u5bfc\u4f53\u7d20\u7279\u5f81\u3002\u7ed3\u5408\u4e0a\u8ff0\u4e24\u79cd\u635f\u5931\uff0c\u5b83\u5728\u63a8\u7406\u65f6\u4ec5\u9700\u591a\u89c6\u89d2\u56fe\u50cf\u8f93\u5165\uff0c\u4fbf\u80fd\u5b9e\u73b0\u9ad8\u6548\u7387\uff080.3\u79d2/\u573a\u666f\uff09\u548c\u9ad8\u7cbe\u5ea6\u3002","title":"2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u8bba"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-29/#3_4","text":"\u964d\u4f4e\u6807\u6ce8\u6210\u672c\uff0c\u52a0\u901f\u7814\u7a76\u4e0e\u5e94\u7528\uff1a 3D\u76ee\u6807\u68c0\u6d4b\uff0c\u7279\u522b\u662f\u5ba4\u5185\u573a\u666f\uff0c\u5176\u6570\u636e\u6807\u6ce8\u6210\u672c\u6781\u9ad8\u3002OpenM3D\u7684\u65e0\u4eba\u5de5\u6807\u6ce8\u8bad\u7ec3\u8303\u5f0f\u5c06\u6781\u5927\u5730\u964d\u4f4e\u8fd9\u4e00\u95e8\u69db\uff0c\u4f7f\u5f97\u7814\u7a76\u4eba\u5458\u548c\u5f00\u53d1\u8005\u80fd\u591f\u66f4\u5feb\u5730\u63a2\u7d22\u548c\u90e8\u7f723D\u89c6\u89c9\u7cfb\u7edf\uff0c\u800c\u65e0\u9700\u6295\u5165\u5927\u91cf\u8d44\u6e90\u8fdb\u884c\u6570\u636e\u6807\u6ce8\u3002 \u63a8\u52a8\u5f00\u653e\u8bcd\u6c473D\u68c0\u6d4b\u53d1\u5c55\uff1a \u76ee\u524d\u5f00\u653e\u8bcd\u6c473D\u68c0\u6d4b\u4ecd\u5904\u4e8e\u65e9\u671f\u9636\u6bb5\uff0c\u4e14\u56fe\u50cf\u57fa\u65b9\u6cd5\u76f8\u5bf9\u6ede\u540e\u3002OpenM3D\u7684\u6210\u529f\u8bc1\u660e\u4e86\u56fe\u50cf\u57fa\u65b9\u6cd5\u5728\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u5f00\u653e\u8bcd\u6c473D\u68c0\u6d4b\u7684\u6f5c\u529b\uff0c\u5c06\u6fc0\u52b1\u66f4\u591a\u7814\u7a76\u8005\u6295\u5165\u5230\u8fd9\u4e00\u65b9\u5411\u3002 \u63d0\u5347\u56fe\u50cf\u57fa3D\u68c0\u6d4b\u7684\u7ade\u4e89\u529b\uff1a \u4f20\u7edf\u4e0a\uff0c\u57fa\u4e8e\u70b9\u4e91\u76843D\u68c0\u6d4b\u65b9\u6cd5\u5728\u7cbe\u5ea6\u4e0a\u5f80\u5f80\u4f18\u4e8e\u56fe\u50cf\u57fa\u65b9\u6cd5\u3002OpenM3D\u5c55\u793a\u4e86\u56fe\u50cf\u57fa\u65b9\u6cd5\u5728\u65e0\u76d1\u7763\u8bbe\u7f6e\u4e0b\u4e5f\u80fd\u8fbe\u5230\u5353\u8d8a\u7684\u6027\u80fd\uff0c\u751a\u81f3\u8d85\u8d8a\u4e86\u67d0\u4e9b\u5f3a\u52b2\u7684\u57fa\u7ebf\u548c\u4e24\u9636\u6bb5\u65b9\u6cd5\uff0c\u8fd9\u6709\u52a9\u4e8e\u63d0\u5347\u56fe\u50cf\u57fa3D\u68c0\u6d4b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u7ade\u4e89\u529b\u3002 \u4fc3\u8fdb\u81ea\u76d1\u7763/\u5f31\u76d1\u77633D\u5b66\u4e60\uff1a \u8be5\u8bba\u6587\u4e3a3D\u89c6\u89c9\u9886\u57df\u7684\u81ea\u76d1\u7763\u548c\u5f31\u76d1\u7763\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u548c\u8303\u4f8b\uff0c\u7279\u522b\u662f\u5982\u4f55\u6709\u6548\u5730\u4ece2D\u4fe1\u606f\uff08\u59822D\u5206\u5272\u3001CLIP\u7279\u5f81\uff09\u4e2d\u63d0\u53d63D\u8bed\u4e49\u548c\u51e0\u4f55\u4fe1\u606f\u3002","title":"3. \u5bf9\u8be5\u9886\u57df\u7684\u6f5c\u5728\u5f71\u54cd"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-29/#4_4","text":"\u673a\u5668\u4eba\u5b66\u548c\u81ea\u4e3b\u7cfb\u7edf\uff1a \u673a\u5668\u4eba\u9700\u8981\u5728\u672a\u77e5\u6216\u4e0d\u65ad\u53d8\u5316\u7684\u73af\u5883\u4e2d\u8bc6\u522b\u548c\u64cd\u4f5c\u5404\u79cd\u7269\u4f53\u3002OpenM3D\u7684\u5f00\u653e\u8bcd\u6c47\u548c\u65e0\u6807\u6ce8\u7279\u6027\u4f7f\u5176\u975e\u5e38\u9002\u5408\u673a\u5668\u4eba\u5bfc\u822a\u3001\u6293\u53d6\u548c\u573a\u666f\u7406\u89e3\uff0c\u5c24\u5176\u662f\u5728\u5bb6\u5ead\u3001\u529e\u516c\u5ba4\u7b49\u5ba4\u5185\u73af\u5883\u4e2d\u3002 \u589e\u5f3a\u73b0\u5b9e (AR) / \u865a\u62df\u73b0\u5b9e (VR)\uff1a AR/VR\u5e94\u7528\u9700\u8981\u5b9e\u65f6\u3001\u51c6\u786e\u5730\u8bc6\u522b\u548c\u5b9a\u4f4d\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u7269\u4f53\uff0c\u4ee5\u4fbf\u8fdb\u884c\u865a\u62df\u5185\u5bb9\u7684\u53e0\u52a0\u548c\u4ea4\u4e92\u3002OpenM3D\u53ef\u4ee5\u5e2e\u52a9AR/VR\u7cfb\u7edf\u66f4\u597d\u5730\u7406\u89e3\u7528\u6237\u5468\u56f4\u7684\u73af\u5883\uff0c\u5b9e\u73b0\u66f4\u6c89\u6d78\u5f0f\u7684\u4f53\u9a8c\u3002 \u6570\u5b57\u5b6a\u751f\u548c\u5ba4\u5185\u6d4b\u7ed8\uff1a \u81ea\u52a8\u5316\u5730\u6784\u5efa\u5177\u6709\u8bed\u4e49\u4fe1\u606f\u7684\u5ba4\u51853D\u6a21\u578b\uff08\u6570\u5b57\u5b6a\u751f\uff09\u662f\u8bb8\u591a\u884c\u4e1a\u7684\u9700\u6c42\u3002OpenM3D\u53ef\u4ee5\u7528\u4e8e\u81ea\u52a8\u8bc6\u522b\u548c\u5206\u7c7b\u5ba4\u5185\u7269\u4f53\uff0c\u52a0\u901f3D\u6a21\u578b\u7684\u8bed\u4e49\u6807\u6ce8\u8fc7\u7a0b\u3002 \u667a\u80fd\u5bb6\u5c45\u548c\u7269\u8054\u7f51 (IoT)\uff1a \u667a\u80fd\u8bbe\u5907\u9700\u8981\u7406\u89e3\u5176\u6240\u5904\u7684\u73af\u5883\u548c\u5176\u4e2d\u7684\u7269\u4f53\u3002OpenM3D\u53ef\u4ee5\u8d4b\u80fd\u667a\u80fd\u5bb6\u5c45\u7cfb\u7edf\uff0c\u4f7f\u5176\u80fd\u591f\u8bc6\u522b\u66f4\u591a\u79cd\u7c7b\u7684\u7269\u4f53\uff0c\u4ece\u800c\u63d0\u4f9b\u66f4\u667a\u80fd\u3001\u66f4\u4e2a\u6027\u5316\u7684\u670d\u52a1\u3002 \u5b89\u5168\u76d1\u63a7\uff1a \u5728\u76d1\u63a7\u573a\u666f\u4e2d\uff0c\u8bc6\u522b\u7279\u5b9a\u7269\u4f53\u6216\u5f02\u5e38\u4e8b\u4ef6\u81f3\u5173\u91cd\u8981\u3002\u5f00\u653e\u8bcd\u6c47\u80fd\u529b\u53ef\u4ee5\u5e2e\u52a9\u7cfb\u7edf\u68c0\u6d4b\u9884\u5b9a\u4e49\u7c7b\u522b\u4e4b\u5916\u7684\u6f5c\u5728\u5a01\u80c1\u6216\u611f\u5174\u8da3\u7684\u7269\u4f53\u3002","title":"4. \u53ef\u80fd\u53d7\u76ca\u4e8e\u8fd9\u9879\u7814\u7a76\u7684\u76f8\u5173\u9886\u57df\u6216\u5e94\u7528"},{"location":"other_categories/daily/arxiv_cv_report_2025-08-29/#5_4","text":"\u9886\u57df\u9650\u5236\uff1a \u8bba\u6587\u660e\u786e\u6307\u51fa\u662f\u9488\u5bf9\u201c \u5ba4\u5185 \u201d3D\u76ee\u6807\u68c0\u6d4b\u3002\u8fd9\u610f\u5473\u7740\u8be5\u65b9\u6cd5\u53ef\u80fd\u4e0d\u76f4\u63a5\u9002\u7528\u4e8e\u5ba4\u5916\u573a\u666f\uff0c\u56e0\u4e3a\u5ba4\u5916\u73af\u5883\u7684\u7269\u4f53\u7c7b\u578b\u3001\u5c3a\u5ea6\u3001\u5149\u7167\u6761\u4ef6\u548c\u906e\u6321\u6a21\u5f0f\u4e0e\u5ba4\u5185\u6709\u663e\u8457\u5dee\u5f02\u3002 \u8f93\u5165\u6570\u636e\u8981\u6c42\uff1a \u5c3d\u7ba1\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\uff0c\u4f46\u6a21\u578b\u4ecd\u9700\u8981\u201c \u591a\u89c6\u89d2RGB-D\u56fe\u50cf \u201d\u4f5c\u4e3a\u8f93\u5165\uff0c\u5e76\u4e14\u8fd9\u4e9b\u56fe\u50cf\u662f\u201c \u5df2\u59ff\u6001\u5316 (posed) \u201d\u7684\u3002\u8fd9\u610f\u5473\u7740\u5b83\u4f9d\u8d56\u4e8e\u6df1\u5ea6\u4f20\u611f\u5668\u548c\u51c6\u786e\u7684\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\uff0c\u8fd9\u5728\u67d0\u4e9b\u5e94\u7528\u573a\u666f\u4e2d\u53ef\u80fd\u662f\u4e00\u4e2a\u9650\u5236\uff0c\u4f8b\u5982\u7eafRGB\u8f93\u5165\u6216\u6ca1\u6709\u7cbe\u786e\u59ff\u6001\u4fe1\u606f\u7684\u573a\u666f\u3002 \u5bf9\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u4f9d\u8d56\uff1a \u6a21\u578b\u7684\u6027\u80fd\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4f9d\u8d56\u4e8e\u5176\u6240\u4f7f\u7528\u7684\u9884\u8bad\u7ec3\u6a21\u578b\uff08\u5982ImGeoNet\u7528\u4e8e2D\u8bf1\u5bfc\u4f53\u7d20\u7279\u5f81\uff0cCLIP\u7528\u4e8e\u8bed\u4e49\u7279\u5f81\uff09\u7684\u8d28\u91cf\u548c\u6cdb\u5316\u80fd\u529b\u3002\u5982\u679c\u8fd9\u4e9b\u57fa\u7840\u6a21\u578b\u5b58\u5728\u504f\u5dee\u6216\u5728\u7279\u5b9a\u9886\u57df\u8868\u73b0\u4e0d\u4f73\uff0c\u53ef\u80fd\u4f1a\u5f71\u54cdOpenM3D\u7684\u6700\u7ec8\u6027\u80fd\u3002 \u4f2a\u6807\u7b7e\u8d28\u91cf\u7684\u654f\u611f\u6027\uff1a \u6458\u8981\u4e2d\u5f3a\u8c03\u4e86\u201c\u9ad8\u8d28\u91cf3D\u4f2a\u6846\u201d\u5bf9\u4e8e\u8bad\u7ec3\u51c6\u786e\u7684\u5355\u9636\u6bb5\u68c0\u6d4b\u5668\u7684\u91cd\u8981\u6027\u3002\u8fd9\u610f\u5473\u7740\u4f2a\u6846\u751f\u6210\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\u81f3\u5173\u91cd\u8981\u3002\u5728\u6781\u7aef\u590d\u6742\u3001\u9ad8\u5ea6\u6742\u4e71\u6216\u5305\u542b\u5927\u91cf\u900f\u660e/\u53cd\u5c04\u7269\u4f53\u7684\u5ba4\u5185\u573a\u666f\u4e2d\uff0c\u4f2a\u6846\u7684\u751f\u6210\u8d28\u91cf\u53ef\u80fd\u4f1a\u4e0b\u964d\uff0c\u4ece\u800c\u5f71\u54cd\u6574\u4f53\u6027\u80fd\u3002 \u201c\u5f00\u653e\u8bcd\u6c47\u201d\u7684\u771f\u6b63\u6cdb\u5316\u80fd\u529b\uff1a \u5c3d\u7ba1\u4f7f\u7528\u4e86CLIP\u7279\u5f81\uff0c\u4f46CLIP\u672c\u8eab\u662f\u5728\u5927\u91cf2D\u56fe\u50cf\u4e0a\u8bad\u7ec3\u7684\u3002\u5b83\u5bf93D\u7269\u4f53\u5c5e\u6027\u7684\u7406\u89e3\uff0c\u4ee5\u53ca\u5728\u975e\u5e38\u89c4\u89c6\u89d2\u6216\u9ad8\u5ea6\u906e\u6321\u60c5\u51b5\u4e0b\u7684\u8bc6\u522b\u80fd\u529b\uff0c\u53ef\u80fd\u4ecd\u6709\u5c40\u9650\u6027\u3002\u6a21\u578b\u5bf9\u5b8c\u5168\u65b0\u9896\u3001\u4e0e2D\u8bad\u7ec3\u6570\u636e\u5dee\u5f02\u5f88\u5927\u76843D\u7269\u4f53\u7684\u6cdb\u5316\u80fd\u529b\u4ecd\u9700\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u3002 Key Findings: We introduce OpenM3D, a novel open-vocabulary multi-view indoor 3D object detector trained without human annotations. We propose a 3D Pseudo Box Generation method using a graph embedding technique that combines 2D segments into coherent 3D structures. Links: PDF arXiv","title":"5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u7684\u5c40\u9650\u6027"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-01/","text":"Arxiv Computer Vision Papers - 2025-09-01 Executive Summary \u597d\u7684\uff0c\u8fd9\u662f\u4e00\u4efd\u9488\u5bf92025\u5e748\u670829\u65e5Arxiv\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u8bba\u6587\u7684\u6bcf\u65e5\u62a5\u544a\u6267\u884c\u6458\u8981\uff1a Arxiv \u8ba1\u7b97\u673a\u89c6\u89c9\u6bcf\u65e5\u62a5\u544a\u6267\u884c\u6458\u8981 (2025\u5e748\u670829\u65e5) \u6982\u8ff0\uff1a \u4eca\u65e5Arxiv\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u7684\u8bba\u6587\u5448\u73b0\u51fa\u51e0\u4e2a\u663e\u8457\u8d8b\u52bf\uff1a \u5408\u6210\u6570\u636e\u751f\u6210 \u5728\u89e3\u51b3\u4f4e\u6570\u636e\u91cf\u548c\u5fae\u8c03\u5927\u578b\u6a21\u578b\u65b9\u9762\u626e\u6f14\u7740\u8d8a\u6765\u8d8a\u91cd\u8981\u7684\u89d2\u8272\uff1b 3D\u89c6\u89c9\u4e0e\u91cd\u5efa \u6280\u672f\u6301\u7eed\u521b\u65b0\uff0c\u7279\u522b\u662f\u7ed3\u5408\u6269\u6563\u6a21\u578b\u5b9e\u73b0\u5355\u56fe3D\u91cd\u5efa\u548c\u9ad8\u6548\u59ff\u6001\u4f30\u8ba1\uff1b \u81ea\u52a8\u9a7e\u9a76\u4e0e\u673a\u5668\u4eba \u9886\u57df\u7684\u7814\u7a76\u4f9d\u7136\u6d3b\u8dc3\uff0c\u6db5\u76d6\u4e86\u6570\u636e\u96c6\u3001\u77e5\u8bc6\u95ee\u7b54\u548c\u9ad8\u7cbe\u5730\u56fe\uff1b\u6b64\u5916\uff0c\u5bf9 \u6a21\u578b\u6548\u7387\u3001\u6cdb\u5316\u80fd\u529b \u4ee5\u53ca \u901a\u7528AI\u4ee3\u7406 \u7684\u63a2\u7d22\u4e5f\u5f15\u4eba\u6ce8\u76ee\u3002 \u4e3b\u8981\u4e3b\u9898\u4e0e\u8d8b\u52bf\uff1a \u9ad8\u6548\u5408\u6210\u6570\u636e\u751f\u6210\uff1a \u591a\u7bc7\u8bba\u6587\u805a\u7126\u4e8e\u5229\u7528LoRA\u7b49\u6280\u672f\u9ad8\u6548\u751f\u6210\u9ad8\u8d28\u91cf\u5408\u6210\u6570\u636e\uff0c\u4ee5\u5e94\u5bf9\u7279\u5b9a\u9886\u57df\uff08\u5982\u5de5\u4e1aCAD\u6a21\u578b\u3001\u4f4e\u6570\u636e\u91cf\u573a\u666f\uff09\u7684\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u5e76\u652f\u6301\u5927\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08LVMs\uff09\u7684\u5fae\u8c03\u3002 3D\u89c6\u89c9\u4e0e\u6269\u6563\u6a21\u578b\u7684\u878d\u5408\uff1a \u6269\u6563\u6a21\u578b\u57283D\u91cd\u5efa\u548c\u59ff\u6001\u4f30\u8ba1\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u6f5c\u529b\uff0c\u5b9e\u73b0\u4e86\u4ece\u5355\u5f20\u56fe\u50cf\u751f\u6210\u5b8c\u6574\u9ad8\u65af\u6cfc\u6e85\uff08Gaussian Splats\uff09\u4ee5\u53ca\u9ad8\u6548\u76843D\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u3002 \u81ea\u52a8\u9a7e\u9a76\u4e0e\u673a\u5668\u4eba\u611f\u77e5\uff1a \u65b0\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\u53d1\u5e03\uff0c\u540c\u65f6\u7814\u7a76\u5173\u6ce8\u4e8e\u81ea\u52a8\u9a7e\u9a76\u77e5\u8bc6\u95ee\u7b54\uff08VQA\uff09\u548c\u9c81\u68d2\u7684\u5728\u7ebf\u9ad8\u7cbe\u5730\u56fe\u6784\u5efa\uff0c\u5f3a\u8c03\u4e86\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u611f\u77e5\u4e0e\u63a8\u7406\u80fd\u529b\u3002 \u6a21\u578b\u6548\u7387\u4e0e\u6cdb\u5316\uff1a \u6709\u7814\u7a76\u63a2\u7d22\u66ff\u4ee3\u4f20\u7edfU-Net\u67b6\u6784\u4ee5\u63d0\u9ad8\u5206\u5272\u6548\u7387\uff0c\u5e76\u6df1\u5165\u63a2\u8ba8\u4e86\u5728\u201c\u91ce\u5916\u201d\u573a\u666f\u4e0b\u5b9e\u73b0\u9886\u57df\u6cdb\u5316\u7684\u65b9\u6cd5\uff0c\u65e8\u5728\u63d0\u5347\u6a21\u578b\u5728\u672a\u77e5\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u3002 \u901a\u7528AI\u4ee3\u7406\u7684\u5d1b\u8d77\uff1a \u51fa\u73b0\u4e86\u65e8\u5728\u6784\u5efa\u57fa\u7840\u6027GUI\u4ee3\u7406\u7684\u5de5\u4f5c\uff0c\u7ed3\u5408\u5148\u8fdb\u7684\u611f\u77e5\u548c\u89c4\u5212\u80fd\u529b\uff0c\u9884\u793a\u7740\u901a\u7528\u578bAI\u5728\u4eba\u673a\u4ea4\u4e92\u9886\u57df\u7684\u5e94\u7528\u524d\u666f\u3002 \u7279\u522b\u91cd\u8981\u6216\u521b\u65b0\u7684\u8bba\u6587\uff1a \"Complete Gaussian Splats from a Single Image with Denoising Diffusion Models\" (Ziwei Liao et al.)\uff1a \u8fd9\u7bc7\u8bba\u6587\u6781\u5177\u521b\u65b0\u6027\uff0c\u5b83\u5229\u7528\u53bb\u566a\u6269\u6563\u6a21\u578b\u5b9e\u73b0\u4e86\u4ece\u5355\u5f20\u56fe\u50cf\u751f\u6210\u5b8c\u6574\u76843D\u9ad8\u65af\u6cfc\u6e85\uff0c\u8fd9\u5728\u5355\u76ee3D\u91cd\u5efa\u9886\u57df\u662f\u4e00\u4e2a\u91cd\u5927\u7a81\u7834\uff0c\u6709\u671b\u5927\u5e45\u7b80\u53163D\u5185\u5bb9\u521b\u5efa\u6d41\u7a0b\u3002 \"UItron: Foundational GUI Agent with Advanced Perception and Planning\" (Zhixiong Zeng et al.)\uff1a \u8fd9\u9879\u5de5\u4f5c\u4ee3\u8868\u4e86\u901a\u7528AI\u4ee3\u7406\u53d1\u5c55\u7684\u91cd\u8981\u4e00\u6b65\u3002\u6784\u5efa\u4e00\u4e2a\u80fd\u591f\u611f\u77e5\u548c\u89c4\u5212\u7684GUI\u4ee3\u7406\uff0c\u5bf9\u4e8e\u5b9e\u73b0\u66f4\u667a\u80fd\u3001\u66f4\u81ea\u4e3b\u7684\u4eba\u673a\u4ea4\u4e92\u5177\u6709\u6df1\u8fdc\u5f71\u54cd\u3002 \"Maybe you don't need a U-Net: convolutional feature upsampling for materials micrograph segmentation\" (Ronan Docherty et al.)\uff1a \u6311\u6218\u4e86U-Net\u8fd9\u4e00\u5728\u5206\u5272\u9886\u57df\u5e7f\u6cdb\u4f7f\u7528\u7684\u67b6\u6784\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u80fd\u66f4\u9ad8\u6548\u7684\u5377\u79ef\u7279\u5f81\u4e0a\u91c7\u6837\u65b9\u6cd5\uff0c\u5bf9\u4e8e\u8ffd\u6c42\u6a21\u578b\u6548\u7387\u7684\u7814\u7a76\u8005\u800c\u8a00\u5177\u6709\u91cd\u8981\u53c2\u8003\u4ef7\u503c\u3002 \"FLORA: Efficient Synthetic Data Generation for Object Detection in Low-Data Regimes via finetuning Flux LoRA\" (Alvaro Patricio et al.)\uff1a \u9488\u5bf9\u4f4e\u6570\u636e\u91cf\u573a\u666f\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u5408\u6210\u6570\u636e\u751f\u6210\u65b9\u6848\uff0c\u5176\u7ed3\u5408LoRA\u5fae\u8c03\u7684\u7b56\u7565\u5177\u6709\u5f88\u5f3a\u7684\u5b9e\u7528\u6027\u548c\u666e\u9002\u6027\u3002 \u65b0\u5174\u7814\u7a76\u65b9\u5411\u6216\u6280\u672f\uff1a \u6269\u6563\u6a21\u578b\u57283D\u751f\u6210\u4e0e\u91cd\u5efa\u4e2d\u7684\u6df1\u5ea6\u5e94\u7528\uff1a \u4e0d\u4ec5\u9650\u4e8e\u56fe\u50cf\u751f\u6210\uff0c\u6269\u6563\u6a21\u578b\u6b63\u6210\u4e3a3D\u5185\u5bb9\u521b\u5efa\uff08\u5982\u9ad8\u65af\u6cfc\u6e85\u3001\u59ff\u6001\u4f30\u8ba1\uff09\u7684\u5173\u952e\u6280\u672f\u3002 \u9762\u5411\u7279\u5b9a\u4efb\u52a1\u548c\u5927\u578b\u6a21\u578b\u7684\u5408\u6210\u6570\u636e\u751f\u6210\uff1a \u7ed3\u5408LoRA\u7b49\u9ad8\u6548\u5fae\u8c03\u6280\u672f\uff0c\u5408\u6210\u6570\u636e\u4e0d\u518d\u4ec5\u4ec5\u662f\u6570\u636e\u589e\u5f3a\uff0c\u800c\u662f\u6210\u4e3a\u5b9a\u5236\u5316\u8bad\u7ec3\u5927\u578b\u6a21\u578b\u3001\u89e3\u51b3\u7279\u5b9a\u9886\u57df\u6570\u636e\u74f6\u9888\u7684\u6838\u5fc3\u7b56\u7565\u3002 \u901a\u7528\u578b\u611f\u77e5-\u89c4\u5212AI\u4ee3\u7406\uff1a \u65e8\u5728\u6784\u5efa\u80fd\u591f\u7406\u89e3\u548c\u64cd\u4f5c\u590d\u6742\u73af\u5883\uff08\u5982GUI\u754c\u9762\uff09\u7684\u901a\u7528\u4ee3\u7406\uff0c\u662f\u8fc8\u5411\u66f4\u9ad8\u7ea7AI\u7684\u91cd\u8981\u4e00\u6b65\u3002 \u9c81\u68d2\u6027\u4e0e\u9886\u57df\u6cdb\u5316\u7684\u65b0\u8303\u5f0f\uff1a \u901a\u8fc7\u89e3\u8026\u8868\u793a\u5b66\u4e60\u548c\u6982\u7387\u5efa\u6a21\u7b49\u65b9\u6cd5\uff0c\u63d0\u5347\u6a21\u578b\u5728\u590d\u6742\u3001\u672a\u77e5\u771f\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u7684\u53ef\u9760\u6027\u3002 \u5efa\u8bae\u9605\u8bfb\u5168\u6587\u7684\u8bba\u6587\uff1a \"Complete Gaussian Splats from a Single Image with Denoising Diffusion Models\" (Ziwei Liao et al.)\uff1a \u5982\u679c\u60a8\u5bf93D\u89c6\u89c9\u3001\u65b0\u9896\u76843D\u91cd\u5efa\u65b9\u6cd5\u6216\u6269\u6563\u6a21\u578b\u611f\u5174\u8da3\uff0c\u8fd9\u7bc7\u8bba\u6587\u63d0\u4f9b\u4e86\u4ee4\u4eba\u5174\u594b\u7684\u8fdb\u5c55\u3002 \"UItron: Foundational GUI Agent with Advanced Perception and Planning\" (Zhixiong Zeng et al.)\uff1a \u5bf9\u4e8e\u5173\u6ce8\u901a\u7528AI\u3001AI\u4ee3\u7406\u6216\u4eba\u673a\u4ea4\u4e92\u7684\u7814\u7a76\u4eba\u5458\uff0c\u8fd9\u7bc7\u8bba\u6587\u5c55\u793a\u4e86\u672a\u6765AI\u5e94\u7528\u7684\u4e00\u4e2a\u91cd\u8981\u65b9\u5411\u3002 \"FLORA: Efficient Synthetic Data Generation for Object Detection in Low-Data Regimes via finetuning Flux LoRA\" (Alvaro Patricio et al.)\uff1a \u5bf9\u4e8e\u9762\u4e34\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u6216\u5e0c\u671b\u9ad8\u6548\u5229\u7528\u5408\u6210\u6570\u636e\u8fdb\u884c\u76ee\u6807\u68c0\u6d4b\u7684\u7814\u7a76\u8005\uff0c\u8fd9\u7bc7\u8bba\u6587\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002 \"Maybe you don't need a U-Net: convolutional feature upsampling for materials micrograph segmentation\" (Ronan Docherty et al.)\uff1a \u5982\u679c\u60a8\u5728\u5206\u5272\u4efb\u52a1\u4e2d\u5bfb\u6c42\u66f4\u9ad8\u6548\u7684\u6a21\u578b\u67b6\u6784\uff0c\u6216\u8005\u5bf9U-Net\u7684\u66ff\u4ee3\u65b9\u6848\u611f\u5174\u8da3\uff0c\u8fd9\u7bc7\u8bba\u6587\u503c\u5f97\u6df1\u5165\u7814\u7a76\u3002 \"DriveQA: Passing the Driving Knowledge Test\" (Maolin Wei et al.)\uff1a \u5bf9\u4e8e\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u7684VQA\u548c\u77e5\u8bc6\u63a8\u7406\u611f\u5174\u8da3\u7684\u7814\u7a76\u4eba\u5458\uff0c\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u57fa\u51c6\u548c\u6311\u6218\u3002 Table of Contents Maybe you don't need a U-Net: convolutional feature upsampling for materials micrograph segmentation FLORA: Efficient Synthetic Data Generation for Object Detection in Low-Data Regimes via finetuning Flux LoRA The Rosario Dataset v2: Multimodal Dataset for Agricultural Robotics Complete Gaussian Splats from a Single Image with Denoising Diffusion Models Efficient Diffusion-Based 3D Human Pose Estimation with Hierarchical Temporal Pruning DriveQA: Passing the Driving Knowledge Test Domain Generalization in-the-Wild: Disentangling Classification from Domain-Aware Representations UItron: Foundational GUI Agent with Advanced Perception and Planning CAD2DMD-SET: Synthetic Generation Tool of Digital Measurement Device CAD Model Datasets for fine-tuning Large Vision-Language Models Mapping like a Skeptic: Probabilistic BEV Projection for Online HD Mapping Papers Maybe you don't need a U-Net: convolutional feature upsampling for materials micrograph segmentation Authors: Ronan Docherty, Antonis Vamvakeros, Samuel J. Cooper Published: 2025-08-29 Categories: cs.CV, cond-mat.mtrl-sci Abstract: Feature foundation models - usually vision transformers - offer rich semantic descriptors of images, useful for downstream tasks such as (interactive) segmentation and object detection. For computational efficiency these descriptors are often patch-based, and so struggle to represent the fine features often present in micrographs; they also struggle with the large image sizes present in materials and biological image analysis. In this work, we train a convolutional neural network to upsample low-resolution (i.e, large patch size) foundation model features with reference to the input image. We apply this upsampler network (without any further training) to efficiently featurise and then segment a variety of microscopy images, including plant cells, a lithium-ion battery cathode and organic crystals. The richness of these upsampled features admits separation of hard to segment phases, like hairline cracks. We demonstrate that interactive segmentation with these deep features produces high-quality segmentations far faster and with far fewer labels than training or finetuning a more traditional convolutional network. Analysis: \u8fd9\u7bc7\u8bba\u6587\u7684\u6458\u8981\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u65e8\u5728\u89e3\u51b3\u5f53\u524d\u7279\u5f81\u57fa\u7840\u6a21\u578b\uff08\u901a\u5e38\u662fVision Transformer\uff09\u5728\u5904\u7406\u663e\u5fae\u56fe\u50cf\u65f6\u9762\u4e34\u7684\u6311\u6218\uff0c\u5373\u96be\u4ee5\u6355\u6349\u7cbe\u7ec6\u7279\u5f81\u548c\u5904\u7406\u5927\u5c3a\u5bf8\u56fe\u50cf\u3002\u4ee5\u4e0b\u662f\u8be6\u7ec6\u5206\u6790\uff1a 1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u660e\u6458\u8981 (Concise Summary) \u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u4e0a\u91c7\u6837\u5668\uff0c\u7528\u4e8e\u5c06\u4f4e\u5206\u8fa8\u7387\u7684\u57fa\u7840\u6a21\u578b\u7279\u5f81\u9ad8\u6548\u5730\u63d0\u5347\u81f3\u9ad8\u5206\u8fa8\u7387\uff0c\u5e76\u7ed3\u5408\u539f\u59cb\u8f93\u5165\u56fe\u50cf\u4fe1\u606f\u3002\u8be5\u4e0a\u91c7\u6837\u5668\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u5e94\u7528\u4e8e\u591a\u79cd\u663e\u5fae\u56fe\u50cf\uff0c\u5b9e\u73b0\u4e86\u5bf9\u590d\u6742\u6750\u6599\u663e\u5fae\u56fe\u50cf\u7684\u9ad8\u8d28\u91cf\u3001\u4ea4\u4e92\u5f0f\u5206\u5272\uff0c\u4e14\u6240\u9700\u6807\u6ce8\u8fdc\u5c11\u4e8e\u4f20\u7edf\u5377\u79ef\u7f51\u7edc\u3002 2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u5b66\u65b9\u6cd5 (Key Innovation or Methodological Approach) \u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u5176\u63d0\u51fa\u7684 \u201c\u5377\u79ef\u7279\u5f81\u4e0a\u91c7\u6837\u5668\u201d\u7f51\u7edc \u3002\u8be5\u7f51\u7edc\u662f\u4e00\u4e2a\u72ec\u7acb\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff0c\u4e13\u95e8\u7528\u4e8e\uff1a 1. \u63a5\u6536\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\uff08\u5982Vision Transformer\uff09\u751f\u6210\u7684 \u4f4e\u5206\u8fa8\u7387\u3001\u7c97\u7c92\u5ea6\u7279\u5f81\u56fe \u3002 2. \u7ed3\u5408\u539f\u59cb\u8f93\u5165\u56fe\u50cf\u7684\u7cbe\u7ec6\u7eb9\u7406\u4fe1\u606f \u4f5c\u4e3a\u53c2\u8003\u3002 3. \u5c06\u8fd9\u4e9b\u7279\u5f81\u4e0a\u91c7\u6837\u4e3a \u9ad8\u5206\u8fa8\u7387\u3001\u8bed\u4e49\u4e30\u5bcc\u7684\u7279\u5f81\u8868\u793a \u3002 \u5173\u952e\u4e4b\u5904\u5728\u4e8e\uff0c\u8fd9\u4e2a\u4e0a\u91c7\u6837\u5668\u4e00\u65e6\u8bad\u7ec3\u5b8c\u6210\uff0c\u4fbf\u53ef\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u5fae\u8c03\uff0c\u76f4\u63a5\u5e94\u7528\u4e8e\u591a\u79cd\u4e0d\u540c\u7684\u663e\u5fae\u56fe\u50cf\u5206\u5272\u4efb\u52a1\uff08\u4f8b\u5982\u690d\u7269\u7ec6\u80de\u3001\u9502\u79bb\u5b50\u7535\u6c60\u9634\u6781\u3001\u6709\u673a\u6676\u4f53\uff09\uff0c\u4ece\u800c\u9ad8\u6548\u5730\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u6df1\u5ea6\u7279\u5f81\uff0c\u8fdb\u800c\u652f\u6301\u4ea4\u4e92\u5f0f\u5206\u5272\u3002\u8fd9\u4e0eU-Net\u7b49\u7aef\u5230\u7aef\u5206\u5272\u7f51\u7edc\u6216\u76f4\u63a5\u5fae\u8c03\u57fa\u7840\u6a21\u578b\u7684\u8303\u5f0f\u4e0d\u540c\uff0c\u5b83\u5c06\u7279\u5f81\u63d0\u53d6\u548c\u7cbe\u7ec6\u5316\u89e3\u8026\uff0c\u4e13\u6ce8\u4e8e\u63d0\u5347\u57fa\u7840\u6a21\u578b\u7684\u7a7a\u95f4\u5206\u8fa8\u7387\u3002 3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd (Potential Impact on the Field) \u8be5\u7814\u7a76\u5bf9\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\uff0c\u7279\u522b\u662f\u79d1\u5b66\u56fe\u50cf\u5206\u6790\uff08\u5982\u6750\u6599\u79d1\u5b66\u3001\u751f\u7269\u533b\u5b66\uff09\u5177\u6709\u663e\u8457\u6f5c\u5728\u5f71\u54cd\uff1a * \u63d0\u5347\u57fa\u7840\u6a21\u578b\u5728\u5bc6\u96c6\u9884\u6d4b\u4efb\u52a1\u4e2d\u7684\u5b9e\u7528\u6027\uff1a \u5b83\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7b56\u7565\uff0c\u80fd\u591f\u5229\u7528\u73b0\u6709\u57fa\u7840\u6a21\u578b\u7684\u5f3a\u5927\u8bed\u4e49\u7406\u89e3\u80fd\u529b\uff0c\u540c\u65f6\u514b\u670d\u5176\u5728\u7a7a\u95f4\u5206\u8fa8\u7387\u4e0a\u7684\u56fa\u6709\u4e0d\u8db3\uff0c\u4f7f\u5176\u66f4\u9002\u7528\u4e8e\u9700\u8981\u50cf\u7d20\u7ea7\u7cbe\u5ea6\u7684\u4efb\u52a1\u3002 * \u52a0\u901f\u663e\u5fae\u56fe\u50cf\u5206\u6790\uff1a \u901a\u8fc7\u9ad8\u6548\u751f\u6210\u9ad8\u8d28\u91cf\u7279\u5f81\uff0c\u6709\u671b\u663e\u8457\u52a0\u901f\u663e\u5fae\u56fe\u50cf\u7684\u81ea\u52a8\u5316\u5206\u6790\u548c\u89e3\u91ca\u3002 * \u964d\u4f4e\u6807\u6ce8\u6210\u672c\uff0c\u63d0\u9ad8\u4ea4\u4e92\u5f0f\u5206\u5272\u6548\u7387\uff1a \u5f3a\u8c03\u4e86\u5728\u4ea4\u4e92\u5f0f\u5206\u5272\u4e2d\uff0c\u8be5\u65b9\u6cd5\u80fd\u4ee5\u66f4\u5feb\u7684\u901f\u5ea6\u548c\u66f4\u5c11\u7684\u6807\u7b7e\u5b9e\u73b0\u9ad8\u8d28\u91cf\u5206\u5272\uff0c\u8fd9\u5bf9\u4e8e\u6807\u6ce8\u6210\u672c\u9ad8\u6602\u7684\u4e13\u4e1a\u9886\u57df\uff08\u5982\u533b\u5b66\u3001\u6750\u6599\uff09\u5177\u6709\u5de8\u5927\u4ef7\u503c\u3002 * \u65b0\u7684\u6a21\u578b\u8303\u5f0f\uff1a \u201c\u4e00\u6b21\u8bad\u7ec3\uff0c\u591a\u4efb\u52a1\u5e94\u7528\u201d\u7684\u4e0a\u91c7\u6837\u5668\u8303\u5f0f\uff0c\u53ef\u80fd\u4e3a\u672a\u6765\u57fa\u7840\u6a21\u578b\u5728\u7279\u5b9a\u9886\u57df\uff08\u5c24\u5176\u662f\u79d1\u5b66\u56fe\u50cf\uff09\u7684\u90e8\u7f72\u548c\u5e94\u7528\u5f00\u8f9f\u65b0\u9014\u5f84\uff0c\u5373\u901a\u8fc7\u4e00\u4e2a\u901a\u7528\u7684\u7279\u5f81\u7cbe\u7ec6\u5316\u6a21\u5757\u6765\u9002\u5e94\u4e0d\u540c\u4efb\u52a1\u3002 4. \u53ef\u80fd\u53d7\u76ca\u7684\u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 (Related Areas or Applications) \u6750\u6599\u79d1\u5b66\u4e0e\u5de5\u7a0b\uff1a \u663e\u5fae\u7ed3\u6784\u5206\u6790\u3001\u7f3a\u9677\u68c0\u6d4b\uff08\u5982\u53d1\u4e1d\u88c2\u7eb9\uff09\u3001\u76f8\u5206\u79bb\u8bc6\u522b\u3001\u6676\u7c92\u8fb9\u754c\u5206\u5272\u3002 \u751f\u7269\u533b\u5b66\u56fe\u50cf\u5206\u6790\uff1a \u7ec6\u80de\u5206\u5272\u3001\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\u5206\u6790\u3001\u795e\u7ecf\u5143\u8ffd\u8e2a\u3001\u7535\u5b50\u663e\u5fae\u955c\u56fe\u50cf\u5904\u7406\u3001\u9ad8\u5206\u8fa8\u7387\u8367\u5149\u663e\u5fae\u56fe\u50cf\u5206\u6790\u3002 \u5de5\u4e1a\u68c0\u6d4b\uff1a \u8868\u9762\u7f3a\u9677\u68c0\u6d4b\u3001\u8d28\u91cf\u63a7\u5236\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u8bc6\u522b\u5fae\u5c0f\u7455\u75b5\u7684\u573a\u666f\u3002 \u9065\u611f\u56fe\u50cf\u5904\u7406\uff1a \u9ad8\u5206\u8fa8\u7387\u536b\u661f\u56fe\u50cf\u4e2d\u7684\u5730\u7269\u5206\u5272\u4e0e\u8bc6\u522b\uff0c\u7279\u522b\u662f\u5bf9\u7cbe\u7ec6\u5730\u7269\uff08\u5982\u9053\u8def\u3001\u5efa\u7b51\u7269\u8fb9\u7f18\uff09\u7684\u63d0\u53d6\u3002 \u4efb\u4f55\u9700\u8981\u7cbe\u7ec6\u50cf\u7d20\u7ea7\u7406\u89e3\u4e14\u56fe\u50cf\u5c3a\u5bf8\u8f83\u5927\u3001\u7279\u5f81\u7ec6\u8282\u4e30\u5bcc\u7684\u9886\u57df\u3002 5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u7684\u5c40\u9650\u6027 (Limitations that can be inferred from the abstract) \u5bf9\u57fa\u7840\u6a21\u578b\u7279\u5f81\u8d28\u91cf\u7684\u4f9d\u8d56\uff1a \u8be5\u65b9\u6cd5\u7684\u6027\u80fd\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u53d6\u51b3\u4e8e\u6240\u4f7f\u7528\u7684\u57fa\u7840\u6a21\u578b\u751f\u6210\u7279\u5f81\u7684\u8d28\u91cf\u3002\u5982\u679c\u57fa\u7840\u6a21\u578b\u672a\u80fd\u6355\u6349\u5230\u67d0\u4e9b\u5173\u952e\u8bed\u4e49\u4fe1\u606f\uff0c\u4e0a\u91c7\u6837\u5668\u4e5f\u96be\u4ee5\u51ed\u7a7a\u521b\u9020\u3002 \u201c\u65e0\u9700\u8fdb\u4e00\u6b65\u8bad\u7ec3\u201d\u7684\u6743\u8861\uff1a \u5c3d\u7ba1\u4e0a\u91c7\u6837\u5668\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u5e94\u7528\u4e8e\u65b0\u4efb\u52a1\u662f\u5176\u4f18\u52bf\uff0c\u4f46\u8fd9\u53ef\u80fd\u610f\u5473\u7740\u5728\u7279\u5b9a\u6570\u636e\u96c6\u4e0a\uff0c\u5176\u6027\u80fd\u53ef\u80fd\u65e0\u6cd5\u8d85\u8d8a\u7ecf\u8fc7\u5145\u5206\u5fae\u8c03\u7684\u7aef\u5230\u7aef\u7f51\u7edc\uff08\u5982U-Net\uff09\uff0c\u5c24\u5176\u662f\u5728\u5168\u81ea\u52a8\u5206\u5272\u573a\u666f\u4e0b\u3002\u6458\u8981\u4e2d\u5f3a\u8c03\u7684\u662f\u4ea4\u4e92\u5f0f\u5206\u5272\u7684\u6548\u7387\u63d0\u5347\u3002 \u4e0eU-Net\u7684\u76f4\u63a5\u6027\u80fd\u5bf9\u6bd4\u672a\u660e\uff1a \u6807\u9898\u6697\u793a\u4e86\u5bf9U-Net\u7684\u66ff\u4ee3\uff0c\u4f46\u6458\u8981\u4e3b\u8981\u5f3a\u8c03\u5728\u4ea4\u4e92\u5f0f\u5206\u5272\u4e2d\u201c\u66f4\u5feb\u3001\u66f4\u5c11\u6807\u7b7e\u201d\u7684\u4f18\u52bf\uff0c\u5e76\u672a\u76f4\u63a5\u7ed9\u51fa\u5728\u5168\u81ea\u52a8\u3001\u975e\u4ea4\u4e92\u5f0f\u573a\u666f\u4e0b\u4e0eU-Net\u7b49\u4f20\u7edf\u65b9\u6cd5\u7684\u91cf\u5316\u6027\u80fd\u5bf9\u6bd4\u3002 \u8ba1\u7b97\u6210\u672c\u7684\u6f5c\u5728\u8003\u91cf\uff1a \u867d\u7136\u5f3a\u8c03\u4e86\u201c\u8ba1\u7b97\u6548\u7387\u201d\uff0c\u4f46\u7ed3\u5408\u57fa\u7840\u6a21\u578b\u548c\u72ec\u7acb\u7684\u4e0a\u91c7\u6837\u5668\u7f51\u7edc\uff0c\u5176\u603b\u4f53\u7684\u63a8\u7406\u65f6\u95f4\u6216\u5185\u5b58\u5360\u7528\u4e0e\u4e00\u4e2a\u4f18\u5316\u826f\u597d\u7684U-Net\u76f8\u6bd4\u5982\u4f55\uff0c\u4ecd\u9700\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u3002 \u5bf9\u8f93\u5165\u56fe\u50cf\u8d28\u91cf\u7684\u654f\u611f\u6027\uff1a \u4e0a\u91c7\u6837\u5668\u201c\u53c2\u8003\u8f93\u5165\u56fe\u50cf\u201d\u8fdb\u884c\u7cbe\u7ec6\u5316\uff0c\u8fd9\u610f\u5473\u7740\u5982\u679c\u8f93\u5165\u56fe\u50cf\u672c\u8eab\u5b58\u5728\u566a\u58f0\u6216\u4f2a\u5f71\uff0c\u53ef\u80fd\u4f1a\u5f71\u54cd\u4e0a\u91c7\u6837\u548c\u5206\u5272\u7684\u51c6\u786e\u6027\u3002 Key Findings: We demonstrate that interactive segmentation with these deep features produces high-quality segmentations far faster and with far fewer labels than training or finetuning a more traditional convolutional network. Links: PDF arXiv FLORA: Efficient Synthetic Data Generation for Object Detection in Low-Data Regimes via finetuning Flux LoRA Authors: Alvaro Patricio, Atabak Dehban, Rodrigo Ventura Published: 2025-08-29 Categories: cs.CV Abstract: Recent advances in diffusion-based generative models have demonstrated significant potential in augmenting scarce datasets for object detection tasks. Nevertheless, most recent models rely on resource-intensive full fine-tuning of large-scale diffusion models, requiring enterprise-grade GPUs (e.g., NVIDIA V100) and thousands of synthetic images. To address these limitations, we propose Flux LoRA Augmentation (FLORA), a lightweight synthetic data generation pipeline. Our approach uses the Flux 1.1 Dev diffusion model, fine-tuned exclusively through Low-Rank Adaptation (LoRA). This dramatically reduces computational requirements, enabling synthetic dataset generation with a consumer-grade GPU (e.g., NVIDIA RTX 4090). We empirically evaluate our approach on seven diverse object detection datasets. Our results demonstrate that training object detectors with just 500 synthetic images generated by our approach yields superior detection performance compared to models trained on 5000 synthetic images from the ODGEN baseline, achieving improvements of up to 21.3% in mAP@.50:.95. This work demonstrates that it is possible to surpass state-of-the-art performance with far greater efficiency, as FLORA achieves superior results using only 10% of the data and a fraction of the computational cost. This work demonstrates that a quality and efficiency-focused approach is more effective than brute-force generation, making advanced synthetic data creation more practical and accessible for real-world scenarios. Analysis: \u4f5c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u4e13\u5bb6\uff0c\u6211\u5bf9\u8fd9\u7bc7\u8bba\u6587\u6458\u8981\u7684\u5206\u6790\u5982\u4e0b\uff1a FLORA: Efficient Synthetic Data Generation for Object Detection in Low-Data Regimes via finetuning Flux LoRA 1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e (Concise Summary) FLORA\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u9ad8\u6548\u7684\u5408\u6210\u6570\u636e\u751f\u6210\u7ba1\u7ebf\uff0c\u4e13\u4e3a\u4f4e\u6570\u636e\u91cf\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u8bbe\u8ba1\u3002\u5b83\u901a\u8fc7\u5bf9Flux 1.1 Dev\u6269\u6563\u6a21\u578b\u8fdb\u884cLoRA\u5fae\u8c03\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\uff0c\u5e76\u8bc1\u660e\u4ec5\u7528\u5c11\u91cf\uff08500\u5f20\uff09\u9ad8\u8d28\u91cf\u5408\u6210\u56fe\u50cf\u5373\u53ef\u8d85\u8d8a\u73b0\u6709\u57fa\u7ebf\uff08ODGEN\uff09\u4f7f\u7528\u5927\u91cf\uff085000\u5f20\uff09\u56fe\u50cf\u7684\u6027\u80fd\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u68c0\u6d4b\u7cbe\u5ea6\u548c\u6548\u7387\u3002\u8fd9\u9879\u5de5\u4f5c\u5f3a\u8c03\u4e86\u8d28\u91cf\u548c\u6548\u7387\u4f18\u5148\u7684\u5408\u6210\u6570\u636e\u751f\u6210\u7b56\u7565\u4f18\u4e8e\u86ee\u529b\u751f\u6210\u3002 2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5 (Key Innovation or Methodological Approach) \u8be5\u8bba\u6587\u7684\u5173\u952e\u521b\u65b0\u5728\u4e8e\uff1a LoRA\u5fae\u8c03\u6269\u6563\u6a21\u578b\u4ee5\u5b9e\u73b0\u9ad8\u6548\u6027\uff1a \u6838\u5fc3\u521b\u65b0\u662f\u5c06\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\u6280\u672f\u5e94\u7528\u4e8eFlux 1.1 Dev\u6269\u6563\u6a21\u578b\u7684\u5fae\u8c03\uff0c\u4ee5\u751f\u6210\u76ee\u6807\u68c0\u6d4b\u6240\u9700\u7684\u5408\u6210\u6570\u636e\u3002\u76f8\u8f83\u4e8e\u4f20\u7edf\u9700\u8981\u5168\u91cf\u5fae\u8c03\u5927\u578b\u6269\u6563\u6a21\u578b\u7684\u65b9\u6cd5\uff0cLoRA\u4ec5\u66f4\u65b0\u5c11\u91cf\u53c2\u6570\uff0c\u6781\u5927\u5730\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\uff0c\u4f7f\u5f97\u5728\u6d88\u8d39\u7ea7GPU\uff08\u5982NVIDIA RTX 4090\uff09\u4e0a\u8fdb\u884c\u9ad8\u6548\u7684\u5408\u6210\u6570\u636e\u751f\u6210\u6210\u4e3a\u53ef\u80fd\u3002 \u201c\u8d28\u91cf\u4f18\u5148\u4e8e\u6570\u91cf\u201d\u7684\u5408\u6210\u6570\u636e\u7b56\u7565\uff1a \u8bba\u6587\u660e\u786e\u63d0\u51fa\u5e76\u9a8c\u8bc1\u4e86\u201c\u8d28\u91cf\u548c\u6548\u7387\u4f18\u5148\u201d\u7684\u7b56\u7565\u4f18\u4e8e\u201c\u86ee\u529b\u751f\u6210\u201d\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4ec5\u7528500\u5f20FLORA\u751f\u6210\u7684\u5408\u6210\u56fe\u50cf\uff0c\u5c31\u80fd\u5728mAP@.50:.95\u4e0a\u53d6\u5f97\u9ad8\u8fbe21.3%\u7684\u63d0\u5347\uff0c\u5e76\u8d85\u8d8a\u4f7f\u75285000\u5f20ODGEN\u57fa\u7ebf\u56fe\u50cf\u8bad\u7ec3\u7684\u6a21\u578b\u3002\u8fd9\u8868\u660eFLORA\u80fd\u591f\u751f\u6210\u66f4\u9ad8\u8d28\u91cf\u3001\u5bf9\u68c0\u6d4b\u5668\u8bad\u7ec3\u66f4\u6709\u6548\u7684\u5408\u6210\u6570\u636e\uff0c\u800c\u975e\u7b80\u5355\u5730\u8ffd\u6c42\u6570\u91cf\u3002 3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd (Potential Impact on the Field) \u5408\u6210\u6570\u636e\u751f\u6210\u7684\u6c11\u4e3b\u5316\uff1a \u6781\u5927\u5730\u964d\u4f4e\u4e86\u5408\u6210\u6570\u636e\u751f\u6210\u7684\u6280\u672f\u95e8\u69db\u548c\u8d44\u6e90\u9700\u6c42\uff0c\u4f7f\u66f4\u591a\u7814\u7a76\u8005\u548c\u5c0f\u578b\u56e2\u961f\u80fd\u591f\u5728\u6d88\u8d39\u7ea7\u786c\u4ef6\u4e0a\u5229\u7528\u5148\u8fdb\u7684\u6269\u6563\u6a21\u578b\u8fdb\u884c\u6570\u636e\u589e\u5f3a\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u6c11\u4e3b\u5316\u3002 \u6539\u53d8\u5408\u6210\u6570\u636e\u7814\u7a76\u8303\u5f0f\uff1a \u6311\u6218\u4e86\u4f20\u7edf\u4e0a\u8ba4\u4e3a\u5408\u6210\u6570\u636e\u91cf\u8d8a\u5927\u8d8a\u597d\u7684\u89c2\u5ff5\uff0c\u5f3a\u8c03\u4e86\u6570\u636e\u8d28\u91cf\u548c\u751f\u6210\u6548\u7387\u7684\u91cd\u8981\u6027\uff0c\u53ef\u80fd\u5f15\u5bfc\u672a\u6765\u5408\u6210\u6570\u636e\u7814\u7a76\u8f6c\u5411\u66f4\u7cbe\u7ec6\u3001\u66f4\u9ad8\u6548\u7684\u751f\u6210\u7b56\u7565\u3002 \u52a0\u901f\u4f4e\u6570\u636e\u91cf\u573a\u666f\u4e0b\u7684AI\u5e94\u7528\uff1a \u5728\u6570\u636e\u7a00\u7f3a\u573a\u666f\u4e0b\uff0cFLORA\u80fd\u591f\u4ee5\u66f4\u4f4e\u7684\u6210\u672c\u3001\u66f4\u5feb\u7684\u901f\u5ea6\u63d0\u4f9b\u9ad8\u8d28\u91cf\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u52a0\u901f\u4e86\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u5728\u7279\u5b9a\u9886\u57df\uff08\u5982\u533b\u7597\u3001\u5de5\u4e1a\u68c0\u6d4b\u3001\u5c0f\u4f17\u7269\u4f53\u8bc6\u522b\uff09\u7684\u5f00\u53d1\u548c\u90e8\u7f72\u3002 \u63a8\u52a8LoRA\u5728\u751f\u6210\u6a21\u578b\u4e2d\u7684\u5e94\u7528\uff1a \u8fdb\u4e00\u6b65\u8bc1\u660e\u4e86LoRA\u4f5c\u4e3a\u4e00\u79cd\u9ad8\u6548\u5fae\u8c03\u7b56\u7565\u5728\u5927\u578b\u751f\u6210\u6a21\u578b\uff08\u7279\u522b\u662f\u6269\u6563\u6a21\u578b\uff09\u4e2d\u7684\u5de8\u5927\u6f5c\u529b\uff0c\u53ef\u80fd\u542f\u53d1\u66f4\u591a\u5c06LoRA\u5e94\u7528\u4e8e\u5176\u4ed6\u590d\u6742\u751f\u6210\u4efb\u52a1\u7684\u7814\u7a76\u3002 4. \u76f8\u5173\u53d7\u76ca\u9886\u57df\u6216\u5e94\u7528 (Related Areas or Applications that Might Benefit) \u6240\u6709\u9762\u4e34\u6570\u636e\u7a00\u7f3a\u6311\u6218\u7684\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\uff1a \u4f8b\u5982\uff1a\u533b\u7597\u5f71\u50cf\u5206\u6790\uff08\u7f55\u89c1\u75be\u75c5\u68c0\u6d4b\uff09\u3001\u5de5\u4e1a\u7f3a\u9677\u68c0\u6d4b\uff08\u7279\u5b9a\u4ea7\u54c1\u7f3a\u9677\uff09\u3001\u519c\u4e1a\uff08\u7279\u5b9a\u4f5c\u7269\u75c5\u866b\u5bb3\uff09\u3001\u81ea\u52a8\u9a7e\u9a76\uff08\u957f\u5c3e\u4e8b\u4ef6\u6216\u7f55\u89c1\u7269\u4f53\uff09\u3001\u673a\u5668\u4eba\u89c6\u89c9\u7b49\u3002 \u8d44\u6e90\u53d7\u9650\u7684\u7814\u7a76\u673a\u6784\u6216\u4f01\u4e1a\uff1a \u65e0\u6cd5\u627f\u62c5\u6602\u8d35\u7684\u4f01\u4e1a\u7ea7GPU\u548c\u5927\u89c4\u6a21\u6570\u636e\u751f\u6210\u6210\u672c\u7684\u573a\u666f\u3002 \u5feb\u901f\u539f\u578b\u5f00\u53d1\u548c\u8fed\u4ee3\uff1a \u9700\u8981\u5feb\u901f\u751f\u6210\u7279\u5b9a\u573a\u666f\u6570\u636e\u4ee5\u9a8c\u8bc1\u6a21\u578b\u6216\u7b97\u6cd5\u7684\u573a\u666f\u3002 \u5176\u4ed6\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\uff1a \u8be5\u65b9\u6cd5\u7684\u6838\u5fc3\u601d\u60f3\uff08LoRA\u5fae\u8c03\u6269\u6563\u6a21\u578b\u4ee5\u9ad8\u6548\u751f\u6210\u9ad8\u8d28\u91cf\u6570\u636e\uff09\u4e5f\u53ef\u80fd\u63a8\u5e7f\u5230\u5176\u4ed6\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\uff0c\u5982\u8bed\u4e49\u5206\u5272\u3001\u5b9e\u4f8b\u5206\u5272\uff0c\u751a\u81f3\u56fe\u50cf\u751f\u6210\u4e0e\u7f16\u8f91\u7b49\uff0c\u53ea\u8981\u8fd9\u4e9b\u4efb\u52a1\u80fd\u4ece\u9ad8\u6548\u3001\u9ad8\u8d28\u91cf\u7684\u5408\u6210\u6570\u636e\u4e2d\u53d7\u76ca\u3002 5. \u6f5c\u5728\u5c40\u9650\u6027 (Limitations that Can Be Inferred from the Abstract) \u201c\u4f4e\u6570\u636e\u91cf\u573a\u666f\u201d\u7684\u5177\u4f53\u5b9a\u4e49\uff1a \u6458\u8981\u4e2d\u63d0\u5230\u7684\u201c\u4f4e\u6570\u636e\u91cf\u573a\u666f\u201d\u7684\u5177\u4f53\u5b9a\u4e49\u548c\u8303\u56f4\u5c1a\u4e0d\u660e\u786e\u3002\u4f8b\u5982\uff0c500\u5f20\u5408\u6210\u56fe\u50cf\u662f\u5426\u8db3\u4ee5\u5e94\u5bf9\u6240\u6709\u6781\u4f4e\u6570\u636e\u91cf\uff08\u5982\u53ea\u6709\u51e0\u5341\u5f20\u771f\u5b9e\u56fe\u50cf\uff09\u7684\u573a\u666f\uff1f \u57fa\u7ebf\u5bf9\u6bd4\u7684\u5168\u9762\u6027\uff1a \u867d\u7136\u4e0eODGEN\u57fa\u7ebf\u8fdb\u884c\u4e86\u5bf9\u6bd4\uff0c\u4f46ODGEN\u662f\u5426\u4ee3\u8868\u4e86\u5f53\u524d\u5408\u6210\u6570\u636e\u751f\u6210\u9886\u57df\u7684\u6700\u65b0SOTA\u65b9\u6cd5\uff1f\u662f\u5426\u6709\u5176\u4ed6\u66f4\u5148\u8fdb\u7684\u3001\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u5168\u91cf\u5fae\u8c03\u65b9\u6cd5\u672a\u88ab\u63d0\u53ca\u6216\u5bf9\u6bd4\uff1f\u8fd9\u4f1a\u5f71\u54cd\u5bf9\u201c\u8d85\u8d8aSOTA\u6027\u80fd\u201d\u8fd9\u4e00\u8bf4\u6cd5\u7684\u5224\u65ad\u3002 Flux\u6a21\u578b\u4f9d\u8d56\u6027\uff1a FLORA\u4f9d\u8d56\u4e8eFlux 1.1 Dev\u6a21\u578b\u3002\u8be5\u65b9\u6cd5\u5728\u5176\u4ed6\u6269\u6563\u6a21\u578b\uff08\u5982Stable Diffusion\u3001DALL-E\u7b49\uff09\u4e0a\u7684\u8868\u73b0\u5982\u4f55\uff1fLoRA\u5fae\u8c03\u7684\u6709\u6548\u6027\u662f\u5426\u4e0e\u57fa\u7840\u6269\u6563\u6a21\u578b\u7684\u67b6\u6784\u6216\u9884\u8bad\u7ec3\u6570\u636e\u5f3a\u76f8\u5173\uff1f \u5408\u6210\u6570\u636e\u8d28\u91cf\u7684\u6df1\u5c42\u8bc4\u4f30\uff1a mAP@.50:.95\u662f\u76ee\u6807\u68c0\u6d4b\u7684\u6807\u51c6\u6307\u6807\uff0c\u4f46\u5408\u6210\u56fe\u50cf\u672c\u8eab\u7684\u89c6\u89c9\u8d28\u91cf\u3001\u591a\u6837\u6027\u4ee5\u53ca\u5bf9\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u7684\u957f\u671f\u5f71\u54cd\uff0c\u5728\u6458\u8981\u4e2d\u672a\u8be6\u7ec6\u8bf4\u660e\u3002\u4f8b\u5982\uff0c\u5408\u6210\u6570\u636e\u662f\u5426\u5f15\u5165\u4e86\u65b0\u7684\u504f\u5dee\u6216\u4f2a\u5f71\uff1f \u771f\u5b9e\u4e16\u754c\u57df\u5dee\u8ddd\uff1a \u5408\u6210\u6570\u636e\u4e0e\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e4b\u95f4\u7684\u57df\u5dee\u8ddd\uff08domain gap\uff09\u59cb\u7ec8\u662f\u4e00\u4e2a\u6311\u6218\u3002\u5c3d\u7ba1FLORA\u63d0\u9ad8\u4e86\u6027\u80fd\uff0c\u4f46\u8fd9\u79cd\u63d0\u5347\u5728\u9762\u5bf9\u6781\u7aef\u771f\u5b9e\u4e16\u754c\u590d\u6742\u6027\u65f6\u7684\u9c81\u68d2\u6027\u5982\u4f55\uff1f\u6a21\u578b\u5728\u7eaf\u5408\u6210\u6570\u636e\u4e0a\u8bad\u7ec3\u540e\uff0c\u5728\u672a\u89c1\u8fc7\u7684\u771f\u5b9e\u6570\u636e\u4e0a\u7684\u8868\u73b0\u5982\u4f55\uff1f Key Findings: Our approach uses the Flux 1.1 Dev diffusion model, fine-tuned exclusively through Low-Rank Adaptation (LoRA). This work demonstrates that it is possible to surpass state-of-the-art performance with far greater efficiency, as FLORA achieves superior results using only 10% of the data and a fraction of the computational cost. Links: PDF arXiv The Rosario Dataset v2: Multimodal Dataset for Agricultural Robotics Authors: Nicolas Soncini, Javier Cremona, Erica Vidal, Maximiliano Garc\u00eda, Gast\u00f3n Castro, Taih\u00fa Pire Published: 2025-08-29 Categories: cs.RO, cs.CV, cs.SY, eess.SY, I.2.9 Abstract: We present a multi-modal dataset collected in a soybean crop field, comprising over two hours of recorded data from sensors such as stereo infrared camera, color camera, accelerometer, gyroscope, magnetometer, GNSS (Single Point Positioning, Real-Time Kinematic and Post-Processed Kinematic), and wheel odometry. This dataset captures key challenges inherent to robotics in agricultural environments, including variations in natural lighting, motion blur, rough terrain, and long, perceptually aliased sequences. By addressing these complexities, the dataset aims to support the development and benchmarking of advanced algorithms for localization, mapping, perception, and navigation in agricultural robotics. The platform and data collection system is designed to meet the key requirements for evaluating multi-modal SLAM systems, including hardware synchronization of sensors, 6-DOF ground truth and loops on long trajectories. We run multimodal state-of-the art SLAM methods on the dataset, showcasing the existing limitations in their application on agricultural settings. The dataset and utilities to work with it are released on https://cifasis.github.io/rosariov2/. Analysis: \u4f5c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u4e13\u5bb6\uff0c\u6211\u5bf9\u8fd9\u7bc7\u8bba\u6587\u6458\u8981\u7684\u5206\u6790\u5982\u4e0b\uff1a \u8bba\u6587\u6458\u8981\u5206\u6790\uff1aThe Rosario Dataset v2: Multimodal Dataset for Agricultural Robotics 1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u660e\u603b\u7ed3 (2-3\u53e5\u8bdd) Rosario Dataset v2 \u662f\u4e00\u4e2a\u9488\u5bf9\u519c\u4e1a\u673a\u5668\u4eba\u9886\u57df\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u5728\u771f\u5b9e\u7684\u5927\u8c46\u519c\u7530\u4e2d\u91c7\u96c6\uff0c\u5305\u542b\u8d85\u8fc7\u4e24\u5c0f\u65f6\u7684\u4e30\u5bcc\u4f20\u611f\u5668\u6570\u636e\uff0c\u5982\u7acb\u4f53\u7ea2\u5916\u76f8\u673a\u3001\u5f69\u8272\u76f8\u673a\u3001IMU\u3001\u591a\u79cdGNSS\u548c\u8f6e\u5f0f\u91cc\u7a0b\u8ba1\u3002\u8be5\u6570\u636e\u96c6\u65e8\u5728\u901a\u8fc7\u63d0\u4f9b\u786c\u4ef6\u540c\u6b65\u30016-DOF\u771f\u503c\u548c\u957f\u8f68\u8ff9\u5faa\u73af\u7b49\u5173\u952e\u7279\u6027\uff0c\u89e3\u51b3\u519c\u4e1a\u73af\u5883\u4e2d\uff08\u5982\u5149\u7167\u53d8\u5316\u3001\u8fd0\u52a8\u6a21\u7cca\u3001\u5d0e\u5c96\u5730\u5f62\u548c\u611f\u77e5\u6df7\u53e0\uff09\u7684\u6311\u6218\uff0c\u4ece\u800c\u652f\u6301\u548c\u57fa\u51c6\u6d4b\u8bd5\u5148\u8fdb\u7684\u5b9a\u4f4d\u3001\u5efa\u56fe\u3001\u611f\u77e5\u548c\u5bfc\u822a\u7b97\u6cd5\u3002\u4f5c\u8005\u8fd8\u5229\u7528\u8be5\u6570\u636e\u96c6\u8fd0\u884c\u4e86\u6700\u5148\u8fdb\u7684\u591a\u6a21\u6001SLAM\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u7b97\u6cd5\u5728\u519c\u4e1a\u5e94\u7528\u4e2d\u7684\u5c40\u9650\u6027\u3002 2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u8bba\u65b9\u6cd5 \u8be5\u8bba\u6587\u7684\u5173\u952e\u521b\u65b0\u5728\u4e8e \u521b\u5efa\u4e86\u4e00\u4e2a\u9ad8\u5ea6\u5168\u9762\u4e14\u6781\u5177\u6311\u6218\u6027\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u4e13\u95e8\u4e3a\u519c\u4e1a\u673a\u5668\u4eba\u5e94\u7528\u91cf\u8eab\u5b9a\u5236 \u3002\u5176\u65b9\u6cd5\u8bba\u4f53\u73b0\u5728\uff1a \u4e30\u5bcc\u7684\u591a\u6a21\u6001\u4f20\u611f\u5668\u878d\u5408\uff1a \u7ed3\u5408\u4e86\u7acb\u4f53\u7ea2\u5916\u76f8\u673a\uff08\u5bf9\u5149\u7167\u53d8\u5316\u9c81\u68d2\uff09\u3001\u5f69\u8272\u76f8\u673a\u3001\u60ef\u6027\u6d4b\u91cf\u5355\u5143\uff08IMU\uff09\u3001\u591a\u79cdGNSS\uff08\u5305\u62ec\u9ad8\u7cbe\u5ea6\u7684RTK\u548cPPK\uff09\u4ee5\u53ca\u8f6e\u5f0f\u91cc\u7a0b\u8ba1\uff0c\u63d0\u4f9b\u4e86\u6781\u5176\u591a\u6837\u5316\u7684\u6570\u636e\u6d41\uff0c\u8fd9\u5bf9\u4e8e\u5f00\u53d1\u9c81\u68d2\u7684\u4f20\u611f\u5668\u878d\u5408\u7b97\u6cd5\u81f3\u5173\u91cd\u8981\u3002 \u9488\u5bf9\u519c\u4e1a\u73af\u5883\u7684\u7279\u5b9a\u6311\u6218\uff1a \u6570\u636e\u96c6\u7684\u8bbe\u8ba1\u548c\u91c7\u96c6\u660e\u786e\u8003\u8651\u4e86\u519c\u4e1a\u73af\u5883\u7684\u72ec\u7279\u96be\u9898\uff0c\u5982\u81ea\u7136\u5149\u7167\u5267\u70c8\u53d8\u5316\u3001\u5d0e\u5c96\u5730\u5f62\u5bfc\u81f4\u7684\u8fd0\u52a8\u6a21\u7cca\u3001\u4ee5\u53ca\u519c\u4f5c\u7269\u884c\u95f4\u91cd\u590d\u6a21\u5f0f\u9020\u6210\u7684\u201c\u611f\u77e5\u6df7\u53e0\u201d\uff08perceptually aliased sequences\uff09\uff0c\u8fd9\u4e9b\u90fd\u662f\u4f20\u7edfCV\u548cSLAM\u7b97\u6cd5\u7684\u75db\u70b9\u3002 \u9ad8\u6807\u51c6\u7684\u6570\u636e\u8d28\u91cf\u548c\u57fa\u51c6\u6d4b\u8bd5\u652f\u6301\uff1a \u5f3a\u8c03\u4e86\u4f20\u611f\u5668\u786c\u4ef6\u540c\u6b65\u3001\u63d0\u4f9b\u9ad8\u7cbe\u5ea6\u76846-DOF\uff08\u516d\u81ea\u7531\u5ea6\uff09\u5730\u9762\u771f\u503c\uff0c\u4ee5\u53ca\u5305\u542b\u957f\u8f68\u8ff9\u4e0a\u7684\u5faa\u73af\uff08loops\uff09\uff0c\u8fd9\u4e9b\u90fd\u662f\u8bc4\u4f30\u548c\u5f00\u53d1\u591a\u6a21\u6001SLAM\u7cfb\u7edf\u4e0d\u53ef\u6216\u7f3a\u7684\u8981\u7d20\u3002\u901a\u8fc7\u8fd0\u884cSOTA SLAM\u65b9\u6cd5\u5e76\u5c55\u793a\u5176\u5c40\u9650\u6027\uff0c\u4f5c\u8005\u4e0d\u4ec5\u9a8c\u8bc1\u4e86\u6570\u636e\u96c6\u7684\u6311\u6218\u6027\uff0c\u4e5f\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u660e\u786e\u7684\u65b9\u5411\u3002 3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd \u52a0\u901f\u519c\u4e1a\u673a\u5668\u4eba\u7814\u7a76\uff1a \u63d0\u4f9b\u4e86\u4e00\u4e2a\u6807\u51c6\u5316\u3001\u9ad8\u6311\u6218\u6027\u4e14\u516c\u5f00\u53ef\u7528\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5c06\u6781\u5927\u5730\u52a0\u901f\u519c\u4e1a\u9886\u57df\u5b9a\u4f4d\u3001\u5efa\u56fe\u3001\u611f\u77e5\u548c\u5bfc\u822a\u7b97\u6cd5\u7684\u5f00\u53d1\u548c\u8bc4\u4f30\u3002 \u63a8\u52a8\u9c81\u68d2\u7b97\u6cd5\u53d1\u5c55\uff1a \u8feb\u4f7f\u7814\u7a76\u4eba\u5458\u5f00\u53d1\u80fd\u591f\u5e94\u5bf9\u771f\u5b9e\u4e16\u754c\u519c\u4e1a\u73af\u5883\u590d\u6742\u6027\u7684\u66f4\u9c81\u68d2\u3001\u66f4\u6cdb\u5316\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\uff0c\u5c24\u5176\u662f\u5728\u5149\u7167\u53d8\u5316\u3001\u7eb9\u7406\u91cd\u590d\u548c\u5d0e\u5c96\u5730\u5f62\u4e0b\u7684\u8868\u73b0\u3002 \u5f25\u5408\u7406\u8bba\u4e0e\u5b9e\u8df5\u7684\u9e3f\u6c9f\uff1a \u901a\u8fc7\u63d0\u4f9b\u771f\u5b9e\u4e16\u754c\u7684\u590d\u6742\u6570\u636e\uff0c\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u5c06\u5b9e\u9a8c\u5ba4\u6210\u679c\u66f4\u597d\u5730\u5e94\u7528\u4e8e\u5b9e\u9645\u7684\u519c\u4e1a\u81ea\u52a8\u5316\u548c\u7cbe\u51c6\u519c\u4e1a\u573a\u666f\u3002 \u4fc3\u8fdb\u591a\u4f20\u611f\u5668\u878d\u5408\u548cSLAM\u6280\u672f\u8fdb\u6b65\uff1a \u6570\u636e\u96c6\u7684\u591a\u6a21\u6001\u7279\u6027\u5c06\u63a8\u52a8\u591a\u4f20\u611f\u5668\u878d\u5408\u6280\u672f\u7684\u53d1\u5c55\uff0c\u7279\u522b\u662f\u5728\u89c6\u89c9-\u60ef\u6027-GNSS-\u91cc\u7a0b\u8ba1\u878d\u5408SLAM\u65b9\u9762\uff0c\u4ee5\u5b9e\u73b0\u66f4\u7cbe\u786e\u548c\u53ef\u9760\u7684\u5b9a\u4f4d\u3002 \u4e3a\u65b0\u7814\u7a76\u65b9\u5411\u63d0\u4f9b\u57fa\u7840\uff1a \u6570\u636e\u96c6\u4e2d\u7684\u6311\u6218\uff08\u5982\u611f\u77e5\u6df7\u53e0\uff09\u53ef\u80fd\u4f1a\u6fc0\u53d1\u65b0\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u4f8b\u5982\u57fa\u4e8e\u8bed\u4e49\u4fe1\u606f\u7684\u5b9a\u4f4d\u3001\u62d3\u6251\u5efa\u56fe\u6216\u66f4\u5148\u8fdb\u7684\u5faa\u73af\u95ed\u5408\u6280\u672f\u3002 4. \u53ef\u80fd\u53d7\u76ca\u7684\u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 \u519c\u4e1a\u673a\u5668\u4eba\u4e0e\u81ea\u52a8\u5316\uff1a \u8fd9\u662f\u6700\u76f4\u63a5\u7684\u53d7\u76ca\u9886\u57df\uff0c\u5305\u62ec\u81ea\u4e3b\u62d6\u62c9\u673a\u3001\u55b7\u6d12\u673a\u5668\u4eba\u3001\u91c7\u6458\u673a\u5668\u4eba\u3001\u4f5c\u7269\u76d1\u6d4b\u65e0\u4eba\u8f66\u7b49\u3002 \u540c\u6b65\u5b9a\u4f4d\u4e0e\u5efa\u56fe (SLAM)\uff1a \u7279\u522b\u662f\u591a\u6a21\u6001SLAM\u3001\u89c6\u89c9-\u60ef\u6027SLAM\u3001\u4ee5\u53ca\u5728\u4f4e\u7eb9\u7406\u6216\u91cd\u590d\u7eb9\u7406\u73af\u5883\u4e0b\u7684SLAM\u3002 \u8ba1\u7b97\u673a\u89c6\u89c9\uff1a \u6237\u5916\u573a\u666f\u7406\u89e3\u3001\u4f5c\u7269/\u6742\u8349\u68c0\u6d4b\u4e0e\u5206\u5272\u30013D\u91cd\u5efa\u3001\u8fd0\u52a8\u4f30\u8ba1\u3001\u5149\u6d41\u5206\u6790\u3001\u4ee5\u53ca\u5728\u6076\u52a3\u5149\u7167\u6761\u4ef6\u4e0b\u7684\u56fe\u50cf\u5904\u7406\u3002 \u673a\u5668\u5b66\u4e60\u4e0e\u6df1\u5ea6\u5b66\u4e60\uff1a \u8bad\u7ec3\u7528\u4e8e\u519c\u4e1a\u73af\u5883\u7684\u9c81\u68d2\u611f\u77e5\u6a21\u578b\uff0c\u4f8b\u5982\u7528\u4e8e\u4f5c\u7269\u5065\u5eb7\u76d1\u6d4b\u3001\u75c5\u866b\u5bb3\u8bc6\u522b\u3001\u4ea7\u91cf\u4f30\u8ba1\u7b49\u3002 \u4f20\u611f\u5668\u878d\u5408\uff1a \u5f00\u53d1\u548c\u6d4b\u8bd5\u878d\u5408\u5f02\u6784\u4f20\u611f\u5668\u6570\u636e\u4ee5\u63d0\u9ad8\u7cfb\u7edf\u6027\u80fd\u7684\u7b97\u6cd5\u3002 \u6237\u5916\u81ea\u4e3b\u5bfc\u822a\uff1a \u4efb\u4f55\u9700\u8981\u5728\u975e\u7ed3\u6784\u5316\u3001\u52a8\u6001\u548c\u6311\u6218\u6027\u6237\u5916\u73af\u5883\u4e2d\u8fdb\u884c\u81ea\u4e3b\u5bfc\u822a\u7684\u7cfb\u7edf\uff08\u4f8b\u5982\uff0c\u5efa\u7b51\u673a\u5668\u4eba\u3001\u91c7\u77ff\u673a\u5668\u4eba\u3001\u6797\u4e1a\u673a\u5668\u4eba\uff09\u3002 \u9ad8\u7cbe\u5ea6GNSS\u5e94\u7528\uff1a \u7ed3\u5408RTK/PPK\u6570\u636e\u8fdb\u884c\u9ad8\u7cbe\u5ea6\u5b9a\u4f4d\u548c\u5730\u56fe\u6821\u51c6\u7684\u7814\u7a76\u3002 5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u7684\u5c40\u9650\u6027 \u4f5c\u7269\u548c\u73af\u5883\u7279\u5f02\u6027\uff1a \u6570\u636e\u96c6\u4ec5\u5728\u5927\u8c46\u519c\u7530\u4e2d\u91c7\u96c6\u3002\u867d\u7136\u5927\u8c46\u662f\u91cd\u8981\u4f5c\u7269\uff0c\u4f46\u5176\u89c6\u89c9\u7279\u5f81\u3001\u751f\u957f\u6a21\u5f0f\u548c\u519c\u7530\u7ed3\u6784\u53ef\u80fd\u4e0e\u5176\u4ed6\u4f5c\u7269\uff08\u5982\u7389\u7c73\u3001\u5c0f\u9ea6\u3001\u679c\u56ed\u3001\u6e29\u5ba4\u852c\u83dc\uff09\u6709\u663e\u8457\u5dee\u5f02\u3002\u8fd9\u9650\u5236\u4e86\u5176\u5728\u5176\u4ed6\u519c\u4e1a\u573a\u666f\u4e2d\u7684\u76f4\u63a5\u6cdb\u5316\u80fd\u529b\u3002 \u5730\u7406\u4f4d\u7f6e\u548c\u6c14\u5019\u9650\u5236\uff1a \u6570\u636e\u96c6\u5728\u201cRosario\u201d\u5730\u533a\u91c7\u96c6\uff0c\u8fd9\u610f\u5473\u7740\u5176\u53ef\u80fd\u53cd\u6620\u7279\u5b9a\u5730\u7406\u533a\u57df\u7684\u6c14\u5019\u3001\u571f\u58e4\u7c7b\u578b\u548c\u5149\u7167\u6761\u4ef6\u3002\u5728\u5176\u4ed6\u5730\u533a\uff0c\u73af\u5883\u56e0\u7d20\u53ef\u80fd\u5927\u76f8\u5f84\u5ead\u3002 \u5e73\u53f0\u7279\u5b9a\u6027\uff1a \u6570\u636e\u662f\u7531\u4e00\u4e2a\u7279\u5b9a\u7684\u201c\u5e73\u53f0\u548c\u6570\u636e\u91c7\u96c6\u7cfb\u7edf\u201d\u6536\u96c6\u7684\u3002\u4f20\u611f\u5668\u7684\u5177\u4f53\u914d\u7f6e\u3001\u5b89\u88c5\u9ad8\u5ea6\u3001\u89c6\u89d2\u4ee5\u53ca\u5e73\u53f0\u81ea\u8eab\u7684\u8fd0\u52a8\u7279\u6027\u53ef\u80fd\u65e0\u6cd5\u5b8c\u5168\u4ee3\u8868\u6240\u6709\u519c\u4e1a\u673a\u5668\u4eba\u7cfb\u7edf\u3002 \u4e3b\u8981\u4fa7\u91cd\u4e8eSLAM/\u5bfc\u822a\uff1a \u5c3d\u7ba1\u6570\u636e\u4e30\u5bcc\uff0c\u4f46\u6458\u8981\u4e2d\u660e\u786e\u5f3a\u8c03\u4e86\u5bf9SLAM\u7cfb\u7edf\u8bc4\u4f30\u7684\u5173\u952e\u8981\u6c42\uff08\u786c\u4ef6\u540c\u6b65\u30016-DOF\u771f\u503c\u3001\u5faa\u73af\uff09\u3002\u8fd9\u8868\u660e\u6570\u636e\u96c6\u7684\u8bbe\u8ba1\u548c\u6807\u6ce8\u53ef\u80fd\u66f4\u4fa7\u91cd\u4e8e\u5b9a\u4f4d\u548c\u5efa\u56fe\u4efb\u52a1\uff0c\u5bf9\u4e8e\u5176\u4ed6\u7ec6\u7c92\u5ea6\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\uff08\u5982\u7cbe\u786e\u7684\u690d\u7269\u8868\u578b\u5206\u6790\u3001\u75c5\u5bb3\u65e9\u671f\u68c0\u6d4b\uff09\u53ef\u80fd\u9700\u8981\u989d\u5916\u7684\u6807\u6ce8\u6216\u5904\u7406\u3002 \u672a\u63d0\u53ca\u5176\u4ed6\u52a8\u6001\u969c\u788d\u7269\uff1a \u6458\u8981\u4e2d\u6ca1\u6709\u660e\u786e\u8bf4\u660e\u6570\u636e\u96c6\u4e2d\u662f\u5426\u5305\u542b\u9664\u4e86\u673a\u5668\u4eba\u672c\u8eab\u4e4b\u5916\u7684\u5176\u4ed6\u52a8\u6001\u969c\u788d\u7269\uff08\u4f8b\u5982\uff0c\u519c\u573a\u5de5\u4eba\u3001\u91ce\u751f\u52a8\u7269\u3001\u5176\u4ed6\u519c\u673a\uff09\u3002\u5728\u771f\u5b9e\u7684\u519c\u4e1a\u73af\u5883\u4e2d\uff0c\u8fd9\u4e9b\u52a8\u6001\u56e0\u7d20\u5bf9\u5bfc\u822a\u548c\u5b89\u5168\u81f3\u5173\u91cd\u8981\u3002 \u6570\u636e\u91cf\uff1a \u201c\u8d85\u8fc7\u4e24\u5c0f\u65f6\u201d\u7684\u6570\u636e\u91cf\u5bf9\u4e8e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8bad\u7ec3\u6765\u8bf4\uff0c\u867d\u7136\u4e0d\u9519\uff0c\u4f46\u53ef\u80fd\u4e0d\u5982\u4e00\u4e9b\u5927\u89c4\u6a21\u57ce\u5e02\u6570\u636e\u96c6\u90a3\u6837\u5e9e\u5927\uff0c\u8fd9\u5728\u67d0\u4e9b\u9700\u8981\u6d77\u91cf\u6570\u636e\u7684\u4efb\u52a1\u4e0a\u53ef\u80fd\u6784\u6210\u9650\u5236\u3002 Key Findings: We present a multi-modal dataset collected in a soybean crop field, comprising over two hours of recorded data from sensors such as stereo infrared camera, color camera, accelerometer, gyroscope, magnetometer, GNSS (Single Point Positioning, Real-Time Kinematic and Post-Processed Kinematic), and wheel odometry. Links: PDF arXiv Complete Gaussian Splats from a Single Image with Denoising Diffusion Models Authors: Ziwei Liao, Mohamed Sayed, Steven L. Waslander, Sara Vicente, Daniyar Turmukhambetov, Michael Firman Published: 2025-08-29 Categories: cs.CV, cs.AI, cs.RO Abstract: Gaussian splatting typically requires dense observations of the scene and can fail to reconstruct occluded and unobserved areas. We propose a latent diffusion model to reconstruct a complete 3D scene with Gaussian splats, including the occluded parts, from only a single image during inference. Completing the unobserved surfaces of a scene is challenging due to the ambiguity of the plausible surfaces. Conventional methods use a regression-based formulation to predict a single \"mode\" for occluded and out-of-frustum surfaces, leading to blurriness, implausibility, and failure to capture multiple possible explanations. Thus, they often address this problem partially, focusing either on objects isolated from the background, reconstructing only visible surfaces, or failing to extrapolate far from the input views. In contrast, we propose a generative formulation to learn a distribution of 3D representations of Gaussian splats conditioned on a single input image. To address the lack of ground-truth training data, we propose a Variational AutoReconstructor to learn a latent space only from 2D images in a self-supervised manner, over which a diffusion model is trained. Our method generates faithful reconstructions and diverse samples with the ability to complete the occluded surfaces for high-quality 360-degree renderings. Analysis: \u8fd9\u7bc7\u8bba\u6587\u7684\u6458\u8981\u5c55\u793a\u4e86\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u4e00\u4e2a\u975e\u5e38\u6709\u8da3\u4e14\u91cd\u8981\u7684\u8fdb\u5c55\u3002\u4ee5\u4e0b\u662f\u6211\u7684\u8be6\u7ec6\u5206\u6790\uff1a 1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u660e\u6458\u8981 (Concise Summary) \u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff0c\u80fd\u591f\u4ec5\u4ece\u5355\u5f20\u56fe\u50cf\u63a8\u65ad\u51fa\u5b8c\u6574\u76843D\u573a\u666f\uff0c\u5305\u62ec\u88ab\u906e\u6321\u548c\u672a\u89c2\u5bdf\u5230\u7684\u533a\u57df\uff0c\u5e76\u4ee5\u9ad8\u65af\u6cfc\u6e85\uff08Gaussian Splats\uff09\u7684\u5f62\u5f0f\u8868\u793a\u3002\u4e0e\u4f20\u7edf\u56de\u5f52\u65b9\u6cd5\u751f\u6210\u5355\u4e00\u6a21\u7cca\u6216\u4e0d\u771f\u5b9e\u7684\u91cd\u5efa\u4e0d\u540c\uff0c\u8be5\u65b9\u6cd5\u91c7\u7528\u751f\u6210\u5f0f\u516c\u5f0f\u6765\u5b66\u4e603D\u8868\u793a\u7684\u5206\u5e03\uff0c\u4ece\u800c\u5b9e\u73b0\u5fe0\u5b9e\u4e14\u591a\u6837\u5316\u7684\u573a\u666f\u8865\u5168\uff0c\u652f\u6301\u9ad8\u8d28\u91cf\u7684360\u5ea6\u6e32\u67d3\u3002 2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u8bba (Key Innovation or Methodological Approach) \u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u5176 \u751f\u6210\u5f0f\u516c\u5f0f \uff0c\u5b83\u4e0d\u662f\u9884\u6d4b\u5355\u4e00\u76843D\u91cd\u5efa\uff0c\u800c\u662f\u5b66\u4e60\u7ed9\u5b9a\u5355\u5f20\u8f93\u5165\u56fe\u50cf\u4e0b3D\u9ad8\u65af\u6cfc\u6e85\u8868\u793a\u7684 \u5206\u5e03 \u3002\u8fd9\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u6355\u6349\u88ab\u906e\u6321\u548c\u672a\u89c2\u5bdf\u533a\u57df\u7684\u591a\u79cd\u5408\u7406\u89e3\u91ca\uff0c\u4ece\u800c\u907f\u514d\u4e86\u4f20\u7edf\u56de\u5f52\u65b9\u6cd5\u5bfc\u81f4\u7684\u6a21\u7cca\u548c\u4e0d\u771f\u5b9e\u3002\u4e3a\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u4e86 \u6f5c\u5728\u6269\u6563\u6a21\u578b \uff0c\u5e76\u5f15\u5165\u4e86 \u53d8\u5206\u81ea\u91cd\u5efa\u5668\uff08Variational AutoReconstructor, VAR\uff09 \uff0c\u4ee5\u81ea\u76d1\u7763\u65b9\u5f0f\u4ec5\u4ece2D\u56fe\u50cf\u5b66\u4e60\u4e00\u4e2a\u5408\u9002\u7684\u6f5c\u5728\u7a7a\u95f4\uff0c\u4ece\u800c\u89e3\u51b3\u4e863D\u771f\u503c\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\u3002 3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd (Potential Impact on the Field) \u8fd9\u9879\u7814\u7a76\u6709\u671b\u663e\u8457\u63a8\u52a8 \u5355\u56fe\u50cf3D\u573a\u666f\u91cd\u5efa \u9886\u57df\u7684\u53d1\u5c55\uff0c\u4f7f\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u5728\u7f3a\u4e4f\u5bc6\u96c6\u591a\u89c6\u89d2\u6570\u636e\u7684\u5b9e\u9645\u5e94\u7528\u4e2d\u66f4\u5177\u5b9e\u7528\u6027\u3002\u901a\u8fc7\u63d0\u4f9b \u5b8c\u6574\u4e14\u591a\u6837\u5316 \u76843D\u573a\u666f\u8868\u793a\uff0c\u5b83\u80fd\u6781\u5927\u5730\u964d\u4f4e3D\u5185\u5bb9\u521b\u5efa\u7684\u6570\u636e\u91c7\u96c6\u95e8\u69db\uff0c\u5e76\u4e3a\u9700\u8981\u7406\u89e3\u88ab\u906e\u6321\u573a\u666f\u90e8\u5206\u7684 \u673a\u5668\u4eba\u3001AR/VR\u548c\u81ea\u52a8\u9a7e\u9a76 \u7b49\u9886\u57df\u63d0\u4f9b\u66f4\u9c81\u68d2\u76843D\u611f\u77e5\u80fd\u529b\u3002\u5b83\u5c06\u5355\u56fe\u50cf3D\u91cd\u5efa\u4ece\u5355\u4e00\u786e\u5b9a\u6027\u8f93\u51fa\u63a8\u5411\u4e86\u751f\u6210\u5f0f\u3001\u591a\u6a21\u6001\u8f93\u51fa\u7684\u65b0\u8303\u5f0f\u3002 4. \u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 (Related Areas or Applications) \u673a\u5668\u4eba\u5b66\u4e0e\u81ea\u52a8\u9a7e\u9a76\uff1a \u63d0\u5347\u5bf9\u590d\u6742\u73af\u5883\u76843D\u611f\u77e5\u80fd\u529b\uff0c\u5c24\u5176\u662f\u5728\u90e8\u5206\u906e\u6321\u6216\u89c6\u89d2\u53d7\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u6709\u52a9\u4e8e\u8def\u5f84\u89c4\u5212\u3001\u907f\u969c\u548c\u4eba\u673a\u4ea4\u4e92\u3002 \u589e\u5f3a\u73b0\u5b9e\uff08AR\uff09\u4e0e\u865a\u62df\u73b0\u5b9e\uff08VR\uff09\uff1a \u5b9e\u73b0\u66f4\u903c\u771f\u3001\u66f4\u5b8c\u6574\u7684\u865a\u62df\u573a\u666f\u6784\u5efa\u548c\u5185\u5bb9\u751f\u6210\uff0c\u63d0\u5347\u6c89\u6d78\u5f0f\u4f53\u9a8c\u3002 3D\u5185\u5bb9\u521b\u4f5c\uff1a \u6781\u5927\u5730\u7b80\u5316\u4ece2D\u56fe\u50cf\u751f\u6210\u5b8c\u65743D\u8d44\u4ea7\u7684\u6d41\u7a0b\uff0c\u964d\u4f4e\u6210\u672c\u548c\u65f6\u95f4\u3002 \u6570\u5b57\u5b6a\u751f\uff1a \u4ece\u6709\u9650\u7684\u56fe\u50cf\u6570\u636e\u4e2d\u6784\u5efa\u66f4\u5168\u9762\u7684\u7269\u7406\u4e16\u754c\u6570\u5b57\u6a21\u578b\u3002 \u9065\u611f\u4e0e\u6d4b\u7ed8\uff1a \u4ece\u5355\u5f20\u822a\u7a7a\u6216\u536b\u661f\u56fe\u50cf\u4e2d\u63a8\u65ad\u51fa\u66f4\u5b8c\u6574\u7684\u5730\u5f62\u6216\u5efa\u7b513D\u6a21\u578b\u3002 5. \u53ef\u4ece\u6458\u8981\u4e2d\u63a8\u65ad\u51fa\u7684\u5c40\u9650\u6027 (Limitations Inferred from the Abstract) \u81ea\u76d1\u7763\u5b66\u4e60\u7684\u5c40\u9650\u6027\uff1a \u5c3d\u7ba1\u901a\u8fc7VAR\u89e3\u51b3\u4e863D\u771f\u503c\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u4f46\u81ea\u76d1\u7763\u5b66\u4e60\u83b7\u5f97\u7684\u6f5c\u5728\u7a7a\u95f4\u7684\u8d28\u91cf\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u53ef\u80fd\u4ecd\u4e0d\u5982\u76f4\u63a5\u4f7f\u7528\u5927\u91cf\u9ad8\u8d28\u91cf3D\u771f\u503c\u6570\u636e\u8fdb\u884c\u76d1\u7763\u5b66\u4e60\u3002 \u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\uff1a \u6269\u6563\u6a21\u578b\uff0c\u5c24\u5176\u662f\u57283D\u751f\u6210\u4efb\u52a1\u4e2d\uff0c\u901a\u5e38\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u9636\u6bb5\u90fd\u9700\u8981\u5927\u91cf\u7684\u8ba1\u7b97\u8d44\u6e90\u548c\u65f6\u95f4\u3002 \u5355\u56fe\u50cf\u8f93\u5165\u7684\u654f\u611f\u6027\uff1a \u6a21\u578b\u7684\u6027\u80fd\u53ef\u80fd\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u5355\u5f20\u8f93\u5165\u56fe\u50cf\u7684\u8d28\u91cf\u3001\u5206\u8fa8\u7387\u3001\u89c6\u89d2\u548c\u5149\u7167\u6761\u4ef6\u3002\u6781\u7aef\u6216\u4f4e\u8d28\u91cf\u7684\u8f93\u5165\u53ef\u80fd\u5bfc\u81f4\u4e0d\u4f73\u7684\u91cd\u5efa\u6548\u679c\u3002 \u751f\u6210\u7ed3\u679c\u7684\u8bed\u4e49\u5408\u7406\u6027\uff1a \u5c3d\u7ba1\u80fd\u591f\u751f\u6210\u591a\u6837\u5316\u7684\u8865\u5168\u7ed3\u679c\uff0c\u4f46\u5982\u4f55\u786e\u4fdd\u6240\u6709\u751f\u6210\u7684\u88ab\u906e\u6321\u90e8\u5206\u5728\u8bed\u4e49\u4e0a\u5b8c\u5168\u5408\u7406\u4e14\u7269\u7406\u4e0a\u4e00\u81f4\uff0c\u5bf9\u4e8e\u590d\u6742\u573a\u666f\u4ecd\u662f\u4e00\u4e2a\u6311\u6218\u3002 \u6cdb\u5316\u80fd\u529b\uff1a \u6458\u8981\u672a\u8be6\u7ec6\u8bf4\u660e\u8bad\u7ec3\u6570\u636e\u7684\u8303\u56f4\uff0c\u56e0\u6b64\u6a21\u578b\u5bf9\u8bad\u7ec3\u96c6\u4e2d\u672a\u51fa\u73b0\u7684\u65b0\u9896\u573a\u666f\u3001\u7269\u4f53\u7c7b\u522b\u6216\u590d\u6742\u4ea4\u4e92\u7684\u6cdb\u5316\u80fd\u529b\u53ef\u80fd\u5b58\u5728\u5c40\u9650\u3002 \u603b\u7ed3\uff1a \u8fd9\u7bc7\u8bba\u6587\u901a\u8fc7\u5c06\u6269\u6563\u6a21\u578b\u4e0e\u9ad8\u65af\u6cfc\u6e85\u7ed3\u5408\uff0c\u5e76\u5de7\u5999\u5730\u89e3\u51b3\u4e863D\u771f\u503c\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u5728\u5355\u56fe\u50cf3D\u91cd\u5efa\u9886\u57df\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\u3002\u5176\u751f\u6210\u5f0f\u65b9\u6cd5\u80fd\u591f\u5904\u7406\u56fa\u6709\u7684\u6b67\u4e49\u6027\uff0c\u4e3a\u672a\u6765\u76843D\u611f\u77e5\u548c\u5185\u5bb9\u751f\u6210\u5f00\u8f9f\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002 Key Findings: We propose a latent diffusion model to reconstruct a complete 3D scene with Gaussian splats, including the occluded parts, from only a single image during inference. In contrast, we propose a generative formulation to learn a distribution of 3D representations of Gaussian splats conditioned on a single input image. To address the lack of ground-truth training data, we propose a Variational AutoReconstructor to learn a latent space only from 2D images in a self-supervised manner, over which a diffusion model is trained. Our method generates faithful reconstructions and diverse samples with the ability to complete the occluded surfaces for high-quality 360-degree renderings. Links: PDF arXiv Efficient Diffusion-Based 3D Human Pose Estimation with Hierarchical Temporal Pruning Authors: Yuquan Bi, Hongsong Wang, Xinli Shi, Zhipeng Gui, Jie Gui, Yuan Yan Tang Published: 2025-08-29 Categories: cs.CV Abstract: Diffusion models have demonstrated strong capabilities in generating high-fidelity 3D human poses, yet their iterative nature and multi-hypothesis requirements incur substantial computational cost. In this paper, we propose an Efficient Diffusion-Based 3D Human Pose Estimation framework with a Hierarchical Temporal Pruning (HTP) strategy, which dynamically prunes redundant pose tokens across both frame and semantic levels while preserving critical motion dynamics. HTP operates in a staged, top-down manner: (1) Temporal Correlation-Enhanced Pruning (TCEP) identifies essential frames by analyzing inter-frame motion correlations through adaptive temporal graph construction; (2) Sparse-Focused Temporal MHSA (SFT MHSA) leverages the resulting frame-level sparsity to reduce attention computation, focusing on motion-relevant tokens; and (3) Mask-Guided Pose Token Pruner (MGPTP) performs fine-grained semantic pruning via clustering, retaining only the most informative pose tokens. Experiments on Human3.6M and MPI-INF-3DHP show that HTP reduces training MACs by 38.5\\%, inference MACs by 56.8\\%, and improves inference speed by an average of 81.1\\% compared to prior diffusion-based methods, while achieving state-of-the-art performance. Analysis: \u8fd9\u7bc7\u8bba\u6587\u7684\u6458\u8981\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5173\u4e8e\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u4e2d3D\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u7684\u6709\u8da3\u8fdb\u5c55\u3002\u4ee5\u4e0b\u662f\u6839\u636e\u6458\u8981\u8fdb\u884c\u7684\u5206\u6790\uff1a 1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u660e\u6458\u8981 (Concise Summary) \u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u9ad8\u4fdd\u771f3D\u4eba\u4f53\u59ff\u6001\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u8fed\u4ee3\u6027\u8d28\u548c\u591a\u5047\u8bbe\u9700\u6c42\u5bfc\u81f4\u4e86\u5de8\u5927\u7684\u8ba1\u7b97\u6210\u672c\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u57fa\u4e8e\u6269\u6563\u76843D\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u91c7\u7528\u5206\u5c42\u65f6\u95f4\u526a\u679d\uff08HTP\uff09\u7b56\u7565\uff0c\u52a8\u6001\u5730\u5728\u5e27\u548c\u8bed\u4e49\u5c42\u9762\u526a\u679d\u5197\u4f59\u59ff\u6001tokens\uff0c\u540c\u65f6\u4fdd\u7559\u5173\u952e\u8fd0\u52a8\u52a8\u6001\u3002HTP\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\uff0c\u5e76\u63d0\u5347\u4e86\u63a8\u7406\u901f\u5ea6\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002 2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u8bba (Key Innovation or Methodological Approach) \u8be5\u8bba\u6587\u7684\u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u5176 \u5206\u5c42\u65f6\u95f4\u526a\u679d\uff08Hierarchical Temporal Pruning, HTP\uff09\u7b56\u7565 \uff0c\u65e8\u5728\u89e3\u51b3\u6269\u6563\u6a21\u578b\u57283D\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u4e2d\u7684\u8ba1\u7b97\u6548\u7387\u95ee\u9898\u3002HTP\u65b9\u6cd5\u8bba\u7684\u72ec\u7279\u6027\u4f53\u73b0\u5728\u5176\u591a\u9636\u6bb5\u3001\u81ea\u4e0a\u800c\u4e0b\u7684\u526a\u679d\u8fc7\u7a0b\uff1a \u65f6\u95f4\u76f8\u5173\u6027\u589e\u5f3a\u526a\u679d (Temporal Correlation-Enhanced Pruning, TCEP) \uff1a\u901a\u8fc7\u6784\u5efa\u81ea\u9002\u5e94\u65f6\u95f4\u56fe\u6765\u5206\u6790\u5e27\u95f4\u8fd0\u52a8\u76f8\u5173\u6027\uff0c\u4ece\u800c\u8bc6\u522b\u5e76\u526a\u679d\u5197\u4f59\u5e27\uff0c\u4fdd\u7559\u5173\u952e\u5e27\u3002\u8fd9\u662f\u4e00\u79cd\u667a\u80fd\u7684\u5e27\u7ea7\u7a00\u758f\u5316\u65b9\u6cd5\uff0c\u800c\u975e\u7b80\u5355\u91c7\u6837\u3002 \u7a00\u758f\u805a\u7126\u65f6\u95f4\u591a\u5934\u81ea\u6ce8\u610f\u529b (Sparse-Focused Temporal MHSA, SFT MHSA) \uff1a\u5229\u7528TCEP\u4ea7\u751f\u7684\u5e27\u7ea7\u7a00\u758f\u6027\uff0c\u4f18\u5316\u591a\u5934\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684\u8ba1\u7b97\uff0c\u4f7f\u5176\u53ea\u5173\u6ce8\u4e0e\u8fd0\u52a8\u76f8\u5173\u7684tokens\uff0c\u4ece\u800c\u51cf\u5c11\u8ba1\u7b97\u91cf\u3002 \u63a9\u7801\u5f15\u5bfc\u59ff\u6001Token\u526a\u679d\u5668 (Mask-Guided Pose Token Pruner, MGPTP) \uff1a\u5728\u66f4\u7ec6\u7c92\u5ea6\u7684\u8bed\u4e49\u5c42\u9762\uff0c\u901a\u8fc7\u805a\u7c7b\u5bf9\u59ff\u6001tokens\u8fdb\u884c\u526a\u679d\uff0c\u53ea\u4fdd\u7559\u6700\u5177\u4fe1\u606f\u91cf\u7684tokens\u3002\u8fd9\u8fdb\u4e00\u6b65\u7cbe\u70bc\u4e86\u59ff\u6001\u8868\u793a\uff0c\u53bb\u9664\u4e86\u8bed\u4e49\u5197\u4f59\u3002 \u8fd9\u79cd\u7ed3\u5408\u4e86\u5e27\u7ea7\u548c\u8bed\u4e49\u7ea7\u526a\u679d\u7684\u5c42\u6b21\u5316\u3001\u52a8\u6001\u65b9\u6cd5\uff0c\u662f\u5176\u5728\u6269\u6563\u6a21\u578b\u6548\u7387\u63d0\u5347\u4e0a\u7684\u5173\u952e\u7a81\u7834\u3002 3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd (Potential Impact on the Field) \u8fd9\u9879\u7814\u7a76\u5bf9\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u5177\u6709\u663e\u8457\u7684\u6f5c\u5728\u5f71\u54cd\uff1a \u63a8\u52a8\u6269\u6563\u6a21\u578b\u7684\u5b9e\u9645\u5e94\u7528 \uff1a\u901a\u8fc7\u5927\u5e45\u63d0\u9ad8\u6269\u6563\u6a21\u578b\u7684\u8ba1\u7b97\u6548\u7387\u548c\u63a8\u7406\u901f\u5ea6\uff0cHTP\u4f7f\u5176\u5728\u5b9e\u65f6\u6216\u8d44\u6e90\u53d7\u9650\u76843D\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u5e94\u7528\u4e2d\u53d8\u5f97\u66f4\u52a0\u53ef\u884c\u548c\u5b9e\u7528\u3002 \u542f\u53d1\u901a\u7528\u6548\u7387\u63d0\u5347\u7b56\u7565 \uff1aHTP\u7684\u5206\u5c42\u526a\u679d\u601d\u60f3\uff0c\u7279\u522b\u662f\u7ed3\u5408\u8fd0\u52a8\u52a8\u6001\u548c\u8bed\u4e49\u4fe1\u606f\u8fdb\u884c\u7a00\u758f\u5316\u7684\u65b9\u6cd5\uff0c\u53ef\u80fd\u4e3a\u5176\u4ed6\u57fa\u4e8eTransformer\u6216\u6269\u6563\u7684\u5e8f\u5217\u751f\u6210\u6a21\u578b\uff08\u5982\u89c6\u9891\u751f\u6210\u3001\u52a8\u4f5c\u5408\u6210\uff09\u63d0\u4f9b\u901a\u7528\u7684\u6548\u7387\u4f18\u5316\u601d\u8def\u3002 \u52a0\u901f\u7814\u7a76\u4e0e\u5f00\u53d1 \uff1a\u66f4\u5feb\u7684\u6a21\u578b\u8bad\u7ec3\u548c\u63a8\u7406\u901f\u5ea6\u5c06\u5141\u8bb8\u7814\u7a76\u4eba\u5458\u8fdb\u884c\u66f4\u591a\u5b9e\u9a8c\uff0c\u52a0\u901f\u65b0\u7b97\u6cd5\u548c\u5e94\u7528\u573a\u666f\u7684\u63a2\u7d22\u3002 \u63d0\u5347\u7528\u6237\u4f53\u9a8c \uff1a\u5728AR/VR\u3001\u4eba\u673a\u4ea4\u4e92\u3001\u8fd0\u52a8\u5206\u6790\u7b49\u9886\u57df\uff0c\u66f4\u9ad8\u6548\u76843D\u59ff\u6001\u4f30\u8ba1\u610f\u5473\u7740\u66f4\u6d41\u7545\u3001\u54cd\u5e94\u66f4\u5feb\u7684\u7528\u6237\u4f53\u9a8c\u3002 4. \u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 (Related Areas or Applications) \u8fd9\u9879\u7814\u7a76\u7684\u6210\u679c\u53ef\u4ee5\u60e0\u53ca\u4ee5\u4e0b\u9886\u57df\u548c\u5e94\u7528\uff1a \u5b9e\u65f63D\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1 \uff1a\u5982\u5728AR/VR\u6e38\u620f\u3001\u865a\u62df\u8bd5\u7a7f\u3001\u8fdc\u7a0b\u534f\u4f5c\u7b49\u573a\u666f\u4e2d\u3002 \u4eba\u673a\u4ea4\u4e92 (HCI) \uff1a\u901a\u8fc7\u66f4\u51c6\u786e\u3001\u4f4e\u5ef6\u8fdf\u7684\u59ff\u6001\u8bc6\u522b\uff0c\u5b9e\u73b0\u66f4\u81ea\u7136\u7684\u7528\u6237\u754c\u9762\u548c\u624b\u52bf\u63a7\u5236\u3002 \u673a\u5668\u4eba\u5b66 \uff1a\u7528\u4e8e\u673a\u5668\u4eba\u6a21\u4eff\u5b66\u4e60\u3001\u4eba\u673a\u534f\u4f5c\u4e2d\u7684\u4eba\u4f53\u59ff\u6001\u7406\u89e3\u548c\u9884\u6d4b\u3002 \u8fd0\u52a8\u5206\u6790\u4e0e\u751f\u7269\u529b\u5b66 \uff1a\u9ad8\u6548\u5206\u6790\u8fd0\u52a8\u5458\u52a8\u4f5c\u3001\u5eb7\u590d\u8bad\u7ec3\u4e2d\u7684\u59ff\u6001\u8bc4\u4f30\u3002 \u7535\u5f71\u3001\u52a8\u753b\u4e0e\u6e38\u620f \uff1a\u5feb\u901f\u751f\u6210\u548c\u7f16\u8f913D\u89d2\u8272\u52a8\u753b\uff0c\u964d\u4f4e\u5236\u4f5c\u6210\u672c\u3002 \u89c6\u9891\u7406\u89e3\u4e0e\u52a8\u4f5c\u8bc6\u522b \uff1a\u4f5c\u4e3a\u9884\u5904\u7406\u6b65\u9aa4\uff0c\u63d0\u4f9b\u9ad8\u6548\u76843D\u59ff\u6001\u7279\u5f81\u3002 \u901a\u7528\u5e8f\u5217\u6a21\u578b\u6548\u7387\u4f18\u5316 \uff1a\u5176\u526a\u679d\u7b56\u7565\u53ef\u80fd\u9002\u7528\u4e8e\u5176\u4ed6\u9700\u8981\u5904\u7406\u957f\u5e8f\u5217\u6216\u9ad8\u7ef4token\u7684Transformer\u6216\u6269\u6563\u6a21\u578b\u3002 5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u7684\u5c40\u9650\u6027 (Limitations Inferred from the Abstract) \u5c3d\u7ba1\u6458\u8981\u5f3a\u8c03\u4e86\u663e\u8457\u7684\u6548\u7387\u63d0\u5347\u548cSOTA\u6027\u80fd\uff0c\u4f46\u4ecd\u53ef\u63a8\u65ad\u51fa\u4e00\u4e9b\u6f5c\u5728\u5c40\u9650\u6027\uff1a \u526a\u679d\u7b56\u7565\u7684\u590d\u6742\u6027 \uff1aHTP\u5305\u542b\u4e09\u4e2a\u9636\u6bb5\uff0c\u6d89\u53ca\u81ea\u9002\u5e94\u56fe\u6784\u5efa\u548c\u805a\u7c7b\u7b49\u64cd\u4f5c\uff0c\u8fd9\u53ef\u80fd\u589e\u52a0\u4e86\u6a21\u578b\u7684\u5b9e\u73b0\u548c\u8c03\u4f18\u590d\u6742\u6027\u3002\u8fd9\u4e9b\u989d\u5916\u6b65\u9aa4\u672c\u8eab\u4e5f\u53ef\u80fd\u5f15\u5165\u4e00\u5b9a\u7684\u8ba1\u7b97\u5f00\u9500\uff0c\u5c3d\u7ba1\u603b\u4f53\u4e0a\u662f\u51c0\u6536\u76ca\u3002 \u201c\u5173\u952e\u8fd0\u52a8\u52a8\u6001\u201d\u7684\u5b9a\u4e49\u4e0e\u9c81\u68d2\u6027 \uff1a\u6458\u8981\u63d0\u5230\u201c\u4fdd\u7559\u5173\u952e\u8fd0\u52a8\u52a8\u6001\u201d\uff0c\u4f46\u5982\u4f55\u7cbe\u786e\u5b9a\u4e49\u548c\u4fdd\u8bc1\u5728\u6240\u6709\u590d\u6742\u8fd0\u52a8\u573a\u666f\u4e0b\u90fd\u80fd\u6709\u6548\u4fdd\u7559\u201c\u5173\u952e\u201d\u4fe1\u606f\uff0c\u800c\u4e0d\u4f1a\u8bef\u526a\u679d\u6389\u7ec6\u5fae\u4f46\u91cd\u8981\u7684\u52a8\u4f5c\u7ec6\u8282\uff0c\u662f\u4e00\u4e2a\u6f5c\u5728\u7684\u6311\u6218\u3002 \u8d85\u53c2\u6570\u654f\u611f\u6027 \uff1a\u81ea\u9002\u5e94\u56fe\u7684\u6784\u5efa\u3001\u805a\u7c7b\u7b97\u6cd5\u7684\u9009\u62e9\u548c\u53c2\u6570\u8bbe\u7f6e\uff0c\u4ee5\u53ca\u5404\u9636\u6bb5\u526a\u679d\u6bd4\u4f8b\u7b49\uff0c\u90fd\u53ef\u80fd\u5f15\u5165\u9700\u8981\u4ed4\u7ec6\u8c03\u4f18\u7684\u8d85\u53c2\u6570\uff0c\u5f71\u54cd\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002 \u5bf9\u7279\u5b9a\u6570\u636e\u96c6\u7684\u4f9d\u8d56 \uff1a\u5b9e\u9a8c\u7ed3\u679c\u5728Human3.6M\u548cMPI-INF-3DHP\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u8fd9\u4e9b\u662f\u6807\u51c6\u6570\u636e\u96c6\uff0c\u4f46\u5728\u66f4\u201c\u91ce\u5916\u201d\uff08in-the-wild\uff09\u7684\u590d\u6742\u73af\u5883\u3001\u906e\u6321\u3001\u591a\u6837\u5316\u670d\u88c5\u548c\u5149\u7167\u6761\u4ef6\u4e0b\uff0c\u526a\u679d\u7b56\u7565\u7684\u6709\u6548\u6027\u53ef\u80fd\u9700\u8981\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u3002 \u672a\u63d0\u53ca\u5176\u4ed6\u6311\u6218 \uff1a\u6458\u8981\u4e3b\u8981\u5173\u6ce8\u6548\u7387\u95ee\u9898\uff0c\u5e76\u672a\u63d0\u53ca\u5bf93D\u59ff\u6001\u4f30\u8ba1\u4e2d\u5176\u4ed6\u5e38\u89c1\u6311\u6218\uff08\u5982\u4e25\u91cd\u906e\u6321\u3001\u591a\u89c6\u89d2\u9c81\u68d2\u6027\u3001\u4e0d\u540c\u4f53\u578b\u548c\u670d\u88c5\u7684\u6cdb\u5316\u80fd\u529b\uff09\u7684\u76f4\u63a5\u6539\u8fdb\u3002 Key Findings: In this paper, we propose an Efficient Diffusion-Based 3D Human Pose Estimation framework with a Hierarchical Temporal Pruning (HTP) strategy, which dynamically prunes redundant pose tokens across both frame and semantic levels while preserving critical motion dynamics. Experiments on Human3.6M and MPI-INF-3DHP show that HTP reduces training MACs by 38.5\\%, inference MACs by 56.8\\%, and improves inference speed by an average of 81.1\\% compared to prior diffusion-based methods, while achieving state-of-the-art performance. Links: PDF arXiv DriveQA: Passing the Driving Knowledge Test Authors: Maolin Wei, Wanzhou Liu, Eshed Ohn-Bar Published: 2025-08-29 Categories: cs.CV Abstract: If a Large Language Model (LLM) were to take a driving knowledge test today, would it pass? Beyond standard spatial and visual question-answering (QA) tasks on current autonomous driving benchmarks, driving knowledge tests require a complete understanding of all traffic rules, signage, and right-of-way principles. To pass this test, human drivers must discern various edge cases that rarely appear in real-world datasets. In this work, we present DriveQA, an extensive open-source text and vision-based benchmark that exhaustively covers traffic regulations and scenarios. Through our experiments using DriveQA, we show that (1) state-of-the-art LLMs and Multimodal LLMs (MLLMs) perform well on basic traffic rules but exhibit significant weaknesses in numerical reasoning and complex right-of-way scenarios, traffic sign variations, and spatial layouts, (2) fine-tuning on DriveQA improves accuracy across multiple categories, particularly in regulatory sign recognition and intersection decision-making, (3) controlled variations in DriveQA-V provide insights into model sensitivity to environmental factors such as lighting, perspective, distance, and weather conditions, and (4) pretraining on DriveQA enhances downstream driving task performance, leading to improved results on real-world datasets such as nuScenes and BDD, while also demonstrating that models can internalize text and synthetic traffic knowledge to generalize effectively across downstream QA tasks. Analysis: \u4f5c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u4e13\u5bb6\uff0c\u6211\u5bf9\u8fd9\u7bc7\u540d\u4e3a\u201cDriveQA: Passing the Driving Knowledge Test\u201d\u7684\u8bba\u6587\u6458\u8981\u8fdb\u884c\u5982\u4e0b\u5206\u6790\uff1a 1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u660e\u603b\u7ed3 (2-3\u53e5\u8bdd) \u672c\u6587\u63d0\u51fa\u4e86DriveQA\uff0c\u4e00\u4e2a\u5168\u9762\u7684\u5f00\u6e90\u6587\u672c\u548c\u89c6\u89c9\u57fa\u51c6\u6d4b\u8bd5\uff0c\u65e8\u5728\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u591a\u6a21\u6001LLMs\uff08MLLMs\uff09\u5bf9\u4ea4\u901a\u89c4\u5219\u3001\u6807\u5fd7\u548c\u8def\u6743\u539f\u5219\u7684\u5b8c\u6574\u7406\u89e3\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u73b0\u6709\u6a21\u578b\u5728\u590d\u6742\u573a\u666f\u548c\u6570\u503c\u63a8\u7406\u65b9\u9762\u5b58\u5728\u663e\u8457\u4e0d\u8db3\uff0c\u4f46\u901a\u8fc7\u5728DriveQA\u4e0a\u8fdb\u884c\u5fae\u8c03\u548c\u9884\u8bad\u7ec3\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u6a21\u578b\u5728\u9a7e\u9a76\u77e5\u8bc6\u6d4b\u8bd5\u548c\u4e0b\u6e38\u771f\u5b9e\u4e16\u754c\u9a7e\u9a76\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002 2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u8bba\u65b9\u6cd5 \u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u6784\u5efa\u4e86DriveQA\u8fd9\u4e00\u5168\u9762\u7684\u3001\u5f00\u6e90\u7684\u6587\u672c\u4e0e\u89c6\u89c9\u57fa\u51c6\u6d4b\u8bd5\u3002\u5b83\u8d85\u8d8a\u4e86\u73b0\u6709\u81ea\u52a8\u9a7e\u9a76\u57fa\u51c6\u7684\u5c40\u9650\uff0c\u901a\u8fc7\u201c\u7a77\u5c3d\u5f0f\u201d\u5730\u8986\u76d6\u4ea4\u901a\u6cd5\u89c4\u3001\u573a\u666f\u548c\u73b0\u5b9e\u4e16\u754c\u6570\u636e\u4e2d\u7f55\u89c1\u7684\u201c\u8fb9\u7f18\u6848\u4f8b\u201d\uff0c\u65e8\u5728\u5168\u9762\u8bc4\u4f30\u6a21\u578b\u5bf9\u9a7e\u9a76\u77e5\u8bc6\u7684\u6df1\u5c42\u7406\u89e3\u3002\u6b64\u5916\uff0cDriveQA-V\u901a\u8fc7\u5f15\u5165\u53d7\u63a7\u7684\u73af\u5883\u56e0\u7d20\uff08\u5982\u5149\u7167\u3001\u89c6\u89d2\u3001\u8ddd\u79bb\u3001\u5929\u6c14\uff09\u53d8\u5316\uff0c\u4e3a\u6df1\u5165\u5206\u6790\u6a21\u578b\u5bf9\u8fd9\u4e9b\u56e0\u7d20\u7684\u654f\u611f\u6027\u63d0\u4f9b\u4e86\u72ec\u7279\u7684\u65b9\u6cd5\uff0c\u8fd9\u5bf9\u4e8e\u7406\u89e3\u6a21\u578b\u5728\u4e0d\u540c\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6027\u81f3\u5173\u91cd\u8981\u3002 3. \u5bf9\u8be5\u9886\u57df\u7684\u6f5c\u5728\u5f71\u54cd \u8be5\u7814\u7a76\u5bf9\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u5177\u6709\u591a\u65b9\u9762\u6f5c\u5728\u5f71\u54cd\u3002\u9996\u5148\uff0c\u5b83\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4ece\u7eaf\u7cb9\u7684\u611f\u77e5\u548c\u9884\u6d4b\u8fc8\u5411\u57fa\u4e8e\u89c4\u5219\u7684\u201c\u7406\u89e3\u201d\u548c\u201c\u63a8\u7406\u201d\u63d0\u4f9b\u4e86\u5173\u952e\u5de5\u5177\u548c\u65b9\u5411\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002\u5176\u6b21\uff0cDriveQA\u4f5c\u4e3a\u4e00\u4e2a\u6781\u5177\u6311\u6218\u6027\u7684\u57fa\u51c6\uff0c\u5c06\u63a8\u52a8LLMs\u548cMLLMs\u5728\u590d\u6742\u63a8\u7406\u3001\u6570\u503c\u7406\u89e3\u548c\u591a\u6a21\u6001\u4fe1\u606f\u878d\u5408\u65b9\u9762\u7684\u80fd\u529b\u8fb9\u754c\u3002\u6700\u540e\uff0c\u901a\u8fc7\u5c55\u793a\u5408\u6210\u6570\u636e\u548c\u6587\u672c\u77e5\u8bc6\u5bf9\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u672a\u6765\u9ad8\u6548\u8bad\u7ec3\u548c\u90e8\u7f72\u667a\u80fd\u9a7e\u9a76\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u7684\u8303\u5f0f\uff0c\u5c24\u5176\u662f\u5728\u96be\u4ee5\u83b7\u53d6\u5927\u91cf\u771f\u5b9e\u4e16\u754c\u8fb9\u7f18\u6848\u4f8b\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u3002 4. \u53ef\u80fd\u53d7\u76ca\u4e8e\u8fd9\u9879\u7814\u7a76\u7684\u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 \u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5f00\u53d1\u4e0e\u6d4b\u8bd5\uff1a \u76f4\u63a5\u5e94\u7528\uff0c\u7528\u4e8e\u8bad\u7ec3\u548c\u8bc4\u4f30\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u51b3\u7b56\u6a21\u5757\uff0c\u7279\u522b\u662f\u5176\u5bf9\u4ea4\u901a\u89c4\u5219\u7684\u7406\u89e3\u548c\u590d\u6742\u573a\u666f\u4e0b\u7684\u63a8\u7406\u80fd\u529b\u3002 \u667a\u80fd\u4ea4\u901a\u7cfb\u7edf (ITS)\uff1a \u8f85\u52a9\u4ea4\u901a\u6d41\u7ba1\u7406\u3001\u4e8b\u6545\u9884\u9632\u548c\u667a\u80fd\u4fe1\u53f7\u63a7\u5236\uff0c\u901a\u8fc7\u66f4\u667a\u80fd\u5730\u7406\u89e3\u4ea4\u901a\u89c4\u5219\u6765\u4f18\u5316\u7cfb\u7edf\u3002 \u673a\u5668\u4eba\u5b66\uff1a \u4efb\u4f55\u9700\u8981\u5728\u590d\u6742\u3001\u89c4\u5219\u9a71\u52a8\u73af\u5883\u4e2d\u64cd\u4f5c\u7684\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u4f8b\u5982\u7269\u6d41\u673a\u5668\u4eba\u6216\u670d\u52a1\u673a\u5668\u4eba\uff0c\u53ef\u4ee5\u501f\u9274\u5176\u89c4\u5219\u7406\u89e3\u548c\u51b3\u7b56\u6846\u67b6\u3002 AI\u5b89\u5168\u4e0e\u53ef\u89e3\u91ca\u6027 (XAI)\uff1a \u6df1\u5165\u7406\u89e3\u6a21\u578b\u5982\u4f55\u57fa\u4e8e\u4ea4\u901a\u89c4\u5219\u505a\u51fa\u51b3\u7b56\uff0c\u63d0\u9ad8\u51b3\u7b56\u7684\u900f\u660e\u5ea6\u548c\u53ef\u4fe1\u5ea6\uff0c\u8fd9\u5728\u5b89\u5168\u5173\u952e\u9886\u57df\u5c24\u4e3a\u91cd\u8981\u3002 \u9a7e\u9a76\u5458\u57f9\u8bad\u4e0e\u6559\u80b2\uff1a \u4f5c\u4e3a\u8f85\u52a9\u5de5\u5177\uff0c\u5e2e\u52a9\u65b0\u9a7e\u9a76\u5458\u7406\u89e3\u590d\u6742\u7684\u4ea4\u901a\u89c4\u5219\u548c\u8fb9\u7f18\u60c5\u51b5\uff0c\u751a\u81f3\u53ef\u4ee5\u7528\u4e8e\u5f00\u53d1\u667a\u80fd\u9a7e\u9a76\u6a21\u62df\u5668\u3002 \u591a\u6a21\u6001LLMs\u7814\u7a76\uff1a \u4e3a\u63d0\u5347LLMs\u548cMLLMs\u5728\u89c6\u89c9\u7406\u89e3\u3001\u5e38\u8bc6\u63a8\u7406\u548c\u89c4\u5219\u9075\u5faa\u65b9\u9762\u7684\u80fd\u529b\u63d0\u4f9b\u65b0\u7684\u7814\u7a76\u65b9\u5411\u548c\u8bc4\u4f30\u6807\u51c6\u3002 5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u51fa\u7684\u5c40\u9650\u6027 \u5408\u6210\u6570\u636e\u4e0e\u771f\u5b9e\u4e16\u754c\u7684\u5dee\u8ddd\uff1a \u5c3d\u7ba1\u6458\u8981\u6307\u51fa\u6a21\u578b\u53ef\u4ee5\u5185\u5316\u6587\u672c\u548c\u5408\u6210\u4ea4\u901a\u77e5\u8bc6\u5e76\u6cdb\u5316\u5230\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\uff0c\u4f46DriveQA\u672c\u8eab\u662f\u57fa\u4e8e\u201c\u5408\u6210\u4ea4\u901a\u77e5\u8bc6\u201d\u6784\u5efa\u7684\u3002\u5408\u6210\u6570\u636e\u5728\u8986\u76d6\u8fb9\u7f18\u6848\u4f8b\u65b9\u9762\u6709\u4f18\u52bf\uff0c\u4f46\u5176\u4e0e\u771f\u5b9e\u4e16\u754c\u6570\u636e\u7684\u5206\u5e03\u5dee\u5f02\u3001\u566a\u58f0\u548c\u4e0d\u53ef\u9884\u6d4b\u6027\u53ef\u80fd\u4ecd\u662f\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u7684\u6f5c\u5728\u9650\u5236\u3002 \u4fa7\u91cd\u77e5\u8bc6\u7406\u89e3\u800c\u975e\u5b9e\u65f6\u51b3\u7b56\u4e0e\u63a7\u5236\uff1a DriveQA\u4e3b\u8981\u662f\u4e00\u4e2a\u201c\u9a7e\u9a76\u77e5\u8bc6\u6d4b\u8bd5\u201d\uff0c\u8bc4\u4f30\u6a21\u578b\u5bf9\u89c4\u5219\u7684\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\u3002\u5b83\u4e0d\u76f4\u63a5\u8bc4\u4f30\u6a21\u578b\u5728\u5b9e\u65f6\u3001\u52a8\u6001\u3001\u9ad8\u538b\u7684\u771f\u5b9e\u9a7e\u9a76\u73af\u5883\u4e2d\u7684\u611f\u77e5\u3001\u9884\u6d4b\u548c\u63a7\u5236\u80fd\u529b\uff0c\u8fd9\u9700\u8981\u66f4\u590d\u6742\u7684\u7aef\u5230\u7aef\u7cfb\u7edf\u3002 \u7279\u5b9a\u6311\u6218\u7684\u6301\u7eed\u5b58\u5728\uff1a \u6458\u8981\u660e\u786e\u6307\u51fa\uff0c\u5373\u4f7f\u662fSOTA\u6a21\u578b\uff0c\u5728\u201c\u6570\u503c\u63a8\u7406\u3001\u590d\u6742\u8def\u6743\u573a\u666f\u3001\u4ea4\u901a\u6807\u5fd7\u53d8\u4f53\u548c\u7a7a\u95f4\u5e03\u5c40\u201d\u65b9\u9762\u4ecd\u5b58\u5728\u663e\u8457\u5f31\u70b9\u3002\u8fd9\u8868\u660e\u8fd9\u4e9b\u662f\u6781\u5176\u56f0\u96be\u7684\u95ee\u9898\uff0cDriveQA\u867d\u7136\u63d0\u4f9b\u4e86\u8bc4\u4f30\u5de5\u5177\uff0c\u4f46\u89e3\u51b3\u8fd9\u4e9b\u6df1\u5c42\u6311\u6218\u4ecd\u9700\u8fdb\u4e00\u6b65\u7684\u7814\u7a76\u3002 \u201c\u7a77\u5c3d\u6027\u201d\u7684\u8303\u56f4\uff1a \u5c3d\u7ba1\u58f0\u79f0\u201c\u7a77\u5c3d\u5f0f\u5730\u8986\u76d6\u4ea4\u901a\u6cd5\u89c4\u548c\u573a\u666f\u201d\uff0c\u4f46\u73b0\u5b9e\u4e16\u754c\u7684\u4ea4\u901a\u89c4\u5219\u548c\u8fb9\u7f18\u6848\u4f8b\u662f\u6781\u5176\u5e9e\u5927\u548c\u590d\u6742\u7684\uff08\u4f8b\u5982\uff0c\u4e0d\u540c\u56fd\u5bb6/\u5730\u533a\u7684\u5177\u4f53\u6cd5\u89c4\u5dee\u5f02\u3001\u5404\u79cd\u975e\u6807\u51c6\u60c5\u51b5\uff09\u3002\u6458\u8981\u5e76\u672a\u8be6\u7ec6\u8bf4\u660e\u5176\u8986\u76d6\u7684\u5e7f\u5ea6\u548c\u6df1\u5ea6\uff0c\u8fd9\u53ef\u80fd\u662f\u4e00\u4e2a\u6f5c\u5728\u7684\u5c40\u9650\u3002 Key Findings: In this work, we present DriveQA, an extensive open-source text and vision-based benchmark that exhaustively covers traffic regulations and scenarios. Through our experiments using DriveQA, we show that (1) state-of-the-art LLMs and Multimodal LLMs (MLLMs) perform well on basic traffic rules but exhibit significant weaknesses in numerical reasoning and complex right-of-way scenarios, traffic sign variations, and spatial layouts, (2) fine-tuning on DriveQA improves accuracy across multiple categories, particularly in regulatory sign recognition and intersection decision-making, (3) controlled variations in DriveQA-V provide insights into model sensitivity to environmental factors such as lighting, perspective, distance, and weather conditions, and (4) pretraining on DriveQA enhances downstream driving task performance, leading to improved results on real-world datasets such as nuScenes and BDD, while also demonstrating that models can internalize text and synthetic traffic knowledge to generalize effectively across downstream QA tasks. Links: PDF arXiv Domain Generalization in-the-Wild: Disentangling Classification from Domain-Aware Representations Authors: Ha Min Son, Zhe Zhao, Shahbaz Rezaei, Xin Liu Published: 2025-08-29 Categories: cs.CV, cs.LG Abstract: Evaluating domain generalization (DG) for foundational models like CLIP is challenging, as web-scale pretraining data potentially covers many existing benchmarks. Consequently, current DG evaluation may neither be sufficiently challenging nor adequately test genuinely unseen data scenarios. To better assess the performance of CLIP on DG in-the-wild, a scenario where CLIP encounters challenging unseen data, we consider two approaches: (1) evaluating on 33 diverse datasets with quantified out-of-distribution (OOD) scores after fine-tuning CLIP on ImageNet, and (2) using unlearning to make CLIP `forget' some domains as an approximation. We observe that CLIP's performance deteriorates significantly on more OOD datasets. To address this, we present CLIP-DCA (Disentangling Classification from enhanced domain Aware representations). Our approach is motivated by the observation that while standard domain invariance losses aim to make representations domain-invariant, this can be harmful to foundation models by forcing the discarding of domain-aware representations beneficial for generalization. We instead hypothesize that enhancing domain awareness is a prerequisite for effective domain-invariant classification in foundation models. CLIP-DCA identifies and enhances domain awareness within CLIP's encoders using a separate domain head and synthetically generated diverse domain data. Simultaneously, it encourages domain-invariant classification through disentanglement from the domain features. CLIP-DCA shows significant improvements within this challenging evaluation compared to existing methods, particularly on datasets that are more OOD. Analysis: \u4f5c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u4e13\u5bb6\uff0c\u6211\u5bf9\u8fd9\u7bc7\u8bba\u6587\u6458\u8981\u7684\u5206\u6790\u5982\u4e0b\uff1a \u8bba\u6587\u6458\u8981\u5206\u6790\uff1aDomain Generalization in-the-Wild: Disentangling Classification from Domain-Aware Representations 1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e (Main Contribution): \u672c\u6587\u9488\u5bf9\u57fa\u7840\u6a21\u578b\uff08\u5982CLIP\uff09\u5728\u771f\u5b9e\u4e16\u754c\u57df\u6cdb\u5316\uff08DG in-the-wild\uff09\u573a\u666f\u4e0b\u7684\u8bc4\u4f30\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u4e00\u5957\u65b0\u7684\u8bc4\u4f30\u8303\u5f0f\uff0c\u5e76\u53d1\u73b0CLIP\u5728\u6b64\u7c7b\u9ad8\u5ea6\u57df\u5916\uff08OOD\uff09\u6570\u636e\u4e0a\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002\u4e3a\u89e3\u51b3\u6b64\u95ee\u9898\uff0c\u8bba\u6587\u5f15\u5165\u4e86CLIP-DCA\u65b9\u6cd5\uff0c\u901a\u8fc7\u589e\u5f3a\u57df\u611f\u77e5\u8868\u793a\u5e76\u5c06\u5176\u4e0e\u5206\u7c7b\u4efb\u52a1\u89e3\u8026\uff0c\u663e\u8457\u63d0\u5347\u4e86\u57fa\u7840\u6a21\u578b\u5728\u590d\u6742\u57df\u6cdb\u5316\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002 2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5 (Key Innovation or Methodological Approach): \u8be5\u8bba\u6587\u7684\u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u5176\u5bf9\u57fa\u7840\u6a21\u578b\u57df\u6cdb\u5316\u5b66\u4e60\u8303\u5f0f\u7684\u6df1\u523b\u8f6c\u53d8\u3002\u4e0e\u4f20\u7edf\u57df\u4e0d\u53d8\u6027\u5b66\u4e60\u65b9\u6cd5\u76f4\u63a5\u5f3a\u5236\u8868\u793a\u57df\u4e0d\u53d8\u6027\uff08\u53ef\u80fd\u5bfc\u81f4\u6709\u7528\u57df\u4fe1\u606f\u7684\u4e22\u5931\uff09\u4e0d\u540c\uff0cCLIP-DCA\u63d0\u51fa\u5e76\u9a8c\u8bc1\u4e86\u201c\u589e\u5f3a\u57df\u611f\u77e5\u80fd\u529b\u662f\u5b9e\u73b0\u6709\u6548\u57df\u4e0d\u53d8\u5206\u7c7b\u7684\u5148\u51b3\u6761\u4ef6\u201d\u8fd9\u4e00\u5047\u8bbe\u3002\u5176\u65b9\u6cd5\u8bba\u901a\u8fc7\u5f15\u5165\u4e00\u4e2a\u72ec\u7acb\u7684\u57df\u5934\u548c\u5229\u7528\u5408\u6210\u57df\u6570\u636e\u6765\u4e3b\u52a8\u8bc6\u522b\u5e76\u589e\u5f3aCLIP\u7f16\u7801\u5668\u4e2d\u7684\u57df\u611f\u77e5\u8868\u793a\uff0c\u540c\u65f6\u5de7\u5999\u5730\u901a\u8fc7\u89e3\u8026\u673a\u5236\u786e\u4fdd\u6700\u7ec8\u7684\u5206\u7c7b\u4efb\u52a1\u80fd\u591f\u4ece\u8fd9\u4e9b\u589e\u5f3a\u7684\u57df\u7279\u5f81\u4e2d\u89e3\u8026\u51fa\u6765\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u9c81\u68d2\u7684\u57df\u4e0d\u53d8\u5206\u7c7b\u3002 3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd (Potential Impact on the Field): \u8be5\u7814\u7a76\u6709\u671b\u5bf9\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u4ea7\u751f\u591a\u65b9\u9762\u5f71\u54cd\u3002\u9996\u5148\uff0c\u5b83\u63d0\u51fa\u4e86\u66f4\u4e25\u8c28\u548c\u5177\u6311\u6218\u6027\u7684\u57fa\u7840\u6a21\u578b\u57df\u6cdb\u5316\u8bc4\u4f30\u8303\u5f0f\uff0c\u4fc3\u4f7f\u9886\u57df\u91cd\u65b0\u601d\u8003\u73b0\u6709\u57fa\u51c6\u7684\u6709\u6548\u6027\u3002\u5176\u6b21\uff0cCLIP-DCA\u7684\u6838\u5fc3\u601d\u60f3\u2014\u2014\u5373\u5148\u589e\u5f3a\u57df\u611f\u77e5\u518d\u89e3\u8026\u5206\u7c7b\u2014\u2014\u4e3a\u672a\u6765\u8bbe\u8ba1\u9488\u5bf9\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\uff08\u5982CLIP\uff09\u7684\u57df\u6cdb\u5316\u7b97\u6cd5\u5f00\u8f9f\u4e86\u65b0\u8def\u5f84\uff0c\u6311\u6218\u4e86\u4f20\u7edf\u57df\u4e0d\u53d8\u6027\u5b66\u4e60\u7684\u5047\u8bbe\u3002\u8fd9\u6709\u671b\u663e\u8457\u63d0\u5347\u57fa\u7840\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u3001\u9ad8\u5ea6\u57df\u5916\uff08OOD\uff09\u573a\u666f\u4e0b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4f7f\u5176\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u66f4\u53ef\u9760\u3002 4. \u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 (Related Areas or Applications that Might Benefit from this Research): \u672c\u7814\u7a76\u7684\u6210\u679c\u5c06\u5bf9\u5e7f\u6cdb\u7684\u5b9e\u9645\u5e94\u7528\u9886\u57df\u4ea7\u751f\u79ef\u6781\u5f71\u54cd\uff0c\u7279\u522b\u662f\u90a3\u4e9b\u5bf9\u6a21\u578b\u5728\u672a\u89c1\u6570\u636e\u4e0a\u6cdb\u5316\u80fd\u529b\u6709\u4e25\u683c\u8981\u6c42\u7684\u573a\u666f\uff1a * \u533b\u7597\u5f71\u50cf\u8bca\u65ad\uff1a \u4e0d\u540c\u533b\u9662\u3001\u8bbe\u5907\u6216\u60a3\u8005\u7fa4\u4f53\u7684\u6570\u636e\u5206\u5e03\u5dee\u5f02\u5de8\u5927\uff0cDG\u80fd\u529b\u81f3\u5173\u91cd\u8981\u3002 * \u81ea\u52a8\u9a7e\u9a76\uff1a \u9762\u5bf9\u591a\u53d8\u7684\u5929\u6c14\u3001\u5149\u7167\u3001\u5730\u7406\u73af\u5883\u7b49\uff0c\u6a21\u578b\u9700\u5177\u5907\u5f3a\u5927\u7684\u57df\u6cdb\u5316\u80fd\u529b\u3002 * \u673a\u5668\u4eba\u89c6\u89c9\uff1a \u673a\u5668\u4eba\u9700\u8981\u5728\u5404\u79cd\u672a\u77e5\u7684\u5ba4\u5185\u5916\u73af\u5883\u4e2d\u6267\u884c\u4efb\u52a1\u3002 * \u9065\u611f\u56fe\u50cf\u5206\u6790\uff1a \u4e0d\u540c\u5730\u7406\u533a\u57df\u3001\u4f20\u611f\u5668\u6216\u65f6\u95f4\u70b9\u7684\u6570\u636e\u5206\u5e03\u5dee\u5f02\u3002 * \u5de5\u4e1a\u7f3a\u9677\u68c0\u6d4b\uff1a \u751f\u4ea7\u7ebf\u73af\u5883\u53d8\u5316\u53ef\u80fd\u5bfc\u81f4\u6570\u636e\u5206\u5e03\u504f\u79fb\u3002 \u6b64\u5916\uff0c\u5bf9\u4e8e\u4efb\u4f55\u9700\u8981\u5c06\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\u90e8\u7f72\u5230\u4e0e\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u5b58\u5728\u663e\u8457\u5dee\u5f02\u7684\u771f\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u7684\u5e94\u7528\uff0c\u672c\u7814\u7a76\u90fd\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u6307\u5bfc\u548c\u89e3\u51b3\u65b9\u6848\u3002 5. \u6f5c\u5728\u5c40\u9650\u6027 (Limitations that Can Be Inferred from the Abstract): \u5c3d\u7ba1\u6458\u8981\u5c55\u793a\u4e86\u4ee4\u4eba\u9f13\u821e\u7684\u7ed3\u679c\uff0c\u4f46\u4ecd\u53ef\u63a8\u65ad\u51fa\u4e00\u4e9b\u6f5c\u5728\u5c40\u9650\u6027\uff1a * \u5408\u6210\u57df\u6570\u636e\u7684\u6709\u6548\u6027\u4e0e\u6cdb\u5316\u6027\uff1a CLIP-DCA\u4f9d\u8d56\u4e8e\u201c\u5408\u6210\u751f\u6210\u591a\u6837\u57df\u6570\u636e\u201d\u6765\u589e\u5f3a\u57df\u611f\u77e5\u3002\u5408\u6210\u6570\u636e\u80fd\u5426\u5145\u5206\u6355\u6349\u771f\u5b9e\u4e16\u754c\u4e2d\u590d\u6742\u4e14\u672a\u77e5\u7684\u57df\u504f\u79fb\u6a21\u5f0f\uff0c\u4ee5\u53ca\u5176\u8d28\u91cf\u548c\u591a\u6837\u6027\u5bf9\u6700\u7ec8\u6cdb\u5316\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u662f\u4e00\u4e2a\u5173\u952e\u95ee\u9898\u3002\u8fc7\u4e8e\u7b80\u5355\u7684\u5408\u6210\u53ef\u80fd\u65e0\u6cd5\u5e94\u5bf9\u6781\u7aefOOD\u573a\u666f\u3002 * \u201c\u9057\u5fd8\u201d\u673a\u5236\u4f5c\u4e3a\u8fd1\u4f3c\u7684\u5c40\u9650\u6027\uff1a \u8bba\u6587\u4f7f\u7528\u201c\u9057\u5fd8\u201d\u6765\u8fd1\u4f3c\u6a21\u62df\u6a21\u578b\u4ece\u672a\u89c1\u8fc7\u67d0\u4e9b\u57df\u7684\u573a\u666f\u3002\u8fd9\u79cd\u8fd1\u4f3c\u65b9\u6cd5\u4e0e\u771f\u6b63\u610f\u4e49\u4e0a\u7684\u201c\u4ece\u672a\u89c1\u8fc7\u201d\u4e4b\u95f4\u53ef\u80fd\u5b58\u5728\u5dee\u5f02\uff0c\u5176\u6709\u6548\u6027\u548c\u51c6\u786e\u6027\u9700\u8981\u66f4\u6df1\u5165\u7684\u9a8c\u8bc1\u3002 * \u8bc4\u4f30\u573a\u666f\u7684\u4ee3\u8868\u6027\uff1a \u5c3d\u7ba1\u4f7f\u7528\u4e8633\u4e2a\u6570\u636e\u96c6\u548cOOD\u5206\u6570\uff0c\u4f46\u201cin-the-wild\u201d\u662f\u4e00\u4e2a\u6781\u5176\u5bbd\u6cdb\u7684\u6982\u5ff5\u3002\u8fd9\u4e9b\u8bc4\u4f30\u662f\u5426\u80fd\u5b8c\u5168\u4ee3\u8868\u6240\u6709\u771f\u5b9e\u4e16\u754c\u4e2d\u53ef\u80fd\u9047\u5230\u7684\u3001\u5177\u6709\u6311\u6218\u6027\u7684\u672a\u89c1\u57df\u573a\u666f\uff0c\u4ecd\u6709\u5f85\u5546\u69b7\u3002 * \u65b9\u6cd5\u590d\u6742\u6027\u4e0e\u8ba1\u7b97\u6210\u672c\uff1a \u5f15\u5165\u72ec\u7acb\u7684\u57df\u5934\u548c\u89e3\u8026\u673a\u5236\u53ef\u80fd\u4f1a\u589e\u52a0\u6a21\u578b\u7684\u53c2\u6570\u91cf\u3001\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u8bad\u7ec3\u65f6\u95f4\uff0c\u8fd9\u5728\u8d44\u6e90\u53d7\u9650\u7684\u573a\u666f\u4e0b\u53ef\u80fd\u662f\u4e00\u4e2a\u8003\u91cf\u56e0\u7d20\u3002 * \u5bf9\u57fa\u7840\u6a21\u578b\u7684\u901a\u7528\u6027\uff1a \u8bba\u6587\u4e3b\u8981\u5173\u6ce8CLIP\u3002CLIP-DCA\u7684\u6709\u6548\u6027\u662f\u5426\u80fd\u76f4\u63a5\u6cdb\u5316\u5230\u5176\u4ed6\u7c7b\u578b\u7684\u57fa\u7840\u6a21\u578b\uff08\u4f8b\u5982\uff0c\u7eaf\u89c6\u89c9Transformer\u6216\u4e0d\u540c\u6a21\u6001\u7684\u9884\u8bad\u7ec3\u6a21\u578b\uff09\u4ecd\u9700\u8fdb\u4e00\u6b65\u63a2\u7d22\u3002 Key Findings: To address this, we present CLIP-DCA (Disentangling Classification from enhanced domain Aware representations). Our approach is motivated by the observation that while standard domain invariance losses aim to make representations domain-invariant, this can be harmful to foundation models by forcing the discarding of domain-aware representations beneficial for generalization. Links: PDF arXiv UItron: Foundational GUI Agent with Advanced Perception and Planning Authors: Zhixiong Zeng, Jing Huang, Liming Zheng, Wenkang Han, Yufeng Zhong, Lei Chen, Longrong Yang, Yingjie Chu, Yuzhi He, Lin Ma Published: 2025-08-29 Categories: cs.CV Abstract: GUI agent aims to enable automated operations on Mobile/PC devices, which is an important task toward achieving artificial general intelligence. The rapid advancement of VLMs accelerates the development of GUI agents, owing to their powerful capabilities in visual understanding and task planning. However, building a GUI agent remains a challenging task due to the scarcity of operation trajectories, the availability of interactive infrastructure, and the limitation of initial capabilities in foundation models. In this work, we introduce UItron, an open-source foundational model for automatic GUI agents, featuring advanced GUI perception, grounding, and planning capabilities. UItron highlights the necessity of systemic data engineering and interactive infrastructure as foundational components for advancing GUI agent development. It not only systematically studies a series of data engineering strategies to enhance training effects, but also establishes an interactive environment connecting both Mobile and PC devices. In training, UItron adopts supervised finetuning over perception and planning tasks in various GUI scenarios, and then develop a curriculum reinforcement learning framework to enable complex reasoning and exploration for online environments. As a result, UItron achieves superior performance in benchmarks of GUI perception, grounding, and planning. In particular, UItron highlights the interaction proficiency with top-tier Chinese mobile APPs, as we identified a general lack of Chinese capabilities even in state-of-the-art solutions. To this end, we manually collect over one million steps of operation trajectories across the top 100 most popular apps, and build the offline and online agent evaluation environments. Experimental results demonstrate that UItron achieves significant progress in Chinese app scenarios, propelling GUI agents one step closer to real-world application. Analysis: UItron\u8fd9\u7bc7\u8bba\u6587\u6458\u8981\u5c55\u793a\u4e86\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\uff0c\u7279\u522b\u662fGUI\uff08\u56fe\u5f62\u7528\u6237\u754c\u9762\uff09\u81ea\u52a8\u5316\u4ee3\u7406\u65b9\u9762\u7684\u91cd\u8981\u8fdb\u5c55\u3002\u4ee5\u4e0b\u662f\u8be6\u7ec6\u5206\u6790\uff1a 1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u6d01\u603b\u7ed3 (2-3\u53e5\u8bdd) UItron\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u3001\u57fa\u7840\u6027\u7684GUI\u81ea\u52a8\u5316\u4ee3\u7406\u6a21\u578b\uff0c\u5b83\u901a\u8fc7\u5148\u8fdb\u7684GUI\u611f\u77e5\u3001\u5b9a\u4f4d\u548c\u89c4\u5212\u80fd\u529b\uff0c\u65e8\u5728\u5b9e\u73b0\u79fb\u52a8\u548cPC\u8bbe\u5907\u7684\u81ea\u52a8\u5316\u64cd\u4f5c\u3002\u8be5\u5de5\u4f5c\u5f3a\u8c03\u4e86\u7cfb\u7edf\u6027\u6570\u636e\u5de5\u7a0b\u548c\u4ea4\u4e92\u5f0f\u57fa\u7840\u8bbe\u65bd\u7684\u91cd\u8981\u6027\uff0c\u5e76\u7ed3\u5408\u76d1\u7763\u5fae\u8c03\u4e0e\u8bfe\u7a0b\u5f3a\u5316\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86GUI\u4ee3\u7406\u5728\u590d\u6742\u5728\u7ebf\u73af\u5883\u4e2d\u7684\u8868\u73b0\u3002\u7279\u522b\u503c\u5f97\u4e00\u63d0\u7684\u662f\uff0cUItron\u901a\u8fc7\u5927\u89c4\u6a21\u624b\u52a8\u6536\u96c6\u7684\u4e2d\u6587\u5e94\u7528\u64cd\u4f5c\u8f68\u8ff9\u6570\u636e\uff0c\u586b\u8865\u4e86\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u5728\u4e2d\u6587\u5e94\u7528\u573a\u666f\u4e0b\u7684\u80fd\u529b\u7a7a\u767d\uff0c\u63a8\u52a8\u4e86GUI\u4ee3\u7406\u5411\u5b9e\u9645\u5e94\u7528\u8fc8\u8fdb\u3002 2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u8bba UItron\u7684\u5173\u952e\u521b\u65b0\u548c\u65b9\u6cd5\u8bba\u4f53\u73b0\u5728\u4ee5\u4e0b\u51e0\u4e2a\u65b9\u9762\uff1a \u7cfb\u7edf\u6027\u6570\u636e\u5de5\u7a0b\u4e0e\u4ea4\u4e92\u5f0f\u57fa\u7840\u8bbe\u65bd\uff1a \u8bba\u6587\u5f3a\u8c03\u5e76\u5b9e\u8df5\u4e86\u7cfb\u7edf\u6027\u7684\u6570\u636e\u5de5\u7a0b\u7b56\u7565\u6765\u589e\u5f3a\u8bad\u7ec3\u6548\u679c\uff0c\u5e76\u5efa\u7acb\u4e86\u4e00\u4e2a\u8fde\u63a5\u79fb\u52a8\u548cPC\u8bbe\u5907\u7684\u7edf\u4e00\u4ea4\u4e92\u73af\u5883\u3002\u8fd9\u4e3aGUI\u4ee3\u7406\u7684\u5f00\u53d1\u63d0\u4f9b\u4e86\u575a\u5b9e\u7684\u57fa\u7840\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u7a00\u7f3a\u548c\u57fa\u7840\u8bbe\u65bd\u4e0d\u8db3\u7684\u6311\u6218\u3002 \u6df7\u5408\u8bad\u7ec3\u8303\u5f0f\uff1a UItron\u91c7\u7528\u4e86\u5206\u9636\u6bb5\u7684\u8bad\u7ec3\u7b56\u7565\u3002\u9996\u5148\uff0c\u901a\u8fc7\u5728\u5404\u79cdGUI\u573a\u666f\u4e0b\u7684\u611f\u77e5\u548c\u89c4\u5212\u4efb\u52a1\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\uff0c\u4e3a\u6a21\u578b\u5960\u5b9a\u521d\u59cb\u80fd\u529b\u3002\u968f\u540e\uff0c\u5f15\u5165\u8bfe\u7a0b\u5f3a\u5316\u5b66\u4e60\uff08Curriculum Reinforcement Learning\uff09\u6846\u67b6\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u5728\u5728\u7ebf\u73af\u5883\u4e2d\u8fdb\u884c\u590d\u6742\u7684\u63a8\u7406\u548c\u63a2\u7d22\uff0c\u4ece\u800c\u5904\u7406\u66f4\u52a8\u6001\u548c\u5f00\u653e\u7684\u4efb\u52a1\u3002 \u5927\u89c4\u6a21\u4e2d\u6587\u5e94\u7528\u6570\u636e\u96c6\u4e0e\u80fd\u529b\u63d0\u5347\uff1a \u9488\u5bf9\u73b0\u6709SOTA\u89e3\u51b3\u65b9\u6848\u666e\u904d\u7f3a\u4e4f\u4e2d\u6587\u5e94\u7528\u80fd\u529b\u7684\u95ee\u9898\uff0cUItron\u624b\u52a8\u6536\u96c6\u4e86\u8d85\u8fc7\u4e00\u767e\u4e07\u6b65\u7684\u3001\u6db5\u76d6\u524d100\u4e2a\u6700\u53d7\u6b22\u8fce\u4e2d\u6587\u5e94\u7528\u7684\u771f\u5b9e\u64cd\u4f5c\u8f68\u8ff9\u6570\u636e\u3002\u8fd9\u4e0d\u4ec5\u6784\u5efa\u4e86\u79bb\u7ebf\u548c\u5728\u7ebf\u7684\u8bc4\u4f30\u73af\u5883\uff0c\u4e5f\u4f7f\u5f97UItron\u5728\u4e2d\u6587\u5e94\u7528\u573a\u666f\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u586b\u8865\u4e86\u4e00\u4e2a\u91cd\u8981\u7684\u5e02\u573a\u548c\u7814\u7a76\u7a7a\u767d\u3002 \u5f00\u6e90\u57fa\u7840\u6a21\u578b\uff1a \u4f5c\u4e3a\u201c\u5f00\u6e90\u57fa\u7840\u6a21\u578b\u201d\uff0cUItron\u65e8\u5728\u964d\u4f4e\u7814\u7a76\u95e8\u69db\uff0c\u4fc3\u8fdb\u6574\u4e2aGUI\u4ee3\u7406\u9886\u57df\u7684\u53d1\u5c55\u3002 3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd \u52a0\u901fAGI\u53d1\u5c55\uff1a GUI\u4ee3\u7406\u88ab\u8ba4\u4e3a\u662f\u5b9e\u73b0\u901a\u7528\u4eba\u5de5\u667a\u80fd\uff08AGI\uff09\u7684\u91cd\u8981\u4e00\u6b65\u3002UItron\u7684\u8fdb\u5c55\uff0c\u7279\u522b\u662f\u5176\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u89c4\u5212\u548c\u63a2\u7d22\u80fd\u529b\uff0c\u5c06\u76f4\u63a5\u63a8\u52a8AGI\u7814\u7a76\u3002 \u63d0\u5347GUI\u81ea\u52a8\u5316\u6c34\u5e73\uff1a UItron\u7684\u5148\u8fdb\u611f\u77e5\u3001\u5b9a\u4f4d\u548c\u89c4\u5212\u80fd\u529b\uff0c\u4ee5\u53ca\u5176\u5728\u4e2d\u6587\u5e94\u7528\u4e0a\u7684\u7a81\u7834\uff0c\u5c06\u663e\u8457\u63d0\u5347\u73b0\u6709GUI\u81ea\u52a8\u5316\u5de5\u5177\u7684\u6027\u80fd\u548c\u9002\u7528\u8303\u56f4\uff0c\u4f7f\u5176\u80fd\u5904\u7406\u66f4\u590d\u6742\u3001\u66f4\u771f\u5b9e\u7684\u4efb\u52a1\u3002 \u63a8\u52a8\u591a\u8bed\u8a00/\u591a\u6587\u5316GUI\u7814\u7a76\uff1a UItron\u5bf9\u4e2d\u6587\u5e94\u7528\u80fd\u529b\u7684\u5f3a\u8c03\u548c\u5b9e\u73b0\uff0c\u5c06\u6fc0\u52b1\u7814\u7a76\u8005\u5173\u6ce8\u5176\u4ed6\u975e\u82f1\u8bed\u8bed\u8a00\u548c\u6587\u5316\u80cc\u666f\u4e0b\u7684GUI\u4ee3\u7406\u5f00\u53d1\uff0c\u4fc3\u8fdb\u66f4\u5177\u666e\u9002\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002 \u63d0\u4f9b\u7814\u7a76\u57fa\u77f3\uff1a \u4f5c\u4e3a\u5f00\u6e90\u7684\u57fa\u7840\u6a21\u578b\uff0cUItron\u5c06\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e00\u4e2a\u5f3a\u5927\u7684\u57fa\u7ebf\u548c\u7814\u7a76\u5e73\u53f0\uff0c\u52a0\u901f\u65b0\u7b97\u6cd5\u3001\u65b0\u67b6\u6784\u7684\u9a8c\u8bc1\u548c\u8fed\u4ee3\u3002 \u4fc3\u8fdb\u6570\u636e\u5de5\u7a0b\u548c\u4ea4\u4e92\u73af\u5883\u7684\u91cd\u89c6\uff1a \u8bba\u6587\u660e\u786e\u6307\u51fa\u6570\u636e\u5de5\u7a0b\u548c\u4ea4\u4e92\u57fa\u7840\u8bbe\u65bd\u662f\u57fa\u7840\u7ec4\u4ef6\uff0c\u8fd9\u5c06\u4fc3\u4f7f\u66f4\u591a\u7814\u7a76\u8005\u548c\u5f00\u53d1\u8005\u6295\u5165\u8d44\u6e90\u6765\u6784\u5efa\u9ad8\u8d28\u91cf\u7684\u6570\u636e\u96c6\u548c\u4eff\u771f\u73af\u5883\u3002 4. \u53ef\u80fd\u53d7\u76ca\u7684\u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 \u8f6f\u4ef6\u6d4b\u8bd5\u4e0e\u8d28\u91cf\u4fdd\u8bc1 (QA)\uff1a \u81ea\u52a8\u5316UI\u6d4b\u8bd5\uff0c\u5c24\u5176\u662f\u5728\u79fb\u52a8\u5e94\u7528\u548c\u591a\u5e73\u53f0\u573a\u666f\u4e0b\uff0c\u53ef\u4ee5\u5927\u5e45\u63d0\u9ad8\u6548\u7387\u548c\u8986\u76d6\u7387\u3002 \u673a\u5668\u4eba\u6d41\u7a0b\u81ea\u52a8\u5316 (RPA)\uff1a \u5728\u4f01\u4e1a\u7ea7\u5e94\u7528\u4e2d\uff0cUItron\u53ef\u4ee5\u81ea\u52a8\u5316\u6267\u884c\u590d\u6742\u7684\u8de8\u5e94\u7528\u3001\u8de8\u8bbe\u5907\u4e1a\u52a1\u6d41\u7a0b\uff0c\u63d0\u9ad8\u5de5\u4f5c\u6548\u7387\u3002 \u8f85\u52a9\u6280\u672f\u4e0e\u65e0\u969c\u788d\u8bbf\u95ee\uff1a \u4e3a\u6b8b\u969c\u4eba\u58eb\u63d0\u4f9b\u66f4\u667a\u80fd\u3001\u66f4\u81ea\u4e3b\u7684\u754c\u9762\u64cd\u4f5c\u8f85\u52a9\uff0c\u63d0\u5347\u6570\u5b57\u751f\u6d3b\u7684\u4fbf\u5229\u6027\u3002 \u4e2a\u4ebaAI\u52a9\u624b\uff1a \u8d4b\u80fdAI\u52a9\u624b\u6267\u884c\u66f4\u590d\u6742\u7684\u3001\u6d89\u53ca\u591a\u6b65\u64cd\u4f5c\u548c\u8de8\u5e94\u7528\u7684\u6307\u4ee4\uff0c\u4f8b\u5982\u201c\u5e2e\u6211\u9884\u8ba2\u4e00\u5f20\u4ece\u4e0a\u6d77\u5230\u5317\u4eac\u7684\u673a\u7968\u201d\u3002 \u6570\u636e\u91c7\u96c6\u4e0e\u7f51\u7edc\u722c\u866b\uff1a \u66f4\u667a\u80fd\u5730\u4ece\u52a8\u6001\u548c\u590d\u6742\u7684GUI\u754c\u9762\u4e2d\u63d0\u53d6\u4fe1\u606f\u3002 \u8de8\u8bbe\u5907\u8ba1\u7b97\uff1a \u5b9e\u73b0\u4efb\u52a1\u5728\u624b\u673a\u548cPC\u4e4b\u95f4\u65e0\u7f1d\u5207\u6362\u548c\u81ea\u52a8\u5316\u6267\u884c\u3002 \u6559\u80b2\u4e0e\u57f9\u8bad\uff1a \u81ea\u52a8\u5316\u6f14\u793a\u8f6f\u4ef6\u64cd\u4f5c\u6d41\u7a0b\uff0c\u7528\u4e8e\u6559\u5b66\u6216\u7528\u6237\u57f9\u8bad\u3002 5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u7684\u5c40\u9650\u6027 \u6570\u636e\u6536\u96c6\u6210\u672c\u4e0e\u53ef\u6269\u5c55\u6027\uff1a \u6458\u8981\u4e2d\u63d0\u5230\u201c\u624b\u52a8\u6536\u96c6\u8d85\u8fc7\u4e00\u767e\u4e07\u6b65\u7684\u64cd\u4f5c\u8f68\u8ff9\u201d\uff0c\u8fd9\u8868\u660e\u6570\u636e\u6536\u96c6\u662f\u4e00\u4e2a\u52b3\u52a8\u5bc6\u96c6\u578b\u4e14\u6210\u672c\u9ad8\u6602\u7684\u8fc7\u7a0b\u3002\u867d\u7136\u5bf9\u4e8e\u7279\u5b9a\u9886\u57df\uff08\u5982\u4e2d\u6587\u5e94\u7528\uff09\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u4f46\u8981\u5c06\u5176\u6269\u5c55\u5230\u5168\u7403\u6240\u6709\u8bed\u8a00\u3001\u6240\u6709\u5e94\u7528\u6216\u66f4\u957f\u5c3e\u7684\u5e94\u7528\u573a\u666f\uff0c\u5176\u53ef\u6269\u5c55\u6027\u53ef\u80fd\u9762\u4e34\u6311\u6218\u3002 \u6cdb\u5316\u80fd\u529b\uff1a \u5c3d\u7ba1\u5728\u201c\u524d100\u4e2a\u6700\u53d7\u6b22\u8fce\u7684\u4e2d\u6587\u5e94\u7528\u201d\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5bf9\u4e8e\u4e0d\u5e38\u89c1\u3001\u8bbe\u8ba1\u98ce\u683c\u8fe5\u5f02\u6216\u66f4\u65b0\u8fed\u4ee3\u9891\u7e41\u7684\u5e94\u7528\uff0c\u5176\u6cdb\u5316\u80fd\u529b\u4ecd\u9700\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u3002 \u5f3a\u5316\u5b66\u4e60\u7684\u6311\u6218\uff1a \u8bfe\u7a0b\u5f3a\u5316\u5b66\u4e60\u867d\u7136\u5f3a\u5927\uff0c\u4f46\u901a\u5e38\u5bf9\u73af\u5883\u7684\u5efa\u6a21\u3001\u5956\u52b1\u51fd\u6570\u7684\u8bbe\u8ba1\u4ee5\u53ca\u8bad\u7ec3\u7684\u7a33\u5b9a\u6027\u6709\u8f83\u9ad8\u8981\u6c42\u3002\u5728\u771f\u5b9e\u3001\u5f00\u653e\u7684GUI\u73af\u5883\u4e2d\uff0c\u5982\u4f55\u6709\u6548\u5904\u7406\u63a2\u7d22-\u5229\u7528\u56f0\u5883\u3001\u7a00\u758f\u5956\u52b1\u4ee5\u53ca\u957f\u5e8f\u5217\u51b3\u7b56\uff0c\u4ecd\u662fRL\u9762\u4e34\u7684\u56fa\u6709\u6311\u6218\u3002 \u201c\u57fa\u7840\u6a21\u578b\u201d\u7684\u5b9a\u4e49\u4e0e\u8303\u56f4\uff1a \u4f5c\u4e3a\u4e00\u4e2a\u201c\u57fa\u7840\u6a21\u578b\u201d\uff0c\u5176\u901a\u7528\u6027\uff08\u5373\u5728\u4e0d\u8fdb\u884c\u989d\u5916\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\u5904\u7406\u5168\u65b0\u4efb\u52a1\u7684\u80fd\u529b\uff09\u4ee5\u53ca\u5bf9\u9ad8\u5ea6\u62bd\u8c61\u6216\u9700\u8981\u6df1\u5c42\u8bed\u4e49\u7406\u89e3\u7684\u4efb\u52a1\u7684\u5904\u7406\u80fd\u529b\uff0c\u5728\u6458\u8981\u4e2d\u672a\u8be6\u7ec6\u8bf4\u660e\u3002 \u5bf9\u7279\u5b9a\u8bed\u8a00/\u6587\u5316UI\u7684\u4f9d\u8d56\uff1a \u867d\u7136\u89e3\u51b3\u4e86\u4e2d\u6587\u5e94\u7528\u7684\u75db\u70b9\uff0c\u4f46\u8fd9\u53ef\u80fd\u610f\u5473\u7740\u6a21\u578b\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u5b66\u4e60\u4e86\u4e2d\u6587UI\u7684\u7279\u5b9a\u6a21\u5f0f\u548c\u4e60\u60ef\u3002\u5c06\u5176\u76f4\u63a5\u5e94\u7528\u4e8e\u5176\u4ed6\u8bed\u8a00\u6216\u6587\u5316\u80cc\u666f\u4e0bUI\uff08\u4f8b\u5982\u65e5\u6587\u3001\u963f\u62c9\u4f2f\u6587UI\uff09\u65f6\uff0c\u53ef\u80fd\u9700\u8981\u7c7b\u4f3c\u7684\u5b9a\u5236\u5316\u6570\u636e\u548c\u8bad\u7ec3\u3002 Key Findings: In particular, UItron highlights the interaction proficiency with top-tier Chinese mobile APPs, as we identified a general lack of Chinese capabilities even in state-of-the-art solutions. Links: PDF arXiv CAD2DMD-SET: Synthetic Generation Tool of Digital Measurement Device CAD Model Datasets for fine-tuning Large Vision-Language Models Authors: Jo\u00e3o Valente, Atabak Dehban, Rodrigo Ventura Published: 2025-08-29 Categories: cs.CV, cs.AI Abstract: Recent advancements in Large Vision-Language Models (LVLMs) have demonstrated impressive capabilities across various multimodal tasks. They continue, however, to struggle with trivial scenarios such as reading values from Digital Measurement Devices (DMDs), particularly in real-world conditions involving clutter, occlusions, extreme viewpoints, and motion blur; common in head-mounted cameras and Augmented Reality (AR) applications. Motivated by these limitations, this work introduces CAD2DMD-SET, a synthetic data generation tool designed to support visual question answering (VQA) tasks involving DMDs. By leveraging 3D CAD models, advanced rendering, and high-fidelity image composition, our tool produces diverse, VQA-labelled synthetic DMD datasets suitable for fine-tuning LVLMs. Additionally, we present DMDBench, a curated validation set of 1,000 annotated real-world images designed to evaluate model performance under practical constraints. Benchmarking three state-of-the-art LVLMs using Average Normalised Levenshtein Similarity (ANLS) and further fine-tuning LoRA's of these models with CAD2DMD-SET's generated dataset yielded substantial improvements, with InternVL showcasing a score increase of 200% without degrading on other tasks. This demonstrates that the CAD2DMD-SET training dataset substantially improves the robustness and performance of LVLMs when operating under the previously stated challenging conditions. The CAD2DMD-SET tool is expected to be released as open-source once the final version of this manuscript is prepared, allowing the community to add different measurement devices and generate their own datasets. Analysis: \u8fd9\u7bc7\u8bba\u6587\u6458\u8981\u5c55\u793a\u4e86\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u4e00\u4e2a\u91cd\u8981\u8fdb\u5c55\uff0c\u7279\u522b\u662f\u5728\u5927\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u7684\u5b9e\u9645\u5e94\u7528\u548c\u6570\u636e\u751f\u6210\u65b9\u9762\u3002 1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u6d01\u603b\u7ed3 (Concise Summary) \u672c\u6587\u63d0\u51fa\u4e86CAD2DMD-SET\uff0c\u4e00\u4e2a\u521b\u65b0\u7684\u5408\u6210\u6570\u636e\u751f\u6210\u5de5\u5177\uff0c\u65e8\u5728\u89e3\u51b3\u5927\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u5728\u590d\u6742\u771f\u5b9e\u4e16\u754c\u6761\u4ef6\u4e0b\u8bfb\u53d6\u6570\u5b57\u6d4b\u91cf\u8bbe\u5907\uff08DMDs\uff09\u6570\u503c\u7684\u96be\u9898\u3002\u901a\u8fc7\u5229\u75283D CAD\u6a21\u578b\u3001\u9ad8\u7ea7\u6e32\u67d3\u548c\u9ad8\u4fdd\u771f\u56fe\u50cf\u5408\u6210\u6280\u672f\uff0c\u8be5\u5de5\u5177\u80fd\u751f\u6210\u591a\u6837\u5316\u4e14\u5e26\u6709VQA\u6807\u6ce8\u7684\u5408\u6210DMD\u6570\u636e\u96c6\uff0c\u5e76\u7ed3\u5408DMDBench\u8fd9\u4e00\u771f\u5b9e\u7684\u9a8c\u8bc1\u96c6\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c\u4f7f\u7528CAD2DMD-SET\u751f\u6210\u7684\u6570\u636e\u96c6\u5bf9LVLMs\u8fdb\u884c\u5fae\u8c03\uff0c\u80fd\u663e\u8457\u63d0\u5347\u5176\u5728DMD\u8bfb\u53d6\u4efb\u52a1\u4e0a\u7684\u9c81\u68d2\u6027\u548c\u6027\u80fd\uff0c\u4e14\u4e0d\u5f71\u54cd\u5176\u901a\u7528\u80fd\u529b\u3002 2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u5b66\u65b9\u6cd5 (Key Innovation or Methodological Approach) \u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u5176 \u5408\u6210\u6570\u636e\u751f\u6210\u8303\u5f0f \uff0c\u7279\u522b\u662f\u9488\u5bf9\u6570\u5b57\u6d4b\u91cf\u8bbe\u5907\uff08DMDs\uff09\u7684\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u4efb\u52a1\u3002\u8be5\u65b9\u6cd5\u5229\u7528 3D CAD\u6a21\u578b \u4f5c\u4e3a\u57fa\u7840\uff0c\u7ed3\u5408 \u9ad8\u7ea7\u6e32\u67d3\u6280\u672f\u548c\u9ad8\u4fdd\u771f\u56fe\u50cf\u5408\u6210 \uff0c\u80fd\u591f\u7cfb\u7edf\u6027\u5730\u751f\u6210\u5305\u542b\u590d\u6742\u771f\u5b9e\u4e16\u754c\u6761\u4ef6\uff08\u5982\u6742\u4e71\u3001\u906e\u6321\u3001\u6781\u7aef\u89c6\u89d2\u548c\u8fd0\u52a8\u6a21\u7cca\uff09\u7684 \u591a\u6837\u5316\u3001VQA\u6807\u6ce8\u7684\u5408\u6210\u6570\u636e\u96c6 \u3002\u6b64\u5916\uff0c\u5f15\u5165 DMDBench \u4f5c\u4e3a\u72ec\u7acb\u7684\u771f\u5b9e\u4e16\u754c\u9a8c\u8bc1\u96c6\uff0c\u4e3a\u8bc4\u4f30\u6a21\u578b\u5728\u5b9e\u9645\u7ea6\u675f\u4e0b\u7684\u6027\u80fd\u63d0\u4f9b\u4e86\u53ef\u9760\u57fa\u51c6\uff0c\u5f62\u6210\u4e86\u4e00\u4e2a\u5b8c\u6574\u7684\u8bad\u7ec3\u4e0e\u8bc4\u4f30\u95ed\u73af\u3002\u8fd9\u79cd\u5c06\u9ad8\u4fdd\u771f\u5408\u6210\u6570\u636e\u4e0e\u771f\u5b9e\u4e16\u754c\u9a8c\u8bc1\u76f8\u7ed3\u5408\u7684\u7b56\u7565\uff0c\u6709\u6548\u5730\u5f25\u8865\u4e86\u201c\u6a21\u62df\u5230\u73b0\u5b9e\u201d\uff08sim-to-real\uff09\u7684\u9e3f\u6c9f\u3002 3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd (Potential Impact on the Field) \u63d0\u5347LVLM\u5728\u7279\u5b9a\u9886\u57df\u5e94\u7528\u7684\u9c81\u68d2\u6027\u4e0e\u51c6\u786e\u6027\uff1a \u672c\u6587\u8bc1\u660e\u4e86\u901a\u8fc7\u9ad8\u8d28\u91cf\u5408\u6210\u6570\u636e\u5bf9LVLMs\u8fdb\u884c\u5fae\u8c03\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u5176\u5728\u7279\u5b9a\u3001\u590d\u6742\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e0b\u7684\u6027\u80fd\uff0c\u4e14\u4e0d\u5f71\u54cd\u5176\u901a\u7528\u80fd\u529b\uff0c\u4e3aLVLM\u5728\u5de5\u4e1a\u3001AR\u7b49\u9886\u57df\u7684\u843d\u5730\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002 \u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff1a \u5bf9\u4e8e\u96be\u4ee5\u83b7\u53d6\u5927\u91cf\u771f\u5b9e\u4e16\u754c\u6807\u6ce8\u6570\u636e\u7684\u7279\u5b9a\u89c6\u89c9\u4efb\u52a1\uff0cCAD2DMD-SET\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u5408\u6210\u6570\u636e\u751f\u6210\u89e3\u51b3\u65b9\u6848\uff0c\u964d\u4f4e\u4e86\u6a21\u578b\u5f00\u53d1\u548c\u90e8\u7f72\u7684\u6210\u672c\u3002 \u63a8\u52a8\u5408\u6210\u6570\u636e\u7814\u7a76\uff1a \u5f3a\u8c03\u4e86\u9ad8\u4fdd\u771f\u6e32\u67d3\u548c\u667a\u80fd\u5408\u6210\u5728\u5f25\u5408\u201c\u6a21\u62df\u5230\u73b0\u5b9e\u201d\u9e3f\u6c9f\u4e2d\u7684\u5173\u952e\u4f5c\u7528\uff0c\u4e3a\u672a\u6765\u66f4\u591a\u5229\u7528\u5408\u6210\u6570\u636e\u8bad\u7ec3\u89c6\u89c9\u6a21\u578b\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u8303\u4f8b\u3002 \u793e\u533a\u8d21\u732e\uff1a \u8ba1\u5212\u7684\u5f00\u6e90\u53d1\u5e03\u5c06\u4fc3\u8fdb\u793e\u533a\u5728\u6b64\u57fa\u7840\u4e0a\u8fdb\u884c\u6269\u5c55\uff0c\u652f\u6301\u66f4\u591a\u6d4b\u91cf\u8bbe\u5907\u548c\u5e94\u7528\u573a\u666f\uff0c\u52a0\u901f\u76f8\u5173\u6280\u672f\u7684\u53d1\u5c55\u3002 4. \u53ef\u80fd\u53d7\u76ca\u7684\u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 (Related Areas or Applications) \u5de5\u4e1a\u81ea\u52a8\u5316\u4e0e\u8d28\u91cf\u63a7\u5236\uff1a \u5728\u5de5\u5382\u73af\u5883\u4e2d\uff0c\u81ea\u52a8\u8bfb\u53d6\u5404\u79cd\u4eea\u8868\u3001\u4f20\u611f\u5668\u548c\u663e\u793a\u5c4f\u7684\u6570\u503c\uff0c\u7528\u4e8e\u8fc7\u7a0b\u76d1\u63a7\u3001\u6545\u969c\u8bca\u65ad\u548c\u4ea7\u54c1\u8d28\u91cf\u68c0\u6d4b\u3002 \u589e\u5f3a\u73b0\u5b9e\uff08AR\uff09\u4e0e\u6df7\u5408\u73b0\u5b9e\uff08MR\uff09\uff1a \u7ed3\u5408\u5934\u6234\u5f0f\u8bbe\u5907\uff0c\u5b9e\u73b0\u5bf9\u7269\u7406\u4e16\u754c\u4e2d\u6570\u5b57\u6d4b\u91cf\u8bbe\u5907\u7684\u5b9e\u65f6\u8bc6\u522b\u548c\u6570\u503c\u63d0\u53d6\uff0c\u4e3a\u7528\u6237\u63d0\u4f9b\u4e0a\u4e0b\u6587\u4fe1\u606f\u6216\u64cd\u4f5c\u6307\u5bfc\u3002 \u673a\u5668\u4eba\u6280\u672f\uff1a \u8d4b\u4e88\u673a\u5668\u4eba\u8bc6\u522b\u548c\u7406\u89e3\u73af\u5883\u4e2d\u5404\u79cd\u6570\u5b57\u663e\u793a\u5668\u7684\u80fd\u529b\uff0c\u4f8b\u5982\u8bfb\u53d6\u8bbe\u5907\u72b6\u6001\u3001\u5b8c\u6210\u7279\u5b9a\u4efb\u52a1\u3002 \u8bbe\u5907\u7ef4\u62a4\u4e0e\u68c0\u6d4b\uff1a \u5e2e\u52a9\u6280\u672f\u4eba\u5458\u901a\u8fc7\u89c6\u89c9\u7cfb\u7edf\u5feb\u901f\u51c6\u786e\u5730\u83b7\u53d6\u8bbe\u5907\u8bfb\u6570\uff0c\u63d0\u9ad8\u7ef4\u62a4\u6548\u7387\u548c\u51c6\u786e\u6027\u3002 \u667a\u80fd\u7a7f\u6234\u8bbe\u5907\uff1a \u63d0\u5347\u667a\u80fd\u773c\u955c\u7b49\u8bbe\u5907\u5728\u590d\u6742\u73af\u5883\u4e2d\u7406\u89e3\u548c\u4ea4\u4e92\u7269\u7406\u4e16\u754c\u4fe1\u606f\u7684\u80fd\u529b\u3002 5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u7684\u5c40\u9650\u6027 (Limitations Inferable from the Abstract) \u9886\u57df\u7279\u5f02\u6027\uff1a CAD2DMD-SET\u5de5\u5177\u662f\u4e13\u95e8\u4e3a\u6570\u5b57\u6d4b\u91cf\u8bbe\u5907\uff08DMDs\uff09\u8bbe\u8ba1\u7684\u3002\u5c06\u5176\u6269\u5c55\u5230\u5176\u4ed6\u7c7b\u578b\u7684\u89c6\u89c9\u4efb\u52a1\u6216\u7269\u4f53\uff08\u4f8b\u5982\uff0c\u6a21\u62df\u4eea\u8868\u3001\u66f4\u590d\u6742\u7684\u5de5\u4e1a\u8bbe\u5907\uff09\u5c06\u9700\u8981\u65b0\u76843D CAD\u6a21\u578b\u548c\u53ef\u80fd\u4e0d\u540c\u7684\u6e32\u67d3\u53ca\u5408\u6210\u7b56\u7565\uff0c\u8fd9\u5e76\u975e\u4e00\u4e2a\u901a\u7528\u89e3\u51b3\u65b9\u6848\u3002 CAD\u6a21\u578b\u4f9d\u8d56\u6027\uff1a \u5408\u6210\u6570\u636e\u7684\u8d28\u91cf\u548c\u591a\u6837\u6027\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u53ef\u75283D CAD\u6a21\u578b\u7684\u51c6\u786e\u6027\u548c\u4e30\u5bcc\u6027\u3002\u83b7\u53d6\u9ad8\u8d28\u91cf\u7684CAD\u6a21\u578b\u672c\u8eab\u53ef\u80fd\u662f\u4e00\u4e2a\u6311\u6218\u6216\u6210\u672c\u6765\u6e90\u3002 \u6f5c\u5728\u7684\u201c\u6a21\u62df\u5230\u73b0\u5b9e\u201d\u9e3f\u6c9f\uff1a \u5c3d\u7ba1\u8bba\u6587\u5f3a\u8c03\u4e86\u201c\u9ad8\u4fdd\u771f\u56fe\u50cf\u5408\u6210\u201d\uff0c\u4f46\u5408\u6210\u6570\u636e\u4e0e\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e4b\u95f4\u4ecd\u53ef\u80fd\u5b58\u5728\u7ec6\u5fae\u7684\u5206\u5e03\u5dee\u5f02\uff08\u5373\u6b8b\u4f59\u7684\u201c\u6a21\u62df\u5230\u73b0\u5b9e\u201d\u9e3f\u6c9f\uff09\uff0c\u8fd9\u53ef\u80fd\u5728\u67d0\u4e9b\u672a\u88abDMDBench\u5145\u5206\u8986\u76d6\u7684\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u6765\u3002DMDBench\u867d\u7136\u63d0\u4f9b\u4e86\u771f\u5b9e\u4e16\u754c\u9a8c\u8bc1\uff0c\u4f46\u51761000\u5f20\u56fe\u50cf\u7684\u89c4\u6a21\u53ef\u80fd\u4e0d\u8db3\u4ee5\u5b8c\u5168\u4ee3\u8868\u6240\u6709\u6781\u7aef\u771f\u5b9e\u4e16\u754c\u6761\u4ef6\u3002 \u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\uff1a \u751f\u6210\u9ad8\u4fdd\u771f\u3001\u591a\u6837\u5316\u7684\u5408\u6210\u6570\u636e\u96c6\uff0c\u7279\u522b\u662f\u6d89\u53ca\u9ad8\u7ea7\u6e32\u67d3\u548c\u590d\u6742\u573a\u666f\u5408\u6210\u65f6\uff0c\u53ef\u80fd\u9700\u8981\u5927\u91cf\u7684\u8ba1\u7b97\u8d44\u6e90\u548c\u65f6\u95f4\u3002 Key Findings: Motivated by these limitations, this work introduces CAD2DMD-SET, a synthetic data generation tool designed to support visual question answering (VQA) tasks involving DMDs. By leveraging 3D CAD models, advanced rendering, and high-fidelity image composition, our tool produces diverse, VQA-labelled synthetic DMD datasets suitable for fine-tuning LVLMs. Additionally, we present DMDBench, a curated validation set of 1,000 annotated real-world images designed to evaluate model performance under practical constraints. Benchmarking three state-of-the-art LVLMs using Average Normalised Levenshtein Similarity (ANLS) and further fine-tuning LoRA's of these models with CAD2DMD-SET's generated dataset yielded substantial improvements, with InternVL showcasing a score increase of 200% without degrading on other tasks. Links: PDF arXiv Mapping like a Skeptic: Probabilistic BEV Projection for Online HD Mapping Authors: Fatih Erdo\u011fan, Merve Rabia Bar\u0131n, Fatma G\u00fcney Published: 2025-08-29 Categories: cs.CV Abstract: Constructing high-definition (HD) maps from sensory input requires accurately mapping the road elements in image space to the Bird's Eye View (BEV) space. The precision of this mapping directly impacts the quality of the final vectorized HD map. Existing HD mapping approaches outsource the projection to standard mapping techniques, such as attention-based ones. However, these methods struggle with accuracy due to generalization problems, often hallucinating non-existent road elements. Our key idea is to start with a geometric mapping based on camera parameters and adapt it to the scene to extract relevant map information from camera images. To implement this, we propose a novel probabilistic projection mechanism with confidence scores to (i) refine the mapping to better align with the scene and (ii) filter out irrelevant elements that should not influence HD map generation. In addition, we improve temporal processing by using confidence scores to selectively accumulate reliable information over time. Experiments on new splits of the nuScenes and Argoverse2 datasets demonstrate improved performance over state-of-the-art approaches, indicating better generalization. The improvements are particularly pronounced on nuScenes and in the challenging long perception range. Our code and model checkpoints are available at https://github.com/Fatih-Erdogan/mapping-like-skeptic . Analysis: \u8fd9\u7bc7\u8bba\u6587\u7684\u6458\u8981\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5173\u4e8e\u5728\u7ebf\u9ad8\u6e05\u5730\u56fe\u6784\u5efa\u4e2d\u9e1f\u77b0\u56fe\uff08BEV\uff09\u6295\u5f71\u7684\u5173\u952e\u6539\u8fdb\u3002\u4ee5\u4e0b\u662f\u8be6\u7ec6\u5206\u6790\uff1a 1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u6d01\u603b\u7ed3 (A concise summary of the paper's main contribution) \u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6982\u7387\u6027\u6295\u5f71\u673a\u5236\uff0c\u7528\u4e8e\u5c06\u56fe\u50cf\u7a7a\u95f4\u4e2d\u7684\u9053\u8def\u5143\u7d20\u51c6\u786e\u6620\u5c04\u5230\u9e1f\u77b0\u56fe\uff08BEV\uff09\u7a7a\u95f4\uff0c\u4ee5\u6784\u5efa\u5728\u7ebf\u9ad8\u6e05\u5730\u56fe\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u57fa\u4e8e\u76f8\u673a\u53c2\u6570\u7684\u51e0\u4f55\u6620\u5c04\u4e0e\u573a\u666f\u81ea\u9002\u5e94\u7684\u7f6e\u4fe1\u5ea6\u5206\u6570\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\uff09\u5728\u6cdb\u5316\u6027\u548c\u907f\u514d\u5e7b\u89c9\uff08hallucination\uff09\u65b9\u9762\u7684\u4e0d\u8db3\u3002\u5b83\u901a\u8fc7\u7f6e\u4fe1\u5ea6\u5206\u6570\u6765\u4f18\u5316\u6620\u5c04\u3001\u8fc7\u6ee4\u4e0d\u76f8\u5173\u5143\u7d20\uff0c\u5e76\u9009\u62e9\u6027\u5730\u7d2f\u79ef\u53ef\u9760\u7684\u957f\u671f\u4fe1\u606f\uff0c\u4ece\u800c\u663e\u8457\u63d0\u9ad8\u4e86\u9ad8\u6e05\u5730\u56fe\u7684\u7cbe\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\u3002 2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u5b66\u65b9\u6cd5 (The key innovation or methodological approach) \u8be5\u8bba\u6587\u7684\u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u5176 \u201c\u6000\u7591\u8bba\u8005\u5f0f\u201d\u7684\u6982\u7387\u6027\u6295\u5f71\u673a\u5236 \uff0c\u5b83\u4e0e\u4f20\u7edf\u7eaf\u7cb9\u4f9d\u8d56\u5b66\u4e60\u6216\u6ce8\u610f\u529b\u673a\u5236\u7684\u65b9\u6cd5\u5f62\u6210\u5bf9\u6bd4\uff1a \u51e0\u4f55\u5148\u9a8c\u4e0e\u573a\u666f\u81ea\u9002\u5e94\u7ed3\u5408\uff1a \u4e0d\u540c\u4e8e\u5b8c\u5168\u4f9d\u8d56\u6570\u636e\u9a71\u52a8\u7684\u5b66\u4e60\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u9996\u5148\u5229\u7528\u57fa\u4e8e\u76f8\u673a\u53c2\u6570\u7684\u51e0\u4f55\u6620\u5c04\u4f5c\u4e3a\u8d77\u70b9\uff0c\u7136\u540e\u901a\u8fc7\u5b66\u4e60\u673a\u5236\u4f7f\u5176\u9002\u5e94\u7279\u5b9a\u573a\u666f\u3002\u8fd9\u63d0\u4f9b\u4e86\u4e00\u4e2a\u66f4\u7a33\u5065\u7684\u521d\u59cb\u57fa\u7840\uff0c\u51cf\u5c11\u4e86\u6a21\u578b\u4ece\u96f6\u5f00\u59cb\u5b66\u4e60\u6295\u5f71\u7684\u96be\u5ea6\u3002 \u7f6e\u4fe1\u5ea6\u5206\u6570\u9a71\u52a8\u7684\u7cbe\u70bc\u4e0e\u8fc7\u6ee4\uff1a \u5f15\u5165\u4e86\u7f6e\u4fe1\u5ea6\u5206\u6570\u6765\uff1a \u7cbe\u70bc\u6620\u5c04\uff1a \u4f7f\u6295\u5f71\u66f4\u597d\u5730\u4e0e\u5b9e\u9645\u573a\u666f\u5bf9\u9f50\u3002 \u8fc7\u6ee4\u65e0\u5173\u5143\u7d20\uff1a \u660e\u786e\u5730\u8bc6\u522b\u5e76\u5254\u9664\u90a3\u4e9b\u53ef\u80fd\u7531\u6a21\u578b\u201c\u5e7b\u89c9\u201d\u51fa\u6765\u7684\u3001\u4e0d\u5b58\u5728\u7684\u9053\u8def\u5143\u7d20\uff0c\u4ece\u800c\u63d0\u9ad8\u5730\u56fe\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002 \u65f6\u95f4\u7ef4\u5ea6\u4e0a\u7684\u9009\u62e9\u6027\u7d2f\u79ef\uff1a \u5229\u7528\u7f6e\u4fe1\u5ea6\u5206\u6570\u5728\u65f6\u95f4\u5e8f\u5217\u4e0a\u9009\u62e9\u6027\u5730\u7d2f\u79ef\u53ef\u9760\u4fe1\u606f\uff0c\u589e\u5f3a\u4e86\u5728\u7ebf\u5730\u56fe\u6784\u5efa\u7684\u9c81\u68d2\u6027\u548c\u4e00\u81f4\u6027\uff0c\u907f\u514d\u4e86\u4e0d\u53ef\u9760\u4fe1\u606f\u7684\u7d2f\u79ef\u3002 3. \u5bf9\u9886\u57df\u7684\u6f5c\u5728\u5f71\u54cd (Potential impact on the field) \u63d0\u9ad8\u9ad8\u6e05\u5730\u56fe\u7684\u53ef\u9760\u6027\u4e0e\u5b89\u5168\u6027\uff1a \u901a\u8fc7\u51cf\u5c11\u5e7b\u89c9\u548c\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u66f4\u51c6\u786e\u3001\u66f4\u53ef\u9760\u7684\u9ad8\u6e05\u5730\u56fe\uff0c\u8fd9\u5bf9\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u5b89\u5168\u8fd0\u884c\u81f3\u5173\u91cd\u8981\u3002 \u63a8\u52a8\u6df7\u5408\u5f0fBEV\u6295\u5f71\u65b9\u6cd5\u7684\u53d1\u5c55\uff1a \u51e0\u4f55\u5148\u9a8c\u4e0e\u5b66\u4e60\u673a\u5236\u7684\u7ed3\u5408\u53ef\u80fd\u542f\u53d1\u66f4\u591a\u6df7\u5408\u5f0f\u65b9\u6cd5\uff0c\u5728\u6570\u636e\u9a71\u52a8\u7684\u7075\u6d3b\u6027\u548c\u51e0\u4f55\u7ea6\u675f\u7684\u9c81\u68d2\u6027\u4e4b\u95f4\u627e\u5230\u66f4\u597d\u7684\u5e73\u8861\u3002 \u89e3\u51b3\u957f\u5c3e\u95ee\u9898\u548c\u6cdb\u5316\u6311\u6218\uff1a \u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u6216\u4e0d\u5e38\u89c1\u573a\u666f\u4e0b\u7684\u6cdb\u5316\u6027\u5dee\u662f\u4e00\u4e2a\u666e\u904d\u95ee\u9898\u3002\u8be5\u8bba\u6587\u7684\u6539\u8fdb\uff0c\u5c24\u5176\u662f\u5728nuScenes\u548c\u957f\u611f\u77e5\u8ddd\u79bb\u4e0a\u7684\u8868\u73b0\uff0c\u8868\u660e\u5b83\u80fd\u66f4\u597d\u5730\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\u3002 \u4fc3\u8fdb\u5728\u7ebf\u5730\u56fe\u6784\u5efa\u7684\u5b9e\u7528\u5316\uff1a \u5f3a\u8c03\u5728\u7ebf\u5904\u7406\u548c\u65f6\u95f4\u4fe1\u606f\u7d2f\u79ef\uff0c\u4f7f\u5176\u66f4\u9002\u7528\u4e8e\u5b9e\u9645\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u5b9e\u65f6\u5730\u56fe\u6784\u5efa\u9700\u6c42\u3002 4. \u53ef\u80fd\u53d7\u76ca\u4e8e\u8fd9\u9879\u7814\u7a76\u7684\u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 (Related areas or applications that might benefit from this research) \u81ea\u52a8\u9a7e\u9a76\uff1a \u8fd9\u662f\u6700\u76f4\u63a5\u7684\u5e94\u7528\u9886\u57df\uff0c\u9ad8\u6e05\u5730\u56fe\u662f\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u3001\u5b9a\u4f4d\u548c\u89c4\u5212\u7684\u6838\u5fc3\u7ec4\u6210\u90e8\u5206\u3002 \u673a\u5668\u4eba\u5bfc\u822a\u4e0eSLAM\uff1a \u4efb\u4f55\u9700\u8981\u4ece\u4f20\u611f\u5668\u8f93\u5165\u6784\u5efa\u73af\u5883\u5730\u56fe\u7684\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u90fd\u53ef\u4ee5\u4ece\u66f4\u51c6\u786e\u3001\u66f4\u53ef\u9760\u7684BEV\u6295\u5f71\u548c\u5730\u56fe\u6784\u5efa\u4e2d\u53d7\u76ca\u3002 3D\u573a\u666f\u91cd\u5efa\uff1a \u4ece2D\u56fe\u50cf\u4e2d\u51c6\u786e\u63a8\u65ad3D\u7ed3\u6784\u548c\u5e03\u5c40\uff0c\u5c24\u5176\u662f\u5728\u7ed3\u6784\u5316\u73af\u5883\u4e2d\uff08\u5982\u9053\u8def\uff09\uff0c\u8be5\u65b9\u6cd5\u53ef\u4ee5\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u53c2\u8003\u3002 \u57ce\u5e02\u89c4\u5212\u4e0e\u6d4b\u7ed8\uff1a \u81ea\u52a8\u5316\u5730\u4ece\u56fe\u50cf\u6570\u636e\u751f\u6210\u9ad8\u7cbe\u5ea6\u5730\u56fe\uff0c\u53ef\u4ee5\u63d0\u9ad8\u57ce\u5e02\u57fa\u7840\u8bbe\u65bd\u7ba1\u7406\u548c\u6d4b\u7ed8\u7684\u6548\u7387\u3002 \u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\uff1a \u5b9e\u65f6\u3001\u9ad8\u7cbe\u5ea6\u7684\u9053\u8def\u5143\u7d20\u4fe1\u606f\u5bf9\u4e8e\u4ea4\u901a\u6d41\u7ba1\u7406\u3001\u4e8b\u4ef6\u68c0\u6d4b\u7b49\u5e94\u7528\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002 5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u51fa\u7684\u5c40\u9650\u6027 (Any limitations that can be inferred from the abstract) \u5bf9\u76f8\u673a\u53c2\u6570\u7684\u4f9d\u8d56\uff1a \u6458\u8981\u4e2d\u63d0\u5230\u201c\u57fa\u4e8e\u76f8\u673a\u53c2\u6570\u7684\u51e0\u4f55\u6620\u5c04\u201d\uff0c\u8fd9\u610f\u5473\u7740\u8be5\u65b9\u6cd5\u53ef\u80fd\u5bf9\u51c6\u786e\u7684\u76f8\u673a\u6807\u5b9a\u6709\u4e00\u5b9a\u4f9d\u8d56\u3002\u5982\u679c\u76f8\u673a\u6807\u5b9a\u4e0d\u51c6\u786e\uff0c\u521d\u59cb\u7684\u51e0\u4f55\u6295\u5f71\u53ef\u80fd\u4f1a\u5f15\u5165\u8bef\u5dee\uff0c\u5c3d\u7ba1\u540e\u7eed\u7684\u6982\u7387\u673a\u5236\u4f1a\u5c1d\u8bd5\u7ea0\u6b63\u3002 \u7f6e\u4fe1\u5ea6\u5206\u6570\u7684\u9c81\u68d2\u6027\uff1a \u5173\u952e\u5728\u4e8e\u5982\u4f55\u51c6\u786e\u5730\u751f\u6210\u548c\u5229\u7528\u7f6e\u4fe1\u5ea6\u5206\u6570\u3002\u5982\u679c\u7f6e\u4fe1\u5ea6\u5206\u6570\u672c\u8eab\u5728\u67d0\u4e9b\u6781\u7aef\u6216\u6a21\u7cca\u573a\u666f\u4e0b\u4e0d\u53ef\u9760\uff0c\u53ef\u80fd\u4f1a\u5f71\u54cd\u6574\u4f53\u6027\u80fd\u3002 \u201c\u573a\u666f\u81ea\u9002\u5e94\u201d\u7684\u8303\u56f4\uff1a \u6458\u8981\u63d0\u5230\u201c\u9002\u5e94\u573a\u666f\u4ee5\u63d0\u53d6\u76f8\u5173\u5730\u56fe\u4fe1\u606f\u201d\uff0c\u4f46\u672a\u8be6\u7ec6\u8bf4\u660e\u8fd9\u79cd\u9002\u5e94\u6027\u5728\u9762\u5bf9\u9ad8\u5ea6\u52a8\u6001\u3001\u590d\u6742\u6216\u5b8c\u5168\u65b0\u9896\u7684\u573a\u666f\u65f6\u7684\u9c81\u68d2\u6027\u5982\u4f55\u3002 \u8ba1\u7b97\u5f00\u9500\uff1a \u5f15\u5165\u6982\u7387\u6027\u673a\u5236\u548c\u7f6e\u4fe1\u5ea6\u5206\u6570\uff0c\u4ee5\u53ca\u65f6\u95f4\u4e0a\u7684\u4fe1\u606f\u7d2f\u79ef\uff0c\u53ef\u80fd\u4f1a\u589e\u52a0\u8ba1\u7b97\u590d\u6742\u6027\uff0c\u8fd9\u5bf9\u4e8e\u5728\u7ebf\uff08\u5b9e\u65f6\uff09\u5e94\u7528\u6765\u8bf4\u662f\u4e00\u4e2a\u9700\u8981\u5173\u6ce8\u7684\u95ee\u9898\u3002 \u6570\u636e\u4f9d\u8d56\u6027\uff1a \u5c3d\u7ba1\u58f0\u79f0\u6cdb\u5316\u6027\u66f4\u597d\uff0c\u4f46\u5b9e\u9a8c\u4ecd\u5728nuScenes\u548cArgoverse2\u6570\u636e\u96c6\u7684\u65b0\u5206\u5272\u4e0a\u8fdb\u884c\u3002\u5728\u66f4\u5e7f\u6cdb\u3001\u66f4\u591a\u6837\u5316\u7684\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u7684\u8868\u73b0\u4ecd\u9700\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u3002 Key Findings: To implement this, we propose a novel probabilistic projection mechanism with confidence scores to (i) refine the mapping to better align with the scene and (ii) filter out irrelevant elements that should not influence HD map generation. Experiments on new splits of the nuScenes and Argoverse2 datasets demonstrate improved performance over state-of-the-art approaches, indicating better generalization. Links: PDF arXiv","title":"Arxiv Computer Vision Papers - 2025-09-01"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-01/#arxiv-computer-vision-papers-2025-09-01","text":"","title":"Arxiv Computer Vision Papers - 2025-09-01"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-01/#executive-summary","text":"\u597d\u7684\uff0c\u8fd9\u662f\u4e00\u4efd\u9488\u5bf92025\u5e748\u670829\u65e5Arxiv\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u8bba\u6587\u7684\u6bcf\u65e5\u62a5\u544a\u6267\u884c\u6458\u8981\uff1a Arxiv \u8ba1\u7b97\u673a\u89c6\u89c9\u6bcf\u65e5\u62a5\u544a\u6267\u884c\u6458\u8981 (2025\u5e748\u670829\u65e5) \u6982\u8ff0\uff1a \u4eca\u65e5Arxiv\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u7684\u8bba\u6587\u5448\u73b0\u51fa\u51e0\u4e2a\u663e\u8457\u8d8b\u52bf\uff1a \u5408\u6210\u6570\u636e\u751f\u6210 \u5728\u89e3\u51b3\u4f4e\u6570\u636e\u91cf\u548c\u5fae\u8c03\u5927\u578b\u6a21\u578b\u65b9\u9762\u626e\u6f14\u7740\u8d8a\u6765\u8d8a\u91cd\u8981\u7684\u89d2\u8272\uff1b 3D\u89c6\u89c9\u4e0e\u91cd\u5efa \u6280\u672f\u6301\u7eed\u521b\u65b0\uff0c\u7279\u522b\u662f\u7ed3\u5408\u6269\u6563\u6a21\u578b\u5b9e\u73b0\u5355\u56fe3D\u91cd\u5efa\u548c\u9ad8\u6548\u59ff\u6001\u4f30\u8ba1\uff1b \u81ea\u52a8\u9a7e\u9a76\u4e0e\u673a\u5668\u4eba \u9886\u57df\u7684\u7814\u7a76\u4f9d\u7136\u6d3b\u8dc3\uff0c\u6db5\u76d6\u4e86\u6570\u636e\u96c6\u3001\u77e5\u8bc6\u95ee\u7b54\u548c\u9ad8\u7cbe\u5730\u56fe\uff1b\u6b64\u5916\uff0c\u5bf9 \u6a21\u578b\u6548\u7387\u3001\u6cdb\u5316\u80fd\u529b \u4ee5\u53ca \u901a\u7528AI\u4ee3\u7406 \u7684\u63a2\u7d22\u4e5f\u5f15\u4eba\u6ce8\u76ee\u3002 \u4e3b\u8981\u4e3b\u9898\u4e0e\u8d8b\u52bf\uff1a \u9ad8\u6548\u5408\u6210\u6570\u636e\u751f\u6210\uff1a \u591a\u7bc7\u8bba\u6587\u805a\u7126\u4e8e\u5229\u7528LoRA\u7b49\u6280\u672f\u9ad8\u6548\u751f\u6210\u9ad8\u8d28\u91cf\u5408\u6210\u6570\u636e\uff0c\u4ee5\u5e94\u5bf9\u7279\u5b9a\u9886\u57df\uff08\u5982\u5de5\u4e1aCAD\u6a21\u578b\u3001\u4f4e\u6570\u636e\u91cf\u573a\u666f\uff09\u7684\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u5e76\u652f\u6301\u5927\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08LVMs\uff09\u7684\u5fae\u8c03\u3002 3D\u89c6\u89c9\u4e0e\u6269\u6563\u6a21\u578b\u7684\u878d\u5408\uff1a \u6269\u6563\u6a21\u578b\u57283D\u91cd\u5efa\u548c\u59ff\u6001\u4f30\u8ba1\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u6f5c\u529b\uff0c\u5b9e\u73b0\u4e86\u4ece\u5355\u5f20\u56fe\u50cf\u751f\u6210\u5b8c\u6574\u9ad8\u65af\u6cfc\u6e85\uff08Gaussian Splats\uff09\u4ee5\u53ca\u9ad8\u6548\u76843D\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u3002 \u81ea\u52a8\u9a7e\u9a76\u4e0e\u673a\u5668\u4eba\u611f\u77e5\uff1a \u65b0\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\u53d1\u5e03\uff0c\u540c\u65f6\u7814\u7a76\u5173\u6ce8\u4e8e\u81ea\u52a8\u9a7e\u9a76\u77e5\u8bc6\u95ee\u7b54\uff08VQA\uff09\u548c\u9c81\u68d2\u7684\u5728\u7ebf\u9ad8\u7cbe\u5730\u56fe\u6784\u5efa\uff0c\u5f3a\u8c03\u4e86\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u611f\u77e5\u4e0e\u63a8\u7406\u80fd\u529b\u3002 \u6a21\u578b\u6548\u7387\u4e0e\u6cdb\u5316\uff1a \u6709\u7814\u7a76\u63a2\u7d22\u66ff\u4ee3\u4f20\u7edfU-Net\u67b6\u6784\u4ee5\u63d0\u9ad8\u5206\u5272\u6548\u7387\uff0c\u5e76\u6df1\u5165\u63a2\u8ba8\u4e86\u5728\u201c\u91ce\u5916\u201d\u573a\u666f\u4e0b\u5b9e\u73b0\u9886\u57df\u6cdb\u5316\u7684\u65b9\u6cd5\uff0c\u65e8\u5728\u63d0\u5347\u6a21\u578b\u5728\u672a\u77e5\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u3002 \u901a\u7528AI\u4ee3\u7406\u7684\u5d1b\u8d77\uff1a \u51fa\u73b0\u4e86\u65e8\u5728\u6784\u5efa\u57fa\u7840\u6027GUI\u4ee3\u7406\u7684\u5de5\u4f5c\uff0c\u7ed3\u5408\u5148\u8fdb\u7684\u611f\u77e5\u548c\u89c4\u5212\u80fd\u529b\uff0c\u9884\u793a\u7740\u901a\u7528\u578bAI\u5728\u4eba\u673a\u4ea4\u4e92\u9886\u57df\u7684\u5e94\u7528\u524d\u666f\u3002 \u7279\u522b\u91cd\u8981\u6216\u521b\u65b0\u7684\u8bba\u6587\uff1a \"Complete Gaussian Splats from a Single Image with Denoising Diffusion Models\" (Ziwei Liao et al.)\uff1a \u8fd9\u7bc7\u8bba\u6587\u6781\u5177\u521b\u65b0\u6027\uff0c\u5b83\u5229\u7528\u53bb\u566a\u6269\u6563\u6a21\u578b\u5b9e\u73b0\u4e86\u4ece\u5355\u5f20\u56fe\u50cf\u751f\u6210\u5b8c\u6574\u76843D\u9ad8\u65af\u6cfc\u6e85\uff0c\u8fd9\u5728\u5355\u76ee3D\u91cd\u5efa\u9886\u57df\u662f\u4e00\u4e2a\u91cd\u5927\u7a81\u7834\uff0c\u6709\u671b\u5927\u5e45\u7b80\u53163D\u5185\u5bb9\u521b\u5efa\u6d41\u7a0b\u3002 \"UItron: Foundational GUI Agent with Advanced Perception and Planning\" (Zhixiong Zeng et al.)\uff1a \u8fd9\u9879\u5de5\u4f5c\u4ee3\u8868\u4e86\u901a\u7528AI\u4ee3\u7406\u53d1\u5c55\u7684\u91cd\u8981\u4e00\u6b65\u3002\u6784\u5efa\u4e00\u4e2a\u80fd\u591f\u611f\u77e5\u548c\u89c4\u5212\u7684GUI\u4ee3\u7406\uff0c\u5bf9\u4e8e\u5b9e\u73b0\u66f4\u667a\u80fd\u3001\u66f4\u81ea\u4e3b\u7684\u4eba\u673a\u4ea4\u4e92\u5177\u6709\u6df1\u8fdc\u5f71\u54cd\u3002 \"Maybe you don't need a U-Net: convolutional feature upsampling for materials micrograph segmentation\" (Ronan Docherty et al.)\uff1a \u6311\u6218\u4e86U-Net\u8fd9\u4e00\u5728\u5206\u5272\u9886\u57df\u5e7f\u6cdb\u4f7f\u7528\u7684\u67b6\u6784\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u80fd\u66f4\u9ad8\u6548\u7684\u5377\u79ef\u7279\u5f81\u4e0a\u91c7\u6837\u65b9\u6cd5\uff0c\u5bf9\u4e8e\u8ffd\u6c42\u6a21\u578b\u6548\u7387\u7684\u7814\u7a76\u8005\u800c\u8a00\u5177\u6709\u91cd\u8981\u53c2\u8003\u4ef7\u503c\u3002 \"FLORA: Efficient Synthetic Data Generation for Object Detection in Low-Data Regimes via finetuning Flux LoRA\" (Alvaro Patricio et al.)\uff1a \u9488\u5bf9\u4f4e\u6570\u636e\u91cf\u573a\u666f\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u5408\u6210\u6570\u636e\u751f\u6210\u65b9\u6848\uff0c\u5176\u7ed3\u5408LoRA\u5fae\u8c03\u7684\u7b56\u7565\u5177\u6709\u5f88\u5f3a\u7684\u5b9e\u7528\u6027\u548c\u666e\u9002\u6027\u3002 \u65b0\u5174\u7814\u7a76\u65b9\u5411\u6216\u6280\u672f\uff1a \u6269\u6563\u6a21\u578b\u57283D\u751f\u6210\u4e0e\u91cd\u5efa\u4e2d\u7684\u6df1\u5ea6\u5e94\u7528\uff1a \u4e0d\u4ec5\u9650\u4e8e\u56fe\u50cf\u751f\u6210\uff0c\u6269\u6563\u6a21\u578b\u6b63\u6210\u4e3a3D\u5185\u5bb9\u521b\u5efa\uff08\u5982\u9ad8\u65af\u6cfc\u6e85\u3001\u59ff\u6001\u4f30\u8ba1\uff09\u7684\u5173\u952e\u6280\u672f\u3002 \u9762\u5411\u7279\u5b9a\u4efb\u52a1\u548c\u5927\u578b\u6a21\u578b\u7684\u5408\u6210\u6570\u636e\u751f\u6210\uff1a \u7ed3\u5408LoRA\u7b49\u9ad8\u6548\u5fae\u8c03\u6280\u672f\uff0c\u5408\u6210\u6570\u636e\u4e0d\u518d\u4ec5\u4ec5\u662f\u6570\u636e\u589e\u5f3a\uff0c\u800c\u662f\u6210\u4e3a\u5b9a\u5236\u5316\u8bad\u7ec3\u5927\u578b\u6a21\u578b\u3001\u89e3\u51b3\u7279\u5b9a\u9886\u57df\u6570\u636e\u74f6\u9888\u7684\u6838\u5fc3\u7b56\u7565\u3002 \u901a\u7528\u578b\u611f\u77e5-\u89c4\u5212AI\u4ee3\u7406\uff1a \u65e8\u5728\u6784\u5efa\u80fd\u591f\u7406\u89e3\u548c\u64cd\u4f5c\u590d\u6742\u73af\u5883\uff08\u5982GUI\u754c\u9762\uff09\u7684\u901a\u7528\u4ee3\u7406\uff0c\u662f\u8fc8\u5411\u66f4\u9ad8\u7ea7AI\u7684\u91cd\u8981\u4e00\u6b65\u3002 \u9c81\u68d2\u6027\u4e0e\u9886\u57df\u6cdb\u5316\u7684\u65b0\u8303\u5f0f\uff1a \u901a\u8fc7\u89e3\u8026\u8868\u793a\u5b66\u4e60\u548c\u6982\u7387\u5efa\u6a21\u7b49\u65b9\u6cd5\uff0c\u63d0\u5347\u6a21\u578b\u5728\u590d\u6742\u3001\u672a\u77e5\u771f\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u7684\u53ef\u9760\u6027\u3002 \u5efa\u8bae\u9605\u8bfb\u5168\u6587\u7684\u8bba\u6587\uff1a \"Complete Gaussian Splats from a Single Image with Denoising Diffusion Models\" (Ziwei Liao et al.)\uff1a \u5982\u679c\u60a8\u5bf93D\u89c6\u89c9\u3001\u65b0\u9896\u76843D\u91cd\u5efa\u65b9\u6cd5\u6216\u6269\u6563\u6a21\u578b\u611f\u5174\u8da3\uff0c\u8fd9\u7bc7\u8bba\u6587\u63d0\u4f9b\u4e86\u4ee4\u4eba\u5174\u594b\u7684\u8fdb\u5c55\u3002 \"UItron: Foundational GUI Agent with Advanced Perception and Planning\" (Zhixiong Zeng et al.)\uff1a \u5bf9\u4e8e\u5173\u6ce8\u901a\u7528AI\u3001AI\u4ee3\u7406\u6216\u4eba\u673a\u4ea4\u4e92\u7684\u7814\u7a76\u4eba\u5458\uff0c\u8fd9\u7bc7\u8bba\u6587\u5c55\u793a\u4e86\u672a\u6765AI\u5e94\u7528\u7684\u4e00\u4e2a\u91cd\u8981\u65b9\u5411\u3002 \"FLORA: Efficient Synthetic Data Generation for Object Detection in Low-Data Regimes via finetuning Flux LoRA\" (Alvaro Patricio et al.)\uff1a \u5bf9\u4e8e\u9762\u4e34\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u6216\u5e0c\u671b\u9ad8\u6548\u5229\u7528\u5408\u6210\u6570\u636e\u8fdb\u884c\u76ee\u6807\u68c0\u6d4b\u7684\u7814\u7a76\u8005\uff0c\u8fd9\u7bc7\u8bba\u6587\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002 \"Maybe you don't need a U-Net: convolutional feature upsampling for materials micrograph segmentation\" (Ronan Docherty et al.)\uff1a \u5982\u679c\u60a8\u5728\u5206\u5272\u4efb\u52a1\u4e2d\u5bfb\u6c42\u66f4\u9ad8\u6548\u7684\u6a21\u578b\u67b6\u6784\uff0c\u6216\u8005\u5bf9U-Net\u7684\u66ff\u4ee3\u65b9\u6848\u611f\u5174\u8da3\uff0c\u8fd9\u7bc7\u8bba\u6587\u503c\u5f97\u6df1\u5165\u7814\u7a76\u3002 \"DriveQA: Passing the Driving Knowledge Test\" (Maolin Wei et al.)\uff1a \u5bf9\u4e8e\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u7684VQA\u548c\u77e5\u8bc6\u63a8\u7406\u611f\u5174\u8da3\u7684\u7814\u7a76\u4eba\u5458\uff0c\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u57fa\u51c6\u548c\u6311\u6218\u3002","title":"Executive Summary"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-01/#table-of-contents","text":"Maybe you don't need a U-Net: convolutional feature upsampling for materials micrograph segmentation FLORA: Efficient Synthetic Data Generation for Object Detection in Low-Data Regimes via finetuning Flux LoRA The Rosario Dataset v2: Multimodal Dataset for Agricultural Robotics Complete Gaussian Splats from a Single Image with Denoising Diffusion Models Efficient Diffusion-Based 3D Human Pose Estimation with Hierarchical Temporal Pruning DriveQA: Passing the Driving Knowledge Test Domain Generalization in-the-Wild: Disentangling Classification from Domain-Aware Representations UItron: Foundational GUI Agent with Advanced Perception and Planning CAD2DMD-SET: Synthetic Generation Tool of Digital Measurement Device CAD Model Datasets for fine-tuning Large Vision-Language Models Mapping like a Skeptic: Probabilistic BEV Projection for Online HD Mapping","title":"Table of Contents"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-01/#papers","text":"","title":"Papers"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-01/#maybe-you-dont-need-a-u-net-convolutional-feature-upsampling-for-materials-micrograph-segmentation","text":"Authors: Ronan Docherty, Antonis Vamvakeros, Samuel J. Cooper Published: 2025-08-29 Categories: cs.CV, cond-mat.mtrl-sci Abstract: Feature foundation models - usually vision transformers - offer rich semantic descriptors of images, useful for downstream tasks such as (interactive) segmentation and object detection. For computational efficiency these descriptors are often patch-based, and so struggle to represent the fine features often present in micrographs; they also struggle with the large image sizes present in materials and biological image analysis. In this work, we train a convolutional neural network to upsample low-resolution (i.e, large patch size) foundation model features with reference to the input image. We apply this upsampler network (without any further training) to efficiently featurise and then segment a variety of microscopy images, including plant cells, a lithium-ion battery cathode and organic crystals. The richness of these upsampled features admits separation of hard to segment phases, like hairline cracks. We demonstrate that interactive segmentation with these deep features produces high-quality segmentations far faster and with far fewer labels than training or finetuning a more traditional convolutional network. Analysis: \u8fd9\u7bc7\u8bba\u6587\u7684\u6458\u8981\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u65e8\u5728\u89e3\u51b3\u5f53\u524d\u7279\u5f81\u57fa\u7840\u6a21\u578b\uff08\u901a\u5e38\u662fVision Transformer\uff09\u5728\u5904\u7406\u663e\u5fae\u56fe\u50cf\u65f6\u9762\u4e34\u7684\u6311\u6218\uff0c\u5373\u96be\u4ee5\u6355\u6349\u7cbe\u7ec6\u7279\u5f81\u548c\u5904\u7406\u5927\u5c3a\u5bf8\u56fe\u50cf\u3002\u4ee5\u4e0b\u662f\u8be6\u7ec6\u5206\u6790\uff1a","title":"Maybe you don't need a U-Net: convolutional feature upsampling for materials micrograph segmentation"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-01/#1-concise-summary","text":"\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u4e0a\u91c7\u6837\u5668\uff0c\u7528\u4e8e\u5c06\u4f4e\u5206\u8fa8\u7387\u7684\u57fa\u7840\u6a21\u578b\u7279\u5f81\u9ad8\u6548\u5730\u63d0\u5347\u81f3\u9ad8\u5206\u8fa8\u7387\uff0c\u5e76\u7ed3\u5408\u539f\u59cb\u8f93\u5165\u56fe\u50cf\u4fe1\u606f\u3002\u8be5\u4e0a\u91c7\u6837\u5668\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u5e94\u7528\u4e8e\u591a\u79cd\u663e\u5fae\u56fe\u50cf\uff0c\u5b9e\u73b0\u4e86\u5bf9\u590d\u6742\u6750\u6599\u663e\u5fae\u56fe\u50cf\u7684\u9ad8\u8d28\u91cf\u3001\u4ea4\u4e92\u5f0f\u5206\u5272\uff0c\u4e14\u6240\u9700\u6807\u6ce8\u8fdc\u5c11\u4e8e\u4f20\u7edf\u5377\u79ef\u7f51\u7edc\u3002","title":"1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u660e\u6458\u8981 (Concise Summary)"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-01/#2-key-innovation-or-methodological-approach","text":"\u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u5176\u63d0\u51fa\u7684 \u201c\u5377\u79ef\u7279\u5f81\u4e0a\u91c7\u6837\u5668\u201d\u7f51\u7edc \u3002\u8be5\u7f51\u7edc\u662f\u4e00\u4e2a\u72ec\u7acb\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff0c\u4e13\u95e8\u7528\u4e8e\uff1a 1. \u63a5\u6536\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\uff08\u5982Vision Transformer\uff09\u751f\u6210\u7684 \u4f4e\u5206\u8fa8\u7387\u3001\u7c97\u7c92\u5ea6\u7279\u5f81\u56fe \u3002 2. \u7ed3\u5408\u539f\u59cb\u8f93\u5165\u56fe\u50cf\u7684\u7cbe\u7ec6\u7eb9\u7406\u4fe1\u606f \u4f5c\u4e3a\u53c2\u8003\u3002 3. \u5c06\u8fd9\u4e9b\u7279\u5f81\u4e0a\u91c7\u6837\u4e3a \u9ad8\u5206\u8fa8\u7387\u3001\u8bed\u4e49\u4e30\u5bcc\u7684\u7279\u5f81\u8868\u793a \u3002 \u5173\u952e\u4e4b\u5904\u5728\u4e8e\uff0c\u8fd9\u4e2a\u4e0a\u91c7\u6837\u5668\u4e00\u65e6\u8bad\u7ec3\u5b8c\u6210\uff0c\u4fbf\u53ef\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u5fae\u8c03\uff0c\u76f4\u63a5\u5e94\u7528\u4e8e\u591a\u79cd\u4e0d\u540c\u7684\u663e\u5fae\u56fe\u50cf\u5206\u5272\u4efb\u52a1\uff08\u4f8b\u5982\u690d\u7269\u7ec6\u80de\u3001\u9502\u79bb\u5b50\u7535\u6c60\u9634\u6781\u3001\u6709\u673a\u6676\u4f53\uff09\uff0c\u4ece\u800c\u9ad8\u6548\u5730\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u6df1\u5ea6\u7279\u5f81\uff0c\u8fdb\u800c\u652f\u6301\u4ea4\u4e92\u5f0f\u5206\u5272\u3002\u8fd9\u4e0eU-Net\u7b49\u7aef\u5230\u7aef\u5206\u5272\u7f51\u7edc\u6216\u76f4\u63a5\u5fae\u8c03\u57fa\u7840\u6a21\u578b\u7684\u8303\u5f0f\u4e0d\u540c\uff0c\u5b83\u5c06\u7279\u5f81\u63d0\u53d6\u548c\u7cbe\u7ec6\u5316\u89e3\u8026\uff0c\u4e13\u6ce8\u4e8e\u63d0\u5347\u57fa\u7840\u6a21\u578b\u7684\u7a7a\u95f4\u5206\u8fa8\u7387\u3002","title":"2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u5b66\u65b9\u6cd5 (Key Innovation or Methodological Approach)"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-01/#3-potential-impact-on-the-field","text":"\u8be5\u7814\u7a76\u5bf9\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\uff0c\u7279\u522b\u662f\u79d1\u5b66\u56fe\u50cf\u5206\u6790\uff08\u5982\u6750\u6599\u79d1\u5b66\u3001\u751f\u7269\u533b\u5b66\uff09\u5177\u6709\u663e\u8457\u6f5c\u5728\u5f71\u54cd\uff1a * \u63d0\u5347\u57fa\u7840\u6a21\u578b\u5728\u5bc6\u96c6\u9884\u6d4b\u4efb\u52a1\u4e2d\u7684\u5b9e\u7528\u6027\uff1a \u5b83\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7b56\u7565\uff0c\u80fd\u591f\u5229\u7528\u73b0\u6709\u57fa\u7840\u6a21\u578b\u7684\u5f3a\u5927\u8bed\u4e49\u7406\u89e3\u80fd\u529b\uff0c\u540c\u65f6\u514b\u670d\u5176\u5728\u7a7a\u95f4\u5206\u8fa8\u7387\u4e0a\u7684\u56fa\u6709\u4e0d\u8db3\uff0c\u4f7f\u5176\u66f4\u9002\u7528\u4e8e\u9700\u8981\u50cf\u7d20\u7ea7\u7cbe\u5ea6\u7684\u4efb\u52a1\u3002 * \u52a0\u901f\u663e\u5fae\u56fe\u50cf\u5206\u6790\uff1a \u901a\u8fc7\u9ad8\u6548\u751f\u6210\u9ad8\u8d28\u91cf\u7279\u5f81\uff0c\u6709\u671b\u663e\u8457\u52a0\u901f\u663e\u5fae\u56fe\u50cf\u7684\u81ea\u52a8\u5316\u5206\u6790\u548c\u89e3\u91ca\u3002 * \u964d\u4f4e\u6807\u6ce8\u6210\u672c\uff0c\u63d0\u9ad8\u4ea4\u4e92\u5f0f\u5206\u5272\u6548\u7387\uff1a \u5f3a\u8c03\u4e86\u5728\u4ea4\u4e92\u5f0f\u5206\u5272\u4e2d\uff0c\u8be5\u65b9\u6cd5\u80fd\u4ee5\u66f4\u5feb\u7684\u901f\u5ea6\u548c\u66f4\u5c11\u7684\u6807\u7b7e\u5b9e\u73b0\u9ad8\u8d28\u91cf\u5206\u5272\uff0c\u8fd9\u5bf9\u4e8e\u6807\u6ce8\u6210\u672c\u9ad8\u6602\u7684\u4e13\u4e1a\u9886\u57df\uff08\u5982\u533b\u5b66\u3001\u6750\u6599\uff09\u5177\u6709\u5de8\u5927\u4ef7\u503c\u3002 * \u65b0\u7684\u6a21\u578b\u8303\u5f0f\uff1a \u201c\u4e00\u6b21\u8bad\u7ec3\uff0c\u591a\u4efb\u52a1\u5e94\u7528\u201d\u7684\u4e0a\u91c7\u6837\u5668\u8303\u5f0f\uff0c\u53ef\u80fd\u4e3a\u672a\u6765\u57fa\u7840\u6a21\u578b\u5728\u7279\u5b9a\u9886\u57df\uff08\u5c24\u5176\u662f\u79d1\u5b66\u56fe\u50cf\uff09\u7684\u90e8\u7f72\u548c\u5e94\u7528\u5f00\u8f9f\u65b0\u9014\u5f84\uff0c\u5373\u901a\u8fc7\u4e00\u4e2a\u901a\u7528\u7684\u7279\u5f81\u7cbe\u7ec6\u5316\u6a21\u5757\u6765\u9002\u5e94\u4e0d\u540c\u4efb\u52a1\u3002","title":"3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd (Potential Impact on the Field)"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-01/#4-related-areas-or-applications","text":"\u6750\u6599\u79d1\u5b66\u4e0e\u5de5\u7a0b\uff1a \u663e\u5fae\u7ed3\u6784\u5206\u6790\u3001\u7f3a\u9677\u68c0\u6d4b\uff08\u5982\u53d1\u4e1d\u88c2\u7eb9\uff09\u3001\u76f8\u5206\u79bb\u8bc6\u522b\u3001\u6676\u7c92\u8fb9\u754c\u5206\u5272\u3002 \u751f\u7269\u533b\u5b66\u56fe\u50cf\u5206\u6790\uff1a \u7ec6\u80de\u5206\u5272\u3001\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\u5206\u6790\u3001\u795e\u7ecf\u5143\u8ffd\u8e2a\u3001\u7535\u5b50\u663e\u5fae\u955c\u56fe\u50cf\u5904\u7406\u3001\u9ad8\u5206\u8fa8\u7387\u8367\u5149\u663e\u5fae\u56fe\u50cf\u5206\u6790\u3002 \u5de5\u4e1a\u68c0\u6d4b\uff1a \u8868\u9762\u7f3a\u9677\u68c0\u6d4b\u3001\u8d28\u91cf\u63a7\u5236\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u8bc6\u522b\u5fae\u5c0f\u7455\u75b5\u7684\u573a\u666f\u3002 \u9065\u611f\u56fe\u50cf\u5904\u7406\uff1a \u9ad8\u5206\u8fa8\u7387\u536b\u661f\u56fe\u50cf\u4e2d\u7684\u5730\u7269\u5206\u5272\u4e0e\u8bc6\u522b\uff0c\u7279\u522b\u662f\u5bf9\u7cbe\u7ec6\u5730\u7269\uff08\u5982\u9053\u8def\u3001\u5efa\u7b51\u7269\u8fb9\u7f18\uff09\u7684\u63d0\u53d6\u3002 \u4efb\u4f55\u9700\u8981\u7cbe\u7ec6\u50cf\u7d20\u7ea7\u7406\u89e3\u4e14\u56fe\u50cf\u5c3a\u5bf8\u8f83\u5927\u3001\u7279\u5f81\u7ec6\u8282\u4e30\u5bcc\u7684\u9886\u57df\u3002","title":"4. \u53ef\u80fd\u53d7\u76ca\u7684\u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 (Related Areas or Applications)"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-01/#5-limitations-that-can-be-inferred-from-the-abstract","text":"\u5bf9\u57fa\u7840\u6a21\u578b\u7279\u5f81\u8d28\u91cf\u7684\u4f9d\u8d56\uff1a \u8be5\u65b9\u6cd5\u7684\u6027\u80fd\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u53d6\u51b3\u4e8e\u6240\u4f7f\u7528\u7684\u57fa\u7840\u6a21\u578b\u751f\u6210\u7279\u5f81\u7684\u8d28\u91cf\u3002\u5982\u679c\u57fa\u7840\u6a21\u578b\u672a\u80fd\u6355\u6349\u5230\u67d0\u4e9b\u5173\u952e\u8bed\u4e49\u4fe1\u606f\uff0c\u4e0a\u91c7\u6837\u5668\u4e5f\u96be\u4ee5\u51ed\u7a7a\u521b\u9020\u3002 \u201c\u65e0\u9700\u8fdb\u4e00\u6b65\u8bad\u7ec3\u201d\u7684\u6743\u8861\uff1a \u5c3d\u7ba1\u4e0a\u91c7\u6837\u5668\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u5e94\u7528\u4e8e\u65b0\u4efb\u52a1\u662f\u5176\u4f18\u52bf\uff0c\u4f46\u8fd9\u53ef\u80fd\u610f\u5473\u7740\u5728\u7279\u5b9a\u6570\u636e\u96c6\u4e0a\uff0c\u5176\u6027\u80fd\u53ef\u80fd\u65e0\u6cd5\u8d85\u8d8a\u7ecf\u8fc7\u5145\u5206\u5fae\u8c03\u7684\u7aef\u5230\u7aef\u7f51\u7edc\uff08\u5982U-Net\uff09\uff0c\u5c24\u5176\u662f\u5728\u5168\u81ea\u52a8\u5206\u5272\u573a\u666f\u4e0b\u3002\u6458\u8981\u4e2d\u5f3a\u8c03\u7684\u662f\u4ea4\u4e92\u5f0f\u5206\u5272\u7684\u6548\u7387\u63d0\u5347\u3002 \u4e0eU-Net\u7684\u76f4\u63a5\u6027\u80fd\u5bf9\u6bd4\u672a\u660e\uff1a \u6807\u9898\u6697\u793a\u4e86\u5bf9U-Net\u7684\u66ff\u4ee3\uff0c\u4f46\u6458\u8981\u4e3b\u8981\u5f3a\u8c03\u5728\u4ea4\u4e92\u5f0f\u5206\u5272\u4e2d\u201c\u66f4\u5feb\u3001\u66f4\u5c11\u6807\u7b7e\u201d\u7684\u4f18\u52bf\uff0c\u5e76\u672a\u76f4\u63a5\u7ed9\u51fa\u5728\u5168\u81ea\u52a8\u3001\u975e\u4ea4\u4e92\u5f0f\u573a\u666f\u4e0b\u4e0eU-Net\u7b49\u4f20\u7edf\u65b9\u6cd5\u7684\u91cf\u5316\u6027\u80fd\u5bf9\u6bd4\u3002 \u8ba1\u7b97\u6210\u672c\u7684\u6f5c\u5728\u8003\u91cf\uff1a \u867d\u7136\u5f3a\u8c03\u4e86\u201c\u8ba1\u7b97\u6548\u7387\u201d\uff0c\u4f46\u7ed3\u5408\u57fa\u7840\u6a21\u578b\u548c\u72ec\u7acb\u7684\u4e0a\u91c7\u6837\u5668\u7f51\u7edc\uff0c\u5176\u603b\u4f53\u7684\u63a8\u7406\u65f6\u95f4\u6216\u5185\u5b58\u5360\u7528\u4e0e\u4e00\u4e2a\u4f18\u5316\u826f\u597d\u7684U-Net\u76f8\u6bd4\u5982\u4f55\uff0c\u4ecd\u9700\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u3002 \u5bf9\u8f93\u5165\u56fe\u50cf\u8d28\u91cf\u7684\u654f\u611f\u6027\uff1a \u4e0a\u91c7\u6837\u5668\u201c\u53c2\u8003\u8f93\u5165\u56fe\u50cf\u201d\u8fdb\u884c\u7cbe\u7ec6\u5316\uff0c\u8fd9\u610f\u5473\u7740\u5982\u679c\u8f93\u5165\u56fe\u50cf\u672c\u8eab\u5b58\u5728\u566a\u58f0\u6216\u4f2a\u5f71\uff0c\u53ef\u80fd\u4f1a\u5f71\u54cd\u4e0a\u91c7\u6837\u548c\u5206\u5272\u7684\u51c6\u786e\u6027\u3002 Key Findings: We demonstrate that interactive segmentation with these deep features produces high-quality segmentations far faster and with far fewer labels than training or finetuning a more traditional convolutional network. Links: PDF arXiv","title":"5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u7684\u5c40\u9650\u6027 (Limitations that can be inferred from the abstract)"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-01/#flora-efficient-synthetic-data-generation-for-object-detection-in-low-data-regimes-via-finetuning-flux-lora","text":"Authors: Alvaro Patricio, Atabak Dehban, Rodrigo Ventura Published: 2025-08-29 Categories: cs.CV Abstract: Recent advances in diffusion-based generative models have demonstrated significant potential in augmenting scarce datasets for object detection tasks. Nevertheless, most recent models rely on resource-intensive full fine-tuning of large-scale diffusion models, requiring enterprise-grade GPUs (e.g., NVIDIA V100) and thousands of synthetic images. To address these limitations, we propose Flux LoRA Augmentation (FLORA), a lightweight synthetic data generation pipeline. Our approach uses the Flux 1.1 Dev diffusion model, fine-tuned exclusively through Low-Rank Adaptation (LoRA). This dramatically reduces computational requirements, enabling synthetic dataset generation with a consumer-grade GPU (e.g., NVIDIA RTX 4090). We empirically evaluate our approach on seven diverse object detection datasets. Our results demonstrate that training object detectors with just 500 synthetic images generated by our approach yields superior detection performance compared to models trained on 5000 synthetic images from the ODGEN baseline, achieving improvements of up to 21.3% in mAP@.50:.95. This work demonstrates that it is possible to surpass state-of-the-art performance with far greater efficiency, as FLORA achieves superior results using only 10% of the data and a fraction of the computational cost. This work demonstrates that a quality and efficiency-focused approach is more effective than brute-force generation, making advanced synthetic data creation more practical and accessible for real-world scenarios. Analysis: \u4f5c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u4e13\u5bb6\uff0c\u6211\u5bf9\u8fd9\u7bc7\u8bba\u6587\u6458\u8981\u7684\u5206\u6790\u5982\u4e0b\uff1a","title":"FLORA: Efficient Synthetic Data Generation for Object Detection in Low-Data Regimes via finetuning Flux LoRA"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-01/#flora-efficient-synthetic-data-generation-for-object-detection-in-low-data-regimes-via-finetuning-flux-lora_1","text":"1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e (Concise Summary) FLORA\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u9ad8\u6548\u7684\u5408\u6210\u6570\u636e\u751f\u6210\u7ba1\u7ebf\uff0c\u4e13\u4e3a\u4f4e\u6570\u636e\u91cf\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u8bbe\u8ba1\u3002\u5b83\u901a\u8fc7\u5bf9Flux 1.1 Dev\u6269\u6563\u6a21\u578b\u8fdb\u884cLoRA\u5fae\u8c03\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\uff0c\u5e76\u8bc1\u660e\u4ec5\u7528\u5c11\u91cf\uff08500\u5f20\uff09\u9ad8\u8d28\u91cf\u5408\u6210\u56fe\u50cf\u5373\u53ef\u8d85\u8d8a\u73b0\u6709\u57fa\u7ebf\uff08ODGEN\uff09\u4f7f\u7528\u5927\u91cf\uff085000\u5f20\uff09\u56fe\u50cf\u7684\u6027\u80fd\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u68c0\u6d4b\u7cbe\u5ea6\u548c\u6548\u7387\u3002\u8fd9\u9879\u5de5\u4f5c\u5f3a\u8c03\u4e86\u8d28\u91cf\u548c\u6548\u7387\u4f18\u5148\u7684\u5408\u6210\u6570\u636e\u751f\u6210\u7b56\u7565\u4f18\u4e8e\u86ee\u529b\u751f\u6210\u3002 2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5 (Key Innovation or Methodological Approach) \u8be5\u8bba\u6587\u7684\u5173\u952e\u521b\u65b0\u5728\u4e8e\uff1a LoRA\u5fae\u8c03\u6269\u6563\u6a21\u578b\u4ee5\u5b9e\u73b0\u9ad8\u6548\u6027\uff1a \u6838\u5fc3\u521b\u65b0\u662f\u5c06\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\u6280\u672f\u5e94\u7528\u4e8eFlux 1.1 Dev\u6269\u6563\u6a21\u578b\u7684\u5fae\u8c03\uff0c\u4ee5\u751f\u6210\u76ee\u6807\u68c0\u6d4b\u6240\u9700\u7684\u5408\u6210\u6570\u636e\u3002\u76f8\u8f83\u4e8e\u4f20\u7edf\u9700\u8981\u5168\u91cf\u5fae\u8c03\u5927\u578b\u6269\u6563\u6a21\u578b\u7684\u65b9\u6cd5\uff0cLoRA\u4ec5\u66f4\u65b0\u5c11\u91cf\u53c2\u6570\uff0c\u6781\u5927\u5730\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\uff0c\u4f7f\u5f97\u5728\u6d88\u8d39\u7ea7GPU\uff08\u5982NVIDIA RTX 4090\uff09\u4e0a\u8fdb\u884c\u9ad8\u6548\u7684\u5408\u6210\u6570\u636e\u751f\u6210\u6210\u4e3a\u53ef\u80fd\u3002 \u201c\u8d28\u91cf\u4f18\u5148\u4e8e\u6570\u91cf\u201d\u7684\u5408\u6210\u6570\u636e\u7b56\u7565\uff1a \u8bba\u6587\u660e\u786e\u63d0\u51fa\u5e76\u9a8c\u8bc1\u4e86\u201c\u8d28\u91cf\u548c\u6548\u7387\u4f18\u5148\u201d\u7684\u7b56\u7565\u4f18\u4e8e\u201c\u86ee\u529b\u751f\u6210\u201d\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4ec5\u7528500\u5f20FLORA\u751f\u6210\u7684\u5408\u6210\u56fe\u50cf\uff0c\u5c31\u80fd\u5728mAP@.50:.95\u4e0a\u53d6\u5f97\u9ad8\u8fbe21.3%\u7684\u63d0\u5347\uff0c\u5e76\u8d85\u8d8a\u4f7f\u75285000\u5f20ODGEN\u57fa\u7ebf\u56fe\u50cf\u8bad\u7ec3\u7684\u6a21\u578b\u3002\u8fd9\u8868\u660eFLORA\u80fd\u591f\u751f\u6210\u66f4\u9ad8\u8d28\u91cf\u3001\u5bf9\u68c0\u6d4b\u5668\u8bad\u7ec3\u66f4\u6709\u6548\u7684\u5408\u6210\u6570\u636e\uff0c\u800c\u975e\u7b80\u5355\u5730\u8ffd\u6c42\u6570\u91cf\u3002 3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd (Potential Impact on the Field) \u5408\u6210\u6570\u636e\u751f\u6210\u7684\u6c11\u4e3b\u5316\uff1a \u6781\u5927\u5730\u964d\u4f4e\u4e86\u5408\u6210\u6570\u636e\u751f\u6210\u7684\u6280\u672f\u95e8\u69db\u548c\u8d44\u6e90\u9700\u6c42\uff0c\u4f7f\u66f4\u591a\u7814\u7a76\u8005\u548c\u5c0f\u578b\u56e2\u961f\u80fd\u591f\u5728\u6d88\u8d39\u7ea7\u786c\u4ef6\u4e0a\u5229\u7528\u5148\u8fdb\u7684\u6269\u6563\u6a21\u578b\u8fdb\u884c\u6570\u636e\u589e\u5f3a\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u6c11\u4e3b\u5316\u3002 \u6539\u53d8\u5408\u6210\u6570\u636e\u7814\u7a76\u8303\u5f0f\uff1a \u6311\u6218\u4e86\u4f20\u7edf\u4e0a\u8ba4\u4e3a\u5408\u6210\u6570\u636e\u91cf\u8d8a\u5927\u8d8a\u597d\u7684\u89c2\u5ff5\uff0c\u5f3a\u8c03\u4e86\u6570\u636e\u8d28\u91cf\u548c\u751f\u6210\u6548\u7387\u7684\u91cd\u8981\u6027\uff0c\u53ef\u80fd\u5f15\u5bfc\u672a\u6765\u5408\u6210\u6570\u636e\u7814\u7a76\u8f6c\u5411\u66f4\u7cbe\u7ec6\u3001\u66f4\u9ad8\u6548\u7684\u751f\u6210\u7b56\u7565\u3002 \u52a0\u901f\u4f4e\u6570\u636e\u91cf\u573a\u666f\u4e0b\u7684AI\u5e94\u7528\uff1a \u5728\u6570\u636e\u7a00\u7f3a\u573a\u666f\u4e0b\uff0cFLORA\u80fd\u591f\u4ee5\u66f4\u4f4e\u7684\u6210\u672c\u3001\u66f4\u5feb\u7684\u901f\u5ea6\u63d0\u4f9b\u9ad8\u8d28\u91cf\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u52a0\u901f\u4e86\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u5728\u7279\u5b9a\u9886\u57df\uff08\u5982\u533b\u7597\u3001\u5de5\u4e1a\u68c0\u6d4b\u3001\u5c0f\u4f17\u7269\u4f53\u8bc6\u522b\uff09\u7684\u5f00\u53d1\u548c\u90e8\u7f72\u3002 \u63a8\u52a8LoRA\u5728\u751f\u6210\u6a21\u578b\u4e2d\u7684\u5e94\u7528\uff1a \u8fdb\u4e00\u6b65\u8bc1\u660e\u4e86LoRA\u4f5c\u4e3a\u4e00\u79cd\u9ad8\u6548\u5fae\u8c03\u7b56\u7565\u5728\u5927\u578b\u751f\u6210\u6a21\u578b\uff08\u7279\u522b\u662f\u6269\u6563\u6a21\u578b\uff09\u4e2d\u7684\u5de8\u5927\u6f5c\u529b\uff0c\u53ef\u80fd\u542f\u53d1\u66f4\u591a\u5c06LoRA\u5e94\u7528\u4e8e\u5176\u4ed6\u590d\u6742\u751f\u6210\u4efb\u52a1\u7684\u7814\u7a76\u3002 4. \u76f8\u5173\u53d7\u76ca\u9886\u57df\u6216\u5e94\u7528 (Related Areas or Applications that Might Benefit) \u6240\u6709\u9762\u4e34\u6570\u636e\u7a00\u7f3a\u6311\u6218\u7684\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\uff1a \u4f8b\u5982\uff1a\u533b\u7597\u5f71\u50cf\u5206\u6790\uff08\u7f55\u89c1\u75be\u75c5\u68c0\u6d4b\uff09\u3001\u5de5\u4e1a\u7f3a\u9677\u68c0\u6d4b\uff08\u7279\u5b9a\u4ea7\u54c1\u7f3a\u9677\uff09\u3001\u519c\u4e1a\uff08\u7279\u5b9a\u4f5c\u7269\u75c5\u866b\u5bb3\uff09\u3001\u81ea\u52a8\u9a7e\u9a76\uff08\u957f\u5c3e\u4e8b\u4ef6\u6216\u7f55\u89c1\u7269\u4f53\uff09\u3001\u673a\u5668\u4eba\u89c6\u89c9\u7b49\u3002 \u8d44\u6e90\u53d7\u9650\u7684\u7814\u7a76\u673a\u6784\u6216\u4f01\u4e1a\uff1a \u65e0\u6cd5\u627f\u62c5\u6602\u8d35\u7684\u4f01\u4e1a\u7ea7GPU\u548c\u5927\u89c4\u6a21\u6570\u636e\u751f\u6210\u6210\u672c\u7684\u573a\u666f\u3002 \u5feb\u901f\u539f\u578b\u5f00\u53d1\u548c\u8fed\u4ee3\uff1a \u9700\u8981\u5feb\u901f\u751f\u6210\u7279\u5b9a\u573a\u666f\u6570\u636e\u4ee5\u9a8c\u8bc1\u6a21\u578b\u6216\u7b97\u6cd5\u7684\u573a\u666f\u3002 \u5176\u4ed6\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\uff1a \u8be5\u65b9\u6cd5\u7684\u6838\u5fc3\u601d\u60f3\uff08LoRA\u5fae\u8c03\u6269\u6563\u6a21\u578b\u4ee5\u9ad8\u6548\u751f\u6210\u9ad8\u8d28\u91cf\u6570\u636e\uff09\u4e5f\u53ef\u80fd\u63a8\u5e7f\u5230\u5176\u4ed6\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\uff0c\u5982\u8bed\u4e49\u5206\u5272\u3001\u5b9e\u4f8b\u5206\u5272\uff0c\u751a\u81f3\u56fe\u50cf\u751f\u6210\u4e0e\u7f16\u8f91\u7b49\uff0c\u53ea\u8981\u8fd9\u4e9b\u4efb\u52a1\u80fd\u4ece\u9ad8\u6548\u3001\u9ad8\u8d28\u91cf\u7684\u5408\u6210\u6570\u636e\u4e2d\u53d7\u76ca\u3002 5. \u6f5c\u5728\u5c40\u9650\u6027 (Limitations that Can Be Inferred from the Abstract) \u201c\u4f4e\u6570\u636e\u91cf\u573a\u666f\u201d\u7684\u5177\u4f53\u5b9a\u4e49\uff1a \u6458\u8981\u4e2d\u63d0\u5230\u7684\u201c\u4f4e\u6570\u636e\u91cf\u573a\u666f\u201d\u7684\u5177\u4f53\u5b9a\u4e49\u548c\u8303\u56f4\u5c1a\u4e0d\u660e\u786e\u3002\u4f8b\u5982\uff0c500\u5f20\u5408\u6210\u56fe\u50cf\u662f\u5426\u8db3\u4ee5\u5e94\u5bf9\u6240\u6709\u6781\u4f4e\u6570\u636e\u91cf\uff08\u5982\u53ea\u6709\u51e0\u5341\u5f20\u771f\u5b9e\u56fe\u50cf\uff09\u7684\u573a\u666f\uff1f \u57fa\u7ebf\u5bf9\u6bd4\u7684\u5168\u9762\u6027\uff1a \u867d\u7136\u4e0eODGEN\u57fa\u7ebf\u8fdb\u884c\u4e86\u5bf9\u6bd4\uff0c\u4f46ODGEN\u662f\u5426\u4ee3\u8868\u4e86\u5f53\u524d\u5408\u6210\u6570\u636e\u751f\u6210\u9886\u57df\u7684\u6700\u65b0SOTA\u65b9\u6cd5\uff1f\u662f\u5426\u6709\u5176\u4ed6\u66f4\u5148\u8fdb\u7684\u3001\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u5168\u91cf\u5fae\u8c03\u65b9\u6cd5\u672a\u88ab\u63d0\u53ca\u6216\u5bf9\u6bd4\uff1f\u8fd9\u4f1a\u5f71\u54cd\u5bf9\u201c\u8d85\u8d8aSOTA\u6027\u80fd\u201d\u8fd9\u4e00\u8bf4\u6cd5\u7684\u5224\u65ad\u3002 Flux\u6a21\u578b\u4f9d\u8d56\u6027\uff1a FLORA\u4f9d\u8d56\u4e8eFlux 1.1 Dev\u6a21\u578b\u3002\u8be5\u65b9\u6cd5\u5728\u5176\u4ed6\u6269\u6563\u6a21\u578b\uff08\u5982Stable Diffusion\u3001DALL-E\u7b49\uff09\u4e0a\u7684\u8868\u73b0\u5982\u4f55\uff1fLoRA\u5fae\u8c03\u7684\u6709\u6548\u6027\u662f\u5426\u4e0e\u57fa\u7840\u6269\u6563\u6a21\u578b\u7684\u67b6\u6784\u6216\u9884\u8bad\u7ec3\u6570\u636e\u5f3a\u76f8\u5173\uff1f \u5408\u6210\u6570\u636e\u8d28\u91cf\u7684\u6df1\u5c42\u8bc4\u4f30\uff1a mAP@.50:.95\u662f\u76ee\u6807\u68c0\u6d4b\u7684\u6807\u51c6\u6307\u6807\uff0c\u4f46\u5408\u6210\u56fe\u50cf\u672c\u8eab\u7684\u89c6\u89c9\u8d28\u91cf\u3001\u591a\u6837\u6027\u4ee5\u53ca\u5bf9\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u7684\u957f\u671f\u5f71\u54cd\uff0c\u5728\u6458\u8981\u4e2d\u672a\u8be6\u7ec6\u8bf4\u660e\u3002\u4f8b\u5982\uff0c\u5408\u6210\u6570\u636e\u662f\u5426\u5f15\u5165\u4e86\u65b0\u7684\u504f\u5dee\u6216\u4f2a\u5f71\uff1f \u771f\u5b9e\u4e16\u754c\u57df\u5dee\u8ddd\uff1a \u5408\u6210\u6570\u636e\u4e0e\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e4b\u95f4\u7684\u57df\u5dee\u8ddd\uff08domain gap\uff09\u59cb\u7ec8\u662f\u4e00\u4e2a\u6311\u6218\u3002\u5c3d\u7ba1FLORA\u63d0\u9ad8\u4e86\u6027\u80fd\uff0c\u4f46\u8fd9\u79cd\u63d0\u5347\u5728\u9762\u5bf9\u6781\u7aef\u771f\u5b9e\u4e16\u754c\u590d\u6742\u6027\u65f6\u7684\u9c81\u68d2\u6027\u5982\u4f55\uff1f\u6a21\u578b\u5728\u7eaf\u5408\u6210\u6570\u636e\u4e0a\u8bad\u7ec3\u540e\uff0c\u5728\u672a\u89c1\u8fc7\u7684\u771f\u5b9e\u6570\u636e\u4e0a\u7684\u8868\u73b0\u5982\u4f55\uff1f Key Findings: Our approach uses the Flux 1.1 Dev diffusion model, fine-tuned exclusively through Low-Rank Adaptation (LoRA). This work demonstrates that it is possible to surpass state-of-the-art performance with far greater efficiency, as FLORA achieves superior results using only 10% of the data and a fraction of the computational cost. Links: PDF arXiv","title":"FLORA: Efficient Synthetic Data Generation for Object Detection in Low-Data Regimes via finetuning Flux LoRA"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-01/#the-rosario-dataset-v2-multimodal-dataset-for-agricultural-robotics","text":"Authors: Nicolas Soncini, Javier Cremona, Erica Vidal, Maximiliano Garc\u00eda, Gast\u00f3n Castro, Taih\u00fa Pire Published: 2025-08-29 Categories: cs.RO, cs.CV, cs.SY, eess.SY, I.2.9 Abstract: We present a multi-modal dataset collected in a soybean crop field, comprising over two hours of recorded data from sensors such as stereo infrared camera, color camera, accelerometer, gyroscope, magnetometer, GNSS (Single Point Positioning, Real-Time Kinematic and Post-Processed Kinematic), and wheel odometry. This dataset captures key challenges inherent to robotics in agricultural environments, including variations in natural lighting, motion blur, rough terrain, and long, perceptually aliased sequences. By addressing these complexities, the dataset aims to support the development and benchmarking of advanced algorithms for localization, mapping, perception, and navigation in agricultural robotics. The platform and data collection system is designed to meet the key requirements for evaluating multi-modal SLAM systems, including hardware synchronization of sensors, 6-DOF ground truth and loops on long trajectories. We run multimodal state-of-the art SLAM methods on the dataset, showcasing the existing limitations in their application on agricultural settings. The dataset and utilities to work with it are released on https://cifasis.github.io/rosariov2/. Analysis: \u4f5c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u4e13\u5bb6\uff0c\u6211\u5bf9\u8fd9\u7bc7\u8bba\u6587\u6458\u8981\u7684\u5206\u6790\u5982\u4e0b\uff1a","title":"The Rosario Dataset v2: Multimodal Dataset for Agricultural Robotics"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-01/#the-rosario-dataset-v2-multimodal-dataset-for-agricultural-robotics_1","text":"1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u660e\u603b\u7ed3 (2-3\u53e5\u8bdd) Rosario Dataset v2 \u662f\u4e00\u4e2a\u9488\u5bf9\u519c\u4e1a\u673a\u5668\u4eba\u9886\u57df\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u5728\u771f\u5b9e\u7684\u5927\u8c46\u519c\u7530\u4e2d\u91c7\u96c6\uff0c\u5305\u542b\u8d85\u8fc7\u4e24\u5c0f\u65f6\u7684\u4e30\u5bcc\u4f20\u611f\u5668\u6570\u636e\uff0c\u5982\u7acb\u4f53\u7ea2\u5916\u76f8\u673a\u3001\u5f69\u8272\u76f8\u673a\u3001IMU\u3001\u591a\u79cdGNSS\u548c\u8f6e\u5f0f\u91cc\u7a0b\u8ba1\u3002\u8be5\u6570\u636e\u96c6\u65e8\u5728\u901a\u8fc7\u63d0\u4f9b\u786c\u4ef6\u540c\u6b65\u30016-DOF\u771f\u503c\u548c\u957f\u8f68\u8ff9\u5faa\u73af\u7b49\u5173\u952e\u7279\u6027\uff0c\u89e3\u51b3\u519c\u4e1a\u73af\u5883\u4e2d\uff08\u5982\u5149\u7167\u53d8\u5316\u3001\u8fd0\u52a8\u6a21\u7cca\u3001\u5d0e\u5c96\u5730\u5f62\u548c\u611f\u77e5\u6df7\u53e0\uff09\u7684\u6311\u6218\uff0c\u4ece\u800c\u652f\u6301\u548c\u57fa\u51c6\u6d4b\u8bd5\u5148\u8fdb\u7684\u5b9a\u4f4d\u3001\u5efa\u56fe\u3001\u611f\u77e5\u548c\u5bfc\u822a\u7b97\u6cd5\u3002\u4f5c\u8005\u8fd8\u5229\u7528\u8be5\u6570\u636e\u96c6\u8fd0\u884c\u4e86\u6700\u5148\u8fdb\u7684\u591a\u6a21\u6001SLAM\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u7b97\u6cd5\u5728\u519c\u4e1a\u5e94\u7528\u4e2d\u7684\u5c40\u9650\u6027\u3002 2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u8bba\u65b9\u6cd5 \u8be5\u8bba\u6587\u7684\u5173\u952e\u521b\u65b0\u5728\u4e8e \u521b\u5efa\u4e86\u4e00\u4e2a\u9ad8\u5ea6\u5168\u9762\u4e14\u6781\u5177\u6311\u6218\u6027\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u4e13\u95e8\u4e3a\u519c\u4e1a\u673a\u5668\u4eba\u5e94\u7528\u91cf\u8eab\u5b9a\u5236 \u3002\u5176\u65b9\u6cd5\u8bba\u4f53\u73b0\u5728\uff1a \u4e30\u5bcc\u7684\u591a\u6a21\u6001\u4f20\u611f\u5668\u878d\u5408\uff1a \u7ed3\u5408\u4e86\u7acb\u4f53\u7ea2\u5916\u76f8\u673a\uff08\u5bf9\u5149\u7167\u53d8\u5316\u9c81\u68d2\uff09\u3001\u5f69\u8272\u76f8\u673a\u3001\u60ef\u6027\u6d4b\u91cf\u5355\u5143\uff08IMU\uff09\u3001\u591a\u79cdGNSS\uff08\u5305\u62ec\u9ad8\u7cbe\u5ea6\u7684RTK\u548cPPK\uff09\u4ee5\u53ca\u8f6e\u5f0f\u91cc\u7a0b\u8ba1\uff0c\u63d0\u4f9b\u4e86\u6781\u5176\u591a\u6837\u5316\u7684\u6570\u636e\u6d41\uff0c\u8fd9\u5bf9\u4e8e\u5f00\u53d1\u9c81\u68d2\u7684\u4f20\u611f\u5668\u878d\u5408\u7b97\u6cd5\u81f3\u5173\u91cd\u8981\u3002 \u9488\u5bf9\u519c\u4e1a\u73af\u5883\u7684\u7279\u5b9a\u6311\u6218\uff1a \u6570\u636e\u96c6\u7684\u8bbe\u8ba1\u548c\u91c7\u96c6\u660e\u786e\u8003\u8651\u4e86\u519c\u4e1a\u73af\u5883\u7684\u72ec\u7279\u96be\u9898\uff0c\u5982\u81ea\u7136\u5149\u7167\u5267\u70c8\u53d8\u5316\u3001\u5d0e\u5c96\u5730\u5f62\u5bfc\u81f4\u7684\u8fd0\u52a8\u6a21\u7cca\u3001\u4ee5\u53ca\u519c\u4f5c\u7269\u884c\u95f4\u91cd\u590d\u6a21\u5f0f\u9020\u6210\u7684\u201c\u611f\u77e5\u6df7\u53e0\u201d\uff08perceptually aliased sequences\uff09\uff0c\u8fd9\u4e9b\u90fd\u662f\u4f20\u7edfCV\u548cSLAM\u7b97\u6cd5\u7684\u75db\u70b9\u3002 \u9ad8\u6807\u51c6\u7684\u6570\u636e\u8d28\u91cf\u548c\u57fa\u51c6\u6d4b\u8bd5\u652f\u6301\uff1a \u5f3a\u8c03\u4e86\u4f20\u611f\u5668\u786c\u4ef6\u540c\u6b65\u3001\u63d0\u4f9b\u9ad8\u7cbe\u5ea6\u76846-DOF\uff08\u516d\u81ea\u7531\u5ea6\uff09\u5730\u9762\u771f\u503c\uff0c\u4ee5\u53ca\u5305\u542b\u957f\u8f68\u8ff9\u4e0a\u7684\u5faa\u73af\uff08loops\uff09\uff0c\u8fd9\u4e9b\u90fd\u662f\u8bc4\u4f30\u548c\u5f00\u53d1\u591a\u6a21\u6001SLAM\u7cfb\u7edf\u4e0d\u53ef\u6216\u7f3a\u7684\u8981\u7d20\u3002\u901a\u8fc7\u8fd0\u884cSOTA SLAM\u65b9\u6cd5\u5e76\u5c55\u793a\u5176\u5c40\u9650\u6027\uff0c\u4f5c\u8005\u4e0d\u4ec5\u9a8c\u8bc1\u4e86\u6570\u636e\u96c6\u7684\u6311\u6218\u6027\uff0c\u4e5f\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u660e\u786e\u7684\u65b9\u5411\u3002 3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd \u52a0\u901f\u519c\u4e1a\u673a\u5668\u4eba\u7814\u7a76\uff1a \u63d0\u4f9b\u4e86\u4e00\u4e2a\u6807\u51c6\u5316\u3001\u9ad8\u6311\u6218\u6027\u4e14\u516c\u5f00\u53ef\u7528\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5c06\u6781\u5927\u5730\u52a0\u901f\u519c\u4e1a\u9886\u57df\u5b9a\u4f4d\u3001\u5efa\u56fe\u3001\u611f\u77e5\u548c\u5bfc\u822a\u7b97\u6cd5\u7684\u5f00\u53d1\u548c\u8bc4\u4f30\u3002 \u63a8\u52a8\u9c81\u68d2\u7b97\u6cd5\u53d1\u5c55\uff1a \u8feb\u4f7f\u7814\u7a76\u4eba\u5458\u5f00\u53d1\u80fd\u591f\u5e94\u5bf9\u771f\u5b9e\u4e16\u754c\u519c\u4e1a\u73af\u5883\u590d\u6742\u6027\u7684\u66f4\u9c81\u68d2\u3001\u66f4\u6cdb\u5316\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\uff0c\u5c24\u5176\u662f\u5728\u5149\u7167\u53d8\u5316\u3001\u7eb9\u7406\u91cd\u590d\u548c\u5d0e\u5c96\u5730\u5f62\u4e0b\u7684\u8868\u73b0\u3002 \u5f25\u5408\u7406\u8bba\u4e0e\u5b9e\u8df5\u7684\u9e3f\u6c9f\uff1a \u901a\u8fc7\u63d0\u4f9b\u771f\u5b9e\u4e16\u754c\u7684\u590d\u6742\u6570\u636e\uff0c\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u5c06\u5b9e\u9a8c\u5ba4\u6210\u679c\u66f4\u597d\u5730\u5e94\u7528\u4e8e\u5b9e\u9645\u7684\u519c\u4e1a\u81ea\u52a8\u5316\u548c\u7cbe\u51c6\u519c\u4e1a\u573a\u666f\u3002 \u4fc3\u8fdb\u591a\u4f20\u611f\u5668\u878d\u5408\u548cSLAM\u6280\u672f\u8fdb\u6b65\uff1a \u6570\u636e\u96c6\u7684\u591a\u6a21\u6001\u7279\u6027\u5c06\u63a8\u52a8\u591a\u4f20\u611f\u5668\u878d\u5408\u6280\u672f\u7684\u53d1\u5c55\uff0c\u7279\u522b\u662f\u5728\u89c6\u89c9-\u60ef\u6027-GNSS-\u91cc\u7a0b\u8ba1\u878d\u5408SLAM\u65b9\u9762\uff0c\u4ee5\u5b9e\u73b0\u66f4\u7cbe\u786e\u548c\u53ef\u9760\u7684\u5b9a\u4f4d\u3002 \u4e3a\u65b0\u7814\u7a76\u65b9\u5411\u63d0\u4f9b\u57fa\u7840\uff1a \u6570\u636e\u96c6\u4e2d\u7684\u6311\u6218\uff08\u5982\u611f\u77e5\u6df7\u53e0\uff09\u53ef\u80fd\u4f1a\u6fc0\u53d1\u65b0\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u4f8b\u5982\u57fa\u4e8e\u8bed\u4e49\u4fe1\u606f\u7684\u5b9a\u4f4d\u3001\u62d3\u6251\u5efa\u56fe\u6216\u66f4\u5148\u8fdb\u7684\u5faa\u73af\u95ed\u5408\u6280\u672f\u3002 4. \u53ef\u80fd\u53d7\u76ca\u7684\u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 \u519c\u4e1a\u673a\u5668\u4eba\u4e0e\u81ea\u52a8\u5316\uff1a \u8fd9\u662f\u6700\u76f4\u63a5\u7684\u53d7\u76ca\u9886\u57df\uff0c\u5305\u62ec\u81ea\u4e3b\u62d6\u62c9\u673a\u3001\u55b7\u6d12\u673a\u5668\u4eba\u3001\u91c7\u6458\u673a\u5668\u4eba\u3001\u4f5c\u7269\u76d1\u6d4b\u65e0\u4eba\u8f66\u7b49\u3002 \u540c\u6b65\u5b9a\u4f4d\u4e0e\u5efa\u56fe (SLAM)\uff1a \u7279\u522b\u662f\u591a\u6a21\u6001SLAM\u3001\u89c6\u89c9-\u60ef\u6027SLAM\u3001\u4ee5\u53ca\u5728\u4f4e\u7eb9\u7406\u6216\u91cd\u590d\u7eb9\u7406\u73af\u5883\u4e0b\u7684SLAM\u3002 \u8ba1\u7b97\u673a\u89c6\u89c9\uff1a \u6237\u5916\u573a\u666f\u7406\u89e3\u3001\u4f5c\u7269/\u6742\u8349\u68c0\u6d4b\u4e0e\u5206\u5272\u30013D\u91cd\u5efa\u3001\u8fd0\u52a8\u4f30\u8ba1\u3001\u5149\u6d41\u5206\u6790\u3001\u4ee5\u53ca\u5728\u6076\u52a3\u5149\u7167\u6761\u4ef6\u4e0b\u7684\u56fe\u50cf\u5904\u7406\u3002 \u673a\u5668\u5b66\u4e60\u4e0e\u6df1\u5ea6\u5b66\u4e60\uff1a \u8bad\u7ec3\u7528\u4e8e\u519c\u4e1a\u73af\u5883\u7684\u9c81\u68d2\u611f\u77e5\u6a21\u578b\uff0c\u4f8b\u5982\u7528\u4e8e\u4f5c\u7269\u5065\u5eb7\u76d1\u6d4b\u3001\u75c5\u866b\u5bb3\u8bc6\u522b\u3001\u4ea7\u91cf\u4f30\u8ba1\u7b49\u3002 \u4f20\u611f\u5668\u878d\u5408\uff1a \u5f00\u53d1\u548c\u6d4b\u8bd5\u878d\u5408\u5f02\u6784\u4f20\u611f\u5668\u6570\u636e\u4ee5\u63d0\u9ad8\u7cfb\u7edf\u6027\u80fd\u7684\u7b97\u6cd5\u3002 \u6237\u5916\u81ea\u4e3b\u5bfc\u822a\uff1a \u4efb\u4f55\u9700\u8981\u5728\u975e\u7ed3\u6784\u5316\u3001\u52a8\u6001\u548c\u6311\u6218\u6027\u6237\u5916\u73af\u5883\u4e2d\u8fdb\u884c\u81ea\u4e3b\u5bfc\u822a\u7684\u7cfb\u7edf\uff08\u4f8b\u5982\uff0c\u5efa\u7b51\u673a\u5668\u4eba\u3001\u91c7\u77ff\u673a\u5668\u4eba\u3001\u6797\u4e1a\u673a\u5668\u4eba\uff09\u3002 \u9ad8\u7cbe\u5ea6GNSS\u5e94\u7528\uff1a \u7ed3\u5408RTK/PPK\u6570\u636e\u8fdb\u884c\u9ad8\u7cbe\u5ea6\u5b9a\u4f4d\u548c\u5730\u56fe\u6821\u51c6\u7684\u7814\u7a76\u3002 5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u7684\u5c40\u9650\u6027 \u4f5c\u7269\u548c\u73af\u5883\u7279\u5f02\u6027\uff1a \u6570\u636e\u96c6\u4ec5\u5728\u5927\u8c46\u519c\u7530\u4e2d\u91c7\u96c6\u3002\u867d\u7136\u5927\u8c46\u662f\u91cd\u8981\u4f5c\u7269\uff0c\u4f46\u5176\u89c6\u89c9\u7279\u5f81\u3001\u751f\u957f\u6a21\u5f0f\u548c\u519c\u7530\u7ed3\u6784\u53ef\u80fd\u4e0e\u5176\u4ed6\u4f5c\u7269\uff08\u5982\u7389\u7c73\u3001\u5c0f\u9ea6\u3001\u679c\u56ed\u3001\u6e29\u5ba4\u852c\u83dc\uff09\u6709\u663e\u8457\u5dee\u5f02\u3002\u8fd9\u9650\u5236\u4e86\u5176\u5728\u5176\u4ed6\u519c\u4e1a\u573a\u666f\u4e2d\u7684\u76f4\u63a5\u6cdb\u5316\u80fd\u529b\u3002 \u5730\u7406\u4f4d\u7f6e\u548c\u6c14\u5019\u9650\u5236\uff1a \u6570\u636e\u96c6\u5728\u201cRosario\u201d\u5730\u533a\u91c7\u96c6\uff0c\u8fd9\u610f\u5473\u7740\u5176\u53ef\u80fd\u53cd\u6620\u7279\u5b9a\u5730\u7406\u533a\u57df\u7684\u6c14\u5019\u3001\u571f\u58e4\u7c7b\u578b\u548c\u5149\u7167\u6761\u4ef6\u3002\u5728\u5176\u4ed6\u5730\u533a\uff0c\u73af\u5883\u56e0\u7d20\u53ef\u80fd\u5927\u76f8\u5f84\u5ead\u3002 \u5e73\u53f0\u7279\u5b9a\u6027\uff1a \u6570\u636e\u662f\u7531\u4e00\u4e2a\u7279\u5b9a\u7684\u201c\u5e73\u53f0\u548c\u6570\u636e\u91c7\u96c6\u7cfb\u7edf\u201d\u6536\u96c6\u7684\u3002\u4f20\u611f\u5668\u7684\u5177\u4f53\u914d\u7f6e\u3001\u5b89\u88c5\u9ad8\u5ea6\u3001\u89c6\u89d2\u4ee5\u53ca\u5e73\u53f0\u81ea\u8eab\u7684\u8fd0\u52a8\u7279\u6027\u53ef\u80fd\u65e0\u6cd5\u5b8c\u5168\u4ee3\u8868\u6240\u6709\u519c\u4e1a\u673a\u5668\u4eba\u7cfb\u7edf\u3002 \u4e3b\u8981\u4fa7\u91cd\u4e8eSLAM/\u5bfc\u822a\uff1a \u5c3d\u7ba1\u6570\u636e\u4e30\u5bcc\uff0c\u4f46\u6458\u8981\u4e2d\u660e\u786e\u5f3a\u8c03\u4e86\u5bf9SLAM\u7cfb\u7edf\u8bc4\u4f30\u7684\u5173\u952e\u8981\u6c42\uff08\u786c\u4ef6\u540c\u6b65\u30016-DOF\u771f\u503c\u3001\u5faa\u73af\uff09\u3002\u8fd9\u8868\u660e\u6570\u636e\u96c6\u7684\u8bbe\u8ba1\u548c\u6807\u6ce8\u53ef\u80fd\u66f4\u4fa7\u91cd\u4e8e\u5b9a\u4f4d\u548c\u5efa\u56fe\u4efb\u52a1\uff0c\u5bf9\u4e8e\u5176\u4ed6\u7ec6\u7c92\u5ea6\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\uff08\u5982\u7cbe\u786e\u7684\u690d\u7269\u8868\u578b\u5206\u6790\u3001\u75c5\u5bb3\u65e9\u671f\u68c0\u6d4b\uff09\u53ef\u80fd\u9700\u8981\u989d\u5916\u7684\u6807\u6ce8\u6216\u5904\u7406\u3002 \u672a\u63d0\u53ca\u5176\u4ed6\u52a8\u6001\u969c\u788d\u7269\uff1a \u6458\u8981\u4e2d\u6ca1\u6709\u660e\u786e\u8bf4\u660e\u6570\u636e\u96c6\u4e2d\u662f\u5426\u5305\u542b\u9664\u4e86\u673a\u5668\u4eba\u672c\u8eab\u4e4b\u5916\u7684\u5176\u4ed6\u52a8\u6001\u969c\u788d\u7269\uff08\u4f8b\u5982\uff0c\u519c\u573a\u5de5\u4eba\u3001\u91ce\u751f\u52a8\u7269\u3001\u5176\u4ed6\u519c\u673a\uff09\u3002\u5728\u771f\u5b9e\u7684\u519c\u4e1a\u73af\u5883\u4e2d\uff0c\u8fd9\u4e9b\u52a8\u6001\u56e0\u7d20\u5bf9\u5bfc\u822a\u548c\u5b89\u5168\u81f3\u5173\u91cd\u8981\u3002 \u6570\u636e\u91cf\uff1a \u201c\u8d85\u8fc7\u4e24\u5c0f\u65f6\u201d\u7684\u6570\u636e\u91cf\u5bf9\u4e8e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8bad\u7ec3\u6765\u8bf4\uff0c\u867d\u7136\u4e0d\u9519\uff0c\u4f46\u53ef\u80fd\u4e0d\u5982\u4e00\u4e9b\u5927\u89c4\u6a21\u57ce\u5e02\u6570\u636e\u96c6\u90a3\u6837\u5e9e\u5927\uff0c\u8fd9\u5728\u67d0\u4e9b\u9700\u8981\u6d77\u91cf\u6570\u636e\u7684\u4efb\u52a1\u4e0a\u53ef\u80fd\u6784\u6210\u9650\u5236\u3002 Key Findings: We present a multi-modal dataset collected in a soybean crop field, comprising over two hours of recorded data from sensors such as stereo infrared camera, color camera, accelerometer, gyroscope, magnetometer, GNSS (Single Point Positioning, Real-Time Kinematic and Post-Processed Kinematic), and wheel odometry. Links: PDF arXiv","title":"\u8bba\u6587\u6458\u8981\u5206\u6790\uff1aThe Rosario Dataset v2: Multimodal Dataset for Agricultural Robotics"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-01/#complete-gaussian-splats-from-a-single-image-with-denoising-diffusion-models","text":"Authors: Ziwei Liao, Mohamed Sayed, Steven L. Waslander, Sara Vicente, Daniyar Turmukhambetov, Michael Firman Published: 2025-08-29 Categories: cs.CV, cs.AI, cs.RO Abstract: Gaussian splatting typically requires dense observations of the scene and can fail to reconstruct occluded and unobserved areas. We propose a latent diffusion model to reconstruct a complete 3D scene with Gaussian splats, including the occluded parts, from only a single image during inference. Completing the unobserved surfaces of a scene is challenging due to the ambiguity of the plausible surfaces. Conventional methods use a regression-based formulation to predict a single \"mode\" for occluded and out-of-frustum surfaces, leading to blurriness, implausibility, and failure to capture multiple possible explanations. Thus, they often address this problem partially, focusing either on objects isolated from the background, reconstructing only visible surfaces, or failing to extrapolate far from the input views. In contrast, we propose a generative formulation to learn a distribution of 3D representations of Gaussian splats conditioned on a single input image. To address the lack of ground-truth training data, we propose a Variational AutoReconstructor to learn a latent space only from 2D images in a self-supervised manner, over which a diffusion model is trained. Our method generates faithful reconstructions and diverse samples with the ability to complete the occluded surfaces for high-quality 360-degree renderings. Analysis: \u8fd9\u7bc7\u8bba\u6587\u7684\u6458\u8981\u5c55\u793a\u4e86\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u4e00\u4e2a\u975e\u5e38\u6709\u8da3\u4e14\u91cd\u8981\u7684\u8fdb\u5c55\u3002\u4ee5\u4e0b\u662f\u6211\u7684\u8be6\u7ec6\u5206\u6790\uff1a","title":"Complete Gaussian Splats from a Single Image with Denoising Diffusion Models"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-01/#1-concise-summary_1","text":"\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff0c\u80fd\u591f\u4ec5\u4ece\u5355\u5f20\u56fe\u50cf\u63a8\u65ad\u51fa\u5b8c\u6574\u76843D\u573a\u666f\uff0c\u5305\u62ec\u88ab\u906e\u6321\u548c\u672a\u89c2\u5bdf\u5230\u7684\u533a\u57df\uff0c\u5e76\u4ee5\u9ad8\u65af\u6cfc\u6e85\uff08Gaussian Splats\uff09\u7684\u5f62\u5f0f\u8868\u793a\u3002\u4e0e\u4f20\u7edf\u56de\u5f52\u65b9\u6cd5\u751f\u6210\u5355\u4e00\u6a21\u7cca\u6216\u4e0d\u771f\u5b9e\u7684\u91cd\u5efa\u4e0d\u540c\uff0c\u8be5\u65b9\u6cd5\u91c7\u7528\u751f\u6210\u5f0f\u516c\u5f0f\u6765\u5b66\u4e603D\u8868\u793a\u7684\u5206\u5e03\uff0c\u4ece\u800c\u5b9e\u73b0\u5fe0\u5b9e\u4e14\u591a\u6837\u5316\u7684\u573a\u666f\u8865\u5168\uff0c\u652f\u6301\u9ad8\u8d28\u91cf\u7684360\u5ea6\u6e32\u67d3\u3002","title":"1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u660e\u6458\u8981 (Concise Summary)"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-01/#2-key-innovation-or-methodological-approach_1","text":"\u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u5176 \u751f\u6210\u5f0f\u516c\u5f0f \uff0c\u5b83\u4e0d\u662f\u9884\u6d4b\u5355\u4e00\u76843D\u91cd\u5efa\uff0c\u800c\u662f\u5b66\u4e60\u7ed9\u5b9a\u5355\u5f20\u8f93\u5165\u56fe\u50cf\u4e0b3D\u9ad8\u65af\u6cfc\u6e85\u8868\u793a\u7684 \u5206\u5e03 \u3002\u8fd9\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u6355\u6349\u88ab\u906e\u6321\u548c\u672a\u89c2\u5bdf\u533a\u57df\u7684\u591a\u79cd\u5408\u7406\u89e3\u91ca\uff0c\u4ece\u800c\u907f\u514d\u4e86\u4f20\u7edf\u56de\u5f52\u65b9\u6cd5\u5bfc\u81f4\u7684\u6a21\u7cca\u548c\u4e0d\u771f\u5b9e\u3002\u4e3a\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u4e86 \u6f5c\u5728\u6269\u6563\u6a21\u578b \uff0c\u5e76\u5f15\u5165\u4e86 \u53d8\u5206\u81ea\u91cd\u5efa\u5668\uff08Variational AutoReconstructor, VAR\uff09 \uff0c\u4ee5\u81ea\u76d1\u7763\u65b9\u5f0f\u4ec5\u4ece2D\u56fe\u50cf\u5b66\u4e60\u4e00\u4e2a\u5408\u9002\u7684\u6f5c\u5728\u7a7a\u95f4\uff0c\u4ece\u800c\u89e3\u51b3\u4e863D\u771f\u503c\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\u3002","title":"2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u8bba (Key Innovation or Methodological Approach)"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-01/#3-potential-impact-on-the-field_1","text":"\u8fd9\u9879\u7814\u7a76\u6709\u671b\u663e\u8457\u63a8\u52a8 \u5355\u56fe\u50cf3D\u573a\u666f\u91cd\u5efa \u9886\u57df\u7684\u53d1\u5c55\uff0c\u4f7f\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u5728\u7f3a\u4e4f\u5bc6\u96c6\u591a\u89c6\u89d2\u6570\u636e\u7684\u5b9e\u9645\u5e94\u7528\u4e2d\u66f4\u5177\u5b9e\u7528\u6027\u3002\u901a\u8fc7\u63d0\u4f9b \u5b8c\u6574\u4e14\u591a\u6837\u5316 \u76843D\u573a\u666f\u8868\u793a\uff0c\u5b83\u80fd\u6781\u5927\u5730\u964d\u4f4e3D\u5185\u5bb9\u521b\u5efa\u7684\u6570\u636e\u91c7\u96c6\u95e8\u69db\uff0c\u5e76\u4e3a\u9700\u8981\u7406\u89e3\u88ab\u906e\u6321\u573a\u666f\u90e8\u5206\u7684 \u673a\u5668\u4eba\u3001AR/VR\u548c\u81ea\u52a8\u9a7e\u9a76 \u7b49\u9886\u57df\u63d0\u4f9b\u66f4\u9c81\u68d2\u76843D\u611f\u77e5\u80fd\u529b\u3002\u5b83\u5c06\u5355\u56fe\u50cf3D\u91cd\u5efa\u4ece\u5355\u4e00\u786e\u5b9a\u6027\u8f93\u51fa\u63a8\u5411\u4e86\u751f\u6210\u5f0f\u3001\u591a\u6a21\u6001\u8f93\u51fa\u7684\u65b0\u8303\u5f0f\u3002","title":"3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd (Potential Impact on the Field)"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-01/#4-related-areas-or-applications_1","text":"\u673a\u5668\u4eba\u5b66\u4e0e\u81ea\u52a8\u9a7e\u9a76\uff1a \u63d0\u5347\u5bf9\u590d\u6742\u73af\u5883\u76843D\u611f\u77e5\u80fd\u529b\uff0c\u5c24\u5176\u662f\u5728\u90e8\u5206\u906e\u6321\u6216\u89c6\u89d2\u53d7\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u6709\u52a9\u4e8e\u8def\u5f84\u89c4\u5212\u3001\u907f\u969c\u548c\u4eba\u673a\u4ea4\u4e92\u3002 \u589e\u5f3a\u73b0\u5b9e\uff08AR\uff09\u4e0e\u865a\u62df\u73b0\u5b9e\uff08VR\uff09\uff1a \u5b9e\u73b0\u66f4\u903c\u771f\u3001\u66f4\u5b8c\u6574\u7684\u865a\u62df\u573a\u666f\u6784\u5efa\u548c\u5185\u5bb9\u751f\u6210\uff0c\u63d0\u5347\u6c89\u6d78\u5f0f\u4f53\u9a8c\u3002 3D\u5185\u5bb9\u521b\u4f5c\uff1a \u6781\u5927\u5730\u7b80\u5316\u4ece2D\u56fe\u50cf\u751f\u6210\u5b8c\u65743D\u8d44\u4ea7\u7684\u6d41\u7a0b\uff0c\u964d\u4f4e\u6210\u672c\u548c\u65f6\u95f4\u3002 \u6570\u5b57\u5b6a\u751f\uff1a \u4ece\u6709\u9650\u7684\u56fe\u50cf\u6570\u636e\u4e2d\u6784\u5efa\u66f4\u5168\u9762\u7684\u7269\u7406\u4e16\u754c\u6570\u5b57\u6a21\u578b\u3002 \u9065\u611f\u4e0e\u6d4b\u7ed8\uff1a \u4ece\u5355\u5f20\u822a\u7a7a\u6216\u536b\u661f\u56fe\u50cf\u4e2d\u63a8\u65ad\u51fa\u66f4\u5b8c\u6574\u7684\u5730\u5f62\u6216\u5efa\u7b513D\u6a21\u578b\u3002","title":"4. \u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 (Related Areas or Applications)"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-01/#5-limitations-inferred-from-the-abstract","text":"\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u5c40\u9650\u6027\uff1a \u5c3d\u7ba1\u901a\u8fc7VAR\u89e3\u51b3\u4e863D\u771f\u503c\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u4f46\u81ea\u76d1\u7763\u5b66\u4e60\u83b7\u5f97\u7684\u6f5c\u5728\u7a7a\u95f4\u7684\u8d28\u91cf\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u53ef\u80fd\u4ecd\u4e0d\u5982\u76f4\u63a5\u4f7f\u7528\u5927\u91cf\u9ad8\u8d28\u91cf3D\u771f\u503c\u6570\u636e\u8fdb\u884c\u76d1\u7763\u5b66\u4e60\u3002 \u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\uff1a \u6269\u6563\u6a21\u578b\uff0c\u5c24\u5176\u662f\u57283D\u751f\u6210\u4efb\u52a1\u4e2d\uff0c\u901a\u5e38\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u9636\u6bb5\u90fd\u9700\u8981\u5927\u91cf\u7684\u8ba1\u7b97\u8d44\u6e90\u548c\u65f6\u95f4\u3002 \u5355\u56fe\u50cf\u8f93\u5165\u7684\u654f\u611f\u6027\uff1a \u6a21\u578b\u7684\u6027\u80fd\u53ef\u80fd\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u5355\u5f20\u8f93\u5165\u56fe\u50cf\u7684\u8d28\u91cf\u3001\u5206\u8fa8\u7387\u3001\u89c6\u89d2\u548c\u5149\u7167\u6761\u4ef6\u3002\u6781\u7aef\u6216\u4f4e\u8d28\u91cf\u7684\u8f93\u5165\u53ef\u80fd\u5bfc\u81f4\u4e0d\u4f73\u7684\u91cd\u5efa\u6548\u679c\u3002 \u751f\u6210\u7ed3\u679c\u7684\u8bed\u4e49\u5408\u7406\u6027\uff1a \u5c3d\u7ba1\u80fd\u591f\u751f\u6210\u591a\u6837\u5316\u7684\u8865\u5168\u7ed3\u679c\uff0c\u4f46\u5982\u4f55\u786e\u4fdd\u6240\u6709\u751f\u6210\u7684\u88ab\u906e\u6321\u90e8\u5206\u5728\u8bed\u4e49\u4e0a\u5b8c\u5168\u5408\u7406\u4e14\u7269\u7406\u4e0a\u4e00\u81f4\uff0c\u5bf9\u4e8e\u590d\u6742\u573a\u666f\u4ecd\u662f\u4e00\u4e2a\u6311\u6218\u3002 \u6cdb\u5316\u80fd\u529b\uff1a \u6458\u8981\u672a\u8be6\u7ec6\u8bf4\u660e\u8bad\u7ec3\u6570\u636e\u7684\u8303\u56f4\uff0c\u56e0\u6b64\u6a21\u578b\u5bf9\u8bad\u7ec3\u96c6\u4e2d\u672a\u51fa\u73b0\u7684\u65b0\u9896\u573a\u666f\u3001\u7269\u4f53\u7c7b\u522b\u6216\u590d\u6742\u4ea4\u4e92\u7684\u6cdb\u5316\u80fd\u529b\u53ef\u80fd\u5b58\u5728\u5c40\u9650\u3002 \u603b\u7ed3\uff1a \u8fd9\u7bc7\u8bba\u6587\u901a\u8fc7\u5c06\u6269\u6563\u6a21\u578b\u4e0e\u9ad8\u65af\u6cfc\u6e85\u7ed3\u5408\uff0c\u5e76\u5de7\u5999\u5730\u89e3\u51b3\u4e863D\u771f\u503c\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u5728\u5355\u56fe\u50cf3D\u91cd\u5efa\u9886\u57df\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\u3002\u5176\u751f\u6210\u5f0f\u65b9\u6cd5\u80fd\u591f\u5904\u7406\u56fa\u6709\u7684\u6b67\u4e49\u6027\uff0c\u4e3a\u672a\u6765\u76843D\u611f\u77e5\u548c\u5185\u5bb9\u751f\u6210\u5f00\u8f9f\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002 Key Findings: We propose a latent diffusion model to reconstruct a complete 3D scene with Gaussian splats, including the occluded parts, from only a single image during inference. In contrast, we propose a generative formulation to learn a distribution of 3D representations of Gaussian splats conditioned on a single input image. To address the lack of ground-truth training data, we propose a Variational AutoReconstructor to learn a latent space only from 2D images in a self-supervised manner, over which a diffusion model is trained. Our method generates faithful reconstructions and diverse samples with the ability to complete the occluded surfaces for high-quality 360-degree renderings. Links: PDF arXiv","title":"5. \u53ef\u4ece\u6458\u8981\u4e2d\u63a8\u65ad\u51fa\u7684\u5c40\u9650\u6027 (Limitations Inferred from the Abstract)"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-01/#efficient-diffusion-based-3d-human-pose-estimation-with-hierarchical-temporal-pruning","text":"Authors: Yuquan Bi, Hongsong Wang, Xinli Shi, Zhipeng Gui, Jie Gui, Yuan Yan Tang Published: 2025-08-29 Categories: cs.CV Abstract: Diffusion models have demonstrated strong capabilities in generating high-fidelity 3D human poses, yet their iterative nature and multi-hypothesis requirements incur substantial computational cost. In this paper, we propose an Efficient Diffusion-Based 3D Human Pose Estimation framework with a Hierarchical Temporal Pruning (HTP) strategy, which dynamically prunes redundant pose tokens across both frame and semantic levels while preserving critical motion dynamics. HTP operates in a staged, top-down manner: (1) Temporal Correlation-Enhanced Pruning (TCEP) identifies essential frames by analyzing inter-frame motion correlations through adaptive temporal graph construction; (2) Sparse-Focused Temporal MHSA (SFT MHSA) leverages the resulting frame-level sparsity to reduce attention computation, focusing on motion-relevant tokens; and (3) Mask-Guided Pose Token Pruner (MGPTP) performs fine-grained semantic pruning via clustering, retaining only the most informative pose tokens. Experiments on Human3.6M and MPI-INF-3DHP show that HTP reduces training MACs by 38.5\\%, inference MACs by 56.8\\%, and improves inference speed by an average of 81.1\\% compared to prior diffusion-based methods, while achieving state-of-the-art performance. Analysis: \u8fd9\u7bc7\u8bba\u6587\u7684\u6458\u8981\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5173\u4e8e\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u4e2d3D\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u7684\u6709\u8da3\u8fdb\u5c55\u3002\u4ee5\u4e0b\u662f\u6839\u636e\u6458\u8981\u8fdb\u884c\u7684\u5206\u6790\uff1a","title":"Efficient Diffusion-Based 3D Human Pose Estimation with Hierarchical Temporal Pruning"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-01/#1-concise-summary_2","text":"\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u9ad8\u4fdd\u771f3D\u4eba\u4f53\u59ff\u6001\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u8fed\u4ee3\u6027\u8d28\u548c\u591a\u5047\u8bbe\u9700\u6c42\u5bfc\u81f4\u4e86\u5de8\u5927\u7684\u8ba1\u7b97\u6210\u672c\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u57fa\u4e8e\u6269\u6563\u76843D\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u91c7\u7528\u5206\u5c42\u65f6\u95f4\u526a\u679d\uff08HTP\uff09\u7b56\u7565\uff0c\u52a8\u6001\u5730\u5728\u5e27\u548c\u8bed\u4e49\u5c42\u9762\u526a\u679d\u5197\u4f59\u59ff\u6001tokens\uff0c\u540c\u65f6\u4fdd\u7559\u5173\u952e\u8fd0\u52a8\u52a8\u6001\u3002HTP\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\uff0c\u5e76\u63d0\u5347\u4e86\u63a8\u7406\u901f\u5ea6\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002","title":"1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u660e\u6458\u8981 (Concise Summary)"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-01/#2-key-innovation-or-methodological-approach_2","text":"\u8be5\u8bba\u6587\u7684\u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u5176 \u5206\u5c42\u65f6\u95f4\u526a\u679d\uff08Hierarchical Temporal Pruning, HTP\uff09\u7b56\u7565 \uff0c\u65e8\u5728\u89e3\u51b3\u6269\u6563\u6a21\u578b\u57283D\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u4e2d\u7684\u8ba1\u7b97\u6548\u7387\u95ee\u9898\u3002HTP\u65b9\u6cd5\u8bba\u7684\u72ec\u7279\u6027\u4f53\u73b0\u5728\u5176\u591a\u9636\u6bb5\u3001\u81ea\u4e0a\u800c\u4e0b\u7684\u526a\u679d\u8fc7\u7a0b\uff1a \u65f6\u95f4\u76f8\u5173\u6027\u589e\u5f3a\u526a\u679d (Temporal Correlation-Enhanced Pruning, TCEP) \uff1a\u901a\u8fc7\u6784\u5efa\u81ea\u9002\u5e94\u65f6\u95f4\u56fe\u6765\u5206\u6790\u5e27\u95f4\u8fd0\u52a8\u76f8\u5173\u6027\uff0c\u4ece\u800c\u8bc6\u522b\u5e76\u526a\u679d\u5197\u4f59\u5e27\uff0c\u4fdd\u7559\u5173\u952e\u5e27\u3002\u8fd9\u662f\u4e00\u79cd\u667a\u80fd\u7684\u5e27\u7ea7\u7a00\u758f\u5316\u65b9\u6cd5\uff0c\u800c\u975e\u7b80\u5355\u91c7\u6837\u3002 \u7a00\u758f\u805a\u7126\u65f6\u95f4\u591a\u5934\u81ea\u6ce8\u610f\u529b (Sparse-Focused Temporal MHSA, SFT MHSA) \uff1a\u5229\u7528TCEP\u4ea7\u751f\u7684\u5e27\u7ea7\u7a00\u758f\u6027\uff0c\u4f18\u5316\u591a\u5934\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684\u8ba1\u7b97\uff0c\u4f7f\u5176\u53ea\u5173\u6ce8\u4e0e\u8fd0\u52a8\u76f8\u5173\u7684tokens\uff0c\u4ece\u800c\u51cf\u5c11\u8ba1\u7b97\u91cf\u3002 \u63a9\u7801\u5f15\u5bfc\u59ff\u6001Token\u526a\u679d\u5668 (Mask-Guided Pose Token Pruner, MGPTP) \uff1a\u5728\u66f4\u7ec6\u7c92\u5ea6\u7684\u8bed\u4e49\u5c42\u9762\uff0c\u901a\u8fc7\u805a\u7c7b\u5bf9\u59ff\u6001tokens\u8fdb\u884c\u526a\u679d\uff0c\u53ea\u4fdd\u7559\u6700\u5177\u4fe1\u606f\u91cf\u7684tokens\u3002\u8fd9\u8fdb\u4e00\u6b65\u7cbe\u70bc\u4e86\u59ff\u6001\u8868\u793a\uff0c\u53bb\u9664\u4e86\u8bed\u4e49\u5197\u4f59\u3002 \u8fd9\u79cd\u7ed3\u5408\u4e86\u5e27\u7ea7\u548c\u8bed\u4e49\u7ea7\u526a\u679d\u7684\u5c42\u6b21\u5316\u3001\u52a8\u6001\u65b9\u6cd5\uff0c\u662f\u5176\u5728\u6269\u6563\u6a21\u578b\u6548\u7387\u63d0\u5347\u4e0a\u7684\u5173\u952e\u7a81\u7834\u3002","title":"2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u8bba (Key Innovation or Methodological Approach)"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-01/#3-potential-impact-on-the-field_2","text":"\u8fd9\u9879\u7814\u7a76\u5bf9\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u5177\u6709\u663e\u8457\u7684\u6f5c\u5728\u5f71\u54cd\uff1a \u63a8\u52a8\u6269\u6563\u6a21\u578b\u7684\u5b9e\u9645\u5e94\u7528 \uff1a\u901a\u8fc7\u5927\u5e45\u63d0\u9ad8\u6269\u6563\u6a21\u578b\u7684\u8ba1\u7b97\u6548\u7387\u548c\u63a8\u7406\u901f\u5ea6\uff0cHTP\u4f7f\u5176\u5728\u5b9e\u65f6\u6216\u8d44\u6e90\u53d7\u9650\u76843D\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u5e94\u7528\u4e2d\u53d8\u5f97\u66f4\u52a0\u53ef\u884c\u548c\u5b9e\u7528\u3002 \u542f\u53d1\u901a\u7528\u6548\u7387\u63d0\u5347\u7b56\u7565 \uff1aHTP\u7684\u5206\u5c42\u526a\u679d\u601d\u60f3\uff0c\u7279\u522b\u662f\u7ed3\u5408\u8fd0\u52a8\u52a8\u6001\u548c\u8bed\u4e49\u4fe1\u606f\u8fdb\u884c\u7a00\u758f\u5316\u7684\u65b9\u6cd5\uff0c\u53ef\u80fd\u4e3a\u5176\u4ed6\u57fa\u4e8eTransformer\u6216\u6269\u6563\u7684\u5e8f\u5217\u751f\u6210\u6a21\u578b\uff08\u5982\u89c6\u9891\u751f\u6210\u3001\u52a8\u4f5c\u5408\u6210\uff09\u63d0\u4f9b\u901a\u7528\u7684\u6548\u7387\u4f18\u5316\u601d\u8def\u3002 \u52a0\u901f\u7814\u7a76\u4e0e\u5f00\u53d1 \uff1a\u66f4\u5feb\u7684\u6a21\u578b\u8bad\u7ec3\u548c\u63a8\u7406\u901f\u5ea6\u5c06\u5141\u8bb8\u7814\u7a76\u4eba\u5458\u8fdb\u884c\u66f4\u591a\u5b9e\u9a8c\uff0c\u52a0\u901f\u65b0\u7b97\u6cd5\u548c\u5e94\u7528\u573a\u666f\u7684\u63a2\u7d22\u3002 \u63d0\u5347\u7528\u6237\u4f53\u9a8c \uff1a\u5728AR/VR\u3001\u4eba\u673a\u4ea4\u4e92\u3001\u8fd0\u52a8\u5206\u6790\u7b49\u9886\u57df\uff0c\u66f4\u9ad8\u6548\u76843D\u59ff\u6001\u4f30\u8ba1\u610f\u5473\u7740\u66f4\u6d41\u7545\u3001\u54cd\u5e94\u66f4\u5feb\u7684\u7528\u6237\u4f53\u9a8c\u3002","title":"3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd (Potential Impact on the Field)"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-01/#4-related-areas-or-applications_2","text":"\u8fd9\u9879\u7814\u7a76\u7684\u6210\u679c\u53ef\u4ee5\u60e0\u53ca\u4ee5\u4e0b\u9886\u57df\u548c\u5e94\u7528\uff1a \u5b9e\u65f63D\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1 \uff1a\u5982\u5728AR/VR\u6e38\u620f\u3001\u865a\u62df\u8bd5\u7a7f\u3001\u8fdc\u7a0b\u534f\u4f5c\u7b49\u573a\u666f\u4e2d\u3002 \u4eba\u673a\u4ea4\u4e92 (HCI) \uff1a\u901a\u8fc7\u66f4\u51c6\u786e\u3001\u4f4e\u5ef6\u8fdf\u7684\u59ff\u6001\u8bc6\u522b\uff0c\u5b9e\u73b0\u66f4\u81ea\u7136\u7684\u7528\u6237\u754c\u9762\u548c\u624b\u52bf\u63a7\u5236\u3002 \u673a\u5668\u4eba\u5b66 \uff1a\u7528\u4e8e\u673a\u5668\u4eba\u6a21\u4eff\u5b66\u4e60\u3001\u4eba\u673a\u534f\u4f5c\u4e2d\u7684\u4eba\u4f53\u59ff\u6001\u7406\u89e3\u548c\u9884\u6d4b\u3002 \u8fd0\u52a8\u5206\u6790\u4e0e\u751f\u7269\u529b\u5b66 \uff1a\u9ad8\u6548\u5206\u6790\u8fd0\u52a8\u5458\u52a8\u4f5c\u3001\u5eb7\u590d\u8bad\u7ec3\u4e2d\u7684\u59ff\u6001\u8bc4\u4f30\u3002 \u7535\u5f71\u3001\u52a8\u753b\u4e0e\u6e38\u620f \uff1a\u5feb\u901f\u751f\u6210\u548c\u7f16\u8f913D\u89d2\u8272\u52a8\u753b\uff0c\u964d\u4f4e\u5236\u4f5c\u6210\u672c\u3002 \u89c6\u9891\u7406\u89e3\u4e0e\u52a8\u4f5c\u8bc6\u522b \uff1a\u4f5c\u4e3a\u9884\u5904\u7406\u6b65\u9aa4\uff0c\u63d0\u4f9b\u9ad8\u6548\u76843D\u59ff\u6001\u7279\u5f81\u3002 \u901a\u7528\u5e8f\u5217\u6a21\u578b\u6548\u7387\u4f18\u5316 \uff1a\u5176\u526a\u679d\u7b56\u7565\u53ef\u80fd\u9002\u7528\u4e8e\u5176\u4ed6\u9700\u8981\u5904\u7406\u957f\u5e8f\u5217\u6216\u9ad8\u7ef4token\u7684Transformer\u6216\u6269\u6563\u6a21\u578b\u3002","title":"4. \u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 (Related Areas or Applications)"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-01/#5-limitations-inferred-from-the-abstract_1","text":"\u5c3d\u7ba1\u6458\u8981\u5f3a\u8c03\u4e86\u663e\u8457\u7684\u6548\u7387\u63d0\u5347\u548cSOTA\u6027\u80fd\uff0c\u4f46\u4ecd\u53ef\u63a8\u65ad\u51fa\u4e00\u4e9b\u6f5c\u5728\u5c40\u9650\u6027\uff1a \u526a\u679d\u7b56\u7565\u7684\u590d\u6742\u6027 \uff1aHTP\u5305\u542b\u4e09\u4e2a\u9636\u6bb5\uff0c\u6d89\u53ca\u81ea\u9002\u5e94\u56fe\u6784\u5efa\u548c\u805a\u7c7b\u7b49\u64cd\u4f5c\uff0c\u8fd9\u53ef\u80fd\u589e\u52a0\u4e86\u6a21\u578b\u7684\u5b9e\u73b0\u548c\u8c03\u4f18\u590d\u6742\u6027\u3002\u8fd9\u4e9b\u989d\u5916\u6b65\u9aa4\u672c\u8eab\u4e5f\u53ef\u80fd\u5f15\u5165\u4e00\u5b9a\u7684\u8ba1\u7b97\u5f00\u9500\uff0c\u5c3d\u7ba1\u603b\u4f53\u4e0a\u662f\u51c0\u6536\u76ca\u3002 \u201c\u5173\u952e\u8fd0\u52a8\u52a8\u6001\u201d\u7684\u5b9a\u4e49\u4e0e\u9c81\u68d2\u6027 \uff1a\u6458\u8981\u63d0\u5230\u201c\u4fdd\u7559\u5173\u952e\u8fd0\u52a8\u52a8\u6001\u201d\uff0c\u4f46\u5982\u4f55\u7cbe\u786e\u5b9a\u4e49\u548c\u4fdd\u8bc1\u5728\u6240\u6709\u590d\u6742\u8fd0\u52a8\u573a\u666f\u4e0b\u90fd\u80fd\u6709\u6548\u4fdd\u7559\u201c\u5173\u952e\u201d\u4fe1\u606f\uff0c\u800c\u4e0d\u4f1a\u8bef\u526a\u679d\u6389\u7ec6\u5fae\u4f46\u91cd\u8981\u7684\u52a8\u4f5c\u7ec6\u8282\uff0c\u662f\u4e00\u4e2a\u6f5c\u5728\u7684\u6311\u6218\u3002 \u8d85\u53c2\u6570\u654f\u611f\u6027 \uff1a\u81ea\u9002\u5e94\u56fe\u7684\u6784\u5efa\u3001\u805a\u7c7b\u7b97\u6cd5\u7684\u9009\u62e9\u548c\u53c2\u6570\u8bbe\u7f6e\uff0c\u4ee5\u53ca\u5404\u9636\u6bb5\u526a\u679d\u6bd4\u4f8b\u7b49\uff0c\u90fd\u53ef\u80fd\u5f15\u5165\u9700\u8981\u4ed4\u7ec6\u8c03\u4f18\u7684\u8d85\u53c2\u6570\uff0c\u5f71\u54cd\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002 \u5bf9\u7279\u5b9a\u6570\u636e\u96c6\u7684\u4f9d\u8d56 \uff1a\u5b9e\u9a8c\u7ed3\u679c\u5728Human3.6M\u548cMPI-INF-3DHP\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u8fd9\u4e9b\u662f\u6807\u51c6\u6570\u636e\u96c6\uff0c\u4f46\u5728\u66f4\u201c\u91ce\u5916\u201d\uff08in-the-wild\uff09\u7684\u590d\u6742\u73af\u5883\u3001\u906e\u6321\u3001\u591a\u6837\u5316\u670d\u88c5\u548c\u5149\u7167\u6761\u4ef6\u4e0b\uff0c\u526a\u679d\u7b56\u7565\u7684\u6709\u6548\u6027\u53ef\u80fd\u9700\u8981\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u3002 \u672a\u63d0\u53ca\u5176\u4ed6\u6311\u6218 \uff1a\u6458\u8981\u4e3b\u8981\u5173\u6ce8\u6548\u7387\u95ee\u9898\uff0c\u5e76\u672a\u63d0\u53ca\u5bf93D\u59ff\u6001\u4f30\u8ba1\u4e2d\u5176\u4ed6\u5e38\u89c1\u6311\u6218\uff08\u5982\u4e25\u91cd\u906e\u6321\u3001\u591a\u89c6\u89d2\u9c81\u68d2\u6027\u3001\u4e0d\u540c\u4f53\u578b\u548c\u670d\u88c5\u7684\u6cdb\u5316\u80fd\u529b\uff09\u7684\u76f4\u63a5\u6539\u8fdb\u3002 Key Findings: In this paper, we propose an Efficient Diffusion-Based 3D Human Pose Estimation framework with a Hierarchical Temporal Pruning (HTP) strategy, which dynamically prunes redundant pose tokens across both frame and semantic levels while preserving critical motion dynamics. Experiments on Human3.6M and MPI-INF-3DHP show that HTP reduces training MACs by 38.5\\%, inference MACs by 56.8\\%, and improves inference speed by an average of 81.1\\% compared to prior diffusion-based methods, while achieving state-of-the-art performance. Links: PDF arXiv","title":"5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u7684\u5c40\u9650\u6027 (Limitations Inferred from the Abstract)"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-01/#driveqa-passing-the-driving-knowledge-test","text":"Authors: Maolin Wei, Wanzhou Liu, Eshed Ohn-Bar Published: 2025-08-29 Categories: cs.CV Abstract: If a Large Language Model (LLM) were to take a driving knowledge test today, would it pass? Beyond standard spatial and visual question-answering (QA) tasks on current autonomous driving benchmarks, driving knowledge tests require a complete understanding of all traffic rules, signage, and right-of-way principles. To pass this test, human drivers must discern various edge cases that rarely appear in real-world datasets. In this work, we present DriveQA, an extensive open-source text and vision-based benchmark that exhaustively covers traffic regulations and scenarios. Through our experiments using DriveQA, we show that (1) state-of-the-art LLMs and Multimodal LLMs (MLLMs) perform well on basic traffic rules but exhibit significant weaknesses in numerical reasoning and complex right-of-way scenarios, traffic sign variations, and spatial layouts, (2) fine-tuning on DriveQA improves accuracy across multiple categories, particularly in regulatory sign recognition and intersection decision-making, (3) controlled variations in DriveQA-V provide insights into model sensitivity to environmental factors such as lighting, perspective, distance, and weather conditions, and (4) pretraining on DriveQA enhances downstream driving task performance, leading to improved results on real-world datasets such as nuScenes and BDD, while also demonstrating that models can internalize text and synthetic traffic knowledge to generalize effectively across downstream QA tasks. Analysis: \u4f5c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u4e13\u5bb6\uff0c\u6211\u5bf9\u8fd9\u7bc7\u540d\u4e3a\u201cDriveQA: Passing the Driving Knowledge Test\u201d\u7684\u8bba\u6587\u6458\u8981\u8fdb\u884c\u5982\u4e0b\u5206\u6790\uff1a","title":"DriveQA: Passing the Driving Knowledge Test"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-01/#1-2-3","text":"\u672c\u6587\u63d0\u51fa\u4e86DriveQA\uff0c\u4e00\u4e2a\u5168\u9762\u7684\u5f00\u6e90\u6587\u672c\u548c\u89c6\u89c9\u57fa\u51c6\u6d4b\u8bd5\uff0c\u65e8\u5728\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u591a\u6a21\u6001LLMs\uff08MLLMs\uff09\u5bf9\u4ea4\u901a\u89c4\u5219\u3001\u6807\u5fd7\u548c\u8def\u6743\u539f\u5219\u7684\u5b8c\u6574\u7406\u89e3\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u73b0\u6709\u6a21\u578b\u5728\u590d\u6742\u573a\u666f\u548c\u6570\u503c\u63a8\u7406\u65b9\u9762\u5b58\u5728\u663e\u8457\u4e0d\u8db3\uff0c\u4f46\u901a\u8fc7\u5728DriveQA\u4e0a\u8fdb\u884c\u5fae\u8c03\u548c\u9884\u8bad\u7ec3\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u6a21\u578b\u5728\u9a7e\u9a76\u77e5\u8bc6\u6d4b\u8bd5\u548c\u4e0b\u6e38\u771f\u5b9e\u4e16\u754c\u9a7e\u9a76\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002","title":"1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u660e\u603b\u7ed3 (2-3\u53e5\u8bdd)"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-01/#2","text":"\u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u6784\u5efa\u4e86DriveQA\u8fd9\u4e00\u5168\u9762\u7684\u3001\u5f00\u6e90\u7684\u6587\u672c\u4e0e\u89c6\u89c9\u57fa\u51c6\u6d4b\u8bd5\u3002\u5b83\u8d85\u8d8a\u4e86\u73b0\u6709\u81ea\u52a8\u9a7e\u9a76\u57fa\u51c6\u7684\u5c40\u9650\uff0c\u901a\u8fc7\u201c\u7a77\u5c3d\u5f0f\u201d\u5730\u8986\u76d6\u4ea4\u901a\u6cd5\u89c4\u3001\u573a\u666f\u548c\u73b0\u5b9e\u4e16\u754c\u6570\u636e\u4e2d\u7f55\u89c1\u7684\u201c\u8fb9\u7f18\u6848\u4f8b\u201d\uff0c\u65e8\u5728\u5168\u9762\u8bc4\u4f30\u6a21\u578b\u5bf9\u9a7e\u9a76\u77e5\u8bc6\u7684\u6df1\u5c42\u7406\u89e3\u3002\u6b64\u5916\uff0cDriveQA-V\u901a\u8fc7\u5f15\u5165\u53d7\u63a7\u7684\u73af\u5883\u56e0\u7d20\uff08\u5982\u5149\u7167\u3001\u89c6\u89d2\u3001\u8ddd\u79bb\u3001\u5929\u6c14\uff09\u53d8\u5316\uff0c\u4e3a\u6df1\u5165\u5206\u6790\u6a21\u578b\u5bf9\u8fd9\u4e9b\u56e0\u7d20\u7684\u654f\u611f\u6027\u63d0\u4f9b\u4e86\u72ec\u7279\u7684\u65b9\u6cd5\uff0c\u8fd9\u5bf9\u4e8e\u7406\u89e3\u6a21\u578b\u5728\u4e0d\u540c\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6027\u81f3\u5173\u91cd\u8981\u3002","title":"2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u8bba\u65b9\u6cd5"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-01/#3","text":"\u8be5\u7814\u7a76\u5bf9\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u5177\u6709\u591a\u65b9\u9762\u6f5c\u5728\u5f71\u54cd\u3002\u9996\u5148\uff0c\u5b83\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4ece\u7eaf\u7cb9\u7684\u611f\u77e5\u548c\u9884\u6d4b\u8fc8\u5411\u57fa\u4e8e\u89c4\u5219\u7684\u201c\u7406\u89e3\u201d\u548c\u201c\u63a8\u7406\u201d\u63d0\u4f9b\u4e86\u5173\u952e\u5de5\u5177\u548c\u65b9\u5411\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002\u5176\u6b21\uff0cDriveQA\u4f5c\u4e3a\u4e00\u4e2a\u6781\u5177\u6311\u6218\u6027\u7684\u57fa\u51c6\uff0c\u5c06\u63a8\u52a8LLMs\u548cMLLMs\u5728\u590d\u6742\u63a8\u7406\u3001\u6570\u503c\u7406\u89e3\u548c\u591a\u6a21\u6001\u4fe1\u606f\u878d\u5408\u65b9\u9762\u7684\u80fd\u529b\u8fb9\u754c\u3002\u6700\u540e\uff0c\u901a\u8fc7\u5c55\u793a\u5408\u6210\u6570\u636e\u548c\u6587\u672c\u77e5\u8bc6\u5bf9\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u672a\u6765\u9ad8\u6548\u8bad\u7ec3\u548c\u90e8\u7f72\u667a\u80fd\u9a7e\u9a76\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u7684\u8303\u5f0f\uff0c\u5c24\u5176\u662f\u5728\u96be\u4ee5\u83b7\u53d6\u5927\u91cf\u771f\u5b9e\u4e16\u754c\u8fb9\u7f18\u6848\u4f8b\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u3002","title":"3. \u5bf9\u8be5\u9886\u57df\u7684\u6f5c\u5728\u5f71\u54cd"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-01/#4","text":"\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5f00\u53d1\u4e0e\u6d4b\u8bd5\uff1a \u76f4\u63a5\u5e94\u7528\uff0c\u7528\u4e8e\u8bad\u7ec3\u548c\u8bc4\u4f30\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u51b3\u7b56\u6a21\u5757\uff0c\u7279\u522b\u662f\u5176\u5bf9\u4ea4\u901a\u89c4\u5219\u7684\u7406\u89e3\u548c\u590d\u6742\u573a\u666f\u4e0b\u7684\u63a8\u7406\u80fd\u529b\u3002 \u667a\u80fd\u4ea4\u901a\u7cfb\u7edf (ITS)\uff1a \u8f85\u52a9\u4ea4\u901a\u6d41\u7ba1\u7406\u3001\u4e8b\u6545\u9884\u9632\u548c\u667a\u80fd\u4fe1\u53f7\u63a7\u5236\uff0c\u901a\u8fc7\u66f4\u667a\u80fd\u5730\u7406\u89e3\u4ea4\u901a\u89c4\u5219\u6765\u4f18\u5316\u7cfb\u7edf\u3002 \u673a\u5668\u4eba\u5b66\uff1a \u4efb\u4f55\u9700\u8981\u5728\u590d\u6742\u3001\u89c4\u5219\u9a71\u52a8\u73af\u5883\u4e2d\u64cd\u4f5c\u7684\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u4f8b\u5982\u7269\u6d41\u673a\u5668\u4eba\u6216\u670d\u52a1\u673a\u5668\u4eba\uff0c\u53ef\u4ee5\u501f\u9274\u5176\u89c4\u5219\u7406\u89e3\u548c\u51b3\u7b56\u6846\u67b6\u3002 AI\u5b89\u5168\u4e0e\u53ef\u89e3\u91ca\u6027 (XAI)\uff1a \u6df1\u5165\u7406\u89e3\u6a21\u578b\u5982\u4f55\u57fa\u4e8e\u4ea4\u901a\u89c4\u5219\u505a\u51fa\u51b3\u7b56\uff0c\u63d0\u9ad8\u51b3\u7b56\u7684\u900f\u660e\u5ea6\u548c\u53ef\u4fe1\u5ea6\uff0c\u8fd9\u5728\u5b89\u5168\u5173\u952e\u9886\u57df\u5c24\u4e3a\u91cd\u8981\u3002 \u9a7e\u9a76\u5458\u57f9\u8bad\u4e0e\u6559\u80b2\uff1a \u4f5c\u4e3a\u8f85\u52a9\u5de5\u5177\uff0c\u5e2e\u52a9\u65b0\u9a7e\u9a76\u5458\u7406\u89e3\u590d\u6742\u7684\u4ea4\u901a\u89c4\u5219\u548c\u8fb9\u7f18\u60c5\u51b5\uff0c\u751a\u81f3\u53ef\u4ee5\u7528\u4e8e\u5f00\u53d1\u667a\u80fd\u9a7e\u9a76\u6a21\u62df\u5668\u3002 \u591a\u6a21\u6001LLMs\u7814\u7a76\uff1a \u4e3a\u63d0\u5347LLMs\u548cMLLMs\u5728\u89c6\u89c9\u7406\u89e3\u3001\u5e38\u8bc6\u63a8\u7406\u548c\u89c4\u5219\u9075\u5faa\u65b9\u9762\u7684\u80fd\u529b\u63d0\u4f9b\u65b0\u7684\u7814\u7a76\u65b9\u5411\u548c\u8bc4\u4f30\u6807\u51c6\u3002","title":"4. \u53ef\u80fd\u53d7\u76ca\u4e8e\u8fd9\u9879\u7814\u7a76\u7684\u76f8\u5173\u9886\u57df\u6216\u5e94\u7528"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-01/#5","text":"\u5408\u6210\u6570\u636e\u4e0e\u771f\u5b9e\u4e16\u754c\u7684\u5dee\u8ddd\uff1a \u5c3d\u7ba1\u6458\u8981\u6307\u51fa\u6a21\u578b\u53ef\u4ee5\u5185\u5316\u6587\u672c\u548c\u5408\u6210\u4ea4\u901a\u77e5\u8bc6\u5e76\u6cdb\u5316\u5230\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\uff0c\u4f46DriveQA\u672c\u8eab\u662f\u57fa\u4e8e\u201c\u5408\u6210\u4ea4\u901a\u77e5\u8bc6\u201d\u6784\u5efa\u7684\u3002\u5408\u6210\u6570\u636e\u5728\u8986\u76d6\u8fb9\u7f18\u6848\u4f8b\u65b9\u9762\u6709\u4f18\u52bf\uff0c\u4f46\u5176\u4e0e\u771f\u5b9e\u4e16\u754c\u6570\u636e\u7684\u5206\u5e03\u5dee\u5f02\u3001\u566a\u58f0\u548c\u4e0d\u53ef\u9884\u6d4b\u6027\u53ef\u80fd\u4ecd\u662f\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u7684\u6f5c\u5728\u9650\u5236\u3002 \u4fa7\u91cd\u77e5\u8bc6\u7406\u89e3\u800c\u975e\u5b9e\u65f6\u51b3\u7b56\u4e0e\u63a7\u5236\uff1a DriveQA\u4e3b\u8981\u662f\u4e00\u4e2a\u201c\u9a7e\u9a76\u77e5\u8bc6\u6d4b\u8bd5\u201d\uff0c\u8bc4\u4f30\u6a21\u578b\u5bf9\u89c4\u5219\u7684\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\u3002\u5b83\u4e0d\u76f4\u63a5\u8bc4\u4f30\u6a21\u578b\u5728\u5b9e\u65f6\u3001\u52a8\u6001\u3001\u9ad8\u538b\u7684\u771f\u5b9e\u9a7e\u9a76\u73af\u5883\u4e2d\u7684\u611f\u77e5\u3001\u9884\u6d4b\u548c\u63a7\u5236\u80fd\u529b\uff0c\u8fd9\u9700\u8981\u66f4\u590d\u6742\u7684\u7aef\u5230\u7aef\u7cfb\u7edf\u3002 \u7279\u5b9a\u6311\u6218\u7684\u6301\u7eed\u5b58\u5728\uff1a \u6458\u8981\u660e\u786e\u6307\u51fa\uff0c\u5373\u4f7f\u662fSOTA\u6a21\u578b\uff0c\u5728\u201c\u6570\u503c\u63a8\u7406\u3001\u590d\u6742\u8def\u6743\u573a\u666f\u3001\u4ea4\u901a\u6807\u5fd7\u53d8\u4f53\u548c\u7a7a\u95f4\u5e03\u5c40\u201d\u65b9\u9762\u4ecd\u5b58\u5728\u663e\u8457\u5f31\u70b9\u3002\u8fd9\u8868\u660e\u8fd9\u4e9b\u662f\u6781\u5176\u56f0\u96be\u7684\u95ee\u9898\uff0cDriveQA\u867d\u7136\u63d0\u4f9b\u4e86\u8bc4\u4f30\u5de5\u5177\uff0c\u4f46\u89e3\u51b3\u8fd9\u4e9b\u6df1\u5c42\u6311\u6218\u4ecd\u9700\u8fdb\u4e00\u6b65\u7684\u7814\u7a76\u3002 \u201c\u7a77\u5c3d\u6027\u201d\u7684\u8303\u56f4\uff1a \u5c3d\u7ba1\u58f0\u79f0\u201c\u7a77\u5c3d\u5f0f\u5730\u8986\u76d6\u4ea4\u901a\u6cd5\u89c4\u548c\u573a\u666f\u201d\uff0c\u4f46\u73b0\u5b9e\u4e16\u754c\u7684\u4ea4\u901a\u89c4\u5219\u548c\u8fb9\u7f18\u6848\u4f8b\u662f\u6781\u5176\u5e9e\u5927\u548c\u590d\u6742\u7684\uff08\u4f8b\u5982\uff0c\u4e0d\u540c\u56fd\u5bb6/\u5730\u533a\u7684\u5177\u4f53\u6cd5\u89c4\u5dee\u5f02\u3001\u5404\u79cd\u975e\u6807\u51c6\u60c5\u51b5\uff09\u3002\u6458\u8981\u5e76\u672a\u8be6\u7ec6\u8bf4\u660e\u5176\u8986\u76d6\u7684\u5e7f\u5ea6\u548c\u6df1\u5ea6\uff0c\u8fd9\u53ef\u80fd\u662f\u4e00\u4e2a\u6f5c\u5728\u7684\u5c40\u9650\u3002 Key Findings: In this work, we present DriveQA, an extensive open-source text and vision-based benchmark that exhaustively covers traffic regulations and scenarios. Through our experiments using DriveQA, we show that (1) state-of-the-art LLMs and Multimodal LLMs (MLLMs) perform well on basic traffic rules but exhibit significant weaknesses in numerical reasoning and complex right-of-way scenarios, traffic sign variations, and spatial layouts, (2) fine-tuning on DriveQA improves accuracy across multiple categories, particularly in regulatory sign recognition and intersection decision-making, (3) controlled variations in DriveQA-V provide insights into model sensitivity to environmental factors such as lighting, perspective, distance, and weather conditions, and (4) pretraining on DriveQA enhances downstream driving task performance, leading to improved results on real-world datasets such as nuScenes and BDD, while also demonstrating that models can internalize text and synthetic traffic knowledge to generalize effectively across downstream QA tasks. Links: PDF arXiv","title":"5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u51fa\u7684\u5c40\u9650\u6027"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-01/#domain-generalization-in-the-wild-disentangling-classification-from-domain-aware-representations","text":"Authors: Ha Min Son, Zhe Zhao, Shahbaz Rezaei, Xin Liu Published: 2025-08-29 Categories: cs.CV, cs.LG Abstract: Evaluating domain generalization (DG) for foundational models like CLIP is challenging, as web-scale pretraining data potentially covers many existing benchmarks. Consequently, current DG evaluation may neither be sufficiently challenging nor adequately test genuinely unseen data scenarios. To better assess the performance of CLIP on DG in-the-wild, a scenario where CLIP encounters challenging unseen data, we consider two approaches: (1) evaluating on 33 diverse datasets with quantified out-of-distribution (OOD) scores after fine-tuning CLIP on ImageNet, and (2) using unlearning to make CLIP `forget' some domains as an approximation. We observe that CLIP's performance deteriorates significantly on more OOD datasets. To address this, we present CLIP-DCA (Disentangling Classification from enhanced domain Aware representations). Our approach is motivated by the observation that while standard domain invariance losses aim to make representations domain-invariant, this can be harmful to foundation models by forcing the discarding of domain-aware representations beneficial for generalization. We instead hypothesize that enhancing domain awareness is a prerequisite for effective domain-invariant classification in foundation models. CLIP-DCA identifies and enhances domain awareness within CLIP's encoders using a separate domain head and synthetically generated diverse domain data. Simultaneously, it encourages domain-invariant classification through disentanglement from the domain features. CLIP-DCA shows significant improvements within this challenging evaluation compared to existing methods, particularly on datasets that are more OOD. Analysis: \u4f5c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u4e13\u5bb6\uff0c\u6211\u5bf9\u8fd9\u7bc7\u8bba\u6587\u6458\u8981\u7684\u5206\u6790\u5982\u4e0b\uff1a","title":"Domain Generalization in-the-Wild: Disentangling Classification from Domain-Aware Representations"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-01/#domain-generalization-in-the-wild-disentangling-classification-from-domain-aware-representations_1","text":"1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e (Main Contribution): \u672c\u6587\u9488\u5bf9\u57fa\u7840\u6a21\u578b\uff08\u5982CLIP\uff09\u5728\u771f\u5b9e\u4e16\u754c\u57df\u6cdb\u5316\uff08DG in-the-wild\uff09\u573a\u666f\u4e0b\u7684\u8bc4\u4f30\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u4e00\u5957\u65b0\u7684\u8bc4\u4f30\u8303\u5f0f\uff0c\u5e76\u53d1\u73b0CLIP\u5728\u6b64\u7c7b\u9ad8\u5ea6\u57df\u5916\uff08OOD\uff09\u6570\u636e\u4e0a\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002\u4e3a\u89e3\u51b3\u6b64\u95ee\u9898\uff0c\u8bba\u6587\u5f15\u5165\u4e86CLIP-DCA\u65b9\u6cd5\uff0c\u901a\u8fc7\u589e\u5f3a\u57df\u611f\u77e5\u8868\u793a\u5e76\u5c06\u5176\u4e0e\u5206\u7c7b\u4efb\u52a1\u89e3\u8026\uff0c\u663e\u8457\u63d0\u5347\u4e86\u57fa\u7840\u6a21\u578b\u5728\u590d\u6742\u57df\u6cdb\u5316\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002 2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5 (Key Innovation or Methodological Approach): \u8be5\u8bba\u6587\u7684\u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u5176\u5bf9\u57fa\u7840\u6a21\u578b\u57df\u6cdb\u5316\u5b66\u4e60\u8303\u5f0f\u7684\u6df1\u523b\u8f6c\u53d8\u3002\u4e0e\u4f20\u7edf\u57df\u4e0d\u53d8\u6027\u5b66\u4e60\u65b9\u6cd5\u76f4\u63a5\u5f3a\u5236\u8868\u793a\u57df\u4e0d\u53d8\u6027\uff08\u53ef\u80fd\u5bfc\u81f4\u6709\u7528\u57df\u4fe1\u606f\u7684\u4e22\u5931\uff09\u4e0d\u540c\uff0cCLIP-DCA\u63d0\u51fa\u5e76\u9a8c\u8bc1\u4e86\u201c\u589e\u5f3a\u57df\u611f\u77e5\u80fd\u529b\u662f\u5b9e\u73b0\u6709\u6548\u57df\u4e0d\u53d8\u5206\u7c7b\u7684\u5148\u51b3\u6761\u4ef6\u201d\u8fd9\u4e00\u5047\u8bbe\u3002\u5176\u65b9\u6cd5\u8bba\u901a\u8fc7\u5f15\u5165\u4e00\u4e2a\u72ec\u7acb\u7684\u57df\u5934\u548c\u5229\u7528\u5408\u6210\u57df\u6570\u636e\u6765\u4e3b\u52a8\u8bc6\u522b\u5e76\u589e\u5f3aCLIP\u7f16\u7801\u5668\u4e2d\u7684\u57df\u611f\u77e5\u8868\u793a\uff0c\u540c\u65f6\u5de7\u5999\u5730\u901a\u8fc7\u89e3\u8026\u673a\u5236\u786e\u4fdd\u6700\u7ec8\u7684\u5206\u7c7b\u4efb\u52a1\u80fd\u591f\u4ece\u8fd9\u4e9b\u589e\u5f3a\u7684\u57df\u7279\u5f81\u4e2d\u89e3\u8026\u51fa\u6765\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u9c81\u68d2\u7684\u57df\u4e0d\u53d8\u5206\u7c7b\u3002 3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd (Potential Impact on the Field): \u8be5\u7814\u7a76\u6709\u671b\u5bf9\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u4ea7\u751f\u591a\u65b9\u9762\u5f71\u54cd\u3002\u9996\u5148\uff0c\u5b83\u63d0\u51fa\u4e86\u66f4\u4e25\u8c28\u548c\u5177\u6311\u6218\u6027\u7684\u57fa\u7840\u6a21\u578b\u57df\u6cdb\u5316\u8bc4\u4f30\u8303\u5f0f\uff0c\u4fc3\u4f7f\u9886\u57df\u91cd\u65b0\u601d\u8003\u73b0\u6709\u57fa\u51c6\u7684\u6709\u6548\u6027\u3002\u5176\u6b21\uff0cCLIP-DCA\u7684\u6838\u5fc3\u601d\u60f3\u2014\u2014\u5373\u5148\u589e\u5f3a\u57df\u611f\u77e5\u518d\u89e3\u8026\u5206\u7c7b\u2014\u2014\u4e3a\u672a\u6765\u8bbe\u8ba1\u9488\u5bf9\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\uff08\u5982CLIP\uff09\u7684\u57df\u6cdb\u5316\u7b97\u6cd5\u5f00\u8f9f\u4e86\u65b0\u8def\u5f84\uff0c\u6311\u6218\u4e86\u4f20\u7edf\u57df\u4e0d\u53d8\u6027\u5b66\u4e60\u7684\u5047\u8bbe\u3002\u8fd9\u6709\u671b\u663e\u8457\u63d0\u5347\u57fa\u7840\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u3001\u9ad8\u5ea6\u57df\u5916\uff08OOD\uff09\u573a\u666f\u4e0b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4f7f\u5176\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u66f4\u53ef\u9760\u3002 4. \u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 (Related Areas or Applications that Might Benefit from this Research): \u672c\u7814\u7a76\u7684\u6210\u679c\u5c06\u5bf9\u5e7f\u6cdb\u7684\u5b9e\u9645\u5e94\u7528\u9886\u57df\u4ea7\u751f\u79ef\u6781\u5f71\u54cd\uff0c\u7279\u522b\u662f\u90a3\u4e9b\u5bf9\u6a21\u578b\u5728\u672a\u89c1\u6570\u636e\u4e0a\u6cdb\u5316\u80fd\u529b\u6709\u4e25\u683c\u8981\u6c42\u7684\u573a\u666f\uff1a * \u533b\u7597\u5f71\u50cf\u8bca\u65ad\uff1a \u4e0d\u540c\u533b\u9662\u3001\u8bbe\u5907\u6216\u60a3\u8005\u7fa4\u4f53\u7684\u6570\u636e\u5206\u5e03\u5dee\u5f02\u5de8\u5927\uff0cDG\u80fd\u529b\u81f3\u5173\u91cd\u8981\u3002 * \u81ea\u52a8\u9a7e\u9a76\uff1a \u9762\u5bf9\u591a\u53d8\u7684\u5929\u6c14\u3001\u5149\u7167\u3001\u5730\u7406\u73af\u5883\u7b49\uff0c\u6a21\u578b\u9700\u5177\u5907\u5f3a\u5927\u7684\u57df\u6cdb\u5316\u80fd\u529b\u3002 * \u673a\u5668\u4eba\u89c6\u89c9\uff1a \u673a\u5668\u4eba\u9700\u8981\u5728\u5404\u79cd\u672a\u77e5\u7684\u5ba4\u5185\u5916\u73af\u5883\u4e2d\u6267\u884c\u4efb\u52a1\u3002 * \u9065\u611f\u56fe\u50cf\u5206\u6790\uff1a \u4e0d\u540c\u5730\u7406\u533a\u57df\u3001\u4f20\u611f\u5668\u6216\u65f6\u95f4\u70b9\u7684\u6570\u636e\u5206\u5e03\u5dee\u5f02\u3002 * \u5de5\u4e1a\u7f3a\u9677\u68c0\u6d4b\uff1a \u751f\u4ea7\u7ebf\u73af\u5883\u53d8\u5316\u53ef\u80fd\u5bfc\u81f4\u6570\u636e\u5206\u5e03\u504f\u79fb\u3002 \u6b64\u5916\uff0c\u5bf9\u4e8e\u4efb\u4f55\u9700\u8981\u5c06\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\u90e8\u7f72\u5230\u4e0e\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u5b58\u5728\u663e\u8457\u5dee\u5f02\u7684\u771f\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u7684\u5e94\u7528\uff0c\u672c\u7814\u7a76\u90fd\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u6307\u5bfc\u548c\u89e3\u51b3\u65b9\u6848\u3002 5. \u6f5c\u5728\u5c40\u9650\u6027 (Limitations that Can Be Inferred from the Abstract): \u5c3d\u7ba1\u6458\u8981\u5c55\u793a\u4e86\u4ee4\u4eba\u9f13\u821e\u7684\u7ed3\u679c\uff0c\u4f46\u4ecd\u53ef\u63a8\u65ad\u51fa\u4e00\u4e9b\u6f5c\u5728\u5c40\u9650\u6027\uff1a * \u5408\u6210\u57df\u6570\u636e\u7684\u6709\u6548\u6027\u4e0e\u6cdb\u5316\u6027\uff1a CLIP-DCA\u4f9d\u8d56\u4e8e\u201c\u5408\u6210\u751f\u6210\u591a\u6837\u57df\u6570\u636e\u201d\u6765\u589e\u5f3a\u57df\u611f\u77e5\u3002\u5408\u6210\u6570\u636e\u80fd\u5426\u5145\u5206\u6355\u6349\u771f\u5b9e\u4e16\u754c\u4e2d\u590d\u6742\u4e14\u672a\u77e5\u7684\u57df\u504f\u79fb\u6a21\u5f0f\uff0c\u4ee5\u53ca\u5176\u8d28\u91cf\u548c\u591a\u6837\u6027\u5bf9\u6700\u7ec8\u6cdb\u5316\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u662f\u4e00\u4e2a\u5173\u952e\u95ee\u9898\u3002\u8fc7\u4e8e\u7b80\u5355\u7684\u5408\u6210\u53ef\u80fd\u65e0\u6cd5\u5e94\u5bf9\u6781\u7aefOOD\u573a\u666f\u3002 * \u201c\u9057\u5fd8\u201d\u673a\u5236\u4f5c\u4e3a\u8fd1\u4f3c\u7684\u5c40\u9650\u6027\uff1a \u8bba\u6587\u4f7f\u7528\u201c\u9057\u5fd8\u201d\u6765\u8fd1\u4f3c\u6a21\u62df\u6a21\u578b\u4ece\u672a\u89c1\u8fc7\u67d0\u4e9b\u57df\u7684\u573a\u666f\u3002\u8fd9\u79cd\u8fd1\u4f3c\u65b9\u6cd5\u4e0e\u771f\u6b63\u610f\u4e49\u4e0a\u7684\u201c\u4ece\u672a\u89c1\u8fc7\u201d\u4e4b\u95f4\u53ef\u80fd\u5b58\u5728\u5dee\u5f02\uff0c\u5176\u6709\u6548\u6027\u548c\u51c6\u786e\u6027\u9700\u8981\u66f4\u6df1\u5165\u7684\u9a8c\u8bc1\u3002 * \u8bc4\u4f30\u573a\u666f\u7684\u4ee3\u8868\u6027\uff1a \u5c3d\u7ba1\u4f7f\u7528\u4e8633\u4e2a\u6570\u636e\u96c6\u548cOOD\u5206\u6570\uff0c\u4f46\u201cin-the-wild\u201d\u662f\u4e00\u4e2a\u6781\u5176\u5bbd\u6cdb\u7684\u6982\u5ff5\u3002\u8fd9\u4e9b\u8bc4\u4f30\u662f\u5426\u80fd\u5b8c\u5168\u4ee3\u8868\u6240\u6709\u771f\u5b9e\u4e16\u754c\u4e2d\u53ef\u80fd\u9047\u5230\u7684\u3001\u5177\u6709\u6311\u6218\u6027\u7684\u672a\u89c1\u57df\u573a\u666f\uff0c\u4ecd\u6709\u5f85\u5546\u69b7\u3002 * \u65b9\u6cd5\u590d\u6742\u6027\u4e0e\u8ba1\u7b97\u6210\u672c\uff1a \u5f15\u5165\u72ec\u7acb\u7684\u57df\u5934\u548c\u89e3\u8026\u673a\u5236\u53ef\u80fd\u4f1a\u589e\u52a0\u6a21\u578b\u7684\u53c2\u6570\u91cf\u3001\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u8bad\u7ec3\u65f6\u95f4\uff0c\u8fd9\u5728\u8d44\u6e90\u53d7\u9650\u7684\u573a\u666f\u4e0b\u53ef\u80fd\u662f\u4e00\u4e2a\u8003\u91cf\u56e0\u7d20\u3002 * \u5bf9\u57fa\u7840\u6a21\u578b\u7684\u901a\u7528\u6027\uff1a \u8bba\u6587\u4e3b\u8981\u5173\u6ce8CLIP\u3002CLIP-DCA\u7684\u6709\u6548\u6027\u662f\u5426\u80fd\u76f4\u63a5\u6cdb\u5316\u5230\u5176\u4ed6\u7c7b\u578b\u7684\u57fa\u7840\u6a21\u578b\uff08\u4f8b\u5982\uff0c\u7eaf\u89c6\u89c9Transformer\u6216\u4e0d\u540c\u6a21\u6001\u7684\u9884\u8bad\u7ec3\u6a21\u578b\uff09\u4ecd\u9700\u8fdb\u4e00\u6b65\u63a2\u7d22\u3002 Key Findings: To address this, we present CLIP-DCA (Disentangling Classification from enhanced domain Aware representations). Our approach is motivated by the observation that while standard domain invariance losses aim to make representations domain-invariant, this can be harmful to foundation models by forcing the discarding of domain-aware representations beneficial for generalization. Links: PDF arXiv","title":"\u8bba\u6587\u6458\u8981\u5206\u6790\uff1aDomain Generalization in-the-Wild: Disentangling Classification from Domain-Aware Representations"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-01/#uitron-foundational-gui-agent-with-advanced-perception-and-planning","text":"Authors: Zhixiong Zeng, Jing Huang, Liming Zheng, Wenkang Han, Yufeng Zhong, Lei Chen, Longrong Yang, Yingjie Chu, Yuzhi He, Lin Ma Published: 2025-08-29 Categories: cs.CV Abstract: GUI agent aims to enable automated operations on Mobile/PC devices, which is an important task toward achieving artificial general intelligence. The rapid advancement of VLMs accelerates the development of GUI agents, owing to their powerful capabilities in visual understanding and task planning. However, building a GUI agent remains a challenging task due to the scarcity of operation trajectories, the availability of interactive infrastructure, and the limitation of initial capabilities in foundation models. In this work, we introduce UItron, an open-source foundational model for automatic GUI agents, featuring advanced GUI perception, grounding, and planning capabilities. UItron highlights the necessity of systemic data engineering and interactive infrastructure as foundational components for advancing GUI agent development. It not only systematically studies a series of data engineering strategies to enhance training effects, but also establishes an interactive environment connecting both Mobile and PC devices. In training, UItron adopts supervised finetuning over perception and planning tasks in various GUI scenarios, and then develop a curriculum reinforcement learning framework to enable complex reasoning and exploration for online environments. As a result, UItron achieves superior performance in benchmarks of GUI perception, grounding, and planning. In particular, UItron highlights the interaction proficiency with top-tier Chinese mobile APPs, as we identified a general lack of Chinese capabilities even in state-of-the-art solutions. To this end, we manually collect over one million steps of operation trajectories across the top 100 most popular apps, and build the offline and online agent evaluation environments. Experimental results demonstrate that UItron achieves significant progress in Chinese app scenarios, propelling GUI agents one step closer to real-world application. Analysis: UItron\u8fd9\u7bc7\u8bba\u6587\u6458\u8981\u5c55\u793a\u4e86\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\uff0c\u7279\u522b\u662fGUI\uff08\u56fe\u5f62\u7528\u6237\u754c\u9762\uff09\u81ea\u52a8\u5316\u4ee3\u7406\u65b9\u9762\u7684\u91cd\u8981\u8fdb\u5c55\u3002\u4ee5\u4e0b\u662f\u8be6\u7ec6\u5206\u6790\uff1a","title":"UItron: Foundational GUI Agent with Advanced Perception and Planning"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-01/#1-2-3_1","text":"UItron\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u3001\u57fa\u7840\u6027\u7684GUI\u81ea\u52a8\u5316\u4ee3\u7406\u6a21\u578b\uff0c\u5b83\u901a\u8fc7\u5148\u8fdb\u7684GUI\u611f\u77e5\u3001\u5b9a\u4f4d\u548c\u89c4\u5212\u80fd\u529b\uff0c\u65e8\u5728\u5b9e\u73b0\u79fb\u52a8\u548cPC\u8bbe\u5907\u7684\u81ea\u52a8\u5316\u64cd\u4f5c\u3002\u8be5\u5de5\u4f5c\u5f3a\u8c03\u4e86\u7cfb\u7edf\u6027\u6570\u636e\u5de5\u7a0b\u548c\u4ea4\u4e92\u5f0f\u57fa\u7840\u8bbe\u65bd\u7684\u91cd\u8981\u6027\uff0c\u5e76\u7ed3\u5408\u76d1\u7763\u5fae\u8c03\u4e0e\u8bfe\u7a0b\u5f3a\u5316\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86GUI\u4ee3\u7406\u5728\u590d\u6742\u5728\u7ebf\u73af\u5883\u4e2d\u7684\u8868\u73b0\u3002\u7279\u522b\u503c\u5f97\u4e00\u63d0\u7684\u662f\uff0cUItron\u901a\u8fc7\u5927\u89c4\u6a21\u624b\u52a8\u6536\u96c6\u7684\u4e2d\u6587\u5e94\u7528\u64cd\u4f5c\u8f68\u8ff9\u6570\u636e\uff0c\u586b\u8865\u4e86\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u5728\u4e2d\u6587\u5e94\u7528\u573a\u666f\u4e0b\u7684\u80fd\u529b\u7a7a\u767d\uff0c\u63a8\u52a8\u4e86GUI\u4ee3\u7406\u5411\u5b9e\u9645\u5e94\u7528\u8fc8\u8fdb\u3002","title":"1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u6d01\u603b\u7ed3 (2-3\u53e5\u8bdd)"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-01/#2_1","text":"UItron\u7684\u5173\u952e\u521b\u65b0\u548c\u65b9\u6cd5\u8bba\u4f53\u73b0\u5728\u4ee5\u4e0b\u51e0\u4e2a\u65b9\u9762\uff1a \u7cfb\u7edf\u6027\u6570\u636e\u5de5\u7a0b\u4e0e\u4ea4\u4e92\u5f0f\u57fa\u7840\u8bbe\u65bd\uff1a \u8bba\u6587\u5f3a\u8c03\u5e76\u5b9e\u8df5\u4e86\u7cfb\u7edf\u6027\u7684\u6570\u636e\u5de5\u7a0b\u7b56\u7565\u6765\u589e\u5f3a\u8bad\u7ec3\u6548\u679c\uff0c\u5e76\u5efa\u7acb\u4e86\u4e00\u4e2a\u8fde\u63a5\u79fb\u52a8\u548cPC\u8bbe\u5907\u7684\u7edf\u4e00\u4ea4\u4e92\u73af\u5883\u3002\u8fd9\u4e3aGUI\u4ee3\u7406\u7684\u5f00\u53d1\u63d0\u4f9b\u4e86\u575a\u5b9e\u7684\u57fa\u7840\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u7a00\u7f3a\u548c\u57fa\u7840\u8bbe\u65bd\u4e0d\u8db3\u7684\u6311\u6218\u3002 \u6df7\u5408\u8bad\u7ec3\u8303\u5f0f\uff1a UItron\u91c7\u7528\u4e86\u5206\u9636\u6bb5\u7684\u8bad\u7ec3\u7b56\u7565\u3002\u9996\u5148\uff0c\u901a\u8fc7\u5728\u5404\u79cdGUI\u573a\u666f\u4e0b\u7684\u611f\u77e5\u548c\u89c4\u5212\u4efb\u52a1\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\uff0c\u4e3a\u6a21\u578b\u5960\u5b9a\u521d\u59cb\u80fd\u529b\u3002\u968f\u540e\uff0c\u5f15\u5165\u8bfe\u7a0b\u5f3a\u5316\u5b66\u4e60\uff08Curriculum Reinforcement Learning\uff09\u6846\u67b6\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u5728\u5728\u7ebf\u73af\u5883\u4e2d\u8fdb\u884c\u590d\u6742\u7684\u63a8\u7406\u548c\u63a2\u7d22\uff0c\u4ece\u800c\u5904\u7406\u66f4\u52a8\u6001\u548c\u5f00\u653e\u7684\u4efb\u52a1\u3002 \u5927\u89c4\u6a21\u4e2d\u6587\u5e94\u7528\u6570\u636e\u96c6\u4e0e\u80fd\u529b\u63d0\u5347\uff1a \u9488\u5bf9\u73b0\u6709SOTA\u89e3\u51b3\u65b9\u6848\u666e\u904d\u7f3a\u4e4f\u4e2d\u6587\u5e94\u7528\u80fd\u529b\u7684\u95ee\u9898\uff0cUItron\u624b\u52a8\u6536\u96c6\u4e86\u8d85\u8fc7\u4e00\u767e\u4e07\u6b65\u7684\u3001\u6db5\u76d6\u524d100\u4e2a\u6700\u53d7\u6b22\u8fce\u4e2d\u6587\u5e94\u7528\u7684\u771f\u5b9e\u64cd\u4f5c\u8f68\u8ff9\u6570\u636e\u3002\u8fd9\u4e0d\u4ec5\u6784\u5efa\u4e86\u79bb\u7ebf\u548c\u5728\u7ebf\u7684\u8bc4\u4f30\u73af\u5883\uff0c\u4e5f\u4f7f\u5f97UItron\u5728\u4e2d\u6587\u5e94\u7528\u573a\u666f\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u586b\u8865\u4e86\u4e00\u4e2a\u91cd\u8981\u7684\u5e02\u573a\u548c\u7814\u7a76\u7a7a\u767d\u3002 \u5f00\u6e90\u57fa\u7840\u6a21\u578b\uff1a \u4f5c\u4e3a\u201c\u5f00\u6e90\u57fa\u7840\u6a21\u578b\u201d\uff0cUItron\u65e8\u5728\u964d\u4f4e\u7814\u7a76\u95e8\u69db\uff0c\u4fc3\u8fdb\u6574\u4e2aGUI\u4ee3\u7406\u9886\u57df\u7684\u53d1\u5c55\u3002","title":"2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u8bba"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-01/#3_1","text":"\u52a0\u901fAGI\u53d1\u5c55\uff1a GUI\u4ee3\u7406\u88ab\u8ba4\u4e3a\u662f\u5b9e\u73b0\u901a\u7528\u4eba\u5de5\u667a\u80fd\uff08AGI\uff09\u7684\u91cd\u8981\u4e00\u6b65\u3002UItron\u7684\u8fdb\u5c55\uff0c\u7279\u522b\u662f\u5176\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u89c4\u5212\u548c\u63a2\u7d22\u80fd\u529b\uff0c\u5c06\u76f4\u63a5\u63a8\u52a8AGI\u7814\u7a76\u3002 \u63d0\u5347GUI\u81ea\u52a8\u5316\u6c34\u5e73\uff1a UItron\u7684\u5148\u8fdb\u611f\u77e5\u3001\u5b9a\u4f4d\u548c\u89c4\u5212\u80fd\u529b\uff0c\u4ee5\u53ca\u5176\u5728\u4e2d\u6587\u5e94\u7528\u4e0a\u7684\u7a81\u7834\uff0c\u5c06\u663e\u8457\u63d0\u5347\u73b0\u6709GUI\u81ea\u52a8\u5316\u5de5\u5177\u7684\u6027\u80fd\u548c\u9002\u7528\u8303\u56f4\uff0c\u4f7f\u5176\u80fd\u5904\u7406\u66f4\u590d\u6742\u3001\u66f4\u771f\u5b9e\u7684\u4efb\u52a1\u3002 \u63a8\u52a8\u591a\u8bed\u8a00/\u591a\u6587\u5316GUI\u7814\u7a76\uff1a UItron\u5bf9\u4e2d\u6587\u5e94\u7528\u80fd\u529b\u7684\u5f3a\u8c03\u548c\u5b9e\u73b0\uff0c\u5c06\u6fc0\u52b1\u7814\u7a76\u8005\u5173\u6ce8\u5176\u4ed6\u975e\u82f1\u8bed\u8bed\u8a00\u548c\u6587\u5316\u80cc\u666f\u4e0b\u7684GUI\u4ee3\u7406\u5f00\u53d1\uff0c\u4fc3\u8fdb\u66f4\u5177\u666e\u9002\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002 \u63d0\u4f9b\u7814\u7a76\u57fa\u77f3\uff1a \u4f5c\u4e3a\u5f00\u6e90\u7684\u57fa\u7840\u6a21\u578b\uff0cUItron\u5c06\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e00\u4e2a\u5f3a\u5927\u7684\u57fa\u7ebf\u548c\u7814\u7a76\u5e73\u53f0\uff0c\u52a0\u901f\u65b0\u7b97\u6cd5\u3001\u65b0\u67b6\u6784\u7684\u9a8c\u8bc1\u548c\u8fed\u4ee3\u3002 \u4fc3\u8fdb\u6570\u636e\u5de5\u7a0b\u548c\u4ea4\u4e92\u73af\u5883\u7684\u91cd\u89c6\uff1a \u8bba\u6587\u660e\u786e\u6307\u51fa\u6570\u636e\u5de5\u7a0b\u548c\u4ea4\u4e92\u57fa\u7840\u8bbe\u65bd\u662f\u57fa\u7840\u7ec4\u4ef6\uff0c\u8fd9\u5c06\u4fc3\u4f7f\u66f4\u591a\u7814\u7a76\u8005\u548c\u5f00\u53d1\u8005\u6295\u5165\u8d44\u6e90\u6765\u6784\u5efa\u9ad8\u8d28\u91cf\u7684\u6570\u636e\u96c6\u548c\u4eff\u771f\u73af\u5883\u3002","title":"3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-01/#4_1","text":"\u8f6f\u4ef6\u6d4b\u8bd5\u4e0e\u8d28\u91cf\u4fdd\u8bc1 (QA)\uff1a \u81ea\u52a8\u5316UI\u6d4b\u8bd5\uff0c\u5c24\u5176\u662f\u5728\u79fb\u52a8\u5e94\u7528\u548c\u591a\u5e73\u53f0\u573a\u666f\u4e0b\uff0c\u53ef\u4ee5\u5927\u5e45\u63d0\u9ad8\u6548\u7387\u548c\u8986\u76d6\u7387\u3002 \u673a\u5668\u4eba\u6d41\u7a0b\u81ea\u52a8\u5316 (RPA)\uff1a \u5728\u4f01\u4e1a\u7ea7\u5e94\u7528\u4e2d\uff0cUItron\u53ef\u4ee5\u81ea\u52a8\u5316\u6267\u884c\u590d\u6742\u7684\u8de8\u5e94\u7528\u3001\u8de8\u8bbe\u5907\u4e1a\u52a1\u6d41\u7a0b\uff0c\u63d0\u9ad8\u5de5\u4f5c\u6548\u7387\u3002 \u8f85\u52a9\u6280\u672f\u4e0e\u65e0\u969c\u788d\u8bbf\u95ee\uff1a \u4e3a\u6b8b\u969c\u4eba\u58eb\u63d0\u4f9b\u66f4\u667a\u80fd\u3001\u66f4\u81ea\u4e3b\u7684\u754c\u9762\u64cd\u4f5c\u8f85\u52a9\uff0c\u63d0\u5347\u6570\u5b57\u751f\u6d3b\u7684\u4fbf\u5229\u6027\u3002 \u4e2a\u4ebaAI\u52a9\u624b\uff1a \u8d4b\u80fdAI\u52a9\u624b\u6267\u884c\u66f4\u590d\u6742\u7684\u3001\u6d89\u53ca\u591a\u6b65\u64cd\u4f5c\u548c\u8de8\u5e94\u7528\u7684\u6307\u4ee4\uff0c\u4f8b\u5982\u201c\u5e2e\u6211\u9884\u8ba2\u4e00\u5f20\u4ece\u4e0a\u6d77\u5230\u5317\u4eac\u7684\u673a\u7968\u201d\u3002 \u6570\u636e\u91c7\u96c6\u4e0e\u7f51\u7edc\u722c\u866b\uff1a \u66f4\u667a\u80fd\u5730\u4ece\u52a8\u6001\u548c\u590d\u6742\u7684GUI\u754c\u9762\u4e2d\u63d0\u53d6\u4fe1\u606f\u3002 \u8de8\u8bbe\u5907\u8ba1\u7b97\uff1a \u5b9e\u73b0\u4efb\u52a1\u5728\u624b\u673a\u548cPC\u4e4b\u95f4\u65e0\u7f1d\u5207\u6362\u548c\u81ea\u52a8\u5316\u6267\u884c\u3002 \u6559\u80b2\u4e0e\u57f9\u8bad\uff1a \u81ea\u52a8\u5316\u6f14\u793a\u8f6f\u4ef6\u64cd\u4f5c\u6d41\u7a0b\uff0c\u7528\u4e8e\u6559\u5b66\u6216\u7528\u6237\u57f9\u8bad\u3002","title":"4. \u53ef\u80fd\u53d7\u76ca\u7684\u76f8\u5173\u9886\u57df\u6216\u5e94\u7528"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-01/#5_1","text":"\u6570\u636e\u6536\u96c6\u6210\u672c\u4e0e\u53ef\u6269\u5c55\u6027\uff1a \u6458\u8981\u4e2d\u63d0\u5230\u201c\u624b\u52a8\u6536\u96c6\u8d85\u8fc7\u4e00\u767e\u4e07\u6b65\u7684\u64cd\u4f5c\u8f68\u8ff9\u201d\uff0c\u8fd9\u8868\u660e\u6570\u636e\u6536\u96c6\u662f\u4e00\u4e2a\u52b3\u52a8\u5bc6\u96c6\u578b\u4e14\u6210\u672c\u9ad8\u6602\u7684\u8fc7\u7a0b\u3002\u867d\u7136\u5bf9\u4e8e\u7279\u5b9a\u9886\u57df\uff08\u5982\u4e2d\u6587\u5e94\u7528\uff09\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u4f46\u8981\u5c06\u5176\u6269\u5c55\u5230\u5168\u7403\u6240\u6709\u8bed\u8a00\u3001\u6240\u6709\u5e94\u7528\u6216\u66f4\u957f\u5c3e\u7684\u5e94\u7528\u573a\u666f\uff0c\u5176\u53ef\u6269\u5c55\u6027\u53ef\u80fd\u9762\u4e34\u6311\u6218\u3002 \u6cdb\u5316\u80fd\u529b\uff1a \u5c3d\u7ba1\u5728\u201c\u524d100\u4e2a\u6700\u53d7\u6b22\u8fce\u7684\u4e2d\u6587\u5e94\u7528\u201d\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5bf9\u4e8e\u4e0d\u5e38\u89c1\u3001\u8bbe\u8ba1\u98ce\u683c\u8fe5\u5f02\u6216\u66f4\u65b0\u8fed\u4ee3\u9891\u7e41\u7684\u5e94\u7528\uff0c\u5176\u6cdb\u5316\u80fd\u529b\u4ecd\u9700\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u3002 \u5f3a\u5316\u5b66\u4e60\u7684\u6311\u6218\uff1a \u8bfe\u7a0b\u5f3a\u5316\u5b66\u4e60\u867d\u7136\u5f3a\u5927\uff0c\u4f46\u901a\u5e38\u5bf9\u73af\u5883\u7684\u5efa\u6a21\u3001\u5956\u52b1\u51fd\u6570\u7684\u8bbe\u8ba1\u4ee5\u53ca\u8bad\u7ec3\u7684\u7a33\u5b9a\u6027\u6709\u8f83\u9ad8\u8981\u6c42\u3002\u5728\u771f\u5b9e\u3001\u5f00\u653e\u7684GUI\u73af\u5883\u4e2d\uff0c\u5982\u4f55\u6709\u6548\u5904\u7406\u63a2\u7d22-\u5229\u7528\u56f0\u5883\u3001\u7a00\u758f\u5956\u52b1\u4ee5\u53ca\u957f\u5e8f\u5217\u51b3\u7b56\uff0c\u4ecd\u662fRL\u9762\u4e34\u7684\u56fa\u6709\u6311\u6218\u3002 \u201c\u57fa\u7840\u6a21\u578b\u201d\u7684\u5b9a\u4e49\u4e0e\u8303\u56f4\uff1a \u4f5c\u4e3a\u4e00\u4e2a\u201c\u57fa\u7840\u6a21\u578b\u201d\uff0c\u5176\u901a\u7528\u6027\uff08\u5373\u5728\u4e0d\u8fdb\u884c\u989d\u5916\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\u5904\u7406\u5168\u65b0\u4efb\u52a1\u7684\u80fd\u529b\uff09\u4ee5\u53ca\u5bf9\u9ad8\u5ea6\u62bd\u8c61\u6216\u9700\u8981\u6df1\u5c42\u8bed\u4e49\u7406\u89e3\u7684\u4efb\u52a1\u7684\u5904\u7406\u80fd\u529b\uff0c\u5728\u6458\u8981\u4e2d\u672a\u8be6\u7ec6\u8bf4\u660e\u3002 \u5bf9\u7279\u5b9a\u8bed\u8a00/\u6587\u5316UI\u7684\u4f9d\u8d56\uff1a \u867d\u7136\u89e3\u51b3\u4e86\u4e2d\u6587\u5e94\u7528\u7684\u75db\u70b9\uff0c\u4f46\u8fd9\u53ef\u80fd\u610f\u5473\u7740\u6a21\u578b\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u5b66\u4e60\u4e86\u4e2d\u6587UI\u7684\u7279\u5b9a\u6a21\u5f0f\u548c\u4e60\u60ef\u3002\u5c06\u5176\u76f4\u63a5\u5e94\u7528\u4e8e\u5176\u4ed6\u8bed\u8a00\u6216\u6587\u5316\u80cc\u666f\u4e0bUI\uff08\u4f8b\u5982\u65e5\u6587\u3001\u963f\u62c9\u4f2f\u6587UI\uff09\u65f6\uff0c\u53ef\u80fd\u9700\u8981\u7c7b\u4f3c\u7684\u5b9a\u5236\u5316\u6570\u636e\u548c\u8bad\u7ec3\u3002 Key Findings: In particular, UItron highlights the interaction proficiency with top-tier Chinese mobile APPs, as we identified a general lack of Chinese capabilities even in state-of-the-art solutions. Links: PDF arXiv","title":"5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u7684\u5c40\u9650\u6027"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-01/#cad2dmd-set-synthetic-generation-tool-of-digital-measurement-device-cad-model-datasets-for-fine-tuning-large-vision-language-models","text":"Authors: Jo\u00e3o Valente, Atabak Dehban, Rodrigo Ventura Published: 2025-08-29 Categories: cs.CV, cs.AI Abstract: Recent advancements in Large Vision-Language Models (LVLMs) have demonstrated impressive capabilities across various multimodal tasks. They continue, however, to struggle with trivial scenarios such as reading values from Digital Measurement Devices (DMDs), particularly in real-world conditions involving clutter, occlusions, extreme viewpoints, and motion blur; common in head-mounted cameras and Augmented Reality (AR) applications. Motivated by these limitations, this work introduces CAD2DMD-SET, a synthetic data generation tool designed to support visual question answering (VQA) tasks involving DMDs. By leveraging 3D CAD models, advanced rendering, and high-fidelity image composition, our tool produces diverse, VQA-labelled synthetic DMD datasets suitable for fine-tuning LVLMs. Additionally, we present DMDBench, a curated validation set of 1,000 annotated real-world images designed to evaluate model performance under practical constraints. Benchmarking three state-of-the-art LVLMs using Average Normalised Levenshtein Similarity (ANLS) and further fine-tuning LoRA's of these models with CAD2DMD-SET's generated dataset yielded substantial improvements, with InternVL showcasing a score increase of 200% without degrading on other tasks. This demonstrates that the CAD2DMD-SET training dataset substantially improves the robustness and performance of LVLMs when operating under the previously stated challenging conditions. The CAD2DMD-SET tool is expected to be released as open-source once the final version of this manuscript is prepared, allowing the community to add different measurement devices and generate their own datasets. Analysis: \u8fd9\u7bc7\u8bba\u6587\u6458\u8981\u5c55\u793a\u4e86\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u4e00\u4e2a\u91cd\u8981\u8fdb\u5c55\uff0c\u7279\u522b\u662f\u5728\u5927\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u7684\u5b9e\u9645\u5e94\u7528\u548c\u6570\u636e\u751f\u6210\u65b9\u9762\u3002","title":"CAD2DMD-SET: Synthetic Generation Tool of Digital Measurement Device CAD Model Datasets for fine-tuning Large Vision-Language Models"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-01/#1-concise-summary_3","text":"\u672c\u6587\u63d0\u51fa\u4e86CAD2DMD-SET\uff0c\u4e00\u4e2a\u521b\u65b0\u7684\u5408\u6210\u6570\u636e\u751f\u6210\u5de5\u5177\uff0c\u65e8\u5728\u89e3\u51b3\u5927\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u5728\u590d\u6742\u771f\u5b9e\u4e16\u754c\u6761\u4ef6\u4e0b\u8bfb\u53d6\u6570\u5b57\u6d4b\u91cf\u8bbe\u5907\uff08DMDs\uff09\u6570\u503c\u7684\u96be\u9898\u3002\u901a\u8fc7\u5229\u75283D CAD\u6a21\u578b\u3001\u9ad8\u7ea7\u6e32\u67d3\u548c\u9ad8\u4fdd\u771f\u56fe\u50cf\u5408\u6210\u6280\u672f\uff0c\u8be5\u5de5\u5177\u80fd\u751f\u6210\u591a\u6837\u5316\u4e14\u5e26\u6709VQA\u6807\u6ce8\u7684\u5408\u6210DMD\u6570\u636e\u96c6\uff0c\u5e76\u7ed3\u5408DMDBench\u8fd9\u4e00\u771f\u5b9e\u7684\u9a8c\u8bc1\u96c6\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c\u4f7f\u7528CAD2DMD-SET\u751f\u6210\u7684\u6570\u636e\u96c6\u5bf9LVLMs\u8fdb\u884c\u5fae\u8c03\uff0c\u80fd\u663e\u8457\u63d0\u5347\u5176\u5728DMD\u8bfb\u53d6\u4efb\u52a1\u4e0a\u7684\u9c81\u68d2\u6027\u548c\u6027\u80fd\uff0c\u4e14\u4e0d\u5f71\u54cd\u5176\u901a\u7528\u80fd\u529b\u3002","title":"1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u6d01\u603b\u7ed3 (Concise Summary)"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-01/#2-key-innovation-or-methodological-approach_3","text":"\u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u5176 \u5408\u6210\u6570\u636e\u751f\u6210\u8303\u5f0f \uff0c\u7279\u522b\u662f\u9488\u5bf9\u6570\u5b57\u6d4b\u91cf\u8bbe\u5907\uff08DMDs\uff09\u7684\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u4efb\u52a1\u3002\u8be5\u65b9\u6cd5\u5229\u7528 3D CAD\u6a21\u578b \u4f5c\u4e3a\u57fa\u7840\uff0c\u7ed3\u5408 \u9ad8\u7ea7\u6e32\u67d3\u6280\u672f\u548c\u9ad8\u4fdd\u771f\u56fe\u50cf\u5408\u6210 \uff0c\u80fd\u591f\u7cfb\u7edf\u6027\u5730\u751f\u6210\u5305\u542b\u590d\u6742\u771f\u5b9e\u4e16\u754c\u6761\u4ef6\uff08\u5982\u6742\u4e71\u3001\u906e\u6321\u3001\u6781\u7aef\u89c6\u89d2\u548c\u8fd0\u52a8\u6a21\u7cca\uff09\u7684 \u591a\u6837\u5316\u3001VQA\u6807\u6ce8\u7684\u5408\u6210\u6570\u636e\u96c6 \u3002\u6b64\u5916\uff0c\u5f15\u5165 DMDBench \u4f5c\u4e3a\u72ec\u7acb\u7684\u771f\u5b9e\u4e16\u754c\u9a8c\u8bc1\u96c6\uff0c\u4e3a\u8bc4\u4f30\u6a21\u578b\u5728\u5b9e\u9645\u7ea6\u675f\u4e0b\u7684\u6027\u80fd\u63d0\u4f9b\u4e86\u53ef\u9760\u57fa\u51c6\uff0c\u5f62\u6210\u4e86\u4e00\u4e2a\u5b8c\u6574\u7684\u8bad\u7ec3\u4e0e\u8bc4\u4f30\u95ed\u73af\u3002\u8fd9\u79cd\u5c06\u9ad8\u4fdd\u771f\u5408\u6210\u6570\u636e\u4e0e\u771f\u5b9e\u4e16\u754c\u9a8c\u8bc1\u76f8\u7ed3\u5408\u7684\u7b56\u7565\uff0c\u6709\u6548\u5730\u5f25\u8865\u4e86\u201c\u6a21\u62df\u5230\u73b0\u5b9e\u201d\uff08sim-to-real\uff09\u7684\u9e3f\u6c9f\u3002","title":"2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u5b66\u65b9\u6cd5 (Key Innovation or Methodological Approach)"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-01/#3-potential-impact-on-the-field_3","text":"\u63d0\u5347LVLM\u5728\u7279\u5b9a\u9886\u57df\u5e94\u7528\u7684\u9c81\u68d2\u6027\u4e0e\u51c6\u786e\u6027\uff1a \u672c\u6587\u8bc1\u660e\u4e86\u901a\u8fc7\u9ad8\u8d28\u91cf\u5408\u6210\u6570\u636e\u5bf9LVLMs\u8fdb\u884c\u5fae\u8c03\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u5176\u5728\u7279\u5b9a\u3001\u590d\u6742\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e0b\u7684\u6027\u80fd\uff0c\u4e14\u4e0d\u5f71\u54cd\u5176\u901a\u7528\u80fd\u529b\uff0c\u4e3aLVLM\u5728\u5de5\u4e1a\u3001AR\u7b49\u9886\u57df\u7684\u843d\u5730\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002 \u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff1a \u5bf9\u4e8e\u96be\u4ee5\u83b7\u53d6\u5927\u91cf\u771f\u5b9e\u4e16\u754c\u6807\u6ce8\u6570\u636e\u7684\u7279\u5b9a\u89c6\u89c9\u4efb\u52a1\uff0cCAD2DMD-SET\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u5408\u6210\u6570\u636e\u751f\u6210\u89e3\u51b3\u65b9\u6848\uff0c\u964d\u4f4e\u4e86\u6a21\u578b\u5f00\u53d1\u548c\u90e8\u7f72\u7684\u6210\u672c\u3002 \u63a8\u52a8\u5408\u6210\u6570\u636e\u7814\u7a76\uff1a \u5f3a\u8c03\u4e86\u9ad8\u4fdd\u771f\u6e32\u67d3\u548c\u667a\u80fd\u5408\u6210\u5728\u5f25\u5408\u201c\u6a21\u62df\u5230\u73b0\u5b9e\u201d\u9e3f\u6c9f\u4e2d\u7684\u5173\u952e\u4f5c\u7528\uff0c\u4e3a\u672a\u6765\u66f4\u591a\u5229\u7528\u5408\u6210\u6570\u636e\u8bad\u7ec3\u89c6\u89c9\u6a21\u578b\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u8303\u4f8b\u3002 \u793e\u533a\u8d21\u732e\uff1a \u8ba1\u5212\u7684\u5f00\u6e90\u53d1\u5e03\u5c06\u4fc3\u8fdb\u793e\u533a\u5728\u6b64\u57fa\u7840\u4e0a\u8fdb\u884c\u6269\u5c55\uff0c\u652f\u6301\u66f4\u591a\u6d4b\u91cf\u8bbe\u5907\u548c\u5e94\u7528\u573a\u666f\uff0c\u52a0\u901f\u76f8\u5173\u6280\u672f\u7684\u53d1\u5c55\u3002","title":"3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd (Potential Impact on the Field)"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-01/#4-related-areas-or-applications_3","text":"\u5de5\u4e1a\u81ea\u52a8\u5316\u4e0e\u8d28\u91cf\u63a7\u5236\uff1a \u5728\u5de5\u5382\u73af\u5883\u4e2d\uff0c\u81ea\u52a8\u8bfb\u53d6\u5404\u79cd\u4eea\u8868\u3001\u4f20\u611f\u5668\u548c\u663e\u793a\u5c4f\u7684\u6570\u503c\uff0c\u7528\u4e8e\u8fc7\u7a0b\u76d1\u63a7\u3001\u6545\u969c\u8bca\u65ad\u548c\u4ea7\u54c1\u8d28\u91cf\u68c0\u6d4b\u3002 \u589e\u5f3a\u73b0\u5b9e\uff08AR\uff09\u4e0e\u6df7\u5408\u73b0\u5b9e\uff08MR\uff09\uff1a \u7ed3\u5408\u5934\u6234\u5f0f\u8bbe\u5907\uff0c\u5b9e\u73b0\u5bf9\u7269\u7406\u4e16\u754c\u4e2d\u6570\u5b57\u6d4b\u91cf\u8bbe\u5907\u7684\u5b9e\u65f6\u8bc6\u522b\u548c\u6570\u503c\u63d0\u53d6\uff0c\u4e3a\u7528\u6237\u63d0\u4f9b\u4e0a\u4e0b\u6587\u4fe1\u606f\u6216\u64cd\u4f5c\u6307\u5bfc\u3002 \u673a\u5668\u4eba\u6280\u672f\uff1a \u8d4b\u4e88\u673a\u5668\u4eba\u8bc6\u522b\u548c\u7406\u89e3\u73af\u5883\u4e2d\u5404\u79cd\u6570\u5b57\u663e\u793a\u5668\u7684\u80fd\u529b\uff0c\u4f8b\u5982\u8bfb\u53d6\u8bbe\u5907\u72b6\u6001\u3001\u5b8c\u6210\u7279\u5b9a\u4efb\u52a1\u3002 \u8bbe\u5907\u7ef4\u62a4\u4e0e\u68c0\u6d4b\uff1a \u5e2e\u52a9\u6280\u672f\u4eba\u5458\u901a\u8fc7\u89c6\u89c9\u7cfb\u7edf\u5feb\u901f\u51c6\u786e\u5730\u83b7\u53d6\u8bbe\u5907\u8bfb\u6570\uff0c\u63d0\u9ad8\u7ef4\u62a4\u6548\u7387\u548c\u51c6\u786e\u6027\u3002 \u667a\u80fd\u7a7f\u6234\u8bbe\u5907\uff1a \u63d0\u5347\u667a\u80fd\u773c\u955c\u7b49\u8bbe\u5907\u5728\u590d\u6742\u73af\u5883\u4e2d\u7406\u89e3\u548c\u4ea4\u4e92\u7269\u7406\u4e16\u754c\u4fe1\u606f\u7684\u80fd\u529b\u3002","title":"4. \u53ef\u80fd\u53d7\u76ca\u7684\u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 (Related Areas or Applications)"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-01/#5-limitations-inferable-from-the-abstract","text":"\u9886\u57df\u7279\u5f02\u6027\uff1a CAD2DMD-SET\u5de5\u5177\u662f\u4e13\u95e8\u4e3a\u6570\u5b57\u6d4b\u91cf\u8bbe\u5907\uff08DMDs\uff09\u8bbe\u8ba1\u7684\u3002\u5c06\u5176\u6269\u5c55\u5230\u5176\u4ed6\u7c7b\u578b\u7684\u89c6\u89c9\u4efb\u52a1\u6216\u7269\u4f53\uff08\u4f8b\u5982\uff0c\u6a21\u62df\u4eea\u8868\u3001\u66f4\u590d\u6742\u7684\u5de5\u4e1a\u8bbe\u5907\uff09\u5c06\u9700\u8981\u65b0\u76843D CAD\u6a21\u578b\u548c\u53ef\u80fd\u4e0d\u540c\u7684\u6e32\u67d3\u53ca\u5408\u6210\u7b56\u7565\uff0c\u8fd9\u5e76\u975e\u4e00\u4e2a\u901a\u7528\u89e3\u51b3\u65b9\u6848\u3002 CAD\u6a21\u578b\u4f9d\u8d56\u6027\uff1a \u5408\u6210\u6570\u636e\u7684\u8d28\u91cf\u548c\u591a\u6837\u6027\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u53ef\u75283D CAD\u6a21\u578b\u7684\u51c6\u786e\u6027\u548c\u4e30\u5bcc\u6027\u3002\u83b7\u53d6\u9ad8\u8d28\u91cf\u7684CAD\u6a21\u578b\u672c\u8eab\u53ef\u80fd\u662f\u4e00\u4e2a\u6311\u6218\u6216\u6210\u672c\u6765\u6e90\u3002 \u6f5c\u5728\u7684\u201c\u6a21\u62df\u5230\u73b0\u5b9e\u201d\u9e3f\u6c9f\uff1a \u5c3d\u7ba1\u8bba\u6587\u5f3a\u8c03\u4e86\u201c\u9ad8\u4fdd\u771f\u56fe\u50cf\u5408\u6210\u201d\uff0c\u4f46\u5408\u6210\u6570\u636e\u4e0e\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e4b\u95f4\u4ecd\u53ef\u80fd\u5b58\u5728\u7ec6\u5fae\u7684\u5206\u5e03\u5dee\u5f02\uff08\u5373\u6b8b\u4f59\u7684\u201c\u6a21\u62df\u5230\u73b0\u5b9e\u201d\u9e3f\u6c9f\uff09\uff0c\u8fd9\u53ef\u80fd\u5728\u67d0\u4e9b\u672a\u88abDMDBench\u5145\u5206\u8986\u76d6\u7684\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u6765\u3002DMDBench\u867d\u7136\u63d0\u4f9b\u4e86\u771f\u5b9e\u4e16\u754c\u9a8c\u8bc1\uff0c\u4f46\u51761000\u5f20\u56fe\u50cf\u7684\u89c4\u6a21\u53ef\u80fd\u4e0d\u8db3\u4ee5\u5b8c\u5168\u4ee3\u8868\u6240\u6709\u6781\u7aef\u771f\u5b9e\u4e16\u754c\u6761\u4ef6\u3002 \u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\uff1a \u751f\u6210\u9ad8\u4fdd\u771f\u3001\u591a\u6837\u5316\u7684\u5408\u6210\u6570\u636e\u96c6\uff0c\u7279\u522b\u662f\u6d89\u53ca\u9ad8\u7ea7\u6e32\u67d3\u548c\u590d\u6742\u573a\u666f\u5408\u6210\u65f6\uff0c\u53ef\u80fd\u9700\u8981\u5927\u91cf\u7684\u8ba1\u7b97\u8d44\u6e90\u548c\u65f6\u95f4\u3002 Key Findings: Motivated by these limitations, this work introduces CAD2DMD-SET, a synthetic data generation tool designed to support visual question answering (VQA) tasks involving DMDs. By leveraging 3D CAD models, advanced rendering, and high-fidelity image composition, our tool produces diverse, VQA-labelled synthetic DMD datasets suitable for fine-tuning LVLMs. Additionally, we present DMDBench, a curated validation set of 1,000 annotated real-world images designed to evaluate model performance under practical constraints. Benchmarking three state-of-the-art LVLMs using Average Normalised Levenshtein Similarity (ANLS) and further fine-tuning LoRA's of these models with CAD2DMD-SET's generated dataset yielded substantial improvements, with InternVL showcasing a score increase of 200% without degrading on other tasks. Links: PDF arXiv","title":"5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u7684\u5c40\u9650\u6027 (Limitations Inferable from the Abstract)"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-01/#mapping-like-a-skeptic-probabilistic-bev-projection-for-online-hd-mapping","text":"Authors: Fatih Erdo\u011fan, Merve Rabia Bar\u0131n, Fatma G\u00fcney Published: 2025-08-29 Categories: cs.CV Abstract: Constructing high-definition (HD) maps from sensory input requires accurately mapping the road elements in image space to the Bird's Eye View (BEV) space. The precision of this mapping directly impacts the quality of the final vectorized HD map. Existing HD mapping approaches outsource the projection to standard mapping techniques, such as attention-based ones. However, these methods struggle with accuracy due to generalization problems, often hallucinating non-existent road elements. Our key idea is to start with a geometric mapping based on camera parameters and adapt it to the scene to extract relevant map information from camera images. To implement this, we propose a novel probabilistic projection mechanism with confidence scores to (i) refine the mapping to better align with the scene and (ii) filter out irrelevant elements that should not influence HD map generation. In addition, we improve temporal processing by using confidence scores to selectively accumulate reliable information over time. Experiments on new splits of the nuScenes and Argoverse2 datasets demonstrate improved performance over state-of-the-art approaches, indicating better generalization. The improvements are particularly pronounced on nuScenes and in the challenging long perception range. Our code and model checkpoints are available at https://github.com/Fatih-Erdogan/mapping-like-skeptic . Analysis: \u8fd9\u7bc7\u8bba\u6587\u7684\u6458\u8981\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5173\u4e8e\u5728\u7ebf\u9ad8\u6e05\u5730\u56fe\u6784\u5efa\u4e2d\u9e1f\u77b0\u56fe\uff08BEV\uff09\u6295\u5f71\u7684\u5173\u952e\u6539\u8fdb\u3002\u4ee5\u4e0b\u662f\u8be6\u7ec6\u5206\u6790\uff1a","title":"Mapping like a Skeptic: Probabilistic BEV Projection for Online HD Mapping"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-01/#1-a-concise-summary-of-the-papers-main-contribution","text":"\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6982\u7387\u6027\u6295\u5f71\u673a\u5236\uff0c\u7528\u4e8e\u5c06\u56fe\u50cf\u7a7a\u95f4\u4e2d\u7684\u9053\u8def\u5143\u7d20\u51c6\u786e\u6620\u5c04\u5230\u9e1f\u77b0\u56fe\uff08BEV\uff09\u7a7a\u95f4\uff0c\u4ee5\u6784\u5efa\u5728\u7ebf\u9ad8\u6e05\u5730\u56fe\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u57fa\u4e8e\u76f8\u673a\u53c2\u6570\u7684\u51e0\u4f55\u6620\u5c04\u4e0e\u573a\u666f\u81ea\u9002\u5e94\u7684\u7f6e\u4fe1\u5ea6\u5206\u6570\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\uff09\u5728\u6cdb\u5316\u6027\u548c\u907f\u514d\u5e7b\u89c9\uff08hallucination\uff09\u65b9\u9762\u7684\u4e0d\u8db3\u3002\u5b83\u901a\u8fc7\u7f6e\u4fe1\u5ea6\u5206\u6570\u6765\u4f18\u5316\u6620\u5c04\u3001\u8fc7\u6ee4\u4e0d\u76f8\u5173\u5143\u7d20\uff0c\u5e76\u9009\u62e9\u6027\u5730\u7d2f\u79ef\u53ef\u9760\u7684\u957f\u671f\u4fe1\u606f\uff0c\u4ece\u800c\u663e\u8457\u63d0\u9ad8\u4e86\u9ad8\u6e05\u5730\u56fe\u7684\u7cbe\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\u3002","title":"1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u6d01\u603b\u7ed3 (A concise summary of the paper's main contribution)"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-01/#2-the-key-innovation-or-methodological-approach","text":"\u8be5\u8bba\u6587\u7684\u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u5176 \u201c\u6000\u7591\u8bba\u8005\u5f0f\u201d\u7684\u6982\u7387\u6027\u6295\u5f71\u673a\u5236 \uff0c\u5b83\u4e0e\u4f20\u7edf\u7eaf\u7cb9\u4f9d\u8d56\u5b66\u4e60\u6216\u6ce8\u610f\u529b\u673a\u5236\u7684\u65b9\u6cd5\u5f62\u6210\u5bf9\u6bd4\uff1a \u51e0\u4f55\u5148\u9a8c\u4e0e\u573a\u666f\u81ea\u9002\u5e94\u7ed3\u5408\uff1a \u4e0d\u540c\u4e8e\u5b8c\u5168\u4f9d\u8d56\u6570\u636e\u9a71\u52a8\u7684\u5b66\u4e60\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u9996\u5148\u5229\u7528\u57fa\u4e8e\u76f8\u673a\u53c2\u6570\u7684\u51e0\u4f55\u6620\u5c04\u4f5c\u4e3a\u8d77\u70b9\uff0c\u7136\u540e\u901a\u8fc7\u5b66\u4e60\u673a\u5236\u4f7f\u5176\u9002\u5e94\u7279\u5b9a\u573a\u666f\u3002\u8fd9\u63d0\u4f9b\u4e86\u4e00\u4e2a\u66f4\u7a33\u5065\u7684\u521d\u59cb\u57fa\u7840\uff0c\u51cf\u5c11\u4e86\u6a21\u578b\u4ece\u96f6\u5f00\u59cb\u5b66\u4e60\u6295\u5f71\u7684\u96be\u5ea6\u3002 \u7f6e\u4fe1\u5ea6\u5206\u6570\u9a71\u52a8\u7684\u7cbe\u70bc\u4e0e\u8fc7\u6ee4\uff1a \u5f15\u5165\u4e86\u7f6e\u4fe1\u5ea6\u5206\u6570\u6765\uff1a \u7cbe\u70bc\u6620\u5c04\uff1a \u4f7f\u6295\u5f71\u66f4\u597d\u5730\u4e0e\u5b9e\u9645\u573a\u666f\u5bf9\u9f50\u3002 \u8fc7\u6ee4\u65e0\u5173\u5143\u7d20\uff1a \u660e\u786e\u5730\u8bc6\u522b\u5e76\u5254\u9664\u90a3\u4e9b\u53ef\u80fd\u7531\u6a21\u578b\u201c\u5e7b\u89c9\u201d\u51fa\u6765\u7684\u3001\u4e0d\u5b58\u5728\u7684\u9053\u8def\u5143\u7d20\uff0c\u4ece\u800c\u63d0\u9ad8\u5730\u56fe\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002 \u65f6\u95f4\u7ef4\u5ea6\u4e0a\u7684\u9009\u62e9\u6027\u7d2f\u79ef\uff1a \u5229\u7528\u7f6e\u4fe1\u5ea6\u5206\u6570\u5728\u65f6\u95f4\u5e8f\u5217\u4e0a\u9009\u62e9\u6027\u5730\u7d2f\u79ef\u53ef\u9760\u4fe1\u606f\uff0c\u589e\u5f3a\u4e86\u5728\u7ebf\u5730\u56fe\u6784\u5efa\u7684\u9c81\u68d2\u6027\u548c\u4e00\u81f4\u6027\uff0c\u907f\u514d\u4e86\u4e0d\u53ef\u9760\u4fe1\u606f\u7684\u7d2f\u79ef\u3002","title":"2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u5b66\u65b9\u6cd5 (The key innovation or methodological approach)"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-01/#3-potential-impact-on-the-field_4","text":"\u63d0\u9ad8\u9ad8\u6e05\u5730\u56fe\u7684\u53ef\u9760\u6027\u4e0e\u5b89\u5168\u6027\uff1a \u901a\u8fc7\u51cf\u5c11\u5e7b\u89c9\u548c\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u66f4\u51c6\u786e\u3001\u66f4\u53ef\u9760\u7684\u9ad8\u6e05\u5730\u56fe\uff0c\u8fd9\u5bf9\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u5b89\u5168\u8fd0\u884c\u81f3\u5173\u91cd\u8981\u3002 \u63a8\u52a8\u6df7\u5408\u5f0fBEV\u6295\u5f71\u65b9\u6cd5\u7684\u53d1\u5c55\uff1a \u51e0\u4f55\u5148\u9a8c\u4e0e\u5b66\u4e60\u673a\u5236\u7684\u7ed3\u5408\u53ef\u80fd\u542f\u53d1\u66f4\u591a\u6df7\u5408\u5f0f\u65b9\u6cd5\uff0c\u5728\u6570\u636e\u9a71\u52a8\u7684\u7075\u6d3b\u6027\u548c\u51e0\u4f55\u7ea6\u675f\u7684\u9c81\u68d2\u6027\u4e4b\u95f4\u627e\u5230\u66f4\u597d\u7684\u5e73\u8861\u3002 \u89e3\u51b3\u957f\u5c3e\u95ee\u9898\u548c\u6cdb\u5316\u6311\u6218\uff1a \u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u6216\u4e0d\u5e38\u89c1\u573a\u666f\u4e0b\u7684\u6cdb\u5316\u6027\u5dee\u662f\u4e00\u4e2a\u666e\u904d\u95ee\u9898\u3002\u8be5\u8bba\u6587\u7684\u6539\u8fdb\uff0c\u5c24\u5176\u662f\u5728nuScenes\u548c\u957f\u611f\u77e5\u8ddd\u79bb\u4e0a\u7684\u8868\u73b0\uff0c\u8868\u660e\u5b83\u80fd\u66f4\u597d\u5730\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\u3002 \u4fc3\u8fdb\u5728\u7ebf\u5730\u56fe\u6784\u5efa\u7684\u5b9e\u7528\u5316\uff1a \u5f3a\u8c03\u5728\u7ebf\u5904\u7406\u548c\u65f6\u95f4\u4fe1\u606f\u7d2f\u79ef\uff0c\u4f7f\u5176\u66f4\u9002\u7528\u4e8e\u5b9e\u9645\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u5b9e\u65f6\u5730\u56fe\u6784\u5efa\u9700\u6c42\u3002","title":"3. \u5bf9\u9886\u57df\u7684\u6f5c\u5728\u5f71\u54cd (Potential impact on the field)"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-01/#4-related-areas-or-applications-that-might-benefit-from-this-research","text":"\u81ea\u52a8\u9a7e\u9a76\uff1a \u8fd9\u662f\u6700\u76f4\u63a5\u7684\u5e94\u7528\u9886\u57df\uff0c\u9ad8\u6e05\u5730\u56fe\u662f\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u3001\u5b9a\u4f4d\u548c\u89c4\u5212\u7684\u6838\u5fc3\u7ec4\u6210\u90e8\u5206\u3002 \u673a\u5668\u4eba\u5bfc\u822a\u4e0eSLAM\uff1a \u4efb\u4f55\u9700\u8981\u4ece\u4f20\u611f\u5668\u8f93\u5165\u6784\u5efa\u73af\u5883\u5730\u56fe\u7684\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u90fd\u53ef\u4ee5\u4ece\u66f4\u51c6\u786e\u3001\u66f4\u53ef\u9760\u7684BEV\u6295\u5f71\u548c\u5730\u56fe\u6784\u5efa\u4e2d\u53d7\u76ca\u3002 3D\u573a\u666f\u91cd\u5efa\uff1a \u4ece2D\u56fe\u50cf\u4e2d\u51c6\u786e\u63a8\u65ad3D\u7ed3\u6784\u548c\u5e03\u5c40\uff0c\u5c24\u5176\u662f\u5728\u7ed3\u6784\u5316\u73af\u5883\u4e2d\uff08\u5982\u9053\u8def\uff09\uff0c\u8be5\u65b9\u6cd5\u53ef\u4ee5\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u53c2\u8003\u3002 \u57ce\u5e02\u89c4\u5212\u4e0e\u6d4b\u7ed8\uff1a \u81ea\u52a8\u5316\u5730\u4ece\u56fe\u50cf\u6570\u636e\u751f\u6210\u9ad8\u7cbe\u5ea6\u5730\u56fe\uff0c\u53ef\u4ee5\u63d0\u9ad8\u57ce\u5e02\u57fa\u7840\u8bbe\u65bd\u7ba1\u7406\u548c\u6d4b\u7ed8\u7684\u6548\u7387\u3002 \u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\uff1a \u5b9e\u65f6\u3001\u9ad8\u7cbe\u5ea6\u7684\u9053\u8def\u5143\u7d20\u4fe1\u606f\u5bf9\u4e8e\u4ea4\u901a\u6d41\u7ba1\u7406\u3001\u4e8b\u4ef6\u68c0\u6d4b\u7b49\u5e94\u7528\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002","title":"4. \u53ef\u80fd\u53d7\u76ca\u4e8e\u8fd9\u9879\u7814\u7a76\u7684\u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 (Related areas or applications that might benefit from this research)"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-01/#5-any-limitations-that-can-be-inferred-from-the-abstract","text":"\u5bf9\u76f8\u673a\u53c2\u6570\u7684\u4f9d\u8d56\uff1a \u6458\u8981\u4e2d\u63d0\u5230\u201c\u57fa\u4e8e\u76f8\u673a\u53c2\u6570\u7684\u51e0\u4f55\u6620\u5c04\u201d\uff0c\u8fd9\u610f\u5473\u7740\u8be5\u65b9\u6cd5\u53ef\u80fd\u5bf9\u51c6\u786e\u7684\u76f8\u673a\u6807\u5b9a\u6709\u4e00\u5b9a\u4f9d\u8d56\u3002\u5982\u679c\u76f8\u673a\u6807\u5b9a\u4e0d\u51c6\u786e\uff0c\u521d\u59cb\u7684\u51e0\u4f55\u6295\u5f71\u53ef\u80fd\u4f1a\u5f15\u5165\u8bef\u5dee\uff0c\u5c3d\u7ba1\u540e\u7eed\u7684\u6982\u7387\u673a\u5236\u4f1a\u5c1d\u8bd5\u7ea0\u6b63\u3002 \u7f6e\u4fe1\u5ea6\u5206\u6570\u7684\u9c81\u68d2\u6027\uff1a \u5173\u952e\u5728\u4e8e\u5982\u4f55\u51c6\u786e\u5730\u751f\u6210\u548c\u5229\u7528\u7f6e\u4fe1\u5ea6\u5206\u6570\u3002\u5982\u679c\u7f6e\u4fe1\u5ea6\u5206\u6570\u672c\u8eab\u5728\u67d0\u4e9b\u6781\u7aef\u6216\u6a21\u7cca\u573a\u666f\u4e0b\u4e0d\u53ef\u9760\uff0c\u53ef\u80fd\u4f1a\u5f71\u54cd\u6574\u4f53\u6027\u80fd\u3002 \u201c\u573a\u666f\u81ea\u9002\u5e94\u201d\u7684\u8303\u56f4\uff1a \u6458\u8981\u63d0\u5230\u201c\u9002\u5e94\u573a\u666f\u4ee5\u63d0\u53d6\u76f8\u5173\u5730\u56fe\u4fe1\u606f\u201d\uff0c\u4f46\u672a\u8be6\u7ec6\u8bf4\u660e\u8fd9\u79cd\u9002\u5e94\u6027\u5728\u9762\u5bf9\u9ad8\u5ea6\u52a8\u6001\u3001\u590d\u6742\u6216\u5b8c\u5168\u65b0\u9896\u7684\u573a\u666f\u65f6\u7684\u9c81\u68d2\u6027\u5982\u4f55\u3002 \u8ba1\u7b97\u5f00\u9500\uff1a \u5f15\u5165\u6982\u7387\u6027\u673a\u5236\u548c\u7f6e\u4fe1\u5ea6\u5206\u6570\uff0c\u4ee5\u53ca\u65f6\u95f4\u4e0a\u7684\u4fe1\u606f\u7d2f\u79ef\uff0c\u53ef\u80fd\u4f1a\u589e\u52a0\u8ba1\u7b97\u590d\u6742\u6027\uff0c\u8fd9\u5bf9\u4e8e\u5728\u7ebf\uff08\u5b9e\u65f6\uff09\u5e94\u7528\u6765\u8bf4\u662f\u4e00\u4e2a\u9700\u8981\u5173\u6ce8\u7684\u95ee\u9898\u3002 \u6570\u636e\u4f9d\u8d56\u6027\uff1a \u5c3d\u7ba1\u58f0\u79f0\u6cdb\u5316\u6027\u66f4\u597d\uff0c\u4f46\u5b9e\u9a8c\u4ecd\u5728nuScenes\u548cArgoverse2\u6570\u636e\u96c6\u7684\u65b0\u5206\u5272\u4e0a\u8fdb\u884c\u3002\u5728\u66f4\u5e7f\u6cdb\u3001\u66f4\u591a\u6837\u5316\u7684\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u7684\u8868\u73b0\u4ecd\u9700\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u3002 Key Findings: To implement this, we propose a novel probabilistic projection mechanism with confidence scores to (i) refine the mapping to better align with the scene and (ii) filter out irrelevant elements that should not influence HD map generation. Experiments on new splits of the nuScenes and Argoverse2 datasets demonstrate improved performance over state-of-the-art approaches, indicating better generalization. Links: PDF arXiv","title":"5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u51fa\u7684\u5c40\u9650\u6027 (Any limitations that can be inferred from the abstract)"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-02/","text":"Arxiv Computer Vision Papers - 2025-09-02 Executive Summary \u597d\u7684\uff0c\u4f5c\u4e3a\u4e00\u540d\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u7814\u7a76\u52a9\u7406\uff0c\u6211\u4e3a\u60a8\u51c6\u5907\u4e86\u8fd9\u4efd\u6bcf\u65e5Arxiv\u8ba1\u7b97\u673a\u89c6\u89c9\u8bba\u6587\u6267\u884c\u6458\u8981\u3002 Arxiv\u8ba1\u7b97\u673a\u89c6\u89c9\u6bcf\u65e5\u62a5\u544a\u6267\u884c\u6458\u8981 \u62a5\u544a\u65e5\u671f\uff1a [\u63d2\u5165\u5f53\u524d\u65e5\u671f\uff0c\u4f8b\u5982\uff1a2023\u5e7410\u670827\u65e5] \u4eca\u65e5\u8bba\u6587\u6570\u91cf\uff1a 0 \u6982\u8ff0\uff1a \u4eca\u65e5Arxiv\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u672a\u53d1\u5e03\u65b0\u7684\u8bba\u6587\u3002\u56e0\u6b64\uff0c\u672c\u62a5\u544a\u65e0\u6cd5\u63d0\u4f9b\u5f53\u65e5\u8bba\u6587\u7684\u5177\u4f53\u5206\u6790\u548c\u8d8b\u52bf\u6d1e\u5bdf\u3002 1. \u4e3b\u8981\u4e3b\u9898\u4e0e\u8d8b\u52bf\uff1a \u7531\u4e8e\u4eca\u65e5\u65e0\u65b0\u8bba\u6587\u53d1\u5e03\uff0c\u672c\u62a5\u544a\u65e0\u6cd5\u8bc6\u522b\u5f53\u65e5\u7684\u4e3b\u8981\u7814\u7a76\u4e3b\u9898\u6216\u8d8b\u52bf\u3002 2. \u91cd\u8981\u4e0e\u521b\u65b0\u8bba\u6587\uff1a \u4eca\u65e5\u65e0\u65b0\u8bba\u6587\u53ef\u4f9b\u7a81\u51fa\u4ecb\u7ecd\u3002 3. \u65b0\u5174\u7814\u7a76\u65b9\u5411\u4e0e\u6280\u672f\uff1a \u4eca\u65e5\u65e0\u65b0\u8bba\u6587\u53ef\u4f9b\u8bc6\u522b\u65b0\u5174\u7814\u7a76\u65b9\u5411\u6216\u6280\u672f\u3002 4. \u5efa\u8bae\u9605\u8bfb\u8bba\u6587\uff1a \u4eca\u65e5\u65e0\u65b0\u8bba\u6587\u53ef\u4f9b\u63a8\u8350\u3002 \u603b\u7ed3\uff1a \u4eca\u65e5Arxiv\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u6ca1\u6709\u65b0\u7684\u8bba\u6587\u53d1\u5e03\u3002\u6211\u4eec\u5c06\u7ee7\u7eed\u5bc6\u5207\u5173\u6ce8Arxiv\u7684\u66f4\u65b0\uff0c\u5e76\u5728\u6709\u65b0\u8bba\u6587\u53d1\u5e03\u65f6\u63d0\u4f9b\u53ca\u65f6\u62a5\u544a\u3002 Table of Contents Papers","title":"Arxiv Computer Vision Papers - 2025-09-02"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-02/#arxiv-computer-vision-papers-2025-09-02","text":"","title":"Arxiv Computer Vision Papers - 2025-09-02"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-02/#executive-summary","text":"\u597d\u7684\uff0c\u4f5c\u4e3a\u4e00\u540d\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u7814\u7a76\u52a9\u7406\uff0c\u6211\u4e3a\u60a8\u51c6\u5907\u4e86\u8fd9\u4efd\u6bcf\u65e5Arxiv\u8ba1\u7b97\u673a\u89c6\u89c9\u8bba\u6587\u6267\u884c\u6458\u8981\u3002 Arxiv\u8ba1\u7b97\u673a\u89c6\u89c9\u6bcf\u65e5\u62a5\u544a\u6267\u884c\u6458\u8981 \u62a5\u544a\u65e5\u671f\uff1a [\u63d2\u5165\u5f53\u524d\u65e5\u671f\uff0c\u4f8b\u5982\uff1a2023\u5e7410\u670827\u65e5] \u4eca\u65e5\u8bba\u6587\u6570\u91cf\uff1a 0 \u6982\u8ff0\uff1a \u4eca\u65e5Arxiv\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u672a\u53d1\u5e03\u65b0\u7684\u8bba\u6587\u3002\u56e0\u6b64\uff0c\u672c\u62a5\u544a\u65e0\u6cd5\u63d0\u4f9b\u5f53\u65e5\u8bba\u6587\u7684\u5177\u4f53\u5206\u6790\u548c\u8d8b\u52bf\u6d1e\u5bdf\u3002 1. \u4e3b\u8981\u4e3b\u9898\u4e0e\u8d8b\u52bf\uff1a \u7531\u4e8e\u4eca\u65e5\u65e0\u65b0\u8bba\u6587\u53d1\u5e03\uff0c\u672c\u62a5\u544a\u65e0\u6cd5\u8bc6\u522b\u5f53\u65e5\u7684\u4e3b\u8981\u7814\u7a76\u4e3b\u9898\u6216\u8d8b\u52bf\u3002 2. \u91cd\u8981\u4e0e\u521b\u65b0\u8bba\u6587\uff1a \u4eca\u65e5\u65e0\u65b0\u8bba\u6587\u53ef\u4f9b\u7a81\u51fa\u4ecb\u7ecd\u3002 3. \u65b0\u5174\u7814\u7a76\u65b9\u5411\u4e0e\u6280\u672f\uff1a \u4eca\u65e5\u65e0\u65b0\u8bba\u6587\u53ef\u4f9b\u8bc6\u522b\u65b0\u5174\u7814\u7a76\u65b9\u5411\u6216\u6280\u672f\u3002 4. \u5efa\u8bae\u9605\u8bfb\u8bba\u6587\uff1a \u4eca\u65e5\u65e0\u65b0\u8bba\u6587\u53ef\u4f9b\u63a8\u8350\u3002 \u603b\u7ed3\uff1a \u4eca\u65e5Arxiv\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u6ca1\u6709\u65b0\u7684\u8bba\u6587\u53d1\u5e03\u3002\u6211\u4eec\u5c06\u7ee7\u7eed\u5bc6\u5207\u5173\u6ce8Arxiv\u7684\u66f4\u65b0\uff0c\u5e76\u5728\u6709\u65b0\u8bba\u6587\u53d1\u5e03\u65f6\u63d0\u4f9b\u53ca\u65f6\u62a5\u544a\u3002","title":"Executive Summary"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-02/#table-of-contents","text":"","title":"Table of Contents"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-02/#papers","text":"","title":"Papers"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-03/","text":"Arxiv Computer Vision Papers - 2025-09-03 Executive Summary \u597d\u7684\uff0c\u4f5c\u4e3a\u4e00\u540d\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u7814\u7a76\u52a9\u7406\uff0c\u6211\u4e3a\u60a8\u51c6\u5907\u4e86\u8fd9\u4efd\u6bcf\u65e5Arxiv\u8ba1\u7b97\u673a\u89c6\u89c9\u8bba\u6587\u6267\u884c\u6458\u8981\u3002 \u6bcf\u65e5Arxiv\u8ba1\u7b97\u673a\u89c6\u89c9\u8bba\u6587\u6267\u884c\u6458\u8981 \u65e5\u671f\uff1a [\u8bf7\u5728\u6b64\u5904\u63d2\u5165\u5f53\u524d\u65e5\u671f\uff0c\u4f8b\u5982\uff1a2023\u5e7410\u670827\u65e5] \u6982\u89c8\uff1a \u672c\u62a5\u544a\u65e8\u5728\u4e3a\u5fd9\u788c\u7684\u7814\u7a76\u4eba\u5458\u63d0\u4f9bArxiv\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u6700\u65b0\u8bba\u6587\u7684\u5feb\u901f\u6982\u89c8\u3002 \u4eca\u65e5Arxiv\u8ba1\u7b97\u673a\u89c6\u89c9\u677f\u5757\u672a\u6709\u65b0\u8bba\u6587\u53d1\u5e03\u3002 1. \u4e3b\u8981\u4e3b\u9898\u6216\u8d8b\u52bf\u6982\u89c8\uff1a \u7531\u4e8e\u4eca\u65e5Arxiv\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u65e0\u65b0\u8bba\u6587\u53d1\u5e03\uff0c\u56e0\u6b64\u65e0\u6cd5\u4ece\u4eca\u65e5\u7684\u51fa\u7248\u7269\u4e2d\u8bc6\u522b\u51fa\u65b0\u7684\u4e3b\u8981\u4e3b\u9898\u6216\u8d8b\u52bf\u3002 2. \u7279\u522b\u91cd\u8981\u6216\u521b\u65b0\u7684\u8bba\u6587\uff1a \u540c\u7406\uff0c\u4eca\u65e5\u65e0\u7279\u522b\u7a81\u51fa\u6216\u5177\u6709\u521b\u65b0\u6027\u7684\u8bba\u6587\u53ef\u4f9b\u91cd\u70b9\u63a8\u8350\u3002 3. \u65b0\u5174\u7814\u7a76\u65b9\u5411\u6216\u6280\u672f\uff1a \u9274\u4e8e\u4eca\u65e5\u65e0\u65b0\u8bba\u6587\uff0c\u672c\u62a5\u544a\u65e0\u6cd5\u6307\u51fa\u65b0\u7684\u65b0\u5174\u7814\u7a76\u65b9\u5411\u6216\u6280\u672f\u3002 4. \u6700\u6709\u4ef7\u503c\u7684\u63a8\u8350\u9605\u8bfb\u8bba\u6587\uff1a \u7531\u4e8e\u4eca\u65e5\u65e0\u65b0\u5185\u5bb9\uff0c\u672c\u62a5\u544a\u6682\u65e0\u5177\u4f53\u8bba\u6587\u63a8\u8350\u3002\u5efa\u8bae\u7814\u7a76\u4eba\u5458\u53ef\u5229\u7528\u6b64\u673a\u4f1a\uff0c\u56de\u987e\u6b64\u524d\u5173\u6ce8\u7684\u91cd\u70b9\u8bba\u6587\uff0c\u6216\u5bf9\u73b0\u6709\u7814\u7a76\u8fdb\u884c\u66f4\u6df1\u5165\u7684\u601d\u8003\u4e0e\u63a2\u7d22\u3002 \u603b\u7ed3\uff1a \u4eca\u65e5Arxiv\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u672a\u6709\u65b0\u8bba\u6587\u53d1\u5e03\u3002\u6211\u4eec\u5c06\u7ee7\u7eed\u5bc6\u5207\u5173\u6ce8\uff0c\u5e76\u5728\u6709\u65b0\u8fdb\u5c55\u65f6\u53ca\u65f6\u66f4\u65b0\u3002 Table of Contents Papers","title":"Arxiv Computer Vision Papers - 2025-09-03"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-03/#arxiv-computer-vision-papers-2025-09-03","text":"","title":"Arxiv Computer Vision Papers - 2025-09-03"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-03/#executive-summary","text":"\u597d\u7684\uff0c\u4f5c\u4e3a\u4e00\u540d\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u7814\u7a76\u52a9\u7406\uff0c\u6211\u4e3a\u60a8\u51c6\u5907\u4e86\u8fd9\u4efd\u6bcf\u65e5Arxiv\u8ba1\u7b97\u673a\u89c6\u89c9\u8bba\u6587\u6267\u884c\u6458\u8981\u3002 \u6bcf\u65e5Arxiv\u8ba1\u7b97\u673a\u89c6\u89c9\u8bba\u6587\u6267\u884c\u6458\u8981 \u65e5\u671f\uff1a [\u8bf7\u5728\u6b64\u5904\u63d2\u5165\u5f53\u524d\u65e5\u671f\uff0c\u4f8b\u5982\uff1a2023\u5e7410\u670827\u65e5] \u6982\u89c8\uff1a \u672c\u62a5\u544a\u65e8\u5728\u4e3a\u5fd9\u788c\u7684\u7814\u7a76\u4eba\u5458\u63d0\u4f9bArxiv\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u6700\u65b0\u8bba\u6587\u7684\u5feb\u901f\u6982\u89c8\u3002 \u4eca\u65e5Arxiv\u8ba1\u7b97\u673a\u89c6\u89c9\u677f\u5757\u672a\u6709\u65b0\u8bba\u6587\u53d1\u5e03\u3002 1. \u4e3b\u8981\u4e3b\u9898\u6216\u8d8b\u52bf\u6982\u89c8\uff1a \u7531\u4e8e\u4eca\u65e5Arxiv\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u65e0\u65b0\u8bba\u6587\u53d1\u5e03\uff0c\u56e0\u6b64\u65e0\u6cd5\u4ece\u4eca\u65e5\u7684\u51fa\u7248\u7269\u4e2d\u8bc6\u522b\u51fa\u65b0\u7684\u4e3b\u8981\u4e3b\u9898\u6216\u8d8b\u52bf\u3002 2. \u7279\u522b\u91cd\u8981\u6216\u521b\u65b0\u7684\u8bba\u6587\uff1a \u540c\u7406\uff0c\u4eca\u65e5\u65e0\u7279\u522b\u7a81\u51fa\u6216\u5177\u6709\u521b\u65b0\u6027\u7684\u8bba\u6587\u53ef\u4f9b\u91cd\u70b9\u63a8\u8350\u3002 3. \u65b0\u5174\u7814\u7a76\u65b9\u5411\u6216\u6280\u672f\uff1a \u9274\u4e8e\u4eca\u65e5\u65e0\u65b0\u8bba\u6587\uff0c\u672c\u62a5\u544a\u65e0\u6cd5\u6307\u51fa\u65b0\u7684\u65b0\u5174\u7814\u7a76\u65b9\u5411\u6216\u6280\u672f\u3002 4. \u6700\u6709\u4ef7\u503c\u7684\u63a8\u8350\u9605\u8bfb\u8bba\u6587\uff1a \u7531\u4e8e\u4eca\u65e5\u65e0\u65b0\u5185\u5bb9\uff0c\u672c\u62a5\u544a\u6682\u65e0\u5177\u4f53\u8bba\u6587\u63a8\u8350\u3002\u5efa\u8bae\u7814\u7a76\u4eba\u5458\u53ef\u5229\u7528\u6b64\u673a\u4f1a\uff0c\u56de\u987e\u6b64\u524d\u5173\u6ce8\u7684\u91cd\u70b9\u8bba\u6587\uff0c\u6216\u5bf9\u73b0\u6709\u7814\u7a76\u8fdb\u884c\u66f4\u6df1\u5165\u7684\u601d\u8003\u4e0e\u63a2\u7d22\u3002 \u603b\u7ed3\uff1a \u4eca\u65e5Arxiv\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u672a\u6709\u65b0\u8bba\u6587\u53d1\u5e03\u3002\u6211\u4eec\u5c06\u7ee7\u7eed\u5bc6\u5207\u5173\u6ce8\uff0c\u5e76\u5728\u6709\u65b0\u8fdb\u5c55\u65f6\u53ca\u65f6\u66f4\u65b0\u3002","title":"Executive Summary"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-03/#table-of-contents","text":"","title":"Table of Contents"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-03/#papers","text":"","title":"Papers"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-04/","text":"Arxiv Computer Vision Papers - 2025-09-04 Executive Summary Arxiv \u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u6700\u65b0\u8bba\u6587\u6267\u884c\u6458\u8981 (2025\u5e749\u67083\u65e5) \u672c\u62a5\u544a\u603b\u7ed3\u4e862025\u5e749\u67083\u65e5\u53d1\u5e03\u768410\u7bc7Arxiv\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u8bba\u6587\uff0c\u65e8\u5728\u4e3a\u5fd9\u788c\u7684\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u5feb\u901f\u6982\u89c8\uff0c\u4e86\u89e3\u8be5\u9886\u57df\u7684\u6700\u65b0\u8fdb\u5c55\u548c\u5173\u952e\u8d8b\u52bf\u3002 1. \u4e3b\u8981\u4e3b\u9898\u4e0e\u8d8b\u52bf\u6982\u89c8 \u672c\u6b21\u53d1\u5e03\u7684\u8bba\u6587\u5c55\u73b0\u4e86\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u4ee5\u4e0b\u51e0\u4e2a\u663e\u8457\u8d8b\u52bf\uff1a \u6269\u6563\u6a21\u578b (Diffusion Models) \u7684\u4e3b\u5bfc\u5730\u4f4d\uff1a \u6269\u6563\u6a21\u578b\u5728\u591a\u6837\u5316\u7684\u751f\u6210\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u80fd\u529b\uff0c\u4ece\u96f6\u6837\u672c\u6df1\u5ea6\u56fe\u6062\u590d (InfraDiffusion)\u3001\u89c6\u9891\u5408\u6210 (GenCompositor)\uff0c\u5230\u903c\u771f\u624b\u7269\u4ea4\u4e92\u5efa\u6a21 (Hand-Object Interaction) \u4ee5\u53ca\u901a\u8fc7\u5408\u6210\u6570\u636e\u589e\u5f3a\u96f6\u6837\u672c\u5b66\u4e60 (Pedestrian Attribute Recognition)\u3002 \u57fa\u7840\u6a21\u578b (Foundation Models) \u7684\u53d1\u5c55\u4e0e\u9886\u57df\u9002\u5e94\uff1a \u7814\u7a76\u91cd\u70b9\u5728\u4e8e\u5982\u4f55\u5c06\u5927\u578b\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u6709\u6548\u5e94\u7528\u4e8e\u7279\u5b9a\u9886\u57df\uff0c\u5982\u533b\u5b66\u56fe\u50cf\u5206\u5272 (MedDINOv3)\uff0c\u4ee5\u53ca\u6784\u5efa\u80fd\u591f\u7edf\u4e00\u7406\u89e3\u548c\u751f\u6210\u7684\u81ea\u56de\u5f52\u6a21\u578b (OneCAT)\u3002 \u533b\u5b66\u5f71\u50cf\u5206\u6790\u7684\u6301\u7eed\u521b\u65b0\uff1a \u591a\u4e2a\u5de5\u4f5c\u805a\u7126\u4e8e\u533b\u5b66\u56fe\u50cf\u5206\u5272\uff0c\u5f3a\u8c03\u4e86\u8f7b\u91cf\u5316 (MedLiteNet)\u3001\u9c81\u68d2\u6027 (LGBP-OrgaNet) \u548c\u57fa\u7840\u6a21\u578b\u5728\u8be5\u9886\u57df\u7684\u5e94\u7528\u3002 \u6a21\u578b\u6548\u7387\u4e0e\u4f18\u5316\uff1a \u9488\u5bf9Vision Transformer\u7684\u6548\u7387\u63d0\u5347 (TinyDrop) \u548c\u8f7b\u91cf\u7ea7\u6a21\u578b\u8bbe\u8ba1 (MedLiteNet) \u4ecd\u7136\u662f\u91cd\u8981\u7684\u7814\u7a76\u65b9\u5411\uff0c\u4ee5\u9002\u5e94\u5b9e\u9645\u90e8\u7f72\u9700\u6c42\u3002 \u6df7\u5408\u67b6\u6784\u7684\u878d\u5408\uff1a CNN\u4e0eTransformer\u7684\u7ed3\u5408 (LGBP-OrgaNet, Watermarking, MedLiteNet) \u6301\u7eed\u88ab\u63a2\u7d22\uff0c\u4ee5\u671f\u7ed3\u5408\u4e24\u8005\u7684\u4f18\u52bf\uff0c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002 2. \u663e\u8457\u6216\u521b\u65b0\u6027\u8bba\u6587\u4eae\u70b9 OneCAT: Decoder-Only Auto-Regressive Model for Unified Understanding and Generation: \u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u7801\u5668-only\u7684\u81ea\u56de\u5f52\u6a21\u578b\uff0c\u65e8\u5728\u5b9e\u73b0\u7edf\u4e00\u7684\u7406\u89e3\u548c\u751f\u6210\u80fd\u529b\uff0c\u9884\u793a\u7740\u901a\u7528\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u65b0\u8303\u5f0f\uff0c\u5177\u6709\u5de8\u5927\u7684\u6f5c\u529b\u3002 InfraDiffusion: zero-shot depth map restoration with diffusion models and prompted segmentation from sparse infrastructure point clouds: \u8be5\u5de5\u4f5c\u5229\u7528\u6269\u6563\u6a21\u578b\u548c\u63d0\u793a\u5206\u5272\uff0c\u5b9e\u73b0\u4e86\u4ece\u7a00\u758f\u57fa\u7840\u8bbe\u65bd\u70b9\u4e91\u8fdb\u884c\u96f6\u6837\u672c\u6df1\u5ea6\u56fe\u6062\u590d\uff0c\u5728\u5b9e\u9645\u5de5\u7a0b\u548c3D\u91cd\u5efa\u9886\u57df\u5177\u6709\u91cd\u8981\u7684\u5e94\u7528\u4ef7\u503c\u548c\u521b\u65b0\u6027\u3002 MedDINOv3: How to adapt vision foundation models for medical image segmentation?: \u6df1\u5165\u63a2\u8ba8\u4e86\u5982\u4f55\u5c06\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u6709\u6548\u9002\u5e94\u4e8e\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4efb\u52a1\uff0c\u4e3a\u8be5\u9886\u57df\u7684\u57fa\u7840\u6a21\u578b\u5e94\u7528\u63d0\u4f9b\u4e86\u5173\u952e\u6307\u5bfc\u548c\u5b9e\u8df5\u7ecf\u9a8c\u3002 GenCompositor: Generative Video Compositing with Diffusion Transformer: \u91c7\u7528Diffusion Transformer\u8fdb\u884c\u751f\u6210\u5f0f\u89c6\u9891\u5408\u6210\uff0c\u5c55\u793a\u4e86\u5728\u590d\u6742\u89c6\u9891\u5185\u5bb9\u521b\u4f5c\u65b9\u9762\u7684\u5148\u8fdb\u6280\u672f\uff0c\u63a8\u52a8\u4e86\u89c6\u9891\u751f\u6210\u9886\u57df\u7684\u53d1\u5c55\u3002 Towards Realistic Hand-Object Interaction with Gravity-Field Based Diffusion Bridge: \u5f15\u5165\u4e86\u57fa\u4e8e\u91cd\u529b\u573a\u7684\u6269\u6563\u6865\uff0c\u4e3a\u5b9e\u73b0\u66f4\u903c\u771f\u3001\u7269\u7406\u4e00\u81f4\u7684\u624b\u7269\u4ea4\u4e92\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u9896\u65b9\u6cd5\uff0c\u5bf93D\u89c6\u89c9\u548c\u673a\u5668\u4eba\u9886\u57df\u5177\u6709\u542f\u53d1\u610f\u4e49\u3002 3. \u65b0\u5174\u7814\u7a76\u65b9\u5411\u6216\u6280\u672f Diffusion Transformer (DiT) \u67b6\u6784\uff1a \u7ed3\u5408\u6269\u6563\u6a21\u578b\u548cTransformer\u7684\u5f3a\u5927\u80fd\u529b\uff0c\u5c24\u5176\u5728\u89c6\u9891\u751f\u6210\u7b49\u590d\u6742\u4efb\u52a1\u4e2d\u5c55\u73b0\u6f5c\u529b (GenCompositor)\u3002 \u7edf\u4e00\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\uff1a \u53d1\u5c55\u80fd\u591f\u5904\u7406\u591a\u6a21\u6001\u8f93\u5165\u5e76\u6267\u884c\u7406\u89e3\u4e0e\u751f\u6210\u4efb\u52a1\u7684\u5355\u4e00\u901a\u7528\u6a21\u578b\uff0c\u5411\u901a\u7528\u4eba\u5de5\u667a\u80fd\u8fc8\u8fdb (OneCAT)\u3002 \u7269\u7406\u4fe1\u606f\u5f15\u5bfc\u7684\u751f\u6210\u6a21\u578b\uff1a \u5c06\u7269\u7406\u7ea6\u675f\uff08\u5982\u91cd\u529b\u573a\uff09\u878d\u5165\u751f\u6210\u8fc7\u7a0b\uff0c\u4ee5\u63d0\u9ad8\u751f\u6210\u5185\u5bb9\u7684\u771f\u5b9e\u6027\u548c\u4e00\u81f4\u6027 (Hand-Object Interaction)\u3002 \u9ad8\u6548\u80fdTransformer\u8bbe\u8ba1\uff1a \u6301\u7eed\u4f18\u5316Transformer\u7ed3\u6784\uff0c\u4f7f\u5176\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u4e5f\u80fd\u9ad8\u6548\u8fd0\u884c\uff0c\u4f8b\u5982\u901a\u8fc7Token Dropping (TinyDrop)\u3002 \u5408\u6210\u6570\u636e\u5728\u96f6\u6837\u672c\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\uff1a \u5229\u7528\u751f\u6210\u6a21\u578b\uff08\u7279\u522b\u662f\u6269\u6563\u6a21\u578b\uff09\u521b\u5efa\u9ad8\u8d28\u91cf\u5408\u6210\u6570\u636e\uff0c\u4ee5\u5f25\u8865\u771f\u5b9e\u6570\u636e\u7a00\u7f3a\u6027\uff0c\u63d0\u5347\u96f6\u6837\u672c\u4efb\u52a1\u6027\u80fd (Pedestrian Attribute Recognition)\u3002 4. \u5efa\u8bae\u5b8c\u6574\u9605\u8bfb\u7684\u8bba\u6587 \u4e3a\u4e86\u66f4\u6df1\u5165\u5730\u4e86\u89e3\u5f53\u524d\u9886\u57df\u7684\u91cd\u8981\u8fdb\u5c55\uff0c\u6211\u4eec\u5efa\u8bae\u60a8\u4f18\u5148\u9605\u8bfb\u4ee5\u4e0b\u8bba\u6587\uff1a OneCAT: \u5bf9\u4e8e\u5173\u6ce8\u901a\u7528AI\u548c\u57fa\u7840\u6a21\u578b\u672a\u6765\u53d1\u5c55\u7684\u7814\u7a76\u4eba\u5458\uff0c\u8fd9\u7bc7\u8bba\u6587\u63d0\u4f9b\u4e86\u5bf9\u7edf\u4e00\u7406\u89e3\u4e0e\u751f\u6210\u6a21\u578b\u7684\u65b0\u89c6\u89d2\u3002 InfraDiffusion: \u5bf9\u4e8e\u4ece\u4e8b3D\u89c6\u89c9\u3001\u91cd\u5efa\u548c\u6269\u6563\u6a21\u578b\u5b9e\u9645\u5e94\u7528\u7684\u7814\u7a76\u4eba\u5458\uff0c\u8be5\u5de5\u4f5c\u5c55\u793a\u4e86\u96f6\u6837\u672c\u6df1\u5ea6\u6062\u590d\u7684\u5f3a\u5927\u6f5c\u529b\u3002 MedDINOv3: \u5bf9\u4e8e\u533b\u5b66AI\u9886\u57df\uff0c\u7279\u522b\u662f\u5e0c\u671b\u5229\u7528\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u7814\u7a76\u4eba\u5458\uff0c\u8fd9\u7bc7\u8bba\u6587\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u9002\u5e94\u7b56\u7565\u3002 GenCompositor: \u5bf9\u4e8e\u89c6\u9891\u751f\u6210\u548c\u9ad8\u7ea7\u6269\u6563\u67b6\u6784\u611f\u5174\u8da3\u7684\u7814\u7a76\u4eba\u5458\uff0c\u8be5\u8bba\u6587\u5c55\u793a\u4e86Diffusion Transformer\u5728\u590d\u6742\u89c6\u9891\u5408\u6210\u4e2d\u7684\u5e94\u7528\u3002 Towards Realistic Hand-Object Interaction with Gravity-Field Based Diffusion Bridge: \u5bf9\u4e8e3D\u89c6\u89c9\u3001\u673a\u5668\u4eba\u548c\u65b0\u9896\u751f\u6210\u5efa\u6a21\u65b9\u6cd5\u7684\u7814\u7a76\u4eba\u5458\uff0c\u8be5\u8bba\u6587\u63d0\u4f9b\u4e86\u4e00\u79cd\u7ed3\u5408\u7269\u7406\u7ea6\u675f\u7684\u521b\u65b0\u65b9\u6cd5\u3002 Table of Contents InfraDiffusion: zero-shot depth map restoration with diffusion models and prompted segmentation from sparse infrastructure point clouds MedDINOv3: How to adapt vision foundation models for medical image segmentation? OneCAT: Decoder-Only Auto-Regressive Model for Unified Understanding and Generation GenCompositor: Generative Video Compositing with Diffusion Transformer Enhancing Robustness in Post-Processing Watermarking: An Ensemble Attack Network Using CNNs and Transformers TinyDrop: Tiny Model Guided Token Dropping for Vision Transformers LGBP-OrgaNet: Learnable Gaussian Band Pass Fusion of CNN and Transformer Features for Robust Organoid Segmentation and Tracking Towards Realistic Hand-Object Interaction with Gravity-Field Based Diffusion Bridge Enhancing Zero-Shot Pedestrian Attribute Recognition with Synthetic Data Generation: A Comparative Study with Image-To-Image Diffusion Models MedLiteNet: Lightweight Hybrid Medical Image Segmentation Model Papers InfraDiffusion: zero-shot depth map restoration with diffusion models and prompted segmentation from sparse infrastructure point clouds Authors: Yixiong Jing, Cheng Zhang, Haibing Wu, Guangming Wang, Olaf Wysocki, Brian Sheil Published: 2025-09-03 Categories: cs.CV Abstract: Point clouds are widely used for infrastructure monitoring by providing geometric information, where segmentation is required for downstream tasks such as defect detection. Existing research has automated semantic segmentation of structural components, while brick-level segmentation (identifying defects such as spalling and mortar loss) has been primarily conducted from RGB images. However, acquiring high-resolution images is impractical in low-light environments like masonry tunnels. Point clouds, though robust to dim lighting, are typically unstructured, sparse, and noisy, limiting fine-grained segmentation. We present InfraDiffusion, a zero-shot framework that projects masonry point clouds into depth maps using virtual cameras and restores them by adapting the Denoising Diffusion Null-space Model (DDNM). Without task-specific training, InfraDiffusion enhances visual clarity and geometric consistency of depth maps. Experiments on masonry bridge and tunnel point cloud datasets show significant improvements in brick-level segmentation using the Segment Anything Model (SAM), underscoring its potential for automated inspection of masonry assets. Our code and data is available at https://github.com/Jingyixiong/InfraDiffusion-official-implement. Analysis: InfraDiffusion \u8bba\u6587\u6458\u8981\u7684\u4e13\u5bb6\u5206\u6790\u5982\u4e0b\uff1a 1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u660e\u603b\u7ed3 (2-3 \u53e5\u8bdd) \u672c\u8bba\u6587\u63d0\u51fa\u4e86 InfraDiffusion\uff0c\u4e00\u4e2a\u96f6\u6837\u672c\uff08zero-shot\uff09\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u5728\u4f4e\u5149\u7167\u73af\u5883\u4e0b\u4ece\u7a00\u758f\u3001\u5608\u6742\u7684\u70b9\u4e91\u4e2d\u8fdb\u884c\u7ec6\u7c92\u5ea6\uff08\u5982\u7816\u5757\u7ea7\uff09\u5206\u5272\u7684\u6311\u6218\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u5c06\u70b9\u4e91\u6295\u5f71\u5230\u6df1\u5ea6\u56fe\u5e76\u5229\u7528\u9002\u5e94\u540e\u7684\u53bb\u566a\u6269\u6563\u96f6\u7a7a\u95f4\u6a21\u578b\uff08DDNM\uff09\u8fdb\u884c\u6062\u590d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6df1\u5ea6\u56fe\u7684\u89c6\u89c9\u6e05\u6670\u5ea6\u548c\u51e0\u4f55\u4e00\u81f4\u6027\u3002\u6700\u7ec8\uff0c\u5b83\u5728\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u663e\u8457\u6539\u5584\u4e86\u7816\u5757\u7ea7\u5206\u5272\u7684\u6027\u80fd\uff0c\u4e3a\u57fa\u7840\u8bbe\u65bd\u7684\u81ea\u52a8\u5316\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002 2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u5b66\u65b9\u6cd5 InfraDiffusion \u7684\u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u5176 \u96f6\u6837\u672c\u6df1\u5ea6\u56fe\u6062\u590d\u65b9\u6cd5 \uff0c\u4ee5\u53ca\u5bf9 Denoising Diffusion Null-space Model (DDNM) \u7684\u5de7\u5999\u9002\u5e94\u548c\u5e94\u7528 \u3002\u5177\u4f53\u800c\u8a00\uff1a \u70b9\u4e91\u5230\u6df1\u5ea6\u56fe\u7684\u8f6c\u6362\u4e0e\u6062\u590d\u8303\u5f0f\uff1a \u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6d41\u7a0b\uff0c\u9996\u5148\u5229\u7528\u865a\u62df\u76f8\u673a\u5c06\u975e\u7ed3\u6784\u5316\u7684\u7a00\u758f\u70b9\u4e91\u6295\u5f71\u6210\u6df1\u5ea6\u56fe\uff0c\u7136\u540e\u5c06\u6df1\u5ea6\u56fe\u7684\u6062\u590d\u89c6\u4e3a\u4e00\u4e2a\u56fe\u50cf\u4fee\u590d\u95ee\u9898\u3002 DDNM \u7684\u521b\u65b0\u6027\u9002\u5e94\uff1a DDNM \u901a\u5e38\u7528\u4e8e\u4ece\u635f\u574f\u56fe\u50cf\u4e2d\u8fdb\u884c\u4fee\u590d\uff08\u5982\u53bb\u566a\u3001\u8865\u5168\uff09\u3002InfraDiffusion \u5c06\u5176\u9002\u5e94\u4e8e\u4ece\u7a00\u758f\u70b9\u4e91\u751f\u6210\u7684\u201c\u4e0d\u5b8c\u6574\u201d\u6df1\u5ea6\u56fe\u8fdb\u884c\u6062\u590d\uff0c\u4ee5\u589e\u5f3a\u5176\u89c6\u89c9\u6e05\u6670\u5ea6\u548c\u51e0\u4f55\u4e00\u81f4\u6027\u3002\u8fd9\u79cd\u9002\u5e94\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u5728\u6ca1\u6709\u7279\u5b9a\u4efb\u52a1\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u4ece\u4f4e\u8d28\u91cf\u7684\u6df1\u5ea6\u56fe\u4e2d\u6062\u590d\u51fa\u9ad8\u4fdd\u771f\u5ea6\u7684\u4fe1\u606f\u3002 \u96f6\u6837\u672c\u80fd\u529b\uff1a \u6574\u4e2a\u6846\u67b6\u65e0\u9700\u9488\u5bf9\u7279\u5b9a\u7684\u7816\u5757\u7ea7\u5206\u5272\u4efb\u52a1\u8fdb\u884c\u8bad\u7ec3\uff0c\u76f4\u63a5\u5229\u7528\u9884\u8bad\u7ec3\u7684\u6269\u6563\u6a21\u578b\u548c\u73b0\u6709\u7684\u5206\u5272\u6a21\u578b\uff08\u5982 SAM\uff09\uff0c\u6781\u5927\u5730\u63d0\u9ad8\u4e86\u5176\u6cdb\u5316\u80fd\u529b\u548c\u5e94\u7528\u6548\u7387\u3002 3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd \u63a8\u52a8\u57fa\u7840\u8bbe\u65bd\u81ea\u52a8\u5316\u68c0\u6d4b\uff1a InfraDiffusion \u4e3a\u6865\u6881\u3001\u96a7\u9053\u7b49\u57fa\u7840\u8bbe\u65bd\u7684\u81ea\u52a8\u5316\u3001\u7cbe\u7ec6\u5316\u68c0\u6d4b\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u5de5\u5177\uff0c\u5c24\u5176\u662f\u5728\u4f20\u7edf\u65b9\u6cd5\u53d7\u9650\u7684\u6076\u52a3\u73af\u5883\uff08\u5982\u4f4e\u5149\u7167\uff09\u4e0b\u3002\u8fd9\u5c06\u63d0\u9ad8\u68c0\u6d4b\u6548\u7387\u3001\u964d\u4f4e\u6210\u672c\u5e76\u589e\u5f3a\u5b89\u5168\u6027\u3002 \u5f25\u5408 3D \u70b9\u4e91\u4e0e 2D \u56fe\u50cf\u5904\u7406\u7684\u9e3f\u6c9f\uff1a \u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u9014\u5f84\uff0c\u5c06\u7a00\u758f\u3001\u975e\u7ed3\u6784\u5316\u7684 3D \u70b9\u4e91\u6570\u636e\u8f6c\u5316\u4e3a\u9ad8\u8d28\u91cf\u7684 2D \u6df1\u5ea6\u56fe\uff0c\u4ece\u800c\u80fd\u591f\u5229\u7528\u5f3a\u5927\u7684 2D \u89c6\u89c9\u6a21\u578b\uff08\u5982 SAM\uff09\u8fdb\u884c\u7ec6\u7c92\u5ea6\u5206\u6790\uff0c\u4e3a\u70b9\u4e91\u5904\u7406\u5f00\u8f9f\u4e86\u65b0\u7684\u601d\u8def\u3002 \u62d3\u5c55\u6269\u6563\u6a21\u578b\u7684\u5e94\u7528\u8fb9\u754c\uff1a \u8bba\u6587\u5c55\u793a\u4e86\u6269\u6563\u6a21\u578b\u5728\u975e\u4f20\u7edf\u56fe\u50cf\u4fee\u590d\u4efb\u52a1\uff08\u4ece\u7a00\u758f 3D \u6570\u636e\u6062\u590d 2D \u6df1\u5ea6\u56fe\uff09\u4e2d\u7684\u5de8\u5927\u6f5c\u529b\uff0c\u53ef\u80fd\u542f\u53d1\u66f4\u591a\u5c06\u6269\u6563\u6a21\u578b\u5e94\u7528\u4e8e\u5176\u4ed6\u7c7b\u578b\u4f20\u611f\u5668\u6570\u636e\u6216\u6570\u636e\u8f6c\u6362\u4efb\u52a1\u7684\u7814\u7a76\u3002 \u4fc3\u8fdb\u96f6\u6837\u672c\u5b66\u4e60\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u843d\u5730\uff1a \u5176\u96f6\u6837\u672c\u7279\u6027\u5bf9\u4e8e\u5b9e\u9645\u5de5\u7a0b\u5e94\u7528\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u56e0\u4e3a\u5b83\u51cf\u5c11\u4e86\u5bf9\u5927\u91cf\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u66f4\u5feb\u5730\u90e8\u7f72\u548c\u9002\u5e94\u65b0\u573a\u666f\u3002 4. \u53ef\u80fd\u53d7\u76ca\u7684\u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 \u571f\u6728\u5de5\u7a0b\u4e0e\u7ed3\u6784\u5065\u5eb7\u76d1\u6d4b\uff1a \u76f4\u63a5\u53d7\u76ca\u9886\u57df\uff0c\u7528\u4e8e\u6865\u6881\u3001\u96a7\u9053\u3001\u5927\u575d\u7b49\u57fa\u7840\u8bbe\u65bd\u7684\u88c2\u7f1d\u3001\u5265\u843d\u3001\u7802\u6d46\u6d41\u5931\u7b49\u7f3a\u9677\u7684\u81ea\u52a8\u5316\u68c0\u6d4b\u3002 \u673a\u5668\u4eba\u4e0e\u81ea\u4e3b\u5de1\u68c0\uff1a \u642d\u8f7d LiDAR \u6216\u6df1\u5ea6\u4f20\u611f\u5668\u7684\u5de1\u68c0\u673a\u5668\u4eba\u6216\u65e0\u4eba\u673a\uff0c\u53ef\u5728\u590d\u6742\u6216\u5371\u9669\u73af\u5883\u4e2d\u5229\u7528\u6b64\u6280\u672f\u8fdb\u884c\u81ea\u4e3b\u6570\u636e\u91c7\u96c6\u548c\u5206\u6790\u3002 \u6587\u5316\u9057\u4ea7\u4fdd\u62a4\uff1a \u5bf9\u5386\u53f2\u5efa\u7b51\u3001\u53e4\u8ff9\u7b49\u7816\u77f3\u7ed3\u6784\u8fdb\u884c\u65e0\u635f\u3001\u7cbe\u7ec6\u5316\u76d1\u6d4b\uff0c\u8bc4\u4f30\u5176\u635f\u574f\u7a0b\u5ea6\u3002 \u77ff\u4e1a\u4e0e\u5730\u4e0b\u7a7a\u95f4\u52d8\u6d4b\uff1a \u5728\u5149\u7167\u6761\u4ef6\u5dee\u7684\u77ff\u4e95\u3001\u5730\u4e0b\u901a\u9053\u7b49\u73af\u5883\u4e2d\uff0c\u8fdb\u884c\u7ed3\u6784\u5b8c\u6574\u6027\u8bc4\u4f30\u3002 \u901a\u7528 3D \u6570\u636e\u5904\u7406\uff1a \u4efb\u4f55\u9700\u8981\u4ece\u7a00\u758f\u3001\u5608\u6742\u7684 3D \u70b9\u4e91\u4e2d\u63d0\u53d6\u9ad8\u7cbe\u5ea6 2D \u51e0\u4f55\u6216\u8bed\u4e49\u4fe1\u606f\u7684\u5e94\u7528\u573a\u666f\u3002 5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u7684\u5c40\u9650\u6027 \u70b9\u4e91\u8d28\u91cf\u7684\u4f9d\u8d56\u6027\uff1a \u5c3d\u7ba1\u8be5\u65b9\u6cd5\u65e8\u5728\u5904\u7406\u7a00\u758f\u548c\u5608\u6742\u7684\u70b9\u4e91\uff0c\u4f46\u5176\u6027\u80fd\u53ef\u80fd\u4ecd\u53d7\u539f\u59cb\u70b9\u4e91\u6781\u7aef\u7a00\u758f\u5ea6\u6216\u566a\u58f0\u6c34\u5e73\u7684\u9650\u5236\u3002\u6458\u8981\u672a\u660e\u786e\u6307\u51fa\u5176\u80fd\u5904\u7406\u7684\u6700\u4f4e\u70b9\u4e91\u5bc6\u5ea6\u6216\u6700\u9ad8\u566a\u58f0\u5bb9\u5fcd\u5ea6\u3002 \u865a\u62df\u76f8\u673a\u8bbe\u7f6e\u7684\u654f\u611f\u6027\uff1a \u865a\u62df\u76f8\u673a\u7684\u9009\u62e9\u3001\u4f4d\u7f6e\u548c\u53c2\u6570\u8bbe\u7f6e\u53ef\u80fd\u4f1a\u5f71\u54cd\u6df1\u5ea6\u56fe\u7684\u8d28\u91cf\u548c\u6062\u590d\u6548\u679c\u3002\u8fd9\u53ef\u80fd\u9700\u8981\u4e00\u5b9a\u7684\u9886\u57df\u77e5\u8bc6\u6216\u7ecf\u9a8c\u6765\u4f18\u5316\u3002 DDNM \u6cdb\u5316\u80fd\u529b\uff1a \u867d\u7136 DDNM \u88ab\u9002\u5e94\u7528\u4e8e\u6df1\u5ea6\u56fe\u6062\u590d\uff0c\u4f46\u5176\u5e95\u5c42\u6269\u6563\u6a21\u578b\u662f\u5728\u56fe\u50cf\u6570\u636e\u4e0a\u9884\u8bad\u7ec3\u7684\u3002\u5176\u5bf9\u6df1\u5ea6\u56fe\u8fd9\u79cd\u975e RGB \u56fe\u50cf\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4ee5\u53ca\u5bf9\u4e0d\u540c\u7c7b\u578b\u57fa\u7840\u8bbe\u65bd\u51e0\u4f55\u7279\u5f81\u7684\u9002\u5e94\u6027\uff0c\u53ef\u80fd\u4ecd\u6709\u5f85\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u3002 \u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\uff1a \u6269\u6563\u6a21\u578b\u901a\u5e38\u8ba1\u7b97\u6210\u672c\u8f83\u9ad8\uff0c\u5c24\u5176\u662f\u5728\u751f\u6210\u9ad8\u5206\u8fa8\u7387\u6df1\u5ea6\u56fe\u65f6\u3002\u6458\u8981\u672a\u63d0\u53ca\u63a8\u7406\u901f\u5ea6\u6216\u8ba1\u7b97\u6548\u7387\uff0c\u8fd9\u5bf9\u4e8e\u5b9e\u65f6\u6216\u5927\u89c4\u6a21\u5e94\u7528\u53ef\u80fd\u662f\u4e00\u4e2a\u8003\u91cf\u56e0\u7d20\u3002 \u5b9e\u9a8c\u8303\u56f4\uff1a \u5b9e\u9a8c\u4ec5\u5728\u201c\u7816\u77f3\u6865\u6881\u548c\u96a7\u9053\u70b9\u4e91\u6570\u636e\u96c6\u201d\u4e0a\u8fdb\u884c\u3002\u5176\u5728\u5176\u4ed6\u7c7b\u578b\u57fa\u7840\u8bbe\u65bd\uff08\u5982\u6df7\u51dd\u571f\u3001\u94a2\u7ed3\u6784\uff09\u6216\u4e0d\u540c\u6750\u6599\u4e0a\u7684\u8868\u73b0\u5c1a\u4e0d\u660e\u786e\u3002 \u201c\u96f6\u6837\u672c\u201d\u7684\u8fb9\u754c\uff1a \u5c3d\u7ba1\u5b9e\u73b0\u4e86\u96f6\u6837\u672c\u5206\u5272\uff0c\u4f46\u6700\u7ec8\u7684\u7f3a\u9677\u68c0\u6d4b\u53ef\u80fd\u4ecd\u9700\u8981\u5bf9\u5206\u5272\u7ed3\u679c\u8fdb\u884c\u540e\u5904\u7406\u6216\u5206\u7c7b\uff0c\u8fd9\u90e8\u5206\u53ef\u80fd\u4e0d\u5c5e\u4e8e\u96f6\u6837\u672c\u8303\u7574\u3002 Key Findings: We present InfraDiffusion, a zero-shot framework that projects masonry point clouds into depth maps using virtual cameras and restores them by adapting the Denoising Diffusion Null-space Model (DDNM). Links: PDF arXiv MedDINOv3: How to adapt vision foundation models for medical image segmentation? Authors: Yuheng Li, Yizhou Wu, Yuxiang Lai, Mingzhe Hu, Xiaofeng Yang Published: 2025-09-02 Categories: cs.CV Abstract: Accurate segmentation of organs and tumors in CT and MRI scans is essential for diagnosis, treatment planning, and disease monitoring. While deep learning has advanced automated segmentation, most models remain task-specific, lacking generalizability across modalities and institutions. Vision foundation models (FMs) pretrained on billion-scale natural images offer powerful and transferable representations. However, adapting them to medical imaging faces two key challenges: (1) the ViT backbone of most foundation models still underperform specialized CNNs on medical image segmentation, and (2) the large domain gap between natural and medical images limits transferability. We introduce MedDINOv3, a simple and effective framework for adapting DINOv3 to medical segmentation. We first revisit plain ViTs and design a simple and effective architecture with multi-scale token aggregation. Then, we perform domain-adaptive pretraining on CT-3M, a curated collection of 3.87M axial CT slices, using a multi-stage DINOv3 recipe to learn robust dense features. MedDINOv3 matches or exceeds state-of-the-art performance across four segmentation benchmarks, demonstrating the potential of vision foundation models as unified backbones for medical image segmentation. The code is available at https://github.com/ricklisz/MedDINOv3. Analysis: \u4f5c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u4e13\u5bb6\uff0c\u6211\u5bf9\u8fd9\u7bc7\u5173\u4e8eMedDINOv3\u7684\u8bba\u6587\u6458\u8981\u8fdb\u884c\u5982\u4e0b\u5206\u6790\uff1a MedDINOv3: \u5982\u4f55\u5c06\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u5e94\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u5206\u5272\uff1f 1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u660e\u603b\u7ed3 (2-3 \u53e5\u8bdd) MedDINOv3\u63d0\u51fa\u4e86\u4e00\u4e2a\u7b80\u5355\u800c\u6709\u6548\u7684\u6846\u67b6\uff0c\u65e8\u5728\u5c06DINOv3\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u6210\u529f\u5e94\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u5206\u5272\u3002\u5b83\u901a\u8fc7\u6539\u8fdbViT\u9aa8\u5e72\u7f51\u7edc\u4ee5\u9002\u5e94\u533b\u5b66\u56fe\u50cf\u7279\u6027\uff0c\u5e76\u7ed3\u5408\u5927\u89c4\u6a21CT\u6570\u636e\u96c6\u4e0a\u7684\u9886\u57df\u81ea\u9002\u5e94\u9884\u8bad\u7ec3\uff0c\u514b\u670d\u4e86\u81ea\u7136\u56fe\u50cf\u4e0e\u533b\u5b66\u56fe\u50cf\u4e4b\u95f4\u7684\u9886\u57df\u9e3f\u6c9f\u3002\u6700\u7ec8\uff0cMedDINOv3\u5728\u591a\u4e2a\u533b\u5b66\u5206\u5272\u57fa\u51c6\u4e0a\u8fbe\u5230\u4e86\u6216\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u57fa\u7840\u6a21\u578b\u4f5c\u4e3a\u533b\u5b66\u56fe\u50cf\u7edf\u4e00\u9aa8\u5e72\u7f51\u7edc\u7684\u5de8\u5927\u6f5c\u529b\u3002 2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u5b66\u65b9\u6cd5 \u8be5\u8bba\u6587\u7684\u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u5176\u53cc\u7ba1\u9f50\u4e0b\u7684\u65b9\u6cd5\uff1a 1. ViT\u67b6\u6784\u7684\u9002\u5e94\u6027\u6539\u8fdb\uff1a \u4f5c\u8005\u91cd\u65b0\u5ba1\u89c6\u4e86\u539f\u59cb\u7684Vision Transformer (ViT) \u67b6\u6784\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5177\u6709\u201c\u591a\u5c3a\u5ea6token\u805a\u5408\u201d\u80fd\u529b\u7684\u7b80\u5355\u800c\u6709\u6548\u7684\u67b6\u6784\u3002\u8fd9\u65e8\u5728\u89e3\u51b3ViT\u5728\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4efb\u52a1\u4e0a\u53ef\u80fd\u4e0d\u5982\u4e13\u7528CNN\u7684\u95ee\u9898\uff0c\u4f7f\u5176\u80fd\u66f4\u597d\u5730\u6355\u6349\u533b\u5b66\u56fe\u50cf\u4e2d\u591a\u5c3a\u5ea6\u3001\u7cbe\u7ec6\u7684\u89e3\u5256\u7ed3\u6784\u4fe1\u606f\u3002 2. \u5927\u89c4\u6a21\u9886\u57df\u81ea\u9002\u5e94\u9884\u8bad\u7ec3\uff1a \u4ed6\u4eec\u5728\u5927\u89c4\u6a21\u7684CT\u6570\u636e\u96c6\uff08CT-3M\uff0c\u5305\u542b3.87M\u8f74\u5411CT\u5207\u7247\uff09\u4e0a\uff0c\u91c7\u7528\u201c\u591a\u9636\u6bb5DINOv3\u914d\u65b9\u201d\u8fdb\u884c\u9886\u57df\u81ea\u9002\u5e94\u9884\u8bad\u7ec3\u3002\u8fd9\u4e00\u7b56\u7565\u6709\u6548\u5730\u5f25\u5408\u4e86\u81ea\u7136\u56fe\u50cf\u4e0e\u533b\u5b66\u56fe\u50cf\u4e4b\u95f4\u7684\u5de8\u5927\u9886\u57df\u9e3f\u6c9f\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u5b66\u4e60\u5230\u5bf9\u533b\u5b66\u56fe\u50cf\u7279\u6709\u7684\u9c81\u68d2\u3001\u5bc6\u96c6\u7684\u89c6\u89c9\u7279\u5f81\uff0c\u4ece\u800c\u63d0\u9ad8\u5176\u5728\u4e0b\u6e38\u5206\u5272\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002 3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd \u8fd9\u9879\u7814\u7a76\u5bf9\u533b\u5b66\u56fe\u50cf\u5206\u6790\u9886\u57df\u5177\u6709\u6df1\u8fdc\u5f71\u54cd\uff1a * \u63a8\u52a8\u901a\u7528\u533b\u5b66AI\u6a21\u578b\u53d1\u5c55\uff1a \u5b83\u4e3a\u5c06\u5f3a\u5927\u7684\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08\u5982DINOv3\uff09\u5e94\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u5206\u5272\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u8303\u5f0f\uff0c\u6709\u671b\u63a8\u52a8\u5f00\u53d1\u51fa\u66f4\u901a\u7528\u3001\u66f4\u5c11\u4f9d\u8d56\u4e8e\u7279\u5b9a\u4efb\u52a1\u6807\u6ce8\u6570\u636e\u7684\u533b\u5b66AI\u6a21\u578b\u3002 * \u63d0\u9ad8\u5206\u5272\u51c6\u786e\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\uff1a \u901a\u8fc7\u63d0\u4f9b\u7edf\u4e00\u4e14\u9ad8\u6027\u80fd\u7684\u9aa8\u5e72\u7f51\u7edc\uff0cMedDINOv3\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u51c6\u786e\u6027\u548c\u8de8\u6a21\u6001\u3001\u8de8\u673a\u6784\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4ece\u800c\u52a0\u901f\u8bca\u65ad\u3001\u6cbb\u7597\u89c4\u5212\u548c\u75be\u75c5\u76d1\u6d4b\u7684\u81ea\u52a8\u5316\u548c\u6807\u51c6\u5316\u8fdb\u7a0b\u3002 * \u964d\u4f4e\u5f00\u53d1\u6210\u672c\uff1a \u9884\u8bad\u7ec3\u7684\u57fa\u7840\u6a21\u578b\u53ef\u4ee5\u4f5c\u4e3a\u65b0\u4efb\u52a1\u7684\u8d77\u70b9\uff0c\u51cf\u5c11\u4ece\u5934\u8bad\u7ec3\u6a21\u578b\u6240\u9700\u7684\u6570\u636e\u91cf\u548c\u8ba1\u7b97\u8d44\u6e90\uff0c\u4ece\u800c\u964d\u4f4e\u533b\u5b66AI\u5e94\u7528\u7684\u5f00\u53d1\u6210\u672c\u548c\u65f6\u95f4\u3002 4. \u53ef\u80fd\u53d7\u76ca\u7684\u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 \u9664\u4e86\u76f4\u63a5\u7684\u5668\u5b98\u548c\u80bf\u7624\u5206\u5272\uff0c\u8fd9\u9879\u7814\u7a76\u7684\u6210\u679c\u53ef\u4ee5\u63a8\u5e7f\u5230\u591a\u79cd\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4efb\u52a1\u548c\u5e94\u7528\u4e2d\uff1a * \u5176\u4ed6\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4efb\u52a1\uff1a \u4f8b\u5982\u75c5\u7076\u68c0\u6d4b\u3001\u75be\u75c5\u5206\u7c7b\u3001\u56fe\u50cf\u914d\u51c6\u30013D\u91cd\u5efa\u7b49\uff0c\u8fd9\u4e9b\u4efb\u52a1\u90fd\u53ef\u4ee5\u53d7\u76ca\u4e8eMedDINOv3\u5b66\u4e60\u5230\u7684\u5f3a\u5927\u4e14\u901a\u7528\u7684\u533b\u5b66\u56fe\u50cf\u7279\u5f81\u3002 * \u4e0d\u540c\u533b\u5b66\u5f71\u50cf\u6a21\u6001\uff1a \u5c3d\u7ba1\u5f53\u524d\u9884\u8bad\u7ec3\u4e3b\u8981\u57fa\u4e8eCT\uff0c\u4f46\u5176\u6846\u67b6\u548c\u65b9\u6cd5\u8bba\u53ef\u542f\u53d1\u5728MRI\u3001X\u5149\u3001\u8d85\u58f0\u3001\u75c5\u7406\u56fe\u50cf\u7b49\u5176\u4ed6\u6a21\u6001\u4e0a\u8fdb\u884c\u7c7b\u4f3c\u7684\u57fa\u7840\u6a21\u578b\u9002\u5e94\uff0c\u4ee5\u6784\u5efa\u591a\u6a21\u6001\u7684\u533b\u5b66\u57fa\u7840\u6a21\u578b\u3002 * \u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\uff1a \u63d0\u9ad8\u81ea\u52a8\u5316\u5206\u6790\u7684\u51c6\u786e\u6027\uff0c\u8f85\u52a9\u533b\u751f\u8fdb\u884c\u8bca\u65ad\u548c\u6cbb\u7597\u65b9\u6848\u5236\u5b9a\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u5927\u91cf\u5f71\u50cf\u6570\u636e\u65f6\u3002 * \u533b\u5b66\u6559\u80b2\u4e0e\u7814\u7a76\uff1a \u63d0\u4f9b\u9ad8\u6027\u80fd\u7684\u5206\u5272\u5de5\u5177\uff0c\u52a0\u901f\u533b\u5b66\u7814\u7a76\u548c\u65b0\u7597\u6cd5\u7684\u5f00\u53d1\uff0c\u4f8b\u5982\u5728\u836f\u7269\u7b5b\u9009\u3001\u751f\u7269\u6807\u5fd7\u7269\u8bc6\u522b\u7b49\u9886\u57df\u3002 5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u51fa\u7684\u5c40\u9650\u6027 \u5c3d\u7ba1MedDINOv3\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u4ece\u6458\u8981\u4e2d\u4ecd\u53ef\u63a8\u65ad\u51fa\u4e00\u4e9b\u6f5c\u5728\u5c40\u9650\u6027\uff1a * \u6a21\u6001\u7279\u5f02\u6027\uff1a \u9886\u57df\u81ea\u9002\u5e94\u9884\u8bad\u7ec3\u4e3b\u8981\u57fa\u4e8eCT-3M\u6570\u636e\u96c6\uff0c\u8fd9\u610f\u5473\u7740\u5176\u5728MRI\u3001\u8d85\u58f0\u3001X\u5149\u7b49\u5176\u4ed6\u533b\u5b66\u5f71\u50cf\u6a21\u6001\u4e0a\u7684\u76f4\u63a5\u6cdb\u5316\u80fd\u529b\u53ef\u80fd\u53d7\u9650\u3002\u8981\u5b9e\u73b0\u771f\u6b63\u7684\u201c\u7edf\u4e00\u9aa8\u5e72\u201d\uff0c\u53ef\u80fd\u9700\u8981\u66f4\u901a\u7528\u7684\u591a\u6a21\u6001\u9884\u8bad\u7ec3\u6570\u636e\u548c\u7b56\u7565\u3002 * \u67b6\u6784\u6539\u8fdb\u7684\u7ec6\u8282\u4e0e\u666e\u9002\u6027\uff1a \u6458\u8981\u4e2d\u63d0\u5230\u5bf9ViT\u67b6\u6784\u8fdb\u884c\u4e86\u201c\u7b80\u5355\u800c\u6709\u6548\u201d\u7684\u6539\u8fdb\uff0c\u4f46\u5177\u4f53\u7ec6\u8282\u672a\u62ab\u9732\u3002\u5176\u590d\u6742\u6027\u3001\u8ba1\u7b97\u6548\u7387\u4ee5\u53ca\u5728\u66f4\u5e7f\u6cdb\u3001\u66f4\u590d\u6742\u7684\u533b\u5b66\u56fe\u50cf\u573a\u666f\u4e0b\u7684\u666e\u9002\u6027\u4ecd\u9700\u901a\u8fc7\u8bba\u6587\u6b63\u6587\u6df1\u5165\u4e86\u89e3\u3002 * \u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\uff1a \u5927\u89c4\u6a21\u6570\u636e\u96c6\uff083.87M CT\u5207\u7247\uff09\u4e0a\u7684\u591a\u9636\u6bb5DINOv3\u9884\u8bad\u7ec3\u901a\u5e38\u9700\u8981\u5de8\u5927\u7684\u8ba1\u7b97\u8d44\u6e90\u548c\u65f6\u95f4\uff0c\u8fd9\u53ef\u80fd\u9650\u5236\u4e86\u5176\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u5e94\u7528\u6216\u8fdb\u4e00\u6b65\u5f00\u53d1\u3002 * \u57fa\u51c6\u6d4b\u8bd5\u7684\u8986\u76d6\u8303\u56f4\uff1a \u5c3d\u7ba1\u5728\u56db\u4e2a\u5206\u5272\u57fa\u51c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4efb\u52a1\u7684\u591a\u6837\u6027\u8fdc\u8d85\u6b64\u8303\u56f4\u3002\u5176\u5728\u7f55\u89c1\u75be\u75c5\u3001\u7279\u5b9a\u75c5\u7406\u6216\u66f4\u590d\u6742\u89e3\u5256\u7ed3\u6784\u4e0a\u7684\u8868\u73b0\u4ecd\u9700\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u3002 * \u53ef\u89e3\u91ca\u6027\u4e0e\u9c81\u68d2\u6027\uff1a \u6458\u8981\u672a\u63d0\u53ca\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u6216\u5bf9\u5bf9\u6297\u6027\u653b\u51fb\u7684\u9c81\u68d2\u6027\u3002\u5728\u5bf9\u5b89\u5168\u6027\u8981\u6c42\u6781\u9ad8\u7684\u533b\u5b66\u5e94\u7528\u4e2d\uff0c\u8fd9\u4e9b\u662f\u91cd\u8981\u7684\u8003\u91cf\u56e0\u7d20\u3002 Key Findings: MedDINOv3 matches or exceeds state-of-the-art performance across four segmentation benchmarks, demonstrating the potential of vision foundation models as unified backbones for medical image segmentation. Links: PDF arXiv OneCAT: Decoder-Only Auto-Regressive Model for Unified Understanding and Generation Authors: Han Li, Xinyu Peng, Yaoming Wang, Zelin Peng, Xin Chen, Rongxiang Weng, Jingang Wang, Xunliang Cai, Wenrui Dai, Hongkai Xiong Published: 2025-09-03 Categories: cs.CV Abstract: We introduce OneCAT, a unified multimodal model that seamlessly integrates understanding, generation, and editing within a novel, pure decoder-only transformer architecture. Our framework uniquely eliminates the need for external components such as Vision Transformers (ViT) or vision tokenizer during inference, leading to significant efficiency gains, especially for high-resolution inputs. This is achieved through a modality-specific Mixture-of-Experts (MoE) structure trained with a single autoregressive (AR) objective, which also natively supports dynamic resolutions. Furthermore, we pioneer a multi-scale visual autoregressive mechanism within the Large Language Model (LLM) that drastically reduces decoding steps compared to diffusion-based methods while maintaining state-of-the-art performance. Our findings demonstrate the powerful potential of pure autoregressive modeling as a sufficient and elegant foundation for unified multimodal intelligence. As a result, OneCAT sets a new performance standard, outperforming existing open-source unified multimodal models across benchmarks for multimodal generation, editing, and understanding. Analysis: \u8fd9\u7bc7\u8bba\u6587\u6458\u8981\u4ecb\u7ecd\u7684OneCAT\u6a21\u578b\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u5177\u6709\u663e\u8457\u7684\u6f5c\u5728\u8da3\u5473\u6027\u548c\u91cd\u8981\u6027\u3002\u4ee5\u4e0b\u662f\u8be6\u7ec6\u5206\u6790\uff1a 1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e (Concise Summary) OneCAT\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u6a21\u6001\u6a21\u578b\uff0c\u91c7\u7528\u7eaf\u89e3\u7801\u5668Transformer\u67b6\u6784\uff0c\u65e0\u7f1d\u6574\u5408\u4e86\u7406\u89e3\u3001\u751f\u6210\u548c\u7f16\u8f91\u529f\u80fd\u3002\u5b83\u901a\u8fc7\u6d88\u9664\u63a8\u7406\u65f6\u5bf9\u5916\u90e8\u89c6\u89c9\u7ec4\u4ef6\uff08\u5982ViT\u6216\u89c6\u89c9tokenizer\uff09\u7684\u4f9d\u8d56\uff0c\u5e76\u5f15\u5165\u591a\u5c3a\u5ea6\u89c6\u89c9\u81ea\u56de\u5f52\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387\u548c\u6027\u80fd\uff0c\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u6a21\u578b\u3002 2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5 (Key Innovation or Methodological Approach) OneCAT\u7684\u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u5176 \u7eaf\u89e3\u7801\u5668Transformer\u67b6\u6784 \uff0c\u8be5\u67b6\u6784\u901a\u8fc7 \u6a21\u6001\u7279\u5b9a\u7684MoE\uff08Mixture-of-Experts\uff09\u7ed3\u6784 \uff0c\u5728\u5355\u4e00\u81ea\u56de\u5f52\u76ee\u6807\u4e0b\u8bad\u7ec3\uff0c\u4ece\u800c\u5728\u63a8\u7406\u65f6\u65e0\u9700\u5916\u90e8ViT\u6216\u89c6\u89c9tokenizer\uff0c\u5e76\u539f\u751f\u652f\u6301\u52a8\u6001\u5206\u8fa8\u7387\u3002\u6b64\u5916\uff0c\u5b83\u5f15\u5165\u4e86 \u591a\u5c3a\u5ea6\u89c6\u89c9\u81ea\u56de\u5f52\u673a\u5236 \uff0c\u663e\u8457\u51cf\u5c11\u4e86\u751f\u6210\u4efb\u52a1\u7684\u89e3\u7801\u6b65\u9aa4\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002 3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd (Potential Impact on the Field) \u8303\u5f0f\u8f6c\u53d8\uff1a \u8bc1\u660e\u4e86\u7eaf\u81ea\u56de\u5f52\u5efa\u6a21\u4f5c\u4e3a\u7edf\u4e00\u591a\u6a21\u6001\u667a\u80fd\u57fa\u7840\u7684\u5f3a\u5927\u6f5c\u529b\uff0c\u53ef\u80fd\u63a8\u52a8\u672a\u6765\u591a\u6a21\u6001\u6a21\u578b\u8bbe\u8ba1\u5411\u66f4\u7b80\u6d01\u3001\u7edf\u4e00\u7684\u81ea\u56de\u5f52\u8303\u5f0f\u53d1\u5c55\uff0c\u6311\u6218\u5f53\u524d\u4e3b\u6d41\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u6216\u6269\u6563\u6a21\u578b\u8303\u5f0f\u3002 \u6548\u7387\u63d0\u5347\uff1a \u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u6a21\u578b\u7684\u63a8\u7406\u6548\u7387\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u9ad8\u5206\u8fa8\u7387\u8f93\u5165\u548c\u751f\u6210\u4efb\u52a1\u65f6\uff0c\u901a\u8fc7\u51cf\u5c11\u5bf9\u5916\u90e8\u7ec4\u4ef6\u7684\u4f9d\u8d56\u548c\u4f18\u5316\u89e3\u7801\u6b65\u9aa4\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\uff08\u5982\u6269\u6563\u6a21\u578b\uff09\u7684\u6548\u7387\u74f6\u9888\u3002 \u6a21\u578b\u7b80\u5316\uff1a \u6d88\u9664\u5bf9\u5916\u90e8\u89c6\u89c9\u7ec4\u4ef6\u7684\u4f9d\u8d56\uff0c\u7b80\u5316\u4e86\u591a\u6a21\u6001\u7cfb\u7edf\u7684\u90e8\u7f72\u548c\u7ef4\u62a4\uff0c\u964d\u4f4e\u4e86\u7cfb\u7edf\u590d\u6742\u6027\u3002 \u6027\u80fd\u65b0\u6807\u6746\uff1a \u5728\u591a\u6a21\u6001\u7406\u89e3\u3001\u751f\u6210\u548c\u7f16\u8f91\u4efb\u52a1\u4e0a\u6811\u7acb\u4e86\u65b0\u7684\u6027\u80fd\u6807\u51c6\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u66f4\u5f3a\u7684\u57fa\u7ebf\u3002 4. \u76f8\u5173\u5e94\u7528\u9886\u57df (Related Areas or Applications that Might Benefit from this Research) \u591a\u6a21\u6001\u5185\u5bb9\u521b\u4f5c\uff1a \u9ad8\u8d28\u91cf\u56fe\u50cf/\u89c6\u9891\u751f\u6210\u3001\u98ce\u683c\u8fc1\u79fb\u3001\u6587\u672c\u5f15\u5bfc\u7684\u56fe\u50cf\u7f16\u8f91\u3001\u865a\u62df\u73b0\u5b9e/\u589e\u5f3a\u73b0\u5b9e\u4e2d\u7684\u5185\u5bb9\u751f\u6210\u3002 \u591a\u6a21\u6001\u7406\u89e3\uff1a \u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u3001\u56fe\u50cf\u5b57\u5e55\u751f\u6210\uff08Image Captioning\uff09\u3001\u89c6\u9891\u7406\u89e3\u3001\u8de8\u6a21\u6001\u68c0\u7d22\u3001\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u73af\u5883\u611f\u77e5\u3002 \u4eba\u673a\u4ea4\u4e92\uff1a \u66f4\u81ea\u7136\u7684\u591a\u6a21\u6001\u5bf9\u8bdd\u7cfb\u7edf\u3001\u667a\u80fd\u52a9\u624b\u3001\u673a\u5668\u4eba\u4e0e\u73af\u5883\u7684\u4ea4\u4e92\u3002 \u9ad8\u5206\u8fa8\u7387\u5e94\u7528\uff1a \u533b\u7597\u5f71\u50cf\u5206\u6790\u3001\u9065\u611f\u56fe\u50cf\u5904\u7406\u3001\u4e13\u4e1a\u8bbe\u8ba1\u9886\u57df\u7b49\u5bf9\u9ad8\u5206\u8fa8\u7387\u8f93\u5165\u6709\u9700\u6c42\u7684\u573a\u666f\uff0c\u5c06\u53d7\u76ca\u4e8e\u5176\u9ad8\u6548\u5904\u7406\u9ad8\u5206\u8fa8\u7387\u8f93\u5165\u7684\u80fd\u529b\u3002 5. \u6f5c\u5728\u5c40\u9650\u6027 (Any Limitations That Can Be Inferred from the Abstract) \u8bad\u7ec3\u6210\u672c\uff1a \u5c3d\u7ba1\u6458\u8981\u5f3a\u8c03\u4e86\u63a8\u7406\u6548\u7387\uff0c\u4f46\u8bad\u7ec3\u4e00\u4e2a\u7eaf\u89e3\u7801\u5668\u3001\u7edf\u4e00\u7684\u591a\u6a21\u6001\u6a21\u578b\uff0c\u7279\u522b\u662f\u5305\u542bMoE\u7ed3\u6784\u548c\u591a\u5c3a\u5ea6\u673a\u5236\u7684\u6a21\u578b\uff0c\u5176 \u8ba1\u7b97\u8d44\u6e90\u548c\u6570\u636e\u9700\u6c42\u53ef\u80fd\u975e\u5e38\u5de8\u5927 \uff0c\u8fd9\u5728\u6458\u8981\u4e2d\u672a\u63d0\u53ca\u3002 MoE\u7684\u590d\u6742\u6027\uff1a \u6a21\u6001\u7279\u5b9a\u7684MoE\u7ed3\u6784\u867d\u7136\u5e26\u6765\u4e86\u7075\u6d3b\u6027\uff0c\u4f46\u5176\u8def\u7531\u673a\u5236\u7684\u590d\u6742\u6027\u3001\u8d1f\u8f7d\u5747\u8861\u4ee5\u53ca\u5982\u4f55\u6709\u6548\u8bad\u7ec3\u4ee5\u907f\u514d\u201c\u4e13\u5bb6\u574d\u584c\u201d\u7b49\u95ee\u9898\uff0c\u53ef\u80fd\u9700\u8981\u7cbe\u7ec6\u7684\u8bbe\u8ba1\u548c\u8c03\u4f18\u3002 \u81ea\u56de\u5f52\u6a21\u578b\u7684\u56fa\u6709\u6311\u6218\uff1a \u5c3d\u7ba1\u58f0\u79f0\u6027\u80fd\u4f18\u8d8a\uff0c\u4f46\u7eaf\u81ea\u56de\u5f52\u6a21\u578b\u5728\u5904\u7406\u67d0\u4e9b\u590d\u6742\u7684\u751f\u6210\u4efb\u52a1\u65f6\uff0c\u5982\u957f\u8ddd\u79bb\u4e00\u81f4\u6027\u3001\u65b0\u9896\u6027\u6216\u591a\u6837\u6027\u65b9\u9762\uff0c\u53ef\u80fd\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u4e0e\u4e13\u95e8\u7684\u6269\u6563\u6a21\u578b\u8fdb\u884c\u7ec6\u81f4\u6bd4\u8f83\u65f6\u3002\u6458\u8981\u4e2d\u672a\u8be6\u7ec6\u8bf4\u660e\u751f\u6210\u8d28\u91cf\u7684\u8fd9\u4e9b\u65b9\u9762\u3002 \u201c\u5f00\u6e90\u201d\u9650\u5b9a\uff1a \u8bba\u6587\u58f0\u79f0\u8d85\u8d8a\u4e86\u201c\u73b0\u6709\u5f00\u6e90\u7edf\u4e00\u591a\u6a21\u6001\u6a21\u578b\u201d\uff0c\u8fd9\u53ef\u80fd\u610f\u5473\u7740\u5176\u6027\u80fd\u5c1a\u672a\u4e0e\u6240\u6709\u6700\u5148\u8fdb\u7684\uff08\u5305\u62ec\u95ed\u6e90\u6216\u975e\u7edf\u4e00\u4f46\u7279\u5b9a\u4efb\u52a1\u8868\u73b0\u5353\u8d8a\u7684\uff09\u6a21\u578b\uff09\u8fdb\u884c\u5168\u9762\u6bd4\u8f83\u3002 \u591a\u5c3a\u5ea6\u673a\u5236\u7684\u6cdb\u5316\u6027\uff1a \u591a\u5c3a\u5ea6\u89c6\u89c9\u81ea\u56de\u5f52\u673a\u5236\u5728\u4e0d\u540c\u5206\u8fa8\u7387\u3001\u4e0d\u540c\u6a21\u6001\u7ec4\u5408\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u3002 Key Findings: We introduce OneCAT, a unified multimodal model that seamlessly integrates understanding, generation, and editing within a novel, pure decoder-only transformer architecture. Furthermore, we pioneer a multi-scale visual autoregressive mechanism within the Large Language Model (LLM) that drastically reduces decoding steps compared to diffusion-based methods while maintaining state-of-the-art performance. As a result, OneCAT sets a new performance standard, outperforming existing open-source unified multimodal models across benchmarks for multimodal generation, editing, and understanding. Links: PDF arXiv GenCompositor: Generative Video Compositing with Diffusion Transformer Authors: Shuzhou Yang, Xiaoyu Li, Xiaodong Cun, Guangzhi Wang, Lingen Li, Ying Shan, Jian Zhang Published: 2025-09-02 Categories: cs.CV Abstract: Video compositing combines live-action footage to create video production, serving as a crucial technique in video creation and film production. Traditional pipelines require intensive labor efforts and expert collaboration, resulting in lengthy production cycles and high manpower costs. To address this issue, we automate this process with generative models, called generative video compositing. This new task strives to adaptively inject identity and motion information of foreground video to the target video in an interactive manner, allowing users to customize the size, motion trajectory, and other attributes of the dynamic elements added in final video. Specifically, we designed a novel Diffusion Transformer (DiT) pipeline based on its intrinsic properties. To maintain consistency of the target video before and after editing, we revised a light-weight DiT-based background preservation branch with masked token injection. As to inherit dynamic elements from other sources, a DiT fusion block is proposed using full self-attention, along with a simple yet effective foreground augmentation for training. Besides, for fusing background and foreground videos with different layouts based on user control, we developed a novel position embedding, named Extended Rotary Position Embedding (ERoPE). Finally, we curated a dataset comprising 61K sets of videos for our new task, called VideoComp. This data includes complete dynamic elements and high-quality target videos. Experiments demonstrate that our method effectively realizes generative video compositing, outperforming existing possible solutions in fidelity and consistency. Analysis: \u4f5c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u4e13\u5bb6\uff0c\u6211\u5bf9\u8fd9\u7bc7\u8bba\u6587\u6458\u8981\u8fdb\u884c\u5982\u4e0b\u5206\u6790\uff1a GenCompositor: Generative Video Compositing with Diffusion Transformer 1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u660e\u6458\u8981 (Concise Summary) \u672c\u6587\u63d0\u51fa\u4e86GenCompositor\uff0c\u4e00\u4e2a\u57fa\u4e8eDiffusion Transformer\uff08DiT\uff09\u7684\u751f\u6210\u5f0f\u89c6\u9891\u5408\u6210\u7cfb\u7edf\uff0c\u65e8\u5728\u81ea\u52a8\u5316\u4f20\u7edf\u89c6\u9891\u5408\u6210\u4e2d\u8017\u65f6\u8017\u529b\u7684\u8fc7\u7a0b\u3002\u5b83\u5141\u8bb8\u7528\u6237\u4ea4\u4e92\u5f0f\u5730\u5c06\u524d\u666f\u89c6\u9891\u7684\u8eab\u4efd\u548c\u8fd0\u52a8\u4fe1\u606f\u81ea\u9002\u5e94\u5730\u6ce8\u5165\u5230\u76ee\u6807\u89c6\u9891\u4e2d\uff0c\u5e76\u63d0\u4f9b\u5bf9\u524d\u666f\u5143\u7d20\u5927\u5c0f\u3001\u8fd0\u52a8\u8f68\u8ff9\u7b49\u5c5e\u6027\u7684\u5b9a\u5236\u5316\u63a7\u5236\u3002\u4e3a\u652f\u6301\u8fd9\u4e00\u65b0\u4efb\u52a1\uff0c\u4f5c\u8005\u8fd8\u6784\u5efa\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u7684\u89c6\u9891\u5408\u6210\u6570\u636e\u96c6VideoComp\u3002 2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u5b66\u65b9\u6cd5 (Key Innovation or Methodological Approach) GenCompositor\u7684\u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u5176\u4e3a\u751f\u6210\u5f0f\u89c6\u9891\u5408\u6210\u4efb\u52a1\u91cf\u8eab\u5b9a\u5236\u7684Diffusion Transformer (DiT) \u7ba1\u9053\u3002\u5177\u4f53\u5305\u62ec\uff1a DiT-based \u80cc\u666f\u4fdd\u6301\u5206\u652f\uff1a \u8bbe\u8ba1\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684DiT\u5206\u652f\uff0c\u901a\u8fc7\u63a9\u7801\u4ee4\u724c\u6ce8\u5165\uff08masked token injection\uff09\u673a\u5236\uff0c\u786e\u4fdd\u76ee\u6807\u89c6\u9891\u5728\u7f16\u8f91\u524d\u540e\u7684\u80cc\u666f\u4e00\u81f4\u6027\uff0c\u8fd9\u662f\u89c6\u9891\u7f16\u8f91\u4efb\u52a1\u4e2d\u4fdd\u6301\u8fde\u8d2f\u6027\u7684\u5173\u952e\u3002 DiT \u878d\u5408\u5757\u4e0e\u524d\u666f\u589e\u5f3a\uff1a \u63d0\u51fa\u4e86\u4e00\u4e2a\u91c7\u7528\u5168\u81ea\u6ce8\u610f\u529b\uff08full self-attention\uff09\u7684DiT\u878d\u5408\u5757\uff0c\u7528\u4e8e\u9ad8\u6548\u5730\u5c06\u524d\u666f\u52a8\u6001\u5143\u7d20\u7ee7\u627f\u5e76\u6ce8\u5165\u5230\u89c6\u9891\u4e2d\uff0c\u5e76\u8f85\u4ee5\u7b80\u5355\u800c\u6709\u6548\u7684\u524d\u666f\u589e\u5f3a\u7b56\u7565\u8fdb\u884c\u8bad\u7ec3\u3002 \u6269\u5c55\u65cb\u8f6c\u4f4d\u7f6e\u7f16\u7801 (ERoPE)\uff1a \u9488\u5bf9\u7528\u6237\u63a7\u5236\u4e0b\u4e0d\u540c\u5e03\u5c40\uff08\u5982\u524d\u666f\u5927\u5c0f\u3001\u4f4d\u7f6e\uff09\u7684\u80cc\u666f\u548c\u524d\u666f\u89c6\u9891\u878d\u5408\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4f4d\u7f6e\u7f16\u7801ERoPE\uff0c\u4ee5\u66f4\u597d\u5730\u5904\u7406\u7a7a\u95f4\u5e03\u5c40\u7684\u7075\u6d3b\u6027\u3002 \u5927\u89c4\u6a21\u65b0\u6570\u636e\u96c6VideoComp\uff1a \u4e3a\u8fd9\u4e00\u65b0\u4efb\u52a1\u7cbe\u5fc3\u7b56\u5212\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b61K\u89c6\u9891\u96c6\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5305\u542b\u5b8c\u6574\u7684\u52a8\u6001\u5143\u7d20\u548c\u9ad8\u8d28\u91cf\u7684\u76ee\u6807\u89c6\u9891\uff0c\u4e3a\u751f\u6210\u5f0f\u89c6\u9891\u5408\u6210\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u8d44\u6e90\u3002 3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd (Potential Impact on the Field) \u89c6\u9891\u5236\u4f5c\u6548\u7387\u9769\u547d\uff1a \u6781\u5927\u5730\u81ea\u52a8\u5316\u4e86\u4f20\u7edf\u4e0a\u52b3\u52a8\u5bc6\u96c6\u578b\u4e14\u6210\u672c\u9ad8\u6602\u7684\u89c6\u9891\u5408\u6210\u6d41\u7a0b\uff0c\u6709\u671b\u663e\u8457\u7f29\u77ed\u5236\u4f5c\u5468\u671f\u5e76\u964d\u4f4e\u4eba\u529b\u6210\u672c\uff0c\u4ece\u800c\u52a0\u901f\u89c6\u9891\u5185\u5bb9\u521b\u4f5c\u3002 \u6280\u672f\u666e\u60e0\u6027\uff1a \u5c06\u590d\u6742\u7684\u89c6\u9891\u5408\u6210\u6280\u672f\u4ece\u4e13\u4e1a\u4eba\u58eb\u6269\u5c55\u5230\u66f4\u5e7f\u6cdb\u7684\u7528\u6237\u7fa4\u4f53\uff0c\u964d\u4f4e\u4e86\u89c6\u9891\u521b\u4f5c\u7684\u95e8\u69db\uff0c\u8d4b\u80fd\u666e\u901a\u7528\u6237\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u4e2a\u6027\u5316\u89c6\u9891\u3002 \u65b0\u4efb\u52a1\u4e0e\u65b0\u8303\u5f0f\uff1a \u6b63\u5f0f\u5b9a\u4e49\u5e76\u89e3\u51b3\u4e86\u201c\u751f\u6210\u5f0f\u89c6\u9891\u5408\u6210\u201d\u8fd9\u4e00\u65b0\u4efb\u52a1\uff0c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u5728\u89c6\u9891\u751f\u6210\u4e0e\u7f16\u8f91\u65b9\u5411\u5f00\u8f9f\u4e86\u65b0\u7684\u7814\u7a76\u8303\u5f0f\u548c\u57fa\u51c6\u3002 DiT\u5e94\u7528\u62d3\u5c55\uff1a \u8fdb\u4e00\u6b65\u5c55\u793a\u4e86Diffusion Transformer\u5728\u590d\u6742\u89c6\u9891\u751f\u6210\u548c\u7f16\u8f91\u4efb\u52a1\u4e2d\u7684\u5f3a\u5927\u6f5c\u529b\uff0c\u63a8\u52a8\u4e86DiT\u6a21\u578b\u5728\u89c6\u9891\u7406\u89e3\u548c\u751f\u6210\u9886\u57df\u7684\u5e94\u7528\u8fb9\u754c\u3002 4. \u76f8\u5173\u9886\u57df\u6216\u5e94\u7528\u53d7\u76ca (Related Areas or Applications that Might Benefit) \u7535\u5f71\u4e0e\u7535\u89c6\u5236\u4f5c\uff1a \u663e\u8457\u7b80\u5316\u7279\u6548\u5408\u6210\u3001\u573a\u666f\u6784\u5efa\u3001\u7eff\u5e55\u62a0\u50cf\u540e\u7684\u5143\u7d20\u878d\u5408\u7b49\u73af\u8282\u3002 \u77ed\u89c6\u9891\u4e0e\u793e\u4ea4\u5a92\u4f53\u5185\u5bb9\u521b\u4f5c\uff1a \u8d4b\u80fd\u666e\u901a\u7528\u6237\u5feb\u901f\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u4e2a\u6027\u5316\u7684\u89c6\u9891\u5185\u5bb9\uff0c\u5982\u5728\u73b0\u6709\u89c6\u9891\u4e2d\u6dfb\u52a0\u865a\u62df\u89d2\u8272\u3001\u52a8\u6001\u8d34\u7eb8\u6216\u7279\u6548\u3002 \u5e7f\u544a\u4e0e\u8425\u9500\uff1a \u5feb\u901f\u5236\u4f5c\u5b9a\u5236\u5316\u7684\u4ea7\u54c1\u6f14\u793a\u6216\u5ba3\u4f20\u89c6\u9891\uff0c\u5b9e\u73b0A/B\u6d4b\u8bd5\u548c\u4e2a\u6027\u5316\u6295\u653e\u3002 \u865a\u62df\u73b0\u5b9e (VR) / \u589e\u5f3a\u73b0\u5b9e (AR)\uff1a \u4e3a\u865a\u62df\u73af\u5883\u6216AR\u5e94\u7528\u4e2d\u7684\u52a8\u6001\u5143\u7d20\u751f\u6210\u63d0\u4f9b\u652f\u6301\uff0c\u5b9e\u73b0\u66f4\u771f\u5b9e\u7684\u865a\u62df\u4e0e\u73b0\u5b9e\u878d\u5408\u3002 \u6e38\u620f\u5f00\u53d1\uff1a \u8f85\u52a9\u751f\u6210\u6e38\u620f\u5185\u8fc7\u573a\u52a8\u753b\u6216\u52a8\u6001\u573a\u666f\u5143\u7d20\uff0c\u63d0\u5347\u5f00\u53d1\u6548\u7387\u3002 \u4e13\u4e1a\u89c6\u9891\u7f16\u8f91\u8f6f\u4ef6\uff1a \u4f5c\u4e3a\u63d2\u4ef6\u6216\u6838\u5fc3\u529f\u80fd\u96c6\u6210\uff0c\u63d0\u5347\u73b0\u6709\u5de5\u5177\uff08\u5982Adobe Premiere, DaVinci Resolve\uff09\u7684\u667a\u80fd\u5316\u6c34\u5e73\u3002 5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u7684\u5c40\u9650\u6027 (Limitations that Can Be Inferred from the Abstract) \u80cc\u666f\u4fdd\u6301\u7684\u9c81\u68d2\u6027\uff1a \u5c3d\u7ba1\u63d0\u51fa\u4e86\u8f7b\u91cf\u7ea7\u80cc\u666f\u4fdd\u6301\u5206\u652f\uff0c\u4f46\u5728\u6781\u7aef\u590d\u6742\u7684\u80cc\u666f\u53d8\u5316\u3001\u5149\u7167\u6761\u4ef6\u6216\u524d\u666f\u4e0e\u80cc\u666f\u6df1\u5ea6\u4ea4\u4e92\u573a\u666f\u4e0b\uff0c\u5176\u4fdd\u6301\u4e00\u81f4\u6027\u7684\u80fd\u529b\u53ef\u80fd\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u53ef\u80fd\u51fa\u73b0\u7ec6\u5fae\u7684\u4f2a\u5f71\u3002 \u524d\u666f\u878d\u5408\u7684\u771f\u5b9e\u611f\u4e0e\u591a\u6837\u6027\uff1a \u6458\u8981\u4e2d\u63d0\u5230\u201c\u7b80\u5355\u800c\u6709\u6548\u7684\u524d\u666f\u589e\u5f3a\u201d\uff0c\u8fd9\u53ef\u80fd\u610f\u5473\u7740\u5728\u5904\u7406\u9ad8\u5ea6\u590d\u6742\u6216\u9700\u8981\u7cbe\u7ec6\u7269\u7406\u4ea4\u4e92\uff08\u5982\u5149\u5f71\u3001\u53cd\u5c04\u3001\u906e\u6321\uff09\u7684\u524d\u666f\u5143\u7d20\u65f6\uff0c\u5176\u751f\u6210\u7684\u591a\u6837\u6027\u548c\u771f\u5b9e\u611f\u53ef\u80fd\u5b58\u5728\u5c40\u9650\u3002 \u7528\u6237\u63a7\u5236\u7684\u7c92\u5ea6\u4e0e\u590d\u6742\u6027\uff1a \u5c3d\u7ba1\u5141\u8bb8\u7528\u6237\u5b9a\u5236\u5927\u5c0f\u3001\u8f68\u8ff9\u7b49\u5c5e\u6027\uff0c\u4f46\u5bf9\u4e8e\u66f4\u9ad8\u7ea7\u7684\u4ea4\u4e92\uff08\u5982\u524d\u666f\u4e0e\u80cc\u666f\u7684\u7269\u7406\u78b0\u649e\u3001\u5149\u5f71\u878d\u5408\u3001\u6750\u8d28\u5339\u914d\u3001\u8bed\u4e49\u7406\u89e3\u9a71\u52a8\u7684\u4ea4\u4e92\u7b49\uff09\uff0c\u6458\u8981\u4e2d\u672a\u660e\u786e\u63d0\u53ca\uff0c\u53ef\u80fd\u4ecd\u9700\u8fdb\u4e00\u6b65\u63a2\u7d22\u3002 \u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\uff1a Diffusion Transformer\u6a21\u578b\u901a\u5e38\u8ba1\u7b97\u6210\u672c\u8f83\u9ad8\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u89c6\u9891\u6570\u636e\u65f6\u3002\u5c3d\u7ba1\u90e8\u5206\u6a21\u5757\u8bbe\u8ba1\u4e3a\u201c\u8f7b\u91cf\u7ea7\u201d\uff0c\u4f46\u6574\u4f53\u6a21\u578b\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u901f\u5ea6\u4ecd\u53ef\u80fd\u662f\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u4e00\u4e2a\u8003\u91cf\u56e0\u7d20\uff0c\u7279\u522b\u662f\u5728\u5b9e\u65f6\u5e94\u7528\u573a\u666f\u4e0b\u3002 \u6570\u636e\u96c6\u7684\u6cdb\u5316\u6027\uff1a \u5c3d\u7ba1\u6784\u5efa\u4e86VideoComp\u6570\u636e\u96c6\uff0c\u4f46\u4efb\u4f55\u6570\u636e\u96c6\u90fd\u5b58\u5728\u5176\u56fa\u6709\u7684\u5206\u5e03\u548c\u504f\u5dee\u3002\u6a21\u578b\u5728\u9762\u5bf9\u4e0e\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u5dee\u5f02\u8f83\u5927\u7684\u771f\u5b9e\u4e16\u754c\u89c6\u9891\u65f6\uff0c\u5176\u6cdb\u5316\u80fd\u529b\u6709\u5f85\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u3002 \u201c\u73b0\u6709\u53ef\u884c\u65b9\u6848\u201d\u7684\u6bd4\u8f83\u57fa\u51c6\uff1a \u8bba\u6587\u63d0\u5230\u8d85\u8d8a\u201c\u73b0\u6709\u53ef\u884c\u65b9\u6848\u201d\uff0c\u8fd9\u53ef\u80fd\u6697\u793a\u76ee\u524d\u5c1a\u65e0\u9488\u5bf9\u201c\u751f\u6210\u5f0f\u89c6\u9891\u5408\u6210\u201d\u8fd9\u4e00\u65b0\u4efb\u52a1\u7684\u76f4\u63a5\u3001\u6210\u719f\u7684\u57fa\u51c6\u6a21\u578b\u3002\u56e0\u6b64\uff0c\u6bd4\u8f83\u7684\u516c\u5e73\u6027\u548c\u5168\u9762\u6027\u53ef\u80fd\u9700\u8981\u66f4\u8be6\u7ec6\u7684\u5b9e\u9a8c\u5206\u6790\u6765\u652f\u6491\uff0c\u4ee5\u786e\u4fdd\u5176\u4f18\u52bf\u662f\u9488\u5bf9\u540c\u7b49\u96be\u5ea6\u548c\u76ee\u6807\u7684\u65b9\u6cd5\u3002 Key Findings: This new task strives to adaptively inject identity and motion information of foreground video to the target video in an interactive manner, allowing users to customize the size, motion trajectory, and other attributes of the dynamic elements added in final video. Specifically, we designed a novel Diffusion Transformer (DiT) pipeline based on its intrinsic properties. Besides, for fusing background and foreground videos with different layouts based on user control, we developed a novel position embedding, named Extended Rotary Position Embedding (ERoPE). Finally, we curated a dataset comprising 61K sets of videos for our new task, called VideoComp. Experiments demonstrate that our method effectively realizes generative video compositing, outperforming existing possible solutions in fidelity and consistency. Links: PDF arXiv Enhancing Robustness in Post-Processing Watermarking: An Ensemble Attack Network Using CNNs and Transformers Authors: Tzuhsuan Huang, Cheng Yu Yeo, Tsai-Ling Huang, Hong-Han Shuai, Wen-Huang Cheng, Jun-Cheng Chen Published: 2025-09-03 Categories: cs.CV Abstract: Recent studies on deep watermarking have predominantly focused on in-processing watermarking, which integrates the watermarking process into image generation. However, post-processing watermarking, which embeds watermarks after image generation, offers more flexibility. It can be applied to outputs from any generative model (e.g. GANs, diffusion models) without needing access to the model's internal structure. It also allows users to embed unique watermarks into individual images. Therefore, this study focuses on post-processing watermarking and enhances its robustness by incorporating an ensemble attack network during training. We construct various versions of attack networks using CNN and Transformer in both spatial and frequency domains to investigate how each combination influences the robustness of the watermarking model. Our results demonstrate that combining a CNN-based attack network in the spatial domain with a Transformer-based attack network in the frequency domain yields the highest robustness in watermarking models. Extensive evaluation on the WAVES benchmark, using average bit accuracy as the metric, demonstrates that our ensemble attack network significantly enhances the robustness of baseline watermarking methods under various stress tests. In particular, for the Regeneration Attack defined in WAVES, our method improves StegaStamp by 18.743%. The code is released at:https://github.com/aiiu-lab/DeepRobustWatermark. Analysis: \u8fd9\u7bc7\u8bba\u6587\u4e13\u6ce8\u4e8e\u63d0\u5347\u540e\u5904\u7406\u6c34\u5370\uff08Post-Processing Watermarking\uff09\u7684\u9c81\u68d2\u6027\uff0c\u8fd9\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\uff0c\u5c24\u5176\u662f\u5728AI\u751f\u6210\u5185\u5bb9\u65e5\u76ca\u666e\u53ca\u7684\u80cc\u666f\u4e0b\uff0c\u5177\u6709\u91cd\u8981\u7684\u7814\u7a76\u610f\u4e49\u3002 1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u660e\u6458\u8981 (Concise Summary) \u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5f15\u5165\u96c6\u6210\u653b\u51fb\u7f51\u7edc\uff08Ensemble Attack Network\uff09\u6765\u589e\u5f3a\u540e\u5904\u7406\u6c34\u5370\u9c81\u68d2\u6027\u7684\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u7cfb\u7edf\u5730\u63a2\u7d22\u4e86\u7ed3\u5408\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u548cTransformer\u67b6\u6784\uff0c\u5e76\u5728\u7a7a\u95f4\u57df\u548c\u9891\u7387\u57df\u6784\u5efa\u653b\u51fb\u7f51\u7edc\uff0c\u53d1\u73b0CNN\uff08\u7a7a\u95f4\u57df\uff09\u4e0eTransformer\uff08\u9891\u7387\u57df\uff09\u7684\u7ec4\u5408\u80fd\u663e\u8457\u63d0\u9ad8\u6c34\u5370\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002\u5728WAVES\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u5404\u79cd\u538b\u529b\u6d4b\u8bd5\u4e0b\uff0c\u7279\u522b\u662f\u518d\u751f\u653b\u51fb\uff08Regeneration Attack\uff09\u4e2d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u57fa\u7ebf\u6c34\u5370\u65b9\u6cd5\u7684\u6027\u80fd\u3002 2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u8bba (Key Innovation or Methodological Approach) \u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u63d0\u51fa\u5e76\u7cfb\u7edf\u6027\u5730\u63a2\u7d22\u4e86\u4e00\u79cd\u7528\u4e8e\u8bad\u7ec3\u9c81\u68d2\u540e\u5904\u7406\u6c34\u5370\u6a21\u578b\u7684\u201c\u96c6\u6210\u653b\u51fb\u7f51\u7edc\u201d\uff08Ensemble Attack Network\uff09\u3002\u8be5\u7f51\u7edc\u5728\u8bad\u7ec3\u9636\u6bb5\u6a21\u62df\u591a\u79cd\u6f5c\u5728\u653b\u51fb\uff0c\u901a\u8fc7\u7ed3\u5408\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u548cTransformer\u67b6\u6784\uff0c\u5e76\u5728\u7a7a\u95f4\u57df\u548c\u9891\u7387\u57df\u540c\u65f6\u8fdb\u884c\u653b\u51fb\u5efa\u6a21\u3002\u8fd9\u79cd\u591a\u6a21\u6001\u3001\u591a\u57df\u7684\u96c6\u6210\u653b\u51fb\u7b56\u7565\uff0c\u4f7f\u5f97\u6c34\u5370\u6a21\u578b\u80fd\u591f\u5b66\u4e60\u62b5\u5fa1\u66f4\u5e7f\u6cdb\u3001\u66f4\u590d\u6742\u7684\u653b\u51fb\u7c7b\u578b\uff0c\u4ece\u800c\u663e\u8457\u63d0\u5347\u5176\u9c81\u90a6\u6027\u3002\u8bba\u6587\u8fd8\u901a\u8fc7\u5b9e\u9a8c\u786e\u5b9a\u4e86CNN\uff08\u7a7a\u95f4\u57df\uff09\u4e0eTransformer\uff08\u9891\u7387\u57df\uff09\u7684\u6700\u4f73\u7ec4\u5408\u3002 3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd (Potential Impact on the Field) \u9274\u4e8e\u5f53\u524d\u751f\u6210\u6a21\u578b\uff08\u5982GANs\u3001\u6269\u6563\u6a21\u578b\uff09\u7684\u98de\u901f\u53d1\u5c55\uff0cAI\u751f\u6210\u5185\u5bb9\u7684\u7248\u6743\u4fdd\u62a4\u3001\u6eaf\u6e90\u548c\u771f\u5b9e\u6027\u9a8c\u8bc1\u53d8\u5f97\u65e5\u76ca\u91cd\u8981\u3002\u672c\u6587\u63d0\u51fa\u7684\u9c81\u68d2\u540e\u5904\u7406\u6c34\u5370\u6280\u672f\uff0c\u4e3a\u8fd9\u4e9b\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7075\u6d3b\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002\u5b83\u4f7f\u5f97\u6c34\u5370\u80fd\u591f\u72ec\u7acb\u4e8e\u751f\u6210\u6a21\u578b\u5d4c\u5165\uff0c\u6781\u5927\u5730\u6269\u5c55\u4e86\u5176\u5e94\u7528\u8303\u56f4\u3002\u5176\u5bf9\u6c34\u5370\u9c81\u68d2\u6027\u7684\u663e\u8457\u63d0\u5347\uff0c\u6709\u671b\u63a8\u52a8\u540e\u5904\u7406\u6c34\u5370\u6280\u672f\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u666e\u53ca\uff0c\u6210\u4e3aAI\u751f\u6210\u5185\u5bb9\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u5173\u952e\u7ec4\u6210\u90e8\u5206\uff0c\u5c24\u5176\u662f\u5728\u5185\u5bb9\u771f\u5b9e\u6027\u3001\u7248\u6743\u5f52\u5c5e\u548c\u6570\u5b57\u8d44\u4ea7\u7ba1\u7406\u65b9\u9762\u3002 4. \u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 (Related Areas or Applications) AI\u751f\u6210\u5185\u5bb9\u7684\u7248\u6743\u4fdd\u62a4\u4e0e\u6eaf\u6e90\uff1a \u786e\u4fddAI\u751f\u6210\u56fe\u50cf\u3001\u89c6\u9891\u7b49\u5185\u5bb9\u7684\u539f\u521b\u6027\uff0c\u9632\u6b62\u672a\u7ecf\u6388\u6743\u7684\u4f7f\u7528\uff0c\u5e76\u8ffd\u8e2a\u5176\u6765\u6e90\u3002 \u6570\u5b57\u5185\u5bb9\u771f\u5b9e\u6027\u9a8c\u8bc1\uff1a \u8f85\u52a9\u8bc6\u522bAI\u751f\u6210\u5185\u5bb9\uff0c\u5c24\u5176\u662f\u5728\u5bf9\u6297\u6df1\u5ea6\u4f2a\u9020\uff08Deepfake\uff09\u65b9\u9762\uff0c\u901a\u8fc7\u6c34\u5370\u7684\u5b58\u5728\u6216\u7f3a\u5931\u6765\u5224\u65ad\u5185\u5bb9\u7684\u6765\u6e90\u548c\u5b8c\u6574\u6027\u3002 \u6570\u5b57\u8d44\u4ea7\u7ba1\u7406\uff08DAM\uff09\uff1a \u4e3a\u5927\u89c4\u6a21AI\u751f\u6210\u5185\u5bb9\u5e93\u63d0\u4f9b\u6709\u6548\u7684\u7ba1\u7406\u548c\u8ffd\u8e2a\u673a\u5236\u3002 \u77e5\u8bc6\u4ea7\u6743\u4fdd\u62a4\uff1a \u4fdd\u62a4\u521b\u4f5c\u8005\u548c\u4f01\u4e1a\u7684\u6570\u5b57\u4f5c\u54c1\u514d\u53d7\u76d7\u7528\u3002 \u591a\u5a92\u4f53\u53d6\u8bc1\uff1a \u5728\u6570\u5b57\u56fe\u50cf\u548c\u89c6\u9891\u7684\u7be1\u6539\u68c0\u6d4b\u548c\u6765\u6e90\u5206\u6790\u4e2d\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u8bc1\u636e\u3002 5. \u53ef\u4ece\u6458\u8981\u63a8\u65ad\u7684\u5c40\u9650\u6027 (Limitations Inferred from the Abstract) \u653b\u51fb\u7c7b\u578b\u8986\u76d6\u8303\u56f4\uff1a \u5c3d\u7ba1\u96c6\u6210\u4e86\u591a\u79cd\u653b\u51fb\u7f51\u7edc\uff0c\u4f46\u6458\u8981\u5e76\u672a\u8be6\u7ec6\u8bf4\u660e\u5176\u80fd\u6709\u6548\u62b5\u5fa1\u7684\u5177\u4f53\u653b\u51fb\u7c7b\u578b\u96c6\u5408\u3002\u5b9e\u9645\u5e94\u7528\u4e2d\u53ef\u80fd\u5b58\u5728\u672a\u88ab\u6a21\u578b\u8bad\u7ec3\u5145\u5206\u8986\u76d6\u7684\u65b0\u578b\u6216\u7ec4\u5408\u653b\u51fb\u3002 \u6c34\u5370\u5bb9\u91cf\u4e0e\u4e0d\u53ef\u611f\u77e5\u6027\uff1a \u6458\u8981\u4e3b\u8981\u5f3a\u8c03\u9c81\u68d2\u6027\uff0c\u4f46\u672a\u63d0\u53ca\u6c34\u5370\u7684\u5bb9\u91cf\uff08\u80fd\u5d4c\u5165\u591a\u5c11\u4fe1\u606f\uff09\u4ee5\u53ca\u5d4c\u5165\u6c34\u5370\u540e\u5bf9\u56fe\u50cf\u89c6\u89c9\u8d28\u91cf\u7684\u5f71\u54cd\uff08\u4e0d\u53ef\u611f\u77e5\u6027\uff09\u3002\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u9c81\u68d2\u6027\u3001\u5bb9\u91cf\u548c\u4e0d\u53ef\u611f\u77e5\u6027\u4e4b\u95f4\u5f80\u5f80\u5b58\u5728\u6743\u8861\u3002 \u8ba1\u7b97\u6210\u672c\uff1a \u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4f7f\u7528\u96c6\u6210\u653b\u51fb\u7f51\u7edc\u53ef\u80fd\u4f1a\u589e\u52a0\u8bad\u7ec3\u7684\u590d\u6742\u6027\u548c\u8ba1\u7b97\u6210\u672c\uff0c\u5c24\u5176\u662f\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u3002 \u6cdb\u5316\u80fd\u529b\uff1a \u5c3d\u7ba1\u5728WAVES\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5728\u9762\u5bf9\u5b8c\u5168\u672a\u77e5\u6216\u9ad8\u5ea6\u5b9a\u5236\u5316\u7684\u653b\u51fb\u65f6\u7684\u6cdb\u5316\u80fd\u529b\u4ecd\u9700\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u3002 \u7279\u5b9a\u653b\u51fb\u7684\u5c40\u9650\u6027\uff1a \u6458\u8981\u4e2d\u63d0\u5230\u5bf9\u201c\u518d\u751f\u653b\u51fb\u201d\u7684\u663e\u8457\u63d0\u5347\uff0818.743%\uff09\uff0c\u8fd9\u6697\u793a\u4e86\u8be5\u65b9\u6cd5\u53ef\u80fd\u5728\u67d0\u4e9b\u7279\u5b9a\u653b\u51fb\u7c7b\u578b\u4e0a\u8868\u73b0\u5c24\u4e3a\u7a81\u51fa\uff0c\u4f46\u5728\u5176\u4ed6\u653b\u51fb\u7c7b\u578b\u4e0a\u7684\u63d0\u5347\u5e45\u5ea6\u53ef\u80fd\u6709\u6240\u4e0d\u540c\u3002 Key Findings: In particular, for the Regeneration Attack defined in WAVES, our method improves StegaStamp by 18.743%. Links: PDF arXiv TinyDrop: Tiny Model Guided Token Dropping for Vision Transformers Authors: Guoxin Wang, Qingyuan Wang, Binhua Huang, Shaowu Chen, Deepu John Published: 2025-09-03 Categories: cs.CV, cs.AI Abstract: Vision Transformers (ViTs) achieve strong performance in image classification but incur high computational costs from processing all image tokens. To reduce inference costs in large ViTs without compromising accuracy, we propose TinyDrop, a training-free token dropping framework guided by a lightweight vision model. The guidance model estimates the importance of tokens while performing inference, thereby selectively discarding low-importance tokens if large vit models need to perform attention calculations. The framework operates plug-and-play, requires no architectural modifications, and is compatible with diverse ViT architectures. Evaluations on standard image classification benchmarks demonstrate that our framework reduces FLOPs by up to 80% for ViTs with minimal accuracy degradation, highlighting its generalization capability and practical utility for efficient ViT-based classification. Analysis: \u597d\u7684\uff0c\u4f5c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u4e13\u5bb6\uff0c\u6211\u5bf9\u8fd9\u7bc7\u8bba\u6587\u6458\u8981\u8fdb\u884c\u5982\u4e0b\u5206\u6790\uff1a TinyDrop: Tiny Model Guided Token Dropping for Vision Transformers 1. \u8bba\u6587\u6838\u5fc3\u8d21\u732e\u7684\u7b80\u6d01\u603b\u7ed3 (Concise Summary) \u672c\u6587\u63d0\u51fa\u4e86TinyDrop\uff0c\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\uff08\u9488\u5bf9\u4e3bViT\u6a21\u578b\uff09token\u4e22\u5f03\u6846\u67b6\uff0c\u65e8\u5728\u663e\u8457\u964d\u4f4e\u5927\u578bVision Transformers (ViTs)\u7684\u63a8\u7406\u8ba1\u7b97\u6210\u672c\u3002\u5b83\u901a\u8fc7\u5229\u7528\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u89c6\u89c9\u6a21\u578b\u5728\u63a8\u7406\u65f6\u52a8\u6001\u8bc4\u4f30\u5e76\u9009\u62e9\u6027\u5730\u4e22\u5f03\u4f4e\u91cd\u8981\u6027token\uff0c\u4ece\u800c\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u5c06ViTs\u7684FLOPs\u51cf\u5c11\u9ad8\u8fbe80%\u3002 2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u5b66\u65b9\u6cd5 (Key Innovation or Methodological Approach) \u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u5176\u201c\u8f7b\u91cf\u7ea7\u6a21\u578b\u5f15\u5bfc\u7684token\u4e22\u5f03\u201d\u7b56\u7565\u3002TinyDrop\u5f15\u5165\u4e86\u4e00\u4e2a\u72ec\u7acb\u7684\u3001\u65e0\u9700\u8bad\u7ec3\u7684\uff08\u9488\u5bf9\u4e3bViT\u800c\u8a00\uff09\u6846\u67b6\uff0c\u5229\u7528\u4e00\u4e2a\u5c0f\u578b\u8f85\u52a9\u6a21\u578b\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5b9e\u65f6\u8bc4\u4f30ViT\u8f93\u5165token\u7684\u91cd\u8981\u6027\u3002\u8fd9\u79cd\u65b9\u6cd5\u5141\u8bb8\u52a8\u6001\u5730\u3001\u9009\u62e9\u6027\u5730\u4e22\u5f03\u4f4e\u91cd\u8981\u6027token\uff0c\u800c\u65e0\u9700\u4fee\u6539\u539f\u59cbViT\u67b6\u6784\u6216\u8fdb\u884c\u989d\u5916\u7684\u5fae\u8c03\uff0c\u5b9e\u73b0\u4e86\u5373\u63d2\u5373\u7528\uff0c\u5e76\u517c\u5bb9\u591a\u79cdViT\u67b6\u6784\u3002 3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd (Potential Impact on the Field) \u8be5\u7814\u7a76\u5bf9\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u5177\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u5c24\u5176\u662f\u5728ViT\u6a21\u578b\u7684\u5b9e\u9645\u90e8\u7f72\u65b9\u9762\u3002\u5b83\u663e\u8457\u964d\u4f4e\u4e86\u5927\u578bViT\u6a21\u578b\u7684\u63a8\u7406\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u7cbe\u5ea6\uff0c\u8fd9\u4f7f\u5f97ViT\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8bbe\u5907\uff08\u5982\u79fb\u52a8\u8bbe\u5907\u3001\u5d4c\u5165\u5f0f\u7cfb\u7edf\uff09\u4e0a\u53d8\u5f97\u66f4\u52a0\u53ef\u884c\u3002\u8fd9\u5c06\u52a0\u901fViT\u5728\u5404\u79cd\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u666e\u53ca\uff0c\u63a8\u52a8\u9ad8\u6548\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u5e76\u53ef\u80fd\u542f\u53d1\u66f4\u591a\u5173\u4e8e\u6a21\u578b\u538b\u7f29\u548c\u63a8\u7406\u4f18\u5316\u7684\u7814\u7a76\u65b9\u5411\u3002 4. \u53ef\u80fd\u53d7\u76ca\u7684\u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 (Related Areas or Applications) \u9ad8\u6548\u56fe\u50cf\u5206\u7c7b: \u8fd9\u662f\u6458\u8981\u4e2d\u660e\u786e\u63d0\u5230\u7684\u4e3b\u8981\u5e94\u7528\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u5feb\u901f\u54cd\u5e94\u7684\u573a\u666f\u3002 \u8fb9\u7f18\u8ba1\u7b97\u4e0e\u79fb\u52a8AI: \u5728\u8ba1\u7b97\u8d44\u6e90\u548c\u529f\u8017\u53d7\u9650\u7684\u8bbe\u5907\u4e0a\u90e8\u7f72\u9ad8\u6027\u80fdViT\u6a21\u578b\uff0c\u5982\u667a\u80fd\u624b\u673a\u3001\u7269\u8054\u7f51\u8bbe\u5907\u3002 \u5b9e\u65f6\u89c6\u89c9\u7cfb\u7edf: \u9700\u8981\u4f4e\u5ef6\u8fdf\u63a8\u7406\u7684\u5e94\u7528\uff0c\u5982\u81ea\u52a8\u9a7e\u9a76\u3001\u673a\u5668\u4eba\u89c6\u89c9\u3001\u89c6\u9891\u76d1\u63a7\u3002 ViT\u4f5c\u4e3a\u9aa8\u5e72\u7f51\u7edc\u7684\u5176\u4ed6\u4efb\u52a1: \u867d\u7136\u6458\u8981\u53ea\u63d0\u5230\u4e86\u5206\u7c7b\uff0c\u4f46\u5982\u679cViT\u88ab\u7528\u4f5c\u76ee\u6807\u68c0\u6d4b\u3001\u8bed\u4e49\u5206\u5272\u3001\u59ff\u6001\u4f30\u8ba1\u7b49\u4efb\u52a1\u7684\u9aa8\u5e72\u7f51\u7edc\uff0cTinyDrop\u7406\u8bba\u4e0a\u4e5f\u80fd\u63d0\u4f9b\u6548\u7387\u63d0\u5347\u3002 \u53ef\u6301\u7eedAI/\u7eff\u8272AI: \u51cf\u5c11\u6a21\u578b\u8fd0\u884c\u7684\u8ba1\u7b97\u91cf\u548c\u80fd\u8017\uff0c\u7b26\u5408\u5f53\u524d\u5bf9\u53ef\u6301\u7eedAI\u53d1\u5c55\u7684\u9700\u6c42\u3002 5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u7684\u5c40\u9650\u6027 (Limitations that can be inferred from the abstract) \u8f7b\u91cf\u7ea7\u5f15\u5bfc\u6a21\u578b\u7684\u5f00\u9500: \u6458\u8981\u672a\u660e\u786e\u6307\u51fa\u8f7b\u91cf\u7ea7\u5f15\u5bfc\u6a21\u578b\u672c\u8eab\u7684\u8ba1\u7b97\u5f00\u9500\u3002\u867d\u7136\u5b83\u5f88\u8f7b\u91cf\uff0c\u4f46\u5728\u6781\u7aef\u8d44\u6e90\u53d7\u9650\u7684\u573a\u666f\u4e0b\uff0c\u5176\u63a8\u7406\u65f6\u95f4\u6216FLOPs\u662f\u5426\u5b8c\u5168\u53ef\u4ee5\u5ffd\u7565\u4e0d\u8ba1\uff1f\u4ee5\u53ca\u8fd9\u4e2a\u5f15\u5bfc\u6a21\u578b\u662f\u5982\u4f55\u83b7\u53d6\u7684\uff08\u662f\u5426\u9700\u8981\u9884\u8bad\u7ec3\u6216\u7279\u5b9a\u4efb\u52a1\u8bad\u7ec3\uff09\uff1f \u201c\u6700\u5c0f\u7cbe\u5ea6\u4e0b\u964d\u201d\u7684\u91cf\u5316: \u5c3d\u7ba1\u58f0\u79f0\u7cbe\u5ea6\u4e0b\u964d\u6700\u5c0f\uff0c\u4f46\u5728\u67d0\u4e9b\u5bf9\u7cbe\u5ea6\u8981\u6c42\u6781\u9ad8\u7684\u5e94\u7528\u4e2d\uff0c\u5373\u4f7f\u662f\u5fae\u5c0f\u7684\u4e0b\u964d\u4e5f\u53ef\u80fd\u65e0\u6cd5\u63a5\u53d7\u3002\u5177\u4f53\u7684\u7cbe\u5ea6-\u6548\u7387\u6743\u8861\u66f2\u7ebf\uff08Pareto Frontier\uff09\u672a\u5728\u6458\u8981\u4e2d\u8be6\u7ec6\u8bf4\u660e\u3002 FLOPs\u4e0e\u5b9e\u9645\u5ef6\u8fdf: FLOPs\u7684\u51cf\u5c11\u4e0d\u603b\u662f\u76f4\u63a5\u7b49\u540c\u4e8e\u5b9e\u9645\u63a8\u7406\u5ef6\u8fdf\u7684\u7ebf\u6027\u964d\u4f4e\u3002token\u4e22\u5f03\u64cd\u4f5c\u672c\u8eab\u4ee5\u53ca\u5f15\u5bfc\u6a21\u578b\u7684\u63a8\u7406\u53ef\u80fd\u4f1a\u5f15\u5165\u989d\u5916\u7684\u5185\u5b58\u8bbf\u95ee\u3001\u6761\u4ef6\u5206\u652f\u7b49\u5f00\u9500\uff0c\u8fd9\u53ef\u80fd\u5f71\u54cd\u5b9e\u9645\u7684\u7aef\u5230\u7aef\u5ef6\u8fdf\u3002 \u4e22\u5f03\u7b56\u7565\u7684\u9c81\u68d2\u6027: \u5c3d\u7ba1\u58f0\u79f0\u5177\u6709\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u5bf9\u4e8e\u4e0d\u540cViT\u67b6\u6784\u3001\u4e0d\u540c\u4efb\u52a1\uff08\u4f8b\u5982\uff0c\u4e0d\u4ec5\u4ec5\u662f\u5206\u7c7b\uff0c\u8fd8\u6709\u68c0\u6d4b\u3001\u5206\u5272\u7b49\uff09\u4ee5\u53ca\u4e0d\u540c\u590d\u6742\u5ea6\u7684\u56fe\u50cf\uff0c\u5176token\u91cd\u8981\u6027\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u548c\u4e22\u5f03\u7b56\u7565\u7684\u9c81\u68d2\u6027\u5982\u4f55\uff0c\u4ecd\u9700\u66f4\u6df1\u5165\u7684\u9a8c\u8bc1\u3002 \u201c\u8bad\u7ec3-free\u201d\u7684\u8303\u56f4: \u6458\u8981\u5f3a\u8c03\u6846\u67b6\u662f\u201c\u8bad\u7ec3-free\u201d\u7684\uff0c\u8fd9\u901a\u5e38\u6307\u4e3bViT\u6a21\u578b\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002\u4f46\u8f7b\u91cf\u7ea7\u5f15\u5bfc\u6a21\u578b\u672c\u8eab\u662f\u5426\u9700\u8981\u8bad\u7ec3\uff1f\u5982\u679c\u662f\uff0c\u5176\u8bad\u7ec3\u6210\u672c\u548c\u6570\u636e\u9700\u6c42\u5982\u4f55\uff1f\u8fd9\u53ef\u80fd\u662f\u4e00\u4e2a\u9690\u85cf\u7684\u6210\u672c\u3002 Key Findings: To reduce inference costs in large ViTs without compromising accuracy, we propose TinyDrop, a training-free token dropping framework guided by a lightweight vision model. Links: PDF arXiv LGBP-OrgaNet: Learnable Gaussian Band Pass Fusion of CNN and Transformer Features for Robust Organoid Segmentation and Tracking Authors: Jing Zhang, Siying Tao, Jiao Li, Tianhe Wang, Junchen Wu, Ruqian Hao, Xiaohui Du, Ruirong Tan, Rui Li Published: 2025-09-03 Categories: cs.CV, cs.AI Abstract: Organoids replicate organ structure and function, playing a crucial role in fields such as tumor treatment and drug screening. Their shape and size can indicate their developmental status, but traditional fluorescence labeling methods risk compromising their structure. Therefore, this paper proposes an automated, non-destructive approach to organoid segmentation and tracking. We introduced the LGBP-OrgaNet, a deep learning-based system proficient in accurately segmenting, tracking, and quantifying organoids. The model leverages complementary information extracted from CNN and Transformer modules and introduces the innovative feature fusion module, Learnable Gaussian Band Pass Fusion, to merge data from two branches. Additionally, in the decoder, the model proposes a Bidirectional Cross Fusion Block to fuse multi-scale features, and finally completes the decoding through progressive concatenation and upsampling. SROrga demonstrates satisfactory segmentation accuracy and robustness on organoids segmentation datasets, providing a potent tool for organoid research. Analysis: \u4f5c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u4e13\u5bb6\uff0c\u6211\u5bf9\u8fd9\u7bc7\u8bba\u6587\u6458\u8981\u7684\u5206\u6790\u5982\u4e0b\uff1a LGBP-OrgaNet: Learnable Gaussian Band Pass Fusion of CNN and Transformer Features for Robust Organoid Segmentation and Tracking 1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u660e\u6458\u8981 (2-3 \u53e5\u8bdd) \u672c\u6587\u63d0\u51fa\u4e86LGBP-OrgaNet\uff0c\u4e00\u4e2a\u7528\u4e8e\u81ea\u52a8\u3001\u65e0\u635f\u7c7b\u5668\u5b98\u5206\u5272\u4e0e\u8ffd\u8e2a\u7684\u6df1\u5ea6\u5b66\u4e60\u7cfb\u7edf\u3002\u8be5\u6a21\u578b\u901a\u8fc7\u521b\u65b0\u7684\u53ef\u5b66\u4e60\u9ad8\u65af\u5e26\u901a\u878d\u5408\u6a21\u5757\u6574\u5408CNN\u548cTransformer\u7684\u4e92\u8865\u7279\u5f81\uff0c\u5e76\u5728\u89e3\u7801\u5668\u4e2d\u5f15\u5165\u53cc\u5411\u4ea4\u53c9\u878d\u5408\u5757\u5904\u7406\u591a\u5c3a\u5ea6\u4fe1\u606f\u3002\u5b83\u5728\u7c7b\u5668\u5b98\u5206\u5272\u6570\u636e\u96c6\u4e0a\u5c55\u73b0\u51fa\u4ee4\u4eba\u6ee1\u610f\u7684\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u7c7b\u5668\u5b98\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177\u3002 2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u5b66\u65b9\u6cd5 \u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u5176 \u53ef\u5b66\u4e60\u9ad8\u65af\u5e26\u901a\u878d\u5408\uff08Learnable Gaussian Band Pass Fusion\uff09\u6a21\u5757 \u3002\u8be5\u6a21\u5757\u80fd\u591f\u81ea\u9002\u5e94\u5730\u878d\u5408\u6765\u81eaCNN\uff08\u64c5\u957f\u5c40\u90e8\u7279\u5f81\u63d0\u53d6\uff09\u548cTransformer\uff08\u64c5\u957f\u5168\u5c40\u4f9d\u8d56\u5efa\u6a21\uff09\u5206\u652f\u7684\u4e92\u8865\u7279\u5f81\uff0c\u4ee5\u5b9e\u73b0\u66f4\u9c81\u68d2\u7684\u8868\u793a\u3002\u6b64\u5916\uff0c\u89e3\u7801\u5668\u4e2d\u63d0\u51fa\u7684 \u53cc\u5411\u4ea4\u53c9\u878d\u5408\u5757\uff08Bidirectional Cross Fusion Block\uff09 \u7528\u4e8e\u6709\u6548\u6574\u5408\u591a\u5c3a\u5ea6\u7279\u5f81\uff0c\u4ee5\u53ca\u6e10\u8fdb\u5f0f\u62fc\u63a5\u548c\u4e0a\u91c7\u6837\u5b8c\u6210\u89e3\u7801\uff0c\u4e5f\u662f\u4e00\u4e2a\u91cd\u8981\u7684\u6280\u672f\u8d21\u732e\u3002 3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd \u8be5\u7814\u7a76\u4e3a\u7c7b\u5668\u5b98\u7814\u7a76\u9886\u57df\u63d0\u4f9b\u4e86\u4e00\u4e2a \u81ea\u52a8\u5316\u3001\u65e0\u635f\u4e14\u9ad8\u7cbe\u5ea6 \u7684\u5206\u6790\u5de5\u5177\u3002\u901a\u8fc7\u66ff\u4ee3\u6216\u51cf\u5c11\u5bf9\u4f20\u7edf\u8367\u5149\u6807\u8bb0\u7b49\u7834\u574f\u6027\u65b9\u6cd5\u7684\u4f9d\u8d56\uff0c\u5b83\u5c06\u6781\u5927\u5730\u52a0\u901f\u7c7b\u5668\u5b98\u5728\u80bf\u7624\u6cbb\u7597\u3001\u836f\u7269\u7b5b\u9009\u548c\u53d1\u80b2\u751f\u7269\u5b66\u7b49\u9886\u57df\u7684\u5e94\u7528\u548c\u7814\u7a76\u8fdb\u7a0b\uff0c\u5b9e\u73b0\u66f4\u53ef\u9760\u3001\u66f4\u957f\u671f\u7684\u52a8\u6001\u89c2\u5bdf\u548c\u91cf\u5316\u5206\u6790\u3002\u8fd9\u79cd\u975e\u4fb5\u5165\u6027\u65b9\u6cd5\u5bf9\u4e8e\u4fdd\u6301\u7c7b\u5668\u5b98\u7684\u751f\u7406\u5b8c\u6574\u6027\u81f3\u5173\u91cd\u8981\u3002 4. \u53ef\u80fd\u53d7\u76ca\u4e8e\u8be5\u7814\u7a76\u7684\u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 \u751f\u7269\u533b\u5b66\u7814\u7a76\uff1a \u7c7b\u5668\u5b98\u7814\u7a76\u3001\u836f\u7269\u7b5b\u9009\u3001\u80bf\u7624\u6cbb\u7597\u3001\u53d1\u80b2\u751f\u7269\u5b66\u3001\u518d\u751f\u533b\u5b66\u3001\u75be\u75c5\u5efa\u6a21\u3002 \u9ad8\u901a\u91cf\u7b5b\u9009\uff1a \u81ea\u52a8\u5316\u5206\u6790\u80fd\u529b\u4f7f\u5176\u975e\u5e38\u9002\u5408\u5927\u89c4\u6a21\u7684\u836f\u7269\u6216\u57fa\u56e0\u529f\u80fd\u7b5b\u9009\uff0c\u63d0\u9ad8\u6548\u7387\u548c\u51c6\u786e\u6027\u3002 \u901a\u7528\u751f\u7269\u56fe\u50cf\u5206\u6790\uff1a \u4efb\u4f55\u9700\u8981\u5bf9\u590d\u6742\u3001\u5f62\u6001\u591a\u53d8\u7684\u751f\u7269\u7ed3\u6784\uff08\u5982\u7ec6\u80de\u7fa4\u3001\u7ec4\u7ec7\u5207\u7247\u3001\u5fae\u6d41\u63a7\u82af\u7247\u4e2d\u7684\u751f\u7269\u6837\u672c\uff09\u8fdb\u884c\u9c81\u68d2\u5206\u5272\u548c\u8ffd\u8e2a\u7684\u573a\u666f\uff0c\u90fd\u53ef\u80fd\u4ece\u5176\u878d\u5408\u67b6\u6784\u548c\u7279\u5f81\u5b66\u4e60\u7b56\u7565\u4e2d\u53d7\u76ca\u3002 \u663e\u5fae\u56fe\u50cf\u5904\u7406\uff1a \u5bf9\u4e8e\u5176\u4ed6\u9700\u8981\u4ece\u590d\u6742\u80cc\u666f\u4e2d\u7cbe\u786e\u63d0\u53d6\u76ee\u6807\u5e76\u8ffd\u8e2a\u5176\u52a8\u6001\u53d8\u5316\u7684\u663e\u5fae\u56fe\u50cf\u5206\u6790\u4efb\u52a1\uff0c\u8be5\u65b9\u6cd5\u4e5f\u53ef\u80fd\u5177\u6709\u501f\u9274\u610f\u4e49\u3002 5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u51fa\u7684\u5c40\u9650\u6027 \u6027\u80fd\u91cf\u5316\u4e0e\u6bd4\u8f83\uff1a \u6458\u8981\u4e2d\u4ec5\u63d0\u53ca\u201c\u4ee4\u4eba\u6ee1\u610f\u7684\u5206\u5272\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u201d\uff0c\u4f46\u7f3a\u4e4f\u4e0e\u73b0\u6709SOTA\u65b9\u6cd5\u8fdb\u884c\u5b9a\u91cf\u6bd4\u8f83\u7684\u5177\u4f53\u6570\u636e\uff0c\u56e0\u6b64\u5176\u76f8\u5bf9\u6027\u80fd\u4f18\u52bf\u5c1a\u4e0d\u660e\u786e\u3002 \u6cdb\u5316\u80fd\u529b\uff1a \u6a21\u578b\u5728\u201c\u7c7b\u5668\u5b98\u5206\u5272\u6570\u636e\u96c6\u201d\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u4f46\u5176\u5bf9\u4e0d\u540c\u7c7b\u578b\u7c7b\u5668\u5b98\uff08\u5982\u4e0d\u540c\u5668\u5b98\u6765\u6e90\u3001\u4e0d\u540c\u57f9\u517b\u6761\u4ef6\uff09\u3001\u4e0d\u540c\u6210\u50cf\u6a21\u6001\u6216\u66f4\u590d\u6742\u75c5\u7406\u60c5\u51b5\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u6709\u5f85\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u3002 \u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\uff1a \u7ed3\u5408CNN\u548cTransformer\u7684\u6df7\u5408\u67b6\u6784\uff0c\u4ee5\u53ca\u53ef\u5b66\u4e60\u7684\u9ad8\u65af\u5e26\u901a\u878d\u5408\u6a21\u5757\u548c\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\uff0c\u53ef\u80fd\u610f\u5473\u7740\u8f83\u9ad8\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u5185\u5b58\u6d88\u8017\uff0c\u8fd9\u5728\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u4e2d\u53ef\u80fd\u662f\u4e00\u4e2a\u8003\u91cf\u3002 \u201cSROrga\u201d\u7684\u6307\u4ee3\uff1a \u6458\u8981\u4e2d\u63d0\u5230\u201cSROrga demonstrates...\u201d\uff0c\u4f46\u6a21\u578b\u540d\u79f0\u662fLGBP-OrgaNet\uff0c\u8fd9\u53ef\u80fd\u662f\u7b14\u8bef\u6216\u6307\u4ee3\u4e0d\u660e\u786e\uff0c\u9700\u8981\u6f84\u6e05\u3002 Key Findings: Additionally, in the decoder, the model proposes a Bidirectional Cross Fusion Block to fuse multi-scale features, and finally completes the decoding through progressive concatenation and upsampling. SROrga demonstrates satisfactory segmentation accuracy and robustness on organoids segmentation datasets, providing a potent tool for organoid research. Links: PDF arXiv Towards Realistic Hand-Object Interaction with Gravity-Field Based Diffusion Bridge Authors: Miao Xu, Xiangyu Zhu, Xusheng Liang, Zidu Wang, Jinlin Wu, Zhen Lei Published: 2025-09-03 Categories: cs.CV Abstract: Existing reconstruction or hand-object pose estimation methods are capable of producing coarse interaction states. However, due to the complex and diverse geometry of both human hands and objects, these approaches often suffer from interpenetration or leave noticeable gaps in regions that are supposed to be in contact. Moreover, the surface of a real human hand undergoes non-negligible deformations during interaction, which are difficult to capture and represent with previous methods. To tackle these challenges, we formulate hand-object interaction as an attraction-driven process and propose a Gravity-Field Based Diffusion Bridge (GravityDB) to simulate interactions between a deformable hand surface and rigid objects. Our approach effectively resolves the aforementioned issues by generating physically plausible interactions that are free of interpenetration, ensure stable grasping, and capture realistic hand deformations. Furthermore, we incorporate semantic information from textual descriptions to guide the construction of the gravitational field, enabling more semantically meaningful interaction regions. Extensive qualitative and quantitative experiments on multiple datasets demonstrate the effectiveness of our method. Analysis: \u8fd9\u7bc7\u8bba\u6587\u300aTowards Realistic Hand-Object Interaction with Gravity-Field Based Diffusion Bridge\u300b\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u51b3\u624b\u90e8\u4e0e\u7269\u4f53\u4ea4\u4e92\u4e2d\u5e38\u89c1\u6311\u6218\u7684\u65b0\u65b9\u6cd5\u3002 1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u6d01\u603b\u7ed3 \u73b0\u6709\u624b\u90e8-\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u51e0\u4f55\u5f62\u72b6\u65f6\uff0c\u5e38\u5bfc\u81f4\u7269\u4f53\u7a7f\u900f\u6216\u63a5\u89e6\u533a\u57df\u51fa\u73b0\u660e\u663e\u95f4\u9699\uff0c\u4e14\u96be\u4ee5\u6355\u6349\u624b\u90e8\u5728\u4ea4\u4e92\u8fc7\u7a0b\u4e2d\u7684\u5f62\u53d8\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f15\u529b\u573a\u7684\u6269\u6563\u6865\uff08GravityDB\uff09\u6a21\u578b\uff0c\u5c06\u624b\u90e8-\u7269\u4f53\u4ea4\u4e92\u5efa\u6a21\u4e3a\u4e00\u79cd\u5438\u5f15\u9a71\u52a8\u8fc7\u7a0b\uff0c\u80fd\u591f\u751f\u6210\u65e0\u7a7f\u900f\u3001\u7a33\u5b9a\u6293\u53d6\u4e14\u6355\u6349\u771f\u5b9e\u624b\u90e8\u5f62\u53d8\u7684\u7269\u7406\u5408\u7406\u4ea4\u4e92\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u8fd8\u878d\u5165\u4e86\u6587\u672c\u63cf\u8ff0\u7684\u8bed\u4e49\u4fe1\u606f\u6765\u6307\u5bfc\u5f15\u529b\u573a\u6784\u5efa\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u5177\u8bed\u4e49\u610f\u4e49\u7684\u4ea4\u4e92\u533a\u57df\u3002 2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u8bba \u8be5\u8bba\u6587\u7684\u5173\u952e\u521b\u65b0\u5728\u4e8e\uff1a 1. \u5f15\u529b\u573a\u9a71\u52a8\u7684\u6269\u6563\u6865\u6a21\u578b (GravityDB) \uff1a\u5c06\u624b\u90e8-\u7269\u4f53\u4ea4\u4e92\u5efa\u6a21\u4e3a\u4e00\u79cd\u65b0\u9896\u7684\u201c\u5438\u5f15\u9a71\u52a8\u8fc7\u7a0b\u201d\uff0c\u5e76\u5229\u7528\u201c\u6269\u6563\u6865\u201d\u6846\u67b6\u6765\u6a21\u62df\u53ef\u53d8\u5f62\u624b\u90e8\u8868\u9762\u4e0e\u521a\u6027\u7269\u4f53\u4e4b\u95f4\u7684\u4ea4\u4e92\u3002\u8fd9\u76f4\u63a5\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u7269\u4f53\u7a7f\u900f\u3001\u95f4\u9699\u4ee5\u53ca\u624b\u90e8\u5f62\u53d8\u96be\u4ee5\u6355\u6349\u7684\u95ee\u9898\u3002 2. \u53ef\u53d8\u5f62\u624b\u90e8\u8868\u9762\u5efa\u6a21 \uff1a\u660e\u786e\u63d0\u51fa\u5e76\u89e3\u51b3\u4e86\u624b\u90e8\u5728\u4ea4\u4e92\u8fc7\u7a0b\u4e2d\u975e\u7ebf\u6027\u5f62\u53d8\u7684\u95ee\u9898\uff0c\u8fd9\u662f\u73b0\u6709\u65b9\u6cd5\u7684\u4e00\u4e2a\u4e3b\u8981\u75db\u70b9\u3002 3. \u8bed\u4e49\u4fe1\u606f\u878d\u5408 \uff1a\u521b\u65b0\u6027\u5730\u5c06\u6587\u672c\u63cf\u8ff0\u4e2d\u7684\u8bed\u4e49\u4fe1\u606f\u878d\u5165\u5230\u5f15\u529b\u573a\u7684\u6784\u5efa\u4e2d\uff0c\u4f7f\u5f97\u751f\u6210\u7684\u4ea4\u4e92\u4e0d\u4ec5\u5728\u7269\u7406\u4e0a\u5408\u7406\uff0c\u800c\u4e14\u5728\u8bed\u4e49\u4e0a\u4e5f\u66f4\u5177\u610f\u4e49\uff0c\u4f8b\u5982\uff0c\u6839\u636e\u6587\u672c\u63cf\u8ff0\u201c\u6293\u4f4f\u676f\u67c4\u201d\u800c\u975e\u201c\u6293\u4f4f\u676f\u8eab\u201d\u3002 3. \u5bf9\u8be5\u9886\u57df\u7684\u6f5c\u5728\u5f71\u54cd \u63d0\u5347\u4ea4\u4e92\u771f\u5b9e\u611f\u548c\u7269\u7406\u5408\u7406\u6027 \uff1a\u901a\u8fc7\u89e3\u51b3\u7a7f\u900f\u3001\u95f4\u9699\u548c\u624b\u90e8\u5f62\u53d8\u95ee\u9898\uff0c\u8be5\u65b9\u6cd5\u5c06\u663e\u8457\u63d0\u9ad8\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u56fe\u5f62\u5b66\u4e2d\u624b\u90e8-\u7269\u4f53\u4ea4\u4e92\u7684\u771f\u5b9e\u611f\u548c\u7269\u7406\u53ef\u4fe1\u5ea6\uff0c\u4e3a\u66f4\u9ad8\u7ea7\u7684\u4ea4\u4e92\u5efa\u6a21\u5960\u5b9a\u57fa\u7840\u3002 \u63a8\u52a8\u591a\u6a21\u6001\u4ea4\u4e92\u7406\u89e3 \uff1a\u7ed3\u5408\u6587\u672c\u8bed\u4e49\u4fe1\u606f\u6765\u6307\u5bfc\u7269\u7406\u4ea4\u4e92\uff0c\u4e3a\u624b\u90e8-\u7269\u4f53\u4ea4\u4e92\u7814\u7a76\u5f15\u5165\u4e86\u591a\u6a21\u6001\u7684\u89c6\u89d2\uff0c\u53ef\u80fd\u542f\u53d1\u66f4\u591a\u7ed3\u5408\u8bed\u8a00\u7406\u89e3\u7684\u5177\u8eab\u667a\u80fd\u7814\u7a76\u3002 \u4e3a\u4e0b\u6e38\u4efb\u52a1\u63d0\u4f9b\u66f4\u9c81\u68d2\u7684\u8f93\u5165 \uff1a\u751f\u6210\u7684\u9ad8\u8d28\u91cf\u3001\u65e0\u7a7f\u900f\u3001\u5f62\u53d8\u51c6\u786e\u7684\u4ea4\u4e92\u72b6\u6001\uff0c\u53ef\u4ee5\u4f5c\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u3001\u865a\u62df\u73b0\u5b9e\u3001\u52a8\u753b\u5236\u4f5c\u7b49\u4e0b\u6e38\u4efb\u52a1\u66f4\u53ef\u9760\u7684\u8f93\u5165\u6570\u636e\u6216\u8bad\u7ec3\u76ee\u6807\u3002 4. \u53ef\u80fd\u53d7\u76ca\u7684\u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 \u673a\u5668\u4eba\u5b66\u4e0e\u7075\u5de7\u64cd\u4f5c \uff1a\u4e3a\u673a\u5668\u4eba\u5b66\u4e60\u548c\u6267\u884c\u590d\u6742\u3001\u7cbe\u7ec6\u7684\u6293\u53d6\u548c\u64cd\u4f5c\u4efb\u52a1\u63d0\u4f9b\u66f4\u771f\u5b9e\u7684\u6a21\u62df\u73af\u5883\u548c\u8bad\u7ec3\u6570\u636e\u3002 \u865a\u62df\u73b0\u5b9e (VR) / \u589e\u5f3a\u73b0\u5b9e (AR) \uff1a\u63d0\u5347\u865a\u62df\u73af\u5883\u4e2d\u7528\u6237\u624b\u90e8\u4e0e\u865a\u62df\u7269\u4f53\u4ea4\u4e92\u7684\u6c89\u6d78\u611f\u548c\u771f\u5b9e\u611f\uff0c\u51cf\u5c11\u4e0d\u81ea\u7136\u7684\u7a7f\u900f\u73b0\u8c61\u3002 \u8ba1\u7b97\u673a\u56fe\u5f62\u5b66\u4e0e\u52a8\u753b \uff1a\u7528\u4e8e\u751f\u6210\u7535\u5f71\u3001\u6e38\u620f\u548c\u5176\u4ed6\u5a92\u4f53\u4e2d\u66f4\u903c\u771f\u3001\u66f4\u81ea\u7136\u7684\u865a\u62df\u89d2\u8272\u624b\u90e8\u4e0e\u7269\u4f53\u7684\u4ea4\u4e92\u52a8\u753b\u3002 \u4eba\u673a\u4ea4\u4e92 (HCI) \uff1a\u5f00\u53d1\u66f4\u667a\u80fd\u3001\u66f4\u76f4\u89c2\u7684\u57fa\u4e8e\u624b\u52bf\u7684\u4ea4\u4e92\u7cfb\u7edf\uff0c\u7406\u89e3\u7528\u6237\u610f\u56fe\u3002 3D \u5185\u5bb9\u521b\u4f5c \uff1a\u4e3a\u827a\u672f\u5bb6\u548c\u8bbe\u8ba1\u5e08\u63d0\u4f9b\u5de5\u5177\uff0c\u4ee5\u66f4\u9ad8\u6548\u5730\u521b\u5efa\u9ad8\u8d28\u91cf\u7684\u624b\u90e8-\u7269\u4f53\u4ea4\u4e92\u573a\u666f\u3002 \u624b\u90e8\u59ff\u6001\u4f30\u8ba1\u4e0e\u91cd\u5efa \uff1a\u4f5c\u4e3a\u540e\u5904\u7406\u6b65\u9aa4\u6216\u4f5c\u4e3a\u8bad\u7ec3\u6570\u636e\u589e\u5f3a\uff0c\u4ee5\u63d0\u9ad8\u73b0\u6709\u65b9\u6cd5\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002 5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u7684\u5c40\u9650\u6027 \u5bf9\u8c61\u521a\u6027\u9650\u5236 \uff1a\u6458\u8981\u660e\u786e\u6307\u51fa\u662f\u201c\u53ef\u53d8\u5f62\u624b\u90e8\u8868\u9762\u548c\u521a\u6027\u7269\u4f53\u201d\u4e4b\u95f4\u7684\u4ea4\u4e92\u3002\u8fd9\u610f\u5473\u7740\u8be5\u65b9\u6cd5\u53ef\u80fd\u4e0d\u9002\u7528\u4e8e\u5904\u7406\u53ef\u53d8\u5f62\u7269\u4f53\uff08\u4f8b\u5982\uff0c\u6324\u538b\u6d77\u7ef5\u3001\u63c9\u634f\u9762\u56e2\uff09\u7684\u4ea4\u4e92\u573a\u666f\u3002 \u8ba1\u7b97\u6210\u672c \uff1a\u6269\u6563\u6a21\u578b\u901a\u5e38\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u9636\u6bb5\u90fd\u5177\u6709\u8f83\u9ad8\u7684\u8ba1\u7b97\u6210\u672c\u3002\u867d\u7136\u6458\u8981\u672a\u76f4\u63a5\u63d0\u53ca\uff0c\u4f46\u201c\u6269\u6563\u6865\u201d\u6a21\u578b\u53ef\u80fd\u610f\u5473\u7740\u5728\u5b9e\u65f6\u5e94\u7528\u6216\u5927\u89c4\u6a21\u573a\u666f\u4e2d\u5b58\u5728\u8ba1\u7b97\u6548\u7387\u7684\u6311\u6218\u3002 \u8bed\u4e49\u4fe1\u606f\u4f9d\u8d56 \uff1a\u8bed\u4e49\u4fe1\u606f\u7684\u8d28\u91cf\u548c\u8986\u76d6\u8303\u56f4\u5c06\u76f4\u63a5\u5f71\u54cd\u5f15\u529b\u573a\u7684\u6784\u5efa\u3002\u5982\u679c\u6587\u672c\u63cf\u8ff0\u6a21\u7cca\u3001\u4e0d\u51c6\u786e\u6216\u7f3a\u4e4f\u7ec6\u8282\uff0c\u53ef\u80fd\u4f1a\u9650\u5236\u4ea4\u4e92\u7684\u7cbe\u786e\u6027\u548c\u591a\u6837\u6027\u3002 \u4ea4\u4e92\u590d\u6742\u6027 \uff1a\u867d\u7136\u89e3\u51b3\u4e86\u7a7f\u900f\u548c\u5f62\u53d8\uff0c\u4f46\u5bf9\u4e8e\u66f4\u590d\u6742\u7684\u4ea4\u4e92\uff0c\u4f8b\u5982\u591a\u6307\u534f\u540c\u64cd\u4f5c\u3001\u5de5\u5177\u4f7f\u7528\u6216\u52a8\u6001\u3001\u957f\u65f6\u95f4\u7684\u4ea4\u4e92\u5e8f\u5217\uff0c\u5176\u5904\u7406\u80fd\u529b\u4ecd\u9700\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u3002 \u6cdb\u5316\u80fd\u529b \uff1a\u5728\u201c\u591a\u4e2a\u6570\u636e\u96c6\u201d\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u4f46\u5176\u5bf9\u672a\u89c1\u8fc7\u7684\u65b0\u9896\u7269\u4f53\u51e0\u4f55\u5f62\u72b6\u6216\u4ea4\u4e92\u7c7b\u578b\u7684\u6cdb\u5316\u80fd\u529b\u5982\u4f55\uff0c\u4ecd\u6709\u5f85\u6df1\u5165\u63a2\u8ba8\u3002 Key Findings: Our approach effectively resolves the aforementioned issues by generating physically plausible interactions that are free of interpenetration, ensure stable grasping, and capture realistic hand deformations. Extensive qualitative and quantitative experiments on multiple datasets demonstrate the effectiveness of our method. Links: PDF arXiv Enhancing Zero-Shot Pedestrian Attribute Recognition with Synthetic Data Generation: A Comparative Study with Image-To-Image Diffusion Models Authors: Pablo Ayuso-Albizu, Juan C. SanMiguel, Pablo Carballeira Published: 2025-09-02 Categories: cs.CV Abstract: Pedestrian Attribute Recognition (PAR) involves identifying various human attributes from images with applications in intelligent monitoring systems. The scarcity of large-scale annotated datasets hinders the generalization of PAR models, specially in complex scenarios involving occlusions, varying poses, and diverse environments. Recent advances in diffusion models have shown promise for generating diverse and realistic synthetic images, allowing to expand the size and variability of training data. However, the potential of diffusion-based data expansion for generating PAR-like images remains underexplored. Such expansion may enhance the robustness and adaptability of PAR models in real-world scenarios. This paper investigates the effectiveness of diffusion models in generating synthetic pedestrian images tailored to PAR tasks. We identify key parameters of img2img diffusion-based data expansion; including text prompts, image properties, and the latest enhancements in diffusion-based data augmentation, and examine their impact on the quality of generated images for PAR. Furthermore, we employ the best-performing expansion approach to generate synthetic images for training PAR models, by enriching the zero-shot datasets. Experimental results show that prompt alignment and image properties are critical factors in image generation, with optimal selection leading to a 4.5% improvement in PAR recognition performance. Analysis: \u4f5c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u4e13\u5bb6\uff0c\u6211\u5bf9\u8fd9\u7bc7\u8bba\u6587\u6458\u8981\u8fdb\u884c\u5982\u4e0b\u5206\u6790\uff1a \u8bba\u6587\u6458\u8981\u5206\u6790\uff1aEnhancing Zero-Shot Pedestrian Attribute Recognition with Synthetic Data Generation: A Comparative Study with Image-To-Image Diffusion Models 1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u660e\u603b\u7ed3 (2-3 \u53e5\u8bdd) \u672c\u6587\u65e8\u5728\u89e3\u51b3\u884c\u4eba\u5c5e\u6027\u8bc6\u522b\uff08PAR\uff09\u4e2d\u6570\u636e\u7a00\u7f3a\uff0c\u5c24\u5176\u662f\u5728\u96f6\u6837\u672c\u573a\u666f\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u95ee\u9898\u3002\u4f5c\u8005\u63a2\u7d22\u4e86\u4f7f\u7528\u56fe\u50cf\u5230\u56fe\u50cf\uff08img2img\uff09\u6269\u6563\u6a21\u578b\u751f\u6210\u5408\u6210\u884c\u4eba\u56fe\u50cf\u7684\u6709\u6548\u6027\uff0c\u5e76\u7cfb\u7edf\u6027\u5730\u7814\u7a76\u4e86\u6587\u672c\u63d0\u793a\u3001\u56fe\u50cf\u5c5e\u6027\u7b49\u5173\u952e\u53c2\u6570\u5bf9\u751f\u6210\u8d28\u91cf\u7684\u5f71\u54cd\u3002\u901a\u8fc7\u5c06\u6700\u4f73\u751f\u6210\u65b9\u6cd5\u5e94\u7528\u4e8e\u96f6\u6837\u672c\u6570\u636e\u96c6\u7684\u6269\u5145\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660ePAR\u8bc6\u522b\u6027\u80fd\u83b7\u5f97\u4e864.5%\u7684\u63d0\u5347\u3002 2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u5b66\u65b9\u6cd5 \u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u7cfb\u7edf\u6027\u5730\u63a2\u7d22\u548c\u4f18\u5316\u56fe\u50cf\u5230\u56fe\u50cf\uff08img2img\uff09\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u9002\u7528\u4e8e\u884c\u4eba\u5c5e\u6027\u8bc6\u522b\uff08PAR\uff09\u4efb\u52a1\u7684\u5408\u6210\u6570\u636e\u65b9\u9762\u7684\u6f5c\u529b\u3002\u8bba\u6587\u8be6\u7ec6\u7814\u7a76\u4e86\u6587\u672c\u63d0\u793a\u3001\u56fe\u50cf\u5c5e\u6027\u4ee5\u53ca\u6269\u6563\u6a21\u578b\u6700\u65b0\u589e\u5f3a\u529f\u80fd\u7b49\u5173\u952e\u53c2\u6570\u5bf9\u751f\u6210\u56fe\u50cf\u8d28\u91cf\u7684\u5f71\u54cd\uff0c\u5e76\u9996\u6b21\u5c06\u8fd9\u79cd\u4f18\u5316\u540e\u7684\u5408\u6210\u6570\u636e\u751f\u6210\u65b9\u6cd5\u5e94\u7528\u4e8e\u96f6\u6837\u672cPAR\u6570\u636e\u96c6\u7684\u6269\u5145\uff0c\u4ee5\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002\u8fd9\u79cd\u9488\u5bf9\u7279\u5b9a\u4e0b\u6e38\u4efb\u52a1\uff08PAR\uff09\u7684\u751f\u6210\u53c2\u6570\u8c03\u4f18\u548c\u6570\u636e\u6269\u5145\u7b56\u7565\u662f\u5176\u65b9\u6cd5\u5b66\u7684\u4eae\u70b9\u3002 3. \u5bf9\u8be5\u9886\u57df\u7684\u6f5c\u5728\u5f71\u54cd \u8fd9\u9879\u7814\u7a76\u4e3a\u89e3\u51b3\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u4e2d\u666e\u904d\u5b58\u5728\u7684\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u662f\u5728\u884c\u4eba\u5c5e\u6027\u8bc6\u522b\uff08PAR\uff09\u7b49\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\u7684\u7ec6\u7c92\u5ea6\u8bc6\u522b\u4efb\u52a1\u4e2d\u3002\u901a\u8fc7\u5229\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u9ad8\u8d28\u91cf\u5408\u6210\u6570\u636e\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8PAR\u6a21\u578b\u5728\u590d\u6742\u771f\u5b9e\u573a\u666f\uff08\u5982\u906e\u6321\u3001\u59ff\u6001\u53d8\u5316\u3001\u591a\u6837\u73af\u5883\uff09\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\uff0c\u4ece\u800c\u63a8\u52a8\u667a\u80fd\u76d1\u63a7\u7cfb\u7edf\u7b49\u5e94\u7528\u7684\u53d1\u5c55\u3002\u6b64\u5916\uff0c\u5176\u65b9\u6cd5\u8bba\u53ef\u80fd\u4e3a\u5176\u4ed6\u6570\u636e\u53d7\u9650\u7684\u7ec6\u7c92\u5ea6\u8bc6\u522b\u6216\u96f6\u6837\u672c\u5b66\u4e60\u4efb\u52a1\u63d0\u4f9b\u501f\u9274\u3002 4. \u53ef\u80fd\u53d7\u76ca\u4e8e\u8fd9\u9879\u7814\u7a76\u7684\u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 * \u667a\u80fd\u76d1\u63a7\u7cfb\u7edf (Intelligent Monitoring Systems): \u76f4\u63a5\u5e94\u7528\uff0c\u63d0\u5347\u884c\u4eba\u5206\u6790\u80fd\u529b\uff0c\u5982\u5f02\u5e38\u884c\u4e3a\u68c0\u6d4b\u3001\u4eba\u5458\u68c0\u7d22\u7b49\u3002 * \u96f6\u6837\u672c/\u5c11\u6837\u672c\u5b66\u4e60 (Zero-shot/Few-shot Learning): \u4e3a\u6570\u636e\u7a00\u7f3a\u7684\u573a\u666f\u63d0\u4f9b\u6709\u6548\u7684\u6570\u636e\u6269\u5145\u7b56\u7565\uff0c\u964d\u4f4e\u5bf9\u5927\u91cf\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\u3002 * \u7ec6\u7c92\u5ea6\u56fe\u50cf\u8bc6\u522b (Fine-grained Image Recognition): \u4efb\u4f55\u9700\u8981\u8bc6\u522b\u5fae\u5c0f\u5dee\u5f02\u4f46\u6570\u636e\u91cf\u6709\u9650\u7684\u4efb\u52a1\uff0c\u5982\u52a8\u7269\u79cd\u7c7b\u8bc6\u522b\u3001\u4ea7\u54c1\u7f3a\u9677\u68c0\u6d4b\u3001\u65f6\u5c1a\u5546\u54c1\u5206\u7c7b\u7b49\u3002 * \u6570\u636e\u589e\u5f3a\u4e0e\u5408\u6210\u6570\u636e\u751f\u6210 (Data Augmentation and Synthetic Data Generation): \u4e3a\u8bad\u7ec3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u63d0\u4f9b\u66f4\u4e30\u5bcc\u3001\u591a\u6837\u7684\u6570\u636e\uff0c\u51cf\u5c11\u8fc7\u62df\u5408\u3002 * \u9886\u57df\u9002\u5e94 (Domain Adaptation): \u5408\u6210\u6570\u636e\u53ef\u7528\u4e8e\u5f25\u5408\u4e0d\u540c\u6570\u636e\u5206\u5e03\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u63d0\u9ad8\u6a21\u578b\u5728\u4e0d\u540c\u73af\u5883\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002 * \u9690\u79c1\u4fdd\u62a4AI (Privacy-preserving AI): \u5728\u67d0\u4e9b\u573a\u666f\u4e0b\uff0c\u5408\u6210\u6570\u636e\u53ef\u4ee5\u66ff\u4ee3\u654f\u611f\u7684\u771f\u5b9e\u6570\u636e\u8fdb\u884c\u6a21\u578b\u8bad\u7ec3\uff0c\u4ece\u800c\u4fdd\u62a4\u7528\u6237\u9690\u79c1\u3002 5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u51fa\u7684\u5c40\u9650\u6027 * \u6027\u80fd\u63d0\u5347\u5e45\u5ea6\uff1a 4.5%\u7684\u6027\u80fd\u63d0\u5347\u867d\u7136\u662f\u79ef\u6781\u7684\uff0c\u4f46\u53ef\u80fd\u8868\u660e\u5408\u6210\u6570\u636e\u5e76\u975e\u89e3\u51b3PAR\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u7684\u7ec8\u6781\u65b9\u6848\uff0c\u4ecd\u6709\u8fdb\u4e00\u6b65\u63d0\u5347\u7684\u7a7a\u95f4\uff0c\u6216\u8005\u9700\u8981\u7ed3\u5408\u5176\u4ed6\u6280\u672f\u3002 * \u53c2\u6570\u63a2\u7d22\u8303\u56f4\uff1a \u8bba\u6587\u867d\u7136\u63a2\u7d22\u4e86\u6587\u672c\u63d0\u793a\u548c\u56fe\u50cf\u5c5e\u6027\u7b49\u5173\u952e\u53c2\u6570\uff0c\u4f46\u6269\u6563\u6a21\u578b\u5177\u6709\u590d\u6742\u7684\u5185\u90e8\u673a\u5236\u548c\u4f17\u591a\u8d85\u53c2\u6570\uff0c\u6458\u8981\u4e2d\u672a\u660e\u786e\u8bf4\u660e\u662f\u5426\u5df2\u7a77\u5c3d\u6240\u6709\u5f71\u54cd\u751f\u6210\u8d28\u91cf\u548cPAR\u6027\u80fd\u7684\u5173\u952e\u56e0\u7d20\u3002 * \u751f\u6210\u6570\u636e\u8d28\u91cf\u7684\u8bc4\u4f30\uff1a \u6458\u8981\u4e2d\u63d0\u5230\u201c\u751f\u6210\u56fe\u50cf\u7684\u8d28\u91cf\u201d\uff0c\u4f46\u672a\u8be6\u7ec6\u8bf4\u660e\u9664\u4e86\u6700\u7ec8\u7684PAR\u8bc6\u522b\u6027\u80fd\u5916\uff0c\u5982\u4f55\u5177\u4f53\u91cf\u5316\u548c\u8bc4\u4f30\u8fd9\u4e9b\u5408\u6210\u56fe\u50cf\u5728\u8bed\u4e49\u4e00\u81f4\u6027\u3001\u5c5e\u6027\u591a\u6837\u6027\u3001\u771f\u5b9e\u611f\u7b49\u65b9\u9762\u7684\u8d28\u91cf\u3002 * \u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\uff1a \u6269\u6563\u6a21\u578b\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\u901a\u5e38\u9700\u8981\u5927\u91cf\u7684\u8ba1\u7b97\u8d44\u6e90\u548c\u65f6\u95f4\uff0c\u8fd9\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u53ef\u80fd\u662f\u4e00\u4e2a\u9650\u5236\u3002\u6458\u8981\u4e2d\u672a\u63d0\u53ca\u751f\u6210\u6570\u636e\u7684\u6548\u7387\u95ee\u9898\u3002 * \u6f5c\u5728\u7684\u5408\u6210\u6570\u636e\u504f\u5dee\uff1a \u5408\u6210\u6570\u636e\u53ef\u80fd\u7ee7\u627f\u6216\u653e\u5927\u539f\u59cb\u8bad\u7ec3\u6570\u636e\u6216\u6269\u6563\u6a21\u578b\u81ea\u8eab\u9884\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u504f\u5dee\uff0c\u4ece\u800c\u5f71\u54cdPAR\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u516c\u5e73\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002 * \u4ec5\u9650\u4e8e\u96f6\u6837\u672c\u573a\u666f\uff1a \u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u96f6\u6837\u672cPAR\u6570\u636e\u96c6\u7684\u6269\u5145\uff0c\u5176\u5728\u5168\u76d1\u7763\u6216\u5c11\u6837\u672cPAR\u573a\u666f\u4e0b\u7684\u6709\u6548\u6027\u672a\u5728\u6458\u8981\u4e2d\u63d0\u53ca\u3002 Key Findings: Experimental results show that prompt alignment and image properties are critical factors in image generation, with optimal selection leading to a 4.5% improvement in PAR recognition performance. Links: PDF arXiv MedLiteNet: Lightweight Hybrid Medical Image Segmentation Model Authors: Pengyang Yu, Haoquan Wang, Gerard Marks, Tahar Kechadi, Laurence T. Yang, Sahraoui Dhelim, Nyothiri Aung Published: 2025-09-03 Categories: cs.CV, cs.AI Abstract: Accurate skin-lesion segmentation remains a key technical challenge for computer-aided diagnosis of skin cancer. Convolutional neural networks, while effective, are constrained by limited receptive fields and thus struggle to model long-range dependencies. Vision Transformers capture global context, yet their quadratic complexity and large parameter budgets hinder use on the small-sample medical datasets common in dermatology. We introduce the MedLiteNet, a lightweight CNN Transformer hybrid tailored for dermoscopic segmentation that achieves high precision through hierarchical feature extraction and multi-scale context aggregation. The encoder stacks depth-wise Mobile Inverted Bottleneck blocks to curb computation, inserts a bottleneck-level cross-scale token-mixing unit to exchange information between resolutions, and embeds a boundary-aware self-attention module to sharpen lesion contours. Analysis: \u4f5c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u4e13\u5bb6\uff0c\u6211\u5bf9\u8fd9\u7bc7\u8bba\u6587\u6458\u8981\u8fdb\u884c\u5982\u4e0b\u5206\u6790\uff1a MedLiteNet: \u8f7b\u91cf\u7ea7\u6df7\u5408\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6a21\u578b 1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u660e\u603b\u7ed3 (2-3 \u53e5\u8bdd) \u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a MedLiteNet \u7684\u8f7b\u91cf\u7ea7 CNN-Transformer \u6df7\u5408\u6a21\u578b\uff0c\u4e13\u95e8\u7528\u4e8e\u76ae\u80a4\u955c\u56fe\u50cf\u7684\u76ae\u80a4\u75c5\u7076\u5206\u5272\u3002\u5b83\u65e8\u5728\u514b\u670d\u4f20\u7edf CNN \u5728\u957f\u8ddd\u79bb\u4f9d\u8d56\u5efa\u6a21\u4e0a\u7684\u5c40\u9650\u6027\u4ee5\u53ca Vision Transformer \u5728\u5c0f\u6837\u672c\u533b\u5b66\u6570\u636e\u96c6\u4e0a\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u53c2\u6570\u91cf\u8fc7\u5927\u7684\u95ee\u9898\u3002\u901a\u8fc7\u7ed3\u5408\u5206\u5c42\u7279\u5f81\u63d0\u53d6\u548c\u591a\u5c3a\u5ea6\u4e0a\u4e0b\u6587\u805a\u5408\uff0cMedLiteNet \u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u75c5\u7076\u5206\u5272\u3002 2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u5b66\u65b9\u6cd5 MedLiteNet \u7684\u5173\u952e\u521b\u65b0\u5728\u4e8e\u5176\u72ec\u7279\u7684\u6df7\u5408\u67b6\u6784\u8bbe\u8ba1\uff0c\u65e8\u5728\u5b9e\u73b0\u8f7b\u91cf\u5316\u548c\u9ad8\u7cbe\u5ea6\uff1a \u8f7b\u91cf\u7ea7\u7f16\u7801\u5668\u8bbe\u8ba1\uff1a \u7f16\u7801\u5668\u5806\u53e0\u4e86 \u6df1\u5ea6\u53ef\u5206\u79bb\u7684 Mobile Inverted Bottleneck Blocks (MobileNetV2 \u4e2d\u7684\u5012\u6b8b\u5dee\u5757) \uff0c\u4ee5\u6709\u6548\u6291\u5236\u8ba1\u7b97\u91cf\uff0c\u4f7f\u5176\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u548c\u5c0f\u6837\u672c\u6570\u636e\u96c6\u3002 \u8de8\u5c3a\u5ea6\u4ee4\u724c\u6df7\u5408\u5355\u5143 (Cross-scale Token-mixing Unit)\uff1a \u5728\u74f6\u9888\u5c42\uff08bottleneck-level\uff09\u63d2\u5165\u4e86 \u8de8\u5c3a\u5ea6\u4ee4\u724c\u6df7\u5408\u5355\u5143 \uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u673a\u5236\uff0c\u7528\u4e8e\u5728\u4e0d\u540c\u5206\u8fa8\u7387\u7684\u7279\u5f81\u4e4b\u95f4\u4ea4\u6362\u4fe1\u606f\u3002\u8fd9\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u6355\u83b7\u5168\u5c40\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u5f25\u8865\u4e86\u4f20\u7edf CNN \u611f\u53d7\u91ce\u6709\u9650\u7684\u7f3a\u70b9\uff0c\u540c\u65f6\u907f\u514d\u4e86\u6807\u51c6 Transformer \u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u3002 \u8fb9\u754c\u611f\u77e5\u81ea\u6ce8\u610f\u529b\u6a21\u5757 (Boundary-aware Self-attention Module)\uff1a \u6a21\u578b\u5d4c\u5165\u4e86 \u8fb9\u754c\u611f\u77e5\u81ea\u6ce8\u610f\u529b\u6a21\u5757 \uff0c\u4e13\u95e8\u7528\u4e8e\u9510\u5316\u75c5\u7076\u8f6e\u5ed3\u3002\u8fd9\u5bf9\u4e8e\u533b\u5b66\u56fe\u50cf\u5206\u5272\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u4e3a\u7cbe\u786e\u7684\u8fb9\u754c\u5bf9\u4e8e\u8bca\u65ad\u5177\u6709\u51b3\u5b9a\u6027\u610f\u4e49\u3002 3. \u5bf9\u8be5\u9886\u57df\u7684\u6f5c\u5728\u5f71\u54cd \u63d0\u5347\u76ae\u80a4\u764c\u8bca\u65ad\u7cbe\u5ea6\uff1a \u901a\u8fc7\u63d0\u4f9b\u66f4\u51c6\u786e\u3001\u66f4\u53ef\u9760\u7684\u76ae\u80a4\u75c5\u7076\u5206\u5272\u5de5\u5177\uff0cMedLiteNet \u6709\u671b\u663e\u8457\u63d0\u9ad8\u8ba1\u7b97\u673a\u8f85\u52a9\u8bca\u65ad\u76ae\u80a4\u764c\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u4ece\u800c\u53ef\u80fd\u6539\u5584\u60a3\u8005\u9884\u540e\u3002 \u63a8\u52a8\u8f7b\u91cf\u7ea7\u6df7\u5408\u6a21\u578b\u53d1\u5c55\uff1a \u8be5\u7814\u7a76\u4e3a\u5728\u8ba1\u7b97\u8d44\u6e90\u53d7\u9650\u548c\u6570\u636e\u7a00\u7f3a\u7684\u573a\u666f\u4e0b\uff0c\u6709\u6548\u7ed3\u5408 CNN \u7684\u5c40\u90e8\u7279\u5f81\u63d0\u53d6\u80fd\u529b\u548c Transformer \u7684\u5168\u5c40\u4e0a\u4e0b\u6587\u5efa\u6a21\u80fd\u529b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6210\u529f\u7684\u8303\u4f8b\u3002\u8fd9\u53ef\u80fd\u4f1a\u542f\u53d1\u66f4\u591a\u9488\u5bf9\u7279\u5b9a\u5e94\u7528\uff08\u5c24\u5176\u662f\u533b\u5b66\u5f71\u50cf\uff09\u7684\u8f7b\u91cf\u7ea7\u6df7\u5408\u67b6\u6784\u8bbe\u8ba1\u3002 \u89e3\u51b3\u533b\u5b66\u5f71\u50cf\u9886\u57df\u6311\u6218\uff1a \u6210\u529f\u89e3\u51b3\u4e86\u533b\u5b66\u5f71\u50cf\u9886\u57df\u5e38\u89c1\u7684\u201c\u5c0f\u6837\u672c\u6570\u636e\u201d\u548c\u201c\u957f\u8ddd\u79bb\u4f9d\u8d56\u201d\u4e4b\u95f4\u7684\u77db\u76fe\uff0c\u4e3a\u5176\u4ed6\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4efb\u52a1\uff08\u5982\u5668\u5b98\u5206\u5272\u3001\u80bf\u7624\u68c0\u6d4b\u7b49\uff09\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u3002 \u8fb9\u7f18\u8ba1\u7b97\u548c\u90e8\u7f72\uff1a \u5176\u8f7b\u91cf\u7ea7\u7279\u6027\u4f7f\u5176\u66f4\u6613\u4e8e\u90e8\u7f72\u5728\u4e34\u5e8a\u73af\u5883\u4e2d\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\uff0c\u5b9e\u73b0\u5b9e\u65f6\u6216\u8fd1\u5b9e\u65f6\u7684\u8bca\u65ad\u8f85\u52a9\u3002 4. \u53ef\u80fd\u53d7\u76ca\u4e8e\u8fd9\u9879\u7814\u7a76\u7684\u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 \u5176\u4ed6\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4efb\u52a1\uff1a \u4efb\u4f55\u9700\u8981\u7cbe\u786e\u5206\u5272\u3001\u5904\u7406\u5c0f\u6837\u672c\u6570\u636e\u96c6\u4e14\u5bf9\u8ba1\u7b97\u6548\u7387\u6709\u8981\u6c42\u7684\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4efb\u52a1\uff0c\u4f8b\u5982\uff1a CT/MRI \u56fe\u50cf\u4e2d\u7684\u5668\u5b98\u6216\u80bf\u7624\u5206\u5272\u3002 \u773c\u5e95\u56fe\u50cf\u4e2d\u7684\u89c6\u7f51\u819c\u8840\u7ba1\u5206\u5272\u3002 \u75c5\u7406\u5207\u7247\u4e2d\u7684\u7ec6\u80de\u6838\u6216\u7ec4\u7ec7\u533a\u57df\u5206\u5272\u3002 \u901a\u7528\u56fe\u50cf\u5206\u5272\uff1a \u5bf9\u4e8e\u5177\u6709\u590d\u6742\u8fb9\u754c\u3001\u9700\u8981\u540c\u65f6\u8003\u8651\u5c40\u90e8\u7ec6\u8282\u548c\u5168\u5c40\u4e0a\u4e0b\u6587\u7684\u901a\u7528\u56fe\u50cf\u5206\u5272\u4efb\u52a1\uff0c\u5c24\u5176\u662f\u5728\u6570\u636e\u96c6\u89c4\u6a21\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u3002 \u8f7b\u91cf\u7ea7\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u8bbe\u8ba1\uff1a \u5bf9\u5982\u4f55\u9ad8\u6548\u5730\u96c6\u6210\u4e0d\u540c\u7c7b\u578b\u795e\u7ecf\u7f51\u7edc\u6a21\u5757\uff08\u5982 CNN \u548c Transformer\uff09\u611f\u5174\u8da3\u7684\u7814\u7a76\u4eba\u5458\u3002 \u8ba1\u7b97\u673a\u8f85\u52a9\u8bca\u65ad\u7cfb\u7edf\uff1a \u4efb\u4f55\u65e8\u5728\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u6539\u8fdb\u8bca\u65ad\u6d41\u7a0b\u7684\u533b\u7597 AI \u5e94\u7528\u3002 5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u51fa\u7684\u5c40\u9650\u6027 \u7279\u5b9a\u9886\u57df\u6027\uff1a \u6458\u8981\u660e\u786e\u6307\u51fa\u6a21\u578b\u662f\u201c\u4e3a\u76ae\u80a4\u955c\u5206\u5272\u91cf\u8eab\u5b9a\u5236\u7684 (tailored for dermoscopic segmentation)\u201d\u3002\u8fd9\u610f\u5473\u7740\u5176\u5728\u5176\u4ed6\u533b\u5b66\u56fe\u50cf\u6a21\u6001\uff08\u5982 CT\u3001MRI\uff09\u6216\u901a\u7528\u56fe\u50cf\u5206\u5272\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u53ef\u80fd\u9700\u8981\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u548c\u8c03\u6574\u3002\u7279\u522b\u662f\u5176\u201c\u8fb9\u754c\u611f\u77e5\u81ea\u6ce8\u610f\u529b\u6a21\u5757\u201d\u53ef\u80fd\u9488\u5bf9\u76ae\u80a4\u75c5\u7076\u7684\u7279\u5b9a\u5f62\u6001\u8fdb\u884c\u4e86\u4f18\u5316\u3002 \u6027\u80fd\u91cf\u5316\u7f3a\u5931\uff1a \u6458\u8981\u4e2d\u63d0\u5230\u201c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6 (achieves high precision)\u201d\uff0c\u4f46\u6ca1\u6709\u63d0\u4f9b\u5177\u4f53\u7684\u91cf\u5316\u6307\u6807\uff08\u5982 Dice \u7cfb\u6570\u3001IoU\u3001\u51c6\u786e\u7387\u7b49\uff09\uff0c\u56e0\u6b64\u65e0\u6cd5\u8bc4\u4f30\u5176\u76f8\u5bf9\u4e8e\u73b0\u6709 SOTA \u65b9\u6cd5\u7684\u5b9e\u9645\u63d0\u5347\u5e45\u5ea6\u3002 \u6cdb\u5316\u80fd\u529b\uff1a \u5c3d\u7ba1\u65e8\u5728\u89e3\u51b3\u5c0f\u6837\u672c\u95ee\u9898\uff0c\u4f46\u201c\u5c0f\u6837\u672c\u533b\u5b66\u6570\u636e\u96c6\u201d\u7684\u5b9a\u4e49\u76f8\u5bf9\u5bbd\u6cdb\u3002\u6a21\u578b\u5728\u9762\u5bf9\u6781\u5ea6\u7a00\u7f3a\u6216\u9ad8\u5ea6\u5f02\u8d28\u6027\u7684\u6570\u636e\u96c6\u65f6\u7684\u6cdb\u5316\u80fd\u529b\u4ecd\u9700\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u3002 \u8bad\u7ec3\u590d\u6742\u6027\uff1a \u6df7\u5408\u67b6\u6784\uff0c\u5c24\u5176\u662f\u5305\u542b\u8de8\u5c3a\u5ea6\u4fe1\u606f\u4ea4\u6362\u548c\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684\uff0c\u901a\u5e38\u5728\u8bad\u7ec3\u4e0a\u53ef\u80fd\u6bd4\u7eaf\u7cb9\u7684 CNN \u66f4\u590d\u6742\uff0c\u53ef\u80fd\u9700\u8981\u66f4\u7cbe\u7ec6\u7684\u8d85\u53c2\u6570\u8c03\u4f18\u3002\u6458\u8981\u4e2d\u672a\u63d0\u53ca\u8bad\u7ec3\u7684\u96be\u5ea6\u6216\u8d44\u6e90\u9700\u6c42\u3002 \u8ba1\u7b97\u6548\u7387\u7684\u7edd\u5bf9\u503c\uff1a \u5c3d\u7ba1\u5f3a\u8c03\u201c\u8f7b\u91cf\u7ea7\u201d\u548c\u201c\u6291\u5236\u8ba1\u7b97\u91cf\u201d\uff0c\u4f46\u6ca1\u6709\u7ed9\u51fa\u5177\u4f53\u7684\u53c2\u6570\u91cf\u3001FLOPs \u6216\u63a8\u7406\u901f\u5ea6\u6570\u636e\uff0c\u56e0\u6b64\u65e0\u6cd5\u4e0e\u5176\u4ed6\u8f7b\u91cf\u7ea7\u6a21\u578b\u8fdb\u884c\u76f4\u63a5\u7684\u91cf\u5316\u6bd4\u8f83\u3002 Key Findings: We introduce the MedLiteNet, a lightweight CNN Transformer hybrid tailored for dermoscopic segmentation that achieves high precision through hierarchical feature extraction and multi-scale context aggregation. Links: PDF arXiv","title":"Arxiv Computer Vision Papers - 2025-09-04"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-04/#arxiv-computer-vision-papers-2025-09-04","text":"","title":"Arxiv Computer Vision Papers - 2025-09-04"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-04/#executive-summary","text":"","title":"Executive Summary"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-04/#arxiv-202593","text":"\u672c\u62a5\u544a\u603b\u7ed3\u4e862025\u5e749\u67083\u65e5\u53d1\u5e03\u768410\u7bc7Arxiv\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u8bba\u6587\uff0c\u65e8\u5728\u4e3a\u5fd9\u788c\u7684\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u5feb\u901f\u6982\u89c8\uff0c\u4e86\u89e3\u8be5\u9886\u57df\u7684\u6700\u65b0\u8fdb\u5c55\u548c\u5173\u952e\u8d8b\u52bf\u3002","title":"Arxiv \u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u6700\u65b0\u8bba\u6587\u6267\u884c\u6458\u8981 (2025\u5e749\u67083\u65e5)"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-04/#1","text":"\u672c\u6b21\u53d1\u5e03\u7684\u8bba\u6587\u5c55\u73b0\u4e86\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u4ee5\u4e0b\u51e0\u4e2a\u663e\u8457\u8d8b\u52bf\uff1a \u6269\u6563\u6a21\u578b (Diffusion Models) \u7684\u4e3b\u5bfc\u5730\u4f4d\uff1a \u6269\u6563\u6a21\u578b\u5728\u591a\u6837\u5316\u7684\u751f\u6210\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u80fd\u529b\uff0c\u4ece\u96f6\u6837\u672c\u6df1\u5ea6\u56fe\u6062\u590d (InfraDiffusion)\u3001\u89c6\u9891\u5408\u6210 (GenCompositor)\uff0c\u5230\u903c\u771f\u624b\u7269\u4ea4\u4e92\u5efa\u6a21 (Hand-Object Interaction) \u4ee5\u53ca\u901a\u8fc7\u5408\u6210\u6570\u636e\u589e\u5f3a\u96f6\u6837\u672c\u5b66\u4e60 (Pedestrian Attribute Recognition)\u3002 \u57fa\u7840\u6a21\u578b (Foundation Models) \u7684\u53d1\u5c55\u4e0e\u9886\u57df\u9002\u5e94\uff1a \u7814\u7a76\u91cd\u70b9\u5728\u4e8e\u5982\u4f55\u5c06\u5927\u578b\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u6709\u6548\u5e94\u7528\u4e8e\u7279\u5b9a\u9886\u57df\uff0c\u5982\u533b\u5b66\u56fe\u50cf\u5206\u5272 (MedDINOv3)\uff0c\u4ee5\u53ca\u6784\u5efa\u80fd\u591f\u7edf\u4e00\u7406\u89e3\u548c\u751f\u6210\u7684\u81ea\u56de\u5f52\u6a21\u578b (OneCAT)\u3002 \u533b\u5b66\u5f71\u50cf\u5206\u6790\u7684\u6301\u7eed\u521b\u65b0\uff1a \u591a\u4e2a\u5de5\u4f5c\u805a\u7126\u4e8e\u533b\u5b66\u56fe\u50cf\u5206\u5272\uff0c\u5f3a\u8c03\u4e86\u8f7b\u91cf\u5316 (MedLiteNet)\u3001\u9c81\u68d2\u6027 (LGBP-OrgaNet) \u548c\u57fa\u7840\u6a21\u578b\u5728\u8be5\u9886\u57df\u7684\u5e94\u7528\u3002 \u6a21\u578b\u6548\u7387\u4e0e\u4f18\u5316\uff1a \u9488\u5bf9Vision Transformer\u7684\u6548\u7387\u63d0\u5347 (TinyDrop) \u548c\u8f7b\u91cf\u7ea7\u6a21\u578b\u8bbe\u8ba1 (MedLiteNet) \u4ecd\u7136\u662f\u91cd\u8981\u7684\u7814\u7a76\u65b9\u5411\uff0c\u4ee5\u9002\u5e94\u5b9e\u9645\u90e8\u7f72\u9700\u6c42\u3002 \u6df7\u5408\u67b6\u6784\u7684\u878d\u5408\uff1a CNN\u4e0eTransformer\u7684\u7ed3\u5408 (LGBP-OrgaNet, Watermarking, MedLiteNet) \u6301\u7eed\u88ab\u63a2\u7d22\uff0c\u4ee5\u671f\u7ed3\u5408\u4e24\u8005\u7684\u4f18\u52bf\uff0c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002","title":"1. \u4e3b\u8981\u4e3b\u9898\u4e0e\u8d8b\u52bf\u6982\u89c8"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-04/#2","text":"OneCAT: Decoder-Only Auto-Regressive Model for Unified Understanding and Generation: \u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u7801\u5668-only\u7684\u81ea\u56de\u5f52\u6a21\u578b\uff0c\u65e8\u5728\u5b9e\u73b0\u7edf\u4e00\u7684\u7406\u89e3\u548c\u751f\u6210\u80fd\u529b\uff0c\u9884\u793a\u7740\u901a\u7528\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u65b0\u8303\u5f0f\uff0c\u5177\u6709\u5de8\u5927\u7684\u6f5c\u529b\u3002 InfraDiffusion: zero-shot depth map restoration with diffusion models and prompted segmentation from sparse infrastructure point clouds: \u8be5\u5de5\u4f5c\u5229\u7528\u6269\u6563\u6a21\u578b\u548c\u63d0\u793a\u5206\u5272\uff0c\u5b9e\u73b0\u4e86\u4ece\u7a00\u758f\u57fa\u7840\u8bbe\u65bd\u70b9\u4e91\u8fdb\u884c\u96f6\u6837\u672c\u6df1\u5ea6\u56fe\u6062\u590d\uff0c\u5728\u5b9e\u9645\u5de5\u7a0b\u548c3D\u91cd\u5efa\u9886\u57df\u5177\u6709\u91cd\u8981\u7684\u5e94\u7528\u4ef7\u503c\u548c\u521b\u65b0\u6027\u3002 MedDINOv3: How to adapt vision foundation models for medical image segmentation?: \u6df1\u5165\u63a2\u8ba8\u4e86\u5982\u4f55\u5c06\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u6709\u6548\u9002\u5e94\u4e8e\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4efb\u52a1\uff0c\u4e3a\u8be5\u9886\u57df\u7684\u57fa\u7840\u6a21\u578b\u5e94\u7528\u63d0\u4f9b\u4e86\u5173\u952e\u6307\u5bfc\u548c\u5b9e\u8df5\u7ecf\u9a8c\u3002 GenCompositor: Generative Video Compositing with Diffusion Transformer: \u91c7\u7528Diffusion Transformer\u8fdb\u884c\u751f\u6210\u5f0f\u89c6\u9891\u5408\u6210\uff0c\u5c55\u793a\u4e86\u5728\u590d\u6742\u89c6\u9891\u5185\u5bb9\u521b\u4f5c\u65b9\u9762\u7684\u5148\u8fdb\u6280\u672f\uff0c\u63a8\u52a8\u4e86\u89c6\u9891\u751f\u6210\u9886\u57df\u7684\u53d1\u5c55\u3002 Towards Realistic Hand-Object Interaction with Gravity-Field Based Diffusion Bridge: \u5f15\u5165\u4e86\u57fa\u4e8e\u91cd\u529b\u573a\u7684\u6269\u6563\u6865\uff0c\u4e3a\u5b9e\u73b0\u66f4\u903c\u771f\u3001\u7269\u7406\u4e00\u81f4\u7684\u624b\u7269\u4ea4\u4e92\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u9896\u65b9\u6cd5\uff0c\u5bf93D\u89c6\u89c9\u548c\u673a\u5668\u4eba\u9886\u57df\u5177\u6709\u542f\u53d1\u610f\u4e49\u3002","title":"2. \u663e\u8457\u6216\u521b\u65b0\u6027\u8bba\u6587\u4eae\u70b9"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-04/#3","text":"Diffusion Transformer (DiT) \u67b6\u6784\uff1a \u7ed3\u5408\u6269\u6563\u6a21\u578b\u548cTransformer\u7684\u5f3a\u5927\u80fd\u529b\uff0c\u5c24\u5176\u5728\u89c6\u9891\u751f\u6210\u7b49\u590d\u6742\u4efb\u52a1\u4e2d\u5c55\u73b0\u6f5c\u529b (GenCompositor)\u3002 \u7edf\u4e00\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\uff1a \u53d1\u5c55\u80fd\u591f\u5904\u7406\u591a\u6a21\u6001\u8f93\u5165\u5e76\u6267\u884c\u7406\u89e3\u4e0e\u751f\u6210\u4efb\u52a1\u7684\u5355\u4e00\u901a\u7528\u6a21\u578b\uff0c\u5411\u901a\u7528\u4eba\u5de5\u667a\u80fd\u8fc8\u8fdb (OneCAT)\u3002 \u7269\u7406\u4fe1\u606f\u5f15\u5bfc\u7684\u751f\u6210\u6a21\u578b\uff1a \u5c06\u7269\u7406\u7ea6\u675f\uff08\u5982\u91cd\u529b\u573a\uff09\u878d\u5165\u751f\u6210\u8fc7\u7a0b\uff0c\u4ee5\u63d0\u9ad8\u751f\u6210\u5185\u5bb9\u7684\u771f\u5b9e\u6027\u548c\u4e00\u81f4\u6027 (Hand-Object Interaction)\u3002 \u9ad8\u6548\u80fdTransformer\u8bbe\u8ba1\uff1a \u6301\u7eed\u4f18\u5316Transformer\u7ed3\u6784\uff0c\u4f7f\u5176\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u4e5f\u80fd\u9ad8\u6548\u8fd0\u884c\uff0c\u4f8b\u5982\u901a\u8fc7Token Dropping (TinyDrop)\u3002 \u5408\u6210\u6570\u636e\u5728\u96f6\u6837\u672c\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\uff1a \u5229\u7528\u751f\u6210\u6a21\u578b\uff08\u7279\u522b\u662f\u6269\u6563\u6a21\u578b\uff09\u521b\u5efa\u9ad8\u8d28\u91cf\u5408\u6210\u6570\u636e\uff0c\u4ee5\u5f25\u8865\u771f\u5b9e\u6570\u636e\u7a00\u7f3a\u6027\uff0c\u63d0\u5347\u96f6\u6837\u672c\u4efb\u52a1\u6027\u80fd (Pedestrian Attribute Recognition)\u3002","title":"3. \u65b0\u5174\u7814\u7a76\u65b9\u5411\u6216\u6280\u672f"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-04/#4","text":"\u4e3a\u4e86\u66f4\u6df1\u5165\u5730\u4e86\u89e3\u5f53\u524d\u9886\u57df\u7684\u91cd\u8981\u8fdb\u5c55\uff0c\u6211\u4eec\u5efa\u8bae\u60a8\u4f18\u5148\u9605\u8bfb\u4ee5\u4e0b\u8bba\u6587\uff1a OneCAT: \u5bf9\u4e8e\u5173\u6ce8\u901a\u7528AI\u548c\u57fa\u7840\u6a21\u578b\u672a\u6765\u53d1\u5c55\u7684\u7814\u7a76\u4eba\u5458\uff0c\u8fd9\u7bc7\u8bba\u6587\u63d0\u4f9b\u4e86\u5bf9\u7edf\u4e00\u7406\u89e3\u4e0e\u751f\u6210\u6a21\u578b\u7684\u65b0\u89c6\u89d2\u3002 InfraDiffusion: \u5bf9\u4e8e\u4ece\u4e8b3D\u89c6\u89c9\u3001\u91cd\u5efa\u548c\u6269\u6563\u6a21\u578b\u5b9e\u9645\u5e94\u7528\u7684\u7814\u7a76\u4eba\u5458\uff0c\u8be5\u5de5\u4f5c\u5c55\u793a\u4e86\u96f6\u6837\u672c\u6df1\u5ea6\u6062\u590d\u7684\u5f3a\u5927\u6f5c\u529b\u3002 MedDINOv3: \u5bf9\u4e8e\u533b\u5b66AI\u9886\u57df\uff0c\u7279\u522b\u662f\u5e0c\u671b\u5229\u7528\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u7814\u7a76\u4eba\u5458\uff0c\u8fd9\u7bc7\u8bba\u6587\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u9002\u5e94\u7b56\u7565\u3002 GenCompositor: \u5bf9\u4e8e\u89c6\u9891\u751f\u6210\u548c\u9ad8\u7ea7\u6269\u6563\u67b6\u6784\u611f\u5174\u8da3\u7684\u7814\u7a76\u4eba\u5458\uff0c\u8be5\u8bba\u6587\u5c55\u793a\u4e86Diffusion Transformer\u5728\u590d\u6742\u89c6\u9891\u5408\u6210\u4e2d\u7684\u5e94\u7528\u3002 Towards Realistic Hand-Object Interaction with Gravity-Field Based Diffusion Bridge: \u5bf9\u4e8e3D\u89c6\u89c9\u3001\u673a\u5668\u4eba\u548c\u65b0\u9896\u751f\u6210\u5efa\u6a21\u65b9\u6cd5\u7684\u7814\u7a76\u4eba\u5458\uff0c\u8be5\u8bba\u6587\u63d0\u4f9b\u4e86\u4e00\u79cd\u7ed3\u5408\u7269\u7406\u7ea6\u675f\u7684\u521b\u65b0\u65b9\u6cd5\u3002","title":"4. \u5efa\u8bae\u5b8c\u6574\u9605\u8bfb\u7684\u8bba\u6587"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-04/#table-of-contents","text":"InfraDiffusion: zero-shot depth map restoration with diffusion models and prompted segmentation from sparse infrastructure point clouds MedDINOv3: How to adapt vision foundation models for medical image segmentation? OneCAT: Decoder-Only Auto-Regressive Model for Unified Understanding and Generation GenCompositor: Generative Video Compositing with Diffusion Transformer Enhancing Robustness in Post-Processing Watermarking: An Ensemble Attack Network Using CNNs and Transformers TinyDrop: Tiny Model Guided Token Dropping for Vision Transformers LGBP-OrgaNet: Learnable Gaussian Band Pass Fusion of CNN and Transformer Features for Robust Organoid Segmentation and Tracking Towards Realistic Hand-Object Interaction with Gravity-Field Based Diffusion Bridge Enhancing Zero-Shot Pedestrian Attribute Recognition with Synthetic Data Generation: A Comparative Study with Image-To-Image Diffusion Models MedLiteNet: Lightweight Hybrid Medical Image Segmentation Model","title":"Table of Contents"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-04/#papers","text":"","title":"Papers"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-04/#infradiffusion-zero-shot-depth-map-restoration-with-diffusion-models-and-prompted-segmentation-from-sparse-infrastructure-point-clouds","text":"Authors: Yixiong Jing, Cheng Zhang, Haibing Wu, Guangming Wang, Olaf Wysocki, Brian Sheil Published: 2025-09-03 Categories: cs.CV Abstract: Point clouds are widely used for infrastructure monitoring by providing geometric information, where segmentation is required for downstream tasks such as defect detection. Existing research has automated semantic segmentation of structural components, while brick-level segmentation (identifying defects such as spalling and mortar loss) has been primarily conducted from RGB images. However, acquiring high-resolution images is impractical in low-light environments like masonry tunnels. Point clouds, though robust to dim lighting, are typically unstructured, sparse, and noisy, limiting fine-grained segmentation. We present InfraDiffusion, a zero-shot framework that projects masonry point clouds into depth maps using virtual cameras and restores them by adapting the Denoising Diffusion Null-space Model (DDNM). Without task-specific training, InfraDiffusion enhances visual clarity and geometric consistency of depth maps. Experiments on masonry bridge and tunnel point cloud datasets show significant improvements in brick-level segmentation using the Segment Anything Model (SAM), underscoring its potential for automated inspection of masonry assets. Our code and data is available at https://github.com/Jingyixiong/InfraDiffusion-official-implement. Analysis: InfraDiffusion \u8bba\u6587\u6458\u8981\u7684\u4e13\u5bb6\u5206\u6790\u5982\u4e0b\uff1a","title":"InfraDiffusion: zero-shot depth map restoration with diffusion models and prompted segmentation from sparse infrastructure point clouds"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-04/#1-2-3","text":"\u672c\u8bba\u6587\u63d0\u51fa\u4e86 InfraDiffusion\uff0c\u4e00\u4e2a\u96f6\u6837\u672c\uff08zero-shot\uff09\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u5728\u4f4e\u5149\u7167\u73af\u5883\u4e0b\u4ece\u7a00\u758f\u3001\u5608\u6742\u7684\u70b9\u4e91\u4e2d\u8fdb\u884c\u7ec6\u7c92\u5ea6\uff08\u5982\u7816\u5757\u7ea7\uff09\u5206\u5272\u7684\u6311\u6218\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u5c06\u70b9\u4e91\u6295\u5f71\u5230\u6df1\u5ea6\u56fe\u5e76\u5229\u7528\u9002\u5e94\u540e\u7684\u53bb\u566a\u6269\u6563\u96f6\u7a7a\u95f4\u6a21\u578b\uff08DDNM\uff09\u8fdb\u884c\u6062\u590d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6df1\u5ea6\u56fe\u7684\u89c6\u89c9\u6e05\u6670\u5ea6\u548c\u51e0\u4f55\u4e00\u81f4\u6027\u3002\u6700\u7ec8\uff0c\u5b83\u5728\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u663e\u8457\u6539\u5584\u4e86\u7816\u5757\u7ea7\u5206\u5272\u7684\u6027\u80fd\uff0c\u4e3a\u57fa\u7840\u8bbe\u65bd\u7684\u81ea\u52a8\u5316\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002","title":"1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u660e\u603b\u7ed3 (2-3 \u53e5\u8bdd)"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-04/#2_1","text":"InfraDiffusion \u7684\u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u5176 \u96f6\u6837\u672c\u6df1\u5ea6\u56fe\u6062\u590d\u65b9\u6cd5 \uff0c\u4ee5\u53ca\u5bf9 Denoising Diffusion Null-space Model (DDNM) \u7684\u5de7\u5999\u9002\u5e94\u548c\u5e94\u7528 \u3002\u5177\u4f53\u800c\u8a00\uff1a \u70b9\u4e91\u5230\u6df1\u5ea6\u56fe\u7684\u8f6c\u6362\u4e0e\u6062\u590d\u8303\u5f0f\uff1a \u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6d41\u7a0b\uff0c\u9996\u5148\u5229\u7528\u865a\u62df\u76f8\u673a\u5c06\u975e\u7ed3\u6784\u5316\u7684\u7a00\u758f\u70b9\u4e91\u6295\u5f71\u6210\u6df1\u5ea6\u56fe\uff0c\u7136\u540e\u5c06\u6df1\u5ea6\u56fe\u7684\u6062\u590d\u89c6\u4e3a\u4e00\u4e2a\u56fe\u50cf\u4fee\u590d\u95ee\u9898\u3002 DDNM \u7684\u521b\u65b0\u6027\u9002\u5e94\uff1a DDNM \u901a\u5e38\u7528\u4e8e\u4ece\u635f\u574f\u56fe\u50cf\u4e2d\u8fdb\u884c\u4fee\u590d\uff08\u5982\u53bb\u566a\u3001\u8865\u5168\uff09\u3002InfraDiffusion \u5c06\u5176\u9002\u5e94\u4e8e\u4ece\u7a00\u758f\u70b9\u4e91\u751f\u6210\u7684\u201c\u4e0d\u5b8c\u6574\u201d\u6df1\u5ea6\u56fe\u8fdb\u884c\u6062\u590d\uff0c\u4ee5\u589e\u5f3a\u5176\u89c6\u89c9\u6e05\u6670\u5ea6\u548c\u51e0\u4f55\u4e00\u81f4\u6027\u3002\u8fd9\u79cd\u9002\u5e94\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u5728\u6ca1\u6709\u7279\u5b9a\u4efb\u52a1\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u4ece\u4f4e\u8d28\u91cf\u7684\u6df1\u5ea6\u56fe\u4e2d\u6062\u590d\u51fa\u9ad8\u4fdd\u771f\u5ea6\u7684\u4fe1\u606f\u3002 \u96f6\u6837\u672c\u80fd\u529b\uff1a \u6574\u4e2a\u6846\u67b6\u65e0\u9700\u9488\u5bf9\u7279\u5b9a\u7684\u7816\u5757\u7ea7\u5206\u5272\u4efb\u52a1\u8fdb\u884c\u8bad\u7ec3\uff0c\u76f4\u63a5\u5229\u7528\u9884\u8bad\u7ec3\u7684\u6269\u6563\u6a21\u578b\u548c\u73b0\u6709\u7684\u5206\u5272\u6a21\u578b\uff08\u5982 SAM\uff09\uff0c\u6781\u5927\u5730\u63d0\u9ad8\u4e86\u5176\u6cdb\u5316\u80fd\u529b\u548c\u5e94\u7528\u6548\u7387\u3002","title":"2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u5b66\u65b9\u6cd5"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-04/#3_1","text":"\u63a8\u52a8\u57fa\u7840\u8bbe\u65bd\u81ea\u52a8\u5316\u68c0\u6d4b\uff1a InfraDiffusion \u4e3a\u6865\u6881\u3001\u96a7\u9053\u7b49\u57fa\u7840\u8bbe\u65bd\u7684\u81ea\u52a8\u5316\u3001\u7cbe\u7ec6\u5316\u68c0\u6d4b\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u5de5\u5177\uff0c\u5c24\u5176\u662f\u5728\u4f20\u7edf\u65b9\u6cd5\u53d7\u9650\u7684\u6076\u52a3\u73af\u5883\uff08\u5982\u4f4e\u5149\u7167\uff09\u4e0b\u3002\u8fd9\u5c06\u63d0\u9ad8\u68c0\u6d4b\u6548\u7387\u3001\u964d\u4f4e\u6210\u672c\u5e76\u589e\u5f3a\u5b89\u5168\u6027\u3002 \u5f25\u5408 3D \u70b9\u4e91\u4e0e 2D \u56fe\u50cf\u5904\u7406\u7684\u9e3f\u6c9f\uff1a \u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u9014\u5f84\uff0c\u5c06\u7a00\u758f\u3001\u975e\u7ed3\u6784\u5316\u7684 3D \u70b9\u4e91\u6570\u636e\u8f6c\u5316\u4e3a\u9ad8\u8d28\u91cf\u7684 2D \u6df1\u5ea6\u56fe\uff0c\u4ece\u800c\u80fd\u591f\u5229\u7528\u5f3a\u5927\u7684 2D \u89c6\u89c9\u6a21\u578b\uff08\u5982 SAM\uff09\u8fdb\u884c\u7ec6\u7c92\u5ea6\u5206\u6790\uff0c\u4e3a\u70b9\u4e91\u5904\u7406\u5f00\u8f9f\u4e86\u65b0\u7684\u601d\u8def\u3002 \u62d3\u5c55\u6269\u6563\u6a21\u578b\u7684\u5e94\u7528\u8fb9\u754c\uff1a \u8bba\u6587\u5c55\u793a\u4e86\u6269\u6563\u6a21\u578b\u5728\u975e\u4f20\u7edf\u56fe\u50cf\u4fee\u590d\u4efb\u52a1\uff08\u4ece\u7a00\u758f 3D \u6570\u636e\u6062\u590d 2D \u6df1\u5ea6\u56fe\uff09\u4e2d\u7684\u5de8\u5927\u6f5c\u529b\uff0c\u53ef\u80fd\u542f\u53d1\u66f4\u591a\u5c06\u6269\u6563\u6a21\u578b\u5e94\u7528\u4e8e\u5176\u4ed6\u7c7b\u578b\u4f20\u611f\u5668\u6570\u636e\u6216\u6570\u636e\u8f6c\u6362\u4efb\u52a1\u7684\u7814\u7a76\u3002 \u4fc3\u8fdb\u96f6\u6837\u672c\u5b66\u4e60\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u843d\u5730\uff1a \u5176\u96f6\u6837\u672c\u7279\u6027\u5bf9\u4e8e\u5b9e\u9645\u5de5\u7a0b\u5e94\u7528\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u56e0\u4e3a\u5b83\u51cf\u5c11\u4e86\u5bf9\u5927\u91cf\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u66f4\u5feb\u5730\u90e8\u7f72\u548c\u9002\u5e94\u65b0\u573a\u666f\u3002","title":"3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-04/#4_1","text":"\u571f\u6728\u5de5\u7a0b\u4e0e\u7ed3\u6784\u5065\u5eb7\u76d1\u6d4b\uff1a \u76f4\u63a5\u53d7\u76ca\u9886\u57df\uff0c\u7528\u4e8e\u6865\u6881\u3001\u96a7\u9053\u3001\u5927\u575d\u7b49\u57fa\u7840\u8bbe\u65bd\u7684\u88c2\u7f1d\u3001\u5265\u843d\u3001\u7802\u6d46\u6d41\u5931\u7b49\u7f3a\u9677\u7684\u81ea\u52a8\u5316\u68c0\u6d4b\u3002 \u673a\u5668\u4eba\u4e0e\u81ea\u4e3b\u5de1\u68c0\uff1a \u642d\u8f7d LiDAR \u6216\u6df1\u5ea6\u4f20\u611f\u5668\u7684\u5de1\u68c0\u673a\u5668\u4eba\u6216\u65e0\u4eba\u673a\uff0c\u53ef\u5728\u590d\u6742\u6216\u5371\u9669\u73af\u5883\u4e2d\u5229\u7528\u6b64\u6280\u672f\u8fdb\u884c\u81ea\u4e3b\u6570\u636e\u91c7\u96c6\u548c\u5206\u6790\u3002 \u6587\u5316\u9057\u4ea7\u4fdd\u62a4\uff1a \u5bf9\u5386\u53f2\u5efa\u7b51\u3001\u53e4\u8ff9\u7b49\u7816\u77f3\u7ed3\u6784\u8fdb\u884c\u65e0\u635f\u3001\u7cbe\u7ec6\u5316\u76d1\u6d4b\uff0c\u8bc4\u4f30\u5176\u635f\u574f\u7a0b\u5ea6\u3002 \u77ff\u4e1a\u4e0e\u5730\u4e0b\u7a7a\u95f4\u52d8\u6d4b\uff1a \u5728\u5149\u7167\u6761\u4ef6\u5dee\u7684\u77ff\u4e95\u3001\u5730\u4e0b\u901a\u9053\u7b49\u73af\u5883\u4e2d\uff0c\u8fdb\u884c\u7ed3\u6784\u5b8c\u6574\u6027\u8bc4\u4f30\u3002 \u901a\u7528 3D \u6570\u636e\u5904\u7406\uff1a \u4efb\u4f55\u9700\u8981\u4ece\u7a00\u758f\u3001\u5608\u6742\u7684 3D \u70b9\u4e91\u4e2d\u63d0\u53d6\u9ad8\u7cbe\u5ea6 2D \u51e0\u4f55\u6216\u8bed\u4e49\u4fe1\u606f\u7684\u5e94\u7528\u573a\u666f\u3002","title":"4. \u53ef\u80fd\u53d7\u76ca\u7684\u76f8\u5173\u9886\u57df\u6216\u5e94\u7528"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-04/#5","text":"\u70b9\u4e91\u8d28\u91cf\u7684\u4f9d\u8d56\u6027\uff1a \u5c3d\u7ba1\u8be5\u65b9\u6cd5\u65e8\u5728\u5904\u7406\u7a00\u758f\u548c\u5608\u6742\u7684\u70b9\u4e91\uff0c\u4f46\u5176\u6027\u80fd\u53ef\u80fd\u4ecd\u53d7\u539f\u59cb\u70b9\u4e91\u6781\u7aef\u7a00\u758f\u5ea6\u6216\u566a\u58f0\u6c34\u5e73\u7684\u9650\u5236\u3002\u6458\u8981\u672a\u660e\u786e\u6307\u51fa\u5176\u80fd\u5904\u7406\u7684\u6700\u4f4e\u70b9\u4e91\u5bc6\u5ea6\u6216\u6700\u9ad8\u566a\u58f0\u5bb9\u5fcd\u5ea6\u3002 \u865a\u62df\u76f8\u673a\u8bbe\u7f6e\u7684\u654f\u611f\u6027\uff1a \u865a\u62df\u76f8\u673a\u7684\u9009\u62e9\u3001\u4f4d\u7f6e\u548c\u53c2\u6570\u8bbe\u7f6e\u53ef\u80fd\u4f1a\u5f71\u54cd\u6df1\u5ea6\u56fe\u7684\u8d28\u91cf\u548c\u6062\u590d\u6548\u679c\u3002\u8fd9\u53ef\u80fd\u9700\u8981\u4e00\u5b9a\u7684\u9886\u57df\u77e5\u8bc6\u6216\u7ecf\u9a8c\u6765\u4f18\u5316\u3002 DDNM \u6cdb\u5316\u80fd\u529b\uff1a \u867d\u7136 DDNM \u88ab\u9002\u5e94\u7528\u4e8e\u6df1\u5ea6\u56fe\u6062\u590d\uff0c\u4f46\u5176\u5e95\u5c42\u6269\u6563\u6a21\u578b\u662f\u5728\u56fe\u50cf\u6570\u636e\u4e0a\u9884\u8bad\u7ec3\u7684\u3002\u5176\u5bf9\u6df1\u5ea6\u56fe\u8fd9\u79cd\u975e RGB \u56fe\u50cf\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4ee5\u53ca\u5bf9\u4e0d\u540c\u7c7b\u578b\u57fa\u7840\u8bbe\u65bd\u51e0\u4f55\u7279\u5f81\u7684\u9002\u5e94\u6027\uff0c\u53ef\u80fd\u4ecd\u6709\u5f85\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u3002 \u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\uff1a \u6269\u6563\u6a21\u578b\u901a\u5e38\u8ba1\u7b97\u6210\u672c\u8f83\u9ad8\uff0c\u5c24\u5176\u662f\u5728\u751f\u6210\u9ad8\u5206\u8fa8\u7387\u6df1\u5ea6\u56fe\u65f6\u3002\u6458\u8981\u672a\u63d0\u53ca\u63a8\u7406\u901f\u5ea6\u6216\u8ba1\u7b97\u6548\u7387\uff0c\u8fd9\u5bf9\u4e8e\u5b9e\u65f6\u6216\u5927\u89c4\u6a21\u5e94\u7528\u53ef\u80fd\u662f\u4e00\u4e2a\u8003\u91cf\u56e0\u7d20\u3002 \u5b9e\u9a8c\u8303\u56f4\uff1a \u5b9e\u9a8c\u4ec5\u5728\u201c\u7816\u77f3\u6865\u6881\u548c\u96a7\u9053\u70b9\u4e91\u6570\u636e\u96c6\u201d\u4e0a\u8fdb\u884c\u3002\u5176\u5728\u5176\u4ed6\u7c7b\u578b\u57fa\u7840\u8bbe\u65bd\uff08\u5982\u6df7\u51dd\u571f\u3001\u94a2\u7ed3\u6784\uff09\u6216\u4e0d\u540c\u6750\u6599\u4e0a\u7684\u8868\u73b0\u5c1a\u4e0d\u660e\u786e\u3002 \u201c\u96f6\u6837\u672c\u201d\u7684\u8fb9\u754c\uff1a \u5c3d\u7ba1\u5b9e\u73b0\u4e86\u96f6\u6837\u672c\u5206\u5272\uff0c\u4f46\u6700\u7ec8\u7684\u7f3a\u9677\u68c0\u6d4b\u53ef\u80fd\u4ecd\u9700\u8981\u5bf9\u5206\u5272\u7ed3\u679c\u8fdb\u884c\u540e\u5904\u7406\u6216\u5206\u7c7b\uff0c\u8fd9\u90e8\u5206\u53ef\u80fd\u4e0d\u5c5e\u4e8e\u96f6\u6837\u672c\u8303\u7574\u3002 Key Findings: We present InfraDiffusion, a zero-shot framework that projects masonry point clouds into depth maps using virtual cameras and restores them by adapting the Denoising Diffusion Null-space Model (DDNM). Links: PDF arXiv","title":"5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u7684\u5c40\u9650\u6027"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-04/#meddinov3-how-to-adapt-vision-foundation-models-for-medical-image-segmentation","text":"Authors: Yuheng Li, Yizhou Wu, Yuxiang Lai, Mingzhe Hu, Xiaofeng Yang Published: 2025-09-02 Categories: cs.CV Abstract: Accurate segmentation of organs and tumors in CT and MRI scans is essential for diagnosis, treatment planning, and disease monitoring. While deep learning has advanced automated segmentation, most models remain task-specific, lacking generalizability across modalities and institutions. Vision foundation models (FMs) pretrained on billion-scale natural images offer powerful and transferable representations. However, adapting them to medical imaging faces two key challenges: (1) the ViT backbone of most foundation models still underperform specialized CNNs on medical image segmentation, and (2) the large domain gap between natural and medical images limits transferability. We introduce MedDINOv3, a simple and effective framework for adapting DINOv3 to medical segmentation. We first revisit plain ViTs and design a simple and effective architecture with multi-scale token aggregation. Then, we perform domain-adaptive pretraining on CT-3M, a curated collection of 3.87M axial CT slices, using a multi-stage DINOv3 recipe to learn robust dense features. MedDINOv3 matches or exceeds state-of-the-art performance across four segmentation benchmarks, demonstrating the potential of vision foundation models as unified backbones for medical image segmentation. The code is available at https://github.com/ricklisz/MedDINOv3. Analysis: \u4f5c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u4e13\u5bb6\uff0c\u6211\u5bf9\u8fd9\u7bc7\u5173\u4e8eMedDINOv3\u7684\u8bba\u6587\u6458\u8981\u8fdb\u884c\u5982\u4e0b\u5206\u6790\uff1a","title":"MedDINOv3: How to adapt vision foundation models for medical image segmentation?"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-04/#meddinov3","text":"1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u660e\u603b\u7ed3 (2-3 \u53e5\u8bdd) MedDINOv3\u63d0\u51fa\u4e86\u4e00\u4e2a\u7b80\u5355\u800c\u6709\u6548\u7684\u6846\u67b6\uff0c\u65e8\u5728\u5c06DINOv3\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u6210\u529f\u5e94\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u5206\u5272\u3002\u5b83\u901a\u8fc7\u6539\u8fdbViT\u9aa8\u5e72\u7f51\u7edc\u4ee5\u9002\u5e94\u533b\u5b66\u56fe\u50cf\u7279\u6027\uff0c\u5e76\u7ed3\u5408\u5927\u89c4\u6a21CT\u6570\u636e\u96c6\u4e0a\u7684\u9886\u57df\u81ea\u9002\u5e94\u9884\u8bad\u7ec3\uff0c\u514b\u670d\u4e86\u81ea\u7136\u56fe\u50cf\u4e0e\u533b\u5b66\u56fe\u50cf\u4e4b\u95f4\u7684\u9886\u57df\u9e3f\u6c9f\u3002\u6700\u7ec8\uff0cMedDINOv3\u5728\u591a\u4e2a\u533b\u5b66\u5206\u5272\u57fa\u51c6\u4e0a\u8fbe\u5230\u4e86\u6216\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u57fa\u7840\u6a21\u578b\u4f5c\u4e3a\u533b\u5b66\u56fe\u50cf\u7edf\u4e00\u9aa8\u5e72\u7f51\u7edc\u7684\u5de8\u5927\u6f5c\u529b\u3002 2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u5b66\u65b9\u6cd5 \u8be5\u8bba\u6587\u7684\u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u5176\u53cc\u7ba1\u9f50\u4e0b\u7684\u65b9\u6cd5\uff1a 1. ViT\u67b6\u6784\u7684\u9002\u5e94\u6027\u6539\u8fdb\uff1a \u4f5c\u8005\u91cd\u65b0\u5ba1\u89c6\u4e86\u539f\u59cb\u7684Vision Transformer (ViT) \u67b6\u6784\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5177\u6709\u201c\u591a\u5c3a\u5ea6token\u805a\u5408\u201d\u80fd\u529b\u7684\u7b80\u5355\u800c\u6709\u6548\u7684\u67b6\u6784\u3002\u8fd9\u65e8\u5728\u89e3\u51b3ViT\u5728\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4efb\u52a1\u4e0a\u53ef\u80fd\u4e0d\u5982\u4e13\u7528CNN\u7684\u95ee\u9898\uff0c\u4f7f\u5176\u80fd\u66f4\u597d\u5730\u6355\u6349\u533b\u5b66\u56fe\u50cf\u4e2d\u591a\u5c3a\u5ea6\u3001\u7cbe\u7ec6\u7684\u89e3\u5256\u7ed3\u6784\u4fe1\u606f\u3002 2. \u5927\u89c4\u6a21\u9886\u57df\u81ea\u9002\u5e94\u9884\u8bad\u7ec3\uff1a \u4ed6\u4eec\u5728\u5927\u89c4\u6a21\u7684CT\u6570\u636e\u96c6\uff08CT-3M\uff0c\u5305\u542b3.87M\u8f74\u5411CT\u5207\u7247\uff09\u4e0a\uff0c\u91c7\u7528\u201c\u591a\u9636\u6bb5DINOv3\u914d\u65b9\u201d\u8fdb\u884c\u9886\u57df\u81ea\u9002\u5e94\u9884\u8bad\u7ec3\u3002\u8fd9\u4e00\u7b56\u7565\u6709\u6548\u5730\u5f25\u5408\u4e86\u81ea\u7136\u56fe\u50cf\u4e0e\u533b\u5b66\u56fe\u50cf\u4e4b\u95f4\u7684\u5de8\u5927\u9886\u57df\u9e3f\u6c9f\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u5b66\u4e60\u5230\u5bf9\u533b\u5b66\u56fe\u50cf\u7279\u6709\u7684\u9c81\u68d2\u3001\u5bc6\u96c6\u7684\u89c6\u89c9\u7279\u5f81\uff0c\u4ece\u800c\u63d0\u9ad8\u5176\u5728\u4e0b\u6e38\u5206\u5272\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002 3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd \u8fd9\u9879\u7814\u7a76\u5bf9\u533b\u5b66\u56fe\u50cf\u5206\u6790\u9886\u57df\u5177\u6709\u6df1\u8fdc\u5f71\u54cd\uff1a * \u63a8\u52a8\u901a\u7528\u533b\u5b66AI\u6a21\u578b\u53d1\u5c55\uff1a \u5b83\u4e3a\u5c06\u5f3a\u5927\u7684\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08\u5982DINOv3\uff09\u5e94\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u5206\u5272\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u8303\u5f0f\uff0c\u6709\u671b\u63a8\u52a8\u5f00\u53d1\u51fa\u66f4\u901a\u7528\u3001\u66f4\u5c11\u4f9d\u8d56\u4e8e\u7279\u5b9a\u4efb\u52a1\u6807\u6ce8\u6570\u636e\u7684\u533b\u5b66AI\u6a21\u578b\u3002 * \u63d0\u9ad8\u5206\u5272\u51c6\u786e\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\uff1a \u901a\u8fc7\u63d0\u4f9b\u7edf\u4e00\u4e14\u9ad8\u6027\u80fd\u7684\u9aa8\u5e72\u7f51\u7edc\uff0cMedDINOv3\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u51c6\u786e\u6027\u548c\u8de8\u6a21\u6001\u3001\u8de8\u673a\u6784\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4ece\u800c\u52a0\u901f\u8bca\u65ad\u3001\u6cbb\u7597\u89c4\u5212\u548c\u75be\u75c5\u76d1\u6d4b\u7684\u81ea\u52a8\u5316\u548c\u6807\u51c6\u5316\u8fdb\u7a0b\u3002 * \u964d\u4f4e\u5f00\u53d1\u6210\u672c\uff1a \u9884\u8bad\u7ec3\u7684\u57fa\u7840\u6a21\u578b\u53ef\u4ee5\u4f5c\u4e3a\u65b0\u4efb\u52a1\u7684\u8d77\u70b9\uff0c\u51cf\u5c11\u4ece\u5934\u8bad\u7ec3\u6a21\u578b\u6240\u9700\u7684\u6570\u636e\u91cf\u548c\u8ba1\u7b97\u8d44\u6e90\uff0c\u4ece\u800c\u964d\u4f4e\u533b\u5b66AI\u5e94\u7528\u7684\u5f00\u53d1\u6210\u672c\u548c\u65f6\u95f4\u3002 4. \u53ef\u80fd\u53d7\u76ca\u7684\u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 \u9664\u4e86\u76f4\u63a5\u7684\u5668\u5b98\u548c\u80bf\u7624\u5206\u5272\uff0c\u8fd9\u9879\u7814\u7a76\u7684\u6210\u679c\u53ef\u4ee5\u63a8\u5e7f\u5230\u591a\u79cd\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4efb\u52a1\u548c\u5e94\u7528\u4e2d\uff1a * \u5176\u4ed6\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4efb\u52a1\uff1a \u4f8b\u5982\u75c5\u7076\u68c0\u6d4b\u3001\u75be\u75c5\u5206\u7c7b\u3001\u56fe\u50cf\u914d\u51c6\u30013D\u91cd\u5efa\u7b49\uff0c\u8fd9\u4e9b\u4efb\u52a1\u90fd\u53ef\u4ee5\u53d7\u76ca\u4e8eMedDINOv3\u5b66\u4e60\u5230\u7684\u5f3a\u5927\u4e14\u901a\u7528\u7684\u533b\u5b66\u56fe\u50cf\u7279\u5f81\u3002 * \u4e0d\u540c\u533b\u5b66\u5f71\u50cf\u6a21\u6001\uff1a \u5c3d\u7ba1\u5f53\u524d\u9884\u8bad\u7ec3\u4e3b\u8981\u57fa\u4e8eCT\uff0c\u4f46\u5176\u6846\u67b6\u548c\u65b9\u6cd5\u8bba\u53ef\u542f\u53d1\u5728MRI\u3001X\u5149\u3001\u8d85\u58f0\u3001\u75c5\u7406\u56fe\u50cf\u7b49\u5176\u4ed6\u6a21\u6001\u4e0a\u8fdb\u884c\u7c7b\u4f3c\u7684\u57fa\u7840\u6a21\u578b\u9002\u5e94\uff0c\u4ee5\u6784\u5efa\u591a\u6a21\u6001\u7684\u533b\u5b66\u57fa\u7840\u6a21\u578b\u3002 * \u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\uff1a \u63d0\u9ad8\u81ea\u52a8\u5316\u5206\u6790\u7684\u51c6\u786e\u6027\uff0c\u8f85\u52a9\u533b\u751f\u8fdb\u884c\u8bca\u65ad\u548c\u6cbb\u7597\u65b9\u6848\u5236\u5b9a\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u5927\u91cf\u5f71\u50cf\u6570\u636e\u65f6\u3002 * \u533b\u5b66\u6559\u80b2\u4e0e\u7814\u7a76\uff1a \u63d0\u4f9b\u9ad8\u6027\u80fd\u7684\u5206\u5272\u5de5\u5177\uff0c\u52a0\u901f\u533b\u5b66\u7814\u7a76\u548c\u65b0\u7597\u6cd5\u7684\u5f00\u53d1\uff0c\u4f8b\u5982\u5728\u836f\u7269\u7b5b\u9009\u3001\u751f\u7269\u6807\u5fd7\u7269\u8bc6\u522b\u7b49\u9886\u57df\u3002 5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u51fa\u7684\u5c40\u9650\u6027 \u5c3d\u7ba1MedDINOv3\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u4ece\u6458\u8981\u4e2d\u4ecd\u53ef\u63a8\u65ad\u51fa\u4e00\u4e9b\u6f5c\u5728\u5c40\u9650\u6027\uff1a * \u6a21\u6001\u7279\u5f02\u6027\uff1a \u9886\u57df\u81ea\u9002\u5e94\u9884\u8bad\u7ec3\u4e3b\u8981\u57fa\u4e8eCT-3M\u6570\u636e\u96c6\uff0c\u8fd9\u610f\u5473\u7740\u5176\u5728MRI\u3001\u8d85\u58f0\u3001X\u5149\u7b49\u5176\u4ed6\u533b\u5b66\u5f71\u50cf\u6a21\u6001\u4e0a\u7684\u76f4\u63a5\u6cdb\u5316\u80fd\u529b\u53ef\u80fd\u53d7\u9650\u3002\u8981\u5b9e\u73b0\u771f\u6b63\u7684\u201c\u7edf\u4e00\u9aa8\u5e72\u201d\uff0c\u53ef\u80fd\u9700\u8981\u66f4\u901a\u7528\u7684\u591a\u6a21\u6001\u9884\u8bad\u7ec3\u6570\u636e\u548c\u7b56\u7565\u3002 * \u67b6\u6784\u6539\u8fdb\u7684\u7ec6\u8282\u4e0e\u666e\u9002\u6027\uff1a \u6458\u8981\u4e2d\u63d0\u5230\u5bf9ViT\u67b6\u6784\u8fdb\u884c\u4e86\u201c\u7b80\u5355\u800c\u6709\u6548\u201d\u7684\u6539\u8fdb\uff0c\u4f46\u5177\u4f53\u7ec6\u8282\u672a\u62ab\u9732\u3002\u5176\u590d\u6742\u6027\u3001\u8ba1\u7b97\u6548\u7387\u4ee5\u53ca\u5728\u66f4\u5e7f\u6cdb\u3001\u66f4\u590d\u6742\u7684\u533b\u5b66\u56fe\u50cf\u573a\u666f\u4e0b\u7684\u666e\u9002\u6027\u4ecd\u9700\u901a\u8fc7\u8bba\u6587\u6b63\u6587\u6df1\u5165\u4e86\u89e3\u3002 * \u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\uff1a \u5927\u89c4\u6a21\u6570\u636e\u96c6\uff083.87M CT\u5207\u7247\uff09\u4e0a\u7684\u591a\u9636\u6bb5DINOv3\u9884\u8bad\u7ec3\u901a\u5e38\u9700\u8981\u5de8\u5927\u7684\u8ba1\u7b97\u8d44\u6e90\u548c\u65f6\u95f4\uff0c\u8fd9\u53ef\u80fd\u9650\u5236\u4e86\u5176\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u5e94\u7528\u6216\u8fdb\u4e00\u6b65\u5f00\u53d1\u3002 * \u57fa\u51c6\u6d4b\u8bd5\u7684\u8986\u76d6\u8303\u56f4\uff1a \u5c3d\u7ba1\u5728\u56db\u4e2a\u5206\u5272\u57fa\u51c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4efb\u52a1\u7684\u591a\u6837\u6027\u8fdc\u8d85\u6b64\u8303\u56f4\u3002\u5176\u5728\u7f55\u89c1\u75be\u75c5\u3001\u7279\u5b9a\u75c5\u7406\u6216\u66f4\u590d\u6742\u89e3\u5256\u7ed3\u6784\u4e0a\u7684\u8868\u73b0\u4ecd\u9700\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u3002 * \u53ef\u89e3\u91ca\u6027\u4e0e\u9c81\u68d2\u6027\uff1a \u6458\u8981\u672a\u63d0\u53ca\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u6216\u5bf9\u5bf9\u6297\u6027\u653b\u51fb\u7684\u9c81\u68d2\u6027\u3002\u5728\u5bf9\u5b89\u5168\u6027\u8981\u6c42\u6781\u9ad8\u7684\u533b\u5b66\u5e94\u7528\u4e2d\uff0c\u8fd9\u4e9b\u662f\u91cd\u8981\u7684\u8003\u91cf\u56e0\u7d20\u3002 Key Findings: MedDINOv3 matches or exceeds state-of-the-art performance across four segmentation benchmarks, demonstrating the potential of vision foundation models as unified backbones for medical image segmentation. Links: PDF arXiv","title":"MedDINOv3: \u5982\u4f55\u5c06\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u5e94\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u5206\u5272\uff1f"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-04/#onecat-decoder-only-auto-regressive-model-for-unified-understanding-and-generation","text":"Authors: Han Li, Xinyu Peng, Yaoming Wang, Zelin Peng, Xin Chen, Rongxiang Weng, Jingang Wang, Xunliang Cai, Wenrui Dai, Hongkai Xiong Published: 2025-09-03 Categories: cs.CV Abstract: We introduce OneCAT, a unified multimodal model that seamlessly integrates understanding, generation, and editing within a novel, pure decoder-only transformer architecture. Our framework uniquely eliminates the need for external components such as Vision Transformers (ViT) or vision tokenizer during inference, leading to significant efficiency gains, especially for high-resolution inputs. This is achieved through a modality-specific Mixture-of-Experts (MoE) structure trained with a single autoregressive (AR) objective, which also natively supports dynamic resolutions. Furthermore, we pioneer a multi-scale visual autoregressive mechanism within the Large Language Model (LLM) that drastically reduces decoding steps compared to diffusion-based methods while maintaining state-of-the-art performance. Our findings demonstrate the powerful potential of pure autoregressive modeling as a sufficient and elegant foundation for unified multimodal intelligence. As a result, OneCAT sets a new performance standard, outperforming existing open-source unified multimodal models across benchmarks for multimodal generation, editing, and understanding. Analysis: \u8fd9\u7bc7\u8bba\u6587\u6458\u8981\u4ecb\u7ecd\u7684OneCAT\u6a21\u578b\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u5177\u6709\u663e\u8457\u7684\u6f5c\u5728\u8da3\u5473\u6027\u548c\u91cd\u8981\u6027\u3002\u4ee5\u4e0b\u662f\u8be6\u7ec6\u5206\u6790\uff1a","title":"OneCAT: Decoder-Only Auto-Regressive Model for Unified Understanding and Generation"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-04/#1-concise-summary","text":"OneCAT\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u6a21\u6001\u6a21\u578b\uff0c\u91c7\u7528\u7eaf\u89e3\u7801\u5668Transformer\u67b6\u6784\uff0c\u65e0\u7f1d\u6574\u5408\u4e86\u7406\u89e3\u3001\u751f\u6210\u548c\u7f16\u8f91\u529f\u80fd\u3002\u5b83\u901a\u8fc7\u6d88\u9664\u63a8\u7406\u65f6\u5bf9\u5916\u90e8\u89c6\u89c9\u7ec4\u4ef6\uff08\u5982ViT\u6216\u89c6\u89c9tokenizer\uff09\u7684\u4f9d\u8d56\uff0c\u5e76\u5f15\u5165\u591a\u5c3a\u5ea6\u89c6\u89c9\u81ea\u56de\u5f52\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387\u548c\u6027\u80fd\uff0c\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u6a21\u578b\u3002","title":"1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e (Concise Summary)"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-04/#2-key-innovation-or-methodological-approach","text":"OneCAT\u7684\u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u5176 \u7eaf\u89e3\u7801\u5668Transformer\u67b6\u6784 \uff0c\u8be5\u67b6\u6784\u901a\u8fc7 \u6a21\u6001\u7279\u5b9a\u7684MoE\uff08Mixture-of-Experts\uff09\u7ed3\u6784 \uff0c\u5728\u5355\u4e00\u81ea\u56de\u5f52\u76ee\u6807\u4e0b\u8bad\u7ec3\uff0c\u4ece\u800c\u5728\u63a8\u7406\u65f6\u65e0\u9700\u5916\u90e8ViT\u6216\u89c6\u89c9tokenizer\uff0c\u5e76\u539f\u751f\u652f\u6301\u52a8\u6001\u5206\u8fa8\u7387\u3002\u6b64\u5916\uff0c\u5b83\u5f15\u5165\u4e86 \u591a\u5c3a\u5ea6\u89c6\u89c9\u81ea\u56de\u5f52\u673a\u5236 \uff0c\u663e\u8457\u51cf\u5c11\u4e86\u751f\u6210\u4efb\u52a1\u7684\u89e3\u7801\u6b65\u9aa4\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002","title":"2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5 (Key Innovation or Methodological Approach)"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-04/#3-potential-impact-on-the-field","text":"\u8303\u5f0f\u8f6c\u53d8\uff1a \u8bc1\u660e\u4e86\u7eaf\u81ea\u56de\u5f52\u5efa\u6a21\u4f5c\u4e3a\u7edf\u4e00\u591a\u6a21\u6001\u667a\u80fd\u57fa\u7840\u7684\u5f3a\u5927\u6f5c\u529b\uff0c\u53ef\u80fd\u63a8\u52a8\u672a\u6765\u591a\u6a21\u6001\u6a21\u578b\u8bbe\u8ba1\u5411\u66f4\u7b80\u6d01\u3001\u7edf\u4e00\u7684\u81ea\u56de\u5f52\u8303\u5f0f\u53d1\u5c55\uff0c\u6311\u6218\u5f53\u524d\u4e3b\u6d41\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u6216\u6269\u6563\u6a21\u578b\u8303\u5f0f\u3002 \u6548\u7387\u63d0\u5347\uff1a \u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u6a21\u578b\u7684\u63a8\u7406\u6548\u7387\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u9ad8\u5206\u8fa8\u7387\u8f93\u5165\u548c\u751f\u6210\u4efb\u52a1\u65f6\uff0c\u901a\u8fc7\u51cf\u5c11\u5bf9\u5916\u90e8\u7ec4\u4ef6\u7684\u4f9d\u8d56\u548c\u4f18\u5316\u89e3\u7801\u6b65\u9aa4\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\uff08\u5982\u6269\u6563\u6a21\u578b\uff09\u7684\u6548\u7387\u74f6\u9888\u3002 \u6a21\u578b\u7b80\u5316\uff1a \u6d88\u9664\u5bf9\u5916\u90e8\u89c6\u89c9\u7ec4\u4ef6\u7684\u4f9d\u8d56\uff0c\u7b80\u5316\u4e86\u591a\u6a21\u6001\u7cfb\u7edf\u7684\u90e8\u7f72\u548c\u7ef4\u62a4\uff0c\u964d\u4f4e\u4e86\u7cfb\u7edf\u590d\u6742\u6027\u3002 \u6027\u80fd\u65b0\u6807\u6746\uff1a \u5728\u591a\u6a21\u6001\u7406\u89e3\u3001\u751f\u6210\u548c\u7f16\u8f91\u4efb\u52a1\u4e0a\u6811\u7acb\u4e86\u65b0\u7684\u6027\u80fd\u6807\u51c6\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u66f4\u5f3a\u7684\u57fa\u7ebf\u3002","title":"3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd (Potential Impact on the Field)"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-04/#4-related-areas-or-applications-that-might-benefit-from-this-research","text":"\u591a\u6a21\u6001\u5185\u5bb9\u521b\u4f5c\uff1a \u9ad8\u8d28\u91cf\u56fe\u50cf/\u89c6\u9891\u751f\u6210\u3001\u98ce\u683c\u8fc1\u79fb\u3001\u6587\u672c\u5f15\u5bfc\u7684\u56fe\u50cf\u7f16\u8f91\u3001\u865a\u62df\u73b0\u5b9e/\u589e\u5f3a\u73b0\u5b9e\u4e2d\u7684\u5185\u5bb9\u751f\u6210\u3002 \u591a\u6a21\u6001\u7406\u89e3\uff1a \u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u3001\u56fe\u50cf\u5b57\u5e55\u751f\u6210\uff08Image Captioning\uff09\u3001\u89c6\u9891\u7406\u89e3\u3001\u8de8\u6a21\u6001\u68c0\u7d22\u3001\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u73af\u5883\u611f\u77e5\u3002 \u4eba\u673a\u4ea4\u4e92\uff1a \u66f4\u81ea\u7136\u7684\u591a\u6a21\u6001\u5bf9\u8bdd\u7cfb\u7edf\u3001\u667a\u80fd\u52a9\u624b\u3001\u673a\u5668\u4eba\u4e0e\u73af\u5883\u7684\u4ea4\u4e92\u3002 \u9ad8\u5206\u8fa8\u7387\u5e94\u7528\uff1a \u533b\u7597\u5f71\u50cf\u5206\u6790\u3001\u9065\u611f\u56fe\u50cf\u5904\u7406\u3001\u4e13\u4e1a\u8bbe\u8ba1\u9886\u57df\u7b49\u5bf9\u9ad8\u5206\u8fa8\u7387\u8f93\u5165\u6709\u9700\u6c42\u7684\u573a\u666f\uff0c\u5c06\u53d7\u76ca\u4e8e\u5176\u9ad8\u6548\u5904\u7406\u9ad8\u5206\u8fa8\u7387\u8f93\u5165\u7684\u80fd\u529b\u3002","title":"4. \u76f8\u5173\u5e94\u7528\u9886\u57df (Related Areas or Applications that Might Benefit from this Research)"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-04/#5-any-limitations-that-can-be-inferred-from-the-abstract","text":"\u8bad\u7ec3\u6210\u672c\uff1a \u5c3d\u7ba1\u6458\u8981\u5f3a\u8c03\u4e86\u63a8\u7406\u6548\u7387\uff0c\u4f46\u8bad\u7ec3\u4e00\u4e2a\u7eaf\u89e3\u7801\u5668\u3001\u7edf\u4e00\u7684\u591a\u6a21\u6001\u6a21\u578b\uff0c\u7279\u522b\u662f\u5305\u542bMoE\u7ed3\u6784\u548c\u591a\u5c3a\u5ea6\u673a\u5236\u7684\u6a21\u578b\uff0c\u5176 \u8ba1\u7b97\u8d44\u6e90\u548c\u6570\u636e\u9700\u6c42\u53ef\u80fd\u975e\u5e38\u5de8\u5927 \uff0c\u8fd9\u5728\u6458\u8981\u4e2d\u672a\u63d0\u53ca\u3002 MoE\u7684\u590d\u6742\u6027\uff1a \u6a21\u6001\u7279\u5b9a\u7684MoE\u7ed3\u6784\u867d\u7136\u5e26\u6765\u4e86\u7075\u6d3b\u6027\uff0c\u4f46\u5176\u8def\u7531\u673a\u5236\u7684\u590d\u6742\u6027\u3001\u8d1f\u8f7d\u5747\u8861\u4ee5\u53ca\u5982\u4f55\u6709\u6548\u8bad\u7ec3\u4ee5\u907f\u514d\u201c\u4e13\u5bb6\u574d\u584c\u201d\u7b49\u95ee\u9898\uff0c\u53ef\u80fd\u9700\u8981\u7cbe\u7ec6\u7684\u8bbe\u8ba1\u548c\u8c03\u4f18\u3002 \u81ea\u56de\u5f52\u6a21\u578b\u7684\u56fa\u6709\u6311\u6218\uff1a \u5c3d\u7ba1\u58f0\u79f0\u6027\u80fd\u4f18\u8d8a\uff0c\u4f46\u7eaf\u81ea\u56de\u5f52\u6a21\u578b\u5728\u5904\u7406\u67d0\u4e9b\u590d\u6742\u7684\u751f\u6210\u4efb\u52a1\u65f6\uff0c\u5982\u957f\u8ddd\u79bb\u4e00\u81f4\u6027\u3001\u65b0\u9896\u6027\u6216\u591a\u6837\u6027\u65b9\u9762\uff0c\u53ef\u80fd\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u4e0e\u4e13\u95e8\u7684\u6269\u6563\u6a21\u578b\u8fdb\u884c\u7ec6\u81f4\u6bd4\u8f83\u65f6\u3002\u6458\u8981\u4e2d\u672a\u8be6\u7ec6\u8bf4\u660e\u751f\u6210\u8d28\u91cf\u7684\u8fd9\u4e9b\u65b9\u9762\u3002 \u201c\u5f00\u6e90\u201d\u9650\u5b9a\uff1a \u8bba\u6587\u58f0\u79f0\u8d85\u8d8a\u4e86\u201c\u73b0\u6709\u5f00\u6e90\u7edf\u4e00\u591a\u6a21\u6001\u6a21\u578b\u201d\uff0c\u8fd9\u53ef\u80fd\u610f\u5473\u7740\u5176\u6027\u80fd\u5c1a\u672a\u4e0e\u6240\u6709\u6700\u5148\u8fdb\u7684\uff08\u5305\u62ec\u95ed\u6e90\u6216\u975e\u7edf\u4e00\u4f46\u7279\u5b9a\u4efb\u52a1\u8868\u73b0\u5353\u8d8a\u7684\uff09\u6a21\u578b\uff09\u8fdb\u884c\u5168\u9762\u6bd4\u8f83\u3002 \u591a\u5c3a\u5ea6\u673a\u5236\u7684\u6cdb\u5316\u6027\uff1a \u591a\u5c3a\u5ea6\u89c6\u89c9\u81ea\u56de\u5f52\u673a\u5236\u5728\u4e0d\u540c\u5206\u8fa8\u7387\u3001\u4e0d\u540c\u6a21\u6001\u7ec4\u5408\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u3002 Key Findings: We introduce OneCAT, a unified multimodal model that seamlessly integrates understanding, generation, and editing within a novel, pure decoder-only transformer architecture. Furthermore, we pioneer a multi-scale visual autoregressive mechanism within the Large Language Model (LLM) that drastically reduces decoding steps compared to diffusion-based methods while maintaining state-of-the-art performance. As a result, OneCAT sets a new performance standard, outperforming existing open-source unified multimodal models across benchmarks for multimodal generation, editing, and understanding. Links: PDF arXiv","title":"5. \u6f5c\u5728\u5c40\u9650\u6027 (Any Limitations That Can Be Inferred from the Abstract)"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-04/#gencompositor-generative-video-compositing-with-diffusion-transformer","text":"Authors: Shuzhou Yang, Xiaoyu Li, Xiaodong Cun, Guangzhi Wang, Lingen Li, Ying Shan, Jian Zhang Published: 2025-09-02 Categories: cs.CV Abstract: Video compositing combines live-action footage to create video production, serving as a crucial technique in video creation and film production. Traditional pipelines require intensive labor efforts and expert collaboration, resulting in lengthy production cycles and high manpower costs. To address this issue, we automate this process with generative models, called generative video compositing. This new task strives to adaptively inject identity and motion information of foreground video to the target video in an interactive manner, allowing users to customize the size, motion trajectory, and other attributes of the dynamic elements added in final video. Specifically, we designed a novel Diffusion Transformer (DiT) pipeline based on its intrinsic properties. To maintain consistency of the target video before and after editing, we revised a light-weight DiT-based background preservation branch with masked token injection. As to inherit dynamic elements from other sources, a DiT fusion block is proposed using full self-attention, along with a simple yet effective foreground augmentation for training. Besides, for fusing background and foreground videos with different layouts based on user control, we developed a novel position embedding, named Extended Rotary Position Embedding (ERoPE). Finally, we curated a dataset comprising 61K sets of videos for our new task, called VideoComp. This data includes complete dynamic elements and high-quality target videos. Experiments demonstrate that our method effectively realizes generative video compositing, outperforming existing possible solutions in fidelity and consistency. Analysis: \u4f5c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u4e13\u5bb6\uff0c\u6211\u5bf9\u8fd9\u7bc7\u8bba\u6587\u6458\u8981\u8fdb\u884c\u5982\u4e0b\u5206\u6790\uff1a","title":"GenCompositor: Generative Video Compositing with Diffusion Transformer"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-04/#gencompositor-generative-video-compositing-with-diffusion-transformer_1","text":"1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u660e\u6458\u8981 (Concise Summary) \u672c\u6587\u63d0\u51fa\u4e86GenCompositor\uff0c\u4e00\u4e2a\u57fa\u4e8eDiffusion Transformer\uff08DiT\uff09\u7684\u751f\u6210\u5f0f\u89c6\u9891\u5408\u6210\u7cfb\u7edf\uff0c\u65e8\u5728\u81ea\u52a8\u5316\u4f20\u7edf\u89c6\u9891\u5408\u6210\u4e2d\u8017\u65f6\u8017\u529b\u7684\u8fc7\u7a0b\u3002\u5b83\u5141\u8bb8\u7528\u6237\u4ea4\u4e92\u5f0f\u5730\u5c06\u524d\u666f\u89c6\u9891\u7684\u8eab\u4efd\u548c\u8fd0\u52a8\u4fe1\u606f\u81ea\u9002\u5e94\u5730\u6ce8\u5165\u5230\u76ee\u6807\u89c6\u9891\u4e2d\uff0c\u5e76\u63d0\u4f9b\u5bf9\u524d\u666f\u5143\u7d20\u5927\u5c0f\u3001\u8fd0\u52a8\u8f68\u8ff9\u7b49\u5c5e\u6027\u7684\u5b9a\u5236\u5316\u63a7\u5236\u3002\u4e3a\u652f\u6301\u8fd9\u4e00\u65b0\u4efb\u52a1\uff0c\u4f5c\u8005\u8fd8\u6784\u5efa\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u7684\u89c6\u9891\u5408\u6210\u6570\u636e\u96c6VideoComp\u3002 2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u5b66\u65b9\u6cd5 (Key Innovation or Methodological Approach) GenCompositor\u7684\u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u5176\u4e3a\u751f\u6210\u5f0f\u89c6\u9891\u5408\u6210\u4efb\u52a1\u91cf\u8eab\u5b9a\u5236\u7684Diffusion Transformer (DiT) \u7ba1\u9053\u3002\u5177\u4f53\u5305\u62ec\uff1a DiT-based \u80cc\u666f\u4fdd\u6301\u5206\u652f\uff1a \u8bbe\u8ba1\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684DiT\u5206\u652f\uff0c\u901a\u8fc7\u63a9\u7801\u4ee4\u724c\u6ce8\u5165\uff08masked token injection\uff09\u673a\u5236\uff0c\u786e\u4fdd\u76ee\u6807\u89c6\u9891\u5728\u7f16\u8f91\u524d\u540e\u7684\u80cc\u666f\u4e00\u81f4\u6027\uff0c\u8fd9\u662f\u89c6\u9891\u7f16\u8f91\u4efb\u52a1\u4e2d\u4fdd\u6301\u8fde\u8d2f\u6027\u7684\u5173\u952e\u3002 DiT \u878d\u5408\u5757\u4e0e\u524d\u666f\u589e\u5f3a\uff1a \u63d0\u51fa\u4e86\u4e00\u4e2a\u91c7\u7528\u5168\u81ea\u6ce8\u610f\u529b\uff08full self-attention\uff09\u7684DiT\u878d\u5408\u5757\uff0c\u7528\u4e8e\u9ad8\u6548\u5730\u5c06\u524d\u666f\u52a8\u6001\u5143\u7d20\u7ee7\u627f\u5e76\u6ce8\u5165\u5230\u89c6\u9891\u4e2d\uff0c\u5e76\u8f85\u4ee5\u7b80\u5355\u800c\u6709\u6548\u7684\u524d\u666f\u589e\u5f3a\u7b56\u7565\u8fdb\u884c\u8bad\u7ec3\u3002 \u6269\u5c55\u65cb\u8f6c\u4f4d\u7f6e\u7f16\u7801 (ERoPE)\uff1a \u9488\u5bf9\u7528\u6237\u63a7\u5236\u4e0b\u4e0d\u540c\u5e03\u5c40\uff08\u5982\u524d\u666f\u5927\u5c0f\u3001\u4f4d\u7f6e\uff09\u7684\u80cc\u666f\u548c\u524d\u666f\u89c6\u9891\u878d\u5408\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4f4d\u7f6e\u7f16\u7801ERoPE\uff0c\u4ee5\u66f4\u597d\u5730\u5904\u7406\u7a7a\u95f4\u5e03\u5c40\u7684\u7075\u6d3b\u6027\u3002 \u5927\u89c4\u6a21\u65b0\u6570\u636e\u96c6VideoComp\uff1a \u4e3a\u8fd9\u4e00\u65b0\u4efb\u52a1\u7cbe\u5fc3\u7b56\u5212\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b61K\u89c6\u9891\u96c6\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5305\u542b\u5b8c\u6574\u7684\u52a8\u6001\u5143\u7d20\u548c\u9ad8\u8d28\u91cf\u7684\u76ee\u6807\u89c6\u9891\uff0c\u4e3a\u751f\u6210\u5f0f\u89c6\u9891\u5408\u6210\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u8d44\u6e90\u3002 3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd (Potential Impact on the Field) \u89c6\u9891\u5236\u4f5c\u6548\u7387\u9769\u547d\uff1a \u6781\u5927\u5730\u81ea\u52a8\u5316\u4e86\u4f20\u7edf\u4e0a\u52b3\u52a8\u5bc6\u96c6\u578b\u4e14\u6210\u672c\u9ad8\u6602\u7684\u89c6\u9891\u5408\u6210\u6d41\u7a0b\uff0c\u6709\u671b\u663e\u8457\u7f29\u77ed\u5236\u4f5c\u5468\u671f\u5e76\u964d\u4f4e\u4eba\u529b\u6210\u672c\uff0c\u4ece\u800c\u52a0\u901f\u89c6\u9891\u5185\u5bb9\u521b\u4f5c\u3002 \u6280\u672f\u666e\u60e0\u6027\uff1a \u5c06\u590d\u6742\u7684\u89c6\u9891\u5408\u6210\u6280\u672f\u4ece\u4e13\u4e1a\u4eba\u58eb\u6269\u5c55\u5230\u66f4\u5e7f\u6cdb\u7684\u7528\u6237\u7fa4\u4f53\uff0c\u964d\u4f4e\u4e86\u89c6\u9891\u521b\u4f5c\u7684\u95e8\u69db\uff0c\u8d4b\u80fd\u666e\u901a\u7528\u6237\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u4e2a\u6027\u5316\u89c6\u9891\u3002 \u65b0\u4efb\u52a1\u4e0e\u65b0\u8303\u5f0f\uff1a \u6b63\u5f0f\u5b9a\u4e49\u5e76\u89e3\u51b3\u4e86\u201c\u751f\u6210\u5f0f\u89c6\u9891\u5408\u6210\u201d\u8fd9\u4e00\u65b0\u4efb\u52a1\uff0c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u5728\u89c6\u9891\u751f\u6210\u4e0e\u7f16\u8f91\u65b9\u5411\u5f00\u8f9f\u4e86\u65b0\u7684\u7814\u7a76\u8303\u5f0f\u548c\u57fa\u51c6\u3002 DiT\u5e94\u7528\u62d3\u5c55\uff1a \u8fdb\u4e00\u6b65\u5c55\u793a\u4e86Diffusion Transformer\u5728\u590d\u6742\u89c6\u9891\u751f\u6210\u548c\u7f16\u8f91\u4efb\u52a1\u4e2d\u7684\u5f3a\u5927\u6f5c\u529b\uff0c\u63a8\u52a8\u4e86DiT\u6a21\u578b\u5728\u89c6\u9891\u7406\u89e3\u548c\u751f\u6210\u9886\u57df\u7684\u5e94\u7528\u8fb9\u754c\u3002 4. \u76f8\u5173\u9886\u57df\u6216\u5e94\u7528\u53d7\u76ca (Related Areas or Applications that Might Benefit) \u7535\u5f71\u4e0e\u7535\u89c6\u5236\u4f5c\uff1a \u663e\u8457\u7b80\u5316\u7279\u6548\u5408\u6210\u3001\u573a\u666f\u6784\u5efa\u3001\u7eff\u5e55\u62a0\u50cf\u540e\u7684\u5143\u7d20\u878d\u5408\u7b49\u73af\u8282\u3002 \u77ed\u89c6\u9891\u4e0e\u793e\u4ea4\u5a92\u4f53\u5185\u5bb9\u521b\u4f5c\uff1a \u8d4b\u80fd\u666e\u901a\u7528\u6237\u5feb\u901f\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u4e2a\u6027\u5316\u7684\u89c6\u9891\u5185\u5bb9\uff0c\u5982\u5728\u73b0\u6709\u89c6\u9891\u4e2d\u6dfb\u52a0\u865a\u62df\u89d2\u8272\u3001\u52a8\u6001\u8d34\u7eb8\u6216\u7279\u6548\u3002 \u5e7f\u544a\u4e0e\u8425\u9500\uff1a \u5feb\u901f\u5236\u4f5c\u5b9a\u5236\u5316\u7684\u4ea7\u54c1\u6f14\u793a\u6216\u5ba3\u4f20\u89c6\u9891\uff0c\u5b9e\u73b0A/B\u6d4b\u8bd5\u548c\u4e2a\u6027\u5316\u6295\u653e\u3002 \u865a\u62df\u73b0\u5b9e (VR) / \u589e\u5f3a\u73b0\u5b9e (AR)\uff1a \u4e3a\u865a\u62df\u73af\u5883\u6216AR\u5e94\u7528\u4e2d\u7684\u52a8\u6001\u5143\u7d20\u751f\u6210\u63d0\u4f9b\u652f\u6301\uff0c\u5b9e\u73b0\u66f4\u771f\u5b9e\u7684\u865a\u62df\u4e0e\u73b0\u5b9e\u878d\u5408\u3002 \u6e38\u620f\u5f00\u53d1\uff1a \u8f85\u52a9\u751f\u6210\u6e38\u620f\u5185\u8fc7\u573a\u52a8\u753b\u6216\u52a8\u6001\u573a\u666f\u5143\u7d20\uff0c\u63d0\u5347\u5f00\u53d1\u6548\u7387\u3002 \u4e13\u4e1a\u89c6\u9891\u7f16\u8f91\u8f6f\u4ef6\uff1a \u4f5c\u4e3a\u63d2\u4ef6\u6216\u6838\u5fc3\u529f\u80fd\u96c6\u6210\uff0c\u63d0\u5347\u73b0\u6709\u5de5\u5177\uff08\u5982Adobe Premiere, DaVinci Resolve\uff09\u7684\u667a\u80fd\u5316\u6c34\u5e73\u3002 5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u7684\u5c40\u9650\u6027 (Limitations that Can Be Inferred from the Abstract) \u80cc\u666f\u4fdd\u6301\u7684\u9c81\u68d2\u6027\uff1a \u5c3d\u7ba1\u63d0\u51fa\u4e86\u8f7b\u91cf\u7ea7\u80cc\u666f\u4fdd\u6301\u5206\u652f\uff0c\u4f46\u5728\u6781\u7aef\u590d\u6742\u7684\u80cc\u666f\u53d8\u5316\u3001\u5149\u7167\u6761\u4ef6\u6216\u524d\u666f\u4e0e\u80cc\u666f\u6df1\u5ea6\u4ea4\u4e92\u573a\u666f\u4e0b\uff0c\u5176\u4fdd\u6301\u4e00\u81f4\u6027\u7684\u80fd\u529b\u53ef\u80fd\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u53ef\u80fd\u51fa\u73b0\u7ec6\u5fae\u7684\u4f2a\u5f71\u3002 \u524d\u666f\u878d\u5408\u7684\u771f\u5b9e\u611f\u4e0e\u591a\u6837\u6027\uff1a \u6458\u8981\u4e2d\u63d0\u5230\u201c\u7b80\u5355\u800c\u6709\u6548\u7684\u524d\u666f\u589e\u5f3a\u201d\uff0c\u8fd9\u53ef\u80fd\u610f\u5473\u7740\u5728\u5904\u7406\u9ad8\u5ea6\u590d\u6742\u6216\u9700\u8981\u7cbe\u7ec6\u7269\u7406\u4ea4\u4e92\uff08\u5982\u5149\u5f71\u3001\u53cd\u5c04\u3001\u906e\u6321\uff09\u7684\u524d\u666f\u5143\u7d20\u65f6\uff0c\u5176\u751f\u6210\u7684\u591a\u6837\u6027\u548c\u771f\u5b9e\u611f\u53ef\u80fd\u5b58\u5728\u5c40\u9650\u3002 \u7528\u6237\u63a7\u5236\u7684\u7c92\u5ea6\u4e0e\u590d\u6742\u6027\uff1a \u5c3d\u7ba1\u5141\u8bb8\u7528\u6237\u5b9a\u5236\u5927\u5c0f\u3001\u8f68\u8ff9\u7b49\u5c5e\u6027\uff0c\u4f46\u5bf9\u4e8e\u66f4\u9ad8\u7ea7\u7684\u4ea4\u4e92\uff08\u5982\u524d\u666f\u4e0e\u80cc\u666f\u7684\u7269\u7406\u78b0\u649e\u3001\u5149\u5f71\u878d\u5408\u3001\u6750\u8d28\u5339\u914d\u3001\u8bed\u4e49\u7406\u89e3\u9a71\u52a8\u7684\u4ea4\u4e92\u7b49\uff09\uff0c\u6458\u8981\u4e2d\u672a\u660e\u786e\u63d0\u53ca\uff0c\u53ef\u80fd\u4ecd\u9700\u8fdb\u4e00\u6b65\u63a2\u7d22\u3002 \u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\uff1a Diffusion Transformer\u6a21\u578b\u901a\u5e38\u8ba1\u7b97\u6210\u672c\u8f83\u9ad8\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u89c6\u9891\u6570\u636e\u65f6\u3002\u5c3d\u7ba1\u90e8\u5206\u6a21\u5757\u8bbe\u8ba1\u4e3a\u201c\u8f7b\u91cf\u7ea7\u201d\uff0c\u4f46\u6574\u4f53\u6a21\u578b\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u901f\u5ea6\u4ecd\u53ef\u80fd\u662f\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u4e00\u4e2a\u8003\u91cf\u56e0\u7d20\uff0c\u7279\u522b\u662f\u5728\u5b9e\u65f6\u5e94\u7528\u573a\u666f\u4e0b\u3002 \u6570\u636e\u96c6\u7684\u6cdb\u5316\u6027\uff1a \u5c3d\u7ba1\u6784\u5efa\u4e86VideoComp\u6570\u636e\u96c6\uff0c\u4f46\u4efb\u4f55\u6570\u636e\u96c6\u90fd\u5b58\u5728\u5176\u56fa\u6709\u7684\u5206\u5e03\u548c\u504f\u5dee\u3002\u6a21\u578b\u5728\u9762\u5bf9\u4e0e\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u5dee\u5f02\u8f83\u5927\u7684\u771f\u5b9e\u4e16\u754c\u89c6\u9891\u65f6\uff0c\u5176\u6cdb\u5316\u80fd\u529b\u6709\u5f85\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u3002 \u201c\u73b0\u6709\u53ef\u884c\u65b9\u6848\u201d\u7684\u6bd4\u8f83\u57fa\u51c6\uff1a \u8bba\u6587\u63d0\u5230\u8d85\u8d8a\u201c\u73b0\u6709\u53ef\u884c\u65b9\u6848\u201d\uff0c\u8fd9\u53ef\u80fd\u6697\u793a\u76ee\u524d\u5c1a\u65e0\u9488\u5bf9\u201c\u751f\u6210\u5f0f\u89c6\u9891\u5408\u6210\u201d\u8fd9\u4e00\u65b0\u4efb\u52a1\u7684\u76f4\u63a5\u3001\u6210\u719f\u7684\u57fa\u51c6\u6a21\u578b\u3002\u56e0\u6b64\uff0c\u6bd4\u8f83\u7684\u516c\u5e73\u6027\u548c\u5168\u9762\u6027\u53ef\u80fd\u9700\u8981\u66f4\u8be6\u7ec6\u7684\u5b9e\u9a8c\u5206\u6790\u6765\u652f\u6491\uff0c\u4ee5\u786e\u4fdd\u5176\u4f18\u52bf\u662f\u9488\u5bf9\u540c\u7b49\u96be\u5ea6\u548c\u76ee\u6807\u7684\u65b9\u6cd5\u3002 Key Findings: This new task strives to adaptively inject identity and motion information of foreground video to the target video in an interactive manner, allowing users to customize the size, motion trajectory, and other attributes of the dynamic elements added in final video. Specifically, we designed a novel Diffusion Transformer (DiT) pipeline based on its intrinsic properties. Besides, for fusing background and foreground videos with different layouts based on user control, we developed a novel position embedding, named Extended Rotary Position Embedding (ERoPE). Finally, we curated a dataset comprising 61K sets of videos for our new task, called VideoComp. Experiments demonstrate that our method effectively realizes generative video compositing, outperforming existing possible solutions in fidelity and consistency. Links: PDF arXiv","title":"GenCompositor: Generative Video Compositing with Diffusion Transformer"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-04/#enhancing-robustness-in-post-processing-watermarking-an-ensemble-attack-network-using-cnns-and-transformers","text":"Authors: Tzuhsuan Huang, Cheng Yu Yeo, Tsai-Ling Huang, Hong-Han Shuai, Wen-Huang Cheng, Jun-Cheng Chen Published: 2025-09-03 Categories: cs.CV Abstract: Recent studies on deep watermarking have predominantly focused on in-processing watermarking, which integrates the watermarking process into image generation. However, post-processing watermarking, which embeds watermarks after image generation, offers more flexibility. It can be applied to outputs from any generative model (e.g. GANs, diffusion models) without needing access to the model's internal structure. It also allows users to embed unique watermarks into individual images. Therefore, this study focuses on post-processing watermarking and enhances its robustness by incorporating an ensemble attack network during training. We construct various versions of attack networks using CNN and Transformer in both spatial and frequency domains to investigate how each combination influences the robustness of the watermarking model. Our results demonstrate that combining a CNN-based attack network in the spatial domain with a Transformer-based attack network in the frequency domain yields the highest robustness in watermarking models. Extensive evaluation on the WAVES benchmark, using average bit accuracy as the metric, demonstrates that our ensemble attack network significantly enhances the robustness of baseline watermarking methods under various stress tests. In particular, for the Regeneration Attack defined in WAVES, our method improves StegaStamp by 18.743%. The code is released at:https://github.com/aiiu-lab/DeepRobustWatermark. Analysis: \u8fd9\u7bc7\u8bba\u6587\u4e13\u6ce8\u4e8e\u63d0\u5347\u540e\u5904\u7406\u6c34\u5370\uff08Post-Processing Watermarking\uff09\u7684\u9c81\u68d2\u6027\uff0c\u8fd9\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\uff0c\u5c24\u5176\u662f\u5728AI\u751f\u6210\u5185\u5bb9\u65e5\u76ca\u666e\u53ca\u7684\u80cc\u666f\u4e0b\uff0c\u5177\u6709\u91cd\u8981\u7684\u7814\u7a76\u610f\u4e49\u3002","title":"Enhancing Robustness in Post-Processing Watermarking: An Ensemble Attack Network Using CNNs and Transformers"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-04/#1-concise-summary_1","text":"\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5f15\u5165\u96c6\u6210\u653b\u51fb\u7f51\u7edc\uff08Ensemble Attack Network\uff09\u6765\u589e\u5f3a\u540e\u5904\u7406\u6c34\u5370\u9c81\u68d2\u6027\u7684\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u7cfb\u7edf\u5730\u63a2\u7d22\u4e86\u7ed3\u5408\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u548cTransformer\u67b6\u6784\uff0c\u5e76\u5728\u7a7a\u95f4\u57df\u548c\u9891\u7387\u57df\u6784\u5efa\u653b\u51fb\u7f51\u7edc\uff0c\u53d1\u73b0CNN\uff08\u7a7a\u95f4\u57df\uff09\u4e0eTransformer\uff08\u9891\u7387\u57df\uff09\u7684\u7ec4\u5408\u80fd\u663e\u8457\u63d0\u9ad8\u6c34\u5370\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002\u5728WAVES\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u5404\u79cd\u538b\u529b\u6d4b\u8bd5\u4e0b\uff0c\u7279\u522b\u662f\u518d\u751f\u653b\u51fb\uff08Regeneration Attack\uff09\u4e2d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u57fa\u7ebf\u6c34\u5370\u65b9\u6cd5\u7684\u6027\u80fd\u3002","title":"1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u660e\u6458\u8981 (Concise Summary)"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-04/#2-key-innovation-or-methodological-approach_1","text":"\u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u63d0\u51fa\u5e76\u7cfb\u7edf\u6027\u5730\u63a2\u7d22\u4e86\u4e00\u79cd\u7528\u4e8e\u8bad\u7ec3\u9c81\u68d2\u540e\u5904\u7406\u6c34\u5370\u6a21\u578b\u7684\u201c\u96c6\u6210\u653b\u51fb\u7f51\u7edc\u201d\uff08Ensemble Attack Network\uff09\u3002\u8be5\u7f51\u7edc\u5728\u8bad\u7ec3\u9636\u6bb5\u6a21\u62df\u591a\u79cd\u6f5c\u5728\u653b\u51fb\uff0c\u901a\u8fc7\u7ed3\u5408\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u548cTransformer\u67b6\u6784\uff0c\u5e76\u5728\u7a7a\u95f4\u57df\u548c\u9891\u7387\u57df\u540c\u65f6\u8fdb\u884c\u653b\u51fb\u5efa\u6a21\u3002\u8fd9\u79cd\u591a\u6a21\u6001\u3001\u591a\u57df\u7684\u96c6\u6210\u653b\u51fb\u7b56\u7565\uff0c\u4f7f\u5f97\u6c34\u5370\u6a21\u578b\u80fd\u591f\u5b66\u4e60\u62b5\u5fa1\u66f4\u5e7f\u6cdb\u3001\u66f4\u590d\u6742\u7684\u653b\u51fb\u7c7b\u578b\uff0c\u4ece\u800c\u663e\u8457\u63d0\u5347\u5176\u9c81\u90a6\u6027\u3002\u8bba\u6587\u8fd8\u901a\u8fc7\u5b9e\u9a8c\u786e\u5b9a\u4e86CNN\uff08\u7a7a\u95f4\u57df\uff09\u4e0eTransformer\uff08\u9891\u7387\u57df\uff09\u7684\u6700\u4f73\u7ec4\u5408\u3002","title":"2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u8bba (Key Innovation or Methodological Approach)"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-04/#3-potential-impact-on-the-field_1","text":"\u9274\u4e8e\u5f53\u524d\u751f\u6210\u6a21\u578b\uff08\u5982GANs\u3001\u6269\u6563\u6a21\u578b\uff09\u7684\u98de\u901f\u53d1\u5c55\uff0cAI\u751f\u6210\u5185\u5bb9\u7684\u7248\u6743\u4fdd\u62a4\u3001\u6eaf\u6e90\u548c\u771f\u5b9e\u6027\u9a8c\u8bc1\u53d8\u5f97\u65e5\u76ca\u91cd\u8981\u3002\u672c\u6587\u63d0\u51fa\u7684\u9c81\u68d2\u540e\u5904\u7406\u6c34\u5370\u6280\u672f\uff0c\u4e3a\u8fd9\u4e9b\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7075\u6d3b\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002\u5b83\u4f7f\u5f97\u6c34\u5370\u80fd\u591f\u72ec\u7acb\u4e8e\u751f\u6210\u6a21\u578b\u5d4c\u5165\uff0c\u6781\u5927\u5730\u6269\u5c55\u4e86\u5176\u5e94\u7528\u8303\u56f4\u3002\u5176\u5bf9\u6c34\u5370\u9c81\u68d2\u6027\u7684\u663e\u8457\u63d0\u5347\uff0c\u6709\u671b\u63a8\u52a8\u540e\u5904\u7406\u6c34\u5370\u6280\u672f\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u666e\u53ca\uff0c\u6210\u4e3aAI\u751f\u6210\u5185\u5bb9\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u5173\u952e\u7ec4\u6210\u90e8\u5206\uff0c\u5c24\u5176\u662f\u5728\u5185\u5bb9\u771f\u5b9e\u6027\u3001\u7248\u6743\u5f52\u5c5e\u548c\u6570\u5b57\u8d44\u4ea7\u7ba1\u7406\u65b9\u9762\u3002","title":"3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd (Potential Impact on the Field)"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-04/#4-related-areas-or-applications","text":"AI\u751f\u6210\u5185\u5bb9\u7684\u7248\u6743\u4fdd\u62a4\u4e0e\u6eaf\u6e90\uff1a \u786e\u4fddAI\u751f\u6210\u56fe\u50cf\u3001\u89c6\u9891\u7b49\u5185\u5bb9\u7684\u539f\u521b\u6027\uff0c\u9632\u6b62\u672a\u7ecf\u6388\u6743\u7684\u4f7f\u7528\uff0c\u5e76\u8ffd\u8e2a\u5176\u6765\u6e90\u3002 \u6570\u5b57\u5185\u5bb9\u771f\u5b9e\u6027\u9a8c\u8bc1\uff1a \u8f85\u52a9\u8bc6\u522bAI\u751f\u6210\u5185\u5bb9\uff0c\u5c24\u5176\u662f\u5728\u5bf9\u6297\u6df1\u5ea6\u4f2a\u9020\uff08Deepfake\uff09\u65b9\u9762\uff0c\u901a\u8fc7\u6c34\u5370\u7684\u5b58\u5728\u6216\u7f3a\u5931\u6765\u5224\u65ad\u5185\u5bb9\u7684\u6765\u6e90\u548c\u5b8c\u6574\u6027\u3002 \u6570\u5b57\u8d44\u4ea7\u7ba1\u7406\uff08DAM\uff09\uff1a \u4e3a\u5927\u89c4\u6a21AI\u751f\u6210\u5185\u5bb9\u5e93\u63d0\u4f9b\u6709\u6548\u7684\u7ba1\u7406\u548c\u8ffd\u8e2a\u673a\u5236\u3002 \u77e5\u8bc6\u4ea7\u6743\u4fdd\u62a4\uff1a \u4fdd\u62a4\u521b\u4f5c\u8005\u548c\u4f01\u4e1a\u7684\u6570\u5b57\u4f5c\u54c1\u514d\u53d7\u76d7\u7528\u3002 \u591a\u5a92\u4f53\u53d6\u8bc1\uff1a \u5728\u6570\u5b57\u56fe\u50cf\u548c\u89c6\u9891\u7684\u7be1\u6539\u68c0\u6d4b\u548c\u6765\u6e90\u5206\u6790\u4e2d\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u8bc1\u636e\u3002","title":"4. \u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 (Related Areas or Applications)"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-04/#5-limitations-inferred-from-the-abstract","text":"\u653b\u51fb\u7c7b\u578b\u8986\u76d6\u8303\u56f4\uff1a \u5c3d\u7ba1\u96c6\u6210\u4e86\u591a\u79cd\u653b\u51fb\u7f51\u7edc\uff0c\u4f46\u6458\u8981\u5e76\u672a\u8be6\u7ec6\u8bf4\u660e\u5176\u80fd\u6709\u6548\u62b5\u5fa1\u7684\u5177\u4f53\u653b\u51fb\u7c7b\u578b\u96c6\u5408\u3002\u5b9e\u9645\u5e94\u7528\u4e2d\u53ef\u80fd\u5b58\u5728\u672a\u88ab\u6a21\u578b\u8bad\u7ec3\u5145\u5206\u8986\u76d6\u7684\u65b0\u578b\u6216\u7ec4\u5408\u653b\u51fb\u3002 \u6c34\u5370\u5bb9\u91cf\u4e0e\u4e0d\u53ef\u611f\u77e5\u6027\uff1a \u6458\u8981\u4e3b\u8981\u5f3a\u8c03\u9c81\u68d2\u6027\uff0c\u4f46\u672a\u63d0\u53ca\u6c34\u5370\u7684\u5bb9\u91cf\uff08\u80fd\u5d4c\u5165\u591a\u5c11\u4fe1\u606f\uff09\u4ee5\u53ca\u5d4c\u5165\u6c34\u5370\u540e\u5bf9\u56fe\u50cf\u89c6\u89c9\u8d28\u91cf\u7684\u5f71\u54cd\uff08\u4e0d\u53ef\u611f\u77e5\u6027\uff09\u3002\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u9c81\u68d2\u6027\u3001\u5bb9\u91cf\u548c\u4e0d\u53ef\u611f\u77e5\u6027\u4e4b\u95f4\u5f80\u5f80\u5b58\u5728\u6743\u8861\u3002 \u8ba1\u7b97\u6210\u672c\uff1a \u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4f7f\u7528\u96c6\u6210\u653b\u51fb\u7f51\u7edc\u53ef\u80fd\u4f1a\u589e\u52a0\u8bad\u7ec3\u7684\u590d\u6742\u6027\u548c\u8ba1\u7b97\u6210\u672c\uff0c\u5c24\u5176\u662f\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u3002 \u6cdb\u5316\u80fd\u529b\uff1a \u5c3d\u7ba1\u5728WAVES\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5728\u9762\u5bf9\u5b8c\u5168\u672a\u77e5\u6216\u9ad8\u5ea6\u5b9a\u5236\u5316\u7684\u653b\u51fb\u65f6\u7684\u6cdb\u5316\u80fd\u529b\u4ecd\u9700\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u3002 \u7279\u5b9a\u653b\u51fb\u7684\u5c40\u9650\u6027\uff1a \u6458\u8981\u4e2d\u63d0\u5230\u5bf9\u201c\u518d\u751f\u653b\u51fb\u201d\u7684\u663e\u8457\u63d0\u5347\uff0818.743%\uff09\uff0c\u8fd9\u6697\u793a\u4e86\u8be5\u65b9\u6cd5\u53ef\u80fd\u5728\u67d0\u4e9b\u7279\u5b9a\u653b\u51fb\u7c7b\u578b\u4e0a\u8868\u73b0\u5c24\u4e3a\u7a81\u51fa\uff0c\u4f46\u5728\u5176\u4ed6\u653b\u51fb\u7c7b\u578b\u4e0a\u7684\u63d0\u5347\u5e45\u5ea6\u53ef\u80fd\u6709\u6240\u4e0d\u540c\u3002 Key Findings: In particular, for the Regeneration Attack defined in WAVES, our method improves StegaStamp by 18.743%. Links: PDF arXiv","title":"5. \u53ef\u4ece\u6458\u8981\u63a8\u65ad\u7684\u5c40\u9650\u6027 (Limitations Inferred from the Abstract)"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-04/#tinydrop-tiny-model-guided-token-dropping-for-vision-transformers","text":"Authors: Guoxin Wang, Qingyuan Wang, Binhua Huang, Shaowu Chen, Deepu John Published: 2025-09-03 Categories: cs.CV, cs.AI Abstract: Vision Transformers (ViTs) achieve strong performance in image classification but incur high computational costs from processing all image tokens. To reduce inference costs in large ViTs without compromising accuracy, we propose TinyDrop, a training-free token dropping framework guided by a lightweight vision model. The guidance model estimates the importance of tokens while performing inference, thereby selectively discarding low-importance tokens if large vit models need to perform attention calculations. The framework operates plug-and-play, requires no architectural modifications, and is compatible with diverse ViT architectures. Evaluations on standard image classification benchmarks demonstrate that our framework reduces FLOPs by up to 80% for ViTs with minimal accuracy degradation, highlighting its generalization capability and practical utility for efficient ViT-based classification. Analysis: \u597d\u7684\uff0c\u4f5c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u4e13\u5bb6\uff0c\u6211\u5bf9\u8fd9\u7bc7\u8bba\u6587\u6458\u8981\u8fdb\u884c\u5982\u4e0b\u5206\u6790\uff1a","title":"TinyDrop: Tiny Model Guided Token Dropping for Vision Transformers"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-04/#tinydrop-tiny-model-guided-token-dropping-for-vision-transformers_1","text":"1. \u8bba\u6587\u6838\u5fc3\u8d21\u732e\u7684\u7b80\u6d01\u603b\u7ed3 (Concise Summary) \u672c\u6587\u63d0\u51fa\u4e86TinyDrop\uff0c\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\uff08\u9488\u5bf9\u4e3bViT\u6a21\u578b\uff09token\u4e22\u5f03\u6846\u67b6\uff0c\u65e8\u5728\u663e\u8457\u964d\u4f4e\u5927\u578bVision Transformers (ViTs)\u7684\u63a8\u7406\u8ba1\u7b97\u6210\u672c\u3002\u5b83\u901a\u8fc7\u5229\u7528\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u89c6\u89c9\u6a21\u578b\u5728\u63a8\u7406\u65f6\u52a8\u6001\u8bc4\u4f30\u5e76\u9009\u62e9\u6027\u5730\u4e22\u5f03\u4f4e\u91cd\u8981\u6027token\uff0c\u4ece\u800c\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u5c06ViTs\u7684FLOPs\u51cf\u5c11\u9ad8\u8fbe80%\u3002 2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u5b66\u65b9\u6cd5 (Key Innovation or Methodological Approach) \u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u5176\u201c\u8f7b\u91cf\u7ea7\u6a21\u578b\u5f15\u5bfc\u7684token\u4e22\u5f03\u201d\u7b56\u7565\u3002TinyDrop\u5f15\u5165\u4e86\u4e00\u4e2a\u72ec\u7acb\u7684\u3001\u65e0\u9700\u8bad\u7ec3\u7684\uff08\u9488\u5bf9\u4e3bViT\u800c\u8a00\uff09\u6846\u67b6\uff0c\u5229\u7528\u4e00\u4e2a\u5c0f\u578b\u8f85\u52a9\u6a21\u578b\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5b9e\u65f6\u8bc4\u4f30ViT\u8f93\u5165token\u7684\u91cd\u8981\u6027\u3002\u8fd9\u79cd\u65b9\u6cd5\u5141\u8bb8\u52a8\u6001\u5730\u3001\u9009\u62e9\u6027\u5730\u4e22\u5f03\u4f4e\u91cd\u8981\u6027token\uff0c\u800c\u65e0\u9700\u4fee\u6539\u539f\u59cbViT\u67b6\u6784\u6216\u8fdb\u884c\u989d\u5916\u7684\u5fae\u8c03\uff0c\u5b9e\u73b0\u4e86\u5373\u63d2\u5373\u7528\uff0c\u5e76\u517c\u5bb9\u591a\u79cdViT\u67b6\u6784\u3002 3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd (Potential Impact on the Field) \u8be5\u7814\u7a76\u5bf9\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u5177\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u5c24\u5176\u662f\u5728ViT\u6a21\u578b\u7684\u5b9e\u9645\u90e8\u7f72\u65b9\u9762\u3002\u5b83\u663e\u8457\u964d\u4f4e\u4e86\u5927\u578bViT\u6a21\u578b\u7684\u63a8\u7406\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u7cbe\u5ea6\uff0c\u8fd9\u4f7f\u5f97ViT\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8bbe\u5907\uff08\u5982\u79fb\u52a8\u8bbe\u5907\u3001\u5d4c\u5165\u5f0f\u7cfb\u7edf\uff09\u4e0a\u53d8\u5f97\u66f4\u52a0\u53ef\u884c\u3002\u8fd9\u5c06\u52a0\u901fViT\u5728\u5404\u79cd\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u666e\u53ca\uff0c\u63a8\u52a8\u9ad8\u6548\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u5e76\u53ef\u80fd\u542f\u53d1\u66f4\u591a\u5173\u4e8e\u6a21\u578b\u538b\u7f29\u548c\u63a8\u7406\u4f18\u5316\u7684\u7814\u7a76\u65b9\u5411\u3002 4. \u53ef\u80fd\u53d7\u76ca\u7684\u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 (Related Areas or Applications) \u9ad8\u6548\u56fe\u50cf\u5206\u7c7b: \u8fd9\u662f\u6458\u8981\u4e2d\u660e\u786e\u63d0\u5230\u7684\u4e3b\u8981\u5e94\u7528\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u5feb\u901f\u54cd\u5e94\u7684\u573a\u666f\u3002 \u8fb9\u7f18\u8ba1\u7b97\u4e0e\u79fb\u52a8AI: \u5728\u8ba1\u7b97\u8d44\u6e90\u548c\u529f\u8017\u53d7\u9650\u7684\u8bbe\u5907\u4e0a\u90e8\u7f72\u9ad8\u6027\u80fdViT\u6a21\u578b\uff0c\u5982\u667a\u80fd\u624b\u673a\u3001\u7269\u8054\u7f51\u8bbe\u5907\u3002 \u5b9e\u65f6\u89c6\u89c9\u7cfb\u7edf: \u9700\u8981\u4f4e\u5ef6\u8fdf\u63a8\u7406\u7684\u5e94\u7528\uff0c\u5982\u81ea\u52a8\u9a7e\u9a76\u3001\u673a\u5668\u4eba\u89c6\u89c9\u3001\u89c6\u9891\u76d1\u63a7\u3002 ViT\u4f5c\u4e3a\u9aa8\u5e72\u7f51\u7edc\u7684\u5176\u4ed6\u4efb\u52a1: \u867d\u7136\u6458\u8981\u53ea\u63d0\u5230\u4e86\u5206\u7c7b\uff0c\u4f46\u5982\u679cViT\u88ab\u7528\u4f5c\u76ee\u6807\u68c0\u6d4b\u3001\u8bed\u4e49\u5206\u5272\u3001\u59ff\u6001\u4f30\u8ba1\u7b49\u4efb\u52a1\u7684\u9aa8\u5e72\u7f51\u7edc\uff0cTinyDrop\u7406\u8bba\u4e0a\u4e5f\u80fd\u63d0\u4f9b\u6548\u7387\u63d0\u5347\u3002 \u53ef\u6301\u7eedAI/\u7eff\u8272AI: \u51cf\u5c11\u6a21\u578b\u8fd0\u884c\u7684\u8ba1\u7b97\u91cf\u548c\u80fd\u8017\uff0c\u7b26\u5408\u5f53\u524d\u5bf9\u53ef\u6301\u7eedAI\u53d1\u5c55\u7684\u9700\u6c42\u3002 5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u7684\u5c40\u9650\u6027 (Limitations that can be inferred from the abstract) \u8f7b\u91cf\u7ea7\u5f15\u5bfc\u6a21\u578b\u7684\u5f00\u9500: \u6458\u8981\u672a\u660e\u786e\u6307\u51fa\u8f7b\u91cf\u7ea7\u5f15\u5bfc\u6a21\u578b\u672c\u8eab\u7684\u8ba1\u7b97\u5f00\u9500\u3002\u867d\u7136\u5b83\u5f88\u8f7b\u91cf\uff0c\u4f46\u5728\u6781\u7aef\u8d44\u6e90\u53d7\u9650\u7684\u573a\u666f\u4e0b\uff0c\u5176\u63a8\u7406\u65f6\u95f4\u6216FLOPs\u662f\u5426\u5b8c\u5168\u53ef\u4ee5\u5ffd\u7565\u4e0d\u8ba1\uff1f\u4ee5\u53ca\u8fd9\u4e2a\u5f15\u5bfc\u6a21\u578b\u662f\u5982\u4f55\u83b7\u53d6\u7684\uff08\u662f\u5426\u9700\u8981\u9884\u8bad\u7ec3\u6216\u7279\u5b9a\u4efb\u52a1\u8bad\u7ec3\uff09\uff1f \u201c\u6700\u5c0f\u7cbe\u5ea6\u4e0b\u964d\u201d\u7684\u91cf\u5316: \u5c3d\u7ba1\u58f0\u79f0\u7cbe\u5ea6\u4e0b\u964d\u6700\u5c0f\uff0c\u4f46\u5728\u67d0\u4e9b\u5bf9\u7cbe\u5ea6\u8981\u6c42\u6781\u9ad8\u7684\u5e94\u7528\u4e2d\uff0c\u5373\u4f7f\u662f\u5fae\u5c0f\u7684\u4e0b\u964d\u4e5f\u53ef\u80fd\u65e0\u6cd5\u63a5\u53d7\u3002\u5177\u4f53\u7684\u7cbe\u5ea6-\u6548\u7387\u6743\u8861\u66f2\u7ebf\uff08Pareto Frontier\uff09\u672a\u5728\u6458\u8981\u4e2d\u8be6\u7ec6\u8bf4\u660e\u3002 FLOPs\u4e0e\u5b9e\u9645\u5ef6\u8fdf: FLOPs\u7684\u51cf\u5c11\u4e0d\u603b\u662f\u76f4\u63a5\u7b49\u540c\u4e8e\u5b9e\u9645\u63a8\u7406\u5ef6\u8fdf\u7684\u7ebf\u6027\u964d\u4f4e\u3002token\u4e22\u5f03\u64cd\u4f5c\u672c\u8eab\u4ee5\u53ca\u5f15\u5bfc\u6a21\u578b\u7684\u63a8\u7406\u53ef\u80fd\u4f1a\u5f15\u5165\u989d\u5916\u7684\u5185\u5b58\u8bbf\u95ee\u3001\u6761\u4ef6\u5206\u652f\u7b49\u5f00\u9500\uff0c\u8fd9\u53ef\u80fd\u5f71\u54cd\u5b9e\u9645\u7684\u7aef\u5230\u7aef\u5ef6\u8fdf\u3002 \u4e22\u5f03\u7b56\u7565\u7684\u9c81\u68d2\u6027: \u5c3d\u7ba1\u58f0\u79f0\u5177\u6709\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u5bf9\u4e8e\u4e0d\u540cViT\u67b6\u6784\u3001\u4e0d\u540c\u4efb\u52a1\uff08\u4f8b\u5982\uff0c\u4e0d\u4ec5\u4ec5\u662f\u5206\u7c7b\uff0c\u8fd8\u6709\u68c0\u6d4b\u3001\u5206\u5272\u7b49\uff09\u4ee5\u53ca\u4e0d\u540c\u590d\u6742\u5ea6\u7684\u56fe\u50cf\uff0c\u5176token\u91cd\u8981\u6027\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u548c\u4e22\u5f03\u7b56\u7565\u7684\u9c81\u68d2\u6027\u5982\u4f55\uff0c\u4ecd\u9700\u66f4\u6df1\u5165\u7684\u9a8c\u8bc1\u3002 \u201c\u8bad\u7ec3-free\u201d\u7684\u8303\u56f4: \u6458\u8981\u5f3a\u8c03\u6846\u67b6\u662f\u201c\u8bad\u7ec3-free\u201d\u7684\uff0c\u8fd9\u901a\u5e38\u6307\u4e3bViT\u6a21\u578b\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002\u4f46\u8f7b\u91cf\u7ea7\u5f15\u5bfc\u6a21\u578b\u672c\u8eab\u662f\u5426\u9700\u8981\u8bad\u7ec3\uff1f\u5982\u679c\u662f\uff0c\u5176\u8bad\u7ec3\u6210\u672c\u548c\u6570\u636e\u9700\u6c42\u5982\u4f55\uff1f\u8fd9\u53ef\u80fd\u662f\u4e00\u4e2a\u9690\u85cf\u7684\u6210\u672c\u3002 Key Findings: To reduce inference costs in large ViTs without compromising accuracy, we propose TinyDrop, a training-free token dropping framework guided by a lightweight vision model. Links: PDF arXiv","title":"TinyDrop: Tiny Model Guided Token Dropping for Vision Transformers"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-04/#lgbp-organet-learnable-gaussian-band-pass-fusion-of-cnn-and-transformer-features-for-robust-organoid-segmentation-and-tracking","text":"Authors: Jing Zhang, Siying Tao, Jiao Li, Tianhe Wang, Junchen Wu, Ruqian Hao, Xiaohui Du, Ruirong Tan, Rui Li Published: 2025-09-03 Categories: cs.CV, cs.AI Abstract: Organoids replicate organ structure and function, playing a crucial role in fields such as tumor treatment and drug screening. Their shape and size can indicate their developmental status, but traditional fluorescence labeling methods risk compromising their structure. Therefore, this paper proposes an automated, non-destructive approach to organoid segmentation and tracking. We introduced the LGBP-OrgaNet, a deep learning-based system proficient in accurately segmenting, tracking, and quantifying organoids. The model leverages complementary information extracted from CNN and Transformer modules and introduces the innovative feature fusion module, Learnable Gaussian Band Pass Fusion, to merge data from two branches. Additionally, in the decoder, the model proposes a Bidirectional Cross Fusion Block to fuse multi-scale features, and finally completes the decoding through progressive concatenation and upsampling. SROrga demonstrates satisfactory segmentation accuracy and robustness on organoids segmentation datasets, providing a potent tool for organoid research. Analysis: \u4f5c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u4e13\u5bb6\uff0c\u6211\u5bf9\u8fd9\u7bc7\u8bba\u6587\u6458\u8981\u7684\u5206\u6790\u5982\u4e0b\uff1a","title":"LGBP-OrgaNet: Learnable Gaussian Band Pass Fusion of CNN and Transformer Features for Robust Organoid Segmentation and Tracking"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-04/#lgbp-organet-learnable-gaussian-band-pass-fusion-of-cnn-and-transformer-features-for-robust-organoid-segmentation-and-tracking_1","text":"1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u660e\u6458\u8981 (2-3 \u53e5\u8bdd) \u672c\u6587\u63d0\u51fa\u4e86LGBP-OrgaNet\uff0c\u4e00\u4e2a\u7528\u4e8e\u81ea\u52a8\u3001\u65e0\u635f\u7c7b\u5668\u5b98\u5206\u5272\u4e0e\u8ffd\u8e2a\u7684\u6df1\u5ea6\u5b66\u4e60\u7cfb\u7edf\u3002\u8be5\u6a21\u578b\u901a\u8fc7\u521b\u65b0\u7684\u53ef\u5b66\u4e60\u9ad8\u65af\u5e26\u901a\u878d\u5408\u6a21\u5757\u6574\u5408CNN\u548cTransformer\u7684\u4e92\u8865\u7279\u5f81\uff0c\u5e76\u5728\u89e3\u7801\u5668\u4e2d\u5f15\u5165\u53cc\u5411\u4ea4\u53c9\u878d\u5408\u5757\u5904\u7406\u591a\u5c3a\u5ea6\u4fe1\u606f\u3002\u5b83\u5728\u7c7b\u5668\u5b98\u5206\u5272\u6570\u636e\u96c6\u4e0a\u5c55\u73b0\u51fa\u4ee4\u4eba\u6ee1\u610f\u7684\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u7c7b\u5668\u5b98\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177\u3002 2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u5b66\u65b9\u6cd5 \u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u5176 \u53ef\u5b66\u4e60\u9ad8\u65af\u5e26\u901a\u878d\u5408\uff08Learnable Gaussian Band Pass Fusion\uff09\u6a21\u5757 \u3002\u8be5\u6a21\u5757\u80fd\u591f\u81ea\u9002\u5e94\u5730\u878d\u5408\u6765\u81eaCNN\uff08\u64c5\u957f\u5c40\u90e8\u7279\u5f81\u63d0\u53d6\uff09\u548cTransformer\uff08\u64c5\u957f\u5168\u5c40\u4f9d\u8d56\u5efa\u6a21\uff09\u5206\u652f\u7684\u4e92\u8865\u7279\u5f81\uff0c\u4ee5\u5b9e\u73b0\u66f4\u9c81\u68d2\u7684\u8868\u793a\u3002\u6b64\u5916\uff0c\u89e3\u7801\u5668\u4e2d\u63d0\u51fa\u7684 \u53cc\u5411\u4ea4\u53c9\u878d\u5408\u5757\uff08Bidirectional Cross Fusion Block\uff09 \u7528\u4e8e\u6709\u6548\u6574\u5408\u591a\u5c3a\u5ea6\u7279\u5f81\uff0c\u4ee5\u53ca\u6e10\u8fdb\u5f0f\u62fc\u63a5\u548c\u4e0a\u91c7\u6837\u5b8c\u6210\u89e3\u7801\uff0c\u4e5f\u662f\u4e00\u4e2a\u91cd\u8981\u7684\u6280\u672f\u8d21\u732e\u3002 3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd \u8be5\u7814\u7a76\u4e3a\u7c7b\u5668\u5b98\u7814\u7a76\u9886\u57df\u63d0\u4f9b\u4e86\u4e00\u4e2a \u81ea\u52a8\u5316\u3001\u65e0\u635f\u4e14\u9ad8\u7cbe\u5ea6 \u7684\u5206\u6790\u5de5\u5177\u3002\u901a\u8fc7\u66ff\u4ee3\u6216\u51cf\u5c11\u5bf9\u4f20\u7edf\u8367\u5149\u6807\u8bb0\u7b49\u7834\u574f\u6027\u65b9\u6cd5\u7684\u4f9d\u8d56\uff0c\u5b83\u5c06\u6781\u5927\u5730\u52a0\u901f\u7c7b\u5668\u5b98\u5728\u80bf\u7624\u6cbb\u7597\u3001\u836f\u7269\u7b5b\u9009\u548c\u53d1\u80b2\u751f\u7269\u5b66\u7b49\u9886\u57df\u7684\u5e94\u7528\u548c\u7814\u7a76\u8fdb\u7a0b\uff0c\u5b9e\u73b0\u66f4\u53ef\u9760\u3001\u66f4\u957f\u671f\u7684\u52a8\u6001\u89c2\u5bdf\u548c\u91cf\u5316\u5206\u6790\u3002\u8fd9\u79cd\u975e\u4fb5\u5165\u6027\u65b9\u6cd5\u5bf9\u4e8e\u4fdd\u6301\u7c7b\u5668\u5b98\u7684\u751f\u7406\u5b8c\u6574\u6027\u81f3\u5173\u91cd\u8981\u3002 4. \u53ef\u80fd\u53d7\u76ca\u4e8e\u8be5\u7814\u7a76\u7684\u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 \u751f\u7269\u533b\u5b66\u7814\u7a76\uff1a \u7c7b\u5668\u5b98\u7814\u7a76\u3001\u836f\u7269\u7b5b\u9009\u3001\u80bf\u7624\u6cbb\u7597\u3001\u53d1\u80b2\u751f\u7269\u5b66\u3001\u518d\u751f\u533b\u5b66\u3001\u75be\u75c5\u5efa\u6a21\u3002 \u9ad8\u901a\u91cf\u7b5b\u9009\uff1a \u81ea\u52a8\u5316\u5206\u6790\u80fd\u529b\u4f7f\u5176\u975e\u5e38\u9002\u5408\u5927\u89c4\u6a21\u7684\u836f\u7269\u6216\u57fa\u56e0\u529f\u80fd\u7b5b\u9009\uff0c\u63d0\u9ad8\u6548\u7387\u548c\u51c6\u786e\u6027\u3002 \u901a\u7528\u751f\u7269\u56fe\u50cf\u5206\u6790\uff1a \u4efb\u4f55\u9700\u8981\u5bf9\u590d\u6742\u3001\u5f62\u6001\u591a\u53d8\u7684\u751f\u7269\u7ed3\u6784\uff08\u5982\u7ec6\u80de\u7fa4\u3001\u7ec4\u7ec7\u5207\u7247\u3001\u5fae\u6d41\u63a7\u82af\u7247\u4e2d\u7684\u751f\u7269\u6837\u672c\uff09\u8fdb\u884c\u9c81\u68d2\u5206\u5272\u548c\u8ffd\u8e2a\u7684\u573a\u666f\uff0c\u90fd\u53ef\u80fd\u4ece\u5176\u878d\u5408\u67b6\u6784\u548c\u7279\u5f81\u5b66\u4e60\u7b56\u7565\u4e2d\u53d7\u76ca\u3002 \u663e\u5fae\u56fe\u50cf\u5904\u7406\uff1a \u5bf9\u4e8e\u5176\u4ed6\u9700\u8981\u4ece\u590d\u6742\u80cc\u666f\u4e2d\u7cbe\u786e\u63d0\u53d6\u76ee\u6807\u5e76\u8ffd\u8e2a\u5176\u52a8\u6001\u53d8\u5316\u7684\u663e\u5fae\u56fe\u50cf\u5206\u6790\u4efb\u52a1\uff0c\u8be5\u65b9\u6cd5\u4e5f\u53ef\u80fd\u5177\u6709\u501f\u9274\u610f\u4e49\u3002 5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u51fa\u7684\u5c40\u9650\u6027 \u6027\u80fd\u91cf\u5316\u4e0e\u6bd4\u8f83\uff1a \u6458\u8981\u4e2d\u4ec5\u63d0\u53ca\u201c\u4ee4\u4eba\u6ee1\u610f\u7684\u5206\u5272\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u201d\uff0c\u4f46\u7f3a\u4e4f\u4e0e\u73b0\u6709SOTA\u65b9\u6cd5\u8fdb\u884c\u5b9a\u91cf\u6bd4\u8f83\u7684\u5177\u4f53\u6570\u636e\uff0c\u56e0\u6b64\u5176\u76f8\u5bf9\u6027\u80fd\u4f18\u52bf\u5c1a\u4e0d\u660e\u786e\u3002 \u6cdb\u5316\u80fd\u529b\uff1a \u6a21\u578b\u5728\u201c\u7c7b\u5668\u5b98\u5206\u5272\u6570\u636e\u96c6\u201d\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u4f46\u5176\u5bf9\u4e0d\u540c\u7c7b\u578b\u7c7b\u5668\u5b98\uff08\u5982\u4e0d\u540c\u5668\u5b98\u6765\u6e90\u3001\u4e0d\u540c\u57f9\u517b\u6761\u4ef6\uff09\u3001\u4e0d\u540c\u6210\u50cf\u6a21\u6001\u6216\u66f4\u590d\u6742\u75c5\u7406\u60c5\u51b5\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u6709\u5f85\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u3002 \u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\uff1a \u7ed3\u5408CNN\u548cTransformer\u7684\u6df7\u5408\u67b6\u6784\uff0c\u4ee5\u53ca\u53ef\u5b66\u4e60\u7684\u9ad8\u65af\u5e26\u901a\u878d\u5408\u6a21\u5757\u548c\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\uff0c\u53ef\u80fd\u610f\u5473\u7740\u8f83\u9ad8\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u5185\u5b58\u6d88\u8017\uff0c\u8fd9\u5728\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u4e2d\u53ef\u80fd\u662f\u4e00\u4e2a\u8003\u91cf\u3002 \u201cSROrga\u201d\u7684\u6307\u4ee3\uff1a \u6458\u8981\u4e2d\u63d0\u5230\u201cSROrga demonstrates...\u201d\uff0c\u4f46\u6a21\u578b\u540d\u79f0\u662fLGBP-OrgaNet\uff0c\u8fd9\u53ef\u80fd\u662f\u7b14\u8bef\u6216\u6307\u4ee3\u4e0d\u660e\u786e\uff0c\u9700\u8981\u6f84\u6e05\u3002 Key Findings: Additionally, in the decoder, the model proposes a Bidirectional Cross Fusion Block to fuse multi-scale features, and finally completes the decoding through progressive concatenation and upsampling. SROrga demonstrates satisfactory segmentation accuracy and robustness on organoids segmentation datasets, providing a potent tool for organoid research. Links: PDF arXiv","title":"LGBP-OrgaNet: Learnable Gaussian Band Pass Fusion of CNN and Transformer Features for Robust Organoid Segmentation and Tracking"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-04/#towards-realistic-hand-object-interaction-with-gravity-field-based-diffusion-bridge","text":"Authors: Miao Xu, Xiangyu Zhu, Xusheng Liang, Zidu Wang, Jinlin Wu, Zhen Lei Published: 2025-09-03 Categories: cs.CV Abstract: Existing reconstruction or hand-object pose estimation methods are capable of producing coarse interaction states. However, due to the complex and diverse geometry of both human hands and objects, these approaches often suffer from interpenetration or leave noticeable gaps in regions that are supposed to be in contact. Moreover, the surface of a real human hand undergoes non-negligible deformations during interaction, which are difficult to capture and represent with previous methods. To tackle these challenges, we formulate hand-object interaction as an attraction-driven process and propose a Gravity-Field Based Diffusion Bridge (GravityDB) to simulate interactions between a deformable hand surface and rigid objects. Our approach effectively resolves the aforementioned issues by generating physically plausible interactions that are free of interpenetration, ensure stable grasping, and capture realistic hand deformations. Furthermore, we incorporate semantic information from textual descriptions to guide the construction of the gravitational field, enabling more semantically meaningful interaction regions. Extensive qualitative and quantitative experiments on multiple datasets demonstrate the effectiveness of our method. Analysis: \u8fd9\u7bc7\u8bba\u6587\u300aTowards Realistic Hand-Object Interaction with Gravity-Field Based Diffusion Bridge\u300b\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u51b3\u624b\u90e8\u4e0e\u7269\u4f53\u4ea4\u4e92\u4e2d\u5e38\u89c1\u6311\u6218\u7684\u65b0\u65b9\u6cd5\u3002","title":"Towards Realistic Hand-Object Interaction with Gravity-Field Based Diffusion Bridge"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-04/#1_1","text":"\u73b0\u6709\u624b\u90e8-\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u51e0\u4f55\u5f62\u72b6\u65f6\uff0c\u5e38\u5bfc\u81f4\u7269\u4f53\u7a7f\u900f\u6216\u63a5\u89e6\u533a\u57df\u51fa\u73b0\u660e\u663e\u95f4\u9699\uff0c\u4e14\u96be\u4ee5\u6355\u6349\u624b\u90e8\u5728\u4ea4\u4e92\u8fc7\u7a0b\u4e2d\u7684\u5f62\u53d8\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f15\u529b\u573a\u7684\u6269\u6563\u6865\uff08GravityDB\uff09\u6a21\u578b\uff0c\u5c06\u624b\u90e8-\u7269\u4f53\u4ea4\u4e92\u5efa\u6a21\u4e3a\u4e00\u79cd\u5438\u5f15\u9a71\u52a8\u8fc7\u7a0b\uff0c\u80fd\u591f\u751f\u6210\u65e0\u7a7f\u900f\u3001\u7a33\u5b9a\u6293\u53d6\u4e14\u6355\u6349\u771f\u5b9e\u624b\u90e8\u5f62\u53d8\u7684\u7269\u7406\u5408\u7406\u4ea4\u4e92\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u8fd8\u878d\u5165\u4e86\u6587\u672c\u63cf\u8ff0\u7684\u8bed\u4e49\u4fe1\u606f\u6765\u6307\u5bfc\u5f15\u529b\u573a\u6784\u5efa\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u5177\u8bed\u4e49\u610f\u4e49\u7684\u4ea4\u4e92\u533a\u57df\u3002","title":"1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u6d01\u603b\u7ed3"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-04/#2_2","text":"\u8be5\u8bba\u6587\u7684\u5173\u952e\u521b\u65b0\u5728\u4e8e\uff1a 1. \u5f15\u529b\u573a\u9a71\u52a8\u7684\u6269\u6563\u6865\u6a21\u578b (GravityDB) \uff1a\u5c06\u624b\u90e8-\u7269\u4f53\u4ea4\u4e92\u5efa\u6a21\u4e3a\u4e00\u79cd\u65b0\u9896\u7684\u201c\u5438\u5f15\u9a71\u52a8\u8fc7\u7a0b\u201d\uff0c\u5e76\u5229\u7528\u201c\u6269\u6563\u6865\u201d\u6846\u67b6\u6765\u6a21\u62df\u53ef\u53d8\u5f62\u624b\u90e8\u8868\u9762\u4e0e\u521a\u6027\u7269\u4f53\u4e4b\u95f4\u7684\u4ea4\u4e92\u3002\u8fd9\u76f4\u63a5\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u7269\u4f53\u7a7f\u900f\u3001\u95f4\u9699\u4ee5\u53ca\u624b\u90e8\u5f62\u53d8\u96be\u4ee5\u6355\u6349\u7684\u95ee\u9898\u3002 2. \u53ef\u53d8\u5f62\u624b\u90e8\u8868\u9762\u5efa\u6a21 \uff1a\u660e\u786e\u63d0\u51fa\u5e76\u89e3\u51b3\u4e86\u624b\u90e8\u5728\u4ea4\u4e92\u8fc7\u7a0b\u4e2d\u975e\u7ebf\u6027\u5f62\u53d8\u7684\u95ee\u9898\uff0c\u8fd9\u662f\u73b0\u6709\u65b9\u6cd5\u7684\u4e00\u4e2a\u4e3b\u8981\u75db\u70b9\u3002 3. \u8bed\u4e49\u4fe1\u606f\u878d\u5408 \uff1a\u521b\u65b0\u6027\u5730\u5c06\u6587\u672c\u63cf\u8ff0\u4e2d\u7684\u8bed\u4e49\u4fe1\u606f\u878d\u5165\u5230\u5f15\u529b\u573a\u7684\u6784\u5efa\u4e2d\uff0c\u4f7f\u5f97\u751f\u6210\u7684\u4ea4\u4e92\u4e0d\u4ec5\u5728\u7269\u7406\u4e0a\u5408\u7406\uff0c\u800c\u4e14\u5728\u8bed\u4e49\u4e0a\u4e5f\u66f4\u5177\u610f\u4e49\uff0c\u4f8b\u5982\uff0c\u6839\u636e\u6587\u672c\u63cf\u8ff0\u201c\u6293\u4f4f\u676f\u67c4\u201d\u800c\u975e\u201c\u6293\u4f4f\u676f\u8eab\u201d\u3002","title":"2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u8bba"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-04/#3_2","text":"\u63d0\u5347\u4ea4\u4e92\u771f\u5b9e\u611f\u548c\u7269\u7406\u5408\u7406\u6027 \uff1a\u901a\u8fc7\u89e3\u51b3\u7a7f\u900f\u3001\u95f4\u9699\u548c\u624b\u90e8\u5f62\u53d8\u95ee\u9898\uff0c\u8be5\u65b9\u6cd5\u5c06\u663e\u8457\u63d0\u9ad8\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u56fe\u5f62\u5b66\u4e2d\u624b\u90e8-\u7269\u4f53\u4ea4\u4e92\u7684\u771f\u5b9e\u611f\u548c\u7269\u7406\u53ef\u4fe1\u5ea6\uff0c\u4e3a\u66f4\u9ad8\u7ea7\u7684\u4ea4\u4e92\u5efa\u6a21\u5960\u5b9a\u57fa\u7840\u3002 \u63a8\u52a8\u591a\u6a21\u6001\u4ea4\u4e92\u7406\u89e3 \uff1a\u7ed3\u5408\u6587\u672c\u8bed\u4e49\u4fe1\u606f\u6765\u6307\u5bfc\u7269\u7406\u4ea4\u4e92\uff0c\u4e3a\u624b\u90e8-\u7269\u4f53\u4ea4\u4e92\u7814\u7a76\u5f15\u5165\u4e86\u591a\u6a21\u6001\u7684\u89c6\u89d2\uff0c\u53ef\u80fd\u542f\u53d1\u66f4\u591a\u7ed3\u5408\u8bed\u8a00\u7406\u89e3\u7684\u5177\u8eab\u667a\u80fd\u7814\u7a76\u3002 \u4e3a\u4e0b\u6e38\u4efb\u52a1\u63d0\u4f9b\u66f4\u9c81\u68d2\u7684\u8f93\u5165 \uff1a\u751f\u6210\u7684\u9ad8\u8d28\u91cf\u3001\u65e0\u7a7f\u900f\u3001\u5f62\u53d8\u51c6\u786e\u7684\u4ea4\u4e92\u72b6\u6001\uff0c\u53ef\u4ee5\u4f5c\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u3001\u865a\u62df\u73b0\u5b9e\u3001\u52a8\u753b\u5236\u4f5c\u7b49\u4e0b\u6e38\u4efb\u52a1\u66f4\u53ef\u9760\u7684\u8f93\u5165\u6570\u636e\u6216\u8bad\u7ec3\u76ee\u6807\u3002","title":"3. \u5bf9\u8be5\u9886\u57df\u7684\u6f5c\u5728\u5f71\u54cd"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-04/#4_2","text":"\u673a\u5668\u4eba\u5b66\u4e0e\u7075\u5de7\u64cd\u4f5c \uff1a\u4e3a\u673a\u5668\u4eba\u5b66\u4e60\u548c\u6267\u884c\u590d\u6742\u3001\u7cbe\u7ec6\u7684\u6293\u53d6\u548c\u64cd\u4f5c\u4efb\u52a1\u63d0\u4f9b\u66f4\u771f\u5b9e\u7684\u6a21\u62df\u73af\u5883\u548c\u8bad\u7ec3\u6570\u636e\u3002 \u865a\u62df\u73b0\u5b9e (VR) / \u589e\u5f3a\u73b0\u5b9e (AR) \uff1a\u63d0\u5347\u865a\u62df\u73af\u5883\u4e2d\u7528\u6237\u624b\u90e8\u4e0e\u865a\u62df\u7269\u4f53\u4ea4\u4e92\u7684\u6c89\u6d78\u611f\u548c\u771f\u5b9e\u611f\uff0c\u51cf\u5c11\u4e0d\u81ea\u7136\u7684\u7a7f\u900f\u73b0\u8c61\u3002 \u8ba1\u7b97\u673a\u56fe\u5f62\u5b66\u4e0e\u52a8\u753b \uff1a\u7528\u4e8e\u751f\u6210\u7535\u5f71\u3001\u6e38\u620f\u548c\u5176\u4ed6\u5a92\u4f53\u4e2d\u66f4\u903c\u771f\u3001\u66f4\u81ea\u7136\u7684\u865a\u62df\u89d2\u8272\u624b\u90e8\u4e0e\u7269\u4f53\u7684\u4ea4\u4e92\u52a8\u753b\u3002 \u4eba\u673a\u4ea4\u4e92 (HCI) \uff1a\u5f00\u53d1\u66f4\u667a\u80fd\u3001\u66f4\u76f4\u89c2\u7684\u57fa\u4e8e\u624b\u52bf\u7684\u4ea4\u4e92\u7cfb\u7edf\uff0c\u7406\u89e3\u7528\u6237\u610f\u56fe\u3002 3D \u5185\u5bb9\u521b\u4f5c \uff1a\u4e3a\u827a\u672f\u5bb6\u548c\u8bbe\u8ba1\u5e08\u63d0\u4f9b\u5de5\u5177\uff0c\u4ee5\u66f4\u9ad8\u6548\u5730\u521b\u5efa\u9ad8\u8d28\u91cf\u7684\u624b\u90e8-\u7269\u4f53\u4ea4\u4e92\u573a\u666f\u3002 \u624b\u90e8\u59ff\u6001\u4f30\u8ba1\u4e0e\u91cd\u5efa \uff1a\u4f5c\u4e3a\u540e\u5904\u7406\u6b65\u9aa4\u6216\u4f5c\u4e3a\u8bad\u7ec3\u6570\u636e\u589e\u5f3a\uff0c\u4ee5\u63d0\u9ad8\u73b0\u6709\u65b9\u6cd5\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002","title":"4. \u53ef\u80fd\u53d7\u76ca\u7684\u76f8\u5173\u9886\u57df\u6216\u5e94\u7528"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-04/#5_1","text":"\u5bf9\u8c61\u521a\u6027\u9650\u5236 \uff1a\u6458\u8981\u660e\u786e\u6307\u51fa\u662f\u201c\u53ef\u53d8\u5f62\u624b\u90e8\u8868\u9762\u548c\u521a\u6027\u7269\u4f53\u201d\u4e4b\u95f4\u7684\u4ea4\u4e92\u3002\u8fd9\u610f\u5473\u7740\u8be5\u65b9\u6cd5\u53ef\u80fd\u4e0d\u9002\u7528\u4e8e\u5904\u7406\u53ef\u53d8\u5f62\u7269\u4f53\uff08\u4f8b\u5982\uff0c\u6324\u538b\u6d77\u7ef5\u3001\u63c9\u634f\u9762\u56e2\uff09\u7684\u4ea4\u4e92\u573a\u666f\u3002 \u8ba1\u7b97\u6210\u672c \uff1a\u6269\u6563\u6a21\u578b\u901a\u5e38\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u9636\u6bb5\u90fd\u5177\u6709\u8f83\u9ad8\u7684\u8ba1\u7b97\u6210\u672c\u3002\u867d\u7136\u6458\u8981\u672a\u76f4\u63a5\u63d0\u53ca\uff0c\u4f46\u201c\u6269\u6563\u6865\u201d\u6a21\u578b\u53ef\u80fd\u610f\u5473\u7740\u5728\u5b9e\u65f6\u5e94\u7528\u6216\u5927\u89c4\u6a21\u573a\u666f\u4e2d\u5b58\u5728\u8ba1\u7b97\u6548\u7387\u7684\u6311\u6218\u3002 \u8bed\u4e49\u4fe1\u606f\u4f9d\u8d56 \uff1a\u8bed\u4e49\u4fe1\u606f\u7684\u8d28\u91cf\u548c\u8986\u76d6\u8303\u56f4\u5c06\u76f4\u63a5\u5f71\u54cd\u5f15\u529b\u573a\u7684\u6784\u5efa\u3002\u5982\u679c\u6587\u672c\u63cf\u8ff0\u6a21\u7cca\u3001\u4e0d\u51c6\u786e\u6216\u7f3a\u4e4f\u7ec6\u8282\uff0c\u53ef\u80fd\u4f1a\u9650\u5236\u4ea4\u4e92\u7684\u7cbe\u786e\u6027\u548c\u591a\u6837\u6027\u3002 \u4ea4\u4e92\u590d\u6742\u6027 \uff1a\u867d\u7136\u89e3\u51b3\u4e86\u7a7f\u900f\u548c\u5f62\u53d8\uff0c\u4f46\u5bf9\u4e8e\u66f4\u590d\u6742\u7684\u4ea4\u4e92\uff0c\u4f8b\u5982\u591a\u6307\u534f\u540c\u64cd\u4f5c\u3001\u5de5\u5177\u4f7f\u7528\u6216\u52a8\u6001\u3001\u957f\u65f6\u95f4\u7684\u4ea4\u4e92\u5e8f\u5217\uff0c\u5176\u5904\u7406\u80fd\u529b\u4ecd\u9700\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u3002 \u6cdb\u5316\u80fd\u529b \uff1a\u5728\u201c\u591a\u4e2a\u6570\u636e\u96c6\u201d\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u4f46\u5176\u5bf9\u672a\u89c1\u8fc7\u7684\u65b0\u9896\u7269\u4f53\u51e0\u4f55\u5f62\u72b6\u6216\u4ea4\u4e92\u7c7b\u578b\u7684\u6cdb\u5316\u80fd\u529b\u5982\u4f55\uff0c\u4ecd\u6709\u5f85\u6df1\u5165\u63a2\u8ba8\u3002 Key Findings: Our approach effectively resolves the aforementioned issues by generating physically plausible interactions that are free of interpenetration, ensure stable grasping, and capture realistic hand deformations. Extensive qualitative and quantitative experiments on multiple datasets demonstrate the effectiveness of our method. Links: PDF arXiv","title":"5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u7684\u5c40\u9650\u6027"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-04/#enhancing-zero-shot-pedestrian-attribute-recognition-with-synthetic-data-generation-a-comparative-study-with-image-to-image-diffusion-models","text":"Authors: Pablo Ayuso-Albizu, Juan C. SanMiguel, Pablo Carballeira Published: 2025-09-02 Categories: cs.CV Abstract: Pedestrian Attribute Recognition (PAR) involves identifying various human attributes from images with applications in intelligent monitoring systems. The scarcity of large-scale annotated datasets hinders the generalization of PAR models, specially in complex scenarios involving occlusions, varying poses, and diverse environments. Recent advances in diffusion models have shown promise for generating diverse and realistic synthetic images, allowing to expand the size and variability of training data. However, the potential of diffusion-based data expansion for generating PAR-like images remains underexplored. Such expansion may enhance the robustness and adaptability of PAR models in real-world scenarios. This paper investigates the effectiveness of diffusion models in generating synthetic pedestrian images tailored to PAR tasks. We identify key parameters of img2img diffusion-based data expansion; including text prompts, image properties, and the latest enhancements in diffusion-based data augmentation, and examine their impact on the quality of generated images for PAR. Furthermore, we employ the best-performing expansion approach to generate synthetic images for training PAR models, by enriching the zero-shot datasets. Experimental results show that prompt alignment and image properties are critical factors in image generation, with optimal selection leading to a 4.5% improvement in PAR recognition performance. Analysis: \u4f5c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u4e13\u5bb6\uff0c\u6211\u5bf9\u8fd9\u7bc7\u8bba\u6587\u6458\u8981\u8fdb\u884c\u5982\u4e0b\u5206\u6790\uff1a \u8bba\u6587\u6458\u8981\u5206\u6790\uff1aEnhancing Zero-Shot Pedestrian Attribute Recognition with Synthetic Data Generation: A Comparative Study with Image-To-Image Diffusion Models 1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u660e\u603b\u7ed3 (2-3 \u53e5\u8bdd) \u672c\u6587\u65e8\u5728\u89e3\u51b3\u884c\u4eba\u5c5e\u6027\u8bc6\u522b\uff08PAR\uff09\u4e2d\u6570\u636e\u7a00\u7f3a\uff0c\u5c24\u5176\u662f\u5728\u96f6\u6837\u672c\u573a\u666f\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u95ee\u9898\u3002\u4f5c\u8005\u63a2\u7d22\u4e86\u4f7f\u7528\u56fe\u50cf\u5230\u56fe\u50cf\uff08img2img\uff09\u6269\u6563\u6a21\u578b\u751f\u6210\u5408\u6210\u884c\u4eba\u56fe\u50cf\u7684\u6709\u6548\u6027\uff0c\u5e76\u7cfb\u7edf\u6027\u5730\u7814\u7a76\u4e86\u6587\u672c\u63d0\u793a\u3001\u56fe\u50cf\u5c5e\u6027\u7b49\u5173\u952e\u53c2\u6570\u5bf9\u751f\u6210\u8d28\u91cf\u7684\u5f71\u54cd\u3002\u901a\u8fc7\u5c06\u6700\u4f73\u751f\u6210\u65b9\u6cd5\u5e94\u7528\u4e8e\u96f6\u6837\u672c\u6570\u636e\u96c6\u7684\u6269\u5145\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660ePAR\u8bc6\u522b\u6027\u80fd\u83b7\u5f97\u4e864.5%\u7684\u63d0\u5347\u3002 2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u5b66\u65b9\u6cd5 \u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u7cfb\u7edf\u6027\u5730\u63a2\u7d22\u548c\u4f18\u5316\u56fe\u50cf\u5230\u56fe\u50cf\uff08img2img\uff09\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u9002\u7528\u4e8e\u884c\u4eba\u5c5e\u6027\u8bc6\u522b\uff08PAR\uff09\u4efb\u52a1\u7684\u5408\u6210\u6570\u636e\u65b9\u9762\u7684\u6f5c\u529b\u3002\u8bba\u6587\u8be6\u7ec6\u7814\u7a76\u4e86\u6587\u672c\u63d0\u793a\u3001\u56fe\u50cf\u5c5e\u6027\u4ee5\u53ca\u6269\u6563\u6a21\u578b\u6700\u65b0\u589e\u5f3a\u529f\u80fd\u7b49\u5173\u952e\u53c2\u6570\u5bf9\u751f\u6210\u56fe\u50cf\u8d28\u91cf\u7684\u5f71\u54cd\uff0c\u5e76\u9996\u6b21\u5c06\u8fd9\u79cd\u4f18\u5316\u540e\u7684\u5408\u6210\u6570\u636e\u751f\u6210\u65b9\u6cd5\u5e94\u7528\u4e8e\u96f6\u6837\u672cPAR\u6570\u636e\u96c6\u7684\u6269\u5145\uff0c\u4ee5\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002\u8fd9\u79cd\u9488\u5bf9\u7279\u5b9a\u4e0b\u6e38\u4efb\u52a1\uff08PAR\uff09\u7684\u751f\u6210\u53c2\u6570\u8c03\u4f18\u548c\u6570\u636e\u6269\u5145\u7b56\u7565\u662f\u5176\u65b9\u6cd5\u5b66\u7684\u4eae\u70b9\u3002 3. \u5bf9\u8be5\u9886\u57df\u7684\u6f5c\u5728\u5f71\u54cd \u8fd9\u9879\u7814\u7a76\u4e3a\u89e3\u51b3\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u4e2d\u666e\u904d\u5b58\u5728\u7684\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u662f\u5728\u884c\u4eba\u5c5e\u6027\u8bc6\u522b\uff08PAR\uff09\u7b49\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\u7684\u7ec6\u7c92\u5ea6\u8bc6\u522b\u4efb\u52a1\u4e2d\u3002\u901a\u8fc7\u5229\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u9ad8\u8d28\u91cf\u5408\u6210\u6570\u636e\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8PAR\u6a21\u578b\u5728\u590d\u6742\u771f\u5b9e\u573a\u666f\uff08\u5982\u906e\u6321\u3001\u59ff\u6001\u53d8\u5316\u3001\u591a\u6837\u73af\u5883\uff09\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\uff0c\u4ece\u800c\u63a8\u52a8\u667a\u80fd\u76d1\u63a7\u7cfb\u7edf\u7b49\u5e94\u7528\u7684\u53d1\u5c55\u3002\u6b64\u5916\uff0c\u5176\u65b9\u6cd5\u8bba\u53ef\u80fd\u4e3a\u5176\u4ed6\u6570\u636e\u53d7\u9650\u7684\u7ec6\u7c92\u5ea6\u8bc6\u522b\u6216\u96f6\u6837\u672c\u5b66\u4e60\u4efb\u52a1\u63d0\u4f9b\u501f\u9274\u3002 4. \u53ef\u80fd\u53d7\u76ca\u4e8e\u8fd9\u9879\u7814\u7a76\u7684\u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 * \u667a\u80fd\u76d1\u63a7\u7cfb\u7edf (Intelligent Monitoring Systems): \u76f4\u63a5\u5e94\u7528\uff0c\u63d0\u5347\u884c\u4eba\u5206\u6790\u80fd\u529b\uff0c\u5982\u5f02\u5e38\u884c\u4e3a\u68c0\u6d4b\u3001\u4eba\u5458\u68c0\u7d22\u7b49\u3002 * \u96f6\u6837\u672c/\u5c11\u6837\u672c\u5b66\u4e60 (Zero-shot/Few-shot Learning): \u4e3a\u6570\u636e\u7a00\u7f3a\u7684\u573a\u666f\u63d0\u4f9b\u6709\u6548\u7684\u6570\u636e\u6269\u5145\u7b56\u7565\uff0c\u964d\u4f4e\u5bf9\u5927\u91cf\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\u3002 * \u7ec6\u7c92\u5ea6\u56fe\u50cf\u8bc6\u522b (Fine-grained Image Recognition): \u4efb\u4f55\u9700\u8981\u8bc6\u522b\u5fae\u5c0f\u5dee\u5f02\u4f46\u6570\u636e\u91cf\u6709\u9650\u7684\u4efb\u52a1\uff0c\u5982\u52a8\u7269\u79cd\u7c7b\u8bc6\u522b\u3001\u4ea7\u54c1\u7f3a\u9677\u68c0\u6d4b\u3001\u65f6\u5c1a\u5546\u54c1\u5206\u7c7b\u7b49\u3002 * \u6570\u636e\u589e\u5f3a\u4e0e\u5408\u6210\u6570\u636e\u751f\u6210 (Data Augmentation and Synthetic Data Generation): \u4e3a\u8bad\u7ec3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u63d0\u4f9b\u66f4\u4e30\u5bcc\u3001\u591a\u6837\u7684\u6570\u636e\uff0c\u51cf\u5c11\u8fc7\u62df\u5408\u3002 * \u9886\u57df\u9002\u5e94 (Domain Adaptation): \u5408\u6210\u6570\u636e\u53ef\u7528\u4e8e\u5f25\u5408\u4e0d\u540c\u6570\u636e\u5206\u5e03\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u63d0\u9ad8\u6a21\u578b\u5728\u4e0d\u540c\u73af\u5883\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002 * \u9690\u79c1\u4fdd\u62a4AI (Privacy-preserving AI): \u5728\u67d0\u4e9b\u573a\u666f\u4e0b\uff0c\u5408\u6210\u6570\u636e\u53ef\u4ee5\u66ff\u4ee3\u654f\u611f\u7684\u771f\u5b9e\u6570\u636e\u8fdb\u884c\u6a21\u578b\u8bad\u7ec3\uff0c\u4ece\u800c\u4fdd\u62a4\u7528\u6237\u9690\u79c1\u3002 5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u51fa\u7684\u5c40\u9650\u6027 * \u6027\u80fd\u63d0\u5347\u5e45\u5ea6\uff1a 4.5%\u7684\u6027\u80fd\u63d0\u5347\u867d\u7136\u662f\u79ef\u6781\u7684\uff0c\u4f46\u53ef\u80fd\u8868\u660e\u5408\u6210\u6570\u636e\u5e76\u975e\u89e3\u51b3PAR\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u7684\u7ec8\u6781\u65b9\u6848\uff0c\u4ecd\u6709\u8fdb\u4e00\u6b65\u63d0\u5347\u7684\u7a7a\u95f4\uff0c\u6216\u8005\u9700\u8981\u7ed3\u5408\u5176\u4ed6\u6280\u672f\u3002 * \u53c2\u6570\u63a2\u7d22\u8303\u56f4\uff1a \u8bba\u6587\u867d\u7136\u63a2\u7d22\u4e86\u6587\u672c\u63d0\u793a\u548c\u56fe\u50cf\u5c5e\u6027\u7b49\u5173\u952e\u53c2\u6570\uff0c\u4f46\u6269\u6563\u6a21\u578b\u5177\u6709\u590d\u6742\u7684\u5185\u90e8\u673a\u5236\u548c\u4f17\u591a\u8d85\u53c2\u6570\uff0c\u6458\u8981\u4e2d\u672a\u660e\u786e\u8bf4\u660e\u662f\u5426\u5df2\u7a77\u5c3d\u6240\u6709\u5f71\u54cd\u751f\u6210\u8d28\u91cf\u548cPAR\u6027\u80fd\u7684\u5173\u952e\u56e0\u7d20\u3002 * \u751f\u6210\u6570\u636e\u8d28\u91cf\u7684\u8bc4\u4f30\uff1a \u6458\u8981\u4e2d\u63d0\u5230\u201c\u751f\u6210\u56fe\u50cf\u7684\u8d28\u91cf\u201d\uff0c\u4f46\u672a\u8be6\u7ec6\u8bf4\u660e\u9664\u4e86\u6700\u7ec8\u7684PAR\u8bc6\u522b\u6027\u80fd\u5916\uff0c\u5982\u4f55\u5177\u4f53\u91cf\u5316\u548c\u8bc4\u4f30\u8fd9\u4e9b\u5408\u6210\u56fe\u50cf\u5728\u8bed\u4e49\u4e00\u81f4\u6027\u3001\u5c5e\u6027\u591a\u6837\u6027\u3001\u771f\u5b9e\u611f\u7b49\u65b9\u9762\u7684\u8d28\u91cf\u3002 * \u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\uff1a \u6269\u6563\u6a21\u578b\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\u901a\u5e38\u9700\u8981\u5927\u91cf\u7684\u8ba1\u7b97\u8d44\u6e90\u548c\u65f6\u95f4\uff0c\u8fd9\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u53ef\u80fd\u662f\u4e00\u4e2a\u9650\u5236\u3002\u6458\u8981\u4e2d\u672a\u63d0\u53ca\u751f\u6210\u6570\u636e\u7684\u6548\u7387\u95ee\u9898\u3002 * \u6f5c\u5728\u7684\u5408\u6210\u6570\u636e\u504f\u5dee\uff1a \u5408\u6210\u6570\u636e\u53ef\u80fd\u7ee7\u627f\u6216\u653e\u5927\u539f\u59cb\u8bad\u7ec3\u6570\u636e\u6216\u6269\u6563\u6a21\u578b\u81ea\u8eab\u9884\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u504f\u5dee\uff0c\u4ece\u800c\u5f71\u54cdPAR\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u516c\u5e73\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002 * \u4ec5\u9650\u4e8e\u96f6\u6837\u672c\u573a\u666f\uff1a \u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u96f6\u6837\u672cPAR\u6570\u636e\u96c6\u7684\u6269\u5145\uff0c\u5176\u5728\u5168\u76d1\u7763\u6216\u5c11\u6837\u672cPAR\u573a\u666f\u4e0b\u7684\u6709\u6548\u6027\u672a\u5728\u6458\u8981\u4e2d\u63d0\u53ca\u3002 Key Findings: Experimental results show that prompt alignment and image properties are critical factors in image generation, with optimal selection leading to a 4.5% improvement in PAR recognition performance. Links: PDF arXiv","title":"Enhancing Zero-Shot Pedestrian Attribute Recognition with Synthetic Data Generation: A Comparative Study with Image-To-Image Diffusion Models"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-04/#medlitenet-lightweight-hybrid-medical-image-segmentation-model","text":"Authors: Pengyang Yu, Haoquan Wang, Gerard Marks, Tahar Kechadi, Laurence T. Yang, Sahraoui Dhelim, Nyothiri Aung Published: 2025-09-03 Categories: cs.CV, cs.AI Abstract: Accurate skin-lesion segmentation remains a key technical challenge for computer-aided diagnosis of skin cancer. Convolutional neural networks, while effective, are constrained by limited receptive fields and thus struggle to model long-range dependencies. Vision Transformers capture global context, yet their quadratic complexity and large parameter budgets hinder use on the small-sample medical datasets common in dermatology. We introduce the MedLiteNet, a lightweight CNN Transformer hybrid tailored for dermoscopic segmentation that achieves high precision through hierarchical feature extraction and multi-scale context aggregation. The encoder stacks depth-wise Mobile Inverted Bottleneck blocks to curb computation, inserts a bottleneck-level cross-scale token-mixing unit to exchange information between resolutions, and embeds a boundary-aware self-attention module to sharpen lesion contours. Analysis: \u4f5c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u4e13\u5bb6\uff0c\u6211\u5bf9\u8fd9\u7bc7\u8bba\u6587\u6458\u8981\u8fdb\u884c\u5982\u4e0b\u5206\u6790\uff1a","title":"MedLiteNet: Lightweight Hybrid Medical Image Segmentation Model"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-04/#medlitenet","text":"1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u660e\u603b\u7ed3 (2-3 \u53e5\u8bdd) \u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a MedLiteNet \u7684\u8f7b\u91cf\u7ea7 CNN-Transformer \u6df7\u5408\u6a21\u578b\uff0c\u4e13\u95e8\u7528\u4e8e\u76ae\u80a4\u955c\u56fe\u50cf\u7684\u76ae\u80a4\u75c5\u7076\u5206\u5272\u3002\u5b83\u65e8\u5728\u514b\u670d\u4f20\u7edf CNN \u5728\u957f\u8ddd\u79bb\u4f9d\u8d56\u5efa\u6a21\u4e0a\u7684\u5c40\u9650\u6027\u4ee5\u53ca Vision Transformer \u5728\u5c0f\u6837\u672c\u533b\u5b66\u6570\u636e\u96c6\u4e0a\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u53c2\u6570\u91cf\u8fc7\u5927\u7684\u95ee\u9898\u3002\u901a\u8fc7\u7ed3\u5408\u5206\u5c42\u7279\u5f81\u63d0\u53d6\u548c\u591a\u5c3a\u5ea6\u4e0a\u4e0b\u6587\u805a\u5408\uff0cMedLiteNet \u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u75c5\u7076\u5206\u5272\u3002 2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u5b66\u65b9\u6cd5 MedLiteNet \u7684\u5173\u952e\u521b\u65b0\u5728\u4e8e\u5176\u72ec\u7279\u7684\u6df7\u5408\u67b6\u6784\u8bbe\u8ba1\uff0c\u65e8\u5728\u5b9e\u73b0\u8f7b\u91cf\u5316\u548c\u9ad8\u7cbe\u5ea6\uff1a \u8f7b\u91cf\u7ea7\u7f16\u7801\u5668\u8bbe\u8ba1\uff1a \u7f16\u7801\u5668\u5806\u53e0\u4e86 \u6df1\u5ea6\u53ef\u5206\u79bb\u7684 Mobile Inverted Bottleneck Blocks (MobileNetV2 \u4e2d\u7684\u5012\u6b8b\u5dee\u5757) \uff0c\u4ee5\u6709\u6548\u6291\u5236\u8ba1\u7b97\u91cf\uff0c\u4f7f\u5176\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u548c\u5c0f\u6837\u672c\u6570\u636e\u96c6\u3002 \u8de8\u5c3a\u5ea6\u4ee4\u724c\u6df7\u5408\u5355\u5143 (Cross-scale Token-mixing Unit)\uff1a \u5728\u74f6\u9888\u5c42\uff08bottleneck-level\uff09\u63d2\u5165\u4e86 \u8de8\u5c3a\u5ea6\u4ee4\u724c\u6df7\u5408\u5355\u5143 \uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u673a\u5236\uff0c\u7528\u4e8e\u5728\u4e0d\u540c\u5206\u8fa8\u7387\u7684\u7279\u5f81\u4e4b\u95f4\u4ea4\u6362\u4fe1\u606f\u3002\u8fd9\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u6355\u83b7\u5168\u5c40\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u5f25\u8865\u4e86\u4f20\u7edf CNN \u611f\u53d7\u91ce\u6709\u9650\u7684\u7f3a\u70b9\uff0c\u540c\u65f6\u907f\u514d\u4e86\u6807\u51c6 Transformer \u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u3002 \u8fb9\u754c\u611f\u77e5\u81ea\u6ce8\u610f\u529b\u6a21\u5757 (Boundary-aware Self-attention Module)\uff1a \u6a21\u578b\u5d4c\u5165\u4e86 \u8fb9\u754c\u611f\u77e5\u81ea\u6ce8\u610f\u529b\u6a21\u5757 \uff0c\u4e13\u95e8\u7528\u4e8e\u9510\u5316\u75c5\u7076\u8f6e\u5ed3\u3002\u8fd9\u5bf9\u4e8e\u533b\u5b66\u56fe\u50cf\u5206\u5272\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u4e3a\u7cbe\u786e\u7684\u8fb9\u754c\u5bf9\u4e8e\u8bca\u65ad\u5177\u6709\u51b3\u5b9a\u6027\u610f\u4e49\u3002 3. \u5bf9\u8be5\u9886\u57df\u7684\u6f5c\u5728\u5f71\u54cd \u63d0\u5347\u76ae\u80a4\u764c\u8bca\u65ad\u7cbe\u5ea6\uff1a \u901a\u8fc7\u63d0\u4f9b\u66f4\u51c6\u786e\u3001\u66f4\u53ef\u9760\u7684\u76ae\u80a4\u75c5\u7076\u5206\u5272\u5de5\u5177\uff0cMedLiteNet \u6709\u671b\u663e\u8457\u63d0\u9ad8\u8ba1\u7b97\u673a\u8f85\u52a9\u8bca\u65ad\u76ae\u80a4\u764c\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u4ece\u800c\u53ef\u80fd\u6539\u5584\u60a3\u8005\u9884\u540e\u3002 \u63a8\u52a8\u8f7b\u91cf\u7ea7\u6df7\u5408\u6a21\u578b\u53d1\u5c55\uff1a \u8be5\u7814\u7a76\u4e3a\u5728\u8ba1\u7b97\u8d44\u6e90\u53d7\u9650\u548c\u6570\u636e\u7a00\u7f3a\u7684\u573a\u666f\u4e0b\uff0c\u6709\u6548\u7ed3\u5408 CNN \u7684\u5c40\u90e8\u7279\u5f81\u63d0\u53d6\u80fd\u529b\u548c Transformer \u7684\u5168\u5c40\u4e0a\u4e0b\u6587\u5efa\u6a21\u80fd\u529b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6210\u529f\u7684\u8303\u4f8b\u3002\u8fd9\u53ef\u80fd\u4f1a\u542f\u53d1\u66f4\u591a\u9488\u5bf9\u7279\u5b9a\u5e94\u7528\uff08\u5c24\u5176\u662f\u533b\u5b66\u5f71\u50cf\uff09\u7684\u8f7b\u91cf\u7ea7\u6df7\u5408\u67b6\u6784\u8bbe\u8ba1\u3002 \u89e3\u51b3\u533b\u5b66\u5f71\u50cf\u9886\u57df\u6311\u6218\uff1a \u6210\u529f\u89e3\u51b3\u4e86\u533b\u5b66\u5f71\u50cf\u9886\u57df\u5e38\u89c1\u7684\u201c\u5c0f\u6837\u672c\u6570\u636e\u201d\u548c\u201c\u957f\u8ddd\u79bb\u4f9d\u8d56\u201d\u4e4b\u95f4\u7684\u77db\u76fe\uff0c\u4e3a\u5176\u4ed6\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4efb\u52a1\uff08\u5982\u5668\u5b98\u5206\u5272\u3001\u80bf\u7624\u68c0\u6d4b\u7b49\uff09\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u3002 \u8fb9\u7f18\u8ba1\u7b97\u548c\u90e8\u7f72\uff1a \u5176\u8f7b\u91cf\u7ea7\u7279\u6027\u4f7f\u5176\u66f4\u6613\u4e8e\u90e8\u7f72\u5728\u4e34\u5e8a\u73af\u5883\u4e2d\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\uff0c\u5b9e\u73b0\u5b9e\u65f6\u6216\u8fd1\u5b9e\u65f6\u7684\u8bca\u65ad\u8f85\u52a9\u3002 4. \u53ef\u80fd\u53d7\u76ca\u4e8e\u8fd9\u9879\u7814\u7a76\u7684\u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 \u5176\u4ed6\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4efb\u52a1\uff1a \u4efb\u4f55\u9700\u8981\u7cbe\u786e\u5206\u5272\u3001\u5904\u7406\u5c0f\u6837\u672c\u6570\u636e\u96c6\u4e14\u5bf9\u8ba1\u7b97\u6548\u7387\u6709\u8981\u6c42\u7684\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4efb\u52a1\uff0c\u4f8b\u5982\uff1a CT/MRI \u56fe\u50cf\u4e2d\u7684\u5668\u5b98\u6216\u80bf\u7624\u5206\u5272\u3002 \u773c\u5e95\u56fe\u50cf\u4e2d\u7684\u89c6\u7f51\u819c\u8840\u7ba1\u5206\u5272\u3002 \u75c5\u7406\u5207\u7247\u4e2d\u7684\u7ec6\u80de\u6838\u6216\u7ec4\u7ec7\u533a\u57df\u5206\u5272\u3002 \u901a\u7528\u56fe\u50cf\u5206\u5272\uff1a \u5bf9\u4e8e\u5177\u6709\u590d\u6742\u8fb9\u754c\u3001\u9700\u8981\u540c\u65f6\u8003\u8651\u5c40\u90e8\u7ec6\u8282\u548c\u5168\u5c40\u4e0a\u4e0b\u6587\u7684\u901a\u7528\u56fe\u50cf\u5206\u5272\u4efb\u52a1\uff0c\u5c24\u5176\u662f\u5728\u6570\u636e\u96c6\u89c4\u6a21\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u3002 \u8f7b\u91cf\u7ea7\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u8bbe\u8ba1\uff1a \u5bf9\u5982\u4f55\u9ad8\u6548\u5730\u96c6\u6210\u4e0d\u540c\u7c7b\u578b\u795e\u7ecf\u7f51\u7edc\u6a21\u5757\uff08\u5982 CNN \u548c Transformer\uff09\u611f\u5174\u8da3\u7684\u7814\u7a76\u4eba\u5458\u3002 \u8ba1\u7b97\u673a\u8f85\u52a9\u8bca\u65ad\u7cfb\u7edf\uff1a \u4efb\u4f55\u65e8\u5728\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u6539\u8fdb\u8bca\u65ad\u6d41\u7a0b\u7684\u533b\u7597 AI \u5e94\u7528\u3002 5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u51fa\u7684\u5c40\u9650\u6027 \u7279\u5b9a\u9886\u57df\u6027\uff1a \u6458\u8981\u660e\u786e\u6307\u51fa\u6a21\u578b\u662f\u201c\u4e3a\u76ae\u80a4\u955c\u5206\u5272\u91cf\u8eab\u5b9a\u5236\u7684 (tailored for dermoscopic segmentation)\u201d\u3002\u8fd9\u610f\u5473\u7740\u5176\u5728\u5176\u4ed6\u533b\u5b66\u56fe\u50cf\u6a21\u6001\uff08\u5982 CT\u3001MRI\uff09\u6216\u901a\u7528\u56fe\u50cf\u5206\u5272\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u53ef\u80fd\u9700\u8981\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u548c\u8c03\u6574\u3002\u7279\u522b\u662f\u5176\u201c\u8fb9\u754c\u611f\u77e5\u81ea\u6ce8\u610f\u529b\u6a21\u5757\u201d\u53ef\u80fd\u9488\u5bf9\u76ae\u80a4\u75c5\u7076\u7684\u7279\u5b9a\u5f62\u6001\u8fdb\u884c\u4e86\u4f18\u5316\u3002 \u6027\u80fd\u91cf\u5316\u7f3a\u5931\uff1a \u6458\u8981\u4e2d\u63d0\u5230\u201c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6 (achieves high precision)\u201d\uff0c\u4f46\u6ca1\u6709\u63d0\u4f9b\u5177\u4f53\u7684\u91cf\u5316\u6307\u6807\uff08\u5982 Dice \u7cfb\u6570\u3001IoU\u3001\u51c6\u786e\u7387\u7b49\uff09\uff0c\u56e0\u6b64\u65e0\u6cd5\u8bc4\u4f30\u5176\u76f8\u5bf9\u4e8e\u73b0\u6709 SOTA \u65b9\u6cd5\u7684\u5b9e\u9645\u63d0\u5347\u5e45\u5ea6\u3002 \u6cdb\u5316\u80fd\u529b\uff1a \u5c3d\u7ba1\u65e8\u5728\u89e3\u51b3\u5c0f\u6837\u672c\u95ee\u9898\uff0c\u4f46\u201c\u5c0f\u6837\u672c\u533b\u5b66\u6570\u636e\u96c6\u201d\u7684\u5b9a\u4e49\u76f8\u5bf9\u5bbd\u6cdb\u3002\u6a21\u578b\u5728\u9762\u5bf9\u6781\u5ea6\u7a00\u7f3a\u6216\u9ad8\u5ea6\u5f02\u8d28\u6027\u7684\u6570\u636e\u96c6\u65f6\u7684\u6cdb\u5316\u80fd\u529b\u4ecd\u9700\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u3002 \u8bad\u7ec3\u590d\u6742\u6027\uff1a \u6df7\u5408\u67b6\u6784\uff0c\u5c24\u5176\u662f\u5305\u542b\u8de8\u5c3a\u5ea6\u4fe1\u606f\u4ea4\u6362\u548c\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684\uff0c\u901a\u5e38\u5728\u8bad\u7ec3\u4e0a\u53ef\u80fd\u6bd4\u7eaf\u7cb9\u7684 CNN \u66f4\u590d\u6742\uff0c\u53ef\u80fd\u9700\u8981\u66f4\u7cbe\u7ec6\u7684\u8d85\u53c2\u6570\u8c03\u4f18\u3002\u6458\u8981\u4e2d\u672a\u63d0\u53ca\u8bad\u7ec3\u7684\u96be\u5ea6\u6216\u8d44\u6e90\u9700\u6c42\u3002 \u8ba1\u7b97\u6548\u7387\u7684\u7edd\u5bf9\u503c\uff1a \u5c3d\u7ba1\u5f3a\u8c03\u201c\u8f7b\u91cf\u7ea7\u201d\u548c\u201c\u6291\u5236\u8ba1\u7b97\u91cf\u201d\uff0c\u4f46\u6ca1\u6709\u7ed9\u51fa\u5177\u4f53\u7684\u53c2\u6570\u91cf\u3001FLOPs \u6216\u63a8\u7406\u901f\u5ea6\u6570\u636e\uff0c\u56e0\u6b64\u65e0\u6cd5\u4e0e\u5176\u4ed6\u8f7b\u91cf\u7ea7\u6a21\u578b\u8fdb\u884c\u76f4\u63a5\u7684\u91cf\u5316\u6bd4\u8f83\u3002 Key Findings: We introduce the MedLiteNet, a lightweight CNN Transformer hybrid tailored for dermoscopic segmentation that achieves high precision through hierarchical feature extraction and multi-scale context aggregation. Links: PDF arXiv","title":"MedLiteNet: \u8f7b\u91cf\u7ea7\u6df7\u5408\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6a21\u578b"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-05/","text":"Arxiv Computer Vision Papers - 2025-09-05 Executive Summary \u597d\u7684\uff0c\u8fd9\u662f\u4e00\u4efd\u4e3a\u5fd9\u788c\u7684\u7814\u7a76\u4eba\u5458\u51c6\u5907\u7684 Arxiv \u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u6700\u65b0\u8bba\u6587\u6267\u884c\u6458\u8981\u3002 Arxiv \u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u6700\u65b0\u8bba\u6587\u6267\u884c\u6458\u8981 (2025-09-03) \u672c\u62a5\u544a\u65e8\u5728\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u5f53\u65e5 Arxiv \u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u6700\u65b0\u53d1\u8868\u8bba\u6587\u7684\u5feb\u901f\u6982\u89c8\uff0c\u91cd\u70b9\u5173\u6ce8\u4e3b\u8981\u8d8b\u52bf\u3001\u521b\u65b0\u4eae\u70b9\u3001\u65b0\u5174\u65b9\u5411\u53ca\u63a8\u8350\u9605\u8bfb\u3002 1. \u4e3b\u8981\u4e3b\u9898\u4e0e\u8d8b\u52bf (Main Themes & Trends) \u4eca\u5929\u7684\u8bba\u6587\u5c55\u793a\u4e86\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u51e0\u4e2a\u5173\u952e\u8d8b\u52bf\u7684\u6301\u7eed\u6df1\u5316\u548c\u4ea4\u53c9\u878d\u5408\uff1a \u6269\u6563\u6a21\u578b (Diffusion Models) \u7684\u5e7f\u6cdb\u5e94\u7528\u4e0e\u521b\u65b0: \u6269\u6563\u6a21\u578b\u7ee7\u7eed\u5728\u5404\u79cd\u4efb\u52a1\u4e2d\u5c55\u73b0\u5176\u5f3a\u5927\u80fd\u529b\uff0c\u4ece\u7a00\u758f3D\u6570\u636e\u91cd\u5efa\uff08\u6df1\u5ea6\u56fe\u6062\u590d\uff09\u5230\u89c6\u9891\u5149\u7167\u4f30\u8ba1\uff0c\u518d\u5230\u590d\u6742\u7684\u6587\u672c\u5230\u56fe\u50cf\u6545\u4e8b\u53ef\u89c6\u5316\u548c\u7f16\u8f91\u3002\u8fd9\u8868\u660e\u6269\u6563\u6a21\u578b\u6b63\u6210\u4e3a\u591a\u6a21\u6001\u751f\u6210\u548c\u611f\u77e5\u4efb\u52a1\u7684\u6838\u5fc3\u6280\u672f\u3002 \u57fa\u7840\u6a21\u578b (Foundation Models) \u7684\u6df1\u5316\u4e0e\u4e13\u4e1a\u5316: \u51fa\u73b0\u4e86\u65e8\u5728\u5b9e\u73b0\u7edf\u4e00\u7406\u89e3\u4e0e\u751f\u6210\u7684\u901a\u7528\u57fa\u7840\u6a21\u578b\uff0c\u4ee5\u53ca\u9488\u5bf9\u7279\u5b9a\u9886\u57df\uff08\u5982\u533b\u5b66\u5f71\u50cf\uff09\u7684\u4e13\u4e1a\u5316\u751f\u6210\u5f0f\u57fa\u7840\u6a21\u578b\uff0c\u9884\u793a\u7740\u672a\u6765AI\u7cfb\u7edf\u5c06\u66f4\u52a0\u901a\u7528\u6216\u5728\u7279\u5b9a\u9886\u57df\u8fbe\u5230\u4e13\u5bb6\u7ea7\u6c34\u5e73\u3002 \u89c6\u89c9 Transformer (Vision Transformers) \u7684\u6548\u7387\u4e0e\u9c81\u68d2\u6027\u4f18\u5316: \u9488\u5bf9 ViT \u7684\u8ba1\u7b97\u6548\u7387\u95ee\u9898\uff0c\u6709\u7814\u7a76\u63d0\u51fa\u4e86\u8f7b\u91cf\u5316\u7b56\u7565\uff08\u5982 token dropping\uff09\uff0c\u540c\u65f6\u4e5f\u6709\u5de5\u4f5c\u5173\u6ce8\u5176\u5728\u5bf9\u6297\u6027\u653b\u51fb\uff08\u5982\u6c34\u5370\uff09\u4e0b\u7684\u9c81\u68d2\u6027\u589e\u5f3a\u3002 \u591a\u6a21\u6001\u4e0e\u8de8\u9886\u57df\u878d\u5408 (Multimodality & Cross-Domain Fusion): \u8bba\u6587\u6db5\u76d6\u4e86\u6587\u672c-\u56fe\u50cf\u3001\u89c6\u9891-\u56fe\u50cf\u3001\u70b9\u4e91-\u56fe\u50cf\u7b49\u591a\u79cd\u6a21\u6001\u7684\u878d\u5408\uff0c\u4ee5\u53ca\u8ba1\u7b97\u673a\u89c6\u89c9\u4e0e\u673a\u5668\u4eba\u3001\u533b\u7597\u3001\u57fa\u7840\u8bbe\u65bd\u7b49\u9886\u57df\u7684\u6df1\u5ea6\u7ed3\u5408\u3002 \u751f\u6210\u5f0f AI \u7684\u7cbe\u7ec6\u63a7\u5236\u4e0e\u7f16\u8f91 (Fine-grained Control & Editing in Generative AI): \u4e0d\u518d\u4ec5\u4ec5\u662f\u751f\u6210\u56fe\u50cf\uff0c\u800c\u662f\u8ffd\u6c42\u5bf9\u751f\u6210\u5185\u5bb9\u66f4\u6df1\u5c42\u6b21\u7684\u7406\u89e3\u3001\u7f16\u8f91\u548c\u53d9\u4e8b\u80fd\u529b\u3002 \u5b9e\u7528\u5de5\u5177\u4e0e\u5e94\u7528 (Practical Tools & Applications): \u51fa\u73b0\u4e86\u63d0\u5347\u6807\u6ce8\u6548\u7387\u7684AI\u8f85\u52a9\u5de5\u5177\uff0c\u4ee5\u53ca\u9762\u5411\u5b9e\u9645\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u5f00\u653e\u8bcd\u6c47\u6293\u53d6\u8f85\u52a9\u7cfb\u7edf\u3002 2. \u663e\u8457\u6216\u521b\u65b0\u6027\u8bba\u6587\u4eae\u70b9 (Significant or Innovative Papers) OneCAT: Decoder-Only Auto-Regressive Model for Unified Understanding and Generation (Han Li et al.) : \u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u89e3\u7801\u5668-only\u81ea\u56de\u5f52\u6a21\u578b\uff0c\u65e8\u5728\u540c\u65f6\u5b9e\u73b0\u7406\u89e3\u548c\u751f\u6210\u4efb\u52a1\u3002\u5176\u6f5c\u529b\u5728\u4e8e\u6784\u5efa\u66f4\u901a\u7528\u3001\u66f4\u5f3a\u5927\u7684AI\u7cfb\u7edf\uff0c\u662f\u8fc8\u5411\u901a\u7528\u667a\u80fd\u7684\u91cd\u8981\u4e00\u6b65\u3002 A Generative Foundation Model for Chest Radiography (Yuanfeng Ji et al.) : \u5728\u533b\u7597\u5f71\u50cf\u9886\u57df\u5f15\u5165\u751f\u6210\u5f0f\u57fa\u7840\u6a21\u578b\uff0c\u6709\u671b\u5f7b\u5e95\u6539\u53d8\u80f8\u90e8X\u5149\u7247\u7684\u5206\u6790\u3001\u8bca\u65ad\u548c\u6570\u636e\u589e\u5f3a\u65b9\u5f0f\uff0c\u5bf9\u533b\u7597AI\u5177\u6709\u91cc\u7a0b\u7891\u610f\u4e49\u3002 InfraDiffusion: zero-shot depth map restoration with diffusion models and prompted segmentation from sparse infrastructure point clouds (Yixiong Jing et al.) : \u521b\u65b0\u6027\u5730\u5c06\u6269\u6563\u6a21\u578b\u5e94\u7528\u4e8e\u4ece\u7a00\u758f\u57fa\u7840\u8bbe\u65bd\u70b9\u4e91\u8fdb\u884c\u96f6\u6837\u672c\u6df1\u5ea6\u56fe\u6062\u590d\uff0c\u89e3\u51b3\u4e863D\u89c6\u89c9\u4e2d\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u5b9e\u9645\u95ee\u9898\u3002 LuxDiT: Lighting Estimation with Video Diffusion Transformer (Ruofan Liang et al.) : \u7ed3\u5408\u4e86\u89c6\u9891\u3001\u6269\u6563\u6a21\u578b\u548c Transformer \u7684\u5f3a\u5927\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u5149\u7167\u4f30\u8ba1\uff0c\u5bf9\u865a\u62df\u73b0\u5b9e\u3001\u7535\u5f71\u5236\u4f5c\u548c\u56fe\u50cf\u7f16\u8f91\u7b49\u9886\u57df\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002 Plot'n Polish: Zero-shot Story Visualization and Disentangled Editing with Text-to-Image Diffusion Models (Kiymet Akdemir et al.) : \u7a81\u7834\u4e86\u7b80\u5355\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\uff0c\u5b9e\u73b0\u4e86\u6545\u4e8b\u7ea7\u522b\u7684\u53ef\u89c6\u5316\u548c\u89e3\u8026\u7f16\u8f91\uff0c\u5c55\u793a\u4e86\u751f\u6210\u5f0fAI\u5728\u521b\u610f\u5185\u5bb9\u751f\u4ea7\u65b9\u9762\u7684\u5de8\u5927\u6f5c\u529b\u3002 TinyDrop: Tiny Model Guided Token Dropping for Vision Transformers (Guoxin Wang et al.) : \u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684 ViT \u4f18\u5316\u7b56\u7565\uff0c\u901a\u8fc7\u5fae\u5c0f\u6a21\u578b\u5f15\u5bfc\u7684 token dropping \u6765\u63d0\u5347\u63a8\u7406\u901f\u5ea6\uff0c\u5bf9 ViT \u7684\u5b9e\u9645\u90e8\u7f72\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002 3. \u65b0\u5174\u7814\u7a76\u65b9\u5411\u6216\u6280\u672f (Emerging Research Directions or Techniques) \u7edf\u4e00\u7684\u7406\u89e3\u4e0e\u751f\u6210\u8303\u5f0f (Unified Understanding & Generation Paradigm): \u4ee5 OneCAT \u4e3a\u4ee3\u8868\uff0c\u63a2\u7d22\u5982\u4f55\u7528\u5355\u4e00\u6a21\u578b\u67b6\u6784\u5904\u7406\u591a\u6a21\u6001\u7684\u611f\u77e5\u4e0e\u751f\u6210\u4efb\u52a1\u3002 \u9886\u57df\u7279\u5b9a\u57fa\u7840\u6a21\u578b (Domain-Specific Foundation Models): \u9488\u5bf9\u7279\u5b9a\u9ad8\u4ef7\u503c\u9886\u57df\uff08\u5982\u533b\u7597\u3001\u5de5\u4e1a\uff09\u5f00\u53d1\u5b9a\u5236\u5316\u7684\u57fa\u7840\u6a21\u578b\uff0c\u4ee5\u5b9e\u73b0\u66f4\u7cbe\u51c6\u3001\u9ad8\u6548\u7684\u5e94\u7528\u3002 \u7a00\u758f/\u4e0d\u5b8c\u6574\u6570\u636e\u4e0a\u7684\u6269\u6563\u6a21\u578b (Diffusion Models on Sparse/Incomplete Data): InfraDiffusion \u5c55\u793a\u4e86\u6269\u6563\u6a21\u578b\u5728\u5904\u7406\u4e0d\u5b8c\u65743D\u6570\u636e\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u672a\u6765\u53ef\u80fd\u6269\u5c55\u5230\u66f4\u591a\u6570\u636e\u7a00\u7f3a\u573a\u666f\u3002 \u9ad8\u6548\u4e14\u9c81\u68d2\u7684 Transformer \u67b6\u6784 (Efficient & Robust Transformer Architectures): TinyDrop \u548c\u6c34\u58a8\u753b\u589e\u5f3a\u9c81\u68d2\u6027\u7684\u7814\u7a76\uff0c\u8868\u660e\u5bf9 Transformer \u6a21\u578b\u6548\u7387\u548c\u5b89\u5168\u6027\u7684\u5173\u6ce8\u5c06\u6301\u7eed\u589e\u52a0\u3002 \u591a\u6a21\u6001\u610f\u56fe\u68c0\u6d4b\u4e0e\u673a\u5668\u4eba\u4ea4\u4e92 (Multimodal Intent Detection & Robotic Interaction): OVGrasp \u5f3a\u8c03\u4e86\u7ed3\u5408\u89c6\u89c9\u3001\u8bed\u8a00\u7b49\u591a\u79cd\u6a21\u6001\u6765\u7406\u89e3\u4eba\u7c7b\u610f\u56fe\uff0c\u4ee5\u5b9e\u73b0\u66f4\u667a\u80fd\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u3002 \u751f\u6210\u5f0f AI \u7684\u53d9\u4e8b\u4e0e\u9ad8\u5c42\u6b21\u7f16\u8f91 (Narrative & High-Level Editing in Generative AI): Plot'n Polish \u9884\u793a\u7740\u751f\u6210\u6a21\u578b\u5c06\u4ece\u56fe\u50cf\u751f\u6210\u8d70\u5411\u66f4\u590d\u6742\u7684\u53d9\u4e8b\u548c\u5185\u5bb9\u521b\u4f5c\u3002 4. \u5efa\u8bae\u6df1\u5165\u9605\u8bfb\u7684\u8bba\u6587 (Recommended Full Reads) \u8003\u8651\u5230\u5176\u6f5c\u5728\u5f71\u54cd\u548c\u521b\u65b0\u6027\uff0c\u6211\u4eec\u5efa\u8bae\u7814\u7a76\u4eba\u5458\u4f18\u5148\u9605\u8bfb\u4ee5\u4e0b\u8bba\u6587\uff1a OneCAT: Decoder-Only Auto-Regressive Model for Unified Understanding and Generation (Han Li et al.) : \u5bf9\u4e8e\u5173\u6ce8\u901a\u7528AI\u3001\u57fa\u7840\u6a21\u578b\u67b6\u6784\u548c\u591a\u6a21\u6001\u5b66\u4e60\u7684\u7814\u7a76\u8005\uff0c\u8fd9\u7bc7\u8bba\u6587\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u672a\u6765\u65b9\u5411\u3002 A Generative Foundation Model for Chest Radiography (Yuanfeng Ji et al.) : \u533b\u7597AI\u9886\u57df\u7684\u7814\u7a76\u4eba\u5458\u5e94\u91cd\u70b9\u5173\u6ce8\uff0c\u5b83\u53ef\u80fd\u4e3a\u533b\u5b66\u5f71\u50cf\u5206\u6790\u5e26\u6765\u8303\u5f0f\u8f6c\u53d8\u3002 InfraDiffusion: zero-shot depth map restoration with diffusion models and prompted segmentation from sparse infrastructure point clouds (Yixiong Jing et al.) : \u4e13\u6ce8\u4e8e3D\u89c6\u89c9\u3001\u6269\u6563\u6a21\u578b\u5728\u7a00\u758f\u6570\u636e\u5e94\u7528\u6216\u57fa\u7840\u8bbe\u65bdAI\u7684\u7814\u7a76\u8005\u4f1a\u4ece\u4e2d\u53d7\u76ca\u3002 TinyDrop: Tiny Model Guided Token Dropping for Vision Transformers (Guoxin Wang et al.) : \u5bf9\u4e8e\u81f4\u529b\u4e8e Vision Transformer \u90e8\u7f72\u3001\u6548\u7387\u4f18\u5316\u548c\u8fb9\u7f18\u8ba1\u7b97\u7684\u7814\u7a76\u8005\uff0c\u8fd9\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002 Plot'n Polish: Zero-shot Story Visualization and Disentangled Editing with Text-to-Image Diffusion Models (Kiymet Akdemir et al.) : \u5bf9\u751f\u6210\u5f0fAI\u3001\u521b\u610f\u5e94\u7528\u3001\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u548c\u53ef\u63a7\u5185\u5bb9\u521b\u4f5c\u611f\u5174\u8da3\u7684\u7814\u7a76\u8005\u4e0d\u5bb9\u9519\u8fc7\u3002 Table of Contents InfraDiffusion: zero-shot depth map restoration with diffusion models and prompted segmentation from sparse infrastructure point clouds VisioFirm: Cross-Platform AI-assisted Annotation Tool for Computer Vision LuxDiT: Lighting Estimation with Video Diffusion Transformer OneCAT: Decoder-Only Auto-Regressive Model for Unified Understanding and Generation A Generative Foundation Model for Chest Radiography OVGrasp: Open-Vocabulary Grasping Assistance via Multimodal Intent Detection Enhancing Robustness in Post-Processing Watermarking: An Ensemble Attack Network Using CNNs and Transformers Plot'n Polish: Zero-shot Story Visualization and Disentangled Editing with Text-to-Image Diffusion Models Differential Morphological Profile Neural Networks for Semantic Segmentation TinyDrop: Tiny Model Guided Token Dropping for Vision Transformers Papers InfraDiffusion: zero-shot depth map restoration with diffusion models and prompted segmentation from sparse infrastructure point clouds Authors: Yixiong Jing, Cheng Zhang, Haibing Wu, Guangming Wang, Olaf Wysocki, Brian Sheil Published: 2025-09-03 Categories: cs.CV Abstract: Point clouds are widely used for infrastructure monitoring by providing geometric information, where segmentation is required for downstream tasks such as defect detection. Existing research has automated semantic segmentation of structural components, while brick-level segmentation (identifying defects such as spalling and mortar loss) has been primarily conducted from RGB images. However, acquiring high-resolution images is impractical in low-light environments like masonry tunnels. Point clouds, though robust to dim lighting, are typically unstructured, sparse, and noisy, limiting fine-grained segmentation. We present InfraDiffusion, a zero-shot framework that projects masonry point clouds into depth maps using virtual cameras and restores them by adapting the Denoising Diffusion Null-space Model (DDNM). Without task-specific training, InfraDiffusion enhances visual clarity and geometric consistency of depth maps. Experiments on masonry bridge and tunnel point cloud datasets show significant improvements in brick-level segmentation using the Segment Anything Model (SAM), underscoring its potential for automated inspection of masonry assets. Our code and data is available at https://github.com/Jingyixiong/InfraDiffusion-official-implement. Analysis: InfraDiffusion \u8bba\u6587\u6458\u8981\u7684\u5206\u6790\u5982\u4e0b\uff1a 1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u660e\u6458\u8981 InfraDiffusion \u63d0\u51fa\u4e86\u4e00\u79cd\u96f6\u6837\u672c\uff08zero-shot\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u7a00\u758f\u3001\u975e\u7ed3\u6784\u5316\u4e14\u5608\u6742\u7684\u7816\u77f3\u70b9\u4e91\u4e2d\u6062\u590d\u9ad8\u8d28\u91cf\u7684\u6df1\u5ea6\u56fe\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u5c06\u70b9\u4e91\u6295\u5f71\u5230\u865a\u62df\u76f8\u673a\u751f\u6210\u7684\u6df1\u5ea6\u56fe\uff0c\u5e76\u5229\u7528\u9002\u5e94\u6027\u4fee\u6539\u7684\u53bb\u566a\u6269\u6563\u96f6\u7a7a\u95f4\u6a21\u578b\uff08DDNM\uff09\u8fdb\u884c\u4fee\u590d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6df1\u5ea6\u56fe\u7684\u89c6\u89c9\u6e05\u6670\u5ea6\u548c\u51e0\u4f55\u4e00\u81f4\u6027\u3002\u5176\u6838\u5fc3\u8d21\u732e\u5728\u4e8e\uff0c\u65e0\u9700\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\u8fdb\u884c\u8bad\u7ec3\uff0c\u5373\u53ef\u663e\u8457\u6539\u5584\u7816\u77f3\u7ed3\u6784\u7684\u7816\u5757\u7ea7\u5206\u5272\u6548\u679c\uff0c\u4ece\u800c\u63a8\u52a8\u81ea\u52a8\u5316\u57fa\u7840\u8bbe\u65bd\u68c0\u6d4b\u3002 2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u5b66\u65b9\u6cd5 \u8be5\u8bba\u6587\u7684\u5173\u952e\u521b\u65b0\u5728\u4e8e\u5176 \u96f6\u6837\u672c\u6df1\u5ea6\u56fe\u6062\u590d\u6846\u67b6 \uff0c\u5b83\u5de7\u5999\u5730\u7ed3\u5408\u4e86\u4ee5\u4e0b\u51e0\u70b9\uff1a * \u70b9\u4e91\u5230\u6df1\u5ea6\u56fe\u7684\u6295\u5f71\uff1a \u5c06\u7a00\u758f\u7684\u7816\u77f3\u70b9\u4e91\u901a\u8fc7\u865a\u62df\u76f8\u673a\u8f6c\u6362\u4e3a\u6df1\u5ea6\u56fe\uff0c\u4e3a\u540e\u7eed\u5904\u7406\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7ed3\u6784\u5316\u7684\u4e8c\u7ef4\u8868\u793a\u3002 * \u9002\u5e94\u6027\u6269\u6563\u6a21\u578b\u5e94\u7528\uff1a \u6838\u5fc3\u5728\u4e8e \u9002\u5e94\u6027\u5730\u4fee\u6539\u548c\u5e94\u7528\u53bb\u566a\u6269\u6563\u96f6\u7a7a\u95f4\u6a21\u578b\uff08DDNM\uff09 \u8fdb\u884c\u6df1\u5ea6\u56fe\u6062\u590d\u3002DDNM \u901a\u5e38\u7528\u4e8e\u56fe\u50cf\u4fee\u590d\u6216\u6761\u4ef6\u751f\u6210\uff0c\u6b64\u5904\u5c06\u5176\u521b\u65b0\u6027\u5730\u5e94\u7528\u4e8e\u4ece\u7a00\u758f\u3001\u4e0d\u5b8c\u6574\u6570\u636e\u4e2d\u6062\u590d\u51e0\u4f55\u4fe1\u606f\u4e30\u5bcc\u7684\u6df1\u5ea6\u56fe\uff0c\u4e14\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u7684\u8bad\u7ec3\u3002 * \u96f6\u6837\u672c\u80fd\u529b\uff1a \u6574\u4e2a\u6846\u67b6\u65e0\u9700\u9488\u5bf9\u7816\u77f3\u7ed3\u6784\u6216\u6df1\u5ea6\u56fe\u6062\u590d\u4efb\u52a1\u8fdb\u884c\u989d\u5916\u7684\u8bad\u7ec3\uff0c\u76f4\u63a5\u5229\u7528\u9884\u8bad\u7ec3\u7684\u6269\u6563\u6a21\u578b\u80fd\u529b\uff0c\u8fd9\u5927\u5927\u964d\u4f4e\u4e86\u6570\u636e\u6807\u6ce8\u548c\u6a21\u578b\u8bad\u7ec3\u7684\u6210\u672c\uff0c\u5e76\u63d0\u5347\u4e86\u6cdb\u5316\u80fd\u529b\u3002 * \u4e0e\u73b0\u6709\u5206\u5272\u6a21\u578b\u7684\u7ed3\u5408\uff1a \u6062\u590d\u540e\u7684\u9ad8\u8d28\u91cf\u6df1\u5ea6\u56fe\u80fd\u591f\u663e\u8457\u63d0\u5347\u5982 Segment Anything Model (SAM) \u7b49\u901a\u7528\u5206\u5272\u6a21\u578b\u5728\u7816\u5757\u7ea7\u5206\u5272\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u5b9e\u73b0\u4e86\u4ece\u4f4e\u8d28\u91cf\u70b9\u4e91\u5230\u9ad8\u7cbe\u5ea6\u8bed\u4e49\u7406\u89e3\u7684\u6865\u6881\u3002 3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd \u57fa\u7840\u8bbe\u65bd\u68c0\u6d4b\u4e0e\u7ef4\u62a4\uff1a \u4e3a\u7816\u77f3\u6865\u6881\u548c\u96a7\u9053\u7b49\u57fa\u7840\u8bbe\u65bd\u7684\u81ea\u52a8\u5316\u3001\u7cbe\u7ec6\u5316\u68c0\u6d4b\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u5de5\u5177\uff0c\u80fd\u591f\u66f4\u51c6\u786e\u5730\u8bc6\u522b\u5265\u843d\u3001\u7802\u6d46\u6d41\u5931\u7b49\u7f3a\u9677\uff0c\u4ece\u800c\u63d0\u9ad8\u7ef4\u62a4\u6548\u7387\u548c\u5b89\u5168\u6027\u3002 \u70b9\u4e91\u6570\u636e\u5229\u7528\u6548\u7387\uff1a \u514b\u670d\u4e86\u7a00\u758f\u3001\u5608\u6742\u70b9\u4e91\u5728\u7ec6\u7c92\u5ea6\u5206\u6790\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u4ece\u4f4e\u8d28\u91cf\u4e09\u7ef4\u6570\u636e\u4e2d\u63d0\u53d6\u9ad8\u4ef7\u503c\u4fe1\u606f\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002 \u6269\u6563\u6a21\u578b\u5e94\u7528\u62d3\u5c55\uff1a \u5c06\u6269\u6563\u6a21\u578b\u7684\u5e94\u7528\u8303\u56f4\u4ece\u4f20\u7edf\u7684\u56fe\u50cf\u751f\u6210\u3001\u4fee\u590d\u7b49\u9886\u57df\u62d3\u5c55\u5230\u51e0\u4f55\u6570\u636e\uff08\u6df1\u5ea6\u56fe\uff09\u7684\u6062\u590d\u548c\u589e\u5f3a\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5904\u7406\u7ed3\u6784\u5316\u51e0\u4f55\u4fe1\u606f\u65b9\u9762\u7684\u5de8\u5927\u6f5c\u529b\u3002 \u96f6\u6837\u672c\u5b66\u4e60\u7684\u5b9e\u8df5\uff1a \u5f3a\u8c03\u4e86\u96f6\u6837\u672c\u65b9\u6cd5\u5728\u5b9e\u9645\u5de5\u7a0b\u5e94\u7528\u4e2d\u7684\u53ef\u884c\u6027\u548c\u4f18\u52bf\uff0c\u5c24\u5176\u662f\u5728\u96be\u4ee5\u83b7\u53d6\u5927\u91cf\u6807\u6ce8\u6570\u636e\u7684\u7279\u5b9a\u9886\u57df\u3002 \u591a\u6a21\u6001\u6570\u636e\u878d\u5408\uff1a \u867d\u7136\u62bd\u8c61\u4e2d\u672a\u76f4\u63a5\u63d0\u53ca\uff0c\u4f46\u8fd9\u79cd\u5c06\u4e09\u7ef4\u70b9\u4e91\u8f6c\u6362\u4e3a\u4e8c\u7ef4\u6df1\u5ea6\u56fe\u5e76\u5229\u7528\u56fe\u50cf\u5904\u7406\u6280\u672f\u8fdb\u884c\u589e\u5f3a\u7684\u601d\u8def\uff0c\u4e3a\u672a\u6765\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u548c\u5904\u7406\u63d0\u4f9b\u4e86\u65b0\u7684\u8303\u5f0f\u3002 4. \u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 \u571f\u6728\u5de5\u7a0b\u4e0e\u7ed3\u6784\u5065\u5eb7\u76d1\u6d4b\uff1a \u81ea\u52a8\u5316\u68c0\u6d4b\u6865\u6881\u3001\u96a7\u9053\u3001\u5927\u575d\u7b49\u57fa\u7840\u8bbe\u65bd\u7684\u7ed3\u6784\u7f3a\u9677\u3002 \u6587\u5316\u9057\u4ea7\u4fdd\u62a4\uff1a \u5bf9\u5386\u53f2\u5efa\u7b51\u3001\u96d5\u5851\u7b49\u8fdb\u884c\u7cbe\u7ec6\u5316\u4e09\u7ef4\u626b\u63cf\u548c\u635f\u4f24\u8bc4\u4f30\u3002 \u673a\u5668\u4eba\u4e0e\u81ea\u4e3b\u68c0\u6d4b\uff1a \u88c5\u5907\u6709\u6fc0\u5149\u96f7\u8fbe\u7684\u673a\u5668\u4eba\u6216\u65e0\u4eba\u673a\u5728\u590d\u6742\u3001\u4f4e\u5149\u7167\u73af\u5883\u4e0b\u8fdb\u884c\u81ea\u4e3b\u5de1\u68c0\u548c\u73af\u5883\u611f\u77e5\u3002 \u6570\u5b57\u5b6a\u751f\uff08Digital Twin\uff09\uff1a \u521b\u5efa\u9ad8\u7cbe\u5ea6\u7684\u7269\u7406\u8d44\u4ea7\u6570\u5b57\u6a21\u578b\uff0c\u7528\u4e8e\u6a21\u62df\u3001\u5206\u6790\u548c\u9884\u6d4b\u3002 \u5efa\u7b51\u4fe1\u606f\u6a21\u578b\uff08BIM\uff09\uff1a \u589e\u5f3a\u73b0\u6709\u5efa\u7b51\u7684BIM\u6a21\u578b\uff0c\u4f7f\u5176\u5305\u542b\u66f4\u8be6\u7ec6\u7684\u7ed3\u6784\u5065\u5eb7\u4fe1\u606f\u3002 \u91c7\u77ff\u4e0e\u5730\u8d28\u52d8\u63a2\uff1a \u96a7\u9053\u3001\u77ff\u4e95\u7b49\u5730\u4e0b\u7a7a\u95f4\u7684\u7ed3\u6784\u7a33\u5b9a\u6027\u76d1\u6d4b\u3002 5. \u53ef\u4ece\u6458\u8981\u4e2d\u63a8\u65ad\u51fa\u7684\u5c40\u9650\u6027 \u4f9d\u8d56\u865a\u62df\u76f8\u673a\u89c6\u89d2\uff1a \u5c06\u70b9\u4e91\u6295\u5f71\u5230\u6df1\u5ea6\u56fe\u7684\u8d28\u91cf\u548c\u5b8c\u6574\u6027\u53ef\u80fd\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u865a\u62df\u76f8\u673a\u7684\u9009\u62e9\u548c\u6570\u91cf\u3002\u5982\u679c\u70b9\u4e91\u5728\u67d0\u4e9b\u533a\u57df\u6781\u5176\u7a00\u758f\uff0c\u6216\u8005\u865a\u62df\u76f8\u673a\u89c6\u89d2\u4e0d\u4f73\uff0c\u53ef\u80fd\u5bfc\u81f4\u6df1\u5ea6\u56fe\u4fe1\u606f\u7f3a\u5931\u6216\u4e0d\u51c6\u786e\u3002 DDNM\u7684\u6cdb\u5316\u80fd\u529b\uff1a \u5c3d\u7ba1\u662f\u201c\u96f6\u6837\u672c\u201d\uff0c\u4f46DDNM\u672c\u8eab\u662f\u9884\u8bad\u7ec3\u6a21\u578b\u3002\u5176\u5728\u5904\u7406\u7816\u77f3\u7ed3\u6784\u7279\u6709\u7684\u51e0\u4f55\u7eb9\u7406\u548c\u7f3a\u9677\u6a21\u5f0f\u4e0a\u7684\u8868\u73b0\uff0c\u53ef\u80fd\u53d7\u9650\u4e8e\u5176\u539f\u59cb\u8bad\u7ec3\u6570\u636e\u7684\u9886\u57df\u3002\u5bf9\u4e8e\u4e0e\u8bad\u7ec3\u6570\u636e\u5dee\u5f02\u8f83\u5927\u7684\u51e0\u4f55\u7ed3\u6784\u6216\u6750\u6599\uff0c\u6548\u679c\u53ef\u80fd\u6709\u6240\u4e0b\u964d\u3002 \u7816\u77f3\u7ed3\u6784\u7684\u7279\u5f02\u6027\uff1a \u8bba\u6587\u5f3a\u8c03\u4e86\u201c\u7816\u77f3\u70b9\u4e91\u201d\u548c\u201c\u7816\u5757\u7ea7\u5206\u5272\u201d\u3002\u8fd9\u8868\u660e\u8be5\u65b9\u6cd5\u53ef\u80fd\u9488\u5bf9\u7816\u77f3\u7ed3\u6784\u8fdb\u884c\u4e86\u4f18\u5316\u6216\u9a8c\u8bc1\uff0c\u5176\u5728\u5176\u4ed6\u7c7b\u578b\u7ed3\u6784\uff08\u5982\u6df7\u51dd\u571f\u3001\u94a2\u7ed3\u6784\uff09\u6216\u66f4\u590d\u6742\u7f3a\u9677\uff08\u5982\u88c2\u7f1d\u3001\u53d8\u5f62\uff09\u4e0a\u7684\u8868\u73b0\u5c1a\u4e0d\u660e\u786e\u3002 \u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\uff1a \u6269\u6563\u6a21\u578b\u901a\u5e38\u8ba1\u7b97\u6210\u672c\u8f83\u9ad8\uff0c\u5c24\u5176\u662f\u5728\u751f\u6210\u9ad8\u5206\u8fa8\u7387\u6df1\u5ea6\u56fe\u65f6\u3002\u6458\u8981\u4e2d\u672a\u63d0\u53ca\u5b9e\u65f6\u6027\u6216\u8ba1\u7b97\u6548\u7387\uff0c\u8fd9\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u53ef\u80fd\u662f\u4e00\u4e2a\u8003\u91cf\u56e0\u7d20\u3002 \u6df1\u5ea6\u56fe\u7684\u5c40\u9650\u6027\uff1a \u6df1\u5ea6\u56fe\u662f2.5D\u8868\u793a\uff0c\u65e0\u6cd5\u5b8c\u5168\u6355\u6349\u4e09\u7ef4\u70b9\u4e91\u7684\u6240\u6709\u51e0\u4f55\u4fe1\u606f\uff0c\u4f8b\u5982\u906e\u6321\u533a\u57df\u540e\u7684\u7ed3\u6784\u3002\u8fd9\u53ef\u80fd\u9650\u5236\u4e86\u5bf9\u67d0\u4e9b\u590d\u6742\u7f3a\u9677\u7684\u68c0\u6d4b\u80fd\u529b\u3002 Key Findings: We present InfraDiffusion, a zero-shot framework that projects masonry point clouds into depth maps using virtual cameras and restores them by adapting the Denoising Diffusion Null-space Model (DDNM). Links: PDF arXiv VisioFirm: Cross-Platform AI-assisted Annotation Tool for Computer Vision Authors: Safouane El Ghazouali, Umberto Michelucci Published: 2025-09-04 Categories: cs.CV, cs.AI Abstract: AI models rely on annotated data to learn pattern and perform prediction. Annotation is usually a labor-intensive step that require associating labels ranging from a simple classification label to more complex tasks such as object detection, oriented bounding box estimation, and instance segmentation. Traditional tools often require extensive manual input, limiting scalability for large datasets. To address this, we introduce VisioFirm, an open-source web application designed to streamline image labeling through AI-assisted automation. VisioFirm integrates state-of-the-art foundation models into an interface with a filtering pipeline to reduce human-in-the-loop efforts. This hybrid approach employs CLIP combined with pre-trained detectors like Ultralytics models for common classes and zero-shot models such as Grounding DINO for custom labels, generating initial annotations with low-confidence thresholding to maximize recall. Through this framework, when tested on COCO-type of classes, initial prediction have been proven to be mostly correct though the users can refine these via interactive tools supporting bounding boxes, oriented bounding boxes, and polygons. Additionally, VisioFirm has on-the-fly segmentation powered by Segment Anything accelerated through WebGPU for browser-side efficiency. The tool supports multiple export formats (YOLO, COCO, Pascal VOC, CSV) and operates offline after model caching, enhancing accessibility. VisioFirm demonstrates up to 90\\% reduction in manual effort through benchmarks on diverse datasets, while maintaining high annotation accuracy via clustering of connected CLIP-based disambiguate components and IoU-graph for redundant detection suppression. VisioFirm can be accessed from \\href{https://github.com/OschAI/VisioFirm}{https://github.com/OschAI/VisioFirm}. Analysis: \u4f5c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u4e13\u5bb6\uff0c\u6211\u5bf9\u8fd9\u7bc7\u8bba\u6587\u6458\u8981\u8fdb\u884c\u5982\u4e0b\u5206\u6790\uff1a 1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u660e\u6458\u8981 (Concise Summary) VisioFirm\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u8de8\u5e73\u53f0AI\u8f85\u52a9\u6807\u6ce8\u5de5\u5177\uff0c\u65e8\u5728\u89e3\u51b3\u8ba1\u7b97\u673a\u89c6\u89c9\u6570\u636e\u6807\u6ce8\u8017\u65f6\u8017\u529b\u7684\u95ee\u9898\u3002\u5b83\u901a\u8fc7\u667a\u80fd\u96c6\u6210CLIP\u3001Grounding DINO\u3001Ultralytics\u7b49\u524d\u6cbf\u57fa\u7840\u6a21\u578b\u548cWebGPU\u52a0\u901f\u7684Segment Anything Model (SAM)\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u6df7\u5408\u4eba\u673a\u534f\u4f5c\u6807\u6ce8\u6d41\u7a0b\uff0c\u58f0\u79f0\u53ef\u5c06\u624b\u52a8\u5de5\u4f5c\u91cf\u51cf\u5c11\u9ad8\u8fbe90%\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6807\u6ce8\u7cbe\u5ea6\u3002 2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u8bba (Key Innovation or Methodological Approach) VisioFirm\u7684\u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u5176 \u6df7\u5408\u5f0fAI\u8f85\u52a9\u6807\u6ce8\u8303\u5f0f \uff0c\u901a\u8fc7\u667a\u80fd\u96c6\u6210\u591a\u79cd\u524d\u6cbf\u57fa\u7840\u6a21\u578b\uff0c\u751f\u6210\u521d\u59cb\u9ad8\u53ec\u56de\u7387\u7684\u4f4e\u7f6e\u4fe1\u5ea6\u9884\u6d4b\uff0c\u518d\u7531\u7528\u6237\u8fdb\u884c\u7cbe\u4fee\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\uff1a \u591a\u6a21\u578b\u878d\u5408\u7b56\u7565\uff1a \u7ed3\u5408\u4e86\u9884\u8bad\u7ec3\u68c0\u6d4b\u5668\uff08\u5982Ultralytics\u6a21\u578b\uff09\u5904\u7406\u5e38\u89c1\u7c7b\u522b\uff0c\u96f6\u6837\u672c\u6a21\u578b\uff08\u5982Grounding DINO\uff09\u5904\u7406\u81ea\u5b9a\u4e49\u6807\u7b7e\uff0c\u4ee5\u53caCLIP\u6a21\u578b\u8fdb\u884c\u8bed\u4e49\u6d88\u6b67\u548c\u8fde\u63a5\u7ec4\u4ef6\u805a\u7c7b\u3002 \u9ad8\u6548\u7684\u6d4f\u89c8\u5668\u7aef\u5206\u5272\uff1a \u5229\u7528 WebGPU\u52a0\u901f\u7684Segment Anything Model (SAM) \uff0c\u5b9e\u73b0\u4e86\u5b9e\u65f6\u3001\u9ad8\u6548\u7684\u201c\u5373\u65f6\u5206\u5272\u201d\u529f\u80fd\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7528\u6237\u4f53\u9a8c\u3002 \u7cbe\u5ea6\u7ef4\u62a4\u673a\u5236\uff1a \u5f15\u5165\u4e86 \u57fa\u4e8eCLIP\u7684\u8fde\u63a5\u7ec4\u4ef6\u805a\u7c7b\u6d88\u6b67 \u548c IoU\u56fe\u5197\u4f59\u68c0\u6d4b\u6291\u5236\u673a\u5236 \uff0c\u4ee5\u5728\u5927\u5e45\u51cf\u5c11\u4eba\u5de5\u5e72\u9884\u7684\u540c\u65f6\u786e\u4fdd\u6807\u6ce8\u7cbe\u5ea6\u3002 \u8de8\u5e73\u53f0\u4e0e\u79bb\u7ebf\u80fd\u529b\uff1a \u4f5c\u4e3aWeb\u5e94\u7528\uff0c\u652f\u6301\u591a\u79cd\u5bfc\u51fa\u683c\u5f0f\uff08YOLO, COCO, Pascal VOC, CSV\uff09\uff0c\u5e76\u5728\u6a21\u578b\u7f13\u5b58\u540e\u652f\u6301\u79bb\u7ebf\u64cd\u4f5c\uff0c\u6781\u5927\u5730\u589e\u5f3a\u4e86\u5de5\u5177\u7684\u53ef\u7528\u6027\u548c\u53ef\u8bbf\u95ee\u6027\u3002 3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd (Potential Impact on the Field) VisioFirm\u6709\u671b\u663e\u8457 \u964d\u4f4e\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u6570\u636e\u6807\u6ce8\u7684\u95e8\u69db\u548c\u6210\u672c \uff0c\u5c24\u5176\u5bf9\u4e8e\u8d44\u6e90\u6709\u9650\u7684\u56e2\u961f\u548c\u7814\u7a76\u8005\u3002\u901a\u8fc7\u5c06\u624b\u52a8\u6807\u6ce8\u5de5\u4f5c\u91cf\u51cf\u5c11\u9ad8\u8fbe90%\uff0c\u5b83\u80fd \u6781\u5927\u52a0\u901f\u6570\u636e\u96c6\u7684\u521b\u5efa\u548c\u8fed\u4ee3\u8fc7\u7a0b \uff0c\u4ece\u800c \u63a8\u52a8AI\u6a21\u578b\u5f00\u53d1\u548c\u90e8\u7f72\u7684\u6548\u7387 \u3002\u5176\u5f00\u6e90\u548c\u8de8\u5e73\u53f0\u7684\u7279\u6027\u4e5f\u6709\u52a9\u4e8e \u4fc3\u8fdbAI\u8f85\u52a9\u6807\u6ce8\u5de5\u5177\u7684\u666e\u53ca\u548c\u6807\u51c6\u5316 \uff0c\u4f7f\u66f4\u591a\u4eba\u80fd\u591f\u5229\u7528\u5148\u8fdb\u7684AI\u80fd\u529b\u8fdb\u884c\u9ad8\u8d28\u91cf\u6570\u636e\u51c6\u5907\uff0c\u4ece\u800c\u52a0\u901f\u6574\u4e2aCV\u751f\u6001\u7cfb\u7edf\u7684\u521b\u65b0\u3002 4. \u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 (Related Areas or Applications) \u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u5f00\u53d1\u4e0e\u90e8\u7f72\uff1a \u4efb\u4f55\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\u6765\u8bad\u7ec3\u3001\u5fae\u8c03\u6216\u8bc4\u4f30\u6a21\u578b\u7684\u573a\u666f\uff0c\u5982\u76ee\u6807\u68c0\u6d4b\u3001\u5b9e\u4f8b\u5206\u5272\u3001\u8bed\u4e49\u5206\u5272\u3001\u59ff\u6001\u4f30\u8ba1\u7b49\u3002 \u81ea\u52a8\u9a7e\u9a76\u4e0e\u673a\u5668\u4eba\uff1a \u7528\u4e8e\u6807\u6ce8\u611f\u77e5\u7cfb\u7edf\u6240\u9700\u7684\u9053\u8def\u3001\u8f66\u8f86\u3001\u884c\u4eba\u3001\u969c\u788d\u7269\u7b49\u6570\u636e\u3002 \u533b\u7597\u5f71\u50cf\u5206\u6790\uff1a \u8f85\u52a9\u533b\u751f\u6216\u7814\u7a76\u4eba\u5458\u6807\u6ce8\u75c5\u7076\u3001\u5668\u5b98\u3001\u7ec6\u80de\u7b49\u533b\u5b66\u56fe\u50cf\u3002 \u5de5\u4e1a\u8d28\u68c0\u4e0e\u5b89\u9632\u76d1\u63a7\uff1a \u5feb\u901f\u6807\u6ce8\u7f3a\u9677\u3001\u5f02\u5e38\u884c\u4e3a\u6216\u7279\u5b9a\u76ee\u6807\u3002 \u519c\u4e1a\u79d1\u6280\uff1a \u6807\u6ce8\u4f5c\u7269\u75c5\u866b\u5bb3\u3001\u679c\u5b9e\u6210\u719f\u5ea6\u3001\u519c\u7530\u533a\u57df\u7b49\u3002 \u5b66\u672f\u7814\u7a76\u4e0e\u6559\u80b2\uff1a \u4e3a\u5b66\u751f\u548c\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e00\u4e2a\u6613\u4e8e\u4f7f\u7528\u7684\u5de5\u5177\u6765\u521b\u5efa\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u3002 \u6570\u636e\u6807\u6ce8\u670d\u52a1\u63d0\u4f9b\u5546\uff1a \u63d0\u9ad8\u5176\u670d\u52a1\u6548\u7387\u548c\u964d\u4f4e\u6210\u672c\u3002 5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u7684\u5c40\u9650\u6027 (Limitations Inferred from the Abstract) \u5bf9\u4eba\u5de5\u5e72\u9884\u7684\u6301\u7eed\u4f9d\u8d56\uff1a \u5c3d\u7ba1\u58f0\u79f0\u51cf\u5c1190%\u7684\u5de5\u4f5c\u91cf\uff0c\u4f46\u201c\u521d\u59cb\u9884\u6d4b\u5927\u591a\u6b63\u786e\u201d\u548c\u201c\u7528\u6237\u53ef\u4ee5\u7ec6\u5316\u201d\u8868\u660e\u4eba\u5de5\u5ba1\u6838\u548c\u4fee\u6b63\u4ecd\u7136\u662f\u786e\u4fdd\u6700\u7ec8\u6807\u6ce8\u8d28\u91cf\u7684\u5173\u952e\u73af\u8282\uff0c\u5e76\u975e\u5b8c\u5168\u81ea\u52a8\u5316\u3002 \u5bf9\u57fa\u7840\u6a21\u578b\u7684\u6027\u80fd\u4f9d\u8d56\uff1a VisioFirm\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u5176\u96c6\u6210\u7684CLIP\u3001Grounding DINO\u3001Ultralytics\u548cSAM\u7b49\u57fa\u7840\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002\u5bf9\u4e8e\u8fd9\u4e9b\u6a21\u578b\u4e0d\u64c5\u957f\u5904\u7406\u7684\u7279\u5b9a\u9886\u57df\u3001\u9ad8\u5ea6\u62bd\u8c61\u6216\u6781\u5ea6\u7ec6\u7c92\u5ea6\u7684\u81ea\u5b9a\u4e49\u7c7b\u522b\uff0c\u5176\u8f85\u52a9\u6548\u679c\u53ef\u80fd\u4f1a\u6253\u6298\u6263\u3002 \u201cCOCO-type of classes\u201d\u7684\u6d4b\u8bd5\u8303\u56f4\uff1a \u5c3d\u7ba1\u5728COCO\u7c7b\u578b\u7c7b\u522b\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5bf9\u4e8e\u9ad8\u5ea6\u4e13\u4e1a\u5316\u3001\u957f\u5c3e\u5206\u5e03\u6216\u89c6\u89c9\u4e0a\u6a21\u7cca\u7684\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\uff0c\u5176\u521d\u59cb\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u53ec\u56de\u7387\u53ef\u80fd\u9700\u8981\u66f4\u9891\u7e41\u7684\u4eba\u5de5\u4fee\u6b63\u3002 WebGPU\u7684\u517c\u5bb9\u6027\u4e0e\u6027\u80fd\uff1a WebGPU\u7684\u52a0\u901f\u6548\u679c\u53ef\u80fd\u53d7\u9650\u4e8e\u7528\u6237\u6d4f\u89c8\u5668\u7248\u672c\u3001\u663e\u5361\u786c\u4ef6\u548c\u9a71\u52a8\u7a0b\u5e8f\uff0c\u5e76\u975e\u6240\u6709\u7528\u6237\u90fd\u80fd\u83b7\u5f97\u6700\u4f73\u7684\u6d4f\u89c8\u5668\u7aef\u6548\u7387\u3002 \u79bb\u7ebf\u80fd\u529b\u7684\u5c40\u9650\u6027\uff1a \u201c\u6a21\u578b\u7f13\u5b58\u540e\u53ef\u79bb\u7ebf\u64cd\u4f5c\u201d\u610f\u5473\u7740\u9996\u6b21\u4f7f\u7528\u6216\u6a21\u578b\u66f4\u65b0\u65f6\u4ecd\u9700\u7f51\u7edc\u8fde\u63a5\u4e0b\u8f7d\u6a21\u578b\uff0c\u4e14\u6a21\u578b\u7f13\u5b58\u53ef\u80fd\u5360\u7528\u5927\u91cf\u672c\u5730\u5b58\u50a8\u7a7a\u95f4\u3002 \u201c\u9ad8\u8fbe90%\u201d\u7684\u51cf\u5c11\u91cf\uff1a \u8fd9\u662f\u4e00\u4e2a\u4e0a\u9650\u503c\uff0c\u5b9e\u9645\u51cf\u5c11\u91cf\u53ef\u80fd\u56e0\u6570\u636e\u96c6\u7684\u590d\u6742\u6027\u3001\u6807\u6ce8\u4efb\u52a1\u7c7b\u578b\u4ee5\u53ca\u7528\u6237\u719f\u7ec3\u5ea6\u800c\u5f02\u3002 \u672a\u63d0\u53ca\u89c6\u9891\u6807\u6ce8\uff1a \u6458\u8981\u4e3b\u8981\u805a\u7126\u4e8e\u56fe\u50cf\u6807\u6ce8\uff0c\u672a\u8bf4\u660e\u5176\u5bf9\u89c6\u9891\u6807\u6ce8\u4efb\u52a1\u7684\u652f\u6301\u80fd\u529b\u3002 Key Findings: To address this, we introduce VisioFirm, an open-source web application designed to streamline image labeling through AI-assisted automation. VisioFirm integrates state-of-the-art foundation models into an interface with a filtering pipeline to reduce human-in-the-loop efforts. Links: PDF arXiv LuxDiT: Lighting Estimation with Video Diffusion Transformer Authors: Ruofan Liang, Kai He, Zan Gojcic, Igor Gilitschenski, Sanja Fidler, Nandita Vijaykumar, Zian Wang Published: 2025-09-03 Categories: cs.GR, cs.AI, cs.CV Abstract: Estimating scene lighting from a single image or video remains a longstanding challenge in computer vision and graphics. Learning-based approaches are constrained by the scarcity of ground-truth HDR environment maps, which are expensive to capture and limited in diversity. While recent generative models offer strong priors for image synthesis, lighting estimation remains difficult due to its reliance on indirect visual cues, the need to infer global (non-local) context, and the recovery of high-dynamic-range outputs. We propose LuxDiT, a novel data-driven approach that fine-tunes a video diffusion transformer to generate HDR environment maps conditioned on visual input. Trained on a large synthetic dataset with diverse lighting conditions, our model learns to infer illumination from indirect visual cues and generalizes effectively to real-world scenes. To improve semantic alignment between the input and the predicted environment map, we introduce a low-rank adaptation finetuning strategy using a collected dataset of HDR panoramas. Our method produces accurate lighting predictions with realistic angular high-frequency details, outperforming existing state-of-the-art techniques in both quantitative and qualitative evaluations. Analysis: \u597d\u7684\uff0c\u4f5c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u4e13\u5bb6\uff0c\u6211\u5bf9\u8fd9\u7bc7\u5173\u4e8e LuxDiT \u7684\u8bba\u6587\u6458\u8981\u8fdb\u884c\u5982\u4e0b\u5206\u6790\uff1a LuxDiT: Lighting Estimation with Video Diffusion Transformer \u6458\u8981\u5206\u6790 1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u6d01\u603b\u7ed3 (2-3 \u53e5\u8bdd) LuxDiT \u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u901a\u8fc7\u5fae\u8c03\u89c6\u9891\u6269\u6563Transformer\u6765\u89e3\u51b3\u4ece\u56fe\u50cf\u6216\u89c6\u9891\u4f30\u8ba1\u573a\u666f\u7167\u660e\u7684\u957f\u671f\u6311\u6218\u3002\u8be5\u6a21\u578b\u5229\u7528\u5927\u89c4\u6a21\u5408\u6210\u6570\u636e\u96c6\u5b66\u4e60\u4ece\u95f4\u63a5\u89c6\u89c9\u7ebf\u7d22\u63a8\u65adHDR\u73af\u5883\u5149\u7167\uff0c\u5e76\u901a\u8fc7\u4f4e\u79e9\u9002\u5e94\u5fae\u8c03\u7b56\u7565\u589e\u5f3a\u8bed\u4e49\u5bf9\u9f50\uff0c\u6700\u7ec8\u751f\u6210\u9ad8\u7cbe\u5ea6\u3001\u7ec6\u8282\u4e30\u5bcc\u7684HDR\u73af\u5883\u56fe\uff0c\u8d85\u8d8a\u73b0\u6709SOTA\u3002 2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u8bba \u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u5c06 \u89c6\u9891\u6269\u6563Transformer \u67b6\u6784\u5e94\u7528\u4e8e HDR\u73af\u5883\u5149\u7167\u4f30\u8ba1 \u8fd9\u4e00\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u3002\u8fd9\u4e0e\u4f20\u7edf\u6269\u6563\u6a21\u578b\u4e3b\u8981\u7528\u4e8e\u56fe\u50cf\u751f\u6210\u4e0d\u540c\uff0c\u5b83\u9700\u8981\u4ece\u95f4\u63a5\u89c6\u89c9\u7ebf\u7d22\u4e2d\u63a8\u65ad\u5168\u5c40\uff08\u975e\u5c40\u90e8\uff09\u4e0a\u4e0b\u6587\u5e76\u8f93\u51fa\u9ad8\u52a8\u6001\u8303\u56f4\u6570\u636e\u3002 \u65b9\u6cd5\u8bba\u4e0a\uff0c\u5b83\u5de7\u5999\u5730\u7ed3\u5408\u4e86\uff1a * \u5927\u89c4\u6a21\u5408\u6210\u6570\u636e\u8bad\u7ec3 \uff1a\u514b\u670d\u771f\u5b9e\u4e16\u754cHDR\u73af\u5883\u56fe\u7a00\u7f3a\u6027\uff0c\u5b66\u4e60\u57fa\u7840\u5149\u7167\u63a8\u65ad\u80fd\u529b\u3002 * \u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\u5fae\u8c03\u7b56\u7565 \uff1a\u5229\u7528\u6536\u96c6\u5230\u7684\u771f\u5b9eHDR\u5168\u666f\u56fe\u6570\u636e\u96c6\uff0c\u9ad8\u6548\u5730\u6539\u5584\u6a21\u578b\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u8bed\u4e49\u5bf9\u9f50\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u540c\u65f6\u907f\u514d\u5bf9\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\u8fdb\u884c\u6602\u8d35\u7684\u5168\u9762\u5fae\u8c03\u3002 3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd \u9ad8\u8d28\u91cf\u6e32\u67d3\u4e0e\u865a\u62df\u5185\u5bb9\u521b\u4f5c \uff1a\u4e3a\u7535\u5f71\u3001\u6e38\u620f\u3001\u865a\u62df\u73b0\u5b9e/\u589e\u5f3a\u73b0\u5b9e\uff08VR/AR\uff09\u7b49\u9886\u57df\u63d0\u4f9b\u66f4\u51c6\u786e\u3001\u66f4\u771f\u5b9e\u7684\u573a\u666f\u5149\u7167\u4f30\u8ba1\uff0c\u6781\u5927\u63d0\u5347\u6e32\u67d3\u8d28\u91cf\u548c\u6c89\u6d78\u611f\u3002 \u9006\u5411\u56fe\u5f62\u5b66\uff08Inverse Graphics\uff09 \uff1a\u63a8\u52a8\u4ece2D\u56fe\u50cf\u6216\u89c6\u9891\u4e2d\u6062\u590d3D\u573a\u666f\u5c5e\u6027\uff08\u5982\u5149\u7167\uff09\u7684\u7814\u7a76\uff0c\u662f\u7406\u89e3\u4e16\u754c\u7684\u91cd\u8981\u4e00\u6b65\u3002 \u6570\u636e\u9a71\u52a8\u6a21\u578b\u8303\u5f0f \uff1a\u5c55\u793a\u4e86\u5982\u4f55\u6709\u6548\u5229\u7528\u5408\u6210\u6570\u636e\u514b\u670d\u771f\u5b9e\u6570\u636e\u7a00\u7f3a\u6027\uff0c\u5e76\u901a\u8fc7\u9ad8\u6548\u5fae\u8c03\u7b56\u7565\uff08\u5982LoRA\uff09\u5b9e\u73b0\u5411\u771f\u5b9e\u4e16\u754c\u7684\u6cdb\u5316\uff0c\u4e3a\u5176\u4ed6\u7c7b\u4f3c\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u8303\u4f8b\u3002 \u6269\u6563\u6a21\u578b\u5e94\u7528\u62d3\u5c55 \uff1a\u5c06\u6269\u6563\u6a21\u578b\u4ece\u5185\u5bb9\u751f\u6210\u6269\u5c55\u5230\u590d\u6742\u7684\u9006\u5411\u95ee\u9898\u89e3\u51b3\uff0c\u62d3\u5bbd\u4e86\u5176\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u7684\u5e94\u7528\u8fb9\u754c\u3002 4. \u53ef\u80fd\u53d7\u76ca\u4e8e\u8fd9\u9879\u7814\u7a76\u7684\u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 \u7535\u5f71\u4e0e\u6e38\u620f\u5236\u4f5c \uff1a\u5b9e\u73b0\u865a\u62df\u89d2\u8272\u4e0e\u771f\u5b9e\u573a\u666f\u7684\u65e0\u7f1d\u878d\u5408\uff0c\u6216\u5bf9\u73b0\u6709\u573a\u666f\u8fdb\u884c\u5149\u7167\u8c03\u6574\u3002 \u589e\u5f3a\u73b0\u5b9e\uff08AR\uff09\u4e0e\u865a\u62df\u73b0\u5b9e\uff08VR\uff09 \uff1a\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u51c6\u786e\u653e\u7f6e\u865a\u62df\u7269\u4f53\uff0c\u5e76\u4f7f\u5176\u5149\u7167\u4e0e\u73af\u5883\u4e00\u81f4\uff0c\u63d0\u5347\u771f\u5b9e\u611f\u548c\u6c89\u6d78\u611f\u3002 3D\u91cd\u5efa\u4e0e\u573a\u666f\u7406\u89e3 \uff1a\u5149\u7167\u662f\u573a\u666f\u51e0\u4f55\u548c\u6750\u8d28\u63a8\u65ad\u7684\u5173\u952e\u7ebf\u7d22\u3002 \u8ba1\u7b97\u6444\u5f71 \uff1a\u56fe\u50cf\u540e\u671f\u5904\u7406\u4e2d\u7684\u5149\u7167\u8c03\u6574\u3001\u98ce\u683c\u8fc1\u79fb\u7b49\u3002 \u6570\u5b57\u4eba\u4e0e\u865a\u62df\u5f62\u8c61 \uff1a\u4e3a\u6570\u5b57\u4eba\u63d0\u4f9b\u903c\u771f\u7684\u73af\u5883\u5149\u7167\uff0c\u4f7f\u5176\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u8868\u73b0\u81ea\u7136\u3002 5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u7684\u5c40\u9650\u6027 \u5408\u6210\u6570\u636e\u4e0e\u771f\u5b9e\u4e16\u754c\u5dee\u8ddd \uff1a\u5c3d\u7ba1\u4f7f\u7528\u4e86\u5927\u89c4\u6a21\u5408\u6210\u6570\u636e\u5e76\u8fdb\u884c\u4e86\u771f\u5b9e\u6570\u636e\u5fae\u8c03\uff0c\u4f46\u5408\u6210\u6570\u636e\u4e0e\u771f\u5b9e\u4e16\u754c\u4e4b\u95f4\u56fa\u6709\u7684\u9886\u57df\u5dee\u8ddd\uff08domain gap\uff09\u53ef\u80fd\u4ecd\u7136\u5b58\u5728\uff0c\u5c24\u5176\u662f\u5728\u6781\u7aef\u6216\u672a\u89c1\u8fc7\u7684\u771f\u5b9e\u5149\u7167\u6761\u4ef6\u4e0b\uff0c\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u53ef\u80fd\u53d7\u5230\u6311\u6218\u3002 \u5bf9\u95f4\u63a5\u89c6\u89c9\u7ebf\u7d22\u7684\u4f9d\u8d56 \uff1a\u5149\u7167\u4f30\u8ba1\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u573a\u666f\u4e2d\u7684\u9634\u5f71\u3001\u53cd\u5c04\u3001\u9ad8\u5149\u7b49\u95f4\u63a5\u7ebf\u7d22\u3002\u5728\u8fd9\u4e9b\u7ebf\u7d22\u4e0d\u660e\u663e\u3001\u6a21\u7cca\u6216\u88ab\u906e\u6321\u7684\u573a\u666f\u4e2d\uff0c\u6a21\u578b\u7684\u9c81\u68d2\u6027\u53ef\u80fd\u4e0b\u964d\u3002 \u5168\u5c40\u4e0a\u4e0b\u6587\u63a8\u65ad\u7684\u5c40\u9650\u6027 \uff1a\u4ece\u6709\u9650\u7684\u89c6\u89c9\u8f93\u5165\uff08\u5355\u5f20\u56fe\u50cf\u6216\u89c6\u9891\u7247\u6bb5\uff09\u63a8\u65ad\u6574\u4e2a360\u5ea6HDR\u73af\u5883\u56fe\uff0c\u672c\u8d28\u4e0a\u662f\u4e00\u4e2a\u6b20\u5b9a\u95ee\u9898\u3002\u6a21\u578b\u53ef\u80fd\u96be\u4ee5\u51c6\u786e\u6355\u6349\u5230\u8f93\u5165\u89c6\u56fe\u4e4b\u5916\u7684\u590d\u6742\u6216\u906e\u6321\u7684\u5149\u7167\u4fe1\u606f\u3002 \u8ba1\u7b97\u6210\u672c \uff1a\u89c6\u9891\u6269\u6563Transformer\u6a21\u578b\u901a\u5e38\u8ba1\u7b97\u91cf\u8f83\u5927\uff0c\u5c24\u5176\u662f\u5728\u751f\u6210\u9ad8\u5206\u8fa8\u7387HDR\u8f93\u51fa\u65f6\uff0c\u63a8\u7406\u901f\u5ea6\u548c\u8d44\u6e90\u6d88\u8017\u53ef\u80fd\u662f\u4e00\u4e2a\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u8003\u91cf\u3002 \u5fae\u8c03\u6570\u636e\u96c6\u7684\u8d28\u91cf\u4e0e\u591a\u6837\u6027 \uff1a\u867d\u7136\u63d0\u5230\u4e86\u4f7f\u7528\u201c\u6536\u96c6\u5230\u7684HDR\u5168\u666f\u56fe\u6570\u636e\u96c6\u201d\u8fdb\u884cLoRA\u5fae\u8c03\uff0c\u4f46\u8be5\u6570\u636e\u96c6\u7684\u89c4\u6a21\u3001\u591a\u6837\u6027\u548c\u4ee3\u8868\u6027\u5c06\u76f4\u63a5\u5f71\u54cd\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u7684\u6700\u7ec8\u6027\u80fd\u548c\u8bed\u4e49\u5bf9\u9f50\u6548\u679c\u3002 Key Findings: We propose LuxDiT, a novel data-driven approach that fine-tunes a video diffusion transformer to generate HDR environment maps conditioned on visual input. To improve semantic alignment between the input and the predicted environment map, we introduce a low-rank adaptation finetuning strategy using a collected dataset of HDR panoramas. Our method produces accurate lighting predictions with realistic angular high-frequency details, outperforming existing state-of-the-art techniques in both quantitative and qualitative evaluations. Links: PDF arXiv OneCAT: Decoder-Only Auto-Regressive Model for Unified Understanding and Generation Authors: Han Li, Xinyu Peng, Yaoming Wang, Zelin Peng, Xin Chen, Rongxiang Weng, Jingang Wang, Xunliang Cai, Wenrui Dai, Hongkai Xiong Published: 2025-09-03 Categories: cs.CV Abstract: We introduce OneCAT, a unified multimodal model that seamlessly integrates understanding, generation, and editing within a novel, pure decoder-only transformer architecture. Our framework uniquely eliminates the need for external components such as Vision Transformers (ViT) or vision tokenizer during inference, leading to significant efficiency gains, especially for high-resolution inputs. This is achieved through a modality-specific Mixture-of-Experts (MoE) structure trained with a single autoregressive (AR) objective, which also natively supports dynamic resolutions. Furthermore, we pioneer a multi-scale visual autoregressive mechanism within the Large Language Model (LLM) that drastically reduces decoding steps compared to diffusion-based methods while maintaining state-of-the-art performance. Our findings demonstrate the powerful potential of pure autoregressive modeling as a sufficient and elegant foundation for unified multimodal intelligence. As a result, OneCAT sets a new performance standard, outperforming existing open-source unified multimodal models across benchmarks for multimodal generation, editing, and understanding. Analysis: \u8fd9\u7bc7\u8bba\u6587\u7684\u6458\u8981\u5c55\u793a\u4e86\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u4e00\u4e2a\u91cd\u8981\u8fdb\u5c55\uff0c\u7279\u522b\u662f\u5728\u7edf\u4e00\u591a\u6a21\u6001\u667a\u80fd\u65b9\u9762\u3002 1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e (Main Contribution) OneCAT\u5f15\u5165\u4e86\u4e00\u4e2a\u7eaf\u89e3\u7801\u5668Transformer\u67b6\u6784\u7684\u7edf\u4e00\u591a\u6a21\u6001\u6a21\u578b\uff0c\u65e0\u7f1d\u6574\u5408\u4e86\u7406\u89e3\u3001\u751f\u6210\u548c\u7f16\u8f91\u4efb\u52a1\u3002\u5176\u6838\u5fc3\u8d21\u732e\u5728\u4e8e\u63a8\u7406\u65f6\u65e0\u9700\u5916\u90e8\u89c6\u89c9\u7ec4\u4ef6\uff08\u5982Vision Transformer\u6216\u89c6\u89c9tokenizer\uff09\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9ad8\u5206\u8fa8\u7387\u8f93\u5165\u7684\u6548\u7387\uff0c\u5e76\u901a\u8fc7\u5355\u4e00\u81ea\u56de\u5f52\u76ee\u6807\u548c\u591a\u5c3a\u5ea6\u89c6\u89c9\u81ea\u56de\u5f52\u673a\u5236\u5b9e\u73b0\u4e86\u8de8\u591a\u6a21\u6001\u57fa\u51c6\u7684SOTA\u6027\u80fd\u3002 2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5 (Key Innovation or Methodological Approach) \u7eaf\u89e3\u7801\u5668Transformer\u67b6\u6784 (Pure Decoder-Only Transformer Architecture): \u6452\u5f03\u4e86\u4f20\u7edf\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u6216\u5e26\u6709\u72ec\u7acb\u89c6\u89c9\u7f16\u7801\u5668\u7684\u67b6\u6784\uff0c\u4ec5\u4f7f\u7528\u4e00\u4e2a\u89e3\u7801\u5668\u6765\u5904\u7406\u6240\u6709\u6a21\u6001\u7684\u8f93\u5165\u548c\u8f93\u51fa\u3002 \u63a8\u7406\u65f6\u65e0\u9700\u5916\u90e8\u89c6\u89c9\u7ec4\u4ef6 (Elimination of External Vision Components during Inference): \u901a\u8fc7\u6a21\u6001\u7279\u5b9a\u7684\u4e13\u5bb6\u6df7\u5408 (Mixture-of-Experts, MoE) \u7ed3\u6784\uff0c\u6a21\u578b\u5728\u63a8\u7406\u65f6\u53ef\u4ee5\u76f4\u63a5\u5904\u7406\u539f\u59cb\u89c6\u89c9\u8f93\u5165\uff0c\u65e0\u9700\u9884\u5904\u7406\u7684ViT\u6216\u89c6\u89c9tokenizer\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6548\u7387\u3002 \u5355\u4e00\u81ea\u56de\u5f52 (AR) \u76ee\u6807\u8bad\u7ec3 (Single Autoregressive Objective Training): \u6574\u4e2a\u6a21\u578b\u901a\u8fc7\u4e00\u4e2a\u7edf\u4e00\u7684\u81ea\u56de\u5f52\u76ee\u6807\u8fdb\u884c\u8bad\u7ec3\uff0c\u7b80\u5316\u4e86\u8bad\u7ec3\u8303\u5f0f\uff0c\u5e76\u539f\u751f\u652f\u6301\u52a8\u6001\u5206\u8fa8\u7387\u3002 \u591a\u5c3a\u5ea6\u89c6\u89c9\u81ea\u56de\u5f52\u673a\u5236 (Multi-scale Visual Autoregressive Mechanism): \u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u5185\u90e8\u5f15\u5165\u4e86\u8fd9\u79cd\u673a\u5236\uff0c\u4e0e\u6269\u6563\u6a21\u578b\u76f8\u6bd4\uff0c\u5b83\u80fd\u5927\u5e45\u51cf\u5c11\u89e3\u7801\u6b65\u9aa4\uff0c\u540c\u65f6\u4fdd\u6301\u9886\u5148\u7684\u6027\u80fd\u3002 3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd (Potential Impact on the Field) \u63a8\u52a8\u7edf\u4e00\u591a\u6a21\u6001\u6a21\u578b\u8303\u5f0f\u53d1\u5c55: OneCAT\u8bc1\u660e\u4e86\u7eaf\u81ea\u56de\u5f52\u5efa\u6a21\u4f5c\u4e3a\u7edf\u4e00\u591a\u6a21\u6001\u667a\u80fd\u57fa\u7840\u7684\u5f3a\u5927\u6f5c\u529b\uff0c\u53ef\u80fd\u5f15\u5bfc\u672a\u6765\u7814\u7a76\u8f6c\u5411\u66f4\u7b80\u6d01\u3001\u4f18\u96c5\u7684\u67b6\u6784\u3002 \u663e\u8457\u63d0\u5347\u591a\u6a21\u6001\u63a8\u7406\u6548\u7387: \u7279\u522b\u662f\u5bf9\u4e8e\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u548c\u89c6\u9891\u5904\u7406\uff0c\u65e0\u9700\u5916\u90e8\u89c6\u89c9\u7ec4\u4ef6\u548c\u51cf\u5c11\u89e3\u7801\u6b65\u9aa4\u7684\u7279\u6027\uff0c\u5c06\u6781\u5927\u5730\u52a0\u901f\u591a\u6a21\u6001\u5e94\u7528\u7684\u90e8\u7f72\u548c\u5b9e\u65f6\u6027\u3002 \u7b80\u5316\u6a21\u578b\u67b6\u6784\u548c\u90e8\u7f72: \u51cf\u5c11\u5bf9\u591a\u4e2a\u72ec\u7acb\u7ec4\u4ef6\u7684\u4f9d\u8d56\uff0c\u4f7f\u5f97\u591a\u6a21\u6001\u6a21\u578b\u7684\u5f00\u53d1\u3001\u8bad\u7ec3\u548c\u90e8\u7f72\u8fc7\u7a0b\u66f4\u52a0\u7b80\u5316\u548c\u9ad8\u6548\u3002 \u4e3a\u591a\u6a21\u6001\u667a\u80fd\u8bbe\u5b9a\u65b0\u6027\u80fd\u6807\u51c6: \u5728\u751f\u6210\u3001\u7f16\u8f91\u548c\u7406\u89e3\u7b49\u591a\u4e2a\u4efb\u52a1\u4e0a\u8d85\u8d8a\u73b0\u6709\u5f00\u6e90\u6a21\u578b\uff0c\u5c06\u6fc0\u52b1\u793e\u533a\u8fdb\u4e00\u6b65\u63a2\u7d22\u548c\u4f18\u5316\u7edf\u4e00\u591a\u6a21\u6001\u6a21\u578b\u3002 4. \u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 (Related Areas or Applications that Might Benefit from this Research) \u591a\u6a21\u6001\u5185\u5bb9\u521b\u4f5c: \u6587\u672c\u5230\u56fe\u50cf/\u89c6\u9891\u751f\u6210\u3001\u56fe\u50cf\u7f16\u8f91\u3001\u98ce\u683c\u8fc1\u79fb\u3001\u521b\u610f\u8bbe\u8ba1\u5de5\u5177\u3002 \u9ad8\u7ea7\u89c6\u89c9\u7406\u89e3: \u56fe\u50cf/\u89c6\u9891\u95ee\u7b54 (VQA)\u3001\u8be6\u7ec6\u63cf\u8ff0\u751f\u6210\u3001\u573a\u666f\u7406\u89e3\u3001\u4e8b\u4ef6\u68c0\u6d4b\u3002 \u4eba\u673a\u4ea4\u4e92: \u66f4\u81ea\u7136\u3001\u9ad8\u6548\u7684\u89c6\u89c9\u4ea4\u4e92\u754c\u9762\uff0c\u4f8b\u5982\u901a\u8fc7\u6587\u672c\u6307\u4ee4\u76f4\u63a5\u7f16\u8f91\u56fe\u50cf\u6216\u751f\u6210\u89c6\u89c9\u5185\u5bb9\u3002 \u8f85\u52a9\u6280\u672f: \u4e3a\u89c6\u89c9\u969c\u788d\u4eba\u58eb\u63d0\u4f9b\u66f4\u51c6\u786e\u3001\u5b9e\u65f6\u7684\u56fe\u50cf\u548c\u89c6\u9891\u63cf\u8ff0\u3002 \u5177\u8eab\u667a\u80fd/\u673a\u5668\u4eba: \u673a\u5668\u4eba\u901a\u8fc7\u89c6\u89c9\u611f\u77e5\u73af\u5883\u3001\u7406\u89e3\u6307\u4ee4\u5e76\u751f\u6210\u76f8\u5e94\u7684\u89c6\u89c9\u53cd\u9988\u6216\u884c\u52a8\u3002 \u533b\u7597\u5f71\u50cf\u5206\u6790: \u7ed3\u5408\u6587\u672c\u62a5\u544a\u751f\u6210\u5f71\u50cf\u3001\u5bf9\u5f71\u50cf\u8fdb\u884c\u7f16\u8f91\u4ee5\u8f85\u52a9\u8bca\u65ad\u3001\u4ece\u5f71\u50cf\u4e2d\u63d0\u53d6\u5173\u952e\u4fe1\u606f\u3002 5. \u53ef\u63a8\u65ad\u7684\u5c40\u9650\u6027 (Limitations that Can Be Inferred from the Abstract) \u8bad\u7ec3\u6210\u672c: \u7edf\u4e00\u591a\u6a21\u6001\u6a21\u578b\uff0c\u7279\u522b\u662f\u7ed3\u5408MoE\u7ed3\u6784\uff0c\u901a\u5e38\u9700\u8981\u5de8\u5927\u7684\u8ba1\u7b97\u8d44\u6e90\u548c\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u3002\u6458\u8981\u4e2d\u672a\u63d0\u53ca\u8bad\u7ec3\u7684\u89c4\u6a21\u548c\u6210\u672c\uff0c\u8fd9\u53ef\u80fd\u662f\u4e00\u4e2a\u6f5c\u5728\u7684\u6311\u6218\u3002 MoE\u7684\u590d\u6742\u6027\u4e0e\u8d1f\u8f7d\u5747\u8861: \u5c3d\u7ba1\u63a8\u7406\u65f6\u6548\u7387\u9ad8\uff0c\u4f46MoE\u7ed3\u6784\u5728\u8bad\u7ec3\u548c\u7ef4\u62a4\u4e0a\u53ef\u80fd\u589e\u52a0\u590d\u6742\u6027\uff0c\u5e76\u9700\u8981\u7cbe\u7ec6\u7684\u8d1f\u8f7d\u5747\u8861\u7b56\u7565\u6765\u786e\u4fdd\u4e13\u5bb6\u7f51\u7edc\u7684\u6709\u6548\u5229\u7528\u3002 \u81ea\u56de\u5f52\u751f\u6210\u56fa\u6709\u9650\u5236: \u5c3d\u7ba1\u58f0\u79f0\u51cf\u5c11\u4e86\u89e3\u7801\u6b65\u9aa4\uff0c\u4f46\u7eaf\u81ea\u56de\u5f52\u751f\u6210\u5728\u67d0\u4e9b\u573a\u666f\u4e0b\u4ecd\u53ef\u80fd\u9762\u4e34\u751f\u6210\u901f\u5ea6\uff08\u76f8\u5bf9\u4e8e\u5b8c\u5168\u5e76\u884c\uff09\u6216\u751f\u6210\u591a\u6837\u6027\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u975e\u5e38\u957f\u7684\u5e8f\u5217\u65f6\u3002 \u591a\u5c3a\u5ea6\u673a\u5236\u7684\u6cdb\u5316\u6027: \u8fd9\u79cd\u65b0\u9896\u7684\u591a\u5c3a\u5ea6\u89c6\u89c9\u81ea\u56de\u5f52\u673a\u5236\u5728\u5904\u7406\u6781\u7aef\u590d\u6742\u6216\u7279\u5b9a\u9886\u57df\u89c6\u89c9\u6570\u636e\uff08\u5982\u533b\u5b66\u5f71\u50cf\u3001\u536b\u661f\u56fe\u50cf\uff09\u65f6\u7684\u9c81\u68d2\u6027\u4ecd\u9700\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u3002 \u5bf9\u65b0\u6a21\u6001\u7684\u6269\u5c55\u6027: \u62bd\u8c61\u4e2d\u4e3b\u8981\u63d0\u53ca\u89c6\u89c9\u548c\u6587\u672c\uff0c\u6a21\u578b\u5982\u4f55\u65e0\u7f1d\u6269\u5c55\u5230\u5176\u4ed6\u6a21\u6001\uff08\u5982\u97f3\u9891\u30013D\u6570\u636e\u3001\u89e6\u89c9\u4fe1\u606f\uff09\u53ef\u80fd\u662f\u4e00\u4e2a\u672a\u6765\u7684\u8003\u91cf\u3002 Key Findings: We introduce OneCAT, a unified multimodal model that seamlessly integrates understanding, generation, and editing within a novel, pure decoder-only transformer architecture. Furthermore, we pioneer a multi-scale visual autoregressive mechanism within the Large Language Model (LLM) that drastically reduces decoding steps compared to diffusion-based methods while maintaining state-of-the-art performance. As a result, OneCAT sets a new performance standard, outperforming existing open-source unified multimodal models across benchmarks for multimodal generation, editing, and understanding. Links: PDF arXiv A Generative Foundation Model for Chest Radiography Authors: Yuanfeng Ji, Dan Lin, Xiyue Wang, Lu Zhang, Wenhui Zhou, Chongjian Ge, Ruihang Chu, Xiaoli Yang, Junhan Zhao, Junsong Chen, Xiangde Luo, Sen Yang, Jin Fang, Ping Luo, Ruijiang Li Published: 2025-09-04 Categories: cs.CV Abstract: The scarcity of well-annotated diverse medical images is a major hurdle for developing reliable AI models in healthcare. Substantial technical advances have been made in generative foundation models for natural images. Here we develop `ChexGen', a generative vision-language foundation model that introduces a unified framework for text-, mask-, and bounding box-guided synthesis of chest radiographs. Built upon the latent diffusion transformer architecture, ChexGen was pretrained on the largest curated chest X-ray dataset to date, consisting of 960,000 radiograph-report pairs. ChexGen achieves accurate synthesis of radiographs through expert evaluations and quantitative metrics. We demonstrate the utility of ChexGen for training data augmentation and supervised pretraining, which led to performance improvements across disease classification, detection, and segmentation tasks using a small fraction of training data. Further, our model enables the creation of diverse patient cohorts that enhance model fairness by detecting and mitigating demographic biases. Our study supports the transformative role of generative foundation models in building more accurate, data-efficient, and equitable medical AI systems. Analysis: \u8fd9\u7bc7\u8bba\u6587\u6458\u8981\u5c55\u793a\u4e86\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u5728\u533b\u7597AI\u5e94\u7528\u65b9\u9762\u7684\u4e00\u4e2a\u91cd\u8981\u8fdb\u5c55\u3002\u4ee5\u4e0b\u662f\u8be6\u7ec6\u5206\u6790\uff1a 1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u660e\u6458\u8981 (2-3\u53e5\u8bdd) \u672c\u6587\u63d0\u51fa\u4e86 ChexGen \uff0c\u4e00\u4e2a\u7528\u4e8e\u80f8\u90e8X\u5c04\u7ebf\u56fe\u50cf\u7684\u751f\u6210\u5f0f\u89c6\u89c9-\u8bed\u8a00\u57fa\u7840\u6a21\u578b\uff0c\u65e8\u5728\u89e3\u51b3\u533b\u5b66\u56fe\u50cf\u6807\u6ce8\u7a00\u7f3a\u7684\u95ee\u9898\u3002\u5b83\u57fa\u4e8e\u6f5c\u5728\u6269\u6563Transformer\u67b6\u6784\uff0c\u80fd\u591f\u5b9e\u73b0\u6587\u672c\u3001\u63a9\u7801\u548c\u8fb9\u754c\u6846\u5f15\u5bfc\u7684\u56fe\u50cf\u5408\u6210\uff0c\u5e76\u5728\u8fc4\u4eca\u4e3a\u6b62\u6700\u5927\u7684\u80f8\u90e8X\u5c04\u7ebf\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u9884\u8bad\u7ec3\u3002\u8be5\u6a21\u578b\u5728\u6570\u636e\u589e\u5f3a\u3001\u9884\u8bad\u7ec3\u4ee5\u53ca\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u548c\u6a21\u578b\u516c\u5e73\u6027\u65b9\u9762\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u9884\u793a\u7740\u751f\u6210\u5f0f\u57fa\u7840\u6a21\u578b\u5728\u6784\u5efa\u66f4\u51c6\u786e\u3001\u6570\u636e\u9ad8\u6548\u548c\u516c\u5e73\u7684\u533b\u7597AI\u7cfb\u7edf\u4e2d\u7684\u53d8\u9769\u6027\u4f5c\u7528\u3002 2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u5b66\u65b9\u6cd5 \u7edf\u4e00\u7684\u751f\u6210\u6846\u67b6\uff1a ChexGen\u7684\u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u5176\u63d0\u4f9b\u4e86\u4e00\u4e2a \u7edf\u4e00\u7684\u6846\u67b6 \uff0c\u80fd\u591f\u5b9e\u73b0 \u6587\u672c\u3001\u63a9\u7801\u548c\u8fb9\u754c\u6846\u5f15\u5bfc \u7684\u80f8\u90e8X\u5c04\u7ebf\u56fe\u50cf\u5408\u6210\u3002\u8fd9\u610f\u5473\u7740\u7528\u6237\u53ef\u4ee5\u901a\u8fc7\u591a\u79cd\u6a21\u6001\uff08\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u3001\u533a\u57df\u63a9\u7801\u6216\u8fb9\u754c\u6846\uff09\u6765\u7cbe\u786e\u63a7\u5236\u56fe\u50cf\u7684\u751f\u6210\u5185\u5bb9\u548c\u7ed3\u6784\uff0c\u8fd9\u5728\u533b\u5b66\u56fe\u50cf\u751f\u6210\u9886\u57df\u662f\u9ad8\u5ea6\u7075\u6d3b\u548c\u65b0\u9896\u7684\u3002 \u751f\u6210\u5f0f\u89c6\u89c9-\u8bed\u8a00\u57fa\u7840\u6a21\u578b\uff1a \u5c06\u201c\u751f\u6210\u5f0f\u6a21\u578b\u201d\u4e0e\u201c\u89c6\u89c9-\u8bed\u8a00\u201d\u80fd\u529b\u76f8\u7ed3\u5408\uff0c\u5e76\u5c06\u5176\u5b9a\u4f4d\u4e3a\u201c\u57fa\u7840\u6a21\u578b\u201d\uff0c\u8868\u660e\u5176\u65e8\u5728\u901a\u8fc7\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u5b66\u4e60\u901a\u7528\u8868\u793a\uff0c\u5e76\u80fd\u9002\u5e94\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\u3002 \u6f5c\u5728\u6269\u6563Transformer\u67b6\u6784\uff1a \u91c7\u7528\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u7684\u751f\u6210\u6a21\u578b\u67b6\u6784\u4e4b\u4e00\u2014\u2014\u6f5c\u5728\u6269\u6563Transformer\u3002\u8fd9\u79cd\u67b6\u6784\u4ee5\u5176\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\u751f\u6210\u80fd\u529b\u548c\u5bf9\u590d\u6742\u6570\u636e\u5206\u5e03\u7684\u5efa\u6a21\u80fd\u529b\u800c\u95fb\u540d\uff0c\u5c06\u5176\u5e94\u7528\u4e8e\u533b\u5b66\u5f71\u50cf\u9886\u57df\u662f\u524d\u6cbf\u5b9e\u8df5\u3002 \u5927\u89c4\u6a21\u533b\u5b66\u6570\u636e\u96c6\u9884\u8bad\u7ec3\uff1a \u5728\u5305\u542b960,000\u5bf9\u653e\u5c04\u56fe\u50cf-\u62a5\u544a\u7684\u201c\u8fc4\u4eca\u4e3a\u6b62\u6700\u5927\u7684\u201d\u80f8\u90e8X\u5c04\u7ebf\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u8fd9\u4e3a\u6a21\u578b\u5b66\u4e60\u5230\u4e30\u5bcc\u7684\u533b\u5b66\u77e5\u8bc6\u548c\u56fe\u50cf\u7279\u5f81\u63d0\u4f9b\u4e86\u575a\u5b9e\u57fa\u7840\uff0c\u662f\u5b9e\u73b0\u5176\u201c\u57fa\u7840\u6a21\u578b\u201d\u80fd\u529b\u7684\u5173\u952e\u3002 3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd \u7f13\u89e3\u533b\u5b66\u6570\u636e\u7a00\u7f3a\u6027\uff1a \u8fd9\u662f\u6700\u76f4\u63a5\u548c\u663e\u8457\u7684\u5f71\u54cd\u3002\u901a\u8fc7\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u591a\u6837\u5316\u7684\u5408\u6210\u533b\u5b66\u56fe\u50cf\uff0cChexGen\u80fd\u591f\u6709\u6548\u8865\u5145\u771f\u5b9e\u6807\u6ce8\u6570\u636e\u7684\u4e0d\u8db3\uff0c\u6781\u5927\u5730\u964d\u4f4e\u4e86\u5f00\u53d1\u548c\u8bad\u7ec3\u9ad8\u6027\u80fd\u533b\u7597AI\u6a21\u578b\u7684\u95e8\u69db\u3002 \u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u548c\u6570\u636e\u6548\u7387\uff1a \u4f5c\u4e3a\u6570\u636e\u589e\u5f3a\u5de5\u5177\u548c\u9884\u8bad\u7ec3\u7b56\u7565\uff0cChexGen\u80fd\u591f\u663e\u8457\u63d0\u5347\u75be\u75c5\u5206\u7c7b\u3001\u68c0\u6d4b\u548c\u5206\u5272\u7b49\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u4ec5\u6709\u5c11\u91cf\u771f\u5b9e\u8bad\u7ec3\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u8fd9\u5bf9\u4e8e\u5feb\u901f\u8fed\u4ee3\u548c\u90e8\u7f72\u533b\u7597AI\u6a21\u578b\u81f3\u5173\u91cd\u8981\u3002 \u4fc3\u8fdb\u6a21\u578b\u516c\u5e73\u6027\u4e0e\u504f\u89c1\u7f13\u89e3\uff1a \u80fd\u591f\u521b\u5efa\u591a\u6837\u5316\u7684\u60a3\u8005\u961f\u5217\uff0c\u7528\u4e8e\u68c0\u6d4b\u548c\u7f13\u89e3AI\u6a21\u578b\u4e2d\u7684\u4eba\u53e3\u7edf\u8ba1\u5b66\u504f\u89c1\uff0c\u8fd9\u5728\u533b\u7597AI\u9886\u57df\u5177\u6709\u6df1\u8fdc\u7684\u793e\u4f1a\u548c\u4f26\u7406\u610f\u4e49\u3002\u5b83\u4e3a\u6784\u5efa\u66f4\u516c\u5e73\u3001\u66f4\u503c\u5f97\u4fe1\u8d56\u7684\u533b\u7597AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u5de5\u5177\u3002 \u63a8\u52a8\u533b\u5b66AI\u57fa\u7840\u6a21\u578b\u53d1\u5c55\uff1a ChexGen\u7684\u6210\u529f\u5c06\u6fc0\u52b1\u66f4\u591a\u7814\u7a76\u8005\u63a2\u7d22\u5728\u5176\u4ed6\u533b\u5b66\u5f71\u50cf\u6a21\u6001\uff08\u5982CT\u3001MRI\uff09\u548c\u75be\u75c5\u9886\u57df\u5f00\u53d1\u7c7b\u4f3c\u7684\u751f\u6210\u5f0f\u57fa\u7840\u6a21\u578b\uff0c\u52a0\u901f\u6574\u4e2a\u533b\u5b66AI\u9886\u57df\u7684\u53d1\u5c55\u3002 4. \u53ef\u80fd\u53d7\u76ca\u7684\u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 \u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e0e\u8bca\u65ad\uff1a \u76f4\u63a5\u5e94\u7528\u4e8e\u75be\u75c5\u5206\u7c7b\u3001\u68c0\u6d4b\u548c\u5206\u5272\uff0c\u8f85\u52a9\u533b\u751f\u8fdb\u884c\u8bca\u65ad\u3002 \u533b\u7597AI\u6a21\u578b\u5f00\u53d1\u4e0e\u90e8\u7f72\uff1a \u4f5c\u4e3a\u6570\u636e\u589e\u5f3a\u5de5\u5177\uff0c\u52a0\u901f\u65b0\u6a21\u578b\u7684\u8bad\u7ec3\u548c\u8fed\u4ee3\uff1b\u4f5c\u4e3a\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u63d0\u5347\u6a21\u578b\u5728\u5c0f\u6837\u672c\u6570\u636e\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u3002 \u6a21\u578b\u516c\u5e73\u6027\u4e0e\u504f\u89c1\u7f13\u89e3\u7814\u7a76\uff1a \u7528\u4e8e\u751f\u6210\u5177\u6709\u7279\u5b9a\u4eba\u53e3\u7edf\u8ba1\u5b66\u7279\u5f81\u7684\u5408\u6210\u6570\u636e\uff0c\u4ee5\u8bc6\u522b\u3001\u91cf\u5316\u548c\u7f13\u89e3AI\u6a21\u578b\u4e2d\u7684\u504f\u89c1\u3002 \u533b\u7597\u6559\u80b2\u4e0e\u6a21\u62df\uff1a \u751f\u6210\u5404\u79cd\u75c5\u7406\u56fe\u50cf\u7528\u4e8e\u6559\u5b66\u548c\u533b\u751f\u57f9\u8bad\uff0c\u63d0\u4f9b\u591a\u6837\u5316\u7684\u5b66\u4e60\u6848\u4f8b\u3002 \u9690\u79c1\u4fdd\u62a4\u6570\u636e\u5171\u4eab\u4e0e\u7814\u7a76\uff1a \u5728\u67d0\u4e9b\u573a\u666f\u4e0b\uff0c\u5408\u6210\u6570\u636e\u53ef\u4ee5\u4f5c\u4e3a\u771f\u5b9e\u6570\u636e\u7684\u66ff\u4ee3\u54c1\uff0c\u7528\u4e8e\u7814\u7a76\u548c\u5f00\u53d1\uff0c\u540c\u65f6\u4fdd\u62a4\u60a3\u8005\u9690\u79c1\u3002 \u4e2a\u6027\u5316\u533b\u7597\uff1a \u7406\u8bba\u4e0a\uff0c\u672a\u6765\u53ef\u4ee5\u6839\u636e\u7279\u5b9a\u60a3\u8005\u7684\u7279\u5f81\u751f\u6210\u5b9a\u5236\u5316\u7684\u6a21\u62df\u56fe\u50cf\uff0c\u7528\u4e8e\u6cbb\u7597\u65b9\u6848\u7684\u89c4\u5212\u548c\u8bc4\u4f30\u3002 5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u7684\u5c40\u9650\u6027 \u5408\u6210\u56fe\u50cf\u7684\u4e34\u5e8a\u771f\u5b9e\u6027\u4e0e\u7ec6\u8282\uff1a \u5c3d\u7ba1\u6458\u8981\u58f0\u79f0\u201c\u51c6\u786e\u5408\u6210\u201d\uff0c\u4f46\u5bf9\u4e8e\u751f\u6210\u56fe\u50cf\u5728\u4e34\u5e8a\u4e0a\u662f\u5426\u80fd\u5b8c\u5168\u6a21\u62df\u771f\u5b9e\u75c5\u7406\u7684\u7ec6\u5fae\u7279\u5f81\u3001\u7f55\u89c1\u75c5\u53d8\u6216\u590d\u6742\u75c5\u7406\u5171\u5b58\u7684\u60c5\u51b5\uff0c\u4ecd\u9700\u66f4\u6df1\u5165\u7684\u9a8c\u8bc1\u3002\u751f\u6210\u6a21\u578b\u53ef\u80fd\u5b58\u5728\u201c\u5e7b\u89c9\u201d\u6216\u751f\u6210\u4e34\u5e8a\u4e0a\u4e0d\u5408\u7406\u7684\u7279\u5f81\u7684\u98ce\u9669\u3002 \u6570\u636e\u96c6\u7684\u8986\u76d6\u8303\u56f4\u4e0e\u591a\u6837\u6027\uff1a \u5c3d\u7ba1\u4f7f\u7528\u4e86\u201c\u8fc4\u4eca\u4e3a\u6b62\u6700\u5927\u7684\u201d\u6570\u636e\u96c6\uff0c\u4f46\u533b\u5b66\u5f71\u50cf\u7684\u590d\u6742\u6027\u548c\u591a\u6837\u6027\u662f\u65e0\u9650\u7684\u3002\u8be5\u6a21\u578b\u53ef\u80fd\u5728\u8bad\u7ec3\u6570\u636e\u4e2d\u672a\u5145\u5206\u4ee3\u8868\u7684\u7f55\u89c1\u75be\u75c5\u3001\u7279\u5b9a\u4eba\u7fa4\u6216\u590d\u6742\u75c5\u7406\u6a21\u5f0f\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002 \u504f\u89c1\u7f13\u89e3\u7684\u5b9e\u9645\u6548\u679c\uff1a \u6458\u8981\u63d0\u5230\u80fd\u591f\u68c0\u6d4b\u548c\u7f13\u89e3\u4eba\u53e3\u7edf\u8ba1\u5b66\u504f\u89c1\uff0c\u4f46\u672a\u8be6\u7ec6\u8bf4\u660e\u5176\u6548\u679c\u7684\u91cf\u5316\u8bc4\u4f30\u548c\u5c40\u9650\u6027\u3002\u751f\u6210\u591a\u6837\u5316\u961f\u5217\u662f\u5426\u80fd\u5b8c\u5168\u6d88\u9664\u6240\u6709\u6f5c\u5728\u504f\u89c1\uff0c\u4ee5\u53ca\u662f\u5426\u4f1a\u5f15\u5165\u65b0\u7684\u5408\u6210\u504f\u89c1\uff0c\u4ecd\u662f\u5f00\u653e\u95ee\u9898\u3002 \u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\uff1a \u57fa\u7840\u6a21\u578b\u901a\u5e38\u9700\u8981\u5927\u91cf\u7684\u8ba1\u7b97\u8d44\u6e90\u8fdb\u884c\u8bad\u7ec3\u548c\u63a8\u7406\uff0c\u8fd9\u53ef\u80fd\u9650\u5236\u5176\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u3002 \u5bf9\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u63d0\u5347\u7684\u7a0b\u5ea6\uff1a \u6458\u8981\u6307\u51fa\u201c\u4f7f\u7528\u4e00\u5c0f\u90e8\u5206\u8bad\u7ec3\u6570\u636e\u201d\u5373\u53ef\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u672a\u660e\u786e\u8fd9\u79cd\u63d0\u5347\u662f\u5426\u80fd\u8fbe\u5230\u6216\u8d85\u8d8a\u4f7f\u7528\u5b8c\u6574\u771f\u5b9e\u6570\u636e\u96c6\u8bad\u7ec3\u7684\u6a21\u578b\u6027\u80fd\uff0c\u4ee5\u53ca\u201c\u4e00\u5c0f\u90e8\u5206\u201d\u7684\u5177\u4f53\u91cf\u5316\u6807\u51c6\u3002 Key Findings: We demonstrate the utility of ChexGen for training data augmentation and supervised pretraining, which led to performance improvements across disease classification, detection, and segmentation tasks using a small fraction of training data. Links: PDF arXiv OVGrasp: Open-Vocabulary Grasping Assistance via Multimodal Intent Detection Authors: Chen Hu, Shan Luo, Letizia Gionfrida Published: 2025-09-04 Categories: cs.RO, cs.CV Abstract: Grasping assistance is essential for restoring autonomy in individuals with motor impairments, particularly in unstructured environments where object categories and user intentions are diverse and unpredictable. We present OVGrasp, a hierarchical control framework for soft exoskeleton-based grasp assistance that integrates RGB-D vision, open-vocabulary prompts, and voice commands to enable robust multimodal interaction. To enhance generalization in open environments, OVGrasp incorporates a vision-language foundation model with an open-vocabulary mechanism, allowing zero-shot detection of previously unseen objects without retraining. A multimodal decision-maker further fuses spatial and linguistic cues to infer user intent, such as grasp or release, in multi-object scenarios. We deploy the complete framework on a custom egocentric-view wearable exoskeleton and conduct systematic evaluations on 15 objects across three grasp types. Experimental results with ten participants demonstrate that OVGrasp achieves a grasping ability score (GAS) of 87.00%, outperforming state-of-the-art baselines and achieving improved kinematic alignment with natural hand motion. Analysis: \u4f5c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u4e13\u5bb6\uff0c\u6211\u5bf9\u8fd9\u7bc7\u8bba\u6587\u6458\u8981\u7684\u5206\u6790\u5982\u4e0b\uff1a OVGrasp: Open-Vocabulary Grasping Assistance via Multimodal Intent Detection 1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u660e\u6458\u8981 (Concise Summary) OVGrasp \u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u8f6f\u4f53\u5916\u9aa8\u9abc\u7684\u5c42\u7ea7\u63a7\u5236\u6846\u67b6\uff0c\u65e8\u5728\u4e3a\u8fd0\u52a8\u969c\u788d\u8005\u63d0\u4f9b\u5f00\u653e\u8bcd\u6c47\u6293\u53d6\u8f85\u52a9\u3002\u5b83\u901a\u8fc7\u6574\u5408 RGB-D \u89c6\u89c9\u3001\u5f00\u653e\u8bcd\u6c47\u63d0\u793a\u548c\u8bed\u97f3\u547d\u4ee4\uff0c\u5229\u7528\u89c6\u89c9-\u8bed\u8a00\u57fa\u7840\u6a21\u578b\u5b9e\u73b0\u5bf9\u672a\u77e5\u7269\u4f53\u7684\u96f6\u6837\u672c\u68c0\u6d4b\uff0c\u5e76\u901a\u8fc7\u591a\u6a21\u6001\u51b3\u7b56\u5668\u63a8\u65ad\u7528\u6237\u5728\u591a\u7269\u4f53\u573a\u666f\u4e2d\u7684\u6293\u53d6\u6216\u91ca\u653e\u610f\u56fe\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u5728\u6293\u53d6\u80fd\u529b\u548c\u8fd0\u52a8\u5bf9\u9f50\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002 2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u5b66\u65b9\u6cd5 (Key Innovation or Methodological Approach) \u8be5\u8bba\u6587\u7684\u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u5176 \u5f00\u653e\u8bcd\u6c47\uff08Open-Vocabulary\uff09\u80fd\u529b \u548c \u591a\u6a21\u6001\u610f\u56fe\u68c0\u6d4b \u3002 1. \u5f00\u653e\u8bcd\u6c47\u673a\u5236\uff1a OVGrasp \u96c6\u6210\u4e86\u4e00\u4e2a\u89c6\u89c9-\u8bed\u8a00\u57fa\u7840\u6a21\u578b\uff0c\u4f7f\u5176\u80fd\u591f\u5bf9\u5148\u524d\u672a\u89c1\u7684\u7269\u4f53\u8fdb\u884c\u96f6\u6837\u672c\u68c0\u6d4b\uff0c\u800c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u3002\u8fd9\u6781\u5927\u5730\u589e\u5f3a\u4e86\u7cfb\u7edf\u5728\u975e\u7ed3\u6784\u5316\u3001\u591a\u6837\u5316\u73af\u5883\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u7684\u4e00\u4e2a\u91cd\u8981\u8fdb\u5c55\uff0c\u5c24\u5176\u662f\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u3002 2. \u591a\u6a21\u6001\u610f\u56fe\u51b3\u7b56\u5668\uff1a \u7cfb\u7edf\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u591a\u6a21\u6001\u51b3\u7b56\u5668\uff0c\u80fd\u591f\u878d\u5408\u6765\u81ea RGB-D \u89c6\u89c9\u7684\u7a7a\u95f4\u7ebf\u7d22\u548c\u6765\u81ea\u5f00\u653e\u8bcd\u6c47\u63d0\u793a/\u8bed\u97f3\u547d\u4ee4\u7684\u8bed\u8a00\u7ebf\u7d22\uff0c\u4ece\u800c\u5728\u591a\u7269\u4f53\u573a\u666f\u4e2d\u51c6\u786e\u63a8\u65ad\u7528\u6237\u7684\u5177\u4f53\u610f\u56fe\uff08\u5982\u6293\u53d6\u6216\u91ca\u653e\uff09\u3002\u8fd9\u79cd\u5bf9\u590d\u6742\u7528\u6237\u610f\u56fe\u7684\u7406\u89e3\uff0c\u8d85\u8d8a\u4e86\u7b80\u5355\u7684\u7269\u4f53\u8bc6\u522b\uff0c\u662f\u4eba\u673a\u4ea4\u4e92\u548c\u8f85\u52a9\u673a\u5668\u4eba\u9886\u57df\u7684\u5173\u952e\u7a81\u7834\u3002 3. \u96c6\u6210\u6846\u67b6\uff1a \u5c06\u8fd9\u4e9b\u5148\u8fdb\u7684 CV/NLP \u6280\u672f\u4e0e\u5b9a\u5236\u7684\u3001\u4f69\u6234\u5f0f\u3001\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u7684\u8f6f\u4f53\u5916\u9aa8\u9abc\u76f8\u7ed3\u5408\uff0c\u5f62\u6210\u4e00\u4e2a\u5b9e\u7528\u7684\u3001\u7aef\u5230\u7aef\u7684\u5c42\u7ea7\u63a7\u5236\u7cfb\u7edf\uff0c\u5b9e\u73b0\u4e86\u4ece\u611f\u77e5\u5230\u51b3\u7b56\u518d\u5230\u6267\u884c\u7684\u95ed\u73af\u3002 3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd (Potential Impact on the Field) \u8f85\u52a9\u673a\u5668\u4eba\u4e0e\u4eba\u673a\u4ea4\u4e92 (Assistive Robotics & HRI)\uff1a OVGrasp \u76f4\u63a5\u89e3\u51b3\u4e86\u8fd0\u52a8\u969c\u788d\u8005\u5728\u65e5\u5e38\u751f\u6d3b\u4e2d\u6062\u590d\u81ea\u4e3b\u6027\u7684\u5173\u952e\u6311\u6218\uff0c\u901a\u8fc7\u63d0\u4f9b\u66f4\u81ea\u7136\u3001\u76f4\u89c2\u4e14\u9002\u5e94\u6027\u5f3a\u7684\u6293\u53d6\u8f85\u52a9\uff0c\u5c06\u6781\u5927\u5730\u6539\u5584\u4ed6\u4eec\u7684\u751f\u6d3b\u8d28\u91cf\u3002\u5b83\u4e3a\u672a\u6765\u8f85\u52a9\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u7684\u8303\u5f0f\u3002 \u8ba1\u7b97\u673a\u89c6\u89c9\u4e0e\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b (Computer Vision & Vision-Language Models)\uff1a \u8be5\u7814\u7a76\u5c55\u793a\u4e86\u89c6\u89c9-\u8bed\u8a00\u57fa\u7840\u6a21\u578b\u5728\u5b9e\u9645\u3001\u5177\u8eab\uff08embodied\uff09\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u7684\u5f3a\u5927\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u5f00\u653e\u4e16\u754c\u3001\u96f6\u6837\u672c\u7269\u4f53\u8bc6\u522b\u548c\u7406\u89e3\u65b9\u9762\u3002\u5b83\u63a8\u52a8\u4e86 CV \u9886\u57df\u4ece\u9759\u6001\u56fe\u50cf\u8bc6\u522b\u5411\u52a8\u6001\u3001\u4ea4\u4e92\u5f0f\u3001\u591a\u6a21\u6001\u611f\u77e5\u7684\u6f14\u8fdb\u3002 \u591a\u6a21\u6001\u4eba\u5de5\u667a\u80fd (Multimodal AI)\uff1a \u8bba\u6587\u5728\u878d\u5408\u7a7a\u95f4\u548c\u8bed\u8a00\u4fe1\u606f\u4ee5\u63a8\u65ad\u590d\u6742\u7528\u6237\u610f\u56fe\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4e3a\u591a\u6a21\u6001\u5b66\u4e60\u548c\u51b3\u7b56\u5236\u5b9a\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u548c\u5b9e\u8bc1\u3002 \u8f6f\u4f53\u673a\u5668\u4eba\u4e0e\u53ef\u7a7f\u6234\u8bbe\u5907 (Soft Robotics & Wearable Devices)\uff1a \u7ed3\u5408\u8f6f\u4f53\u5916\u9aa8\u9abc\u548c\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u89c6\u89c9\uff0c\u4e3a\u53ef\u7a7f\u6234\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u8bbe\u8ba1\u548c\u63a7\u5236\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u7ecf\u9a8c\u548c\u6280\u672f\u53c2\u8003\u3002 4. \u53ef\u80fd\u53d7\u76ca\u4e8e\u6b64\u7814\u7a76\u7684\u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 (Related Areas or Applications) \u901a\u7528\u578b\u673a\u5668\u4eba\u64cd\u4f5c (General-purpose Robotic Manipulation)\uff1a \u63d0\u5347\u673a\u5668\u4eba\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u5904\u7406\u591a\u6837\u5316\u7269\u4f53\u7684\u80fd\u529b\uff0c\u4f8b\u5982\u5728\u7269\u6d41\u3001\u4ed3\u50a8\u6216\u670d\u52a1\u673a\u5668\u4eba\u9886\u57df\u3002 \u4eba\u673a\u534f\u4f5c (Human-Robot Collaboration)\uff1a \u6539\u8fdb\u673a\u5668\u4eba\u5bf9\u4eba\u7c7b\u6307\u4ee4\u548c\u610f\u56fe\u7684\u7406\u89e3\uff0c\u5b9e\u73b0\u66f4\u6d41\u7545\u3001\u66f4\u5b89\u5168\u7684\u534f\u4f5c\uff0c\u5c24\u5176\u662f\u5728\u5de5\u4e1a\u6216\u533b\u7597\u573a\u666f\u4e2d\u3002 \u8fdc\u7a0b\u64cd\u4f5c\u4e0e\u63a2\u7d22 (Teleoperation & Exploration)\uff1a \u4e3a\u8fdc\u7a0b\u63a7\u5236\u7cfb\u7edf\u63d0\u4f9b\u66f4\u667a\u80fd\u7684\u7269\u4f53\u8bc6\u522b\u548c\u610f\u56fe\u63a8\u65ad\u80fd\u529b\uff0c\u51cf\u5c11\u64cd\u4f5c\u5458\u7684\u8ba4\u77e5\u8d1f\u62c5\u3002 \u667a\u80fd\u5bb6\u5c45\u4e0e\u667a\u6167\u533b\u7597 (Smart Home & Smart Healthcare)\uff1a \u5c06\u5f00\u653e\u8bcd\u6c47\u548c\u591a\u6a21\u6001\u4ea4\u4e92\u80fd\u529b\u96c6\u6210\u5230\u5176\u4ed6\u667a\u80fd\u8bbe\u5907\u4e2d\uff0c\u63d0\u4f9b\u66f4\u5e7f\u6cdb\u7684\u667a\u80fd\u8f85\u52a9\u670d\u52a1\u3002 \u589e\u5f3a\u73b0\u5b9e/\u865a\u62df\u73b0\u5b9e (Augmented Reality/Virtual Reality)\uff1a \u5728 AR/VR \u73af\u5883\u4e2d\u5b9e\u73b0\u66f4\u81ea\u7136\u7684\u7269\u4f53\u4ea4\u4e92\u548c\u7528\u6237\u610f\u56fe\u7406\u89e3\u3002 5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u51fa\u7684\u5c40\u9650\u6027 (Limitations that can be inferred from the abstract) \u8bc4\u4f30\u8303\u56f4\u7684\u5c40\u9650\u6027 (Limited Evaluation Scope): \u5c3d\u7ba1\u572815\u4e2a\u7269\u4f53\u548c3\u79cd\u6293\u53d6\u7c7b\u578b\u4e0a\u8fdb\u884c\u4e86\u7cfb\u7edf\u8bc4\u4f30\uff0c\u5e76\u670910\u540d\u53c2\u4e0e\u8005\uff0c\u4f46\u4e0e\u201c\u591a\u6837\u5316\u4e14\u4e0d\u53ef\u9884\u6d4b\u201d\u7684\u771f\u5b9e\u4e16\u754c\u975e\u7ed3\u6784\u5316\u73af\u5883\u76f8\u6bd4\uff0c\u8fd9\u4ecd\u7136\u662f\u4e00\u4e2a\u76f8\u5bf9\u6709\u9650\u7684\u6d4b\u8bd5\u96c6\u3002\u5f00\u653e\u8bcd\u6c47\u80fd\u529b\u867d\u5f3a\uff0c\u4f46\u5176\u5728\u6781\u7aef\u591a\u6837\u6027\u3001\u7f55\u89c1\u7269\u4f53\u6216\u9ad8\u5ea6\u76f8\u4f3c\u7269\u4f53\u533a\u5206\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u4ecd\u9700\u66f4\u5e7f\u6cdb\u3001\u66f4\u4e25\u82db\u7684\u9a8c\u8bc1\u3002 \u610f\u56fe\u8bc6\u522b\u7684\u590d\u6742\u6027 (Complexity of Intent Recognition): \u76ee\u524d\u7684\u610f\u56fe\u8bc6\u522b\u4e3b\u8981\u96c6\u4e2d\u5728\u201c\u6293\u53d6\u201d\u6216\u201c\u91ca\u653e\u201d\u4e24\u79cd\u57fa\u672c\u52a8\u4f5c\u3002\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u7528\u6237\u7684\u610f\u56fe\u53ef\u80fd\u66f4\u4e3a\u590d\u6742\u548c\u7ec6\u81f4\uff08\u4f8b\u5982\uff0c\u201c\u8f7b\u8f7b\u6293\u53d6\u201d\u3001\u201c\u79fb\u52a8\u5230\u67d0\u4e2a\u4f4d\u7f6e\u201d\u3001\u201c\u6293\u53d6\u7ea2\u8272\u7684\u90a3\u4e2a\u201d\uff09\uff0c\u8fd9\u53ef\u80fd\u9700\u8981\u66f4\u9ad8\u7ea7\u3001\u66f4\u5177\u4e0a\u4e0b\u6587\u611f\u77e5\u80fd\u529b\u7684\u610f\u56fe\u7406\u89e3\u6a21\u578b\u3002 \u73af\u5883\u9c81\u68d2\u6027 (Environmental Robustness): \u6458\u8981\u672a\u8be6\u7ec6\u8bf4\u660e\u7cfb\u7edf\u5728\u6781\u7aef\u5149\u7167\u53d8\u5316\u3001\u4e25\u91cd\u906e\u6321\u3001\u9ad8\u5ea6\u6742\u4e71\u3001\u5305\u542b\u900f\u660e/\u53cd\u5149\u7269\u4f53\u7b49\u590d\u6742\u771f\u5b9e\u4e16\u754c\u6761\u4ef6\u4e0b\u7684\u8868\u73b0\u3002\u8fd9\u4e9b\u662f RGB-D \u89c6\u89c9\u548c\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u5e38\u89c1\u7684\u6311\u6218\u3002 \u8ba1\u7b97\u8d44\u6e90\u4e0e\u5b9e\u65f6\u6027 (Computational Resources and Real-time Performance): \u89c6\u89c9-\u8bed\u8a00\u57fa\u7840\u6a21\u578b\u901a\u5e38\u8ba1\u7b97\u91cf\u5927\u3002\u5bf9\u4e8e\u4e00\u4e2a\u53ef\u7a7f\u6234\u3001\u5b9e\u65f6\u54cd\u5e94\u7684\u7cfb\u7edf\uff0c\u5176\u8ba1\u7b97\u5f00\u9500\u3001\u529f\u8017\u548c\u5ef6\u8fdf\u662f\u5173\u952e\u8003\u91cf\uff0c\u6458\u8981\u4e2d\u672a\u63d0\u53ca\u8fd9\u4e9b\u6027\u80fd\u6307\u6807\u3002 \u7528\u6237\u9002\u5e94\u6027\u4e0e\u4e2a\u6027\u5316 (User Adaptability & Personalization): \u6458\u8981\u63d0\u5230\u201c\u6539\u8fdb\u4e86\u4e0e\u81ea\u7136\u624b\u90e8\u8fd0\u52a8\u7684\u8fd0\u52a8\u5bf9\u9f50\u201d\uff0c\u4f46\u672a\u8be6\u7ec6\u8bf4\u660e\u7cfb\u7edf\u5982\u4f55\u9002\u5e94\u4e0d\u540c\u7528\u6237\u7684\u751f\u7406\u5dee\u5f02\u3001\u504f\u597d\u6216\u5b66\u4e60\u66f2\u7ebf\u3002\u4e2a\u6027\u5316\u8c03\u6574\u5bf9\u4e8e\u8f85\u52a9\u8bbe\u5907\u81f3\u5173\u91cd\u8981\u3002 Key Findings: We present OVGrasp, a hierarchical control framework for soft exoskeleton-based grasp assistance that integrates RGB-D vision, open-vocabulary prompts, and voice commands to enable robust multimodal interaction. Experimental results with ten participants demonstrate that OVGrasp achieves a grasping ability score (GAS) of 87.00%, outperforming state-of-the-art baselines and achieving improved kinematic alignment with natural hand motion. Links: PDF arXiv Enhancing Robustness in Post-Processing Watermarking: An Ensemble Attack Network Using CNNs and Transformers Authors: Tzuhsuan Huang, Cheng Yu Yeo, Tsai-Ling Huang, Hong-Han Shuai, Wen-Huang Cheng, Jun-Cheng Chen Published: 2025-09-03 Categories: cs.CV Abstract: Recent studies on deep watermarking have predominantly focused on in-processing watermarking, which integrates the watermarking process into image generation. However, post-processing watermarking, which embeds watermarks after image generation, offers more flexibility. It can be applied to outputs from any generative model (e.g. GANs, diffusion models) without needing access to the model's internal structure. It also allows users to embed unique watermarks into individual images. Therefore, this study focuses on post-processing watermarking and enhances its robustness by incorporating an ensemble attack network during training. We construct various versions of attack networks using CNN and Transformer in both spatial and frequency domains to investigate how each combination influences the robustness of the watermarking model. Our results demonstrate that combining a CNN-based attack network in the spatial domain with a Transformer-based attack network in the frequency domain yields the highest robustness in watermarking models. Extensive evaluation on the WAVES benchmark, using average bit accuracy as the metric, demonstrates that our ensemble attack network significantly enhances the robustness of baseline watermarking methods under various stress tests. In particular, for the Regeneration Attack defined in WAVES, our method improves StegaStamp by 18.743%. The code is released at:https://github.com/aiiu-lab/DeepRobustWatermark. Analysis: \u8fd9\u7bc7\u8bba\u6587\u6df1\u5165\u63a2\u8ba8\u4e86\u540e\u5904\u7406\u6570\u5b57\u6c34\u5370\u7684\u9c81\u68d2\u6027\u589e\u5f3a\u95ee\u9898\uff0c\u4ee5\u4e0b\u662f\u8be6\u7ec6\u5206\u6790\uff1a 1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u660e\u6458\u8981 (Concise Summary) \u672c\u6587\u4e13\u6ce8\u4e8e\u63d0\u5347\u540e\u5904\u7406\u6570\u5b57\u6c34\u5370\u7684\u9c81\u68d2\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u8bad\u7ec3\u9636\u6bb5\u5229\u7528\u96c6\u6210\u653b\u51fb\u7f51\u7edc\u7684\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u57fa\u4e8eCNN\u7684\u7a7a\u95f4\u57df\u653b\u51fb\u548c\u57fa\u4e8eTransformer\u7684\u9891\u7387\u57df\u653b\u51fb\uff0c\u5b9e\u9a8c\u8bc1\u660e\u8fd9\u79cd\u7ec4\u5408\u80fd\u663e\u8457\u589e\u5f3a\u6c34\u5370\u6a21\u578b\u7684\u6297\u653b\u51fb\u80fd\u529b\uff0c\u5e76\u5728WAVES\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u73b0\u51fa\u4f18\u5f02\u6027\u80fd\uff0c\u5c24\u5176\u5728\u518d\u751f\u653b\u51fb\u4e0b\u5bf9\u73b0\u6709\u65b9\u6cd5\u6709\u663e\u8457\u63d0\u5347\u3002 2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u8bba (Key Innovation or Methodological Approach) \u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u201c\u96c6\u6210\u653b\u51fb\u7f51\u7edc\u201d\uff08Ensemble Attack Network\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u8bad\u7ec3\u9636\u6bb5\u63d0\u5347\u540e\u5904\u7406\u6c34\u5370\u7684\u9c81\u68d2\u6027\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u4e0d\u540c\u67b6\u6784\uff08CNN\u548cTransformer\uff09\u548c\u4e0d\u540c\u4f5c\u7528\u57df\uff08\u7a7a\u95f4\u57df\u548c\u9891\u7387\u57df\uff09\u7684\u653b\u51fb\u6a21\u5757\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u591a\u6837\u5316\u7684\u653b\u51fb\u96c6\u6210\u4f53\u3002\u8fd9\u79cd\u96c6\u6210\u653b\u51fb\u7b56\u7565\u8feb\u4f7f\u6c34\u5370\u6a21\u578b\u5b66\u4e60\u5bf9\u591a\u79cd\u6f5c\u5728\u653b\u51fb\u66f4\u5177\u62b5\u6297\u529b\u7684\u7279\u5f81\uff0c\u4ece\u800c\u663e\u8457\u589e\u5f3a\u5176\u9c81\u68d2\u6027\u3002\u8bba\u6587\u8fd8\u901a\u8fc7\u5b9e\u9a8c\u53d1\u73b0\uff0c\u5c06\u57fa\u4e8eCNN\u7684\u7a7a\u95f4\u57df\u653b\u51fb\u4e0e\u57fa\u4e8eTransformer\u7684\u9891\u7387\u57df\u653b\u51fb\u76f8\u7ed3\u5408\uff0c\u80fd\u8fbe\u5230\u6700\u4f73\u7684\u9c81\u68d2\u6027\u6548\u679c\u3002 3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd (Potential Impact on the Field) \u672c\u7814\u7a76\u663e\u8457\u63d0\u5347\u4e86\u540e\u5904\u7406\u6570\u5b57\u6c34\u5370\u7684\u5b9e\u7528\u6027\u548c\u53ef\u9760\u6027\uff0c\u4f7f\u5176\u80fd\u66f4\u6709\u6548\u5730\u5e94\u7528\u4e8e\u4efb\u4f55\u751f\u6210\u6a21\u578b\uff08\u5982GANs\u3001\u6269\u6563\u6a21\u578b\uff09\u7684\u8f93\u51fa\uff0c\u800c\u65e0\u9700\u8bbf\u95ee\u5176\u5185\u90e8\u7ed3\u6784\u3002\u8fd9\u5bf9\u4e8eAI\u751f\u6210\u5185\u5bb9\u7684\u7248\u6743\u4fdd\u62a4\u3001\u6eaf\u6e90\u3001\u771f\u5b9e\u6027\u9a8c\u8bc1\u4ee5\u53ca\u6253\u51fb\u6df1\u5ea6\u4f2a\u9020\u7b49\u9886\u57df\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002\u5b83\u4e5f\u4e3a\u672a\u6765\u6c34\u5370\u6280\u672f\uff0c\u7279\u522b\u662f\u5bf9\u6297\u6027\u9c81\u68d2\u6027\u7814\u7a76\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u7684\u3001\u6709\u6548\u7684\u8bad\u7ec3\u8303\u5f0f\uff0c\u5373\u901a\u8fc7\u96c6\u6210\u591a\u6837\u5316\u653b\u51fb\u8fdb\u884c\u5bf9\u6297\u8bad\u7ec3\u3002 4. \u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 (Related Areas or Applications) AI\u751f\u6210\u5185\u5bb9\u7684\u7248\u6743\u4fdd\u62a4\u4e0e\u6eaf\u6e90\uff1a \u786e\u4fddAI\u751f\u6210\u56fe\u50cf\u3001\u89c6\u9891\u7b49\u5185\u5bb9\u7684\u539f\u521b\u6027\u5f52\u5c5e\uff0c\u9632\u6b62\u672a\u7ecf\u6388\u6743\u7684\u4f7f\u7528\u3002 \u6570\u5b57\u5a92\u4f53\u771f\u5b9e\u6027\u9a8c\u8bc1\u4e0e\u9632\u7be1\u6539\uff1a \u9a8c\u8bc1\u56fe\u50cf\u6216\u89c6\u9891\u662f\u5426\u88ab\u7be1\u6539\uff0c\u5c24\u5176\u662f\u5728\u65b0\u95fb\u3001\u6cd5\u5f8b\u3001\u533b\u7597\u7b49\u5bf9\u5185\u5bb9\u771f\u5b9e\u6027\u8981\u6c42\u6781\u9ad8\u7684\u9886\u57df\u3002 \u6df1\u5ea6\u4f2a\u9020\uff08Deepfake\uff09\u68c0\u6d4b\u4e0e\u6eaf\u6e90\uff1a \u901a\u8fc7\u5d4c\u5165\u6c34\u5370\u6765\u8bc6\u522b\u548c\u8ffd\u8e2a\u5408\u6210\u5185\u5bb9\uff0c\u5bf9\u6297\u6076\u610f\u6df1\u5ea6\u4f2a\u9020\u3002 \u6570\u5b57\u53d6\u8bc1\uff1a \u5728\u7f51\u7edc\u72af\u7f6a\u8c03\u67e5\u4e2d\u63d0\u4f9b\u5185\u5bb9\u6765\u6e90\u548c\u4fee\u6539\u5386\u53f2\u7684\u7ebf\u7d22\u3002 \u77e5\u8bc6\u4ea7\u6743\u7ba1\u7406\uff1a \u4fdd\u62a4\u6570\u5b57\u827a\u672f\u54c1\u548c\u521b\u610f\u5185\u5bb9\u7684\u77e5\u8bc6\u4ea7\u6743\u3002 5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u7684\u5c40\u9650\u6027 (Limitations Inferable from the Abstract) \u9c81\u68d2\u6027\u7684\u7edd\u5bf9\u4e0a\u9650\uff1a \u5c3d\u7ba1\u663e\u8457\u63d0\u5347\u4e86\u9c81\u68d2\u6027\uff0c\u4f46\u4efb\u4f55\u6c34\u5370\u7cfb\u7edf\u90fd\u65e0\u6cd5\u4fdd\u8bc1\u5bf9\u6240\u6709\u6f5c\u5728\u653b\u51fb\u7684\u7edd\u5bf9\u62b5\u6297\u529b\uff0c\u672a\u6765\u53ef\u80fd\u51fa\u73b0\u66f4\u590d\u6742\u7684\u653b\u51fb\u3002 \u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\uff1a \u8bad\u7ec3\u96c6\u6210\u653b\u51fb\u7f51\u7edc\uff0c\u7279\u522b\u662f\u7ed3\u5408\u4e86CNN\u548cTransformer\u7684\u590d\u6742\u6a21\u578b\uff0c\u53ef\u80fd\u4f1a\u5e26\u6765\u8f83\u9ad8\u7684\u8ba1\u7b97\u8d44\u6e90\u548c\u65f6\u95f4\u6210\u672c\u3002 \u5bf9\u672a\u77e5\u653b\u51fb\u7684\u6cdb\u5316\u80fd\u529b\uff1a \u5c3d\u7ba1\u96c6\u6210\u4e86\u591a\u6837\u5316\u653b\u51fb\uff0c\u4f46\u5176\u5bf9\u8bad\u7ec3\u96c6\u4e2d\u672a\u5305\u542b\u7684\u3001\u5168\u65b0\u7c7b\u578b\u7684\u653b\u51fb\u7684\u6cdb\u5316\u80fd\u529b\u4ecd\u9700\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u3002 \u8bc4\u4f30\u8303\u56f4\uff1a \u8bc4\u4f30\u4e3b\u8981\u57fa\u4e8eWAVES\u57fa\u51c6\u6d4b\u8bd5\u548c\u5e73\u5747\u6bd4\u7279\u51c6\u786e\u7387\uff0c\u5176\u5728\u5176\u4ed6\u7279\u5b9a\u573a\u666f\u6216\u4f7f\u7528\u5176\u4ed6\u8bc4\u4f30\u6307\u6807\u65f6\u7684\u8868\u73b0\u53ef\u80fd\u6709\u6240\u4e0d\u540c\u3002 \u4ec5\u9650\u4e8e\u540e\u5904\u7406\u6c34\u5370\uff1a \u672c\u7814\u7a76\u4e13\u6ce8\u4e8e\u540e\u5904\u7406\u6c34\u5370\uff0c\u5176\u7ed3\u8bba\u548c\u65b9\u6cd5\u4e0d\u76f4\u63a5\u9002\u7528\u4e8e\u5728\u56fe\u50cf\u751f\u6210\u8fc7\u7a0b\u4e2d\u5d4c\u5165\u6c34\u5370\u7684\u201c\u524d\u5904\u7406\u6c34\u5370\u201d\u573a\u666f\u3002 Key Findings: In particular, for the Regeneration Attack defined in WAVES, our method improves StegaStamp by 18.743%. Links: PDF arXiv Plot'n Polish: Zero-shot Story Visualization and Disentangled Editing with Text-to-Image Diffusion Models Authors: Kiymet Akdemir, Jing Shi, Kushal Kafle, Brian Price, Pinar Yanardag Published: 2025-09-04 Categories: cs.CV Abstract: Text-to-image diffusion models have demonstrated significant capabilities to generate diverse and detailed visuals in various domains, and story visualization is emerging as a particularly promising application. However, as their use in real-world creative domains increases, the need for providing enhanced control, refinement, and the ability to modify images post-generation in a consistent manner becomes an important challenge. Existing methods often lack the flexibility to apply fine or coarse edits while maintaining visual and narrative consistency across multiple frames, preventing creators from seamlessly crafting and refining their visual stories. To address these challenges, we introduce Plot'n Polish, a zero-shot framework that enables consistent story generation and provides fine-grained control over story visualizations at various levels of detail. Analysis: \u4f5c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u4e13\u5bb6\uff0c\u6211\u5bf9\u8fd9\u7bc7\u8bba\u6587\u6458\u8981\u8fdb\u884c\u5982\u4e0b\u5206\u6790\uff1a 1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u660e\u603b\u7ed3 (Concise Summary) \u672c\u6587\u63d0\u51fa\u4e86 Plot'n Polish\uff0c\u4e00\u4e2a\u96f6\u6837\u672c\uff08zero-shot\uff09\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u5f53\u524d\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u5728\u6545\u4e8b\u53ef\u89c6\u5316\u4e2d\u7f3a\u4e4f\u4e00\u81f4\u6027\u63a7\u5236\u548c\u540e\u671f\u7f16\u8f91\u7075\u6d3b\u6027\u7684\u95ee\u9898\u3002\u5b83\u5b9e\u73b0\u4e86\u8de8\u591a\u5e27\u7684\u89c6\u89c9\u548c\u53d9\u4e8b\u4e00\u81f4\u6027\u6545\u4e8b\u751f\u6210\uff0c\u5e76\u63d0\u4f9b\u4e86\u5bf9\u6545\u4e8b\u53ef\u89c6\u5316\u5728\u4e0d\u540c\u7ec6\u8282\u5c42\u7ea7\u7684\u7ec6\u7c92\u5ea6\u63a7\u5236\u548c\u89e3\u8026\u7f16\u8f91\u80fd\u529b\uff0c\u4ece\u800c\u4f7f\u521b\u4f5c\u8005\u80fd\u591f\u65e0\u7f1d\u5730\u7cbe\u4fee\u5176\u89c6\u89c9\u6545\u4e8b\u3002 2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u5b66\u65b9\u6cd5 (Key Innovation or Methodological Approach) \u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u5176 \u96f6\u6837\u672c\uff08zero-shot\uff09 \u7279\u6027\uff0c\u8fd9\u610f\u5473\u7740\u8be5\u6846\u67b6\u65e0\u9700\u9488\u5bf9\u7279\u5b9a\u6545\u4e8b\u6216\u7f16\u8f91\u4efb\u52a1\u8fdb\u884c\u989d\u5916\u8bad\u7ec3\u3002\u66f4\u91cd\u8981\u7684\u662f\uff0c\u5b83\u5f15\u5165\u4e86\u4e00\u79cd \u89e3\u8026\u7f16\u8f91\uff08disentangled editing\uff09 \u7684\u65b9\u6cd5\uff08\u4ece\u6807\u9898\u63a8\u65ad\uff09\uff0c\u4f7f\u5f97\u7528\u6237\u80fd\u591f\u5bf9\u6545\u4e8b\u53ef\u89c6\u5316\u8fdb\u884c \u7ec6\u7c92\u5ea6\uff08fine-grained\uff09 \u63a7\u5236\uff0c\u5e76\u5728 \u4e0d\u540c\u7ec6\u8282\u5c42\u7ea7 \uff08\u4ece\u6574\u4f53\u53d9\u4e8b\u5230\u5c40\u90e8\u5143\u7d20\uff09\u4e0a\u8fdb\u884c\u4fee\u6539\uff0c\u540c\u65f6\u786e\u4fdd \u8de8\u591a\u5e27\u7684\u89c6\u89c9\u548c\u53d9\u4e8b\u4e00\u81f4\u6027 \u3002\u8fd9\u79cd\u5728\u4fdd\u6301\u4e00\u81f4\u6027\u524d\u63d0\u4e0b\u7684\u591a\u5c42\u7ea7\u3001\u89e3\u8026\u63a7\u5236\u662f\u73b0\u6709\u65b9\u6cd5\u6240\u6b20\u7f3a\u7684\uff0c\u5b83\u89e3\u51b3\u4e86\u5728\u6545\u4e8b\u751f\u6210\u4e2d\uff0c\u89d2\u8272\u3001\u573a\u666f\u3001\u98ce\u683c\u7b49\u5143\u7d20\u5728\u4e0d\u540c\u5e27\u4e4b\u95f4\u4fdd\u6301\u8fde\u8d2f\u6027\u7684\u6838\u5fc3\u6311\u6218\u3002 3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd (Potential Impact on the Field) \u8be5\u7814\u7a76\u5c06\u663e\u8457\u63d0\u5347\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u5728 \u6545\u4e8b\u53ef\u89c6\u5316 \u9886\u57df\u7684\u5b9e\u7528\u6027\u548c\u53ef\u63a7\u6027\u3002\u5b83\u4e3a\u521b\u4f5c\u8005\u63d0\u4f9b\u4e86\u524d\u6240\u672a\u6709\u7684\u7075\u6d3b\u6027\uff0c\u4f7f\u5176\u80fd\u591f \u65e0\u7f1d\u5730\u521b\u4f5c\u3001\u8fed\u4ee3\u548c\u7cbe\u4fee\u89c6\u89c9\u6545\u4e8b \uff0c\u4ece\u800c\u964d\u4f4e\u4e86\u9ad8\u8d28\u91cf\u89c6\u89c9\u53d9\u4e8b\u5185\u5bb9\u7684\u521b\u4f5c\u95e8\u69db\u3002\u8fd9\u4e0d\u4ec5\u80fd\u63a8\u52a8 \u521b\u610f\u4ea7\u4e1a \uff08\u5982\u52a8\u753b\u3001\u6f2b\u753b\u3001\u6e38\u620f\u6982\u5ff5\u827a\u672f\u3001\u5e7f\u544a\uff09\u7684\u53d1\u5c55\uff0c\u4e5f\u4e3a\u672a\u6765\u66f4\u590d\u6742\u7684 \u4eba\u673a\u534f\u4f5c\u5185\u5bb9\u751f\u6210 \u6a21\u5f0f\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u4f7fAI\u5de5\u5177\u4ece\u5355\u7eaf\u7684\u751f\u6210\u5668\u8f6c\u53d8\u4e3a\u66f4\u5f3a\u5927\u7684\u521b\u610f\u52a9\u624b\uff0c\u80fd\u591f\u7406\u89e3\u5e76\u54cd\u5e94\u7528\u6237\u5728\u53d9\u4e8b\u548c\u89c6\u89c9\u7f16\u8f91\u4e0a\u7684\u590d\u6742\u610f\u56fe\u3002 4. \u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 (Related Areas or Applications) \u521b\u610f\u5185\u5bb9\u751f\u6210: \u52a8\u753b\u3001\u6f2b\u753b\u3001\u7535\u5f71\u9884\u53ef\u89c6\u5316\uff08pre-visualization\uff09\u3001\u6e38\u620f\u6982\u5ff5\u827a\u672f\u3001\u5e7f\u544a\u521b\u610f\u3001\u6570\u5b57\u63d2\u753b\u3002 \u4e2a\u6027\u5316\u5185\u5bb9: \u6839\u636e\u7528\u6237\u8f93\u5165\u751f\u6210\u5b9a\u5236\u5316\u7684\u6545\u4e8b\u3001\u6559\u80b2\u6750\u6599\u6216\u4ea4\u4e92\u5f0f\u4f53\u9a8c\u3002 \u865a\u62df\u73b0\u5b9e/\u589e\u5f3a\u73b0\u5b9e (VR/AR): \u5feb\u901f\u751f\u6210\u548c\u8fed\u4ee3\u865a\u62df\u573a\u666f\u6216\u89d2\u8272\u8d44\u4ea7\uff0c\u4fdd\u6301\u5176\u5728\u4e0d\u540c\u4ea4\u4e92\u72b6\u6001\u4e0b\u7684\u4e00\u81f4\u6027\u3002 \u6570\u5b57\u4eba/\u865a\u62df\u5076\u50cf: \u4fdd\u6301\u6570\u5b57\u89d2\u8272\u5728\u4e0d\u540c\u59ff\u6001\u3001\u8868\u60c5\u548c\u573a\u666f\u4e2d\u7684\u4e00\u81f4\u6027\uff0c\u5e76\u8fdb\u884c\u7cbe\u7ec6\u5316\u7f16\u8f91\u3002 \u591a\u6a21\u6001\u5185\u5bb9\u7406\u89e3\u4e0e\u751f\u6210: \u4e3a\u6587\u672c\u6545\u4e8b\u81ea\u52a8\u914d\u56fe\u6216\u751f\u6210\u89c6\u9891\u811a\u672c\uff0c\u5e76\u5141\u8bb8\u7528\u6237\u5bf9\u751f\u6210\u7ed3\u679c\u8fdb\u884c\u7cbe\u7ec6\u8c03\u6574\u3002 5. \u53ef\u4ece\u6458\u8981\u63a8\u65ad\u7684\u5c40\u9650\u6027 (Limitations that can be inferred from the abstract) \u96f6\u6837\u672c\u65b9\u6cd5\u7684\u5c40\u9650\u6027: \u5c3d\u7ba1\u96f6\u6837\u672c\u662f\u4f18\u52bf\uff0c\u4f46\u5bf9\u4e8e\u9ad8\u5ea6\u7279\u5b9a\u3001\u98ce\u683c\u5316\u6216\u9700\u8981\u6781\u9ad8\u7ec6\u8282\u4fdd\u771f\u5ea6\u7684\u573a\u666f\uff0c\u5176\u6027\u80fd\u53ef\u80fd\u4e0d\u5982\u7ecf\u8fc7\u7279\u5b9a\u6570\u636e\u5fae\u8c03\u7684\u6a21\u578b\u3002\u5728\u67d0\u4e9b\u6781\u7aef\u60c5\u51b5\u4e0b\uff0c\u96f6\u6837\u672c\u65b9\u6cd5\u53ef\u80fd\u96be\u4ee5\u6355\u6349\u5230\u975e\u5e38\u7ec6\u5fae\u6216\u72ec\u7279\u7684\u89c6\u89c9\u7279\u5f81\u3002 \u4e00\u81f4\u6027\u4fdd\u6301\u7684\u9c81\u68d2\u6027: \u6458\u8981\u4e2d\u5f3a\u8c03\u4e86\u201c\u4e00\u81f4\u6027\u201d\uff0c\u4f46\u5bf9\u4e8e\u6781\u5176\u590d\u6742\u3001\u53d9\u4e8b\u8de8\u5ea6\u957f\u6216\u89d2\u8272/\u573a\u666f\u53d1\u751f\u5267\u70c8\u53d8\u5316\u7684\u6545\u4e8b\uff0c\u7ef4\u6301\u5b8c\u7f8e\u7684\u4e00\u81f4\u6027\u4ecd\u662f\u4e00\u4e2a\u5de8\u5927\u6311\u6218\u3002\u6a21\u578b\u5982\u4f55\u5904\u7406\u89d2\u8272\u670d\u88c5\u3001\u53d1\u578b\u3001\u9762\u90e8\u7279\u5f81\u5728\u4e0d\u540c\u5e27\u4e2d\u7684\u7ec6\u5fae\u53d8\u5316\uff0c\u4ee5\u53ca\u80cc\u666f\u73af\u5883\u7684\u8fde\u8d2f\u6027\uff0c\u662f\u9700\u8981\u9a8c\u8bc1\u7684\u3002 \u201c\u7ec6\u7c92\u5ea6\u63a7\u5236\u201d\u7684\u5b9e\u9645\u8fb9\u754c: \u6458\u8981\u4e2d\u63d0\u5230\u201c\u7ec6\u7c92\u5ea6\u63a7\u5236\u201d\uff0c\u4f46\u5176\u5177\u4f53\u80fd\u8fbe\u5230\u4f55\u79cd\u7a0b\u5ea6\u7684\u7cbe\u7ec6\u5316\u7f16\u8f91\uff08\u4f8b\u5982\uff0c\u80fd\u5426\u7cbe\u786e\u4fee\u6539\u67d0\u4e2a\u89d2\u8272\u7684\u5fae\u5c0f\u8868\u60c5\u3001\u7279\u5b9a\u9053\u5177\u7684\u7ec6\u8282\uff09\uff0c\u4ee5\u53ca\u8fd9\u79cd\u63a7\u5236\u7684\u76f4\u89c2\u6027/\u6613\u7528\u6027\uff0c\u4ecd\u9700\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u68c0\u9a8c\u3002\u89e3\u8026\u7f16\u8f91\u7684\u8d28\u91cf\u76f4\u63a5\u5f71\u54cd\u8fd9\u4e00\u70b9\uff0c\u5982\u679c\u89e3\u8026\u4e0d\u5f7b\u5e95\uff0c\u53ef\u80fd\u4f1a\u5728\u7f16\u8f91\u4e00\u4e2a\u5143\u7d20\u65f6\u610f\u5916\u5f71\u54cd\u5230\u5176\u4ed6\u5143\u7d20\u3002 \u8ba1\u7b97\u8d44\u6e90\u4e0e\u6548\u7387: \u6269\u6563\u6a21\u578b\u901a\u5e38\u8ba1\u7b97\u6210\u672c\u8f83\u9ad8\uff0c\u5c24\u5176\u662f\u5728\u8fdb\u884c\u591a\u5e27\u751f\u6210\u548c\u8fed\u4ee3\u7f16\u8f91\u65f6\u3002\u6458\u8981\u4e2d\u672a\u63d0\u53ca\u6027\u80fd\u6216\u901f\u5ea6\uff0c\u8fd9\u53ef\u80fd\u662f\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u4e00\u4e2a\u6f5c\u5728\u9650\u5236\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u5feb\u901f\u8fed\u4ee3\u7684\u521b\u610f\u5de5\u4f5c\u6d41\u4e2d\u3002 Key Findings: To address these challenges, we introduce Plot'n Polish, a zero-shot framework that enables consistent story generation and provides fine-grained control over story visualizations at various levels of detail. Links: PDF arXiv Differential Morphological Profile Neural Networks for Semantic Segmentation Authors: David Huangal, J. Alex Hurt Published: 2025-09-04 Categories: cs.CV Abstract: Semantic segmentation of overhead remote sensing imagery enables applications in mapping, urban planning, and disaster response. State-of-the-art segmentation networks are typically developed and tuned on ground-perspective photographs and do not directly address remote sensing challenges such as extreme scale variation, foreground-background imbalance, and large image sizes. We explore the incorporation of the differential morphological profile (DMP), a multi-scale shape extraction method based on grayscale morphology, into modern segmentation networks. Prior studies have shown that the DMP can provide critical shape information to Deep Neural Networks to enable superior detection and classification performance in overhead imagery. In this work, we extend prior DMPNet work beyond classification and object detection by integrating DMP features into three state-of-the-art convolutional and transformer semantic segmentation architectures. We utilize both direct input, which adapts the input stem of feature extraction architectures to accept DMP channels, and hybrid architectures, a dual-stream design that fuses RGB and DMP encoders. Using the iSAID benchmark dataset, we evaluate a variety of DMP differentials and structuring element shapes to more effectively provide shape information to the model. Our results show that while non-DMP models generally outperform the direct-input variants, hybrid DMP consistently outperforms direct-input and is capable of surpassing a non-DMP model on mIoU, F1, and Recall. Analysis: \u4f5c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u4e13\u5bb6\uff0c\u6211\u5bf9\u8fd9\u7bc7\u8bba\u6587\u6458\u8981\u7684\u5206\u6790\u5982\u4e0b\uff1a \u8bba\u6587\u6458\u8981\u5206\u6790\uff1aDifferential Morphological Profile Neural Networks for Semantic Segmentation 1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u6d01\u603b\u7ed3 (2-3 \u53e5\u8bdd) \u672c\u6587\u65e8\u5728\u89e3\u51b3\u9065\u611f\u56fe\u50cf\u8bed\u4e49\u5206\u5272\u4e2d\u5b58\u5728\u7684\u6781\u7aef\u5c3a\u5ea6\u53d8\u5316\u3001\u524d\u666f\u80cc\u666f\u4e0d\u5e73\u8861\u7b49\u6311\u6218\uff0c\u63d0\u51fa\u5c06\u5dee\u5206\u5f62\u6001\u5b66\u5256\u9762\uff08DMP\uff09\u8fd9\u4e00\u591a\u5c3a\u5ea6\u5f62\u72b6\u63d0\u53d6\u65b9\u6cd5\u878d\u5165\u5230\u5148\u8fdb\u7684\u8bed\u4e49\u5206\u5272\u7f51\u7edc\u4e2d\u3002\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7RGB\u548cDMP\u7f16\u7801\u5668\u878d\u5408\u7684\u53cc\u6d41\u201c\u6df7\u5408\u67b6\u6784\u201d\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u5728mIoU\u3001F1\u548cRecall\u7b49\u6307\u6807\u4e0a\u8d85\u8d8a\u975eDMP\u57fa\u7ebf\u6a21\u578b\uff0c\u4e3a\u9065\u611f\u56fe\u50cf\u5206\u6790\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002 2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u5b66\u65b9\u6cd5 \u672c\u6587\u7684\u5173\u952e\u521b\u65b0\u5728\u4e8e\u5c06\u5dee\u5206\u5f62\u6001\u5b66\u5256\u9762\uff08DMP\uff09\u2014\u2014\u4e00\u79cd\u57fa\u4e8e\u7070\u5ea6\u5f62\u6001\u5b66\u7684\u591a\u5c3a\u5ea6\u5f62\u72b6\u63d0\u53d6\u65b9\u6cd5\u2014\u2014\u9996\u6b21\u6269\u5c55\u5e76\u6210\u529f\u5e94\u7528\u4e8e\u8bed\u4e49\u5206\u5272\u4efb\u52a1\uff0c\u8d85\u8d8a\u4e86\u5176\u5728\u5206\u7c7b\u548c\u76ee\u6807\u68c0\u6d4b\u4e2d\u7684\u4f20\u7edf\u5e94\u7528\u3002\u5176\u6838\u5fc3\u65b9\u6cd5\u5b66\u8d21\u732e\u662f\u63d0\u51fa\u4e86\u4e24\u79cdDMP\u96c6\u6210\u7b56\u7565\uff1a * \u76f4\u63a5\u8f93\u5165 (Direct Input) \uff1a\u8c03\u6574\u7279\u5f81\u63d0\u53d6\u67b6\u6784\u7684\u8f93\u5165\u5c42\u4ee5\u63a5\u53d7DMP\u901a\u9053\u3002 * \u6df7\u5408\u67b6\u6784 (Hybrid Architectures) \uff1a\u4e00\u79cd\u66f4\u6709\u6548\u7684\u53cc\u6d41\u8bbe\u8ba1\uff0c\u5206\u522b\u4f7f\u7528RGB\u548cDMP\u7f16\u7801\u5668\uff0c\u5e76\u5c06\u4e24\u8005\u7684\u7279\u5f81\u8fdb\u884c\u878d\u5408\u3002 \u901a\u8fc7\u5728iSAID\u6570\u636e\u96c6\u4e0a\u5bf9\u4e0d\u540cDMP\u5dee\u5206\u548c\u7ed3\u6784\u5143\u7d20\u5f62\u72b6\u7684\u7cfb\u7edf\u8bc4\u4f30\uff0c\u8bc1\u660e\u4e86\u6df7\u5408\u67b6\u6784\u5728\u63d0\u4f9b\u5173\u952e\u5f62\u72b6\u4fe1\u606f\u65b9\u9762\u7684\u4f18\u8d8a\u6027\u3002 3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd \u63d0\u5347\u9065\u611f\u56fe\u50cf\u5206\u6790\u7cbe\u5ea6\uff1a \u672c\u7814\u7a76\u76f4\u63a5\u89e3\u51b3\u4e86\u9065\u611f\u56fe\u50cf\u8bed\u4e49\u5206\u5272\u7684\u56fa\u6709\u6311\u6218\uff0c\u6709\u671b\u663e\u8457\u63d0\u9ad8\u5730\u56fe\u7ed8\u5236\u3001\u57ce\u5e02\u89c4\u5212\u548c\u707e\u5bb3\u54cd\u5e94\u7b49\u5e94\u7528\u4e2d\u7684\u81ea\u52a8\u5316\u548c\u7cbe\u5ea6\u3002 \u91cd\u65b0\u5ba1\u89c6\u7279\u5f81\u5de5\u7a0b\u7684\u4ef7\u503c\uff1a \u5728\u6df1\u5ea6\u5b66\u4e60\u4e3b\u5bfc\u7684\u65f6\u4ee3\uff0c\u672c\u6587\u5f3a\u8c03\u4e86\u7ed3\u5408\u4f20\u7edf\u3001\u9886\u57df\u7279\u5b9a\uff08\u5982\u5f62\u6001\u5b66\uff09\u7279\u5f81\u4e0e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u6f5c\u529b\uff0c\u4e3a\u6df7\u5408\u6a21\u578b\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u3002 \u542f\u53d1\u591a\u6a21\u6001/\u591a\u7279\u5f81\u878d\u5408\uff1a \u6210\u529f\u878d\u5408DMP\u548cRGB\u4fe1\u606f\uff0c\u53ef\u80fd\u542f\u53d1\u7814\u7a76\u4eba\u5458\u63a2\u7d22\u5c06\u5176\u4ed6\u4e92\u8865\u7684\u3001\u975e\u50cf\u7d20\u7ea7\u7279\u5f81\uff08\u5982\u9ad8\u7a0b\u3001\u5149\u8c31\u6307\u6570\u7b49\uff09\u878d\u5165\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u4ee5\u5e94\u5bf9\u66f4\u590d\u6742\u7684\u89c6\u89c9\u4efb\u52a1\u3002 \u63a8\u52a8DMP\u5728\u66f4\u5e7f\u6cdbCV\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\uff1a \u8bc1\u660eDMP\u5728\u8bed\u4e49\u5206\u5272\u4e2d\u7684\u6709\u6548\u6027\uff0c\u53ef\u80fd\u4f1a\u4fc3\u4f7fDMP\u5728\u5176\u4ed6\u9700\u8981\u7cbe\u7ec6\u5f62\u72b6\u7406\u89e3\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u4e2d\u5f97\u5230\u66f4\u5e7f\u6cdb\u7684\u63a2\u7d22\u3002 4. \u53ef\u80fd\u53d7\u76ca\u7684\u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 \u5730\u7406\u4fe1\u606f\u7cfb\u7edf (GIS) \u548c\u6d4b\u7ed8\uff1a \u63d0\u9ad8\u5730\u7269\u5206\u7c7b\u3001\u571f\u5730\u8986\u76d6\u5236\u56fe\u548c\u53d8\u5316\u68c0\u6d4b\u7684\u81ea\u52a8\u5316\u6c34\u5e73\u548c\u51c6\u786e\u6027\u3002 \u667a\u6167\u57ce\u5e02\u548c\u57ce\u5e02\u89c4\u5212\uff1a \u7cbe\u51c6\u8bc6\u522b\u5efa\u7b51\u7269\u3001\u9053\u8def\u3001\u7eff\u5730\u7b49\uff0c\u652f\u6301\u57ce\u5e02\u57fa\u7840\u8bbe\u65bd\u7ba1\u7406\u548c\u53d1\u5c55\u89c4\u5212\u3002 \u707e\u5bb3\u54cd\u5e94\u4e0e\u7ba1\u7406\uff1a \u5feb\u901f\u51c6\u786e\u5730\u8bc4\u4f30\u707e\u60c5\uff0c\u5982\u6d2a\u6c34\u6df9\u6ca1\u533a\u57df\u3001\u53d7\u635f\u5efa\u7b51\u7269\u8bc6\u522b\uff0c\u8f85\u52a9\u6551\u63f4\u51b3\u7b56\u3002 \u73af\u5883\u76d1\u6d4b\uff1a \u76d1\u6d4b\u68ee\u6797\u780d\u4f10\u3001\u51b0\u5ddd\u878d\u5316\u3001\u519c\u4f5c\u7269\u5065\u5eb7\u72b6\u51b5\u7b49\uff0c\u63d0\u4f9b\u7cbe\u7ec6\u5316\u7684\u73af\u5883\u6570\u636e\u3002 \u519c\u4e1a\u9065\u611f\uff1a \u7cbe\u51c6\u8bc6\u522b\u519c\u4f5c\u7269\u7c7b\u578b\u3001\u751f\u957f\u533a\u57df\uff0c\u652f\u6301\u667a\u80fd\u519c\u4e1a\u7ba1\u7406\u3002 \u56fd\u9632\u4e0e\u60c5\u62a5\uff1a \u63d0\u5347\u5bf9\u519b\u4e8b\u8bbe\u65bd\u3001\u4ea4\u901a\u7f51\u7edc\u7b49\u76ee\u6807\u7684\u8bc6\u522b\u548c\u5206\u6790\u80fd\u529b\u3002 5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u7684\u5c40\u9650\u6027 \u8ba1\u7b97\u6210\u672c\u589e\u52a0\uff1a \u6df7\u5408\u53cc\u6d41\u67b6\u6784\u901a\u5e38\u6bd4\u5355\u6d41\u6a21\u578b\u5177\u6709\u66f4\u9ad8\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u53c2\u6570\u91cf\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u8bad\u7ec3\u548c\u63a8\u7406\u65f6\u95f4\u589e\u52a0\uff0c\u5bf9\u8d44\u6e90\u53d7\u9650\u7684\u5e94\u7528\u6784\u6210\u6311\u6218\u3002 DMP\u53c2\u6570\u8c03\u4f18\u7684\u590d\u6742\u6027\uff1a \u6458\u8981\u63d0\u5230\u8bc4\u4f30\u4e86\u201c\u5404\u79cdDMP\u5dee\u5206\u548c\u7ed3\u6784\u5143\u7d20\u5f62\u72b6\u201d\uff0c\u8fd9\u6697\u793aDMP\u7684\u53c2\u6570\uff08\u5982\u7ed3\u6784\u5143\u7d20\u5927\u5c0f\u3001\u5f62\u72b6\u3001\u5dee\u5206\u9636\u6570\uff09\u53ef\u80fd\u9700\u8981\u9488\u5bf9\u4e0d\u540c\u6570\u636e\u96c6\u6216\u4efb\u52a1\u8fdb\u884c\u7ec6\u81f4\u7684\u8c03\u4f18\uff0c\u8fd9\u589e\u52a0\u4e86\u6a21\u578b\u7684\u90e8\u7f72\u548c\u6cdb\u5316\u96be\u5ea6\u3002 \u201c\u53ef\u4ee5\u8d85\u8d8a\u201d\u7684\u9650\u5b9a\u6027\uff1a \u7ed3\u679c\u663e\u793a\u201chybrid DMP consistently outperforms direct-input and is capable of surpassing a non-DMP model\u201d\uff0c\u5176\u4e2d\u201ccapable of surpassing\u201d\u53ef\u80fd\u610f\u5473\u7740\u5e76\u975e\u5728\u6240\u6709\u60c5\u51b5\u4e0b\u6216\u6240\u6709\u6307\u6807\u4e0a\u90fd\u80fd\u7edd\u5bf9\u8d85\u8d8a\u975eDMP\u6a21\u578b\uff0c\u6216\u8005\u8d85\u8d8a\u7684\u5e45\u5ea6\u53ef\u80fd\u6709\u9650\uff0c\u8fd9\u9700\u8981\u8fdb\u4e00\u6b65\u7684\u5b9e\u9a8c\u7ec6\u8282\u6765\u9a8c\u8bc1\u3002 \u5bf9\u7070\u5ea6\u5f62\u6001\u5b66\u7684\u4f9d\u8d56\uff1a DMP\u57fa\u4e8e\u7070\u5ea6\u5f62\u6001\u5b66\uff0c\u5176\u6709\u6548\u6027\u53ef\u80fd\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4f9d\u8d56\u4e8e\u56fe\u50cf\u4e2d\u5f62\u72b6\u4fe1\u606f\u5728\u7070\u5ea6\u901a\u9053\u4e2d\u7684\u53ef\u63d0\u53d6\u6027\u3002\u5bf9\u4e8e\u989c\u8272\u6216\u7eb9\u7406\u4fe1\u606f\u66f4\u4e3a\u5173\u952e\u7684\u573a\u666f\uff0cDMP\u7684\u76f4\u63a5\u8d21\u732e\u53ef\u80fd\u9700\u8981\u4e0e\u5176\u4ed6\u7279\u5f81\u7ed3\u5408\u3002 Key Findings: State-of-the-art segmentation networks are typically developed and tuned on ground-perspective photographs and do not directly address remote sensing challenges such as extreme scale variation, foreground-background imbalance, and large image sizes. In this work, we extend prior DMPNet work beyond classification and object detection by integrating DMP features into three state-of-the-art convolutional and transformer semantic segmentation architectures. Our results show that while non-DMP models generally outperform the direct-input variants, hybrid DMP consistently outperforms direct-input and is capable of surpassing a non-DMP model on mIoU, F1, and Recall. Links: PDF arXiv TinyDrop: Tiny Model Guided Token Dropping for Vision Transformers Authors: Guoxin Wang, Qingyuan Wang, Binhua Huang, Shaowu Chen, Deepu John Published: 2025-09-03 Categories: cs.CV, cs.AI Abstract: Vision Transformers (ViTs) achieve strong performance in image classification but incur high computational costs from processing all image tokens. To reduce inference costs in large ViTs without compromising accuracy, we propose TinyDrop, a training-free token dropping framework guided by a lightweight vision model. The guidance model estimates the importance of tokens while performing inference, thereby selectively discarding low-importance tokens if large vit models need to perform attention calculations. The framework operates plug-and-play, requires no architectural modifications, and is compatible with diverse ViT architectures. Evaluations on standard image classification benchmarks demonstrate that our framework reduces FLOPs by up to 80% for ViTs with minimal accuracy degradation, highlighting its generalization capability and practical utility for efficient ViT-based classification. Analysis: \u4f5c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u4e13\u5bb6\uff0c\u6211\u5bf9\u8fd9\u7bc7\u5173\u4e8eTinyDrop\u7684\u8bba\u6587\u6458\u8981\u8fdb\u884c\u4e86\u5206\u6790\uff1a \u8bba\u6587\u6458\u8981\u5206\u6790\uff1aTinyDrop: Tiny Model Guided Token Dropping for Vision Transformers 1. \u8bba\u6587\u6838\u5fc3\u8d21\u732e\u7684\u7b80\u660e\u603b\u7ed3 (Concise Summary) TinyDrop\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9Vision Transformers (ViTs) \u7684\u8bad\u7ec3\u65e0\u5173\uff08training-free\uff09\u7684token\u4e22\u5f03\u6846\u67b6\uff0c\u65e8\u5728\u663e\u8457\u964d\u4f4e\u5927\u578bViTs\u7684\u63a8\u7406\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u89c6\u89c9\u6a21\u578b\u5728\u63a8\u7406\u65f6\u52a8\u6001\u8bc4\u4f30token\u7684\u91cd\u8981\u6027\uff0c\u5e76\u9009\u62e9\u6027\u5730\u4e22\u5f03\u4f4e\u91cd\u8981\u6027\u7684token\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0cTinyDrop\u80fd\u5c06ViTs\u7684FLOPs\u964d\u4f4e\u9ad8\u8fbe80%\uff0c\u4e14\u4ec5\u5e26\u6765\u6781\u5c0f\u7684\u7cbe\u5ea6\u635f\u5931\uff0c\u5c55\u73b0\u4e86\u5176\u5728\u63d0\u9ad8ViT\u6548\u7387\u65b9\u9762\u7684\u5b9e\u7528\u4ef7\u503c\u3002 2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u5b66\u65b9\u6cd5 (Key Innovation or Methodological Approach) \u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u5176 \u8bad\u7ec3\u65e0\u5173\uff08training-free\uff09 \u548c \u7531\u8f7b\u91cf\u7ea7\u6a21\u578b\u5f15\u5bfc\u7684\u52a8\u6001token\u4e22\u5f03\u673a\u5236 \u3002\u4e0e\u9700\u8981\u91cd\u65b0\u8bad\u7ec3\u6216\u4fee\u6539\u67b6\u6784\u7684\u73b0\u6709\u65b9\u6cd5\u4e0d\u540c\uff0cTinyDrop\u5f15\u5165\u4e86\u4e00\u4e2a\u5916\u90e8\u7684\u3001\u8f7b\u91cf\u7ea7\u6307\u5bfc\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u80fd\u591f\u5728ViT\u63a8\u7406\u8fc7\u7a0b\u4e2d \u5b9e\u65f6\uff08on-the-fly\uff09 \u8bc4\u4f30\u6bcf\u4e2atoken\u7684\u91cd\u8981\u6027\u3002\u8fd9\u79cd\u65b9\u6cd5\u5141\u8bb8\u5728\u5927\u578bViT\u6a21\u578b\u6267\u884c\u8ba1\u7b97\u5bc6\u96c6\u578b\u7684\u6ce8\u610f\u529b\u64cd\u4f5c\u4e4b\u524d\uff0c \u81ea\u9002\u5e94\u5730\u3001\u9009\u62e9\u6027\u5730 \u4e22\u5f03\u4e0d\u91cd\u8981\u7684token\u3002\u5176\u201c\u5373\u63d2\u5373\u7528\u201d\u7684\u7279\u6027\u548c\u5bf9\u591a\u79cdViT\u67b6\u6784\u7684\u517c\u5bb9\u6027\uff0c\u4e5f\u6781\u5927\u5730\u964d\u4f4e\u4e86\u5176\u5e94\u7528\u95e8\u69db\u548c\u5b9e\u7528\u6027\u3002 3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd (Potential Impact on the Field) \u8fd9\u9879\u7814\u7a76\u5bf9\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u5177\u6709\u6df1\u8fdc\u5f71\u54cd\u3002\u5b83\u6709\u671b \u6c11\u4e3b\u5316\u5927\u578b\u9ad8\u6027\u80fdViT\u6a21\u578b\u7684\u90e8\u7f72 \uff0c\u4f7f\u5176\u80fd\u591f\u5728\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\uff08\u5982\u79fb\u52a8\u8bbe\u5907\u3001\u8fb9\u7f18\u8ba1\u7b97\u3001\u5d4c\u5165\u5f0f\u7cfb\u7edf\uff09\u4e2d\u8fd0\u884c\uff0c\u8fd9\u4e9b\u573a\u666f\u5bf9\u8ba1\u7b97\u9884\u7b97\u548c\u5ef6\u8fdf\u6709\u4e25\u683c\u8981\u6c42\u3002\u901a\u8fc7\u5927\u5e45\u964d\u4f4e\u63a8\u7406FLOPs\uff0cTinyDrop\u4e5f\u4e3a \u53ef\u6301\u7eedAI \u505a\u51fa\u4e86\u8d21\u732e\uff0c\u51cf\u5c11\u4e86\u5927\u578b\u6a21\u578b\u8fd0\u884c\u7684\u80fd\u6e90\u6d88\u8017\u3002\u6b64\u5916\uff0c\u5176\u5373\u63d2\u5373\u7528\u7684\u7279\u6027\u53ef\u4ee5\u52a0\u901fViT\u6a21\u578b\u7684\u7814\u53d1\u548c\u5e94\u7528\uff0c\u4e3a\u73b0\u6709\u548c\u672a\u6765\u7684ViT\u67b6\u6784\u63d0\u4f9b\u4e00\u4e2a\u6613\u4e8e\u96c6\u6210\u7684\u6548\u7387\u4f18\u5316\u65b9\u6848\u3002 4. \u53ef\u80fd\u53d7\u76ca\u7684\u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 (Related Areas or Applications) \u5c3d\u7ba1\u6458\u8981\u4e3b\u8981\u5173\u6ce8\u56fe\u50cf\u5206\u7c7b\uff0c\u4f46token\u9ad8\u6548\u5904\u7406\u7684\u6838\u5fc3\u601d\u60f3\u5bf9\u5e7f\u6cdb\u7684ViT\u5e94\u7528\u90fd\u5177\u6709\u4ef7\u503c\uff1a \u5b9e\u65f6\u8ba1\u7b97\u673a\u89c6\u89c9\u7cfb\u7edf\uff1a \u81ea\u52a8\u9a7e\u9a76\u3001\u673a\u5668\u4eba\u3001\u89c6\u9891\u76d1\u63a7\u7b49\u5bf9\u4f4e\u5ef6\u8fdf\u6709\u4e25\u683c\u8981\u6c42\u7684\u573a\u666f\u3002 \u8fb9\u7f18AI/\u79fb\u52a8\u8ba1\u7b97\uff1a \u5728\u8ba1\u7b97\u80fd\u529b\u548c\u7535\u6c60\u5bff\u547d\u6709\u9650\u7684\u8bbe\u5907\u4e0a\u90e8\u7f72\u590d\u6742\u7684ViT\u6a21\u578b\u3002 \u89c6\u9891\u7406\u89e3\uff1a \u901a\u8fc7\u4e22\u5f03\u65f6\u95f4\u548c\u7a7a\u95f4\u4e0a\u7684\u5197\u4f59token\uff0c\u9ad8\u6548\u5904\u7406\u89c6\u9891\u5e8f\u5217\u3002 \u533b\u5b66\u5f71\u50cf\u5206\u6790\uff1a \u52a0\u901f\u5bf9\u5927\u578b\u533b\u5b66\u56fe\u50cf\u7684\u5206\u6790\uff0c\u53ef\u80fd\u7f29\u77ed\u8bca\u65ad\u65f6\u95f4\u3002 \u5176\u4ed6ViT-based\u4efb\u52a1\uff1a \u4f8b\u5982\u76ee\u6807\u68c0\u6d4b\u3001\u8bed\u4e49\u5206\u5272\u3001\u5b9e\u4f8b\u5206\u5272\u4ee5\u53ca\u4f7f\u7528ViT\u4f5c\u4e3a\u9aa8\u5e72\u7f51\u7edc\u7684\u751f\u6210\u6a21\u578b\uff0c\u8fd9\u4e9b\u4efb\u52a1\u4e2dViT\u4f5c\u4e3a\u7279\u5f81\u63d0\u53d6\u5668\uff0c\u5176\u6548\u7387\u63d0\u5347\u5c06\u5e26\u6765\u6574\u4f53\u6027\u80fd\u7684\u6539\u5584\u3002 5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u7684\u5c40\u9650\u6027 (Limitations that can be inferred from the abstract) \u201c\u6781\u5c0f\u201d\u7cbe\u5ea6\u635f\u5931\u7684\u91cf\u5316\uff1a \u6458\u8981\u4e2d\u63d0\u5230\u201cminimal accuracy degradation\u201d\uff0c\u4f46\u201c\u6781\u5c0f\u201d\u662f\u4e00\u4e2a\u76f8\u5bf9\u6982\u5ff5\u3002\u5728\u5bf9\u7cbe\u5ea6\u8981\u6c42\u6781\u9ad8\u7684\u5e94\u7528\u4e2d\uff0c\u5373\u4f7f\u662f\u5fae\u5c0f\u7684\u635f\u5931\u4e5f\u53ef\u80fd\u65e0\u6cd5\u63a5\u53d7\u3002\u5177\u4f53\u7684FLOPs-\u7cbe\u5ea6\u6743\u8861\u66f2\u7ebf\uff08trade-off curve\uff09\u672a\u5728\u6458\u8981\u4e2d\u8be6\u7ec6\u8bf4\u660e\u3002 \u6307\u5bfc\u6a21\u578b\u7684\u5f00\u9500\u548c\u83b7\u53d6\uff1a \u5c3d\u7ba1\u6307\u5bfc\u6a21\u578b\u662f\u201c\u8f7b\u91cf\u7ea7\u201d\u7684\uff0c\u4f46\u5b83\u4ecd\u7136\u5f15\u5165\u4e86\u989d\u5916\u7684\u8ba1\u7b97\u5f00\u9500\u3002\u6458\u8981\u6ca1\u6709\u91cf\u5316\u8fd9\u79cd\u5f00\u9500\u76f8\u5bf9\u4e8e\u8282\u7701\u7684FLOPs\u7684\u6bd4\u4f8b\uff0c\u4e5f\u6ca1\u6709\u8bf4\u660e\u8fd9\u4e2a\u6307\u5bfc\u6a21\u578b\u662f\u5982\u4f55\u83b7\u53d6\u6216\u8bad\u7ec3\u7684\uff08\u4f8b\u5982\uff0c\u662f\u5426\u9700\u8981\u9884\u8bad\u7ec3\u3001\u662f\u5426\u9700\u8981\u7279\u5b9a\u6570\u636e\uff0c\u6216\u8005\u662f\u5426\u662f\u81ea\u76d1\u7763\u7684\uff09\u3002\u867d\u7136token\u4e22\u5f03\u6846\u67b6\u662f\u8bad\u7ec3\u65e0\u5173\u7684\uff0c\u4f46\u6307\u5bfc\u6a21\u578b\u672c\u8eab\u53ef\u80fd\u4e0d\u662f\u3002 Token\u91cd\u8981\u6027\u4f30\u8ba1\u7684\u9c81\u68d2\u6027\uff1a \u6846\u67b6\u7684\u6709\u6548\u6027\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u6307\u5bfc\u6a21\u578b\u51c6\u786e\u4f30\u8ba1token\u91cd\u8981\u6027\u7684\u80fd\u529b\u3002\u5bf9\u4e8e\u5206\u5e03\u5916\u6570\u636e\uff08out-of-distribution data\uff09\u3001\u5bf9\u6297\u6027\u6837\u672c\u6216\u56fe\u50cf\u4e2d\u7ec6\u5fae\u7279\u5f81\u7684\u9c81\u68d2\u6027\u53ef\u80fd\u662f\u4e00\u4e2a\u6f5c\u5728\u95ee\u9898\u3002 \u5bf9\u590d\u6742\u4efb\u52a1\u7684\u6cdb\u5316\u80fd\u529b\uff1a \u8bc4\u4f30\u4e3b\u8981\u5728\u201c\u6807\u51c6\u56fe\u50cf\u5206\u7c7b\u57fa\u51c6\u201d\u4e0a\u8fdb\u884c\u3002\u5bf9\u4e8e\u66f4\u590d\u6742\u7684\u4efb\u52a1\uff0c\u5982\u9700\u8981\u7cbe\u7ec6\u50cf\u7d20\u7ea7\u4fe1\u606f\u7684\u5bc6\u96c6\u9884\u6d4b\u4efb\u52a1\uff08\u5982\u5206\u5272\u3001\u68c0\u6d4b\uff09\uff0c\u4e22\u5f03token\u53ef\u80fd\u4f1a\u5bf9\u6027\u80fd\u4ea7\u751f\u66f4\u5927\u7684\u8d1f\u9762\u5f71\u54cd\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u4efb\u52a1\u901a\u5e38\u5bf9\u5c40\u90e8\u7ec6\u8282\u66f4\u654f\u611f\u3002 \u8d85\u53c2\u6570\u8c03\u4f18\uff1a Token\u4e22\u5f03\u9608\u503c\u901a\u5e38\u9700\u8981\u8fdb\u884c\u8c03\u4f18\u3002\u867d\u7136\u6846\u67b6\u672c\u8eab\u662f\u8bad\u7ec3\u65e0\u5173\u7684\uff0c\u4f46\u5728\u65b0\u7684ViT\u6a21\u578b\u6216\u6570\u636e\u96c6\u4e0a\u627e\u5230\u6700\u4f73\u7684\u4e22\u5f03\u7b56\u7565\u53ef\u80fd\u4ecd\u9700\u8981\u4e00\u5b9a\u7684\u7ecf\u9a8c\u6027\u641c\u7d22\u3002 Key Findings: To reduce inference costs in large ViTs without compromising accuracy, we propose TinyDrop, a training-free token dropping framework guided by a lightweight vision model. Links: PDF arXiv","title":"Arxiv Computer Vision Papers - 2025-09-05"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-05/#arxiv-computer-vision-papers-2025-09-05","text":"","title":"Arxiv Computer Vision Papers - 2025-09-05"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-05/#executive-summary","text":"\u597d\u7684\uff0c\u8fd9\u662f\u4e00\u4efd\u4e3a\u5fd9\u788c\u7684\u7814\u7a76\u4eba\u5458\u51c6\u5907\u7684 Arxiv \u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u6700\u65b0\u8bba\u6587\u6267\u884c\u6458\u8981\u3002 Arxiv \u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u6700\u65b0\u8bba\u6587\u6267\u884c\u6458\u8981 (2025-09-03) \u672c\u62a5\u544a\u65e8\u5728\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u5f53\u65e5 Arxiv \u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u6700\u65b0\u53d1\u8868\u8bba\u6587\u7684\u5feb\u901f\u6982\u89c8\uff0c\u91cd\u70b9\u5173\u6ce8\u4e3b\u8981\u8d8b\u52bf\u3001\u521b\u65b0\u4eae\u70b9\u3001\u65b0\u5174\u65b9\u5411\u53ca\u63a8\u8350\u9605\u8bfb\u3002 1. \u4e3b\u8981\u4e3b\u9898\u4e0e\u8d8b\u52bf (Main Themes & Trends) \u4eca\u5929\u7684\u8bba\u6587\u5c55\u793a\u4e86\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u51e0\u4e2a\u5173\u952e\u8d8b\u52bf\u7684\u6301\u7eed\u6df1\u5316\u548c\u4ea4\u53c9\u878d\u5408\uff1a \u6269\u6563\u6a21\u578b (Diffusion Models) \u7684\u5e7f\u6cdb\u5e94\u7528\u4e0e\u521b\u65b0: \u6269\u6563\u6a21\u578b\u7ee7\u7eed\u5728\u5404\u79cd\u4efb\u52a1\u4e2d\u5c55\u73b0\u5176\u5f3a\u5927\u80fd\u529b\uff0c\u4ece\u7a00\u758f3D\u6570\u636e\u91cd\u5efa\uff08\u6df1\u5ea6\u56fe\u6062\u590d\uff09\u5230\u89c6\u9891\u5149\u7167\u4f30\u8ba1\uff0c\u518d\u5230\u590d\u6742\u7684\u6587\u672c\u5230\u56fe\u50cf\u6545\u4e8b\u53ef\u89c6\u5316\u548c\u7f16\u8f91\u3002\u8fd9\u8868\u660e\u6269\u6563\u6a21\u578b\u6b63\u6210\u4e3a\u591a\u6a21\u6001\u751f\u6210\u548c\u611f\u77e5\u4efb\u52a1\u7684\u6838\u5fc3\u6280\u672f\u3002 \u57fa\u7840\u6a21\u578b (Foundation Models) \u7684\u6df1\u5316\u4e0e\u4e13\u4e1a\u5316: \u51fa\u73b0\u4e86\u65e8\u5728\u5b9e\u73b0\u7edf\u4e00\u7406\u89e3\u4e0e\u751f\u6210\u7684\u901a\u7528\u57fa\u7840\u6a21\u578b\uff0c\u4ee5\u53ca\u9488\u5bf9\u7279\u5b9a\u9886\u57df\uff08\u5982\u533b\u5b66\u5f71\u50cf\uff09\u7684\u4e13\u4e1a\u5316\u751f\u6210\u5f0f\u57fa\u7840\u6a21\u578b\uff0c\u9884\u793a\u7740\u672a\u6765AI\u7cfb\u7edf\u5c06\u66f4\u52a0\u901a\u7528\u6216\u5728\u7279\u5b9a\u9886\u57df\u8fbe\u5230\u4e13\u5bb6\u7ea7\u6c34\u5e73\u3002 \u89c6\u89c9 Transformer (Vision Transformers) \u7684\u6548\u7387\u4e0e\u9c81\u68d2\u6027\u4f18\u5316: \u9488\u5bf9 ViT \u7684\u8ba1\u7b97\u6548\u7387\u95ee\u9898\uff0c\u6709\u7814\u7a76\u63d0\u51fa\u4e86\u8f7b\u91cf\u5316\u7b56\u7565\uff08\u5982 token dropping\uff09\uff0c\u540c\u65f6\u4e5f\u6709\u5de5\u4f5c\u5173\u6ce8\u5176\u5728\u5bf9\u6297\u6027\u653b\u51fb\uff08\u5982\u6c34\u5370\uff09\u4e0b\u7684\u9c81\u68d2\u6027\u589e\u5f3a\u3002 \u591a\u6a21\u6001\u4e0e\u8de8\u9886\u57df\u878d\u5408 (Multimodality & Cross-Domain Fusion): \u8bba\u6587\u6db5\u76d6\u4e86\u6587\u672c-\u56fe\u50cf\u3001\u89c6\u9891-\u56fe\u50cf\u3001\u70b9\u4e91-\u56fe\u50cf\u7b49\u591a\u79cd\u6a21\u6001\u7684\u878d\u5408\uff0c\u4ee5\u53ca\u8ba1\u7b97\u673a\u89c6\u89c9\u4e0e\u673a\u5668\u4eba\u3001\u533b\u7597\u3001\u57fa\u7840\u8bbe\u65bd\u7b49\u9886\u57df\u7684\u6df1\u5ea6\u7ed3\u5408\u3002 \u751f\u6210\u5f0f AI \u7684\u7cbe\u7ec6\u63a7\u5236\u4e0e\u7f16\u8f91 (Fine-grained Control & Editing in Generative AI): \u4e0d\u518d\u4ec5\u4ec5\u662f\u751f\u6210\u56fe\u50cf\uff0c\u800c\u662f\u8ffd\u6c42\u5bf9\u751f\u6210\u5185\u5bb9\u66f4\u6df1\u5c42\u6b21\u7684\u7406\u89e3\u3001\u7f16\u8f91\u548c\u53d9\u4e8b\u80fd\u529b\u3002 \u5b9e\u7528\u5de5\u5177\u4e0e\u5e94\u7528 (Practical Tools & Applications): \u51fa\u73b0\u4e86\u63d0\u5347\u6807\u6ce8\u6548\u7387\u7684AI\u8f85\u52a9\u5de5\u5177\uff0c\u4ee5\u53ca\u9762\u5411\u5b9e\u9645\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u5f00\u653e\u8bcd\u6c47\u6293\u53d6\u8f85\u52a9\u7cfb\u7edf\u3002 2. \u663e\u8457\u6216\u521b\u65b0\u6027\u8bba\u6587\u4eae\u70b9 (Significant or Innovative Papers) OneCAT: Decoder-Only Auto-Regressive Model for Unified Understanding and Generation (Han Li et al.) : \u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u89e3\u7801\u5668-only\u81ea\u56de\u5f52\u6a21\u578b\uff0c\u65e8\u5728\u540c\u65f6\u5b9e\u73b0\u7406\u89e3\u548c\u751f\u6210\u4efb\u52a1\u3002\u5176\u6f5c\u529b\u5728\u4e8e\u6784\u5efa\u66f4\u901a\u7528\u3001\u66f4\u5f3a\u5927\u7684AI\u7cfb\u7edf\uff0c\u662f\u8fc8\u5411\u901a\u7528\u667a\u80fd\u7684\u91cd\u8981\u4e00\u6b65\u3002 A Generative Foundation Model for Chest Radiography (Yuanfeng Ji et al.) : \u5728\u533b\u7597\u5f71\u50cf\u9886\u57df\u5f15\u5165\u751f\u6210\u5f0f\u57fa\u7840\u6a21\u578b\uff0c\u6709\u671b\u5f7b\u5e95\u6539\u53d8\u80f8\u90e8X\u5149\u7247\u7684\u5206\u6790\u3001\u8bca\u65ad\u548c\u6570\u636e\u589e\u5f3a\u65b9\u5f0f\uff0c\u5bf9\u533b\u7597AI\u5177\u6709\u91cc\u7a0b\u7891\u610f\u4e49\u3002 InfraDiffusion: zero-shot depth map restoration with diffusion models and prompted segmentation from sparse infrastructure point clouds (Yixiong Jing et al.) : \u521b\u65b0\u6027\u5730\u5c06\u6269\u6563\u6a21\u578b\u5e94\u7528\u4e8e\u4ece\u7a00\u758f\u57fa\u7840\u8bbe\u65bd\u70b9\u4e91\u8fdb\u884c\u96f6\u6837\u672c\u6df1\u5ea6\u56fe\u6062\u590d\uff0c\u89e3\u51b3\u4e863D\u89c6\u89c9\u4e2d\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u5b9e\u9645\u95ee\u9898\u3002 LuxDiT: Lighting Estimation with Video Diffusion Transformer (Ruofan Liang et al.) : \u7ed3\u5408\u4e86\u89c6\u9891\u3001\u6269\u6563\u6a21\u578b\u548c Transformer \u7684\u5f3a\u5927\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u5149\u7167\u4f30\u8ba1\uff0c\u5bf9\u865a\u62df\u73b0\u5b9e\u3001\u7535\u5f71\u5236\u4f5c\u548c\u56fe\u50cf\u7f16\u8f91\u7b49\u9886\u57df\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002 Plot'n Polish: Zero-shot Story Visualization and Disentangled Editing with Text-to-Image Diffusion Models (Kiymet Akdemir et al.) : \u7a81\u7834\u4e86\u7b80\u5355\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\uff0c\u5b9e\u73b0\u4e86\u6545\u4e8b\u7ea7\u522b\u7684\u53ef\u89c6\u5316\u548c\u89e3\u8026\u7f16\u8f91\uff0c\u5c55\u793a\u4e86\u751f\u6210\u5f0fAI\u5728\u521b\u610f\u5185\u5bb9\u751f\u4ea7\u65b9\u9762\u7684\u5de8\u5927\u6f5c\u529b\u3002 TinyDrop: Tiny Model Guided Token Dropping for Vision Transformers (Guoxin Wang et al.) : \u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684 ViT \u4f18\u5316\u7b56\u7565\uff0c\u901a\u8fc7\u5fae\u5c0f\u6a21\u578b\u5f15\u5bfc\u7684 token dropping \u6765\u63d0\u5347\u63a8\u7406\u901f\u5ea6\uff0c\u5bf9 ViT \u7684\u5b9e\u9645\u90e8\u7f72\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002 3. \u65b0\u5174\u7814\u7a76\u65b9\u5411\u6216\u6280\u672f (Emerging Research Directions or Techniques) \u7edf\u4e00\u7684\u7406\u89e3\u4e0e\u751f\u6210\u8303\u5f0f (Unified Understanding & Generation Paradigm): \u4ee5 OneCAT \u4e3a\u4ee3\u8868\uff0c\u63a2\u7d22\u5982\u4f55\u7528\u5355\u4e00\u6a21\u578b\u67b6\u6784\u5904\u7406\u591a\u6a21\u6001\u7684\u611f\u77e5\u4e0e\u751f\u6210\u4efb\u52a1\u3002 \u9886\u57df\u7279\u5b9a\u57fa\u7840\u6a21\u578b (Domain-Specific Foundation Models): \u9488\u5bf9\u7279\u5b9a\u9ad8\u4ef7\u503c\u9886\u57df\uff08\u5982\u533b\u7597\u3001\u5de5\u4e1a\uff09\u5f00\u53d1\u5b9a\u5236\u5316\u7684\u57fa\u7840\u6a21\u578b\uff0c\u4ee5\u5b9e\u73b0\u66f4\u7cbe\u51c6\u3001\u9ad8\u6548\u7684\u5e94\u7528\u3002 \u7a00\u758f/\u4e0d\u5b8c\u6574\u6570\u636e\u4e0a\u7684\u6269\u6563\u6a21\u578b (Diffusion Models on Sparse/Incomplete Data): InfraDiffusion \u5c55\u793a\u4e86\u6269\u6563\u6a21\u578b\u5728\u5904\u7406\u4e0d\u5b8c\u65743D\u6570\u636e\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u672a\u6765\u53ef\u80fd\u6269\u5c55\u5230\u66f4\u591a\u6570\u636e\u7a00\u7f3a\u573a\u666f\u3002 \u9ad8\u6548\u4e14\u9c81\u68d2\u7684 Transformer \u67b6\u6784 (Efficient & Robust Transformer Architectures): TinyDrop \u548c\u6c34\u58a8\u753b\u589e\u5f3a\u9c81\u68d2\u6027\u7684\u7814\u7a76\uff0c\u8868\u660e\u5bf9 Transformer \u6a21\u578b\u6548\u7387\u548c\u5b89\u5168\u6027\u7684\u5173\u6ce8\u5c06\u6301\u7eed\u589e\u52a0\u3002 \u591a\u6a21\u6001\u610f\u56fe\u68c0\u6d4b\u4e0e\u673a\u5668\u4eba\u4ea4\u4e92 (Multimodal Intent Detection & Robotic Interaction): OVGrasp \u5f3a\u8c03\u4e86\u7ed3\u5408\u89c6\u89c9\u3001\u8bed\u8a00\u7b49\u591a\u79cd\u6a21\u6001\u6765\u7406\u89e3\u4eba\u7c7b\u610f\u56fe\uff0c\u4ee5\u5b9e\u73b0\u66f4\u667a\u80fd\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u3002 \u751f\u6210\u5f0f AI \u7684\u53d9\u4e8b\u4e0e\u9ad8\u5c42\u6b21\u7f16\u8f91 (Narrative & High-Level Editing in Generative AI): Plot'n Polish \u9884\u793a\u7740\u751f\u6210\u6a21\u578b\u5c06\u4ece\u56fe\u50cf\u751f\u6210\u8d70\u5411\u66f4\u590d\u6742\u7684\u53d9\u4e8b\u548c\u5185\u5bb9\u521b\u4f5c\u3002 4. \u5efa\u8bae\u6df1\u5165\u9605\u8bfb\u7684\u8bba\u6587 (Recommended Full Reads) \u8003\u8651\u5230\u5176\u6f5c\u5728\u5f71\u54cd\u548c\u521b\u65b0\u6027\uff0c\u6211\u4eec\u5efa\u8bae\u7814\u7a76\u4eba\u5458\u4f18\u5148\u9605\u8bfb\u4ee5\u4e0b\u8bba\u6587\uff1a OneCAT: Decoder-Only Auto-Regressive Model for Unified Understanding and Generation (Han Li et al.) : \u5bf9\u4e8e\u5173\u6ce8\u901a\u7528AI\u3001\u57fa\u7840\u6a21\u578b\u67b6\u6784\u548c\u591a\u6a21\u6001\u5b66\u4e60\u7684\u7814\u7a76\u8005\uff0c\u8fd9\u7bc7\u8bba\u6587\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u672a\u6765\u65b9\u5411\u3002 A Generative Foundation Model for Chest Radiography (Yuanfeng Ji et al.) : \u533b\u7597AI\u9886\u57df\u7684\u7814\u7a76\u4eba\u5458\u5e94\u91cd\u70b9\u5173\u6ce8\uff0c\u5b83\u53ef\u80fd\u4e3a\u533b\u5b66\u5f71\u50cf\u5206\u6790\u5e26\u6765\u8303\u5f0f\u8f6c\u53d8\u3002 InfraDiffusion: zero-shot depth map restoration with diffusion models and prompted segmentation from sparse infrastructure point clouds (Yixiong Jing et al.) : \u4e13\u6ce8\u4e8e3D\u89c6\u89c9\u3001\u6269\u6563\u6a21\u578b\u5728\u7a00\u758f\u6570\u636e\u5e94\u7528\u6216\u57fa\u7840\u8bbe\u65bdAI\u7684\u7814\u7a76\u8005\u4f1a\u4ece\u4e2d\u53d7\u76ca\u3002 TinyDrop: Tiny Model Guided Token Dropping for Vision Transformers (Guoxin Wang et al.) : \u5bf9\u4e8e\u81f4\u529b\u4e8e Vision Transformer \u90e8\u7f72\u3001\u6548\u7387\u4f18\u5316\u548c\u8fb9\u7f18\u8ba1\u7b97\u7684\u7814\u7a76\u8005\uff0c\u8fd9\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002 Plot'n Polish: Zero-shot Story Visualization and Disentangled Editing with Text-to-Image Diffusion Models (Kiymet Akdemir et al.) : \u5bf9\u751f\u6210\u5f0fAI\u3001\u521b\u610f\u5e94\u7528\u3001\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u548c\u53ef\u63a7\u5185\u5bb9\u521b\u4f5c\u611f\u5174\u8da3\u7684\u7814\u7a76\u8005\u4e0d\u5bb9\u9519\u8fc7\u3002","title":"Executive Summary"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-05/#table-of-contents","text":"InfraDiffusion: zero-shot depth map restoration with diffusion models and prompted segmentation from sparse infrastructure point clouds VisioFirm: Cross-Platform AI-assisted Annotation Tool for Computer Vision LuxDiT: Lighting Estimation with Video Diffusion Transformer OneCAT: Decoder-Only Auto-Regressive Model for Unified Understanding and Generation A Generative Foundation Model for Chest Radiography OVGrasp: Open-Vocabulary Grasping Assistance via Multimodal Intent Detection Enhancing Robustness in Post-Processing Watermarking: An Ensemble Attack Network Using CNNs and Transformers Plot'n Polish: Zero-shot Story Visualization and Disentangled Editing with Text-to-Image Diffusion Models Differential Morphological Profile Neural Networks for Semantic Segmentation TinyDrop: Tiny Model Guided Token Dropping for Vision Transformers","title":"Table of Contents"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-05/#papers","text":"","title":"Papers"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-05/#infradiffusion-zero-shot-depth-map-restoration-with-diffusion-models-and-prompted-segmentation-from-sparse-infrastructure-point-clouds","text":"Authors: Yixiong Jing, Cheng Zhang, Haibing Wu, Guangming Wang, Olaf Wysocki, Brian Sheil Published: 2025-09-03 Categories: cs.CV Abstract: Point clouds are widely used for infrastructure monitoring by providing geometric information, where segmentation is required for downstream tasks such as defect detection. Existing research has automated semantic segmentation of structural components, while brick-level segmentation (identifying defects such as spalling and mortar loss) has been primarily conducted from RGB images. However, acquiring high-resolution images is impractical in low-light environments like masonry tunnels. Point clouds, though robust to dim lighting, are typically unstructured, sparse, and noisy, limiting fine-grained segmentation. We present InfraDiffusion, a zero-shot framework that projects masonry point clouds into depth maps using virtual cameras and restores them by adapting the Denoising Diffusion Null-space Model (DDNM). Without task-specific training, InfraDiffusion enhances visual clarity and geometric consistency of depth maps. Experiments on masonry bridge and tunnel point cloud datasets show significant improvements in brick-level segmentation using the Segment Anything Model (SAM), underscoring its potential for automated inspection of masonry assets. Our code and data is available at https://github.com/Jingyixiong/InfraDiffusion-official-implement. Analysis: InfraDiffusion \u8bba\u6587\u6458\u8981\u7684\u5206\u6790\u5982\u4e0b\uff1a","title":"InfraDiffusion: zero-shot depth map restoration with diffusion models and prompted segmentation from sparse infrastructure point clouds"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-05/#1","text":"InfraDiffusion \u63d0\u51fa\u4e86\u4e00\u79cd\u96f6\u6837\u672c\uff08zero-shot\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u7a00\u758f\u3001\u975e\u7ed3\u6784\u5316\u4e14\u5608\u6742\u7684\u7816\u77f3\u70b9\u4e91\u4e2d\u6062\u590d\u9ad8\u8d28\u91cf\u7684\u6df1\u5ea6\u56fe\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u5c06\u70b9\u4e91\u6295\u5f71\u5230\u865a\u62df\u76f8\u673a\u751f\u6210\u7684\u6df1\u5ea6\u56fe\uff0c\u5e76\u5229\u7528\u9002\u5e94\u6027\u4fee\u6539\u7684\u53bb\u566a\u6269\u6563\u96f6\u7a7a\u95f4\u6a21\u578b\uff08DDNM\uff09\u8fdb\u884c\u4fee\u590d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6df1\u5ea6\u56fe\u7684\u89c6\u89c9\u6e05\u6670\u5ea6\u548c\u51e0\u4f55\u4e00\u81f4\u6027\u3002\u5176\u6838\u5fc3\u8d21\u732e\u5728\u4e8e\uff0c\u65e0\u9700\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\u8fdb\u884c\u8bad\u7ec3\uff0c\u5373\u53ef\u663e\u8457\u6539\u5584\u7816\u77f3\u7ed3\u6784\u7684\u7816\u5757\u7ea7\u5206\u5272\u6548\u679c\uff0c\u4ece\u800c\u63a8\u52a8\u81ea\u52a8\u5316\u57fa\u7840\u8bbe\u65bd\u68c0\u6d4b\u3002","title":"1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u660e\u6458\u8981"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-05/#2","text":"\u8be5\u8bba\u6587\u7684\u5173\u952e\u521b\u65b0\u5728\u4e8e\u5176 \u96f6\u6837\u672c\u6df1\u5ea6\u56fe\u6062\u590d\u6846\u67b6 \uff0c\u5b83\u5de7\u5999\u5730\u7ed3\u5408\u4e86\u4ee5\u4e0b\u51e0\u70b9\uff1a * \u70b9\u4e91\u5230\u6df1\u5ea6\u56fe\u7684\u6295\u5f71\uff1a \u5c06\u7a00\u758f\u7684\u7816\u77f3\u70b9\u4e91\u901a\u8fc7\u865a\u62df\u76f8\u673a\u8f6c\u6362\u4e3a\u6df1\u5ea6\u56fe\uff0c\u4e3a\u540e\u7eed\u5904\u7406\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7ed3\u6784\u5316\u7684\u4e8c\u7ef4\u8868\u793a\u3002 * \u9002\u5e94\u6027\u6269\u6563\u6a21\u578b\u5e94\u7528\uff1a \u6838\u5fc3\u5728\u4e8e \u9002\u5e94\u6027\u5730\u4fee\u6539\u548c\u5e94\u7528\u53bb\u566a\u6269\u6563\u96f6\u7a7a\u95f4\u6a21\u578b\uff08DDNM\uff09 \u8fdb\u884c\u6df1\u5ea6\u56fe\u6062\u590d\u3002DDNM \u901a\u5e38\u7528\u4e8e\u56fe\u50cf\u4fee\u590d\u6216\u6761\u4ef6\u751f\u6210\uff0c\u6b64\u5904\u5c06\u5176\u521b\u65b0\u6027\u5730\u5e94\u7528\u4e8e\u4ece\u7a00\u758f\u3001\u4e0d\u5b8c\u6574\u6570\u636e\u4e2d\u6062\u590d\u51e0\u4f55\u4fe1\u606f\u4e30\u5bcc\u7684\u6df1\u5ea6\u56fe\uff0c\u4e14\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u7684\u8bad\u7ec3\u3002 * \u96f6\u6837\u672c\u80fd\u529b\uff1a \u6574\u4e2a\u6846\u67b6\u65e0\u9700\u9488\u5bf9\u7816\u77f3\u7ed3\u6784\u6216\u6df1\u5ea6\u56fe\u6062\u590d\u4efb\u52a1\u8fdb\u884c\u989d\u5916\u7684\u8bad\u7ec3\uff0c\u76f4\u63a5\u5229\u7528\u9884\u8bad\u7ec3\u7684\u6269\u6563\u6a21\u578b\u80fd\u529b\uff0c\u8fd9\u5927\u5927\u964d\u4f4e\u4e86\u6570\u636e\u6807\u6ce8\u548c\u6a21\u578b\u8bad\u7ec3\u7684\u6210\u672c\uff0c\u5e76\u63d0\u5347\u4e86\u6cdb\u5316\u80fd\u529b\u3002 * \u4e0e\u73b0\u6709\u5206\u5272\u6a21\u578b\u7684\u7ed3\u5408\uff1a \u6062\u590d\u540e\u7684\u9ad8\u8d28\u91cf\u6df1\u5ea6\u56fe\u80fd\u591f\u663e\u8457\u63d0\u5347\u5982 Segment Anything Model (SAM) \u7b49\u901a\u7528\u5206\u5272\u6a21\u578b\u5728\u7816\u5757\u7ea7\u5206\u5272\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u5b9e\u73b0\u4e86\u4ece\u4f4e\u8d28\u91cf\u70b9\u4e91\u5230\u9ad8\u7cbe\u5ea6\u8bed\u4e49\u7406\u89e3\u7684\u6865\u6881\u3002","title":"2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u5b66\u65b9\u6cd5"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-05/#3","text":"\u57fa\u7840\u8bbe\u65bd\u68c0\u6d4b\u4e0e\u7ef4\u62a4\uff1a \u4e3a\u7816\u77f3\u6865\u6881\u548c\u96a7\u9053\u7b49\u57fa\u7840\u8bbe\u65bd\u7684\u81ea\u52a8\u5316\u3001\u7cbe\u7ec6\u5316\u68c0\u6d4b\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u5de5\u5177\uff0c\u80fd\u591f\u66f4\u51c6\u786e\u5730\u8bc6\u522b\u5265\u843d\u3001\u7802\u6d46\u6d41\u5931\u7b49\u7f3a\u9677\uff0c\u4ece\u800c\u63d0\u9ad8\u7ef4\u62a4\u6548\u7387\u548c\u5b89\u5168\u6027\u3002 \u70b9\u4e91\u6570\u636e\u5229\u7528\u6548\u7387\uff1a \u514b\u670d\u4e86\u7a00\u758f\u3001\u5608\u6742\u70b9\u4e91\u5728\u7ec6\u7c92\u5ea6\u5206\u6790\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u4ece\u4f4e\u8d28\u91cf\u4e09\u7ef4\u6570\u636e\u4e2d\u63d0\u53d6\u9ad8\u4ef7\u503c\u4fe1\u606f\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002 \u6269\u6563\u6a21\u578b\u5e94\u7528\u62d3\u5c55\uff1a \u5c06\u6269\u6563\u6a21\u578b\u7684\u5e94\u7528\u8303\u56f4\u4ece\u4f20\u7edf\u7684\u56fe\u50cf\u751f\u6210\u3001\u4fee\u590d\u7b49\u9886\u57df\u62d3\u5c55\u5230\u51e0\u4f55\u6570\u636e\uff08\u6df1\u5ea6\u56fe\uff09\u7684\u6062\u590d\u548c\u589e\u5f3a\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5904\u7406\u7ed3\u6784\u5316\u51e0\u4f55\u4fe1\u606f\u65b9\u9762\u7684\u5de8\u5927\u6f5c\u529b\u3002 \u96f6\u6837\u672c\u5b66\u4e60\u7684\u5b9e\u8df5\uff1a \u5f3a\u8c03\u4e86\u96f6\u6837\u672c\u65b9\u6cd5\u5728\u5b9e\u9645\u5de5\u7a0b\u5e94\u7528\u4e2d\u7684\u53ef\u884c\u6027\u548c\u4f18\u52bf\uff0c\u5c24\u5176\u662f\u5728\u96be\u4ee5\u83b7\u53d6\u5927\u91cf\u6807\u6ce8\u6570\u636e\u7684\u7279\u5b9a\u9886\u57df\u3002 \u591a\u6a21\u6001\u6570\u636e\u878d\u5408\uff1a \u867d\u7136\u62bd\u8c61\u4e2d\u672a\u76f4\u63a5\u63d0\u53ca\uff0c\u4f46\u8fd9\u79cd\u5c06\u4e09\u7ef4\u70b9\u4e91\u8f6c\u6362\u4e3a\u4e8c\u7ef4\u6df1\u5ea6\u56fe\u5e76\u5229\u7528\u56fe\u50cf\u5904\u7406\u6280\u672f\u8fdb\u884c\u589e\u5f3a\u7684\u601d\u8def\uff0c\u4e3a\u672a\u6765\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u548c\u5904\u7406\u63d0\u4f9b\u4e86\u65b0\u7684\u8303\u5f0f\u3002","title":"3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-05/#4","text":"\u571f\u6728\u5de5\u7a0b\u4e0e\u7ed3\u6784\u5065\u5eb7\u76d1\u6d4b\uff1a \u81ea\u52a8\u5316\u68c0\u6d4b\u6865\u6881\u3001\u96a7\u9053\u3001\u5927\u575d\u7b49\u57fa\u7840\u8bbe\u65bd\u7684\u7ed3\u6784\u7f3a\u9677\u3002 \u6587\u5316\u9057\u4ea7\u4fdd\u62a4\uff1a \u5bf9\u5386\u53f2\u5efa\u7b51\u3001\u96d5\u5851\u7b49\u8fdb\u884c\u7cbe\u7ec6\u5316\u4e09\u7ef4\u626b\u63cf\u548c\u635f\u4f24\u8bc4\u4f30\u3002 \u673a\u5668\u4eba\u4e0e\u81ea\u4e3b\u68c0\u6d4b\uff1a \u88c5\u5907\u6709\u6fc0\u5149\u96f7\u8fbe\u7684\u673a\u5668\u4eba\u6216\u65e0\u4eba\u673a\u5728\u590d\u6742\u3001\u4f4e\u5149\u7167\u73af\u5883\u4e0b\u8fdb\u884c\u81ea\u4e3b\u5de1\u68c0\u548c\u73af\u5883\u611f\u77e5\u3002 \u6570\u5b57\u5b6a\u751f\uff08Digital Twin\uff09\uff1a \u521b\u5efa\u9ad8\u7cbe\u5ea6\u7684\u7269\u7406\u8d44\u4ea7\u6570\u5b57\u6a21\u578b\uff0c\u7528\u4e8e\u6a21\u62df\u3001\u5206\u6790\u548c\u9884\u6d4b\u3002 \u5efa\u7b51\u4fe1\u606f\u6a21\u578b\uff08BIM\uff09\uff1a \u589e\u5f3a\u73b0\u6709\u5efa\u7b51\u7684BIM\u6a21\u578b\uff0c\u4f7f\u5176\u5305\u542b\u66f4\u8be6\u7ec6\u7684\u7ed3\u6784\u5065\u5eb7\u4fe1\u606f\u3002 \u91c7\u77ff\u4e0e\u5730\u8d28\u52d8\u63a2\uff1a \u96a7\u9053\u3001\u77ff\u4e95\u7b49\u5730\u4e0b\u7a7a\u95f4\u7684\u7ed3\u6784\u7a33\u5b9a\u6027\u76d1\u6d4b\u3002","title":"4. \u76f8\u5173\u9886\u57df\u6216\u5e94\u7528"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-05/#5","text":"\u4f9d\u8d56\u865a\u62df\u76f8\u673a\u89c6\u89d2\uff1a \u5c06\u70b9\u4e91\u6295\u5f71\u5230\u6df1\u5ea6\u56fe\u7684\u8d28\u91cf\u548c\u5b8c\u6574\u6027\u53ef\u80fd\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u865a\u62df\u76f8\u673a\u7684\u9009\u62e9\u548c\u6570\u91cf\u3002\u5982\u679c\u70b9\u4e91\u5728\u67d0\u4e9b\u533a\u57df\u6781\u5176\u7a00\u758f\uff0c\u6216\u8005\u865a\u62df\u76f8\u673a\u89c6\u89d2\u4e0d\u4f73\uff0c\u53ef\u80fd\u5bfc\u81f4\u6df1\u5ea6\u56fe\u4fe1\u606f\u7f3a\u5931\u6216\u4e0d\u51c6\u786e\u3002 DDNM\u7684\u6cdb\u5316\u80fd\u529b\uff1a \u5c3d\u7ba1\u662f\u201c\u96f6\u6837\u672c\u201d\uff0c\u4f46DDNM\u672c\u8eab\u662f\u9884\u8bad\u7ec3\u6a21\u578b\u3002\u5176\u5728\u5904\u7406\u7816\u77f3\u7ed3\u6784\u7279\u6709\u7684\u51e0\u4f55\u7eb9\u7406\u548c\u7f3a\u9677\u6a21\u5f0f\u4e0a\u7684\u8868\u73b0\uff0c\u53ef\u80fd\u53d7\u9650\u4e8e\u5176\u539f\u59cb\u8bad\u7ec3\u6570\u636e\u7684\u9886\u57df\u3002\u5bf9\u4e8e\u4e0e\u8bad\u7ec3\u6570\u636e\u5dee\u5f02\u8f83\u5927\u7684\u51e0\u4f55\u7ed3\u6784\u6216\u6750\u6599\uff0c\u6548\u679c\u53ef\u80fd\u6709\u6240\u4e0b\u964d\u3002 \u7816\u77f3\u7ed3\u6784\u7684\u7279\u5f02\u6027\uff1a \u8bba\u6587\u5f3a\u8c03\u4e86\u201c\u7816\u77f3\u70b9\u4e91\u201d\u548c\u201c\u7816\u5757\u7ea7\u5206\u5272\u201d\u3002\u8fd9\u8868\u660e\u8be5\u65b9\u6cd5\u53ef\u80fd\u9488\u5bf9\u7816\u77f3\u7ed3\u6784\u8fdb\u884c\u4e86\u4f18\u5316\u6216\u9a8c\u8bc1\uff0c\u5176\u5728\u5176\u4ed6\u7c7b\u578b\u7ed3\u6784\uff08\u5982\u6df7\u51dd\u571f\u3001\u94a2\u7ed3\u6784\uff09\u6216\u66f4\u590d\u6742\u7f3a\u9677\uff08\u5982\u88c2\u7f1d\u3001\u53d8\u5f62\uff09\u4e0a\u7684\u8868\u73b0\u5c1a\u4e0d\u660e\u786e\u3002 \u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\uff1a \u6269\u6563\u6a21\u578b\u901a\u5e38\u8ba1\u7b97\u6210\u672c\u8f83\u9ad8\uff0c\u5c24\u5176\u662f\u5728\u751f\u6210\u9ad8\u5206\u8fa8\u7387\u6df1\u5ea6\u56fe\u65f6\u3002\u6458\u8981\u4e2d\u672a\u63d0\u53ca\u5b9e\u65f6\u6027\u6216\u8ba1\u7b97\u6548\u7387\uff0c\u8fd9\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u53ef\u80fd\u662f\u4e00\u4e2a\u8003\u91cf\u56e0\u7d20\u3002 \u6df1\u5ea6\u56fe\u7684\u5c40\u9650\u6027\uff1a \u6df1\u5ea6\u56fe\u662f2.5D\u8868\u793a\uff0c\u65e0\u6cd5\u5b8c\u5168\u6355\u6349\u4e09\u7ef4\u70b9\u4e91\u7684\u6240\u6709\u51e0\u4f55\u4fe1\u606f\uff0c\u4f8b\u5982\u906e\u6321\u533a\u57df\u540e\u7684\u7ed3\u6784\u3002\u8fd9\u53ef\u80fd\u9650\u5236\u4e86\u5bf9\u67d0\u4e9b\u590d\u6742\u7f3a\u9677\u7684\u68c0\u6d4b\u80fd\u529b\u3002 Key Findings: We present InfraDiffusion, a zero-shot framework that projects masonry point clouds into depth maps using virtual cameras and restores them by adapting the Denoising Diffusion Null-space Model (DDNM). Links: PDF arXiv","title":"5. \u53ef\u4ece\u6458\u8981\u4e2d\u63a8\u65ad\u51fa\u7684\u5c40\u9650\u6027"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-05/#visiofirm-cross-platform-ai-assisted-annotation-tool-for-computer-vision","text":"Authors: Safouane El Ghazouali, Umberto Michelucci Published: 2025-09-04 Categories: cs.CV, cs.AI Abstract: AI models rely on annotated data to learn pattern and perform prediction. Annotation is usually a labor-intensive step that require associating labels ranging from a simple classification label to more complex tasks such as object detection, oriented bounding box estimation, and instance segmentation. Traditional tools often require extensive manual input, limiting scalability for large datasets. To address this, we introduce VisioFirm, an open-source web application designed to streamline image labeling through AI-assisted automation. VisioFirm integrates state-of-the-art foundation models into an interface with a filtering pipeline to reduce human-in-the-loop efforts. This hybrid approach employs CLIP combined with pre-trained detectors like Ultralytics models for common classes and zero-shot models such as Grounding DINO for custom labels, generating initial annotations with low-confidence thresholding to maximize recall. Through this framework, when tested on COCO-type of classes, initial prediction have been proven to be mostly correct though the users can refine these via interactive tools supporting bounding boxes, oriented bounding boxes, and polygons. Additionally, VisioFirm has on-the-fly segmentation powered by Segment Anything accelerated through WebGPU for browser-side efficiency. The tool supports multiple export formats (YOLO, COCO, Pascal VOC, CSV) and operates offline after model caching, enhancing accessibility. VisioFirm demonstrates up to 90\\% reduction in manual effort through benchmarks on diverse datasets, while maintaining high annotation accuracy via clustering of connected CLIP-based disambiguate components and IoU-graph for redundant detection suppression. VisioFirm can be accessed from \\href{https://github.com/OschAI/VisioFirm}{https://github.com/OschAI/VisioFirm}. Analysis: \u4f5c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u4e13\u5bb6\uff0c\u6211\u5bf9\u8fd9\u7bc7\u8bba\u6587\u6458\u8981\u8fdb\u884c\u5982\u4e0b\u5206\u6790\uff1a","title":"VisioFirm: Cross-Platform AI-assisted Annotation Tool for Computer Vision"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-05/#1-concise-summary","text":"VisioFirm\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u8de8\u5e73\u53f0AI\u8f85\u52a9\u6807\u6ce8\u5de5\u5177\uff0c\u65e8\u5728\u89e3\u51b3\u8ba1\u7b97\u673a\u89c6\u89c9\u6570\u636e\u6807\u6ce8\u8017\u65f6\u8017\u529b\u7684\u95ee\u9898\u3002\u5b83\u901a\u8fc7\u667a\u80fd\u96c6\u6210CLIP\u3001Grounding DINO\u3001Ultralytics\u7b49\u524d\u6cbf\u57fa\u7840\u6a21\u578b\u548cWebGPU\u52a0\u901f\u7684Segment Anything Model (SAM)\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u6df7\u5408\u4eba\u673a\u534f\u4f5c\u6807\u6ce8\u6d41\u7a0b\uff0c\u58f0\u79f0\u53ef\u5c06\u624b\u52a8\u5de5\u4f5c\u91cf\u51cf\u5c11\u9ad8\u8fbe90%\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6807\u6ce8\u7cbe\u5ea6\u3002","title":"1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u660e\u6458\u8981 (Concise Summary)"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-05/#2-key-innovation-or-methodological-approach","text":"VisioFirm\u7684\u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u5176 \u6df7\u5408\u5f0fAI\u8f85\u52a9\u6807\u6ce8\u8303\u5f0f \uff0c\u901a\u8fc7\u667a\u80fd\u96c6\u6210\u591a\u79cd\u524d\u6cbf\u57fa\u7840\u6a21\u578b\uff0c\u751f\u6210\u521d\u59cb\u9ad8\u53ec\u56de\u7387\u7684\u4f4e\u7f6e\u4fe1\u5ea6\u9884\u6d4b\uff0c\u518d\u7531\u7528\u6237\u8fdb\u884c\u7cbe\u4fee\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\uff1a \u591a\u6a21\u578b\u878d\u5408\u7b56\u7565\uff1a \u7ed3\u5408\u4e86\u9884\u8bad\u7ec3\u68c0\u6d4b\u5668\uff08\u5982Ultralytics\u6a21\u578b\uff09\u5904\u7406\u5e38\u89c1\u7c7b\u522b\uff0c\u96f6\u6837\u672c\u6a21\u578b\uff08\u5982Grounding DINO\uff09\u5904\u7406\u81ea\u5b9a\u4e49\u6807\u7b7e\uff0c\u4ee5\u53caCLIP\u6a21\u578b\u8fdb\u884c\u8bed\u4e49\u6d88\u6b67\u548c\u8fde\u63a5\u7ec4\u4ef6\u805a\u7c7b\u3002 \u9ad8\u6548\u7684\u6d4f\u89c8\u5668\u7aef\u5206\u5272\uff1a \u5229\u7528 WebGPU\u52a0\u901f\u7684Segment Anything Model (SAM) \uff0c\u5b9e\u73b0\u4e86\u5b9e\u65f6\u3001\u9ad8\u6548\u7684\u201c\u5373\u65f6\u5206\u5272\u201d\u529f\u80fd\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7528\u6237\u4f53\u9a8c\u3002 \u7cbe\u5ea6\u7ef4\u62a4\u673a\u5236\uff1a \u5f15\u5165\u4e86 \u57fa\u4e8eCLIP\u7684\u8fde\u63a5\u7ec4\u4ef6\u805a\u7c7b\u6d88\u6b67 \u548c IoU\u56fe\u5197\u4f59\u68c0\u6d4b\u6291\u5236\u673a\u5236 \uff0c\u4ee5\u5728\u5927\u5e45\u51cf\u5c11\u4eba\u5de5\u5e72\u9884\u7684\u540c\u65f6\u786e\u4fdd\u6807\u6ce8\u7cbe\u5ea6\u3002 \u8de8\u5e73\u53f0\u4e0e\u79bb\u7ebf\u80fd\u529b\uff1a \u4f5c\u4e3aWeb\u5e94\u7528\uff0c\u652f\u6301\u591a\u79cd\u5bfc\u51fa\u683c\u5f0f\uff08YOLO, COCO, Pascal VOC, CSV\uff09\uff0c\u5e76\u5728\u6a21\u578b\u7f13\u5b58\u540e\u652f\u6301\u79bb\u7ebf\u64cd\u4f5c\uff0c\u6781\u5927\u5730\u589e\u5f3a\u4e86\u5de5\u5177\u7684\u53ef\u7528\u6027\u548c\u53ef\u8bbf\u95ee\u6027\u3002","title":"2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u8bba (Key Innovation or Methodological Approach)"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-05/#3-potential-impact-on-the-field","text":"VisioFirm\u6709\u671b\u663e\u8457 \u964d\u4f4e\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u6570\u636e\u6807\u6ce8\u7684\u95e8\u69db\u548c\u6210\u672c \uff0c\u5c24\u5176\u5bf9\u4e8e\u8d44\u6e90\u6709\u9650\u7684\u56e2\u961f\u548c\u7814\u7a76\u8005\u3002\u901a\u8fc7\u5c06\u624b\u52a8\u6807\u6ce8\u5de5\u4f5c\u91cf\u51cf\u5c11\u9ad8\u8fbe90%\uff0c\u5b83\u80fd \u6781\u5927\u52a0\u901f\u6570\u636e\u96c6\u7684\u521b\u5efa\u548c\u8fed\u4ee3\u8fc7\u7a0b \uff0c\u4ece\u800c \u63a8\u52a8AI\u6a21\u578b\u5f00\u53d1\u548c\u90e8\u7f72\u7684\u6548\u7387 \u3002\u5176\u5f00\u6e90\u548c\u8de8\u5e73\u53f0\u7684\u7279\u6027\u4e5f\u6709\u52a9\u4e8e \u4fc3\u8fdbAI\u8f85\u52a9\u6807\u6ce8\u5de5\u5177\u7684\u666e\u53ca\u548c\u6807\u51c6\u5316 \uff0c\u4f7f\u66f4\u591a\u4eba\u80fd\u591f\u5229\u7528\u5148\u8fdb\u7684AI\u80fd\u529b\u8fdb\u884c\u9ad8\u8d28\u91cf\u6570\u636e\u51c6\u5907\uff0c\u4ece\u800c\u52a0\u901f\u6574\u4e2aCV\u751f\u6001\u7cfb\u7edf\u7684\u521b\u65b0\u3002","title":"3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd (Potential Impact on the Field)"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-05/#4-related-areas-or-applications","text":"\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u5f00\u53d1\u4e0e\u90e8\u7f72\uff1a \u4efb\u4f55\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\u6765\u8bad\u7ec3\u3001\u5fae\u8c03\u6216\u8bc4\u4f30\u6a21\u578b\u7684\u573a\u666f\uff0c\u5982\u76ee\u6807\u68c0\u6d4b\u3001\u5b9e\u4f8b\u5206\u5272\u3001\u8bed\u4e49\u5206\u5272\u3001\u59ff\u6001\u4f30\u8ba1\u7b49\u3002 \u81ea\u52a8\u9a7e\u9a76\u4e0e\u673a\u5668\u4eba\uff1a \u7528\u4e8e\u6807\u6ce8\u611f\u77e5\u7cfb\u7edf\u6240\u9700\u7684\u9053\u8def\u3001\u8f66\u8f86\u3001\u884c\u4eba\u3001\u969c\u788d\u7269\u7b49\u6570\u636e\u3002 \u533b\u7597\u5f71\u50cf\u5206\u6790\uff1a \u8f85\u52a9\u533b\u751f\u6216\u7814\u7a76\u4eba\u5458\u6807\u6ce8\u75c5\u7076\u3001\u5668\u5b98\u3001\u7ec6\u80de\u7b49\u533b\u5b66\u56fe\u50cf\u3002 \u5de5\u4e1a\u8d28\u68c0\u4e0e\u5b89\u9632\u76d1\u63a7\uff1a \u5feb\u901f\u6807\u6ce8\u7f3a\u9677\u3001\u5f02\u5e38\u884c\u4e3a\u6216\u7279\u5b9a\u76ee\u6807\u3002 \u519c\u4e1a\u79d1\u6280\uff1a \u6807\u6ce8\u4f5c\u7269\u75c5\u866b\u5bb3\u3001\u679c\u5b9e\u6210\u719f\u5ea6\u3001\u519c\u7530\u533a\u57df\u7b49\u3002 \u5b66\u672f\u7814\u7a76\u4e0e\u6559\u80b2\uff1a \u4e3a\u5b66\u751f\u548c\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e00\u4e2a\u6613\u4e8e\u4f7f\u7528\u7684\u5de5\u5177\u6765\u521b\u5efa\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u3002 \u6570\u636e\u6807\u6ce8\u670d\u52a1\u63d0\u4f9b\u5546\uff1a \u63d0\u9ad8\u5176\u670d\u52a1\u6548\u7387\u548c\u964d\u4f4e\u6210\u672c\u3002","title":"4. \u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 (Related Areas or Applications)"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-05/#5-limitations-inferred-from-the-abstract","text":"\u5bf9\u4eba\u5de5\u5e72\u9884\u7684\u6301\u7eed\u4f9d\u8d56\uff1a \u5c3d\u7ba1\u58f0\u79f0\u51cf\u5c1190%\u7684\u5de5\u4f5c\u91cf\uff0c\u4f46\u201c\u521d\u59cb\u9884\u6d4b\u5927\u591a\u6b63\u786e\u201d\u548c\u201c\u7528\u6237\u53ef\u4ee5\u7ec6\u5316\u201d\u8868\u660e\u4eba\u5de5\u5ba1\u6838\u548c\u4fee\u6b63\u4ecd\u7136\u662f\u786e\u4fdd\u6700\u7ec8\u6807\u6ce8\u8d28\u91cf\u7684\u5173\u952e\u73af\u8282\uff0c\u5e76\u975e\u5b8c\u5168\u81ea\u52a8\u5316\u3002 \u5bf9\u57fa\u7840\u6a21\u578b\u7684\u6027\u80fd\u4f9d\u8d56\uff1a VisioFirm\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u5176\u96c6\u6210\u7684CLIP\u3001Grounding DINO\u3001Ultralytics\u548cSAM\u7b49\u57fa\u7840\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002\u5bf9\u4e8e\u8fd9\u4e9b\u6a21\u578b\u4e0d\u64c5\u957f\u5904\u7406\u7684\u7279\u5b9a\u9886\u57df\u3001\u9ad8\u5ea6\u62bd\u8c61\u6216\u6781\u5ea6\u7ec6\u7c92\u5ea6\u7684\u81ea\u5b9a\u4e49\u7c7b\u522b\uff0c\u5176\u8f85\u52a9\u6548\u679c\u53ef\u80fd\u4f1a\u6253\u6298\u6263\u3002 \u201cCOCO-type of classes\u201d\u7684\u6d4b\u8bd5\u8303\u56f4\uff1a \u5c3d\u7ba1\u5728COCO\u7c7b\u578b\u7c7b\u522b\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5bf9\u4e8e\u9ad8\u5ea6\u4e13\u4e1a\u5316\u3001\u957f\u5c3e\u5206\u5e03\u6216\u89c6\u89c9\u4e0a\u6a21\u7cca\u7684\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\uff0c\u5176\u521d\u59cb\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u53ec\u56de\u7387\u53ef\u80fd\u9700\u8981\u66f4\u9891\u7e41\u7684\u4eba\u5de5\u4fee\u6b63\u3002 WebGPU\u7684\u517c\u5bb9\u6027\u4e0e\u6027\u80fd\uff1a WebGPU\u7684\u52a0\u901f\u6548\u679c\u53ef\u80fd\u53d7\u9650\u4e8e\u7528\u6237\u6d4f\u89c8\u5668\u7248\u672c\u3001\u663e\u5361\u786c\u4ef6\u548c\u9a71\u52a8\u7a0b\u5e8f\uff0c\u5e76\u975e\u6240\u6709\u7528\u6237\u90fd\u80fd\u83b7\u5f97\u6700\u4f73\u7684\u6d4f\u89c8\u5668\u7aef\u6548\u7387\u3002 \u79bb\u7ebf\u80fd\u529b\u7684\u5c40\u9650\u6027\uff1a \u201c\u6a21\u578b\u7f13\u5b58\u540e\u53ef\u79bb\u7ebf\u64cd\u4f5c\u201d\u610f\u5473\u7740\u9996\u6b21\u4f7f\u7528\u6216\u6a21\u578b\u66f4\u65b0\u65f6\u4ecd\u9700\u7f51\u7edc\u8fde\u63a5\u4e0b\u8f7d\u6a21\u578b\uff0c\u4e14\u6a21\u578b\u7f13\u5b58\u53ef\u80fd\u5360\u7528\u5927\u91cf\u672c\u5730\u5b58\u50a8\u7a7a\u95f4\u3002 \u201c\u9ad8\u8fbe90%\u201d\u7684\u51cf\u5c11\u91cf\uff1a \u8fd9\u662f\u4e00\u4e2a\u4e0a\u9650\u503c\uff0c\u5b9e\u9645\u51cf\u5c11\u91cf\u53ef\u80fd\u56e0\u6570\u636e\u96c6\u7684\u590d\u6742\u6027\u3001\u6807\u6ce8\u4efb\u52a1\u7c7b\u578b\u4ee5\u53ca\u7528\u6237\u719f\u7ec3\u5ea6\u800c\u5f02\u3002 \u672a\u63d0\u53ca\u89c6\u9891\u6807\u6ce8\uff1a \u6458\u8981\u4e3b\u8981\u805a\u7126\u4e8e\u56fe\u50cf\u6807\u6ce8\uff0c\u672a\u8bf4\u660e\u5176\u5bf9\u89c6\u9891\u6807\u6ce8\u4efb\u52a1\u7684\u652f\u6301\u80fd\u529b\u3002 Key Findings: To address this, we introduce VisioFirm, an open-source web application designed to streamline image labeling through AI-assisted automation. VisioFirm integrates state-of-the-art foundation models into an interface with a filtering pipeline to reduce human-in-the-loop efforts. Links: PDF arXiv","title":"5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u7684\u5c40\u9650\u6027 (Limitations Inferred from the Abstract)"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-05/#luxdit-lighting-estimation-with-video-diffusion-transformer","text":"Authors: Ruofan Liang, Kai He, Zan Gojcic, Igor Gilitschenski, Sanja Fidler, Nandita Vijaykumar, Zian Wang Published: 2025-09-03 Categories: cs.GR, cs.AI, cs.CV Abstract: Estimating scene lighting from a single image or video remains a longstanding challenge in computer vision and graphics. Learning-based approaches are constrained by the scarcity of ground-truth HDR environment maps, which are expensive to capture and limited in diversity. While recent generative models offer strong priors for image synthesis, lighting estimation remains difficult due to its reliance on indirect visual cues, the need to infer global (non-local) context, and the recovery of high-dynamic-range outputs. We propose LuxDiT, a novel data-driven approach that fine-tunes a video diffusion transformer to generate HDR environment maps conditioned on visual input. Trained on a large synthetic dataset with diverse lighting conditions, our model learns to infer illumination from indirect visual cues and generalizes effectively to real-world scenes. To improve semantic alignment between the input and the predicted environment map, we introduce a low-rank adaptation finetuning strategy using a collected dataset of HDR panoramas. Our method produces accurate lighting predictions with realistic angular high-frequency details, outperforming existing state-of-the-art techniques in both quantitative and qualitative evaluations. Analysis: \u597d\u7684\uff0c\u4f5c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u4e13\u5bb6\uff0c\u6211\u5bf9\u8fd9\u7bc7\u5173\u4e8e LuxDiT \u7684\u8bba\u6587\u6458\u8981\u8fdb\u884c\u5982\u4e0b\u5206\u6790\uff1a","title":"LuxDiT: Lighting Estimation with Video Diffusion Transformer"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-05/#luxdit-lighting-estimation-with-video-diffusion-transformer_1","text":"1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u6d01\u603b\u7ed3 (2-3 \u53e5\u8bdd) LuxDiT \u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u901a\u8fc7\u5fae\u8c03\u89c6\u9891\u6269\u6563Transformer\u6765\u89e3\u51b3\u4ece\u56fe\u50cf\u6216\u89c6\u9891\u4f30\u8ba1\u573a\u666f\u7167\u660e\u7684\u957f\u671f\u6311\u6218\u3002\u8be5\u6a21\u578b\u5229\u7528\u5927\u89c4\u6a21\u5408\u6210\u6570\u636e\u96c6\u5b66\u4e60\u4ece\u95f4\u63a5\u89c6\u89c9\u7ebf\u7d22\u63a8\u65adHDR\u73af\u5883\u5149\u7167\uff0c\u5e76\u901a\u8fc7\u4f4e\u79e9\u9002\u5e94\u5fae\u8c03\u7b56\u7565\u589e\u5f3a\u8bed\u4e49\u5bf9\u9f50\uff0c\u6700\u7ec8\u751f\u6210\u9ad8\u7cbe\u5ea6\u3001\u7ec6\u8282\u4e30\u5bcc\u7684HDR\u73af\u5883\u56fe\uff0c\u8d85\u8d8a\u73b0\u6709SOTA\u3002 2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u8bba \u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u5c06 \u89c6\u9891\u6269\u6563Transformer \u67b6\u6784\u5e94\u7528\u4e8e HDR\u73af\u5883\u5149\u7167\u4f30\u8ba1 \u8fd9\u4e00\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u3002\u8fd9\u4e0e\u4f20\u7edf\u6269\u6563\u6a21\u578b\u4e3b\u8981\u7528\u4e8e\u56fe\u50cf\u751f\u6210\u4e0d\u540c\uff0c\u5b83\u9700\u8981\u4ece\u95f4\u63a5\u89c6\u89c9\u7ebf\u7d22\u4e2d\u63a8\u65ad\u5168\u5c40\uff08\u975e\u5c40\u90e8\uff09\u4e0a\u4e0b\u6587\u5e76\u8f93\u51fa\u9ad8\u52a8\u6001\u8303\u56f4\u6570\u636e\u3002 \u65b9\u6cd5\u8bba\u4e0a\uff0c\u5b83\u5de7\u5999\u5730\u7ed3\u5408\u4e86\uff1a * \u5927\u89c4\u6a21\u5408\u6210\u6570\u636e\u8bad\u7ec3 \uff1a\u514b\u670d\u771f\u5b9e\u4e16\u754cHDR\u73af\u5883\u56fe\u7a00\u7f3a\u6027\uff0c\u5b66\u4e60\u57fa\u7840\u5149\u7167\u63a8\u65ad\u80fd\u529b\u3002 * \u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\u5fae\u8c03\u7b56\u7565 \uff1a\u5229\u7528\u6536\u96c6\u5230\u7684\u771f\u5b9eHDR\u5168\u666f\u56fe\u6570\u636e\u96c6\uff0c\u9ad8\u6548\u5730\u6539\u5584\u6a21\u578b\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u8bed\u4e49\u5bf9\u9f50\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u540c\u65f6\u907f\u514d\u5bf9\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\u8fdb\u884c\u6602\u8d35\u7684\u5168\u9762\u5fae\u8c03\u3002 3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd \u9ad8\u8d28\u91cf\u6e32\u67d3\u4e0e\u865a\u62df\u5185\u5bb9\u521b\u4f5c \uff1a\u4e3a\u7535\u5f71\u3001\u6e38\u620f\u3001\u865a\u62df\u73b0\u5b9e/\u589e\u5f3a\u73b0\u5b9e\uff08VR/AR\uff09\u7b49\u9886\u57df\u63d0\u4f9b\u66f4\u51c6\u786e\u3001\u66f4\u771f\u5b9e\u7684\u573a\u666f\u5149\u7167\u4f30\u8ba1\uff0c\u6781\u5927\u63d0\u5347\u6e32\u67d3\u8d28\u91cf\u548c\u6c89\u6d78\u611f\u3002 \u9006\u5411\u56fe\u5f62\u5b66\uff08Inverse Graphics\uff09 \uff1a\u63a8\u52a8\u4ece2D\u56fe\u50cf\u6216\u89c6\u9891\u4e2d\u6062\u590d3D\u573a\u666f\u5c5e\u6027\uff08\u5982\u5149\u7167\uff09\u7684\u7814\u7a76\uff0c\u662f\u7406\u89e3\u4e16\u754c\u7684\u91cd\u8981\u4e00\u6b65\u3002 \u6570\u636e\u9a71\u52a8\u6a21\u578b\u8303\u5f0f \uff1a\u5c55\u793a\u4e86\u5982\u4f55\u6709\u6548\u5229\u7528\u5408\u6210\u6570\u636e\u514b\u670d\u771f\u5b9e\u6570\u636e\u7a00\u7f3a\u6027\uff0c\u5e76\u901a\u8fc7\u9ad8\u6548\u5fae\u8c03\u7b56\u7565\uff08\u5982LoRA\uff09\u5b9e\u73b0\u5411\u771f\u5b9e\u4e16\u754c\u7684\u6cdb\u5316\uff0c\u4e3a\u5176\u4ed6\u7c7b\u4f3c\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u8303\u4f8b\u3002 \u6269\u6563\u6a21\u578b\u5e94\u7528\u62d3\u5c55 \uff1a\u5c06\u6269\u6563\u6a21\u578b\u4ece\u5185\u5bb9\u751f\u6210\u6269\u5c55\u5230\u590d\u6742\u7684\u9006\u5411\u95ee\u9898\u89e3\u51b3\uff0c\u62d3\u5bbd\u4e86\u5176\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u7684\u5e94\u7528\u8fb9\u754c\u3002 4. \u53ef\u80fd\u53d7\u76ca\u4e8e\u8fd9\u9879\u7814\u7a76\u7684\u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 \u7535\u5f71\u4e0e\u6e38\u620f\u5236\u4f5c \uff1a\u5b9e\u73b0\u865a\u62df\u89d2\u8272\u4e0e\u771f\u5b9e\u573a\u666f\u7684\u65e0\u7f1d\u878d\u5408\uff0c\u6216\u5bf9\u73b0\u6709\u573a\u666f\u8fdb\u884c\u5149\u7167\u8c03\u6574\u3002 \u589e\u5f3a\u73b0\u5b9e\uff08AR\uff09\u4e0e\u865a\u62df\u73b0\u5b9e\uff08VR\uff09 \uff1a\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u51c6\u786e\u653e\u7f6e\u865a\u62df\u7269\u4f53\uff0c\u5e76\u4f7f\u5176\u5149\u7167\u4e0e\u73af\u5883\u4e00\u81f4\uff0c\u63d0\u5347\u771f\u5b9e\u611f\u548c\u6c89\u6d78\u611f\u3002 3D\u91cd\u5efa\u4e0e\u573a\u666f\u7406\u89e3 \uff1a\u5149\u7167\u662f\u573a\u666f\u51e0\u4f55\u548c\u6750\u8d28\u63a8\u65ad\u7684\u5173\u952e\u7ebf\u7d22\u3002 \u8ba1\u7b97\u6444\u5f71 \uff1a\u56fe\u50cf\u540e\u671f\u5904\u7406\u4e2d\u7684\u5149\u7167\u8c03\u6574\u3001\u98ce\u683c\u8fc1\u79fb\u7b49\u3002 \u6570\u5b57\u4eba\u4e0e\u865a\u62df\u5f62\u8c61 \uff1a\u4e3a\u6570\u5b57\u4eba\u63d0\u4f9b\u903c\u771f\u7684\u73af\u5883\u5149\u7167\uff0c\u4f7f\u5176\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u8868\u73b0\u81ea\u7136\u3002 5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u7684\u5c40\u9650\u6027 \u5408\u6210\u6570\u636e\u4e0e\u771f\u5b9e\u4e16\u754c\u5dee\u8ddd \uff1a\u5c3d\u7ba1\u4f7f\u7528\u4e86\u5927\u89c4\u6a21\u5408\u6210\u6570\u636e\u5e76\u8fdb\u884c\u4e86\u771f\u5b9e\u6570\u636e\u5fae\u8c03\uff0c\u4f46\u5408\u6210\u6570\u636e\u4e0e\u771f\u5b9e\u4e16\u754c\u4e4b\u95f4\u56fa\u6709\u7684\u9886\u57df\u5dee\u8ddd\uff08domain gap\uff09\u53ef\u80fd\u4ecd\u7136\u5b58\u5728\uff0c\u5c24\u5176\u662f\u5728\u6781\u7aef\u6216\u672a\u89c1\u8fc7\u7684\u771f\u5b9e\u5149\u7167\u6761\u4ef6\u4e0b\uff0c\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u53ef\u80fd\u53d7\u5230\u6311\u6218\u3002 \u5bf9\u95f4\u63a5\u89c6\u89c9\u7ebf\u7d22\u7684\u4f9d\u8d56 \uff1a\u5149\u7167\u4f30\u8ba1\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u573a\u666f\u4e2d\u7684\u9634\u5f71\u3001\u53cd\u5c04\u3001\u9ad8\u5149\u7b49\u95f4\u63a5\u7ebf\u7d22\u3002\u5728\u8fd9\u4e9b\u7ebf\u7d22\u4e0d\u660e\u663e\u3001\u6a21\u7cca\u6216\u88ab\u906e\u6321\u7684\u573a\u666f\u4e2d\uff0c\u6a21\u578b\u7684\u9c81\u68d2\u6027\u53ef\u80fd\u4e0b\u964d\u3002 \u5168\u5c40\u4e0a\u4e0b\u6587\u63a8\u65ad\u7684\u5c40\u9650\u6027 \uff1a\u4ece\u6709\u9650\u7684\u89c6\u89c9\u8f93\u5165\uff08\u5355\u5f20\u56fe\u50cf\u6216\u89c6\u9891\u7247\u6bb5\uff09\u63a8\u65ad\u6574\u4e2a360\u5ea6HDR\u73af\u5883\u56fe\uff0c\u672c\u8d28\u4e0a\u662f\u4e00\u4e2a\u6b20\u5b9a\u95ee\u9898\u3002\u6a21\u578b\u53ef\u80fd\u96be\u4ee5\u51c6\u786e\u6355\u6349\u5230\u8f93\u5165\u89c6\u56fe\u4e4b\u5916\u7684\u590d\u6742\u6216\u906e\u6321\u7684\u5149\u7167\u4fe1\u606f\u3002 \u8ba1\u7b97\u6210\u672c \uff1a\u89c6\u9891\u6269\u6563Transformer\u6a21\u578b\u901a\u5e38\u8ba1\u7b97\u91cf\u8f83\u5927\uff0c\u5c24\u5176\u662f\u5728\u751f\u6210\u9ad8\u5206\u8fa8\u7387HDR\u8f93\u51fa\u65f6\uff0c\u63a8\u7406\u901f\u5ea6\u548c\u8d44\u6e90\u6d88\u8017\u53ef\u80fd\u662f\u4e00\u4e2a\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u8003\u91cf\u3002 \u5fae\u8c03\u6570\u636e\u96c6\u7684\u8d28\u91cf\u4e0e\u591a\u6837\u6027 \uff1a\u867d\u7136\u63d0\u5230\u4e86\u4f7f\u7528\u201c\u6536\u96c6\u5230\u7684HDR\u5168\u666f\u56fe\u6570\u636e\u96c6\u201d\u8fdb\u884cLoRA\u5fae\u8c03\uff0c\u4f46\u8be5\u6570\u636e\u96c6\u7684\u89c4\u6a21\u3001\u591a\u6837\u6027\u548c\u4ee3\u8868\u6027\u5c06\u76f4\u63a5\u5f71\u54cd\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u7684\u6700\u7ec8\u6027\u80fd\u548c\u8bed\u4e49\u5bf9\u9f50\u6548\u679c\u3002 Key Findings: We propose LuxDiT, a novel data-driven approach that fine-tunes a video diffusion transformer to generate HDR environment maps conditioned on visual input. To improve semantic alignment between the input and the predicted environment map, we introduce a low-rank adaptation finetuning strategy using a collected dataset of HDR panoramas. Our method produces accurate lighting predictions with realistic angular high-frequency details, outperforming existing state-of-the-art techniques in both quantitative and qualitative evaluations. Links: PDF arXiv","title":"LuxDiT: Lighting Estimation with Video Diffusion Transformer \u6458\u8981\u5206\u6790"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-05/#onecat-decoder-only-auto-regressive-model-for-unified-understanding-and-generation","text":"Authors: Han Li, Xinyu Peng, Yaoming Wang, Zelin Peng, Xin Chen, Rongxiang Weng, Jingang Wang, Xunliang Cai, Wenrui Dai, Hongkai Xiong Published: 2025-09-03 Categories: cs.CV Abstract: We introduce OneCAT, a unified multimodal model that seamlessly integrates understanding, generation, and editing within a novel, pure decoder-only transformer architecture. Our framework uniquely eliminates the need for external components such as Vision Transformers (ViT) or vision tokenizer during inference, leading to significant efficiency gains, especially for high-resolution inputs. This is achieved through a modality-specific Mixture-of-Experts (MoE) structure trained with a single autoregressive (AR) objective, which also natively supports dynamic resolutions. Furthermore, we pioneer a multi-scale visual autoregressive mechanism within the Large Language Model (LLM) that drastically reduces decoding steps compared to diffusion-based methods while maintaining state-of-the-art performance. Our findings demonstrate the powerful potential of pure autoregressive modeling as a sufficient and elegant foundation for unified multimodal intelligence. As a result, OneCAT sets a new performance standard, outperforming existing open-source unified multimodal models across benchmarks for multimodal generation, editing, and understanding. Analysis: \u8fd9\u7bc7\u8bba\u6587\u7684\u6458\u8981\u5c55\u793a\u4e86\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u4e00\u4e2a\u91cd\u8981\u8fdb\u5c55\uff0c\u7279\u522b\u662f\u5728\u7edf\u4e00\u591a\u6a21\u6001\u667a\u80fd\u65b9\u9762\u3002","title":"OneCAT: Decoder-Only Auto-Regressive Model for Unified Understanding and Generation"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-05/#1-main-contribution","text":"OneCAT\u5f15\u5165\u4e86\u4e00\u4e2a\u7eaf\u89e3\u7801\u5668Transformer\u67b6\u6784\u7684\u7edf\u4e00\u591a\u6a21\u6001\u6a21\u578b\uff0c\u65e0\u7f1d\u6574\u5408\u4e86\u7406\u89e3\u3001\u751f\u6210\u548c\u7f16\u8f91\u4efb\u52a1\u3002\u5176\u6838\u5fc3\u8d21\u732e\u5728\u4e8e\u63a8\u7406\u65f6\u65e0\u9700\u5916\u90e8\u89c6\u89c9\u7ec4\u4ef6\uff08\u5982Vision Transformer\u6216\u89c6\u89c9tokenizer\uff09\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9ad8\u5206\u8fa8\u7387\u8f93\u5165\u7684\u6548\u7387\uff0c\u5e76\u901a\u8fc7\u5355\u4e00\u81ea\u56de\u5f52\u76ee\u6807\u548c\u591a\u5c3a\u5ea6\u89c6\u89c9\u81ea\u56de\u5f52\u673a\u5236\u5b9e\u73b0\u4e86\u8de8\u591a\u6a21\u6001\u57fa\u51c6\u7684SOTA\u6027\u80fd\u3002","title":"1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e (Main Contribution)"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-05/#2-key-innovation-or-methodological-approach_1","text":"\u7eaf\u89e3\u7801\u5668Transformer\u67b6\u6784 (Pure Decoder-Only Transformer Architecture): \u6452\u5f03\u4e86\u4f20\u7edf\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u6216\u5e26\u6709\u72ec\u7acb\u89c6\u89c9\u7f16\u7801\u5668\u7684\u67b6\u6784\uff0c\u4ec5\u4f7f\u7528\u4e00\u4e2a\u89e3\u7801\u5668\u6765\u5904\u7406\u6240\u6709\u6a21\u6001\u7684\u8f93\u5165\u548c\u8f93\u51fa\u3002 \u63a8\u7406\u65f6\u65e0\u9700\u5916\u90e8\u89c6\u89c9\u7ec4\u4ef6 (Elimination of External Vision Components during Inference): \u901a\u8fc7\u6a21\u6001\u7279\u5b9a\u7684\u4e13\u5bb6\u6df7\u5408 (Mixture-of-Experts, MoE) \u7ed3\u6784\uff0c\u6a21\u578b\u5728\u63a8\u7406\u65f6\u53ef\u4ee5\u76f4\u63a5\u5904\u7406\u539f\u59cb\u89c6\u89c9\u8f93\u5165\uff0c\u65e0\u9700\u9884\u5904\u7406\u7684ViT\u6216\u89c6\u89c9tokenizer\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6548\u7387\u3002 \u5355\u4e00\u81ea\u56de\u5f52 (AR) \u76ee\u6807\u8bad\u7ec3 (Single Autoregressive Objective Training): \u6574\u4e2a\u6a21\u578b\u901a\u8fc7\u4e00\u4e2a\u7edf\u4e00\u7684\u81ea\u56de\u5f52\u76ee\u6807\u8fdb\u884c\u8bad\u7ec3\uff0c\u7b80\u5316\u4e86\u8bad\u7ec3\u8303\u5f0f\uff0c\u5e76\u539f\u751f\u652f\u6301\u52a8\u6001\u5206\u8fa8\u7387\u3002 \u591a\u5c3a\u5ea6\u89c6\u89c9\u81ea\u56de\u5f52\u673a\u5236 (Multi-scale Visual Autoregressive Mechanism): \u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u5185\u90e8\u5f15\u5165\u4e86\u8fd9\u79cd\u673a\u5236\uff0c\u4e0e\u6269\u6563\u6a21\u578b\u76f8\u6bd4\uff0c\u5b83\u80fd\u5927\u5e45\u51cf\u5c11\u89e3\u7801\u6b65\u9aa4\uff0c\u540c\u65f6\u4fdd\u6301\u9886\u5148\u7684\u6027\u80fd\u3002","title":"2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5 (Key Innovation or Methodological Approach)"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-05/#3-potential-impact-on-the-field_1","text":"\u63a8\u52a8\u7edf\u4e00\u591a\u6a21\u6001\u6a21\u578b\u8303\u5f0f\u53d1\u5c55: OneCAT\u8bc1\u660e\u4e86\u7eaf\u81ea\u56de\u5f52\u5efa\u6a21\u4f5c\u4e3a\u7edf\u4e00\u591a\u6a21\u6001\u667a\u80fd\u57fa\u7840\u7684\u5f3a\u5927\u6f5c\u529b\uff0c\u53ef\u80fd\u5f15\u5bfc\u672a\u6765\u7814\u7a76\u8f6c\u5411\u66f4\u7b80\u6d01\u3001\u4f18\u96c5\u7684\u67b6\u6784\u3002 \u663e\u8457\u63d0\u5347\u591a\u6a21\u6001\u63a8\u7406\u6548\u7387: \u7279\u522b\u662f\u5bf9\u4e8e\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u548c\u89c6\u9891\u5904\u7406\uff0c\u65e0\u9700\u5916\u90e8\u89c6\u89c9\u7ec4\u4ef6\u548c\u51cf\u5c11\u89e3\u7801\u6b65\u9aa4\u7684\u7279\u6027\uff0c\u5c06\u6781\u5927\u5730\u52a0\u901f\u591a\u6a21\u6001\u5e94\u7528\u7684\u90e8\u7f72\u548c\u5b9e\u65f6\u6027\u3002 \u7b80\u5316\u6a21\u578b\u67b6\u6784\u548c\u90e8\u7f72: \u51cf\u5c11\u5bf9\u591a\u4e2a\u72ec\u7acb\u7ec4\u4ef6\u7684\u4f9d\u8d56\uff0c\u4f7f\u5f97\u591a\u6a21\u6001\u6a21\u578b\u7684\u5f00\u53d1\u3001\u8bad\u7ec3\u548c\u90e8\u7f72\u8fc7\u7a0b\u66f4\u52a0\u7b80\u5316\u548c\u9ad8\u6548\u3002 \u4e3a\u591a\u6a21\u6001\u667a\u80fd\u8bbe\u5b9a\u65b0\u6027\u80fd\u6807\u51c6: \u5728\u751f\u6210\u3001\u7f16\u8f91\u548c\u7406\u89e3\u7b49\u591a\u4e2a\u4efb\u52a1\u4e0a\u8d85\u8d8a\u73b0\u6709\u5f00\u6e90\u6a21\u578b\uff0c\u5c06\u6fc0\u52b1\u793e\u533a\u8fdb\u4e00\u6b65\u63a2\u7d22\u548c\u4f18\u5316\u7edf\u4e00\u591a\u6a21\u6001\u6a21\u578b\u3002","title":"3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd (Potential Impact on the Field)"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-05/#4-related-areas-or-applications-that-might-benefit-from-this-research","text":"\u591a\u6a21\u6001\u5185\u5bb9\u521b\u4f5c: \u6587\u672c\u5230\u56fe\u50cf/\u89c6\u9891\u751f\u6210\u3001\u56fe\u50cf\u7f16\u8f91\u3001\u98ce\u683c\u8fc1\u79fb\u3001\u521b\u610f\u8bbe\u8ba1\u5de5\u5177\u3002 \u9ad8\u7ea7\u89c6\u89c9\u7406\u89e3: \u56fe\u50cf/\u89c6\u9891\u95ee\u7b54 (VQA)\u3001\u8be6\u7ec6\u63cf\u8ff0\u751f\u6210\u3001\u573a\u666f\u7406\u89e3\u3001\u4e8b\u4ef6\u68c0\u6d4b\u3002 \u4eba\u673a\u4ea4\u4e92: \u66f4\u81ea\u7136\u3001\u9ad8\u6548\u7684\u89c6\u89c9\u4ea4\u4e92\u754c\u9762\uff0c\u4f8b\u5982\u901a\u8fc7\u6587\u672c\u6307\u4ee4\u76f4\u63a5\u7f16\u8f91\u56fe\u50cf\u6216\u751f\u6210\u89c6\u89c9\u5185\u5bb9\u3002 \u8f85\u52a9\u6280\u672f: \u4e3a\u89c6\u89c9\u969c\u788d\u4eba\u58eb\u63d0\u4f9b\u66f4\u51c6\u786e\u3001\u5b9e\u65f6\u7684\u56fe\u50cf\u548c\u89c6\u9891\u63cf\u8ff0\u3002 \u5177\u8eab\u667a\u80fd/\u673a\u5668\u4eba: \u673a\u5668\u4eba\u901a\u8fc7\u89c6\u89c9\u611f\u77e5\u73af\u5883\u3001\u7406\u89e3\u6307\u4ee4\u5e76\u751f\u6210\u76f8\u5e94\u7684\u89c6\u89c9\u53cd\u9988\u6216\u884c\u52a8\u3002 \u533b\u7597\u5f71\u50cf\u5206\u6790: \u7ed3\u5408\u6587\u672c\u62a5\u544a\u751f\u6210\u5f71\u50cf\u3001\u5bf9\u5f71\u50cf\u8fdb\u884c\u7f16\u8f91\u4ee5\u8f85\u52a9\u8bca\u65ad\u3001\u4ece\u5f71\u50cf\u4e2d\u63d0\u53d6\u5173\u952e\u4fe1\u606f\u3002","title":"4. \u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 (Related Areas or Applications that Might Benefit from this Research)"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-05/#5-limitations-that-can-be-inferred-from-the-abstract","text":"\u8bad\u7ec3\u6210\u672c: \u7edf\u4e00\u591a\u6a21\u6001\u6a21\u578b\uff0c\u7279\u522b\u662f\u7ed3\u5408MoE\u7ed3\u6784\uff0c\u901a\u5e38\u9700\u8981\u5de8\u5927\u7684\u8ba1\u7b97\u8d44\u6e90\u548c\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u3002\u6458\u8981\u4e2d\u672a\u63d0\u53ca\u8bad\u7ec3\u7684\u89c4\u6a21\u548c\u6210\u672c\uff0c\u8fd9\u53ef\u80fd\u662f\u4e00\u4e2a\u6f5c\u5728\u7684\u6311\u6218\u3002 MoE\u7684\u590d\u6742\u6027\u4e0e\u8d1f\u8f7d\u5747\u8861: \u5c3d\u7ba1\u63a8\u7406\u65f6\u6548\u7387\u9ad8\uff0c\u4f46MoE\u7ed3\u6784\u5728\u8bad\u7ec3\u548c\u7ef4\u62a4\u4e0a\u53ef\u80fd\u589e\u52a0\u590d\u6742\u6027\uff0c\u5e76\u9700\u8981\u7cbe\u7ec6\u7684\u8d1f\u8f7d\u5747\u8861\u7b56\u7565\u6765\u786e\u4fdd\u4e13\u5bb6\u7f51\u7edc\u7684\u6709\u6548\u5229\u7528\u3002 \u81ea\u56de\u5f52\u751f\u6210\u56fa\u6709\u9650\u5236: \u5c3d\u7ba1\u58f0\u79f0\u51cf\u5c11\u4e86\u89e3\u7801\u6b65\u9aa4\uff0c\u4f46\u7eaf\u81ea\u56de\u5f52\u751f\u6210\u5728\u67d0\u4e9b\u573a\u666f\u4e0b\u4ecd\u53ef\u80fd\u9762\u4e34\u751f\u6210\u901f\u5ea6\uff08\u76f8\u5bf9\u4e8e\u5b8c\u5168\u5e76\u884c\uff09\u6216\u751f\u6210\u591a\u6837\u6027\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u975e\u5e38\u957f\u7684\u5e8f\u5217\u65f6\u3002 \u591a\u5c3a\u5ea6\u673a\u5236\u7684\u6cdb\u5316\u6027: \u8fd9\u79cd\u65b0\u9896\u7684\u591a\u5c3a\u5ea6\u89c6\u89c9\u81ea\u56de\u5f52\u673a\u5236\u5728\u5904\u7406\u6781\u7aef\u590d\u6742\u6216\u7279\u5b9a\u9886\u57df\u89c6\u89c9\u6570\u636e\uff08\u5982\u533b\u5b66\u5f71\u50cf\u3001\u536b\u661f\u56fe\u50cf\uff09\u65f6\u7684\u9c81\u68d2\u6027\u4ecd\u9700\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u3002 \u5bf9\u65b0\u6a21\u6001\u7684\u6269\u5c55\u6027: \u62bd\u8c61\u4e2d\u4e3b\u8981\u63d0\u53ca\u89c6\u89c9\u548c\u6587\u672c\uff0c\u6a21\u578b\u5982\u4f55\u65e0\u7f1d\u6269\u5c55\u5230\u5176\u4ed6\u6a21\u6001\uff08\u5982\u97f3\u9891\u30013D\u6570\u636e\u3001\u89e6\u89c9\u4fe1\u606f\uff09\u53ef\u80fd\u662f\u4e00\u4e2a\u672a\u6765\u7684\u8003\u91cf\u3002 Key Findings: We introduce OneCAT, a unified multimodal model that seamlessly integrates understanding, generation, and editing within a novel, pure decoder-only transformer architecture. Furthermore, we pioneer a multi-scale visual autoregressive mechanism within the Large Language Model (LLM) that drastically reduces decoding steps compared to diffusion-based methods while maintaining state-of-the-art performance. As a result, OneCAT sets a new performance standard, outperforming existing open-source unified multimodal models across benchmarks for multimodal generation, editing, and understanding. Links: PDF arXiv","title":"5. \u53ef\u63a8\u65ad\u7684\u5c40\u9650\u6027 (Limitations that Can Be Inferred from the Abstract)"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-05/#a-generative-foundation-model-for-chest-radiography","text":"Authors: Yuanfeng Ji, Dan Lin, Xiyue Wang, Lu Zhang, Wenhui Zhou, Chongjian Ge, Ruihang Chu, Xiaoli Yang, Junhan Zhao, Junsong Chen, Xiangde Luo, Sen Yang, Jin Fang, Ping Luo, Ruijiang Li Published: 2025-09-04 Categories: cs.CV Abstract: The scarcity of well-annotated diverse medical images is a major hurdle for developing reliable AI models in healthcare. Substantial technical advances have been made in generative foundation models for natural images. Here we develop `ChexGen', a generative vision-language foundation model that introduces a unified framework for text-, mask-, and bounding box-guided synthesis of chest radiographs. Built upon the latent diffusion transformer architecture, ChexGen was pretrained on the largest curated chest X-ray dataset to date, consisting of 960,000 radiograph-report pairs. ChexGen achieves accurate synthesis of radiographs through expert evaluations and quantitative metrics. We demonstrate the utility of ChexGen for training data augmentation and supervised pretraining, which led to performance improvements across disease classification, detection, and segmentation tasks using a small fraction of training data. Further, our model enables the creation of diverse patient cohorts that enhance model fairness by detecting and mitigating demographic biases. Our study supports the transformative role of generative foundation models in building more accurate, data-efficient, and equitable medical AI systems. Analysis: \u8fd9\u7bc7\u8bba\u6587\u6458\u8981\u5c55\u793a\u4e86\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u5728\u533b\u7597AI\u5e94\u7528\u65b9\u9762\u7684\u4e00\u4e2a\u91cd\u8981\u8fdb\u5c55\u3002\u4ee5\u4e0b\u662f\u8be6\u7ec6\u5206\u6790\uff1a","title":"A Generative Foundation Model for Chest Radiography"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-05/#1-2-3","text":"\u672c\u6587\u63d0\u51fa\u4e86 ChexGen \uff0c\u4e00\u4e2a\u7528\u4e8e\u80f8\u90e8X\u5c04\u7ebf\u56fe\u50cf\u7684\u751f\u6210\u5f0f\u89c6\u89c9-\u8bed\u8a00\u57fa\u7840\u6a21\u578b\uff0c\u65e8\u5728\u89e3\u51b3\u533b\u5b66\u56fe\u50cf\u6807\u6ce8\u7a00\u7f3a\u7684\u95ee\u9898\u3002\u5b83\u57fa\u4e8e\u6f5c\u5728\u6269\u6563Transformer\u67b6\u6784\uff0c\u80fd\u591f\u5b9e\u73b0\u6587\u672c\u3001\u63a9\u7801\u548c\u8fb9\u754c\u6846\u5f15\u5bfc\u7684\u56fe\u50cf\u5408\u6210\uff0c\u5e76\u5728\u8fc4\u4eca\u4e3a\u6b62\u6700\u5927\u7684\u80f8\u90e8X\u5c04\u7ebf\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u9884\u8bad\u7ec3\u3002\u8be5\u6a21\u578b\u5728\u6570\u636e\u589e\u5f3a\u3001\u9884\u8bad\u7ec3\u4ee5\u53ca\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u548c\u6a21\u578b\u516c\u5e73\u6027\u65b9\u9762\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u9884\u793a\u7740\u751f\u6210\u5f0f\u57fa\u7840\u6a21\u578b\u5728\u6784\u5efa\u66f4\u51c6\u786e\u3001\u6570\u636e\u9ad8\u6548\u548c\u516c\u5e73\u7684\u533b\u7597AI\u7cfb\u7edf\u4e2d\u7684\u53d8\u9769\u6027\u4f5c\u7528\u3002","title":"1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u660e\u6458\u8981 (2-3\u53e5\u8bdd)"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-05/#2_1","text":"\u7edf\u4e00\u7684\u751f\u6210\u6846\u67b6\uff1a ChexGen\u7684\u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u5176\u63d0\u4f9b\u4e86\u4e00\u4e2a \u7edf\u4e00\u7684\u6846\u67b6 \uff0c\u80fd\u591f\u5b9e\u73b0 \u6587\u672c\u3001\u63a9\u7801\u548c\u8fb9\u754c\u6846\u5f15\u5bfc \u7684\u80f8\u90e8X\u5c04\u7ebf\u56fe\u50cf\u5408\u6210\u3002\u8fd9\u610f\u5473\u7740\u7528\u6237\u53ef\u4ee5\u901a\u8fc7\u591a\u79cd\u6a21\u6001\uff08\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u3001\u533a\u57df\u63a9\u7801\u6216\u8fb9\u754c\u6846\uff09\u6765\u7cbe\u786e\u63a7\u5236\u56fe\u50cf\u7684\u751f\u6210\u5185\u5bb9\u548c\u7ed3\u6784\uff0c\u8fd9\u5728\u533b\u5b66\u56fe\u50cf\u751f\u6210\u9886\u57df\u662f\u9ad8\u5ea6\u7075\u6d3b\u548c\u65b0\u9896\u7684\u3002 \u751f\u6210\u5f0f\u89c6\u89c9-\u8bed\u8a00\u57fa\u7840\u6a21\u578b\uff1a \u5c06\u201c\u751f\u6210\u5f0f\u6a21\u578b\u201d\u4e0e\u201c\u89c6\u89c9-\u8bed\u8a00\u201d\u80fd\u529b\u76f8\u7ed3\u5408\uff0c\u5e76\u5c06\u5176\u5b9a\u4f4d\u4e3a\u201c\u57fa\u7840\u6a21\u578b\u201d\uff0c\u8868\u660e\u5176\u65e8\u5728\u901a\u8fc7\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u5b66\u4e60\u901a\u7528\u8868\u793a\uff0c\u5e76\u80fd\u9002\u5e94\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\u3002 \u6f5c\u5728\u6269\u6563Transformer\u67b6\u6784\uff1a \u91c7\u7528\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u7684\u751f\u6210\u6a21\u578b\u67b6\u6784\u4e4b\u4e00\u2014\u2014\u6f5c\u5728\u6269\u6563Transformer\u3002\u8fd9\u79cd\u67b6\u6784\u4ee5\u5176\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\u751f\u6210\u80fd\u529b\u548c\u5bf9\u590d\u6742\u6570\u636e\u5206\u5e03\u7684\u5efa\u6a21\u80fd\u529b\u800c\u95fb\u540d\uff0c\u5c06\u5176\u5e94\u7528\u4e8e\u533b\u5b66\u5f71\u50cf\u9886\u57df\u662f\u524d\u6cbf\u5b9e\u8df5\u3002 \u5927\u89c4\u6a21\u533b\u5b66\u6570\u636e\u96c6\u9884\u8bad\u7ec3\uff1a \u5728\u5305\u542b960,000\u5bf9\u653e\u5c04\u56fe\u50cf-\u62a5\u544a\u7684\u201c\u8fc4\u4eca\u4e3a\u6b62\u6700\u5927\u7684\u201d\u80f8\u90e8X\u5c04\u7ebf\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u8fd9\u4e3a\u6a21\u578b\u5b66\u4e60\u5230\u4e30\u5bcc\u7684\u533b\u5b66\u77e5\u8bc6\u548c\u56fe\u50cf\u7279\u5f81\u63d0\u4f9b\u4e86\u575a\u5b9e\u57fa\u7840\uff0c\u662f\u5b9e\u73b0\u5176\u201c\u57fa\u7840\u6a21\u578b\u201d\u80fd\u529b\u7684\u5173\u952e\u3002","title":"2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u5b66\u65b9\u6cd5"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-05/#3_1","text":"\u7f13\u89e3\u533b\u5b66\u6570\u636e\u7a00\u7f3a\u6027\uff1a \u8fd9\u662f\u6700\u76f4\u63a5\u548c\u663e\u8457\u7684\u5f71\u54cd\u3002\u901a\u8fc7\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u591a\u6837\u5316\u7684\u5408\u6210\u533b\u5b66\u56fe\u50cf\uff0cChexGen\u80fd\u591f\u6709\u6548\u8865\u5145\u771f\u5b9e\u6807\u6ce8\u6570\u636e\u7684\u4e0d\u8db3\uff0c\u6781\u5927\u5730\u964d\u4f4e\u4e86\u5f00\u53d1\u548c\u8bad\u7ec3\u9ad8\u6027\u80fd\u533b\u7597AI\u6a21\u578b\u7684\u95e8\u69db\u3002 \u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u548c\u6570\u636e\u6548\u7387\uff1a \u4f5c\u4e3a\u6570\u636e\u589e\u5f3a\u5de5\u5177\u548c\u9884\u8bad\u7ec3\u7b56\u7565\uff0cChexGen\u80fd\u591f\u663e\u8457\u63d0\u5347\u75be\u75c5\u5206\u7c7b\u3001\u68c0\u6d4b\u548c\u5206\u5272\u7b49\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u4ec5\u6709\u5c11\u91cf\u771f\u5b9e\u8bad\u7ec3\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u8fd9\u5bf9\u4e8e\u5feb\u901f\u8fed\u4ee3\u548c\u90e8\u7f72\u533b\u7597AI\u6a21\u578b\u81f3\u5173\u91cd\u8981\u3002 \u4fc3\u8fdb\u6a21\u578b\u516c\u5e73\u6027\u4e0e\u504f\u89c1\u7f13\u89e3\uff1a \u80fd\u591f\u521b\u5efa\u591a\u6837\u5316\u7684\u60a3\u8005\u961f\u5217\uff0c\u7528\u4e8e\u68c0\u6d4b\u548c\u7f13\u89e3AI\u6a21\u578b\u4e2d\u7684\u4eba\u53e3\u7edf\u8ba1\u5b66\u504f\u89c1\uff0c\u8fd9\u5728\u533b\u7597AI\u9886\u57df\u5177\u6709\u6df1\u8fdc\u7684\u793e\u4f1a\u548c\u4f26\u7406\u610f\u4e49\u3002\u5b83\u4e3a\u6784\u5efa\u66f4\u516c\u5e73\u3001\u66f4\u503c\u5f97\u4fe1\u8d56\u7684\u533b\u7597AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u5de5\u5177\u3002 \u63a8\u52a8\u533b\u5b66AI\u57fa\u7840\u6a21\u578b\u53d1\u5c55\uff1a ChexGen\u7684\u6210\u529f\u5c06\u6fc0\u52b1\u66f4\u591a\u7814\u7a76\u8005\u63a2\u7d22\u5728\u5176\u4ed6\u533b\u5b66\u5f71\u50cf\u6a21\u6001\uff08\u5982CT\u3001MRI\uff09\u548c\u75be\u75c5\u9886\u57df\u5f00\u53d1\u7c7b\u4f3c\u7684\u751f\u6210\u5f0f\u57fa\u7840\u6a21\u578b\uff0c\u52a0\u901f\u6574\u4e2a\u533b\u5b66AI\u9886\u57df\u7684\u53d1\u5c55\u3002","title":"3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-05/#4_1","text":"\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e0e\u8bca\u65ad\uff1a \u76f4\u63a5\u5e94\u7528\u4e8e\u75be\u75c5\u5206\u7c7b\u3001\u68c0\u6d4b\u548c\u5206\u5272\uff0c\u8f85\u52a9\u533b\u751f\u8fdb\u884c\u8bca\u65ad\u3002 \u533b\u7597AI\u6a21\u578b\u5f00\u53d1\u4e0e\u90e8\u7f72\uff1a \u4f5c\u4e3a\u6570\u636e\u589e\u5f3a\u5de5\u5177\uff0c\u52a0\u901f\u65b0\u6a21\u578b\u7684\u8bad\u7ec3\u548c\u8fed\u4ee3\uff1b\u4f5c\u4e3a\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u63d0\u5347\u6a21\u578b\u5728\u5c0f\u6837\u672c\u6570\u636e\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u3002 \u6a21\u578b\u516c\u5e73\u6027\u4e0e\u504f\u89c1\u7f13\u89e3\u7814\u7a76\uff1a \u7528\u4e8e\u751f\u6210\u5177\u6709\u7279\u5b9a\u4eba\u53e3\u7edf\u8ba1\u5b66\u7279\u5f81\u7684\u5408\u6210\u6570\u636e\uff0c\u4ee5\u8bc6\u522b\u3001\u91cf\u5316\u548c\u7f13\u89e3AI\u6a21\u578b\u4e2d\u7684\u504f\u89c1\u3002 \u533b\u7597\u6559\u80b2\u4e0e\u6a21\u62df\uff1a \u751f\u6210\u5404\u79cd\u75c5\u7406\u56fe\u50cf\u7528\u4e8e\u6559\u5b66\u548c\u533b\u751f\u57f9\u8bad\uff0c\u63d0\u4f9b\u591a\u6837\u5316\u7684\u5b66\u4e60\u6848\u4f8b\u3002 \u9690\u79c1\u4fdd\u62a4\u6570\u636e\u5171\u4eab\u4e0e\u7814\u7a76\uff1a \u5728\u67d0\u4e9b\u573a\u666f\u4e0b\uff0c\u5408\u6210\u6570\u636e\u53ef\u4ee5\u4f5c\u4e3a\u771f\u5b9e\u6570\u636e\u7684\u66ff\u4ee3\u54c1\uff0c\u7528\u4e8e\u7814\u7a76\u548c\u5f00\u53d1\uff0c\u540c\u65f6\u4fdd\u62a4\u60a3\u8005\u9690\u79c1\u3002 \u4e2a\u6027\u5316\u533b\u7597\uff1a \u7406\u8bba\u4e0a\uff0c\u672a\u6765\u53ef\u4ee5\u6839\u636e\u7279\u5b9a\u60a3\u8005\u7684\u7279\u5f81\u751f\u6210\u5b9a\u5236\u5316\u7684\u6a21\u62df\u56fe\u50cf\uff0c\u7528\u4e8e\u6cbb\u7597\u65b9\u6848\u7684\u89c4\u5212\u548c\u8bc4\u4f30\u3002","title":"4. \u53ef\u80fd\u53d7\u76ca\u7684\u76f8\u5173\u9886\u57df\u6216\u5e94\u7528"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-05/#5_1","text":"\u5408\u6210\u56fe\u50cf\u7684\u4e34\u5e8a\u771f\u5b9e\u6027\u4e0e\u7ec6\u8282\uff1a \u5c3d\u7ba1\u6458\u8981\u58f0\u79f0\u201c\u51c6\u786e\u5408\u6210\u201d\uff0c\u4f46\u5bf9\u4e8e\u751f\u6210\u56fe\u50cf\u5728\u4e34\u5e8a\u4e0a\u662f\u5426\u80fd\u5b8c\u5168\u6a21\u62df\u771f\u5b9e\u75c5\u7406\u7684\u7ec6\u5fae\u7279\u5f81\u3001\u7f55\u89c1\u75c5\u53d8\u6216\u590d\u6742\u75c5\u7406\u5171\u5b58\u7684\u60c5\u51b5\uff0c\u4ecd\u9700\u66f4\u6df1\u5165\u7684\u9a8c\u8bc1\u3002\u751f\u6210\u6a21\u578b\u53ef\u80fd\u5b58\u5728\u201c\u5e7b\u89c9\u201d\u6216\u751f\u6210\u4e34\u5e8a\u4e0a\u4e0d\u5408\u7406\u7684\u7279\u5f81\u7684\u98ce\u9669\u3002 \u6570\u636e\u96c6\u7684\u8986\u76d6\u8303\u56f4\u4e0e\u591a\u6837\u6027\uff1a \u5c3d\u7ba1\u4f7f\u7528\u4e86\u201c\u8fc4\u4eca\u4e3a\u6b62\u6700\u5927\u7684\u201d\u6570\u636e\u96c6\uff0c\u4f46\u533b\u5b66\u5f71\u50cf\u7684\u590d\u6742\u6027\u548c\u591a\u6837\u6027\u662f\u65e0\u9650\u7684\u3002\u8be5\u6a21\u578b\u53ef\u80fd\u5728\u8bad\u7ec3\u6570\u636e\u4e2d\u672a\u5145\u5206\u4ee3\u8868\u7684\u7f55\u89c1\u75be\u75c5\u3001\u7279\u5b9a\u4eba\u7fa4\u6216\u590d\u6742\u75c5\u7406\u6a21\u5f0f\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002 \u504f\u89c1\u7f13\u89e3\u7684\u5b9e\u9645\u6548\u679c\uff1a \u6458\u8981\u63d0\u5230\u80fd\u591f\u68c0\u6d4b\u548c\u7f13\u89e3\u4eba\u53e3\u7edf\u8ba1\u5b66\u504f\u89c1\uff0c\u4f46\u672a\u8be6\u7ec6\u8bf4\u660e\u5176\u6548\u679c\u7684\u91cf\u5316\u8bc4\u4f30\u548c\u5c40\u9650\u6027\u3002\u751f\u6210\u591a\u6837\u5316\u961f\u5217\u662f\u5426\u80fd\u5b8c\u5168\u6d88\u9664\u6240\u6709\u6f5c\u5728\u504f\u89c1\uff0c\u4ee5\u53ca\u662f\u5426\u4f1a\u5f15\u5165\u65b0\u7684\u5408\u6210\u504f\u89c1\uff0c\u4ecd\u662f\u5f00\u653e\u95ee\u9898\u3002 \u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\uff1a \u57fa\u7840\u6a21\u578b\u901a\u5e38\u9700\u8981\u5927\u91cf\u7684\u8ba1\u7b97\u8d44\u6e90\u8fdb\u884c\u8bad\u7ec3\u548c\u63a8\u7406\uff0c\u8fd9\u53ef\u80fd\u9650\u5236\u5176\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u3002 \u5bf9\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u63d0\u5347\u7684\u7a0b\u5ea6\uff1a \u6458\u8981\u6307\u51fa\u201c\u4f7f\u7528\u4e00\u5c0f\u90e8\u5206\u8bad\u7ec3\u6570\u636e\u201d\u5373\u53ef\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u672a\u660e\u786e\u8fd9\u79cd\u63d0\u5347\u662f\u5426\u80fd\u8fbe\u5230\u6216\u8d85\u8d8a\u4f7f\u7528\u5b8c\u6574\u771f\u5b9e\u6570\u636e\u96c6\u8bad\u7ec3\u7684\u6a21\u578b\u6027\u80fd\uff0c\u4ee5\u53ca\u201c\u4e00\u5c0f\u90e8\u5206\u201d\u7684\u5177\u4f53\u91cf\u5316\u6807\u51c6\u3002 Key Findings: We demonstrate the utility of ChexGen for training data augmentation and supervised pretraining, which led to performance improvements across disease classification, detection, and segmentation tasks using a small fraction of training data. Links: PDF arXiv","title":"5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u7684\u5c40\u9650\u6027"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-05/#ovgrasp-open-vocabulary-grasping-assistance-via-multimodal-intent-detection","text":"Authors: Chen Hu, Shan Luo, Letizia Gionfrida Published: 2025-09-04 Categories: cs.RO, cs.CV Abstract: Grasping assistance is essential for restoring autonomy in individuals with motor impairments, particularly in unstructured environments where object categories and user intentions are diverse and unpredictable. We present OVGrasp, a hierarchical control framework for soft exoskeleton-based grasp assistance that integrates RGB-D vision, open-vocabulary prompts, and voice commands to enable robust multimodal interaction. To enhance generalization in open environments, OVGrasp incorporates a vision-language foundation model with an open-vocabulary mechanism, allowing zero-shot detection of previously unseen objects without retraining. A multimodal decision-maker further fuses spatial and linguistic cues to infer user intent, such as grasp or release, in multi-object scenarios. We deploy the complete framework on a custom egocentric-view wearable exoskeleton and conduct systematic evaluations on 15 objects across three grasp types. Experimental results with ten participants demonstrate that OVGrasp achieves a grasping ability score (GAS) of 87.00%, outperforming state-of-the-art baselines and achieving improved kinematic alignment with natural hand motion. Analysis: \u4f5c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u4e13\u5bb6\uff0c\u6211\u5bf9\u8fd9\u7bc7\u8bba\u6587\u6458\u8981\u7684\u5206\u6790\u5982\u4e0b\uff1a","title":"OVGrasp: Open-Vocabulary Grasping Assistance via Multimodal Intent Detection"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-05/#ovgrasp-open-vocabulary-grasping-assistance-via-multimodal-intent-detection_1","text":"1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u660e\u6458\u8981 (Concise Summary) OVGrasp \u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u8f6f\u4f53\u5916\u9aa8\u9abc\u7684\u5c42\u7ea7\u63a7\u5236\u6846\u67b6\uff0c\u65e8\u5728\u4e3a\u8fd0\u52a8\u969c\u788d\u8005\u63d0\u4f9b\u5f00\u653e\u8bcd\u6c47\u6293\u53d6\u8f85\u52a9\u3002\u5b83\u901a\u8fc7\u6574\u5408 RGB-D \u89c6\u89c9\u3001\u5f00\u653e\u8bcd\u6c47\u63d0\u793a\u548c\u8bed\u97f3\u547d\u4ee4\uff0c\u5229\u7528\u89c6\u89c9-\u8bed\u8a00\u57fa\u7840\u6a21\u578b\u5b9e\u73b0\u5bf9\u672a\u77e5\u7269\u4f53\u7684\u96f6\u6837\u672c\u68c0\u6d4b\uff0c\u5e76\u901a\u8fc7\u591a\u6a21\u6001\u51b3\u7b56\u5668\u63a8\u65ad\u7528\u6237\u5728\u591a\u7269\u4f53\u573a\u666f\u4e2d\u7684\u6293\u53d6\u6216\u91ca\u653e\u610f\u56fe\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u5728\u6293\u53d6\u80fd\u529b\u548c\u8fd0\u52a8\u5bf9\u9f50\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002 2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u5b66\u65b9\u6cd5 (Key Innovation or Methodological Approach) \u8be5\u8bba\u6587\u7684\u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u5176 \u5f00\u653e\u8bcd\u6c47\uff08Open-Vocabulary\uff09\u80fd\u529b \u548c \u591a\u6a21\u6001\u610f\u56fe\u68c0\u6d4b \u3002 1. \u5f00\u653e\u8bcd\u6c47\u673a\u5236\uff1a OVGrasp \u96c6\u6210\u4e86\u4e00\u4e2a\u89c6\u89c9-\u8bed\u8a00\u57fa\u7840\u6a21\u578b\uff0c\u4f7f\u5176\u80fd\u591f\u5bf9\u5148\u524d\u672a\u89c1\u7684\u7269\u4f53\u8fdb\u884c\u96f6\u6837\u672c\u68c0\u6d4b\uff0c\u800c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u3002\u8fd9\u6781\u5927\u5730\u589e\u5f3a\u4e86\u7cfb\u7edf\u5728\u975e\u7ed3\u6784\u5316\u3001\u591a\u6837\u5316\u73af\u5883\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u7684\u4e00\u4e2a\u91cd\u8981\u8fdb\u5c55\uff0c\u5c24\u5176\u662f\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u3002 2. \u591a\u6a21\u6001\u610f\u56fe\u51b3\u7b56\u5668\uff1a \u7cfb\u7edf\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u591a\u6a21\u6001\u51b3\u7b56\u5668\uff0c\u80fd\u591f\u878d\u5408\u6765\u81ea RGB-D \u89c6\u89c9\u7684\u7a7a\u95f4\u7ebf\u7d22\u548c\u6765\u81ea\u5f00\u653e\u8bcd\u6c47\u63d0\u793a/\u8bed\u97f3\u547d\u4ee4\u7684\u8bed\u8a00\u7ebf\u7d22\uff0c\u4ece\u800c\u5728\u591a\u7269\u4f53\u573a\u666f\u4e2d\u51c6\u786e\u63a8\u65ad\u7528\u6237\u7684\u5177\u4f53\u610f\u56fe\uff08\u5982\u6293\u53d6\u6216\u91ca\u653e\uff09\u3002\u8fd9\u79cd\u5bf9\u590d\u6742\u7528\u6237\u610f\u56fe\u7684\u7406\u89e3\uff0c\u8d85\u8d8a\u4e86\u7b80\u5355\u7684\u7269\u4f53\u8bc6\u522b\uff0c\u662f\u4eba\u673a\u4ea4\u4e92\u548c\u8f85\u52a9\u673a\u5668\u4eba\u9886\u57df\u7684\u5173\u952e\u7a81\u7834\u3002 3. \u96c6\u6210\u6846\u67b6\uff1a \u5c06\u8fd9\u4e9b\u5148\u8fdb\u7684 CV/NLP \u6280\u672f\u4e0e\u5b9a\u5236\u7684\u3001\u4f69\u6234\u5f0f\u3001\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u7684\u8f6f\u4f53\u5916\u9aa8\u9abc\u76f8\u7ed3\u5408\uff0c\u5f62\u6210\u4e00\u4e2a\u5b9e\u7528\u7684\u3001\u7aef\u5230\u7aef\u7684\u5c42\u7ea7\u63a7\u5236\u7cfb\u7edf\uff0c\u5b9e\u73b0\u4e86\u4ece\u611f\u77e5\u5230\u51b3\u7b56\u518d\u5230\u6267\u884c\u7684\u95ed\u73af\u3002 3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd (Potential Impact on the Field) \u8f85\u52a9\u673a\u5668\u4eba\u4e0e\u4eba\u673a\u4ea4\u4e92 (Assistive Robotics & HRI)\uff1a OVGrasp \u76f4\u63a5\u89e3\u51b3\u4e86\u8fd0\u52a8\u969c\u788d\u8005\u5728\u65e5\u5e38\u751f\u6d3b\u4e2d\u6062\u590d\u81ea\u4e3b\u6027\u7684\u5173\u952e\u6311\u6218\uff0c\u901a\u8fc7\u63d0\u4f9b\u66f4\u81ea\u7136\u3001\u76f4\u89c2\u4e14\u9002\u5e94\u6027\u5f3a\u7684\u6293\u53d6\u8f85\u52a9\uff0c\u5c06\u6781\u5927\u5730\u6539\u5584\u4ed6\u4eec\u7684\u751f\u6d3b\u8d28\u91cf\u3002\u5b83\u4e3a\u672a\u6765\u8f85\u52a9\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u7684\u8303\u5f0f\u3002 \u8ba1\u7b97\u673a\u89c6\u89c9\u4e0e\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b (Computer Vision & Vision-Language Models)\uff1a \u8be5\u7814\u7a76\u5c55\u793a\u4e86\u89c6\u89c9-\u8bed\u8a00\u57fa\u7840\u6a21\u578b\u5728\u5b9e\u9645\u3001\u5177\u8eab\uff08embodied\uff09\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u7684\u5f3a\u5927\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u5f00\u653e\u4e16\u754c\u3001\u96f6\u6837\u672c\u7269\u4f53\u8bc6\u522b\u548c\u7406\u89e3\u65b9\u9762\u3002\u5b83\u63a8\u52a8\u4e86 CV \u9886\u57df\u4ece\u9759\u6001\u56fe\u50cf\u8bc6\u522b\u5411\u52a8\u6001\u3001\u4ea4\u4e92\u5f0f\u3001\u591a\u6a21\u6001\u611f\u77e5\u7684\u6f14\u8fdb\u3002 \u591a\u6a21\u6001\u4eba\u5de5\u667a\u80fd (Multimodal AI)\uff1a \u8bba\u6587\u5728\u878d\u5408\u7a7a\u95f4\u548c\u8bed\u8a00\u4fe1\u606f\u4ee5\u63a8\u65ad\u590d\u6742\u7528\u6237\u610f\u56fe\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4e3a\u591a\u6a21\u6001\u5b66\u4e60\u548c\u51b3\u7b56\u5236\u5b9a\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u548c\u5b9e\u8bc1\u3002 \u8f6f\u4f53\u673a\u5668\u4eba\u4e0e\u53ef\u7a7f\u6234\u8bbe\u5907 (Soft Robotics & Wearable Devices)\uff1a \u7ed3\u5408\u8f6f\u4f53\u5916\u9aa8\u9abc\u548c\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u89c6\u89c9\uff0c\u4e3a\u53ef\u7a7f\u6234\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u8bbe\u8ba1\u548c\u63a7\u5236\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u7ecf\u9a8c\u548c\u6280\u672f\u53c2\u8003\u3002 4. \u53ef\u80fd\u53d7\u76ca\u4e8e\u6b64\u7814\u7a76\u7684\u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 (Related Areas or Applications) \u901a\u7528\u578b\u673a\u5668\u4eba\u64cd\u4f5c (General-purpose Robotic Manipulation)\uff1a \u63d0\u5347\u673a\u5668\u4eba\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u5904\u7406\u591a\u6837\u5316\u7269\u4f53\u7684\u80fd\u529b\uff0c\u4f8b\u5982\u5728\u7269\u6d41\u3001\u4ed3\u50a8\u6216\u670d\u52a1\u673a\u5668\u4eba\u9886\u57df\u3002 \u4eba\u673a\u534f\u4f5c (Human-Robot Collaboration)\uff1a \u6539\u8fdb\u673a\u5668\u4eba\u5bf9\u4eba\u7c7b\u6307\u4ee4\u548c\u610f\u56fe\u7684\u7406\u89e3\uff0c\u5b9e\u73b0\u66f4\u6d41\u7545\u3001\u66f4\u5b89\u5168\u7684\u534f\u4f5c\uff0c\u5c24\u5176\u662f\u5728\u5de5\u4e1a\u6216\u533b\u7597\u573a\u666f\u4e2d\u3002 \u8fdc\u7a0b\u64cd\u4f5c\u4e0e\u63a2\u7d22 (Teleoperation & Exploration)\uff1a \u4e3a\u8fdc\u7a0b\u63a7\u5236\u7cfb\u7edf\u63d0\u4f9b\u66f4\u667a\u80fd\u7684\u7269\u4f53\u8bc6\u522b\u548c\u610f\u56fe\u63a8\u65ad\u80fd\u529b\uff0c\u51cf\u5c11\u64cd\u4f5c\u5458\u7684\u8ba4\u77e5\u8d1f\u62c5\u3002 \u667a\u80fd\u5bb6\u5c45\u4e0e\u667a\u6167\u533b\u7597 (Smart Home & Smart Healthcare)\uff1a \u5c06\u5f00\u653e\u8bcd\u6c47\u548c\u591a\u6a21\u6001\u4ea4\u4e92\u80fd\u529b\u96c6\u6210\u5230\u5176\u4ed6\u667a\u80fd\u8bbe\u5907\u4e2d\uff0c\u63d0\u4f9b\u66f4\u5e7f\u6cdb\u7684\u667a\u80fd\u8f85\u52a9\u670d\u52a1\u3002 \u589e\u5f3a\u73b0\u5b9e/\u865a\u62df\u73b0\u5b9e (Augmented Reality/Virtual Reality)\uff1a \u5728 AR/VR \u73af\u5883\u4e2d\u5b9e\u73b0\u66f4\u81ea\u7136\u7684\u7269\u4f53\u4ea4\u4e92\u548c\u7528\u6237\u610f\u56fe\u7406\u89e3\u3002 5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u51fa\u7684\u5c40\u9650\u6027 (Limitations that can be inferred from the abstract) \u8bc4\u4f30\u8303\u56f4\u7684\u5c40\u9650\u6027 (Limited Evaluation Scope): \u5c3d\u7ba1\u572815\u4e2a\u7269\u4f53\u548c3\u79cd\u6293\u53d6\u7c7b\u578b\u4e0a\u8fdb\u884c\u4e86\u7cfb\u7edf\u8bc4\u4f30\uff0c\u5e76\u670910\u540d\u53c2\u4e0e\u8005\uff0c\u4f46\u4e0e\u201c\u591a\u6837\u5316\u4e14\u4e0d\u53ef\u9884\u6d4b\u201d\u7684\u771f\u5b9e\u4e16\u754c\u975e\u7ed3\u6784\u5316\u73af\u5883\u76f8\u6bd4\uff0c\u8fd9\u4ecd\u7136\u662f\u4e00\u4e2a\u76f8\u5bf9\u6709\u9650\u7684\u6d4b\u8bd5\u96c6\u3002\u5f00\u653e\u8bcd\u6c47\u80fd\u529b\u867d\u5f3a\uff0c\u4f46\u5176\u5728\u6781\u7aef\u591a\u6837\u6027\u3001\u7f55\u89c1\u7269\u4f53\u6216\u9ad8\u5ea6\u76f8\u4f3c\u7269\u4f53\u533a\u5206\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u4ecd\u9700\u66f4\u5e7f\u6cdb\u3001\u66f4\u4e25\u82db\u7684\u9a8c\u8bc1\u3002 \u610f\u56fe\u8bc6\u522b\u7684\u590d\u6742\u6027 (Complexity of Intent Recognition): \u76ee\u524d\u7684\u610f\u56fe\u8bc6\u522b\u4e3b\u8981\u96c6\u4e2d\u5728\u201c\u6293\u53d6\u201d\u6216\u201c\u91ca\u653e\u201d\u4e24\u79cd\u57fa\u672c\u52a8\u4f5c\u3002\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u7528\u6237\u7684\u610f\u56fe\u53ef\u80fd\u66f4\u4e3a\u590d\u6742\u548c\u7ec6\u81f4\uff08\u4f8b\u5982\uff0c\u201c\u8f7b\u8f7b\u6293\u53d6\u201d\u3001\u201c\u79fb\u52a8\u5230\u67d0\u4e2a\u4f4d\u7f6e\u201d\u3001\u201c\u6293\u53d6\u7ea2\u8272\u7684\u90a3\u4e2a\u201d\uff09\uff0c\u8fd9\u53ef\u80fd\u9700\u8981\u66f4\u9ad8\u7ea7\u3001\u66f4\u5177\u4e0a\u4e0b\u6587\u611f\u77e5\u80fd\u529b\u7684\u610f\u56fe\u7406\u89e3\u6a21\u578b\u3002 \u73af\u5883\u9c81\u68d2\u6027 (Environmental Robustness): \u6458\u8981\u672a\u8be6\u7ec6\u8bf4\u660e\u7cfb\u7edf\u5728\u6781\u7aef\u5149\u7167\u53d8\u5316\u3001\u4e25\u91cd\u906e\u6321\u3001\u9ad8\u5ea6\u6742\u4e71\u3001\u5305\u542b\u900f\u660e/\u53cd\u5149\u7269\u4f53\u7b49\u590d\u6742\u771f\u5b9e\u4e16\u754c\u6761\u4ef6\u4e0b\u7684\u8868\u73b0\u3002\u8fd9\u4e9b\u662f RGB-D \u89c6\u89c9\u548c\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u5e38\u89c1\u7684\u6311\u6218\u3002 \u8ba1\u7b97\u8d44\u6e90\u4e0e\u5b9e\u65f6\u6027 (Computational Resources and Real-time Performance): \u89c6\u89c9-\u8bed\u8a00\u57fa\u7840\u6a21\u578b\u901a\u5e38\u8ba1\u7b97\u91cf\u5927\u3002\u5bf9\u4e8e\u4e00\u4e2a\u53ef\u7a7f\u6234\u3001\u5b9e\u65f6\u54cd\u5e94\u7684\u7cfb\u7edf\uff0c\u5176\u8ba1\u7b97\u5f00\u9500\u3001\u529f\u8017\u548c\u5ef6\u8fdf\u662f\u5173\u952e\u8003\u91cf\uff0c\u6458\u8981\u4e2d\u672a\u63d0\u53ca\u8fd9\u4e9b\u6027\u80fd\u6307\u6807\u3002 \u7528\u6237\u9002\u5e94\u6027\u4e0e\u4e2a\u6027\u5316 (User Adaptability & Personalization): \u6458\u8981\u63d0\u5230\u201c\u6539\u8fdb\u4e86\u4e0e\u81ea\u7136\u624b\u90e8\u8fd0\u52a8\u7684\u8fd0\u52a8\u5bf9\u9f50\u201d\uff0c\u4f46\u672a\u8be6\u7ec6\u8bf4\u660e\u7cfb\u7edf\u5982\u4f55\u9002\u5e94\u4e0d\u540c\u7528\u6237\u7684\u751f\u7406\u5dee\u5f02\u3001\u504f\u597d\u6216\u5b66\u4e60\u66f2\u7ebf\u3002\u4e2a\u6027\u5316\u8c03\u6574\u5bf9\u4e8e\u8f85\u52a9\u8bbe\u5907\u81f3\u5173\u91cd\u8981\u3002 Key Findings: We present OVGrasp, a hierarchical control framework for soft exoskeleton-based grasp assistance that integrates RGB-D vision, open-vocabulary prompts, and voice commands to enable robust multimodal interaction. Experimental results with ten participants demonstrate that OVGrasp achieves a grasping ability score (GAS) of 87.00%, outperforming state-of-the-art baselines and achieving improved kinematic alignment with natural hand motion. Links: PDF arXiv","title":"OVGrasp: Open-Vocabulary Grasping Assistance via Multimodal Intent Detection"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-05/#enhancing-robustness-in-post-processing-watermarking-an-ensemble-attack-network-using-cnns-and-transformers","text":"Authors: Tzuhsuan Huang, Cheng Yu Yeo, Tsai-Ling Huang, Hong-Han Shuai, Wen-Huang Cheng, Jun-Cheng Chen Published: 2025-09-03 Categories: cs.CV Abstract: Recent studies on deep watermarking have predominantly focused on in-processing watermarking, which integrates the watermarking process into image generation. However, post-processing watermarking, which embeds watermarks after image generation, offers more flexibility. It can be applied to outputs from any generative model (e.g. GANs, diffusion models) without needing access to the model's internal structure. It also allows users to embed unique watermarks into individual images. Therefore, this study focuses on post-processing watermarking and enhances its robustness by incorporating an ensemble attack network during training. We construct various versions of attack networks using CNN and Transformer in both spatial and frequency domains to investigate how each combination influences the robustness of the watermarking model. Our results demonstrate that combining a CNN-based attack network in the spatial domain with a Transformer-based attack network in the frequency domain yields the highest robustness in watermarking models. Extensive evaluation on the WAVES benchmark, using average bit accuracy as the metric, demonstrates that our ensemble attack network significantly enhances the robustness of baseline watermarking methods under various stress tests. In particular, for the Regeneration Attack defined in WAVES, our method improves StegaStamp by 18.743%. The code is released at:https://github.com/aiiu-lab/DeepRobustWatermark. Analysis: \u8fd9\u7bc7\u8bba\u6587\u6df1\u5165\u63a2\u8ba8\u4e86\u540e\u5904\u7406\u6570\u5b57\u6c34\u5370\u7684\u9c81\u68d2\u6027\u589e\u5f3a\u95ee\u9898\uff0c\u4ee5\u4e0b\u662f\u8be6\u7ec6\u5206\u6790\uff1a","title":"Enhancing Robustness in Post-Processing Watermarking: An Ensemble Attack Network Using CNNs and Transformers"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-05/#1-concise-summary_1","text":"\u672c\u6587\u4e13\u6ce8\u4e8e\u63d0\u5347\u540e\u5904\u7406\u6570\u5b57\u6c34\u5370\u7684\u9c81\u68d2\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u8bad\u7ec3\u9636\u6bb5\u5229\u7528\u96c6\u6210\u653b\u51fb\u7f51\u7edc\u7684\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u57fa\u4e8eCNN\u7684\u7a7a\u95f4\u57df\u653b\u51fb\u548c\u57fa\u4e8eTransformer\u7684\u9891\u7387\u57df\u653b\u51fb\uff0c\u5b9e\u9a8c\u8bc1\u660e\u8fd9\u79cd\u7ec4\u5408\u80fd\u663e\u8457\u589e\u5f3a\u6c34\u5370\u6a21\u578b\u7684\u6297\u653b\u51fb\u80fd\u529b\uff0c\u5e76\u5728WAVES\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u73b0\u51fa\u4f18\u5f02\u6027\u80fd\uff0c\u5c24\u5176\u5728\u518d\u751f\u653b\u51fb\u4e0b\u5bf9\u73b0\u6709\u65b9\u6cd5\u6709\u663e\u8457\u63d0\u5347\u3002","title":"1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u660e\u6458\u8981 (Concise Summary)"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-05/#2-key-innovation-or-methodological-approach_2","text":"\u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u201c\u96c6\u6210\u653b\u51fb\u7f51\u7edc\u201d\uff08Ensemble Attack Network\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u8bad\u7ec3\u9636\u6bb5\u63d0\u5347\u540e\u5904\u7406\u6c34\u5370\u7684\u9c81\u68d2\u6027\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u4e0d\u540c\u67b6\u6784\uff08CNN\u548cTransformer\uff09\u548c\u4e0d\u540c\u4f5c\u7528\u57df\uff08\u7a7a\u95f4\u57df\u548c\u9891\u7387\u57df\uff09\u7684\u653b\u51fb\u6a21\u5757\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u591a\u6837\u5316\u7684\u653b\u51fb\u96c6\u6210\u4f53\u3002\u8fd9\u79cd\u96c6\u6210\u653b\u51fb\u7b56\u7565\u8feb\u4f7f\u6c34\u5370\u6a21\u578b\u5b66\u4e60\u5bf9\u591a\u79cd\u6f5c\u5728\u653b\u51fb\u66f4\u5177\u62b5\u6297\u529b\u7684\u7279\u5f81\uff0c\u4ece\u800c\u663e\u8457\u589e\u5f3a\u5176\u9c81\u68d2\u6027\u3002\u8bba\u6587\u8fd8\u901a\u8fc7\u5b9e\u9a8c\u53d1\u73b0\uff0c\u5c06\u57fa\u4e8eCNN\u7684\u7a7a\u95f4\u57df\u653b\u51fb\u4e0e\u57fa\u4e8eTransformer\u7684\u9891\u7387\u57df\u653b\u51fb\u76f8\u7ed3\u5408\uff0c\u80fd\u8fbe\u5230\u6700\u4f73\u7684\u9c81\u68d2\u6027\u6548\u679c\u3002","title":"2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u8bba (Key Innovation or Methodological Approach)"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-05/#3-potential-impact-on-the-field_2","text":"\u672c\u7814\u7a76\u663e\u8457\u63d0\u5347\u4e86\u540e\u5904\u7406\u6570\u5b57\u6c34\u5370\u7684\u5b9e\u7528\u6027\u548c\u53ef\u9760\u6027\uff0c\u4f7f\u5176\u80fd\u66f4\u6709\u6548\u5730\u5e94\u7528\u4e8e\u4efb\u4f55\u751f\u6210\u6a21\u578b\uff08\u5982GANs\u3001\u6269\u6563\u6a21\u578b\uff09\u7684\u8f93\u51fa\uff0c\u800c\u65e0\u9700\u8bbf\u95ee\u5176\u5185\u90e8\u7ed3\u6784\u3002\u8fd9\u5bf9\u4e8eAI\u751f\u6210\u5185\u5bb9\u7684\u7248\u6743\u4fdd\u62a4\u3001\u6eaf\u6e90\u3001\u771f\u5b9e\u6027\u9a8c\u8bc1\u4ee5\u53ca\u6253\u51fb\u6df1\u5ea6\u4f2a\u9020\u7b49\u9886\u57df\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002\u5b83\u4e5f\u4e3a\u672a\u6765\u6c34\u5370\u6280\u672f\uff0c\u7279\u522b\u662f\u5bf9\u6297\u6027\u9c81\u68d2\u6027\u7814\u7a76\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u7684\u3001\u6709\u6548\u7684\u8bad\u7ec3\u8303\u5f0f\uff0c\u5373\u901a\u8fc7\u96c6\u6210\u591a\u6837\u5316\u653b\u51fb\u8fdb\u884c\u5bf9\u6297\u8bad\u7ec3\u3002","title":"3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd (Potential Impact on the Field)"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-05/#4-related-areas-or-applications_1","text":"AI\u751f\u6210\u5185\u5bb9\u7684\u7248\u6743\u4fdd\u62a4\u4e0e\u6eaf\u6e90\uff1a \u786e\u4fddAI\u751f\u6210\u56fe\u50cf\u3001\u89c6\u9891\u7b49\u5185\u5bb9\u7684\u539f\u521b\u6027\u5f52\u5c5e\uff0c\u9632\u6b62\u672a\u7ecf\u6388\u6743\u7684\u4f7f\u7528\u3002 \u6570\u5b57\u5a92\u4f53\u771f\u5b9e\u6027\u9a8c\u8bc1\u4e0e\u9632\u7be1\u6539\uff1a \u9a8c\u8bc1\u56fe\u50cf\u6216\u89c6\u9891\u662f\u5426\u88ab\u7be1\u6539\uff0c\u5c24\u5176\u662f\u5728\u65b0\u95fb\u3001\u6cd5\u5f8b\u3001\u533b\u7597\u7b49\u5bf9\u5185\u5bb9\u771f\u5b9e\u6027\u8981\u6c42\u6781\u9ad8\u7684\u9886\u57df\u3002 \u6df1\u5ea6\u4f2a\u9020\uff08Deepfake\uff09\u68c0\u6d4b\u4e0e\u6eaf\u6e90\uff1a \u901a\u8fc7\u5d4c\u5165\u6c34\u5370\u6765\u8bc6\u522b\u548c\u8ffd\u8e2a\u5408\u6210\u5185\u5bb9\uff0c\u5bf9\u6297\u6076\u610f\u6df1\u5ea6\u4f2a\u9020\u3002 \u6570\u5b57\u53d6\u8bc1\uff1a \u5728\u7f51\u7edc\u72af\u7f6a\u8c03\u67e5\u4e2d\u63d0\u4f9b\u5185\u5bb9\u6765\u6e90\u548c\u4fee\u6539\u5386\u53f2\u7684\u7ebf\u7d22\u3002 \u77e5\u8bc6\u4ea7\u6743\u7ba1\u7406\uff1a \u4fdd\u62a4\u6570\u5b57\u827a\u672f\u54c1\u548c\u521b\u610f\u5185\u5bb9\u7684\u77e5\u8bc6\u4ea7\u6743\u3002","title":"4. \u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 (Related Areas or Applications)"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-05/#5-limitations-inferable-from-the-abstract","text":"\u9c81\u68d2\u6027\u7684\u7edd\u5bf9\u4e0a\u9650\uff1a \u5c3d\u7ba1\u663e\u8457\u63d0\u5347\u4e86\u9c81\u68d2\u6027\uff0c\u4f46\u4efb\u4f55\u6c34\u5370\u7cfb\u7edf\u90fd\u65e0\u6cd5\u4fdd\u8bc1\u5bf9\u6240\u6709\u6f5c\u5728\u653b\u51fb\u7684\u7edd\u5bf9\u62b5\u6297\u529b\uff0c\u672a\u6765\u53ef\u80fd\u51fa\u73b0\u66f4\u590d\u6742\u7684\u653b\u51fb\u3002 \u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\uff1a \u8bad\u7ec3\u96c6\u6210\u653b\u51fb\u7f51\u7edc\uff0c\u7279\u522b\u662f\u7ed3\u5408\u4e86CNN\u548cTransformer\u7684\u590d\u6742\u6a21\u578b\uff0c\u53ef\u80fd\u4f1a\u5e26\u6765\u8f83\u9ad8\u7684\u8ba1\u7b97\u8d44\u6e90\u548c\u65f6\u95f4\u6210\u672c\u3002 \u5bf9\u672a\u77e5\u653b\u51fb\u7684\u6cdb\u5316\u80fd\u529b\uff1a \u5c3d\u7ba1\u96c6\u6210\u4e86\u591a\u6837\u5316\u653b\u51fb\uff0c\u4f46\u5176\u5bf9\u8bad\u7ec3\u96c6\u4e2d\u672a\u5305\u542b\u7684\u3001\u5168\u65b0\u7c7b\u578b\u7684\u653b\u51fb\u7684\u6cdb\u5316\u80fd\u529b\u4ecd\u9700\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u3002 \u8bc4\u4f30\u8303\u56f4\uff1a \u8bc4\u4f30\u4e3b\u8981\u57fa\u4e8eWAVES\u57fa\u51c6\u6d4b\u8bd5\u548c\u5e73\u5747\u6bd4\u7279\u51c6\u786e\u7387\uff0c\u5176\u5728\u5176\u4ed6\u7279\u5b9a\u573a\u666f\u6216\u4f7f\u7528\u5176\u4ed6\u8bc4\u4f30\u6307\u6807\u65f6\u7684\u8868\u73b0\u53ef\u80fd\u6709\u6240\u4e0d\u540c\u3002 \u4ec5\u9650\u4e8e\u540e\u5904\u7406\u6c34\u5370\uff1a \u672c\u7814\u7a76\u4e13\u6ce8\u4e8e\u540e\u5904\u7406\u6c34\u5370\uff0c\u5176\u7ed3\u8bba\u548c\u65b9\u6cd5\u4e0d\u76f4\u63a5\u9002\u7528\u4e8e\u5728\u56fe\u50cf\u751f\u6210\u8fc7\u7a0b\u4e2d\u5d4c\u5165\u6c34\u5370\u7684\u201c\u524d\u5904\u7406\u6c34\u5370\u201d\u573a\u666f\u3002 Key Findings: In particular, for the Regeneration Attack defined in WAVES, our method improves StegaStamp by 18.743%. Links: PDF arXiv","title":"5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u7684\u5c40\u9650\u6027 (Limitations Inferable from the Abstract)"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-05/#plotn-polish-zero-shot-story-visualization-and-disentangled-editing-with-text-to-image-diffusion-models","text":"Authors: Kiymet Akdemir, Jing Shi, Kushal Kafle, Brian Price, Pinar Yanardag Published: 2025-09-04 Categories: cs.CV Abstract: Text-to-image diffusion models have demonstrated significant capabilities to generate diverse and detailed visuals in various domains, and story visualization is emerging as a particularly promising application. However, as their use in real-world creative domains increases, the need for providing enhanced control, refinement, and the ability to modify images post-generation in a consistent manner becomes an important challenge. Existing methods often lack the flexibility to apply fine or coarse edits while maintaining visual and narrative consistency across multiple frames, preventing creators from seamlessly crafting and refining their visual stories. To address these challenges, we introduce Plot'n Polish, a zero-shot framework that enables consistent story generation and provides fine-grained control over story visualizations at various levels of detail. Analysis: \u4f5c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u4e13\u5bb6\uff0c\u6211\u5bf9\u8fd9\u7bc7\u8bba\u6587\u6458\u8981\u8fdb\u884c\u5982\u4e0b\u5206\u6790\uff1a","title":"Plot'n Polish: Zero-shot Story Visualization and Disentangled Editing with Text-to-Image Diffusion Models"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-05/#1-concise-summary_2","text":"\u672c\u6587\u63d0\u51fa\u4e86 Plot'n Polish\uff0c\u4e00\u4e2a\u96f6\u6837\u672c\uff08zero-shot\uff09\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u5f53\u524d\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u5728\u6545\u4e8b\u53ef\u89c6\u5316\u4e2d\u7f3a\u4e4f\u4e00\u81f4\u6027\u63a7\u5236\u548c\u540e\u671f\u7f16\u8f91\u7075\u6d3b\u6027\u7684\u95ee\u9898\u3002\u5b83\u5b9e\u73b0\u4e86\u8de8\u591a\u5e27\u7684\u89c6\u89c9\u548c\u53d9\u4e8b\u4e00\u81f4\u6027\u6545\u4e8b\u751f\u6210\uff0c\u5e76\u63d0\u4f9b\u4e86\u5bf9\u6545\u4e8b\u53ef\u89c6\u5316\u5728\u4e0d\u540c\u7ec6\u8282\u5c42\u7ea7\u7684\u7ec6\u7c92\u5ea6\u63a7\u5236\u548c\u89e3\u8026\u7f16\u8f91\u80fd\u529b\uff0c\u4ece\u800c\u4f7f\u521b\u4f5c\u8005\u80fd\u591f\u65e0\u7f1d\u5730\u7cbe\u4fee\u5176\u89c6\u89c9\u6545\u4e8b\u3002","title":"1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u660e\u603b\u7ed3 (Concise Summary)"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-05/#2-key-innovation-or-methodological-approach_3","text":"\u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u5176 \u96f6\u6837\u672c\uff08zero-shot\uff09 \u7279\u6027\uff0c\u8fd9\u610f\u5473\u7740\u8be5\u6846\u67b6\u65e0\u9700\u9488\u5bf9\u7279\u5b9a\u6545\u4e8b\u6216\u7f16\u8f91\u4efb\u52a1\u8fdb\u884c\u989d\u5916\u8bad\u7ec3\u3002\u66f4\u91cd\u8981\u7684\u662f\uff0c\u5b83\u5f15\u5165\u4e86\u4e00\u79cd \u89e3\u8026\u7f16\u8f91\uff08disentangled editing\uff09 \u7684\u65b9\u6cd5\uff08\u4ece\u6807\u9898\u63a8\u65ad\uff09\uff0c\u4f7f\u5f97\u7528\u6237\u80fd\u591f\u5bf9\u6545\u4e8b\u53ef\u89c6\u5316\u8fdb\u884c \u7ec6\u7c92\u5ea6\uff08fine-grained\uff09 \u63a7\u5236\uff0c\u5e76\u5728 \u4e0d\u540c\u7ec6\u8282\u5c42\u7ea7 \uff08\u4ece\u6574\u4f53\u53d9\u4e8b\u5230\u5c40\u90e8\u5143\u7d20\uff09\u4e0a\u8fdb\u884c\u4fee\u6539\uff0c\u540c\u65f6\u786e\u4fdd \u8de8\u591a\u5e27\u7684\u89c6\u89c9\u548c\u53d9\u4e8b\u4e00\u81f4\u6027 \u3002\u8fd9\u79cd\u5728\u4fdd\u6301\u4e00\u81f4\u6027\u524d\u63d0\u4e0b\u7684\u591a\u5c42\u7ea7\u3001\u89e3\u8026\u63a7\u5236\u662f\u73b0\u6709\u65b9\u6cd5\u6240\u6b20\u7f3a\u7684\uff0c\u5b83\u89e3\u51b3\u4e86\u5728\u6545\u4e8b\u751f\u6210\u4e2d\uff0c\u89d2\u8272\u3001\u573a\u666f\u3001\u98ce\u683c\u7b49\u5143\u7d20\u5728\u4e0d\u540c\u5e27\u4e4b\u95f4\u4fdd\u6301\u8fde\u8d2f\u6027\u7684\u6838\u5fc3\u6311\u6218\u3002","title":"2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u5b66\u65b9\u6cd5 (Key Innovation or Methodological Approach)"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-05/#3-potential-impact-on-the-field_3","text":"\u8be5\u7814\u7a76\u5c06\u663e\u8457\u63d0\u5347\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u5728 \u6545\u4e8b\u53ef\u89c6\u5316 \u9886\u57df\u7684\u5b9e\u7528\u6027\u548c\u53ef\u63a7\u6027\u3002\u5b83\u4e3a\u521b\u4f5c\u8005\u63d0\u4f9b\u4e86\u524d\u6240\u672a\u6709\u7684\u7075\u6d3b\u6027\uff0c\u4f7f\u5176\u80fd\u591f \u65e0\u7f1d\u5730\u521b\u4f5c\u3001\u8fed\u4ee3\u548c\u7cbe\u4fee\u89c6\u89c9\u6545\u4e8b \uff0c\u4ece\u800c\u964d\u4f4e\u4e86\u9ad8\u8d28\u91cf\u89c6\u89c9\u53d9\u4e8b\u5185\u5bb9\u7684\u521b\u4f5c\u95e8\u69db\u3002\u8fd9\u4e0d\u4ec5\u80fd\u63a8\u52a8 \u521b\u610f\u4ea7\u4e1a \uff08\u5982\u52a8\u753b\u3001\u6f2b\u753b\u3001\u6e38\u620f\u6982\u5ff5\u827a\u672f\u3001\u5e7f\u544a\uff09\u7684\u53d1\u5c55\uff0c\u4e5f\u4e3a\u672a\u6765\u66f4\u590d\u6742\u7684 \u4eba\u673a\u534f\u4f5c\u5185\u5bb9\u751f\u6210 \u6a21\u5f0f\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u4f7fAI\u5de5\u5177\u4ece\u5355\u7eaf\u7684\u751f\u6210\u5668\u8f6c\u53d8\u4e3a\u66f4\u5f3a\u5927\u7684\u521b\u610f\u52a9\u624b\uff0c\u80fd\u591f\u7406\u89e3\u5e76\u54cd\u5e94\u7528\u6237\u5728\u53d9\u4e8b\u548c\u89c6\u89c9\u7f16\u8f91\u4e0a\u7684\u590d\u6742\u610f\u56fe\u3002","title":"3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd (Potential Impact on the Field)"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-05/#4-related-areas-or-applications_2","text":"\u521b\u610f\u5185\u5bb9\u751f\u6210: \u52a8\u753b\u3001\u6f2b\u753b\u3001\u7535\u5f71\u9884\u53ef\u89c6\u5316\uff08pre-visualization\uff09\u3001\u6e38\u620f\u6982\u5ff5\u827a\u672f\u3001\u5e7f\u544a\u521b\u610f\u3001\u6570\u5b57\u63d2\u753b\u3002 \u4e2a\u6027\u5316\u5185\u5bb9: \u6839\u636e\u7528\u6237\u8f93\u5165\u751f\u6210\u5b9a\u5236\u5316\u7684\u6545\u4e8b\u3001\u6559\u80b2\u6750\u6599\u6216\u4ea4\u4e92\u5f0f\u4f53\u9a8c\u3002 \u865a\u62df\u73b0\u5b9e/\u589e\u5f3a\u73b0\u5b9e (VR/AR): \u5feb\u901f\u751f\u6210\u548c\u8fed\u4ee3\u865a\u62df\u573a\u666f\u6216\u89d2\u8272\u8d44\u4ea7\uff0c\u4fdd\u6301\u5176\u5728\u4e0d\u540c\u4ea4\u4e92\u72b6\u6001\u4e0b\u7684\u4e00\u81f4\u6027\u3002 \u6570\u5b57\u4eba/\u865a\u62df\u5076\u50cf: \u4fdd\u6301\u6570\u5b57\u89d2\u8272\u5728\u4e0d\u540c\u59ff\u6001\u3001\u8868\u60c5\u548c\u573a\u666f\u4e2d\u7684\u4e00\u81f4\u6027\uff0c\u5e76\u8fdb\u884c\u7cbe\u7ec6\u5316\u7f16\u8f91\u3002 \u591a\u6a21\u6001\u5185\u5bb9\u7406\u89e3\u4e0e\u751f\u6210: \u4e3a\u6587\u672c\u6545\u4e8b\u81ea\u52a8\u914d\u56fe\u6216\u751f\u6210\u89c6\u9891\u811a\u672c\uff0c\u5e76\u5141\u8bb8\u7528\u6237\u5bf9\u751f\u6210\u7ed3\u679c\u8fdb\u884c\u7cbe\u7ec6\u8c03\u6574\u3002","title":"4. \u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 (Related Areas or Applications)"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-05/#5-limitations-that-can-be-inferred-from-the-abstract_1","text":"\u96f6\u6837\u672c\u65b9\u6cd5\u7684\u5c40\u9650\u6027: \u5c3d\u7ba1\u96f6\u6837\u672c\u662f\u4f18\u52bf\uff0c\u4f46\u5bf9\u4e8e\u9ad8\u5ea6\u7279\u5b9a\u3001\u98ce\u683c\u5316\u6216\u9700\u8981\u6781\u9ad8\u7ec6\u8282\u4fdd\u771f\u5ea6\u7684\u573a\u666f\uff0c\u5176\u6027\u80fd\u53ef\u80fd\u4e0d\u5982\u7ecf\u8fc7\u7279\u5b9a\u6570\u636e\u5fae\u8c03\u7684\u6a21\u578b\u3002\u5728\u67d0\u4e9b\u6781\u7aef\u60c5\u51b5\u4e0b\uff0c\u96f6\u6837\u672c\u65b9\u6cd5\u53ef\u80fd\u96be\u4ee5\u6355\u6349\u5230\u975e\u5e38\u7ec6\u5fae\u6216\u72ec\u7279\u7684\u89c6\u89c9\u7279\u5f81\u3002 \u4e00\u81f4\u6027\u4fdd\u6301\u7684\u9c81\u68d2\u6027: \u6458\u8981\u4e2d\u5f3a\u8c03\u4e86\u201c\u4e00\u81f4\u6027\u201d\uff0c\u4f46\u5bf9\u4e8e\u6781\u5176\u590d\u6742\u3001\u53d9\u4e8b\u8de8\u5ea6\u957f\u6216\u89d2\u8272/\u573a\u666f\u53d1\u751f\u5267\u70c8\u53d8\u5316\u7684\u6545\u4e8b\uff0c\u7ef4\u6301\u5b8c\u7f8e\u7684\u4e00\u81f4\u6027\u4ecd\u662f\u4e00\u4e2a\u5de8\u5927\u6311\u6218\u3002\u6a21\u578b\u5982\u4f55\u5904\u7406\u89d2\u8272\u670d\u88c5\u3001\u53d1\u578b\u3001\u9762\u90e8\u7279\u5f81\u5728\u4e0d\u540c\u5e27\u4e2d\u7684\u7ec6\u5fae\u53d8\u5316\uff0c\u4ee5\u53ca\u80cc\u666f\u73af\u5883\u7684\u8fde\u8d2f\u6027\uff0c\u662f\u9700\u8981\u9a8c\u8bc1\u7684\u3002 \u201c\u7ec6\u7c92\u5ea6\u63a7\u5236\u201d\u7684\u5b9e\u9645\u8fb9\u754c: \u6458\u8981\u4e2d\u63d0\u5230\u201c\u7ec6\u7c92\u5ea6\u63a7\u5236\u201d\uff0c\u4f46\u5176\u5177\u4f53\u80fd\u8fbe\u5230\u4f55\u79cd\u7a0b\u5ea6\u7684\u7cbe\u7ec6\u5316\u7f16\u8f91\uff08\u4f8b\u5982\uff0c\u80fd\u5426\u7cbe\u786e\u4fee\u6539\u67d0\u4e2a\u89d2\u8272\u7684\u5fae\u5c0f\u8868\u60c5\u3001\u7279\u5b9a\u9053\u5177\u7684\u7ec6\u8282\uff09\uff0c\u4ee5\u53ca\u8fd9\u79cd\u63a7\u5236\u7684\u76f4\u89c2\u6027/\u6613\u7528\u6027\uff0c\u4ecd\u9700\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u68c0\u9a8c\u3002\u89e3\u8026\u7f16\u8f91\u7684\u8d28\u91cf\u76f4\u63a5\u5f71\u54cd\u8fd9\u4e00\u70b9\uff0c\u5982\u679c\u89e3\u8026\u4e0d\u5f7b\u5e95\uff0c\u53ef\u80fd\u4f1a\u5728\u7f16\u8f91\u4e00\u4e2a\u5143\u7d20\u65f6\u610f\u5916\u5f71\u54cd\u5230\u5176\u4ed6\u5143\u7d20\u3002 \u8ba1\u7b97\u8d44\u6e90\u4e0e\u6548\u7387: \u6269\u6563\u6a21\u578b\u901a\u5e38\u8ba1\u7b97\u6210\u672c\u8f83\u9ad8\uff0c\u5c24\u5176\u662f\u5728\u8fdb\u884c\u591a\u5e27\u751f\u6210\u548c\u8fed\u4ee3\u7f16\u8f91\u65f6\u3002\u6458\u8981\u4e2d\u672a\u63d0\u53ca\u6027\u80fd\u6216\u901f\u5ea6\uff0c\u8fd9\u53ef\u80fd\u662f\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u4e00\u4e2a\u6f5c\u5728\u9650\u5236\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u5feb\u901f\u8fed\u4ee3\u7684\u521b\u610f\u5de5\u4f5c\u6d41\u4e2d\u3002 Key Findings: To address these challenges, we introduce Plot'n Polish, a zero-shot framework that enables consistent story generation and provides fine-grained control over story visualizations at various levels of detail. Links: PDF arXiv","title":"5. \u53ef\u4ece\u6458\u8981\u63a8\u65ad\u7684\u5c40\u9650\u6027 (Limitations that can be inferred from the abstract)"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-05/#differential-morphological-profile-neural-networks-for-semantic-segmentation","text":"Authors: David Huangal, J. Alex Hurt Published: 2025-09-04 Categories: cs.CV Abstract: Semantic segmentation of overhead remote sensing imagery enables applications in mapping, urban planning, and disaster response. State-of-the-art segmentation networks are typically developed and tuned on ground-perspective photographs and do not directly address remote sensing challenges such as extreme scale variation, foreground-background imbalance, and large image sizes. We explore the incorporation of the differential morphological profile (DMP), a multi-scale shape extraction method based on grayscale morphology, into modern segmentation networks. Prior studies have shown that the DMP can provide critical shape information to Deep Neural Networks to enable superior detection and classification performance in overhead imagery. In this work, we extend prior DMPNet work beyond classification and object detection by integrating DMP features into three state-of-the-art convolutional and transformer semantic segmentation architectures. We utilize both direct input, which adapts the input stem of feature extraction architectures to accept DMP channels, and hybrid architectures, a dual-stream design that fuses RGB and DMP encoders. Using the iSAID benchmark dataset, we evaluate a variety of DMP differentials and structuring element shapes to more effectively provide shape information to the model. Our results show that while non-DMP models generally outperform the direct-input variants, hybrid DMP consistently outperforms direct-input and is capable of surpassing a non-DMP model on mIoU, F1, and Recall. Analysis: \u4f5c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u4e13\u5bb6\uff0c\u6211\u5bf9\u8fd9\u7bc7\u8bba\u6587\u6458\u8981\u7684\u5206\u6790\u5982\u4e0b\uff1a","title":"Differential Morphological Profile Neural Networks for Semantic Segmentation"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-05/#differential-morphological-profile-neural-networks-for-semantic-segmentation_1","text":"1. \u8bba\u6587\u4e3b\u8981\u8d21\u732e\u7684\u7b80\u6d01\u603b\u7ed3 (2-3 \u53e5\u8bdd) \u672c\u6587\u65e8\u5728\u89e3\u51b3\u9065\u611f\u56fe\u50cf\u8bed\u4e49\u5206\u5272\u4e2d\u5b58\u5728\u7684\u6781\u7aef\u5c3a\u5ea6\u53d8\u5316\u3001\u524d\u666f\u80cc\u666f\u4e0d\u5e73\u8861\u7b49\u6311\u6218\uff0c\u63d0\u51fa\u5c06\u5dee\u5206\u5f62\u6001\u5b66\u5256\u9762\uff08DMP\uff09\u8fd9\u4e00\u591a\u5c3a\u5ea6\u5f62\u72b6\u63d0\u53d6\u65b9\u6cd5\u878d\u5165\u5230\u5148\u8fdb\u7684\u8bed\u4e49\u5206\u5272\u7f51\u7edc\u4e2d\u3002\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7RGB\u548cDMP\u7f16\u7801\u5668\u878d\u5408\u7684\u53cc\u6d41\u201c\u6df7\u5408\u67b6\u6784\u201d\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u5728mIoU\u3001F1\u548cRecall\u7b49\u6307\u6807\u4e0a\u8d85\u8d8a\u975eDMP\u57fa\u7ebf\u6a21\u578b\uff0c\u4e3a\u9065\u611f\u56fe\u50cf\u5206\u6790\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002 2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u5b66\u65b9\u6cd5 \u672c\u6587\u7684\u5173\u952e\u521b\u65b0\u5728\u4e8e\u5c06\u5dee\u5206\u5f62\u6001\u5b66\u5256\u9762\uff08DMP\uff09\u2014\u2014\u4e00\u79cd\u57fa\u4e8e\u7070\u5ea6\u5f62\u6001\u5b66\u7684\u591a\u5c3a\u5ea6\u5f62\u72b6\u63d0\u53d6\u65b9\u6cd5\u2014\u2014\u9996\u6b21\u6269\u5c55\u5e76\u6210\u529f\u5e94\u7528\u4e8e\u8bed\u4e49\u5206\u5272\u4efb\u52a1\uff0c\u8d85\u8d8a\u4e86\u5176\u5728\u5206\u7c7b\u548c\u76ee\u6807\u68c0\u6d4b\u4e2d\u7684\u4f20\u7edf\u5e94\u7528\u3002\u5176\u6838\u5fc3\u65b9\u6cd5\u5b66\u8d21\u732e\u662f\u63d0\u51fa\u4e86\u4e24\u79cdDMP\u96c6\u6210\u7b56\u7565\uff1a * \u76f4\u63a5\u8f93\u5165 (Direct Input) \uff1a\u8c03\u6574\u7279\u5f81\u63d0\u53d6\u67b6\u6784\u7684\u8f93\u5165\u5c42\u4ee5\u63a5\u53d7DMP\u901a\u9053\u3002 * \u6df7\u5408\u67b6\u6784 (Hybrid Architectures) \uff1a\u4e00\u79cd\u66f4\u6709\u6548\u7684\u53cc\u6d41\u8bbe\u8ba1\uff0c\u5206\u522b\u4f7f\u7528RGB\u548cDMP\u7f16\u7801\u5668\uff0c\u5e76\u5c06\u4e24\u8005\u7684\u7279\u5f81\u8fdb\u884c\u878d\u5408\u3002 \u901a\u8fc7\u5728iSAID\u6570\u636e\u96c6\u4e0a\u5bf9\u4e0d\u540cDMP\u5dee\u5206\u548c\u7ed3\u6784\u5143\u7d20\u5f62\u72b6\u7684\u7cfb\u7edf\u8bc4\u4f30\uff0c\u8bc1\u660e\u4e86\u6df7\u5408\u67b6\u6784\u5728\u63d0\u4f9b\u5173\u952e\u5f62\u72b6\u4fe1\u606f\u65b9\u9762\u7684\u4f18\u8d8a\u6027\u3002 3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd \u63d0\u5347\u9065\u611f\u56fe\u50cf\u5206\u6790\u7cbe\u5ea6\uff1a \u672c\u7814\u7a76\u76f4\u63a5\u89e3\u51b3\u4e86\u9065\u611f\u56fe\u50cf\u8bed\u4e49\u5206\u5272\u7684\u56fa\u6709\u6311\u6218\uff0c\u6709\u671b\u663e\u8457\u63d0\u9ad8\u5730\u56fe\u7ed8\u5236\u3001\u57ce\u5e02\u89c4\u5212\u548c\u707e\u5bb3\u54cd\u5e94\u7b49\u5e94\u7528\u4e2d\u7684\u81ea\u52a8\u5316\u548c\u7cbe\u5ea6\u3002 \u91cd\u65b0\u5ba1\u89c6\u7279\u5f81\u5de5\u7a0b\u7684\u4ef7\u503c\uff1a \u5728\u6df1\u5ea6\u5b66\u4e60\u4e3b\u5bfc\u7684\u65f6\u4ee3\uff0c\u672c\u6587\u5f3a\u8c03\u4e86\u7ed3\u5408\u4f20\u7edf\u3001\u9886\u57df\u7279\u5b9a\uff08\u5982\u5f62\u6001\u5b66\uff09\u7279\u5f81\u4e0e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u6f5c\u529b\uff0c\u4e3a\u6df7\u5408\u6a21\u578b\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u3002 \u542f\u53d1\u591a\u6a21\u6001/\u591a\u7279\u5f81\u878d\u5408\uff1a \u6210\u529f\u878d\u5408DMP\u548cRGB\u4fe1\u606f\uff0c\u53ef\u80fd\u542f\u53d1\u7814\u7a76\u4eba\u5458\u63a2\u7d22\u5c06\u5176\u4ed6\u4e92\u8865\u7684\u3001\u975e\u50cf\u7d20\u7ea7\u7279\u5f81\uff08\u5982\u9ad8\u7a0b\u3001\u5149\u8c31\u6307\u6570\u7b49\uff09\u878d\u5165\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u4ee5\u5e94\u5bf9\u66f4\u590d\u6742\u7684\u89c6\u89c9\u4efb\u52a1\u3002 \u63a8\u52a8DMP\u5728\u66f4\u5e7f\u6cdbCV\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\uff1a \u8bc1\u660eDMP\u5728\u8bed\u4e49\u5206\u5272\u4e2d\u7684\u6709\u6548\u6027\uff0c\u53ef\u80fd\u4f1a\u4fc3\u4f7fDMP\u5728\u5176\u4ed6\u9700\u8981\u7cbe\u7ec6\u5f62\u72b6\u7406\u89e3\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u4e2d\u5f97\u5230\u66f4\u5e7f\u6cdb\u7684\u63a2\u7d22\u3002 4. \u53ef\u80fd\u53d7\u76ca\u7684\u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 \u5730\u7406\u4fe1\u606f\u7cfb\u7edf (GIS) \u548c\u6d4b\u7ed8\uff1a \u63d0\u9ad8\u5730\u7269\u5206\u7c7b\u3001\u571f\u5730\u8986\u76d6\u5236\u56fe\u548c\u53d8\u5316\u68c0\u6d4b\u7684\u81ea\u52a8\u5316\u6c34\u5e73\u548c\u51c6\u786e\u6027\u3002 \u667a\u6167\u57ce\u5e02\u548c\u57ce\u5e02\u89c4\u5212\uff1a \u7cbe\u51c6\u8bc6\u522b\u5efa\u7b51\u7269\u3001\u9053\u8def\u3001\u7eff\u5730\u7b49\uff0c\u652f\u6301\u57ce\u5e02\u57fa\u7840\u8bbe\u65bd\u7ba1\u7406\u548c\u53d1\u5c55\u89c4\u5212\u3002 \u707e\u5bb3\u54cd\u5e94\u4e0e\u7ba1\u7406\uff1a \u5feb\u901f\u51c6\u786e\u5730\u8bc4\u4f30\u707e\u60c5\uff0c\u5982\u6d2a\u6c34\u6df9\u6ca1\u533a\u57df\u3001\u53d7\u635f\u5efa\u7b51\u7269\u8bc6\u522b\uff0c\u8f85\u52a9\u6551\u63f4\u51b3\u7b56\u3002 \u73af\u5883\u76d1\u6d4b\uff1a \u76d1\u6d4b\u68ee\u6797\u780d\u4f10\u3001\u51b0\u5ddd\u878d\u5316\u3001\u519c\u4f5c\u7269\u5065\u5eb7\u72b6\u51b5\u7b49\uff0c\u63d0\u4f9b\u7cbe\u7ec6\u5316\u7684\u73af\u5883\u6570\u636e\u3002 \u519c\u4e1a\u9065\u611f\uff1a \u7cbe\u51c6\u8bc6\u522b\u519c\u4f5c\u7269\u7c7b\u578b\u3001\u751f\u957f\u533a\u57df\uff0c\u652f\u6301\u667a\u80fd\u519c\u4e1a\u7ba1\u7406\u3002 \u56fd\u9632\u4e0e\u60c5\u62a5\uff1a \u63d0\u5347\u5bf9\u519b\u4e8b\u8bbe\u65bd\u3001\u4ea4\u901a\u7f51\u7edc\u7b49\u76ee\u6807\u7684\u8bc6\u522b\u548c\u5206\u6790\u80fd\u529b\u3002 5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u7684\u5c40\u9650\u6027 \u8ba1\u7b97\u6210\u672c\u589e\u52a0\uff1a \u6df7\u5408\u53cc\u6d41\u67b6\u6784\u901a\u5e38\u6bd4\u5355\u6d41\u6a21\u578b\u5177\u6709\u66f4\u9ad8\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u53c2\u6570\u91cf\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u8bad\u7ec3\u548c\u63a8\u7406\u65f6\u95f4\u589e\u52a0\uff0c\u5bf9\u8d44\u6e90\u53d7\u9650\u7684\u5e94\u7528\u6784\u6210\u6311\u6218\u3002 DMP\u53c2\u6570\u8c03\u4f18\u7684\u590d\u6742\u6027\uff1a \u6458\u8981\u63d0\u5230\u8bc4\u4f30\u4e86\u201c\u5404\u79cdDMP\u5dee\u5206\u548c\u7ed3\u6784\u5143\u7d20\u5f62\u72b6\u201d\uff0c\u8fd9\u6697\u793aDMP\u7684\u53c2\u6570\uff08\u5982\u7ed3\u6784\u5143\u7d20\u5927\u5c0f\u3001\u5f62\u72b6\u3001\u5dee\u5206\u9636\u6570\uff09\u53ef\u80fd\u9700\u8981\u9488\u5bf9\u4e0d\u540c\u6570\u636e\u96c6\u6216\u4efb\u52a1\u8fdb\u884c\u7ec6\u81f4\u7684\u8c03\u4f18\uff0c\u8fd9\u589e\u52a0\u4e86\u6a21\u578b\u7684\u90e8\u7f72\u548c\u6cdb\u5316\u96be\u5ea6\u3002 \u201c\u53ef\u4ee5\u8d85\u8d8a\u201d\u7684\u9650\u5b9a\u6027\uff1a \u7ed3\u679c\u663e\u793a\u201chybrid DMP consistently outperforms direct-input and is capable of surpassing a non-DMP model\u201d\uff0c\u5176\u4e2d\u201ccapable of surpassing\u201d\u53ef\u80fd\u610f\u5473\u7740\u5e76\u975e\u5728\u6240\u6709\u60c5\u51b5\u4e0b\u6216\u6240\u6709\u6307\u6807\u4e0a\u90fd\u80fd\u7edd\u5bf9\u8d85\u8d8a\u975eDMP\u6a21\u578b\uff0c\u6216\u8005\u8d85\u8d8a\u7684\u5e45\u5ea6\u53ef\u80fd\u6709\u9650\uff0c\u8fd9\u9700\u8981\u8fdb\u4e00\u6b65\u7684\u5b9e\u9a8c\u7ec6\u8282\u6765\u9a8c\u8bc1\u3002 \u5bf9\u7070\u5ea6\u5f62\u6001\u5b66\u7684\u4f9d\u8d56\uff1a DMP\u57fa\u4e8e\u7070\u5ea6\u5f62\u6001\u5b66\uff0c\u5176\u6709\u6548\u6027\u53ef\u80fd\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4f9d\u8d56\u4e8e\u56fe\u50cf\u4e2d\u5f62\u72b6\u4fe1\u606f\u5728\u7070\u5ea6\u901a\u9053\u4e2d\u7684\u53ef\u63d0\u53d6\u6027\u3002\u5bf9\u4e8e\u989c\u8272\u6216\u7eb9\u7406\u4fe1\u606f\u66f4\u4e3a\u5173\u952e\u7684\u573a\u666f\uff0cDMP\u7684\u76f4\u63a5\u8d21\u732e\u53ef\u80fd\u9700\u8981\u4e0e\u5176\u4ed6\u7279\u5f81\u7ed3\u5408\u3002 Key Findings: State-of-the-art segmentation networks are typically developed and tuned on ground-perspective photographs and do not directly address remote sensing challenges such as extreme scale variation, foreground-background imbalance, and large image sizes. In this work, we extend prior DMPNet work beyond classification and object detection by integrating DMP features into three state-of-the-art convolutional and transformer semantic segmentation architectures. Our results show that while non-DMP models generally outperform the direct-input variants, hybrid DMP consistently outperforms direct-input and is capable of surpassing a non-DMP model on mIoU, F1, and Recall. Links: PDF arXiv","title":"\u8bba\u6587\u6458\u8981\u5206\u6790\uff1aDifferential Morphological Profile Neural Networks for Semantic Segmentation"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-05/#tinydrop-tiny-model-guided-token-dropping-for-vision-transformers","text":"Authors: Guoxin Wang, Qingyuan Wang, Binhua Huang, Shaowu Chen, Deepu John Published: 2025-09-03 Categories: cs.CV, cs.AI Abstract: Vision Transformers (ViTs) achieve strong performance in image classification but incur high computational costs from processing all image tokens. To reduce inference costs in large ViTs without compromising accuracy, we propose TinyDrop, a training-free token dropping framework guided by a lightweight vision model. The guidance model estimates the importance of tokens while performing inference, thereby selectively discarding low-importance tokens if large vit models need to perform attention calculations. The framework operates plug-and-play, requires no architectural modifications, and is compatible with diverse ViT architectures. Evaluations on standard image classification benchmarks demonstrate that our framework reduces FLOPs by up to 80% for ViTs with minimal accuracy degradation, highlighting its generalization capability and practical utility for efficient ViT-based classification. Analysis: \u4f5c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u4e13\u5bb6\uff0c\u6211\u5bf9\u8fd9\u7bc7\u5173\u4e8eTinyDrop\u7684\u8bba\u6587\u6458\u8981\u8fdb\u884c\u4e86\u5206\u6790\uff1a","title":"TinyDrop: Tiny Model Guided Token Dropping for Vision Transformers"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-05/#tinydrop-tiny-model-guided-token-dropping-for-vision-transformers_1","text":"1. \u8bba\u6587\u6838\u5fc3\u8d21\u732e\u7684\u7b80\u660e\u603b\u7ed3 (Concise Summary) TinyDrop\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9Vision Transformers (ViTs) \u7684\u8bad\u7ec3\u65e0\u5173\uff08training-free\uff09\u7684token\u4e22\u5f03\u6846\u67b6\uff0c\u65e8\u5728\u663e\u8457\u964d\u4f4e\u5927\u578bViTs\u7684\u63a8\u7406\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u89c6\u89c9\u6a21\u578b\u5728\u63a8\u7406\u65f6\u52a8\u6001\u8bc4\u4f30token\u7684\u91cd\u8981\u6027\uff0c\u5e76\u9009\u62e9\u6027\u5730\u4e22\u5f03\u4f4e\u91cd\u8981\u6027\u7684token\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0cTinyDrop\u80fd\u5c06ViTs\u7684FLOPs\u964d\u4f4e\u9ad8\u8fbe80%\uff0c\u4e14\u4ec5\u5e26\u6765\u6781\u5c0f\u7684\u7cbe\u5ea6\u635f\u5931\uff0c\u5c55\u73b0\u4e86\u5176\u5728\u63d0\u9ad8ViT\u6548\u7387\u65b9\u9762\u7684\u5b9e\u7528\u4ef7\u503c\u3002 2. \u5173\u952e\u521b\u65b0\u6216\u65b9\u6cd5\u5b66\u65b9\u6cd5 (Key Innovation or Methodological Approach) \u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u5176 \u8bad\u7ec3\u65e0\u5173\uff08training-free\uff09 \u548c \u7531\u8f7b\u91cf\u7ea7\u6a21\u578b\u5f15\u5bfc\u7684\u52a8\u6001token\u4e22\u5f03\u673a\u5236 \u3002\u4e0e\u9700\u8981\u91cd\u65b0\u8bad\u7ec3\u6216\u4fee\u6539\u67b6\u6784\u7684\u73b0\u6709\u65b9\u6cd5\u4e0d\u540c\uff0cTinyDrop\u5f15\u5165\u4e86\u4e00\u4e2a\u5916\u90e8\u7684\u3001\u8f7b\u91cf\u7ea7\u6307\u5bfc\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u80fd\u591f\u5728ViT\u63a8\u7406\u8fc7\u7a0b\u4e2d \u5b9e\u65f6\uff08on-the-fly\uff09 \u8bc4\u4f30\u6bcf\u4e2atoken\u7684\u91cd\u8981\u6027\u3002\u8fd9\u79cd\u65b9\u6cd5\u5141\u8bb8\u5728\u5927\u578bViT\u6a21\u578b\u6267\u884c\u8ba1\u7b97\u5bc6\u96c6\u578b\u7684\u6ce8\u610f\u529b\u64cd\u4f5c\u4e4b\u524d\uff0c \u81ea\u9002\u5e94\u5730\u3001\u9009\u62e9\u6027\u5730 \u4e22\u5f03\u4e0d\u91cd\u8981\u7684token\u3002\u5176\u201c\u5373\u63d2\u5373\u7528\u201d\u7684\u7279\u6027\u548c\u5bf9\u591a\u79cdViT\u67b6\u6784\u7684\u517c\u5bb9\u6027\uff0c\u4e5f\u6781\u5927\u5730\u964d\u4f4e\u4e86\u5176\u5e94\u7528\u95e8\u69db\u548c\u5b9e\u7528\u6027\u3002 3. \u5bf9\u9886\u57df\u6f5c\u5728\u5f71\u54cd (Potential Impact on the Field) \u8fd9\u9879\u7814\u7a76\u5bf9\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u5177\u6709\u6df1\u8fdc\u5f71\u54cd\u3002\u5b83\u6709\u671b \u6c11\u4e3b\u5316\u5927\u578b\u9ad8\u6027\u80fdViT\u6a21\u578b\u7684\u90e8\u7f72 \uff0c\u4f7f\u5176\u80fd\u591f\u5728\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\uff08\u5982\u79fb\u52a8\u8bbe\u5907\u3001\u8fb9\u7f18\u8ba1\u7b97\u3001\u5d4c\u5165\u5f0f\u7cfb\u7edf\uff09\u4e2d\u8fd0\u884c\uff0c\u8fd9\u4e9b\u573a\u666f\u5bf9\u8ba1\u7b97\u9884\u7b97\u548c\u5ef6\u8fdf\u6709\u4e25\u683c\u8981\u6c42\u3002\u901a\u8fc7\u5927\u5e45\u964d\u4f4e\u63a8\u7406FLOPs\uff0cTinyDrop\u4e5f\u4e3a \u53ef\u6301\u7eedAI \u505a\u51fa\u4e86\u8d21\u732e\uff0c\u51cf\u5c11\u4e86\u5927\u578b\u6a21\u578b\u8fd0\u884c\u7684\u80fd\u6e90\u6d88\u8017\u3002\u6b64\u5916\uff0c\u5176\u5373\u63d2\u5373\u7528\u7684\u7279\u6027\u53ef\u4ee5\u52a0\u901fViT\u6a21\u578b\u7684\u7814\u53d1\u548c\u5e94\u7528\uff0c\u4e3a\u73b0\u6709\u548c\u672a\u6765\u7684ViT\u67b6\u6784\u63d0\u4f9b\u4e00\u4e2a\u6613\u4e8e\u96c6\u6210\u7684\u6548\u7387\u4f18\u5316\u65b9\u6848\u3002 4. \u53ef\u80fd\u53d7\u76ca\u7684\u76f8\u5173\u9886\u57df\u6216\u5e94\u7528 (Related Areas or Applications) \u5c3d\u7ba1\u6458\u8981\u4e3b\u8981\u5173\u6ce8\u56fe\u50cf\u5206\u7c7b\uff0c\u4f46token\u9ad8\u6548\u5904\u7406\u7684\u6838\u5fc3\u601d\u60f3\u5bf9\u5e7f\u6cdb\u7684ViT\u5e94\u7528\u90fd\u5177\u6709\u4ef7\u503c\uff1a \u5b9e\u65f6\u8ba1\u7b97\u673a\u89c6\u89c9\u7cfb\u7edf\uff1a \u81ea\u52a8\u9a7e\u9a76\u3001\u673a\u5668\u4eba\u3001\u89c6\u9891\u76d1\u63a7\u7b49\u5bf9\u4f4e\u5ef6\u8fdf\u6709\u4e25\u683c\u8981\u6c42\u7684\u573a\u666f\u3002 \u8fb9\u7f18AI/\u79fb\u52a8\u8ba1\u7b97\uff1a \u5728\u8ba1\u7b97\u80fd\u529b\u548c\u7535\u6c60\u5bff\u547d\u6709\u9650\u7684\u8bbe\u5907\u4e0a\u90e8\u7f72\u590d\u6742\u7684ViT\u6a21\u578b\u3002 \u89c6\u9891\u7406\u89e3\uff1a \u901a\u8fc7\u4e22\u5f03\u65f6\u95f4\u548c\u7a7a\u95f4\u4e0a\u7684\u5197\u4f59token\uff0c\u9ad8\u6548\u5904\u7406\u89c6\u9891\u5e8f\u5217\u3002 \u533b\u5b66\u5f71\u50cf\u5206\u6790\uff1a \u52a0\u901f\u5bf9\u5927\u578b\u533b\u5b66\u56fe\u50cf\u7684\u5206\u6790\uff0c\u53ef\u80fd\u7f29\u77ed\u8bca\u65ad\u65f6\u95f4\u3002 \u5176\u4ed6ViT-based\u4efb\u52a1\uff1a \u4f8b\u5982\u76ee\u6807\u68c0\u6d4b\u3001\u8bed\u4e49\u5206\u5272\u3001\u5b9e\u4f8b\u5206\u5272\u4ee5\u53ca\u4f7f\u7528ViT\u4f5c\u4e3a\u9aa8\u5e72\u7f51\u7edc\u7684\u751f\u6210\u6a21\u578b\uff0c\u8fd9\u4e9b\u4efb\u52a1\u4e2dViT\u4f5c\u4e3a\u7279\u5f81\u63d0\u53d6\u5668\uff0c\u5176\u6548\u7387\u63d0\u5347\u5c06\u5e26\u6765\u6574\u4f53\u6027\u80fd\u7684\u6539\u5584\u3002 5. \u4ece\u6458\u8981\u4e2d\u53ef\u63a8\u65ad\u7684\u5c40\u9650\u6027 (Limitations that can be inferred from the abstract) \u201c\u6781\u5c0f\u201d\u7cbe\u5ea6\u635f\u5931\u7684\u91cf\u5316\uff1a \u6458\u8981\u4e2d\u63d0\u5230\u201cminimal accuracy degradation\u201d\uff0c\u4f46\u201c\u6781\u5c0f\u201d\u662f\u4e00\u4e2a\u76f8\u5bf9\u6982\u5ff5\u3002\u5728\u5bf9\u7cbe\u5ea6\u8981\u6c42\u6781\u9ad8\u7684\u5e94\u7528\u4e2d\uff0c\u5373\u4f7f\u662f\u5fae\u5c0f\u7684\u635f\u5931\u4e5f\u53ef\u80fd\u65e0\u6cd5\u63a5\u53d7\u3002\u5177\u4f53\u7684FLOPs-\u7cbe\u5ea6\u6743\u8861\u66f2\u7ebf\uff08trade-off curve\uff09\u672a\u5728\u6458\u8981\u4e2d\u8be6\u7ec6\u8bf4\u660e\u3002 \u6307\u5bfc\u6a21\u578b\u7684\u5f00\u9500\u548c\u83b7\u53d6\uff1a \u5c3d\u7ba1\u6307\u5bfc\u6a21\u578b\u662f\u201c\u8f7b\u91cf\u7ea7\u201d\u7684\uff0c\u4f46\u5b83\u4ecd\u7136\u5f15\u5165\u4e86\u989d\u5916\u7684\u8ba1\u7b97\u5f00\u9500\u3002\u6458\u8981\u6ca1\u6709\u91cf\u5316\u8fd9\u79cd\u5f00\u9500\u76f8\u5bf9\u4e8e\u8282\u7701\u7684FLOPs\u7684\u6bd4\u4f8b\uff0c\u4e5f\u6ca1\u6709\u8bf4\u660e\u8fd9\u4e2a\u6307\u5bfc\u6a21\u578b\u662f\u5982\u4f55\u83b7\u53d6\u6216\u8bad\u7ec3\u7684\uff08\u4f8b\u5982\uff0c\u662f\u5426\u9700\u8981\u9884\u8bad\u7ec3\u3001\u662f\u5426\u9700\u8981\u7279\u5b9a\u6570\u636e\uff0c\u6216\u8005\u662f\u5426\u662f\u81ea\u76d1\u7763\u7684\uff09\u3002\u867d\u7136token\u4e22\u5f03\u6846\u67b6\u662f\u8bad\u7ec3\u65e0\u5173\u7684\uff0c\u4f46\u6307\u5bfc\u6a21\u578b\u672c\u8eab\u53ef\u80fd\u4e0d\u662f\u3002 Token\u91cd\u8981\u6027\u4f30\u8ba1\u7684\u9c81\u68d2\u6027\uff1a \u6846\u67b6\u7684\u6709\u6548\u6027\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u6307\u5bfc\u6a21\u578b\u51c6\u786e\u4f30\u8ba1token\u91cd\u8981\u6027\u7684\u80fd\u529b\u3002\u5bf9\u4e8e\u5206\u5e03\u5916\u6570\u636e\uff08out-of-distribution data\uff09\u3001\u5bf9\u6297\u6027\u6837\u672c\u6216\u56fe\u50cf\u4e2d\u7ec6\u5fae\u7279\u5f81\u7684\u9c81\u68d2\u6027\u53ef\u80fd\u662f\u4e00\u4e2a\u6f5c\u5728\u95ee\u9898\u3002 \u5bf9\u590d\u6742\u4efb\u52a1\u7684\u6cdb\u5316\u80fd\u529b\uff1a \u8bc4\u4f30\u4e3b\u8981\u5728\u201c\u6807\u51c6\u56fe\u50cf\u5206\u7c7b\u57fa\u51c6\u201d\u4e0a\u8fdb\u884c\u3002\u5bf9\u4e8e\u66f4\u590d\u6742\u7684\u4efb\u52a1\uff0c\u5982\u9700\u8981\u7cbe\u7ec6\u50cf\u7d20\u7ea7\u4fe1\u606f\u7684\u5bc6\u96c6\u9884\u6d4b\u4efb\u52a1\uff08\u5982\u5206\u5272\u3001\u68c0\u6d4b\uff09\uff0c\u4e22\u5f03token\u53ef\u80fd\u4f1a\u5bf9\u6027\u80fd\u4ea7\u751f\u66f4\u5927\u7684\u8d1f\u9762\u5f71\u54cd\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u4efb\u52a1\u901a\u5e38\u5bf9\u5c40\u90e8\u7ec6\u8282\u66f4\u654f\u611f\u3002 \u8d85\u53c2\u6570\u8c03\u4f18\uff1a Token\u4e22\u5f03\u9608\u503c\u901a\u5e38\u9700\u8981\u8fdb\u884c\u8c03\u4f18\u3002\u867d\u7136\u6846\u67b6\u672c\u8eab\u662f\u8bad\u7ec3\u65e0\u5173\u7684\uff0c\u4f46\u5728\u65b0\u7684ViT\u6a21\u578b\u6216\u6570\u636e\u96c6\u4e0a\u627e\u5230\u6700\u4f73\u7684\u4e22\u5f03\u7b56\u7565\u53ef\u80fd\u4ecd\u9700\u8981\u4e00\u5b9a\u7684\u7ecf\u9a8c\u6027\u641c\u7d22\u3002 Key Findings: To reduce inference costs in large ViTs without compromising accuracy, we propose TinyDrop, a training-free token dropping framework guided by a lightweight vision model. Links: PDF arXiv","title":"\u8bba\u6587\u6458\u8981\u5206\u6790\uff1aTinyDrop: Tiny Model Guided Token Dropping for Vision Transformers"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-07/","text":"Arxiv Computer Vision Papers - 2025-09-07 Executive Summary \u597d\u7684\uff0c\u8fd9\u662f\u4e00\u4efd\u9488\u5bf9\u60a8\u63d0\u4f9b\u7684\u6bcf\u65e5\u62a5\u544a\u7684\u6267\u884c\u6458\u8981\uff0c\u5373\u4f7f\u4eca\u5929\u6ca1\u6709\u65b0\u8bba\u6587\uff0c\u6211\u4eec\u4e5f\u53ef\u4ee5\u6839\u636e\u5e38\u89c1\u60c5\u51b5\u548c\u9884\u671f\u6765\u6784\u5efa\u4e00\u4e2a\u6709\u7528\u7684\u6a21\u677f\u3002 \u6bcf\u65e5ArXiv\u8ba1\u7b97\u673a\u89c6\u89c9\u8bba\u6587\u6267\u884c\u6458\u8981 \u65e5\u671f\uff1a [\u4eca\u65e5\u65e5\u671f] \u62a5\u544a\u4eba\uff1a [\u60a8\u7684\u59d3\u540d/\u7814\u7a76\u52a9\u7406] \u4eca\u65e5\u8bba\u6587\u6570\u91cf\uff1a 0 1. \u4e3b\u8981\u4e3b\u9898\u4e0e\u8d8b\u52bf\u6982\u8ff0\uff1a \u9274\u4e8e\u4eca\u65e5\u65e0\u65b0\u8bba\u6587\u53d1\u5e03\uff0c\u6211\u4eec\u65e0\u6cd5\u76f4\u63a5\u8bc6\u522b\u4eca\u65e5\u7684\u5177\u4f53\u4e3b\u9898\u3002\u7136\u800c\uff0c\u6839\u636e\u8fd1\u671fArXiv\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u7684\u666e\u904d\u8d8b\u52bf\uff0c\u6211\u4eec\u53ef\u4ee5\u9884\u671f\u672a\u6765\u51e0\u5929\u7684\u8bba\u6587\u53ef\u80fd\u4f1a\u7ee7\u7eed\u805a\u7126\u4e8e\u4ee5\u4e0b\u51e0\u4e2a\u6838\u5fc3\u9886\u57df\uff1a \u57fa\u7840\u6a21\u578b\u4e0e\u591a\u6a21\u6001\u5b66\u4e60\uff1a \u6301\u7eed\u63a2\u7d22\u66f4\u5927\u3001\u66f4\u901a\u7528\u7684\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08\u5982ViT\u53d8\u4f53\u3001MAE\u3001SAM\u7b49\uff09\uff0c\u4ee5\u53ca\u5b83\u4eec\u4e0e\u8bed\u8a00\u3001\u97f3\u9891\u7b49\u5176\u4ed6\u6a21\u6001\u7684\u878d\u5408\uff0c\u4ee5\u5b9e\u73b0\u66f4\u5f3a\u5927\u7684\u901a\u7528\u611f\u77e5\u80fd\u529b\u3002 \u9ad8\u6548\u4e0e\u8f7b\u91cf\u5316\u6a21\u578b\uff1a \u9488\u5bf9\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\uff08\u5982\u8fb9\u7f18\u8bbe\u5907\u3001\u79fb\u52a8\u7aef\uff09\u7684\u8f7b\u91cf\u7ea7\u6a21\u578b\u8bbe\u8ba1\u3001\u6a21\u578b\u538b\u7f29\uff08\u526a\u679d\u3001\u91cf\u5316\u3001\u77e5\u8bc6\u84b8\u998f\uff09\u548c\u9ad8\u6548\u63a8\u7406\u65b9\u6cd5\u4ecd\u662f\u91cd\u8981\u65b9\u5411\u3002 3D\u89c6\u89c9\u4e0e\u65b0\u8303\u5f0f\uff1a NeRF\uff08\u795e\u7ecf\u8f90\u5c04\u573a\uff09\u53ca\u5176\u53d8\u4f53\u7684\u6301\u7eed\u6f14\u8fdb\uff0c\u4ee5\u53ca\u5176\u4ed63D\u91cd\u5efa\u3001\u573a\u666f\u7406\u89e3\u3001\u59ff\u6001\u4f30\u8ba1\u548c\u673a\u5668\u4eba\u89c6\u89c9\u76f8\u5173\u7684\u7814\u7a76\u3002 \u53ef\u89e3\u91ca\u6027\u4e0e\u9c81\u68d2\u6027\uff1a \u63d0\u9ad8\u6a21\u578b\u51b3\u7b56\u7684\u53ef\u89e3\u91ca\u6027\u3001\u5bf9\u6297\u653b\u51fb\u7684\u9c81\u68d2\u6027\u4ee5\u53ca\u5728\u590d\u6742\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002 \u81ea\u76d1\u7763\u4e0e\u534a\u76d1\u7763\u5b66\u4e60\uff1a \u51cf\u5c11\u5bf9\u5927\u91cf\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u548c\u534a\u76d1\u7763\u5b66\u4e60\u6765\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002 2. \u7279\u522b\u91cd\u8981\u6216\u521b\u65b0\u8bba\u6587\u4eae\u70b9\uff1a \uff08\u7531\u4e8e\u4eca\u65e5\u65e0\u8bba\u6587\uff0c\u6b64\u90e8\u5206\u7559\u7a7a\u3002\u5728\u6709\u8bba\u6587\u65f6\uff0c\u6211\u4eec\u4f1a\u6311\u90091-2\u7bc7\u5177\u6709\u7a81\u7834\u6027\u65b9\u6cd5\u3001\u663e\u8457\u6027\u80fd\u63d0\u5347\u6216\u5f00\u8f9f\u65b0\u7814\u7a76\u65b9\u5411\u7684\u8bba\u6587\u8fdb\u884c\u8be6\u7ec6\u4ecb\u7ecd\u3002\uff09 3. \u65b0\u5174\u7814\u7a76\u65b9\u5411\u6216\u6280\u672f\uff1a \uff08\u7531\u4e8e\u4eca\u65e5\u65e0\u8bba\u6587\uff0c\u6b64\u90e8\u5206\u7559\u7a7a\u3002\u5728\u6709\u8bba\u6587\u65f6\uff0c\u6211\u4eec\u4f1a\u6839\u636e\u8bba\u6587\u5185\u5bb9\u8bc6\u522b\u51fa\u65b0\u7684\u6a21\u578b\u67b6\u6784\u3001\u8bad\u7ec3\u7b56\u7565\u3001\u6570\u636e\u589e\u5f3a\u6280\u672f\u6216\u5e94\u7528\u9886\u57df\u3002\uff09 4. \u5efa\u8bae\u9605\u8bfb\u5168\u6587\u7684\u8bba\u6587\uff1a \uff08\u7531\u4e8e\u4eca\u65e5\u65e0\u8bba\u6587\uff0c\u6b64\u90e8\u5206\u7559\u7a7a\u3002\u5728\u6709\u8bba\u6587\u65f6\uff0c\u6211\u4eec\u4f1a\u6839\u636e\u8bba\u6587\u7684\u521b\u65b0\u6027\u3001\u6f5c\u5728\u5f71\u54cd\u529b\u4ee5\u53ca\u4e0e\u6211\u4eec\u5f53\u524d\u7814\u7a76\u65b9\u5411\u7684\u76f8\u5173\u6027\uff0c\u63a8\u83501-3\u7bc7\u6700\u503c\u5f97\u6df1\u5165\u9605\u8bfb\u7684\u8bba\u6587\u3002\uff09 \u7ed9\u5fd9\u788c\u7814\u7a76\u4eba\u5458\u7684\u5feb\u901f\u63d0\u793a\uff1a \u867d\u7136\u4eca\u5929\u6ca1\u6709\u65b0\u8bba\u6587\uff0c\u4f46\u8bf7\u4fdd\u6301\u5bf9\u4e0a\u8ff0\u4e3b\u8981\u8d8b\u52bf\u7684\u5173\u6ce8\u3002\u5f53\u6709\u65b0\u8bba\u6587\u53d1\u5e03\u65f6\uff0c\u6b64\u6458\u8981\u5c06\u5e2e\u52a9\u60a8\uff1a \u5feb\u901f\u7b5b\u9009\uff1a \u8fc5\u901f\u4e86\u89e3\u54ea\u4e9b\u8bba\u6587\u4e0e\u60a8\u7684\u7814\u7a76\u65b9\u5411\u6700\u76f8\u5173\u3002 \u8282\u7701\u65f6\u95f4\uff1a \u901a\u8fc7\u6458\u8981\u4e86\u89e3\u8bba\u6587\u6838\u5fc3\u601d\u60f3\uff0c\u51b3\u5b9a\u662f\u5426\u9700\u8981\u6df1\u5165\u9605\u8bfb\u3002 \u4fdd\u6301\u524d\u6cbf\uff1a \u53ca\u65f6\u638c\u63e1\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u7684\u6700\u65b0\u8fdb\u5c55\u548c\u6280\u672f\u7a81\u7834\u3002 \u671f\u5f85\u660e\u65e5\u7684\u66f4\u65b0\uff01 Table of Contents Papers","title":"Arxiv Computer Vision Papers - 2025-09-07"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-07/#arxiv-computer-vision-papers-2025-09-07","text":"","title":"Arxiv Computer Vision Papers - 2025-09-07"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-07/#executive-summary","text":"\u597d\u7684\uff0c\u8fd9\u662f\u4e00\u4efd\u9488\u5bf9\u60a8\u63d0\u4f9b\u7684\u6bcf\u65e5\u62a5\u544a\u7684\u6267\u884c\u6458\u8981\uff0c\u5373\u4f7f\u4eca\u5929\u6ca1\u6709\u65b0\u8bba\u6587\uff0c\u6211\u4eec\u4e5f\u53ef\u4ee5\u6839\u636e\u5e38\u89c1\u60c5\u51b5\u548c\u9884\u671f\u6765\u6784\u5efa\u4e00\u4e2a\u6709\u7528\u7684\u6a21\u677f\u3002 \u6bcf\u65e5ArXiv\u8ba1\u7b97\u673a\u89c6\u89c9\u8bba\u6587\u6267\u884c\u6458\u8981 \u65e5\u671f\uff1a [\u4eca\u65e5\u65e5\u671f] \u62a5\u544a\u4eba\uff1a [\u60a8\u7684\u59d3\u540d/\u7814\u7a76\u52a9\u7406] \u4eca\u65e5\u8bba\u6587\u6570\u91cf\uff1a 0 1. \u4e3b\u8981\u4e3b\u9898\u4e0e\u8d8b\u52bf\u6982\u8ff0\uff1a \u9274\u4e8e\u4eca\u65e5\u65e0\u65b0\u8bba\u6587\u53d1\u5e03\uff0c\u6211\u4eec\u65e0\u6cd5\u76f4\u63a5\u8bc6\u522b\u4eca\u65e5\u7684\u5177\u4f53\u4e3b\u9898\u3002\u7136\u800c\uff0c\u6839\u636e\u8fd1\u671fArXiv\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u7684\u666e\u904d\u8d8b\u52bf\uff0c\u6211\u4eec\u53ef\u4ee5\u9884\u671f\u672a\u6765\u51e0\u5929\u7684\u8bba\u6587\u53ef\u80fd\u4f1a\u7ee7\u7eed\u805a\u7126\u4e8e\u4ee5\u4e0b\u51e0\u4e2a\u6838\u5fc3\u9886\u57df\uff1a \u57fa\u7840\u6a21\u578b\u4e0e\u591a\u6a21\u6001\u5b66\u4e60\uff1a \u6301\u7eed\u63a2\u7d22\u66f4\u5927\u3001\u66f4\u901a\u7528\u7684\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08\u5982ViT\u53d8\u4f53\u3001MAE\u3001SAM\u7b49\uff09\uff0c\u4ee5\u53ca\u5b83\u4eec\u4e0e\u8bed\u8a00\u3001\u97f3\u9891\u7b49\u5176\u4ed6\u6a21\u6001\u7684\u878d\u5408\uff0c\u4ee5\u5b9e\u73b0\u66f4\u5f3a\u5927\u7684\u901a\u7528\u611f\u77e5\u80fd\u529b\u3002 \u9ad8\u6548\u4e0e\u8f7b\u91cf\u5316\u6a21\u578b\uff1a \u9488\u5bf9\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\uff08\u5982\u8fb9\u7f18\u8bbe\u5907\u3001\u79fb\u52a8\u7aef\uff09\u7684\u8f7b\u91cf\u7ea7\u6a21\u578b\u8bbe\u8ba1\u3001\u6a21\u578b\u538b\u7f29\uff08\u526a\u679d\u3001\u91cf\u5316\u3001\u77e5\u8bc6\u84b8\u998f\uff09\u548c\u9ad8\u6548\u63a8\u7406\u65b9\u6cd5\u4ecd\u662f\u91cd\u8981\u65b9\u5411\u3002 3D\u89c6\u89c9\u4e0e\u65b0\u8303\u5f0f\uff1a NeRF\uff08\u795e\u7ecf\u8f90\u5c04\u573a\uff09\u53ca\u5176\u53d8\u4f53\u7684\u6301\u7eed\u6f14\u8fdb\uff0c\u4ee5\u53ca\u5176\u4ed63D\u91cd\u5efa\u3001\u573a\u666f\u7406\u89e3\u3001\u59ff\u6001\u4f30\u8ba1\u548c\u673a\u5668\u4eba\u89c6\u89c9\u76f8\u5173\u7684\u7814\u7a76\u3002 \u53ef\u89e3\u91ca\u6027\u4e0e\u9c81\u68d2\u6027\uff1a \u63d0\u9ad8\u6a21\u578b\u51b3\u7b56\u7684\u53ef\u89e3\u91ca\u6027\u3001\u5bf9\u6297\u653b\u51fb\u7684\u9c81\u68d2\u6027\u4ee5\u53ca\u5728\u590d\u6742\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002 \u81ea\u76d1\u7763\u4e0e\u534a\u76d1\u7763\u5b66\u4e60\uff1a \u51cf\u5c11\u5bf9\u5927\u91cf\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u548c\u534a\u76d1\u7763\u5b66\u4e60\u6765\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002 2. \u7279\u522b\u91cd\u8981\u6216\u521b\u65b0\u8bba\u6587\u4eae\u70b9\uff1a \uff08\u7531\u4e8e\u4eca\u65e5\u65e0\u8bba\u6587\uff0c\u6b64\u90e8\u5206\u7559\u7a7a\u3002\u5728\u6709\u8bba\u6587\u65f6\uff0c\u6211\u4eec\u4f1a\u6311\u90091-2\u7bc7\u5177\u6709\u7a81\u7834\u6027\u65b9\u6cd5\u3001\u663e\u8457\u6027\u80fd\u63d0\u5347\u6216\u5f00\u8f9f\u65b0\u7814\u7a76\u65b9\u5411\u7684\u8bba\u6587\u8fdb\u884c\u8be6\u7ec6\u4ecb\u7ecd\u3002\uff09 3. \u65b0\u5174\u7814\u7a76\u65b9\u5411\u6216\u6280\u672f\uff1a \uff08\u7531\u4e8e\u4eca\u65e5\u65e0\u8bba\u6587\uff0c\u6b64\u90e8\u5206\u7559\u7a7a\u3002\u5728\u6709\u8bba\u6587\u65f6\uff0c\u6211\u4eec\u4f1a\u6839\u636e\u8bba\u6587\u5185\u5bb9\u8bc6\u522b\u51fa\u65b0\u7684\u6a21\u578b\u67b6\u6784\u3001\u8bad\u7ec3\u7b56\u7565\u3001\u6570\u636e\u589e\u5f3a\u6280\u672f\u6216\u5e94\u7528\u9886\u57df\u3002\uff09 4. \u5efa\u8bae\u9605\u8bfb\u5168\u6587\u7684\u8bba\u6587\uff1a \uff08\u7531\u4e8e\u4eca\u65e5\u65e0\u8bba\u6587\uff0c\u6b64\u90e8\u5206\u7559\u7a7a\u3002\u5728\u6709\u8bba\u6587\u65f6\uff0c\u6211\u4eec\u4f1a\u6839\u636e\u8bba\u6587\u7684\u521b\u65b0\u6027\u3001\u6f5c\u5728\u5f71\u54cd\u529b\u4ee5\u53ca\u4e0e\u6211\u4eec\u5f53\u524d\u7814\u7a76\u65b9\u5411\u7684\u76f8\u5173\u6027\uff0c\u63a8\u83501-3\u7bc7\u6700\u503c\u5f97\u6df1\u5165\u9605\u8bfb\u7684\u8bba\u6587\u3002\uff09 \u7ed9\u5fd9\u788c\u7814\u7a76\u4eba\u5458\u7684\u5feb\u901f\u63d0\u793a\uff1a \u867d\u7136\u4eca\u5929\u6ca1\u6709\u65b0\u8bba\u6587\uff0c\u4f46\u8bf7\u4fdd\u6301\u5bf9\u4e0a\u8ff0\u4e3b\u8981\u8d8b\u52bf\u7684\u5173\u6ce8\u3002\u5f53\u6709\u65b0\u8bba\u6587\u53d1\u5e03\u65f6\uff0c\u6b64\u6458\u8981\u5c06\u5e2e\u52a9\u60a8\uff1a \u5feb\u901f\u7b5b\u9009\uff1a \u8fc5\u901f\u4e86\u89e3\u54ea\u4e9b\u8bba\u6587\u4e0e\u60a8\u7684\u7814\u7a76\u65b9\u5411\u6700\u76f8\u5173\u3002 \u8282\u7701\u65f6\u95f4\uff1a \u901a\u8fc7\u6458\u8981\u4e86\u89e3\u8bba\u6587\u6838\u5fc3\u601d\u60f3\uff0c\u51b3\u5b9a\u662f\u5426\u9700\u8981\u6df1\u5165\u9605\u8bfb\u3002 \u4fdd\u6301\u524d\u6cbf\uff1a \u53ca\u65f6\u638c\u63e1\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u7684\u6700\u65b0\u8fdb\u5c55\u548c\u6280\u672f\u7a81\u7834\u3002 \u671f\u5f85\u660e\u65e5\u7684\u66f4\u65b0\uff01","title":"Executive Summary"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-07/#table-of-contents","text":"","title":"Table of Contents"},{"location":"other_categories/daily/arxiv_cv_report_2025-09-07/#papers","text":"","title":"Papers"},{"location":"other_categories/depth_completion/DNet/","text":"Toward Hierarchical Self-Supervised Monocular Absolute Depth Estimation for Autonomous Driving Applications \u672c\u6587\u6307\u51fa\u76ee\u524d\u7684\u6df1\u5ea6\u9884\u6d4b\u7f51\u7edc\u6709\u4e24\u5927\u7f3a\u9677\uff0c\u4e00\u4e2a\u662f\u5bf9object level\u7684\u6df1\u5ea6\u9884\u6d4b\u4e0d\u51c6\u786e\uff0c\u53e6\u4e00\u4e2a\u662f\u7f3a\u4e4fscale\u4fe1\u606f\u3002 \u672c\u6587\u5206\u522b\u63d0\u51fa DCP layer \u4ee5\u53ca DGC layer \u5206\u522b\u5904\u7406\u8fd9\u4e24\u4e2a\u95ee\u9898. \u4e0b\u56fe\u5c55\u793a\u4e86\u4e0e\u4f20\u7edf\u65b9\u6848\u7684\u5bf9\u6bd4 PipeLine \u7f51\u7edc\u9996\u5148\u4f7f\u7528DCP\u7f51\u7edc\u9884\u6d4b\u5b9e\u73b0\u76f8\u5bf9\u6df1\u5ea6\u9884\u6d4b\uff0c\u7136\u540e\u5229\u7528\u5730\u9762\u4e0e\u76f8\u673a\u4e4b\u95f4\u7684\u51e0\u4f55\u5173\u7cfb\uff0c\u63d0\u53d6\u51fa\u5730\u9762\u70b9\uff0c\u7136\u540e\u8ba1\u7b97\u76f8\u673a\u7684\u9ad8\u5ea6\uff0c\u5176\u4e2d\u4f4d\u6570\u7528\u6765\u4f30\u8ba1scale factor. \u5f97\u5230\u7684scale factor\u4e0e\u76f8\u5bf9\u6df1\u5ea6\u76f8\u4e58\u5f97\u5230\u7edd\u5bf9\u6df1\u5ea6\u3002 \u57fa\u51c6\u6a21\u578b\u4e3a MonoDepth2 , \u7f51\u7edc\u4e0a\u57fa\u4e8eMonoDepth2\u7684\u4fee\u6539\u90e8\u5206: \u603b\u4f53Loss\u5bf9\u4f4eresolution\u7684\u6743\u91cd\u4e0b\u964d\u4e86\uff0c\u4f5c\u8005\u7684 code \u662f\u6309\u7167 2 ** scale DCP Layer\uff0c\u5176\u5b9e\u5c31\u662fDensely connected layers. Scale Recovery : \u8ba1\u7b97surface Norm \u5728\u56fe\u7247\u5750\u6807\u4e0a\uff0c\u4ee5\u76ee\u6807\u70b9\u4e3a\u4e2d\u5fc3\u7684\u9644\u8fd18\u4e2a\u70b9\uff0c\u5982\u56fe\u4e2d\u7684\u989c\u8272\u4e00\u6837\u5206\u6210\u56db\u7ec4\uff0c\u7531\u540c\u989c\u8272\u7684\u4e24\u4e2a\u77e2\u91cf\u53c9\u4e58\u4e00\u5171\u80fd\u5f97\u5230\u56db\u4e2a\u6cd5\u5411\u91cf\u7684\u4f30\u8ba1\uff0c\u6700\u7ec8\u4f30\u8ba1\u7684\u6cd5\u5411\u91cf\u4e3a: \\mathbf{N}\\left(\\mathbf{P}_{i, j}\\right)=\\frac{\\sum_{g} \\mathbf{n} /\\left\\|\\mathbf{n}_{g}\\right\\|_{2}}{4} \u5176 code \u9ad8\u5ea6\u66b4\u529b\u3002 \u5730\u9762\u68c0\u6d4b \u7406\u60f3\u7684\u5730\u9762\u7684\u6cd5\u5411\u4e3a\u3000 \\tilde{\\bold{n}} = (0, 1, 0)^\\top \u4f5c\u8005\u4f7f\u7528\u57fa\u4e8e\u89d2\u5ea6\u7684\u76f8\u4f3c\u5ea6\u51fd\u6570 S=s\\left(\\mathbf{P}_{i, j}\\right)=\\left|\\angle\\left(\\tilde{\\mathbf{n}}, \\mathbf{N}\\left(\\mathbf{P}_{i, j}\\right)\\right)\\right|=\\left|\\arccos \\frac{\\tilde{\\mathbf{n}} \\cdot \\mathbf{P}_{i, j}}{\\|\\tilde{\\mathbf{n}}\\|\\left\\|\\mathbf{P}_{i, j}\\right\\|}\\right| \u7531\u4e8e\u76f8\u673a\u4e0d\u4e00\u5b9a\u662f\u4e0e\u5730\u9762\u6cd5\u5411\u5b8c\u5168\u5782\u76f4\u7684\uff0c\u6240\u4ee5\u4f5c\u8005\u7559\u4e86\u4e00\u4e2athreshold, \u4ee3\u7801\u4e2d\u8fd9\u4e2a\u89d2\u5ea6\u5dee\u7684\u9608\u503c\u4e3a\u6b63\u8d1f\u4e94\u5ea6\u3002 \u76f8\u5bf9\u9ad8\u5ea6\u4f30\u8ba1 \u7531\u4e0a\u56fe\uff0c\u53ef\u4ee5\u6839\u636e\u5730\u9762\u70b9\u4e0e\u76f8\u673a\u7684\u76f8\u5bf9\u77e2\u91cf\u4ee5\u53ca\u5176\u8be5\u70b9\u7684\u6cd5\u5411\u8ba1\u7b97\u76f8\u673a\u7684\u9ad8\u5ea6\uff0c h\\left(\\mathbf{P}_{i, j}\\right)=\\mathbf{N}\\left(\\mathbf{P}_{i, j}\\right)^{\\top} \\cdot \\overline{\\mathbf{O P}_{i, j}} tu","title":"Toward Hierarchical Self-Supervised Monocular Absolute Depth Estimation for Autonomous Driving Applications"},{"location":"other_categories/depth_completion/DNet/#toward-hierarchical-self-supervised-monocular-absolute-depth-estimation-for-autonomous-driving-applications","text":"\u672c\u6587\u6307\u51fa\u76ee\u524d\u7684\u6df1\u5ea6\u9884\u6d4b\u7f51\u7edc\u6709\u4e24\u5927\u7f3a\u9677\uff0c\u4e00\u4e2a\u662f\u5bf9object level\u7684\u6df1\u5ea6\u9884\u6d4b\u4e0d\u51c6\u786e\uff0c\u53e6\u4e00\u4e2a\u662f\u7f3a\u4e4fscale\u4fe1\u606f\u3002 \u672c\u6587\u5206\u522b\u63d0\u51fa DCP layer \u4ee5\u53ca DGC layer \u5206\u522b\u5904\u7406\u8fd9\u4e24\u4e2a\u95ee\u9898. \u4e0b\u56fe\u5c55\u793a\u4e86\u4e0e\u4f20\u7edf\u65b9\u6848\u7684\u5bf9\u6bd4","title":"Toward Hierarchical Self-Supervised Monocular Absolute Depth Estimation for Autonomous Driving Applications"},{"location":"other_categories/depth_completion/DNet/#pipeline","text":"\u7f51\u7edc\u9996\u5148\u4f7f\u7528DCP\u7f51\u7edc\u9884\u6d4b\u5b9e\u73b0\u76f8\u5bf9\u6df1\u5ea6\u9884\u6d4b\uff0c\u7136\u540e\u5229\u7528\u5730\u9762\u4e0e\u76f8\u673a\u4e4b\u95f4\u7684\u51e0\u4f55\u5173\u7cfb\uff0c\u63d0\u53d6\u51fa\u5730\u9762\u70b9\uff0c\u7136\u540e\u8ba1\u7b97\u76f8\u673a\u7684\u9ad8\u5ea6\uff0c\u5176\u4e2d\u4f4d\u6570\u7528\u6765\u4f30\u8ba1scale factor. \u5f97\u5230\u7684scale factor\u4e0e\u76f8\u5bf9\u6df1\u5ea6\u76f8\u4e58\u5f97\u5230\u7edd\u5bf9\u6df1\u5ea6\u3002 \u57fa\u51c6\u6a21\u578b\u4e3a MonoDepth2 , \u7f51\u7edc\u4e0a\u57fa\u4e8eMonoDepth2\u7684\u4fee\u6539\u90e8\u5206: \u603b\u4f53Loss\u5bf9\u4f4eresolution\u7684\u6743\u91cd\u4e0b\u964d\u4e86\uff0c\u4f5c\u8005\u7684 code \u662f\u6309\u7167 2 ** scale DCP Layer\uff0c\u5176\u5b9e\u5c31\u662fDensely connected layers. Scale Recovery :","title":"PipeLine"},{"location":"other_categories/depth_completion/DNet/#surface-norm","text":"\u5728\u56fe\u7247\u5750\u6807\u4e0a\uff0c\u4ee5\u76ee\u6807\u70b9\u4e3a\u4e2d\u5fc3\u7684\u9644\u8fd18\u4e2a\u70b9\uff0c\u5982\u56fe\u4e2d\u7684\u989c\u8272\u4e00\u6837\u5206\u6210\u56db\u7ec4\uff0c\u7531\u540c\u989c\u8272\u7684\u4e24\u4e2a\u77e2\u91cf\u53c9\u4e58\u4e00\u5171\u80fd\u5f97\u5230\u56db\u4e2a\u6cd5\u5411\u91cf\u7684\u4f30\u8ba1\uff0c\u6700\u7ec8\u4f30\u8ba1\u7684\u6cd5\u5411\u91cf\u4e3a: \\mathbf{N}\\left(\\mathbf{P}_{i, j}\\right)=\\frac{\\sum_{g} \\mathbf{n} /\\left\\|\\mathbf{n}_{g}\\right\\|_{2}}{4} \u5176 code \u9ad8\u5ea6\u66b4\u529b\u3002","title":"\u8ba1\u7b97surface Norm"},{"location":"other_categories/depth_completion/DNet/#_1","text":"\u7406\u60f3\u7684\u5730\u9762\u7684\u6cd5\u5411\u4e3a\u3000 \\tilde{\\bold{n}} = (0, 1, 0)^\\top \u4f5c\u8005\u4f7f\u7528\u57fa\u4e8e\u89d2\u5ea6\u7684\u76f8\u4f3c\u5ea6\u51fd\u6570 S=s\\left(\\mathbf{P}_{i, j}\\right)=\\left|\\angle\\left(\\tilde{\\mathbf{n}}, \\mathbf{N}\\left(\\mathbf{P}_{i, j}\\right)\\right)\\right|=\\left|\\arccos \\frac{\\tilde{\\mathbf{n}} \\cdot \\mathbf{P}_{i, j}}{\\|\\tilde{\\mathbf{n}}\\|\\left\\|\\mathbf{P}_{i, j}\\right\\|}\\right| \u7531\u4e8e\u76f8\u673a\u4e0d\u4e00\u5b9a\u662f\u4e0e\u5730\u9762\u6cd5\u5411\u5b8c\u5168\u5782\u76f4\u7684\uff0c\u6240\u4ee5\u4f5c\u8005\u7559\u4e86\u4e00\u4e2athreshold, \u4ee3\u7801\u4e2d\u8fd9\u4e2a\u89d2\u5ea6\u5dee\u7684\u9608\u503c\u4e3a\u6b63\u8d1f\u4e94\u5ea6\u3002","title":"\u5730\u9762\u68c0\u6d4b"},{"location":"other_categories/depth_completion/DNet/#_2","text":"\u7531\u4e0a\u56fe\uff0c\u53ef\u4ee5\u6839\u636e\u5730\u9762\u70b9\u4e0e\u76f8\u673a\u7684\u76f8\u5bf9\u77e2\u91cf\u4ee5\u53ca\u5176\u8be5\u70b9\u7684\u6cd5\u5411\u8ba1\u7b97\u76f8\u673a\u7684\u9ad8\u5ea6\uff0c h\\left(\\mathbf{P}_{i, j}\\right)=\\mathbf{N}\\left(\\mathbf{P}_{i, j}\\right)^{\\top} \\cdot \\overline{\\mathbf{O P}_{i, j}} tu","title":"\u76f8\u5bf9\u9ad8\u5ea6\u4f30\u8ba1"},{"location":"other_categories/depth_completion/DeepLineEncode/","text":"Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction \u8fd9\u7bc7paper\u5728 GAC \u6587\u7ae0\u4e0e\u4ee3\u7801\u7684\u57fa\u7840\u8fdb\u4e00\u6b65\u5f00\u53d1. \u7531\u4e8e\u9a7e\u9a76\u573a\u666f\u5730\u9762\u4e0d\u4e00\u5b9a\u662f\u5e73\u6574\u7684,\u4f46\u662f\u5730\u9762\u4e0a\u7684\u7ebf\u7279\u5f81\u53ef\u4ee5\u8f85\u52a9\u7f51\u7edc\u5b66\u4e60\u8fd9\u4e9b\u4fe1\u606f.\u672c\u6587\u5c31\u7740\u91cd\u4ece\u7ebf\u7279\u5f81\u63d0\u53d6\u6765\u5165\u624b\u8fdb\u4e00\u6b65\u63d0\u5347GAC\u7684\u6027\u80fd. \u5176\u4e3b\u8981\u4fee\u6539\u4e86Core Module\u7684\u6d41\u7a0b\u4ee3\u7801. class SelfMask(nn.Module): \"\"\" The Conv Soft-max operation on (a) subgraph to select the most significant line at each channel. \"\"\" def __init__(self, c): super(SelfMask, self).__init__() self.mask1 = nn.Conv2d(c, c, kernel_size=1) self.max1 = nn.Softmax(dim=-1) def forward(self, x): mask1 = self.mask1(x) n, c, h, w = mask1.shape[0], mask1.shape[1], mask1.shape[2], mask1.shape[3] mask1 = mask1.view(n, c, -1) mask1 = self.max1(mask1) mask1 = mask1.view(n, c, h, w) x1 = x * mask1 x1 = x1.sum(dim=-1,keepdim=True).sum(dim=-2,keepdim=True) return x1 class YoloMono3DCore(nn.Module): \"\"\"Some Information about YoloMono3DCore\"\"\" def __init__(self, backbone_arguments=dict()): super(YoloMono3DCore, self).__init__() self.backbone =resnet(**backbone_arguments) self.cord = nn.Sequential( CoordinateConv(256+512, 512, kernel_size=3, padding=1), nn.BatchNorm2d(512), nn.ReLU(True), nn.Conv2d(512, 512, kernel_size=3, padding=1), nn.BatchNorm2d(512), nn.ReLU(True)) # The coorindate Conv after line pooling on (b) subgraph self.vote_index = hough_transform(72,320,3.0,3.0) self.vote_index = torch.tensor(self.vote_index).cuda().contiguous().float() self.dht = HT(self.vote_index) # Hough Transform Layer self.dht_backbone = nn.Sequential( CoordinateConv(16, 64, kernel_size=3, padding=1), nn.BatchNorm2d(64), nn.ReLU(True), nn.Conv2d(64, 256, kernel_size=3, padding=1), nn.BatchNorm2d(256), nn.ReLU(True), SelfMask(256)) # Line Pooling Module self.bt = nn.Sequential( nn.Conv2d(256, 64, kernel_size=1), nn.BatchNorm2d(64), nn.ReLU(True), nn.Conv2d(64, 16, kernel_size=1), nn.BatchNorm2d(16), nn.ReLU(True)) # Compress the number of channels for hough transform. def forward(self, x): #x = self.backbone(x['image']) #x = x[0] x1, x2 = self.backbone.forward1(x['image']) # Take the ResNet's output from layer1 and layer2; scale 4, 8 x1 = 0.1 * x1 + 0.9 * x1.detach() x1 = self.bt(x1) # Reduce features before hough transform dht = self.dht(x1) # hough transform dht = self.dht_backbone(dht) # Line Pooling h, w = x2.shape[2], x2.shape[3] dht = dht.expand(-1, -1, h, w) x2 = torch.cat([x2, dht], 1) x2 = self.cord(x2) # Merge back x2 = self.backbone.forward2(x2) # ResNet at Scale 16 return x2","title":"Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction"},{"location":"other_categories/depth_completion/DeepLineEncode/#deep-line-encoding-for-monocular-3d-object-detection-and-depth-prediction","text":"\u8fd9\u7bc7paper\u5728 GAC \u6587\u7ae0\u4e0e\u4ee3\u7801\u7684\u57fa\u7840\u8fdb\u4e00\u6b65\u5f00\u53d1. \u7531\u4e8e\u9a7e\u9a76\u573a\u666f\u5730\u9762\u4e0d\u4e00\u5b9a\u662f\u5e73\u6574\u7684,\u4f46\u662f\u5730\u9762\u4e0a\u7684\u7ebf\u7279\u5f81\u53ef\u4ee5\u8f85\u52a9\u7f51\u7edc\u5b66\u4e60\u8fd9\u4e9b\u4fe1\u606f.\u672c\u6587\u5c31\u7740\u91cd\u4ece\u7ebf\u7279\u5f81\u63d0\u53d6\u6765\u5165\u624b\u8fdb\u4e00\u6b65\u63d0\u5347GAC\u7684\u6027\u80fd. \u5176\u4e3b\u8981\u4fee\u6539\u4e86Core Module\u7684\u6d41\u7a0b\u4ee3\u7801. class SelfMask(nn.Module): \"\"\" The Conv Soft-max operation on (a) subgraph to select the most significant line at each channel. \"\"\" def __init__(self, c): super(SelfMask, self).__init__() self.mask1 = nn.Conv2d(c, c, kernel_size=1) self.max1 = nn.Softmax(dim=-1) def forward(self, x): mask1 = self.mask1(x) n, c, h, w = mask1.shape[0], mask1.shape[1], mask1.shape[2], mask1.shape[3] mask1 = mask1.view(n, c, -1) mask1 = self.max1(mask1) mask1 = mask1.view(n, c, h, w) x1 = x * mask1 x1 = x1.sum(dim=-1,keepdim=True).sum(dim=-2,keepdim=True) return x1 class YoloMono3DCore(nn.Module): \"\"\"Some Information about YoloMono3DCore\"\"\" def __init__(self, backbone_arguments=dict()): super(YoloMono3DCore, self).__init__() self.backbone =resnet(**backbone_arguments) self.cord = nn.Sequential( CoordinateConv(256+512, 512, kernel_size=3, padding=1), nn.BatchNorm2d(512), nn.ReLU(True), nn.Conv2d(512, 512, kernel_size=3, padding=1), nn.BatchNorm2d(512), nn.ReLU(True)) # The coorindate Conv after line pooling on (b) subgraph self.vote_index = hough_transform(72,320,3.0,3.0) self.vote_index = torch.tensor(self.vote_index).cuda().contiguous().float() self.dht = HT(self.vote_index) # Hough Transform Layer self.dht_backbone = nn.Sequential( CoordinateConv(16, 64, kernel_size=3, padding=1), nn.BatchNorm2d(64), nn.ReLU(True), nn.Conv2d(64, 256, kernel_size=3, padding=1), nn.BatchNorm2d(256), nn.ReLU(True), SelfMask(256)) # Line Pooling Module self.bt = nn.Sequential( nn.Conv2d(256, 64, kernel_size=1), nn.BatchNorm2d(64), nn.ReLU(True), nn.Conv2d(64, 16, kernel_size=1), nn.BatchNorm2d(16), nn.ReLU(True)) # Compress the number of channels for hough transform. def forward(self, x): #x = self.backbone(x['image']) #x = x[0] x1, x2 = self.backbone.forward1(x['image']) # Take the ResNet's output from layer1 and layer2; scale 4, 8 x1 = 0.1 * x1 + 0.9 * x1.detach() x1 = self.bt(x1) # Reduce features before hough transform dht = self.dht(x1) # hough transform dht = self.dht_backbone(dht) # Line Pooling h, w = x2.shape[2], x2.shape[3] dht = dht.expand(-1, -1, h, w) x2 = torch.cat([x2, dht], 1) x2 = self.cord(x2) # Merge back x2 = self.backbone.forward2(x2) # ResNet at Scale 16 return x2","title":"Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction"},{"location":"other_categories/depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/","text":"Deterministic Guided LiDAR Depth Map Completion \u8fd9\u7bc7paper\u63d0\u51fa\u4e86\u4e00\u5957\u4e0d\u7528learning\u7684\u6df1\u5ea6\u8865\u5168\u7b97\u6cd5\u3002\u4e0e IP-basic \u76f8\u6bd4\uff0c\u529f\u80fd\u66f4\u9f50\u5168\uff0c\u5206\u6790\u66f4\u6709\u6df1\u5ea6\u3002\u4e14\u53ef\u4ee5\u548c\u5176\u4ed6\u65b9\u6cd5\u76f8\u4e92\u914d\u5408. \u76f8\u5173\u5de5\u4f5c \u8fd9\u7bc7paper\u63d0\u5230\u7684related works \u91cc\u9762\uff0c\u63d0\u5230\u4e86TOF image \u8d85\u5206\u8fa8\u7387\u95ee\u9898\uff0c\u8fd9\u4e2a\u5728\u6027\u8d28\u4e0a\u4e0eLiDAR\u7684\u6df1\u5ea6\u8865\u5168\u5728\u51e0\u4f55\u6027\u4e0a\u662f\u6709\u5f88\u5f3a\u7684\u76f8\u4f3c\u6027\u7684\u3002 \u5176\u4e2d\u63d0\u5230\u4e86 \u57fa\u4e8e\u7ea6\u675f\u7684bilateral solver \u5bf9\u6df1\u5ea6\u8865\u5168,\u7b49\u504f\u5fae\u5206\u65b9\u7a0b\u95ee\u9898\u7684\u6548\u679c. \u4e3b\u8981\u63d0\u51fa\u5185\u5bb9 \u5f71\u54cdLiDAR\u6df1\u5ea6\u8865\u5168\u7684\u4e00\u4e9b\u5e38\u89c1\u95ee\u9898 \u8fd9\u91cc\u6587\u7ae0\u7ed9\u51fa\u4e86\u5f88\u7ec6\u4f46\u662f\u975e\u5e38\u9ad8\u8d28\u91cf\u7684\u56fe \u56fe\u50cfartifacts, \u5728\u4e00\u4e9b\u6bd4\u8f83\u8fdc\u7684\u7269\u4f53\u8fb9\u7f18\uff0c\u53ef\u80fd\u4f1a\u56e0\u4e3a\u6270\u52a8\u4ea7\u751f\u4e0d\u81ea\u7136\u7684\u989c\u8272\u53d8\u6362;\u5982\u4e0a\u4fa7\u7684\u56fe\u7684\u4e0a\u534a\u90e8\u5206\u3002\u8fd9\u79cd\u8bef\u5dee\u5f88\u96be\u5904\u7406\uff0c\u9664\u975e\u6709\u56fe\u50cf\u66f4\u539f\u59cb\u7684\u6570\u636e\u3002 \u53ef\u53cd\u5c04\u7684\u8868\u9762\u3002 \u79bb\u6563\u5316\u8bef\u5dee\uff0c\u56e0\u4e3a\u6211\u4eec\u9700\u8981\u5c06\u70b9\u4e91\u6295\u5c04\u5230\u56fe\u7247\u5750\u6807\u7cfb\u4e0b\uff0c\u5e76\u4e14\u7ed9\u4e00\u4e2a\u50cf\u7d20\u5bf9\u5e94\u4e00\u4e2a\u6df1\u5ea6\u3002\u8fd9\u91cc\u5c31\u6d89\u53ca\u5230\u4e86\u70b9\u7684\u79bb\u6563\u5316 Misalignment. \u7531\u4e8e\u76f8\u673a\u548c\u96f7\u8fbe\u7684\u653e\u7f6e\u4f4d\u7f6e\u4e0d\u540c\uff0c\u56e0\u6b64\u70b9\u4e91\u6709\u65f6\u5019\u80fd\u770b\u5230\u76f8\u673a\u770b\u4e0d\u5230\u7684\u540e\u9762\u7684\u70b9\u6bd4\u5982\u4e0a\u4fa7\u56fe\u4e0b\u73ed\u90e8\u5206\u7684\u6811\u5e72\u4e0a\u7684\u80cc\u666f\u70b9\u3002 \u4e0d\u540c\u7684\u83b7\u53d6\u65f6\u95f4\uff0c\u70b9\u4e91\u7684\u8fd0\u52a8\u7578\u53d8\u4e5f\u662f\u4e00\u4e2a\u5178\u578b\u95ee\u9898\u3002 \u8bed\u4e49\u5206\u5272\u8d85\u50cf\u7d20 \u56fe\u7247\u7684\u4e0d\u540c\u8bed\u610f\u533a\u57df\u662f\u9700\u8981\u7528\u4e0d\u540c\u7684\u51e0\u4f55\u903b\u8f91\u53bb\u5904\u7406\u7684\uff0c\u6574\u4e2a\u56fe\u7684\u51e0\u4f55\u7279\u6027\u592a\u96be\u3002\u6240\u4ee5\u6211\u4eec\u8fd8\u662f\u9700\u8981\u5148\u505a\u8bed\u4e49\u5206\u5272\uff0c\u8fd9\u91cc\u672c\u6587\u91c7\u7528\u5feb\u6377\u7684 Simple Linear Iterative Clustering (SLIC) , \u5b9e\u9645\u4e0a\u662f\u4e00\u4e2a\u8003\u8651\u4e86\u5f69\u8272\u4ee5\u53ca\u7a7a\u95f4\u4f4d\u7f6e\u7684 K-mean \u7b97\u6cd5. \u5e73\u9762\u8fd1\u4f3c\u4e0e\u63d2\u503c \u8ba1\u7b97\u5e73\u9762\u7684\u7b97\u6cd5\u5176\u5b9e\u5c31\u662f\u7528\u4e00\u4e2a\u5e73\u9762\u53bb\u62df\u5408\u8d85\u50cf\u7d20\u91cc\u9762\u7684\u6240\u6709\u70b9\u4e91\u70b9. \u53ef\u4ee5\u88ab\u63cf\u8ff0\u4e3a\u4e00\u4e2a\u4f18\u5316\u95ee\u9898: \\min _{\\mid \\vec{n} \\|_{2}=1} E_{T L S}(\\vec{n})=\\min _{\\|\\vec{n}\\|_{2}=1}\\|A \\vec{n}\\|_{2} \u53ef\u4ee5\u7528SVD\u6c42\u89e3 \u5bf9\u4e8e\u540c\u4e00\u8d85\u50cf\u7d20\u7684\u70b9\uff0c\u53ef\u4ee5\u7528\u5e73\u9762\u63d2\u503c\u5f97\u5230\u4e00\u4e2a\u9884\u4f30\u3002 \u81f3\u4e8e\u7528\u4e00\u4e2a\u5e73\u9762\u662f\u5426\u53ef\u4ee5\uff0c\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7valid check(\u73b0\u6709\u70b9\u548c\u5e73\u9762\u7684\u8ddd\u79bb)\uff0c\u5224\u65ad\u5e73\u9762\u662f\u5426\u6b63\u786e\u62df\u5408\u4e86\u3002 \u51f8\u5305\u65b9\u5f0f. \u5982\u679c\u5e73\u9762\u8fd1\u4f3c\u5931\u8d25\u4e86\uff0c\u90a3\u4e48\u4f1a\u91c7\u7528\u51f8\u5305\u4f30\u8ba1\u7684\u65b9\u5f0f\u3002 \u591a\u5c42\u7ea7\u878d\u5408 \u4e3a\u4e86\u589e\u52a0\u80fd\u591f\u63d2\u503c\u7684\u533a\u57df\uff0c\u56fe\u7247\u4f1a\u5728\u4e0d\u540c\u7684\u5206\u8fa8\u7387\u4e0b\u88absegment\u591a\u518c\uff0c\u5f97\u5230\u4e0d\u540c\u7684\u6df1\u5ea6\u3002\u6700\u540e\u7684\u6df1\u5ea6\u4e3a\u540c\u4e00\u4e2a\u70b9\u6240\u6709\u6df1\u5ea6\u4e2d\u4f4d\u6570.\u8fd9\u4e5f\u80fd\u5141\u8bb8\u8fd9\u4e2a\u7b97\u6cd5\u548c\u5176\u4ed6\u6df1\u5ea6\u5b66\u4e60\u7b97\u6cd5\u8fdb\u884c\u878d\u5408\u3002","title":"Deterministic Guided LiDAR Depth Map Completion"},{"location":"other_categories/depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/#deterministic-guided-lidar-depth-map-completion","text":"\u8fd9\u7bc7paper\u63d0\u51fa\u4e86\u4e00\u5957\u4e0d\u7528learning\u7684\u6df1\u5ea6\u8865\u5168\u7b97\u6cd5\u3002\u4e0e IP-basic \u76f8\u6bd4\uff0c\u529f\u80fd\u66f4\u9f50\u5168\uff0c\u5206\u6790\u66f4\u6709\u6df1\u5ea6\u3002\u4e14\u53ef\u4ee5\u548c\u5176\u4ed6\u65b9\u6cd5\u76f8\u4e92\u914d\u5408.","title":"Deterministic Guided LiDAR Depth Map Completion"},{"location":"other_categories/depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/#_1","text":"\u8fd9\u7bc7paper\u63d0\u5230\u7684related works \u91cc\u9762\uff0c\u63d0\u5230\u4e86TOF image \u8d85\u5206\u8fa8\u7387\u95ee\u9898\uff0c\u8fd9\u4e2a\u5728\u6027\u8d28\u4e0a\u4e0eLiDAR\u7684\u6df1\u5ea6\u8865\u5168\u5728\u51e0\u4f55\u6027\u4e0a\u662f\u6709\u5f88\u5f3a\u7684\u76f8\u4f3c\u6027\u7684\u3002 \u5176\u4e2d\u63d0\u5230\u4e86 \u57fa\u4e8e\u7ea6\u675f\u7684bilateral solver \u5bf9\u6df1\u5ea6\u8865\u5168,\u7b49\u504f\u5fae\u5206\u65b9\u7a0b\u95ee\u9898\u7684\u6548\u679c.","title":"\u76f8\u5173\u5de5\u4f5c"},{"location":"other_categories/depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/#_2","text":"","title":"\u4e3b\u8981\u63d0\u51fa\u5185\u5bb9"},{"location":"other_categories/depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/#lidar","text":"\u8fd9\u91cc\u6587\u7ae0\u7ed9\u51fa\u4e86\u5f88\u7ec6\u4f46\u662f\u975e\u5e38\u9ad8\u8d28\u91cf\u7684\u56fe \u56fe\u50cfartifacts, \u5728\u4e00\u4e9b\u6bd4\u8f83\u8fdc\u7684\u7269\u4f53\u8fb9\u7f18\uff0c\u53ef\u80fd\u4f1a\u56e0\u4e3a\u6270\u52a8\u4ea7\u751f\u4e0d\u81ea\u7136\u7684\u989c\u8272\u53d8\u6362;\u5982\u4e0a\u4fa7\u7684\u56fe\u7684\u4e0a\u534a\u90e8\u5206\u3002\u8fd9\u79cd\u8bef\u5dee\u5f88\u96be\u5904\u7406\uff0c\u9664\u975e\u6709\u56fe\u50cf\u66f4\u539f\u59cb\u7684\u6570\u636e\u3002 \u53ef\u53cd\u5c04\u7684\u8868\u9762\u3002 \u79bb\u6563\u5316\u8bef\u5dee\uff0c\u56e0\u4e3a\u6211\u4eec\u9700\u8981\u5c06\u70b9\u4e91\u6295\u5c04\u5230\u56fe\u7247\u5750\u6807\u7cfb\u4e0b\uff0c\u5e76\u4e14\u7ed9\u4e00\u4e2a\u50cf\u7d20\u5bf9\u5e94\u4e00\u4e2a\u6df1\u5ea6\u3002\u8fd9\u91cc\u5c31\u6d89\u53ca\u5230\u4e86\u70b9\u7684\u79bb\u6563\u5316 Misalignment. \u7531\u4e8e\u76f8\u673a\u548c\u96f7\u8fbe\u7684\u653e\u7f6e\u4f4d\u7f6e\u4e0d\u540c\uff0c\u56e0\u6b64\u70b9\u4e91\u6709\u65f6\u5019\u80fd\u770b\u5230\u76f8\u673a\u770b\u4e0d\u5230\u7684\u540e\u9762\u7684\u70b9\u6bd4\u5982\u4e0a\u4fa7\u56fe\u4e0b\u73ed\u90e8\u5206\u7684\u6811\u5e72\u4e0a\u7684\u80cc\u666f\u70b9\u3002 \u4e0d\u540c\u7684\u83b7\u53d6\u65f6\u95f4\uff0c\u70b9\u4e91\u7684\u8fd0\u52a8\u7578\u53d8\u4e5f\u662f\u4e00\u4e2a\u5178\u578b\u95ee\u9898\u3002","title":"\u5f71\u54cdLiDAR\u6df1\u5ea6\u8865\u5168\u7684\u4e00\u4e9b\u5e38\u89c1\u95ee\u9898"},{"location":"other_categories/depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/#_3","text":"\u56fe\u7247\u7684\u4e0d\u540c\u8bed\u610f\u533a\u57df\u662f\u9700\u8981\u7528\u4e0d\u540c\u7684\u51e0\u4f55\u903b\u8f91\u53bb\u5904\u7406\u7684\uff0c\u6574\u4e2a\u56fe\u7684\u51e0\u4f55\u7279\u6027\u592a\u96be\u3002\u6240\u4ee5\u6211\u4eec\u8fd8\u662f\u9700\u8981\u5148\u505a\u8bed\u4e49\u5206\u5272\uff0c\u8fd9\u91cc\u672c\u6587\u91c7\u7528\u5feb\u6377\u7684 Simple Linear Iterative Clustering (SLIC) , \u5b9e\u9645\u4e0a\u662f\u4e00\u4e2a\u8003\u8651\u4e86\u5f69\u8272\u4ee5\u53ca\u7a7a\u95f4\u4f4d\u7f6e\u7684 K-mean \u7b97\u6cd5.","title":"\u8bed\u4e49\u5206\u5272\u8d85\u50cf\u7d20"},{"location":"other_categories/depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/#_4","text":"\u8ba1\u7b97\u5e73\u9762\u7684\u7b97\u6cd5\u5176\u5b9e\u5c31\u662f\u7528\u4e00\u4e2a\u5e73\u9762\u53bb\u62df\u5408\u8d85\u50cf\u7d20\u91cc\u9762\u7684\u6240\u6709\u70b9\u4e91\u70b9. \u53ef\u4ee5\u88ab\u63cf\u8ff0\u4e3a\u4e00\u4e2a\u4f18\u5316\u95ee\u9898: \\min _{\\mid \\vec{n} \\|_{2}=1} E_{T L S}(\\vec{n})=\\min _{\\|\\vec{n}\\|_{2}=1}\\|A \\vec{n}\\|_{2} \u53ef\u4ee5\u7528SVD\u6c42\u89e3 \u5bf9\u4e8e\u540c\u4e00\u8d85\u50cf\u7d20\u7684\u70b9\uff0c\u53ef\u4ee5\u7528\u5e73\u9762\u63d2\u503c\u5f97\u5230\u4e00\u4e2a\u9884\u4f30\u3002 \u81f3\u4e8e\u7528\u4e00\u4e2a\u5e73\u9762\u662f\u5426\u53ef\u4ee5\uff0c\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7valid check(\u73b0\u6709\u70b9\u548c\u5e73\u9762\u7684\u8ddd\u79bb)\uff0c\u5224\u65ad\u5e73\u9762\u662f\u5426\u6b63\u786e\u62df\u5408\u4e86\u3002","title":"\u5e73\u9762\u8fd1\u4f3c\u4e0e\u63d2\u503c"},{"location":"other_categories/depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/#_5","text":"\u5982\u679c\u5e73\u9762\u8fd1\u4f3c\u5931\u8d25\u4e86\uff0c\u90a3\u4e48\u4f1a\u91c7\u7528\u51f8\u5305\u4f30\u8ba1\u7684\u65b9\u5f0f\u3002","title":"\u51f8\u5305\u65b9\u5f0f."},{"location":"other_categories/depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/#_6","text":"\u4e3a\u4e86\u589e\u52a0\u80fd\u591f\u63d2\u503c\u7684\u533a\u57df\uff0c\u56fe\u7247\u4f1a\u5728\u4e0d\u540c\u7684\u5206\u8fa8\u7387\u4e0b\u88absegment\u591a\u518c\uff0c\u5f97\u5230\u4e0d\u540c\u7684\u6df1\u5ea6\u3002\u6700\u540e\u7684\u6df1\u5ea6\u4e3a\u540c\u4e00\u4e2a\u70b9\u6240\u6709\u6df1\u5ea6\u4e2d\u4f4d\u6570.\u8fd9\u4e5f\u80fd\u5141\u8bb8\u8fd9\u4e2a\u7b97\u6cd5\u548c\u5176\u4ed6\u6df1\u5ea6\u5b66\u4e60\u7b97\u6cd5\u8fdb\u884c\u878d\u5408\u3002","title":"\u591a\u5c42\u7ea7\u878d\u5408"},{"location":"other_categories/depth_completion/OrdinalRegression/","text":"Soft Labels for Ordinal Regression \u4e0e DORN \u4e00\u6837\u3002\u91c7\u53d6\u5e8f\u6570\u56de\u5f52\u7684\u6982\u5ff5\u5bf9\u6df1\u5ea6\u8fdb\u884c\u56de\u5f52\u3002\u5e8f\u6570\u56de\u5f52\u7684\u6982\u5ff5\u5728\u4e8e\u88ab\u56de\u5f52\u7684\u503c\u6709\u4e00\u5b9a\u7684\u81ea\u7136\u903b\u8f91\u987a\u5e8f\u3002 SORD \u5e8f\u6570\u56de\u5f52\u7684target: y_{i}=\\frac{e^{-\\phi\\left(r_{t}, r_{i}\\right)}}{\\sum_{k=1}^{K} e^{-\\phi\\left(r_{t}, r_{k}\\right)}} \\quad \\forall r_{i} \\in \\mathcal{Y} \u7c7b\u4f3c\u4e8e\u4ea4\u53c9\u71b5\u635f\u5931\u51fd\u6570\u7684\u53cd\u4f20: \\frac{\\partial L}{\\partial p_{i}}=-\\frac{e^{-\\phi\\left(r_{t}, r_{i}\\right)}}{e^{o_{i}^{\\prime}}}=-e^{-\\phi\\left(r_{t}, r_{i}\\right)-o_{i}^{\\prime}} \u5bf9\u4e8e\u6df1\u5ea6\u9884\u6d4b\uff0c\u6587\u4e2d\u63d0\u5230\u4e86\u4e09\u79cd\u6838\u51fd\u6570 \\phi Square Difference(SD): \\phi\\left(r_{t}, r_{i}\\right)=\\left\\|r_{t}-r_{i}\\right\\|^{2} Square Log Difference(SL): \\phi\\left(r_{t}, r_{i}\\right)=\\left\\|\\log r_{t}-\\log r_{i}\\right\\|^{2} Square Invariant Logarithmic Error (SI): \\phi\\left(r_{t}, r_{i}\\right)=d_{r_{t}, r_{i}}^{2}-\\frac{d_{r_{t}, r_{i}}}{n}\\left(d_{r_{t}, r_{i}}+\\sum_{p^{\\prime} \\neq p} d_{p^{\\prime}}\\right) SI Loss \u6e90\u81ea\u4e8e\u8fd9\u7bc7 paper.pdf \\begin{aligned} D\\left(y, y^{*}\\right) &=\\frac{1}{n^{2}} \\sum_{i, j}\\left(\\left(\\log y_{i}-\\log y_{j}\\right)-\\left(\\log y_{i}^{*}-\\log y_{j}^{*}\\right)\\right)^{2} \\\\ &=\\frac{1}{n} \\sum_{i} d_{i}^{2}-\\frac{1}{n^{2}} \\sum_{i, j} d_{i} d_{j}=\\frac{1}{n} \\sum_{i} d_{i}^{2}-\\frac{1}{n^{2}}\\left(\\sum_{i} d_{i}\\right)^{2} \\end{aligned} \u90a3\u7bc7paper\u63d0\u51fa\u7684\u4e00\u4e2a\u6df7\u5408\u7684Loss: L\\left(y, y^{*}\\right)=\\frac{1}{n} \\sum_{i} d_{i}^{2}-\\frac{\\lambda}{n^{2}}\\left(\\sum_{i} d_{i}\\right)^{2}","title":"Soft Labels for Ordinal Regression"},{"location":"other_categories/depth_completion/OrdinalRegression/#soft-labels-for-ordinal-regression","text":"\u4e0e DORN \u4e00\u6837\u3002\u91c7\u53d6\u5e8f\u6570\u56de\u5f52\u7684\u6982\u5ff5\u5bf9\u6df1\u5ea6\u8fdb\u884c\u56de\u5f52\u3002\u5e8f\u6570\u56de\u5f52\u7684\u6982\u5ff5\u5728\u4e8e\u88ab\u56de\u5f52\u7684\u503c\u6709\u4e00\u5b9a\u7684\u81ea\u7136\u903b\u8f91\u987a\u5e8f\u3002","title":"Soft Labels for Ordinal Regression"},{"location":"other_categories/depth_completion/OrdinalRegression/#sord","text":"\u5e8f\u6570\u56de\u5f52\u7684target: y_{i}=\\frac{e^{-\\phi\\left(r_{t}, r_{i}\\right)}}{\\sum_{k=1}^{K} e^{-\\phi\\left(r_{t}, r_{k}\\right)}} \\quad \\forall r_{i} \\in \\mathcal{Y} \u7c7b\u4f3c\u4e8e\u4ea4\u53c9\u71b5\u635f\u5931\u51fd\u6570\u7684\u53cd\u4f20: \\frac{\\partial L}{\\partial p_{i}}=-\\frac{e^{-\\phi\\left(r_{t}, r_{i}\\right)}}{e^{o_{i}^{\\prime}}}=-e^{-\\phi\\left(r_{t}, r_{i}\\right)-o_{i}^{\\prime}} \u5bf9\u4e8e\u6df1\u5ea6\u9884\u6d4b\uff0c\u6587\u4e2d\u63d0\u5230\u4e86\u4e09\u79cd\u6838\u51fd\u6570 \\phi Square Difference(SD): \\phi\\left(r_{t}, r_{i}\\right)=\\left\\|r_{t}-r_{i}\\right\\|^{2} Square Log Difference(SL): \\phi\\left(r_{t}, r_{i}\\right)=\\left\\|\\log r_{t}-\\log r_{i}\\right\\|^{2} Square Invariant Logarithmic Error (SI): \\phi\\left(r_{t}, r_{i}\\right)=d_{r_{t}, r_{i}}^{2}-\\frac{d_{r_{t}, r_{i}}}{n}\\left(d_{r_{t}, r_{i}}+\\sum_{p^{\\prime} \\neq p} d_{p^{\\prime}}\\right) SI Loss \u6e90\u81ea\u4e8e\u8fd9\u7bc7 paper.pdf \\begin{aligned} D\\left(y, y^{*}\\right) &=\\frac{1}{n^{2}} \\sum_{i, j}\\left(\\left(\\log y_{i}-\\log y_{j}\\right)-\\left(\\log y_{i}^{*}-\\log y_{j}^{*}\\right)\\right)^{2} \\\\ &=\\frac{1}{n} \\sum_{i} d_{i}^{2}-\\frac{1}{n^{2}} \\sum_{i, j} d_{i} d_{j}=\\frac{1}{n} \\sum_{i} d_{i}^{2}-\\frac{1}{n^{2}}\\left(\\sum_{i} d_{i}\\right)^{2} \\end{aligned} \u90a3\u7bc7paper\u63d0\u51fa\u7684\u4e00\u4e2a\u6df7\u5408\u7684Loss: L\\left(y, y^{*}\\right)=\\frac{1}{n} \\sum_{i} d_{i}^{2}-\\frac{\\lambda}{n^{2}}\\left(\\sum_{i} d_{i}\\right)^{2}","title":"SORD"},{"location":"other_categories/depth_completion/Sparse_and_noisy_LiDAR_completion/","text":"Sparse and noisy LiDAR completion with RGB guidance and uncertainty \u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e0d\u7b97\u590d\u6742\u7684\u65b9\u5f0f\uff0c\u572820ms\u949f\u5185\u5b8c\u6210\u5bf9 1256 \\times 256 \u5206\u8fa8\u7387\u7684\u56fe\u50cf\u4e0e\u7a00\u758f\u70b9\u4e91\u8fdb\u884c\u56fe\u50cf\u8865\u5168\u3002 \u603b\u4f53\u6846\u67b6 \u505a\u6cd5\u603b\u4f53\u6bd4\u8f83\u76f4\u89c2\uff0c\u9996\u5148RGB\u56fe\u50cf\u4e0e\u7a00\u758f\u7684\u6295\u5f71\u540e\u7684lidar\u6df1\u5ea6\u56fe\u8fdb\u884cconcat,\u8f93\u5165\u5230\u4ee5 ERFNet \u4e3a\u4e3b\u5e72\u7684encoder-decoder\u7f51\u7edc\u4e2d\uff0c(\u9700\u8981\u6ce8\u610f\u7684\u662f\u6839\u636e ERFNet \u7684github\u7f51\u9875\u663e\u793a\uff0c\u5176\u6a21\u578b\u9700\u8981\u79c1\u804a\u624d\u53ef\u83b7\u5f97),\u8f93\u51fa\u4e09\u4e2a\u90e8\u5206\uff0c\u4e00\u4e2a\u662flidar map\u7684\u5c40\u90e8\u6b8b\u5dee,\u4e00\u4e2a\u662f\u5168\u5c40\u6df1\u5ea6\u9884\u6d4b\uff0c\u4e00\u4e2a\u662f\u5168\u5c40\u6df1\u5ea6\u7684\u7f6e\u4fe1\u5ea6\u3002 \u800c\u7a00\u758f\u7684\u6295\u5f71\u540e\u7684Lidar\u6df1\u5ea6\u56fe\u8865\u4e0a\u6b8b\u5dee\u540e\u540c\u65f6\u8f93\u5165\u5230\u5c40\u90e8\u5206\u652f\u4e2d\uff0c\u4f7f\u7528\u6570\u4e2a\u6b8b\u5dee\u8fde\u63a5\u7684 StackedHourGlass \u6a21\u5757,\u518d\u5377\u79ef\u8f93\u51fa\u4e24\u4e2a\u5206\u652f\uff0c\u4e00\u4e2a\u662f\u5c40\u90e8\u6df1\u5ea6\u9884\u6d4b\uff0c\u4e00\u4e2a\u662f\u5c40\u90e8\u6df1\u5ea6\u7684\u7f6e\u4fe1\u5ea6\u3002 \u5168\u5c40\u5206\u652f\u4e0e\u5c40\u90e8\u5206\u652f\uff0c\u5c06\u4e24\u5206\u652f\u7684\u7f6e\u4fe1\u5ea6\u8fde\u63a5\u5e76\u4f7f\u7528\u4f7f\u7528softmax\u5c42\u5f62\u6210\u5206\u522b\u7684\u6743\u91cd\uff0c\u518d\u5c06\u4e24\u8005\u7684\u6df1\u5ea6\u9884\u6d4b\u503c\u52a0\u6743\u6c42\u548c\u5f97\u5230\u3002 \u672c\u6587\u5177\u4f53\u4f7f\u7528\u7684 StackedHourGlass \u6a21\u5757\u5982\u56fe \u8bad\u7ec3\u7ec6\u8282 \u8bad\u7ec3\u987a\u5e8f\u4e0a\uff0c\u4f5c\u8005\u4f7f\u7528pretrained ERFNet \u5148\u5355\u72ec\u5bf9\u4e24\u4e2a\u5206\u652f\u8fdb\u884c\u8bad\u7ec3\uff0c\u7136\u540e\u518d\u52a0\u5165guidance\u90e8\u5206\u4ee5\u53ca\u52a0\u6743\u90e8\u5206\u8fdb\u884c\u7aef\u5230\u7aef\u8bad\u7ec3. \u635f\u5931\u51fd\u6570\u65b9\u9762\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u6240\u8c13focal-MSE Loss\u3002\u6570\u5b66\u8868\u8fbe\u4e3a: \\lambda(\\hat y, y) = \\frac{1}{n} \\sum^n_{i=1}(1+0.05 * epoch * |y_i - \\hat y_i|) * (y_i - \\hat y_i)^2 \u6700\u7ec8\u635f\u5931\u4e3a \\Lambda=w_{1} \\cdot \\lambda\\left(\\hat{y}_{g l o b a l}, y\\right)+w_{2} \\cdot \\lambda\\left(\\hat{y}_{l o c a l}, y\\right)+w_{3} \\cdot \\lambda\\left(\\hat{y}_{o u t}, y\\right) \u5176\u4e2d\u4e09\u4e2a\u53c2\u6570\u5206\u522b\u4e3a0.1, 0.1, 1","title":"Sparse and noisy LiDAR completion with RGB guidance and uncertainty"},{"location":"other_categories/depth_completion/Sparse_and_noisy_LiDAR_completion/#sparse-and-noisy-lidar-completion-with-rgb-guidance-and-uncertainty","text":"\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e0d\u7b97\u590d\u6742\u7684\u65b9\u5f0f\uff0c\u572820ms\u949f\u5185\u5b8c\u6210\u5bf9 1256 \\times 256 \u5206\u8fa8\u7387\u7684\u56fe\u50cf\u4e0e\u7a00\u758f\u70b9\u4e91\u8fdb\u884c\u56fe\u50cf\u8865\u5168\u3002","title":"Sparse and noisy LiDAR completion with RGB guidance and uncertainty"},{"location":"other_categories/depth_completion/Sparse_and_noisy_LiDAR_completion/#_1","text":"\u505a\u6cd5\u603b\u4f53\u6bd4\u8f83\u76f4\u89c2\uff0c\u9996\u5148RGB\u56fe\u50cf\u4e0e\u7a00\u758f\u7684\u6295\u5f71\u540e\u7684lidar\u6df1\u5ea6\u56fe\u8fdb\u884cconcat,\u8f93\u5165\u5230\u4ee5 ERFNet \u4e3a\u4e3b\u5e72\u7684encoder-decoder\u7f51\u7edc\u4e2d\uff0c(\u9700\u8981\u6ce8\u610f\u7684\u662f\u6839\u636e ERFNet \u7684github\u7f51\u9875\u663e\u793a\uff0c\u5176\u6a21\u578b\u9700\u8981\u79c1\u804a\u624d\u53ef\u83b7\u5f97),\u8f93\u51fa\u4e09\u4e2a\u90e8\u5206\uff0c\u4e00\u4e2a\u662flidar map\u7684\u5c40\u90e8\u6b8b\u5dee,\u4e00\u4e2a\u662f\u5168\u5c40\u6df1\u5ea6\u9884\u6d4b\uff0c\u4e00\u4e2a\u662f\u5168\u5c40\u6df1\u5ea6\u7684\u7f6e\u4fe1\u5ea6\u3002 \u800c\u7a00\u758f\u7684\u6295\u5f71\u540e\u7684Lidar\u6df1\u5ea6\u56fe\u8865\u4e0a\u6b8b\u5dee\u540e\u540c\u65f6\u8f93\u5165\u5230\u5c40\u90e8\u5206\u652f\u4e2d\uff0c\u4f7f\u7528\u6570\u4e2a\u6b8b\u5dee\u8fde\u63a5\u7684 StackedHourGlass \u6a21\u5757,\u518d\u5377\u79ef\u8f93\u51fa\u4e24\u4e2a\u5206\u652f\uff0c\u4e00\u4e2a\u662f\u5c40\u90e8\u6df1\u5ea6\u9884\u6d4b\uff0c\u4e00\u4e2a\u662f\u5c40\u90e8\u6df1\u5ea6\u7684\u7f6e\u4fe1\u5ea6\u3002 \u5168\u5c40\u5206\u652f\u4e0e\u5c40\u90e8\u5206\u652f\uff0c\u5c06\u4e24\u5206\u652f\u7684\u7f6e\u4fe1\u5ea6\u8fde\u63a5\u5e76\u4f7f\u7528\u4f7f\u7528softmax\u5c42\u5f62\u6210\u5206\u522b\u7684\u6743\u91cd\uff0c\u518d\u5c06\u4e24\u8005\u7684\u6df1\u5ea6\u9884\u6d4b\u503c\u52a0\u6743\u6c42\u548c\u5f97\u5230\u3002 \u672c\u6587\u5177\u4f53\u4f7f\u7528\u7684 StackedHourGlass \u6a21\u5757\u5982\u56fe","title":"\u603b\u4f53\u6846\u67b6"},{"location":"other_categories/depth_completion/Sparse_and_noisy_LiDAR_completion/#_2","text":"\u8bad\u7ec3\u987a\u5e8f\u4e0a\uff0c\u4f5c\u8005\u4f7f\u7528pretrained ERFNet \u5148\u5355\u72ec\u5bf9\u4e24\u4e2a\u5206\u652f\u8fdb\u884c\u8bad\u7ec3\uff0c\u7136\u540e\u518d\u52a0\u5165guidance\u90e8\u5206\u4ee5\u53ca\u52a0\u6743\u90e8\u5206\u8fdb\u884c\u7aef\u5230\u7aef\u8bad\u7ec3. \u635f\u5931\u51fd\u6570\u65b9\u9762\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u6240\u8c13focal-MSE Loss\u3002\u6570\u5b66\u8868\u8fbe\u4e3a: \\lambda(\\hat y, y) = \\frac{1}{n} \\sum^n_{i=1}(1+0.05 * epoch * |y_i - \\hat y_i|) * (y_i - \\hat y_i)^2 \u6700\u7ec8\u635f\u5931\u4e3a \\Lambda=w_{1} \\cdot \\lambda\\left(\\hat{y}_{g l o b a l}, y\\right)+w_{2} \\cdot \\lambda\\left(\\hat{y}_{l o c a l}, y\\right)+w_{3} \\cdot \\lambda\\left(\\hat{y}_{o u t}, y\\right) \u5176\u4e2d\u4e09\u4e2a\u53c2\u6570\u5206\u522b\u4e3a0.1, 0.1, 1","title":"\u8bad\u7ec3\u7ec6\u8282"},{"location":"other_categories/depth_completion/advancing_monodepth_splidar/","text":"Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR \u8fd9\u7bc7paper\u4e0e\u65e0\u76d1\u7763\u7684\u6df1\u5ea6\u4f30\u8ba1\u53c8\u6709\u4e00\u70b9\u4e0d\u540c\uff0c\u672c\u8d28\u4e0a\u662f\u4e00\u4e2a\u65e0\u76d1\u7763\u6df1\u5ea6\u8865\u5168\u7684\u4efb\u52a1\uff0c\u4f46\u662f\u8fd9\u4e2a\u4efb\u52a1\u672c\u8eab\u7684\u7814\u7a76\u5c31\u6bd4\u8f83\u5c11\u53c8\u6709\u76f8\u5f53\u7684\u62d3\u5c55\u524d\u666f\uff0c\u56e0\u800c\u540c\u6837\u503c\u5f97\u4e86\u89e3\u3002 Architecture \u6982\u5ff5\u4e0a\u7684\u8981\u70b9 - \u8fd9\u662f\u4e00\u4e2a\u4e8c\u9636\u6bb5\u7684\u81ea\u76d1\u7763\u7f51\u7edc\uff0c\u5e76\u4e0d\u9700\u8981\u7a20\u5bc6\u6df1\u5ea6\u6216\u8005\u5b8c\u6574\u70b9\u4e91\u8fdb\u884c\u76d1\u7763 - \u8f93\u5165\u7f51\u7edc\u7684\u70b9\u4e91\u4ece64\u7ebf\u4e2d\u91c7\u68374\u7ebf \u8bbe\u8ba1\u4e0e\u524d\u6587\u7684\u8fde\u63a5\u4e0e\u5927\u81f4\u601d\u60f3: \u4e0e Monodepth2 \u4e00\u81f4\uff0c\u901a\u8fc7posenet\u548cdepthnet\u5b8c\u6210\u4e00\u4e2a\u57fa\u7840\u7684\u76d1\u7763. \u4f7f\u7528 Plidar++ \u4e2d\u7684 GDC (graph-based depth correction)\u7b97\u6cd5\u76f4\u63a5\u6839\u636e\u7a00\u758f\u70b9\u4e91\u4f18\u5316\u7a20\u5bc6\u7684\u6df1\u5ea6\u4f30\u8ba1; \u4f46\u662f\u4e3a\u4e86\u6574\u4e2a\u63a8\u7406\u8fc7\u7a0b\u7684\u5b9e\u65f6\u6027\uff0c\u4f7f\u7528\u989d\u5916\u7684refinenet\u53bb\u62df\u5408\u8fd9\u4e2aGDC\u7684\u4f18\u5316\u8fc7\u7a0b\uff0c\u8fd9\u4e2arefinenet\u7531GDC\u76d1\u7763. Pseudo Dense Representations (PDR): PDR\u7531\u4e24\u4e2achannel\u7ec4\u6210 \u6df1\u5ea6 \\begin{aligned} r(x, y) &=\\sqrt{\\left(u_{i}-x\\right)^{2}+\\left(v_{i}-y\\right)^{2}} \\\\ D(x, y) &=\\left\\{\\begin{array}{ll} Z_{i}, & \\text { if } r(x, y)<R \\\\ 0, & \\text { otherwise } \\end{array}\\right. \\end{aligned} \u7f6e\u4fe1\u5ea6. C(x, y)=\\left\\{\\begin{array}{ll} \\frac{1}{r}(x, y), & \\text { if } r(x, y)<R \\\\ 0, & \\text { otherwise } \\end{array}\\right. GDC GDC\u91c7\u7528\u7684\u662fPLiDAR++ \u7684\u65b9\u6cd5. \u4ee3\u7801","title":"Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR"},{"location":"other_categories/depth_completion/advancing_monodepth_splidar/#advancing-self-supervised-monocular-depth-learning-with-sparse-lidar","text":"\u8fd9\u7bc7paper\u4e0e\u65e0\u76d1\u7763\u7684\u6df1\u5ea6\u4f30\u8ba1\u53c8\u6709\u4e00\u70b9\u4e0d\u540c\uff0c\u672c\u8d28\u4e0a\u662f\u4e00\u4e2a\u65e0\u76d1\u7763\u6df1\u5ea6\u8865\u5168\u7684\u4efb\u52a1\uff0c\u4f46\u662f\u8fd9\u4e2a\u4efb\u52a1\u672c\u8eab\u7684\u7814\u7a76\u5c31\u6bd4\u8f83\u5c11\u53c8\u6709\u76f8\u5f53\u7684\u62d3\u5c55\u524d\u666f\uff0c\u56e0\u800c\u540c\u6837\u503c\u5f97\u4e86\u89e3\u3002","title":"Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR"},{"location":"other_categories/depth_completion/advancing_monodepth_splidar/#architecture","text":"\u6982\u5ff5\u4e0a\u7684\u8981\u70b9 - \u8fd9\u662f\u4e00\u4e2a\u4e8c\u9636\u6bb5\u7684\u81ea\u76d1\u7763\u7f51\u7edc\uff0c\u5e76\u4e0d\u9700\u8981\u7a20\u5bc6\u6df1\u5ea6\u6216\u8005\u5b8c\u6574\u70b9\u4e91\u8fdb\u884c\u76d1\u7763 - \u8f93\u5165\u7f51\u7edc\u7684\u70b9\u4e91\u4ece64\u7ebf\u4e2d\u91c7\u68374\u7ebf \u8bbe\u8ba1\u4e0e\u524d\u6587\u7684\u8fde\u63a5\u4e0e\u5927\u81f4\u601d\u60f3: \u4e0e Monodepth2 \u4e00\u81f4\uff0c\u901a\u8fc7posenet\u548cdepthnet\u5b8c\u6210\u4e00\u4e2a\u57fa\u7840\u7684\u76d1\u7763. \u4f7f\u7528 Plidar++ \u4e2d\u7684 GDC (graph-based depth correction)\u7b97\u6cd5\u76f4\u63a5\u6839\u636e\u7a00\u758f\u70b9\u4e91\u4f18\u5316\u7a20\u5bc6\u7684\u6df1\u5ea6\u4f30\u8ba1; \u4f46\u662f\u4e3a\u4e86\u6574\u4e2a\u63a8\u7406\u8fc7\u7a0b\u7684\u5b9e\u65f6\u6027\uff0c\u4f7f\u7528\u989d\u5916\u7684refinenet\u53bb\u62df\u5408\u8fd9\u4e2aGDC\u7684\u4f18\u5316\u8fc7\u7a0b\uff0c\u8fd9\u4e2arefinenet\u7531GDC\u76d1\u7763.","title":"Architecture"},{"location":"other_categories/depth_completion/advancing_monodepth_splidar/#pseudo-dense-representations-pdr","text":"PDR\u7531\u4e24\u4e2achannel\u7ec4\u6210 \u6df1\u5ea6 \\begin{aligned} r(x, y) &=\\sqrt{\\left(u_{i}-x\\right)^{2}+\\left(v_{i}-y\\right)^{2}} \\\\ D(x, y) &=\\left\\{\\begin{array}{ll} Z_{i}, & \\text { if } r(x, y)<R \\\\ 0, & \\text { otherwise } \\end{array}\\right. \\end{aligned} \u7f6e\u4fe1\u5ea6. C(x, y)=\\left\\{\\begin{array}{ll} \\frac{1}{r}(x, y), & \\text { if } r(x, y)<R \\\\ 0, & \\text { otherwise } \\end{array}\\right.","title":"Pseudo Dense Representations (PDR):"},{"location":"other_categories/depth_completion/advancing_monodepth_splidar/#gdc","text":"GDC\u91c7\u7528\u7684\u662fPLiDAR++ \u7684\u65b9\u6cd5. \u4ee3\u7801","title":"GDC"},{"location":"other_categories/depth_completion/depth_pred_before/","text":"Depth Prediction Before Deep Learning \u672c\u6587\u8bb0\u5f55\u51e0\u7bc7\u5728\u6df1\u5ea6\u5b66\u4e60\u65f6\u4ee3\u4e4b\u524d\u7684\u5355\u76ee\u6df1\u5ea6\u76f8\u5173\u7684\u5de5\u4f5c\u3002 Single Image Depth Estimation From Predicted Semantic Labels pdf Semantic Segmentation with MRF MRF\u80fd\u91cf\u5b9a\u4e49\u4e3a: \\mathbf{E}(\\mathbf{L} \\mid \\mathcal{I})=\\sum_{p} \\psi_{p}\\left(L_{p}\\right)+\\lambda \\sum_{p q} \\psi_{p q}\\left(L_{p}, L_{q}\\right) \u5bf9\u4e8e\u5355\u9879\u52bf\u80fd\u9009\u62e9\u7684\u5219\u662f\u4e0e \u8fd9\u7bc7\u6587\u7ae0 \u4e00\u81f4\u768417\u79cd\u5377\u79ef\u6838\u7684\u7279\u5f81\u3002\u8272\u5f69\u7a7a\u95f4\u9009\u62e9 CIE Lab ,\u4e09\u4e2a\u65b9\u5dee\u4e3a1,2,4\u7684\u9ad8\u65af\u6838\u5206\u522b\u7528\u5728\u4e09\u4e2achannel\u4e0a\uff0c\u56db\u4e2aLaplacian Gaussian filters (\u65b9\u5dee\u4e3a1,2,4,8)\u4ec5\u5728L\u9891\u9053\u4e0a\uff0c\u5728x,y\u65b9\u5411\u4e0a\u4ee5\u65b9\u5dee(2, 4)\u5171\u4f5c4\u4e2a\u9ad8\u65af\u4e00\u9636\u5bfc\u7684\u5377\u79ef\u6838(\u4ec5 L\u9891\u9053)\u3002\u517117\u4e2a\u9891\u9053\u3002 Scene Geometry \u5bf9\u6bcf\u4e00\u4e2a p \u70b9\uff0c\u5176\u4e2d g \u4e3a\u56fe\u7247\u4e0a\u540c\u4e00x\u8f74\u4e0a\u6700\u9ad8\u7684\u5730\u9762\u70b9\uff0c b \u70b9\u4e3a\u56fe\u7247\u4e0a\u540c\u4e00x\u8f74\u4e0a\u540c\u4e00instance\u7684\u6700\u4f4e\u70b9\uff0c t \u4e3a\u56fe\u7247\u4e0a\u540c\u4e00x\u8f74\u4e0a\u540c\u4e00instance\u7684\u6700\u9ad8\u70b9\uff0c\u5219\u70b9p\u7684\u6df1\u5ea6\u8303\u56f4\u4e3a: d_{g}\\left(\\frac{r_{g}^{T} e_{3}}{r_{p}^{T} e_{3}}\\right) \\leq d_{p} \\leq d_{g}\\left(\\frac{r_{g}^{T} e_{2}}{r_{b}^{T} e_{2}}\\right)\\left(\\frac{r_{b}^{T} e_{3}}{r_{p}^{T} e_{3}}\\right) \u5bf9\u6bcf\u4e00\u4e2a\u50cf\u7d20\u70b9\uff0c\u4f5c\u8005\u5bf9training set\u4e0a\u6bcf\u4e00\u5f20\u56fe\u6bcf\u4e00\u4e2a\u50cf\u7d20\u70b9\u4f4d\u7f6e(u, v)\u8ba1\u7b97\u4e86\u5176\u5e73\u5747\u7684log-depth.\u540c\u65f6\u7528\u524d\u6587geometric hints\u91cc\u9762\u63d0\u5230\u7684\u51e0\u4e2a\u76f8\u5173pixel\u7684prior depth\u5bf9\u5f53\u524d\u70b9\u6df1\u5ea6\u8fdb\u884c\u4f30\u7b97\uff0c\u4f5c\u4e3a\u7b2c\u4e8c\u4e2aMRF\u7684features\u3002 Learning Depth from Single Monocular Images pdf \u8fd9\u7bc7paper\u4e5f\u662f\u4f7f\u7528MRF\u8fdb\u884c\u5206\u6790\u3002 \u6838\u5fc3\u65b0\u610f\u6709\u4e24\u4e2a\uff0c\u7b2c\u4e00\u4e2a\u662fmulti-scale,\u7b2c\u4e8c\u4e2a\u662f\u8ba1\u7b97\u4e0a\u4e0b\u76f8\u90bb\u7684\u4e24\u4e2afeature\u7684histgram vector\u7684\u5dee\u503c\u6765\u9884\u6d4b\u4e24\u8005\u7684\u6df1\u5ea6\u5dee\u3002","title":"Depth Prediction Before Deep Learning"},{"location":"other_categories/depth_completion/depth_pred_before/#depth-prediction-before-deep-learning","text":"\u672c\u6587\u8bb0\u5f55\u51e0\u7bc7\u5728\u6df1\u5ea6\u5b66\u4e60\u65f6\u4ee3\u4e4b\u524d\u7684\u5355\u76ee\u6df1\u5ea6\u76f8\u5173\u7684\u5de5\u4f5c\u3002","title":"Depth Prediction Before Deep Learning"},{"location":"other_categories/depth_completion/depth_pred_before/#single-image-depth-estimation-from-predicted-semantic-labels","text":"pdf","title":"Single Image Depth Estimation From Predicted Semantic Labels"},{"location":"other_categories/depth_completion/depth_pred_before/#semantic-segmentation-with-mrf","text":"MRF\u80fd\u91cf\u5b9a\u4e49\u4e3a: \\mathbf{E}(\\mathbf{L} \\mid \\mathcal{I})=\\sum_{p} \\psi_{p}\\left(L_{p}\\right)+\\lambda \\sum_{p q} \\psi_{p q}\\left(L_{p}, L_{q}\\right) \u5bf9\u4e8e\u5355\u9879\u52bf\u80fd\u9009\u62e9\u7684\u5219\u662f\u4e0e \u8fd9\u7bc7\u6587\u7ae0 \u4e00\u81f4\u768417\u79cd\u5377\u79ef\u6838\u7684\u7279\u5f81\u3002\u8272\u5f69\u7a7a\u95f4\u9009\u62e9 CIE Lab ,\u4e09\u4e2a\u65b9\u5dee\u4e3a1,2,4\u7684\u9ad8\u65af\u6838\u5206\u522b\u7528\u5728\u4e09\u4e2achannel\u4e0a\uff0c\u56db\u4e2aLaplacian Gaussian filters (\u65b9\u5dee\u4e3a1,2,4,8)\u4ec5\u5728L\u9891\u9053\u4e0a\uff0c\u5728x,y\u65b9\u5411\u4e0a\u4ee5\u65b9\u5dee(2, 4)\u5171\u4f5c4\u4e2a\u9ad8\u65af\u4e00\u9636\u5bfc\u7684\u5377\u79ef\u6838(\u4ec5 L\u9891\u9053)\u3002\u517117\u4e2a\u9891\u9053\u3002","title":"Semantic Segmentation with MRF"},{"location":"other_categories/depth_completion/depth_pred_before/#scene-geometry","text":"\u5bf9\u6bcf\u4e00\u4e2a p \u70b9\uff0c\u5176\u4e2d g \u4e3a\u56fe\u7247\u4e0a\u540c\u4e00x\u8f74\u4e0a\u6700\u9ad8\u7684\u5730\u9762\u70b9\uff0c b \u70b9\u4e3a\u56fe\u7247\u4e0a\u540c\u4e00x\u8f74\u4e0a\u540c\u4e00instance\u7684\u6700\u4f4e\u70b9\uff0c t \u4e3a\u56fe\u7247\u4e0a\u540c\u4e00x\u8f74\u4e0a\u540c\u4e00instance\u7684\u6700\u9ad8\u70b9\uff0c\u5219\u70b9p\u7684\u6df1\u5ea6\u8303\u56f4\u4e3a: d_{g}\\left(\\frac{r_{g}^{T} e_{3}}{r_{p}^{T} e_{3}}\\right) \\leq d_{p} \\leq d_{g}\\left(\\frac{r_{g}^{T} e_{2}}{r_{b}^{T} e_{2}}\\right)\\left(\\frac{r_{b}^{T} e_{3}}{r_{p}^{T} e_{3}}\\right) \u5bf9\u6bcf\u4e00\u4e2a\u50cf\u7d20\u70b9\uff0c\u4f5c\u8005\u5bf9training set\u4e0a\u6bcf\u4e00\u5f20\u56fe\u6bcf\u4e00\u4e2a\u50cf\u7d20\u70b9\u4f4d\u7f6e(u, v)\u8ba1\u7b97\u4e86\u5176\u5e73\u5747\u7684log-depth.\u540c\u65f6\u7528\u524d\u6587geometric hints\u91cc\u9762\u63d0\u5230\u7684\u51e0\u4e2a\u76f8\u5173pixel\u7684prior depth\u5bf9\u5f53\u524d\u70b9\u6df1\u5ea6\u8fdb\u884c\u4f30\u7b97\uff0c\u4f5c\u4e3a\u7b2c\u4e8c\u4e2aMRF\u7684features\u3002","title":"Scene Geometry"},{"location":"other_categories/depth_completion/depth_pred_before/#learning-depth-from-single-monocular-images","text":"pdf \u8fd9\u7bc7paper\u4e5f\u662f\u4f7f\u7528MRF\u8fdb\u884c\u5206\u6790\u3002 \u6838\u5fc3\u65b0\u610f\u6709\u4e24\u4e2a\uff0c\u7b2c\u4e00\u4e2a\u662fmulti-scale,\u7b2c\u4e8c\u4e2a\u662f\u8ba1\u7b97\u4e0a\u4e0b\u76f8\u90bb\u7684\u4e24\u4e2afeature\u7684histgram vector\u7684\u5dee\u503c\u6765\u9884\u6d4b\u4e24\u8005\u7684\u6df1\u5ea6\u5dee\u3002","title":"Learning Depth from Single Monocular Images"},{"location":"other_categories/depth_completion/dorn/","text":"Deep Ordinal Regression Network for Monocular Depth Estimation \u8fd9\u7bc7paper\u65f6\u95f4\u867d\u7136\u6bd4\u8f83\u65e9\uff0c\u4f46\u662f\u662f\u5c5e\u4e8e\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u7684SOTA\u7ed3\u679c\u3002\u91cd\u70b9\u5728\u4e8e\u5982\u4f55\u5bf9\u8de8\u5ea6\u5f88\u5927\u7684\u6df1\u5ea6\u503c\u8fdb\u884c\u56de\u5f52\u8ba1\u7b97\u3002 \u7f51\u7edc\u7ed3\u6784 Ordinal Regression \u635f\u5931\u51fd\u6570\u8ba1\u7b97: \\begin{aligned} \\mathcal{L}(\\chi, \\Theta)=&-\\frac{1}{\\mathcal{N}} \\sum_{w=0}^{W-1} \\sum_{h=0}^{H-1} \\Psi(w, h, \\chi, \\Theta) \\\\ \\Psi(h, w, \\chi, \\Theta)=& \\sum_{k=0}^{l_{(w, h)-1}} \\log \\left(\\mathcal{P}_{(w, h)}^{k}\\right) \\\\ &+\\sum_{k=l_{(w, h)}}^{K-1}\\left(\\log \\left(1-\\mathcal{P}_{(w, h)}^{k}\\right)\\right) \\\\ \\mathcal{P}_{(w, h)}^{k}=& P\\left(\\hat{l}_{(w, h)}>k | \\chi, \\Theta\\right) \\end{aligned} \u63a8\u7406\u65f6\uff1a \\begin{aligned} \\hat{d}_{(w, h)} &=\\frac{t_{\\hat{l}_{(w, h)}}+t_{\\hat{l}_{(w, h)}+1}}{2}-\\xi \\\\ \\hat{l}_{(w, h)} &=\\sum_{k=0}^{K-1} \\eta\\left(\\mathcal{P}_{(w, h)}^{k}>=0.5\\right) \\end{aligned} \u7ffb\u8bd1\u6210\u4e2d\u6587\uff0c\u5c31\u662f\u8bf4\u9996\u5148\u8fd9\u4e2a\u95ee\u9898\u88ab\u8003\u8651\u4e3a\u4e00\u4e2amulti-bin\u5206\u7c7b\u7684\u95ee\u9898\uff0c\u5982\u679c\u7269\u4f53\u5728\u7b2c t \u4e2abin\u91cc\u9762\uff0c\u5219\u524d\u9762t\u4e2a\u8282\u70b9\u7684\u5206\u7c7b\u90fd\u5e94\u8be5\u4e3a\u6b63\uff0c\u63a8\u7406\u7684\u65f6\u5019\u5148\u8ba1\u7b97\u9884\u6d4b\u4e3a\u6b63\u7684\u8282\u70b9\u6709\u591a\u5c11\u4e2a\uff0c\u5224\u65ad\u7269\u4f53\u7684\u6df1\u5ea6\u5728\u54ea\u4e00\u4e2abin\u4e0a\uff0c\u7136\u540e\u8f93\u51fa\u7684target\u503c\u4e3a\u5747\u503c\u3002","title":"Deep Ordinal Regression Network for Monocular Depth Estimation"},{"location":"other_categories/depth_completion/dorn/#deep-ordinal-regression-network-for-monocular-depth-estimation","text":"\u8fd9\u7bc7paper\u65f6\u95f4\u867d\u7136\u6bd4\u8f83\u65e9\uff0c\u4f46\u662f\u662f\u5c5e\u4e8e\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u7684SOTA\u7ed3\u679c\u3002\u91cd\u70b9\u5728\u4e8e\u5982\u4f55\u5bf9\u8de8\u5ea6\u5f88\u5927\u7684\u6df1\u5ea6\u503c\u8fdb\u884c\u56de\u5f52\u8ba1\u7b97\u3002","title":"Deep Ordinal Regression Network for Monocular Depth Estimation"},{"location":"other_categories/depth_completion/dorn/#_1","text":"","title":"\u7f51\u7edc\u7ed3\u6784"},{"location":"other_categories/depth_completion/dorn/#ordinal-regression","text":"\u635f\u5931\u51fd\u6570\u8ba1\u7b97: \\begin{aligned} \\mathcal{L}(\\chi, \\Theta)=&-\\frac{1}{\\mathcal{N}} \\sum_{w=0}^{W-1} \\sum_{h=0}^{H-1} \\Psi(w, h, \\chi, \\Theta) \\\\ \\Psi(h, w, \\chi, \\Theta)=& \\sum_{k=0}^{l_{(w, h)-1}} \\log \\left(\\mathcal{P}_{(w, h)}^{k}\\right) \\\\ &+\\sum_{k=l_{(w, h)}}^{K-1}\\left(\\log \\left(1-\\mathcal{P}_{(w, h)}^{k}\\right)\\right) \\\\ \\mathcal{P}_{(w, h)}^{k}=& P\\left(\\hat{l}_{(w, h)}>k | \\chi, \\Theta\\right) \\end{aligned} \u63a8\u7406\u65f6\uff1a \\begin{aligned} \\hat{d}_{(w, h)} &=\\frac{t_{\\hat{l}_{(w, h)}}+t_{\\hat{l}_{(w, h)}+1}}{2}-\\xi \\\\ \\hat{l}_{(w, h)} &=\\sum_{k=0}^{K-1} \\eta\\left(\\mathcal{P}_{(w, h)}^{k}>=0.5\\right) \\end{aligned} \u7ffb\u8bd1\u6210\u4e2d\u6587\uff0c\u5c31\u662f\u8bf4\u9996\u5148\u8fd9\u4e2a\u95ee\u9898\u88ab\u8003\u8651\u4e3a\u4e00\u4e2amulti-bin\u5206\u7c7b\u7684\u95ee\u9898\uff0c\u5982\u679c\u7269\u4f53\u5728\u7b2c t \u4e2abin\u91cc\u9762\uff0c\u5219\u524d\u9762t\u4e2a\u8282\u70b9\u7684\u5206\u7c7b\u90fd\u5e94\u8be5\u4e3a\u6b63\uff0c\u63a8\u7406\u7684\u65f6\u5019\u5148\u8ba1\u7b97\u9884\u6d4b\u4e3a\u6b63\u7684\u8282\u70b9\u6709\u591a\u5c11\u4e2a\uff0c\u5224\u65ad\u7269\u4f53\u7684\u6df1\u5ea6\u5728\u54ea\u4e00\u4e2abin\u4e0a\uff0c\u7136\u540e\u8f93\u51fa\u7684target\u503c\u4e3a\u5747\u503c\u3002","title":"Ordinal Regression"},{"location":"other_categories/depth_completion/guideNet/","text":"Learning Guided Convolutional Network for Depth Completion \u8fd9\u7bc7\u6587\u7ae0\u57fa\u4e8e Dynamic Filtering Networks \u7684\u7406\u5ff5\u3002 \u603b\u4f53\u7ed3\u6784 \u8f93\u5165\u662fRGB\u56fe\u7247\u4ee5\u53ca\u7a00\u758f\u7684\u6df1\u5ea6\u56fe\uff0c\u6700\u7ec8\u8f93\u51fa\u7684\u662f\u7a20\u5bc6\u7684\u6df1\u5ea6\u56fe\u3002\u6574\u4f53\u7ed3\u6784\u5f88\u7c7b\u4f3c\u4e8e\u57fa\u7840\u7684FPN Guided Convolution Module \u5de6\u56fe\u8868\u8fbe\u4e86\u603b\u4f53\u7684\u601d\u8def\uff0c\u5c31\u662f\u4f7f\u7528RGB image\u7684\u7279\u5f81\u4f5c\u4e3a\u5f15\u5bfc\u5c42\uff0c\u5c40\u90e8\u5730\u751f\u6210\u5377\u79ef\u6838\uff0c\u5bf9\u7a00\u758f\u6df1\u5ea6\u56fe\u8fdb\u884c\u5377\u79ef(\u4e0e DFN \u7684\u7b2c\u4e8c\u79cd\u5b9e\u73b0\u76f8\u4f3c)\u3002 \u4f46\u662f\u8fd9\u79cd\u7c7b\u4f3c\u4e8e\u5c40\u90e8\u5377\u79ef\u7684\u65b9\u5f0f\u5bf9GPU\u663e\u5b58\u6d88\u8017\u5f88\u5927\u3002\u4f5c\u8005\u63d0\u51fa\u6a21\u4effMobile Net\u7684\u601d\u8def\u3002\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528Channel-wise \u5c40\u90e8\u5377\u79ef\uff0c\u7b2c\u4e8c\u9636\u6bb5\u4f7f\u7528Spatial-invariant\u7684\u666e\u901a\u5377\u79ef.","title":"Learning Guided Convolutional Network for Depth Completion"},{"location":"other_categories/depth_completion/guideNet/#learning-guided-convolutional-network-for-depth-completion","text":"\u8fd9\u7bc7\u6587\u7ae0\u57fa\u4e8e Dynamic Filtering Networks \u7684\u7406\u5ff5\u3002","title":"Learning Guided Convolutional Network for Depth Completion"},{"location":"other_categories/depth_completion/guideNet/#_1","text":"\u8f93\u5165\u662fRGB\u56fe\u7247\u4ee5\u53ca\u7a00\u758f\u7684\u6df1\u5ea6\u56fe\uff0c\u6700\u7ec8\u8f93\u51fa\u7684\u662f\u7a20\u5bc6\u7684\u6df1\u5ea6\u56fe\u3002\u6574\u4f53\u7ed3\u6784\u5f88\u7c7b\u4f3c\u4e8e\u57fa\u7840\u7684FPN","title":"\u603b\u4f53\u7ed3\u6784"},{"location":"other_categories/depth_completion/guideNet/#guided-convolution-module","text":"\u5de6\u56fe\u8868\u8fbe\u4e86\u603b\u4f53\u7684\u601d\u8def\uff0c\u5c31\u662f\u4f7f\u7528RGB image\u7684\u7279\u5f81\u4f5c\u4e3a\u5f15\u5bfc\u5c42\uff0c\u5c40\u90e8\u5730\u751f\u6210\u5377\u79ef\u6838\uff0c\u5bf9\u7a00\u758f\u6df1\u5ea6\u56fe\u8fdb\u884c\u5377\u79ef(\u4e0e DFN \u7684\u7b2c\u4e8c\u79cd\u5b9e\u73b0\u76f8\u4f3c)\u3002 \u4f46\u662f\u8fd9\u79cd\u7c7b\u4f3c\u4e8e\u5c40\u90e8\u5377\u79ef\u7684\u65b9\u5f0f\u5bf9GPU\u663e\u5b58\u6d88\u8017\u5f88\u5927\u3002\u4f5c\u8005\u63d0\u51fa\u6a21\u4effMobile Net\u7684\u601d\u8def\u3002\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528Channel-wise \u5c40\u90e8\u5377\u79ef\uff0c\u7b2c\u4e8c\u9636\u6bb5\u4f7f\u7528Spatial-invariant\u7684\u666e\u901a\u5377\u79ef.","title":"Guided Convolution Module"},{"location":"other_categories/depth_completion/image_synthesis_loss/","text":"On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation \u65b0\u8fd1\u4e3b\u6d41\u7684\u65e0\u76d1\u7763\u6df1\u5ea6\u9884\u6d4b 1 2 3 , \u65e0\u76d1\u7763\u7684\u5149\u6d41\u9884\u6d4b 4 , \u65e0\u76d1\u7763\u7684\u53cc\u76ee\u4f30\u8ba1 5 \u90fd\u6709\u4e00\u4e2a\u5171\u540c\u70b9\uff0c\u5c31\u662f\u4f7f\u7528\u56fe\u7247\u4e4b\u95f4\u7684\u76f8\u4e92\u7684\u91cd\u5efa\u4f5c\u4e3a\u7f51\u7edc\u5b66\u4e60\u7684\u76d1\u7763\u4fe1\u53f7\u3002\u5176\u4e2d\u4e00\u4e2a\u5f88\u5f3a\u7684\u5047\u8bbe\u5728\u4e8e\uff0c\u5047\u8bbe\u73af\u5883\u7684\u6df1\u5ea6\u662f\u88ab\u6b63\u786e\u611f\u77e5\u7684\uff0c\u90a3\u4e48\u5728\u4e0d\u540c\u56fe\u7247(\u65f6\u5e8f\u56fe\u7247, \u53cc\u76ee\u56fe\u7247)\u4e4b\u95f4\u91cd\u6295\u5f71\u5c31\u4f1a\u662f\u5b8c\u7f8e\u7684\uff0c \u5e76\u4e14\u53cd\u4e4b\u4ea6\u7136 . \u8fd9\u7bc7paper\u6ca1\u6709\u7ed9\u51fa\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u662f\u5bf9\u524d\u9762\u63d0\u5230\u7684\u5047\u8bbe\u63d0\u51fa\u4e86\u8d28\u7591\u5e76\u7ed9\u51fa\u4e86\u76f8\u5173\u7684\u53cd\u4f8b\u8bc1\u636e\u3002\u6838\u5fc3\u7684\u51e0\u4e2a\u7ed3\u8bba: \u56fe\u7247\u91cd\u5efa\u5408\u6210\u7684\u8d28\u91cf\u4f5c\u4e3a\u6df1\u5ea6\u4f30\u8ba1\u7f51\u7edc\u7684\u4e00\u4e2a\u8f85\u52a9\u8bad\u7ec3\u7f51\u7edc\u662f\u5426\u6709\u6548? \u56de\u7b54\uff1a\u6709\u6548\uff0c\u4f46\u662f\u53ea\u5230\u4e00\u5b9a\u7a0b\u5ea6\u3002\u5728\u4e00\u4e2a\u9608\u503c\u4e4b\u4e0a\u7ee7\u7eed\u63a8\u8fdb\u4f18\u5316\u56fe\u7247\u91cd\u5efa\u7684\u8d28\u91cf\u4f1a\u964d\u4f4e\u6df1\u5ea6\u4f30\u8ba1\u7684\u51c6\u786e\u7387. \u4f7f\u7528\u771f\u5b9e\u7684\u6df1\u5ea6\u503c\u662f\u5426\u80fd\u591f\u5b9e\u73b0\u5b8c\u7f8e\u7684\u56fe\u7247\u91cd\u5efa? \u56de\u7b54: \u662f \u4e0d\u53ef\u4ee5 \u7684. \u4e14\u5982\u679c\u56fe\u7247\u91cd\u5efa\u662f\u552f\u4e00\u7684\u635f\u5931\u51fd\u6570\uff0c\u5728\u5f97\u5230\u771f\u5b9e\u6df1\u5ea6\u7684\u7f51\u7edc\u9644\u8fd1\uff0c\u635f\u5931\u51fd\u6570\u7684\u5bfc\u6570\u4e0d\u4e3a0\uff0c \u56e0\u800c\u7f51\u7edc\u4e0d\u4f1a\u7a33\u5b9a\u505c\u7559\u5728\u8f93\u51fa\u771f\u5b9e\u6df1\u5ea6\u7684\u4f4d\u7f6e. \u662f\u4ec0\u4e48\u9020\u6210\u4e86\u56fe\u7247\u5408\u6210\u8d28\u91cf\u548c\u6df1\u5ea6\u4f30\u8ba1\u51c6\u786e\u5ea6\u4e4b\u95f4\u7684divergence? \u672c\u6587\u5206\u6790\u4e86\u76f8\u5173\u7684\u635f\u5931\u5e73\u9762 (loss manifold), \u6700\u7ec8\u5c06\u8fd9\u4e2a\u95ee\u9898\u5f52\u56e0\u4e8e\u56fe\u7247\u6570\u636e\u4e2d\u7684\u4e00\u4e9b\u95ee\u9898\u6216\u7279\u5f81\u3002 \u5b9e\u9a8c\u5206\u6790\u4e0a\uff0c\u672c\u6587\u4e3b\u8981\u662f\u5229\u7528\u51e0\u4e2a\u5e26\u6709label\u7684\u53cc\u76ee\u6570\u636e\u96c6\uff0c\u89c2\u5bdf\u8bad\u7ec3\u5f00\u59cb\u524d\u540e \u56fe\u7247\u5408\u6210\u7684\u76d1\u7763\u635f\u5931 L_{syn} \u4e0e \u7edd\u5bf9disparity error EPE (end-point-error). \u53ef\u4ee5\u4ece\u56fe\u91cc\u9762\u53d1\u73b0\u6709\u5f88\u591a\u7f51\u7edc\u4e2d\u80fd\u89c2\u5bdf\u5230,\u968f\u7740\u8bad\u7ec3\u8fdb\u884c\uff0c L_{syn} \u4e0eEPE\u5e76\u4e0d\u80fd\u4e00\u8d77\u8fdb\u6b65\u3002 \u4f7f\u7528\u771f\u5b9e\u6df1\u5ea6\u8fdb\u884c\u8fd8\u539f: Loss\u8fd8\u662f\u5f88\u5de8\u5927\u7684\u3002 \u89c2\u5bdf\u4e00\u4e9b\u7279\u6b8a\u7684\u70b9\uff0c \u8fd9\u4e9b\u70b9\u5728EPE=0\u7684\u5730\u65b9\u90fd\u5c55\u73b0\u51fa\u975e\u51f8\u7684\u5408\u6210\u635f\u5931: \u53ef\u4ee5\u603b\u7ed3\u51fa\u51e0\u4e2a\u7279\u70b9: \u955c\u9762\u53cd\u5c04 (non-Lambertian surfaces, non-diffusely reflecting surface)\u4e0e\u5f71\u5b50\uff0c\u955c\u9762\u53cd\u5c04\u548c\u5f71\u5b50\u4f1a\u5728\u56fe\u7247\u4e0a\u8868\u73b0\u51fa\u6bd4\u8f83\u5176\u4ed6\u7684\u5149\u5b66\u6548\u679c\uff0c\u5f71\u54cd\u5728\u4e0d\u540c\u4f4d\u7f6e\u7684\u89c2\u6d4b\u3002 \u7269\u4f53\u8fb9\u7f18\u4ee5\u53ca\u906e\u6321\u8fb9\u7f18. \u65e0\u7279\u5f81\u6216\u91cd\u590d\u7eb9\u7406\u533a\u57df\u3002\u5373\u4f7f\u4f7f\u7528\u4e86\u5149\u6ed1\u7684regularization, \u4f30\u8ba1\u7684\u51c6\u786e\u6027\u4e5f\u662f\u65e0\u6cd5\u4fdd\u8bc1\u7684.","title":"On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation"},{"location":"other_categories/depth_completion/image_synthesis_loss/#on-the-sins-of-image-synthesis-loss-for-self-supervised-depth-estimation","text":"\u65b0\u8fd1\u4e3b\u6d41\u7684\u65e0\u76d1\u7763\u6df1\u5ea6\u9884\u6d4b 1 2 3 , \u65e0\u76d1\u7763\u7684\u5149\u6d41\u9884\u6d4b 4 , \u65e0\u76d1\u7763\u7684\u53cc\u76ee\u4f30\u8ba1 5 \u90fd\u6709\u4e00\u4e2a\u5171\u540c\u70b9\uff0c\u5c31\u662f\u4f7f\u7528\u56fe\u7247\u4e4b\u95f4\u7684\u76f8\u4e92\u7684\u91cd\u5efa\u4f5c\u4e3a\u7f51\u7edc\u5b66\u4e60\u7684\u76d1\u7763\u4fe1\u53f7\u3002\u5176\u4e2d\u4e00\u4e2a\u5f88\u5f3a\u7684\u5047\u8bbe\u5728\u4e8e\uff0c\u5047\u8bbe\u73af\u5883\u7684\u6df1\u5ea6\u662f\u88ab\u6b63\u786e\u611f\u77e5\u7684\uff0c\u90a3\u4e48\u5728\u4e0d\u540c\u56fe\u7247(\u65f6\u5e8f\u56fe\u7247, \u53cc\u76ee\u56fe\u7247)\u4e4b\u95f4\u91cd\u6295\u5f71\u5c31\u4f1a\u662f\u5b8c\u7f8e\u7684\uff0c \u5e76\u4e14\u53cd\u4e4b\u4ea6\u7136 . \u8fd9\u7bc7paper\u6ca1\u6709\u7ed9\u51fa\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u662f\u5bf9\u524d\u9762\u63d0\u5230\u7684\u5047\u8bbe\u63d0\u51fa\u4e86\u8d28\u7591\u5e76\u7ed9\u51fa\u4e86\u76f8\u5173\u7684\u53cd\u4f8b\u8bc1\u636e\u3002\u6838\u5fc3\u7684\u51e0\u4e2a\u7ed3\u8bba: \u56fe\u7247\u91cd\u5efa\u5408\u6210\u7684\u8d28\u91cf\u4f5c\u4e3a\u6df1\u5ea6\u4f30\u8ba1\u7f51\u7edc\u7684\u4e00\u4e2a\u8f85\u52a9\u8bad\u7ec3\u7f51\u7edc\u662f\u5426\u6709\u6548? \u56de\u7b54\uff1a\u6709\u6548\uff0c\u4f46\u662f\u53ea\u5230\u4e00\u5b9a\u7a0b\u5ea6\u3002\u5728\u4e00\u4e2a\u9608\u503c\u4e4b\u4e0a\u7ee7\u7eed\u63a8\u8fdb\u4f18\u5316\u56fe\u7247\u91cd\u5efa\u7684\u8d28\u91cf\u4f1a\u964d\u4f4e\u6df1\u5ea6\u4f30\u8ba1\u7684\u51c6\u786e\u7387. \u4f7f\u7528\u771f\u5b9e\u7684\u6df1\u5ea6\u503c\u662f\u5426\u80fd\u591f\u5b9e\u73b0\u5b8c\u7f8e\u7684\u56fe\u7247\u91cd\u5efa? \u56de\u7b54: \u662f \u4e0d\u53ef\u4ee5 \u7684. \u4e14\u5982\u679c\u56fe\u7247\u91cd\u5efa\u662f\u552f\u4e00\u7684\u635f\u5931\u51fd\u6570\uff0c\u5728\u5f97\u5230\u771f\u5b9e\u6df1\u5ea6\u7684\u7f51\u7edc\u9644\u8fd1\uff0c\u635f\u5931\u51fd\u6570\u7684\u5bfc\u6570\u4e0d\u4e3a0\uff0c \u56e0\u800c\u7f51\u7edc\u4e0d\u4f1a\u7a33\u5b9a\u505c\u7559\u5728\u8f93\u51fa\u771f\u5b9e\u6df1\u5ea6\u7684\u4f4d\u7f6e. \u662f\u4ec0\u4e48\u9020\u6210\u4e86\u56fe\u7247\u5408\u6210\u8d28\u91cf\u548c\u6df1\u5ea6\u4f30\u8ba1\u51c6\u786e\u5ea6\u4e4b\u95f4\u7684divergence? \u672c\u6587\u5206\u6790\u4e86\u76f8\u5173\u7684\u635f\u5931\u5e73\u9762 (loss manifold), \u6700\u7ec8\u5c06\u8fd9\u4e2a\u95ee\u9898\u5f52\u56e0\u4e8e\u56fe\u7247\u6570\u636e\u4e2d\u7684\u4e00\u4e9b\u95ee\u9898\u6216\u7279\u5f81\u3002 \u5b9e\u9a8c\u5206\u6790\u4e0a\uff0c\u672c\u6587\u4e3b\u8981\u662f\u5229\u7528\u51e0\u4e2a\u5e26\u6709label\u7684\u53cc\u76ee\u6570\u636e\u96c6\uff0c\u89c2\u5bdf\u8bad\u7ec3\u5f00\u59cb\u524d\u540e \u56fe\u7247\u5408\u6210\u7684\u76d1\u7763\u635f\u5931 L_{syn} \u4e0e \u7edd\u5bf9disparity error EPE (end-point-error). \u53ef\u4ee5\u4ece\u56fe\u91cc\u9762\u53d1\u73b0\u6709\u5f88\u591a\u7f51\u7edc\u4e2d\u80fd\u89c2\u5bdf\u5230,\u968f\u7740\u8bad\u7ec3\u8fdb\u884c\uff0c L_{syn} \u4e0eEPE\u5e76\u4e0d\u80fd\u4e00\u8d77\u8fdb\u6b65\u3002 \u4f7f\u7528\u771f\u5b9e\u6df1\u5ea6\u8fdb\u884c\u8fd8\u539f: Loss\u8fd8\u662f\u5f88\u5de8\u5927\u7684\u3002 \u89c2\u5bdf\u4e00\u4e9b\u7279\u6b8a\u7684\u70b9\uff0c \u8fd9\u4e9b\u70b9\u5728EPE=0\u7684\u5730\u65b9\u90fd\u5c55\u73b0\u51fa\u975e\u51f8\u7684\u5408\u6210\u635f\u5931: \u53ef\u4ee5\u603b\u7ed3\u51fa\u51e0\u4e2a\u7279\u70b9: \u955c\u9762\u53cd\u5c04 (non-Lambertian surfaces, non-diffusely reflecting surface)\u4e0e\u5f71\u5b50\uff0c\u955c\u9762\u53cd\u5c04\u548c\u5f71\u5b50\u4f1a\u5728\u56fe\u7247\u4e0a\u8868\u73b0\u51fa\u6bd4\u8f83\u5176\u4ed6\u7684\u5149\u5b66\u6548\u679c\uff0c\u5f71\u54cd\u5728\u4e0d\u540c\u4f4d\u7f6e\u7684\u89c2\u6d4b\u3002 \u7269\u4f53\u8fb9\u7f18\u4ee5\u53ca\u906e\u6321\u8fb9\u7f18. \u65e0\u7279\u5f81\u6216\u91cd\u590d\u7eb9\u7406\u533a\u57df\u3002\u5373\u4f7f\u4f7f\u7528\u4e86\u5149\u6ed1\u7684regularization, \u4f30\u8ba1\u7684\u51c6\u786e\u6027\u4e5f\u662f\u65e0\u6cd5\u4fdd\u8bc1\u7684.","title":"On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation"},{"location":"other_categories/depth_completion/manydepth/","text":"The Temporal Opportunist: Self-Supervised Multi-Frame Monocular Depth (ManyDepth) \u8fd9\u7bc7paper\u5c06\u50cf\u7d20\u7ea7\u7684\u5e27\u95f4\u5339\u914d\u5f15\u5165\u4e86\u6df1\u5ea6\u9884\u6d4b\u4e2d\uff0c\u4f7f\u5f97\u65f6\u5e8f\u5355\u76ee\u6df1\u5ea6\u9884\u6d4b\u6709\u4e86\u66f4\u52a0\u79d1\u5b66\u7684\u65b9\u5411\u3002\u540c\u65f6\u672c\u6587\u63d0\u4f9b\u4e86\u4ee3\u7801\u4ee5\u53ca\u76f8\u5173\u9644\u5e26\u7684Tricks,\u4f7f\u5f97\u8fd9\u4e00\u5207\u53d8\u5f97\u771f\u7684\u53ef\u884c. \u7f51\u7edc\u7ed3\u6784 \u81ea\u76d1\u7763\u8bad\u7ec3: \u5c06\u76f8\u90bb\u5e27\u7684\u56fe\u7247\u6839\u636eposenet\u7684\u7ed3\u679c\u91cd\u6295\u5f71\u5230\u4e3b\u56fe\u7247\u4e0a\uff0c\u7136\u540e\u8ba1\u7b97\u91cd\u6295\u5f71\u7684\u8bef\u5dee\uff0c\u91cd\u6295\u5f71\u8bef\u5dee\u7531 SSIM\u4e0eL1 \u7ec4\u6210\uff0c\u8fd9\u4e2a\u548c MonoDepth\u4e00\u81f4\u3002 Cost Volume Bulding: \u901a\u8fc7\u7ed9\u5b9a\u4e00\u7cfb\u5217\u7684\u6df1\u5ea6\u5e73\u9762 P , \u5c06\u76f8\u90bb\u5e27\u7684feature map\u6295\u5f71\u5230\u4e3bfeature map\u4e0a\uff0c\u8ba1\u7b97\u4ed6\u4eec\u7684L1\u8ddd\u79bb\uff0c\u4e0e\u53cc\u76ee\u5339\u914d\u7684\u7c7b\u4f3c\uff0c\u4e0d\u8fc7\u8fd9\u91cc\u8981\u8003\u8651\u66f4\u590d\u6742\u7684\u5750\u6807\u8f6c\u6362\uff0c\u4e14\u7528\u6df1\u5ea6\u800c\u4e0d\u662fdisparity\u4f5c\u4e3a\u8f6c\u6362\u6807\u51c6\u3002 \u540c\u65f6\u7531\u4e8e\u65e0\u76d1\u7763\u7684\u6df1\u5ea6\u9884\u6d4b\u7f51\u7edc\u65e0\u8bba\u5982\u4f55\u5b66\u4e60\u5230\u7684\u6df1\u5ea6\u548cpose\u90fd\u53ea\u662f'up to a scale'. \u4e5f\u5c31\u662f\u6df1\u5ea6\u5e73\u9762\u7684 d_{min} , d_{max} \u5f88\u96be\u786e\u8ba4\uff0c\u672c\u6587\u8ba9\u8fd9\u4e2a\u8d85\u53c2\u53d8\u6210\u4e00\u4e2a\u53ef\u4ee5\u5b66\u4e60\u7684\u53c2\u6570\u3002\u5e26\u4e0a\u4e00\u4e2a\u6bd4\u8f83\u5f3a\u7684momentum. Cost volume overfitting: \u4f5c\u8005\u6307\u51fa\uff0c\u5982\u679c\u76f4\u63a5\u7528 cost volume, \u5355\u76ee\u7684\u7ed3\u679c\u4f1a\u6709\u5f88\u591aartifacts, \u5305\u62ec\u8fd0\u52a8\u7269\u4f53\u4e0a\u4f1a\u6709\u5f88\u5927\u7684holes. \u5176\u5b9e\uff0c\u4ece\u76f4\u89c9\u4e0a\u6765\u8bf4\uff0c\u4e00\u4e2a\u9ad8\u6807\u51c6\u7684monodepth2 baseline,\u52a0\u4e0a\u989d\u5916\u7684 cost volume\u7684\u52a0\u6301\uff0c\u6027\u80fd\u5e94\u5f53\u63d0\u5347\u4e86\u624d\u5bf9\u3002\u4f46\u662fcost volume\u4e00\u822c\u53ea\u5728\u9759\u6001\u3001\u6709texture\u7684\u533a\u57df\u6709\u6548\uff0c\u5f53\u7269\u4f53\u8fd0\u52a8\u3001\u65e0texture\u7684\u65f6\u5019\uff0ccost volume\u7684\u4fe1\u606f\u53cd\u800c\u4f1a\u4e0d\u51c6\u786e\u3002\u5982\u679c\u7f51\u7edc\u53d8\u5f97\u8fc7\u5ea6\u4f9d\u8d56\u50cf\u7d20\u5339\u914d\uff0c\u8fd9\u4e9b\u95ee\u9898\u5c31\u4f1a\u5f88\u4e25\u91cd\u3002\u6211\u4eec\u9700\u8981\u6559\u4f1a\u7f51\u7edc\u907f\u5f00\u8fd9\u4e9b\u96be\u70b9\u3002 \u672c\u6587\u7528\u4e00\u4e2a\u5355\u72ec\u7684\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u7f51\u7edc\uff0c\u4ed6\u4f1a\u5355\u72ec\u4ea7\u751f \\hat D_t . \u7136\u540e\u5bf9Cost volume\u6bcf\u4e00\u4e2a\u50cf\u7d20\u8ba1\u7b97 \\text{argmin} , \u5f97\u5230 D_{cv} . \u90a3\u4e48 D_{cv} \u5e94\u8be5\u548c D_{t} \u63a5\u8fd1\uff0c\u624d\u8ba4\u4e3a\u8fd9\u662f\u503c\u5f97\u4fe1\u8d56\u7684\u533a\u57df\u3002 mask\u516c\u5f0f (\u5982\u679c\u5dee\u4e00\u500d\u5219\u8ba4\u4e3a\u4e0d\u53ef\u9760) M=\\max \\left(\\frac{D_{\\mathrm{cv}}-\\hat{D}_{t}}{\\hat{D}_{t}}, \\frac{\\hat{D}_{t}-D_{\\mathrm{cv}}}{D_{\\mathrm{cv}}}\\right)>1 Static Camera and Start of sequences \u5982\u679c\u76f8\u673a\u4e0d\u52a8\uff0c\u6216\u8005\u5982\u679c\u662f\u5728Sequence\u7684\u5f00\u59cb\uff0c\u6df1\u5ea6\u5e94\u8be5\u5982\u4f55\u8ba1\u7b97\u5462? \u4e3a\u4e86\u514b\u670d\u7b2c\u4e00\u4e2a\u8fd9\u4e2a\u95ee\u9898\uff0c \u5728\u8bad\u7ec3\u7684\u65f6\u5019\uff0c\u6709 p \u7684\u6982\u7387\uff0ccost volume\u4f1a\u7528\u4e00\u4e2a\u96f6\u77e9\u9635\u66ff\u4ee3\u3002\u5bf9\u8fd9\u4e9b\u56fe\u7247\uff0c\u6211\u4eec\u4f1a\u9f13\u52b1\u7f51\u7edc\u53ea\u7528\u5355\u56fe\u7684\u7279\u5f81\u8fdb\u884c\u9884\u6d4b\uff1b\u6d4b\u8bd5\u7b2c\u4e00\u5f20\u56fe\u7684\u65f6\u5019\u7528\u96f6\u77e9\u9635\u66ff\u4ee3\u5c31\u53ef\u4ee5\u997f\u4e86\u3002\u4e3a\u4e86\u5ba2\u670d\u7b2c\u4e8c\u4e2a\u95ee\u9898\uff0c\u5728\u8bad\u7ec3\u7684\u65f6\u5019\uff0c\u6709 q \u7684\u6982\u7387\uff0c\u6211\u4eec\u5c06 I_{t-1} \u66ff\u6362\u4e3a\u6570\u636e\u589e\u5f3a\u540e\u7684 I_{t} , \u4f46\u662f\u4ecd\u7136\u7528 I_{t-1} \u505a\u91cd\u91c7\u6837\u5e76\u8ba1\u7b97\u81ea\u76d1\u7763\u635f\u5931\u3002\u4e5f\u5c31\u4f1a\u4fc3\u4f7f\u7f51\u7edc\u4e0d\u8981\u8fc7\u4e8e\u4f9d\u8d56cost volume\uff0c\u5e76\u5728cost volume\u662f\u6765\u81ea\u9759\u6001\u56fe\u7684\u65f6\u5019\u4e5f\u80fd\u6709\u53ef\u9760\u7684\u6df1\u5ea6\u4f30\u8ba1.","title":"The Temporal Opportunist: Self-Supervised Multi-Frame Monocular Depth (ManyDepth)"},{"location":"other_categories/depth_completion/manydepth/#the-temporal-opportunist-self-supervised-multi-frame-monocular-depth-manydepth","text":"\u8fd9\u7bc7paper\u5c06\u50cf\u7d20\u7ea7\u7684\u5e27\u95f4\u5339\u914d\u5f15\u5165\u4e86\u6df1\u5ea6\u9884\u6d4b\u4e2d\uff0c\u4f7f\u5f97\u65f6\u5e8f\u5355\u76ee\u6df1\u5ea6\u9884\u6d4b\u6709\u4e86\u66f4\u52a0\u79d1\u5b66\u7684\u65b9\u5411\u3002\u540c\u65f6\u672c\u6587\u63d0\u4f9b\u4e86\u4ee3\u7801\u4ee5\u53ca\u76f8\u5173\u9644\u5e26\u7684Tricks,\u4f7f\u5f97\u8fd9\u4e00\u5207\u53d8\u5f97\u771f\u7684\u53ef\u884c.","title":"The Temporal Opportunist: Self-Supervised Multi-Frame Monocular Depth (ManyDepth)"},{"location":"other_categories/depth_completion/manydepth/#_1","text":"","title":"\u7f51\u7edc\u7ed3\u6784"},{"location":"other_categories/depth_completion/manydepth/#_2","text":"\u5c06\u76f8\u90bb\u5e27\u7684\u56fe\u7247\u6839\u636eposenet\u7684\u7ed3\u679c\u91cd\u6295\u5f71\u5230\u4e3b\u56fe\u7247\u4e0a\uff0c\u7136\u540e\u8ba1\u7b97\u91cd\u6295\u5f71\u7684\u8bef\u5dee\uff0c\u91cd\u6295\u5f71\u8bef\u5dee\u7531 SSIM\u4e0eL1 \u7ec4\u6210\uff0c\u8fd9\u4e2a\u548c MonoDepth\u4e00\u81f4\u3002","title":"\u81ea\u76d1\u7763\u8bad\u7ec3:"},{"location":"other_categories/depth_completion/manydepth/#cost-volume-bulding","text":"\u901a\u8fc7\u7ed9\u5b9a\u4e00\u7cfb\u5217\u7684\u6df1\u5ea6\u5e73\u9762 P , \u5c06\u76f8\u90bb\u5e27\u7684feature map\u6295\u5f71\u5230\u4e3bfeature map\u4e0a\uff0c\u8ba1\u7b97\u4ed6\u4eec\u7684L1\u8ddd\u79bb\uff0c\u4e0e\u53cc\u76ee\u5339\u914d\u7684\u7c7b\u4f3c\uff0c\u4e0d\u8fc7\u8fd9\u91cc\u8981\u8003\u8651\u66f4\u590d\u6742\u7684\u5750\u6807\u8f6c\u6362\uff0c\u4e14\u7528\u6df1\u5ea6\u800c\u4e0d\u662fdisparity\u4f5c\u4e3a\u8f6c\u6362\u6807\u51c6\u3002 \u540c\u65f6\u7531\u4e8e\u65e0\u76d1\u7763\u7684\u6df1\u5ea6\u9884\u6d4b\u7f51\u7edc\u65e0\u8bba\u5982\u4f55\u5b66\u4e60\u5230\u7684\u6df1\u5ea6\u548cpose\u90fd\u53ea\u662f'up to a scale'. \u4e5f\u5c31\u662f\u6df1\u5ea6\u5e73\u9762\u7684 d_{min} , d_{max} \u5f88\u96be\u786e\u8ba4\uff0c\u672c\u6587\u8ba9\u8fd9\u4e2a\u8d85\u53c2\u53d8\u6210\u4e00\u4e2a\u53ef\u4ee5\u5b66\u4e60\u7684\u53c2\u6570\u3002\u5e26\u4e0a\u4e00\u4e2a\u6bd4\u8f83\u5f3a\u7684momentum.","title":"Cost Volume Bulding:"},{"location":"other_categories/depth_completion/manydepth/#cost-volume-overfitting","text":"\u4f5c\u8005\u6307\u51fa\uff0c\u5982\u679c\u76f4\u63a5\u7528 cost volume, \u5355\u76ee\u7684\u7ed3\u679c\u4f1a\u6709\u5f88\u591aartifacts, \u5305\u62ec\u8fd0\u52a8\u7269\u4f53\u4e0a\u4f1a\u6709\u5f88\u5927\u7684holes. \u5176\u5b9e\uff0c\u4ece\u76f4\u89c9\u4e0a\u6765\u8bf4\uff0c\u4e00\u4e2a\u9ad8\u6807\u51c6\u7684monodepth2 baseline,\u52a0\u4e0a\u989d\u5916\u7684 cost volume\u7684\u52a0\u6301\uff0c\u6027\u80fd\u5e94\u5f53\u63d0\u5347\u4e86\u624d\u5bf9\u3002\u4f46\u662fcost volume\u4e00\u822c\u53ea\u5728\u9759\u6001\u3001\u6709texture\u7684\u533a\u57df\u6709\u6548\uff0c\u5f53\u7269\u4f53\u8fd0\u52a8\u3001\u65e0texture\u7684\u65f6\u5019\uff0ccost volume\u7684\u4fe1\u606f\u53cd\u800c\u4f1a\u4e0d\u51c6\u786e\u3002\u5982\u679c\u7f51\u7edc\u53d8\u5f97\u8fc7\u5ea6\u4f9d\u8d56\u50cf\u7d20\u5339\u914d\uff0c\u8fd9\u4e9b\u95ee\u9898\u5c31\u4f1a\u5f88\u4e25\u91cd\u3002\u6211\u4eec\u9700\u8981\u6559\u4f1a\u7f51\u7edc\u907f\u5f00\u8fd9\u4e9b\u96be\u70b9\u3002 \u672c\u6587\u7528\u4e00\u4e2a\u5355\u72ec\u7684\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u7f51\u7edc\uff0c\u4ed6\u4f1a\u5355\u72ec\u4ea7\u751f \\hat D_t . \u7136\u540e\u5bf9Cost volume\u6bcf\u4e00\u4e2a\u50cf\u7d20\u8ba1\u7b97 \\text{argmin} , \u5f97\u5230 D_{cv} . \u90a3\u4e48 D_{cv} \u5e94\u8be5\u548c D_{t} \u63a5\u8fd1\uff0c\u624d\u8ba4\u4e3a\u8fd9\u662f\u503c\u5f97\u4fe1\u8d56\u7684\u533a\u57df\u3002 mask\u516c\u5f0f (\u5982\u679c\u5dee\u4e00\u500d\u5219\u8ba4\u4e3a\u4e0d\u53ef\u9760) M=\\max \\left(\\frac{D_{\\mathrm{cv}}-\\hat{D}_{t}}{\\hat{D}_{t}}, \\frac{\\hat{D}_{t}-D_{\\mathrm{cv}}}{D_{\\mathrm{cv}}}\\right)>1","title":"Cost volume overfitting:"},{"location":"other_categories/depth_completion/manydepth/#static-camera-and-start-of-sequences","text":"\u5982\u679c\u76f8\u673a\u4e0d\u52a8\uff0c\u6216\u8005\u5982\u679c\u662f\u5728Sequence\u7684\u5f00\u59cb\uff0c\u6df1\u5ea6\u5e94\u8be5\u5982\u4f55\u8ba1\u7b97\u5462? \u4e3a\u4e86\u514b\u670d\u7b2c\u4e00\u4e2a\u8fd9\u4e2a\u95ee\u9898\uff0c \u5728\u8bad\u7ec3\u7684\u65f6\u5019\uff0c\u6709 p \u7684\u6982\u7387\uff0ccost volume\u4f1a\u7528\u4e00\u4e2a\u96f6\u77e9\u9635\u66ff\u4ee3\u3002\u5bf9\u8fd9\u4e9b\u56fe\u7247\uff0c\u6211\u4eec\u4f1a\u9f13\u52b1\u7f51\u7edc\u53ea\u7528\u5355\u56fe\u7684\u7279\u5f81\u8fdb\u884c\u9884\u6d4b\uff1b\u6d4b\u8bd5\u7b2c\u4e00\u5f20\u56fe\u7684\u65f6\u5019\u7528\u96f6\u77e9\u9635\u66ff\u4ee3\u5c31\u53ef\u4ee5\u997f\u4e86\u3002\u4e3a\u4e86\u5ba2\u670d\u7b2c\u4e8c\u4e2a\u95ee\u9898\uff0c\u5728\u8bad\u7ec3\u7684\u65f6\u5019\uff0c\u6709 q \u7684\u6982\u7387\uff0c\u6211\u4eec\u5c06 I_{t-1} \u66ff\u6362\u4e3a\u6570\u636e\u589e\u5f3a\u540e\u7684 I_{t} , \u4f46\u662f\u4ecd\u7136\u7528 I_{t-1} \u505a\u91cd\u91c7\u6837\u5e76\u8ba1\u7b97\u81ea\u76d1\u7763\u635f\u5931\u3002\u4e5f\u5c31\u4f1a\u4fc3\u4f7f\u7f51\u7edc\u4e0d\u8981\u8fc7\u4e8e\u4f9d\u8d56cost volume\uff0c\u5e76\u5728cost volume\u662f\u6765\u81ea\u9759\u6001\u56fe\u7684\u65f6\u5019\u4e5f\u80fd\u6709\u53ef\u9760\u7684\u6df1\u5ea6\u4f30\u8ba1.","title":"Static Camera and Start of sequences"},{"location":"other_categories/depth_completion/mono_uncer/","text":"On the uncertainty of self-supervised monocular depth estimation \u8fd9\u7bc7paper\u7684\u4e3b\u8981\u76ee\u6807\u662f\u7814\u7a76\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u904d\u5386\u7814\u7a76\u4e86\u4e00\u7cfb\u5217\u8ba1\u7b97\u4e0d\u786e\u5b9a\u6027\u7684\u65b9\u6848\uff0c\u5e76\u63d0\u51fa\u4e86\u5bf9\u4e0d\u786e\u5b9a\u6027\u7684\u4f30\u8ba1\u65b9\u6cd5\u3002 \u6ce8\u610f\u672c\u6587\u5173\u6ce8\u7684\u662f\u4e0d\u786e\u5b9a\u6027\u7684\u76f8\u5bf9\u503c\u3002\u4e5f\u5c31\u662f\u76ee\u6807\u662f\u627e\u51fa\u56fe\u7247\u4e2d\u4e0d\u786e\u5b9a\u6027\u4ece\u5927\u5230\u5c0f\u7684\u6b63\u786e\u6392\u5217\uff0c\u800c\u4e0d\u662f\u5173\u6ce8\u4e0d\u786e\u5b9a\u6027\u7684\u7edd\u5bf9\u503c\u3002 Uncertainty Computation : Overview Image Flipping. \u5c06\u56fe\u7247\u7ffb\u8f6c\uff0c\u8ba1\u7b97\u4e24\u6b21\u6df1\u5ea6\u9884\u6d4b\u7ed3\u679c\u7684\u5dee\u503c\uff0c\u5404\u4e2a\u4f4d\u7f6e\u4e0a\u7684\u5dee\u503c\u5c31\u662f\u4e0d\u786e\u5b9a\u6027\u7684\u5927\u5c0f u_{post} = |d - d_{flip}| Dropout Sampling. \u4e0e\u4f20\u7edf\u7684Monte Carlo Network uncertainty \u76f8\u4f3c\uff0c\u5229\u7528dropout\u751f\u6210 N \u4e2a\u6982\u7387\u4e0a\u76f8\u5bf9\u72ec\u7acb\u7684\u6a21\u578b\uff0c\u901a\u8fc7\u8ba1\u7b97\u6bcf\u4e00\u4e2a\u50cf\u7d20\u6df1\u5ea6\u5728 N \u4e2a\u968f\u673adropout\u7f51\u7edc\u4e2d\u7684\u65b9\u5dee\u4e0e\u5747\u503c\uff0c\u5f97\u5230\u4e0d\u786e\u5b9a\u6027\u3002 Bootstrapped Ensemble. \u7528 N \u4e2a\u7f51\u7edc\u5728 N \u4e2a\u4e0d\u540c\u7684\u7f51\u7edc\u5b50\u96c6\u4e0a\u8bad\u7ec3\uff0c\u63a8\u7406\u7684\u65f6\u5019\u8fd1\u4f3c\u80fd\u5f97\u5230\u4e00\u4e2a\u5206\u5e03\uff0c\u4e0edropout\u6709\u76f8\u4f3c\u7684\u7406\u5ff5\u3002 Snapshot Ensemble. \u7528\u5faa\u73af\u7684\u5b66\u4e60\u7387\uff0c \u5faa\u73af C > N \u4e2a\u5468\u671f\uff0c\u5728\u4e00\u6b21\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u9009\u53d6\u5f97\u5230 N \u4e2a\u6a21\u578b\u3002\u7136\u540e\u505aensemble\u3002 \u4ee5\u4e0a\u4e09\u4e2a\u65b9\u6cd5\u5f52\u7c7b\u4e3aEmpirical Estimation,\u90fd\u662f\u60f3\u901a\u8fc7\u91c7\u6837 N \u4e2a\u6a21\u578b\u83b7\u53d6\u4e0d\u786e\u5b9a\u6027\u3002\u53e6\u5916\u4e00\u7c7b\u65b9\u6cd5\u5219\u662f\u901a\u8fc7\u7f51\u7edc\u9884\u6d4b\u8f93\u51fa\u4e0d\u786e\u5b9a\u6027 (Predicted Estimation)\u3002 Learned Reprojection error. \u5728\u9884\u6d4b\u65b9\u6cd5\u662f\u4e00\u4e2a\u5206\u7c7b\u4efb\u52a1(\u5149\u6d41\u548c\u53cc\u76ee)\u7684\u60c5\u51b5\u4e0b\uff0c\u6211\u4eec\u53ef\u4ee5\u8bad\u7ec3\u4e00\u4e2a\u8f93\u51fa\u5c42\uff0c\u9884\u6d4b\u6bcf\u4e2a\u50cf\u7d20\u4e0a\u7684\u56fe\u7247\u91cd\u5efa\u8bef\u5dee\u3002\u76f4\u89c9\u6765\u8bf4\u5c31\u662f\u91cd\u5efa\u8bef\u5dee\u8d8a\u5927\u7684\u5730\u65b9\uff0c\u4e0d\u786e\u5b9a\u6027\u8d8a\u9ad8\uff0c\u800c\u8fd9\u4e9b\u5f80\u5f80\u548c\u963b\u6321\u3001\u65e0\u7eb9\u7406\u7b49\u8bed\u4e49\u4fe1\u606f\u6709\u5173\uff0c\u6240\u4ee5\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u53ef\u4ee5\u5b66\u51fa\u6765\u3002 Log-likelihood Maximization. \u6709\u4e00\u4e9b\u9884\u6d4b\u65b9\u6848\u662f\u8ba9\u7f51\u7edc\u9884\u6d4b\u7f51\u7edc\u53c2\u6570\u7684\u5747\u503c\u4e0e\u65b9\u5dee(bayesian network\u4e2d\u7684\u53c2\u6570\u4e0d\u786e\u5b9a\u6027\u7684\u63cf\u8ff0)\uff0c Self-Teaching, \u52a0\u5165teacher\u7f51\u7edc\u4f5c\u4e3a\u989d\u5916\u7684\u56de\u5f52\u76d1\u7763\u3002\u4f7f\u5f97\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u6210\u4e3a\u53ef\u80fd\u3002 Bayesian Estimation.\u4e5f\u5c31\u662f\u878d\u5408\u4e86 Empirical Estimation and Predicted Estimation. Metric : How to evaluate uncertainty \u672c\u6587\u63d0\u51fa\u4e86\u4e24\u4e2ametric\u6765\u63cf\u8ff0\u8fd9\u4e2a\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u7684\u8d28\u91cf\u3002\u76f4\u89c2\u6765\u8bf4\u5c31\u662f\uff0c\u6309\u7167\u9ad8\u4e0d\u786e\u5b9a\u6027\u5230\u4f4e\u4e0d\u786e\u5b9a\u6027\u6392\u5e8f\uff0c\u9010\u6b65\u5730(\u6bcf\u6b212%)\u5254\u9664\u50cf\u7d20\uff0c\u8ba1\u7b97\u5269\u4f59\u50cf\u7d20\u7684\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u53ef\u4ee5\u5f97\u5230\u4e00\u4e2a substract w.r.t. accuracy \u7684 \u66f2\u7ebf\u3002\u8fd9\u4e2a\u66f2\u7ebf\u9762\u79ef\u8d8a\u9ad8\uff0c\u8bc1\u660e\u5254\u9664\u50cf\u7d20\u7684\u987a\u5e8f\u8d8a\u6b63\u786e(\u5927\u8bef\u5dee\u7684\u70b9\u5148\u88ab\u5254\u9664\uff0c\u51c6\u786e\u7684\u70b9\u8f83\u540e\u5254\u51fa)\u3002\u5bf9\u4e8e\u4e00\u822c\u6709\u6548\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u7b97\u6cd5\u6765\u8bf4\uff0caccuracy \u4f1a\u968f substraction\u9010\u6b65\u4e0a\u5347 (error\u5219\u4e3a\u4e0b\u964d)\u3002 \u4ee5\u4e0a\u662f\u4e00\u4e2a\u4e0e\u6df1\u5ea6\u9884\u6d4b\u51c6\u786e\u5ea6\u8026\u5408\u7684\u6570\u503c\u3002\u6211\u4eec\u9700\u8981\u7684\u662f\u4e00\u4e2a\u76f8\u5bf9\u503c\u3002\u6240\u4ee5\u8fd9\u4e2a\u50cf\u7d20\u7684\u6392\u5e8f\u7684\u4f9d\u636e\u518d\u5f15\u5165\u4e24\u79cd\u6781\u7aef\u60c5\u51b5\u4f5c\u4e3a\u6bd4\u8f83\uff0c\u7b2c\u4e00\u79cd\u662f\u8fd9\u4e2a\u4e0d\u786e\u5b9a\u6027\u5c31\u662f\u6839\u636e\u771f\u5b9e\u4e0e\u96f7\u8fbe\u70b9\u7684\u8bef\u5dee\u503c\u5f97\u5230\u7684(uncertainty \u5b8c\u7f8e\u7684\u60c5\u51b5\uff0c Sparsification Error). \u7b2c\u4e8c\u79cd\u662f\u968f\u673a\u5254\u9664\uff0c\u8fd9\u79cd\u5b9e\u9645\u8ba1\u7b97\u65f6\u5c31\u662f\u8ba9substract-accuracy\u66f2\u7ebf\u4fdd\u6301\u4e00\u4e2a\u6c34\u5e73\u7ebf\u3002 \u7f51\u7edc\u7ed9\u51fa\u7684\u4e0d\u786e\u5b9a\u6027\u66f2\u7ebf\u4e0e\u7b2c\u4e00\u79cd\u65b9\u5f0f\u7684\u5dee\u4e3a AUSE(area under the sparsification error), \u4e00\u822c\u8d8a\u5c0f\u8d8a\u597d\u3002 \u66f2\u7ebf\u4e0e\u7b2c\u4e8c\u79cd\u65b9\u5f0f\u7684\u5dee\u4e3a AURG (area under the random gain), \u4e00\u822c\u8d8a\u5927\u8d8a\u597d\u3002 \u672c\u6587\u4e3b\u8981\u5173\u6ce8 absRel error, RMSE \u548c a_1 score. \u5927\u6982\u7684\u6570\u503c\u6bd4\u8f83\u5982\u56fe:","title":"On the uncertainty of self-supervised monocular depth estimation"},{"location":"other_categories/depth_completion/mono_uncer/#on-the-uncertainty-of-self-supervised-monocular-depth-estimation","text":"\u8fd9\u7bc7paper\u7684\u4e3b\u8981\u76ee\u6807\u662f\u7814\u7a76\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u904d\u5386\u7814\u7a76\u4e86\u4e00\u7cfb\u5217\u8ba1\u7b97\u4e0d\u786e\u5b9a\u6027\u7684\u65b9\u6848\uff0c\u5e76\u63d0\u51fa\u4e86\u5bf9\u4e0d\u786e\u5b9a\u6027\u7684\u4f30\u8ba1\u65b9\u6cd5\u3002 \u6ce8\u610f\u672c\u6587\u5173\u6ce8\u7684\u662f\u4e0d\u786e\u5b9a\u6027\u7684\u76f8\u5bf9\u503c\u3002\u4e5f\u5c31\u662f\u76ee\u6807\u662f\u627e\u51fa\u56fe\u7247\u4e2d\u4e0d\u786e\u5b9a\u6027\u4ece\u5927\u5230\u5c0f\u7684\u6b63\u786e\u6392\u5217\uff0c\u800c\u4e0d\u662f\u5173\u6ce8\u4e0d\u786e\u5b9a\u6027\u7684\u7edd\u5bf9\u503c\u3002","title":"On the uncertainty of self-supervised monocular depth estimation"},{"location":"other_categories/depth_completion/mono_uncer/#uncertainty-computation-overview","text":"Image Flipping. \u5c06\u56fe\u7247\u7ffb\u8f6c\uff0c\u8ba1\u7b97\u4e24\u6b21\u6df1\u5ea6\u9884\u6d4b\u7ed3\u679c\u7684\u5dee\u503c\uff0c\u5404\u4e2a\u4f4d\u7f6e\u4e0a\u7684\u5dee\u503c\u5c31\u662f\u4e0d\u786e\u5b9a\u6027\u7684\u5927\u5c0f u_{post} = |d - d_{flip}| Dropout Sampling. \u4e0e\u4f20\u7edf\u7684Monte Carlo Network uncertainty \u76f8\u4f3c\uff0c\u5229\u7528dropout\u751f\u6210 N \u4e2a\u6982\u7387\u4e0a\u76f8\u5bf9\u72ec\u7acb\u7684\u6a21\u578b\uff0c\u901a\u8fc7\u8ba1\u7b97\u6bcf\u4e00\u4e2a\u50cf\u7d20\u6df1\u5ea6\u5728 N \u4e2a\u968f\u673adropout\u7f51\u7edc\u4e2d\u7684\u65b9\u5dee\u4e0e\u5747\u503c\uff0c\u5f97\u5230\u4e0d\u786e\u5b9a\u6027\u3002 Bootstrapped Ensemble. \u7528 N \u4e2a\u7f51\u7edc\u5728 N \u4e2a\u4e0d\u540c\u7684\u7f51\u7edc\u5b50\u96c6\u4e0a\u8bad\u7ec3\uff0c\u63a8\u7406\u7684\u65f6\u5019\u8fd1\u4f3c\u80fd\u5f97\u5230\u4e00\u4e2a\u5206\u5e03\uff0c\u4e0edropout\u6709\u76f8\u4f3c\u7684\u7406\u5ff5\u3002 Snapshot Ensemble. \u7528\u5faa\u73af\u7684\u5b66\u4e60\u7387\uff0c \u5faa\u73af C > N \u4e2a\u5468\u671f\uff0c\u5728\u4e00\u6b21\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u9009\u53d6\u5f97\u5230 N \u4e2a\u6a21\u578b\u3002\u7136\u540e\u505aensemble\u3002 \u4ee5\u4e0a\u4e09\u4e2a\u65b9\u6cd5\u5f52\u7c7b\u4e3aEmpirical Estimation,\u90fd\u662f\u60f3\u901a\u8fc7\u91c7\u6837 N \u4e2a\u6a21\u578b\u83b7\u53d6\u4e0d\u786e\u5b9a\u6027\u3002\u53e6\u5916\u4e00\u7c7b\u65b9\u6cd5\u5219\u662f\u901a\u8fc7\u7f51\u7edc\u9884\u6d4b\u8f93\u51fa\u4e0d\u786e\u5b9a\u6027 (Predicted Estimation)\u3002 Learned Reprojection error. \u5728\u9884\u6d4b\u65b9\u6cd5\u662f\u4e00\u4e2a\u5206\u7c7b\u4efb\u52a1(\u5149\u6d41\u548c\u53cc\u76ee)\u7684\u60c5\u51b5\u4e0b\uff0c\u6211\u4eec\u53ef\u4ee5\u8bad\u7ec3\u4e00\u4e2a\u8f93\u51fa\u5c42\uff0c\u9884\u6d4b\u6bcf\u4e2a\u50cf\u7d20\u4e0a\u7684\u56fe\u7247\u91cd\u5efa\u8bef\u5dee\u3002\u76f4\u89c9\u6765\u8bf4\u5c31\u662f\u91cd\u5efa\u8bef\u5dee\u8d8a\u5927\u7684\u5730\u65b9\uff0c\u4e0d\u786e\u5b9a\u6027\u8d8a\u9ad8\uff0c\u800c\u8fd9\u4e9b\u5f80\u5f80\u548c\u963b\u6321\u3001\u65e0\u7eb9\u7406\u7b49\u8bed\u4e49\u4fe1\u606f\u6709\u5173\uff0c\u6240\u4ee5\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u53ef\u4ee5\u5b66\u51fa\u6765\u3002 Log-likelihood Maximization. \u6709\u4e00\u4e9b\u9884\u6d4b\u65b9\u6848\u662f\u8ba9\u7f51\u7edc\u9884\u6d4b\u7f51\u7edc\u53c2\u6570\u7684\u5747\u503c\u4e0e\u65b9\u5dee(bayesian network\u4e2d\u7684\u53c2\u6570\u4e0d\u786e\u5b9a\u6027\u7684\u63cf\u8ff0)\uff0c Self-Teaching, \u52a0\u5165teacher\u7f51\u7edc\u4f5c\u4e3a\u989d\u5916\u7684\u56de\u5f52\u76d1\u7763\u3002\u4f7f\u5f97\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u6210\u4e3a\u53ef\u80fd\u3002 Bayesian Estimation.\u4e5f\u5c31\u662f\u878d\u5408\u4e86 Empirical Estimation and Predicted Estimation.","title":"Uncertainty Computation : Overview"},{"location":"other_categories/depth_completion/mono_uncer/#metric-how-to-evaluate-uncertainty","text":"\u672c\u6587\u63d0\u51fa\u4e86\u4e24\u4e2ametric\u6765\u63cf\u8ff0\u8fd9\u4e2a\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u7684\u8d28\u91cf\u3002\u76f4\u89c2\u6765\u8bf4\u5c31\u662f\uff0c\u6309\u7167\u9ad8\u4e0d\u786e\u5b9a\u6027\u5230\u4f4e\u4e0d\u786e\u5b9a\u6027\u6392\u5e8f\uff0c\u9010\u6b65\u5730(\u6bcf\u6b212%)\u5254\u9664\u50cf\u7d20\uff0c\u8ba1\u7b97\u5269\u4f59\u50cf\u7d20\u7684\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u53ef\u4ee5\u5f97\u5230\u4e00\u4e2a substract w.r.t. accuracy \u7684 \u66f2\u7ebf\u3002\u8fd9\u4e2a\u66f2\u7ebf\u9762\u79ef\u8d8a\u9ad8\uff0c\u8bc1\u660e\u5254\u9664\u50cf\u7d20\u7684\u987a\u5e8f\u8d8a\u6b63\u786e(\u5927\u8bef\u5dee\u7684\u70b9\u5148\u88ab\u5254\u9664\uff0c\u51c6\u786e\u7684\u70b9\u8f83\u540e\u5254\u51fa)\u3002\u5bf9\u4e8e\u4e00\u822c\u6709\u6548\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u7b97\u6cd5\u6765\u8bf4\uff0caccuracy \u4f1a\u968f substraction\u9010\u6b65\u4e0a\u5347 (error\u5219\u4e3a\u4e0b\u964d)\u3002 \u4ee5\u4e0a\u662f\u4e00\u4e2a\u4e0e\u6df1\u5ea6\u9884\u6d4b\u51c6\u786e\u5ea6\u8026\u5408\u7684\u6570\u503c\u3002\u6211\u4eec\u9700\u8981\u7684\u662f\u4e00\u4e2a\u76f8\u5bf9\u503c\u3002\u6240\u4ee5\u8fd9\u4e2a\u50cf\u7d20\u7684\u6392\u5e8f\u7684\u4f9d\u636e\u518d\u5f15\u5165\u4e24\u79cd\u6781\u7aef\u60c5\u51b5\u4f5c\u4e3a\u6bd4\u8f83\uff0c\u7b2c\u4e00\u79cd\u662f\u8fd9\u4e2a\u4e0d\u786e\u5b9a\u6027\u5c31\u662f\u6839\u636e\u771f\u5b9e\u4e0e\u96f7\u8fbe\u70b9\u7684\u8bef\u5dee\u503c\u5f97\u5230\u7684(uncertainty \u5b8c\u7f8e\u7684\u60c5\u51b5\uff0c Sparsification Error). \u7b2c\u4e8c\u79cd\u662f\u968f\u673a\u5254\u9664\uff0c\u8fd9\u79cd\u5b9e\u9645\u8ba1\u7b97\u65f6\u5c31\u662f\u8ba9substract-accuracy\u66f2\u7ebf\u4fdd\u6301\u4e00\u4e2a\u6c34\u5e73\u7ebf\u3002 \u7f51\u7edc\u7ed9\u51fa\u7684\u4e0d\u786e\u5b9a\u6027\u66f2\u7ebf\u4e0e\u7b2c\u4e00\u79cd\u65b9\u5f0f\u7684\u5dee\u4e3a AUSE(area under the sparsification error), \u4e00\u822c\u8d8a\u5c0f\u8d8a\u597d\u3002 \u66f2\u7ebf\u4e0e\u7b2c\u4e8c\u79cd\u65b9\u5f0f\u7684\u5dee\u4e3a AURG (area under the random gain), \u4e00\u822c\u8d8a\u5927\u8d8a\u597d\u3002 \u672c\u6587\u4e3b\u8981\u5173\u6ce8 absRel error, RMSE \u548c a_1 score. \u5927\u6982\u7684\u6570\u503c\u6bd4\u8f83\u5982\u56fe:","title":"Metric : How to evaluate uncertainty"},{"location":"other_categories/depth_completion/monorec/","text":"MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera \u5b98\u65b9\u9875\u9762 \u8fd9\u7bc7paper\u5b9e\u73b0\u4e86\u4f7f\u7528\u5355\u4e00\u76f8\u673a\u5e8f\u5217\u8f93\u5165\u5b9e\u73b0\u4e09\u7ef4\u91cd\u5efa. Network Architecture SSIM based cost volume \u4ee5key frame\u4e3a\u57fa\u51c6\uff0c\u8bbe\u50cf\u7d20\u6df1\u5ea6\u4e3a d , \u5c06\u56fe\u7247 I^t_{t'} \u6295\u5f71\u5230 I_t , \u57fa\u4e8eSSIM\u8ba1\u7b97photometric error pe p e(\\mathbf{x}, d)=\\frac{1-\\operatorname{SSIM}\\left(I_{t^{\\prime}}^{t}(\\mathbf{x}, d), I_{t}(\\mathbf{x})\\right)}{2} \u5bf9\u4e00\u7cfb\u5217\u7684\u56fe\u7247\u7684 t' , Cost volume\u7684\u6bcf\u4e00\u4e2a\u4f4d\u7f6e\u5b58\u50a8\u503c\u4e3a: C(\\mathbf{x}, d)=1-2 \\cdot \\frac{1}{\\sum_{t^{\\prime}} \\omega_{t^{\\prime}}} \\cdot \\sum_{t^{\\prime}} p e_{t^{\\prime}}^{t}(\\mathbf{x}, d) \\cdot \\omega_{t^{\\prime}}(\\mathbf{x}) \u5176\u4e2d\u6743\u91cd w \\begin{aligned} w_{t^{\\prime}}(\\mathbf{x})=& 1-\\frac{1}{M-1} \\\\ & \\sum_{d \\neq d^{*}} \\exp \\left(-\\alpha\\left(p e_{t^{\\prime}}^{t}(\\mathbf{x}, d)-p e_{t^{\\prime}}^{t}\\left(\\mathbf{x}, d^{*}\\right)\\right)^{2}\\right) \\end{aligned} d*_{t'} = \\argmin_d{pe_{t'}^t(x, d)} \u6307\u7684\u662f\u7279\u5b9a\u56fe\u7247\u4e0a\u67d0\u4e00\u70b9\u5728\u4e0d\u540c\u6df1\u5ea6\u3002\u82e5\u6df1\u5ea6\u5206\u5e03\u8d8a\u5e73\u6574(matching\u4e0d\u786e\u5b9a\u6027\u8d8a\u9ad8)\uff0c\u5219\u6743\u91cd w \u8d8a\u5c0f\u3002 Mask Module \u8fd9\u4e2a\u90e8\u5206\u8d1f\u8d23\u8f93\u51fa\u4e00\u4e2a\u6982\u7387\uff0c\u9884\u6d4bkeyframe\u7684\u6bcf\u4e00\u4e2a\u50cf\u7d20\u662f\u4e0d\u662f\u5c5e\u4e8emoving object.\u4f5c\u8005\u8ba4\u4e3a\uff0c\u4e24\u5e27\u56fe\u7247\u4e4b\u95f4inconsistent\u7684matching\u7ed3\u679c\u662fmoving object\u7684\u4e3b\u8981\u9884\u6d4b\u624b\u6bb5. \u672c\u6587\u6700\u7ec8\u7528\u4e00\u4e2aRes18\u63d0\u53d6 I_t \u7684\u7279\u5f81\uff0c\u7136\u540e\u4e0e \u5404\u4e2aC_{t'} \u8fdb\u884c\u878d\u5408\uff0c Depth Module \u8fd9\u4e2a\u90e8\u5206\u8d1f\u8d23\u8f93\u51fadense pixel-wise inverse depth prediction. \u9996\u5148\u7528 M_t mask\u6389 C \u4e2d\u7684\u79fb\u52a8\u50cf\u7d20\uff0c\u7136\u540e\u7528\u4e00\u4e2aU-Net\u7ed3\u6784\u8f93\u51fa\u591a\u5c3a\u5ea6\u6df1\u5ea6. Multi-stage Training \u8d77\u59cb\u9636\u6bb5: MaskModule\u548cDepthModule\u5206\u5f00\u8bad\u7ec3\u3002\u6df1\u5ea6\u6a21\u5757\u7528\u65e0mask\u7684 C \u4f5c\u4e3a\u8f93\u5165\u9884\u6d4b D_t , \u6df1\u5ea6\u7531\u7a20\u5bc6\u7684\u81ea\u76d1\u7763\u635f\u5931\u4ee5\u53ca\u7a00\u758f\u7684\u6765\u81eaVO\u7684\u6df1\u5ea6\u4f5c\u4e3a\u7a00\u758f\u76d1\u7763 \\mathcal{L}_{\\text {depth }}=\\sum_{s=0}^{3} \\mathcal{L}_{\\text {self }, s}+\\alpha \\mathcal{L}_{\\text {sparse }, s}+\\beta \\mathcal{L}_{\\text {smooth } s} MaskModule\u91c7\u7528Mask-RCNN\u4e0e\u6df1\u5ea6\u9884\u6d4b\u4ea7\u751f\u4e00\u4e2a\u5047ground truth. \u9996\u5148\u4f7f\u7528Mask-RCNN\u5206\u8fa8\u51fa movable \u7269\u4f53\u3002\u7136\u540e\u5982\u679c\u4e00\u4e2amovable \u7269\u4f53\u6709\u5f88\u591a\u50cf\u7d20\u662ftemporal inconsistent\u7684\uff0c\u90a3\u4e48\u5c31\u4f1a\u88ab\u6807\u8bb0\u4e3a moving \u7269\u4f53. MaskModule Refinement, \u5229\u7528\u53cc\u76ee\u7684\u6df1\u5ea6\u8f85\u52a9mask\u7684\u8bad\u7ec3: \\begin{aligned} \\mathcal{L}_{m_{-} r e f}=& \\sum_{s=0}^{3}\\left(M_{t} \\mathcal{L}_{\\text {depth }, s}^{\\prime S}+\\left(1-M_{t}\\right) \\mathcal{L}_{\\text {depth,s }}^{\\prime T}\\right) \\\\ &+\\mathcal{L}_{\\text {mask }} \\end{aligned} DepthModule Refinement, \u521d\u59cb\u9636\u6bb5\u6ca1\u6709\u8003\u8651mask\u7684\u5b58\u5728\u3002\u6240\u4ee5refine\u8fd9\u4e2a\u90e8\u5206\u5173\u6ce8\u4e8e\u963b\u6b62\u9759\u6001\u7269\u4f53\u7684\u4f20\u64ad \\begin{aligned} \\mathcal{L}_{d_{-} r e f, s}=&\\left(1-M_{t}\\right)\\left(\\mathcal{L}_{\\text {self }, s}+\\alpha \\mathcal{L}_{\\text {sparse,s }}\\right) \\\\ &+M_{t}\\left(\\mathcal{L}_{\\text {self }, s}^{S}+\\gamma\\left\\|D_{t}-D_{t}^{S}\\right\\|_{1}\\right) \\\\ &+\\beta \\mathcal{L}_{\\text {smooth,s }} . \\end{aligned}","title":"MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera"},{"location":"other_categories/depth_completion/monorec/#monorec-semi-supervised-dense-reconstruction-in-dynamic-environments-from-a-single-moving-camera","text":"\u5b98\u65b9\u9875\u9762 \u8fd9\u7bc7paper\u5b9e\u73b0\u4e86\u4f7f\u7528\u5355\u4e00\u76f8\u673a\u5e8f\u5217\u8f93\u5165\u5b9e\u73b0\u4e09\u7ef4\u91cd\u5efa.","title":"MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera"},{"location":"other_categories/depth_completion/monorec/#network-architecture","text":"","title":"Network Architecture"},{"location":"other_categories/depth_completion/monorec/#ssim-based-cost-volume","text":"\u4ee5key frame\u4e3a\u57fa\u51c6\uff0c\u8bbe\u50cf\u7d20\u6df1\u5ea6\u4e3a d , \u5c06\u56fe\u7247 I^t_{t'} \u6295\u5f71\u5230 I_t , \u57fa\u4e8eSSIM\u8ba1\u7b97photometric error pe p e(\\mathbf{x}, d)=\\frac{1-\\operatorname{SSIM}\\left(I_{t^{\\prime}}^{t}(\\mathbf{x}, d), I_{t}(\\mathbf{x})\\right)}{2} \u5bf9\u4e00\u7cfb\u5217\u7684\u56fe\u7247\u7684 t' , Cost volume\u7684\u6bcf\u4e00\u4e2a\u4f4d\u7f6e\u5b58\u50a8\u503c\u4e3a: C(\\mathbf{x}, d)=1-2 \\cdot \\frac{1}{\\sum_{t^{\\prime}} \\omega_{t^{\\prime}}} \\cdot \\sum_{t^{\\prime}} p e_{t^{\\prime}}^{t}(\\mathbf{x}, d) \\cdot \\omega_{t^{\\prime}}(\\mathbf{x}) \u5176\u4e2d\u6743\u91cd w \\begin{aligned} w_{t^{\\prime}}(\\mathbf{x})=& 1-\\frac{1}{M-1} \\\\ & \\sum_{d \\neq d^{*}} \\exp \\left(-\\alpha\\left(p e_{t^{\\prime}}^{t}(\\mathbf{x}, d)-p e_{t^{\\prime}}^{t}\\left(\\mathbf{x}, d^{*}\\right)\\right)^{2}\\right) \\end{aligned} d*_{t'} = \\argmin_d{pe_{t'}^t(x, d)} \u6307\u7684\u662f\u7279\u5b9a\u56fe\u7247\u4e0a\u67d0\u4e00\u70b9\u5728\u4e0d\u540c\u6df1\u5ea6\u3002\u82e5\u6df1\u5ea6\u5206\u5e03\u8d8a\u5e73\u6574(matching\u4e0d\u786e\u5b9a\u6027\u8d8a\u9ad8)\uff0c\u5219\u6743\u91cd w \u8d8a\u5c0f\u3002","title":"SSIM based cost volume"},{"location":"other_categories/depth_completion/monorec/#mask-module","text":"\u8fd9\u4e2a\u90e8\u5206\u8d1f\u8d23\u8f93\u51fa\u4e00\u4e2a\u6982\u7387\uff0c\u9884\u6d4bkeyframe\u7684\u6bcf\u4e00\u4e2a\u50cf\u7d20\u662f\u4e0d\u662f\u5c5e\u4e8emoving object.\u4f5c\u8005\u8ba4\u4e3a\uff0c\u4e24\u5e27\u56fe\u7247\u4e4b\u95f4inconsistent\u7684matching\u7ed3\u679c\u662fmoving object\u7684\u4e3b\u8981\u9884\u6d4b\u624b\u6bb5. \u672c\u6587\u6700\u7ec8\u7528\u4e00\u4e2aRes18\u63d0\u53d6 I_t \u7684\u7279\u5f81\uff0c\u7136\u540e\u4e0e \u5404\u4e2aC_{t'} \u8fdb\u884c\u878d\u5408\uff0c","title":"Mask Module"},{"location":"other_categories/depth_completion/monorec/#depth-module","text":"\u8fd9\u4e2a\u90e8\u5206\u8d1f\u8d23\u8f93\u51fadense pixel-wise inverse depth prediction. \u9996\u5148\u7528 M_t mask\u6389 C \u4e2d\u7684\u79fb\u52a8\u50cf\u7d20\uff0c\u7136\u540e\u7528\u4e00\u4e2aU-Net\u7ed3\u6784\u8f93\u51fa\u591a\u5c3a\u5ea6\u6df1\u5ea6.","title":"Depth Module"},{"location":"other_categories/depth_completion/monorec/#multi-stage-training","text":"\u8d77\u59cb\u9636\u6bb5: MaskModule\u548cDepthModule\u5206\u5f00\u8bad\u7ec3\u3002\u6df1\u5ea6\u6a21\u5757\u7528\u65e0mask\u7684 C \u4f5c\u4e3a\u8f93\u5165\u9884\u6d4b D_t , \u6df1\u5ea6\u7531\u7a20\u5bc6\u7684\u81ea\u76d1\u7763\u635f\u5931\u4ee5\u53ca\u7a00\u758f\u7684\u6765\u81eaVO\u7684\u6df1\u5ea6\u4f5c\u4e3a\u7a00\u758f\u76d1\u7763 \\mathcal{L}_{\\text {depth }}=\\sum_{s=0}^{3} \\mathcal{L}_{\\text {self }, s}+\\alpha \\mathcal{L}_{\\text {sparse }, s}+\\beta \\mathcal{L}_{\\text {smooth } s} MaskModule\u91c7\u7528Mask-RCNN\u4e0e\u6df1\u5ea6\u9884\u6d4b\u4ea7\u751f\u4e00\u4e2a\u5047ground truth. \u9996\u5148\u4f7f\u7528Mask-RCNN\u5206\u8fa8\u51fa movable \u7269\u4f53\u3002\u7136\u540e\u5982\u679c\u4e00\u4e2amovable \u7269\u4f53\u6709\u5f88\u591a\u50cf\u7d20\u662ftemporal inconsistent\u7684\uff0c\u90a3\u4e48\u5c31\u4f1a\u88ab\u6807\u8bb0\u4e3a moving \u7269\u4f53. MaskModule Refinement, \u5229\u7528\u53cc\u76ee\u7684\u6df1\u5ea6\u8f85\u52a9mask\u7684\u8bad\u7ec3: \\begin{aligned} \\mathcal{L}_{m_{-} r e f}=& \\sum_{s=0}^{3}\\left(M_{t} \\mathcal{L}_{\\text {depth }, s}^{\\prime S}+\\left(1-M_{t}\\right) \\mathcal{L}_{\\text {depth,s }}^{\\prime T}\\right) \\\\ &+\\mathcal{L}_{\\text {mask }} \\end{aligned} DepthModule Refinement, \u521d\u59cb\u9636\u6bb5\u6ca1\u6709\u8003\u8651mask\u7684\u5b58\u5728\u3002\u6240\u4ee5refine\u8fd9\u4e2a\u90e8\u5206\u5173\u6ce8\u4e8e\u963b\u6b62\u9759\u6001\u7269\u4f53\u7684\u4f20\u64ad \\begin{aligned} \\mathcal{L}_{d_{-} r e f, s}=&\\left(1-M_{t}\\right)\\left(\\mathcal{L}_{\\text {self }, s}+\\alpha \\mathcal{L}_{\\text {sparse,s }}\\right) \\\\ &+M_{t}\\left(\\mathcal{L}_{\\text {self }, s}^{S}+\\gamma\\left\\|D_{t}-D_{t}^{S}\\right\\|_{1}\\right) \\\\ &+\\beta \\mathcal{L}_{\\text {smooth,s }} . \\end{aligned}","title":"Multi-stage Training"},{"location":"other_categories/depth_completion/movedepth/","text":"Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning \u8fd9\u7bc7Paper\u63d0\u51faMoveDepthNet,\u4f7f\u7528\u591a(\u4e24)\u5e27RGB\u56fe\u9884\u6d4b\u6df1\u5ea6\uff0c\u4e0e ManyDepth \u4e00\u81f4\u3002 \u6838\u5fc3\u7684\u521b\u65b0\u601d\u8def\u5728\u4e8e\u4f7f\u7528MonoDepth\u9884\u6d4b\u4e00\u4e2aprior depth,\u7136\u540e\u4ee5\u6b64\u4e3a\u57fa\u51c6\u7f29\u5c0f\u591a\u5e27\u5339\u914d\u7684\u65f6\u5019\u7684\u9700\u8981\u8fdb\u884c\u7684\u6df1\u5ea6\u641c\u7d22\u8303\u56f4\uff0c\u63d0\u9ad8\u6df1\u5ea6\u641c\u7d22\u7cbe\u5ea6\u3002 \u8fd9\u91cc\u7684\u641c\u7d22\u65b9\u5f0f\u662f\u628a\u4ee5 <script type=\"math/tex; mode=display\">d/1.3, 1.3d \u4e4b\u95f4\u5212\u520616\u4e2a\u7b49\u5206\u5047\u8bbe\uff0c\u4ee5\u6b64\u6784\u5efacost volume.\u8fd9\u4e2a\u5e45\u5ea6\u5728\u8bad\u7ec3\u540e\u671f\u4e0e\u9884\u6d4b\u7684\u901f\u5ea6\u76f8\u5173\u3002 \u4e0e ManyDepth \u76f8\u4f3c,\u5728\u8f93\u51fa\u7684\u65f6\u5019\u91c7\u7528\u4e00\u4e2a\u7f6e\u4fe1\u5ea6mask\u8f93\u51fa\uff0c\u4e0eMonoDepth\u534f\u540c\u8f93\u51fa\u3002 \u5176\u6b21\u672c\u6587\u5f15\u5165\u4e86\u5927\u91cfMVS\u7684trick\uff0c\u4eceMVS\u7684\u89c6\u89d2\u5904\u7406\u591a\u5e27MonoDepth\u7684\u7b97\u6cd5: group correlation , \u5728\u6784\u5efacost volume\u7684\u65f6\u5019\uff0c\u628a C \u901a\u9053\u8f6c\u6362\u4e3a g \u7ec4 C/g \u7684\u8584\u7279\u5f81\uff0c\u6bcf\u4e00\u7ec4\u8584\u7279\u5f81\u5206\u522bcorrelation, \u4e0d\u540c\u7ec4\u4e4b\u95f4\u5219\u91c7\u7528 mean\u878d\u5408\u3002 local-max , \u5728\u4ece\u6df1\u5ea6mult-channel weight\u751f\u6210\u6df1\u5ea6\u7684\u65f6\u5019\uff0c\u91c7\u7528\u7684\u662flocal-max,\u4e5f\u5c31\u662f\u628a\u9884\u6d4b\u6982\u7387\u6700\u9ad8\u7684\u70b9\u4ee5\u53ca\u9644\u8fd1(\u6b63\u8d1f1)\u4e00\u4e2a\u8303\u56f4\u5185\u7684weight \u5c40\u90e8normalized \u52a0\u6743\u6c42\u5747\u503c\u3002 convex interpolation, RAFT ,\u5728 1/4\u7684\u53cc\u5e27\u5339\u914d\u8f93\u51fa\u5230\u5168\u5c3a\u5bf8\u65f6\uff0c\u91c7\u7528\u7684\u662f Aug , \u8fd9\u91cc\u7684\u505a\u6cd5\u662f\u5728\u8bad\u7ec3\u7684\u65f6\u5019\uff0c\u9664\u4e86\u4e3b\u5e72\u7684\u7f51\u7edc\u63a8\u7406\u4e0e\u8bad\u7ec3\u4e4b\u5916\uff0c\u628a\u672c\u5e27\u8f93\u51fa\u56fe\u7247\u968f\u673amask \u4e00\u4e2a\u65b9\u5757\u4e3a0\u8f93\u5165\u7f51\u7edc\u4e2d\u5f97\u5230\u7279\u5f81\uff0c\u4e0e\u6b63\u5e38\u63a8\u7406\u5f97\u5230\u7684\u7279\u5f81\u8fdb\u884c\u5339\u914d\uff0c\u5f97\u5230cost volume\u548c\u6df1\u5ea6\uff0c\u4e0e\u4e3b\u5e72\u7f51\u7edc\u5f97\u5230\u7684\u6df1\u5ea6\u548c\u7279\u5f81\u8ba1\u7b97L1-loss,\u786e\u4fdd\u4ed6\u4eec\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027 (\u8981\u6c42\u9664\u4e86masked\u65b9\u5757\u4e4b\u5916\u7684\u50cf\u7d20\u90fd\u76f8\u4f3c)\u3002 \u6700\u540e\u5f97\u5230\u4e86\u6bd4manydepth\u66f4\u4f18\u7684\u6027\u80fd\u3002","title":"Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning"},{"location":"other_categories/depth_completion/movedepth/#crafting-monocular-cues-and-velocity-guidance-for-self-supervised-multi-frame-depth-learning","text":"\u8fd9\u7bc7Paper\u63d0\u51faMoveDepthNet,\u4f7f\u7528\u591a(\u4e24)\u5e27RGB\u56fe\u9884\u6d4b\u6df1\u5ea6\uff0c\u4e0e ManyDepth \u4e00\u81f4\u3002 \u6838\u5fc3\u7684\u521b\u65b0\u601d\u8def\u5728\u4e8e\u4f7f\u7528MonoDepth\u9884\u6d4b\u4e00\u4e2aprior depth,\u7136\u540e\u4ee5\u6b64\u4e3a\u57fa\u51c6\u7f29\u5c0f\u591a\u5e27\u5339\u914d\u7684\u65f6\u5019\u7684\u9700\u8981\u8fdb\u884c\u7684\u6df1\u5ea6\u641c\u7d22\u8303\u56f4\uff0c\u63d0\u9ad8\u6df1\u5ea6\u641c\u7d22\u7cbe\u5ea6\u3002 \u8fd9\u91cc\u7684\u641c\u7d22\u65b9\u5f0f\u662f\u628a\u4ee5 <script type=\"math/tex; mode=display\">d/1.3, 1.3d \u4e4b\u95f4\u5212\u520616\u4e2a\u7b49\u5206\u5047\u8bbe\uff0c\u4ee5\u6b64\u6784\u5efacost volume.\u8fd9\u4e2a\u5e45\u5ea6\u5728\u8bad\u7ec3\u540e\u671f\u4e0e\u9884\u6d4b\u7684\u901f\u5ea6\u76f8\u5173\u3002 \u4e0e ManyDepth \u76f8\u4f3c,\u5728\u8f93\u51fa\u7684\u65f6\u5019\u91c7\u7528\u4e00\u4e2a\u7f6e\u4fe1\u5ea6mask\u8f93\u51fa\uff0c\u4e0eMonoDepth\u534f\u540c\u8f93\u51fa\u3002 \u5176\u6b21\u672c\u6587\u5f15\u5165\u4e86\u5927\u91cfMVS\u7684trick\uff0c\u4eceMVS\u7684\u89c6\u89d2\u5904\u7406\u591a\u5e27MonoDepth\u7684\u7b97\u6cd5: group correlation , \u5728\u6784\u5efacost volume\u7684\u65f6\u5019\uff0c\u628a C \u901a\u9053\u8f6c\u6362\u4e3a g \u7ec4 C/g \u7684\u8584\u7279\u5f81\uff0c\u6bcf\u4e00\u7ec4\u8584\u7279\u5f81\u5206\u522bcorrelation, \u4e0d\u540c\u7ec4\u4e4b\u95f4\u5219\u91c7\u7528 mean\u878d\u5408\u3002 local-max , \u5728\u4ece\u6df1\u5ea6mult-channel weight\u751f\u6210\u6df1\u5ea6\u7684\u65f6\u5019\uff0c\u91c7\u7528\u7684\u662flocal-max,\u4e5f\u5c31\u662f\u628a\u9884\u6d4b\u6982\u7387\u6700\u9ad8\u7684\u70b9\u4ee5\u53ca\u9644\u8fd1(\u6b63\u8d1f1)\u4e00\u4e2a\u8303\u56f4\u5185\u7684weight \u5c40\u90e8normalized \u52a0\u6743\u6c42\u5747\u503c\u3002 convex interpolation, RAFT ,\u5728 1/4\u7684\u53cc\u5e27\u5339\u914d\u8f93\u51fa\u5230\u5168\u5c3a\u5bf8\u65f6\uff0c\u91c7\u7528\u7684\u662f Aug , \u8fd9\u91cc\u7684\u505a\u6cd5\u662f\u5728\u8bad\u7ec3\u7684\u65f6\u5019\uff0c\u9664\u4e86\u4e3b\u5e72\u7684\u7f51\u7edc\u63a8\u7406\u4e0e\u8bad\u7ec3\u4e4b\u5916\uff0c\u628a\u672c\u5e27\u8f93\u51fa\u56fe\u7247\u968f\u673amask \u4e00\u4e2a\u65b9\u5757\u4e3a0\u8f93\u5165\u7f51\u7edc\u4e2d\u5f97\u5230\u7279\u5f81\uff0c\u4e0e\u6b63\u5e38\u63a8\u7406\u5f97\u5230\u7684\u7279\u5f81\u8fdb\u884c\u5339\u914d\uff0c\u5f97\u5230cost volume\u548c\u6df1\u5ea6\uff0c\u4e0e\u4e3b\u5e72\u7f51\u7edc\u5f97\u5230\u7684\u6df1\u5ea6\u548c\u7279\u5f81\u8ba1\u7b97L1-loss,\u786e\u4fdd\u4ed6\u4eec\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027 (\u8981\u6c42\u9664\u4e86masked\u65b9\u5757\u4e4b\u5916\u7684\u50cf\u7d20\u90fd\u76f8\u4f3c)\u3002 \u6700\u540e\u5f97\u5230\u4e86\u6bd4manydepth\u66f4\u4f18\u7684\u6027\u80fd\u3002","title":"Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning"},{"location":"other_categories/depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/","text":"Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks \u8fd9\u7bc7paper\u505a\u7684\u662f\u57fa\u4e8e\u89c6\u9891\u7684 Multi-view Stereo(MVS) \u8fd9\u6837\u4e00\u4e2a\u4e09\u7ef4\u91cd\u5efa\u95ee\u9898\u3002\u91cd\u70b9\u8981\u89e3\u51b3\u7684\u662f\u89c6\u9891\u4e2d\u4e0d\u540c\u5e27\u9884\u6d4b\u51fa\u6765\u7684\u6df1\u5ea6\u7684\u4e0d\u4e00\u81f4\u95ee\u9898\u3002 \u5982\u679c\u6240\u793a\u8fde\u7eed\u51e0\u5e27\u7684\u9884\u6d4b\u4e2d\uff0c\u5bf9\u4e8e\u540c\u4e00\u4e2a\u7269\u4f53\u7684\u4f30\u8ba1\u53ef\u80fd\u4f1a\u53d1\u751f\u5f88\u5f3a\u7684\u6296\u52a8\uff0c\u8fd9\u5728\u57fa\u4e8e\u89c6\u9891\u7684\u91cd\u5efa\u6765\u8bf4\u5e76\u4e0d\u597d. \u7f51\u7edc\u57fa\u672c\u7ed3\u6784 \u57fa\u7840\u67b6\u6784\u6765\u8bf4\uff0c\u8fd9\u4e2a\u7f51\u7edc\u7684hybrid Cost Volume Generation\u4e0e\u4e00\u822c\u7684MVS\u7b97\u6cd5\u8fd8\u7b97\u76f8\u4f3c\u3002Cost Volume\u91c7\u7528\u7684\u662fconcat\u7684\u505a\u6cd5\u3002\u7136\u540e\u5728\u6700\u540e\u7684Fuse \u90e8\u5206\uff0c\u628acost volume\u548c\u5355\u901a\u9053\u5728\u6df1\u5ea6\u7ef4\u5ea6\u4e0aconcat\u5728\u4e00\u8d77\u3002 EST Transformer \u672c\u6587\u8fdb\u4e00\u6b65\u5c06\u4e0d\u540c\u65f6\u95f4frame\u7684 cost volume\u8fdb\u884c\u878d\u5408\uff0c\u63d0\u5347\u4ed6\u4eec\u7684consistency. \u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u7c7b\u4f3c\u4e8e\u4e00\u4e2a non-local 3D","title":"Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks"},{"location":"other_categories/depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/#multi-view-depth-estimation-using-epipolar-spatio-temporal-networks","text":"\u8fd9\u7bc7paper\u505a\u7684\u662f\u57fa\u4e8e\u89c6\u9891\u7684 Multi-view Stereo(MVS) \u8fd9\u6837\u4e00\u4e2a\u4e09\u7ef4\u91cd\u5efa\u95ee\u9898\u3002\u91cd\u70b9\u8981\u89e3\u51b3\u7684\u662f\u89c6\u9891\u4e2d\u4e0d\u540c\u5e27\u9884\u6d4b\u51fa\u6765\u7684\u6df1\u5ea6\u7684\u4e0d\u4e00\u81f4\u95ee\u9898\u3002 \u5982\u679c\u6240\u793a\u8fde\u7eed\u51e0\u5e27\u7684\u9884\u6d4b\u4e2d\uff0c\u5bf9\u4e8e\u540c\u4e00\u4e2a\u7269\u4f53\u7684\u4f30\u8ba1\u53ef\u80fd\u4f1a\u53d1\u751f\u5f88\u5f3a\u7684\u6296\u52a8\uff0c\u8fd9\u5728\u57fa\u4e8e\u89c6\u9891\u7684\u91cd\u5efa\u6765\u8bf4\u5e76\u4e0d\u597d.","title":"Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks"},{"location":"other_categories/depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/#_1","text":"\u57fa\u7840\u67b6\u6784\u6765\u8bf4\uff0c\u8fd9\u4e2a\u7f51\u7edc\u7684hybrid Cost Volume Generation\u4e0e\u4e00\u822c\u7684MVS\u7b97\u6cd5\u8fd8\u7b97\u76f8\u4f3c\u3002Cost Volume\u91c7\u7528\u7684\u662fconcat\u7684\u505a\u6cd5\u3002\u7136\u540e\u5728\u6700\u540e\u7684Fuse \u90e8\u5206\uff0c\u628acost volume\u548c\u5355\u901a\u9053\u5728\u6df1\u5ea6\u7ef4\u5ea6\u4e0aconcat\u5728\u4e00\u8d77\u3002","title":"\u7f51\u7edc\u57fa\u672c\u7ed3\u6784"},{"location":"other_categories/depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/#est-transformer","text":"\u672c\u6587\u8fdb\u4e00\u6b65\u5c06\u4e0d\u540c\u65f6\u95f4frame\u7684 cost volume\u8fdb\u884c\u878d\u5408\uff0c\u63d0\u5347\u4ed6\u4eec\u7684consistency. \u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u7c7b\u4f3c\u4e8e\u4e00\u4e2a non-local 3D","title":"EST Transformer"},{"location":"other_categories/depth_completion/pRGB-D_SLAM/","text":"Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction \u8fd9\u7bc7paper\u878d\u5408\u4e86\u5355\u76ee\u65e0\u76d1\u7763\u7684\u6df1\u5ea6\u9884\u6d4b\u4ee5\u53caSLAM\uff0c\u5728\u8bad\u7ec3\u6570\u636e\u53ea\u6709\u5355\u76ee\u89c6\u9891\u7684\u60c5\u51b5\u4e0b\u5b8c\u6210\u5bf9\u5355\u76ee\u6df1\u5ea6\u9884\u6d4b\u7684\u8bad\u7ec3\u4ee5\u53caSLAM\u3002 \u5bf9SLAM\u4f7f\u7528\u7684\u4f18\u5316 \u672c\u6587\u4f7f\u7528\u7684SLAM\u57fa\u51c6\u662fRGB-D \u7248\u672c\u7684 ORB-SLAM, \u5728RGBD\u8bbe\u5b9a\u4e0b\uff0cORB-SLAM\u4f1a\u5c06depth\u56fe\u8f6c\u6362\u4e3adisparity \u56fe\uff0c\u4ece\u800c\u590d\u7528Stereo-ORBSLAM\u7684\u6846\u67b6\u3002\u9700\u8981\u8bbe\u5b9a\u865a\u62df\u53cc\u76ee\u7684\u57fa\u7ebf\uff0c\u5bf9\u6bcf\u4e00\u4e2a\u8f93\u5165\u5e8f\u5217,\u5148\u8ba1\u7b97\u7f51\u7edc\u9884\u6d4b\u7684\u6700\u5927\u6df1\u5ea6 d_{max} \uff0c baseline\u7684\u8bbe\u5b9a: b = \\frac{b^{KITTI}}{d_{max}^{KITTI}} * d_{max} \u6df1\u5ea6\u4f18\u5316 \u7f51\u7edc\u6df1\u5ea6\u7684\u7f51\u7edc\u662fMonoDepth2, \u4f46\u662f\u4f18\u5316\u7684\u65b9\u6cd5\u8fdb\u4e00\u6b65\u8003\u8651\u957f\u671f\u7684consistency, \u56fe\u7247\u4e2d I_c, I_{k1}, I_{k2} \u4e3a keyframes. Depth Transfer Loss \\mathcal{T}_{c \\leftrightarrow k 1}^{i}(\\mathbf{w})=\\left|d_{c \\rightarrow k 1}^{i}(\\mathbf{w})-d_{k 1}^{i}(\\mathbf{w})\\right|+\\left|d_{k 1 \\rightarrow c}^{i}(\\mathbf{w})-d_{c}^{i}(\\mathbf{w})\\right| \u4e5f\u5373\u662f\u5c06 c \u5e27\u7684\u901a\u8fc7SLAM\u5f97\u5230\u76843D keypoint\u901a\u8fc7SLAM\u5f97\u5230\u7684transformation \u8f6c\u79fb\u5230 k1 \u5e27\u89c6\u89d2\u4e2d\uff0c\u8981\u6c42\u4e24\u8005\u7684\u6df1\u5ea6\u9884\u6d4b\u503c\u76f8\u7b49\u3002\u53cd\u65b9\u5411\u7684\u8f6c\u6362\u4ea6\u7136\u3002 transfer loss\u540c\u6837\u4f5c\u7528\u4e8e c \\leftrightarrow k2, k1 \\leftrightarrow k2 Depth Consistency Loss \\mathcal{D}_{c}=\\frac{\\sum_{i \\in \\mathcal{X}}\\left|d_{c}^{i}(\\mathbf{w})-d_{c}^{i}(\\operatorname{sLAM})\\right|}{|\\mathcal{X}|} Photometric Reconstruction Loss \\mathcal{P}_{c}=p e\\left(\\mathcal{I}_{c+1 \\rightarrow c}\\left(d_{c}(\\mathbf{w}), \\mathbf{T}_{c+1 \\rightarrow c}^{\\mathrm{s} \\mathrm{LAM}}, \\mathbf{K}\\right), \\mathcal{I}_{c}\\right)+p e\\left(\\mathcal{I}_{c-1 \\rightarrow c}\\left(d_{c}(\\mathbf{w}), \\mathbf{T}_{c-1 \\rightarrow c}^{\\mathrm{SLAM}}, \\mathbf{K}\\right), \\mathcal{I}_{c}\\right) \u5229\u7528 c+1 \u7684\u6df1\u5ea6\uff0c\u76f8\u5bf9\u4f4d\u79fb\u7528 c+1 \u5e27\u7684\u56fe\u50cf\u91cd\u5efa c \u5e27\u7684\u56fe\u50cf\uff0c\u6309\u7167MonoDepth2\u7684paper\u8ba1\u7b97\u635f\u5931","title":"Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction"},{"location":"other_categories/depth_completion/pRGB-D_SLAM/#pseudo-rgb-d-for-self-improving-monocular-slam-and-depth-prediction","text":"\u8fd9\u7bc7paper\u878d\u5408\u4e86\u5355\u76ee\u65e0\u76d1\u7763\u7684\u6df1\u5ea6\u9884\u6d4b\u4ee5\u53caSLAM\uff0c\u5728\u8bad\u7ec3\u6570\u636e\u53ea\u6709\u5355\u76ee\u89c6\u9891\u7684\u60c5\u51b5\u4e0b\u5b8c\u6210\u5bf9\u5355\u76ee\u6df1\u5ea6\u9884\u6d4b\u7684\u8bad\u7ec3\u4ee5\u53caSLAM\u3002","title":"Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction"},{"location":"other_categories/depth_completion/pRGB-D_SLAM/#slam","text":"\u672c\u6587\u4f7f\u7528\u7684SLAM\u57fa\u51c6\u662fRGB-D \u7248\u672c\u7684 ORB-SLAM, \u5728RGBD\u8bbe\u5b9a\u4e0b\uff0cORB-SLAM\u4f1a\u5c06depth\u56fe\u8f6c\u6362\u4e3adisparity \u56fe\uff0c\u4ece\u800c\u590d\u7528Stereo-ORBSLAM\u7684\u6846\u67b6\u3002\u9700\u8981\u8bbe\u5b9a\u865a\u62df\u53cc\u76ee\u7684\u57fa\u7ebf\uff0c\u5bf9\u6bcf\u4e00\u4e2a\u8f93\u5165\u5e8f\u5217,\u5148\u8ba1\u7b97\u7f51\u7edc\u9884\u6d4b\u7684\u6700\u5927\u6df1\u5ea6 d_{max} \uff0c baseline\u7684\u8bbe\u5b9a: b = \\frac{b^{KITTI}}{d_{max}^{KITTI}} * d_{max}","title":"\u5bf9SLAM\u4f7f\u7528\u7684\u4f18\u5316"},{"location":"other_categories/depth_completion/pRGB-D_SLAM/#_1","text":"\u7f51\u7edc\u6df1\u5ea6\u7684\u7f51\u7edc\u662fMonoDepth2, \u4f46\u662f\u4f18\u5316\u7684\u65b9\u6cd5\u8fdb\u4e00\u6b65\u8003\u8651\u957f\u671f\u7684consistency, \u56fe\u7247\u4e2d I_c, I_{k1}, I_{k2} \u4e3a keyframes. Depth Transfer Loss \\mathcal{T}_{c \\leftrightarrow k 1}^{i}(\\mathbf{w})=\\left|d_{c \\rightarrow k 1}^{i}(\\mathbf{w})-d_{k 1}^{i}(\\mathbf{w})\\right|+\\left|d_{k 1 \\rightarrow c}^{i}(\\mathbf{w})-d_{c}^{i}(\\mathbf{w})\\right| \u4e5f\u5373\u662f\u5c06 c \u5e27\u7684\u901a\u8fc7SLAM\u5f97\u5230\u76843D keypoint\u901a\u8fc7SLAM\u5f97\u5230\u7684transformation \u8f6c\u79fb\u5230 k1 \u5e27\u89c6\u89d2\u4e2d\uff0c\u8981\u6c42\u4e24\u8005\u7684\u6df1\u5ea6\u9884\u6d4b\u503c\u76f8\u7b49\u3002\u53cd\u65b9\u5411\u7684\u8f6c\u6362\u4ea6\u7136\u3002 transfer loss\u540c\u6837\u4f5c\u7528\u4e8e c \\leftrightarrow k2, k1 \\leftrightarrow k2 Depth Consistency Loss \\mathcal{D}_{c}=\\frac{\\sum_{i \\in \\mathcal{X}}\\left|d_{c}^{i}(\\mathbf{w})-d_{c}^{i}(\\operatorname{sLAM})\\right|}{|\\mathcal{X}|} Photometric Reconstruction Loss \\mathcal{P}_{c}=p e\\left(\\mathcal{I}_{c+1 \\rightarrow c}\\left(d_{c}(\\mathbf{w}), \\mathbf{T}_{c+1 \\rightarrow c}^{\\mathrm{s} \\mathrm{LAM}}, \\mathbf{K}\\right), \\mathcal{I}_{c}\\right)+p e\\left(\\mathcal{I}_{c-1 \\rightarrow c}\\left(d_{c}(\\mathbf{w}), \\mathbf{T}_{c-1 \\rightarrow c}^{\\mathrm{SLAM}}, \\mathbf{K}\\right), \\mathcal{I}_{c}\\right) \u5229\u7528 c+1 \u7684\u6df1\u5ea6\uff0c\u76f8\u5bf9\u4f4d\u79fb\u7528 c+1 \u5e27\u7684\u56fe\u50cf\u91cd\u5efa c \u5e27\u7684\u56fe\u50cf\uff0c\u6309\u7167MonoDepth2\u7684paper\u8ba1\u7b97\u635f\u5931","title":"\u6df1\u5ea6\u4f18\u5316"},{"location":"other_categories/depth_completion/steering_kernels/","text":"Learning Steering Kernels for Guided Depth Completion \u8fd9\u7bc7paper\u7814\u7a76\u7684\u662f\u6df1\u5ea6\u8865\u5168\u95ee\u9898\uff0c\u73b0\u6709\u76f8\u5f53\u4e00\u90e8\u5206\u7684\u57fa\u4e8e\u5b66\u4e60\u7684\u6df1\u5ea6\u8865\u5168\u7f51\u7edc\u90fd\u662f\u4f7f\u7528\u5377\u79ef\u7f51\u7edc\u5bf9\u7a00\u758f\u7684\u6df1\u5ea6\u56fe\u76f4\u63a5\u8fdb\u884c\u5904\u7406 1 2 . \u4e5f\u6709\u4e00\u4e9b\u975elearning\u7684\u65b9\u6cd5\u5c1d\u8bd5\u5bf9\u6df1\u5ea6\u8fdb\u884c\u63d2\u503c\uff0c\u56fe\u7247\u662f\u4f5c\u4e3a\u5176\u4e2d\u7684\u4e00\u4e2a\u63d0\u793a 3 . \u524d\u8005\u5728\u7406\u8bba\u4e0a\u5b58\u5728\u4e00\u5b9a\u7684\u95ee\u9898\uff0c\u7531\u4e8e\u7a00\u758f\u7684\u6df1\u5ea6\u56fe\u4e2d\u6709\u5f88\u591a\u50cf\u7d20\u7684'0'\u662f\u6ca1\u6709\u7279\u6b8a\u7684\u610f\u4e49\u7684\uff0c\u76f4\u63a5\u4f7f\u7528\u5377\u79ef\u8fdb\u884c\u8ba1\u7b97\u4ece\u76f4\u89c9\u4e0e\u7406\u8bba\u6765\u8bf4\u5e76\u4e0d\u5408\u7406\uff0c\u4e14\u5982\u6b64\u7f51\u7edc\u5bf9\u4e8e\u8f93\u5165\u70b9\u7684\u7a00\u758f\u5ea6\u5c31\u6ca1\u6709\u4e86\u9c81\u68d2\u6027\uff1b\u540e\u8005\u5728\u7406\u8bba\u4e0a\u7ecf\u5e38\u90fd\u6bd4\u8f83\u5e72\u51c0\u4e14\u5408\u7406\uff0c\u8ba1\u7b97\u901f\u5ea6\u4e5f\u4f1a\u5f88\u5feb\uff0c\u4f46\u662f\u4e5f\u7531\u4e8e\u4e0d\u80fd\u6570\u636e\u9a71\u52a8\u5730\u5b66\u4e60\uff0c\u5bf9\u6574\u4f53\u7684\u6027\u80fd\u6709\u4e00\u5b9a\u7684\u5f71\u54cd\uff0c\u5927\u91cf\u7684hand-craft\u64cd\u4f5c\u4e5f\u5f71\u54cd\u4e86\u540e\u7eed\u4f18\u5316\u7684\u4e0a\u9650\u3002 \u8fd9\u7bc7paper\u7684\u7279\u70b9\u5728\u4e8e\u7ed9\u51fa\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u5b8c\u6574\u65b9\u6848\uff1a \u63d0\u51fa\u4e86\u4e00\u4e2a\u53ef\u5fae\u5206\u7684 kernel regression layer\u4f5c\u4e3a\u53ef\u5fae\u5206\u63d2\u503c\u7684\u65b9\u6cd5 \u628a\u63d2\u503c\u540e\u7ed3\u679c\u4e0eRGB\u56fe\u7247\u878d\u5408\u8f93\u51fa\u771f\u5b9e\u6df1\u5ea6\u4e0e\u63d2\u503c\u540e\u6df1\u5ea6\u56fe\u7684\u6b8b\u5dee\u3002 Kernel Regression wiki \u6df1\u5ea6\u9884\u6d4b\u503c\u53ef\u4ee5\u7531\u5df2\u6709\u7684\u7a00\u758f\u6d4b\u91cf\u4e2d\u63d2\u503c\u83b7\u5f97\uff0c\u8ba1\u7b97\u8868\u8fbe\u4e3a: \\tilde{\\mathbf{D}}(\\mathbf{x})=\\frac{\\sum_{i=1 . . P} \\mathbf{K}_{\\mathbf{H}}\\left(\\mathbf{x}_{i}-\\mathbf{x}\\right) d_{i}}{\\sum_{i=1 \\ldots P} \\mathbf{K}_{\\mathbf{H}}\\left(\\mathbf{x}_{i}-\\mathbf{x}\\right)} \u5176\u4e2d \\tilde{D} \u4e3a\u9884\u6d4b\u6df1\u5ea6\uff0c K_H{x_i - x} \u662f\u6240\u8c13\u7684\u6838\u51fd\u6570\uff0c\u4e5f\u5c31\u662f\u6bcf\u4e00\u4e2a\u7a00\u758f\u6837\u672c\u5bf9\u5e94\u7684\u4e00\u4e2a\u6743\u91cd\u3002\u8f93\u51fa\u503c\u6b63\u662f\u4e00\u4e2a\u52a0\u6743\u5e73\u5747. \u901a\u7528\u7684\u76f4\u89c9\u4e0a\u6765\u8bf4\uff0ckernel regression \u6838\u56de\u5f52\u7684\u672c\u8d28\u662f\u4e00\u79cd\u6700\u8fd1\u90bb\u63d2\u503c\uff0c\u5f97\u51fa\u9884\u6d4b\u7684\u503c\u53ef\u4ee5\u8fd1\u4f3c\u7406\u89e3\u4e3a\u9644\u8fd1\u6837\u672c\u70b9\u4e4b\u95f4\u7684\u63d2\u503c\uff0c\u8fd1\u7684\u6837\u672c\u70b9\u6743\u91cd\u9ad8\uff0c\u8fdc\u7684\u6743\u91cd\u4f4e\u3002\u8fdb\u4e00\u6b65\u7684\u9700\u8981\u8bbe\u8ba1\u4e00\u4e2a\u597d\u7684\u6838\u51fd\u6570\uff0c\u7528\u6765\u8ba1\u7b97\u70b9\u7684\"\u8ddd\u79bb\"(\u53ef\u4ee5\u662f\u56fe\u7247\u4e0a\u7684\u6b27\u6c0f\u8ddd\u79bb\uff0c\u4e5f\u53ef\u4ee5\u662fFeature\u7684\u5176\u4ed6\u7279\u6b8a\u5ea6\u91cf\u503c), \u6216\u8005\u8ba1\u7b97\u57fa\u4e8e\"\u8ddd\u79bb\"\u7684\u6743\u91cd\u503c\u3002\u5e38\u7528\u7684\u6743\u91cd\u51fd\u6570\u662f\u4e00\u4e2a\u9ad8\u65af\u51fd\u6570\u3002 Data-Independent Kernel \u5e38\u7528\u7684\u9009\u62e9\u662f\u4e00\u4e2a\u56fa\u5b9a\u7684\u9ad8\u65af\u6838: \\mathbf{K}_{\\mathbf{H}}\\left(\\mathbf{x}_{i}-\\mathbf{x}\\right)=\\frac{1}{2 \\pi \\sqrt{\\operatorname{det}\\left(\\mathbf{H}^{2}\\right)}} \\exp \\left\\{-\\frac{\\left(\\mathbf{x}_{i}-\\mathbf{x}\\right)^{T} \\mathbf{H}^{-2}\\left(\\mathbf{x}_{i}-\\mathbf{x}\\right)}{2}\\right\\} \u63a5\u8fd1\u4e8e\u7b80\u5355\u5730\u57fa\u4e8e\u7a7a\u95f4\u8ddd\u79bb\u7684\u5e73\u6ed1. \u901a\u8fc7\u5b9a\u4e49\u4e0d\u540c\u7684 H \\in \\mathbb{R}^{2\\times 2} \u53ef\u4ee5\u5f97\u5230\u4e0d\u540c\u7684\u63d2\u503c\u548c\u5e73\u6ed1\u6548\u679c. Data-Dependent Kernel \\mathbf{K}_{\\mathbf{H}_{i}}\\left(\\mathbf{x}_{i}-\\mathbf{x}\\right)=\\frac{1}{2 \\pi \\sqrt{\\operatorname{det}\\left(\\mathbf{H}_{i}^{2}\\right)}} \\exp \\left\\{-\\frac{\\left(\\mathbf{x}_{i}-\\mathbf{x}\\right)^{T} \\mathbf{H}_{i}^{-2}\\left(\\mathbf{x}_{i}-\\mathbf{x}\\right)}{2}\\right\\} \u5176\u4e2d\u7684H\u7531RGB\u56fe\u7247\u4e2d\u7684\u6bcf\u4e00\u4e2a\u70b9\u9644\u8fd1\u7684\u68af\u5ea6\u65b9\u5411\u51b3\u5b9a\uff0c\u6bcf\u4e00\u4e2a\u70b9\u90fd\u53ef\u4ee5\u6709\u81ea\u5df1\u7684 H \u77e9\u9635\uff0c\u6765\u6307\u793a\u5177\u4f53\u65b9\u5411\u3002 Differentiable Kernel Regression \u5b9a\u4e49\u6bcf\u4e00\u4e2a\u70b9\u4e0a\u7684 H \u77e9\u9635\u4e3a \\begin{aligned} H_i &= h \\mu_i C_{i}^{-\\frac{1}{2}} \\\\ &= h \\mu_i \\gamma_i U_{\\theta_i} \\Lambda_i U_{\\theta_i}^T\\\\ U_{\\theta_i} &= \\begin{bmatrix} \\text{cos}\\theta_i & \\text{sin}\\theta_i \\\\ -\\text{sin}\\theta_i & \\text{cos} \\theta_i \\end{bmatrix} \\\\ \\Lambda_i &= \\begin{bmatrix}\\sigma_i & 0 \\\\ 0 & \\sigma_i^{-1} \\end{bmatrix} \\end{aligned} \u5176\u4e2d h , \\mu_i \u8bbe\u5b9a\u4e3a\u5e38\u6570\u3002 \\gamma, \\theta, \\sigma \u5206\u522b\u8868\u8fbe scaling, rotation\u4ee5\u53ca elongation. \u7f51\u7edc\u4f1a\u8f93\u51fa\u7a20\u5bc6\u7684\u4e09\u4e2a \\Gamma, \\Theta, \\Sigma \u77e9\u9635\uff0c\u7528\u6765\u8ba1\u7b97\u6bcf\u4e00\u4e2a\u70b9\u7684Kernel Formulation.\u6574\u4e2akernel regression\u6d41\u7a0b\u4f1a\u7ef4\u6301\u53ef\u5fae\u5206\u3002","title":"Learning Steering Kernels for Guided Depth Completion"},{"location":"other_categories/depth_completion/steering_kernels/#learning-steering-kernels-for-guided-depth-completion","text":"\u8fd9\u7bc7paper\u7814\u7a76\u7684\u662f\u6df1\u5ea6\u8865\u5168\u95ee\u9898\uff0c\u73b0\u6709\u76f8\u5f53\u4e00\u90e8\u5206\u7684\u57fa\u4e8e\u5b66\u4e60\u7684\u6df1\u5ea6\u8865\u5168\u7f51\u7edc\u90fd\u662f\u4f7f\u7528\u5377\u79ef\u7f51\u7edc\u5bf9\u7a00\u758f\u7684\u6df1\u5ea6\u56fe\u76f4\u63a5\u8fdb\u884c\u5904\u7406 1 2 . \u4e5f\u6709\u4e00\u4e9b\u975elearning\u7684\u65b9\u6cd5\u5c1d\u8bd5\u5bf9\u6df1\u5ea6\u8fdb\u884c\u63d2\u503c\uff0c\u56fe\u7247\u662f\u4f5c\u4e3a\u5176\u4e2d\u7684\u4e00\u4e2a\u63d0\u793a 3 . \u524d\u8005\u5728\u7406\u8bba\u4e0a\u5b58\u5728\u4e00\u5b9a\u7684\u95ee\u9898\uff0c\u7531\u4e8e\u7a00\u758f\u7684\u6df1\u5ea6\u56fe\u4e2d\u6709\u5f88\u591a\u50cf\u7d20\u7684'0'\u662f\u6ca1\u6709\u7279\u6b8a\u7684\u610f\u4e49\u7684\uff0c\u76f4\u63a5\u4f7f\u7528\u5377\u79ef\u8fdb\u884c\u8ba1\u7b97\u4ece\u76f4\u89c9\u4e0e\u7406\u8bba\u6765\u8bf4\u5e76\u4e0d\u5408\u7406\uff0c\u4e14\u5982\u6b64\u7f51\u7edc\u5bf9\u4e8e\u8f93\u5165\u70b9\u7684\u7a00\u758f\u5ea6\u5c31\u6ca1\u6709\u4e86\u9c81\u68d2\u6027\uff1b\u540e\u8005\u5728\u7406\u8bba\u4e0a\u7ecf\u5e38\u90fd\u6bd4\u8f83\u5e72\u51c0\u4e14\u5408\u7406\uff0c\u8ba1\u7b97\u901f\u5ea6\u4e5f\u4f1a\u5f88\u5feb\uff0c\u4f46\u662f\u4e5f\u7531\u4e8e\u4e0d\u80fd\u6570\u636e\u9a71\u52a8\u5730\u5b66\u4e60\uff0c\u5bf9\u6574\u4f53\u7684\u6027\u80fd\u6709\u4e00\u5b9a\u7684\u5f71\u54cd\uff0c\u5927\u91cf\u7684hand-craft\u64cd\u4f5c\u4e5f\u5f71\u54cd\u4e86\u540e\u7eed\u4f18\u5316\u7684\u4e0a\u9650\u3002 \u8fd9\u7bc7paper\u7684\u7279\u70b9\u5728\u4e8e\u7ed9\u51fa\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u5b8c\u6574\u65b9\u6848\uff1a \u63d0\u51fa\u4e86\u4e00\u4e2a\u53ef\u5fae\u5206\u7684 kernel regression layer\u4f5c\u4e3a\u53ef\u5fae\u5206\u63d2\u503c\u7684\u65b9\u6cd5 \u628a\u63d2\u503c\u540e\u7ed3\u679c\u4e0eRGB\u56fe\u7247\u878d\u5408\u8f93\u51fa\u771f\u5b9e\u6df1\u5ea6\u4e0e\u63d2\u503c\u540e\u6df1\u5ea6\u56fe\u7684\u6b8b\u5dee\u3002","title":"Learning Steering Kernels for Guided Depth Completion"},{"location":"other_categories/depth_completion/steering_kernels/#kernel-regression","text":"wiki \u6df1\u5ea6\u9884\u6d4b\u503c\u53ef\u4ee5\u7531\u5df2\u6709\u7684\u7a00\u758f\u6d4b\u91cf\u4e2d\u63d2\u503c\u83b7\u5f97\uff0c\u8ba1\u7b97\u8868\u8fbe\u4e3a: \\tilde{\\mathbf{D}}(\\mathbf{x})=\\frac{\\sum_{i=1 . . P} \\mathbf{K}_{\\mathbf{H}}\\left(\\mathbf{x}_{i}-\\mathbf{x}\\right) d_{i}}{\\sum_{i=1 \\ldots P} \\mathbf{K}_{\\mathbf{H}}\\left(\\mathbf{x}_{i}-\\mathbf{x}\\right)} \u5176\u4e2d \\tilde{D} \u4e3a\u9884\u6d4b\u6df1\u5ea6\uff0c K_H{x_i - x} \u662f\u6240\u8c13\u7684\u6838\u51fd\u6570\uff0c\u4e5f\u5c31\u662f\u6bcf\u4e00\u4e2a\u7a00\u758f\u6837\u672c\u5bf9\u5e94\u7684\u4e00\u4e2a\u6743\u91cd\u3002\u8f93\u51fa\u503c\u6b63\u662f\u4e00\u4e2a\u52a0\u6743\u5e73\u5747. \u901a\u7528\u7684\u76f4\u89c9\u4e0a\u6765\u8bf4\uff0ckernel regression \u6838\u56de\u5f52\u7684\u672c\u8d28\u662f\u4e00\u79cd\u6700\u8fd1\u90bb\u63d2\u503c\uff0c\u5f97\u51fa\u9884\u6d4b\u7684\u503c\u53ef\u4ee5\u8fd1\u4f3c\u7406\u89e3\u4e3a\u9644\u8fd1\u6837\u672c\u70b9\u4e4b\u95f4\u7684\u63d2\u503c\uff0c\u8fd1\u7684\u6837\u672c\u70b9\u6743\u91cd\u9ad8\uff0c\u8fdc\u7684\u6743\u91cd\u4f4e\u3002\u8fdb\u4e00\u6b65\u7684\u9700\u8981\u8bbe\u8ba1\u4e00\u4e2a\u597d\u7684\u6838\u51fd\u6570\uff0c\u7528\u6765\u8ba1\u7b97\u70b9\u7684\"\u8ddd\u79bb\"(\u53ef\u4ee5\u662f\u56fe\u7247\u4e0a\u7684\u6b27\u6c0f\u8ddd\u79bb\uff0c\u4e5f\u53ef\u4ee5\u662fFeature\u7684\u5176\u4ed6\u7279\u6b8a\u5ea6\u91cf\u503c), \u6216\u8005\u8ba1\u7b97\u57fa\u4e8e\"\u8ddd\u79bb\"\u7684\u6743\u91cd\u503c\u3002\u5e38\u7528\u7684\u6743\u91cd\u51fd\u6570\u662f\u4e00\u4e2a\u9ad8\u65af\u51fd\u6570\u3002","title":"Kernel Regression"},{"location":"other_categories/depth_completion/steering_kernels/#data-independent-kernel","text":"\u5e38\u7528\u7684\u9009\u62e9\u662f\u4e00\u4e2a\u56fa\u5b9a\u7684\u9ad8\u65af\u6838: \\mathbf{K}_{\\mathbf{H}}\\left(\\mathbf{x}_{i}-\\mathbf{x}\\right)=\\frac{1}{2 \\pi \\sqrt{\\operatorname{det}\\left(\\mathbf{H}^{2}\\right)}} \\exp \\left\\{-\\frac{\\left(\\mathbf{x}_{i}-\\mathbf{x}\\right)^{T} \\mathbf{H}^{-2}\\left(\\mathbf{x}_{i}-\\mathbf{x}\\right)}{2}\\right\\} \u63a5\u8fd1\u4e8e\u7b80\u5355\u5730\u57fa\u4e8e\u7a7a\u95f4\u8ddd\u79bb\u7684\u5e73\u6ed1. \u901a\u8fc7\u5b9a\u4e49\u4e0d\u540c\u7684 H \\in \\mathbb{R}^{2\\times 2} \u53ef\u4ee5\u5f97\u5230\u4e0d\u540c\u7684\u63d2\u503c\u548c\u5e73\u6ed1\u6548\u679c.","title":"Data-Independent Kernel"},{"location":"other_categories/depth_completion/steering_kernels/#data-dependent-kernel","text":"\\mathbf{K}_{\\mathbf{H}_{i}}\\left(\\mathbf{x}_{i}-\\mathbf{x}\\right)=\\frac{1}{2 \\pi \\sqrt{\\operatorname{det}\\left(\\mathbf{H}_{i}^{2}\\right)}} \\exp \\left\\{-\\frac{\\left(\\mathbf{x}_{i}-\\mathbf{x}\\right)^{T} \\mathbf{H}_{i}^{-2}\\left(\\mathbf{x}_{i}-\\mathbf{x}\\right)}{2}\\right\\} \u5176\u4e2d\u7684H\u7531RGB\u56fe\u7247\u4e2d\u7684\u6bcf\u4e00\u4e2a\u70b9\u9644\u8fd1\u7684\u68af\u5ea6\u65b9\u5411\u51b3\u5b9a\uff0c\u6bcf\u4e00\u4e2a\u70b9\u90fd\u53ef\u4ee5\u6709\u81ea\u5df1\u7684 H \u77e9\u9635\uff0c\u6765\u6307\u793a\u5177\u4f53\u65b9\u5411\u3002","title":"Data-Dependent Kernel"},{"location":"other_categories/depth_completion/steering_kernels/#differentiable-kernel-regression","text":"\u5b9a\u4e49\u6bcf\u4e00\u4e2a\u70b9\u4e0a\u7684 H \u77e9\u9635\u4e3a \\begin{aligned} H_i &= h \\mu_i C_{i}^{-\\frac{1}{2}} \\\\ &= h \\mu_i \\gamma_i U_{\\theta_i} \\Lambda_i U_{\\theta_i}^T\\\\ U_{\\theta_i} &= \\begin{bmatrix} \\text{cos}\\theta_i & \\text{sin}\\theta_i \\\\ -\\text{sin}\\theta_i & \\text{cos} \\theta_i \\end{bmatrix} \\\\ \\Lambda_i &= \\begin{bmatrix}\\sigma_i & 0 \\\\ 0 & \\sigma_i^{-1} \\end{bmatrix} \\end{aligned} \u5176\u4e2d h , \\mu_i \u8bbe\u5b9a\u4e3a\u5e38\u6570\u3002 \\gamma, \\theta, \\sigma \u5206\u522b\u8868\u8fbe scaling, rotation\u4ee5\u53ca elongation. \u7f51\u7edc\u4f1a\u8f93\u51fa\u7a20\u5bc6\u7684\u4e09\u4e2a \\Gamma, \\Theta, \\Sigma \u77e9\u9635\uff0c\u7528\u6765\u8ba1\u7b97\u6bcf\u4e00\u4e2a\u70b9\u7684Kernel Formulation.\u6574\u4e2akernel regression\u6d41\u7a0b\u4f1a\u7ef4\u6301\u53ef\u5fae\u5206\u3002","title":"Differentiable Kernel Regression"},{"location":"other_categories/depth_completion/sub_depth/","text":"SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation \u8fd9\u7bc7\u8bba\u6587\u5728\u81ea\u76d1\u7763\u6df1\u5ea6\u9884\u6d4b\u7684\u95ee\u9898\u4e0a\u5f15\u5165\u4e86\u84b8\u998f\u4ee5\u53ca\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u3002 \u7f51\u7edc\u67b6\u6784 \u6559\u5e08\u7f51\u7edc\u8f93\u51fa\u4e00\u4e2a\u76d1\u7763\u4fe1\u53f7\uff0c\u7ed9\u81ea\u7f51\u7edc\u8bad\u7ec3\uff0c\u6559\u5e08\u7f51\u7edc\u662f\u7528\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\uff0c\u53ea\u4f7f\u7528\u9ad8\u4eae\u7684\u57fa\u672c\u65b9\u6848\u8bad\u7ec3\u3002 \u5c3d\u7ba1\u6559\u5e08\u7f51\u7edc\u7684\u8bad\u7ec3\u65b9\u6cd5\u4ecd\u7136\u8fd8\u53ea\u662f\u7528\u539f\u6765\u7684\u65e0\u76d1\u7763\u65b9\u6848\u8bad\u7ec3\uff0c\u5b9e\u9a8c\u8bc1\u660e\u8fd9\u6837\u7684self-distillation\u4e5f\u80fd\u6709\u63d0\u5347\uff0c \u4e0d\u786e\u5b9a\u6027\u4e0a\uff0c\u628a\u7cfb\u7edf\u7684\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u4e3aLaplace\u5206\u5e03(L1\u8bad\u7ec3)\uff0c \u5206\u522b\u65f6\u56fe\u7247\u91cd\u5efa\u7684\u4e0d\u786e\u5b9a\u6027\u4ee5\u53ca\u81ea\u84b8\u998f\u7684\u4e0d\u786e\u5b9a\u6027\u3002 l_{reconstruction} = \\frac{l_{photometric}}{\\sigma_{pho}} + log(\\sigma_{pho}) l_{distillation} = \\frac{l_{regression}}{\\sigma_{reg}} + log(\\sigma_{reg})","title":"SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation"},{"location":"other_categories/depth_completion/sub_depth/#sub-depth-self-distillation-and-uncertainty-boosting-self-supervised-monocular-depth-estimation","text":"\u8fd9\u7bc7\u8bba\u6587\u5728\u81ea\u76d1\u7763\u6df1\u5ea6\u9884\u6d4b\u7684\u95ee\u9898\u4e0a\u5f15\u5165\u4e86\u84b8\u998f\u4ee5\u53ca\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u3002","title":"SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation"},{"location":"other_categories/depth_completion/sub_depth/#_1","text":"\u6559\u5e08\u7f51\u7edc\u8f93\u51fa\u4e00\u4e2a\u76d1\u7763\u4fe1\u53f7\uff0c\u7ed9\u81ea\u7f51\u7edc\u8bad\u7ec3\uff0c\u6559\u5e08\u7f51\u7edc\u662f\u7528\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\uff0c\u53ea\u4f7f\u7528\u9ad8\u4eae\u7684\u57fa\u672c\u65b9\u6848\u8bad\u7ec3\u3002 \u5c3d\u7ba1\u6559\u5e08\u7f51\u7edc\u7684\u8bad\u7ec3\u65b9\u6cd5\u4ecd\u7136\u8fd8\u53ea\u662f\u7528\u539f\u6765\u7684\u65e0\u76d1\u7763\u65b9\u6848\u8bad\u7ec3\uff0c\u5b9e\u9a8c\u8bc1\u660e\u8fd9\u6837\u7684self-distillation\u4e5f\u80fd\u6709\u63d0\u5347\uff0c \u4e0d\u786e\u5b9a\u6027\u4e0a\uff0c\u628a\u7cfb\u7edf\u7684\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u4e3aLaplace\u5206\u5e03(L1\u8bad\u7ec3)\uff0c \u5206\u522b\u65f6\u56fe\u7247\u91cd\u5efa\u7684\u4e0d\u786e\u5b9a\u6027\u4ee5\u53ca\u81ea\u84b8\u998f\u7684\u4e0d\u786e\u5b9a\u6027\u3002 l_{reconstruction} = \\frac{l_{photometric}}{\\sigma_{pho}} + log(\\sigma_{pho}) l_{distillation} = \\frac{l_{regression}}{\\sigma_{reg}} + log(\\sigma_{reg})","title":"\u7f51\u7edc\u67b6\u6784"},{"location":"other_categories/depth_completion/sungoesdown/","text":"When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation \u8fd9\u7bc7paper\u89e3\u51b3\u7684\u662f\u5728\u591c\u95f4\u73af\u5883\u4e0b\u7684monodepth2\u8bad\u7ec3\u95ee\u9898\u3002 \u9996\u5148\u4f5c\u8005\u603b\u7ed3\u4e86\u4e00\u4e0bmonodepth2\u8bad\u7ec3\u9700\u8981\u7684\u5047\u8bbe\uff1a \u65f6\u5e8f\u4e0a\u5149\u7167\u7684\u4e00\u81f4\u6027 \u9759\u6b62\u573a\u666f \u6ca1\u6709\u56fe\u7247\u566a\u58f0\u4e0e\u906e\u6321 \u5728\u767d\u5929\u73af\u5883\u4e0b\uff0c\u4e3b\u8981\u7684\u5149\u6e90\u662f\u592a\u9633\u5149\uff0c\u592a\u9633\u5149\u53ea\u4f1a\u968f\u7740\u65f6\u95f4\u7f13\u6162\u53d8\u5316\uff0c\u4f46\u662f\u5728\u591c\u665a\u73af\u5883\u4e0b\uff0c\u4e3b\u8981\u7684\u5149\u6e90\u662f\u8def\u706f\u3001\u8f66\u706f\u7b49\u70b9\u5149\u6e90\uff0c\u968f\u7740\u6444\u50cf\u673a\u4e0e\u8fd9\u4e9b\u5149\u6e90\u7684\u4f4d\u7f6e\u53d8\u5316\uff0c\u753b\u9762\u6574\u4f53\u7684\u5149\u7167\u4f1a\u6709\u6bd4\u8f83\u5927\u7684\u53d8\u5316\uff0c\u8f66\u706f\u7684\u70b9\u5149\u6e90\u66f4\u662f\u8fd0\u52a8\u7269\u4f53\u3002\u4e14\u76f8\u673a\u4e3a\u4e86\u62cd\u6444\u591c\u665a\u7684\u56fe\u7247\uff0c\u5f80\u5f80\u4f1a\u6709\u6bd4\u8f83\u5927\u7684\u968f\u673a\u566a\u70b9\uff1b\u4e14\u968f\u7740\u76f8\u673a\u66dd\u5149\u65f6\u95f4\u7684\u589e\u957f\uff0c\u56fe\u7247\u4e2d\u52a8\u6001\u7269\u4f53\u7684\u52a8\u6001\u6a21\u7cca\u4f1a\u66f4\u52a0\u4e25\u91cd\u3002 \u6574\u4f53\u7684\u8ba1\u7b97\u516c\u5f0f \\begin{aligned} D_{t} &=\\mathcal{D}\\left(I_{t}\\right), f_{n}=\\mathcal{M} \\mathcal{E}_{1: n}\\left(\\left[I_{t}, I_{t+1}\\right]\\right) \\\\ T_{t, t+1} &=\\mathcal{M D}\\left(f_{N}\\right), R_{t}=\\mathcal{R F} \\mathcal{F}\\left(\\left\\{f_{n}: 1 \\leq n \\leq N\\right\\}\\right),\\left(C_{t}, B_{t}\\right)=\\mathcal{L C D}\\left(f_{N}\\right) \\\\ \\mathcal{I}_{t} &=\\mathcal{D N}\\left(I_{t}\\right), \\mathcal{I}_{t+1}=\\mathcal{D N}\\left(I_{t+1}\\right) \\\\ \\mathcal{I}_{t}^{\\prime} &=\\operatorname{reconstruct}\\left(\\mathcal{I}_{t+1}, V_{t}+R_{t}\\right) \\\\ \\tilde{\\mathcal{I}}_{t} &=C_{t} \\odot \\mathcal{I}_{t}^{\\prime}+B_{t} \\\\ L_{p}^{(t)} &=\\frac{1}{\\left|M_{t}\\right|} \\sum_{\\mathbf{u} \\in M_{t}}\\left(\\alpha \\frac{1-\\operatorname{SSIM}\\left(\\mathcal{I}_{t}(\\mathbf{u}), \\tilde{\\mathcal{I}}_{t}(\\mathbf{u})\\right)}{2}+(1-\\alpha)\\left|\\mathcal{I}_{t}(\\mathbf{u})-\\tilde{\\mathcal{I}}_{t}(\\mathbf{u})\\right|\\right) \\end{aligned} \u4e0emonodepth2\u9644\u52a0\u7684\u4e3b\u8981\u5185\u5bb9\u662f: \u4f7f\u7528\u4e00\u4e2a\u65e0\u76d1\u7763\u7684\u53bb\u566a\u58f0\u7f51\u7edc \\mathbf{DN} \u5728\u8ba1\u7b97\u635f\u5931\u7684\u65f6\u5019\u8fdb\u884c\u53bb\u566a\u3002\u91c7\u7528\u7684\u65b9\u6848\u662fneighbour2neighbour. \u4f7f\u7528\u7f51\u7edc\u9884\u6d4b\u4e00\u4e2a pixel-wise residual flow R_t , \u5728\u91cd\u5efa\u56fe\u7247\u7684\u65f6\u5019\u5bf9\u91cd\u6295\u5f71\u5750\u6807\u52a0\u4e00\u4e2a\u504f\u79fb\u3002 \u4f7f\u7528\u7f51\u7edc\u9884\u6d4b\u4e00\u4e2a pixel-wise photometric augmentation. C_t, B_t \u5982\u516c\u5f0f\u4f7f\u7528\u3002 \u635f\u5931\u51fd\u6570\u9664\u4e86\u539f\u6709\u7684photometric loss\u51fd\u6570\u4e4b\u5916\uff0c\u8fd8\u52a0\u4e0aresidual flow\u7684sparcity regularization. L_{r}^{(t)}=\\sum_{s=0}^{3}\\left\\langle\\left|R_{t, s}\\right|\\right\\rangle / 2^{s} \\sum_{\\mathbf{u} \\in \\Omega\\left(I_{t, s}\\right)} \\sqrt{1+\\left|R_{t, s}(\\mathbf{u})\\right| /\\left\\langle\\left|R_{t, s}\\right|\\right\\rangle}","title":"When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation"},{"location":"other_categories/depth_completion/sungoesdown/#when-the-sun-goes-down-repairing-photometric-losses-for-all-day-depth-estimation","text":"\u8fd9\u7bc7paper\u89e3\u51b3\u7684\u662f\u5728\u591c\u95f4\u73af\u5883\u4e0b\u7684monodepth2\u8bad\u7ec3\u95ee\u9898\u3002 \u9996\u5148\u4f5c\u8005\u603b\u7ed3\u4e86\u4e00\u4e0bmonodepth2\u8bad\u7ec3\u9700\u8981\u7684\u5047\u8bbe\uff1a \u65f6\u5e8f\u4e0a\u5149\u7167\u7684\u4e00\u81f4\u6027 \u9759\u6b62\u573a\u666f \u6ca1\u6709\u56fe\u7247\u566a\u58f0\u4e0e\u906e\u6321 \u5728\u767d\u5929\u73af\u5883\u4e0b\uff0c\u4e3b\u8981\u7684\u5149\u6e90\u662f\u592a\u9633\u5149\uff0c\u592a\u9633\u5149\u53ea\u4f1a\u968f\u7740\u65f6\u95f4\u7f13\u6162\u53d8\u5316\uff0c\u4f46\u662f\u5728\u591c\u665a\u73af\u5883\u4e0b\uff0c\u4e3b\u8981\u7684\u5149\u6e90\u662f\u8def\u706f\u3001\u8f66\u706f\u7b49\u70b9\u5149\u6e90\uff0c\u968f\u7740\u6444\u50cf\u673a\u4e0e\u8fd9\u4e9b\u5149\u6e90\u7684\u4f4d\u7f6e\u53d8\u5316\uff0c\u753b\u9762\u6574\u4f53\u7684\u5149\u7167\u4f1a\u6709\u6bd4\u8f83\u5927\u7684\u53d8\u5316\uff0c\u8f66\u706f\u7684\u70b9\u5149\u6e90\u66f4\u662f\u8fd0\u52a8\u7269\u4f53\u3002\u4e14\u76f8\u673a\u4e3a\u4e86\u62cd\u6444\u591c\u665a\u7684\u56fe\u7247\uff0c\u5f80\u5f80\u4f1a\u6709\u6bd4\u8f83\u5927\u7684\u968f\u673a\u566a\u70b9\uff1b\u4e14\u968f\u7740\u76f8\u673a\u66dd\u5149\u65f6\u95f4\u7684\u589e\u957f\uff0c\u56fe\u7247\u4e2d\u52a8\u6001\u7269\u4f53\u7684\u52a8\u6001\u6a21\u7cca\u4f1a\u66f4\u52a0\u4e25\u91cd\u3002 \u6574\u4f53\u7684\u8ba1\u7b97\u516c\u5f0f \\begin{aligned} D_{t} &=\\mathcal{D}\\left(I_{t}\\right), f_{n}=\\mathcal{M} \\mathcal{E}_{1: n}\\left(\\left[I_{t}, I_{t+1}\\right]\\right) \\\\ T_{t, t+1} &=\\mathcal{M D}\\left(f_{N}\\right), R_{t}=\\mathcal{R F} \\mathcal{F}\\left(\\left\\{f_{n}: 1 \\leq n \\leq N\\right\\}\\right),\\left(C_{t}, B_{t}\\right)=\\mathcal{L C D}\\left(f_{N}\\right) \\\\ \\mathcal{I}_{t} &=\\mathcal{D N}\\left(I_{t}\\right), \\mathcal{I}_{t+1}=\\mathcal{D N}\\left(I_{t+1}\\right) \\\\ \\mathcal{I}_{t}^{\\prime} &=\\operatorname{reconstruct}\\left(\\mathcal{I}_{t+1}, V_{t}+R_{t}\\right) \\\\ \\tilde{\\mathcal{I}}_{t} &=C_{t} \\odot \\mathcal{I}_{t}^{\\prime}+B_{t} \\\\ L_{p}^{(t)} &=\\frac{1}{\\left|M_{t}\\right|} \\sum_{\\mathbf{u} \\in M_{t}}\\left(\\alpha \\frac{1-\\operatorname{SSIM}\\left(\\mathcal{I}_{t}(\\mathbf{u}), \\tilde{\\mathcal{I}}_{t}(\\mathbf{u})\\right)}{2}+(1-\\alpha)\\left|\\mathcal{I}_{t}(\\mathbf{u})-\\tilde{\\mathcal{I}}_{t}(\\mathbf{u})\\right|\\right) \\end{aligned} \u4e0emonodepth2\u9644\u52a0\u7684\u4e3b\u8981\u5185\u5bb9\u662f: \u4f7f\u7528\u4e00\u4e2a\u65e0\u76d1\u7763\u7684\u53bb\u566a\u58f0\u7f51\u7edc \\mathbf{DN} \u5728\u8ba1\u7b97\u635f\u5931\u7684\u65f6\u5019\u8fdb\u884c\u53bb\u566a\u3002\u91c7\u7528\u7684\u65b9\u6848\u662fneighbour2neighbour. \u4f7f\u7528\u7f51\u7edc\u9884\u6d4b\u4e00\u4e2a pixel-wise residual flow R_t , \u5728\u91cd\u5efa\u56fe\u7247\u7684\u65f6\u5019\u5bf9\u91cd\u6295\u5f71\u5750\u6807\u52a0\u4e00\u4e2a\u504f\u79fb\u3002 \u4f7f\u7528\u7f51\u7edc\u9884\u6d4b\u4e00\u4e2a pixel-wise photometric augmentation. C_t, B_t \u5982\u516c\u5f0f\u4f7f\u7528\u3002 \u635f\u5931\u51fd\u6570\u9664\u4e86\u539f\u6709\u7684photometric loss\u51fd\u6570\u4e4b\u5916\uff0c\u8fd8\u52a0\u4e0aresidual flow\u7684sparcity regularization. L_{r}^{(t)}=\\sum_{s=0}^{3}\\left\\langle\\left|R_{t, s}\\right|\\right\\rangle / 2^{s} \\sum_{\\mathbf{u} \\in \\Omega\\left(I_{t, s}\\right)} \\sqrt{1+\\left|R_{t, s}(\\mathbf{u})\\right| /\\left\\langle\\left|R_{t, s}\\right|\\right\\rangle}","title":"When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation"},{"location":"other_categories/depth_completion/variational_monodepth/","text":"Variational Monocular Depth Estimation for Reliability Prediction \u8fd9\u7bc7paper\u4ee5\u7f51\u7edc\u8f93\u51fa\u6bcf\u4e2a\u70b9\u7684\u6df1\u5ea6\u7684\u6982\u7387\u503c\u4e3a\u72ec\u7acb\u7684\u9ad8\u65af\u5206\u5e03\u4e3a\u57fa\u7840\uff0c\u6b63\u9762\u5730\u4ece\u6982\u7387\u63a8\u5bfc\u4e86\u5355\u76ee\u6df1\u5ea6\u7684variational inference \u65b9\u6cd5. From Probabilistic to Loss function \u8bbe\u56fe\u7247\u5e8f\u5217 \\mathcal{I} = \\{I_0, ..., I_{N_t}\\} . \u5047\u8bbe\u6df1\u5ea6\u56fe\u662f\u7531\u70b9\u4e91 Q \u8fd9\u4e00\u9690\u53d8\u91cf\u51b3\u5b9a\u7684. \u4ece\u56fe\u7247\u4e2d\u4f30\u8ba1\u51fa\u70b9\u4e91\u7684\u6982\u7387\u8bb0\u4e3a p_{\\theta^*}(Q_{t-1} | I_{t-1}) . \u4e0b\u4e00\u65f6\u523b\u7684\u56fe\u7247\u4e5f\u662f\u6709\u76f8\u540c\u7684\u70b9\u4e91\u751f\u6210\u7684 p_{\\theta^*}(I_t | I_{t-1}, Q_{t-1}) \\theta^* = \\underset{\\theta}{\\text{argmax}} \\log p_\\theta(\\mathcal{I}) \u5b9a\u4e49\u6bcf\u4e00\u4e2a\u70b9 Q_{t-1, j} variational posterior q_\\phi(Q_{t-1} | I_{t-1}, I_t) = \\mathcal{N}(T[\\mu^T_{Q_{t-1},j}, 1]^T, R\\Sigma_{Q_{t-1, j}}R^T) . \u53ef\u4ee5\u7406\u89e3\u4e3a t-1 \u65f6\u523b\u751f\u6210\u7684\u70b9\u4e91\u5728 t \u65f6\u523b\u5750\u6807\u7cfb\u4e0b\u7684\u4e0d\u786e\u5b9a\u5ea6. \\begin{aligned} \\log &p\\left(I_{t} \\mid I_{t-1}\\right) \\\\ =& \\log \\int p_{\\theta}\\left(I_{t} \\mid I_{t-1}, Q_{t-1}\\right) p_{\\theta}\\left(Q_{t-1} \\mid I_{t-1}\\right) d Q_{t-1} \\\\ =& \\log \\int q_{\\phi}\\left(Q_{t-1} \\mid I_{t}, I_{t-1}\\right) \\\\ \\quad & \\cdot \\frac{p_{\\theta}\\left(I_{t} \\mid I_{t-1}, Q_{t-1}\\right) p_{\\theta}\\left(Q_{t-1} \\mid I_{t-1}\\right)}{q_{\\phi}\\left(Q_{t-1} \\mid I_{t}, I_{t-1}\\right)} d Q_{t-1} \\\\ \\geq & \\int q_{\\phi}\\left(Q_{t-1} \\mid I_{t}, I_{t-1}\\right) \\\\ \\quad & \\cdot \\log \\frac{p_{\\theta}\\left(I_{t} \\mid I_{t-1}, Q_{t-1}\\right) p_{\\theta}\\left(Q_{t-1} \\mid I_{t-1}\\right)}{q_{\\phi}\\left(Q_{t-1} \\mid I_{t}, I_{t-1}\\right)} d Q_{t-1} \\\\ =&-\\int q_{\\phi}\\left(Q_{t-1} \\mid I_{t}, I_{t-1}\\right) \\log q_{\\phi}\\left(Q_{t-1} \\mid I_{t}, I_{t-1}\\right) d Q_{t-1} \\\\ &+\\int q_{\\phi}\\left(Q_{t-1} \\mid I_{t}, I_{t-1}\\right) \\log p_{\\theta}\\left(Q_{t-1} \\mid I_{t-1}\\right) d Q_{t-1} \\\\ &+\\int q_{\\phi}\\left(Q_{t-1} \\mid I_{t}, I_{t-1}\\right) \\log p_{\\theta}\\left(I_{t-1}\\right) \\\\ = &- H[q_\\phi(Q_{t-1} | I_t, I_{t-1})]\\\\ &+\\mathbb{E}_{q_{\\phi}\\left(Q_{t-1} \\mid I_{t}, I_{t-1}\\right)}\\left[\\log p_{\\theta}\\left(Q_{t-1} \\mid I_{t-1}\\right)\\right] \\\\ &+\\mathbb{E}_{q_{\\phi}\\left(Q_{t-1} \\mid I_{t}, I_{t-1}\\right)}\\left[\\log p_{\\theta}\\left(I_{t-1} \\mid Q_{t-1}, I_{t-1}\\right)\\right] \\end{aligned} \u5176\u4e2d\uff1a H[q_\\phi(Q_{t-1} | I_t, I_{t-1})] \u4e3a\u9ad8\u65af\u5206\u5e03\u7684\u71b5\uff0c \u53ef\u4ee5\u8bc1\u660e\u9ad8\u65af\u5206\u5e03\u7684\u71b5\u4ec5\u4e0e\u5176\u65b9\u5dee\u6709\u5173 h(X) = \\ln(\\sigma\\sqrt{2\\pi}) p_\\theta(Q_{t-1,i}| I_{t-1} \u5b9a\u4e49\u4e3a p_{\\theta}\\left(Q_{t-1, i} \\mid I_{t-1}\\right)=\\prod_{j=1}^{N} \\mathcal{N}\\left(\\mu_{Q_{t-1, j}}, \\Sigma_{Q_{t-1, j}}\\right)^{\\pi_{t-1}^{(i, j)}} \u5176\u4e2d \\pi \u662f t, t-1 \u65f6\u523b\u70b9\u76840,1\u5bf9\u5e94\u503c\u3002\u4f5c\u8005\u8981\u6c42\u4e24\u5e27\u70b9\u4e91\u7684\u70b9\u4e00\u4e00\u5bf9\u5e94\uff0c \\begin{aligned} & \\frac{1}{S} \\sum_{s=1}^{S} \\log \\prod_{i=1}^{N} \\prod_{j=1}^{N} \\mathcal{N}\\left(\\hat{Q}_{t-1}^{(s)} \\mid \\mu_{Q_{t-1, j}}, \\Sigma_{Q_{t-1, j}}\\right)^{\\pi_{t-1}^{(i, j)}} \\\\ =&-\\frac{1}{2} \\sum_{j=1}^{N} \\log \\left|\\Sigma_{Q_{t-1, j}}\\right|-\\frac{1}{2 S} \\sum_{s=1}^{S} \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\pi_{t-1}^{(i, j)} \\Delta_{t-1}^{(i, j)}+C \\end{aligned} \u7f51\u7edc\u8f93\u51fa\u7ed3\u6784 \\Delta_{t-1}^{(i,j)} = (\\hat Q_{t-1}^{(s)} - \\mu Q_{t-1,j}) \\Sigma^{-1}_{Q_{t-1,j}}(\\hat Q_{t-1}^{(s)} - \\mu Q_{t-1,j})^T , \u662f\u5c06T\u65f6\u523b\u7684\u70b9\u4e91\u6295\u56deT-1\u65f6\u523b\u540e\u7684\u5339\u914d\u635f\u5931\u3002 \\pi\\Delta \u5173\u4e8e \\pi \u7684\u6700\u5c0f\u5316\u53ef\u4ee5\u7528\u4e00\u4e2a Optimal Transport \u6765\u6c42\u89e3\u8fd9\u4e2a\u4e00\u4e00\u5bf9\u5e94\u95ee\u9898. \u7b2c\u4e09\u9879\u5219\u53ef\u4ee5\u7406\u89e3\u4e3a\u91cd\u5efa: \\frac{1}{S} \\sum_{s=1}^{S} \\log \\mathcal{N}\\left(\\hat{I}_{t}^{(s)}, \\Sigma_{I_{t}}\\right)=-\\frac{\\lambda_{\\mathrm{img}}}{S} \\sum_{s=1}^{S}\\left\\|I_{t}-\\hat{I}_{t}^{(s)}\\right\\|_{2}^{2}+C","title":"Variational Monocular Depth Estimation for Reliability Prediction"},{"location":"other_categories/depth_completion/variational_monodepth/#variational-monocular-depth-estimation-for-reliability-prediction","text":"\u8fd9\u7bc7paper\u4ee5\u7f51\u7edc\u8f93\u51fa\u6bcf\u4e2a\u70b9\u7684\u6df1\u5ea6\u7684\u6982\u7387\u503c\u4e3a\u72ec\u7acb\u7684\u9ad8\u65af\u5206\u5e03\u4e3a\u57fa\u7840\uff0c\u6b63\u9762\u5730\u4ece\u6982\u7387\u63a8\u5bfc\u4e86\u5355\u76ee\u6df1\u5ea6\u7684variational inference \u65b9\u6cd5.","title":"Variational Monocular Depth Estimation for Reliability Prediction"},{"location":"other_categories/depth_completion/variational_monodepth/#from-probabilistic-to-loss-function","text":"\u8bbe\u56fe\u7247\u5e8f\u5217 \\mathcal{I} = \\{I_0, ..., I_{N_t}\\} . \u5047\u8bbe\u6df1\u5ea6\u56fe\u662f\u7531\u70b9\u4e91 Q \u8fd9\u4e00\u9690\u53d8\u91cf\u51b3\u5b9a\u7684. \u4ece\u56fe\u7247\u4e2d\u4f30\u8ba1\u51fa\u70b9\u4e91\u7684\u6982\u7387\u8bb0\u4e3a p_{\\theta^*}(Q_{t-1} | I_{t-1}) . \u4e0b\u4e00\u65f6\u523b\u7684\u56fe\u7247\u4e5f\u662f\u6709\u76f8\u540c\u7684\u70b9\u4e91\u751f\u6210\u7684 p_{\\theta^*}(I_t | I_{t-1}, Q_{t-1}) \\theta^* = \\underset{\\theta}{\\text{argmax}} \\log p_\\theta(\\mathcal{I}) \u5b9a\u4e49\u6bcf\u4e00\u4e2a\u70b9 Q_{t-1, j} variational posterior q_\\phi(Q_{t-1} | I_{t-1}, I_t) = \\mathcal{N}(T[\\mu^T_{Q_{t-1},j}, 1]^T, R\\Sigma_{Q_{t-1, j}}R^T) . \u53ef\u4ee5\u7406\u89e3\u4e3a t-1 \u65f6\u523b\u751f\u6210\u7684\u70b9\u4e91\u5728 t \u65f6\u523b\u5750\u6807\u7cfb\u4e0b\u7684\u4e0d\u786e\u5b9a\u5ea6. \\begin{aligned} \\log &p\\left(I_{t} \\mid I_{t-1}\\right) \\\\ =& \\log \\int p_{\\theta}\\left(I_{t} \\mid I_{t-1}, Q_{t-1}\\right) p_{\\theta}\\left(Q_{t-1} \\mid I_{t-1}\\right) d Q_{t-1} \\\\ =& \\log \\int q_{\\phi}\\left(Q_{t-1} \\mid I_{t}, I_{t-1}\\right) \\\\ \\quad & \\cdot \\frac{p_{\\theta}\\left(I_{t} \\mid I_{t-1}, Q_{t-1}\\right) p_{\\theta}\\left(Q_{t-1} \\mid I_{t-1}\\right)}{q_{\\phi}\\left(Q_{t-1} \\mid I_{t}, I_{t-1}\\right)} d Q_{t-1} \\\\ \\geq & \\int q_{\\phi}\\left(Q_{t-1} \\mid I_{t}, I_{t-1}\\right) \\\\ \\quad & \\cdot \\log \\frac{p_{\\theta}\\left(I_{t} \\mid I_{t-1}, Q_{t-1}\\right) p_{\\theta}\\left(Q_{t-1} \\mid I_{t-1}\\right)}{q_{\\phi}\\left(Q_{t-1} \\mid I_{t}, I_{t-1}\\right)} d Q_{t-1} \\\\ =&-\\int q_{\\phi}\\left(Q_{t-1} \\mid I_{t}, I_{t-1}\\right) \\log q_{\\phi}\\left(Q_{t-1} \\mid I_{t}, I_{t-1}\\right) d Q_{t-1} \\\\ &+\\int q_{\\phi}\\left(Q_{t-1} \\mid I_{t}, I_{t-1}\\right) \\log p_{\\theta}\\left(Q_{t-1} \\mid I_{t-1}\\right) d Q_{t-1} \\\\ &+\\int q_{\\phi}\\left(Q_{t-1} \\mid I_{t}, I_{t-1}\\right) \\log p_{\\theta}\\left(I_{t-1}\\right) \\\\ = &- H[q_\\phi(Q_{t-1} | I_t, I_{t-1})]\\\\ &+\\mathbb{E}_{q_{\\phi}\\left(Q_{t-1} \\mid I_{t}, I_{t-1}\\right)}\\left[\\log p_{\\theta}\\left(Q_{t-1} \\mid I_{t-1}\\right)\\right] \\\\ &+\\mathbb{E}_{q_{\\phi}\\left(Q_{t-1} \\mid I_{t}, I_{t-1}\\right)}\\left[\\log p_{\\theta}\\left(I_{t-1} \\mid Q_{t-1}, I_{t-1}\\right)\\right] \\end{aligned} \u5176\u4e2d\uff1a H[q_\\phi(Q_{t-1} | I_t, I_{t-1})] \u4e3a\u9ad8\u65af\u5206\u5e03\u7684\u71b5\uff0c \u53ef\u4ee5\u8bc1\u660e\u9ad8\u65af\u5206\u5e03\u7684\u71b5\u4ec5\u4e0e\u5176\u65b9\u5dee\u6709\u5173 h(X) = \\ln(\\sigma\\sqrt{2\\pi}) p_\\theta(Q_{t-1,i}| I_{t-1} \u5b9a\u4e49\u4e3a p_{\\theta}\\left(Q_{t-1, i} \\mid I_{t-1}\\right)=\\prod_{j=1}^{N} \\mathcal{N}\\left(\\mu_{Q_{t-1, j}}, \\Sigma_{Q_{t-1, j}}\\right)^{\\pi_{t-1}^{(i, j)}} \u5176\u4e2d \\pi \u662f t, t-1 \u65f6\u523b\u70b9\u76840,1\u5bf9\u5e94\u503c\u3002\u4f5c\u8005\u8981\u6c42\u4e24\u5e27\u70b9\u4e91\u7684\u70b9\u4e00\u4e00\u5bf9\u5e94\uff0c \\begin{aligned} & \\frac{1}{S} \\sum_{s=1}^{S} \\log \\prod_{i=1}^{N} \\prod_{j=1}^{N} \\mathcal{N}\\left(\\hat{Q}_{t-1}^{(s)} \\mid \\mu_{Q_{t-1, j}}, \\Sigma_{Q_{t-1, j}}\\right)^{\\pi_{t-1}^{(i, j)}} \\\\ =&-\\frac{1}{2} \\sum_{j=1}^{N} \\log \\left|\\Sigma_{Q_{t-1, j}}\\right|-\\frac{1}{2 S} \\sum_{s=1}^{S} \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\pi_{t-1}^{(i, j)} \\Delta_{t-1}^{(i, j)}+C \\end{aligned}","title":"From Probabilistic to Loss function"},{"location":"other_categories/depth_completion/variational_monodepth/#_1","text":"\\Delta_{t-1}^{(i,j)} = (\\hat Q_{t-1}^{(s)} - \\mu Q_{t-1,j}) \\Sigma^{-1}_{Q_{t-1,j}}(\\hat Q_{t-1}^{(s)} - \\mu Q_{t-1,j})^T , \u662f\u5c06T\u65f6\u523b\u7684\u70b9\u4e91\u6295\u56deT-1\u65f6\u523b\u540e\u7684\u5339\u914d\u635f\u5931\u3002 \\pi\\Delta \u5173\u4e8e \\pi \u7684\u6700\u5c0f\u5316\u53ef\u4ee5\u7528\u4e00\u4e2a Optimal Transport \u6765\u6c42\u89e3\u8fd9\u4e2a\u4e00\u4e00\u5bf9\u5e94\u95ee\u9898. \u7b2c\u4e09\u9879\u5219\u53ef\u4ee5\u7406\u89e3\u4e3a\u91cd\u5efa: \\frac{1}{S} \\sum_{s=1}^{S} \\log \\mathcal{N}\\left(\\hat{I}_{t}^{(s)}, \\Sigma_{I_{t}}\\right)=-\\frac{\\lambda_{\\mathrm{img}}}{S} \\sum_{s=1}^{S}\\left\\|I_{t}-\\hat{I}_{t}^{(s)}\\right\\|_{2}^{2}+C","title":"\u7f51\u7edc\u8f93\u51fa\u7ed3\u6784"},{"location":"other_categories/object_detection_2D/AP_loss/","text":"AP-Loss for Accurate One-Stage Object Detection \u8fd9\u7bc7TPAMI paper \u5c06 average precision \u8fd9\u4e2adetection\u7684\u8bc4\u5224\u6807\u51c6\u53d8\u4e3a\u53ef\u4ee5\u76f4\u63a5\u4f18\u5316\u7684loss(gradient \u4ea7\u751f\u7684\u4f9d\u636e)\u3002 \u672c\u6587\u63d0\u51fa\u8fd9\u4e2aAP loss,\u4f5c\u8005\u58f0\u79f0\u80fd\u66f4\u597d\u7684\u5904\u7406\u6b63\u8d1f\u6837\u672c\u6bd4\u4f8b\u5931\u8c03\u4ee5\u53ca\u7c7b\u522b\u4e4b\u95f4\u6837\u672c\u5931\u8c03\u7684\u95ee\u9898\u3002\u4f46\u662f average precision\u7684\u8ba1\u7b97\u662f\u65e0\u6cd5\u6c42\u5bfc\u7684\uff0c\u672c\u6587\u7ed3\u5408 perceptron learning algorithm \u7684\u601d\u60f3\u7ed9\u51fa\u68af\u5ea6\u7684\u8ba1\u7b97\u65b9\u5f0f\u3002\u6700\u7ec8\u8fd9\u4e2aAP loss\u53ef\u4ee5\u50cf\u662f\u4e00\u822c\u7684loss\u4e00\u6837\u5d4c\u5165\u5728\u76ee\u6807\u68c0\u6d4b\u6846\u67b6\u4e2d\u3002 Background of Perceptron Learning Algorithm blog wiki \u57fa\u672c\u66f4\u65b0\u7b97\u6cd5 Perceptron learning algorithm \u662f\u7ed9\u5355\u795e\u7ecf\u5143\u5206\u7c7b\u5668\u8bad\u7ec3\u7684\u7b97\u6cd5\uff0c\u662f\u4e00\u4e2agradient-free\u7684\u7b97\u6cd5\uff0c\u5176\u601d\u8def\u662ferror-driven update. \u4e5f\u5373\u662f\u8bf4\u66f4\u65b0\u6b65\u957f\u76f4\u63a5\u6765\u6e90\u4e8e\u76ee\u6807\u8f93\u51fa\u4ee5\u53ca\u5f53\u524d\u8f93\u51fa\u7684\u5dee\u503c. \u521d\u59cb\u5316\u6743\u91cd \u8ba1\u7b97\u8f93\u51fa\u6743\u91cd y = sign(w^T x) \u66f4\u65b0\u6743\u91cd w(t+1) = w(t) + \\alpha (y_{des} - y_t)x Convergence \u5355\u5c42\u795e\u7ecf\u5143\u662f\u4e00\u4e2a\u7ebf\u6027\u5206\u7c7b\u5668;\u5982\u679c\u6570\u636e\u7ebf\u6027\u53ef\u5206\uff0c\u6709\u8bc1\u660e\u8fd9\u4e2a\u7b97\u6cd5\u4f1a\u6536\u655b\uff0c\u4e14step\u6570\u91cf\u6709\u4e0a\u9650\uff0c\u4f46\u662f\u4e0d\u80fd\u4fdd\u8bc1\u89e3\u7684\u8d28\u91cf\uff1b\u5982\u679c\u6570\u636e\u4e0d\u53ef\u5206\uff0c\u7b97\u6cd5\u4e0d\u4f1a\u6536\u655b\u5230\u4e00\u4e2a\u63a5\u8fd1\u7684\u89e3\uff0c\u800c\u662f\u4f1a\u574f\u6389\u3002 AP Loss Computation code AP Loss \\mathcal{L}_{AP} \u5b9a\u4e49\u4e3a 1 - AP . \u800c\u63d2\u503cAP\u7684\u8ba1\u7b97\u65b9\u5f0f\u53ef\u89c1 evaluation metrics . \u8fd9\u91cc\u91c7\u7528\u4e86 rank-based \u7684\u63cf\u8ff0\u65b9\u5f0f\u8fdb\u884c\u5165\u624b \\begin{array}{l} \\mathcal{L}_{A P}=1-\\mathrm{AP}=1-\\frac{1}{|\\mathcal{P}|} \\sum_{i \\in \\mathcal{P}} \\frac{\\operatorname{rank}^{+}(i)}{\\operatorname{rank}(i)} \\\\ =1-\\frac{1}{|\\mathcal{P}|} \\sum_{i \\in \\mathcal{P}} \\frac{1+\\sum_{j \\in \\mathcal{P}, j \\neq i} H\\left(x_{i j}\\right)}{1+\\sum_{j \\in \\mathcal{P}, j \\neq i} H\\left(x_{i j}\\right)+\\sum_{j \\in \\mathcal{N}} H\\left(x_{i j}\\right)} \\\\ =\\frac{1}{|\\mathcal{P}|} \\sum_{i \\in \\mathcal{P}} \\sum_{j \\in \\mathcal{N}} L_{i j}=\\frac{1}{|\\mathcal{P}|} \\sum_{i, j} L_{i j} \\cdot y_{i j}=\\frac{1}{|\\mathcal{P}|}\\langle\\boldsymbol{L}(\\boldsymbol{x}), \\boldsymbol{y}\\rangle \\end{array} \u51e0\u4e2a\u7b26\u53f7: rank(i), rank^+(i) \u6307\u8be5\u6837\u672c\u7684score\u5728\u6240\u6709\u6b63\u786e\u5339\u914d\u7684\u6837\u672c\u4ee5\u53ca\u6240\u6709\u6b63\u6837\u672c(\u7f51\u7edc\u8f93\u51fa\u4e3a\u6b63\u5373\u53ef)\u4e2d\u7684score \u6392\u540d \\mathcal{P}, \\mathcal{N} \u5206\u522b\u6307annotation\u4e2d\u7684\u6b63\u6837\u672c\u4e0e\u8d1f\u6837\u672c\u7684\u96c6\u5408\u3002 H(x_{ij}) \u5728\u8fd9\u4e2a\u9636\u6bb5\u662f\u4e00\u4e2a\u9636\u8dc3\u51fd\u6570\uff0c x_{ij} = s_j - s_i \u5d4c\u5957\u7684\u7ed3\u679c\u65e2\u662f\u5982\u679c j \u6bd4 i score\u66f4\u9ad8\uff0c\u5219\u4e3a1\u5426\u5219\u4e3a0. \u51e0\u4e2a\u89e3\u91ca\uff1a\u8bba\u6587\u4e2d\u662f\u5148\u4ecb\u7ecd\u4e86\u51e0\u4e2a\u5316\u7b80\u7528\u7684\u4e2d\u95f4\u51fd\u6570 L, H \u7b49\uff0c\u7136\u540e\u518d\u5165\u624b\u8ba1\u7b97 AP \uff0c\u4f46\u662f\u5bf9\u4e8e\u7406\u89e3AP\u524d\u540e\u5411\u8fd0\u7b97\u7684\u903b\u8f91\u6765\u8bf4\uff0c\u4e2d\u95f4\u91cf\u5e76\u4e0d\u5fc5\u8981\uff0c\u5c31\u662f\u4e00\u4e2a\u5316\u7b80\u7684\u65b9\u5f0f\u800c\u5df2 \u7b97\u6cd5: \u5176\u4e2d\u8f93\u5165 s_i, t_i \u5f62\u72b6\u4e3a [B, N, C] , t_i \\in \\{-1, 0, 1\\} , \u5176\u4e2d-1\u6307\u7701\u7565\u3002 \u5faa\u73af\u4e2d\u5219\u662f\u53ef\u4ee5\u7406\u89e3\u4e3athreshold\u9010\u6b65\u63d0\u5347,recall\u9010\u6e10\u4e0b\u964d\u7684\u8fc7\u7a0b\u3002\u6ce8\u610f\u8ba1\u7b97precision\u65f6\uff0c\u672c\u6587\u91c7\u7528\u7684\u662f\u5206\u6bb5step function,\u53ef\u4ee5\u7406\u89e3\u4e3asoft positive & negative label. f(x)=\\left\\{\\begin{array}{cc} 0, & x<-\\delta \\\\ \\frac{x}{2 \\delta}+0.5, & -\\delta \\leq x \\leq \\delta \\\\ 1, & \\delta<x \\end{array}\\right. \u4ee3\u7801\u5b9e\u73b0\u4e0a\u8fd8\u6709paper\u91cc\u9762\u51e0\u4e2a\u6709\u7684\u70b9\uff1a 1. \u68af\u5ea6\u4e0eforward\u662f\u4e00\u8d77\u8ba1\u7b97\u7684\uff0c\u56e0\u800c\u4f5c\u8005\u662f\u5199\u6210\u4e86\u4e00\u4e2afunctional,\u5e76\u91cd\u8f7d\u5176backward\u6765\u5b9e\u73b0\u7684\u3002 2. mini-batch training\uff0c\u4f5c\u8005\u62c5\u5fc3\u4e0d\u540cimage\u4e4b\u95f4\u4f1a\u6709score-shift\u4f7f\u5f97AP\u7684\u8ba1\u7b97\u4e0d\u51c6\u786e\u3002(\u6781\u7aef\u7684\u60c5\u51b5\u4e0b\u4e00\u5f20\u56fe\u7684\u6700\u4f4escore\u9ad8\u4e8e\u53e6\u4e00\u5f20\u56fe\u7684\u6700\u9ad8score)\uff0c\u56e0\u800cbatch ranking\u662f\u5fc5\u987b\u7684 3. \u5bf9\u6700\u5c0f\u7684\u8d1f\u6837\u672c\u70b9\u76f4\u63a5\u820d\u5f03\uff0c\u5982\u679c\u4e00\u4e9b\u8d1f\u6837\u672c\u70b9score\u5f88\u4f4e\u4e86\u5c31\u4e0d\u4f1a\u53c2\u4e0e\u68af\u5ea6\u7684\u8ba1\u7b97,\u540e\u9762\u5927\u5e45\u5ea6\u52a0\u901f\u8ba1\u7b97\u3002 def AP_loss(logits:torch.Tensor,targets:torch.Tensor): \"\"\" logits: Conv output, shape [B, N, Classes] (flatten from multi-scale) targets: Int Tensor, shape [B, N, Classes] (-1: ignore, 0: negative, 1:positive) \"\"\" delta=1.0 # delta paramater for piecewise Step function -> soft step function grad=torch.zeros(logits.shape).cuda() metric=torch.zeros(1).cuda() if torch.max(targets)<=0: return grad, metric labels_p=(targets==1) fg_logits=logits[labels_p] threshold_logit=torch.min(fg_logits)-delta ######## Ignore those negative j that satisfy (L_{ij}=0 for all positive i), to accelerate the AP-loss computation. valid_labels_n=((targets==0)&(logits>=threshold_logit)) valid_bg_logits=logits[valid_labels_n] valid_bg_grad=torch.zeros(len(valid_bg_logits)).cuda() ######## fg_num=len(fg_logits) prec=torch.zeros(fg_num).cuda() order=torch.argsort(fg_logits) max_prec=0 for ii in order: tmp1=fg_logits-fg_logits[ii] tmp1=torch.clamp(tmp1/(2*delta)+0.5,min=0,max=1) tmp2=valid_bg_logits-fg_logits[ii] tmp2=torch.clamp(tmp2/(2*delta)+0.5,min=0,max=1) a=torch.sum(tmp1)+0.5 b=torch.sum(tmp2) tmp2/=(a+b) current_prec=a/(a+b) if (max_prec<=current_prec): max_prec=current_prec else: tmp2*=((1-max_prec)/(1-current_prec)) valid_bg_grad+=tmp2 prec[ii]=max_prec grad[valid_labels_n]=valid_bg_grad grad[labels_p]=-(1-prec) fg_num=max(fg_num,1) grad /= (fg_num) metric=torch.sum(prec,dim=0,keepdim=True)/fg_num return grad, 1-metric","title":"AP-Loss for Accurate One-Stage Object Detection"},{"location":"other_categories/object_detection_2D/AP_loss/#ap-loss-for-accurate-one-stage-object-detection","text":"\u8fd9\u7bc7TPAMI paper \u5c06 average precision \u8fd9\u4e2adetection\u7684\u8bc4\u5224\u6807\u51c6\u53d8\u4e3a\u53ef\u4ee5\u76f4\u63a5\u4f18\u5316\u7684loss(gradient \u4ea7\u751f\u7684\u4f9d\u636e)\u3002 \u672c\u6587\u63d0\u51fa\u8fd9\u4e2aAP loss,\u4f5c\u8005\u58f0\u79f0\u80fd\u66f4\u597d\u7684\u5904\u7406\u6b63\u8d1f\u6837\u672c\u6bd4\u4f8b\u5931\u8c03\u4ee5\u53ca\u7c7b\u522b\u4e4b\u95f4\u6837\u672c\u5931\u8c03\u7684\u95ee\u9898\u3002\u4f46\u662f average precision\u7684\u8ba1\u7b97\u662f\u65e0\u6cd5\u6c42\u5bfc\u7684\uff0c\u672c\u6587\u7ed3\u5408 perceptron learning algorithm \u7684\u601d\u60f3\u7ed9\u51fa\u68af\u5ea6\u7684\u8ba1\u7b97\u65b9\u5f0f\u3002\u6700\u7ec8\u8fd9\u4e2aAP loss\u53ef\u4ee5\u50cf\u662f\u4e00\u822c\u7684loss\u4e00\u6837\u5d4c\u5165\u5728\u76ee\u6807\u68c0\u6d4b\u6846\u67b6\u4e2d\u3002","title":"AP-Loss for Accurate One-Stage Object Detection"},{"location":"other_categories/object_detection_2D/AP_loss/#background-of-perceptron-learning-algorithm","text":"blog wiki","title":"Background of Perceptron Learning Algorithm"},{"location":"other_categories/object_detection_2D/AP_loss/#_1","text":"Perceptron learning algorithm \u662f\u7ed9\u5355\u795e\u7ecf\u5143\u5206\u7c7b\u5668\u8bad\u7ec3\u7684\u7b97\u6cd5\uff0c\u662f\u4e00\u4e2agradient-free\u7684\u7b97\u6cd5\uff0c\u5176\u601d\u8def\u662ferror-driven update. \u4e5f\u5373\u662f\u8bf4\u66f4\u65b0\u6b65\u957f\u76f4\u63a5\u6765\u6e90\u4e8e\u76ee\u6807\u8f93\u51fa\u4ee5\u53ca\u5f53\u524d\u8f93\u51fa\u7684\u5dee\u503c. \u521d\u59cb\u5316\u6743\u91cd \u8ba1\u7b97\u8f93\u51fa\u6743\u91cd y = sign(w^T x) \u66f4\u65b0\u6743\u91cd w(t+1) = w(t) + \\alpha (y_{des} - y_t)x","title":"\u57fa\u672c\u66f4\u65b0\u7b97\u6cd5"},{"location":"other_categories/object_detection_2D/AP_loss/#convergence","text":"\u5355\u5c42\u795e\u7ecf\u5143\u662f\u4e00\u4e2a\u7ebf\u6027\u5206\u7c7b\u5668;\u5982\u679c\u6570\u636e\u7ebf\u6027\u53ef\u5206\uff0c\u6709\u8bc1\u660e\u8fd9\u4e2a\u7b97\u6cd5\u4f1a\u6536\u655b\uff0c\u4e14step\u6570\u91cf\u6709\u4e0a\u9650\uff0c\u4f46\u662f\u4e0d\u80fd\u4fdd\u8bc1\u89e3\u7684\u8d28\u91cf\uff1b\u5982\u679c\u6570\u636e\u4e0d\u53ef\u5206\uff0c\u7b97\u6cd5\u4e0d\u4f1a\u6536\u655b\u5230\u4e00\u4e2a\u63a5\u8fd1\u7684\u89e3\uff0c\u800c\u662f\u4f1a\u574f\u6389\u3002","title":"Convergence"},{"location":"other_categories/object_detection_2D/AP_loss/#ap-loss-computation","text":"code AP Loss \\mathcal{L}_{AP} \u5b9a\u4e49\u4e3a 1 - AP . \u800c\u63d2\u503cAP\u7684\u8ba1\u7b97\u65b9\u5f0f\u53ef\u89c1 evaluation metrics . \u8fd9\u91cc\u91c7\u7528\u4e86 rank-based \u7684\u63cf\u8ff0\u65b9\u5f0f\u8fdb\u884c\u5165\u624b \\begin{array}{l} \\mathcal{L}_{A P}=1-\\mathrm{AP}=1-\\frac{1}{|\\mathcal{P}|} \\sum_{i \\in \\mathcal{P}} \\frac{\\operatorname{rank}^{+}(i)}{\\operatorname{rank}(i)} \\\\ =1-\\frac{1}{|\\mathcal{P}|} \\sum_{i \\in \\mathcal{P}} \\frac{1+\\sum_{j \\in \\mathcal{P}, j \\neq i} H\\left(x_{i j}\\right)}{1+\\sum_{j \\in \\mathcal{P}, j \\neq i} H\\left(x_{i j}\\right)+\\sum_{j \\in \\mathcal{N}} H\\left(x_{i j}\\right)} \\\\ =\\frac{1}{|\\mathcal{P}|} \\sum_{i \\in \\mathcal{P}} \\sum_{j \\in \\mathcal{N}} L_{i j}=\\frac{1}{|\\mathcal{P}|} \\sum_{i, j} L_{i j} \\cdot y_{i j}=\\frac{1}{|\\mathcal{P}|}\\langle\\boldsymbol{L}(\\boldsymbol{x}), \\boldsymbol{y}\\rangle \\end{array} \u51e0\u4e2a\u7b26\u53f7: rank(i), rank^+(i) \u6307\u8be5\u6837\u672c\u7684score\u5728\u6240\u6709\u6b63\u786e\u5339\u914d\u7684\u6837\u672c\u4ee5\u53ca\u6240\u6709\u6b63\u6837\u672c(\u7f51\u7edc\u8f93\u51fa\u4e3a\u6b63\u5373\u53ef)\u4e2d\u7684score \u6392\u540d \\mathcal{P}, \\mathcal{N} \u5206\u522b\u6307annotation\u4e2d\u7684\u6b63\u6837\u672c\u4e0e\u8d1f\u6837\u672c\u7684\u96c6\u5408\u3002 H(x_{ij}) \u5728\u8fd9\u4e2a\u9636\u6bb5\u662f\u4e00\u4e2a\u9636\u8dc3\u51fd\u6570\uff0c x_{ij} = s_j - s_i \u5d4c\u5957\u7684\u7ed3\u679c\u65e2\u662f\u5982\u679c j \u6bd4 i score\u66f4\u9ad8\uff0c\u5219\u4e3a1\u5426\u5219\u4e3a0. \u51e0\u4e2a\u89e3\u91ca\uff1a\u8bba\u6587\u4e2d\u662f\u5148\u4ecb\u7ecd\u4e86\u51e0\u4e2a\u5316\u7b80\u7528\u7684\u4e2d\u95f4\u51fd\u6570 L, H \u7b49\uff0c\u7136\u540e\u518d\u5165\u624b\u8ba1\u7b97 AP \uff0c\u4f46\u662f\u5bf9\u4e8e\u7406\u89e3AP\u524d\u540e\u5411\u8fd0\u7b97\u7684\u903b\u8f91\u6765\u8bf4\uff0c\u4e2d\u95f4\u91cf\u5e76\u4e0d\u5fc5\u8981\uff0c\u5c31\u662f\u4e00\u4e2a\u5316\u7b80\u7684\u65b9\u5f0f\u800c\u5df2 \u7b97\u6cd5: \u5176\u4e2d\u8f93\u5165 s_i, t_i \u5f62\u72b6\u4e3a [B, N, C] , t_i \\in \\{-1, 0, 1\\} , \u5176\u4e2d-1\u6307\u7701\u7565\u3002 \u5faa\u73af\u4e2d\u5219\u662f\u53ef\u4ee5\u7406\u89e3\u4e3athreshold\u9010\u6b65\u63d0\u5347,recall\u9010\u6e10\u4e0b\u964d\u7684\u8fc7\u7a0b\u3002\u6ce8\u610f\u8ba1\u7b97precision\u65f6\uff0c\u672c\u6587\u91c7\u7528\u7684\u662f\u5206\u6bb5step function,\u53ef\u4ee5\u7406\u89e3\u4e3asoft positive & negative label. f(x)=\\left\\{\\begin{array}{cc} 0, & x<-\\delta \\\\ \\frac{x}{2 \\delta}+0.5, & -\\delta \\leq x \\leq \\delta \\\\ 1, & \\delta<x \\end{array}\\right. \u4ee3\u7801\u5b9e\u73b0\u4e0a\u8fd8\u6709paper\u91cc\u9762\u51e0\u4e2a\u6709\u7684\u70b9\uff1a 1. \u68af\u5ea6\u4e0eforward\u662f\u4e00\u8d77\u8ba1\u7b97\u7684\uff0c\u56e0\u800c\u4f5c\u8005\u662f\u5199\u6210\u4e86\u4e00\u4e2afunctional,\u5e76\u91cd\u8f7d\u5176backward\u6765\u5b9e\u73b0\u7684\u3002 2. mini-batch training\uff0c\u4f5c\u8005\u62c5\u5fc3\u4e0d\u540cimage\u4e4b\u95f4\u4f1a\u6709score-shift\u4f7f\u5f97AP\u7684\u8ba1\u7b97\u4e0d\u51c6\u786e\u3002(\u6781\u7aef\u7684\u60c5\u51b5\u4e0b\u4e00\u5f20\u56fe\u7684\u6700\u4f4escore\u9ad8\u4e8e\u53e6\u4e00\u5f20\u56fe\u7684\u6700\u9ad8score)\uff0c\u56e0\u800cbatch ranking\u662f\u5fc5\u987b\u7684 3. \u5bf9\u6700\u5c0f\u7684\u8d1f\u6837\u672c\u70b9\u76f4\u63a5\u820d\u5f03\uff0c\u5982\u679c\u4e00\u4e9b\u8d1f\u6837\u672c\u70b9score\u5f88\u4f4e\u4e86\u5c31\u4e0d\u4f1a\u53c2\u4e0e\u68af\u5ea6\u7684\u8ba1\u7b97,\u540e\u9762\u5927\u5e45\u5ea6\u52a0\u901f\u8ba1\u7b97\u3002 def AP_loss(logits:torch.Tensor,targets:torch.Tensor): \"\"\" logits: Conv output, shape [B, N, Classes] (flatten from multi-scale) targets: Int Tensor, shape [B, N, Classes] (-1: ignore, 0: negative, 1:positive) \"\"\" delta=1.0 # delta paramater for piecewise Step function -> soft step function grad=torch.zeros(logits.shape).cuda() metric=torch.zeros(1).cuda() if torch.max(targets)<=0: return grad, metric labels_p=(targets==1) fg_logits=logits[labels_p] threshold_logit=torch.min(fg_logits)-delta ######## Ignore those negative j that satisfy (L_{ij}=0 for all positive i), to accelerate the AP-loss computation. valid_labels_n=((targets==0)&(logits>=threshold_logit)) valid_bg_logits=logits[valid_labels_n] valid_bg_grad=torch.zeros(len(valid_bg_logits)).cuda() ######## fg_num=len(fg_logits) prec=torch.zeros(fg_num).cuda() order=torch.argsort(fg_logits) max_prec=0 for ii in order: tmp1=fg_logits-fg_logits[ii] tmp1=torch.clamp(tmp1/(2*delta)+0.5,min=0,max=1) tmp2=valid_bg_logits-fg_logits[ii] tmp2=torch.clamp(tmp2/(2*delta)+0.5,min=0,max=1) a=torch.sum(tmp1)+0.5 b=torch.sum(tmp2) tmp2/=(a+b) current_prec=a/(a+b) if (max_prec<=current_prec): max_prec=current_prec else: tmp2*=((1-max_prec)/(1-current_prec)) valid_bg_grad+=tmp2 prec[ii]=max_prec grad[valid_labels_n]=valid_bg_grad grad[labels_p]=-(1-prec) fg_num=max(fg_num,1) grad /= (fg_num) metric=torch.sum(prec,dim=0,keepdim=True)/fg_num return grad, 1-metric","title":"AP Loss Computation"},{"location":"other_categories/object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/","text":"Associative Embedding: End-to-End Learning for Joint Detection and Grouping \u8fd9\u7bc7\u8bba\u6587\u5bf9\u6211\u6765\u8bf4\u662f CornerNet \u7684\u524d\u7f6e.\u4e24\u7bc7\u4e5f\u662f\u540c\u4e00\u4e2a\u4f5c\u8005\u3002 Associative Embedding \u7b80\u4ecb \u8fd9\u7bc7\u8bba\u6587\u4f7f\u75281D embedding\uff0c\u76ee\u7684\u662f\u8bad\u7ec3\u7f51\u7edc\u5bf9\u6765\u81ea\u540c\u4e00\u4e2agroup\u7684detection\u8f93\u51fa\u76f8\u4f3c\u7684tags\uff0cdifferent tags for detection\u3002 Stacked Hourglass Architecture hourglass\u7ed3\u6784\u53ef\u4ee5\u5728 \u8fd9\u7bc7\u8bba\u6587 \u770b\u5230\u4e5f\u5728 CornerNet \u8fd9\u7bc7\u8bba\u6587\u7528\u8fc7\uff0c\u5927\u5bb6\u7a0d\u6709\u4e0d\u540c\uff0c\u4e0d\u8fc7\u603b\u4f53\u601d\u8def\u4e00\u81f4\u3002 \u591a\u4eba\u80a2\u4f53\u4f30\u8ba1 \u672c\u6587\u4f7f\u7528\u524d\u9762\u7684backbone\u9884\u6d4b\u6bcf\u4e00\u4e2apixel\u7684detection score for each joint(\"left writst\", \"right shoulder\")\uff0c \u8981\u8fdb\u4e00\u6b65\u5b8c\u6210\u6574\u4e2akeypoint detections\u3002\u5982\u679c\u6709 m \u4e2a\u5173\u8282,\u5219\u8f93\u51fa 2m \u4e2achannel,\u5176\u4e2d m \u4e2a\u4f5c\u4e3adetection\u7684heatmap, m \u4e2a\u4f5c\u4e3agrouping\u7684 tags\u3002 \u6574\u4e2acost: \\begin{aligned} L_{g}(h, T)=& \\frac{1}{N} \\sum_{n} \\sum_{k}\\left(\\bar{h}_{n}-h_{k}\\left(x_{n k},\\right)\\right)^{2} \\\\ &+\\frac{1}{N^{2}} \\sum_{n} \\sum_{n^{\\prime}} \\exp \\left\\{-\\frac{1}{2 \\sigma^{2}}\\left(\\bar{h}_{n}-\\bar{h}_{n^{\\prime}}\\right)^{2}\\right\\} \\end{aligned} \u5176\u4e2d h(x) \u662fpixel x \u5bf9\u5e94\u7684tag value\u3002 T = {(x_{nk})} ,\u5176\u4e2d x_{nk} \u662f\u7b2c n \u4e2a\u4eba\u7684\u7b2c k \u4e2a\u8eab\u4f53\u5173\u8282\u7684pixel\u4f4d\u7f6e. \\bar{h}_{n}=\\frac{1}{K} \\sum_{k} h_{k}\\left(x_{n k}\\right) inference\u65f6\u7684\u6d41\u7a0b \u9996\u5148\u5bf9\u7b2c\u4e00\u4e2a\u5173\u8282\uff0c\u7ed9\u4e00\u4e2a\u9608\u503c\uff0c\u7136\u540e\u505anon-max suppression.\u5f97\u5230\u5404\u4e2a\u4eba\u7269\u7684\u521d\u59cb\u4f30\u8ba1\u3002 \u4e4b\u540e\u5bf9\u5176\u4ed6\u6bcf\u4e00\u4e2a\u5173\u8282\uff0c\u505a\u4e00\u4e2amaximum matching\uff0c\u540c\u65f6\u57fa\u4e8etag value\u4ee5\u53cadetection score\u3002","title":"Associative Embedding: End-to-End Learning for Joint Detection and Grouping"},{"location":"other_categories/object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/#associative-embedding-end-to-end-learning-for-joint-detection-and-grouping","text":"\u8fd9\u7bc7\u8bba\u6587\u5bf9\u6211\u6765\u8bf4\u662f CornerNet \u7684\u524d\u7f6e.\u4e24\u7bc7\u4e5f\u662f\u540c\u4e00\u4e2a\u4f5c\u8005\u3002","title":"Associative Embedding: End-to-End Learning for Joint Detection and Grouping"},{"location":"other_categories/object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/#associative-embedding","text":"\u8fd9\u7bc7\u8bba\u6587\u4f7f\u75281D embedding\uff0c\u76ee\u7684\u662f\u8bad\u7ec3\u7f51\u7edc\u5bf9\u6765\u81ea\u540c\u4e00\u4e2agroup\u7684detection\u8f93\u51fa\u76f8\u4f3c\u7684tags\uff0cdifferent tags for detection\u3002","title":"Associative Embedding \u7b80\u4ecb"},{"location":"other_categories/object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/#stacked-hourglass-architecture","text":"hourglass\u7ed3\u6784\u53ef\u4ee5\u5728 \u8fd9\u7bc7\u8bba\u6587 \u770b\u5230\u4e5f\u5728 CornerNet \u8fd9\u7bc7\u8bba\u6587\u7528\u8fc7\uff0c\u5927\u5bb6\u7a0d\u6709\u4e0d\u540c\uff0c\u4e0d\u8fc7\u603b\u4f53\u601d\u8def\u4e00\u81f4\u3002","title":"Stacked Hourglass Architecture"},{"location":"other_categories/object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/#_1","text":"\u672c\u6587\u4f7f\u7528\u524d\u9762\u7684backbone\u9884\u6d4b\u6bcf\u4e00\u4e2apixel\u7684detection score for each joint(\"left writst\", \"right shoulder\")\uff0c \u8981\u8fdb\u4e00\u6b65\u5b8c\u6210\u6574\u4e2akeypoint detections\u3002\u5982\u679c\u6709 m \u4e2a\u5173\u8282,\u5219\u8f93\u51fa 2m \u4e2achannel,\u5176\u4e2d m \u4e2a\u4f5c\u4e3adetection\u7684heatmap, m \u4e2a\u4f5c\u4e3agrouping\u7684 tags\u3002 \u6574\u4e2acost: \\begin{aligned} L_{g}(h, T)=& \\frac{1}{N} \\sum_{n} \\sum_{k}\\left(\\bar{h}_{n}-h_{k}\\left(x_{n k},\\right)\\right)^{2} \\\\ &+\\frac{1}{N^{2}} \\sum_{n} \\sum_{n^{\\prime}} \\exp \\left\\{-\\frac{1}{2 \\sigma^{2}}\\left(\\bar{h}_{n}-\\bar{h}_{n^{\\prime}}\\right)^{2}\\right\\} \\end{aligned} \u5176\u4e2d h(x) \u662fpixel x \u5bf9\u5e94\u7684tag value\u3002 T = {(x_{nk})} ,\u5176\u4e2d x_{nk} \u662f\u7b2c n \u4e2a\u4eba\u7684\u7b2c k \u4e2a\u8eab\u4f53\u5173\u8282\u7684pixel\u4f4d\u7f6e. \\bar{h}_{n}=\\frac{1}{K} \\sum_{k} h_{k}\\left(x_{n k}\\right) inference\u65f6\u7684\u6d41\u7a0b \u9996\u5148\u5bf9\u7b2c\u4e00\u4e2a\u5173\u8282\uff0c\u7ed9\u4e00\u4e2a\u9608\u503c\uff0c\u7136\u540e\u505anon-max suppression.\u5f97\u5230\u5404\u4e2a\u4eba\u7269\u7684\u521d\u59cb\u4f30\u8ba1\u3002 \u4e4b\u540e\u5bf9\u5176\u4ed6\u6bcf\u4e00\u4e2a\u5173\u8282\uff0c\u505a\u4e00\u4e2amaximum matching\uff0c\u540c\u65f6\u57fa\u4e8etag value\u4ee5\u53cadetection score\u3002","title":"\u591a\u4eba\u80a2\u4f53\u4f30\u8ba1"},{"location":"other_categories/object_detection_2D/BoFDetection/","text":"Bag of Freebies for Training Object Detection Neural Networks \u8fd9\u7bc7\u8bba\u6587\u7cfb\u7edf\u6027\u5730\u603b\u7ed3\u6216\u63d0\u51fa\u4e86\u4e00\u7cfb\u5217\u7684\u63d0\u5347\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u7684\u4e00\u4e9b\u8bad\u7ec3\u6280\u5de7\uff0c\u4e0d\u66f4\u6539\u7f51\u7edc\u3001\u66f4\u6539inference\u65f6\u95f4\u7684\u524d\u63d0\u4e0b\uff0c\u80fd\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u7ed3\u679c\u3002\u672c\u7efc\u8ff0\u5728review\u8fd9\u7bc7\u8bba\u6587\u7684\u540c\u65f6\u641c\u96c6\u4e86\u5404\u4e2a\u6a21\u5757\u5178\u578b\u7684pytorch\u5b9e\u73b0. Image Mixup \u5c06\u4e24\u5f20\u56fe\u53eacrop\u4e0d\u6539\u53d8aspect ratio\u5730\u878d\u5408\uff0c\u76ee\u6807\u5e8f\u5217\u5219concat\u5728\u4e00\u8d77 \u5176\u4e2d\u878d\u5408\u7684\u53c2\u6570 \\alpha, \\beta \u7531\u4e00\u4e2a \\Beta \u5206\u5e03\u8fdb\u884c\u62bd\u6837\u3002 \u5173\u4e8eMixup, \u5176\u524d\u7f6e\u8bba\u6587\u6709\u5bf9\u5e94\u7684\u5f88\u7b80\u6d01\u7684 \u4ee3\u7801\u5b9e\u73b0 Classification Head Label Smoothing \u8fd9\u4e2aidea\u6e90\u81ea\u4e8e Inception-V3 \u4e5f\u5c31\u662f\u5f31\u5316CrossEntropy\u4e2d\u7684target q_{i}=\\left\\{\\begin{array}{ll}{1-\\varepsilon} & {\\text { if } i=y} \\\\ {\\varepsilon /(K-1)} & {\\text { otherwise }}\\end{array}\\right. \u4e00\u4e2a\u5178\u578b\u7684pytorch\u5b9e\u73b0\u5728\u8fd9\u91cc \u6570\u636e\u589e\u5f3a \u6570\u636e\u589e\u5f3a\u4e3b\u8981\u5305\u62ec\u4e24\u79cd \u968f\u673a\u51e0\u4f55\u53d8\u6362\uff0c\u5305\u62ec\u968f\u673a\u88c1\u5207\uff0c\u968f\u673a\u6269\u5927\uff0c\u6c34\u5e73\u7ffb\u8f6c\u4ee5\u53ca\u968f\u673aresize \u968f\u673a\u8272\u5f69\u6296\u52a8\uff0c\u5305\u62ec\u4eae\u5ea6\u3001\u8272\u76f8\u3001\u9971\u548c\u5ea6\u548c\u5bf9\u6bd4\u5ea6\u3002 \u4f5c\u8005\u53d1\u73b0one-stage\u68c0\u6d4b\u53d7\u968f\u673a\u51e0\u4f55\u53d8\u6362\u6bd4\u8f83\u654f\u611f\uff0c\u56e0\u800c\u9700\u8981\u4f7f\u7528\u968f\u673a\u51e0\u4f55\u53d8\u6362\u6765\u8fdb\u884c\u6570\u636e\u589e\u5f3a\uff0c\u800ctwo-stage\u68c0\u6d4b\u7531\u4e8eRoIAlign\u6216Pooling\u7684\u6027\u8d28\uff0c\u5bf9\u968f\u673a\u51e0\u4f55\u53d8\u6362\u7684\u654f\u611f\u5ea6\u4e0d\u5927\uff0c\u56e0\u800c\u51e0\u4f55\u53d8\u6362\u6570\u636e\u589e\u5f3a\u5bf9\u5176\u5f71\u54cd\u4e0d\u5927\u3002 \u5b66\u4e60\u7387\u8c03\u6574 \u7ecf\u8fc7\u5927\u91cf\u5b9e\u9a8c\u4f5c\u8005\u53d1\u73b0cosine\u53d8\u5316\u5f88\u597d\uff0c\u540c\u65f6\u8981\u6ce8\u610f\u7684\u662fwarmup learning rate\u5bf9YoLO\u7684\u8bad\u7ec3\u81f3\u5173\u91cd\u8981\u3002 stack-overflow\u4e0a\u9762\u6709\u5bf9\u8fd9\u4e2awarmup\u5728\u673a\u5668\u5b66\u4e60\u65b9\u9762\u7684\u4e00\u4e2a\u76f4\u89c9\u6027\u7684 \u89e3\u7b54 \u8bfb\u61c2\u4e0a\u8ff0\u89e3\u7b54\u4e4b\u540e\uff0c\u6211\u4eec\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528github\u4e0a\u5bf9Warm-up learning rate\u7684 pytorch\u590d\u73b0 Synchronized Batch Normalization \u7531\u4e8ebatch-size\u6bd4\u8f83\u5c0f\uff0c\u6211\u4eec\u9700\u8981synchronized batch-norm\u6765\u8de8GPU\u8bad\u7ec3.\u672c\u8d28\u4e0a\u6765\u8bf4\u5c31\u662f\u5f3a\u5236\u5728batchnorm\u7684\u65f6\u5019\u7b49\u5f85\u6240\u6709\u7684GPU\u4e00\u8d77\u8ba1\u7b97\uff0c\u5e76\u4e14\u5171\u7528\u540c\u4e00\u4e2a\u5f53\u524d\u7684\u5747\u503c\u4e0e\u65b9\u5dee\u3002 \u4e00\u4e2a\u5178\u578b\u7684pytorch\u5b9e\u73b0\u5728\u8fd9\u91cc \u968f\u673a\u5f62\u72b6\u8bad\u7ec3 \u5bf9\u4e8eone-stage\u7684\u7f51\u7edc\uff0c\u4f5c\u8005\u63d0\u51fa\u4fee\u6539\u8f93\u5165\u56fe\u7247\u7684\u5f62\u72b6\uff0c\u968f\u673a\u8f93\u5165\u6765\u8bad\u7ec3 \u4ee5\u4e0a\u6240\u6709tricks\u90fd\u5728\u57fa\u4e8eMXNET\u7684gluon-CV\u4e0a\u6709\u590d\u73b0\uff0c","title":"Bag of Freebies for Training Object Detection Neural Networks"},{"location":"other_categories/object_detection_2D/BoFDetection/#bag-of-freebies-for-training-object-detection-neural-networks","text":"\u8fd9\u7bc7\u8bba\u6587\u7cfb\u7edf\u6027\u5730\u603b\u7ed3\u6216\u63d0\u51fa\u4e86\u4e00\u7cfb\u5217\u7684\u63d0\u5347\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u7684\u4e00\u4e9b\u8bad\u7ec3\u6280\u5de7\uff0c\u4e0d\u66f4\u6539\u7f51\u7edc\u3001\u66f4\u6539inference\u65f6\u95f4\u7684\u524d\u63d0\u4e0b\uff0c\u80fd\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u7ed3\u679c\u3002\u672c\u7efc\u8ff0\u5728review\u8fd9\u7bc7\u8bba\u6587\u7684\u540c\u65f6\u641c\u96c6\u4e86\u5404\u4e2a\u6a21\u5757\u5178\u578b\u7684pytorch\u5b9e\u73b0.","title":"Bag of Freebies for Training Object Detection Neural Networks"},{"location":"other_categories/object_detection_2D/BoFDetection/#image-mixup","text":"\u5c06\u4e24\u5f20\u56fe\u53eacrop\u4e0d\u6539\u53d8aspect ratio\u5730\u878d\u5408\uff0c\u76ee\u6807\u5e8f\u5217\u5219concat\u5728\u4e00\u8d77 \u5176\u4e2d\u878d\u5408\u7684\u53c2\u6570 \\alpha, \\beta \u7531\u4e00\u4e2a \\Beta \u5206\u5e03\u8fdb\u884c\u62bd\u6837\u3002 \u5173\u4e8eMixup, \u5176\u524d\u7f6e\u8bba\u6587\u6709\u5bf9\u5e94\u7684\u5f88\u7b80\u6d01\u7684 \u4ee3\u7801\u5b9e\u73b0","title":"Image Mixup"},{"location":"other_categories/object_detection_2D/BoFDetection/#classification-head-label-smoothing","text":"\u8fd9\u4e2aidea\u6e90\u81ea\u4e8e Inception-V3 \u4e5f\u5c31\u662f\u5f31\u5316CrossEntropy\u4e2d\u7684target q_{i}=\\left\\{\\begin{array}{ll}{1-\\varepsilon} & {\\text { if } i=y} \\\\ {\\varepsilon /(K-1)} & {\\text { otherwise }}\\end{array}\\right. \u4e00\u4e2a\u5178\u578b\u7684pytorch\u5b9e\u73b0\u5728\u8fd9\u91cc","title":"Classification Head Label Smoothing"},{"location":"other_categories/object_detection_2D/BoFDetection/#_1","text":"\u6570\u636e\u589e\u5f3a\u4e3b\u8981\u5305\u62ec\u4e24\u79cd \u968f\u673a\u51e0\u4f55\u53d8\u6362\uff0c\u5305\u62ec\u968f\u673a\u88c1\u5207\uff0c\u968f\u673a\u6269\u5927\uff0c\u6c34\u5e73\u7ffb\u8f6c\u4ee5\u53ca\u968f\u673aresize \u968f\u673a\u8272\u5f69\u6296\u52a8\uff0c\u5305\u62ec\u4eae\u5ea6\u3001\u8272\u76f8\u3001\u9971\u548c\u5ea6\u548c\u5bf9\u6bd4\u5ea6\u3002 \u4f5c\u8005\u53d1\u73b0one-stage\u68c0\u6d4b\u53d7\u968f\u673a\u51e0\u4f55\u53d8\u6362\u6bd4\u8f83\u654f\u611f\uff0c\u56e0\u800c\u9700\u8981\u4f7f\u7528\u968f\u673a\u51e0\u4f55\u53d8\u6362\u6765\u8fdb\u884c\u6570\u636e\u589e\u5f3a\uff0c\u800ctwo-stage\u68c0\u6d4b\u7531\u4e8eRoIAlign\u6216Pooling\u7684\u6027\u8d28\uff0c\u5bf9\u968f\u673a\u51e0\u4f55\u53d8\u6362\u7684\u654f\u611f\u5ea6\u4e0d\u5927\uff0c\u56e0\u800c\u51e0\u4f55\u53d8\u6362\u6570\u636e\u589e\u5f3a\u5bf9\u5176\u5f71\u54cd\u4e0d\u5927\u3002","title":"\u6570\u636e\u589e\u5f3a"},{"location":"other_categories/object_detection_2D/BoFDetection/#_2","text":"\u7ecf\u8fc7\u5927\u91cf\u5b9e\u9a8c\u4f5c\u8005\u53d1\u73b0cosine\u53d8\u5316\u5f88\u597d\uff0c\u540c\u65f6\u8981\u6ce8\u610f\u7684\u662fwarmup learning rate\u5bf9YoLO\u7684\u8bad\u7ec3\u81f3\u5173\u91cd\u8981\u3002 stack-overflow\u4e0a\u9762\u6709\u5bf9\u8fd9\u4e2awarmup\u5728\u673a\u5668\u5b66\u4e60\u65b9\u9762\u7684\u4e00\u4e2a\u76f4\u89c9\u6027\u7684 \u89e3\u7b54 \u8bfb\u61c2\u4e0a\u8ff0\u89e3\u7b54\u4e4b\u540e\uff0c\u6211\u4eec\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528github\u4e0a\u5bf9Warm-up learning rate\u7684 pytorch\u590d\u73b0","title":"\u5b66\u4e60\u7387\u8c03\u6574"},{"location":"other_categories/object_detection_2D/BoFDetection/#synchronized-batch-normalization","text":"\u7531\u4e8ebatch-size\u6bd4\u8f83\u5c0f\uff0c\u6211\u4eec\u9700\u8981synchronized batch-norm\u6765\u8de8GPU\u8bad\u7ec3.\u672c\u8d28\u4e0a\u6765\u8bf4\u5c31\u662f\u5f3a\u5236\u5728batchnorm\u7684\u65f6\u5019\u7b49\u5f85\u6240\u6709\u7684GPU\u4e00\u8d77\u8ba1\u7b97\uff0c\u5e76\u4e14\u5171\u7528\u540c\u4e00\u4e2a\u5f53\u524d\u7684\u5747\u503c\u4e0e\u65b9\u5dee\u3002 \u4e00\u4e2a\u5178\u578b\u7684pytorch\u5b9e\u73b0\u5728\u8fd9\u91cc","title":"Synchronized Batch Normalization"},{"location":"other_categories/object_detection_2D/BoFDetection/#_3","text":"\u5bf9\u4e8eone-stage\u7684\u7f51\u7edc\uff0c\u4f5c\u8005\u63d0\u51fa\u4fee\u6539\u8f93\u5165\u56fe\u7247\u7684\u5f62\u72b6\uff0c\u968f\u673a\u8f93\u5165\u6765\u8bad\u7ec3 \u4ee5\u4e0a\u6240\u6709tricks\u90fd\u5728\u57fa\u4e8eMXNET\u7684gluon-CV\u4e0a\u6709\u590d\u73b0\uff0c","title":"\u968f\u673a\u5f62\u72b6\u8bad\u7ec3"},{"location":"other_categories/object_detection_2D/CPN/","text":"Corner Proposal Network for Anchor-free, Two-stage Object Detection \u672c\u6587\u7684\u4f5c\u8005\u662f\u57fa\u4e8e CornetNet \u5199\u51fa CenterNet \u7684Kaiwen Duan. Main Motivation \u4f5c\u8005\u7ed9\u51fa\u4e00\u4e9b\u4f8b\u5b50\u5e76\u4e14\u4ece\u76f4\u89c9\u4e0a\u6307\u51fa Anchor-based \u7b97\u6cd5\u5bf9\u4e8e\u5f62\u72b6\u590d\u6742\u7684\u7269\u4f53\u6216\u8005\u957f\u5bbd\u6bd4\u6bd4\u8f83\u6781\u7aef\u7684\u7269\u4f53\u53ec\u56de\u7387\u6bd4\u8f83\u4f4e\u3002 Anchor-free \u7b97\u6cd5\u5bb9\u6613\u4ea7\u751f\u5047\u9633\u6837\u672c\uff0c\u5c24\u5176\u662f\u5bb9\u6613\u9519\u8bef\u5730\u5c06\u4e0d\u76f8\u5173\u7684keypoints\u5206\u5728\u4e00\u8d77\u3002 \u4f5c\u8005\u60f3\u51fa\u4e86\u4e00\u4e2atwo-stage\u7684\u65b9\u6848\u878d\u5408\u4e24\u8005\u7684\u601d\u8def\u3002 Architecture \u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528CornerNet\u9884\u6d4b\u89d2\u70b9\uff0c\u5c06\u540c\u7c7b\u7684\u5de6\u4e0a\u70b9\u548c\u53f3\u4e0b\u70b9\u5339\u914d\uff0c\u4ea7\u751f\u5927\u91cf\u7684proposals\u3002 \u7b2c\u4e8c\u9636\u6bb5\u9996\u5148\u5c06\u6bcf\u4e00\u4e2a\u5339\u914d\u5f97\u5230\u7684detector, RoIAlign\u63d0\u53d6\u6846\u5185\u7684\u7279\u5f81\uff0c\u5148\u4f7f\u7528 Focal Loss \u76d1\u7763\u4e00\u4e2a\u8f7b\u91cf\u7684\u4e8c\u5206\u7c7b\u5206\u7c7b\u5668\u3002 \u7136\u540efilter\u6389 objectness\u6bd4\u8f83\u4f4e\u7684\u7269\u4f53\u4e4b\u540e\uff0c\u518d\u4f7f\u7528\u4e00\u4e2a\u66f4\u5927\u7684\u5206\u7c7b\u5668\uff0c\u91cd\u65b0\u786e\u5b9a\u8fd9\u4e2a\u6846\u7684\u5206\u7c7b\u3002 \u6ce8\u610f\u672c\u6587(or reviewers)\u89c9\u5f97\u8fd9\u7bc7paper\u7684\u601d\u8def\u548cDeNet\u5f88\u50cf\uff0c\u4f46\u662f\u672c\u6587\u5728\u6d88\u9664\u8d1f\u6837\u672c\u7684\u6548\u7387\u4e0a\u505a\u7684\u66f4\u597d\uff0c \u6587\u7ae0 \u4e0e \u4ee3\u7801 \u7686\u5f00\u653e\uff0c\u4e0d\u8fc7\u662f\u57fa\u4e8etheano\u7684\u3002","title":"Corner Proposal Network for Anchor-free, Two-stage Object Detection"},{"location":"other_categories/object_detection_2D/CPN/#corner-proposal-network-for-anchor-free-two-stage-object-detection","text":"\u672c\u6587\u7684\u4f5c\u8005\u662f\u57fa\u4e8e CornetNet \u5199\u51fa CenterNet \u7684Kaiwen Duan.","title":"Corner Proposal Network for Anchor-free, Two-stage Object Detection"},{"location":"other_categories/object_detection_2D/CPN/#main-motivation","text":"\u4f5c\u8005\u7ed9\u51fa\u4e00\u4e9b\u4f8b\u5b50\u5e76\u4e14\u4ece\u76f4\u89c9\u4e0a\u6307\u51fa Anchor-based \u7b97\u6cd5\u5bf9\u4e8e\u5f62\u72b6\u590d\u6742\u7684\u7269\u4f53\u6216\u8005\u957f\u5bbd\u6bd4\u6bd4\u8f83\u6781\u7aef\u7684\u7269\u4f53\u53ec\u56de\u7387\u6bd4\u8f83\u4f4e\u3002 Anchor-free \u7b97\u6cd5\u5bb9\u6613\u4ea7\u751f\u5047\u9633\u6837\u672c\uff0c\u5c24\u5176\u662f\u5bb9\u6613\u9519\u8bef\u5730\u5c06\u4e0d\u76f8\u5173\u7684keypoints\u5206\u5728\u4e00\u8d77\u3002 \u4f5c\u8005\u60f3\u51fa\u4e86\u4e00\u4e2atwo-stage\u7684\u65b9\u6848\u878d\u5408\u4e24\u8005\u7684\u601d\u8def\u3002","title":"Main Motivation"},{"location":"other_categories/object_detection_2D/CPN/#architecture","text":"\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528CornerNet\u9884\u6d4b\u89d2\u70b9\uff0c\u5c06\u540c\u7c7b\u7684\u5de6\u4e0a\u70b9\u548c\u53f3\u4e0b\u70b9\u5339\u914d\uff0c\u4ea7\u751f\u5927\u91cf\u7684proposals\u3002 \u7b2c\u4e8c\u9636\u6bb5\u9996\u5148\u5c06\u6bcf\u4e00\u4e2a\u5339\u914d\u5f97\u5230\u7684detector, RoIAlign\u63d0\u53d6\u6846\u5185\u7684\u7279\u5f81\uff0c\u5148\u4f7f\u7528 Focal Loss \u76d1\u7763\u4e00\u4e2a\u8f7b\u91cf\u7684\u4e8c\u5206\u7c7b\u5206\u7c7b\u5668\u3002 \u7136\u540efilter\u6389 objectness\u6bd4\u8f83\u4f4e\u7684\u7269\u4f53\u4e4b\u540e\uff0c\u518d\u4f7f\u7528\u4e00\u4e2a\u66f4\u5927\u7684\u5206\u7c7b\u5668\uff0c\u91cd\u65b0\u786e\u5b9a\u8fd9\u4e2a\u6846\u7684\u5206\u7c7b\u3002 \u6ce8\u610f\u672c\u6587(or reviewers)\u89c9\u5f97\u8fd9\u7bc7paper\u7684\u601d\u8def\u548cDeNet\u5f88\u50cf\uff0c\u4f46\u662f\u672c\u6587\u5728\u6d88\u9664\u8d1f\u6837\u672c\u7684\u6548\u7387\u4e0a\u505a\u7684\u66f4\u597d\uff0c \u6587\u7ae0 \u4e0e \u4ee3\u7801 \u7686\u5f00\u653e\uff0c\u4e0d\u8fc7\u662f\u57fa\u4e8etheano\u7684\u3002","title":"Architecture"},{"location":"other_categories/object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/","text":"CenterNet: Keypoint Triplets for Object Detection CenterNet\u4e5f\u5c31\u662f\u901a\u8fc7\u8f93\u51fa\u6bcf\u4e00\u4e2aobject\u4e3a\u5de6\u4e0a\u89d2\u3001\u53f3\u4e0a\u89d2\u4e0e\u4e2d\u5fc3\u70b9\u7684\u4e00\u4e2atriplet,\u8fd9\u4e2a\u601d\u8def\u6e90\u81ea\u4e8e CornerNet \u3002 \u603b\u4f53\u7ed3\u6784\u4e0epipeline \u4e0eCornerNet\u76f8\u4f3c\u7684\uff0cbackbone\u7684\u9009\u62e9\u4e5f\u662f stacked hourglass \u7b2c\u4e00\u5206\u652f\u7ecf\u8fc7\u65b0\u7684Cascade Corner Pooling(\u65b0\u7684\u64cd\u4f5c)\u5f97\u5230Corner Heatmaps\u5e76\u901a\u8fc7 associative embedding \u5f97\u5230\u521d\u59cb2D\u6846\u3002 \u7b2c\u4e8c\u5206\u652f\u7ecf\u8fc7Center Pooling\u5f97\u5230Center Heatmap\u3002 \u6700\u540e\u62fc\u5728\u4e00\u8d77\u5f97\u5230\u8f93\u51fa\u3002 \u878d\u5408\u68c0\u6d4bkeypoints \u7b97\u6cd5 1. \u9009\u62e9top-k\u4e2a\u4e2d\u5fc3keypoints 2. \u4f7f\u7528\u5bf9\u5e94offset\u6295\u5f71\u5230\u8f93\u5165\u56fe\u7247\u4e2d 3. \u5bf9\u6bcf\u4e00\u4e2acorner heatmap\u8f93\u51fa\u76842D box\uff0c\u67e5\u627e\u6709\u4e2d\u5fc3\u70b9\u662f\u5426\u5728\u4e2d\u5fc3\u533a\u57df 4. \u5982\u679c\u6709\u4e2d\u5fc3\u70b9\u5728\u4e2d\u5fc3\u533a\u57df\uff0c\u4fdd\u7559\u8fd9\u4e2a\u6846 \u5bf9\u4e2d\u5fc3\u533a\u57df\u7684\u5b9a\u4e49\uff1a \u6ee1\u8db3: \\left\\{\\begin{array}{l}{\\operatorname{ct} 1_{x}=\\frac{(n+1) \\operatorname{tl}_{x}+(n-1) \\operatorname{br}_{x}}{2 n}} \\\\ {\\operatorname{ct} l_{y}=\\frac{(n+1) \\operatorname{tl}_{y}+(n-1) \\operatorname{br}_{y}}{2 n}} \\\\ {\\operatorname{cbr}_{x}=\\frac{\\left.(n-1) \\operatorname{tl}\\right|_{x}+(n+1) \\operatorname{br}_{x}}{2 n}} \\\\ {\\operatorname{cbr}_{y}=\\frac{(n-1) \\operatorname{tl}_{y}+(n+1) \\operatorname{br}_{y}}{2 n}}\\end{array}\\right. \u672c\u6587\u4e3b\u8981\u6307\u4ee3 n \u4e3a3\u548c5,\u5206\u522b\u5bf9\u5e94scale\u5c0f\u4e8e\u548c\u5927\u4e8e150\u76842Dbox\u3002 Center Pooling \u4e0e Cascade Corner Pooling \u7b80\u5355\u6765\u8bf4\uff0cCenter Pooling\u7684\u7b97\u6cd5\u5c31\u662f\u53d6\u540c\u884c\u3001\u540c\u5217\u7684\u6700\u5927\u503c\u5e76\u7d2f\u52a0\u3002Cascade Corner Pooling\u7684\u7b97\u6cd5\u662f\uff0c\u53d6\u540c\u884c\u3001\u540c\u5217\u7684\u6700\u5927\u503c\uff0c\u518d\u5728\u5bf9\u5e94\u7684\u53d6\u6700\u503c\u7684\u70b9\u5bfb\u627e\u540c\u5217\u3001\u540c\u884c(\u9519\u5f00)\u7684\u6700\u5927\u503c\uff0c\u8f93\u51fa\u4e3a4\u4e2a\u70b9\u7684\u7d2f\u52a0\u3002 \u53ef\u89c6\u5316\u663e\u793a\u5982\u56fe \u90fd\u53ef\u4ee5\u7528 Corner Pooling\u5b9e\u73b0 \u3002\u5982\u56fe","title":"CenterNet: Keypoint Triplets for Object Detection"},{"location":"other_categories/object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/#centernet-keypoint-triplets-for-object-detection","text":"CenterNet\u4e5f\u5c31\u662f\u901a\u8fc7\u8f93\u51fa\u6bcf\u4e00\u4e2aobject\u4e3a\u5de6\u4e0a\u89d2\u3001\u53f3\u4e0a\u89d2\u4e0e\u4e2d\u5fc3\u70b9\u7684\u4e00\u4e2atriplet,\u8fd9\u4e2a\u601d\u8def\u6e90\u81ea\u4e8e CornerNet \u3002","title":"CenterNet: Keypoint Triplets for Object Detection"},{"location":"other_categories/object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/#pipeline","text":"\u4e0eCornerNet\u76f8\u4f3c\u7684\uff0cbackbone\u7684\u9009\u62e9\u4e5f\u662f stacked hourglass \u7b2c\u4e00\u5206\u652f\u7ecf\u8fc7\u65b0\u7684Cascade Corner Pooling(\u65b0\u7684\u64cd\u4f5c)\u5f97\u5230Corner Heatmaps\u5e76\u901a\u8fc7 associative embedding \u5f97\u5230\u521d\u59cb2D\u6846\u3002 \u7b2c\u4e8c\u5206\u652f\u7ecf\u8fc7Center Pooling\u5f97\u5230Center Heatmap\u3002 \u6700\u540e\u62fc\u5728\u4e00\u8d77\u5f97\u5230\u8f93\u51fa\u3002","title":"\u603b\u4f53\u7ed3\u6784\u4e0epipeline"},{"location":"other_categories/object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/#keypoints","text":"\u7b97\u6cd5 1. \u9009\u62e9top-k\u4e2a\u4e2d\u5fc3keypoints 2. \u4f7f\u7528\u5bf9\u5e94offset\u6295\u5f71\u5230\u8f93\u5165\u56fe\u7247\u4e2d 3. \u5bf9\u6bcf\u4e00\u4e2acorner heatmap\u8f93\u51fa\u76842D box\uff0c\u67e5\u627e\u6709\u4e2d\u5fc3\u70b9\u662f\u5426\u5728\u4e2d\u5fc3\u533a\u57df 4. \u5982\u679c\u6709\u4e2d\u5fc3\u70b9\u5728\u4e2d\u5fc3\u533a\u57df\uff0c\u4fdd\u7559\u8fd9\u4e2a\u6846 \u5bf9\u4e2d\u5fc3\u533a\u57df\u7684\u5b9a\u4e49\uff1a \u6ee1\u8db3: \\left\\{\\begin{array}{l}{\\operatorname{ct} 1_{x}=\\frac{(n+1) \\operatorname{tl}_{x}+(n-1) \\operatorname{br}_{x}}{2 n}} \\\\ {\\operatorname{ct} l_{y}=\\frac{(n+1) \\operatorname{tl}_{y}+(n-1) \\operatorname{br}_{y}}{2 n}} \\\\ {\\operatorname{cbr}_{x}=\\frac{\\left.(n-1) \\operatorname{tl}\\right|_{x}+(n+1) \\operatorname{br}_{x}}{2 n}} \\\\ {\\operatorname{cbr}_{y}=\\frac{(n-1) \\operatorname{tl}_{y}+(n+1) \\operatorname{br}_{y}}{2 n}}\\end{array}\\right. \u672c\u6587\u4e3b\u8981\u6307\u4ee3 n \u4e3a3\u548c5,\u5206\u522b\u5bf9\u5e94scale\u5c0f\u4e8e\u548c\u5927\u4e8e150\u76842Dbox\u3002","title":"\u878d\u5408\u68c0\u6d4bkeypoints"},{"location":"other_categories/object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/#center-pooling-cascade-corner-pooling","text":"\u7b80\u5355\u6765\u8bf4\uff0cCenter Pooling\u7684\u7b97\u6cd5\u5c31\u662f\u53d6\u540c\u884c\u3001\u540c\u5217\u7684\u6700\u5927\u503c\u5e76\u7d2f\u52a0\u3002Cascade Corner Pooling\u7684\u7b97\u6cd5\u662f\uff0c\u53d6\u540c\u884c\u3001\u540c\u5217\u7684\u6700\u5927\u503c\uff0c\u518d\u5728\u5bf9\u5e94\u7684\u53d6\u6700\u503c\u7684\u70b9\u5bfb\u627e\u540c\u5217\u3001\u540c\u884c(\u9519\u5f00)\u7684\u6700\u5927\u503c\uff0c\u8f93\u51fa\u4e3a4\u4e2a\u70b9\u7684\u7d2f\u52a0\u3002 \u53ef\u89c6\u5316\u663e\u793a\u5982\u56fe \u90fd\u53ef\u4ee5\u7528 Corner Pooling\u5b9e\u73b0 \u3002\u5982\u56fe","title":"Center Pooling \u4e0e Cascade Corner Pooling"},{"location":"other_categories/object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/","text":"CornerNet-Lite: Efficient Keypoint Based Object Detection \u8fd9\u7bc7\u8bba\u6587\u5728\u8fdb\u884cobject detection\u7684\u65f6\u5019\uff0c\u4f7f\u7528\u7684\u662f\u57fa\u4e8ekeypoint\u7684\u65b9\u6cd5\u800c\u4e0d\u662f\u57fa\u4e8eproposal\u7684\u65b9\u6cd5\u3002\u5efa\u8bae\u5148\u9605\u8bfb \u8fd9\u7bc7\u8bba\u6587 \u7f51\u7edc\u7ed3\u6784 \u4f7f\u7528attention map\u9884\u6d4b\u4e00\u7cfb\u5217\u4e0d\u540cscale\u7684keypoint\uff0c\u5728\u8fd9\u4e2akeypoint\u5468\u56f4crop\u51fa\u4e00\u5b9a\u91cf\u7684\u65b9\u5757\u56fe\uff0c\u8fdb\u884c\u5206\u7c7b\u3001\u6c47\u603b\uff0c\u7136\u540e\u8fdb\u884cNMS\u3002","title":"CornerNet-Lite: Efficient Keypoint Based Object Detection"},{"location":"other_categories/object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/#cornernet-lite-efficient-keypoint-based-object-detection","text":"\u8fd9\u7bc7\u8bba\u6587\u5728\u8fdb\u884cobject detection\u7684\u65f6\u5019\uff0c\u4f7f\u7528\u7684\u662f\u57fa\u4e8ekeypoint\u7684\u65b9\u6cd5\u800c\u4e0d\u662f\u57fa\u4e8eproposal\u7684\u65b9\u6cd5\u3002\u5efa\u8bae\u5148\u9605\u8bfb \u8fd9\u7bc7\u8bba\u6587","title":"CornerNet-Lite: Efficient Keypoint Based Object Detection"},{"location":"other_categories/object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/#_1","text":"\u4f7f\u7528attention map\u9884\u6d4b\u4e00\u7cfb\u5217\u4e0d\u540cscale\u7684keypoint\uff0c\u5728\u8fd9\u4e2akeypoint\u5468\u56f4crop\u51fa\u4e00\u5b9a\u91cf\u7684\u65b9\u5757\u56fe\uff0c\u8fdb\u884c\u5206\u7c7b\u3001\u6c47\u603b\uff0c\u7136\u540e\u8fdb\u884cNMS\u3002","title":"\u7f51\u7edc\u7ed3\u6784"},{"location":"other_categories/object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/","text":"CornerNet: Detecting Objects as Paired Keypoints \u8fd9\u7bc7\u6587\u7ae0\u662f \u8fd9\u7bc7\u6587\u7ae0 \u7684\u524d\u7f6e,\u5b9e\u9645\u4e0a\u4e5f\u786e\u5b9e\u6709\u66f4\u591a\u7684\u5185\u5bb9,\u5728\u5177\u4f53\u5b9e\u73b0\u4e0a\u6709\u533a\u522b\u3002 \u8fd9\u7bc7\u6587\u7ae0\u7ed9\u51fa\u7684\u601d\u8def\u662f\uff0c\u8ba9\u795e\u7ecf\u7f51\u7edc\u5206\u522b\u7ed9\u51fa\u67d0\u4e00\u7c7b\u522b\u5de6\u4e0a\u89d2\u4e0e\u53f3\u4e0b\u89d2\u7684heatmap\uff0c\u7136\u540e\u901a\u8fc7embedding vector\u7684\u76f8\u4f3c\u6027\u8fdb\u884c\u4e24\u4e2a\u89d2\u843d\u7684\u5339\u914d\u3002\u53e6\u5916\u4e3a\u4e86\u63d0\u9ad8\u6027\u80fd\uff0c\u8fd8\u7ed9\u51fa\u4e86corner pooling\u4ee5\u53ca\u5b83\u7684GPU\u5b9e\u73b0\u3002\u6574\u4e2a\u7f51\u7edc\u6d41\u7a0b\u57fa\u672c\u662fone-stage \u7ed3\u6784overview backbone\u7f51\u7edc\u4f7f\u7528\u7684\u662f hourglass \u4e4b\u540e\u8ddf\u968f\u7684\u662f\u4e24\u4e2a\u9884\u6d4b\u6a21\u5757\uff0c\u4e00\u4e2a\u9884\u6d4b\u8f93\u51fa\u662f\u5de6\u4e0a\u89d2\uff0c\u53e6\u4e00\u4e2a\u7ed9\u51fa\u7684\u662f\u53f3\u4e0b\u89d2\u3002\u8fd9\u4e24\u4e2a\u6a21\u5757\u6709\u5404\u81ea\u7684corner pooling\u3002 \u9884\u6d4b\u89d2\u70b9 \u7f51\u7edc\u6700\u7ec8\u8f93\u51fa\u7684\u662f\u4e24\u7ec4heatmap\uff0c\u4e00\u4e2a\u7ed9\u5de6\u4e0a\u89d2\u4e00\u4e2a\u7ed9\u53f3\u4e0b\u89d2\uff0c\u6bcf\u4e00\u7ec4\u70ed\u56fe\u6709 C \u4e2a\u7279\u5f81\uff0c\u4e0e\u7c7b\u522b\u6570\u4e00\u81f4(\u6bcf\u4e00\u7c7b\u4e00\u4e2achannel\u7684\u70ed\u56fe)\uff0cfeature map\u5f62\u72b6\u662f H\\times W .\u4e0d\u50cfyolo\u6216\u8005SSD\u4e00\u6837\u5e26\u6709background channel\u3002 \u5728\u8bad\u7ec3\u7684\u65f6\u5019\uff0c\u76f4\u89c9\u4e0e\u7ecf\u9a8c\u8868\u793a\u4e0d\u5e94\u8be5\u7b80\u5355\u5730\u60e9\u7f5a\u4e0d\u6b63\u786e\u7684\u89d2\u70b9\u4f4d\u7f6e\u3002\u8fd9\u91cc\u6839\u636e\u7269\u4f53\u7684\u4f53\u79ef\u7684\u8bbe\u5b9a\u4e0d\u540c\u7684radius cost. radius \u867d\u7136\u8868\u8fbe\u7684\u662f\u4e00\u4e2a\u5706\u7684\u6982\u5ff5\uff0c\u4f46\u662f\u5b9e\u9645\u5b9e\u73b0\u7684\u65f6\u5019\u662f\u4e00\u4e2a\u6b63\u65b9\u5f62\u7684\u534a\u8fb9\u957f, \u5176\u957f\u5ea6\u7684\u8bbe\u5b9a\u662f\u8bf4\u4e00\u5bf9ground truth\u70b9\u7684\u6b63\u65b9\u5f62\u533a\u57df\u5185\uff0c\u4efb\u9009\u4e24\u70b9\uff0c\u4e24\u70b9\u6784\u6210\u7684bbox\u4e0egt\u7684bbox\u7684IoU\u4e0b\u9650\u4e0d\u4f4e\u4e8e\u67d0\u4e00\u4e2a\u8bbe\u5b9a\u503c(\u672c\u6587\u53d60.3). \\forall p_i\\in{LeftTop}, \\forall p_j \\in {RightBottom} \\quad IoU(bbox_{p_i, p_j}, bbox_{gt}) \u539f\u7248code\u6709\u9519\u8bef\uff0c\u7ecf\u8fc7\u4e00\u4e9b\u4fee\u6b63\u540e\u6b63\u786e\u7684\u5b9e\u73b0\u5e94\u5f53\u662f\u3002 def compute_radius(det_size, min_overlap=0.7): \"\"\" Compute radius from ground truth bbox. original equation: (w - 2r) (h - 2r) / (wh) = min_overlap Take the solution with a smaller magnitude. \"\"\" height, width = det_size a2 = 4 b2 = 2 * (height + width) c2 = (1 - min_overlap) * width * height sq2 = np.sqrt(b2 ** 2 - 4 * a2 * c2) r2 = (b2 - sq2) / (2 * a2) return r2 \u8fd9\u4e2aidea\u70ed\u70b9\u56fe\u7684idea\u4ee5\u53caofficial code(\u5c3d\u7ba1\u4e0d\u4e00\u5b9a\u6b63\u786e)\u88ab\u591a\u4e2a\u540e\u7eed\u7684\u6587\u7ae0\u4f7f\u7528, \u5305\u62ec Object as point , RTM3D Unofficial Implementation \u6700\u7ec8\u8bbe\u8ba1\u51fa\u4e00\u4e2afocal loss,\u539f\u7248focal loss\u6e90\u81ea\u4e0e \u8fd9\u7bc7\u6587\u7ae0 ,\u5728 \u8fd9\u91cc \u6709\u7b80\u4ecb\u3002\u8fd9\u91cc\u7684\u5b9a\u4e49\u662f L_{det}=\\frac{-1}{N} \\sum_{c=1}^{C} \\sum_{i=1}^{H} \\sum_{j=1}^{W}\\left\\{\\begin{array}{c}{\\left(1-p_{c i j}\\right)^{\\alpha} \\log \\left(p_{c i j}\\right)} if (y_{cij} == 1) \\\\ {\\left(1-y_{c i j}\\right)^{\\beta}\\left(p_{c i j}\\right)^{\\alpha} \\log \\left(1-p_{c i j}\\right) \\text { otherwise }}\\end{array}\\right. \u5176\u4e2d p_{cij} \u4e3a (i,j) \u4f4d\u7f6e\u4e0a\u7684score\uff0c e^{-\\frac{x^2+y^2}{2\\sigma^2}} ,\u5176\u4e2d \\sigma \u662fradius\u662f 1/3 , N \u662f\u56fe\u7247\u4e2d\u7269\u4f53\u7684\u6570\u76ee\u3002 \\alpha, \\beta \u662f\u53ef\u8c03\u8282\u7684\u8d85\u53c2\u6570\u3002 \u7531\u4e8e\u5377\u79ef\u7f51\u7edc\u91cc\u9762\u6211\u4eec\u9700\u8981\u5bf9\u56fe\u7247\u4e0b\u91c7\u6837\uff0c (x,y) \\rightarrow (\\frac{x}{n}, \\frac{y}{n}) ,\u9700\u8981\u989d\u5916\u5b66\u4e60\u4e00\u4e2aoffsets\u53bb\u8865\u507f \\boldsymbol{o}_{k}=\\left(\\frac{x_{k}}{n}-\\left\\lfloor\\frac{x_{k}}{n}\\right\\rfloor, \\frac{y_{k}}{n}-\\left\\lfloor\\frac{y_{k}}{n}\\right\\rfloor\\right) \u8fd9\u4e2acost\u53ef\u4ee5\u7528 L_{o f f}=\\frac{1}{N} \\sum_{k=1}^{N} \\operatorname{SmoothL} 1 \\operatorname{Loss}\\left(\\boldsymbol{o}_{k}, \\hat{\\boldsymbol{o}}_{k}\\right) \u5c06\u89d2\u70b9\u805a\u56e2 \u56e0\u4e3a\u4e00\u5f20\u56fe\u5982\u679c\u6709\u591a\u4e2a\u7269\u4f53\uff0c\u4e00\u4e2a\u56fe\u4f1a\u6709\u4e0d\u6b62\u4e00\u5bf9\u7684\u5de6\u4e0a\u89d2\u70b9\u548c\u53f3\u4e0b\u70b9\u3002\u672c\u6587\u7684\u505a\u6cd5\u63d0\u5230\u4e86 \u8fd9\u7bc7\u8bba\u6587 , \u7b80\u4ecb \u7f51\u7edc\u7ed9\u6bcf\u4e00\u4e2a\u5de6\u4e0a\u89d2\u4e0e\u53f3\u4e0b\u89d2\u89d2\u9884\u6d4b\u4e00\u4e2aembedding vector\uff0c\u672c\u6587\u8fd9\u91cc\u6a21\u4eff\u524d\u4e00\u7bc7\u8bba\u6587\u7684\u505a\u6cd5\uff0c\u7ef4\u5ea6\u4ec5\u4e3a1\u3002\u5982\u679c\u4ee3\u8868\u7684\u662f\u540c\u4e00\u4e2abounding box\uff0c\u90a3\u4e48\u4e24\u8005\u8ddd\u79bb\u5c31\u4f1a\u6bd4\u8f83\u5c0f\u3002\u5206\u4e3a\u4e24\u4e2aloss\uff0c\u4e00\u4e2a\u662fpull\u4e00\u4e2apush\u3002 L_{p u l l}=\\frac{1}{N} \\sum_{k=1}^{N}\\left[\\left(e_{t_{k}}-e_{k}\\right)^{2}+\\left(e_{b_{k}}-e_{k}\\right)^{2}\\right] L_{p u s h}=\\frac{1}{N(N-1)} \\sum_{k=1}^{N} \\sum_{j=1 \\atop j \\neq k}^{N} \\max \\left(0, \\Delta-\\left|e_{k}-e_{j}\\right|\\right) e_{t_k} \u4f5c\u4e3a\u7b2c k \u4e2a\u7269\u4f53\uff0c\u5de6\u4e0a\u89d2\u7684embedding, e_{b_k} \u4f5c\u4e3a\u53f3\u4e0b\u89d2\u7684\u3002 e_k \u662f e_{t_k}, e_{b_k} \u7684\u5747\u503c\uff0c\u7136\u540e\u8bbe\u5b9a \\Delta=1 \uff0c\u4e0eoffset\u4e00\u6837\uff0c\u8fd9\u4e2aloss\u53ea\u6267\u884c\u5728gt\u7684\u89d2\u843d\u4f4d\u7f6e\u4e0a. Corner Pooling \u7528\u6765\u5141\u8bb8\u6bcf\u4e00\u4e2a\uff0c\u8ba1\u7b97\u65b9\u5f0f\u5982\u56fe t_{i j}=\\left\\{\\begin{array}{cl}{\\max \\left(f_{t_{i j}}, t_{(i+1) j}\\right)} & {\\text { if } i<H} \\\\ {f_{t_{H j}}} & {\\text { otherwise }}\\end{array}\\right. l_{i j}=\\left\\{\\begin{array}{cl}{\\max \\left(f_{l_{i j}}, l_{i(j+1)}\\right)} & {\\text { if } j<W} \\\\ {f_{l_{i W}}} & {\\text { otherwise }}\\end{array}\\right. \u7136\u540e\u5bf9 t_{ij}, l_{ij} \u76f8\u52a0\u5f97\u5230\u7ed3\u679c\u3002\u4e3a\u53f3\u4e0b\u89d2\u7684pooling layer\u53d6max\u7684\u65b9\u5411\u76f8\u53cd\u3002 \u5148\u5c06\u57fa\u7840\u7684resblock\u7684\u7b2c\u4e00\u4e2a 3\\times 3 \u5377\u79ef\u6539\u4e3acorner pooling.","title":"CornerNet: Detecting Objects as Paired Keypoints"},{"location":"other_categories/object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/#cornernet-detecting-objects-as-paired-keypoints","text":"\u8fd9\u7bc7\u6587\u7ae0\u662f \u8fd9\u7bc7\u6587\u7ae0 \u7684\u524d\u7f6e,\u5b9e\u9645\u4e0a\u4e5f\u786e\u5b9e\u6709\u66f4\u591a\u7684\u5185\u5bb9,\u5728\u5177\u4f53\u5b9e\u73b0\u4e0a\u6709\u533a\u522b\u3002 \u8fd9\u7bc7\u6587\u7ae0\u7ed9\u51fa\u7684\u601d\u8def\u662f\uff0c\u8ba9\u795e\u7ecf\u7f51\u7edc\u5206\u522b\u7ed9\u51fa\u67d0\u4e00\u7c7b\u522b\u5de6\u4e0a\u89d2\u4e0e\u53f3\u4e0b\u89d2\u7684heatmap\uff0c\u7136\u540e\u901a\u8fc7embedding vector\u7684\u76f8\u4f3c\u6027\u8fdb\u884c\u4e24\u4e2a\u89d2\u843d\u7684\u5339\u914d\u3002\u53e6\u5916\u4e3a\u4e86\u63d0\u9ad8\u6027\u80fd\uff0c\u8fd8\u7ed9\u51fa\u4e86corner pooling\u4ee5\u53ca\u5b83\u7684GPU\u5b9e\u73b0\u3002\u6574\u4e2a\u7f51\u7edc\u6d41\u7a0b\u57fa\u672c\u662fone-stage","title":"CornerNet: Detecting Objects as Paired Keypoints"},{"location":"other_categories/object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/#overview","text":"backbone\u7f51\u7edc\u4f7f\u7528\u7684\u662f hourglass \u4e4b\u540e\u8ddf\u968f\u7684\u662f\u4e24\u4e2a\u9884\u6d4b\u6a21\u5757\uff0c\u4e00\u4e2a\u9884\u6d4b\u8f93\u51fa\u662f\u5de6\u4e0a\u89d2\uff0c\u53e6\u4e00\u4e2a\u7ed9\u51fa\u7684\u662f\u53f3\u4e0b\u89d2\u3002\u8fd9\u4e24\u4e2a\u6a21\u5757\u6709\u5404\u81ea\u7684corner pooling\u3002","title":"\u7ed3\u6784overview"},{"location":"other_categories/object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/#_1","text":"\u7f51\u7edc\u6700\u7ec8\u8f93\u51fa\u7684\u662f\u4e24\u7ec4heatmap\uff0c\u4e00\u4e2a\u7ed9\u5de6\u4e0a\u89d2\u4e00\u4e2a\u7ed9\u53f3\u4e0b\u89d2\uff0c\u6bcf\u4e00\u7ec4\u70ed\u56fe\u6709 C \u4e2a\u7279\u5f81\uff0c\u4e0e\u7c7b\u522b\u6570\u4e00\u81f4(\u6bcf\u4e00\u7c7b\u4e00\u4e2achannel\u7684\u70ed\u56fe)\uff0cfeature map\u5f62\u72b6\u662f H\\times W .\u4e0d\u50cfyolo\u6216\u8005SSD\u4e00\u6837\u5e26\u6709background channel\u3002 \u5728\u8bad\u7ec3\u7684\u65f6\u5019\uff0c\u76f4\u89c9\u4e0e\u7ecf\u9a8c\u8868\u793a\u4e0d\u5e94\u8be5\u7b80\u5355\u5730\u60e9\u7f5a\u4e0d\u6b63\u786e\u7684\u89d2\u70b9\u4f4d\u7f6e\u3002\u8fd9\u91cc\u6839\u636e\u7269\u4f53\u7684\u4f53\u79ef\u7684\u8bbe\u5b9a\u4e0d\u540c\u7684radius cost. radius \u867d\u7136\u8868\u8fbe\u7684\u662f\u4e00\u4e2a\u5706\u7684\u6982\u5ff5\uff0c\u4f46\u662f\u5b9e\u9645\u5b9e\u73b0\u7684\u65f6\u5019\u662f\u4e00\u4e2a\u6b63\u65b9\u5f62\u7684\u534a\u8fb9\u957f, \u5176\u957f\u5ea6\u7684\u8bbe\u5b9a\u662f\u8bf4\u4e00\u5bf9ground truth\u70b9\u7684\u6b63\u65b9\u5f62\u533a\u57df\u5185\uff0c\u4efb\u9009\u4e24\u70b9\uff0c\u4e24\u70b9\u6784\u6210\u7684bbox\u4e0egt\u7684bbox\u7684IoU\u4e0b\u9650\u4e0d\u4f4e\u4e8e\u67d0\u4e00\u4e2a\u8bbe\u5b9a\u503c(\u672c\u6587\u53d60.3). \\forall p_i\\in{LeftTop}, \\forall p_j \\in {RightBottom} \\quad IoU(bbox_{p_i, p_j}, bbox_{gt}) \u539f\u7248code\u6709\u9519\u8bef\uff0c\u7ecf\u8fc7\u4e00\u4e9b\u4fee\u6b63\u540e\u6b63\u786e\u7684\u5b9e\u73b0\u5e94\u5f53\u662f\u3002 def compute_radius(det_size, min_overlap=0.7): \"\"\" Compute radius from ground truth bbox. original equation: (w - 2r) (h - 2r) / (wh) = min_overlap Take the solution with a smaller magnitude. \"\"\" height, width = det_size a2 = 4 b2 = 2 * (height + width) c2 = (1 - min_overlap) * width * height sq2 = np.sqrt(b2 ** 2 - 4 * a2 * c2) r2 = (b2 - sq2) / (2 * a2) return r2 \u8fd9\u4e2aidea\u70ed\u70b9\u56fe\u7684idea\u4ee5\u53caofficial code(\u5c3d\u7ba1\u4e0d\u4e00\u5b9a\u6b63\u786e)\u88ab\u591a\u4e2a\u540e\u7eed\u7684\u6587\u7ae0\u4f7f\u7528, \u5305\u62ec Object as point , RTM3D Unofficial Implementation \u6700\u7ec8\u8bbe\u8ba1\u51fa\u4e00\u4e2afocal loss,\u539f\u7248focal loss\u6e90\u81ea\u4e0e \u8fd9\u7bc7\u6587\u7ae0 ,\u5728 \u8fd9\u91cc \u6709\u7b80\u4ecb\u3002\u8fd9\u91cc\u7684\u5b9a\u4e49\u662f L_{det}=\\frac{-1}{N} \\sum_{c=1}^{C} \\sum_{i=1}^{H} \\sum_{j=1}^{W}\\left\\{\\begin{array}{c}{\\left(1-p_{c i j}\\right)^{\\alpha} \\log \\left(p_{c i j}\\right)} if (y_{cij} == 1) \\\\ {\\left(1-y_{c i j}\\right)^{\\beta}\\left(p_{c i j}\\right)^{\\alpha} \\log \\left(1-p_{c i j}\\right) \\text { otherwise }}\\end{array}\\right. \u5176\u4e2d p_{cij} \u4e3a (i,j) \u4f4d\u7f6e\u4e0a\u7684score\uff0c e^{-\\frac{x^2+y^2}{2\\sigma^2}} ,\u5176\u4e2d \\sigma \u662fradius\u662f 1/3 , N \u662f\u56fe\u7247\u4e2d\u7269\u4f53\u7684\u6570\u76ee\u3002 \\alpha, \\beta \u662f\u53ef\u8c03\u8282\u7684\u8d85\u53c2\u6570\u3002 \u7531\u4e8e\u5377\u79ef\u7f51\u7edc\u91cc\u9762\u6211\u4eec\u9700\u8981\u5bf9\u56fe\u7247\u4e0b\u91c7\u6837\uff0c (x,y) \\rightarrow (\\frac{x}{n}, \\frac{y}{n}) ,\u9700\u8981\u989d\u5916\u5b66\u4e60\u4e00\u4e2aoffsets\u53bb\u8865\u507f \\boldsymbol{o}_{k}=\\left(\\frac{x_{k}}{n}-\\left\\lfloor\\frac{x_{k}}{n}\\right\\rfloor, \\frac{y_{k}}{n}-\\left\\lfloor\\frac{y_{k}}{n}\\right\\rfloor\\right) \u8fd9\u4e2acost\u53ef\u4ee5\u7528 L_{o f f}=\\frac{1}{N} \\sum_{k=1}^{N} \\operatorname{SmoothL} 1 \\operatorname{Loss}\\left(\\boldsymbol{o}_{k}, \\hat{\\boldsymbol{o}}_{k}\\right)","title":"\u9884\u6d4b\u89d2\u70b9"},{"location":"other_categories/object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/#_2","text":"\u56e0\u4e3a\u4e00\u5f20\u56fe\u5982\u679c\u6709\u591a\u4e2a\u7269\u4f53\uff0c\u4e00\u4e2a\u56fe\u4f1a\u6709\u4e0d\u6b62\u4e00\u5bf9\u7684\u5de6\u4e0a\u89d2\u70b9\u548c\u53f3\u4e0b\u70b9\u3002\u672c\u6587\u7684\u505a\u6cd5\u63d0\u5230\u4e86 \u8fd9\u7bc7\u8bba\u6587 , \u7b80\u4ecb \u7f51\u7edc\u7ed9\u6bcf\u4e00\u4e2a\u5de6\u4e0a\u89d2\u4e0e\u53f3\u4e0b\u89d2\u89d2\u9884\u6d4b\u4e00\u4e2aembedding vector\uff0c\u672c\u6587\u8fd9\u91cc\u6a21\u4eff\u524d\u4e00\u7bc7\u8bba\u6587\u7684\u505a\u6cd5\uff0c\u7ef4\u5ea6\u4ec5\u4e3a1\u3002\u5982\u679c\u4ee3\u8868\u7684\u662f\u540c\u4e00\u4e2abounding box\uff0c\u90a3\u4e48\u4e24\u8005\u8ddd\u79bb\u5c31\u4f1a\u6bd4\u8f83\u5c0f\u3002\u5206\u4e3a\u4e24\u4e2aloss\uff0c\u4e00\u4e2a\u662fpull\u4e00\u4e2apush\u3002 L_{p u l l}=\\frac{1}{N} \\sum_{k=1}^{N}\\left[\\left(e_{t_{k}}-e_{k}\\right)^{2}+\\left(e_{b_{k}}-e_{k}\\right)^{2}\\right] L_{p u s h}=\\frac{1}{N(N-1)} \\sum_{k=1}^{N} \\sum_{j=1 \\atop j \\neq k}^{N} \\max \\left(0, \\Delta-\\left|e_{k}-e_{j}\\right|\\right) e_{t_k} \u4f5c\u4e3a\u7b2c k \u4e2a\u7269\u4f53\uff0c\u5de6\u4e0a\u89d2\u7684embedding, e_{b_k} \u4f5c\u4e3a\u53f3\u4e0b\u89d2\u7684\u3002 e_k \u662f e_{t_k}, e_{b_k} \u7684\u5747\u503c\uff0c\u7136\u540e\u8bbe\u5b9a \\Delta=1 \uff0c\u4e0eoffset\u4e00\u6837\uff0c\u8fd9\u4e2aloss\u53ea\u6267\u884c\u5728gt\u7684\u89d2\u843d\u4f4d\u7f6e\u4e0a.","title":"\u5c06\u89d2\u70b9\u805a\u56e2"},{"location":"other_categories/object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/#corner-pooling","text":"\u7528\u6765\u5141\u8bb8\u6bcf\u4e00\u4e2a\uff0c\u8ba1\u7b97\u65b9\u5f0f\u5982\u56fe t_{i j}=\\left\\{\\begin{array}{cl}{\\max \\left(f_{t_{i j}}, t_{(i+1) j}\\right)} & {\\text { if } i<H} \\\\ {f_{t_{H j}}} & {\\text { otherwise }}\\end{array}\\right. l_{i j}=\\left\\{\\begin{array}{cl}{\\max \\left(f_{l_{i j}}, l_{i(j+1)}\\right)} & {\\text { if } j<W} \\\\ {f_{l_{i W}}} & {\\text { otherwise }}\\end{array}\\right. \u7136\u540e\u5bf9 t_{ij}, l_{ij} \u76f8\u52a0\u5f97\u5230\u7ed3\u679c\u3002\u4e3a\u53f3\u4e0b\u89d2\u7684pooling layer\u53d6max\u7684\u65b9\u5411\u76f8\u53cd\u3002 \u5148\u5c06\u57fa\u7840\u7684resblock\u7684\u7b2c\u4e00\u4e2a 3\\times 3 \u5377\u79ef\u6539\u4e3acorner pooling.","title":"Corner Pooling"},{"location":"other_categories/object_detection_2D/DIoULoss/","text":"Distance-IoU Loss: Faster and Better Learning for Bounding Box Regression This paper introduce distance-IoU based on the basic IoU metric and the GIoU . [GIoU]\u7684\u5f15\u51fa\u672c\u6765\u662f\u4e3a\u4e86\u89e3\u51b3\u4e24\u4e2abounding box \u4e0d\u91cd\u5408\u65f6\u635f\u5931\u51fd\u6570\u53ef\u5bfc\u6027\u7684\u95ee\u9898\u3002\u4f46\u662f\u4f5c\u8005\u901a\u8fc7\u4e00\u4e2a\u975e\u5e38\u6709\u542f\u53d1\u6027\u7684\u4eff\u771f\u5b9e\u9a8c\uff0c\u8bf4\u660e\u4e86 GIoU\u7684\u6536\u655b\u6027\u95ee\u9898\u3002 \u7136\u540e\u63d0\u51fa\u4e86\u5c3a\u5ea6\u4e0ainvariant\u4e14\u6536\u655b\u6027\u66f4\u597d\u7684DIoU\u3002 \u4f5c\u8005\u4e4b\u540e\u8fdb\u4e00\u6b65\u8003\u8651\u4e86aspect ratio\u957f\u5bbd\u6bd4\u7684regularization \u95ee\u9898\uff0c\u63d0\u51fa\u4e86CIoU. Update 2020.05.09: CIoU and Cluster-NMS: pdf code Simulation on the convergence of IoU losses \u4f5c\u8005\u4eff\u771f\u4e86\u4e0d\u540c\u8d77\u59cb\u6761\u4ef6\u4e0b\uff0c\u4f7f\u7528\u4e0d\u540c\u7684IoU\u51fd\u6570\u8fdb\u884c\u68af\u5ea6\u4e0b\u964d\uff0c\u89c2\u5bdf\u6536\u655b\u6027\u3002 \u7b97\u6cd5\u5982\u4e0b \u8fd9\u4e2a\u7b97\u6cd5\u6709\u51e0\u4e2a\u503c\u5f97\u6ce8\u610f\u7684\u5730\u65b9\uff0c\u9996\u5148\u662f\u76ee\u6807\u6846\u4ee5\u53ca\u8d77\u59cb\u76f8\u5173\u6570\u636e\u7684\u9009\u62e9\uff0c\u5176\u6b21\u662f\u68af\u5ea6\u4e0b\u964d\u7684\u6a21\u62df\u9000\u706b\uff0c\u6700\u540e\u8981\u5173\u6ce8\u7684\u662f\u68af\u5ea6\u4e0b\u964d\u66f4\u65b0(\u7b2c9\u884c)\u524d\u9762\u7684\u56e0\u5b50. \u6700\u540e\u7684\u6536\u655b\u6548\u679c\u5982\u56fe \u6587\u7ae0\u6307\u51fa\uff0c\u4f7f\u7528GIoU\u4f5c\u8bad\u7ec3\u7684\u65f6\u5019,\u4f1a\u5148\u4f7fprediction\u9762\u79ef\u589e\u5927\uff0c\u5c3d\u53ef\u80fd\u5b9e\u73b0\u76f8\u4ea4\uff0c\u518d\u8fdb\u884c\u62df\u5408\uff0c\u4f18\u5316\u8fc7\u7a0b\u6bd4\u8f83\u626d\u66f2\uff0c\u4e0d\u592a\u597d\uff0c\u5b9e\u9a8c\u7ed3\u679c\u4e5f\u8bc1\u660e\u5176\u6536\u655b\u5f88\u6162\u3002 \u4eff\u771f\u5b9e\u9a8c\u4ee3\u7801\u5728 \u989d\u5916\u7684\u4ed3\u5e93 DIoU\u4e0eCIoU \\mathcal{L}_{D I o U}=1-\\operatorname{IoU}+\\frac{\\rho^{2}\\left(\\mathbf{b}, \\mathbf{b}^{g t}\\right)}{c^{2}} \u5176\u4e2d\u5206\u5b50\u4e0a\u4e3a\u4e24\u4e2abounding box \u4e2d\u5fc3\u70b9\u7684\u76f4\u7ebf\u8ddd\u79bb\uff0c c \u4e3a\u4e24\u4e2abounding box\u6700\u5c0f\u5305\u7edc\u6846\u7684\u5bf9\u89d2\u7ebf\u957f\u5ea6\u3002 CIoU\u5219\u662f: \\mathcal{L}_{C I o U}=1-I o U+\\frac{\\rho^{2}\\left(\\mathbf{b}, \\mathbf{b}^{g t}\\right)}{c^{2}}+\\alpha v v=\\frac{4}{\\pi^{2}}\\left(\\arctan \\frac{w^{g t}}{h^{g t}}-\\arctan \\frac{w}{h}\\right)^{2} \\alpha=\\frac{v}{(1-I o U)+v} \u8fd9\u91cc\u4e00\u4e2a\u4e2a\u5206\u6790\uff0c\u9996\u5148 v \u6307\u4ee3\u7684\u662f\u4e24\u4e2abounding box\u4e4b\u95f4\u7531\u4e8easpect ratio\u4e0d\u540c\u4ea7\u751f\u7684\u8bef\u5dee\uff0c\u8fd9\u91cc\u7528 arctan \u7684\u5dee\u503c\u63cf\u8ff0\u3002 \\alpha \u662f\u4e00\u4e2a\u53c2\u6570\uff0c\u5982\u679cIoU \u63a5\u8fd1\u4e8e1\uff0c\u5219 \\alpha \u63a5\u8fd11\uff0c\u6743\u91cd\u4e3a\u6700\u5927\uff0c\u5982\u679c IoU \u63a5\u8fd1\u4e8e0\uff0c\u5219\u957f\u5bbd\u6bd4\u5bf9\u5e94\u7684\u6743\u91cd\u4e0b\u8c03\u3002 \u4f5c\u8005\u5bf9CIoU\u7684\u53cd\u5411\u4f20\u64ad\u4e5f\u505a\u4e86\u4e00\u4e2a\u8fd1\u4f3c\u4f18\u5316\u6765\u63d0\u5347\u7a33\u5b9a\u6027\uff0c\u5177\u4f53\u770b\u539f\u6587\u4ee5\u53ca\u6e90\u7801\u3002 Cluster NMS (update on 2020.05.09) \u7ffb\u8bd1: 1. \u5047\u8bbe\u6240\u6709bounding box\u90fd\u4fdd\u7559\u4e0b\u6765\uff0c\u4e5f\u90fd\u5bf9\u5176\u4ed6\u6bd4\u5b83score\u4f4e\u7684boxes\u6709\u6291\u5236\u53ef\u80fd\u6027 2. \u7701\u7565\u4e0a\u4e00\u4e2a\u5faa\u73af\u4e2d\u88ab\u6291\u5236\u7684boxes\uff0c\u8fed\u4ee3 FastNMS \u51e0\u4e2a\u5c0f\u7ed3\u8bba\uff1a 1. \u82e5\u8fed\u4ee3\u6b21\u6570\u4e3a1\uff0c\u7b97\u6cd5\u5c31\u7b49\u540c\u4e8eFastNMS 2. \u82e5\u8fed\u4ee3\u6b21\u6570\u4e3aN\uff0c\u7b97\u6cd5\u7ed3\u679c\u4e00\u5b9a\u7b49\u540c\u4e8e\u539f\u59cbNMS(\u4f5c\u8005\u6709\u8bc1\u660e) 3. \u4e00\u822c\u60c5\u51b5\u4e0b\u8fed\u4ee3\u6b21\u6570\u663e\u8457\u5730\u5c0f\u4e8eN 4. torchVision\u7684NMS\u662f\u539f\u7248NMS\uff0c\u4f46\u662f\u901a\u8fc7\u786c\u4ef6\u52a0\u901f\u6bd4\u8fd9\u4e2a\u8fd8\u8981\u5feb...???","title":"Distance-IoU Loss: Faster and Better Learning for Bounding Box Regression"},{"location":"other_categories/object_detection_2D/DIoULoss/#distance-iou-loss-faster-and-better-learning-for-bounding-box-regression","text":"This paper introduce distance-IoU based on the basic IoU metric and the GIoU . [GIoU]\u7684\u5f15\u51fa\u672c\u6765\u662f\u4e3a\u4e86\u89e3\u51b3\u4e24\u4e2abounding box \u4e0d\u91cd\u5408\u65f6\u635f\u5931\u51fd\u6570\u53ef\u5bfc\u6027\u7684\u95ee\u9898\u3002\u4f46\u662f\u4f5c\u8005\u901a\u8fc7\u4e00\u4e2a\u975e\u5e38\u6709\u542f\u53d1\u6027\u7684\u4eff\u771f\u5b9e\u9a8c\uff0c\u8bf4\u660e\u4e86 GIoU\u7684\u6536\u655b\u6027\u95ee\u9898\u3002 \u7136\u540e\u63d0\u51fa\u4e86\u5c3a\u5ea6\u4e0ainvariant\u4e14\u6536\u655b\u6027\u66f4\u597d\u7684DIoU\u3002 \u4f5c\u8005\u4e4b\u540e\u8fdb\u4e00\u6b65\u8003\u8651\u4e86aspect ratio\u957f\u5bbd\u6bd4\u7684regularization \u95ee\u9898\uff0c\u63d0\u51fa\u4e86CIoU. Update 2020.05.09: CIoU and Cluster-NMS: pdf code","title":"Distance-IoU Loss: Faster and Better Learning for Bounding Box Regression"},{"location":"other_categories/object_detection_2D/DIoULoss/#simulation-on-the-convergence-of-iou-losses","text":"\u4f5c\u8005\u4eff\u771f\u4e86\u4e0d\u540c\u8d77\u59cb\u6761\u4ef6\u4e0b\uff0c\u4f7f\u7528\u4e0d\u540c\u7684IoU\u51fd\u6570\u8fdb\u884c\u68af\u5ea6\u4e0b\u964d\uff0c\u89c2\u5bdf\u6536\u655b\u6027\u3002 \u7b97\u6cd5\u5982\u4e0b \u8fd9\u4e2a\u7b97\u6cd5\u6709\u51e0\u4e2a\u503c\u5f97\u6ce8\u610f\u7684\u5730\u65b9\uff0c\u9996\u5148\u662f\u76ee\u6807\u6846\u4ee5\u53ca\u8d77\u59cb\u76f8\u5173\u6570\u636e\u7684\u9009\u62e9\uff0c\u5176\u6b21\u662f\u68af\u5ea6\u4e0b\u964d\u7684\u6a21\u62df\u9000\u706b\uff0c\u6700\u540e\u8981\u5173\u6ce8\u7684\u662f\u68af\u5ea6\u4e0b\u964d\u66f4\u65b0(\u7b2c9\u884c)\u524d\u9762\u7684\u56e0\u5b50. \u6700\u540e\u7684\u6536\u655b\u6548\u679c\u5982\u56fe \u6587\u7ae0\u6307\u51fa\uff0c\u4f7f\u7528GIoU\u4f5c\u8bad\u7ec3\u7684\u65f6\u5019,\u4f1a\u5148\u4f7fprediction\u9762\u79ef\u589e\u5927\uff0c\u5c3d\u53ef\u80fd\u5b9e\u73b0\u76f8\u4ea4\uff0c\u518d\u8fdb\u884c\u62df\u5408\uff0c\u4f18\u5316\u8fc7\u7a0b\u6bd4\u8f83\u626d\u66f2\uff0c\u4e0d\u592a\u597d\uff0c\u5b9e\u9a8c\u7ed3\u679c\u4e5f\u8bc1\u660e\u5176\u6536\u655b\u5f88\u6162\u3002 \u4eff\u771f\u5b9e\u9a8c\u4ee3\u7801\u5728 \u989d\u5916\u7684\u4ed3\u5e93","title":"Simulation on the convergence of IoU losses"},{"location":"other_categories/object_detection_2D/DIoULoss/#diouciou","text":"\\mathcal{L}_{D I o U}=1-\\operatorname{IoU}+\\frac{\\rho^{2}\\left(\\mathbf{b}, \\mathbf{b}^{g t}\\right)}{c^{2}} \u5176\u4e2d\u5206\u5b50\u4e0a\u4e3a\u4e24\u4e2abounding box \u4e2d\u5fc3\u70b9\u7684\u76f4\u7ebf\u8ddd\u79bb\uff0c c \u4e3a\u4e24\u4e2abounding box\u6700\u5c0f\u5305\u7edc\u6846\u7684\u5bf9\u89d2\u7ebf\u957f\u5ea6\u3002 CIoU\u5219\u662f: \\mathcal{L}_{C I o U}=1-I o U+\\frac{\\rho^{2}\\left(\\mathbf{b}, \\mathbf{b}^{g t}\\right)}{c^{2}}+\\alpha v v=\\frac{4}{\\pi^{2}}\\left(\\arctan \\frac{w^{g t}}{h^{g t}}-\\arctan \\frac{w}{h}\\right)^{2} \\alpha=\\frac{v}{(1-I o U)+v} \u8fd9\u91cc\u4e00\u4e2a\u4e2a\u5206\u6790\uff0c\u9996\u5148 v \u6307\u4ee3\u7684\u662f\u4e24\u4e2abounding box\u4e4b\u95f4\u7531\u4e8easpect ratio\u4e0d\u540c\u4ea7\u751f\u7684\u8bef\u5dee\uff0c\u8fd9\u91cc\u7528 arctan \u7684\u5dee\u503c\u63cf\u8ff0\u3002 \\alpha \u662f\u4e00\u4e2a\u53c2\u6570\uff0c\u5982\u679cIoU \u63a5\u8fd1\u4e8e1\uff0c\u5219 \\alpha \u63a5\u8fd11\uff0c\u6743\u91cd\u4e3a\u6700\u5927\uff0c\u5982\u679c IoU \u63a5\u8fd1\u4e8e0\uff0c\u5219\u957f\u5bbd\u6bd4\u5bf9\u5e94\u7684\u6743\u91cd\u4e0b\u8c03\u3002 \u4f5c\u8005\u5bf9CIoU\u7684\u53cd\u5411\u4f20\u64ad\u4e5f\u505a\u4e86\u4e00\u4e2a\u8fd1\u4f3c\u4f18\u5316\u6765\u63d0\u5347\u7a33\u5b9a\u6027\uff0c\u5177\u4f53\u770b\u539f\u6587\u4ee5\u53ca\u6e90\u7801\u3002","title":"DIoU\u4e0eCIoU"},{"location":"other_categories/object_detection_2D/DIoULoss/#cluster-nms-update-on-20200509","text":"\u7ffb\u8bd1: 1. \u5047\u8bbe\u6240\u6709bounding box\u90fd\u4fdd\u7559\u4e0b\u6765\uff0c\u4e5f\u90fd\u5bf9\u5176\u4ed6\u6bd4\u5b83score\u4f4e\u7684boxes\u6709\u6291\u5236\u53ef\u80fd\u6027 2. \u7701\u7565\u4e0a\u4e00\u4e2a\u5faa\u73af\u4e2d\u88ab\u6291\u5236\u7684boxes\uff0c\u8fed\u4ee3 FastNMS \u51e0\u4e2a\u5c0f\u7ed3\u8bba\uff1a 1. \u82e5\u8fed\u4ee3\u6b21\u6570\u4e3a1\uff0c\u7b97\u6cd5\u5c31\u7b49\u540c\u4e8eFastNMS 2. \u82e5\u8fed\u4ee3\u6b21\u6570\u4e3aN\uff0c\u7b97\u6cd5\u7ed3\u679c\u4e00\u5b9a\u7b49\u540c\u4e8e\u539f\u59cbNMS(\u4f5c\u8005\u6709\u8bc1\u660e) 3. \u4e00\u822c\u60c5\u51b5\u4e0b\u8fed\u4ee3\u6b21\u6570\u663e\u8457\u5730\u5c0f\u4e8eN 4. torchVision\u7684NMS\u662f\u539f\u7248NMS\uff0c\u4f46\u662f\u901a\u8fc7\u786c\u4ef6\u52a0\u901f\u6bd4\u8fd9\u4e2a\u8fd8\u8981\u5feb...???","title":"Cluster NMS (update on 2020.05.09)"},{"location":"other_categories/object_detection_2D/DRN_and_SKU110K-R/","text":"Dynamic Refinement Network for Oriented and Densely Packed Object Detection \u8fd9\u7bc7paper\u5904\u7406\u7684\u662fSKU\u6570\u636e\u96c6\uff0c\u7279\u70b9\u662f\u7269\u4f53\u6570\u91cf\u5f88\u5927\uff0c\u5f88\u5bc6\u96c6\uff0c\u4e14\u4f5c\u8005\u8fdb\u4e00\u6b65\u63d0\u51faSKU110K-R\u6570\u636e\u96c6\uff0cbounding box\u5e26\u6709\u65cb\u8f6c\u91cf\uff0c\u8fd9\u4e2a\u6570\u636e\u96c6\u6765\u81ea\u4e8eSKU110K\u76f4\u63a5\u9009\u62e9\uff0c\u63d0\u51fa\u8fd9\u4e2a\u6570\u636e\u96c6\u7684\u610f\u4e49\u5728\u4e8e\u4fc3\u8fdbdensely oriented bounding box\u7684\u7814\u7a76\u3002\u7f51\u7edc\u7ed3\u6784\u4e0a\u63d0\u51fa\u4e86\u51e0\u4e2a\u65b0\u7684\u6a21\u5757\uff0c\u9002\u5e94\u4e0d\u540c\u7684\u7269\u4f53\u5f62\u72b6\u4e0e\u7c7b\u522b\u7279\u5b9a\u7684\u4fe1\u606f\u3002 \u7f51\u7edc\u7ed3\u6784 \u7f51\u7edc\u601d\u8def\u57fa\u4e8e Object as Point . \u7f51\u7edc\u4f1a\u9884\u6d4b \\theta \u89d2\uff0cbounding box\u7531\u65cb\u8f6c\u5f97\u6765 \\begin{array}{l} P_{l t}=M_{r}[-w / 2,-h / 2]^{T}+\\left[c_{x}+\\delta_{x}, c_{y}+\\delta_{y}\\right]^{T} \\\\ P_{r t}=M_{r}[+w / 2,-h / 2]^{T}+\\left[c_{x}+\\delta_{x}, c_{y}+\\delta_{y}\\right]^{T} \\\\ P_{l b}=M_{r}[-w / 2,+h / 2]^{T}+\\left[c_{x}+\\delta_{x}, c_{y}+\\delta_{y}\\right]^{T} \\\\ P_{r b}=M_{r}[+w / 2,+h / 2]^{T}+\\left[c_{x}+\\delta_{x}, c_{y}+\\delta_{y}\\right]^{T} \\end{array} \u503c\u5f97\u6ce8\u610f\u7684\u5730\u65b9\u5728\u4e8e\u7531\u4e8eobject as point\u7684 NMS\u53ea\u662f\u5bf9heatmap\u8fdb\u884c\u7b80\u5355\u7684maxpooling\u8ba1\u7b97\uff0c\u56e0\u800c\u4e0d\u9700\u8981\u8ba1\u7b97rotated IoU\uff0c\u901f\u5ea6\u5feb\u800c\u4e0d\u5bb9\u6613\u6709\u9519\u8bef\uff0c\u800c\u4e14object as point\u7684\u8f93\u51fa\u70b9\u6bd4\u8f83\u5bc6\u96c6\uff0c\u56e0\u800c\u5f88\u9002\u5408\u8fd9\u4e2a\u5bc6\u96c6\u7684\u6570\u636e\u96c6\u3002 \u989d\u5916\u7684\u7f51\u7edc\u6a21\u5757 Feature Selection Module & Rotation Convolution \u8fd9\u91cc\u662f\u501f\u52a9\u4e86deformable convolution\u7684\u601d\u8def\u4e0e\u4ee3\u7801 \u4f7f\u7528\u7f51\u7edc\u8f93\u51fa\u4e00\u4e2a\u8f6c\u89d2 \\theta \uff0c\u4fee\u6539\u539f\u59cbconvolution\u5bf9\u5e94\u7684offset\uff0c \\delta p_{i}=M_{r}(\\theta) \\cdot p_{i}-p_{i} X_{i}\\left(p_{0}\\right)=\\sum_{p_{n} \\in \\mathcal{R}} w\\left(p_{n}\\right) \\cdot X_{c}\\left(p_{0}+p_{n}+\\delta p_{n}\\right) \u7279\u5f81\u9009\u62e9\u4f7f\u7528\u7684\u662f\u7b80\u5355\u7684attention block. A_{i}^{\\prime}=\\operatorname{SoftMax}\\left(\\left[A_{1}, A_{2}, A_{3}\\right]\\right) Y=\\sum_{i} A_{i}^{\\prime} \\cdot X_{i} Dynamic Refinement Head H_{c}=C\\left(\\left(1+\\varepsilon \\cdot F_{\\Delta} /\\left\\|F_{\\Delta}\\right\\|\\right) \\cdot F_{m i d} ; \\Phi\\right) \\begin{array}{l} H_{b}=R\\left(F_{m i d} ; \\Psi\\right) \\\\ H_{r}=\\left(1+\\epsilon \\cdot \\tanh \\left(H_{\\Delta}\\right)\\right) \\cdot H_{b} \\end{array} Dataset","title":"Dynamic Refinement Network for Oriented and Densely Packed Object Detection"},{"location":"other_categories/object_detection_2D/DRN_and_SKU110K-R/#dynamic-refinement-network-for-oriented-and-densely-packed-object-detection","text":"\u8fd9\u7bc7paper\u5904\u7406\u7684\u662fSKU\u6570\u636e\u96c6\uff0c\u7279\u70b9\u662f\u7269\u4f53\u6570\u91cf\u5f88\u5927\uff0c\u5f88\u5bc6\u96c6\uff0c\u4e14\u4f5c\u8005\u8fdb\u4e00\u6b65\u63d0\u51faSKU110K-R\u6570\u636e\u96c6\uff0cbounding box\u5e26\u6709\u65cb\u8f6c\u91cf\uff0c\u8fd9\u4e2a\u6570\u636e\u96c6\u6765\u81ea\u4e8eSKU110K\u76f4\u63a5\u9009\u62e9\uff0c\u63d0\u51fa\u8fd9\u4e2a\u6570\u636e\u96c6\u7684\u610f\u4e49\u5728\u4e8e\u4fc3\u8fdbdensely oriented bounding box\u7684\u7814\u7a76\u3002\u7f51\u7edc\u7ed3\u6784\u4e0a\u63d0\u51fa\u4e86\u51e0\u4e2a\u65b0\u7684\u6a21\u5757\uff0c\u9002\u5e94\u4e0d\u540c\u7684\u7269\u4f53\u5f62\u72b6\u4e0e\u7c7b\u522b\u7279\u5b9a\u7684\u4fe1\u606f\u3002","title":"Dynamic Refinement Network for Oriented and Densely Packed Object Detection"},{"location":"other_categories/object_detection_2D/DRN_and_SKU110K-R/#_1","text":"\u7f51\u7edc\u601d\u8def\u57fa\u4e8e Object as Point . \u7f51\u7edc\u4f1a\u9884\u6d4b \\theta \u89d2\uff0cbounding box\u7531\u65cb\u8f6c\u5f97\u6765 \\begin{array}{l} P_{l t}=M_{r}[-w / 2,-h / 2]^{T}+\\left[c_{x}+\\delta_{x}, c_{y}+\\delta_{y}\\right]^{T} \\\\ P_{r t}=M_{r}[+w / 2,-h / 2]^{T}+\\left[c_{x}+\\delta_{x}, c_{y}+\\delta_{y}\\right]^{T} \\\\ P_{l b}=M_{r}[-w / 2,+h / 2]^{T}+\\left[c_{x}+\\delta_{x}, c_{y}+\\delta_{y}\\right]^{T} \\\\ P_{r b}=M_{r}[+w / 2,+h / 2]^{T}+\\left[c_{x}+\\delta_{x}, c_{y}+\\delta_{y}\\right]^{T} \\end{array} \u503c\u5f97\u6ce8\u610f\u7684\u5730\u65b9\u5728\u4e8e\u7531\u4e8eobject as point\u7684 NMS\u53ea\u662f\u5bf9heatmap\u8fdb\u884c\u7b80\u5355\u7684maxpooling\u8ba1\u7b97\uff0c\u56e0\u800c\u4e0d\u9700\u8981\u8ba1\u7b97rotated IoU\uff0c\u901f\u5ea6\u5feb\u800c\u4e0d\u5bb9\u6613\u6709\u9519\u8bef\uff0c\u800c\u4e14object as point\u7684\u8f93\u51fa\u70b9\u6bd4\u8f83\u5bc6\u96c6\uff0c\u56e0\u800c\u5f88\u9002\u5408\u8fd9\u4e2a\u5bc6\u96c6\u7684\u6570\u636e\u96c6\u3002","title":"\u7f51\u7edc\u7ed3\u6784"},{"location":"other_categories/object_detection_2D/DRN_and_SKU110K-R/#_2","text":"","title":"\u989d\u5916\u7684\u7f51\u7edc\u6a21\u5757"},{"location":"other_categories/object_detection_2D/DRN_and_SKU110K-R/#feature-selection-module-rotation-convolution","text":"\u8fd9\u91cc\u662f\u501f\u52a9\u4e86deformable convolution\u7684\u601d\u8def\u4e0e\u4ee3\u7801 \u4f7f\u7528\u7f51\u7edc\u8f93\u51fa\u4e00\u4e2a\u8f6c\u89d2 \\theta \uff0c\u4fee\u6539\u539f\u59cbconvolution\u5bf9\u5e94\u7684offset\uff0c \\delta p_{i}=M_{r}(\\theta) \\cdot p_{i}-p_{i} X_{i}\\left(p_{0}\\right)=\\sum_{p_{n} \\in \\mathcal{R}} w\\left(p_{n}\\right) \\cdot X_{c}\\left(p_{0}+p_{n}+\\delta p_{n}\\right) \u7279\u5f81\u9009\u62e9\u4f7f\u7528\u7684\u662f\u7b80\u5355\u7684attention block. A_{i}^{\\prime}=\\operatorname{SoftMax}\\left(\\left[A_{1}, A_{2}, A_{3}\\right]\\right) Y=\\sum_{i} A_{i}^{\\prime} \\cdot X_{i}","title":"Feature Selection Module &amp; Rotation Convolution"},{"location":"other_categories/object_detection_2D/DRN_and_SKU110K-R/#dynamic-refinement-head","text":"H_{c}=C\\left(\\left(1+\\varepsilon \\cdot F_{\\Delta} /\\left\\|F_{\\Delta}\\right\\|\\right) \\cdot F_{m i d} ; \\Phi\\right) \\begin{array}{l} H_{b}=R\\left(F_{m i d} ; \\Psi\\right) \\\\ H_{r}=\\left(1+\\epsilon \\cdot \\tanh \\left(H_{\\Delta}\\right)\\right) \\cdot H_{b} \\end{array}","title":"Dynamic Refinement Head"},{"location":"other_categories/object_detection_2D/DRN_and_SKU110K-R/#dataset","text":"","title":"Dataset"},{"location":"other_categories/object_detection_2D/DynamicRCNN/","text":"Dynamic R-CNN: Towards High Quality Object Detection via Dynamic Training \u672c\u6587\u63d0\u4f9b\u4e00\u4e2a\u57fa\u4e8emmdetection\u7684config\u5b9e\u73b0\uff0c\u5df2\u7ecf\u52a0\u5165mmdetection master\u5206\u652f . \u672c\u6587\u7684\u60f3\u6cd5\u662f\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u52a8\u6001\u8c03\u6574Label Assignment\u7684\u65b9\u5f0f\u4ee5\u53ca SmoothL1 Loss\u7684\u53c2\u6570\uff0c\u4f7f\u5f97\u968f\u7740\u8bad\u7ec3\u7684\u8fdb\u6b65.\u8bad\u7ec3\u51fa\u6765\u7684\u7f51\u7edc\u5b9a\u4f4d\u7cbe\u5ea6\u8f83\u5927\u5e45\u5ea6\u63d0\u5347(IoU threshold 0.9\u65f6AP\u63d0\u53475.5%).\u4e14\u4e0d\u589e\u52a0\u63a8\u7406\u6d88\u8d39\u3002 Dynamic Label Assignment \u4f5c\u8005\u6307\u51faanchor-based \u65b9\u6cd5\u5728\u5206\u914danchor\u7684\u65f6\u5019\u4f7f\u7528\u7684IoU threshold\u5c3d\u7ba1\u4e0e\u603b\u4f53\u7684mAP\u6ca1\u6709\u7ebf\u6027\u76f8\u5173\u6027\uff0c\u4f46\u662f\u968f\u7740IoU threshold\u589e\u5927\uff0c\u7f51\u7edc\u5728\u9ad8IoU threshold\u4e0b\u7684AP\u5c31\u8d8a\u9ad8(\u4f4eIoU threshold\u4e0b\u7684AP\u8d8a\u4f4e)\u3002 \u4f5c\u8005\u56e0\u800c\u6307\u51fa\uff0c\u5982\u679cobject detector\u60f3\u5f97\u5230\u66f4\u9ad8\u7684\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u5728\u8bad\u7ec3\u7f51\u7edc\u90fd\u65f6\u5019\u5e94\u8be5\u9700\u8981\u6bd4\u8f83\u8fbe\u5230IoU Threshold. \u4f46\u662f\u76f4\u63a5\u8fd9\u6837\u505a\u4f1a\u4f7f\u5f97\u6b63\u6837\u672c\u90fd\u6ca1\u4e86\uff0c \u4f5c\u8005\u6307\u51fa\uff0c\u968f\u7740\u8bad\u7ec3\u8fc7\u7a0b\u7684\u63a8\u8fdb\uff0c\u56de\u5f52\u6548\u679c\u4e5f\u90fd\u5728\u63d0\u5347\u3002\u56e0\u800c\u76f4\u89c9\u6765\u770b\uff0c\u5728\u8bad\u7ec3\u5f00\u59cb\u7684\u65f6\u5019\uff0c\u4f7f\u7528\u8f83\u4f4e\u7684IoU threshold,\u540e\u9762\u9010\u6e10\u52a0\u5927(\u6307\u5f97\u662f\u7b2c\u4e8c\u9636\u6bb5\u7684Gt assignment) Dynamic Loss","title":"Dynamic R-CNN: Towards High Quality Object Detection via Dynamic Training"},{"location":"other_categories/object_detection_2D/DynamicRCNN/#dynamic-r-cnn-towards-high-quality-object-detection-via-dynamic-training","text":"\u672c\u6587\u63d0\u4f9b\u4e00\u4e2a\u57fa\u4e8emmdetection\u7684config\u5b9e\u73b0\uff0c\u5df2\u7ecf\u52a0\u5165mmdetection master\u5206\u652f . \u672c\u6587\u7684\u60f3\u6cd5\u662f\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u52a8\u6001\u8c03\u6574Label Assignment\u7684\u65b9\u5f0f\u4ee5\u53ca SmoothL1 Loss\u7684\u53c2\u6570\uff0c\u4f7f\u5f97\u968f\u7740\u8bad\u7ec3\u7684\u8fdb\u6b65.\u8bad\u7ec3\u51fa\u6765\u7684\u7f51\u7edc\u5b9a\u4f4d\u7cbe\u5ea6\u8f83\u5927\u5e45\u5ea6\u63d0\u5347(IoU threshold 0.9\u65f6AP\u63d0\u53475.5%).\u4e14\u4e0d\u589e\u52a0\u63a8\u7406\u6d88\u8d39\u3002","title":"Dynamic R-CNN: Towards High Quality Object Detection via Dynamic Training"},{"location":"other_categories/object_detection_2D/DynamicRCNN/#dynamic-label-assignment","text":"\u4f5c\u8005\u6307\u51faanchor-based \u65b9\u6cd5\u5728\u5206\u914danchor\u7684\u65f6\u5019\u4f7f\u7528\u7684IoU threshold\u5c3d\u7ba1\u4e0e\u603b\u4f53\u7684mAP\u6ca1\u6709\u7ebf\u6027\u76f8\u5173\u6027\uff0c\u4f46\u662f\u968f\u7740IoU threshold\u589e\u5927\uff0c\u7f51\u7edc\u5728\u9ad8IoU threshold\u4e0b\u7684AP\u5c31\u8d8a\u9ad8(\u4f4eIoU threshold\u4e0b\u7684AP\u8d8a\u4f4e)\u3002 \u4f5c\u8005\u56e0\u800c\u6307\u51fa\uff0c\u5982\u679cobject detector\u60f3\u5f97\u5230\u66f4\u9ad8\u7684\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u5728\u8bad\u7ec3\u7f51\u7edc\u90fd\u65f6\u5019\u5e94\u8be5\u9700\u8981\u6bd4\u8f83\u8fbe\u5230IoU Threshold. \u4f46\u662f\u76f4\u63a5\u8fd9\u6837\u505a\u4f1a\u4f7f\u5f97\u6b63\u6837\u672c\u90fd\u6ca1\u4e86\uff0c \u4f5c\u8005\u6307\u51fa\uff0c\u968f\u7740\u8bad\u7ec3\u8fc7\u7a0b\u7684\u63a8\u8fdb\uff0c\u56de\u5f52\u6548\u679c\u4e5f\u90fd\u5728\u63d0\u5347\u3002\u56e0\u800c\u76f4\u89c9\u6765\u770b\uff0c\u5728\u8bad\u7ec3\u5f00\u59cb\u7684\u65f6\u5019\uff0c\u4f7f\u7528\u8f83\u4f4e\u7684IoU threshold,\u540e\u9762\u9010\u6e10\u52a0\u5927(\u6307\u5f97\u662f\u7b2c\u4e8c\u9636\u6bb5\u7684Gt assignment)","title":"Dynamic Label Assignment"},{"location":"other_categories/object_detection_2D/DynamicRCNN/#dynamic-loss","text":"","title":"Dynamic Loss"},{"location":"other_categories/object_detection_2D/EfficientDet/","text":"EfficientDet: Scalable and Efficient Object Detection \u8fd9\u7bc7\u6587\u7ae0\u7cfb\u7edf\u5730\u63d0\u51fa\u4e86 BiFPN(weighted bidirectional feature pyramid network),\u5e76\u63d0\u51fa\u4e86\u4e00\u5957scale up\u7684\u65b9\u6cd5\uff0c\u6700\u7ec8\u63d0\u51fa\u4e86EfficientDet,\u6839\u636epaper\u7684\u8bf4\u6cd5\u76f8\u8f83\u4e8e\u73b0\u6709\u7684benchmark\u53ef\u4ee5\u663e\u8457\u5730\u63d0\u5347\u901f\u5ea6\uff0c\u51cf\u5c11\u53c2\u6570\uff0c\u63d0\u5347\u7cbe\u5ea6.\u83dc\u5355\u4e0a\u7684\u4ee3\u7801\u8d85\u94fe\u63a5\u4e3a\u5b98\u65b9\u7684Tensorflow implementation, \u800c\u975e\u5b98\u65b9\u7684 Pytorch implementation \u4e5f\u5df2\u7ecf\u6709\u94fe\u63a5\u4e86.\u8fd9\u7bc7\u6587\u7ae0\u672c\u8d28\u4e0a\u662f EfficientNet \u8fd9\u4e00SOAT\u6a21\u578b\u5728\u68c0\u6d4b\u9886\u57df\u7684\u63a5\u7eed\u3002 BiFPN Block \u4f20\u7edf\u6765\u8bf4FPN\u53ef\u4ee5\u7528\u5982\u4e0b\u7684\u65b9\u7a0b\u5f0f\u8868\u8fbe: \\begin{array}{l}{P_{7}^{o u t}=\\operatorname{Conv}\\left(P_{7}^{i n}\\right)} \\\\ {P_{6}^{o u t}=\\operatorname{Conv}\\left(P_{6}^{i n}+\\operatorname{Resize}\\left(P_{7}^{o u t}\\right)\\right)} \\\\ {\\cdots} \\\\ {P_{3}^{o u t}=\\operatorname{Conv}\\left(P_{3}^{i n}+\\operatorname{Resize}\\left(P_{4}^{o u t}\\right)\\right)}\\end{array} \u5176\u4e2d Resize \u6307\u4ee3\u4e0a\u4e0b\u91c7\u6837\u4ee5\u8fbe\u5230\u76f8\u540c\u7684\u5206\u8fa8\u7387\u3002\u4e0d\u540c\u5206\u8fa8\u7387\u7684\u7279\u5f81\u5728\u878d\u5408\u7684\u65f6\u5019\u662f\u7b49\u6743\u5730\u6c42\u548c\u7684\u3002\u4f46\u662f\uff0c\u53ef\u4ee5\u731c\u6d4b and \u89c2\u6d4b\u5230\u4e0d\u540cscale\u7684\u7279\u5f81\u5bf9\u5f53\u524dscale\u7684\u8f93\u51fa\u7684\u8d21\u732e\u5e94\u8be5\u662f\u4e0d\u540c\u7684\uff0c\u56e0\u6b64\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e09\u79cd\u52a0\u6743\u878d\u5408\u7279\u5f81\u7684\u65b9\u5f0f 1. Unbounded Fusion: O = \\sum_i w_iI_i \u4e5f\u5c31\u662f\u76f4\u63a5\u6c42\u548c\uff0c\u4f5c\u8005\u53d1\u73b0\u8fd9\u79cd\u505a\u6cd5\u4f1a\u4f7f\u5f97\u8bad\u7ec3\u7ed3\u679c\u4e0d\u7a33\u5b9a\u3002\u56e0\u4e3a\u662fUnbounded\u7684\u878d\u5408 2. Softmax Fusion: O = \\sum_i \\frac{e^{w_i}}{\\sum_j e^{w_j}} I_i ,\u4f5c\u8005\u53d1\u73b0\u8fd9\u79cd\u505a\u6cd5\u7684\u8ba1\u7b97\u901f\u5ea6\u592a\u6162\u4e86\u3002 3. Fast-normalized Fusion: O = \\sum_i \\frac{w_i}{\\epsilon + \\sum_j w_j} I_j \u5176\u4e2d\u6bcf\u4e00\u4e2a\u6743\u91cd w_i \u90fd\u662fReLU\u7684\u7ed3\u679c\uff0c\u800c \\epsilon \u662f\u4e00\u4e2a\u5f88\u5c0f\u7684\u503c\u7528\u4e8e\u7a33\u5b9a\u8ba1\u7b97\u3002\u8ba1\u7b97\u901f\u5ea6\u8f83\u5feb\uff0c\u6027\u80fd\u4e0esoftmax\u76f8\u8fd1\u3002 EfficientDet Backbone\u4e3a\u540c\u4e00\u7ec4\u4f5c\u8005\u7684 EfficientNet Scaling Up \u6b63\u5982 EfficientNet \u7684\u505a\u6cd5\uff0c\u4f5c\u8005\u540c\u6837\u575a\u6301\u968f\u7740\u8f93\u5165\u56fe\u7247\u7684\u50cf\u7d20\u589e\u5927\uff0c\u6a21\u578b\u4e5f\u9700\u8981\u53d8\u5927\u3001\u53d8\u6df1\u3001\u53d8\u5bbd\u3002\u5728Detection\u4e2d\uff0cBackbone\u7684Scale Up\u4e0e EfficientNet \u4e2d\u7684\u4e00\u81f4\u3002 BiFPN \u7684channel\u6570,\u6df1\u5ea6\u4e0e\u9884\u6d4b\u7f51\u7edc\u7684\u6df1\u5ea6\u53d8\u5316\u5982\u4e0b W_{b i f p n}=64 \\cdot\\left(1.35^{\\phi}\\right), \\quad D_{b i f p n}=2+\\phi D_{b o x}=D_{c l a s s}=3+\\lfloor\\phi / 3\\rfloor \u8f93\u5165\u7684\u53d8\u5316\u4e3a R_{input} = 512+ \\phi * 128 \u4f5c\u8005\u7684\u5b9e\u9a8c\u5f88\u5b8c\u6574\uff0c\u503c\u5f97\u4e00\u8bfb","title":"EfficientDet: Scalable and Efficient Object Detection"},{"location":"other_categories/object_detection_2D/EfficientDet/#efficientdet-scalable-and-efficient-object-detection","text":"\u8fd9\u7bc7\u6587\u7ae0\u7cfb\u7edf\u5730\u63d0\u51fa\u4e86 BiFPN(weighted bidirectional feature pyramid network),\u5e76\u63d0\u51fa\u4e86\u4e00\u5957scale up\u7684\u65b9\u6cd5\uff0c\u6700\u7ec8\u63d0\u51fa\u4e86EfficientDet,\u6839\u636epaper\u7684\u8bf4\u6cd5\u76f8\u8f83\u4e8e\u73b0\u6709\u7684benchmark\u53ef\u4ee5\u663e\u8457\u5730\u63d0\u5347\u901f\u5ea6\uff0c\u51cf\u5c11\u53c2\u6570\uff0c\u63d0\u5347\u7cbe\u5ea6.\u83dc\u5355\u4e0a\u7684\u4ee3\u7801\u8d85\u94fe\u63a5\u4e3a\u5b98\u65b9\u7684Tensorflow implementation, \u800c\u975e\u5b98\u65b9\u7684 Pytorch implementation \u4e5f\u5df2\u7ecf\u6709\u94fe\u63a5\u4e86.\u8fd9\u7bc7\u6587\u7ae0\u672c\u8d28\u4e0a\u662f EfficientNet \u8fd9\u4e00SOAT\u6a21\u578b\u5728\u68c0\u6d4b\u9886\u57df\u7684\u63a5\u7eed\u3002","title":"EfficientDet: Scalable and Efficient Object Detection"},{"location":"other_categories/object_detection_2D/EfficientDet/#bifpn-block","text":"\u4f20\u7edf\u6765\u8bf4FPN\u53ef\u4ee5\u7528\u5982\u4e0b\u7684\u65b9\u7a0b\u5f0f\u8868\u8fbe: \\begin{array}{l}{P_{7}^{o u t}=\\operatorname{Conv}\\left(P_{7}^{i n}\\right)} \\\\ {P_{6}^{o u t}=\\operatorname{Conv}\\left(P_{6}^{i n}+\\operatorname{Resize}\\left(P_{7}^{o u t}\\right)\\right)} \\\\ {\\cdots} \\\\ {P_{3}^{o u t}=\\operatorname{Conv}\\left(P_{3}^{i n}+\\operatorname{Resize}\\left(P_{4}^{o u t}\\right)\\right)}\\end{array} \u5176\u4e2d Resize \u6307\u4ee3\u4e0a\u4e0b\u91c7\u6837\u4ee5\u8fbe\u5230\u76f8\u540c\u7684\u5206\u8fa8\u7387\u3002\u4e0d\u540c\u5206\u8fa8\u7387\u7684\u7279\u5f81\u5728\u878d\u5408\u7684\u65f6\u5019\u662f\u7b49\u6743\u5730\u6c42\u548c\u7684\u3002\u4f46\u662f\uff0c\u53ef\u4ee5\u731c\u6d4b and \u89c2\u6d4b\u5230\u4e0d\u540cscale\u7684\u7279\u5f81\u5bf9\u5f53\u524dscale\u7684\u8f93\u51fa\u7684\u8d21\u732e\u5e94\u8be5\u662f\u4e0d\u540c\u7684\uff0c\u56e0\u6b64\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e09\u79cd\u52a0\u6743\u878d\u5408\u7279\u5f81\u7684\u65b9\u5f0f 1. Unbounded Fusion: O = \\sum_i w_iI_i \u4e5f\u5c31\u662f\u76f4\u63a5\u6c42\u548c\uff0c\u4f5c\u8005\u53d1\u73b0\u8fd9\u79cd\u505a\u6cd5\u4f1a\u4f7f\u5f97\u8bad\u7ec3\u7ed3\u679c\u4e0d\u7a33\u5b9a\u3002\u56e0\u4e3a\u662fUnbounded\u7684\u878d\u5408 2. Softmax Fusion: O = \\sum_i \\frac{e^{w_i}}{\\sum_j e^{w_j}} I_i ,\u4f5c\u8005\u53d1\u73b0\u8fd9\u79cd\u505a\u6cd5\u7684\u8ba1\u7b97\u901f\u5ea6\u592a\u6162\u4e86\u3002 3. Fast-normalized Fusion: O = \\sum_i \\frac{w_i}{\\epsilon + \\sum_j w_j} I_j \u5176\u4e2d\u6bcf\u4e00\u4e2a\u6743\u91cd w_i \u90fd\u662fReLU\u7684\u7ed3\u679c\uff0c\u800c \\epsilon \u662f\u4e00\u4e2a\u5f88\u5c0f\u7684\u503c\u7528\u4e8e\u7a33\u5b9a\u8ba1\u7b97\u3002\u8ba1\u7b97\u901f\u5ea6\u8f83\u5feb\uff0c\u6027\u80fd\u4e0esoftmax\u76f8\u8fd1\u3002","title":"BiFPN Block"},{"location":"other_categories/object_detection_2D/EfficientDet/#efficientdet","text":"Backbone\u4e3a\u540c\u4e00\u7ec4\u4f5c\u8005\u7684 EfficientNet","title":"EfficientDet"},{"location":"other_categories/object_detection_2D/EfficientDet/#scaling-up","text":"\u6b63\u5982 EfficientNet \u7684\u505a\u6cd5\uff0c\u4f5c\u8005\u540c\u6837\u575a\u6301\u968f\u7740\u8f93\u5165\u56fe\u7247\u7684\u50cf\u7d20\u589e\u5927\uff0c\u6a21\u578b\u4e5f\u9700\u8981\u53d8\u5927\u3001\u53d8\u6df1\u3001\u53d8\u5bbd\u3002\u5728Detection\u4e2d\uff0cBackbone\u7684Scale Up\u4e0e EfficientNet \u4e2d\u7684\u4e00\u81f4\u3002 BiFPN \u7684channel\u6570,\u6df1\u5ea6\u4e0e\u9884\u6d4b\u7f51\u7edc\u7684\u6df1\u5ea6\u53d8\u5316\u5982\u4e0b W_{b i f p n}=64 \\cdot\\left(1.35^{\\phi}\\right), \\quad D_{b i f p n}=2+\\phi D_{b o x}=D_{c l a s s}=3+\\lfloor\\phi / 3\\rfloor \u8f93\u5165\u7684\u53d8\u5316\u4e3a R_{input} = 512+ \\phi * 128 \u4f5c\u8005\u7684\u5b9e\u9a8c\u5f88\u5b8c\u6574\uff0c\u503c\u5f97\u4e00\u8bfb","title":"Scaling Up"},{"location":"other_categories/object_detection_2D/FCOS/","text":"FCOS: Fully Convolutional One-Stage Object Detection \u672c\u6587\u662fanchor-free object detection\u7684\u53c8\u4e00\u4ee3\u8868\u4f5c\u54c1\uff0c\u5176\u6709\u6548\u6027\u5e94\u8be5\u662f\u663e\u8457\uff0c\u56e0\u4e3a\u5728\u540e\u6765\u7684\u5176\u4ed6object detection\u8bba\u6587\u4e2d\u8fdb\u884c\u4e86\u4f7f\u7528\uff0c\u5305\u62ec DSGN . \u672c\u6587\u5728 mmdetection \u4e2d\u4e5f\u6709\u590d\u73b0\uff0c\u6bd4\u5229\u7528maskrcnn-benchmark\u7684\u5b98\u65b9\u4ee3\u7801\u8981\u5bb9\u6613\u8bfb,\u5176\u5728 mmdetection \u4e2d\u7684\u4e3b\u8981\u4ee3\u7801\u5728 fcos_head.py FCOS - Output definition \u5bf9\u4e8e\u56fe\u7247\u4e2d\u7684\u6bcf\u4e00\u4e2a\u70b9\uff0c\u5982\u679c\u5b83\u662f\u6b63\u6837\u672c\uff0c\u90a3\u4e48\u76f4\u63a5\u56de\u5f52\u5b83\u5230\u56db\u4e2a\u8fb9\u754c\u7684\u8ddd\u79bb\u3002\u8fd9\u4e2a\u662fFCOS\u4e3b\u8981\u7684\u56de\u5f52\u5f62\u5f0f\uff0c\u4ee5\u6b64\u6765\u66ff\u4ee3anchor box FCOS - Ground Truth seperation \u8fd9\u4e00\u6bb5\u518d\u8865\u5145\u8bf4\u660e\u57fa\u4e8eretinanet\u7684FCOS\u5728\u65e0anchors\u7684\u60c5\u51b5\u4e0b\u5982\u4f55\u5206\u914dground truth\uff0c\u7531\u4e8eRetinanet\u7684\u7ed3\u6784\u6027\u8d28\uff0c\u4e0d\u540c\u5c42\u88ab\u5206\u914d\u4e0d\u540c\u7684\u611f\u53d7\u91ce\u4efb\u52a1\uff0c\u4f5c\u8005hardcode\u4e86\uff0c\u5bf9\u4e8e\u7b2c i \u5c42\u7684\u7f51\u7edc\uff0c\u53ea\u8d1f\u8d23 m_{i-1}< max(l*, r*, t*, b*) < m_i \u7684object,\u5176\u4e2d m_i \u4e3a\u6839\u636eFPN\u611f\u53d7\u91ce\u7684hardcode\u503c\uff0c\u5177\u4f53\u770b\u8bba\u6587\u3002 \u6bcf\u4e00\u4e2a\u70b9\uff0c\u53ea\u8981\u5b83\u5728\u5bf9\u5e94object\u7684\u6846\u5185\uff0c\u5b83\u5c31\u662f\u6b63\u6837\u672c\uff0c\u5982\u679c\u540c\u4e00\u4e2ascale\u5185\u4e00\u4e2a\u70b9\u4ecd\u5bf9\u5e94\u591a\u4e2agt object\uff0c\u5c31\u4ee5\u9762\u79ef\u6700\u5c0f\u7684\u4e3atarget. FCOS - centerness \u7531\u4e8e\u5206\u914d\u4e86\u5927\u91cf\u7684\u6b63\u6837\u672c\uff0c\u8fd9\u6837\u76f4\u63a5\u8bad\u7ec3\u7684\u7ed3\u679c\u662f\u6709\u5927\u91cf\u7684\u8fb9\u7f18\u70b9\u7ed9\u51fa\u9519\u8bef\u7684bbox\uff0c\u4f5c\u8005\u8fdb\u4e00\u6b65\u8865\u5145\uff0c\u8981\u6c42\u7f51\u7edc\u5bf9\u6bcf\u4e00\u4e2a\u70b9\u7ed9\u51fa\u4e00\u4e2a\"centerness\"\u7684\u4f30\u8ba1\uff0c\u5176\u76ee\u6807\u503c\u4e3a\uff1a \\text { centerness }^{*}=\\sqrt{\\frac{\\min \\left(l^{*}, r^{*}\\right)}{\\max \\left(l^{*}, r^{*}\\right)} \\times \\frac{\\min \\left(t^{*}, b^{*}\\right)}{\\max \\left(t^{*}, b^{*}\\right)}} \u5728inference\u7684\u65f6\u5019\uff0c\u6839\u636ecenterness\u8fdb\u884cNMS\u800c\u975e\u5206\u7c7b\u7f6e\u4fe1\u5ea6\uff0c\u6700\u7ec8\u80fd\u663e\u8457\u7684\u63d0\u5347\u7ed3\u679c\u3002","title":"FCOS: Fully Convolutional One-Stage Object Detection"},{"location":"other_categories/object_detection_2D/FCOS/#fcos-fully-convolutional-one-stage-object-detection","text":"\u672c\u6587\u662fanchor-free object detection\u7684\u53c8\u4e00\u4ee3\u8868\u4f5c\u54c1\uff0c\u5176\u6709\u6548\u6027\u5e94\u8be5\u662f\u663e\u8457\uff0c\u56e0\u4e3a\u5728\u540e\u6765\u7684\u5176\u4ed6object detection\u8bba\u6587\u4e2d\u8fdb\u884c\u4e86\u4f7f\u7528\uff0c\u5305\u62ec DSGN . \u672c\u6587\u5728 mmdetection \u4e2d\u4e5f\u6709\u590d\u73b0\uff0c\u6bd4\u5229\u7528maskrcnn-benchmark\u7684\u5b98\u65b9\u4ee3\u7801\u8981\u5bb9\u6613\u8bfb,\u5176\u5728 mmdetection \u4e2d\u7684\u4e3b\u8981\u4ee3\u7801\u5728 fcos_head.py","title":"FCOS: Fully Convolutional One-Stage Object Detection"},{"location":"other_categories/object_detection_2D/FCOS/#fcos-output-definition","text":"\u5bf9\u4e8e\u56fe\u7247\u4e2d\u7684\u6bcf\u4e00\u4e2a\u70b9\uff0c\u5982\u679c\u5b83\u662f\u6b63\u6837\u672c\uff0c\u90a3\u4e48\u76f4\u63a5\u56de\u5f52\u5b83\u5230\u56db\u4e2a\u8fb9\u754c\u7684\u8ddd\u79bb\u3002\u8fd9\u4e2a\u662fFCOS\u4e3b\u8981\u7684\u56de\u5f52\u5f62\u5f0f\uff0c\u4ee5\u6b64\u6765\u66ff\u4ee3anchor box","title":"FCOS - Output definition"},{"location":"other_categories/object_detection_2D/FCOS/#fcos-ground-truth-seperation","text":"\u8fd9\u4e00\u6bb5\u518d\u8865\u5145\u8bf4\u660e\u57fa\u4e8eretinanet\u7684FCOS\u5728\u65e0anchors\u7684\u60c5\u51b5\u4e0b\u5982\u4f55\u5206\u914dground truth\uff0c\u7531\u4e8eRetinanet\u7684\u7ed3\u6784\u6027\u8d28\uff0c\u4e0d\u540c\u5c42\u88ab\u5206\u914d\u4e0d\u540c\u7684\u611f\u53d7\u91ce\u4efb\u52a1\uff0c\u4f5c\u8005hardcode\u4e86\uff0c\u5bf9\u4e8e\u7b2c i \u5c42\u7684\u7f51\u7edc\uff0c\u53ea\u8d1f\u8d23 m_{i-1}< max(l*, r*, t*, b*) < m_i \u7684object,\u5176\u4e2d m_i \u4e3a\u6839\u636eFPN\u611f\u53d7\u91ce\u7684hardcode\u503c\uff0c\u5177\u4f53\u770b\u8bba\u6587\u3002 \u6bcf\u4e00\u4e2a\u70b9\uff0c\u53ea\u8981\u5b83\u5728\u5bf9\u5e94object\u7684\u6846\u5185\uff0c\u5b83\u5c31\u662f\u6b63\u6837\u672c\uff0c\u5982\u679c\u540c\u4e00\u4e2ascale\u5185\u4e00\u4e2a\u70b9\u4ecd\u5bf9\u5e94\u591a\u4e2agt object\uff0c\u5c31\u4ee5\u9762\u79ef\u6700\u5c0f\u7684\u4e3atarget.","title":"FCOS - Ground Truth seperation"},{"location":"other_categories/object_detection_2D/FCOS/#fcos-centerness","text":"\u7531\u4e8e\u5206\u914d\u4e86\u5927\u91cf\u7684\u6b63\u6837\u672c\uff0c\u8fd9\u6837\u76f4\u63a5\u8bad\u7ec3\u7684\u7ed3\u679c\u662f\u6709\u5927\u91cf\u7684\u8fb9\u7f18\u70b9\u7ed9\u51fa\u9519\u8bef\u7684bbox\uff0c\u4f5c\u8005\u8fdb\u4e00\u6b65\u8865\u5145\uff0c\u8981\u6c42\u7f51\u7edc\u5bf9\u6bcf\u4e00\u4e2a\u70b9\u7ed9\u51fa\u4e00\u4e2a\"centerness\"\u7684\u4f30\u8ba1\uff0c\u5176\u76ee\u6807\u503c\u4e3a\uff1a \\text { centerness }^{*}=\\sqrt{\\frac{\\min \\left(l^{*}, r^{*}\\right)}{\\max \\left(l^{*}, r^{*}\\right)} \\times \\frac{\\min \\left(t^{*}, b^{*}\\right)}{\\max \\left(t^{*}, b^{*}\\right)}} \u5728inference\u7684\u65f6\u5019\uff0c\u6839\u636ecenterness\u8fdb\u884cNMS\u800c\u975e\u5206\u7c7b\u7f6e\u4fe1\u5ea6\uff0c\u6700\u7ec8\u80fd\u663e\u8457\u7684\u63d0\u5347\u7ed3\u679c\u3002","title":"FCOS - centerness"},{"location":"other_categories/object_detection_2D/GFocalLoss/","text":"Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection \u8fd9\u7bc7paper\u7684\u521d\u8877\u662f\u5206\u6790IoU Centerness\u4e0eclassification loss\u7684\u76f8\u5173\u95ee\u9898\uff0c\u5728NMS\u7684\u65f6\u5019\uff0c\u6211\u4eec\u4f7f\u7528\u7684\u662fIoU Centerness\u548ccls Score\u7684\u4e58\u79ef\uff0c\u4f46\u662f\u8bad\u7ec3\u7684\u65f6\u5019\uff0ccls Score\u4f7f\u7528focal loss\u800cIoU Centerness\u88ab\u89c6\u4e3a\u56de\u5f52\u95ee\u9898\u3002\u8fd9\u5c31\u6784\u6210\u4e86\u4e00\u5b9a\u7684\u4e0d\u5339\u914d\u3002 \u672c\u6587\u501f\u6b64\u9996\u5148\u63d0\u51fa\u4e86 Quality Focal loss,\u4f7f\u5f97focal loss\u53ef\u4ee5\u7edf\u4e00\u5230\"\u8fde\u7eed\u7684\u5206\u7c7b\u95ee\u9898\",\u5176\u4e8c\u672c\u6587\u8fdb\u4e00\u6b65\u8003\u8651\u5c06\u8fd9\u4e2a\u6982\u5ff5\u8fdb\u884c\u6269\u5c55\u5230Distribution Focal Loss\uff0c\u53ef\u4ee5\u62df\u5408\u4efb\u610floss\uff0c\u6700\u540e\u63d0\u51fa\u7684Generalized Focal loss\u878d\u5408\u591a\u79cd\u60c5\u51b5\u3002 Focal loss \\mathbf{F L}(p)=-\\left(1-p_{t}\\right)^{\\gamma} \\log \\left(p_{t}\\right), p_{t}=\\left\\{\\begin{aligned} p, & \\text { when } y=1 \\\\ 1-p, & \\text { when } y=0 \\end{aligned}\\right. Quality Focal Loss \\mathbf{Q} \\mathbf{F} \\mathbf{L}(\\sigma)=-|y-\\sigma|^{\\beta}((1-y) \\log (1-\\sigma)+y \\log (\\sigma)) def quality_focal_loss( pred, # (n, 80) label, # (n) 0, 1-80: 0 is neg, 1-80 is positive score, # (n) reg target 0-1, only positive is good weight=None, beta=2.0, reduction='mean', avg_factor=None): \"\"\" from https://github.com/implus/GFocal/blob/cc0e72680f16a8abe0770eb531d6baa07a6e511f/mmdet/models/losses/gfocal_loss.py \"\"\" # all goes to 0 pred_sigmoid = pred.sigmoid() pt = pred_sigmoid zerolabel = pt.new_zeros(pred.shape) loss = F.binary_cross_entropy_with_logits( pred, zerolabel, reduction='none') * pt.pow(beta) label = label - 1 pos = (label >= 0).nonzero().squeeze(1) a = pos b = label[pos].long() # positive goes to bbox quality pt = score[a] - pred_sigmoid[a, b] loss[a,b] = F.binary_cross_entropy_with_logits( pred[a,b], score[a], reduction='none') * pt.pow(beta) loss = weight_reduce_loss(loss, weight, reduction, avg_factor) return loss Distribution Focal Loss (DFL) \u5f53\u6211\u4eec\u4f7f\u7528\u5e8f\u5217\u7684\u591a\u4e2a\u5206\u7c7b\u503c(multi-bin)\u5206\u7c7b\u65f6\uff0cinference\u7684\u65f6\u5019\u6211\u4eec\u4f7f\u7528 \\hat{y}=\\sum_{i=0}^{n} P\\left(y_{i}\\right) y_{i} \u3002 \\mathbf{D F L}\\left(\\mathcal{S}_{i}, \\mathcal{S}_{i+1}\\right)=-\\left(\\left(y_{i+1}-y\\right) \\log \\left(\\mathcal{S}_{i}\\right)+\\left(y-y_{i}\\right) \\log \\left(\\mathcal{S}_{i+1}\\right)\\right) \u5176\u4e2d \\mathcal{S}_{i}=\\frac{y_{i+1}-y}{y_{i+1}-y_{i}}, \\mathcal{S}_{i+1}=\\frac{y-y_{i}}{y_{i+1}-y_{i}} ,\u76f4\u89c9\u5c31\u662f\u6309\u7167\u6743\u91cd\u4f7f\u7528loss\u9f13\u52b1multibin ground truth\u4e34\u8fd1\u7684\u4e24\u4fa7\u5206\u7c7b\u70b9\u7684\u6743\u91cd\u3002 def distribution_focal_loss( pred, label, weight=None, reduction='mean', avg_factor=None): \"\"\" from https://github.com/implus/GFocal/blob/cc0e72680f16a8abe0770eb531d6baa07a6e511f/mmdet/models/losses/gfocal_loss.py \"\"\" disl = label.long() disr = disl + 1 wl = disr.float() - label wr = label - disl.float() loss = F.cross_entropy(pred, disl, reduction='none') * wl \\ + F.cross_entropy(pred, disr, reduction='none') * wr loss = weight_reduce_loss(loss, weight, reduction, avg_factor) return loss Generalized Focal Loss (GFL) \\mathbf{G F L}\\left(p_{y_{l}}, p_{y_{r}}\\right)=-\\left|y-\\left(y_{l} p_{y_{l}}+y_{r} p_{y_{r}}\\right)\\right|^{\\beta}\\left(\\left(y_{r}-y\\right) \\log \\left(p_{y_{l}}\\right)+\\left(y-y_{l}\\right) \\log \\left(p_{y_{r}}\\right)\\right) function focal_loss_y(x, gamma){ return - Math.pow(1-x, gamma) * Math.log(x) } function quality_focal_loss_y(x, y){ return - Math.pow(y-x, 2) * ((1-y) * Math.log(1-x) + y * Math.log(x)) } function get_focal_loss_list(p, gamma){ focal = [] for (j = 0; j < 98;j++){ focal.push(focal_loss_y(p[j], gamma)) } return focal } function get_quality_focal_loss_list(p, y){ focal = [] for (j = 0; j < 98;j++){ focal.push(quality_focal_loss_y(p[j], y)) } return focal } focalLoss = document.getElementById('focalLoss'); var p = []; for (i = 1; i < 99;i++){ p.push(i * 0.01); } var focal = get_focal_loss_list(p, 0.2) slider_steps = [] for (i = 0.2; i < 4; i += 0.2){ slider_steps.push( { method: 'animate', label: Math.floor(i * 100) /100, args: [ { data: [{ x:p, y: get_focal_loss_list(p, i)}], }, { transition: {duration: 20}, frame: {duration: 20, redraw: false}, } ] } ) } var qfocal = get_quality_focal_loss_list(p, 0.5) qslider_steps = [] for (i = 0.02; i < 0.98; i += 0.02){ qslider_steps.push( { method: 'animate', label: Math.floor(i * 100) /100, args: [ { data: [{ x:p, y: get_quality_focal_loss_list(p, i)}], }, { transition: {duration: 20}, frame: {duration: 20, redraw: false}, } ] } ) } Plotly.plot(focalLoss, [{ x: p, y: focal, }], { title: 'Focal Loss for positive samples', xaxis: { title: 'p' }, yaxis: { title: 'loss' }, sliders: [{ pad: {t: 30}, currentvalue: { xanchor: 'right', prefix: 'gamma: ', font: { color: '#888', size: 20 } }, steps: slider_steps }] }); Plotly.plot(qfocalLoss, [{ x: p, y: qfocal, }], { title: 'Quality Focal Loss with beta=2', xaxis: { title: 'p' }, yaxis: { title: 'loss' }, sliders: [{ pad: {t: 30}, currentvalue: { xanchor: 'right', prefix: 'gt_y: ', font: { color: '#888', size: 20 } }, steps: qslider_steps }] });","title":"Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection"},{"location":"other_categories/object_detection_2D/GFocalLoss/#generalized-focal-loss-learning-qualified-and-distributed-bounding-boxes-for-dense-object-detection","text":"\u8fd9\u7bc7paper\u7684\u521d\u8877\u662f\u5206\u6790IoU Centerness\u4e0eclassification loss\u7684\u76f8\u5173\u95ee\u9898\uff0c\u5728NMS\u7684\u65f6\u5019\uff0c\u6211\u4eec\u4f7f\u7528\u7684\u662fIoU Centerness\u548ccls Score\u7684\u4e58\u79ef\uff0c\u4f46\u662f\u8bad\u7ec3\u7684\u65f6\u5019\uff0ccls Score\u4f7f\u7528focal loss\u800cIoU Centerness\u88ab\u89c6\u4e3a\u56de\u5f52\u95ee\u9898\u3002\u8fd9\u5c31\u6784\u6210\u4e86\u4e00\u5b9a\u7684\u4e0d\u5339\u914d\u3002 \u672c\u6587\u501f\u6b64\u9996\u5148\u63d0\u51fa\u4e86 Quality Focal loss,\u4f7f\u5f97focal loss\u53ef\u4ee5\u7edf\u4e00\u5230\"\u8fde\u7eed\u7684\u5206\u7c7b\u95ee\u9898\",\u5176\u4e8c\u672c\u6587\u8fdb\u4e00\u6b65\u8003\u8651\u5c06\u8fd9\u4e2a\u6982\u5ff5\u8fdb\u884c\u6269\u5c55\u5230Distribution Focal Loss\uff0c\u53ef\u4ee5\u62df\u5408\u4efb\u610floss\uff0c\u6700\u540e\u63d0\u51fa\u7684Generalized Focal loss\u878d\u5408\u591a\u79cd\u60c5\u51b5\u3002","title":"Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection"},{"location":"other_categories/object_detection_2D/GFocalLoss/#focal-loss","text":"\\mathbf{F L}(p)=-\\left(1-p_{t}\\right)^{\\gamma} \\log \\left(p_{t}\\right), p_{t}=\\left\\{\\begin{aligned} p, & \\text { when } y=1 \\\\ 1-p, & \\text { when } y=0 \\end{aligned}\\right.","title":"Focal loss"},{"location":"other_categories/object_detection_2D/GFocalLoss/#quality-focal-loss","text":"\\mathbf{Q} \\mathbf{F} \\mathbf{L}(\\sigma)=-|y-\\sigma|^{\\beta}((1-y) \\log (1-\\sigma)+y \\log (\\sigma)) def quality_focal_loss( pred, # (n, 80) label, # (n) 0, 1-80: 0 is neg, 1-80 is positive score, # (n) reg target 0-1, only positive is good weight=None, beta=2.0, reduction='mean', avg_factor=None): \"\"\" from https://github.com/implus/GFocal/blob/cc0e72680f16a8abe0770eb531d6baa07a6e511f/mmdet/models/losses/gfocal_loss.py \"\"\" # all goes to 0 pred_sigmoid = pred.sigmoid() pt = pred_sigmoid zerolabel = pt.new_zeros(pred.shape) loss = F.binary_cross_entropy_with_logits( pred, zerolabel, reduction='none') * pt.pow(beta) label = label - 1 pos = (label >= 0).nonzero().squeeze(1) a = pos b = label[pos].long() # positive goes to bbox quality pt = score[a] - pred_sigmoid[a, b] loss[a,b] = F.binary_cross_entropy_with_logits( pred[a,b], score[a], reduction='none') * pt.pow(beta) loss = weight_reduce_loss(loss, weight, reduction, avg_factor) return loss","title":"Quality Focal Loss"},{"location":"other_categories/object_detection_2D/GFocalLoss/#distribution-focal-loss-dfl","text":"\u5f53\u6211\u4eec\u4f7f\u7528\u5e8f\u5217\u7684\u591a\u4e2a\u5206\u7c7b\u503c(multi-bin)\u5206\u7c7b\u65f6\uff0cinference\u7684\u65f6\u5019\u6211\u4eec\u4f7f\u7528 \\hat{y}=\\sum_{i=0}^{n} P\\left(y_{i}\\right) y_{i} \u3002 \\mathbf{D F L}\\left(\\mathcal{S}_{i}, \\mathcal{S}_{i+1}\\right)=-\\left(\\left(y_{i+1}-y\\right) \\log \\left(\\mathcal{S}_{i}\\right)+\\left(y-y_{i}\\right) \\log \\left(\\mathcal{S}_{i+1}\\right)\\right) \u5176\u4e2d \\mathcal{S}_{i}=\\frac{y_{i+1}-y}{y_{i+1}-y_{i}}, \\mathcal{S}_{i+1}=\\frac{y-y_{i}}{y_{i+1}-y_{i}} ,\u76f4\u89c9\u5c31\u662f\u6309\u7167\u6743\u91cd\u4f7f\u7528loss\u9f13\u52b1multibin ground truth\u4e34\u8fd1\u7684\u4e24\u4fa7\u5206\u7c7b\u70b9\u7684\u6743\u91cd\u3002 def distribution_focal_loss( pred, label, weight=None, reduction='mean', avg_factor=None): \"\"\" from https://github.com/implus/GFocal/blob/cc0e72680f16a8abe0770eb531d6baa07a6e511f/mmdet/models/losses/gfocal_loss.py \"\"\" disl = label.long() disr = disl + 1 wl = disr.float() - label wr = label - disl.float() loss = F.cross_entropy(pred, disl, reduction='none') * wl \\ + F.cross_entropy(pred, disr, reduction='none') * wr loss = weight_reduce_loss(loss, weight, reduction, avg_factor) return loss","title":"Distribution Focal Loss (DFL)"},{"location":"other_categories/object_detection_2D/GFocalLoss/#generalized-focal-loss-gfl","text":"\\mathbf{G F L}\\left(p_{y_{l}}, p_{y_{r}}\\right)=-\\left|y-\\left(y_{l} p_{y_{l}}+y_{r} p_{y_{r}}\\right)\\right|^{\\beta}\\left(\\left(y_{r}-y\\right) \\log \\left(p_{y_{l}}\\right)+\\left(y-y_{l}\\right) \\log \\left(p_{y_{r}}\\right)\\right) function focal_loss_y(x, gamma){ return - Math.pow(1-x, gamma) * Math.log(x) } function quality_focal_loss_y(x, y){ return - Math.pow(y-x, 2) * ((1-y) * Math.log(1-x) + y * Math.log(x)) } function get_focal_loss_list(p, gamma){ focal = [] for (j = 0; j < 98;j++){ focal.push(focal_loss_y(p[j], gamma)) } return focal } function get_quality_focal_loss_list(p, y){ focal = [] for (j = 0; j < 98;j++){ focal.push(quality_focal_loss_y(p[j], y)) } return focal } focalLoss = document.getElementById('focalLoss'); var p = []; for (i = 1; i < 99;i++){ p.push(i * 0.01); } var focal = get_focal_loss_list(p, 0.2) slider_steps = [] for (i = 0.2; i < 4; i += 0.2){ slider_steps.push( { method: 'animate', label: Math.floor(i * 100) /100, args: [ { data: [{ x:p, y: get_focal_loss_list(p, i)}], }, { transition: {duration: 20}, frame: {duration: 20, redraw: false}, } ] } ) } var qfocal = get_quality_focal_loss_list(p, 0.5) qslider_steps = [] for (i = 0.02; i < 0.98; i += 0.02){ qslider_steps.push( { method: 'animate', label: Math.floor(i * 100) /100, args: [ { data: [{ x:p, y: get_quality_focal_loss_list(p, i)}], }, { transition: {duration: 20}, frame: {duration: 20, redraw: false}, } ] } ) } Plotly.plot(focalLoss, [{ x: p, y: focal, }], { title: 'Focal Loss for positive samples', xaxis: { title: 'p' }, yaxis: { title: 'loss' }, sliders: [{ pad: {t: 30}, currentvalue: { xanchor: 'right', prefix: 'gamma: ', font: { color: '#888', size: 20 } }, steps: slider_steps }] }); Plotly.plot(qfocalLoss, [{ x: p, y: qfocal, }], { title: 'Quality Focal Loss with beta=2', xaxis: { title: 'p' }, yaxis: { title: 'loss' }, sliders: [{ pad: {t: 30}, currentvalue: { xanchor: 'right', prefix: 'gt_y: ', font: { color: '#888', size: 20 } }, steps: qslider_steps }] });","title":"Generalized Focal Loss (GFL)"},{"location":"other_categories/object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/","text":"Gaussian YOLOv3: An Accurate and Fast Object Detector Using Localization Uncertainty for Autonomous Driving \u8fd9\u7bc7\u8bba\u6587\u5f15\u5165\u4e86Gaussian Yolov3,\u5728inference\u4e2d\u9884\u6d4b\u5b9a\u4f4d\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u51cf\u5c11\u4e86False Positive,\u4ee5\u63d0\u5347\u70b9\u6570\u3002\u8fd9\u4e2a\u8bba\u6587\u7684\u4e3b\u8981\u601d\u8def\u8d21\u732e\u5728\u4e8e\uff0c\u5728inference\u7684\u65f6\u5019\u901a\u8fc7\u5bf9\u8fb9\u6846\u7684\u4e0d\u786e\u5b9a\u6027\u7684\u4f30\u8ba1\u4fee\u6b63inference\u65f6\u7684\u4f30\u8ba1\u3002\u4f7f\u5f97NMS\u65f6\u4f1a\u771f\u6b63\u9009\u62e9\u5bf9\u4f4d\u7f6e\u786e\u5b9a\u6027\u6700\u9ad8\u7684\u6846\u3002 Gaussian Yolov3 \u7ed3\u6784 Yolov3\u56de\u5f52\u8f93\u51fa\u7684bbox\u7ed3\u6784\u5305\u62ec t_x, t_y, t_w, t_h ,\u8fd9\u6837\u53ef\u4ee5\u7528\u9ad8\u65af\u6a21\u578b\u6765\u4f30\u8ba1\u3002\u672c\u6587\u7684\u7b97\u6cd5\u8f93\u5165\u5982\u56fe \u8f93\u51fa\u7ed3\u6784 \\begin{aligned} \\mu_{t_{x}}=\\sigma\\left(\\hat{\\mu}_{t_{x}}\\right), \\mu_{t_{y}} &=\\sigma\\left(\\hat{\\mu}_{t_{y}}\\right), \\mu_{t_{w}}=\\hat{\\mu}_{t_{w}}, \\mu_{t_{h}}=\\hat{\\mu}_{t_{h}} \\\\ \\Sigma_{t_{x}} &=\\sigma\\left(\\hat{\\Sigma}_{t_{x}}\\right), \\Sigma_{t_{y}}=\\sigma\\left(\\hat{\\Sigma}_{t_{y}}\\right) \\\\ \\Sigma_{t_{w}} &=\\sigma\\left(\\hat{\\Sigma}_{t_{w}}\\right), \\Sigma_{t_{h}}=\\sigma\\left(\\hat{\\Sigma}_{t_{h}}\\right) \\\\ \\sigma(x) &=\\frac{1}{(1+\\exp (-x))} \\end{aligned} \u635f\u5931\u51fd\u6570 \u8fd9\u91cc\u7528negative log likelihood\u635f\u5931 \\begin{aligned} L_x = - \\sum_{i=1}^W \\sum_{j=1}^H \\sum_{k=1}^K \\gamma_{ijk}log(N&(x^G_{jik}|\\mu_{t_x}(x_{ijk}))),\\\\ &\\sum_{t_x}(x_{ijk})) + \\epsilon) \\end{aligned} \u5176\u4e2dW,H,\u4e3agrids\u8f93\u51fa,K\u662fanchors\u7684\u6570\u91cf. inference \u5f53\u8f93\u51fa\u7684\u65f6\u5019,inference\u65f6\u5c06\u5404\u4e2aindex\u7684\u4e0d\u786e\u5b9a\u6027\u53d6\u5747\u503c\uff0c\u8fd9\u6837\u4f1a\u5bf9\u7c7b\u522b\u7684class score,\u4f1a\u5f62\u6210\u5f71\u54cd. C r .=\\sigma(\\text {Object}) \\times \\sigma\\left(\\text {Class}_{i}\\right) \\times\\left(1-\\text {Uncertainty}_{\\text {aver}}\\right)","title":"Gaussian YOLOv3: An Accurate and Fast Object Detector Using Localization Uncertainty for Autonomous Driving"},{"location":"other_categories/object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/#gaussian-yolov3-an-accurate-and-fast-object-detector-using-localization-uncertainty-for-autonomous-driving","text":"\u8fd9\u7bc7\u8bba\u6587\u5f15\u5165\u4e86Gaussian Yolov3,\u5728inference\u4e2d\u9884\u6d4b\u5b9a\u4f4d\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u51cf\u5c11\u4e86False Positive,\u4ee5\u63d0\u5347\u70b9\u6570\u3002\u8fd9\u4e2a\u8bba\u6587\u7684\u4e3b\u8981\u601d\u8def\u8d21\u732e\u5728\u4e8e\uff0c\u5728inference\u7684\u65f6\u5019\u901a\u8fc7\u5bf9\u8fb9\u6846\u7684\u4e0d\u786e\u5b9a\u6027\u7684\u4f30\u8ba1\u4fee\u6b63inference\u65f6\u7684\u4f30\u8ba1\u3002\u4f7f\u5f97NMS\u65f6\u4f1a\u771f\u6b63\u9009\u62e9\u5bf9\u4f4d\u7f6e\u786e\u5b9a\u6027\u6700\u9ad8\u7684\u6846\u3002","title":"Gaussian YOLOv3: An Accurate and Fast Object Detector Using Localization Uncertainty for Autonomous Driving"},{"location":"other_categories/object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/#gaussian-yolov3","text":"Yolov3\u56de\u5f52\u8f93\u51fa\u7684bbox\u7ed3\u6784\u5305\u62ec t_x, t_y, t_w, t_h ,\u8fd9\u6837\u53ef\u4ee5\u7528\u9ad8\u65af\u6a21\u578b\u6765\u4f30\u8ba1\u3002\u672c\u6587\u7684\u7b97\u6cd5\u8f93\u5165\u5982\u56fe","title":"Gaussian Yolov3 \u7ed3\u6784"},{"location":"other_categories/object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/#_1","text":"\\begin{aligned} \\mu_{t_{x}}=\\sigma\\left(\\hat{\\mu}_{t_{x}}\\right), \\mu_{t_{y}} &=\\sigma\\left(\\hat{\\mu}_{t_{y}}\\right), \\mu_{t_{w}}=\\hat{\\mu}_{t_{w}}, \\mu_{t_{h}}=\\hat{\\mu}_{t_{h}} \\\\ \\Sigma_{t_{x}} &=\\sigma\\left(\\hat{\\Sigma}_{t_{x}}\\right), \\Sigma_{t_{y}}=\\sigma\\left(\\hat{\\Sigma}_{t_{y}}\\right) \\\\ \\Sigma_{t_{w}} &=\\sigma\\left(\\hat{\\Sigma}_{t_{w}}\\right), \\Sigma_{t_{h}}=\\sigma\\left(\\hat{\\Sigma}_{t_{h}}\\right) \\\\ \\sigma(x) &=\\frac{1}{(1+\\exp (-x))} \\end{aligned}","title":"\u8f93\u51fa\u7ed3\u6784"},{"location":"other_categories/object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/#_2","text":"\u8fd9\u91cc\u7528negative log likelihood\u635f\u5931 \\begin{aligned} L_x = - \\sum_{i=1}^W \\sum_{j=1}^H \\sum_{k=1}^K \\gamma_{ijk}log(N&(x^G_{jik}|\\mu_{t_x}(x_{ijk}))),\\\\ &\\sum_{t_x}(x_{ijk})) + \\epsilon) \\end{aligned} \u5176\u4e2dW,H,\u4e3agrids\u8f93\u51fa,K\u662fanchors\u7684\u6570\u91cf.","title":"\u635f\u5931\u51fd\u6570"},{"location":"other_categories/object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/#inference","text":"\u5f53\u8f93\u51fa\u7684\u65f6\u5019,inference\u65f6\u5c06\u5404\u4e2aindex\u7684\u4e0d\u786e\u5b9a\u6027\u53d6\u5747\u503c\uff0c\u8fd9\u6837\u4f1a\u5bf9\u7c7b\u522b\u7684class score,\u4f1a\u5f62\u6210\u5f71\u54cd. C r .=\\sigma(\\text {Object}) \\times \\sigma\\left(\\text {Class}_{i}\\right) \\times\\left(1-\\text {Uncertainty}_{\\text {aver}}\\right)","title":"inference"},{"location":"other_categories/object_detection_2D/IoU-uniform_R-CNN/","text":"IoU-uniform R-CNN: Breaking Through the Limitations of RPN \u8fd9\u7bc7\u8bba\u6587\u662f\u4e00\u7cfb\u5217\u9488\u5bf9NMS\u6216\u8005region proposal\u8bad\u7ec3\u63d0\u5347\u7684\u8bba\u6587\u4e4b\u4e00\uff0c\u5176\u4e2d\u6d89\u53ca\u4e86\u591a\u4e00\u4e2a\u6709\u610f\u4e49\u7684\u6982\u5ff5\u4e0e\u601d\u8def\u3002\u672c\u6587\u6838\u5fc3\u8d21\u732e\u662f\u5728RCNN(pooling \u6216 align\u540e\u7684\u7f51\u7edc)\u8bad\u7ec3\u8fc7\u7a0b\u4e2d. RPN \u4e0e rCNN \u4e24\u5927\u95ee\u9898 \u7b2c\u4e00\uff0c\u5728\u8bad\u7ec3\u7684\u65f6\u5019\uff0c\u76f8\u5f53\u90e8\u5206\u7684\u63d0\u4f9b\u7ed9RCNN\u8bad\u7ec3\u7528\u7684\u6b63\u6837\u672c\u90fd\u662f\u6765\u81ea\u4e0eground truth\u7684proposal\uff0c\u4f46\u662f\u6d4b\u8bd5\u63a8\u7406\u7684\u65f6\u5019\uff0c\u6211\u4eec\u8981\u6c42RCNN\u7684\u8f93\u5165\u662fRPN\u7ed9\u51fa\u7684proposal\uff0c\u4e0eGT\u76f8\u6bd4\u4f1a\u6709\u4e00\u5b9a\u7684\u8bef\u5dee\uff0c\u8fd9\u5176\u4e2d\u7684misalignment\u5c31\u4f1a\u4f7f\u5f97RCNN\u5728test\u7684\u65f6\u5019\u6027\u80fd\u53d8\u5dee\uff0c\u8bad\u7ec3\u7684\u65f6\u5019\u4e5f\u5bb9\u6613\u8fc7\u62df\u5408\u3002 \u7b2c\u4e8c\uff0cNMS supress\u4e86\u5206\u7c7b\u4efb\u52a1\u4e2d\u786e\u5b9a\u5ea6\u4e0d\u662f\u6700\u9ad8\u7684\u6846\uff0c\u4f46\u662f\u8fd9\u5b8c\u5168\u53ef\u80fd\u538b\u5236\u4e86\u4e8b\u5b9e\u4e0a\u5b9a\u4f4d\u6700\u51c6\u786e\u7684\u5206\u7c7b\u6846\uff0c\u56e0\u800c IoU-Net.pdf ,\u8981\u6c42\u795e\u7ecf\u7f51\u7edc\u8f93\u51fa\u5f53\u524d\u6846\u7684IoU\u4f30\u8ba1\uff0cNMS\u4ee5\u8fd9\u4e2aIoU\u4f30\u8ba1\u4e3a\u57fa\u51c6.\u503c\u5f97\u6ce8\u610f\u7684\u662f gaussian Yolov3(\u7b80\u4ecb) ,\u540c\u6837\u4f7f\u7528\u5b9a\u4f4d\u4e0d\u786e\u5b9a\u5ea6\u6765\u63a7\u5236NMS\uff0c\u53ea\u4e0d\u8fc7\u5b83\u8981\u6c42\u6bcf\u4e00\u5b9a\u4f4d\u7ef4\u5ea6\u7684\u4e0d\u786e\u5b9a\u5ea6\u7684\u603b\u5408\uff0c\u800cIoU-Net\u8981\u6c42\u7f51\u7edc\u76f4\u63a5\u8f93\u51faIoU\u4f30\u8ba1. IoU-uniform sampling \u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u7684\u60f3\u6cd5\u662f\u5982\u4e0a\u56fe\uff0c\u5728\u7ed9RCNN\u63d0\u4f9b\u6b63\u6837\u672c\u65f6\uff0c\u968f\u673a\u6270\u52a8ground truth\u7684ROI\uff0c\u4ee5\u6b64\u5f97\u5230\u591a\u4e2a\u5e26\u6709\u566a\u58f0\u6b63\u6837\u672c\u7528\u4f5ctraining\u3002\u4f5c\u8005\u8ba4\u4e3a\u8fd9\u6837\u53ef\u4ee5\u4f7f\u5f97training phase\u4e0einference phase\u4e2dRCNN\u7684\u8f93\u5165\u5206\u5e03\u66f4\u76f8\u8fd1\u3002 \u5728NMS\u7684\u65f6\u5019\u4f9d\u636e\u4e3aIoU\u4f30\u8ba1\u503c\u4e0eprobs\u7684\u4e58\u79ef\u3002","title":"IoU-uniform R-CNN: Breaking Through the Limitations of RPN"},{"location":"other_categories/object_detection_2D/IoU-uniform_R-CNN/#iou-uniform-r-cnn-breaking-through-the-limitations-of-rpn","text":"\u8fd9\u7bc7\u8bba\u6587\u662f\u4e00\u7cfb\u5217\u9488\u5bf9NMS\u6216\u8005region proposal\u8bad\u7ec3\u63d0\u5347\u7684\u8bba\u6587\u4e4b\u4e00\uff0c\u5176\u4e2d\u6d89\u53ca\u4e86\u591a\u4e00\u4e2a\u6709\u610f\u4e49\u7684\u6982\u5ff5\u4e0e\u601d\u8def\u3002\u672c\u6587\u6838\u5fc3\u8d21\u732e\u662f\u5728RCNN(pooling \u6216 align\u540e\u7684\u7f51\u7edc)\u8bad\u7ec3\u8fc7\u7a0b\u4e2d.","title":"IoU-uniform R-CNN: Breaking Through the Limitations of RPN"},{"location":"other_categories/object_detection_2D/IoU-uniform_R-CNN/#rpn-rcnn","text":"\u7b2c\u4e00\uff0c\u5728\u8bad\u7ec3\u7684\u65f6\u5019\uff0c\u76f8\u5f53\u90e8\u5206\u7684\u63d0\u4f9b\u7ed9RCNN\u8bad\u7ec3\u7528\u7684\u6b63\u6837\u672c\u90fd\u662f\u6765\u81ea\u4e0eground truth\u7684proposal\uff0c\u4f46\u662f\u6d4b\u8bd5\u63a8\u7406\u7684\u65f6\u5019\uff0c\u6211\u4eec\u8981\u6c42RCNN\u7684\u8f93\u5165\u662fRPN\u7ed9\u51fa\u7684proposal\uff0c\u4e0eGT\u76f8\u6bd4\u4f1a\u6709\u4e00\u5b9a\u7684\u8bef\u5dee\uff0c\u8fd9\u5176\u4e2d\u7684misalignment\u5c31\u4f1a\u4f7f\u5f97RCNN\u5728test\u7684\u65f6\u5019\u6027\u80fd\u53d8\u5dee\uff0c\u8bad\u7ec3\u7684\u65f6\u5019\u4e5f\u5bb9\u6613\u8fc7\u62df\u5408\u3002 \u7b2c\u4e8c\uff0cNMS supress\u4e86\u5206\u7c7b\u4efb\u52a1\u4e2d\u786e\u5b9a\u5ea6\u4e0d\u662f\u6700\u9ad8\u7684\u6846\uff0c\u4f46\u662f\u8fd9\u5b8c\u5168\u53ef\u80fd\u538b\u5236\u4e86\u4e8b\u5b9e\u4e0a\u5b9a\u4f4d\u6700\u51c6\u786e\u7684\u5206\u7c7b\u6846\uff0c\u56e0\u800c IoU-Net.pdf ,\u8981\u6c42\u795e\u7ecf\u7f51\u7edc\u8f93\u51fa\u5f53\u524d\u6846\u7684IoU\u4f30\u8ba1\uff0cNMS\u4ee5\u8fd9\u4e2aIoU\u4f30\u8ba1\u4e3a\u57fa\u51c6.\u503c\u5f97\u6ce8\u610f\u7684\u662f gaussian Yolov3(\u7b80\u4ecb) ,\u540c\u6837\u4f7f\u7528\u5b9a\u4f4d\u4e0d\u786e\u5b9a\u5ea6\u6765\u63a7\u5236NMS\uff0c\u53ea\u4e0d\u8fc7\u5b83\u8981\u6c42\u6bcf\u4e00\u5b9a\u4f4d\u7ef4\u5ea6\u7684\u4e0d\u786e\u5b9a\u5ea6\u7684\u603b\u5408\uff0c\u800cIoU-Net\u8981\u6c42\u7f51\u7edc\u76f4\u63a5\u8f93\u51faIoU\u4f30\u8ba1.","title":"RPN \u4e0e rCNN \u4e24\u5927\u95ee\u9898"},{"location":"other_categories/object_detection_2D/IoU-uniform_R-CNN/#iou-uniform-sampling","text":"\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u7684\u60f3\u6cd5\u662f\u5982\u4e0a\u56fe\uff0c\u5728\u7ed9RCNN\u63d0\u4f9b\u6b63\u6837\u672c\u65f6\uff0c\u968f\u673a\u6270\u52a8ground truth\u7684ROI\uff0c\u4ee5\u6b64\u5f97\u5230\u591a\u4e2a\u5e26\u6709\u566a\u58f0\u6b63\u6837\u672c\u7528\u4f5ctraining\u3002\u4f5c\u8005\u8ba4\u4e3a\u8fd9\u6837\u53ef\u4ee5\u4f7f\u5f97training phase\u4e0einference phase\u4e2dRCNN\u7684\u8f93\u5165\u5206\u5e03\u66f4\u76f8\u8fd1\u3002 \u5728NMS\u7684\u65f6\u5019\u4f9d\u636e\u4e3aIoU\u4f30\u8ba1\u503c\u4e0eprobs\u7684\u4e58\u79ef\u3002","title":"IoU-uniform sampling"},{"location":"other_categories/object_detection_2D/IoUNet%28s%29/","text":"IoU Nets \u672c\u6587\u4f1a\u5c1d\u8bd5\u540c\u65f6\u878d\u5408\u4e24\u7bc7idea\u76f8\u4f3c\u7684\u8bba\u6587\u7684\u60f3\u6cd5\uff0c\u7b2c\u4e00\u4e3a Acquisition of Localization Confidence for Accurate Object Detection \u3002\u7b2c\u4e8c\u7bc7\u4e3a IoU-aware Single-stage Object Detector for Accurate Localization \u5176\u4e2d\u7b2c\u4e00\u7bc7\u8bba\u6587\u7531 Iou-uniform-rcnn \u63d0\u53ca\uff0c\u4e24\u7bc7\u8bba\u6587\u7684\u6838\u5fc3idea\u90fd\u662f\u6d89\u53ca\u5728NMS\u4e2d\u4f7f\u7528IoU\u9884\u6d4b\u503c\u66ff\u4ee3classification score Acquisition of Localization Confidence for Accurate Object Detection \u8fd9\u7bc7\u8bba\u6587\u7684\u5185\u5bb9\u6bd4\u8f83\u591a\uff0c\u9996\u5148\u662fIoU-guided NMS,\u5176\u6b21\u662fIoU-guided optimization,\u6700\u540e\u662fPrROI pooling. \u7b2c\u4e00\u90e8\u5206\u662f\u5728RoI\u540e\u589e\u52a0\u4e00\u4e2a\u5206\u652f\u9884\u6d4bIoU\u503c,\u5728NMS\u65f6\u4ee5IoU\u66ff\u4ee3\u5206\u7c7b\u5206\u6570 IoU-guided optimization \u8fd9\u91cc\u505a\u7684\u662f\u5728\u63d0\u4f9bproposal\u7ed9RCNN\u63d0\u4f9b\u8f93\u5165\u65f6\uff0c\u7528\u68af\u5ea6\u4f18\u5316\u5fae\u8c03FPN\u8f93\u51fa\u7684bounding box\uff0c\u76ee\u6807\u51fd\u6570\u662fRCNN\u8f93\u51fa\u7684IoU,\u76f4\u89c9\u610f\u601d\u662f\u5fae\u8c03FPN\u7684bounding box\uff0c\u6700\u4f18\u5316RCNN\u9884\u6d4b\u7684IoU\u3002\u5728IoU\u9884\u6d4b\u51c6\u786e\u7684\u524d\u63d0\u4e0b\uff0c\u8fd9\u6837\u53ef\u4ee5\u4f18\u5316FPN\u7684proposal\u3002 PrROI pooling PrROI\u5df2\u7ecf \u5f00\u6e90 \u8fd9\u4e2a\u79f0\u4e3aPrecise RoI Pooling \\begin{aligned} f(x, y) &= \\sum_{i,j}IC(x,y,i,j) \\times w_{i,j} \\\\ IC(x,y,i,j) &= max(0, 1-|x-i|) \\times max(0, 1-|y-j|) \\\\ \\end{aligned} \\operatorname{PrPool}(b i n, \\mathcal{F})=\\frac{\\int_{y 1}^{y 2} \\int_{x 1}^{x 2} f(x, y) d x d y}{\\left(x_{2}-x_{1}\\right) \\times\\left(y_{2}-y_{1}\\right)} IoU-aware Single-stage Object Detector for Accurate Localization \u8fd9\u7bc7\u7a0d\u5fae\u7b80\u5355\u5c06IoU NMS\u7528\u5728one-stage\u7684retina net\u4e2d\u3002\u5982\u56fe \u635f\u5931\u51fd\u6570\u5982\u4e0b \\begin{aligned} L_{c l s}=& \\frac{1}{N_{P o s}}\\left(\\sum_{i \\in P o s}^{N} \\mathrm{FL}\\left(p_{i}, \\hat{p}_{i}\\right)+\\sum_{i \\in N e g}^{M} \\mathrm{FL}\\left(p_{i}, \\hat{p}_{i}\\right)\\right) \\\\ L_{l o c}=& \\frac{1}{N_{P o s}} \\sum_{i \\in P o s}^{N} \\sum_{m \\in c x, c y, w, h} \\mathrm{smooth}_{\\mathrm{L} 1}\\left(l_{i}^{m}-\\hat{g}_{i}^{m}\\right) \\\\ & L_{I o U}=\\frac{1}{N_{P o s}} \\sum_{i \\in P o s}^{N} \\mathrm{CE}\\left(I o U_{i}, I \\hat{o} U_{i}\\right) \\\\ & L_{t o t a l}=L_{c l s}+L_{l o c}+L_{I o U} \\end{aligned} \u4e3b\u8981\u5728\u4e8eIoU\u7684\u8bad\u7ec3\u65b9\u5f0f\u7528\u7684\u662fBinary Cross Entropy(\u5e94\u5f53\u5047\u8bbe\u539fIoU\u4e3asigmoid)","title":"IoU Nets"},{"location":"other_categories/object_detection_2D/IoUNet%28s%29/#iou-nets","text":"\u672c\u6587\u4f1a\u5c1d\u8bd5\u540c\u65f6\u878d\u5408\u4e24\u7bc7idea\u76f8\u4f3c\u7684\u8bba\u6587\u7684\u60f3\u6cd5\uff0c\u7b2c\u4e00\u4e3a Acquisition of Localization Confidence for Accurate Object Detection \u3002\u7b2c\u4e8c\u7bc7\u4e3a IoU-aware Single-stage Object Detector for Accurate Localization \u5176\u4e2d\u7b2c\u4e00\u7bc7\u8bba\u6587\u7531 Iou-uniform-rcnn \u63d0\u53ca\uff0c\u4e24\u7bc7\u8bba\u6587\u7684\u6838\u5fc3idea\u90fd\u662f\u6d89\u53ca\u5728NMS\u4e2d\u4f7f\u7528IoU\u9884\u6d4b\u503c\u66ff\u4ee3classification score","title":"IoU Nets"},{"location":"other_categories/object_detection_2D/IoUNet%28s%29/#acquisition-of-localization-confidence-for-accurate-object-detection","text":"\u8fd9\u7bc7\u8bba\u6587\u7684\u5185\u5bb9\u6bd4\u8f83\u591a\uff0c\u9996\u5148\u662fIoU-guided NMS,\u5176\u6b21\u662fIoU-guided optimization,\u6700\u540e\u662fPrROI pooling. \u7b2c\u4e00\u90e8\u5206\u662f\u5728RoI\u540e\u589e\u52a0\u4e00\u4e2a\u5206\u652f\u9884\u6d4bIoU\u503c,\u5728NMS\u65f6\u4ee5IoU\u66ff\u4ee3\u5206\u7c7b\u5206\u6570","title":"Acquisition of Localization Confidence for Accurate Object Detection"},{"location":"other_categories/object_detection_2D/IoUNet%28s%29/#iou-guided-optimization","text":"\u8fd9\u91cc\u505a\u7684\u662f\u5728\u63d0\u4f9bproposal\u7ed9RCNN\u63d0\u4f9b\u8f93\u5165\u65f6\uff0c\u7528\u68af\u5ea6\u4f18\u5316\u5fae\u8c03FPN\u8f93\u51fa\u7684bounding box\uff0c\u76ee\u6807\u51fd\u6570\u662fRCNN\u8f93\u51fa\u7684IoU,\u76f4\u89c9\u610f\u601d\u662f\u5fae\u8c03FPN\u7684bounding box\uff0c\u6700\u4f18\u5316RCNN\u9884\u6d4b\u7684IoU\u3002\u5728IoU\u9884\u6d4b\u51c6\u786e\u7684\u524d\u63d0\u4e0b\uff0c\u8fd9\u6837\u53ef\u4ee5\u4f18\u5316FPN\u7684proposal\u3002","title":"IoU-guided optimization"},{"location":"other_categories/object_detection_2D/IoUNet%28s%29/#prroi-pooling","text":"PrROI\u5df2\u7ecf \u5f00\u6e90 \u8fd9\u4e2a\u79f0\u4e3aPrecise RoI Pooling \\begin{aligned} f(x, y) &= \\sum_{i,j}IC(x,y,i,j) \\times w_{i,j} \\\\ IC(x,y,i,j) &= max(0, 1-|x-i|) \\times max(0, 1-|y-j|) \\\\ \\end{aligned} \\operatorname{PrPool}(b i n, \\mathcal{F})=\\frac{\\int_{y 1}^{y 2} \\int_{x 1}^{x 2} f(x, y) d x d y}{\\left(x_{2}-x_{1}\\right) \\times\\left(y_{2}-y_{1}\\right)}","title":"PrROI pooling"},{"location":"other_categories/object_detection_2D/IoUNet%28s%29/#iou-aware-single-stage-object-detector-for-accurate-localization","text":"\u8fd9\u7bc7\u7a0d\u5fae\u7b80\u5355\u5c06IoU NMS\u7528\u5728one-stage\u7684retina net\u4e2d\u3002\u5982\u56fe \u635f\u5931\u51fd\u6570\u5982\u4e0b \\begin{aligned} L_{c l s}=& \\frac{1}{N_{P o s}}\\left(\\sum_{i \\in P o s}^{N} \\mathrm{FL}\\left(p_{i}, \\hat{p}_{i}\\right)+\\sum_{i \\in N e g}^{M} \\mathrm{FL}\\left(p_{i}, \\hat{p}_{i}\\right)\\right) \\\\ L_{l o c}=& \\frac{1}{N_{P o s}} \\sum_{i \\in P o s}^{N} \\sum_{m \\in c x, c y, w, h} \\mathrm{smooth}_{\\mathrm{L} 1}\\left(l_{i}^{m}-\\hat{g}_{i}^{m}\\right) \\\\ & L_{I o U}=\\frac{1}{N_{P o s}} \\sum_{i \\in P o s}^{N} \\mathrm{CE}\\left(I o U_{i}, I \\hat{o} U_{i}\\right) \\\\ & L_{t o t a l}=L_{c l s}+L_{l o c}+L_{I o U} \\end{aligned} \u4e3b\u8981\u5728\u4e8eIoU\u7684\u8bad\u7ec3\u65b9\u5f0f\u7528\u7684\u662fBinary Cross Entropy(\u5e94\u5f53\u5047\u8bbe\u539fIoU\u4e3asigmoid)","title":"IoU-aware Single-stage Object Detector for Accurate Localization"},{"location":"other_categories/object_detection_2D/MMDetection/","text":"Some Collections around MMDetection \u8fd9\u4e00\u9875\u9762\u4e3b\u8981\u4e3a\u4e86\u6536\u96c6mmdetection\u4e2d\u63d0\u4f9b\u5b9e\u73b0\u7684\u8bba\u6587\u3002\u8fd9\u91cc\u6536\u96c6\u6216\u8005\u63d0\u4f9b\u94fe\u63a5\u7684\u4e3b\u8981\u662f\u76f8\u5bf9\u51b7\u95e8\u7684paper\uff0c\u4e3b\u6d41\u7684\u5982Faster-RCNN\u4ee5\u53caRetinanet\u4e0d\u4f1a\u518d\u91cd\u590d\u3002 \u5176\u4f59\u6709\u5b9e\u73b0\uff0c\u5e76\u8bb0\u5f55\u5728\u672c\u7f51\u7ad9\u5176\u4ed6\u5730\u65b9\u7684\u6709 FCOS , FreeAnchor , ATSS , RepPoints Update: 2020/06/04: Updates HTC , DetectoRS Single Stage Methods GHM: Gradient Harmonized Single-stage Detector pdf \u8fd9\u7bc7paper\u4e3b\u8981idea\u662floss function\u5e94\u8be5\u5e73\u8861\u4e0d\u540c\u6837\u672c\u4e4b\u95f4\u7684gradient norm. \u8fc7\u4e8e\u56f0\u96be\u7684instance gradient norm\u8f83\u5927\u800c\u8fc7\u4e8e\u7b80\u5355\u7684\u7684instance gradient\u7406\u5e94\u4f1a\u5f88\u5c0f\uff0c \u4e00\u4e2awell trained detector\u7684gradient norm\u5206\u5e03\u5982\u56fe \u4f5c\u8005\u7684\u60f3\u6cd5\u662f\u5e94\u8be5\u63d0\u5347\u4e2d\u95f4\u5c42\uff0c\u6216\u8005\u8bf4gradient\u5bc6\u5ea6\u6bd4\u8f83\u5c0f\u7684\u90e8\u5206\u7684\u68af\u5ea6\u8d21\u732e\u3002 \u5b9a\u4e49\u68af\u5ea6\u5bc6\u5ea6\u51fd\u6570: G D(g)=\\frac{1}{l_{\\epsilon}(g)} \\sum_{k=1}^{N} \\delta_{\\epsilon}\\left(g_{k}, g\\right) \\begin{aligned} &\\delta_{\\epsilon}(x, y)=\\left\\{\\begin{array}{ll} 1 & \\text { if } y-\\frac{\\epsilon}{2}<=x<y+\\frac{\\epsilon}{2} \\\\ 0 & \\text { otherwise } \\end{array}\\right.\\\\ &l_{\\epsilon}(g)=\\min \\left(g+\\frac{\\epsilon}{2}, 1\\right)-\\max \\left(g-\\frac{\\epsilon}{2}, 0\\right) \\end{aligned} \u7b80\u5355\u800c\u8a00\u5c31\u662f GD(g) \u4e3a\u4e0e\u68af\u5ea6g\u4e34\u8fd1\u7684\u533a\u95f4\u5185\uff0c\u6709\u76f8\u8fd1\u68af\u5ea6norm\u7684example\u7684\u4e2a\u6570/\u68af\u5ea6\u533a\u95f4\u957f\u5ea6\u3002 \\beta_{i}=\\frac{N}{G D\\left(g_{i}\\right)} \\begin{aligned} L_{G H M-C} &=\\frac{1}{N} \\sum_{i=1}^{N} \\beta_{i} L_{C E}\\left(p_{i}, p_{i}^{*}\\right) \\\\ &=\\sum_{i=1}^{N} \\frac{L_{C E}\\left(p_{i}, p_{i}^{*}\\right)}{G D\\left(g_{i}\\right)} \\end{aligned} \u4e5f\u5373\u662f\u7ed9loss\u52a0\u4e0a\u4e0e\u68af\u5ea6example\u5bc6\u5ea6\u6210\u53cd\u6bd4\u7684\u5bf9\u5e94\u7684\u6743\u91cd\u3002 FSAF: Feature Selective Anchor-Free Module for Single-Shot Object Detection pdf \u8fd9\u7bc7paper\u7684idea\u662f\u8ba9RetinaNet\u540c\u65f6\u7ef4\u62a4\u4e00\u4e2aanchor free\u4e00\u4e2aanchor based\u7684\u5206\u652f\uff0canchor_free\u7684\u5206\u652f\u5728\u6bcf\u4e00\u4e2ascale\u4e0a\u90fd\u4f1a\u53d7\u8bad\u7ec3\u3002 \u4f5c\u8005\u7684online selection\u601d\u8def\u662f\u8ba9\u6bcf\u4e00\u4e2ascale\u4e0a\u7684anchor free\u5206\u652f\u90fd\u9884\u6d4b\u4e00\u6b21\uff0c\u7136\u540e\u5f97\u5230\u5bf9\u5e94\u7684loss\uff0c\u5bfb\u627eanchor free loss\u6700\u5c0f\u7684scale\uff0ctrain\u5bf9\u5e94scale\u7684anchor-based\u5206\u652f\u3002 Inference\u7684\u65f6\u5019\u5219\u8ba9\u6240\u67096\u4e2a\u5206\u652f\u5404\u81ea\u8f93\u51fa\uff0c\u76f4\u63a5merge FoveaBox: Beyond Anchor-based Object Detector pdf code \u8fd9\u7bc7paper\u7684\u601d\u8def\u662f\u53d6Retinanet\u6240\u6709\u4e4b\u7cbe\u534e\uff0c\u6307\u51fa\u5728FPN\u7684multi-scale\u652f\u6301\u4e0b\uff0c\u5df2\u7ecf\u4e0d\u9700\u8981anchor\u4e86\uff0c\u5176\u5b9e\u4e5f\u5c31\u662f\u6bcf\u4e00\u4e2ascale\u6709\u4e00\u4e2aanchor\u5c31\u591f\u4e86\u3002 Grid R-CNN pdf code \u8fd9\u7bc7paper\u4ece\u4eca\u65e5\u7684\u89d2\u5ea6\u6765\u8bf4\u53ef\u4ee5\u7406\u89e3\u4e3aRoIPooling\u540e\u7684 keypointNet \uff0c\u5728grid point\u9009\u62e9\u4e0a\u6709\u70b9\u4e0d\u540c\uff0c\u4f46\u662f\u601d\u8def\u662f\u76f8\u4f3c\u7684\u3002 VarifocalNet: An IoU-aware Dense Object Detector pdf \u8fd9\u7bc7paper\u7684\u4e3b\u8981\u5de5\u4f5c: Star Conv: \u5728\u7b2c\u4e00\u6b21bounding box\u9884\u6d4b\u7684\u57fa\u7840\u4e0a\u591a\u8dd1\u4e00\u4e2adeform conv, offset\u6b63\u662fbounding box\u7684\u5404\u4e2a\u89d2\u843d.(\u8ba9\u5404\u4e2a\u89d2\u843d\u7684feature\u878d\u5408\u5230\u4e2d\u5fc3\u6765\u7684\u601d\u8def), \u540e\u9762\u518d\u8f93\u51fa\u4e00\u4e2a\u6bd4\u4f8b\u4fee\u6b63\u9879. varifocal loss: \\operatorname{VFL}(\\mathrm{p}, \\mathrm{q})=\\left\\{\\begin{array}{ll} -\\mathrm{q}(\\mathrm{q} \\log (\\mathrm{p})+(1-\\mathrm{q}) \\log (1-\\mathrm{p})) & \\mathrm{q}>0 \\\\ -\\alpha \\mathrm{p}^{\\gamma} \\log (1-\\mathrm{p}) & \\mathrm{q}=0 \\end{array}\\right. Two Stage Methods HTC pdf code motivation: 1. instance segmentation, detection\u751a\u81f3pixel-wise segmentation\u4e4b\u95f4\u662f\u4e92\u76f8\u8865\u5145\u7684\u3002\u4f46\u662f\u5355\u7eaf\u7684\u8ba9\u7f51\u7edc\u540c\u65f6\u8bad\u7ec3\u8fd9\u51e0\u4e2a\u4efb\u52a1\u5e76\u4e0d\u8db3\u591f\u3002 2. cascade RCNN\u63d0\u51fa\u4e86bounding box\u7684\u8fed\u4ee3\u4f18\u5316 \u672c\u7bc7paper\u5c31\u63d0\u51fa\u8ba9\u7f51\u7edc\u540c\u65f6\u8fed\u4ee3\u7684\u5b66\u4e60\u8fd9\u4e09\u4e2a\u4efb\u52a1(\u8fed\u4ee3\u4e3b\u8981\u53d1\u751f\u5728bounding box\u548cinstance seg\u91cc\u9762)\uff0c\u5728\u6bcf\u6b21\u8fed\u4ee3\u7684\u65f6\u5019\u901a\u8fc7\u5176\u4ed6\u4efb\u52a1\u5f53\u524d\u7684\u9884\u6d4b\u8fdb\u5ea6\u8fdb\u884c\u4fe1\u606f\u8865\u5145\u3002 \u4fe1\u606f\u6d41\u56fe: \u5c06Mask RCNN\u7684\u9884\u6d4b\u5206\u652f\u653e\u5230cascade RCNN\u4e0a\uff0c\u5f97\u5230(a)\u6d41\u56fe: \\begin{aligned} \\mathbf{x}_{t}^{b o x} &=\\mathcal{P}\\left(\\mathbf{x}, \\mathbf{r}_{t-1}\\right), \\quad \\mathbf{r}_{t}=B_{t}\\left(\\mathbf{x}_{t}^{b o x}\\right) \\\\ \\mathbf{x}_{t}^{\\operatorname{mask}} &=\\mathcal{P}\\left(\\mathbf{x}, \\mathbf{r}_{t-1}\\right), \\quad \\mathbf{m}_{t}=M_{t}\\left(\\mathbf{x}_{t}^{\\operatorname{mask} }\\right) \\end{aligned} \u5176\u4e2d \\mathcal{P} \u662fpooling\u64cd\u4f5c\uff0c B, M \u5206\u522b\u662fbbox_head\u4ee5\u53camask_head. \u8fd9\u79cd\u8bbe\u7f6e\u7684\u4e00\u4e2a\u63d0\u5347\u65b9\u5411\u662fbbox\u4ee5\u53camask\u8fd8\u662f\u5e76\u884c\u7684\u3002\u4f5c\u8005\u8ba9\u6700\u65b0\u7684bbox\u9884\u6d4b\u503c\u7528\u4e8emask head\u7684\u8f93\u51fa\uff0c\u5f97\u5230(b) \\begin{array}{ll} \\mathbf{x}_{t}^{b o x}=\\mathcal{P}\\left(\\mathbf{x}, \\mathbf{r}_{t-1}\\right), & \\mathbf{r}_{t}=B_{t}\\left(\\mathbf{x}_{t}^{b o x}\\right) \\\\ \\mathbf{x}_{t}^{\\operatorname{mask}}=\\mathcal{P}\\left(\\mathbf{x}, \\mathbf{r}_{t}\\right), & \\mathbf{m}_{t}=M_{t}\\left(\\mathbf{x}_{t}^{\\operatorname{mask} }\\right) \\end{array} \u4f5c\u8005\u8fdb\u4e00\u6b65\u8003\u8651\u5c06\u524d\u4e00\u6b21\u8fed\u4ee3\u7684mask\u7684\u4fe1\u606f\u6765\u8865\u5145\u65b0mask\u7684\u4fe1\u606f: \\begin{array}{ll} \\mathbf{x}_{t}^{b o x}=\\mathcal{P}\\left(\\mathbf{x}, \\mathbf{r}_{t-1}\\right), & \\mathbf{r}_{t}=B_{t}\\left(\\mathbf{x}_{t}^{b o x}\\right) \\\\ \\mathbf{x}_{t}^{\\operatorname{mask}}=\\mathcal{P}\\left(\\mathbf{x}, \\mathbf{r}_{t}\\right), & \\mathbf{m}_{t}=M_{t}\\left(\\mathcal{F}\\left(\\mathbf{x}_{t}^{\\operatorname{mask}}, \\mathbf{m}_{t-1}^{-}\\right)\\right) \\end{array} \\mathcal{F}\\left(\\mathbf{x}_{t}^{\\operatorname{mask}}, \\mathbf{m}_{t-1}\\right)=\\mathbf{x}_{t}^{\\operatorname{mask}}+\\mathcal{G}_{t}\\left(\\mathbf{m}_{t-1}^{-}\\right) \u4f5c\u8005\u8fdb\u4e00\u6b65\u8003\u8651\u5c06semantic segmentation\u7684\u4fe1\u606f\u52a0\u8fdb\u6765\uff0c\u5f97\u5230: \\begin{aligned} \\mathbf{x}_{t}^{b o x} &=\\mathcal{P}\\left(\\mathbf{x}, \\mathbf{r}_{t-1}\\right)+\\mathcal{P}\\left(S(\\mathbf{x}), \\mathbf{r}_{t-1}\\right) \\\\ \\mathbf{r}_{t} &=B_{t}\\left(\\mathbf{x}_{t}^{b o x}\\right) \\\\ \\mathbf{x}_{t}^{m a s k} &=\\mathcal{P}\\left(\\mathbf{x}, \\mathbf{r}_{t}\\right)+\\mathcal{P}\\left(S(\\mathbf{x}), \\mathbf{r}_{t}\\right) \\\\ \\mathbf{m}_{t} &=M_{t}\\left(\\mathcal{F}\\left(\\mathbf{x}_{t}^{m a s k}, \\mathbf{m}_{t-1}^{-}\\right)\\right) \\end{aligned} \u5176\u4e2d S(x) \u4e3a\u8bed\u4e49\u5206\u5272\u8f93\u51fa DetectoRS pdf code \u672c\u6587\u5728\u66f4\u65b0\u7684\u65f6\u5019\u8fd8\u6ca1\u6709\u653e\u5230mmdetection\u4e3b\u5206\u652f\u4e0a\uff0c\u662f\u57fa\u4e8emmdetection 1.0\u7248\u672c\u8bbe\u8ba1\u7684\u4ee3\u7801\u3002\u540c\u65f6\u66f4\u6539\u4e86backbone,\u4ee3\u7801\u57fa\u4e8eHTC. \u4e3b\u8981\u63d0\u51fa\u7684\u70b9\u662fRecursive Feature Pyramid\u4ee5\u53ca Switchable Atrous Convolution \u70b9\u6570\u63d0\u5347\u8f83\u4e3a\u60ca\u4eba\uff0c\u6a21\u578b\u5927\u5c0f\u4ee5\u53ca\u5185\u5b58\u5360\u7528\u6bd4\u8f83\u5927(\u6ce8\u610f\u7531\u4e8ebackbone\u7684\u6539\u53d8\uff0c\u5176ResNet-50\u6a21\u578b\u7684\u5927\u5c0f\u8fd1\u4e4e\u4e8eHTC ResNeXt-101-64x4d\u7684\u5927\u5c0f\uff0c\u4e14\u7531\u4e8eRFP\u7684\u8fed\u4ee3\u539f\u56e0\uff0c\u5360\u7528\u663e\u5b58\u53ef\u80fd\u66f4\u5927)\u3002 CentripetalNet: Pursuing High-quality Keypoint Pairs for Object Detection pdf \u8fd9\u4e2apaper\u5ef6\u7eed\u7684\u662f CornerNet \u548c CenterNet \u7684\u601d\u8def. \u7528heatmap\u5206\u522b\u9884\u6d4b\u5de6\u4e0a\u89d2\u4ee5\u53ca\u53f3\u4e0b\u89d2 \u9884\u6d4b\u4e00\u4e2a\u5de6\u4e0a\u89d2/\u53f3\u4e0b\u89d2\u5230\u4e2d\u5fc3\u70b9\u7684\u504f\u79fb\uff0c\u4f5c\u4e3aguiding\uff0c \u8fd9\u4e2aguiding\u53ef\u4ee5\u88ab\u76d1\u7763 \u4ee5\u6b64\u504f\u79fb\u4e3a\u57fa\u51c6\u8dd1deformable conv, \u8f93\u51fa\u66f4\u7cbe\u786e\u7684 centripetal shift \u57fa\u4e8e\u5bf9\u4e2d\u5fc3\u70b9\u7684\u9884\u6d4b\u5c06\u5de6\u4e0a\u89d2\u4e0e\u53f3\u4e0b\u89d2\u8fdb\u884c\u5339\u914d\u805a\u7c7b \u4ee5\u9884\u6d4b\u51fa\u6765\u76842D bounding box\u4e3a\u57fa\u7840\u9884\u6d4b instance mask. \u70ed\u70b9\u9884\u6d4b\u4e2d\u89d2\u70b9\u7684\u504f\u79fb(\u7528\u4e8efinetuning corner) / Guiding shift / \u57fa\u4e8e\u4e2d\u5fc3\u70b9\u9884\u6d4b\u7684\u5339\u914d\u4e2d\u5fc3\u533a: \u4f7f\u7528guiding deform conv\u7684 intuition \u4ee3\u7801\u4e0a\u672c\u6587\u7684\u4ee3\u7801\u6709\u70b9\u6df7\u4e71\uff0chead\u4e0ecorner head\u4ee3\u7801\u6709\u70b9\u91cd\u53e0\u6df7\u6742. SCNet: Training Inference Sample Consistency for Instance Segmentation pdf code \u8fd9\u7bc7paper\u57fa\u4e8e cascade mask RCNN \u4ee5\u53ca HTC\u7684\u4e32\u7ea7\u601d\u8def\u3002 \u4e3b\u8981\u63d0\u51fa\u7684\u4e00\u4e2aidea\u662f\u514b\u670d\u8bad\u7ec3\u65f6\u7684RoI \u51c6\u786e\u7387\u548c\u63a8\u7406\u65f6\u4e0d\u540c\u8fd9\u4e2a\u95ee\u9898\u3002 \u4f5c\u8005\u53d1\u73b0\u4f7f\u7528cascade\u8bad\u7ec3\u7684\u65f6\u5019\uff0c\u63d0\u4f9b\u7ed9segmentation\u7684bounding box\u6709\u5f88\u591a\u662f\u57fa\u4e8e\u6ca1\u6709\u90a3\u4e48\u51c6\u786e\u7684\u524d\u65b9\u5206\u652f\uff0c\u800cinferece\u7684\u65f6\u5019\u5219\u53ea\u4f7f\u7528\u6700\u540e\u7684\u5206\u652f\u3002\u8fd9\u91cc\u5b58\u5728\u9519\u914d\u3002\u56e0\u800c\u63d0\u51famask\u53ea\u5e94\u8be5\u5728\u6700\u540e\u4e00\u4e2a\u5206\u652f\u51fa\u73b0\u3002","title":"Some Collections around MMDetection"},{"location":"other_categories/object_detection_2D/MMDetection/#some-collections-around-mmdetection","text":"\u8fd9\u4e00\u9875\u9762\u4e3b\u8981\u4e3a\u4e86\u6536\u96c6mmdetection\u4e2d\u63d0\u4f9b\u5b9e\u73b0\u7684\u8bba\u6587\u3002\u8fd9\u91cc\u6536\u96c6\u6216\u8005\u63d0\u4f9b\u94fe\u63a5\u7684\u4e3b\u8981\u662f\u76f8\u5bf9\u51b7\u95e8\u7684paper\uff0c\u4e3b\u6d41\u7684\u5982Faster-RCNN\u4ee5\u53caRetinanet\u4e0d\u4f1a\u518d\u91cd\u590d\u3002 \u5176\u4f59\u6709\u5b9e\u73b0\uff0c\u5e76\u8bb0\u5f55\u5728\u672c\u7f51\u7ad9\u5176\u4ed6\u5730\u65b9\u7684\u6709 FCOS , FreeAnchor , ATSS , RepPoints Update: 2020/06/04: Updates HTC , DetectoRS","title":"Some Collections around MMDetection"},{"location":"other_categories/object_detection_2D/MMDetection/#single-stage-methods","text":"","title":"Single Stage Methods"},{"location":"other_categories/object_detection_2D/MMDetection/#ghm-gradient-harmonized-single-stage-detector","text":"pdf \u8fd9\u7bc7paper\u4e3b\u8981idea\u662floss function\u5e94\u8be5\u5e73\u8861\u4e0d\u540c\u6837\u672c\u4e4b\u95f4\u7684gradient norm. \u8fc7\u4e8e\u56f0\u96be\u7684instance gradient norm\u8f83\u5927\u800c\u8fc7\u4e8e\u7b80\u5355\u7684\u7684instance gradient\u7406\u5e94\u4f1a\u5f88\u5c0f\uff0c \u4e00\u4e2awell trained detector\u7684gradient norm\u5206\u5e03\u5982\u56fe \u4f5c\u8005\u7684\u60f3\u6cd5\u662f\u5e94\u8be5\u63d0\u5347\u4e2d\u95f4\u5c42\uff0c\u6216\u8005\u8bf4gradient\u5bc6\u5ea6\u6bd4\u8f83\u5c0f\u7684\u90e8\u5206\u7684\u68af\u5ea6\u8d21\u732e\u3002 \u5b9a\u4e49\u68af\u5ea6\u5bc6\u5ea6\u51fd\u6570: G D(g)=\\frac{1}{l_{\\epsilon}(g)} \\sum_{k=1}^{N} \\delta_{\\epsilon}\\left(g_{k}, g\\right) \\begin{aligned} &\\delta_{\\epsilon}(x, y)=\\left\\{\\begin{array}{ll} 1 & \\text { if } y-\\frac{\\epsilon}{2}<=x<y+\\frac{\\epsilon}{2} \\\\ 0 & \\text { otherwise } \\end{array}\\right.\\\\ &l_{\\epsilon}(g)=\\min \\left(g+\\frac{\\epsilon}{2}, 1\\right)-\\max \\left(g-\\frac{\\epsilon}{2}, 0\\right) \\end{aligned} \u7b80\u5355\u800c\u8a00\u5c31\u662f GD(g) \u4e3a\u4e0e\u68af\u5ea6g\u4e34\u8fd1\u7684\u533a\u95f4\u5185\uff0c\u6709\u76f8\u8fd1\u68af\u5ea6norm\u7684example\u7684\u4e2a\u6570/\u68af\u5ea6\u533a\u95f4\u957f\u5ea6\u3002 \\beta_{i}=\\frac{N}{G D\\left(g_{i}\\right)} \\begin{aligned} L_{G H M-C} &=\\frac{1}{N} \\sum_{i=1}^{N} \\beta_{i} L_{C E}\\left(p_{i}, p_{i}^{*}\\right) \\\\ &=\\sum_{i=1}^{N} \\frac{L_{C E}\\left(p_{i}, p_{i}^{*}\\right)}{G D\\left(g_{i}\\right)} \\end{aligned} \u4e5f\u5373\u662f\u7ed9loss\u52a0\u4e0a\u4e0e\u68af\u5ea6example\u5bc6\u5ea6\u6210\u53cd\u6bd4\u7684\u5bf9\u5e94\u7684\u6743\u91cd\u3002","title":"GHM: Gradient Harmonized Single-stage Detector"},{"location":"other_categories/object_detection_2D/MMDetection/#fsaf-feature-selective-anchor-free-module-for-single-shot-object-detection","text":"pdf \u8fd9\u7bc7paper\u7684idea\u662f\u8ba9RetinaNet\u540c\u65f6\u7ef4\u62a4\u4e00\u4e2aanchor free\u4e00\u4e2aanchor based\u7684\u5206\u652f\uff0canchor_free\u7684\u5206\u652f\u5728\u6bcf\u4e00\u4e2ascale\u4e0a\u90fd\u4f1a\u53d7\u8bad\u7ec3\u3002 \u4f5c\u8005\u7684online selection\u601d\u8def\u662f\u8ba9\u6bcf\u4e00\u4e2ascale\u4e0a\u7684anchor free\u5206\u652f\u90fd\u9884\u6d4b\u4e00\u6b21\uff0c\u7136\u540e\u5f97\u5230\u5bf9\u5e94\u7684loss\uff0c\u5bfb\u627eanchor free loss\u6700\u5c0f\u7684scale\uff0ctrain\u5bf9\u5e94scale\u7684anchor-based\u5206\u652f\u3002 Inference\u7684\u65f6\u5019\u5219\u8ba9\u6240\u67096\u4e2a\u5206\u652f\u5404\u81ea\u8f93\u51fa\uff0c\u76f4\u63a5merge","title":"FSAF: Feature Selective Anchor-Free Module for Single-Shot Object Detection"},{"location":"other_categories/object_detection_2D/MMDetection/#foveabox-beyond-anchor-based-object-detector","text":"pdf code \u8fd9\u7bc7paper\u7684\u601d\u8def\u662f\u53d6Retinanet\u6240\u6709\u4e4b\u7cbe\u534e\uff0c\u6307\u51fa\u5728FPN\u7684multi-scale\u652f\u6301\u4e0b\uff0c\u5df2\u7ecf\u4e0d\u9700\u8981anchor\u4e86\uff0c\u5176\u5b9e\u4e5f\u5c31\u662f\u6bcf\u4e00\u4e2ascale\u6709\u4e00\u4e2aanchor\u5c31\u591f\u4e86\u3002","title":"FoveaBox: Beyond Anchor-based Object Detector"},{"location":"other_categories/object_detection_2D/MMDetection/#grid-r-cnn","text":"pdf code \u8fd9\u7bc7paper\u4ece\u4eca\u65e5\u7684\u89d2\u5ea6\u6765\u8bf4\u53ef\u4ee5\u7406\u89e3\u4e3aRoIPooling\u540e\u7684 keypointNet \uff0c\u5728grid point\u9009\u62e9\u4e0a\u6709\u70b9\u4e0d\u540c\uff0c\u4f46\u662f\u601d\u8def\u662f\u76f8\u4f3c\u7684\u3002","title":"Grid R-CNN"},{"location":"other_categories/object_detection_2D/MMDetection/#varifocalnet-an-iou-aware-dense-object-detector","text":"pdf \u8fd9\u7bc7paper\u7684\u4e3b\u8981\u5de5\u4f5c: Star Conv: \u5728\u7b2c\u4e00\u6b21bounding box\u9884\u6d4b\u7684\u57fa\u7840\u4e0a\u591a\u8dd1\u4e00\u4e2adeform conv, offset\u6b63\u662fbounding box\u7684\u5404\u4e2a\u89d2\u843d.(\u8ba9\u5404\u4e2a\u89d2\u843d\u7684feature\u878d\u5408\u5230\u4e2d\u5fc3\u6765\u7684\u601d\u8def), \u540e\u9762\u518d\u8f93\u51fa\u4e00\u4e2a\u6bd4\u4f8b\u4fee\u6b63\u9879. varifocal loss: \\operatorname{VFL}(\\mathrm{p}, \\mathrm{q})=\\left\\{\\begin{array}{ll} -\\mathrm{q}(\\mathrm{q} \\log (\\mathrm{p})+(1-\\mathrm{q}) \\log (1-\\mathrm{p})) & \\mathrm{q}>0 \\\\ -\\alpha \\mathrm{p}^{\\gamma} \\log (1-\\mathrm{p}) & \\mathrm{q}=0 \\end{array}\\right.","title":"VarifocalNet: An IoU-aware Dense Object Detector"},{"location":"other_categories/object_detection_2D/MMDetection/#two-stage-methods","text":"","title":"Two Stage Methods"},{"location":"other_categories/object_detection_2D/MMDetection/#htc","text":"pdf code motivation: 1. instance segmentation, detection\u751a\u81f3pixel-wise segmentation\u4e4b\u95f4\u662f\u4e92\u76f8\u8865\u5145\u7684\u3002\u4f46\u662f\u5355\u7eaf\u7684\u8ba9\u7f51\u7edc\u540c\u65f6\u8bad\u7ec3\u8fd9\u51e0\u4e2a\u4efb\u52a1\u5e76\u4e0d\u8db3\u591f\u3002 2. cascade RCNN\u63d0\u51fa\u4e86bounding box\u7684\u8fed\u4ee3\u4f18\u5316 \u672c\u7bc7paper\u5c31\u63d0\u51fa\u8ba9\u7f51\u7edc\u540c\u65f6\u8fed\u4ee3\u7684\u5b66\u4e60\u8fd9\u4e09\u4e2a\u4efb\u52a1(\u8fed\u4ee3\u4e3b\u8981\u53d1\u751f\u5728bounding box\u548cinstance seg\u91cc\u9762)\uff0c\u5728\u6bcf\u6b21\u8fed\u4ee3\u7684\u65f6\u5019\u901a\u8fc7\u5176\u4ed6\u4efb\u52a1\u5f53\u524d\u7684\u9884\u6d4b\u8fdb\u5ea6\u8fdb\u884c\u4fe1\u606f\u8865\u5145\u3002 \u4fe1\u606f\u6d41\u56fe: \u5c06Mask RCNN\u7684\u9884\u6d4b\u5206\u652f\u653e\u5230cascade RCNN\u4e0a\uff0c\u5f97\u5230(a)\u6d41\u56fe: \\begin{aligned} \\mathbf{x}_{t}^{b o x} &=\\mathcal{P}\\left(\\mathbf{x}, \\mathbf{r}_{t-1}\\right), \\quad \\mathbf{r}_{t}=B_{t}\\left(\\mathbf{x}_{t}^{b o x}\\right) \\\\ \\mathbf{x}_{t}^{\\operatorname{mask}} &=\\mathcal{P}\\left(\\mathbf{x}, \\mathbf{r}_{t-1}\\right), \\quad \\mathbf{m}_{t}=M_{t}\\left(\\mathbf{x}_{t}^{\\operatorname{mask} }\\right) \\end{aligned} \u5176\u4e2d \\mathcal{P} \u662fpooling\u64cd\u4f5c\uff0c B, M \u5206\u522b\u662fbbox_head\u4ee5\u53camask_head. \u8fd9\u79cd\u8bbe\u7f6e\u7684\u4e00\u4e2a\u63d0\u5347\u65b9\u5411\u662fbbox\u4ee5\u53camask\u8fd8\u662f\u5e76\u884c\u7684\u3002\u4f5c\u8005\u8ba9\u6700\u65b0\u7684bbox\u9884\u6d4b\u503c\u7528\u4e8emask head\u7684\u8f93\u51fa\uff0c\u5f97\u5230(b) \\begin{array}{ll} \\mathbf{x}_{t}^{b o x}=\\mathcal{P}\\left(\\mathbf{x}, \\mathbf{r}_{t-1}\\right), & \\mathbf{r}_{t}=B_{t}\\left(\\mathbf{x}_{t}^{b o x}\\right) \\\\ \\mathbf{x}_{t}^{\\operatorname{mask}}=\\mathcal{P}\\left(\\mathbf{x}, \\mathbf{r}_{t}\\right), & \\mathbf{m}_{t}=M_{t}\\left(\\mathbf{x}_{t}^{\\operatorname{mask} }\\right) \\end{array} \u4f5c\u8005\u8fdb\u4e00\u6b65\u8003\u8651\u5c06\u524d\u4e00\u6b21\u8fed\u4ee3\u7684mask\u7684\u4fe1\u606f\u6765\u8865\u5145\u65b0mask\u7684\u4fe1\u606f: \\begin{array}{ll} \\mathbf{x}_{t}^{b o x}=\\mathcal{P}\\left(\\mathbf{x}, \\mathbf{r}_{t-1}\\right), & \\mathbf{r}_{t}=B_{t}\\left(\\mathbf{x}_{t}^{b o x}\\right) \\\\ \\mathbf{x}_{t}^{\\operatorname{mask}}=\\mathcal{P}\\left(\\mathbf{x}, \\mathbf{r}_{t}\\right), & \\mathbf{m}_{t}=M_{t}\\left(\\mathcal{F}\\left(\\mathbf{x}_{t}^{\\operatorname{mask}}, \\mathbf{m}_{t-1}^{-}\\right)\\right) \\end{array} \\mathcal{F}\\left(\\mathbf{x}_{t}^{\\operatorname{mask}}, \\mathbf{m}_{t-1}\\right)=\\mathbf{x}_{t}^{\\operatorname{mask}}+\\mathcal{G}_{t}\\left(\\mathbf{m}_{t-1}^{-}\\right) \u4f5c\u8005\u8fdb\u4e00\u6b65\u8003\u8651\u5c06semantic segmentation\u7684\u4fe1\u606f\u52a0\u8fdb\u6765\uff0c\u5f97\u5230: \\begin{aligned} \\mathbf{x}_{t}^{b o x} &=\\mathcal{P}\\left(\\mathbf{x}, \\mathbf{r}_{t-1}\\right)+\\mathcal{P}\\left(S(\\mathbf{x}), \\mathbf{r}_{t-1}\\right) \\\\ \\mathbf{r}_{t} &=B_{t}\\left(\\mathbf{x}_{t}^{b o x}\\right) \\\\ \\mathbf{x}_{t}^{m a s k} &=\\mathcal{P}\\left(\\mathbf{x}, \\mathbf{r}_{t}\\right)+\\mathcal{P}\\left(S(\\mathbf{x}), \\mathbf{r}_{t}\\right) \\\\ \\mathbf{m}_{t} &=M_{t}\\left(\\mathcal{F}\\left(\\mathbf{x}_{t}^{m a s k}, \\mathbf{m}_{t-1}^{-}\\right)\\right) \\end{aligned} \u5176\u4e2d S(x) \u4e3a\u8bed\u4e49\u5206\u5272\u8f93\u51fa","title":"HTC"},{"location":"other_categories/object_detection_2D/MMDetection/#detectors","text":"pdf code \u672c\u6587\u5728\u66f4\u65b0\u7684\u65f6\u5019\u8fd8\u6ca1\u6709\u653e\u5230mmdetection\u4e3b\u5206\u652f\u4e0a\uff0c\u662f\u57fa\u4e8emmdetection 1.0\u7248\u672c\u8bbe\u8ba1\u7684\u4ee3\u7801\u3002\u540c\u65f6\u66f4\u6539\u4e86backbone,\u4ee3\u7801\u57fa\u4e8eHTC. \u4e3b\u8981\u63d0\u51fa\u7684\u70b9\u662fRecursive Feature Pyramid\u4ee5\u53ca Switchable Atrous Convolution \u70b9\u6570\u63d0\u5347\u8f83\u4e3a\u60ca\u4eba\uff0c\u6a21\u578b\u5927\u5c0f\u4ee5\u53ca\u5185\u5b58\u5360\u7528\u6bd4\u8f83\u5927(\u6ce8\u610f\u7531\u4e8ebackbone\u7684\u6539\u53d8\uff0c\u5176ResNet-50\u6a21\u578b\u7684\u5927\u5c0f\u8fd1\u4e4e\u4e8eHTC ResNeXt-101-64x4d\u7684\u5927\u5c0f\uff0c\u4e14\u7531\u4e8eRFP\u7684\u8fed\u4ee3\u539f\u56e0\uff0c\u5360\u7528\u663e\u5b58\u53ef\u80fd\u66f4\u5927)\u3002","title":"DetectoRS"},{"location":"other_categories/object_detection_2D/MMDetection/#centripetalnet-pursuing-high-quality-keypoint-pairs-for-object-detection","text":"pdf \u8fd9\u4e2apaper\u5ef6\u7eed\u7684\u662f CornerNet \u548c CenterNet \u7684\u601d\u8def. \u7528heatmap\u5206\u522b\u9884\u6d4b\u5de6\u4e0a\u89d2\u4ee5\u53ca\u53f3\u4e0b\u89d2 \u9884\u6d4b\u4e00\u4e2a\u5de6\u4e0a\u89d2/\u53f3\u4e0b\u89d2\u5230\u4e2d\u5fc3\u70b9\u7684\u504f\u79fb\uff0c\u4f5c\u4e3aguiding\uff0c \u8fd9\u4e2aguiding\u53ef\u4ee5\u88ab\u76d1\u7763 \u4ee5\u6b64\u504f\u79fb\u4e3a\u57fa\u51c6\u8dd1deformable conv, \u8f93\u51fa\u66f4\u7cbe\u786e\u7684 centripetal shift \u57fa\u4e8e\u5bf9\u4e2d\u5fc3\u70b9\u7684\u9884\u6d4b\u5c06\u5de6\u4e0a\u89d2\u4e0e\u53f3\u4e0b\u89d2\u8fdb\u884c\u5339\u914d\u805a\u7c7b \u4ee5\u9884\u6d4b\u51fa\u6765\u76842D bounding box\u4e3a\u57fa\u7840\u9884\u6d4b instance mask. \u70ed\u70b9\u9884\u6d4b\u4e2d\u89d2\u70b9\u7684\u504f\u79fb(\u7528\u4e8efinetuning corner) / Guiding shift / \u57fa\u4e8e\u4e2d\u5fc3\u70b9\u9884\u6d4b\u7684\u5339\u914d\u4e2d\u5fc3\u533a: \u4f7f\u7528guiding deform conv\u7684 intuition \u4ee3\u7801\u4e0a\u672c\u6587\u7684\u4ee3\u7801\u6709\u70b9\u6df7\u4e71\uff0chead\u4e0ecorner head\u4ee3\u7801\u6709\u70b9\u91cd\u53e0\u6df7\u6742.","title":"CentripetalNet: Pursuing High-quality Keypoint Pairs for Object Detection"},{"location":"other_categories/object_detection_2D/MMDetection/#scnet-training-inference-sample-consistency-for-instance-segmentation","text":"pdf code \u8fd9\u7bc7paper\u57fa\u4e8e cascade mask RCNN \u4ee5\u53ca HTC\u7684\u4e32\u7ea7\u601d\u8def\u3002 \u4e3b\u8981\u63d0\u51fa\u7684\u4e00\u4e2aidea\u662f\u514b\u670d\u8bad\u7ec3\u65f6\u7684RoI \u51c6\u786e\u7387\u548c\u63a8\u7406\u65f6\u4e0d\u540c\u8fd9\u4e2a\u95ee\u9898\u3002 \u4f5c\u8005\u53d1\u73b0\u4f7f\u7528cascade\u8bad\u7ec3\u7684\u65f6\u5019\uff0c\u63d0\u4f9b\u7ed9segmentation\u7684bounding box\u6709\u5f88\u591a\u662f\u57fa\u4e8e\u6ca1\u6709\u90a3\u4e48\u51c6\u786e\u7684\u524d\u65b9\u5206\u652f\uff0c\u800cinferece\u7684\u65f6\u5019\u5219\u53ea\u4f7f\u7528\u6700\u540e\u7684\u5206\u652f\u3002\u8fd9\u91cc\u5b58\u5728\u9519\u914d\u3002\u56e0\u800c\u63d0\u51famask\u53ea\u5e94\u8be5\u5728\u6700\u540e\u4e00\u4e2a\u5206\u652f\u51fa\u73b0\u3002","title":"SCNet: Training Inference Sample Consistency for Instance Segmentation"},{"location":"other_categories/object_detection_2D/NGA/","text":"Exploiting Event Cameras by Using a Network Grafting Algorithm \u8fd9\u7bc7paper\u4f7f\u7528\u77e5\u8bc6\u84b8\u998f\u7684\u6982\u5ff5\u5c06RGB\u56fe\u4e2d\u7684\u76ee\u6807\u68c0\u6d4b\u80fd\u529b\u4f20\u9012\u5230event camera\u7684\u68c0\u6d4b\u4e2d\u3002 NGA structure \u7b80\u5355\u6765\u8bf4\u662f\u7528\u5728\u66f4\u5927\u6570\u636e\u96c6\u4e0a\u9884\u8bad\u7ec3\u597d\u7684\u57fa\u4e8eRGB\u76842D object detection\u6765\u8bad\u7ec3\u6570\u636e\u91cf\u66f4\u5c0f\u7684event camera \u7f51\u7edc\u3002 Event Camera \u7684Volume\u8868\u8fbe\u65b9\u5f0f \u5c06\u4e00\u6bb5\u65f6\u95f4\u5185\u7684event \u6d41\u6309\u7167\u65f6\u95f4\u7247\u5747\u5300\u5730\u5206\u5272\u4e3a D \u5757\uff0c\u5f62\u6210D\u4e2achannel\uff0c\u6bcf\u4e00\u4e2achannel\u5c06\u8fd9\u4e2a\u65f6\u95f4\u7247\u5185\u7684\u6240\u6709event\u5728\u5bf9\u5e94\u5750\u6807\u4e0a\u6c42\u548c\u5373\u53ef\u3002 \u672c\u6587\u7684\u65f6\u95f4\u7247\u5212\u5206\u5206\u4e3aD=3\u4e0eD=10\u4e24\u79cd NGA \u8bad\u7ec3 \u9996\u5148\u5c06\u9700\u8981\u8bad\u7ec3\u7684\u524d\u9988\u7f51\u7edc\u5206\u4e3a\u4e24\u5c42\uff0c\u7b2c\u4e00\u5c42\u7684\u8f93\u51fa\u4e3a H_t \uff0c\u7b2c\u4e8c\u5c42\u7684\u8f93\u51fa\u4e3a R_t . \u5bf9\u4e8e\u4e24\u4e2a\u7279\u5f81\u56fe\u7684\u5dee\u7684\u635f\u5931\uff0c\u672c\u6587\u63d0\u51fa\u76f4\u63a5\u4f7f\u7528MSE \\mathcal{L}_{\\text {recon }}=\\operatorname{MSE}(H, \\hat{H}) \\mathcal{L}_{\\mathrm{eval}}=\\operatorname{MSE}(R, \\hat{R}) \\mathcal{L}_{style} is based on Gram Matrix \\begin{array}{l} \\operatorname{Gram}(F)^{(i, j)}=\\sum_{\\forall t} \\tilde{F}_{t}^{(i) \\top} \\tilde{F}_{t}^{(j)}, \\quad \\text { where } \\tilde{F}_{t}=F_{t}-\\operatorname{mean}\\left(F_{t}\\right) \\\\ \\mathcal{L}_{\\text {style }}=\\gamma_{h} \\operatorname{MSE}(\\operatorname{Gram}(H), \\operatorname{Gram}(\\hat{H}))+\\gamma_{r} \\operatorname{MSE}(\\operatorname{Gram}(R), \\operatorname{Gram}(\\hat{R})) \\end{array} \u6700\u7ec8\uff1a \\mathcal{L}_{\\mathrm{tot}}=\\alpha \\mathcal{L}_{\\mathrm{recon}}+\\beta \\mathcal{L}_{\\mathrm{eval}}+\\mathcal{L}_{\\mathrm{style}} Style loss with Gram Matrix review pdf1 pdf2 The description is much clearer in pdf1 . \u8fd9\u91cc C_j, H_j, W_j \u4e3a\u7279\u5f81\u56fe \\phi_j \u7684\u5f62\u72b6, [h, w, c] \u6307indexing, G_j \u4e3a\u7279\u5f81\u56fe\u7684gram\u77e9\u9635\u3002\u4e0b\u5f0f\u8bf4\u660e\u683c\u62c9\u59c6\u77e9\u9635\u6bcf\u4e00\u4e2a\u5143\u7d20\u7684\u8fd0\u7b97\u3002 G_{j}^{\\phi}(x)[c, c^{\\prime}]=\\frac{1}{C_{j} H_{j} W_{j}} \\sum_{h=1}^{H_{j}} \\sum_{w=1}^{W_{j}} \\phi_{j}(x)[h, w, c] \\times \\phi_{j}(x) [h, w, c^{\\prime}] Style Loss\u5c31\u662f\u4e24\u4e2a\u7279\u5f81\u56fe\u683c\u62c9\u59c6\u77e9\u9635\u7684\u5dee\u7684\u4e8c\u8303\u6570 \\ell_{\\text {style}}^{\\phi, j}(\\hat{y}, y)=\\left\\|G_{j}^{\\phi}(\\hat{y})-G_{j}^{\\phi}(y)\\right\\|_{F}^{2} code keras example for style transfer def gram_matrix(x): \"\"\" x.shape = [C, H, W] for 'channels_first' \"\"\" assert K.ndim(x) == 3 if K.image_data_format() == 'channels_first': features = K.batch_flatten(x) #[C, H*W] else: features = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1))) gram = K.dot(features, K.transpose(features)) return gram Network seperation \u5bf9\u4e8eYOLOv3\u4e00\u51715\u6b21\u4e0b\u91c7\u6837\uff0c\u5206\u62106\u6bb5\u7f51\u7edc\uff0c\u5c06\u7b2c\u4e8c or \u7b2c\u4e09\u6bb5\u7684\u8f93\u51fa\u4f5c\u4e3a H \uff0c\u5c06\u7b2c\u56db\u6bb5\u7684\u8f93\u51fa\u4f5c\u4e3a R \u5f97\u5230\u7684\u7ed3\u679c\u8f83\u597d\uff08\u91cd\u590d\u5b9e\u9a8c\u7ed3\u679c\uff09 Other Stuffs \u6570\u636e\u96c6\uff1a The MVSEC Dataset,\u5e26\u6709\u6df1\u5ea6\u7684\u53cc\u76eeRGB+\u53cc\u76eeevent stream\u6570\u636e\u96c6\u3002 Event dataset from moving static images: https://www.garrickorchard.com/datasets","title":"Exploiting Event Cameras by Using a Network Grafting Algorithm"},{"location":"other_categories/object_detection_2D/NGA/#exploiting-event-cameras-by-using-a-network-grafting-algorithm","text":"\u8fd9\u7bc7paper\u4f7f\u7528\u77e5\u8bc6\u84b8\u998f\u7684\u6982\u5ff5\u5c06RGB\u56fe\u4e2d\u7684\u76ee\u6807\u68c0\u6d4b\u80fd\u529b\u4f20\u9012\u5230event camera\u7684\u68c0\u6d4b\u4e2d\u3002","title":"Exploiting Event Cameras by Using a Network Grafting Algorithm"},{"location":"other_categories/object_detection_2D/NGA/#nga-structure","text":"\u7b80\u5355\u6765\u8bf4\u662f\u7528\u5728\u66f4\u5927\u6570\u636e\u96c6\u4e0a\u9884\u8bad\u7ec3\u597d\u7684\u57fa\u4e8eRGB\u76842D object detection\u6765\u8bad\u7ec3\u6570\u636e\u91cf\u66f4\u5c0f\u7684event camera \u7f51\u7edc\u3002","title":"NGA structure"},{"location":"other_categories/object_detection_2D/NGA/#event-camera-volume","text":"\u5c06\u4e00\u6bb5\u65f6\u95f4\u5185\u7684event \u6d41\u6309\u7167\u65f6\u95f4\u7247\u5747\u5300\u5730\u5206\u5272\u4e3a D \u5757\uff0c\u5f62\u6210D\u4e2achannel\uff0c\u6bcf\u4e00\u4e2achannel\u5c06\u8fd9\u4e2a\u65f6\u95f4\u7247\u5185\u7684\u6240\u6709event\u5728\u5bf9\u5e94\u5750\u6807\u4e0a\u6c42\u548c\u5373\u53ef\u3002 \u672c\u6587\u7684\u65f6\u95f4\u7247\u5212\u5206\u5206\u4e3aD=3\u4e0eD=10\u4e24\u79cd","title":"Event Camera \u7684Volume\u8868\u8fbe\u65b9\u5f0f"},{"location":"other_categories/object_detection_2D/NGA/#nga","text":"\u9996\u5148\u5c06\u9700\u8981\u8bad\u7ec3\u7684\u524d\u9988\u7f51\u7edc\u5206\u4e3a\u4e24\u5c42\uff0c\u7b2c\u4e00\u5c42\u7684\u8f93\u51fa\u4e3a H_t \uff0c\u7b2c\u4e8c\u5c42\u7684\u8f93\u51fa\u4e3a R_t . \u5bf9\u4e8e\u4e24\u4e2a\u7279\u5f81\u56fe\u7684\u5dee\u7684\u635f\u5931\uff0c\u672c\u6587\u63d0\u51fa\u76f4\u63a5\u4f7f\u7528MSE \\mathcal{L}_{\\text {recon }}=\\operatorname{MSE}(H, \\hat{H}) \\mathcal{L}_{\\mathrm{eval}}=\\operatorname{MSE}(R, \\hat{R}) \\mathcal{L}_{style} is based on Gram Matrix \\begin{array}{l} \\operatorname{Gram}(F)^{(i, j)}=\\sum_{\\forall t} \\tilde{F}_{t}^{(i) \\top} \\tilde{F}_{t}^{(j)}, \\quad \\text { where } \\tilde{F}_{t}=F_{t}-\\operatorname{mean}\\left(F_{t}\\right) \\\\ \\mathcal{L}_{\\text {style }}=\\gamma_{h} \\operatorname{MSE}(\\operatorname{Gram}(H), \\operatorname{Gram}(\\hat{H}))+\\gamma_{r} \\operatorname{MSE}(\\operatorname{Gram}(R), \\operatorname{Gram}(\\hat{R})) \\end{array} \u6700\u7ec8\uff1a \\mathcal{L}_{\\mathrm{tot}}=\\alpha \\mathcal{L}_{\\mathrm{recon}}+\\beta \\mathcal{L}_{\\mathrm{eval}}+\\mathcal{L}_{\\mathrm{style}}","title":"NGA \u8bad\u7ec3"},{"location":"other_categories/object_detection_2D/NGA/#style-loss-with-gram-matrix-review","text":"pdf1 pdf2 The description is much clearer in pdf1 . \u8fd9\u91cc C_j, H_j, W_j \u4e3a\u7279\u5f81\u56fe \\phi_j \u7684\u5f62\u72b6, [h, w, c] \u6307indexing, G_j \u4e3a\u7279\u5f81\u56fe\u7684gram\u77e9\u9635\u3002\u4e0b\u5f0f\u8bf4\u660e\u683c\u62c9\u59c6\u77e9\u9635\u6bcf\u4e00\u4e2a\u5143\u7d20\u7684\u8fd0\u7b97\u3002 G_{j}^{\\phi}(x)[c, c^{\\prime}]=\\frac{1}{C_{j} H_{j} W_{j}} \\sum_{h=1}^{H_{j}} \\sum_{w=1}^{W_{j}} \\phi_{j}(x)[h, w, c] \\times \\phi_{j}(x) [h, w, c^{\\prime}] Style Loss\u5c31\u662f\u4e24\u4e2a\u7279\u5f81\u56fe\u683c\u62c9\u59c6\u77e9\u9635\u7684\u5dee\u7684\u4e8c\u8303\u6570 \\ell_{\\text {style}}^{\\phi, j}(\\hat{y}, y)=\\left\\|G_{j}^{\\phi}(\\hat{y})-G_{j}^{\\phi}(y)\\right\\|_{F}^{2}","title":"Style loss with Gram Matrix review"},{"location":"other_categories/object_detection_2D/NGA/#code","text":"keras example for style transfer def gram_matrix(x): \"\"\" x.shape = [C, H, W] for 'channels_first' \"\"\" assert K.ndim(x) == 3 if K.image_data_format() == 'channels_first': features = K.batch_flatten(x) #[C, H*W] else: features = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1))) gram = K.dot(features, K.transpose(features)) return gram","title":"code"},{"location":"other_categories/object_detection_2D/NGA/#network-seperation","text":"\u5bf9\u4e8eYOLOv3\u4e00\u51715\u6b21\u4e0b\u91c7\u6837\uff0c\u5206\u62106\u6bb5\u7f51\u7edc\uff0c\u5c06\u7b2c\u4e8c or \u7b2c\u4e09\u6bb5\u7684\u8f93\u51fa\u4f5c\u4e3a H \uff0c\u5c06\u7b2c\u56db\u6bb5\u7684\u8f93\u51fa\u4f5c\u4e3a R \u5f97\u5230\u7684\u7ed3\u679c\u8f83\u597d\uff08\u91cd\u590d\u5b9e\u9a8c\u7ed3\u679c\uff09","title":"Network seperation"},{"location":"other_categories/object_detection_2D/NGA/#other-stuffs","text":"\u6570\u636e\u96c6\uff1a The MVSEC Dataset,\u5e26\u6709\u6df1\u5ea6\u7684\u53cc\u76eeRGB+\u53cc\u76eeevent stream\u6570\u636e\u96c6\u3002 Event dataset from moving static images: https://www.garrickorchard.com/datasets","title":"Other Stuffs"},{"location":"other_categories/object_detection_2D/Object_as_points/","text":"Detection and Tracking as Point \u672c\u6587\u5f15\u5165\u4e24\u7bc7paper\uff0c\u7b2c\u4e00\u7bc7\u662f\u4e0eCenterNet\u649e\u540d\u7684 Object as Point , \u7b2c\u4e8c\u7bc7\u662f\u4ee5\u6b64\u4e3a\u57fa\u7840\u7684 Tracking as Point ,\u5b83\u662f\u597d\u591a\u7bc72D/3D\u68c0\u6d4b\u7684\u524d\u7f6epaper\uff0c\u5176\u7279\u70b9\u662f\u901f\u5ea6\u5f88\u5feb\uff0c\u4e0d\u9700\u8981\u7279\u6b8a\u7684NMS(\u771f\u6b63\u610f\u4e49\u5730\u629b\u5374NMS)\uff0c\u4e14\u6a21\u578b\u6269\u5c55\u6027\u5f88\u5f3a\u2014\u2014\u5728\u7f51\u7edc\u4e2d\u4e8b\u5b9e\u4e0a\u5f88\u591a\u4e1c\u897f\u90fd\u662f\u4e00\u4e2apoint Object as Point pdf code \u8fd9\u7bc7paper\u7684keypoint\u68c0\u6d4b\u90e8\u5206\u4e0e CornerNet \u662f\u4e00\u81f4\u7684,\u4f46\u662f\u6b63\u6837\u672c\u7684\u5b9a\u4e49\u6709\u533a\u522b\uff0c\u6700\u9760\u8fd1\u7269\u4f53\u4e2d\u5fc3\u7684\u70b9\u4f1a\u88ab\u6807\u8bb0\u4e3a\u6b63\u6837\u672c\uff0c\u4f7f\u7528\u4e00\u4e2a\u9ad8\u65af\u6838\uff0c\u6839\u636e\u7269\u4f53\u5927\u5c0fsmooth out \u5206\u7c7b\u7f51\u7edc\u7684\u8d1f\u6837\u672c\u60e9\u7f5a. L_{k}=\\frac{-1}{N} \\sum_{x y c}\\left\\{\\begin{array}{ll} \\left(1-\\hat{Y}_{x y c}\\right)^{\\alpha} \\log \\left(\\hat{Y}_{x y c}\\right) & \\text { if } Y_{x y c}=1 \\\\ \\left(1-Y_{x y c}\\right)^{\\beta}\\left(\\hat{Y}_{x y c}\\right)^{\\alpha} \\log \\left(1-\\hat{Y}_{x y c}\\right) & \\text { otherwise } \\end{array}\\right. \u6ce8\u610fCenterTrack\u7684\u516c\u5f0f\u662f\u6709bug\u7684\uff0c\u6f0f\u4e00\u4e2a\u4e00\u4e2a\u8d1f\u53f7,\u8840\u6cea\u6559\u8bad \u5bf9\u4e8e2D\u6846\u7684\u957f\u5bbd\u4ee5\u53cacx cy,\u4e0dnormalize,\u76f4\u63a5L1loss\u56de\u5f52raw pixel coordinates,\u9009\u62e9scale loss(\u4e5f\u5c31\u662f\u8bf4\u8f93\u51fa\u7684\u6570\u503c\u4f1a\u5f88\u5927\uff0c\u4f46\u662floss\u4f1a\u6bd4\u8f83\u6b63\u5e38) L_{d e t}=L_{k}+\\lambda_{s i z e} L_{s i z e}+\\lambda_{o f f} L_{o f f} \u8fdb\u884cNMS\u7684\u65f6\u5019\uff0c\u4e0d\u9700\u8981\u4f7f\u7528\u7279\u6b8a\u7684NMS\uff0c\u4f5c\u8005\u91c7\u7528\u7684\u662f\u66f4\u4e3a\u7684Maxpooling def _nms(heat, kernel=3): pad = (kernel - 1) // 2 hmax = nn.functional.max_pool2d( heat, (kernel, kernel), stride=1, padding=pad) keep = (hmax == heat).float() return heat * keep \u8fdb\u800c\u4f5c\u8005\u5ef6\u4f38\u51fa\u66f4\u591a\u7684application\uff0c\u6bd4\u5982\u6bd4\u5982\u501f\u52a9\u9884\u6d4boffset\u4ee5\u53caheatmap\u7684\u673a\u5236\u8fdb\u884cpose-estimation,\u540c\u65f6\u9884\u6d4b3D\u4fe1\u606f\u5b9e\u73b03D object detection\u3002\u901f\u5ea6\u5feb\u800c\u7cbe\u786e Tracking Objects as Points pdf code \u8fd9\u7bc7paper\u4e5f\u88ab\u79f0\u4f5cCenterTrack. \u601d\u8def\u662f\u5c06\u8fde\u7eed\u4e24\u5e27\u7684\u56fe\u7247\u4ee5\u53ca\u4e0a\u4e00\u5e27\u7684tracking\u7ed3\u679c\u6784\u6210\u7684 heatMap\u4f5c\u4e3a\u8f93\u5165\uff0c\u56e0\u6b64\u7f51\u7edc\u7684\u7ed3\u6784\u4e0e\u524d\u6587\u4e00\u81f4\uff0c\u4ec5\u4ec5\u533a\u522b\u5728\u4e8e\u591a4\u4e2achannel\u7684\u8f93\u5165\u3002\u8f93\u51fa\u989d\u5916\u591a\u4e24\u4e2achannel,\u4e5f\u5c31\u662f\u4e24\u5e27ground truth\u4e4b\u95f4\u7684\u533a\u522b\uff0c L_{o f f}=\\frac{1}{N} \\sum_{i=1}^{N}\\left|\\hat{D}_{\\mathbf{p}_{i}^{(t)}}-\\left(\\mathbf{p}_{i}^{(t-1)}-\\mathbf{p}_{i}^{(t)}\\right)\\right| \u56e0\u800c\u5728tracking\u7684\u65f6\u5019\uff0c\u6309\u7167confidence\u7684\u6392\u5e8f\uff0c\u5c06\u5f53\u524d\u4f4d\u7f6e\u70b9\u4e0e\u6700\u9760\u8fd1 p - D_p \u7684\u524d\u4e00\u5e27\u70b9\u8d2a\u5a6a\u5730\u5339\u914d\uff0c\u5982\u679c\u53d1\u73b0\u5728\u4e00\u5b9a\u8303\u56f4\u5185unmatched\uff0c\u5c31\u8ba4\u4e3a\u4ea7\u751f\u4e86\u4e00\u4e2a\u65b0\u7684\u7269\u4f53\u3002 \u6bd4\u8f83\u795e\u5947\u7684\u662f\uff0c\u672c\u6587\u662f\u53ef\u4ee5\u7528\u9759\u6001\u56fe\u7247\u8fdb\u884c\u8bad\u7ec3\u7684 ,\u968f\u673a\u5c06\u56fe\u7247\u5e73\u79fb\uff0cscale up\uff0c\u7ed3\u679c\u60ca\u4eba\u5730\u597d","title":"Detection and Tracking as Point"},{"location":"other_categories/object_detection_2D/Object_as_points/#detection-and-tracking-as-point","text":"\u672c\u6587\u5f15\u5165\u4e24\u7bc7paper\uff0c\u7b2c\u4e00\u7bc7\u662f\u4e0eCenterNet\u649e\u540d\u7684 Object as Point , \u7b2c\u4e8c\u7bc7\u662f\u4ee5\u6b64\u4e3a\u57fa\u7840\u7684 Tracking as Point ,\u5b83\u662f\u597d\u591a\u7bc72D/3D\u68c0\u6d4b\u7684\u524d\u7f6epaper\uff0c\u5176\u7279\u70b9\u662f\u901f\u5ea6\u5f88\u5feb\uff0c\u4e0d\u9700\u8981\u7279\u6b8a\u7684NMS(\u771f\u6b63\u610f\u4e49\u5730\u629b\u5374NMS)\uff0c\u4e14\u6a21\u578b\u6269\u5c55\u6027\u5f88\u5f3a\u2014\u2014\u5728\u7f51\u7edc\u4e2d\u4e8b\u5b9e\u4e0a\u5f88\u591a\u4e1c\u897f\u90fd\u662f\u4e00\u4e2apoint","title":"Detection and Tracking as Point"},{"location":"other_categories/object_detection_2D/Object_as_points/#object-as-point","text":"pdf code \u8fd9\u7bc7paper\u7684keypoint\u68c0\u6d4b\u90e8\u5206\u4e0e CornerNet \u662f\u4e00\u81f4\u7684,\u4f46\u662f\u6b63\u6837\u672c\u7684\u5b9a\u4e49\u6709\u533a\u522b\uff0c\u6700\u9760\u8fd1\u7269\u4f53\u4e2d\u5fc3\u7684\u70b9\u4f1a\u88ab\u6807\u8bb0\u4e3a\u6b63\u6837\u672c\uff0c\u4f7f\u7528\u4e00\u4e2a\u9ad8\u65af\u6838\uff0c\u6839\u636e\u7269\u4f53\u5927\u5c0fsmooth out \u5206\u7c7b\u7f51\u7edc\u7684\u8d1f\u6837\u672c\u60e9\u7f5a. L_{k}=\\frac{-1}{N} \\sum_{x y c}\\left\\{\\begin{array}{ll} \\left(1-\\hat{Y}_{x y c}\\right)^{\\alpha} \\log \\left(\\hat{Y}_{x y c}\\right) & \\text { if } Y_{x y c}=1 \\\\ \\left(1-Y_{x y c}\\right)^{\\beta}\\left(\\hat{Y}_{x y c}\\right)^{\\alpha} \\log \\left(1-\\hat{Y}_{x y c}\\right) & \\text { otherwise } \\end{array}\\right. \u6ce8\u610fCenterTrack\u7684\u516c\u5f0f\u662f\u6709bug\u7684\uff0c\u6f0f\u4e00\u4e2a\u4e00\u4e2a\u8d1f\u53f7,\u8840\u6cea\u6559\u8bad \u5bf9\u4e8e2D\u6846\u7684\u957f\u5bbd\u4ee5\u53cacx cy,\u4e0dnormalize,\u76f4\u63a5L1loss\u56de\u5f52raw pixel coordinates,\u9009\u62e9scale loss(\u4e5f\u5c31\u662f\u8bf4\u8f93\u51fa\u7684\u6570\u503c\u4f1a\u5f88\u5927\uff0c\u4f46\u662floss\u4f1a\u6bd4\u8f83\u6b63\u5e38) L_{d e t}=L_{k}+\\lambda_{s i z e} L_{s i z e}+\\lambda_{o f f} L_{o f f} \u8fdb\u884cNMS\u7684\u65f6\u5019\uff0c\u4e0d\u9700\u8981\u4f7f\u7528\u7279\u6b8a\u7684NMS\uff0c\u4f5c\u8005\u91c7\u7528\u7684\u662f\u66f4\u4e3a\u7684Maxpooling def _nms(heat, kernel=3): pad = (kernel - 1) // 2 hmax = nn.functional.max_pool2d( heat, (kernel, kernel), stride=1, padding=pad) keep = (hmax == heat).float() return heat * keep \u8fdb\u800c\u4f5c\u8005\u5ef6\u4f38\u51fa\u66f4\u591a\u7684application\uff0c\u6bd4\u5982\u6bd4\u5982\u501f\u52a9\u9884\u6d4boffset\u4ee5\u53caheatmap\u7684\u673a\u5236\u8fdb\u884cpose-estimation,\u540c\u65f6\u9884\u6d4b3D\u4fe1\u606f\u5b9e\u73b03D object detection\u3002\u901f\u5ea6\u5feb\u800c\u7cbe\u786e","title":"Object as Point"},{"location":"other_categories/object_detection_2D/Object_as_points/#tracking-objects-as-points","text":"pdf code \u8fd9\u7bc7paper\u4e5f\u88ab\u79f0\u4f5cCenterTrack. \u601d\u8def\u662f\u5c06\u8fde\u7eed\u4e24\u5e27\u7684\u56fe\u7247\u4ee5\u53ca\u4e0a\u4e00\u5e27\u7684tracking\u7ed3\u679c\u6784\u6210\u7684 heatMap\u4f5c\u4e3a\u8f93\u5165\uff0c\u56e0\u6b64\u7f51\u7edc\u7684\u7ed3\u6784\u4e0e\u524d\u6587\u4e00\u81f4\uff0c\u4ec5\u4ec5\u533a\u522b\u5728\u4e8e\u591a4\u4e2achannel\u7684\u8f93\u5165\u3002\u8f93\u51fa\u989d\u5916\u591a\u4e24\u4e2achannel,\u4e5f\u5c31\u662f\u4e24\u5e27ground truth\u4e4b\u95f4\u7684\u533a\u522b\uff0c L_{o f f}=\\frac{1}{N} \\sum_{i=1}^{N}\\left|\\hat{D}_{\\mathbf{p}_{i}^{(t)}}-\\left(\\mathbf{p}_{i}^{(t-1)}-\\mathbf{p}_{i}^{(t)}\\right)\\right| \u56e0\u800c\u5728tracking\u7684\u65f6\u5019\uff0c\u6309\u7167confidence\u7684\u6392\u5e8f\uff0c\u5c06\u5f53\u524d\u4f4d\u7f6e\u70b9\u4e0e\u6700\u9760\u8fd1 p - D_p \u7684\u524d\u4e00\u5e27\u70b9\u8d2a\u5a6a\u5730\u5339\u914d\uff0c\u5982\u679c\u53d1\u73b0\u5728\u4e00\u5b9a\u8303\u56f4\u5185unmatched\uff0c\u5c31\u8ba4\u4e3a\u4ea7\u751f\u4e86\u4e00\u4e2a\u65b0\u7684\u7269\u4f53\u3002 \u6bd4\u8f83\u795e\u5947\u7684\u662f\uff0c\u672c\u6587\u662f\u53ef\u4ee5\u7528\u9759\u6001\u56fe\u7247\u8fdb\u884c\u8bad\u7ec3\u7684 ,\u968f\u673a\u5c06\u56fe\u7247\u5e73\u79fb\uff0cscale up\uff0c\u7ed3\u679c\u60ca\u4eba\u5730\u597d","title":"Tracking Objects as Points"},{"location":"other_categories/object_detection_2D/PAA/","text":"Probabilistic Anchor Assignment with IoU Prediction for Object Detection \u8fd9\u7bc7paper\u7684\u63a2\u7d22\u7684\u662fanchor\u5206\u914dground truth\u7684\u7b56\u7565,\u7c7b\u4f3c\u7684\u4efb\u52a1\u672c\u7ad9\u5206\u6790\u7684\u6709 FreeAnchor , \u4ee5\u53ca ATSS Method \u672c\u6587\u7684\u601d\u8def\uff1a \u7528IoU Loss\u4ee5\u53ca\u5206\u7c7bloss\u7684\u7ec4\u5408\u4f5c\u4e3a anchor assignment\u7684\u4f9d\u636e\u3002 \u5bf9\u4e8e\u6bcf\u4e00\u4e2agrount truth\uff0canchors\u4e0e\u5b83\u7684score\u53ef\u4ee5\u7531\u4e24\u4e2a\u5cf0\u503c\u7684\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u8fdb\u884c\u5efa\u6a21\uff0c\u76f4\u89c2\u6765\u8bf4\u4e00\u4e2a\u5bf9\u5e94\u8d1f\u6837\u672c\uff0c\u4e00\u4e2a\u5bf9\u5e94\u6b63\u6837\u672c\u3002 \u7531\u4e8egt assignment\u4ee5Classification\u4ee5\u53caIoU\u4f5c\u4e3a\u57fa\u7840\uff0c\u6240\u6709NMS\u4e5f\u9700\u8981\u8003\u8651IoU\u7ed3\u679c\u3002 Score voting \u5bf9NMS\u540e\u7684bbox \\begin{array}{c} p_{i}=e^{-\\left(1-\\mathrm{IoU}\\left(b, b_{i}\\right)\\right)^{2} / \\sigma_{t}} \\\\ \\hat{b}=\\frac{\\sum_{i} p_{i} s_{i} b_{i}}{\\sum_{i} p_{i} s_{i}} \\text { subject to } \\mathrm{IoU}\\left(b, b_{i}\\right)>0 \\end{array}","title":"Probabilistic Anchor Assignment with IoU Prediction for Object Detection"},{"location":"other_categories/object_detection_2D/PAA/#probabilistic-anchor-assignment-with-iou-prediction-for-object-detection","text":"\u8fd9\u7bc7paper\u7684\u63a2\u7d22\u7684\u662fanchor\u5206\u914dground truth\u7684\u7b56\u7565,\u7c7b\u4f3c\u7684\u4efb\u52a1\u672c\u7ad9\u5206\u6790\u7684\u6709 FreeAnchor , \u4ee5\u53ca ATSS","title":"Probabilistic Anchor Assignment with IoU Prediction for Object Detection"},{"location":"other_categories/object_detection_2D/PAA/#method","text":"\u672c\u6587\u7684\u601d\u8def\uff1a \u7528IoU Loss\u4ee5\u53ca\u5206\u7c7bloss\u7684\u7ec4\u5408\u4f5c\u4e3a anchor assignment\u7684\u4f9d\u636e\u3002 \u5bf9\u4e8e\u6bcf\u4e00\u4e2agrount truth\uff0canchors\u4e0e\u5b83\u7684score\u53ef\u4ee5\u7531\u4e24\u4e2a\u5cf0\u503c\u7684\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u8fdb\u884c\u5efa\u6a21\uff0c\u76f4\u89c2\u6765\u8bf4\u4e00\u4e2a\u5bf9\u5e94\u8d1f\u6837\u672c\uff0c\u4e00\u4e2a\u5bf9\u5e94\u6b63\u6837\u672c\u3002 \u7531\u4e8egt assignment\u4ee5Classification\u4ee5\u53caIoU\u4f5c\u4e3a\u57fa\u7840\uff0c\u6240\u6709NMS\u4e5f\u9700\u8981\u8003\u8651IoU\u7ed3\u679c\u3002","title":"Method"},{"location":"other_categories/object_detection_2D/PAA/#score-voting","text":"\u5bf9NMS\u540e\u7684bbox \\begin{array}{c} p_{i}=e^{-\\left(1-\\mathrm{IoU}\\left(b, b_{i}\\right)\\right)^{2} / \\sigma_{t}} \\\\ \\hat{b}=\\frac{\\sum_{i} p_{i} s_{i} b_{i}}{\\sum_{i} p_{i} s_{i}} \\text { subject to } \\mathrm{IoU}\\left(b, b_{i}\\right)>0 \\end{array}","title":"Score voting"},{"location":"other_categories/object_detection_2D/RVT/","text":"Recurrent Vision Transformers for Object Detection with Event Cameras \u8fd9\u7bc7paper\u7528Recurrent \u7f51\u7edc\u4f7f\u7528event cameras \u8dd1\u51fa\u4e86\u8f83\u9ad8\u7684\u68c0\u6d4b\u8d28\u91cf\u3002\u8fd0\u884c\u901f\u5ea6\u4e5f\u662f\u6bd4\u8f83\u5feb\u7684\u3002 \u4f5c\u8005\u6307\u51fa\uff0cevent camera\u505a\u68c0\u6d4b\u7684\u96be\u70b9\u5728\u4e8e\u9700\u8981\u65f6\u95f4\u7d2f\u8ba1\u4fe1\u606f\uff0c\u4f46\u662f\u53c8\u9700\u8981\u63a7\u5236\u7cfb\u7edf\u6574\u4f53\u7684\u8fd0\u7b97\u91cf\u3002\u65f6\u5e8f\u7684\u5173\u8054\u4e0e\u7a7a\u95f4\u9700\u8981\u540c\u65f6\u987e\u8651\uff0c\u5374\u9700\u8981\u63a7\u5236\u6574\u4f53\u63a8\u7406\u65f6\u95f4\u3002\u8fd9\u4e5f\u662f\u672c\u6587\u5c1d\u8bd5\u8bbe\u8ba1\u7a81\u7834\u7684\u70b9\u3002 \u6700\u7ec8\u603b\u7ed3\u51fa\u4e86\u4e09\u4e2a\u6bd4\u8f83\u5173\u952e\u7684\u7ecf\u9a8c\uff1a \u6570\u636e\u7684\u5904\u7406\u4f7f\u7528\u65f6\u5e8f\u7684\u3001\u5c0f\u5355\u4f4d\u79bb\u6563\u7d2f\u79ef\u7684event maps \u4f7f\u7528\u5377\u79ef\u505a\u521d\u59cb\u5904\u7406\uff0c\u5e76\u4e14\u5728\u5377\u79ef\u8f93\u5165\u65f6\u6dfb\u52a0positional embedding, \u6ce8\u5165\u7edd\u5bf9\u4f4d\u7f6e\u4fe1\u606f\u3002\u9002\u5f53\u4e0b\u91c7\u6837\uff0c\u63a7\u5236\u540e\u7eed\u7279\u5f81\u5927\u5c0f\u3002 \u4f7f\u7528Multi-Axis Attention.\u4e00\u4e2a\u662f\u628a\u56fe\u7247\u5212\u4e3a\u4e00\u4e2a\u4e2ablock\uff0c\u5728block\u5185\u90e8transformer, \u4e00\u4e2a\u662f\u8de8block \u7528\u4e00\u4e2adilated \u7f51\u683c\u53d6\u70b9\uff0cdilated\u7f51\u683c\u4e0a\u7684\u70b9\u4e4b\u95f4 transformer\u3002 \u4f7f\u7528LSTM\u800c\u4e0d\u662fConv-LSTM\u5904\u7406\u65f6\u5e8f\u5173\u7cfb\u3002","title":"Recurrent Vision Transformers for Object Detection with Event Cameras"},{"location":"other_categories/object_detection_2D/RVT/#recurrent-vision-transformers-for-object-detection-with-event-cameras","text":"\u8fd9\u7bc7paper\u7528Recurrent \u7f51\u7edc\u4f7f\u7528event cameras \u8dd1\u51fa\u4e86\u8f83\u9ad8\u7684\u68c0\u6d4b\u8d28\u91cf\u3002\u8fd0\u884c\u901f\u5ea6\u4e5f\u662f\u6bd4\u8f83\u5feb\u7684\u3002 \u4f5c\u8005\u6307\u51fa\uff0cevent camera\u505a\u68c0\u6d4b\u7684\u96be\u70b9\u5728\u4e8e\u9700\u8981\u65f6\u95f4\u7d2f\u8ba1\u4fe1\u606f\uff0c\u4f46\u662f\u53c8\u9700\u8981\u63a7\u5236\u7cfb\u7edf\u6574\u4f53\u7684\u8fd0\u7b97\u91cf\u3002\u65f6\u5e8f\u7684\u5173\u8054\u4e0e\u7a7a\u95f4\u9700\u8981\u540c\u65f6\u987e\u8651\uff0c\u5374\u9700\u8981\u63a7\u5236\u6574\u4f53\u63a8\u7406\u65f6\u95f4\u3002\u8fd9\u4e5f\u662f\u672c\u6587\u5c1d\u8bd5\u8bbe\u8ba1\u7a81\u7834\u7684\u70b9\u3002 \u6700\u7ec8\u603b\u7ed3\u51fa\u4e86\u4e09\u4e2a\u6bd4\u8f83\u5173\u952e\u7684\u7ecf\u9a8c\uff1a \u6570\u636e\u7684\u5904\u7406\u4f7f\u7528\u65f6\u5e8f\u7684\u3001\u5c0f\u5355\u4f4d\u79bb\u6563\u7d2f\u79ef\u7684event maps \u4f7f\u7528\u5377\u79ef\u505a\u521d\u59cb\u5904\u7406\uff0c\u5e76\u4e14\u5728\u5377\u79ef\u8f93\u5165\u65f6\u6dfb\u52a0positional embedding, \u6ce8\u5165\u7edd\u5bf9\u4f4d\u7f6e\u4fe1\u606f\u3002\u9002\u5f53\u4e0b\u91c7\u6837\uff0c\u63a7\u5236\u540e\u7eed\u7279\u5f81\u5927\u5c0f\u3002 \u4f7f\u7528Multi-Axis Attention.\u4e00\u4e2a\u662f\u628a\u56fe\u7247\u5212\u4e3a\u4e00\u4e2a\u4e2ablock\uff0c\u5728block\u5185\u90e8transformer, \u4e00\u4e2a\u662f\u8de8block \u7528\u4e00\u4e2adilated \u7f51\u683c\u53d6\u70b9\uff0cdilated\u7f51\u683c\u4e0a\u7684\u70b9\u4e4b\u95f4 transformer\u3002 \u4f7f\u7528LSTM\u800c\u4e0d\u662fConv-LSTM\u5904\u7406\u65f6\u5e8f\u5173\u7cfb\u3002","title":"Recurrent Vision Transformers for Object Detection with Event Cameras"},{"location":"other_categories/object_detection_2D/Reppoints/","text":"RepPoints / Dense RepPoints / RepPointsV2 pdf1 code1 pdf2 code2 pdf3 code-all RepPoints reppoints\u8bba\u6587\u7684\u6838\u5fc3\u601d\u8def\u662f\u8ba9\u7f51\u7edc\u81ea\u9002\u5e94\u5730\u5b66\u4e60keypoints,\u5e76\u5229\u7528keypoints\u5206\u67902D\u6846\u7684\u4f4d\u7f6e\u4e0e\u5927\u5c0f\u3002\u8fd9\u91cc\u7275\u6d89\u7684\u96be\u70b9\u5728\u4e8ebbox detection\u4efb\u52a1\u4e2d\u662f\u6ca1\u6709keypoint\u8bbe\u7f6e\u7684\uff0c\u56e0\u4e3a\u7f51\u7edc\u9700\u8981\u81ea\u5df1\u53bb\u5b66keypoint\u7684\u4f4d\u7f6e\u3002 \u7f51\u7edc\u7ed3\u6784 # pts_out_init -> offsets fields from CNN pts_out_init_grad_mul = (1 - self.gradient_mul) * pts_out_init.detach() + self.gradient_mul * pts_out_init # a trick to wind down the gradient from DCN dcn_offset = pts_out_init_grad_mul - dcn_base_offset cls_out = self.reppoints_cls_out( self.relu(self.reppoints_cls_conv(cls_feat, dcn_offset))) # conv ( relu (deform_conv)) pts_out_refine = self.reppoints_pts_refine_out( self.relu(self.reppoints_pts_refine_conv(pts_feat, dcn_offset))) # conv ( relu (deform_conv)) \u8fd9\u91cc\u5b9e\u9645\u4e0a\u6709\u4e24\u6b21Ground Truth Assign\u7684\u8fc7\u7a0b\uff0c\u7b2c\u4e00\u6b21\u91c7\u7528\u7684K\u6700\u8fd1\u70b9\u7684\u5206\u914d,\u9996\u5148\u6839\u636e\u6846\u7684\u5927\u5c0f\u5206\u914d\u5230\u6307\u5b9a\u7684FPN Scale\u4e0a\uff0c\u7136\u540e\u5bfb\u627e\u4e2d\u70b9\u6700\u8fd1\u7684k(\u9ed8\u8ba4\u6b63\u6837\u672c)\u4e2a\u70b9\u7ed9\u6b63\u6837\u672c.\u7b2c\u4e8c\u6b21\u5219\u662f\u6839\u636e\u7b2c\u4e00\u9636\u6bb5\u7684\u9884\u6d4b\u6846\u4f5c\u4e3aanchor(\u8fd9\u6837\u6bcf\u4e00\u4e2ascale\u4e0a\u6bcf\u4e00\u4e2a\u70b9\u90fd\u6709\u4e00\u4e2aanchor),\u9009\u62e9IoU\u8d85\u8d8athreshold\u7684\u4f5c\u4e3a\u6b63\u6837\u672c\u3002\u6700\u540e\u5206\u7c7bloss\u6309\u7167\u5728\u7b2c\u4e8c\u6b21\u5206\u914d\u7684\u7ed3\u679c\u6765\u7b97\u3002 \u800c\u7531\u70b9\u5230bounding box\u7684\u7b97\u6cd5\u4f5c\u8005\u7ed9\u4e86\u4e09\u4e2a\u65b9\u6848\uff0c\u6700\u76f4\u89c2\u7684\u662f\u5176\u4e2d\u7684min-max\u65b9\u6848\u3002\u4f46\u662f\u6700\u540edefault\u7684\u662f\u81ea\u9002\u5e94\u7684 pts_y_mean = pts_y.mean(dim=1, keepdim=True) pts_x_mean = pts_x.mean(dim=1, keepdim=True) pts_y_std = torch.std(pts_y - pts_y_mean, dim=1, keepdim=True) pts_x_std = torch.std(pts_x - pts_x_mean, dim=1, keepdim=True) moment_transfer = (self.moment_transfer * self.moment_mul) + ( self.moment_transfer.detach() * (1 - self.moment_mul)) moment_width_transfer = moment_transfer[0] moment_height_transfer = moment_transfer[1] half_width = pts_x_std * torch.exp(moment_width_transfer) # \\lambda_x in paper, learnable parameters of the module half_height = pts_y_std * torch.exp(moment_height_transfer) bbox = torch.cat([ pts_x_mean - half_width, pts_y_mean - half_height, pts_x_mean + half_width, pts_y_mean + half_height ], dim=1) Dense RepPoints \u8fd9\u7bc7paper\u57282D bounding box detection\u7684\u57fa\u672c\u6982\u5ff5\u662f\u5728RepPoints\u7684\u57fa\u7840\u4e0a\u9884\u6d4b\u66f4\u591a\u7684representation points\uff0c \u540c\u65f6\u63d0\u51fa\u4e86 Group Pooling / Shared offset fields / Shared attribute map. (\u4ee3\u7801\u5b9e\u73b0\u4e0e\u4e0a\u9762\u4e00\u6837\u6bd4\u8f83\u6df7\u4e71, \u4e14Group Pooling\u4e0e\u6587\u4e2d\u56fe\u7247\u5b9e\u73b0\u7684\u6548\u679c\u6709\u5dee\u5f02\uff0c\u4ec5\u4f9b\u53c2\u8003) RepPoint2 v2 \u8fd9\u7bc7paper\u7684\u601d\u8def\u662f\u901a\u8fc7\u989d\u5916\u9884\u6d4b\u524d\u666f\u4ee5\u53ca\u89d2\u70b9\u7684\u9884\u6d4b\u6765\u63d0\u5347\u68c0\u6d4b\u7684\u51c6\u786e\u5ea6","title":"RepPoints / Dense RepPoints / RepPointsV2"},{"location":"other_categories/object_detection_2D/Reppoints/#reppoints-dense-reppoints-reppointsv2","text":"pdf1 code1 pdf2 code2 pdf3 code-all","title":"RepPoints / Dense RepPoints / RepPointsV2"},{"location":"other_categories/object_detection_2D/Reppoints/#reppoints","text":"reppoints\u8bba\u6587\u7684\u6838\u5fc3\u601d\u8def\u662f\u8ba9\u7f51\u7edc\u81ea\u9002\u5e94\u5730\u5b66\u4e60keypoints,\u5e76\u5229\u7528keypoints\u5206\u67902D\u6846\u7684\u4f4d\u7f6e\u4e0e\u5927\u5c0f\u3002\u8fd9\u91cc\u7275\u6d89\u7684\u96be\u70b9\u5728\u4e8ebbox detection\u4efb\u52a1\u4e2d\u662f\u6ca1\u6709keypoint\u8bbe\u7f6e\u7684\uff0c\u56e0\u4e3a\u7f51\u7edc\u9700\u8981\u81ea\u5df1\u53bb\u5b66keypoint\u7684\u4f4d\u7f6e\u3002","title":"RepPoints"},{"location":"other_categories/object_detection_2D/Reppoints/#_1","text":"# pts_out_init -> offsets fields from CNN pts_out_init_grad_mul = (1 - self.gradient_mul) * pts_out_init.detach() + self.gradient_mul * pts_out_init # a trick to wind down the gradient from DCN dcn_offset = pts_out_init_grad_mul - dcn_base_offset cls_out = self.reppoints_cls_out( self.relu(self.reppoints_cls_conv(cls_feat, dcn_offset))) # conv ( relu (deform_conv)) pts_out_refine = self.reppoints_pts_refine_out( self.relu(self.reppoints_pts_refine_conv(pts_feat, dcn_offset))) # conv ( relu (deform_conv)) \u8fd9\u91cc\u5b9e\u9645\u4e0a\u6709\u4e24\u6b21Ground Truth Assign\u7684\u8fc7\u7a0b\uff0c\u7b2c\u4e00\u6b21\u91c7\u7528\u7684K\u6700\u8fd1\u70b9\u7684\u5206\u914d,\u9996\u5148\u6839\u636e\u6846\u7684\u5927\u5c0f\u5206\u914d\u5230\u6307\u5b9a\u7684FPN Scale\u4e0a\uff0c\u7136\u540e\u5bfb\u627e\u4e2d\u70b9\u6700\u8fd1\u7684k(\u9ed8\u8ba4\u6b63\u6837\u672c)\u4e2a\u70b9\u7ed9\u6b63\u6837\u672c.\u7b2c\u4e8c\u6b21\u5219\u662f\u6839\u636e\u7b2c\u4e00\u9636\u6bb5\u7684\u9884\u6d4b\u6846\u4f5c\u4e3aanchor(\u8fd9\u6837\u6bcf\u4e00\u4e2ascale\u4e0a\u6bcf\u4e00\u4e2a\u70b9\u90fd\u6709\u4e00\u4e2aanchor),\u9009\u62e9IoU\u8d85\u8d8athreshold\u7684\u4f5c\u4e3a\u6b63\u6837\u672c\u3002\u6700\u540e\u5206\u7c7bloss\u6309\u7167\u5728\u7b2c\u4e8c\u6b21\u5206\u914d\u7684\u7ed3\u679c\u6765\u7b97\u3002 \u800c\u7531\u70b9\u5230bounding box\u7684\u7b97\u6cd5\u4f5c\u8005\u7ed9\u4e86\u4e09\u4e2a\u65b9\u6848\uff0c\u6700\u76f4\u89c2\u7684\u662f\u5176\u4e2d\u7684min-max\u65b9\u6848\u3002\u4f46\u662f\u6700\u540edefault\u7684\u662f\u81ea\u9002\u5e94\u7684 pts_y_mean = pts_y.mean(dim=1, keepdim=True) pts_x_mean = pts_x.mean(dim=1, keepdim=True) pts_y_std = torch.std(pts_y - pts_y_mean, dim=1, keepdim=True) pts_x_std = torch.std(pts_x - pts_x_mean, dim=1, keepdim=True) moment_transfer = (self.moment_transfer * self.moment_mul) + ( self.moment_transfer.detach() * (1 - self.moment_mul)) moment_width_transfer = moment_transfer[0] moment_height_transfer = moment_transfer[1] half_width = pts_x_std * torch.exp(moment_width_transfer) # \\lambda_x in paper, learnable parameters of the module half_height = pts_y_std * torch.exp(moment_height_transfer) bbox = torch.cat([ pts_x_mean - half_width, pts_y_mean - half_height, pts_x_mean + half_width, pts_y_mean + half_height ], dim=1)","title":"\u7f51\u7edc\u7ed3\u6784"},{"location":"other_categories/object_detection_2D/Reppoints/#dense-reppoints","text":"\u8fd9\u7bc7paper\u57282D bounding box detection\u7684\u57fa\u672c\u6982\u5ff5\u662f\u5728RepPoints\u7684\u57fa\u7840\u4e0a\u9884\u6d4b\u66f4\u591a\u7684representation points\uff0c \u540c\u65f6\u63d0\u51fa\u4e86 Group Pooling / Shared offset fields / Shared attribute map. (\u4ee3\u7801\u5b9e\u73b0\u4e0e\u4e0a\u9762\u4e00\u6837\u6bd4\u8f83\u6df7\u4e71, \u4e14Group Pooling\u4e0e\u6587\u4e2d\u56fe\u7247\u5b9e\u73b0\u7684\u6548\u679c\u6709\u5dee\u5f02\uff0c\u4ec5\u4f9b\u53c2\u8003)","title":"Dense RepPoints"},{"location":"other_categories/object_detection_2D/Reppoints/#reppoint2-v2","text":"\u8fd9\u7bc7paper\u7684\u601d\u8def\u662f\u901a\u8fc7\u989d\u5916\u9884\u6d4b\u524d\u666f\u4ee5\u53ca\u89d2\u70b9\u7684\u9884\u6d4b\u6765\u63d0\u5347\u68c0\u6d4b\u7684\u51c6\u786e\u5ea6","title":"RepPoint2 v2"},{"location":"other_categories/object_detection_2D/SSD/","text":"SSD: Single Shot MultiBox Detector \u8bad\u7ec3loss\u7531\u5b9a\u4f4d\u4e0e\u5206\u7c7b\u4e24\u4e2a\u90e8\u5206\u7ec4\u6210 \u5173\u4e8e\u5b9a\u4f4dloss: \u5206\u4e3a\u6846\u7684\u4e2d\u70b9\u7684x\uff0cy\u4e0e\u5bbd\u5ea6\u4e0e\u9ad8\u5ea6\u3002 x, y\u7684\u76ee\u6807\u503c\u4e3a(\u6ce8\u610f\u4e0d\u540c\u5206\u8fa8\u7387\u4e2d\u683c\u5b50\u7684\u5bbd\u5ea6\u662f\u4e0d\u540c\u7684) \\frac{\u5b9e\u9645\u4e2d\u70b9\u7684\u4f4d\u7f6e-\u683c\u5b50\u4e2d\u70b9\u7684\u4f4d\u7f6e}{\u683c\u5b50\u5bbd\u5ea6} \u800c\u5bbd\u5ea6\u4e0e\u9ad8\u5ea6\u7684\u76ee\u6807\u503c\u4e3a log\\frac{\u76ee\u6807\u957f\u5bbd}{\u683c\u5b50\u957f\u5bbd}","title":"SSD: Single Shot MultiBox Detector"},{"location":"other_categories/object_detection_2D/SSD/#ssd-single-shot-multibox-detector","text":"","title":"SSD: Single Shot MultiBox Detector"},{"location":"other_categories/object_detection_2D/SSD/#loss","text":"","title":"\u8bad\u7ec3loss\u7531\u5b9a\u4f4d\u4e0e\u5206\u7c7b\u4e24\u4e2a\u90e8\u5206\u7ec4\u6210"},{"location":"other_categories/object_detection_2D/SSD/#loss_1","text":"\u5206\u4e3a\u6846\u7684\u4e2d\u70b9\u7684x\uff0cy\u4e0e\u5bbd\u5ea6\u4e0e\u9ad8\u5ea6\u3002 x, y\u7684\u76ee\u6807\u503c\u4e3a(\u6ce8\u610f\u4e0d\u540c\u5206\u8fa8\u7387\u4e2d\u683c\u5b50\u7684\u5bbd\u5ea6\u662f\u4e0d\u540c\u7684) \\frac{\u5b9e\u9645\u4e2d\u70b9\u7684\u4f4d\u7f6e-\u683c\u5b50\u4e2d\u70b9\u7684\u4f4d\u7f6e}{\u683c\u5b50\u5bbd\u5ea6} \u800c\u5bbd\u5ea6\u4e0e\u9ad8\u5ea6\u7684\u76ee\u6807\u503c\u4e3a log\\frac{\u76ee\u6807\u957f\u5bbd}{\u683c\u5b50\u957f\u5bbd}","title":"\u5173\u4e8e\u5b9a\u4f4dloss:"},{"location":"other_categories/object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/","text":"ThunderNet: Towards Real-time Generic Object Detection on Mobile Devices \u8fd9\u7bc7\u8bba\u6587\u5c06\u4e00\u4e2atwo-stage\u7684detection\u7f51\u7edc\u5c3d\u53ef\u80fd\u5730\u8f7b\u91cf\u5316\u4ee5\u5b9e\u73b0\u5b9e\u65f6\u8ba1\u7b97\u7684\u76ee\u6807\uff0c\u8fd9\u7bc7\u6587\u7ae0 \u6709\u4e00\u7bc7\u5bf9\u8bfb\u8005\u5f88\u53cb\u597d\u7684Medieum\u89e3\u8bfb\uff0c\u8fd9\u91cc\u505a\u4e00\u4e2a\u5f15\u7528 . \u4e0e\u672c\u7f51\u9875\u5176\u4ed6\u6587\u7ae0\u4e00\u6837\uff0c\u672c\u6587\u4e0e\u5176\u76f8\u6bd4\u4f1a\u66f4\u6ce8\u91cd\u65b9\u6cd5\u7684\u89e3\u8bfb\u3002 \u603b\u4f53PipeLine \u5176\u4e2dSNet\u7f51\u7edc\u662f\u57fa\u4e8e ShuffleNetV2 .\u9ed8\u8ba4\u7684\u8f93\u5165\u5206\u8fa8\u7387\u662f 320\\times320 ,\u672c\u6587\u5728backbone\u65b9\u9762\u7684\u63d0\u5347\u4e3b\u8981\u662f\u63d0\u5347\u611f\u53d7\u573a\u7684\u5927\u5c0f\u3002 Context Enhancement Module \u8fd9\u4e00\u4e2a\u6a21\u578b\u7684\u8fd0\u7b97\u5728\u56fe\u4e2d\u8f83\u4e3a\u6e05\u695a\uff0c\u5b9e\u8d28\u4e0a\u5c31\u662f\u7528\u5c3d\u53ef\u80fd\u5c11\u7684\u8fd0\u7b97\u91cf\uff0c\u5b9e\u73b0\u8f83\u4e3a\u590d\u6742\u591a\u6837\u7684\u590d\u5408\u611f\u53d7\u91ce\u3002 Spatial Attention Module \u5728RoI resize\u4e4b\u524d\uff0c\u5b9e\u73b0\u5bf9channel\u7684\u4e00\u4e2areweights\u3002 \u5c06\u8fd9\u5f20\u56fe\u653e\u56depipeline\u56fe\u4e2d\u53ef\u4ee5\u770b\u5230\uff0c F^{RPN} \u662fCEM\u8f6c\u6362\u540e\u7684\u8f93\u51fa,\u4f5c\u4e3aRoIAlign\u7684\u6743\u91cd\u4ee5\u53cabase bounding box\u7684\u8f93\u51fa\u6e90\uff0c\u800c\u8fd9\u4e2a\u6a21\u5757\u6700\u540e\u7ed9\u51fa\u7684 F^{SAM} \u662fRoIAlign\u5bf9\u5e94bounding box\u53d6\u5185\u5bb9\u8f93\u5165\u5230\u4e0b\u4e00\u5c42\u7684\u8f93\u51fa\u6e90\u3002 \u5b9e\u9a8c\u7ed3\u679c \u6700\u6d45\u7684\u6a21\u5757\u6700\u7ec8\u80fd\u5728\u9a81\u9f99845\u4e0a\u8dd1\u523025hz\u3002","title":"ThunderNet: Towards Real-time Generic Object Detection on Mobile Devices"},{"location":"other_categories/object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/#thundernet-towards-real-time-generic-object-detection-on-mobile-devices","text":"\u8fd9\u7bc7\u8bba\u6587\u5c06\u4e00\u4e2atwo-stage\u7684detection\u7f51\u7edc\u5c3d\u53ef\u80fd\u5730\u8f7b\u91cf\u5316\u4ee5\u5b9e\u73b0\u5b9e\u65f6\u8ba1\u7b97\u7684\u76ee\u6807\uff0c\u8fd9\u7bc7\u6587\u7ae0 \u6709\u4e00\u7bc7\u5bf9\u8bfb\u8005\u5f88\u53cb\u597d\u7684Medieum\u89e3\u8bfb\uff0c\u8fd9\u91cc\u505a\u4e00\u4e2a\u5f15\u7528 . \u4e0e\u672c\u7f51\u9875\u5176\u4ed6\u6587\u7ae0\u4e00\u6837\uff0c\u672c\u6587\u4e0e\u5176\u76f8\u6bd4\u4f1a\u66f4\u6ce8\u91cd\u65b9\u6cd5\u7684\u89e3\u8bfb\u3002","title":"ThunderNet: Towards Real-time Generic Object Detection on Mobile Devices"},{"location":"other_categories/object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/#pipeline","text":"\u5176\u4e2dSNet\u7f51\u7edc\u662f\u57fa\u4e8e ShuffleNetV2 .\u9ed8\u8ba4\u7684\u8f93\u5165\u5206\u8fa8\u7387\u662f 320\\times320 ,\u672c\u6587\u5728backbone\u65b9\u9762\u7684\u63d0\u5347\u4e3b\u8981\u662f\u63d0\u5347\u611f\u53d7\u573a\u7684\u5927\u5c0f\u3002","title":"\u603b\u4f53PipeLine"},{"location":"other_categories/object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/#context-enhancement-module","text":"\u8fd9\u4e00\u4e2a\u6a21\u578b\u7684\u8fd0\u7b97\u5728\u56fe\u4e2d\u8f83\u4e3a\u6e05\u695a\uff0c\u5b9e\u8d28\u4e0a\u5c31\u662f\u7528\u5c3d\u53ef\u80fd\u5c11\u7684\u8fd0\u7b97\u91cf\uff0c\u5b9e\u73b0\u8f83\u4e3a\u590d\u6742\u591a\u6837\u7684\u590d\u5408\u611f\u53d7\u91ce\u3002","title":"Context Enhancement Module"},{"location":"other_categories/object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/#spatial-attention-module","text":"\u5728RoI resize\u4e4b\u524d\uff0c\u5b9e\u73b0\u5bf9channel\u7684\u4e00\u4e2areweights\u3002 \u5c06\u8fd9\u5f20\u56fe\u653e\u56depipeline\u56fe\u4e2d\u53ef\u4ee5\u770b\u5230\uff0c F^{RPN} \u662fCEM\u8f6c\u6362\u540e\u7684\u8f93\u51fa,\u4f5c\u4e3aRoIAlign\u7684\u6743\u91cd\u4ee5\u53cabase bounding box\u7684\u8f93\u51fa\u6e90\uff0c\u800c\u8fd9\u4e2a\u6a21\u5757\u6700\u540e\u7ed9\u51fa\u7684 F^{SAM} \u662fRoIAlign\u5bf9\u5e94bounding box\u53d6\u5185\u5bb9\u8f93\u5165\u5230\u4e0b\u4e00\u5c42\u7684\u8f93\u51fa\u6e90\u3002","title":"Spatial Attention Module"},{"location":"other_categories/object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/#_1","text":"\u6700\u6d45\u7684\u6a21\u5757\u6700\u7ec8\u80fd\u5728\u9a81\u9f99845\u4e0a\u8dd1\u523025hz\u3002","title":"\u5b9e\u9a8c\u7ed3\u679c"},{"location":"other_categories/object_detection_2D/YOLOv4/","text":"YOLOv4: Optimal Speed and Accuracy of Object Detection \u8fd9\u7bc7Yolov4\u662f\"\u5b98\u65b9\u7ee7\u627f\u8005\"\u7684\u5b98\u65b9paper,\u5927\u89c4\u6a21\u5730\u96c6\u6210\u4e86\u5404\u79cd\u5404\u6837\u7684trick\uff0c\u4e5f\u8c03\u6574\u4e86\u5f88\u591a\u6a21\u578b\u3002\u503c\u5f97\u5173\u6ce8\u7684\u5730\u65b9\u5728\u4e8e\u4f5c\u8005\u5f88\u4e13\u6ce8\u4e8e\u5728GPU\u4e0a\u7684\u8fd0\u7b97\u901f\u5ea6\u4ee5\u53ca\u5728GPU\u4e0a\u7684\u8bad\u7ec3\u53ef\u80fd\u6027\u3002Paper \u5bf9\u73b0\u6709Tricks\u505a\u4e86\u76f8\u5f53\u591a\u7684Review\u8ddf\u6d4b\u8bd5\u3002\u8fd9\u7bc7paper\u4ee5\u53ca\u8fd9\u4e2a\u9875\u9762\u53ef\u4ee5\u7b97\u662fobject detection tricks\u7684\u4e00\u4e2a\u5c0fWiki\u3002 Related Works Models \u8fd9\u91cc\u63d0\u4f9b\u4e00\u4e9b\u5df2\u6709\u7684\u7b80\u4ecb: Efficient ; SPP in PSM ; RFB ; SAM in CBAM ; SSD ; CornetNet ; CenterNet ; FCOS \u8fd9\u91cc\u8865\u5145\u4e24\u4e2a\u94fe\u63a5,ASPP \u6765\u81ea\u4e8e DeepLabv3 ; CSPNet pdf CSP\u7f51\u7edc\u6765\u81ea\u4e8e\u4f5c\u8005\u7684\u524d\u4e00\u6279\u5de5\u4f5c\u3002 Bag of Freebies \u4e00\u7cfb\u5217\u8bad\u7ec3trick\uff0c\u79f0\u4e3afreebies\u56e0\u4e3a\u4e0d\u589e\u52a0\u63a8\u7406\u6210\u672c. \u5148\u53c2\u8003Amazon\u7684paper\u7684 \u7b80\u4ecb \u4e3b\u8981\u5206\u7c7b\u4e3a input augmentation/feature map dropping; label-softing; Loss(IoU Loss). \u8fd9\u91cc\u8865\u5145\u7ed3\u679ctrick\u7684\u7b80\u4ecb\u7684\u94fe\u63a5\u3002 DropBlock ; MixUp ; label-smoothing ; Focal loss ; GIoU ; DIoU and CIoU \u6570\u636e\u589e\u5f3a\u793a\u4f8b Bag of specials \u4e00\u7cfb\u5217\u63d0\u5347\u7f51\u7edc\u6027\u80fd\u7684Module/necks/activation function/post-processing. \u4f5c\u8005\u8868\u793a\u4e00\u4e2a\u9b54\u6539\u540e\u7684SPP module\u5bf9\u6027\u80fd\u7684\u63d0\u5347\u5f88\u663e\u8457\u4e14\u8fd0\u7b97\u5dee\u8ddd\u4e0d\u5927\u3002 RFB\u539f\u6587\u4e2d\u6027\u80fd\u63d0\u5347\u5c1a\u53ef\uff0c\u4f46\u662f\u8fd0\u7b97\u65f6\u95f4\u7684\u63d0\u5347\u4e5f\u662f\u5f88\u663e\u8457\u7684\u3002 Squeeze-and-Excitation \u5c3d\u7ba1FLOPs\u63d0\u5347\u4e0d\u5927\uff0c\u4f46\u662f\u5728GPU\u63a8\u7406\u65f6\u95f4\u7684\u589e\u52a0\u4e0a\u662f\u76f8\u5bf9\u6bd4\u8f83\u5927\u7684\uff0c\u800c\u53e6\u5916 SAM \u6027\u80fd\u63d0\u5347\u7a0d\u5fae\u6ca1\u90a3\u4e48\u663e\u8457\uff0c\u4f46\u662fGPU\u8fd0\u7b97\u65f6\u95f4\u5dee\u8ddd\u4e0d\u5927\u3002 Mish pdf \u542f\u53d1\u4e0eSwish f(x) = x \\cdot sigmoid(x) Mish\u7684\u8ba1\u7b97 \\begin{aligned} &f(x)=x \\cdot \\tanh (\\zeta(x))\\\\ & \\zeta(x)=\\ln \\left(1+e^{x}\\right) \\rightarrow \\text{softplus} \\end{aligned} Mish\u4f5c\u8005\u6307\u51fa\u5efa\u8bae\u4f7f\u7528\u66f4\u4f4e\u7684learning rate. System Design \u4f5c\u8005\u7ed9\u51fa\u7ed3\u8bba\uff0c\u4e3a\u4e86\u63d0\u5347\u6027\u80fd\uff0c\u9700\u8981: 1. \u8f93\u5165\u56fe\u7247\u5206\u8fa8\u7387\u5e94\u8be5\u6bd4\u8f83\u5927 2. \u7f51\u7edc\u5c42\u6570\u5927\uff0c\u611f\u53d7\u91ce\u5927 3. \u7f51\u7edc\u53c2\u6570\u8981\u8db3\u591f\u591a\u3002 neck\u9009\u62e9\u7684\u662fPANet PANet pdf \u4e00\u4e2a\u6bd4\u8f83\u597d\u7684 \u7b80\u4ecb\u5728CSDN . \u989d\u5916\u63d0\u5347trick Mosaic \u6570\u636e\u589e\u5f3atrick\uff0c\u9576\u5d4c; \u5982\u56fe: Self-Adversarital Training \u65b0\u7684\u8bad\u7ec3\u53ca\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u4e00\u6b21\u8bad\u7ec3\u9700\u8981\u4e24\u6b21Forward backward.\u7b2c\u4e00\u4e2astage\uff0c\u795e\u7ecf\u7f51\u7edc\u4f1a\u4fee\u6539\u539f\u56fe\u800c\u4e0d\u662f\u7f51\u7edc\u6743\u91cd\uff08\u76f8\u5f53\u4e8e\u5bf9\u6297\u6027\u5730\u4fee\u6539\u539f\u56fe\u7247\uff09.\u7b2c\u4e8c\u4e2astage\uff0c\u624d\u6b63\u5e38\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u3002 Cross mini-Batch Normalization CmBN\u542f\u53d1\u4e8eminibatch-Batchnorm(\u4e00\u4e2abatch\u5206\u51e0\u4e2amini-batch)\u4ee5\u53ca Cross iteration BatchNorm Modified SAM and PAN Over all \u8865\u5145\u8bf4\u660e Eliminate grid sensitivity \u5728\u9884\u6d4b2D bounding box\u4e2d\u5fc3\u70b9\u65f6\uff0c\u4e00\u4e2a\u5199\u6cd5\u662f bx=\\sigma(t_x) + cx ,\u5176\u4e2d \\sigma(t_x) \u662f\u9884\u6d4b\u7684\u4e2d\u5fc3\u70b9\u4e0eanchor\u7684\u504f\u79fb\u3002\u5f53 \\sigma(t_x) \u63a5\u8fd10/1\u65f6\uff0c\u7f51\u7edc\u8f93\u51fa\u503c\u4f1a\u5f88\u5927\uff0c\u4e0d\u4fbf\u4e8e\uff0c\u5b66\u4e60\uff0c\u6240\u4ee5\u4f5c\u8005\u5728sigmoid\u524d\u9762\u4e58\u4e86\u4e00\u4e2a\u5927\u4e8e1.0\u7684factor\u3002 Genetic algorithms for optimal hyperparameters \u8fd9\u4e2a\u5b9e\u73b0\u7ec6\u8282\u6709\u5f85\u89c2\u5bdf\uff0c\u610f\u601d\u662f\u5728training\u524d\u671f\uff0c\u4f7f\u7528GA\u9009\u62e9\u6700\u4f18\u7684\u8d85\u53c2\uff0c\u6bd4\u5982\u5b66\u4e60\u7387\u7b49\u3002 \u5b9e\u9a8c\u5bf9\u6bd4\u5404\u4e2atrick Bag of Freebies on Classifier \u6700\u7ec8\u53d1\u73b0CutMix, Mosaic, Label Smoothing, Mish\u6709\u7528\u3002 Bag of Freebies on Detector \u4e2a\u4eba\u89c2\u5bdf\u6765\u6bd4\u8f83\u6709\u6548\u679c\u800c\u4e14\u7279\u6b8a\u7684\u662fMosaic\u6709\u6548\uff0cGA\u6709\u6548\uff0cGIoU\u6216\u8005CIoU\u6709\u6548\u3002 \u6700\u7ec8\u7ecf\u8fc7\u9009\u62e9\u4e0e\u5904\u7406\u540e\uff0c\u5de5\u7a0b\u5ea6\u6781\u9ad8\u7684Yolov4\u5728\u5f88\u9ad8\u7684FPS\u7684\u57fa\u7840\u4e0a\u8dd1\u51fa\u4e86\u6bd4\u8f83\u9ad8\u7684\u7cbe\u786e\u5ea6(\u7cbe\u5ea6\u9ad8\u4e8eYolov3, \u901f\u5ea6\u5feb\u4e8eEfficientDet)","title":"YOLOv4: Optimal Speed and Accuracy of Object Detection"},{"location":"other_categories/object_detection_2D/YOLOv4/#yolov4-optimal-speed-and-accuracy-of-object-detection","text":"\u8fd9\u7bc7Yolov4\u662f\"\u5b98\u65b9\u7ee7\u627f\u8005\"\u7684\u5b98\u65b9paper,\u5927\u89c4\u6a21\u5730\u96c6\u6210\u4e86\u5404\u79cd\u5404\u6837\u7684trick\uff0c\u4e5f\u8c03\u6574\u4e86\u5f88\u591a\u6a21\u578b\u3002\u503c\u5f97\u5173\u6ce8\u7684\u5730\u65b9\u5728\u4e8e\u4f5c\u8005\u5f88\u4e13\u6ce8\u4e8e\u5728GPU\u4e0a\u7684\u8fd0\u7b97\u901f\u5ea6\u4ee5\u53ca\u5728GPU\u4e0a\u7684\u8bad\u7ec3\u53ef\u80fd\u6027\u3002Paper \u5bf9\u73b0\u6709Tricks\u505a\u4e86\u76f8\u5f53\u591a\u7684Review\u8ddf\u6d4b\u8bd5\u3002\u8fd9\u7bc7paper\u4ee5\u53ca\u8fd9\u4e2a\u9875\u9762\u53ef\u4ee5\u7b97\u662fobject detection tricks\u7684\u4e00\u4e2a\u5c0fWiki\u3002","title":"YOLOv4: Optimal Speed and Accuracy of Object Detection"},{"location":"other_categories/object_detection_2D/YOLOv4/#related-works","text":"","title":"Related Works"},{"location":"other_categories/object_detection_2D/YOLOv4/#models","text":"\u8fd9\u91cc\u63d0\u4f9b\u4e00\u4e9b\u5df2\u6709\u7684\u7b80\u4ecb: Efficient ; SPP in PSM ; RFB ; SAM in CBAM ; SSD ; CornetNet ; CenterNet ; FCOS \u8fd9\u91cc\u8865\u5145\u4e24\u4e2a\u94fe\u63a5,ASPP \u6765\u81ea\u4e8e DeepLabv3 ;","title":"Models"},{"location":"other_categories/object_detection_2D/YOLOv4/#cspnet","text":"pdf CSP\u7f51\u7edc\u6765\u81ea\u4e8e\u4f5c\u8005\u7684\u524d\u4e00\u6279\u5de5\u4f5c\u3002","title":"CSPNet"},{"location":"other_categories/object_detection_2D/YOLOv4/#bag-of-freebies","text":"\u4e00\u7cfb\u5217\u8bad\u7ec3trick\uff0c\u79f0\u4e3afreebies\u56e0\u4e3a\u4e0d\u589e\u52a0\u63a8\u7406\u6210\u672c. \u5148\u53c2\u8003Amazon\u7684paper\u7684 \u7b80\u4ecb \u4e3b\u8981\u5206\u7c7b\u4e3a input augmentation/feature map dropping; label-softing; Loss(IoU Loss). \u8fd9\u91cc\u8865\u5145\u7ed3\u679ctrick\u7684\u7b80\u4ecb\u7684\u94fe\u63a5\u3002 DropBlock ; MixUp ; label-smoothing ; Focal loss ; GIoU ; DIoU and CIoU \u6570\u636e\u589e\u5f3a\u793a\u4f8b","title":"Bag of Freebies"},{"location":"other_categories/object_detection_2D/YOLOv4/#bag-of-specials","text":"\u4e00\u7cfb\u5217\u63d0\u5347\u7f51\u7edc\u6027\u80fd\u7684Module/necks/activation function/post-processing. \u4f5c\u8005\u8868\u793a\u4e00\u4e2a\u9b54\u6539\u540e\u7684SPP module\u5bf9\u6027\u80fd\u7684\u63d0\u5347\u5f88\u663e\u8457\u4e14\u8fd0\u7b97\u5dee\u8ddd\u4e0d\u5927\u3002 RFB\u539f\u6587\u4e2d\u6027\u80fd\u63d0\u5347\u5c1a\u53ef\uff0c\u4f46\u662f\u8fd0\u7b97\u65f6\u95f4\u7684\u63d0\u5347\u4e5f\u662f\u5f88\u663e\u8457\u7684\u3002 Squeeze-and-Excitation \u5c3d\u7ba1FLOPs\u63d0\u5347\u4e0d\u5927\uff0c\u4f46\u662f\u5728GPU\u63a8\u7406\u65f6\u95f4\u7684\u589e\u52a0\u4e0a\u662f\u76f8\u5bf9\u6bd4\u8f83\u5927\u7684\uff0c\u800c\u53e6\u5916 SAM \u6027\u80fd\u63d0\u5347\u7a0d\u5fae\u6ca1\u90a3\u4e48\u663e\u8457\uff0c\u4f46\u662fGPU\u8fd0\u7b97\u65f6\u95f4\u5dee\u8ddd\u4e0d\u5927\u3002","title":"Bag of specials"},{"location":"other_categories/object_detection_2D/YOLOv4/#mish","text":"pdf \u542f\u53d1\u4e0eSwish f(x) = x \\cdot sigmoid(x) Mish\u7684\u8ba1\u7b97 \\begin{aligned} &f(x)=x \\cdot \\tanh (\\zeta(x))\\\\ & \\zeta(x)=\\ln \\left(1+e^{x}\\right) \\rightarrow \\text{softplus} \\end{aligned} Mish\u4f5c\u8005\u6307\u51fa\u5efa\u8bae\u4f7f\u7528\u66f4\u4f4e\u7684learning rate.","title":"Mish"},{"location":"other_categories/object_detection_2D/YOLOv4/#system-design","text":"\u4f5c\u8005\u7ed9\u51fa\u7ed3\u8bba\uff0c\u4e3a\u4e86\u63d0\u5347\u6027\u80fd\uff0c\u9700\u8981: 1. \u8f93\u5165\u56fe\u7247\u5206\u8fa8\u7387\u5e94\u8be5\u6bd4\u8f83\u5927 2. \u7f51\u7edc\u5c42\u6570\u5927\uff0c\u611f\u53d7\u91ce\u5927 3. \u7f51\u7edc\u53c2\u6570\u8981\u8db3\u591f\u591a\u3002 neck\u9009\u62e9\u7684\u662fPANet","title":"System Design"},{"location":"other_categories/object_detection_2D/YOLOv4/#panet","text":"pdf \u4e00\u4e2a\u6bd4\u8f83\u597d\u7684 \u7b80\u4ecb\u5728CSDN .","title":"PANet"},{"location":"other_categories/object_detection_2D/YOLOv4/#trick","text":"","title":"\u989d\u5916\u63d0\u5347trick"},{"location":"other_categories/object_detection_2D/YOLOv4/#mosaic","text":"\u6570\u636e\u589e\u5f3atrick\uff0c\u9576\u5d4c; \u5982\u56fe:","title":"Mosaic"},{"location":"other_categories/object_detection_2D/YOLOv4/#self-adversarital-training","text":"\u65b0\u7684\u8bad\u7ec3\u53ca\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u4e00\u6b21\u8bad\u7ec3\u9700\u8981\u4e24\u6b21Forward backward.\u7b2c\u4e00\u4e2astage\uff0c\u795e\u7ecf\u7f51\u7edc\u4f1a\u4fee\u6539\u539f\u56fe\u800c\u4e0d\u662f\u7f51\u7edc\u6743\u91cd\uff08\u76f8\u5f53\u4e8e\u5bf9\u6297\u6027\u5730\u4fee\u6539\u539f\u56fe\u7247\uff09.\u7b2c\u4e8c\u4e2astage\uff0c\u624d\u6b63\u5e38\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u3002","title":"Self-Adversarital Training"},{"location":"other_categories/object_detection_2D/YOLOv4/#cross-mini-batch-normalization","text":"CmBN\u542f\u53d1\u4e8eminibatch-Batchnorm(\u4e00\u4e2abatch\u5206\u51e0\u4e2amini-batch)\u4ee5\u53ca Cross iteration BatchNorm","title":"Cross mini-Batch Normalization"},{"location":"other_categories/object_detection_2D/YOLOv4/#modified-sam-and-pan","text":"","title":"Modified SAM and PAN"},{"location":"other_categories/object_detection_2D/YOLOv4/#over-all","text":"\u8865\u5145\u8bf4\u660e","title":"Over all"},{"location":"other_categories/object_detection_2D/YOLOv4/#eliminate-grid-sensitivity","text":"\u5728\u9884\u6d4b2D bounding box\u4e2d\u5fc3\u70b9\u65f6\uff0c\u4e00\u4e2a\u5199\u6cd5\u662f bx=\\sigma(t_x) + cx ,\u5176\u4e2d \\sigma(t_x) \u662f\u9884\u6d4b\u7684\u4e2d\u5fc3\u70b9\u4e0eanchor\u7684\u504f\u79fb\u3002\u5f53 \\sigma(t_x) \u63a5\u8fd10/1\u65f6\uff0c\u7f51\u7edc\u8f93\u51fa\u503c\u4f1a\u5f88\u5927\uff0c\u4e0d\u4fbf\u4e8e\uff0c\u5b66\u4e60\uff0c\u6240\u4ee5\u4f5c\u8005\u5728sigmoid\u524d\u9762\u4e58\u4e86\u4e00\u4e2a\u5927\u4e8e1.0\u7684factor\u3002","title":"Eliminate grid sensitivity"},{"location":"other_categories/object_detection_2D/YOLOv4/#genetic-algorithms-for-optimal-hyperparameters","text":"\u8fd9\u4e2a\u5b9e\u73b0\u7ec6\u8282\u6709\u5f85\u89c2\u5bdf\uff0c\u610f\u601d\u662f\u5728training\u524d\u671f\uff0c\u4f7f\u7528GA\u9009\u62e9\u6700\u4f18\u7684\u8d85\u53c2\uff0c\u6bd4\u5982\u5b66\u4e60\u7387\u7b49\u3002","title":"Genetic algorithms for optimal hyperparameters"},{"location":"other_categories/object_detection_2D/YOLOv4/#trick_1","text":"","title":"\u5b9e\u9a8c\u5bf9\u6bd4\u5404\u4e2atrick"},{"location":"other_categories/object_detection_2D/YOLOv4/#bag-of-freebies-on-classifier","text":"\u6700\u7ec8\u53d1\u73b0CutMix, Mosaic, Label Smoothing, Mish\u6709\u7528\u3002","title":"Bag of Freebies on Classifier"},{"location":"other_categories/object_detection_2D/YOLOv4/#bag-of-freebies-on-detector","text":"\u4e2a\u4eba\u89c2\u5bdf\u6765\u6bd4\u8f83\u6709\u6548\u679c\u800c\u4e14\u7279\u6b8a\u7684\u662fMosaic\u6709\u6548\uff0cGA\u6709\u6548\uff0cGIoU\u6216\u8005CIoU\u6709\u6548\u3002 \u6700\u7ec8\u7ecf\u8fc7\u9009\u62e9\u4e0e\u5904\u7406\u540e\uff0c\u5de5\u7a0b\u5ea6\u6781\u9ad8\u7684Yolov4\u5728\u5f88\u9ad8\u7684FPS\u7684\u57fa\u7840\u4e0a\u8dd1\u51fa\u4e86\u6bd4\u8f83\u9ad8\u7684\u7cbe\u786e\u5ea6(\u7cbe\u5ea6\u9ad8\u4e8eYolov3, \u901f\u5ea6\u5feb\u4e8eEfficientDet)","title":"Bag of Freebies on Detector"},{"location":"other_categories/object_detection_2D/aLRPLoss/","text":"A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection \u8fd9\u7bc7paper\u5728 AP loss \u7684\u57fa\u7840\u4e0a\u63d0\u51fa\u4e86\u66f4\u5e7f\u4e49\u7684\u4f18\u5316\u67b6\u6784\u4ee5\u53ca average Localization-Recall-Precision (aLRP) loss. \u7531\u540d\u5b57\u53ef\u77e5\uff0caLRP loss \u7684\u4e3b\u8981motivation\u5728\u4e8e\u4f7f\u7528\u5355\u4e00\u4e00\u4e2a rank-based loss \u540c\u65f6\u4f18\u5316\u5206\u7c7b\u4e0e\u56de\u5f52\u3002 AP loss \u662f\u53ea\u5904\u7406\u56de\u5f52\u95ee\u9898\u7684\u3002 \u7531\u8fd9\u5f20\u56fe\u53ef\u4ee5\u53d1\u73b0\uff0c\u76ee\u524d\u76ee\u6807\u68c0\u6d4b\u7684\u76ee\u6807\u662f\u5e0c\u671b\u56de\u5f52\u6570\u503c\u66f4\u51c6\u786e\u7684\u6837\u672c\u7f6e\u4fe1\u5ea6\u66f4\u9ad8\uff0c\u8fd9\u6837\u7684\u8bdd\u53ef\u4ee5\u63d0\u5347\u66f4\u9ad8IoU threshold\u65f6\u7684AP. \u4e5f\u5c31\u80fd\u63d0\u5347\u5e73\u5747AP. \u4f5c\u8005\u8fd9\u91cc\u7ed9\u51fa\u7684\u4e00\u4e2a\u7279\u6b8a\u6848\u4f8b\u8bf4\u660e\u5f53\u524d\u7684\u5404\u79cdloss\u90fd\u6ca1\u6709\u529e\u6cd5\u5f15\u5bfc\u7f51\u7edc\u50cf R_1 \u4e2d\u7684\u5408\u7406\u6392\u5e03\u9760\u62e2\u3002\u800c\u672c\u6587\u63d0\u51fa\u7684 aLRP loss \u662f\u8026\u5408\u4e86\u5206\u7c7b\u6392\u5e8f\u4e0eIoU\u7684\u635f\u5931\uff0c\u56e0\u800c\u80fd\u533a\u5206\u8fd9\u79cd\u60c5\u51b5\u3002 Review AP Loss: \\begin{array}{l} \\mathcal{L}_{A P}=1-\\mathrm{AP}=1-\\frac{1}{|\\mathcal{P}|} \\sum_{i \\in \\mathcal{P}} \\frac{\\operatorname{rank}^{+}(i)}{\\operatorname{rank}(i)} \\\\ =1-\\frac{1}{|\\mathcal{P}|} \\sum_{i \\in \\mathcal{P}} \\frac{1+\\sum_{j \\in \\mathcal{P}, j \\neq i} H\\left(x_{i j}\\right)}{1+\\sum_{j \\in \\mathcal{P}, j \\neq i} H\\left(x_{i j}\\right)+\\sum_{j \\in \\mathcal{N}} H\\left(x_{i j}\\right)} \\\\ =\\frac{1}{|\\mathcal{P}|} \\sum_{i \\in \\mathcal{P}} \\sum_{j \\in \\mathcal{N}} L_{i j} \\end{array} Localization-Recall-Precision (LRP) Performance Metric: \\operatorname{LRP}(s)=\\frac{1}{N_{F P}+N_{F N}+N_{T P}}\\left(N_{F P}+N_{F N}+\\sum_{k \\in T P} \\mathcal{E}_{\\operatorname{loc}}(k)\\right) \u5176\u4e2d \\mathcal{E}_{loc}(k) =\\frac{1 - IoU(k)}{1 - \\tau} , \\tau \u4e00\u822c\u662f\u6b63\u8d1f\u6837\u672c\u7684threshold\uff0c\u4e00\u822c\u53d6 \\tau = 0.5 \u8fd9\u4e2aMetric \u4e0e AP Loss \u4e00\u6837\uff0c\u90fd\u662f\u8d8a\u5c0f\u8d8a\u597d\u7684\u3002\u5982\u679c IoU \u90fd\u4e3a 1\uff0c \u5219\u8fd9\u4e2aloss\u4e0eAP loss \u7684\u8868\u8fbe\u5f0f\u57fa\u672c\u4e00\u81f4\u3002 \u66f4\u901a\u7528\u7684 Error-Driving Optimization \u4f5c\u8005\u6307\u51fa\uff0c\u5c06 AP loss \u7684\u4f18\u5316\u65b9\u6cd5\u6269\u5c55\u5230\u4e0d\u540c\u7684 ranking-based loss\u4e0a\u5e76\u4e0d\u7b80\u5355\u3002 \u4f5c\u8005\u8fd9\u91cc\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u5b9a\u4e49\u65b9\u6848\uff1a ranking-based loss function \\mathcal{L} = \\frac{1}{Z} \\sum_{i\\in \\mathcal{P}}\\ell(i) \u5b9a\u4e49\u4e3a\u5bf9\u7531\u4e00\u7cfb\u5217\u6b63\u6837\u672c\u5bf9\u5e94\u7684loss. Primary term L_{i j}=\\left\\{\\begin{array}{ll}\\ell(i) p(j \\mid i), & \\text { for } i \\in \\mathcal{P}, j \\in \\mathcal{N} \\\\ 0, & \\text { otherwise }\\end{array}\\right. \u8fd9\u4e2a\u5b9a\u4e49\u4e0b\uff0c\u6709 \u5b9a\u7406\u4e00\uff1a \\mathcal{L}=\\frac{1}{Z} \\sum_{i \\in \\mathcal{P}} \\ell(i)=\\frac{1}{Z} \\sum_{i \\in \\mathcal{P}} \\sum_{j \\in \\mathcal{N}} L_{i j} \u5b9a\u7406\u4e8c\uff1a \\sum_{i \\in \\mathcal{P}}\\left|\\frac{\\partial \\mathcal{L}}{\\partial s_{i}}\\right|=\\sum_{i \\in \\mathcal{N}}\\left|\\frac{\\partial \\mathcal{L}}{\\partial s_{i}}\\right| AP Loss. \\ell^{AP}(i) = 1 - precision(i) = N_{FP}(i) / rank(i) , \u4e14 p(j|i) = H(x_{ij}) / N_{FP}(i) aLRP Loss \\ell^{\\mathrm{LRP}}(i)=\\frac{1}{\\operatorname{rank}(\\mathrm{i})}\\left(N_{F P}(i)+\\mathcal{E}_{\\operatorname{loc}}(i)+\\sum_{k \\in \\mathcal{P}, k \\neq i} \\mathcal{E}_{\\operatorname{loc}}(k) H\\left(x_{i k}\\right)\\right) \u8fd9\u53ef\u4ee5\u88ab\u5206\u6210\u4e24\u9879 \\mathcal{L}_{c l s}^{\\mathrm{aLRP}}=\\frac{1}{|\\mathcal{P}|} \\sum_{i \\in \\mathcal{P}} \\frac{N_{F P}(i)}{\\operatorname{rank}(i)} \\mathcal{L}_{l o c}^{\\mathrm{aLRP}}=\\frac{1}{|\\mathcal{P}|} \\sum_{i \\in \\mathcal{P}} \\frac{1}{\\operatorname{rank}(i)}\\left(\\mathcal{E}_{l o c}(i)+\\sum_{k \\in \\mathcal{P}, k \\neq i} \\mathcal{E}_{\\text {loc}}(k) H\\left(x_{i k}\\right)\\right) \u4f18\u5316\u65b9\u6cd5: \\Delta x_{i j}=L_{i j}^{\\mathrm{aLRP}^{*}}-L_{i j}^{\\mathrm{aLRP}}=-\\frac{1}{\\operatorname{rank}(i)}\\left(N_{F P}(i)+\\sum_{k \\in \\mathcal{P}, k \\neq i} \\mathcal{E}_{\\text {loc}}(k) H\\left(x_{i k}\\right)\\right) \\frac{H\\left(x_{i j}\\right)}{N_{F P}(i)} \u5b9e\u9a8c\u4e0a\u5f97\u5230\u4e86\u6bd4\u6240\u6709one-stage detector\u66f4\u9ad8\u7684\u6027\u80fd(\u76f4\u63a5\u63d0\u53474\u70b9mAP)\u3002","title":"A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection"},{"location":"other_categories/object_detection_2D/aLRPLoss/#a-ranking-based-balanced-loss-function-unifying-classification-and-localisation-in-object-detection","text":"\u8fd9\u7bc7paper\u5728 AP loss \u7684\u57fa\u7840\u4e0a\u63d0\u51fa\u4e86\u66f4\u5e7f\u4e49\u7684\u4f18\u5316\u67b6\u6784\u4ee5\u53ca average Localization-Recall-Precision (aLRP) loss. \u7531\u540d\u5b57\u53ef\u77e5\uff0caLRP loss \u7684\u4e3b\u8981motivation\u5728\u4e8e\u4f7f\u7528\u5355\u4e00\u4e00\u4e2a rank-based loss \u540c\u65f6\u4f18\u5316\u5206\u7c7b\u4e0e\u56de\u5f52\u3002 AP loss \u662f\u53ea\u5904\u7406\u56de\u5f52\u95ee\u9898\u7684\u3002 \u7531\u8fd9\u5f20\u56fe\u53ef\u4ee5\u53d1\u73b0\uff0c\u76ee\u524d\u76ee\u6807\u68c0\u6d4b\u7684\u76ee\u6807\u662f\u5e0c\u671b\u56de\u5f52\u6570\u503c\u66f4\u51c6\u786e\u7684\u6837\u672c\u7f6e\u4fe1\u5ea6\u66f4\u9ad8\uff0c\u8fd9\u6837\u7684\u8bdd\u53ef\u4ee5\u63d0\u5347\u66f4\u9ad8IoU threshold\u65f6\u7684AP. \u4e5f\u5c31\u80fd\u63d0\u5347\u5e73\u5747AP. \u4f5c\u8005\u8fd9\u91cc\u7ed9\u51fa\u7684\u4e00\u4e2a\u7279\u6b8a\u6848\u4f8b\u8bf4\u660e\u5f53\u524d\u7684\u5404\u79cdloss\u90fd\u6ca1\u6709\u529e\u6cd5\u5f15\u5bfc\u7f51\u7edc\u50cf R_1 \u4e2d\u7684\u5408\u7406\u6392\u5e03\u9760\u62e2\u3002\u800c\u672c\u6587\u63d0\u51fa\u7684 aLRP loss \u662f\u8026\u5408\u4e86\u5206\u7c7b\u6392\u5e8f\u4e0eIoU\u7684\u635f\u5931\uff0c\u56e0\u800c\u80fd\u533a\u5206\u8fd9\u79cd\u60c5\u51b5\u3002","title":"A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection"},{"location":"other_categories/object_detection_2D/aLRPLoss/#review","text":"AP Loss: \\begin{array}{l} \\mathcal{L}_{A P}=1-\\mathrm{AP}=1-\\frac{1}{|\\mathcal{P}|} \\sum_{i \\in \\mathcal{P}} \\frac{\\operatorname{rank}^{+}(i)}{\\operatorname{rank}(i)} \\\\ =1-\\frac{1}{|\\mathcal{P}|} \\sum_{i \\in \\mathcal{P}} \\frac{1+\\sum_{j \\in \\mathcal{P}, j \\neq i} H\\left(x_{i j}\\right)}{1+\\sum_{j \\in \\mathcal{P}, j \\neq i} H\\left(x_{i j}\\right)+\\sum_{j \\in \\mathcal{N}} H\\left(x_{i j}\\right)} \\\\ =\\frac{1}{|\\mathcal{P}|} \\sum_{i \\in \\mathcal{P}} \\sum_{j \\in \\mathcal{N}} L_{i j} \\end{array} Localization-Recall-Precision (LRP) Performance Metric: \\operatorname{LRP}(s)=\\frac{1}{N_{F P}+N_{F N}+N_{T P}}\\left(N_{F P}+N_{F N}+\\sum_{k \\in T P} \\mathcal{E}_{\\operatorname{loc}}(k)\\right) \u5176\u4e2d \\mathcal{E}_{loc}(k) =\\frac{1 - IoU(k)}{1 - \\tau} , \\tau \u4e00\u822c\u662f\u6b63\u8d1f\u6837\u672c\u7684threshold\uff0c\u4e00\u822c\u53d6 \\tau = 0.5 \u8fd9\u4e2aMetric \u4e0e AP Loss \u4e00\u6837\uff0c\u90fd\u662f\u8d8a\u5c0f\u8d8a\u597d\u7684\u3002\u5982\u679c IoU \u90fd\u4e3a 1\uff0c \u5219\u8fd9\u4e2aloss\u4e0eAP loss \u7684\u8868\u8fbe\u5f0f\u57fa\u672c\u4e00\u81f4\u3002","title":"Review"},{"location":"other_categories/object_detection_2D/aLRPLoss/#error-driving-optimization","text":"\u4f5c\u8005\u6307\u51fa\uff0c\u5c06 AP loss \u7684\u4f18\u5316\u65b9\u6cd5\u6269\u5c55\u5230\u4e0d\u540c\u7684 ranking-based loss\u4e0a\u5e76\u4e0d\u7b80\u5355\u3002 \u4f5c\u8005\u8fd9\u91cc\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u5b9a\u4e49\u65b9\u6848\uff1a ranking-based loss function \\mathcal{L} = \\frac{1}{Z} \\sum_{i\\in \\mathcal{P}}\\ell(i) \u5b9a\u4e49\u4e3a\u5bf9\u7531\u4e00\u7cfb\u5217\u6b63\u6837\u672c\u5bf9\u5e94\u7684loss. Primary term L_{i j}=\\left\\{\\begin{array}{ll}\\ell(i) p(j \\mid i), & \\text { for } i \\in \\mathcal{P}, j \\in \\mathcal{N} \\\\ 0, & \\text { otherwise }\\end{array}\\right. \u8fd9\u4e2a\u5b9a\u4e49\u4e0b\uff0c\u6709 \u5b9a\u7406\u4e00\uff1a \\mathcal{L}=\\frac{1}{Z} \\sum_{i \\in \\mathcal{P}} \\ell(i)=\\frac{1}{Z} \\sum_{i \\in \\mathcal{P}} \\sum_{j \\in \\mathcal{N}} L_{i j} \u5b9a\u7406\u4e8c\uff1a \\sum_{i \\in \\mathcal{P}}\\left|\\frac{\\partial \\mathcal{L}}{\\partial s_{i}}\\right|=\\sum_{i \\in \\mathcal{N}}\\left|\\frac{\\partial \\mathcal{L}}{\\partial s_{i}}\\right| AP Loss. \\ell^{AP}(i) = 1 - precision(i) = N_{FP}(i) / rank(i) , \u4e14 p(j|i) = H(x_{ij}) / N_{FP}(i)","title":"\u66f4\u901a\u7528\u7684 Error-Driving Optimization"},{"location":"other_categories/object_detection_2D/aLRPLoss/#alrp-loss","text":"\\ell^{\\mathrm{LRP}}(i)=\\frac{1}{\\operatorname{rank}(\\mathrm{i})}\\left(N_{F P}(i)+\\mathcal{E}_{\\operatorname{loc}}(i)+\\sum_{k \\in \\mathcal{P}, k \\neq i} \\mathcal{E}_{\\operatorname{loc}}(k) H\\left(x_{i k}\\right)\\right) \u8fd9\u53ef\u4ee5\u88ab\u5206\u6210\u4e24\u9879 \\mathcal{L}_{c l s}^{\\mathrm{aLRP}}=\\frac{1}{|\\mathcal{P}|} \\sum_{i \\in \\mathcal{P}} \\frac{N_{F P}(i)}{\\operatorname{rank}(i)} \\mathcal{L}_{l o c}^{\\mathrm{aLRP}}=\\frac{1}{|\\mathcal{P}|} \\sum_{i \\in \\mathcal{P}} \\frac{1}{\\operatorname{rank}(i)}\\left(\\mathcal{E}_{l o c}(i)+\\sum_{k \\in \\mathcal{P}, k \\neq i} \\mathcal{E}_{\\text {loc}}(k) H\\left(x_{i k}\\right)\\right) \u4f18\u5316\u65b9\u6cd5: \\Delta x_{i j}=L_{i j}^{\\mathrm{aLRP}^{*}}-L_{i j}^{\\mathrm{aLRP}}=-\\frac{1}{\\operatorname{rank}(i)}\\left(N_{F P}(i)+\\sum_{k \\in \\mathcal{P}, k \\neq i} \\mathcal{E}_{\\text {loc}}(k) H\\left(x_{i k}\\right)\\right) \\frac{H\\left(x_{i j}\\right)}{N_{F P}(i)} \u5b9e\u9a8c\u4e0a\u5f97\u5230\u4e86\u6bd4\u6240\u6709one-stage detector\u66f4\u9ad8\u7684\u6027\u80fd(\u76f4\u63a5\u63d0\u53474\u70b9mAP)\u3002","title":"aLRP Loss"},{"location":"other_categories/object_detection_2D/anchordetr/","text":"Anchor DETR:Query Design for Transformer-Based Detector \u8fd9\u7bc7paper\u662f\u4e00\u4e2a\u5c06Anchor\u5728DETR\u8303\u5f0f\u5177\u8c61\u5316\u7684\u4e00\u7bc7\u8bba\u6587\u3002 \u8fd9\u7bc7paper\u5c06DETR\u7684\u8bad\u7ec3\u901f\u5ea6\u4e0e\u7a33\u5b9a\u6027\u5927\u5e45\u5ea6\u63d0\u9ad8\u3002 \u6838\u5fc3Decoder\u6539\u8fdb \u5176\u6838\u5fc3\u7684\u66f4\u6539\u5728\u4e8e\u5176Decoder\u7684\u8bbe\u8ba1\u3002 \u9996\u5148\u4f1a\u6709\u4e00\u4e2a\u53ef\u5b66\u4e60\u7684\uff0c\u6240\u6709scale\u4ee5\u53ca\u6240\u6709\u56fe\u901a\u7528\u7684\u4e00\u7ec4\u53ef\u5b66\u4e60reference points\u3002\u7531\u4e00\u7ec4\u53ef\u5b66\u4e60\u7684\u53c2\u6570\u63cf\u8ff0\u3002\u800c\u6bcf\u4e00\u4e2a\u70b9\u4f1a\u6709\u4e00\u7ec4\u7684pattern,\u4fbf\u4e8e\u8ba9\u4e00\u4e2a\u70b9\u8f93\u51fa\u591a\u4e2aprediction. \u6700\u7ec8\u4f1a\u8f93\u51fa \\text{num\\_position} \\times \\text{num\\_pattern} \u4e2a\u68c0\u6d4b\u8f93\u51fa,\u6bcf\u4e00\u4e2aposition\u4e0a\u7684anchor\u70b9\u5750\u6807\u662f\u4e00\u6837\u7684,\u800c\u6bcf\u4e00\u4e2a\u70b9\u4f4d\u7f6e\u4e0a,\u540c\u4e00\u4e2a\u901a\u9053\u7684pattern\u7684\u7279\u5f81\u4e5f\u662f\u4e00\u6837\u7684.\u6240\u4ee5\u672c\u6587\u5c31\u6309\u7167\u8fd9\u4e2a\u903b\u8f91\u6784\u5efa\u7f51\u7edc\u53ef\u5b66\u4e60\u53c2\u6570,\u4ee5\u53ca\u5728\u63a8\u7406\u7684\u65f6\u5019repeat\u6210\u76ee\u6807\u5f62\u72b6. ## During Initialization self.position = nn.Embedding(self.num_position, 2) self.pattern = nn.Embedding(self.num_pattern, d_model) ## During Inferencing reference_points = self.position.weight.unsqueeze(0).repeat(bs, self.num_pattern, 1) tgt = self.pattern.weight.reshape(1, self.num_pattern, 1, c).repeat(bs, 1, self.num_position, 1).reshape( bs, self.num_pattern * self.num_position, c) \u5728\u6bcf\u4e00\u5c42decoder\u7f51\u7edc\u7684\u8f93\u51fa\u5f62\u5f0f: # output \u4e0e \u524d\u6587\u7684 tgt\u7ecf\u8fc7transformer\u7684\u8fde\u63a5, \u6ce8\u610f:tgt\u4e0eoutput\u6709\u76f4\u63a5\u7684\u6b8b\u5dee\u8fde\u63a5(\u4e5f\u5c31\u662f\u7f51\u7edc\u521a\u521d\u59cb\u5316\u65f6,\u63a5\u8fd1\u4e8e output=tgt). # output\u5728\u6bcf\u4e00\u5c42decoder\u4e2d\u4f1a\u6301\u7eed\u4f18\u5316.\u4f46\u662f\u5168\u90e8\u90fd\u662f\u6b8b\u5dee\u8fde\u63a5. reference = inverse_sigmoid(reference_points) tmp = self.bbox_embed[lid](output) # MLP mapping tmp += reference # tmp = reference_anchor + network_residual outputs_coord = tmp.sigmoid() RCDA\u6a21\u5f0f\u7684\u6bcf\u4e00\u5c42decoder\u7531\u81ea\u6ce8\u610f\u529b,\u4ea4\u53c9\u6ce8\u610f\u529b,\u6b8b\u5dee\u5168\u8fde\u63a5\u5806\u53e0\u800c\u6210. - \u81ea\u6ce8\u610f\u529b\u7684Q,K\u4e3a\u5e26\u6709anchor\u70b9\u4f4d\u7f6eembedding\u7684tgt, \u800cV\u4e3atgt. - \u4ea4\u53c9\u6ce8\u610f\u529b\u7684Q\u4e3a tgt+ X embedding, K\u4e3a tgt + Y embedding, V\u4e3a\u56fe\u7247\u7279\u5f81 \u53ef\u4ee5\u7406\u89e3\u4e3a\u501f\u52a9\u56fe\u7247\u7279\u5f81\u6301\u7eed\u6539\u8fdboutput prediction\u7684embedding. \u672c\u6587\u5173\u952e\u6709Anchor\u7684\u5e2e\u52a9,\u5f88\u5feb\u7684\u7f51\u7edc\u5c31\u80fd\u591f\u8d8a\u8fc7DETR\u524d\u671f\u8bad\u7ec3\u65f6\u6837\u672c\u5206\u914d\u6df7\u4e71\u7684\u95ee\u9898,\u6781\u5927\u52a0\u901f\u7f51\u7edc\u7684\u8bad\u7ec3\u5730\u7a33\u5b9a\u6027\u4ee5\u53ca\u901f\u5ea6.","title":"Anchor DETR:Query Design for Transformer-Based Detector"},{"location":"other_categories/object_detection_2D/anchordetr/#anchor-detrquery-design-for-transformer-based-detector","text":"\u8fd9\u7bc7paper\u662f\u4e00\u4e2a\u5c06Anchor\u5728DETR\u8303\u5f0f\u5177\u8c61\u5316\u7684\u4e00\u7bc7\u8bba\u6587\u3002 \u8fd9\u7bc7paper\u5c06DETR\u7684\u8bad\u7ec3\u901f\u5ea6\u4e0e\u7a33\u5b9a\u6027\u5927\u5e45\u5ea6\u63d0\u9ad8\u3002","title":"Anchor DETR:Query Design for Transformer-Based Detector"},{"location":"other_categories/object_detection_2D/anchordetr/#decoder","text":"\u5176\u6838\u5fc3\u7684\u66f4\u6539\u5728\u4e8e\u5176Decoder\u7684\u8bbe\u8ba1\u3002 \u9996\u5148\u4f1a\u6709\u4e00\u4e2a\u53ef\u5b66\u4e60\u7684\uff0c\u6240\u6709scale\u4ee5\u53ca\u6240\u6709\u56fe\u901a\u7528\u7684\u4e00\u7ec4\u53ef\u5b66\u4e60reference points\u3002\u7531\u4e00\u7ec4\u53ef\u5b66\u4e60\u7684\u53c2\u6570\u63cf\u8ff0\u3002\u800c\u6bcf\u4e00\u4e2a\u70b9\u4f1a\u6709\u4e00\u7ec4\u7684pattern,\u4fbf\u4e8e\u8ba9\u4e00\u4e2a\u70b9\u8f93\u51fa\u591a\u4e2aprediction. \u6700\u7ec8\u4f1a\u8f93\u51fa \\text{num\\_position} \\times \\text{num\\_pattern} \u4e2a\u68c0\u6d4b\u8f93\u51fa,\u6bcf\u4e00\u4e2aposition\u4e0a\u7684anchor\u70b9\u5750\u6807\u662f\u4e00\u6837\u7684,\u800c\u6bcf\u4e00\u4e2a\u70b9\u4f4d\u7f6e\u4e0a,\u540c\u4e00\u4e2a\u901a\u9053\u7684pattern\u7684\u7279\u5f81\u4e5f\u662f\u4e00\u6837\u7684.\u6240\u4ee5\u672c\u6587\u5c31\u6309\u7167\u8fd9\u4e2a\u903b\u8f91\u6784\u5efa\u7f51\u7edc\u53ef\u5b66\u4e60\u53c2\u6570,\u4ee5\u53ca\u5728\u63a8\u7406\u7684\u65f6\u5019repeat\u6210\u76ee\u6807\u5f62\u72b6. ## During Initialization self.position = nn.Embedding(self.num_position, 2) self.pattern = nn.Embedding(self.num_pattern, d_model) ## During Inferencing reference_points = self.position.weight.unsqueeze(0).repeat(bs, self.num_pattern, 1) tgt = self.pattern.weight.reshape(1, self.num_pattern, 1, c).repeat(bs, 1, self.num_position, 1).reshape( bs, self.num_pattern * self.num_position, c) \u5728\u6bcf\u4e00\u5c42decoder\u7f51\u7edc\u7684\u8f93\u51fa\u5f62\u5f0f: # output \u4e0e \u524d\u6587\u7684 tgt\u7ecf\u8fc7transformer\u7684\u8fde\u63a5, \u6ce8\u610f:tgt\u4e0eoutput\u6709\u76f4\u63a5\u7684\u6b8b\u5dee\u8fde\u63a5(\u4e5f\u5c31\u662f\u7f51\u7edc\u521a\u521d\u59cb\u5316\u65f6,\u63a5\u8fd1\u4e8e output=tgt). # output\u5728\u6bcf\u4e00\u5c42decoder\u4e2d\u4f1a\u6301\u7eed\u4f18\u5316.\u4f46\u662f\u5168\u90e8\u90fd\u662f\u6b8b\u5dee\u8fde\u63a5. reference = inverse_sigmoid(reference_points) tmp = self.bbox_embed[lid](output) # MLP mapping tmp += reference # tmp = reference_anchor + network_residual outputs_coord = tmp.sigmoid() RCDA\u6a21\u5f0f\u7684\u6bcf\u4e00\u5c42decoder\u7531\u81ea\u6ce8\u610f\u529b,\u4ea4\u53c9\u6ce8\u610f\u529b,\u6b8b\u5dee\u5168\u8fde\u63a5\u5806\u53e0\u800c\u6210. - \u81ea\u6ce8\u610f\u529b\u7684Q,K\u4e3a\u5e26\u6709anchor\u70b9\u4f4d\u7f6eembedding\u7684tgt, \u800cV\u4e3atgt. - \u4ea4\u53c9\u6ce8\u610f\u529b\u7684Q\u4e3a tgt+ X embedding, K\u4e3a tgt + Y embedding, V\u4e3a\u56fe\u7247\u7279\u5f81 \u53ef\u4ee5\u7406\u89e3\u4e3a\u501f\u52a9\u56fe\u7247\u7279\u5f81\u6301\u7eed\u6539\u8fdboutput prediction\u7684embedding. \u672c\u6587\u5173\u952e\u6709Anchor\u7684\u5e2e\u52a9,\u5f88\u5feb\u7684\u7f51\u7edc\u5c31\u80fd\u591f\u8d8a\u8fc7DETR\u524d\u671f\u8bad\u7ec3\u65f6\u6837\u672c\u5206\u914d\u6df7\u4e71\u7684\u95ee\u9898,\u6781\u5927\u52a0\u901f\u7f51\u7edc\u7684\u8bad\u7ec3\u5730\u7a33\u5b9a\u6027\u4ee5\u53ca\u901f\u5ea6.","title":"\u6838\u5fc3Decoder\u6539\u8fdb"},{"location":"other_categories/object_detection_2D/balanced_oriented_focal_loss/","text":"Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection \u8fd9\u7bc7paper\u7684\u5173\u952e\u76ee\u6807\u5728\u4e8e\u8ba9\u76ee\u6807\u68c0\u6d4b\u7684\u5206\u7c7b\u635f\u5931\u51fd\u6570\u540c\u65f6\u8003\u8651\u524d\u666f-\u540e\u666f\u7684\u4e0d\u5e73\u8861\u4ee5\u53ca\u524d\u666f-\u524d\u666f\u7684\u4e0d\u5e73\u8861\u3002 Class Balance Focal Loss Class Balanced Focal loss \u662f\u524d\u4eba\u63d0\u51fa\u7684\u4e00\u4e2a\u635f\u5931\u51fd\u6570\uff0c\u4f7f\u7528focal loss\u7684\u5f62\u5f0f\u53bb\u89e3\u51b3class-imbalance \u95ee\u9898. \u5176\u8ba1\u7b97\u516c\u5f0f\u4e3a CB_{focal}(y) = -\\alpha_y \\sum_{i=1}^C (1-p_i^t)^\\gamma \\log (p_i^t) \u5176\u4e2d, \\alpha_y = \\frac{1-\\beta}{1-\\beta^{n_y}} , \u5176\u4e2d C \u4e3a\u7c7b\u522b\u6570\u91cf\uff0c p_i \u4e3a\u9884\u6d4b\u6982\u7387\u503c, n_y \u4e3a y \u7c7b\u522b\u7684\u6837\u672c\u6570\u91cf\u3002 N \u4e3a\u6837\u672c\u603b\u6570, N = 1/(1-\\beta) \uff0c\u4e0efocal loss\u76f8\u6bd4\uff0c\u6838\u5fc3\u95ee\u9898\u7684\u53d8\u5316\u5c31\u662f\u53c2\u6570 \\alpha_y \u5bf9\u4e0d\u540c\u6b63\u6837\u672c\u8fdb\u884c\u4e86reweight. \u8fd9\u4e2a\u65b9\u6848\u76f4\u63a5\u5728detection\u4e0a\u4f7f\u7528\u7684\u95ee\u9898\u5728\u4e8e\u524d\u666f\u4e0e\u80cc\u666f\u7684\u6bd4\u4f8b\u592a\u5927\u3002 Balance-Oriented Focal Loss with Linear Scheduling \u8fd9\u7bc7paper\u5728\u4e0a\u6587\u7684\u57fa\u7840\u4e0a\u63d0\u51fa\u4e86\u4e24\u4e2a\u65b0\u7684\u64cd\u4f5c\u3002 Linear Scheduling, Batch-Wise Balancing BO_{focal}(y, k) = -\\sum^C_{i=1} \\hat w_{i,k} (1-p_i^t)^\\gamma \\log (p_i^t) \\hat w_{i,k} = \\hat \\alpha_i \\hat\\eta^{n_{i,k}} \\hat\\alpha_i = 1 + \\lambda(\\alpha_i - 1) \u5176\u4e2d \\alpha_i \u4e3a\u539f\u6765\u7684\u5206\u7c7breweight, \\lambda \u4e3a normalized epoch. \u5728\u8bad\u7ec3\u5f00\u59cb\u7684\u65f6\u5019 \\alpha_i \u4e3a 1 , \u57fa\u672c\u90fd\u662f\u5728\u533a\u5206\u6b63\u8d1f\u6837\u672c\uff0c\u8bad\u7ec3\u540e\u671f\uff0c \\alpha_i \u5f00\u59cb\u5173\u6ce8class\u4e4b\u95f4\u7684reweight. n_{i,k} \u6307\u7684\u662f\u5728\u7b2c k \u4e2abatch \u6837\u672c\u7684\u6570\u91cf. \\eta \u662f\u4e00\u4e2a [0,1] \u4e4b\u95f4\u7684\u8d85\u53c2. \u5b9e\u9a8c\u4e2d Weight factor \u548c Linear Scheduling \u63d0\u5347\u6027\u80fd\u6bd4\u8f83\u591a\u3002","title":"Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection"},{"location":"other_categories/object_detection_2D/balanced_oriented_focal_loss/#balance-oriented-focal-loss-with-linear-scheduling-for-anchor-free-object-detection","text":"\u8fd9\u7bc7paper\u7684\u5173\u952e\u76ee\u6807\u5728\u4e8e\u8ba9\u76ee\u6807\u68c0\u6d4b\u7684\u5206\u7c7b\u635f\u5931\u51fd\u6570\u540c\u65f6\u8003\u8651\u524d\u666f-\u540e\u666f\u7684\u4e0d\u5e73\u8861\u4ee5\u53ca\u524d\u666f-\u524d\u666f\u7684\u4e0d\u5e73\u8861\u3002","title":"Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection"},{"location":"other_categories/object_detection_2D/balanced_oriented_focal_loss/#class-balance-focal-loss","text":"Class Balanced Focal loss \u662f\u524d\u4eba\u63d0\u51fa\u7684\u4e00\u4e2a\u635f\u5931\u51fd\u6570\uff0c\u4f7f\u7528focal loss\u7684\u5f62\u5f0f\u53bb\u89e3\u51b3class-imbalance \u95ee\u9898. \u5176\u8ba1\u7b97\u516c\u5f0f\u4e3a CB_{focal}(y) = -\\alpha_y \\sum_{i=1}^C (1-p_i^t)^\\gamma \\log (p_i^t) \u5176\u4e2d, \\alpha_y = \\frac{1-\\beta}{1-\\beta^{n_y}} , \u5176\u4e2d C \u4e3a\u7c7b\u522b\u6570\u91cf\uff0c p_i \u4e3a\u9884\u6d4b\u6982\u7387\u503c, n_y \u4e3a y \u7c7b\u522b\u7684\u6837\u672c\u6570\u91cf\u3002 N \u4e3a\u6837\u672c\u603b\u6570, N = 1/(1-\\beta) \uff0c\u4e0efocal loss\u76f8\u6bd4\uff0c\u6838\u5fc3\u95ee\u9898\u7684\u53d8\u5316\u5c31\u662f\u53c2\u6570 \\alpha_y \u5bf9\u4e0d\u540c\u6b63\u6837\u672c\u8fdb\u884c\u4e86reweight. \u8fd9\u4e2a\u65b9\u6848\u76f4\u63a5\u5728detection\u4e0a\u4f7f\u7528\u7684\u95ee\u9898\u5728\u4e8e\u524d\u666f\u4e0e\u80cc\u666f\u7684\u6bd4\u4f8b\u592a\u5927\u3002","title":"Class Balance Focal Loss"},{"location":"other_categories/object_detection_2D/balanced_oriented_focal_loss/#balance-oriented-focal-loss-with-linear-scheduling","text":"\u8fd9\u7bc7paper\u5728\u4e0a\u6587\u7684\u57fa\u7840\u4e0a\u63d0\u51fa\u4e86\u4e24\u4e2a\u65b0\u7684\u64cd\u4f5c\u3002 Linear Scheduling, Batch-Wise Balancing BO_{focal}(y, k) = -\\sum^C_{i=1} \\hat w_{i,k} (1-p_i^t)^\\gamma \\log (p_i^t) \\hat w_{i,k} = \\hat \\alpha_i \\hat\\eta^{n_{i,k}} \\hat\\alpha_i = 1 + \\lambda(\\alpha_i - 1) \u5176\u4e2d \\alpha_i \u4e3a\u539f\u6765\u7684\u5206\u7c7breweight, \\lambda \u4e3a normalized epoch. \u5728\u8bad\u7ec3\u5f00\u59cb\u7684\u65f6\u5019 \\alpha_i \u4e3a 1 , \u57fa\u672c\u90fd\u662f\u5728\u533a\u5206\u6b63\u8d1f\u6837\u672c\uff0c\u8bad\u7ec3\u540e\u671f\uff0c \\alpha_i \u5f00\u59cb\u5173\u6ce8class\u4e4b\u95f4\u7684reweight. n_{i,k} \u6307\u7684\u662f\u5728\u7b2c k \u4e2abatch \u6837\u672c\u7684\u6570\u91cf. \\eta \u662f\u4e00\u4e2a [0,1] \u4e4b\u95f4\u7684\u8d85\u53c2. \u5b9e\u9a8c\u4e2d Weight factor \u548c Linear Scheduling \u63d0\u5347\u6027\u80fd\u6bd4\u8f83\u591a\u3002","title":"Balance-Oriented Focal Loss with Linear Scheduling"},{"location":"other_categories/object_detection_2D/deformable_detr/","text":"Deformable DETR: Deformable Transformers for End-to-End Object Detection Deformable DETR\u8fd9\u7bc7paper\u5728 DETR \u7684\u57fa\u7840\u4e0a\u8fdb\u884c\u4e86\u8fdb\u4e00\u6b65\u7684\u601d\u8003\u3002 DETR\u8868\u73b0\u51fa\u6765\u7684\u95ee\u9898\uff1a \u8bad\u7ec3\u5468\u671f\u5f88\u957f\uff0c\u76f8\u6bd4\u6807\u51c6\u7684one-stage/two-stage detection\u5728COCO\u4e0a 12 epochs\u5c31\u80fd\u51fa\u597d\u7684\u7ed3\u679c\uff0cDETR\u6807\u914d200 epochs. \u5bf9\u5c0f\u76ee\u6807\u4e0d\u53cb\u597d \u4f5c\u8005\u6307\u51fa\uff0cDETR\u4e2dTransformer\u7684\u95ee\u9898\u662f\uff0c\u5728\u521d\u59cb\u5316\u7684\u65f6\u5019\uff0c\u7f51\u7edc\u51e0\u4e4e\u662f\u628a\u6743\u91cd\u5747\u7b49\u5730\u7a20\u5bc6\u5730\u5206\u914d\u7ed9\u5168\u56fe\uff0c\u4f46\u662f\u5728\u8bad\u7ec3\u597d\u7684DETR\u4e2d\uff0c\u7f51\u7edc\u6bcf\u4e00\u4e2a\u50cf\u7d20\u5bf9\u56fe\u50cf\u4e0a\u50cf\u7d20\u7684\u6743\u91cd\u5206\u914d\u53c8\u662f\u53ca\u5176\u7a00\u758f\u7684\uff0c\u8fd9\u4e2a\u4ece\u5747\u7b49\u3001\u7a20\u5bc6\u7684attention\u5230\u96c6\u4e2d\u3001\u7a00\u758f\u7684Attention\u60c5\u51b5\u7684\u8fc7\u5ea6\u8fc7\u7a0b\u4e2d\uff0c\u9700\u8981\u5927\u91cf\u7684\u8bad\u7ec3\u6570\u636e\u4ee5\u53ca\u5927\u91cf\u7684\u8bad\u7ec3step. \u8fd9\u5c31\u4f7f\u5f97\u6574\u4e2a\u8bad\u7ec3\u5468\u671f\u5f88\u957f\u3002 \u800c\u6b64\u524d\u5728 attention \u76f8\u5173\u7684\u4e00\u4e9b\u6587\u7ae0\u5c31\u53ef\u4ee5\u53d1\u73b0\uff0cTransformer\u7684\u7279\u70b9\u5728\u4e8e\u7406\u8bba\u4e0a\u7684\u5168\u5c40\u611f\u53d7\u91ce\uff0c\u4f46\u662f\u5b9e\u9645\u4e0a\u8bad\u7ec3\u7684\u6536\u655b\u7ed3\u679c\u5374\u662f\u7a00\u758f\u7684attenttion(\u4f46\u662f\u5173\u6ce8\u70b9\u53ef\u4ee5\u904d\u5e03\u56fe\u50cf\u4e0a\u4e0d\u540c\u5730\u65b9, \u800c\u4e0d\u662f\u50cfCNN\u4e00\u6837\u5c40\u9650\u5728\u5468\u56f4\u50cf\u7d20)\u3002 \u8fd9\u7bc7Deformable DETR\u5219\u5728\u5168\u5c40\u7684\u5173\u6ce8\u8303\u56f4\u4ee5\u53ca\u7a00\u758f\u6027\u53d6\u5f97\u5e73\u8861. \u5355\u4e2aDeformable Attention\u6a21\u5757\u5982\u4e0b\u56fe . \u8f93\u5165\u5305\u62ec \u7279\u5f81\u56fe(coding\u4e0aflatten, \u4f46\u662f\u903b\u8f91\u4e0aspatial) x , [B, \\sum_i{H_i\\cdot W_i}, C] \u7279\u5f81\u56fe\u5f80\u5f80\u662f\u524d\u9762\u5904\u7406\u5f97\u5230\u7684\u56fe\u7247\u591a\u5c42feature, Query Feature z_q , [N_{query},C] Query\u5f80\u5f80\u662fModule \u5b58\u653e\u7684\u4e00\u7ec4\u53ef\u5b66\u4e60\u7684embedding, \u53ef\u4ee5\u7406\u89e3\u4e3aoutput anchor, \u901a\u8fc7\u4e00\u7ec4\u53ef\u5b66\u4e60\u7684\u6620\u5c04\u53ef\u4ee5\u76f4\u63a5\u6620\u5c04\u5230\u8f93\u51fa\u3002\u53c2\u8003\u70b9Reference Point [N_{query}, n_{level}, 2] , \u662f\u4eceoutput anchor \u6620\u5c04\u51fa\u6765\u7684(\u4e5f\u53ef\u4ee5\u4e0d\u662f)\u7684\u57fa\u51c6\u70b9,\u4e5f\u5c31\u662fdeformable \u5750\u6807\u7684\u57fa\u51c6\u4e2d\u5fc3\u70b9\u3002 \u4e2d\u95f4\u53d8\u91cf\uff0csampling offsets \u901a\u8fc7 z_q \u540e\u63a5\u7684\u5168\u8fde\u63a5\u5c42\u83b7\u5f97 \\Delta p_{mqk} , shape [N_{query}, n_{heads}, n_{levels}, n_{points}, 2] , \u57fa\u51c6\u70b9\u662f\u4e0d\u540cscale\u4e0d\u540cheads\u90fd\u540c\u4e00\u4f4d\u7f6e\u7684, sampling offsets\u5219\u662f\u5bf9\u6bcf\u4e00\u4e2ascale/\u6bcf\u4e00\u4e2aheads\u6709\u4e00\u4e2a\u5355\u72ec\u7684offset. Attention Weights A , \u5f62\u72b6 N_{query}, n_{heads}, n_{levels}, n_{points} \\operatorname{MSDeformAttn}\\left(\\boldsymbol{z}_{q}, \\hat{\\boldsymbol{p}}_{q},\\left\\{\\boldsymbol{x}^{l}\\right\\}_{l=1}^{L}\\right)=\\sum_{m=1}^{M} \\boldsymbol{W}_{m}\\left[\\sum_{l=1}^{L} \\sum_{k=1}^{K} A_{m l q k} \\cdot \\boldsymbol{W}_{m}^{\\prime} \\boldsymbol{x}^{l}\\left(\\phi_{l}\\left(\\hat{\\boldsymbol{p}}_{q}\\right)+\\Delta \\boldsymbol{p}_{m l q k}\\right)\\right] \u672c\u6587\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e2aiterative bounding box refinement\u7684\u65b9\u6848\uff0c\u5c31\u662f\u6bcf\u4e00\u5c42\u90fd\u4f1a\u66f4\u65b0reference points, \u53c2\u8003\u7684\u662foptical flow\u7684\u601d\u8def\u3002 MonoDETR pdf code \u672c\u7ad9\u94fe\u63a5 \u8fd9\u7bc7\u884d\u751f\u6587\u7ae0\u7684\u4e00\u5927\u7279\u70b9\u5c31\u662f\u4f7f\u7528\u4e86Deformable DETR\u4f5c\u4e3a\u4e3b\u8981\u7684\u8fd0\u7b97\u6a21\u5757\u3002\u8fd9\u4e5f\u53ef\u80fd\u662fMonoDETR\u80fdtrain\u5f97\u51fa\u6765\u800c\u57fa\u7840\u7684DETR\u4e0d\u884c\u5f97\u4e00\u4e2a\u539f\u56e0\u3002","title":"Deformable DETR: Deformable Transformers for End-to-End Object Detection"},{"location":"other_categories/object_detection_2D/deformable_detr/#deformable-detr-deformable-transformers-for-end-to-end-object-detection","text":"Deformable DETR\u8fd9\u7bc7paper\u5728 DETR \u7684\u57fa\u7840\u4e0a\u8fdb\u884c\u4e86\u8fdb\u4e00\u6b65\u7684\u601d\u8003\u3002 DETR\u8868\u73b0\u51fa\u6765\u7684\u95ee\u9898\uff1a \u8bad\u7ec3\u5468\u671f\u5f88\u957f\uff0c\u76f8\u6bd4\u6807\u51c6\u7684one-stage/two-stage detection\u5728COCO\u4e0a 12 epochs\u5c31\u80fd\u51fa\u597d\u7684\u7ed3\u679c\uff0cDETR\u6807\u914d200 epochs. \u5bf9\u5c0f\u76ee\u6807\u4e0d\u53cb\u597d \u4f5c\u8005\u6307\u51fa\uff0cDETR\u4e2dTransformer\u7684\u95ee\u9898\u662f\uff0c\u5728\u521d\u59cb\u5316\u7684\u65f6\u5019\uff0c\u7f51\u7edc\u51e0\u4e4e\u662f\u628a\u6743\u91cd\u5747\u7b49\u5730\u7a20\u5bc6\u5730\u5206\u914d\u7ed9\u5168\u56fe\uff0c\u4f46\u662f\u5728\u8bad\u7ec3\u597d\u7684DETR\u4e2d\uff0c\u7f51\u7edc\u6bcf\u4e00\u4e2a\u50cf\u7d20\u5bf9\u56fe\u50cf\u4e0a\u50cf\u7d20\u7684\u6743\u91cd\u5206\u914d\u53c8\u662f\u53ca\u5176\u7a00\u758f\u7684\uff0c\u8fd9\u4e2a\u4ece\u5747\u7b49\u3001\u7a20\u5bc6\u7684attention\u5230\u96c6\u4e2d\u3001\u7a00\u758f\u7684Attention\u60c5\u51b5\u7684\u8fc7\u5ea6\u8fc7\u7a0b\u4e2d\uff0c\u9700\u8981\u5927\u91cf\u7684\u8bad\u7ec3\u6570\u636e\u4ee5\u53ca\u5927\u91cf\u7684\u8bad\u7ec3step. \u8fd9\u5c31\u4f7f\u5f97\u6574\u4e2a\u8bad\u7ec3\u5468\u671f\u5f88\u957f\u3002 \u800c\u6b64\u524d\u5728 attention \u76f8\u5173\u7684\u4e00\u4e9b\u6587\u7ae0\u5c31\u53ef\u4ee5\u53d1\u73b0\uff0cTransformer\u7684\u7279\u70b9\u5728\u4e8e\u7406\u8bba\u4e0a\u7684\u5168\u5c40\u611f\u53d7\u91ce\uff0c\u4f46\u662f\u5b9e\u9645\u4e0a\u8bad\u7ec3\u7684\u6536\u655b\u7ed3\u679c\u5374\u662f\u7a00\u758f\u7684attenttion(\u4f46\u662f\u5173\u6ce8\u70b9\u53ef\u4ee5\u904d\u5e03\u56fe\u50cf\u4e0a\u4e0d\u540c\u5730\u65b9, \u800c\u4e0d\u662f\u50cfCNN\u4e00\u6837\u5c40\u9650\u5728\u5468\u56f4\u50cf\u7d20)\u3002 \u8fd9\u7bc7Deformable DETR\u5219\u5728\u5168\u5c40\u7684\u5173\u6ce8\u8303\u56f4\u4ee5\u53ca\u7a00\u758f\u6027\u53d6\u5f97\u5e73\u8861. \u5355\u4e2aDeformable Attention\u6a21\u5757\u5982\u4e0b\u56fe . \u8f93\u5165\u5305\u62ec \u7279\u5f81\u56fe(coding\u4e0aflatten, \u4f46\u662f\u903b\u8f91\u4e0aspatial) x , [B, \\sum_i{H_i\\cdot W_i}, C] \u7279\u5f81\u56fe\u5f80\u5f80\u662f\u524d\u9762\u5904\u7406\u5f97\u5230\u7684\u56fe\u7247\u591a\u5c42feature, Query Feature z_q , [N_{query},C] Query\u5f80\u5f80\u662fModule \u5b58\u653e\u7684\u4e00\u7ec4\u53ef\u5b66\u4e60\u7684embedding, \u53ef\u4ee5\u7406\u89e3\u4e3aoutput anchor, \u901a\u8fc7\u4e00\u7ec4\u53ef\u5b66\u4e60\u7684\u6620\u5c04\u53ef\u4ee5\u76f4\u63a5\u6620\u5c04\u5230\u8f93\u51fa\u3002\u53c2\u8003\u70b9Reference Point [N_{query}, n_{level}, 2] , \u662f\u4eceoutput anchor \u6620\u5c04\u51fa\u6765\u7684(\u4e5f\u53ef\u4ee5\u4e0d\u662f)\u7684\u57fa\u51c6\u70b9,\u4e5f\u5c31\u662fdeformable \u5750\u6807\u7684\u57fa\u51c6\u4e2d\u5fc3\u70b9\u3002 \u4e2d\u95f4\u53d8\u91cf\uff0csampling offsets \u901a\u8fc7 z_q \u540e\u63a5\u7684\u5168\u8fde\u63a5\u5c42\u83b7\u5f97 \\Delta p_{mqk} , shape [N_{query}, n_{heads}, n_{levels}, n_{points}, 2] , \u57fa\u51c6\u70b9\u662f\u4e0d\u540cscale\u4e0d\u540cheads\u90fd\u540c\u4e00\u4f4d\u7f6e\u7684, sampling offsets\u5219\u662f\u5bf9\u6bcf\u4e00\u4e2ascale/\u6bcf\u4e00\u4e2aheads\u6709\u4e00\u4e2a\u5355\u72ec\u7684offset. Attention Weights A , \u5f62\u72b6 N_{query}, n_{heads}, n_{levels}, n_{points} \\operatorname{MSDeformAttn}\\left(\\boldsymbol{z}_{q}, \\hat{\\boldsymbol{p}}_{q},\\left\\{\\boldsymbol{x}^{l}\\right\\}_{l=1}^{L}\\right)=\\sum_{m=1}^{M} \\boldsymbol{W}_{m}\\left[\\sum_{l=1}^{L} \\sum_{k=1}^{K} A_{m l q k} \\cdot \\boldsymbol{W}_{m}^{\\prime} \\boldsymbol{x}^{l}\\left(\\phi_{l}\\left(\\hat{\\boldsymbol{p}}_{q}\\right)+\\Delta \\boldsymbol{p}_{m l q k}\\right)\\right] \u672c\u6587\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e2aiterative bounding box refinement\u7684\u65b9\u6848\uff0c\u5c31\u662f\u6bcf\u4e00\u5c42\u90fd\u4f1a\u66f4\u65b0reference points, \u53c2\u8003\u7684\u662foptical flow\u7684\u601d\u8def\u3002","title":"Deformable DETR: Deformable Transformers for End-to-End Object Detection"},{"location":"other_categories/object_detection_2D/deformable_detr/#monodetr","text":"pdf code \u672c\u7ad9\u94fe\u63a5 \u8fd9\u7bc7\u884d\u751f\u6587\u7ae0\u7684\u4e00\u5927\u7279\u70b9\u5c31\u662f\u4f7f\u7528\u4e86Deformable DETR\u4f5c\u4e3a\u4e3b\u8981\u7684\u8fd0\u7b97\u6a21\u5757\u3002\u8fd9\u4e5f\u53ef\u80fd\u662fMonoDETR\u80fdtrain\u5f97\u51fa\u6765\u800c\u57fa\u7840\u7684DETR\u4e0d\u884c\u5f97\u4e00\u4e2a\u539f\u56e0\u3002","title":"MonoDETR"},{"location":"other_categories/object_detection_2D/detr/","text":"End-to-End Object Detection with Transformers \u8fd9\u7bc7paper\u7ed9\u7684\u601d\u8def\u662f\u4e00\u4e2a\u7528\u65b0\u5de5\u5177\u8fdb\u884c\u590d\u53e4\u7684\u601d\u8def\u3002\u5728Yolo\u4e0eRCNN\u7edf\u4e00\u4e4b\u524d\uff0cobject detection\u7684\u4e00\u4e2a\u505a\u6cd5\u662f\u4f7f\u7528RNN\u5e8f\u5217\u5730\u8f93\u51fabounding boxes\uff0c\u5f53\u65f6\u6709\u4e00\u4e2a\u9700\u8981\u6ce8\u610f\u7684trick\u5c31\u662floss function\u5728\u8ba1\u7b97\u7684\u65f6\u5019\u9700\u8981\u5148\u5c06Ground truth\u4e0e\u9884\u6d4b\u6846\u8ba1\u7b97\u4e00\u4e2a\u6700\u4f18\u5339\u914d\uff0c\u7136\u540e\u5bf9\u5e94\u8ba1\u7b97loss. \u672c\u6587\u7684\u65b0\u9896\u4e4b\u5904\u5c31\u662f\u4f7f\u7528transformer\u66ff\u4ee3RNN\u5e76\u884cdecode. \u7f51\u7edc\u7ed3\u6784 \u4e00\u4e9b\u7ec6\u8282: \u5339\u914d\u7684\u65f6\u5019\u4f7f\u7528entropy\u635f\u5931+ giou +L1loss, \u5339\u914d\u5728CPU\u4e0a\u5b9e\u73b0\uff0c\u4f7f\u7528\u7684\u662f scipy.optimize.linear_sum_assignment \u635f\u5931\u51fd\u6570\u4f7f\u7528 giou + L1 Loss. \u8bad\u7ec3epoch\u6570\u6bd4\u8f83\u957f\uff0c\u5728custom dataset\u4e0a\u590d\u73b0\u96be\u5ea6\u633a\u5927","title":"End-to-End Object Detection with Transformers"},{"location":"other_categories/object_detection_2D/detr/#end-to-end-object-detection-with-transformers","text":"\u8fd9\u7bc7paper\u7ed9\u7684\u601d\u8def\u662f\u4e00\u4e2a\u7528\u65b0\u5de5\u5177\u8fdb\u884c\u590d\u53e4\u7684\u601d\u8def\u3002\u5728Yolo\u4e0eRCNN\u7edf\u4e00\u4e4b\u524d\uff0cobject detection\u7684\u4e00\u4e2a\u505a\u6cd5\u662f\u4f7f\u7528RNN\u5e8f\u5217\u5730\u8f93\u51fabounding boxes\uff0c\u5f53\u65f6\u6709\u4e00\u4e2a\u9700\u8981\u6ce8\u610f\u7684trick\u5c31\u662floss function\u5728\u8ba1\u7b97\u7684\u65f6\u5019\u9700\u8981\u5148\u5c06Ground truth\u4e0e\u9884\u6d4b\u6846\u8ba1\u7b97\u4e00\u4e2a\u6700\u4f18\u5339\u914d\uff0c\u7136\u540e\u5bf9\u5e94\u8ba1\u7b97loss. \u672c\u6587\u7684\u65b0\u9896\u4e4b\u5904\u5c31\u662f\u4f7f\u7528transformer\u66ff\u4ee3RNN\u5e76\u884cdecode.","title":"End-to-End Object Detection with Transformers"},{"location":"other_categories/object_detection_2D/detr/#_1","text":"\u4e00\u4e9b\u7ec6\u8282: \u5339\u914d\u7684\u65f6\u5019\u4f7f\u7528entropy\u635f\u5931+ giou +L1loss, \u5339\u914d\u5728CPU\u4e0a\u5b9e\u73b0\uff0c\u4f7f\u7528\u7684\u662f scipy.optimize.linear_sum_assignment \u635f\u5931\u51fd\u6570\u4f7f\u7528 giou + L1 Loss. \u8bad\u7ec3epoch\u6570\u6bd4\u8f83\u957f\uff0c\u5728custom dataset\u4e0a\u590d\u73b0\u96be\u5ea6\u633a\u5927","title":"\u7f51\u7edc\u7ed3\u6784"},{"location":"other_categories/object_detection_2D/implicit_FPN/","text":"Implicit Feature Pyramid Network for Object Detection \u8fd9\u7bc7paper\u5c06 MDEQ \u7684\u5c42\u7ea7\u8fde\u63a5\u653e\u5728\u4e86\u76ee\u6807\u68c0\u6d4b\u7684neck\u4e0a\u9762\uff0c\u7528implicit neural network\u5b9e\u73b0\u65e0\u9650\u7684FPN\u5c42\u7ea7\u95f4\u4fe1\u606f\u4ea4\u4e92. \u6548\u679c\u5982\u6b64\u56fe: \u8ba1\u7b97\u8fc7\u7a0b\u76f8\u5f53\u4e8e\u76f4\u63a5\u8ba1\u7b97\u4e0d\u52a8\u70b9\u3002 \u4f5c\u8005\u8ba9 RetinaNet, Faster-RCNN, FCOS, ATSS\u7b49\u7b97\u6cd5\u5168\u90e8\u76f4\u63a5\u6da8\u70b9\u3002 \u8ba1\u7b97\u7ed3\u6784 \u4e0d\u52a8\u70b9\u7684\u8ba1\u7b97\u516c\u5f0f\u4e3a: P^* = G_\\theta(P^* + B) \u5176\u4e2d B \u4e3abackbone\u7ed9\u51fa\u7684\u8f93\u51fafeature, G_\\theta \u4e3a\u5c42\u7ea7\u95f4\u7684\u878d\u5408\uff0c\u5377\u79ef\u4ee5\u53ca\u975e\u7ebf\u6027\u8ba1\u7b97\u3002 \u4e0d\u540c\u7684 G_\\theta \u8fd0\u7b97\u5982\u4e0b\u56fe (\u6700\u7ec8\u7684\u8bbe\u8ba1\u662fc\u56fe)","title":"Implicit Feature Pyramid Network for Object Detection"},{"location":"other_categories/object_detection_2D/implicit_FPN/#implicit-feature-pyramid-network-for-object-detection","text":"\u8fd9\u7bc7paper\u5c06 MDEQ \u7684\u5c42\u7ea7\u8fde\u63a5\u653e\u5728\u4e86\u76ee\u6807\u68c0\u6d4b\u7684neck\u4e0a\u9762\uff0c\u7528implicit neural network\u5b9e\u73b0\u65e0\u9650\u7684FPN\u5c42\u7ea7\u95f4\u4fe1\u606f\u4ea4\u4e92. \u6548\u679c\u5982\u6b64\u56fe: \u8ba1\u7b97\u8fc7\u7a0b\u76f8\u5f53\u4e8e\u76f4\u63a5\u8ba1\u7b97\u4e0d\u52a8\u70b9\u3002 \u4f5c\u8005\u8ba9 RetinaNet, Faster-RCNN, FCOS, ATSS\u7b49\u7b97\u6cd5\u5168\u90e8\u76f4\u63a5\u6da8\u70b9\u3002","title":"Implicit Feature Pyramid Network for Object Detection"},{"location":"other_categories/object_detection_2D/implicit_FPN/#_1","text":"\u4e0d\u52a8\u70b9\u7684\u8ba1\u7b97\u516c\u5f0f\u4e3a: P^* = G_\\theta(P^* + B) \u5176\u4e2d B \u4e3abackbone\u7ed9\u51fa\u7684\u8f93\u51fafeature, G_\\theta \u4e3a\u5c42\u7ea7\u95f4\u7684\u878d\u5408\uff0c\u5377\u79ef\u4ee5\u53ca\u975e\u7ebf\u6027\u8ba1\u7b97\u3002 \u4e0d\u540c\u7684 G_\\theta \u8fd0\u7b97\u5982\u4e0b\u56fe (\u6700\u7ec8\u7684\u8bbe\u8ba1\u662fc\u56fe)","title":"\u8ba1\u7b97\u7ed3\u6784"},{"location":"other_categories/object_detection_2D/onenet/","text":"OneNet: Towards End-to-End One-Stage Object Detection \u8fd9\u7bc7paper\u7684\u76ee\u7684\u662f\u5b9e\u73b0\u4e0d\u9700\u8981NMS\u7684\u76ee\u6807\u68c0\u6d4b\uff0c\u8fd9\u7bc7paper\u7684\u4e3b\u8981\u505a\u6cd5\u5728\u4e8e\u66f4\u65b0 ground truth label assignment\u7684\u65b9\u6cd5\u3002 \u76ee\u524d\u4e3b\u6d41\u7b97\u6cd5\u7684assignement\u65b9\u5f0f\u7528\u4e0b\u56fe\u8868\u793a: \u5982\u679c\u7528\u4e00\u4e2a\u635f\u5931\u51fd\u6570\u6765\u63cf\u8ff0\u5404\u81ea\u7684Matching\u6807\u51c6\uff0c\u5219RetinaNet\u4ee5\u53caFCOS\u90fd\u662f\u57fa\u4e8e\u4f4d\u7f6eloss\u7684\uff0c C_{loc} = \\lambda_{iou} C_{iou} + \\lambda_{L1} C_{L1} , \u8981\u4e48\u662f\u4e2d\u5fc3\u70b9(FCOS)\uff0c\u8981\u4e48\u662fIoU(RetinaNet),\u6216\u8005\u662f\u4e24\u8005\u7684\u7ed3\u5408\u3002 \u672c\u6587\u63d0\u51fa\u5c06\u5206\u7c7bloss\u4e5f\u52a0\u5230 matching cost\u4e0a C = \\lambda_{cls} \\cdot C_{cls} + C_{loc} \u4e14\u6700\u7ec8\u6b63\u6837\u672c\u53ea\u4f1a\u5206\u914d\u7ed9\u635f\u5931\u6700\u5c0f\u7684\u90a3\u4e2a\u6837\u672c (minimum cost assignment) \u7ed3\u679c\u5e38\u89c4\uff0c\u4f46\u662f\u9ed8\u8ba4\u662fSingle head\u7684 \u5f97\u5230\u7684\u7f51\u7edc\u901f\u5ea6\u76f8\u5f53\u5feb\u3002","title":"OneNet: Towards End-to-End One-Stage Object Detection"},{"location":"other_categories/object_detection_2D/onenet/#onenet-towards-end-to-end-one-stage-object-detection","text":"\u8fd9\u7bc7paper\u7684\u76ee\u7684\u662f\u5b9e\u73b0\u4e0d\u9700\u8981NMS\u7684\u76ee\u6807\u68c0\u6d4b\uff0c\u8fd9\u7bc7paper\u7684\u4e3b\u8981\u505a\u6cd5\u5728\u4e8e\u66f4\u65b0 ground truth label assignment\u7684\u65b9\u6cd5\u3002 \u76ee\u524d\u4e3b\u6d41\u7b97\u6cd5\u7684assignement\u65b9\u5f0f\u7528\u4e0b\u56fe\u8868\u793a: \u5982\u679c\u7528\u4e00\u4e2a\u635f\u5931\u51fd\u6570\u6765\u63cf\u8ff0\u5404\u81ea\u7684Matching\u6807\u51c6\uff0c\u5219RetinaNet\u4ee5\u53caFCOS\u90fd\u662f\u57fa\u4e8e\u4f4d\u7f6eloss\u7684\uff0c C_{loc} = \\lambda_{iou} C_{iou} + \\lambda_{L1} C_{L1} , \u8981\u4e48\u662f\u4e2d\u5fc3\u70b9(FCOS)\uff0c\u8981\u4e48\u662fIoU(RetinaNet),\u6216\u8005\u662f\u4e24\u8005\u7684\u7ed3\u5408\u3002 \u672c\u6587\u63d0\u51fa\u5c06\u5206\u7c7bloss\u4e5f\u52a0\u5230 matching cost\u4e0a C = \\lambda_{cls} \\cdot C_{cls} + C_{loc} \u4e14\u6700\u7ec8\u6b63\u6837\u672c\u53ea\u4f1a\u5206\u914d\u7ed9\u635f\u5931\u6700\u5c0f\u7684\u90a3\u4e2a\u6837\u672c (minimum cost assignment) \u7ed3\u679c\u5e38\u89c4\uff0c\u4f46\u662f\u9ed8\u8ba4\u662fSingle head\u7684 \u5f97\u5230\u7684\u7f51\u7edc\u901f\u5ea6\u76f8\u5f53\u5feb\u3002","title":"OneNet: Towards End-to-End One-Stage Object Detection"},{"location":"other_categories/object_detection_2D/pix2seq/","text":"PIX2SEQ: A Language Modeling Framework for Object Detection \u975e\u5b98\u65b9\u590d\u73b0 \u77e5\u4e4e\u89e3\u8bfb \u8fd9\u7bc7paper\u5c1d\u8bd5\u5c062D\u76ee\u6807\u68c0\u6d4b\u8f6c\u6362\u6210\u4e00\u4e2a\u5e8f\u5217\u8f93\u51fa\u7684\u95ee\u9898. \u65b9\u6cd5\u7684\u76f4\u89c9\u662f\u8bf4\u5982\u679c\u7f51\u7edc\u77e5\u9053\u7269\u4f53\u7684\u4f4d\u7f6e\uff0c\u7c7b\u522b\u548c\u5927\u5c0f\uff0c\u5b83\u53ef\u4ee5\u76f4\u63a5\u628a\u5b83\"\u8bfb\u51fa\u6765\". \u65b0\u7684\u6784\u6210\u4e3b\u8981\u9700\u8981\u56db\u4e2a\u90e8\u4ef6\uff0c\u5206\u522b\u662f\u6570\u636e\u589e\u5f3a\uff0c\u8f93\u5165\u8f93\u51fa\u6784\u6210\uff0cencoder-decoder\u7f51\u7edc\u7ed3\u6784\uff0c\u635f\u5931\u51fd\u6570. \u8f93\u5165\u8f93\u51fa\u6784\u6210, \u6570\u636e\u52a0\u5f3a 2D bounding boxes \u7531\u5de6\u4e0a\u4e0e\u53f3\u4e0b\u4e24\u4e2a\u70b9\u7684\u5750\u6807\u5b9a\u4e49\u3002\u4e00\u822c\u6765\u8bf4\u8fd9\u662f\u5c0f\u6570\u800c\u4e0d\u662f\u6574\u6570\u3002\u8fd9\u91cc\u7684\u9009\u62e9\u662f\u628a\u56fe\u7247\u7684\u6a2a\u7eb5\u5750\u6807\u79bb\u6563\u5316\uff0c\u5982\u679c\u56fe\u7247\u662f 600 \\times 600 \uff0c \u5219\u53ea\u9700\u8981x,y\u5404600\u4e2abins\u5c31\u53ef\u4ee5\u5b8c\u5168\u628a\u56fe\u7247\u5750\u6807\u79bb\u6563\u5316.\u548c\u8bed\u8a00\u6a21\u578b\u5e38\u89c1\u7684\u8bcd\u6c47\u5b57\u5178\u5927\u5c0f\u76f8\u6bd4\u662f\u5f88\u5c0f\u7684\u3002\u6700\u7ec8\u6bcf\u4e2a\u7269\u4f53\u7531\u4e94\u4e2a\u79bb\u6563\u7684tokens [y_{min}, x_{min}, y_{max}, x_{max}, c] \u5b8c\u5168\u5b9a\u4e49. \u7531\u4e8e\u4e0d\u540c\u7684\u56fe\u7247\u6709\u4e0d\u540c\u7684\u7269\u4f53\u6570\u91cf\uff0c\u56e0\u800c\u7f51\u7edc\u91c7\u53d6\u7c7b\u4f3cRNN\u7684\u8fed\u4ee3\u8f93\u51fa\uff0c\u5e76\u8bbe\u8ba1\u4e00\u4e2aEOS token\u6307\u4ee3\u76ee\u6807\u8f93\u51fa\u7ed3\u675f. \u8bad\u7ec3\u7684\u65f6\u5019\u4f1a\u6dfb\u52a0\u4e00\u4e9bnoise token\u62d6\u5ef6end token\u7684\u4f4d\u7f6e.\u907f\u514d\u7f51\u7edc\u8fc7\u5ea6\u5b66\u4e60","title":"PIX2SEQ: A Language Modeling Framework for Object Detection"},{"location":"other_categories/object_detection_2D/pix2seq/#pix2seq-a-language-modeling-framework-for-object-detection","text":"\u975e\u5b98\u65b9\u590d\u73b0 \u77e5\u4e4e\u89e3\u8bfb \u8fd9\u7bc7paper\u5c1d\u8bd5\u5c062D\u76ee\u6807\u68c0\u6d4b\u8f6c\u6362\u6210\u4e00\u4e2a\u5e8f\u5217\u8f93\u51fa\u7684\u95ee\u9898. \u65b9\u6cd5\u7684\u76f4\u89c9\u662f\u8bf4\u5982\u679c\u7f51\u7edc\u77e5\u9053\u7269\u4f53\u7684\u4f4d\u7f6e\uff0c\u7c7b\u522b\u548c\u5927\u5c0f\uff0c\u5b83\u53ef\u4ee5\u76f4\u63a5\u628a\u5b83\"\u8bfb\u51fa\u6765\". \u65b0\u7684\u6784\u6210\u4e3b\u8981\u9700\u8981\u56db\u4e2a\u90e8\u4ef6\uff0c\u5206\u522b\u662f\u6570\u636e\u589e\u5f3a\uff0c\u8f93\u5165\u8f93\u51fa\u6784\u6210\uff0cencoder-decoder\u7f51\u7edc\u7ed3\u6784\uff0c\u635f\u5931\u51fd\u6570.","title":"PIX2SEQ: A Language Modeling Framework for Object Detection"},{"location":"other_categories/object_detection_2D/pix2seq/#_1","text":"2D bounding boxes \u7531\u5de6\u4e0a\u4e0e\u53f3\u4e0b\u4e24\u4e2a\u70b9\u7684\u5750\u6807\u5b9a\u4e49\u3002\u4e00\u822c\u6765\u8bf4\u8fd9\u662f\u5c0f\u6570\u800c\u4e0d\u662f\u6574\u6570\u3002\u8fd9\u91cc\u7684\u9009\u62e9\u662f\u628a\u56fe\u7247\u7684\u6a2a\u7eb5\u5750\u6807\u79bb\u6563\u5316\uff0c\u5982\u679c\u56fe\u7247\u662f 600 \\times 600 \uff0c \u5219\u53ea\u9700\u8981x,y\u5404600\u4e2abins\u5c31\u53ef\u4ee5\u5b8c\u5168\u628a\u56fe\u7247\u5750\u6807\u79bb\u6563\u5316.\u548c\u8bed\u8a00\u6a21\u578b\u5e38\u89c1\u7684\u8bcd\u6c47\u5b57\u5178\u5927\u5c0f\u76f8\u6bd4\u662f\u5f88\u5c0f\u7684\u3002\u6700\u7ec8\u6bcf\u4e2a\u7269\u4f53\u7531\u4e94\u4e2a\u79bb\u6563\u7684tokens [y_{min}, x_{min}, y_{max}, x_{max}, c] \u5b8c\u5168\u5b9a\u4e49. \u7531\u4e8e\u4e0d\u540c\u7684\u56fe\u7247\u6709\u4e0d\u540c\u7684\u7269\u4f53\u6570\u91cf\uff0c\u56e0\u800c\u7f51\u7edc\u91c7\u53d6\u7c7b\u4f3cRNN\u7684\u8fed\u4ee3\u8f93\u51fa\uff0c\u5e76\u8bbe\u8ba1\u4e00\u4e2aEOS token\u6307\u4ee3\u76ee\u6807\u8f93\u51fa\u7ed3\u675f. \u8bad\u7ec3\u7684\u65f6\u5019\u4f1a\u6dfb\u52a0\u4e00\u4e9bnoise token\u62d6\u5ef6end token\u7684\u4f4d\u7f6e.\u907f\u514d\u7f51\u7edc\u8fc7\u5ea6\u5b66\u4e60","title":"\u8f93\u5165\u8f93\u51fa\u6784\u6210, \u6570\u636e\u52a0\u5f3a"},{"location":"other_categories/object_detection_2D/polarnet/","text":"PolarNet: Learning to Optimize Polar Keypoints for Keypoint Based Object Detection \u8fd9\u7bc7\u6587\u7ae0\u662fICLR 2021 under-review\u7684\u4e00\u7bc7\u6587\u7ae0\uff0c\u8fd9\u7bc7\u6587\u7ae0\u7684\u4e3b\u8981\u6545\u4e8b\u662f\u8981\u89e3\u51b3 FCOS \u56de\u5f52\u8fc7\u7a0b\u4e2d\u7684scale\u7684\u95ee\u9898. \u4ece FCOS \u7684\u4ee3\u7801\u4ee5\u53ca\u5b9a\u4e49\u53ef\u4ee5\u53d1\u73b0\uff0c\u5728\u7f51\u7edc\u7684\u56de\u5f52\u5206\u652f\u9700\u8981\u56de\u5f52\u7684\u8303\u56f4\u4ee5\u53cavariance\u5f88\u5927\uff0c\u4e3a (0, L) \u5176\u4e2d L \u4e3a\u5f53\u524d\u5c42\u7ea7\u4e2dbounding box\u7684\u6700\u957f\u8fb9\u7684\u5927\u5c0f, \u8fd9\u76f4\u63a5\u5f71\u54cd\u4e86FCOS\u7684\u56de\u5f52\u7cbe\u5ea6. \u8fd9\u7bc7\u6587\u7ae0\u5f15\u5165\u4e86polar coordinate-based\u7684\u56de\u5f52\u7cfb\u7edf\uff0c\u5de7\u5999\u5730\u89e3\u51b3\u4e86 FCOS \u7684\u6570\u503c\u95ee\u9898. Recent and Proposed Works \u4ece\u56fe\u4e2d\u53ef\u4ee5\u770b\u51fa\uff0c FCOS \u56de\u5f52\u7684\u662f\u4e2d\u5fc3\u70b9\u5230\u8fb9\u7f18\u7684\u56db\u4e2a\u8ddd\u79bb\uff0c\u5728\u6700\u574f\u7684\u60c5\u51b5\u4e0b\uff0c\u56de\u5f52\u7684\u76ee\u6807\u503c\u4f1a\u5728 (0, L) \u7684\u8303\u56f4\u5185;PolarNet \u4f1a\u5728 (\u5de6\u4e0a \\nwarrow ,\u53f3\u4e0b \\searrow ) \u4ee5\u53ca (\u5de6\u4e0b \\swarrow \uff0c\u53f3\u4e0a \\nearrow ) \u4e24\u5bf9\u4e2d\u9009\u62e9 variance \u5c0f\u7684\u90a3\u4e00\u5bf9; training \u4ee5\u53ca inference\u7684\u65f6\u5019\u90fd\u662f\u5982\u6b64\u3002 \u4e3a\u4e86\u5728inference\u7684\u65f6\u5019\u63d0\u4f9b\u4e00\u4e2a\u81ea\u52a8\u5316\u7684\"\u9009\u62e9\"\u9879\uff0c\u6bcf\u4e00\u4e2a\u70b9\u9700\u8981\u4e00\u4e2a\u989d\u5916\u7684\u5206\u7c7b\u8f93\u51fa\uff0c\u8fd9\u4e2a\u5206\u7c7b\u5668\u4f1a\u5b66\u4e60\u53bb\u5224\u65ad\u7a76\u7adf\u6bcf\u4e00\u4e2a\u70b9\u662f\u4ee5\u54ea\u4e00\u5bf9\u70b9\u7684\u8f93\u51fa\u4e3a\u51c6 \u635f\u5931\u51fd\u6570\u7531\u4e09\u4e2a\u90e8\u5206\u7ec4\u6210. L_{overall} = L_{cls} + L_{loc} + L_{corner} + L_{sel} \u5176\u4e2d\u5206\u7c7b\u635f\u5931 L_{cls} \u662f\u901a\u7528\u7684 Focal loss . L_{loc} \u662f\u65b0\u8fd1\u5e38\u7528\u7684 GIoU Loss. L_{corner} \u4e0e\u5177\u4f53\"\u9009\u62e9\"\u7684\u89d2\u70b9\u6709\u5173\uff0c\u53ea\u8bad\u7ec3\u88ab\u9009\u62e9\u7684\u89d2\u70b9\u3002 L_{\\mathrm{corner}}\\left(\\mathbf{B}_{\\mathbf{x}, \\mathbf{y}}, \\mathbf{B}_{\\mathbf{x}, \\mathbf{y}}^{*}\\right)=\\left|\\rho_{\\mathrm{br}}-\\rho_{\\mathrm{br}}^{*}\\right|+\\left|\\rho_{\\mathrm{tl}}-\\rho_{\\mathrm{t}}^{*}\\right|+\\tan \\left|\\theta_{\\mathrm{br}}-\\theta_{\\mathrm{br}}^{*}\\right|+\\tan \\left|\\theta_{\\mathrm{tl}}-\\theta_{\\mathrm{tl}}^{*}\\right| \u6709reviewer\u6307\u51fa\u4f7f\u7528 \\tan \u4f5c\u4e3a\u635f\u5931\u51fd\u6570\u9700\u8981\u907f\u514d\u4e0d\u8fde\u7eed\u70b9\u4e0a\u7684\u68af\u5ea6\u7206\u70b8; \u5728rebuttal\u7684\u65f6\u5019\u4f5c\u8005\u8865\u5145: \u89d2\u5ea6 \\theta \u662f\u88ab\u9650\u5236\u5728 [0, \\pi/2] \u7684.","title":"PolarNet: Learning to Optimize Polar Keypoints for Keypoint Based Object Detection"},{"location":"other_categories/object_detection_2D/polarnet/#polarnet-learning-to-optimize-polar-keypoints-for-keypoint-based-object-detection","text":"\u8fd9\u7bc7\u6587\u7ae0\u662fICLR 2021 under-review\u7684\u4e00\u7bc7\u6587\u7ae0\uff0c\u8fd9\u7bc7\u6587\u7ae0\u7684\u4e3b\u8981\u6545\u4e8b\u662f\u8981\u89e3\u51b3 FCOS \u56de\u5f52\u8fc7\u7a0b\u4e2d\u7684scale\u7684\u95ee\u9898. \u4ece FCOS \u7684\u4ee3\u7801\u4ee5\u53ca\u5b9a\u4e49\u53ef\u4ee5\u53d1\u73b0\uff0c\u5728\u7f51\u7edc\u7684\u56de\u5f52\u5206\u652f\u9700\u8981\u56de\u5f52\u7684\u8303\u56f4\u4ee5\u53cavariance\u5f88\u5927\uff0c\u4e3a (0, L) \u5176\u4e2d L \u4e3a\u5f53\u524d\u5c42\u7ea7\u4e2dbounding box\u7684\u6700\u957f\u8fb9\u7684\u5927\u5c0f, \u8fd9\u76f4\u63a5\u5f71\u54cd\u4e86FCOS\u7684\u56de\u5f52\u7cbe\u5ea6. \u8fd9\u7bc7\u6587\u7ae0\u5f15\u5165\u4e86polar coordinate-based\u7684\u56de\u5f52\u7cfb\u7edf\uff0c\u5de7\u5999\u5730\u89e3\u51b3\u4e86 FCOS \u7684\u6570\u503c\u95ee\u9898.","title":"PolarNet: Learning to Optimize Polar Keypoints for Keypoint Based Object Detection"},{"location":"other_categories/object_detection_2D/polarnet/#recent-and-proposed-works","text":"\u4ece\u56fe\u4e2d\u53ef\u4ee5\u770b\u51fa\uff0c FCOS \u56de\u5f52\u7684\u662f\u4e2d\u5fc3\u70b9\u5230\u8fb9\u7f18\u7684\u56db\u4e2a\u8ddd\u79bb\uff0c\u5728\u6700\u574f\u7684\u60c5\u51b5\u4e0b\uff0c\u56de\u5f52\u7684\u76ee\u6807\u503c\u4f1a\u5728 (0, L) \u7684\u8303\u56f4\u5185;PolarNet \u4f1a\u5728 (\u5de6\u4e0a \\nwarrow ,\u53f3\u4e0b \\searrow ) \u4ee5\u53ca (\u5de6\u4e0b \\swarrow \uff0c\u53f3\u4e0a \\nearrow ) \u4e24\u5bf9\u4e2d\u9009\u62e9 variance \u5c0f\u7684\u90a3\u4e00\u5bf9; training \u4ee5\u53ca inference\u7684\u65f6\u5019\u90fd\u662f\u5982\u6b64\u3002 \u4e3a\u4e86\u5728inference\u7684\u65f6\u5019\u63d0\u4f9b\u4e00\u4e2a\u81ea\u52a8\u5316\u7684\"\u9009\u62e9\"\u9879\uff0c\u6bcf\u4e00\u4e2a\u70b9\u9700\u8981\u4e00\u4e2a\u989d\u5916\u7684\u5206\u7c7b\u8f93\u51fa\uff0c\u8fd9\u4e2a\u5206\u7c7b\u5668\u4f1a\u5b66\u4e60\u53bb\u5224\u65ad\u7a76\u7adf\u6bcf\u4e00\u4e2a\u70b9\u662f\u4ee5\u54ea\u4e00\u5bf9\u70b9\u7684\u8f93\u51fa\u4e3a\u51c6 \u635f\u5931\u51fd\u6570\u7531\u4e09\u4e2a\u90e8\u5206\u7ec4\u6210. L_{overall} = L_{cls} + L_{loc} + L_{corner} + L_{sel} \u5176\u4e2d\u5206\u7c7b\u635f\u5931 L_{cls} \u662f\u901a\u7528\u7684 Focal loss . L_{loc} \u662f\u65b0\u8fd1\u5e38\u7528\u7684 GIoU Loss. L_{corner} \u4e0e\u5177\u4f53\"\u9009\u62e9\"\u7684\u89d2\u70b9\u6709\u5173\uff0c\u53ea\u8bad\u7ec3\u88ab\u9009\u62e9\u7684\u89d2\u70b9\u3002 L_{\\mathrm{corner}}\\left(\\mathbf{B}_{\\mathbf{x}, \\mathbf{y}}, \\mathbf{B}_{\\mathbf{x}, \\mathbf{y}}^{*}\\right)=\\left|\\rho_{\\mathrm{br}}-\\rho_{\\mathrm{br}}^{*}\\right|+\\left|\\rho_{\\mathrm{tl}}-\\rho_{\\mathrm{t}}^{*}\\right|+\\tan \\left|\\theta_{\\mathrm{br}}-\\theta_{\\mathrm{br}}^{*}\\right|+\\tan \\left|\\theta_{\\mathrm{tl}}-\\theta_{\\mathrm{tl}}^{*}\\right| \u6709reviewer\u6307\u51fa\u4f7f\u7528 \\tan \u4f5c\u4e3a\u635f\u5931\u51fd\u6570\u9700\u8981\u907f\u514d\u4e0d\u8fde\u7eed\u70b9\u4e0a\u7684\u68af\u5ea6\u7206\u70b8; \u5728rebuttal\u7684\u65f6\u5019\u4f5c\u8005\u8865\u5145: \u89d2\u5ea6 \\theta \u662f\u88ab\u9650\u5236\u5728 [0, \\pi/2] \u7684.","title":"Recent and Proposed Works"},{"location":"other_categories/object_detection_2D/slender_object_detection/","text":"Slender Object Detection: Diagnoses and Improvements \u8fd9\u7bc7paper\u5206\u6790\u8bca\u65ad\u4e86\u76ee\u524d\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\u5bf9\u7ec6\u957f\u7269\u4f53\u7684\u68c0\u6d4b\u6027\u80fd\u4ee5\u53ca\u80cc\u540e\u7684\u673a\u5236\u539f\u7406\uff0c\u7136\u540e\u63d0\u51fa\u81ea\u5df1\u7684\u65b9\u6cd5\u63d0\u9ad8\u76ee\u6807\u68c0\u6d4b\u7b97\u6cd5\u5bf9\u7ec6\u957f\u7269\u4f53\u7684\u68c0\u6d4b\u80fd\u529b\u3002 \u7ec6\u957f\u7269\u4f53\u7684\u5b9a\u4e49 \u76f4\u89c9\u6765\u8bf4\u4f3c\u4e4e\u6211\u4eec\u53ef\u4ee5\u76f4\u63a5\u8ba1\u7b97\u6807\u51c62D\u68c0\u6d4b\u6846\u7684\u957f\u5bbd\u6bd4\uff0c \u76f4\u63a5\u5f97\u5230 r_b = w_b / h_b . \u4f46\u662f\u4f5c\u8005\u6307\u51fa\u8fd9\u4f1a\u6f0f\u8ba1\u7b97\u56fe\u7247\u4e2d\u65cb\u8f6c\u4e86\u7684\u7ec6\u957f\u7269\u4f53\uff0c \u5982\u56fe \u672c\u6587\u5b9a\u4e49\u4e86\u6307\u6807 slenderness s . \u9996\u5148\u5229\u7528Instance Segmentation\u7684\u7ed3\u679c\u53d6pixel-wise annotation. \u7136\u540e\u8ba1\u7b97\u4e00\u4e2a\u6700\u5c0f\u9762\u79ef\u7684\u77e9\u5f62\u8ba1\u7b97\uff0cslenderness s = \\text{min}(w,h) \\text{max} (w, h) \u6700\u5c0f\u5305\u56f4\u77e9\u5f62\u8ba1\u7b97 : 1. \u8ba1\u7b97\u6700\u5c0f\u51f8\u5305\uff0c\u5982Graham Scan\u7b97\u6cd5, CSDN Reference 2. \u6700\u5c0f\u9762\u79ef\u5916\u63a5\u77e9\u5f62\u7684\u4e00\u6761\u8fb9\u80af\u5b9a\u4e0e\u70b9\u96c6\u7684\u51f8\u5305\u7684\u4e00\u6761\u8fb9\u91cd\u5408\uff0c\u6240\u4ee5\u601d\u8def\u4e3a\u904d\u5386\u51f8\u5305\u7684\u6bcf\u6761\u8fb9\uff0c\u6784\u9020\u5916\u63a5\u77e9\u5f62\uff0c\u8ba1\u7b97\u9762\u79ef\uff0c\u627e\u51fa\u9762\u79ef\u6700\u5c0f\u7684\u90a3\u4e2a\u5916\u63a5\u77e9\u5f62\u5373\u53ef. Reference COCO\u6570\u636e\u96c6\u4e0a\u7684\u5206\u5e03 \u4e0e\u76f4\u89c9\u4e00\u81f4\uff0cCOCO\u6570\u636e\u96c6\u4e0a\u666e\u901a\u5927\u5c0f\u7684\u7269\u4f53\u6bd4\u8f83\u591a\uff0c\u800c\u5f62\u72b6\u7ec6\u957f\u7684\u7269\u4f53\u6bd4\u8f83\u5c11\u3002 \u73b0\u6709\u6a21\u578b\u7684\u5206\u6790 \u4f5c\u8005\u5c06\u73b0\u6709\u7684\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u7684\u7ed3\u6784\u62bd\u8c61\u4e3a\u56db\u4e2a\u6210\u5206: - Feature Extraction (FE): \u7279\u5f81\u63d0\u53d6 - Intermediate Prediction (IP):: \u4e2d\u95f4\u9884\u6d4b - Feature Adaption (FA):: \u7279\u5f81\u91cd\u6574 - Detection Formation (DF):: \u6700\u7ec8\u7ed3\u679c Feature Adaption(FA)\u662fmulti-stage\u6216\u8005 RepPoints \u4ee5\u53ca VarifocalNet \u91cc\u9762\u7684\u4f18\u5316\u601d\u8def\u3002 \u9664\u4e86\u4ee5\u4e0a\u7684\u51e0\u4e2a\u90e8\u5206\u4e4b\u540e\uff0c\u8fd8\u6709\u51e0\u4e2a\u5173\u952e\u7684\u6b65\u9aa4: - Loss Function (LF) \u635f\u5931\u51fd\u6570 - Label Assignment (LA) \u6807\u7b7e\u5206\u914d \u51e0\u4e2a\u5178\u578b\u7f51\u7edc\u5982\u4e0b\u56fe: \u4f5c\u8005\u5c1d\u8bd5\u5bf9RetinaNet, FCOS \u4ee5\u53caRepPoints\u7684\u5404\u4e2a\u6a21\u5757\u8fdb\u884c\u5b9e\u9a8c\uff0c\u76ee\u7684\u662f\u627e\u51fa\u4ec0\u4e48\u6a21\u5757\u56e0\u4e3a\u4ec0\u4e48\u673a\u5236\u5bf9\u7ec6\u957f\u7269\u4f53\u68c0\u6d4b\u6709\u6548\u679c\u3002 \u5b9e\u9a8c\u7ed3\u679c\u6587\u5b57\u5316\u603b\u7ed3: RetinaNet\u7684\u57fa\u7ebf\u6a21\u578b\u6bd4FCOS\u7684\u57fa\u7ebf\u6a21\u578b\u66f4\u5f3a\uff0c\u5c3d\u7ba1\u5728COCO\u4e0a\u7684\u603bmAP\u4e0d\u5982\uff0c\u5176\u5728\u7ec6\u957f\u7269\u4f53\u4e0a\u7684\u6548\u679c\u662f\u66f4\u597d\u7684\u3002 RetinaNet\u6a21\u4effFCOS\uff0c\u6846\u5185\u6240\u6709\u70b9\u90fd\u7ed9\u6b63\u6837\u672c\u540e\u6240\u6709\u70b9\u6570\u90fd\u4f1a\u53d8\u5dee,\u76f4\u5230\u589e\u52a0centerness\u624d\u7f13\u548c\uff0c\u4f46\u662f\u5728\u7ec6\u957f\u7269\u4f53\u4e0a\u8fd8\u662f\u4e0d\u5982FCOS\u7684\u57fa\u7ebf\u3002\u8fd9\u91cc\u7684\u7ed3\u8bba\u662f\u540c\u6837\u7684settings\u4e0bFCOS\u4f1a\u66f4\u5f3a\uff0c\u4f46\u662fRetinaNet\u901a\u8fc7IoU Matching\u6652\u9664\u4f4e\u8d28\u91cfground truth anchor\u7684\u673a\u5236\u8fdc\u8fdc\u80dc\u4e8eFCOS\u7684\u6807\u7b7e\u5206\u914d\u673a\u5236\u3002 RepPoints\u7684\u6807\u7b7e\u5206\u914d\u673a\u5236(\u53ea\u9009\u62e9\u6700\u8fd1\u7684\u4e2d\u5fc3\u70b9\u4e3aground truth)\uff0c\u4ee5RetinaNet\u505abaseline\u6765\u6bd4\u8f83\uff0c\u6548\u679c\u6bd4FCOS\u7684\u597d\u3002 RepPoints\u7684Deformable Conv\u673a\u5236\u5bf9RetinaNet\u7684\u63d0\u5347\u6700\u663e\u8457\u3002 \u57fa\u4e8eReppoints\u7684\u63d0\u5347 \u5b9e\u9a8c\u7ed3\u679c\u53d1\u73b0\u662f\u4e09\u4e2a\u5143\u7d20\u63d0\u5347\u6bd4\u8f83\u660e\u663e - RepPoints\u7684\u91c7\u6837\u4f4d\u79fb\u5e26\u4e0a\u4e00\u4e2abase offsets - \u91c7\u6837\u4f4d\u79fb\u5b8c\u5168\u7531\u5b66\u4e60\u800c\u6765\uff0c\u800c\u4e0d\u662f\u6765\u81ea\u4e8e\u76f4\u63a5\u76d1\u7763\u3002 - Slenderness prior, \u5728FCOS\u539f\u6765\u7684centerness\u7684\u57fa\u7840\u4e0a\uff0c\u51cf\u5c11\u5bf9\u4e00\u822c\u7269\u4f53\u7684\u6b63\u6837\u672c\u6291\u5236\uff0c\u589e\u52a0\u5bf9\u7ec6\u957f\u7269\u4f53\u7684\u6b63\u6837\u672c\u6291\u5236\u3002 \\text { centerness }^{*}=\\left(\\frac{\\min (l, r)}{\\max (l, r)} \\times \\frac{\\min (t, b)}{\\max (t, b)}\\right)^{s}","title":"Slender Object Detection: Diagnoses and Improvements"},{"location":"other_categories/object_detection_2D/slender_object_detection/#slender-object-detection-diagnoses-and-improvements","text":"\u8fd9\u7bc7paper\u5206\u6790\u8bca\u65ad\u4e86\u76ee\u524d\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\u5bf9\u7ec6\u957f\u7269\u4f53\u7684\u68c0\u6d4b\u6027\u80fd\u4ee5\u53ca\u80cc\u540e\u7684\u673a\u5236\u539f\u7406\uff0c\u7136\u540e\u63d0\u51fa\u81ea\u5df1\u7684\u65b9\u6cd5\u63d0\u9ad8\u76ee\u6807\u68c0\u6d4b\u7b97\u6cd5\u5bf9\u7ec6\u957f\u7269\u4f53\u7684\u68c0\u6d4b\u80fd\u529b\u3002","title":"Slender Object Detection: Diagnoses and Improvements"},{"location":"other_categories/object_detection_2D/slender_object_detection/#_1","text":"\u76f4\u89c9\u6765\u8bf4\u4f3c\u4e4e\u6211\u4eec\u53ef\u4ee5\u76f4\u63a5\u8ba1\u7b97\u6807\u51c62D\u68c0\u6d4b\u6846\u7684\u957f\u5bbd\u6bd4\uff0c \u76f4\u63a5\u5f97\u5230 r_b = w_b / h_b . \u4f46\u662f\u4f5c\u8005\u6307\u51fa\u8fd9\u4f1a\u6f0f\u8ba1\u7b97\u56fe\u7247\u4e2d\u65cb\u8f6c\u4e86\u7684\u7ec6\u957f\u7269\u4f53\uff0c \u5982\u56fe \u672c\u6587\u5b9a\u4e49\u4e86\u6307\u6807 slenderness s . \u9996\u5148\u5229\u7528Instance Segmentation\u7684\u7ed3\u679c\u53d6pixel-wise annotation. \u7136\u540e\u8ba1\u7b97\u4e00\u4e2a\u6700\u5c0f\u9762\u79ef\u7684\u77e9\u5f62\u8ba1\u7b97\uff0cslenderness s = \\text{min}(w,h) \\text{max} (w, h) \u6700\u5c0f\u5305\u56f4\u77e9\u5f62\u8ba1\u7b97 : 1. \u8ba1\u7b97\u6700\u5c0f\u51f8\u5305\uff0c\u5982Graham Scan\u7b97\u6cd5, CSDN Reference 2. \u6700\u5c0f\u9762\u79ef\u5916\u63a5\u77e9\u5f62\u7684\u4e00\u6761\u8fb9\u80af\u5b9a\u4e0e\u70b9\u96c6\u7684\u51f8\u5305\u7684\u4e00\u6761\u8fb9\u91cd\u5408\uff0c\u6240\u4ee5\u601d\u8def\u4e3a\u904d\u5386\u51f8\u5305\u7684\u6bcf\u6761\u8fb9\uff0c\u6784\u9020\u5916\u63a5\u77e9\u5f62\uff0c\u8ba1\u7b97\u9762\u79ef\uff0c\u627e\u51fa\u9762\u79ef\u6700\u5c0f\u7684\u90a3\u4e2a\u5916\u63a5\u77e9\u5f62\u5373\u53ef. Reference","title":"\u7ec6\u957f\u7269\u4f53\u7684\u5b9a\u4e49"},{"location":"other_categories/object_detection_2D/slender_object_detection/#coco","text":"\u4e0e\u76f4\u89c9\u4e00\u81f4\uff0cCOCO\u6570\u636e\u96c6\u4e0a\u666e\u901a\u5927\u5c0f\u7684\u7269\u4f53\u6bd4\u8f83\u591a\uff0c\u800c\u5f62\u72b6\u7ec6\u957f\u7684\u7269\u4f53\u6bd4\u8f83\u5c11\u3002","title":"COCO\u6570\u636e\u96c6\u4e0a\u7684\u5206\u5e03"},{"location":"other_categories/object_detection_2D/slender_object_detection/#_2","text":"\u4f5c\u8005\u5c06\u73b0\u6709\u7684\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u7684\u7ed3\u6784\u62bd\u8c61\u4e3a\u56db\u4e2a\u6210\u5206: - Feature Extraction (FE): \u7279\u5f81\u63d0\u53d6 - Intermediate Prediction (IP):: \u4e2d\u95f4\u9884\u6d4b - Feature Adaption (FA):: \u7279\u5f81\u91cd\u6574 - Detection Formation (DF):: \u6700\u7ec8\u7ed3\u679c Feature Adaption(FA)\u662fmulti-stage\u6216\u8005 RepPoints \u4ee5\u53ca VarifocalNet \u91cc\u9762\u7684\u4f18\u5316\u601d\u8def\u3002 \u9664\u4e86\u4ee5\u4e0a\u7684\u51e0\u4e2a\u90e8\u5206\u4e4b\u540e\uff0c\u8fd8\u6709\u51e0\u4e2a\u5173\u952e\u7684\u6b65\u9aa4: - Loss Function (LF) \u635f\u5931\u51fd\u6570 - Label Assignment (LA) \u6807\u7b7e\u5206\u914d \u51e0\u4e2a\u5178\u578b\u7f51\u7edc\u5982\u4e0b\u56fe: \u4f5c\u8005\u5c1d\u8bd5\u5bf9RetinaNet, FCOS \u4ee5\u53caRepPoints\u7684\u5404\u4e2a\u6a21\u5757\u8fdb\u884c\u5b9e\u9a8c\uff0c\u76ee\u7684\u662f\u627e\u51fa\u4ec0\u4e48\u6a21\u5757\u56e0\u4e3a\u4ec0\u4e48\u673a\u5236\u5bf9\u7ec6\u957f\u7269\u4f53\u68c0\u6d4b\u6709\u6548\u679c\u3002 \u5b9e\u9a8c\u7ed3\u679c\u6587\u5b57\u5316\u603b\u7ed3: RetinaNet\u7684\u57fa\u7ebf\u6a21\u578b\u6bd4FCOS\u7684\u57fa\u7ebf\u6a21\u578b\u66f4\u5f3a\uff0c\u5c3d\u7ba1\u5728COCO\u4e0a\u7684\u603bmAP\u4e0d\u5982\uff0c\u5176\u5728\u7ec6\u957f\u7269\u4f53\u4e0a\u7684\u6548\u679c\u662f\u66f4\u597d\u7684\u3002 RetinaNet\u6a21\u4effFCOS\uff0c\u6846\u5185\u6240\u6709\u70b9\u90fd\u7ed9\u6b63\u6837\u672c\u540e\u6240\u6709\u70b9\u6570\u90fd\u4f1a\u53d8\u5dee,\u76f4\u5230\u589e\u52a0centerness\u624d\u7f13\u548c\uff0c\u4f46\u662f\u5728\u7ec6\u957f\u7269\u4f53\u4e0a\u8fd8\u662f\u4e0d\u5982FCOS\u7684\u57fa\u7ebf\u3002\u8fd9\u91cc\u7684\u7ed3\u8bba\u662f\u540c\u6837\u7684settings\u4e0bFCOS\u4f1a\u66f4\u5f3a\uff0c\u4f46\u662fRetinaNet\u901a\u8fc7IoU Matching\u6652\u9664\u4f4e\u8d28\u91cfground truth anchor\u7684\u673a\u5236\u8fdc\u8fdc\u80dc\u4e8eFCOS\u7684\u6807\u7b7e\u5206\u914d\u673a\u5236\u3002 RepPoints\u7684\u6807\u7b7e\u5206\u914d\u673a\u5236(\u53ea\u9009\u62e9\u6700\u8fd1\u7684\u4e2d\u5fc3\u70b9\u4e3aground truth)\uff0c\u4ee5RetinaNet\u505abaseline\u6765\u6bd4\u8f83\uff0c\u6548\u679c\u6bd4FCOS\u7684\u597d\u3002 RepPoints\u7684Deformable Conv\u673a\u5236\u5bf9RetinaNet\u7684\u63d0\u5347\u6700\u663e\u8457\u3002","title":"\u73b0\u6709\u6a21\u578b\u7684\u5206\u6790"},{"location":"other_categories/object_detection_2D/slender_object_detection/#reppoints","text":"\u5b9e\u9a8c\u7ed3\u679c\u53d1\u73b0\u662f\u4e09\u4e2a\u5143\u7d20\u63d0\u5347\u6bd4\u8f83\u660e\u663e - RepPoints\u7684\u91c7\u6837\u4f4d\u79fb\u5e26\u4e0a\u4e00\u4e2abase offsets - \u91c7\u6837\u4f4d\u79fb\u5b8c\u5168\u7531\u5b66\u4e60\u800c\u6765\uff0c\u800c\u4e0d\u662f\u6765\u81ea\u4e8e\u76f4\u63a5\u76d1\u7763\u3002 - Slenderness prior, \u5728FCOS\u539f\u6765\u7684centerness\u7684\u57fa\u7840\u4e0a\uff0c\u51cf\u5c11\u5bf9\u4e00\u822c\u7269\u4f53\u7684\u6b63\u6837\u672c\u6291\u5236\uff0c\u589e\u52a0\u5bf9\u7ec6\u957f\u7269\u4f53\u7684\u6b63\u6837\u672c\u6291\u5236\u3002 \\text { centerness }^{*}=\\left(\\frac{\\min (l, r)}{\\max (l, r)} \\times \\frac{\\min (t, b)}{\\max (t, b)}\\right)^{s}","title":"\u57fa\u4e8eReppoints\u7684\u63d0\u5347"},{"location":"other_categories/object_detection_2D/toward_open_world_detection/","text":"Towards Open World Object Detection (OWOD) \u8fd9\u7bc7paper\u5c1d\u8bd5\u89e3\u51b3\u4e00\u4e2a\u5f88\u5927\u7684\u95ee\u9898\uff0c\u5b83\u4eec\u60f3\u8981\u8bad\u7ec3\u4e00\u4e2a\u7f51\u7edc: \u5b9e\u73b02D\u76ee\u6807\u68c0\u6d4b\uff0c\u4e14\u5bf9\u4e8e\u8fd8\u6ca1\u88ab\u5f15\u5165\u7684\u7269\u4f53\u6807\u6ce8\u4e3a\"\u672a\u77e5\"\uff0c\u5e76\u5c1d\u8bd5\u6846\u51fa\u6765. \u589e\u91cf\u5730\u5b66\u4e60\u65b0\u7684label\uff0c\u540c\u65f6\u4e0d\u80fd\u5fd8\u6389\u4e4b\u524d\u5b66\u4f1a\u7684\u7c7b\u522b. Architecture and Pipeline Auto-labelling Unknowns with RPN \u8fd9\u4e2a\u90e8\u5206\u542c\u8d77\u6765\u6bd4\u8f83\u7384\u5b66\uff0c\u5b9e\u9645\u64cd\u4f5c\u4e0a\u5373\u662f\u4ece\u80cc\u666fproposals (first stage detection)\u91cc\u9762\u9009\u62e9top-k\u6700\u9ad8objectness scores\u7684candidates\u5373\u4e3a\u672a\u77e5\u7269\u4f53\u7684candidate. Energy Based Unknown Identifier \u8fd9\u4e00\u6bb5\u5728\u8bba\u6587\u4e2d\u672c\u6587\u5e76\u4e0d\u80fd\u7406\u89e3\u5730\u5f88\u900f\u5f7b\uff0c\u4f46\u662f\u4ece\u4ee3\u7801\u4e2d\u53ef\u4ee5\u5148\u7406\u89e3\u5b9e\u9645\u8fdb\u884c\u7684\u4e8b\u60c5: \u5728\u8bad\u7ec3\u7684\u8fc7\u7a0b\u4e2d\uff0c\u8ba9RPN\u8f93\u51fa\u4e00\u7cfb\u5217\u7684proposals (nms, top_k, gt), roi_head\u518d\u8f93\u51fa\u6700\u7ec8\u7684\u5206\u7c7blogits. \u4ece\u4e2d\u9009\u53d6\u4e0d\u662f\u7701\u7565\u7684(IoU matching\u5728\u4e34\u754c\u4e4b\u95f4)\u3002\u5982\u679c\u4e3abackground, \u5219\u7406\u89e3\u4e3aunknown. \u5982\u679c\u4e3a\u5728\u8bad\u7ec3\u7684\u7c7b\uff0c\u5219\u7406\u89e3\u4e3aknown.\u5b58\u50a8 logits\u77e2\u91cf\u548c\u5206\u7c7b\u7684\u7ed3\u679c\u5230\u6587\u4ef6\u4e2d. \u5728\u8bad\u7ec3\u7ed3\u675f\u7684\u65f6\u5019\uff0c\u5206\u6790\u80fd\u91cf\uff0c \u4f7f\u7528\u6587\u7ae0\u7684\u516c\u5f0f\u8ba1\u7b97\u80fd\u91cf lse = T\\log\\sum_j\\exp{(g_l(\\mathbf{f})/T)} .\u6587\u4e2d\u63cf\u8ff0\uff0c\u4ece\u76f4\u89c9\u4e0a\u53ef\u4ee5\u7406\u89e3\u7684\u662fknown classes\u7684\u4e0d\u786e\u5b9a\u5ea6\u6bd4\u8f83\u5c0f\uff0c\u80fd\u91cf\u7684\u7edd\u5bf9\u503c\u66f4\u5927\uff0cunknown\u7684\u4e0d\u786e\u5b9a\u5ea6\u6bd4\u8f83\u5927\uff0c\u80fd\u91cf\u7684\u7edd\u5bf9\u503c\u6bd4\u8f83\u5c0f\u3002 \u6839\u636e\u4e24\u7c7b\u7684\u6570\u636e\u5206\u5e03\uff0c\u62df\u5408\u4e24\u4e2a Weibull \u5206\u5e03\uff0c\u4f7f\u7528\u7684\u5e93\u662f reliability.Fitters.Fit_Weibull_3P . \u5b58\u4e0b\u4e24\u4e2a\u6a21\u578b \u5728\u63a8\u7406/\u53ef\u89c6\u5316\u7684\u65f6\u5019\uff0c\u9700\u8981\u5148\u8bbe\u5b9a\u4e00\u4e2athreshold(0.61)\u5e76\u8f93\u51fa\u4e00\u7cfb\u5217\u7684bounding boxes, \u6839\u636e\u6bcf\u4e2a\u8f93\u51fabounding boxes \u7684logit\u5728\u4e24\u4e2aweibull\u5206\u5e03\u4e0a\u7684\u6982\u7387\u5bc6\u5ea6(\u5b9e\u9645\u64cd\u4f5c\u662f\u9644\u8fd1\u4e00\u4e2a\u5c0f\u8303\u56f4\u7684\u79ef\u5206)\u6bd4\u8f83\uff0c\u5224\u65ad\u8fd9\u4e2aobject\u7a76\u7adf\u662f\u5df2\u77e5\u8fd8\u662f\u672a\u77e5. \u5b9e\u8d28\u4e0a\u5c31\u662f\u6839\u636e\u662f\u8f93\u51fa\u7f51\u7edc\u81ea\u9002\u5e94\u7684\u5b66\u4e60\u4e00\u4e2a\u5206\u7c7b\u5668\u5224\u65ad\u73b0\u5728\u7684\u5206\u7c7b\u662f\u786e\u5b9a\u7684\u8fd8\u662f\u4e0d\u786e\u5b9a\u7684\u3002 GDumb: A Simple Approach that Questions Our Progress in Continual Learning pdf code OWOD\u7684\u7b2c\u56db\u90e8\u5206\u5728\u4e8e\u62b5\u5236\u9057\u5fd8\uff0c\u8fd9\u5f15\u81ea\u4e8e GDumb\u8fd9\u7bc7\u6587\u7ae0\u7684\u4e00\u4e2a\u7b80\u5355\u505a\u6cd5\u3002 GDumb\u4e00\u6587\u5bf9 Continuous Learning \u793e\u533a\u63d0\u51fa\u4e86\u5f88\u5f3a\u7684\u8d28\u7591\u3002 GDumb\u7684\u7b97\u6cd5\u5c31\u662f\u5728\u8bad\u7ec3\u7684\u8fc7\u7a0b\u4e2d\u8d2a\u5a6a\u5730\u7ef4\u62a4\u4e00\u4e2a\u989d\u5b9a\u5927\u5c0f\u7684\u6570\u636e\u8bb0\u5fc6\u6c60(python deque\uff0c\u6bcf\u4e2astep\u8bad\u7ec3\u90fd\u5c06\u6570\u636e\u5b58\u8d77\u6765). \u5728incremental learning\u7684\u65f6\u5019\u5c06\u539f\u6765\u7684\u6570\u636e\u62ff\u51fa\u6765\u91cd\u65b0\u8bad\u7ec3\u4e00\u4e0b\u5373\u53ef\uff0c\u8fd9\u4e2a\u505a\u6cd5\u505a\u51fa\u4e86\u5f88\u9ad8\u7684\u6027\u80fd\u3002 OWOD\u672c\u6587\u5219\u57fa\u4e8e\u8fd9\u4e2a\u505a\u6cd5\uff0c\u7ef4\u62a4\u4e00\u4e2a class-balanced \u7684\u6570\u636e\u6c60,\u6bcf\u6b21incremental learning\u7684\u65f6\u5019\u5c31\u628a\u6a21\u578b\u5728\u8fd9\u4e9b\u6570\u636e\u4e0afine-tune\u4e00\u4e0b\u3002","title":"Towards Open World Object Detection (OWOD)"},{"location":"other_categories/object_detection_2D/toward_open_world_detection/#towards-open-world-object-detection-owod","text":"\u8fd9\u7bc7paper\u5c1d\u8bd5\u89e3\u51b3\u4e00\u4e2a\u5f88\u5927\u7684\u95ee\u9898\uff0c\u5b83\u4eec\u60f3\u8981\u8bad\u7ec3\u4e00\u4e2a\u7f51\u7edc: \u5b9e\u73b02D\u76ee\u6807\u68c0\u6d4b\uff0c\u4e14\u5bf9\u4e8e\u8fd8\u6ca1\u88ab\u5f15\u5165\u7684\u7269\u4f53\u6807\u6ce8\u4e3a\"\u672a\u77e5\"\uff0c\u5e76\u5c1d\u8bd5\u6846\u51fa\u6765. \u589e\u91cf\u5730\u5b66\u4e60\u65b0\u7684label\uff0c\u540c\u65f6\u4e0d\u80fd\u5fd8\u6389\u4e4b\u524d\u5b66\u4f1a\u7684\u7c7b\u522b.","title":"Towards Open World Object Detection (OWOD)"},{"location":"other_categories/object_detection_2D/toward_open_world_detection/#architecture-and-pipeline","text":"","title":"Architecture and Pipeline"},{"location":"other_categories/object_detection_2D/toward_open_world_detection/#auto-labelling-unknowns-with-rpn","text":"\u8fd9\u4e2a\u90e8\u5206\u542c\u8d77\u6765\u6bd4\u8f83\u7384\u5b66\uff0c\u5b9e\u9645\u64cd\u4f5c\u4e0a\u5373\u662f\u4ece\u80cc\u666fproposals (first stage detection)\u91cc\u9762\u9009\u62e9top-k\u6700\u9ad8objectness scores\u7684candidates\u5373\u4e3a\u672a\u77e5\u7269\u4f53\u7684candidate.","title":"Auto-labelling Unknowns with RPN"},{"location":"other_categories/object_detection_2D/toward_open_world_detection/#energy-based-unknown-identifier","text":"\u8fd9\u4e00\u6bb5\u5728\u8bba\u6587\u4e2d\u672c\u6587\u5e76\u4e0d\u80fd\u7406\u89e3\u5730\u5f88\u900f\u5f7b\uff0c\u4f46\u662f\u4ece\u4ee3\u7801\u4e2d\u53ef\u4ee5\u5148\u7406\u89e3\u5b9e\u9645\u8fdb\u884c\u7684\u4e8b\u60c5: \u5728\u8bad\u7ec3\u7684\u8fc7\u7a0b\u4e2d\uff0c\u8ba9RPN\u8f93\u51fa\u4e00\u7cfb\u5217\u7684proposals (nms, top_k, gt), roi_head\u518d\u8f93\u51fa\u6700\u7ec8\u7684\u5206\u7c7blogits. \u4ece\u4e2d\u9009\u53d6\u4e0d\u662f\u7701\u7565\u7684(IoU matching\u5728\u4e34\u754c\u4e4b\u95f4)\u3002\u5982\u679c\u4e3abackground, \u5219\u7406\u89e3\u4e3aunknown. \u5982\u679c\u4e3a\u5728\u8bad\u7ec3\u7684\u7c7b\uff0c\u5219\u7406\u89e3\u4e3aknown.\u5b58\u50a8 logits\u77e2\u91cf\u548c\u5206\u7c7b\u7684\u7ed3\u679c\u5230\u6587\u4ef6\u4e2d. \u5728\u8bad\u7ec3\u7ed3\u675f\u7684\u65f6\u5019\uff0c\u5206\u6790\u80fd\u91cf\uff0c \u4f7f\u7528\u6587\u7ae0\u7684\u516c\u5f0f\u8ba1\u7b97\u80fd\u91cf lse = T\\log\\sum_j\\exp{(g_l(\\mathbf{f})/T)} .\u6587\u4e2d\u63cf\u8ff0\uff0c\u4ece\u76f4\u89c9\u4e0a\u53ef\u4ee5\u7406\u89e3\u7684\u662fknown classes\u7684\u4e0d\u786e\u5b9a\u5ea6\u6bd4\u8f83\u5c0f\uff0c\u80fd\u91cf\u7684\u7edd\u5bf9\u503c\u66f4\u5927\uff0cunknown\u7684\u4e0d\u786e\u5b9a\u5ea6\u6bd4\u8f83\u5927\uff0c\u80fd\u91cf\u7684\u7edd\u5bf9\u503c\u6bd4\u8f83\u5c0f\u3002 \u6839\u636e\u4e24\u7c7b\u7684\u6570\u636e\u5206\u5e03\uff0c\u62df\u5408\u4e24\u4e2a Weibull \u5206\u5e03\uff0c\u4f7f\u7528\u7684\u5e93\u662f reliability.Fitters.Fit_Weibull_3P . \u5b58\u4e0b\u4e24\u4e2a\u6a21\u578b \u5728\u63a8\u7406/\u53ef\u89c6\u5316\u7684\u65f6\u5019\uff0c\u9700\u8981\u5148\u8bbe\u5b9a\u4e00\u4e2athreshold(0.61)\u5e76\u8f93\u51fa\u4e00\u7cfb\u5217\u7684bounding boxes, \u6839\u636e\u6bcf\u4e2a\u8f93\u51fabounding boxes \u7684logit\u5728\u4e24\u4e2aweibull\u5206\u5e03\u4e0a\u7684\u6982\u7387\u5bc6\u5ea6(\u5b9e\u9645\u64cd\u4f5c\u662f\u9644\u8fd1\u4e00\u4e2a\u5c0f\u8303\u56f4\u7684\u79ef\u5206)\u6bd4\u8f83\uff0c\u5224\u65ad\u8fd9\u4e2aobject\u7a76\u7adf\u662f\u5df2\u77e5\u8fd8\u662f\u672a\u77e5. \u5b9e\u8d28\u4e0a\u5c31\u662f\u6839\u636e\u662f\u8f93\u51fa\u7f51\u7edc\u81ea\u9002\u5e94\u7684\u5b66\u4e60\u4e00\u4e2a\u5206\u7c7b\u5668\u5224\u65ad\u73b0\u5728\u7684\u5206\u7c7b\u662f\u786e\u5b9a\u7684\u8fd8\u662f\u4e0d\u786e\u5b9a\u7684\u3002","title":"Energy Based Unknown Identifier"},{"location":"other_categories/object_detection_2D/toward_open_world_detection/#gdumb-a-simple-approach-that-questions-our-progress-in-continual-learning","text":"pdf code OWOD\u7684\u7b2c\u56db\u90e8\u5206\u5728\u4e8e\u62b5\u5236\u9057\u5fd8\uff0c\u8fd9\u5f15\u81ea\u4e8e GDumb\u8fd9\u7bc7\u6587\u7ae0\u7684\u4e00\u4e2a\u7b80\u5355\u505a\u6cd5\u3002 GDumb\u4e00\u6587\u5bf9 Continuous Learning \u793e\u533a\u63d0\u51fa\u4e86\u5f88\u5f3a\u7684\u8d28\u7591\u3002 GDumb\u7684\u7b97\u6cd5\u5c31\u662f\u5728\u8bad\u7ec3\u7684\u8fc7\u7a0b\u4e2d\u8d2a\u5a6a\u5730\u7ef4\u62a4\u4e00\u4e2a\u989d\u5b9a\u5927\u5c0f\u7684\u6570\u636e\u8bb0\u5fc6\u6c60(python deque\uff0c\u6bcf\u4e2astep\u8bad\u7ec3\u90fd\u5c06\u6570\u636e\u5b58\u8d77\u6765). \u5728incremental learning\u7684\u65f6\u5019\u5c06\u539f\u6765\u7684\u6570\u636e\u62ff\u51fa\u6765\u91cd\u65b0\u8bad\u7ec3\u4e00\u4e0b\u5373\u53ef\uff0c\u8fd9\u4e2a\u505a\u6cd5\u505a\u51fa\u4e86\u5f88\u9ad8\u7684\u6027\u80fd\u3002 OWOD\u672c\u6587\u5219\u57fa\u4e8e\u8fd9\u4e2a\u505a\u6cd5\uff0c\u7ef4\u62a4\u4e00\u4e2a class-balanced \u7684\u6570\u636e\u6c60,\u6bcf\u6b21incremental learning\u7684\u65f6\u5019\u5c31\u628a\u6a21\u578b\u5728\u8fd9\u4e9b\u6570\u636e\u4e0afine-tune\u4e00\u4e0b\u3002","title":"GDumb: A Simple Approach that Questions Our Progress in Continual Learning"},{"location":"other_categories/object_detection_2D/uncertainty_detector/","text":"Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors \u8fd9\u7bc7paper\u662f ICLR 2021 under-review\u7684\u4e00\u7bc7paper\uff0c\u5206\u6570\u4e3a 9666. \u5728\u8ba8\u8bba\u4e0d\u786e\u5b9a\u6027\u4e0a\u6709\u4e00\u5b9a\u7684\u4ef7\u503c. \u5bf9\u4e8e\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\uff0c\u4f20\u7edf\u7684\u65b9\u6cd5\u662f\u9009\u62e9\u8ba9\u7f51\u7edc\u9884\u6d4b\u5206\u652f\u4f18\u5316\u4e00\u4e2a\u8d1flog likelihood\u51fd\u6570 \\mathrm{NLL}=\\frac{1}{2 N} \\sum_{n=1}^{N}\\left(\\boldsymbol{z}_{n}-\\boldsymbol{\\mu}\\left(\\boldsymbol{x}_{n}, \\boldsymbol{\\theta}\\right)\\right)^{\\top} \\boldsymbol{\\Sigma}\\left(\\boldsymbol{x}_{n}, \\boldsymbol{\\theta}\\right)^{-1}\\left(\\boldsymbol{z}_{n}-\\boldsymbol{\\mu}\\left(\\boldsymbol{x}_{n}, \\boldsymbol{\\theta}\\right)\\right)+\\log \\operatorname{det} \\boldsymbol{\\Sigma}\\left(\\boldsymbol{x}_{n}, \\boldsymbol{\\theta}\\right) \u4f5c\u8005\u6307\u51fa\u7684\u95ee\u9898\u5728\u4e8e\u8fd9\u4e2a\u635f\u5931\u51fd\u6570\u4f1a\u4f7f\u5f97\u9884\u6d4b\u7ed3\u679c\u8fc7\u4e8e\u4e0d\u81ea\u4fe1\uff0c\u4ece\u635f\u5931\u51fd\u6570\u4e0a\u6765\u770b\uff0c\u5c3d\u7ba1\u6700\u4f18\u503c\u662f\u5408\u7406\u7684\uff0c\u4f46\u662f\u5bf9\u9ad8\u7f6e\u4fe1\u5ea6\u7684\u60e9\u7f5a\u5f88\u5927\u3002 \u672c\u6587\u63d0\u51fa\u4e00\u4e2a Energy Score: \\mathrm{ES}=\\frac{1}{N} \\sum_{n=1}^{N}\\left(\\frac{1}{M} \\sum_{i=1}^{M}\\left\\|\\mathbf{z}_{n, i}-\\boldsymbol{z}_{n}\\right\\|-\\frac{1}{2(M-1)} \\sum_{i=1}^{M-1}\\left\\|\\mathbf{z}_{n, i}-\\mathbf{z}_{n, i+1}\\right\\|\\right) \u53ef\u4ee5\u8ba1\u7b97\u53d1\u73b0\u8fd9\u4e2a\u57fa\u4e8e\u91c7\u6837\u7684\u635f\u5931\u51fd\u6570\u76f8\u5bf9\u800c\u8a00\u66f4\u9f13\u52b1\u9ad8\u81ea\u4fe1\u5ea6\u7684\u7ed3\u679c\uff0c\u800c\u4e14\u6b63\u8d1f\u7ed3\u679c\u7684\u60e9\u7f5a\u66f4\u5747\u5300\u3002","title":"Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors"},{"location":"other_categories/object_detection_2D/uncertainty_detector/#estimating-and-evaluating-regression-predictive-uncertainty-in-deep-object-detectors","text":"\u8fd9\u7bc7paper\u662f ICLR 2021 under-review\u7684\u4e00\u7bc7paper\uff0c\u5206\u6570\u4e3a 9666. \u5728\u8ba8\u8bba\u4e0d\u786e\u5b9a\u6027\u4e0a\u6709\u4e00\u5b9a\u7684\u4ef7\u503c. \u5bf9\u4e8e\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\uff0c\u4f20\u7edf\u7684\u65b9\u6cd5\u662f\u9009\u62e9\u8ba9\u7f51\u7edc\u9884\u6d4b\u5206\u652f\u4f18\u5316\u4e00\u4e2a\u8d1flog likelihood\u51fd\u6570 \\mathrm{NLL}=\\frac{1}{2 N} \\sum_{n=1}^{N}\\left(\\boldsymbol{z}_{n}-\\boldsymbol{\\mu}\\left(\\boldsymbol{x}_{n}, \\boldsymbol{\\theta}\\right)\\right)^{\\top} \\boldsymbol{\\Sigma}\\left(\\boldsymbol{x}_{n}, \\boldsymbol{\\theta}\\right)^{-1}\\left(\\boldsymbol{z}_{n}-\\boldsymbol{\\mu}\\left(\\boldsymbol{x}_{n}, \\boldsymbol{\\theta}\\right)\\right)+\\log \\operatorname{det} \\boldsymbol{\\Sigma}\\left(\\boldsymbol{x}_{n}, \\boldsymbol{\\theta}\\right) \u4f5c\u8005\u6307\u51fa\u7684\u95ee\u9898\u5728\u4e8e\u8fd9\u4e2a\u635f\u5931\u51fd\u6570\u4f1a\u4f7f\u5f97\u9884\u6d4b\u7ed3\u679c\u8fc7\u4e8e\u4e0d\u81ea\u4fe1\uff0c\u4ece\u635f\u5931\u51fd\u6570\u4e0a\u6765\u770b\uff0c\u5c3d\u7ba1\u6700\u4f18\u503c\u662f\u5408\u7406\u7684\uff0c\u4f46\u662f\u5bf9\u9ad8\u7f6e\u4fe1\u5ea6\u7684\u60e9\u7f5a\u5f88\u5927\u3002 \u672c\u6587\u63d0\u51fa\u4e00\u4e2a Energy Score: \\mathrm{ES}=\\frac{1}{N} \\sum_{n=1}^{N}\\left(\\frac{1}{M} \\sum_{i=1}^{M}\\left\\|\\mathbf{z}_{n, i}-\\boldsymbol{z}_{n}\\right\\|-\\frac{1}{2(M-1)} \\sum_{i=1}^{M-1}\\left\\|\\mathbf{z}_{n, i}-\\mathbf{z}_{n, i+1}\\right\\|\\right) \u53ef\u4ee5\u8ba1\u7b97\u53d1\u73b0\u8fd9\u4e2a\u57fa\u4e8e\u91c7\u6837\u7684\u635f\u5931\u51fd\u6570\u76f8\u5bf9\u800c\u8a00\u66f4\u9f13\u52b1\u9ad8\u81ea\u4fe1\u5ea6\u7684\u7ed3\u679c\uff0c\u800c\u4e14\u6b63\u8d1f\u7ed3\u679c\u7684\u60e9\u7f5a\u66f4\u5747\u5300\u3002","title":"Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors"},{"location":"other_categories/object_detection_2D/yolof/","text":"You Only Look One-level Feature \u8fd9\u7bc7paper\u8d28\u7591\u4e86object detection\u91cc\u9762FPN\u7684\u771f\u5b9e\u4f5c\u7528\u3002\u672c\u6587\u6307\u51fa\uff0cFPN\u91cc\u9762\uff0c\u5b9e\u9645\u4e0a\u591a\u5c3a\u5ea6\u7684\u8f93\u5165\u662f\u6ca1\u6709\u5fc5\u8981\u7684\uff0c\u771f\u6b63\u8ba9\u6027\u80fd\u63d0\u5347\u7684\u662f\u5bf9\u4e0d\u540c\u5c3a\u5ea6\u7684\u8f93\u51fa\u7528\u4e0d\u540c\u7684feature map\u8fd9\u6837\u7684\u5206\u800c\u6cbb\u4e4b\u601d\u8def\u3002 \u8fd9\u7bc7paper\u8d77\u6b65\u4f7f\u7528\u4e00\u4e2a\u5b9e\u9a8c\u5f97\u5230\u4e00\u4e2a\u6bd4\u8f83\u51fa\u4e4e\u610f\u6599\u7684\u7ed3\u8bba. \u901a\u8fc7\u5206\u6790RetinaNet\u7684\u63a8\u7406\u901f\u5ea6\uff0c\u53d1\u73b0\u9ad8\u5206\u8fa8\u7387\u7684\u7279\u5f81\u82b1\u8d39\u7684\u8fd0\u7b97\u66f4\u52a0\u591a\u3002 YOLOF \u5982\u679c\u5355\u5c3a\u5ea6\u8f93\u51fa\uff0c\u6709\u51e0\u4e2a\u95ee\u9898: \u611f\u53d7\u91ce: \u4f7f\u7528dilated encoder \u63d0\u4f9b\u66f4\u591a\u611f\u53d7\u91ce Ground truth matching: \u4f7f\u7528Max-IoU matching\u4f1a\u4f7f\u5f97\u7269\u4f53\u503e\u5411\u4e8e\u9884\u6d4b\u9762\u79ef\u5927\u7684\u6846\uff0c\u91c7\u7528 Uniform Matching\u7684\u601d\u8def (K Nearest anchor). \u8fd9\u7bc7paper\u4e5f\u662f\u672c\u4eba\u770b\u5230\u7b2c\u4e00\u4e2a\u4f7f\u7528 TIDE \u5bf9\u7f51\u7edc\u7684\u9884\u6d4b\u8fdb\u884c\u5206\u6790\u7684paper.","title":"You Only Look One-level Feature"},{"location":"other_categories/object_detection_2D/yolof/#you-only-look-one-level-feature","text":"\u8fd9\u7bc7paper\u8d28\u7591\u4e86object detection\u91cc\u9762FPN\u7684\u771f\u5b9e\u4f5c\u7528\u3002\u672c\u6587\u6307\u51fa\uff0cFPN\u91cc\u9762\uff0c\u5b9e\u9645\u4e0a\u591a\u5c3a\u5ea6\u7684\u8f93\u5165\u662f\u6ca1\u6709\u5fc5\u8981\u7684\uff0c\u771f\u6b63\u8ba9\u6027\u80fd\u63d0\u5347\u7684\u662f\u5bf9\u4e0d\u540c\u5c3a\u5ea6\u7684\u8f93\u51fa\u7528\u4e0d\u540c\u7684feature map\u8fd9\u6837\u7684\u5206\u800c\u6cbb\u4e4b\u601d\u8def\u3002 \u8fd9\u7bc7paper\u8d77\u6b65\u4f7f\u7528\u4e00\u4e2a\u5b9e\u9a8c\u5f97\u5230\u4e00\u4e2a\u6bd4\u8f83\u51fa\u4e4e\u610f\u6599\u7684\u7ed3\u8bba. \u901a\u8fc7\u5206\u6790RetinaNet\u7684\u63a8\u7406\u901f\u5ea6\uff0c\u53d1\u73b0\u9ad8\u5206\u8fa8\u7387\u7684\u7279\u5f81\u82b1\u8d39\u7684\u8fd0\u7b97\u66f4\u52a0\u591a\u3002","title":"You Only Look One-level Feature"},{"location":"other_categories/object_detection_2D/yolof/#yolof","text":"\u5982\u679c\u5355\u5c3a\u5ea6\u8f93\u51fa\uff0c\u6709\u51e0\u4e2a\u95ee\u9898: \u611f\u53d7\u91ce: \u4f7f\u7528dilated encoder \u63d0\u4f9b\u66f4\u591a\u611f\u53d7\u91ce Ground truth matching: \u4f7f\u7528Max-IoU matching\u4f1a\u4f7f\u5f97\u7269\u4f53\u503e\u5411\u4e8e\u9884\u6d4b\u9762\u79ef\u5927\u7684\u6846\uff0c\u91c7\u7528 Uniform Matching\u7684\u601d\u8def (K Nearest anchor). \u8fd9\u7bc7paper\u4e5f\u662f\u672c\u4eba\u770b\u5230\u7b2c\u4e00\u4e2a\u4f7f\u7528 TIDE \u5bf9\u7f51\u7edc\u7684\u9884\u6d4b\u8fdb\u884c\u5206\u6790\u7684paper.","title":"YOLOF"},{"location":"other_categories/object_detection_2D/yolox/","text":"YOLOX: Exceeding YOLO Series in 2021 \u8fd9\u7bc7paper \u662f\u65f7\u89c6\u505a\u7684\u3002 \u8fd9\u7bc7paper\u5c06anchor-free\u4e0edecoupled head\u5f15\u5165\u4e86YOLO\u7cfb\u5217\u3002 \u4ee3\u7801\u4e0a\u63d0\u4f9b\u4e86\u5b8c\u6574\u7684ONNX, TensorRT, NCNN\u7b49\u652f\u6301\u3002\u6027\u80fd\u4e0a\u6709\u4e86\u4e0d\u9519\u7684\u63d0\u5347. \u6838\u5fc3\u65b0\u7684\u5185\u5bb9: Anchor Free Decoupled Head label assignment with SimOTA Anchor Free \u8fd9\u7bc7paper\u91c7\u7528\u7684\u65b9\u6848\u4e0e FCOS \u7684\u76f8\u4f3c\u4f46\u662f\u6709\u4e00\u5b9a\u4e0d\u540c\uff0c\u56db\u4e2a\u8f93\u51fa\u503c\u5206\u522b\u4e3a\u5230\u5de6\u4e0a\u89d2\u7684\u8ddd\u79bb\uff0c\u4ee5\u53ca\u6846\u7684\u5bbd\u548c\u9ad8\uff1b\u6b63\u6837\u672c\u5206\u914d\u65b9\u6848\u4e5f\u5982\u4e0b\u9762\u7684\u8ba8\u8bba\uff0c\u6709\u6240\u4e0d\u540c Decoupled Head label assignment with SimOTA \u8fd9\u4e2a\u65b9\u6cd5\u57fa\u4e8e OTA \u7684\u505a\u6cd5\u3002\u4f46\u662f\u6ca1\u6709\u91c7\u7528sinkhorn\u8fed\u4ee3\u6765\u4f18\u5316\u5206\u914d\u65b9\u6848\uff0c\u53ea\u662f\u8ba1\u7b97\u4e86cost matrix\u4e4b\u540e\uff0c\u5bf9\u6bcf\u4e00\u4e2aground truth\u9009\u53d6cost\u6700\u5c0f\u7684\u524dk\u4e2a\u503c. k\u91c7\u7528\u52a8\u6001\u4f30\u8ba1\u65b9\u6cd5\uff0c\u7b97\u6cd5\u4e0eOTA\u4e00\u81f4.","title":"YOLOX: Exceeding YOLO Series in 2021"},{"location":"other_categories/object_detection_2D/yolox/#yolox-exceeding-yolo-series-in-2021","text":"\u8fd9\u7bc7paper \u662f\u65f7\u89c6\u505a\u7684\u3002 \u8fd9\u7bc7paper\u5c06anchor-free\u4e0edecoupled head\u5f15\u5165\u4e86YOLO\u7cfb\u5217\u3002 \u4ee3\u7801\u4e0a\u63d0\u4f9b\u4e86\u5b8c\u6574\u7684ONNX, TensorRT, NCNN\u7b49\u652f\u6301\u3002\u6027\u80fd\u4e0a\u6709\u4e86\u4e0d\u9519\u7684\u63d0\u5347. \u6838\u5fc3\u65b0\u7684\u5185\u5bb9: Anchor Free Decoupled Head label assignment with SimOTA","title":"YOLOX: Exceeding YOLO Series in 2021"},{"location":"other_categories/object_detection_2D/yolox/#anchor-free","text":"\u8fd9\u7bc7paper\u91c7\u7528\u7684\u65b9\u6848\u4e0e FCOS \u7684\u76f8\u4f3c\u4f46\u662f\u6709\u4e00\u5b9a\u4e0d\u540c\uff0c\u56db\u4e2a\u8f93\u51fa\u503c\u5206\u522b\u4e3a\u5230\u5de6\u4e0a\u89d2\u7684\u8ddd\u79bb\uff0c\u4ee5\u53ca\u6846\u7684\u5bbd\u548c\u9ad8\uff1b\u6b63\u6837\u672c\u5206\u914d\u65b9\u6848\u4e5f\u5982\u4e0b\u9762\u7684\u8ba8\u8bba\uff0c\u6709\u6240\u4e0d\u540c","title":"Anchor Free"},{"location":"other_categories/object_detection_2D/yolox/#decoupled-head","text":"","title":"Decoupled Head"},{"location":"other_categories/object_detection_2D/yolox/#label-assignment-with-simota","text":"\u8fd9\u4e2a\u65b9\u6cd5\u57fa\u4e8e OTA \u7684\u505a\u6cd5\u3002\u4f46\u662f\u6ca1\u6709\u91c7\u7528sinkhorn\u8fed\u4ee3\u6765\u4f18\u5316\u5206\u914d\u65b9\u6848\uff0c\u53ea\u662f\u8ba1\u7b97\u4e86cost matrix\u4e4b\u540e\uff0c\u5bf9\u6bcf\u4e00\u4e2aground truth\u9009\u53d6cost\u6700\u5c0f\u7684\u524dk\u4e2a\u503c. k\u91c7\u7528\u52a8\u6001\u4f30\u8ba1\u65b9\u6cd5\uff0c\u7b97\u6cd5\u4e0eOTA\u4e00\u81f4.","title":"label assignment with SimOTA"},{"location":"other_categories/others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/","text":"Adaptive Unimodal Cost Volume Filtering for Deep Stereo Matching \u8fd9\u7bc7paper\u89e3\u51b3\u4e86\u53cc\u76ee\u5339\u914d->Cost Volume->Disparity\u8f93\u51fa\u8fd9\u4e2a\u6d41\u7a0b\u4e2d\u6700\u540e\u4e00\u6b65\u4e0a\u7684\u75db\u70b9\u95ee\u9898\u3002 \u5728 PSMNet \u4e2d\uff0c\u8ba1\u7b97\u5f97\u5230Cost Volume\u540e\uff0c\u4f5c\u8005\u91c7\u7528Soft-min\u7684\u65b9\u5f0f\u5f97\u5230\u5404\u4e2a\u70b9\u5bf9\u5404\u4e2adisparity\u7684\u7f6e\u4fe1\u5ea6\uff0c\u52a0\u6743\u6c42\u5e73\u5747\u540e\u518d\u4e0eground truth\u4f5c\u5dee\u6c42loss\u3002 \u8fd9\u4e2aloss\u7684\u5f62\u6210\u95ee\u9898\u5728\u4e8e\u4e0d\u540c\u4f4d\u7f6e\u7684\u68af\u5ea6\u5e76\u4e0d\u5747\u5300\uff0c\u53d6\u51b3\u4e8e\u5bf9\u5e94\u7684\u5750\u6807\u503c\u3002\u7b80\u5355\u7684\u5b9e\u9a8c\u53ef\u4ee5\u8bc1\u660e\u8fd9\u4e2aloss\u662f\u4e0d\u80fd\u6536\u655b\u5230\u6700\u4f18\u89e3\u7684(TO DO). \u672c\u6587\u63d0\u4f9b\u4e86\u66f4\u597d\u7684disparity encoding \u65b9\u5f0f\uff0c \u63d0\u51fastereo focal loss\u3002 Pipeline \u8fd9\u56fe\u8bf4\u660e\u4e86\u672c\u6587\u4e0e\u57fa\u7840\u7684 PSMNet \u7684\u533a\u522b\u3002 Loss Formulation Probabilistic Encoding \u9996\u5148\u6839\u636econfidence map\uff0c\u4ee5\u53ca\u5404\u4e2adispairty channel\u4f4d\u4e0eground truth dispairty\u7684\u6570\u503c\u5dee\uff0c\u6c42\u51fa\u671f\u671b\u5f97\u5230\u7684\u6982\u7387\u5206\u5e03: \\begin{aligned} P(d) &=\\operatorname{softmax}\\left(-\\frac{\\left|d-d^{g t}\\right|}{\\sigma}\\right) \\\\ &=\\frac{\\exp \\left(-c_{d}^{g t}\\right)}{\\sum_{d^{\\prime}=0}^{D-1} \\exp \\left(-c_{d^{\\prime}}^{g t}\\right)} \\end{aligned} Encoding\u8fd9\u90e8\u5206\u7684\u4ee3\u7801\u4e3b\u8981\u5728 \u8fd9\u91cc Stereo Focal Loss \u63a5\u7740\u9700\u8981\u4e00\u4e2aloss\u6765\u63cf\u8ff0\u4e24\u4e2a\u6982\u7387\u5206\u5e03\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u8fd9\u91cc\u91c7\u7528\u7684\u516c\u5f0f\u5982\u4e0b \\mathcal{L}_{S F}=\\frac{1}{|\\mathcal{P}|} \\sum_{p \\in \\mathcal{P}}\\left(\\sum_{d=0}^{D-1}\\left(1-P_{p}(d)\\right)^{-\\alpha} \\cdot\\left(-P_{p}(d) \\cdot \\log \\hat{P}_{p}(d)\\right)\\right) \u5b9e\u8d28\u4e0a\u662f Cross Entropy\u7684\u5f62\u6001\u3002 Multi-level Stereo Cost\u7684\u4ee3\u7801\u4e3b\u8981\u5728 \u8fd9\u91cc","title":"Adaptive Unimodal Cost Volume Filtering for Deep Stereo Matching"},{"location":"other_categories/others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/#adaptive-unimodal-cost-volume-filtering-for-deep-stereo-matching","text":"\u8fd9\u7bc7paper\u89e3\u51b3\u4e86\u53cc\u76ee\u5339\u914d->Cost Volume->Disparity\u8f93\u51fa\u8fd9\u4e2a\u6d41\u7a0b\u4e2d\u6700\u540e\u4e00\u6b65\u4e0a\u7684\u75db\u70b9\u95ee\u9898\u3002 \u5728 PSMNet \u4e2d\uff0c\u8ba1\u7b97\u5f97\u5230Cost Volume\u540e\uff0c\u4f5c\u8005\u91c7\u7528Soft-min\u7684\u65b9\u5f0f\u5f97\u5230\u5404\u4e2a\u70b9\u5bf9\u5404\u4e2adisparity\u7684\u7f6e\u4fe1\u5ea6\uff0c\u52a0\u6743\u6c42\u5e73\u5747\u540e\u518d\u4e0eground truth\u4f5c\u5dee\u6c42loss\u3002 \u8fd9\u4e2aloss\u7684\u5f62\u6210\u95ee\u9898\u5728\u4e8e\u4e0d\u540c\u4f4d\u7f6e\u7684\u68af\u5ea6\u5e76\u4e0d\u5747\u5300\uff0c\u53d6\u51b3\u4e8e\u5bf9\u5e94\u7684\u5750\u6807\u503c\u3002\u7b80\u5355\u7684\u5b9e\u9a8c\u53ef\u4ee5\u8bc1\u660e\u8fd9\u4e2aloss\u662f\u4e0d\u80fd\u6536\u655b\u5230\u6700\u4f18\u89e3\u7684(TO DO). \u672c\u6587\u63d0\u4f9b\u4e86\u66f4\u597d\u7684disparity encoding \u65b9\u5f0f\uff0c \u63d0\u51fastereo focal loss\u3002","title":"Adaptive Unimodal Cost Volume Filtering for Deep Stereo Matching"},{"location":"other_categories/others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/#pipeline","text":"\u8fd9\u56fe\u8bf4\u660e\u4e86\u672c\u6587\u4e0e\u57fa\u7840\u7684 PSMNet \u7684\u533a\u522b\u3002","title":"Pipeline"},{"location":"other_categories/others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/#loss-formulation","text":"","title":"Loss Formulation"},{"location":"other_categories/others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/#probabilistic-encoding","text":"\u9996\u5148\u6839\u636econfidence map\uff0c\u4ee5\u53ca\u5404\u4e2adispairty channel\u4f4d\u4e0eground truth dispairty\u7684\u6570\u503c\u5dee\uff0c\u6c42\u51fa\u671f\u671b\u5f97\u5230\u7684\u6982\u7387\u5206\u5e03: \\begin{aligned} P(d) &=\\operatorname{softmax}\\left(-\\frac{\\left|d-d^{g t}\\right|}{\\sigma}\\right) \\\\ &=\\frac{\\exp \\left(-c_{d}^{g t}\\right)}{\\sum_{d^{\\prime}=0}^{D-1} \\exp \\left(-c_{d^{\\prime}}^{g t}\\right)} \\end{aligned} Encoding\u8fd9\u90e8\u5206\u7684\u4ee3\u7801\u4e3b\u8981\u5728 \u8fd9\u91cc","title":"Probabilistic Encoding"},{"location":"other_categories/others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/#stereo-focal-loss","text":"\u63a5\u7740\u9700\u8981\u4e00\u4e2aloss\u6765\u63cf\u8ff0\u4e24\u4e2a\u6982\u7387\u5206\u5e03\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u8fd9\u91cc\u91c7\u7528\u7684\u516c\u5f0f\u5982\u4e0b \\mathcal{L}_{S F}=\\frac{1}{|\\mathcal{P}|} \\sum_{p \\in \\mathcal{P}}\\left(\\sum_{d=0}^{D-1}\\left(1-P_{p}(d)\\right)^{-\\alpha} \\cdot\\left(-P_{p}(d) \\cdot \\log \\hat{P}_{p}(d)\\right)\\right) \u5b9e\u8d28\u4e0a\u662f Cross Entropy\u7684\u5f62\u6001\u3002 Multi-level Stereo Cost\u7684\u4ee3\u7801\u4e3b\u8981\u5728 \u8fd9\u91cc","title":"Stereo Focal Loss"},{"location":"other_categories/others/Attacking_Optical_Flow/","text":"Attacking Optical Flow \u8fd9\u7bc7paper follow Adversarial Patch \u7684\u505a\u6cd5\uff0c\u4e0d\u540c\u4e4b\u5904\u5728\u4e8e\u4f7f\u7528patch\u53bb\u653b\u51fboptical flow \u800c\u4e0d\u662f\u5355\u5206\u7c7b\u7f51\u7edc\u3002 Approach \u4f18\u5316\u7684\u6307\u6807\u662f\u6700\u5927\u5316\u88ab\u653b\u51fb\u540e\u7684\u5149\u6d41\u65b9\u5411\u4e0e\u6b63\u786e\u5149\u6d41\u65b9\u5411\u7684\u5939\u89d2\u3002 \u4f5c\u8005\u6307\u51fa\u73b0\u5728\u6ca1\u6709\u8db3\u591f\u5927\u91cf\u7684 dense\u3000\u7684optical flow\u3000\u6570\u636e\u96c6\uff0c\u56e0\u800c\u4f7f\u7528\u73b0\u6210model\u751f\u6210Pseudo Ground Truth.\u8fd9\u4e5f\u6709\u4e00\u4e2a\u597d\u5904\u5728\u4e8e\u4f7f\u5f97\u8fd9\u4e2a\u65b9\u6cd5\u53ef\u4ee5\u5728\u4efb\u610f\u672a\u6807\u6ce8\u7684\u89c6\u9891\u4e0a\u8bad\u7ec3\uff0c\u5e76\u653b\u51fb\u4efb\u610f\u7684\u5149\u6d41\u6a21\u578b\u3002 \\hat{p}=\\underset{p}{\\operatorname{argmin}} \\mathbb{E}_{\\left(I_{t}, I_{t+1}\\right) \\sim \\mathcal{I}, l \\sim \\mathcal{L}, \\delta \\sim \\mathcal{T}} \\frac{(u, v) \\cdot(\\tilde{u}, \\tilde{v})}{\\|(u, v)\\| \\cdot\\|(\\tilde{u}, \\tilde{v})\\|} \u5177\u4f53\u6027\u80fd\u53ef\u4ee5\u89c2\u770b\u89c6\u9891","title":"Attacking Optical Flow"},{"location":"other_categories/others/Attacking_Optical_Flow/#attacking-optical-flow","text":"\u8fd9\u7bc7paper follow Adversarial Patch \u7684\u505a\u6cd5\uff0c\u4e0d\u540c\u4e4b\u5904\u5728\u4e8e\u4f7f\u7528patch\u53bb\u653b\u51fboptical flow \u800c\u4e0d\u662f\u5355\u5206\u7c7b\u7f51\u7edc\u3002","title":"Attacking Optical Flow"},{"location":"other_categories/others/Attacking_Optical_Flow/#approach","text":"\u4f18\u5316\u7684\u6307\u6807\u662f\u6700\u5927\u5316\u88ab\u653b\u51fb\u540e\u7684\u5149\u6d41\u65b9\u5411\u4e0e\u6b63\u786e\u5149\u6d41\u65b9\u5411\u7684\u5939\u89d2\u3002 \u4f5c\u8005\u6307\u51fa\u73b0\u5728\u6ca1\u6709\u8db3\u591f\u5927\u91cf\u7684 dense\u3000\u7684optical flow\u3000\u6570\u636e\u96c6\uff0c\u56e0\u800c\u4f7f\u7528\u73b0\u6210model\u751f\u6210Pseudo Ground Truth.\u8fd9\u4e5f\u6709\u4e00\u4e2a\u597d\u5904\u5728\u4e8e\u4f7f\u5f97\u8fd9\u4e2a\u65b9\u6cd5\u53ef\u4ee5\u5728\u4efb\u610f\u672a\u6807\u6ce8\u7684\u89c6\u9891\u4e0a\u8bad\u7ec3\uff0c\u5e76\u653b\u51fb\u4efb\u610f\u7684\u5149\u6d41\u6a21\u578b\u3002 \\hat{p}=\\underset{p}{\\operatorname{argmin}} \\mathbb{E}_{\\left(I_{t}, I_{t+1}\\right) \\sim \\mathcal{I}, l \\sim \\mathcal{L}, \\delta \\sim \\mathcal{T}} \\frac{(u, v) \\cdot(\\tilde{u}, \\tilde{v})}{\\|(u, v)\\| \\cdot\\|(\\tilde{u}, \\tilde{v})\\|} \u5177\u4f53\u6027\u80fd\u53ef\u4ee5\u89c2\u770b\u89c6\u9891","title":"Approach"},{"location":"other_categories/others/CAM/","text":"CAM: Class Activation Map \u672c\u6587\u6536\u96c6\u4e0e CAM \u53ef\u89c6\u5316/\u5f31\u76d1\u7763\u5b66\u4e60\u76f8\u5173\u7684paper, \u4e00\u4e2a\u89e3\u91ca\u5f97\u6bd4\u8f83\u6e05\u695a\u5730 \u77e5\u4e4e\u6587\u7ae0 Learning Deep Features for Discriminative Localization pdf code \u8fd9\u7bc7paper\u63d0\u51fa\u7684\u53ef\u89e3\u91ca\u6027\u5206\u7c7b\u65b9\u6cd5\u9700\u8981\u5bf9\u6a21\u578b\u8fdb\u884c\u6539\u9020\uff0c\u8ba9CNN\u7684\u6700\u540e\u4e00\u5c42\u8f93\u51fa\u505a\u4e00\u4e2aglobal average pooling, \u7136\u540e\u4e00\u4e2a\u5168\u8fde\u63a5\u5f97\u5230\u6700\u540e\u7684\u8f93\u51fa\u3002\u8fd9\u6837\u7684\u8bbe\u8ba1\u4e0b CNN\u6700\u540e\u4e00\u5c42\u7684\u7279\u5f81\u5c31\u6709\u5f88\u660e\u786e\u7684\u610f\u4e49\u4e86 \u7f51\u7edc\u8f93\u51fa\u6709\u4ee5\u4e0b\u516c\u5f0f: \\begin{aligned} S_c &= \\sum_k w_k^c F_k \\\\ &= \\sum_k w_k^c \\sum_{x,y} f_k(x, y)\\\\ P_c &= \\frac{\\exp{(S_c)}}{\\sum_c \\exp{(S_c)}} \\end{aligned} \u5176\u4e2d c \u6307\u7c7b\u522b, k \u6307GAP\u540e\u7684\u7279\u5f81\u5c42. Classification map M_c \u4e3a M_c(x,y) = \\sum_kw_k^cf_k(x,y) \u5b9e\u8d28\u4e0a\u5c31\u662f\u5c06\u4e00\u4e2a\u4f4d\u7f6e\u7684\u7279\u5f81\u7528\u5bf9\u5e94\u7c7b\u522b\u7684\u6743\u91cd\u52a0\u548c. Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization pdf code \u7531\u4e8e\u8fd9\u7bc7paper\u7684\u4f7f\u7528\u5df2\u7ecf\u592a\u5927\u4e86\uff0c\u56e0\u800c\u6709\u4e00\u4e2a Keras\u5b98\u65b9\u7684code \u8fd9\u7bc7paper\u7684\u4e3b\u8981\u76ee\u7684\u662f\u8981\u5904\u7406CAM\u5bf9\u7f51\u7edc\u7ed3\u6784\u6709\u8981\u6c42\u8fd9\u4e2a\u95ee\u9898. \u5b9a\u4e49\u7279\u5f81\u56fe k \u5bf9\u5206\u7c7b c \u7684\u91cd\u8981\u6027 \u4e3a\u5bf9\u5e94\u5206\u7c7b\u503c\u8f93\u51fa\u5bf9\u7b2c k \u7279\u5f81\u56fe\u7684\u68af\u5ea6\u7684\u5168\u5c40\u548c. \\alpha_k^c = \\frac{1}{Z} \\sum_i \\sum_j \\frac{\\partial y^c}{\\partial A_{ij}^k} \u5b9a\u4e49activation map\u4e3a (\u53ea\u5173\u6ce8\u6b63\u5411\u6548\u679c\u7684) L_{Grad-CAM}^c = ReLU(\\sum_k \\alpha_k^c A^k) \u6570\u503c\u4e0a\u53ef\u4ee5\u8bc1\u660e\u5728CAM\u7684\u6a21\u578b\u4e0b,\u4e0eCAM\u7684\u8ba1\u7b97\u662f\u4e00\u81f4\u7684.","title":"CAM: Class Activation Map"},{"location":"other_categories/others/CAM/#cam-class-activation-map","text":"\u672c\u6587\u6536\u96c6\u4e0e CAM \u53ef\u89c6\u5316/\u5f31\u76d1\u7763\u5b66\u4e60\u76f8\u5173\u7684paper, \u4e00\u4e2a\u89e3\u91ca\u5f97\u6bd4\u8f83\u6e05\u695a\u5730 \u77e5\u4e4e\u6587\u7ae0","title":"CAM: Class Activation Map"},{"location":"other_categories/others/CAM/#learning-deep-features-for-discriminative-localization","text":"pdf code \u8fd9\u7bc7paper\u63d0\u51fa\u7684\u53ef\u89e3\u91ca\u6027\u5206\u7c7b\u65b9\u6cd5\u9700\u8981\u5bf9\u6a21\u578b\u8fdb\u884c\u6539\u9020\uff0c\u8ba9CNN\u7684\u6700\u540e\u4e00\u5c42\u8f93\u51fa\u505a\u4e00\u4e2aglobal average pooling, \u7136\u540e\u4e00\u4e2a\u5168\u8fde\u63a5\u5f97\u5230\u6700\u540e\u7684\u8f93\u51fa\u3002\u8fd9\u6837\u7684\u8bbe\u8ba1\u4e0b CNN\u6700\u540e\u4e00\u5c42\u7684\u7279\u5f81\u5c31\u6709\u5f88\u660e\u786e\u7684\u610f\u4e49\u4e86 \u7f51\u7edc\u8f93\u51fa\u6709\u4ee5\u4e0b\u516c\u5f0f: \\begin{aligned} S_c &= \\sum_k w_k^c F_k \\\\ &= \\sum_k w_k^c \\sum_{x,y} f_k(x, y)\\\\ P_c &= \\frac{\\exp{(S_c)}}{\\sum_c \\exp{(S_c)}} \\end{aligned} \u5176\u4e2d c \u6307\u7c7b\u522b, k \u6307GAP\u540e\u7684\u7279\u5f81\u5c42. Classification map M_c \u4e3a M_c(x,y) = \\sum_kw_k^cf_k(x,y) \u5b9e\u8d28\u4e0a\u5c31\u662f\u5c06\u4e00\u4e2a\u4f4d\u7f6e\u7684\u7279\u5f81\u7528\u5bf9\u5e94\u7c7b\u522b\u7684\u6743\u91cd\u52a0\u548c.","title":"Learning Deep Features for Discriminative Localization"},{"location":"other_categories/others/CAM/#grad-cam-visual-explanations-from-deep-networks-via-gradient-based-localization","text":"pdf code \u7531\u4e8e\u8fd9\u7bc7paper\u7684\u4f7f\u7528\u5df2\u7ecf\u592a\u5927\u4e86\uff0c\u56e0\u800c\u6709\u4e00\u4e2a Keras\u5b98\u65b9\u7684code \u8fd9\u7bc7paper\u7684\u4e3b\u8981\u76ee\u7684\u662f\u8981\u5904\u7406CAM\u5bf9\u7f51\u7edc\u7ed3\u6784\u6709\u8981\u6c42\u8fd9\u4e2a\u95ee\u9898. \u5b9a\u4e49\u7279\u5f81\u56fe k \u5bf9\u5206\u7c7b c \u7684\u91cd\u8981\u6027 \u4e3a\u5bf9\u5e94\u5206\u7c7b\u503c\u8f93\u51fa\u5bf9\u7b2c k \u7279\u5f81\u56fe\u7684\u68af\u5ea6\u7684\u5168\u5c40\u548c. \\alpha_k^c = \\frac{1}{Z} \\sum_i \\sum_j \\frac{\\partial y^c}{\\partial A_{ij}^k} \u5b9a\u4e49activation map\u4e3a (\u53ea\u5173\u6ce8\u6b63\u5411\u6548\u679c\u7684) L_{Grad-CAM}^c = ReLU(\\sum_k \\alpha_k^c A^k) \u6570\u503c\u4e0a\u53ef\u4ee5\u8bc1\u660e\u5728CAM\u7684\u6a21\u578b\u4e0b,\u4e0eCAM\u7684\u8ba1\u7b97\u662f\u4e00\u81f4\u7684.","title":"Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization"},{"location":"other_categories/others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/","text":"Continuous-time Intensity Estimation Using Event Cameras \u8fd9\u7bc7\u6587\u7ae0\u7ed9\u51fa\u4e86\u4e00\u4e2a\u4f7f\u7528event camera\u589e\u5f3a\u666e\u901acamera\u7684\u7b97\u6cd5\u3002 Event Camera \u4e0e\u6570\u5b66\u57fa\u7840 \u8bbe p, t \u4e3a\u5750\u6807\u4e0e\u65f6\u95f4, Y(p,t_j) \u4ee3\u8868\u539f\u7167\u7247\u5bf9\u5e94\u5750\u6807\u3001\u65f6\u95f4\u4e0a\u7684\u5149\u5f3a\u5ea6\u3002 event camera,\u6bcf\u4e00\u4e2a\u50cf\u7d20\u4f1a\u5b58\u50a8\u5149\u5f3a\u7684\u53d8\u5316\uff0c\u8d85\u8fc7\u4e00\u4e2a\u9608\u503c c \u540e\u4f1a\u53d1\u51fa\u4e00\u4e2a\u5bf9\u5e94\u7684\u51b2\u6fc0\uff0c\u4f1a\u5728\u5404\u4e2a\u65f6\u523b\u5728\u5404\u4e2a\u76f8\u673a\u50cf\u7d20stream,\u7ed9\u51fa e_i(p,t)=\\sigma^p_i c \\delta(t-t^p_i) ,\u5176\u4e2d \\delta \u4e3a\u51b2\u6fc0\u51fd\u6570\u3002 \\sigma \u6307\u6b63\u8d1f\u6781\u6027 E(\\boldsymbol{p}, t):=\\sum_{i=1}^{\\infty} e_{i}(\\boldsymbol{p}, t)=\\sum_{i=1}^{\\infty} \\sigma_{i}^{p} c \\delta\\left(t-t_{i}^{p}\\right) \u53d6log\u4e3a: L^{E}(\\boldsymbol{p}, t):=\\int_{0}^{t} E(\\boldsymbol{p}, \\tau) d \\tau=\\int_{0}^{t} \\sum_{i=1}^{\\infty} \\sigma_{i}^{\\boldsymbol{p}} c \\delta\\left(\\tau-t_{i}^{\\boldsymbol{p}}\\right) d \\tau \u4e92\u8865\u6ee4\u6ce2\u5668 \u516c\u5f0fODE\uff1a \\frac{\\partial}{\\partial t} \\hat{L}(\\boldsymbol{p}, t)=E(\\boldsymbol{p}, t)-\\alpha\\left(\\hat{L}(\\boldsymbol{p}, t)-L^{F}(\\boldsymbol{p}, t)\\right) (\u4e2a\u4eba\u7406\u89e3\u5176\u4f20\u9012\u51fd\u6570\u4e3a \\hat L = \\frac{s}{s+\\alpha}L^E + \\frac{\\alpha}{s+\\alpha}L^F ) \u672c\u6587\u63d0\u5230\uff0c\u7528\u4f4e\u901a\u6ee4\u6ce2\u5668\u5904\u7406\u76f8\u673a\u539f\u6570\u636e\uff0c\u7528\u9ad8\u901a\u6ee4\u6ce2\u5668\u5904\u7406event camera\u7684\u6570\u636e. \u5177\u4f53\u7b97\u6cd5: \u7b2c\u516d\u884c\uff1a\u6307\u7684\u662f\u65f6\u95f4\u95f4\u9694\u5185\uff0c\u76f8\u5f53\u4e8e\u524d\u6587\u63d0\u5230\u7684ODE\u53f3\u8fb9\u53ea\u6709\u7b2c\u4e8c\u9879\uff0c\u6c42\u89e3\u8fd9\u4e2a\u5e38\u5fae\u5206\u65b9\u7a0b\u3002 \u7b2c\u516b\u884c\uff1a \\hat{L}\\left(\\boldsymbol{p}, \\hat{t}_{k+1}^{p}\\right)=\\hat{L}\\left(\\boldsymbol{p},\\left(\\hat{t}_{k+1}^{p}\\right)^{-}\\right)+\\sigma_{k+1}^{\\boldsymbol{p}} c \u662f\u5c06\u51b2\u6fc0\u503c\u76f4\u63a5\u52a0\u5230\u5bf9\u5e94\u7684\u5750\u6807\u4e2d \u7b2c\u5341\u884c\uff0c\u6ce8\u610f\u4e0d\u76f4\u63a5\u6539\u53d8\u8f93\u51fa\u7684 \\hat L \uff0c\u4f46\u662f\u4f1a\u6539\u53d8\u540e\u9762\u5faa\u73af\u7684\u7b2c13\u884c \u7b2c\u5341\u4e00\u884c\uff0c\u63cf\u8ff0\u7684\u662f\u6839\u636e\u8fc7\u66dd\u5149\u6216\u8005\u4f4e\u66dd\u5149\u7684\u53ef\u80fd(\u6bcf\u4e2a\u5750\u6807\u70b9\u6709\u5bf9\u5e94\u7684 \\alpha_p \u503c,\u4e0e\u5f53\u524d\u76f8\u673a\u5bf9\u5e94\u5750\u6807\u7684\u5149\u5f3a L^F \u6709\u5173).\u76f4\u89c9\u4e0a\u6765\u8bf4 \\alpha \u8d8a\u5c0f\uff0c\u8d8a\u4fe1\u4efbevent camera,\u5728\u5149\u5f3a\u63a5\u8fd1\u6700\u5927\u6700\u5c0f\u503c\u7684\u65f6\u5019\u4fe1\u4efbevent camera,\u5149\u5f3a\u4e2d\u95f4\u503c\u7684\u65f6\u5019\u76f8\u5bf9\u66f4\u4fe1\u4efb\u57fa\u7840\u76f8\u673a\uff0c\u5176\u516c\u5f0f\u5982\u4e0b: \\alpha(\\boldsymbol{p}, t)=\\left\\{\\begin{array}{ll}{\\lambda \\alpha_{1}+(1-\\lambda) \\alpha_{1} \\frac{\\left(L^{F}(\\boldsymbol{p}, t)-L_{\\min }\\right)}{\\left(L_{1}-L_{\\min }\\right)}} & {L_{\\min } \\leq L^{F}(\\boldsymbol{p}, t)<L_{1}} \\\\ {\\alpha_{1}} & {L_{1} \\leq L^{F}(\\boldsymbol{p}, t) \\leq L_{2}} \\\\ {\\lambda \\alpha_{1}+(1-\\lambda) \\alpha_{1} \\frac{\\left(L^{F}(\\boldsymbol{p}, t)-L_{\\max }\\right)}{\\left(L_{2}-L_{\\max }\\right)}} & {L_{2}<L^{F}(\\boldsymbol{p}, t) \\leq L_{\\max }}\\end{array}\\right. \u672c\u6587 \\alpha_1 = 2 \\pi, \\lambda=0.1 [L_1, L_2] = [L_{min} +k, L_{max} -k], k = 0.05*(L_{max}-L_{min}) \u5728\u672c\u4eba(Owen Liu)\u7535\u8111\u91cc\u9762\u8fd0\u884c\u5f97\u5230\u7684\u7ed3\u679c \u4e0a\u534a\u56fe\u4e3a\u539f\u56fe\uff0c\u4e0b\u534a\u56fe\u4e3a\u589e\u5f3a\u540e\u7684\u56fe.","title":"Continuous-time Intensity Estimation Using Event Cameras"},{"location":"other_categories/others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/#continuous-time-intensity-estimation-using-event-cameras","text":"\u8fd9\u7bc7\u6587\u7ae0\u7ed9\u51fa\u4e86\u4e00\u4e2a\u4f7f\u7528event camera\u589e\u5f3a\u666e\u901acamera\u7684\u7b97\u6cd5\u3002","title":"Continuous-time Intensity Estimation Using Event Cameras"},{"location":"other_categories/others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/#event-camera","text":"\u8bbe p, t \u4e3a\u5750\u6807\u4e0e\u65f6\u95f4, Y(p,t_j) \u4ee3\u8868\u539f\u7167\u7247\u5bf9\u5e94\u5750\u6807\u3001\u65f6\u95f4\u4e0a\u7684\u5149\u5f3a\u5ea6\u3002 event camera,\u6bcf\u4e00\u4e2a\u50cf\u7d20\u4f1a\u5b58\u50a8\u5149\u5f3a\u7684\u53d8\u5316\uff0c\u8d85\u8fc7\u4e00\u4e2a\u9608\u503c c \u540e\u4f1a\u53d1\u51fa\u4e00\u4e2a\u5bf9\u5e94\u7684\u51b2\u6fc0\uff0c\u4f1a\u5728\u5404\u4e2a\u65f6\u523b\u5728\u5404\u4e2a\u76f8\u673a\u50cf\u7d20stream,\u7ed9\u51fa e_i(p,t)=\\sigma^p_i c \\delta(t-t^p_i) ,\u5176\u4e2d \\delta \u4e3a\u51b2\u6fc0\u51fd\u6570\u3002 \\sigma \u6307\u6b63\u8d1f\u6781\u6027 E(\\boldsymbol{p}, t):=\\sum_{i=1}^{\\infty} e_{i}(\\boldsymbol{p}, t)=\\sum_{i=1}^{\\infty} \\sigma_{i}^{p} c \\delta\\left(t-t_{i}^{p}\\right) \u53d6log\u4e3a: L^{E}(\\boldsymbol{p}, t):=\\int_{0}^{t} E(\\boldsymbol{p}, \\tau) d \\tau=\\int_{0}^{t} \\sum_{i=1}^{\\infty} \\sigma_{i}^{\\boldsymbol{p}} c \\delta\\left(\\tau-t_{i}^{\\boldsymbol{p}}\\right) d \\tau","title":"Event Camera \u4e0e\u6570\u5b66\u57fa\u7840"},{"location":"other_categories/others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/#_1","text":"\u516c\u5f0fODE\uff1a \\frac{\\partial}{\\partial t} \\hat{L}(\\boldsymbol{p}, t)=E(\\boldsymbol{p}, t)-\\alpha\\left(\\hat{L}(\\boldsymbol{p}, t)-L^{F}(\\boldsymbol{p}, t)\\right) (\u4e2a\u4eba\u7406\u89e3\u5176\u4f20\u9012\u51fd\u6570\u4e3a \\hat L = \\frac{s}{s+\\alpha}L^E + \\frac{\\alpha}{s+\\alpha}L^F ) \u672c\u6587\u63d0\u5230\uff0c\u7528\u4f4e\u901a\u6ee4\u6ce2\u5668\u5904\u7406\u76f8\u673a\u539f\u6570\u636e\uff0c\u7528\u9ad8\u901a\u6ee4\u6ce2\u5668\u5904\u7406event camera\u7684\u6570\u636e. \u5177\u4f53\u7b97\u6cd5: \u7b2c\u516d\u884c\uff1a\u6307\u7684\u662f\u65f6\u95f4\u95f4\u9694\u5185\uff0c\u76f8\u5f53\u4e8e\u524d\u6587\u63d0\u5230\u7684ODE\u53f3\u8fb9\u53ea\u6709\u7b2c\u4e8c\u9879\uff0c\u6c42\u89e3\u8fd9\u4e2a\u5e38\u5fae\u5206\u65b9\u7a0b\u3002 \u7b2c\u516b\u884c\uff1a \\hat{L}\\left(\\boldsymbol{p}, \\hat{t}_{k+1}^{p}\\right)=\\hat{L}\\left(\\boldsymbol{p},\\left(\\hat{t}_{k+1}^{p}\\right)^{-}\\right)+\\sigma_{k+1}^{\\boldsymbol{p}} c \u662f\u5c06\u51b2\u6fc0\u503c\u76f4\u63a5\u52a0\u5230\u5bf9\u5e94\u7684\u5750\u6807\u4e2d \u7b2c\u5341\u884c\uff0c\u6ce8\u610f\u4e0d\u76f4\u63a5\u6539\u53d8\u8f93\u51fa\u7684 \\hat L \uff0c\u4f46\u662f\u4f1a\u6539\u53d8\u540e\u9762\u5faa\u73af\u7684\u7b2c13\u884c \u7b2c\u5341\u4e00\u884c\uff0c\u63cf\u8ff0\u7684\u662f\u6839\u636e\u8fc7\u66dd\u5149\u6216\u8005\u4f4e\u66dd\u5149\u7684\u53ef\u80fd(\u6bcf\u4e2a\u5750\u6807\u70b9\u6709\u5bf9\u5e94\u7684 \\alpha_p \u503c,\u4e0e\u5f53\u524d\u76f8\u673a\u5bf9\u5e94\u5750\u6807\u7684\u5149\u5f3a L^F \u6709\u5173).\u76f4\u89c9\u4e0a\u6765\u8bf4 \\alpha \u8d8a\u5c0f\uff0c\u8d8a\u4fe1\u4efbevent camera,\u5728\u5149\u5f3a\u63a5\u8fd1\u6700\u5927\u6700\u5c0f\u503c\u7684\u65f6\u5019\u4fe1\u4efbevent camera,\u5149\u5f3a\u4e2d\u95f4\u503c\u7684\u65f6\u5019\u76f8\u5bf9\u66f4\u4fe1\u4efb\u57fa\u7840\u76f8\u673a\uff0c\u5176\u516c\u5f0f\u5982\u4e0b: \\alpha(\\boldsymbol{p}, t)=\\left\\{\\begin{array}{ll}{\\lambda \\alpha_{1}+(1-\\lambda) \\alpha_{1} \\frac{\\left(L^{F}(\\boldsymbol{p}, t)-L_{\\min }\\right)}{\\left(L_{1}-L_{\\min }\\right)}} & {L_{\\min } \\leq L^{F}(\\boldsymbol{p}, t)<L_{1}} \\\\ {\\alpha_{1}} & {L_{1} \\leq L^{F}(\\boldsymbol{p}, t) \\leq L_{2}} \\\\ {\\lambda \\alpha_{1}+(1-\\lambda) \\alpha_{1} \\frac{\\left(L^{F}(\\boldsymbol{p}, t)-L_{\\max }\\right)}{\\left(L_{2}-L_{\\max }\\right)}} & {L_{2}<L^{F}(\\boldsymbol{p}, t) \\leq L_{\\max }}\\end{array}\\right. \u672c\u6587 \\alpha_1 = 2 \\pi, \\lambda=0.1 [L_1, L_2] = [L_{min} +k, L_{max} -k], k = 0.05*(L_{max}-L_{min})","title":"\u4e92\u8865\u6ee4\u6ce2\u5668"},{"location":"other_categories/others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/#owen-liu","text":"\u4e0a\u534a\u56fe\u4e3a\u539f\u56fe\uff0c\u4e0b\u534a\u56fe\u4e3a\u589e\u5f3a\u540e\u7684\u56fe.","title":"\u5728\u672c\u4eba(Owen Liu)\u7535\u8111\u91cc\u9762\u8fd0\u884c\u5f97\u5230\u7684\u7ed3\u679c"},{"location":"other_categories/others/FADNet/","text":"FADNet: A Fast and Accurate Network for Disparity Estimation \u8fd9\u7bc7paper\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u5b9e\u65f6\u7684\u5feb\u901f\u7684disparity estimation\u7f51\u7edc\uff0c\u4e0e PSMNet \u5bf9\u6bd4\uff0c\u663e\u5b58\u6d88\u8017\u663e\u8457\u51cf\u5c0f\uff0c\u540c\u65f6\u4e00\u5e27\u53cc\u76ee\u901f\u5ea6\u8fbe\u523018.7ms\uff0c\u7565\u5fae\u60ca\u4eba\u3002 architecture \u6a21\u4eff\u4e86DispNet\u7684\u7ed3\u6784\uff0cdispnet\u5219\u57fa\u4e8e FlowNet .\u5176\u4e2d\u4f7f\u7528\u4e86correlation \u5c42\uff0c\u5bf9\u53f3\u56fe\u6c34\u5e73\u65b9\u5411\u4e0a\u5e73\u79fbD\u4e2a\u4f4d\u7f6e\uff0c\u4ee5\u6bcf\u4e00\u4e2a\u5de6\u53f3\u56fe\u5bf9\u5e94\u50cf\u7d20\u4e3a\u4e2d\u5fc3\u9644\u8fd1patch\u5404\u81ea\u6c42\u70b9\u4e58\u7684\u7d2f\u52a0 c\\left(\\mathbf{x}_{1}, \\mathbf{x}_{2}\\right)=\\sum_{\\mathbf{o} \\in[-k, k] \\times[-k, k]}\\left\\langle\\mathbf{f}_{1}\\left(\\mathbf{x}_{1}+\\mathbf{o}\\right), \\mathbf{f}_{2}\\left(\\mathbf{x}_{2}+\\mathbf{o}\\right)\\right\\rangle \u4f5c\u8005\u6307\u51fa\u8fd9\u91cc\u6697\u793a\u4e86\u4e00\u4e2apatch\u91cc\u9762\u4e0d\u540c\u4f4d\u7f6e\u7684\u6743\u91cd\u4e00\u81f4\uff0c\u800c\u8fd9\u5e76\u4e0d\u4e00\u5b9a\u6b63\u786e\uff0c\u56e0\u800c\u9009\u62e9\u5148\u5206\u522b\u505a\u4e00\u4e2a 3\\times3 \u7684\u5377\u79ef\u518d\u8fd0\u884c\u70b9\u4e58 c\\left(\\mathbf{x}_{1}, \\mathbf{x}_{2}\\right)=\\sum\\left\\langle\\mathbf{f}_{1}\\left(\\mathbf{x}_{1}\\right), \\mathbf{f}_{2}\\left(\\mathbf{x}_{2}\\right)\\right\\rangle def build_corr(img_left, img_right, max_disp=40): B, C, H, W = img_left.shape volume = img_left.new_zeros([B, max_disp, H, W]) for i in range(max_disp): if i > 0: volume[:, i, :, i:] = (img_left[:, :, :, i:] * img_right[:, :, :, :-i]).mean(dim=1) else: volume[:, i, :, :] = (img_left[:, :, :, :] * img_right[:, :, :, :]).mean(dim=1) volume = volume.contiguous() return volume \u7b2c\u4e8c\u90e8\u5206\u91c7\u7528\u7684\u662f\u4e0e FlowNetV2 \u4e00\u81f4\u7684\u601d\u8def\uff0c\u5c06\u53f3\u56fe\u91cd\u91c7\u6837\u5230\u5de6\u56fe. inputs_net2 = torch.cat((inputs, resampled_img1, dispnetc_final_flow, norm_diff_img0), dim = 1)","title":"FADNet: A Fast and Accurate Network for Disparity Estimation"},{"location":"other_categories/others/FADNet/#fadnet-a-fast-and-accurate-network-for-disparity-estimation","text":"\u8fd9\u7bc7paper\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u5b9e\u65f6\u7684\u5feb\u901f\u7684disparity estimation\u7f51\u7edc\uff0c\u4e0e PSMNet \u5bf9\u6bd4\uff0c\u663e\u5b58\u6d88\u8017\u663e\u8457\u51cf\u5c0f\uff0c\u540c\u65f6\u4e00\u5e27\u53cc\u76ee\u901f\u5ea6\u8fbe\u523018.7ms\uff0c\u7565\u5fae\u60ca\u4eba\u3002","title":"FADNet: A Fast and Accurate Network for Disparity Estimation"},{"location":"other_categories/others/FADNet/#architecture","text":"\u6a21\u4eff\u4e86DispNet\u7684\u7ed3\u6784\uff0cdispnet\u5219\u57fa\u4e8e FlowNet .\u5176\u4e2d\u4f7f\u7528\u4e86correlation \u5c42\uff0c\u5bf9\u53f3\u56fe\u6c34\u5e73\u65b9\u5411\u4e0a\u5e73\u79fbD\u4e2a\u4f4d\u7f6e\uff0c\u4ee5\u6bcf\u4e00\u4e2a\u5de6\u53f3\u56fe\u5bf9\u5e94\u50cf\u7d20\u4e3a\u4e2d\u5fc3\u9644\u8fd1patch\u5404\u81ea\u6c42\u70b9\u4e58\u7684\u7d2f\u52a0 c\\left(\\mathbf{x}_{1}, \\mathbf{x}_{2}\\right)=\\sum_{\\mathbf{o} \\in[-k, k] \\times[-k, k]}\\left\\langle\\mathbf{f}_{1}\\left(\\mathbf{x}_{1}+\\mathbf{o}\\right), \\mathbf{f}_{2}\\left(\\mathbf{x}_{2}+\\mathbf{o}\\right)\\right\\rangle \u4f5c\u8005\u6307\u51fa\u8fd9\u91cc\u6697\u793a\u4e86\u4e00\u4e2apatch\u91cc\u9762\u4e0d\u540c\u4f4d\u7f6e\u7684\u6743\u91cd\u4e00\u81f4\uff0c\u800c\u8fd9\u5e76\u4e0d\u4e00\u5b9a\u6b63\u786e\uff0c\u56e0\u800c\u9009\u62e9\u5148\u5206\u522b\u505a\u4e00\u4e2a 3\\times3 \u7684\u5377\u79ef\u518d\u8fd0\u884c\u70b9\u4e58 c\\left(\\mathbf{x}_{1}, \\mathbf{x}_{2}\\right)=\\sum\\left\\langle\\mathbf{f}_{1}\\left(\\mathbf{x}_{1}\\right), \\mathbf{f}_{2}\\left(\\mathbf{x}_{2}\\right)\\right\\rangle def build_corr(img_left, img_right, max_disp=40): B, C, H, W = img_left.shape volume = img_left.new_zeros([B, max_disp, H, W]) for i in range(max_disp): if i > 0: volume[:, i, :, i:] = (img_left[:, :, :, i:] * img_right[:, :, :, :-i]).mean(dim=1) else: volume[:, i, :, :] = (img_left[:, :, :, :] * img_right[:, :, :, :]).mean(dim=1) volume = volume.contiguous() return volume \u7b2c\u4e8c\u90e8\u5206\u91c7\u7528\u7684\u662f\u4e0e FlowNetV2 \u4e00\u81f4\u7684\u601d\u8def\uff0c\u5c06\u53f3\u56fe\u91cd\u91c7\u6837\u5230\u5de6\u56fe. inputs_net2 = torch.cat((inputs, resampled_img1, dispnetc_final_flow, norm_diff_img0), dim = 1)","title":"architecture"},{"location":"other_categories/others/GPLVM/","text":"Gaussian Process Latent Variable Models for Visualisation of High Dimensional Data (GPLVM) GPLVM \u662f\u4e00\u4e2a\u5bf9\u9ad8\u7ef4\u6570\u636e\u964d\u7ef4\u7684\u65b9\u6cd5.\u6839\u636e\u4f5c\u8005\u7684talks\uff0c\u5e94\u8be5\u88ab\u547d\u540d\u4e3a probablistic dimentional reduction. \u6570\u636e\u964d\u7ef4\u6700\u4e3a\u57fa\u7840\u7684\u601d\u8def\u5c31\u662f\u627e\u51fa\u6570\u636e\u4e2d\u65b9\u5dee\u6700\u5927\u7684\u4e3b\u7ebf\u6027\u8f74, \u4e5f\u5c31\u662f PCA\u7b97\u6cd5\u7684\u7ed3\u679c. \u672c\u6587\u9996\u5148\u5c1d\u8bd5\u4ece\u6982\u7387\u89d2\u5ea6\u5bf9PCA\u8fdb\u884c\u91cd\u65b0\u63cf\u8ff0,\u5e76\u63d0\u51fa\u66f4\u901a\u7528\u7684GPLVM. waterloo\u7684\u4e00\u4e2apaper summary \u4f5c\u8005\u7684\u4e00\u4e2atalk.pdf Linear Probabilistic PCA (PPCA) \u6570\u636e\u4e0e latent variables\u4e4b\u95f4\u6709\u7ebf\u6027\u9ad8\u65af\u5173\u7cfb, \u5df2\u77e5latent variables X \u6ee1\u8db3\u9ad8\u65af\u5206\u5e03 Marginalize w.r.t. X \\begin{array}{c} p(\\mathbf{Y} \\mid \\mathbf{X}, \\mathbf{W})=\\prod_{i=1}^{n} \\mathcal{N}\\left(\\mathbf{y}_{i_{i}} \\mid \\mathbf{W} \\mathbf{x}_{i, i}, \\sigma^{2} \\mathbf{I}\\right) \\\\ p(\\mathbf{X})=\\prod_{i=1}^{n} \\mathcal{N}\\left(\\mathbf{x}_{i, i} \\mid \\mathbf{0}, \\mathbf{I}\\right) \\\\ p(\\mathbf{Y} \\mid \\mathbf{W})=\\prod_{i=1}^{n} \\mathcal{N}\\left(\\mathbf{y}_{i, j} \\mid \\mathbf{0}, \\mathbf{W} \\mathbf{W}^{\\top}+\\sigma^{2} \\mathbf{I}\\right) \\end{array} \u524d\u4eba\u5206\u6790\u53d1\u73b0\u8fd9\u4e2aPPCA\u7684 W \u7684\u89e3\u4e0ePCA\u7684\u4e00\u81f4. Dual Probabilistic PCA (Dual PCA) \u6570\u636e\u4e0e latent variables\u4e4b\u95f4\u6709\u7ebf\u6027\u9ad8\u65af\u5173\u7cfb \u5df2\u77e5 \u53c2\u6570 W \u6ee1\u8db3\u9ad8\u65af\u5206\u5e03 Marginalize w.r.t. W \\begin{array}{c} p(\\mathbf{Y} \\mid \\mathbf{X}, \\mathbf{W})=\\prod_{i=1}^{n} \\mathcal{N}\\left(\\mathbf{y}_{i,} \\mid \\mathbf{W} \\mathbf{x}_{i, \\prime}, \\sigma^{2} \\mathbf{I}\\right) \\\\ p(\\mathbf{W})=\\prod_{i=1}^{p} \\mathcal{N}\\left(\\mathbf{w}_{i, j} \\mid \\mathbf{0}, \\mathbf{I}\\right) \\\\ p(\\mathbf{Y} \\mid \\mathbf{X})=\\prod_{j=1}^{p} \\mathcal{N}\\left(\\mathbf{y}_{\\mathbf{i}, j} \\mid \\mathbf{0}, \\mathbf{X} \\mathbf{X}^{\\top}+\\sigma^{2} \\mathbf{I}\\right) \\end{array} \u5bf9\u5e94\u7684 log-likelihood \u662f \\begin{aligned} L &= -\\frac{DN}{2}\\ln{(2\\pi)} - \\frac{D}{2} \\ln |K| - \\frac{1}{2} \\text{tr} (K^{-1} Y Y^T) \\\\ K &= \\alpha XX^T + \\beta^{-1}I \\end{aligned} \u4f7f L \u5173\u4e8e X \u7684\u5bfc\u6570\u4e3a\u96f6\uff0c\u53ef\u4ee5\u6c42\u5f97 X \u6ee1\u8db3\u4ee5\u4e0b\u65b9\u7a0b \\frac{1}{D}YY^TK^{-1}X = X \u89c2\u5bdf\u524d\u9762\u7684\u5f0f\u5b50\u53ef\u4ee5\u53d1\u73b0\uff0c \u6570\u636e\u7684\u5206\u5e03\u53d8\u4e3a\u4e86\u4ee5\u96f6\u4e3a\u671f\u671b\uff0c\u7531latent variable\u7ec4\u6210\u7684\u534f\u65b9\u5dee\u77e9\u9635. \u4e00\u4e2a\u81ea\u7136\u7684\u5ef6\u4f38\u5c31\u662f\u5047\u8bbe\u8fd9\u4e2a\u534f\u65b9\u5dee\u77e9\u9635\u662f\u7531\u901a\u7528\u7684\u53ef\u80fd\u975e\u7ebf\u6027\u7684\u6838\u51fd\u6570 K \u66ff\u4ee3\u3002 GPLVM \u5982\u5c06\u6838\u51fd\u6570\u8bbe\u5b9a\u4e3a RBF \u6307\u6570\u6838: k_{n,m} = \\alpha \\exp (-\\frac{\\gamma}{2}(x_n- x_m)^T (x_n - x_m)) + \\delta_{nm}\\beta^{-1} \u635f\u5931\u51fd\u6570 L \u5173\u4e8e\u6838\u51fd\u6570\u7684\u5bfc\u6570\u4ecd\u7136\u6ca1\u6709\u53d8\u5316\uff0c\u53ef\u4ee5\u901a\u8fc7\u4e00\u9636\u4f18\u5316\u65b9\u6cd5\u8fed\u4ee3\u5f97\u5230local minima\u89e3. \u5b9e\u9645\u6267\u884c \u5b9e\u9645\u6267\u884cGPLVM\u7684\u65f6\u5019\u53ef\u4ee5\u91c7\u53d6\u4e00\u4e9b\u6280\u5de7\u51cf\u5c11\u8fd0\u7b97\u91cf: \u7a00\u758f\u5316: \u4ec5\u7528\u90e8\u5206\u6570\u636e(active set)\u8ba1\u7b97\u534f\u65b9\u5dee\u77e9\u9635\u62df\u5408\u9ad8\u65af\u8fc7\u7a0b\u6a21\u578b. \u9009\u62e9\u65b9\u6cd5\u4f9d\u636e: \u4f5c\u8005\u7684\u53e6\u4e00\u7bc7\u5173\u4e8esparcity & GP\u7684\u6587\u7ae0 latent variable\u4f18\u5316:\u4ece active set \u5f97\u5230\u534f\u65b9\u5dee\u4e4b\u540e\uff0c\u7528\u9ad8\u65af\u8fc7\u7a0b\u63a8\u7406\u7684\u65b9\u6cd5\u5f97\u5230\u6bcf\u4e00\u4e2a\u6570\u636e\u7684forward pass, \u5176\u68af\u5ea6\u4ec5\u4e0e\u81ea\u5df1\u4ee5\u53ca\u5176\u4ed6active set\u4e2d\u7684\u6570\u636e\u6709\u5173\uff0c\u56e0\u800c\u6240\u6709 inactive set\u4e2d\u7684\u6570\u636e\u53ef\u4ee5\u5e76\u884c\u6c42\u89e3\u5f97\u5230\u81ea\u5df1\u5bf9\u5e94\u7684latent variable. Extension: Unified Objective Function \u53c2\u8003 \u8fd9\u91cc\u6307\u51fa\u53ef\u4ee5\u7528\u4e00\u4e2a KL divergence\u6765\u7edf\u4e00 PCA, kernel PCA, PPCA, GP-LVM, \u5b83\u4eec\u5206\u522b\u5bf9\u5e94\u5bf9 S, K \u7684\u4e0d\u540c\u8bbe\u7f6e. \\mathrm{KL}(N(\\mathbf{z|0,S)} || N(\\mathbf{z|0,K})) = -\\int N(\\mathbf{z|0,S)}\\;ln\\frac{N(\\mathbf{z|0,K})}{N(\\mathbf{z|0,S)}}\\;d\\mathbf{z} = \\frac{1}{2} ln |\\mathbf{K}| - \\frac{1}{2} ln |\\mathbf{S}| + \\frac{1}{2} tr(\\mathbf{SK^{-1}}) - \\frac{N}{2}","title":"Gaussian Process Latent Variable Models for Visualisation of High Dimensional Data (GPLVM)"},{"location":"other_categories/others/GPLVM/#gaussian-process-latent-variable-models-for-visualisation-of-high-dimensional-data-gplvm","text":"GPLVM \u662f\u4e00\u4e2a\u5bf9\u9ad8\u7ef4\u6570\u636e\u964d\u7ef4\u7684\u65b9\u6cd5.\u6839\u636e\u4f5c\u8005\u7684talks\uff0c\u5e94\u8be5\u88ab\u547d\u540d\u4e3a probablistic dimentional reduction. \u6570\u636e\u964d\u7ef4\u6700\u4e3a\u57fa\u7840\u7684\u601d\u8def\u5c31\u662f\u627e\u51fa\u6570\u636e\u4e2d\u65b9\u5dee\u6700\u5927\u7684\u4e3b\u7ebf\u6027\u8f74, \u4e5f\u5c31\u662f PCA\u7b97\u6cd5\u7684\u7ed3\u679c. \u672c\u6587\u9996\u5148\u5c1d\u8bd5\u4ece\u6982\u7387\u89d2\u5ea6\u5bf9PCA\u8fdb\u884c\u91cd\u65b0\u63cf\u8ff0,\u5e76\u63d0\u51fa\u66f4\u901a\u7528\u7684GPLVM. waterloo\u7684\u4e00\u4e2apaper summary \u4f5c\u8005\u7684\u4e00\u4e2atalk.pdf","title":"Gaussian Process Latent Variable Models for Visualisation of High Dimensional Data (GPLVM)"},{"location":"other_categories/others/GPLVM/#linear-probabilistic-pca-ppca","text":"\u6570\u636e\u4e0e latent variables\u4e4b\u95f4\u6709\u7ebf\u6027\u9ad8\u65af\u5173\u7cfb, \u5df2\u77e5latent variables X \u6ee1\u8db3\u9ad8\u65af\u5206\u5e03 Marginalize w.r.t. X \\begin{array}{c} p(\\mathbf{Y} \\mid \\mathbf{X}, \\mathbf{W})=\\prod_{i=1}^{n} \\mathcal{N}\\left(\\mathbf{y}_{i_{i}} \\mid \\mathbf{W} \\mathbf{x}_{i, i}, \\sigma^{2} \\mathbf{I}\\right) \\\\ p(\\mathbf{X})=\\prod_{i=1}^{n} \\mathcal{N}\\left(\\mathbf{x}_{i, i} \\mid \\mathbf{0}, \\mathbf{I}\\right) \\\\ p(\\mathbf{Y} \\mid \\mathbf{W})=\\prod_{i=1}^{n} \\mathcal{N}\\left(\\mathbf{y}_{i, j} \\mid \\mathbf{0}, \\mathbf{W} \\mathbf{W}^{\\top}+\\sigma^{2} \\mathbf{I}\\right) \\end{array} \u524d\u4eba\u5206\u6790\u53d1\u73b0\u8fd9\u4e2aPPCA\u7684 W \u7684\u89e3\u4e0ePCA\u7684\u4e00\u81f4.","title":"Linear Probabilistic PCA (PPCA)"},{"location":"other_categories/others/GPLVM/#dual-probabilistic-pca-dual-pca","text":"\u6570\u636e\u4e0e latent variables\u4e4b\u95f4\u6709\u7ebf\u6027\u9ad8\u65af\u5173\u7cfb \u5df2\u77e5 \u53c2\u6570 W \u6ee1\u8db3\u9ad8\u65af\u5206\u5e03 Marginalize w.r.t. W \\begin{array}{c} p(\\mathbf{Y} \\mid \\mathbf{X}, \\mathbf{W})=\\prod_{i=1}^{n} \\mathcal{N}\\left(\\mathbf{y}_{i,} \\mid \\mathbf{W} \\mathbf{x}_{i, \\prime}, \\sigma^{2} \\mathbf{I}\\right) \\\\ p(\\mathbf{W})=\\prod_{i=1}^{p} \\mathcal{N}\\left(\\mathbf{w}_{i, j} \\mid \\mathbf{0}, \\mathbf{I}\\right) \\\\ p(\\mathbf{Y} \\mid \\mathbf{X})=\\prod_{j=1}^{p} \\mathcal{N}\\left(\\mathbf{y}_{\\mathbf{i}, j} \\mid \\mathbf{0}, \\mathbf{X} \\mathbf{X}^{\\top}+\\sigma^{2} \\mathbf{I}\\right) \\end{array} \u5bf9\u5e94\u7684 log-likelihood \u662f \\begin{aligned} L &= -\\frac{DN}{2}\\ln{(2\\pi)} - \\frac{D}{2} \\ln |K| - \\frac{1}{2} \\text{tr} (K^{-1} Y Y^T) \\\\ K &= \\alpha XX^T + \\beta^{-1}I \\end{aligned} \u4f7f L \u5173\u4e8e X \u7684\u5bfc\u6570\u4e3a\u96f6\uff0c\u53ef\u4ee5\u6c42\u5f97 X \u6ee1\u8db3\u4ee5\u4e0b\u65b9\u7a0b \\frac{1}{D}YY^TK^{-1}X = X \u89c2\u5bdf\u524d\u9762\u7684\u5f0f\u5b50\u53ef\u4ee5\u53d1\u73b0\uff0c \u6570\u636e\u7684\u5206\u5e03\u53d8\u4e3a\u4e86\u4ee5\u96f6\u4e3a\u671f\u671b\uff0c\u7531latent variable\u7ec4\u6210\u7684\u534f\u65b9\u5dee\u77e9\u9635. \u4e00\u4e2a\u81ea\u7136\u7684\u5ef6\u4f38\u5c31\u662f\u5047\u8bbe\u8fd9\u4e2a\u534f\u65b9\u5dee\u77e9\u9635\u662f\u7531\u901a\u7528\u7684\u53ef\u80fd\u975e\u7ebf\u6027\u7684\u6838\u51fd\u6570 K \u66ff\u4ee3\u3002","title":"Dual Probabilistic PCA (Dual PCA)"},{"location":"other_categories/others/GPLVM/#gplvm","text":"\u5982\u5c06\u6838\u51fd\u6570\u8bbe\u5b9a\u4e3a RBF \u6307\u6570\u6838: k_{n,m} = \\alpha \\exp (-\\frac{\\gamma}{2}(x_n- x_m)^T (x_n - x_m)) + \\delta_{nm}\\beta^{-1} \u635f\u5931\u51fd\u6570 L \u5173\u4e8e\u6838\u51fd\u6570\u7684\u5bfc\u6570\u4ecd\u7136\u6ca1\u6709\u53d8\u5316\uff0c\u53ef\u4ee5\u901a\u8fc7\u4e00\u9636\u4f18\u5316\u65b9\u6cd5\u8fed\u4ee3\u5f97\u5230local minima\u89e3.","title":"GPLVM"},{"location":"other_categories/others/GPLVM/#_1","text":"\u5b9e\u9645\u6267\u884cGPLVM\u7684\u65f6\u5019\u53ef\u4ee5\u91c7\u53d6\u4e00\u4e9b\u6280\u5de7\u51cf\u5c11\u8fd0\u7b97\u91cf: \u7a00\u758f\u5316: \u4ec5\u7528\u90e8\u5206\u6570\u636e(active set)\u8ba1\u7b97\u534f\u65b9\u5dee\u77e9\u9635\u62df\u5408\u9ad8\u65af\u8fc7\u7a0b\u6a21\u578b. \u9009\u62e9\u65b9\u6cd5\u4f9d\u636e: \u4f5c\u8005\u7684\u53e6\u4e00\u7bc7\u5173\u4e8esparcity & GP\u7684\u6587\u7ae0 latent variable\u4f18\u5316:\u4ece active set \u5f97\u5230\u534f\u65b9\u5dee\u4e4b\u540e\uff0c\u7528\u9ad8\u65af\u8fc7\u7a0b\u63a8\u7406\u7684\u65b9\u6cd5\u5f97\u5230\u6bcf\u4e00\u4e2a\u6570\u636e\u7684forward pass, \u5176\u68af\u5ea6\u4ec5\u4e0e\u81ea\u5df1\u4ee5\u53ca\u5176\u4ed6active set\u4e2d\u7684\u6570\u636e\u6709\u5173\uff0c\u56e0\u800c\u6240\u6709 inactive set\u4e2d\u7684\u6570\u636e\u53ef\u4ee5\u5e76\u884c\u6c42\u89e3\u5f97\u5230\u81ea\u5df1\u5bf9\u5e94\u7684latent variable.","title":"\u5b9e\u9645\u6267\u884c"},{"location":"other_categories/others/GPLVM/#extension-unified-objective-function","text":"\u53c2\u8003 \u8fd9\u91cc\u6307\u51fa\u53ef\u4ee5\u7528\u4e00\u4e2a KL divergence\u6765\u7edf\u4e00 PCA, kernel PCA, PPCA, GP-LVM, \u5b83\u4eec\u5206\u522b\u5bf9\u5e94\u5bf9 S, K \u7684\u4e0d\u540c\u8bbe\u7f6e. \\mathrm{KL}(N(\\mathbf{z|0,S)} || N(\\mathbf{z|0,K})) = -\\int N(\\mathbf{z|0,S)}\\;ln\\frac{N(\\mathbf{z|0,K})}{N(\\mathbf{z|0,S)}}\\;d\\mathbf{z} = \\frac{1}{2} ln |\\mathbf{K}| - \\frac{1}{2} ln |\\mathbf{S}| + \\frac{1}{2} tr(\\mathbf{SK^{-1}}) - \\frac{N}{2}","title":"Extension: Unified Objective Function"},{"location":"other_categories/others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/","text":"Gated2Depth: Real-Time Dense Lidar From Gated Images \u8fd9\u7bc7\u8bba\u6587\u4f7f\u7528Gated Image\u4f30\u8ba1\u7a20\u5bc6\u7684\u6df1\u5ea6\u56fe\u3002\u4eae\u70b9\u6709\u51e0\u4e2a\uff0c\u9996\u5148\u662f\u5bf9Gate Imaging\u7684\u539f\u7406\u8ba4\u8bc6\u4ee5\u53ca\u4f7f\u7528\uff0c\u5176\u6b21\u662f\u5176\u7528\u7a00\u758f\u70b9\u4e91\u4f5c\u4e3a\u76d1\u7763\u6765\u6e90\uff0c\u8fdb\u884c\u534a\u76d1\u7763\u7684\u5b66\u4e60. Gated Imaging\u57fa\u672c\u539f\u7406 Gated camera\u672c\u8d28\u4e0a\u662f\u4e00\u4e2a\u4e3b\u52a8\u7684TOF\u76f8\u673a\uff0c\u4e0e\u6fc0\u5149\u6e90\u65f6\u95f4\u540c\u6b65\uff0c\u7cfb\u7edf\u53ef\u4ee5\u63a7\u5236\u76f8\u673a\u7684\u5feb\u95e8\u63a5\u53d7\u5149\u7167\u7684\u65f6\u95f4\uff0c\u5982\u4e0a\u56fe\uff0c\u6bcf\u4e00\u5f20\u56fe\u91cc\u9762\u53ea\u4f1a\u663e\u793a\u4e00\u4e2a\u8ddd\u79bb\u533a\u95f4\u5185\u7684\u7269\u4f53\uff0c\u800c\u6bcf\u4e00\u5f20\u56fe\u4e2d\u6bcf\u4e00\u4e2a\u50cf\u7d20\u7684\u6570\u503c\u4ee3\u8868\u5bf9\u5e94\u50cf\u7d20\u7684\u53cd\u5c04\u7684\u6fc0\u5149\u5f3a\u5ea6\u3002 \u66f4\u591a\u4fe1\u606f\u53ef\u4ee5\u770b NIT\u7684\u4ea7\u54c1pdf \u4ee5\u53ca wiki\u9875\u9762 Depth Learning Pipeline \u8f93\u5165\u5230\u7f51\u7edc\u7684\u8f93\u5165\u662f3\u5c42\u7684\u539f\u59cbgated image.\u4f7f\u7528\u751f\u6210\u7f51\u7edc G \u8f93\u51fa\u591a\u5c3a\u5bf8\u7684\u6df1\u5ea6\u4f30\u8ba1\u56fe\uff0c\u7ed3\u6784\u91c7\u7528\u7684\u662f\u7ecf\u5178\u7684 Unet(pdf) \uff0c\u6709\u4e09\u4e2aloss\u51fd\u6570\u3002\u5176\u4e2d\u7b2c\u4e00\u4e2a\u635f\u5931\u662f \\mathcal{L}_{mult} \u662f\u7a00\u758f\u7684\uff0c\u7528\u7a00\u758f\u7684\u6fc0\u5149\u96f7\u8fbe\u70b9\u4e91\u56fe\u8fdb\u884c\u76d1\u7763\u3002\u540e\u4e24\u4e2aloss\u5728\u540e\u6587\u63cf\u8ff0 \u56fe\u4e2d\u6709\u4e00\u4e2a\u5206\u7c7b\u5668D,\u7c7b\u4f3c\u4e8ePathchGAN. \u5728\u7b2c\u4e00\u4e2a\u8bad\u7ec3\u9636\u6bb5\uff0c\u672c\u6587\u7528\u4eff\u771f\u5408\u6210\u7684\u6570\u636e\u7528\u7c7b\u4f3cLeast-square GAN\u7684\u65b9\u5f0f\uff0c\u8ba9\u8fd9\u4e2a\u5206\u7c7b\u5668\u7f51\u7edc\u5224\u65ad\u4e00\u5f20\u6df1\u5ea6\u56fe\u662f\u6765\u81ea\u4e8e\u5408\u6210\u7684\u6b63\u786e\u56fe\u8fd8\u662f\u6765\u81ea\u4e8eGenerator\u7684\u8f93\u51fa\u3002 \u7b2c\u4e8c\u4e2a\u9636\u6bb5\uff0c\u672c\u6587\u7528\u771f\u5b9egated image,\u7136\u540e\u4fdd\u6301\u5206\u7c7b\u5668\u56fa\u5b9a\uff0c\u540c\u65f6\u7528\u6fc0\u5149\u96f7\u8fbe\u70b9\u4e91\u8fdb\u884c\u76d1\u7763\u8bad\u7ec3, \u635f\u5931\u51fd\u6570 \\mathcal{L} = \\mathcal{L}_{mult} + \\lambda_s \\mathcal{L}_{smooth} + \\lambda_a \\mathcal{L}_{adv} \u76f4\u63a5\u76d1\u7763 \\mathcal{L}_{\\mathrm{mult}}(d, \\tilde{d})=\\sum_{i=1}^{M} \\lambda_{m_{i}} \\mathcal{L}_{\\mathrm{Ll}}\\left(d^{(i)}, \\tilde{d}^{(i)}\\right) \u5176\u4e2d d^{(i)} \u8fd8\u6709 \\tilde{d}^{(i)} \u6307\u4ee3\u5728 scale (i), \\mathcal{L}_{L1} \u6307\u5728\u5bf9\u5e94\u5c3a\u5ea6\u4e0a\u7684\u635f\u5931\u3002\u6bcf\u4e00\u4e2a\u5c3a\u5ea6\u90fd\u662f\u4f7f\u7528 L_1 \u635f\u5931 \u5728\u4f7f\u7528\u5408\u6210\u8bad\u7ec3\u96c6\u8bad\u7ec3\u65f6\uff0c\u6240\u6709\u50cf\u7d20\u90fd\u6709\u5bf9\u5e94\u7684\u8bef\u5dee\u503c\u3002\u5728\u4f7f\u7528\u5b9e\u9645\u7684\u635f\u5931\u51fd\u6570\u65f6\uff0c\u53ea\u6709\u90e8\u5206\u50cf\u7d20\u6709\u76d1\u7763\uff0c\u5982\u4e0b \\mathcal{L}_{\\mathrm{LI}}\\left(d^{(i)}, \\tilde{d}^{(i)}\\right)=\\frac{1}{N} \\sum_{j, k}\\left|d_{j k}^{(i)}-\\tilde{d}_{j k}^{(i)}\\right| m_{j k}^{(i)} \u5149\u6ed1\u635f\u5931 \\mathcal{L}_{\\text {smooth }}=\\frac{1}{N} \\sum_{i, j}\\left|\\partial_{x} d_{i, j}\\right| \\epsilon^{-\\left|\\partial_{x} z_{i, j}\\right|}+\\left|\\partial_{y} d_{i, j}\\right| \\epsilon^{-\\left|\\partial_{y} z_{i, j}\\right|} \u5176\u4e2d z \u4e3a\u8f93\u5165\u56fe\u7247\u7684\u5f3a\u5ea6\u503c\uff0c d \u662f\u6df1\u5ea6\u503c\uff0c \u5bf9\u6297\u635f\u5931 \\begin{aligned} \\mathcal{L}_{\\mathrm{adv}}=& \\frac{1}{2} \\mathbb{E}_{y \\sim p_{\\mathrm{depth}}(y)}\\left[(D(y)-1)^{2}\\right]+\\\\ & \\frac{1}{2} \\mathbb{E}_{x \\sim p_{\\mathrm{gated}}(x)}\\left[(D(G(x)))^{2}\\right] \\end{aligned}","title":"Gated2Depth: Real-Time Dense Lidar From Gated Images"},{"location":"other_categories/others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/#gated2depth-real-time-dense-lidar-from-gated-images","text":"\u8fd9\u7bc7\u8bba\u6587\u4f7f\u7528Gated Image\u4f30\u8ba1\u7a20\u5bc6\u7684\u6df1\u5ea6\u56fe\u3002\u4eae\u70b9\u6709\u51e0\u4e2a\uff0c\u9996\u5148\u662f\u5bf9Gate Imaging\u7684\u539f\u7406\u8ba4\u8bc6\u4ee5\u53ca\u4f7f\u7528\uff0c\u5176\u6b21\u662f\u5176\u7528\u7a00\u758f\u70b9\u4e91\u4f5c\u4e3a\u76d1\u7763\u6765\u6e90\uff0c\u8fdb\u884c\u534a\u76d1\u7763\u7684\u5b66\u4e60.","title":"Gated2Depth: Real-Time Dense Lidar From Gated Images"},{"location":"other_categories/others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/#gated-imaging","text":"Gated camera\u672c\u8d28\u4e0a\u662f\u4e00\u4e2a\u4e3b\u52a8\u7684TOF\u76f8\u673a\uff0c\u4e0e\u6fc0\u5149\u6e90\u65f6\u95f4\u540c\u6b65\uff0c\u7cfb\u7edf\u53ef\u4ee5\u63a7\u5236\u76f8\u673a\u7684\u5feb\u95e8\u63a5\u53d7\u5149\u7167\u7684\u65f6\u95f4\uff0c\u5982\u4e0a\u56fe\uff0c\u6bcf\u4e00\u5f20\u56fe\u91cc\u9762\u53ea\u4f1a\u663e\u793a\u4e00\u4e2a\u8ddd\u79bb\u533a\u95f4\u5185\u7684\u7269\u4f53\uff0c\u800c\u6bcf\u4e00\u5f20\u56fe\u4e2d\u6bcf\u4e00\u4e2a\u50cf\u7d20\u7684\u6570\u503c\u4ee3\u8868\u5bf9\u5e94\u50cf\u7d20\u7684\u53cd\u5c04\u7684\u6fc0\u5149\u5f3a\u5ea6\u3002 \u66f4\u591a\u4fe1\u606f\u53ef\u4ee5\u770b NIT\u7684\u4ea7\u54c1pdf \u4ee5\u53ca wiki\u9875\u9762","title":"Gated Imaging\u57fa\u672c\u539f\u7406"},{"location":"other_categories/others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/#depth-learning-pipeline","text":"\u8f93\u5165\u5230\u7f51\u7edc\u7684\u8f93\u5165\u662f3\u5c42\u7684\u539f\u59cbgated image.\u4f7f\u7528\u751f\u6210\u7f51\u7edc G \u8f93\u51fa\u591a\u5c3a\u5bf8\u7684\u6df1\u5ea6\u4f30\u8ba1\u56fe\uff0c\u7ed3\u6784\u91c7\u7528\u7684\u662f\u7ecf\u5178\u7684 Unet(pdf) \uff0c\u6709\u4e09\u4e2aloss\u51fd\u6570\u3002\u5176\u4e2d\u7b2c\u4e00\u4e2a\u635f\u5931\u662f \\mathcal{L}_{mult} \u662f\u7a00\u758f\u7684\uff0c\u7528\u7a00\u758f\u7684\u6fc0\u5149\u96f7\u8fbe\u70b9\u4e91\u56fe\u8fdb\u884c\u76d1\u7763\u3002\u540e\u4e24\u4e2aloss\u5728\u540e\u6587\u63cf\u8ff0 \u56fe\u4e2d\u6709\u4e00\u4e2a\u5206\u7c7b\u5668D,\u7c7b\u4f3c\u4e8ePathchGAN. \u5728\u7b2c\u4e00\u4e2a\u8bad\u7ec3\u9636\u6bb5\uff0c\u672c\u6587\u7528\u4eff\u771f\u5408\u6210\u7684\u6570\u636e\u7528\u7c7b\u4f3cLeast-square GAN\u7684\u65b9\u5f0f\uff0c\u8ba9\u8fd9\u4e2a\u5206\u7c7b\u5668\u7f51\u7edc\u5224\u65ad\u4e00\u5f20\u6df1\u5ea6\u56fe\u662f\u6765\u81ea\u4e8e\u5408\u6210\u7684\u6b63\u786e\u56fe\u8fd8\u662f\u6765\u81ea\u4e8eGenerator\u7684\u8f93\u51fa\u3002 \u7b2c\u4e8c\u4e2a\u9636\u6bb5\uff0c\u672c\u6587\u7528\u771f\u5b9egated image,\u7136\u540e\u4fdd\u6301\u5206\u7c7b\u5668\u56fa\u5b9a\uff0c\u540c\u65f6\u7528\u6fc0\u5149\u96f7\u8fbe\u70b9\u4e91\u8fdb\u884c\u76d1\u7763\u8bad\u7ec3,","title":"Depth Learning Pipeline"},{"location":"other_categories/others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/#_1","text":"\\mathcal{L} = \\mathcal{L}_{mult} + \\lambda_s \\mathcal{L}_{smooth} + \\lambda_a \\mathcal{L}_{adv}","title":"\u635f\u5931\u51fd\u6570"},{"location":"other_categories/others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/#_2","text":"\\mathcal{L}_{\\mathrm{mult}}(d, \\tilde{d})=\\sum_{i=1}^{M} \\lambda_{m_{i}} \\mathcal{L}_{\\mathrm{Ll}}\\left(d^{(i)}, \\tilde{d}^{(i)}\\right) \u5176\u4e2d d^{(i)} \u8fd8\u6709 \\tilde{d}^{(i)} \u6307\u4ee3\u5728 scale (i), \\mathcal{L}_{L1} \u6307\u5728\u5bf9\u5e94\u5c3a\u5ea6\u4e0a\u7684\u635f\u5931\u3002\u6bcf\u4e00\u4e2a\u5c3a\u5ea6\u90fd\u662f\u4f7f\u7528 L_1 \u635f\u5931 \u5728\u4f7f\u7528\u5408\u6210\u8bad\u7ec3\u96c6\u8bad\u7ec3\u65f6\uff0c\u6240\u6709\u50cf\u7d20\u90fd\u6709\u5bf9\u5e94\u7684\u8bef\u5dee\u503c\u3002\u5728\u4f7f\u7528\u5b9e\u9645\u7684\u635f\u5931\u51fd\u6570\u65f6\uff0c\u53ea\u6709\u90e8\u5206\u50cf\u7d20\u6709\u76d1\u7763\uff0c\u5982\u4e0b \\mathcal{L}_{\\mathrm{LI}}\\left(d^{(i)}, \\tilde{d}^{(i)}\\right)=\\frac{1}{N} \\sum_{j, k}\\left|d_{j k}^{(i)}-\\tilde{d}_{j k}^{(i)}\\right| m_{j k}^{(i)}","title":"\u76f4\u63a5\u76d1\u7763"},{"location":"other_categories/others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/#_3","text":"\\mathcal{L}_{\\text {smooth }}=\\frac{1}{N} \\sum_{i, j}\\left|\\partial_{x} d_{i, j}\\right| \\epsilon^{-\\left|\\partial_{x} z_{i, j}\\right|}+\\left|\\partial_{y} d_{i, j}\\right| \\epsilon^{-\\left|\\partial_{y} z_{i, j}\\right|} \u5176\u4e2d z \u4e3a\u8f93\u5165\u56fe\u7247\u7684\u5f3a\u5ea6\u503c\uff0c d \u662f\u6df1\u5ea6\u503c\uff0c","title":"\u5149\u6ed1\u635f\u5931"},{"location":"other_categories/others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/#_4","text":"\\begin{aligned} \\mathcal{L}_{\\mathrm{adv}}=& \\frac{1}{2} \\mathbb{E}_{y \\sim p_{\\mathrm{depth}}(y)}\\left[(D(y)-1)^{2}\\right]+\\\\ & \\frac{1}{2} \\mathbb{E}_{x \\sim p_{\\mathrm{gated}}(x)}\\left[(D(G(x)))^{2}\\right] \\end{aligned}","title":"\u5bf9\u6297\u635f\u5931"},{"location":"other_categories/others/GaussianProcessVAE/","text":"Gaussian Process and Variantional Autoencoder Gaussian Process Prior Variational Autoencoders pdf code \u57fa\u7840\u7684 Variantional Autoencoder \u5047\u8bbe\u4e0d\u540csample X \u7684latent variables Z \u662f\u72ec\u7acb\u540c\u5206\u5e03\u7684. \u8fd9\u7bc7 NIPS paper \u6307\u51fa\u8fd9\u4e2a\u5047\u8bbe\u5e76\u4e0d\u5408\u7406\uff0c \u5bf9\u4e8e\u6bd4\u5982\u65f6\u5e8f\u56fe\u7247\uff0c\u4e0d\u540csample\u663e\u7136\u4e0d\u662f\u72ec\u7acb\u540c\u5206\u5e03\u7684\uff0c\u9700\u8981\u8003\u8651sample data\u4e4b\u95f4\u7684 covariance. \u672c\u6587\u63d0\u51fa\u7684\u7406\u8bba\u76f4\u89c9\u5c31\u662f\u901a\u8fc7\u9ad8\u65af\u8fc7\u7a0b\u6765\u5efa\u6a21\u4e0d\u540c\u6570\u636e\u6837\u672c X \u7684latent variables Z \u7684\u534f\u65b9\u5dee\u5206\u5e03. Formulation \u8bbe: N \u4e3a\u6837\u672c\u6570, P \u4e3aunique \u7269\u4f53\u7684\u6570\u91cf, Q \u662f\u5404\u4e2a\u7269\u4f53\u4e0d\u540c\u89c6\u89d2\u7684\u6570\u91cf, \\{y_n\\}^N_{n=1} \u662f\u6bcf\u4e00\u4e2a\u6bcf\u4e00\u4e2a\u6837\u672c\u7684 representation. \\{x_p\\}^{P}_{p=1} \u6307 \u5404\u4e2a\u7269\u4f53\u7684 feature. \\{w_q\\}_{q=1}^Q \u6307\u6bcf\u4e00\u4e2a\u89c6\u89d2\u7684\u7279\u5f81. \u6700\u7ec8 z_n \u6307\u7684\u662f\u6bcf\u4e00\u4e2a\u6837\u672c\u7684latent representation. latent representation z_n \u8bbe\u4e3a \u7531\u89c6\u89d2\u7279\u5f81\u4ee5\u53ca\u7269\u4f53\u7279\u5f81 \u52a0\u566a\u58f0\u6620\u5c04\u800c\u5f97 z_n = f(x_{p_n}, w_{q_n}) + \\eta_n \\quad \\eta_n \\sim \\mathcal{N}(0, \\alpha I_L) \u56fe\u7247 y_n \u7531laten representation z_n \u901a\u8fc7\u4e00\u4e2agenerator\u5f97\u5230: y_n = g(z_n) + \\epsilon_n \\quad \\epsilon_n \\sim \\mathcal{N}(0, \\sigma_y^2 I_k) \u4f5c\u8005\u7528\u4e00\u4e2a\u9ad8\u65af\u8fc7\u7a0b\u6765\u5bf9 f \u8fdb\u884c\u5efa\u6a21, \u5173\u952e\u662f\u8981\u5efa\u6a21\u4e0d\u540c\u6837\u672c\u7684covatiance. \u4f7f\u7528CNN\u4f5c\u4e3a g generator. p(Y|X, W, \\phi, \\sigma_y^2, \\theta, \\alpha) = \\int{p(Y|Z, \\phi, \\sigma_y^2) p(Z | X, W, \\theta, \\alpha) dZ} \u5728\u9ad8\u65af\u8fc7\u7a0b\u6a21\u578b\u4e2d, \u8f93\u51fa z \u7684\u6982\u7387\u5206\u5e03\u6ee1\u8db3: \u5747\u503c\u4e3a 0 \u4e0e\u5176\u4ed6\u6240\u6709\u6837\u672c\u7684\u534f\u65b9\u5dee\u4e3a K_\\theta(X, W) , \u5176\u4e2d X \u4e3a\u7279\u5f81, K_\\theta \u4e3a\u6838\u51fd\u6570. \u56e0\u800c\u6709: p(Z | X, W, \\theta, \\alpha) = \\Pi_{l=1}^L \\mathcal{N}(z^l | 0, K_\\theta(X, W) + \\alpha I_N) \u6839\u636e\u672c\u6587\u7684\u5047\u8bbe\u5bf9\u8fd9\u4e2a\u6838\u51fd\u6570\u8fdb\u884c\u62c6\u89e3\uff0c\u89c6\u89d2\u77e2\u91cf\u4ec5\u4e0e\u5176\u4ed6\u6837\u672c\u7684\u89c6\u89d2\u77e2\u91cf\u7684\u7279\u5f81\u6709\u76f8\u5173\u6027, \u7269\u4f53\u7c7b\u522b\u77e2\u91cf\u540c\u7406, \\boldsymbol{K}_{\\boldsymbol{\\theta}}(\\boldsymbol{X}, \\boldsymbol{W})_{n m}=\\mathcal{K}_{\\boldsymbol{\\theta}}^{(\\mathrm{view})}\\left(\\boldsymbol{w}_{q_{n}}, \\boldsymbol{w}_{q_{m}}\\right) \\mathcal{K}_{\\boldsymbol{\\theta}}^{(\\mathrm{object})}\\left(\\boldsymbol{x}_{p_{n}}, \\boldsymbol{x}_{p_{m}}\\right) \u5c06\u524d\u6587\u7684\u5168\u6982\u7387\u516c\u5f0f\u8fdb\u884c\u62c6\u89e3, \u5199\u6210\u671f\u671b: \\begin{aligned} \\log p\\left(\\boldsymbol{Y} \\mid \\boldsymbol{X}, \\boldsymbol{W}, \\boldsymbol{\\phi}, \\sigma_{y}^{2}, \\boldsymbol{\\theta}\\right) \\geq & \\mathbb{E}_{\\boldsymbol{Z} \\sim q_{\\psi}}\\left[\\sum_{n} \\log \\mathcal{N}\\left(\\boldsymbol{y}_{n} \\mid g_{\\boldsymbol{\\phi}}\\left(\\boldsymbol{z}_{n}\\right), \\sigma_{y}^{2} \\boldsymbol{I}_{K}\\right)+\\log p(\\boldsymbol{Z} \\mid \\boldsymbol{X}, \\boldsymbol{W}, \\boldsymbol{\\theta}, \\alpha)\\right]+\\\\ &+\\frac{1}{2} \\sum_{n l} \\log \\left(\\boldsymbol{\\sigma}_{\\psi}^{z 2}\\left(\\boldsymbol{y}_{n}\\right)_{l}\\right)+\\mathrm{const.} \\end{aligned} \u5728loss\u4e2d, \u62c6\u89e3\u5404\u9879\uff0c\u635f\u5931\u51fd\u6570\u4e3a \\begin{array}{l} l\\left(\\phi, \\boldsymbol{\\psi}, \\boldsymbol{\\theta}, \\alpha, \\sigma_{y}^{2}\\right)= \\\\ =N K \\log \\sigma_{y}^{2}+\\underbrace{\\sum_{n} \\frac{\\left\\|\\boldsymbol{y}_{n}-g_{\\phi}\\left(\\boldsymbol{z}_{\\psi_{n}}\\right)\\right\\|^{2}}{2 \\sigma_{y}^{2}}}_{\\text {\u56fe\u50cf\u91cd\u5efa\u9879 }}-\\underbrace{\\log p\\left(\\boldsymbol{Z}_{\\psi} \\mid \\boldsymbol{X}, \\boldsymbol{W}, \\boldsymbol{\\theta}, \\alpha\\right)}_{\\text {\u9ad8\u65af\u8fc7\u7a0b\u9879}}+\\underbrace{\\left.\\frac{1}{2} \\sum_{n l} \\log \\left(\\boldsymbol{\\sigma}_{\\psi}^{z}\\right)_{\\boldsymbol{j}}\\left(\\boldsymbol{y}_{n}\\right)_{l}\\right)}_{\\text {regularization \u9879}}, \\end{array} Efficient GP \u8bbe\u8ba1\u4f7f\u5f97\u9ad8\u65af\u7684\u6838\u51fd\u6570\u77e9\u9635\u662f\u4f4e\u7ef4(low-rank)\u7684\uff0c\u8bbe K = VV^T + \\alpha I . \u5176\u4e2d V \\in \\mathbb{R}^{N\\times H} . \u4f1a\u6709 \\begin{aligned} \\boldsymbol{K}^{-1} \\boldsymbol{M} &=\\frac{1}{\\alpha} \\boldsymbol{I}-\\frac{1}{\\alpha} \\boldsymbol{V}\\left(\\alpha \\boldsymbol{I}+\\boldsymbol{V}^{T} \\boldsymbol{V}\\right)^{-1} \\boldsymbol{V}^{T} \\boldsymbol{M} \\\\ \\log |\\boldsymbol{K}| &=N L \\log \\alpha+\\log \\left|\\boldsymbol{I}+\\frac{1}{\\alpha} \\boldsymbol{V}^{T} \\boldsymbol{V}\\right| \\end{aligned}","title":"Gaussian Process and Variantional Autoencoder"},{"location":"other_categories/others/GaussianProcessVAE/#gaussian-process-and-variantional-autoencoder","text":"","title":"Gaussian Process and Variantional Autoencoder"},{"location":"other_categories/others/GaussianProcessVAE/#gaussian-process-prior-variational-autoencoders","text":"pdf code \u57fa\u7840\u7684 Variantional Autoencoder \u5047\u8bbe\u4e0d\u540csample X \u7684latent variables Z \u662f\u72ec\u7acb\u540c\u5206\u5e03\u7684. \u8fd9\u7bc7 NIPS paper \u6307\u51fa\u8fd9\u4e2a\u5047\u8bbe\u5e76\u4e0d\u5408\u7406\uff0c \u5bf9\u4e8e\u6bd4\u5982\u65f6\u5e8f\u56fe\u7247\uff0c\u4e0d\u540csample\u663e\u7136\u4e0d\u662f\u72ec\u7acb\u540c\u5206\u5e03\u7684\uff0c\u9700\u8981\u8003\u8651sample data\u4e4b\u95f4\u7684 covariance. \u672c\u6587\u63d0\u51fa\u7684\u7406\u8bba\u76f4\u89c9\u5c31\u662f\u901a\u8fc7\u9ad8\u65af\u8fc7\u7a0b\u6765\u5efa\u6a21\u4e0d\u540c\u6570\u636e\u6837\u672c X \u7684latent variables Z \u7684\u534f\u65b9\u5dee\u5206\u5e03.","title":"Gaussian Process Prior Variational Autoencoders"},{"location":"other_categories/others/GaussianProcessVAE/#formulation","text":"\u8bbe: N \u4e3a\u6837\u672c\u6570, P \u4e3aunique \u7269\u4f53\u7684\u6570\u91cf, Q \u662f\u5404\u4e2a\u7269\u4f53\u4e0d\u540c\u89c6\u89d2\u7684\u6570\u91cf, \\{y_n\\}^N_{n=1} \u662f\u6bcf\u4e00\u4e2a\u6bcf\u4e00\u4e2a\u6837\u672c\u7684 representation. \\{x_p\\}^{P}_{p=1} \u6307 \u5404\u4e2a\u7269\u4f53\u7684 feature. \\{w_q\\}_{q=1}^Q \u6307\u6bcf\u4e00\u4e2a\u89c6\u89d2\u7684\u7279\u5f81. \u6700\u7ec8 z_n \u6307\u7684\u662f\u6bcf\u4e00\u4e2a\u6837\u672c\u7684latent representation. latent representation z_n \u8bbe\u4e3a \u7531\u89c6\u89d2\u7279\u5f81\u4ee5\u53ca\u7269\u4f53\u7279\u5f81 \u52a0\u566a\u58f0\u6620\u5c04\u800c\u5f97 z_n = f(x_{p_n}, w_{q_n}) + \\eta_n \\quad \\eta_n \\sim \\mathcal{N}(0, \\alpha I_L) \u56fe\u7247 y_n \u7531laten representation z_n \u901a\u8fc7\u4e00\u4e2agenerator\u5f97\u5230: y_n = g(z_n) + \\epsilon_n \\quad \\epsilon_n \\sim \\mathcal{N}(0, \\sigma_y^2 I_k) \u4f5c\u8005\u7528\u4e00\u4e2a\u9ad8\u65af\u8fc7\u7a0b\u6765\u5bf9 f \u8fdb\u884c\u5efa\u6a21, \u5173\u952e\u662f\u8981\u5efa\u6a21\u4e0d\u540c\u6837\u672c\u7684covatiance. \u4f7f\u7528CNN\u4f5c\u4e3a g generator. p(Y|X, W, \\phi, \\sigma_y^2, \\theta, \\alpha) = \\int{p(Y|Z, \\phi, \\sigma_y^2) p(Z | X, W, \\theta, \\alpha) dZ} \u5728\u9ad8\u65af\u8fc7\u7a0b\u6a21\u578b\u4e2d, \u8f93\u51fa z \u7684\u6982\u7387\u5206\u5e03\u6ee1\u8db3: \u5747\u503c\u4e3a 0 \u4e0e\u5176\u4ed6\u6240\u6709\u6837\u672c\u7684\u534f\u65b9\u5dee\u4e3a K_\\theta(X, W) , \u5176\u4e2d X \u4e3a\u7279\u5f81, K_\\theta \u4e3a\u6838\u51fd\u6570. \u56e0\u800c\u6709: p(Z | X, W, \\theta, \\alpha) = \\Pi_{l=1}^L \\mathcal{N}(z^l | 0, K_\\theta(X, W) + \\alpha I_N) \u6839\u636e\u672c\u6587\u7684\u5047\u8bbe\u5bf9\u8fd9\u4e2a\u6838\u51fd\u6570\u8fdb\u884c\u62c6\u89e3\uff0c\u89c6\u89d2\u77e2\u91cf\u4ec5\u4e0e\u5176\u4ed6\u6837\u672c\u7684\u89c6\u89d2\u77e2\u91cf\u7684\u7279\u5f81\u6709\u76f8\u5173\u6027, \u7269\u4f53\u7c7b\u522b\u77e2\u91cf\u540c\u7406, \\boldsymbol{K}_{\\boldsymbol{\\theta}}(\\boldsymbol{X}, \\boldsymbol{W})_{n m}=\\mathcal{K}_{\\boldsymbol{\\theta}}^{(\\mathrm{view})}\\left(\\boldsymbol{w}_{q_{n}}, \\boldsymbol{w}_{q_{m}}\\right) \\mathcal{K}_{\\boldsymbol{\\theta}}^{(\\mathrm{object})}\\left(\\boldsymbol{x}_{p_{n}}, \\boldsymbol{x}_{p_{m}}\\right) \u5c06\u524d\u6587\u7684\u5168\u6982\u7387\u516c\u5f0f\u8fdb\u884c\u62c6\u89e3, \u5199\u6210\u671f\u671b: \\begin{aligned} \\log p\\left(\\boldsymbol{Y} \\mid \\boldsymbol{X}, \\boldsymbol{W}, \\boldsymbol{\\phi}, \\sigma_{y}^{2}, \\boldsymbol{\\theta}\\right) \\geq & \\mathbb{E}_{\\boldsymbol{Z} \\sim q_{\\psi}}\\left[\\sum_{n} \\log \\mathcal{N}\\left(\\boldsymbol{y}_{n} \\mid g_{\\boldsymbol{\\phi}}\\left(\\boldsymbol{z}_{n}\\right), \\sigma_{y}^{2} \\boldsymbol{I}_{K}\\right)+\\log p(\\boldsymbol{Z} \\mid \\boldsymbol{X}, \\boldsymbol{W}, \\boldsymbol{\\theta}, \\alpha)\\right]+\\\\ &+\\frac{1}{2} \\sum_{n l} \\log \\left(\\boldsymbol{\\sigma}_{\\psi}^{z 2}\\left(\\boldsymbol{y}_{n}\\right)_{l}\\right)+\\mathrm{const.} \\end{aligned} \u5728loss\u4e2d, \u62c6\u89e3\u5404\u9879\uff0c\u635f\u5931\u51fd\u6570\u4e3a \\begin{array}{l} l\\left(\\phi, \\boldsymbol{\\psi}, \\boldsymbol{\\theta}, \\alpha, \\sigma_{y}^{2}\\right)= \\\\ =N K \\log \\sigma_{y}^{2}+\\underbrace{\\sum_{n} \\frac{\\left\\|\\boldsymbol{y}_{n}-g_{\\phi}\\left(\\boldsymbol{z}_{\\psi_{n}}\\right)\\right\\|^{2}}{2 \\sigma_{y}^{2}}}_{\\text {\u56fe\u50cf\u91cd\u5efa\u9879 }}-\\underbrace{\\log p\\left(\\boldsymbol{Z}_{\\psi} \\mid \\boldsymbol{X}, \\boldsymbol{W}, \\boldsymbol{\\theta}, \\alpha\\right)}_{\\text {\u9ad8\u65af\u8fc7\u7a0b\u9879}}+\\underbrace{\\left.\\frac{1}{2} \\sum_{n l} \\log \\left(\\boldsymbol{\\sigma}_{\\psi}^{z}\\right)_{\\boldsymbol{j}}\\left(\\boldsymbol{y}_{n}\\right)_{l}\\right)}_{\\text {regularization \u9879}}, \\end{array}","title":"Formulation"},{"location":"other_categories/others/GaussianProcessVAE/#efficient-gp","text":"\u8bbe\u8ba1\u4f7f\u5f97\u9ad8\u65af\u7684\u6838\u51fd\u6570\u77e9\u9635\u662f\u4f4e\u7ef4(low-rank)\u7684\uff0c\u8bbe K = VV^T + \\alpha I . \u5176\u4e2d V \\in \\mathbb{R}^{N\\times H} . \u4f1a\u6709 \\begin{aligned} \\boldsymbol{K}^{-1} \\boldsymbol{M} &=\\frac{1}{\\alpha} \\boldsymbol{I}-\\frac{1}{\\alpha} \\boldsymbol{V}\\left(\\alpha \\boldsymbol{I}+\\boldsymbol{V}^{T} \\boldsymbol{V}\\right)^{-1} \\boldsymbol{V}^{T} \\boldsymbol{M} \\\\ \\log |\\boldsymbol{K}| &=N L \\log \\alpha+\\log \\left|\\boldsymbol{I}+\\frac{1}{\\alpha} \\boldsymbol{V}^{T} \\boldsymbol{V}\\right| \\end{aligned}","title":"Efficient GP"},{"location":"other_categories/others/Hyperparameter_tuning/","text":"Hyperparameter Tuning \u8fd9\u7bc7paper\u662f\u9605\u8bfb AABO \u4ee5\u53ca\u5176\u524d\u7f6epaper\u7684\u9605\u8bfb\u5f52\u7eb3.\u8fd9\u5176\u4e2d\u7684\u95ee\u9898\u662f\u5982\u4f55\u8fdb\u884c\u81ea\u52a8\u5316\u7684\u8d85\u53c2\u6570\u7684\u8c03\u53c2\u3002 \u7531\u4e8e\u8d85\u53c2\u6570\u65e0\u6cd5\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u672c\u8eab\u7684\u4f18\u5316\u7b97\u6cd5\u4f18\u5316\uff0cgenerally speaking\u662f\u4e00\u4e2a\u65e0\u6cd5\u83b7\u5f97\u68af\u5ea6\u7684\u4f18\u5316\u95ee\u9898\u3002\u56e0\u800c\u53ef\u4ee5\u7406\u89e3\u4e3a\u662f\u4e00\u4e2a\u9ed1\u76d2\u4f18\u5316\u95ee\u9898\u3002 \u4f20\u7edf\u4f18\u5316\u7406\u8bba\u6765\u8bf4\uff0c\u9ed1\u76d2\u4f18\u5316\u5e38\u89c1\u7684\u6c42\u89e3\u65b9\u6cd5\u662f\u641c\u7d22: grid search random search simulated annealing genetic algorithms ... \u5728\u6df1\u5ea6\u5b66\u4e60\u9636\u6bb5\uff0c\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4e5f\u88ab\u7528\u4e8e\u9ed1\u76d2\u4f18\u5316\u3002 \u4f46\u662f\u5bf9\u4e8e\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u8d85\u53c2\u6570\u4f18\u5316\u6765\u8bf4\uff0c\u7279\u70b9\u5728\u4e8e\u6bcf\u4e00\u7ec4\u8d85\u53c2\u7684evaluation\u6240\u9700\u8981\u7684\u7b97\u529b\u6bd4\u8f83\u5927\u3002\u56e0\u800c\u9700\u8981data efficient\u7684\u8d85\u53c2\u641c\u7d22\u7b97\u6cd5\u3002 Surrogate Function pdf code \u8fd9\u4e00\u7c7b\u7684\u7b97\u6cd5\u7684 motivation \u5728\u4e8e\u6839\u636e\u73b0\u6709\u7684\u8d85\u53c2\u3001\u6027\u80fd\u6570\u636e\u7ec4\u62df\u5408\u4e00\u4e2a \"value function\"\uff0c\u7136\u540e\u6839\u636e\u8fd9\u4e2a\u4ef7\u503c\u51fd\u6570\u5f15\u5bfc\u540e\u7eed\u7684\u641c\u7d22\u3002 Gaussian Process \u5728\u6570\u636e\u70b9\u6570\u91cf\u8f83\u5c11\u7684\u60c5\u51b5\u4e0b\uff0c\u4e00\u4e2a\u5e38\u89c1\u7684\u62df\u5408\u7b97\u6cd5\u5c31\u662fGP. \u4e0b\u56fe\u5f15\u81ea medium . Tree-structured Parzen estimators (TPE) \u7531\u4e8e\u5f88\u591a\u8d85\u53c2\u7684\u8bbe\u7f6e\u662f\u5448\u6811\u72b6\u7ed3\u6784(\u5b58\u5728\u5c42\u7ea7\u7ed3\u6784)\uff0c\u4e14\u662f\u79bb\u6563\u503c\u3002\u8fd9\u65f6\u5019\u6bd4\u8f83\u91c7\u7528\u7684\u662f TPE\u7b97\u6cd5\u3002 \u7b97\u6cd5\u7684\u6846\u67b6\u662f\u5c06\u73b0\u6709\u7684\u6570\u636e\u70b9\u6839\u636eperformance\u7684\u6392\u5e8f\uff0c\u5206\u4e3a\"\u597d\"\u7684\u70b9\u4ee5\u53ca\"\u4e0d\u597d\"\u7684\u70b9;\u7136\u540e\u7528\u6838\u51fd\u6570(\u5982\u6df7\u5408\u9ad8\u65af\u6a21\u578b)\u6765\u62df\u5408\u597d\u7684\u70b9\u7684\u6982\u7387\u5206\u5e03\u4ee5\u53ca\u4e0d\u597d\u7684\u70b9\u7684\u6982\u7387\u5206\u5e03: p(x \\mid y)=\\left\\{\\begin{array}{ll} \\ell(x) & \\text { if } y<y^{*} \\\\ g(x) & \\text { if } y \\geq y^{*} \\end{array}\\right. \u5f53\u524d\u4f30\u8ba1\u7684\u6700\u4f18\u70b9\u5219\u662f EI = \\frac{l(x)}{g(x)} \u53d6\u6700\u5927\u503c\u7684\u70b9\u3002 Hyperband pdf Hyperband\u5c5e\u4e8e\u968f\u673a\u641c\u7d22\u7b97\u6cd5. \u9996\u5148\u901a\u8fc7\u76f4\u89c2\u4f8b\u5b50\u4ecb\u7ecdsuccessive halfing algorithm: \u968f\u673a\u5728\u641c\u7d22\u7a7a\u95f4\u4e2d\u5bfb\u627e64\u4e2a\u8d85\u53c2\u7ec4 100 iterations\u4e4b\u540e\u8bc4\u4ef7\u5404\u81ea\u7684 validation loss \u4e22\u5f03\u6027\u80fd\u6700\u5dee\u7684\u4e00\u534a \u5269\u4f59\u8d85\u53c2\u7ec4\u518d\u8bad\u7ec3\u591a100 iterations \u518d\u4e22\u5f03\u6027\u80fd\u6700\u5dee\u7684\u4e00\u534a \u5269\u4f59\u8d85\u53c2\u7ec4\u518d\u8bad\u7ec3\u591a200 iterations \u91cd\u590d\u76f4\u5230\u53ea\u5269\u4e0b\u4e00\u4e2amodel \u7531\u4e8e\u6bcf\u6b21\u4e22\u5f03\u7684\u6bd4\u4f8b\u662f\u4e00\u4e2a\u53ef\u4ee5\u8c03\u8282\u7684\u91cd\u8981\u7684\u8d85\u53c2\u3002Hyperband\u53ef\u4ee5\u7406\u89e3\u4e3a\u5bf9 n \u8fdb\u884cGrid Search. BOHB: ROBUST AND EFFICIENT HYPERPARAMETER OPTIMIZATION AT SCALE pdf blog BOHB\u878d\u5408\u4e86 surrogate model \u4e0ehyperband\u7684\u601d\u8def\u3002 \u4f7f\u7528 TPE \u5bf9\u8d85\u53c2\u6570\u7ec4\u7684\u6027\u80fd\u8fdb\u884c\u9884\u4f30\uff0c\u5e76\u9009\u62e9\u9700\u8981\u5206\u6790\u7684\u8d85\u53c2\u7ec4\u3002\u4f7f\u7528 Hyperband \u964d\u4f4e\u5206\u6790\u6240\u9700\u8981\u7684\u65f6\u95f4\u3002 AABO: Adaptive Anchor Box Optimization for Object Detection via Bayesian Sub-sampling pdf \u8fd9\u7bc7paper\u9996\u5148\u662f\u4e00\u7bc7\u9488\u5bf9\u63d0\u5347\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\u7684\u6587\u7ae0\u3002\u4f5c\u8005\u901a\u8fc7\u4e00\u4e9b\u7b80\u5355\u7684\u5b9e\u9a8c\u5f97\u5230\u4e24\u4e2a\u7ed3\u8bba: \u76ee\u524d\u9884\u8bbe\u7684anchors\u5728COCO\u6570\u636e\u96c6\u4e0a\u4e5f\u8fd8\u4e0d\u662f\u6700\u4f18\u7684\u3002 \u901a\u8fc7\u641c\u7d22anchors configuration\u5f97\u5230\u7684\u63d0\u5347\u53ef\u80fd\u4f1a\u9ad8\u4e8e\u5bf9RPN head\u7684\u641c\u7d22\u5f97\u5230\u7684\u7ed3\u679c\u3002 \u7136\u540e\u63d0\u51fa\u4e86 \u4e0d\u540cfeature map\u9700\u8981\u4e0d\u540c\u7684 predefined anchor. SMC \u91c7\u6837","title":"Hyperparameter Tuning"},{"location":"other_categories/others/Hyperparameter_tuning/#hyperparameter-tuning","text":"\u8fd9\u7bc7paper\u662f\u9605\u8bfb AABO \u4ee5\u53ca\u5176\u524d\u7f6epaper\u7684\u9605\u8bfb\u5f52\u7eb3.\u8fd9\u5176\u4e2d\u7684\u95ee\u9898\u662f\u5982\u4f55\u8fdb\u884c\u81ea\u52a8\u5316\u7684\u8d85\u53c2\u6570\u7684\u8c03\u53c2\u3002 \u7531\u4e8e\u8d85\u53c2\u6570\u65e0\u6cd5\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u672c\u8eab\u7684\u4f18\u5316\u7b97\u6cd5\u4f18\u5316\uff0cgenerally speaking\u662f\u4e00\u4e2a\u65e0\u6cd5\u83b7\u5f97\u68af\u5ea6\u7684\u4f18\u5316\u95ee\u9898\u3002\u56e0\u800c\u53ef\u4ee5\u7406\u89e3\u4e3a\u662f\u4e00\u4e2a\u9ed1\u76d2\u4f18\u5316\u95ee\u9898\u3002 \u4f20\u7edf\u4f18\u5316\u7406\u8bba\u6765\u8bf4\uff0c\u9ed1\u76d2\u4f18\u5316\u5e38\u89c1\u7684\u6c42\u89e3\u65b9\u6cd5\u662f\u641c\u7d22: grid search random search simulated annealing genetic algorithms ... \u5728\u6df1\u5ea6\u5b66\u4e60\u9636\u6bb5\uff0c\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4e5f\u88ab\u7528\u4e8e\u9ed1\u76d2\u4f18\u5316\u3002 \u4f46\u662f\u5bf9\u4e8e\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u8d85\u53c2\u6570\u4f18\u5316\u6765\u8bf4\uff0c\u7279\u70b9\u5728\u4e8e\u6bcf\u4e00\u7ec4\u8d85\u53c2\u7684evaluation\u6240\u9700\u8981\u7684\u7b97\u529b\u6bd4\u8f83\u5927\u3002\u56e0\u800c\u9700\u8981data efficient\u7684\u8d85\u53c2\u641c\u7d22\u7b97\u6cd5\u3002","title":"Hyperparameter Tuning"},{"location":"other_categories/others/Hyperparameter_tuning/#surrogate-function","text":"pdf code \u8fd9\u4e00\u7c7b\u7684\u7b97\u6cd5\u7684 motivation \u5728\u4e8e\u6839\u636e\u73b0\u6709\u7684\u8d85\u53c2\u3001\u6027\u80fd\u6570\u636e\u7ec4\u62df\u5408\u4e00\u4e2a \"value function\"\uff0c\u7136\u540e\u6839\u636e\u8fd9\u4e2a\u4ef7\u503c\u51fd\u6570\u5f15\u5bfc\u540e\u7eed\u7684\u641c\u7d22\u3002","title":"Surrogate Function"},{"location":"other_categories/others/Hyperparameter_tuning/#gaussian-process","text":"\u5728\u6570\u636e\u70b9\u6570\u91cf\u8f83\u5c11\u7684\u60c5\u51b5\u4e0b\uff0c\u4e00\u4e2a\u5e38\u89c1\u7684\u62df\u5408\u7b97\u6cd5\u5c31\u662fGP. \u4e0b\u56fe\u5f15\u81ea medium .","title":"Gaussian Process"},{"location":"other_categories/others/Hyperparameter_tuning/#tree-structured-parzen-estimators-tpe","text":"\u7531\u4e8e\u5f88\u591a\u8d85\u53c2\u7684\u8bbe\u7f6e\u662f\u5448\u6811\u72b6\u7ed3\u6784(\u5b58\u5728\u5c42\u7ea7\u7ed3\u6784)\uff0c\u4e14\u662f\u79bb\u6563\u503c\u3002\u8fd9\u65f6\u5019\u6bd4\u8f83\u91c7\u7528\u7684\u662f TPE\u7b97\u6cd5\u3002 \u7b97\u6cd5\u7684\u6846\u67b6\u662f\u5c06\u73b0\u6709\u7684\u6570\u636e\u70b9\u6839\u636eperformance\u7684\u6392\u5e8f\uff0c\u5206\u4e3a\"\u597d\"\u7684\u70b9\u4ee5\u53ca\"\u4e0d\u597d\"\u7684\u70b9;\u7136\u540e\u7528\u6838\u51fd\u6570(\u5982\u6df7\u5408\u9ad8\u65af\u6a21\u578b)\u6765\u62df\u5408\u597d\u7684\u70b9\u7684\u6982\u7387\u5206\u5e03\u4ee5\u53ca\u4e0d\u597d\u7684\u70b9\u7684\u6982\u7387\u5206\u5e03: p(x \\mid y)=\\left\\{\\begin{array}{ll} \\ell(x) & \\text { if } y<y^{*} \\\\ g(x) & \\text { if } y \\geq y^{*} \\end{array}\\right. \u5f53\u524d\u4f30\u8ba1\u7684\u6700\u4f18\u70b9\u5219\u662f EI = \\frac{l(x)}{g(x)} \u53d6\u6700\u5927\u503c\u7684\u70b9\u3002","title":"Tree-structured Parzen estimators (TPE)"},{"location":"other_categories/others/Hyperparameter_tuning/#hyperband","text":"pdf Hyperband\u5c5e\u4e8e\u968f\u673a\u641c\u7d22\u7b97\u6cd5. \u9996\u5148\u901a\u8fc7\u76f4\u89c2\u4f8b\u5b50\u4ecb\u7ecdsuccessive halfing algorithm: \u968f\u673a\u5728\u641c\u7d22\u7a7a\u95f4\u4e2d\u5bfb\u627e64\u4e2a\u8d85\u53c2\u7ec4 100 iterations\u4e4b\u540e\u8bc4\u4ef7\u5404\u81ea\u7684 validation loss \u4e22\u5f03\u6027\u80fd\u6700\u5dee\u7684\u4e00\u534a \u5269\u4f59\u8d85\u53c2\u7ec4\u518d\u8bad\u7ec3\u591a100 iterations \u518d\u4e22\u5f03\u6027\u80fd\u6700\u5dee\u7684\u4e00\u534a \u5269\u4f59\u8d85\u53c2\u7ec4\u518d\u8bad\u7ec3\u591a200 iterations \u91cd\u590d\u76f4\u5230\u53ea\u5269\u4e0b\u4e00\u4e2amodel \u7531\u4e8e\u6bcf\u6b21\u4e22\u5f03\u7684\u6bd4\u4f8b\u662f\u4e00\u4e2a\u53ef\u4ee5\u8c03\u8282\u7684\u91cd\u8981\u7684\u8d85\u53c2\u3002Hyperband\u53ef\u4ee5\u7406\u89e3\u4e3a\u5bf9 n \u8fdb\u884cGrid Search.","title":"Hyperband"},{"location":"other_categories/others/Hyperparameter_tuning/#bohb-robust-and-efficient-hyperparameter-optimization-at-scale","text":"pdf blog BOHB\u878d\u5408\u4e86 surrogate model \u4e0ehyperband\u7684\u601d\u8def\u3002 \u4f7f\u7528 TPE \u5bf9\u8d85\u53c2\u6570\u7ec4\u7684\u6027\u80fd\u8fdb\u884c\u9884\u4f30\uff0c\u5e76\u9009\u62e9\u9700\u8981\u5206\u6790\u7684\u8d85\u53c2\u7ec4\u3002\u4f7f\u7528 Hyperband \u964d\u4f4e\u5206\u6790\u6240\u9700\u8981\u7684\u65f6\u95f4\u3002","title":"BOHB: ROBUST AND EFFICIENT HYPERPARAMETER OPTIMIZATION AT SCALE"},{"location":"other_categories/others/Hyperparameter_tuning/#aabo-adaptive-anchor-box-optimization-for-object-detection-via-bayesian-sub-sampling","text":"pdf \u8fd9\u7bc7paper\u9996\u5148\u662f\u4e00\u7bc7\u9488\u5bf9\u63d0\u5347\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\u7684\u6587\u7ae0\u3002\u4f5c\u8005\u901a\u8fc7\u4e00\u4e9b\u7b80\u5355\u7684\u5b9e\u9a8c\u5f97\u5230\u4e24\u4e2a\u7ed3\u8bba: \u76ee\u524d\u9884\u8bbe\u7684anchors\u5728COCO\u6570\u636e\u96c6\u4e0a\u4e5f\u8fd8\u4e0d\u662f\u6700\u4f18\u7684\u3002 \u901a\u8fc7\u641c\u7d22anchors configuration\u5f97\u5230\u7684\u63d0\u5347\u53ef\u80fd\u4f1a\u9ad8\u4e8e\u5bf9RPN head\u7684\u641c\u7d22\u5f97\u5230\u7684\u7ed3\u679c\u3002 \u7136\u540e\u63d0\u51fa\u4e86 \u4e0d\u540cfeature map\u9700\u8981\u4e0d\u540c\u7684 predefined anchor. SMC \u91c7\u6837","title":"AABO: Adaptive Anchor Box Optimization for Object Detection via Bayesian Sub-sampling"},{"location":"other_categories/others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/","text":"In Defense of Classical Image Processing: Fast Depth Completion on the CPU \u8fd9\u7bc7\u6587\u7ae0\u89e3\u51b3\u7684\u95ee\u9898\u662f\u4eceCPU\u5b9e\u73b0\u6df1\u5ea6\u8865\u5168\uff0c\u8fd9\u7bc7\u662fAVOD(\u6682\u672a\u6709\u7b80\u4ecb)\u4ee5\u53ca Syn-Multi-view \u7684\u524d\u7f6e. \u5e76\u4e0d\u662f\u6700\u597d\u7684\u7b97\u6cd5\uff0c\u4e0d\u8fc7\u662f\u4e00\u4e2a\u4ee3\u7801\u4f7f\u7528\u96be\u5ea6\u4e0d\u9ad8(\u57fa\u672c\u53ea\u6709\u57fa\u672c\u7684numpy \u4e0e opencv\u8fd0\u7b97)\uff0c\u901f\u5ea6\u5feb(90HZ)\u4e14\u7cbe\u786e\u5ea6\u5c1a\u53ef\u7684\u7b97\u6cd5.\u66f4\u5173\u952e\u662f\u5b8c\u5168\u6ca1\u6709\u4f7f\u7528\u56fe\u50cf\u6570\u636e \u603b\u4f53\u6d41\u7a0b \u6b64\u56fe\u603b\u89c8\u4e86\u6574\u4e2a\u7b97\u6cd5\u7684\u5b8c\u6574\u6d41\u7a0b\uff0c\u4e0b\u9762\u4ece\u95ee\u9898\u7684\u6570\u5b66\u5b9a\u4e49\u4ee5\u53ca\u7b97\u6cd5\u6b65\u9aa4\u8fdb\u884c\u4ecb\u7ecd\u3002 \u95ee\u9898\u6570\u5b66\u5b9a\u4e49 \u5bf9\u4e8e\u4e00\u5f20\u56fe\u7247 I ,\u4e00\u4e2a\u7a00\u758f\u7684\u6df1\u5ea6\u56fe D_{sparse} ,\u627e\u5230\u4e00\u4e2a \\hat f , \u5176\u4e2d f(I,D_{sparse}) = D_{dense} ,\u95ee\u9898\u53ef\u4ee5\u63cf\u8ff0\u4e3a: \\min .\\left\\|\\hat{f}\\left(I, D_{s p a r s e}\\right)-f\\left(I, D_{s p a r s e}\\right)\\right\\|_{F}^{2}=0 \u6d41\u7a0b\u4ecb\u7ecd Depth Inversion \u4ece\u6570\u636e\u7279\u6027\u4e0a\uff0ckitti\u4e2d\u7684\u70b9\u7684\u8ddd\u79bb\u4ece0\u523080m\uff0c\u4e0d\u8fc7\uff0c\u7a7a\u50cf\u7d20\u540c\u6837\u4e3a0\uff0c\u8fd9\u4e0d\u5229\u4e8e\u57fa\u7840OpenCV\u7684\u8ba1\u7b97\u3002\u56e0\u6b64\u8fdb\u884c\u4e00\u4e2a\u64cd\u4f5c\uff0c\u5bf9\u975e\u7a7a\u50cf\u7d20\u8ba1\u7b97 D_{inverted} = 100.0 - D_{input} \uff0c\u4f1a\u5f62\u621020m\u7684\u7f13\u51b2\u533a\u3002\u65b9\u4fbf\u5f62\u6210\u4e00\u4e2avalid or not \u7684mask. \u81ea\u5b9a\u4e49Dilation \u9996\u5148\u5c1d\u8bd5\u8ba9\u7a7a\u50cf\u7d20\u88ab\u9644\u8fd1\u7684\u6709\u6548\u50cf\u7d20\u8986\u76d6(dilation),\u6700\u7ec8\u91c7\u7528\u7684\u662fdiamand kernel \u5c0f\u6d1e\u586b\u8865 \u5148\u8ba1\u7b97\u4e00\u4e2anon-empty mask,\u4f7f\u7528 7\\times 7 full \u6838\u8fdb\u884cdilation,non-empth\u7684\u70b9\u4e0d\u6539\u53d8\u3002 Extension \u4e3a\u4e86\u8865\u5145\u6bd4\u8f83\u9ad8\u7684\u7269\u4f53\uff0c\u6bd4\u5982\u6811\u548c\u5929\u7a7a\uff0c\u5c06\u6bcf\u5217\u6700\u9ad8\u7684\u6709\u6548\u70b9\u76f4\u63a5\u8fde\u63a5\u5230\u6700\u5916\u90e8\u3002 \u5927\u6d1e\u8865\u5145 \u6700\u540e\u4e00\u6b65\u586b\u8865\u524d\u6587\u5b8c\u5168\u6ca1\u6709\u8865\u5145\u7684\u70b9,\u7531\u4e8e\u6ca1\u6709\u56fe\u7247\u6570\u636e\uff0c\u4f7f\u7528\u4e00\u4e2a 31 \\times 31 \u7684\u6838\uff0c\u5bfb\u627e\u6700\u9760\u8fd1\u7684\u672a\u586b\u8865\u70b9\u3002 Blur \u8fde\u7eed\u4f7f\u7528\u4e00\u4e2a 5\\times 5 \u4e2d\u4f4d\u6570\u6a21\u7cca,\u4ee5\u53ca\u4e00\u4e2a 5\\times 5 \u9ad8\u65af\u6a21\u7cca\u3002 Depth Inversion \u5c06\u7b2c\u4e00\u6b65\u7684\u6df1\u5ea6\u8f6c\u56de\u6765\u3002","title":"In Defense of Classical Image Processing: Fast Depth Completion on the CPU"},{"location":"other_categories/others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/#in-defense-of-classical-image-processing-fast-depth-completion-on-the-cpu","text":"\u8fd9\u7bc7\u6587\u7ae0\u89e3\u51b3\u7684\u95ee\u9898\u662f\u4eceCPU\u5b9e\u73b0\u6df1\u5ea6\u8865\u5168\uff0c\u8fd9\u7bc7\u662fAVOD(\u6682\u672a\u6709\u7b80\u4ecb)\u4ee5\u53ca Syn-Multi-view \u7684\u524d\u7f6e. \u5e76\u4e0d\u662f\u6700\u597d\u7684\u7b97\u6cd5\uff0c\u4e0d\u8fc7\u662f\u4e00\u4e2a\u4ee3\u7801\u4f7f\u7528\u96be\u5ea6\u4e0d\u9ad8(\u57fa\u672c\u53ea\u6709\u57fa\u672c\u7684numpy \u4e0e opencv\u8fd0\u7b97)\uff0c\u901f\u5ea6\u5feb(90HZ)\u4e14\u7cbe\u786e\u5ea6\u5c1a\u53ef\u7684\u7b97\u6cd5.\u66f4\u5173\u952e\u662f\u5b8c\u5168\u6ca1\u6709\u4f7f\u7528\u56fe\u50cf\u6570\u636e","title":"In Defense of Classical Image Processing: Fast Depth Completion on the CPU"},{"location":"other_categories/others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/#_1","text":"\u6b64\u56fe\u603b\u89c8\u4e86\u6574\u4e2a\u7b97\u6cd5\u7684\u5b8c\u6574\u6d41\u7a0b\uff0c\u4e0b\u9762\u4ece\u95ee\u9898\u7684\u6570\u5b66\u5b9a\u4e49\u4ee5\u53ca\u7b97\u6cd5\u6b65\u9aa4\u8fdb\u884c\u4ecb\u7ecd\u3002","title":"\u603b\u4f53\u6d41\u7a0b"},{"location":"other_categories/others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/#_2","text":"\u5bf9\u4e8e\u4e00\u5f20\u56fe\u7247 I ,\u4e00\u4e2a\u7a00\u758f\u7684\u6df1\u5ea6\u56fe D_{sparse} ,\u627e\u5230\u4e00\u4e2a \\hat f , \u5176\u4e2d f(I,D_{sparse}) = D_{dense} ,\u95ee\u9898\u53ef\u4ee5\u63cf\u8ff0\u4e3a: \\min .\\left\\|\\hat{f}\\left(I, D_{s p a r s e}\\right)-f\\left(I, D_{s p a r s e}\\right)\\right\\|_{F}^{2}=0","title":"\u95ee\u9898\u6570\u5b66\u5b9a\u4e49"},{"location":"other_categories/others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/#_3","text":"","title":"\u6d41\u7a0b\u4ecb\u7ecd"},{"location":"other_categories/others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/#depth-inversion","text":"\u4ece\u6570\u636e\u7279\u6027\u4e0a\uff0ckitti\u4e2d\u7684\u70b9\u7684\u8ddd\u79bb\u4ece0\u523080m\uff0c\u4e0d\u8fc7\uff0c\u7a7a\u50cf\u7d20\u540c\u6837\u4e3a0\uff0c\u8fd9\u4e0d\u5229\u4e8e\u57fa\u7840OpenCV\u7684\u8ba1\u7b97\u3002\u56e0\u6b64\u8fdb\u884c\u4e00\u4e2a\u64cd\u4f5c\uff0c\u5bf9\u975e\u7a7a\u50cf\u7d20\u8ba1\u7b97 D_{inverted} = 100.0 - D_{input} \uff0c\u4f1a\u5f62\u621020m\u7684\u7f13\u51b2\u533a\u3002\u65b9\u4fbf\u5f62\u6210\u4e00\u4e2avalid or not \u7684mask.","title":"Depth Inversion"},{"location":"other_categories/others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/#dilation","text":"\u9996\u5148\u5c1d\u8bd5\u8ba9\u7a7a\u50cf\u7d20\u88ab\u9644\u8fd1\u7684\u6709\u6548\u50cf\u7d20\u8986\u76d6(dilation),\u6700\u7ec8\u91c7\u7528\u7684\u662fdiamand kernel","title":"\u81ea\u5b9a\u4e49Dilation"},{"location":"other_categories/others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/#_4","text":"\u5148\u8ba1\u7b97\u4e00\u4e2anon-empty mask,\u4f7f\u7528 7\\times 7 full \u6838\u8fdb\u884cdilation,non-empth\u7684\u70b9\u4e0d\u6539\u53d8\u3002","title":"\u5c0f\u6d1e\u586b\u8865"},{"location":"other_categories/others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/#extension","text":"\u4e3a\u4e86\u8865\u5145\u6bd4\u8f83\u9ad8\u7684\u7269\u4f53\uff0c\u6bd4\u5982\u6811\u548c\u5929\u7a7a\uff0c\u5c06\u6bcf\u5217\u6700\u9ad8\u7684\u6709\u6548\u70b9\u76f4\u63a5\u8fde\u63a5\u5230\u6700\u5916\u90e8\u3002","title":"Extension"},{"location":"other_categories/others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/#_5","text":"\u6700\u540e\u4e00\u6b65\u586b\u8865\u524d\u6587\u5b8c\u5168\u6ca1\u6709\u8865\u5145\u7684\u70b9,\u7531\u4e8e\u6ca1\u6709\u56fe\u7247\u6570\u636e\uff0c\u4f7f\u7528\u4e00\u4e2a 31 \\times 31 \u7684\u6838\uff0c\u5bfb\u627e\u6700\u9760\u8fd1\u7684\u672a\u586b\u8865\u70b9\u3002","title":"\u5927\u6d1e\u8865\u5145"},{"location":"other_categories/others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/#blur","text":"\u8fde\u7eed\u4f7f\u7528\u4e00\u4e2a 5\\times 5 \u4e2d\u4f4d\u6570\u6a21\u7cca,\u4ee5\u53ca\u4e00\u4e2a 5\\times 5 \u9ad8\u65af\u6a21\u7cca\u3002","title":"Blur"},{"location":"other_categories/others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/#depth-inversion_1","text":"\u5c06\u7b2c\u4e00\u6b65\u7684\u6df1\u5ea6\u8f6c\u56de\u6765\u3002","title":"Depth Inversion"},{"location":"other_categories/others/Jointly_derain_dehaze/","text":"A Convolutional Network for Joint Deraining and Dehazing from A Single Image for Autonomous Driving in Rain \u8fd9\u7bc7\u8bba\u6587\u662firos2019\u7684\u8bba\u6587\uff0c\u5728\u5b9e\u9a8c\u5ba4\u7684NAS\u4e0a\u6709PDF\u7248\u672c\u3002\u8fd9\u7bc7\u8bba\u6587\u5728\u4eba\u5de5\u5408\u6210\u7684city-scape\u6570\u636e\u96c6\u4e0a\u5bf9\u5355\u4e2a\u56fe\u7247\u8fdb\u884cderain\u4e0edehaze\u3002\u5c06\u8fd9\u4e24\u4e2a\u95ee\u9898\u4e00\u8d77\u7814\u7a76\u7684motivation\u662f\u96e8\u6ef4\u672c\u8eab\u4f1a\u4f7f\u5f97\u573a\u666f\u4e2d\u5e26\u6709\u5149\u7684\u6563\u5c04\uff0c\u5f62\u6210\u96fe\u7b49\u626d\u66f2\u7684\u5149\u6548\u679c\uff0c\u6240\u4ee5\u5728\u96e8\u5929\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e2d\u5e94\u8be5\u5bf9\u4e24\u4e2a\u90e8\u5206\u540c\u65f6\u8fdb\u884c\u77eb\u6b63\u3002\u672c\u6587\u63d0\u51fa\u7684\u6a21\u578b\u901f\u5ea6\u5f88\u5feb\uff0c\u5bf9 1024\\times 512 \u7684\u56fe\u7247\u5904\u7406\u65f6\u95f4\u4ec5\u4ec5\u53ea\u67090.05s. \u6570\u5b66\u6a21\u578b \u7ecf\u5178\u7684\u6a21\u578b\u4e2d\u96e8\u5929\u56fe\u50cf\u7684\u5f62\u6210\u53ef\u4ee5\u7528\u4e0b\u5f0f\u7684\u6a21\u578b\u63cf\u8ff0\uff1a I(x) = (J(x) + R(x))t(x) + A(1 - t(x)) \u5176\u4e2d I(x) \u4e3a\u4e0b\u96e8\u56fe\u7247, J(x) \u4e3a\u76ee\u6807\u8fd8\u539f\u56fe\u7247, R(x) \u4e3a\u76f8\u673a\u9644\u8fd1\u7684\u96e8\u6ef4, t(x) = e^{-\\beta d(x)} \u4e3a\u4e0e\u56fe\u7247\u4e2d\u7269\u4f53\u4e0e\u76f8\u673a\u8ddd\u79bb d(x) \u6709\u5173\u7684\u4e0d\u88ab\u5e72\u6270\u7684\u5149\u7684\u6bd4\u4f8b,A\u662f\u8868\u8fbe\u96fe\u7684\u6a21\u7cca\u4eae\u5ea6\u7684\u4e00\u4e2a\u5168\u5c40\u503c\uff0c\u5f88\u591a\u6a21\u578b\u5c1d\u8bd5\u5bf9 R(x), t(x) \u5efa\u6a21\uff0c\u628a A \u8bbe\u7f6e\u4e3a\u4e00\u4e2a\u56fa\u5b9a\u503c\uff0c\u4f46\u662f\u6211\u4eec\u4f1a\u53d1\u73b0 A \u4e0e t(x) \u8fd8\u662f\u6709\u76f8\u5173\u6027\u7684\u3002\u8fd9\u91cc\u5bf9\u539f\u6765\u7684\u516c\u5f0f\u8fdb\u884c\u91cd\u65b0\u8868\u8fbe\u3002 \\begin{aligned} J(x) &= \\frac{1}{t(x)} I(x) - A\\frac{1}{t(x)} + A - R(x) \\\\ J(x) &= (K_1(x) - K_2(x)) I(x) - (K_1(x) - K_2(x)) \\end{aligned} \u5176\u4e2d K_1(x) = \\frac{\\frac{1}{t(x)}(I(x)-A) + A}{I(x) - 1}, K_2{x} = \\frac{R(x)}{I(x)-1} \u5982\u6b64\u6211\u4eec\u5bf9\u6bcf\u4e00\u4e2a\u50cf\u7d20\u70b9\u76f4\u63a5\u4f30\u8ba1 K_1(x), K_2(x) \u672c\u8d28\u4e0a\u5c31\u662f\u5b8c\u6210\u4e86\u4e00\u5f0f\u7684\uff0c\u8fd9\u91cc\u7684\u7ecf\u9a8c\u662f\u5bf9 K_2(x) \u5b66\u4e60\u7684\u65f6\u5019\u76f4\u63a5\u5b66\u4e60 R(x) \u7f51\u7edc\u6a21\u578b \u6570\u636e\u7684\u4ea7\u751f \u4e8b\u5b9e\u4e0a\u6211\u4eec\u5f88\u96be\u5f97\u5230\u4e0b\u96e8\u3001\u4e0d\u4e0b\u96e8\u7684\u6570\u636e\u5bf9(\u51e0\u4e4e\u4e0d\u53ef\u80fd\uff0c\u53ea\u80fd\u4f9d\u9760\u5b9a\u70b9\u62cd\u6444\uff0c\u8fd9\u79cd\u91c7\u96c6\u65b9\u5f0f\u4f7f\u5f97\u573a\u666f\u6bd4\u8f83\u5c11)\uff0c\u8fd9\u91cc\u6700\u540e\u9009\u62e9\u7684\u662f\u4f7f\u7528\u5408\u6210\u6570\u636e\u96c6\u3002\u9996\u5148\u6709CityScapes\u6570\u636e\u96c6\uff0c\u7136\u540e\u6709foggy-Cityscapes\u5408\u6210\u6570\u636e\u96c6(\u8fd9\u4e2a\u6570\u636e\u96c6\u5176\u5b9e\u5f88\u6709\u610f\u601d\uff0c\u5176\u751f\u6210\u65b9\u5f0f\u53ef\u4ee5\u4e00\u8bfb)\uff0c\u8fd9\u4e2a\u6570\u636e\u96c6\u57fa\u4e8eCityScapes\u5e76\u52a0\u4e0a\u4e86\u96fe\u5316\u7684\u6548\u679c\u3002\u7136\u540e\u4f5c\u8005\u4ee5\u6b64\u4e3a\u57fa\u7840\uff0c\u5bf9\u6bcf\u4e00\u5f20\u56fe\u6839\u636e\u4e00\u4e2a \u6559\u7a0b \u52a0\u4e0a\u4e86\u4e0d\u540c\u65b9\u5411\u4e0d\u540c\u5f3a\u5ea6\u7684\u96e8\u6c34\u6548\u679c\uff0c\u5f62\u6210\u65b0\u7684\u6570\u636e\u96c6\u3002 \u5b9e\u9a8c \u8fd9\u91cc\u8c08\u53ca\u5b9e\u9a8c\u7684\u539f\u56e0\uff0c\u4e3b\u8981\u662f\u56e0\u4e3a\u672c\u6587\u5728\u9a8c\u8bc1\u81ea\u5df1\u7684\u91cd\u5efa\u6027\u80fd\u7684\u65f6\u5019\uff0c\u9664\u4e86\u5728\u7f51\u7edc\u4e2d\u9a8c\u8bc1\u91cd\u5efa\u8bef\u5dee\uff0c\u8fd8\u8fdb\u884c\u4e86\u9ad8\u9636\u4efb\u52a1\u7684\u5b9e\u9a8c\uff0c\u4e5f\u5c31\u662f\u8ba9\u7269\u4f53\u68c0\u6d4b\u4ee5\u53ca\u8bed\u4e49\u5206\u5272\u9884\u8bad\u7ec3\u7f51\u7edc\u5728\u539f\u56fe\u3001\u5408\u6210\u56fe\u3001\u8fd8\u539f\u56fe\u4e0a\u5206\u522b\u8fd0\u884c\u6d4b\u8bd5\uff0c\u5b9e\u9a8c\u53d1\u73b0\u5728\u8fd8\u539f\u56fe\u4e0a\u5f97\u5230\u7684\u6027\u80fd\u70b9\u6570\u4e00\u822c\u90fd\u6bd4\u5408\u6210\u56fe\u4e0a\u7684\u9ad8\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\u672c\u6587\u5e76\u6ca1\u6709\u8fdb\u884cfine-tune\u3002","title":"A Convolutional Network for Joint Deraining and Dehazing from A Single Image for Autonomous Driving in Rain"},{"location":"other_categories/others/Jointly_derain_dehaze/#a-convolutional-network-for-joint-deraining-and-dehazing-from-a-single-image-for-autonomous-driving-in-rain","text":"\u8fd9\u7bc7\u8bba\u6587\u662firos2019\u7684\u8bba\u6587\uff0c\u5728\u5b9e\u9a8c\u5ba4\u7684NAS\u4e0a\u6709PDF\u7248\u672c\u3002\u8fd9\u7bc7\u8bba\u6587\u5728\u4eba\u5de5\u5408\u6210\u7684city-scape\u6570\u636e\u96c6\u4e0a\u5bf9\u5355\u4e2a\u56fe\u7247\u8fdb\u884cderain\u4e0edehaze\u3002\u5c06\u8fd9\u4e24\u4e2a\u95ee\u9898\u4e00\u8d77\u7814\u7a76\u7684motivation\u662f\u96e8\u6ef4\u672c\u8eab\u4f1a\u4f7f\u5f97\u573a\u666f\u4e2d\u5e26\u6709\u5149\u7684\u6563\u5c04\uff0c\u5f62\u6210\u96fe\u7b49\u626d\u66f2\u7684\u5149\u6548\u679c\uff0c\u6240\u4ee5\u5728\u96e8\u5929\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e2d\u5e94\u8be5\u5bf9\u4e24\u4e2a\u90e8\u5206\u540c\u65f6\u8fdb\u884c\u77eb\u6b63\u3002\u672c\u6587\u63d0\u51fa\u7684\u6a21\u578b\u901f\u5ea6\u5f88\u5feb\uff0c\u5bf9 1024\\times 512 \u7684\u56fe\u7247\u5904\u7406\u65f6\u95f4\u4ec5\u4ec5\u53ea\u67090.05s.","title":"A Convolutional Network for Joint Deraining and Dehazing from A Single Image for Autonomous Driving in Rain"},{"location":"other_categories/others/Jointly_derain_dehaze/#_1","text":"\u7ecf\u5178\u7684\u6a21\u578b\u4e2d\u96e8\u5929\u56fe\u50cf\u7684\u5f62\u6210\u53ef\u4ee5\u7528\u4e0b\u5f0f\u7684\u6a21\u578b\u63cf\u8ff0\uff1a I(x) = (J(x) + R(x))t(x) + A(1 - t(x)) \u5176\u4e2d I(x) \u4e3a\u4e0b\u96e8\u56fe\u7247, J(x) \u4e3a\u76ee\u6807\u8fd8\u539f\u56fe\u7247, R(x) \u4e3a\u76f8\u673a\u9644\u8fd1\u7684\u96e8\u6ef4, t(x) = e^{-\\beta d(x)} \u4e3a\u4e0e\u56fe\u7247\u4e2d\u7269\u4f53\u4e0e\u76f8\u673a\u8ddd\u79bb d(x) \u6709\u5173\u7684\u4e0d\u88ab\u5e72\u6270\u7684\u5149\u7684\u6bd4\u4f8b,A\u662f\u8868\u8fbe\u96fe\u7684\u6a21\u7cca\u4eae\u5ea6\u7684\u4e00\u4e2a\u5168\u5c40\u503c\uff0c\u5f88\u591a\u6a21\u578b\u5c1d\u8bd5\u5bf9 R(x), t(x) \u5efa\u6a21\uff0c\u628a A \u8bbe\u7f6e\u4e3a\u4e00\u4e2a\u56fa\u5b9a\u503c\uff0c\u4f46\u662f\u6211\u4eec\u4f1a\u53d1\u73b0 A \u4e0e t(x) \u8fd8\u662f\u6709\u76f8\u5173\u6027\u7684\u3002\u8fd9\u91cc\u5bf9\u539f\u6765\u7684\u516c\u5f0f\u8fdb\u884c\u91cd\u65b0\u8868\u8fbe\u3002 \\begin{aligned} J(x) &= \\frac{1}{t(x)} I(x) - A\\frac{1}{t(x)} + A - R(x) \\\\ J(x) &= (K_1(x) - K_2(x)) I(x) - (K_1(x) - K_2(x)) \\end{aligned} \u5176\u4e2d K_1(x) = \\frac{\\frac{1}{t(x)}(I(x)-A) + A}{I(x) - 1}, K_2{x} = \\frac{R(x)}{I(x)-1} \u5982\u6b64\u6211\u4eec\u5bf9\u6bcf\u4e00\u4e2a\u50cf\u7d20\u70b9\u76f4\u63a5\u4f30\u8ba1 K_1(x), K_2(x) \u672c\u8d28\u4e0a\u5c31\u662f\u5b8c\u6210\u4e86\u4e00\u5f0f\u7684\uff0c\u8fd9\u91cc\u7684\u7ecf\u9a8c\u662f\u5bf9 K_2(x) \u5b66\u4e60\u7684\u65f6\u5019\u76f4\u63a5\u5b66\u4e60 R(x)","title":"\u6570\u5b66\u6a21\u578b"},{"location":"other_categories/others/Jointly_derain_dehaze/#_2","text":"","title":"\u7f51\u7edc\u6a21\u578b"},{"location":"other_categories/others/Jointly_derain_dehaze/#_3","text":"\u4e8b\u5b9e\u4e0a\u6211\u4eec\u5f88\u96be\u5f97\u5230\u4e0b\u96e8\u3001\u4e0d\u4e0b\u96e8\u7684\u6570\u636e\u5bf9(\u51e0\u4e4e\u4e0d\u53ef\u80fd\uff0c\u53ea\u80fd\u4f9d\u9760\u5b9a\u70b9\u62cd\u6444\uff0c\u8fd9\u79cd\u91c7\u96c6\u65b9\u5f0f\u4f7f\u5f97\u573a\u666f\u6bd4\u8f83\u5c11)\uff0c\u8fd9\u91cc\u6700\u540e\u9009\u62e9\u7684\u662f\u4f7f\u7528\u5408\u6210\u6570\u636e\u96c6\u3002\u9996\u5148\u6709CityScapes\u6570\u636e\u96c6\uff0c\u7136\u540e\u6709foggy-Cityscapes\u5408\u6210\u6570\u636e\u96c6(\u8fd9\u4e2a\u6570\u636e\u96c6\u5176\u5b9e\u5f88\u6709\u610f\u601d\uff0c\u5176\u751f\u6210\u65b9\u5f0f\u53ef\u4ee5\u4e00\u8bfb)\uff0c\u8fd9\u4e2a\u6570\u636e\u96c6\u57fa\u4e8eCityScapes\u5e76\u52a0\u4e0a\u4e86\u96fe\u5316\u7684\u6548\u679c\u3002\u7136\u540e\u4f5c\u8005\u4ee5\u6b64\u4e3a\u57fa\u7840\uff0c\u5bf9\u6bcf\u4e00\u5f20\u56fe\u6839\u636e\u4e00\u4e2a \u6559\u7a0b \u52a0\u4e0a\u4e86\u4e0d\u540c\u65b9\u5411\u4e0d\u540c\u5f3a\u5ea6\u7684\u96e8\u6c34\u6548\u679c\uff0c\u5f62\u6210\u65b0\u7684\u6570\u636e\u96c6\u3002","title":"\u6570\u636e\u7684\u4ea7\u751f"},{"location":"other_categories/others/Jointly_derain_dehaze/#_4","text":"\u8fd9\u91cc\u8c08\u53ca\u5b9e\u9a8c\u7684\u539f\u56e0\uff0c\u4e3b\u8981\u662f\u56e0\u4e3a\u672c\u6587\u5728\u9a8c\u8bc1\u81ea\u5df1\u7684\u91cd\u5efa\u6027\u80fd\u7684\u65f6\u5019\uff0c\u9664\u4e86\u5728\u7f51\u7edc\u4e2d\u9a8c\u8bc1\u91cd\u5efa\u8bef\u5dee\uff0c\u8fd8\u8fdb\u884c\u4e86\u9ad8\u9636\u4efb\u52a1\u7684\u5b9e\u9a8c\uff0c\u4e5f\u5c31\u662f\u8ba9\u7269\u4f53\u68c0\u6d4b\u4ee5\u53ca\u8bed\u4e49\u5206\u5272\u9884\u8bad\u7ec3\u7f51\u7edc\u5728\u539f\u56fe\u3001\u5408\u6210\u56fe\u3001\u8fd8\u539f\u56fe\u4e0a\u5206\u522b\u8fd0\u884c\u6d4b\u8bd5\uff0c\u5b9e\u9a8c\u53d1\u73b0\u5728\u8fd8\u539f\u56fe\u4e0a\u5f97\u5230\u7684\u6027\u80fd\u70b9\u6570\u4e00\u822c\u90fd\u6bd4\u5408\u6210\u56fe\u4e0a\u7684\u9ad8\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\u672c\u6587\u5e76\u6ca1\u6709\u8fdb\u884cfine-tune\u3002","title":"\u5b9e\u9a8c"},{"location":"other_categories/others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/","text":"OptNet: Differentiable Optimization as a Layer in Neural Networks \u8fd9\u7bc7\u8bba\u6587\u5b9a\u4e49\u4e86\u4e00\u4e2a\u57fa\u4e8e\u7ebf\u6027\u4e8c\u6b21\u4f18\u5316\u7684\u65b0\u7684\u795e\u7ecf\u7f51\u7edc\u5c42 \u8fd9\u4e2a\u5c42\u5bf9\u5e94\u7684\u4f18\u5316\u95ee\u9898\u5982\u4e0b z_{i+1} = argmin(\\frac{1}{2} z^T Q(z_i) z + q(z_i) z) \u7b26\u5408\u7ea6\u675f A(z_i)z = b(z_i) \u4e0e G(z_i)z <= h(z_i) \u5176\u4e2d Q,q,A,b,G,h \u90fd\u662f\u53ef\u4ee5\u8ddf\u968f\u8f93\u5165 z \u53d8\u5316\u7684\u503c\uff0c\u8bba\u6587\u63d0\u51fa\u4e86\u6c42\u89e3\u8fd9\u4e2a\u4f18\u5316\u95ee\u9898\u4ee5\u53ca\u6c42\u51fa\u8fd9\u4e2a\u4f18\u5316\u95ee\u9898\u5bf9\u5e94\u7684\u68af\u5ea6\u7684\u5feb\u901f\u65b9\u5f0f\u3002 \u6838\u5fc3\u601d\u8def\u4e0e\u516c\u5f0f \u6838\u5fc3\u601d\u8def\u662f\u5728\u53cd\u4f20\u7684\u65f6\u5019\u4f7f\u7528KKT condition\uff0c\u4e5f\u5c31\u662f\u5047\u8bbe\u4f18\u5316\u95ee\u9898\u5df2\u7ecf\u5f97\u5230\u4e86\u6700\u4f18\u89e3\uff0c\u5bf9KKT condition\u8fdb\u884c\u6c42\u5fae\u5206\u5f97\u5230\u5404\u4e2a\u5143\u7d20\u4e4b\u95f4\u7684\u5fae\u5206\u5173\u7cfb\uff08\u8bba\u6587\u516c\u5f0f6-8\uff09 \u53e6\u5916\u9700\u8981\u5728GPU\u4e0a\u5b9e\u73b0\u5b8c\u6574\u7684\u4f18\u5316\u7b97\u6cd5\uff0c\u4f5c\u8005\u4f7f\u7528\u4e86\u516c\u5f0f(9-10)\uff08\u66f4\u591a\u7684\u516c\u5f0f\u5efa\u8bae\u67e5\u770bgithub\uff0cqpth\u5e93\uff09","title":"OptNet: Differentiable Optimization as a Layer in Neural Networks"},{"location":"other_categories/others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/#optnet-differentiable-optimization-as-a-layer-in-neural-networks","text":"\u8fd9\u7bc7\u8bba\u6587\u5b9a\u4e49\u4e86\u4e00\u4e2a\u57fa\u4e8e\u7ebf\u6027\u4e8c\u6b21\u4f18\u5316\u7684\u65b0\u7684\u795e\u7ecf\u7f51\u7edc\u5c42 \u8fd9\u4e2a\u5c42\u5bf9\u5e94\u7684\u4f18\u5316\u95ee\u9898\u5982\u4e0b z_{i+1} = argmin(\\frac{1}{2} z^T Q(z_i) z + q(z_i) z) \u7b26\u5408\u7ea6\u675f A(z_i)z = b(z_i) \u4e0e G(z_i)z <= h(z_i) \u5176\u4e2d Q,q,A,b,G,h \u90fd\u662f\u53ef\u4ee5\u8ddf\u968f\u8f93\u5165 z \u53d8\u5316\u7684\u503c\uff0c\u8bba\u6587\u63d0\u51fa\u4e86\u6c42\u89e3\u8fd9\u4e2a\u4f18\u5316\u95ee\u9898\u4ee5\u53ca\u6c42\u51fa\u8fd9\u4e2a\u4f18\u5316\u95ee\u9898\u5bf9\u5e94\u7684\u68af\u5ea6\u7684\u5feb\u901f\u65b9\u5f0f\u3002","title":"OptNet: Differentiable Optimization as a Layer in Neural Networks"},{"location":"other_categories/others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/#_1","text":"\u6838\u5fc3\u601d\u8def\u662f\u5728\u53cd\u4f20\u7684\u65f6\u5019\u4f7f\u7528KKT condition\uff0c\u4e5f\u5c31\u662f\u5047\u8bbe\u4f18\u5316\u95ee\u9898\u5df2\u7ecf\u5f97\u5230\u4e86\u6700\u4f18\u89e3\uff0c\u5bf9KKT condition\u8fdb\u884c\u6c42\u5fae\u5206\u5f97\u5230\u5404\u4e2a\u5143\u7d20\u4e4b\u95f4\u7684\u5fae\u5206\u5173\u7cfb\uff08\u8bba\u6587\u516c\u5f0f6-8\uff09 \u53e6\u5916\u9700\u8981\u5728GPU\u4e0a\u5b9e\u73b0\u5b8c\u6574\u7684\u4f18\u5316\u7b97\u6cd5\uff0c\u4f5c\u8005\u4f7f\u7528\u4e86\u516c\u5f0f(9-10)\uff08\u66f4\u591a\u7684\u516c\u5f0f\u5efa\u8bae\u67e5\u770bgithub\uff0cqpth\u5e93\uff09","title":"\u6838\u5fc3\u601d\u8def\u4e0e\u516c\u5f0f"},{"location":"other_categories/others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/","text":"Photorealistic Image Reconstruction From Hybrid Intensity and Event Based Sensor \u8fd9\u7bc7\u8bba\u6587\u89e3\u51b3\u7684\u4efb\u52a1\u5982\u4e0b\uff0c\u8f93\u5165\u8fde\u7eed\u7684\u666e\u901a\u56fe\u7247\u4ee5\u53ca\u4e00\u7cfb\u5217\u5bc6\u96c6\u7684event camera\u4fe1\u606f\uff0c\u8fd8\u539f\u51fa\u66f4\u52a0\u771f\u5b9e\u7684\u539f\u59cb\u56fe\u7247,\u5173\u4e8eevent camera\u7684\u57fa\u7840\u539f\u7406\u4ee5\u53ca\u5bf9\u56fe\u7247\u7684\u589e\u5f3a\uff0c\u8fd9\u91cc\u53ef\u53c2\u8003 \u8fd9\u7bc7\u8bba\u6587 \u5de5\u4f5c\u6d41\u7a0b \u8fd9\u91cc\u5206\u4e3a\u56db\u4e2a\u6b65\u9aa4\uff0c\u7b2c\u4e00\u6b65\u662f\u901a\u8fc7\u4e24\u5f20\u57fa\u7840\u56fe\u7247\u5f97\u5230\u6df1\u5ea6\u4f30\u8ba1\u3002\u7b2c\u4e8c\u6b65\u662f\u4f7f\u7528event frames\u5b9e\u73b0\u5bf9\u4e2d\u95f4\u5e27\u7684\u63d2\u503c\uff0c\u7b2c\u4e09\u6b65\u662f\u5bf9\u63d2\u503c\u7ed3\u679c\u4f7f\u7528VO\u5f97\u5230\u4e00\u4e2a\u4f4d\u59ff\u4f30\u8ba1\uff0c\u7b2c\u56db\u6b65\u662f\u6839\u636e\u6df1\u5ea6\u4e0e\u59ff\u6001\u5f97\u5230\u8f6c\u6362\u540e\u7684\u5f3a\u5ea6\u56fe\u3002 \u6df1\u5ea6\u4f30\u8ba1 \u901a\u8fc7\u5149\u6d41\u521d\u59cb\u5316\u6df1\u5ea6\u4f30\u8ba1 d_k, d_{k+1} \uff0c\u5c06\u76f8\u5bf9\u4f4d\u79fb\u4e0e\u65cb\u8f6c\u521d\u59cb\u5316\u4e3a0,\u6839\u636e\u6df1\u5ea6\u4ee5\u53ca\u4f4d\u79fb\uff0c\u53ef\u4ee5\u8ba9\u4e24\u5f20\u56fe\u76f8\u4e92\u8f6c\u6362\uff0c\u8fd9\u91cc\u5b9a\u4e49\u91cd\u6784\u635f\u5931 \\mathcal{L}_{p h}\\left(d_{k}, d_{k+1}, \\xi\\right)=\\left\\|\\left(\\hat{I}_{k}-I_{k}\\right)\\right\\|_{1}+\\left\\|\\left(\\hat{I}_{k+1}-I_{k+1}\\right)\\right\\|_{1} \u8fdb\u4e00\u6b65\u5b9a\u4e49\u4e00\u4e2a\u4e0eedge\u6709\u5173\u7684\u635f\u5931 \\mathcal{L}_{s m}(d)=\\sum\\left\\|\\nabla_{x} d\\right\\| e^{-\\beta\\left\\|\\nabla_{x} I\\right\\|}+\\left\\|\\nabla_{y} d\\right\\| e^{-\\beta\\left\\|\\nabla_{y} I\\right\\|} \u5176\u4e2d d \u4e3a\u6df1\u5ea6\u56fe \\nabla_x, \\nabla_y \u6307\u7684\u662fx,y\u65b9\u5411\u7684\u8fd0\u7b97\u7b26, \u4f18\u5316\u4ee5\u4e0a\u4e24\u4e2a\u635f\u5931\u51fd\u6570\u7684\u52a0\u6743\u6c42\u548c\u662f\u4e00\u4e2a\u975e\u51f8\u95ee\u9898\uff0c\u6211\u4eec\u9700\u8981\u4e00\u4e2a\u597d\u7684\u5c40\u90e8\u89e3\u4f5c\u4e3a\u521d\u59cb\u89e3\u3002\u8fd9\u91cc\u4f7f\u7528 PWC-Net \u751f\u6210\u521d\u59cb\u5149\u6d41\uff0c\u8fd9\u91cc\u4f7f\u7528\u5149\u6d41\u7684inverse\u76f4\u63a5\u5f97\u5230\u6df1\u5ea6\u521d\u59cb\u503c\u3002 \u76f8\u5bf9\u4f4d\u79fb\u4f30\u8ba1 \u8fd9\u91cc\u4f7f\u7528event camera\u5bf9\u4e24\u5e27\u4e4b\u95f4\u7684\u56fe\u50cf\u8fdb\u884c\u63d2\u503c\uff0c\u751f\u6210\u4e00\u7cfb\u5217\u7684\u4e2d\u95f4\u56fe\u50cf\u3002 \\begin{aligned} \\mathcal{L}_{p}\\left(\\xi_{k}^{j}\\right) &=\\left\\|E_{k}^{0}-\\hat{E}_{k}^{0}\\right\\|_{1} \\\\ \\mathcal{L}_{p}\\left(\\xi_{k+1}^{j}\\right) &=\\left\\|E_{k+1}^{0}-\\hat{E}_{k+1}^{0}\\right\\|_{1} \\end{aligned} \\xi_{k}^{j}, \\xi_{k+1}^{j}=\\underset{\\xi_{k}^{j}, \\xi_{k+1}^{j}}{\\arg \\min } \\mathcal{L}_{p}\\left(\\xi_{k}^{j}\\right)+\\mathcal{L}_{p}\\left(\\xi_{k+1}^{j}\\right)+\\lambda_{r} \\mathcal{L}_{p}\\left(\\xi_{k}^{j}, \\xi_{k+1}^{j}\\right) \u5176\u4e2d \\xi^j_k,\\xi^j_{k+1} \u5206\u522b\u6307\u4ee3\u7b2c j \u5f20\u4e2d\u95f4\u56fe\u76f8\u5bf9\u4e8e\u7b2c\u4e00\u5e27\u3001\u4e0b\u4e00\u5e27\u4e4b\u95f4\u7684\u76f8\u5bf9\u4f4d\u59ff\u53d8\u5316\uff0c \u878d\u5408 \u6839\u636e\u6df1\u5ea6\u3001\u591a\u4e2a\u4e2d\u95f4\u503c\u7684\u4f4d\u59ff\uff0c\u5c06\u7b2c\u4e00\u5e27\u4e0e\u7b2c\u4e8c\u5e27\u5206\u522b\u5411\u540e\u3001\u5411\u524dwarp\uff0calpha-blend\u4e24\u6b21\u7684\u7ed3\u679c\uff0c\u5f97\u5230\u4e00\u7cfb\u5217\u66f4\u7cbe\u786e\u7684\u9996\u5c3e\u4e0e\u4e2d\u95f4\u56fe\u3002","title":"Photorealistic Image Reconstruction From Hybrid Intensity and Event Based Sensor"},{"location":"other_categories/others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/#photorealistic-image-reconstruction-from-hybrid-intensity-and-event-based-sensor","text":"\u8fd9\u7bc7\u8bba\u6587\u89e3\u51b3\u7684\u4efb\u52a1\u5982\u4e0b\uff0c\u8f93\u5165\u8fde\u7eed\u7684\u666e\u901a\u56fe\u7247\u4ee5\u53ca\u4e00\u7cfb\u5217\u5bc6\u96c6\u7684event camera\u4fe1\u606f\uff0c\u8fd8\u539f\u51fa\u66f4\u52a0\u771f\u5b9e\u7684\u539f\u59cb\u56fe\u7247,\u5173\u4e8eevent camera\u7684\u57fa\u7840\u539f\u7406\u4ee5\u53ca\u5bf9\u56fe\u7247\u7684\u589e\u5f3a\uff0c\u8fd9\u91cc\u53ef\u53c2\u8003 \u8fd9\u7bc7\u8bba\u6587","title":"Photorealistic Image Reconstruction From Hybrid Intensity and Event Based Sensor"},{"location":"other_categories/others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/#_1","text":"\u8fd9\u91cc\u5206\u4e3a\u56db\u4e2a\u6b65\u9aa4\uff0c\u7b2c\u4e00\u6b65\u662f\u901a\u8fc7\u4e24\u5f20\u57fa\u7840\u56fe\u7247\u5f97\u5230\u6df1\u5ea6\u4f30\u8ba1\u3002\u7b2c\u4e8c\u6b65\u662f\u4f7f\u7528event frames\u5b9e\u73b0\u5bf9\u4e2d\u95f4\u5e27\u7684\u63d2\u503c\uff0c\u7b2c\u4e09\u6b65\u662f\u5bf9\u63d2\u503c\u7ed3\u679c\u4f7f\u7528VO\u5f97\u5230\u4e00\u4e2a\u4f4d\u59ff\u4f30\u8ba1\uff0c\u7b2c\u56db\u6b65\u662f\u6839\u636e\u6df1\u5ea6\u4e0e\u59ff\u6001\u5f97\u5230\u8f6c\u6362\u540e\u7684\u5f3a\u5ea6\u56fe\u3002","title":"\u5de5\u4f5c\u6d41\u7a0b"},{"location":"other_categories/others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/#_2","text":"\u901a\u8fc7\u5149\u6d41\u521d\u59cb\u5316\u6df1\u5ea6\u4f30\u8ba1 d_k, d_{k+1} \uff0c\u5c06\u76f8\u5bf9\u4f4d\u79fb\u4e0e\u65cb\u8f6c\u521d\u59cb\u5316\u4e3a0,\u6839\u636e\u6df1\u5ea6\u4ee5\u53ca\u4f4d\u79fb\uff0c\u53ef\u4ee5\u8ba9\u4e24\u5f20\u56fe\u76f8\u4e92\u8f6c\u6362\uff0c\u8fd9\u91cc\u5b9a\u4e49\u91cd\u6784\u635f\u5931 \\mathcal{L}_{p h}\\left(d_{k}, d_{k+1}, \\xi\\right)=\\left\\|\\left(\\hat{I}_{k}-I_{k}\\right)\\right\\|_{1}+\\left\\|\\left(\\hat{I}_{k+1}-I_{k+1}\\right)\\right\\|_{1} \u8fdb\u4e00\u6b65\u5b9a\u4e49\u4e00\u4e2a\u4e0eedge\u6709\u5173\u7684\u635f\u5931 \\mathcal{L}_{s m}(d)=\\sum\\left\\|\\nabla_{x} d\\right\\| e^{-\\beta\\left\\|\\nabla_{x} I\\right\\|}+\\left\\|\\nabla_{y} d\\right\\| e^{-\\beta\\left\\|\\nabla_{y} I\\right\\|} \u5176\u4e2d d \u4e3a\u6df1\u5ea6\u56fe \\nabla_x, \\nabla_y \u6307\u7684\u662fx,y\u65b9\u5411\u7684\u8fd0\u7b97\u7b26, \u4f18\u5316\u4ee5\u4e0a\u4e24\u4e2a\u635f\u5931\u51fd\u6570\u7684\u52a0\u6743\u6c42\u548c\u662f\u4e00\u4e2a\u975e\u51f8\u95ee\u9898\uff0c\u6211\u4eec\u9700\u8981\u4e00\u4e2a\u597d\u7684\u5c40\u90e8\u89e3\u4f5c\u4e3a\u521d\u59cb\u89e3\u3002\u8fd9\u91cc\u4f7f\u7528 PWC-Net \u751f\u6210\u521d\u59cb\u5149\u6d41\uff0c\u8fd9\u91cc\u4f7f\u7528\u5149\u6d41\u7684inverse\u76f4\u63a5\u5f97\u5230\u6df1\u5ea6\u521d\u59cb\u503c\u3002","title":"\u6df1\u5ea6\u4f30\u8ba1"},{"location":"other_categories/others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/#_3","text":"\u8fd9\u91cc\u4f7f\u7528event camera\u5bf9\u4e24\u5e27\u4e4b\u95f4\u7684\u56fe\u50cf\u8fdb\u884c\u63d2\u503c\uff0c\u751f\u6210\u4e00\u7cfb\u5217\u7684\u4e2d\u95f4\u56fe\u50cf\u3002 \\begin{aligned} \\mathcal{L}_{p}\\left(\\xi_{k}^{j}\\right) &=\\left\\|E_{k}^{0}-\\hat{E}_{k}^{0}\\right\\|_{1} \\\\ \\mathcal{L}_{p}\\left(\\xi_{k+1}^{j}\\right) &=\\left\\|E_{k+1}^{0}-\\hat{E}_{k+1}^{0}\\right\\|_{1} \\end{aligned} \\xi_{k}^{j}, \\xi_{k+1}^{j}=\\underset{\\xi_{k}^{j}, \\xi_{k+1}^{j}}{\\arg \\min } \\mathcal{L}_{p}\\left(\\xi_{k}^{j}\\right)+\\mathcal{L}_{p}\\left(\\xi_{k+1}^{j}\\right)+\\lambda_{r} \\mathcal{L}_{p}\\left(\\xi_{k}^{j}, \\xi_{k+1}^{j}\\right) \u5176\u4e2d \\xi^j_k,\\xi^j_{k+1} \u5206\u522b\u6307\u4ee3\u7b2c j \u5f20\u4e2d\u95f4\u56fe\u76f8\u5bf9\u4e8e\u7b2c\u4e00\u5e27\u3001\u4e0b\u4e00\u5e27\u4e4b\u95f4\u7684\u76f8\u5bf9\u4f4d\u59ff\u53d8\u5316\uff0c","title":"\u76f8\u5bf9\u4f4d\u79fb\u4f30\u8ba1"},{"location":"other_categories/others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/#_4","text":"\u6839\u636e\u6df1\u5ea6\u3001\u591a\u4e2a\u4e2d\u95f4\u503c\u7684\u4f4d\u59ff\uff0c\u5c06\u7b2c\u4e00\u5e27\u4e0e\u7b2c\u4e8c\u5e27\u5206\u522b\u5411\u540e\u3001\u5411\u524dwarp\uff0calpha-blend\u4e24\u6b21\u7684\u7ed3\u679c\uff0c\u5f97\u5230\u4e00\u7cfb\u5217\u66f4\u7cbe\u786e\u7684\u9996\u5c3e\u4e0e\u4e2d\u95f4\u56fe\u3002","title":"\u878d\u5408"},{"location":"other_categories/others/PSMNet/","text":"Pyramid Stereo Matching Network PSM \u7f51\u7edc\u662f\u76ee\u524d\u975e\u5e38\u5e38\u7528\u7684\u4e00\u4e2a\u53cc\u76ee\u6df1\u5ea6\u5b66\u4e60\u7f51\u7edc\uff0c\u5728\u53cc\u76ee\u6df1\u5ea6\u4f30\u8ba1\uff0c\u53cc\u76ee\u68c0\u6d4b\uff0c\u53cc\u76ee\u5149\u6d41\u7b49\u4efb\u52a1\u4e2d\u90fd\u6709\u5e7f\u6cdb\u5e94\u7528 PSM\u603b\u4f53\u7ed3\u6784 \u56fe\u4e2dSPP\u6a21\u5757\uff0c\u672c\u8d28\u4e0a\u5c31\u662f\u4e00\u4e2a\u5927\u611f\u53d7\u91ce\u7684CNN\u6a21\u5757\uff0c\u8fd9\u4e2abackbone\u5728\u5f88\u591a\u7f51\u7edc\u4e2d\u90fd\u88ab\u91cd\u590d\u4f7f\u7528\u3002 \u56fe\u4e2d\u7684Cost volume \u6765\u81ea\u4e8e \u53e6\u4e00\u7bc7\u6587\u7ae0.pdf \uff0c \u4e0e PSV \u4e0d\u540c\u7684\u5730\u65b9\u5728\u4e8e\uff0c\u4e0e\u5176\u662f\u5728\u5b9e\u9645\u7684\u8ddd\u79bb\u5355\u4f4d\u4e0a\u5747\u5300\u53d6Z\u8f74\u518d\u8c03\u7528\u8f83\u4e3a\u8017\u8d39\u8d44\u6e90\u7684grid_sample\u51fd\u6570\uff0c\u8fd9\u91cc\u9009\u62e9\u7684\u662f\u5c06\u6df1\u5ea6\u8f74\u76f4\u63a5\u7406\u89e3\u4e3a\u79bb\u6563\u7684disparity\uff0c\u4f7f\u5f97grid_sample\u53ef\u4ee5\u7528\u8f7b\u4fbf\u7684indexing\u4ee3\u66ff\uff0c\u901f\u5ea6\u66f4\u5feb\uff0c\u6548\u679c\u5dee\u8ddd\u4e0d\u7b97\u5927,\u53ea\u4e0d\u8fc7\u662f\u6839\u636e\u56fe\u7247\u672c\u8eab\u7684\u5927\u5c0fdisparity\u503c\u7684\u5f71\u54cd\u4f1a\u76f4\u63a5\u5f71\u54cd\u5230\u6a21\u578b\u7684\u5927\u5c0f\u6216\u8005\u8bf4\u901f\u5ea6\u3002 \u5982\u679c\u4f7f\u7528 stacked-hourglass\u4f5c\u4e3a inference\u7684\u65f6\u5019\uff0c\u5c31\u4f1a\u5728\u540c\u4e00\u5927\u5c0f\u7684\u8f93\u51fa\u5c42\u4e0a\u591a\u6b21\u505a\u51faprediction\u3002 \u6838\u5fc3\u5728\u4e8e\u5b9e\u73b0\u4ee5\u53ca\u8bad\u7ec3\u7ec6\u8282\uff0c\u8fd9\u4e9b\u90fd\u5728\u4ee3\u7801\u4e2d\u6709\u4ea4\u4ee3\u3002Cost volume\u7684\u5b9e\u73b0\u5df2\u7ecf\u5728\u5c1d\u8bd5\u8fc1\u79fb\u4f7f\u7528\u3002","title":"Pyramid Stereo Matching Network"},{"location":"other_categories/others/PSMNet/#pyramid-stereo-matching-network","text":"PSM \u7f51\u7edc\u662f\u76ee\u524d\u975e\u5e38\u5e38\u7528\u7684\u4e00\u4e2a\u53cc\u76ee\u6df1\u5ea6\u5b66\u4e60\u7f51\u7edc\uff0c\u5728\u53cc\u76ee\u6df1\u5ea6\u4f30\u8ba1\uff0c\u53cc\u76ee\u68c0\u6d4b\uff0c\u53cc\u76ee\u5149\u6d41\u7b49\u4efb\u52a1\u4e2d\u90fd\u6709\u5e7f\u6cdb\u5e94\u7528","title":"Pyramid Stereo Matching Network"},{"location":"other_categories/others/PSMNet/#psm","text":"\u56fe\u4e2dSPP\u6a21\u5757\uff0c\u672c\u8d28\u4e0a\u5c31\u662f\u4e00\u4e2a\u5927\u611f\u53d7\u91ce\u7684CNN\u6a21\u5757\uff0c\u8fd9\u4e2abackbone\u5728\u5f88\u591a\u7f51\u7edc\u4e2d\u90fd\u88ab\u91cd\u590d\u4f7f\u7528\u3002 \u56fe\u4e2d\u7684Cost volume \u6765\u81ea\u4e8e \u53e6\u4e00\u7bc7\u6587\u7ae0.pdf \uff0c \u4e0e PSV \u4e0d\u540c\u7684\u5730\u65b9\u5728\u4e8e\uff0c\u4e0e\u5176\u662f\u5728\u5b9e\u9645\u7684\u8ddd\u79bb\u5355\u4f4d\u4e0a\u5747\u5300\u53d6Z\u8f74\u518d\u8c03\u7528\u8f83\u4e3a\u8017\u8d39\u8d44\u6e90\u7684grid_sample\u51fd\u6570\uff0c\u8fd9\u91cc\u9009\u62e9\u7684\u662f\u5c06\u6df1\u5ea6\u8f74\u76f4\u63a5\u7406\u89e3\u4e3a\u79bb\u6563\u7684disparity\uff0c\u4f7f\u5f97grid_sample\u53ef\u4ee5\u7528\u8f7b\u4fbf\u7684indexing\u4ee3\u66ff\uff0c\u901f\u5ea6\u66f4\u5feb\uff0c\u6548\u679c\u5dee\u8ddd\u4e0d\u7b97\u5927,\u53ea\u4e0d\u8fc7\u662f\u6839\u636e\u56fe\u7247\u672c\u8eab\u7684\u5927\u5c0fdisparity\u503c\u7684\u5f71\u54cd\u4f1a\u76f4\u63a5\u5f71\u54cd\u5230\u6a21\u578b\u7684\u5927\u5c0f\u6216\u8005\u8bf4\u901f\u5ea6\u3002 \u5982\u679c\u4f7f\u7528 stacked-hourglass\u4f5c\u4e3a inference\u7684\u65f6\u5019\uff0c\u5c31\u4f1a\u5728\u540c\u4e00\u5927\u5c0f\u7684\u8f93\u51fa\u5c42\u4e0a\u591a\u6b21\u505a\u51faprediction\u3002 \u6838\u5fc3\u5728\u4e8e\u5b9e\u73b0\u4ee5\u53ca\u8bad\u7ec3\u7ec6\u8282\uff0c\u8fd9\u4e9b\u90fd\u5728\u4ee3\u7801\u4e2d\u6709\u4ea4\u4ee3\u3002Cost volume\u7684\u5b9e\u73b0\u5df2\u7ecf\u5728\u5c1d\u8bd5\u8fc1\u79fb\u4f7f\u7528\u3002","title":"PSM\u603b\u4f53\u7ed3\u6784"},{"location":"other_categories/others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/","text":"PointFlow : 3D Point Cloud Generation with Continuous Normalizing Flows \u8fd9\u7bc7\u8bba\u6587\u6709\u4e00\u4e2a project page \uff0c\u6839\u636e \u89c6\u9891 \u5b8c\u6210\u7684\u4e00\u4e2a\u4efb\u52a1\u53ef\u4ee5\u8fd9\u6837\u63cf\u8ff0\u3002\u6bcf\u4e00\u4e2a\u7269\u4f53\u7684\u4e00\u79cd\u70b9\u4e91\u8868\u8fbe\u53ef\u4ee5\u7406\u89e3\u4e3a\u4ece\u4e00\u4e2a\u7531\u5f62\u72b6\u51b3\u5b9a\u7684\u6982\u7387\u5206\u5e03\u91c7\u6837\u70b9\u3002\u6211\u4eec\u53ef\u4ee5\u5229\u7528\u4e00\u4e2a\u5750\u6807\u8f6c\u6362\u51fd\u6570\uff0c\u5c06\u9ad8\u65af\u91c7\u6837\u70b9\u4e91\u8f6c\u6362\u4e3a\u6700\u540e\u7684\u8f93\u51fa\u3002\u672c\u6587\u7ed9\u51fa\u7684\u7f51\u7edc\u53ef\u4ee5\u6839\u636e\u7c7b\u522b\uff0c\u4ee5\u53ca\u4e00\u4e9b\u968f\u673a\u6570\uff0c\u91c7\u6837\u51fa\u4e0d\u540c\u5f62\u72b6\u7684\u4e00\u4e2a\u5750\u6807\u8f6c\u6362\u51fd\u6570\u3002 Background: Continuous normalizing flow \u5b9a\u4e49 f_1,...,f_n \u6307\u4ee3\u4e00\u7cfb\u5217\u7684\u53ef\u9006\u53d8\u6362\u3002\u8f93\u5165\u9690\u53d8\u91cf y \u7684\u6982\u7387\u5206\u5e03\u4e3a P(y) . x=f_{n} \\circ f_{n-1} \\circ \\cdots \\circ f_{1}(y) \u4f5c\u4e3a\u8f93\u51fa\u3002\u8f93\u51fa\u53d8\u91cf\u7684\u6982\u7387\u5bc6\u5ea6\u5219\u53d8\u4e3a \\log P(x)=\\log P(y)-\\sum_{k=1}^{n} \\log \\left|\\operatorname{det} \\frac{\\partial f_{k}}{\\partial y_{k-1}}\\right| \u8fd9\u6837 y \u53ef\u4ee5\u4ece x \u4f7f\u7528inverse flow\u8ba1\u7b97: y=f_{1}^{-1} \\circ \\cdots \\circ f_{n}^{-1}(x) . \u8fd9\u91cc f_1,...,f_n \u5728\u8fd9\u91cc\u5b9e\u4f53\u5316\u4f5c\u4e3a\u795e\u7ecf\u7f51\u7edc\uff0c\u8fd9\u6837\u5bfc\u6570\u7684\u884c\u5217\u5f0f\u8ba1\u7b97\u96be\u5ea6\u4e0d\u5927\u3002 continuous normalizing flow(CNF) for P(x) \u5c31\u662f x=y\\left(t_{0}\\right)+\\int_{t_{0}}^{t_{1}} f(y(t), t) d t, \\quad y\\left(t_{0}\\right) \\sim P(y) \\log P(x)=\\log P\\left(y\\left(t_{0}\\right)\\right)-\\int_{t_{0}}^{t_{1}} \\operatorname{Tr}\\left(\\frac{\\partial f}{\\partial y(t)}\\right) d t inverse flow y(t_0) = x + \\int^{t_0}_{t_1}f(y(t),t)dt .\u8fd9\u91cc\u7684 f \u4ecd\u7136\u662f\u4e00\u4e2a\u795e\u7ecf\u7f51\u7edc,\u7528ODE\u6c42\u89e3\u5668\u4f5c\u4e3a\u8f93\u51fa\u3002\u8fd9\u91cc\u7684\u5173\u952e\u601d\u7ef4\u8f6c\u53d8\u662f\u5229\u7528\u4e86\uff0c Neural Ordinary Equation \u7684\u601d\u8def\uff0c\u5c06\u8fde\u7eed\u53e0\u5c42\u7684\u5171\u4eab\u6743\u91cd\u7684\u795e\u7ecf\u7f51\u7edc\u7528\u5e38\u5fae\u5206\u65b9\u7a0b\u4ee3\u66ff.\u7528\u8fd9\u4e2aneural ODE\u7684\u4e00\u4e2a\u91cd\u70b9\u662f\u4f7f\u5f97\u9006\u53d8\u6362\u53ef\u4ee5\u540c\u6837\u901a\u8fc7ODE\u65f6\u95f4\u4e0a\u76f8\u53cd\u7684\u4e00\u4e2aforward pass\u5b9e\u73b0\u3002 Variational auto-encoder \u539f\u6587 .VAE\u4e2d\uff0c z \u662flatent space,\u5305\u542b\u4e00\u4e2a\u63cf\u8ff0 P_\\theta(X|z) \u4f5c\u4e3adecoder,\u8fd8\u5b66\u4e60\u4e00\u4e2aencoder Q_\\phi(z|X) ,\u5b83\u4eec\u5171\u540c\u8bad\u7ec3\u53bb\u6700\u5927\u5316\u89c2\u5bdf\u5668\u7684log-likelihood \\begin{aligned} \\log P_{\\theta}(X) & \\geq \\log P_{\\theta}(X)-D_{K L}\\left(Q_{\\phi}(z | X) \\| P_{\\theta}(z | X)\\right) \\\\ &=\\mathbb{E}_{Q_{\\phi}(z | x)}\\left[\\log P_{\\theta}(X | z)\\right]-D_{K L}\\left(Q_{\\phi}(z | X) \\| P_{\\psi}(z)\\right) \\\\ & \\triangleq \\mathcal{L}(X ; \\phi, \\psi, \\theta) \\end{aligned} z = \\mu_phi(X) + \\sigma_\\phi(X) \\epsilon .\u8fd9\u4e2a\u5747\u503c\u4e0e\u65b9\u5dee\u90fd\u662f\u7f51\u7edc\u76f4\u63a5\u7684\u8f93\u51fa\u3002 \u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5 \u9996\u5148\u5305\u542b\u4e09\u4e2a\u6a21\u5757\uff0c\u7b2c\u4e00\u4e2a\u662fencoder\uff0c Q_\\phi(z|X) ,\u5c06\u70b9\u4e91\u7f16\u7801\u4e3a\u4e00\u4e2a\u9690\u53d8\u91cf z ,\u4e00\u4e2a\u5148\u9a8c P\\psi(z) over shapes.\u7136\u540e\u4e00\u4e2adecoder, P_\\theta(X|z) . point generation from shape representations \\log P_{\\theta}(X | z)=\\sum_{x \\in X} \\log P_{\\theta}(x | z) \u5177\u4f53\u6765\u8bf4\uff0c\u4e00\u4e2a\u5728\u96c6\u5408 X \u4e2d\u7684\u70b9 x \uff0c\u662f\u4f7f\u7528CNF conditioned on z ,\u8f6c\u6362 y(t_0) \u7684\u7ed3\u679c,\u4e5f\u5c31\u662fneural ODE\u7684forward pass\u3002 x=G_{\\theta}\\left(y\\left(t_{0}\\right) ; z\\right) \\triangleq y\\left(t_{0}\\right)+\\int_{t_{0}}^{t_{1}} g_{\\theta}(y(t), t, z) d t, y\\left(t_{0}\\right) \\sim P(y) \u8fd9\u91cc g_\\theta \u5b9a\u4e49\u4e3aflow G_\\theta \u7684\u8fde\u7eed\u52a8\u529b\u5b66\u3002 G_{\\theta}^{-1}(x ; z)=x+\\int_{t_{1}}^{t_{0}} g_{\\theta}(y(t), t, z) d t with y(t_1)=x Flow-based \u5148\u9a8c over shapes \u56de\u987eKL-divergence(\u76f8\u5bf9\u71b5)\uff0c D_{KL}(p||q) = H(p, q) - H(p) forward\u65f6: z=F_{\\psi}\\left(w\\left(t_{0}\\right)\\right) \\triangleq w\\left(t_{0}\\right)+\\int_{t_{0}}^{t_{1}} f_{\\psi}(w(t), t) d t, w\\left(t_{0}\\right) \\sim P(w) \u6700\u7ec8training \u76ee\u6807 ELBO:VAE\u76ee\u6807 \\begin{aligned} \\mathcal{L}(X ; \\phi, \\psi, \\theta) &=\\mathbb{E}_{Q_{\\phi}(z | x)}\\left[\\log P_{\\psi}(z)+\\log P_{\\theta}(X | z)\\right]+H\\left[Q_{\\phi}(z | X)\\right] \\\\ &=\\mathbb{E}_{Q_{\\phi}(z | X)}\\left[\\log P\\left(F_{\\psi}^{-1}(z)\\right)-\\int_{t_{0}}^{t_{1}} \\operatorname{Tr}\\left(\\frac{\\partial f_{\\psi}}{\\partial w(t)}\\right) d t\\right.\\\\ &\\left.+\\sum_{x \\in X}\\left(\\log P\\left(G_{\\theta}^{-1}(x ; z)\\right)-\\int_{t_{0}}^{t_{1}} \\operatorname{Tr}\\left(\\frac{\\partial g_{\\theta}}{\\partial y(t)}\\right) d t\\right)\\right] \\\\ &+H\\left[Q_{\\phi}(z | X)\\right] \\end{aligned} \u53ef\u4ee5\u5206\u89e3\u4e3a\u4e09\u4e2a\u90e8\u5206 Prior:\u9f13\u52b1\u5927\u6982\u7387\uff0c\u8f83\u786e\u5b9a\u7684\u7ed3\u679c(\u8fd9\u91cc\u7528\u5355\u6b21\u8499\u7279\u5361\u6d1b\u91c7\u6837\u5bf9\u90a3\u4e2a\u671f\u671b\u8fdb\u884c\u4f30\u8ba1) \\mathbb{E}_{Q_{\\phi}(z | x)}\\left[\\log P_{\\psi}(z)\\right] \\approx \\frac{1}{L} \\sum_{l=1}^{L} \\log P_{\\psi}\\left(\\mu+\\epsilon_{l} \\odot \\sigma\\right) Reconstruction: \\mathcal{L}_{\\mathrm{recon}}(X ; \\theta, \\phi) \\triangleq \\mathbb{E}_{Q_{\\phi}}(z | x)\\left[\\log P_{\\theta}(X | z)\\right] Posterior Entropy: \\mathcal{L}_{\\mathrm{ent}}(X ; \\phi) \\triangleq H\\left[Q_{\\phi}(z | X)\\right]=\\frac{d}{2}(1+\\ln (2 \\pi))+\\sum_{i=1}^{d} \\ln \\sigma_{i} \u672c\u6587\u6a21\u578b\u9700\u8981\u6700\u5927\u5316\u8fd9\u4e2a\u76ee\u6807\u3002 \u603b\u89c8 \u8fdb\u4e00\u6b65\u7ec6\u8282\u89e3\u91ca\uff1a \u56fe\u4e2d\u7684 Q_\\phi \u4e3a\u7c7b\u4f3cpointNet\u7ed3\u6784\u7684\uff0cencoder\uff0c\u662f\u4e00\u7ef4\u5377\u79ef\uff0c\u6700\u540e\u5168\u8fde\u63a5\u8f93\u51fa\u4e24\u4e2a D_z \u7ef4\u5ea6\u7684\u5747\u503c\u4e0e\u65b9\u5dee\u9884\u6d4b f_\\phi \u4f7f\u7528\u7684\u662f FFJORD ,\u91cc\u9762\u4f7f\u7528\u4e86\u4e09\u4e2aconcatsquash\u5c42 \\mathrm{CS}(x, t)=\\left(W_{x} x+b_{x}\\right) \\sigma\\left(W_{t} t+b_{t}\\right)+\\left(W_{b} t+b_{b} t\\right) g_\\theta \u5904\u62d3\u5c55\u4e86concatsquash\u5c42\u4ee5\u4f7f\u5f97\u7ed3\u679c\u4e0ez\u76f8\u5173 \\begin{aligned} \\operatorname{CCS}(x, z, t)=&\\left(W_{x} x+b_{x}\\right) \\sigma\\left(W_{t t} t+W_{t z} z+b_{t}\\right) \\\\ &+\\left(W_{b t} t+W_{b z} z+b_{b} t\\right) \\end{aligned}","title":"PointFlow : 3D Point Cloud Generation with Continuous Normalizing Flows"},{"location":"other_categories/others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/#pointflow-3d-point-cloud-generation-with-continuous-normalizing-flows","text":"\u8fd9\u7bc7\u8bba\u6587\u6709\u4e00\u4e2a project page \uff0c\u6839\u636e \u89c6\u9891 \u5b8c\u6210\u7684\u4e00\u4e2a\u4efb\u52a1\u53ef\u4ee5\u8fd9\u6837\u63cf\u8ff0\u3002\u6bcf\u4e00\u4e2a\u7269\u4f53\u7684\u4e00\u79cd\u70b9\u4e91\u8868\u8fbe\u53ef\u4ee5\u7406\u89e3\u4e3a\u4ece\u4e00\u4e2a\u7531\u5f62\u72b6\u51b3\u5b9a\u7684\u6982\u7387\u5206\u5e03\u91c7\u6837\u70b9\u3002\u6211\u4eec\u53ef\u4ee5\u5229\u7528\u4e00\u4e2a\u5750\u6807\u8f6c\u6362\u51fd\u6570\uff0c\u5c06\u9ad8\u65af\u91c7\u6837\u70b9\u4e91\u8f6c\u6362\u4e3a\u6700\u540e\u7684\u8f93\u51fa\u3002\u672c\u6587\u7ed9\u51fa\u7684\u7f51\u7edc\u53ef\u4ee5\u6839\u636e\u7c7b\u522b\uff0c\u4ee5\u53ca\u4e00\u4e9b\u968f\u673a\u6570\uff0c\u91c7\u6837\u51fa\u4e0d\u540c\u5f62\u72b6\u7684\u4e00\u4e2a\u5750\u6807\u8f6c\u6362\u51fd\u6570\u3002","title":"PointFlow : 3D Point Cloud Generation with Continuous Normalizing Flows"},{"location":"other_categories/others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/#background-continuous-normalizing-flow","text":"\u5b9a\u4e49 f_1,...,f_n \u6307\u4ee3\u4e00\u7cfb\u5217\u7684\u53ef\u9006\u53d8\u6362\u3002\u8f93\u5165\u9690\u53d8\u91cf y \u7684\u6982\u7387\u5206\u5e03\u4e3a P(y) . x=f_{n} \\circ f_{n-1} \\circ \\cdots \\circ f_{1}(y) \u4f5c\u4e3a\u8f93\u51fa\u3002\u8f93\u51fa\u53d8\u91cf\u7684\u6982\u7387\u5bc6\u5ea6\u5219\u53d8\u4e3a \\log P(x)=\\log P(y)-\\sum_{k=1}^{n} \\log \\left|\\operatorname{det} \\frac{\\partial f_{k}}{\\partial y_{k-1}}\\right| \u8fd9\u6837 y \u53ef\u4ee5\u4ece x \u4f7f\u7528inverse flow\u8ba1\u7b97: y=f_{1}^{-1} \\circ \\cdots \\circ f_{n}^{-1}(x) . \u8fd9\u91cc f_1,...,f_n \u5728\u8fd9\u91cc\u5b9e\u4f53\u5316\u4f5c\u4e3a\u795e\u7ecf\u7f51\u7edc\uff0c\u8fd9\u6837\u5bfc\u6570\u7684\u884c\u5217\u5f0f\u8ba1\u7b97\u96be\u5ea6\u4e0d\u5927\u3002 continuous normalizing flow(CNF) for P(x) \u5c31\u662f x=y\\left(t_{0}\\right)+\\int_{t_{0}}^{t_{1}} f(y(t), t) d t, \\quad y\\left(t_{0}\\right) \\sim P(y) \\log P(x)=\\log P\\left(y\\left(t_{0}\\right)\\right)-\\int_{t_{0}}^{t_{1}} \\operatorname{Tr}\\left(\\frac{\\partial f}{\\partial y(t)}\\right) d t inverse flow y(t_0) = x + \\int^{t_0}_{t_1}f(y(t),t)dt .\u8fd9\u91cc\u7684 f \u4ecd\u7136\u662f\u4e00\u4e2a\u795e\u7ecf\u7f51\u7edc,\u7528ODE\u6c42\u89e3\u5668\u4f5c\u4e3a\u8f93\u51fa\u3002\u8fd9\u91cc\u7684\u5173\u952e\u601d\u7ef4\u8f6c\u53d8\u662f\u5229\u7528\u4e86\uff0c Neural Ordinary Equation \u7684\u601d\u8def\uff0c\u5c06\u8fde\u7eed\u53e0\u5c42\u7684\u5171\u4eab\u6743\u91cd\u7684\u795e\u7ecf\u7f51\u7edc\u7528\u5e38\u5fae\u5206\u65b9\u7a0b\u4ee3\u66ff.\u7528\u8fd9\u4e2aneural ODE\u7684\u4e00\u4e2a\u91cd\u70b9\u662f\u4f7f\u5f97\u9006\u53d8\u6362\u53ef\u4ee5\u540c\u6837\u901a\u8fc7ODE\u65f6\u95f4\u4e0a\u76f8\u53cd\u7684\u4e00\u4e2aforward pass\u5b9e\u73b0\u3002","title":"Background: Continuous normalizing flow"},{"location":"other_categories/others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/#variational-auto-encoder","text":"\u539f\u6587 .VAE\u4e2d\uff0c z \u662flatent space,\u5305\u542b\u4e00\u4e2a\u63cf\u8ff0 P_\\theta(X|z) \u4f5c\u4e3adecoder,\u8fd8\u5b66\u4e60\u4e00\u4e2aencoder Q_\\phi(z|X) ,\u5b83\u4eec\u5171\u540c\u8bad\u7ec3\u53bb\u6700\u5927\u5316\u89c2\u5bdf\u5668\u7684log-likelihood \\begin{aligned} \\log P_{\\theta}(X) & \\geq \\log P_{\\theta}(X)-D_{K L}\\left(Q_{\\phi}(z | X) \\| P_{\\theta}(z | X)\\right) \\\\ &=\\mathbb{E}_{Q_{\\phi}(z | x)}\\left[\\log P_{\\theta}(X | z)\\right]-D_{K L}\\left(Q_{\\phi}(z | X) \\| P_{\\psi}(z)\\right) \\\\ & \\triangleq \\mathcal{L}(X ; \\phi, \\psi, \\theta) \\end{aligned} z = \\mu_phi(X) + \\sigma_\\phi(X) \\epsilon .\u8fd9\u4e2a\u5747\u503c\u4e0e\u65b9\u5dee\u90fd\u662f\u7f51\u7edc\u76f4\u63a5\u7684\u8f93\u51fa\u3002","title":"Variational auto-encoder"},{"location":"other_categories/others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/#_1","text":"\u9996\u5148\u5305\u542b\u4e09\u4e2a\u6a21\u5757\uff0c\u7b2c\u4e00\u4e2a\u662fencoder\uff0c Q_\\phi(z|X) ,\u5c06\u70b9\u4e91\u7f16\u7801\u4e3a\u4e00\u4e2a\u9690\u53d8\u91cf z ,\u4e00\u4e2a\u5148\u9a8c P\\psi(z) over shapes.\u7136\u540e\u4e00\u4e2adecoder, P_\\theta(X|z) .","title":"\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5"},{"location":"other_categories/others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/#point-generation-from-shape-representations","text":"\\log P_{\\theta}(X | z)=\\sum_{x \\in X} \\log P_{\\theta}(x | z) \u5177\u4f53\u6765\u8bf4\uff0c\u4e00\u4e2a\u5728\u96c6\u5408 X \u4e2d\u7684\u70b9 x \uff0c\u662f\u4f7f\u7528CNF conditioned on z ,\u8f6c\u6362 y(t_0) \u7684\u7ed3\u679c,\u4e5f\u5c31\u662fneural ODE\u7684forward pass\u3002 x=G_{\\theta}\\left(y\\left(t_{0}\\right) ; z\\right) \\triangleq y\\left(t_{0}\\right)+\\int_{t_{0}}^{t_{1}} g_{\\theta}(y(t), t, z) d t, y\\left(t_{0}\\right) \\sim P(y) \u8fd9\u91cc g_\\theta \u5b9a\u4e49\u4e3aflow G_\\theta \u7684\u8fde\u7eed\u52a8\u529b\u5b66\u3002 G_{\\theta}^{-1}(x ; z)=x+\\int_{t_{1}}^{t_{0}} g_{\\theta}(y(t), t, z) d t with y(t_1)=x","title":"point generation from shape representations"},{"location":"other_categories/others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/#flow-based-over-shapes","text":"\u56de\u987eKL-divergence(\u76f8\u5bf9\u71b5)\uff0c D_{KL}(p||q) = H(p, q) - H(p) forward\u65f6: z=F_{\\psi}\\left(w\\left(t_{0}\\right)\\right) \\triangleq w\\left(t_{0}\\right)+\\int_{t_{0}}^{t_{1}} f_{\\psi}(w(t), t) d t, w\\left(t_{0}\\right) \\sim P(w)","title":"Flow-based \u5148\u9a8c over shapes"},{"location":"other_categories/others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/#training","text":"ELBO:VAE\u76ee\u6807 \\begin{aligned} \\mathcal{L}(X ; \\phi, \\psi, \\theta) &=\\mathbb{E}_{Q_{\\phi}(z | x)}\\left[\\log P_{\\psi}(z)+\\log P_{\\theta}(X | z)\\right]+H\\left[Q_{\\phi}(z | X)\\right] \\\\ &=\\mathbb{E}_{Q_{\\phi}(z | X)}\\left[\\log P\\left(F_{\\psi}^{-1}(z)\\right)-\\int_{t_{0}}^{t_{1}} \\operatorname{Tr}\\left(\\frac{\\partial f_{\\psi}}{\\partial w(t)}\\right) d t\\right.\\\\ &\\left.+\\sum_{x \\in X}\\left(\\log P\\left(G_{\\theta}^{-1}(x ; z)\\right)-\\int_{t_{0}}^{t_{1}} \\operatorname{Tr}\\left(\\frac{\\partial g_{\\theta}}{\\partial y(t)}\\right) d t\\right)\\right] \\\\ &+H\\left[Q_{\\phi}(z | X)\\right] \\end{aligned} \u53ef\u4ee5\u5206\u89e3\u4e3a\u4e09\u4e2a\u90e8\u5206 Prior:\u9f13\u52b1\u5927\u6982\u7387\uff0c\u8f83\u786e\u5b9a\u7684\u7ed3\u679c(\u8fd9\u91cc\u7528\u5355\u6b21\u8499\u7279\u5361\u6d1b\u91c7\u6837\u5bf9\u90a3\u4e2a\u671f\u671b\u8fdb\u884c\u4f30\u8ba1) \\mathbb{E}_{Q_{\\phi}(z | x)}\\left[\\log P_{\\psi}(z)\\right] \\approx \\frac{1}{L} \\sum_{l=1}^{L} \\log P_{\\psi}\\left(\\mu+\\epsilon_{l} \\odot \\sigma\\right) Reconstruction: \\mathcal{L}_{\\mathrm{recon}}(X ; \\theta, \\phi) \\triangleq \\mathbb{E}_{Q_{\\phi}}(z | x)\\left[\\log P_{\\theta}(X | z)\\right] Posterior Entropy: \\mathcal{L}_{\\mathrm{ent}}(X ; \\phi) \\triangleq H\\left[Q_{\\phi}(z | X)\\right]=\\frac{d}{2}(1+\\ln (2 \\pi))+\\sum_{i=1}^{d} \\ln \\sigma_{i} \u672c\u6587\u6a21\u578b\u9700\u8981\u6700\u5927\u5316\u8fd9\u4e2a\u76ee\u6807\u3002","title":"\u6700\u7ec8training \u76ee\u6807"},{"location":"other_categories/others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/#_2","text":"\u8fdb\u4e00\u6b65\u7ec6\u8282\u89e3\u91ca\uff1a \u56fe\u4e2d\u7684 Q_\\phi \u4e3a\u7c7b\u4f3cpointNet\u7ed3\u6784\u7684\uff0cencoder\uff0c\u662f\u4e00\u7ef4\u5377\u79ef\uff0c\u6700\u540e\u5168\u8fde\u63a5\u8f93\u51fa\u4e24\u4e2a D_z \u7ef4\u5ea6\u7684\u5747\u503c\u4e0e\u65b9\u5dee\u9884\u6d4b f_\\phi \u4f7f\u7528\u7684\u662f FFJORD ,\u91cc\u9762\u4f7f\u7528\u4e86\u4e09\u4e2aconcatsquash\u5c42 \\mathrm{CS}(x, t)=\\left(W_{x} x+b_{x}\\right) \\sigma\\left(W_{t} t+b_{t}\\right)+\\left(W_{b} t+b_{b} t\\right) g_\\theta \u5904\u62d3\u5c55\u4e86concatsquash\u5c42\u4ee5\u4f7f\u5f97\u7ed3\u679c\u4e0ez\u76f8\u5173 \\begin{aligned} \\operatorname{CCS}(x, z, t)=&\\left(W_{x} x+b_{x}\\right) \\sigma\\left(W_{t t} t+W_{t z} z+b_{t}\\right) \\\\ &+\\left(W_{b t} t+W_{b z} z+b_{b} t\\right) \\end{aligned}","title":"\u603b\u89c8"},{"location":"other_categories/others/R2D2/","text":"R2D2: Repeatable and Reliable Detector and Descriptor \u8fd9\u7bc7paper\u5728\u8f93\u51fakeypoints\u7684\u65f6\u5019\u8f93\u51fa\u4e24\u4e2a\u5206\u652f\u9884\u6d4b\u5176\u91cd\u590d\u6027\u4ee5\u53ca\u53ef\u9760\u6027\u3002paper\u7684\u4e3b\u8981\u8d21\u732e\u6709\u4e24\u70b9\uff0c\u7b2c\u4e00\u5728\u4e8e\u5224\u65ad\u8ba4\u4e3a\u6709\u5fc5\u8981\u5c06\u53ef\u9760\u6027\u4e0e\u53ef\u91cd\u590d\u6027\u5206\u6210\u4e24\u4e2a\u5355\u72ec\u7684metrics\u8fdb\u884c\u9884\u6d4b\uff0c\u4e14\u4f7f\u7528\u4e0d\u540c\u7684\u8bad\u7ec3\u65b9\u5f0f\u3002\u7b2c\u4e8c\u5728\u4e8e\u4f7f\u7528metric learning\u8bad\u7ec3\u53ef\u9760\u6027\u4e0e\u51c6\u786e\u5ea6(\u4e0d\u7b97\u72ec\u521b\u4f46\u662f\u5c5e\u4e8e\u6709\u6548\u7684\u878d\u5408) \u7f51\u7edc\u7ed3\u6784 \u53ef\u91cd\u590d\u6027 \u53ef\u91cd\u590d\u6027\u8fd9\u91cc\u4e0e\u5176\u4ed6\u505a\u6cd5\u76f8\u4f3c\uff0c\u9009\u62e9\u7684\u662f\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u65b9\u6cd5\u3002 \u5bf9\u4e8e\u4e24\u5f20\u5df2\u77e5correspondence\u7684\u56fe\uff0c\u635f\u5931\u7684\u76ee\u7684\u662f\u8ba9\u4e24\u5f20\u56fe\u5bf9\u5e94\u7684\u90e8\u5206\u6709\u8f83\u9ad8\u7684\u76f8\u4f3c\u5ea6(cosine similarity)\uff0c \\mathcal{L}_{\\operatorname{cosim}}\\left(I, I^{\\prime}, U\\right)=1-\\frac{1}{|\\mathcal{P}|} \\sum_{p \\in \\mathcal{P}} \\operatorname{cosim}\\left(\\boldsymbol{S}[p], \\boldsymbol{S}_{U}^{\\prime}[p]\\right) \u8fd9\u4f1a\u6709\u4e00\u4e2atrivial\u7684\u89e3\u5c31\u662f\u6240\u6709\u503c\u4e3a\u5e38\u6570\uff0c\u6240\u4ee5\u7ed9\u51fa\u4e00\u4e2a\u8865\u5145\u7684\u635f\u5931\u51fd\u6570\u8981\u6c42\u63d0\u5347\u533a\u57df\u5185\u6700\u5927\u503c\u4e0e\u5747\u503c\u7684\u5dee \\mathcal{L}_{\\text {peaky}}(I)=1-\\frac{1}{|\\mathcal{P}|} \\sum_{p \\in \\mathcal{P}}\\left(\\max _{(i, j) \\in p} \\boldsymbol{S}_{i j}-\\operatorname{mean}_{(i, j) \\in p} \\boldsymbol{S}_{i j}\\right) \u53ef\u9760\u6027 \u5373\u8981\u6c42\u5339\u914d\u7684\u51c6\u786e\u5ea6\u63d0\u5347\uff0c\u8fd9\u91cc\u5f15\u7528\u4e86 Local Descriptors Optimized for Average Precision.pdf \u7684\u505a\u6cd5\u3002\u5176\u4e2d\u4e24\u4e2ahistogram\u4e4b\u95f4\u7684loss\u6765\u81ea\u4e8e Learning Deep Embeddings with Histogram Loss.pdf code in here","title":"R2D2: Repeatable and Reliable Detector and Descriptor"},{"location":"other_categories/others/R2D2/#r2d2-repeatable-and-reliable-detector-and-descriptor","text":"\u8fd9\u7bc7paper\u5728\u8f93\u51fakeypoints\u7684\u65f6\u5019\u8f93\u51fa\u4e24\u4e2a\u5206\u652f\u9884\u6d4b\u5176\u91cd\u590d\u6027\u4ee5\u53ca\u53ef\u9760\u6027\u3002paper\u7684\u4e3b\u8981\u8d21\u732e\u6709\u4e24\u70b9\uff0c\u7b2c\u4e00\u5728\u4e8e\u5224\u65ad\u8ba4\u4e3a\u6709\u5fc5\u8981\u5c06\u53ef\u9760\u6027\u4e0e\u53ef\u91cd\u590d\u6027\u5206\u6210\u4e24\u4e2a\u5355\u72ec\u7684metrics\u8fdb\u884c\u9884\u6d4b\uff0c\u4e14\u4f7f\u7528\u4e0d\u540c\u7684\u8bad\u7ec3\u65b9\u5f0f\u3002\u7b2c\u4e8c\u5728\u4e8e\u4f7f\u7528metric learning\u8bad\u7ec3\u53ef\u9760\u6027\u4e0e\u51c6\u786e\u5ea6(\u4e0d\u7b97\u72ec\u521b\u4f46\u662f\u5c5e\u4e8e\u6709\u6548\u7684\u878d\u5408)","title":"R2D2: Repeatable and Reliable Detector and Descriptor"},{"location":"other_categories/others/R2D2/#_1","text":"","title":"\u7f51\u7edc\u7ed3\u6784"},{"location":"other_categories/others/R2D2/#_2","text":"\u53ef\u91cd\u590d\u6027\u8fd9\u91cc\u4e0e\u5176\u4ed6\u505a\u6cd5\u76f8\u4f3c\uff0c\u9009\u62e9\u7684\u662f\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u65b9\u6cd5\u3002 \u5bf9\u4e8e\u4e24\u5f20\u5df2\u77e5correspondence\u7684\u56fe\uff0c\u635f\u5931\u7684\u76ee\u7684\u662f\u8ba9\u4e24\u5f20\u56fe\u5bf9\u5e94\u7684\u90e8\u5206\u6709\u8f83\u9ad8\u7684\u76f8\u4f3c\u5ea6(cosine similarity)\uff0c \\mathcal{L}_{\\operatorname{cosim}}\\left(I, I^{\\prime}, U\\right)=1-\\frac{1}{|\\mathcal{P}|} \\sum_{p \\in \\mathcal{P}} \\operatorname{cosim}\\left(\\boldsymbol{S}[p], \\boldsymbol{S}_{U}^{\\prime}[p]\\right) \u8fd9\u4f1a\u6709\u4e00\u4e2atrivial\u7684\u89e3\u5c31\u662f\u6240\u6709\u503c\u4e3a\u5e38\u6570\uff0c\u6240\u4ee5\u7ed9\u51fa\u4e00\u4e2a\u8865\u5145\u7684\u635f\u5931\u51fd\u6570\u8981\u6c42\u63d0\u5347\u533a\u57df\u5185\u6700\u5927\u503c\u4e0e\u5747\u503c\u7684\u5dee \\mathcal{L}_{\\text {peaky}}(I)=1-\\frac{1}{|\\mathcal{P}|} \\sum_{p \\in \\mathcal{P}}\\left(\\max _{(i, j) \\in p} \\boldsymbol{S}_{i j}-\\operatorname{mean}_{(i, j) \\in p} \\boldsymbol{S}_{i j}\\right)","title":"\u53ef\u91cd\u590d\u6027"},{"location":"other_categories/others/R2D2/#_3","text":"\u5373\u8981\u6c42\u5339\u914d\u7684\u51c6\u786e\u5ea6\u63d0\u5347\uff0c\u8fd9\u91cc\u5f15\u7528\u4e86 Local Descriptors Optimized for Average Precision.pdf \u7684\u505a\u6cd5\u3002\u5176\u4e2d\u4e24\u4e2ahistogram\u4e4b\u95f4\u7684loss\u6765\u81ea\u4e8e Learning Deep Embeddings with Histogram Loss.pdf code in here","title":"\u53ef\u9760\u6027"},{"location":"other_categories/others/SPM_SPR/","text":"Single-Stage Multi-Person Pose Machines \u4e0a\u9762\u7ed9\u51fa\u7684\u5f00\u6e90\u4ee3\u7801\u662f\u975e\u5b98\u65b9\uff0c\u4f5c\u8005\u7ed9\u51fa\u4e86\u89e3\u8bfb\u8fd9\u7bc7paper\u7684 csdn blog , \u6838\u5fc3\u601d\u8def\u5c31\u662f\u5c06\u80a2\u4f53\u5173\u8282\u7684\u56de\u5f52\u5206\u6210\u4e24\u90e8\u5206\uff0c\u4e00\u4e2a\u662froot\uff0c\u4e5f\u5c31\u662f\u4e2d\u5fc3\u70b9\u7684\u4f30\u8ba1\uff0c\u8fd9\u4e2a\u7528heat-map regression\u5b9e\u73b0\u3002\u53e6\u4e00\u4e2a\u662f\u6811\u72b6\u7684\uff0c\u5404\u4e2a\u80a2\u4f53\u5173\u8282\u70b9\u4e0e\u5176\u76f8\u90bb\u7236\u5173\u8282\u70b9\u7684\u76f8\u5bf9\u4f4d\u79fb\u3002\u4f5c\u8005\u603b\u5171\u5206\u4e3a5\u4e2a\u6811\uff0c root->neck->head root->left/right shoulder->left/right elbow->left/right wrist root->left/right hip->left/right knee->left/right ankle \u63a8\u65ad\u7684\u65f6\u5019\u4ec5\u9700\u8981\u9884\u6d4broot\uff0c\u5e76\u901a\u8fc7\u76f8\u5bf9\u4f4d\u79fb\u5904\u7406\u5373\u53ef\u3002\u6700\u7ec8\u7f51\u7edc\u662f\u5355\u9636\u6bb5\u7684\u63a8\u7406\u3002","title":"Single-Stage Multi-Person Pose Machines"},{"location":"other_categories/others/SPM_SPR/#single-stage-multi-person-pose-machines","text":"\u4e0a\u9762\u7ed9\u51fa\u7684\u5f00\u6e90\u4ee3\u7801\u662f\u975e\u5b98\u65b9\uff0c\u4f5c\u8005\u7ed9\u51fa\u4e86\u89e3\u8bfb\u8fd9\u7bc7paper\u7684 csdn blog , \u6838\u5fc3\u601d\u8def\u5c31\u662f\u5c06\u80a2\u4f53\u5173\u8282\u7684\u56de\u5f52\u5206\u6210\u4e24\u90e8\u5206\uff0c\u4e00\u4e2a\u662froot\uff0c\u4e5f\u5c31\u662f\u4e2d\u5fc3\u70b9\u7684\u4f30\u8ba1\uff0c\u8fd9\u4e2a\u7528heat-map regression\u5b9e\u73b0\u3002\u53e6\u4e00\u4e2a\u662f\u6811\u72b6\u7684\uff0c\u5404\u4e2a\u80a2\u4f53\u5173\u8282\u70b9\u4e0e\u5176\u76f8\u90bb\u7236\u5173\u8282\u70b9\u7684\u76f8\u5bf9\u4f4d\u79fb\u3002\u4f5c\u8005\u603b\u5171\u5206\u4e3a5\u4e2a\u6811\uff0c root->neck->head root->left/right shoulder->left/right elbow->left/right wrist root->left/right hip->left/right knee->left/right ankle \u63a8\u65ad\u7684\u65f6\u5019\u4ec5\u9700\u8981\u9884\u6d4broot\uff0c\u5e76\u901a\u8fc7\u76f8\u5bf9\u4f4d\u79fb\u5904\u7406\u5373\u53ef\u3002\u6700\u7ec8\u7f51\u7edc\u662f\u5355\u9636\u6bb5\u7684\u63a8\u7406\u3002","title":"Single-Stage Multi-Person Pose Machines"},{"location":"other_categories/others/SomePapersOnDifferentiableCvxOpt/","text":"Some Papers on Differentiable Convex Optimization \u5728\u8fd9\u4e2a\u9875\u9762\u8ba1\u5212\u8bb0\u5f55\u4e24\u7bc7\u5173\u4e8e\u53ef\u5fae\u5206\u51f8\u4f18\u5316\u7684\u7b97\u6cd5\u7684\u8bb0\u5f55.\u672c\u6587\u6b64\u5904\u4f1a\u5927\u81f4\u8bb0\u5f55\u4e09\u7bc7\u6587\u7ae0\u544a\u8bc9\u6211\u7684\u4e00\u4e9b\u65b0\u7684\u6982\u5ff5,\u5206\u522b\u662f\u4e00\u7bc7\u5173\u4e8e\u51f8\u4f18\u5316\u4ee5\u53caDisciplined convex programming\u7684\u7efc\u8ff0 (1) ,\u4e00\u7bc7\u5173\u4e8e\u51f8\u4f18\u5316\u7684\u5bfc\u6570\u7684paper (2) ,\u4e00\u7bc7\u4ecb\u7ecd[cvxlayers]\u7684paper (3) .\u5b83\u4eec\u90fd\u6765\u81ea\u4e8eStanford Boyd\u5927\u4f6c\u7684\u7ec4\u3002 Convex optimization and Disciplined Convex Programming \u4e00\u822c\u51f8\u4f18\u5316\u95ee\u9898\u5b9a\u4e49\uff1a \\begin{aligned} \\text { minimize } & f_{0}(x, \\theta) \\\\ & f(x, \\theta) \\preceq 0 \\\\ & h(x, \\theta)=0 \\end{aligned} \u7efc\u8ff0 (1) \u5bf9\u51f8\u4f18\u5316\u95ee\u9898\u7684\u7279\u6b8a\u60c5\u51b5(\u7ebf\u6027\u6700\u4f18\uff0c\u4e8c\u6b21\u6700\u4f18\uff0c\u6700\u5c0f\u4e8c\u4e58)\u8fdb\u884c\u5206\u7c7b\uff0c\u540c\u65f6\u6307\u51fa\u4e86\u4e13\u7528\u95ee\u9898\u4e13\u7528\u7b97\u6cd5\u4ee5\u53ca\u975e\u7ebf\u6027\u51f8\u4f18\u5316\u666e\u904d\u65b9\u6cd5\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u6307\u51fa\u4e86\u975e\u7ebf\u6027\u51f8\u4f18\u5316\u4e00\u822c\u65b9\u6cd5\u6bd4\u5982\u5185\u70b9\u6cd5\u5728\u6548\u7387\u4e0a\u5f80\u5f80\u4f1a\u548c\u7ebf\u6027\u6216\u4e8c\u6b21\u51f8\u4f18\u5316\u7b97\u6cd5\u76f8\u8fd1\uff0c\u539f\u56e0\u662f\u90fd\u5229\u7528\u4e86\u5c40\u90e8\u7684convexity. Disciplined Convex Programming\u5305\u542b\u4e24\u4e2a\u5185\u5bb9\uff0c\u4e00\u4e2a\u662f\u5c06\u7b26\u5408\u8981\u6c42\u7684\u51f8\u4f18\u5316\u95ee\u9898\u901a\u8fc7\u4e00\u7cfb\u5217\u539f\u5b50\u64cd\u4f5c\u5e76\u8f6c\u6362\u4e3a\u6807\u51c6\u95ee\u9898\uff0c\u53e6\u4e00\u4e2a\u662f\u4e00\u5957\u7528\u4e8e\u5224\u65ad\u7528\u6237\u8f93\u5165\u7684\u95ee\u9898\u662f\u5426\u4e3a\u7b26\u5408\u8981\u6c42\u7684DCP\u7684\u89c4\u5219\u7cfb\u7edf\u3002\u5728\u672c\u6587\u4f5c\u8005\u8be6\u7ec6\u63cf\u8ff0\u4e86DCP\u89c4\u5219\u7cfb\u7edf\u7684\u7b97\u6cd5\uff0c\u5e76\u63d0\u4f9b\u4e86\u51b3\u7b56\u6811\u4f8b\u5b50\u3002\u5728[cvxlayers]\u7684paper (3) \u4e2d\u4ee5\u53ca\u4ee3\u7801\u6587\u6863\u4e2d,\u6211\u4eec\u53ef\u4ee5\u77e5\u6653\u76ee\u524d\u5e93\u4e2d\u652f\u6301\u7684\u539f\u5b50\u64cd\u4f5c(\u7edd\u5927\u591a\u6570\u7684\u5355\u8c03\u51fd\u6570\u4ee5\u53ca\u591a\u9879\u5f0f\u51fd\u6570) \u57fa\u4e8eKKT condition\u7684 Convex optimization \u5bfc\u6570\u4f20\u9012 \u8fd9\u91cc\u91c7\u7528\u7684\u601d\u8def\u5f88\u63a5\u8fd1\u4e0e OptNet ,paper (2) \u5c06\u8fd9\u4e2a\u95ee\u9898\u7684\u89e3\u7b54\u62d3\u5c55\u5230\u8fd1\u4e4e\u4efb\u610f\u51f8\u4f18\u5316\u95ee\u9898\uff0c\u5728\u63a5\u8fd1\u6700\u4f18\u7684\u60c5\u51b5\u4e0b\uff0c\u4f7f\u7528KKT condition\u8fdb\u884c\u53cd\u4f20\u3002 KKT\u6761\u4ef6: \\begin{aligned} f(\\tilde{x}, \\theta) & \\preceq 0 \\\\ h(\\tilde{x}, \\theta) &=0 \\\\ \\tilde{\\lambda}_{i} & \\geq 0, \\quad i=1, \\ldots, m \\\\ \\tilde{\\lambda}_{i} f_{i}(\\tilde{x}, \\theta) &=0, \\quad i=1, \\ldots, m \\\\ \\nabla_{x} L(\\tilde{x}, \\tilde{\\lambda}, \\tilde{\\nu}, \\theta) &=0 \\end{aligned} \u5176\u4e2d \\mathcal{L} \u4e3a\u539f\u51fd\u6570\u52a0\u4e0a\u62c9\u683c\u6717\u65e5\u4e58\u5b50\uff0c\u4e3a: L(x, \\lambda, \\nu, \\theta)=f_{0}(x, \\theta)+\\lambda^{T} f(x, \\theta)+\\nu^{T} h(x, \\theta) \u518d\u5b9a\u4e49 g(z, \\theta)=\\left[\\begin{array}{c} {\\nabla_{x} L(x, \\lambda, \\nu, \\theta)} \\\\ {\\operatorname{diag}(\\lambda) f(x, \\theta)} \\\\ {h(x, \\theta)} \\end{array}\\right] \u4f5c\u8005\u6700\u7ec8\u7ed9\u51fa\u7684\u5bfc\u6570\u516c\u5f0f \\mathrm{D}_{z} g(\\tilde{z}, \\theta)=\\left[\\begin{array}{ccc} {\\mathrm{D}_{x} \\nabla_{x} L(\\tilde{x}, \\tilde{\\lambda}, \\tilde{\\nu}, \\theta)} & {\\mathrm{D}_{x} f(\\tilde{x}, \\theta)^{T}} & {\\mathrm{D}_{x} h(\\tilde{x}, \\theta)^{T}} \\\\ {\\operatorname{diag}(\\tilde{\\lambda}) \\mathrm{D}_{x} f(\\tilde{x}, \\theta)} & {\\operatorname{diag}(f(\\tilde{x}, \\theta))} & {0} \\\\ {\\mathrm{D}_{x} h(\\tilde{x}, \\theta)} & {0} & {0} \\end{array}\\right] \\mathrm{D}_{\\theta} g(\\tilde{z}, \\theta)=\\left[\\begin{array}{c} {\\mathrm{D}_{\\theta} \\nabla_{x} L(\\tilde{x}, \\tilde{\\lambda}, \\tilde{\\nu}, \\theta)} \\\\ {\\operatorname{diag}(\\tilde{\\lambda}) \\mathrm{D}_{\\theta} f(\\tilde{x}, \\theta)} \\\\ {\\mathrm{D}_{\\theta} h(\\tilde{x}, \\theta)} \\end{array}\\right] \\mathrm{D}_{\\theta} s(\\theta)=-\\mathrm{D}_{z} g(\\tilde{x}, \\tilde{\\lambda}, \\tilde{\\nu}, \\theta)^{-1} \\mathrm{D}_{\\theta} g(\\tilde{x}, \\tilde{\\lambda}, \\tilde{\\nu}, \\theta) \\text { for every } \\theta \\in Q \u8003\u8651\u4e8c\u6b21\u89c4\u5212\u4f5c\u4e3a\u672c\u6587\u7684\u7279\u6b8a\u60c5\u51b5\uff0c\u5e26\u5165\u516c\u5f0f\u6709 \\mathrm{D}_{x} g(\\tilde{x}, \\tilde{\\lambda}, \\tilde{\\nu}, \\theta)=\\left[\\begin{array}{ccc} {Q} & {G^{T}} & {A^{T}} \\\\ {\\operatorname{diag}(\\tilde{\\lambda}) G} & {\\operatorname{diag}(G \\tilde{x}-h)} & {0} \\\\ {A} & {0} & {0} \\end{array}\\right] \\mathrm{D}_{\\theta} g(\\dot{x}, \\tilde{\\lambda}, \\tilde{\\nu}, \\theta)=\\left[\\begin{array}{c} {\\mathrm{d} Q \\tilde{x}+\\mathrm{D}_{\\theta} q+\\mathrm{d} G^{T} \\tilde{\\lambda}+\\mathrm{d} A^{T} \\tilde{\\nu}} \\\\ {\\operatorname{diag}(\\lambda)\\left(\\mathrm{d} G \\tilde{x}-\\mathrm{D}_{\\theta} h\\right)} \\\\ {\\mathrm{d} A \\tilde{x}-\\mathrm{D}_{\\theta} b} \\end{array}\\right] \u6700\u7ec8\u7684\u516c\u5f0f\u4e0e OptNet \u662f\u4e00\u81f4\u7684 Convex Optimization as a Differentiable learnable layer \u57282019 NeurIPS\u7684paper (3) \u4e2d, \u4f5c\u8005\u5c06\u521d\u59cb\u89e3 x_0 ,\u6700\u4f18\u5316\u95ee\u9898\u53c2\u6570 \\theta ,\u6700\u7ec8\u8f93\u51fa s \u7406\u89e3\u4e3a\u795e\u7ecf\u7f51\u7edc\u5c42\u7684\u8f93\u5165\u3001\u53c2\u6570\u4ee5\u53ca\u8f93\u51fa\u3002\u8fdb\u4e00\u6b65\u5730\uff0c\u4f5c\u8005\u7cc5\u5408\u524d\u4e24\u4e2a\u7ae0\u8282\u7684idea,\u4f7f\u7528 Disciplined Convex Programming\u4f7f\u5f97\u7528\u6237\u5728\u8bbe\u8ba1\u4f18\u5316\u5c42\u7684\u65f6\u5019\u6709\u66f4\u9ad8\u7684\u81ea\u7531\u5ea6(\u4e0d\u518d\u53ea\u5c40\u9650\u4e8e\u4e00\u6b21\u6216\u4e8c\u6b21\u578b\u7b49\u7279\u6027)\uff0c\u8f93\u5165\u4e0e\u53c2\u6570\u5728\u5f62\u6210\u6210\u672c\u51fd\u6570\u4e0e\u7ea6\u675f\u7684\u8ba1\u7b97\u8fc7\u7a0b\u4e2d\u5141\u8bb8\u6309\u7167DCP\u89c4\u5219\u94fe\u63a5\u5927\u91cf\u7684\u539f\u5b50\u64cd\u4f5c\uff0c\u5f62\u6210\u66f4\u4e3a\u590d\u6742\u800c\u81ea\u7136\u7684\"\u4e00\u822c\"\u4f18\u5316\u95ee\u9898\u8f93\u5165(\u6ce8: cvxpylayers \u867d\u7136\u652f\u6301\u5f88\u591a\u522b\u7684\u51fd\u6570\uff0c\u4f46\u662f\u4e0d\u652f\u6301\u6b63\u4f59\u5f26\u51fd\u6570)\u3002\u4f7f\u7528KKT\u5f53\u4f18\u5316\u7ed3\u679c\u6536\u655b\u65f6\u5feb\u901f\u5730\u5b9e\u73b0\u53cd\u5411\u4f20\u64ad\u3002 \u4f5c\u8005\u5728 (3) \u7684\u9644\u5f55\u4e2d\u7ed9\u51fa\u4e86\u4f7f\u7528cvxpylayers\u7528\u6700\u4f18\u5316\u95ee\u9898\u5b9e\u73b0ReLU,Softmax\uff0cQP\u7b49\u95ee\u9898\u7684\u4ee3\u7801\u3002 cvxpylayers \u7684\u4ee3\u7801\u540c\u6837\u4e5f\u6709\u5f88\u591a\u7684\u4f8b\u7a0b,\u5728RL\u3001\u63a7\u5236\u3001\u7f51\u7edc\u540e\u4f18\u5316\u9886\u57df\u6709\u8f83\u5f3a\u7684\u4f7f\u7528\u7a7a\u95f4\u3002","title":"Some Papers on Differentiable Convex Optimization"},{"location":"other_categories/others/SomePapersOnDifferentiableCvxOpt/#some-papers-on-differentiable-convex-optimization","text":"\u5728\u8fd9\u4e2a\u9875\u9762\u8ba1\u5212\u8bb0\u5f55\u4e24\u7bc7\u5173\u4e8e\u53ef\u5fae\u5206\u51f8\u4f18\u5316\u7684\u7b97\u6cd5\u7684\u8bb0\u5f55.\u672c\u6587\u6b64\u5904\u4f1a\u5927\u81f4\u8bb0\u5f55\u4e09\u7bc7\u6587\u7ae0\u544a\u8bc9\u6211\u7684\u4e00\u4e9b\u65b0\u7684\u6982\u5ff5,\u5206\u522b\u662f\u4e00\u7bc7\u5173\u4e8e\u51f8\u4f18\u5316\u4ee5\u53caDisciplined convex programming\u7684\u7efc\u8ff0 (1) ,\u4e00\u7bc7\u5173\u4e8e\u51f8\u4f18\u5316\u7684\u5bfc\u6570\u7684paper (2) ,\u4e00\u7bc7\u4ecb\u7ecd[cvxlayers]\u7684paper (3) .\u5b83\u4eec\u90fd\u6765\u81ea\u4e8eStanford Boyd\u5927\u4f6c\u7684\u7ec4\u3002","title":"Some Papers on Differentiable Convex Optimization"},{"location":"other_categories/others/SomePapersOnDifferentiableCvxOpt/#convex-optimization-and-disciplined-convex-programming","text":"\u4e00\u822c\u51f8\u4f18\u5316\u95ee\u9898\u5b9a\u4e49\uff1a \\begin{aligned} \\text { minimize } & f_{0}(x, \\theta) \\\\ & f(x, \\theta) \\preceq 0 \\\\ & h(x, \\theta)=0 \\end{aligned} \u7efc\u8ff0 (1) \u5bf9\u51f8\u4f18\u5316\u95ee\u9898\u7684\u7279\u6b8a\u60c5\u51b5(\u7ebf\u6027\u6700\u4f18\uff0c\u4e8c\u6b21\u6700\u4f18\uff0c\u6700\u5c0f\u4e8c\u4e58)\u8fdb\u884c\u5206\u7c7b\uff0c\u540c\u65f6\u6307\u51fa\u4e86\u4e13\u7528\u95ee\u9898\u4e13\u7528\u7b97\u6cd5\u4ee5\u53ca\u975e\u7ebf\u6027\u51f8\u4f18\u5316\u666e\u904d\u65b9\u6cd5\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u6307\u51fa\u4e86\u975e\u7ebf\u6027\u51f8\u4f18\u5316\u4e00\u822c\u65b9\u6cd5\u6bd4\u5982\u5185\u70b9\u6cd5\u5728\u6548\u7387\u4e0a\u5f80\u5f80\u4f1a\u548c\u7ebf\u6027\u6216\u4e8c\u6b21\u51f8\u4f18\u5316\u7b97\u6cd5\u76f8\u8fd1\uff0c\u539f\u56e0\u662f\u90fd\u5229\u7528\u4e86\u5c40\u90e8\u7684convexity. Disciplined Convex Programming\u5305\u542b\u4e24\u4e2a\u5185\u5bb9\uff0c\u4e00\u4e2a\u662f\u5c06\u7b26\u5408\u8981\u6c42\u7684\u51f8\u4f18\u5316\u95ee\u9898\u901a\u8fc7\u4e00\u7cfb\u5217\u539f\u5b50\u64cd\u4f5c\u5e76\u8f6c\u6362\u4e3a\u6807\u51c6\u95ee\u9898\uff0c\u53e6\u4e00\u4e2a\u662f\u4e00\u5957\u7528\u4e8e\u5224\u65ad\u7528\u6237\u8f93\u5165\u7684\u95ee\u9898\u662f\u5426\u4e3a\u7b26\u5408\u8981\u6c42\u7684DCP\u7684\u89c4\u5219\u7cfb\u7edf\u3002\u5728\u672c\u6587\u4f5c\u8005\u8be6\u7ec6\u63cf\u8ff0\u4e86DCP\u89c4\u5219\u7cfb\u7edf\u7684\u7b97\u6cd5\uff0c\u5e76\u63d0\u4f9b\u4e86\u51b3\u7b56\u6811\u4f8b\u5b50\u3002\u5728[cvxlayers]\u7684paper (3) \u4e2d\u4ee5\u53ca\u4ee3\u7801\u6587\u6863\u4e2d,\u6211\u4eec\u53ef\u4ee5\u77e5\u6653\u76ee\u524d\u5e93\u4e2d\u652f\u6301\u7684\u539f\u5b50\u64cd\u4f5c(\u7edd\u5927\u591a\u6570\u7684\u5355\u8c03\u51fd\u6570\u4ee5\u53ca\u591a\u9879\u5f0f\u51fd\u6570)","title":"Convex optimization and Disciplined Convex Programming"},{"location":"other_categories/others/SomePapersOnDifferentiableCvxOpt/#kkt-condition-convex-optimization","text":"\u8fd9\u91cc\u91c7\u7528\u7684\u601d\u8def\u5f88\u63a5\u8fd1\u4e0e OptNet ,paper (2) \u5c06\u8fd9\u4e2a\u95ee\u9898\u7684\u89e3\u7b54\u62d3\u5c55\u5230\u8fd1\u4e4e\u4efb\u610f\u51f8\u4f18\u5316\u95ee\u9898\uff0c\u5728\u63a5\u8fd1\u6700\u4f18\u7684\u60c5\u51b5\u4e0b\uff0c\u4f7f\u7528KKT condition\u8fdb\u884c\u53cd\u4f20\u3002 KKT\u6761\u4ef6: \\begin{aligned} f(\\tilde{x}, \\theta) & \\preceq 0 \\\\ h(\\tilde{x}, \\theta) &=0 \\\\ \\tilde{\\lambda}_{i} & \\geq 0, \\quad i=1, \\ldots, m \\\\ \\tilde{\\lambda}_{i} f_{i}(\\tilde{x}, \\theta) &=0, \\quad i=1, \\ldots, m \\\\ \\nabla_{x} L(\\tilde{x}, \\tilde{\\lambda}, \\tilde{\\nu}, \\theta) &=0 \\end{aligned} \u5176\u4e2d \\mathcal{L} \u4e3a\u539f\u51fd\u6570\u52a0\u4e0a\u62c9\u683c\u6717\u65e5\u4e58\u5b50\uff0c\u4e3a: L(x, \\lambda, \\nu, \\theta)=f_{0}(x, \\theta)+\\lambda^{T} f(x, \\theta)+\\nu^{T} h(x, \\theta) \u518d\u5b9a\u4e49 g(z, \\theta)=\\left[\\begin{array}{c} {\\nabla_{x} L(x, \\lambda, \\nu, \\theta)} \\\\ {\\operatorname{diag}(\\lambda) f(x, \\theta)} \\\\ {h(x, \\theta)} \\end{array}\\right] \u4f5c\u8005\u6700\u7ec8\u7ed9\u51fa\u7684\u5bfc\u6570\u516c\u5f0f \\mathrm{D}_{z} g(\\tilde{z}, \\theta)=\\left[\\begin{array}{ccc} {\\mathrm{D}_{x} \\nabla_{x} L(\\tilde{x}, \\tilde{\\lambda}, \\tilde{\\nu}, \\theta)} & {\\mathrm{D}_{x} f(\\tilde{x}, \\theta)^{T}} & {\\mathrm{D}_{x} h(\\tilde{x}, \\theta)^{T}} \\\\ {\\operatorname{diag}(\\tilde{\\lambda}) \\mathrm{D}_{x} f(\\tilde{x}, \\theta)} & {\\operatorname{diag}(f(\\tilde{x}, \\theta))} & {0} \\\\ {\\mathrm{D}_{x} h(\\tilde{x}, \\theta)} & {0} & {0} \\end{array}\\right] \\mathrm{D}_{\\theta} g(\\tilde{z}, \\theta)=\\left[\\begin{array}{c} {\\mathrm{D}_{\\theta} \\nabla_{x} L(\\tilde{x}, \\tilde{\\lambda}, \\tilde{\\nu}, \\theta)} \\\\ {\\operatorname{diag}(\\tilde{\\lambda}) \\mathrm{D}_{\\theta} f(\\tilde{x}, \\theta)} \\\\ {\\mathrm{D}_{\\theta} h(\\tilde{x}, \\theta)} \\end{array}\\right] \\mathrm{D}_{\\theta} s(\\theta)=-\\mathrm{D}_{z} g(\\tilde{x}, \\tilde{\\lambda}, \\tilde{\\nu}, \\theta)^{-1} \\mathrm{D}_{\\theta} g(\\tilde{x}, \\tilde{\\lambda}, \\tilde{\\nu}, \\theta) \\text { for every } \\theta \\in Q \u8003\u8651\u4e8c\u6b21\u89c4\u5212\u4f5c\u4e3a\u672c\u6587\u7684\u7279\u6b8a\u60c5\u51b5\uff0c\u5e26\u5165\u516c\u5f0f\u6709 \\mathrm{D}_{x} g(\\tilde{x}, \\tilde{\\lambda}, \\tilde{\\nu}, \\theta)=\\left[\\begin{array}{ccc} {Q} & {G^{T}} & {A^{T}} \\\\ {\\operatorname{diag}(\\tilde{\\lambda}) G} & {\\operatorname{diag}(G \\tilde{x}-h)} & {0} \\\\ {A} & {0} & {0} \\end{array}\\right] \\mathrm{D}_{\\theta} g(\\dot{x}, \\tilde{\\lambda}, \\tilde{\\nu}, \\theta)=\\left[\\begin{array}{c} {\\mathrm{d} Q \\tilde{x}+\\mathrm{D}_{\\theta} q+\\mathrm{d} G^{T} \\tilde{\\lambda}+\\mathrm{d} A^{T} \\tilde{\\nu}} \\\\ {\\operatorname{diag}(\\lambda)\\left(\\mathrm{d} G \\tilde{x}-\\mathrm{D}_{\\theta} h\\right)} \\\\ {\\mathrm{d} A \\tilde{x}-\\mathrm{D}_{\\theta} b} \\end{array}\\right] \u6700\u7ec8\u7684\u516c\u5f0f\u4e0e OptNet \u662f\u4e00\u81f4\u7684","title":"\u57fa\u4e8eKKT condition\u7684 Convex optimization \u5bfc\u6570\u4f20\u9012"},{"location":"other_categories/others/SomePapersOnDifferentiableCvxOpt/#convex-optimization-as-a-differentiable-learnable-layer","text":"\u57282019 NeurIPS\u7684paper (3) \u4e2d, \u4f5c\u8005\u5c06\u521d\u59cb\u89e3 x_0 ,\u6700\u4f18\u5316\u95ee\u9898\u53c2\u6570 \\theta ,\u6700\u7ec8\u8f93\u51fa s \u7406\u89e3\u4e3a\u795e\u7ecf\u7f51\u7edc\u5c42\u7684\u8f93\u5165\u3001\u53c2\u6570\u4ee5\u53ca\u8f93\u51fa\u3002\u8fdb\u4e00\u6b65\u5730\uff0c\u4f5c\u8005\u7cc5\u5408\u524d\u4e24\u4e2a\u7ae0\u8282\u7684idea,\u4f7f\u7528 Disciplined Convex Programming\u4f7f\u5f97\u7528\u6237\u5728\u8bbe\u8ba1\u4f18\u5316\u5c42\u7684\u65f6\u5019\u6709\u66f4\u9ad8\u7684\u81ea\u7531\u5ea6(\u4e0d\u518d\u53ea\u5c40\u9650\u4e8e\u4e00\u6b21\u6216\u4e8c\u6b21\u578b\u7b49\u7279\u6027)\uff0c\u8f93\u5165\u4e0e\u53c2\u6570\u5728\u5f62\u6210\u6210\u672c\u51fd\u6570\u4e0e\u7ea6\u675f\u7684\u8ba1\u7b97\u8fc7\u7a0b\u4e2d\u5141\u8bb8\u6309\u7167DCP\u89c4\u5219\u94fe\u63a5\u5927\u91cf\u7684\u539f\u5b50\u64cd\u4f5c\uff0c\u5f62\u6210\u66f4\u4e3a\u590d\u6742\u800c\u81ea\u7136\u7684\"\u4e00\u822c\"\u4f18\u5316\u95ee\u9898\u8f93\u5165(\u6ce8: cvxpylayers \u867d\u7136\u652f\u6301\u5f88\u591a\u522b\u7684\u51fd\u6570\uff0c\u4f46\u662f\u4e0d\u652f\u6301\u6b63\u4f59\u5f26\u51fd\u6570)\u3002\u4f7f\u7528KKT\u5f53\u4f18\u5316\u7ed3\u679c\u6536\u655b\u65f6\u5feb\u901f\u5730\u5b9e\u73b0\u53cd\u5411\u4f20\u64ad\u3002 \u4f5c\u8005\u5728 (3) \u7684\u9644\u5f55\u4e2d\u7ed9\u51fa\u4e86\u4f7f\u7528cvxpylayers\u7528\u6700\u4f18\u5316\u95ee\u9898\u5b9e\u73b0ReLU,Softmax\uff0cQP\u7b49\u95ee\u9898\u7684\u4ee3\u7801\u3002 cvxpylayers \u7684\u4ee3\u7801\u540c\u6837\u4e5f\u6709\u5f88\u591a\u7684\u4f8b\u7a0b,\u5728RL\u3001\u63a7\u5236\u3001\u7f51\u7edc\u540e\u4f18\u5316\u9886\u57df\u6709\u8f83\u5f3a\u7684\u4f7f\u7528\u7a7a\u95f4\u3002","title":"Convex Optimization as a Differentiable learnable layer"},{"location":"other_categories/others/TRPO/","text":"Trust Region Policy Optimization \u8fd9\u7bc7\u8bba\u6587\u662f\u7ecf\u5178\u7684\u5f3a\u5316\u5b66\u4e60\u8bba\u6587\uff0c\u76f4\u89c9\u7684motivation\u6765\u8bf4\u5c31\u662f\u5728\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u8fc7\u7a0b\u4e2d\u566a\u97f3\u5f88\u5927\uff0c\u5f88\u5bb9\u6613\u5d29\u6e83\uff0c\u6240\u4ee5\u8fd9\u91cc\u4f1a\u5c1d\u8bd5\u9650\u5236\u4e00\u4e2aTrust Region\uff0c\u6a21\u4eff\u76f8\u5173\u4f18\u5316\u95ee\u9898\u7684\u601d\u8def\u8fdb\u884c\u63a7\u5236\u3002\u5176\u5b9e\u672c\u6587\u540e\u7eed\u6709\u4e00\u7bc7implementation\u66f4intuitive\u7684 PPO . \u672c\u6587\u5728Spinningup\u4e0a\u6709\u5f88\u4f18\u79c0\u7684 \u5b98\u65b9\u4ecb\u7ecd \uff0c\u672c\u9875\u4e3b\u8981\u901a\u8fc7\u4e2d\u6587\u7ffb\u8bd1+\u7ed3\u5408\u8bba\u6587\u4e0e\u5176\u4ed6\u8d44\u6599\u8fdb\u884c\u8865\u5145\u3002 \u672c\u6587\u6709\u5bf9\u5e94\u7684pytorch\u5f00\u6e90\u4ee3\u7801, \u94fe\u63a5 \u6570\u5b66\u4e0a\u7684Motivation \u4e3a\u4ec0\u4e48\u5f3a\u5316\u5b66\u4e60\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u566a\u97f3\u8fd9\u4e48\u4e25\u91cd\uff1f\u6570\u5b66\u4e0a\u53ef\u4ee5\u5f52\u7eb3\u4e3a\u4ee5\u4e0b\u7684\u516c\u5f0f\u8bf4\u660e\u3002\u5bf9\u4e8e\u5f53\u524d\u4ee5\u53ca\u6700\u4f18\u7684policy\u4e4b\u95f4\u7684expected cost\u7684\u5dee\u522b\uff0c\u51c6\u786e\u7684\u516c\u5f0f\u662f: \\eta(\\tilde\\pi) = \\eta(\\pi) + \\sum_s\\rho_{\\tilde\\pi}(s)\\sum_s\\tilde\\pi(a|s)A_\\pi(s,a) \u4f46\u662f\u7531\u4e8e\u6ca1\u6709\u529e\u6cd5\u7528optimal policy\u8fdb\u884csample.\u6240\u4ee5\u4e00\u822c\u5b9e\u9645\u64cd\u4f5c\u7684\u65f6\u5019\u53ea\u80fd\u7528\u5f53\u524d\u7684policy\u5bf9\u8def\u5f84\u8fdb\u884c\u91c7\u6837\uff0c\u5f97\u5230 L_\\pi(\\tilde\\pi) = \\eta(\\pi) + \\sum_s\\rho_{\\pi}(s)\\sum_s\\tilde\\pi(a|s)A_\\pi(s,a) \u7b97\u6cd5\u6570\u5b66\u63cf\u8ff0 \u4f18\u5316\u95ee\u9898\u63cf\u8ff0: \\begin{aligned} \\theta_{k+1} = &\\argmax_\\theta \\mathcal{L}(\\theta_k,\\theta) \\\\ &s.t. D_{KL}(\\theta||\\theta_k) \\le \\delta \\end{aligned} \u5176\u4e2d \\mathcal{L}(\\theta_k,\\theta) \u542b\u4e49\u4e0e\u524d\u6587\u57fa\u672c\u4e00\u81f4\uff0c\u5177\u4f53\u6765\u8bf4\u662f \\mathcal{L}(\\theta_k,\\theta) = E_{s,a~\\pi_{\\theta_k}}[\\frac{\\pi_\\theta(a|s)}{\\pi_{\\theta_k}(a|s)}A^{\\pi_{\\theta_k}}(s,a)] D_{KL}(\\theta||\\theta_k) = E_{s~\\pi_{\\theta_k}}[D_{KL}(\\pi_\\theta(\u00b7|s) ||\\pi_{\\theta_k}(\u00b7|s) )] \u5176\u4e2dKL divergence\u7684\u5b9a\u4e49\u8bf7\u67e5\u9605 wiki D_{KL}(P||Q) = \\sum_P P(x) ln(\\frac{P(x)}{Q(x)}) \u5bf9\u4e0a\u9762\u7684\u95ee\u9898\u53d6\u6cf0\u52d2\u8fd1\u4f3c\uff0c\u4f18\u5316\u95ee\u9898\u53d8\u4e3a\u4e00\u4e2a\u5e26\u7ea6\u675f\u7684\u4e8c\u6b21\u65b9\u7a0b\uff0c\u8bbe g \u4e3a L \u5173\u4e8e\u7f51\u7edc\u53c2\u6570 \\theta \u7684\u68af\u5ea6. \\begin{aligned} \\theta_{k+1} = \\argmax_\\theta &g^T(\\theta - \\theta_k) \\\\ s.t. &\\frac{1}{2}(\\theta - \\theta_k)^TH(\\theta - \\theta_k) \\le\\delta \\end{aligned} \u6839\u636e\u51f8\u4f18\u5316\u7684\u6700\u4f18\u89e3\u662f \\theta_{k+1} = \\theta_k + \\sqrt{\\frac{2\\delta}{g^TH^{-1}g}}H^{-1}g \u5bf9\u4e8e\u53c2\u6570\u6570\u91cf\u5f88\u5927\u7684 \\theta \u7f51\u7edc\uff0chessian\u77e9\u9635\u7684\u8ba1\u7b97\uff0c\u5c24\u5176\u662f\u9006\u7684\u77e9\u9635\u8fd0\u7b97\u91cf\u5f88\u5927\uff0c\u8fd9\u91cc\u4f7f\u7528\u7684\u662f conjugate gradient\u7b97\u6cd5 \uff0c\u8fd9\u662f\u4e00\u4e2a\u6c42\u89e3\u5bf9\u79f0\u77e9\u9635\u7684\u9006\u6216\u8005\u6c42\u89e3\u7a00\u758f\u7ebf\u6027\u65b9\u7a0b\u7684\u8fed\u4ee3\u7b97\u6cd5\u3002 \u4e0b\u6587\u6765\u81eawiki\u94fe\u63a5\u4e2d\u7684\u662f\u6c42\u89e3\u7ebf\u6027\u65b9\u7a0b Ax = b \u7684\u7b97\u6cd5 \u6700\u540eopenai spinningup\u603b\u7ed3\u4e86\u7b97\u6cd5\u5982\u4e0b\u56fe \u8865\u5145\u63cf\u8ff0\uff1a \u5173\u4e8estep 9 line search: \u5c31\u662f\u9010\u4e2a\u67e5\u770b\u9009\u62e9\u5b9e\u9645\u80fd\u53d6\u7684\u6700\u957f\u7684\u6b65\u957f(\u56e0\u4e3a\u524d\u9762\u6709\u505a\u4e8c\u6b21\u8fd1\u4f3c) \u5173\u4e8estep 10: \u5e38\u89c4policy gradient \u7b97\u6cd5\u4e2d\u5bf9value\u7f51\u7edc\u7684\u62df\u5408.","title":"Trust Region Policy Optimization"},{"location":"other_categories/others/TRPO/#trust-region-policy-optimization","text":"\u8fd9\u7bc7\u8bba\u6587\u662f\u7ecf\u5178\u7684\u5f3a\u5316\u5b66\u4e60\u8bba\u6587\uff0c\u76f4\u89c9\u7684motivation\u6765\u8bf4\u5c31\u662f\u5728\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u8fc7\u7a0b\u4e2d\u566a\u97f3\u5f88\u5927\uff0c\u5f88\u5bb9\u6613\u5d29\u6e83\uff0c\u6240\u4ee5\u8fd9\u91cc\u4f1a\u5c1d\u8bd5\u9650\u5236\u4e00\u4e2aTrust Region\uff0c\u6a21\u4eff\u76f8\u5173\u4f18\u5316\u95ee\u9898\u7684\u601d\u8def\u8fdb\u884c\u63a7\u5236\u3002\u5176\u5b9e\u672c\u6587\u540e\u7eed\u6709\u4e00\u7bc7implementation\u66f4intuitive\u7684 PPO . \u672c\u6587\u5728Spinningup\u4e0a\u6709\u5f88\u4f18\u79c0\u7684 \u5b98\u65b9\u4ecb\u7ecd \uff0c\u672c\u9875\u4e3b\u8981\u901a\u8fc7\u4e2d\u6587\u7ffb\u8bd1+\u7ed3\u5408\u8bba\u6587\u4e0e\u5176\u4ed6\u8d44\u6599\u8fdb\u884c\u8865\u5145\u3002 \u672c\u6587\u6709\u5bf9\u5e94\u7684pytorch\u5f00\u6e90\u4ee3\u7801, \u94fe\u63a5","title":"Trust Region Policy Optimization"},{"location":"other_categories/others/TRPO/#motivation","text":"\u4e3a\u4ec0\u4e48\u5f3a\u5316\u5b66\u4e60\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u566a\u97f3\u8fd9\u4e48\u4e25\u91cd\uff1f\u6570\u5b66\u4e0a\u53ef\u4ee5\u5f52\u7eb3\u4e3a\u4ee5\u4e0b\u7684\u516c\u5f0f\u8bf4\u660e\u3002\u5bf9\u4e8e\u5f53\u524d\u4ee5\u53ca\u6700\u4f18\u7684policy\u4e4b\u95f4\u7684expected cost\u7684\u5dee\u522b\uff0c\u51c6\u786e\u7684\u516c\u5f0f\u662f: \\eta(\\tilde\\pi) = \\eta(\\pi) + \\sum_s\\rho_{\\tilde\\pi}(s)\\sum_s\\tilde\\pi(a|s)A_\\pi(s,a) \u4f46\u662f\u7531\u4e8e\u6ca1\u6709\u529e\u6cd5\u7528optimal policy\u8fdb\u884csample.\u6240\u4ee5\u4e00\u822c\u5b9e\u9645\u64cd\u4f5c\u7684\u65f6\u5019\u53ea\u80fd\u7528\u5f53\u524d\u7684policy\u5bf9\u8def\u5f84\u8fdb\u884c\u91c7\u6837\uff0c\u5f97\u5230 L_\\pi(\\tilde\\pi) = \\eta(\\pi) + \\sum_s\\rho_{\\pi}(s)\\sum_s\\tilde\\pi(a|s)A_\\pi(s,a)","title":"\u6570\u5b66\u4e0a\u7684Motivation"},{"location":"other_categories/others/TRPO/#_1","text":"\u4f18\u5316\u95ee\u9898\u63cf\u8ff0: \\begin{aligned} \\theta_{k+1} = &\\argmax_\\theta \\mathcal{L}(\\theta_k,\\theta) \\\\ &s.t. D_{KL}(\\theta||\\theta_k) \\le \\delta \\end{aligned} \u5176\u4e2d \\mathcal{L}(\\theta_k,\\theta) \u542b\u4e49\u4e0e\u524d\u6587\u57fa\u672c\u4e00\u81f4\uff0c\u5177\u4f53\u6765\u8bf4\u662f \\mathcal{L}(\\theta_k,\\theta) = E_{s,a~\\pi_{\\theta_k}}[\\frac{\\pi_\\theta(a|s)}{\\pi_{\\theta_k}(a|s)}A^{\\pi_{\\theta_k}}(s,a)] D_{KL}(\\theta||\\theta_k) = E_{s~\\pi_{\\theta_k}}[D_{KL}(\\pi_\\theta(\u00b7|s) ||\\pi_{\\theta_k}(\u00b7|s) )] \u5176\u4e2dKL divergence\u7684\u5b9a\u4e49\u8bf7\u67e5\u9605 wiki D_{KL}(P||Q) = \\sum_P P(x) ln(\\frac{P(x)}{Q(x)}) \u5bf9\u4e0a\u9762\u7684\u95ee\u9898\u53d6\u6cf0\u52d2\u8fd1\u4f3c\uff0c\u4f18\u5316\u95ee\u9898\u53d8\u4e3a\u4e00\u4e2a\u5e26\u7ea6\u675f\u7684\u4e8c\u6b21\u65b9\u7a0b\uff0c\u8bbe g \u4e3a L \u5173\u4e8e\u7f51\u7edc\u53c2\u6570 \\theta \u7684\u68af\u5ea6. \\begin{aligned} \\theta_{k+1} = \\argmax_\\theta &g^T(\\theta - \\theta_k) \\\\ s.t. &\\frac{1}{2}(\\theta - \\theta_k)^TH(\\theta - \\theta_k) \\le\\delta \\end{aligned} \u6839\u636e\u51f8\u4f18\u5316\u7684\u6700\u4f18\u89e3\u662f \\theta_{k+1} = \\theta_k + \\sqrt{\\frac{2\\delta}{g^TH^{-1}g}}H^{-1}g \u5bf9\u4e8e\u53c2\u6570\u6570\u91cf\u5f88\u5927\u7684 \\theta \u7f51\u7edc\uff0chessian\u77e9\u9635\u7684\u8ba1\u7b97\uff0c\u5c24\u5176\u662f\u9006\u7684\u77e9\u9635\u8fd0\u7b97\u91cf\u5f88\u5927\uff0c\u8fd9\u91cc\u4f7f\u7528\u7684\u662f conjugate gradient\u7b97\u6cd5 \uff0c\u8fd9\u662f\u4e00\u4e2a\u6c42\u89e3\u5bf9\u79f0\u77e9\u9635\u7684\u9006\u6216\u8005\u6c42\u89e3\u7a00\u758f\u7ebf\u6027\u65b9\u7a0b\u7684\u8fed\u4ee3\u7b97\u6cd5\u3002 \u4e0b\u6587\u6765\u81eawiki\u94fe\u63a5\u4e2d\u7684\u662f\u6c42\u89e3\u7ebf\u6027\u65b9\u7a0b Ax = b \u7684\u7b97\u6cd5 \u6700\u540eopenai spinningup\u603b\u7ed3\u4e86\u7b97\u6cd5\u5982\u4e0b\u56fe","title":"\u7b97\u6cd5\u6570\u5b66\u63cf\u8ff0"},{"location":"other_categories/others/TRPO/#_2","text":"\u5173\u4e8estep 9 line search: \u5c31\u662f\u9010\u4e2a\u67e5\u770b\u9009\u62e9\u5b9e\u9645\u80fd\u53d6\u7684\u6700\u957f\u7684\u6b65\u957f(\u56e0\u4e3a\u524d\u9762\u6709\u505a\u4e8c\u6b21\u8fd1\u4f3c) \u5173\u4e8estep 10: \u5e38\u89c4policy gradient \u7b97\u6cd5\u4e2d\u5bf9value\u7f51\u7edc\u7684\u62df\u5408.","title":"\u8865\u5145\u63cf\u8ff0\uff1a"},{"location":"other_categories/others/Unsupervised_depth_prediction/","text":"Unsupervised Monocular Depth Estimation with Left-Right Consistency \u8fd9\u7bc7\u8bba\u6587\u89e3\u51b3\u7684\u95ee\u9898\u662f\u65e0\u76d1\u7763\u6761\u4ef6\u4e0b\uff0c\u4ec5\u4f7f\u7528\u5355\u4e2a\u76f8\u673a\u5355\u6b21\u7684\u62cd\u6444\u56fe\uff0c\u5f97\u5230\u5bf9\u6df1\u5ea6\u7684\u4f30\u8ba1\u3002\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u9700\u8981\u7b2c\u4e8c\u4e2a\u76f8\u673a\u540c\u65f6\u62cd\u6444\u7684\u7ed3\u679c\u6765\u6784\u9020\u635f\u5931\u51fd\u6570\u7528\u4e8e\u8bad\u7ec3\u3002\u53ef\u4ee5\u7406\u89e3\u4e3a\u662f\u4f7f\u7528\u7b2c\u4e8c\u4e2a\u76f8\u673a\u4f5c\u4e3a\u76d1\u7763\u8f93\u5165\uff0c\u4f46\u662f\u8fd9\u4e5f\u8282\u7701\u4e86\u6570\u636e\u6807\u5b9a\u7684\u96be\u5ea6\u3002\u672c\u6587\u540c\u65f6\u6709\u975e\u5b98\u65b9\u7684 pytorch\u5b9e\u73b0 inference\u7ed3\u6784 \u672c\u6587\u795e\u7ecf\u7f51\u7edc\u7684\u8f93\u5165\u662f\u5355\u5f20RGB\u56fe\u7247\uff0c\u8f93\u51fa\u662fmulti-scale\u7684disparity,\u5e76\u4e14\u5728\u6bcf\u4e2ascale\u662f\u540c\u65f6\u8f93\u51fa\u5de6\u56fe->\u53f3\u56fe\u7684disparity\u4ee5\u53ca\u53f3\u56fe->\u5de6\u56fe\u7684disparity.\u4f5c\u8005\u7684\u601d\u8def\u662f\u5728\u591a\u4e2ascale\u4e0a\u540c\u65f6\u5bf9\u4e24\u4e2adisparity\u68c0\u6d4b\u7684\u6b63\u786e\u6027\u3001\u7edf\u4e00\u6027\u8fdb\u884c\u68c0\u9a8c\u3002 Loss\u7ed3\u6784 \u5b9a\u4e49\u5728scale s \u4e0a\u7684\u635f\u5931\u503c\u4e3a C_s , \u635f\u5931\u51fd\u6570\u4e3a C_{s}=\\alpha_{a p}\\left(C_{a p}^{l}+C_{a p}^{r}\\right)+\\alpha_{d s}\\left(C_{d s}^{l}+C_{d s}^{r}\\right)+\\alpha_{l r}\\left(C_{l r}^{l}+C_{l r}^{r}\\right) \u91cd\u6784\u635f\u5931 C_{ap} \u6307\u7684\u662f\u5de6\u53f3\u56fe\u76f8\u4e92\u91cd\u6784\u65f6\u7684\u8bef\u5dee\u635f\u5931\uff0c\u672c\u6587\u540c\u65f6\u91c7\u7528naive\u7684 L1 \u8ddd\u79bb\u4ee5\u53ca SSIM\u8ddd\u79bb \u7684\u52a0\u6743\u6c42\u548c\uff0c\u4ee3\u7801\u4e2d\u53ef\u4ee5\u6e05\u695a\u5730\u7559\u610f\u5230\u4f5c\u8005\u662f\u5982\u4f55\u4f7f\u7528Pytorch\u539f\u751f\u5c42\u4ee5\u53ca\u57fa\u672c\u64cd\u4f5c\u5b9e\u73b0SSIM\u7684\u8ba1\u7b97\uff0c\u5e76\u4e14\u5141\u8bb8\u53cd\u4f20\u3002 C_{a p}^{l}=\\frac{1}{N} \\sum_{i, j} \\alpha \\frac{1-\\operatorname{SSIM}\\left(I_{i j}^{l}, \\tilde{I}_{i j}^{l}\\right)}{2}+(1-\\alpha)\\left\\|I_{i j}^{l}-\\tilde{I}_{i j}^{l}\\right\\| def SSIM(self, x, y): C1 = 0.01 ** 2 C2 = 0.03 ** 2 mu_x = nn.AvgPool2d(3, 1)(x) mu_y = nn.AvgPool2d(3, 1)(y) mu_x_mu_y = mu_x * mu_y mu_x_sq = mu_x.pow(2) mu_y_sq = mu_y.pow(2) sigma_x = nn.AvgPool2d(3, 1)(x * x) - mu_x_sq sigma_y = nn.AvgPool2d(3, 1)(y * y) - mu_y_sq sigma_xy = nn.AvgPool2d(3, 1)(x * y) - mu_x_mu_y SSIM_n = (2 * mu_x_mu_y + C1) * (2 * sigma_xy + C2) SSIM_d = (mu_x_sq + mu_y_sq + C1) * (sigma_x + sigma_y + C2) SSIM = SSIM_n / SSIM_d return torch.clamp((1 - SSIM) / 2, 0, 1) \u5149\u6ed1\u635f\u5931 C_{ds} \u4ee3\u8868\u7684\u662fdisparity-smoothness loss\uff0c\u8fd9\u91cc\u7684\u505a\u6cd5\u662f\u5bf9disparity map\u7684\u68af\u5ea6\u8fdb\u884c\u60e9\u7f5a. C_{d s}^{l}=\\frac{1}{N} \\sum_{i, j}\\left|\\partial_{x} d_{i j}^{l}\\right| e^{-\\left\\|\\partial_{x} I_{i j}^{l}\\right\\|}+\\left|\\partial_{y} d_{i j}^{l}\\right| e^{-\\left\\|\\partial_{y} I_{i j}^{l}\\right\\|} \u5de6\u53f3\u5dee\u503c\u635f\u5931 C_{lr} \u7528\u6765\u8868\u5f81\u4e24\u4e2adisparity map\u7684\u81ea\u6d3d\u6027\u3002\u6839\u636e\u5de6\u56fe\u7684disparity\uff0c\u5c06\u5de6\u56fe\u7684\u70b9\u6295\u5230\u53f3\u56fe\uff0c\u8fd9\u4e24\u4e2a\u5bf9\u5e94\u70b9\u7684disparity\u5e94\u8be5\u662f\u4e00\u81f4\u7684\u3002 C_{l r}^{l}=\\frac{1}{N} \\sum_{i, j}\\left|d_{i j}^{l}-d_{i j+d_{i j}^{l}}^{r}\\right| Deep Depth Estimation from Visual-Inertial SLAM pdf \u8fd9\u7bc7\u6587\u7ae0\u662f\u76d1\u7763\u5b66\u4e60\u7684\uff0c\u4f7f\u7528\u5e73\u9762\u5047\u8bbe\u53bb\u589e\u5f3aSLAM\u7684\u6df1\u5ea6\u5bc6\u5ea6. Self-Supervised Sparse-to-Dense: Self-Supervised Depth Completion from LiDAR and Monocular Camera pdf code \u8865\u5168\u662f\u57fa\u4e8esparse lidar\u8fdb\u884c\u8865\u5168.","title":"Unsupervised Monocular Depth Estimation with Left-Right Consistency"},{"location":"other_categories/others/Unsupervised_depth_prediction/#unsupervised-monocular-depth-estimation-with-left-right-consistency","text":"\u8fd9\u7bc7\u8bba\u6587\u89e3\u51b3\u7684\u95ee\u9898\u662f\u65e0\u76d1\u7763\u6761\u4ef6\u4e0b\uff0c\u4ec5\u4f7f\u7528\u5355\u4e2a\u76f8\u673a\u5355\u6b21\u7684\u62cd\u6444\u56fe\uff0c\u5f97\u5230\u5bf9\u6df1\u5ea6\u7684\u4f30\u8ba1\u3002\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u9700\u8981\u7b2c\u4e8c\u4e2a\u76f8\u673a\u540c\u65f6\u62cd\u6444\u7684\u7ed3\u679c\u6765\u6784\u9020\u635f\u5931\u51fd\u6570\u7528\u4e8e\u8bad\u7ec3\u3002\u53ef\u4ee5\u7406\u89e3\u4e3a\u662f\u4f7f\u7528\u7b2c\u4e8c\u4e2a\u76f8\u673a\u4f5c\u4e3a\u76d1\u7763\u8f93\u5165\uff0c\u4f46\u662f\u8fd9\u4e5f\u8282\u7701\u4e86\u6570\u636e\u6807\u5b9a\u7684\u96be\u5ea6\u3002\u672c\u6587\u540c\u65f6\u6709\u975e\u5b98\u65b9\u7684 pytorch\u5b9e\u73b0","title":"Unsupervised Monocular Depth Estimation with Left-Right Consistency"},{"location":"other_categories/others/Unsupervised_depth_prediction/#inference","text":"\u672c\u6587\u795e\u7ecf\u7f51\u7edc\u7684\u8f93\u5165\u662f\u5355\u5f20RGB\u56fe\u7247\uff0c\u8f93\u51fa\u662fmulti-scale\u7684disparity,\u5e76\u4e14\u5728\u6bcf\u4e2ascale\u662f\u540c\u65f6\u8f93\u51fa\u5de6\u56fe->\u53f3\u56fe\u7684disparity\u4ee5\u53ca\u53f3\u56fe->\u5de6\u56fe\u7684disparity.\u4f5c\u8005\u7684\u601d\u8def\u662f\u5728\u591a\u4e2ascale\u4e0a\u540c\u65f6\u5bf9\u4e24\u4e2adisparity\u68c0\u6d4b\u7684\u6b63\u786e\u6027\u3001\u7edf\u4e00\u6027\u8fdb\u884c\u68c0\u9a8c\u3002","title":"inference\u7ed3\u6784"},{"location":"other_categories/others/Unsupervised_depth_prediction/#loss","text":"\u5b9a\u4e49\u5728scale s \u4e0a\u7684\u635f\u5931\u503c\u4e3a C_s , \u635f\u5931\u51fd\u6570\u4e3a C_{s}=\\alpha_{a p}\\left(C_{a p}^{l}+C_{a p}^{r}\\right)+\\alpha_{d s}\\left(C_{d s}^{l}+C_{d s}^{r}\\right)+\\alpha_{l r}\\left(C_{l r}^{l}+C_{l r}^{r}\\right)","title":"Loss\u7ed3\u6784"},{"location":"other_categories/others/Unsupervised_depth_prediction/#_1","text":"C_{ap} \u6307\u7684\u662f\u5de6\u53f3\u56fe\u76f8\u4e92\u91cd\u6784\u65f6\u7684\u8bef\u5dee\u635f\u5931\uff0c\u672c\u6587\u540c\u65f6\u91c7\u7528naive\u7684 L1 \u8ddd\u79bb\u4ee5\u53ca SSIM\u8ddd\u79bb \u7684\u52a0\u6743\u6c42\u548c\uff0c\u4ee3\u7801\u4e2d\u53ef\u4ee5\u6e05\u695a\u5730\u7559\u610f\u5230\u4f5c\u8005\u662f\u5982\u4f55\u4f7f\u7528Pytorch\u539f\u751f\u5c42\u4ee5\u53ca\u57fa\u672c\u64cd\u4f5c\u5b9e\u73b0SSIM\u7684\u8ba1\u7b97\uff0c\u5e76\u4e14\u5141\u8bb8\u53cd\u4f20\u3002 C_{a p}^{l}=\\frac{1}{N} \\sum_{i, j} \\alpha \\frac{1-\\operatorname{SSIM}\\left(I_{i j}^{l}, \\tilde{I}_{i j}^{l}\\right)}{2}+(1-\\alpha)\\left\\|I_{i j}^{l}-\\tilde{I}_{i j}^{l}\\right\\| def SSIM(self, x, y): C1 = 0.01 ** 2 C2 = 0.03 ** 2 mu_x = nn.AvgPool2d(3, 1)(x) mu_y = nn.AvgPool2d(3, 1)(y) mu_x_mu_y = mu_x * mu_y mu_x_sq = mu_x.pow(2) mu_y_sq = mu_y.pow(2) sigma_x = nn.AvgPool2d(3, 1)(x * x) - mu_x_sq sigma_y = nn.AvgPool2d(3, 1)(y * y) - mu_y_sq sigma_xy = nn.AvgPool2d(3, 1)(x * y) - mu_x_mu_y SSIM_n = (2 * mu_x_mu_y + C1) * (2 * sigma_xy + C2) SSIM_d = (mu_x_sq + mu_y_sq + C1) * (sigma_x + sigma_y + C2) SSIM = SSIM_n / SSIM_d return torch.clamp((1 - SSIM) / 2, 0, 1)","title":"\u91cd\u6784\u635f\u5931"},{"location":"other_categories/others/Unsupervised_depth_prediction/#_2","text":"C_{ds} \u4ee3\u8868\u7684\u662fdisparity-smoothness loss\uff0c\u8fd9\u91cc\u7684\u505a\u6cd5\u662f\u5bf9disparity map\u7684\u68af\u5ea6\u8fdb\u884c\u60e9\u7f5a. C_{d s}^{l}=\\frac{1}{N} \\sum_{i, j}\\left|\\partial_{x} d_{i j}^{l}\\right| e^{-\\left\\|\\partial_{x} I_{i j}^{l}\\right\\|}+\\left|\\partial_{y} d_{i j}^{l}\\right| e^{-\\left\\|\\partial_{y} I_{i j}^{l}\\right\\|}","title":"\u5149\u6ed1\u635f\u5931"},{"location":"other_categories/others/Unsupervised_depth_prediction/#_3","text":"C_{lr} \u7528\u6765\u8868\u5f81\u4e24\u4e2adisparity map\u7684\u81ea\u6d3d\u6027\u3002\u6839\u636e\u5de6\u56fe\u7684disparity\uff0c\u5c06\u5de6\u56fe\u7684\u70b9\u6295\u5230\u53f3\u56fe\uff0c\u8fd9\u4e24\u4e2a\u5bf9\u5e94\u70b9\u7684disparity\u5e94\u8be5\u662f\u4e00\u81f4\u7684\u3002 C_{l r}^{l}=\\frac{1}{N} \\sum_{i, j}\\left|d_{i j}^{l}-d_{i j+d_{i j}^{l}}^{r}\\right|","title":"\u5de6\u53f3\u5dee\u503c\u635f\u5931"},{"location":"other_categories/others/Unsupervised_depth_prediction/#deep-depth-estimation-from-visual-inertial-slam","text":"pdf \u8fd9\u7bc7\u6587\u7ae0\u662f\u76d1\u7763\u5b66\u4e60\u7684\uff0c\u4f7f\u7528\u5e73\u9762\u5047\u8bbe\u53bb\u589e\u5f3aSLAM\u7684\u6df1\u5ea6\u5bc6\u5ea6.","title":"Deep Depth Estimation from Visual-Inertial SLAM"},{"location":"other_categories/others/Unsupervised_depth_prediction/#self-supervised-sparse-to-dense-self-supervised-depth-completion-from-lidar-and-monocular-camera","text":"pdf code \u8865\u5168\u662f\u57fa\u4e8esparse lidar\u8fdb\u884c\u8865\u5168.","title":"Self-Supervised Sparse-to-Dense: Self-Supervised Depth Completion from LiDAR and Monocular Camera"},{"location":"other_categories/others/adversarialPatch/","text":"Adversarial Patch \u8fd9\u7bc7paper\u505a\u5de5\u4f5c\u5982\u56fe. \u5176\u4e0e\u4e00\u822c\u7684\u5bf9\u6297\u6027\u653b\u51fb\u4e0d\u540c\u7684\u5730\u65b9\u5728\u4e8e\uff1a 1. \u7406\u8bba\u4e0a\u6765\u8bf4\u53ef\u4ee5\u5bf9\u6297\u5728ImageNet\u4e0a\u8bad\u7ec3\u7684\u4e0d\u540cmodel\u7684\u6240\u6709image\u3002 2. \u53ef\u4ee5\u6253\u5370\u51fa\u6765\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u653e\u7f6e\u3002\u5f62\u6210\u5b9e\u65f6\u653b\u51fb Approach \u65b9\u6cd5\u9700\u6c42, \u4e00\u4e2aimage patch(\u6bd4\u5982\u672c\u6587\u7684\u4e00\u5c0f\u4e2a toaster), \u4e00\u4e2a\u6216\u591a\u4e2a\u5df2\u7ecf\u8bad\u7ec3\u597d\u7684\u5206\u7c7b\u7f51\u7edc\uff0c\u4e00\u4e2a\u5206\u7c7b\u6570\u636e\u96c6(\u4e0d\u9700\u8981target)\uff0c\u4e00\u4e2a\u76ee\u6807\u7c7b\u522b(\u8981\u6c42\u88ab\u653b\u51fb\u7684\u7f51\u7edc\u8f93\u51fa\u7684\u7ed3\u679c)\u3002 \u6bcf\u4e00\u4e2a\u5faa\u73af\u4e2d\uff0c\u968f\u673a\u5730\u5c06\u5f53\u524d\u7684image patch\u66ff\u6362\u56fe\u7247\u7684\u4e00\u4e2a\u90e8\u5206\uff0c\u4f4d\u7f6e\u65cb\u8f6c\u4ee5\u53cascale\u6709\u4e00\u5b9a\u968f\u673a\u6027\uff0c\u7136\u540e\u4f7f\u7528\u68af\u5ea6\u4f18\u5316\uff0cupdate\u8f93\u5165\u7684image patch\uff0c\u4f7f\u5f97\u7f51\u7edc\u5bf9\u76ee\u6807\u7c7b\u522b\u9884\u6d4b\u7684\u6982\u7387\u503c\u6700\u5927\u5316\u3002 \\widehat{p}=\\arg \\max _{p} \\mathbb{E}_{x \\sim X, t \\sim T, l \\sim L}[\\log \\operatorname{Pr}(\\widehat{y} | A(p, x, l, t)] \u7ed3\u679c \u5982\u679c\u7f51\u662f\u5df2\u77e5\u7684\uff0c\u4f7f\u7528white box (\u8bad\u7ec3\u65f6\u7f51\u7edc\u4e0e\u6d4b\u8bd5\u7f51\u7edc\u4e00\u81f4)\uff0c\u8fd9\u4e2apatch\u53ea\u9700\u8981\u5360\u636e\u56fe\u7247\u768410%\u4e0d\u5230\u7684\u5927\u5c0f\u5c31\u53ef\u4ee5\u5927\u6982\u7387\u8ff7\u60d1\u7f51\u7edc\u3002 \u5982\u679c\u7f51\u7edc\u662f\u672a\u77e5\u7684\uff0c\u4f7f\u7528 black box(\u672c\u6587\u7684\u5b9e\u9a8c\u662f\u4f7f\u75284\u4e2a\u4e0d\u540c\u7684\u7f51\u7edctrain\u8fd9\u4e2abatch\uff0c\u4f7f\u7528\u53e6\u5916\u4e00\u4e2a\u7f51\u7edc\u8fdb\u884c\u6d4b\u8bd5)\uff0c\u8fd9\u4e2apatch\u53ea\u8981\u5360\u636e\u539f\u56fe\u7247\u768410%-20%\u5c31\u53ef\u4ee5\u5927\u6982\u7387\u8ff7\u60d1\u8fd9\u4e2a\u672a\u77e5\u7684\u7f51\u7edc\u3002","title":"Adversarial Patch"},{"location":"other_categories/others/adversarialPatch/#adversarial-patch","text":"\u8fd9\u7bc7paper\u505a\u5de5\u4f5c\u5982\u56fe. \u5176\u4e0e\u4e00\u822c\u7684\u5bf9\u6297\u6027\u653b\u51fb\u4e0d\u540c\u7684\u5730\u65b9\u5728\u4e8e\uff1a 1. \u7406\u8bba\u4e0a\u6765\u8bf4\u53ef\u4ee5\u5bf9\u6297\u5728ImageNet\u4e0a\u8bad\u7ec3\u7684\u4e0d\u540cmodel\u7684\u6240\u6709image\u3002 2. \u53ef\u4ee5\u6253\u5370\u51fa\u6765\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u653e\u7f6e\u3002\u5f62\u6210\u5b9e\u65f6\u653b\u51fb","title":"Adversarial Patch"},{"location":"other_categories/others/adversarialPatch/#approach","text":"\u65b9\u6cd5\u9700\u6c42, \u4e00\u4e2aimage patch(\u6bd4\u5982\u672c\u6587\u7684\u4e00\u5c0f\u4e2a toaster), \u4e00\u4e2a\u6216\u591a\u4e2a\u5df2\u7ecf\u8bad\u7ec3\u597d\u7684\u5206\u7c7b\u7f51\u7edc\uff0c\u4e00\u4e2a\u5206\u7c7b\u6570\u636e\u96c6(\u4e0d\u9700\u8981target)\uff0c\u4e00\u4e2a\u76ee\u6807\u7c7b\u522b(\u8981\u6c42\u88ab\u653b\u51fb\u7684\u7f51\u7edc\u8f93\u51fa\u7684\u7ed3\u679c)\u3002 \u6bcf\u4e00\u4e2a\u5faa\u73af\u4e2d\uff0c\u968f\u673a\u5730\u5c06\u5f53\u524d\u7684image patch\u66ff\u6362\u56fe\u7247\u7684\u4e00\u4e2a\u90e8\u5206\uff0c\u4f4d\u7f6e\u65cb\u8f6c\u4ee5\u53cascale\u6709\u4e00\u5b9a\u968f\u673a\u6027\uff0c\u7136\u540e\u4f7f\u7528\u68af\u5ea6\u4f18\u5316\uff0cupdate\u8f93\u5165\u7684image patch\uff0c\u4f7f\u5f97\u7f51\u7edc\u5bf9\u76ee\u6807\u7c7b\u522b\u9884\u6d4b\u7684\u6982\u7387\u503c\u6700\u5927\u5316\u3002 \\widehat{p}=\\arg \\max _{p} \\mathbb{E}_{x \\sim X, t \\sim T, l \\sim L}[\\log \\operatorname{Pr}(\\widehat{y} | A(p, x, l, t)]","title":"Approach"},{"location":"other_categories/others/adversarialPatch/#_1","text":"\u5982\u679c\u7f51\u662f\u5df2\u77e5\u7684\uff0c\u4f7f\u7528white box (\u8bad\u7ec3\u65f6\u7f51\u7edc\u4e0e\u6d4b\u8bd5\u7f51\u7edc\u4e00\u81f4)\uff0c\u8fd9\u4e2apatch\u53ea\u9700\u8981\u5360\u636e\u56fe\u7247\u768410%\u4e0d\u5230\u7684\u5927\u5c0f\u5c31\u53ef\u4ee5\u5927\u6982\u7387\u8ff7\u60d1\u7f51\u7edc\u3002 \u5982\u679c\u7f51\u7edc\u662f\u672a\u77e5\u7684\uff0c\u4f7f\u7528 black box(\u672c\u6587\u7684\u5b9e\u9a8c\u662f\u4f7f\u75284\u4e2a\u4e0d\u540c\u7684\u7f51\u7edctrain\u8fd9\u4e2abatch\uff0c\u4f7f\u7528\u53e6\u5916\u4e00\u4e2a\u7f51\u7edc\u8fdb\u884c\u6d4b\u8bd5)\uff0c\u8fd9\u4e2apatch\u53ea\u8981\u5360\u636e\u539f\u56fe\u7247\u768410%-20%\u5c31\u53ef\u4ee5\u5927\u6982\u7387\u8ff7\u60d1\u8fd9\u4e2a\u672a\u77e5\u7684\u7f51\u7edc\u3002","title":"\u7ed3\u679c"},{"location":"other_categories/others/detic/","text":"Detecting Twenty-thousand Classes using Image-level Supervision \u8fd9\u7bc7paper\u89e3\u51b3\u7684\u95ee\u9898\u662f\u4f7f\u7528\u56fe\u7247\u5206\u7c7b\u7684\u6570\u636e\u96c6\u5e2e\u52a9\u68c0\u6d4b\u6570\u636e\u96c6\u5b66\u4e60\u8db3\u591f\u7684\u591a\u7684\u7c7b\u522b\u3002\u9488\u5bf9\u7684\u662f\u4e8c\u9636\u6bb5\u76ee\u6807\u68c0\u6d4b\u3002 \u601d\u8def\u4e0a\u4e0e\u6b64\u524d\u7684\u5f31\u76d1\u7763\u5b8c\u5168\u4e0d\u4e00\u6837\u3002\u6b64\u524d\u7684\u5f31\u76d1\u7763\u7684\u601d\u8def\u5f80\u5f80\u662f\u60f3\u529e\u6cd5\u628a\u56fe\u7247\u7684\u5206\u7c7blabel \u5206\u914d\u7ed9\u5c40\u90e8\u5c0f\u56fe\u3002\u96be\u70b9\u5c31\u5728\u4e8elabel assignment\u4e0a\u3002 \u672c\u6587\u91c7\u53d6\u4e86\u53ef\u80fd\u80fd\u60f3\u5230\u7684\u6700\u7b80\u5355\u7684label assignment\u65b9\u6cd5\uff0c\u5982\u4e0a\u56fe\u3002 \u68c0\u6d4b\u5668\u540c\u65f6\u63a5\u53d7Detection\u6570\u636e\u7684\u8bad\u7ec3\u4ee5\u53caimage-label\u6570\u636e\u7684\u8bad\u7ec3\uff0c\u5bf9\u4e8edetection\u6570\u636e\uff0c\u8bad\u7ec3\u65b9\u6848\u4e0e\u4f20\u7edf\u68c0\u6d4b\u4e00\u6837\uff0c\u540c\u65f6\u8bad\u7ec3\u4e24\u9636\u6bb5\u7684\u68c0\u6d4b\u5668\uff0c\u6ce8\u610f\u7b2c\u4e00\u9636\u6bb5RPN\u7684\u68c0\u6d4b\u5668\u662f\u4e00\u4e2a\u4e8c\u5206\u7c7b\u4efb\u52a1\uff0c\u53ea\u4ee3\u8868\u6709\u65e0\u3002\u5bf9\u4e8eimage-label\u6570\u636e\uff0c\u4ece\u7b2c\u4e00\u9636\u6bb5RPN\u8f93\u51fa\u5927\u91cf\u7684proposal\u6846\uff0c\u4ece\u4e2d\u9009\u62e9\u6700\u5927\u7684\u6846\u5206\u914d\u672c\u56felabel\u7684GT\uff0c\u8fd0\u884c\u5e76\u8bad\u7ec3\u7b2c\u4e8c\u9636\u6bb5\u7684\u5206\u7c7b\u5668\u3002 \u7528\u8fd9\u4e2a\u65b9\u6cd5\u4f5c\u8005\u5b9e\u73b0\u4e86\u5728Image22K\u6570\u636e\u96c6\u7ea7\u522b\u5206\u7c7b\u6570\u7684\u76ee\u6807\u68c0\u6d4b\u3002 \u4f5c\u8005\u70b9\u51fa\u8fd9\u4e2a\u65b9\u6cd5\u7684\u4e00\u4e2a\u5173\u952e\u5728\u4e8e\u5206\u7c7b\u7684\u8bad\u7ec3\u4e0d\u5e94\u8be5\u53d7\u5236\u4e8e\u7b2c\u4e8c\u9636\u6bb5\u5206\u7c7b\u5668\u7684\u7ed3\u679c\uff1b\u4ee5\u524d\u7684\u5f31\u76d1\u7763\u7f51\u7edc\u5728\u4e8c\u9636\u6bb5\u8bad\u7ec3\u65f6\uff0c\u9700\u8981\u597d\u7684\u5206\u7c7b\u9884\u6d4b\u5668\u6765\u505a\u597d\u5206\u914d\uff0c\u53c8\u9700\u8981\u597d\u7684\u5206\u914d\u6765\u505a\u5206\u7c7b\u9884\u6d4b\uff0c\u8fd9\u4e2a\u9e21\u4e0e\u86cb\u7684\u95ee\u9898\u65e0\u6cd5\u51b7\u542f\u52a8\u4f7f\u5f97\u6027\u80fd\u5f98\u5f8a\u4e0d\u524d\u3002\u672c\u6587\u7684\u5173\u952e\u5728\u4e8e\u63d0\u4f9b\u4e86\u5168\u65b0\u7684\u8303\u5f0f\uff0c\u76f4\u63a5\u7ed5\u8fc7\u4e86\u8fd9\u4e2a\u95ee\u9898\uff0c\u4f9d\u8d56\u4e00\u9636\u6bb5\u7684\u7f51\u7edc\u76f4\u63a5\u5b9e\u73b0\u4e86\u8d85\u8d8a\u6b64\u524d\u5f88\u591a\u8bba\u6587\u7684\u6027\u80fd\u3002","title":"Detecting Twenty-thousand Classes using Image-level Supervision"},{"location":"other_categories/others/detic/#detecting-twenty-thousand-classes-using-image-level-supervision","text":"\u8fd9\u7bc7paper\u89e3\u51b3\u7684\u95ee\u9898\u662f\u4f7f\u7528\u56fe\u7247\u5206\u7c7b\u7684\u6570\u636e\u96c6\u5e2e\u52a9\u68c0\u6d4b\u6570\u636e\u96c6\u5b66\u4e60\u8db3\u591f\u7684\u591a\u7684\u7c7b\u522b\u3002\u9488\u5bf9\u7684\u662f\u4e8c\u9636\u6bb5\u76ee\u6807\u68c0\u6d4b\u3002 \u601d\u8def\u4e0a\u4e0e\u6b64\u524d\u7684\u5f31\u76d1\u7763\u5b8c\u5168\u4e0d\u4e00\u6837\u3002\u6b64\u524d\u7684\u5f31\u76d1\u7763\u7684\u601d\u8def\u5f80\u5f80\u662f\u60f3\u529e\u6cd5\u628a\u56fe\u7247\u7684\u5206\u7c7blabel \u5206\u914d\u7ed9\u5c40\u90e8\u5c0f\u56fe\u3002\u96be\u70b9\u5c31\u5728\u4e8elabel assignment\u4e0a\u3002 \u672c\u6587\u91c7\u53d6\u4e86\u53ef\u80fd\u80fd\u60f3\u5230\u7684\u6700\u7b80\u5355\u7684label assignment\u65b9\u6cd5\uff0c\u5982\u4e0a\u56fe\u3002 \u68c0\u6d4b\u5668\u540c\u65f6\u63a5\u53d7Detection\u6570\u636e\u7684\u8bad\u7ec3\u4ee5\u53caimage-label\u6570\u636e\u7684\u8bad\u7ec3\uff0c\u5bf9\u4e8edetection\u6570\u636e\uff0c\u8bad\u7ec3\u65b9\u6848\u4e0e\u4f20\u7edf\u68c0\u6d4b\u4e00\u6837\uff0c\u540c\u65f6\u8bad\u7ec3\u4e24\u9636\u6bb5\u7684\u68c0\u6d4b\u5668\uff0c\u6ce8\u610f\u7b2c\u4e00\u9636\u6bb5RPN\u7684\u68c0\u6d4b\u5668\u662f\u4e00\u4e2a\u4e8c\u5206\u7c7b\u4efb\u52a1\uff0c\u53ea\u4ee3\u8868\u6709\u65e0\u3002\u5bf9\u4e8eimage-label\u6570\u636e\uff0c\u4ece\u7b2c\u4e00\u9636\u6bb5RPN\u8f93\u51fa\u5927\u91cf\u7684proposal\u6846\uff0c\u4ece\u4e2d\u9009\u62e9\u6700\u5927\u7684\u6846\u5206\u914d\u672c\u56felabel\u7684GT\uff0c\u8fd0\u884c\u5e76\u8bad\u7ec3\u7b2c\u4e8c\u9636\u6bb5\u7684\u5206\u7c7b\u5668\u3002 \u7528\u8fd9\u4e2a\u65b9\u6cd5\u4f5c\u8005\u5b9e\u73b0\u4e86\u5728Image22K\u6570\u636e\u96c6\u7ea7\u522b\u5206\u7c7b\u6570\u7684\u76ee\u6807\u68c0\u6d4b\u3002 \u4f5c\u8005\u70b9\u51fa\u8fd9\u4e2a\u65b9\u6cd5\u7684\u4e00\u4e2a\u5173\u952e\u5728\u4e8e\u5206\u7c7b\u7684\u8bad\u7ec3\u4e0d\u5e94\u8be5\u53d7\u5236\u4e8e\u7b2c\u4e8c\u9636\u6bb5\u5206\u7c7b\u5668\u7684\u7ed3\u679c\uff1b\u4ee5\u524d\u7684\u5f31\u76d1\u7763\u7f51\u7edc\u5728\u4e8c\u9636\u6bb5\u8bad\u7ec3\u65f6\uff0c\u9700\u8981\u597d\u7684\u5206\u7c7b\u9884\u6d4b\u5668\u6765\u505a\u597d\u5206\u914d\uff0c\u53c8\u9700\u8981\u597d\u7684\u5206\u914d\u6765\u505a\u5206\u7c7b\u9884\u6d4b\uff0c\u8fd9\u4e2a\u9e21\u4e0e\u86cb\u7684\u95ee\u9898\u65e0\u6cd5\u51b7\u542f\u52a8\u4f7f\u5f97\u6027\u80fd\u5f98\u5f8a\u4e0d\u524d\u3002\u672c\u6587\u7684\u5173\u952e\u5728\u4e8e\u63d0\u4f9b\u4e86\u5168\u65b0\u7684\u8303\u5f0f\uff0c\u76f4\u63a5\u7ed5\u8fc7\u4e86\u8fd9\u4e2a\u95ee\u9898\uff0c\u4f9d\u8d56\u4e00\u9636\u6bb5\u7684\u7f51\u7edc\u76f4\u63a5\u5b9e\u73b0\u4e86\u8d85\u8d8a\u6b64\u524d\u5f88\u591a\u8bba\u6587\u7684\u6027\u80fd\u3002","title":"Detecting Twenty-thousand Classes using Image-level Supervision"},{"location":"other_categories/others/flownet/","text":"FlowNet and Variants \u8fd9\u91cc\u7efc\u5408\u8bb0\u5f55FlowNet\u4ee5\u53ca\u5b83\u7684\u4e00\u4e9b\u53d8\u4f53 FlowNet Base pdf \u8fd9\u7bc7paper\u662fflownet\u7684\u7b2c\u4e00\u7bc7\u3002\u63d0\u51fa\u4f7f\u7528sliding disparity \u4ee5\u53cacorrelation. c\\left(\\mathbf{x}_{1}, \\mathbf{x}_{2}\\right)=\\sum_{\\mathbf{o} \\in[-k, k] \\times[-k, k]}\\left\\langle\\mathbf{f}_{1}\\left(\\mathbf{x}_{1}+\\mathbf{o}\\right), \\mathbf{f}_{2}\\left(\\mathbf{x}_{2}+\\mathbf{o}\\right)\\right\\rangle FlowNet 2 pdf FlowNetC\u4e0e\u7b2c\u4e00paper\u7684\u4e00\u81f4\u3002\u800cFlowNetS\u5219\u4e3a\u76f4\u63a5\u7684encoder-decoder. \u672c\u6587\u4ee3\u7801 \u6709 channelnorm, correlation, resample_2d\u7684cuda\u5b9e\u73b0\u3002","title":"FlowNet and Variants"},{"location":"other_categories/others/flownet/#flownet-and-variants","text":"\u8fd9\u91cc\u7efc\u5408\u8bb0\u5f55FlowNet\u4ee5\u53ca\u5b83\u7684\u4e00\u4e9b\u53d8\u4f53","title":"FlowNet and Variants"},{"location":"other_categories/others/flownet/#flownet-base","text":"pdf \u8fd9\u7bc7paper\u662fflownet\u7684\u7b2c\u4e00\u7bc7\u3002\u63d0\u51fa\u4f7f\u7528sliding disparity \u4ee5\u53cacorrelation. c\\left(\\mathbf{x}_{1}, \\mathbf{x}_{2}\\right)=\\sum_{\\mathbf{o} \\in[-k, k] \\times[-k, k]}\\left\\langle\\mathbf{f}_{1}\\left(\\mathbf{x}_{1}+\\mathbf{o}\\right), \\mathbf{f}_{2}\\left(\\mathbf{x}_{2}+\\mathbf{o}\\right)\\right\\rangle","title":"FlowNet Base"},{"location":"other_categories/others/flownet/#flownet-2","text":"pdf FlowNetC\u4e0e\u7b2c\u4e00paper\u7684\u4e00\u81f4\u3002\u800cFlowNetS\u5219\u4e3a\u76f4\u63a5\u7684encoder-decoder. \u672c\u6587\u4ee3\u7801 \u6709 channelnorm, correlation, resample_2d\u7684cuda\u5b9e\u73b0\u3002","title":"FlowNet 2"},{"location":"other_categories/others/gaussian_splatting/","text":"3D Gaussian Splatting for Real-Time Radiance Field Rendering \u8fd9\u7bc7\u8bba\u6587\u662f\u8fd1\u671f\u6bd4\u8f83\u91cd\u8981\u7684\u4e09\u7ef4\u91cd\u5efa\u6587\u7ae0\u3002\u540e\u7eed\u6709\u5927\u91cf\u7684\u5de5\u4f5c\u4f1a\u57fa\u4e8e\u5b83\u8fdb\u884c\u5f00\u53d1\u3002\u8fd9\u91cc\u4f1a\u6574\u7406\u5b83\u7684\u539f\u7406\u4ee5\u53ca\u76f8\u5173\u7684\u91cd\u8981\u77e5\u8bc6\u3002 \u8bba\u6587\u89e3\u6790 Modeling the Environment as set of 3D Gaussian \u5bf9\u573a\u666f\u7684\u5efa\u6a21\uff0c\u672c\u6587\u5728review\u76f8\u5173\u5de5\u4f5c\u7684\u65f6\u5019\u63d0\u5230\uff0c\u66fe\u7ecf\u6709Mesh/\u9762\u5143 \u8fd9\u4e9b\u90fd\u662f\u5e38\u89c1\u7684\u5730\u56fe\u63cf\u8ff0\u5143\u7d20\u3002NERF\u662f\u628a\u573a\u666f\u5efa\u6a21\u6210\u4e86\u4e00\u4e2a\u8fde\u7eed\u51fd\u6570 f(x,y,z)->(\\text{color}, \\text{occupancy}) ,\u800c\u8fd9\u4e2a\u8fde\u7eed\u51fd\u6570\u7528\u8bad\u7ec3\u51fa\u6765\u7684\u795e\u7ecf\u7f51\u7edc\u7684\u6743\u91cd\u6765\u8868\u8fbe\u3002\u4e0e\u6b64\u4e0d\u540c\u7684\uff0c\u7a81\u51fa\u5229\u7528\u573a\u666f\u79bb\u6563\u6027\u4ee5\u8282\u7701\u8d44\u6e90\u7684\u65b9\u6848\uff0c\u6709\u57fa\u4e8e \u9762\u5143\uff08\u70b9+\u6cd5\u5411+\u692d\u5706\u5e73\u9762\u5927\u5c0f\uff09\u6216\u8005\u70b9\u7684\u65b9\u6848\u3002 Supplementary: An Example of point based neural rendering pdf code \u89e3\u8bfb \u8fd9\u7bc7paper NPDG\u7684motivation\u5c31\u662f\u8981\u89e3\u51b3\u70b9\u4e91\u6295\u5f71\u5b9e\u73b0novel view synthesis\u7684\u65f6\u5019\u70b9\u7684\u906e\u6321\u4e0e\u7a7a\u6d1e\u95ee\u9898\u3002\u5982\u679c\u56fe\u7247\u5206\u8fa8\u7387\u5f88\u9ad8\uff0c\u6bcf\u4e2a\u50cf\u7d20\u5f88\u5c0f\uff0c\u6295\u5f71\u4ea7\u751f\u7684\u56fe\u7247\u5c31\u4f1a\u6709\u5f88\u591a\u7a7a\u6d1e\uff1b\u5982\u679c\u56fe\u7247\u5206\u8fa8\u7387\u5f88\u4f4e\uff0c\u6bcf\u4e2a\u50cf\u7d20\u5f88\u5927\uff0c\u6bcf\u4e2a\u50cf\u7d20\u5c31\u4f1a\u5bf9\u5e94\u591a\u4e2a\u70b9\u4ea7\u751f\u5927\u91cf\u906e\u6321\uff0c\u8fd9\u7bc7paper\u91c7\u7528\u4e00\u4e2a\u5b66\u4e60\u7684\u65b9\u5f0f\uff0c\u878d\u5408\u591a\u5c3a\u5ea6\u7684\u6295\u5f71\u7ed3\u679c\u8f93\u51fa\u6700\u7ec8\u56fe\u50cf\u3002 Describing 3D Gaussian \u6570\u5b66\u4ecb\u7ecd \u56de\u5230\u8fd9\u7bc7paper Gaussian Splatting\uff0c\u6587\u7ae0\u6307\u51fa\uff0c\u5982\u679c\u6211\u4eec\u91c7\u7528SFM\u7684\u70b9\u4e91\u4f5c\u4e3a\u8f93\u5165\uff0c\u7531\u4e8eSFM\u70b9\u4e91\u76f8\u5bf9\u8fd8\u662f\u6bd4\u8f83\u7a00\u758f\u7684\uff0c\u5982\u679c\u4ece\u4e2d\u63d0\u53d6\u6cd5\u5411\u4f5c\u4e3a\u8f93\u5165\u6216\u8005\u662f\u5148\u9a8c\uff0c\u8fd9\u4e2a\u7ed3\u679c\u7684\u51c6\u786e\u5ea6\u662f\u6bd4\u8f83\u6709\u9650\u7684\uff0c\u6240\u4ee5\u672c\u6587\u9009\u62e9\u76f4\u63a5\u63d0\u53d6SFM\u7684\u70b9\u7684\u4f4d\u7f6e\u4f5c\u4e3a\u5148\u9a8c\uff0c\u5e76\u4e14\u628a\u4e00\u4e2a\u9762\u5143/\u5355\u4f4d\u4f53\uff0c\u5efa\u6a21\u4e3a\u4e00\u4e2a\u4e09\u7ef4\u9ad8\u65af\uff08\u6cbf\u7a7a\u95f4xyz\u8f74\u5206\u5e03\u7684\u6709\u504f\u9ad8\u65af\uff0c\u521d\u59cb\u5316\u65f6\u8bbe\u5b9a\u4e3a\u65e0\u504f\u9ad8\u65af\uff0c\u65b9\u5dee\u4e3a\u76f8\u90bb\u4e09\u4e2a\u70b9\u7684\u8ddd\u79bb\u7684\u5747\u503c\uff09\u3002 \u4f8b\u5b50\uff1a\u628a\u4e00\u4e2a\u6295\u5f71\u5230\u56fe\u7247\u57df\u76842D\u9ad8\u65af\u53ef\u89c6\u5316\u51fa\u6765\u5982\u56fe, \u5176\u4e2d\u4e00\u4e2a\u50cf\u7d20\u7684\u6e32\u67d3\u53ef\u4ee5\u8ba1\u7b97\u4e3a C=\\Sigma_{i \\in N} c_i\\alpha_i \\Pi^{i-1}_{j=1}(1-\\alpha_i) , \u5176\u4e2d c_1, \\alpha_1 \u5206\u522b\u4e3a\u989c\u8272\u4ee5\u53ca\u6982\u7387\u5bc6\u5ea6: \u8fd9\u91cc\u9762\u7275\u6d89\u5230\u6570\u4e2a\u6280\u672f\u7ec6\u8282: \u76f8\u673a\u6295\u5f71\u662f\u4e00\u4e2a\u975e\u7ebf\u6027\u53d8\u6362\uff0c\u4e09\u7ef4\u4e2d\u7684\u9ad8\u65af\u5206\u5e03\u6295\u5f71\u5230\u76f8\u673a\u5750\u6807\u7cfb\u4e2d\u5e76\u4e0d\u662f\u4e00\u4e2a\u9ad8\u65af\u5206\u5e03\u3002\u672c\u6587\u5728\u8fd9\u91cc\u4e0e\u524d\u4eba\u76f8\u8fd1\uff0c\u91c7\u7528EKF\u7684\u65b9\u5f0f\u8fdb\u884c\u9ad8\u65af\u6295\u5f71\u7684\u8fd1\u4f3c \\Sigma' = JW\\Sigma W^TJ^T \u5176\u4e2d W \u662f\u4e16\u754c\u5750\u6807\u5230\u76f8\u673a\u5750\u6807\u7684\u8f6c\u6362\uff0c J \u662f\u76f8\u673a\u6295\u5f71\u7684\u96c5\u514b\u6bd4\u77e9\u9635\u3002\u4ee5\u6b64\u5b9e\u73b0\u9ad8\u65af\u5757\u7684\u53ef\u5bfc\u6295\u5f71. \u9ad8\u65af\u5757\u7684\u989c\u8272\uff0c\u5982\u679c\u6211\u4eec\u5047\u8bbe\u6bcf\u4e2a3D\u9ad8\u65af\u662f\u4e00\u4e2a\u56fa\u5b9a\u7684\u989c\u8272\uff0c\u90a3\u4e48\u6e32\u67d3\u6548\u679c\u53ef\u80fd\u4f1a\u975e\u5e38\u7cdf\u7cd5\u3002\u672c\u6587\u8fd9\u91cc\u628a\u9ad8\u65af\u5757\u4e0a\u9762\u7684\u70b9\u7684\u989c\u8272\u5efa\u6a21\u6210\u4e86\u4e00\u4e2a\u7403\u8c10\u51fd\u6570\u3002\u8fd9\u4e2a\u95ee\u9898\u672c\u8d28\u4e0a\u662f\u5bfb\u627e\u4e00\u4e2a\u5355\u4f4d\u7403\u9762(\u9ad8\u65af\u5c31\u662f\u4e00\u4e2a\u692d\u7403\u9762\uff0c\u62c9\u4f38\u8fc7\u7684\u7403\u9762)\u4e0a\u7684\u70b9\u4e0e\u6807\u91cf\uff08\u989c\u8272\uff09\u7684\u6620\u5c04\u51fd\u6570\uff0c\u4e0e\u5b9e\u6570\u57df\u4e2d\u7684\u591a\u9879\u5f0f\u5206\u89e3/\u5085\u7acb\u53f6\u5206\u89e3\u7c7b\u4f3c\uff0c\u7403\u8c10\u51fd\u6570\u662f\u4e00\u7ec4\u7403\u9762\u4e0a\u7684\u57fa\u51fd\u6570\uff0c\u5b83\u4fdd\u8bc1\u4e86\u7403\u9762\u4e0a\u51fd\u6570\u7684\u8fde\u7eed\u6027\uff0c\u540c\u65f6\u53ef\u4ee5\u6839\u636e\u9009\u62e9\u7684\u7cfb\u6570\u4ee5\u8f83\u4f4e\u7684\u8bef\u5dee\u7387\u62df\u5408\u4f4e\u9891\u7684\u989c\u8272\u51fd\u6570\uff0c\u672c\u6587\u62df\u5408\u5230\u4e09\u9636\u7cfb\u6570\uff0c\u8fd9\u4e2a\u4e5f\u662f\u6e38\u620f\u6e32\u67d3\u4e2d\u5e94\u5bf9\u5149\u7167\u5e38\u7528\u53c2\u6570\u9009\u62e9\u3002 \u5982\u4f55\u8bad\u7ec3\u534f\u65b9\u5dee\u77e9\u9635\u4fdd\u8bc1\u534a\u6b63\u5b9a\u6027\uff1f\u672c\u6587\u7684\u9009\u62e9\u662f\u5b9a\u4e49\u975e\u8d1f\u7684\u4e09\u7ef4\u5c3a\u5ea6\u77e2\u91cf S (\u4ece\u6307\u6570\u6fc0\u6d3b\u5f97\u5230),\u4ee5\u53ca\u56db\u5143\u6570 q ,\u5f62\u8c61\u4e0a\u7684\u7406\u89e3\uff0c\u5c31\u662f\u4ece\u4e00\u4e2a\u5355\u4f4d\u5706\u5f00\u59cb\uff0c\u6cbf\u4e09\u8f74\u500d\u7387\u653e\u7f29\u62c9\u4f38\uff0c\u518d\u65cb\u8f6c\u8fd9\u4e2a\u692d\u7403\u3002\u5f97\u5230\u4e00\u4e2a\u4efb\u610f\u59ff\u6001\u7684\u692d\u7403\u3002\u6570\u5b66\u4e0a \\Sigma = RSS^TR^T \u5176\u4e2dR\u5c31\u662f\u56db\u5143\u6570\u8ba1\u7b97\u5f97\u5230\u7684\u65cb\u8f6c\u77e9\u9635\u3002 Training Structure Gaussian Splatting\u63d0\u51fa\u7684\u8ba1\u7b97\u65b9\u6848\u5982\u56fe\u3002\u521d\u59cb\u5316, \u52a8\u6001\u5bc6\u5ea6\u63a7\u5236\u5728\u540e\u9762\u8be6\u7ec6\u5c55\u5f00\u3002\u9664\u6b64\u5916\u5c31\u662f3D Gaussian\u7684\u53ef\u5fae\u5206\u6295\u5f71\u4e0e\u91cd\u5efa\u635f\u5931\u6c42\u5bfc\u3002 Initialization Code Line \u8f93\u5165SFM\u5f97\u5230\u7684\u70b9\u4e91\uff0c\u521d\u59cb3D Gaussian\u7684\u70b9\u7684\u6570\u91cf\u4ee5\u53ca\u70b9\u7684\u4f4d\u7f6e\u4e0eSFM\u7ed3\u679c\u76f8\u540c\u3002\u5176\u989c\u8272\u6570\u636e\u5c31\u662fSFM\u7684\u989c\u8272\u8f6c\u6362\u5230SH(Sphere Harmonic) \u7403\u8c10\u7cfb\u6570,\u521d\u59cb\u5316\u4e3a\u6ca1\u6709\u9ad8\u9636\u9879\u7684\u5e38\u6570\u51fd\u6570\u3002\u65b9\u5dee\u7684\u521d\u59cb\u5316\u91cc\uff0c\u5176\u4e2d S=[r, r, r] r\u5b9a\u4e3a\u6700\u8fd1\u8ddd\u79bb\u7684\u4e09\u4e2a\u70b9\u7684\u5747\u503c, \u65cb\u8f6c q=[1,0,0,0] \u4e5f\u5c31\u662f\u6ca1\u6709\u65cb\u8f6c. \u900f\u8fc7\u7387opacities\u5b9a\u4e3a0.1 (\u7528sigmoid\u540e\u4e3a0.1) Adaptive Density Control \u52a8\u6001\u5bc6\u5ea6\u63a7\u5236\uff0c\u8fd9\u662fgaussian splatting\u672c\u6587\u7684\u4e00\u5927\u9b54\u6cd5\u3002 \u5229\u7528\u7b80\u5355\u5143\u7d20\u505a\u573a\u666f\u91cd\u5efa\u65f6\uff0c\u5e38\u89c1\u7684\u95ee\u9898\u6709\u4ee5\u4e0b\u51e0\u4e2a\uff1a under-reconstruction, \u5bf9\u4e8e\u4e00\u4e2a\u5b9e\u9645\u4e0a\u5f88\u5927\u7684\u7269\u4f53\uff0c\u53ea\u6709\u4e00\u4e2a\u5f88\u5c0f\u7684gaussian\u53bb\u8986\u76d6\u5b83 over-reconstruction, \u5bf9\u4e8e\u4e00\u4e2a\u5b9e\u9645\u4e0a\u5f88\u5c0f\u53c8\u590d\u6742\u7684\u7269\u4f53\uff0c\u6709\u4e00\u4e2a\u5f88\u5927\u7684gaussian\u53bb\u8986\u76d6\u5b83\u3002 \u76f8\u673a\u524d\u65b9\u5728\u5f88\u8fd1\u5904\u6709floating objects\u963b\u788d\u4e86\u8bad\u7ec3\u3002 \u5bf9\u4e8eunder-reconstruction\u7684\u60c5\u51b5\uff0c code line \u7684\u64cd\u4f5c\u548c\u5b9e\u9645\u6709\u4e00\u5b9a\u533a\u522b,\u5b9e\u9645\u64cd\u4f5c\u662f\u8fdb\u884c\u4e86\u70b9\u7684\u590d\u5236\uff0c\u800c\u5e76\u6ca1\u6709\u504f\u79fb\u3002 \u5bf9\u4e8eover-reconstruction\u7684\u60c5\u51b5, code line \u6b63\u662f\u6587\u4e2d\u7684\u64cd\u4f5c\uff0c\u5728\u539f\u6765\u7684\u5927\u5206\u5e03\u4e0b\u9762\uff0c\u91c7\u6837\u4e24\u4e2a\u70b9\uff0c\u53d6\u65b0\u65b9\u5dee\u7f29\u5c0f\u4e3a\u4e3a\u539f\u6765\u7684 1/1.6\u500d\u3002 \u5bf9\u4e8e\u7b2c\u4e09\u4e2a\u95ee\u9898\uff0c code line , code line2 \uff0c\u6587\u4e2d\u7684\u64cd\u4f5c\u662f\u6bcf\u9694\u4e00\u6bb5iteration, \u628a\u6240\u6709\u7684gaussians\u7684opacity\u91cd\u7f6e\u4e3a0.01. \u7531\u4e8e\u5bf9\u7b2c\u4e00\u4e8c\u4e2a\u95ee\u9898\u7684\u4fee\u6b63\uff0cgaussian\u6570\u91cf\u4f1a\u8d8a\u6765\u8d8a\u591a,\u672c\u6587\u4f1a\u5728\u6bcf\u6b21densification\u540e\u628aopacity \u5c0f\u4e8e0.005\u7684gaussian\u53bb\u9664\u3002 Optimized GPU Rendering \u5c06\u5c4f\u5e55\u5206\u4e3a16*16\u7684tiles\uff0c\u4fdd\u755999%\u5728\u89c6\u9525\u91cc\u7684Gaussians\uff0c \u7ed9\u6bcf\u4e2aGaussian\u4e00\u4e2akey=depth+ID\uff0c\u7136\u540e\u5c06Gaussians\u8fdb\u884cGPU Radix Sort\uff0c\u5f97\u5230\u4e86\u6bcf\u4e2atile\u91cc\u6839\u636edepth\u6392\u5e8f\u7684Gaussian\u5217\u8868\uff0c \u6bcf\u4e2atile\u5206\u914dthread block\uff0c\u6bcf\u4e2ablock\u5148load\u5230shared memory\uff0c\u5bf9\u4e8e\u6bcf\u4e2apixel\uff0c\u7d2f\u79efcolor\u548calpha\uff0c\u8fbe\u5230\u9971\u548c\u65f6\u505c\u6b62\u3002","title":"3D Gaussian Splatting for Real-Time Radiance Field Rendering"},{"location":"other_categories/others/gaussian_splatting/#3d-gaussian-splatting-for-real-time-radiance-field-rendering","text":"\u8fd9\u7bc7\u8bba\u6587\u662f\u8fd1\u671f\u6bd4\u8f83\u91cd\u8981\u7684\u4e09\u7ef4\u91cd\u5efa\u6587\u7ae0\u3002\u540e\u7eed\u6709\u5927\u91cf\u7684\u5de5\u4f5c\u4f1a\u57fa\u4e8e\u5b83\u8fdb\u884c\u5f00\u53d1\u3002\u8fd9\u91cc\u4f1a\u6574\u7406\u5b83\u7684\u539f\u7406\u4ee5\u53ca\u76f8\u5173\u7684\u91cd\u8981\u77e5\u8bc6\u3002 \u8bba\u6587\u89e3\u6790","title":"3D Gaussian Splatting for Real-Time Radiance Field Rendering"},{"location":"other_categories/others/gaussian_splatting/#modeling-the-environment-as-set-of-3d-gaussian","text":"\u5bf9\u573a\u666f\u7684\u5efa\u6a21\uff0c\u672c\u6587\u5728review\u76f8\u5173\u5de5\u4f5c\u7684\u65f6\u5019\u63d0\u5230\uff0c\u66fe\u7ecf\u6709Mesh/\u9762\u5143 \u8fd9\u4e9b\u90fd\u662f\u5e38\u89c1\u7684\u5730\u56fe\u63cf\u8ff0\u5143\u7d20\u3002NERF\u662f\u628a\u573a\u666f\u5efa\u6a21\u6210\u4e86\u4e00\u4e2a\u8fde\u7eed\u51fd\u6570 f(x,y,z)->(\\text{color}, \\text{occupancy}) ,\u800c\u8fd9\u4e2a\u8fde\u7eed\u51fd\u6570\u7528\u8bad\u7ec3\u51fa\u6765\u7684\u795e\u7ecf\u7f51\u7edc\u7684\u6743\u91cd\u6765\u8868\u8fbe\u3002\u4e0e\u6b64\u4e0d\u540c\u7684\uff0c\u7a81\u51fa\u5229\u7528\u573a\u666f\u79bb\u6563\u6027\u4ee5\u8282\u7701\u8d44\u6e90\u7684\u65b9\u6848\uff0c\u6709\u57fa\u4e8e \u9762\u5143\uff08\u70b9+\u6cd5\u5411+\u692d\u5706\u5e73\u9762\u5927\u5c0f\uff09\u6216\u8005\u70b9\u7684\u65b9\u6848\u3002","title":"Modeling the Environment as set of 3D Gaussian"},{"location":"other_categories/others/gaussian_splatting/#supplementary-an-example-of-point-based-neural-rendering","text":"pdf code \u89e3\u8bfb \u8fd9\u7bc7paper NPDG\u7684motivation\u5c31\u662f\u8981\u89e3\u51b3\u70b9\u4e91\u6295\u5f71\u5b9e\u73b0novel view synthesis\u7684\u65f6\u5019\u70b9\u7684\u906e\u6321\u4e0e\u7a7a\u6d1e\u95ee\u9898\u3002\u5982\u679c\u56fe\u7247\u5206\u8fa8\u7387\u5f88\u9ad8\uff0c\u6bcf\u4e2a\u50cf\u7d20\u5f88\u5c0f\uff0c\u6295\u5f71\u4ea7\u751f\u7684\u56fe\u7247\u5c31\u4f1a\u6709\u5f88\u591a\u7a7a\u6d1e\uff1b\u5982\u679c\u56fe\u7247\u5206\u8fa8\u7387\u5f88\u4f4e\uff0c\u6bcf\u4e2a\u50cf\u7d20\u5f88\u5927\uff0c\u6bcf\u4e2a\u50cf\u7d20\u5c31\u4f1a\u5bf9\u5e94\u591a\u4e2a\u70b9\u4ea7\u751f\u5927\u91cf\u906e\u6321\uff0c\u8fd9\u7bc7paper\u91c7\u7528\u4e00\u4e2a\u5b66\u4e60\u7684\u65b9\u5f0f\uff0c\u878d\u5408\u591a\u5c3a\u5ea6\u7684\u6295\u5f71\u7ed3\u679c\u8f93\u51fa\u6700\u7ec8\u56fe\u50cf\u3002","title":"Supplementary: An Example of point based neural rendering"},{"location":"other_categories/others/gaussian_splatting/#describing-3d-gaussian","text":"\u6570\u5b66\u4ecb\u7ecd \u56de\u5230\u8fd9\u7bc7paper Gaussian Splatting\uff0c\u6587\u7ae0\u6307\u51fa\uff0c\u5982\u679c\u6211\u4eec\u91c7\u7528SFM\u7684\u70b9\u4e91\u4f5c\u4e3a\u8f93\u5165\uff0c\u7531\u4e8eSFM\u70b9\u4e91\u76f8\u5bf9\u8fd8\u662f\u6bd4\u8f83\u7a00\u758f\u7684\uff0c\u5982\u679c\u4ece\u4e2d\u63d0\u53d6\u6cd5\u5411\u4f5c\u4e3a\u8f93\u5165\u6216\u8005\u662f\u5148\u9a8c\uff0c\u8fd9\u4e2a\u7ed3\u679c\u7684\u51c6\u786e\u5ea6\u662f\u6bd4\u8f83\u6709\u9650\u7684\uff0c\u6240\u4ee5\u672c\u6587\u9009\u62e9\u76f4\u63a5\u63d0\u53d6SFM\u7684\u70b9\u7684\u4f4d\u7f6e\u4f5c\u4e3a\u5148\u9a8c\uff0c\u5e76\u4e14\u628a\u4e00\u4e2a\u9762\u5143/\u5355\u4f4d\u4f53\uff0c\u5efa\u6a21\u4e3a\u4e00\u4e2a\u4e09\u7ef4\u9ad8\u65af\uff08\u6cbf\u7a7a\u95f4xyz\u8f74\u5206\u5e03\u7684\u6709\u504f\u9ad8\u65af\uff0c\u521d\u59cb\u5316\u65f6\u8bbe\u5b9a\u4e3a\u65e0\u504f\u9ad8\u65af\uff0c\u65b9\u5dee\u4e3a\u76f8\u90bb\u4e09\u4e2a\u70b9\u7684\u8ddd\u79bb\u7684\u5747\u503c\uff09\u3002 \u4f8b\u5b50\uff1a\u628a\u4e00\u4e2a\u6295\u5f71\u5230\u56fe\u7247\u57df\u76842D\u9ad8\u65af\u53ef\u89c6\u5316\u51fa\u6765\u5982\u56fe, \u5176\u4e2d\u4e00\u4e2a\u50cf\u7d20\u7684\u6e32\u67d3\u53ef\u4ee5\u8ba1\u7b97\u4e3a C=\\Sigma_{i \\in N} c_i\\alpha_i \\Pi^{i-1}_{j=1}(1-\\alpha_i) , \u5176\u4e2d c_1, \\alpha_1 \u5206\u522b\u4e3a\u989c\u8272\u4ee5\u53ca\u6982\u7387\u5bc6\u5ea6: \u8fd9\u91cc\u9762\u7275\u6d89\u5230\u6570\u4e2a\u6280\u672f\u7ec6\u8282: \u76f8\u673a\u6295\u5f71\u662f\u4e00\u4e2a\u975e\u7ebf\u6027\u53d8\u6362\uff0c\u4e09\u7ef4\u4e2d\u7684\u9ad8\u65af\u5206\u5e03\u6295\u5f71\u5230\u76f8\u673a\u5750\u6807\u7cfb\u4e2d\u5e76\u4e0d\u662f\u4e00\u4e2a\u9ad8\u65af\u5206\u5e03\u3002\u672c\u6587\u5728\u8fd9\u91cc\u4e0e\u524d\u4eba\u76f8\u8fd1\uff0c\u91c7\u7528EKF\u7684\u65b9\u5f0f\u8fdb\u884c\u9ad8\u65af\u6295\u5f71\u7684\u8fd1\u4f3c \\Sigma' = JW\\Sigma W^TJ^T \u5176\u4e2d W \u662f\u4e16\u754c\u5750\u6807\u5230\u76f8\u673a\u5750\u6807\u7684\u8f6c\u6362\uff0c J \u662f\u76f8\u673a\u6295\u5f71\u7684\u96c5\u514b\u6bd4\u77e9\u9635\u3002\u4ee5\u6b64\u5b9e\u73b0\u9ad8\u65af\u5757\u7684\u53ef\u5bfc\u6295\u5f71. \u9ad8\u65af\u5757\u7684\u989c\u8272\uff0c\u5982\u679c\u6211\u4eec\u5047\u8bbe\u6bcf\u4e2a3D\u9ad8\u65af\u662f\u4e00\u4e2a\u56fa\u5b9a\u7684\u989c\u8272\uff0c\u90a3\u4e48\u6e32\u67d3\u6548\u679c\u53ef\u80fd\u4f1a\u975e\u5e38\u7cdf\u7cd5\u3002\u672c\u6587\u8fd9\u91cc\u628a\u9ad8\u65af\u5757\u4e0a\u9762\u7684\u70b9\u7684\u989c\u8272\u5efa\u6a21\u6210\u4e86\u4e00\u4e2a\u7403\u8c10\u51fd\u6570\u3002\u8fd9\u4e2a\u95ee\u9898\u672c\u8d28\u4e0a\u662f\u5bfb\u627e\u4e00\u4e2a\u5355\u4f4d\u7403\u9762(\u9ad8\u65af\u5c31\u662f\u4e00\u4e2a\u692d\u7403\u9762\uff0c\u62c9\u4f38\u8fc7\u7684\u7403\u9762)\u4e0a\u7684\u70b9\u4e0e\u6807\u91cf\uff08\u989c\u8272\uff09\u7684\u6620\u5c04\u51fd\u6570\uff0c\u4e0e\u5b9e\u6570\u57df\u4e2d\u7684\u591a\u9879\u5f0f\u5206\u89e3/\u5085\u7acb\u53f6\u5206\u89e3\u7c7b\u4f3c\uff0c\u7403\u8c10\u51fd\u6570\u662f\u4e00\u7ec4\u7403\u9762\u4e0a\u7684\u57fa\u51fd\u6570\uff0c\u5b83\u4fdd\u8bc1\u4e86\u7403\u9762\u4e0a\u51fd\u6570\u7684\u8fde\u7eed\u6027\uff0c\u540c\u65f6\u53ef\u4ee5\u6839\u636e\u9009\u62e9\u7684\u7cfb\u6570\u4ee5\u8f83\u4f4e\u7684\u8bef\u5dee\u7387\u62df\u5408\u4f4e\u9891\u7684\u989c\u8272\u51fd\u6570\uff0c\u672c\u6587\u62df\u5408\u5230\u4e09\u9636\u7cfb\u6570\uff0c\u8fd9\u4e2a\u4e5f\u662f\u6e38\u620f\u6e32\u67d3\u4e2d\u5e94\u5bf9\u5149\u7167\u5e38\u7528\u53c2\u6570\u9009\u62e9\u3002 \u5982\u4f55\u8bad\u7ec3\u534f\u65b9\u5dee\u77e9\u9635\u4fdd\u8bc1\u534a\u6b63\u5b9a\u6027\uff1f\u672c\u6587\u7684\u9009\u62e9\u662f\u5b9a\u4e49\u975e\u8d1f\u7684\u4e09\u7ef4\u5c3a\u5ea6\u77e2\u91cf S (\u4ece\u6307\u6570\u6fc0\u6d3b\u5f97\u5230),\u4ee5\u53ca\u56db\u5143\u6570 q ,\u5f62\u8c61\u4e0a\u7684\u7406\u89e3\uff0c\u5c31\u662f\u4ece\u4e00\u4e2a\u5355\u4f4d\u5706\u5f00\u59cb\uff0c\u6cbf\u4e09\u8f74\u500d\u7387\u653e\u7f29\u62c9\u4f38\uff0c\u518d\u65cb\u8f6c\u8fd9\u4e2a\u692d\u7403\u3002\u5f97\u5230\u4e00\u4e2a\u4efb\u610f\u59ff\u6001\u7684\u692d\u7403\u3002\u6570\u5b66\u4e0a \\Sigma = RSS^TR^T \u5176\u4e2dR\u5c31\u662f\u56db\u5143\u6570\u8ba1\u7b97\u5f97\u5230\u7684\u65cb\u8f6c\u77e9\u9635\u3002","title":"Describing 3D Gaussian"},{"location":"other_categories/others/gaussian_splatting/#training-structure","text":"Gaussian Splatting\u63d0\u51fa\u7684\u8ba1\u7b97\u65b9\u6848\u5982\u56fe\u3002\u521d\u59cb\u5316, \u52a8\u6001\u5bc6\u5ea6\u63a7\u5236\u5728\u540e\u9762\u8be6\u7ec6\u5c55\u5f00\u3002\u9664\u6b64\u5916\u5c31\u662f3D Gaussian\u7684\u53ef\u5fae\u5206\u6295\u5f71\u4e0e\u91cd\u5efa\u635f\u5931\u6c42\u5bfc\u3002","title":"Training Structure"},{"location":"other_categories/others/gaussian_splatting/#initialization","text":"Code Line \u8f93\u5165SFM\u5f97\u5230\u7684\u70b9\u4e91\uff0c\u521d\u59cb3D Gaussian\u7684\u70b9\u7684\u6570\u91cf\u4ee5\u53ca\u70b9\u7684\u4f4d\u7f6e\u4e0eSFM\u7ed3\u679c\u76f8\u540c\u3002\u5176\u989c\u8272\u6570\u636e\u5c31\u662fSFM\u7684\u989c\u8272\u8f6c\u6362\u5230SH(Sphere Harmonic) \u7403\u8c10\u7cfb\u6570,\u521d\u59cb\u5316\u4e3a\u6ca1\u6709\u9ad8\u9636\u9879\u7684\u5e38\u6570\u51fd\u6570\u3002\u65b9\u5dee\u7684\u521d\u59cb\u5316\u91cc\uff0c\u5176\u4e2d S=[r, r, r] r\u5b9a\u4e3a\u6700\u8fd1\u8ddd\u79bb\u7684\u4e09\u4e2a\u70b9\u7684\u5747\u503c, \u65cb\u8f6c q=[1,0,0,0] \u4e5f\u5c31\u662f\u6ca1\u6709\u65cb\u8f6c. \u900f\u8fc7\u7387opacities\u5b9a\u4e3a0.1 (\u7528sigmoid\u540e\u4e3a0.1)","title":"Initialization"},{"location":"other_categories/others/gaussian_splatting/#adaptive-density-control","text":"\u52a8\u6001\u5bc6\u5ea6\u63a7\u5236\uff0c\u8fd9\u662fgaussian splatting\u672c\u6587\u7684\u4e00\u5927\u9b54\u6cd5\u3002 \u5229\u7528\u7b80\u5355\u5143\u7d20\u505a\u573a\u666f\u91cd\u5efa\u65f6\uff0c\u5e38\u89c1\u7684\u95ee\u9898\u6709\u4ee5\u4e0b\u51e0\u4e2a\uff1a under-reconstruction, \u5bf9\u4e8e\u4e00\u4e2a\u5b9e\u9645\u4e0a\u5f88\u5927\u7684\u7269\u4f53\uff0c\u53ea\u6709\u4e00\u4e2a\u5f88\u5c0f\u7684gaussian\u53bb\u8986\u76d6\u5b83 over-reconstruction, \u5bf9\u4e8e\u4e00\u4e2a\u5b9e\u9645\u4e0a\u5f88\u5c0f\u53c8\u590d\u6742\u7684\u7269\u4f53\uff0c\u6709\u4e00\u4e2a\u5f88\u5927\u7684gaussian\u53bb\u8986\u76d6\u5b83\u3002 \u76f8\u673a\u524d\u65b9\u5728\u5f88\u8fd1\u5904\u6709floating objects\u963b\u788d\u4e86\u8bad\u7ec3\u3002 \u5bf9\u4e8eunder-reconstruction\u7684\u60c5\u51b5\uff0c code line \u7684\u64cd\u4f5c\u548c\u5b9e\u9645\u6709\u4e00\u5b9a\u533a\u522b,\u5b9e\u9645\u64cd\u4f5c\u662f\u8fdb\u884c\u4e86\u70b9\u7684\u590d\u5236\uff0c\u800c\u5e76\u6ca1\u6709\u504f\u79fb\u3002 \u5bf9\u4e8eover-reconstruction\u7684\u60c5\u51b5, code line \u6b63\u662f\u6587\u4e2d\u7684\u64cd\u4f5c\uff0c\u5728\u539f\u6765\u7684\u5927\u5206\u5e03\u4e0b\u9762\uff0c\u91c7\u6837\u4e24\u4e2a\u70b9\uff0c\u53d6\u65b0\u65b9\u5dee\u7f29\u5c0f\u4e3a\u4e3a\u539f\u6765\u7684 1/1.6\u500d\u3002 \u5bf9\u4e8e\u7b2c\u4e09\u4e2a\u95ee\u9898\uff0c code line , code line2 \uff0c\u6587\u4e2d\u7684\u64cd\u4f5c\u662f\u6bcf\u9694\u4e00\u6bb5iteration, \u628a\u6240\u6709\u7684gaussians\u7684opacity\u91cd\u7f6e\u4e3a0.01. \u7531\u4e8e\u5bf9\u7b2c\u4e00\u4e8c\u4e2a\u95ee\u9898\u7684\u4fee\u6b63\uff0cgaussian\u6570\u91cf\u4f1a\u8d8a\u6765\u8d8a\u591a,\u672c\u6587\u4f1a\u5728\u6bcf\u6b21densification\u540e\u628aopacity \u5c0f\u4e8e0.005\u7684gaussian\u53bb\u9664\u3002","title":"Adaptive Density Control"},{"location":"other_categories/others/gaussian_splatting/#optimized-gpu-rendering","text":"\u5c06\u5c4f\u5e55\u5206\u4e3a16*16\u7684tiles\uff0c\u4fdd\u755999%\u5728\u89c6\u9525\u91cc\u7684Gaussians\uff0c \u7ed9\u6bcf\u4e2aGaussian\u4e00\u4e2akey=depth+ID\uff0c\u7136\u540e\u5c06Gaussians\u8fdb\u884cGPU Radix Sort\uff0c\u5f97\u5230\u4e86\u6bcf\u4e2atile\u91cc\u6839\u636edepth\u6392\u5e8f\u7684Gaussian\u5217\u8868\uff0c \u6bcf\u4e2atile\u5206\u914dthread block\uff0c\u6bcf\u4e2ablock\u5148load\u5230shared memory\uff0c\u5bf9\u4e8e\u6bcf\u4e2apixel\uff0c\u7d2f\u79efcolor\u548calpha\uff0c\u8fbe\u5230\u9971\u548c\u65f6\u505c\u6b62\u3002","title":"Optimized GPU Rendering"},{"location":"other_categories/others/generate_model_score_matching/","text":"Generative Modeling by Estimating Gradients of the Data Distribution \u8fd9\u7bc7paper\u8ba8\u8bba\u7684\u662fscore-based generative model \u7684\u65b0\u6539\u8fdb\u3002 \u80cc\u666f\u77e5\u8bc6 Score-Based Generative Model \u57fa\u4e8eGAN\u6216\u8005VAE\u7684generative model \u5f80\u5f80\u901a\u8fc7\u67d0\u4e9bfeature(random noise)\u76f4\u63a5\u751f\u6210generated image, generation/decoder \u7f51\u7edc\u76f4\u63a5\u62df\u5408\u7684\u662f p(\\bold{x}|z) . score-based generative model \u5219\u4f1a\u9884\u6d4b \u5f53\u524d\u6570\u636e\u4e3a\u771f\u7684\u6982\u7387\u76f8\u5bf9\u4e8e\u6bcf\u4e00\u4e2a\u50cf\u7d20\u503c\u7684\u68af\u5ea6 \\nabla_\\bold{x} log p(\\bold{x}) .\u4f18\u5316\u76ee\u6807\u4e3a \\frac{1}{2} \\mathbb{E}_{p_{\\text {data }}}\\left[\\left\\|\\mathbf{s}_{\\boldsymbol{\\theta}}(\\mathbf{x})-\\nabla_{\\mathbf{x}} \\log p_{\\text {data }}(\\mathbf{x})\\right\\|_{2}^{2}\\right] \u63a8\u7406\u7684\u65f6\u5019\u4f7f\u7528\u90ce\u4e4b\u4e07\u52a8\u529b\u5b66\u8fed\u4ee3\uff0c\u9010\u6b65\u66f4\u65b0\u66f4\u597d\u7684\u751f\u6210\u7ed3\u679c\u3002 \\tilde{\\mathbf{x}}_{t}=\\tilde{\\mathbf{x}}_{t-1}+\\frac{\\epsilon}{2} \\nabla_{\\mathbf{x}} \\log p\\left(\\tilde{\\mathbf{x}}_{t-1}\\right)+\\sqrt{\\epsilon} \\mathbf{z}_{t} \u5176\u4e2d z_t \u4ece\u6807\u51c6\u6b63\u6001\u91c7\u6837\uff0c \\epsilon \u4e3a\u6b65\u957f. Sliced score matching \u8fd9\u4e2a\u4f18\u5316\u7684\u4e3b\u8981\u96be\u70b9\u5728\u4e8eGround Truth p_{data}(x) \u96be\u4ee5\u6c42\u51fa\uff0c\u672c\u6587\u4f5c\u8005\u5728\u53e6\u4e00\u7bc7paper\u4e2d\u901a\u8fc7\u4e00\u7cfb\u5217\u7684\u63a8\u5bfc\u7ed9\u51fa\u4e86\u4e00\u79cd\u8fd1\u4f3c\u6c42\u89e3\u65b9\u6cd5 pdf Denoising score matching \u8fd9\u4e2a\u6c42\u89e3\u65b9\u5f0f\u7684\u7279\u70b9\u5728\u4e8e\u7528\u9ad8\u65af\u6270\u52a8\u7684data sample\u6765\u8fd1\u4f3c\u6570\u636e\u96c6\u4e2d\u7684 p_{data}(x) , \u6709\u4e00\u7bc7 paper \u8868\u660e\u4e86\u5728\u8fd9\u4e2asetting\u4e0b,\u5b66\u4e60\u9ad8\u65af\u6270\u52a8\u4e0b\u7684\u6982\u7387\u68af\u5ea6\uff0c\u8ba1\u7b97\u65b9\u6cd5\u7b49\u540c\u4e8e\u6570\u636e\u7684denoising\u3002 \u4e3b\u8981\u601d\u8def \u4f7f\u7528Denoising score matching\u7684\u65f6\u5019\uff0c\u7b2c\u4e00\u4e2a\u95ee\u9898\u5728\u4e8e\u5927\u90e8\u5206\u7684\u6570\u636e\u7a7a\u95f4\u90fd\u662f\u65e0\u610f\u4e49\u7684\uff0c\u53ea\u6709\u5c11\u90e8\u5206\u7684\u6570\u636e\u7a7a\u95f4\u6709\u610f\u4e49\u3002\u4e14\u8fd8\u53ef\u4ee5\u5c06\u6570\u636e\u5efa\u6a21\u4e3a\u591a\u6a21\u6001\u7684\u6982\u7387\u5206\u5e03\u3002 \u56e0\u800c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e24\u4e2a\u70b9\uff0c\u7b2c\u4e00\u4e2a\u662fnoise conditioned network,\u53e6\u4e00\u4e2a\u662f annealed Langevin dynamics Code \u4ee3\u7801\u4e0d\u7b97\u590d\u6742\u3002 \u6570\u636e\u6270\u52a8\u4e0eTarget\u751f\u6210 : def anneal_dsm_score_estimation(scorenet, samples, labels, sigmas, anneal_power=2.): used_sigmas = sigmas[labels].view(samples.shape[0], *([1] * len(samples.shape[1:]))) perturbed_samples = samples + torch.randn_like(samples) * used_sigmas target = - 1 / (used_sigmas ** 2) * (perturbed_samples - samples) scores = scorenet(perturbed_samples, labels) #[B, 3, H, W] target = target.view(target.shape[0], -1) scores = scores.view(scores.shape[0], -1) loss = 1 / 2. * ((scores - target) ** 2).sum(dim=-1) * used_sigmas.squeeze() ** anneal_power return loss.mean(dim=0)","title":"Generative Modeling by Estimating Gradients of the Data Distribution"},{"location":"other_categories/others/generate_model_score_matching/#generative-modeling-by-estimating-gradients-of-the-data-distribution","text":"\u8fd9\u7bc7paper\u8ba8\u8bba\u7684\u662fscore-based generative model \u7684\u65b0\u6539\u8fdb\u3002","title":"Generative Modeling by Estimating Gradients of the Data Distribution"},{"location":"other_categories/others/generate_model_score_matching/#_1","text":"","title":"\u80cc\u666f\u77e5\u8bc6"},{"location":"other_categories/others/generate_model_score_matching/#score-based-generative-model","text":"\u57fa\u4e8eGAN\u6216\u8005VAE\u7684generative model \u5f80\u5f80\u901a\u8fc7\u67d0\u4e9bfeature(random noise)\u76f4\u63a5\u751f\u6210generated image, generation/decoder \u7f51\u7edc\u76f4\u63a5\u62df\u5408\u7684\u662f p(\\bold{x}|z) . score-based generative model \u5219\u4f1a\u9884\u6d4b \u5f53\u524d\u6570\u636e\u4e3a\u771f\u7684\u6982\u7387\u76f8\u5bf9\u4e8e\u6bcf\u4e00\u4e2a\u50cf\u7d20\u503c\u7684\u68af\u5ea6 \\nabla_\\bold{x} log p(\\bold{x}) .\u4f18\u5316\u76ee\u6807\u4e3a \\frac{1}{2} \\mathbb{E}_{p_{\\text {data }}}\\left[\\left\\|\\mathbf{s}_{\\boldsymbol{\\theta}}(\\mathbf{x})-\\nabla_{\\mathbf{x}} \\log p_{\\text {data }}(\\mathbf{x})\\right\\|_{2}^{2}\\right] \u63a8\u7406\u7684\u65f6\u5019\u4f7f\u7528\u90ce\u4e4b\u4e07\u52a8\u529b\u5b66\u8fed\u4ee3\uff0c\u9010\u6b65\u66f4\u65b0\u66f4\u597d\u7684\u751f\u6210\u7ed3\u679c\u3002 \\tilde{\\mathbf{x}}_{t}=\\tilde{\\mathbf{x}}_{t-1}+\\frac{\\epsilon}{2} \\nabla_{\\mathbf{x}} \\log p\\left(\\tilde{\\mathbf{x}}_{t-1}\\right)+\\sqrt{\\epsilon} \\mathbf{z}_{t} \u5176\u4e2d z_t \u4ece\u6807\u51c6\u6b63\u6001\u91c7\u6837\uff0c \\epsilon \u4e3a\u6b65\u957f.","title":"Score-Based Generative Model"},{"location":"other_categories/others/generate_model_score_matching/#sliced-score-matching","text":"\u8fd9\u4e2a\u4f18\u5316\u7684\u4e3b\u8981\u96be\u70b9\u5728\u4e8eGround Truth p_{data}(x) \u96be\u4ee5\u6c42\u51fa\uff0c\u672c\u6587\u4f5c\u8005\u5728\u53e6\u4e00\u7bc7paper\u4e2d\u901a\u8fc7\u4e00\u7cfb\u5217\u7684\u63a8\u5bfc\u7ed9\u51fa\u4e86\u4e00\u79cd\u8fd1\u4f3c\u6c42\u89e3\u65b9\u6cd5 pdf","title":"Sliced score matching"},{"location":"other_categories/others/generate_model_score_matching/#denoising-score-matching","text":"\u8fd9\u4e2a\u6c42\u89e3\u65b9\u5f0f\u7684\u7279\u70b9\u5728\u4e8e\u7528\u9ad8\u65af\u6270\u52a8\u7684data sample\u6765\u8fd1\u4f3c\u6570\u636e\u96c6\u4e2d\u7684 p_{data}(x) , \u6709\u4e00\u7bc7 paper \u8868\u660e\u4e86\u5728\u8fd9\u4e2asetting\u4e0b,\u5b66\u4e60\u9ad8\u65af\u6270\u52a8\u4e0b\u7684\u6982\u7387\u68af\u5ea6\uff0c\u8ba1\u7b97\u65b9\u6cd5\u7b49\u540c\u4e8e\u6570\u636e\u7684denoising\u3002","title":"Denoising score matching"},{"location":"other_categories/others/generate_model_score_matching/#_2","text":"\u4f7f\u7528Denoising score matching\u7684\u65f6\u5019\uff0c\u7b2c\u4e00\u4e2a\u95ee\u9898\u5728\u4e8e\u5927\u90e8\u5206\u7684\u6570\u636e\u7a7a\u95f4\u90fd\u662f\u65e0\u610f\u4e49\u7684\uff0c\u53ea\u6709\u5c11\u90e8\u5206\u7684\u6570\u636e\u7a7a\u95f4\u6709\u610f\u4e49\u3002\u4e14\u8fd8\u53ef\u4ee5\u5c06\u6570\u636e\u5efa\u6a21\u4e3a\u591a\u6a21\u6001\u7684\u6982\u7387\u5206\u5e03\u3002 \u56e0\u800c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e24\u4e2a\u70b9\uff0c\u7b2c\u4e00\u4e2a\u662fnoise conditioned network,\u53e6\u4e00\u4e2a\u662f annealed Langevin dynamics","title":"\u4e3b\u8981\u601d\u8def"},{"location":"other_categories/others/generate_model_score_matching/#code","text":"\u4ee3\u7801\u4e0d\u7b97\u590d\u6742\u3002 \u6570\u636e\u6270\u52a8\u4e0eTarget\u751f\u6210 : def anneal_dsm_score_estimation(scorenet, samples, labels, sigmas, anneal_power=2.): used_sigmas = sigmas[labels].view(samples.shape[0], *([1] * len(samples.shape[1:]))) perturbed_samples = samples + torch.randn_like(samples) * used_sigmas target = - 1 / (used_sigmas ** 2) * (perturbed_samples - samples) scores = scorenet(perturbed_samples, labels) #[B, 3, H, W] target = target.view(target.shape[0], -1) scores = scores.view(scores.shape[0], -1) loss = 1 / 2. * ((scores - target) ** 2).sum(dim=-1) * used_sigmas.squeeze() ** anneal_power return loss.mean(dim=0)","title":"Code"},{"location":"other_categories/others/meta_updater_tracking/","text":"High-Performance Long-Term Tracking with Meta-Updater \u957f\u65f6\u8ddf\u8e2a\u662f\u4e00\u4e2a\u4e0e\u77ed\u671f\u8ddf\u8e2a\u622a\u7136\u4e0d\u540c\u7684\u9886\u57df\uff0c\u91cd\u70b9\u5728\u4e8e\u5b9e\u73b0\u957f\u65f6\u95f4\u3001\u4e0d\u6f02\u79fb\u3001\u7269\u4f53\u56fe\u50cf\u53d8\u5316\u7684\u60c5\u51b5\u3002\u8fd9\u91cc\u7ed3\u5408\u8fd9\u7bc7paper\u8bf4\u660e\u4e00\u4e0b\u76f8\u5173\u505a\u6cd5. Priors \u56fe\u50cf\u4e0a\u957f\u65f6\u95f4\u8ddf\u8e2a\u4e0e\u77ed\u65f6\u95f4\u8ddf\u8e2a\u76f8\u6bd4\u4e00\u4e2a\u7a81\u51fa\u7684\u96be\u70b9\u5728\u4e8e\u7269\u4f53\u7684\u5f62\u6001\u3001\u5149\u7167\u7b49\u4f1a\u6301\u7eed\u5730\u53d8\u5316, \u56e0\u800c\u7406\u8bba\u4e0a\u6765\u8bf4\u9700\u8981\u65f6\u4e0d\u65f6\u6216\u8005\u6301\u7eed\u7684\u66f4\u65b0\u88ab\u8ddf\u8e2a\u7684\u76ee\u6807\u7684template / embedding. \u4f46\u662f\u5982\u679c\u6bcf\u65f6\u6bcf\u523b\u90fd\u8fdb\u884c\u66f4\u65b0\uff0c\u5219\u8ddf\u8e2a\u7cfb\u7edf\u53ef\u80fd\u4f1a\u968f\u7740\u566a\u97f3\u7684\u6301\u7eed\u52a0\u5165\u800c\u6f02\u79fb\u3002\u56e0\u800c\u5bf9\u4e8etemplate\u7684\u7ba1\u7406\u662f\u4e00\u4e2a\u6bd4\u8f83\u91cd\u8981\u7684\u8bdd\u9898. \u672c\u6587\u4e4b\u524d\u7684 MBMD\u67b6\u6784 pdf \u5b83\u989d\u5916\u751f\u6210\u4e00\u4e2aconfidence score\u6765\u5224\u65ad\u662f\u5426\u9700\u8981\u8fdb\u884c\u5168\u5c40\u641c\u7d22\u5e76\u66f4\u65b0template\uff0c\u5426\u5219\u7ee7\u7eed\u5c40\u90e8\u641c\u7d22 Structures MBMD\u7684\u95ee\u9898\u5728\u4e8e\u867d\u7136 long-term tracking \u662f\u4e0e short-term tracking \u4e0d\u540c\uff0c\u4f46\u662f\u76ee\u524d\u9636\u6bb5\u5982\u679c\u80fd\u590d\u7528 short-term tracking \u5f00\u53d1\u7684tracker\u53bb\u63d0\u5347tracking\u80fd\u529b\uff0c\u90a3\u4e48\u8fd8\u662f\u80fd\u5bf9\u6027\u80fd\u6709\u5f88\u5927\u63d0\u5347\u7684\u3002\u672c\u6587\u63d0\u51fa\u4e86\u5168\u65b0\u7684\u6846\u67b6\u628a short-term tracking\u8fdb\u884c\u4e86\u5d4c\u5957.\u5e76\u4f7f\u7528meta-updater \u6307\u5f15\u5c40\u90e8tracker\u7684\u66f4\u65b0. \u6574\u4f53\u6846\u67b6\u548cMBMD\u4e00\u6837\uff0c\u6709\u4e00\u4e2averifier\u786e\u8ba4\u5f53\u524d\u5c0f\u533a\u57df\u5185\u7269\u4f53\u7684\u5b58\u5728\uff0c\u4e14\u6b63\u786etracked\u3002\u8ddf\u8e2a\u7b97\u6cd5\u7531\u5c40\u90e8\u7684local-tracker\u51b3\u5b9a. \u5728local tracking\u90e8\u5206\uff0c\u6709\u4e00\u4e2ameta-updater, \u4f7f\u7528LSTM\u5b58\u7740\u65f6\u5e8f\u4fe1\u606f\u6307\u5f15 local tracker\u7684\u66f4\u65b0. Meta-updater\u7684\u8f93\u5165\u662f \u6846\u7684\u4f4d\u7f6e\u5927\u5c0f\uff0cResponse Map, \u4ee5\u53ca\u5b83\u4e0e template\u7684\u76f8\u4f3c\u5ea6\u3002","title":"High-Performance Long-Term Tracking with Meta-Updater"},{"location":"other_categories/others/meta_updater_tracking/#high-performance-long-term-tracking-with-meta-updater","text":"\u957f\u65f6\u8ddf\u8e2a\u662f\u4e00\u4e2a\u4e0e\u77ed\u671f\u8ddf\u8e2a\u622a\u7136\u4e0d\u540c\u7684\u9886\u57df\uff0c\u91cd\u70b9\u5728\u4e8e\u5b9e\u73b0\u957f\u65f6\u95f4\u3001\u4e0d\u6f02\u79fb\u3001\u7269\u4f53\u56fe\u50cf\u53d8\u5316\u7684\u60c5\u51b5\u3002\u8fd9\u91cc\u7ed3\u5408\u8fd9\u7bc7paper\u8bf4\u660e\u4e00\u4e0b\u76f8\u5173\u505a\u6cd5.","title":"High-Performance Long-Term Tracking with Meta-Updater"},{"location":"other_categories/others/meta_updater_tracking/#priors","text":"\u56fe\u50cf\u4e0a\u957f\u65f6\u95f4\u8ddf\u8e2a\u4e0e\u77ed\u65f6\u95f4\u8ddf\u8e2a\u76f8\u6bd4\u4e00\u4e2a\u7a81\u51fa\u7684\u96be\u70b9\u5728\u4e8e\u7269\u4f53\u7684\u5f62\u6001\u3001\u5149\u7167\u7b49\u4f1a\u6301\u7eed\u5730\u53d8\u5316, \u56e0\u800c\u7406\u8bba\u4e0a\u6765\u8bf4\u9700\u8981\u65f6\u4e0d\u65f6\u6216\u8005\u6301\u7eed\u7684\u66f4\u65b0\u88ab\u8ddf\u8e2a\u7684\u76ee\u6807\u7684template / embedding. \u4f46\u662f\u5982\u679c\u6bcf\u65f6\u6bcf\u523b\u90fd\u8fdb\u884c\u66f4\u65b0\uff0c\u5219\u8ddf\u8e2a\u7cfb\u7edf\u53ef\u80fd\u4f1a\u968f\u7740\u566a\u97f3\u7684\u6301\u7eed\u52a0\u5165\u800c\u6f02\u79fb\u3002\u56e0\u800c\u5bf9\u4e8etemplate\u7684\u7ba1\u7406\u662f\u4e00\u4e2a\u6bd4\u8f83\u91cd\u8981\u7684\u8bdd\u9898. \u672c\u6587\u4e4b\u524d\u7684 MBMD\u67b6\u6784 pdf \u5b83\u989d\u5916\u751f\u6210\u4e00\u4e2aconfidence score\u6765\u5224\u65ad\u662f\u5426\u9700\u8981\u8fdb\u884c\u5168\u5c40\u641c\u7d22\u5e76\u66f4\u65b0template\uff0c\u5426\u5219\u7ee7\u7eed\u5c40\u90e8\u641c\u7d22","title":"Priors"},{"location":"other_categories/others/meta_updater_tracking/#structures","text":"MBMD\u7684\u95ee\u9898\u5728\u4e8e\u867d\u7136 long-term tracking \u662f\u4e0e short-term tracking \u4e0d\u540c\uff0c\u4f46\u662f\u76ee\u524d\u9636\u6bb5\u5982\u679c\u80fd\u590d\u7528 short-term tracking \u5f00\u53d1\u7684tracker\u53bb\u63d0\u5347tracking\u80fd\u529b\uff0c\u90a3\u4e48\u8fd8\u662f\u80fd\u5bf9\u6027\u80fd\u6709\u5f88\u5927\u63d0\u5347\u7684\u3002\u672c\u6587\u63d0\u51fa\u4e86\u5168\u65b0\u7684\u6846\u67b6\u628a short-term tracking\u8fdb\u884c\u4e86\u5d4c\u5957.\u5e76\u4f7f\u7528meta-updater \u6307\u5f15\u5c40\u90e8tracker\u7684\u66f4\u65b0. \u6574\u4f53\u6846\u67b6\u548cMBMD\u4e00\u6837\uff0c\u6709\u4e00\u4e2averifier\u786e\u8ba4\u5f53\u524d\u5c0f\u533a\u57df\u5185\u7269\u4f53\u7684\u5b58\u5728\uff0c\u4e14\u6b63\u786etracked\u3002\u8ddf\u8e2a\u7b97\u6cd5\u7531\u5c40\u90e8\u7684local-tracker\u51b3\u5b9a. \u5728local tracking\u90e8\u5206\uff0c\u6709\u4e00\u4e2ameta-updater, \u4f7f\u7528LSTM\u5b58\u7740\u65f6\u5e8f\u4fe1\u606f\u6307\u5f15 local tracker\u7684\u66f4\u65b0. Meta-updater\u7684\u8f93\u5165\u662f \u6846\u7684\u4f4d\u7f6e\u5927\u5c0f\uff0cResponse Map, \u4ee5\u53ca\u5b83\u4e0e template\u7684\u76f8\u4f3c\u5ea6\u3002","title":"Structures"},{"location":"other_categories/others/mixmatch/","text":"MixMatch: A Holistic Approach to Semi-Supervised Learning \u8fd9\u7bc7paper\u4ecb\u7ecd\u4e86MixMatch\u7b97\u6cd5\uff0c\u5176\u662f\u4e00\u4e2a\u6027\u80fd\u5f88\u5f3a\u7684 semi-supervised learning\u65b9\u6cd5\u3002\u5728\u8fd9\u91cc\u7684semi-supervised\u6307\u7684\u662f\u4f7f\u7528\u8f83\u5c11\u7684labelled data\u4ee5\u53ca\u5927\u91cf\u7684unlabel data\u8fdb\u884c\u8bad\u7ec3\u3002 \u6709\u4e00\u4e2a\u6bd4\u8f83\u6e05\u6670\u7684 \u77e5\u4e4e\u6587\u7ae0 \uff0c\u4ecb\u7ecd\u4e86\u8fd9\u4e2a\u7b97\u6cd5\u7684\u6765\u9f99\u53bb\u8109. Primary Algorithms \u81ea\u6d3d\u6b63\u5219\u5316 (Consistency Regularization) \u8fd9\u4e2a\u662f\u4e00\u4e2a\u901a\u7528\u7684\u6269\u5c55\u6570\u636e\u7684\u65b9\u6848, \u4e5f\u5728 SSL-RTM3D \u4e2d\u4f7f\u7528\u4e86\uff0c\u4e5f\u5c31\u662f\u4f7f\u7528\u6570\u636e\u589e\u5f3a\u540e\uff0c\u7f51\u7edc\u5bf9\u589e\u5f3a\u540e\u7684\u6570\u636e\u7684\u9884\u6d4b\u5e94\u8be5\u662f\u81ea\u6d3d\u7684\uff0c\u4f5c\u8005\u7528\u8fd9\u4e2a\u65b9\u6cd5\u6765\u8bad\u7ec3\u7f51\u7edc\u5728\u672a\u6807\u6ce8\u6570\u636e\u4e0a\u7684\u6cdb\u7528\u6027. L_{mathcal{U}} = \\frac{1}{L|\\mathcal{U'}|} \\underset{u, q \\in \\mathcal{U'}}{\\sum} ||q - p_{model}(y | u;\\theta)||^2_2 \u8fd9\u91cc\u867d\u7136\u662f\u8981\u6c42\u6982\u7387\u4e0a\u9760\u62e2\uff0c\u4f46\u662f\u4f7f\u7528\u7684\u5e76\u4e0d\u662f\u57fa\u4e8e\u6982\u7387\u503c\u4e0e\u71b5\u7684\u635f\u5931\u51fd\u6570\uff0c\u800c\u662f\u8981\u6c42 L_2 \u635f\u5931\uff0c\u8fd9\u91cc\u7a81\u51fa\u7684\u662f L_2 \u635f\u5931\u5bf9\u5e38\u6570\u4e0d\u654f\u611f\uff0c\u56e0\u800c\u662f\u4e00\u4e2a\u66f4\u5f3a\u7684\u7ea6\u675f. Entropy Minimization \u5bf9\u4e8e\u6ca1\u6709\u6807\u6ce8\u7684\u6570\u636e\uff0c\u4f5c\u8005\u7684\u65b9\u6848\u662f\u8ba9\u5b83\u6570\u636e\u589e\u5f3a\u591a\u6b21(K=2)\uff0c\u5bf9\u8f93\u51fa\u6982\u7387\u77e2\u91cf\u53d6\u5747\u503c\uff0c\u5176\u4e2d\u6982\u7387\u6700\u5927\u503c\u4e3a\u9884\u6d4b\u7684 pseudo-label. \u4f5c\u8005\u6307\u51fa\u9700\u8981\u964d\u4f4e\u9884\u6d4b\u7684\u71b5\u503c\uff0c\u9f13\u52b1\u7f51\u7edc\u505a\u51fa\u5224\u65ad\u3002 \u5b9e\u73b0\u4e0a\u5305\u542b - Label Guessing \\bar q_b = \\frac{1}{K} \\sum_{k=1}^K p_{model}(y|\\hat u_{b,k};\\theta) - Sharpening: q_b := p_i^{\\frac{1}{T}} / \\sum_{j=1}^{L}p_j^{frac{1}{T}} MixUp \u8fd9\u4e2a\u662f\u4e00\u4e2a\u91cd\u8981\u7684regularization \u7b97\u6cd5, \u672c\u7ad9\u5728 bag of freebies for object detection \u4e2d\u6709\u5f15\u5165. \u4f5c\u8005\u8fd9\u91cc\u5bf9labled data, unlabeled data \u5728\u8bad\u7ec3\u7684\u65f6\u5019\u5206\u522b\u8fdb\u884c mixup. \u6846\u67b6 \\begin{aligned} \\mathcal{X}^{\\prime}, \\mathcal{U}^{\\prime} &=\\operatorname{MixMatch}(\\mathcal{X}, \\mathcal{U}, T, K, \\alpha) \\\\ \\mathcal{L}_{\\mathcal{X}} &=\\frac{1}{\\left|\\mathcal{X}^{\\prime}\\right|} \\sum_{x, p \\in \\mathcal{X}^{\\prime}} \\mathrm{H}\\left(p, \\mathrm{p}_{\\text {model }}(y \\mid x ; \\theta)\\right) \\\\ \\mathcal{L}_{\\mathcal{U}} &=\\frac{1}{L\\left|\\mathcal{U}^{\\prime}\\right|} \\sum_{u, q \\in \\mathcal{U}^{\\prime}} \\| q- p_{model}(y \\mid u ; \\theta) \\|_{2}^{2} \\\\ \\mathcal{L} &=\\mathcal{L}_{\\mathcal{X}}+\\lambda_{\\mathcal{U}} \\mathcal{L}_{\\mathcal{U}} \\end{aligned}","title":"MixMatch: A Holistic Approach to Semi-Supervised Learning"},{"location":"other_categories/others/mixmatch/#mixmatch-a-holistic-approach-to-semi-supervised-learning","text":"\u8fd9\u7bc7paper\u4ecb\u7ecd\u4e86MixMatch\u7b97\u6cd5\uff0c\u5176\u662f\u4e00\u4e2a\u6027\u80fd\u5f88\u5f3a\u7684 semi-supervised learning\u65b9\u6cd5\u3002\u5728\u8fd9\u91cc\u7684semi-supervised\u6307\u7684\u662f\u4f7f\u7528\u8f83\u5c11\u7684labelled data\u4ee5\u53ca\u5927\u91cf\u7684unlabel data\u8fdb\u884c\u8bad\u7ec3\u3002 \u6709\u4e00\u4e2a\u6bd4\u8f83\u6e05\u6670\u7684 \u77e5\u4e4e\u6587\u7ae0 \uff0c\u4ecb\u7ecd\u4e86\u8fd9\u4e2a\u7b97\u6cd5\u7684\u6765\u9f99\u53bb\u8109.","title":"MixMatch: A Holistic Approach to Semi-Supervised Learning"},{"location":"other_categories/others/mixmatch/#primary-algorithms","text":"","title":"Primary Algorithms"},{"location":"other_categories/others/mixmatch/#consistency-regularization","text":"\u8fd9\u4e2a\u662f\u4e00\u4e2a\u901a\u7528\u7684\u6269\u5c55\u6570\u636e\u7684\u65b9\u6848, \u4e5f\u5728 SSL-RTM3D \u4e2d\u4f7f\u7528\u4e86\uff0c\u4e5f\u5c31\u662f\u4f7f\u7528\u6570\u636e\u589e\u5f3a\u540e\uff0c\u7f51\u7edc\u5bf9\u589e\u5f3a\u540e\u7684\u6570\u636e\u7684\u9884\u6d4b\u5e94\u8be5\u662f\u81ea\u6d3d\u7684\uff0c\u4f5c\u8005\u7528\u8fd9\u4e2a\u65b9\u6cd5\u6765\u8bad\u7ec3\u7f51\u7edc\u5728\u672a\u6807\u6ce8\u6570\u636e\u4e0a\u7684\u6cdb\u7528\u6027. L_{mathcal{U}} = \\frac{1}{L|\\mathcal{U'}|} \\underset{u, q \\in \\mathcal{U'}}{\\sum} ||q - p_{model}(y | u;\\theta)||^2_2 \u8fd9\u91cc\u867d\u7136\u662f\u8981\u6c42\u6982\u7387\u4e0a\u9760\u62e2\uff0c\u4f46\u662f\u4f7f\u7528\u7684\u5e76\u4e0d\u662f\u57fa\u4e8e\u6982\u7387\u503c\u4e0e\u71b5\u7684\u635f\u5931\u51fd\u6570\uff0c\u800c\u662f\u8981\u6c42 L_2 \u635f\u5931\uff0c\u8fd9\u91cc\u7a81\u51fa\u7684\u662f L_2 \u635f\u5931\u5bf9\u5e38\u6570\u4e0d\u654f\u611f\uff0c\u56e0\u800c\u662f\u4e00\u4e2a\u66f4\u5f3a\u7684\u7ea6\u675f.","title":"\u81ea\u6d3d\u6b63\u5219\u5316 (Consistency Regularization)"},{"location":"other_categories/others/mixmatch/#entropy-minimization","text":"\u5bf9\u4e8e\u6ca1\u6709\u6807\u6ce8\u7684\u6570\u636e\uff0c\u4f5c\u8005\u7684\u65b9\u6848\u662f\u8ba9\u5b83\u6570\u636e\u589e\u5f3a\u591a\u6b21(K=2)\uff0c\u5bf9\u8f93\u51fa\u6982\u7387\u77e2\u91cf\u53d6\u5747\u503c\uff0c\u5176\u4e2d\u6982\u7387\u6700\u5927\u503c\u4e3a\u9884\u6d4b\u7684 pseudo-label. \u4f5c\u8005\u6307\u51fa\u9700\u8981\u964d\u4f4e\u9884\u6d4b\u7684\u71b5\u503c\uff0c\u9f13\u52b1\u7f51\u7edc\u505a\u51fa\u5224\u65ad\u3002 \u5b9e\u73b0\u4e0a\u5305\u542b - Label Guessing \\bar q_b = \\frac{1}{K} \\sum_{k=1}^K p_{model}(y|\\hat u_{b,k};\\theta) - Sharpening: q_b := p_i^{\\frac{1}{T}} / \\sum_{j=1}^{L}p_j^{frac{1}{T}}","title":"Entropy Minimization"},{"location":"other_categories/others/mixmatch/#mixup","text":"\u8fd9\u4e2a\u662f\u4e00\u4e2a\u91cd\u8981\u7684regularization \u7b97\u6cd5, \u672c\u7ad9\u5728 bag of freebies for object detection \u4e2d\u6709\u5f15\u5165. \u4f5c\u8005\u8fd9\u91cc\u5bf9labled data, unlabeled data \u5728\u8bad\u7ec3\u7684\u65f6\u5019\u5206\u522b\u8fdb\u884c mixup.","title":"MixUp"},{"location":"other_categories/others/mixmatch/#_1","text":"\\begin{aligned} \\mathcal{X}^{\\prime}, \\mathcal{U}^{\\prime} &=\\operatorname{MixMatch}(\\mathcal{X}, \\mathcal{U}, T, K, \\alpha) \\\\ \\mathcal{L}_{\\mathcal{X}} &=\\frac{1}{\\left|\\mathcal{X}^{\\prime}\\right|} \\sum_{x, p \\in \\mathcal{X}^{\\prime}} \\mathrm{H}\\left(p, \\mathrm{p}_{\\text {model }}(y \\mid x ; \\theta)\\right) \\\\ \\mathcal{L}_{\\mathcal{U}} &=\\frac{1}{L\\left|\\mathcal{U}^{\\prime}\\right|} \\sum_{u, q \\in \\mathcal{U}^{\\prime}} \\| q- p_{model}(y \\mid u ; \\theta) \\|_{2}^{2} \\\\ \\mathcal{L} &=\\mathcal{L}_{\\mathcal{X}}+\\lambda_{\\mathcal{U}} \\mathcal{L}_{\\mathcal{U}} \\end{aligned}","title":"\u6846\u67b6"},{"location":"other_categories/others/monodepth_collections/","text":"Collections on Monodepth (unsupervised) MonoDepth2 pdf code MonoDepth2\u662f\u975e\u76d1\u7763\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u7684\u4e00\u4e2aBaseline\uff0c \u4e3b\u8981\u51e0\u4e2a\u601d\u8def: \u7b80\u5355\u7684res18\u4ee5\u53cadecoder\u51fa\u6df1\u5ea6\u4f30\u8ba1\u7ed3\u679c. \u56fe\u7247\u5e76\u63a5\u76f4\u63a5\u8f93\u51fa\u76f8\u5bf9pose \u7528\u524d\u4e00\u5e27\u6216\u8005\u540e\u4e00\u5e27\u6216\u8005\u53cc\u76ee\u7684\u56fe\u7247\u91cd\u5efa\u5904\u5f53\u524d\u5e27. \u91cd\u5efaloss\u4f7f\u7528 SSIM, \u4e14\u9009\u62e9\u91cd\u5efa\u7684min\u7684loss\uff0c\u91cd\u5efa\u635f\u5931\u8fc7\u5927\u7684\u906e\u6321\u90e8\u5206\u88ab\u6ee4\u6389\u4e86. Self-Supervised Monocular Depth Hints pdf code \u8fd9\u7bc7paper\u4ee5\u524d\u9762monodepth2\u7684\u6587\u7ae0\u4e3a\u57fa\u7840\uff0c\u7740\u91cd\u63d0\u5347\u4e86\u5728\u4f7f\u7528stereo pairs\u8fdb\u884c\u81ea\u76d1\u7763\u6df1\u5ea6\u8bad\u7ec3\u65f6\u7684\u8bad\u7ec3\u7cbe\u5ea6\u3002 \u6839\u636emonodepth2, \u6bcf\u4e2a\u50cf\u7d20\u7684\u635f\u5931\u51fd\u6570: l_{r}\\left(d_{i}\\right)=\\alpha \\frac{1-\\operatorname{SSIM}\\left(I_{i}, \\tilde{I}_{i}\\right)}{2}+(1-\\alpha)\\left|I_{i}-\\tilde{I}_{i}\\right| \u8fd9\u91cc\u878d\u5165\u4e86\u53cc\u76ee\u5339\u914d\u7684\u7ed3\u679c\uff0c\u5728\u5355\u76ee\u9884\u6d4b\u6548\u679c\u6bd4\u8f83\u5dee\u7684\u50cf\u7d20\u7ed9\u4e88\u53cc\u76ee\u7684\u76d1\u7763: l_{\\text {ours }}\\left(d_{i}\\right)=\\left\\{\\begin{array}{ll} l_{r}\\left(d_{i}\\right)+l_{s}^{\\log L_{1}}\\left(d_{i}, h_{i}\\right) & \\text { if } l_{r}\\left(h_{i}\\right)<l_{r}\\left(d_{i}\\right) \\\\ l_{r}\\left(d_{i}\\right) & \\text { otherwise } \\end{array}\\right. Full Surround Monodepth from Multiple Cameras pdf \u63d0\u51fa\u505a\u591a\u6444\u50cf\u673a\u7684\u6df1\u5ea6\u4f30\u8ba1 \u8003\u8651\u65f6\u5e8f+\u7a7a\u95f4\u4e24\u4e2a\u65b9\u5411\u7684consistency \u9700\u8981self-occlusion mask\u53bb\u6389\u76f8\u673a\u5185\u8f66\u4f53\u76f8\u5173\u7684\u90e8\u5206\uff0c\u9700\u8981non-overlapping areas\u53bb\u9664\u76f8\u673a\u4e4b\u95f4\u4e0d\u91cd\u53e0\u7684\u90e8\u5206. Towards Good Practice for CNN-Based Monocular Depth Estimation pdf code Monocular Depth Prediction through Continuous 3D Loss pdf code","title":"Collections on Monodepth (unsupervised)"},{"location":"other_categories/others/monodepth_collections/#collections-on-monodepth-unsupervised","text":"","title":"Collections on Monodepth (unsupervised)"},{"location":"other_categories/others/monodepth_collections/#monodepth2","text":"pdf code MonoDepth2\u662f\u975e\u76d1\u7763\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u7684\u4e00\u4e2aBaseline\uff0c \u4e3b\u8981\u51e0\u4e2a\u601d\u8def: \u7b80\u5355\u7684res18\u4ee5\u53cadecoder\u51fa\u6df1\u5ea6\u4f30\u8ba1\u7ed3\u679c. \u56fe\u7247\u5e76\u63a5\u76f4\u63a5\u8f93\u51fa\u76f8\u5bf9pose \u7528\u524d\u4e00\u5e27\u6216\u8005\u540e\u4e00\u5e27\u6216\u8005\u53cc\u76ee\u7684\u56fe\u7247\u91cd\u5efa\u5904\u5f53\u524d\u5e27. \u91cd\u5efaloss\u4f7f\u7528 SSIM, \u4e14\u9009\u62e9\u91cd\u5efa\u7684min\u7684loss\uff0c\u91cd\u5efa\u635f\u5931\u8fc7\u5927\u7684\u906e\u6321\u90e8\u5206\u88ab\u6ee4\u6389\u4e86.","title":"MonoDepth2"},{"location":"other_categories/others/monodepth_collections/#self-supervised-monocular-depth-hints","text":"pdf code \u8fd9\u7bc7paper\u4ee5\u524d\u9762monodepth2\u7684\u6587\u7ae0\u4e3a\u57fa\u7840\uff0c\u7740\u91cd\u63d0\u5347\u4e86\u5728\u4f7f\u7528stereo pairs\u8fdb\u884c\u81ea\u76d1\u7763\u6df1\u5ea6\u8bad\u7ec3\u65f6\u7684\u8bad\u7ec3\u7cbe\u5ea6\u3002 \u6839\u636emonodepth2, \u6bcf\u4e2a\u50cf\u7d20\u7684\u635f\u5931\u51fd\u6570: l_{r}\\left(d_{i}\\right)=\\alpha \\frac{1-\\operatorname{SSIM}\\left(I_{i}, \\tilde{I}_{i}\\right)}{2}+(1-\\alpha)\\left|I_{i}-\\tilde{I}_{i}\\right| \u8fd9\u91cc\u878d\u5165\u4e86\u53cc\u76ee\u5339\u914d\u7684\u7ed3\u679c\uff0c\u5728\u5355\u76ee\u9884\u6d4b\u6548\u679c\u6bd4\u8f83\u5dee\u7684\u50cf\u7d20\u7ed9\u4e88\u53cc\u76ee\u7684\u76d1\u7763: l_{\\text {ours }}\\left(d_{i}\\right)=\\left\\{\\begin{array}{ll} l_{r}\\left(d_{i}\\right)+l_{s}^{\\log L_{1}}\\left(d_{i}, h_{i}\\right) & \\text { if } l_{r}\\left(h_{i}\\right)<l_{r}\\left(d_{i}\\right) \\\\ l_{r}\\left(d_{i}\\right) & \\text { otherwise } \\end{array}\\right.","title":"Self-Supervised Monocular Depth Hints"},{"location":"other_categories/others/monodepth_collections/#full-surround-monodepth-from-multiple-cameras","text":"pdf \u63d0\u51fa\u505a\u591a\u6444\u50cf\u673a\u7684\u6df1\u5ea6\u4f30\u8ba1 \u8003\u8651\u65f6\u5e8f+\u7a7a\u95f4\u4e24\u4e2a\u65b9\u5411\u7684consistency \u9700\u8981self-occlusion mask\u53bb\u6389\u76f8\u673a\u5185\u8f66\u4f53\u76f8\u5173\u7684\u90e8\u5206\uff0c\u9700\u8981non-overlapping areas\u53bb\u9664\u76f8\u673a\u4e4b\u95f4\u4e0d\u91cd\u53e0\u7684\u90e8\u5206.","title":"Full Surround Monodepth from Multiple Cameras"},{"location":"other_categories/others/monodepth_collections/#towards-good-practice-for-cnn-based-monocular-depth-estimation","text":"pdf code","title":"Towards Good Practice for CNN-Based Monocular Depth Estimation"},{"location":"other_categories/others/monodepth_collections/#monocular-depth-prediction-through-continuous-3d-loss","text":"pdf code","title":"Monocular Depth Prediction through Continuous 3D Loss"},{"location":"other_categories/others/monolayout/","text":"MonoLayout: Amodal scene layout from a single image \u8fd9\u7bc7paper\u505a\u7684\u662f\u4e00\u4e2a\u548cTesla\u6700\u65b0\u516c\u5e03\u7684BEV\u5206\u6790\u65b9\u6848\u6709\u4e00\u5b9a\u76f8\u5173\u6027\u7684\u4efb\u52a1\uff0c\u5373\u4ece\u5355\u76ee\u56fe\u7247\u5230BEV\u5168\u666f\u5206\u5272\u7684\u6620\u5c04\u3002 \u6570\u636e\u6765\u81ea\u4e8eargoverse\u7684\u8def\u7f51\uff0c\u4ee5\u53caKITTI\u7684\u8bed\u4e49\u5206\u5272\u52a0\u70b9\u4e91\u6295\u5f71\u3002 \u540c\u65f6\u8981\u7531supervised training, \u4e5f\u9700\u8981adversarial training\u6765\u63d0\u5347\u89c6\u89c9\u6548\u679c\u3002 \u7f51\u7edc\u4e0a\u4f7f\u7528\u4e00\u4e2aencoder\u5904\u7406\u56fe\u7247\u6570\u636e\uff0c\u7136\u540e\u901a\u8fc7reshape,\u8f93\u51fa\u52a8\u6001\u4e0e\u9759\u6001\u4e24\u4e2amask\uff0c \u8bad\u7ec3\u4e0a\u4e00\u65b9\u9762\u4f7f\u7528\u76d1\u7763\uff0c\u53e6\u4e00\u65b9\u9762\u4f7f\u7528patchGAN\u5bf9\u6297\u8bad\u7ec3, \\begin{array}{l} \\min _{\\phi, \\nu, \\psi, \\theta_{S}, \\theta_{D}} \\mathcal{L}_{s u p}(\\phi, \\nu, \\psi)+\\mathcal{L}_{a d v}(\\phi, \\theta, \\psi)+\\mathcal{L}_{d i s c r}(\\phi, \\nu) \\\\ \\mathcal{L}_{\\text {sup }}=\\sum_{i=1}^{N}\\left\\|\\mathcal{S}_{\\phi, \\nu}\\left(\\mathcal{I}^{i}\\right)-\\mathcal{S}_{g t}^{i}\\right\\|^{2}+\\left\\|\\mathcal{D}_{\\phi, \\psi}\\left(\\mathcal{I}^{i}\\right)-\\mathcal{D}_{g t}^{i}\\right\\|^{2} \\\\ \\mathcal{L}_{a d v}(S, D ; \\phi, \\theta, \\psi)=\\mathbb{E}_{\\theta \\sim p_{\\text {fake }}}\\left[\\left(D\\left(\\theta_{S}\\right)-1\\right)^{2}\\right] \\\\ +\\mathbb{E}_{\\theta \\sim p_{\\text {fake }}}\\left[\\left(D\\left(\\theta_{D}\\right)-1\\right)^{2}\\right] \\\\ \\mathcal{L}_{\\text {discr }}(D ; \\theta)=\\sum_{\\theta \\in\\left\\{\\theta_{D}, \\theta_{S}\\right\\}} \\mathbb{E}_{\\theta \\sim p_{\\text {true }}}\\left[(D(\\theta)-1)^{2}\\right] \\\\ +\\mathbb{E}_{\\theta \\sim p_{\\text {fake }}}\\left[(D(\\theta)-1)^{2}\\right] \\end{array} \u6280\u672f\u7ec6\u8282: \u5206\u6210\u4e24\u4e2a\u5355\u72ec\u7684decoder\u5bf9\u6027\u80fd\u5f71\u54cd\u5f88\u5927 \u76f4\u63a5\u4f7f\u7528UNet\u6548\u679c\u5e76\u4e0d\u597d\uff0c\u8fd8\u662f\u9700\u8981\u5212\u5f52\u4e3a\u5355\u4e00\u77e2\u91cf\u91cd\u7ec4\u540e\u518d\u8f93\u51fa\u3002","title":"MonoLayout: Amodal scene layout from a single image"},{"location":"other_categories/others/monolayout/#monolayout-amodal-scene-layout-from-a-single-image","text":"\u8fd9\u7bc7paper\u505a\u7684\u662f\u4e00\u4e2a\u548cTesla\u6700\u65b0\u516c\u5e03\u7684BEV\u5206\u6790\u65b9\u6848\u6709\u4e00\u5b9a\u76f8\u5173\u6027\u7684\u4efb\u52a1\uff0c\u5373\u4ece\u5355\u76ee\u56fe\u7247\u5230BEV\u5168\u666f\u5206\u5272\u7684\u6620\u5c04\u3002 \u6570\u636e\u6765\u81ea\u4e8eargoverse\u7684\u8def\u7f51\uff0c\u4ee5\u53caKITTI\u7684\u8bed\u4e49\u5206\u5272\u52a0\u70b9\u4e91\u6295\u5f71\u3002 \u540c\u65f6\u8981\u7531supervised training, \u4e5f\u9700\u8981adversarial training\u6765\u63d0\u5347\u89c6\u89c9\u6548\u679c\u3002 \u7f51\u7edc\u4e0a\u4f7f\u7528\u4e00\u4e2aencoder\u5904\u7406\u56fe\u7247\u6570\u636e\uff0c\u7136\u540e\u901a\u8fc7reshape,\u8f93\u51fa\u52a8\u6001\u4e0e\u9759\u6001\u4e24\u4e2amask\uff0c \u8bad\u7ec3\u4e0a\u4e00\u65b9\u9762\u4f7f\u7528\u76d1\u7763\uff0c\u53e6\u4e00\u65b9\u9762\u4f7f\u7528patchGAN\u5bf9\u6297\u8bad\u7ec3, \\begin{array}{l} \\min _{\\phi, \\nu, \\psi, \\theta_{S}, \\theta_{D}} \\mathcal{L}_{s u p}(\\phi, \\nu, \\psi)+\\mathcal{L}_{a d v}(\\phi, \\theta, \\psi)+\\mathcal{L}_{d i s c r}(\\phi, \\nu) \\\\ \\mathcal{L}_{\\text {sup }}=\\sum_{i=1}^{N}\\left\\|\\mathcal{S}_{\\phi, \\nu}\\left(\\mathcal{I}^{i}\\right)-\\mathcal{S}_{g t}^{i}\\right\\|^{2}+\\left\\|\\mathcal{D}_{\\phi, \\psi}\\left(\\mathcal{I}^{i}\\right)-\\mathcal{D}_{g t}^{i}\\right\\|^{2} \\\\ \\mathcal{L}_{a d v}(S, D ; \\phi, \\theta, \\psi)=\\mathbb{E}_{\\theta \\sim p_{\\text {fake }}}\\left[\\left(D\\left(\\theta_{S}\\right)-1\\right)^{2}\\right] \\\\ +\\mathbb{E}_{\\theta \\sim p_{\\text {fake }}}\\left[\\left(D\\left(\\theta_{D}\\right)-1\\right)^{2}\\right] \\\\ \\mathcal{L}_{\\text {discr }}(D ; \\theta)=\\sum_{\\theta \\in\\left\\{\\theta_{D}, \\theta_{S}\\right\\}} \\mathbb{E}_{\\theta \\sim p_{\\text {true }}}\\left[(D(\\theta)-1)^{2}\\right] \\\\ +\\mathbb{E}_{\\theta \\sim p_{\\text {fake }}}\\left[(D(\\theta)-1)^{2}\\right] \\end{array} \u6280\u672f\u7ec6\u8282: \u5206\u6210\u4e24\u4e2a\u5355\u72ec\u7684decoder\u5bf9\u6027\u80fd\u5f71\u54cd\u5f88\u5927 \u76f4\u63a5\u4f7f\u7528UNet\u6548\u679c\u5e76\u4e0d\u597d\uff0c\u8fd8\u662f\u9700\u8981\u5212\u5f52\u4e3a\u5355\u4e00\u77e2\u91cf\u91cd\u7ec4\u540e\u518d\u8f93\u51fa\u3002","title":"MonoLayout: Amodal scene layout from a single image"},{"location":"other_categories/others/objectvivit/","text":"How can objects help action recognition? \u8fd9\u7bc7paper\u8ba8\u8bba\u7684\u662faction recognition\u4e2d\u5982\u4f55\u4f7f\u7528\u7269\u4f53\u4fe1\u606f\uff0c\u9996\u5148\u76f8\u5173\u5de5\u4f5c\u800c\u8a00\uff0c\u4ece\u5e8f\u5217\u56fe\u7247\u4e2d\u7528transformer\u505a\u884c\u4e3a\u8bc6\u522b\uff0c\u9700\u8981\u5f88\u591a\u7684spatial-temporal tokens, \u56e0\u800c\u8fd0\u7b97\u91cf\u6bd4\u8f83\u5927\u3002\u672c\u6587\u7684\u601d\u8def\u5728\u4e8e\uff0c\u5bf9\u4e8e\u884c\u4e3a\u68c0\u6d4b\u800c\u8a00\uff0c\u4eba\u4e0e\u7269\u4f53\u7684\u4ea4\u4e92\u662f\u975e\u5e38\u91cd\u8981\u7684,\u5982\u4f55\u4f7f\u7528\u7269\u4f53\u4fe1\u606f\u6765\u589e\u5f3a\u7279\u5f81\uff0c\u7b80\u5316\u8fd0\u7b97\uff0c\u662f\u672c\u6587\u7684\u4e3b\u8981\u8003\u8651\u70b9\u3002 \u4e3b\u8981\u6846\u67b6 \u7cfb\u7edf\u7684\u8f93\u5165\u5305\u542b\u5e8f\u5217\u56fe\u7247\u4e0e\u5e8f\u5217\u56fe\u7247\u7684\u76ee\u6807\u68c0\u6d4b\u7ed3\u679c\u3002\u4f7f\u7528ViT\u8ba1\u7b97\u5e8f\u5217\u56fe\u7247\u7684patch tokens. \u5f15\u5165 object-guided-token sampling (OGS)\u7528\u4e8e\u7b5b\u9009\u51fa\u524d\u666ftoken\uff0c\u51cf\u8f7b\u8fd0\u7b97\u91cf\uff0c\u5f15\u5165 object-aware attention module (OAM) \u7528\u4e8e\u65f6\u5e8f\u5730\u589e\u5f3atoken\u7279\u5f81\u3002 OGS\u7b97\u6cd5\u5982\u4e0a\u56fe\uff0c\u5229\u7528Center-net\u7684heatmap\u6e32\u67d3\u65b9\u6cd5\uff0c\u628a\u68c0\u6d4b\u51fa\u6765\u7684\u524d\u666f\u7269\u4f53\u4e0d\u533a\u5206\u5177\u4f53\u79cd\u7c7b\u5730\u6e32\u67d3\u4e3a\u5355\u4e00Channel\u7684heatmap, \u6bcf\u4e00\u4e2apatch token\u533a\u57df\u5185heatmap\u5f3a\u5ea6\u4e4b\u548c\u4e3a\u8fd9\u4e2atoken\u5c5e\u4e8e\u524d\u666f\u7684\u5f97\u5206\u3002 \u8fd9\u91cc\u9009\u62e9\u4e00\u4e2a\u56fa\u5b9a\u7684 top K \u65b9\u6cd5\u9009\u62e9\u524d\u666ftoken\uff0c\u5269\u4e0b\u7684\u90e8\u5206\u7528\u4e00\u4e2a\u5f88\u4f4e\u7684\u6bd4\u7387\u968f\u673a\u4e0b\u91c7\u6837\u3002 \u5f97\u5230\u6bcf\u4e00\u65f6\u95f4\u5e27token\u7279\u5f81 z_t .\u4f7f\u7528centernet\u7684heatmap\u6e32\u67d3\u5bf9\u6bcf\u4e00\u5e27\u3001\u6bcf\u4e00\u4e2aobject\u5355\u72ec\u6784\u5efa\u4e00\u4e2aheatmap H_{o,t} \uff0c \\mathbf{w}_{o, t}^l=\\operatorname{MaxPool}\\left(\\operatorname{MLP}\\left(\\widehat{\\mathbf{H}}_{o, t} * \\widehat{\\mathbf{z}}_t^l\\right)\\right) \u628a\u7269\u4f53\u7684token\u548c\u7279\u5f81\u589e\u5f3a\u540e\u8f93\u5165transformer: \\mathbf{y}^l=\\operatorname{MHA}\\left(\\mathbf{z}^l,\\left[\\mathbf{z}^l, \\mathbf{w}^l\\right],\\left[\\mathbf{z}^l, \\mathbf{w}^l\\right]\\right)+\\mathbf{z}^l","title":"How can objects help action recognition?"},{"location":"other_categories/others/objectvivit/#how-can-objects-help-action-recognition","text":"\u8fd9\u7bc7paper\u8ba8\u8bba\u7684\u662faction recognition\u4e2d\u5982\u4f55\u4f7f\u7528\u7269\u4f53\u4fe1\u606f\uff0c\u9996\u5148\u76f8\u5173\u5de5\u4f5c\u800c\u8a00\uff0c\u4ece\u5e8f\u5217\u56fe\u7247\u4e2d\u7528transformer\u505a\u884c\u4e3a\u8bc6\u522b\uff0c\u9700\u8981\u5f88\u591a\u7684spatial-temporal tokens, \u56e0\u800c\u8fd0\u7b97\u91cf\u6bd4\u8f83\u5927\u3002\u672c\u6587\u7684\u601d\u8def\u5728\u4e8e\uff0c\u5bf9\u4e8e\u884c\u4e3a\u68c0\u6d4b\u800c\u8a00\uff0c\u4eba\u4e0e\u7269\u4f53\u7684\u4ea4\u4e92\u662f\u975e\u5e38\u91cd\u8981\u7684,\u5982\u4f55\u4f7f\u7528\u7269\u4f53\u4fe1\u606f\u6765\u589e\u5f3a\u7279\u5f81\uff0c\u7b80\u5316\u8fd0\u7b97\uff0c\u662f\u672c\u6587\u7684\u4e3b\u8981\u8003\u8651\u70b9\u3002","title":"How can objects help action recognition?"},{"location":"other_categories/others/objectvivit/#_1","text":"\u7cfb\u7edf\u7684\u8f93\u5165\u5305\u542b\u5e8f\u5217\u56fe\u7247\u4e0e\u5e8f\u5217\u56fe\u7247\u7684\u76ee\u6807\u68c0\u6d4b\u7ed3\u679c\u3002\u4f7f\u7528ViT\u8ba1\u7b97\u5e8f\u5217\u56fe\u7247\u7684patch tokens. \u5f15\u5165 object-guided-token sampling (OGS)\u7528\u4e8e\u7b5b\u9009\u51fa\u524d\u666ftoken\uff0c\u51cf\u8f7b\u8fd0\u7b97\u91cf\uff0c\u5f15\u5165 object-aware attention module (OAM) \u7528\u4e8e\u65f6\u5e8f\u5730\u589e\u5f3atoken\u7279\u5f81\u3002 OGS\u7b97\u6cd5\u5982\u4e0a\u56fe\uff0c\u5229\u7528Center-net\u7684heatmap\u6e32\u67d3\u65b9\u6cd5\uff0c\u628a\u68c0\u6d4b\u51fa\u6765\u7684\u524d\u666f\u7269\u4f53\u4e0d\u533a\u5206\u5177\u4f53\u79cd\u7c7b\u5730\u6e32\u67d3\u4e3a\u5355\u4e00Channel\u7684heatmap, \u6bcf\u4e00\u4e2apatch token\u533a\u57df\u5185heatmap\u5f3a\u5ea6\u4e4b\u548c\u4e3a\u8fd9\u4e2atoken\u5c5e\u4e8e\u524d\u666f\u7684\u5f97\u5206\u3002 \u8fd9\u91cc\u9009\u62e9\u4e00\u4e2a\u56fa\u5b9a\u7684 top K \u65b9\u6cd5\u9009\u62e9\u524d\u666ftoken\uff0c\u5269\u4e0b\u7684\u90e8\u5206\u7528\u4e00\u4e2a\u5f88\u4f4e\u7684\u6bd4\u7387\u968f\u673a\u4e0b\u91c7\u6837\u3002 \u5f97\u5230\u6bcf\u4e00\u65f6\u95f4\u5e27token\u7279\u5f81 z_t .\u4f7f\u7528centernet\u7684heatmap\u6e32\u67d3\u5bf9\u6bcf\u4e00\u5e27\u3001\u6bcf\u4e00\u4e2aobject\u5355\u72ec\u6784\u5efa\u4e00\u4e2aheatmap H_{o,t} \uff0c \\mathbf{w}_{o, t}^l=\\operatorname{MaxPool}\\left(\\operatorname{MLP}\\left(\\widehat{\\mathbf{H}}_{o, t} * \\widehat{\\mathbf{z}}_t^l\\right)\\right) \u628a\u7269\u4f53\u7684token\u548c\u7279\u5f81\u589e\u5f3a\u540e\u8f93\u5165transformer: \\mathbf{y}^l=\\operatorname{MHA}\\left(\\mathbf{z}^l,\\left[\\mathbf{z}^l, \\mathbf{w}^l\\right],\\left[\\mathbf{z}^l, \\mathbf{w}^l\\right]\\right)+\\mathbf{z}^l","title":"\u4e3b\u8981\u6846\u67b6"},{"location":"other_categories/others/octsqueeze/","text":"OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression \u8fd9\u7bc7paper\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u6fc0\u5149\u96f7\u8fbe\u70b9\u4e91\u7684\u538b\u7f29\u7b97\u6cd5\uff0c\u53ef\u4ee5\u7528\u6765\u538b\u7f29\u70b9\u4e91\u3002\u538b\u7f29\u70b9\u4e91\u4e3b\u8981\u662f\u4e3a\u4e86\u51cf\u5c11\u70b9\u4e91\u7684\u5927\u5c0f\uff0c\u4ece\u800c\u51cf\u5c11\u6fc0\u5149\u96f7\u8fbe\u7684\u5b58\u50a8\u91cf\u4ee5\u53ca\u4f20\u8f93\u901f\u7387\u3002\u800c\u538b\u7f29\u70b9\u4e91\u7684\u65b9\u6cd5\u4e0e\u4f20\u7edf\u7684\u4fe1\u606f\u538b\u7f29\u5bc6\u5207\u76f8\u5173, \u4e5f\u5c31\u662f\u8981\u4f7f\u7528\u4e8c\u8fdb\u5236\u538b\u7f29\u7b97\u6cd5\u5c3d\u53ef\u80fd\u65e0\u635f\u5730\u538b\u7f29\u70b9\u4e91\u4e3a\u9ad8\u4fe1\u606f\u7387\u7684\u4e8c\u8fdb\u5236\u6d41\u3002 \u5982\u4e0a\u56fe\u6240\u793a, \u672c\u6587\u9996\u5148\u5c1d\u8bd5\u7528\u4e00\u4e2a\u516b\u5206\u6811(oct-tree)\u6765\u7f16\u7801\u6574\u4e2a\u70b9\u4e91, \u516b\u53c9\u6811\u4e0a\u7684\u6bcf\u4e00\u4e2a\u70b9\u7684\u4f4d\u7f6e\u90fd\u662fpre-defined\u7684\uff0c\u8fd9\u6837\u5c31\u53ef\u4ee5\u7528\u6bcf\u4e00\u5c42\u4e0a\u9762\u7684occupancy 0-1\u4e8c\u8fdb\u5236\u4e32\u6765\u8fd1\u4f3c\u8868\u8fbe\u6574\u4e2a\u70b9\u4e91\u3002 \u5982\u679c\u6211\u4eec\u91c7\u7528\u4e00\u4e2a\u7b80\u5355\u7684\u538b\u7f29\u7b97\u6cd5\u76f4\u63a5\u5bf9\u6574\u4e2a\u516b\u5206\u6811\u4e0a\u76840-1\u4e8c\u8fdb\u5236\u4e32\u8fdb\u884c\u538b\u7f29\uff0c\u90a3\u4e48\u53ef\u4ee5\u5f97\u5230\u4e00\u4e2a\u6bd4\u8f83\u7a33\u5b9a\u7684\u538b\u7f29\u7387\uff1b\u4e0d\u8fc7\u7531\u4e8e\u8fd9\u6837\u7684\u4e8c\u8fdb\u5236\u4e32\u6ca1\u6709\u592a\u591a\u7684\u89c4\u5f8b\uff0c\u7a00\u758f\u5ea6\u4e0d\u9ad8\uff0c\u4e14\u6ca1\u6709\u89c4\u5f8b\uff0c\u56e0\u800c\u4fe1\u606f\u71b5\u6bd4\u8f83\u5927\uff1b \u672c\u6587\u7684idea: \u7528\u795e\u7ecf\u7f51\u7edc\uff0c\u4f7f\u7528\u4e0a\u4e00\u5c42\u7684\u8282\u70b9\u7684\u7279\u5f81\uff0c\u9884\u6d4b\u4e0b\u4e00\u5c42\u8282\u70b9\u7684occupancy probability. \u5728\u538b\u7f29\u7684\u65f6\u5019\uff0c\u8bb0\u5f55\u4e0bGT occupancy probability\u548cpredicted occupancy probability\u7684\u5dee\u503c(0-1\u5e8f\u5217).\u5982\u679c\u7f51\u8def\u7684\u9884\u6d4b\u51c6\u786e\u7387\u6bd4\u8f83\u9ad8\u7684\u8bdd\uff0c\u8fd9\u4e2a0-1\u5e8f\u5217\u5dee\u503c\u5f88\u591a\u90fd\u4f1a\u662f0\uff0c\u5c06\u4f1a\u662f\u4e00\u4e2a\u5f88\u7a00\u758f\u7684\u5e8f\u5217\uff0c\u7136\u540e\u7528\u4e00\u822c\u7684\u4e8c\u8fdb\u5236\u538b\u7f29\u7b97\u6cd5(\u5982zip, huffman coding)\u7b49\u53ef\u4ee5\u5f97\u5230\u5f88\u9ad8\u7684\u538b\u7f29\u7387. \u5728\u89e3\u538b\u7f29\u7684\u65f6\u5019\uff0c\u8ba9\u7f51\u8def\u5728\u6bcf\u4e00\u5c42\u8fdb\u884c\u6982\u7387\u9884\u6d4b\uff0c\u7136\u540e\u5c06\u5dee\u503c\u8fdb\u884c\u89e3\u7801\uff1b\u5c06\u7f51\u7edc\u7684\u9884\u6d4b\u503c\u4e0e\u89e3\u7801\u503c\u8fdb\u884c\u76f8\u52a0\uff0c\u5c31\u80fd\u65e0\u635f\u5730\u8fd8\u539f\u70b9\u4e91\u538b\u7f29\u65f6\u88ab\u8f93\u5165\u7f51\u7edc\u76840-1\u5e8f\u5217. \u5728\u5c24\u5176\u662f\u9ad8\u5c42\u7684\u65f6\u5019\uff0c\u7a00\u758f\u5e8f\u5217\u7684\u5b58\u50a8\u52a0\u8d77\u6765\u6bd4\u539f\u6765\u7a20\u5bc6\u5e8f\u5217\u7684\u5b58\u50a8\u8981\u66f4\u5c11\uff0c\u5b9e\u73b0\u538b\u7f29\u3002\u800c\u7f51\u7edc\u6743\u91cd\u5219\u662f\u5728\u4e0d\u540c\u7f51\u7edc\u95f4\u901a\u7528\u7684\u3002","title":"OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression"},{"location":"other_categories/others/octsqueeze/#octsqueeze-octree-structured-entropy-model-for-lidar-compression","text":"\u8fd9\u7bc7paper\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u6fc0\u5149\u96f7\u8fbe\u70b9\u4e91\u7684\u538b\u7f29\u7b97\u6cd5\uff0c\u53ef\u4ee5\u7528\u6765\u538b\u7f29\u70b9\u4e91\u3002\u538b\u7f29\u70b9\u4e91\u4e3b\u8981\u662f\u4e3a\u4e86\u51cf\u5c11\u70b9\u4e91\u7684\u5927\u5c0f\uff0c\u4ece\u800c\u51cf\u5c11\u6fc0\u5149\u96f7\u8fbe\u7684\u5b58\u50a8\u91cf\u4ee5\u53ca\u4f20\u8f93\u901f\u7387\u3002\u800c\u538b\u7f29\u70b9\u4e91\u7684\u65b9\u6cd5\u4e0e\u4f20\u7edf\u7684\u4fe1\u606f\u538b\u7f29\u5bc6\u5207\u76f8\u5173, \u4e5f\u5c31\u662f\u8981\u4f7f\u7528\u4e8c\u8fdb\u5236\u538b\u7f29\u7b97\u6cd5\u5c3d\u53ef\u80fd\u65e0\u635f\u5730\u538b\u7f29\u70b9\u4e91\u4e3a\u9ad8\u4fe1\u606f\u7387\u7684\u4e8c\u8fdb\u5236\u6d41\u3002 \u5982\u4e0a\u56fe\u6240\u793a, \u672c\u6587\u9996\u5148\u5c1d\u8bd5\u7528\u4e00\u4e2a\u516b\u5206\u6811(oct-tree)\u6765\u7f16\u7801\u6574\u4e2a\u70b9\u4e91, \u516b\u53c9\u6811\u4e0a\u7684\u6bcf\u4e00\u4e2a\u70b9\u7684\u4f4d\u7f6e\u90fd\u662fpre-defined\u7684\uff0c\u8fd9\u6837\u5c31\u53ef\u4ee5\u7528\u6bcf\u4e00\u5c42\u4e0a\u9762\u7684occupancy 0-1\u4e8c\u8fdb\u5236\u4e32\u6765\u8fd1\u4f3c\u8868\u8fbe\u6574\u4e2a\u70b9\u4e91\u3002 \u5982\u679c\u6211\u4eec\u91c7\u7528\u4e00\u4e2a\u7b80\u5355\u7684\u538b\u7f29\u7b97\u6cd5\u76f4\u63a5\u5bf9\u6574\u4e2a\u516b\u5206\u6811\u4e0a\u76840-1\u4e8c\u8fdb\u5236\u4e32\u8fdb\u884c\u538b\u7f29\uff0c\u90a3\u4e48\u53ef\u4ee5\u5f97\u5230\u4e00\u4e2a\u6bd4\u8f83\u7a33\u5b9a\u7684\u538b\u7f29\u7387\uff1b\u4e0d\u8fc7\u7531\u4e8e\u8fd9\u6837\u7684\u4e8c\u8fdb\u5236\u4e32\u6ca1\u6709\u592a\u591a\u7684\u89c4\u5f8b\uff0c\u7a00\u758f\u5ea6\u4e0d\u9ad8\uff0c\u4e14\u6ca1\u6709\u89c4\u5f8b\uff0c\u56e0\u800c\u4fe1\u606f\u71b5\u6bd4\u8f83\u5927\uff1b \u672c\u6587\u7684idea: \u7528\u795e\u7ecf\u7f51\u7edc\uff0c\u4f7f\u7528\u4e0a\u4e00\u5c42\u7684\u8282\u70b9\u7684\u7279\u5f81\uff0c\u9884\u6d4b\u4e0b\u4e00\u5c42\u8282\u70b9\u7684occupancy probability. \u5728\u538b\u7f29\u7684\u65f6\u5019\uff0c\u8bb0\u5f55\u4e0bGT occupancy probability\u548cpredicted occupancy probability\u7684\u5dee\u503c(0-1\u5e8f\u5217).\u5982\u679c\u7f51\u8def\u7684\u9884\u6d4b\u51c6\u786e\u7387\u6bd4\u8f83\u9ad8\u7684\u8bdd\uff0c\u8fd9\u4e2a0-1\u5e8f\u5217\u5dee\u503c\u5f88\u591a\u90fd\u4f1a\u662f0\uff0c\u5c06\u4f1a\u662f\u4e00\u4e2a\u5f88\u7a00\u758f\u7684\u5e8f\u5217\uff0c\u7136\u540e\u7528\u4e00\u822c\u7684\u4e8c\u8fdb\u5236\u538b\u7f29\u7b97\u6cd5(\u5982zip, huffman coding)\u7b49\u53ef\u4ee5\u5f97\u5230\u5f88\u9ad8\u7684\u538b\u7f29\u7387. \u5728\u89e3\u538b\u7f29\u7684\u65f6\u5019\uff0c\u8ba9\u7f51\u8def\u5728\u6bcf\u4e00\u5c42\u8fdb\u884c\u6982\u7387\u9884\u6d4b\uff0c\u7136\u540e\u5c06\u5dee\u503c\u8fdb\u884c\u89e3\u7801\uff1b\u5c06\u7f51\u7edc\u7684\u9884\u6d4b\u503c\u4e0e\u89e3\u7801\u503c\u8fdb\u884c\u76f8\u52a0\uff0c\u5c31\u80fd\u65e0\u635f\u5730\u8fd8\u539f\u70b9\u4e91\u538b\u7f29\u65f6\u88ab\u8f93\u5165\u7f51\u7edc\u76840-1\u5e8f\u5217. \u5728\u5c24\u5176\u662f\u9ad8\u5c42\u7684\u65f6\u5019\uff0c\u7a00\u758f\u5e8f\u5217\u7684\u5b58\u50a8\u52a0\u8d77\u6765\u6bd4\u539f\u6765\u7a20\u5bc6\u5e8f\u5217\u7684\u5b58\u50a8\u8981\u66f4\u5c11\uff0c\u5b9e\u73b0\u538b\u7f29\u3002\u800c\u7f51\u7edc\u6743\u91cd\u5219\u662f\u5728\u4e0d\u540c\u7f51\u7edc\u95f4\u901a\u7528\u7684\u3002","title":"OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression"},{"location":"other_categories/others/openpose_part_afinity_fileds/","text":"Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields \u8fd9\u7bc7\u8bba\u6587\u662f Open Pose\u5e93 \u8fc8\u5411\u5b9e\u65f6\u591a\u4eba\u80a2\u4f53\u4f30\u8ba1\u7684\u7b2c\u4e00\u7bc7\u8bba\u6587\uff0c\u4e3b\u8981\u662f\u89e3\u51b3\u80a2\u4f53\u4f30\u8ba1\u4e2d\u7684\u5339\u914d\u95ee\u9898\u3002 Pose Estimation pipeline \u603b\u4f53\u6846\u67b6\u5982\u56fe\uff0cCNN\u4f1a\u8f93\u51fa\u4e24\u4e2a\u5206\u652f\uff0c\u4e00\u4e2a\u5206\u652f\u662f\u5173\u8282\u4f4d\u7f6e\u70ed\u56fe\uff0c\u4e00\u4e2a\u5206\u652f\u662f\u5c40\u90e8\u80a2\u4f53\u77e2\u91cf\u56fe(Part Affinity Fields, PAF)\u3002\u901a\u8fc7NMS\u5f97\u5230\u56fe\u4e2d\u6240\u6709\u4eba\u5404\u4e2a\u5173\u8282\u7684\u4f4d\u7f6e\u3002\u518d\u901a\u8fc7PAF\u63a8\u7406\u3001\u5206\u7ec4\u51fa\u5404\u4e2a\u4eba\u5404\u81ea\u7684\u5173\u8282\u3002 \u7f51\u7edc\u7ed3\u6784 \u5148\u4f7f\u7528VGG backbone\u63d0\u51fa feature map F , \u518d\u591a\u4e2astage \u5206\u522b\u8f93\u51fa S \\in R^{w\\times h}, L \\in R^{w\\times h \\times 2} \uff0c\u4e0d\u540c\u5c42\u4e4b\u95f4\u7528concat\u94fe\u63a5 \\begin{aligned} S^t &= \\rho^t(F, S^{t-1}, L^{t-1}) \\\\ L^t &= \\phi^t(F, S^{t-1}, L^{t-1}) \\end{aligned} S Confidence map training \u6bcf\u4e00\u4e2a\u6709\u6807\u6ce8\u7684\u5173\u8282\uff0c\u4ee5\u5176\u5750\u6807\u4e3a\u4e2d\u5fc3\uff0c\u8ba1\u7b97\u4e00\u4e2aGaussian heat map, S \u7531\u4e0d\u540c\u4eba\u540c\u4e00\u5173\u8282\u7684heat map\u7684\u6700\u5927\u503c\u7ec4\u6210\u3002 \u5728\u63a8\u7406\u7684\u65f6\u5019\uff0c\u4f7f\u7528NMS\u5254\u9664\u4e0d\u5728\u6700\u9ad8\u70b9\u7684\u70b9 PAF for part association pipeline\u4e2d\u63d0\u5230\u8fc7 PAF( L )\u662f\u4e00\u4e2a2\u7ef4\u7684\u5411\u91cf\u573a\u56fe,\u8fd9\u91cc\u6709\u4e00\u4e2a\"limb\"(\u80a2\u4f53)\u7684\u6982\u5ff5\uff0c\u5b9a\u4e49\u4e3a\u4e24\u4e2a\u76f8\u90bb\u5173\u8282\u4e4b\u95f4\u7684\u8fde\u63a5\uff0c\u4f5c\u8005\u6839\u636e\u4eba\u4f53\u7279\u5f81\u9884\u5b9a\u4e49\u4e86\u4e00\u7cfb\u5217\u7684\"limb\". \u8ddd\u79bb\u76f8\u90bb\u5173\u8282\u7ebf\u6bb5\u6700\u77ed\u8ddd\u79bb\u5c0f\u4e8e\u4e00\u4e2a\u9608\u503c\uff0c\u540c\u65f6\u6295\u5f71\u5728\u7ebf\u6bb5\u5185\u7684\u70b9\uff0c\u4f1a\u5e26\u4e0a\u4e00\u4e2a\u6807\u6ce8\uff0c\u5176\u6807\u6ce8\u503c\u4e3a\u8fd9\u4e24\u4e2a\u5173\u8282\u4e4b\u95f4\u7684\u5355\u4f4d\u76f4\u7ebf\u77e2\u91cf,\u76ee\u6807\u7684PAF\u5219\u662f\u540c\u4e00\u56fe\u4e2d\u6240\u6709\u4eba\u540c\u4e00limb\u7684\u5747\u503c\u3002 \u5728inference\u7684\u65f6\u5019\uff0c\u88ab\u4e00\u4e2alimb\u6240\u8fde\u63a5\u7684\u4e24\u4e2a\u5173\u8282\u4e4b\u95f4\u76f8\u4e92\u5339\u914d\u7684\u6743\u91cd\u4e3a\u5982\u4e0b\u7684\u79ef\u5206 E = \\int^{u=1}_{u=0}L_c(p(u)) \\frac{d_{j2} - d_{j1}}{||d_{j2} - d_{j1}||_2} du \u5176\u4e2d p(u) \u7531\u53cc\u7ebf\u6027\u63d2\u503c\u5f97\u5230\uff0c\u800c\u5b9e\u9645\u4e0a\u7684\u79ef\u5206\u4e5f\u6709\u5747\u5300\u91c7\u6837\u4e2d\u95f4\u70b9\u5f97\u5230\u3002 \u5339\u914d\u95ee\u9898 \u4ecepartition graph\u7684\u70b9\u5339\u914d\u95ee\u9898\u7684\u89d2\u5ea6,\u56de\u987e\u4e00\u4e0b\u5173\u8282\u70b9\u7684\u5339\u914d\u95ee\u9898 \u4e25\u683c\u6765\u8bf4\uff0c\u6211\u4eec\u9700\u8981\u8ba1\u7b97\u7684\u662f\u5982 b \u56fe\u4e00\u6837\u7684\u4ece K \u4e2a\u5206\u79bb\u56fe\u4e2d\u5f97\u5230 N \u7ec4\u6700\u4f18\u7684\u7531 K \u4e2a\u5173\u8282\u7ec4\u6210\u7684\u5339\u914d\u7ec4\uff0c\u4f5c\u8005\u6307\u51fa\u8fd9\u662f\u4e00\u4e2aNP\u96be\u95ee\u9898.\u4e3a\u4e86\u5728\u80a2\u4f53\u4f30\u8ba1\u8fd9\u4e2a\u9886\u57df\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u4f5c\u8005\u505a\u4e86\u4e00\u5b9a\u7684\u7b80\u5316\uff0c\u4e5f\u5c31\u662f\u53ea\u5b8c\u6210\u5bf9 \u76f8\u90bb\u5173\u8282 \u4e4b\u95f4\u7684\u76f8\u4e92\u6700\u4f18\u5339\u914d\u800c\u4e0d\u505a \u5168\u5c40 \u7684\u6700\u4f18\u5339\u914d. \u76f8\u90bb\u7684\u4e8c\u5206\u79bb\u7684\u5339\u914d\u95ee\u9898\uff0c\u5176\u6743\u91cd\u7531\u524d\u6587\u7684PAF\u8ba1\u7b97\u5f97\u5230\uff0c\u518d\u7528 \u5308\u7259\u5229\u7b97\u6cd5 \u5b8c\u6210\u5339\u914d\u3002\u5982\u6b64\u7b80\u5316\u53ef\u4ee5\u6307\u6570\u7ea7\u5730\u51cf\u5c11\u8fd0\u7b97\u91cf\uff0c\u4f7f\u5f97\u7b97\u6cd5\u53ef\u4f7f\u7528\u3002","title":"Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields"},{"location":"other_categories/others/openpose_part_afinity_fileds/#realtime-multi-person-2d-pose-estimation-using-part-affinity-fields","text":"\u8fd9\u7bc7\u8bba\u6587\u662f Open Pose\u5e93 \u8fc8\u5411\u5b9e\u65f6\u591a\u4eba\u80a2\u4f53\u4f30\u8ba1\u7684\u7b2c\u4e00\u7bc7\u8bba\u6587\uff0c\u4e3b\u8981\u662f\u89e3\u51b3\u80a2\u4f53\u4f30\u8ba1\u4e2d\u7684\u5339\u914d\u95ee\u9898\u3002","title":"Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields"},{"location":"other_categories/others/openpose_part_afinity_fileds/#pose-estimation-pipeline","text":"\u603b\u4f53\u6846\u67b6\u5982\u56fe\uff0cCNN\u4f1a\u8f93\u51fa\u4e24\u4e2a\u5206\u652f\uff0c\u4e00\u4e2a\u5206\u652f\u662f\u5173\u8282\u4f4d\u7f6e\u70ed\u56fe\uff0c\u4e00\u4e2a\u5206\u652f\u662f\u5c40\u90e8\u80a2\u4f53\u77e2\u91cf\u56fe(Part Affinity Fields, PAF)\u3002\u901a\u8fc7NMS\u5f97\u5230\u56fe\u4e2d\u6240\u6709\u4eba\u5404\u4e2a\u5173\u8282\u7684\u4f4d\u7f6e\u3002\u518d\u901a\u8fc7PAF\u63a8\u7406\u3001\u5206\u7ec4\u51fa\u5404\u4e2a\u4eba\u5404\u81ea\u7684\u5173\u8282\u3002","title":"Pose Estimation pipeline"},{"location":"other_categories/others/openpose_part_afinity_fileds/#_1","text":"\u5148\u4f7f\u7528VGG backbone\u63d0\u51fa feature map F , \u518d\u591a\u4e2astage \u5206\u522b\u8f93\u51fa S \\in R^{w\\times h}, L \\in R^{w\\times h \\times 2} \uff0c\u4e0d\u540c\u5c42\u4e4b\u95f4\u7528concat\u94fe\u63a5 \\begin{aligned} S^t &= \\rho^t(F, S^{t-1}, L^{t-1}) \\\\ L^t &= \\phi^t(F, S^{t-1}, L^{t-1}) \\end{aligned}","title":"\u7f51\u7edc\u7ed3\u6784"},{"location":"other_categories/others/openpose_part_afinity_fileds/#s-confidence-map-training","text":"\u6bcf\u4e00\u4e2a\u6709\u6807\u6ce8\u7684\u5173\u8282\uff0c\u4ee5\u5176\u5750\u6807\u4e3a\u4e2d\u5fc3\uff0c\u8ba1\u7b97\u4e00\u4e2aGaussian heat map, S \u7531\u4e0d\u540c\u4eba\u540c\u4e00\u5173\u8282\u7684heat map\u7684\u6700\u5927\u503c\u7ec4\u6210\u3002 \u5728\u63a8\u7406\u7684\u65f6\u5019\uff0c\u4f7f\u7528NMS\u5254\u9664\u4e0d\u5728\u6700\u9ad8\u70b9\u7684\u70b9","title":"S Confidence map training"},{"location":"other_categories/others/openpose_part_afinity_fileds/#paf-for-part-association","text":"pipeline\u4e2d\u63d0\u5230\u8fc7 PAF( L )\u662f\u4e00\u4e2a2\u7ef4\u7684\u5411\u91cf\u573a\u56fe,\u8fd9\u91cc\u6709\u4e00\u4e2a\"limb\"(\u80a2\u4f53)\u7684\u6982\u5ff5\uff0c\u5b9a\u4e49\u4e3a\u4e24\u4e2a\u76f8\u90bb\u5173\u8282\u4e4b\u95f4\u7684\u8fde\u63a5\uff0c\u4f5c\u8005\u6839\u636e\u4eba\u4f53\u7279\u5f81\u9884\u5b9a\u4e49\u4e86\u4e00\u7cfb\u5217\u7684\"limb\". \u8ddd\u79bb\u76f8\u90bb\u5173\u8282\u7ebf\u6bb5\u6700\u77ed\u8ddd\u79bb\u5c0f\u4e8e\u4e00\u4e2a\u9608\u503c\uff0c\u540c\u65f6\u6295\u5f71\u5728\u7ebf\u6bb5\u5185\u7684\u70b9\uff0c\u4f1a\u5e26\u4e0a\u4e00\u4e2a\u6807\u6ce8\uff0c\u5176\u6807\u6ce8\u503c\u4e3a\u8fd9\u4e24\u4e2a\u5173\u8282\u4e4b\u95f4\u7684\u5355\u4f4d\u76f4\u7ebf\u77e2\u91cf,\u76ee\u6807\u7684PAF\u5219\u662f\u540c\u4e00\u56fe\u4e2d\u6240\u6709\u4eba\u540c\u4e00limb\u7684\u5747\u503c\u3002 \u5728inference\u7684\u65f6\u5019\uff0c\u88ab\u4e00\u4e2alimb\u6240\u8fde\u63a5\u7684\u4e24\u4e2a\u5173\u8282\u4e4b\u95f4\u76f8\u4e92\u5339\u914d\u7684\u6743\u91cd\u4e3a\u5982\u4e0b\u7684\u79ef\u5206 E = \\int^{u=1}_{u=0}L_c(p(u)) \\frac{d_{j2} - d_{j1}}{||d_{j2} - d_{j1}||_2} du \u5176\u4e2d p(u) \u7531\u53cc\u7ebf\u6027\u63d2\u503c\u5f97\u5230\uff0c\u800c\u5b9e\u9645\u4e0a\u7684\u79ef\u5206\u4e5f\u6709\u5747\u5300\u91c7\u6837\u4e2d\u95f4\u70b9\u5f97\u5230\u3002","title":"PAF for part association"},{"location":"other_categories/others/openpose_part_afinity_fileds/#_2","text":"\u4ecepartition graph\u7684\u70b9\u5339\u914d\u95ee\u9898\u7684\u89d2\u5ea6,\u56de\u987e\u4e00\u4e0b\u5173\u8282\u70b9\u7684\u5339\u914d\u95ee\u9898 \u4e25\u683c\u6765\u8bf4\uff0c\u6211\u4eec\u9700\u8981\u8ba1\u7b97\u7684\u662f\u5982 b \u56fe\u4e00\u6837\u7684\u4ece K \u4e2a\u5206\u79bb\u56fe\u4e2d\u5f97\u5230 N \u7ec4\u6700\u4f18\u7684\u7531 K \u4e2a\u5173\u8282\u7ec4\u6210\u7684\u5339\u914d\u7ec4\uff0c\u4f5c\u8005\u6307\u51fa\u8fd9\u662f\u4e00\u4e2aNP\u96be\u95ee\u9898.\u4e3a\u4e86\u5728\u80a2\u4f53\u4f30\u8ba1\u8fd9\u4e2a\u9886\u57df\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u4f5c\u8005\u505a\u4e86\u4e00\u5b9a\u7684\u7b80\u5316\uff0c\u4e5f\u5c31\u662f\u53ea\u5b8c\u6210\u5bf9 \u76f8\u90bb\u5173\u8282 \u4e4b\u95f4\u7684\u76f8\u4e92\u6700\u4f18\u5339\u914d\u800c\u4e0d\u505a \u5168\u5c40 \u7684\u6700\u4f18\u5339\u914d. \u76f8\u90bb\u7684\u4e8c\u5206\u79bb\u7684\u5339\u914d\u95ee\u9898\uff0c\u5176\u6743\u91cd\u7531\u524d\u6587\u7684PAF\u8ba1\u7b97\u5f97\u5230\uff0c\u518d\u7528 \u5308\u7259\u5229\u7b97\u6cd5 \u5b8c\u6210\u5339\u914d\u3002\u5982\u6b64\u7b80\u5316\u53ef\u4ee5\u6307\u6570\u7ea7\u5730\u51cf\u5c11\u8fd0\u7b97\u91cf\uff0c\u4f7f\u5f97\u7b97\u6cd5\u53ef\u4f7f\u7528\u3002","title":"\u5339\u914d\u95ee\u9898"},{"location":"other_categories/others/patchmatchnet/","text":"PatchmatchNet: Learned Multi-View Patchmatch Stereo \u4e3b\u8981\u8d21\u732e: \u51cf\u5c11\u6d4b\u8bd5\u65f6\u7684\u8fd0\u7b97\u91cf\u4ee5\u53ca\u8bb0\u5fc6\u6d88\u8017, \u4f7f\u7528coarse-to-fine\u7684\u5c42\u7ea7\u7ed3\u6784 \u7528\u53ef\u5b66\u4e60\u6a21\u5757\u66ff\u6362 propagation \u548c cost evaluation; \u8bad\u7ec3\u4e2d\u52a0\u5165randomness, Methods \u5bf9N\u5f20\u8f93\u5165\u56fe\u7247\uff0c\u8dd1FPN\uff0c\u5f97\u5230\u591a\u5c42feature Learnable Patchmatch Depth Map Refinement Patchmatch \u521d\u59cb\u5316\u4ee5\u53ca Pertubation. \u5728\u7b2c\u4e00\u4e2a\u5faa\u73af\uff0c \u57fa\u4e8e [d_{min}, d_{max}] , \u5206\u6210 D_f \u4e2a\u95f4\u9694\u3002\u5728\u540e\u9762\u7684stage, \u5bf9\u6bcf\u4e2apixel\u6267\u884c N_k \u4e2ahypotheses, \u89c1 \u8fd9\u7bc7 Adaptive Propagation \u5b9e\u9645\u505a\u6cd5\u6bd4DCN\u8fd8\u8981\u518d\u590d\u6742\u4e00\u4e9b\uff0c\u9996\u5148\u6709\u4e00\u4e2abaseline\u76845*5\u5377\u79ef\u6838\uff0c\u4f7f\u7528CNN\u8f93\u51fa\u4e00\u4e2a\u989d\u5916\u7684offset. \u4f46\u662f\u4e0d\u592a\u770b\u5f97\u51fa\u6765\u5dee\u8ddd\u5728\u54ea\u91cc Differentiable Warping \u57fa\u4e8e\u9884\u6d4b\u7684\u6df1\u5ea6\u4ee5\u53ca\u5916\u53c2\u8fdb\u884csample","title":"PatchmatchNet: Learned Multi-View Patchmatch Stereo"},{"location":"other_categories/others/patchmatchnet/#patchmatchnet-learned-multi-view-patchmatch-stereo","text":"\u4e3b\u8981\u8d21\u732e: \u51cf\u5c11\u6d4b\u8bd5\u65f6\u7684\u8fd0\u7b97\u91cf\u4ee5\u53ca\u8bb0\u5fc6\u6d88\u8017, \u4f7f\u7528coarse-to-fine\u7684\u5c42\u7ea7\u7ed3\u6784 \u7528\u53ef\u5b66\u4e60\u6a21\u5757\u66ff\u6362 propagation \u548c cost evaluation; \u8bad\u7ec3\u4e2d\u52a0\u5165randomness,","title":"PatchmatchNet: Learned Multi-View Patchmatch Stereo"},{"location":"other_categories/others/patchmatchnet/#methods","text":"\u5bf9N\u5f20\u8f93\u5165\u56fe\u7247\uff0c\u8dd1FPN\uff0c\u5f97\u5230\u591a\u5c42feature Learnable Patchmatch Depth Map Refinement","title":"Methods"},{"location":"other_categories/others/patchmatchnet/#patchmatch","text":"\u521d\u59cb\u5316\u4ee5\u53ca Pertubation. \u5728\u7b2c\u4e00\u4e2a\u5faa\u73af\uff0c \u57fa\u4e8e [d_{min}, d_{max}] , \u5206\u6210 D_f \u4e2a\u95f4\u9694\u3002\u5728\u540e\u9762\u7684stage, \u5bf9\u6bcf\u4e2apixel\u6267\u884c N_k \u4e2ahypotheses, \u89c1 \u8fd9\u7bc7 Adaptive Propagation \u5b9e\u9645\u505a\u6cd5\u6bd4DCN\u8fd8\u8981\u518d\u590d\u6742\u4e00\u4e9b\uff0c\u9996\u5148\u6709\u4e00\u4e2abaseline\u76845*5\u5377\u79ef\u6838\uff0c\u4f7f\u7528CNN\u8f93\u51fa\u4e00\u4e2a\u989d\u5916\u7684offset. \u4f46\u662f\u4e0d\u592a\u770b\u5f97\u51fa\u6765\u5dee\u8ddd\u5728\u54ea\u91cc Differentiable Warping \u57fa\u4e8e\u9884\u6d4b\u7684\u6df1\u5ea6\u4ee5\u53ca\u5916\u53c2\u8fdb\u884csample","title":"Patchmatch"},{"location":"other_categories/others/pruning/","text":"Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning \u8fd9\u7bc7paper\u6307\u51fa\u4e86\u73b0\u6709pruning \u65b9\u6cd5\u4ee5\u53ca\u73b0\u6709benchmark\u7684\u95ee\u9898\u3002\u5e76\u4e14\u6307\u51fa\u4e86\u4e24\u4e2a\u5947\u602a\u7684\u73b0\u8c61\u5bf9\u73b0\u6709\u65b9\u6cd5\u63d0\u51fa\u4e86\u8d28\u7591\uff1a M1: \u5728prunining\u4e4b\u540e\u8fdb\u884cfine-tuning\u7684\u65f6\u5019\uff0c\u5982\u679c\u91c7\u7528\u8f83\u5927\u7684learning rate\uff0c\u53ef\u5f97\u5230\u6bd4\u8f83\u597d\u7684\u7ed3\u679c\u3002\u8fd9\u4e0efine-tuning\u7684\u76f4\u89c9\u4e0d\u7b26\u5408\u3002\u4f5c\u8005\u5c1d\u8bd5\u5bf9\u57fa\u7840\u7684L1-norm pruning \u8fdb\u884c\u5904\u7406\uff0c\u518d\u4f7f\u7528\u66f4\u5927\u7684learning rate\u8fdb\u884c\u540e\u5904\u7406\u8bad\u7ec3\uff0c\u5f97\u5230\u4e86\u5b8c\u5168\u4e0d\u4e9a\u4e8e\u76ee\u524dSOTA\u7684\u8868\u73b0\u3002 M2: \u76f4\u63a5\u5bf9\u4fee\u526a\u540e\u7684\u6a21\u578b\uff0c\u4ece\u9884\u8bad\u7ec3\u5f00\u59cb\u91cd\u65b0\u8bad\u7ec3\uff0c\u53ef\u80fd\u6bd4fine-tuning\u66f4\u957f\u7684\u65f6\u95f4\uff0c\u4f46\u662f\u53ef\u4ee5\u5f97\u5230\u4e0d\u4e9a\u4e8efine-tuning\u7684\u7ed3\u679c\u3002 \u672c\u6587\u8bbe\u5b9a\u4e86\u56db\u79cd\u7b49\u7ea7\u7684fairness principle, \u5e76\u5f15\u5165\u4e86\u7f51\u7edc\u7684\u53ef\u8bad\u7ec3\u6027 trainability \u8fd9\u4e2a\u6982\u5ff5\u6765\u89e3\u91ca\u3002 To summarize, our results suggest a larger LR does not really \u201cimprove\u201d the performance. What really happens is, a larger LR accelerates the optimization process, making the higher performance observed earlier.","title":"Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning"},{"location":"other_categories/others/pruning/#why-is-the-state-of-neural-network-pruning-so-confusing-on-the-fairness-comparison-setup-and-trainability-in-network-pruning","text":"\u8fd9\u7bc7paper\u6307\u51fa\u4e86\u73b0\u6709pruning \u65b9\u6cd5\u4ee5\u53ca\u73b0\u6709benchmark\u7684\u95ee\u9898\u3002\u5e76\u4e14\u6307\u51fa\u4e86\u4e24\u4e2a\u5947\u602a\u7684\u73b0\u8c61\u5bf9\u73b0\u6709\u65b9\u6cd5\u63d0\u51fa\u4e86\u8d28\u7591\uff1a M1: \u5728prunining\u4e4b\u540e\u8fdb\u884cfine-tuning\u7684\u65f6\u5019\uff0c\u5982\u679c\u91c7\u7528\u8f83\u5927\u7684learning rate\uff0c\u53ef\u5f97\u5230\u6bd4\u8f83\u597d\u7684\u7ed3\u679c\u3002\u8fd9\u4e0efine-tuning\u7684\u76f4\u89c9\u4e0d\u7b26\u5408\u3002\u4f5c\u8005\u5c1d\u8bd5\u5bf9\u57fa\u7840\u7684L1-norm pruning \u8fdb\u884c\u5904\u7406\uff0c\u518d\u4f7f\u7528\u66f4\u5927\u7684learning rate\u8fdb\u884c\u540e\u5904\u7406\u8bad\u7ec3\uff0c\u5f97\u5230\u4e86\u5b8c\u5168\u4e0d\u4e9a\u4e8e\u76ee\u524dSOTA\u7684\u8868\u73b0\u3002 M2: \u76f4\u63a5\u5bf9\u4fee\u526a\u540e\u7684\u6a21\u578b\uff0c\u4ece\u9884\u8bad\u7ec3\u5f00\u59cb\u91cd\u65b0\u8bad\u7ec3\uff0c\u53ef\u80fd\u6bd4fine-tuning\u66f4\u957f\u7684\u65f6\u95f4\uff0c\u4f46\u662f\u53ef\u4ee5\u5f97\u5230\u4e0d\u4e9a\u4e8efine-tuning\u7684\u7ed3\u679c\u3002 \u672c\u6587\u8bbe\u5b9a\u4e86\u56db\u79cd\u7b49\u7ea7\u7684fairness principle, \u5e76\u5f15\u5165\u4e86\u7f51\u7edc\u7684\u53ef\u8bad\u7ec3\u6027 trainability \u8fd9\u4e2a\u6982\u5ff5\u6765\u89e3\u91ca\u3002 To summarize, our results suggest a larger LR does not really \u201cimprove\u201d the performance. What really happens is, a larger LR accelerates the optimization process, making the higher performance observed earlier.","title":"Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning"},{"location":"other_categories/others/self_supervised_stereo/","text":"Self-Supervised Learning for Stereo Matching with Self-Improving Ability \u8fd9\u7bc7paper\u5173\u6ce8\u7684\u662f\u81ea\u76d1\u7763\u53cc\u76ee\u5339\u914d\u3002\u91cd\u70b9\u5728\u4e8e\u4e0d\u9700\u8981\u70b9\u4e91\u76d1\u7763\uff0c\u800c\u4e14\u5728\u65b0\u573a\u666f\u6d4b\u8bd5\u7684\u65f6\u5019\u53ef\u4ee5\u91cd\u65b0\u8fed\u4ee3\u8fdb\u6b65\u3002 \u7f51\u7edc\u7ed3\u6784 \u5176\u4e2d\u7684feature mapping \u5de6\u53f3\u76ee\u5206\u522b\u8fd0\u884c\u5bf9\u79f0\u7684\u7f51\u7edc\u3002 \u635f\u5931\u51fd\u6570 \u5de6\u53f3\u56fe\u751f\u6210\u4e00\u81f4\u6027 \\begin{aligned} \\mathcal{L}_{u}^{l}\\left(I_{L}, I_{L}^{\\prime}\\right) &=\\frac{1}{N} \\sum \\lambda_{1} \\frac{1-\\mathcal{S}\\left(I_{L}, I_{L}^{\\prime}\\right)}{2} \\\\ &+\\lambda_{2}\\left|I_{L}-I_{L}^{\\prime}\\right|+\\lambda_{3}\\left|\\nabla I_{L}-\\nabla I_{L}^{\\prime}\\right| \\end{aligned} \u5206\u4e09\u9879\uff0c\u5206\u522b\u662fSSIM,\u5de6\u56fe\uff0c\u56fe\u7247\u68af\u5ea6\u3002 def SSIM(self, x, y): C1 = 0.01 ** 2 C2 = 0.03 ** 2 mu_x = nn.AvgPool2d(3, 1)(x) mu_y = nn.AvgPool2d(3, 1)(y) mu_x_mu_y = mu_x * mu_y mu_x_sq = mu_x.pow(2) mu_y_sq = mu_y.pow(2) sigma_x = nn.AvgPool2d(3, 1)(x * x) - mu_x_sq sigma_y = nn.AvgPool2d(3, 1)(y * y) - mu_y_sq sigma_xy = nn.AvgPool2d(3, 1)(x * y) - mu_x_mu_y SSIM_n = (2 * mu_x_mu_y + C1) * (2 * sigma_xy + C2) SSIM_d = (mu_x_sq + mu_y_sq + C1) * (sigma_x + sigma_y + C2) SSIM = SSIM_n / SSIM_d return torch.clamp((1 - SSIM) / 2, 0, 1) \u89c4\u8303\u5316 \u5e73\u7a33\u6027,\u56fe\u7247\u4e8c\u6b21\u68af\u5ea6 \\mathcal{L}_{s}^{l}=\\frac{1}{N} \\sum\\left|\\nabla_{u}^{2} d_{L}\\right| e^{-\\left|\\nabla_{u}^{2} I_{L}\\right|}+\\left|\\nabla_{v}^{2} d_{L}\\right| e^{-\\left|\\nabla_{v}^{2} I_{L}\\right|} \u5faa\u73af\u4e00\u81f4\u6027 \\mathcal{L}_{c}^{L}=\\left|I_{L}-I_{L}^{\\prime \\prime}\\right| Maximum-Depth Heuristic \u6700\u5c0f\u5316dispairty\u7684\u603b\u548c\u3002 \\mathcal{L}_{m}^{L}=\\frac{1}{N} \\sum\\left|d^{L}\\right| \u81ea\u63d0\u5347 \u5728\u6d4b\u8bd5\u7684\u65f6\u5019\u53ef\u4ee5\u8fdb\u4e00\u6b65fine-tune\u7f51\u7edc\u3002\u6027\u80fd\u8fd8\u80fd\u63d0\u5347","title":"Self-Supervised Learning for Stereo Matching with Self-Improving Ability"},{"location":"other_categories/others/self_supervised_stereo/#self-supervised-learning-for-stereo-matching-with-self-improving-ability","text":"\u8fd9\u7bc7paper\u5173\u6ce8\u7684\u662f\u81ea\u76d1\u7763\u53cc\u76ee\u5339\u914d\u3002\u91cd\u70b9\u5728\u4e8e\u4e0d\u9700\u8981\u70b9\u4e91\u76d1\u7763\uff0c\u800c\u4e14\u5728\u65b0\u573a\u666f\u6d4b\u8bd5\u7684\u65f6\u5019\u53ef\u4ee5\u91cd\u65b0\u8fed\u4ee3\u8fdb\u6b65\u3002","title":"Self-Supervised Learning for Stereo Matching with Self-Improving Ability"},{"location":"other_categories/others/self_supervised_stereo/#_1","text":"\u5176\u4e2d\u7684feature mapping \u5de6\u53f3\u76ee\u5206\u522b\u8fd0\u884c\u5bf9\u79f0\u7684\u7f51\u7edc\u3002","title":"\u7f51\u7edc\u7ed3\u6784"},{"location":"other_categories/others/self_supervised_stereo/#_2","text":"","title":"\u635f\u5931\u51fd\u6570"},{"location":"other_categories/others/self_supervised_stereo/#_3","text":"\\begin{aligned} \\mathcal{L}_{u}^{l}\\left(I_{L}, I_{L}^{\\prime}\\right) &=\\frac{1}{N} \\sum \\lambda_{1} \\frac{1-\\mathcal{S}\\left(I_{L}, I_{L}^{\\prime}\\right)}{2} \\\\ &+\\lambda_{2}\\left|I_{L}-I_{L}^{\\prime}\\right|+\\lambda_{3}\\left|\\nabla I_{L}-\\nabla I_{L}^{\\prime}\\right| \\end{aligned} \u5206\u4e09\u9879\uff0c\u5206\u522b\u662fSSIM,\u5de6\u56fe\uff0c\u56fe\u7247\u68af\u5ea6\u3002 def SSIM(self, x, y): C1 = 0.01 ** 2 C2 = 0.03 ** 2 mu_x = nn.AvgPool2d(3, 1)(x) mu_y = nn.AvgPool2d(3, 1)(y) mu_x_mu_y = mu_x * mu_y mu_x_sq = mu_x.pow(2) mu_y_sq = mu_y.pow(2) sigma_x = nn.AvgPool2d(3, 1)(x * x) - mu_x_sq sigma_y = nn.AvgPool2d(3, 1)(y * y) - mu_y_sq sigma_xy = nn.AvgPool2d(3, 1)(x * y) - mu_x_mu_y SSIM_n = (2 * mu_x_mu_y + C1) * (2 * sigma_xy + C2) SSIM_d = (mu_x_sq + mu_y_sq + C1) * (sigma_x + sigma_y + C2) SSIM = SSIM_n / SSIM_d return torch.clamp((1 - SSIM) / 2, 0, 1)","title":"\u5de6\u53f3\u56fe\u751f\u6210\u4e00\u81f4\u6027"},{"location":"other_categories/others/self_supervised_stereo/#_4","text":"\u5e73\u7a33\u6027,\u56fe\u7247\u4e8c\u6b21\u68af\u5ea6 \\mathcal{L}_{s}^{l}=\\frac{1}{N} \\sum\\left|\\nabla_{u}^{2} d_{L}\\right| e^{-\\left|\\nabla_{u}^{2} I_{L}\\right|}+\\left|\\nabla_{v}^{2} d_{L}\\right| e^{-\\left|\\nabla_{v}^{2} I_{L}\\right|}","title":"\u89c4\u8303\u5316"},{"location":"other_categories/others/self_supervised_stereo/#_5","text":"\\mathcal{L}_{c}^{L}=\\left|I_{L}-I_{L}^{\\prime \\prime}\\right|","title":"\u5faa\u73af\u4e00\u81f4\u6027"},{"location":"other_categories/others/self_supervised_stereo/#maximum-depth-heuristic","text":"\u6700\u5c0f\u5316dispairty\u7684\u603b\u548c\u3002 \\mathcal{L}_{m}^{L}=\\frac{1}{N} \\sum\\left|d^{L}\\right|","title":"Maximum-Depth Heuristic"},{"location":"other_categories/others/self_supervised_stereo/#_6","text":"\u5728\u6d4b\u8bd5\u7684\u65f6\u5019\u53ef\u4ee5\u8fdb\u4e00\u6b65fine-tune\u7f51\u7edc\u3002\u6027\u80fd\u8fd8\u80fd\u63d0\u5347","title":"\u81ea\u63d0\u5347"},{"location":"other_categories/others/socialgan/","text":"Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks Social GAN \u4e00\u4e2a\u7ecf\u5178\u7684\u884c\u4eba\u8f68\u8ff9\u9884\u6d4b\u7f51\u7edc\u3002 Motivation: \u91cd\u89c6\u4eba\u4e0e\u4eba\u4e4b\u95f4\u7684\u4ea4\u4e92\u6027 \u4eba\u7684\u884c\u4e3a\u4e0d\u53ea\u662f\u7269\u7406\u53ef\u5b9e\u73b0\uff0c\u8fd8\u60f3\u8981Socially acceptable. \u4eba\u7684\u884c\u4e3a\u4e0d\u53ea\u6709\u4e00\u79cd\u53ef\u884c\u6027\uff0c\u662f\u4e00\u4e2amultimodal\u5206\u5e03. \\begin{aligned} e_{i}^{t} &=\\phi\\left(x_{i}^{t-1}, y_{i}^{t-1} ; W_{e d}\\right) \\\\ P_{i} &=P M\\left(h_{d 1}^{t-1}, \\ldots, h_{d n}^{t}\\right) \\\\ h_{d i}^{t} &=L S T M\\left(\\gamma\\left(P_{i}, h_{d i}^{t-1}\\right), e_{i}^{t} ; W_{\\text {decoder }}\\right) \\\\ \\left(\\hat{x}_{i}^{t}, \\hat{y}_{i}^{t}\\right) &=\\gamma\\left(h_{d i}^{t}\\right) \\end{aligned} \u65b9\u6cd5: \u4f7f\u7528LSTM\u5904\u7406\u5e8f\u5217\u8f93\u5165\u4e0e\u8f93\u51fa \u4f7f\u7528GAN\u7684\u673a\u5236\uff0c\u8bad\u7ec3\u4e00\u4e2a\u5206\u7c7b\u5668\u6765\u5224\u65ad\u5e8f\u5217\u662f\u5426\u5408\u7406\uff0c\u800c\u4e0d\u662f\u5f3a\u884coverfit \u5355\u4e2a\u771f\u5b9e\u503c\u3002","title":"Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks"},{"location":"other_categories/others/socialgan/#social-gan-socially-acceptable-trajectories-with-generative-adversarial-networks","text":"Social GAN \u4e00\u4e2a\u7ecf\u5178\u7684\u884c\u4eba\u8f68\u8ff9\u9884\u6d4b\u7f51\u7edc\u3002 Motivation: \u91cd\u89c6\u4eba\u4e0e\u4eba\u4e4b\u95f4\u7684\u4ea4\u4e92\u6027 \u4eba\u7684\u884c\u4e3a\u4e0d\u53ea\u662f\u7269\u7406\u53ef\u5b9e\u73b0\uff0c\u8fd8\u60f3\u8981Socially acceptable. \u4eba\u7684\u884c\u4e3a\u4e0d\u53ea\u6709\u4e00\u79cd\u53ef\u884c\u6027\uff0c\u662f\u4e00\u4e2amultimodal\u5206\u5e03. \\begin{aligned} e_{i}^{t} &=\\phi\\left(x_{i}^{t-1}, y_{i}^{t-1} ; W_{e d}\\right) \\\\ P_{i} &=P M\\left(h_{d 1}^{t-1}, \\ldots, h_{d n}^{t}\\right) \\\\ h_{d i}^{t} &=L S T M\\left(\\gamma\\left(P_{i}, h_{d i}^{t-1}\\right), e_{i}^{t} ; W_{\\text {decoder }}\\right) \\\\ \\left(\\hat{x}_{i}^{t}, \\hat{y}_{i}^{t}\\right) &=\\gamma\\left(h_{d i}^{t}\\right) \\end{aligned} \u65b9\u6cd5: \u4f7f\u7528LSTM\u5904\u7406\u5e8f\u5217\u8f93\u5165\u4e0e\u8f93\u51fa \u4f7f\u7528GAN\u7684\u673a\u5236\uff0c\u8bad\u7ec3\u4e00\u4e2a\u5206\u7c7b\u5668\u6765\u5224\u65ad\u5e8f\u5217\u662f\u5426\u5408\u7406\uff0c\u800c\u4e0d\u662f\u5f3a\u884coverfit \u5355\u4e2a\u771f\u5b9e\u503c\u3002","title":"Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks"},{"location":"other_categories/others/space_sweep/","text":"A Space-Sweep Approach to True Multi-Image Matching \u8fd9\u7bc7\u4f5c\u8005\u63d0\u51fa\u4f7f\u7528 plane sweeping algorithm\u53bb\u5904\u7406\u591a\u56fe\u7247matching\u7684\u95ee\u9898\u3002 \u9996\u5148\u4f5c\u8005\u6307\u660e\uff0c\u6240\u8c13\"True Multi-Image Matching\"\uff0c\u9700\u8981\u6ee1\u8db3\u4e09\u4e2a\u6761\u4ef6 1. \u8be5\u65b9\u6cd5\u53ef\u4ee5\u7528\u5728\u4efb\u610f\u591a\u7684\u76f8\u673a\u4e2d\u3002 2. \u7b97\u6cd5\u7684\u590d\u6742\u5ea6\u6b63\u6bd4\u4e8e\u76f8\u673a\u7684\u4e2a\u6570\u3002 3. \u6240\u6709\u56fe\u7247\u5728\u8ba1\u7b97\u4e0a\u7684\u5904\u7406\u662f\u4e00\u81f4\u7684\u3002 Plane sweeping \u603b\u4f53\u6846\u67b6\u4e0a\u6765\u8bf4\uff0cspace sweep\u7684\u7b97\u6cd5\u63cf\u8ff0\u7684\u662f\u9009\u53d6\u4e00\u7cfb\u5217\u5e73\u884c\u7684\u5e73\u9762\u3002\u5c06\u6bcf\u4e00\u4e2aimage\u4e0a\u7684\u6bcf\u4e00\u4e2akeypoint\u6295\u5f71\u5230\u6bcf\u4e00\u4e2a\u5e73\u9762\u4e0a\u3002\u5728\u6bcf\u4e00\u4e2a\u5e73\u9762\u4e0a\u7684\u6240\u6709keypoint\u8fdb\u884ccelluar voting\uff0c\u786e\u5b9akeypoint\u805a\u96c6\u76843D\u4f4d\u7f6e\u5e76\u5224\u65ad\u6b64\u5904\u786e\u5b9e\u6709\u4e00\u4e2a\u70b9\u3002 \u6240\u4ee5\u4e3a\u4e86\u9ad8\u6548\u7387\u5730\u8ba1\u7b97\uff0c\u8fd9\u91cc\u9700\u8981\u5206\u522b\u8ba8\u8bba\u8fd9\u4e24\u4e2a\u6b65\u9aa4\u7684\u5b9e\u73b0\u65b9\u6cd5\u3002 \u6295\u5f71 \u6807\u51c6\u7684\u601d\u8def\u662f\u5bf9\u4e8e\u6bcf\u4e00\u4e2a\u5e73\u9762 z_i ,\u56fe\u7247\u4e0a\u7684\u6bcf\u4e00\u4e2akeypoint\u90fd\u9700\u8981\u8fd0\u884c\u4e00\u4e2a\u9006\u6295\u5f71\uff0c\u8fd9\u4e2a\u65b9\u6cd5\u6bd4\u8f83\u6162\uff0c\u4f5c\u8005\u63d0\u51fa\uff0c\u53ef\u4ee5\u5148\u5c06\u56fe\u50cf\u70b9\u6295\u5f71\u5230\u67d0\u4e00\u4e2a\u5e73\u9762 z_0 \u5904\uff0c\u7136\u540e\u518d\u5e73\u884c\u7684\u5176\u4ed6\u5e73\u9762\u4e0a\u7684\u6295\u5f71\u53ef\u4ee5\u7531\u4e24\u4e2a\u5355\u5e94\u6027\u77e9\u9635 H \u53d8\u6362\u800c\u5f97. \\begin{array}{l} {x_{i}=\\delta x_{0}+(1-\\delta) C_{x}} \\\\ {y_{i}=\\delta y_{0}+(1-\\delta) C_{y}} \\end{array} \\delta = (z_i - C_z)/(z_0 - C_z) C_x, C_y, C_z = -r_{1} \\cdot t, -r_{2} \\cdot t, -r_{3} \\cdot t \u76f8\u5f53\u4e8e\u662f\u4f7f\u7528\u76f8\u4f3c\u4e09\u89d2\u5f62\u5728\u8fdb\u884c\u8ba1\u7b97\u3002 voting \u4e25\u8c28\u6765\u8bf4\uff0c\u540c\u4e00\u4e2apoint\u53ef\u4ee5\u7ed9\u6295\u5f71\u70b9\u9644\u4ef6\u4e00\u4e2a\u533a\u57df\u7684cell\u8fdb\u884c\u6295\u7968\uff0c\u7406\u8bba\u4e0a\u6765\u8bf4\u9700\u8981\u6c42jacobian\u3002\u672c\u6587\u6700\u7ec8\u91c7\u53d6\u7684\u65b9\u5f0f\u662f\u8ba9\u8ddd\u79bb\u76f8\u540c\u7684\u70b9\u6295\u7968\u8303\u56f4\u76f8\u540c\uff0c\u5e76\u4e14\u4e3a\u4e00\u4e2aconstant\u3002\u4e5f\u5c31\u662f\u8bf4\u6295\u7968\u7684\u8303\u56f4\u3001\u91c7\u6837\u6982\u7387\u4ec5\u4e0e\u8ddd\u79bb\u6210\u6b63\u6bd4\u5373\u53ef\u3002 \u6700\u7ec8\u901a\u8fc7\u9608\u503c\u8bbe\u7f6e\u5f97\u5230solid 3D point.","title":"A Space-Sweep Approach to True Multi-Image Matching"},{"location":"other_categories/others/space_sweep/#a-space-sweep-approach-to-true-multi-image-matching","text":"\u8fd9\u7bc7\u4f5c\u8005\u63d0\u51fa\u4f7f\u7528 plane sweeping algorithm\u53bb\u5904\u7406\u591a\u56fe\u7247matching\u7684\u95ee\u9898\u3002 \u9996\u5148\u4f5c\u8005\u6307\u660e\uff0c\u6240\u8c13\"True Multi-Image Matching\"\uff0c\u9700\u8981\u6ee1\u8db3\u4e09\u4e2a\u6761\u4ef6 1. \u8be5\u65b9\u6cd5\u53ef\u4ee5\u7528\u5728\u4efb\u610f\u591a\u7684\u76f8\u673a\u4e2d\u3002 2. \u7b97\u6cd5\u7684\u590d\u6742\u5ea6\u6b63\u6bd4\u4e8e\u76f8\u673a\u7684\u4e2a\u6570\u3002 3. \u6240\u6709\u56fe\u7247\u5728\u8ba1\u7b97\u4e0a\u7684\u5904\u7406\u662f\u4e00\u81f4\u7684\u3002","title":"A Space-Sweep Approach to True Multi-Image Matching"},{"location":"other_categories/others/space_sweep/#plane-sweeping","text":"\u603b\u4f53\u6846\u67b6\u4e0a\u6765\u8bf4\uff0cspace sweep\u7684\u7b97\u6cd5\u63cf\u8ff0\u7684\u662f\u9009\u53d6\u4e00\u7cfb\u5217\u5e73\u884c\u7684\u5e73\u9762\u3002\u5c06\u6bcf\u4e00\u4e2aimage\u4e0a\u7684\u6bcf\u4e00\u4e2akeypoint\u6295\u5f71\u5230\u6bcf\u4e00\u4e2a\u5e73\u9762\u4e0a\u3002\u5728\u6bcf\u4e00\u4e2a\u5e73\u9762\u4e0a\u7684\u6240\u6709keypoint\u8fdb\u884ccelluar voting\uff0c\u786e\u5b9akeypoint\u805a\u96c6\u76843D\u4f4d\u7f6e\u5e76\u5224\u65ad\u6b64\u5904\u786e\u5b9e\u6709\u4e00\u4e2a\u70b9\u3002 \u6240\u4ee5\u4e3a\u4e86\u9ad8\u6548\u7387\u5730\u8ba1\u7b97\uff0c\u8fd9\u91cc\u9700\u8981\u5206\u522b\u8ba8\u8bba\u8fd9\u4e24\u4e2a\u6b65\u9aa4\u7684\u5b9e\u73b0\u65b9\u6cd5\u3002","title":"Plane sweeping"},{"location":"other_categories/others/space_sweep/#_1","text":"\u6807\u51c6\u7684\u601d\u8def\u662f\u5bf9\u4e8e\u6bcf\u4e00\u4e2a\u5e73\u9762 z_i ,\u56fe\u7247\u4e0a\u7684\u6bcf\u4e00\u4e2akeypoint\u90fd\u9700\u8981\u8fd0\u884c\u4e00\u4e2a\u9006\u6295\u5f71\uff0c\u8fd9\u4e2a\u65b9\u6cd5\u6bd4\u8f83\u6162\uff0c\u4f5c\u8005\u63d0\u51fa\uff0c\u53ef\u4ee5\u5148\u5c06\u56fe\u50cf\u70b9\u6295\u5f71\u5230\u67d0\u4e00\u4e2a\u5e73\u9762 z_0 \u5904\uff0c\u7136\u540e\u518d\u5e73\u884c\u7684\u5176\u4ed6\u5e73\u9762\u4e0a\u7684\u6295\u5f71\u53ef\u4ee5\u7531\u4e24\u4e2a\u5355\u5e94\u6027\u77e9\u9635 H \u53d8\u6362\u800c\u5f97. \\begin{array}{l} {x_{i}=\\delta x_{0}+(1-\\delta) C_{x}} \\\\ {y_{i}=\\delta y_{0}+(1-\\delta) C_{y}} \\end{array} \\delta = (z_i - C_z)/(z_0 - C_z) C_x, C_y, C_z = -r_{1} \\cdot t, -r_{2} \\cdot t, -r_{3} \\cdot t \u76f8\u5f53\u4e8e\u662f\u4f7f\u7528\u76f8\u4f3c\u4e09\u89d2\u5f62\u5728\u8fdb\u884c\u8ba1\u7b97\u3002","title":"\u6295\u5f71"},{"location":"other_categories/others/space_sweep/#voting","text":"\u4e25\u8c28\u6765\u8bf4\uff0c\u540c\u4e00\u4e2apoint\u53ef\u4ee5\u7ed9\u6295\u5f71\u70b9\u9644\u4ef6\u4e00\u4e2a\u533a\u57df\u7684cell\u8fdb\u884c\u6295\u7968\uff0c\u7406\u8bba\u4e0a\u6765\u8bf4\u9700\u8981\u6c42jacobian\u3002\u672c\u6587\u6700\u7ec8\u91c7\u53d6\u7684\u65b9\u5f0f\u662f\u8ba9\u8ddd\u79bb\u76f8\u540c\u7684\u70b9\u6295\u7968\u8303\u56f4\u76f8\u540c\uff0c\u5e76\u4e14\u4e3a\u4e00\u4e2aconstant\u3002\u4e5f\u5c31\u662f\u8bf4\u6295\u7968\u7684\u8303\u56f4\u3001\u91c7\u6837\u6982\u7387\u4ec5\u4e0e\u8ddd\u79bb\u6210\u6b63\u6bd4\u5373\u53ef\u3002 \u6700\u7ec8\u901a\u8fc7\u9608\u503c\u8bbe\u7f6e\u5f97\u5230solid 3D point.","title":"voting"},{"location":"other_categories/others/surround_depth/","text":"About Surround Monodepth \u672c\u9875\u5305\u542b\u5173\u4e8e\u73af\u89c6\u6444\u50cf\u5934\u7684\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u3002\u8fd9\u4e2a\u4efb\u52a1\u4e0e\u4f20\u7edf\u7684monodepth2\u7684\u76f8\u540c\u4e0e\u533a\u522b: \u5bf9\u4e8e\u4e00\u7ec4\u73af\u89c6\u6444\u50cf\u5934\uff0c\u6bcf\u4e00\u4e2a\u6444\u50cf\u5934\u7684\u7f51\u7edc\u63a8\u7406\u90fd\u662f\u72ec\u7acb\u8fdb\u884c\u7684\uff0c\u56e0\u6b64\u63a8\u7406\u65b9\u5f0f\u4e0emonodepth2\u4e00\u81f4\u3002 \u4e0emonodepth2\u4e0d\u540c\uff0c\u5c3d\u7ba1\u63a8\u7406\u7ed3\u6784\u53ea\u6709\u5355\u4e2a\u6444\u50cf\u5934\uff0c\u7f51\u7edc\u7684\u8f93\u51fa\u662f\u5e26\u6709\u5c3a\u5ea6\u7684\u6df1\u5ea6\uff0c\u53cd\u5e94\u5b9e\u9645\u7684\u73af\u5883\u5c3a\u5ea6\u3002monodepth2\u5728\u8fd9\u4e2a\u8bbe\u5b9a\u4e0b\u4e3b\u8981\u5173\u6ce8\u7684\u662f\u76f8\u5bf9\u6df1\u5ea6\uff0c\u800c\u4e0d\u9700\u8981\u5b9e\u9645\u73af\u5883\u6df1\u5ea6\u3002 \u4e0d\u540c\u6444\u50cf\u5934\u4e4b\u95f4\u6709\u5c0f\u90e8\u5206\u7684\u53ef\u89c6\u7a7a\u95f4\u91cd\u53e0\u3002\u53ef\u4ee5\u5229\u7528\u91cd\u53e0\u90e8\u5206\u4ee5\u53ca\u76f8\u673a\u4e4b\u95f4\u7684\u5916\u53c2\uff0c\u83b7\u53d6\u5b9e\u9645\u7684\u5c3a\u5ea6\u3002 \u91c7\u7528\u7684\u6570\u636e\uff0c\u4e3b\u8981\u662fnuscenes\uff0c\u4ee5\u53ca toyota\u7684DDAD\u6570\u636e\u96c6 Full Surround Monodepth from Multiple Cameras pdf \u63d0\u51facontext picture\u7684\u6982\u5ff5\uff0c\u5141\u8bb8context picture\u6765\u81ea\u4e8e\u4e0d\u540c\u5e27\u7684\u4e0d\u540c\u6444\u50cf\u673a\u4f4d\u7f6e\uff0c\u53ea\u8981\u6709\u91cd\u53e0\u7a7a\u95f4\u5373\u53ef\u3002 \u9884\u6d4bpose\u7684\u65f6\u5019\uff0c\u90fd\u9884\u6d4b\u524d\u5411\u6444\u50cf\u5934\u7684pose(\u7ecf\u5e38\u63a5\u8fd1\u4e8eidentity transform\uff0c \u5bb9\u6613\u9884\u6d4b)\uff0c \u518d\u6839\u636e\u76f8\u673a\u5916\u53c2\u628a\u76f8\u5bf9\u4f4d\u79fb\u8f6c\u5230\u5f53\u524d\u9884\u6d4b\u7684\u6444\u50cf\u5934\u4e0a\u3002\u53ef\u4ee5\u589e\u5f3apose\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002 \u540c\u4e00\u5e27\u56fe\u7247\u4e0d\u540c\u6444\u50cf\u673a\u9884\u6d4b\u7684\u4f4d\u79fb\uff0c\u5728\u8f6c\u5230\u524d\u5411\u5750\u6807\u7cfb\u540e\uff0c\u5e94\u8be5\u90fd\u662f\u76f8\u4f3c\u7684\u503c\uff0c\u6240\u4ee5\u672c\u6587\u63d0\u51fapose consistency loss\u7ea6\u675f\u540c\u4e00\u5e27pose\u7684\u76f8\u4f3c\u6027\u3002\u5b83\u4e0e\u7b2c\u4e00\u70b9\u7ed3\u5408\u53ef\u4ee5\u63d0\u5347\u540c\u4e00\u5e27\u4e0d\u540c\u6444\u50cf\u673a\u6df1\u5ea6\u7684\u534f\u8c03\u6027\u3002 \u52a0\u5165 non-overlapping area\u7ea6\u675f\u91cd\u5efa\u6df1\u5ea6\u3002\u7528torch grid_sample\u91cd\u5efa\u56fe\u7247\u7684\u65f6\u5019\uff0c\u5982\u679c\u539f\u6765\u7684\u6df1\u5ea6\u70b9\u4e91\u5728\u65b0\u76f8\u673a\u4e2d\u7684\u6295\u5f71\u662f\u5728\u56fe\u7247\u5916\u7684\u65f6\u5019\uff0c\u8be5\u51fd\u6570\u4f1a\u53d6\u8fb9\u754c\u70b9\u8fdb\u884c\u63d2\u503c\uff0c\u5728\u4e0d\u540c\u6444\u50cf\u673a\u76f8\u4e92\u76d1\u7763\uff0c\u6216\u8005\u4e0d\u540c\u56fe\u7247\u95f4\u91cd\u53e0\u91cf\u5f88\u5c11\u7684\u65f6\u5019\u8fd9\u4e2amask\u975e\u5e38\u91cd\u8981\u3002 \u52a0\u5165 self-occlusion mask, \u5728\u4e00\u4e9b\u6570\u636e\u96c6\u4e0a\uff0c\u6709\u4e00\u90e8\u5206\u7684\u8f66\u5b50\u662f\u4f1a\u5728\u76f8\u673a\u4e2d\u6709\u6295\u5f71\uff0c\u5728\u8fd0\u884c\u4e2d\u4e00\u76f4\u8ddf\u7740\u8f66\u5b50\u79fb\u52a8\u3002\u8fd9\u7bc7paper\u624b\u52a8\u5730\u7ed9\u6bcf\u4e00\u4e2a\u6570\u636e\u96c6\u6bcf\u4e00\u4e2a\u8f66\u5b50\u7684\u6bcf\u4e00\u4e2a\u6444\u50cf\u5934\u753b\u4e00\u4e2amask, \u628a\u8f66\u5b50\u7684\u6295\u5f71\u53bb\u6389\u4e0d\u8ba1\u7b97\u635f\u5931\u3002(Nuscenes\u7684\u540e\u6444\u50cf\u5934\u5c31\u6709\u8f66\u8eab\u7684\u4e00\u90e8\u5206\uff0c DDAD\u7684\u6444\u50cf\u5934\u4e5f\u53ef\u80fd\u6709) SurroundDepth: Entangling Surrounding Views for Self-Supervised Multi-Camera Depth Estimation pdf code \u4f7f\u7528CVT\u878d\u5408\u591a\u5e27\u7684\u7279\u5f81 \u4f7f\u7528SFM pretrain \u540c\u6837\u53d1\u73b0\u4e86\u76f4\u63a5\u7528pose transformation\u8bad\u7ec3\u539f\u59cb\u521d\u59cb\u5316\u7684\u7f51\u7edc\u662f\u8bad\u7ec3\u4e0d\u51fa\u6765\u7684\uff0c\u56e0\u800c\u9700\u8981sfm\u505a\u6df1\u5ea6\u9884\u8bad\u7ec3\u3002 \u540c\u6837\u9700\u8981FSM\u63d0\u5230\u7684mask","title":"About Surround Monodepth"},{"location":"other_categories/others/surround_depth/#about-surround-monodepth","text":"\u672c\u9875\u5305\u542b\u5173\u4e8e\u73af\u89c6\u6444\u50cf\u5934\u7684\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u3002\u8fd9\u4e2a\u4efb\u52a1\u4e0e\u4f20\u7edf\u7684monodepth2\u7684\u76f8\u540c\u4e0e\u533a\u522b: \u5bf9\u4e8e\u4e00\u7ec4\u73af\u89c6\u6444\u50cf\u5934\uff0c\u6bcf\u4e00\u4e2a\u6444\u50cf\u5934\u7684\u7f51\u7edc\u63a8\u7406\u90fd\u662f\u72ec\u7acb\u8fdb\u884c\u7684\uff0c\u56e0\u6b64\u63a8\u7406\u65b9\u5f0f\u4e0emonodepth2\u4e00\u81f4\u3002 \u4e0emonodepth2\u4e0d\u540c\uff0c\u5c3d\u7ba1\u63a8\u7406\u7ed3\u6784\u53ea\u6709\u5355\u4e2a\u6444\u50cf\u5934\uff0c\u7f51\u7edc\u7684\u8f93\u51fa\u662f\u5e26\u6709\u5c3a\u5ea6\u7684\u6df1\u5ea6\uff0c\u53cd\u5e94\u5b9e\u9645\u7684\u73af\u5883\u5c3a\u5ea6\u3002monodepth2\u5728\u8fd9\u4e2a\u8bbe\u5b9a\u4e0b\u4e3b\u8981\u5173\u6ce8\u7684\u662f\u76f8\u5bf9\u6df1\u5ea6\uff0c\u800c\u4e0d\u9700\u8981\u5b9e\u9645\u73af\u5883\u6df1\u5ea6\u3002 \u4e0d\u540c\u6444\u50cf\u5934\u4e4b\u95f4\u6709\u5c0f\u90e8\u5206\u7684\u53ef\u89c6\u7a7a\u95f4\u91cd\u53e0\u3002\u53ef\u4ee5\u5229\u7528\u91cd\u53e0\u90e8\u5206\u4ee5\u53ca\u76f8\u673a\u4e4b\u95f4\u7684\u5916\u53c2\uff0c\u83b7\u53d6\u5b9e\u9645\u7684\u5c3a\u5ea6\u3002 \u91c7\u7528\u7684\u6570\u636e\uff0c\u4e3b\u8981\u662fnuscenes\uff0c\u4ee5\u53ca toyota\u7684DDAD\u6570\u636e\u96c6","title":"About Surround Monodepth"},{"location":"other_categories/others/surround_depth/#full-surround-monodepth-from-multiple-cameras","text":"pdf \u63d0\u51facontext picture\u7684\u6982\u5ff5\uff0c\u5141\u8bb8context picture\u6765\u81ea\u4e8e\u4e0d\u540c\u5e27\u7684\u4e0d\u540c\u6444\u50cf\u673a\u4f4d\u7f6e\uff0c\u53ea\u8981\u6709\u91cd\u53e0\u7a7a\u95f4\u5373\u53ef\u3002 \u9884\u6d4bpose\u7684\u65f6\u5019\uff0c\u90fd\u9884\u6d4b\u524d\u5411\u6444\u50cf\u5934\u7684pose(\u7ecf\u5e38\u63a5\u8fd1\u4e8eidentity transform\uff0c \u5bb9\u6613\u9884\u6d4b)\uff0c \u518d\u6839\u636e\u76f8\u673a\u5916\u53c2\u628a\u76f8\u5bf9\u4f4d\u79fb\u8f6c\u5230\u5f53\u524d\u9884\u6d4b\u7684\u6444\u50cf\u5934\u4e0a\u3002\u53ef\u4ee5\u589e\u5f3apose\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002 \u540c\u4e00\u5e27\u56fe\u7247\u4e0d\u540c\u6444\u50cf\u673a\u9884\u6d4b\u7684\u4f4d\u79fb\uff0c\u5728\u8f6c\u5230\u524d\u5411\u5750\u6807\u7cfb\u540e\uff0c\u5e94\u8be5\u90fd\u662f\u76f8\u4f3c\u7684\u503c\uff0c\u6240\u4ee5\u672c\u6587\u63d0\u51fapose consistency loss\u7ea6\u675f\u540c\u4e00\u5e27pose\u7684\u76f8\u4f3c\u6027\u3002\u5b83\u4e0e\u7b2c\u4e00\u70b9\u7ed3\u5408\u53ef\u4ee5\u63d0\u5347\u540c\u4e00\u5e27\u4e0d\u540c\u6444\u50cf\u673a\u6df1\u5ea6\u7684\u534f\u8c03\u6027\u3002 \u52a0\u5165 non-overlapping area\u7ea6\u675f\u91cd\u5efa\u6df1\u5ea6\u3002\u7528torch grid_sample\u91cd\u5efa\u56fe\u7247\u7684\u65f6\u5019\uff0c\u5982\u679c\u539f\u6765\u7684\u6df1\u5ea6\u70b9\u4e91\u5728\u65b0\u76f8\u673a\u4e2d\u7684\u6295\u5f71\u662f\u5728\u56fe\u7247\u5916\u7684\u65f6\u5019\uff0c\u8be5\u51fd\u6570\u4f1a\u53d6\u8fb9\u754c\u70b9\u8fdb\u884c\u63d2\u503c\uff0c\u5728\u4e0d\u540c\u6444\u50cf\u673a\u76f8\u4e92\u76d1\u7763\uff0c\u6216\u8005\u4e0d\u540c\u56fe\u7247\u95f4\u91cd\u53e0\u91cf\u5f88\u5c11\u7684\u65f6\u5019\u8fd9\u4e2amask\u975e\u5e38\u91cd\u8981\u3002 \u52a0\u5165 self-occlusion mask, \u5728\u4e00\u4e9b\u6570\u636e\u96c6\u4e0a\uff0c\u6709\u4e00\u90e8\u5206\u7684\u8f66\u5b50\u662f\u4f1a\u5728\u76f8\u673a\u4e2d\u6709\u6295\u5f71\uff0c\u5728\u8fd0\u884c\u4e2d\u4e00\u76f4\u8ddf\u7740\u8f66\u5b50\u79fb\u52a8\u3002\u8fd9\u7bc7paper\u624b\u52a8\u5730\u7ed9\u6bcf\u4e00\u4e2a\u6570\u636e\u96c6\u6bcf\u4e00\u4e2a\u8f66\u5b50\u7684\u6bcf\u4e00\u4e2a\u6444\u50cf\u5934\u753b\u4e00\u4e2amask, \u628a\u8f66\u5b50\u7684\u6295\u5f71\u53bb\u6389\u4e0d\u8ba1\u7b97\u635f\u5931\u3002(Nuscenes\u7684\u540e\u6444\u50cf\u5934\u5c31\u6709\u8f66\u8eab\u7684\u4e00\u90e8\u5206\uff0c DDAD\u7684\u6444\u50cf\u5934\u4e5f\u53ef\u80fd\u6709)","title":"Full Surround Monodepth from Multiple Cameras"},{"location":"other_categories/others/surround_depth/#surrounddepth-entangling-surrounding-views-for-self-supervised-multi-camera-depth-estimation","text":"pdf code \u4f7f\u7528CVT\u878d\u5408\u591a\u5e27\u7684\u7279\u5f81 \u4f7f\u7528SFM pretrain \u540c\u6837\u53d1\u73b0\u4e86\u76f4\u63a5\u7528pose transformation\u8bad\u7ec3\u539f\u59cb\u521d\u59cb\u5316\u7684\u7f51\u7edc\u662f\u8bad\u7ec3\u4e0d\u51fa\u6765\u7684\uff0c\u56e0\u800c\u9700\u8981sfm\u505a\u6df1\u5ea6\u9884\u8bad\u7ec3\u3002 \u540c\u6837\u9700\u8981FSM\u63d0\u5230\u7684mask","title":"SurroundDepth: Entangling Surrounding Views for Self-Supervised Multi-Camera Depth Estimation"},{"location":"other_categories/others/track2detection_and_seg/","text":"Track to Detect and Segment: An Online Multi-Object Tracker \u8fd9\u7bc7paper\u4ee5 CenterTrack \u4e3a\u57fa\u51c6. CenterTrack\u4f1a\u5229\u7528\u4e0a\u4e00\u5e27\u7684\u9884\u6d4b\u70ed\u56fe\u8f85\u52a9\u672c\u5e27\u7684\u9884\u6d4b\uff0c\u540c\u65f6\u8fd8\u4f1a\u8f93\u51fa\u4e00\u4e2aOffset\u6765\u8f85\u52a9data association. \u672c\u6587\u6709\u4e24\u4e2a\u7a81\u7834\u70b9\uff0c\u7b2c\u4e00\u4e2a\u662f\u5229\u7528\u7c7b\u4f3c RAFT \u7684\u7ed3\u6784\u7aef\u5230\u7aef\u5b66\u4e60\u4e24\u5e27\u4e4b\u95f4\u7684\u5339\u914d\u3002\u7136\u540e\u5229\u7528\u53ef\u53d8\u5f62\u5377\u79ef\u5c06\u524d\u4e00\u5e27\u7684\u7279\u5f81\u878d\u5165\u5230\u672c\u5e27\u4e2d\u8fdb\u884c\u9884\u6d4b\u3002 Model Architecture \u7279\u5f81\u63d0\u53d6\u7f51\u7edc\u91c7\u7528\u7684\u662fCenterNet, \u7136\u540e\u4f7f\u7528\u51e0\u4e2a\u5377\u79ef\u5c42\u4ee5\u53ca\u4e0b\u91c7\u6837\u5c42\u8f93\u51fa e \\in \\mathbb{R}^{H_C\\times W_C\\times 128} . \u8f93\u51fa\u7684\u7279\u5f81\u56fe\u5927\u5c0f\u4e3a\u8f93\u5165\u56fe\u7247\u7684 1/8 . Cost Volume \u7684\u751f\u6210\u4e0e RAFT \u7c7b\u4f3c\uff0c\u5c06\u4e24\u5e27\u56fe\u5404\u4e2a\u7279\u5f81\u76f8\u4e92\u5185\u79ef\uff0c\u5f97\u5230Cost Volume C\\in \\mathbb{R}^{H_C\\times W_C\\times H_C\\times W_C} . C_{i,j,k,l} = e^{'t}_{i,j} e^{'t-\\tau}_{k,l} Offset\u7684\u8f93\u51fa\u65b9\u6cd5: \u5148\u5bf9\u4e24\u4e2a\u65b9\u5411\u5206\u522b\u505aMaxpooling/softmax\uff0c\u5f97\u5230 C^W_{i,j} \\in [0, 1]^{1\\times W_C} \u4ee5\u53ca C^H_{i,j}\\in[0, 1]^{H_C \\times 1} . \u7136\u540e\u52a0\u6743\u5e73\u5747\u5f97\u5230\u4e24\u4e2a\u65b9\u5411\u7684offset. \u8ba1\u7b97\u65b9\u6cd5\u548c\u5149\u6d41\u7684\u8ba1\u7b97\u5f88\u76f8\u4f3c\u3002 \u4e3a\u4e86\u76d1\u7763\u8fd9\u4e2aoffset, \u8fd9\u91cc\u91c7\u7528tracking label\u4e2d\u5fc3\u7684offset\u6765\u8bad\u7ec3\u8fd9\u4e2a\u7f51\u7edc.","title":"Track to Detect and Segment: An Online Multi-Object Tracker"},{"location":"other_categories/others/track2detection_and_seg/#track-to-detect-and-segment-an-online-multi-object-tracker","text":"\u8fd9\u7bc7paper\u4ee5 CenterTrack \u4e3a\u57fa\u51c6. CenterTrack\u4f1a\u5229\u7528\u4e0a\u4e00\u5e27\u7684\u9884\u6d4b\u70ed\u56fe\u8f85\u52a9\u672c\u5e27\u7684\u9884\u6d4b\uff0c\u540c\u65f6\u8fd8\u4f1a\u8f93\u51fa\u4e00\u4e2aOffset\u6765\u8f85\u52a9data association. \u672c\u6587\u6709\u4e24\u4e2a\u7a81\u7834\u70b9\uff0c\u7b2c\u4e00\u4e2a\u662f\u5229\u7528\u7c7b\u4f3c RAFT \u7684\u7ed3\u6784\u7aef\u5230\u7aef\u5b66\u4e60\u4e24\u5e27\u4e4b\u95f4\u7684\u5339\u914d\u3002\u7136\u540e\u5229\u7528\u53ef\u53d8\u5f62\u5377\u79ef\u5c06\u524d\u4e00\u5e27\u7684\u7279\u5f81\u878d\u5165\u5230\u672c\u5e27\u4e2d\u8fdb\u884c\u9884\u6d4b\u3002","title":"Track to Detect and Segment: An Online Multi-Object Tracker"},{"location":"other_categories/others/track2detection_and_seg/#model-architecture","text":"\u7279\u5f81\u63d0\u53d6\u7f51\u7edc\u91c7\u7528\u7684\u662fCenterNet, \u7136\u540e\u4f7f\u7528\u51e0\u4e2a\u5377\u79ef\u5c42\u4ee5\u53ca\u4e0b\u91c7\u6837\u5c42\u8f93\u51fa e \\in \\mathbb{R}^{H_C\\times W_C\\times 128} . \u8f93\u51fa\u7684\u7279\u5f81\u56fe\u5927\u5c0f\u4e3a\u8f93\u5165\u56fe\u7247\u7684 1/8 . Cost Volume \u7684\u751f\u6210\u4e0e RAFT \u7c7b\u4f3c\uff0c\u5c06\u4e24\u5e27\u56fe\u5404\u4e2a\u7279\u5f81\u76f8\u4e92\u5185\u79ef\uff0c\u5f97\u5230Cost Volume C\\in \\mathbb{R}^{H_C\\times W_C\\times H_C\\times W_C} . C_{i,j,k,l} = e^{'t}_{i,j} e^{'t-\\tau}_{k,l} Offset\u7684\u8f93\u51fa\u65b9\u6cd5: \u5148\u5bf9\u4e24\u4e2a\u65b9\u5411\u5206\u522b\u505aMaxpooling/softmax\uff0c\u5f97\u5230 C^W_{i,j} \\in [0, 1]^{1\\times W_C} \u4ee5\u53ca C^H_{i,j}\\in[0, 1]^{H_C \\times 1} . \u7136\u540e\u52a0\u6743\u5e73\u5747\u5f97\u5230\u4e24\u4e2a\u65b9\u5411\u7684offset. \u8ba1\u7b97\u65b9\u6cd5\u548c\u5149\u6d41\u7684\u8ba1\u7b97\u5f88\u76f8\u4f3c\u3002 \u4e3a\u4e86\u76d1\u7763\u8fd9\u4e2aoffset, \u8fd9\u91cc\u91c7\u7528tracking label\u4e2d\u5fc3\u7684offset\u6765\u8bad\u7ec3\u8fd9\u4e2a\u7f51\u7edc.","title":"Model Architecture"},{"location":"other_categories/others/vectornet/","text":"VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation \u8fd9\u7bc7paper\u662f\u8f66\u8f86\u7684\u8f68\u8ff9\u9884\u6d4b\u4e2d\u503c\u5f97\u5173\u6ce8\u7684\u601d\u8def \u4e3b\u8981\u7684\u601d\u8def\u662f\u4f7f\u7528\u591a\u9879\u5f0f\u53bb\u8868\u8fbe\u4e34\u8fd1\u8f66\u9053\uff0c\u5e76\u4f7f\u7528GNN/attention\u53bb\u878d\u5408\u9053\u8def\u4fe1\u606f\uff0c\u800c\u4e0d\u662f\u7528\u56fe\u50cf\u7ea7\u522b\u7684rasterized HD map.","title":"VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation"},{"location":"other_categories/others/vectornet/#vectornet-encoding-hd-maps-and-agent-dynamics-from-vectorized-representation","text":"\u8fd9\u7bc7paper\u662f\u8f66\u8f86\u7684\u8f68\u8ff9\u9884\u6d4b\u4e2d\u503c\u5f97\u5173\u6ce8\u7684\u601d\u8def \u4e3b\u8981\u7684\u601d\u8def\u662f\u4f7f\u7528\u591a\u9879\u5f0f\u53bb\u8868\u8fbe\u4e34\u8fd1\u8f66\u9053\uff0c\u5e76\u4f7f\u7528GNN/attention\u53bb\u878d\u5408\u9053\u8def\u4fe1\u606f\uff0c\u800c\u4e0d\u662f\u7528\u56fe\u50cf\u7ea7\u522b\u7684rasterized HD map.","title":"VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation"},{"location":"other_categories/others/vip_deeplab/","text":"ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation \u8fd9\u7bc7paper\u4e3b\u8981\u7814\u7a76\u7684\u662f\u89c6\u9891\u4e2d\u7684panoptic segmentation. \u6bd4\u8f83\u5947\u5999\u7684\u662f\u6700\u7ec8\u8fd8\u80fd\u8fbe\u5230\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u7684SOTA\u7ed3\u679c. \u4efb\u52a1\u63cf\u8ff0 \u5728\u89c6\u9891\u4e2d\uff0c\u5168\u666f\u5206\u5272\u7684\u6bcf\u4e00\u4e2ainstance\u88ab\u63cf\u8ff0\u4e3a\u4e00\u4e2a\u5e26\u6709\u65f6\u95f4\u7ef4\u7684tube, \u672c\u6587\u63d0\u51fa\u4e86Video Panoptic Quality (VPQ) \u4f5c\u4e3a\u8bc4\u5224\u6807\u51c6,\u5176\u5b9a\u4e49\u4e3a \\text{VPQ}^k = \\frac{1}{N_{classes}} \\sum_c \\frac{\\sum_{(u,\\hat u) \\in \\text{TP}_c} \\text{IoU}(u, \\hat u)}{|TP_c| + \\frac{1}{2}|FP_c| + \\frac{1}{2} |FN_c|} \u5176\u4e2d \\text{TP} = \\{ (u, \\hat u) \\in U \\times \\hat U : \\text{IoU} (u, \\hat u) > 0.5 \\} \u672c\u6587\u5168\u666f\u5206\u5272\u7684\u7b97\u6cd5\u7ee7\u627f panoptic-DeepLab\uff0c \u5206\u6210\u4e09\u4e2a\u5b50\u4efb\u52a1: \u5bf9\"thing\" \u4e0e \"stuff\"\u8fdb\u884c\u8bed\u4e49\u5206\u5272 \u5bf9\u6bcf\u4e00\u4e2a\"thing\"\u7684instance, \u9884\u6d4b\u5176\u4e2d\u5fc3pixel \u5bf9\u6bcf\u4e00\u4e2a\"thing\" \u7684pixel\u9884\u6d4b\u5b83\u5230\u5176instance \u4e2d\u5fc3\u7684\u8ddd\u79bb\u3002 \u63a8\u7406\u7684\u65f6\u5019\u57fa\u4e8e\u7b2c\u4e8c\u4e2a\u4efb\u52a1\u4ee5\u53ca\u7b2c\u4e09\u4e2a\u4efb\u52a1\u5b9e\u73b0\u5bf9\u540cinstance \u50cf\u7d20\u7684\u805a\u7c7b\u3002 \u7f51\u7edc\u7ed3\u679c \u8fd9\u7bc7paper\u63d0\u51fa\u7684\u7f51\u7edc\u67b6\u6784\u53ef\u4ee5\u540c\u65f6\u63a8\u7406\u4e24\u5f20\u56fe\uff0c\u5e76\u8fdb\u884c\u6df1\u5ea6\u9884\u6d4b. \u6df1\u5ea6\u9884\u6d4b\u9009\u62e9\u7684loss: \\mathcal{L}_{depth}(d, \\hat d) = \\frac{1}{n}\\sum_i(\\log d_i - \\log \\hat d_i)^2 - \\frac{1}{n^2}(\\sum_i \\log d_i - \\log \\hat d_i)^2 + (\\frac{1}{n} \\sum_i (\\frac{d_i - \\hat d_i}{d_i})^2)^0.5","title":"ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation"},{"location":"other_categories/others/vip_deeplab/#vip-deeplab-learning-visual-perception-with-depth-aware-video-panoptic-segmentation","text":"\u8fd9\u7bc7paper\u4e3b\u8981\u7814\u7a76\u7684\u662f\u89c6\u9891\u4e2d\u7684panoptic segmentation. \u6bd4\u8f83\u5947\u5999\u7684\u662f\u6700\u7ec8\u8fd8\u80fd\u8fbe\u5230\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u7684SOTA\u7ed3\u679c.","title":"ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation"},{"location":"other_categories/others/vip_deeplab/#_1","text":"\u5728\u89c6\u9891\u4e2d\uff0c\u5168\u666f\u5206\u5272\u7684\u6bcf\u4e00\u4e2ainstance\u88ab\u63cf\u8ff0\u4e3a\u4e00\u4e2a\u5e26\u6709\u65f6\u95f4\u7ef4\u7684tube, \u672c\u6587\u63d0\u51fa\u4e86Video Panoptic Quality (VPQ) \u4f5c\u4e3a\u8bc4\u5224\u6807\u51c6,\u5176\u5b9a\u4e49\u4e3a \\text{VPQ}^k = \\frac{1}{N_{classes}} \\sum_c \\frac{\\sum_{(u,\\hat u) \\in \\text{TP}_c} \\text{IoU}(u, \\hat u)}{|TP_c| + \\frac{1}{2}|FP_c| + \\frac{1}{2} |FN_c|} \u5176\u4e2d \\text{TP} = \\{ (u, \\hat u) \\in U \\times \\hat U : \\text{IoU} (u, \\hat u) > 0.5 \\} \u672c\u6587\u5168\u666f\u5206\u5272\u7684\u7b97\u6cd5\u7ee7\u627f panoptic-DeepLab\uff0c \u5206\u6210\u4e09\u4e2a\u5b50\u4efb\u52a1: \u5bf9\"thing\" \u4e0e \"stuff\"\u8fdb\u884c\u8bed\u4e49\u5206\u5272 \u5bf9\u6bcf\u4e00\u4e2a\"thing\"\u7684instance, \u9884\u6d4b\u5176\u4e2d\u5fc3pixel \u5bf9\u6bcf\u4e00\u4e2a\"thing\" \u7684pixel\u9884\u6d4b\u5b83\u5230\u5176instance \u4e2d\u5fc3\u7684\u8ddd\u79bb\u3002 \u63a8\u7406\u7684\u65f6\u5019\u57fa\u4e8e\u7b2c\u4e8c\u4e2a\u4efb\u52a1\u4ee5\u53ca\u7b2c\u4e09\u4e2a\u4efb\u52a1\u5b9e\u73b0\u5bf9\u540cinstance \u50cf\u7d20\u7684\u805a\u7c7b\u3002","title":"\u4efb\u52a1\u63cf\u8ff0"},{"location":"other_categories/others/vip_deeplab/#_2","text":"\u8fd9\u7bc7paper\u63d0\u51fa\u7684\u7f51\u7edc\u67b6\u6784\u53ef\u4ee5\u540c\u65f6\u63a8\u7406\u4e24\u5f20\u56fe\uff0c\u5e76\u8fdb\u884c\u6df1\u5ea6\u9884\u6d4b. \u6df1\u5ea6\u9884\u6d4b\u9009\u62e9\u7684loss: \\mathcal{L}_{depth}(d, \\hat d) = \\frac{1}{n}\\sum_i(\\log d_i - \\log \\hat d_i)^2 - \\frac{1}{n^2}(\\sum_i \\log d_i - \\log \\hat d_i)^2 + (\\frac{1}{n} \\sum_i (\\frac{d_i - \\hat d_i}{d_i})^2)^0.5","title":"\u7f51\u7edc\u7ed3\u679c"},{"location":"other_categories/undone/ApplicationofHDRalgorithmstosolvedirectsunlightproblemswhenautonomousvehiclesusingmachinevisionsystemsaredrivingintosun/","text":"Application of HDR algorithms to solve direct sunlight problems when autonomous vehicles using machine vision systems are driving into sun","title":"Application of HDR algorithms to solve direct sunlight problems when autonomous vehicles using machine vision systems are driving into sun"},{"location":"other_categories/undone/ApplicationofHDRalgorithmstosolvedirectsunlightproblemswhenautonomousvehiclesusingmachinevisionsystemsaredrivingintosun/#application-of-hdr-algorithms-to-solve-direct-sunlight-problems-when-autonomous-vehicles-using-machine-vision-systems-are-driving-into-sun","text":"","title":"Application of HDR algorithms to solve direct sunlight problems when autonomous vehicles using machine vision systems are driving into sun"},{"location":"other_categories/undone/Autonomous%20%20Racing%20%20using%20%20Learning%20%20Model%20%20Predictive%20%20Control/","text":"Autonomous Racing using Learning Model Predictive Control","title":"Autonomous Racing using Learning Model Predictive Control"},{"location":"other_categories/undone/Autonomous%20%20Racing%20%20using%20%20Learning%20%20Model%20%20Predictive%20%20Control/#autonomous-racing-using-learning-model-predictive-control","text":"","title":"Autonomous Racing using Learning Model Predictive Control"},{"location":"other_categories/undone/BeliefPropagationLayer/","text":"Belief Propagation Reloaded: Learning BP-Layers for Labeling Problems","title":"Belief Propagation Reloaded: Learning BP-Layers for Labeling Problems"},{"location":"other_categories/undone/BeliefPropagationLayer/#belief-propagation-reloaded-learning-bp-layers-for-labeling-problems","text":"","title":"Belief Propagation Reloaded: Learning BP-Layers for Labeling Problems"},{"location":"other_categories/undone/Beyond%20Pixels%20Leveraging%20Geometry%20and%20Shape%20Cues%20for%20OnlineMulti-Object%20Tracking/","text":"Beyond Pixels: Leveraging Geometry and Shape Cues for OnlineMulti-Object Tracking","title":"Beyond Pixels: Leveraging Geometry and Shape Cues for OnlineMulti-Object Tracking"},{"location":"other_categories/undone/Beyond%20Pixels%20Leveraging%20Geometry%20and%20Shape%20Cues%20for%20OnlineMulti-Object%20Tracking/#beyond-pixels-leveraging-geometry-and-shape-cues-for-onlinemulti-object-tracking","text":"","title":"Beyond Pixels: Leveraging Geometry and Shape Cues for OnlineMulti-Object Tracking"},{"location":"other_categories/undone/Data-Driven%20Modeling%20and%20Control%20of%20an%20Autonomous%20Race%20Car/","text":"Data-Driven Modeling and Control of an Autonomous Race Car","title":"Data-Driven Modeling and Control of an Autonomous Race Car"},{"location":"other_categories/undone/Data-Driven%20Modeling%20and%20Control%20of%20an%20Autonomous%20Race%20Car/#data-driven-modeling-and-control-of-an-autonomous-race-car","text":"","title":"Data-Driven Modeling and Control of an Autonomous Race Car"},{"location":"other_categories/undone/Deep%20Hough%20Voting%20for%203D%20Object%20Detection%20in%20Point%20Clouds/","text":"Deep Hough Voting for 3D Object Detection in Point Clouds","title":"Deep Hough Voting for 3D Object Detection in Point Clouds"},{"location":"other_categories/undone/Deep%20Hough%20Voting%20for%203D%20Object%20Detection%20in%20Point%20Clouds/#deep-hough-voting-for-3d-object-detection-in-point-clouds","text":"","title":"Deep Hough Voting for 3D Object Detection in Point Clouds"},{"location":"other_categories/undone/Deep%20Imitative%20Models%20for%20Flexible%20Inference%2CPlanning%2C%20and%20Control/","text":"Deep Imitative Models for Flexible Inference,Planning, and Control","title":"Deep Imitative Models for Flexible Inference,Planning, and Control"},{"location":"other_categories/undone/Deep%20Imitative%20Models%20for%20Flexible%20Inference%2CPlanning%2C%20and%20Control/#deep-imitative-models-for-flexible-inferenceplanning-and-control","text":"","title":"Deep Imitative Models for Flexible Inference,Planning, and Control"},{"location":"other_categories/undone/END-TO-END%20LEARNABLE%20HISTOGRAM%20FILTERS/","text":"END-TO-END LEARNABLE HISTOGRAM FILTERS","title":"END-TO-END LEARNABLE HISTOGRAM FILTERS"},{"location":"other_categories/undone/END-TO-END%20LEARNABLE%20HISTOGRAM%20FILTERS/#end-to-end-learnable-histogram-filters","text":"","title":"END-TO-END LEARNABLE HISTOGRAM FILTERS"},{"location":"other_categories/undone/End-To-End%20Multi-Modal%20Sensors%20Fusion%20System%20ForUrban%20Automated%20Driving/","text":"End-To-End Multi-Modal Sensors Fusion System For Urban Automated Driving","title":"End-To-End Multi-Modal Sensors Fusion System For Urban Automated Driving"},{"location":"other_categories/undone/End-To-End%20Multi-Modal%20Sensors%20Fusion%20System%20ForUrban%20Automated%20Driving/#end-to-end-multi-modal-sensors-fusion-system-for-urban-automated-driving","text":"","title":"End-To-End Multi-Modal Sensors Fusion System For Urban Automated Driving"},{"location":"other_categories/undone/Enhanced%20%20free%20%20space%20%20detection%20%20in%20%20multiple%20%20lanes%20%20based%20%20on%20%20single%20%20CNNwith%20%20scene%20%20identification/","text":"Enhanced free space detection in multiple lanes based on single CNN with scene identification","title":"Enhanced free space detection in multiple lanes based on single CNN with scene identification"},{"location":"other_categories/undone/Enhanced%20%20free%20%20space%20%20detection%20%20in%20%20multiple%20%20lanes%20%20based%20%20on%20%20single%20%20CNNwith%20%20scene%20%20identification/#enhanced-free-space-detection-in-multiple-lanes-based-on-single-cnn-with-scene-identification","text":"","title":"Enhanced free space detection in multiple lanes based on single CNN with scene identification"},{"location":"other_categories/undone/Exploring%20the%20structure%20of%20a%20real-time%2C%20arbitrary%20neuralartistic%20stylization%20network/","text":"Exploring the structure of a real-time, arbitrary neuralartistic stylization network","title":"Exploring the structure of a real-time, arbitrary neuralartistic stylization network"},{"location":"other_categories/undone/Exploring%20the%20structure%20of%20a%20real-time%2C%20arbitrary%20neuralartistic%20stylization%20network/#exploring-the-structure-of-a-real-time-arbitrary-neuralartistic-stylization-network","text":"","title":"Exploring the structure of a real-time, arbitrary neuralartistic stylization network"},{"location":"other_categories/undone/From%20Perception%20to%20Decision%20A%20Data-driven%20Approach%20to%20End-to-endMotion%20Planning%20for%20Autonomous%20Ground%20Robots/","text":"From Perception to Decision: A Data-driven Approach to End-to-end Motion Planning for Autonomous Ground Robots","title":"From Perception to Decision: A Data-driven Approach to End-to-end Motion Planning for Autonomous Ground Robots"},{"location":"other_categories/undone/From%20Perception%20to%20Decision%20A%20Data-driven%20Approach%20to%20End-to-endMotion%20Planning%20for%20Autonomous%20Ground%20Robots/#from-perception-to-decision-a-data-driven-approach-to-end-to-end-motion-planning-for-autonomous-ground-robots","text":"","title":"From Perception to Decision: A Data-driven Approach to End-to-end Motion Planning for Autonomous Ground Robots"},{"location":"other_categories/undone/IMAGENET-TRAINEDCNNS%20%20%20ARE%20%20%20BIASED%20%20%20TOWARDSTEXTURE%3BINCREASING%20SHAPE%20BIAS%20IMPROVESACCURACY%20AND%20ROBUSTNESS/","text":"IMAGENET-TRAINEDCNNS ARE BIASED TOWARDS TEXTURE;INCREASING SHAPE BIAS IMPROVES ACCURACY AND ROBUSTNESS","title":"IMAGENET-TRAINEDCNNS ARE BIASED TOWARDS TEXTURE;INCREASING SHAPE BIAS IMPROVES ACCURACY AND ROBUSTNESS"},{"location":"other_categories/undone/IMAGENET-TRAINEDCNNS%20%20%20ARE%20%20%20BIASED%20%20%20TOWARDSTEXTURE%3BINCREASING%20SHAPE%20BIAS%20IMPROVESACCURACY%20AND%20ROBUSTNESS/#imagenet-trainedcnns-are-biased-towards-textureincreasing-shape-bias-improves-accuracy-and-robustness","text":"","title":"IMAGENET-TRAINEDCNNS ARE BIASED TOWARDS TEXTURE;INCREASING SHAPE BIAS IMPROVES ACCURACY AND ROBUSTNESS"},{"location":"other_categories/undone/InfoGAN%20Interpretable%20Representation%20Learning%20byInformation%20Maximizing%20Generative%20Adversarial%20Nets/","text":"InfoGAN: Interpretable Representation Learning byInformation Maximizing Generative Adversarial Nets","title":"InfoGAN: Interpretable Representation Learning byInformation Maximizing Generative Adversarial Nets"},{"location":"other_categories/undone/InfoGAN%20Interpretable%20Representation%20Learning%20byInformation%20Maximizing%20Generative%20Adversarial%20Nets/#infogan-interpretable-representation-learning-byinformation-maximizing-generative-adversarial-nets","text":"","title":"InfoGAN: Interpretable Representation Learning byInformation Maximizing Generative Adversarial Nets"},{"location":"other_categories/undone/LP-3DCNN%20Unveiling%20Local%20Phase%20in%203D%20Convolutional%20Neural%20Networks/","text":"LP-3DCNN: Unveiling Local Phase in 3D Convolutional Neural Networks","title":"LP-3DCNN: Unveiling Local Phase in 3D Convolutional Neural Networks"},{"location":"other_categories/undone/LP-3DCNN%20Unveiling%20Local%20Phase%20in%203D%20Convolutional%20Neural%20Networks/#lp-3dcnn-unveiling-local-phase-in-3d-convolutional-neural-networks","text":"","title":"LP-3DCNN: Unveiling Local Phase in 3D Convolutional Neural Networks"},{"location":"other_categories/undone/LiDAR%20Sensor%20modeling%20and%20Data%20augmentationwith%20GANs%20for%20Autonomous%20driving/","text":"LiDAR Sensor modeling and Data augmentationwith GANs for Autonomous driving","title":"LiDAR Sensor modeling and Data augmentationwith GANs for Autonomous driving"},{"location":"other_categories/undone/LiDAR%20Sensor%20modeling%20and%20Data%20augmentationwith%20GANs%20for%20Autonomous%20driving/#lidar-sensor-modeling-and-data-augmentationwith-gans-for-autonomous-driving","text":"","title":"LiDAR Sensor modeling and Data augmentationwith GANs for Autonomous driving"},{"location":"other_categories/undone/Libra%20R-CNN%20Towards%20Balanced%20Learning%20for%20Object%20Detection/","text":"Libra R-CNN: Towards Balanced Learning for Object Detection","title":"Libra R-CNN: Towards Balanced Learning for Object Detection"},{"location":"other_categories/undone/Libra%20R-CNN%20Towards%20Balanced%20Learning%20for%20Object%20Detection/#libra-r-cnn-towards-balanced-learning-for-object-detection","text":"","title":"Libra R-CNN: Towards Balanced Learning for Object Detection"},{"location":"other_categories/undone/MPC-Inspired%20Neural%20Network%20Policies%20for%20Sequential%20Decision%20Making/","text":"MPC-Inspired Neural Network Policies for Sequential Decision Making","title":"MPC-Inspired Neural Network Policies for Sequential Decision Making"},{"location":"other_categories/undone/MPC-Inspired%20Neural%20Network%20Policies%20for%20Sequential%20Decision%20Making/#mpc-inspired-neural-network-policies-for-sequential-decision-making","text":"","title":"MPC-Inspired Neural Network Policies for Sequential Decision Making"},{"location":"other_categories/undone/MULTI-SCALEDENSENETWORKSFORRESOURCEEFFICIENTIMAGECLASSIFICATION/","text":"MULTI-SCALE DENSE NETWORKS FOR RESOURCE EFFICIENT IMAGE CLASSIFICATION","title":"MULTI-SCALE DENSE NETWORKS FOR RESOURCE EFFICIENT IMAGE CLASSIFICATION"},{"location":"other_categories/undone/MULTI-SCALEDENSENETWORKSFORRESOURCEEFFICIENTIMAGECLASSIFICATION/#multi-scale-dense-networks-for-resource-efficient-image-classification","text":"","title":"MULTI-SCALE DENSE NETWORKS FOR RESOURCE EFFICIENT IMAGE CLASSIFICATION"},{"location":"other_categories/undone/Motion%20Planning%20for%20Autonomous%20Driving%20with%20a%20Conformal%20Spatiotemporal%20Lattice/","text":"Motion Planning for Autonomous Driving with a Conformal Spatiotemporal Lattice","title":"Motion Planning for Autonomous Driving with a Conformal Spatiotemporal Lattice"},{"location":"other_categories/undone/Motion%20Planning%20for%20Autonomous%20Driving%20with%20a%20Conformal%20Spatiotemporal%20Lattice/#motion-planning-for-autonomous-driving-with-a-conformal-spatiotemporal-lattice","text":"","title":"Motion Planning for Autonomous Driving with a Conformal Spatiotemporal Lattice"},{"location":"other_categories/undone/Optimization%20Beyond%20the%20Convolution%20Generalizing%20Spatial%20Relationswith%20End-to-End%20Metric%20Learning/","text":"Optimization Beyond the Convolution: Generalizing Spatial Relationswith End-to-End Metric Learning","title":"Optimization Beyond the Convolution: Generalizing Spatial Relationswith End-to-End Metric Learning"},{"location":"other_categories/undone/Optimization%20Beyond%20the%20Convolution%20Generalizing%20Spatial%20Relationswith%20End-to-End%20Metric%20Learning/#optimization-beyond-the-convolution-generalizing-spatial-relationswith-end-to-end-metric-learning","text":"","title":"Optimization Beyond the Convolution: Generalizing Spatial Relationswith End-to-End Metric Learning"},{"location":"other_categories/undone/POI%20Multiple%20Object%20Tracking%20with%20HighPerformance%20Detection%20and%20Appearance%20Feature/","text":"POI: Multiple Object Tracking with HighPerformance Detection and Appearance Feature","title":"POI: Multiple Object Tracking with HighPerformance Detection and Appearance Feature"},{"location":"other_categories/undone/POI%20Multiple%20Object%20Tracking%20with%20HighPerformance%20Detection%20and%20Appearance%20Feature/#poi-multiple-object-tracking-with-highperformance-detection-and-appearance-feature","text":"","title":"POI: Multiple Object Tracking with HighPerformance Detection and Appearance Feature"},{"location":"other_categories/undone/Probabilistic%20Decision-Making%20under%20Uncertainty%20for%20AutonomousDriving%20using%20Continuous%20POMDPs/","text":"Probabilistic Decision-Making under Uncertainty for AutonomousDriving using Continuous POMDPs","title":"Probabilistic Decision-Making under Uncertainty for AutonomousDriving using Continuous POMDPs"},{"location":"other_categories/undone/Probabilistic%20Decision-Making%20under%20Uncertainty%20for%20AutonomousDriving%20using%20Continuous%20POMDPs/#probabilistic-decision-making-under-uncertainty-for-autonomousdriving-using-continuous-pomdps","text":"","title":"Probabilistic Decision-Making under Uncertainty for AutonomousDriving using Continuous POMDPs"},{"location":"other_categories/undone/RETHINKINGSELF-DRIVING%20%20MULTI-TASKKNOWL-EDGE%20%20FORBETTERGENERALIZATION%20%20ANDACCIDENTEXPLANATIONABILITY/","text":"RETHINKING SELF-DRIVING: MULTI-TASK KNOWLEDGE FOR BETTER GENERALIZATION AND ACCIDENT EXPLANATIONABILITY","title":"RETHINKING SELF-DRIVING: MULTI-TASK KNOWLEDGE FOR BETTER GENERALIZATION AND ACCIDENT EXPLANATIONABILITY"},{"location":"other_categories/undone/RETHINKINGSELF-DRIVING%20%20MULTI-TASKKNOWL-EDGE%20%20FORBETTERGENERALIZATION%20%20ANDACCIDENTEXPLANATIONABILITY/#rethinking-self-driving-multi-task-knowledge-for-better-generalization-and-accident-explanationability","text":"","title":"RETHINKING SELF-DRIVING: MULTI-TASK KNOWLEDGE FOR BETTER GENERALIZATION AND ACCIDENT EXPLANATIONABILITY"},{"location":"other_categories/undone/Real-Time%20Freespace%20Segmentation%20on%20AutonomousRobots%20for%20Detection%20of%20Obstacles%20and%20Drop-Offs/","text":"Real-Time Freespace Segmentation on AutonomousRobots for Detection of Obstacles and Drop-Offs","title":"Real-Time Freespace Segmentation on AutonomousRobots for Detection of Obstacles and Drop-Offs"},{"location":"other_categories/undone/Real-Time%20Freespace%20Segmentation%20on%20AutonomousRobots%20for%20Detection%20of%20Obstacles%20and%20Drop-Offs/#real-time-freespace-segmentation-on-autonomousrobots-for-detection-of-obstacles-and-drop-offs","text":"","title":"Real-Time Freespace Segmentation on AutonomousRobots for Detection of Obstacles and Drop-Offs"},{"location":"other_categories/undone/Restricted%20Deformable%20Convolution%20basedRoad%20Scene%20Semantic%20SegmentationUsing%20Surround%20View%20Cameras/","text":"Restricted Deformable Convolution based Road Scene Semantic Segmentation Using Surround View Cameras","title":"Restricted Deformable Convolution based Road Scene Semantic Segmentation Using Surround View Cameras"},{"location":"other_categories/undone/Restricted%20Deformable%20Convolution%20basedRoad%20Scene%20Semantic%20SegmentationUsing%20Surround%20View%20Cameras/#restricted-deformable-convolution-based-road-scene-semantic-segmentation-using-surround-view-cameras","text":"","title":"Restricted Deformable Convolution based Road Scene Semantic Segmentation Using Surround View Cameras"},{"location":"other_categories/undone/Semantic%20Image%20Synthesis%20with%20Spatially-Adaptive%20Normalization/","text":"Semantic Image Synthesis with Spatially-Adaptive Normalization","title":"Semantic Image Synthesis with Spatially-Adaptive Normalization"},{"location":"other_categories/undone/Semantic%20Image%20Synthesis%20with%20Spatially-Adaptive%20Normalization/#semantic-image-synthesis-with-spatially-adaptive-normalization","text":"","title":"Semantic Image Synthesis with Spatially-Adaptive Normalization"},{"location":"other_categories/undone/Style%20Augmentation%20Data%20Augmentation%20via%20StyleRandomization/","text":"Style Augmentation: Data Augmentation via StyleRandomization","title":"Style Augmentation: Data Augmentation via StyleRandomization"},{"location":"other_categories/undone/Style%20Augmentation%20Data%20Augmentation%20via%20StyleRandomization/#style-augmentation-data-augmentation-via-stylerandomization","text":"","title":"Style Augmentation: Data Augmentation via StyleRandomization"},{"location":"other_categories/undone/TossingBot%20Learning%20to%20Throw%20Arbitrary%20Objectswith%20Residual%20Physics/","text":"TossingBot: Learning to Throw Arbitrary Objectswith Residual Physics","title":"TossingBot: Learning to Throw Arbitrary Objectswith Residual Physics"},{"location":"other_categories/undone/TossingBot%20Learning%20to%20Throw%20Arbitrary%20Objectswith%20Residual%20Physics/#tossingbot-learning-to-throw-arbitrary-objectswith-residual-physics","text":"","title":"TossingBot: Learning to Throw Arbitrary Objectswith Residual Physics"},{"location":"other_categories/undone/Toward%20Driving%20Scene%20Understanding%20A%20Dataset%20for%20Learning%20Driver%20Behavior%20and%20Causal%20Reasoning/","text":"Toward Driving Scene Understanding:A Dataset for Learning Driver Behavior and Causal Reasoning","title":"Toward Driving Scene Understanding:A Dataset for Learning Driver Behavior and Causal Reasoning"},{"location":"other_categories/undone/Toward%20Driving%20Scene%20Understanding%20A%20Dataset%20for%20Learning%20Driver%20Behavior%20and%20Causal%20Reasoning/#toward-driving-scene-understandinga-dataset-for-learning-driver-behavior-and-causal-reasoning","text":"","title":"Toward Driving Scene Understanding:A Dataset for Learning Driver Behavior and Causal Reasoning"},{"location":"other_categories/undone/Towards%20Practical%20Hierarchical%20ReinforcementLearning%20for%20Multi-lane%20Autonomous%20Driving/","text":"Towards Practical Hierarchical ReinforcementLearning for Multi-lane Autonomous Driving","title":"Towards Practical Hierarchical ReinforcementLearning for Multi-lane Autonomous Driving"},{"location":"other_categories/undone/Towards%20Practical%20Hierarchical%20ReinforcementLearning%20for%20Multi-lane%20Autonomous%20Driving/#towards-practical-hierarchical-reinforcementlearning-for-multi-lane-autonomous-driving","text":"","title":"Towards Practical Hierarchical ReinforcementLearning for Multi-lane Autonomous Driving"},{"location":"other_categories/undone/Unsupervised%20Learning%20of%20Depth%20and%20Ego-Motion%20from%20Monocular%20VideoUsing%203D%20Geometric%20Constraints/","text":"Unsupervised Learning of Depth and Ego-Motion from Monocular VideoUsing 3D Geometric Constraints","title":"Unsupervised Learning of Depth and Ego-Motion from Monocular VideoUsing 3D Geometric Constraints"},{"location":"other_categories/undone/Unsupervised%20Learning%20of%20Depth%20and%20Ego-Motion%20from%20Monocular%20VideoUsing%203D%20Geometric%20Constraints/#unsupervised-learning-of-depth-and-ego-motion-from-monocular-videousing-3d-geometric-constraints","text":"","title":"Unsupervised Learning of Depth and Ego-Motion from Monocular VideoUsing 3D Geometric Constraints"},{"location":"other_categories/undone/Virtual-to-real%20%20Deep%20%20Reinforcement%20%20Learning%20Continuous%20%20Control%20%20of%20%20Mobile%20%20Robots%20%20for%20%20Mapless%20%20Navigation/","text":"Virtual-to-real Deep Reinforcement Learning:Continuous Control of Mobile Robots for Mapless Navigation","title":"Virtual-to-real Deep Reinforcement Learning:Continuous Control of Mobile Robots for Mapless Navigation"},{"location":"other_categories/undone/Virtual-to-real%20%20Deep%20%20Reinforcement%20%20Learning%20Continuous%20%20Control%20%20of%20%20Mobile%20%20Robots%20%20for%20%20Mapless%20%20Navigation/#virtual-to-real-deep-reinforcement-learningcontinuous-control-of-mobile-robots-for-mapless-navigation","text":"","title":"Virtual-to-real Deep Reinforcement Learning:Continuous Control of Mobile Robots for Mapless Navigation"},{"location":"other_categories/undone/Visual-based%20Autonomous%20Driving%20Deployment%20from%20a%20Stochastic%20andUncertainty-aware%20Perspective/","text":"Visual-based Autonomous Driving Deployment from a Stochastic andUncertainty-aware Perspective","title":"Visual-based Autonomous Driving Deployment from a Stochastic andUncertainty-aware Perspective"},{"location":"other_categories/undone/Visual-based%20Autonomous%20Driving%20Deployment%20from%20a%20Stochastic%20andUncertainty-aware%20Perspective/#visual-based-autonomous-driving-deployment-from-a-stochastic-anduncertainty-aware-perspective","text":"","title":"Visual-based Autonomous Driving Deployment from a Stochastic andUncertainty-aware Perspective"},{"location":"other_categories/undone/aaaaa/","text":"\u6536\u96c6\u4e86\u672a\u5b8c\u6210\u7684\u624b\u7a3f\uff0c\u4f46\u662f\u5f88\u591a\u6807\u9898\u5bf9\u5e94\u7684\u8bba\u6587\u90fd\u503c\u5f97\u4e00\u770b","title":"Aaaaa"},{"location":"other_categories/undone/depth_prediction_citation/","text":"Single Image Depth Estimation From Predicted Semantic Labels pdf \u8fd9\u7bc7paper\u7ed9\u51fa\u4e86look ground\u7684intuition. @INPROCEEDINGS{5539823, author={B. {Liu} and S. {Gould} and D. {Koller}}, booktitle={2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition}, title={Single image depth estimation from predicted semantic labels}, year={2010}, volume={}, number={}, pages={1253-1260},}","title":"Single Image Depth Estimation From Predicted Semantic Labels"},{"location":"other_categories/undone/depth_prediction_citation/#single-image-depth-estimation-from-predicted-semantic-labels","text":"pdf \u8fd9\u7bc7paper\u7ed9\u51fa\u4e86look ground\u7684intuition. @INPROCEEDINGS{5539823, author={B. {Liu} and S. {Gould} and D. {Koller}}, booktitle={2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition}, title={Single image depth estimation from predicted semantic labels}, year={2010}, volume={}, number={}, pages={1253-1260},}","title":"Single Image Depth Estimation From Predicted Semantic Labels"}]}