<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-10-24 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DEâ«¶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Birdâs-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">è®ºæéè¯»</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-Î±
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-10-23/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../../depth_completion/DNet/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-10-24">Arxiv Computer Vision Papers - 2025-10-24</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#multimedia-aware-question-answering-a-review-of-retrieval-and-cross-modal-reasoning-architectures" class="nav-link">Multimedia-Aware Question Answering: A Review of Retrieval and Cross-Modal Reasoning Architectures</a>
                </li>
                <li class="nav-item">
                    <a href="#a-survey-on-cache-methods-in-diffusion-models-toward-efficient-multi-modal-generation" class="nav-link">A Survey on Cache Methods in Diffusion Models: Toward Efficient Multi-Modal Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#advances-in-4d-representation-geometry-motion-and-interaction" class="nav-link">Advances in 4D Representation: Geometry, Motion, and Interaction</a>
                </li>
                <li class="nav-item">
                    <a href="#holocine-holistic-generation-of-cinematic-multi-shot-long-video-narratives" class="nav-link">HoloCine: Holistic Generation of Cinematic Multi-Shot Long Video Narratives</a>
                </li>
                <li class="nav-item">
                    <a href="#towards-general-modality-translation-with-contrastive-and-predictive-latent-diffusion-bridge" class="nav-link">Towards General Modality Translation with Contrastive and Predictive Latent Diffusion Bridge</a>
                </li>
                <li class="nav-item">
                    <a href="#gsworld-closed-loop-photo-realistic-simulation-suite-for-robotic-manipulation" class="nav-link">GSWorld: Closed-Loop Photo-Realistic Simulation Suite for Robotic Manipulation</a>
                </li>
                <li class="nav-item">
                    <a href="#small-drafts-big-verdict-information-intensive-visual-reasoning-via-speculation" class="nav-link">Small Drafts, Big Verdict: Information-Intensive Visual Reasoning via Speculation</a>
                </li>
                <li class="nav-item">
                    <a href="#radar-camera-fused-multi-object-tracking-online-calibration-and-common-feature" class="nav-link">Radar-Camera Fused Multi-Object Tracking: Online Calibration and Common Feature</a>
                </li>
                <li class="nav-item">
                    <a href="#cupid-pose-grounded-generative-3d-reconstruction-from-a-single-image" class="nav-link">CUPID: Pose-Grounded Generative 3D Reconstruction from a Single Image</a>
                </li>
                <li class="nav-item">
                    <a href="#alphaflow-understanding-and-improving-meanflow-models" class="nav-link">AlphaFlow: Understanding and Improving MeanFlow Models</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-10-24">Arxiv Computer Vision Papers - 2025-10-24</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>å¥½çï¼è¿æ¯ä¸ä»½éå¯¹2025å¹´10æ23æ¥Arxivè®¡ç®æºè§è§é¢åè®ºæçæ¯æ¥æ¥åæ§è¡æè¦ï¼æ¨å¨å¸®å©å¿ç¢çç ç©¶äººåå¿«éäºè§£ææ°è¿å±ã</p>
<hr />
<p><strong>æ¯æ¥Arxivè®¡ç®æºè§è§æ¥åæ§è¡æè¦ (2025-10-23)</strong></p>
<p><strong>æ¦è¿°ä¸ä¸»è¦è¶å¿ï¼</strong>
ä»å¤©çè®ºæéå±ç¤ºäºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åæç»­å<strong>å¤æ¨¡æçè§£ä¸çæ</strong>ã<strong>é«æä¸éç¨æ¨¡å</strong>ä»¥å<strong>3D/4Dè¡¨ç¤ºä¸éå»º</strong>æ¹ååå±çå¼ºå²è¶å¿ãç¹å«å¼å¾æ³¨æçæ¯ï¼æ©æ£æ¨¡åå¨å¤æ¨¡æçæä¸­çåºç¨åå¶æçä¼åæä¸ºä¸ä¸ªçªåºä¸»é¢ï¼åæ¶ï¼å¯¹å¤æåºæ¯ï¼å¦é¿è§é¢åäºãæºå¨äººæä½ï¼çå»ºæ¨¡åæ¨çè½åä¹å¨ä¸æ­æåã</p>
<p><strong>ç¹å«éè¦æåæ°çè®ºæï¼</strong></p>
<ul>
<li><strong>HoloCine: Holistic Generation of Cinematic Multi-Shot Long Video Narratives (Yihao Meng et al.)</strong>: è¿ç¯è®ºæå¨é¿è§é¢çæé¢ååå¾äºæ¾èçªç ´ï¼è¶è¶äºåéå¤´éå¶ï¼å®ç°äºçµå½±çº§å¤éå¤´åäºçæ´ä½çæãå¶å¯¹å¤ææ¶é´ä¸è´æ§ååäºç»æçå»ºæ¨¡ï¼é¢ç¤ºçè§é¢çæææ¯è¿åæ´é«çº§å«çåºç¨ã</li>
<li><strong>Towards General Modality Translation with Contrastive and Predictive Latent Diffusion Bridge (Nimrod Berman et al.)</strong>: è¯¥å·¥ä½æåºäºä¸ä¸ªéç¨çæ¨¡æç¿»è¯æ¡æ¶ï¼å©ç¨å¯¹æ¯åé¢æµçæ½å¨æ©æ£æ¡¥ï¼ææå®ç°æ´å¹¿æ³ãæ´çµæ´»çè·¨æ¨¡æè½¬æ¢ï¼å·æå·¨å¤§çæ½å¨åºç¨ä»·å¼ã</li>
<li><strong>GSWorld: Closed-Loop Photo-Realistic Simulation Suite for Robotic Manipulation (Guangqi Jiang et al.)</strong>: è¿ç¯è®ºæä¸ºæºå¨äººæä½æä¾äºä¸ä¸ªé­ç¯ãç§ççº§çå®çæ¨¡æå¥ä»¶ï¼å¯¹äºæ¨å¨æºå¨äººå­¦ä¹ åé¨ç½²è³å³éè¦ï¼è§£å³äºç°å®ä¸çæ°æ®è·åçææã</li>
</ul>
<p><strong>æ°å´ç ç©¶æ¹åæææ¯ï¼</strong></p>
<ol>
<li><strong>æ©æ£æ¨¡åçé«æä¸éç¨åï¼</strong> å¤ç¯è®ºæï¼å¦"A Survey on Cache Methods in Diffusion Models"ã"Towards General Modality Translation"ï¼èç¦äºæ©æ£æ¨¡åçæçä¼ååè·¨æ¨¡æéç¨æ§ï¼è¡¨æè¯¥æ¨¡åå®¶æä»æ¯ç ç©¶ç­ç¹ï¼ä¸æ­£åæ´å®ç¨ãæ´å¹¿æ³çåºç¨åå±ã</li>
<li><strong>4Dè¡¨ç¤ºä¸çè§£ï¼</strong> "Advances in 4D Representation"çåºç°ï¼é¢ç¤ºçå¯¹å¨æä¸ç»´åºæ¯ï¼åå«æ¶é´ç»´åº¦ï¼çå»ºæ¨¡åçè§£å°æä¸ºä¸ä¸ªæ¥çéè¦çæ¹åï¼è¿å¯¹äºAR/VRãæºå¨äººåèªå¨é©¾é©¶ç­é¢åè³å³éè¦ã</li>
<li><strong>ä¿¡æ¯å¯éåè§è§æ¨çï¼</strong> "Small Drafts, Big Verdict"å¼ºè°äºå¨ä¿¡æ¯éå¤§çè§è§åºæ¯ä¸­è¿è¡æ¨ççè½åï¼è¿å¯¹äºå¤æé®ç­ãå³ç­æ¯æç­é«çº§AIåºç¨å·ææå¯¼æä¹ã</li>
<li><strong>å¤æ¨¡æèåä¸æ ¡åï¼</strong> "Radar-Camera Fused Multi-Object Tracking"å±ç¤ºäºå¤ä¼ æå¨èåå¨é²æ£æç¥ä¸­çéè¦æ§ï¼ç¹å«æ¯å¯¹å¨çº¿æ ¡ååç¹å¾å¯¹é½çå³æ³¨ã</li>
</ol>
<p><strong>å»ºè®®éè¯»å¨æçè®ºæï¼</strong></p>
<p>å¯¹äºä¸åå´è¶£çç ç©¶äººåï¼å»ºè®®éè¯»ä»¥ä¸è®ºæï¼</p>
<ul>
<li><strong>è§é¢çæä¸åäºï¼</strong> <strong>HoloCine: Holistic Generation of Cinematic Multi-Shot Long Video Narratives</strong> (Yihao Meng et al.) - å¿è¯»ï¼ä»£è¡¨äºè§é¢çæçåæ²¿ã</li>
<li><strong>éç¨æ¨¡æè½¬æ¢ï¼</strong> <strong>Towards General Modality Translation with Contrastive and Predictive Latent Diffusion Bridge</strong> (Nimrod Berman et al.) - å¯¹è·¨æ¨¡æç ç©¶èæå·ä»·å¼ã</li>
<li><strong>æºå¨äººä¸æ¨¡æï¼</strong> <strong>GSWorld: Closed-Loop Photo-Realistic Simulation Suite for Robotic Manipulation</strong> (Guangqi Jiang et al.) - æºå¨äººå­¦ä¹ åä»¿çé¢åçå³é®è¿å±ã</li>
<li><strong>æ©æ£æ¨¡åä¼åï¼</strong> <strong>A Survey on Cache Methods in Diffusion Models: Toward Efficient Multi-Modal Generation</strong> (Jiacheng Liu et al.) - äºè§£æ©æ£æ¨¡åæçä¼åçææ°ç»¼è¿°ã</li>
<li><strong>3D/4Dè¡¨ç¤ºï¼</strong> <strong>Advances in 4D Representation: Geometry, Motion, and Interaction</strong> (Mingrui Zhao et al.) - å³æ³¨æªæ¥ä¸ç»´å¨æåºæ¯å»ºæ¨¡çç ç©¶èã</li>
</ul>
<hr />
<p>è¿ä»½æè¦æ¨å¨æä¾ä¸ä¸ªé«å±æ¬¡çæ¦è§ï¼å¸®å©æ¨å¿«éè¯å«ä¸æ¨ç ç©¶æ¹åæç¸å³çè®ºæãå»ºè®®æ ¹æ®æ¨çå·ä½å´è¶£ï¼è¿ä¸æ­¥æ·±å¥éè¯»ææ¨èçè®ºæã</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2510.20193v1">Multimedia-Aware Question Answering: A Review of Retrieval and Cross-Modal Reasoning Architectures</a></li>
<li><a href="#2510.19755v2">A Survey on Cache Methods in Diffusion Models: Toward Efficient Multi-Modal Generation</a></li>
<li><a href="#2510.19255v1">Advances in 4D Representation: Geometry, Motion, and Interaction</a></li>
<li><a href="#2510.20822v1">HoloCine: Holistic Generation of Cinematic Multi-Shot Long Video Narratives</a></li>
<li><a href="#2510.20819v1">Towards General Modality Translation with Contrastive and Predictive Latent Diffusion Bridge</a></li>
<li><a href="#2510.20813v1">GSWorld: Closed-Loop Photo-Realistic Simulation Suite for Robotic Manipulation</a></li>
<li><a href="#2510.20812v1">Small Drafts, Big Verdict: Information-Intensive Visual Reasoning via Speculation</a></li>
<li><a href="#2510.20794v1">Radar-Camera Fused Multi-Object Tracking: Online Calibration and Common Feature</a></li>
<li><a href="#2510.20776v1">CUPID: Pose-Grounded Generative 3D Reconstruction from a Single Image</a></li>
<li><a href="#2510.20771v1">AlphaFlow: Understanding and Improving MeanFlow Models</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2510.20193v1'></a></p>
<h2 id="multimedia-aware-question-answering-a-review-of-retrieval-and-cross-modal-reasoning-architectures"><a href="https://arxiv.org/abs/2510.20193v1">Multimedia-Aware Question Answering: A Review of Retrieval and Cross-Modal Reasoning Architectures</a></h2>
<p><strong>Authors:</strong> Rahul Raja, Arpita Vats</p>
<p><strong>Published:</strong> 2025-10-23</p>
<p><strong>Categories:</strong> cs.IR, cs.CL, cs.CV, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Question Answering (QA) systems have traditionally relied on structured text
data, but the rapid growth of multimedia content (images, audio, video, and
structured metadata) has introduced new challenges and opportunities for
retrieval-augmented QA. In this survey, we review recent advancements in QA
systems that integrate multimedia retrieval pipelines, focusing on
architectures that align vision, language, and audio modalities with user
queries. We categorize approaches based on retrieval methods, fusion
techniques, and answer generation strategies, and analyze benchmark datasets,
evaluation protocols, and performance tradeoffs. Furthermore, we highlight key
challenges such as cross-modal alignment, latency-accuracy tradeoffs, and
semantic grounding, and outline open problems and future research directions
for building more robust and context-aware QA systems leveraging multimedia
data.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Rahul RajaåArpita Vatsæ°åçè®ºæâMultimedia-Aware Question Answering: A Review of Retrieval and Cross-Modal Reasoning Architecturesâçå¨é¢æè¦ã</p>
<hr />
<p><strong>è®ºææè¦ï¼å¤åªä½æç¥é®ç­ï¼æ£ç´¢ä¸è·¨æ¨¡ææ¨çæ¶æç»¼è¿°</strong></p>
<p><strong>1. è§£å³çä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³ä¼ ç»é®ç­ï¼QAï¼ç³»ç»ä¸»è¦ä¾èµç»æåææ¬æ°æ®çå±éæ§ï¼å¹¶æ¢è®¨å¨å¤åªä½åå®¹ï¼å¾åãé³é¢ãè§é¢åç»æååæ°æ®ï¼å¿«éå¢é¿çèæ¯ä¸ï¼å¦ä½æå»ºæ´å¼ºå¤§ãæ´å·ä¸ä¸ææç¥è½åçé®ç­ç³»ç»ãæ ¸å¿é®é¢æ¯å¦ä½å°å¤åªä½æ£ç´¢ç®¡ééæå°QAç³»ç»ä¸­ï¼ä»¥å®ç°è§è§ãè¯­è¨åé³é¢æ¨¡æä¸ç¨æ·æ¥è¯¢çå¯¹é½ï¼ä»èçæåç¡®ãææ ¹æ®çç­æ¡ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
è¯¥è®ºæéè¿å¯¹ç°æç ç©¶çå¨é¢ç»¼è¿°ï¼ç³»ç»å°åç±»ååæäºå¤åªä½QAç³»ç»çå³é®åæ°åæ¹æ³è®ºè´¡ç®ï¼</p>
<ul>
<li><strong>åå±åç±»æ³ï¼</strong> è®ºææåºäºä¸ä¸ªå¤åªä½QAç³»ç»çåå±åç±»æ³ï¼æ ¹æ®è¾å¥æ¨¡æï¼å¦åæ¨¡æè¯­è¨QAãéæè§è§-è¯­è¨QAãæ¶ç©ºè§è§-è¯­è¨QAãå£°å­¦-è¯­è¨QAï¼ãä»»å¡å¶å®ï¼å¦æ¨¡ææç¥å®ä½QAãå ææ¨çQAãä¸ä¸æäº¤äºQAãæ¶é´äºä»¶QAãè·¨æ¨¡ææ¨çQAï¼åæ£ç´¢ç­ç¥ï¼å¦å¯éæ£ç´¢ãå¤æ¨¡æåµå¥æ£ç´¢ãè·¨æ¨¡ææ£ç´¢ãæ¶é´è§é¢çæ®µæ£ç´¢ãè§å¬æ£ç´¢ï¼å¯¹ç°ææ¹æ³è¿è¡å½ç±»ã</li>
<li><strong>æ¨¡æç¹å®QAç³»ç»ï¼</strong> è¯¦ç»ä»ç»äºéå¯¹ä¸åæ¨¡æï¼ææ¬ãå¾åãè§é¢ãé³é¢ï¼çQAç³»ç»åå±ï¼åæ¬ä»æ©æåºäºCNNåRNNçæ¨¡åå°åºäºTransformerçè§è§-è¯­è¨é¢è®­ç»æ¨¡åï¼å¦LXMERTãUNITERãFlamingoãBLIP-2ï¼çæ¼åã</li>
<li><strong>ä»»å¡å¯¼åQAç³»ç»ï¼</strong> æ¢è®¨äºä¸åæ¨çæ·±åº¦çQAä»»å¡ï¼å¦äºå®åQAãå ææ¨çQAãå¯¹è¯å¼QAåæ¶é´äºä»¶QAï¼ä»¥åå®ä»¬å¦ä½å©ç¨å¾ç¥ç»ç½ç»ãè®°å¿å¢å¼ºåTransformerç­ææ¯è¿è¡è¯æ®åæåå¤è·³æ¨çã</li>
<li><strong>å¤æ¨¡ææ£ç´¢ç­ç¥ï¼</strong> æ·±å¥åæäºäºç§å³é®æ£ç´¢èå¼ï¼<ul>
<li><strong>å¯éæ£ç´¢ï¼</strong> å¼ºè°äºDPRãColBERTv2ãAtlasãInParsåBGEæ¨¡åå¨è¯­ä¹ç©ºé´åµå¥åé«ææ£ç´¢æ¹é¢çè¿å±ã</li>
<li><strong>åµå¥æ£ç´¢ï¼</strong> è®¨è®ºäºCLIPãBLIPåImageBindç­æ¨¡åå¦ä½éè¿å¯¹æ¯å­¦ä¹ å°ä¸åæ¨¡æåµå¥å±äº«æ½å¨ç©ºé´ï¼å®ç°è·¨æ¨¡ææ£ç´¢ã</li>
<li><strong>è·¨æ¨¡ææ£ç´¢ï¼</strong> å³æ³¨VATTåMMTç­æ¨¡åå¦ä½å©ç¨èªçç£è®­ç»åèåå±å®ç°æ¨¡æé´çå¯¹é½åå¤å«æ§è¡¨ç¤ºã</li>
<li><strong>æ¶é´è§é¢çæ®µæ£ç´¢ï¼</strong> ä»ç»äºHEROåClipBERTç­æ¨¡åå¦ä½éè¿åå±Transformeråç¨çæ¶é´éæ ·å®ç°è§é¢çæ®µçæ¶é´å®ä½ã</li>
<li><strong>è§å¬æ£ç´¢ï¼</strong> æ¢è®¨äºAVTSåAVIDç­æ¨¡åå¦ä½éè¿èªçç£å­¦ä¹ åè·¨æ¨¡ææ³¨æåæºå¶å®ç°é³é¢åè§è§ä¿¡å·çèåè¡¨ç¤ºã</li>
</ul>
</li>
<li><strong>å¤æ¨¡æQAæ¶æï¼</strong> æ»ç»äºåç§ä¸»å¯¼è®¾è®¡èå¼ï¼âåæ£ç´¢åéè¯»âï¼Retrieve then Readï¼ãâç«¯å°ç«¯èåâï¼End-to-End Fusionï¼ãâLLM + å¤æ¨¡ææ£ç´¢âï¼LLM + Multimodal Retrieverï¼åâç¥è¯å¾è°±å¤æ¨¡æQAâï¼Knowledge-Grounded Multimodal QAï¼ï¼å¹¶åæäºå®ä»¬çä¼ç¼ºç¹ã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
è¯¥è®ºæéè¿å¯¹å¤§éç°æå·¥ä½çåé¡¾ï¼æ­ç¤ºäºå¤åªä½QAç³»ç»å¨ä»¥ä¸æ¹é¢åå¾äºæ¾èè¿å±ï¼</p>
<ul>
<li><strong>è¯­ä¹çè§£è½åæåï¼</strong> åå©å¤§åé¢è®­ç»è§è§-è¯­è¨æ¨¡åï¼ç³»ç»è½å¤æ´å¥½å°çè§£è·¨æ¨¡æåå®¹ä¸­çè¯­ä¹å³ç³»ã</li>
<li><strong>è·¨æ¨¡æå¯¹é½åèåï¼</strong> åç§æ£ç´¢åèåææ¯ï¼å¦å¯¹æ¯å­¦ä¹ ãè·¨æ¨¡ææ³¨æåï¼ä½¿å¾ä¸åæ¨¡æçä¿¡æ¯è½å¤ææå¯¹é½åæ´åã</li>
<li><strong>å¤ææ¨çè½åï¼</strong> éå¯¹å æãä¸ä¸æåæ¶é´æ¨çç­å¤æä»»å¡ï¼ç³»ç»è½å¤çææ´å·è§£éæ§åè¿è´¯æ§çç­æ¡ã</li>
<li><strong>å¯æ©å±æ§ï¼</strong> å¯éæ£ç´¢åæ£ç´¢å¢å¼ºçæï¼RAGï¼æ¶æçè¿æ­¥ä½¿å¾QAç³»ç»è½å¤å¤çå¤§è§æ¨¡å¤åªä½è¯­æåºã</li>
</ul>
<p>è¿äºè¿å±å¯¹äºæå»ºè½å¤å¤çç°å®ä¸çå¤æå¤åªä½æ°æ®çæºè½ç³»ç»å·æéè¦æä¹ï¼å°¤å¶æ¯å¨è§è§é®ç­ãè§é¢é®ç­ãæå­¦é®ç­åå¤åªä½åå®¹æ£ç´¢å¢å¼ºçæç­åºç¨ä¸­ã</p>
<p><strong>4. è®ºæä¸­æå°çå±éæ§ï¼</strong>
å°½ç®¡åå¾äºæ¾èè¿å±ï¼è®ºæä¹æåºäºå½åå¤åªä½QAç³»ç»å­å¨çå ä¸ªå³é®å±éæ§ï¼</p>
<ul>
<li><strong>ç»ç²åº¦è·¨æ¨¡æå¯¹é½ï¼</strong> é¾ä»¥å®ç°è¯­é³ä¸è§è§åºæ¯ç­ç»ç²åº¦æ¨¡æé´çç²¾ç¡®åæ­¥ã</li>
<li><strong>é²æ£æ§ä¸å¯ä¿¡åº¦æºå¶ï¼</strong> ç¼ºä¹å¯é çæ¨¡æå½å æçæ®µçº§å¼ç¨ç­æºå¶ï¼å½±åäºç³»ç»çå¯ä¿¡èµæ§ã</li>
<li><strong>è®¡ç®å¼éï¼</strong> å®æ¶æå¤§è§æ¨¡æ£ç´¢å¼å¥çè®¡ç®å¼éä»ç¶æ¯ä¸ä¸ªææã</li>
<li><strong>å¤è¯­è¨æ¥è¯¢åä½èµæºæ¨¡æï¼</strong> å¤çå¤è¯­è¨æ¥è¯¢åæ¯æä½èµæºæ¨¡æçå¤ææ§ã</li>
<li><strong>ç­æ¡è´¨éè¯ä¼°ï¼</strong> è·¨æ¨¡æè¯ä¼°ç­æ¡è´¨éä»ç¶æ¯ä¸ä¸ªæç»­çææã</li>
</ul>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
ä¸ºäºåæä¸è¿°å±éæ§ï¼è®ºææåºäºä»¥ä¸æªæ¥ç ç©¶æ¹åï¼</p>
<ul>
<li><strong>å¼åéæçRAGç³»ç»ï¼</strong> æå»ºè½å¤æä¾éæè§£éåè¯æ®çå¤æ¨¡ææ£ç´¢å¢å¼ºçæï¼RAGï¼ç³»ç»ã</li>
<li><strong>ç»ä¸åµå¥ç©ºé´ï¼</strong> æ¨å¨ç»ä¸åµå¥ç©ºé´çç ç©¶ï¼ä»¥å®ç°é«æåå¯æ©å±çè·¨æ¨¡ææ£ç´¢ã</li>
<li><strong>è½»éçº§æ¶æï¼</strong> ä¼åå¼åè½»éçº§æ¶æï¼ä»¥éåºèµæºåéç¯å¢ä¸­çé¨ç½²éæ±ã</li>
<li><strong>å¯æç¤ºæ£ç´¢å¨ï¼</strong> ç ç©¶è½å¤å¨æéåºä¸æ­æ¼åçå¤åªä½åå®¹çå¯æç¤ºæ£ç´¢å¨ã</li>
<li><strong>å®æ¶QAç®¡éï¼</strong> å¼åè½å¤çè§£ä¼è®®ãçæ§å½ååä»¥èªæä¸ºä¸­å¿è§é¢ç­ç´æ­æ°æ®çå®æ¶QAç®¡éã</li>
<li><strong>æ åååºååè¯ä¼°åè®®ï¼</strong> ç¤¾åºéè¦æèµäºæ åååºåãå¼æºå·¥å·ååå±äº«è¯ä¼°åè®®ï¼ä»¥ä¿è¿è¿å±ã</li>
<li><strong>å¯è§£éãå¯ä¿¡èµåååºå¼ç³»ç»ï¼</strong> è´åäºæå»ºä¸ä»åç¡®ï¼èä¸å¯è§£éãå¯ä¿¡èµå¹¶è½ååºç°å®ä¸çå¤åªä½è®¾ç½®çQAç³»ç»ã</li>
</ul>
<hr />
<p>è¿ä»½æè¦æ¨å¨å¨é¢æ¶µçè®ºæçæ ¸å¿åå®¹ï¼çªåºå¶å¯¹è®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçéè¦æ§ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Question Answering (QA) systems have traditionally relied on structured text
data, but the rapid growth of multimedia content (images, audio, video, and
structured metadata) has introduced new challenges and opportunities for
retrieval-augmented QA.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.20193v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.20193v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.19755v2'></a></p>
<h2 id="a-survey-on-cache-methods-in-diffusion-models-toward-efficient-multi-modal-generation"><a href="https://arxiv.org/abs/2510.19755v2">A Survey on Cache Methods in Diffusion Models: Toward Efficient Multi-Modal Generation</a></h2>
<p><strong>Authors:</strong> Jiacheng Liu, Xinyu Wang, Yuqi Lin, Zhikai Wang, Peiru Wang, Peiliang Cai, Qinming Zhou, Zhengan Yan, Zexuan Yan, Zhengyi Shi, Chang Zou, Yue Ma, Linfeng Zhang</p>
<p><strong>Published:</strong> 2025-10-22</p>
<p><strong>Categories:</strong> cs.LG, cs.AI, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Diffusion Models have become a cornerstone of modern generative AI for their
exceptional generation quality and controllability. However, their inherent
\textit{multi-step iterations} and \textit{complex backbone networks} lead to
prohibitive computational overhead and generation latency, forming a major
bottleneck for real-time applications. Although existing acceleration
techniques have made progress, they still face challenges such as limited
applicability, high training costs, or quality degradation.
  Against this backdrop, \textbf{Diffusion Caching} offers a promising
training-free, architecture-agnostic, and efficient inference paradigm. Its
core mechanism identifies and reuses intrinsic computational redundancies in
the diffusion process. By enabling feature-level cross-step reuse and
inter-layer scheduling, it reduces computation without modifying model
parameters. This paper systematically reviews the theoretical foundations and
evolution of Diffusion Caching and proposes a unified framework for its
classification and analysis.
  Through comparative analysis of representative methods, we show that
Diffusion Caching evolves from \textit{static reuse} to \textit{dynamic
prediction}. This trend enhances caching flexibility across diverse tasks and
enables integration with other acceleration techniques such as sampling
optimization and model distillation, paving the way for a unified, efficient
inference framework for future multimodal and interactive applications. We
argue that this paradigm will become a key enabler of real-time and efficient
generative AI, injecting new vitality into both theory and practice of
\textit{Efficient Generative Intelligence}.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Jiacheng Liuç­äººæ°åçè®ºæâA Survey on Cache Methods in Diffusion Models: Toward Efficient Multi-Modal Generationâçå¨é¢æè¦ã</p>
<hr />
<h3 id="_1">è®ºææè¦ï¼æ©æ£æ¨¡åä¸­çç¼å­æ¹æ³ï¼è¿åé«æå¤æ¨¡æçæ</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³æ©æ£æ¨¡åï¼DMsï¼å¨çæå¼AIé¢ååå¾çªç ´æ§è¿å±çåæ¶ï¼æé¢ä¸´çä¸¥å³»è®¡ç®æçææãå°½ç®¡æ©æ£æ¨¡åå¨çæè´¨éåå¯æ§æ§æ¹é¢è¡¨ç°åºè²ï¼ä½å¶åºæçâå¤æ­¥è¿­ä»£âåâå¤æéª¨å¹²ç½ç»âæ¨çèå¼å¯¼è´äºå·¨å¤§çè®¡ç®å¼éåçæå»¶è¿ãè¿ä¸¥éé»ç¢äºæ©æ£æ¨¡åå¨å®æ¶äº¤äºåºç¨ä¸­çé¨ç½²ãç°æçå éææ¯ï¼å¦éæ ·ä¼åãæ¨¡åè¸é¦ãåªæãå¹¶è¡åç­ï¼è½ç¶åå¾äºä¸äºè¿å±ï¼ä½ä»é¢ä¸´éç¨æ§æéãè®­ç»ææ¬é«ææçæè´¨éä¸éç­ææãå æ­¤ï¼è®ºæçæ ¸å¿é®é¢æ¯ï¼å¦ä½å¨ä¸çºç²çæè´¨éçåæä¸ï¼é«æå°å éæ©æ£æ¨¡åçæ¨çè¿ç¨ï¼ä»¥å®ç°å®æ¶æ§è½åå¹¿æ³åºç¨ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
è¯¥è®ºæçæ ¸å¿åæ°å¨äºç³»ç»æ§å°æåºäºâæ©æ£ç¼å­âï¼Diffusion Cachingï¼è¿ä¸è®­ç»æ å³ãæ¶ææ å³ä¸é«æçæ¨çèå¼ãå¶æ ¸å¿æºå¶å¨äºè¯å«åéç¨æ©æ£æ¨çè¿ç¨ä¸­åºæçè®¡ç®åä½ãå·ä½è´¡ç®åæ¬ï¼</p>
<ul>
<li><strong>ç»ä¸ççè®ºæ¡æ¶ååç±»ï¼</strong> è®ºæé¦æ¬¡ç³»ç»æ§å°æ»ç»äºæ©æ£ç¼å­ççè®ºåºç¡åææ¯æ¼è¿ï¼å¹¶æåºäºä¸ä¸ªç»ä¸çåç±»ååææ¡æ¶ãè¯¥æ¡æ¶ä»âè§¦åæ¡ä»¶âãâéç¨ç²åº¦âåâæ´æ°ç­ç¥âä¸ä¸ªç»´åº¦å¯¹ç°ææ¹æ³è¿è¡åç±»ï¼æ­ç¤ºäºä¸åæ¹æ³ä¹é´çåå¨é»è¾åææ¯æ¼è¿ã</li>
<li><strong>æ¼è¿è·¯å¾çæ­ç¤ºï¼</strong> è®ºæéè¿å¯¹ä»£è¡¨æ§æ¹æ³çæ¯è¾åæï¼æåºæ©æ£ç¼å­ææ¯åç°åºä»âéæéç¨âåâå¨æé¢æµâçæ¸æ°æ¼è¿è½¨è¿¹ã<ul>
<li><strong>éæç¼å­ï¼Static Cachingï¼ï¼</strong> éç¨åºå®éç¨ç­ç¥ï¼å¨é¢å®ä¹å±ææ¶é´æ­¥è¿è¡ç¼å­ï¼éç¨äºU-NetåDiTç­ä¸åæ¶æï¼å¦DeepCacheãFasterDiffusionãFORAç­ã</li>
<li><strong>å¨æç¼å­ï¼Dynamic Cachingï¼ï¼</strong> å¼å¥éè¯¯æ£æ¥æºå¶ï¼æ ¹æ®ç¹å¾å¨æè°æ´ç¼å­æ¿æ´»åå·æ°æ¶æºãè¿ä¸æ­¥ç»åä¸ºï¼<ul>
<li><strong>æ¶é´æ­¥èªéåºç¼å­ï¼Timestep-Adaptive Cachingï¼ï¼</strong> æ ¹æ®ç¹å¾å¨ä¸åæ©æ£é¶æ®µçç¨³å®æ§å¨æè°æ´ç¼å­ç­ç¥ï¼å¦TeaCacheãLazyDiTãMagCacheãEasyCacheç­ã</li>
<li><strong>å±èªéåºç¼å­ï¼Layer-Adaptive Cachingï¼ï¼</strong> æ ¹æ®ç½ç»å±ä¹é´çç»æå¼ææ§ï¼è°æ´æ¯å±çç¼å­åæ´æ°é¢çï¼å¦Block CachingãAdaCacheãDBCacheãForesightç­ã</li>
<li><strong>é¢æµç¼å­ï¼Predictive Cachingï¼ï¼</strong> å°ç¼å­è§ä¸ºæ°å¼é¢æµé®é¢ï¼å©ç¨æ³°åçº§æ°å±å¼æé«é¶æ°å¼æ±è§£å¨é¢æµæªæ¥ç¹å¾ç¶æï¼å®ç°âCache-Then-Forecastâèå¼ï¼å¦TaylorSeerãAB-CacheãHiCacheãFoCaç­ã</li>
<li><strong>æ··åç¼å­ï¼Hybrid Cachingï¼ï¼</strong> ç»åæ¶é´æ­¥ãç½ç»å±çº§åç¹å¾å¨æç­å¤ä¸ªç»´åº¦ï¼å®ç°æ´çµæ´»åé²æ£çç¼å­éç¨ï¼å¦ClusCaãSpeCaãOmniCacheãHyCaãProfilingDiTç­ã</li>
</ul>
</li>
</ul>
</li>
<li><strong>å¹¿æ³çåºç¨åºæ¯ï¼</strong> è®ºæè¯¦ç»éè¿°äºæ©æ£ç¼å­ææ¯å¨å¾ååè§é¢ç¼è¾ã3Dçæãé³é¢çæãè¶åè¾¨çãä¸çæ¨¡åãç¦»æ£æ©æ£æ¨¡ååAI for Scienceç­å¤ä¸ªå¤æ¨¡æçæä»»å¡ä¸­çåºç¨ï¼å±ç¤ºäºå¶å¼ºå¤§çéåºæ§åéç¨æ§ã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
è®ºæå¼ºè°æ©æ£ç¼å­ä½ä¸ºä¸ç§è®­ç»æ å³ãæ¶ææ å³çæ¨çèå¼ï¼éè¿è¯å«åéç¨è®¡ç®åä½ï¼æ¾èéä½äºè®¡ç®å¼éï¼åæ¶ä¿æäºçæè´¨éãç¹å«æ¯âCache-Then-Forecastâæ¹æ³çå´èµ·ï¼ä½¿å¾æ©æ£ç¼å­è½å¤å¨ä¸»æµæ¨¡åä¸å®ç°æ æå éï¼å¹¶è¾¾å°é«å éæ¯ãè¿æå³çæ©æ£ç¼å­ä¸ä»è½ç¬ç«å éï¼è¿è½ä¸å¶ä»å éææ¯ï¼å¦éæ ·ä¼ååæ¨¡åè¸é¦ï¼ååå·¥ä½ï¼å±åæå»ºç»ä¸ãé«æçæ¨çæ¡æ¶ãè¿å¯¹äºæ¨å¨çæå¼AIåå®æ¶æ§è½åå¹¿æ³åºç¨åå±å·æéè¦æä¹ï¼ä¸ºâé«æçææºè½âççè®ºæå»ºåå®è·µå®ç°æ³¨å¥äºæ°çæ´»åã</p>
<p><strong>4. è®ºæä¸­æå°çå±éæ§ï¼</strong>
å°½ç®¡æ©æ£ç¼å­åæ¯å¹¿éï¼è®ºæä¹å¦è¯å°æåºäºå½åæ¹æ³çå±éæ§ï¼
*   <strong>åå­æ¶èææï¼</strong> ç¼å­ä¸­é´æ¿æ´»éè¦å¤§éGPUåå­ï¼å°¤å¶æ¯å¨é«åè¾¨çå¾å/é¿åºåè§é¢çæåå¤ä»»å¡å¹¶åæ¨çåºæ¯ä¸ï¼å¯è½å¯¼è´åå­æº¢åºï¼OOMï¼ã
*   <strong>çæè´¨éä¸éï¼</strong> ç¼å­å¼å¥çè¿ä¼¼è¯¯å·®å¯è½å¯¼è´çæè´¨éä¸éï¼è¡¨ç°ä¸ºçº¹çæ¨¡ç³ãè¾¹ç¼å¤±çåå¾®ç»æä¸¢å¤±ï¼å¨é«å éæ¯ä¸å°¤ä¸ºææ¾ï¼éå¶äºå¶å¨é«ç²¾åº¦ä»»å¡ä¸­çåºç¨ã
*   <strong>çè®ºåºç¡ä¸è¶³ï¼</strong> ç°æç¼å­æ¹æ³å¤§å¤æ¯å·¥ç¨é©±å¨çæ¢ç´¢ï¼ç¼ºä¹ä¸¥æ ¼çæ°å­¦çè®ºæ¯æï¼å¯¹ç¼å­å¼èµ·çè¯¯å·®ä¼ æ­åä¸å¶ä»éæ ·ç­ç¥çå¼å®¹æ§çè§£ä¸è¶³ã
*   <strong>ä¸å¶ä»å éç­ç¥çéæï¼</strong> å°½ç®¡ç¼å­å·æçµæ´»æ§åæ­£äº¤æ§ï¼ä½å¦ä½ææå°å°å¤ç§å éæºå¶ï¼å¦ç¼å­ãéåãåªæï¼ç»åï¼ä»¥å¹³è¡¡æ§è½åæçï¼ä»æ¯ä¸ä¸ªææã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
åºäºä¸è¿°å±éæ§ï¼è®ºææåºäºä»¥ä¸æªæ¥ç ç©¶æ¹åï¼
*   <strong>åå­æçä¼åï¼</strong> å¼åæ´é«æçç¼å­ç­ç¥ï¼ä¾å¦é¢çæç¥ç¼å­ï¼FreqCaï¼éè¿ç´¯è®¡æ®å·®ç¹å¾ç¼å­ï¼å°åå­ä½¿ç¨éå¤§å¹éä½ãæªæ¥å¯æ¢ç´¢æ´ç²¾ç»çåå­ç®¡çååç¼©ææ¯ã
*   <strong>è¯¯å·®åæä¸è´¨éä¿è¯ï¼</strong> å»ºç«æ´ä¸¥æ ¼ãç»ä¸çç¼å­è¯±å¯¼è¯¯å·®åæçè®ºæ¡æ¶ï¼éåå¶å¯¹æ©æ£å¨æçå½±åï¼å¹¶æ¢ç´¢ç¼å­ç­ç¥ä¸ä¸åéæ ·æ¹æ³ï¼å¦DDIMãFlow Matchingï¼çå¼å®¹æ§ã
*   <strong>å¤ç»´åº¦éæä¸ååä¼åï¼</strong> è®¾è®¡ååæ§çéææ¡æ¶ï¼å°ç¼å­ä¸å¶ä»å éææ¯ï¼å¦æ¨¡åè¸é¦ãåªæãéåï¼æ·±åº¦èåï¼ä»¥ç¼è§£è¯¯å·®ç´¯ç§¯å¹¶å®ç°æ§è½ä¸è´¨éçå¹³è¡¡ã
*   <strong>å¨æèªéåºä¸é¢æµè½åï¼</strong> è¿ä¸æ­¥æåç¼å­æºå¶çå¨æèªéåºåé¢æµè½åï¼ä½¿å¶è½æ ¹æ®åå®¹å¤ææ§ãæ¨¡åç¶æåä»»å¡éæ±ï¼å®æ¶è°æ´ç¼å­ç­ç¥ã
*   <strong>è·¨å¹³å°é¨ç½²ä¸ç¡¬ä»¶å éï¼</strong> æ¢ç´¢ç¼å­æºå¶å¨ç§»å¨è®¾å¤ãè¾¹ç¼è®¡ç®ç­èµæºåéå¹³å°ä¸çé¨ç½²ï¼å¹¶ä¸ç¡¬ä»¶å éææ¯ï¼å¦TensorRTãFlashAttentionï¼ç»åï¼å®ç°æ´å¹¿æ³çåºç¨ã</p>
<hr />
<p>è¿ä»½æè¦æ¨å¨å¨é¢æ¶µçè®ºæçæ ¸å¿åå®¹ï¼çªåºå¶å¨æ©æ£æ¨¡åå éé¢åçè´¡ç®ãææåæªæ¥å±æã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Through comparative analysis of representative methods, we show that
Diffusion Caching evolves from \textit{static reuse} to \textit{dynamic
prediction}.</li>
<li>We
argue that this paradigm will become a key enabler of real-time and efficient
generative AI, injecting new vitality into both theory and practice of
\textit{Efficient Generative Intelligence}.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.19755v2">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.19755v2">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.19255v1'></a></p>
<h2 id="advances-in-4d-representation-geometry-motion-and-interaction"><a href="https://arxiv.org/abs/2510.19255v1">Advances in 4D Representation: Geometry, Motion, and Interaction</a></h2>
<p><strong>Authors:</strong> Mingrui Zhao, Sauradip Nag, Kai Wang, Aditya Vora, Guangda Ji, Peter Chun, Ali Mahdavi-Amiri, Hao Zhang</p>
<p><strong>Published:</strong> 2025-10-22</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We present a survey on 4D generation and reconstruction, a fast-evolving
subfield of computer graphics whose developments have been propelled by recent
advances in neural fields, geometric and motion deep learning, as well 3D
generative artificial intelligence (GenAI). While our survey is not the first
of its kind, we build our coverage of the domain from a unique and distinctive
perspective of 4D representations\/}, to model 3D geometry evolving over time
while exhibiting motion and interaction. Specifically, instead of offering an
exhaustive enumeration of many works, we take a more selective approach by
focusing on representative works to highlight both the desirable properties and
ensuing challenges of each representation under different computation,
application, and data scenarios. The main take-away message we aim to convey to
the readers is on how to select and then customize the appropriate 4D
representations for their tasks. Organizationally, we separate the 4D
representations based on three key pillars: geometry, motion, and interaction.
Our discourse will not only encompass the most popular representations of
today, such as neural radiance fields (NeRFs) and 3D Gaussian Splatting (3DGS),
but also bring attention to relatively under-explored representations in the 4D
context, such as structured models and long-range motions. Throughout our
survey, we will reprise the role of large language models (LLMs) and video
foundational models (VFMs) in a variety of 4D applications, while steering our
discussion towards their current limitations and how they can be addressed. We
also provide a dedicated coverage on what 4D datasets are currently available,
as well as what is lacking, in driving the subfield forward. Project
page:https://mingrui-zhao.github.io/4DRep-GMI/</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼è¿æ¯ä¸ç¯å³äºMingrui Zhaoç­äººæ°åçè®ºæâAdvances in 4D Representation: Geometry, Motion, and Interactionâçæè¦ã</p>
<p><strong>è®ºææè¦ï¼4Dè¡¨ç¤ºçè¿å±ï¼å ä½ãè¿å¨åäº¤äº</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨å¨é¢åé¡¾ååæ4Dæ°æ®ï¼å³éæ¶é´æ¼åå¹¶å±ç°è¿å¨åäº¤äºç3Då ä½ï¼ççæåéå»ºé¢åãæ ¸å¿é®é¢æ¯å¦ä½éæ©åå®å¶åéç4Dè¡¨ç¤ºï¼ä»¥åºå¯¹ä¸åè®¡ç®ãåºç¨åæ°æ®åºæ¯ä¸çææï¼åæ¶è§£å³ç°ææ¹æ³å¨å ä½ãè¿å¨åäº¤äºå»ºæ¨¡æ¹é¢çå±éæ§ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
*   <strong>ç¬ç¹ç4Dè¡¨ç¤ºè§è§ï¼</strong> è®ºææ²¡æç®åå°æä¸¾ç°æå·¥ä½ï¼èæ¯ä»4Dè¡¨ç¤ºçç¬ç¹è§è§åºåï¼å°å¶åä¸ºä¸ä¸ªå³é®æ¯æ±ï¼å ä½ãè¿å¨åäº¤äºãè¿ç§åç±»æå©äºè¯»èçè§£å¦ä½æ ¹æ®ä»»å¡éæ©åå®å¶åéç4Dè¡¨ç¤ºã
*   <strong>å ä½è¡¨ç¤ºçåç±»ä¸åæï¼</strong> åºåäºéç»æåè¡¨ç¤ºï¼å¦NeRFså3DGSï¼åç»æåè¡¨ç¤ºï¼å¦æ¨¡æ¿ãåºäºé¨ä»¶çæ¨¡ååå¾ï¼ï¼å¹¶è¯¦ç»è®¨è®ºäºå®ä»¬åèªçä¼ç¼ºç¹ãéç¨åºæ¯åé¢ä¸´çææã
*   <strong>è¿å¨å»ºæ¨¡çå¨é¢è¦çï¼</strong> æ·±å¥æ¢è®¨äºé°æ¥è¿å¨ãåºäºåå½¢çè¿å¨ãåºäºè·è¸ªçè¿å¨åæ··åè¿å¨ç­ä¸»è¦è¿å¨ç±»å«ï¼åæäºä¸åè¿å¨ç±»åå¦ä½ä¸è¡¨ç¤ºéæ©ç¸äºä½ç¨ï¼ä»¥åå¦ä½ç¡®ä¿æ¶é´ä¸è´æ§ã
*   <strong>äº¤äºå»ºæ¨¡çå³æ³¨ï¼</strong> ä¸é¨è®¨è®ºäºäº¤äºè¡¨ç¤ºï¼åæ¬å¨ä½ãå¯ä¾æ§ãå§¿æãæ¥è§¦åç©çç­å³é®æ¹é¢ï¼å¼ºè°äºç©çåéªå¨ç¡®ä¿äº¤äºçå®æ§ä¸­çéè¦æ§ã
*   <strong>å¤§åè¯­è¨æ¨¡åï¼LLMsï¼åè§é¢åºç¡æ¨¡åï¼VFMsï¼çä½ç¨ï¼</strong> å¨æ´ä¸ªç»¼è¿°ä¸­ï¼è®ºææ¢è®¨äºLLMsåVFMså¨åç§4Dåºç¨ä¸­çä½ç¨ï¼å¹¶è®¨è®ºäºå®ä»¬å½åçå±éæ§ä»¥åå¦ä½è§£å³è¿äºé®é¢ã
*   <strong>æ°æ®éååºåçä¸é¨è¦çï¼</strong> æä¾äºå¯¹ç°æ4Dæ°æ®éçè¯¦ç»åæï¼æåºäºå½åæ°æ®éçå¯ç¨æ§ãä¸è¶³ä¹å¤ä»¥åæ¨å¨è¯¥å­é¢ååå±æéçæ¹é¢ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>è¡¨ç¤ºéæ©çæå¯¼ï¼</strong> è®ºæçæ ¸å¿ä¿¡æ¯æ¯ä¸ºè¯»èæä¾ä¸ä¸ªæ¡æ¶ï¼ä»¥çè§£å¦ä½æ ¹æ®ç¹å®ä»»å¡çéæ±ï¼å¦è®¡ç®æçãä¿çåº¦ãæ³åè½åï¼æ¥éæ©åå®å¶æåéç4Dè¡¨ç¤ºã
*   <strong>æ­ç¤ºäºä¸åè¡¨ç¤ºçæè¡¡ï¼</strong> éè¿å¯¹å ä½ãè¿å¨åäº¤äºçæ·±å¥åæï¼è®ºææ­ç¤ºäºä¸åè¡¨ç¤ºå¨æ°æ®åå¤ãæ¹æ³è®¾è®¡ãè®¡ç®éæ±åå¯å®ç°ç»ææ¹é¢çåºææè¡¡ã
*   <strong>å¼ºè°äºç»æåæ¨¡åçéè¦æ§ï¼</strong> é¤äºæµè¡çNeRFså3DGSç­éç»æåè¡¨ç¤ºå¤ï¼è®ºæè¿ç¹å«å³æ³¨äºå¨4Dèæ¯ä¸ç¸å¯¹æªååæ¢ç´¢çç»æåæ¨¡ååé¿ç¨è¿å¨ï¼è¿å¯¹äºå¯æ§åå¯è§£éç4Då»ºæ¨¡è³å³éè¦ã
*   <strong>æ¨å¨æªæ¥ç ç©¶æ¹åï¼</strong> è®ºæéè¿è¯å«å½åæ¹æ³çå±éæ§ï¼ä¸ºæªæ¥çç ç©¶ææäºæ¹åï¼ä¾å¦å¼åç»ä¸ãèªéåºåç»ææç¥çè¡¨ç¤ºï¼ä»¥æ ç¼å¤çä¸åè¿å¨ç±»åãç©ºé´å°ºåº¦åææååã</p>
<p><strong>4. è®ºæä¸­æå°çå±éæ§ï¼</strong>
*   <strong>è®¡ç®ææ¬é«æï¼</strong> è®¸å¤åºäºä¼åç4Dæ¹æ³è®¡ç®ææ¬é«æï¼æ¶æç¼æ¢ï¼ä¸éç¨äºå¤§è§æ¨¡åºç¨æå®æ¶äº¤äºã
*   <strong>æ°æ®ç¨ç¼ºæ§ï¼</strong> 4Dè¡¨ç¤ºå­¦ä¹ é¢ä¸´æ°æ®ç¨ç¼ºé®é¢ï¼çæ³ç4Dæ°æ®éï¼åå«å®æ´çå°é¢çå®å ä½ãå¤è§ãè¿å¨åäº¤äºæ æ³¨ï¼ä»ç¶æéã
*   <strong>æ³åè½åæéï¼</strong> è®¸å¤æ¨¡åå¨ç­çªçæ°æ®åå¸ä¸è®­ç»ï¼é¾ä»¥æ³åå°æªè§è¿çåºæ¯æç©ä½ã
*   <strong>ææçµæ´»æ§ä¸è¶³ï¼</strong> ä¼ ç»ç½æ ¼è¡¨ç¤ºå¨å¤çææååï¼å¦åè£ãåå¹¶ï¼åä½ç§¯ç°è±¡ï¼å¦çé¾ãç«ç°ï¼æ¶å­å¨å°é¾ã
*   <strong>ç©ççå®æ§ä¸è¶³ï¼</strong> å°½ç®¡åå¾äºè¿å±ï¼ä½è®¸å¤æ°æ®é©±å¨ç4Dæ¨¡åä»ç¶çæè§è§ä¸å¼äººæ³¨ç®ä½ç©çä¸ä¸çå®çå¨æã
*   <strong>ç¼ºä¹ç»ä¸åºåï¼</strong> 4Dè¯ä¼°ç¼ºä¹å¨é¢çåºåï¼æ æ³ç³»ç»å°æ¯è¾ä¸åè¡¨ç¤ºå¨åç§ç©ä½ç±»å«ãè¿å¨ç±»ååæ¡ä»¶æ¨¡æä¸çæ§è½ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>åé¦éå»ºåæ··åçæ-éå»ºï¼</strong> ä»éåºæ¯ä¼åååé¦æ¨çåæ··åçæ-éå»ºæµæ°´çº¿çè½¬åï¼ä»¥æé«è®¡ç®æçåå¤çç¨çè§å¾ã
*   <strong>æ´åä¸çç¥è¯ï¼</strong> å©ç¨LLMsåVFMsè¿è¡å¤æ¨¡ææ¨çåå¸¸è¯æå¯¼ï¼ä»¥åæ´åç©çåéªï¼éè¿å¯å¾®åç©çæå¤±æéå»º-æ¨¡ææµæ°´çº¿ï¼ï¼ä»¥ç¡®ä¿4Dåå®¹çç©ççå®æ§ã
*   <strong>ç»ä¸ãèªéåºåç»ææç¥çè¡¨ç¤ºï¼</strong> å¼åè½å¤æ ç¼å¤çä¸åè¿å¨ç±»åãç©ºé´å°ºåº¦åææååçæ··åè¡¨ç¤ºï¼ç»åç»æåæ¨¡åçè§£éæ§åéå¼ç¥ç»è¡¨ç¤ºççµæ´»æ§ã
*   <strong>æ¢ç´¢ç»æåæ¨¡åçä½ç¨ï¼</strong> è¿ä¸æ­¥æ¢ç´¢åå±ãåºäºé¨ä»¶æç©çåºç¡çè¡¨ç¤ºå¦ä½å¢å¼º4Då­¦ä¹ ä¸­çè¿å¨æ¨çãäº¤äºå»ºæ¨¡åç»åæ§ã
*   <strong>å¼åå¤§è§æ¨¡ãæ åå4Dæ°æ®éååºåï¼</strong> è§£å³æ°æ®ç¨ç¼ºé®é¢ï¼ä»¥æ¨å¨è¯¥é¢åçè¿ä¸æ­¥åå±ã
*   <strong>èªçç£ãå æåç©çä¿¡æ¯å­¦ä¹ ï¼</strong> åå±è¶è¶éå»ºæçç£é©±å¨è®­ç»çå­¦ä¹ èå¼ï¼ç´æ¥ä»ç¨çåå¤æ¨¡æè¾å¥ä¸­æ¨æ­è¿å¨ãäº¤äºåæå¾ã
*   <strong>è§£è¦è¡¨ç¤ºï¼</strong> è¿ä¸æ­¥ç ç©¶å ä½ãè¿å¨ãå¤è§ååç§ç¬ç«å¯æä½çè§£è¦è¡¨ç¤ºï¼ä»¥æé«å¯æ§æ§åå¯ç¼è¾æ§ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present a survey on 4D generation and reconstruction, a fast-evolving
subfield of computer graphics whose developments have been propelled by recent
advances in neural fields, geometric and motion deep learning, as well 3D
generative artificial intelligence (GenAI).</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.19255v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.19255v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.20822v1'></a></p>
<h2 id="holocine-holistic-generation-of-cinematic-multi-shot-long-video-narratives"><a href="https://arxiv.org/abs/2510.20822v1">HoloCine: Holistic Generation of Cinematic Multi-Shot Long Video Narratives</a></h2>
<p><strong>Authors:</strong> Yihao Meng, Hao Ouyang, Yue Yu, Qiuyu Wang, Wen Wang, Ka Leong Cheng, Hanlin Wang, Yixuan Li, Cheng Chen, Yanhong Zeng, Yujun Shen, Huamin Qu</p>
<p><strong>Published:</strong> 2025-10-23</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>State-of-the-art text-to-video models excel at generating isolated clips but
fall short of creating the coherent, multi-shot narratives, which are the
essence of storytelling. We bridge this "narrative gap" with HoloCine, a model
that generates entire scenes holistically to ensure global consistency from the
first shot to the last. Our architecture achieves precise directorial control
through a Window Cross-Attention mechanism that localizes text prompts to
specific shots, while a Sparse Inter-Shot Self-Attention pattern (dense within
shots but sparse between them) ensures the efficiency required for minute-scale
generation. Beyond setting a new state-of-the-art in narrative coherence,
HoloCine develops remarkable emergent abilities: a persistent memory for
characters and scenes, and an intuitive grasp of cinematic techniques. Our work
marks a pivotal shift from clip synthesis towards automated filmmaking, making
end-to-end cinematic creation a tangible future. Our code is available at:
https://holo-cine.github.io/.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾è®ºæâHoloCine: Holistic Generation of Cinematic Multi-Shot Long Video Narrativesâçå¨é¢æè¦ã</p>
<hr />
<h3 id="holocine">HoloCine: çµå½±å¤éå¤´é¿è§é¢åäºçæ´ä½çæ</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
å½åæåè¿çææ¬å°è§é¢æ¨¡åå¨çæç¬ç«ç­è§é¢çæ®µæ¹é¢è¡¨ç°åºè²ï¼ä½å¨åå»ºè¿è´¯çå¤éå¤´åäºï¼çµå½±åäºçç²¾é«ï¼æ¹é¢å­å¨ä¸è¶³ãè¿äºæ¨¡åé¾ä»¥å¨å¤ä¸ªéå¤´ä¹é´ä¿æè§è²ãåºæ¯åé£æ ¼çä¸è´æ§ï¼å¹¶ä¸ç¼ºä¹å¯¹éå¤´åæ¢ååäºæµç¨çç²¾ç¡®æ§å¶ãè¿ç¯è®ºææ¨å¨å¼¥åè¿ç§âåäºé¸¿æ²âï¼å®ç°ä»ææ¬æç¤ºå°è¿è´¯ãå¤éå¤´é¿è§é¢åäºçæ´ä½çæã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
HoloCine å¼å¥äºä¸ä¸ªæ°é¢çæ´ä½çææ¡æ¶ï¼éè¿ä»¥ä¸ä¸¤ä¸ªæ ¸å¿æºå¶è§£å³äºä¸è¿°é®é¢ï¼
*   <strong>çªå£äº¤åæ³¨æåï¼Window Cross-Attentionï¼ï¼</strong> è¯¥æºå¶éè¿å°ææ¬æç¤ºå±é¨åå°ç¹å®éå¤´ï¼å®ç°äºç²¾ç¡®çå¯¼æ¼æ§å¶ãå®ç¡®ä¿æ¯ä¸ªéå¤´çåå®¹åè¾¹çé½ä¸ç¸åºçææ¬æè¿°ç²¾ç¡®å¯¹é½ï¼ä»èå®ç°æ¸æ°ãåäºé©±å¨çéå¤´è½¬æ¢ã
*   <strong>ç¨çéå¤´é´èªæ³¨æåï¼Sparse Inter-Shot Self-Attentionï¼ï¼</strong> ä¸ºäºåæå¨èªæ³¨æåæºå¶å¨å¤çé¿åºåæ¶è®¡ç®ææ¬è¿é«çé®é¢ï¼HoloCine éç¨äºä¸ç§æ··åç¨çæ¨¡å¼ãå®å¨éå¤´åé¨ä¿æå¯éæ³¨æåä»¥ç¡®ä¿è¿å¨è¿ç»­æ§ï¼åæ¶å¨éå¤´ä¹é´ä½¿ç¨åºäºç´§åæè¦çç¨çè¿æ¥è¿è¡é«æä¿¡æ¯äº¤æ¢ãè¿ç§è®¾è®¡å°è®¡ç®å¤æåº¦ä»åºåé¿åº¦çå¹³æ¹çº§éä½å°æ¥è¿çº¿æ§ï¼ä½¿å¾åéçº§æ´ä½çææä¸ºå¯è½ã
*   <strong>æ°æ®æ´çä¸åå±æ æ³¨ï¼</strong> ä¸ºäºè®­ç»è¯¥æ¡æ¶ï¼ä½èæå»ºäºä¸ä¸ªå¤§è§æ¨¡ãåå±æ æ³¨çå¤éå¤´åºæ¯æ°æ®éï¼ä»çµå½±åçµè§å§ä¸­æåå¹¶å¤çåå®¹ï¼å¹¶ä½¿ç¨ Gemini 2.5 Flash è¿è¡åå±æç¤ºæ æ³¨ï¼åæ¬å¨å±åºæ¯æè¿°åæ¯ä¸ªéå¤´çå·ä½æä»¤ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶éè¦æ§ï¼</strong>
HoloCine å¨å¤éå¤´é¿è§é¢çæä»»å¡ä¸­åå¾äºæ¾èçæ§è½æåï¼å¨åäºè¿è´¯æ§æ¹é¢æ ç«äºæ°çææ¯æ åã
*   <strong>åè¶çè¿è´¯æ§ï¼</strong> æ¨¡åå¨è§è²èº«ä»½ãèæ¯åæ´ä½é£æ ¼æ¹é¢è¡¨ç°åºè²çé¿æä¸è´æ§ï¼æ¾èä¼äºç°æåºçº¿æ¹æ³ï¼åæ¬é¢è®­ç»è§é¢æ©æ£æ¨¡åãä¸¤é¶æ®µå³é®å¸§å°è§é¢çææ¹æ³åæ´ä½å¤éå¤´çææ¹æ³ï¼ã
*   <strong>ç²¾ç¡®çæ§å¶ï¼</strong> HoloCine å®ç°äºå¯¹éå¤´åæ¢ãéå¤´å°ºåº¦ãæåæºè§åº¦åæåæºè¿å¨çç²¾ç¡®å¯¼æ¼æ§å¶ï¼è½å¤æ ¹æ®ææ¬æç¤ºçæç¬¦åçµå½±è¯­è¨çå¤æåäºã
*   <strong>æ¶ç°è½åï¼</strong> æ¨¡åå±ç°åºä»¤äººæè®¶çæ¶ç°è®°å¿è½åï¼åæ¬è·¨è§è§çè§è²/ç©ä½æä¹æ§ãé¿è·ç¦»ä¸è´æ§ï¼å³ä½¿å¨æ å³éå¤´ä¸­æ­åä¹è½åå¿èµ·ä¸»ä½ï¼ä»¥åå¯¹ç»ç²åº¦éæ¾èæ§ç»èçæä¹è®°å¿ï¼è¿è¡¨ææ¨¡åå¯¹è§è§åäºæäºæ´æ·±å±æ¬¡çéå¼çè§£ã
*   <strong>è®¡ç®æçï¼</strong> ç¨çéå¤´é´èªæ³¨æåæºå¶å¨ä¿æçæè´¨éçåæ¶ï¼å¤§å¹éä½äºè®¡ç®ææ¬ï¼ä½¿å¾åéçº§æ´ä½çæå¨å®éä¸­å¯è¡ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
å°½ç®¡ HoloCine å¨ä¿æè§è§ä¸è´æ§æ¹é¢è¡¨ç°åºè²ï¼ä½å®å¨<strong>å ææ¨ç</strong>æ¹é¢å­å¨å±éæ§ãæ¨¡åå¯è½æ æ³çè§£ä¸ä¸ªå¨ä½å¦ä½æ¹åç©ä½çç©çç¶æãä¾å¦ï¼å¨âåæ°´âçåºæ¯ä¸­ï¼æ¨¡åæªè½æ­£ç¡®æ¸²æé»è¾ç»æï¼èæ¯ä¼åä¿æäºä¸åå§éå¤´ä¸è´çè§è§ç¶æï¼èéå¨ä½çç©çåæã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
æªæ¥çç ç©¶å¯ä»¥éä¸­å¨è§£å³æ¨¡åçå ææ¨çè½åä¸è¶³çé®é¢ï¼ä½¿å¶è½å¤ä»æç¥ä¸è´æ§è¿ä¸æ­¥åå±å°é»è¾çãå æå³ç³»çæ¨çãè¿å°æ¯å®ç°èªå¨åçµå½±å¶ä½çå³é®ä¸æ­¥ã</p>
<hr />
<p><strong>æ»ç»ï¼</strong>
HoloCine éè¿å¼å¥çªå£äº¤åæ³¨æååç¨çéå¤´é´èªæ³¨æåæºå¶ï¼æåå¼¥åäºææ¬å°è§é¢çæä¸­çâåäºé¸¿æ²âãå®ä¸ä»å¨å¤éå¤´è§é¢åäºè¿è´¯æ§åæ§å¶æ¹é¢è¾¾å°äºæ°çææ¯æ°´å¹³ï¼è¿å±ç°äºä»¤äººå°è±¡æ·±å»çæ¶ç°è®°å¿è½ååå¯¹çµå½±è¯­è¨çç´è§çè§£ãè¿é¡¹å·¥ä½æ å¿çä»å­¤ç«çæ®µåæå°èªå¨åçµå½±å¶ä½çå³é®è½¬åï¼ä¸ºç«¯å°ç«¯çµå½±åä½çæªæ¥å¥ å®äºåºç¡ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>State-of-the-art text-to-video models excel at generating isolated clips but
fall short of creating the coherent, multi-shot narratives, which are the
essence of storytelling.</li>
<li>Beyond setting a new state-of-the-art in narrative coherence,
HoloCine develops remarkable emergent abilities: a persistent memory for
characters and scenes, and an intuitive grasp of cinematic techniques.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.20822v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.20822v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.20819v1'></a></p>
<h2 id="towards-general-modality-translation-with-contrastive-and-predictive-latent-diffusion-bridge"><a href="https://arxiv.org/abs/2510.20819v1">Towards General Modality Translation with Contrastive and Predictive Latent Diffusion Bridge</a></h2>
<p><strong>Authors:</strong> Nimrod Berman, Omkar Joglekar, Eitan Kosman, Dotan Di Castro, Omri Azencot</p>
<p><strong>Published:</strong> 2025-10-23</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Recent advances in generative modeling have positioned diffusion models as
state-of-the-art tools for sampling from complex data distributions. While
these models have shown remarkable success across single-modality domains such
as images and audio, extending their capabilities to Modality Translation (MT),
translating information across different sensory modalities, remains an open
challenge. Existing approaches often rely on restrictive assumptions, including
shared dimensionality, Gaussian source priors, and modality-specific
architectures, which limit their generality and theoretical grounding. In this
work, we propose the Latent Denoising Diffusion Bridge Model (LDDBM), a
general-purpose framework for modality translation based on a latent-variable
extension of Denoising Diffusion Bridge Models. By operating in a shared latent
space, our method learns a bridge between arbitrary modalities without
requiring aligned dimensions. We introduce a contrastive alignment loss to
enforce semantic consistency between paired samples and design a
domain-agnostic encoder-decoder architecture tailored for noise prediction in
latent space. Additionally, we propose a predictive loss to guide training
toward accurate cross-domain translation and explore several training
strategies to improve stability. Our approach supports arbitrary modality pairs
and performs strongly on diverse MT tasks, including multi-view to 3D shape
generation, image super-resolution, and multi-view scene synthesis.
Comprehensive experiments and ablations validate the effectiveness of our
framework, establishing a new strong baseline in general modality translation.
For more information, see our project page:
https://sites.google.com/view/lddbm/home.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Nimrod Bermanç­äººæ°åçè®ºæâTowards General Modality Translation with Contrastive and Predictive Latent Diffusion Bridgeâçå¨é¢æè¦ã</p>
<hr />
<p><strong>è®ºææè¦ï¼Towards General Modality Translation with Contrastive and Predictive Latent Diffusion Bridge</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³æ¨¡æç¿»è¯ï¼Modality Translation, MTï¼é¢åçå¼æ¾æ§ææãå°½ç®¡æ©æ£æ¨¡åå¨åä¸æ¨¡æï¼å¦å¾ååé³é¢ï¼çææ¹é¢åå¾äºæ¾èæåï¼ä½å°å¶è½åæ©å±å°è·¨ä¸åæå®æ¨¡æçä¿¡æ¯ç¿»è¯ä»ç¶é¢ä¸´å°é¾ãç°ææ¹æ³éå¸¸ä¾èµäºéå¶æ§åè®¾ï¼å¦å±äº«ç»´åº¦ãé«æ¯æºåéªåæ¨¡æç¹å®æ¶æï¼è¿éå¶äºå®ä»¬çéç¨æ§åçè®ºåºç¡ãå æ­¤ï¼æ ¸å¿é®é¢æ¯å¼åä¸ä¸ªéç¨ä¸çè®ºåºç¡æå®çæ¡æ¶ï¼è½å¤å®ç°ä»»ææ¨¡æä¹é´çç¿»è¯ï¼èæ éè¿äºä¸¥æ ¼çåè®¾ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
ä½èæåºäº<strong>æ½å¨å»åªæ©æ£æ¡¥æ¨¡åï¼Latent Denoising Diffusion Bridge Model, LDDBMï¼</strong>ï¼è¿æ¯ä¸ä¸ªåºäºå»åªæ©æ£æ¡¥æ¨¡åï¼DDBMï¼çæ½å¨åéæ©å±çéç¨æ¨¡æç¿»è¯æ¡æ¶ãå¶ä¸»è¦åæ°åæ¬ï¼
*   <strong>å±äº«æ½å¨ç©ºé´æä½ï¼</strong> LDDBMå¨å±äº«æ½å¨ç©ºé´ä¸­è¿è¡ï¼å­¦ä¹ ä»»ææ¨¡æä¹é´çâæ¡¥æ¢âï¼æ éå¯¹é½ç»´åº¦ãè¿åæäºç°æDDBMæ¨¡åè¦æ±æ¨¡æå·æç¸åç»´åº¦çéå¶ã
*   <strong>å¯¹æ¯å¯¹é½æå¤±ï¼Contrastive Alignment Lossï¼ï¼</strong> å¼å¥äºåCLIPå¯åçå¯¹æ¯æå¤±ï¼ç¨äºå¨éå¯¹æ ·æ¬ä¹é´å¼ºå¶æ§è¡è¯­ä¹ä¸è´æ§ï¼å°å¯¹åºå¯¹æè¿ï¼å°ä¸ç¸å³å¯¹æ¨å¼ã
*   <strong>é¢åæ å³ç¼ç å¨-è§£ç å¨æ¶æï¼</strong> è®¾è®¡äºä¸ç§é¢åæ å³çç¼ç å¨-è§£ç å¨æ¶æï¼ä¸é¨ç¨äºæ½å¨ç©ºé´ä¸­çåªå£°é¢æµï¼åå°äºæ¶æåå·®ã
*   <strong>é¢æµæå¤±ï¼Predictive Lossï¼ï¼</strong> æåºäºä¸ç§é¢æµæå¤±ï¼ä»¥æå¯¼è®­ç»å®ç°åç¡®çè·¨é¢åç¿»è¯ï¼ç¡®ä¿æ´ä¸ªç¼ç -æ¡¥æ¥-è§£ç ç®¡éçè¯­ä¹åå®¹ä¿çã
*   <strong>è¿­ä»£è®­ç»ç­ç¥ï¼</strong> æ¢ç´¢äºå ç§è®­ç»ç­ç¥ä»¥æé«æ¨¡åçç¨³å®æ§åæ§è½ï¼å¶ä¸­è¿­ä»£æ¹æ³è¢«è¯ææ¯æä¼çï¼å®å¨å¯¹é½åå»åªæ­¥éª¤ä¹é´äº¤æ¿è¿è¡ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
LDDBMå¨å¤ç§æ¨¡æç¿»è¯ä»»å¡ä¸è¡¨ç°åºè²ï¼åæ¬ï¼
*   <strong>å¤è§è§å°3Då½¢ç¶çæï¼</strong> å¨ShapeNetæ°æ®éä¸ï¼LDDBMå¨1-NNAï¼0.508ï¼åIoUï¼0.664ï¼ææ ä¸åä¼äºææåºçº¿ï¼è¡¨æå¶å¨çæä¸çå®æ°æ®åå¸æ´ç¸ä¼¼ç3Då½¢ç¶æ¹é¢å·æåè¶ççæè½ååä¿çåº¦ã
*   <strong>å¾åè¶åè¾¨çï¼</strong> å¨é¶æ ·æ¬ä½åè¾¨çå°é«åè¾¨ççæä»»å¡ä¸­ï¼LDDBMå¨PSNRï¼25.6ï¼ãSSIMï¼0.68ï¼åLPIPSï¼0.32ï¼æ¹é¢ååå¾æä½³ç»æï¼çæäºæç¥ä¸æ´çå®ãæ´å¯é çå¾åã
*   <strong>å¤è§è§åºæ¯åæï¼</strong> å¨nuScenes-Occupancyæ°æ®éä¸ï¼LDDBMå¨1-NNAï¼0.807ï¼åIoUï¼0.233ï¼æ¹é¢ä¹è¡¨ç°æä½³ï¼å±ç¤ºäºå¶å¨æ´å¤æãæ´çå®çèªå¨é©¾é©¶åºæ¯ä¸­ççµæ´»æ§åéç¨æ§ã
*   <strong>å¾åå°å¾åç¿»è¯ï¼Edges â Bagsï¼ï¼</strong> LDDBMå¨è´¨éä¸å·æç«äºåï¼FID 4.17ï¼ï¼åæ¶æ¨çéåº¦æ¯DDBMå¿«ä¸¤åä»¥ä¸ï¼7.8ç§ vs 16.9ç§ï¼ã
*   <strong>æ¶ææ¶èç ç©¶ï¼</strong> éªè¯äºTransformerç¼ç å¨-è§£ç å¨è®¾è®¡ãç©ºé´åµå¥åå¯å­¦ä¹ ç[MASK] tokenå¯¹æ§è½çè´¡ç®ã
*   <strong>æå¤±å½æ°æ¶èç ç©¶ï¼</strong> è¯æäºç»åé¢æµæå¤±åå¯¹æ¯æå¤±çå®æ´éç½®å®ç°äºæé«æ§è½ã</p>
<p>è¿äºç»æå±åéªè¯äºLDDBMæ¡æ¶çæææ§ï¼ä¸ºéç¨æ¨¡æç¿»è¯å»ºç«äºæ°çå¼ºå¤§åºçº¿ï¼å¹¶å±ç¤ºäºå¶å¨å¼ææ¨¡æåºæ¯ä¸­çé²æ£æ§åæ³åè½åã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
è®ºæä¸­è®¨è®ºçå±éæ§ä¸»è¦éä¸­å¨è®¡ç®æçåæ½å¨çæªæ¥æ¹è¿æ¹åä¸ãè½ç¶LDDBMå¨æ§è½ä¸ä¼äºè®¸å¤åºçº¿ï¼ä½ä½èæåºï¼ä¸ç¹å®ä»»å¡çSOTAæ¹æ³ç¸æ¯ï¼ä»å­å¨æ§è½å·®è·ï¼è¿æ¯ç±äºå¶éç¨æ§èéé¢åç¹å®ä¼åæè´ãæ­¤å¤ï¼å°½ç®¡è¿­ä»£è®­ç»ç­ç¥æé«äºç¨³å®æ§ï¼ä½æ¨¡ææ¡¥æ¥åç¼ç å¨ç½ç»ä¹é´çåºæå²çªä»ç¶æ¯ä¸ä¸ªéè¦è§£å³çé®é¢ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
ä½èå±æäºæªæ¥ç ç©¶çå ä¸ªæ¹åï¼
*   <strong>ééå¯¹æ¨¡æç¿»è¯ï¼</strong> å°LDDBMæ¡æ¶æ©å±å°å¤çééå¯¹æ¨¡æç¿»è¯ä»»å¡ã
*   <strong>åºåæé«ç»´æ°æ®ï¼</strong> å°æ¡æ¶æ©å±å°è§é¢åä½ç§¯è¡¨ç¤ºç­åºåæé«ç»´æ°æ®ï¼ä»¥è¿ä¸æ­¥æ©å¤§å¶éç¨æ§ã
*   <strong>è®¡ç®æçåå¯æ©å±æ§ï¼</strong> è¿ä¸æ­¥ä¼åæ¨¡åçè®¡ç®æçåå¯æ©å±æ§ï¼ä»¥å¤çæ´å¤§è§æ¨¡çæ°æ®éåæ´å¤æçä»»å¡ã</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>Recent advances in generative modeling have positioned diffusion models as
state-of-the-art tools for sampling from complex data distributions.</li>
<li>In this
work, we propose the Latent Denoising Diffusion Bridge Model (LDDBM), a
general-purpose framework for modality translation based on a latent-variable
extension of Denoising Diffusion Bridge Models.</li>
<li>By operating in a shared latent
space, our method learns a bridge between arbitrary modalities without
requiring aligned dimensions.</li>
<li>We introduce a contrastive alignment loss to
enforce semantic consistency between paired samples and design a
domain-agnostic encoder-decoder architecture tailored for noise prediction in
latent space.</li>
<li>Additionally, we propose a predictive loss to guide training
toward accurate cross-domain translation and explore several training
strategies to improve stability.</li>
<li>Our approach supports arbitrary modality pairs
and performs strongly on diverse MT tasks, including multi-view to 3D shape
generation, image super-resolution, and multi-view scene synthesis.</li>
<li>Comprehensive experiments and ablations validate the effectiveness of our
framework, establishing a new strong baseline in general modality translation.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.20819v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.20819v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.20813v1'></a></p>
<h2 id="gsworld-closed-loop-photo-realistic-simulation-suite-for-robotic-manipulation"><a href="https://arxiv.org/abs/2510.20813v1">GSWorld: Closed-Loop Photo-Realistic Simulation Suite for Robotic Manipulation</a></h2>
<p><strong>Authors:</strong> Guangqi Jiang, Haoran Chang, Ri-Zhao Qiu, Yutong Liang, Mazeyu Ji, Jiyue Zhu, Zhao Dong, Xueyan Zou, Xiaolong Wang</p>
<p><strong>Published:</strong> 2025-10-23</p>
<p><strong>Categories:</strong> cs.RO, cs.AI, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>This paper presents GSWorld, a robust, photo-realistic simulator for robotics
manipulation that combines 3D Gaussian Splatting with physics engines. Our
framework advocates "closing the loop" of developing manipulation policies with
reproducible evaluation of policies learned from real-robot data and sim2real
policy training without using real robots. To enable photo-realistic rendering
of diverse scenes, we propose a new asset format, which we term GSDF (Gaussian
Scene Description File), that infuses Gaussian-on-Mesh representation with
robot URDF and other objects. With a streamlined reconstruction pipeline, we
curate a database of GSDF that contains 3 robot embodiments for single-arm and
bimanual manipulation, as well as more than 40 objects. Combining GSDF with
physics engines, we demonstrate several immediate interesting applications: (1)
learning zero-shot sim2real pixel-to-action manipulation policy with
photo-realistic rendering, (2) automated high-quality DAgger data collection
for adapting policies to deployment environments, (3) reproducible benchmarking
of real-robot manipulation policies in simulation, (4) simulation data
collection by virtual teleoperation, and (5) zero-shot sim2real visual
reinforcement learning. Website: https://3dgsworld.github.io/.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Guangqi Jiangç­äººæ°åçè®ºæâGSWorld: Closed-Loop Photo-Realistic Simulation Suite for Robotic Manipulationâçå¨é¢æè¦ã</p>
<hr />
<p><strong>è®ºææè¦ï¼GSWorld: ç¨äºæºå¨äººæä½çé­ç¯ç§ççº§çå®ææ¨¡æå¥ä»¶</strong></p>
<p><strong>1. è§£å³çä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³æºå¨äººæä½ç­ç¥å¼åä¸­çæ ¸å¿ææï¼å³å¦ä½å¼¥åçå®ä¸çä¸æ¨¡æç¯å¢ä¹é´çâè§è§é¸¿æ²âåâå¨ä½ç©ºé´é¸¿æ²âãä¼ ç»çæ¨¡æå¨å¨è§è§çå®æåä¸çå®æºå¨äººAPIçå¯¹é½æ¹é¢å­å¨ä¸è¶³ï¼å¯¼è´ç­ç¥é¾ä»¥ä»æ¨¡æç¯å¢é¶æ ·æ¬è¿ç§»å°çå®ä¸çãæ­¤å¤ï¼æ¶éé«è´¨éççå®ä¸çæºå¨äººæ°æ®è¿è¡ç­ç¥è®­ç»åè¯ä¼°ææ¬é«æä¸é¾ä»¥æ©å±ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
GSWorldéè¿ä»¥ä¸åæ°ç¹æä¾äºä¸ä¸ªé­ç¯ãç§ççº§çå®æçæ¨¡æå¥ä»¶ï¼
*   <strong>ç»å3Dé«æ¯æ³¼æºï¼3DGSï¼ä¸ç©çå¼æï¼</strong> è¿æ¯æ ¸å¿åæ°ï¼å©ç¨3DGSå®ç°åºæ¯åå¯¹è±¡çç§ççº§çå®ææ¸²æï¼åæ¶ç»åç©çå¼æç¡®ä¿ç©çäº¤äºçåç¡®æ§ã
*   <strong>æ°çèµäº§æ ¼å¼GSDFï¼Gaussian Scene Description Fileï¼ï¼</strong> æåºäºä¸ç§å°é«æ¯-ç½æ ¼è¡¨ç¤ºä¸æºå¨äººURDFåå¶ä»å¯¹è±¡ç»åçæ°èµäº§æ ¼å¼ï¼ä½¿å¾ç§ççº§çå®ææ¸²æåç©çæ¨¡æè½å¤æ ç¼éæã
*   <strong>ç®åçéå»ºæµæ°´çº¿ï¼</strong> å®ç°äºä»çå®ä¸çåºæ¯ï¼åæ¬æºå¨äººåç©ä½ï¼å°æ¨¡æç¯å¢çåº¦éç²¾ç¡®æ°å­å­ªçéå»ºï¼éè¿ArUcoæ è®°è¿è¡å°ºåº¦å¯¹é½ï¼å¹¶éè¿ICPå°æºå¨äººURDFä¸åºæ¯å¯¹é½ã
*   <strong>é­ç¯DAggerè®­ç»ï¼</strong> åè®¸å¨æ¨¡æä¸­èªå¨æ¶éçº æ­£æ°æ®ï¼ä»¥è¿­ä»£æ¹è¿å¨é¨ç½²ç¯å¢ä¸­å¤±è´¥çç­ç¥ï¼æ¾èæé«äºæ°æ®æçåç­ç¥éåºæ§ã
*   <strong>æ¯æå¤ç§ç­ç¥å­¦ä¹ èå¼ï¼</strong> æ¨¡æå¨æ¯æé¶æ ·æ¬Sim2Realåç´ å°å¨ä½æä½ç­ç¥å­¦ä¹ ãDAggeræ°æ®æ¶éãèæé¥æä½æ°æ®æ¶éä»¥åè§è§å¼ºåå­¦ä¹ ã
*   <strong>å¹¿æ³çèµäº§æ°æ®åºï¼</strong> åå«äº3ç§æºå¨äººï¼åèååèï¼å40å¤ä¸ªå¯¹è±¡çGSDFæ°æ®åºï¼æ¯æå¤æ ·åçæä½ä»»å¡ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶éè¦æ§ï¼</strong>
GSWorldå±ç¤ºäºå¤é¡¹ä»¤äººå°è±¡æ·±å»çåºç¨åç»æï¼
*   <strong>é¶æ ·æ¬Sim2Realè¿ç§»ï¼</strong> è®ºæè¯æäºGSWorldè½å¤ææå¼¥åSim2Realé¸¿æ²ï¼å®ç°é¶æ ·æ¬ç­ç¥è¿ç§»ï¼å¨å¤ç§æä½ä»»å¡ä¸­åå¾äºæå¸æçæåçã
*   <strong>é­ç¯DAggerç­ç¥æ¹è¿ï¼</strong> éè¿å¨æ¨¡æä¸­æ¶éçº æ­£æ°æ®ï¼DAggeræ¹æ³æ¾èæé«äºç­ç¥æ§è½ï¼å¹¶ä¼äºä»å¤´å¼å§è®­ç»çç­ç¥ï¼å°¤å¶æ¯å¨å¤ççå®ä¸çç­ç¥é¨ç½²åçå¤±è´¥æåµæ¶ã
*   <strong>è§è§åºåæµè¯ï¼</strong> GSWorldä¸­çæ¨¡ææ§è½ä¸çå®ä¸çæ§è½é«åº¦ç¸å³ï¼è¡¨æå¶å¯ä»¥ä½ä¸ºå¯é çåºåæµè¯å·¥å·ï¼ç¨äºè¯ä¼°çå®æºå¨äººæä½ç­ç¥ï¼èæ éç©çé¨ç½²ã
*   <strong>èæé¥æä½ï¼</strong> å®ç°äºéè¿èæé¥æä½å¨æ¨¡æä¸­é«ææ¶éé«è´¨éæ°æ®ï¼éä½äºçå®ä¸çæ°æ®æ¶éçææ¬åé¾åº¦ã
*   <strong>è§è§å¼ºåå­¦ä¹ ï¼</strong> GSWorldæ¯æå¹¶è¡ç¯å¢ï¼æå©äºè®­ç»è§è§RLç­ç¥ï¼å¹¶è½ææåå°RLçSim2Realè§è§é¸¿æ²ã</p>
<p>è¿äºç»æçéè¦æ§å¨äºï¼GSWorldæä¾äºä¸ä¸ªå¼ºå¤§çå·¥å·ï¼è½å¤å éæºå¨äººæä½ç­ç¥çå¼ååè¯ä¼°ï¼éä½äºå¯¹æè´µä¸èæ¶ççå®æºå¨äººå®éªçä¾èµï¼åæ¶æé«äºç­ç¥çé²æ£æ§åæ³åè½åã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
è®ºæä¸­æªæç¡®æåæ¾èçå±éæ§ï¼ä½ä»ææ¯ç»èä¸­å¯ä»¥æ¨æ­åºä¸äºæ½å¨çèéï¼
*   <strong>3DGSéå»ºçè®¡ç®ææ¬ï¼</strong> å°½ç®¡3DGSå¨æ¸²ææçä¸ä¼äºNeRFsï¼ä½é«è´¨éç3DGSéå»ºï¼ç¹å«æ¯å¯¹äºå¤æå¨æåºæ¯ï¼ä»ç¶å¯è½éè¦å¤§éçè®¡ç®èµæºåæ¶é´ã
*   <strong>ç©çå¼æçåç¡®æ§ï¼</strong> å°½ç®¡è®ºæå¼ºè°ç»åäºç©çå¼æï¼ä½æ¨¡æç©çä¸çå®ä¸çç©çä¹é´çç»å¾®å·®å¼ï¼ä¾å¦æ©æ¦ãç¢°ææ¨¡åï¼ä»å¯è½å­å¨ï¼è¿å¯è½å½±åæäºé«ç²¾åº¦ä»»å¡çSim2Realè¿ç§»ã
*   <strong>æ³åè½åï¼</strong> å°½ç®¡GSWorldæ¯æå¤æ ·åçåºæ¯åå¯¹è±¡ï¼ä½å¶å¨å®å¨æªç¥æé«åº¦å¤æççå®ä¸çç¯å¢ä¸­çæ³åè½åä»éè¿ä¸æ­¥éªè¯ã
*   <strong>DAggeræ°æ®æ¶éçèªå¨åç¨åº¦ï¼</strong> è®ºææå°DAggeræ°æ®æ¶éæ¯èªå¨åçï¼ä½å¶å¯¹âç¹æä¿¡æ¯âçä¾èµï¼ä¾å¦æ¨¡æå¨æä¾çç²¾ç¡®ç¶æï¼å¨çå®ä¸çä¸­å¯è½é¾ä»¥å®å¨å¤å¶ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
åºäºGSWorldçè´¡ç®åæ½å¨èéï¼æªæ¥çç ç©¶æ¹åå¯è½åæ¬ï¼
*   <strong>æ´å¤æçå¨æåºæ¯åäº¤äºï¼</strong> æ©å±GSWorldä»¥å¤çæ´å¤æçå¨æåºæ¯ãè½¯ä½æºå¨äººæä½æå¤æºå¨äººåä½ä»»å¡ã
*   <strong>ç©çå¼æçæ¹è¿åéªè¯ï¼</strong> è¿ä¸æ­¥æåæ¨¡æç©çå¼æçåç¡®æ§ï¼å¹¶å¼åæ´ä¸¥æ ¼çåº¦éæ åæ¥éåSim2Realç©çé¸¿æ²ã
*   <strong>æ´é«æçéå»ºåæ´æ°ï¼</strong> æ¢ç´¢æ´å¿«éãæ´èªå¨åç3DGSéå»ºååºæ¯æ´æ°æ¹æ³ï¼ä»¥éåºå¿«éååççå®ä¸çç¯å¢ã
*   <strong>ç»åè¯­è¨æ¨¡ååé«çº§è§åï¼</strong> å°GSWorldä¸å¤§åè¯­è¨æ¨¡åæé«çº§è§åç®æ³ç»åï¼ä»¥å®ç°æ´æºè½ãæ´éç¨çæºå¨äººæä½ç­ç¥ã
*   <strong>å¼æ¾ä¸çæ³åï¼</strong> ç ç©¶å¦ä½å©ç¨GSWorldçæçæ°æ®åæ¨¡æè½åï¼è®­ç»åºè½å¤æ³åå°å®å¨æªç¥åå¼æ¾ä¸çç¯å¢çæºå¨äººç­ç¥ã</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>To enable photo-realistic rendering
of diverse scenes, we propose a new asset format, which we term GSDF (Gaussian
Scene Description File), that infuses Gaussian-on-Mesh representation with
robot URDF and other objects.</li>
<li>Combining GSDF with
physics engines, we demonstrate several immediate interesting applications: (1)
learning zero-shot sim2real pixel-to-action manipulation policy with
photo-realistic rendering, (2) automated high-quality DAgger data collection
for adapting policies to deployment environments, (3) reproducible benchmarking
of real-robot manipulation policies in simulation, (4) simulation data
collection by virtual teleoperation, and (5) zero-shot sim2real visual
reinforcement learning.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.20813v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.20813v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.20812v1'></a></p>
<h2 id="small-drafts-big-verdict-information-intensive-visual-reasoning-via-speculation"><a href="https://arxiv.org/abs/2510.20812v1">Small Drafts, Big Verdict: Information-Intensive Visual Reasoning via Speculation</a></h2>
<p><strong>Authors:</strong> Yuhan Liu, Lianhui Qin, Shengjie Wang</p>
<p><strong>Published:</strong> 2025-10-23</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.CL</p>
<p><strong>Abstract:</strong></p>
<p>Large Vision-Language Models (VLMs) have achieved remarkable progress in
multimodal understanding, yet they struggle when reasoning over
information-intensive images that densely interleave textual annotations with
fine-grained graphical elements. The main challenges lie in precisely
localizing critical cues in dense layouts and multi-hop reasoning to integrate
dispersed evidence. We propose Speculative Verdict (SV), a training-free
framework inspired by speculative decoding that combines multiple lightweight
draft experts with a large verdict model. In the draft stage, small VLMs act as
draft experts to generate reasoning paths that provide diverse localization
candidates; in the verdict stage, a strong VLM synthesizes these paths to
produce the final answer, minimizing computational cost while recovering
correct answers. To further improve efficiency and accuracy, SV introduces a
consensus expert selection mechanism that forwards only high-agreement
reasoning paths to the verdict. Empirically, SV achieves consistent gains on
challenging information-intensive and high-resolution visual question answering
benchmarks, including InfographicVQA, ChartMuseum, ChartQAPro, and HR-Bench 4K.
By synthesizing correct insights from multiple partially accurate reasoning
paths, SV achieves both error correction and cost-efficiency compared to large
proprietary models or training pipelines. Code is available at
https://github.com/Tinaliu0123/speculative-verdict</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼è¿æ¯ä¸ç¯å³äºYuhan Liu, Lianhui Qin, Shengjie Wangæ°åçè®ºæâSmall Drafts, Big Verdict: Information-Intensive Visual Reasoning via Speculationâçå¨é¢æè¦ï¼</p>
<p><strong>è®ºææè¦ï¼Small Drafts, Big Verdict: Information-Intensive Visual Reasoning via Speculation</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³å¤§åè§è§-è¯­è¨æ¨¡åï¼VLMsï¼å¨å¤çä¿¡æ¯å¯éåå¾åï¼å³ææ¬æ³¨éä¸ç»ç²åº¦å¾å½¢åç´ å¯éäº¤ç»çå¾åï¼å¦ä¿¡æ¯å¾ãå¾è¡¨ï¼æ¶æé¢ä¸´çææãè¿äºå¾åéè¦ç²¾ç¡®å°å®ä½å¯éå¸å±ä¸­çå³é®çº¿ç´¢ï¼å¹¶è¿è¡å¤è·³æ¨çä»¥æ´ååæ£çè¯æ®ï¼èç°æVLMså¨æ­¤ç±»ä»»å¡ä¸è¡¨ç°ä¸ä½³ï¼å®¹æåºç°å®ä½ä¸ååéè¯¯ä¼ æ­çé®é¢ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
è®ºææåºäºä¸ä¸ªåä¸ºâSpeculative Verdict (SV)âçæ è®­ç»æ¡æ¶ï¼çµææ¥æºäºæ¨æµè§£ç ï¼speculative decodingï¼ãå¶æ ¸å¿åæ°ç¹åæ¬ï¼
*   <strong>ä¸¤é¶æ®µæ¨çæ¡æ¶ï¼</strong>
    *   <strong>èç¨¿é¶æ®µï¼Draft Stageï¼ï¼</strong> å¤ä¸ªè½»éçº§VLMä½ä¸ºâèç¨¿ä¸å®¶âï¼çæå¤æ ·åçæ¨çè·¯å¾ï¼æä¾ä¸åçå®ä½åéã
    *   <strong>å¤å³é¶æ®µï¼Verdict Stageï¼ï¼</strong> ä¸ä¸ªå¼ºå¤§çVLMä½ä¸ºâå¤å³æ¨¡åâï¼æ¥æ¶è¿äºæ¨çè·¯å¾ä½ä¸ºä¸ä¸æè¯æ®ï¼ç»¼ååæå¹¶è¾åºæç»ç­æ¡ãè¿ç§è®¾è®¡æ¨å¨éè¿ç»¼åå¤ä¸ªè§è§æ¥çº æ­£éè¯¯ï¼å¹¶é¿åå¤§åæ¨¡åå¨æ¯ä¸ªå¾ååºåä¸è¿è¡è¿­ä»£æ¨çæå¸¦æ¥çé«æè®¡ç®ææ¬ã
*   <strong>å±è¯ä¸å®¶éæ©æºå¶ï¼Consensus Expert Selection Mechanismï¼ï¼</strong> ä¸ºäºè¿ä¸æ­¥æé«æçååç¡®æ§ï¼SVå¼å¥äºä¸ç§æºå¶ï¼å¨èç¨¿é¶æ®µæ ¹æ®åéç­æ¡çå±è¯åº¦ï¼éè¿è®¡ç®æå¯¹çè´å¯¹æ°ä¼¼ç¶å·®å¼ï¼éæ©é«ä¸è´æ§çæ¨çè·¯å¾ï¼åªå°è¿äºè·¯å¾è½¬åç»å¤å³æ¨¡åãè¿ç¡®ä¿äºå¤å³æ¨¡åæ¥æ¶å°çè¾å¥æ¢ä¿¡æ¯ä¸°å¯åç´§åã
*   <strong>å¤å³æ¨¡åä½ä¸ºåæå¨èéæç¥¨å¨ï¼</strong> å¤å³æ¨¡åä¸ç®åå°è¿è¡å¤æ°æç¥¨ï¼èæ¯è¯ä¼°æ¨çè·¯å¾çæ¥å°ä¸è´æ§ï¼è¯å«çç¾ï¼å¹¶å°ä¸è´ççº¿ç´¢åæä¸ºè¿è´¯çé¢æµï¼ä»èå®ç°éè¯¯çº æ­£ï¼å³ä½¿å°æ°ä¸å®¶æ¯æ­£ç¡®çä¹è½æ¢å¤æ­£ç¡®ç­æ¡ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
SVå¨å¤ä¸ªä¿¡æ¯å¯éååé«åè¾¨çè§è§é®ç­åºåæµè¯ï¼åæ¬InfographicVQA, ChartMuseum, ChartQAPro, å HR-Bench 4Kï¼ä¸åå¾äºæ¾èä¸ä¸è´çæ§è½æåï¼
*   <strong>è¶è¶åºçº¿æ¨¡åï¼</strong> SVæç»­ä¼äºå¼ºå¤§çå¼æºæ¨¡åãå¤§åä¸ææ¨¡åï¼å¦GPT-4oï¼ä»¥ååºäºå·¥å·çæç´¢æ¹æ³ãä¾å¦ï¼ä½¿ç¨GPT-4oä½ä¸ºå¤å³æ¨¡åæ¶ï¼SVå¨InfographicVQAä¸æ¯GPT-4oåºçº¿æé«äº11.9%ã
*   <strong>å¼ºå¤§çéè¯¯çº æ­£è½åï¼</strong> SVæåçº æ­£äº47-53%çå°æ°æ­£ç¡®æ¡ä¾ï¼å³å°æ°èç¨¿ä¸å®¶æ­£ç¡®ä½å¤å³æ¨¡ååç¬å¤±è´¥çæ¡ä¾ï¼ï¼çè³å¨2.5-4.5%çé¶æ­£ç¡®æ¡ä¾ï¼å³ææèç¨¿ä¸å®¶åå¤å³æ¨¡ååç¬é½å¤±è´¥çæ¡ä¾ï¼ä¸­ä¹è½æ¢å¤æ­£ç¡®ç­æ¡ãè¿è¡¨æSVè½å¤ä»é¨ååç¡®çæ¨çè·¯å¾ä¸­åææ­£ç¡®çæ´å¯ï¼ææçº æ­£ä¼ ç»éææ¹æ³é¾ä»¥è§£å³çéè¯¯ã
*   <strong>ææ¬æçï¼</strong> éè¿ä»å¨å¤å³é¶æ®µè°ç¨å¤§åVLMä¸æ¬¡ï¼SVå¨æ¢å¤æ­£ç¡®ç­æ¡çåæ¶ï¼æ¾èéä½äºè®¡ç®ææ¬ï¼æ¯å¤§åä¸ææ¨¡åæè®­ç»ç®¡éæ´å·ææ¬æçã
*   <strong>å¯¹é«åè¾¨çå¾åçæ³åè½åï¼</strong> SVå¨HR-Bench 4Kåºåæµè¯ä¸è¶è¶äºææåºçº¿ï¼è¯æäºå¶å¨æææ§å¤æ¨¡ææ¨çåºæ¯ä¸­å¤çç»ç²åº¦è§è§æç¥çæææ§ã
*   <strong>æ¶èç ç©¶ï¼</strong> è¯å®äºå±è¯ä¸å®¶éæ©æºå¶çæææ§ï¼ä»¥åæä¾å®æ´æ¨çè·¯å¾ä½ä¸ºå¤å³æ¨¡åè¾å¥çéè¦æ§ãè§è§è¾å¥å¯¹äºå¤å³æ¨¡åäº¤åéªè¯äºå®åç¡®æ§è³å³éè¦ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>å±è¯ä¸å®¶éæ©çä¾èµæ§ï¼</strong> å°½ç®¡å±è¯éæ©æºå¶ææï¼ä½å¶æ§è½ä»ä¾èµäºèç¨¿ä¸å®¶æ± ä¸­è³å°å­å¨ä¸äºè½å¤æä¾åçæ¨çè·¯å¾çæ¨¡åã
*   <strong>ç»æåå¾åè¾å¥çè¾å©æ§ï¼</strong> è®ºæåç°ç»æåOCRæ´¾çä¿¡å·å¹¶éSVæ ¸å¿æ§è½çå¿éåï¼ä½å¯è½æå©äºå¤å³æ¨¡ååºåç¸äºç«äºçæ¨çè·¯å¾ã
*   <strong>å·¥å·é©±å¨ç®¡éçå±éæ§ï¼</strong> è®ºæå¨éè¯¯åæä¸­æåºï¼DeepEyesç­å·¥å·é©±å¨æ¹æ³å¨ä¿¡æ¯å¯éåå¾åä¸å­å¨å±éæ§ï¼åæ¬å¾åäºå­é¢æ¥å°ãå·¥å·ä½¿ç¨æçä½ä¸ä»¥åå¨é¿èå¯éçå¾åä¸ç¼ºä¹é²æ£æ§ï¼è¿é´æ¥çªåºäºSVçä¼å¿ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>æ´æºè½çèç¨¿ä¸å®¶éæ©ï¼</strong> æ¢ç´¢æ´å¨ææèªéåºçèç¨¿ä¸å®¶éæ©ç­ç¥ï¼ä»¥è¿ä¸æ­¥ä¼åæ§è½åæçã
*   <strong>å¤å³æ¨¡åçæ³åæ§ï¼</strong> ç ç©¶SVæ¡æ¶å¨æ´å¹¿æ³çå¤æ¨¡æä»»å¡åæ°æ®ç±»åä¸çæ³åè½åã
*   <strong>ç»åå·¥å·é©±å¨æ¹æ³ï¼</strong> å°½ç®¡SVç®åæ¯æ è®­ç»çï¼ä½å¯ä»¥æ¢ç´¢å¦ä½å°SVçåæè½åä¸å·¥å·é©±å¨æ¹æ³çå±é¨æ¥å°ä¼å¿ç¸ç»åï¼ä»¥å®ç°æ´å¼ºå¤§çç³»ç»ã
*   <strong>å¯¹æ¨çè·¯å¾çæ´ç»ç²åº¦åæï¼</strong> æ·±å¥ç ç©¶å¤å³æ¨¡åå¦ä½ç²¾ç¡®å°è¯å«åçº æ­£èç¨¿è·¯å¾ä¸­çéè¯¯ï¼ä»¥è¿ä¸æ­¥ä¼åå¶åææºå¶ã</p>
<p>æ»èè¨ä¹ï¼è¿ç¯è®ºææåºäºä¸ç§æ°é¢ä¸é«æçæ è®­ç»æ¡æ¶SVï¼éè¿ç»åè½»éçº§èç¨¿ä¸å®¶çå¤æ ·åæ¨çè·¯å¾åå¼ºå¤§å¤å³æ¨¡åçåæè½åï¼æ¾èæåäºä¿¡æ¯å¯éåè§è§æ¨çä»»å¡çæ§è½ï¼åæ¶å®ç°äºéè¯¯çº æ­£åææ¬æçã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose Speculative Verdict (SV), a training-free
framework inspired by speculative decoding that combines multiple lightweight
draft experts with a large verdict model.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.20812v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.20812v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.20794v1'></a></p>
<h2 id="radar-camera-fused-multi-object-tracking-online-calibration-and-common-feature"><a href="https://arxiv.org/abs/2510.20794v1">Radar-Camera Fused Multi-Object Tracking: Online Calibration and Common Feature</a></h2>
<p><strong>Authors:</strong> Lei Cheng, Siyang Cao</p>
<p><strong>Published:</strong> 2025-10-23</p>
<p><strong>Categories:</strong> cs.CV, eess.SP</p>
<p><strong>Abstract:</strong></p>
<p>This paper presents a Multi-Object Tracking (MOT) framework that fuses radar
and camera data to enhance tracking efficiency while minimizing manual
interventions. Contrary to many studies that underutilize radar and assign it a
supplementary role--despite its capability to provide accurate range/depth
information of targets in a world 3D coordinate system--our approach positions
radar in a crucial role. Meanwhile, this paper utilizes common features to
enable online calibration to autonomously associate detections from radar and
camera. The main contributions of this work include: (1) the development of a
radar-camera fusion MOT framework that exploits online radar-camera calibration
to simplify the integration of detection results from these two sensors, (2)
the utilization of common features between radar and camera data to accurately
derive real-world positions of detected objects, and (3) the adoption of
feature matching and category-consistency checking to surpass the limitations
of mere position matching in enhancing sensor association accuracy. To the best
of our knowledge, we are the first to investigate the integration of
radar-camera common features and their use in online calibration for achieving
MOT. The efficacy of our framework is demonstrated by its ability to streamline
the radar-camera mapping process and improve tracking precision, as evidenced
by real-world experiments conducted in both controlled environments and actual
traffic scenarios. Code is available at
https://github.com/radar-lab/Radar_Camera_MOT</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Lei ChengåSiyang Caoæ°åçè®ºæâRadar-Camera Fused Multi-Object Tracking: Online Calibration and Common Featureâçå¨é¢æè¦ã</p>
<hr />
<h3 id="-">è®ºææè¦ï¼é·è¾¾-ç¸æºèåå¤ç®æ è·è¸ªï¼å¨çº¿æ ¡åä¸å±åç¹å¾</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³ä¼ ç»å¤ç®æ è·è¸ªï¼MOTï¼æ¡æ¶ä¸­é·è¾¾åç¸æºæ°æ®èåçå±éæ§ï¼ç¹å«æ¯é·è¾¾æ°æ®æªè¢«ååå©ç¨ãä¼ æå¨æ ¡åè¿ç¨éè¦å¤§éæå¨å¹²é¢ãä»¥åä»åºäºä½ç½®å¹éçä¼ æå¨å³èåç¡®æ§ä¸è¶³ç­é®é¢ãæ ¸å¿ææå¨äºå¦ä½å¼åä¸ä¸ªè½å¤ææèåé·è¾¾åç¸æºæ°æ®ãå®ç°èªå¨å¨çº¿æ ¡åãå¹¶æé«è·è¸ªç²¾åº¦åé²æ£æ§çMOTæ¡æ¶ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
è¯¥è®ºææåºäºä»¥ä¸å³é®åæ°ï¼
*   <strong>é·è¾¾å¨MOTä¸­çæ ¸å¿ä½ç¨ï¼</strong> ä¸è®¸å¤å°é·è¾¾è§ä¸ºè¾å©ä¼ æå¨çç ç©¶ä¸åï¼æ¬æå°é·è¾¾ç½®äºæ ¸å¿å°ä½ï¼å©ç¨å¶æä¾ç²¾ç¡®çè·ç¦»/æ·±åº¦ä¿¡æ¯ã
*   <strong>å¨çº¿é·è¾¾-ç¸æºæ ¡åæ¡æ¶ï¼</strong> æåºäºä¸ç§å©ç¨é·è¾¾åç¸æºæ°æ®ä¹é´å±åç¹å¾çå¨çº¿ãæ ç®æ æ ¡åæ¹æ³ãè¿ç®åäºä¼ æå¨éæï¼é¿åäºæå¨æµéä¼ æå¨å®è£ä½ç½®åè§åº¦æç§»å¨æ ¡åç®æ ãéè¿å°ç¸æºæ£æµç»æç´æ¥æå½±å°é·è¾¾çè·ç¦»-æ¹ä½ï¼RAï¼å¹³é¢ï¼ç»è¿äºæåºéçç¸æºBEVï¼é¸ç°å¾ï¼åæ¢ã
*   <strong>å±åç¹å¾çå©ç¨ï¼</strong> å¼å¥äºâå±åç¹å¾å¤å«å¨âï¼Common Feature Discriminatorï¼ï¼è¿æ¯ä¸ä¸ªåºäºæ·±åº¦å­¦ä¹ çæ¨¡åï¼ç¨äºå­¦ä¹ åæåé·è¾¾RADæ°æ®åç¸æºå¾åä¹é´å±äº«çç¹å¾ãè¿äºå±åç¹å¾ç¨äºå®ç°èªå¨å¨çº¿æ ¡ååæ´åç¡®çä¼ æå¨å³èã
*   <strong>å¤é¶æ®µå¹éç­ç¥ï¼</strong> èåäºç¹å¾å¹éåç±»å«ä¸è´æ§æ£æ¥ï¼ä»¥æé«ä¼ æå¨å³èçåç¡®æ§ï¼è¶è¶äºåçº¯çä½ç½®å¹éãè¿å¢å¼ºäºè·¨å¸§å¯¹è±¡å³èçé²æ£æ§ã
*   <strong>ä¸ä¸åç¦»æ ¡åæ¹æ³ï¼</strong> éå¯¹è¿è·ç¦»åè¿è·ç¦»ç®æ æµéç²¾åº¦å·®å¼çé®é¢ï¼å°ä¼ æå¨è§éåä¸ºä¸ä¸ä¸¤é¨åï¼å¹¶å¯¹æ¯ä¸ªåºåè¿è¡ç¬ç«æ ¡åï¼ä»¥æé«æ´ä½åç¡®æ§ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
è¯¥æ¡æ¶å¨åæ§ç¯å¢åå®éäº¤éåºæ¯ä¸­ççå®ä¸çå®éªä¸­å¾å°äºéªè¯ã
*   <strong>è·è¸ªæçåç²¾åº¦æåï¼</strong> èåæ¡æ¶å¨å¤ç®æ è·è¸ªæ§è½ä¸ä¼äºç¬ç«çç¸æºæé·è¾¾è·è¸ªå¨ãç¹å«æ¯å¨åæ§å®éªä¸­ï¼ä¼ æå¨èåè·è¸ªå¨å®ç°äºæä½çè¯¯æ¥çï¼FNRï¼ï¼æ¾èä¼äºç¬ç«ç³»ç»ï¼é·è¾¾è·è¸ªå¨FNRé«è¾¾21.91%ï¼ã
*   <strong>é²æ£æ§å¢å¼ºï¼</strong> èåæ¹æ³å¨æ¶å£å¤©æ°æ¡ä»¶ï¼å¦é´å¤©ãå°é¨ï¼ä¸è¡¨ç°åºå¼ºå¤§çé²æ£æ§ï¼é·è¾¾æ£æµæ§è½åå¤©æ°å½±åæå°ã
*   <strong>å¤çä¼ æå¨æéï¼</strong> ä¼ æå¨èåè·è¸ªå¨è½å¤ææå¤çåä¸ªä¼ æå¨æéçæåµï¼ç¡®ä¿å¨ç¸æºæé·è¾¾æ°æ®ç¼ºå¤±æ¶ä»è½æç»­è·è¸ªå¯¹è±¡ï¼ä»èæé«äºç³»ç»çå¯é æ§ã
*   <strong>MOTAåMOTPçå¹³è¡¡ï¼</strong> èåè·è¸ªå¨å¨MOTAï¼å¤ç®æ è·è¸ªåç¡®æ§ï¼åMOTPï¼å¤ç®æ è·è¸ªç²¾åº¦ï¼ä¹é´å®ç°äºå¹³è¡¡ï¼ç»åäºç¸æºçé«åè¾¨çåé·è¾¾çç²¾ç¡®è·ç¦»æµéä¼å¿ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>å±åç¹å¾çæ§è´¨ä¸æç¡®ï¼</strong> å°½ç®¡å¼åäºå±åç¹å¾å¤å«å¨ï¼ä½è¿äºå­¦ä¹ å°çå±åç¹å¾çæ§è´¨ä»ä¸æ¸æ¥ï¼éè¦è¿ä¸æ­¥çå¯è§ååè§£éæ§ç ç©¶ã
*   <strong>å¯¹æ ¡åç²¾åº¦çä¾èµï¼</strong> ç¸æºæ£æµççå®ä¸çä½ç½®æ¯éè¿å°å¾åç¹æå½±å°é·è¾¾åæ ç³»æ¥ç¡®å®çï¼å æ­¤æ ¡åè¯¯å·®å¯è½ç´æ¥å½±åç¸æºæ£æµçå®ä½ç²¾åº¦ã
*   <strong>é·è¾¾æ°æ®å±éæ§ï¼</strong> é·è¾¾RADæ°æ®è½ç¶ä¿¡æ¯ä¸°å¯ï¼ä½ç¼ºä¹äººç±»å¯è¯»æ§ï¼è§åè¾¨çæéï¼å¨é¢ç¹é®æ¡æ¡ä»¶ä¸ï¼ç¹å«æ¯å¯¹ç´§å¯é´éçç®æ ï¼å®¹æå¯¼è´æ¼æ£ã
*   <strong>è®¡ç®è´è·åè¿è¡æ¶æçï¼</strong> RadarYOLOéè¦éå¯¹æ¯ä¸ªæ°çé·è¾¾éç½®è¿è¡åè®­ç»ï¼ä¸RADå¼ éçå¤§å°ºå¯¸å¸¦æ¥äºæ¾èçè®¡ç®è´è·ãå½åå®ç°å¸§ççº¦ä¸º19 Hzï¼æªè½æ»¡è¶³å®æ¶è¦æ±ï¼éè¦è¿ä¸æ­¥ä¼åã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
æªæ¥çå·¥ä½å°éä¸­äºï¼
*   <strong>è§£éå­¦ä¹ å°çç¹å¾ï¼</strong> æ·±å¥ç ç©¶åè§£éå±åç¹å¾çæ§è´¨ï¼ä»¥æé«æ¨¡åçå¯è§£éæ§ã
*   <strong>èªéåºæ ¡åï¼</strong> å¼åæ´æºè½çèªéåºæ ¡åæºå¶ã
*   <strong>ä¼åå®æ¶æ§è½ï¼</strong> éè¿è¿ä¸æ­¥ä¼åï¼ä¾å¦ï¼å¨C/C++ç¯å¢ä¸­éæ°å®ç°ç®¡éï¼å¹¶ç¨è½»éçº§æ¿ä»£æ¹æ¡æ¿æ¢éª¨å¹²ç½ç»ï¼æ¥æé«è¿è¡æ¶æçï¼ä»¥æ»¡è¶³å®æ¶åºç¨çéæ±ã</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>Contrary to many studies that underutilize radar and assign it a
supplementary role--despite its capability to provide accurate range/depth
information of targets in a world 3D coordinate system--our approach positions
radar in a crucial role.</li>
<li>The main contributions of this work include: (1) the development of a
radar-camera fusion MOT framework that exploits online radar-camera calibration
to simplify the integration of detection results from these two sensors, (2)
the utilization of common features between radar and camera data to accurately
derive real-world positions of detected objects, and (3) the adoption of
feature matching and category-consistency checking to surpass the limitations
of mere position matching in enhancing sensor association accuracy.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.20794v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.20794v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.20776v1'></a></p>
<h2 id="cupid-pose-grounded-generative-3d-reconstruction-from-a-single-image"><a href="https://arxiv.org/abs/2510.20776v1">CUPID: Pose-Grounded Generative 3D Reconstruction from a Single Image</a></h2>
<p><strong>Authors:</strong> Binbin Huang, Haobin Duan, Yiqun Zhao, Zibo Zhao, Yi Ma, Shenghua Gao</p>
<p><strong>Published:</strong> 2025-10-23</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>This work proposes a new generation-based 3D reconstruction method, named
Cupid, that accurately infers the camera pose, 3D shape, and texture of an
object from a single 2D image. Cupid casts 3D reconstruction as a conditional
sampling process from a learned distribution of 3D objects, and it jointly
generates voxels and pixel-voxel correspondences, enabling robust pose and
shape estimation under a unified generative framework. By representing both
input camera poses and 3D shape as a distribution in a shared 3D latent space,
Cupid adopts a two-stage flow matching pipeline: (1) a coarse stage that
produces initial 3D geometry with associated 2D projections for pose recovery;
and (2) a refinement stage that integrates pose-aligned image features to
enhance structural fidelity and appearance details. Extensive experiments
demonstrate Cupid outperforms leading 3D reconstruction methods with an over 3
dB PSNR gain and an over 10% Chamfer Distance reduction, while matching
monocular estimators on pose accuracy and delivering superior visual fidelity
over baseline 3D generative models. For an immersive view of the 3D results
generated by Cupid, please visit cupid3d.github.io.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°å¯¹è¿ç¯è®ºææè¦è¿è¡è¯¦ç»åæã</p>
<hr />
<p><strong>è®ºææè¦åæï¼CUPID: Pose-Grounded Generative 3D Reconstruction from a Single Image</strong></p>
<p><strong>1. è®ºæä¸»è¦è´¡ç®çç®æ´æ»ç» (2-3 å¥è¯)</strong></p>
<p>CUPID æåºäºä¸ç§æ°é¢çåºäºçæå¼æ¨¡åç 3D éå»ºæ¹æ³ï¼è½å¤ä»åå¼  2D å¾åä¸­åç¡®æ¨æ­åºç©ä½çç¸æºå§¿æã3D å½¢ç¶åçº¹çãå®å° 3D éå»ºè§ä¸ºä»å­¦ä¹ å°ç 3D å¯¹è±¡åå¸ä¸­è¿è¡æ¡ä»¶éæ ·ï¼å¹¶éè¿èåçæä½ç´ ååç´ -ä½ç´ å¯¹åºå³ç³»ï¼å¨ä¸ä¸ªç»ä¸ççææ¡æ¶ä¸å®ç°äºé²æ£çå§¿æåå½¢ç¶ä¼°è®¡ãè¯¥æ¹æ³å¨æ§è½ä¸æ¾èè¶è¶äºç°æé¢åç 3D éå»ºæ¹æ³ï¼å¹¶å¨å§¿æç²¾åº¦åè§è§ä¿çåº¦æ¹é¢è¡¨ç°åºè²ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³å­¦æ¹æ³</strong></p>
<p>CUPID çå³é®åæ°å¨äºå¶å° 3D éå»ºé®é¢éæ°å®ä¹ä¸ºä»å­¦ä¹ å°ç 3D å¯¹è±¡åå¸ä¸­è¿è¡æ¡ä»¶éæ ·ï¼å¹¶å¼å¥äºä¸ä¸ªç¬ç¹çä¸¤é¶æ®µæµå¹éï¼flow matchingï¼ç®¡éï¼</p>
<ul>
<li><strong>ç»ä¸ççææ¡æ¶ä¸èåçæï¼</strong> CUPID å° 3D éå»ºï¼åæ¬å§¿æãå½¢ç¶åçº¹çï¼æ´åå°ä¸ä¸ªç»ä¸ççææ¡æ¶ä¸­ãå®ä¸ä»ä»çæ 3D å½¢ç¶ï¼è¿åæ¶çæåç´ -ä½ç´ å¯¹åºå³ç³»ï¼è¿å¯¹äºå¨çæè¿ç¨ä¸­å»ºç« 2D å¾åç¹å¾ä¸ 3D å ä½ä¹é´çèç³»è³å³éè¦ï¼ä»èå®ç°é²æ£çå§¿æåå½¢ç¶ä¼°è®¡ã</li>
<li><strong>å±äº« 3D æ½å¨ç©ºé´ä¸­çåå¸è¡¨ç¤ºï¼</strong> è¾å¥ç¸æºå§¿æå 3D å½¢ç¶é½è¢«è¡¨ç¤ºä¸ºå±äº« 3D æ½å¨ç©ºé´ä¸­çåå¸ï¼è¿ä½¿å¾æ¨¡åè½å¤å­¦ä¹ å°å§¿æåå½¢ç¶ä¹é´çåå¨å³èï¼å¹¶ä¿è¿äºçæè¿ç¨ä¸­çååä¼åã</li>
<li><strong>ä¸¤é¶æ®µæµå¹éç®¡éï¼</strong><ul>
<li><strong>ç²ç¥é¶æ®µ (Coarse Stage)ï¼</strong> æ¨å¨çæåå§ç 3D å ä½ç»æåå¶å³èç 2D æå½±ï¼ä¸»è¦ç¨äºå§¿ææ¢å¤ãè¿è§£å³äºä¼ ç»åç® 3D éå»ºä¸­å§¿æä¼°è®¡çææï¼ä¸ºåç»­çç²¾ç»åæä¾äºè¯å¥½çèµ·ç¹ã</li>
<li><strong>ç²¾ç»åé¶æ®µ (Refinement Stage)ï¼</strong> å¨ç²ç¥é¶æ®µæ¢å¤çå§¿æåºç¡ä¸ï¼æ´åå§¿æå¯¹é½çå¾åç¹å¾ï¼ä»¥å¢å¼º 3D ç»æçä¿çåº¦åå¤è§ç»èãè¿ç§ç±ç²å°ç²¾çç­ç¥æ¯è®¸å¤å¤æçæä»»å¡çæææ¹æ³ï¼ç¡®ä¿äºæç»ç»æçè´¨éã</li>
</ul>
</li>
<li><strong>æµå¹é (Flow Matching)ï¼</strong> æè¦ä¸­æå°âtwo-stage flow matching pipelineâï¼è¿æç¤ºäºæ¨¡åå¯è½å©ç¨äºæè¿å¨çææ¨¡åé¢åå´èµ·çæµå¹éææ¯ãæµå¹éæ¯ä¸ç§æ¿ä»£æ©æ£æ¨¡åçæ°åçæèå¼ï¼å®éè¿å­¦ä¹ ä¸ä¸ªè¿ç»­çåéåºï¼flowï¼æ¥å°ç®åçåªå£°åå¸æ å°å°å¤æçæ°æ®åå¸ï¼éå¸¸å¨è®­ç»æçåæ ·æ¬è´¨éä¸å·æä¼å¿ãå°å¶åºç¨äº 3D éå»ºï¼ç¹å«æ¯ç»åå§¿æä¿¡æ¯ï¼æ¯ä¸ä¸ªæ°é¢ä¸æåæ¯çæ¹åã</li>
</ul>
<p><strong>3. å¯¹è¯¥é¢åçæ½å¨å½±å</strong></p>
<ul>
<li><strong>æååç® 3D éå»ºçæ§è½ä¸éï¼</strong> CUPID å¨ PSNR å Chamfer Distance ä¸çæ¾èæåè¡¨æå®å¨å®éææ ä¸è¶è¶äºç°æé¢åæ¹æ³ï¼è¿å¯è½ä¸ºåç® 3D éå»ºè®¾å®æ°çåºåã</li>
<li><strong>ç»ä¸å§¿æä¸å½¢ç¶ä¼°è®¡ï¼</strong> å°ç¸æºå§¿æã3D å½¢ç¶åçº¹çä¼°è®¡æ´åå°ä¸ä¸ªçææ¡æ¶ä¸­ï¼ç®åäºä¼ ç»ä¸éè¦å¤ä¸ªç¬ç«æ¨¡åæå¤æç®¡éçæµç¨ï¼å¯è½ä¸ºæªæ¥ç 3D éå»ºç³»ç»è®¾è®¡æä¾æ°çæè·¯ã</li>
<li><strong>æ¨å¨çæå¼ 3D æ¨¡åçåå±ï¼</strong> ç»åæµå¹éåå±äº«æ½å¨ç©ºé´çæ¦å¿µï¼CUPID å±ç¤ºäºçæå¼æ¨¡åå¨å¤çå¤æ 3D å ä½åå§¿ææ¨çæ¹é¢çå¼ºå¤§æ½åï¼å¯è½ä¼æ¿åæ´å¤åºäºçæèå¼ç 3D è§è§ç ç©¶ã</li>
<li><strong>æ´å¹¿æ³çåºç¨åæ¯ï¼</strong> æ´åç¡®ãæ´é²æ£çåç® 3D éå»ºå°å é AR/VRãæºå¨äººãèªå¨é©¾é©¶ãåå®¹åä½ç­é¢åçåå±ã</li>
</ul>
<p><strong>4. å¯è½åçäºè¿é¡¹ç ç©¶çç¸å³é¢åæåºç¨</strong></p>
<ul>
<li><strong>å¢å¼ºç°å® (AR) åèæç°å® (VR)ï¼</strong> å®æ¶ãé«è´¨éçåç® 3D éå»ºæ¯ AR/VR åºæ¯çè§£ååå®¹äº¤äºçåºç¡ï¼CUPID å¯ä»¥æ¾èæåç¨æ·ä½éªã</li>
<li><strong>æºå¨äººå­¦åèªä¸»å¯¼èªï¼</strong> æºå¨äººéè¦åç¡®æç¥å¨å´ç¯å¢ç 3D ç»æåç©ä½å§¿æï¼ä»¥ä¾¿è¿è¡è·¯å¾è§åãæååäº¤äºã</li>
<li><strong>èªå¨é©¾é©¶ï¼</strong> ä»è½¦è½½æåå¤´å¾åä¸­å¿«éåç¡®å°éå»ºéè·¯ãè½¦è¾åè¡äººç­ 3D ä¿¡æ¯ï¼å¯¹äºç¯å¢æç¥åå³ç­è³å³éè¦ã</li>
<li><strong>3D åå®¹åä½åæ¸¸æå¼åï¼</strong> èºæ¯å®¶åå¼åèå¯ä»¥å©ç¨åå¼ å¾åå¿«éçæé«è´¨éç 3D æ¨¡åï¼å¤§å¤§æé«å·¥ä½æçã</li>
<li><strong>æ°å­äººä¸èæè¯ç©¿ï¼</strong> ä»åå¼ ç§çéå»ºäººä½ 3D æ¨¡åï¼å¯ç¨äºèæè¯ç©¿ãæ°å­æ¿èº«ç­åºç¨ã</li>
<li><strong>æåéäº§æ°å­åï¼</strong> å¿«éãç»æµå°å¯¹æç©è¿è¡ 3D æ«æåéå»ºã</li>
</ul>
<p><strong>5. ä»æè¦ä¸­å¯ä»¥æ¨æ­åºçä»»ä½å±éæ§</strong></p>
<ul>
<li><strong>è®¡ç®èµæºéæ±ï¼</strong> ä½ä¸ºä¸ä¸ªåºäºçæå¼æ¨¡åï¼ç¹å«æ¯æµå¹éï¼çæ¹æ³ï¼å¹¶ä¸æ¶åä½ç´ è¡¨ç¤ºï¼CUPID å¨è®­ç»åæ¨çé¶æ®µå¯è½éè¦å¤§éçè®¡ç®èµæºï¼GPU åå­åè®¡ç®è½åï¼ãæè¦ä¸­æªæåæ¨çéåº¦ï¼è¿å¯è½æ¯ä¸ä¸ªæ½å¨çç¶é¢ã</li>
<li><strong>æ³åè½åï¼</strong> æè¦ä¸­æªæç¡®è¯´ææ¨¡åæ¯å¨ä½ç§æ°æ®éä¸è¿è¡è®­ç»åè¯ä¼°çãå¶å¨è®­ç»æ°æ®åå¸ä¹å¤çç©ä½ç±»å«ãåç§æ¡ä»¶ãçº¹çå¤ææ§æé®æ¡æåµä¸çæ³åè½åä»éè¿ä¸æ­¥éªè¯ã</li>
<li><strong>ä½ç´ è¡¨ç¤ºçéå¶ï¼</strong> ä½ç´ è¡¨ç¤ºè½ç¶å¨æäºæ¹é¢å·æä¼å¿ï¼ä½å¶åè¾¨çåéäºåå­åè®¡ç®ææ¬ï¼å¯è½é¾ä»¥æææç²¾ç»çå ä½ç»èï¼æèå¨è¡¨ç¤ºå¤§å°ºåº¦åºæ¯æ¶æçä¸é«ãè½ç¶æè¦æå°âå¢å¼ºç»æä¿çåº¦âï¼ä½ä½ç´ åºæçç¦»æ£æ§ä»å¯è½æ¯ä¸ä¸ªéå¶ã</li>
<li><strong>âPose-Groundedâçå«ä¹ï¼</strong> æè¦å¼ºè°äºâPose-Groundedâï¼ä½å·ä½å¦ä½å°å§¿æä¿¡æ¯âæ¥å°âå°çæè¿ç¨ä¸­ï¼ä»¥åè¿ç§æ¥å°æ¹å¼å¯¹å§¿æä¼°è®¡çé²æ£æ§ååç¡®æ§æå¤å¤§è´¡ç®ï¼éè¦éè¯»æ­£ææè½æ·±å¥çè§£ãä¾å¦ï¼æ¯å¦éè¦é¢è®­ç»çå§¿æä¼°è®¡å¨ï¼æèå§¿ææ¯å¦å®å¨ç±çææ¨¡åç«¯å°ç«¯å­¦ä¹ ã</li>
<li><strong>åå¼ å¾åçåºææ¨¡ç³æ§ï¼</strong> å°½ç®¡ CUPID åå¾äºæ¾èè¿æ­¥ï¼ä½ä»åå¼  2D å¾åéå»º 3D ä¿¡æ¯æ¬è´¨ä¸æ¯ä¸ä¸ªçæé®é¢ï¼å­å¨æ·±åº¦æ¨¡ç³æ§ãæ¨¡åå¦ä½ææå°è§£å³æç¼è§£è¿ç§æ¨¡ç³æ§ï¼ä»¥åå¨æç«¯è§è§æä¿¡æ¯ç¼ºå¤±æåµä¸çè¡¨ç°ï¼æ¯å¼å¾å³æ³¨çã</li>
<li><strong>âlearned distribution of 3D objectsâï¼</strong> è¿ç§åå¸çè´¨éåè¦çèå´ç´æ¥å½±åæ¨¡åççæè½åãå¦æè®­ç»æ°æ®ä¸è¶³æå¤æ ·æ§ä¸å¤ï¼æ¨¡åå¯è½æ æ³çæé«è´¨éæå¤æ ·åç 3D å¯¹è±¡ã</li>
</ul>
<hr />
<p>æ»èè¨ä¹ï¼CUPID æåºäºä¸ç§ä»¤äººå´å¥çæ°æ¹æ³ï¼å°çæå¼æ¨¡ååæµå¹éææ¯å¼å¥åç® 3D éå»ºé¢åï¼å¹¶åå¾äºæ¾èçæ§è½æåãå¶ç»ä¸çæ¡æ¶åä¸¤é¶æ®µç­ç¥ä¸ºè§£å³è¿ä¸å¤æé®é¢æä¾äºæ°çè§è§ï¼ææå¨å¤ä¸ªåºç¨é¢åäº§çéè¦å½±åã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>This work proposes a new generation-based 3D reconstruction method, named
Cupid, that accurately infers the camera pose, 3D shape, and texture of an
object from a single 2D image.</li>
<li>Extensive experiments
demonstrate Cupid outperforms leading 3D reconstruction methods with an over 3
dB PSNR gain and an over 10% Chamfer Distance reduction, while matching
monocular estimators on pose accuracy and delivering superior visual fidelity
over baseline 3D generative models.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.20776v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.20776v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.20771v1'></a></p>
<h2 id="alphaflow-understanding-and-improving-meanflow-models"><a href="https://arxiv.org/abs/2510.20771v1">AlphaFlow: Understanding and Improving MeanFlow Models</a></h2>
<p><strong>Authors:</strong> Huijie Zhang, Aliaksandr Siarohin, Willi Menapace, Michael Vasilkovsky, Sergey Tulyakov, Qing Qu, Ivan Skorokhodov</p>
<p><strong>Published:</strong> 2025-10-23</p>
<p><strong>Categories:</strong> cs.CV, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>MeanFlow has recently emerged as a powerful framework for few-step generative
modeling trained from scratch, but its success is not yet fully understood. In
this work, we show that the MeanFlow objective naturally decomposes into two
parts: trajectory flow matching and trajectory consistency. Through gradient
analysis, we find that these terms are strongly negatively correlated, causing
optimization conflict and slow convergence. Motivated by these insights, we
introduce <script type="math/tex">\alpha</script>-Flow, a broad family of objectives that unifies trajectory
flow matching, Shortcut Model, and MeanFlow under one formulation. By adopting
a curriculum strategy that smoothly anneals from trajectory flow matching to
MeanFlow, <script type="math/tex">\alpha</script>-Flow disentangles the conflicting objectives, and achieves
better convergence. When trained from scratch on class-conditional ImageNet-1K
256x256 with vanilla DiT backbones, <script type="math/tex">\alpha</script>-Flow consistently outperforms
MeanFlow across scales and settings. Our largest <script type="math/tex">\alpha</script>-Flow-XL/2+ model
achieves new state-of-the-art results using vanilla DiT backbones, with FID
scores of 2.58 (1-NFE) and 2.15 (2-NFE).</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°å¯¹è¿ç¯è®ºææè¦è¿è¡æ·±å¥åæã</p>
<hr />
<p><strong>è®ºææè¦åæï¼AlphaFlow: Understanding and Improving MeanFlow Models</strong></p>
<p><strong>1. è®ºæä¸»è¦è´¡ç®çç®æ´æ»ç» (2-3 å¥è¯)</strong></p>
<p>è¿ç¯è®ºææ·±å¥åæäºæ°å´ç MeanFlow æ¨¡åçä¼åæºå¶ï¼åç°å¶ç®æ å½æ°åé¨å­å¨è½¨è¿¹æµå¹éåè½¨è¿¹ä¸è´æ§è¿ä¸¤ä¸ªç¸äºå²çªçä¼åé¡¹ï¼å¯¼è´æ¶æç¼æ¢ãå¨æ­¤åºç¡ä¸ï¼ä½èæåºäº <script type="math/tex">\alpha</script>-Flowï¼ä¸ä¸ªç»ä¸äºå¤ç§ç°ææ¹æ³çå¹¿ä¹ç®æ å½æ°å®¶æï¼å¹¶éè¿è¯¾ç¨å­¦ä¹ ç­ç¥ææè§£è¦äºè¿äºå²çªï¼æ¾èæåäº MeanFlow æ¨¡åçæ¶æéåº¦åçææ§è½ï¼å¨ ImageNet-1K ä¸åå¾äºæ°ç SOTA ç»æã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®º</strong></p>
<ul>
<li><strong>MeanFlow ç®æ å½æ°çåè§£ä¸å²çªåæï¼</strong> è®ºæçæ ¸å¿åæ°å¨äºé¦æ¬¡å° MeanFlow ç®æ å½æ°åè§£ä¸ºâè½¨è¿¹æµå¹éâåâè½¨è¿¹ä¸è´æ§âä¸¤é¨åï¼å¹¶éè¿æ¢¯åº¦åææ­ç¤ºäºè¿ä¸¤é¨åä¹é´å¼ºççè´ç¸å³æ§ï¼ä»èè§£éäº MeanFlow ä¼åå°é¾åæ¶æç¼æ¢çæ ¹æ¬åå ã</li>
<li><strong><script type="math/tex">\alpha</script>-Flow ç»ä¸æ¡æ¶çæåºï¼</strong> åºäºå¯¹å²çªççè§£ï¼è®ºææåºäº <script type="math/tex">\alpha</script>-Flowï¼è¿æ¯ä¸ä¸ªæ´å¹¿ä¹çæ¡æ¶ï¼å®å°è½¨è¿¹æµå¹éãShortcut Model å MeanFlow ç»ä¸å¨ä¸ä¸ªå¬å¼ä¸ï¼ä¸ºçè§£åæ¹è¿è¿äºæ¨¡åæä¾äºæ°çè§è§ã</li>
<li><strong>è¯¾ç¨å­¦ä¹ ç­ç¥çåºç¨ï¼</strong> <script type="math/tex">\alpha</script>-Flow éç¨äºä¸ç§åæ°çè¯¾ç¨å­¦ä¹ ç­ç¥ï¼éè¿å¹³æ»å°ä»è½¨è¿¹æµå¹éè¿æ¸¡å° MeanFlowï¼éæ­¥è§£è¦äºå²çªçä¼åç®æ ï¼ä»èå®ç°äºæ´å¿«çæ¶æåæ´å¥½çæ§è½ãè¿è¡¨æäºå¨å¤æä¼åé®é¢ä¸­ï¼åé¶æ®µææ¸è¿å¼å­¦ä¹ çéè¦æ§ã</li>
</ul>
<p><strong>3. å¯¹é¢åæ½å¨å½±å</strong></p>
<ul>
<li><strong>æ¨å¨å°æ­¥çææ¨¡åçåå±ï¼</strong> MeanFlow ä½ä¸ºä¸ç§æ°å´çå°æ­¥çææ¨¡åï¼å¶ä¼åæºå¶çæ·±å¥çè§£åæ¹è¿å°æå¤§å°å éè¯¥é¢åçç ç©¶è¿å±ã<script type="math/tex">\alpha</script>-Flow çæåºä¸ºè®¾è®¡æ´é«æãæ´ç¨³å®çå°æ­¥çææ¨¡åæä¾äºæ°çèå¼åå·¥å·ã</li>
<li><strong>ä¼åçè®ºçå¯åï¼</strong> è®ºææ­ç¤ºçä¼åç®æ åé¨å²çªåå¶è§£è¦æ¹æ³ï¼å¯¹å¶ä»å·æå¤ç®æ æå¤æç®æ å½æ°çæºå¨å­¦ä¹ æ¨¡åä¼åä¹å·æåé´æä¹ï¼å¯è½å¯åæ°çä¼åç­ç¥ã</li>
<li><strong>æåçææ¨¡åæ§è½åºçº¿ï¼</strong> å¨ ImageNet-1K 256x256 ä¸åå¾ç SOTA FID åæ°ï¼1-NFE 2.58ï¼2-NFE 2.15ï¼è¡¨æ <script type="math/tex">\alpha</script>-Flow å¨çæè´¨éåæçä¸é½è¾¾å°äºæ°çé«åº¦ï¼ä¸ºåç»­ç ç©¶è®¾å®äºæ´é«çåºåã</li>
<li><strong>ä¿è¿å¯¹çææ¨¡ååçççè§£ï¼</strong> éè¿å¯¹ MeanFlow åé¨æºå¶çåæï¼è®ºæå æ·±äºæä»¬å¯¹è¿ç±»åºäºæµå¹éåä¸è´æ§åçççææ¨¡åå·¥ä½æ¹å¼ççè§£ï¼æå©äºæªæ¥æ´å·çè®ºåºç¡çæ¨¡åè®¾è®¡ã</li>
</ul>
<p><strong>4. ç¸å³é¢åæåºç¨å¯è½åç</strong></p>
<ul>
<li><strong>å¾åçæä¸ç¼è¾ï¼</strong> ä½ä¸ºç´æ¥çåºç¨ï¼<script type="math/tex">\alpha</script>-Flow å°æåå¾åçææ¨¡åçæçåè´¨éï¼å°¤å¶æ¯å¨éè¦å¿«éçæé«è´¨éå¾åçåºæ¯ï¼å¦åå®¹åä½ãèæç°å®ãæ¸¸æå¼åç­ã</li>
<li><strong>è§é¢çæï¼</strong> ç±»ä¼¼ MeanFlow çå°æ­¥çææ¡æ¶ä¹å¯è½åºç¨äºè§é¢çæï¼<script type="math/tex">\alpha</script>-Flow çä¼åç­ç¥ææå éè§é¢çææ¨¡åçè®­ç»åæ¨çã</li>
<li><strong>3D åå®¹çæï¼</strong> éçæ©æ£æ¨¡åå 3D é¢åæ©å±ï¼<script type="math/tex">\alpha</script>-Flow çåçåæ¹æ³ä¹å¯è½è¢«åé´å° 3D å½¢ç¶ãçº¹çæåºæ¯çå°æ­¥çæä¸­ã</li>
<li><strong>ç§å­¦è®¡ç®ä¸æ¨¡æï¼</strong> å¨éè¦ä»å¤ææ°æ®åå¸ä¸­å¿«ééæ ·æçææ°æ ·æ¬çç§å­¦è®¡ç®é¢åï¼ä¾å¦ææç§å­¦ãè¯ç©åç°ç­ï¼å°æ­¥çææ¨¡åçé«ææ§å°éå¸¸æä»·å¼ã</li>
<li><strong>å¯¹ææ§æ ·æ¬çæä¸é²å¾¡ï¼</strong> å¯¹çææ¨¡åä¼åæºå¶çæ·±å¥çè§£ï¼ä¹å¯è½é´æ¥å¸®å©æä»¬æ´å¥½å°çè§£ååºå¯¹å¯¹ææ§æ»å»ã</li>
</ul>
<p><strong>5. ä»æè¦ä¸­å¯æ¨æ­çå±éæ§</strong></p>
<ul>
<li><strong>çè®ºæ®éæ§å¾éªè¯ï¼</strong> å°½ç®¡ <script type="math/tex">\alpha</script>-Flow ç»ä¸äºå¤ç§æ¹æ³ï¼ä½å¶æåºçä¼åå²çªåè§£è¦ç­ç¥æ¯å¦è½æ®éäºææåºäºæµå¹éæä¸è´æ§åçççææ¨¡åï¼è¿éè¦æ´å¹¿æ³ççè®ºåå®éªéªè¯ã</li>
<li><strong>è®¡ç®ææ¬ï¼</strong> æè¦ä¸­æªæå <script type="math/tex">\alpha</script>-Flow ç¸è¾äº MeanFlow å¨è®­ç»ææ¨çæ¶çé¢å¤è®¡ç®ææ¬ãè½ç¶å®æé«äºæ¶æéåº¦ï¼ä½å¼å¥è¯¾ç¨å­¦ä¹ ç­ç¥æ¯å¦ä¼å¢å æ´ä½è®­ç»æ¶é´æåå­æ¶èï¼æ¯ä¸ä¸ªéè¦å³æ³¨çé®é¢ã</li>
<li><strong>ç¹å®éª¨å¹²ç½ç»ï¼</strong> è®ºææå°ä½¿ç¨âvanilla DiT backbonesâï¼è¿æå³çå¶æ§è½æ¯å¨ç¹å®æ¶æä¸éªè¯çã<script type="math/tex">\alpha</script>-Flow å¨å¶ä»ç±»åçéª¨å¹²ç½ç»ï¼å¦ U-NetãTransformer ç­ï¼ä¸çè¡¨ç°å¦ä½ï¼ä»éè¿ä¸æ­¥æ¢ç´¢ã</li>
<li><strong>âä»å¤´è®­ç»âçå«ä¹ï¼</strong> æè¦å¼ºè°âtrained from scratchâï¼è¿è¡¨ææ¨¡åæ²¡æä½¿ç¨é¢è®­ç»æéãè½ç¶è¿å±ç¤ºäºå¶å¼ºå¤§çä»é¶å¼å§å­¦ä¹ è½åï¼ä½å¦æç»åé¢è®­ç»ï¼æ§è½æ¯å¦è½è¿ä¸æ­¥æåï¼ä»¥åå¶å¨è¿ç§»å­¦ä¹ åºæ¯ä¸çè¡¨ç°ï¼æ¯å¼å¾æ¢è®¨çã</li>
<li><strong>ä»éäº ImageNet-1Kï¼</strong> å®éªç»æä¸»è¦å¨ ImageNet-1K 256x256 ä¸éªè¯ãå¨æ´é«åè¾¨çãæ´å¤æçæ°æ®éæä¸åæ¨¡æï¼å¦ææ¬ãé³é¢ï¼ä¸çè¡¨ç°ï¼ä»éè¿ä¸æ­¥è¯ä¼°ã</li>
</ul>
<hr />
<p>æ»èè¨ä¹ï¼è¿ç¯è®ºæå¨çè®ºåæåå®éææä¸é½åå¾äºæ¾èè¿å±ï¼å¯¹å°æ­¥çææ¨¡åé¢åå·æéè¦çæ¨å¨ä½ç¨ãå®ä¸ä»è§£å³äº MeanFlow ä¼åä¸­çä¸ä¸ªæ ¸å¿é¾é¢ï¼è¿æåºäºä¸ä¸ªæ´å·æ®éæ§çæ¡æ¶ï¼ä¸ºæªæ¥çææ¨¡åçè®¾è®¡åä¼åæä¾äºå®è´µçè§è§£ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In
this work, we show that the MeanFlow objective naturally decomposes into two
parts: trajectory flow matching and trajectory consistency.</li>
<li>When trained from scratch on class-conditional ImageNet-1K
256x256 with vanilla DiT backbones, <script type="math/tex">\alpha</script>-Flow consistently outperforms
MeanFlow across scales and settings.</li>
<li>Our largest <script type="math/tex">\alpha</script>-Flow-XL/2+ model
achieves new state-of-the-art results using vanilla DiT backbones, with FID
scores of 2.58 (1-NFE) and 2.15 (2-NFE).</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.20771v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.20771v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-10-24 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
