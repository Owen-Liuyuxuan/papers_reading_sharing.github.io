<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-09-05 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DEâ«¶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Birdâs-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">è®ºæéè¯»</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-Î±
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-09-04/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-09-07/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-09-05">Arxiv Computer Vision Papers - 2025-09-05</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#infradiffusion-zero-shot-depth-map-restoration-with-diffusion-models-and-prompted-segmentation-from-sparse-infrastructure-point-clouds" class="nav-link">InfraDiffusion: zero-shot depth map restoration with diffusion models and prompted segmentation from sparse infrastructure point clouds</a>
                </li>
                <li class="nav-item">
                    <a href="#visiofirm-cross-platform-ai-assisted-annotation-tool-for-computer-vision" class="nav-link">VisioFirm: Cross-Platform AI-assisted Annotation Tool for Computer Vision</a>
                </li>
                <li class="nav-item">
                    <a href="#luxdit-lighting-estimation-with-video-diffusion-transformer" class="nav-link">LuxDiT: Lighting Estimation with Video Diffusion Transformer</a>
                </li>
                <li class="nav-item">
                    <a href="#onecat-decoder-only-auto-regressive-model-for-unified-understanding-and-generation" class="nav-link">OneCAT: Decoder-Only Auto-Regressive Model for Unified Understanding and Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#a-generative-foundation-model-for-chest-radiography" class="nav-link">A Generative Foundation Model for Chest Radiography</a>
                </li>
                <li class="nav-item">
                    <a href="#ovgrasp-open-vocabulary-grasping-assistance-via-multimodal-intent-detection" class="nav-link">OVGrasp: Open-Vocabulary Grasping Assistance via Multimodal Intent Detection</a>
                </li>
                <li class="nav-item">
                    <a href="#enhancing-robustness-in-post-processing-watermarking-an-ensemble-attack-network-using-cnns-and-transformers" class="nav-link">Enhancing Robustness in Post-Processing Watermarking: An Ensemble Attack Network Using CNNs and Transformers</a>
                </li>
                <li class="nav-item">
                    <a href="#plotn-polish-zero-shot-story-visualization-and-disentangled-editing-with-text-to-image-diffusion-models" class="nav-link">Plot'n Polish: Zero-shot Story Visualization and Disentangled Editing with Text-to-Image Diffusion Models</a>
                </li>
                <li class="nav-item">
                    <a href="#differential-morphological-profile-neural-networks-for-semantic-segmentation" class="nav-link">Differential Morphological Profile Neural Networks for Semantic Segmentation</a>
                </li>
                <li class="nav-item">
                    <a href="#tinydrop-tiny-model-guided-token-dropping-for-vision-transformers" class="nav-link">TinyDrop: Tiny Model Guided Token Dropping for Vision Transformers</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-09-05">Arxiv Computer Vision Papers - 2025-09-05</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>å¥½çï¼è¿æ¯ä¸ä»½ä¸ºå¿ç¢çç ç©¶äººååå¤ç Arxiv è®¡ç®æºè§è§é¢åææ°è®ºææ§è¡æè¦ã</p>
<hr />
<p><strong>Arxiv è®¡ç®æºè§è§é¢åææ°è®ºææ§è¡æè¦ (2025-09-03)</strong></p>
<p>æ¬æ¥åæ¨å¨ä¸ºç ç©¶äººåæä¾å½æ¥ Arxiv è®¡ç®æºè§è§é¢åææ°åè¡¨è®ºæçå¿«éæ¦è§ï¼éç¹å³æ³¨ä¸»è¦è¶å¿ãåæ°äº®ç¹ãæ°å´æ¹ååæ¨èéè¯»ã</p>
<hr />
<p><strong>1. ä¸»è¦ä¸»é¢ä¸è¶å¿ (Main Themes &amp; Trends)</strong></p>
<p>ä»å¤©çè®ºæå±ç¤ºäºè®¡ç®æºè§è§é¢åå ä¸ªå³é®è¶å¿çæç»­æ·±ååäº¤åèåï¼</p>
<ul>
<li><strong>æ©æ£æ¨¡å (Diffusion Models) çå¹¿æ³åºç¨ä¸åæ°:</strong> æ©æ£æ¨¡åç»§ç»­å¨åç§ä»»å¡ä¸­å±ç°å¶å¼ºå¤§è½åï¼ä»ç¨ç3Dæ°æ®éå»ºï¼æ·±åº¦å¾æ¢å¤ï¼å°è§é¢åç§ä¼°è®¡ï¼åå°å¤æçææ¬å°å¾åæäºå¯è§ååç¼è¾ãè¿è¡¨ææ©æ£æ¨¡åæ­£æä¸ºå¤æ¨¡æçæåæç¥ä»»å¡çæ ¸å¿ææ¯ã</li>
<li><strong>åºç¡æ¨¡å (Foundation Models) çæ·±åä¸ä¸ä¸å:</strong> åºç°äºæ¨å¨å®ç°ç»ä¸çè§£ä¸çæçéç¨åºç¡æ¨¡åï¼ä»¥åéå¯¹ç¹å®é¢åï¼å¦å»å­¦å½±åï¼çä¸ä¸åçæå¼åºç¡æ¨¡åï¼é¢ç¤ºçæªæ¥AIç³»ç»å°æ´å éç¨æå¨ç¹å®é¢åè¾¾å°ä¸å®¶çº§æ°´å¹³ã</li>
<li><strong>è§è§ Transformer (Vision Transformers) çæçä¸é²æ£æ§ä¼å:</strong> éå¯¹ ViT çè®¡ç®æçé®é¢ï¼æç ç©¶æåºäºè½»éåç­ç¥ï¼å¦ token droppingï¼ï¼åæ¶ä¹æå·¥ä½å³æ³¨å¶å¨å¯¹ææ§æ»å»ï¼å¦æ°´å°ï¼ä¸çé²æ£æ§å¢å¼ºã</li>
<li><strong>å¤æ¨¡æä¸è·¨é¢åèå (Multimodality &amp; Cross-Domain Fusion):</strong> è®ºææ¶µçäºææ¬-å¾åãè§é¢-å¾åãç¹äº-å¾åç­å¤ç§æ¨¡æçèåï¼ä»¥åè®¡ç®æºè§è§ä¸æºå¨äººãå»çãåºç¡è®¾æ½ç­é¢åçæ·±åº¦ç»åã</li>
<li><strong>çæå¼ AI çç²¾ç»æ§å¶ä¸ç¼è¾ (Fine-grained Control &amp; Editing in Generative AI):</strong> ä¸åä»ä»æ¯çæå¾åï¼èæ¯è¿½æ±å¯¹çæåå®¹æ´æ·±å±æ¬¡ççè§£ãç¼è¾ååäºè½åã</li>
<li><strong>å®ç¨å·¥å·ä¸åºç¨ (Practical Tools &amp; Applications):</strong> åºç°äºæåæ æ³¨æççAIè¾å©å·¥å·ï¼ä»¥åé¢åå®éæºå¨äººæä½çå¼æ¾è¯æ±æåè¾å©ç³»ç»ã</li>
</ul>
<p><strong>2. æ¾èæåæ°æ§è®ºæäº®ç¹ (Significant or Innovative Papers)</strong></p>
<ul>
<li><strong>OneCAT: Decoder-Only Auto-Regressive Model for Unified Understanding and Generation (Han Li et al.)</strong>: è¿ç¯è®ºææåºäºä¸ä¸ªç»ä¸çè§£ç å¨-onlyèªåå½æ¨¡åï¼æ¨å¨åæ¶å®ç°çè§£åçæä»»å¡ãå¶æ½åå¨äºæå»ºæ´éç¨ãæ´å¼ºå¤§çAIç³»ç»ï¼æ¯è¿åéç¨æºè½çéè¦ä¸æ­¥ã</li>
<li><strong>A Generative Foundation Model for Chest Radiography (Yuanfeng Ji et al.)</strong>: å¨å»çå½±åé¢åå¼å¥çæå¼åºç¡æ¨¡åï¼ææå½»åºæ¹åè¸é¨Xåççåæãè¯æ­åæ°æ®å¢å¼ºæ¹å¼ï¼å¯¹å»çAIå·æéç¨ç¢æä¹ã</li>
<li><strong>InfraDiffusion: zero-shot depth map restoration with diffusion models and prompted segmentation from sparse infrastructure point clouds (Yixiong Jing et al.)</strong>: åæ°æ§å°å°æ©æ£æ¨¡ååºç¨äºä»ç¨çåºç¡è®¾æ½ç¹äºè¿è¡é¶æ ·æ¬æ·±åº¦å¾æ¢å¤ï¼è§£å³äº3Dè§è§ä¸­ä¸ä¸ªå·ææææ§çå®éé®é¢ã</li>
<li><strong>LuxDiT: Lighting Estimation with Video Diffusion Transformer (Ruofan Liang et al.)</strong>: ç»åäºè§é¢ãæ©æ£æ¨¡åå Transformer çå¼ºå¤§è½åï¼å®ç°äºé«è´¨éçåç§ä¼°è®¡ï¼å¯¹èæç°å®ãçµå½±å¶ä½åå¾åç¼è¾ç­é¢åå·æéè¦ä»·å¼ã</li>
<li><strong>Plot'n Polish: Zero-shot Story Visualization and Disentangled Editing with Text-to-Image Diffusion Models (Kiymet Akdemir et al.)</strong>: çªç ´äºç®åçææ¬å°å¾åçæï¼å®ç°äºæäºçº§å«çå¯è§ååè§£è¦ç¼è¾ï¼å±ç¤ºäºçæå¼AIå¨åæåå®¹çäº§æ¹é¢çå·¨å¤§æ½åã</li>
<li><strong>TinyDrop: Tiny Model Guided Token Dropping for Vision Transformers (Guoxin Wang et al.)</strong>: æåºäºä¸ç§é«æç ViT ä¼åç­ç¥ï¼éè¿å¾®å°æ¨¡åå¼å¯¼ç token dropping æ¥æåæ¨çéåº¦ï¼å¯¹ ViT çå®éé¨ç½²å·æéè¦æä¹ã</li>
</ul>
<p><strong>3. æ°å´ç ç©¶æ¹åæææ¯ (Emerging Research Directions or Techniques)</strong></p>
<ul>
<li><strong>ç»ä¸ççè§£ä¸çæèå¼ (Unified Understanding &amp; Generation Paradigm):</strong> ä»¥ OneCAT ä¸ºä»£è¡¨ï¼æ¢ç´¢å¦ä½ç¨åä¸æ¨¡åæ¶æå¤çå¤æ¨¡æçæç¥ä¸çæä»»å¡ã</li>
<li><strong>é¢åç¹å®åºç¡æ¨¡å (Domain-Specific Foundation Models):</strong> éå¯¹ç¹å®é«ä»·å¼é¢åï¼å¦å»çãå·¥ä¸ï¼å¼åå®å¶åçåºç¡æ¨¡åï¼ä»¥å®ç°æ´ç²¾åãé«æçåºç¨ã</li>
<li><strong>ç¨ç/ä¸å®æ´æ°æ®ä¸çæ©æ£æ¨¡å (Diffusion Models on Sparse/Incomplete Data):</strong> InfraDiffusion å±ç¤ºäºæ©æ£æ¨¡åå¨å¤çä¸å®æ´3Dæ°æ®æ¹é¢çæ½åï¼æªæ¥å¯è½æ©å±å°æ´å¤æ°æ®ç¨ç¼ºåºæ¯ã</li>
<li><strong>é«æä¸é²æ£ç Transformer æ¶æ (Efficient &amp; Robust Transformer Architectures):</strong> TinyDrop åæ°´å¢¨ç»å¢å¼ºé²æ£æ§çç ç©¶ï¼è¡¨æå¯¹ Transformer æ¨¡åæçåå®å¨æ§çå³æ³¨å°æç»­å¢å ã</li>
<li><strong>å¤æ¨¡ææå¾æ£æµä¸æºå¨äººäº¤äº (Multimodal Intent Detection &amp; Robotic Interaction):</strong> OVGrasp å¼ºè°äºç»åè§è§ãè¯­è¨ç­å¤ç§æ¨¡ææ¥çè§£äººç±»æå¾ï¼ä»¥å®ç°æ´æºè½çæºå¨äººæä½ã</li>
<li><strong>çæå¼ AI çåäºä¸é«å±æ¬¡ç¼è¾ (Narrative &amp; High-Level Editing in Generative AI):</strong> Plot'n Polish é¢ç¤ºççææ¨¡åå°ä»å¾åçæèµ°åæ´å¤æçåäºååå®¹åä½ã</li>
</ul>
<p><strong>4. å»ºè®®æ·±å¥éè¯»çè®ºæ (Recommended Full Reads)</strong></p>
<p>èèå°å¶æ½å¨å½±åååæ°æ§ï¼æä»¬å»ºè®®ç ç©¶äººåä¼åéè¯»ä»¥ä¸è®ºæï¼</p>
<ul>
<li><strong>OneCAT: Decoder-Only Auto-Regressive Model for Unified Understanding and Generation (Han Li et al.)</strong>: å¯¹äºå³æ³¨éç¨AIãåºç¡æ¨¡åæ¶æåå¤æ¨¡æå­¦ä¹ çç ç©¶èï¼è¿ç¯è®ºææä¾äºéè¦çæªæ¥æ¹åã</li>
<li><strong>A Generative Foundation Model for Chest Radiography (Yuanfeng Ji et al.)</strong>: å»çAIé¢åçç ç©¶äººååºéç¹å³æ³¨ï¼å®å¯è½ä¸ºå»å­¦å½±ååæå¸¦æ¥èå¼è½¬åã</li>
<li><strong>InfraDiffusion: zero-shot depth map restoration with diffusion models and prompted segmentation from sparse infrastructure point clouds (Yixiong Jing et al.)</strong>: ä¸æ³¨äº3Dè§è§ãæ©æ£æ¨¡åå¨ç¨çæ°æ®åºç¨æåºç¡è®¾æ½AIçç ç©¶èä¼ä»ä¸­åçã</li>
<li><strong>TinyDrop: Tiny Model Guided Token Dropping for Vision Transformers (Guoxin Wang et al.)</strong>: å¯¹äºè´åäº Vision Transformer é¨ç½²ãæçä¼ååè¾¹ç¼è®¡ç®çç ç©¶èï¼è¿æä¾äºå®ç¨çè§£å³æ¹æ¡ã</li>
<li><strong>Plot'n Polish: Zero-shot Story Visualization and Disentangled Editing with Text-to-Image Diffusion Models (Kiymet Akdemir et al.)</strong>: å¯¹çæå¼AIãåæåºç¨ãææ¬å°å¾åçæåå¯æ§åå®¹åä½æå´è¶£çç ç©¶èä¸å®¹éè¿ã</li>
</ul>
<hr />
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2509.03324v1">InfraDiffusion: zero-shot depth map restoration with diffusion models and prompted segmentation from sparse infrastructure point clouds</a></li>
<li><a href="#2509.04180v1">VisioFirm: Cross-Platform AI-assisted Annotation Tool for Computer Vision</a></li>
<li><a href="#2509.03680v1">LuxDiT: Lighting Estimation with Video Diffusion Transformer</a></li>
<li><a href="#2509.03498v1">OneCAT: Decoder-Only Auto-Regressive Model for Unified Understanding and Generation</a></li>
<li><a href="#2509.03903v1">A Generative Foundation Model for Chest Radiography</a></li>
<li><a href="#2509.04324v1">OVGrasp: Open-Vocabulary Grasping Assistance via Multimodal Intent Detection</a></li>
<li><a href="#2509.03006v1">Enhancing Robustness in Post-Processing Watermarking: An Ensemble Attack Network Using CNNs and Transformers</a></li>
<li><a href="#2509.04446v1">Plot'n Polish: Zero-shot Story Visualization and Disentangled Editing with Text-to-Image Diffusion Models</a></li>
<li><a href="#2509.04268v1">Differential Morphological Profile Neural Networks for Semantic Segmentation</a></li>
<li><a href="#2509.03379v1">TinyDrop: Tiny Model Guided Token Dropping for Vision Transformers</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2509.03324v1'></a></p>
<h2 id="infradiffusion-zero-shot-depth-map-restoration-with-diffusion-models-and-prompted-segmentation-from-sparse-infrastructure-point-clouds"><a href="https://arxiv.org/abs/2509.03324v1">InfraDiffusion: zero-shot depth map restoration with diffusion models and prompted segmentation from sparse infrastructure point clouds</a></h2>
<p><strong>Authors:</strong> Yixiong Jing, Cheng Zhang, Haibing Wu, Guangming Wang, Olaf Wysocki, Brian Sheil</p>
<p><strong>Published:</strong> 2025-09-03</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Point clouds are widely used for infrastructure monitoring by providing
geometric information, where segmentation is required for downstream tasks such
as defect detection. Existing research has automated semantic segmentation of
structural components, while brick-level segmentation (identifying defects such
as spalling and mortar loss) has been primarily conducted from RGB images.
However, acquiring high-resolution images is impractical in low-light
environments like masonry tunnels. Point clouds, though robust to dim lighting,
are typically unstructured, sparse, and noisy, limiting fine-grained
segmentation. We present InfraDiffusion, a zero-shot framework that projects
masonry point clouds into depth maps using virtual cameras and restores them by
adapting the Denoising Diffusion Null-space Model (DDNM). Without task-specific
training, InfraDiffusion enhances visual clarity and geometric consistency of
depth maps. Experiments on masonry bridge and tunnel point cloud datasets show
significant improvements in brick-level segmentation using the Segment Anything
Model (SAM), underscoring its potential for automated inspection of masonry
assets. Our code and data is available at
https://github.com/Jingyixiong/InfraDiffusion-official-implement.</p>
<p><strong>Analysis:</strong></p>
<p>InfraDiffusion è®ºææè¦çåæå¦ä¸ï¼</p>
<h3 id="1">1. è®ºæä¸»è¦è´¡ç®çç®ææè¦</h3>
<p>InfraDiffusion æåºäºä¸ç§é¶æ ·æ¬ï¼zero-shotï¼æ¡æ¶ï¼ç¨äºä»ç¨çãéç»æåä¸åæçç ç³ç¹äºä¸­æ¢å¤é«è´¨éçæ·±åº¦å¾ãè¯¥æ¡æ¶éè¿å°ç¹äºæå½±å°èæç¸æºçæçæ·±åº¦å¾ï¼å¹¶å©ç¨éåºæ§ä¿®æ¹çå»åªæ©æ£é¶ç©ºé´æ¨¡åï¼DDNMï¼è¿è¡ä¿®å¤ï¼æ¾èæåäºæ·±åº¦å¾çè§è§æ¸æ°åº¦åå ä½ä¸è´æ§ãå¶æ ¸å¿è´¡ç®å¨äºï¼æ ééå¯¹ç¹å®ä»»å¡è¿è¡è®­ç»ï¼å³å¯æ¾èæ¹åç ç³ç»æçç åçº§åå²ææï¼ä»èæ¨å¨èªå¨ååºç¡è®¾æ½æ£æµã</p>
<h3 id="2">2. å³é®åæ°ææ¹æ³å­¦æ¹æ³</h3>
<p>è¯¥è®ºæçå³é®åæ°å¨äºå¶<strong>é¶æ ·æ¬æ·±åº¦å¾æ¢å¤æ¡æ¶</strong>ï¼å®å·§å¦å°ç»åäºä»¥ä¸å ç¹ï¼
*   <strong>ç¹äºå°æ·±åº¦å¾çæå½±ï¼</strong> å°ç¨ççç ç³ç¹äºéè¿èæç¸æºè½¬æ¢ä¸ºæ·±åº¦å¾ï¼ä¸ºåç»­å¤çæä¾äºä¸ä¸ªç»æåçäºç»´è¡¨ç¤ºã
*   <strong>éåºæ§æ©æ£æ¨¡ååºç¨ï¼</strong> æ ¸å¿å¨äº<strong>éåºæ§å°ä¿®æ¹ååºç¨å»åªæ©æ£é¶ç©ºé´æ¨¡åï¼DDNMï¼</strong>è¿è¡æ·±åº¦å¾æ¢å¤ãDDNM éå¸¸ç¨äºå¾åä¿®å¤ææ¡ä»¶çæï¼æ­¤å¤å°å¶åæ°æ§å°åºç¨äºä»ç¨çãä¸å®æ´æ°æ®ä¸­æ¢å¤å ä½ä¿¡æ¯ä¸°å¯çæ·±åº¦å¾ï¼ä¸æ éä»»å¡ç¹å®çè®­ç»ã
*   <strong>é¶æ ·æ¬è½åï¼</strong> æ´ä¸ªæ¡æ¶æ ééå¯¹ç ç³ç»æææ·±åº¦å¾æ¢å¤ä»»å¡è¿è¡é¢å¤çè®­ç»ï¼ç´æ¥å©ç¨é¢è®­ç»çæ©æ£æ¨¡åè½åï¼è¿å¤§å¤§éä½äºæ°æ®æ æ³¨åæ¨¡åè®­ç»çææ¬ï¼å¹¶æåäºæ³åè½åã
*   <strong>ä¸ç°æåå²æ¨¡åçç»åï¼</strong> æ¢å¤åçé«è´¨éæ·±åº¦å¾è½å¤æ¾èæåå¦ Segment Anything Model (SAM) ç­éç¨åå²æ¨¡åå¨ç åçº§åå²ä»»å¡ä¸çè¡¨ç°ï¼å®ç°äºä»ä½è´¨éç¹äºå°é«ç²¾åº¦è¯­ä¹çè§£çæ¡¥æ¢ã</p>
<h3 id="3">3. å¯¹é¢åæ½å¨å½±å</h3>
<ul>
<li><strong>åºç¡è®¾æ½æ£æµä¸ç»´æ¤ï¼</strong> ä¸ºç ç³æ¡¥æ¢åé§éç­åºç¡è®¾æ½çèªå¨åãç²¾ç»åæ£æµæä¾äºå¼ºå¤§çå·¥å·ï¼è½å¤æ´åç¡®å°è¯å«å¥è½ãç æµæµå¤±ç­ç¼ºé·ï¼ä»èæé«ç»´æ¤æçåå®å¨æ§ã</li>
<li><strong>ç¹äºæ°æ®å©ç¨æçï¼</strong> åæäºç¨çãåæç¹äºå¨ç»ç²åº¦åæä¸çå±éæ§ï¼ä¸ºä»ä½è´¨éä¸ç»´æ°æ®ä¸­æåé«ä»·å¼ä¿¡æ¯å¼è¾äºæ°éå¾ã</li>
<li><strong>æ©æ£æ¨¡ååºç¨æå±ï¼</strong> å°æ©æ£æ¨¡åçåºç¨èå´ä»ä¼ ç»çå¾åçæãä¿®å¤ç­é¢åæå±å°å ä½æ°æ®ï¼æ·±åº¦å¾ï¼çæ¢å¤åå¢å¼ºï¼å±ç¤ºäºå¶å¨å¤çç»æåå ä½ä¿¡æ¯æ¹é¢çå·¨å¤§æ½åã</li>
<li><strong>é¶æ ·æ¬å­¦ä¹ çå®è·µï¼</strong> å¼ºè°äºé¶æ ·æ¬æ¹æ³å¨å®éå·¥ç¨åºç¨ä¸­çå¯è¡æ§åä¼å¿ï¼å°¤å¶æ¯å¨é¾ä»¥è·åå¤§éæ æ³¨æ°æ®çç¹å®é¢åã</li>
<li><strong>å¤æ¨¡ææ°æ®èåï¼</strong> è½ç¶æ½è±¡ä¸­æªç´æ¥æåï¼ä½è¿ç§å°ä¸ç»´ç¹äºè½¬æ¢ä¸ºäºç»´æ·±åº¦å¾å¹¶å©ç¨å¾åå¤çææ¯è¿è¡å¢å¼ºçæè·¯ï¼ä¸ºæªæ¥å¤æ¨¡ææ°æ®èååå¤çæä¾äºæ°çèå¼ã</li>
</ul>
<h3 id="4">4. ç¸å³é¢åæåºç¨</h3>
<ul>
<li><strong>åæ¨å·¥ç¨ä¸ç»æå¥åº·çæµï¼</strong> èªå¨åæ£æµæ¡¥æ¢ãé§éãå¤§åç­åºç¡è®¾æ½çç»æç¼ºé·ã</li>
<li><strong>æåéäº§ä¿æ¤ï¼</strong> å¯¹åå²å»ºç­ãéå¡ç­è¿è¡ç²¾ç»åä¸ç»´æ«æåæä¼¤è¯ä¼°ã</li>
<li><strong>æºå¨äººä¸èªä¸»æ£æµï¼</strong> è£å¤ææ¿åé·è¾¾çæºå¨äººææ äººæºå¨å¤æãä½åç§ç¯å¢ä¸è¿è¡èªä¸»å·¡æ£åç¯å¢æç¥ã</li>
<li><strong>æ°å­å­ªçï¼Digital Twinï¼ï¼</strong> åå»ºé«ç²¾åº¦çç©çèµäº§æ°å­æ¨¡åï¼ç¨äºæ¨¡æãåæåé¢æµã</li>
<li><strong>å»ºç­ä¿¡æ¯æ¨¡åï¼BIMï¼ï¼</strong> å¢å¼ºç°æå»ºç­çBIMæ¨¡åï¼ä½¿å¶åå«æ´è¯¦ç»çç»æå¥åº·ä¿¡æ¯ã</li>
<li><strong>éç¿ä¸å°è´¨åæ¢ï¼</strong> é§éãç¿äºç­å°ä¸ç©ºé´çç»æç¨³å®æ§çæµã</li>
</ul>
<h3 id="5">5. å¯ä»æè¦ä¸­æ¨æ­åºçå±éæ§</h3>
<ul>
<li><strong>ä¾èµèæç¸æºè§è§ï¼</strong> å°ç¹äºæå½±å°æ·±åº¦å¾çè´¨éåå®æ´æ§å¯è½é«åº¦ä¾èµäºèæç¸æºçéæ©åæ°éãå¦æç¹äºå¨æäºåºåæå¶ç¨çï¼æèèæç¸æºè§è§ä¸ä½³ï¼å¯è½å¯¼è´æ·±åº¦å¾ä¿¡æ¯ç¼ºå¤±æä¸åç¡®ã</li>
<li><strong>DDNMçæ³åè½åï¼</strong> å°½ç®¡æ¯âé¶æ ·æ¬âï¼ä½DDNMæ¬èº«æ¯é¢è®­ç»æ¨¡åãå¶å¨å¤çç ç³ç»æç¹æçå ä½çº¹çåç¼ºé·æ¨¡å¼ä¸çè¡¨ç°ï¼å¯è½åéäºå¶åå§è®­ç»æ°æ®çé¢åãå¯¹äºä¸è®­ç»æ°æ®å·®å¼è¾å¤§çå ä½ç»ææææï¼ææå¯è½ææä¸éã</li>
<li><strong>ç ç³ç»æçç¹å¼æ§ï¼</strong> è®ºæå¼ºè°äºâç ç³ç¹äºâåâç åçº§åå²âãè¿è¡¨æè¯¥æ¹æ³å¯è½éå¯¹ç ç³ç»æè¿è¡äºä¼åæéªè¯ï¼å¶å¨å¶ä»ç±»åç»æï¼å¦æ··ååãé¢ç»æï¼ææ´å¤æç¼ºé·ï¼å¦è£ç¼ãåå½¢ï¼ä¸çè¡¨ç°å°ä¸æç¡®ã</li>
<li><strong>è®¡ç®èµæºéæ±ï¼</strong> æ©æ£æ¨¡åéå¸¸è®¡ç®ææ¬è¾é«ï¼å°¤å¶æ¯å¨çæé«åè¾¨çæ·±åº¦å¾æ¶ãæè¦ä¸­æªæåå®æ¶æ§æè®¡ç®æçï¼è¿å¨å®éé¨ç½²ä¸­å¯è½æ¯ä¸ä¸ªèéå ç´ ã</li>
<li><strong>æ·±åº¦å¾çå±éæ§ï¼</strong> æ·±åº¦å¾æ¯2.5Dè¡¨ç¤ºï¼æ æ³å®å¨ææä¸ç»´ç¹äºçææå ä½ä¿¡æ¯ï¼ä¾å¦é®æ¡åºååçç»æãè¿å¯è½éå¶äºå¯¹æäºå¤æç¼ºé·çæ£æµè½åã</li>
</ul>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present InfraDiffusion, a zero-shot framework that projects
masonry point clouds into depth maps using virtual cameras and restores them by
adapting the Denoising Diffusion Null-space Model (DDNM).</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="http://arxiv.org/pdf/2509.03324v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.03324v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.04180v1'></a></p>
<h2 id="visiofirm-cross-platform-ai-assisted-annotation-tool-for-computer-vision"><a href="https://arxiv.org/abs/2509.04180v1">VisioFirm: Cross-Platform AI-assisted Annotation Tool for Computer Vision</a></h2>
<p><strong>Authors:</strong> Safouane El Ghazouali, Umberto Michelucci</p>
<p><strong>Published:</strong> 2025-09-04</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>AI models rely on annotated data to learn pattern and perform prediction.
Annotation is usually a labor-intensive step that require associating labels
ranging from a simple classification label to more complex tasks such as object
detection, oriented bounding box estimation, and instance segmentation.
Traditional tools often require extensive manual input, limiting scalability
for large datasets. To address this, we introduce VisioFirm, an open-source web
application designed to streamline image labeling through AI-assisted
automation. VisioFirm integrates state-of-the-art foundation models into an
interface with a filtering pipeline to reduce human-in-the-loop efforts. This
hybrid approach employs CLIP combined with pre-trained detectors like
Ultralytics models for common classes and zero-shot models such as Grounding
DINO for custom labels, generating initial annotations with low-confidence
thresholding to maximize recall. Through this framework, when tested on
COCO-type of classes, initial prediction have been proven to be mostly correct
though the users can refine these via interactive tools supporting bounding
boxes, oriented bounding boxes, and polygons. Additionally, VisioFirm has
on-the-fly segmentation powered by Segment Anything accelerated through WebGPU
for browser-side efficiency. The tool supports multiple export formats (YOLO,
COCO, Pascal VOC, CSV) and operates offline after model caching, enhancing
accessibility. VisioFirm demonstrates up to 90\% reduction in manual effort
through benchmarks on diverse datasets, while maintaining high annotation
accuracy via clustering of connected CLIP-based disambiguate components and
IoU-graph for redundant detection suppression. VisioFirm can be accessed from
\href{https://github.com/OschAI/VisioFirm}{https://github.com/OschAI/VisioFirm}.</p>
<p><strong>Analysis:</strong></p>
<p>ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå¯¹è¿ç¯è®ºææè¦è¿è¡å¦ä¸åæï¼</p>
<hr />
<h3 id="1-concise-summary">1. è®ºæä¸»è¦è´¡ç®çç®ææè¦ (Concise Summary)</h3>
<p>VisioFirmæ¯ä¸ä¸ªå¼æºçè·¨å¹³å°AIè¾å©æ æ³¨å·¥å·ï¼æ¨å¨è§£å³è®¡ç®æºè§è§æ°æ®æ æ³¨èæ¶èåçé®é¢ãå®éè¿æºè½éæCLIPãGrounding DINOãUltralyticsç­åæ²¿åºç¡æ¨¡ååWebGPUå éçSegment Anything Model (SAM)ï¼å®ç°äºé«æçæ··åäººæºåä½æ æ³¨æµç¨ï¼å£°ç§°å¯å°æå¨å·¥ä½éåå°é«è¾¾90%ï¼åæ¶ä¿æé«æ æ³¨ç²¾åº¦ã</p>
<h3 id="2-key-innovation-or-methodological-approach">2. å³é®åæ°ææ¹æ³è®º (Key Innovation or Methodological Approach)</h3>
<p>VisioFirmçæ ¸å¿åæ°å¨äºå¶<strong>æ··åå¼AIè¾å©æ æ³¨èå¼</strong>ï¼éè¿æºè½éæå¤ç§åæ²¿åºç¡æ¨¡åï¼çæåå§é«å¬åççä½ç½®ä¿¡åº¦é¢æµï¼åç±ç¨æ·è¿è¡ç²¾ä¿®ãå·ä½æ¹æ³åæ¬ï¼</p>
<ul>
<li><strong>å¤æ¨¡åèåç­ç¥ï¼</strong> ç»åäºé¢è®­ç»æ£æµå¨ï¼å¦Ultralyticsæ¨¡åï¼å¤çå¸¸è§ç±»å«ï¼é¶æ ·æ¬æ¨¡åï¼å¦Grounding DINOï¼å¤çèªå®ä¹æ ç­¾ï¼ä»¥åCLIPæ¨¡åè¿è¡è¯­ä¹æ¶æ­§åè¿æ¥ç»ä»¶èç±»ã</li>
<li><strong>é«æçæµè§å¨ç«¯åå²ï¼</strong> å©ç¨<strong>WebGPUå éçSegment Anything Model (SAM)</strong>ï¼å®ç°äºå®æ¶ãé«æçâå³æ¶åå²âåè½ï¼æ¾èæåäºç¨æ·ä½éªã</li>
<li><strong>ç²¾åº¦ç»´æ¤æºå¶ï¼</strong> å¼å¥äº<strong>åºäºCLIPçè¿æ¥ç»ä»¶èç±»æ¶æ­§</strong>å<strong>IoUå¾åä½æ£æµæå¶æºå¶</strong>ï¼ä»¥å¨å¤§å¹åå°äººå·¥å¹²é¢çåæ¶ç¡®ä¿æ æ³¨ç²¾åº¦ã</li>
<li><strong>è·¨å¹³å°ä¸ç¦»çº¿è½åï¼</strong> ä½ä¸ºWebåºç¨ï¼æ¯æå¤ç§å¯¼åºæ ¼å¼ï¼YOLO, COCO, Pascal VOC, CSVï¼ï¼å¹¶å¨æ¨¡åç¼å­åæ¯æç¦»çº¿æä½ï¼æå¤§å°å¢å¼ºäºå·¥å·çå¯ç¨æ§åå¯è®¿é®æ§ã</li>
</ul>
<h3 id="3-potential-impact-on-the-field">3. å¯¹é¢åæ½å¨å½±å (Potential Impact on the Field)</h3>
<p>VisioFirmæææ¾è<strong>éä½è®¡ç®æºè§è§é¢åæ°æ®æ æ³¨çé¨æ§åææ¬</strong>ï¼å°¤å¶å¯¹äºèµæºæéçå¢éåç ç©¶èãéè¿å°æå¨æ æ³¨å·¥ä½éåå°é«è¾¾90%ï¼å®è½<strong>æå¤§å éæ°æ®éçåå»ºåè¿­ä»£è¿ç¨</strong>ï¼ä»è<strong>æ¨å¨AIæ¨¡åå¼ååé¨ç½²çæç</strong>ãå¶å¼æºåè·¨å¹³å°çç¹æ§ä¹æå©äº<strong>ä¿è¿AIè¾å©æ æ³¨å·¥å·çæ®ååæ åå</strong>ï¼ä½¿æ´å¤äººè½å¤å©ç¨åè¿çAIè½åè¿è¡é«è´¨éæ°æ®åå¤ï¼ä»èå éæ´ä¸ªCVçæç³»ç»çåæ°ã</p>
<h3 id="4-related-areas-or-applications">4. ç¸å³é¢åæåºç¨ (Related Areas or Applications)</h3>
<ul>
<li><strong>è®¡ç®æºè§è§æ¨¡åå¼åä¸é¨ç½²ï¼</strong> ä»»ä½éè¦å¤§éæ æ³¨æ°æ®æ¥è®­ç»ãå¾®è°æè¯ä¼°æ¨¡åçåºæ¯ï¼å¦ç®æ æ£æµãå®ä¾åå²ãè¯­ä¹åå²ãå§¿æä¼°è®¡ç­ã</li>
<li><strong>èªå¨é©¾é©¶ä¸æºå¨äººï¼</strong> ç¨äºæ æ³¨æç¥ç³»ç»æéçéè·¯ãè½¦è¾ãè¡äººãéç¢ç©ç­æ°æ®ã</li>
<li><strong>å»çå½±ååæï¼</strong> è¾å©å»çæç ç©¶äººåæ æ³¨çç¶ãå¨å®ãç»èç­å»å­¦å¾åã</li>
<li><strong>å·¥ä¸è´¨æ£ä¸å®é²çæ§ï¼</strong> å¿«éæ æ³¨ç¼ºé·ãå¼å¸¸è¡ä¸ºæç¹å®ç®æ ã</li>
<li><strong>åä¸ç§æï¼</strong> æ æ³¨ä½ç©çè«å®³ãæå®æçåº¦ãåç°åºåç­ã</li>
<li><strong>å­¦æ¯ç ç©¶ä¸æè²ï¼</strong> ä¸ºå­¦çåç ç©¶äººåæä¾ä¸ä¸ªæäºä½¿ç¨çå·¥å·æ¥åå»ºèªå®ä¹æ°æ®éã</li>
<li><strong>æ°æ®æ æ³¨æå¡æä¾åï¼</strong> æé«å¶æå¡æçåéä½ææ¬ã</li>
</ul>
<h3 id="5-limitations-inferred-from-the-abstract">5. ä»æè¦ä¸­å¯æ¨æ­çå±éæ§ (Limitations Inferred from the Abstract)</h3>
<ul>
<li><strong>å¯¹äººå·¥å¹²é¢çæç»­ä¾èµï¼</strong> å°½ç®¡å£°ç§°åå°90%çå·¥ä½éï¼ä½âåå§é¢æµå¤§å¤æ­£ç¡®âåâç¨æ·å¯ä»¥ç»åâè¡¨æäººå·¥å®¡æ ¸åä¿®æ­£ä»ç¶æ¯ç¡®ä¿æç»æ æ³¨è´¨éçå³é®ç¯èï¼å¹¶éå®å¨èªå¨åã</li>
<li><strong>å¯¹åºç¡æ¨¡åçæ§è½ä¾èµï¼</strong> VisioFirmçæçååç¡®æ§é«åº¦ä¾èµäºå¶éæçCLIPãGrounding DINOãUltralyticsåSAMç­åºç¡æ¨¡åçæ³åè½åãå¯¹äºè¿äºæ¨¡åä¸æé¿å¤ççç¹å®é¢åãé«åº¦æ½è±¡ææåº¦ç»ç²åº¦çèªå®ä¹ç±»å«ï¼å¶è¾å©ææå¯è½ä¼æææ£ã</li>
<li><strong>âCOCO-type of classesâçæµè¯èå´ï¼</strong> å°½ç®¡å¨COCOç±»åç±»å«ä¸è¡¨ç°è¯å¥½ï¼ä½å¯¹äºé«åº¦ä¸ä¸åãé¿å°¾åå¸æè§è§ä¸æ¨¡ç³çèªå®ä¹æ°æ®éï¼å¶åå§é¢æµçåç¡®æ§åå¬åçå¯è½éè¦æ´é¢ç¹çäººå·¥ä¿®æ­£ã</li>
<li><strong>WebGPUçå¼å®¹æ§ä¸æ§è½ï¼</strong> WebGPUçå éææå¯è½åéäºç¨æ·æµè§å¨çæ¬ãæ¾å¡ç¡¬ä»¶åé©±å¨ç¨åºï¼å¹¶éææç¨æ·é½è½è·å¾æä½³çæµè§å¨ç«¯æçã</li>
<li><strong>ç¦»çº¿è½åçå±éæ§ï¼</strong> âæ¨¡åç¼å­åå¯ç¦»çº¿æä½âæå³çé¦æ¬¡ä½¿ç¨ææ¨¡åæ´æ°æ¶ä»éç½ç»è¿æ¥ä¸è½½æ¨¡åï¼ä¸æ¨¡åç¼å­å¯è½å ç¨å¤§éæ¬å°å­å¨ç©ºé´ã</li>
<li><strong>âé«è¾¾90%âçåå°éï¼</strong> è¿æ¯ä¸ä¸ªä¸éå¼ï¼å®éåå°éå¯è½å æ°æ®éçå¤ææ§ãæ æ³¨ä»»å¡ç±»åä»¥åç¨æ·çç»åº¦èå¼ã</li>
<li><strong>æªæåè§é¢æ æ³¨ï¼</strong> æè¦ä¸»è¦èç¦äºå¾åæ æ³¨ï¼æªè¯´æå¶å¯¹è§é¢æ æ³¨ä»»å¡çæ¯æè½åã</li>
</ul>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this, we introduce VisioFirm, an open-source web
application designed to streamline image labeling through AI-assisted
automation.</li>
<li>VisioFirm integrates state-of-the-art foundation models into an
interface with a filtering pipeline to reduce human-in-the-loop efforts.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="http://arxiv.org/pdf/2509.04180v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.04180v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.03680v1'></a></p>
<h2 id="luxdit-lighting-estimation-with-video-diffusion-transformer"><a href="https://arxiv.org/abs/2509.03680v1">LuxDiT: Lighting Estimation with Video Diffusion Transformer</a></h2>
<p><strong>Authors:</strong> Ruofan Liang, Kai He, Zan Gojcic, Igor Gilitschenski, Sanja Fidler, Nandita Vijaykumar, Zian Wang</p>
<p><strong>Published:</strong> 2025-09-03</p>
<p><strong>Categories:</strong> cs.GR, cs.AI, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Estimating scene lighting from a single image or video remains a longstanding
challenge in computer vision and graphics. Learning-based approaches are
constrained by the scarcity of ground-truth HDR environment maps, which are
expensive to capture and limited in diversity. While recent generative models
offer strong priors for image synthesis, lighting estimation remains difficult
due to its reliance on indirect visual cues, the need to infer global
(non-local) context, and the recovery of high-dynamic-range outputs. We propose
LuxDiT, a novel data-driven approach that fine-tunes a video diffusion
transformer to generate HDR environment maps conditioned on visual input.
Trained on a large synthetic dataset with diverse lighting conditions, our
model learns to infer illumination from indirect visual cues and generalizes
effectively to real-world scenes. To improve semantic alignment between the
input and the predicted environment map, we introduce a low-rank adaptation
finetuning strategy using a collected dataset of HDR panoramas. Our method
produces accurate lighting predictions with realistic angular high-frequency
details, outperforming existing state-of-the-art techniques in both
quantitative and qualitative evaluations.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå¯¹è¿ç¯å³äº LuxDiT çè®ºææè¦è¿è¡å¦ä¸åæï¼</p>
<hr />
<h3 id="luxdit-lighting-estimation-with-video-diffusion-transformer_1">LuxDiT: Lighting Estimation with Video Diffusion Transformer æè¦åæ</h3>
<p><strong>1. è®ºæä¸»è¦è´¡ç®çç®æ´æ»ç» (2-3 å¥è¯)</strong></p>
<p>LuxDiT æåºäºä¸ç§æ°é¢çæ°æ®é©±å¨æ¹æ³ï¼éè¿å¾®è°è§é¢æ©æ£Transformeræ¥è§£å³ä»å¾åæè§é¢ä¼°è®¡åºæ¯ç§æçé¿æææãè¯¥æ¨¡åå©ç¨å¤§è§æ¨¡åææ°æ®éå­¦ä¹ ä»é´æ¥è§è§çº¿ç´¢æ¨æ­HDRç¯å¢åç§ï¼å¹¶éè¿ä½ç§©éåºå¾®è°ç­ç¥å¢å¼ºè¯­ä¹å¯¹é½ï¼æç»çæé«ç²¾åº¦ãç»èä¸°å¯çHDRç¯å¢å¾ï¼è¶è¶ç°æSOTAã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®º</strong></p>
<p>æ ¸å¿åæ°å¨äºå°<strong>è§é¢æ©æ£Transformer</strong>æ¶æåºç¨äº<strong>HDRç¯å¢åç§ä¼°è®¡</strong>è¿ä¸å·ææææ§çä»»å¡ãè¿ä¸ä¼ ç»æ©æ£æ¨¡åä¸»è¦ç¨äºå¾åçæä¸åï¼å®éè¦ä»é´æ¥è§è§çº¿ç´¢ä¸­æ¨æ­å¨å±ï¼éå±é¨ï¼ä¸ä¸æå¹¶è¾åºé«å¨æèå´æ°æ®ã</p>
<p>æ¹æ³è®ºä¸ï¼å®å·§å¦å°ç»åäºï¼
*   <strong>å¤§è§æ¨¡åææ°æ®è®­ç»</strong>ï¼åæçå®ä¸çHDRç¯å¢å¾ç¨ç¼ºæ§ï¼å­¦ä¹ åºç¡åç§æ¨æ­è½åã
*   <strong>ä½ç§©éåºï¼LoRAï¼å¾®è°ç­ç¥</strong>ï¼å©ç¨æ¶éå°ççå®HDRå¨æ¯å¾æ°æ®éï¼é«æå°æ¹åæ¨¡åå¨çå®åºæ¯ä¸­çè¯­ä¹å¯¹é½åæ³åè½åï¼åæ¶é¿åå¯¹å¤§åé¢è®­ç»æ¨¡åè¿è¡æè´µçå¨é¢å¾®è°ã</p>
<p><strong>3. å¯¹é¢åæ½å¨å½±å</strong></p>
<ul>
<li><strong>é«è´¨éæ¸²æä¸èæåå®¹åä½</strong>ï¼ä¸ºçµå½±ãæ¸¸æãèæç°å®/å¢å¼ºç°å®ï¼VR/ARï¼ç­é¢åæä¾æ´åç¡®ãæ´çå®çåºæ¯åç§ä¼°è®¡ï¼æå¤§æåæ¸²æè´¨éåæ²æµ¸æã</li>
<li><strong>éåå¾å½¢å­¦ï¼Inverse Graphicsï¼</strong>ï¼æ¨å¨ä»2Då¾åæè§é¢ä¸­æ¢å¤3Dåºæ¯å±æ§ï¼å¦åç§ï¼çç ç©¶ï¼æ¯çè§£ä¸ççéè¦ä¸æ­¥ã</li>
<li><strong>æ°æ®é©±å¨æ¨¡åèå¼</strong>ï¼å±ç¤ºäºå¦ä½ææå©ç¨åææ°æ®åæçå®æ°æ®ç¨ç¼ºæ§ï¼å¹¶éè¿é«æå¾®è°ç­ç¥ï¼å¦LoRAï¼å®ç°åçå®ä¸ççæ³åï¼ä¸ºå¶ä»ç±»ä¼¼ä»»å¡æä¾äºæä»·å¼çèä¾ã</li>
<li><strong>æ©æ£æ¨¡ååºç¨æå±</strong>ï¼å°æ©æ£æ¨¡åä»åå®¹çææ©å±å°å¤æçéåé®é¢è§£å³ï¼æå®½äºå¶å¨è®¡ç®æºè§è§é¢åçåºç¨è¾¹çã</li>
</ul>
<p><strong>4. å¯è½åçäºè¿é¡¹ç ç©¶çç¸å³é¢åæåºç¨</strong></p>
<ul>
<li><strong>çµå½±ä¸æ¸¸æå¶ä½</strong>ï¼å®ç°èæè§è²ä¸çå®åºæ¯çæ ç¼èåï¼æå¯¹ç°æåºæ¯è¿è¡åç§è°æ´ã</li>
<li><strong>å¢å¼ºç°å®ï¼ARï¼ä¸èæç°å®ï¼VRï¼</strong>ï¼å¨çå®ç¯å¢ä¸­åç¡®æ¾ç½®èæç©ä½ï¼å¹¶ä½¿å¶åç§ä¸ç¯å¢ä¸è´ï¼æåçå®æåæ²æµ¸æã</li>
<li><strong>3Déå»ºä¸åºæ¯çè§£</strong>ï¼åç§æ¯åºæ¯å ä½åæè´¨æ¨æ­çå³é®çº¿ç´¢ã</li>
<li><strong>è®¡ç®æå½±</strong>ï¼å¾ååæå¤çä¸­çåç§è°æ´ãé£æ ¼è¿ç§»ç­ã</li>
<li><strong>æ°å­äººä¸èæå½¢è±¡</strong>ï¼ä¸ºæ°å­äººæä¾é¼ççç¯å¢åç§ï¼ä½¿å¶å¨ä¸ååºæ¯ä¸è¡¨ç°èªç¶ã</li>
</ul>
<p><strong>5. ä»æè¦ä¸­å¯æ¨æ­çå±éæ§</strong></p>
<ul>
<li><strong>åææ°æ®ä¸çå®ä¸çå·®è·</strong>ï¼å°½ç®¡ä½¿ç¨äºå¤§è§æ¨¡åææ°æ®å¹¶è¿è¡äºçå®æ°æ®å¾®è°ï¼ä½åææ°æ®ä¸çå®ä¸çä¹é´åºæçé¢åå·®è·ï¼domain gapï¼å¯è½ä»ç¶å­å¨ï¼å°¤å¶æ¯å¨æç«¯ææªè§è¿ççå®åç§æ¡ä»¶ä¸ï¼æ¨¡åçæ³åè½åå¯è½åå°ææã</li>
<li><strong>å¯¹é´æ¥è§è§çº¿ç´¢çä¾èµ</strong>ï¼åç§ä¼°è®¡é«åº¦ä¾èµäºåºæ¯ä¸­çé´å½±ãåå°ãé«åç­é´æ¥çº¿ç´¢ãå¨è¿äºçº¿ç´¢ä¸ææ¾ãæ¨¡ç³æè¢«é®æ¡çåºæ¯ä¸­ï¼æ¨¡åçé²æ£æ§å¯è½ä¸éã</li>
<li><strong>å¨å±ä¸ä¸ææ¨æ­çå±éæ§</strong>ï¼ä»æéçè§è§è¾å¥ï¼åå¼ å¾åæè§é¢çæ®µï¼æ¨æ­æ´ä¸ª360åº¦HDRç¯å¢å¾ï¼æ¬è´¨ä¸æ¯ä¸ä¸ªæ¬ å®é®é¢ãæ¨¡åå¯è½é¾ä»¥åç¡®ææå°è¾å¥è§å¾ä¹å¤çå¤ææé®æ¡çåç§ä¿¡æ¯ã</li>
<li><strong>è®¡ç®ææ¬</strong>ï¼è§é¢æ©æ£Transformeræ¨¡åéå¸¸è®¡ç®éè¾å¤§ï¼å°¤å¶æ¯å¨çæé«åè¾¨çHDRè¾åºæ¶ï¼æ¨çéåº¦åèµæºæ¶èå¯è½æ¯ä¸ä¸ªå®éåºç¨ä¸­çèéã</li>
<li><strong>å¾®è°æ°æ®éçè´¨éä¸å¤æ ·æ§</strong>ï¼è½ç¶æå°äºä½¿ç¨âæ¶éå°çHDRå¨æ¯å¾æ°æ®éâè¿è¡LoRAå¾®è°ï¼ä½è¯¥æ°æ®éçè§æ¨¡ãå¤æ ·æ§åä»£è¡¨æ§å°ç´æ¥å½±åæ¨¡åå¨çå®ä¸çåºæ¯ä¸­çæç»æ§è½åè¯­ä¹å¯¹é½ææã</li>
</ul>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose
LuxDiT, a novel data-driven approach that fine-tunes a video diffusion
transformer to generate HDR environment maps conditioned on visual input.</li>
<li>To improve semantic alignment between the
input and the predicted environment map, we introduce a low-rank adaptation
finetuning strategy using a collected dataset of HDR panoramas.</li>
<li>Our method
produces accurate lighting predictions with realistic angular high-frequency
details, outperforming existing state-of-the-art techniques in both
quantitative and qualitative evaluations.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="http://arxiv.org/pdf/2509.03680v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.03680v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.03498v1'></a></p>
<h2 id="onecat-decoder-only-auto-regressive-model-for-unified-understanding-and-generation"><a href="https://arxiv.org/abs/2509.03498v1">OneCAT: Decoder-Only Auto-Regressive Model for Unified Understanding and Generation</a></h2>
<p><strong>Authors:</strong> Han Li, Xinyu Peng, Yaoming Wang, Zelin Peng, Xin Chen, Rongxiang Weng, Jingang Wang, Xunliang Cai, Wenrui Dai, Hongkai Xiong</p>
<p><strong>Published:</strong> 2025-09-03</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We introduce OneCAT, a unified multimodal model that seamlessly integrates
understanding, generation, and editing within a novel, pure decoder-only
transformer architecture. Our framework uniquely eliminates the need for
external components such as Vision Transformers (ViT) or vision tokenizer
during inference, leading to significant efficiency gains, especially for
high-resolution inputs. This is achieved through a modality-specific
Mixture-of-Experts (MoE) structure trained with a single autoregressive (AR)
objective, which also natively supports dynamic resolutions. Furthermore, we
pioneer a multi-scale visual autoregressive mechanism within the Large Language
Model (LLM) that drastically reduces decoding steps compared to diffusion-based
methods while maintaining state-of-the-art performance. Our findings
demonstrate the powerful potential of pure autoregressive modeling as a
sufficient and elegant foundation for unified multimodal intelligence. As a
result, OneCAT sets a new performance standard, outperforming existing
open-source unified multimodal models across benchmarks for multimodal
generation, editing, and understanding.</p>
<p><strong>Analysis:</strong></p>
<p>è¿ç¯è®ºæçæè¦å±ç¤ºäºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸ä¸ªéè¦è¿å±ï¼ç¹å«æ¯å¨ç»ä¸å¤æ¨¡ææºè½æ¹é¢ã</p>
<hr />
<h3 id="1-main-contribution">1. è®ºæä¸»è¦è´¡ç® (Main Contribution)</h3>
<p>OneCATå¼å¥äºä¸ä¸ªçº¯è§£ç å¨Transformeræ¶æçç»ä¸å¤æ¨¡ææ¨¡åï¼æ ç¼æ´åäºçè§£ãçæåç¼è¾ä»»å¡ãå¶æ ¸å¿è´¡ç®å¨äºæ¨çæ¶æ éå¤é¨è§è§ç»ä»¶ï¼å¦Vision Transformeræè§è§tokenizerï¼ï¼æ¾èæåäºé«åè¾¨çè¾å¥çæçï¼å¹¶éè¿åä¸èªåå½ç®æ åå¤å°ºåº¦è§è§èªåå½æºå¶å®ç°äºè·¨å¤æ¨¡æåºåçSOTAæ§è½ã</p>
<h3 id="2-key-innovation-or-methodological-approach_1">2. å³é®åæ°ææ¹æ³ (Key Innovation or Methodological Approach)</h3>
<ul>
<li><strong>çº¯è§£ç å¨Transformeræ¶æ (Pure Decoder-Only Transformer Architecture):</strong> æå¼äºä¼ ç»çç¼ç å¨-è§£ç å¨æå¸¦æç¬ç«è§è§ç¼ç å¨çæ¶æï¼ä»ä½¿ç¨ä¸ä¸ªè§£ç å¨æ¥å¤çæææ¨¡æçè¾å¥åè¾åºã</li>
<li><strong>æ¨çæ¶æ éå¤é¨è§è§ç»ä»¶ (Elimination of External Vision Components during Inference):</strong> éè¿æ¨¡æç¹å®çä¸å®¶æ··å (Mixture-of-Experts, MoE) ç»æï¼æ¨¡åå¨æ¨çæ¶å¯ä»¥ç´æ¥å¤çåå§è§è§è¾å¥ï¼æ éé¢å¤ççViTæè§è§tokenizerï¼æ¾èæé«äºæçã</li>
<li><strong>åä¸èªåå½ (AR) ç®æ è®­ç» (Single Autoregressive Objective Training):</strong> æ´ä¸ªæ¨¡åéè¿ä¸ä¸ªç»ä¸çèªåå½ç®æ è¿è¡è®­ç»ï¼ç®åäºè®­ç»èå¼ï¼å¹¶åçæ¯æå¨æåè¾¨çã</li>
<li><strong>å¤å°ºåº¦è§è§èªåå½æºå¶ (Multi-scale Visual Autoregressive Mechanism):</strong> å¨å¤§åè¯­è¨æ¨¡å (LLM) åé¨å¼å¥äºè¿ç§æºå¶ï¼ä¸æ©æ£æ¨¡åç¸æ¯ï¼å®è½å¤§å¹åå°è§£ç æ­¥éª¤ï¼åæ¶ä¿æé¢åçæ§è½ã</li>
</ul>
<h3 id="3-potential-impact-on-the-field_1">3. å¯¹é¢åæ½å¨å½±å (Potential Impact on the Field)</h3>
<ul>
<li><strong>æ¨å¨ç»ä¸å¤æ¨¡ææ¨¡åèå¼åå±:</strong> OneCATè¯æäºçº¯èªåå½å»ºæ¨¡ä½ä¸ºç»ä¸å¤æ¨¡ææºè½åºç¡çå¼ºå¤§æ½åï¼å¯è½å¼å¯¼æªæ¥ç ç©¶è½¬åæ´ç®æ´ãä¼éçæ¶æã</li>
<li><strong>æ¾èæåå¤æ¨¡ææ¨çæç:</strong> ç¹å«æ¯å¯¹äºé«åè¾¨çå¾ååè§é¢å¤çï¼æ éå¤é¨è§è§ç»ä»¶ååå°è§£ç æ­¥éª¤çç¹æ§ï¼å°æå¤§å°å éå¤æ¨¡æåºç¨çé¨ç½²åå®æ¶æ§ã</li>
<li><strong>ç®åæ¨¡åæ¶æåé¨ç½²:</strong> åå°å¯¹å¤ä¸ªç¬ç«ç»ä»¶çä¾èµï¼ä½¿å¾å¤æ¨¡ææ¨¡åçå¼åãè®­ç»åé¨ç½²è¿ç¨æ´å ç®ååé«æã</li>
<li><strong>ä¸ºå¤æ¨¡ææºè½è®¾å®æ°æ§è½æ å:</strong> å¨çæãç¼è¾åçè§£ç­å¤ä¸ªä»»å¡ä¸è¶è¶ç°æå¼æºæ¨¡åï¼å°æ¿å±ç¤¾åºè¿ä¸æ­¥æ¢ç´¢åä¼åç»ä¸å¤æ¨¡ææ¨¡åã</li>
</ul>
<h3 id="4-related-areas-or-applications-that-might-benefit-from-this-research">4. ç¸å³é¢åæåºç¨ (Related Areas or Applications that Might Benefit from this Research)</h3>
<ul>
<li><strong>å¤æ¨¡æåå®¹åä½:</strong> ææ¬å°å¾å/è§é¢çæãå¾åç¼è¾ãé£æ ¼è¿ç§»ãåæè®¾è®¡å·¥å·ã</li>
<li><strong>é«çº§è§è§çè§£:</strong> å¾å/è§é¢é®ç­ (VQA)ãè¯¦ç»æè¿°çæãåºæ¯çè§£ãäºä»¶æ£æµã</li>
<li><strong>äººæºäº¤äº:</strong> æ´èªç¶ãé«æçè§è§äº¤äºçé¢ï¼ä¾å¦éè¿ææ¬æä»¤ç´æ¥ç¼è¾å¾åæçæè§è§åå®¹ã</li>
<li><strong>è¾å©ææ¯:</strong> ä¸ºè§è§éç¢äººå£«æä¾æ´åç¡®ãå®æ¶çå¾ååè§é¢æè¿°ã</li>
<li><strong>å·èº«æºè½/æºå¨äºº:</strong> æºå¨äººéè¿è§è§æç¥ç¯å¢ãçè§£æä»¤å¹¶çæç¸åºçè§è§åé¦æè¡å¨ã</li>
<li><strong>å»çå½±ååæ:</strong> ç»åææ¬æ¥åçæå½±åãå¯¹å½±åè¿è¡ç¼è¾ä»¥è¾å©è¯æ­ãä»å½±åä¸­æåå³é®ä¿¡æ¯ã</li>
</ul>
<h3 id="5-limitations-that-can-be-inferred-from-the-abstract">5. å¯æ¨æ­çå±éæ§ (Limitations that Can Be Inferred from the Abstract)</h3>
<ul>
<li><strong>è®­ç»ææ¬:</strong> ç»ä¸å¤æ¨¡ææ¨¡åï¼ç¹å«æ¯ç»åMoEç»æï¼éå¸¸éè¦å·¨å¤§çè®¡ç®èµæºåæ°æ®è¿è¡è®­ç»ãæè¦ä¸­æªæåè®­ç»çè§æ¨¡åææ¬ï¼è¿å¯è½æ¯ä¸ä¸ªæ½å¨çææã</li>
<li><strong>MoEçå¤ææ§ä¸è´è½½åè¡¡:</strong> å°½ç®¡æ¨çæ¶æçé«ï¼ä½MoEç»æå¨è®­ç»åç»´æ¤ä¸å¯è½å¢å å¤ææ§ï¼å¹¶éè¦ç²¾ç»çè´è½½åè¡¡ç­ç¥æ¥ç¡®ä¿ä¸å®¶ç½ç»çææå©ç¨ã</li>
<li><strong>èªåå½çæåºæéå¶:</strong> å°½ç®¡å£°ç§°åå°äºè§£ç æ­¥éª¤ï¼ä½çº¯èªåå½çæå¨æäºåºæ¯ä¸ä»å¯è½é¢ä¸´çæéåº¦ï¼ç¸å¯¹äºå®å¨å¹¶è¡ï¼æçæå¤æ ·æ§çææï¼å°¤å¶æ¯å¨å¤çéå¸¸é¿çåºåæ¶ã</li>
<li><strong>å¤å°ºåº¦æºå¶çæ³åæ§:</strong> è¿ç§æ°é¢çå¤å°ºåº¦è§è§èªåå½æºå¶å¨å¤çæç«¯å¤ææç¹å®é¢åè§è§æ°æ®ï¼å¦å»å­¦å½±åãå«æå¾åï¼æ¶çé²æ£æ§ä»éè¿ä¸æ­¥éªè¯ã</li>
<li><strong>å¯¹æ°æ¨¡æçæ©å±æ§:</strong> æ½è±¡ä¸­ä¸»è¦æåè§è§åææ¬ï¼æ¨¡åå¦ä½æ ç¼æ©å±å°å¶ä»æ¨¡æï¼å¦é³é¢ã3Dæ°æ®ãè§¦è§ä¿¡æ¯ï¼å¯è½æ¯ä¸ä¸ªæªæ¥çèéã</li>
</ul>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce OneCAT, a unified multimodal model that seamlessly integrates
understanding, generation, and editing within a novel, pure decoder-only
transformer architecture.</li>
<li>Furthermore, we
pioneer a multi-scale visual autoregressive mechanism within the Large Language
Model (LLM) that drastically reduces decoding steps compared to diffusion-based
methods while maintaining state-of-the-art performance.</li>
<li>As a
result, OneCAT sets a new performance standard, outperforming existing
open-source unified multimodal models across benchmarks for multimodal
generation, editing, and understanding.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="http://arxiv.org/pdf/2509.03498v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.03498v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.03903v1'></a></p>
<h2 id="a-generative-foundation-model-for-chest-radiography"><a href="https://arxiv.org/abs/2509.03903v1">A Generative Foundation Model for Chest Radiography</a></h2>
<p><strong>Authors:</strong> Yuanfeng Ji, Dan Lin, Xiyue Wang, Lu Zhang, Wenhui Zhou, Chongjian Ge, Ruihang Chu, Xiaoli Yang, Junhan Zhao, Junsong Chen, Xiangde Luo, Sen Yang, Jin Fang, Ping Luo, Ruijiang Li</p>
<p><strong>Published:</strong> 2025-09-04</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>The scarcity of well-annotated diverse medical images is a major hurdle for
developing reliable AI models in healthcare. Substantial technical advances
have been made in generative foundation models for natural images. Here we
develop `ChexGen', a generative vision-language foundation model that
introduces a unified framework for text-, mask-, and bounding box-guided
synthesis of chest radiographs. Built upon the latent diffusion transformer
architecture, ChexGen was pretrained on the largest curated chest X-ray dataset
to date, consisting of 960,000 radiograph-report pairs. ChexGen achieves
accurate synthesis of radiographs through expert evaluations and quantitative
metrics. We demonstrate the utility of ChexGen for training data augmentation
and supervised pretraining, which led to performance improvements across
disease classification, detection, and segmentation tasks using a small
fraction of training data. Further, our model enables the creation of diverse
patient cohorts that enhance model fairness by detecting and mitigating
demographic biases. Our study supports the transformative role of generative
foundation models in building more accurate, data-efficient, and equitable
medical AI systems.</p>
<p><strong>Analysis:</strong></p>
<p>è¿ç¯è®ºææè¦å±ç¤ºäºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åå¨å»çAIåºç¨æ¹é¢çä¸ä¸ªéè¦è¿å±ãä»¥ä¸æ¯è¯¦ç»åæï¼</p>
<hr />
<h3 id="1-2-3">1. è®ºæä¸»è¦è´¡ç®çç®ææè¦ (2-3å¥è¯)</h3>
<p>æ¬ææåºäº<code>ChexGen</code>ï¼ä¸ä¸ªç¨äºè¸é¨Xå°çº¿å¾åççæå¼è§è§-è¯­è¨åºç¡æ¨¡åï¼æ¨å¨è§£å³å»å­¦å¾åæ æ³¨ç¨ç¼ºçé®é¢ãå®åºäºæ½å¨æ©æ£Transformeræ¶æï¼è½å¤å®ç°ææ¬ãæ©ç åè¾¹çæ¡å¼å¯¼çå¾ååæï¼å¹¶å¨è¿ä»ä¸ºæ­¢æå¤§çè¸é¨Xå°çº¿æ°æ®éä¸è¿è¡äºé¢è®­ç»ãè¯¥æ¨¡åå¨æ°æ®å¢å¼ºãé¢è®­ç»ä»¥åæåä¸æ¸¸ä»»å¡æ§è½åæ¨¡åå¬å¹³æ§æ¹é¢å±ç°åºå·¨å¤§æ½åï¼é¢ç¤ºççæå¼åºç¡æ¨¡åå¨æå»ºæ´åç¡®ãæ°æ®é«æåå¬å¹³çå»çAIç³»ç»ä¸­çåé©æ§ä½ç¨ã</p>
<h3 id="2_1">2. å³é®åæ°ææ¹æ³å­¦æ¹æ³</h3>
<ul>
<li><strong>ç»ä¸ççææ¡æ¶ï¼</strong> ChexGençæ ¸å¿åæ°å¨äºå¶æä¾äºä¸ä¸ª<strong>ç»ä¸çæ¡æ¶</strong>ï¼è½å¤å®ç°<strong>ææ¬ãæ©ç åè¾¹çæ¡å¼å¯¼</strong>çè¸é¨Xå°çº¿å¾ååæãè¿æå³çç¨æ·å¯ä»¥éè¿å¤ç§æ¨¡æï¼èªç¶è¯­è¨æè¿°ãåºåæ©ç æè¾¹çæ¡ï¼æ¥ç²¾ç¡®æ§å¶å¾åççæåå®¹åç»æï¼è¿å¨å»å­¦å¾åçæé¢åæ¯é«åº¦çµæ´»åæ°é¢çã</li>
<li><strong>çæå¼è§è§-è¯­è¨åºç¡æ¨¡åï¼</strong> å°âçæå¼æ¨¡åâä¸âè§è§-è¯­è¨âè½åç¸ç»åï¼å¹¶å°å¶å®ä½ä¸ºâåºç¡æ¨¡åâï¼è¡¨æå¶æ¨å¨éè¿å¤§è§æ¨¡é¢è®­ç»å­¦ä¹ éç¨è¡¨ç¤ºï¼å¹¶è½éåºå¤ç§ä¸æ¸¸ä»»å¡ã</li>
<li><strong>æ½å¨æ©æ£Transformeræ¶æï¼</strong> éç¨äºå½åæåè¿ççææ¨¡åæ¶æä¹ä¸ââæ½å¨æ©æ£Transformerãè¿ç§æ¶æä»¥å¶é«è´¨éçå¾åçæè½ååå¯¹å¤ææ°æ®åå¸çå»ºæ¨¡è½åèé»åï¼å°å¶åºç¨äºå»å­¦å½±åé¢åæ¯åæ²¿å®è·µã</li>
<li><strong>å¤§è§æ¨¡å»å­¦æ°æ®éé¢è®­ç»ï¼</strong> å¨åå«960,000å¯¹æ¾å°å¾å-æ¥åçâè¿ä»ä¸ºæ­¢æå¤§çâè¸é¨Xå°çº¿æ°æ®éä¸è¿è¡é¢è®­ç»ï¼è¿ä¸ºæ¨¡åå­¦ä¹ å°ä¸°å¯çå»å­¦ç¥è¯åå¾åç¹å¾æä¾äºåå®åºç¡ï¼æ¯å®ç°å¶âåºç¡æ¨¡åâè½åçå³é®ã</li>
</ul>
<h3 id="3_1">3. å¯¹é¢åæ½å¨å½±å</h3>
<ul>
<li><strong>ç¼è§£å»å­¦æ°æ®ç¨ç¼ºæ§ï¼</strong> è¿æ¯æç´æ¥åæ¾èçå½±åãéè¿çæé«è´¨éãå¤æ ·åçåæå»å­¦å¾åï¼ChexGenè½å¤ææè¡¥åçå®æ æ³¨æ°æ®çä¸è¶³ï¼æå¤§å°éä½äºå¼ååè®­ç»é«æ§è½å»çAIæ¨¡åçé¨æ§ã</li>
<li><strong>æåä¸æ¸¸ä»»å¡æ§è½åæ°æ®æçï¼</strong> ä½ä¸ºæ°æ®å¢å¼ºå·¥å·åé¢è®­ç»ç­ç¥ï¼ChexGenè½å¤æ¾èæåç¾çåç±»ãæ£æµååå²ç­ä»»å¡çæ§è½ï¼å°¤å¶æ¯å¨ä»æå°éçå®è®­ç»æ°æ®çæåµä¸ï¼è¿å¯¹äºå¿«éè¿­ä»£åé¨ç½²å»çAIæ¨¡åè³å³éè¦ã</li>
<li><strong>ä¿è¿æ¨¡åå¬å¹³æ§ä¸åè§ç¼è§£ï¼</strong> è½å¤åå»ºå¤æ ·åçæ£èéåï¼ç¨äºæ£æµåç¼è§£AIæ¨¡åä¸­çäººå£ç»è®¡å­¦åè§ï¼è¿å¨å»çAIé¢åå·ææ·±è¿çç¤¾ä¼åä¼¦çæä¹ãå®ä¸ºæå»ºæ´å¬å¹³ãæ´å¼å¾ä¿¡èµçå»çAIç³»ç»æä¾äºå¼ºå¤§çå·¥å·ã</li>
<li><strong>æ¨å¨å»å­¦AIåºç¡æ¨¡ååå±ï¼</strong> ChexGençæåå°æ¿å±æ´å¤ç ç©¶èæ¢ç´¢å¨å¶ä»å»å­¦å½±åæ¨¡æï¼å¦CTãMRIï¼åç¾çé¢åå¼åç±»ä¼¼ççæå¼åºç¡æ¨¡åï¼å éæ´ä¸ªå»å­¦AIé¢åçåå±ã</li>
</ul>
<h3 id="4_1">4. å¯è½åççç¸å³é¢åæåºç¨</h3>
<ul>
<li><strong>å»å­¦å¾ååæä¸è¯æ­ï¼</strong> ç´æ¥åºç¨äºç¾çåç±»ãæ£æµååå²ï¼è¾å©å»çè¿è¡è¯æ­ã</li>
<li><strong>å»çAIæ¨¡åå¼åä¸é¨ç½²ï¼</strong> ä½ä¸ºæ°æ®å¢å¼ºå·¥å·ï¼å éæ°æ¨¡åçè®­ç»åè¿­ä»£ï¼ä½ä¸ºé¢è®­ç»ç­ç¥ï¼æåæ¨¡åå¨å°æ ·æ¬æ°æ®ä¸çæ³åè½åã</li>
<li><strong>æ¨¡åå¬å¹³æ§ä¸åè§ç¼è§£ç ç©¶ï¼</strong> ç¨äºçæå·æç¹å®äººå£ç»è®¡å­¦ç¹å¾çåææ°æ®ï¼ä»¥è¯å«ãéååç¼è§£AIæ¨¡åä¸­çåè§ã</li>
<li><strong>å»çæè²ä¸æ¨¡æï¼</strong> çæåç§ççå¾åç¨äºæå­¦åå»çå¹è®­ï¼æä¾å¤æ ·åçå­¦ä¹ æ¡ä¾ã</li>
<li><strong>éç§ä¿æ¤æ°æ®å±äº«ä¸ç ç©¶ï¼</strong> å¨æäºåºæ¯ä¸ï¼åææ°æ®å¯ä»¥ä½ä¸ºçå®æ°æ®çæ¿ä»£åï¼ç¨äºç ç©¶åå¼åï¼åæ¶ä¿æ¤æ£èéç§ã</li>
<li><strong>ä¸ªæ§åå»çï¼</strong> çè®ºä¸ï¼æªæ¥å¯ä»¥æ ¹æ®ç¹å®æ£èçç¹å¾çæå®å¶åçæ¨¡æå¾åï¼ç¨äºæ²»çæ¹æ¡çè§ååè¯ä¼°ã</li>
</ul>
<h3 id="5_1">5. ä»æè¦ä¸­å¯æ¨æ­çå±éæ§</h3>
<ul>
<li><strong>åæå¾åçä¸´åºçå®æ§ä¸ç»èï¼</strong> å°½ç®¡æè¦å£°ç§°âåç¡®åæâï¼ä½å¯¹äºçæå¾åå¨ä¸´åºä¸æ¯å¦è½å®å¨æ¨¡æçå®çççç»å¾®ç¹å¾ãç½è§çåæå¤æççå±å­çæåµï¼ä»éæ´æ·±å¥çéªè¯ãçææ¨¡åå¯è½å­å¨âå¹»è§âæçæä¸´åºä¸ä¸åççç¹å¾çé£é©ã</li>
<li><strong>æ°æ®éçè¦çèå´ä¸å¤æ ·æ§ï¼</strong> å°½ç®¡ä½¿ç¨äºâè¿ä»ä¸ºæ­¢æå¤§çâæ°æ®éï¼ä½å»å­¦å½±åçå¤ææ§åå¤æ ·æ§æ¯æ éçãè¯¥æ¨¡åå¯è½å¨è®­ç»æ°æ®ä¸­æªååä»£è¡¨çç½è§ç¾çãç¹å®äººç¾¤æå¤æççæ¨¡å¼ä¸è¡¨ç°ä¸ä½³ã</li>
<li><strong>åè§ç¼è§£çå®éææï¼</strong> æè¦æå°è½å¤æ£æµåç¼è§£äººå£ç»è®¡å­¦åè§ï¼ä½æªè¯¦ç»è¯´æå¶ææçéåè¯ä¼°åå±éæ§ãçæå¤æ ·åéåæ¯å¦è½å®å¨æ¶é¤æææ½å¨åè§ï¼ä»¥åæ¯å¦ä¼å¼å¥æ°çåæåè§ï¼ä»æ¯å¼æ¾é®é¢ã</li>
<li><strong>è®¡ç®èµæºéæ±ï¼</strong> åºç¡æ¨¡åéå¸¸éè¦å¤§éçè®¡ç®èµæºè¿è¡è®­ç»åæ¨çï¼è¿å¯è½éå¶å¶å¨èµæºåéç¯å¢ä¸­çå¹¿æ³åºç¨ã</li>
<li><strong>å¯¹ä¸æ¸¸ä»»å¡æ§è½æåçç¨åº¦ï¼</strong> æè¦æåºâä½¿ç¨ä¸å°é¨åè®­ç»æ°æ®âå³å¯æåæ§è½ï¼ä½æªæç¡®è¿ç§æåæ¯å¦è½è¾¾å°æè¶è¶ä½¿ç¨å®æ´çå®æ°æ®éè®­ç»çæ¨¡åæ§è½ï¼ä»¥åâä¸å°é¨åâçå·ä½éåæ åã</li>
</ul>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We demonstrate the utility of ChexGen for training data augmentation
and supervised pretraining, which led to performance improvements across
disease classification, detection, and segmentation tasks using a small
fraction of training data.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="http://arxiv.org/pdf/2509.03903v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.03903v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.04324v1'></a></p>
<h2 id="ovgrasp-open-vocabulary-grasping-assistance-via-multimodal-intent-detection"><a href="https://arxiv.org/abs/2509.04324v1">OVGrasp: Open-Vocabulary Grasping Assistance via Multimodal Intent Detection</a></h2>
<p><strong>Authors:</strong> Chen Hu, Shan Luo, Letizia Gionfrida</p>
<p><strong>Published:</strong> 2025-09-04</p>
<p><strong>Categories:</strong> cs.RO, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Grasping assistance is essential for restoring autonomy in individuals with
motor impairments, particularly in unstructured environments where object
categories and user intentions are diverse and unpredictable. We present
OVGrasp, a hierarchical control framework for soft exoskeleton-based grasp
assistance that integrates RGB-D vision, open-vocabulary prompts, and voice
commands to enable robust multimodal interaction. To enhance generalization in
open environments, OVGrasp incorporates a vision-language foundation model with
an open-vocabulary mechanism, allowing zero-shot detection of previously unseen
objects without retraining. A multimodal decision-maker further fuses spatial
and linguistic cues to infer user intent, such as grasp or release, in
multi-object scenarios. We deploy the complete framework on a custom
egocentric-view wearable exoskeleton and conduct systematic evaluations on 15
objects across three grasp types. Experimental results with ten participants
demonstrate that OVGrasp achieves a grasping ability score (GAS) of 87.00%,
outperforming state-of-the-art baselines and achieving improved kinematic
alignment with natural hand motion.</p>
<p><strong>Analysis:</strong></p>
<p>ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå¯¹è¿ç¯è®ºææè¦çåæå¦ä¸ï¼</p>
<hr />
<h3 id="ovgrasp-open-vocabulary-grasping-assistance-via-multimodal-intent-detection_1">OVGrasp: Open-Vocabulary Grasping Assistance via Multimodal Intent Detection</h3>
<p><strong>1. è®ºæä¸»è¦è´¡ç®çç®ææè¦ (Concise Summary)</strong></p>
<p>OVGrasp æåºäºä¸ä¸ªç¨äºè½¯ä½å¤éª¨éª¼çå±çº§æ§å¶æ¡æ¶ï¼æ¨å¨ä¸ºè¿å¨éç¢èæä¾å¼æ¾è¯æ±æåè¾å©ãå®éè¿æ´å RGB-D è§è§ãå¼æ¾è¯æ±æç¤ºåè¯­é³å½ä»¤ï¼å©ç¨è§è§-è¯­è¨åºç¡æ¨¡åå®ç°å¯¹æªç¥ç©ä½çé¶æ ·æ¬æ£æµï¼å¹¶éè¿å¤æ¨¡æå³ç­å¨æ¨æ­ç¨æ·å¨å¤ç©ä½åºæ¯ä¸­çæåæéæ¾æå¾ãå®éªç»æè¡¨æï¼è¯¥ç³»ç»å¨æåè½ååè¿å¨å¯¹é½æ¹é¢åä¼äºç°æææ¯ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³å­¦æ¹æ³ (Key Innovation or Methodological Approach)</strong></p>
<p>è¯¥è®ºæçæ ¸å¿åæ°å¨äºå¶<strong>å¼æ¾è¯æ±ï¼Open-Vocabularyï¼è½å</strong>å<strong>å¤æ¨¡ææå¾æ£æµ</strong>ã
1.  <strong>å¼æ¾è¯æ±æºå¶ï¼</strong> OVGrasp éæäºä¸ä¸ªè§è§-è¯­è¨åºç¡æ¨¡åï¼ä½¿å¶è½å¤å¯¹ååæªè§çç©ä½è¿è¡é¶æ ·æ¬æ£æµï¼èæ ééæ°è®­ç»ãè¿æå¤§å°å¢å¼ºäºç³»ç»å¨éç»æåãå¤æ ·åç¯å¢ä¸­çæ³åè½åï¼æ¯è®¡ç®æºè§è§é¢åçä¸ä¸ªéè¦è¿å±ï¼å°¤å¶æ¯å¨å®éåºç¨ä¸­ã
2.  <strong>å¤æ¨¡ææå¾å³ç­å¨ï¼</strong> ç³»ç»è®¾è®¡äºä¸ä¸ªå¤æ¨¡æå³ç­å¨ï¼è½å¤èåæ¥èª RGB-D è§è§çç©ºé´çº¿ç´¢åæ¥èªå¼æ¾è¯æ±æç¤º/è¯­é³å½ä»¤çè¯­è¨çº¿ç´¢ï¼ä»èå¨å¤ç©ä½åºæ¯ä¸­åç¡®æ¨æ­ç¨æ·çå·ä½æå¾ï¼å¦æåæéæ¾ï¼ãè¿ç§å¯¹å¤æç¨æ·æå¾ççè§£ï¼è¶è¶äºç®åçç©ä½è¯å«ï¼æ¯äººæºäº¤äºåè¾å©æºå¨äººé¢åçå³é®çªç ´ã
3.  <strong>éææ¡æ¶ï¼</strong> å°è¿äºåè¿ç CV/NLP ææ¯ä¸å®å¶çãä½©æ´å¼ãç¬¬ä¸äººç§°è§è§çè½¯ä½å¤éª¨éª¼ç¸ç»åï¼å½¢æä¸ä¸ªå®ç¨çãç«¯å°ç«¯çå±çº§æ§å¶ç³»ç»ï¼å®ç°äºä»æç¥å°å³ç­åå°æ§è¡çé­ç¯ã</p>
<p><strong>3. å¯¹é¢åæ½å¨å½±å (Potential Impact on the Field)</strong></p>
<ol>
<li><strong>è¾å©æºå¨äººä¸äººæºäº¤äº (Assistive Robotics &amp; HRI)ï¼</strong> OVGrasp ç´æ¥è§£å³äºè¿å¨éç¢èå¨æ¥å¸¸çæ´»ä¸­æ¢å¤èªä¸»æ§çå³é®ææï¼éè¿æä¾æ´èªç¶ãç´è§ä¸éåºæ§å¼ºçæåè¾å©ï¼å°æå¤§å°æ¹åä»ä»¬ççæ´»è´¨éãå®ä¸ºæªæ¥è¾å©æºå¨äººç³»ç»çè®¾è®¡æä¾äºæ°çèå¼ã</li>
<li><strong>è®¡ç®æºè§è§ä¸è§è§-è¯­è¨æ¨¡å (Computer Vision &amp; Vision-Language Models)ï¼</strong> è¯¥ç ç©¶å±ç¤ºäºè§è§-è¯­è¨åºç¡æ¨¡åå¨å®éãå·èº«ï¼embodiedï¼æºå¨äººåºç¨ä¸­çå¼ºå¤§æ½åï¼ç¹å«æ¯å¨å¼æ¾ä¸çãé¶æ ·æ¬ç©ä½è¯å«åçè§£æ¹é¢ãå®æ¨å¨äº CV é¢åä»éæå¾åè¯å«åå¨æãäº¤äºå¼ãå¤æ¨¡ææç¥çæ¼è¿ã</li>
<li><strong>å¤æ¨¡æäººå·¥æºè½ (Multimodal AI)ï¼</strong> è®ºæå¨èåç©ºé´åè¯­è¨ä¿¡æ¯ä»¥æ¨æ­å¤æç¨æ·æå¾æ¹é¢åå¾äºè¿å±ï¼ä¸ºå¤æ¨¡æå­¦ä¹ åå³ç­å¶å®æä¾äºæ°çæè·¯åå®è¯ã</li>
<li><strong>è½¯ä½æºå¨äººä¸å¯ç©¿æ´è®¾å¤ (Soft Robotics &amp; Wearable Devices)ï¼</strong> ç»åè½¯ä½å¤éª¨éª¼åç¬¬ä¸äººç§°è§è§è§è§ï¼ä¸ºå¯ç©¿æ´æºå¨äººç³»ç»çè®¾è®¡åæ§å¶æä¾äºå®è´µçç»éªåææ¯åèã</li>
</ol>
<p><strong>4. å¯è½åçäºæ­¤ç ç©¶çç¸å³é¢åæåºç¨ (Related Areas or Applications)</strong></p>
<ol>
<li><strong>éç¨åæºå¨äººæä½ (General-purpose Robotic Manipulation)ï¼</strong> æåæºå¨äººå¨éç»æåç¯å¢ä¸­å¤çå¤æ ·åç©ä½çè½åï¼ä¾å¦å¨ç©æµãä»å¨ææå¡æºå¨äººé¢åã</li>
<li><strong>äººæºåä½ (Human-Robot Collaboration)ï¼</strong> æ¹è¿æºå¨äººå¯¹äººç±»æä»¤åæå¾ççè§£ï¼å®ç°æ´æµçãæ´å®å¨çåä½ï¼å°¤å¶æ¯å¨å·¥ä¸æå»çåºæ¯ä¸­ã</li>
<li><strong>è¿ç¨æä½ä¸æ¢ç´¢ (Teleoperation &amp; Exploration)ï¼</strong> ä¸ºè¿ç¨æ§å¶ç³»ç»æä¾æ´æºè½çç©ä½è¯å«åæå¾æ¨æ­è½åï¼åå°æä½åçè®¤ç¥è´æã</li>
<li><strong>æºè½å®¶å±ä¸æºæ§å»ç (Smart Home &amp; Smart Healthcare)ï¼</strong> å°å¼æ¾è¯æ±åå¤æ¨¡æäº¤äºè½åéæå°å¶ä»æºè½è®¾å¤ä¸­ï¼æä¾æ´å¹¿æ³çæºè½è¾å©æå¡ã</li>
<li><strong>å¢å¼ºç°å®/èæç°å® (Augmented Reality/Virtual Reality)ï¼</strong> å¨ AR/VR ç¯å¢ä¸­å®ç°æ´èªç¶çç©ä½äº¤äºåç¨æ·æå¾çè§£ã</li>
</ol>
<p><strong>5. ä»æè¦ä¸­å¯æ¨æ­åºçå±éæ§ (Limitations that can be inferred from the abstract)</strong></p>
<ol>
<li><strong>è¯ä¼°èå´çå±éæ§ (Limited Evaluation Scope):</strong> å°½ç®¡å¨15ä¸ªç©ä½å3ç§æåç±»åä¸è¿è¡äºç³»ç»è¯ä¼°ï¼å¹¶æ10ååä¸èï¼ä½ä¸âå¤æ ·åä¸ä¸å¯é¢æµâççå®ä¸çéç»æåç¯å¢ç¸æ¯ï¼è¿ä»ç¶æ¯ä¸ä¸ªç¸å¯¹æéçæµè¯éãå¼æ¾è¯æ±è½åè½å¼ºï¼ä½å¶å¨æç«¯å¤æ ·æ§ãç½è§ç©ä½æé«åº¦ç¸ä¼¼ç©ä½åºåä¸çæ³åè½åä»éæ´å¹¿æ³ãæ´ä¸¥èçéªè¯ã</li>
<li><strong>æå¾è¯å«çå¤ææ§ (Complexity of Intent Recognition):</strong> ç®åçæå¾è¯å«ä¸»è¦éä¸­å¨âæåâæâéæ¾âä¸¤ç§åºæ¬å¨ä½ãå¨å®éåºç¨ä¸­ï¼ç¨æ·çæå¾å¯è½æ´ä¸ºå¤æåç»è´ï¼ä¾å¦ï¼âè½»è½»æåâãâç§»å¨å°æä¸ªä½ç½®âãâæåçº¢è²çé£ä¸ªâï¼ï¼è¿å¯è½éè¦æ´é«çº§ãæ´å·ä¸ä¸ææç¥è½åçæå¾çè§£æ¨¡åã</li>
<li><strong>ç¯å¢é²æ£æ§ (Environmental Robustness):</strong> æè¦æªè¯¦ç»è¯´æç³»ç»å¨æç«¯åç§ååãä¸¥éé®æ¡ãé«åº¦æä¹±ãåå«éæ/ååç©ä½ç­å¤æçå®ä¸çæ¡ä»¶ä¸çè¡¨ç°ãè¿äºæ¯ RGB-D è§è§åè§è§-è¯­è¨æ¨¡åå¨å®éé¨ç½²ä¸­å¸¸è§çææã</li>
<li><strong>è®¡ç®èµæºä¸å®æ¶æ§ (Computational Resources and Real-time Performance):</strong> è§è§-è¯­è¨åºç¡æ¨¡åéå¸¸è®¡ç®éå¤§ãå¯¹äºä¸ä¸ªå¯ç©¿æ´ãå®æ¶ååºçç³»ç»ï¼å¶è®¡ç®å¼éãåèåå»¶è¿æ¯å³é®èéï¼æè¦ä¸­æªæåè¿äºæ§è½ææ ã</li>
<li><strong>ç¨æ·éåºæ§ä¸ä¸ªæ§å (User Adaptability &amp; Personalization):</strong> æè¦æå°âæ¹è¿äºä¸èªç¶æé¨è¿å¨çè¿å¨å¯¹é½âï¼ä½æªè¯¦ç»è¯´æç³»ç»å¦ä½éåºä¸åç¨æ·çççå·®å¼ãåå¥½æå­¦ä¹ æ²çº¿ãä¸ªæ§åè°æ´å¯¹äºè¾å©è®¾å¤è³å³éè¦ã</li>
</ol>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present
OVGrasp, a hierarchical control framework for soft exoskeleton-based grasp
assistance that integrates RGB-D vision, open-vocabulary prompts, and voice
commands to enable robust multimodal interaction.</li>
<li>Experimental results with ten participants
demonstrate that OVGrasp achieves a grasping ability score (GAS) of 87.00%,
outperforming state-of-the-art baselines and achieving improved kinematic
alignment with natural hand motion.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="http://arxiv.org/pdf/2509.04324v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.04324v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.03006v1'></a></p>
<h2 id="enhancing-robustness-in-post-processing-watermarking-an-ensemble-attack-network-using-cnns-and-transformers"><a href="https://arxiv.org/abs/2509.03006v1">Enhancing Robustness in Post-Processing Watermarking: An Ensemble Attack Network Using CNNs and Transformers</a></h2>
<p><strong>Authors:</strong> Tzuhsuan Huang, Cheng Yu Yeo, Tsai-Ling Huang, Hong-Han Shuai, Wen-Huang Cheng, Jun-Cheng Chen</p>
<p><strong>Published:</strong> 2025-09-03</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Recent studies on deep watermarking have predominantly focused on
in-processing watermarking, which integrates the watermarking process into
image generation. However, post-processing watermarking, which embeds
watermarks after image generation, offers more flexibility. It can be applied
to outputs from any generative model (e.g. GANs, diffusion models) without
needing access to the model's internal structure. It also allows users to embed
unique watermarks into individual images. Therefore, this study focuses on
post-processing watermarking and enhances its robustness by incorporating an
ensemble attack network during training. We construct various versions of
attack networks using CNN and Transformer in both spatial and frequency domains
to investigate how each combination influences the robustness of the
watermarking model. Our results demonstrate that combining a CNN-based attack
network in the spatial domain with a Transformer-based attack network in the
frequency domain yields the highest robustness in watermarking models.
Extensive evaluation on the WAVES benchmark, using average bit accuracy as the
metric, demonstrates that our ensemble attack network significantly enhances
the robustness of baseline watermarking methods under various stress tests. In
particular, for the Regeneration Attack defined in WAVES, our method improves
StegaStamp by 18.743%. The code is released
at:https://github.com/aiiu-lab/DeepRobustWatermark.</p>
<p><strong>Analysis:</strong></p>
<p>è¿ç¯è®ºææ·±å¥æ¢è®¨äºåå¤çæ°å­æ°´å°çé²æ£æ§å¢å¼ºé®é¢ï¼ä»¥ä¸æ¯è¯¦ç»åæï¼</p>
<h3 id="1-concise-summary_1">1. è®ºæä¸»è¦è´¡ç®çç®ææè¦ (Concise Summary)</h3>
<p>æ¬æä¸æ³¨äºæååå¤çæ°å­æ°´å°çé²æ£æ§ï¼æåºäºä¸ç§å¨è®­ç»é¶æ®µå©ç¨éææ»å»ç½ç»çæ¹æ³ãè¯¥æ¹æ³ç»åäºåºäºCNNçç©ºé´åæ»å»ååºäºTransformerçé¢çåæ»å»ï¼å®éªè¯æè¿ç§ç»åè½æ¾èå¢å¼ºæ°´å°æ¨¡åçææ»å»è½åï¼å¹¶å¨WAVESåºåæµè¯ä¸­å±ç°åºä¼å¼æ§è½ï¼å°¤å¶å¨åçæ»å»ä¸å¯¹ç°ææ¹æ³ææ¾èæåã</p>
<h3 id="2-key-innovation-or-methodological-approach_2">2. å³é®åæ°ææ¹æ³è®º (Key Innovation or Methodological Approach)</h3>
<p>æ ¸å¿åæ°å¨äºæåºäºä¸ç§æ°é¢çâéææ»å»ç½ç»âï¼Ensemble Attack Networkï¼æ¹æ³ï¼ç¨äºå¨è®­ç»é¶æ®µæååå¤çæ°´å°çé²æ£æ§ãè¯¥æ¹æ³éè¿ç»åä¸åæ¶æï¼CNNåTransformerï¼åä¸åä½ç¨åï¼ç©ºé´ååé¢çåï¼çæ»å»æ¨¡åï¼æå»ºäºä¸ä¸ªå¤æ ·åçæ»å»éæä½ãè¿ç§éææ»å»ç­ç¥è¿«ä½¿æ°´å°æ¨¡åå­¦ä¹ å¯¹å¤ç§æ½å¨æ»å»æ´å·æµæåçç¹å¾ï¼ä»èæ¾èå¢å¼ºå¶é²æ£æ§ãè®ºæè¿éè¿å®éªåç°ï¼å°åºäºCNNçç©ºé´åæ»å»ä¸åºäºTransformerçé¢çåæ»å»ç¸ç»åï¼è½è¾¾å°æä½³çé²æ£æ§ææã</p>
<h3 id="3-potential-impact-on-the-field_2">3. å¯¹é¢åæ½å¨å½±å (Potential Impact on the Field)</h3>
<p>æ¬ç ç©¶æ¾èæåäºåå¤çæ°å­æ°´å°çå®ç¨æ§åå¯é æ§ï¼ä½¿å¶è½æ´ææå°åºç¨äºä»»ä½çææ¨¡åï¼å¦GANsãæ©æ£æ¨¡åï¼çè¾åºï¼èæ éè®¿é®å¶åé¨ç»æãè¿å¯¹äºAIçæåå®¹ççæä¿æ¤ãæº¯æºãçå®æ§éªè¯ä»¥åæå»æ·±åº¦ä¼ªé ç­é¢åå·æéè¦æä¹ãå®ä¹ä¸ºæªæ¥æ°´å°ææ¯ï¼ç¹å«æ¯å¯¹ææ§é²æ£æ§ç ç©¶ï¼æä¾äºä¸ä¸ªæ°çãææçè®­ç»èå¼ï¼å³éè¿éæå¤æ ·åæ»å»è¿è¡å¯¹æè®­ç»ã</p>
<h3 id="4-related-areas-or-applications_1">4. ç¸å³é¢åæåºç¨ (Related Areas or Applications)</h3>
<ul>
<li><strong>AIçæåå®¹ççæä¿æ¤ä¸æº¯æºï¼</strong> ç¡®ä¿AIçæå¾åãè§é¢ç­åå®¹çååæ§å½å±ï¼é²æ­¢æªç»ææçä½¿ç¨ã</li>
<li><strong>æ°å­åªä½çå®æ§éªè¯ä¸é²ç¯¡æ¹ï¼</strong> éªè¯å¾åæè§é¢æ¯å¦è¢«ç¯¡æ¹ï¼å°¤å¶æ¯å¨æ°é»ãæ³å¾ãå»çç­å¯¹åå®¹çå®æ§è¦æ±æé«çé¢åã</li>
<li><strong>æ·±åº¦ä¼ªé ï¼Deepfakeï¼æ£æµä¸æº¯æºï¼</strong> éè¿åµå¥æ°´å°æ¥è¯å«åè¿½è¸ªåæåå®¹ï¼å¯¹ææ¶ææ·±åº¦ä¼ªé ã</li>
<li><strong>æ°å­åè¯ï¼</strong> å¨ç½ç»ç¯ç½ªè°æ¥ä¸­æä¾åå®¹æ¥æºåä¿®æ¹åå²ççº¿ç´¢ã</li>
<li><strong>ç¥è¯äº§æç®¡çï¼</strong> ä¿æ¤æ°å­èºæ¯åååæåå®¹çç¥è¯äº§æã</li>
</ul>
<h3 id="5-limitations-inferable-from-the-abstract">5. ä»æè¦ä¸­å¯æ¨æ­çå±éæ§ (Limitations Inferable from the Abstract)</h3>
<ul>
<li><strong>é²æ£æ§çç»å¯¹ä¸éï¼</strong> å°½ç®¡æ¾èæåäºé²æ£æ§ï¼ä½ä»»ä½æ°´å°ç³»ç»é½æ æ³ä¿è¯å¯¹æææ½å¨æ»å»çç»å¯¹æµæåï¼æªæ¥å¯è½åºç°æ´å¤æçæ»å»ã</li>
<li><strong>è®¡ç®èµæºæ¶èï¼</strong> è®­ç»éææ»å»ç½ç»ï¼ç¹å«æ¯ç»åäºCNNåTransformerçå¤ææ¨¡åï¼å¯è½ä¼å¸¦æ¥è¾é«çè®¡ç®èµæºåæ¶é´ææ¬ã</li>
<li><strong>å¯¹æªç¥æ»å»çæ³åè½åï¼</strong> å°½ç®¡éæäºå¤æ ·åæ»å»ï¼ä½å¶å¯¹è®­ç»éä¸­æªåå«çãå¨æ°ç±»åçæ»å»çæ³åè½åä»éè¿ä¸æ­¥éªè¯ã</li>
<li><strong>è¯ä¼°èå´ï¼</strong> è¯ä¼°ä¸»è¦åºäºWAVESåºåæµè¯åå¹³åæ¯ç¹åç¡®çï¼å¶å¨å¶ä»ç¹å®åºæ¯æä½¿ç¨å¶ä»è¯ä¼°ææ æ¶çè¡¨ç°å¯è½ææä¸åã</li>
<li><strong>ä»éäºåå¤çæ°´å°ï¼</strong> æ¬ç ç©¶ä¸æ³¨äºåå¤çæ°´å°ï¼å¶ç»è®ºåæ¹æ³ä¸ç´æ¥éç¨äºå¨å¾åçæè¿ç¨ä¸­åµå¥æ°´å°çâåå¤çæ°´å°âåºæ¯ã</li>
</ul>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In
particular, for the Regeneration Attack defined in WAVES, our method improves
StegaStamp by 18.743%.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="http://arxiv.org/pdf/2509.03006v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.03006v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.04446v1'></a></p>
<h2 id="plotn-polish-zero-shot-story-visualization-and-disentangled-editing-with-text-to-image-diffusion-models"><a href="https://arxiv.org/abs/2509.04446v1">Plot'n Polish: Zero-shot Story Visualization and Disentangled Editing with Text-to-Image Diffusion Models</a></h2>
<p><strong>Authors:</strong> Kiymet Akdemir, Jing Shi, Kushal Kafle, Brian Price, Pinar Yanardag</p>
<p><strong>Published:</strong> 2025-09-04</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Text-to-image diffusion models have demonstrated significant capabilities to
generate diverse and detailed visuals in various domains, and story
visualization is emerging as a particularly promising application. However, as
their use in real-world creative domains increases, the need for providing
enhanced control, refinement, and the ability to modify images post-generation
in a consistent manner becomes an important challenge. Existing methods often
lack the flexibility to apply fine or coarse edits while maintaining visual and
narrative consistency across multiple frames, preventing creators from
seamlessly crafting and refining their visual stories. To address these
challenges, we introduce Plot'n Polish, a zero-shot framework that enables
consistent story generation and provides fine-grained control over story
visualizations at various levels of detail.</p>
<p><strong>Analysis:</strong></p>
<p>ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå¯¹è¿ç¯è®ºææè¦è¿è¡å¦ä¸åæï¼</p>
<hr />
<h3 id="1-concise-summary_2">1. è®ºæä¸»è¦è´¡ç®çç®ææ»ç» (Concise Summary)</h3>
<p>æ¬ææåºäº Plot'n Polishï¼ä¸ä¸ªé¶æ ·æ¬ï¼zero-shotï¼æ¡æ¶ï¼æ¨å¨è§£å³å½åææ¬å°å¾åæ©æ£æ¨¡åå¨æäºå¯è§åä¸­ç¼ºä¹ä¸è´æ§æ§å¶ååæç¼è¾çµæ´»æ§çé®é¢ãå®å®ç°äºè·¨å¤å¸§çè§è§ååäºä¸è´æ§æäºçæï¼å¹¶æä¾äºå¯¹æäºå¯è§åå¨ä¸åç»èå±çº§çç»ç²åº¦æ§å¶åè§£è¦ç¼è¾è½åï¼ä»èä½¿åä½èè½å¤æ ç¼å°ç²¾ä¿®å¶è§è§æäºã</p>
<h3 id="2-key-innovation-or-methodological-approach_3">2. å³é®åæ°ææ¹æ³å­¦æ¹æ³ (Key Innovation or Methodological Approach)</h3>
<p>æ ¸å¿åæ°å¨äºå¶<strong>é¶æ ·æ¬ï¼zero-shotï¼</strong>ç¹æ§ï¼è¿æå³çè¯¥æ¡æ¶æ ééå¯¹ç¹å®æäºæç¼è¾ä»»å¡è¿è¡é¢å¤è®­ç»ãæ´éè¦çæ¯ï¼å®å¼å¥äºä¸ç§<strong>è§£è¦ç¼è¾ï¼disentangled editingï¼</strong>çæ¹æ³ï¼ä»æ é¢æ¨æ­ï¼ï¼ä½¿å¾ç¨æ·è½å¤å¯¹æäºå¯è§åè¿è¡<strong>ç»ç²åº¦ï¼fine-grainedï¼</strong>æ§å¶ï¼å¹¶å¨<strong>ä¸åç»èå±çº§</strong>ï¼ä»æ´ä½åäºå°å±é¨åç´ ï¼ä¸è¿è¡ä¿®æ¹ï¼åæ¶ç¡®ä¿<strong>è·¨å¤å¸§çè§è§ååäºä¸è´æ§</strong>ãè¿ç§å¨ä¿æä¸è´æ§åæä¸çå¤å±çº§ãè§£è¦æ§å¶æ¯ç°ææ¹æ³ææ¬ ç¼ºçï¼å®è§£å³äºå¨æäºçæä¸­ï¼è§è²ãåºæ¯ãé£æ ¼ç­åç´ å¨ä¸åå¸§ä¹é´ä¿æè¿è´¯æ§çæ ¸å¿ææã</p>
<h3 id="3-potential-impact-on-the-field_3">3. å¯¹é¢åæ½å¨å½±å (Potential Impact on the Field)</h3>
<p>è¯¥ç ç©¶å°æ¾èæåææ¬å°å¾åæ©æ£æ¨¡åå¨<strong>æäºå¯è§å</strong>é¢åçå®ç¨æ§åå¯æ§æ§ãå®ä¸ºåä½èæä¾äºåææªæççµæ´»æ§ï¼ä½¿å¶è½å¤<strong>æ ç¼å°åä½ãè¿­ä»£åç²¾ä¿®è§è§æäº</strong>ï¼ä»èéä½äºé«è´¨éè§è§åäºåå®¹çåä½é¨æ§ãè¿ä¸ä»è½æ¨å¨<strong>åæäº§ä¸</strong>ï¼å¦å¨ç»ãæ¼«ç»ãæ¸¸ææ¦å¿µèºæ¯ãå¹¿åï¼çåå±ï¼ä¹ä¸ºæªæ¥æ´å¤æç<strong>äººæºåä½åå®¹çæ</strong>æ¨¡å¼å¥ å®äºåºç¡ï¼ä½¿AIå·¥å·ä»åçº¯ççæå¨è½¬åä¸ºæ´å¼ºå¤§çåæå©æï¼è½å¤çè§£å¹¶ååºç¨æ·å¨åäºåè§è§ç¼è¾ä¸çå¤ææå¾ã</p>
<h3 id="4-related-areas-or-applications_2">4. ç¸å³é¢åæåºç¨ (Related Areas or Applications)</h3>
<ul>
<li><strong>åæåå®¹çæ:</strong> å¨ç»ãæ¼«ç»ãçµå½±é¢å¯è§åï¼pre-visualizationï¼ãæ¸¸ææ¦å¿µèºæ¯ãå¹¿ååæãæ°å­æç»ã</li>
<li><strong>ä¸ªæ§ååå®¹:</strong> æ ¹æ®ç¨æ·è¾å¥çæå®å¶åçæäºãæè²æææäº¤äºå¼ä½éªã</li>
<li><strong>èæç°å®/å¢å¼ºç°å® (VR/AR):</strong> å¿«éçæåè¿­ä»£èæåºæ¯æè§è²èµäº§ï¼ä¿æå¶å¨ä¸åäº¤äºç¶æä¸çä¸è´æ§ã</li>
<li><strong>æ°å­äºº/èæå¶å:</strong> ä¿ææ°å­è§è²å¨ä¸åå§¿æãè¡¨æååºæ¯ä¸­çä¸è´æ§ï¼å¹¶è¿è¡ç²¾ç»åç¼è¾ã</li>
<li><strong>å¤æ¨¡æåå®¹çè§£ä¸çæ:</strong> ä¸ºææ¬æäºèªå¨éå¾æçæè§é¢èæ¬ï¼å¹¶åè®¸ç¨æ·å¯¹çæç»æè¿è¡ç²¾ç»è°æ´ã</li>
</ul>
<h3 id="5-limitations-that-can-be-inferred-from-the-abstract_1">5. å¯ä»æè¦æ¨æ­çå±éæ§ (Limitations that can be inferred from the abstract)</h3>
<ul>
<li><strong>é¶æ ·æ¬æ¹æ³çå±éæ§:</strong> å°½ç®¡é¶æ ·æ¬æ¯ä¼å¿ï¼ä½å¯¹äºé«åº¦ç¹å®ãé£æ ¼åæéè¦æé«ç»èä¿çåº¦çåºæ¯ï¼å¶æ§è½å¯è½ä¸å¦ç»è¿ç¹å®æ°æ®å¾®è°çæ¨¡åãå¨æäºæç«¯æåµä¸ï¼é¶æ ·æ¬æ¹æ³å¯è½é¾ä»¥ææå°éå¸¸ç»å¾®æç¬ç¹çè§è§ç¹å¾ã</li>
<li><strong>ä¸è´æ§ä¿æçé²æ£æ§:</strong> æè¦ä¸­å¼ºè°äºâä¸è´æ§âï¼ä½å¯¹äºæå¶å¤æãåäºè·¨åº¦é¿æè§è²/åºæ¯åçå§çååçæäºï¼ç»´æå®ç¾çä¸è´æ§ä»æ¯ä¸ä¸ªå·¨å¤§ææãæ¨¡åå¦ä½å¤çè§è²æè£ãååãé¢é¨ç¹å¾å¨ä¸åå¸§ä¸­çç»å¾®ååï¼ä»¥åèæ¯ç¯å¢çè¿è´¯æ§ï¼æ¯éè¦éªè¯çã</li>
<li><strong>âç»ç²åº¦æ§å¶âçå®éè¾¹ç:</strong> æè¦ä¸­æå°âç»ç²åº¦æ§å¶âï¼ä½å¶å·ä½è½è¾¾å°ä½ç§ç¨åº¦çç²¾ç»åç¼è¾ï¼ä¾å¦ï¼è½å¦ç²¾ç¡®ä¿®æ¹æä¸ªè§è²çå¾®å°è¡¨æãç¹å®éå·çç»èï¼ï¼ä»¥åè¿ç§æ§å¶çç´è§æ§/æç¨æ§ï¼ä»éå¨å®éåºç¨ä¸­æ£éªãè§£è¦ç¼è¾çè´¨éç´æ¥å½±åè¿ä¸ç¹ï¼å¦æè§£è¦ä¸å½»åºï¼å¯è½ä¼å¨ç¼è¾ä¸ä¸ªåç´ æ¶æå¤å½±åå°å¶ä»åç´ ã</li>
<li><strong>è®¡ç®èµæºä¸æç:</strong> æ©æ£æ¨¡åéå¸¸è®¡ç®ææ¬è¾é«ï¼å°¤å¶æ¯å¨è¿è¡å¤å¸§çæåè¿­ä»£ç¼è¾æ¶ãæè¦ä¸­æªæåæ§è½æéåº¦ï¼è¿å¯è½æ¯å®éåºç¨ä¸­çä¸ä¸ªæ½å¨éå¶ï¼ç¹å«æ¯å¨éè¦å¿«éè¿­ä»£çåæå·¥ä½æµä¸­ã</li>
</ul>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address these
challenges, we introduce Plot'n Polish, a zero-shot framework that enables
consistent story generation and provides fine-grained control over story
visualizations at various levels of detail.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="http://arxiv.org/pdf/2509.04446v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.04446v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.04268v1'></a></p>
<h2 id="differential-morphological-profile-neural-networks-for-semantic-segmentation"><a href="https://arxiv.org/abs/2509.04268v1">Differential Morphological Profile Neural Networks for Semantic Segmentation</a></h2>
<p><strong>Authors:</strong> David Huangal, J. Alex Hurt</p>
<p><strong>Published:</strong> 2025-09-04</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Semantic segmentation of overhead remote sensing imagery enables applications
in mapping, urban planning, and disaster response. State-of-the-art
segmentation networks are typically developed and tuned on ground-perspective
photographs and do not directly address remote sensing challenges such as
extreme scale variation, foreground-background imbalance, and large image
sizes. We explore the incorporation of the differential morphological profile
(DMP), a multi-scale shape extraction method based on grayscale morphology,
into modern segmentation networks. Prior studies have shown that the DMP can
provide critical shape information to Deep Neural Networks to enable superior
detection and classification performance in overhead imagery. In this work, we
extend prior DMPNet work beyond classification and object detection by
integrating DMP features into three state-of-the-art convolutional and
transformer semantic segmentation architectures. We utilize both direct input,
which adapts the input stem of feature extraction architectures to accept DMP
channels, and hybrid architectures, a dual-stream design that fuses RGB and DMP
encoders. Using the iSAID benchmark dataset, we evaluate a variety of DMP
differentials and structuring element shapes to more effectively provide shape
information to the model. Our results show that while non-DMP models generally
outperform the direct-input variants, hybrid DMP consistently outperforms
direct-input and is capable of surpassing a non-DMP model on mIoU, F1, and
Recall.</p>
<p><strong>Analysis:</strong></p>
<p>ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå¯¹è¿ç¯è®ºææè¦çåæå¦ä¸ï¼</p>
<hr />
<h3 id="differential-morphological-profile-neural-networks-for-semantic-segmentation_1">è®ºææè¦åæï¼Differential Morphological Profile Neural Networks for Semantic Segmentation</h3>
<p><strong>1. è®ºæä¸»è¦è´¡ç®çç®æ´æ»ç» (2-3 å¥è¯)</strong></p>
<p>æ¬ææ¨å¨è§£å³é¥æå¾åè¯­ä¹åå²ä¸­å­å¨çæç«¯å°ºåº¦ååãåæ¯èæ¯ä¸å¹³è¡¡ç­ææï¼æåºå°å·®åå½¢æå­¦åé¢ï¼DMPï¼è¿ä¸å¤å°ºåº¦å½¢ç¶æåæ¹æ³èå¥å°åè¿çè¯­ä¹åå²ç½ç»ä¸­ãç ç©¶è¡¨æï¼éè¿RGBåDMPç¼ç å¨èåçåæµâæ··åæ¶æâï¼è½å¤æ¾èæåæ¨¡åæ§è½ï¼å¨mIoUãF1åRecallç­ææ ä¸è¶è¶éDMPåºçº¿æ¨¡åï¼ä¸ºé¥æå¾ååææä¾äºæ´é²æ£çè§£å³æ¹æ¡ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³å­¦æ¹æ³</strong></p>
<p>æ¬æçå³é®åæ°å¨äºå°å·®åå½¢æå­¦åé¢ï¼DMPï¼ââä¸ç§åºäºç°åº¦å½¢æå­¦çå¤å°ºåº¦å½¢ç¶æåæ¹æ³ââé¦æ¬¡æ©å±å¹¶æååºç¨äºè¯­ä¹åå²ä»»å¡ï¼è¶è¶äºå¶å¨åç±»åç®æ æ£æµä¸­çä¼ ç»åºç¨ãå¶æ ¸å¿æ¹æ³å­¦è´¡ç®æ¯æåºäºä¸¤ç§DMPéæç­ç¥ï¼
*   <strong>ç´æ¥è¾å¥ (Direct Input)</strong>ï¼è°æ´ç¹å¾æåæ¶æçè¾å¥å±ä»¥æ¥åDMPééã
*   <strong>æ··åæ¶æ (Hybrid Architectures)</strong>ï¼ä¸ç§æ´ææçåæµè®¾è®¡ï¼åå«ä½¿ç¨RGBåDMPç¼ç å¨ï¼å¹¶å°ä¸¤èçç¹å¾è¿è¡èåã
éè¿å¨iSAIDæ°æ®éä¸å¯¹ä¸åDMPå·®ååç»æåç´ å½¢ç¶çç³»ç»è¯ä¼°ï¼è¯æäºæ··åæ¶æå¨æä¾å³é®å½¢ç¶ä¿¡æ¯æ¹é¢çä¼è¶æ§ã</p>
<p><strong>3. å¯¹é¢åæ½å¨å½±å</strong></p>
<ul>
<li><strong>æåé¥æå¾ååæç²¾åº¦ï¼</strong> æ¬ç ç©¶ç´æ¥è§£å³äºé¥æå¾åè¯­ä¹åå²çåºæææï¼æææ¾èæé«å°å¾ç»å¶ãåå¸è§ååç¾å®³ååºç­åºç¨ä¸­çèªå¨ååç²¾åº¦ã</li>
<li><strong>éæ°å®¡è§ç¹å¾å·¥ç¨çä»·å¼ï¼</strong> å¨æ·±åº¦å­¦ä¹ ä¸»å¯¼çæ¶ä»£ï¼æ¬æå¼ºè°äºç»åä¼ ç»ãé¢åç¹å®ï¼å¦å½¢æå­¦ï¼ç¹å¾ä¸æ·±åº¦å­¦ä¹ æ¨¡åçæ½åï¼ä¸ºæ··åæ¨¡åè®¾è®¡æä¾äºæ°çæè·¯ã</li>
<li><strong>å¯åå¤æ¨¡æ/å¤ç¹å¾èåï¼</strong> æåèåDMPåRGBä¿¡æ¯ï¼å¯è½å¯åç ç©¶äººåæ¢ç´¢å°å¶ä»äºè¡¥çãéåç´ çº§ç¹å¾ï¼å¦é«ç¨ãåè°±ææ°ç­ï¼èå¥æ·±åº¦å­¦ä¹ æ¨¡åï¼ä»¥åºå¯¹æ´å¤æçè§è§ä»»å¡ã</li>
<li><strong>æ¨å¨DMPå¨æ´å¹¿æ³CVä»»å¡ä¸­çåºç¨ï¼</strong> è¯æDMPå¨è¯­ä¹åå²ä¸­çæææ§ï¼å¯è½ä¼ä¿ä½¿DMPå¨å¶ä»éè¦ç²¾ç»å½¢ç¶çè§£çè®¡ç®æºè§è§ä»»å¡ä¸­å¾å°æ´å¹¿æ³çæ¢ç´¢ã</li>
</ul>
<p><strong>4. å¯è½åççç¸å³é¢åæåºç¨</strong></p>
<ul>
<li><strong>å°çä¿¡æ¯ç³»ç» (GIS) åæµç»ï¼</strong> æé«å°ç©åç±»ãåå°è¦çå¶å¾åååæ£æµçèªå¨åæ°´å¹³ååç¡®æ§ã</li>
<li><strong>æºæ§åå¸ååå¸è§åï¼</strong> ç²¾åè¯å«å»ºç­ç©ãéè·¯ãç»¿å°ç­ï¼æ¯æåå¸åºç¡è®¾æ½ç®¡çååå±è§åã</li>
<li><strong>ç¾å®³ååºä¸ç®¡çï¼</strong> å¿«éåç¡®å°è¯ä¼°ç¾æï¼å¦æ´ªæ°´æ·¹æ²¡åºåãåæå»ºç­ç©è¯å«ï¼è¾å©ææ´å³ç­ã</li>
<li><strong>ç¯å¢çæµï¼</strong> çæµæ£®æç ä¼ãå°å·èåãåä½ç©å¥åº·ç¶åµç­ï¼æä¾ç²¾ç»åçç¯å¢æ°æ®ã</li>
<li><strong>åä¸é¥æï¼</strong> ç²¾åè¯å«åä½ç©ç±»åãçé¿åºåï¼æ¯ææºè½åä¸ç®¡çã</li>
<li><strong>å½é²ä¸ææ¥ï¼</strong> æåå¯¹åäºè®¾æ½ãäº¤éç½ç»ç­ç®æ çè¯å«ååæè½åã</li>
</ul>
<p><strong>5. ä»æè¦ä¸­å¯æ¨æ­çå±éæ§</strong></p>
<ul>
<li><strong>è®¡ç®ææ¬å¢å ï¼</strong> æ··ååæµæ¶æéå¸¸æ¯åæµæ¨¡åå·ææ´é«çè®¡ç®å¤æåº¦ååæ°éï¼è¿å¯è½å¯¼è´è®­ç»åæ¨çæ¶é´å¢å ï¼å¯¹èµæºåéçåºç¨ææææã</li>
<li><strong>DMPåæ°è°ä¼çå¤ææ§ï¼</strong> æè¦æå°è¯ä¼°äºâåç§DMPå·®ååç»æåç´ å½¢ç¶âï¼è¿æç¤ºDMPçåæ°ï¼å¦ç»æåç´ å¤§å°ãå½¢ç¶ãå·®åé¶æ°ï¼å¯è½éè¦éå¯¹ä¸åæ°æ®éæä»»å¡è¿è¡ç»è´çè°ä¼ï¼è¿å¢å äºæ¨¡åçé¨ç½²åæ³åé¾åº¦ã</li>
<li><strong>âå¯ä»¥è¶è¶âçéå®æ§ï¼</strong> ç»ææ¾ç¤ºâhybrid DMP consistently outperforms direct-input and is capable of surpassing a non-DMP modelâï¼å¶ä¸­âcapable of surpassingâå¯è½æå³çå¹¶éå¨æææåµä¸æææææ ä¸é½è½ç»å¯¹è¶è¶éDMPæ¨¡åï¼æèè¶è¶çå¹åº¦å¯è½æéï¼è¿éè¦è¿ä¸æ­¥çå®éªç»èæ¥éªè¯ã</li>
<li><strong>å¯¹ç°åº¦å½¢æå­¦çä¾èµï¼</strong> DMPåºäºç°åº¦å½¢æå­¦ï¼å¶æææ§å¯è½å¨å¾å¤§ç¨åº¦ä¸ä¾èµäºå¾åä¸­å½¢ç¶ä¿¡æ¯å¨ç°åº¦ééä¸­çå¯æåæ§ãå¯¹äºé¢è²æçº¹çä¿¡æ¯æ´ä¸ºå³é®çåºæ¯ï¼DMPçç´æ¥è´¡ç®å¯è½éè¦ä¸å¶ä»ç¹å¾ç»åã</li>
</ul>
<p><strong>Key Findings:</strong></p>
<ul>
<li>State-of-the-art
segmentation networks are typically developed and tuned on ground-perspective
photographs and do not directly address remote sensing challenges such as
extreme scale variation, foreground-background imbalance, and large image
sizes.</li>
<li>In this work, we
extend prior DMPNet work beyond classification and object detection by
integrating DMP features into three state-of-the-art convolutional and
transformer semantic segmentation architectures.</li>
<li>Our results show that while non-DMP models generally
outperform the direct-input variants, hybrid DMP consistently outperforms
direct-input and is capable of surpassing a non-DMP model on mIoU, F1, and
Recall.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="http://arxiv.org/pdf/2509.04268v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.04268v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.03379v1'></a></p>
<h2 id="tinydrop-tiny-model-guided-token-dropping-for-vision-transformers"><a href="https://arxiv.org/abs/2509.03379v1">TinyDrop: Tiny Model Guided Token Dropping for Vision Transformers</a></h2>
<p><strong>Authors:</strong> Guoxin Wang, Qingyuan Wang, Binhua Huang, Shaowu Chen, Deepu John</p>
<p><strong>Published:</strong> 2025-09-03</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>Vision Transformers (ViTs) achieve strong performance in image classification
but incur high computational costs from processing all image tokens. To reduce
inference costs in large ViTs without compromising accuracy, we propose
TinyDrop, a training-free token dropping framework guided by a lightweight
vision model. The guidance model estimates the importance of tokens while
performing inference, thereby selectively discarding low-importance tokens if
large vit models need to perform attention calculations. The framework operates
plug-and-play, requires no architectural modifications, and is compatible with
diverse ViT architectures. Evaluations on standard image classification
benchmarks demonstrate that our framework reduces FLOPs by up to 80% for ViTs
with minimal accuracy degradation, highlighting its generalization capability
and practical utility for efficient ViT-based classification.</p>
<p><strong>Analysis:</strong></p>
<p>ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå¯¹è¿ç¯å³äºTinyDropçè®ºææè¦è¿è¡äºåæï¼</p>
<hr />
<h3 id="tinydrop-tiny-model-guided-token-dropping-for-vision-transformers_1">è®ºææè¦åæï¼TinyDrop: Tiny Model Guided Token Dropping for Vision Transformers</h3>
<p><strong>1. è®ºææ ¸å¿è´¡ç®çç®ææ»ç» (Concise Summary)</strong></p>
<p>TinyDropæåºäºä¸ç§éå¯¹Vision Transformers (ViTs) çè®­ç»æ å³ï¼training-freeï¼çtokenä¸¢å¼æ¡æ¶ï¼æ¨å¨æ¾èéä½å¤§åViTsçæ¨çè®¡ç®ææ¬ï¼åæ¶ä¿æé«ç²¾åº¦ãè¯¥æ¡æ¶éè¿ä¸ä¸ªè½»éçº§è§è§æ¨¡åå¨æ¨çæ¶å¨æè¯ä¼°tokençéè¦æ§ï¼å¹¶éæ©æ§å°ä¸¢å¼ä½éè¦æ§çtokenãå®éªè¯æï¼TinyDropè½å°ViTsçFLOPséä½é«è¾¾80%ï¼ä¸ä»å¸¦æ¥æå°çç²¾åº¦æå¤±ï¼å±ç°äºå¶å¨æé«ViTæçæ¹é¢çå®ç¨ä»·å¼ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³å­¦æ¹æ³ (Key Innovation or Methodological Approach)</strong></p>
<p>æ ¸å¿åæ°å¨äºå¶<strong>è®­ç»æ å³ï¼training-freeï¼</strong>å<strong>ç±è½»éçº§æ¨¡åå¼å¯¼çå¨ætokenä¸¢å¼æºå¶</strong>ãä¸éè¦éæ°è®­ç»æä¿®æ¹æ¶æçç°ææ¹æ³ä¸åï¼TinyDropå¼å¥äºä¸ä¸ªå¤é¨çãè½»éçº§æå¯¼æ¨¡åï¼è¯¥æ¨¡åè½å¤å¨ViTæ¨çè¿ç¨ä¸­<strong>å®æ¶ï¼on-the-flyï¼</strong>è¯ä¼°æ¯ä¸ªtokençéè¦æ§ãè¿ç§æ¹æ³åè®¸å¨å¤§åViTæ¨¡åæ§è¡è®¡ç®å¯éåçæ³¨æåæä½ä¹åï¼<strong>èªéåºå°ãéæ©æ§å°</strong>ä¸¢å¼ä¸éè¦çtokenãå¶âå³æå³ç¨âçç¹æ§åå¯¹å¤ç§ViTæ¶æçå¼å®¹æ§ï¼ä¹æå¤§å°éä½äºå¶åºç¨é¨æ§åå®ç¨æ§ã</p>
<p><strong>3. å¯¹é¢åæ½å¨å½±å (Potential Impact on the Field)</strong></p>
<p>è¿é¡¹ç ç©¶å¯¹è®¡ç®æºè§è§é¢åå·ææ·±è¿å½±åãå®ææ<strong>æ°ä¸»åå¤§åé«æ§è½ViTæ¨¡åçé¨ç½²</strong>ï¼ä½¿å¶è½å¤å¨èµæºåéçç¯å¢ï¼å¦ç§»å¨è®¾å¤ãè¾¹ç¼è®¡ç®ãåµå¥å¼ç³»ç»ï¼ä¸­è¿è¡ï¼è¿äºåºæ¯å¯¹è®¡ç®é¢ç®åå»¶è¿æä¸¥æ ¼è¦æ±ãéè¿å¤§å¹éä½æ¨çFLOPsï¼TinyDropä¹ä¸º<strong>å¯æç»­AI</strong>ååºäºè´¡ç®ï¼åå°äºå¤§åæ¨¡åè¿è¡çè½æºæ¶èãæ­¤å¤ï¼å¶å³æå³ç¨çç¹æ§å¯ä»¥å éViTæ¨¡åçç åååºç¨ï¼ä¸ºç°æåæªæ¥çViTæ¶ææä¾ä¸ä¸ªæäºéæçæçä¼åæ¹æ¡ã</p>
<p><strong>4. å¯è½åççç¸å³é¢åæåºç¨ (Related Areas or Applications)</strong></p>
<p>å°½ç®¡æè¦ä¸»è¦å³æ³¨å¾ååç±»ï¼ä½tokené«æå¤ççæ ¸å¿ææ³å¯¹å¹¿æ³çViTåºç¨é½å·æä»·å¼ï¼</p>
<ul>
<li><strong>å®æ¶è®¡ç®æºè§è§ç³»ç»ï¼</strong> èªå¨é©¾é©¶ãæºå¨äººãè§é¢çæ§ç­å¯¹ä½å»¶è¿æä¸¥æ ¼è¦æ±çåºæ¯ã</li>
<li><strong>è¾¹ç¼AI/ç§»å¨è®¡ç®ï¼</strong> å¨è®¡ç®è½ååçµæ± å¯¿å½æéçè®¾å¤ä¸é¨ç½²å¤æçViTæ¨¡åã</li>
<li><strong>è§é¢çè§£ï¼</strong> éè¿ä¸¢å¼æ¶é´åç©ºé´ä¸çåä½tokenï¼é«æå¤çè§é¢åºåã</li>
<li><strong>å»å­¦å½±ååæï¼</strong> å éå¯¹å¤§åå»å­¦å¾åçåæï¼å¯è½ç¼©ç­è¯æ­æ¶é´ã</li>
<li><strong>å¶ä»ViT-basedä»»å¡ï¼</strong> ä¾å¦ç®æ æ£æµãè¯­ä¹åå²ãå®ä¾åå²ä»¥åä½¿ç¨ViTä½ä¸ºéª¨å¹²ç½ç»ççææ¨¡åï¼è¿äºä»»å¡ä¸­ViTä½ä¸ºç¹å¾æåå¨ï¼å¶æçæåå°å¸¦æ¥æ´ä½æ§è½çæ¹åã</li>
</ul>
<p><strong>5. ä»æè¦ä¸­å¯æ¨æ­çå±éæ§ (Limitations that can be inferred from the abstract)</strong></p>
<ul>
<li><strong>âæå°âç²¾åº¦æå¤±çéåï¼</strong> æè¦ä¸­æå°âminimal accuracy degradationâï¼ä½âæå°âæ¯ä¸ä¸ªç¸å¯¹æ¦å¿µãå¨å¯¹ç²¾åº¦è¦æ±æé«çåºç¨ä¸­ï¼å³ä½¿æ¯å¾®å°çæå¤±ä¹å¯è½æ æ³æ¥åãå·ä½çFLOPs-ç²¾åº¦æè¡¡æ²çº¿ï¼trade-off curveï¼æªå¨æè¦ä¸­è¯¦ç»è¯´æã</li>
<li><strong>æå¯¼æ¨¡åçå¼éåè·åï¼</strong> å°½ç®¡æå¯¼æ¨¡åæ¯âè½»éçº§âçï¼ä½å®ä»ç¶å¼å¥äºé¢å¤çè®¡ç®å¼éãæè¦æ²¡æéåè¿ç§å¼éç¸å¯¹äºèççFLOPsçæ¯ä¾ï¼ä¹æ²¡æè¯´æè¿ä¸ªæå¯¼æ¨¡åæ¯å¦ä½è·åæè®­ç»çï¼ä¾å¦ï¼æ¯å¦éè¦é¢è®­ç»ãæ¯å¦éè¦ç¹å®æ°æ®ï¼æèæ¯å¦æ¯èªçç£çï¼ãè½ç¶tokenä¸¢å¼æ¡æ¶æ¯è®­ç»æ å³çï¼ä½æå¯¼æ¨¡åæ¬èº«å¯è½ä¸æ¯ã</li>
<li><strong>Tokenéè¦æ§ä¼°è®¡çé²æ£æ§ï¼</strong> æ¡æ¶çæææ§é«åº¦ä¾èµäºæå¯¼æ¨¡ååç¡®ä¼°è®¡tokenéè¦æ§çè½åãå¯¹äºåå¸å¤æ°æ®ï¼out-of-distribution dataï¼ãå¯¹ææ§æ ·æ¬æå¾åä¸­ç»å¾®ç¹å¾çé²æ£æ§å¯è½æ¯ä¸ä¸ªæ½å¨é®é¢ã</li>
<li><strong>å¯¹å¤æä»»å¡çæ³åè½åï¼</strong> è¯ä¼°ä¸»è¦å¨âæ åå¾ååç±»åºåâä¸è¿è¡ãå¯¹äºæ´å¤æçä»»å¡ï¼å¦éè¦ç²¾ç»åç´ çº§ä¿¡æ¯çå¯éé¢æµä»»å¡ï¼å¦åå²ãæ£æµï¼ï¼ä¸¢å¼tokenå¯è½ä¼å¯¹æ§è½äº§çæ´å¤§çè´é¢å½±åï¼å ä¸ºè¿äºä»»å¡éå¸¸å¯¹å±é¨ç»èæ´ææã</li>
<li><strong>è¶åæ°è°ä¼ï¼</strong> Tokenä¸¢å¼éå¼éå¸¸éè¦è¿è¡è°ä¼ãè½ç¶æ¡æ¶æ¬èº«æ¯è®­ç»æ å³çï¼ä½å¨æ°çViTæ¨¡åææ°æ®éä¸æ¾å°æä½³çä¸¢å¼ç­ç¥å¯è½ä»éè¦ä¸å®çç»éªæ§æç´¢ã</li>
</ul>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To reduce
inference costs in large ViTs without compromising accuracy, we propose
TinyDrop, a training-free token dropping framework guided by a lightweight
vision model.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="http://arxiv.org/pdf/2509.03379v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.03379v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-09-05 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
