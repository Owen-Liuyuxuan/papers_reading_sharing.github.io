<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-09-05 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-13
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-09-04/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-09-07/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-09-05">Arxiv Computer Vision Papers - 2025-09-05</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#infradiffusion-zero-shot-depth-map-restoration-with-diffusion-models-and-prompted-segmentation-from-sparse-infrastructure-point-clouds" class="nav-link">InfraDiffusion: zero-shot depth map restoration with diffusion models and prompted segmentation from sparse infrastructure point clouds</a>
                </li>
                <li class="nav-item">
                    <a href="#visiofirm-cross-platform-ai-assisted-annotation-tool-for-computer-vision" class="nav-link">VisioFirm: Cross-Platform AI-assisted Annotation Tool for Computer Vision</a>
                </li>
                <li class="nav-item">
                    <a href="#luxdit-lighting-estimation-with-video-diffusion-transformer" class="nav-link">LuxDiT: Lighting Estimation with Video Diffusion Transformer</a>
                </li>
                <li class="nav-item">
                    <a href="#onecat-decoder-only-auto-regressive-model-for-unified-understanding-and-generation" class="nav-link">OneCAT: Decoder-Only Auto-Regressive Model for Unified Understanding and Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#a-generative-foundation-model-for-chest-radiography" class="nav-link">A Generative Foundation Model for Chest Radiography</a>
                </li>
                <li class="nav-item">
                    <a href="#ovgrasp-open-vocabulary-grasping-assistance-via-multimodal-intent-detection" class="nav-link">OVGrasp: Open-Vocabulary Grasping Assistance via Multimodal Intent Detection</a>
                </li>
                <li class="nav-item">
                    <a href="#enhancing-robustness-in-post-processing-watermarking-an-ensemble-attack-network-using-cnns-and-transformers" class="nav-link">Enhancing Robustness in Post-Processing Watermarking: An Ensemble Attack Network Using CNNs and Transformers</a>
                </li>
                <li class="nav-item">
                    <a href="#plotn-polish-zero-shot-story-visualization-and-disentangled-editing-with-text-to-image-diffusion-models" class="nav-link">Plot'n Polish: Zero-shot Story Visualization and Disentangled Editing with Text-to-Image Diffusion Models</a>
                </li>
                <li class="nav-item">
                    <a href="#differential-morphological-profile-neural-networks-for-semantic-segmentation" class="nav-link">Differential Morphological Profile Neural Networks for Semantic Segmentation</a>
                </li>
                <li class="nav-item">
                    <a href="#tinydrop-tiny-model-guided-token-dropping-for-vision-transformers" class="nav-link">TinyDrop: Tiny Model Guided Token Dropping for Vision Transformers</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-09-05">Arxiv Computer Vision Papers - 2025-09-05</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>好的，这是一份为忙碌的研究人员准备的 Arxiv 计算机视觉领域最新论文执行摘要。</p>
<hr />
<p><strong>Arxiv 计算机视觉领域最新论文执行摘要 (2025-09-03)</strong></p>
<p>本报告旨在为研究人员提供当日 Arxiv 计算机视觉领域最新发表论文的快速概览，重点关注主要趋势、创新亮点、新兴方向及推荐阅读。</p>
<hr />
<p><strong>1. 主要主题与趋势 (Main Themes &amp; Trends)</strong></p>
<p>今天的论文展示了计算机视觉领域几个关键趋势的持续深化和交叉融合：</p>
<ul>
<li><strong>扩散模型 (Diffusion Models) 的广泛应用与创新:</strong> 扩散模型继续在各种任务中展现其强大能力，从稀疏3D数据重建（深度图恢复）到视频光照估计，再到复杂的文本到图像故事可视化和编辑。这表明扩散模型正成为多模态生成和感知任务的核心技术。</li>
<li><strong>基础模型 (Foundation Models) 的深化与专业化:</strong> 出现了旨在实现统一理解与生成的通用基础模型，以及针对特定领域（如医学影像）的专业化生成式基础模型，预示着未来AI系统将更加通用或在特定领域达到专家级水平。</li>
<li><strong>视觉 Transformer (Vision Transformers) 的效率与鲁棒性优化:</strong> 针对 ViT 的计算效率问题，有研究提出了轻量化策略（如 token dropping），同时也有工作关注其在对抗性攻击（如水印）下的鲁棒性增强。</li>
<li><strong>多模态与跨领域融合 (Multimodality &amp; Cross-Domain Fusion):</strong> 论文涵盖了文本-图像、视频-图像、点云-图像等多种模态的融合，以及计算机视觉与机器人、医疗、基础设施等领域的深度结合。</li>
<li><strong>生成式 AI 的精细控制与编辑 (Fine-grained Control &amp; Editing in Generative AI):</strong> 不再仅仅是生成图像，而是追求对生成内容更深层次的理解、编辑和叙事能力。</li>
<li><strong>实用工具与应用 (Practical Tools &amp; Applications):</strong> 出现了提升标注效率的AI辅助工具，以及面向实际机器人操作的开放词汇抓取辅助系统。</li>
</ul>
<p><strong>2. 显著或创新性论文亮点 (Significant or Innovative Papers)</strong></p>
<ul>
<li><strong>OneCAT: Decoder-Only Auto-Regressive Model for Unified Understanding and Generation (Han Li et al.)</strong>: 这篇论文提出了一个统一的解码器-only自回归模型，旨在同时实现理解和生成任务。其潜力在于构建更通用、更强大的AI系统，是迈向通用智能的重要一步。</li>
<li><strong>A Generative Foundation Model for Chest Radiography (Yuanfeng Ji et al.)</strong>: 在医疗影像领域引入生成式基础模型，有望彻底改变胸部X光片的分析、诊断和数据增强方式，对医疗AI具有里程碑意义。</li>
<li><strong>InfraDiffusion: zero-shot depth map restoration with diffusion models and prompted segmentation from sparse infrastructure point clouds (Yixiong Jing et al.)</strong>: 创新性地将扩散模型应用于从稀疏基础设施点云进行零样本深度图恢复，解决了3D视觉中一个具有挑战性的实际问题。</li>
<li><strong>LuxDiT: Lighting Estimation with Video Diffusion Transformer (Ruofan Liang et al.)</strong>: 结合了视频、扩散模型和 Transformer 的强大能力，实现了高质量的光照估计，对虚拟现实、电影制作和图像编辑等领域具有重要价值。</li>
<li><strong>Plot'n Polish: Zero-shot Story Visualization and Disentangled Editing with Text-to-Image Diffusion Models (Kiymet Akdemir et al.)</strong>: 突破了简单的文本到图像生成，实现了故事级别的可视化和解耦编辑，展示了生成式AI在创意内容生产方面的巨大潜力。</li>
<li><strong>TinyDrop: Tiny Model Guided Token Dropping for Vision Transformers (Guoxin Wang et al.)</strong>: 提出了一种高效的 ViT 优化策略，通过微小模型引导的 token dropping 来提升推理速度，对 ViT 的实际部署具有重要意义。</li>
</ul>
<p><strong>3. 新兴研究方向或技术 (Emerging Research Directions or Techniques)</strong></p>
<ul>
<li><strong>统一的理解与生成范式 (Unified Understanding &amp; Generation Paradigm):</strong> 以 OneCAT 为代表，探索如何用单一模型架构处理多模态的感知与生成任务。</li>
<li><strong>领域特定基础模型 (Domain-Specific Foundation Models):</strong> 针对特定高价值领域（如医疗、工业）开发定制化的基础模型，以实现更精准、高效的应用。</li>
<li><strong>稀疏/不完整数据上的扩散模型 (Diffusion Models on Sparse/Incomplete Data):</strong> InfraDiffusion 展示了扩散模型在处理不完整3D数据方面的潜力，未来可能扩展到更多数据稀缺场景。</li>
<li><strong>高效且鲁棒的 Transformer 架构 (Efficient &amp; Robust Transformer Architectures):</strong> TinyDrop 和水墨画增强鲁棒性的研究，表明对 Transformer 模型效率和安全性的关注将持续增加。</li>
<li><strong>多模态意图检测与机器人交互 (Multimodal Intent Detection &amp; Robotic Interaction):</strong> OVGrasp 强调了结合视觉、语言等多种模态来理解人类意图，以实现更智能的机器人操作。</li>
<li><strong>生成式 AI 的叙事与高层次编辑 (Narrative &amp; High-Level Editing in Generative AI):</strong> Plot'n Polish 预示着生成模型将从图像生成走向更复杂的叙事和内容创作。</li>
</ul>
<p><strong>4. 建议深入阅读的论文 (Recommended Full Reads)</strong></p>
<p>考虑到其潜在影响和创新性，我们建议研究人员优先阅读以下论文：</p>
<ul>
<li><strong>OneCAT: Decoder-Only Auto-Regressive Model for Unified Understanding and Generation (Han Li et al.)</strong>: 对于关注通用AI、基础模型架构和多模态学习的研究者，这篇论文提供了重要的未来方向。</li>
<li><strong>A Generative Foundation Model for Chest Radiography (Yuanfeng Ji et al.)</strong>: 医疗AI领域的研究人员应重点关注，它可能为医学影像分析带来范式转变。</li>
<li><strong>InfraDiffusion: zero-shot depth map restoration with diffusion models and prompted segmentation from sparse infrastructure point clouds (Yixiong Jing et al.)</strong>: 专注于3D视觉、扩散模型在稀疏数据应用或基础设施AI的研究者会从中受益。</li>
<li><strong>TinyDrop: Tiny Model Guided Token Dropping for Vision Transformers (Guoxin Wang et al.)</strong>: 对于致力于 Vision Transformer 部署、效率优化和边缘计算的研究者，这提供了实用的解决方案。</li>
<li><strong>Plot'n Polish: Zero-shot Story Visualization and Disentangled Editing with Text-to-Image Diffusion Models (Kiymet Akdemir et al.)</strong>: 对生成式AI、创意应用、文本到图像生成和可控内容创作感兴趣的研究者不容错过。</li>
</ul>
<hr />
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2509.03324v1">InfraDiffusion: zero-shot depth map restoration with diffusion models and prompted segmentation from sparse infrastructure point clouds</a></li>
<li><a href="#2509.04180v1">VisioFirm: Cross-Platform AI-assisted Annotation Tool for Computer Vision</a></li>
<li><a href="#2509.03680v1">LuxDiT: Lighting Estimation with Video Diffusion Transformer</a></li>
<li><a href="#2509.03498v1">OneCAT: Decoder-Only Auto-Regressive Model for Unified Understanding and Generation</a></li>
<li><a href="#2509.03903v1">A Generative Foundation Model for Chest Radiography</a></li>
<li><a href="#2509.04324v1">OVGrasp: Open-Vocabulary Grasping Assistance via Multimodal Intent Detection</a></li>
<li><a href="#2509.03006v1">Enhancing Robustness in Post-Processing Watermarking: An Ensemble Attack Network Using CNNs and Transformers</a></li>
<li><a href="#2509.04446v1">Plot'n Polish: Zero-shot Story Visualization and Disentangled Editing with Text-to-Image Diffusion Models</a></li>
<li><a href="#2509.04268v1">Differential Morphological Profile Neural Networks for Semantic Segmentation</a></li>
<li><a href="#2509.03379v1">TinyDrop: Tiny Model Guided Token Dropping for Vision Transformers</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2509.03324v1'></a></p>
<h2 id="infradiffusion-zero-shot-depth-map-restoration-with-diffusion-models-and-prompted-segmentation-from-sparse-infrastructure-point-clouds"><a href="https://arxiv.org/abs/2509.03324v1">InfraDiffusion: zero-shot depth map restoration with diffusion models and prompted segmentation from sparse infrastructure point clouds</a></h2>
<p><strong>Authors:</strong> Yixiong Jing, Cheng Zhang, Haibing Wu, Guangming Wang, Olaf Wysocki, Brian Sheil</p>
<p><strong>Published:</strong> 2025-09-03</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Point clouds are widely used for infrastructure monitoring by providing
geometric information, where segmentation is required for downstream tasks such
as defect detection. Existing research has automated semantic segmentation of
structural components, while brick-level segmentation (identifying defects such
as spalling and mortar loss) has been primarily conducted from RGB images.
However, acquiring high-resolution images is impractical in low-light
environments like masonry tunnels. Point clouds, though robust to dim lighting,
are typically unstructured, sparse, and noisy, limiting fine-grained
segmentation. We present InfraDiffusion, a zero-shot framework that projects
masonry point clouds into depth maps using virtual cameras and restores them by
adapting the Denoising Diffusion Null-space Model (DDNM). Without task-specific
training, InfraDiffusion enhances visual clarity and geometric consistency of
depth maps. Experiments on masonry bridge and tunnel point cloud datasets show
significant improvements in brick-level segmentation using the Segment Anything
Model (SAM), underscoring its potential for automated inspection of masonry
assets. Our code and data is available at
https://github.com/Jingyixiong/InfraDiffusion-official-implement.</p>
<p><strong>Analysis:</strong></p>
<p>InfraDiffusion 论文摘要的分析如下：</p>
<h3 id="1">1. 论文主要贡献的简明摘要</h3>
<p>InfraDiffusion 提出了一种零样本（zero-shot）框架，用于从稀疏、非结构化且嘈杂的砖石点云中恢复高质量的深度图。该框架通过将点云投影到虚拟相机生成的深度图，并利用适应性修改的去噪扩散零空间模型（DDNM）进行修复，显著提升了深度图的视觉清晰度和几何一致性。其核心贡献在于，无需针对特定任务进行训练，即可显著改善砖石结构的砖块级分割效果，从而推动自动化基础设施检测。</p>
<h3 id="2">2. 关键创新或方法学方法</h3>
<p>该论文的关键创新在于其<strong>零样本深度图恢复框架</strong>，它巧妙地结合了以下几点：
*   <strong>点云到深度图的投影：</strong> 将稀疏的砖石点云通过虚拟相机转换为深度图，为后续处理提供了一个结构化的二维表示。
*   <strong>适应性扩散模型应用：</strong> 核心在于<strong>适应性地修改和应用去噪扩散零空间模型（DDNM）</strong>进行深度图恢复。DDNM 通常用于图像修复或条件生成，此处将其创新性地应用于从稀疏、不完整数据中恢复几何信息丰富的深度图，且无需任务特定的训练。
*   <strong>零样本能力：</strong> 整个框架无需针对砖石结构或深度图恢复任务进行额外的训练，直接利用预训练的扩散模型能力，这大大降低了数据标注和模型训练的成本，并提升了泛化能力。
*   <strong>与现有分割模型的结合：</strong> 恢复后的高质量深度图能够显著提升如 Segment Anything Model (SAM) 等通用分割模型在砖块级分割任务上的表现，实现了从低质量点云到高精度语义理解的桥梁。</p>
<h3 id="3">3. 对领域潜在影响</h3>
<ul>
<li><strong>基础设施检测与维护：</strong> 为砖石桥梁和隧道等基础设施的自动化、精细化检测提供了强大的工具，能够更准确地识别剥落、砂浆流失等缺陷，从而提高维护效率和安全性。</li>
<li><strong>点云数据利用效率：</strong> 克服了稀疏、嘈杂点云在细粒度分析上的局限性，为从低质量三维数据中提取高价值信息开辟了新途径。</li>
<li><strong>扩散模型应用拓展：</strong> 将扩散模型的应用范围从传统的图像生成、修复等领域拓展到几何数据（深度图）的恢复和增强，展示了其在处理结构化几何信息方面的巨大潜力。</li>
<li><strong>零样本学习的实践：</strong> 强调了零样本方法在实际工程应用中的可行性和优势，尤其是在难以获取大量标注数据的特定领域。</li>
<li><strong>多模态数据融合：</strong> 虽然抽象中未直接提及，但这种将三维点云转换为二维深度图并利用图像处理技术进行增强的思路，为未来多模态数据融合和处理提供了新的范式。</li>
</ul>
<h3 id="4">4. 相关领域或应用</h3>
<ul>
<li><strong>土木工程与结构健康监测：</strong> 自动化检测桥梁、隧道、大坝等基础设施的结构缺陷。</li>
<li><strong>文化遗产保护：</strong> 对历史建筑、雕塑等进行精细化三维扫描和损伤评估。</li>
<li><strong>机器人与自主检测：</strong> 装备有激光雷达的机器人或无人机在复杂、低光照环境下进行自主巡检和环境感知。</li>
<li><strong>数字孪生（Digital Twin）：</strong> 创建高精度的物理资产数字模型，用于模拟、分析和预测。</li>
<li><strong>建筑信息模型（BIM）：</strong> 增强现有建筑的BIM模型，使其包含更详细的结构健康信息。</li>
<li><strong>采矿与地质勘探：</strong> 隧道、矿井等地下空间的结构稳定性监测。</li>
</ul>
<h3 id="5">5. 可从摘要中推断出的局限性</h3>
<ul>
<li><strong>依赖虚拟相机视角：</strong> 将点云投影到深度图的质量和完整性可能高度依赖于虚拟相机的选择和数量。如果点云在某些区域极其稀疏，或者虚拟相机视角不佳，可能导致深度图信息缺失或不准确。</li>
<li><strong>DDNM的泛化能力：</strong> 尽管是“零样本”，但DDNM本身是预训练模型。其在处理砖石结构特有的几何纹理和缺陷模式上的表现，可能受限于其原始训练数据的领域。对于与训练数据差异较大的几何结构或材料，效果可能有所下降。</li>
<li><strong>砖石结构的特异性：</strong> 论文强调了“砖石点云”和“砖块级分割”。这表明该方法可能针对砖石结构进行了优化或验证，其在其他类型结构（如混凝土、钢结构）或更复杂缺陷（如裂缝、变形）上的表现尚不明确。</li>
<li><strong>计算资源需求：</strong> 扩散模型通常计算成本较高，尤其是在生成高分辨率深度图时。摘要中未提及实时性或计算效率，这在实际部署中可能是一个考量因素。</li>
<li><strong>深度图的局限性：</strong> 深度图是2.5D表示，无法完全捕捉三维点云的所有几何信息，例如遮挡区域后的结构。这可能限制了对某些复杂缺陷的检测能力。</li>
</ul>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present InfraDiffusion, a zero-shot framework that projects
masonry point clouds into depth maps using virtual cameras and restores them by
adapting the Denoising Diffusion Null-space Model (DDNM).</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="http://arxiv.org/pdf/2509.03324v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.03324v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.04180v1'></a></p>
<h2 id="visiofirm-cross-platform-ai-assisted-annotation-tool-for-computer-vision"><a href="https://arxiv.org/abs/2509.04180v1">VisioFirm: Cross-Platform AI-assisted Annotation Tool for Computer Vision</a></h2>
<p><strong>Authors:</strong> Safouane El Ghazouali, Umberto Michelucci</p>
<p><strong>Published:</strong> 2025-09-04</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>AI models rely on annotated data to learn pattern and perform prediction.
Annotation is usually a labor-intensive step that require associating labels
ranging from a simple classification label to more complex tasks such as object
detection, oriented bounding box estimation, and instance segmentation.
Traditional tools often require extensive manual input, limiting scalability
for large datasets. To address this, we introduce VisioFirm, an open-source web
application designed to streamline image labeling through AI-assisted
automation. VisioFirm integrates state-of-the-art foundation models into an
interface with a filtering pipeline to reduce human-in-the-loop efforts. This
hybrid approach employs CLIP combined with pre-trained detectors like
Ultralytics models for common classes and zero-shot models such as Grounding
DINO for custom labels, generating initial annotations with low-confidence
thresholding to maximize recall. Through this framework, when tested on
COCO-type of classes, initial prediction have been proven to be mostly correct
though the users can refine these via interactive tools supporting bounding
boxes, oriented bounding boxes, and polygons. Additionally, VisioFirm has
on-the-fly segmentation powered by Segment Anything accelerated through WebGPU
for browser-side efficiency. The tool supports multiple export formats (YOLO,
COCO, Pascal VOC, CSV) and operates offline after model caching, enhancing
accessibility. VisioFirm demonstrates up to 90\% reduction in manual effort
through benchmarks on diverse datasets, while maintaining high annotation
accuracy via clustering of connected CLIP-based disambiguate components and
IoU-graph for redundant detection suppression. VisioFirm can be accessed from
\href{https://github.com/OschAI/VisioFirm}{https://github.com/OschAI/VisioFirm}.</p>
<p><strong>Analysis:</strong></p>
<p>作为计算机视觉和机器学习领域的专家，我对这篇论文摘要进行如下分析：</p>
<hr />
<h3 id="1-concise-summary">1. 论文主要贡献的简明摘要 (Concise Summary)</h3>
<p>VisioFirm是一个开源的跨平台AI辅助标注工具，旨在解决计算机视觉数据标注耗时耗力的问题。它通过智能集成CLIP、Grounding DINO、Ultralytics等前沿基础模型和WebGPU加速的Segment Anything Model (SAM)，实现了高效的混合人机协作标注流程，声称可将手动工作量减少高达90%，同时保持高标注精度。</p>
<h3 id="2-key-innovation-or-methodological-approach">2. 关键创新或方法论 (Key Innovation or Methodological Approach)</h3>
<p>VisioFirm的核心创新在于其<strong>混合式AI辅助标注范式</strong>，通过智能集成多种前沿基础模型，生成初始高召回率的低置信度预测，再由用户进行精修。具体方法包括：</p>
<ul>
<li><strong>多模型融合策略：</strong> 结合了预训练检测器（如Ultralytics模型）处理常见类别，零样本模型（如Grounding DINO）处理自定义标签，以及CLIP模型进行语义消歧和连接组件聚类。</li>
<li><strong>高效的浏览器端分割：</strong> 利用<strong>WebGPU加速的Segment Anything Model (SAM)</strong>，实现了实时、高效的“即时分割”功能，显著提升了用户体验。</li>
<li><strong>精度维护机制：</strong> 引入了<strong>基于CLIP的连接组件聚类消歧</strong>和<strong>IoU图冗余检测抑制机制</strong>，以在大幅减少人工干预的同时确保标注精度。</li>
<li><strong>跨平台与离线能力：</strong> 作为Web应用，支持多种导出格式（YOLO, COCO, Pascal VOC, CSV），并在模型缓存后支持离线操作，极大地增强了工具的可用性和可访问性。</li>
</ul>
<h3 id="3-potential-impact-on-the-field">3. 对领域潜在影响 (Potential Impact on the Field)</h3>
<p>VisioFirm有望显著<strong>降低计算机视觉领域数据标注的门槛和成本</strong>，尤其对于资源有限的团队和研究者。通过将手动标注工作量减少高达90%，它能<strong>极大加速数据集的创建和迭代过程</strong>，从而<strong>推动AI模型开发和部署的效率</strong>。其开源和跨平台的特性也有助于<strong>促进AI辅助标注工具的普及和标准化</strong>，使更多人能够利用先进的AI能力进行高质量数据准备，从而加速整个CV生态系统的创新。</p>
<h3 id="4-related-areas-or-applications">4. 相关领域或应用 (Related Areas or Applications)</h3>
<ul>
<li><strong>计算机视觉模型开发与部署：</strong> 任何需要大量标注数据来训练、微调或评估模型的场景，如目标检测、实例分割、语义分割、姿态估计等。</li>
<li><strong>自动驾驶与机器人：</strong> 用于标注感知系统所需的道路、车辆、行人、障碍物等数据。</li>
<li><strong>医疗影像分析：</strong> 辅助医生或研究人员标注病灶、器官、细胞等医学图像。</li>
<li><strong>工业质检与安防监控：</strong> 快速标注缺陷、异常行为或特定目标。</li>
<li><strong>农业科技：</strong> 标注作物病虫害、果实成熟度、农田区域等。</li>
<li><strong>学术研究与教育：</strong> 为学生和研究人员提供一个易于使用的工具来创建自定义数据集。</li>
<li><strong>数据标注服务提供商：</strong> 提高其服务效率和降低成本。</li>
</ul>
<h3 id="5-limitations-inferred-from-the-abstract">5. 从摘要中可推断的局限性 (Limitations Inferred from the Abstract)</h3>
<ul>
<li><strong>对人工干预的持续依赖：</strong> 尽管声称减少90%的工作量，但“初始预测大多正确”和“用户可以细化”表明人工审核和修正仍然是确保最终标注质量的关键环节，并非完全自动化。</li>
<li><strong>对基础模型的性能依赖：</strong> VisioFirm的效率和准确性高度依赖于其集成的CLIP、Grounding DINO、Ultralytics和SAM等基础模型的泛化能力。对于这些模型不擅长处理的特定领域、高度抽象或极度细粒度的自定义类别，其辅助效果可能会打折扣。</li>
<li><strong>“COCO-type of classes”的测试范围：</strong> 尽管在COCO类型类别上表现良好，但对于高度专业化、长尾分布或视觉上模糊的自定义数据集，其初始预测的准确性和召回率可能需要更频繁的人工修正。</li>
<li><strong>WebGPU的兼容性与性能：</strong> WebGPU的加速效果可能受限于用户浏览器版本、显卡硬件和驱动程序，并非所有用户都能获得最佳的浏览器端效率。</li>
<li><strong>离线能力的局限性：</strong> “模型缓存后可离线操作”意味着首次使用或模型更新时仍需网络连接下载模型，且模型缓存可能占用大量本地存储空间。</li>
<li><strong>“高达90%”的减少量：</strong> 这是一个上限值，实际减少量可能因数据集的复杂性、标注任务类型以及用户熟练度而异。</li>
<li><strong>未提及视频标注：</strong> 摘要主要聚焦于图像标注，未说明其对视频标注任务的支持能力。</li>
</ul>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this, we introduce VisioFirm, an open-source web
application designed to streamline image labeling through AI-assisted
automation.</li>
<li>VisioFirm integrates state-of-the-art foundation models into an
interface with a filtering pipeline to reduce human-in-the-loop efforts.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="http://arxiv.org/pdf/2509.04180v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.04180v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.03680v1'></a></p>
<h2 id="luxdit-lighting-estimation-with-video-diffusion-transformer"><a href="https://arxiv.org/abs/2509.03680v1">LuxDiT: Lighting Estimation with Video Diffusion Transformer</a></h2>
<p><strong>Authors:</strong> Ruofan Liang, Kai He, Zan Gojcic, Igor Gilitschenski, Sanja Fidler, Nandita Vijaykumar, Zian Wang</p>
<p><strong>Published:</strong> 2025-09-03</p>
<p><strong>Categories:</strong> cs.GR, cs.AI, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Estimating scene lighting from a single image or video remains a longstanding
challenge in computer vision and graphics. Learning-based approaches are
constrained by the scarcity of ground-truth HDR environment maps, which are
expensive to capture and limited in diversity. While recent generative models
offer strong priors for image synthesis, lighting estimation remains difficult
due to its reliance on indirect visual cues, the need to infer global
(non-local) context, and the recovery of high-dynamic-range outputs. We propose
LuxDiT, a novel data-driven approach that fine-tunes a video diffusion
transformer to generate HDR environment maps conditioned on visual input.
Trained on a large synthetic dataset with diverse lighting conditions, our
model learns to infer illumination from indirect visual cues and generalizes
effectively to real-world scenes. To improve semantic alignment between the
input and the predicted environment map, we introduce a low-rank adaptation
finetuning strategy using a collected dataset of HDR panoramas. Our method
produces accurate lighting predictions with realistic angular high-frequency
details, outperforming existing state-of-the-art techniques in both
quantitative and qualitative evaluations.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我对这篇关于 LuxDiT 的论文摘要进行如下分析：</p>
<hr />
<h3 id="luxdit-lighting-estimation-with-video-diffusion-transformer_1">LuxDiT: Lighting Estimation with Video Diffusion Transformer 摘要分析</h3>
<p><strong>1. 论文主要贡献的简洁总结 (2-3 句话)</strong></p>
<p>LuxDiT 提出了一种新颖的数据驱动方法，通过微调视频扩散Transformer来解决从图像或视频估计场景照明的长期挑战。该模型利用大规模合成数据集学习从间接视觉线索推断HDR环境光照，并通过低秩适应微调策略增强语义对齐，最终生成高精度、细节丰富的HDR环境图，超越现有SOTA。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<p>核心创新在于将<strong>视频扩散Transformer</strong>架构应用于<strong>HDR环境光照估计</strong>这一具有挑战性的任务。这与传统扩散模型主要用于图像生成不同，它需要从间接视觉线索中推断全局（非局部）上下文并输出高动态范围数据。</p>
<p>方法论上，它巧妙地结合了：
*   <strong>大规模合成数据训练</strong>：克服真实世界HDR环境图稀缺性，学习基础光照推断能力。
*   <strong>低秩适应（LoRA）微调策略</strong>：利用收集到的真实HDR全景图数据集，高效地改善模型在真实场景中的语义对齐和泛化能力，同时避免对大型预训练模型进行昂贵的全面微调。</p>
<p><strong>3. 对领域潜在影响</strong></p>
<ul>
<li><strong>高质量渲染与虚拟内容创作</strong>：为电影、游戏、虚拟现实/增强现实（VR/AR）等领域提供更准确、更真实的场景光照估计，极大提升渲染质量和沉浸感。</li>
<li><strong>逆向图形学（Inverse Graphics）</strong>：推动从2D图像或视频中恢复3D场景属性（如光照）的研究，是理解世界的重要一步。</li>
<li><strong>数据驱动模型范式</strong>：展示了如何有效利用合成数据克服真实数据稀缺性，并通过高效微调策略（如LoRA）实现向真实世界的泛化，为其他类似任务提供了有价值的范例。</li>
<li><strong>扩散模型应用拓展</strong>：将扩散模型从内容生成扩展到复杂的逆向问题解决，拓宽了其在计算机视觉领域的应用边界。</li>
</ul>
<p><strong>4. 可能受益于这项研究的相关领域或应用</strong></p>
<ul>
<li><strong>电影与游戏制作</strong>：实现虚拟角色与真实场景的无缝融合，或对现有场景进行光照调整。</li>
<li><strong>增强现实（AR）与虚拟现实（VR）</strong>：在真实环境中准确放置虚拟物体，并使其光照与环境一致，提升真实感和沉浸感。</li>
<li><strong>3D重建与场景理解</strong>：光照是场景几何和材质推断的关键线索。</li>
<li><strong>计算摄影</strong>：图像后期处理中的光照调整、风格迁移等。</li>
<li><strong>数字人与虚拟形象</strong>：为数字人提供逼真的环境光照，使其在不同场景下表现自然。</li>
</ul>
<p><strong>5. 从摘要中可推断的局限性</strong></p>
<ul>
<li><strong>合成数据与真实世界差距</strong>：尽管使用了大规模合成数据并进行了真实数据微调，但合成数据与真实世界之间固有的领域差距（domain gap）可能仍然存在，尤其是在极端或未见过的真实光照条件下，模型的泛化能力可能受到挑战。</li>
<li><strong>对间接视觉线索的依赖</strong>：光照估计高度依赖于场景中的阴影、反射、高光等间接线索。在这些线索不明显、模糊或被遮挡的场景中，模型的鲁棒性可能下降。</li>
<li><strong>全局上下文推断的局限性</strong>：从有限的视觉输入（单张图像或视频片段）推断整个360度HDR环境图，本质上是一个欠定问题。模型可能难以准确捕捉到输入视图之外的复杂或遮挡的光照信息。</li>
<li><strong>计算成本</strong>：视频扩散Transformer模型通常计算量较大，尤其是在生成高分辨率HDR输出时，推理速度和资源消耗可能是一个实际应用中的考量。</li>
<li><strong>微调数据集的质量与多样性</strong>：虽然提到了使用“收集到的HDR全景图数据集”进行LoRA微调，但该数据集的规模、多样性和代表性将直接影响模型在真实世界场景中的最终性能和语义对齐效果。</li>
</ul>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose
LuxDiT, a novel data-driven approach that fine-tunes a video diffusion
transformer to generate HDR environment maps conditioned on visual input.</li>
<li>To improve semantic alignment between the
input and the predicted environment map, we introduce a low-rank adaptation
finetuning strategy using a collected dataset of HDR panoramas.</li>
<li>Our method
produces accurate lighting predictions with realistic angular high-frequency
details, outperforming existing state-of-the-art techniques in both
quantitative and qualitative evaluations.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="http://arxiv.org/pdf/2509.03680v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.03680v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.03498v1'></a></p>
<h2 id="onecat-decoder-only-auto-regressive-model-for-unified-understanding-and-generation"><a href="https://arxiv.org/abs/2509.03498v1">OneCAT: Decoder-Only Auto-Regressive Model for Unified Understanding and Generation</a></h2>
<p><strong>Authors:</strong> Han Li, Xinyu Peng, Yaoming Wang, Zelin Peng, Xin Chen, Rongxiang Weng, Jingang Wang, Xunliang Cai, Wenrui Dai, Hongkai Xiong</p>
<p><strong>Published:</strong> 2025-09-03</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We introduce OneCAT, a unified multimodal model that seamlessly integrates
understanding, generation, and editing within a novel, pure decoder-only
transformer architecture. Our framework uniquely eliminates the need for
external components such as Vision Transformers (ViT) or vision tokenizer
during inference, leading to significant efficiency gains, especially for
high-resolution inputs. This is achieved through a modality-specific
Mixture-of-Experts (MoE) structure trained with a single autoregressive (AR)
objective, which also natively supports dynamic resolutions. Furthermore, we
pioneer a multi-scale visual autoregressive mechanism within the Large Language
Model (LLM) that drastically reduces decoding steps compared to diffusion-based
methods while maintaining state-of-the-art performance. Our findings
demonstrate the powerful potential of pure autoregressive modeling as a
sufficient and elegant foundation for unified multimodal intelligence. As a
result, OneCAT sets a new performance standard, outperforming existing
open-source unified multimodal models across benchmarks for multimodal
generation, editing, and understanding.</p>
<p><strong>Analysis:</strong></p>
<p>这篇论文的摘要展示了计算机视觉和机器学习领域的一个重要进展，特别是在统一多模态智能方面。</p>
<hr />
<h3 id="1-main-contribution">1. 论文主要贡献 (Main Contribution)</h3>
<p>OneCAT引入了一个纯解码器Transformer架构的统一多模态模型，无缝整合了理解、生成和编辑任务。其核心贡献在于推理时无需外部视觉组件（如Vision Transformer或视觉tokenizer），显著提升了高分辨率输入的效率，并通过单一自回归目标和多尺度视觉自回归机制实现了跨多模态基准的SOTA性能。</p>
<h3 id="2-key-innovation-or-methodological-approach_1">2. 关键创新或方法 (Key Innovation or Methodological Approach)</h3>
<ul>
<li><strong>纯解码器Transformer架构 (Pure Decoder-Only Transformer Architecture):</strong> 摒弃了传统的编码器-解码器或带有独立视觉编码器的架构，仅使用一个解码器来处理所有模态的输入和输出。</li>
<li><strong>推理时无需外部视觉组件 (Elimination of External Vision Components during Inference):</strong> 通过模态特定的专家混合 (Mixture-of-Experts, MoE) 结构，模型在推理时可以直接处理原始视觉输入，无需预处理的ViT或视觉tokenizer，显著提高了效率。</li>
<li><strong>单一自回归 (AR) 目标训练 (Single Autoregressive Objective Training):</strong> 整个模型通过一个统一的自回归目标进行训练，简化了训练范式，并原生支持动态分辨率。</li>
<li><strong>多尺度视觉自回归机制 (Multi-scale Visual Autoregressive Mechanism):</strong> 在大型语言模型 (LLM) 内部引入了这种机制，与扩散模型相比，它能大幅减少解码步骤，同时保持领先的性能。</li>
</ul>
<h3 id="3-potential-impact-on-the-field_1">3. 对领域潜在影响 (Potential Impact on the Field)</h3>
<ul>
<li><strong>推动统一多模态模型范式发展:</strong> OneCAT证明了纯自回归建模作为统一多模态智能基础的强大潜力，可能引导未来研究转向更简洁、优雅的架构。</li>
<li><strong>显著提升多模态推理效率:</strong> 特别是对于高分辨率图像和视频处理，无需外部视觉组件和减少解码步骤的特性，将极大地加速多模态应用的部署和实时性。</li>
<li><strong>简化模型架构和部署:</strong> 减少对多个独立组件的依赖，使得多模态模型的开发、训练和部署过程更加简化和高效。</li>
<li><strong>为多模态智能设定新性能标准:</strong> 在生成、编辑和理解等多个任务上超越现有开源模型，将激励社区进一步探索和优化统一多模态模型。</li>
</ul>
<h3 id="4-related-areas-or-applications-that-might-benefit-from-this-research">4. 相关领域或应用 (Related Areas or Applications that Might Benefit from this Research)</h3>
<ul>
<li><strong>多模态内容创作:</strong> 文本到图像/视频生成、图像编辑、风格迁移、创意设计工具。</li>
<li><strong>高级视觉理解:</strong> 图像/视频问答 (VQA)、详细描述生成、场景理解、事件检测。</li>
<li><strong>人机交互:</strong> 更自然、高效的视觉交互界面，例如通过文本指令直接编辑图像或生成视觉内容。</li>
<li><strong>辅助技术:</strong> 为视觉障碍人士提供更准确、实时的图像和视频描述。</li>
<li><strong>具身智能/机器人:</strong> 机器人通过视觉感知环境、理解指令并生成相应的视觉反馈或行动。</li>
<li><strong>医疗影像分析:</strong> 结合文本报告生成影像、对影像进行编辑以辅助诊断、从影像中提取关键信息。</li>
</ul>
<h3 id="5-limitations-that-can-be-inferred-from-the-abstract">5. 可推断的局限性 (Limitations that Can Be Inferred from the Abstract)</h3>
<ul>
<li><strong>训练成本:</strong> 统一多模态模型，特别是结合MoE结构，通常需要巨大的计算资源和数据进行训练。摘要中未提及训练的规模和成本，这可能是一个潜在的挑战。</li>
<li><strong>MoE的复杂性与负载均衡:</strong> 尽管推理时效率高，但MoE结构在训练和维护上可能增加复杂性，并需要精细的负载均衡策略来确保专家网络的有效利用。</li>
<li><strong>自回归生成固有限制:</strong> 尽管声称减少了解码步骤，但纯自回归生成在某些场景下仍可能面临生成速度（相对于完全并行）或生成多样性的挑战，尤其是在处理非常长的序列时。</li>
<li><strong>多尺度机制的泛化性:</strong> 这种新颖的多尺度视觉自回归机制在处理极端复杂或特定领域视觉数据（如医学影像、卫星图像）时的鲁棒性仍需进一步验证。</li>
<li><strong>对新模态的扩展性:</strong> 抽象中主要提及视觉和文本，模型如何无缝扩展到其他模态（如音频、3D数据、触觉信息）可能是一个未来的考量。</li>
</ul>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce OneCAT, a unified multimodal model that seamlessly integrates
understanding, generation, and editing within a novel, pure decoder-only
transformer architecture.</li>
<li>Furthermore, we
pioneer a multi-scale visual autoregressive mechanism within the Large Language
Model (LLM) that drastically reduces decoding steps compared to diffusion-based
methods while maintaining state-of-the-art performance.</li>
<li>As a
result, OneCAT sets a new performance standard, outperforming existing
open-source unified multimodal models across benchmarks for multimodal
generation, editing, and understanding.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="http://arxiv.org/pdf/2509.03498v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.03498v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.03903v1'></a></p>
<h2 id="a-generative-foundation-model-for-chest-radiography"><a href="https://arxiv.org/abs/2509.03903v1">A Generative Foundation Model for Chest Radiography</a></h2>
<p><strong>Authors:</strong> Yuanfeng Ji, Dan Lin, Xiyue Wang, Lu Zhang, Wenhui Zhou, Chongjian Ge, Ruihang Chu, Xiaoli Yang, Junhan Zhao, Junsong Chen, Xiangde Luo, Sen Yang, Jin Fang, Ping Luo, Ruijiang Li</p>
<p><strong>Published:</strong> 2025-09-04</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>The scarcity of well-annotated diverse medical images is a major hurdle for
developing reliable AI models in healthcare. Substantial technical advances
have been made in generative foundation models for natural images. Here we
develop `ChexGen', a generative vision-language foundation model that
introduces a unified framework for text-, mask-, and bounding box-guided
synthesis of chest radiographs. Built upon the latent diffusion transformer
architecture, ChexGen was pretrained on the largest curated chest X-ray dataset
to date, consisting of 960,000 radiograph-report pairs. ChexGen achieves
accurate synthesis of radiographs through expert evaluations and quantitative
metrics. We demonstrate the utility of ChexGen for training data augmentation
and supervised pretraining, which led to performance improvements across
disease classification, detection, and segmentation tasks using a small
fraction of training data. Further, our model enables the creation of diverse
patient cohorts that enhance model fairness by detecting and mitigating
demographic biases. Our study supports the transformative role of generative
foundation models in building more accurate, data-efficient, and equitable
medical AI systems.</p>
<p><strong>Analysis:</strong></p>
<p>这篇论文摘要展示了计算机视觉和机器学习领域在医疗AI应用方面的一个重要进展。以下是详细分析：</p>
<hr />
<h3 id="1-2-3">1. 论文主要贡献的简明摘要 (2-3句话)</h3>
<p>本文提出了<code>ChexGen</code>，一个用于胸部X射线图像的生成式视觉-语言基础模型，旨在解决医学图像标注稀缺的问题。它基于潜在扩散Transformer架构，能够实现文本、掩码和边界框引导的图像合成，并在迄今为止最大的胸部X射线数据集上进行了预训练。该模型在数据增强、预训练以及提升下游任务性能和模型公平性方面展现出巨大潜力，预示着生成式基础模型在构建更准确、数据高效和公平的医疗AI系统中的变革性作用。</p>
<h3 id="2_1">2. 关键创新或方法学方法</h3>
<ul>
<li><strong>统一的生成框架：</strong> ChexGen的核心创新在于其提供了一个<strong>统一的框架</strong>，能够实现<strong>文本、掩码和边界框引导</strong>的胸部X射线图像合成。这意味着用户可以通过多种模态（自然语言描述、区域掩码或边界框）来精确控制图像的生成内容和结构，这在医学图像生成领域是高度灵活和新颖的。</li>
<li><strong>生成式视觉-语言基础模型：</strong> 将“生成式模型”与“视觉-语言”能力相结合，并将其定位为“基础模型”，表明其旨在通过大规模预训练学习通用表示，并能适应多种下游任务。</li>
<li><strong>潜在扩散Transformer架构：</strong> 采用了当前最先进的生成模型架构之一——潜在扩散Transformer。这种架构以其高质量的图像生成能力和对复杂数据分布的建模能力而闻名，将其应用于医学影像领域是前沿实践。</li>
<li><strong>大规模医学数据集预训练：</strong> 在包含960,000对放射图像-报告的“迄今为止最大的”胸部X射线数据集上进行预训练，这为模型学习到丰富的医学知识和图像特征提供了坚实基础，是实现其“基础模型”能力的关键。</li>
</ul>
<h3 id="3_1">3. 对领域潜在影响</h3>
<ul>
<li><strong>缓解医学数据稀缺性：</strong> 这是最直接和显著的影响。通过生成高质量、多样化的合成医学图像，ChexGen能够有效补充真实标注数据的不足，极大地降低了开发和训练高性能医疗AI模型的门槛。</li>
<li><strong>提升下游任务性能和数据效率：</strong> 作为数据增强工具和预训练策略，ChexGen能够显著提升疾病分类、检测和分割等任务的性能，尤其是在仅有少量真实训练数据的情况下，这对于快速迭代和部署医疗AI模型至关重要。</li>
<li><strong>促进模型公平性与偏见缓解：</strong> 能够创建多样化的患者队列，用于检测和缓解AI模型中的人口统计学偏见，这在医疗AI领域具有深远的社会和伦理意义。它为构建更公平、更值得信赖的医疗AI系统提供了强大的工具。</li>
<li><strong>推动医学AI基础模型发展：</strong> ChexGen的成功将激励更多研究者探索在其他医学影像模态（如CT、MRI）和疾病领域开发类似的生成式基础模型，加速整个医学AI领域的发展。</li>
</ul>
<h3 id="4_1">4. 可能受益的相关领域或应用</h3>
<ul>
<li><strong>医学图像分析与诊断：</strong> 直接应用于疾病分类、检测和分割，辅助医生进行诊断。</li>
<li><strong>医疗AI模型开发与部署：</strong> 作为数据增强工具，加速新模型的训练和迭代；作为预训练策略，提升模型在小样本数据上的泛化能力。</li>
<li><strong>模型公平性与偏见缓解研究：</strong> 用于生成具有特定人口统计学特征的合成数据，以识别、量化和缓解AI模型中的偏见。</li>
<li><strong>医疗教育与模拟：</strong> 生成各种病理图像用于教学和医生培训，提供多样化的学习案例。</li>
<li><strong>隐私保护数据共享与研究：</strong> 在某些场景下，合成数据可以作为真实数据的替代品，用于研究和开发，同时保护患者隐私。</li>
<li><strong>个性化医疗：</strong> 理论上，未来可以根据特定患者的特征生成定制化的模拟图像，用于治疗方案的规划和评估。</li>
</ul>
<h3 id="5_1">5. 从摘要中可推断的局限性</h3>
<ul>
<li><strong>合成图像的临床真实性与细节：</strong> 尽管摘要声称“准确合成”，但对于生成图像在临床上是否能完全模拟真实病理的细微特征、罕见病变或复杂病理共存的情况，仍需更深入的验证。生成模型可能存在“幻觉”或生成临床上不合理的特征的风险。</li>
<li><strong>数据集的覆盖范围与多样性：</strong> 尽管使用了“迄今为止最大的”数据集，但医学影像的复杂性和多样性是无限的。该模型可能在训练数据中未充分代表的罕见疾病、特定人群或复杂病理模式上表现不佳。</li>
<li><strong>偏见缓解的实际效果：</strong> 摘要提到能够检测和缓解人口统计学偏见，但未详细说明其效果的量化评估和局限性。生成多样化队列是否能完全消除所有潜在偏见，以及是否会引入新的合成偏见，仍是开放问题。</li>
<li><strong>计算资源需求：</strong> 基础模型通常需要大量的计算资源进行训练和推理，这可能限制其在资源受限环境中的广泛应用。</li>
<li><strong>对下游任务性能提升的程度：</strong> 摘要指出“使用一小部分训练数据”即可提升性能，但未明确这种提升是否能达到或超越使用完整真实数据集训练的模型性能，以及“一小部分”的具体量化标准。</li>
</ul>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We demonstrate the utility of ChexGen for training data augmentation
and supervised pretraining, which led to performance improvements across
disease classification, detection, and segmentation tasks using a small
fraction of training data.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="http://arxiv.org/pdf/2509.03903v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.03903v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.04324v1'></a></p>
<h2 id="ovgrasp-open-vocabulary-grasping-assistance-via-multimodal-intent-detection"><a href="https://arxiv.org/abs/2509.04324v1">OVGrasp: Open-Vocabulary Grasping Assistance via Multimodal Intent Detection</a></h2>
<p><strong>Authors:</strong> Chen Hu, Shan Luo, Letizia Gionfrida</p>
<p><strong>Published:</strong> 2025-09-04</p>
<p><strong>Categories:</strong> cs.RO, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Grasping assistance is essential for restoring autonomy in individuals with
motor impairments, particularly in unstructured environments where object
categories and user intentions are diverse and unpredictable. We present
OVGrasp, a hierarchical control framework for soft exoskeleton-based grasp
assistance that integrates RGB-D vision, open-vocabulary prompts, and voice
commands to enable robust multimodal interaction. To enhance generalization in
open environments, OVGrasp incorporates a vision-language foundation model with
an open-vocabulary mechanism, allowing zero-shot detection of previously unseen
objects without retraining. A multimodal decision-maker further fuses spatial
and linguistic cues to infer user intent, such as grasp or release, in
multi-object scenarios. We deploy the complete framework on a custom
egocentric-view wearable exoskeleton and conduct systematic evaluations on 15
objects across three grasp types. Experimental results with ten participants
demonstrate that OVGrasp achieves a grasping ability score (GAS) of 87.00%,
outperforming state-of-the-art baselines and achieving improved kinematic
alignment with natural hand motion.</p>
<p><strong>Analysis:</strong></p>
<p>作为计算机视觉和机器学习领域的专家，我对这篇论文摘要的分析如下：</p>
<hr />
<h3 id="ovgrasp-open-vocabulary-grasping-assistance-via-multimodal-intent-detection_1">OVGrasp: Open-Vocabulary Grasping Assistance via Multimodal Intent Detection</h3>
<p><strong>1. 论文主要贡献的简明摘要 (Concise Summary)</strong></p>
<p>OVGrasp 提出了一个用于软体外骨骼的层级控制框架，旨在为运动障碍者提供开放词汇抓取辅助。它通过整合 RGB-D 视觉、开放词汇提示和语音命令，利用视觉-语言基础模型实现对未知物体的零样本检测，并通过多模态决策器推断用户在多物体场景中的抓取或释放意图。实验结果表明，该系统在抓取能力和运动对齐方面均优于现有技术。</p>
<p><strong>2. 关键创新或方法学方法 (Key Innovation or Methodological Approach)</strong></p>
<p>该论文的核心创新在于其<strong>开放词汇（Open-Vocabulary）能力</strong>和<strong>多模态意图检测</strong>。
1.  <strong>开放词汇机制：</strong> OVGrasp 集成了一个视觉-语言基础模型，使其能够对先前未见的物体进行零样本检测，而无需重新训练。这极大地增强了系统在非结构化、多样化环境中的泛化能力，是计算机视觉领域的一个重要进展，尤其是在实际应用中。
2.  <strong>多模态意图决策器：</strong> 系统设计了一个多模态决策器，能够融合来自 RGB-D 视觉的空间线索和来自开放词汇提示/语音命令的语言线索，从而在多物体场景中准确推断用户的具体意图（如抓取或释放）。这种对复杂用户意图的理解，超越了简单的物体识别，是人机交互和辅助机器人领域的关键突破。
3.  <strong>集成框架：</strong> 将这些先进的 CV/NLP 技术与定制的、佩戴式、第一人称视角的软体外骨骼相结合，形成一个实用的、端到端的层级控制系统，实现了从感知到决策再到执行的闭环。</p>
<p><strong>3. 对领域潜在影响 (Potential Impact on the Field)</strong></p>
<ol>
<li><strong>辅助机器人与人机交互 (Assistive Robotics &amp; HRI)：</strong> OVGrasp 直接解决了运动障碍者在日常生活中恢复自主性的关键挑战，通过提供更自然、直观且适应性强的抓取辅助，将极大地改善他们的生活质量。它为未来辅助机器人系统的设计提供了新的范式。</li>
<li><strong>计算机视觉与视觉-语言模型 (Computer Vision &amp; Vision-Language Models)：</strong> 该研究展示了视觉-语言基础模型在实际、具身（embodied）机器人应用中的强大潜力，特别是在开放世界、零样本物体识别和理解方面。它推动了 CV 领域从静态图像识别向动态、交互式、多模态感知的演进。</li>
<li><strong>多模态人工智能 (Multimodal AI)：</strong> 论文在融合空间和语言信息以推断复杂用户意图方面取得了进展，为多模态学习和决策制定提供了新的思路和实证。</li>
<li><strong>软体机器人与可穿戴设备 (Soft Robotics &amp; Wearable Devices)：</strong> 结合软体外骨骼和第一人称视角视觉，为可穿戴机器人系统的设计和控制提供了宝贵的经验和技术参考。</li>
</ol>
<p><strong>4. 可能受益于此研究的相关领域或应用 (Related Areas or Applications)</strong></p>
<ol>
<li><strong>通用型机器人操作 (General-purpose Robotic Manipulation)：</strong> 提升机器人在非结构化环境中处理多样化物体的能力，例如在物流、仓储或服务机器人领域。</li>
<li><strong>人机协作 (Human-Robot Collaboration)：</strong> 改进机器人对人类指令和意图的理解，实现更流畅、更安全的协作，尤其是在工业或医疗场景中。</li>
<li><strong>远程操作与探索 (Teleoperation &amp; Exploration)：</strong> 为远程控制系统提供更智能的物体识别和意图推断能力，减少操作员的认知负担。</li>
<li><strong>智能家居与智慧医疗 (Smart Home &amp; Smart Healthcare)：</strong> 将开放词汇和多模态交互能力集成到其他智能设备中，提供更广泛的智能辅助服务。</li>
<li><strong>增强现实/虚拟现实 (Augmented Reality/Virtual Reality)：</strong> 在 AR/VR 环境中实现更自然的物体交互和用户意图理解。</li>
</ol>
<p><strong>5. 从摘要中可推断出的局限性 (Limitations that can be inferred from the abstract)</strong></p>
<ol>
<li><strong>评估范围的局限性 (Limited Evaluation Scope):</strong> 尽管在15个物体和3种抓取类型上进行了系统评估，并有10名参与者，但与“多样化且不可预测”的真实世界非结构化环境相比，这仍然是一个相对有限的测试集。开放词汇能力虽强，但其在极端多样性、罕见物体或高度相似物体区分上的泛化能力仍需更广泛、更严苛的验证。</li>
<li><strong>意图识别的复杂性 (Complexity of Intent Recognition):</strong> 目前的意图识别主要集中在“抓取”或“释放”两种基本动作。在实际应用中，用户的意图可能更为复杂和细致（例如，“轻轻抓取”、“移动到某个位置”、“抓取红色的那个”），这可能需要更高级、更具上下文感知能力的意图理解模型。</li>
<li><strong>环境鲁棒性 (Environmental Robustness):</strong> 摘要未详细说明系统在极端光照变化、严重遮挡、高度杂乱、包含透明/反光物体等复杂真实世界条件下的表现。这些是 RGB-D 视觉和视觉-语言模型在实际部署中常见的挑战。</li>
<li><strong>计算资源与实时性 (Computational Resources and Real-time Performance):</strong> 视觉-语言基础模型通常计算量大。对于一个可穿戴、实时响应的系统，其计算开销、功耗和延迟是关键考量，摘要中未提及这些性能指标。</li>
<li><strong>用户适应性与个性化 (User Adaptability &amp; Personalization):</strong> 摘要提到“改进了与自然手部运动的运动对齐”，但未详细说明系统如何适应不同用户的生理差异、偏好或学习曲线。个性化调整对于辅助设备至关重要。</li>
</ol>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present
OVGrasp, a hierarchical control framework for soft exoskeleton-based grasp
assistance that integrates RGB-D vision, open-vocabulary prompts, and voice
commands to enable robust multimodal interaction.</li>
<li>Experimental results with ten participants
demonstrate that OVGrasp achieves a grasping ability score (GAS) of 87.00%,
outperforming state-of-the-art baselines and achieving improved kinematic
alignment with natural hand motion.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="http://arxiv.org/pdf/2509.04324v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.04324v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.03006v1'></a></p>
<h2 id="enhancing-robustness-in-post-processing-watermarking-an-ensemble-attack-network-using-cnns-and-transformers"><a href="https://arxiv.org/abs/2509.03006v1">Enhancing Robustness in Post-Processing Watermarking: An Ensemble Attack Network Using CNNs and Transformers</a></h2>
<p><strong>Authors:</strong> Tzuhsuan Huang, Cheng Yu Yeo, Tsai-Ling Huang, Hong-Han Shuai, Wen-Huang Cheng, Jun-Cheng Chen</p>
<p><strong>Published:</strong> 2025-09-03</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Recent studies on deep watermarking have predominantly focused on
in-processing watermarking, which integrates the watermarking process into
image generation. However, post-processing watermarking, which embeds
watermarks after image generation, offers more flexibility. It can be applied
to outputs from any generative model (e.g. GANs, diffusion models) without
needing access to the model's internal structure. It also allows users to embed
unique watermarks into individual images. Therefore, this study focuses on
post-processing watermarking and enhances its robustness by incorporating an
ensemble attack network during training. We construct various versions of
attack networks using CNN and Transformer in both spatial and frequency domains
to investigate how each combination influences the robustness of the
watermarking model. Our results demonstrate that combining a CNN-based attack
network in the spatial domain with a Transformer-based attack network in the
frequency domain yields the highest robustness in watermarking models.
Extensive evaluation on the WAVES benchmark, using average bit accuracy as the
metric, demonstrates that our ensemble attack network significantly enhances
the robustness of baseline watermarking methods under various stress tests. In
particular, for the Regeneration Attack defined in WAVES, our method improves
StegaStamp by 18.743%. The code is released
at:https://github.com/aiiu-lab/DeepRobustWatermark.</p>
<p><strong>Analysis:</strong></p>
<p>这篇论文深入探讨了后处理数字水印的鲁棒性增强问题，以下是详细分析：</p>
<h3 id="1-concise-summary_1">1. 论文主要贡献的简明摘要 (Concise Summary)</h3>
<p>本文专注于提升后处理数字水印的鲁棒性，提出了一种在训练阶段利用集成攻击网络的方法。该方法结合了基于CNN的空间域攻击和基于Transformer的频率域攻击，实验证明这种组合能显著增强水印模型的抗攻击能力，并在WAVES基准测试中展现出优异性能，尤其在再生攻击下对现有方法有显著提升。</p>
<h3 id="2-key-innovation-or-methodological-approach_2">2. 关键创新或方法论 (Key Innovation or Methodological Approach)</h3>
<p>核心创新在于提出了一种新颖的“集成攻击网络”（Ensemble Attack Network）方法，用于在训练阶段提升后处理水印的鲁棒性。该方法通过结合不同架构（CNN和Transformer）和不同作用域（空间域和频率域）的攻击模块，构建了一个多样化的攻击集成体。这种集成攻击策略迫使水印模型学习对多种潜在攻击更具抵抗力的特征，从而显著增强其鲁棒性。论文还通过实验发现，将基于CNN的空间域攻击与基于Transformer的频率域攻击相结合，能达到最佳的鲁棒性效果。</p>
<h3 id="3-potential-impact-on-the-field_2">3. 对领域潜在影响 (Potential Impact on the Field)</h3>
<p>本研究显著提升了后处理数字水印的实用性和可靠性，使其能更有效地应用于任何生成模型（如GANs、扩散模型）的输出，而无需访问其内部结构。这对于AI生成内容的版权保护、溯源、真实性验证以及打击深度伪造等领域具有重要意义。它也为未来水印技术，特别是对抗性鲁棒性研究，提供了一个新的、有效的训练范式，即通过集成多样化攻击进行对抗训练。</p>
<h3 id="4-related-areas-or-applications_1">4. 相关领域或应用 (Related Areas or Applications)</h3>
<ul>
<li><strong>AI生成内容的版权保护与溯源：</strong> 确保AI生成图像、视频等内容的原创性归属，防止未经授权的使用。</li>
<li><strong>数字媒体真实性验证与防篡改：</strong> 验证图像或视频是否被篡改，尤其是在新闻、法律、医疗等对内容真实性要求极高的领域。</li>
<li><strong>深度伪造（Deepfake）检测与溯源：</strong> 通过嵌入水印来识别和追踪合成内容，对抗恶意深度伪造。</li>
<li><strong>数字取证：</strong> 在网络犯罪调查中提供内容来源和修改历史的线索。</li>
<li><strong>知识产权管理：</strong> 保护数字艺术品和创意内容的知识产权。</li>
</ul>
<h3 id="5-limitations-inferable-from-the-abstract">5. 从摘要中可推断的局限性 (Limitations Inferable from the Abstract)</h3>
<ul>
<li><strong>鲁棒性的绝对上限：</strong> 尽管显著提升了鲁棒性，但任何水印系统都无法保证对所有潜在攻击的绝对抵抗力，未来可能出现更复杂的攻击。</li>
<li><strong>计算资源消耗：</strong> 训练集成攻击网络，特别是结合了CNN和Transformer的复杂模型，可能会带来较高的计算资源和时间成本。</li>
<li><strong>对未知攻击的泛化能力：</strong> 尽管集成了多样化攻击，但其对训练集中未包含的、全新类型的攻击的泛化能力仍需进一步验证。</li>
<li><strong>评估范围：</strong> 评估主要基于WAVES基准测试和平均比特准确率，其在其他特定场景或使用其他评估指标时的表现可能有所不同。</li>
<li><strong>仅限于后处理水印：</strong> 本研究专注于后处理水印，其结论和方法不直接适用于在图像生成过程中嵌入水印的“前处理水印”场景。</li>
</ul>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In
particular, for the Regeneration Attack defined in WAVES, our method improves
StegaStamp by 18.743%.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="http://arxiv.org/pdf/2509.03006v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.03006v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.04446v1'></a></p>
<h2 id="plotn-polish-zero-shot-story-visualization-and-disentangled-editing-with-text-to-image-diffusion-models"><a href="https://arxiv.org/abs/2509.04446v1">Plot'n Polish: Zero-shot Story Visualization and Disentangled Editing with Text-to-Image Diffusion Models</a></h2>
<p><strong>Authors:</strong> Kiymet Akdemir, Jing Shi, Kushal Kafle, Brian Price, Pinar Yanardag</p>
<p><strong>Published:</strong> 2025-09-04</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Text-to-image diffusion models have demonstrated significant capabilities to
generate diverse and detailed visuals in various domains, and story
visualization is emerging as a particularly promising application. However, as
their use in real-world creative domains increases, the need for providing
enhanced control, refinement, and the ability to modify images post-generation
in a consistent manner becomes an important challenge. Existing methods often
lack the flexibility to apply fine or coarse edits while maintaining visual and
narrative consistency across multiple frames, preventing creators from
seamlessly crafting and refining their visual stories. To address these
challenges, we introduce Plot'n Polish, a zero-shot framework that enables
consistent story generation and provides fine-grained control over story
visualizations at various levels of detail.</p>
<p><strong>Analysis:</strong></p>
<p>作为计算机视觉和机器学习领域的专家，我对这篇论文摘要进行如下分析：</p>
<hr />
<h3 id="1-concise-summary_2">1. 论文主要贡献的简明总结 (Concise Summary)</h3>
<p>本文提出了 Plot'n Polish，一个零样本（zero-shot）框架，旨在解决当前文本到图像扩散模型在故事可视化中缺乏一致性控制和后期编辑灵活性的问题。它实现了跨多帧的视觉和叙事一致性故事生成，并提供了对故事可视化在不同细节层级的细粒度控制和解耦编辑能力，从而使创作者能够无缝地精修其视觉故事。</p>
<h3 id="2-key-innovation-or-methodological-approach_3">2. 关键创新或方法学方法 (Key Innovation or Methodological Approach)</h3>
<p>核心创新在于其<strong>零样本（zero-shot）</strong>特性，这意味着该框架无需针对特定故事或编辑任务进行额外训练。更重要的是，它引入了一种<strong>解耦编辑（disentangled editing）</strong>的方法（从标题推断），使得用户能够对故事可视化进行<strong>细粒度（fine-grained）</strong>控制，并在<strong>不同细节层级</strong>（从整体叙事到局部元素）上进行修改，同时确保<strong>跨多帧的视觉和叙事一致性</strong>。这种在保持一致性前提下的多层级、解耦控制是现有方法所欠缺的，它解决了在故事生成中，角色、场景、风格等元素在不同帧之间保持连贯性的核心挑战。</p>
<h3 id="3-potential-impact-on-the-field_3">3. 对领域潜在影响 (Potential Impact on the Field)</h3>
<p>该研究将显著提升文本到图像扩散模型在<strong>故事可视化</strong>领域的实用性和可控性。它为创作者提供了前所未有的灵活性，使其能够<strong>无缝地创作、迭代和精修视觉故事</strong>，从而降低了高质量视觉叙事内容的创作门槛。这不仅能推动<strong>创意产业</strong>（如动画、漫画、游戏概念艺术、广告）的发展，也为未来更复杂的<strong>人机协作内容生成</strong>模式奠定了基础，使AI工具从单纯的生成器转变为更强大的创意助手，能够理解并响应用户在叙事和视觉编辑上的复杂意图。</p>
<h3 id="4-related-areas-or-applications_2">4. 相关领域或应用 (Related Areas or Applications)</h3>
<ul>
<li><strong>创意内容生成:</strong> 动画、漫画、电影预可视化（pre-visualization）、游戏概念艺术、广告创意、数字插画。</li>
<li><strong>个性化内容:</strong> 根据用户输入生成定制化的故事、教育材料或交互式体验。</li>
<li><strong>虚拟现实/增强现实 (VR/AR):</strong> 快速生成和迭代虚拟场景或角色资产，保持其在不同交互状态下的一致性。</li>
<li><strong>数字人/虚拟偶像:</strong> 保持数字角色在不同姿态、表情和场景中的一致性，并进行精细化编辑。</li>
<li><strong>多模态内容理解与生成:</strong> 为文本故事自动配图或生成视频脚本，并允许用户对生成结果进行精细调整。</li>
</ul>
<h3 id="5-limitations-that-can-be-inferred-from-the-abstract_1">5. 可从摘要推断的局限性 (Limitations that can be inferred from the abstract)</h3>
<ul>
<li><strong>零样本方法的局限性:</strong> 尽管零样本是优势，但对于高度特定、风格化或需要极高细节保真度的场景，其性能可能不如经过特定数据微调的模型。在某些极端情况下，零样本方法可能难以捕捉到非常细微或独特的视觉特征。</li>
<li><strong>一致性保持的鲁棒性:</strong> 摘要中强调了“一致性”，但对于极其复杂、叙事跨度长或角色/场景发生剧烈变化的故事，维持完美的一致性仍是一个巨大挑战。模型如何处理角色服装、发型、面部特征在不同帧中的细微变化，以及背景环境的连贯性，是需要验证的。</li>
<li><strong>“细粒度控制”的实际边界:</strong> 摘要中提到“细粒度控制”，但其具体能达到何种程度的精细化编辑（例如，能否精确修改某个角色的微小表情、特定道具的细节），以及这种控制的直观性/易用性，仍需在实际应用中检验。解耦编辑的质量直接影响这一点，如果解耦不彻底，可能会在编辑一个元素时意外影响到其他元素。</li>
<li><strong>计算资源与效率:</strong> 扩散模型通常计算成本较高，尤其是在进行多帧生成和迭代编辑时。摘要中未提及性能或速度，这可能是实际应用中的一个潜在限制，特别是在需要快速迭代的创意工作流中。</li>
</ul>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address these
challenges, we introduce Plot'n Polish, a zero-shot framework that enables
consistent story generation and provides fine-grained control over story
visualizations at various levels of detail.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="http://arxiv.org/pdf/2509.04446v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.04446v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.04268v1'></a></p>
<h2 id="differential-morphological-profile-neural-networks-for-semantic-segmentation"><a href="https://arxiv.org/abs/2509.04268v1">Differential Morphological Profile Neural Networks for Semantic Segmentation</a></h2>
<p><strong>Authors:</strong> David Huangal, J. Alex Hurt</p>
<p><strong>Published:</strong> 2025-09-04</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Semantic segmentation of overhead remote sensing imagery enables applications
in mapping, urban planning, and disaster response. State-of-the-art
segmentation networks are typically developed and tuned on ground-perspective
photographs and do not directly address remote sensing challenges such as
extreme scale variation, foreground-background imbalance, and large image
sizes. We explore the incorporation of the differential morphological profile
(DMP), a multi-scale shape extraction method based on grayscale morphology,
into modern segmentation networks. Prior studies have shown that the DMP can
provide critical shape information to Deep Neural Networks to enable superior
detection and classification performance in overhead imagery. In this work, we
extend prior DMPNet work beyond classification and object detection by
integrating DMP features into three state-of-the-art convolutional and
transformer semantic segmentation architectures. We utilize both direct input,
which adapts the input stem of feature extraction architectures to accept DMP
channels, and hybrid architectures, a dual-stream design that fuses RGB and DMP
encoders. Using the iSAID benchmark dataset, we evaluate a variety of DMP
differentials and structuring element shapes to more effectively provide shape
information to the model. Our results show that while non-DMP models generally
outperform the direct-input variants, hybrid DMP consistently outperforms
direct-input and is capable of surpassing a non-DMP model on mIoU, F1, and
Recall.</p>
<p><strong>Analysis:</strong></p>
<p>作为计算机视觉和机器学习领域的专家，我对这篇论文摘要的分析如下：</p>
<hr />
<h3 id="differential-morphological-profile-neural-networks-for-semantic-segmentation_1">论文摘要分析：Differential Morphological Profile Neural Networks for Semantic Segmentation</h3>
<p><strong>1. 论文主要贡献的简洁总结 (2-3 句话)</strong></p>
<p>本文旨在解决遥感图像语义分割中存在的极端尺度变化、前景背景不平衡等挑战，提出将差分形态学剖面（DMP）这一多尺度形状提取方法融入到先进的语义分割网络中。研究表明，通过RGB和DMP编码器融合的双流“混合架构”，能够显著提升模型性能，在mIoU、F1和Recall等指标上超越非DMP基线模型，为遥感图像分析提供了更鲁棒的解决方案。</p>
<p><strong>2. 关键创新或方法学方法</strong></p>
<p>本文的关键创新在于将差分形态学剖面（DMP）——一种基于灰度形态学的多尺度形状提取方法——首次扩展并成功应用于语义分割任务，超越了其在分类和目标检测中的传统应用。其核心方法学贡献是提出了两种DMP集成策略：
*   <strong>直接输入 (Direct Input)</strong>：调整特征提取架构的输入层以接受DMP通道。
*   <strong>混合架构 (Hybrid Architectures)</strong>：一种更有效的双流设计，分别使用RGB和DMP编码器，并将两者的特征进行融合。
通过在iSAID数据集上对不同DMP差分和结构元素形状的系统评估，证明了混合架构在提供关键形状信息方面的优越性。</p>
<p><strong>3. 对领域潜在影响</strong></p>
<ul>
<li><strong>提升遥感图像分析精度：</strong> 本研究直接解决了遥感图像语义分割的固有挑战，有望显著提高地图绘制、城市规划和灾害响应等应用中的自动化和精度。</li>
<li><strong>重新审视特征工程的价值：</strong> 在深度学习主导的时代，本文强调了结合传统、领域特定（如形态学）特征与深度学习模型的潜力，为混合模型设计提供了新的思路。</li>
<li><strong>启发多模态/多特征融合：</strong> 成功融合DMP和RGB信息，可能启发研究人员探索将其他互补的、非像素级特征（如高程、光谱指数等）融入深度学习模型，以应对更复杂的视觉任务。</li>
<li><strong>推动DMP在更广泛CV任务中的应用：</strong> 证明DMP在语义分割中的有效性，可能会促使DMP在其他需要精细形状理解的计算机视觉任务中得到更广泛的探索。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用</strong></p>
<ul>
<li><strong>地理信息系统 (GIS) 和测绘：</strong> 提高地物分类、土地覆盖制图和变化检测的自动化水平和准确性。</li>
<li><strong>智慧城市和城市规划：</strong> 精准识别建筑物、道路、绿地等，支持城市基础设施管理和发展规划。</li>
<li><strong>灾害响应与管理：</strong> 快速准确地评估灾情，如洪水淹没区域、受损建筑物识别，辅助救援决策。</li>
<li><strong>环境监测：</strong> 监测森林砍伐、冰川融化、农作物健康状况等，提供精细化的环境数据。</li>
<li><strong>农业遥感：</strong> 精准识别农作物类型、生长区域，支持智能农业管理。</li>
<li><strong>国防与情报：</strong> 提升对军事设施、交通网络等目标的识别和分析能力。</li>
</ul>
<p><strong>5. 从摘要中可推断的局限性</strong></p>
<ul>
<li><strong>计算成本增加：</strong> 混合双流架构通常比单流模型具有更高的计算复杂度和参数量，这可能导致训练和推理时间增加，对资源受限的应用构成挑战。</li>
<li><strong>DMP参数调优的复杂性：</strong> 摘要提到评估了“各种DMP差分和结构元素形状”，这暗示DMP的参数（如结构元素大小、形状、差分阶数）可能需要针对不同数据集或任务进行细致的调优，这增加了模型的部署和泛化难度。</li>
<li><strong>“可以超越”的限定性：</strong> 结果显示“hybrid DMP consistently outperforms direct-input and is capable of surpassing a non-DMP model”，其中“capable of surpassing”可能意味着并非在所有情况下或所有指标上都能绝对超越非DMP模型，或者超越的幅度可能有限，这需要进一步的实验细节来验证。</li>
<li><strong>对灰度形态学的依赖：</strong> DMP基于灰度形态学，其有效性可能在很大程度上依赖于图像中形状信息在灰度通道中的可提取性。对于颜色或纹理信息更为关键的场景，DMP的直接贡献可能需要与其他特征结合。</li>
</ul>
<p><strong>Key Findings:</strong></p>
<ul>
<li>State-of-the-art
segmentation networks are typically developed and tuned on ground-perspective
photographs and do not directly address remote sensing challenges such as
extreme scale variation, foreground-background imbalance, and large image
sizes.</li>
<li>In this work, we
extend prior DMPNet work beyond classification and object detection by
integrating DMP features into three state-of-the-art convolutional and
transformer semantic segmentation architectures.</li>
<li>Our results show that while non-DMP models generally
outperform the direct-input variants, hybrid DMP consistently outperforms
direct-input and is capable of surpassing a non-DMP model on mIoU, F1, and
Recall.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="http://arxiv.org/pdf/2509.04268v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.04268v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.03379v1'></a></p>
<h2 id="tinydrop-tiny-model-guided-token-dropping-for-vision-transformers"><a href="https://arxiv.org/abs/2509.03379v1">TinyDrop: Tiny Model Guided Token Dropping for Vision Transformers</a></h2>
<p><strong>Authors:</strong> Guoxin Wang, Qingyuan Wang, Binhua Huang, Shaowu Chen, Deepu John</p>
<p><strong>Published:</strong> 2025-09-03</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>Vision Transformers (ViTs) achieve strong performance in image classification
but incur high computational costs from processing all image tokens. To reduce
inference costs in large ViTs without compromising accuracy, we propose
TinyDrop, a training-free token dropping framework guided by a lightweight
vision model. The guidance model estimates the importance of tokens while
performing inference, thereby selectively discarding low-importance tokens if
large vit models need to perform attention calculations. The framework operates
plug-and-play, requires no architectural modifications, and is compatible with
diverse ViT architectures. Evaluations on standard image classification
benchmarks demonstrate that our framework reduces FLOPs by up to 80% for ViTs
with minimal accuracy degradation, highlighting its generalization capability
and practical utility for efficient ViT-based classification.</p>
<p><strong>Analysis:</strong></p>
<p>作为计算机视觉和机器学习领域的专家，我对这篇关于TinyDrop的论文摘要进行了分析：</p>
<hr />
<h3 id="tinydrop-tiny-model-guided-token-dropping-for-vision-transformers_1">论文摘要分析：TinyDrop: Tiny Model Guided Token Dropping for Vision Transformers</h3>
<p><strong>1. 论文核心贡献的简明总结 (Concise Summary)</strong></p>
<p>TinyDrop提出了一种针对Vision Transformers (ViTs) 的训练无关（training-free）的token丢弃框架，旨在显著降低大型ViTs的推理计算成本，同时保持高精度。该框架通过一个轻量级视觉模型在推理时动态评估token的重要性，并选择性地丢弃低重要性的token。实验证明，TinyDrop能将ViTs的FLOPs降低高达80%，且仅带来极小的精度损失，展现了其在提高ViT效率方面的实用价值。</p>
<p><strong>2. 关键创新或方法学方法 (Key Innovation or Methodological Approach)</strong></p>
<p>核心创新在于其<strong>训练无关（training-free）</strong>和<strong>由轻量级模型引导的动态token丢弃机制</strong>。与需要重新训练或修改架构的现有方法不同，TinyDrop引入了一个外部的、轻量级指导模型，该模型能够在ViT推理过程中<strong>实时（on-the-fly）</strong>评估每个token的重要性。这种方法允许在大型ViT模型执行计算密集型的注意力操作之前，<strong>自适应地、选择性地</strong>丢弃不重要的token。其“即插即用”的特性和对多种ViT架构的兼容性，也极大地降低了其应用门槛和实用性。</p>
<p><strong>3. 对领域潜在影响 (Potential Impact on the Field)</strong></p>
<p>这项研究对计算机视觉领域具有深远影响。它有望<strong>民主化大型高性能ViT模型的部署</strong>，使其能够在资源受限的环境（如移动设备、边缘计算、嵌入式系统）中运行，这些场景对计算预算和延迟有严格要求。通过大幅降低推理FLOPs，TinyDrop也为<strong>可持续AI</strong>做出了贡献，减少了大型模型运行的能源消耗。此外，其即插即用的特性可以加速ViT模型的研发和应用，为现有和未来的ViT架构提供一个易于集成的效率优化方案。</p>
<p><strong>4. 可能受益的相关领域或应用 (Related Areas or Applications)</strong></p>
<p>尽管摘要主要关注图像分类，但token高效处理的核心思想对广泛的ViT应用都具有价值：</p>
<ul>
<li><strong>实时计算机视觉系统：</strong> 自动驾驶、机器人、视频监控等对低延迟有严格要求的场景。</li>
<li><strong>边缘AI/移动计算：</strong> 在计算能力和电池寿命有限的设备上部署复杂的ViT模型。</li>
<li><strong>视频理解：</strong> 通过丢弃时间和空间上的冗余token，高效处理视频序列。</li>
<li><strong>医学影像分析：</strong> 加速对大型医学图像的分析，可能缩短诊断时间。</li>
<li><strong>其他ViT-based任务：</strong> 例如目标检测、语义分割、实例分割以及使用ViT作为骨干网络的生成模型，这些任务中ViT作为特征提取器，其效率提升将带来整体性能的改善。</li>
</ul>
<p><strong>5. 从摘要中可推断的局限性 (Limitations that can be inferred from the abstract)</strong></p>
<ul>
<li><strong>“极小”精度损失的量化：</strong> 摘要中提到“minimal accuracy degradation”，但“极小”是一个相对概念。在对精度要求极高的应用中，即使是微小的损失也可能无法接受。具体的FLOPs-精度权衡曲线（trade-off curve）未在摘要中详细说明。</li>
<li><strong>指导模型的开销和获取：</strong> 尽管指导模型是“轻量级”的，但它仍然引入了额外的计算开销。摘要没有量化这种开销相对于节省的FLOPs的比例，也没有说明这个指导模型是如何获取或训练的（例如，是否需要预训练、是否需要特定数据，或者是否是自监督的）。虽然token丢弃框架是训练无关的，但指导模型本身可能不是。</li>
<li><strong>Token重要性估计的鲁棒性：</strong> 框架的有效性高度依赖于指导模型准确估计token重要性的能力。对于分布外数据（out-of-distribution data）、对抗性样本或图像中细微特征的鲁棒性可能是一个潜在问题。</li>
<li><strong>对复杂任务的泛化能力：</strong> 评估主要在“标准图像分类基准”上进行。对于更复杂的任务，如需要精细像素级信息的密集预测任务（如分割、检测），丢弃token可能会对性能产生更大的负面影响，因为这些任务通常对局部细节更敏感。</li>
<li><strong>超参数调优：</strong> Token丢弃阈值通常需要进行调优。虽然框架本身是训练无关的，但在新的ViT模型或数据集上找到最佳的丢弃策略可能仍需要一定的经验性搜索。</li>
</ul>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To reduce
inference costs in large ViTs without compromising accuracy, we propose
TinyDrop, a training-free token dropping framework guided by a lightweight
vision model.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="http://arxiv.org/pdf/2509.03379v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.03379v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-09-05 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
