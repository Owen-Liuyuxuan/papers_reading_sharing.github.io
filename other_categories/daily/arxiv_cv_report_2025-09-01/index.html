<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-09-01 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DEâ«¶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Birdâs-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">è®ºæéè¯»</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-Î±
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-08-29/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-09-02/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-09-01">Arxiv Computer Vision Papers - 2025-09-01</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#maybe-you-dont-need-a-u-net-convolutional-feature-upsampling-for-materials-micrograph-segmentation" class="nav-link">Maybe you don't need a U-Net: convolutional feature upsampling for materials micrograph segmentation</a>
                </li>
                <li class="nav-item">
                    <a href="#flora-efficient-synthetic-data-generation-for-object-detection-in-low-data-regimes-via-finetuning-flux-lora" class="nav-link">FLORA: Efficient Synthetic Data Generation for Object Detection in Low-Data Regimes via finetuning Flux LoRA</a>
                </li>
                <li class="nav-item">
                    <a href="#the-rosario-dataset-v2-multimodal-dataset-for-agricultural-robotics" class="nav-link">The Rosario Dataset v2: Multimodal Dataset for Agricultural Robotics</a>
                </li>
                <li class="nav-item">
                    <a href="#complete-gaussian-splats-from-a-single-image-with-denoising-diffusion-models" class="nav-link">Complete Gaussian Splats from a Single Image with Denoising Diffusion Models</a>
                </li>
                <li class="nav-item">
                    <a href="#efficient-diffusion-based-3d-human-pose-estimation-with-hierarchical-temporal-pruning" class="nav-link">Efficient Diffusion-Based 3D Human Pose Estimation with Hierarchical Temporal Pruning</a>
                </li>
                <li class="nav-item">
                    <a href="#driveqa-passing-the-driving-knowledge-test" class="nav-link">DriveQA: Passing the Driving Knowledge Test</a>
                </li>
                <li class="nav-item">
                    <a href="#domain-generalization-in-the-wild-disentangling-classification-from-domain-aware-representations" class="nav-link">Domain Generalization in-the-Wild: Disentangling Classification from Domain-Aware Representations</a>
                </li>
                <li class="nav-item">
                    <a href="#uitron-foundational-gui-agent-with-advanced-perception-and-planning" class="nav-link">UItron: Foundational GUI Agent with Advanced Perception and Planning</a>
                </li>
                <li class="nav-item">
                    <a href="#cad2dmd-set-synthetic-generation-tool-of-digital-measurement-device-cad-model-datasets-for-fine-tuning-large-vision-language-models" class="nav-link">CAD2DMD-SET: Synthetic Generation Tool of Digital Measurement Device CAD Model Datasets for fine-tuning Large Vision-Language Models</a>
                </li>
                <li class="nav-item">
                    <a href="#mapping-like-a-skeptic-probabilistic-bev-projection-for-online-hd-mapping" class="nav-link">Mapping like a Skeptic: Probabilistic BEV Projection for Online HD Mapping</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-09-01">Arxiv Computer Vision Papers - 2025-09-01</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>å¥½çï¼è¿æ¯ä¸ä»½éå¯¹2025å¹´8æ29æ¥Arxivè®¡ç®æºè§è§é¢åè®ºæçæ¯æ¥æ¥åæ§è¡æè¦ï¼</p>
<hr />
<p><strong>Arxiv è®¡ç®æºè§è§æ¯æ¥æ¥åæ§è¡æè¦ (2025å¹´8æ29æ¥)</strong></p>
<p><strong>æ¦è¿°ï¼</strong>
ä»æ¥Arxivè®¡ç®æºè§è§é¢åçè®ºæåç°åºå ä¸ªæ¾èè¶å¿ï¼<strong>åææ°æ®çæ</strong>å¨è§£å³ä½æ°æ®éåå¾®è°å¤§åæ¨¡åæ¹é¢æ®æ¼çè¶æ¥è¶éè¦çè§è²ï¼<strong>3Dè§è§ä¸éå»º</strong>ææ¯æç»­åæ°ï¼ç¹å«æ¯ç»åæ©æ£æ¨¡åå®ç°åå¾3Déå»ºåé«æå§¿æä¼°è®¡ï¼<strong>èªå¨é©¾é©¶ä¸æºå¨äºº</strong>é¢åçç ç©¶ä¾ç¶æ´»è·ï¼æ¶µçäºæ°æ®éãç¥è¯é®ç­åé«ç²¾å°å¾ï¼æ­¤å¤ï¼å¯¹<strong>æ¨¡åæçãæ³åè½å</strong>ä»¥å<strong>éç¨AIä»£ç</strong>çæ¢ç´¢ä¹å¼äººæ³¨ç®ã</p>
<p><strong>ä¸»è¦ä¸»é¢ä¸è¶å¿ï¼</strong></p>
<ol>
<li><strong>é«æåææ°æ®çæï¼</strong> å¤ç¯è®ºæèç¦äºå©ç¨LoRAç­ææ¯é«æçæé«è´¨éåææ°æ®ï¼ä»¥åºå¯¹ç¹å®é¢åï¼å¦å·¥ä¸CADæ¨¡åãä½æ°æ®éåºæ¯ï¼çæ°æ®ç¨ç¼ºé®é¢ï¼å¹¶æ¯æå¤§åè§è§-è¯­è¨æ¨¡åï¼LVMsï¼çå¾®è°ã</li>
<li><strong>3Dè§è§ä¸æ©æ£æ¨¡åçèåï¼</strong> æ©æ£æ¨¡åå¨3Déå»ºåå§¿æä¼°è®¡ä¸­å±ç°åºå¼ºå¤§æ½åï¼å®ç°äºä»åå¼ å¾åçæå®æ´é«æ¯æ³¼æºï¼Gaussian Splatsï¼ä»¥åé«æç3Däººä½å§¿æä¼°è®¡ã</li>
<li><strong>èªå¨é©¾é©¶ä¸æºå¨äººæç¥ï¼</strong> æ°çå¤æ¨¡ææ°æ®éåå¸ï¼åæ¶ç ç©¶å³æ³¨äºèªå¨é©¾é©¶ç¥è¯é®ç­ï¼VQAï¼åé²æ£çå¨çº¿é«ç²¾å°å¾æå»ºï¼å¼ºè°äºå®éåºç¨ä¸­çæç¥ä¸æ¨çè½åã</li>
<li><strong>æ¨¡åæçä¸æ³åï¼</strong> æç ç©¶æ¢ç´¢æ¿ä»£ä¼ ç»U-Netæ¶æä»¥æé«åå²æçï¼å¹¶æ·±å¥æ¢è®¨äºå¨âéå¤âåºæ¯ä¸å®ç°é¢åæ³åçæ¹æ³ï¼æ¨å¨æåæ¨¡åå¨æªç¥ç¯å¢ä¸­çé²æ£æ§ã</li>
<li><strong>éç¨AIä»£ççå´èµ·ï¼</strong> åºç°äºæ¨å¨æå»ºåºç¡æ§GUIä»£ççå·¥ä½ï¼ç»ååè¿çæç¥åè§åè½åï¼é¢ç¤ºçéç¨åAIå¨äººæºäº¤äºé¢åçåºç¨åæ¯ã</li>
</ol>
<p><strong>ç¹å«éè¦æåæ°çè®ºæï¼</strong></p>
<ul>
<li><strong>"Complete Gaussian Splats from a Single Image with Denoising Diffusion Models" (Ziwei Liao et al.)ï¼</strong> è¿ç¯è®ºææå·åæ°æ§ï¼å®å©ç¨å»åªæ©æ£æ¨¡åå®ç°äºä»åå¼ å¾åçæå®æ´ç3Dé«æ¯æ³¼æºï¼è¿å¨åç®3Déå»ºé¢åæ¯ä¸ä¸ªéå¤§çªç ´ï¼ææå¤§å¹ç®å3Dåå®¹åå»ºæµç¨ã</li>
<li><strong>"UItron: Foundational GUI Agent with Advanced Perception and Planning" (Zhixiong Zeng et al.)ï¼</strong> è¿é¡¹å·¥ä½ä»£è¡¨äºéç¨AIä»£çåå±çéè¦ä¸æ­¥ãæå»ºä¸ä¸ªè½å¤æç¥åè§åçGUIä»£çï¼å¯¹äºå®ç°æ´æºè½ãæ´èªä¸»çäººæºäº¤äºå·ææ·±è¿å½±åã</li>
<li><strong>"Maybe you don't need a U-Net: convolutional feature upsampling for materials micrograph segmentation" (Ronan Docherty et al.)ï¼</strong> ææäºU-Netè¿ä¸å¨åå²é¢åå¹¿æ³ä½¿ç¨çæ¶æï¼æåºäºä¸ç§å¯è½æ´é«æçå·ç§¯ç¹å¾ä¸éæ ·æ¹æ³ï¼å¯¹äºè¿½æ±æ¨¡åæççç ç©¶èèè¨å·æéè¦åèä»·å¼ã</li>
<li><strong>"FLORA: Efficient Synthetic Data Generation for Object Detection in Low-Data Regimes via finetuning Flux LoRA" (Alvaro Patricio et al.)ï¼</strong> éå¯¹ä½æ°æ®éåºæ¯æä¾äºé«æçåææ°æ®çææ¹æ¡ï¼å¶ç»åLoRAå¾®è°çç­ç¥å·æå¾å¼ºçå®ç¨æ§åæ®éæ§ã</li>
</ul>
<p><strong>æ°å´ç ç©¶æ¹åæææ¯ï¼</strong></p>
<ul>
<li><strong>æ©æ£æ¨¡åå¨3Dçæä¸éå»ºä¸­çæ·±åº¦åºç¨ï¼</strong> ä¸ä»éäºå¾åçæï¼æ©æ£æ¨¡åæ­£æä¸º3Dåå®¹åå»ºï¼å¦é«æ¯æ³¼æºãå§¿æä¼°è®¡ï¼çå³é®ææ¯ã</li>
<li><strong>é¢åç¹å®ä»»å¡åå¤§åæ¨¡åçåææ°æ®çæï¼</strong> ç»åLoRAç­é«æå¾®è°ææ¯ï¼åææ°æ®ä¸åä»ä»æ¯æ°æ®å¢å¼ºï¼èæ¯æä¸ºå®å¶åè®­ç»å¤§åæ¨¡åãè§£å³ç¹å®é¢åæ°æ®ç¶é¢çæ ¸å¿ç­ç¥ã</li>
<li><strong>éç¨åæç¥-è§åAIä»£çï¼</strong> æ¨å¨æå»ºè½å¤çè§£åæä½å¤æç¯å¢ï¼å¦GUIçé¢ï¼çéç¨ä»£çï¼æ¯è¿åæ´é«çº§AIçéè¦ä¸æ­¥ã</li>
<li><strong>é²æ£æ§ä¸é¢åæ³åçæ°èå¼ï¼</strong> éè¿è§£è¦è¡¨ç¤ºå­¦ä¹ åæ¦çå»ºæ¨¡ç­æ¹æ³ï¼æåæ¨¡åå¨å¤æãæªç¥çå®ä¸çç¯å¢ä¸­çå¯é æ§ã</li>
</ul>
<p><strong>å»ºè®®éè¯»å¨æçè®ºæï¼</strong></p>
<ol>
<li><strong>"Complete Gaussian Splats from a Single Image with Denoising Diffusion Models" (Ziwei Liao et al.)ï¼</strong> å¦ææ¨å¯¹3Dè§è§ãæ°é¢ç3Déå»ºæ¹æ³ææ©æ£æ¨¡åæå´è¶£ï¼è¿ç¯è®ºææä¾äºä»¤äººå´å¥çè¿å±ã</li>
<li><strong>"UItron: Foundational GUI Agent with Advanced Perception and Planning" (Zhixiong Zeng et al.)ï¼</strong> å¯¹äºå³æ³¨éç¨AIãAIä»£çæäººæºäº¤äºçç ç©¶äººåï¼è¿ç¯è®ºæå±ç¤ºäºæªæ¥AIåºç¨çä¸ä¸ªéè¦æ¹åã</li>
<li><strong>"FLORA: Efficient Synthetic Data Generation for Object Detection in Low-Data Regimes via finetuning Flux LoRA" (Alvaro Patricio et al.)ï¼</strong> å¯¹äºé¢ä¸´æ°æ®ç¨ç¼ºé®é¢æå¸æé«æå©ç¨åææ°æ®è¿è¡ç®æ æ£æµçç ç©¶èï¼è¿ç¯è®ºææä¾äºå®ç¨çè§£å³æ¹æ¡ã</li>
<li><strong>"Maybe you don't need a U-Net: convolutional feature upsampling for materials micrograph segmentation" (Ronan Docherty et al.)ï¼</strong> å¦ææ¨å¨åå²ä»»å¡ä¸­å¯»æ±æ´é«æçæ¨¡åæ¶æï¼æèå¯¹U-Netçæ¿ä»£æ¹æ¡æå´è¶£ï¼è¿ç¯è®ºæå¼å¾æ·±å¥ç ç©¶ã</li>
<li><strong>"DriveQA: Passing the Driving Knowledge Test" (Maolin Wei et al.)ï¼</strong> å¯¹äºèªå¨é©¾é©¶é¢åçVQAåç¥è¯æ¨çæå´è¶£çç ç©¶äººåï¼è¿ç¯è®ºææåºäºä¸ä¸ªæ°é¢çåºååææã</li>
</ol>
<hr />
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2508.21529v1">Maybe you don't need a U-Net: convolutional feature upsampling for materials micrograph segmentation</a></li>
<li><a href="#2508.21712v1">FLORA: Efficient Synthetic Data Generation for Object Detection in Low-Data Regimes via finetuning Flux LoRA</a></li>
<li><a href="#2508.21635v1">The Rosario Dataset v2: Multimodal Dataset for Agricultural Robotics</a></li>
<li><a href="#2508.21542v1">Complete Gaussian Splats from a Single Image with Denoising Diffusion Models</a></li>
<li><a href="#2508.21363v1">Efficient Diffusion-Based 3D Human Pose Estimation with Hierarchical Temporal Pruning</a></li>
<li><a href="#2508.21824v1">DriveQA: Passing the Driving Knowledge Test</a></li>
<li><a href="#2508.21769v1">Domain Generalization in-the-Wild: Disentangling Classification from Domain-Aware Representations</a></li>
<li><a href="#2508.21767v1">UItron: Foundational GUI Agent with Advanced Perception and Planning</a></li>
<li><a href="#2508.21732v1">CAD2DMD-SET: Synthetic Generation Tool of Digital Measurement Device CAD Model Datasets for fine-tuning Large Vision-Language Models</a></li>
<li><a href="#2508.21689v1">Mapping like a Skeptic: Probabilistic BEV Projection for Online HD Mapping</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2508.21529v1'></a></p>
<h2 id="maybe-you-dont-need-a-u-net-convolutional-feature-upsampling-for-materials-micrograph-segmentation"><a href="https://arxiv.org/abs/2508.21529v1">Maybe you don't need a U-Net: convolutional feature upsampling for materials micrograph segmentation</a></h2>
<p><strong>Authors:</strong> Ronan Docherty, Antonis Vamvakeros, Samuel J. Cooper</p>
<p><strong>Published:</strong> 2025-08-29</p>
<p><strong>Categories:</strong> cs.CV, cond-mat.mtrl-sci</p>
<p><strong>Abstract:</strong></p>
<p>Feature foundation models - usually vision transformers - offer rich semantic
descriptors of images, useful for downstream tasks such as (interactive)
segmentation and object detection. For computational efficiency these
descriptors are often patch-based, and so struggle to represent the fine
features often present in micrographs; they also struggle with the large image
sizes present in materials and biological image analysis. In this work, we
train a convolutional neural network to upsample low-resolution (i.e, large
patch size) foundation model features with reference to the input image. We
apply this upsampler network (without any further training) to efficiently
featurise and then segment a variety of microscopy images, including plant
cells, a lithium-ion battery cathode and organic crystals. The richness of
these upsampled features admits separation of hard to segment phases, like
hairline cracks. We demonstrate that interactive segmentation with these deep
features produces high-quality segmentations far faster and with far fewer
labels than training or finetuning a more traditional convolutional network.</p>
<p><strong>Analysis:</strong></p>
<p>è¿ç¯è®ºæçæè¦æåºäºä¸ç§æ°é¢çæ¹æ³ï¼æ¨å¨è§£å³å½åç¹å¾åºç¡æ¨¡åï¼éå¸¸æ¯Vision Transformerï¼å¨å¤çæ¾å¾®å¾åæ¶é¢ä¸´çææï¼å³é¾ä»¥ææç²¾ç»ç¹å¾åå¤çå¤§å°ºå¯¸å¾åãä»¥ä¸æ¯è¯¦ç»åæï¼</p>
<hr />
<h3 id="1-concise-summary">1. è®ºæä¸»è¦è´¡ç®çç®ææè¦ (Concise Summary)</h3>
<p>æ¬ææåºäºä¸ç§åæ°çå·ç§¯ç¥ç»ç½ç»ä¸éæ ·å¨ï¼ç¨äºå°ä½åè¾¨ççåºç¡æ¨¡åç¹å¾é«æå°æåè³é«åè¾¨çï¼å¹¶ç»ååå§è¾å¥å¾åä¿¡æ¯ãè¯¥ä¸éæ ·å¨æ éé¢å¤è®­ç»å³å¯åºç¨äºå¤ç§æ¾å¾®å¾åï¼å®ç°äºå¯¹å¤ææææ¾å¾®å¾åçé«è´¨éãäº¤äºå¼åå²ï¼ä¸æéæ æ³¨è¿å°äºä¼ ç»å·ç§¯ç½ç»ã</p>
<h3 id="2-key-innovation-or-methodological-approach">2. å³é®åæ°ææ¹æ³å­¦æ¹æ³ (Key Innovation or Methodological Approach)</h3>
<p>æ ¸å¿åæ°å¨äºå¶æåºç<strong>âå·ç§¯ç¹å¾ä¸éæ ·å¨âç½ç»</strong>ãè¯¥ç½ç»æ¯ä¸ä¸ªç¬ç«çå·ç§¯ç¥ç»ç½ç»ï¼ä¸é¨ç¨äºï¼
1.  æ¥æ¶é¢è®­ç»åºç¡æ¨¡åï¼å¦Vision Transformerï¼çæç<strong>ä½åè¾¨çãç²ç²åº¦ç¹å¾å¾</strong>ã
2.  <strong>ç»ååå§è¾å¥å¾åçç²¾ç»çº¹çä¿¡æ¯</strong>ä½ä¸ºåèã
3.  å°è¿äºç¹å¾ä¸éæ ·ä¸º<strong>é«åè¾¨çãè¯­ä¹ä¸°å¯çç¹å¾è¡¨ç¤º</strong>ã</p>
<p>å³é®ä¹å¤å¨äºï¼è¿ä¸ªä¸éæ ·å¨ä¸æ¦è®­ç»å®æï¼ä¾¿å¯æ éé¢å¤è®­ç»æå¾®è°ï¼ç´æ¥åºç¨äºå¤ç§ä¸åçæ¾å¾®å¾ååå²ä»»å¡ï¼ä¾å¦æ¤ç©ç»èãéç¦»å­çµæ± é´æãææºæ¶ä½ï¼ï¼ä»èé«æå°çæé«è´¨éçæ·±åº¦ç¹å¾ï¼è¿èæ¯æäº¤äºå¼åå²ãè¿ä¸U-Netç­ç«¯å°ç«¯åå²ç½ç»æç´æ¥å¾®è°åºç¡æ¨¡åçèå¼ä¸åï¼å®å°ç¹å¾æååç²¾ç»åè§£è¦ï¼ä¸æ³¨äºæååºç¡æ¨¡åçç©ºé´åè¾¨çã</p>
<h3 id="3-potential-impact-on-the-field">3. å¯¹é¢åæ½å¨å½±å (Potential Impact on the Field)</h3>
<p>è¯¥ç ç©¶å¯¹è®¡ç®æºè§è§é¢åï¼ç¹å«æ¯ç§å­¦å¾ååæï¼å¦ææç§å­¦ãçç©å»å­¦ï¼å·ææ¾èæ½å¨å½±åï¼
*   <strong>æååºç¡æ¨¡åå¨å¯éé¢æµä»»å¡ä¸­çå®ç¨æ§ï¼</strong> å®æä¾äºä¸ç§ææç­ç¥ï¼è½å¤å©ç¨ç°æåºç¡æ¨¡åçå¼ºå¤§è¯­ä¹çè§£è½åï¼åæ¶åæå¶å¨ç©ºé´åè¾¨çä¸çåºæä¸è¶³ï¼ä½¿å¶æ´éç¨äºéè¦åç´ çº§ç²¾åº¦çä»»å¡ã
*   <strong>å éæ¾å¾®å¾ååæï¼</strong> éè¿é«æçæé«è´¨éç¹å¾ï¼æææ¾èå éæ¾å¾®å¾åçèªå¨ååæåè§£éã
*   <strong>éä½æ æ³¨ææ¬ï¼æé«äº¤äºå¼åå²æçï¼</strong> å¼ºè°äºå¨äº¤äºå¼åå²ä¸­ï¼è¯¥æ¹æ³è½ä»¥æ´å¿«çéåº¦åæ´å°çæ ç­¾å®ç°é«è´¨éåå²ï¼è¿å¯¹äºæ æ³¨ææ¬é«æçä¸ä¸é¢åï¼å¦å»å­¦ãææï¼å·æå·¨å¤§ä»·å¼ã
*   <strong>æ°çæ¨¡åèå¼ï¼</strong> âä¸æ¬¡è®­ç»ï¼å¤ä»»å¡åºç¨âçä¸éæ ·å¨èå¼ï¼å¯è½ä¸ºæªæ¥åºç¡æ¨¡åå¨ç¹å®é¢åï¼å°¤å¶æ¯ç§å­¦å¾åï¼çé¨ç½²ååºç¨å¼è¾æ°éå¾ï¼å³éè¿ä¸ä¸ªéç¨çç¹å¾ç²¾ç»åæ¨¡åæ¥éåºä¸åä»»å¡ã</p>
<h3 id="4-related-areas-or-applications">4. å¯è½åççç¸å³é¢åæåºç¨ (Related Areas or Applications)</h3>
<ul>
<li><strong>ææç§å­¦ä¸å·¥ç¨ï¼</strong> æ¾å¾®ç»æåæãç¼ºé·æ£æµï¼å¦åä¸è£çº¹ï¼ãç¸åç¦»è¯å«ãæ¶ç²è¾¹çåå²ã</li>
<li><strong>çç©å»å­¦å¾ååæï¼</strong> ç»èåå²ãç»ç»ççå­¦å¾ååæãç¥ç»åè¿½è¸ªãçµå­æ¾å¾®éå¾åå¤çãé«åè¾¨çè§åæ¾å¾®å¾ååæã</li>
<li><strong>å·¥ä¸æ£æµï¼</strong> è¡¨é¢ç¼ºé·æ£æµãè´¨éæ§å¶ï¼å°¤å¶æ¯å¨éè¦è¯å«å¾®å°ççµçåºæ¯ã</li>
<li><strong>é¥æå¾åå¤çï¼</strong> é«åè¾¨çå«æå¾åä¸­çå°ç©åå²ä¸è¯å«ï¼ç¹å«æ¯å¯¹ç²¾ç»å°ç©ï¼å¦éè·¯ãå»ºç­ç©è¾¹ç¼ï¼çæåã</li>
<li><strong>ä»»ä½éè¦ç²¾ç»åç´ çº§çè§£ä¸å¾åå°ºå¯¸è¾å¤§ãç¹å¾ç»èä¸°å¯çé¢åã</strong></li>
</ul>
<h3 id="5-limitations-that-can-be-inferred-from-the-abstract">5. ä»æè¦ä¸­å¯æ¨æ­çå±éæ§ (Limitations that can be inferred from the abstract)</h3>
<ul>
<li><strong>å¯¹åºç¡æ¨¡åç¹å¾è´¨éçä¾èµï¼</strong> è¯¥æ¹æ³çæ§è½å¨ä¸å®ç¨åº¦ä¸åå³äºæä½¿ç¨çåºç¡æ¨¡åçæç¹å¾çè´¨éãå¦æåºç¡æ¨¡åæªè½ææå°æäºå³é®è¯­ä¹ä¿¡æ¯ï¼ä¸éæ ·å¨ä¹é¾ä»¥å­ç©ºåé ã</li>
<li><strong>âæ éè¿ä¸æ­¥è®­ç»âçæè¡¡ï¼</strong> å°½ç®¡ä¸éæ ·å¨æ éé¢å¤è®­ç»å³å¯åºç¨äºæ°ä»»å¡æ¯å¶ä¼å¿ï¼ä½è¿å¯è½æå³çå¨ç¹å®æ°æ®éä¸ï¼å¶æ§è½å¯è½æ æ³è¶è¶ç»è¿ååå¾®è°çç«¯å°ç«¯ç½ç»ï¼å¦U-Netï¼ï¼å°¤å¶æ¯å¨å¨èªå¨åå²åºæ¯ä¸ãæè¦ä¸­å¼ºè°çæ¯äº¤äºå¼åå²çæçæåã</li>
<li><strong>ä¸U-Netçç´æ¥æ§è½å¯¹æ¯æªæï¼</strong> æ é¢æç¤ºäºå¯¹U-Netçæ¿ä»£ï¼ä½æè¦ä¸»è¦å¼ºè°å¨äº¤äºå¼åå²ä¸­âæ´å¿«ãæ´å°æ ç­¾âçä¼å¿ï¼å¹¶æªç´æ¥ç»åºå¨å¨èªå¨ãéäº¤äºå¼åºæ¯ä¸ä¸U-Netç­ä¼ ç»æ¹æ³çéåæ§è½å¯¹æ¯ã</li>
<li><strong>è®¡ç®ææ¬çæ½å¨èéï¼</strong> è½ç¶å¼ºè°äºâè®¡ç®æçâï¼ä½ç»ååºç¡æ¨¡ååç¬ç«çä¸éæ ·å¨ç½ç»ï¼å¶æ»ä½çæ¨çæ¶é´æåå­å ç¨ä¸ä¸ä¸ªä¼åè¯å¥½çU-Netç¸æ¯å¦ä½ï¼ä»éè¿ä¸æ­¥éªè¯ã</li>
<li><strong>å¯¹è¾å¥å¾åè´¨éçæææ§ï¼</strong> ä¸éæ ·å¨âåèè¾å¥å¾åâè¿è¡ç²¾ç»åï¼è¿æå³çå¦æè¾å¥å¾åæ¬èº«å­å¨åªå£°æä¼ªå½±ï¼å¯è½ä¼å½±åä¸éæ ·ååå²çåç¡®æ§ã</li>
</ul>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>We demonstrate that interactive segmentation with these deep
features produces high-quality segmentations far faster and with far fewer
labels than training or finetuning a more traditional convolutional network.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="http://arxiv.org/pdf/2508.21529v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2508.21529v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2508.21712v1'></a></p>
<h2 id="flora-efficient-synthetic-data-generation-for-object-detection-in-low-data-regimes-via-finetuning-flux-lora"><a href="https://arxiv.org/abs/2508.21712v1">FLORA: Efficient Synthetic Data Generation for Object Detection in Low-Data Regimes via finetuning Flux LoRA</a></h2>
<p><strong>Authors:</strong> Alvaro Patricio, Atabak Dehban, Rodrigo Ventura</p>
<p><strong>Published:</strong> 2025-08-29</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Recent advances in diffusion-based generative models have demonstrated
significant potential in augmenting scarce datasets for object detection tasks.
Nevertheless, most recent models rely on resource-intensive full fine-tuning of
large-scale diffusion models, requiring enterprise-grade GPUs (e.g., NVIDIA
V100) and thousands of synthetic images. To address these limitations, we
propose Flux LoRA Augmentation (FLORA), a lightweight synthetic data generation
pipeline. Our approach uses the Flux 1.1 Dev diffusion model, fine-tuned
exclusively through Low-Rank Adaptation (LoRA). This dramatically reduces
computational requirements, enabling synthetic dataset generation with a
consumer-grade GPU (e.g., NVIDIA RTX 4090). We empirically evaluate our
approach on seven diverse object detection datasets. Our results demonstrate
that training object detectors with just 500 synthetic images generated by our
approach yields superior detection performance compared to models trained on
5000 synthetic images from the ODGEN baseline, achieving improvements of up to
21.3% in mAP@.50:.95. This work demonstrates that it is possible to surpass
state-of-the-art performance with far greater efficiency, as FLORA achieves
superior results using only 10% of the data and a fraction of the computational
cost. This work demonstrates that a quality and efficiency-focused approach is
more effective than brute-force generation, making advanced synthetic data
creation more practical and accessible for real-world scenarios.</p>
<p><strong>Analysis:</strong></p>
<p>ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå¯¹è¿ç¯è®ºææè¦çåæå¦ä¸ï¼</p>
<hr />
<h3 id="flora-efficient-synthetic-data-generation-for-object-detection-in-low-data-regimes-via-finetuning-flux-lora_1">FLORA: Efficient Synthetic Data Generation for Object Detection in Low-Data Regimes via finetuning Flux LoRA</h3>
<p><strong>1. è®ºæä¸»è¦è´¡ç® (Concise Summary)</strong></p>
<p>FLORAæåºäºä¸ç§è½»éçº§ãé«æçåææ°æ®çæç®¡çº¿ï¼ä¸ä¸ºä½æ°æ®éç®æ æ£æµä»»å¡è®¾è®¡ãå®éè¿å¯¹Flux 1.1 Devæ©æ£æ¨¡åè¿è¡LoRAå¾®è°ï¼æ¾èéä½äºè®¡ç®èµæºéæ±ï¼å¹¶è¯æä»ç¨å°éï¼500å¼ ï¼é«è´¨éåæå¾åå³å¯è¶è¶ç°æåºçº¿ï¼ODGENï¼ä½¿ç¨å¤§éï¼5000å¼ ï¼å¾åçæ§è½ï¼å®ç°äºæ´é«çæ£æµç²¾åº¦åæçãè¿é¡¹å·¥ä½å¼ºè°äºè´¨éåæçä¼åçåææ°æ®çæç­ç¥ä¼äºè®åçæã</p>
<p><strong>2. å³é®åæ°ææ¹æ³ (Key Innovation or Methodological Approach)</strong></p>
<p>è¯¥è®ºæçå³é®åæ°å¨äºï¼</p>
<ul>
<li><strong>LoRAå¾®è°æ©æ£æ¨¡åä»¥å®ç°é«ææ§ï¼</strong> æ ¸å¿åæ°æ¯å°ä½ç§©éåºï¼LoRAï¼ææ¯åºç¨äºFlux 1.1 Devæ©æ£æ¨¡åçå¾®è°ï¼ä»¥çæç®æ æ£æµæéçåææ°æ®ãç¸è¾äºä¼ ç»éè¦å¨éå¾®è°å¤§åæ©æ£æ¨¡åçæ¹æ³ï¼LoRAä»æ´æ°å°éåæ°ï¼æå¤§å°éä½äºè®¡ç®ææ¬ï¼ä½¿å¾å¨æ¶è´¹çº§GPUï¼å¦NVIDIA RTX 4090ï¼ä¸è¿è¡é«æçåææ°æ®çææä¸ºå¯è½ã</li>
<li><strong>âè´¨éä¼åäºæ°éâçåææ°æ®ç­ç¥ï¼</strong> è®ºææç¡®æåºå¹¶éªè¯äºâè´¨éåæçä¼åâçç­ç¥ä¼äºâè®åçæâãå®éªç»æè¡¨æï¼ä»ç¨500å¼ FLORAçæçåæå¾åï¼å°±è½å¨mAP@.50:.95ä¸åå¾é«è¾¾21.3%çæåï¼å¹¶è¶è¶ä½¿ç¨5000å¼ ODGENåºçº¿å¾åè®­ç»çæ¨¡åãè¿è¡¨æFLORAè½å¤çææ´é«è´¨éãå¯¹æ£æµå¨è®­ç»æ´ææçåææ°æ®ï¼èéç®åå°è¿½æ±æ°éã</li>
</ul>
<p><strong>3. å¯¹é¢åæ½å¨å½±å (Potential Impact on the Field)</strong></p>
<ul>
<li><strong>åææ°æ®çæçæ°ä¸»åï¼</strong> æå¤§å°éä½äºåææ°æ®çæçææ¯é¨æ§åèµæºéæ±ï¼ä½¿æ´å¤ç ç©¶èåå°åå¢éè½å¤å¨æ¶è´¹çº§ç¡¬ä»¶ä¸å©ç¨åè¿çæ©æ£æ¨¡åè¿è¡æ°æ®å¢å¼ºï¼æ¨å¨äºè¯¥é¢åçæ°ä¸»åã</li>
<li><strong>æ¹ååææ°æ®ç ç©¶èå¼ï¼</strong> ææäºä¼ ç»ä¸è®¤ä¸ºåææ°æ®éè¶å¤§è¶å¥½çè§å¿µï¼å¼ºè°äºæ°æ®è´¨éåçææççéè¦æ§ï¼å¯è½å¼å¯¼æªæ¥åææ°æ®ç ç©¶è½¬åæ´ç²¾ç»ãæ´é«æççæç­ç¥ã</li>
<li><strong>å éä½æ°æ®éåºæ¯ä¸çAIåºç¨ï¼</strong> å¨æ°æ®ç¨ç¼ºåºæ¯ä¸ï¼FLORAè½å¤ä»¥æ´ä½çææ¬ãæ´å¿«çéåº¦æä¾é«è´¨éçè®­ç»æ°æ®ï¼å éäºç®æ æ£æµæ¨¡åå¨ç¹å®é¢åï¼å¦å»çãå·¥ä¸æ£æµãå°ä¼ç©ä½è¯å«ï¼çå¼ååé¨ç½²ã</li>
<li><strong>æ¨å¨LoRAå¨çææ¨¡åä¸­çåºç¨ï¼</strong> è¿ä¸æ­¥è¯æäºLoRAä½ä¸ºä¸ç§é«æå¾®è°ç­ç¥å¨å¤§åçææ¨¡åï¼ç¹å«æ¯æ©æ£æ¨¡åï¼ä¸­çå·¨å¤§æ½åï¼å¯è½å¯åæ´å¤å°LoRAåºç¨äºå¶ä»å¤æçæä»»å¡çç ç©¶ã</li>
</ul>
<p><strong>4. ç¸å³åçé¢åæåºç¨ (Related Areas or Applications that Might Benefit)</strong></p>
<ul>
<li><strong>ææé¢ä¸´æ°æ®ç¨ç¼ºææçç®æ æ£æµä»»å¡ï¼</strong> ä¾å¦ï¼å»çå½±ååæï¼ç½è§ç¾çæ£æµï¼ãå·¥ä¸ç¼ºé·æ£æµï¼ç¹å®äº§åç¼ºé·ï¼ãåä¸ï¼ç¹å®ä½ç©çè«å®³ï¼ãèªå¨é©¾é©¶ï¼é¿å°¾äºä»¶æç½è§ç©ä½ï¼ãæºå¨äººè§è§ç­ã</li>
<li><strong>èµæºåéçç ç©¶æºææä¼ä¸ï¼</strong> æ æ³æ¿ææè´µçä¼ä¸çº§GPUåå¤§è§æ¨¡æ°æ®çæææ¬çåºæ¯ã</li>
<li><strong>å¿«éååå¼ååè¿­ä»£ï¼</strong> éè¦å¿«éçæç¹å®åºæ¯æ°æ®ä»¥éªè¯æ¨¡åæç®æ³çåºæ¯ã</li>
<li><strong>å¶ä»è®¡ç®æºè§è§ä»»å¡ï¼</strong> è¯¥æ¹æ³çæ ¸å¿ææ³ï¼LoRAå¾®è°æ©æ£æ¨¡åä»¥é«æçæé«è´¨éæ°æ®ï¼ä¹å¯è½æ¨å¹¿å°å¶ä»è®¡ç®æºè§è§ä»»å¡ï¼å¦è¯­ä¹åå²ãå®ä¾åå²ï¼çè³å¾åçæä¸ç¼è¾ç­ï¼åªè¦è¿äºä»»å¡è½ä»é«æãé«è´¨éçåææ°æ®ä¸­åçã</li>
</ul>
<p><strong>5. æ½å¨å±éæ§ (Limitations that Can Be Inferred from the Abstract)</strong></p>
<ul>
<li><strong>âä½æ°æ®éåºæ¯âçå·ä½å®ä¹ï¼</strong> æè¦ä¸­æå°çâä½æ°æ®éåºæ¯âçå·ä½å®ä¹åèå´å°ä¸æç¡®ãä¾å¦ï¼500å¼ åæå¾åæ¯å¦è¶³ä»¥åºå¯¹æææä½æ°æ®éï¼å¦åªæå åå¼ çå®å¾åï¼çåºæ¯ï¼</li>
<li><strong>åºçº¿å¯¹æ¯çå¨é¢æ§ï¼</strong> è½ç¶ä¸ODGENåºçº¿è¿è¡äºå¯¹æ¯ï¼ä½ODGENæ¯å¦ä»£è¡¨äºå½ååææ°æ®çæé¢åçææ°SOTAæ¹æ³ï¼æ¯å¦æå¶ä»æ´åè¿çãåºäºæ©æ£æ¨¡åçå¨éå¾®è°æ¹æ³æªè¢«æåæå¯¹æ¯ï¼è¿ä¼å½±åå¯¹âè¶è¶SOTAæ§è½âè¿ä¸è¯´æ³çå¤æ­ã</li>
<li><strong>Fluxæ¨¡åä¾èµæ§ï¼</strong> FLORAä¾èµäºFlux 1.1 Devæ¨¡åãè¯¥æ¹æ³å¨å¶ä»æ©æ£æ¨¡åï¼å¦Stable DiffusionãDALL-Eç­ï¼ä¸çè¡¨ç°å¦ä½ï¼LoRAå¾®è°çæææ§æ¯å¦ä¸åºç¡æ©æ£æ¨¡åçæ¶ææé¢è®­ç»æ°æ®å¼ºç¸å³ï¼</li>
<li><strong>åææ°æ®è´¨éçæ·±å±è¯ä¼°ï¼</strong> mAP@.50:.95æ¯ç®æ æ£æµçæ åææ ï¼ä½åæå¾åæ¬èº«çè§è§è´¨éãå¤æ ·æ§ä»¥åå¯¹æ¨¡åæ³åè½åçé¿æå½±åï¼å¨æè¦ä¸­æªè¯¦ç»è¯´æãä¾å¦ï¼åææ°æ®æ¯å¦å¼å¥äºæ°çåå·®æä¼ªå½±ï¼</li>
<li><strong>çå®ä¸çåå·®è·ï¼</strong> åææ°æ®ä¸çå®ä¸çæ°æ®ä¹é´çåå·®è·ï¼domain gapï¼å§ç»æ¯ä¸ä¸ªææãå°½ç®¡FLORAæé«äºæ§è½ï¼ä½è¿ç§æåå¨é¢å¯¹æç«¯çå®ä¸çå¤ææ§æ¶çé²æ£æ§å¦ä½ï¼æ¨¡åå¨çº¯åææ°æ®ä¸è®­ç»åï¼å¨æªè§è¿ççå®æ°æ®ä¸çè¡¨ç°å¦ä½ï¼</li>
</ul>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>Our approach uses the Flux 1.1 Dev diffusion model, fine-tuned
exclusively through Low-Rank Adaptation (LoRA).</li>
<li>This work demonstrates that it is possible to surpass
state-of-the-art performance with far greater efficiency, as FLORA achieves
superior results using only 10% of the data and a fraction of the computational
cost.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="http://arxiv.org/pdf/2508.21712v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2508.21712v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2508.21635v1'></a></p>
<h2 id="the-rosario-dataset-v2-multimodal-dataset-for-agricultural-robotics"><a href="https://arxiv.org/abs/2508.21635v1">The Rosario Dataset v2: Multimodal Dataset for Agricultural Robotics</a></h2>
<p><strong>Authors:</strong> Nicolas Soncini, Javier Cremona, Erica Vidal, Maximiliano GarcÃ­a, GastÃ³n Castro, TaihÃº Pire</p>
<p><strong>Published:</strong> 2025-08-29</p>
<p><strong>Categories:</strong> cs.RO, cs.CV, cs.SY, eess.SY, I.2.9</p>
<p><strong>Abstract:</strong></p>
<p>We present a multi-modal dataset collected in a soybean crop field,
comprising over two hours of recorded data from sensors such as stereo infrared
camera, color camera, accelerometer, gyroscope, magnetometer, GNSS (Single
Point Positioning, Real-Time Kinematic and Post-Processed Kinematic), and wheel
odometry. This dataset captures key challenges inherent to robotics in
agricultural environments, including variations in natural lighting, motion
blur, rough terrain, and long, perceptually aliased sequences. By addressing
these complexities, the dataset aims to support the development and
benchmarking of advanced algorithms for localization, mapping, perception, and
navigation in agricultural robotics. The platform and data collection system is
designed to meet the key requirements for evaluating multi-modal SLAM systems,
including hardware synchronization of sensors, 6-DOF ground truth and loops on
long trajectories.
  We run multimodal state-of-the art SLAM methods on the dataset, showcasing
the existing limitations in their application on agricultural settings. The
dataset and utilities to work with it are released on
https://cifasis.github.io/rosariov2/.</p>
<p><strong>Analysis:</strong></p>
<p>ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå¯¹è¿ç¯è®ºææè¦çåæå¦ä¸ï¼</p>
<hr />
<h3 id="the-rosario-dataset-v2-multimodal-dataset-for-agricultural-robotics_1">è®ºææè¦åæï¼The Rosario Dataset v2: Multimodal Dataset for Agricultural Robotics</h3>
<p><strong>1. è®ºæä¸»è¦è´¡ç®çç®ææ»ç» (2-3å¥è¯)</strong></p>
<p>Rosario Dataset v2 æ¯ä¸ä¸ªéå¯¹åä¸æºå¨äººé¢åçå¤æ¨¡ææ°æ®éï¼å¨çå®çå¤§è±åç°ä¸­ééï¼åå«è¶è¿ä¸¤å°æ¶çä¸°å¯ä¼ æå¨æ°æ®ï¼å¦ç«ä½çº¢å¤ç¸æºãå½©è²ç¸æºãIMUãå¤ç§GNSSåè½®å¼éç¨è®¡ãè¯¥æ°æ®éæ¨å¨éè¿æä¾ç¡¬ä»¶åæ­¥ã6-DOFçå¼åé¿è½¨è¿¹å¾ªç¯ç­å³é®ç¹æ§ï¼è§£å³åä¸ç¯å¢ä¸­ï¼å¦åç§ååãè¿å¨æ¨¡ç³ãå´å²å°å½¢åæç¥æ··å ï¼çææï¼ä»èæ¯æååºåæµè¯åè¿çå®ä½ãå»ºå¾ãæç¥åå¯¼èªç®æ³ãä½èè¿å©ç¨è¯¥æ°æ®éè¿è¡äºæåè¿çå¤æ¨¡æSLAMæ¹æ³ï¼æ­ç¤ºäºç°æç®æ³å¨åä¸åºç¨ä¸­çå±éæ§ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºæ¹æ³</strong></p>
<p>è¯¥è®ºæçå³é®åæ°å¨äº<strong>åå»ºäºä¸ä¸ªé«åº¦å¨é¢ä¸æå·æææ§çå¤æ¨¡ææ°æ®éï¼ä¸é¨ä¸ºåä¸æºå¨äººåºç¨éèº«å®å¶</strong>ãå¶æ¹æ³è®ºä½ç°å¨ï¼</p>
<ul>
<li><strong>ä¸°å¯çå¤æ¨¡æä¼ æå¨èåï¼</strong> ç»åäºç«ä½çº¢å¤ç¸æºï¼å¯¹åç§ååé²æ£ï¼ãå½©è²ç¸æºãæ¯æ§æµéååï¼IMUï¼ãå¤ç§GNSSï¼åæ¬é«ç²¾åº¦çRTKåPPKï¼ä»¥åè½®å¼éç¨è®¡ï¼æä¾äºæå¶å¤æ ·åçæ°æ®æµï¼è¿å¯¹äºå¼åé²æ£çä¼ æå¨èåç®æ³è³å³éè¦ã</li>
<li><strong>éå¯¹åä¸ç¯å¢çç¹å®ææï¼</strong> æ°æ®éçè®¾è®¡åééæç¡®èèäºåä¸ç¯å¢çç¬ç¹é¾é¢ï¼å¦èªç¶åç§å§çååãå´å²å°å½¢å¯¼è´çè¿å¨æ¨¡ç³ãä»¥ååä½ç©è¡é´éå¤æ¨¡å¼é æçâæç¥æ··å âï¼perceptually aliased sequencesï¼ï¼è¿äºé½æ¯ä¼ ç»CVåSLAMç®æ³ççç¹ã</li>
<li><strong>é«æ åçæ°æ®è´¨éååºåæµè¯æ¯æï¼</strong> å¼ºè°äºä¼ æå¨ç¡¬ä»¶åæ­¥ãæä¾é«ç²¾åº¦ç6-DOFï¼å­èªç±åº¦ï¼å°é¢çå¼ï¼ä»¥ååå«é¿è½¨è¿¹ä¸çå¾ªç¯ï¼loopsï¼ï¼è¿äºé½æ¯è¯ä¼°åå¼åå¤æ¨¡æSLAMç³»ç»ä¸å¯æç¼ºçè¦ç´ ãéè¿è¿è¡SOTA SLAMæ¹æ³å¹¶å±ç¤ºå¶å±éæ§ï¼ä½èä¸ä»éªè¯äºæ°æ®éçæææ§ï¼ä¹ä¸ºåç»­ç ç©¶æä¾äºæç¡®çæ¹åã</li>
</ul>
<p><strong>3. å¯¹é¢åæ½å¨å½±å</strong></p>
<ul>
<li><strong>å éåä¸æºå¨äººç ç©¶ï¼</strong> æä¾äºä¸ä¸ªæ ååãé«æææ§ä¸å¬å¼å¯ç¨çåºåæ°æ®éï¼å°æå¤§å°å éåä¸é¢åå®ä½ãå»ºå¾ãæç¥åå¯¼èªç®æ³çå¼ååè¯ä¼°ã</li>
<li><strong>æ¨å¨é²æ£ç®æ³åå±ï¼</strong> è¿«ä½¿ç ç©¶äººåå¼åè½å¤åºå¯¹çå®ä¸çåä¸ç¯å¢å¤ææ§çæ´é²æ£ãæ´æ³åçè®¡ç®æºè§è§åæºå¨å­¦ä¹ ç®æ³ï¼å°¤å¶æ¯å¨åç§ååãçº¹çéå¤åå´å²å°å½¢ä¸çè¡¨ç°ã</li>
<li><strong>å¼¥åçè®ºä¸å®è·µçé¸¿æ²ï¼</strong> éè¿æä¾çå®ä¸ççå¤ææ°æ®ï¼å¸®å©ç ç©¶äººåå°å®éªå®¤æææ´å¥½å°åºç¨äºå®éçåä¸èªå¨ååç²¾ååä¸åºæ¯ã</li>
<li><strong>ä¿è¿å¤ä¼ æå¨èååSLAMææ¯è¿æ­¥ï¼</strong> æ°æ®éçå¤æ¨¡æç¹æ§å°æ¨å¨å¤ä¼ æå¨èåææ¯çåå±ï¼ç¹å«æ¯å¨è§è§-æ¯æ§-GNSS-éç¨è®¡èåSLAMæ¹é¢ï¼ä»¥å®ç°æ´ç²¾ç¡®åå¯é çå®ä½ã</li>
<li><strong>ä¸ºæ°ç ç©¶æ¹åæä¾åºç¡ï¼</strong> æ°æ®éä¸­çææï¼å¦æç¥æ··å ï¼å¯è½ä¼æ¿åæ°çè®¡ç®æºè§è§åæºå¨å­¦ä¹ æ¹æ³ï¼ä¾å¦åºäºè¯­ä¹ä¿¡æ¯çå®ä½ãææå»ºå¾ææ´åè¿çå¾ªç¯é­åææ¯ã</li>
</ul>
<p><strong>4. å¯è½åççç¸å³é¢åæåºç¨</strong></p>
<ul>
<li><strong>åä¸æºå¨äººä¸èªå¨åï¼</strong> è¿æ¯æç´æ¥çåçé¢åï¼åæ¬èªä¸»æææºãå·æ´æºå¨äººãéææºå¨äººãä½ç©çæµæ äººè½¦ç­ã</li>
<li><strong>åæ­¥å®ä½ä¸å»ºå¾ (SLAM)ï¼</strong> ç¹å«æ¯å¤æ¨¡æSLAMãè§è§-æ¯æ§SLAMãä»¥åå¨ä½çº¹çæéå¤çº¹çç¯å¢ä¸çSLAMã</li>
<li><strong>è®¡ç®æºè§è§ï¼</strong> æ·å¤åºæ¯çè§£ãä½ç©/æèæ£æµä¸åå²ã3Déå»ºãè¿å¨ä¼°è®¡ãåæµåæãä»¥åå¨æ¶å£åç§æ¡ä»¶ä¸çå¾åå¤çã</li>
<li><strong>æºå¨å­¦ä¹ ä¸æ·±åº¦å­¦ä¹ ï¼</strong> è®­ç»ç¨äºåä¸ç¯å¢çé²æ£æç¥æ¨¡åï¼ä¾å¦ç¨äºä½ç©å¥åº·çæµãçè«å®³è¯å«ãäº§éä¼°è®¡ç­ã</li>
<li><strong>ä¼ æå¨èåï¼</strong> å¼ååæµè¯èåå¼æä¼ æå¨æ°æ®ä»¥æé«ç³»ç»æ§è½çç®æ³ã</li>
<li><strong>æ·å¤èªä¸»å¯¼èªï¼</strong> ä»»ä½éè¦å¨éç»æåãå¨æåæææ§æ·å¤ç¯å¢ä¸­è¿è¡èªä¸»å¯¼èªçç³»ç»ï¼ä¾å¦ï¼å»ºç­æºå¨äººãéç¿æºå¨äººãæä¸æºå¨äººï¼ã</li>
<li><strong>é«ç²¾åº¦GNSSåºç¨ï¼</strong> ç»åRTK/PPKæ°æ®è¿è¡é«ç²¾åº¦å®ä½åå°å¾æ ¡åçç ç©¶ã</li>
</ul>
<p><strong>5. ä»æè¦ä¸­å¯æ¨æ­çå±éæ§</strong></p>
<ul>
<li><strong>ä½ç©åç¯å¢ç¹å¼æ§ï¼</strong> æ°æ®éä»å¨å¤§è±åç°ä¸­ééãè½ç¶å¤§è±æ¯éè¦ä½ç©ï¼ä½å¶è§è§ç¹å¾ãçé¿æ¨¡å¼ååç°ç»æå¯è½ä¸å¶ä»ä½ç©ï¼å¦çç±³ãå°éº¦ãæå­ãæ¸©å®¤è¬èï¼ææ¾èå·®å¼ãè¿éå¶äºå¶å¨å¶ä»åä¸åºæ¯ä¸­çç´æ¥æ³åè½åã</li>
<li><strong>å°çä½ç½®åæ°åéå¶ï¼</strong> æ°æ®éå¨âRosarioâå°åºééï¼è¿æå³çå¶å¯è½åæ ç¹å®å°çåºåçæ°åãåå£¤ç±»åååç§æ¡ä»¶ãå¨å¶ä»å°åºï¼ç¯å¢å ç´ å¯è½å¤§ç¸å¾åº­ã</li>
<li><strong>å¹³å°ç¹å®æ§ï¼</strong> æ°æ®æ¯ç±ä¸ä¸ªç¹å®çâå¹³å°åæ°æ®ééç³»ç»âæ¶éçãä¼ æå¨çå·ä½éç½®ãå®è£é«åº¦ãè§è§ä»¥åå¹³å°èªèº«çè¿å¨ç¹æ§å¯è½æ æ³å®å¨ä»£è¡¨ææåä¸æºå¨äººç³»ç»ã</li>
<li><strong>ä¸»è¦ä¾§éäºSLAM/å¯¼èªï¼</strong> å°½ç®¡æ°æ®ä¸°å¯ï¼ä½æè¦ä¸­æç¡®å¼ºè°äºå¯¹SLAMç³»ç»è¯ä¼°çå³é®è¦æ±ï¼ç¡¬ä»¶åæ­¥ã6-DOFçå¼ãå¾ªç¯ï¼ãè¿è¡¨ææ°æ®éçè®¾è®¡åæ æ³¨å¯è½æ´ä¾§éäºå®ä½åå»ºå¾ä»»å¡ï¼å¯¹äºå¶ä»ç»ç²åº¦çè®¡ç®æºè§è§ä»»å¡ï¼å¦ç²¾ç¡®çæ¤ç©è¡¨ååæãçå®³æ©ææ£æµï¼å¯è½éè¦é¢å¤çæ æ³¨æå¤çã</li>
<li><strong>æªæåå¶ä»å¨æéç¢ç©ï¼</strong> æè¦ä¸­æ²¡ææç¡®è¯´ææ°æ®éä¸­æ¯å¦åå«é¤äºæºå¨äººæ¬èº«ä¹å¤çå¶ä»å¨æéç¢ç©ï¼ä¾å¦ï¼ååºå·¥äººãéçå¨ç©ãå¶ä»åæºï¼ãå¨çå®çåä¸ç¯å¢ä¸­ï¼è¿äºå¨æå ç´ å¯¹å¯¼èªåå®å¨è³å³éè¦ã</li>
<li><strong>æ°æ®éï¼</strong> âè¶è¿ä¸¤å°æ¶âçæ°æ®éå¯¹äºæ·±åº¦å­¦ä¹ æ¨¡åè®­ç»æ¥è¯´ï¼è½ç¶ä¸éï¼ä½å¯è½ä¸å¦ä¸äºå¤§è§æ¨¡åå¸æ°æ®éé£æ ·åºå¤§ï¼è¿å¨æäºéè¦æµ·éæ°æ®çä»»å¡ä¸å¯è½ææéå¶ã</li>
</ul>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present a multi-modal dataset collected in a soybean crop field,
comprising over two hours of recorded data from sensors such as stereo infrared
camera, color camera, accelerometer, gyroscope, magnetometer, GNSS (Single
Point Positioning, Real-Time Kinematic and Post-Processed Kinematic), and wheel
odometry.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="http://arxiv.org/pdf/2508.21635v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2508.21635v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2508.21542v1'></a></p>
<h2 id="complete-gaussian-splats-from-a-single-image-with-denoising-diffusion-models"><a href="https://arxiv.org/abs/2508.21542v1">Complete Gaussian Splats from a Single Image with Denoising Diffusion Models</a></h2>
<p><strong>Authors:</strong> Ziwei Liao, Mohamed Sayed, Steven L. Waslander, Sara Vicente, Daniyar Turmukhambetov, Michael Firman</p>
<p><strong>Published:</strong> 2025-08-29</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>Gaussian splatting typically requires dense observations of the scene and can
fail to reconstruct occluded and unobserved areas. We propose a latent
diffusion model to reconstruct a complete 3D scene with Gaussian splats,
including the occluded parts, from only a single image during inference.
Completing the unobserved surfaces of a scene is challenging due to the
ambiguity of the plausible surfaces. Conventional methods use a
regression-based formulation to predict a single "mode" for occluded and
out-of-frustum surfaces, leading to blurriness, implausibility, and failure to
capture multiple possible explanations. Thus, they often address this problem
partially, focusing either on objects isolated from the background,
reconstructing only visible surfaces, or failing to extrapolate far from the
input views. In contrast, we propose a generative formulation to learn a
distribution of 3D representations of Gaussian splats conditioned on a single
input image. To address the lack of ground-truth training data, we propose a
Variational AutoReconstructor to learn a latent space only from 2D images in a
self-supervised manner, over which a diffusion model is trained. Our method
generates faithful reconstructions and diverse samples with the ability to
complete the occluded surfaces for high-quality 360-degree renderings.</p>
<p><strong>Analysis:</strong></p>
<p>è¿ç¯è®ºæçæè¦å±ç¤ºäºè®¡ç®æºè§è§é¢åä¸ä¸ªéå¸¸æè¶£ä¸éè¦çè¿å±ãä»¥ä¸æ¯æçè¯¦ç»åæï¼</p>
<hr />
<h3 id="1-concise-summary_1">1. è®ºæä¸»è¦è´¡ç®çç®ææè¦ (Concise Summary)</h3>
<p>æ¬ææåºäºä¸ç§åæ°çæ½å¨æ©æ£æ¨¡åï¼è½å¤ä»ä»åå¼ å¾åæ¨æ­åºå®æ´ç3Dåºæ¯ï¼åæ¬è¢«é®æ¡åæªè§å¯å°çåºåï¼å¹¶ä»¥é«æ¯æ³¼æºï¼Gaussian Splatsï¼çå½¢å¼è¡¨ç¤ºãä¸ä¼ ç»åå½æ¹æ³çæåä¸æ¨¡ç³æä¸çå®çéå»ºä¸åï¼è¯¥æ¹æ³éç¨çæå¼å¬å¼æ¥å­¦ä¹ 3Dè¡¨ç¤ºçåå¸ï¼ä»èå®ç°å¿ å®ä¸å¤æ ·åçåºæ¯è¡¥å¨ï¼æ¯æé«è´¨éç360åº¦æ¸²æã</p>
<h3 id="2-key-innovation-or-methodological-approach_1">2. å³é®åæ°ææ¹æ³è®º (Key Innovation or Methodological Approach)</h3>
<p>æ ¸å¿åæ°å¨äºå¶<strong>çæå¼å¬å¼</strong>ï¼å®ä¸æ¯é¢æµåä¸ç3Déå»ºï¼èæ¯å­¦ä¹ ç»å®åå¼ è¾å¥å¾åä¸3Dé«æ¯æ³¼æºè¡¨ç¤ºç<strong>åå¸</strong>ãè¿ä½¿å¾æ¨¡åè½å¤ææè¢«é®æ¡åæªè§å¯åºåçå¤ç§åçè§£éï¼ä»èé¿åäºä¼ ç»åå½æ¹æ³å¯¼è´çæ¨¡ç³åä¸çå®ãä¸ºå®ç°è¿ä¸ç®æ ï¼è¯¥æ¹æ³å©ç¨äº<strong>æ½å¨æ©æ£æ¨¡å</strong>ï¼å¹¶å¼å¥äº<strong>ååèªéå»ºå¨ï¼Variational AutoReconstructor, VARï¼</strong>ï¼ä»¥èªçç£æ¹å¼ä»ä»2Då¾åå­¦ä¹ ä¸ä¸ªåéçæ½å¨ç©ºé´ï¼ä»èè§£å³äº3Dçå¼æ°æ®ç¨ç¼ºçé®é¢ã</p>
<h3 id="3-potential-impact-on-the-field_1">3. å¯¹é¢åæ½å¨å½±å (Potential Impact on the Field)</h3>
<p>è¿é¡¹ç ç©¶æææ¾èæ¨å¨<strong>åå¾å3Dåºæ¯éå»º</strong>é¢åçåå±ï¼ä½¿é«æ¯æ³¼æºææ¯å¨ç¼ºä¹å¯éå¤è§è§æ°æ®çå®éåºç¨ä¸­æ´å·å®ç¨æ§ãéè¿æä¾<strong>å®æ´ä¸å¤æ ·å</strong>ç3Dåºæ¯è¡¨ç¤ºï¼å®è½æå¤§å°éä½3Dåå®¹åå»ºçæ°æ®ééé¨æ§ï¼å¹¶ä¸ºéè¦çè§£è¢«é®æ¡åºæ¯é¨åç<strong>æºå¨äººãAR/VRåèªå¨é©¾é©¶</strong>ç­é¢åæä¾æ´é²æ£ç3Dæç¥è½åãå®å°åå¾å3Déå»ºä»åä¸ç¡®å®æ§è¾åºæ¨åäºçæå¼ãå¤æ¨¡æè¾åºçæ°èå¼ã</p>
<h3 id="4-related-areas-or-applications_1">4. ç¸å³é¢åæåºç¨ (Related Areas or Applications)</h3>
<ul>
<li><strong>æºå¨äººå­¦ä¸èªå¨é©¾é©¶ï¼</strong> æåå¯¹å¤æç¯å¢ç3Dæç¥è½åï¼å°¤å¶æ¯å¨é¨åé®æ¡æè§è§åéçæåµä¸ï¼æå©äºè·¯å¾è§åãé¿éåäººæºäº¤äºã</li>
<li><strong>å¢å¼ºç°å®ï¼ARï¼ä¸èæç°å®ï¼VRï¼ï¼</strong> å®ç°æ´é¼çãæ´å®æ´çèæåºæ¯æå»ºååå®¹çæï¼æåæ²æµ¸å¼ä½éªã</li>
<li><strong>3Dåå®¹åä½ï¼</strong> æå¤§å°ç®åä»2Då¾åçæå®æ´3Dèµäº§çæµç¨ï¼éä½ææ¬åæ¶é´ã</li>
<li><strong>æ°å­å­ªçï¼</strong> ä»æéçå¾åæ°æ®ä¸­æå»ºæ´å¨é¢çç©çä¸çæ°å­æ¨¡åã</li>
<li><strong>é¥æä¸æµç»ï¼</strong> ä»åå¼ èªç©ºæå«æå¾åä¸­æ¨æ­åºæ´å®æ´çå°å½¢æå»ºç­3Dæ¨¡åã</li>
</ul>
<h3 id="5-limitations-inferred-from-the-abstract">5. å¯ä»æè¦ä¸­æ¨æ­åºçå±éæ§ (Limitations Inferred from the Abstract)</h3>
<ul>
<li><strong>èªçç£å­¦ä¹ çå±éæ§ï¼</strong> å°½ç®¡éè¿VARè§£å³äº3Dçå¼æ°æ®ç¨ç¼ºçé®é¢ï¼ä½èªçç£å­¦ä¹ è·å¾çæ½å¨ç©ºé´çè´¨éåæ³åè½åï¼å¯è½ä»ä¸å¦ç´æ¥ä½¿ç¨å¤§éé«è´¨é3Dçå¼æ°æ®è¿è¡çç£å­¦ä¹ ã</li>
<li><strong>è®¡ç®èµæºæ¶èï¼</strong> æ©æ£æ¨¡åï¼å°¤å¶æ¯å¨3Dçæä»»å¡ä¸­ï¼éå¸¸å¨è®­ç»åæ¨çé¶æ®µé½éè¦å¤§éçè®¡ç®èµæºåæ¶é´ã</li>
<li><strong>åå¾åè¾å¥çæææ§ï¼</strong> æ¨¡åçæ§è½å¯è½é«åº¦ä¾èµäºåå¼ è¾å¥å¾åçè´¨éãåè¾¨çãè§è§ååç§æ¡ä»¶ãæç«¯æä½è´¨éçè¾å¥å¯è½å¯¼è´ä¸ä½³çéå»ºææã</li>
<li><strong>çæç»æçè¯­ä¹åçæ§ï¼</strong> å°½ç®¡è½å¤çæå¤æ ·åçè¡¥å¨ç»æï¼ä½å¦ä½ç¡®ä¿ææçæçè¢«é®æ¡é¨åå¨è¯­ä¹ä¸å®å¨åçä¸ç©çä¸ä¸è´ï¼å¯¹äºå¤æåºæ¯ä»æ¯ä¸ä¸ªææã</li>
<li><strong>æ³åè½åï¼</strong> æè¦æªè¯¦ç»è¯´æè®­ç»æ°æ®çèå´ï¼å æ­¤æ¨¡åå¯¹è®­ç»éä¸­æªåºç°çæ°é¢åºæ¯ãç©ä½ç±»å«æå¤æäº¤äºçæ³åè½åå¯è½å­å¨å±éã</li>
</ul>
<hr />
<p><strong>æ»ç»ï¼</strong> è¿ç¯è®ºæéè¿å°æ©æ£æ¨¡åä¸é«æ¯æ³¼æºç»åï¼å¹¶å·§å¦å°è§£å³äº3Dçå¼æ°æ®ç¨ç¼ºçé®é¢ï¼å¨åå¾å3Déå»ºé¢åè¿åºäºéè¦ä¸æ­¥ãå¶çæå¼æ¹æ³è½å¤å¤çåºæçæ­§ä¹æ§ï¼ä¸ºæªæ¥ç3Dæç¥ååå®¹çæå¼è¾äºæ°çå¯è½æ§ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose a latent
diffusion model to reconstruct a complete 3D scene with Gaussian splats,
including the occluded parts, from only a single image during inference.</li>
<li>In contrast, we propose a generative formulation to learn a
distribution of 3D representations of Gaussian splats conditioned on a single
input image.</li>
<li>To address the lack of ground-truth training data, we propose a
Variational AutoReconstructor to learn a latent space only from 2D images in a
self-supervised manner, over which a diffusion model is trained.</li>
<li>Our method
generates faithful reconstructions and diverse samples with the ability to
complete the occluded surfaces for high-quality 360-degree renderings.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="http://arxiv.org/pdf/2508.21542v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2508.21542v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2508.21363v1'></a></p>
<h2 id="efficient-diffusion-based-3d-human-pose-estimation-with-hierarchical-temporal-pruning"><a href="https://arxiv.org/abs/2508.21363v1">Efficient Diffusion-Based 3D Human Pose Estimation with Hierarchical Temporal Pruning</a></h2>
<p><strong>Authors:</strong> Yuquan Bi, Hongsong Wang, Xinli Shi, Zhipeng Gui, Jie Gui, Yuan Yan Tang</p>
<p><strong>Published:</strong> 2025-08-29</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Diffusion models have demonstrated strong capabilities in generating
high-fidelity 3D human poses, yet their iterative nature and multi-hypothesis
requirements incur substantial computational cost. In this paper, we propose an
Efficient Diffusion-Based 3D Human Pose Estimation framework with a
Hierarchical Temporal Pruning (HTP) strategy, which dynamically prunes
redundant pose tokens across both frame and semantic levels while preserving
critical motion dynamics. HTP operates in a staged, top-down manner: (1)
Temporal Correlation-Enhanced Pruning (TCEP) identifies essential frames by
analyzing inter-frame motion correlations through adaptive temporal graph
construction; (2) Sparse-Focused Temporal MHSA (SFT MHSA) leverages the
resulting frame-level sparsity to reduce attention computation, focusing on
motion-relevant tokens; and (3) Mask-Guided Pose Token Pruner (MGPTP) performs
fine-grained semantic pruning via clustering, retaining only the most
informative pose tokens. Experiments on Human3.6M and MPI-INF-3DHP show that
HTP reduces training MACs by 38.5\%, inference MACs by 56.8\%, and improves
inference speed by an average of 81.1\% compared to prior diffusion-based
methods, while achieving state-of-the-art performance.</p>
<p><strong>Analysis:</strong></p>
<p>è¿ç¯è®ºæçæè¦æä¾äºä¸ä¸ªå³äºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åä¸­3Däººä½å§¿æä¼°è®¡çæè¶£è¿å±ãä»¥ä¸æ¯æ ¹æ®æè¦è¿è¡çåæï¼</p>
<hr />
<h3 id="1-concise-summary_2">1. è®ºæä¸»è¦è´¡ç®çç®ææè¦ (Concise Summary)</h3>
<p>æ©æ£æ¨¡åå¨çæé«ä¿ç3Däººä½å§¿ææ¹é¢è¡¨ç°åºè²ï¼ä½å¶è¿­ä»£æ§è´¨åå¤åè®¾éæ±å¯¼è´äºå·¨å¤§çè®¡ç®ææ¬ãæ¬ææåºäºä¸ç§é«æçåºäºæ©æ£ç3Däººä½å§¿æä¼°è®¡æ¡æ¶ï¼è¯¥æ¡æ¶éç¨åå±æ¶é´åªæï¼HTPï¼ç­ç¥ï¼å¨æå°å¨å¸§åè¯­ä¹å±é¢åªæåä½å§¿ætokensï¼åæ¶ä¿çå³é®è¿å¨å¨æãHTPæ¾èéä½äºè®¡ç®èµæºæ¶èï¼å¹¶æåäºæ¨çéåº¦ï¼åæ¶å®ç°äºæåè¿çæ§è½ã</p>
<h3 id="2-key-innovation-or-methodological-approach_2">2. å³é®åæ°ææ¹æ³è®º (Key Innovation or Methodological Approach)</h3>
<p>è¯¥è®ºæçæ ¸å¿åæ°å¨äºå¶<strong>åå±æ¶é´åªæï¼Hierarchical Temporal Pruning, HTPï¼ç­ç¥</strong>ï¼æ¨å¨è§£å³æ©æ£æ¨¡åå¨3Däººä½å§¿æä¼°è®¡ä¸­çè®¡ç®æçé®é¢ãHTPæ¹æ³è®ºçç¬ç¹æ§ä½ç°å¨å¶å¤é¶æ®µãèªä¸èä¸çåªæè¿ç¨ï¼</p>
<ol>
<li><strong>æ¶é´ç¸å³æ§å¢å¼ºåªæ (Temporal Correlation-Enhanced Pruning, TCEP)</strong>ï¼éè¿æå»ºèªéåºæ¶é´å¾æ¥åæå¸§é´è¿å¨ç¸å³æ§ï¼ä»èè¯å«å¹¶åªæåä½å¸§ï¼ä¿çå³é®å¸§ãè¿æ¯ä¸ç§æºè½çå¸§çº§ç¨çåæ¹æ³ï¼èéç®åéæ ·ã</li>
<li><strong>ç¨çèç¦æ¶é´å¤å¤´èªæ³¨æå (Sparse-Focused Temporal MHSA, SFT MHSA)</strong>ï¼å©ç¨TCEPäº§ççå¸§çº§ç¨çæ§ï¼ä¼åå¤å¤´èªæ³¨æåæºå¶çè®¡ç®ï¼ä½¿å¶åªå³æ³¨ä¸è¿å¨ç¸å³çtokensï¼ä»èåå°è®¡ç®éã</li>
<li><strong>æ©ç å¼å¯¼å§¿æTokenåªæå¨ (Mask-Guided Pose Token Pruner, MGPTP)</strong>ï¼å¨æ´ç»ç²åº¦çè¯­ä¹å±é¢ï¼éè¿èç±»å¯¹å§¿ætokensè¿è¡åªæï¼åªä¿çæå·ä¿¡æ¯éçtokensãè¿è¿ä¸æ­¥ç²¾ç¼äºå§¿æè¡¨ç¤ºï¼å»é¤äºè¯­ä¹åä½ã</li>
</ol>
<p>è¿ç§ç»åäºå¸§çº§åè¯­ä¹çº§åªæçå±æ¬¡åãå¨ææ¹æ³ï¼æ¯å¶å¨æ©æ£æ¨¡åæçæåä¸çå³é®çªç ´ã</p>
<h3 id="3-potential-impact-on-the-field_2">3. å¯¹é¢åæ½å¨å½±å (Potential Impact on the Field)</h3>
<p>è¿é¡¹ç ç©¶å¯¹è®¡ç®æºè§è§é¢åå·ææ¾èçæ½å¨å½±åï¼</p>
<ul>
<li><strong>æ¨å¨æ©æ£æ¨¡åçå®éåºç¨</strong>ï¼éè¿å¤§å¹æé«æ©æ£æ¨¡åçè®¡ç®æçåæ¨çéåº¦ï¼HTPä½¿å¶å¨å®æ¶æèµæºåéç3Däººä½å§¿æä¼°è®¡åºç¨ä¸­åå¾æ´å å¯è¡åå®ç¨ã</li>
<li><strong>å¯åéç¨æçæåç­ç¥</strong>ï¼HTPçåå±åªæææ³ï¼ç¹å«æ¯ç»åè¿å¨å¨æåè¯­ä¹ä¿¡æ¯è¿è¡ç¨çåçæ¹æ³ï¼å¯è½ä¸ºå¶ä»åºäºTransformerææ©æ£çåºåçææ¨¡åï¼å¦è§é¢çæãå¨ä½åæï¼æä¾éç¨çæçä¼åæè·¯ã</li>
<li><strong>å éç ç©¶ä¸å¼å</strong>ï¼æ´å¿«çæ¨¡åè®­ç»åæ¨çéåº¦å°åè®¸ç ç©¶äººåè¿è¡æ´å¤å®éªï¼å éæ°ç®æ³ååºç¨åºæ¯çæ¢ç´¢ã</li>
<li><strong>æåç¨æ·ä½éª</strong>ï¼å¨AR/VRãäººæºäº¤äºãè¿å¨åæç­é¢åï¼æ´é«æç3Då§¿æä¼°è®¡æå³çæ´æµçãååºæ´å¿«çç¨æ·ä½éªã</li>
</ul>
<h3 id="4-related-areas-or-applications_2">4. ç¸å³é¢åæåºç¨ (Related Areas or Applications)</h3>
<p>è¿é¡¹ç ç©¶çææå¯ä»¥æ åä»¥ä¸é¢åååºç¨ï¼</p>
<ul>
<li><strong>å®æ¶3Däººä½å§¿æä¼°è®¡</strong>ï¼å¦å¨AR/VRæ¸¸æãèæè¯ç©¿ãè¿ç¨åä½ç­åºæ¯ä¸­ã</li>
<li><strong>äººæºäº¤äº (HCI)</strong>ï¼éè¿æ´åç¡®ãä½å»¶è¿çå§¿æè¯å«ï¼å®ç°æ´èªç¶çç¨æ·çé¢åæå¿æ§å¶ã</li>
<li><strong>æºå¨äººå­¦</strong>ï¼ç¨äºæºå¨äººæ¨¡ä»¿å­¦ä¹ ãäººæºåä½ä¸­çäººä½å§¿æçè§£åé¢æµã</li>
<li><strong>è¿å¨åæä¸çç©åå­¦</strong>ï¼é«æåæè¿å¨åå¨ä½ãåº·å¤è®­ç»ä¸­çå§¿æè¯ä¼°ã</li>
<li><strong>çµå½±ãå¨ç»ä¸æ¸¸æ</strong>ï¼å¿«éçæåç¼è¾3Dè§è²å¨ç»ï¼éä½å¶ä½ææ¬ã</li>
<li><strong>è§é¢çè§£ä¸å¨ä½è¯å«</strong>ï¼ä½ä¸ºé¢å¤çæ­¥éª¤ï¼æä¾é«æç3Då§¿æç¹å¾ã</li>
<li><strong>éç¨åºåæ¨¡åæçä¼å</strong>ï¼å¶åªæç­ç¥å¯è½éç¨äºå¶ä»éè¦å¤çé¿åºåæé«ç»´tokençTransformerææ©æ£æ¨¡åã</li>
</ul>
<h3 id="5-limitations-inferred-from-the-abstract_1">5. ä»æè¦ä¸­å¯æ¨æ­çå±éæ§ (Limitations Inferred from the Abstract)</h3>
<p>å°½ç®¡æè¦å¼ºè°äºæ¾èçæçæååSOTAæ§è½ï¼ä½ä»å¯æ¨æ­åºä¸äºæ½å¨å±éæ§ï¼</p>
<ul>
<li><strong>åªæç­ç¥çå¤ææ§</strong>ï¼HTPåå«ä¸ä¸ªé¶æ®µï¼æ¶åèªéåºå¾æå»ºåèç±»ç­æä½ï¼è¿å¯è½å¢å äºæ¨¡åçå®ç°åè°ä¼å¤ææ§ãè¿äºé¢å¤æ­¥éª¤æ¬èº«ä¹å¯è½å¼å¥ä¸å®çè®¡ç®å¼éï¼å°½ç®¡æ»ä½ä¸æ¯åæ¶çã</li>
<li><strong>âå³é®è¿å¨å¨æâçå®ä¹ä¸é²æ£æ§</strong>ï¼æè¦æå°âä¿çå³é®è¿å¨å¨æâï¼ä½å¦ä½ç²¾ç¡®å®ä¹åä¿è¯å¨ææå¤æè¿å¨åºæ¯ä¸é½è½ææä¿çâå³é®âä¿¡æ¯ï¼èä¸ä¼è¯¯åªææç»å¾®ä½éè¦çå¨ä½ç»èï¼æ¯ä¸ä¸ªæ½å¨çææã</li>
<li><strong>è¶åæ°æææ§</strong>ï¼èªéåºå¾çæå»ºãèç±»ç®æ³çéæ©ååæ°è®¾ç½®ï¼ä»¥ååé¶æ®µåªææ¯ä¾ç­ï¼é½å¯è½å¼å¥éè¦ä»ç»è°ä¼çè¶åæ°ï¼å½±åæ¨¡åçæ³åè½ååé²æ£æ§ã</li>
<li><strong>å¯¹ç¹å®æ°æ®éçä¾èµ</strong>ï¼å®éªç»æå¨Human3.6MåMPI-INF-3DHPä¸è¡¨ç°åºè²ï¼è¿äºæ¯æ åæ°æ®éï¼ä½å¨æ´âéå¤âï¼in-the-wildï¼çå¤æç¯å¢ãé®æ¡ãå¤æ ·åæè£ååç§æ¡ä»¶ä¸ï¼åªæç­ç¥çæææ§å¯è½éè¦è¿ä¸æ­¥éªè¯ã</li>
<li><strong>æªæåå¶ä»ææ</strong>ï¼æè¦ä¸»è¦å³æ³¨æçé®é¢ï¼å¹¶æªæåå¯¹3Då§¿æä¼°è®¡ä¸­å¶ä»å¸¸è§ææï¼å¦ä¸¥éé®æ¡ãå¤è§è§é²æ£æ§ãä¸åä½ååæè£çæ³åè½åï¼çç´æ¥æ¹è¿ã</li>
</ul>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this paper, we propose an
Efficient Diffusion-Based 3D Human Pose Estimation framework with a
Hierarchical Temporal Pruning (HTP) strategy, which dynamically prunes
redundant pose tokens across both frame and semantic levels while preserving
critical motion dynamics.</li>
<li>Experiments on Human3.6M and MPI-INF-3DHP show that
HTP reduces training MACs by 38.5\%, inference MACs by 56.8\%, and improves
inference speed by an average of 81.1\% compared to prior diffusion-based
methods, while achieving state-of-the-art performance.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="http://arxiv.org/pdf/2508.21363v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2508.21363v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2508.21824v1'></a></p>
<h2 id="driveqa-passing-the-driving-knowledge-test"><a href="https://arxiv.org/abs/2508.21824v1">DriveQA: Passing the Driving Knowledge Test</a></h2>
<p><strong>Authors:</strong> Maolin Wei, Wanzhou Liu, Eshed Ohn-Bar</p>
<p><strong>Published:</strong> 2025-08-29</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>If a Large Language Model (LLM) were to take a driving knowledge test today,
would it pass? Beyond standard spatial and visual question-answering (QA) tasks
on current autonomous driving benchmarks, driving knowledge tests require a
complete understanding of all traffic rules, signage, and right-of-way
principles. To pass this test, human drivers must discern various edge cases
that rarely appear in real-world datasets. In this work, we present DriveQA, an
extensive open-source text and vision-based benchmark that exhaustively covers
traffic regulations and scenarios. Through our experiments using DriveQA, we
show that (1) state-of-the-art LLMs and Multimodal LLMs (MLLMs) perform well on
basic traffic rules but exhibit significant weaknesses in numerical reasoning
and complex right-of-way scenarios, traffic sign variations, and spatial
layouts, (2) fine-tuning on DriveQA improves accuracy across multiple
categories, particularly in regulatory sign recognition and intersection
decision-making, (3) controlled variations in DriveQA-V provide insights into
model sensitivity to environmental factors such as lighting, perspective,
distance, and weather conditions, and (4) pretraining on DriveQA enhances
downstream driving task performance, leading to improved results on real-world
datasets such as nuScenes and BDD, while also demonstrating that models can
internalize text and synthetic traffic knowledge to generalize effectively
across downstream QA tasks.</p>
<p><strong>Analysis:</strong></p>
<p>ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå¯¹è¿ç¯åä¸ºâDriveQA: Passing the Driving Knowledge Testâçè®ºææè¦è¿è¡å¦ä¸åæï¼</p>
<hr />
<h3 id="1-2-3">1. è®ºæä¸»è¦è´¡ç®çç®ææ»ç» (2-3å¥è¯)</h3>
<p>æ¬ææåºäºDriveQAï¼ä¸ä¸ªå¨é¢çå¼æºææ¬åè§è§åºåæµè¯ï¼æ¨å¨è¯ä¼°å¤§åè¯­è¨æ¨¡åï¼LLMsï¼åå¤æ¨¡æLLMsï¼MLLMsï¼å¯¹äº¤éè§åãæ å¿åè·¯æååçå®æ´çè§£ãç ç©¶åç°ï¼ç°ææ¨¡åå¨å¤æåºæ¯åæ°å¼æ¨çæ¹é¢å­å¨æ¾èä¸è¶³ï¼ä½éè¿å¨DriveQAä¸è¿è¡å¾®è°åé¢è®­ç»ï¼å¯ä»¥æææåæ¨¡åå¨é©¾é©¶ç¥è¯æµè¯åä¸æ¸¸çå®ä¸çé©¾é©¶ä»»å¡ä¸­çæ§è½åæ³åè½åã</p>
<h3 id="2">2. å³é®åæ°ææ¹æ³è®ºæ¹æ³</h3>
<p>æ ¸å¿åæ°å¨äºæå»ºäºDriveQAè¿ä¸å¨é¢çãå¼æºçææ¬ä¸è§è§åºåæµè¯ãå®è¶è¶äºç°æèªå¨é©¾é©¶åºåçå±éï¼éè¿âç©·å°½å¼âå°è¦çäº¤éæ³è§ãåºæ¯åç°å®ä¸çæ°æ®ä¸­ç½è§çâè¾¹ç¼æ¡ä¾âï¼æ¨å¨å¨é¢è¯ä¼°æ¨¡åå¯¹é©¾é©¶ç¥è¯çæ·±å±çè§£ãæ­¤å¤ï¼DriveQA-Véè¿å¼å¥åæ§çç¯å¢å ç´ ï¼å¦åç§ãè§è§ãè·ç¦»ãå¤©æ°ï¼ååï¼ä¸ºæ·±å¥åææ¨¡åå¯¹è¿äºå ç´ çæææ§æä¾äºç¬ç¹çæ¹æ³ï¼è¿å¯¹äºçè§£æ¨¡åå¨ä¸åæ¡ä»¶ä¸çé²æ£æ§è³å³éè¦ã</p>
<h3 id="3">3. å¯¹è¯¥é¢åçæ½å¨å½±å</h3>
<p>è¯¥ç ç©¶å¯¹è®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åå·æå¤æ¹é¢æ½å¨å½±åãé¦åï¼å®ä¸ºèªå¨é©¾é©¶ç³»ç»ä»çº¯ç²¹çæç¥åé¢æµè¿ååºäºè§åçâçè§£âåâæ¨çâæä¾äºå³é®å·¥å·åæ¹åï¼æå©äºæåèªå¨é©¾é©¶ç³»ç»çå®å¨æ§åå¯é æ§ãå¶æ¬¡ï¼DriveQAä½ä¸ºä¸ä¸ªæå·æææ§çåºåï¼å°æ¨å¨LLMsåMLLMså¨å¤ææ¨çãæ°å¼çè§£åå¤æ¨¡æä¿¡æ¯èåæ¹é¢çè½åè¾¹çãæåï¼éè¿å±ç¤ºåææ°æ®åææ¬ç¥è¯å¯¹çå®ä¸çä»»å¡çæ³åè½åï¼ä¸ºæªæ¥é«æè®­ç»åé¨ç½²æºè½é©¾é©¶ç³»ç»æä¾äºæ°çèå¼ï¼å°¤å¶æ¯å¨é¾ä»¥è·åå¤§éçå®ä¸çè¾¹ç¼æ¡ä¾æ°æ®çæåµä¸ã</p>
<h3 id="4">4. å¯è½åçäºè¿é¡¹ç ç©¶çç¸å³é¢åæåºç¨</h3>
<ul>
<li><strong>èªå¨é©¾é©¶ç³»ç»å¼åä¸æµè¯ï¼</strong> ç´æ¥åºç¨ï¼ç¨äºè®­ç»åè¯ä¼°èªå¨é©¾é©¶è½¦è¾çå³ç­æ¨¡åï¼ç¹å«æ¯å¶å¯¹äº¤éè§åççè§£åå¤æåºæ¯ä¸çæ¨çè½åã</li>
<li><strong>æºè½äº¤éç³»ç» (ITS)ï¼</strong> è¾å©äº¤éæµç®¡çãäºæé¢é²åæºè½ä¿¡å·æ§å¶ï¼éè¿æ´æºè½å°çè§£äº¤éè§åæ¥ä¼åç³»ç»ã</li>
<li><strong>æºå¨äººå­¦ï¼</strong> ä»»ä½éè¦å¨å¤æãè§åé©±å¨ç¯å¢ä¸­æä½çæºå¨äººç³»ç»ï¼ä¾å¦ç©æµæºå¨äººææå¡æºå¨äººï¼å¯ä»¥åé´å¶è§åçè§£åå³ç­æ¡æ¶ã</li>
<li><strong>AIå®å¨ä¸å¯è§£éæ§ (XAI)ï¼</strong> æ·±å¥çè§£æ¨¡åå¦ä½åºäºäº¤éè§åååºå³ç­ï¼æé«å³ç­çéæåº¦åå¯ä¿¡åº¦ï¼è¿å¨å®å¨å³é®é¢åå°¤ä¸ºéè¦ã</li>
<li><strong>é©¾é©¶åå¹è®­ä¸æè²ï¼</strong> ä½ä¸ºè¾å©å·¥å·ï¼å¸®å©æ°é©¾é©¶åçè§£å¤æçäº¤éè§ååè¾¹ç¼æåµï¼çè³å¯ä»¥ç¨äºå¼åæºè½é©¾é©¶æ¨¡æå¨ã</li>
<li><strong>å¤æ¨¡æLLMsç ç©¶ï¼</strong> ä¸ºæåLLMsåMLLMså¨è§è§çè§£ãå¸¸è¯æ¨çåè§åéµå¾ªæ¹é¢çè½åæä¾æ°çç ç©¶æ¹ååè¯ä¼°æ åã</li>
</ul>
<h3 id="5">5. ä»æè¦ä¸­å¯æ¨æ­åºçå±éæ§</h3>
<ul>
<li><strong>åææ°æ®ä¸çå®ä¸ççå·®è·ï¼</strong> å°½ç®¡æè¦æåºæ¨¡åå¯ä»¥ååææ¬ååæäº¤éç¥è¯å¹¶æ³åå°çå®ä¸çæ°æ®éï¼ä½DriveQAæ¬èº«æ¯åºäºâåæäº¤éç¥è¯âæå»ºçãåææ°æ®å¨è¦çè¾¹ç¼æ¡ä¾æ¹é¢æä¼å¿ï¼ä½å¶ä¸çå®ä¸çæ°æ®çåå¸å·®å¼ãåªå£°åä¸å¯é¢æµæ§å¯è½ä»æ¯æ¨¡åæ³åè½åçæ½å¨éå¶ã</li>
<li><strong>ä¾§éç¥è¯çè§£èéå®æ¶å³ç­ä¸æ§å¶ï¼</strong> DriveQAä¸»è¦æ¯ä¸ä¸ªâé©¾é©¶ç¥è¯æµè¯âï¼è¯ä¼°æ¨¡åå¯¹è§åççè§£åæ¨çè½åãå®ä¸ç´æ¥è¯ä¼°æ¨¡åå¨å®æ¶ãå¨æãé«åççå®é©¾é©¶ç¯å¢ä¸­çæç¥ãé¢æµåæ§å¶è½åï¼è¿éè¦æ´å¤æçç«¯å°ç«¯ç³»ç»ã</li>
<li><strong>ç¹å®ææçæç»­å­å¨ï¼</strong> æè¦æç¡®æåºï¼å³ä½¿æ¯SOTAæ¨¡åï¼å¨âæ°å¼æ¨çãå¤æè·¯æåºæ¯ãäº¤éæ å¿åä½åç©ºé´å¸å±âæ¹é¢ä»å­å¨æ¾èå¼±ç¹ãè¿è¡¨æè¿äºæ¯æå¶å°é¾çé®é¢ï¼DriveQAè½ç¶æä¾äºè¯ä¼°å·¥å·ï¼ä½è§£å³è¿äºæ·±å±ææä»éè¿ä¸æ­¥çç ç©¶ã</li>
<li><strong>âç©·å°½æ§âçèå´ï¼</strong> å°½ç®¡å£°ç§°âç©·å°½å¼å°è¦çäº¤éæ³è§ååºæ¯âï¼ä½ç°å®ä¸ççäº¤éè§ååè¾¹ç¼æ¡ä¾æ¯æå¶åºå¤§åå¤æçï¼ä¾å¦ï¼ä¸åå½å®¶/å°åºçå·ä½æ³è§å·®å¼ãåç§éæ åæåµï¼ãæè¦å¹¶æªè¯¦ç»è¯´æå¶è¦ççå¹¿åº¦åæ·±åº¦ï¼è¿å¯è½æ¯ä¸ä¸ªæ½å¨çå±éã</li>
</ul>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this work, we present DriveQA, an
extensive open-source text and vision-based benchmark that exhaustively covers
traffic regulations and scenarios.</li>
<li>Through our experiments using DriveQA, we
show that (1) state-of-the-art LLMs and Multimodal LLMs (MLLMs) perform well on
basic traffic rules but exhibit significant weaknesses in numerical reasoning
and complex right-of-way scenarios, traffic sign variations, and spatial
layouts, (2) fine-tuning on DriveQA improves accuracy across multiple
categories, particularly in regulatory sign recognition and intersection
decision-making, (3) controlled variations in DriveQA-V provide insights into
model sensitivity to environmental factors such as lighting, perspective,
distance, and weather conditions, and (4) pretraining on DriveQA enhances
downstream driving task performance, leading to improved results on real-world
datasets such as nuScenes and BDD, while also demonstrating that models can
internalize text and synthetic traffic knowledge to generalize effectively
across downstream QA tasks.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="http://arxiv.org/pdf/2508.21824v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2508.21824v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2508.21769v1'></a></p>
<h2 id="domain-generalization-in-the-wild-disentangling-classification-from-domain-aware-representations"><a href="https://arxiv.org/abs/2508.21769v1">Domain Generalization in-the-Wild: Disentangling Classification from Domain-Aware Representations</a></h2>
<p><strong>Authors:</strong> Ha Min Son, Zhe Zhao, Shahbaz Rezaei, Xin Liu</p>
<p><strong>Published:</strong> 2025-08-29</p>
<p><strong>Categories:</strong> cs.CV, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Evaluating domain generalization (DG) for foundational models like CLIP is
challenging, as web-scale pretraining data potentially covers many existing
benchmarks. Consequently, current DG evaluation may neither be sufficiently
challenging nor adequately test genuinely unseen data scenarios. To better
assess the performance of CLIP on DG in-the-wild, a scenario where CLIP
encounters challenging unseen data, we consider two approaches: (1) evaluating
on 33 diverse datasets with quantified out-of-distribution (OOD) scores after
fine-tuning CLIP on ImageNet, and (2) using unlearning to make CLIP `forget'
some domains as an approximation. We observe that CLIP's performance
deteriorates significantly on more OOD datasets. To address this, we present
CLIP-DCA (Disentangling Classification from enhanced domain Aware
representations). Our approach is motivated by the observation that while
standard domain invariance losses aim to make representations domain-invariant,
this can be harmful to foundation models by forcing the discarding of
domain-aware representations beneficial for generalization. We instead
hypothesize that enhancing domain awareness is a prerequisite for effective
domain-invariant classification in foundation models. CLIP-DCA identifies and
enhances domain awareness within CLIP's encoders using a separate domain head
and synthetically generated diverse domain data. Simultaneously, it encourages
domain-invariant classification through disentanglement from the domain
features. CLIP-DCA shows significant improvements within this challenging
evaluation compared to existing methods, particularly on datasets that are more
OOD.</p>
<p><strong>Analysis:</strong></p>
<p>ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå¯¹è¿ç¯è®ºææè¦çåæå¦ä¸ï¼</p>
<hr />
<h3 id="domain-generalization-in-the-wild-disentangling-classification-from-domain-aware-representations_1">è®ºææè¦åæï¼Domain Generalization in-the-Wild: Disentangling Classification from Domain-Aware Representations</h3>
<p><strong>1. è®ºæä¸»è¦è´¡ç® (Main Contribution):</strong>
æ¬æéå¯¹åºç¡æ¨¡åï¼å¦CLIPï¼å¨çå®ä¸çåæ³åï¼DG in-the-wildï¼åºæ¯ä¸çè¯ä¼°ææï¼æåºäºä¸å¥æ°çè¯ä¼°èå¼ï¼å¹¶åç°CLIPå¨æ­¤ç±»é«åº¦åå¤ï¼OODï¼æ°æ®ä¸æ§è½æ¾èä¸éãä¸ºè§£å³æ­¤é®é¢ï¼è®ºæå¼å¥äºCLIP-DCAæ¹æ³ï¼éè¿å¢å¼ºåæç¥è¡¨ç¤ºå¹¶å°å¶ä¸åç±»ä»»å¡è§£è¦ï¼æ¾èæåäºåºç¡æ¨¡åå¨å¤æåæ³åä»»å¡ä¸­çæ§è½ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³ (Key Innovation or Methodological Approach):</strong>
è¯¥è®ºæçæ ¸å¿åæ°å¨äºå¶å¯¹åºç¡æ¨¡ååæ³åå­¦ä¹ èå¼çæ·±å»è½¬åãä¸ä¼ ç»åä¸åæ§å­¦ä¹ æ¹æ³ç´æ¥å¼ºå¶è¡¨ç¤ºåä¸åæ§ï¼å¯è½å¯¼è´æç¨åä¿¡æ¯çä¸¢å¤±ï¼ä¸åï¼CLIP-DCAæåºå¹¶éªè¯äºâå¢å¼ºåæç¥è½åæ¯å®ç°ææåä¸ååç±»çåå³æ¡ä»¶âè¿ä¸åè®¾ãå¶æ¹æ³è®ºéè¿å¼å¥ä¸ä¸ªç¬ç«çåå¤´åå©ç¨åæåæ°æ®æ¥ä¸»å¨è¯å«å¹¶å¢å¼ºCLIPç¼ç å¨ä¸­çåæç¥è¡¨ç¤ºï¼åæ¶å·§å¦å°éè¿è§£è¦æºå¶ç¡®ä¿æç»çåç±»ä»»å¡è½å¤ä»è¿äºå¢å¼ºçåç¹å¾ä¸­è§£è¦åºæ¥ï¼ä»èå®ç°æ´é²æ£çåä¸ååç±»ã</p>
<p><strong>3. å¯¹é¢åæ½å¨å½±å (Potential Impact on the Field):</strong>
è¯¥ç ç©¶ææå¯¹è®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åäº§çå¤æ¹é¢å½±åãé¦åï¼å®æåºäºæ´ä¸¥è°¨åå·æææ§çåºç¡æ¨¡ååæ³åè¯ä¼°èå¼ï¼ä¿ä½¿é¢åéæ°æèç°æåºåçæææ§ãå¶æ¬¡ï¼CLIP-DCAçæ ¸å¿ææ³ââå³åå¢å¼ºåæç¥åè§£è¦åç±»ââä¸ºæªæ¥è®¾è®¡éå¯¹å¤§åé¢è®­ç»æ¨¡åï¼å¦CLIPï¼çåæ³åç®æ³å¼è¾äºæ°è·¯å¾ï¼ææäºä¼ ç»åä¸åæ§å­¦ä¹ çåè®¾ãè¿æææ¾èæååºç¡æ¨¡åå¨çå®ä¸çãé«åº¦åå¤ï¼OODï¼åºæ¯ä¸çé²æ£æ§åæ³åè½åï¼ä½¿å¶å¨å®éé¨ç½²ä¸­æ´å¯é ã</p>
<p><strong>4. ç¸å³é¢åæåºç¨ (Related Areas or Applications that Might Benefit from this Research):</strong>
æ¬ç ç©¶çææå°å¯¹å¹¿æ³çå®éåºç¨é¢åäº§çç§¯æå½±åï¼ç¹å«æ¯é£äºå¯¹æ¨¡åå¨æªè§æ°æ®ä¸æ³åè½åæä¸¥æ ¼è¦æ±çåºæ¯ï¼
*   <strong>å»çå½±åè¯æ­ï¼</strong> ä¸åå»é¢ãè®¾å¤ææ£èç¾¤ä½çæ°æ®åå¸å·®å¼å·¨å¤§ï¼DGè½åè³å³éè¦ã
*   <strong>èªå¨é©¾é©¶ï¼</strong> é¢å¯¹å¤åçå¤©æ°ãåç§ãå°çç¯å¢ç­ï¼æ¨¡åéå·å¤å¼ºå¤§çåæ³åè½åã
*   <strong>æºå¨äººè§è§ï¼</strong> æºå¨äººéè¦å¨åç§æªç¥çå®¤åå¤ç¯å¢ä¸­æ§è¡ä»»å¡ã
*   <strong>é¥æå¾ååæï¼</strong> ä¸åå°çåºåãä¼ æå¨ææ¶é´ç¹çæ°æ®åå¸å·®å¼ã
*   <strong>å·¥ä¸ç¼ºé·æ£æµï¼</strong> çäº§çº¿ç¯å¢ååå¯è½å¯¼è´æ°æ®åå¸åç§»ã
æ­¤å¤ï¼å¯¹äºä»»ä½éè¦å°å¤§åé¢è®­ç»æ¨¡åé¨ç½²å°ä¸è®­ç»æ°æ®åå¸å­å¨æ¾èå·®å¼ççå®ä¸çç¯å¢ä¸­çåºç¨ï¼æ¬ç ç©¶é½æä¾äºå®è´µçæå¯¼åè§£å³æ¹æ¡ã</p>
<p><strong>5. æ½å¨å±éæ§ (Limitations that Can Be Inferred from the Abstract):</strong>
å°½ç®¡æè¦å±ç¤ºäºä»¤äººé¼èçç»æï¼ä½ä»å¯æ¨æ­åºä¸äºæ½å¨å±éæ§ï¼
*   <strong>åæåæ°æ®çæææ§ä¸æ³åæ§ï¼</strong> CLIP-DCAä¾èµäºâåæçæå¤æ ·åæ°æ®âæ¥å¢å¼ºåæç¥ãåææ°æ®è½å¦ååææçå®ä¸çä¸­å¤æä¸æªç¥çååç§»æ¨¡å¼ï¼ä»¥åå¶è´¨éåå¤æ ·æ§å¯¹æç»æ³åæ§è½çå½±åï¼æ¯ä¸ä¸ªå³é®é®é¢ãè¿äºç®åçåæå¯è½æ æ³åºå¯¹æç«¯OODåºæ¯ã
*   <strong>âéå¿âæºå¶ä½ä¸ºè¿ä¼¼çå±éæ§ï¼</strong> è®ºæä½¿ç¨âéå¿âæ¥è¿ä¼¼æ¨¡ææ¨¡åä»æªè§è¿æäºåçåºæ¯ãè¿ç§è¿ä¼¼æ¹æ³ä¸çæ­£æä¹ä¸çâä»æªè§è¿âä¹é´å¯è½å­å¨å·®å¼ï¼å¶æææ§ååç¡®æ§éè¦æ´æ·±å¥çéªè¯ã
*   <strong>è¯ä¼°åºæ¯çä»£è¡¨æ§ï¼</strong> å°½ç®¡ä½¿ç¨äº33ä¸ªæ°æ®éåOODåæ°ï¼ä½âin-the-wildâæ¯ä¸ä¸ªæå¶å®½æ³çæ¦å¿µãè¿äºè¯ä¼°æ¯å¦è½å®å¨ä»£è¡¨ææçå®ä¸çä¸­å¯è½éå°çãå·ææææ§çæªè§ååºæ¯ï¼ä»æå¾åæ¦·ã
*   <strong>æ¹æ³å¤ææ§ä¸è®¡ç®ææ¬ï¼</strong> å¼å¥ç¬ç«çåå¤´åè§£è¦æºå¶å¯è½ä¼å¢å æ¨¡åçåæ°éãè®¡ç®å¤æåº¦åè®­ç»æ¶é´ï¼è¿å¨èµæºåéçåºæ¯ä¸å¯è½æ¯ä¸ä¸ªèéå ç´ ã
*   <strong>å¯¹åºç¡æ¨¡åçéç¨æ§ï¼</strong> è®ºæä¸»è¦å³æ³¨CLIPãCLIP-DCAçæææ§æ¯å¦è½ç´æ¥æ³åå°å¶ä»ç±»åçåºç¡æ¨¡åï¼ä¾å¦ï¼çº¯è§è§Transformeræä¸åæ¨¡æçé¢è®­ç»æ¨¡åï¼ä»éè¿ä¸æ­¥æ¢ç´¢ã</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this, we present
CLIP-DCA (Disentangling Classification from enhanced domain Aware
representations).</li>
<li>Our approach is motivated by the observation that while
standard domain invariance losses aim to make representations domain-invariant,
this can be harmful to foundation models by forcing the discarding of
domain-aware representations beneficial for generalization.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="http://arxiv.org/pdf/2508.21769v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2508.21769v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2508.21767v1'></a></p>
<h2 id="uitron-foundational-gui-agent-with-advanced-perception-and-planning"><a href="https://arxiv.org/abs/2508.21767v1">UItron: Foundational GUI Agent with Advanced Perception and Planning</a></h2>
<p><strong>Authors:</strong> Zhixiong Zeng, Jing Huang, Liming Zheng, Wenkang Han, Yufeng Zhong, Lei Chen, Longrong Yang, Yingjie Chu, Yuzhi He, Lin Ma</p>
<p><strong>Published:</strong> 2025-08-29</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>GUI agent aims to enable automated operations on Mobile/PC devices, which is
an important task toward achieving artificial general intelligence. The rapid
advancement of VLMs accelerates the development of GUI agents, owing to their
powerful capabilities in visual understanding and task planning. However,
building a GUI agent remains a challenging task due to the scarcity of
operation trajectories, the availability of interactive infrastructure, and the
limitation of initial capabilities in foundation models. In this work, we
introduce UItron, an open-source foundational model for automatic GUI agents,
featuring advanced GUI perception, grounding, and planning capabilities. UItron
highlights the necessity of systemic data engineering and interactive
infrastructure as foundational components for advancing GUI agent development.
It not only systematically studies a series of data engineering strategies to
enhance training effects, but also establishes an interactive environment
connecting both Mobile and PC devices. In training, UItron adopts supervised
finetuning over perception and planning tasks in various GUI scenarios, and
then develop a curriculum reinforcement learning framework to enable complex
reasoning and exploration for online environments. As a result, UItron achieves
superior performance in benchmarks of GUI perception, grounding, and planning.
In particular, UItron highlights the interaction proficiency with top-tier
Chinese mobile APPs, as we identified a general lack of Chinese capabilities
even in state-of-the-art solutions. To this end, we manually collect over one
million steps of operation trajectories across the top 100 most popular apps,
and build the offline and online agent evaluation environments. Experimental
results demonstrate that UItron achieves significant progress in Chinese app
scenarios, propelling GUI agents one step closer to real-world application.</p>
<p><strong>Analysis:</strong></p>
<p>UItronè¿ç¯è®ºææè¦å±ç¤ºäºå¨è®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åï¼ç¹å«æ¯GUIï¼å¾å½¢ç¨æ·çé¢ï¼èªå¨åä»£çæ¹é¢çéè¦è¿å±ãä»¥ä¸æ¯è¯¦ç»åæï¼</p>
<hr />
<h3 id="1-2-3_1">1. è®ºæä¸»è¦è´¡ç®çç®æ´æ»ç» (2-3å¥è¯)</h3>
<p>UItronæ¯ä¸ä¸ªå¼æºçãåºç¡æ§çGUIèªå¨åä»£çæ¨¡åï¼å®éè¿åè¿çGUIæç¥ãå®ä½åè§åè½åï¼æ¨å¨å®ç°ç§»å¨åPCè®¾å¤çèªå¨åæä½ãè¯¥å·¥ä½å¼ºè°äºç³»ç»æ§æ°æ®å·¥ç¨åäº¤äºå¼åºç¡è®¾æ½çéè¦æ§ï¼å¹¶ç»åçç£å¾®è°ä¸è¯¾ç¨å¼ºåå­¦ä¹ ï¼æ¾èæåäºGUIä»£çå¨å¤æå¨çº¿ç¯å¢ä¸­çè¡¨ç°ãç¹å«å¼å¾ä¸æçæ¯ï¼UItronéè¿å¤§è§æ¨¡æå¨æ¶éçä¸­æåºç¨æä½è½¨è¿¹æ°æ®ï¼å¡«è¡¥äºç°æè§£å³æ¹æ¡å¨ä¸­æåºç¨åºæ¯ä¸çè½åç©ºç½ï¼æ¨å¨äºGUIä»£çåå®éåºç¨è¿è¿ã</p>
<h3 id="2_1">2. å³é®åæ°ææ¹æ³è®º</h3>
<p>UItronçå³é®åæ°åæ¹æ³è®ºä½ç°å¨ä»¥ä¸å ä¸ªæ¹é¢ï¼</p>
<ul>
<li><strong>ç³»ç»æ§æ°æ®å·¥ç¨ä¸äº¤äºå¼åºç¡è®¾æ½ï¼</strong> è®ºæå¼ºè°å¹¶å®è·µäºç³»ç»æ§çæ°æ®å·¥ç¨ç­ç¥æ¥å¢å¼ºè®­ç»ææï¼å¹¶å»ºç«äºä¸ä¸ªè¿æ¥ç§»å¨åPCè®¾å¤çç»ä¸äº¤äºç¯å¢ãè¿ä¸ºGUIä»£ççå¼åæä¾äºåå®çåºç¡ï¼è§£å³äºæ°æ®ç¨ç¼ºååºç¡è®¾æ½ä¸è¶³çææã</li>
<li><strong>æ··åè®­ç»èå¼ï¼</strong> UItronéç¨äºåé¶æ®µçè®­ç»ç­ç¥ãé¦åï¼éè¿å¨åç§GUIåºæ¯ä¸çæç¥åè§åä»»å¡è¿è¡çç£å¾®è°ï¼SFTï¼ï¼ä¸ºæ¨¡åå¥ å®åå§è½åãéåï¼å¼å¥è¯¾ç¨å¼ºåå­¦ä¹ ï¼Curriculum Reinforcement Learningï¼æ¡æ¶ï¼ä½¿æ¨¡åè½å¤å¨å¨çº¿ç¯å¢ä¸­è¿è¡å¤æçæ¨çåæ¢ç´¢ï¼ä»èå¤çæ´å¨æåå¼æ¾çä»»å¡ã</li>
<li><strong>å¤§è§æ¨¡ä¸­æåºç¨æ°æ®éä¸è½åæåï¼</strong> éå¯¹ç°æSOTAè§£å³æ¹æ¡æ®éç¼ºä¹ä¸­æåºç¨è½åçé®é¢ï¼UItronæå¨æ¶éäºè¶è¿ä¸ç¾ä¸æ­¥çãæ¶µçå100ä¸ªæåæ¬¢è¿ä¸­æåºç¨ççå®æä½è½¨è¿¹æ°æ®ãè¿ä¸ä»æå»ºäºç¦»çº¿åå¨çº¿çè¯ä¼°ç¯å¢ï¼ä¹ä½¿å¾UItronå¨ä¸­æåºç¨åºæ¯ä¸­åå¾äºæ¾èçæ§è½æåï¼å¡«è¡¥äºä¸ä¸ªéè¦çå¸åºåç ç©¶ç©ºç½ã</li>
<li><strong>å¼æºåºç¡æ¨¡åï¼</strong> ä½ä¸ºâå¼æºåºç¡æ¨¡åâï¼UItronæ¨å¨éä½ç ç©¶é¨æ§ï¼ä¿è¿æ´ä¸ªGUIä»£çé¢åçåå±ã</li>
</ul>
<h3 id="3_1">3. å¯¹é¢åæ½å¨å½±å</h3>
<ul>
<li><strong>å éAGIåå±ï¼</strong> GUIä»£çè¢«è®¤ä¸ºæ¯å®ç°éç¨äººå·¥æºè½ï¼AGIï¼çéè¦ä¸æ­¥ãUItronçè¿å±ï¼ç¹å«æ¯å¶å¨å¤æç¯å¢ä¸­çè§ååæ¢ç´¢è½åï¼å°ç´æ¥æ¨å¨AGIç ç©¶ã</li>
<li><strong>æåGUIèªå¨åæ°´å¹³ï¼</strong> UItronçåè¿æç¥ãå®ä½åè§åè½åï¼ä»¥åå¶å¨ä¸­æåºç¨ä¸ççªç ´ï¼å°æ¾èæåç°æGUIèªå¨åå·¥å·çæ§è½åéç¨èå´ï¼ä½¿å¶è½å¤çæ´å¤æãæ´çå®çä»»å¡ã</li>
<li><strong>æ¨å¨å¤è¯­è¨/å¤æåGUIç ç©¶ï¼</strong> UItronå¯¹ä¸­æåºç¨è½åçå¼ºè°åå®ç°ï¼å°æ¿å±ç ç©¶èå³æ³¨å¶ä»éè±è¯­è¯­è¨åæåèæ¯ä¸çGUIä»£çå¼åï¼ä¿è¿æ´å·æ®éæ§çè§£å³æ¹æ¡ã</li>
<li><strong>æä¾ç ç©¶åºç³ï¼</strong> ä½ä¸ºå¼æºçåºç¡æ¨¡åï¼UItronå°ä¸ºåç»­ç ç©¶æä¾ä¸ä¸ªå¼ºå¤§çåºçº¿åç ç©¶å¹³å°ï¼å éæ°ç®æ³ãæ°æ¶æçéªè¯åè¿­ä»£ã</li>
<li><strong>ä¿è¿æ°æ®å·¥ç¨åäº¤äºç¯å¢çéè§ï¼</strong> è®ºææç¡®æåºæ°æ®å·¥ç¨åäº¤äºåºç¡è®¾æ½æ¯åºç¡ç»ä»¶ï¼è¿å°ä¿ä½¿æ´å¤ç ç©¶èåå¼åèæå¥èµæºæ¥æå»ºé«è´¨éçæ°æ®éåä»¿çç¯å¢ã</li>
</ul>
<h3 id="4_1">4. å¯è½åççç¸å³é¢åæåºç¨</h3>
<ul>
<li><strong>è½¯ä»¶æµè¯ä¸è´¨éä¿è¯ (QA)ï¼</strong> èªå¨åUIæµè¯ï¼å°¤å¶æ¯å¨ç§»å¨åºç¨åå¤å¹³å°åºæ¯ä¸ï¼å¯ä»¥å¤§å¹æé«æçåè¦ççã</li>
<li><strong>æºå¨äººæµç¨èªå¨å (RPA)ï¼</strong> å¨ä¼ä¸çº§åºç¨ä¸­ï¼UItronå¯ä»¥èªå¨åæ§è¡å¤æçè·¨åºç¨ãè·¨è®¾å¤ä¸å¡æµç¨ï¼æé«å·¥ä½æçã</li>
<li><strong>è¾å©ææ¯ä¸æ éç¢è®¿é®ï¼</strong> ä¸ºæ®éäººå£«æä¾æ´æºè½ãæ´èªä¸»ççé¢æä½è¾å©ï¼æåæ°å­çæ´»çä¾¿å©æ§ã</li>
<li><strong>ä¸ªäººAIå©æï¼</strong> èµè½AIå©ææ§è¡æ´å¤æçãæ¶åå¤æ­¥æä½åè·¨åºç¨çæä»¤ï¼ä¾å¦âå¸®æé¢è®¢ä¸å¼ ä»ä¸æµ·å°åäº¬çæºç¥¨âã</li>
<li><strong>æ°æ®ééä¸ç½ç»ç¬è«ï¼</strong> æ´æºè½å°ä»å¨æåå¤æçGUIçé¢ä¸­æåä¿¡æ¯ã</li>
<li><strong>è·¨è®¾å¤è®¡ç®ï¼</strong> å®ç°ä»»å¡å¨ææºåPCä¹é´æ ç¼åæ¢åèªå¨åæ§è¡ã</li>
<li><strong>æè²ä¸å¹è®­ï¼</strong> èªå¨åæ¼ç¤ºè½¯ä»¶æä½æµç¨ï¼ç¨äºæå­¦æç¨æ·å¹è®­ã</li>
</ul>
<h3 id="5_1">5. ä»æè¦ä¸­å¯æ¨æ­çå±éæ§</h3>
<ul>
<li><strong>æ°æ®æ¶éææ¬ä¸å¯æ©å±æ§ï¼</strong> æè¦ä¸­æå°âæå¨æ¶éè¶è¿ä¸ç¾ä¸æ­¥çæä½è½¨è¿¹âï¼è¿è¡¨ææ°æ®æ¶éæ¯ä¸ä¸ªå³å¨å¯éåä¸ææ¬é«æçè¿ç¨ãè½ç¶å¯¹äºç¹å®é¢åï¼å¦ä¸­æåºç¨ï¼åå¾äºæåï¼ä½è¦å°å¶æ©å±å°å¨çææè¯­è¨ãææåºç¨ææ´é¿å°¾çåºç¨åºæ¯ï¼å¶å¯æ©å±æ§å¯è½é¢ä¸´ææã</li>
<li><strong>æ³åè½åï¼</strong> å°½ç®¡å¨âå100ä¸ªæåæ¬¢è¿çä¸­æåºç¨âä¸è¡¨ç°åºè²ï¼ä½å¯¹äºä¸å¸¸è§ãè®¾è®¡é£æ ¼è¿¥å¼ææ´æ°è¿­ä»£é¢ç¹çåºç¨ï¼å¶æ³åè½åä»éè¿ä¸æ­¥éªè¯ã</li>
<li><strong>å¼ºåå­¦ä¹ çææï¼</strong> è¯¾ç¨å¼ºåå­¦ä¹ è½ç¶å¼ºå¤§ï¼ä½éå¸¸å¯¹ç¯å¢çå»ºæ¨¡ãå¥å±å½æ°çè®¾è®¡ä»¥åè®­ç»çç¨³å®æ§æè¾é«è¦æ±ãå¨çå®ãå¼æ¾çGUIç¯å¢ä¸­ï¼å¦ä½ææå¤çæ¢ç´¢-å©ç¨å°å¢ãç¨çå¥å±ä»¥åé¿åºåå³ç­ï¼ä»æ¯RLé¢ä¸´çåºæææã</li>
<li><strong>âåºç¡æ¨¡åâçå®ä¹ä¸èå´ï¼</strong> ä½ä¸ºä¸ä¸ªâåºç¡æ¨¡åâï¼å¶éç¨æ§ï¼å³å¨ä¸è¿è¡é¢å¤å¾®è°çæåµä¸å¤çå¨æ°ä»»å¡çè½åï¼ä»¥åå¯¹é«åº¦æ½è±¡æéè¦æ·±å±è¯­ä¹çè§£çä»»å¡çå¤çè½åï¼å¨æè¦ä¸­æªè¯¦ç»è¯´æã</li>
<li><strong>å¯¹ç¹å®è¯­è¨/æåUIçä¾èµï¼</strong> è½ç¶è§£å³äºä¸­æåºç¨ççç¹ï¼ä½è¿å¯è½æå³çæ¨¡åå¨ä¸å®ç¨åº¦ä¸å­¦ä¹ äºä¸­æUIçç¹å®æ¨¡å¼åä¹ æ¯ãå°å¶ç´æ¥åºç¨äºå¶ä»è¯­è¨ææåèæ¯ä¸UIï¼ä¾å¦æ¥æãé¿æä¼¯æUIï¼æ¶ï¼å¯è½éè¦ç±»ä¼¼çå®å¶åæ°æ®åè®­ç»ã</li>
</ul>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In particular, UItron highlights the interaction proficiency with top-tier
Chinese mobile APPs, as we identified a general lack of Chinese capabilities
even in state-of-the-art solutions.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="http://arxiv.org/pdf/2508.21767v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2508.21767v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2508.21732v1'></a></p>
<h2 id="cad2dmd-set-synthetic-generation-tool-of-digital-measurement-device-cad-model-datasets-for-fine-tuning-large-vision-language-models"><a href="https://arxiv.org/abs/2508.21732v1">CAD2DMD-SET: Synthetic Generation Tool of Digital Measurement Device CAD Model Datasets for fine-tuning Large Vision-Language Models</a></h2>
<p><strong>Authors:</strong> JoÃ£o Valente, Atabak Dehban, Rodrigo Ventura</p>
<p><strong>Published:</strong> 2025-08-29</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>Recent advancements in Large Vision-Language Models (LVLMs) have demonstrated
impressive capabilities across various multimodal tasks. They continue,
however, to struggle with trivial scenarios such as reading values from Digital
Measurement Devices (DMDs), particularly in real-world conditions involving
clutter, occlusions, extreme viewpoints, and motion blur; common in
head-mounted cameras and Augmented Reality (AR) applications. Motivated by
these limitations, this work introduces CAD2DMD-SET, a synthetic data
generation tool designed to support visual question answering (VQA) tasks
involving DMDs. By leveraging 3D CAD models, advanced rendering, and
high-fidelity image composition, our tool produces diverse, VQA-labelled
synthetic DMD datasets suitable for fine-tuning LVLMs. Additionally, we present
DMDBench, a curated validation set of 1,000 annotated real-world images
designed to evaluate model performance under practical constraints.
Benchmarking three state-of-the-art LVLMs using Average Normalised Levenshtein
Similarity (ANLS) and further fine-tuning LoRA's of these models with
CAD2DMD-SET's generated dataset yielded substantial improvements, with InternVL
showcasing a score increase of 200% without degrading on other tasks. This
demonstrates that the CAD2DMD-SET training dataset substantially improves the
robustness and performance of LVLMs when operating under the previously stated
challenging conditions. The CAD2DMD-SET tool is expected to be released as
open-source once the final version of this manuscript is prepared, allowing the
community to add different measurement devices and generate their own datasets.</p>
<p><strong>Analysis:</strong></p>
<p>è¿ç¯è®ºææè¦å±ç¤ºäºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸ä¸ªéè¦è¿å±ï¼ç¹å«æ¯å¨å¤§åè§è§-è¯­è¨æ¨¡åï¼LVLMsï¼çå®éåºç¨åæ°æ®çææ¹é¢ã</p>
<hr />
<h3 id="1-concise-summary_3">1. è®ºæä¸»è¦è´¡ç®çç®æ´æ»ç» (Concise Summary)</h3>
<p>æ¬ææåºäºCAD2DMD-SETï¼ä¸ä¸ªåæ°çåææ°æ®çæå·¥å·ï¼æ¨å¨è§£å³å¤§åè§è§-è¯­è¨æ¨¡åï¼LVLMsï¼å¨å¤æçå®ä¸çæ¡ä»¶ä¸è¯»åæ°å­æµéè®¾å¤ï¼DMDsï¼æ°å¼çé¾é¢ãéè¿å©ç¨3D CADæ¨¡åãé«çº§æ¸²æåé«ä¿çå¾ååæææ¯ï¼è¯¥å·¥å·è½çæå¤æ ·åä¸å¸¦æVQAæ æ³¨çåæDMDæ°æ®éï¼å¹¶ç»åDMDBenchè¿ä¸çå®çéªè¯éãå®éªè¯æï¼ä½¿ç¨CAD2DMD-SETçæçæ°æ®éå¯¹LVLMsè¿è¡å¾®è°ï¼è½æ¾èæåå¶å¨DMDè¯»åä»»å¡ä¸çé²æ£æ§åæ§è½ï¼ä¸ä¸å½±åå¶éç¨è½åã</p>
<h3 id="2-key-innovation-or-methodological-approach_3">2. å³é®åæ°ææ¹æ³å­¦æ¹æ³ (Key Innovation or Methodological Approach)</h3>
<p>æ ¸å¿åæ°å¨äºå¶<strong>åææ°æ®çæèå¼</strong>ï¼ç¹å«æ¯éå¯¹æ°å­æµéè®¾å¤ï¼DMDsï¼çè§è§é®ç­ï¼VQAï¼ä»»å¡ãè¯¥æ¹æ³å©ç¨<strong>3D CADæ¨¡å</strong>ä½ä¸ºåºç¡ï¼ç»å<strong>é«çº§æ¸²æææ¯åé«ä¿çå¾ååæ</strong>ï¼è½å¤ç³»ç»æ§å°çæåå«å¤æçå®ä¸çæ¡ä»¶ï¼å¦æä¹±ãé®æ¡ãæç«¯è§è§åè¿å¨æ¨¡ç³ï¼ç<strong>å¤æ ·åãVQAæ æ³¨çåææ°æ®é</strong>ãæ­¤å¤ï¼å¼å¥<strong>DMDBench</strong>ä½ä¸ºç¬ç«ççå®ä¸çéªè¯éï¼ä¸ºè¯ä¼°æ¨¡åå¨å®éçº¦æä¸çæ§è½æä¾äºå¯é åºåï¼å½¢æäºä¸ä¸ªå®æ´çè®­ç»ä¸è¯ä¼°é­ç¯ãè¿ç§å°é«ä¿çåææ°æ®ä¸çå®ä¸çéªè¯ç¸ç»åçç­ç¥ï¼ææå°å¼¥è¡¥äºâæ¨¡æå°ç°å®âï¼sim-to-realï¼çé¸¿æ²ã</p>
<h3 id="3-potential-impact-on-the-field_3">3. å¯¹é¢åæ½å¨å½±å (Potential Impact on the Field)</h3>
<ul>
<li><strong>æåLVLMå¨ç¹å®é¢ååºç¨çé²æ£æ§ä¸åç¡®æ§ï¼</strong> æ¬æè¯æäºéè¿é«è´¨éåææ°æ®å¯¹LVLMsè¿è¡å¾®è°ï¼å¯ä»¥æ¾èæåå¶å¨ç¹å®ãå¤æçå®ä¸çåºæ¯ä¸çæ§è½ï¼ä¸ä¸å½±åå¶éç¨è½åï¼ä¸ºLVLMå¨å·¥ä¸ãARç­é¢åçè½å°æä¾äºææéå¾ã</li>
<li><strong>è§£å³æ°æ®ç¨ç¼ºé®é¢ï¼</strong> å¯¹äºé¾ä»¥è·åå¤§éçå®ä¸çæ æ³¨æ°æ®çç¹å®è§è§ä»»å¡ï¼CAD2DMD-SETæä¾äºä¸ç§é«æãå¯æ©å±çåææ°æ®çæè§£å³æ¹æ¡ï¼éä½äºæ¨¡åå¼ååé¨ç½²çææ¬ã</li>
<li><strong>æ¨å¨åææ°æ®ç ç©¶ï¼</strong> å¼ºè°äºé«ä¿çæ¸²æåæºè½åæå¨å¼¥åâæ¨¡æå°ç°å®âé¸¿æ²ä¸­çå³é®ä½ç¨ï¼ä¸ºæªæ¥æ´å¤å©ç¨åææ°æ®è®­ç»è§è§æ¨¡åçç ç©¶æä¾äºèä¾ã</li>
<li><strong>ç¤¾åºè´¡ç®ï¼</strong> è®¡åçå¼æºåå¸å°ä¿è¿ç¤¾åºå¨æ­¤åºç¡ä¸è¿è¡æ©å±ï¼æ¯ææ´å¤æµéè®¾å¤ååºç¨åºæ¯ï¼å éç¸å³ææ¯çåå±ã</li>
</ul>
<h3 id="4-related-areas-or-applications_3">4. å¯è½åççç¸å³é¢åæåºç¨ (Related Areas or Applications)</h3>
<ul>
<li><strong>å·¥ä¸èªå¨åä¸è´¨éæ§å¶ï¼</strong> å¨å·¥åç¯å¢ä¸­ï¼èªå¨è¯»ååç§ä»ªè¡¨ãä¼ æå¨åæ¾ç¤ºå±çæ°å¼ï¼ç¨äºè¿ç¨çæ§ãæéè¯æ­åäº§åè´¨éæ£æµã</li>
<li><strong>å¢å¼ºç°å®ï¼ARï¼ä¸æ··åç°å®ï¼MRï¼ï¼</strong> ç»åå¤´æ´å¼è®¾å¤ï¼å®ç°å¯¹ç©çä¸çä¸­æ°å­æµéè®¾å¤çå®æ¶è¯å«åæ°å¼æåï¼ä¸ºç¨æ·æä¾ä¸ä¸æä¿¡æ¯ææä½æå¯¼ã</li>
<li><strong>æºå¨äººææ¯ï¼</strong> èµäºæºå¨äººè¯å«åçè§£ç¯å¢ä¸­åç§æ°å­æ¾ç¤ºå¨çè½åï¼ä¾å¦è¯»åè®¾å¤ç¶æãå®æç¹å®ä»»å¡ã</li>
<li><strong>è®¾å¤ç»´æ¤ä¸æ£æµï¼</strong> å¸®å©ææ¯äººåéè¿è§è§ç³»ç»å¿«éåç¡®å°è·åè®¾å¤è¯»æ°ï¼æé«ç»´æ¤æçååç¡®æ§ã</li>
<li><strong>æºè½ç©¿æ´è®¾å¤ï¼</strong> æåæºè½ç¼éç­è®¾å¤å¨å¤æç¯å¢ä¸­çè§£åäº¤äºç©çä¸çä¿¡æ¯çè½åã</li>
</ul>
<h3 id="5-limitations-inferable-from-the-abstract">5. ä»æè¦ä¸­å¯æ¨æ­çå±éæ§ (Limitations Inferable from the Abstract)</h3>
<ul>
<li><strong>é¢åç¹å¼æ§ï¼</strong> CAD2DMD-SETå·¥å·æ¯ä¸é¨ä¸ºæ°å­æµéè®¾å¤ï¼DMDsï¼è®¾è®¡çãå°å¶æ©å±å°å¶ä»ç±»åçè§è§ä»»å¡æç©ä½ï¼ä¾å¦ï¼æ¨¡æä»ªè¡¨ãæ´å¤æçå·¥ä¸è®¾å¤ï¼å°éè¦æ°ç3D CADæ¨¡ååå¯è½ä¸åçæ¸²æååæç­ç¥ï¼è¿å¹¶éä¸ä¸ªéç¨è§£å³æ¹æ¡ã</li>
<li><strong>CADæ¨¡åä¾èµæ§ï¼</strong> åææ°æ®çè´¨éåå¤æ ·æ§é«åº¦ä¾èµäºå¯ç¨3D CADæ¨¡åçåç¡®æ§åä¸°å¯æ§ãè·åé«è´¨éçCADæ¨¡åæ¬èº«å¯è½æ¯ä¸ä¸ªæææææ¬æ¥æºã</li>
<li><strong>æ½å¨çâæ¨¡æå°ç°å®âé¸¿æ²ï¼</strong> å°½ç®¡è®ºæå¼ºè°äºâé«ä¿çå¾ååæâï¼ä½åææ°æ®ä¸çå®ä¸çæ°æ®ä¹é´ä»å¯è½å­å¨ç»å¾®çåå¸å·®å¼ï¼å³æ®ä½çâæ¨¡æå°ç°å®âé¸¿æ²ï¼ï¼è¿å¯è½å¨æäºæªè¢«DMDBenchååè¦çççå®ä¸çåºæ¯ä¸­è¡¨ç°åºæ¥ãDMDBenchè½ç¶æä¾äºçå®ä¸çéªè¯ï¼ä½å¶1000å¼ å¾åçè§æ¨¡å¯è½ä¸è¶³ä»¥å®å¨ä»£è¡¨æææç«¯çå®ä¸çæ¡ä»¶ã</li>
<li><strong>è®¡ç®èµæºéæ±ï¼</strong> çæé«ä¿çãå¤æ ·åçåææ°æ®éï¼ç¹å«æ¯æ¶åé«çº§æ¸²æåå¤æåºæ¯åææ¶ï¼å¯è½éè¦å¤§éçè®¡ç®èµæºåæ¶é´ã</li>
</ul>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Motivated by
these limitations, this work introduces CAD2DMD-SET, a synthetic data
generation tool designed to support visual question answering (VQA) tasks
involving DMDs. By leveraging 3D CAD models, advanced rendering, and
high-fidelity image composition, our tool produces diverse, VQA-labelled
synthetic DMD datasets suitable for fine-tuning LVLMs. Additionally, we present
DMDBench, a curated validation set of 1,000 annotated real-world images
designed to evaluate model performance under practical constraints.</li>
<li>Benchmarking three state-of-the-art LVLMs using Average Normalised Levenshtein
Similarity (ANLS) and further fine-tuning LoRA's of these models with
CAD2DMD-SET's generated dataset yielded substantial improvements, with InternVL
showcasing a score increase of 200% without degrading on other tasks.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="http://arxiv.org/pdf/2508.21732v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2508.21732v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2508.21689v1'></a></p>
<h2 id="mapping-like-a-skeptic-probabilistic-bev-projection-for-online-hd-mapping"><a href="https://arxiv.org/abs/2508.21689v1">Mapping like a Skeptic: Probabilistic BEV Projection for Online HD Mapping</a></h2>
<p><strong>Authors:</strong> Fatih ErdoÄan, Merve Rabia BarÄ±n, Fatma GÃ¼ney</p>
<p><strong>Published:</strong> 2025-08-29</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Constructing high-definition (HD) maps from sensory input requires accurately
mapping the road elements in image space to the Bird's Eye View (BEV) space.
The precision of this mapping directly impacts the quality of the final
vectorized HD map. Existing HD mapping approaches outsource the projection to
standard mapping techniques, such as attention-based ones. However, these
methods struggle with accuracy due to generalization problems, often
hallucinating non-existent road elements. Our key idea is to start with a
geometric mapping based on camera parameters and adapt it to the scene to
extract relevant map information from camera images. To implement this, we
propose a novel probabilistic projection mechanism with confidence scores to
(i) refine the mapping to better align with the scene and (ii) filter out
irrelevant elements that should not influence HD map generation. In addition,
we improve temporal processing by using confidence scores to selectively
accumulate reliable information over time. Experiments on new splits of the
nuScenes and Argoverse2 datasets demonstrate improved performance over
state-of-the-art approaches, indicating better generalization. The improvements
are particularly pronounced on nuScenes and in the challenging long perception
range. Our code and model checkpoints are available at
https://github.com/Fatih-Erdogan/mapping-like-skeptic .</p>
<p><strong>Analysis:</strong></p>
<p>è¿ç¯è®ºæçæè¦æä¾äºä¸ä¸ªå³äºå¨çº¿é«æ¸å°å¾æå»ºä¸­é¸ç°å¾ï¼BEVï¼æå½±çå³é®æ¹è¿ãä»¥ä¸æ¯è¯¦ç»åæï¼</p>
<hr />
<h3 id="1-a-concise-summary-of-the-papers-main-contribution">1. è®ºæä¸»è¦è´¡ç®çç®æ´æ»ç» (A concise summary of the paper's main contribution)</h3>
<p>è¯¥è®ºææåºäºä¸ç§æ°é¢çæ¦çæ§æå½±æºå¶ï¼ç¨äºå°å¾åç©ºé´ä¸­çéè·¯åç´ åç¡®æ å°å°é¸ç°å¾ï¼BEVï¼ç©ºé´ï¼ä»¥æå»ºå¨çº¿é«æ¸å°å¾ãè¯¥æ¹æ³éè¿ç»ååºäºç¸æºåæ°çå ä½æ å°ä¸åºæ¯èªéåºçç½®ä¿¡åº¦åæ°ï¼è§£å³äºç°ææ¹æ³ï¼å¦åºäºæ³¨æåæºå¶ï¼å¨æ³åæ§åé¿åå¹»è§ï¼hallucinationï¼æ¹é¢çä¸è¶³ãå®éè¿ç½®ä¿¡åº¦åæ°æ¥ä¼åæ å°ãè¿æ»¤ä¸ç¸å³åç´ ï¼å¹¶éæ©æ§å°ç´¯ç§¯å¯é çé¿æä¿¡æ¯ï¼ä»èæ¾èæé«äºé«æ¸å°å¾çç²¾åº¦åæ³åè½åã</p>
<h3 id="2-the-key-innovation-or-methodological-approach">2. å³é®åæ°ææ¹æ³å­¦æ¹æ³ (The key innovation or methodological approach)</h3>
<p>è¯¥è®ºæçæ ¸å¿åæ°å¨äºå¶<strong>âæçè®ºèå¼âçæ¦çæ§æå½±æºå¶</strong>ï¼å®ä¸ä¼ ç»çº¯ç²¹ä¾èµå­¦ä¹ ææ³¨æåæºå¶çæ¹æ³å½¢æå¯¹æ¯ï¼</p>
<ul>
<li><strong>å ä½åéªä¸åºæ¯èªéåºç»åï¼</strong> ä¸åäºå®å¨ä¾èµæ°æ®é©±å¨çå­¦ä¹ æ¹æ³ï¼è¯¥æ¹æ³é¦åå©ç¨åºäºç¸æºåæ°çå ä½æ å°ä½ä¸ºèµ·ç¹ï¼ç¶åéè¿å­¦ä¹ æºå¶ä½¿å¶éåºç¹å®åºæ¯ãè¿æä¾äºä¸ä¸ªæ´ç¨³å¥çåå§åºç¡ï¼åå°äºæ¨¡åä»é¶å¼å§å­¦ä¹ æå½±çé¾åº¦ã</li>
<li><strong>ç½®ä¿¡åº¦åæ°é©±å¨çç²¾ç¼ä¸è¿æ»¤ï¼</strong> å¼å¥äºç½®ä¿¡åº¦åæ°æ¥ï¼<ul>
<li><strong>ç²¾ç¼æ å°ï¼</strong> ä½¿æå½±æ´å¥½å°ä¸å®éåºæ¯å¯¹é½ã</li>
<li><strong>è¿æ»¤æ å³åç´ ï¼</strong> æç¡®å°è¯å«å¹¶åé¤é£äºå¯è½ç±æ¨¡åâå¹»è§âåºæ¥çãä¸å­å¨çéè·¯åç´ ï¼ä»èæé«å°å¾çåç¡®æ§åå¯é æ§ã</li>
</ul>
</li>
<li><strong>æ¶é´ç»´åº¦ä¸çéæ©æ§ç´¯ç§¯ï¼</strong> å©ç¨ç½®ä¿¡åº¦åæ°å¨æ¶é´åºåä¸éæ©æ§å°ç´¯ç§¯å¯é ä¿¡æ¯ï¼å¢å¼ºäºå¨çº¿å°å¾æå»ºçé²æ£æ§åä¸è´æ§ï¼é¿åäºä¸å¯é ä¿¡æ¯çç´¯ç§¯ã</li>
</ul>
<h3 id="3-potential-impact-on-the-field_4">3. å¯¹é¢åçæ½å¨å½±å (Potential impact on the field)</h3>
<ul>
<li><strong>æé«é«æ¸å°å¾çå¯é æ§ä¸å®å¨æ§ï¼</strong> éè¿åå°å¹»è§åæé«æ³åè½åï¼è¯¥æ¹æ³è½å¤çææ´åç¡®ãæ´å¯é çé«æ¸å°å¾ï¼è¿å¯¹èªå¨é©¾é©¶ç³»ç»çå®å¨è¿è¡è³å³éè¦ã</li>
<li><strong>æ¨å¨æ··åå¼BEVæå½±æ¹æ³çåå±ï¼</strong> å ä½åéªä¸å­¦ä¹ æºå¶çç»åå¯è½å¯åæ´å¤æ··åå¼æ¹æ³ï¼å¨æ°æ®é©±å¨ççµæ´»æ§åå ä½çº¦æçé²æ£æ§ä¹é´æ¾å°æ´å¥½çå¹³è¡¡ã</li>
<li><strong>è§£å³é¿å°¾é®é¢åæ³åææï¼</strong> ç°ææ¹æ³å¨å¤ææä¸å¸¸è§åºæ¯ä¸çæ³åæ§å·®æ¯ä¸ä¸ªæ®éé®é¢ãè¯¥è®ºæçæ¹è¿ï¼å°¤å¶æ¯å¨nuScenesåé¿æç¥è·ç¦»ä¸çè¡¨ç°ï¼è¡¨æå®è½æ´å¥½å°åºå¯¹è¿äºææã</li>
<li><strong>ä¿è¿å¨çº¿å°å¾æå»ºçå®ç¨åï¼</strong> å¼ºè°å¨çº¿å¤çåæ¶é´ä¿¡æ¯ç´¯ç§¯ï¼ä½¿å¶æ´éç¨äºå®éèªå¨é©¾é©¶è½¦è¾çå®æ¶å°å¾æå»ºéæ±ã</li>
</ul>
<h3 id="4-related-areas-or-applications-that-might-benefit-from-this-research">4. å¯è½åçäºè¿é¡¹ç ç©¶çç¸å³é¢åæåºç¨ (Related areas or applications that might benefit from this research)</h3>
<ul>
<li><strong>èªå¨é©¾é©¶ï¼</strong> è¿æ¯æç´æ¥çåºç¨é¢åï¼é«æ¸å°å¾æ¯èªå¨é©¾é©¶æç¥ãå®ä½åè§åçæ ¸å¿ç»æé¨åã</li>
<li><strong>æºå¨äººå¯¼èªä¸SLAMï¼</strong> ä»»ä½éè¦ä»ä¼ æå¨è¾å¥æå»ºç¯å¢å°å¾çæºå¨äººç³»ç»ï¼é½å¯ä»¥ä»æ´åç¡®ãæ´å¯é çBEVæå½±åå°å¾æå»ºä¸­åçã</li>
<li><strong>3Dåºæ¯éå»ºï¼</strong> ä»2Då¾åä¸­åç¡®æ¨æ­3Dç»æåå¸å±ï¼å°¤å¶æ¯å¨ç»æåç¯å¢ä¸­ï¼å¦éè·¯ï¼ï¼è¯¥æ¹æ³å¯ä»¥æä¾æä»·å¼çåèã</li>
<li><strong>åå¸è§åä¸æµç»ï¼</strong> èªå¨åå°ä»å¾åæ°æ®çæé«ç²¾åº¦å°å¾ï¼å¯ä»¥æé«åå¸åºç¡è®¾æ½ç®¡çåæµç»çæçã</li>
<li><strong>æºè½äº¤éç³»ç»ï¼</strong> å®æ¶ãé«ç²¾åº¦çéè·¯åç´ ä¿¡æ¯å¯¹äºäº¤éæµç®¡çãäºä»¶æ£æµç­åºç¨å·æéè¦æä¹ã</li>
</ul>
<h3 id="5-any-limitations-that-can-be-inferred-from-the-abstract">5. ä»æè¦ä¸­å¯æ¨æ­åºçå±éæ§ (Any limitations that can be inferred from the abstract)</h3>
<ul>
<li><strong>å¯¹ç¸æºåæ°çä¾èµï¼</strong> æè¦ä¸­æå°âåºäºç¸æºåæ°çå ä½æ å°âï¼è¿æå³çè¯¥æ¹æ³å¯è½å¯¹åç¡®çç¸æºæ å®æä¸å®ä¾èµãå¦æç¸æºæ å®ä¸åç¡®ï¼åå§çå ä½æå½±å¯è½ä¼å¼å¥è¯¯å·®ï¼å°½ç®¡åç»­çæ¦çæºå¶ä¼å°è¯çº æ­£ã</li>
<li><strong>ç½®ä¿¡åº¦åæ°çé²æ£æ§ï¼</strong> å³é®å¨äºå¦ä½åç¡®å°çæåå©ç¨ç½®ä¿¡åº¦åæ°ãå¦æç½®ä¿¡åº¦åæ°æ¬èº«å¨æäºæç«¯ææ¨¡ç³åºæ¯ä¸ä¸å¯é ï¼å¯è½ä¼å½±åæ´ä½æ§è½ã</li>
<li><strong>âåºæ¯èªéåºâçèå´ï¼</strong> æè¦æå°âéåºåºæ¯ä»¥æåç¸å³å°å¾ä¿¡æ¯âï¼ä½æªè¯¦ç»è¯´æè¿ç§éåºæ§å¨é¢å¯¹é«åº¦å¨æãå¤ææå®å¨æ°é¢çåºæ¯æ¶çé²æ£æ§å¦ä½ã</li>
<li><strong>è®¡ç®å¼éï¼</strong> å¼å¥æ¦çæ§æºå¶åç½®ä¿¡åº¦åæ°ï¼ä»¥åæ¶é´ä¸çä¿¡æ¯ç´¯ç§¯ï¼å¯è½ä¼å¢å è®¡ç®å¤ææ§ï¼è¿å¯¹äºå¨çº¿ï¼å®æ¶ï¼åºç¨æ¥è¯´æ¯ä¸ä¸ªéè¦å³æ³¨çé®é¢ã</li>
<li><strong>æ°æ®ä¾èµæ§ï¼</strong> å°½ç®¡å£°ç§°æ³åæ§æ´å¥½ï¼ä½å®éªä»å¨nuScenesåArgoverse2æ°æ®éçæ°åå²ä¸è¿è¡ãå¨æ´å¹¿æ³ãæ´å¤æ ·åççå®ä¸çåºæ¯ä¸­çè¡¨ç°ä»éè¿ä¸æ­¥éªè¯ã</li>
</ul>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To implement this, we
propose a novel probabilistic projection mechanism with confidence scores to
(i) refine the mapping to better align with the scene and (ii) filter out
irrelevant elements that should not influence HD map generation.</li>
<li>Experiments on new splits of the
nuScenes and Argoverse2 datasets demonstrate improved performance over
state-of-the-art approaches, indicating better generalization.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="http://arxiv.org/pdf/2508.21689v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2508.21689v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-09-01 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
