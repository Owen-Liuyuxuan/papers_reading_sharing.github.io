<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-09-01 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-03
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-08-29/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-09-02/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-09-01">Arxiv Computer Vision Papers - 2025-09-01</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#maybe-you-dont-need-a-u-net-convolutional-feature-upsampling-for-materials-micrograph-segmentation" class="nav-link">Maybe you don't need a U-Net: convolutional feature upsampling for materials micrograph segmentation</a>
                </li>
                <li class="nav-item">
                    <a href="#flora-efficient-synthetic-data-generation-for-object-detection-in-low-data-regimes-via-finetuning-flux-lora" class="nav-link">FLORA: Efficient Synthetic Data Generation for Object Detection in Low-Data Regimes via finetuning Flux LoRA</a>
                </li>
                <li class="nav-item">
                    <a href="#the-rosario-dataset-v2-multimodal-dataset-for-agricultural-robotics" class="nav-link">The Rosario Dataset v2: Multimodal Dataset for Agricultural Robotics</a>
                </li>
                <li class="nav-item">
                    <a href="#complete-gaussian-splats-from-a-single-image-with-denoising-diffusion-models" class="nav-link">Complete Gaussian Splats from a Single Image with Denoising Diffusion Models</a>
                </li>
                <li class="nav-item">
                    <a href="#efficient-diffusion-based-3d-human-pose-estimation-with-hierarchical-temporal-pruning" class="nav-link">Efficient Diffusion-Based 3D Human Pose Estimation with Hierarchical Temporal Pruning</a>
                </li>
                <li class="nav-item">
                    <a href="#driveqa-passing-the-driving-knowledge-test" class="nav-link">DriveQA: Passing the Driving Knowledge Test</a>
                </li>
                <li class="nav-item">
                    <a href="#domain-generalization-in-the-wild-disentangling-classification-from-domain-aware-representations" class="nav-link">Domain Generalization in-the-Wild: Disentangling Classification from Domain-Aware Representations</a>
                </li>
                <li class="nav-item">
                    <a href="#uitron-foundational-gui-agent-with-advanced-perception-and-planning" class="nav-link">UItron: Foundational GUI Agent with Advanced Perception and Planning</a>
                </li>
                <li class="nav-item">
                    <a href="#cad2dmd-set-synthetic-generation-tool-of-digital-measurement-device-cad-model-datasets-for-fine-tuning-large-vision-language-models" class="nav-link">CAD2DMD-SET: Synthetic Generation Tool of Digital Measurement Device CAD Model Datasets for fine-tuning Large Vision-Language Models</a>
                </li>
                <li class="nav-item">
                    <a href="#mapping-like-a-skeptic-probabilistic-bev-projection-for-online-hd-mapping" class="nav-link">Mapping like a Skeptic: Probabilistic BEV Projection for Online HD Mapping</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-09-01">Arxiv Computer Vision Papers - 2025-09-01</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>好的，这是一份针对2025年8月29日Arxiv计算机视觉领域论文的每日报告执行摘要：</p>
<hr />
<p><strong>Arxiv 计算机视觉每日报告执行摘要 (2025年8月29日)</strong></p>
<p><strong>概述：</strong>
今日Arxiv计算机视觉领域的论文呈现出几个显著趋势：<strong>合成数据生成</strong>在解决低数据量和微调大型模型方面扮演着越来越重要的角色；<strong>3D视觉与重建</strong>技术持续创新，特别是结合扩散模型实现单图3D重建和高效姿态估计；<strong>自动驾驶与机器人</strong>领域的研究依然活跃，涵盖了数据集、知识问答和高精地图；此外，对<strong>模型效率、泛化能力</strong>以及<strong>通用AI代理</strong>的探索也引人注目。</p>
<p><strong>主要主题与趋势：</strong></p>
<ol>
<li><strong>高效合成数据生成：</strong> 多篇论文聚焦于利用LoRA等技术高效生成高质量合成数据，以应对特定领域（如工业CAD模型、低数据量场景）的数据稀缺问题，并支持大型视觉-语言模型（LVMs）的微调。</li>
<li><strong>3D视觉与扩散模型的融合：</strong> 扩散模型在3D重建和姿态估计中展现出强大潜力，实现了从单张图像生成完整高斯泼溅（Gaussian Splats）以及高效的3D人体姿态估计。</li>
<li><strong>自动驾驶与机器人感知：</strong> 新的多模态数据集发布，同时研究关注于自动驾驶知识问答（VQA）和鲁棒的在线高精地图构建，强调了实际应用中的感知与推理能力。</li>
<li><strong>模型效率与泛化：</strong> 有研究探索替代传统U-Net架构以提高分割效率，并深入探讨了在“野外”场景下实现领域泛化的方法，旨在提升模型在未知环境中的鲁棒性。</li>
<li><strong>通用AI代理的崛起：</strong> 出现了旨在构建基础性GUI代理的工作，结合先进的感知和规划能力，预示着通用型AI在人机交互领域的应用前景。</li>
</ol>
<p><strong>特别重要或创新的论文：</strong></p>
<ul>
<li><strong>"Complete Gaussian Splats from a Single Image with Denoising Diffusion Models" (Ziwei Liao et al.)：</strong> 这篇论文极具创新性，它利用去噪扩散模型实现了从单张图像生成完整的3D高斯泼溅，这在单目3D重建领域是一个重大突破，有望大幅简化3D内容创建流程。</li>
<li><strong>"UItron: Foundational GUI Agent with Advanced Perception and Planning" (Zhixiong Zeng et al.)：</strong> 这项工作代表了通用AI代理发展的重要一步。构建一个能够感知和规划的GUI代理，对于实现更智能、更自主的人机交互具有深远影响。</li>
<li><strong>"Maybe you don't need a U-Net: convolutional feature upsampling for materials micrograph segmentation" (Ronan Docherty et al.)：</strong> 挑战了U-Net这一在分割领域广泛使用的架构，提出了一种可能更高效的卷积特征上采样方法，对于追求模型效率的研究者而言具有重要参考价值。</li>
<li><strong>"FLORA: Efficient Synthetic Data Generation for Object Detection in Low-Data Regimes via finetuning Flux LoRA" (Alvaro Patricio et al.)：</strong> 针对低数据量场景提供了高效的合成数据生成方案，其结合LoRA微调的策略具有很强的实用性和普适性。</li>
</ul>
<p><strong>新兴研究方向或技术：</strong></p>
<ul>
<li><strong>扩散模型在3D生成与重建中的深度应用：</strong> 不仅限于图像生成，扩散模型正成为3D内容创建（如高斯泼溅、姿态估计）的关键技术。</li>
<li><strong>面向特定任务和大型模型的合成数据生成：</strong> 结合LoRA等高效微调技术，合成数据不再仅仅是数据增强，而是成为定制化训练大型模型、解决特定领域数据瓶颈的核心策略。</li>
<li><strong>通用型感知-规划AI代理：</strong> 旨在构建能够理解和操作复杂环境（如GUI界面）的通用代理，是迈向更高级AI的重要一步。</li>
<li><strong>鲁棒性与领域泛化的新范式：</strong> 通过解耦表示学习和概率建模等方法，提升模型在复杂、未知真实世界环境中的可靠性。</li>
</ul>
<p><strong>建议阅读全文的论文：</strong></p>
<ol>
<li><strong>"Complete Gaussian Splats from a Single Image with Denoising Diffusion Models" (Ziwei Liao et al.)：</strong> 如果您对3D视觉、新颖的3D重建方法或扩散模型感兴趣，这篇论文提供了令人兴奋的进展。</li>
<li><strong>"UItron: Foundational GUI Agent with Advanced Perception and Planning" (Zhixiong Zeng et al.)：</strong> 对于关注通用AI、AI代理或人机交互的研究人员，这篇论文展示了未来AI应用的一个重要方向。</li>
<li><strong>"FLORA: Efficient Synthetic Data Generation for Object Detection in Low-Data Regimes via finetuning Flux LoRA" (Alvaro Patricio et al.)：</strong> 对于面临数据稀缺问题或希望高效利用合成数据进行目标检测的研究者，这篇论文提供了实用的解决方案。</li>
<li><strong>"Maybe you don't need a U-Net: convolutional feature upsampling for materials micrograph segmentation" (Ronan Docherty et al.)：</strong> 如果您在分割任务中寻求更高效的模型架构，或者对U-Net的替代方案感兴趣，这篇论文值得深入研究。</li>
<li><strong>"DriveQA: Passing the Driving Knowledge Test" (Maolin Wei et al.)：</strong> 对于自动驾驶领域的VQA和知识推理感兴趣的研究人员，这篇论文提出了一个新颖的基准和挑战。</li>
</ol>
<hr />
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2508.21529v1">Maybe you don't need a U-Net: convolutional feature upsampling for materials micrograph segmentation</a></li>
<li><a href="#2508.21712v1">FLORA: Efficient Synthetic Data Generation for Object Detection in Low-Data Regimes via finetuning Flux LoRA</a></li>
<li><a href="#2508.21635v1">The Rosario Dataset v2: Multimodal Dataset for Agricultural Robotics</a></li>
<li><a href="#2508.21542v1">Complete Gaussian Splats from a Single Image with Denoising Diffusion Models</a></li>
<li><a href="#2508.21363v1">Efficient Diffusion-Based 3D Human Pose Estimation with Hierarchical Temporal Pruning</a></li>
<li><a href="#2508.21824v1">DriveQA: Passing the Driving Knowledge Test</a></li>
<li><a href="#2508.21769v1">Domain Generalization in-the-Wild: Disentangling Classification from Domain-Aware Representations</a></li>
<li><a href="#2508.21767v1">UItron: Foundational GUI Agent with Advanced Perception and Planning</a></li>
<li><a href="#2508.21732v1">CAD2DMD-SET: Synthetic Generation Tool of Digital Measurement Device CAD Model Datasets for fine-tuning Large Vision-Language Models</a></li>
<li><a href="#2508.21689v1">Mapping like a Skeptic: Probabilistic BEV Projection for Online HD Mapping</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2508.21529v1'></a></p>
<h2 id="maybe-you-dont-need-a-u-net-convolutional-feature-upsampling-for-materials-micrograph-segmentation"><a href="https://arxiv.org/abs/2508.21529v1">Maybe you don't need a U-Net: convolutional feature upsampling for materials micrograph segmentation</a></h2>
<p><strong>Authors:</strong> Ronan Docherty, Antonis Vamvakeros, Samuel J. Cooper</p>
<p><strong>Published:</strong> 2025-08-29</p>
<p><strong>Categories:</strong> cs.CV, cond-mat.mtrl-sci</p>
<p><strong>Abstract:</strong></p>
<p>Feature foundation models - usually vision transformers - offer rich semantic
descriptors of images, useful for downstream tasks such as (interactive)
segmentation and object detection. For computational efficiency these
descriptors are often patch-based, and so struggle to represent the fine
features often present in micrographs; they also struggle with the large image
sizes present in materials and biological image analysis. In this work, we
train a convolutional neural network to upsample low-resolution (i.e, large
patch size) foundation model features with reference to the input image. We
apply this upsampler network (without any further training) to efficiently
featurise and then segment a variety of microscopy images, including plant
cells, a lithium-ion battery cathode and organic crystals. The richness of
these upsampled features admits separation of hard to segment phases, like
hairline cracks. We demonstrate that interactive segmentation with these deep
features produces high-quality segmentations far faster and with far fewer
labels than training or finetuning a more traditional convolutional network.</p>
<p><strong>Analysis:</strong></p>
<p>这篇论文的摘要提出了一种新颖的方法，旨在解决当前特征基础模型（通常是Vision Transformer）在处理显微图像时面临的挑战，即难以捕捉精细特征和处理大尺寸图像。以下是详细分析：</p>
<hr />
<h3 id="1-concise-summary">1. 论文主要贡献的简明摘要 (Concise Summary)</h3>
<p>本文提出了一种创新的卷积神经网络上采样器，用于将低分辨率的基础模型特征高效地提升至高分辨率，并结合原始输入图像信息。该上采样器无需额外训练即可应用于多种显微图像，实现了对复杂材料显微图像的高质量、交互式分割，且所需标注远少于传统卷积网络。</p>
<h3 id="2-key-innovation-or-methodological-approach">2. 关键创新或方法学方法 (Key Innovation or Methodological Approach)</h3>
<p>核心创新在于其提出的<strong>“卷积特征上采样器”网络</strong>。该网络是一个独立的卷积神经网络，专门用于：
1.  接收预训练基础模型（如Vision Transformer）生成的<strong>低分辨率、粗粒度特征图</strong>。
2.  <strong>结合原始输入图像的精细纹理信息</strong>作为参考。
3.  将这些特征上采样为<strong>高分辨率、语义丰富的特征表示</strong>。</p>
<p>关键之处在于，这个上采样器一旦训练完成，便可无需额外训练或微调，直接应用于多种不同的显微图像分割任务（例如植物细胞、锂离子电池阴极、有机晶体），从而高效地生成高质量的深度特征，进而支持交互式分割。这与U-Net等端到端分割网络或直接微调基础模型的范式不同，它将特征提取和精细化解耦，专注于提升基础模型的空间分辨率。</p>
<h3 id="3-potential-impact-on-the-field">3. 对领域潜在影响 (Potential Impact on the Field)</h3>
<p>该研究对计算机视觉领域，特别是科学图像分析（如材料科学、生物医学）具有显著潜在影响：
*   <strong>提升基础模型在密集预测任务中的实用性：</strong> 它提供了一种有效策略，能够利用现有基础模型的强大语义理解能力，同时克服其在空间分辨率上的固有不足，使其更适用于需要像素级精度的任务。
*   <strong>加速显微图像分析：</strong> 通过高效生成高质量特征，有望显著加速显微图像的自动化分析和解释。
*   <strong>降低标注成本，提高交互式分割效率：</strong> 强调了在交互式分割中，该方法能以更快的速度和更少的标签实现高质量分割，这对于标注成本高昂的专业领域（如医学、材料）具有巨大价值。
*   <strong>新的模型范式：</strong> “一次训练，多任务应用”的上采样器范式，可能为未来基础模型在特定领域（尤其是科学图像）的部署和应用开辟新途径，即通过一个通用的特征精细化模块来适应不同任务。</p>
<h3 id="4-related-areas-or-applications">4. 可能受益的相关领域或应用 (Related Areas or Applications)</h3>
<ul>
<li><strong>材料科学与工程：</strong> 显微结构分析、缺陷检测（如发丝裂纹）、相分离识别、晶粒边界分割。</li>
<li><strong>生物医学图像分析：</strong> 细胞分割、组织病理学图像分析、神经元追踪、电子显微镜图像处理、高分辨率荧光显微图像分析。</li>
<li><strong>工业检测：</strong> 表面缺陷检测、质量控制，尤其是在需要识别微小瑕疵的场景。</li>
<li><strong>遥感图像处理：</strong> 高分辨率卫星图像中的地物分割与识别，特别是对精细地物（如道路、建筑物边缘）的提取。</li>
<li><strong>任何需要精细像素级理解且图像尺寸较大、特征细节丰富的领域。</strong></li>
</ul>
<h3 id="5-limitations-that-can-be-inferred-from-the-abstract">5. 从摘要中可推断的局限性 (Limitations that can be inferred from the abstract)</h3>
<ul>
<li><strong>对基础模型特征质量的依赖：</strong> 该方法的性能在一定程度上取决于所使用的基础模型生成特征的质量。如果基础模型未能捕捉到某些关键语义信息，上采样器也难以凭空创造。</li>
<li><strong>“无需进一步训练”的权衡：</strong> 尽管上采样器无需额外训练即可应用于新任务是其优势，但这可能意味着在特定数据集上，其性能可能无法超越经过充分微调的端到端网络（如U-Net），尤其是在全自动分割场景下。摘要中强调的是交互式分割的效率提升。</li>
<li><strong>与U-Net的直接性能对比未明：</strong> 标题暗示了对U-Net的替代，但摘要主要强调在交互式分割中“更快、更少标签”的优势，并未直接给出在全自动、非交互式场景下与U-Net等传统方法的量化性能对比。</li>
<li><strong>计算成本的潜在考量：</strong> 虽然强调了“计算效率”，但结合基础模型和独立的上采样器网络，其总体的推理时间或内存占用与一个优化良好的U-Net相比如何，仍需进一步验证。</li>
<li><strong>对输入图像质量的敏感性：</strong> 上采样器“参考输入图像”进行精细化，这意味着如果输入图像本身存在噪声或伪影，可能会影响上采样和分割的准确性。</li>
</ul>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>We demonstrate that interactive segmentation with these deep
features produces high-quality segmentations far faster and with far fewer
labels than training or finetuning a more traditional convolutional network.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="http://arxiv.org/pdf/2508.21529v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2508.21529v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2508.21712v1'></a></p>
<h2 id="flora-efficient-synthetic-data-generation-for-object-detection-in-low-data-regimes-via-finetuning-flux-lora"><a href="https://arxiv.org/abs/2508.21712v1">FLORA: Efficient Synthetic Data Generation for Object Detection in Low-Data Regimes via finetuning Flux LoRA</a></h2>
<p><strong>Authors:</strong> Alvaro Patricio, Atabak Dehban, Rodrigo Ventura</p>
<p><strong>Published:</strong> 2025-08-29</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Recent advances in diffusion-based generative models have demonstrated
significant potential in augmenting scarce datasets for object detection tasks.
Nevertheless, most recent models rely on resource-intensive full fine-tuning of
large-scale diffusion models, requiring enterprise-grade GPUs (e.g., NVIDIA
V100) and thousands of synthetic images. To address these limitations, we
propose Flux LoRA Augmentation (FLORA), a lightweight synthetic data generation
pipeline. Our approach uses the Flux 1.1 Dev diffusion model, fine-tuned
exclusively through Low-Rank Adaptation (LoRA). This dramatically reduces
computational requirements, enabling synthetic dataset generation with a
consumer-grade GPU (e.g., NVIDIA RTX 4090). We empirically evaluate our
approach on seven diverse object detection datasets. Our results demonstrate
that training object detectors with just 500 synthetic images generated by our
approach yields superior detection performance compared to models trained on
5000 synthetic images from the ODGEN baseline, achieving improvements of up to
21.3% in mAP@.50:.95. This work demonstrates that it is possible to surpass
state-of-the-art performance with far greater efficiency, as FLORA achieves
superior results using only 10% of the data and a fraction of the computational
cost. This work demonstrates that a quality and efficiency-focused approach is
more effective than brute-force generation, making advanced synthetic data
creation more practical and accessible for real-world scenarios.</p>
<p><strong>Analysis:</strong></p>
<p>作为计算机视觉和机器学习领域的专家，我对这篇论文摘要的分析如下：</p>
<hr />
<h3 id="flora-efficient-synthetic-data-generation-for-object-detection-in-low-data-regimes-via-finetuning-flux-lora_1">FLORA: Efficient Synthetic Data Generation for Object Detection in Low-Data Regimes via finetuning Flux LoRA</h3>
<p><strong>1. 论文主要贡献 (Concise Summary)</strong></p>
<p>FLORA提出了一种轻量级、高效的合成数据生成管线，专为低数据量目标检测任务设计。它通过对Flux 1.1 Dev扩散模型进行LoRA微调，显著降低了计算资源需求，并证明仅用少量（500张）高质量合成图像即可超越现有基线（ODGEN）使用大量（5000张）图像的性能，实现了更高的检测精度和效率。这项工作强调了质量和效率优先的合成数据生成策略优于蛮力生成。</p>
<p><strong>2. 关键创新或方法 (Key Innovation or Methodological Approach)</strong></p>
<p>该论文的关键创新在于：</p>
<ul>
<li><strong>LoRA微调扩散模型以实现高效性：</strong> 核心创新是将低秩适应（LoRA）技术应用于Flux 1.1 Dev扩散模型的微调，以生成目标检测所需的合成数据。相较于传统需要全量微调大型扩散模型的方法，LoRA仅更新少量参数，极大地降低了计算成本，使得在消费级GPU（如NVIDIA RTX 4090）上进行高效的合成数据生成成为可能。</li>
<li><strong>“质量优先于数量”的合成数据策略：</strong> 论文明确提出并验证了“质量和效率优先”的策略优于“蛮力生成”。实验结果表明，仅用500张FLORA生成的合成图像，就能在mAP@.50:.95上取得高达21.3%的提升，并超越使用5000张ODGEN基线图像训练的模型。这表明FLORA能够生成更高质量、对检测器训练更有效的合成数据，而非简单地追求数量。</li>
</ul>
<p><strong>3. 对领域潜在影响 (Potential Impact on the Field)</strong></p>
<ul>
<li><strong>合成数据生成的民主化：</strong> 极大地降低了合成数据生成的技术门槛和资源需求，使更多研究者和小型团队能够在消费级硬件上利用先进的扩散模型进行数据增强，推动了该领域的民主化。</li>
<li><strong>改变合成数据研究范式：</strong> 挑战了传统上认为合成数据量越大越好的观念，强调了数据质量和生成效率的重要性，可能引导未来合成数据研究转向更精细、更高效的生成策略。</li>
<li><strong>加速低数据量场景下的AI应用：</strong> 在数据稀缺场景下，FLORA能够以更低的成本、更快的速度提供高质量的训练数据，加速了目标检测模型在特定领域（如医疗、工业检测、小众物体识别）的开发和部署。</li>
<li><strong>推动LoRA在生成模型中的应用：</strong> 进一步证明了LoRA作为一种高效微调策略在大型生成模型（特别是扩散模型）中的巨大潜力，可能启发更多将LoRA应用于其他复杂生成任务的研究。</li>
</ul>
<p><strong>4. 相关受益领域或应用 (Related Areas or Applications that Might Benefit)</strong></p>
<ul>
<li><strong>所有面临数据稀缺挑战的目标检测任务：</strong> 例如：医疗影像分析（罕见疾病检测）、工业缺陷检测（特定产品缺陷）、农业（特定作物病虫害）、自动驾驶（长尾事件或罕见物体）、机器人视觉等。</li>
<li><strong>资源受限的研究机构或企业：</strong> 无法承担昂贵的企业级GPU和大规模数据生成成本的场景。</li>
<li><strong>快速原型开发和迭代：</strong> 需要快速生成特定场景数据以验证模型或算法的场景。</li>
<li><strong>其他计算机视觉任务：</strong> 该方法的核心思想（LoRA微调扩散模型以高效生成高质量数据）也可能推广到其他计算机视觉任务，如语义分割、实例分割，甚至图像生成与编辑等，只要这些任务能从高效、高质量的合成数据中受益。</li>
</ul>
<p><strong>5. 潜在局限性 (Limitations that Can Be Inferred from the Abstract)</strong></p>
<ul>
<li><strong>“低数据量场景”的具体定义：</strong> 摘要中提到的“低数据量场景”的具体定义和范围尚不明确。例如，500张合成图像是否足以应对所有极低数据量（如只有几十张真实图像）的场景？</li>
<li><strong>基线对比的全面性：</strong> 虽然与ODGEN基线进行了对比，但ODGEN是否代表了当前合成数据生成领域的最新SOTA方法？是否有其他更先进的、基于扩散模型的全量微调方法未被提及或对比？这会影响对“超越SOTA性能”这一说法的判断。</li>
<li><strong>Flux模型依赖性：</strong> FLORA依赖于Flux 1.1 Dev模型。该方法在其他扩散模型（如Stable Diffusion、DALL-E等）上的表现如何？LoRA微调的有效性是否与基础扩散模型的架构或预训练数据强相关？</li>
<li><strong>合成数据质量的深层评估：</strong> mAP@.50:.95是目标检测的标准指标，但合成图像本身的视觉质量、多样性以及对模型泛化能力的长期影响，在摘要中未详细说明。例如，合成数据是否引入了新的偏差或伪影？</li>
<li><strong>真实世界域差距：</strong> 合成数据与真实世界数据之间的域差距（domain gap）始终是一个挑战。尽管FLORA提高了性能，但这种提升在面对极端真实世界复杂性时的鲁棒性如何？模型在纯合成数据上训练后，在未见过的真实数据上的表现如何？</li>
</ul>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>Our approach uses the Flux 1.1 Dev diffusion model, fine-tuned
exclusively through Low-Rank Adaptation (LoRA).</li>
<li>This work demonstrates that it is possible to surpass
state-of-the-art performance with far greater efficiency, as FLORA achieves
superior results using only 10% of the data and a fraction of the computational
cost.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="http://arxiv.org/pdf/2508.21712v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2508.21712v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2508.21635v1'></a></p>
<h2 id="the-rosario-dataset-v2-multimodal-dataset-for-agricultural-robotics"><a href="https://arxiv.org/abs/2508.21635v1">The Rosario Dataset v2: Multimodal Dataset for Agricultural Robotics</a></h2>
<p><strong>Authors:</strong> Nicolas Soncini, Javier Cremona, Erica Vidal, Maximiliano García, Gastón Castro, Taihú Pire</p>
<p><strong>Published:</strong> 2025-08-29</p>
<p><strong>Categories:</strong> cs.RO, cs.CV, cs.SY, eess.SY, I.2.9</p>
<p><strong>Abstract:</strong></p>
<p>We present a multi-modal dataset collected in a soybean crop field,
comprising over two hours of recorded data from sensors such as stereo infrared
camera, color camera, accelerometer, gyroscope, magnetometer, GNSS (Single
Point Positioning, Real-Time Kinematic and Post-Processed Kinematic), and wheel
odometry. This dataset captures key challenges inherent to robotics in
agricultural environments, including variations in natural lighting, motion
blur, rough terrain, and long, perceptually aliased sequences. By addressing
these complexities, the dataset aims to support the development and
benchmarking of advanced algorithms for localization, mapping, perception, and
navigation in agricultural robotics. The platform and data collection system is
designed to meet the key requirements for evaluating multi-modal SLAM systems,
including hardware synchronization of sensors, 6-DOF ground truth and loops on
long trajectories.
  We run multimodal state-of-the art SLAM methods on the dataset, showcasing
the existing limitations in their application on agricultural settings. The
dataset and utilities to work with it are released on
https://cifasis.github.io/rosariov2/.</p>
<p><strong>Analysis:</strong></p>
<p>作为计算机视觉和机器学习领域的专家，我对这篇论文摘要的分析如下：</p>
<hr />
<h3 id="the-rosario-dataset-v2-multimodal-dataset-for-agricultural-robotics_1">论文摘要分析：The Rosario Dataset v2: Multimodal Dataset for Agricultural Robotics</h3>
<p><strong>1. 论文主要贡献的简明总结 (2-3句话)</strong></p>
<p>Rosario Dataset v2 是一个针对农业机器人领域的多模态数据集，在真实的大豆农田中采集，包含超过两小时的丰富传感器数据，如立体红外相机、彩色相机、IMU、多种GNSS和轮式里程计。该数据集旨在通过提供硬件同步、6-DOF真值和长轨迹循环等关键特性，解决农业环境中（如光照变化、运动模糊、崎岖地形和感知混叠）的挑战，从而支持和基准测试先进的定位、建图、感知和导航算法。作者还利用该数据集运行了最先进的多模态SLAM方法，揭示了现有算法在农业应用中的局限性。</p>
<p><strong>2. 关键创新或方法论方法</strong></p>
<p>该论文的关键创新在于<strong>创建了一个高度全面且极具挑战性的多模态数据集，专门为农业机器人应用量身定制</strong>。其方法论体现在：</p>
<ul>
<li><strong>丰富的多模态传感器融合：</strong> 结合了立体红外相机（对光照变化鲁棒）、彩色相机、惯性测量单元（IMU）、多种GNSS（包括高精度的RTK和PPK）以及轮式里程计，提供了极其多样化的数据流，这对于开发鲁棒的传感器融合算法至关重要。</li>
<li><strong>针对农业环境的特定挑战：</strong> 数据集的设计和采集明确考虑了农业环境的独特难题，如自然光照剧烈变化、崎岖地形导致的运动模糊、以及农作物行间重复模式造成的“感知混叠”（perceptually aliased sequences），这些都是传统CV和SLAM算法的痛点。</li>
<li><strong>高标准的数据质量和基准测试支持：</strong> 强调了传感器硬件同步、提供高精度的6-DOF（六自由度）地面真值，以及包含长轨迹上的循环（loops），这些都是评估和开发多模态SLAM系统不可或缺的要素。通过运行SOTA SLAM方法并展示其局限性，作者不仅验证了数据集的挑战性，也为后续研究提供了明确的方向。</li>
</ul>
<p><strong>3. 对领域潜在影响</strong></p>
<ul>
<li><strong>加速农业机器人研究：</strong> 提供了一个标准化、高挑战性且公开可用的基准数据集，将极大地加速农业领域定位、建图、感知和导航算法的开发和评估。</li>
<li><strong>推动鲁棒算法发展：</strong> 迫使研究人员开发能够应对真实世界农业环境复杂性的更鲁棒、更泛化的计算机视觉和机器学习算法，尤其是在光照变化、纹理重复和崎岖地形下的表现。</li>
<li><strong>弥合理论与实践的鸿沟：</strong> 通过提供真实世界的复杂数据，帮助研究人员将实验室成果更好地应用于实际的农业自动化和精准农业场景。</li>
<li><strong>促进多传感器融合和SLAM技术进步：</strong> 数据集的多模态特性将推动多传感器融合技术的发展，特别是在视觉-惯性-GNSS-里程计融合SLAM方面，以实现更精确和可靠的定位。</li>
<li><strong>为新研究方向提供基础：</strong> 数据集中的挑战（如感知混叠）可能会激发新的计算机视觉和机器学习方法，例如基于语义信息的定位、拓扑建图或更先进的循环闭合技术。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用</strong></p>
<ul>
<li><strong>农业机器人与自动化：</strong> 这是最直接的受益领域，包括自主拖拉机、喷洒机器人、采摘机器人、作物监测无人车等。</li>
<li><strong>同步定位与建图 (SLAM)：</strong> 特别是多模态SLAM、视觉-惯性SLAM、以及在低纹理或重复纹理环境下的SLAM。</li>
<li><strong>计算机视觉：</strong> 户外场景理解、作物/杂草检测与分割、3D重建、运动估计、光流分析、以及在恶劣光照条件下的图像处理。</li>
<li><strong>机器学习与深度学习：</strong> 训练用于农业环境的鲁棒感知模型，例如用于作物健康监测、病虫害识别、产量估计等。</li>
<li><strong>传感器融合：</strong> 开发和测试融合异构传感器数据以提高系统性能的算法。</li>
<li><strong>户外自主导航：</strong> 任何需要在非结构化、动态和挑战性户外环境中进行自主导航的系统（例如，建筑机器人、采矿机器人、林业机器人）。</li>
<li><strong>高精度GNSS应用：</strong> 结合RTK/PPK数据进行高精度定位和地图校准的研究。</li>
</ul>
<p><strong>5. 从摘要中可推断的局限性</strong></p>
<ul>
<li><strong>作物和环境特异性：</strong> 数据集仅在大豆农田中采集。虽然大豆是重要作物，但其视觉特征、生长模式和农田结构可能与其他作物（如玉米、小麦、果园、温室蔬菜）有显著差异。这限制了其在其他农业场景中的直接泛化能力。</li>
<li><strong>地理位置和气候限制：</strong> 数据集在“Rosario”地区采集，这意味着其可能反映特定地理区域的气候、土壤类型和光照条件。在其他地区，环境因素可能大相径庭。</li>
<li><strong>平台特定性：</strong> 数据是由一个特定的“平台和数据采集系统”收集的。传感器的具体配置、安装高度、视角以及平台自身的运动特性可能无法完全代表所有农业机器人系统。</li>
<li><strong>主要侧重于SLAM/导航：</strong> 尽管数据丰富，但摘要中明确强调了对SLAM系统评估的关键要求（硬件同步、6-DOF真值、循环）。这表明数据集的设计和标注可能更侧重于定位和建图任务，对于其他细粒度的计算机视觉任务（如精确的植物表型分析、病害早期检测）可能需要额外的标注或处理。</li>
<li><strong>未提及其他动态障碍物：</strong> 摘要中没有明确说明数据集中是否包含除了机器人本身之外的其他动态障碍物（例如，农场工人、野生动物、其他农机）。在真实的农业环境中，这些动态因素对导航和安全至关重要。</li>
<li><strong>数据量：</strong> “超过两小时”的数据量对于深度学习模型训练来说，虽然不错，但可能不如一些大规模城市数据集那样庞大，这在某些需要海量数据的任务上可能构成限制。</li>
</ul>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present a multi-modal dataset collected in a soybean crop field,
comprising over two hours of recorded data from sensors such as stereo infrared
camera, color camera, accelerometer, gyroscope, magnetometer, GNSS (Single
Point Positioning, Real-Time Kinematic and Post-Processed Kinematic), and wheel
odometry.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="http://arxiv.org/pdf/2508.21635v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2508.21635v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2508.21542v1'></a></p>
<h2 id="complete-gaussian-splats-from-a-single-image-with-denoising-diffusion-models"><a href="https://arxiv.org/abs/2508.21542v1">Complete Gaussian Splats from a Single Image with Denoising Diffusion Models</a></h2>
<p><strong>Authors:</strong> Ziwei Liao, Mohamed Sayed, Steven L. Waslander, Sara Vicente, Daniyar Turmukhambetov, Michael Firman</p>
<p><strong>Published:</strong> 2025-08-29</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>Gaussian splatting typically requires dense observations of the scene and can
fail to reconstruct occluded and unobserved areas. We propose a latent
diffusion model to reconstruct a complete 3D scene with Gaussian splats,
including the occluded parts, from only a single image during inference.
Completing the unobserved surfaces of a scene is challenging due to the
ambiguity of the plausible surfaces. Conventional methods use a
regression-based formulation to predict a single "mode" for occluded and
out-of-frustum surfaces, leading to blurriness, implausibility, and failure to
capture multiple possible explanations. Thus, they often address this problem
partially, focusing either on objects isolated from the background,
reconstructing only visible surfaces, or failing to extrapolate far from the
input views. In contrast, we propose a generative formulation to learn a
distribution of 3D representations of Gaussian splats conditioned on a single
input image. To address the lack of ground-truth training data, we propose a
Variational AutoReconstructor to learn a latent space only from 2D images in a
self-supervised manner, over which a diffusion model is trained. Our method
generates faithful reconstructions and diverse samples with the ability to
complete the occluded surfaces for high-quality 360-degree renderings.</p>
<p><strong>Analysis:</strong></p>
<p>这篇论文的摘要展示了计算机视觉领域一个非常有趣且重要的进展。以下是我的详细分析：</p>
<hr />
<h3 id="1-concise-summary_1">1. 论文主要贡献的简明摘要 (Concise Summary)</h3>
<p>本文提出了一种创新的潜在扩散模型，能够仅从单张图像推断出完整的3D场景，包括被遮挡和未观察到的区域，并以高斯泼溅（Gaussian Splats）的形式表示。与传统回归方法生成单一模糊或不真实的重建不同，该方法采用生成式公式来学习3D表示的分布，从而实现忠实且多样化的场景补全，支持高质量的360度渲染。</p>
<h3 id="2-key-innovation-or-methodological-approach_1">2. 关键创新或方法论 (Key Innovation or Methodological Approach)</h3>
<p>核心创新在于其<strong>生成式公式</strong>，它不是预测单一的3D重建，而是学习给定单张输入图像下3D高斯泼溅表示的<strong>分布</strong>。这使得模型能够捕捉被遮挡和未观察区域的多种合理解释，从而避免了传统回归方法导致的模糊和不真实。为实现这一目标，该方法利用了<strong>潜在扩散模型</strong>，并引入了<strong>变分自重建器（Variational AutoReconstructor, VAR）</strong>，以自监督方式仅从2D图像学习一个合适的潜在空间，从而解决了3D真值数据稀缺的问题。</p>
<h3 id="3-potential-impact-on-the-field_1">3. 对领域潜在影响 (Potential Impact on the Field)</h3>
<p>这项研究有望显著推动<strong>单图像3D场景重建</strong>领域的发展，使高斯泼溅技术在缺乏密集多视角数据的实际应用中更具实用性。通过提供<strong>完整且多样化</strong>的3D场景表示，它能极大地降低3D内容创建的数据采集门槛，并为需要理解被遮挡场景部分的<strong>机器人、AR/VR和自动驾驶</strong>等领域提供更鲁棒的3D感知能力。它将单图像3D重建从单一确定性输出推向了生成式、多模态输出的新范式。</p>
<h3 id="4-related-areas-or-applications_1">4. 相关领域或应用 (Related Areas or Applications)</h3>
<ul>
<li><strong>机器人学与自动驾驶：</strong> 提升对复杂环境的3D感知能力，尤其是在部分遮挡或视角受限的情况下，有助于路径规划、避障和人机交互。</li>
<li><strong>增强现实（AR）与虚拟现实（VR）：</strong> 实现更逼真、更完整的虚拟场景构建和内容生成，提升沉浸式体验。</li>
<li><strong>3D内容创作：</strong> 极大地简化从2D图像生成完整3D资产的流程，降低成本和时间。</li>
<li><strong>数字孪生：</strong> 从有限的图像数据中构建更全面的物理世界数字模型。</li>
<li><strong>遥感与测绘：</strong> 从单张航空或卫星图像中推断出更完整的地形或建筑3D模型。</li>
</ul>
<h3 id="5-limitations-inferred-from-the-abstract">5. 可从摘要中推断出的局限性 (Limitations Inferred from the Abstract)</h3>
<ul>
<li><strong>自监督学习的局限性：</strong> 尽管通过VAR解决了3D真值数据稀缺的问题，但自监督学习获得的潜在空间的质量和泛化能力，可能仍不如直接使用大量高质量3D真值数据进行监督学习。</li>
<li><strong>计算资源消耗：</strong> 扩散模型，尤其是在3D生成任务中，通常在训练和推理阶段都需要大量的计算资源和时间。</li>
<li><strong>单图像输入的敏感性：</strong> 模型的性能可能高度依赖于单张输入图像的质量、分辨率、视角和光照条件。极端或低质量的输入可能导致不佳的重建效果。</li>
<li><strong>生成结果的语义合理性：</strong> 尽管能够生成多样化的补全结果，但如何确保所有生成的被遮挡部分在语义上完全合理且物理上一致，对于复杂场景仍是一个挑战。</li>
<li><strong>泛化能力：</strong> 摘要未详细说明训练数据的范围，因此模型对训练集中未出现的新颖场景、物体类别或复杂交互的泛化能力可能存在局限。</li>
</ul>
<hr />
<p><strong>总结：</strong> 这篇论文通过将扩散模型与高斯泼溅结合，并巧妙地解决了3D真值数据稀缺的问题，在单图像3D重建领域迈出了重要一步。其生成式方法能够处理固有的歧义性，为未来的3D感知和内容生成开辟了新的可能性。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose a latent
diffusion model to reconstruct a complete 3D scene with Gaussian splats,
including the occluded parts, from only a single image during inference.</li>
<li>In contrast, we propose a generative formulation to learn a
distribution of 3D representations of Gaussian splats conditioned on a single
input image.</li>
<li>To address the lack of ground-truth training data, we propose a
Variational AutoReconstructor to learn a latent space only from 2D images in a
self-supervised manner, over which a diffusion model is trained.</li>
<li>Our method
generates faithful reconstructions and diverse samples with the ability to
complete the occluded surfaces for high-quality 360-degree renderings.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="http://arxiv.org/pdf/2508.21542v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2508.21542v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2508.21363v1'></a></p>
<h2 id="efficient-diffusion-based-3d-human-pose-estimation-with-hierarchical-temporal-pruning"><a href="https://arxiv.org/abs/2508.21363v1">Efficient Diffusion-Based 3D Human Pose Estimation with Hierarchical Temporal Pruning</a></h2>
<p><strong>Authors:</strong> Yuquan Bi, Hongsong Wang, Xinli Shi, Zhipeng Gui, Jie Gui, Yuan Yan Tang</p>
<p><strong>Published:</strong> 2025-08-29</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Diffusion models have demonstrated strong capabilities in generating
high-fidelity 3D human poses, yet their iterative nature and multi-hypothesis
requirements incur substantial computational cost. In this paper, we propose an
Efficient Diffusion-Based 3D Human Pose Estimation framework with a
Hierarchical Temporal Pruning (HTP) strategy, which dynamically prunes
redundant pose tokens across both frame and semantic levels while preserving
critical motion dynamics. HTP operates in a staged, top-down manner: (1)
Temporal Correlation-Enhanced Pruning (TCEP) identifies essential frames by
analyzing inter-frame motion correlations through adaptive temporal graph
construction; (2) Sparse-Focused Temporal MHSA (SFT MHSA) leverages the
resulting frame-level sparsity to reduce attention computation, focusing on
motion-relevant tokens; and (3) Mask-Guided Pose Token Pruner (MGPTP) performs
fine-grained semantic pruning via clustering, retaining only the most
informative pose tokens. Experiments on Human3.6M and MPI-INF-3DHP show that
HTP reduces training MACs by 38.5\%, inference MACs by 56.8\%, and improves
inference speed by an average of 81.1\% compared to prior diffusion-based
methods, while achieving state-of-the-art performance.</p>
<p><strong>Analysis:</strong></p>
<p>这篇论文的摘要提供了一个关于计算机视觉和机器学习领域中3D人体姿态估计的有趣进展。以下是根据摘要进行的分析：</p>
<hr />
<h3 id="1-concise-summary_2">1. 论文主要贡献的简明摘要 (Concise Summary)</h3>
<p>扩散模型在生成高保真3D人体姿态方面表现出色，但其迭代性质和多假设需求导致了巨大的计算成本。本文提出了一种高效的基于扩散的3D人体姿态估计框架，该框架采用分层时间剪枝（HTP）策略，动态地在帧和语义层面剪枝冗余姿态tokens，同时保留关键运动动态。HTP显著降低了计算资源消耗，并提升了推理速度，同时实现了最先进的性能。</p>
<h3 id="2-key-innovation-or-methodological-approach_2">2. 关键创新或方法论 (Key Innovation or Methodological Approach)</h3>
<p>该论文的核心创新在于其<strong>分层时间剪枝（Hierarchical Temporal Pruning, HTP）策略</strong>，旨在解决扩散模型在3D人体姿态估计中的计算效率问题。HTP方法论的独特性体现在其多阶段、自上而下的剪枝过程：</p>
<ol>
<li><strong>时间相关性增强剪枝 (Temporal Correlation-Enhanced Pruning, TCEP)</strong>：通过构建自适应时间图来分析帧间运动相关性，从而识别并剪枝冗余帧，保留关键帧。这是一种智能的帧级稀疏化方法，而非简单采样。</li>
<li><strong>稀疏聚焦时间多头自注意力 (Sparse-Focused Temporal MHSA, SFT MHSA)</strong>：利用TCEP产生的帧级稀疏性，优化多头自注意力机制的计算，使其只关注与运动相关的tokens，从而减少计算量。</li>
<li><strong>掩码引导姿态Token剪枝器 (Mask-Guided Pose Token Pruner, MGPTP)</strong>：在更细粒度的语义层面，通过聚类对姿态tokens进行剪枝，只保留最具信息量的tokens。这进一步精炼了姿态表示，去除了语义冗余。</li>
</ol>
<p>这种结合了帧级和语义级剪枝的层次化、动态方法，是其在扩散模型效率提升上的关键突破。</p>
<h3 id="3-potential-impact-on-the-field_2">3. 对领域潜在影响 (Potential Impact on the Field)</h3>
<p>这项研究对计算机视觉领域具有显著的潜在影响：</p>
<ul>
<li><strong>推动扩散模型的实际应用</strong>：通过大幅提高扩散模型的计算效率和推理速度，HTP使其在实时或资源受限的3D人体姿态估计应用中变得更加可行和实用。</li>
<li><strong>启发通用效率提升策略</strong>：HTP的分层剪枝思想，特别是结合运动动态和语义信息进行稀疏化的方法，可能为其他基于Transformer或扩散的序列生成模型（如视频生成、动作合成）提供通用的效率优化思路。</li>
<li><strong>加速研究与开发</strong>：更快的模型训练和推理速度将允许研究人员进行更多实验，加速新算法和应用场景的探索。</li>
<li><strong>提升用户体验</strong>：在AR/VR、人机交互、运动分析等领域，更高效的3D姿态估计意味着更流畅、响应更快的用户体验。</li>
</ul>
<h3 id="4-related-areas-or-applications_2">4. 相关领域或应用 (Related Areas or Applications)</h3>
<p>这项研究的成果可以惠及以下领域和应用：</p>
<ul>
<li><strong>实时3D人体姿态估计</strong>：如在AR/VR游戏、虚拟试穿、远程协作等场景中。</li>
<li><strong>人机交互 (HCI)</strong>：通过更准确、低延迟的姿态识别，实现更自然的用户界面和手势控制。</li>
<li><strong>机器人学</strong>：用于机器人模仿学习、人机协作中的人体姿态理解和预测。</li>
<li><strong>运动分析与生物力学</strong>：高效分析运动员动作、康复训练中的姿态评估。</li>
<li><strong>电影、动画与游戏</strong>：快速生成和编辑3D角色动画，降低制作成本。</li>
<li><strong>视频理解与动作识别</strong>：作为预处理步骤，提供高效的3D姿态特征。</li>
<li><strong>通用序列模型效率优化</strong>：其剪枝策略可能适用于其他需要处理长序列或高维token的Transformer或扩散模型。</li>
</ul>
<h3 id="5-limitations-inferred-from-the-abstract_1">5. 从摘要中可推断的局限性 (Limitations Inferred from the Abstract)</h3>
<p>尽管摘要强调了显著的效率提升和SOTA性能，但仍可推断出一些潜在局限性：</p>
<ul>
<li><strong>剪枝策略的复杂性</strong>：HTP包含三个阶段，涉及自适应图构建和聚类等操作，这可能增加了模型的实现和调优复杂性。这些额外步骤本身也可能引入一定的计算开销，尽管总体上是净收益。</li>
<li><strong>“关键运动动态”的定义与鲁棒性</strong>：摘要提到“保留关键运动动态”，但如何精确定义和保证在所有复杂运动场景下都能有效保留“关键”信息，而不会误剪枝掉细微但重要的动作细节，是一个潜在的挑战。</li>
<li><strong>超参数敏感性</strong>：自适应图的构建、聚类算法的选择和参数设置，以及各阶段剪枝比例等，都可能引入需要仔细调优的超参数，影响模型的泛化能力和鲁棒性。</li>
<li><strong>对特定数据集的依赖</strong>：实验结果在Human3.6M和MPI-INF-3DHP上表现出色，这些是标准数据集，但在更“野外”（in-the-wild）的复杂环境、遮挡、多样化服装和光照条件下，剪枝策略的有效性可能需要进一步验证。</li>
<li><strong>未提及其他挑战</strong>：摘要主要关注效率问题，并未提及对3D姿态估计中其他常见挑战（如严重遮挡、多视角鲁棒性、不同体型和服装的泛化能力）的直接改进。</li>
</ul>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this paper, we propose an
Efficient Diffusion-Based 3D Human Pose Estimation framework with a
Hierarchical Temporal Pruning (HTP) strategy, which dynamically prunes
redundant pose tokens across both frame and semantic levels while preserving
critical motion dynamics.</li>
<li>Experiments on Human3.6M and MPI-INF-3DHP show that
HTP reduces training MACs by 38.5\%, inference MACs by 56.8\%, and improves
inference speed by an average of 81.1\% compared to prior diffusion-based
methods, while achieving state-of-the-art performance.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="http://arxiv.org/pdf/2508.21363v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2508.21363v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2508.21824v1'></a></p>
<h2 id="driveqa-passing-the-driving-knowledge-test"><a href="https://arxiv.org/abs/2508.21824v1">DriveQA: Passing the Driving Knowledge Test</a></h2>
<p><strong>Authors:</strong> Maolin Wei, Wanzhou Liu, Eshed Ohn-Bar</p>
<p><strong>Published:</strong> 2025-08-29</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>If a Large Language Model (LLM) were to take a driving knowledge test today,
would it pass? Beyond standard spatial and visual question-answering (QA) tasks
on current autonomous driving benchmarks, driving knowledge tests require a
complete understanding of all traffic rules, signage, and right-of-way
principles. To pass this test, human drivers must discern various edge cases
that rarely appear in real-world datasets. In this work, we present DriveQA, an
extensive open-source text and vision-based benchmark that exhaustively covers
traffic regulations and scenarios. Through our experiments using DriveQA, we
show that (1) state-of-the-art LLMs and Multimodal LLMs (MLLMs) perform well on
basic traffic rules but exhibit significant weaknesses in numerical reasoning
and complex right-of-way scenarios, traffic sign variations, and spatial
layouts, (2) fine-tuning on DriveQA improves accuracy across multiple
categories, particularly in regulatory sign recognition and intersection
decision-making, (3) controlled variations in DriveQA-V provide insights into
model sensitivity to environmental factors such as lighting, perspective,
distance, and weather conditions, and (4) pretraining on DriveQA enhances
downstream driving task performance, leading to improved results on real-world
datasets such as nuScenes and BDD, while also demonstrating that models can
internalize text and synthetic traffic knowledge to generalize effectively
across downstream QA tasks.</p>
<p><strong>Analysis:</strong></p>
<p>作为计算机视觉和机器学习领域的专家，我对这篇名为“DriveQA: Passing the Driving Knowledge Test”的论文摘要进行如下分析：</p>
<hr />
<h3 id="1-2-3">1. 论文主要贡献的简明总结 (2-3句话)</h3>
<p>本文提出了DriveQA，一个全面的开源文本和视觉基准测试，旨在评估大型语言模型（LLMs）和多模态LLMs（MLLMs）对交通规则、标志和路权原则的完整理解。研究发现，现有模型在复杂场景和数值推理方面存在显著不足，但通过在DriveQA上进行微调和预训练，可以有效提升模型在驾驶知识测试和下游真实世界驾驶任务中的性能和泛化能力。</p>
<h3 id="2">2. 关键创新或方法论方法</h3>
<p>核心创新在于构建了DriveQA这一全面的、开源的文本与视觉基准测试。它超越了现有自动驾驶基准的局限，通过“穷尽式”地覆盖交通法规、场景和现实世界数据中罕见的“边缘案例”，旨在全面评估模型对驾驶知识的深层理解。此外，DriveQA-V通过引入受控的环境因素（如光照、视角、距离、天气）变化，为深入分析模型对这些因素的敏感性提供了独特的方法，这对于理解模型在不同条件下的鲁棒性至关重要。</p>
<h3 id="3">3. 对该领域的潜在影响</h3>
<p>该研究对计算机视觉和机器学习领域具有多方面潜在影响。首先，它为自动驾驶系统从纯粹的感知和预测迈向基于规则的“理解”和“推理”提供了关键工具和方向，有助于提升自动驾驶系统的安全性和可靠性。其次，DriveQA作为一个极具挑战性的基准，将推动LLMs和MLLMs在复杂推理、数值理解和多模态信息融合方面的能力边界。最后，通过展示合成数据和文本知识对真实世界任务的泛化能力，为未来高效训练和部署智能驾驶系统提供了新的范式，尤其是在难以获取大量真实世界边缘案例数据的情况下。</p>
<h3 id="4">4. 可能受益于这项研究的相关领域或应用</h3>
<ul>
<li><strong>自动驾驶系统开发与测试：</strong> 直接应用，用于训练和评估自动驾驶车辆的决策模块，特别是其对交通规则的理解和复杂场景下的推理能力。</li>
<li><strong>智能交通系统 (ITS)：</strong> 辅助交通流管理、事故预防和智能信号控制，通过更智能地理解交通规则来优化系统。</li>
<li><strong>机器人学：</strong> 任何需要在复杂、规则驱动环境中操作的机器人系统，例如物流机器人或服务机器人，可以借鉴其规则理解和决策框架。</li>
<li><strong>AI安全与可解释性 (XAI)：</strong> 深入理解模型如何基于交通规则做出决策，提高决策的透明度和可信度，这在安全关键领域尤为重要。</li>
<li><strong>驾驶员培训与教育：</strong> 作为辅助工具，帮助新驾驶员理解复杂的交通规则和边缘情况，甚至可以用于开发智能驾驶模拟器。</li>
<li><strong>多模态LLMs研究：</strong> 为提升LLMs和MLLMs在视觉理解、常识推理和规则遵循方面的能力提供新的研究方向和评估标准。</li>
</ul>
<h3 id="5">5. 从摘要中可推断出的局限性</h3>
<ul>
<li><strong>合成数据与真实世界的差距：</strong> 尽管摘要指出模型可以内化文本和合成交通知识并泛化到真实世界数据集，但DriveQA本身是基于“合成交通知识”构建的。合成数据在覆盖边缘案例方面有优势，但其与真实世界数据的分布差异、噪声和不可预测性可能仍是模型泛化能力的潜在限制。</li>
<li><strong>侧重知识理解而非实时决策与控制：</strong> DriveQA主要是一个“驾驶知识测试”，评估模型对规则的理解和推理能力。它不直接评估模型在实时、动态、高压的真实驾驶环境中的感知、预测和控制能力，这需要更复杂的端到端系统。</li>
<li><strong>特定挑战的持续存在：</strong> 摘要明确指出，即使是SOTA模型，在“数值推理、复杂路权场景、交通标志变体和空间布局”方面仍存在显著弱点。这表明这些是极其困难的问题，DriveQA虽然提供了评估工具，但解决这些深层挑战仍需进一步的研究。</li>
<li><strong>“穷尽性”的范围：</strong> 尽管声称“穷尽式地覆盖交通法规和场景”，但现实世界的交通规则和边缘案例是极其庞大和复杂的（例如，不同国家/地区的具体法规差异、各种非标准情况）。摘要并未详细说明其覆盖的广度和深度，这可能是一个潜在的局限。</li>
</ul>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this work, we present DriveQA, an
extensive open-source text and vision-based benchmark that exhaustively covers
traffic regulations and scenarios.</li>
<li>Through our experiments using DriveQA, we
show that (1) state-of-the-art LLMs and Multimodal LLMs (MLLMs) perform well on
basic traffic rules but exhibit significant weaknesses in numerical reasoning
and complex right-of-way scenarios, traffic sign variations, and spatial
layouts, (2) fine-tuning on DriveQA improves accuracy across multiple
categories, particularly in regulatory sign recognition and intersection
decision-making, (3) controlled variations in DriveQA-V provide insights into
model sensitivity to environmental factors such as lighting, perspective,
distance, and weather conditions, and (4) pretraining on DriveQA enhances
downstream driving task performance, leading to improved results on real-world
datasets such as nuScenes and BDD, while also demonstrating that models can
internalize text and synthetic traffic knowledge to generalize effectively
across downstream QA tasks.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="http://arxiv.org/pdf/2508.21824v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2508.21824v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2508.21769v1'></a></p>
<h2 id="domain-generalization-in-the-wild-disentangling-classification-from-domain-aware-representations"><a href="https://arxiv.org/abs/2508.21769v1">Domain Generalization in-the-Wild: Disentangling Classification from Domain-Aware Representations</a></h2>
<p><strong>Authors:</strong> Ha Min Son, Zhe Zhao, Shahbaz Rezaei, Xin Liu</p>
<p><strong>Published:</strong> 2025-08-29</p>
<p><strong>Categories:</strong> cs.CV, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Evaluating domain generalization (DG) for foundational models like CLIP is
challenging, as web-scale pretraining data potentially covers many existing
benchmarks. Consequently, current DG evaluation may neither be sufficiently
challenging nor adequately test genuinely unseen data scenarios. To better
assess the performance of CLIP on DG in-the-wild, a scenario where CLIP
encounters challenging unseen data, we consider two approaches: (1) evaluating
on 33 diverse datasets with quantified out-of-distribution (OOD) scores after
fine-tuning CLIP on ImageNet, and (2) using unlearning to make CLIP `forget'
some domains as an approximation. We observe that CLIP's performance
deteriorates significantly on more OOD datasets. To address this, we present
CLIP-DCA (Disentangling Classification from enhanced domain Aware
representations). Our approach is motivated by the observation that while
standard domain invariance losses aim to make representations domain-invariant,
this can be harmful to foundation models by forcing the discarding of
domain-aware representations beneficial for generalization. We instead
hypothesize that enhancing domain awareness is a prerequisite for effective
domain-invariant classification in foundation models. CLIP-DCA identifies and
enhances domain awareness within CLIP's encoders using a separate domain head
and synthetically generated diverse domain data. Simultaneously, it encourages
domain-invariant classification through disentanglement from the domain
features. CLIP-DCA shows significant improvements within this challenging
evaluation compared to existing methods, particularly on datasets that are more
OOD.</p>
<p><strong>Analysis:</strong></p>
<p>作为计算机视觉和机器学习领域的专家，我对这篇论文摘要的分析如下：</p>
<hr />
<h3 id="domain-generalization-in-the-wild-disentangling-classification-from-domain-aware-representations_1">论文摘要分析：Domain Generalization in-the-Wild: Disentangling Classification from Domain-Aware Representations</h3>
<p><strong>1. 论文主要贡献 (Main Contribution):</strong>
本文针对基础模型（如CLIP）在真实世界域泛化（DG in-the-wild）场景下的评估挑战，提出了一套新的评估范式，并发现CLIP在此类高度域外（OOD）数据上性能显著下降。为解决此问题，论文引入了CLIP-DCA方法，通过增强域感知表示并将其与分类任务解耦，显著提升了基础模型在复杂域泛化任务中的性能。</p>
<p><strong>2. 关键创新或方法 (Key Innovation or Methodological Approach):</strong>
该论文的核心创新在于其对基础模型域泛化学习范式的深刻转变。与传统域不变性学习方法直接强制表示域不变性（可能导致有用域信息的丢失）不同，CLIP-DCA提出并验证了“增强域感知能力是实现有效域不变分类的先决条件”这一假设。其方法论通过引入一个独立的域头和利用合成域数据来主动识别并增强CLIP编码器中的域感知表示，同时巧妙地通过解耦机制确保最终的分类任务能够从这些增强的域特征中解耦出来，从而实现更鲁棒的域不变分类。</p>
<p><strong>3. 对领域潜在影响 (Potential Impact on the Field):</strong>
该研究有望对计算机视觉和机器学习领域产生多方面影响。首先，它提出了更严谨和具挑战性的基础模型域泛化评估范式，促使领域重新思考现有基准的有效性。其次，CLIP-DCA的核心思想——即先增强域感知再解耦分类——为未来设计针对大型预训练模型（如CLIP）的域泛化算法开辟了新路径，挑战了传统域不变性学习的假设。这有望显著提升基础模型在真实世界、高度域外（OOD）场景下的鲁棒性和泛化能力，使其在实际部署中更可靠。</p>
<p><strong>4. 相关领域或应用 (Related Areas or Applications that Might Benefit from this Research):</strong>
本研究的成果将对广泛的实际应用领域产生积极影响，特别是那些对模型在未见数据上泛化能力有严格要求的场景：
*   <strong>医疗影像诊断：</strong> 不同医院、设备或患者群体的数据分布差异巨大，DG能力至关重要。
*   <strong>自动驾驶：</strong> 面对多变的天气、光照、地理环境等，模型需具备强大的域泛化能力。
*   <strong>机器人视觉：</strong> 机器人需要在各种未知的室内外环境中执行任务。
*   <strong>遥感图像分析：</strong> 不同地理区域、传感器或时间点的数据分布差异。
*   <strong>工业缺陷检测：</strong> 生产线环境变化可能导致数据分布偏移。
此外，对于任何需要将大型预训练模型部署到与训练数据分布存在显著差异的真实世界环境中的应用，本研究都提供了宝贵的指导和解决方案。</p>
<p><strong>5. 潜在局限性 (Limitations that Can Be Inferred from the Abstract):</strong>
尽管摘要展示了令人鼓舞的结果，但仍可推断出一些潜在局限性：
*   <strong>合成域数据的有效性与泛化性：</strong> CLIP-DCA依赖于“合成生成多样域数据”来增强域感知。合成数据能否充分捕捉真实世界中复杂且未知的域偏移模式，以及其质量和多样性对最终泛化性能的影响，是一个关键问题。过于简单的合成可能无法应对极端OOD场景。
*   <strong>“遗忘”机制作为近似的局限性：</strong> 论文使用“遗忘”来近似模拟模型从未见过某些域的场景。这种近似方法与真正意义上的“从未见过”之间可能存在差异，其有效性和准确性需要更深入的验证。
*   <strong>评估场景的代表性：</strong> 尽管使用了33个数据集和OOD分数，但“in-the-wild”是一个极其宽泛的概念。这些评估是否能完全代表所有真实世界中可能遇到的、具有挑战性的未见域场景，仍有待商榷。
*   <strong>方法复杂性与计算成本：</strong> 引入独立的域头和解耦机制可能会增加模型的参数量、计算复杂度和训练时间，这在资源受限的场景下可能是一个考量因素。
*   <strong>对基础模型的通用性：</strong> 论文主要关注CLIP。CLIP-DCA的有效性是否能直接泛化到其他类型的基础模型（例如，纯视觉Transformer或不同模态的预训练模型）仍需进一步探索。</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this, we present
CLIP-DCA (Disentangling Classification from enhanced domain Aware
representations).</li>
<li>Our approach is motivated by the observation that while
standard domain invariance losses aim to make representations domain-invariant,
this can be harmful to foundation models by forcing the discarding of
domain-aware representations beneficial for generalization.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="http://arxiv.org/pdf/2508.21769v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2508.21769v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2508.21767v1'></a></p>
<h2 id="uitron-foundational-gui-agent-with-advanced-perception-and-planning"><a href="https://arxiv.org/abs/2508.21767v1">UItron: Foundational GUI Agent with Advanced Perception and Planning</a></h2>
<p><strong>Authors:</strong> Zhixiong Zeng, Jing Huang, Liming Zheng, Wenkang Han, Yufeng Zhong, Lei Chen, Longrong Yang, Yingjie Chu, Yuzhi He, Lin Ma</p>
<p><strong>Published:</strong> 2025-08-29</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>GUI agent aims to enable automated operations on Mobile/PC devices, which is
an important task toward achieving artificial general intelligence. The rapid
advancement of VLMs accelerates the development of GUI agents, owing to their
powerful capabilities in visual understanding and task planning. However,
building a GUI agent remains a challenging task due to the scarcity of
operation trajectories, the availability of interactive infrastructure, and the
limitation of initial capabilities in foundation models. In this work, we
introduce UItron, an open-source foundational model for automatic GUI agents,
featuring advanced GUI perception, grounding, and planning capabilities. UItron
highlights the necessity of systemic data engineering and interactive
infrastructure as foundational components for advancing GUI agent development.
It not only systematically studies a series of data engineering strategies to
enhance training effects, but also establishes an interactive environment
connecting both Mobile and PC devices. In training, UItron adopts supervised
finetuning over perception and planning tasks in various GUI scenarios, and
then develop a curriculum reinforcement learning framework to enable complex
reasoning and exploration for online environments. As a result, UItron achieves
superior performance in benchmarks of GUI perception, grounding, and planning.
In particular, UItron highlights the interaction proficiency with top-tier
Chinese mobile APPs, as we identified a general lack of Chinese capabilities
even in state-of-the-art solutions. To this end, we manually collect over one
million steps of operation trajectories across the top 100 most popular apps,
and build the offline and online agent evaluation environments. Experimental
results demonstrate that UItron achieves significant progress in Chinese app
scenarios, propelling GUI agents one step closer to real-world application.</p>
<p><strong>Analysis:</strong></p>
<p>UItron这篇论文摘要展示了在计算机视觉和机器学习领域，特别是GUI（图形用户界面）自动化代理方面的重要进展。以下是详细分析：</p>
<hr />
<h3 id="1-2-3_1">1. 论文主要贡献的简洁总结 (2-3句话)</h3>
<p>UItron是一个开源的、基础性的GUI自动化代理模型，它通过先进的GUI感知、定位和规划能力，旨在实现移动和PC设备的自动化操作。该工作强调了系统性数据工程和交互式基础设施的重要性，并结合监督微调与课程强化学习，显著提升了GUI代理在复杂在线环境中的表现。特别值得一提的是，UItron通过大规模手动收集的中文应用操作轨迹数据，填补了现有解决方案在中文应用场景下的能力空白，推动了GUI代理向实际应用迈进。</p>
<h3 id="2_1">2. 关键创新或方法论</h3>
<p>UItron的关键创新和方法论体现在以下几个方面：</p>
<ul>
<li><strong>系统性数据工程与交互式基础设施：</strong> 论文强调并实践了系统性的数据工程策略来增强训练效果，并建立了一个连接移动和PC设备的统一交互环境。这为GUI代理的开发提供了坚实的基础，解决了数据稀缺和基础设施不足的挑战。</li>
<li><strong>混合训练范式：</strong> UItron采用了分阶段的训练策略。首先，通过在各种GUI场景下的感知和规划任务进行监督微调（SFT），为模型奠定初始能力。随后，引入课程强化学习（Curriculum Reinforcement Learning）框架，使模型能够在在线环境中进行复杂的推理和探索，从而处理更动态和开放的任务。</li>
<li><strong>大规模中文应用数据集与能力提升：</strong> 针对现有SOTA解决方案普遍缺乏中文应用能力的问题，UItron手动收集了超过一百万步的、涵盖前100个最受欢迎中文应用的真实操作轨迹数据。这不仅构建了离线和在线的评估环境，也使得UItron在中文应用场景中取得了显著的性能提升，填补了一个重要的市场和研究空白。</li>
<li><strong>开源基础模型：</strong> 作为“开源基础模型”，UItron旨在降低研究门槛，促进整个GUI代理领域的发展。</li>
</ul>
<h3 id="3_1">3. 对领域潜在影响</h3>
<ul>
<li><strong>加速AGI发展：</strong> GUI代理被认为是实现通用人工智能（AGI）的重要一步。UItron的进展，特别是其在复杂环境中的规划和探索能力，将直接推动AGI研究。</li>
<li><strong>提升GUI自动化水平：</strong> UItron的先进感知、定位和规划能力，以及其在中文应用上的突破，将显著提升现有GUI自动化工具的性能和适用范围，使其能处理更复杂、更真实的任务。</li>
<li><strong>推动多语言/多文化GUI研究：</strong> UItron对中文应用能力的强调和实现，将激励研究者关注其他非英语语言和文化背景下的GUI代理开发，促进更具普适性的解决方案。</li>
<li><strong>提供研究基石：</strong> 作为开源的基础模型，UItron将为后续研究提供一个强大的基线和研究平台，加速新算法、新架构的验证和迭代。</li>
<li><strong>促进数据工程和交互环境的重视：</strong> 论文明确指出数据工程和交互基础设施是基础组件，这将促使更多研究者和开发者投入资源来构建高质量的数据集和仿真环境。</li>
</ul>
<h3 id="4_1">4. 可能受益的相关领域或应用</h3>
<ul>
<li><strong>软件测试与质量保证 (QA)：</strong> 自动化UI测试，尤其是在移动应用和多平台场景下，可以大幅提高效率和覆盖率。</li>
<li><strong>机器人流程自动化 (RPA)：</strong> 在企业级应用中，UItron可以自动化执行复杂的跨应用、跨设备业务流程，提高工作效率。</li>
<li><strong>辅助技术与无障碍访问：</strong> 为残障人士提供更智能、更自主的界面操作辅助，提升数字生活的便利性。</li>
<li><strong>个人AI助手：</strong> 赋能AI助手执行更复杂的、涉及多步操作和跨应用的指令，例如“帮我预订一张从上海到北京的机票”。</li>
<li><strong>数据采集与网络爬虫：</strong> 更智能地从动态和复杂的GUI界面中提取信息。</li>
<li><strong>跨设备计算：</strong> 实现任务在手机和PC之间无缝切换和自动化执行。</li>
<li><strong>教育与培训：</strong> 自动化演示软件操作流程，用于教学或用户培训。</li>
</ul>
<h3 id="5_1">5. 从摘要中可推断的局限性</h3>
<ul>
<li><strong>数据收集成本与可扩展性：</strong> 摘要中提到“手动收集超过一百万步的操作轨迹”，这表明数据收集是一个劳动密集型且成本高昂的过程。虽然对于特定领域（如中文应用）取得了成功，但要将其扩展到全球所有语言、所有应用或更长尾的应用场景，其可扩展性可能面临挑战。</li>
<li><strong>泛化能力：</strong> 尽管在“前100个最受欢迎的中文应用”上表现出色，但对于不常见、设计风格迥异或更新迭代频繁的应用，其泛化能力仍需进一步验证。</li>
<li><strong>强化学习的挑战：</strong> 课程强化学习虽然强大，但通常对环境的建模、奖励函数的设计以及训练的稳定性有较高要求。在真实、开放的GUI环境中，如何有效处理探索-利用困境、稀疏奖励以及长序列决策，仍是RL面临的固有挑战。</li>
<li><strong>“基础模型”的定义与范围：</strong> 作为一个“基础模型”，其通用性（即在不进行额外微调的情况下处理全新任务的能力）以及对高度抽象或需要深层语义理解的任务的处理能力，在摘要中未详细说明。</li>
<li><strong>对特定语言/文化UI的依赖：</strong> 虽然解决了中文应用的痛点，但这可能意味着模型在一定程度上学习了中文UI的特定模式和习惯。将其直接应用于其他语言或文化背景下UI（例如日文、阿拉伯文UI）时，可能需要类似的定制化数据和训练。</li>
</ul>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In particular, UItron highlights the interaction proficiency with top-tier
Chinese mobile APPs, as we identified a general lack of Chinese capabilities
even in state-of-the-art solutions.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="http://arxiv.org/pdf/2508.21767v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2508.21767v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2508.21732v1'></a></p>
<h2 id="cad2dmd-set-synthetic-generation-tool-of-digital-measurement-device-cad-model-datasets-for-fine-tuning-large-vision-language-models"><a href="https://arxiv.org/abs/2508.21732v1">CAD2DMD-SET: Synthetic Generation Tool of Digital Measurement Device CAD Model Datasets for fine-tuning Large Vision-Language Models</a></h2>
<p><strong>Authors:</strong> João Valente, Atabak Dehban, Rodrigo Ventura</p>
<p><strong>Published:</strong> 2025-08-29</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>Recent advancements in Large Vision-Language Models (LVLMs) have demonstrated
impressive capabilities across various multimodal tasks. They continue,
however, to struggle with trivial scenarios such as reading values from Digital
Measurement Devices (DMDs), particularly in real-world conditions involving
clutter, occlusions, extreme viewpoints, and motion blur; common in
head-mounted cameras and Augmented Reality (AR) applications. Motivated by
these limitations, this work introduces CAD2DMD-SET, a synthetic data
generation tool designed to support visual question answering (VQA) tasks
involving DMDs. By leveraging 3D CAD models, advanced rendering, and
high-fidelity image composition, our tool produces diverse, VQA-labelled
synthetic DMD datasets suitable for fine-tuning LVLMs. Additionally, we present
DMDBench, a curated validation set of 1,000 annotated real-world images
designed to evaluate model performance under practical constraints.
Benchmarking three state-of-the-art LVLMs using Average Normalised Levenshtein
Similarity (ANLS) and further fine-tuning LoRA's of these models with
CAD2DMD-SET's generated dataset yielded substantial improvements, with InternVL
showcasing a score increase of 200% without degrading on other tasks. This
demonstrates that the CAD2DMD-SET training dataset substantially improves the
robustness and performance of LVLMs when operating under the previously stated
challenging conditions. The CAD2DMD-SET tool is expected to be released as
open-source once the final version of this manuscript is prepared, allowing the
community to add different measurement devices and generate their own datasets.</p>
<p><strong>Analysis:</strong></p>
<p>这篇论文摘要展示了计算机视觉和机器学习领域的一个重要进展，特别是在大型视觉-语言模型（LVLMs）的实际应用和数据生成方面。</p>
<hr />
<h3 id="1-concise-summary_3">1. 论文主要贡献的简洁总结 (Concise Summary)</h3>
<p>本文提出了CAD2DMD-SET，一个创新的合成数据生成工具，旨在解决大型视觉-语言模型（LVLMs）在复杂真实世界条件下读取数字测量设备（DMDs）数值的难题。通过利用3D CAD模型、高级渲染和高保真图像合成技术，该工具能生成多样化且带有VQA标注的合成DMD数据集，并结合DMDBench这一真实的验证集。实验证明，使用CAD2DMD-SET生成的数据集对LVLMs进行微调，能显著提升其在DMD读取任务上的鲁棒性和性能，且不影响其通用能力。</p>
<h3 id="2-key-innovation-or-methodological-approach_3">2. 关键创新或方法学方法 (Key Innovation or Methodological Approach)</h3>
<p>核心创新在于其<strong>合成数据生成范式</strong>，特别是针对数字测量设备（DMDs）的视觉问答（VQA）任务。该方法利用<strong>3D CAD模型</strong>作为基础，结合<strong>高级渲染技术和高保真图像合成</strong>，能够系统性地生成包含复杂真实世界条件（如杂乱、遮挡、极端视角和运动模糊）的<strong>多样化、VQA标注的合成数据集</strong>。此外，引入<strong>DMDBench</strong>作为独立的真实世界验证集，为评估模型在实际约束下的性能提供了可靠基准，形成了一个完整的训练与评估闭环。这种将高保真合成数据与真实世界验证相结合的策略，有效地弥补了“模拟到现实”（sim-to-real）的鸿沟。</p>
<h3 id="3-potential-impact-on-the-field_3">3. 对领域潜在影响 (Potential Impact on the Field)</h3>
<ul>
<li><strong>提升LVLM在特定领域应用的鲁棒性与准确性：</strong> 本文证明了通过高质量合成数据对LVLMs进行微调，可以显著提升其在特定、复杂真实世界场景下的性能，且不影响其通用能力，为LVLM在工业、AR等领域的落地提供了有效途径。</li>
<li><strong>解决数据稀缺问题：</strong> 对于难以获取大量真实世界标注数据的特定视觉任务，CAD2DMD-SET提供了一种高效、可扩展的合成数据生成解决方案，降低了模型开发和部署的成本。</li>
<li><strong>推动合成数据研究：</strong> 强调了高保真渲染和智能合成在弥合“模拟到现实”鸿沟中的关键作用，为未来更多利用合成数据训练视觉模型的研究提供了范例。</li>
<li><strong>社区贡献：</strong> 计划的开源发布将促进社区在此基础上进行扩展，支持更多测量设备和应用场景，加速相关技术的发展。</li>
</ul>
<h3 id="4-related-areas-or-applications_3">4. 可能受益的相关领域或应用 (Related Areas or Applications)</h3>
<ul>
<li><strong>工业自动化与质量控制：</strong> 在工厂环境中，自动读取各种仪表、传感器和显示屏的数值，用于过程监控、故障诊断和产品质量检测。</li>
<li><strong>增强现实（AR）与混合现实（MR）：</strong> 结合头戴式设备，实现对物理世界中数字测量设备的实时识别和数值提取，为用户提供上下文信息或操作指导。</li>
<li><strong>机器人技术：</strong> 赋予机器人识别和理解环境中各种数字显示器的能力，例如读取设备状态、完成特定任务。</li>
<li><strong>设备维护与检测：</strong> 帮助技术人员通过视觉系统快速准确地获取设备读数，提高维护效率和准确性。</li>
<li><strong>智能穿戴设备：</strong> 提升智能眼镜等设备在复杂环境中理解和交互物理世界信息的能力。</li>
</ul>
<h3 id="5-limitations-inferable-from-the-abstract">5. 从摘要中可推断的局限性 (Limitations Inferable from the Abstract)</h3>
<ul>
<li><strong>领域特异性：</strong> CAD2DMD-SET工具是专门为数字测量设备（DMDs）设计的。将其扩展到其他类型的视觉任务或物体（例如，模拟仪表、更复杂的工业设备）将需要新的3D CAD模型和可能不同的渲染及合成策略，这并非一个通用解决方案。</li>
<li><strong>CAD模型依赖性：</strong> 合成数据的质量和多样性高度依赖于可用3D CAD模型的准确性和丰富性。获取高质量的CAD模型本身可能是一个挑战或成本来源。</li>
<li><strong>潜在的“模拟到现实”鸿沟：</strong> 尽管论文强调了“高保真图像合成”，但合成数据与真实世界数据之间仍可能存在细微的分布差异（即残余的“模拟到现实”鸿沟），这可能在某些未被DMDBench充分覆盖的真实世界场景中表现出来。DMDBench虽然提供了真实世界验证，但其1000张图像的规模可能不足以完全代表所有极端真实世界条件。</li>
<li><strong>计算资源需求：</strong> 生成高保真、多样化的合成数据集，特别是涉及高级渲染和复杂场景合成时，可能需要大量的计算资源和时间。</li>
</ul>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Motivated by
these limitations, this work introduces CAD2DMD-SET, a synthetic data
generation tool designed to support visual question answering (VQA) tasks
involving DMDs. By leveraging 3D CAD models, advanced rendering, and
high-fidelity image composition, our tool produces diverse, VQA-labelled
synthetic DMD datasets suitable for fine-tuning LVLMs. Additionally, we present
DMDBench, a curated validation set of 1,000 annotated real-world images
designed to evaluate model performance under practical constraints.</li>
<li>Benchmarking three state-of-the-art LVLMs using Average Normalised Levenshtein
Similarity (ANLS) and further fine-tuning LoRA's of these models with
CAD2DMD-SET's generated dataset yielded substantial improvements, with InternVL
showcasing a score increase of 200% without degrading on other tasks.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="http://arxiv.org/pdf/2508.21732v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2508.21732v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2508.21689v1'></a></p>
<h2 id="mapping-like-a-skeptic-probabilistic-bev-projection-for-online-hd-mapping"><a href="https://arxiv.org/abs/2508.21689v1">Mapping like a Skeptic: Probabilistic BEV Projection for Online HD Mapping</a></h2>
<p><strong>Authors:</strong> Fatih Erdoğan, Merve Rabia Barın, Fatma Güney</p>
<p><strong>Published:</strong> 2025-08-29</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Constructing high-definition (HD) maps from sensory input requires accurately
mapping the road elements in image space to the Bird's Eye View (BEV) space.
The precision of this mapping directly impacts the quality of the final
vectorized HD map. Existing HD mapping approaches outsource the projection to
standard mapping techniques, such as attention-based ones. However, these
methods struggle with accuracy due to generalization problems, often
hallucinating non-existent road elements. Our key idea is to start with a
geometric mapping based on camera parameters and adapt it to the scene to
extract relevant map information from camera images. To implement this, we
propose a novel probabilistic projection mechanism with confidence scores to
(i) refine the mapping to better align with the scene and (ii) filter out
irrelevant elements that should not influence HD map generation. In addition,
we improve temporal processing by using confidence scores to selectively
accumulate reliable information over time. Experiments on new splits of the
nuScenes and Argoverse2 datasets demonstrate improved performance over
state-of-the-art approaches, indicating better generalization. The improvements
are particularly pronounced on nuScenes and in the challenging long perception
range. Our code and model checkpoints are available at
https://github.com/Fatih-Erdogan/mapping-like-skeptic .</p>
<p><strong>Analysis:</strong></p>
<p>这篇论文的摘要提供了一个关于在线高清地图构建中鸟瞰图（BEV）投影的关键改进。以下是详细分析：</p>
<hr />
<h3 id="1-a-concise-summary-of-the-papers-main-contribution">1. 论文主要贡献的简洁总结 (A concise summary of the paper's main contribution)</h3>
<p>该论文提出了一种新颖的概率性投影机制，用于将图像空间中的道路元素准确映射到鸟瞰图（BEV）空间，以构建在线高清地图。该方法通过结合基于相机参数的几何映射与场景自适应的置信度分数，解决了现有方法（如基于注意力机制）在泛化性和避免幻觉（hallucination）方面的不足。它通过置信度分数来优化映射、过滤不相关元素，并选择性地累积可靠的长期信息，从而显著提高了高清地图的精度和泛化能力。</p>
<h3 id="2-the-key-innovation-or-methodological-approach">2. 关键创新或方法学方法 (The key innovation or methodological approach)</h3>
<p>该论文的核心创新在于其<strong>“怀疑论者式”的概率性投影机制</strong>，它与传统纯粹依赖学习或注意力机制的方法形成对比：</p>
<ul>
<li><strong>几何先验与场景自适应结合：</strong> 不同于完全依赖数据驱动的学习方法，该方法首先利用基于相机参数的几何映射作为起点，然后通过学习机制使其适应特定场景。这提供了一个更稳健的初始基础，减少了模型从零开始学习投影的难度。</li>
<li><strong>置信度分数驱动的精炼与过滤：</strong> 引入了置信度分数来：<ul>
<li><strong>精炼映射：</strong> 使投影更好地与实际场景对齐。</li>
<li><strong>过滤无关元素：</strong> 明确地识别并剔除那些可能由模型“幻觉”出来的、不存在的道路元素，从而提高地图的准确性和可靠性。</li>
</ul>
</li>
<li><strong>时间维度上的选择性累积：</strong> 利用置信度分数在时间序列上选择性地累积可靠信息，增强了在线地图构建的鲁棒性和一致性，避免了不可靠信息的累积。</li>
</ul>
<h3 id="3-potential-impact-on-the-field_4">3. 对领域的潜在影响 (Potential impact on the field)</h3>
<ul>
<li><strong>提高高清地图的可靠性与安全性：</strong> 通过减少幻觉和提高泛化能力，该方法能够生成更准确、更可靠的高清地图，这对自动驾驶系统的安全运行至关重要。</li>
<li><strong>推动混合式BEV投影方法的发展：</strong> 几何先验与学习机制的结合可能启发更多混合式方法，在数据驱动的灵活性和几何约束的鲁棒性之间找到更好的平衡。</li>
<li><strong>解决长尾问题和泛化挑战：</strong> 现有方法在复杂或不常见场景下的泛化性差是一个普遍问题。该论文的改进，尤其是在nuScenes和长感知距离上的表现，表明它能更好地应对这些挑战。</li>
<li><strong>促进在线地图构建的实用化：</strong> 强调在线处理和时间信息累积，使其更适用于实际自动驾驶车辆的实时地图构建需求。</li>
</ul>
<h3 id="4-related-areas-or-applications-that-might-benefit-from-this-research">4. 可能受益于这项研究的相关领域或应用 (Related areas or applications that might benefit from this research)</h3>
<ul>
<li><strong>自动驾驶：</strong> 这是最直接的应用领域，高清地图是自动驾驶感知、定位和规划的核心组成部分。</li>
<li><strong>机器人导航与SLAM：</strong> 任何需要从传感器输入构建环境地图的机器人系统，都可以从更准确、更可靠的BEV投影和地图构建中受益。</li>
<li><strong>3D场景重建：</strong> 从2D图像中准确推断3D结构和布局，尤其是在结构化环境中（如道路），该方法可以提供有价值的参考。</li>
<li><strong>城市规划与测绘：</strong> 自动化地从图像数据生成高精度地图，可以提高城市基础设施管理和测绘的效率。</li>
<li><strong>智能交通系统：</strong> 实时、高精度的道路元素信息对于交通流管理、事件检测等应用具有重要意义。</li>
</ul>
<h3 id="5-any-limitations-that-can-be-inferred-from-the-abstract">5. 从摘要中可推断出的局限性 (Any limitations that can be inferred from the abstract)</h3>
<ul>
<li><strong>对相机参数的依赖：</strong> 摘要中提到“基于相机参数的几何映射”，这意味着该方法可能对准确的相机标定有一定依赖。如果相机标定不准确，初始的几何投影可能会引入误差，尽管后续的概率机制会尝试纠正。</li>
<li><strong>置信度分数的鲁棒性：</strong> 关键在于如何准确地生成和利用置信度分数。如果置信度分数本身在某些极端或模糊场景下不可靠，可能会影响整体性能。</li>
<li><strong>“场景自适应”的范围：</strong> 摘要提到“适应场景以提取相关地图信息”，但未详细说明这种适应性在面对高度动态、复杂或完全新颖的场景时的鲁棒性如何。</li>
<li><strong>计算开销：</strong> 引入概率性机制和置信度分数，以及时间上的信息累积，可能会增加计算复杂性，这对于在线（实时）应用来说是一个需要关注的问题。</li>
<li><strong>数据依赖性：</strong> 尽管声称泛化性更好，但实验仍在nuScenes和Argoverse2数据集的新分割上进行。在更广泛、更多样化的真实世界场景中的表现仍需进一步验证。</li>
</ul>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To implement this, we
propose a novel probabilistic projection mechanism with confidence scores to
(i) refine the mapping to better align with the scene and (ii) filter out
irrelevant elements that should not influence HD map generation.</li>
<li>Experiments on new splits of the
nuScenes and Argoverse2 datasets demonstrate improved performance over
state-of-the-art approaches, indicating better generalization.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="http://arxiv.org/pdf/2508.21689v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2508.21689v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-09-01 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
