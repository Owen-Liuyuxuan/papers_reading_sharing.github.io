<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-10-30 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DEâ«¶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Birdâs-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">è®ºæéè¯»</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-Î±
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-10-29/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-10-31/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-10-30">Arxiv Computer Vision Papers - 2025-10-30</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#multimodal-spatial-reasoning-in-the-large-model-era-a-survey-and-benchmarks" class="nav-link">Multimodal Spatial Reasoning in the Large Model Era: A Survey and Benchmarks</a>
                </li>
                <li class="nav-item">
                    <a href="#ming-flash-omni-a-sparse-unified-architecture-for-multimodal-perception-and-generation" class="nav-link">Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#vfxmaster-unlocking-dynamic-visual-effect-generation-via-in-context-learning" class="nav-link">VFXMaster: Unlocking Dynamic Visual Effect Generation via In-Context Learning</a>
                </li>
                <li class="nav-item">
                    <a href="#instance-level-composed-image-retrieval" class="nav-link">Instance-Level Composed Image Retrieval</a>
                </li>
                <li class="nav-item">
                    <a href="#streamingcot-a-dataset-for-temporal-dynamics-and-multimodal-chain-of-thought-reasoning-in-streaming-videoqa" class="nav-link">StreamingCoT: A Dataset for Temporal Dynamics and Multimodal Chain-of-Thought Reasoning in Streaming VideoQA</a>
                </li>
                <li class="nav-item">
                    <a href="#seeing-clearly-and-deeply-an-rgbd-imaging-approach-with-a-bio-inspired-monocentric-design" class="nav-link">Seeing Clearly and Deeply: An RGBD Imaging Approach with a Bio-inspired Monocentric Design</a>
                </li>
                <li class="nav-item">
                    <a href="#synhlmasynthesizing-hand-language-manipulation-for-articulated-object-with-discrete-human-object-interaction-representation" class="nav-link">SynHLMA:Synthesizing Hand Language Manipulation for Articulated Object with Discrete Human Object Interaction Representation</a>
                </li>
                <li class="nav-item">
                    <a href="#langhops-language-grounded-hierarchical-open-vocabulary-part-segmentation" class="nav-link">LangHOPS: Language Grounded Hierarchical Open-Vocabulary Part Segmentation</a>
                </li>
                <li class="nav-item">
                    <a href="#test-time-adaptive-object-detection-with-foundation-model" class="nav-link">Test-Time Adaptive Object Detection with Foundation Model</a>
                </li>
                <li class="nav-item">
                    <a href="#visual-diversity-and-region-aware-prompt-learning-for-zero-shot-hoi-detection" class="nav-link">Visual Diversity and Region-aware Prompt Learning for Zero-shot HOI Detection</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-10-30">Arxiv Computer Vision Papers - 2025-10-30</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>å¥½çï¼è¿æ¯ä¸ä»½éå¯¹2025å¹´10æ29æ¥Arxivè®¡ç®æºè§è§è®ºæçæ¯æ¥æ¥åæ§è¡æè¦ï¼æ¨å¨å¸®å©å¿ç¢çç ç©¶äººåå¿«éäºè§£è¯¥é¢åçéè¦åå±ã</p>
<hr />
<p><strong>æ¯æ¥Arxivè®¡ç®æºè§è§è®ºææ¥åæ§è¡æè¦ (2025å¹´10æ29æ¥)</strong></p>
<p><strong>1. ä¸»è¦ä¸»é¢åè¶å¿æ¦è¿°ï¼</strong></p>
<p>ä»å¤©çè®ºæéå±ç°äºè®¡ç®æºè§è§é¢åå ä¸ªå³é®ä¸ç¸äºå³èçè¶å¿ï¼</p>
<ul>
<li><strong>å¤æ¨¡æèåä¸å¤§åæ¨¡å (LLMs/VLMs) çæ·±åº¦æ´åï¼</strong> æ¾èçè¶å¿æ¯å©ç¨å¤§åæ¨¡åçè½åè¿è¡æ´å¤æçè§è§çè§£åçæä»»å¡ãå¤æ¨¡ææ¨çãæç¥åçææ¯æ ¸å¿ç¦ç¹ï¼å°¤å¶æ¯å¨å¤çææ¬ãå¾ååè§é¢æ°æ®æ¹é¢ã</li>
<li><strong>å·èº«æºè½ä¸äººæºäº¤äº (HRI) çè¿æ­¥ï¼</strong> è®ºææ¶åäºæé¨è¯­è¨æä½ãç©ä½äº¤äºä»¥åå¯¹å¤æåºæ¯ä¸­äººç±»è¡ä¸ºççè§£ï¼è¿è¡¨æäºå¯¹å·èº«æºè½åæ´èªç¶äººæºäº¤äºçæ¥çå¢é¿çå´è¶£ã</li>
<li><strong>ç»ç²åº¦çè§£ä¸æ§å¶ï¼</strong> ä»å®ä¾çº§å¾åæ£ç´¢å°å¼æ¾è¯æ±é¨ååå²ï¼ç ç©¶äººåæ­£å¨è¿½æ±å¯¹è§è§åå®¹æ´ç»è´ãæ´å¯æ§ççè§£åæä½ã</li>
<li><strong>å¨æä¸æ¶åºæ¨çï¼</strong> è§é¢QAãæµå¼æ°æ®å¤çä»¥åå¨æè§è§ææçæç­ä¸»é¢å¼ºè°äºå¯¹æ¶åºä¿¡æ¯åå¨æåºæ¯å»ºæ¨¡çéè§ã</li>
<li><strong>æçä¸éåºæ§ï¼</strong> ç¨çæ¶æãæµè¯æ¶èªéåºä»¥åé¶æ ·æ¬å­¦ä¹ ç­ææ¯è¡¨æäºå¯¹æ¨¡åæçãæ³åè½ååå¨æªç¥ç¯å¢ä¸­éåºæ§çæç»­è¿½æ±ã</li>
</ul>
<p><strong>2. ç¹å«éè¦æåæ°çè®ºæï¼</strong></p>
<ul>
<li><strong>"Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation" (Inclusion AI et al.)ï¼</strong> è¿ç¯è®ºæå¯è½ä»£è¡¨äºå¤æ¨¡æé¢åçä¸ä¸ªéè¦æ¹åãå¶æåºçâç¨çãç»ä¸æ¶æâææå¨æçåæ§è½ä¹é´åå¾å¹³è¡¡ï¼å¯¹äºæªæ¥å¤§åå¤æ¨¡ææ¨¡åçé¨ç½²ååºç¨å·ææ½å¨ççªç ´æ§æä¹ã</li>
<li><strong>"VFXMaster: Unlocking Dynamic Visual Effect Generation via In-Context Learning" (Baolu Li et al.)ï¼</strong> å©ç¨âä¸ä¸æå­¦ä¹ âè¿è¡å¨æè§è§ææçææ¯ä¸ä¸ªéå¸¸æ°é¢ä¸å®ç¨çæ¹åãå®å¯è½ä¸ºåæäº§ä¸ååå®¹çæå¸¦æ¥é©å½æ§çååï¼å±ç¤ºäºå¤§åæ¨¡åå¨å¤æçæä»»å¡ä¸­çå¼ºå¤§æ½åã</li>
<li><strong>"Multimodal Spatial Reasoning in the Large Model Era: A Survey and Benchmarks" (Xu Zheng et al.)ï¼</strong> ä½ä¸ºä¸ç¯ç»¼è¿°ååºåè®ºæï¼å®ä¸ä»æ»ç»äºå½åå¤æ¨¡æç©ºé´æ¨ççè¿å±ï¼è¿æåºäºæ°çè¯ä¼°æ åï¼å¯¹äºæå¯¼æªæ¥è¯¥é¢åçç ç©¶å·æéè¦ä»·å¼ã</li>
</ul>
<p><strong>3. æ°å´ç ç©¶æ¹åæææ¯ï¼</strong></p>
<ul>
<li><strong>ç¨çç»ä¸å¤æ¨¡ææ¶æï¼</strong> "Ming-Flash-Omni" æåºçæ¦å¿µï¼æ¨å¨è§£å³å¤§åå¤æ¨¡ææ¨¡åçè®¡ç®ååå­ç¶é¢ï¼åæ¶ä¿æå¶éç¨æ§ã</li>
<li><strong>ä¸ä¸æå­¦ä¹ å¨å¤æçæä»»å¡ä¸­çåºç¨ï¼</strong> "VFXMaster" å±ç¤ºäºå¦ä½å©ç¨å¤§åæ¨¡åçä¸ä¸æå­¦ä¹ è½åæ¥çæé«åº¦å¨æåå¤æçè§è§ææï¼è¿é¢ç¤ºççææ¨¡åçæ°èå¼ã</li>
<li><strong>çç©å¯åå¼RGBDæåï¼</strong> "Seeing Clearly and Deeply" æåºçâä»¿çåä¸­å¿è®¾è®¡âä¸ºæ·±åº¦æç¥åæåææ¯æä¾äºæ°çæè·¯ï¼å¯è½å¨æºå¨äººãèªå¨é©¾é©¶ç­é¢åæåºç¨æ½åã</li>
<li><strong>è¯­è¨æ¥å°åå±å¼æ¾è¯æ±é¨ååå²ï¼</strong> "LangHOPS" ç»åäºè¯­è¨çè§£åç»ç²åº¦è§è§åå²ï¼æ¯å®ç°æ´èªç¶ãæ´çµæ´»è§è§çè§£çå³é®ä¸æ­¥ã</li>
<li><strong>åºäºåºç¡æ¨¡åçæµè¯æ¶èªéåºï¼</strong> "Test-Time Adaptive Object Detection" å©ç¨åºç¡æ¨¡åè¿è¡æµè¯æ¶èªéåºï¼æé«äºæ¨¡åå¨æªç¥æååç¯å¢ä¸­çé²æ£æ§ã</li>
</ul>
<p><strong>4. å»ºè®®å®æ´éè¯»çè®ºæï¼</strong></p>
<p>åºäºå¶æ½å¨å½±åãåæ°æ§åå¯¹æªæ¥ç ç©¶çæå¯¼æä¹ï¼å»ºè®®ä¼åå®æ´éè¯»ä»¥ä¸è®ºæï¼</p>
<ul>
<li><strong>"Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation" (Inclusion AI et al.)ï¼</strong> äºè§£ä¸ä¸ä»£å¤æ¨¡ææ¨¡åçæ¶æè®¾è®¡åæçæåç­ç¥ã</li>
<li><strong>"VFXMaster: Unlocking Dynamic Visual Effect Generation via In-Context Learning" (Baolu Li et al.)ï¼</strong> æ¢ç´¢å¤§åæ¨¡åå¨å¤æãå¨æè§è§åå®¹çææ¹é¢çææ°è¿å±ååºç¨æ½åã</li>
<li><strong>"Multimodal Spatial Reasoning in the Large Model Era: A Survey and Benchmarks" (Xu Zheng et al.)ï¼</strong> ä½ä¸ºè¯¥é¢åçç»¼è¿°ï¼å®æä¾äºå¨é¢çèæ¯ç¥è¯åæªæ¥ç ç©¶æ¹åçæå¼ã</li>
<li><strong>"StreamingCoT: A Dataset for Temporal Dynamics and Multimodal Chain-of-Thought Reasoning in Streaming VideoQA" (Yuhang Hu et al.)ï¼</strong> å¯¹äºå³æ³¨è§é¢çè§£ãæ¶åºæ¨çåå¤æ¨¡æé¾å¼æèçç ç©¶äººåï¼è¿ä»½æ°æ®éåç¸å³ç ç©¶å·æéè¦åèä»·å¼ã</li>
</ul>
<hr />
<p>è¿ä»½æè¦æ¨å¨ä¸ºç ç©¶äººåæä¾ä¸ä¸ªå¿«éçæ¦è§ï¼å¸®å©ä»ä»¬è¯å«ä»å¤©Arxivä¸æç¸å³åææåæ¯çè®¡ç®æºè§è§ç ç©¶ã</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2510.25760v1">Multimodal Spatial Reasoning in the Large Model Era: A Survey and Benchmarks</a></li>
<li><a href="#2510.24821v1">Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation</a></li>
<li><a href="#2510.25772v1">VFXMaster: Unlocking Dynamic Visual Effect Generation via In-Context Learning</a></li>
<li><a href="#2510.25387v1">Instance-Level Composed Image Retrieval</a></li>
<li><a href="#2510.25332v1">StreamingCoT: A Dataset for Temporal Dynamics and Multimodal Chain-of-Thought Reasoning in Streaming VideoQA</a></li>
<li><a href="#2510.25314v1">Seeing Clearly and Deeply: An RGBD Imaging Approach with a Bio-inspired Monocentric Design</a></li>
<li><a href="#2510.25268v1">SynHLMA:Synthesizing Hand Language Manipulation for Articulated Object with Discrete Human Object Interaction Representation</a></li>
<li><a href="#2510.25263v1">LangHOPS: Language Grounded Hierarchical Open-Vocabulary Part Segmentation</a></li>
<li><a href="#2510.25175v1">Test-Time Adaptive Object Detection with Foundation Model</a></li>
<li><a href="#2510.25094v1">Visual Diversity and Region-aware Prompt Learning for Zero-shot HOI Detection</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2510.25760v1'></a></p>
<h2 id="multimodal-spatial-reasoning-in-the-large-model-era-a-survey-and-benchmarks"><a href="https://arxiv.org/abs/2510.25760v1">Multimodal Spatial Reasoning in the Large Model Era: A Survey and Benchmarks</a></h2>
<p><strong>Authors:</strong> Xu Zheng, Zihao Dongfang, Lutao Jiang, Boyuan Zheng, Yulong Guo, Zhenquan Zhang, Giuliano Albanese, Runyi Yang, Mengjiao Ma, Zixin Zhang, Chenfei Liao, Dingcheng Zhen, Yuanhuiyi Lyu, Yuqian Fu, Bin Ren, Linfeng Zhang, Danda Pani Paudel, Nicu Sebe, Luc Van Gool, Xuming Hu</p>
<p><strong>Published:</strong> 2025-10-29</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Humans possess spatial reasoning abilities that enable them to understand
spaces through multimodal observations, such as vision and sound. Large
multimodal reasoning models extend these abilities by learning to perceive and
reason, showing promising performance across diverse spatial tasks. However,
systematic reviews and publicly available benchmarks for these models remain
limited. In this survey, we provide a comprehensive review of multimodal
spatial reasoning tasks with large models, categorizing recent progress in
multimodal large language models (MLLMs) and introducing open benchmarks for
evaluation. We begin by outlining general spatial reasoning, focusing on
post-training techniques, explainability, and architecture. Beyond classical 2D
tasks, we examine spatial relationship reasoning, scene and layout
understanding, as well as visual question answering and grounding in 3D space.
We also review advances in embodied AI, including vision-language navigation
and action models. Additionally, we consider emerging modalities such as audio
and egocentric video, which contribute to novel spatial understanding through
new sensors. We believe this survey establishes a solid foundation and offers
insights into the growing field of multimodal spatial reasoning. Updated
information about this survey, codes and implementation of the open benchmarks
can be found at https://github.com/zhengxuJosh/Awesome-Spatial-Reasoning.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°å¯¹è¿ç¯è®ºææè¦è¿è¡åæã</p>
<hr />
<p><strong>è®ºæåæï¼Multimodal Spatial Reasoning in the Large Model Era: A Survey and Benchmarks</strong></p>
<p><strong>1. è®ºæä¸»è¦è´¡ç®çç®æ´æ»ç» (2-3 å¥è¯)</strong></p>
<p>è¿ç¯è®ºæå¯¹å¤§åå¤æ¨¡ææ¨çæ¨¡åå¨ç©ºé´æ¨çé¢åçææ°è¿å±è¿è¡äºå¨é¢ç»¼è¿°ï¼å¹¶é¦æ¬¡ç³»ç»å°åç±»äºå¤æ¨¡æå¤§è¯­è¨æ¨¡åï¼MLLMsï¼å¨ç©ºé´ä»»å¡ä¸­çåºç¨ãå®è¿å¼å¥äºå¬å¼åºåï¼ä»¥ä¿è¿å¯¹è¿äºæ¨¡åè¿è¡æ ååè¯ä¼°ï¼ä»èä¸ºå¤æ¨¡æç©ºé´æ¨çè¿ä¸æ°å´é¢åå¥ å®äºåå®åºç¡ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºæ¹æ³</strong></p>
<p>è¯¥è®ºæçå³é®åæ°å¨äºå¶<strong>ç³»ç»æ§çç»¼è¿°æ¡æ¶åå¼å¥å¼æ¾åºå</strong>ãå®ä¸ä»è¶è¶äºä¼ ç»ç2Dä»»å¡ï¼æ·±å¥æ¢è®¨äº3Dç©ºé´ä¸­çç©ºé´å³ç³»æ¨çãåºæ¯ä¸å¸å±çè§£ãè§è§é®ç­åå®ä½ï¼è¿æ¶µçäºå·èº«AIï¼å¦è§è§-è¯­è¨å¯¼èªåå¨ä½æ¨¡åï¼ä»¥åæ°å´æ¨¡æï¼å¦é³é¢åç¬¬ä¸äººç§°è§è§è§é¢ï¼å¯¹ç©ºé´çè§£çè´¡ç®ãè¿ç§å¨é¢çåç±»åè¯ä¼°å·¥å·çæä¾ï¼æ¯å¶æ¹æ³è®ºä¸çæ ¸å¿ã</p>
<p><strong>3. å¯¹é¢åæ½å¨å½±å</strong></p>
<p>è¿ç¯è®ºæå¯¹è®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åå·ææ¾èçæ½å¨å½±åï¼</p>
<ul>
<li><strong>æ ååä¸å éç ç©¶ï¼</strong> éè¿æä¾å¨é¢çç»¼è¿°åå¼æ¾åºåï¼å®å°å¸®å©ç ç©¶äººåå¿«éäºè§£å½åè¿å±ï¼å¹¶ä¸ºæ¨¡åè¯ä¼°æä¾ç»ä¸æ åï¼ä»èå éè¯¥é¢åçç ç©¶ååå±ã</li>
<li><strong>æ¨å¨å¤æ¨¡æèåï¼</strong> å¼ºè°äºè§è§ãå¬è§ãè¯­è¨ç­å¤ç§æ¨¡æå¨ç©ºé´çè§£ä¸­çååä½ç¨ï¼å°é¼å±æ´å¤ç ç©¶å³æ³¨å¤æ¨¡ææ°æ®èååè·¨æ¨¡ææ¨çã</li>
<li><strong>ä¿è¿å·èº«æºè½åå±ï¼</strong> å¯¹å·èº«AIçå³æ³¨ï¼ç¹å«æ¯è§è§-è¯­è¨å¯¼èªåå¨ä½æ¨¡åï¼å°ç´æ¥æ¨å¨æºå¨äººå­¦ãèªå¨é©¾é©¶åæºè½ä»£çç­é¢åçåå±ã</li>
<li><strong>å¯åæ°ç ç©¶æ¹åï¼</strong> å¯¹æ°å´æ¨¡æï¼å¦é³é¢åç¬¬ä¸äººç§°è§è§è§é¢ï¼çæ¢è®¨ï¼å¯è½ä¼æ¿åæ°çä¼ æå¨åºç¨åç©ºé´çè§£èå¼ã</li>
</ul>
<p><strong>4. å¯è½åçäºè¿é¡¹ç ç©¶çç¸å³é¢åæåºç¨</strong></p>
<ul>
<li><strong>æºå¨äººå­¦åå·èº«AIï¼</strong> æºå¨äººéè¦çè§£å¶ç¯å¢çç©ºé´å¸å±ãç©ä½å³ç³»ä»¥åå¯¼èªè·¯å¾ï¼è¿ç¯ç»¼è¿°ååºåå°ç´æ¥å¸®å©å¼åæ´æºè½ãæ´å·é²æ£æ§çæºå¨äººã</li>
<li><strong>èªå¨é©¾é©¶ï¼</strong> è½¦è¾éè¦å®æ¶çè§£å¤æç3Dåºæ¯ãè¯å«äº¤éåä¸èãé¢æµå¶è¡ä¸ºå¹¶è§åå®å¨è·¯å¾ï¼å¤æ¨¡æç©ºé´æ¨çè½åè³å³éè¦ã</li>
<li><strong>èæç°å®/å¢å¼ºç°å® (VR/AR)ï¼</strong> æå»ºæ²æµ¸å¼åäº¤äºå¼VR/ARä½éªéè¦ç²¾ç¡®çç©ºé´æç¥ãåºæ¯çè§£åç¨æ·å®ä½ã</li>
<li><strong>æºè½å®¶å±åæºæ§åå¸ï¼</strong> æºè½è®¾å¤éè¦çè§£å®¶åº­æåå¸ç¯å¢çç©ºé´ç»æï¼ä»¥æä¾æ´æºè½çæå¡ï¼å¦æºè½å¯¼èªãå®å¨çæ§ç­ã</li>
<li><strong>å»å­¦å½±ååæï¼</strong> å»çéè¦çè§£å»å­¦å¾åä¸­ç3Dç»æåçåçç©ºé´å³ç³»ï¼è¿å¯è½ä»å¤æ¨¡æç©ºé´æ¨çææ¯ä¸­åçã</li>
<li><strong>å°çä¿¡æ¯ç³»ç» (GIS) åé¥æï¼</strong> åæå«æå¾ååå°çæ°æ®ä»¥çè§£å°çè¡¨é¢çç©ºé´æ¨¡å¼åååã</li>
</ul>
<p><strong>5. ä»æè¦ä¸­å¯ä»¥æ¨æ­åºçä»»ä½å±éæ§</strong></p>
<ul>
<li><strong>ä¾§éäºâå¤§åæ¨¡åâï¼</strong> æè¦æç¡®æåºæ¯âå¤§åæ¨¡åæ¶ä»£âçå¤æ¨¡æç©ºé´æ¨çï¼è¿æå³çå¶ç»¼è¿°ååºåå¯è½ä¸»è¦å³æ³¨åæ°éå¤§ãè®¡ç®èµæºéæ±é«çæ¨¡åãå¯¹äºèµæºåéçè¾¹ç¼è®¾å¤æç¹å®åºç¨åºæ¯ï¼è¿äºæ¨¡åçé¨ç½²åæçå¯è½æ¯ä¸ä¸ªææï¼è¿æ¹é¢å¯è½æªè¢«æ·±å¥æ¢è®¨ã</li>
<li><strong>âæ°å´æ¨¡æâçæçåº¦ï¼</strong> å°½ç®¡æå°äºé³é¢åç¬¬ä¸äººç§°è§è§è§é¢ç­æ°å´æ¨¡æï¼ä½è¿äºæ¨¡æå¨ç©ºé´æ¨çä¸­çåºç¨å¯è½ä»å¤äºæ©æé¶æ®µï¼å¶æ°æ®å¯ç¨æ§ãæ æ³¨é¾åº¦åæ¨¡åæ§è½å¯è½ä¸å¦ä¼ ç»è§è§æ¨¡ææçãç»¼è¿°å¯è½æ´å¤å°æ¯å±æèéæ·±å¥åæå¶å½åç¶é¢ã</li>
<li><strong>è§£éæ§ï¼Explainabilityï¼çæ·±åº¦ï¼</strong> æè¦ä¸­æå°äºâè§£éæ§âï¼ä½å¨å¤§åå¤æ¨¡ææ¨¡åä¸­å®ç°çæ­£å¯ä¿¡èµåå¯æä½çè§£éæ§ä»ç¶æ¯ä¸ä¸ªå¼æ¾ä¸æå·æææ§çé®é¢ãç»¼è¿°å¯è½åªæ¯è§¦åäºè¿ä¸æ¹é¢ï¼èæªè½æä¾çªç ´æ§çè§£å³æ¹æ¡ææ·±å¥çåæã</li>
<li><strong>åå¸æ¥æå¨æªæ¥ï¼</strong> è®ºæçåå¸æ¥ææ¯2025å¹´10æ29æ¥ï¼è¿æå³çå®æ¯ä¸ç¯é¢åå¸ææ­£å¨è¿è¡ä¸­çå·¥ä½ãè½ç¶è¿è¡¨æå¶åå®¹å·æåç»æ§ï¼ä½ä¹å¯è½æå³çæäºç»èæå®éªç»æå°æªå®å¨æ²å®ï¼æèå¨æç»çæ¬ä¸­ä¼ææè°æ´ã</li>
</ul>
<hr />
<p>æ»çæ¥è¯´ï¼è¿ç¯è®ºæå¨è®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åå·æéè¦çå°ä½ï¼å®ä¸ä»ç³»ç»å°æ¢³çäºå½åå¤æ¨¡æç©ºé´æ¨ççè¿å±ï¼è¿éè¿å¼å¥å¼æ¾åºåï¼ä¸ºæªæ¥çç ç©¶åè¯ä¼°æä¾äºå®è´µçå·¥å·ãå¶å¯¹3Dç©ºé´ãå·èº«AIåæ°å´æ¨¡æçå³æ³¨ï¼é¢ç¤ºçè¯¥é¢åæªæ¥åå±çæ¹åã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Additionally, we consider emerging modalities such as audio
and egocentric video, which contribute to novel spatial understanding through
new sensors.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.25760v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.25760v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.24821v1'></a></p>
<h2 id="ming-flash-omni-a-sparse-unified-architecture-for-multimodal-perception-and-generation"><a href="https://arxiv.org/abs/2510.24821v1">Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation</a></h2>
<p><strong>Authors:</strong> Inclusion AI, :, Bowen Ma, Cheng Zou, Canxiang Yan, Chunxiang Jin, Chunjie Shen, Dandan Zheng, Fudong Wang, Furong Xu, GuangMing Yao, Jun Zhou, Jingdong Chen, Jianing Li, Jianxin Sun, Jiajia Liu, Jianjiang Zhu, Jianping Jiang, Jun Peng, Kaixiang Ji, Kaimeng Ren, Libin Wang, Lixiang Ru, Longhua Tan, Lan Wang, Mochen Bai, Ning Gao, Qingpei Guo, Qinglong Zhang, Qiang Xu, Rui Liu, Ruijie Xiong, Ruobing Zheng, Sirui Gao, Tianqi Li, Tinghao Liu, Weilong Chai, Xinyu Xiao, Xiaomei Wang, Xiaolong Wang, Xiao Lu, Xiaoyu Li, Xingning Dong, Xuzheng Yu, Yi Yuan, Yuting Gao, Yuting Xiao, Yunxiao Sun, Yipeng Chen, Yifan Mao, Yifei Wu, Yongjie Lyu, Ziping Ma, Zhiqiang Fang, Zhihao Qiu, Ziyuan Huang, Zizheng Yang, Zhengyu He</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>We propose Ming-Flash-Omni, an upgraded version of Ming-Omni, built upon a
sparser Mixture-of-Experts (MoE) variant of Ling-Flash-2.0 with 100 billion
total parameters, of which only 6.1 billion are active per token. This
architecture enables highly efficient scaling (dramatically improving
computational efficiency while significantly expanding model capacity) and
empowers stronger unified multimodal intelligence across vision, speech, and
language, representing a key step toward Artificial General Intelligence (AGI).
Compared to its predecessor, the upgraded version exhibits substantial
improvements across multimodal understanding and generation. We significantly
advance speech recognition capabilities, achieving state-of-the-art performance
in contextual ASR and highly competitive results in dialect-aware ASR. In image
generation, Ming-Flash-Omni introduces high-fidelity text rendering and
demonstrates marked gains in scene consistency and identity preservation during
image editing. Furthermore, Ming-Flash-Omni introduces generative segmentation,
a capability that not only achieves strong standalone segmentation performance
but also enhances spatial control in image generation and improves editing
consistency. Notably, Ming-Flash-Omni achieves state-of-the-art results in
text-to-image generation and generative segmentation, and sets new records on
all 12 contextual ASR benchmarks, all within a single unified architecture.</p>
<p><strong>Analysis:</strong></p>
<p>ä»¥ä¸æ¯è®ºæâMing-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generationâçæè¦ï¼</p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢</strong>
è¯¥è®ºææ¨å¨è§£å³æå»ºç»ä¸å¤æ¨¡ææºè½æ¨¡åæé¢ä¸´çææï¼ç¹å«æ¯å¦ä½ææå°æ´åè·¨è§è§ãè¯­é³åè¯­è¨ç­å¤ç§æ¨¡æççè§£ä¸çæè½åï¼ä»¥å®ç°æ´å¼ºå¤§çç»ä¸å¤æ¨¡ææºè½ï¼å¹¶æç»è¿åéç¨äººå·¥æºè½ï¼AGIï¼ãç°ææ¨¡åå¨å¤æ¨¡æçè§£åçææ¹é¢å­å¨è¡¨ç¤ºå·®å¼åæ¨¡æä¸å¹³è¡¡çé®é¢ï¼éå¶äºå¶å¨ç²¾ç»æ§å¶åé«ä¿çåº¦ä»»å¡ä¸çè¡¨ç°ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®</strong>
*   <strong>ç¨çMoEæ¶æï¼</strong> æåºäºMing-Flash-Omniï¼å®æ¯Ming-Omniçåçº§çï¼åºäºLing-Flash-2.0çç¨çæ··åä¸å®¶ï¼MoEï¼åä½ï¼æ»åæ°è¾¾1000äº¿ï¼ä½æ¯ä¸ªtokenä»æ¿æ´»61äº¿åæ°ãè¿ç§æ¶æå®ç°äºé«ææ©å±ï¼æ¾èæåäºè®¡ç®æçå¹¶æ©å¤§äºæ¨¡åå®¹éã
*   <strong>ç»ä¸å¤æ¨¡ææºè½ï¼</strong> å¨è§è§ãè¯­é³åè¯­è¨æ¹é¢å®ç°äºæ´å¼ºçç»ä¸å¤æ¨¡ææºè½ã
*   <strong>ä¸ä¸ææç¥ASRï¼</strong> æ¾èæåäºè¯­é³è¯å«è½åï¼å¨ä¸ä¸ææç¥ASRåæ¹è¨æç¥ASRæ¹é¢åå¾äºæåè¿çæ§è½ã
*   <strong>é«ä¿çææ¬æ¸²æåå¾åç¼è¾ï¼</strong> å¨å¾åçæä¸­å¼å¥äºé«ä¿çææ¬æ¸²æï¼å¹¶å¨å¾åç¼è¾ä¸­æ¾èæåäºåºæ¯ä¸è´æ§åèº«ä»½ä¿æè½åã
*   <strong>çæå¼åå²ï¼</strong> å¼å¥äºçæå¼åå²è½åï¼ä¸ä»å®ç°äºå¼ºå¤§çç¬ç«åå²æ§è½ï¼è¿å¢å¼ºäºå¾åçæçç©ºé´æ§å¶åç¼è¾ä¸è´æ§ã
*   <strong>VideoRoPEï¼</strong> å¨çè§£ä¾§ï¼åçº§äºä½ç½®ç¼ç ä¸ºVideoRoPEï¼ä»¥æ´å¥½å°ææè§é¢åºåä¸­çæ¶é´å¨æï¼å¢å¼ºæ¨¡åçè§£å¤æè§è§äºä»¶çè½åã
*   <strong>è¿ç»­è¯­é³è¡¨ç¤ºï¼</strong> å¨çæä¾§ï¼ç¨è¿ç»­å£°å­¦æ½å¨è¡¨ç¤ºåä»£äºç¦»æ£è¯­é³tokenï¼é¿åäºéåæå¤±ï¼æé«äºä¿çåº¦ã
*   <strong>çæå¼åå²å³ç¼è¾ï¼</strong> æåºäºä¸ç§ååè®­ç»èå¼ï¼å°å¾ååå²éæ°å®ä¹ä¸ºçæå¼ç¼è¾ä»»å¡ï¼å¼ºå¶å°çè§£åçæçç®æ ç»å®ï¼ä»èå¹å»äºç²¾ç»çç©ºé´è¯­ä¹æ§å¶è½åã
*   <strong>åºç¡è®¾æ½ä¼åï¼</strong> å¨Megatron-LMæ¡æ¶ä¸è¿è¡äºå¢å¼ºï¼åæ¬åºåæåï¼Sequence Packingï¼ä»¥å¤çå¨æè¾å¥å½¢ç¶ï¼ä»¥åçµæ´»ç¼ç å¨åçï¼Flexible Encoder Shardingï¼ä»¥ç¼è§£æµæ°´çº¿æ°æ³¡ï¼å°è®­ç»ååéæé«äºä¸¤åä»¥ä¸ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹</strong>
*   <strong>æåè¿çæ§è½ï¼</strong> Ming-Flash-Omniå¨ææ¬å°å¾åçæåçæå¼åå²æ¹é¢åå¾äºæåè¿çï¼SOTAï¼ç»æï¼å¹¶å¨ææ12ä¸ªä¸ä¸æASRåºåæµè¯ä¸­åé äºæ°çºªå½ï¼ææè¿äºé½éæå¨ä¸ä¸ªç»ä¸çæ¶æä¸­ã
*   <strong>å¾åçè§£ï¼</strong> å¨MathVistaä¸å¾å81.9%ï¼å¨OCRç¸å³åºåæµè¯ä¸­è¡¨ç°åºè²ï¼å¨å¤å¾åçè§£ä»»å¡ä¸­ä¼äºQwen3-VL-30B-A3Bã
*   <strong>è§é¢çè§£ï¼</strong> å¨MVBenchä¸å¾å74.6%ï¼å¨VideoMMEï¼å¸¦å­å¹ï¼ä¸å¾å73.0%ï¼å±ç°äºå¼ºå¤§çè§é¢æ¨çè½åã
*   <strong>å¾åçæï¼</strong> å¨GenEvalåºåæµè¯ä¸­å¾å0.90ï¼è¶è¶äºææéå¼ºåå­¦ä¹ æ¹æ³ï¼å¨âä½ç½®âåâé¢è²âå­ç±»å«ä¸­è¡¨ç°å°¤ä¸ºçªåºã
*   <strong>å¾åç¼è¾ï¼</strong> æ¾èä¼äºææå¶ä»ç»ä¸æ¨¡åï¼æ¯æä¸­æç¼è¾æä»¤ï¼å¹¶ä»¥æ´é«æç2B DiTå¤´é¨å®ç°äºä¸20B DiTå¤´é¨ç¸å½çè¯­ä¹ä¸è´æ§åæç¥è´¨éã
*   <strong>å¤æ¨¡æçè§£åçæï¼</strong> å¨å¤æ¨¡æçè§£åçæä»»å¡ä¸­åå±ç°åºåè¶çè·¨æ¨¡ææ§è½ï¼å¨å¾åæç¥ä»»å¡ä¸­ä¸Qwen3-Omniæ§è½ç¸å½ï¼å¹¶å¨ç«¯å°ç«¯è¯­é³çè§£åçææ¹é¢è¡¨ç°ä¼å¼ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§</strong>
*   å¨éç¨ç®ççè§£ä»»å¡ä¸­ï¼ä¸æåè¿çè§è§è¯­è¨æ¨¡åç¸æ¯ï¼ä»å­å¨è½»å¾®å·®è·ã
*   å¨å¤å¾åçè§£çMuirBenchä¸ï¼æ§è½ç¥æå·®è·ï¼è¡¨æä»ææ¹è¿ç©ºé´ã
*   å¨é¿è§é¢çè§£æ¹é¢ï¼æ§è½ç¥ä½ã
*   Qwen-Image-Editå¨æ¨çæ¶å¸¸é¾ä»¥åç¡®è¯å«ç®æ å¯¹è±¡ï¼Nano-bananaåç»å¸¸è¯¯è§£ç¨æ·æå¾ï¼è¿çªæ¾äºMing-Flash-Omniå¨åå²æ¹é¢çä¼å¿ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹å</strong>
*   è¿ä¸æ­¥ç¼©å°å¨éç¨ç®ççè§£åå¤å¾åçè§£ï¼å¦MuirBenchï¼æ¹é¢çæ§è½å·®è·ã
*   æ¢ç´¢æ´æ·±å±æ¬¡çå¤æ¨¡æäº¤äºåèåæºå¶ï¼ä»¥è¿ä¸æ­¥æåæ¨¡åçç²¾ç»æ§å¶åé«ä¿çåº¦ã
*   éè¿å¼æºæ¨¡ååä»£ç ï¼ä¿è¿AGIçåå±ï¼æ¨å¨å¤æ¨¡ææºè½ç ç©¶åæ´å¹¿æ³çå®éåºç¨ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose Ming-Flash-Omni, an upgraded version of Ming-Omni, built upon a
sparser Mixture-of-Experts (MoE) variant of Ling-Flash-2.0 with 100 billion
total parameters, of which only 6.1 billion are active per token.</li>
<li>We significantly
advance speech recognition capabilities, achieving state-of-the-art performance
in contextual ASR and highly competitive results in dialect-aware ASR.</li>
<li>Notably, Ming-Flash-Omni achieves state-of-the-art results in
text-to-image generation and generative segmentation, and sets new records on
all 12 contextual ASR benchmarks, all within a single unified architecture.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.24821v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.24821v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.25772v1'></a></p>
<h2 id="vfxmaster-unlocking-dynamic-visual-effect-generation-via-in-context-learning"><a href="https://arxiv.org/abs/2510.25772v1">VFXMaster: Unlocking Dynamic Visual Effect Generation via In-Context Learning</a></h2>
<p><strong>Authors:</strong> Baolu Li, Yiming Zhang, Qinghe Wang, Liqian Ma, Xiaoyu Shi, Xintao Wang, Pengfei Wan, Zhenfei Yin, Yunzhi Zhuge, Huchuan Lu, Xu Jia</p>
<p><strong>Published:</strong> 2025-10-29</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Visual effects (VFX) are crucial to the expressive power of digital media,
yet their creation remains a major challenge for generative AI. Prevailing
methods often rely on the one-LoRA-per-effect paradigm, which is
resource-intensive and fundamentally incapable of generalizing to unseen
effects, thus limiting scalability and creation. To address this challenge, we
introduce VFXMaster, the first unified, reference-based framework for VFX video
generation. It recasts effect generation as an in-context learning task,
enabling it to reproduce diverse dynamic effects from a reference video onto
target content. In addition, it demonstrates remarkable generalization to
unseen effect categories. Specifically, we design an in-context conditioning
strategy that prompts the model with a reference example. An in-context
attention mask is designed to precisely decouple and inject the essential
effect attributes, allowing a single unified model to master the effect
imitation without information leakage. In addition, we propose an efficient
one-shot effect adaptation mechanism to boost generalization capability on
tough unseen effects from a single user-provided video rapidly. Extensive
experiments demonstrate that our method effectively imitates various categories
of effect information and exhibits outstanding generalization to out-of-domain
effects. To foster future research, we will release our code, models, and a
comprehensive dataset to the community.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾è®ºæâVFXMaster: Unlocking Dynamic Visual Effect Generation via In-Context Learningâçå¨é¢æè¦ã</p>
<p><strong>è®ºææè¦ï¼VFXMasterï¼éè¿ä¸ä¸æå­¦ä¹ è§£éå¨æè§è§ææçæ</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢</strong>
æ°å­åªä½ä¸­çè§è§ææï¼VFXï¼å¯¹äºè¡¨è¾¾åè³å³éè¦ï¼ä½å¶åä½å¯¹äºçæå¼AIæ¥è¯´ä»ç¶æ¯ä¸ä¸ªéå¤§ææãç°ææ¹æ³éå¸¸ä¾èµäºâæ¯ä¸ªææä¸ä¸ªLoRAâçèå¼ï¼è¿ç§æ¹æ³èµæºå¯éï¼å¹¶ä¸æ æ³æ³åå°æªè§è¿çææï¼ä»èéå¶äºå¯æ©å±æ§ååä½è½åãæ¬ææ¨å¨è§£å³è¿ä¸éå¶ï¼æåºä¸ä¸ªç»ä¸çæ¡æ¶ï¼è½å¤é«æãçµæ´»å°çæåç§å¨æè§è§ææï¼å¹¶å·å¤å¼ºå¤§çæ³åè½åã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®</strong>
VFXMasteræ¯é¦ä¸ªç»ä¸çãåºäºåèçVFXè§é¢çææ¡æ¶ï¼å¶æ ¸å¿åæ°ç¹åæ¬ï¼</p>
<ul>
<li><strong>å°ææçæéæä¸ºä¸ä¸æå­¦ä¹ ä»»å¡ï¼</strong> è®ºæå°VFXçæè§ä¸ºä¸ä¸ªä¸ä¸æå­¦ä¹ ä»»å¡ï¼ä½¿å¾æ¨¡åè½å¤ä»åèè§é¢ä¸­å­¦ä¹ å¹¶å°å¶å¨æææå¤å¶å°ç®æ åå®¹ä¸ã</li>
<li><strong>ä¸ä¸ææ¡ä»¶ç­ç¥ï¼</strong> è®¾è®¡äºä¸ç§ä¸ä¸ææ¡ä»¶ç­ç¥ï¼éè¿åèç¤ºä¾æ¥æç¤ºæ¨¡åãä¸ä¸ªåèæç¤º-è§é¢å¯¹ä½ä¸ºç¤ºä¾ï¼èç®æ æç¤ºåç¬¬ä¸å¸§ä½ä¸ºæ¥è¯¢æ¥æ¡ä»¶åæ¨¡åçæç®æ è§é¢ã</li>
<li><strong>ä¸ä¸ææ³¨æåæ©ç ï¼</strong> å¼å¥äºä¸ç§ä¸ä¸ææ³¨æåæ©ç æºå¶ï¼ä»¥ç²¾ç¡®å°è§£è¦åæ³¨å¥å¿è¦çè§è§ææå±æ§ï¼åæ¶é²æ­¢ä¿¡æ¯æ³é²ãè¿ä½¿å¾åä¸ªç»ä¸æ¨¡åè½å¤ææ¡æææ¨¡ä»¿ï¼èä¸ä¼å°ä¸ç¸å³çèæ¯æä¸»ä½ä¿¡æ¯è½¬ç§»å°çæç»æä¸­ã</li>
<li><strong>é«æçåæ¬¡ææéåºæºå¶ï¼</strong> éå¯¹é¾ä»¥å¤ççåå¤ï¼OODï¼ææï¼æåºäºä¸ç§é«æçåæ¬¡ææéåºç­ç¥ãéè¿å¼å¥ä¸ç»å¯å­¦ä¹ çæ¦å¿µå¢å¼ºä»¤çï¼concept-enhancing tokensï¼ï¼æ¨¡åå¯ä»¥ä»åä¸ªç¨æ·æä¾çè§é¢ä¸­å¿«éå­¦ä¹ ç»ç²åº¦çVFXå¨æåè½¬æ¢ï¼ä»èæ¾èæé«æ³åè½åã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹</strong>
VFXMasterå¨å¹¿æ³çå®éªä¸­å±ç¤ºäºåè¶çæ§è½ï¼</p>
<ul>
<li><strong>ååææçåè¶æ§è½ï¼</strong> å¨OpenVFXæ°æ®éä¸çå®éªè¡¨æï¼VFXMasterå¨ææè¯ä¼°ææ ä¸åä¼äºç°ææåè¿çVFXçææ¹æ³ï¼å¦VFXCreatoråOminiEffectsï¼ä»¥åç»è¿å¾®è°çåºçº¿æ¨¡åCogVideoXï¼å°¤å¶æ¯å¨å¤çå¤æç»ææå¼ºçè¿å¨çæææ¶è¡¨ç°åºè²ã</li>
<li><strong>å¼ºå¤§çåå¤æ³åè½åï¼</strong> è®ºææå»ºäºä¸ä¸ªä¸é¨çOODæµè¯éï¼å¹¶è®¾è®¡äºVFX-Comprehensive Assessment Score (VFX-Cons.) è¯ä¼°æ¡æ¶ãç»æè¡¨æï¼VFXMasterå¨æªè§è¿çææç±»å«ä¸è¡¨ç°åºå¼ºå¤§çæ³åè½åãä¸ä¸ææ¡ä»¶ç­ç¥æ¬èº«å°±èµäºäºæ¨¡åä¸å®çOODæ³åè½åï¼èåæ¬¡ææéåºæºå¶åè¿ä¸æ­¥æ¾èæåäºæ§è½ï¼ç¹å«æ¯å¨ææä¿çåº¦ï¼EFSï¼ååå®¹æ³é²ï¼CLSï¼æ¹é¢ã</li>
<li><strong>ç¨æ·ç ç©¶éªè¯ï¼</strong> ç¨æ·ç ç©¶ç»ææ¾ç¤ºï¼VFXMasterå¨ææä¸è´æ§åæ´ä½ç¾å­¦è´¨éæ¹é¢è·å¾äºææ¾çç¨æ·åå¥½ï¼è¿ä¸æ­¥è¯å®äºå¶æææ§ã</li>
<li><strong>æ°æ®å¯æ©å±æ§ï¼</strong> å®éªè¯æï¼è®­ç»æ°æ®éä¸æ¨¡åæ§è½åæ­£ç¸å³ï¼å°¤å¶æ¯å¨OODæ³åææ ä¸ï¼è¿çªæ¾äºVFXMasteræ¡æ¶çå¯æ©å±æ§ã</li>
</ul>
<p>è¿äºç»æçæä¹å¨äºï¼VFXMasteråæäºç°ææ¹æ³å¨å¯æ©å±æ§åæ³åæ§æ¹é¢çæ ¹æ¬éå¶ï¼ä¸ºå¨æè§è§ææçææä¾äºä¸ä¸ªç»ä¸ãé«æä¸ç¨æ·åå¥½çè§£å³æ¹æ¡ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§</strong>
è®ºæä¸­æ²¡ææç¡®æåºVFXMasterçç¹å®å±éæ§ãç¶èï¼å®é´æ¥æåäºç°ææ¹æ³çå±éæ§ï¼è¿äºå±éæ§æ¯VFXMasteræ¨å¨è§£å³çï¼
*   <strong>ç°ææ¹æ³çèµæºå¯éæ§ï¼</strong> ä¼ ç»çâæ¯ä¸ªææä¸ä¸ªLoRAâèå¼éè¦ä¸ºæ¯ä¸ªææè¿è¡ä¸é¨çæ°æ®åè®­ç»ï¼è¿éå¶äºå¯æ©å±æ§ã
*   <strong>ç°ææ¹æ³æ³åè½åä¸è¶³ï¼</strong> ç°ææ¹æ³æ æ³æ³åå°æªè§è¿çææç±»å«ï¼è¿éå¶äºå¶éç¨æ§åç¨æ·çåä½èªç±ã
*   <strong>ä¸ä¸æä¿¡æ¯æ³é²ï¼</strong> å¨ä¸ä¸æå­¦ä¹ ä¸­ï¼å¦æä¸å ä»¥æ§å¶ï¼åèä¸ä¸æä¸­çä¸ç¸å³ä¿¡æ¯å¯è½ä¼æ³é²å¹¶å¹²æ°ç®æ çæãVFXMasteréè¿æ³¨æåæ©ç è§£å³äºè¿ä¸ªé®é¢ï¼ä½è¿ä¹æç¤ºäºå¨è®¾è®¡æ­¤ç±»ç³»ç»æ¶éè¦ä»ç»èèä¿¡æ¯æµã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹å</strong>
è®ºææç¡®æåºï¼ä¸ºäºä¿è¿æªæ¥çç ç©¶ï¼ä»ä»¬å°åå¸ä»£ç ãæ¨¡ååå¨é¢çæ°æ®éãè¿æ¬èº«å°±ä¸ºç¤¾åºæä¾äºè¿ä¸æ­¥æ¢ç´¢åæ¹è¿VFXMasteræ¡æ¶çåºç¡ãæ½å¨çæªæ¥ç ç©¶æ¹åå¯è½åæ¬ï¼
*   <strong>æ´å¹¿æ³çOODæ³åï¼</strong> å°½ç®¡VFXMasterå¨OODææä¸è¡¨ç°åºå¼ºå¤§çæ³åè½åï¼ä½ä»ææ¹è¿ç©ºé´ï¼å¯ä»¥æ¢ç´¢æ´åè¿çéåºæºå¶ææ´éç¨çè¡¨ç¤ºå­¦ä¹ æ¹æ³ï¼ä»¥åºå¯¹æ´å·æææ§çæªè§ææã
*   <strong>äº¤äºæ§åç¨æ·æ§å¶ï¼</strong> è¿ä¸æ­¥å¢å¼ºç¨æ·å¯¹VFXçæè¿ç¨çæ§å¶ç²åº¦ï¼ä¾å¦åè®¸ç¨æ·æ´ç²¾ç»å°è°æ´ææçå¼ºåº¦ãä½ç½®ææ¶é´å¨æã
*   <strong>æçåå®æ¶æ§ï¼</strong> å°½ç®¡è®ºææå°äºæçï¼ä½è¿ä¸æ­¥ä¼åæ¨¡åçæ¨çéåº¦åè®¡ç®ææ¬ï¼ä»¥å®ç°æ´æ¥è¿å®æ¶çVFXçæï¼å°æ¯ä¸ä¸ªéè¦çæ¹åã
*   <strong>å¤æ¨¡æè¾å¥ï¼</strong> æ¢ç´¢é¤äºåèè§é¢åå¾åä¹å¤ï¼ç»åæ´å¤æ¨¡æè¾å¥ï¼å¦é³é¢ã3Dæ¨¡åæç¨æ·æç»èå¾ï¼æ¥æå¯¼VFXçæã
*   <strong>å¤æåºæ¯åé¿è§é¢çæï¼</strong> æ©å±VFXMasterä»¥å¤çæ´å¤æçåºæ¯ãæ´é¿çè§é¢åºåææ´å¤æçVFXç»åã</p>
<p>æ»èè¨ä¹ï¼VFXMasterä¸ºå¨æè§è§ææçæé¢åå¸¦æ¥äºæ¾èçè¿æ­¥ï¼éè¿å¶åæ°çä¸ä¸æå­¦ä¹ ååæ¬¡éåºæºå¶ï¼ä¸ºå®ç°å¯æ©å±ãæ³ååé«è´¨éçVFXåä½éºå¹³äºéè·¯ï¼ææéä½æ°å­åªä½åå®¹çäº§çé¨æ§ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In addition, we propose an efficient
one-shot effect adaptation mechanism to boost generalization capability on
tough unseen effects from a single user-provided video rapidly.</li>
<li>Extensive
experiments demonstrate that our method effectively imitates various categories
of effect information and exhibits outstanding generalization to out-of-domain
effects.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.25772v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.25772v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.25387v1'></a></p>
<h2 id="instance-level-composed-image-retrieval"><a href="https://arxiv.org/abs/2510.25387v1">Instance-Level Composed Image Retrieval</a></h2>
<p><strong>Authors:</strong> Bill Psomas, George Retsinas, Nikos Efthymiadis, Panagiotis Filntisis, Yannis Avrithis, Petros Maragos, Ondrej Chum, Giorgos Tolias</p>
<p><strong>Published:</strong> 2025-10-29</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>The progress of composed image retrieval (CIR), a popular research direction
in image retrieval, where a combined visual and textual query is used, is held
back by the absence of high-quality training and evaluation data. We introduce
a new evaluation dataset, i-CIR, which, unlike existing datasets, focuses on an
instance-level class definition. The goal is to retrieve images that contain
the same particular object as the visual query, presented under a variety of
modifications defined by textual queries. Its design and curation process keep
the dataset compact to facilitate future research, while maintaining its
challenge-comparable to retrieval among more than 40M random
distractors-through a semi-automated selection of hard negatives.
  To overcome the challenge of obtaining clean, diverse, and suitable training
data, we leverage pre-trained vision-and-language models (VLMs) in a
training-free approach called BASIC. The method separately estimates
query-image-to-image and query-text-to-image similarities, performing late
fusion to upweight images that satisfy both queries, while down-weighting those
that exhibit high similarity with only one of the two. Each individual
similarity is further improved by a set of components that are simple and
intuitive. BASIC sets a new state of the art on i-CIR but also on existing CIR
datasets that follow a semantic-level class definition. Project page:
https://vrg.fel.cvut.cz/icir/.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°å¯¹è¿ç¯è®ºææè¦è¿è¡åæã</p>
<hr />
<p><strong>è®ºæåæï¼Instance-Level Composed Image Retrieval</strong></p>
<p><strong>1. è®ºæä¸»è¦è´¡ç®çç®æ´æ»ç» (2-3 å¥è¯)</strong></p>
<p>è¿ç¯è®ºæä¸»è¦è´¡ç®å¨äºè§£å³äºç»åå¾åæ£ç´¢ (CIR) é¢åä¸­é«è´¨éè®­ç»åè¯ä¼°æ°æ®ç¼ºå¤±çé®é¢ãå®å¼å¥äºä¸ä¸ªæ°çå®ä¾çº§è¯ä¼°æ°æ®é i-CIRï¼æ¨å¨æ£ç´¢åå«ä¸è§è§æ¥è¯¢ç¸åç¹å®å¯¹è±¡çå¾åï¼å¹¶ç±ææ¬æ¥è¯¢å®ä¹åç§ä¿®æ¹ãæ­¤å¤ï¼è®ºææåºäºä¸ç§åä¸º BASIC çåè®­ç»æ¹æ³ï¼å©ç¨é¢è®­ç»çè§è§-è¯­è¨æ¨¡å (VLMs) è¿è¡ææèåï¼å¨ i-CIR åç°æè¯­ä¹çº§ CIR æ°æ®éä¸åè¾¾å°äºæ°çæåè¿æ°´å¹³ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®º</strong></p>
<ul>
<li><strong>å®ä¾çº§æ°æ®é i-CIRï¼</strong> æå¤§çåæ°æ¯å¼å¥äºç¬¬ä¸ä¸ªä¸æ³¨äºâå®ä¾çº§âç±»å®ä¹ç CIR è¯ä¼°æ°æ®é i-CIRãä¸ç°ææ°æ®éçè¯­ä¹çº§å®ä¹ä¸åï¼i-CIR è¦æ±æ£ç´¢çæ¯ä¸è§è§æ¥è¯¢ä¸­âç¹å®å¯¹è±¡âç¸åçå®ä¾ï¼å³ä½¿è¯¥å¯¹è±¡ç»è¿äºææ¬æè¿°çä¿®æ¹ãè¿ç§å®ä¾çº§çç²åº¦æ¾èå¢å äºä»»å¡çæææ§åå®éåºç¨ä»·å¼ã</li>
<li><strong>ç´§åä¸å·ææææ§çæ°æ®éè®¾è®¡ï¼</strong> i-CIR çè®¾è®¡åç­åè¿ç¨ä½¿å¶ä¿æç´§åï¼ä½éè¿åèªå¨åéæ©å°é¾è´æ ·æ¬ï¼ä½¿å¶æææ§ä¸å¨è¶è¿ 4000 ä¸éæºå¹²æ°ç©ä¸­æ£ç´¢ç¸å½ã</li>
<li><strong>åè®­ç»æ¹æ³ BASICï¼</strong> éå¯¹è®­ç»æ°æ®è·åçææï¼BASIC æ¹æ³å©ç¨é¢è®­ç»ç VLMsï¼éè¿âææèåâç­ç¥å°å¾å-å¾åç¸ä¼¼åº¦åææ¬-å¾åç¸ä¼¼åº¦ç»åèµ·æ¥ãå®ä¼æé«åæ¶æ»¡è¶³ä¸¤ç§æ¥è¯¢çå¾åæéï¼å¹¶éä½ä»æ»¡è¶³å¶ä¸­ä¸ç§æ¥è¯¢çå¾åæéãè¿ç§æ¹æ³é¿åäºæè´µçè®­ç»æ°æ®æ æ³¨ï¼å¹¶å©ç¨äºç°æ VLMs çå¼ºå¤§è½åã</li>
<li><strong>ç®åç´è§çç¸ä¼¼åº¦æ¹è¿ç»ä»¶ï¼</strong> æè¦æå°ï¼æ¯ä¸ªåç¬çç¸ä¼¼åº¦ï¼å¾å-å¾ååææ¬-å¾åï¼é½éè¿ä¸ç»ç®åç´è§çç»ä»¶å¾å°äºè¿ä¸æ­¥æ¹è¿ï¼è¿æç¤ºäºè¯¥æ¹æ³å¯è½å·æè¯å¥½çå¯è§£éæ§åå¯æ©å±æ§ã</li>
</ul>
<p><strong>3. å¯¹è¯¥é¢åçæ½å¨å½±å</strong></p>
<ul>
<li><strong>æ¨å¨ CIR ç ç©¶ï¼</strong> i-CIR æ°æ®éçåå¸å°ä¸º CIR é¢åæä¾ä¸ä¸ªæ¥éçé«è´¨éãå®ä¾çº§è¯ä¼°åºåï¼ä»èåºæ¿æ°çç®æ³åæ¨¡åå¼åï¼ä»¥åºå¯¹æ´ç²¾ç»çæ£ç´¢éæ±ã</li>
<li><strong>æ¹å CIR ä»»å¡å®ä¹ï¼</strong> ä»è¯­ä¹çº§å°å®ä¾çº§çè½¬åï¼å°ä½¿ CIR ä»»å¡æ´æ¥è¿ç°å®ä¸ççåºç¨åºæ¯ï¼ä¾å¦äº§åæ£ç´¢ãç¹å®ç©ä½è¯å«åä¿®æ¹ç­ã</li>
<li><strong>å¯ååè®­ç»æå°æ ·æ¬å­¦ä¹ ï¼</strong> BASIC æ¹æ³çæåè¡¨æï¼å¨æ°æ®ç¨ç¼ºçé¢åï¼å©ç¨é¢è®­ç»æ¨¡åè¿è¡å·§å¦çç»ååèåï¼å¯ä»¥åå¾æ¾èçææï¼è¿å¯è½ä¼å¯åæ´å¤åè®­ç»æå°æ ·æ¬å­¦ä¹ æ¹æ³å¨è®¡ç®æºè§è§é¢åçåºç¨ã</li>
<li><strong>æå VLM çå®ç¨æ§ï¼</strong> è¯¥ç ç©¶è¿ä¸æ­¥å±ç¤ºäºé¢è®­ç»è§è§-è¯­è¨æ¨¡åå¨å¤æå¤æ¨¡ææ£ç´¢ä»»å¡ä¸­çå¼ºå¤§æ½ååå®ç¨æ§ã</li>
</ul>
<p><strong>4. å¯è½åçäºè¿é¡¹ç ç©¶çç¸å³é¢åæåºç¨</strong></p>
<ul>
<li><strong>çµå­åå¡åäº§åæ£ç´¢ï¼</strong> ç¨æ·å¯ä»¥ä¸ä¼ ä¸å¼ äº§åå¾çå¹¶ç¨ææ¬æè¿°ä¿®æ¹ï¼ä¾å¦âåæ¬¾ä½é¢è²ä¸ºçº¢è²âãâç±»ä¼¼æ¬¾å¼ä½æè´¨ä¸ºç®é©âï¼ï¼ç³»ç»è½åç¡®æ£ç´¢å°ç¹å®äº§åå®ä¾ã</li>
<li><strong>åå®¹åä½åç¼è¾ï¼</strong> èºæ¯å®¶æè®¾è®¡å¸å¯ä»¥æ ¹æ®è§è§åèåææ¬æä»¤ï¼ä¾å¦âè¿å¼ å¾çä¸­çæ¤å­ï¼ä½æ¢æç°ä»£é£æ ¼âï¼ï¼å¿«éæ¾å°ç¬¦åè¦æ±çå¾åç´ æã</li>
<li><strong>æ°å­èµäº§ç®¡çï¼</strong> å¨å¤§åå¾ååºä¸­ï¼ç¨æ·å¯ä»¥æ´ç²¾ç¡®å°æ£ç´¢ç¹å®å¯¹è±¡å®ä¾ï¼èä¸æ¯ä»ä»æ¯åå«è¯¥å¯¹è±¡ç±»å«çå¾åã</li>
<li><strong>æºå¨äººè§è§åäººæºäº¤äºï¼</strong> æºå¨äººå¯ä»¥æ ¹æ®è§è§è¾å¥åèªç¶è¯­è¨æä»¤ï¼ä¾å¦âæ¾å°è¿ä¸ªæ¯å­ï¼ä½ä¸é¢æè±çº¹çé£ä¸ªâï¼ï¼è¯å«åå®ä½ç¹å®ç©ä½ã</li>
<li><strong>æ¶å°åè®¾è®¡ï¼</strong> å¸®å©ç¨æ·æ¾å°ç¹å®æ¬¾å¼çæè£æéé¥°ï¼å¹¶æ ¹æ®ææ¬æè¿°è¿è¡ä¿®æ¹ã</li>
</ul>
<p><strong>5. ä»æè¦ä¸­å¯ä»¥æ¨æ­åºçå±éæ§</strong></p>
<ul>
<li><strong>âç®åç´è§çç»ä»¶âçå·ä½ç»èç¼ºå¤±ï¼</strong> æè¦æå°âæ¯ä¸ªåç¬çç¸ä¼¼åº¦é½éè¿ä¸ç»ç®åç´è§çç»ä»¶å¾å°äºè¿ä¸æ­¥æ¹è¿âï¼ä½æ²¡æè¯¦ç»è¯´æè¿äºç»ä»¶æ¯ä»ä¹ãè¿ä½¿å¾æä»¬æ æ³è¯ä¼°è¿äºæ¹è¿çå¤ææ§ãéç¨æ§ææ½å¨çå±éæ§ã</li>
<li><strong>å¯¹é¢è®­ç» VLM çä¾èµï¼</strong> BASIC æ¹æ³ä¸¥éä¾èµäºé¢è®­ç»ç VLMsãå¦æåºå±ç VLM å­å¨åå·®ãæ³åè½åä¸è¶³æå¯¹ç¹å®é¢åæ°æ®ä¸ææï¼é£ä¹ BASIC æ¹æ³çæ§è½å¯è½ä¼åå°å½±åã</li>
<li><strong>âææèåâçæ½å¨å±éæ§ï¼</strong> ææèåè½ç¶ç®åææï¼ä½å¯è½æ æ³ææå°è§è§åææ¬ä¿¡æ¯ä¹é´æ´æ·±å±æ¬¡ãæ´å¤æçäº¤äºãæ©ææä¸­æèåç­ç¥å¨æäºæåµä¸å¯è½è¡¨ç°æ´å¥½ï¼ä½éè¦æ´å¤çè®­ç»æ°æ®ã</li>
<li><strong>æ°æ®éçâç´§åæ§âï¼</strong> å°½ç®¡æè¦å¼ºè° i-CIR ä¿æç´§åä»¥æ¹ä¾¿æªæ¥ç ç©¶ï¼ä½ä¸ä¸äºè¶å¤§è§æ¨¡æ°æ®éç¸æ¯ï¼å¶è§æ¨¡å¯è½ä»ç¶æéãè¿å¯è½ä¼å½±åæ¨¡åå¨æç«¯å¤æ ·æ§æé¿å°¾åå¸æ°æ®ä¸çæ³åè½åã</li>
<li><strong>âåèªå¨åéæ©å°é¾è´æ ·æ¬âçæ½å¨åå·®ï¼</strong> è½ç¶åèªå¨åæå©äºæé«æçï¼ä½å¶éæ©å°é¾è´æ ·æ¬çç­ç¥å¯è½å­å¨æç§åå·®ï¼è¿å¯è½ä¼å½±åæ°æ®éçå¨é¢æ§åå¬å¹³æ§ã</li>
</ul>
<hr />
<p>æ»èè¨ä¹ï¼è¿ç¯è®ºæéè¿å¼å¥å®ä¾çº§ CIR ä»»å¡åç¸åºçæ°æ®é i-CIRï¼ä»¥åæåºåè®­ç»ç BASIC æ¹æ³ï¼ä¸ºç»åå¾åæ£ç´¢é¢åå¸¦æ¥äºéè¦çè¿å±ãå¶å¯¹å®éåºç¨çæ½å¨å½±åå·¨å¤§ï¼å°¤å¶æ¯å¨éè¦ç²¾ç¡®è¯å«åä¿®æ¹ç¹å®å¯¹è±¡å®ä¾çåºæ¯ä¸­ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce
a new evaluation dataset, i-CIR, which, unlike existing datasets, focuses on an
instance-level class definition.</li>
<li>BASIC sets a new state of the art on i-CIR but also on existing CIR
datasets that follow a semantic-level class definition.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.25387v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.25387v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.25332v1'></a></p>
<h2 id="streamingcot-a-dataset-for-temporal-dynamics-and-multimodal-chain-of-thought-reasoning-in-streaming-videoqa"><a href="https://arxiv.org/abs/2510.25332v1">StreamingCoT: A Dataset for Temporal Dynamics and Multimodal Chain-of-Thought Reasoning in Streaming VideoQA</a></h2>
<p><strong>Authors:</strong> Yuhang Hu, Zhenyu Yang, Shihan Wang, Shengsheng Qian, Bin Wen, Fan Yang, Tingting Gao, Changsheng Xu</p>
<p><strong>Published:</strong> 2025-10-29</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>The rapid growth of streaming video applications demands multimodal models
with enhanced capabilities for temporal dynamics understanding and complex
reasoning. However, current Video Question Answering (VideoQA) datasets suffer
from two critical limitations: 1) Static annotation mechanisms fail to capture
the evolving nature of answers in temporal video streams, and 2) The absence of
explicit reasoning process annotations restricts model interpretability and
logical deduction capabilities. To address these challenges, We introduce
StreamingCoT, the first dataset explicitly designed for temporally evolving
reasoning in streaming VideoQA and multimodal Chain-of-Thought (CoT) tasks. Our
framework first establishes a dynamic hierarchical annotation architecture that
generates per-second dense descriptions and constructs temporally-dependent
semantic segments through similarity fusion, paired with question-answer sets
constrained by temporal evolution patterns. We further propose an explicit
reasoning chain generation paradigm that extracts spatiotemporal objects via
keyframe semantic alignment, derives object state transition-based reasoning
paths using large language models, and ensures logical coherence through
human-verified validation. This dataset establishes a foundation for advancing
research in streaming video understanding, complex temporal reasoning, and
multimodal inference. Our StreamingCoT and its construction toolkit can be
accessed at https://github.com/Fleeting-hyh/StreamingCoT.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Yuhang Huç­äººæ°åçè®ºæâStreamingCoT: A Dataset for Temporal Dynamics and Multimodal Chain-of-Thought Reasoning in Streaming VideoQAâçå¨é¢æè¦ã</p>
<hr />
<p><strong>è®ºææè¦ï¼StreamingCoT: æµåªä½è§é¢é®ç­ä¸­æ¶åºå¨æåå¤æ¨¡ææç»´é¾æ¨çæ°æ®é</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
å½åæµåªä½è§é¢åºç¨å¯¹å¤æ¨¡ææ¨¡åçè§£æ¶åºå¨æåå¤ææ¨çè½åæåºäºæ´é«è¦æ±ãç¶èï¼ç°æçè§é¢é®ç­ï¼VideoQAï¼æ°æ®éå­å¨ä¸¤ä¸ªå³é®å±éï¼
1. <strong>éææ æ³¨æºå¶æ æ³æææµåªä½ä¸­ç­æ¡çæ¼åæ§è´¨</strong>ï¼ä¼ ç»æ°æ®éçéæãåºäºæ¶é´æ³çæ æ³¨æ æ³åæ ç­æ¡éæ¶é´è¿ç»­ååçå¨æç¹æ§ã
2. <strong>ç¼ºä¹æ¾å¼æ¨çè¿ç¨æ æ³¨</strong>ï¼è¿éå¶äºæ¨¡åçè§£éæ§åé»è¾æ¼ç»è½åï¼å¯¼è´æ¨¡åå¯è½ä¾èµè¡¨é¢ç»è®¡å³èèéçæ­£çå¤æ¨¡ææ¨çã
è®ºææ¨å¨è§£å³è¿äºææï¼ä¸ºæµåªä½è§é¢çè§£åå¤æ¨¡ææç»´é¾ï¼CoTï¼æ¨çæä¾ä¸ä¸ªä¸é¨è®¾è®¡çæ°æ®éã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
ä¸ºäºè§£å³ä¸è¿°é®é¢ï¼è®ºæå¼å¥äº<strong>StreamingCoT</strong>ï¼è¿æ¯é¦ä¸ªæç¡®ä¸ºæµåªä½è§é¢é®ç­ä¸­çæ¶åºæ¼åæ¨çåå¤æ¨¡æCoTä»»å¡è®¾è®¡çæ°æ®éãå¶æ¡æ¶åå«ä»¥ä¸å³é®åæ°ï¼</p>
<ul>
<li><strong>å¨æåå±æ æ³¨æ¶æ</strong>ï¼<ul>
<li><strong>å¯ééç§æè¿°çæ</strong>ï¼ä¸ºè§é¢çææ¯ç§çç»ç²åº¦ææ¬æè¿°ã</li>
<li><strong>æ¶åºä¾èµè¯­ä¹çæ®µæå»º</strong>ï¼éè¿ç¸ä¼¼æ§èåå°éç§æè¿°èåæè¯­ä¹çæ®µï¼å½¢æç»æåçè§é¢åäºã</li>
<li><strong>åæ¶åºæ¼åæ¨¡å¼çº¦æçé®ç­å¯¹çæ</strong>ï¼æå»ºå¨ææ¼åçé®ç­å¯¹ï¼ç¡®ä¿ç­æ¡éé¡¹ä¸è§é¢åå®¹æ¼åç´§å¯å¯¹é½ï¼å¹¶è®¾è®¡äºåºåå¨ï¼distractorï¼ä»¥åæ å¸¸è§çæ¶åºè¯¯è§£ã</li>
</ul>
</li>
<li><strong>æ¾å¼æ¨çé¾çæèå¼</strong>ï¼<ul>
<li><strong>æ¶ç©ºå¯¹è±¡æåä¸å³é®å¸§è¯­ä¹å¯¹é½</strong>ï¼éè¿å³é®å¸§è¯­ä¹å¯¹é½æåè§é¢ä¸­çæ¶ç©ºå¯¹è±¡ã</li>
<li><strong>åºäºå¤§è¯­è¨æ¨¡åï¼LLMsï¼çå¯¹è±¡ç¶æè½¬æ¢æ¨çè·¯å¾æ¨å¯¼</strong>ï¼å©ç¨LLMsçæåºäºå¯¹è±¡ç¶æè½¬æ¢çé»è¾æ¨çé¾ã</li>
<li><strong>äººå·¥éªè¯ç¡®ä¿é»è¾ä¸è´æ§</strong>ï¼éè¿äººå·¥éªè¯ç¡®ä¿æ¨çé¾çé»è¾è¿è´¯æ§åæ¶åºä¸è´æ§ã</li>
</ul>
</li>
<li><strong>æ ååæµç¨</strong>ï¼è®ºææä¾äºä¸ä¸ªå¯å¤ç°çæ°æ®éæå»ºæµç¨ï¼åæ¬æ°æ®æ¶éãå¨ææ æ³¨åå¤é¶æ®µéªè¯ï¼ä¸ºæå»ºæ¶åºæç¥è§é¢çè§£æ°æ®éæä¾äºæ¹æ³è®ºæå¯¼ã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
StreamingCoTæ°æ®éåå«ï¼
*   <strong>5,000ä¸ªé«è´¨éç­è§é¢</strong>ï¼ç»è¿å¤æ¨¡æè´¨éè¯ä¼°ç²¾å¿ç­éï¼å¹³åæ¶é¿25.6ç§ã
*   <strong>243,185ä¸ªæ¶é´éå®çéç§å¯éå­å¹</strong>ï¼éè¿å¨æè¯­ä¹èåç®æ³çæ68,940ä¸ªè¯­ä¹çæ®µï¼å¹³åæ¯ä¸ªè§é¢12ä¸ªçæ®µã
*   <strong>34,470ä¸ªå¨æé®ç­å¯¹</strong>ï¼æ¯ä¸ªè§é¢5ä¸ªé®ç­å¯¹ï¼æ¶µçå­ç§æ¶åºæ¼åé®é¢ç±»åï¼ç´¯ç§¯è®¡æ°ãå¨ææ¨¡å¼è¯å«ãé¡ºåºæ­¥éª¤è¯å«ãç¶ææç»­æ¶é´ãå¯¹è±¡ç¶æè¯å«ãçº¿ç´¢æ­ç¤ºååºï¼ã
*   <strong>68,940ä¸ªå¤æ¨¡ææç»´é¾ï¼CoTï¼æ æ³¨</strong>ï¼å·ææ¶ç©ºåºç¡ï¼åå«206,820ä¸ªéè¿å³é®å¸§å¯¹é½åè®®å®ä½çå³é®å¯¹è±¡è¾¹çæ¡ã
*   <strong>è¦ç32ä¸ªä¸»é¢ç±»å«</strong>ï¼åæ¬æå­¦æ´»å¨ãèªç¶ç°è±¡ãç¤¾äº¤äºå¨ãæºæ¢°è¿ç¨åèºæ¯è¡¨æ¼ç­ã</p>
<p>è¯¥æ°æ®éçæä¹å¨äºï¼
*   <strong>æ¨å¨æµåªä½è§é¢çè§£ç ç©¶</strong>ï¼ä¸ºå¤çæµåªä½è§é¢ä¸­ç­æ¡å¨ææ¼ååå¤ææ¶åºæ¨çæä¾äºåºç¡ã
*   <strong>æåæ¨¡åè§£éæ§åå¤æ¨¡ææ¨çè½å</strong>ï¼éè¿æ¾å¼æ¨çé¾æ æ³¨ï¼æ¨¡åä¸ä»è½é¢æµç­æ¡ï¼è¿è½æä¾å¯å®¡è®¡çæ¨çè·¯å¾ï¼å¢å¼ºäºæ¨¡åçå¯è§£éæ§åé»è¾ä¸è´æ§ã
*   <strong>å»ºç«æ°çè¯ä¼°æ å</strong>ï¼ä¸ºè¯ä¼°å¤æ¨¡æç³»ç»ä¸­çæ¶åºçè§£è½åè®¾å®äºæ°æ åã</p>
<p><strong>4. è®ºæä¸­æå°çå±éæ§ï¼</strong>
è®ºæä¸»è¦å³æ³¨å¶ä½ä¸ºæ°æ®éçåæ°æ§åæå»ºæ¹æ³ï¼å¹¶æªæç¡®æåèªèº«çå±éæ§ãç¶èï¼ä»å¶æ¹æ³è®ºä¸­å¯ä»¥æ¨æ­åºä¸äºæ½å¨çææææªæ¥æ¹è¿æ¹åï¼
*   <strong>LLMsçææ¨çé¾çè´¨éä¾èµ</strong>ï¼æ¨çé¾ççæä¾èµäºå¤§è¯­è¨æ¨¡åï¼å¶åç¡®æ§åé»è¾ä¸¥è°¨æ§å¯è½åéäºLLMsæ¬èº«çè½ååè®­ç»æ°æ®ãå°½ç®¡æäººå·¥éªè¯ï¼ä½å¤§è§æ¨¡çæä»å¯è½é¢ä¸´æçåä¸è´æ§ææã
*   <strong>äººå·¥éªè¯çææ¬åä¸»è§æ§</strong>ï¼è½ç¶äººå·¥éªè¯ç¡®ä¿äºé«è´¨éï¼ä½è¿æ¯ä¸ä¸ªå³å¨å¯éåä¸ææ¬é«æçè¿ç¨ï¼ä¸äººå·¥å¤æ­å¯è½å­å¨ä¸å®ä¸»è§æ§ã
*   <strong>æ°æ®éè§æ¨¡çè¿ä¸æ­¥æ©å±</strong>ï¼å°½ç®¡StreamingCoTæ¯ä¸ä¸ªå¤§è§æ¨¡æ°æ®éï¼ä½ä¸äºèç½ä¸æ éçæµåªä½è§é¢ç¸æ¯ï¼å¶è§æ¨¡ä»ææ©å±ç©ºé´ï¼ä»¥è¦çæ´å¹¿æ³çåºæ¯åæ´å¤æçæ¶åºå¨æã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>æ´é«æçCoTçæä¸éªè¯</strong>ï¼æ¢ç´¢æ´èªå¨åãæ´é«æçCoTçæåéªè¯æ¹æ³ï¼åå°å¯¹äººå·¥çä¾èµï¼åæ¶ä¿æé«è´¨éãä¾å¦ï¼ç»åå¼ºåå­¦ä¹ æä¸»å¨å­¦ä¹ æ¥ä¼åLLMsçæçæ¨çé¾ã
*   <strong>è·¨æ¨¡ææ¨ççæ³åè½å</strong>ï¼ç ç©¶æ¨¡åå¦ä½å©ç¨StreamingCoTè®­ç»åºçè½åï¼æ³åå°å¶ä»æ¨¡æï¼å¦é³é¢ãææ¬ï¼æ´å¤æçæ¶åºæ¨çä»»å¡ä¸­ã
*   <strong>å®æ¶æµåªä½æ¨ç</strong>ï¼å©ç¨StreamingCoTçå¨ææ æ³¨ç¹æ§ï¼å¼åè½å¤è¿è¡å®æ¶ãå¢éå¼æ¨ççæ¨¡åï¼ä»¥éåºæµåªä½çä½å»¶è¿è¦æ±ã
*   <strong>å¤æ¨¡æCoTçå¯è§£éæ§è¯ä¼°</strong>ï¼è®¾è®¡æ´ç²¾ç»çææ åæ¹æ³æ¥è¯ä¼°å¤æ¨¡æCoTçè§£éæ§ï¼ä¾å¦ï¼éåæ¨çé¾çå®æ´æ§ãåç¡®æ§åå¯è¿½æº¯æ§ã
*   <strong>ç»åå¤é¨ç¥è¯</strong>ï¼æ¢ç´¢å¦ä½å°å¤é¨ç¥è¯åºä¸StreamingCoTç»åï¼ä»¥å¢å¼ºæ¨¡åå¤çéè¦å¸¸è¯æé¢åç¹å®ç¥è¯çæ¨çä»»å¡çè½åã</p>
<hr />
<p>è¿ä»½æè¦æ¨å¨å¨é¢æ¦æ¬è®ºæçæ ¸å¿åå®¹ï¼çªåºå¶å¨æµåªä½è§é¢çè§£åå¤æ¨¡ææç»´é¾æ¨çé¢åçè´¡ç®ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address these challenges, We introduce
StreamingCoT, the first dataset explicitly designed for temporally evolving
reasoning in streaming VideoQA and multimodal Chain-of-Thought (CoT) tasks.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.25332v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.25332v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.25314v1'></a></p>
<h2 id="seeing-clearly-and-deeply-an-rgbd-imaging-approach-with-a-bio-inspired-monocentric-design"><a href="https://arxiv.org/abs/2510.25314v1">Seeing Clearly and Deeply: An RGBD Imaging Approach with a Bio-inspired Monocentric Design</a></h2>
<p><strong>Authors:</strong> Zongxi Yu, Xiaolong Qian, Shaohua Gao, Qi Jiang, Yao Gao, Kailun Yang, Kaiwei Wang</p>
<p><strong>Published:</strong> 2025-10-29</p>
<p><strong>Categories:</strong> cs.CV, cs.RO, eess.IV, physics.optics</p>
<p><strong>Abstract:</strong></p>
<p>Achieving high-fidelity, compact RGBD imaging presents a dual challenge:
conventional compact optics struggle with RGB sharpness across the entire
depth-of-field, while software-only Monocular Depth Estimation (MDE) is an
ill-posed problem reliant on unreliable semantic priors. While deep optics with
elements like DOEs can encode depth, they introduce trade-offs in fabrication
complexity and chromatic aberrations, compromising simplicity. To address this,
we first introduce a novel bio-inspired all-spherical monocentric lens, around
which we build the Bionic Monocentric Imaging (BMI) framework, a holistic
co-design. This optical design naturally encodes depth into its depth-varying
Point Spread Functions (PSFs) without requiring complex diffractive or freeform
elements. We establish a rigorous physically-based forward model to generate a
synthetic dataset by precisely simulating the optical degradation process. This
simulation pipeline is co-designed with a dual-head, multi-scale reconstruction
network that employs a shared encoder to jointly recover a high-fidelity
All-in-Focus (AiF) image and a precise depth map from a single coded capture.
Extensive experiments validate the state-of-the-art performance of the proposed
framework. In depth estimation, the method attains an Abs Rel of 0.026 and an
RMSE of 0.130, markedly outperforming leading software-only approaches and
other deep optics systems. For image restoration, the system achieves an SSIM
of 0.960 and a perceptual LPIPS score of 0.082, thereby confirming a superior
balance between image fidelity and depth accuracy. This study illustrates that
the integration of bio-inspired, fully spherical optics with a joint
reconstruction algorithm constitutes an effective strategy for addressing the
intrinsic challenges in high-performance compact RGBD imaging. Source code will
be publicly available at https://github.com/ZongxiYu-ZJU/BMI.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°å¯¹è¿ç¯è®ºææè¦è¿è¡æ·±å¥åæã</p>
<hr />
<p><strong>è®ºææè¦åæï¼Seeing Clearly and Deeply: An RGBD Imaging Approach with a Bio-inspired Monocentric Design</strong></p>
<p><strong>1. è®ºæä¸»è¦è´¡ç®çç®æ´æ»ç» (2-3 å¥è¯)</strong></p>
<p>è¿ç¯è®ºææåºäºä¸ç§åä¸ºâä»¿çåä¸­å¿æå (BMI)âçæ´ä½ååè®¾è®¡æ¡æ¶ï¼ç¨äºå®ç°é«ä¿çãç´§åç RGBD æåãå¶æ ¸å¿è´¡ç®å¨äºå¼å¥äºä¸ç§æ°åçãåçç©å¯åçå¨çé¢åä¸­å¿ééè®¾è®¡ï¼è¯¥è®¾è®¡è½èªç¶å°å°æ·±åº¦ä¿¡æ¯ç¼ç å°å¶æ·±åº¦ååç PSF ä¸­ï¼å¹¶ç»åä¸ä¸ªåå¤´ãå¤å°ºåº¦éå»ºç½ç»ï¼ä»åæ¬¡ç¼ç æè·ä¸­èåæ¢å¤é«ä¿çå¨èç¦ (AiF) å¾ååç²¾ç¡®æ·±åº¦å¾ãè¯¥æ¹æ³å¨æ·±åº¦ä¼°è®¡åå¾åæ¢å¤æ¹é¢ååå¾äºæåè¿çæ§è½ï¼è§£å³äºä¼ ç»ç´§ååå­¦åçº¯è½¯ä»¶ MDE çåºæææã</p>
<p><strong>2. å³é®åæ°ææ¹æ³å­¦æ¹æ³</strong></p>
<p>è¯¥è®ºæçå³é®åæ°åæ¹æ³å­¦æ¹æ³ä½ç°å¨ä»¥ä¸å ä¸ªæ¹é¢ï¼</p>
<ul>
<li><strong>çç©å¯åå¼å¨çé¢åä¸­å¿ééè®¾è®¡ï¼</strong> è¿æ¯æ ¸å¿çåå­¦åæ°ãä¸ä¼ ç»çå¤æè¡å°æèªç±æ²é¢åå­¦åä»¶ä¸åï¼è¿ç§è®¾è®¡å©ç¨å¶åºæçåå­¦ç¹æ§ï¼å°æ·±åº¦ä¿¡æ¯ç¼ç å°å¶æ·±åº¦ååç PSF ä¸­ãè¿ç§âèªç¶ç¼ç âé¿åäºä¼ ç»æ·±åº¦åå­¦åä»¶å¨å¶é å¤ææ§åè²å·®æ¹é¢çæè¡¡ï¼ä»èå®ç°äºæ´ç®æ´ãæ´ç´§åçç³»ç»ã</li>
<li><strong>æ´ä½ååè®¾è®¡ (Holistic Co-design) æ¡æ¶ï¼</strong> è®ºæå¼ºè°äºåå­¦è®¾è®¡ä¸è®¡ç®éå»ºç®æ³çç´§å¯ç»åãè¿ä¸ä»ä»æ¯ç¡¬ä»¶åè½¯ä»¶çç®åç»åï¼èæ¯ä¸ç§ä»è®¾è®¡ä¹åå°±èèä¸¤èç¸äºä½ç¨çååä¼åã</li>
<li><strong>ç©çåºç¡çååæ¨¡åååææ°æ®éçæï¼</strong> ä¸ºäºè®­ç»æ·±åº¦å­¦ä¹ æ¨¡åï¼è®ºæå»ºç«äºä¸ä¸ªä¸¥æ ¼çç©çåºç¡ååæ¨¡åï¼ç²¾ç¡®æ¨¡æåå­¦éçº§è¿ç¨ï¼ä»èçæé«è´¨éçåææ°æ®éãè¿å¯¹äºå¨æ²¡æå¤§éçå®ä¸çæ°æ®çæåµä¸å¼ååéªè¯ç®æ³è³å³éè¦ï¼å°¤å¶æ¯å¨æ°ååå­¦ç³»ç»è®¾è®¡ä¸­ã</li>
<li><strong>åå¤´ãå¤å°ºåº¦éå»ºç½ç»ï¼</strong> è¿æ¯ä¸ä¸ªè®¡ç®åæ°ãè¯¥ç½ç»éç¨å±äº«ç¼ç å¨ï¼è½å¤åæ¶ä»åæ¬¡ç¼ç æè·ä¸­æ¢å¤ä¸¤ä¸ªå³é®è¾åºï¼é«ä¿çå¨èç¦å¾åï¼è§£å³æ¯æ·±éå¶ï¼åç²¾ç¡®æ·±åº¦å¾ï¼è§£å³æ·±åº¦ä¼°è®¡é®é¢ï¼ãå¤å°ºåº¦è®¾è®¡å¯è½æå©äºææä¸åå°ºåº¦çç¹å¾ï¼ä»èæé«éå»ºè´¨éã</li>
<li><strong>åæ¬¡ç¼ç æè·ï¼</strong> å¼ºè°äºä»ä¸æ¬¡æè·ä¸­è·åææä¿¡æ¯çè½åï¼è¿å¯¹äºå®æ¶åºç¨åç´§åç³»ç»è³å³éè¦ï¼é¿åäºå¤å¸§æè·çå¤ææ§ã</li>
</ul>
<p><strong>3. å¯¹é¢åæ½å¨å½±å</strong></p>
<p>è¿ç¯è®ºæå¯¹è®¡ç®æºè§è§åç¸å³é¢åå·æä»¥ä¸æ½å¨å½±åï¼</p>
<ul>
<li><strong>æ¨å¨ç´§åå RGBD ä¼ æå¨çåå±ï¼</strong> è§£å³äºä¼ ç»ç´§ååå­¦å¨æ¯æ·±åçº¯è½¯ä»¶ MDE å¨å¯é æ§æ¹é¢ççç¹ï¼ä¸ºå¼åæ´å°ãæ´è½»ãæ§è½æ´å¥½ç RGBD ä¼ æå¨æä¾äºæ°çèå¼ãè¿å¯¹äºç§»å¨è®¾å¤ãæºå¨äººãAR/VR å¤´æ¾ç­åºç¨è³å³éè¦ã</li>
<li><strong>éæ°æèåå­¦ä¸è®¡ç®çååè®¾è®¡ï¼</strong> å¼ºè°äºåå­¦è®¾è®¡å¨æ·±åº¦ä¿¡æ¯ç¼ç ä¸­çä½ç¨ï¼å¹¶å±ç¤ºäºå¦ä½éè¿ä¸è®¡ç®ç®æ³çç´§å¯ç»åæ¥è§£éæ°çæ§è½æ°´å¹³ãè¿å¯è½ä¼é¼å±æ´å¤çç ç©¶äººåå¨åå­¦åç®æ³å±é¢è¿è¡èååæ°ï¼èä¸æ¯å°å®ä»¬è§ä¸ºç¬ç«çç»ä»¶ã</li>
<li><strong>ä¸ºçç©å¯åå¼åå­¦è®¾è®¡æä¾æ°æè·¯ï¼</strong> è¯æäºä»çç©ç³»ç»ä¸­æ±²åçµæå¯ä»¥å¸¦æ¥çªç ´æ§çåå­¦è®¾è®¡ï¼å°¤å¶æ¯å¨è§£å³ä¼ ç»åå­¦æææ¹é¢ã</li>
<li><strong>æååç®æ·±åº¦ä¼°è®¡çå¯é æ§ï¼</strong> éè¿å°æ·±åº¦ä¿¡æ¯ç¼ç å°åå­¦ PSF ä¸­ï¼è¯¥æ¹æ³ä¸ºåç®æ·±åº¦ä¼°è®¡æä¾äºæ´å¯é çç©çåºç¡ï¼åå°äºå¯¹ä¸å¯é è¯­ä¹åéªçä¾èµï¼ä»èå¯è½æé« MDE å¨å¤æåºæ¯ä¸çé²æ£æ§ååç¡®æ§ã</li>
<li><strong>ä¿è¿åææ°æ®çæåç©çå»ºæ¨¡ï¼</strong> è®ºæä¸­ä¸¥è°¨çç©çååæ¨¡åååææ°æ®éçææ¹æ³ï¼ä¸ºå¶ä»åå­¦-è®¡ç®æåç³»ç»çå¼åæä¾äºå®è´µçç»éªåèä¾ã</li>
</ul>
<p><strong>4. ç¸å³é¢åæåºç¨åç</strong></p>
<ul>
<li><strong>ç§»å¨è®¡ç®åæºè½ææºï¼</strong> ç´§åãé«æ§è½ç RGBD ä¼ æå¨å¯ä»¥æ¾èæåææºçæç§ä½éªï¼ä¾å¦æ´å¥½çèæ¯èåã3D æ«æè½åï¼å AR åºç¨ã</li>
<li><strong>æºå¨äººåèªä¸»ç³»ç»ï¼</strong> ç²¾ç¡®çæ·±åº¦æç¥å¯¹äºæºå¨äººçå¯¼èªãé¿éãç©ä½æååç¯å¢çè§£è³å³éè¦ãç´§åçä¼ æå¨å¯ä»¥æ´å®¹æå°éæå°å°åæºå¨äººææ äººæºä¸­ã</li>
<li><strong>å¢å¼ºç°å® (AR) åèæç°å® (VR)ï¼</strong> é«è´¨éç RGBD æ°æ®æ¯å®ç°æ²æµ¸å¼ AR/VR ä½éªçå³é®ï¼åæ¬ç¯å¢çè§£ãæå¿è¯å«åèæç©ä½ä¸çå®ä¸ççèåã</li>
<li><strong>å»çæåï¼</strong> æäºå»çåºç¨å¯è½åçäºé«ä¿çãç´§åç 3D æåç³»ç»ã</li>
<li><strong>å·¥ä¸æ£æµåè´¨éæ§å¶ï¼</strong> ç²¾ç¡®ç 3D æµéåè¡¨é¢ç¼ºé·æ£æµã</li>
<li><strong>è®¡ç®æå½±ï¼</strong> æåæ¯æ·±æ©å±ãååºæåç­é«çº§æå½±åè½ã</li>
</ul>
<p><strong>5. ä»æè¦ä¸­æ¨æ­åºçå±éæ§</strong></p>
<p>å°½ç®¡æè¦å±ç¤ºäºä»¤äººå°è±¡æ·±å»çææï¼ä½ä»å¯ä»¥æ¨æ­åºä¸äºæ½å¨çå±éæ§ï¼</p>
<ul>
<li><strong>å¶é å¤ææ§ï¼ç¸å¯¹èè¨ï¼ï¼</strong> å°½ç®¡æè¦å£°ç§°é¿åäºâå¤æè¡å°æèªç±æ²é¢åä»¶âï¼ä½âçç©å¯åå¼å¨çé¢åä¸­å¿ééâæ¬èº«å¯è½ä»éè¦é«ç²¾åº¦çå¶é å·¥èºï¼å°¤å¶æ¯å¨å®ç°å¶æ·±åº¦ç¼ç ç¹æ§æ¹é¢ãä¸æ åçé¢ééç¸æ¯ï¼å¶å¶é é¾åº¦å¯è½æ´é«ã</li>
<li><strong>è®¡ç®ææ¬ï¼</strong> åå¤´ãå¤å°ºåº¦éå»ºç½ç»ï¼å°¤å¶æ¯å¨å¤çé«åè¾¨çå¾åæ¶ï¼å¯è½éè¦æ¾èçè®¡ç®èµæºåæ¶é´ãè½ç¶æè¦å¼ºè°äºâåæ¬¡ç¼ç æè·âï¼ä½å®æ¶å¤çè½åï¼ä¾å¦å¨åµå¥å¼è®¾å¤ä¸ï¼ä»éè¿ä¸æ­¥éªè¯ã</li>
<li><strong>æ³åè½åï¼</strong> å°½ç®¡ä½¿ç¨äºç©çåºç¡çåææ°æ®éï¼ä½æ¨¡åå¨é¢å¯¹ä¸è®­ç»æ°æ®åå¸å·®å¼è¾å¤§ççå®ä¸çå¤æåºæ¯ï¼ä¾å¦æç«¯åç§ãçº¹çç¼ºå¤±ãé«åº¦åå°è¡¨é¢ï¼æ¶çæ³åè½åä»æå¾å¨é¢è¯ä¼°ã</li>
<li><strong>ç³»ç»æ ¡åï¼</strong> ä»»ä½åå­¦-è®¡ç®ååç³»ç»é½éè¦ç²¾ç¡®çæ ¡åãè¿ç§æ°ååå­¦è®¾è®¡å¯è½éè¦ä¸å¥ä¸é¨çæ ¡åæµç¨ï¼å¶å¤ææ§æªç¥ã</li>
<li><strong>å¯¹ç¹å®åå­¦è®¾è®¡çä¾èµï¼</strong> æ§è½çä¼è¶æ§å¯è½é«åº¦ä¾èµäºææåºçç¹å®âä»¿çåä¸­å¿ééâè®¾è®¡ãå°è¯¥æ¡æ¶åºç¨äºå¶ä»åå­¦ç³»ç»å¯è½éè¦éæ°è®¾è®¡åä¼åã</li>
<li><strong>âç¼ç æè·âçæ§è´¨ï¼</strong> æè¦ä¸­æå°âåæ¬¡ç¼ç æè·âï¼ä½æ²¡æè¯¦ç»è¯´æç¼ç çå·ä½æ¹å¼ï¼ä¾å¦ï¼æ¯å¦æ¶åç¹å®çç§ææ¨¡å¼ãä¼ æå¨éµåæåå­¦è°å¶ï¼ãè¿ç§ç¼ç å¯è½ä¼å¯¹æè·éåº¦ãä¿¡åªæ¯æå¯¹ç¯å¢åçé²æ£æ§äº§çå½±åã</li>
</ul>
<hr />
<p>æ»èè¨ä¹ï¼è¿ç¯è®ºæéè¿å°åæ°ççç©å¯åå¼åå­¦è®¾è®¡ä¸åè¿çæ·±åº¦å­¦ä¹ éå»ºç®æ³ç¸ç»åï¼ä¸ºç´§åå RGBD æåæä¾äºä¸ä¸ªå¼äººæ³¨ç®çæ°æ¹åãå¶å¨æ§è½ä¸çæ¾èæåé¢ç¤ºçå¨å¤ä¸ªåºç¨é¢åå·æå·¨å¤§çæ½åï¼å¹¶å¯è½æ¿ååå­¦åè®¡ç®æºè§è§é¢åæ´æ·±å±æ¬¡çååç ç©¶ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this,
we first introduce a novel bio-inspired all-spherical monocentric lens, around
which we build the Bionic Monocentric Imaging (BMI) framework, a holistic
co-design.</li>
<li>Extensive experiments validate the state-of-the-art performance of the proposed
framework.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.25314v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.25314v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.25268v1'></a></p>
<h2 id="synhlmasynthesizing-hand-language-manipulation-for-articulated-object-with-discrete-human-object-interaction-representation"><a href="https://arxiv.org/abs/2510.25268v1">SynHLMA:Synthesizing Hand Language Manipulation for Articulated Object with Discrete Human Object Interaction Representation</a></h2>
<p><strong>Authors:</strong> Wang zhi, Yuyan Liu, Liu Liu, Li Zhang, Ruixuan Lu, Dan Guo</p>
<p><strong>Published:</strong> 2025-10-29</p>
<p><strong>Categories:</strong> cs.RO, cs.AI, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Generating hand grasps with language instructions is a widely studied topic
that benefits from embodied AI and VR/AR applications. While transferring into
hand articulatied object interaction (HAOI), the hand grasps synthesis requires
not only object functionality but also long-term manipulation sequence along
the object deformation. This paper proposes a novel HAOI sequence generation
framework SynHLMA, to synthesize hand language manipulation for articulated
objects. Given a complete point cloud of an articulated object, we utilize a
discrete HAOI representation to model each hand object interaction frame. Along
with the natural language embeddings, the representations are trained by an
HAOI manipulation language model to align the grasping process with its
language description in a shared representation space. A joint-aware loss is
employed to ensure hand grasps follow the dynamic variations of articulated
object joints. In this way, our SynHLMA achieves three typical hand
manipulation tasks for articulated objects of HAOI generation, HAOI prediction
and HAOI interpolation. We evaluate SynHLMA on our built HAOI-lang dataset and
experimental results demonstrate the superior hand grasp sequence generation
performance comparing with state-of-the-art. We also show a robotics grasp
application that enables dexterous grasps execution from imitation learning
using the manipulation sequence provided by our SynHLMA. Our codes and datasets
will be made publicly available.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼è¿æ¯ä¸ç¯å³äºWang Zhi, Yuyan Liu, Liu Liu, Li Zhang, Ruixuan Lu, Dan Guoæ°åçè®ºæâSynHLMA: Synthesizing Hand Language Manipulation for Articulated Object with Discrete Human Object Interaction Representationâçå¨é¢æè¦ï¼</p>
<p><strong>è®ºææè¦ï¼SynHLMAï¼åºäºç¦»æ£äººæºäº¤äºè¡¨ç¤ºçå³èç©ä½æè¯­æä½åæ</strong></p>
<p><strong>1. è§£å³çä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³å¨å·èº«AIåVR/ARåºç¨ä¸­ï¼æ ¹æ®è¯­è¨æä»¤çææé¨æåï¼hand graspsï¼çææï¼ç¹å«æ¯éå¯¹<strong>å³èç©ä½æé¨äº¤äºï¼Hand Articulated Object Interaction, HAOIï¼</strong>ãç°æçæååææ¹æ³éå¸¸åªå³æ³¨ç©ä½åè½æ§ï¼èå¿½ç¥äºç©ä½åå½¢è¿ç¨ä¸­çé¿ææä½åºåãå æ­¤ï¼æ ¸å¿é®é¢æ¯å¦ä½åæç¬¦åè¯­è¨æä»¤ãèèç©ä½å³èå¨æååãå¹¶è½çæé¿ææä½åºåçHAOIã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
SynHLMAæ¡æ¶æåºäºä»¥ä¸å³é®åæ°ï¼
*   <strong>ç¦»æ£HAOIè¡¨ç¤ºï¼</strong> å¼å¥äºä¸ç§æ°é¢çç¦»æ£HAOIè¡¨ç¤ºï¼ç¨äºå»ºæ¨¡æ¯ä¸ªæé¨ç©ä½äº¤äºå¸§ãè¿ç§è¡¨ç¤ºéè¿å¤é¶æ®µVQ-VAEï¼åå«å³èæç¥çº¦æï¼å°å®æ´çæåè½¨è¿¹ç¦»æ£åä¸ºtokenåºåï¼ç¼ç æé¨ä½ç½®ãå§¿æãè°æ´åç©ä½éç½®ã
*   <strong>HAOIæä½è¯­è¨æ¨¡åï¼</strong> å°ç¦»æ£è¡¨ç¤ºä¸èªç¶è¯­è¨åµå¥ç¸ç»åï¼éè¿ä¸ä¸ªHAOIæä½è¯­è¨æ¨¡åè¿è¡è®­ç»ï¼ä»¥å¨å±äº«è¡¨ç¤ºç©ºé´ä¸­å¯¹é½æåè¿ç¨åå¶è¯­è¨æè¿°ãè¯¥æ¨¡åéç¨èªåå½ç­ç¥é¢æµå¢éå·®å¼ï¼ä»¥å®ç°é¿åºåçæã
*   <strong>å³èæç¥æå¤±ï¼Joint-aware lossï¼ï¼</strong> å¼å¥äºä¸ç§å³èæç¥æå¤±ï¼ä»¥ç¡®ä¿æé¨æåè½å¤éµå¾ªå³èç©ä½çå¨æååï¼ä»èæé«æåä¸ç©ä½å³èç©çç¶æåè¿å¨å­¦éç½®çä¸è´æ§ã
*   <strong>HAOI-Langæ°æ®éï¼</strong> æå»ºäºä¸ä¸ªæ°çHAOI-Langæ°æ®éï¼å¶ä¸­åå«ä¸°å¯çHAOIäº¤äºåºåï¼å¹¶å©ç¨GPT-4çæäºå¤æ ·åçèªç¶è¯­è¨æ³¨éï¼æ¶µçæåæå¾ãæ¹ååä½ç½®ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>åè¶çæ§è½ï¼</strong> SynHLMAå¨HAOIçæãHAOIé¢æµåHAOIæå¼è¿ä¸é¡¹å¸åæé¨æä½ä»»å¡ä¸åå¾äºä¼äºç°ææåè¿æ¹æ³çæ§è½ãå¨HAOIçæä»»å¡ä¸­ï¼FIDåæ°æé«äº4.919%ï¼å¤æ ·æ§å¢å äº13.986%ãå¨HAOIé¢æµä»»å¡ä¸­ï¼FIDæé«äº14.64%ï¼å¤æ ·æ§æé«äº19.572%ãå¨æå¼ä»»å¡ä¸­ï¼FIDéä½äº9.731%ï¼å¤æ ·æ§å¢å äº19.969%ã
*   <strong>æºå¨äººæååºç¨ï¼</strong> è®ºæå±ç¤ºäºSynHLMAçæçæçºµåºåå¯ä»¥ææå°æå¯¼æºå¨äººæååºç¨ä¸­çæ¨¡ä»¿å­¦ä¹ ï¼å®ç°çµå·§æåã
*   <strong>æ³åè½åï¼</strong> æ¨¡åå¨ä¸åå°ºå¯¸çå³èç©ä½ï¼å¦ç¼éãç¬è®°æ¬çµèãæå­ãæ´ç¢æºï¼ä¸è¡¨ç°åºå¼ºå¤§çæ³åè½åï¼è½å¤çæä¸åä½ç½®ãå¤æ ·æåæ¹åçåçHAOIå¨ä½ï¼å¹¶ä¿æé²æ£æ§è½ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
è®ºæä¸­æ²¡ææç¡®æåå½åæ¹æ³çå·ä½å±éæ§ãç¶èï¼ä»å¶æªæ¥å·¥ä½æ¹åå¯ä»¥æ¨æ­ï¼å½åæ¨¡åå¯è½å°æªå®å¨æ¶µçæ´ç²¾ç»ååè°çåæå¨æä½ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
æªæ¥çç ç©¶å·¥ä½å°è´åäºæ¢ç´¢æ´ç²¾ç»ååè°ç<strong>åæå¨æä½ï¼bimanual manipulationï¼</strong>ã</p>
<p>æ»èè¨ä¹ï¼SynHLMAéè¿å¼å¥ç¦»æ£HAOIè¡¨ç¤ºãå³èæç¥æå¤±åHAOIæä½è¯­è¨æ¨¡åï¼ä¸ºå³èç©ä½çè¯­è¨é©±å¨æé¨æä½åææä¾äºä¸ä¸ªæ°é¢ä¸é«æçæ¡æ¶ãå¶å¨HAOIçæãé¢æµåæå¼ä»»å¡ä¸çä¼å¼è¡¨ç°ï¼ä»¥åå¨æºå¨äººæåä¸­çåºç¨æ½åï¼é½çªæ¾äºè¯¥æ¹æ³å¨å·èº«AIåæºå¨äººé¢åçéè¦æä¹ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>This paper proposes a novel HAOI sequence generation
framework SynHLMA, to synthesize hand language manipulation for articulated
objects.</li>
<li>We evaluate SynHLMA on our built HAOI-lang dataset and
experimental results demonstrate the superior hand grasp sequence generation
performance comparing with state-of-the-art.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.25268v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.25268v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.25263v1'></a></p>
<h2 id="langhops-language-grounded-hierarchical-open-vocabulary-part-segmentation"><a href="https://arxiv.org/abs/2510.25263v1">LangHOPS: Language Grounded Hierarchical Open-Vocabulary Part Segmentation</a></h2>
<p><strong>Authors:</strong> Yang Miao, Jan-Nico Zaech, Xi Wang, Fabien Despinoy, Danda Pani Paudel, Luc Van Gool</p>
<p><strong>Published:</strong> 2025-10-29</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We propose LangHOPS, the first Multimodal Large Language Model (MLLM) based
framework for open-vocabulary object-part instance segmentation. Given an
image, LangHOPS can jointly detect and segment hierarchical object and part
instances from open-vocabulary candidate categories. Unlike prior approaches
that rely on heuristic or learnable visual grouping, our approach grounds
object-part hierarchies in language space. It integrates the MLLM into the
object-part parsing pipeline to leverage its rich knowledge and reasoning
capabilities, and link multi-granularity concepts within the hierarchies. We
evaluate LangHOPS across multiple challenging scenarios, including in-domain
and cross-dataset object-part instance segmentation, and zero-shot semantic
segmentation. LangHOPS achieves state-of-the-art results, surpassing previous
methods by 5.5% Average Precision (AP) (in-domain) and 4.8% (cross-dataset) on
the PartImageNet dataset and by 2.5% mIOU on unseen object parts in ADE20K
(zero-shot). Ablation studies further validate the effectiveness of the
language-grounded hierarchy and MLLM driven part query refinement strategy. The
code will be released here.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°å¯¹è¿ç¯è®ºææè¦è¿è¡æ·±å¥åæã</p>
<hr />
<p><strong>è®ºææè¦åæï¼LangHOPS: Language Grounded Hierarchical Open-Vocabulary Part Segmentation</strong></p>
<p><strong>1. è®ºæä¸»è¦è´¡ç®çç®æ´æ»ç» (2-3 å¥è¯)</strong></p>
<p>LangHOPS æåºäºé¦ä¸ªåºäºå¤æ¨¡æå¤§è¯­è¨æ¨¡å (MLLM) çå¼æ¾è¯æ±å¯¹è±¡-é¨ä»¶å®ä¾åå²æ¡æ¶ãå®è½å¤ä»å¼æ¾è¯æ±ç±»å«ä¸­èåæ£æµååå²å¾åä¸­çåå±å¯¹è±¡åé¨ä»¶å®ä¾ï¼å¶æ ¸å¿åæ°å¨äºå°å¯¹è±¡-é¨ä»¶å±çº§ç»ææ ¹æ¤äºè¯­è¨ç©ºé´ï¼èéä¾èµä¼ ç»çè§è§åç»æ¹æ³ãè¯¥æ¹æ³å¨å¤ä¸ªæææ§åºæ¯ä¸åå¾äºæåè¿çæ§è½ï¼æ¾èè¶è¶äºç°ææ¹æ³ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®º</strong></p>
<p>è¯¥è®ºæçå³é®åæ°å¨äºï¼</p>
<ul>
<li><strong>è¯­è¨æ ¹æ¤çåå±ç»æ (Language Grounded Hierarchy):</strong> ä¼ ç»æ¹æ³éå¸¸ä¾èµå¯åå¼æå¯å­¦ä¹ çè§è§åç»æ¥æå»ºå¯¹è±¡-é¨ä»¶å±çº§ãLangHOPS åå°è¿ç§å±çº§ç»æâæ ¹æ¤âäºè¯­è¨ç©ºé´ãè¿æå³çå®å©ç¨ MLLM ä¸°å¯çç¥è¯åæ¨çè½åæ¥çè§£åé¾æ¥ä¸åç²åº¦çæ¦å¿µï¼ä¾å¦ï¼âæ±½è½¦âåå«âè½¦è½®âï¼âè½¦è½®âåå«âè½®æ¯âï¼ï¼ä»èå¨è¯­è¨å±é¢å»ºç«èµ·å¯¹è±¡ä¸é¨ä»¶ä¹é´çè¯­ä¹å³ç³»ã</li>
<li><strong>MLLM é©±å¨çé¨ä»¶æ¥è¯¢ç»åç­ç¥ (MLLM Driven Part Query Refinement Strategy):</strong> æè¦ä¸­æå°ï¼MLLM è¢«æ´åå°å¯¹è±¡-é¨ä»¶è§£ææµç¨ä¸­ï¼ä»¥å©ç¨å¶ç¥è¯åæ¨çè½åãè¿æç¤º MLLM ä¸ä»ç¨äºçè§£å±çº§å³ç³»ï¼è¿å¯è½ç¨äºæå¯¼æç»åé¨ä»¶çæ¥è¯¢ååå²è¿ç¨ï¼ä½¿å¶æ´ç¬¦åè¯­è¨æè¿°çè¯­ä¹ã</li>
<li><strong>å¼æ¾è¯æ±è½å (Open-Vocabulary Capability):</strong> ç»å MLLM çå¼ºå¤§æ³åè½åï¼LangHOPS è½å¤å¤çå¨è®­ç»è¿ç¨ä¸­æªæ¾è§è¿çå¯¹è±¡åé¨ä»¶ç±»å«ï¼è¿å¯¹äºå®éåºç¨è³å³éè¦ã</li>
</ul>
<p><strong>3. å¯¹é¢åæ½å¨å½±å</strong></p>
<p>LangHOPS çç ç©¶å¯¹è®¡ç®æºè§è§é¢åå·æä»¥ä¸æ½å¨å½±åï¼</p>
<ul>
<li><strong>æ¨å¨å¼æ¾è¯æ±åå²ææ¯åå±ï¼</strong> å° MLLM å¼å¥å¯¹è±¡-é¨ä»¶åå²ï¼æå¤§å°æåäºæ¨¡åå¤çæªç¥ç±»å«åå¤æè¯­ä¹å³ç³»çè½åï¼ä¸ºå¼æ¾è¯æ±åå²è®¾å®äºæ°çåºåã</li>
<li><strong>æ·±åè¯­è¨ä¸è§è§çèåï¼</strong> å¼ºè°äºè¯­è¨å¨çè§£å¤æè§è§ç»æï¼å¦å±çº§å³ç³»ï¼ä¸­çæ ¸å¿ä½ç¨ï¼ä¸ºæªæ¥çå¤æ¨¡æç ç©¶æä¾äºæ°çèå¼ï¼å³è¯­è¨ä¸ä»ä»æ¯æ ç­¾ï¼æ´æ¯ç»æåæ¨ççè½½ä½ã</li>
<li><strong>æåç»ç²åº¦çè§£è½åï¼</strong> è½å¤åç¡®å°åå²åçè§£å¯¹è±¡åé¨çé¨ä»¶ç»æï¼å¯¹äºéè¦ç²¾ç»äº¤äºååæçåºç¨ï¼å¦æºå¨äººæä½ãAR/VRãå»å­¦å¾ååæï¼å·æéè¦ä»·å¼ã</li>
<li><strong>å¯åæ°çæ¨¡åæ¶æï¼</strong> å° MLLM ä½ä¸ºæ ¸å¿ç»ä»¶æ´åå°ä¼ ç»çåå²æµç¨ä¸­ï¼å¯è½ä¼å¯åæ´å¤å°å¤§åé¢è®­ç»æ¨¡åï¼å°¤å¶æ¯è¯­è¨æ¨¡åï¼ä¸ç¹å®è§è§ä»»å¡æ·±åº¦ç»åçæ°æ¶æè®¾è®¡ã</li>
</ul>
<p><strong>4. ç¸å³é¢åæåºç¨</strong></p>
<p>ä»¥ä¸é¢åæåºç¨å¯è½ä¼ä»è¿é¡¹ç ç©¶ä¸­åçï¼</p>
<ul>
<li><strong>æºå¨äººå­¦åå·èº«æºè½ (Robotics and Embodied AI):</strong> æºå¨äººéè¦çè§£ç©ä½çé¨ä»¶ç»ææè½è¿è¡ç²¾ç»æä½ãæååç»è£ãLangHOPS å¯ä»¥å¸®å©æºå¨äººæ´å¥½å°çè§£ç¯å¢ä¸­çç©ä½ã</li>
<li><strong>å¢å¼ºç°å®/èæç°å® (AR/VR):</strong> å¨èæç¯å¢ä¸­å®ç°æ´çå®çç©ä½äº¤äºåç¼è¾ï¼ä¾å¦ï¼ç¨æ·å¯ä»¥ç²¾ç¡®å°éæ©åä¿®æ¹èææ±½è½¦çæä¸ªé¨ä»¶ã</li>
<li><strong>å»å­¦å¾ååæ (Medical Image Analysis):</strong> ç²¾ç¡®åå²å¨å®çå­ç»ææçååºåï¼æå©äºè¯æ­åæ²»çè§åã</li>
<li><strong>èªå¨é©¾é©¶ (Autonomous Driving):</strong> è¯å«è½¦è¾çåä¸ªé¨ä»¶ï¼å¦è½¦è½®ãè½¦é¨ãåè§éï¼å¯¹äºæ´é«çº§å«çåºæ¯çè§£åå³ç­è³å³éè¦ã</li>
<li><strong>äº§åè®¾è®¡ä¸è´¨éæ£æµ (Product Design and Quality Inspection):</strong> èªå¨åæ£æµäº§åé¨ä»¶çå®æ´æ§åç¼ºé·ã</li>
<li><strong>å¾åç¼è¾ååå®¹çæ (Image Editing and Content Generation):</strong> å®ç°æ´ç²¾ç»çå¾åç¼è¾ï¼ä¾å¦ï¼åªä¿®æ¹å¾åä¸­æä¸ªç¹å®é¨ä»¶çé¢è²æçº¹çã</li>
</ul>
<p><strong>5. ä»æè¦ä¸­å¯æ¨æ­çå±éæ§</strong></p>
<p>å°½ç®¡æè¦å±ç¤ºäºä»¤äººå°è±¡æ·±å»çææï¼ä½ä»å¯æ¨æ­åºä¸äºæ½å¨å±éæ§ï¼</p>
<ul>
<li><strong>è®¡ç®èµæºéæ±ï¼</strong> MLLM éå¸¸è®¡ç®ææ¬é«æï¼æ´å MLLM å°åå²æµç¨ä¸­å¯è½ä¼å¯¼è´æ¨¡åè®­ç»åæ¨ççè®¡ç®èµæºéæ±å¤§å¹å¢å ï¼è¿å¯è½éå¶å¶å¨èµæºåéç¯å¢ä¸­çåºç¨ã</li>
<li><strong>æ°æ®ä¾èµæ§ï¼</strong> å°½ç®¡æ¯å¼æ¾è¯æ±ï¼ä½ MLLM çç¥è¯åæ¨çè½åä»ç¶ä¾èµäºå¶è®­ç»æ°æ®ãå¦ææäºç¹å®é¢åæéå¸¸è§çé¨ä»¶å³ç³»å¨ MLLM çè®­ç»æ°æ®ä¸­ä½ç°ä¸è¶³ï¼å¶æ§è½å¯è½ä¼åå°å½±åã</li>
<li><strong>âè¯­è¨æ ¹æ¤âçé²æ£æ§ï¼</strong> è¯­è¨çæ­§ä¹æ§åå¤ææ§å¯è½å¯¹âè¯­è¨æ ¹æ¤âçå±çº§ç»æå¸¦æ¥ææãä¾å¦ï¼æäºé¨ä»¶çæè¿°å¯è½å ä¸ä¸æèå¼ï¼æèå­å¨å¤ç§ææçè¯­è¨æè¿°æ¹å¼ãå¦ä½ç¡®ä¿ MLLM å¨åç§è¯­è¨è¡¨è¾¾ä¸é½è½ç¨³å®å°çè§£åæå»ºå±çº§ï¼æ¯ä¸ä¸ªæ½å¨é®é¢ã</li>
<li><strong>å¯¹ MLLM åé¨æºå¶çä¾èµï¼</strong> æ¨¡åçæ§è½é«åº¦ä¾èµäºæä½¿ç¨ç MLLM çè½åãå¦æ MLLM å­å¨åè§ææ¨çéè¯¯ï¼è¿äºé®é¢å¯è½ä¼ä¼ éå°åå²ç»æä¸­ã</li>
<li><strong>å®æ¶æ§ï¼</strong> é´äº MLLM çå¤ææ§ï¼LangHOPS å¯è½é¾ä»¥å®ç°å®æ¶æè¿å®æ¶çåå²ï¼è¿å¯¹äºæäºéè¦å¿«éååºçåºç¨ï¼å¦èªå¨é©¾é©¶ï¼å¯è½æ¯ä¸ä¸ªéå¶ã</li>
<li><strong>âå¯åå¼æå¯å­¦ä¹ è§è§åç»âçå®å¨æ¿ä»£ï¼</strong> æè¦ä¸­æå°âUnlike prior approaches that rely on heuristic or learnable visual groupingâï¼è¿æç¤º LangHOPS å¯è½å¨å¾å¤§ç¨åº¦ä¸æè±äºå¯¹çº¯è§è§åç»çä¾èµãç¶èï¼å®å¨è±ç¦»è§è§ç¹å¾æ¥å®ä¹é¨ä»¶è¾¹çå¯è½ä¸ç°å®ï¼æ¨¡åå¯è½ä»ç¶éè¦æç§å½¢å¼çè§è§çº¿ç´¢æ¥ç²¾ç¡®å°å®ä½ååå²é¨ä»¶ãå·ä½å¦ä½å¹³è¡¡è¯­è¨æå¯¼åè§è§ç»èï¼æ¯å¼å¾å³æ³¨çæ¹é¢ã</li>
</ul>
<hr />
<p>æ»èè¨ä¹ï¼LangHOPS ä»£è¡¨äºå¼æ¾è¯æ±å¯¹è±¡-é¨ä»¶åå²é¢åçä¸ä¸ªéè¦è¿æ­¥ï¼éè¿å° MLLM çå¼ºå¤§è½åå¼å¥å°å±çº§ç»æçè§£ä¸­ï¼ä¸ºæªæ¥çå¤æ¨¡æè§è§çè§£ä»»å¡å¼è¾äºæ°çéè·¯ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose LangHOPS, the first Multimodal Large Language Model (MLLM) based
framework for open-vocabulary object-part instance segmentation.</li>
<li>Unlike prior approaches
that rely on heuristic or learnable visual grouping, our approach grounds
object-part hierarchies in language space.</li>
<li>LangHOPS achieves state-of-the-art results, surpassing previous
methods by 5.5% Average Precision (AP) (in-domain) and 4.8% (cross-dataset) on
the PartImageNet dataset and by 2.5% mIOU on unseen object parts in ADE20K
(zero-shot).</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.25263v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.25263v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.25175v1'></a></p>
<h2 id="test-time-adaptive-object-detection-with-foundation-model"><a href="https://arxiv.org/abs/2510.25175v1">Test-Time Adaptive Object Detection with Foundation Model</a></h2>
<p><strong>Authors:</strong> Yingjie Gao, Yanan Zhang, Zhi Cai, Di Huang</p>
<p><strong>Published:</strong> 2025-10-29</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>In recent years, test-time adaptive object detection has attracted increasing
attention due to its unique advantages in online domain adaptation, which
aligns more closely with real-world application scenarios. However, existing
approaches heavily rely on source-derived statistical characteristics while
making the strong assumption that the source and target domains share an
identical category space. In this paper, we propose the first foundation
model-powered test-time adaptive object detection method that eliminates the
need for source data entirely and overcomes traditional closed-set limitations.
Specifically, we design a Multi-modal Prompt-based Mean-Teacher framework for
vision-language detector-driven test-time adaptation, which incorporates text
and visual prompt tuning to adapt both language and vision representation
spaces on the test data in a parameter-efficient manner. Correspondingly, we
propose a Test-time Warm-start strategy tailored for the visual prompts to
effectively preserve the representation capability of the vision branch.
Furthermore, to guarantee high-quality pseudo-labels in every test batch, we
maintain an Instance Dynamic Memory (IDM) module that stores high-quality
pseudo-labels from previous test samples, and propose two novel
strategies-Memory Enhancement and Memory Hallucination-to leverage IDM's
high-quality instances for enhancing original predictions and hallucinating
images without available pseudo-labels, respectively. Extensive experiments on
cross-corruption and cross-dataset benchmarks demonstrate that our method
consistently outperforms previous state-of-the-art methods, and can adapt to
arbitrary cross-domain and cross-category target data. Code is available at
https://github.com/gaoyingjay/ttaod_foundation.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Yingjie Gao, Yanan Zhang, Zhi Cai, Zhi Cai, Di Huangæ°åçè®ºæâTest-Time Adaptive Object Detection with Foundation Modelâçå¨é¢æè¦ã</p>
<hr />
<h3 id="_1">è®ºææè¦ï¼åºäºåºç¡æ¨¡åçæµè¯æ¶èªéåºç®æ æ£æµ</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
ä¼ ç»çæµè¯æ¶èªéåºç®æ æ£æµï¼TTAODï¼æ¹æ³å­å¨ä¸¤ä¸ªä¸»è¦éå¶ï¼
1. å®ä»¬ä¸¥éä¾èµæºåçç»è®¡ç¹å¾ï¼è¿ä¸TTAçâæ æºæ°æ®âååç¸æã
2. å®ä»¬åè®¾æºååç®æ åå±äº«ç¸åçç±»å«ç©ºé´ï¼å°é­ééå¶ï¼ï¼è¿éå¶äºå¶å¨å¼æ¾åºæ¯ä¸­çéç¨æ§ã
æ¬ç ç©¶æ¨å¨è§£å³è¿äºé®é¢ï¼æåºä¸ç§æ éæºæ°æ®ãè½å¤éåºä»»æè·¨ååè·¨ç±»å«ç®æ æ°æ®çæµè¯æ¶èªéåºç®æ æ£æµæ¹æ³ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
è¯¥è®ºææåºäºé¦ä¸ªç±åºç¡æ¨¡åé©±å¨çæµè¯æ¶èªéåºç®æ æ£æµæ¹æ³ï¼å¶æ ¸å¿åæ°åæ¬ï¼</p>
<ul>
<li><strong>å¤æ¨¡ææç¤ºå¼å¹³åæå¸ï¼Multi-modal Prompt-based Mean-Teacherï¼æ¡æ¶ï¼</strong> éå¯¹è§è§-è¯­è¨æ£æµå¨é©±å¨çæµè¯æ¶èªéåºï¼è¯¥æ¡æ¶ç»åäºææ¬åè§è§æç¤ºå¾®è°ï¼ä»¥åæ°é«æçæ¹å¼åæ¶éåºæµè¯æ°æ®ä¸çè¯­è¨åè§è§è¡¨ç¤ºç©ºé´ã</li>
<li><strong>æµè¯æ¶ç­å¯å¨ï¼Test-time Warm-startï¼ç­ç¥ï¼</strong> ä¸é¨ä¸ºè§è§æç¤ºè®¾è®¡ï¼éè¿å¯¹ç¬¬ä¸ä¸ªæµè¯æ ·æ¬ä¸­æåçå¾åtokenè¿è¡å¹³åæ± åæ¥åå§åè§è§æç¤ºï¼ææä¿çè§è§åæ¯çè¡¨ç¤ºè½åï¼ä»èç¼è§£æ¬¡ä¼è§è§æç¤ºåå§åå¯¼è´çæ§è½ä¸éã</li>
<li><strong>å®ä¾å¨æè®°å¿ï¼Instance Dynamic Memory, IDMï¼æ¨¡åï¼</strong> ä¸ºäºç¡®ä¿æ¯ä¸ªæµè¯æ¹æ¬¡ä¸­é«è´¨éä¼ªæ ç­¾ççæï¼IDMæ¨¡åå­å¨æ¥èªååæµè¯æ ·æ¬çé«è´¨éä¼ªæ ç­¾ã</li>
<li><strong>è®°å¿å¢å¼ºï¼Memory Enhancementï¼åè®°å¿å¹»è§ï¼Memory Hallucinationï¼ç­ç¥ï¼</strong><ul>
<li><strong>è®°å¿å¢å¼ºï¼</strong> å©ç¨IDMä¸­å­å¨çé«è´¨éå®ä¾æ¥ç»åå½åæµè¯å¾åçåå§é¢æµã</li>
<li><strong>è®°å¿å¹»è§ï¼</strong> å°ä»IDMä¸­éæ ·çå®ä¾æ´åå°ç¼ºä¹å¯ç¨ä¼ªæ ç­¾çè´é¢æµè¯å¾åä¸­ï¼ä»¥çæåæçæ­£é¢æ ·æ¬ï¼ä»èè§£å³æäºæµè¯æ°æ®å¯è½æ²¡æå¯ç¨ä¼ªæ ç­¾è¿è¡éåºçé®é¢ã</li>
</ul>
</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>æ§è½è¶è¶SOTAï¼</strong> å¨è·¨æåï¼cross-corruptionï¼åè·¨æ°æ®éï¼cross-datasetï¼åºåä¸çå¹¿æ³å®éªè¡¨æï¼è¯¥æ¹æ³å§ç»ä¼äºååçæåè¿æ¹æ³ã
*   <strong>æ æºæ°æ®åå¼æ¾è¯æ±è½åï¼</strong> è¯¥æ¹æ³å®å¨æ¶é¤äºå¯¹æºæ°æ®çéæ±ï¼å¹¶æååæäºä¼ ç»å°é­ééå¶ï¼è½å¤éåºä»»æè·¨ååè·¨ç±»å«ç®æ æ°æ®ã
*   <strong>åæ°é«ææ§ï¼</strong> æåºçå¤æ¨¡ææç¤ºå¼å¹³åæå¸æ¡æ¶ä»éå¾®è°æå°éçåæ°ï¼0.05%ï¼ï¼åæ¶ä¿æäºé¢è®­ç»ç¥è¯ï¼å¹¶å¨å»¶è¿åGPUåå­å ç¨æ¹é¢ä¼äºå¨åæ°å¾®è°ã
*   <strong>ç»ä»¶æææ§ï¼</strong> æ¶èç ç©¶è¯å®äºæ¯ä¸ªç»ä»¶ï¼ææ¬æç¤ºå¾®è°ãè§è§æç¤ºå¾®è°ãæµè¯æ¶ç­å¯å¨ãè®°å¿å¢å¼ºãè®°å¿å¹»è§ï¼å¯¹æ´ä½æ§è½çè´¡ç®ï¼ç¹å«æ¯æµè¯æ¶ç­å¯å¨ç­ç¥ææç¼è§£äºè§è§æç¤ºåå§åé®é¢ï¼è®°å¿å¢å¼ºåè®°å¿å¹»è§æ¾èæåäºæ§è½ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
è®ºæä¸­æç¡®æå°ï¼å¨æäºè·¨æ°æ®éï¼å¦MuåPaï¼ä¸ï¼ç±äºå¯ç¨æµè¯æ ·æ¬æå°ï¼Muä¸º5ä¸ªï¼Paä¸º4ä¸ªï¼ï¼å¯¼è´è§è§-è¯­è¨æ£æµå¨æ æ³ååéåºç®æ åï¼ä»èå½±åäºæ§è½ãè¿è¡¨æå¨æ°æ®éæå°çæåµä¸ï¼æ¨¡åçéåºè½åä»å¯è½åéã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
è®ºæä¸­æªæç¡®æåºæªæ¥çç ç©¶æ¹åï¼ä½ä»å¶è´¡ç®åå±éæ§å¯ä»¥æ¨æ­åºä¸äºæ½å¨æ¹åï¼
*   <strong>å°æ ·æ¬/é¶æ ·æ¬éåºçé²æ£æ§ï¼</strong> éå¯¹æ°æ®éæå°çç®æ åï¼è¿ä¸æ­¥æåæ¨¡åçéåºè½ååé²æ£æ§ã
*   <strong>æçä¼åï¼</strong> å°½ç®¡æ¬ææ¹æ³å·²åæ°é«æï¼ä½ä»å¯æ¢ç´¢æ´åè¿çæçä¼åç­ç¥ï¼å¦ç»åSkipç­ç¥ï¼ï¼ä»¥è¿ä¸æ­¥éä½æ¨çå»¶è¿ï¼å°¤å¶æ¯å¨å®æ¶åºç¨ä¸­ã
*   <strong>æ´å¤æçè·¨æ¨¡æäº¤äºï¼</strong> æ¢ç´¢æ´å¤æçææ¬åè§è§æç¤ºäº¤äºæºå¶ï¼ä»¥å®ç°æ´ç²¾ç»çè¡¨ç¤ºç©ºé´éåºã
*   <strong>çè®ºåæï¼</strong> å¯¹ææåºçå¤æ¨¡ææç¤ºå¼å¹³åæå¸æ¡æ¶ãIDMåå¶ç­ç¥ççè®ºåºç¡è¿è¡æ´æ·±å¥çåæï¼ä»¥æ´å¥½å°çè§£å¶å·¥ä½åçåæ³åè½åã</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this paper, we propose the first foundation
model-powered test-time adaptive object detection method that eliminates the
need for source data entirely and overcomes traditional closed-set limitations.</li>
<li>Furthermore, to guarantee high-quality pseudo-labels in every test batch, we
maintain an Instance Dynamic Memory (IDM) module that stores high-quality
pseudo-labels from previous test samples, and propose two novel
strategies-Memory Enhancement and Memory Hallucination-to leverage IDM's
high-quality instances for enhancing original predictions and hallucinating
images without available pseudo-labels, respectively.</li>
<li>Extensive experiments on
cross-corruption and cross-dataset benchmarks demonstrate that our method
consistently outperforms previous state-of-the-art methods, and can adapt to
arbitrary cross-domain and cross-category target data.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.25175v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.25175v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.25094v1'></a></p>
<h2 id="visual-diversity-and-region-aware-prompt-learning-for-zero-shot-hoi-detection"><a href="https://arxiv.org/abs/2510.25094v1">Visual Diversity and Region-aware Prompt Learning for Zero-shot HOI Detection</a></h2>
<p><strong>Authors:</strong> Chanhyeong Yang, Taehoon Song, Jihwan Park, Hyunwoo J. Kim</p>
<p><strong>Published:</strong> 2025-10-29</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Zero-shot Human-Object Interaction detection aims to localize humans and
objects in an image and recognize their interaction, even when specific
verb-object pairs are unseen during training. Recent works have shown promising
results using prompt learning with pretrained vision-language models such as
CLIP, which align natural language prompts with visual features in a shared
embedding space. However, existing approaches still fail to handle the visual
complexity of interaction, including (1) intra-class visual diversity, where
instances of the same verb appear in diverse poses and contexts, and (2)
inter-class visual entanglement, where distinct verbs yield visually similar
patterns. To address these challenges, we propose VDRP, a framework for Visual
Diversity and Region-aware Prompt learning. First, we introduce a visual
diversity-aware prompt learning strategy that injects group-wise visual
variance into the context embedding. We further apply Gaussian perturbation to
encourage the prompts to capture diverse visual variations of a verb. Second,
we retrieve region-specific concepts from the human, object, and union regions.
These are used to augment the diversity-aware prompt embeddings, yielding
region-aware prompts that enhance verb-level discrimination. Experiments on the
HICO-DET benchmark demonstrate that our method achieves state-of-the-art
performance under four zero-shot evaluation settings, effectively addressing
both intra-class diversity and inter-class visual entanglement. Code is
available at https://github.com/mlvlab/VDRP.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Chanhyeong Yangç­äººæ°åçè®ºæâVisual Diversity and Region-aware Prompt Learning for Zero-shot HOI Detectionâçå¨é¢æè¦ã</p>
<hr />
<h3 id="hoi">è®ºææè¦ï¼é¶æ ·æ¬HOIæ£æµä¸­çè§è§å¤æ ·æ§ååºåæç¥æç¤ºå­¦ä¹ </h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³é¶æ ·æ¬äºº-ç©äº¤äºï¼HOIï¼æ£æµä¸­çæ ¸å¿ææãé¶æ ·æ¬HOIæ£æµè¦æ±æ¨¡åè½å¤è¯å«è®­ç»ä¸­æªæ¾è§è¿çå¨è¯-ç©ä½å¯¹çäº¤äºãç°ææ¹æ³å¨å¤çäº¤äºçè§è§å¤ææ§æ¹é¢å­å¨ä¸è¶³ï¼å·ä½è¡¨ç°ä¸ºï¼
1. <strong>ç±»åè§è§å¤æ ·æ§ï¼Intra-class visual diversityï¼ï¼</strong> åä¸å¨è¯çå®ä¾å¨å§¿æãå°ºåº¦ååºæ¯èæ¯ä¸å¯è½å·®å¼å·¨å¤§ï¼å¯¼è´åä¸éæåµå¥é¾ä»¥ææå¶ææè§è§åä½ã
2. <strong>ç±»é´è§è§çº ç¼ ï¼Inter-class visual entanglementï¼ï¼</strong> ä¸åçå¨è¯ï¼ä¾å¦âåâåâèâï¼å¯è½äº§çè§è§ä¸ç¸ä¼¼çæ¨¡å¼ï¼ä½¿å¾æ¨¡åé¾ä»¥åç¡®åºåã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
ä¸ºè§£å³ä¸è¿°ææï¼ä½èæåºäºä¸ä¸ªåä¸º<strong>VDRPï¼Visual Diversity and Region-aware Prompt learningï¼</strong> çæ°æ¡æ¶ï¼å¶ä¸»è¦åæ°ç¹åæ¬ï¼</p>
<ul>
<li>
<p><strong>è§è§å¤æ ·æ§æç¥æç¤ºå­¦ä¹ ï¼Visual Diversity-aware Prompt Learningï¼ï¼</strong></p>
<ul>
<li>éè¿å°<strong>ç»åè§è§æ¹å·®</strong>æ³¨å¥å°å¯å­¦ä¹ çä¸ä¸æåµå¥ä¸­ï¼æ¨¡åè½å¤æ´å¥½å°ææç±»åè§è§å¤æ ·æ§ã</li>
<li>è¿ä¸æ­¥åºç¨<strong>é«æ¯æ°å¨</strong>ï¼å¹¶æ ¹æ®è§è§æ¹å·®è¿è¡ç¼©æ¾ï¼é¼å±æç¤ºææå¨è¯çå¤ç§è§è§åä½ï¼ä»èæé«æ³åè½åã</li>
<li>éè¿è®¡ç®CLIPææ¬åµå¥çä½å¼¦ç¸ä¼¼åº¦ï¼å°è¯­ä¹ç¸ä¼¼çå¨è¯è¿è¡åç»ï¼ä»¥è·å¾æ´ç¨³å®çç»åæ¹å·®ä¼°è®¡ã</li>
</ul>
</li>
<li>
<p><strong>åºåæç¥æç¤ºå¢å¼ºï¼Region-aware Prompt Augmentationï¼ï¼</strong></p>
<ul>
<li>ä»<strong>äººãç©ä½åèååºå</strong>ä¸­æ£ç´¢åºåç¹å®æ¦å¿µï¼ä»¥å¢å¼ºå¤æ ·æ§æç¥æç¤ºåµå¥ã</li>
<li>å©ç¨å¤§åè¯­è¨æ¨¡åï¼LLMsï¼çæè¿äºåºåç¹å®æ¦å¿µï¼å¹¶éè¿CLIPææ¬ç¼ç å¨å°å¶ç¼ç ä¸ºæ¦å¿µæ± ã</li>
<li>éè¿è®¡ç®åºåç¹å¾ä¸æ¦å¿µæ± ä¸­æ¦å¿µçä½å¼¦ç¸ä¼¼åº¦ï¼å¹¶åºç¨<strong>Sparsemax</strong>æ¿æ´»å½æ°è¿è¡ç¨çéæ©ï¼ä»¥çªåºæç¸å³çæ¦å¿µï¼ä»èçæåºåæç¥æ¦å¿µåéã</li>
<li>å°è¿äºåºåæç¥æ¦å¿µåéä¸å¤æ ·æ§æç¥æç¤ºèåï¼çææç»çåºåæç¥æç¤ºï¼æ¾èæé«å¨è¯çº§å«çå¤å«è½åï¼å°¤å¶æ¯å¨è§è§ç¸ä¼¼çå¨è¯ä¹é´ã</li>
</ul>
</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
è¯¥æ¹æ³å¨HICO-DETåºåæµè¯ä¸è¿è¡äºå¹¿æ³å®éªï¼å¹¶å¨åç§é¶æ ·æ¬è¯ä¼°è®¾ç½®ï¼NF-UCãRF-UCãUOãUVï¼ä¸ååå¾äº<strong>æåè¿ï¼state-of-the-artï¼çæ§è½</strong>ã
*   å¨NF-UCåRF-UCè®¾ç½®ä¸ï¼VDRPå¨ææææ ä¸åè¡¨ç°æä½³ï¼æ¾èä¼äºCLIP4HOIåEZ-HOIç­ç°ææ¹æ³ã
*   å¨UOåUVè®¾ç½®ä¸ï¼VDRPä¹åå¾äºæä½³çHMåUnseen mAPåæ°ï¼åæ¬¡éªè¯äºå¶æææ§ã
*   æ¶èç ç©¶è¯å®ï¼è§è§å¤æ ·æ§æç¥æç¤ºï¼VDPï¼ååºåæç¥æç¤ºï¼RAPï¼æ¨¡åé½å¯¹æ§è½æåååºäºäºè¡¥è´¡ç®ã
*   VDPä¸­çæ¹å·®-onlyå»ºæ¨¡ä¼äºåå¼-æ¹å·®ç»åï¼è¡¨ææ¹å·®æ¬èº«æ¯ææç±»åå¤æ ·æ§çå¼ºä¿¡å·ã
*   RAPä¸­çäººãç©ä½åèååºååæ¯çç»åä¼äºä»»ä½åä¸åæ¯ï¼å¼ºè°äºåºåçº§æ¦å¿µçäºè¡¥æ§ã
*   ä½¿ç¨ChatGPT-4çæçç©ä½æ¦å¿µæ¯LLaMA-7Bçæçæ¦å¿µæ´å·å¯¹è±¡ä¸­å¿æ§ï¼è¿ä¸æ­¥æåäºæ¨¡åæ§è½ï¼è¡¨ææ¦å¿µè´¨éçéè¦æ§ã
*   è¯¥æ¡æ¶å¨åæ°æçæ¹é¢ä¹è¡¨ç°åºè²ï¼ä½¿ç¨è¾å°çè®­ç»åæ°å®ç°äºåè¶æ§è½ã
*   å³ä½¿å¨è§è§éª¨å¹²ç½ç»æ©å±å°CLIP ViT-L/14æ¶ï¼æ¨¡åæ§è½ä¾ç¶ä¿æç¨³å®æåï¼è¯æäºæ¡æ¶çå¯æ©å±æ§ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>LLMçææ¦å¿µçåªå£°ååå·®ï¼</strong> åºåæç¥æç¤ºï¼RAPï¼æ¨¡åä¾èµäºå¤§åè¯­è¨æ¨¡åçæçåºåçº§æ¦å¿µãè¿äºæ¦å¿µå¯è½å­å¨åªå£°æä¸è§è§æ¦å¿µä¸å®å¨å¯¹é½ï¼å ä¸ºLLMså¨è¯­è¨-è§è§æ¥å°æ¹é¢å­å¨åºæéå¶ã
*   <strong>æç¤ºå­¦ä¹ çç»ææ§ææï¼</strong> æ¡æ¶åºäºæç¤ºå­¦ä¹ ï¼å½åºç¨äºå¤§éç±»å«æå¤æ ·äº¤äºç±»åæ¶ï¼æç¤ºè¡¨ç¤ºå¯è½å¯¹ç¼©æ¾ç­ç¥ãåå§ååä¼åå¨æææï¼å¯¼è´ç¨³å®æ§ä¸éãè¿åæ äºæç¤ºæ¨¡åå¨æ³åè½åãè¡¨ç¤ºå´©æºæç»åç»æç¼ºä¹æ¹é¢çæ®éå±éæ§ã
*   <strong>é¢è®­ç»æ°æ®ä¸­çåå·®ï¼</strong> æ¨¡åå»ºç«å¨CLIPç­è§è§-è¯­è¨æ¨¡åä¹ä¸ï¼è¿äºæ¨¡åå¨å¤§è§æ¨¡ç½ç»æåå¾å-ææ¬å¯¹ä¸è¿è¡è®­ç»ï¼å¯è½æ æä¸­ç»§æ¿äºé¢è®­ç»æ°æ®ä¸­å­å¨çåå·®ï¼ä¾å¦æåå»æ¿å°è±¡æäººå£ç»è®¡ç¾¤ä½ä¸­çä¸å¹³è¡¡è¡¨ç¤ºã
*   <strong>æ½å¨çè¯¯ç¨é£é©ï¼</strong> å¨ææé¢åï¼å¦æ§æ³æå»çä¿å¥ï¼ä¸­ï¼å¯¹äºº-ç©äº¤äºçè¯¯è§£å¯è½å¯¼è´æå®³åæã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   éè¿ç½®ä¿¡åº¦æç¥è¿æ»¤æè§è§å¯¹é½çæ¦å¿µç»åæ¥æé«LLMçææ¦å¿µçé²æ£æ§ã
*   æ¢ç´¢æ´é²æ£çæç¤ºåå§åãèªéåºç¼©æ¾æºå¶æç»åæç¤ºæå»ºï¼ä»¥æé«æç¤ºæ¨¡åçç¨³å®æ§åæ³åè½åã
*   å¨é¨ç½²å°å®éåºç¨ä¸­æ¶ï¼åºè°¨æè¡äºï¼ç¹å«æ¯å¨æ¶åçæ§ãè¡ä¸ºåææäººç±»æ´»å¨è§£éçåºæ¯ä¸­ã
*   è¿ä¸æ­¥è¯ä¼°å¬å¹³æ§ææ ï¼ä»¥ç¡®ä¿æ¨¡åå¨ä¸åå­ç¾¤ä½ä¸­çå¬å¹³æ§ã</p>
<hr />
<p>è¿ä»½æè¦æ¨å¨å¨é¢ææè®ºæçæ ¸å¿åå®¹ï¼çªåºå¶å¨é¶æ ·æ¬HOIæ£æµé¢åçæ°é¢è´¡ç®åå®éæä¹ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address these challenges, we propose VDRP, a framework for Visual
Diversity and Region-aware Prompt learning.</li>
<li>First, we introduce a visual
diversity-aware prompt learning strategy that injects group-wise visual
variance into the context embedding.</li>
<li>Experiments on the
HICO-DET benchmark demonstrate that our method achieves state-of-the-art
performance under four zero-shot evaluation settings, effectively addressing
both intra-class diversity and inter-class visual entanglement.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.25094v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.25094v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-10-30 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
