<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-10-30 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-07
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-10-29/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-10-31/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-10-30">Arxiv Computer Vision Papers - 2025-10-30</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#multimodal-spatial-reasoning-in-the-large-model-era-a-survey-and-benchmarks" class="nav-link">Multimodal Spatial Reasoning in the Large Model Era: A Survey and Benchmarks</a>
                </li>
                <li class="nav-item">
                    <a href="#ming-flash-omni-a-sparse-unified-architecture-for-multimodal-perception-and-generation" class="nav-link">Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#vfxmaster-unlocking-dynamic-visual-effect-generation-via-in-context-learning" class="nav-link">VFXMaster: Unlocking Dynamic Visual Effect Generation via In-Context Learning</a>
                </li>
                <li class="nav-item">
                    <a href="#instance-level-composed-image-retrieval" class="nav-link">Instance-Level Composed Image Retrieval</a>
                </li>
                <li class="nav-item">
                    <a href="#streamingcot-a-dataset-for-temporal-dynamics-and-multimodal-chain-of-thought-reasoning-in-streaming-videoqa" class="nav-link">StreamingCoT: A Dataset for Temporal Dynamics and Multimodal Chain-of-Thought Reasoning in Streaming VideoQA</a>
                </li>
                <li class="nav-item">
                    <a href="#seeing-clearly-and-deeply-an-rgbd-imaging-approach-with-a-bio-inspired-monocentric-design" class="nav-link">Seeing Clearly and Deeply: An RGBD Imaging Approach with a Bio-inspired Monocentric Design</a>
                </li>
                <li class="nav-item">
                    <a href="#synhlmasynthesizing-hand-language-manipulation-for-articulated-object-with-discrete-human-object-interaction-representation" class="nav-link">SynHLMA:Synthesizing Hand Language Manipulation for Articulated Object with Discrete Human Object Interaction Representation</a>
                </li>
                <li class="nav-item">
                    <a href="#langhops-language-grounded-hierarchical-open-vocabulary-part-segmentation" class="nav-link">LangHOPS: Language Grounded Hierarchical Open-Vocabulary Part Segmentation</a>
                </li>
                <li class="nav-item">
                    <a href="#test-time-adaptive-object-detection-with-foundation-model" class="nav-link">Test-Time Adaptive Object Detection with Foundation Model</a>
                </li>
                <li class="nav-item">
                    <a href="#visual-diversity-and-region-aware-prompt-learning-for-zero-shot-hoi-detection" class="nav-link">Visual Diversity and Region-aware Prompt Learning for Zero-shot HOI Detection</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-10-30">Arxiv Computer Vision Papers - 2025-10-30</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>好的，这是一份针对2025年10月29日Arxiv计算机视觉论文的每日报告执行摘要，旨在帮助忙碌的研究人员快速了解该领域的重要发展。</p>
<hr />
<p><strong>每日Arxiv计算机视觉论文报告执行摘要 (2025年10月29日)</strong></p>
<p><strong>1. 主要主题和趋势概述：</strong></p>
<p>今天的论文集展现了计算机视觉领域几个关键且相互关联的趋势：</p>
<ul>
<li><strong>多模态融合与大型模型 (LLMs/VLMs) 的深度整合：</strong> 显著的趋势是利用大型模型的能力进行更复杂的视觉理解和生成任务。多模态推理、感知和生成是核心焦点，尤其是在处理文本、图像和视频数据方面。</li>
<li><strong>具身智能与人机交互 (HRI) 的进步：</strong> 论文涉及了手部语言操作、物体交互以及对复杂场景中人类行为的理解，这表明了对具身智能和更自然人机交互的日益增长的兴趣。</li>
<li><strong>细粒度理解与控制：</strong> 从实例级图像检索到开放词汇部分分割，研究人员正在追求对视觉内容更细致、更可控的理解和操作。</li>
<li><strong>动态与时序推理：</strong> 视频QA、流式数据处理以及动态视觉效果生成等主题强调了对时序信息和动态场景建模的重视。</li>
<li><strong>效率与适应性：</strong> 稀疏架构、测试时自适应以及零样本学习等技术表明了对模型效率、泛化能力和在未知环境中适应性的持续追求。</li>
</ul>
<p><strong>2. 特别重要或创新的论文：</strong></p>
<ul>
<li><strong>"Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation" (Inclusion AI et al.)：</strong> 这篇论文可能代表了多模态领域的一个重要方向。其提出的“稀疏、统一架构”有望在效率和性能之间取得平衡，对于未来大型多模态模型的部署和应用具有潜在的突破性意义。</li>
<li><strong>"VFXMaster: Unlocking Dynamic Visual Effect Generation via In-Context Learning" (Baolu Li et al.)：</strong> 利用“上下文学习”进行动态视觉效果生成是一个非常新颖且实用的方向。它可能为创意产业和内容生成带来革命性的变化，展示了大型模型在复杂生成任务中的强大潜力。</li>
<li><strong>"Multimodal Spatial Reasoning in the Large Model Era: A Survey and Benchmarks" (Xu Zheng et al.)：</strong> 作为一篇综述和基准论文，它不仅总结了当前多模态空间推理的进展，还提出了新的评估标准，对于指导未来该领域的研究具有重要价值。</li>
</ul>
<p><strong>3. 新兴研究方向或技术：</strong></p>
<ul>
<li><strong>稀疏统一多模态架构：</strong> "Ming-Flash-Omni" 提出的概念，旨在解决大型多模态模型的计算和内存瓶颈，同时保持其通用性。</li>
<li><strong>上下文学习在复杂生成任务中的应用：</strong> "VFXMaster" 展示了如何利用大型模型的上下文学习能力来生成高度动态和复杂的视觉效果，这预示着生成模型的新范式。</li>
<li><strong>生物启发式RGBD成像：</strong> "Seeing Clearly and Deeply" 提出的“仿生单中心设计”为深度感知和成像技术提供了新的思路，可能在机器人、自动驾驶等领域有应用潜力。</li>
<li><strong>语言接地分层开放词汇部分分割：</strong> "LangHOPS" 结合了语言理解和细粒度视觉分割，是实现更自然、更灵活视觉理解的关键一步。</li>
<li><strong>基于基础模型的测试时自适应：</strong> "Test-Time Adaptive Object Detection" 利用基础模型进行测试时自适应，提高了模型在未知或变化环境中的鲁棒性。</li>
</ul>
<p><strong>4. 建议完整阅读的论文：</strong></p>
<p>基于其潜在影响、创新性和对未来研究的指导意义，建议优先完整阅读以下论文：</p>
<ul>
<li><strong>"Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation" (Inclusion AI et al.)：</strong> 了解下一代多模态模型的架构设计和效率提升策略。</li>
<li><strong>"VFXMaster: Unlocking Dynamic Visual Effect Generation via In-Context Learning" (Baolu Li et al.)：</strong> 探索大型模型在复杂、动态视觉内容生成方面的最新进展和应用潜力。</li>
<li><strong>"Multimodal Spatial Reasoning in the Large Model Era: A Survey and Benchmarks" (Xu Zheng et al.)：</strong> 作为该领域的综述，它提供了全面的背景知识和未来研究方向的指引。</li>
<li><strong>"StreamingCoT: A Dataset for Temporal Dynamics and Multimodal Chain-of-Thought Reasoning in Streaming VideoQA" (Yuhang Hu et al.)：</strong> 对于关注视频理解、时序推理和多模态链式思考的研究人员，这份数据集和相关研究具有重要参考价值。</li>
</ul>
<hr />
<p>这份摘要旨在为研究人员提供一个快速的概览，帮助他们识别今天Arxiv上最相关和最有前景的计算机视觉研究。</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2510.25760v1">Multimodal Spatial Reasoning in the Large Model Era: A Survey and Benchmarks</a></li>
<li><a href="#2510.24821v1">Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation</a></li>
<li><a href="#2510.25772v1">VFXMaster: Unlocking Dynamic Visual Effect Generation via In-Context Learning</a></li>
<li><a href="#2510.25387v1">Instance-Level Composed Image Retrieval</a></li>
<li><a href="#2510.25332v1">StreamingCoT: A Dataset for Temporal Dynamics and Multimodal Chain-of-Thought Reasoning in Streaming VideoQA</a></li>
<li><a href="#2510.25314v1">Seeing Clearly and Deeply: An RGBD Imaging Approach with a Bio-inspired Monocentric Design</a></li>
<li><a href="#2510.25268v1">SynHLMA:Synthesizing Hand Language Manipulation for Articulated Object with Discrete Human Object Interaction Representation</a></li>
<li><a href="#2510.25263v1">LangHOPS: Language Grounded Hierarchical Open-Vocabulary Part Segmentation</a></li>
<li><a href="#2510.25175v1">Test-Time Adaptive Object Detection with Foundation Model</a></li>
<li><a href="#2510.25094v1">Visual Diversity and Region-aware Prompt Learning for Zero-shot HOI Detection</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2510.25760v1'></a></p>
<h2 id="multimodal-spatial-reasoning-in-the-large-model-era-a-survey-and-benchmarks"><a href="https://arxiv.org/abs/2510.25760v1">Multimodal Spatial Reasoning in the Large Model Era: A Survey and Benchmarks</a></h2>
<p><strong>Authors:</strong> Xu Zheng, Zihao Dongfang, Lutao Jiang, Boyuan Zheng, Yulong Guo, Zhenquan Zhang, Giuliano Albanese, Runyi Yang, Mengjiao Ma, Zixin Zhang, Chenfei Liao, Dingcheng Zhen, Yuanhuiyi Lyu, Yuqian Fu, Bin Ren, Linfeng Zhang, Danda Pani Paudel, Nicu Sebe, Luc Van Gool, Xuming Hu</p>
<p><strong>Published:</strong> 2025-10-29</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Humans possess spatial reasoning abilities that enable them to understand
spaces through multimodal observations, such as vision and sound. Large
multimodal reasoning models extend these abilities by learning to perceive and
reason, showing promising performance across diverse spatial tasks. However,
systematic reviews and publicly available benchmarks for these models remain
limited. In this survey, we provide a comprehensive review of multimodal
spatial reasoning tasks with large models, categorizing recent progress in
multimodal large language models (MLLMs) and introducing open benchmarks for
evaluation. We begin by outlining general spatial reasoning, focusing on
post-training techniques, explainability, and architecture. Beyond classical 2D
tasks, we examine spatial relationship reasoning, scene and layout
understanding, as well as visual question answering and grounding in 3D space.
We also review advances in embodied AI, including vision-language navigation
and action models. Additionally, we consider emerging modalities such as audio
and egocentric video, which contribute to novel spatial understanding through
new sensors. We believe this survey establishes a solid foundation and offers
insights into the growing field of multimodal spatial reasoning. Updated
information about this survey, codes and implementation of the open benchmarks
can be found at https://github.com/zhengxuJosh/Awesome-Spatial-Reasoning.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将对这篇论文摘要进行分析。</p>
<hr />
<p><strong>论文分析：Multimodal Spatial Reasoning in the Large Model Era: A Survey and Benchmarks</strong></p>
<p><strong>1. 论文主要贡献的简洁总结 (2-3 句话)</strong></p>
<p>这篇论文对大型多模态推理模型在空间推理领域的最新进展进行了全面综述，并首次系统地分类了多模态大语言模型（MLLMs）在空间任务中的应用。它还引入了公开基准，以促进对这些模型进行标准化评估，从而为多模态空间推理这一新兴领域奠定了坚实基础。</p>
<p><strong>2. 关键创新或方法论方法</strong></p>
<p>该论文的关键创新在于其<strong>系统性的综述框架和引入开放基准</strong>。它不仅超越了传统的2D任务，深入探讨了3D空间中的空间关系推理、场景与布局理解、视觉问答和定位，还涵盖了具身AI（如视觉-语言导航和动作模型）以及新兴模态（如音频和第一人称视角视频）对空间理解的贡献。这种全面的分类和评估工具的提供，是其方法论上的核心。</p>
<p><strong>3. 对领域潜在影响</strong></p>
<p>这篇论文对计算机视觉和机器学习领域具有显著的潜在影响：</p>
<ul>
<li><strong>标准化与加速研究：</strong> 通过提供全面的综述和开放基准，它将帮助研究人员快速了解当前进展，并为模型评估提供统一标准，从而加速该领域的研究和发展。</li>
<li><strong>推动多模态融合：</strong> 强调了视觉、听觉、语言等多种模态在空间理解中的协同作用，将鼓励更多研究关注多模态数据融合和跨模态推理。</li>
<li><strong>促进具身智能发展：</strong> 对具身AI的关注，特别是视觉-语言导航和动作模型，将直接推动机器人学、自动驾驶和智能代理等领域的发展。</li>
<li><strong>启发新研究方向：</strong> 对新兴模态（如音频和第一人称视角视频）的探讨，可能会激发新的传感器应用和空间理解范式。</li>
</ul>
<p><strong>4. 可能受益于这项研究的相关领域或应用</strong></p>
<ul>
<li><strong>机器人学和具身AI：</strong> 机器人需要理解其环境的空间布局、物体关系以及导航路径，这篇综述和基准将直接帮助开发更智能、更具鲁棒性的机器人。</li>
<li><strong>自动驾驶：</strong> 车辆需要实时理解复杂的3D场景、识别交通参与者、预测其行为并规划安全路径，多模态空间推理能力至关重要。</li>
<li><strong>虚拟现实/增强现实 (VR/AR)：</strong> 构建沉浸式和交互式VR/AR体验需要精确的空间感知、场景理解和用户定位。</li>
<li><strong>智能家居和智慧城市：</strong> 智能设备需要理解家庭或城市环境的空间结构，以提供更智能的服务，如智能导航、安全监控等。</li>
<li><strong>医学影像分析：</strong> 医生需要理解医学图像中的3D结构和病变的空间关系，这可能从多模态空间推理技术中受益。</li>
<li><strong>地理信息系统 (GIS) 和遥感：</strong> 分析卫星图像和地理数据以理解地球表面的空间模式和变化。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的任何局限性</strong></p>
<ul>
<li><strong>侧重于“大型模型”：</strong> 摘要明确指出是“大型模型时代”的多模态空间推理，这意味着其综述和基准可能主要关注参数量大、计算资源需求高的模型。对于资源受限的边缘设备或特定应用场景，这些模型的部署和效率可能是一个挑战，这方面可能未被深入探讨。</li>
<li><strong>“新兴模态”的成熟度：</strong> 尽管提到了音频和第一人称视角视频等新兴模态，但这些模态在空间推理中的应用可能仍处于早期阶段，其数据可用性、标注难度和模型性能可能不如传统视觉模态成熟。综述可能更多地是展望而非深入分析其当前瓶颈。</li>
<li><strong>解释性（Explainability）的深度：</strong> 摘要中提到了“解释性”，但在大型多模态模型中实现真正可信赖和可操作的解释性仍然是一个开放且极具挑战性的问题。综述可能只是触及了这一方面，而未能提供突破性的解决方案或深入的分析。</li>
<li><strong>发布日期在未来：</strong> 论文的发布日期是2025年10月29日，这意味着它是一篇预发布或正在进行中的工作。虽然这表明其内容具有前瞻性，但也可能意味着某些细节或实验结果尚未完全敲定，或者在最终版本中会有所调整。</li>
</ul>
<hr />
<p>总的来说，这篇论文在计算机视觉和机器学习领域具有重要的地位，它不仅系统地梳理了当前多模态空间推理的进展，还通过引入开放基准，为未来的研究和评估提供了宝贵的工具。其对3D空间、具身AI和新兴模态的关注，预示着该领域未来发展的方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Additionally, we consider emerging modalities such as audio
and egocentric video, which contribute to novel spatial understanding through
new sensors.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.25760v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.25760v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.24821v1'></a></p>
<h2 id="ming-flash-omni-a-sparse-unified-architecture-for-multimodal-perception-and-generation"><a href="https://arxiv.org/abs/2510.24821v1">Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation</a></h2>
<p><strong>Authors:</strong> Inclusion AI, :, Bowen Ma, Cheng Zou, Canxiang Yan, Chunxiang Jin, Chunjie Shen, Dandan Zheng, Fudong Wang, Furong Xu, GuangMing Yao, Jun Zhou, Jingdong Chen, Jianing Li, Jianxin Sun, Jiajia Liu, Jianjiang Zhu, Jianping Jiang, Jun Peng, Kaixiang Ji, Kaimeng Ren, Libin Wang, Lixiang Ru, Longhua Tan, Lan Wang, Mochen Bai, Ning Gao, Qingpei Guo, Qinglong Zhang, Qiang Xu, Rui Liu, Ruijie Xiong, Ruobing Zheng, Sirui Gao, Tianqi Li, Tinghao Liu, Weilong Chai, Xinyu Xiao, Xiaomei Wang, Xiaolong Wang, Xiao Lu, Xiaoyu Li, Xingning Dong, Xuzheng Yu, Yi Yuan, Yuting Gao, Yuting Xiao, Yunxiao Sun, Yipeng Chen, Yifan Mao, Yifei Wu, Yongjie Lyu, Ziping Ma, Zhiqiang Fang, Zhihao Qiu, Ziyuan Huang, Zizheng Yang, Zhengyu He</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>We propose Ming-Flash-Omni, an upgraded version of Ming-Omni, built upon a
sparser Mixture-of-Experts (MoE) variant of Ling-Flash-2.0 with 100 billion
total parameters, of which only 6.1 billion are active per token. This
architecture enables highly efficient scaling (dramatically improving
computational efficiency while significantly expanding model capacity) and
empowers stronger unified multimodal intelligence across vision, speech, and
language, representing a key step toward Artificial General Intelligence (AGI).
Compared to its predecessor, the upgraded version exhibits substantial
improvements across multimodal understanding and generation. We significantly
advance speech recognition capabilities, achieving state-of-the-art performance
in contextual ASR and highly competitive results in dialect-aware ASR. In image
generation, Ming-Flash-Omni introduces high-fidelity text rendering and
demonstrates marked gains in scene consistency and identity preservation during
image editing. Furthermore, Ming-Flash-Omni introduces generative segmentation,
a capability that not only achieves strong standalone segmentation performance
but also enhances spatial control in image generation and improves editing
consistency. Notably, Ming-Flash-Omni achieves state-of-the-art results in
text-to-image generation and generative segmentation, and sets new records on
all 12 contextual ASR benchmarks, all within a single unified architecture.</p>
<p><strong>Analysis:</strong></p>
<p>以下是论文“Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation”的摘要：</p>
<p><strong>1. 主要问题或研究问题</strong>
该论文旨在解决构建统一多模态智能模型所面临的挑战，特别是如何有效地整合跨视觉、语音和语言等多种模态的理解与生成能力，以实现更强大的统一多模态智能，并最终迈向通用人工智能（AGI）。现有模型在多模态理解和生成方面存在表示差异和模态不平衡的问题，限制了其在精细控制和高保真度任务上的表现。</p>
<p><strong>2. 关键创新或方法论贡献</strong>
*   <strong>稀疏MoE架构：</strong> 提出了Ming-Flash-Omni，它是Ming-Omni的升级版，基于Ling-Flash-2.0的稀疏混合专家（MoE）变体，总参数达1000亿，但每个token仅激活61亿参数。这种架构实现了高效扩展，显著提升了计算效率并扩大了模型容量。
*   <strong>统一多模态智能：</strong> 在视觉、语音和语言方面实现了更强的统一多模态智能。
*   <strong>上下文感知ASR：</strong> 显著提升了语音识别能力，在上下文感知ASR和方言感知ASR方面取得了最先进的性能。
*   <strong>高保真文本渲染和图像编辑：</strong> 在图像生成中引入了高保真文本渲染，并在图像编辑中显著提升了场景一致性和身份保持能力。
*   <strong>生成式分割：</strong> 引入了生成式分割能力，不仅实现了强大的独立分割性能，还增强了图像生成的空间控制和编辑一致性。
*   <strong>VideoRoPE：</strong> 在理解侧，升级了位置编码为VideoRoPE，以更好地捕捉视频序列中的时间动态，增强模型理解复杂视觉事件的能力。
*   <strong>连续语音表示：</strong> 在生成侧，用连续声学潜在表示取代了离散语音token，避免了量化损失，提高了保真度。
*   <strong>生成式分割即编辑：</strong> 提出了一种协同训练范式，将图像分割重新定义为生成式编辑任务，强制将理解和生成的目标绑定，从而培养了精细的空间语义控制能力。
*   <strong>基础设施优化：</strong> 在Megatron-LM框架上进行了增强，包括序列打包（Sequence Packing）以处理动态输入形状，以及灵活编码器分片（Flexible Encoder Sharding）以缓解流水线气泡，将训练吞吐量提高了两倍以上。</p>
<p><strong>3. 主要结果及其意义</strong>
*   <strong>最先进的性能：</strong> Ming-Flash-Omni在文本到图像生成和生成式分割方面取得了最先进的（SOTA）结果，并在所有12个上下文ASR基准测试中创造了新纪录，所有这些都集成在一个统一的架构中。
*   <strong>图像理解：</strong> 在MathVista上得分81.9%，在OCR相关基准测试中表现出色，在多图像理解任务中优于Qwen3-VL-30B-A3B。
*   <strong>视频理解：</strong> 在MVBench上得分74.6%，在VideoMME（带字幕）上得分73.0%，展现了强大的视频推理能力。
*   <strong>图像生成：</strong> 在GenEval基准测试中得分0.90，超越了所有非强化学习方法，在“位置”和“颜色”子类别中表现尤为突出。
*   <strong>图像编辑：</strong> 显著优于所有其他统一模型，支持中文编辑指令，并以更高效的2B DiT头部实现了与20B DiT头部相当的语义一致性和感知质量。
*   <strong>多模态理解和生成：</strong> 在多模态理解和生成任务中均展现出卓越的跨模态性能，在图像感知任务中与Qwen3-Omni性能相当，并在端到端语音理解和生成方面表现优异。</p>
<p><strong>4. 论文中提及的局限性</strong>
*   在通用目的理解任务中，与最先进的视觉语言模型相比，仍存在轻微差距。
*   在多图像理解的MuirBench上，性能略有差距，表明仍有改进空间。
*   在长视频理解方面，性能略低。
*   Qwen-Image-Edit在推理时常难以准确识别目标对象，Nano-banana则经常误解用户意图，这突显了Ming-Flash-Omni在分割方面的优势。</p>
<p><strong>5. 潜在的未来研究方向</strong>
*   进一步缩小在通用目的理解和多图像理解（如MuirBench）方面的性能差距。
*   探索更深层次的多模态交互和融合机制，以进一步提升模型的精细控制和高保真度。
*   通过开源模型和代码，促进AGI的发展，推动多模态智能研究和更广泛的实际应用。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose Ming-Flash-Omni, an upgraded version of Ming-Omni, built upon a
sparser Mixture-of-Experts (MoE) variant of Ling-Flash-2.0 with 100 billion
total parameters, of which only 6.1 billion are active per token.</li>
<li>We significantly
advance speech recognition capabilities, achieving state-of-the-art performance
in contextual ASR and highly competitive results in dialect-aware ASR.</li>
<li>Notably, Ming-Flash-Omni achieves state-of-the-art results in
text-to-image generation and generative segmentation, and sets new records on
all 12 contextual ASR benchmarks, all within a single unified architecture.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.24821v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.24821v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.25772v1'></a></p>
<h2 id="vfxmaster-unlocking-dynamic-visual-effect-generation-via-in-context-learning"><a href="https://arxiv.org/abs/2510.25772v1">VFXMaster: Unlocking Dynamic Visual Effect Generation via In-Context Learning</a></h2>
<p><strong>Authors:</strong> Baolu Li, Yiming Zhang, Qinghe Wang, Liqian Ma, Xiaoyu Shi, Xintao Wang, Pengfei Wan, Zhenfei Yin, Yunzhi Zhuge, Huchuan Lu, Xu Jia</p>
<p><strong>Published:</strong> 2025-10-29</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Visual effects (VFX) are crucial to the expressive power of digital media,
yet their creation remains a major challenge for generative AI. Prevailing
methods often rely on the one-LoRA-per-effect paradigm, which is
resource-intensive and fundamentally incapable of generalizing to unseen
effects, thus limiting scalability and creation. To address this challenge, we
introduce VFXMaster, the first unified, reference-based framework for VFX video
generation. It recasts effect generation as an in-context learning task,
enabling it to reproduce diverse dynamic effects from a reference video onto
target content. In addition, it demonstrates remarkable generalization to
unseen effect categories. Specifically, we design an in-context conditioning
strategy that prompts the model with a reference example. An in-context
attention mask is designed to precisely decouple and inject the essential
effect attributes, allowing a single unified model to master the effect
imitation without information leakage. In addition, we propose an efficient
one-shot effect adaptation mechanism to boost generalization capability on
tough unseen effects from a single user-provided video rapidly. Extensive
experiments demonstrate that our method effectively imitates various categories
of effect information and exhibits outstanding generalization to out-of-domain
effects. To foster future research, we will release our code, models, and a
comprehensive dataset to the community.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供论文“VFXMaster: Unlocking Dynamic Visual Effect Generation via In-Context Learning”的全面摘要。</p>
<p><strong>论文摘要：VFXMaster：通过上下文学习解锁动态视觉效果生成</strong></p>
<p><strong>1. 主要问题或研究问题</strong>
数字媒体中的视觉效果（VFX）对于表达力至关重要，但其创作对于生成式AI来说仍然是一个重大挑战。现有方法通常依赖于“每个效果一个LoRA”的范式，这种方法资源密集，并且无法泛化到未见过的效果，从而限制了可扩展性和创作能力。本文旨在解决这一限制，提出一个统一的框架，能够高效、灵活地生成各种动态视觉效果，并具备强大的泛化能力。</p>
<p><strong>2. 关键创新或方法论贡献</strong>
VFXMaster是首个统一的、基于参考的VFX视频生成框架，其核心创新点包括：</p>
<ul>
<li><strong>将效果生成重构为上下文学习任务：</strong> 论文将VFX生成视为一个上下文学习任务，使得模型能够从参考视频中学习并将其动态效果复制到目标内容上。</li>
<li><strong>上下文条件策略：</strong> 设计了一种上下文条件策略，通过参考示例来提示模型。一个参考提示-视频对作为示例，而目标提示和第一帧作为查询来条件化模型生成目标视频。</li>
<li><strong>上下文注意力掩码：</strong> 引入了一种上下文注意力掩码机制，以精确地解耦和注入必要的视觉效果属性，同时防止信息泄露。这使得单个统一模型能够掌握效果模仿，而不会将不相关的背景或主体信息转移到生成结果中。</li>
<li><strong>高效的单次效果适应机制：</strong> 针对难以处理的域外（OOD）效果，提出了一种高效的单次效果适应策略。通过引入一组可学习的概念增强令牌（concept-enhancing tokens），模型可以从单个用户提供的视频中快速学习细粒度的VFX动态和转换，从而显著提高泛化能力。</li>
</ul>
<p><strong>3. 主要结果及其意义</strong>
VFXMaster在广泛的实验中展示了卓越的性能：</p>
<ul>
<li><strong>域内效果的卓越性能：</strong> 在OpenVFX数据集上的实验表明，VFXMaster在所有评估指标上均优于现有最先进的VFX生成方法（如VFXCreator和OminiEffects）以及经过微调的基线模型CogVideoX，尤其是在处理复杂结构或强烈运动的效果时表现出色。</li>
<li><strong>强大的域外泛化能力：</strong> 论文构建了一个专门的OOD测试集，并设计了VFX-Comprehensive Assessment Score (VFX-Cons.) 评估框架。结果表明，VFXMaster在未见过的效果类别上表现出强大的泛化能力。上下文条件策略本身就赋予了模型一定的OOD泛化能力，而单次效果适应机制则进一步显著提升了性能，特别是在效果保真度（EFS）和内容泄露（CLS）方面。</li>
<li><strong>用户研究验证：</strong> 用户研究结果显示，VFXMaster在效果一致性和整体美学质量方面获得了明显的用户偏好，进一步证实了其有效性。</li>
<li><strong>数据可扩展性：</strong> 实验证明，训练数据量与模型性能呈正相关，尤其是在OOD泛化指标上，这突显了VFXMaster框架的可扩展性。</li>
</ul>
<p>这些结果的意义在于，VFXMaster克服了现有方法在可扩展性和泛化性方面的根本限制，为动态视觉效果生成提供了一个统一、高效且用户友好的解决方案。</p>
<p><strong>4. 论文中提及的局限性</strong>
论文中没有明确指出VFXMaster的特定局限性。然而，它间接提及了现有方法的局限性，这些局限性是VFXMaster旨在解决的：
*   <strong>现有方法的资源密集性：</strong> 传统的“每个效果一个LoRA”范式需要为每个效果进行专门的数据和训练，这限制了可扩展性。
*   <strong>现有方法泛化能力不足：</strong> 现有方法无法泛化到未见过的效果类别，这限制了其适用性和用户的创作自由。
*   <strong>上下文信息泄露：</strong> 在上下文学习中，如果不加以控制，参考上下文中的不相关信息可能会泄露并干扰目标生成。VFXMaster通过注意力掩码解决了这个问题，但这也暗示了在设计此类系统时需要仔细考虑信息流。</p>
<p><strong>5. 潜在的未来研究方向</strong>
论文明确指出，为了促进未来的研究，他们将发布代码、模型和全面的数据集。这本身就为社区提供了进一步探索和改进VFXMaster框架的基础。潜在的未来研究方向可能包括：
*   <strong>更广泛的OOD泛化：</strong> 尽管VFXMaster在OOD效果上表现出强大的泛化能力，但仍有改进空间，可以探索更先进的适应机制或更通用的表示学习方法，以应对更具挑战性的未见效果。
*   <strong>交互性和用户控制：</strong> 进一步增强用户对VFX生成过程的控制粒度，例如允许用户更精细地调整效果的强度、位置或时间动态。
*   <strong>效率和实时性：</strong> 尽管论文提到了效率，但进一步优化模型的推理速度和计算成本，以实现更接近实时的VFX生成，将是一个重要的方向。
*   <strong>多模态输入：</strong> 探索除了参考视频和图像之外，结合更多模态输入（如音频、3D模型或用户手绘草图）来指导VFX生成。
*   <strong>复杂场景和长视频生成：</strong> 扩展VFXMaster以处理更复杂的场景、更长的视频序列或更复杂的VFX组合。</p>
<p>总而言之，VFXMaster为动态视觉效果生成领域带来了显著的进步，通过其创新的上下文学习和单次适应机制，为实现可扩展、泛化和高质量的VFX创作铺平了道路，有望降低数字媒体内容生产的门槛。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In addition, we propose an efficient
one-shot effect adaptation mechanism to boost generalization capability on
tough unseen effects from a single user-provided video rapidly.</li>
<li>Extensive
experiments demonstrate that our method effectively imitates various categories
of effect information and exhibits outstanding generalization to out-of-domain
effects.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.25772v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.25772v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.25387v1'></a></p>
<h2 id="instance-level-composed-image-retrieval"><a href="https://arxiv.org/abs/2510.25387v1">Instance-Level Composed Image Retrieval</a></h2>
<p><strong>Authors:</strong> Bill Psomas, George Retsinas, Nikos Efthymiadis, Panagiotis Filntisis, Yannis Avrithis, Petros Maragos, Ondrej Chum, Giorgos Tolias</p>
<p><strong>Published:</strong> 2025-10-29</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>The progress of composed image retrieval (CIR), a popular research direction
in image retrieval, where a combined visual and textual query is used, is held
back by the absence of high-quality training and evaluation data. We introduce
a new evaluation dataset, i-CIR, which, unlike existing datasets, focuses on an
instance-level class definition. The goal is to retrieve images that contain
the same particular object as the visual query, presented under a variety of
modifications defined by textual queries. Its design and curation process keep
the dataset compact to facilitate future research, while maintaining its
challenge-comparable to retrieval among more than 40M random
distractors-through a semi-automated selection of hard negatives.
  To overcome the challenge of obtaining clean, diverse, and suitable training
data, we leverage pre-trained vision-and-language models (VLMs) in a
training-free approach called BASIC. The method separately estimates
query-image-to-image and query-text-to-image similarities, performing late
fusion to upweight images that satisfy both queries, while down-weighting those
that exhibit high similarity with only one of the two. Each individual
similarity is further improved by a set of components that are simple and
intuitive. BASIC sets a new state of the art on i-CIR but also on existing CIR
datasets that follow a semantic-level class definition. Project page:
https://vrg.fel.cvut.cz/icir/.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将对这篇论文摘要进行分析。</p>
<hr />
<p><strong>论文分析：Instance-Level Composed Image Retrieval</strong></p>
<p><strong>1. 论文主要贡献的简洁总结 (2-3 句话)</strong></p>
<p>这篇论文主要贡献在于解决了组合图像检索 (CIR) 领域中高质量训练和评估数据缺失的问题。它引入了一个新的实例级评估数据集 i-CIR，旨在检索包含与视觉查询相同特定对象的图像，并由文本查询定义各种修改。此外，论文提出了一种名为 BASIC 的免训练方法，利用预训练的视觉-语言模型 (VLMs) 进行晚期融合，在 i-CIR 和现有语义级 CIR 数据集上均达到了新的最先进水平。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<ul>
<li><strong>实例级数据集 i-CIR：</strong> 最大的创新是引入了第一个专注于“实例级”类定义的 CIR 评估数据集 i-CIR。与现有数据集的语义级定义不同，i-CIR 要求检索的是与视觉查询中“特定对象”相同的实例，即使该对象经过了文本描述的修改。这种实例级的粒度显著增加了任务的挑战性和实际应用价值。</li>
<li><strong>紧凑且具有挑战性的数据集设计：</strong> i-CIR 的设计和策划过程使其保持紧凑，但通过半自动化选择困难负样本，使其挑战性与在超过 4000 万随机干扰物中检索相当。</li>
<li><strong>免训练方法 BASIC：</strong> 针对训练数据获取的挑战，BASIC 方法利用预训练的 VLMs，通过“晚期融合”策略将图像-图像相似度和文本-图像相似度结合起来。它会提高同时满足两种查询的图像权重，并降低仅满足其中一种查询的图像权重。这种方法避免了昂贵的训练数据标注，并利用了现有 VLMs 的强大能力。</li>
<li><strong>简单直观的相似度改进组件：</strong> 摘要提到，每个单独的相似度（图像-图像和文本-图像）都通过一组简单直观的组件得到了进一步改进，这暗示了该方法可能具有良好的可解释性和可扩展性。</li>
</ul>
<p><strong>3. 对该领域的潜在影响</strong></p>
<ul>
<li><strong>推动 CIR 研究：</strong> i-CIR 数据集的发布将为 CIR 领域提供一个急需的高质量、实例级评估基准，从而刺激新的算法和模型开发，以应对更精细的检索需求。</li>
<li><strong>改变 CIR 任务定义：</strong> 从语义级到实例级的转变，将使 CIR 任务更接近现实世界的应用场景，例如产品检索、特定物体识别和修改等。</li>
<li><strong>启发免训练或少样本学习：</strong> BASIC 方法的成功表明，在数据稀缺的领域，利用预训练模型进行巧妙的组合和融合，可以取得显著的成果，这可能会启发更多免训练或少样本学习方法在计算机视觉领域的应用。</li>
<li><strong>提升 VLM 的实用性：</strong> 该研究进一步展示了预训练视觉-语言模型在复杂多模态检索任务中的强大潜力和实用性。</li>
</ul>
<p><strong>4. 可能受益于这项研究的相关领域或应用</strong></p>
<ul>
<li><strong>电子商务和产品检索：</strong> 用户可以上传一张产品图片并用文本描述修改（例如“同款但颜色为红色”、“类似款式但材质为皮革”），系统能准确检索到特定产品实例。</li>
<li><strong>内容创作和编辑：</strong> 艺术家或设计师可以根据视觉参考和文本指令（例如“这张图片中的椅子，但换成现代风格”），快速找到符合要求的图像素材。</li>
<li><strong>数字资产管理：</strong> 在大型图像库中，用户可以更精确地检索特定对象实例，而不是仅仅是包含该对象类别的图像。</li>
<li><strong>机器人视觉和人机交互：</strong> 机器人可以根据视觉输入和自然语言指令（例如“找到这个杯子，但上面有花纹的那个”），识别和定位特定物体。</li>
<li><strong>时尚和设计：</strong> 帮助用户找到特定款式的服装或配饰，并根据文本描述进行修改。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性</strong></p>
<ul>
<li><strong>“简单直观的组件”的具体细节缺失：</strong> 摘要提到“每个单独的相似度都通过一组简单直观的组件得到了进一步改进”，但没有详细说明这些组件是什么。这使得我们无法评估这些改进的复杂性、通用性或潜在的局限性。</li>
<li><strong>对预训练 VLM 的依赖：</strong> BASIC 方法严重依赖于预训练的 VLMs。如果底层的 VLM 存在偏差、泛化能力不足或对特定领域数据不敏感，那么 BASIC 方法的性能可能会受到影响。</li>
<li><strong>“晚期融合”的潜在局限性：</strong> 晚期融合虽然简单有效，但可能无法捕捉到视觉和文本信息之间更深层次、更复杂的交互。早期或中期融合策略在某些情况下可能表现更好，但需要更多的训练数据。</li>
<li><strong>数据集的“紧凑性”：</strong> 尽管摘要强调 i-CIR 保持紧凑以方便未来研究，但与一些超大规模数据集相比，其规模可能仍然有限。这可能会影响模型在极端多样性或长尾分布数据上的泛化能力。</li>
<li><strong>“半自动化选择困难负样本”的潜在偏差：</strong> 虽然半自动化有助于提高效率，但其选择困难负样本的策略可能存在某种偏差，这可能会影响数据集的全面性和公平性。</li>
</ul>
<hr />
<p>总而言之，这篇论文通过引入实例级 CIR 任务和相应的数据集 i-CIR，以及提出免训练的 BASIC 方法，为组合图像检索领域带来了重要的进展。其对实际应用的潜在影响巨大，尤其是在需要精确识别和修改特定对象实例的场景中。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce
a new evaluation dataset, i-CIR, which, unlike existing datasets, focuses on an
instance-level class definition.</li>
<li>BASIC sets a new state of the art on i-CIR but also on existing CIR
datasets that follow a semantic-level class definition.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.25387v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.25387v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.25332v1'></a></p>
<h2 id="streamingcot-a-dataset-for-temporal-dynamics-and-multimodal-chain-of-thought-reasoning-in-streaming-videoqa"><a href="https://arxiv.org/abs/2510.25332v1">StreamingCoT: A Dataset for Temporal Dynamics and Multimodal Chain-of-Thought Reasoning in Streaming VideoQA</a></h2>
<p><strong>Authors:</strong> Yuhang Hu, Zhenyu Yang, Shihan Wang, Shengsheng Qian, Bin Wen, Fan Yang, Tingting Gao, Changsheng Xu</p>
<p><strong>Published:</strong> 2025-10-29</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>The rapid growth of streaming video applications demands multimodal models
with enhanced capabilities for temporal dynamics understanding and complex
reasoning. However, current Video Question Answering (VideoQA) datasets suffer
from two critical limitations: 1) Static annotation mechanisms fail to capture
the evolving nature of answers in temporal video streams, and 2) The absence of
explicit reasoning process annotations restricts model interpretability and
logical deduction capabilities. To address these challenges, We introduce
StreamingCoT, the first dataset explicitly designed for temporally evolving
reasoning in streaming VideoQA and multimodal Chain-of-Thought (CoT) tasks. Our
framework first establishes a dynamic hierarchical annotation architecture that
generates per-second dense descriptions and constructs temporally-dependent
semantic segments through similarity fusion, paired with question-answer sets
constrained by temporal evolution patterns. We further propose an explicit
reasoning chain generation paradigm that extracts spatiotemporal objects via
keyframe semantic alignment, derives object state transition-based reasoning
paths using large language models, and ensures logical coherence through
human-verified validation. This dataset establishes a foundation for advancing
research in streaming video understanding, complex temporal reasoning, and
multimodal inference. Our StreamingCoT and its construction toolkit can be
accessed at https://github.com/Fleeting-hyh/StreamingCoT.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Yuhang Hu等人撰写的论文“StreamingCoT: A Dataset for Temporal Dynamics and Multimodal Chain-of-Thought Reasoning in Streaming VideoQA”的全面摘要。</p>
<hr />
<p><strong>论文摘要：StreamingCoT: 流媒体视频问答中时序动态和多模态思维链推理数据集</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
当前流媒体视频应用对多模态模型理解时序动态和复杂推理能力提出了更高要求。然而，现有的视频问答（VideoQA）数据集存在两个关键局限：
1. <strong>静态标注机制无法捕捉流媒体中答案的演变性质</strong>：传统数据集的静态、基于时间戳的标注无法反映答案随时间连续变化的动态特性。
2. <strong>缺乏显式推理过程标注</strong>：这限制了模型的解释性和逻辑演绎能力，导致模型可能依赖表面统计关联而非真正的多模态推理。
论文旨在解决这些挑战，为流媒体视频理解和多模态思维链（CoT）推理提供一个专门设计的数据集。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
为了解决上述问题，论文引入了<strong>StreamingCoT</strong>，这是首个明确为流媒体视频问答中的时序演变推理和多模态CoT任务设计的数据集。其框架包含以下关键创新：</p>
<ul>
<li><strong>动态分层标注架构</strong>：<ul>
<li><strong>密集逐秒描述生成</strong>：为视频生成每秒的细粒度文本描述。</li>
<li><strong>时序依赖语义片段构建</strong>：通过相似性融合将逐秒描述聚合成语义片段，形成结构化的视频叙事。</li>
<li><strong>受时序演变模式约束的问答对生成</strong>：构建动态演变的问答对，确保答案选项与视频内容演变紧密对齐，并设计了区分器（distractor）以反映常见的时序误解。</li>
</ul>
</li>
<li><strong>显式推理链生成范式</strong>：<ul>
<li><strong>时空对象提取与关键帧语义对齐</strong>：通过关键帧语义对齐提取视频中的时空对象。</li>
<li><strong>基于大语言模型（LLMs）的对象状态转换推理路径推导</strong>：利用LLMs生成基于对象状态转换的逻辑推理链。</li>
<li><strong>人工验证确保逻辑一致性</strong>：通过人工验证确保推理链的逻辑连贯性和时序一致性。</li>
</ul>
</li>
<li><strong>标准化流程</strong>：论文提供了一个可复现的数据集构建流程，包括数据收集、动态标注和多阶段验证，为构建时序感知视频理解数据集提供了方法论指导。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong>
StreamingCoT数据集包含：
*   <strong>5,000个高质量短视频</strong>：经过多模态质量评估精心筛选，平均时长25.6秒。
*   <strong>243,185个时间锚定的逐秒密集字幕</strong>：通过动态语义融合算法生成68,940个语义片段，平均每个视频12个片段。
*   <strong>34,470个动态问答对</strong>：每个视频5个问答对，涵盖六种时序演变问题类型（累积计数、周期模式识别、顺序步骤识别、状态持续时间、对象状态识别、线索揭示响应）。
*   <strong>68,940个多模态思维链（CoT）标注</strong>：具有时空基础，包含206,820个通过关键帧对齐协议定位的关键对象边界框。
*   <strong>覆盖32个主题类别</strong>：包括教学活动、自然现象、社交互动、机械过程和艺术表演等。</p>
<p>该数据集的意义在于：
*   <strong>推动流媒体视频理解研究</strong>：为处理流媒体视频中答案动态演变和复杂时序推理提供了基础。
*   <strong>提升模型解释性和多模态推理能力</strong>：通过显式推理链标注，模型不仅能预测答案，还能提供可审计的推理路径，增强了模型的可解释性和逻辑一致性。
*   <strong>建立新的评估标准</strong>：为评估多模态系统中的时序理解能力设定了新标准。</p>
<p><strong>4. 论文中提到的局限性：</strong>
论文主要关注其作为数据集的创新性和构建方法，并未明确提及自身的局限性。然而，从其方法论中可以推断出一些潜在的挑战或未来改进方向：
*   <strong>LLMs生成推理链的质量依赖</strong>：推理链的生成依赖于大语言模型，其准确性和逻辑严谨性可能受限于LLMs本身的能力和训练数据。尽管有人工验证，但大规模生成仍可能面临效率和一致性挑战。
*   <strong>人工验证的成本和主观性</strong>：虽然人工验证确保了高质量，但这是一个劳动密集型且成本高昂的过程，且人工判断可能存在一定主观性。
*   <strong>数据集规模的进一步扩展</strong>：尽管StreamingCoT是一个大规模数据集，但与互联网上无限的流媒体视频相比，其规模仍有扩展空间，以覆盖更广泛的场景和更复杂的时序动态。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>更高效的CoT生成与验证</strong>：探索更自动化、更高效的CoT生成和验证方法，减少对人工的依赖，同时保持高质量。例如，结合强化学习或主动学习来优化LLMs生成的推理链。
*   <strong>跨模态推理的泛化能力</strong>：研究模型如何利用StreamingCoT训练出的能力，泛化到其他模态（如音频、文本）更复杂的时序推理任务中。
*   <strong>实时流媒体推理</strong>：利用StreamingCoT的动态标注特性，开发能够进行实时、增量式推理的模型，以适应流媒体的低延迟要求。
*   <strong>多模态CoT的可解释性评估</strong>：设计更精细的指标和方法来评估多模态CoT的解释性，例如，量化推理链的完整性、准确性和可追溯性。
*   <strong>结合外部知识</strong>：探索如何将外部知识库与StreamingCoT结合，以增强模型处理需要常识或领域特定知识的推理任务的能力。</p>
<hr />
<p>这份摘要旨在全面概括论文的核心内容，突出其在流媒体视频理解和多模态思维链推理领域的贡献。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address these challenges, We introduce
StreamingCoT, the first dataset explicitly designed for temporally evolving
reasoning in streaming VideoQA and multimodal Chain-of-Thought (CoT) tasks.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.25332v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.25332v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.25314v1'></a></p>
<h2 id="seeing-clearly-and-deeply-an-rgbd-imaging-approach-with-a-bio-inspired-monocentric-design"><a href="https://arxiv.org/abs/2510.25314v1">Seeing Clearly and Deeply: An RGBD Imaging Approach with a Bio-inspired Monocentric Design</a></h2>
<p><strong>Authors:</strong> Zongxi Yu, Xiaolong Qian, Shaohua Gao, Qi Jiang, Yao Gao, Kailun Yang, Kaiwei Wang</p>
<p><strong>Published:</strong> 2025-10-29</p>
<p><strong>Categories:</strong> cs.CV, cs.RO, eess.IV, physics.optics</p>
<p><strong>Abstract:</strong></p>
<p>Achieving high-fidelity, compact RGBD imaging presents a dual challenge:
conventional compact optics struggle with RGB sharpness across the entire
depth-of-field, while software-only Monocular Depth Estimation (MDE) is an
ill-posed problem reliant on unreliable semantic priors. While deep optics with
elements like DOEs can encode depth, they introduce trade-offs in fabrication
complexity and chromatic aberrations, compromising simplicity. To address this,
we first introduce a novel bio-inspired all-spherical monocentric lens, around
which we build the Bionic Monocentric Imaging (BMI) framework, a holistic
co-design. This optical design naturally encodes depth into its depth-varying
Point Spread Functions (PSFs) without requiring complex diffractive or freeform
elements. We establish a rigorous physically-based forward model to generate a
synthetic dataset by precisely simulating the optical degradation process. This
simulation pipeline is co-designed with a dual-head, multi-scale reconstruction
network that employs a shared encoder to jointly recover a high-fidelity
All-in-Focus (AiF) image and a precise depth map from a single coded capture.
Extensive experiments validate the state-of-the-art performance of the proposed
framework. In depth estimation, the method attains an Abs Rel of 0.026 and an
RMSE of 0.130, markedly outperforming leading software-only approaches and
other deep optics systems. For image restoration, the system achieves an SSIM
of 0.960 and a perceptual LPIPS score of 0.082, thereby confirming a superior
balance between image fidelity and depth accuracy. This study illustrates that
the integration of bio-inspired, fully spherical optics with a joint
reconstruction algorithm constitutes an effective strategy for addressing the
intrinsic challenges in high-performance compact RGBD imaging. Source code will
be publicly available at https://github.com/ZongxiYu-ZJU/BMI.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将对这篇论文摘要进行深入分析。</p>
<hr />
<p><strong>论文摘要分析：Seeing Clearly and Deeply: An RGBD Imaging Approach with a Bio-inspired Monocentric Design</strong></p>
<p><strong>1. 论文主要贡献的简洁总结 (2-3 句话)</strong></p>
<p>这篇论文提出了一种名为“仿生单中心成像 (BMI)”的整体协同设计框架，用于实现高保真、紧凑的 RGBD 成像。其核心贡献在于引入了一种新型的、受生物启发的全球面单中心透镜设计，该设计能自然地将深度信息编码到其深度变化的 PSF 中，并结合一个双头、多尺度重建网络，从单次编码捕获中联合恢复高保真全聚焦 (AiF) 图像和精确深度图。该方法在深度估计和图像恢复方面均取得了最先进的性能，解决了传统紧凑光学和纯软件 MDE 的固有挑战。</p>
<p><strong>2. 关键创新或方法学方法</strong></p>
<p>该论文的关键创新和方法学方法体现在以下几个方面：</p>
<ul>
<li><strong>生物启发式全球面单中心透镜设计：</strong> 这是核心的光学创新。与传统的复杂衍射或自由曲面光学元件不同，这种设计利用其固有的光学特性，将深度信息编码到其深度变化的 PSF 中。这种“自然编码”避免了传统深度光学元件在制造复杂性和色差方面的权衡，从而实现了更简洁、更紧凑的系统。</li>
<li><strong>整体协同设计 (Holistic Co-design) 框架：</strong> 论文强调了光学设计与计算重建算法的紧密结合。这不仅仅是硬件和软件的简单组合，而是一种从设计之初就考虑两者相互作用的协同优化。</li>
<li><strong>物理基础的前向模型和合成数据集生成：</strong> 为了训练深度学习模型，论文建立了一个严格的物理基础前向模型，精确模拟光学降级过程，从而生成高质量的合成数据集。这对于在没有大量真实世界数据的情况下开发和验证算法至关重要，尤其是在新型光学系统设计中。</li>
<li><strong>双头、多尺度重建网络：</strong> 这是一个计算创新。该网络采用共享编码器，能够同时从单次编码捕获中恢复两个关键输出：高保真全聚焦图像（解决景深限制）和精确深度图（解决深度估计问题）。多尺度设计可能有助于捕捉不同尺度的特征，从而提高重建质量。</li>
<li><strong>单次编码捕获：</strong> 强调了从一次捕获中获取所有信息的能力，这对于实时应用和紧凑系统至关重要，避免了多帧捕获的复杂性。</li>
</ul>
<p><strong>3. 对领域潜在影响</strong></p>
<p>这篇论文对计算机视觉和相关领域具有以下潜在影响：</p>
<ul>
<li><strong>推动紧凑型 RGBD 传感器的发展：</strong> 解决了传统紧凑光学在景深和纯软件 MDE 在可靠性方面的痛点，为开发更小、更轻、性能更好的 RGBD 传感器提供了新的范式。这对于移动设备、机器人、AR/VR 头显等应用至关重要。</li>
<li><strong>重新思考光学与计算的协同设计：</strong> 强调了光学设计在深度信息编码中的作用，并展示了如何通过与计算算法的紧密结合来解锁新的性能水平。这可能会鼓励更多的研究人员在光学和算法层面进行联合创新，而不是将它们视为独立的组件。</li>
<li><strong>为生物启发式光学设计提供新思路：</strong> 证明了从生物系统中汲取灵感可以带来突破性的光学设计，尤其是在解决传统光学挑战方面。</li>
<li><strong>提升单目深度估计的可靠性：</strong> 通过将深度信息编码到光学 PSF 中，该方法为单目深度估计提供了更可靠的物理基础，减少了对不可靠语义先验的依赖，从而可能提高 MDE 在复杂场景下的鲁棒性和准确性。</li>
<li><strong>促进合成数据生成和物理建模：</strong> 论文中严谨的物理前向模型和合成数据集生成方法，为其他光学-计算成像系统的开发提供了宝贵的经验和范例。</li>
</ul>
<p><strong>4. 相关领域或应用受益</strong></p>
<ul>
<li><strong>移动计算和智能手机：</strong> 紧凑、高性能的 RGBD 传感器可以显著提升手机的拍照体验（例如更好的背景虚化、3D 扫描能力）和 AR 应用。</li>
<li><strong>机器人和自主系统：</strong> 精确的深度感知对于机器人的导航、避障、物体抓取和环境理解至关重要。紧凑的传感器可以更容易地集成到小型机器人或无人机中。</li>
<li><strong>增强现实 (AR) 和虚拟现实 (VR)：</strong> 高质量的 RGBD 数据是实现沉浸式 AR/VR 体验的关键，包括环境理解、手势识别和虚拟物体与真实世界的融合。</li>
<li><strong>医疗成像：</strong> 某些医疗应用可能受益于高保真、紧凑的 3D 成像系统。</li>
<li><strong>工业检测和质量控制：</strong> 精确的 3D 测量和表面缺陷检测。</li>
<li><strong>计算摄影：</strong> 提升景深扩展、光场成像等高级摄影功能。</li>
</ul>
<p><strong>5. 从摘要中推断出的局限性</strong></p>
<p>尽管摘要展示了令人印象深刻的成果，但仍可以推断出一些潜在的局限性：</p>
<ul>
<li><strong>制造复杂性（相对而言）：</strong> 尽管摘要声称避免了“复杂衍射或自由曲面元件”，但“生物启发式全球面单中心透镜”本身可能仍需要高精度的制造工艺，尤其是在实现其深度编码特性方面。与标准球面透镜相比，其制造难度可能更高。</li>
<li><strong>计算成本：</strong> 双头、多尺度重建网络，尤其是在处理高分辨率图像时，可能需要显著的计算资源和时间。虽然摘要强调了“单次编码捕获”，但实时处理能力（例如在嵌入式设备上）仍需进一步验证。</li>
<li><strong>泛化能力：</strong> 尽管使用了物理基础的合成数据集，但模型在面对与训练数据分布差异较大的真实世界复杂场景（例如极端光照、纹理缺失、高度反射表面）时的泛化能力仍有待全面评估。</li>
<li><strong>系统校准：</strong> 任何光学-计算协同系统都需要精确的校准。这种新型光学设计可能需要一套专门的校准流程，其复杂性未知。</li>
<li><strong>对特定光学设计的依赖：</strong> 性能的优越性可能高度依赖于所提出的特定“仿生单中心透镜”设计。将该框架应用于其他光学系统可能需要重新设计和优化。</li>
<li><strong>“编码捕获”的性质：</strong> 摘要中提到“单次编码捕获”，但没有详细说明编码的具体方式（例如，是否涉及特定的照明模式、传感器阵列或光学调制）。这种编码可能会对捕获速度、信噪比或对环境光的鲁棒性产生影响。</li>
</ul>
<hr />
<p>总而言之，这篇论文通过将创新的生物启发式光学设计与先进的深度学习重建算法相结合，为紧凑型 RGBD 成像提供了一个引人注目的新方向。其在性能上的显著提升预示着在多个应用领域具有巨大的潜力，并可能激发光学和计算机视觉领域更深层次的协同研究。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this,
we first introduce a novel bio-inspired all-spherical monocentric lens, around
which we build the Bionic Monocentric Imaging (BMI) framework, a holistic
co-design.</li>
<li>Extensive experiments validate the state-of-the-art performance of the proposed
framework.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.25314v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.25314v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.25268v1'></a></p>
<h2 id="synhlmasynthesizing-hand-language-manipulation-for-articulated-object-with-discrete-human-object-interaction-representation"><a href="https://arxiv.org/abs/2510.25268v1">SynHLMA:Synthesizing Hand Language Manipulation for Articulated Object with Discrete Human Object Interaction Representation</a></h2>
<p><strong>Authors:</strong> Wang zhi, Yuyan Liu, Liu Liu, Li Zhang, Ruixuan Lu, Dan Guo</p>
<p><strong>Published:</strong> 2025-10-29</p>
<p><strong>Categories:</strong> cs.RO, cs.AI, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Generating hand grasps with language instructions is a widely studied topic
that benefits from embodied AI and VR/AR applications. While transferring into
hand articulatied object interaction (HAOI), the hand grasps synthesis requires
not only object functionality but also long-term manipulation sequence along
the object deformation. This paper proposes a novel HAOI sequence generation
framework SynHLMA, to synthesize hand language manipulation for articulated
objects. Given a complete point cloud of an articulated object, we utilize a
discrete HAOI representation to model each hand object interaction frame. Along
with the natural language embeddings, the representations are trained by an
HAOI manipulation language model to align the grasping process with its
language description in a shared representation space. A joint-aware loss is
employed to ensure hand grasps follow the dynamic variations of articulated
object joints. In this way, our SynHLMA achieves three typical hand
manipulation tasks for articulated objects of HAOI generation, HAOI prediction
and HAOI interpolation. We evaluate SynHLMA on our built HAOI-lang dataset and
experimental results demonstrate the superior hand grasp sequence generation
performance comparing with state-of-the-art. We also show a robotics grasp
application that enables dexterous grasps execution from imitation learning
using the manipulation sequence provided by our SynHLMA. Our codes and datasets
will be made publicly available.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于Wang Zhi, Yuyan Liu, Liu Liu, Li Zhang, Ruixuan Lu, Dan Guo撰写的论文“SynHLMA: Synthesizing Hand Language Manipulation for Articulated Object with Discrete Human Object Interaction Representation”的全面摘要：</p>
<p><strong>论文摘要：SynHLMA：基于离散人机交互表示的关节物体手语操作合成</strong></p>
<p><strong>1. 解决的主要问题或研究问题：</strong>
该论文旨在解决在具身AI和VR/AR应用中，根据语言指令生成手部抓取（hand grasps）的挑战，特别是针对<strong>关节物体手部交互（Hand Articulated Object Interaction, HAOI）</strong>。现有的抓取合成方法通常只关注物体功能性，而忽略了物体变形过程中的长期操作序列。因此，核心问题是如何合成符合语言指令、考虑物体关节动态变化、并能生成长期操作序列的HAOI。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
SynHLMA框架提出了以下关键创新：
*   <strong>离散HAOI表示：</strong> 引入了一种新颖的离散HAOI表示，用于建模每个手部物体交互帧。这种表示通过多阶段VQ-VAE（包含关节感知约束）将完整的抓取轨迹离散化为token序列，编码手部位置、姿态、调整和物体配置。
*   <strong>HAOI操作语言模型：</strong> 将离散表示与自然语言嵌入相结合，通过一个HAOI操作语言模型进行训练，以在共享表示空间中对齐抓取过程及其语言描述。该模型采用自回归策略预测增量差异，以实现长序列生成。
*   <strong>关节感知损失（Joint-aware loss）：</strong> 引入了一种关节感知损失，以确保手部抓取能够遵循关节物体的动态变化，从而提高抓取与物体关节物理状态和运动学配置的一致性。
*   <strong>HAOI-Lang数据集：</strong> 构建了一个新的HAOI-Lang数据集，其中包含丰富的HAOI交互序列，并利用GPT-4生成了多样化的自然语言注释，涵盖抓取意图、方向和位置。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>卓越的性能：</strong> SynHLMA在HAOI生成、HAOI预测和HAOI插值这三项典型手部操作任务上取得了优于现有最先进方法的性能。在HAOI生成任务中，FID分数提高了4.919%，多样性增加了13.986%。在HAOI预测任务中，FID提高了14.64%，多样性提高了19.572%。在插值任务中，FID降低了9.731%，多样性增加了19.969%。
*   <strong>机器人抓取应用：</strong> 论文展示了SynHLMA生成的操纵序列可以有效地指导机器人抓取应用中的模仿学习，实现灵巧抓取。
*   <strong>泛化能力：</strong> 模型在不同尺寸的关节物体（如眼镜、笔记本电脑、柜子、洗碗机）上表现出强大的泛化能力，能够生成不同位置、多样抓取方向的合理HAOI动作，并保持鲁棒性能。</p>
<p><strong>4. 论文中提及的局限性：</strong>
论文中没有明确提及当前方法的具体局限性。然而，从其未来工作方向可以推断，当前模型可能尚未完全涵盖更精细和协调的双手动操作。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
未来的研究工作将致力于探索更精细和协调的<strong>双手动操作（bimanual manipulation）</strong>。</p>
<p>总而言之，SynHLMA通过引入离散HAOI表示、关节感知损失和HAOI操作语言模型，为关节物体的语言驱动手部操作合成提供了一个新颖且高效的框架。其在HAOI生成、预测和插值任务上的优异表现，以及在机器人抓取中的应用潜力，都突显了该方法在具身AI和机器人领域的重要意义。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>This paper proposes a novel HAOI sequence generation
framework SynHLMA, to synthesize hand language manipulation for articulated
objects.</li>
<li>We evaluate SynHLMA on our built HAOI-lang dataset and
experimental results demonstrate the superior hand grasp sequence generation
performance comparing with state-of-the-art.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.25268v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.25268v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.25263v1'></a></p>
<h2 id="langhops-language-grounded-hierarchical-open-vocabulary-part-segmentation"><a href="https://arxiv.org/abs/2510.25263v1">LangHOPS: Language Grounded Hierarchical Open-Vocabulary Part Segmentation</a></h2>
<p><strong>Authors:</strong> Yang Miao, Jan-Nico Zaech, Xi Wang, Fabien Despinoy, Danda Pani Paudel, Luc Van Gool</p>
<p><strong>Published:</strong> 2025-10-29</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We propose LangHOPS, the first Multimodal Large Language Model (MLLM) based
framework for open-vocabulary object-part instance segmentation. Given an
image, LangHOPS can jointly detect and segment hierarchical object and part
instances from open-vocabulary candidate categories. Unlike prior approaches
that rely on heuristic or learnable visual grouping, our approach grounds
object-part hierarchies in language space. It integrates the MLLM into the
object-part parsing pipeline to leverage its rich knowledge and reasoning
capabilities, and link multi-granularity concepts within the hierarchies. We
evaluate LangHOPS across multiple challenging scenarios, including in-domain
and cross-dataset object-part instance segmentation, and zero-shot semantic
segmentation. LangHOPS achieves state-of-the-art results, surpassing previous
methods by 5.5% Average Precision (AP) (in-domain) and 4.8% (cross-dataset) on
the PartImageNet dataset and by 2.5% mIOU on unseen object parts in ADE20K
(zero-shot). Ablation studies further validate the effectiveness of the
language-grounded hierarchy and MLLM driven part query refinement strategy. The
code will be released here.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将对这篇论文摘要进行深入分析。</p>
<hr />
<p><strong>论文摘要分析：LangHOPS: Language Grounded Hierarchical Open-Vocabulary Part Segmentation</strong></p>
<p><strong>1. 论文主要贡献的简洁总结 (2-3 句话)</strong></p>
<p>LangHOPS 提出了首个基于多模态大语言模型 (MLLM) 的开放词汇对象-部件实例分割框架。它能够从开放词汇类别中联合检测和分割图像中的分层对象和部件实例，其核心创新在于将对象-部件层级结构根植于语言空间，而非依赖传统的视觉分组方法。该方法在多个挑战性场景下取得了最先进的性能，显著超越了现有方法。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<p>该论文的关键创新在于：</p>
<ul>
<li><strong>语言根植的分层结构 (Language Grounded Hierarchy):</strong> 传统方法通常依赖启发式或可学习的视觉分组来构建对象-部件层级。LangHOPS 则将这种层级结构“根植”于语言空间。这意味着它利用 MLLM 丰富的知识和推理能力来理解和链接不同粒度的概念（例如，“汽车”包含“车轮”，“车轮”包含“轮毂”），从而在语言层面建立起对象与部件之间的语义关系。</li>
<li><strong>MLLM 驱动的部件查询细化策略 (MLLM Driven Part Query Refinement Strategy):</strong> 摘要中提到，MLLM 被整合到对象-部件解析流程中，以利用其知识和推理能力。这暗示 MLLM 不仅用于理解层级关系，还可能用于指导或细化部件的查询和分割过程，使其更符合语言描述的语义。</li>
<li><strong>开放词汇能力 (Open-Vocabulary Capability):</strong> 结合 MLLM 的强大泛化能力，LangHOPS 能够处理在训练过程中未曾见过的对象和部件类别，这对于实际应用至关重要。</li>
</ul>
<p><strong>3. 对领域潜在影响</strong></p>
<p>LangHOPS 的研究对计算机视觉领域具有以下潜在影响：</p>
<ul>
<li><strong>推动开放词汇分割技术发展：</strong> 将 MLLM 引入对象-部件分割，极大地提升了模型处理未知类别和复杂语义关系的能力，为开放词汇分割设定了新的基准。</li>
<li><strong>深化语言与视觉的融合：</strong> 强调了语言在理解复杂视觉结构（如层级关系）中的核心作用，为未来的多模态研究提供了新的范式，即语言不仅仅是标签，更是结构和推理的载体。</li>
<li><strong>提升细粒度理解能力：</strong> 能够准确地分割和理解对象内部的部件结构，对于需要精细交互和分析的应用（如机器人操作、AR/VR、医学图像分析）具有重要价值。</li>
<li><strong>启发新的模型架构：</strong> 将 MLLM 作为核心组件整合到传统的分割流程中，可能会启发更多将大型预训练模型（尤其是语言模型）与特定视觉任务深度结合的新架构设计。</li>
</ul>
<p><strong>4. 相关领域或应用</strong></p>
<p>以下领域或应用可能会从这项研究中受益：</p>
<ul>
<li><strong>机器人学和具身智能 (Robotics and Embodied AI):</strong> 机器人需要理解物体的部件结构才能进行精细操作、抓取和组装。LangHOPS 可以帮助机器人更好地理解环境中的物体。</li>
<li><strong>增强现实/虚拟现实 (AR/VR):</strong> 在虚拟环境中实现更真实的物体交互和编辑，例如，用户可以精确地选择和修改虚拟汽车的某个部件。</li>
<li><strong>医学图像分析 (Medical Image Analysis):</strong> 精确分割器官的子结构或病变区域，有助于诊断和治疗规划。</li>
<li><strong>自动驾驶 (Autonomous Driving):</strong> 识别车辆的各个部件（如车轮、车门、后视镜）对于更高级别的场景理解和决策至关重要。</li>
<li><strong>产品设计与质量检测 (Product Design and Quality Inspection):</strong> 自动化检测产品部件的完整性和缺陷。</li>
<li><strong>图像编辑和内容生成 (Image Editing and Content Generation):</strong> 实现更精细的图像编辑，例如，只修改图像中某个特定部件的颜色或纹理。</li>
</ul>
<p><strong>5. 从摘要中可推断的局限性</strong></p>
<p>尽管摘要展示了令人印象深刻的成果，但仍可推断出一些潜在局限性：</p>
<ul>
<li><strong>计算资源需求：</strong> MLLM 通常计算成本高昂，整合 MLLM 到分割流程中可能会导致模型训练和推理的计算资源需求大幅增加，这可能限制其在资源受限环境中的应用。</li>
<li><strong>数据依赖性：</strong> 尽管是开放词汇，但 MLLM 的知识和推理能力仍然依赖于其训练数据。如果某些特定领域或非常规的部件关系在 MLLM 的训练数据中体现不足，其性能可能会受到影响。</li>
<li><strong>“语言根植”的鲁棒性：</strong> 语言的歧义性和复杂性可能对“语言根植”的层级结构带来挑战。例如，某些部件的描述可能因上下文而异，或者存在多种有效的语言描述方式。如何确保 MLLM 在各种语言表达下都能稳定地理解和构建层级，是一个潜在问题。</li>
<li><strong>对 MLLM 内部机制的依赖：</strong> 模型的性能高度依赖于所使用的 MLLM 的能力。如果 MLLM 存在偏见或推理错误，这些问题可能会传递到分割结果中。</li>
<li><strong>实时性：</strong> 鉴于 MLLM 的复杂性，LangHOPS 可能难以实现实时或近实时的分割，这对于某些需要快速响应的应用（如自动驾驶）可能是一个限制。</li>
<li><strong>“启发式或可学习视觉分组”的完全替代？</strong> 摘要中提到“Unlike prior approaches that rely on heuristic or learnable visual grouping”，这暗示 LangHOPS 可能在很大程度上摆脱了对纯视觉分组的依赖。然而，完全脱离视觉特征来定义部件边界可能不现实，模型可能仍然需要某种形式的视觉线索来精确地定位和分割部件。具体如何平衡语言指导和视觉细节，是值得关注的方面。</li>
</ul>
<hr />
<p>总而言之，LangHOPS 代表了开放词汇对象-部件分割领域的一个重要进步，通过将 MLLM 的强大能力引入到层级结构理解中，为未来的多模态视觉理解任务开辟了新的道路。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose LangHOPS, the first Multimodal Large Language Model (MLLM) based
framework for open-vocabulary object-part instance segmentation.</li>
<li>Unlike prior approaches
that rely on heuristic or learnable visual grouping, our approach grounds
object-part hierarchies in language space.</li>
<li>LangHOPS achieves state-of-the-art results, surpassing previous
methods by 5.5% Average Precision (AP) (in-domain) and 4.8% (cross-dataset) on
the PartImageNet dataset and by 2.5% mIOU on unseen object parts in ADE20K
(zero-shot).</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.25263v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.25263v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.25175v1'></a></p>
<h2 id="test-time-adaptive-object-detection-with-foundation-model"><a href="https://arxiv.org/abs/2510.25175v1">Test-Time Adaptive Object Detection with Foundation Model</a></h2>
<p><strong>Authors:</strong> Yingjie Gao, Yanan Zhang, Zhi Cai, Di Huang</p>
<p><strong>Published:</strong> 2025-10-29</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>In recent years, test-time adaptive object detection has attracted increasing
attention due to its unique advantages in online domain adaptation, which
aligns more closely with real-world application scenarios. However, existing
approaches heavily rely on source-derived statistical characteristics while
making the strong assumption that the source and target domains share an
identical category space. In this paper, we propose the first foundation
model-powered test-time adaptive object detection method that eliminates the
need for source data entirely and overcomes traditional closed-set limitations.
Specifically, we design a Multi-modal Prompt-based Mean-Teacher framework for
vision-language detector-driven test-time adaptation, which incorporates text
and visual prompt tuning to adapt both language and vision representation
spaces on the test data in a parameter-efficient manner. Correspondingly, we
propose a Test-time Warm-start strategy tailored for the visual prompts to
effectively preserve the representation capability of the vision branch.
Furthermore, to guarantee high-quality pseudo-labels in every test batch, we
maintain an Instance Dynamic Memory (IDM) module that stores high-quality
pseudo-labels from previous test samples, and propose two novel
strategies-Memory Enhancement and Memory Hallucination-to leverage IDM's
high-quality instances for enhancing original predictions and hallucinating
images without available pseudo-labels, respectively. Extensive experiments on
cross-corruption and cross-dataset benchmarks demonstrate that our method
consistently outperforms previous state-of-the-art methods, and can adapt to
arbitrary cross-domain and cross-category target data. Code is available at
https://github.com/gaoyingjay/ttaod_foundation.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Yingjie Gao, Yanan Zhang, Zhi Cai, Zhi Cai, Di Huang撰写的论文“Test-Time Adaptive Object Detection with Foundation Model”的全面摘要。</p>
<hr />
<h3 id="_1">论文摘要：基于基础模型的测试时自适应目标检测</h3>
<p><strong>1. 主要问题或研究问题：</strong>
传统的测试时自适应目标检测（TTAOD）方法存在两个主要限制：
1. 它们严重依赖源域的统计特征，这与TTA的“无源数据”原则相悖。
2. 它们假设源域和目标域共享相同的类别空间（封闭集限制），这限制了其在开放场景中的适用性。
本研究旨在解决这些问题，提出一种无需源数据、能够适应任意跨域和跨类别目标数据的测试时自适应目标检测方法。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
该论文提出了首个由基础模型驱动的测试时自适应目标检测方法，其核心创新包括：</p>
<ul>
<li><strong>多模态提示式平均教师（Multi-modal Prompt-based Mean-Teacher）框架：</strong> 针对视觉-语言检测器驱动的测试时自适应，该框架结合了文本和视觉提示微调，以参数高效的方式同时适应测试数据上的语言和视觉表示空间。</li>
<li><strong>测试时热启动（Test-time Warm-start）策略：</strong> 专门为视觉提示设计，通过对第一个测试样本中提取的图像token进行平均池化来初始化视觉提示，有效保留视觉分支的表示能力，从而缓解次优视觉提示初始化导致的性能下降。</li>
<li><strong>实例动态记忆（Instance Dynamic Memory, IDM）模块：</strong> 为了确保每个测试批次中高质量伪标签的生成，IDM模块存储来自先前测试样本的高质量伪标签。</li>
<li><strong>记忆增强（Memory Enhancement）和记忆幻觉（Memory Hallucination）策略：</strong><ul>
<li><strong>记忆增强：</strong> 利用IDM中存储的高质量实例来细化当前测试图像的原始预测。</li>
<li><strong>记忆幻觉：</strong> 将从IDM中采样的实例整合到缺乏可用伪标签的负面测试图像中，以生成合成的正面样本，从而解决某些测试数据可能没有可用伪标签进行适应的问题。</li>
</ul>
</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>性能超越SOTA：</strong> 在跨损坏（cross-corruption）和跨数据集（cross-dataset）基准上的广泛实验表明，该方法始终优于先前的最先进方法。
*   <strong>无源数据和开放词汇能力：</strong> 该方法完全消除了对源数据的需求，并成功克服了传统封闭集限制，能够适应任意跨域和跨类别目标数据。
*   <strong>参数高效性：</strong> 提出的多模态提示式平均教师框架仅需微调极少量的参数（0.05%），同时保持了预训练知识，并在延迟和GPU内存占用方面优于全参数微调。
*   <strong>组件有效性：</strong> 消融研究证实了每个组件（文本提示微调、视觉提示微调、测试时热启动、记忆增强、记忆幻觉）对整体性能的贡献，特别是测试时热启动策略有效缓解了视觉提示初始化问题，记忆增强和记忆幻觉显著提升了性能。</p>
<p><strong>4. 论文中提及的局限性：</strong>
论文中明确提到，在某些跨数据集（如Mu和Pa）上，由于可用测试样本极少（Mu为5个，Pa为4个），导致视觉-语言检测器无法充分适应目标域，从而影响了性能。这表明在数据量极少的情况下，模型的适应能力仍可能受限。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
论文中未明确提出未来的研究方向，但从其贡献和局限性可以推断出一些潜在方向：
*   <strong>小样本/零样本适应的鲁棒性：</strong> 针对数据量极少的目标域，进一步提升模型的适应能力和鲁棒性。
*   <strong>效率优化：</strong> 尽管本文方法已参数高效，但仍可探索更先进的效率优化策略（如结合Skip策略），以进一步降低推理延迟，尤其是在实时应用中。
*   <strong>更复杂的跨模态交互：</strong> 探索更复杂的文本和视觉提示交互机制，以实现更精细的表示空间适应。
*   <strong>理论分析：</strong> 对所提出的多模态提示式平均教师框架、IDM及其策略的理论基础进行更深入的分析，以更好地理解其工作原理和泛化能力。</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this paper, we propose the first foundation
model-powered test-time adaptive object detection method that eliminates the
need for source data entirely and overcomes traditional closed-set limitations.</li>
<li>Furthermore, to guarantee high-quality pseudo-labels in every test batch, we
maintain an Instance Dynamic Memory (IDM) module that stores high-quality
pseudo-labels from previous test samples, and propose two novel
strategies-Memory Enhancement and Memory Hallucination-to leverage IDM's
high-quality instances for enhancing original predictions and hallucinating
images without available pseudo-labels, respectively.</li>
<li>Extensive experiments on
cross-corruption and cross-dataset benchmarks demonstrate that our method
consistently outperforms previous state-of-the-art methods, and can adapt to
arbitrary cross-domain and cross-category target data.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.25175v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.25175v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.25094v1'></a></p>
<h2 id="visual-diversity-and-region-aware-prompt-learning-for-zero-shot-hoi-detection"><a href="https://arxiv.org/abs/2510.25094v1">Visual Diversity and Region-aware Prompt Learning for Zero-shot HOI Detection</a></h2>
<p><strong>Authors:</strong> Chanhyeong Yang, Taehoon Song, Jihwan Park, Hyunwoo J. Kim</p>
<p><strong>Published:</strong> 2025-10-29</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Zero-shot Human-Object Interaction detection aims to localize humans and
objects in an image and recognize their interaction, even when specific
verb-object pairs are unseen during training. Recent works have shown promising
results using prompt learning with pretrained vision-language models such as
CLIP, which align natural language prompts with visual features in a shared
embedding space. However, existing approaches still fail to handle the visual
complexity of interaction, including (1) intra-class visual diversity, where
instances of the same verb appear in diverse poses and contexts, and (2)
inter-class visual entanglement, where distinct verbs yield visually similar
patterns. To address these challenges, we propose VDRP, a framework for Visual
Diversity and Region-aware Prompt learning. First, we introduce a visual
diversity-aware prompt learning strategy that injects group-wise visual
variance into the context embedding. We further apply Gaussian perturbation to
encourage the prompts to capture diverse visual variations of a verb. Second,
we retrieve region-specific concepts from the human, object, and union regions.
These are used to augment the diversity-aware prompt embeddings, yielding
region-aware prompts that enhance verb-level discrimination. Experiments on the
HICO-DET benchmark demonstrate that our method achieves state-of-the-art
performance under four zero-shot evaluation settings, effectively addressing
both intra-class diversity and inter-class visual entanglement. Code is
available at https://github.com/mlvlab/VDRP.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Chanhyeong Yang等人撰写的论文“Visual Diversity and Region-aware Prompt Learning for Zero-shot HOI Detection”的全面摘要。</p>
<hr />
<h3 id="hoi">论文摘要：零样本HOI检测中的视觉多样性和区域感知提示学习</h3>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决零样本人-物交互（HOI）检测中的核心挑战。零样本HOI检测要求模型能够识别训练中未曾见过的动词-物体对的交互。现有方法在处理交互的视觉复杂性方面存在不足，具体表现为：
1. <strong>类内视觉多样性（Intra-class visual diversity）：</strong> 同一动词的实例在姿态、尺度和场景背景上可能差异巨大，导致单一静态嵌入难以捕捉其所有视觉变体。
2. <strong>类间视觉纠缠（Inter-class visual entanglement）：</strong> 不同的动词（例如“吃”和“舔”）可能产生视觉上相似的模式，使得模型难以准确区分。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
为解决上述挑战，作者提出了一个名为<strong>VDRP（Visual Diversity and Region-aware Prompt learning）</strong> 的新框架，其主要创新点包括：</p>
<ul>
<li>
<p><strong>视觉多样性感知提示学习（Visual Diversity-aware Prompt Learning）：</strong></p>
<ul>
<li>通过将<strong>组内视觉方差</strong>注入到可学习的上下文嵌入中，模型能够更好地捕捉类内视觉多样性。</li>
<li>进一步应用<strong>高斯扰动</strong>，并根据视觉方差进行缩放，鼓励提示捕捉动词的多种视觉变体，从而提高泛化能力。</li>
<li>通过计算CLIP文本嵌入的余弦相似度，将语义相似的动词进行分组，以获得更稳定的组内方差估计。</li>
</ul>
</li>
<li>
<p><strong>区域感知提示增强（Region-aware Prompt Augmentation）：</strong></p>
<ul>
<li>从<strong>人、物体和联合区域</strong>中检索区域特定概念，以增强多样性感知提示嵌入。</li>
<li>利用大型语言模型（LLMs）生成这些区域特定概念，并通过CLIP文本编码器将其编码为概念池。</li>
<li>通过计算区域特征与概念池中概念的余弦相似度，并应用<strong>Sparsemax</strong>激活函数进行稀疏选择，以突出最相关的概念，从而生成区域感知概念向量。</li>
<li>将这些区域感知概念向量与多样性感知提示融合，生成最终的区域感知提示，显著提高动词级别的判别能力，尤其是在视觉相似的动词之间。</li>
</ul>
</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong>
该方法在HICO-DET基准测试上进行了广泛实验，并在四种零样本评估设置（NF-UC、RF-UC、UO、UV）下均取得了<strong>最先进（state-of-the-art）的性能</strong>。
*   在NF-UC和RF-UC设置下，VDRP在所有指标上均表现最佳，显著优于CLIP4HOI和EZ-HOI等现有方法。
*   在UO和UV设置下，VDRP也取得了最佳的HM和Unseen mAP分数，再次验证了其有效性。
*   消融研究证实，视觉多样性感知提示（VDP）和区域感知提示（RAP）模块都对性能提升做出了互补贡献。
*   VDP中的方差-only建模优于均值-方差组合，表明方差本身是捕捉类内多样性的强信号。
*   RAP中的人、物体和联合区域分支的组合优于任何单一分支，强调了区域级概念的互补性。
*   使用ChatGPT-4生成的物体概念比LLaMA-7B生成的概念更具对象中心性，进一步提升了模型性能，表明概念质量的重要性。
*   该框架在参数效率方面也表现出色，使用较少的训练参数实现了卓越性能。
*   即使在视觉骨干网络扩展到CLIP ViT-L/14时，模型性能依然保持稳定提升，证明了框架的可扩展性。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>LLM生成概念的噪声和偏差：</strong> 区域感知提示（RAP）模块依赖于大型语言模型生成的区域级概念。这些概念可能存在噪声或与视觉概念不完全对齐，因为LLMs在语言-视觉接地方面存在固有限制。
*   <strong>提示学习的结构性挑战：</strong> 框架基于提示学习，当应用于大量类别或多样交互类型时，提示表示可能对缩放策略、初始化和优化动态敏感，导致稳定性下降。这反映了提示模型在泛化能力、表示崩溃或组合结构缺乏方面的普遍局限性。
*   <strong>预训练数据中的偏差：</strong> 模型建立在CLIP等视觉-语言模型之上，这些模型在大规模网络抓取图像-文本对上进行训练，可能无意中继承了预训练数据中存在的偏差，例如文化刻板印象或人口统计群体中的不平衡表示。
*   <strong>潜在的误用风险：</strong> 在敏感领域（如执法或医疗保健）中，对人-物交互的误解可能导致有害后果。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   通过置信度感知过滤或视觉对齐的概念细化来提高LLM生成概念的鲁棒性。
*   探索更鲁棒的提示初始化、自适应缩放机制或组合提示构建，以提高提示模型的稳定性和泛化能力。
*   在部署到实际应用中时，应谨慎行事，特别是在涉及监控、行为分析或人类活动解释的场景中。
*   进一步评估公平性指标，以确保模型在不同子群体中的公平性。</p>
<hr />
<p>这份摘要旨在全面捕捉论文的核心内容，突出其在零样本HOI检测领域的新颖贡献和实际意义。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address these challenges, we propose VDRP, a framework for Visual
Diversity and Region-aware Prompt learning.</li>
<li>First, we introduce a visual
diversity-aware prompt learning strategy that injects group-wise visual
variance into the context embedding.</li>
<li>Experiments on the
HICO-DET benchmark demonstrate that our method achieves state-of-the-art
performance under four zero-shot evaluation settings, effectively addressing
both intra-class diversity and inter-class visual entanglement.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.25094v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.25094v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-10-30 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
