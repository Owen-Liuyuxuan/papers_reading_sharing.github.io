<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-09-10 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DEâ«¶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Birdâs-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">è®ºæéè¯»</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-Î±
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-09-09/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-09-11/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-09-10">Arxiv Computer Vision Papers - 2025-09-10</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#one-view-many-worlds-single-image-to-3d-object-meets-generative-domain-randomization-for-one-shot-6d-pose-estimation" class="nav-link">One View, Many Worlds: Single-Image to 3D Object Meets Generative Domain Randomization for One-Shot 6D Pose Estimation</a>
                </li>
                <li class="nav-item">
                    <a href="#visual-tableqa-open-domain-benchmark-for-reasoning-over-table-images" class="nav-link">Visual-TableQA: Open-Domain Benchmark for Reasoning over Table Images</a>
                </li>
                <li class="nav-item">
                    <a href="#raygaussx-accelerating-gaussian-based-ray-marching-for-real-time-and-high-quality-novel-view-synthesis" class="nav-link">RayGaussX: Accelerating Gaussian-Based Ray Marching for Real-Time and High-Quality Novel View Synthesis</a>
                </li>
                <li class="nav-item">
                    <a href="#hairgs-hair-strand-reconstruction-based-on-3d-gaussian-splatting" class="nav-link">HairGS: Hair Strand Reconstruction based on 3D Gaussian Splatting</a>
                </li>
                <li class="nav-item">
                    <a href="#seec-segmentation-assisted-multi-entropy-models-for-learned-lossless-image-compression" class="nav-link">SEEC: Segmentation-Assisted Multi-Entropy Models for Learned Lossless Image Compression</a>
                </li>
                <li class="nav-item">
                    <a href="#semantic-watermarking-reinvented-enhancing-robustness-and-generation-quality-with-fourier-integrity" class="nav-link">Semantic Watermarking Reinvented: Enhancing Robustness and Generation Quality with Fourier Integrity</a>
                </li>
                <li class="nav-item">
                    <a href="#universal-few-shot-spatial-control-for-diffusion-models" class="nav-link">Universal Few-Shot Spatial Control for Diffusion Models</a>
                </li>
                <li class="nav-item">
                    <a href="#mvat-multi-view-aware-teacher-for-weakly-supervised-3d-object-detection" class="nav-link">MVAT: Multi-View Aware Teacher for Weakly Supervised 3D Object Detection</a>
                </li>
                <li class="nav-item">
                    <a href="#dreamlifting-a-plug-in-module-lifting-mv-diffusion-models-for-3d-asset-generation" class="nav-link">DreamLifting: A Plug-in Module Lifting MV Diffusion Models for 3D Asset Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#dimensionally-reduced-open-world-clustering-drowcula" class="nav-link">Dimensionally Reduced Open-World Clustering: DROWCULA</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-09-10">Arxiv Computer Vision Papers - 2025-09-10</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>å¥½çï¼è¿æ¯ä¸ä»½éå¯¹2025å¹´9æ9æ¥Arxivè®¡ç®æºè§è§è®ºæçæ§è¡æè¦ï¼æ¨å¨å¸®å©å¿ç¢çç ç©¶äººåå¿«éäºè§£ææ°è¿å±ï¼</p>
<hr />
<p><strong>Arxiv è®¡ç®æºè§è§æ¯æ¥æ¥åï¼2025å¹´9æ9æ¥</strong></p>
<p><strong>æ§è¡æè¦</strong></p>
<p>ä»å¤©çArxivè®¡ç®æºè§è§è®ºæéå±ç¤ºäºè¯¥é¢åå¨<strong>3Dè§è§ä¸çæãå¤æ¨¡ææ¨çä»¥åæçä¸é²æ£æ§</strong>æ¹é¢çæç»­å¿«éåå±ãç¹å«å¼å¾å³æ³¨çæ¯ï¼<strong>3Déå»ºåæ°è§è§åæ</strong>ææ¯æ­£éè¿ç»åé«æ¯æºå°ï¼Gaussian Splattingï¼åçææ¨¡åå®ç°æ¾èçè´¨éåéåº¦æåãåæ¶ï¼<strong>å¤æ¨¡æçè§£</strong>ï¼å°¤å¶æ¯å¾åä¸è¡¨æ ¼æ°æ®çç»åï¼ä»¥å<strong>æ©æ£æ¨¡åå¨3Dåå¯æ§çæ</strong>æ¹é¢çåºç¨ï¼ä¹æ¾ç¤ºåºå¼ºå¤§çæ½åã</p>
<p><strong>1. ä¸»è¦ä¸»é¢ä¸è¶å¿ï¼</strong></p>
<ul>
<li><strong>3Dè§è§ä¸çæï¼</strong> æ¾èå³æ³¨ä»2Då¾åçæ3Då¯¹è±¡ã6Då§¿æä¼°è®¡ã3Déå»ºï¼ç¹å«æ¯æ¯åï¼ä»¥åæ°è§è§åæãé«æ¯æºå°ææ¯å¨å¤ä¸ª3Dä»»å¡ä¸­è¢«å¹¿æ³åºç¨ï¼ä»¥æé«æçåè´¨éã</li>
<li><strong>å¤æ¨¡æä¸æ¨çï¼</strong> åºç°äºç»åè§è§åææ¬ï¼å°¤å¶æ¯è¡¨æ ¼æ°æ®ï¼è¿è¡å¤ææ¨ççåºååæ¹æ³ã</li>
<li><strong>çææ¨¡åä¸æ§å¶ï¼</strong> æ©æ£æ¨¡åå¨å®ç°ç²¾ç»çç©ºé´æ§å¶å3Dèµäº§çææ¹é¢æç»­æ¼è¿ï¼å¹¶æ¢ç´¢äºå¦ä½æåå¶æçåå¤è§å¾ä¸è´æ§ã</li>
<li><strong>æçä¸é²æ£æ§ï¼</strong> å¨å¾ååç¼©ãè¯­ä¹æ°´å°å3Dæ£æµç­é¢åï¼ç ç©¶äººåè´åäºæåæ¨¡åçæçãé²æ£æ§åæ³åè½åã</li>
</ul>
<p><strong>2. æ¾èæåæ°è®ºæï¼</strong></p>
<ul>
<li><strong>"RayGaussX: Accelerating Gaussian-Based Ray Marching for Real-Time and High-Quality Novel View Synthesis" (Blanc et al.)</strong>ï¼è¯¥è®ºæéè¿ä¼åé«æ¯æºå°çæ¸²æè¿ç¨ï¼å®ç°äºå®æ¶ãé«è´¨éçæ°è§è§åæï¼æ¯è¯¥é¢åæçæåçéè¦ä¸æ­¥ã</li>
<li><strong>"DreamLifting: A Plug-in Module Lifting MV Diffusion Models for 3D Asset Generation" (Yin et al.)</strong>ï¼å±ç¤ºäºå¦ä½ææå°å°å¤è§å¾æ©æ£æ¨¡åâæåâå°3Dèµäº§çæï¼ä¸º3Dåå®¹åä½æä¾äºå¼ºå¤§çæ°å·¥å·ãå¶å³æå³ç¨çç¹æ§å·æå¾é«çå®ç¨ä»·å¼ã</li>
<li><strong>"One View, Many Worlds: Single-Image to 3D Object Meets Generative Domain Randomization for One-Shot 6D Pose Estimation" (Geng et al.)</strong>ï¼ç»åäºåå¾åå°3Dçæåçæå¼åéæºåï¼ä»¥è§£å³åæ¬¡6Då§¿æä¼°è®¡çææï¼ä¸ºæºå¨äººåAR/VRåºç¨æä¾äºæ°çæè·¯ã</li>
</ul>
<p><strong>3. æ°å´ç ç©¶æ¹åæææ¯ï¼</strong></p>
<ul>
<li><strong>é«æ¯æºå°ï¼Gaussian Splattingï¼çæ³åä¸ä¼åï¼</strong> ä¸ä»ç¨äºæ°è§è§åæï¼è¿æ©å±å°æ¯åéå»ºç­æ´ç²¾ç»ç3Dç»æï¼å¹¶æç»­è¿è¡æ¸²ææçä¼åã</li>
<li><strong>æ©æ£æ¨¡åå¨3Dé¢åçæ·±åº¦èåï¼</strong> ä»2Då¾åçæ3Då¯¹è±¡ãå¤è§å¾ä¸è´æ§å°ç´æ¥çæ3Dèµäº§ï¼æ©æ£æ¨¡åæ­£æä¸º3Dåå®¹çæçæ ¸å¿é©±å¨åã</li>
<li><strong>å¤æ¨¡æè¡¨æ ¼å¾åçè§£ï¼</strong> "Visual-TableQA"çåºç°è¡¨æï¼å¯¹å¤æè§è§ä¿¡æ¯ï¼å¦è¡¨æ ¼å¾åï¼è¿è¡å¼æ¾åæ¨ççéæ±æ¥çå¢é¿ï¼è¿å°æ¨å¨æ´é«çº§çè§è§è¯­è¨æ¨¡ååå±ã</li>
<li><strong>çæå¼åéæºåï¼Generative Domain Randomizationï¼ï¼</strong> å¨æ°æ®ç¨ç¼ºææ³åè½åè¦æ±é«çä»»å¡ä¸­ï¼å©ç¨çææ¨¡ååå»ºå¤æ ·åè®­ç»æ°æ®ï¼ä»¥æåæ¨¡åé²æ£æ§ã</li>
</ul>
<p><strong>4. å»ºè®®éè¯»å¨æçè®ºæï¼</strong></p>
<p>å¯¹äºä¸åå´è¶£çç ç©¶äººåï¼ä»¥ä¸è®ºæå¼å¾æ·±å¥éè¯»ï¼</p>
<ul>
<li><strong>å¯¹äº3Dè§è§åå®æ¶æ¸²æç ç©¶èï¼</strong><ul>
<li><strong>"RayGaussX: Accelerating Gaussian-Based Ray Marching for Real-Time and High-Quality Novel View Synthesis" (Blanc et al.)</strong> - äºè§£é«æ¯æºå°çææ°ä¼åææ¯ã</li>
<li><strong>"DreamLifting: A Plug-in Module Lifting MV Diffusion Models for 3D Asset Generation" (Yin et al.)</strong> - æ¢ç´¢æ©æ£æ¨¡åå¨3Dèµäº§çæä¸­çåºç¨ã</li>
<li><strong>"HairGS: Hair Strand Reconstruction based on 3D Gaussian Splatting" (Pan et al.)</strong> - äºè§£é«æ¯æºå°å¨ç²¾ç»ç»æéå»ºä¸­çåæ°åºç¨ã</li>
</ul>
</li>
<li><strong>å¯¹äºå¤æ¨¡æåæ¨çç ç©¶èï¼</strong><ul>
<li><strong>"Visual-TableQA: Open-Domain Benchmark for Reasoning over Table Images" (Lompo, Haraoui)</strong> - äºè§£è¡¨æ ¼å¾åçè§£çææ°åºååææã</li>
</ul>
</li>
<li><strong>å¯¹äºçææ¨¡ååæ§å¶ç ç©¶èï¼</strong><ul>
<li><strong>"Universal Few-Shot Spatial Control for Diffusion Models" (Nguyen et al.)</strong> - æ¢ç´¢æ©æ£æ¨¡åå¨å°éæ ·æ¬ä¸å®ç°éç¨ç©ºé´æ§å¶çæ¹æ³ã</li>
</ul>
</li>
<li><strong>å¯¹äºé²æ£æ§åæ³åç ç©¶èï¼</strong><ul>
<li><strong>"One View, Many Worlds: Single-Image to 3D Object Meets Generative Domain Randomization for One-Shot 6D Pose Estimation" (Geng et al.)</strong> - äºè§£çæå¼åéæºåå¨6Då§¿æä¼°è®¡ä¸­çåºç¨ã</li>
</ul>
</li>
</ul>
<hr />
<p>è¿ä»½æè¦æ¨å¨æä¾ä¸ä¸ªé«å±æ¬¡çæ¦è§ï¼å¸®å©æ¨å¿«éè¯å«ä¸æ¨ç ç©¶æ¹åç¸å³çå³é®è¿å±ã</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2509.07978v1">One View, Many Worlds: Single-Image to 3D Object Meets Generative Domain Randomization for One-Shot 6D Pose Estimation</a></li>
<li><a href="#2509.07966v1">Visual-TableQA: Open-Domain Benchmark for Reasoning over Table Images</a></li>
<li><a href="#2509.07782v1">RayGaussX: Accelerating Gaussian-Based Ray Marching for Real-Time and High-Quality Novel View Synthesis</a></li>
<li><a href="#2509.07774v1">HairGS: Hair Strand Reconstruction based on 3D Gaussian Splatting</a></li>
<li><a href="#2509.07704v1">SEEC: Segmentation-Assisted Multi-Entropy Models for Learned Lossless Image Compression</a></li>
<li><a href="#2509.07647v1">Semantic Watermarking Reinvented: Enhancing Robustness and Generation Quality with Fourier Integrity</a></li>
<li><a href="#2509.07530v1">Universal Few-Shot Spatial Control for Diffusion Models</a></li>
<li><a href="#2509.07507v1">MVAT: Multi-View Aware Teacher for Weakly Supervised 3D Object Detection</a></li>
<li><a href="#2509.07435v1">DreamLifting: A Plug-in Module Lifting MV Diffusion Models for 3D Asset Generation</a></li>
<li><a href="#2509.07184v1">Dimensionally Reduced Open-World Clustering: DROWCULA</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2509.07978v1'></a></p>
<h2 id="one-view-many-worlds-single-image-to-3d-object-meets-generative-domain-randomization-for-one-shot-6d-pose-estimation"><a href="https://arxiv.org/abs/2509.07978v1">One View, Many Worlds: Single-Image to 3D Object Meets Generative Domain Randomization for One-Shot 6D Pose Estimation</a></h2>
<p><strong>Authors:</strong> Zheng Geng, Nan Wang, Shaocong Xu, Chongjie Ye, Bohan Li, Zhaoxi Chen, Sida Peng, Hao Zhao</p>
<p><strong>Published:</strong> 2025-09-09</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Estimating the 6D pose of arbitrary unseen objects from a single reference
image is critical for robotics operating in the long-tail of real-world
instances. However, this setting is notoriously challenging: 3D models are
rarely available, single-view reconstructions lack metric scale, and domain
gaps between generated models and real-world images undermine robustness. We
propose OnePoseViaGen, a pipeline that tackles these challenges through two key
components. First, a coarse-to-fine alignment module jointly refines scale and
pose by combining multi-view feature matching with render-and-compare
refinement. Second, a text-guided generative domain randomization strategy
diversifies textures, enabling effective fine-tuning of pose estimators with
synthetic data. Together, these steps allow high-fidelity single-view 3D
generation to support reliable one-shot 6D pose estimation. On challenging
benchmarks (YCBInEOAT, Toyota-Light, LM-O), OnePoseViaGen achieves
state-of-the-art performance far surpassing prior approaches. We further
demonstrate robust dexterous grasping with a real robot hand, validating the
practicality of our method in real-world manipulation. Project page:
https://gzwsama.github.io/OnePoseviaGen.github.io/</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Zheng Gengç­äººæ°åçè®ºæâOne View, Many Worlds: Single-Image to 3D Object Meets Generative Domain Randomization for One-Shot 6D Pose Estimationâçå¨é¢æè¦ã</p>
<hr />
<p><strong>è®ºææè¦ï¼One View, Many Worlds: Single-Image to 3D Object Meets Generative Domain Randomization for One-Shot 6D Pose Estimation</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³æºå¨äººææ¯ä¸­ä¸ä¸ªæ ¸å¿ä¸æå·æææ§çé®é¢ï¼å¦ä½ä»å­ä¸å¼ åèå¾åï¼å¯¹ä»»ææªè§è¿çç©ä½è¿è¡ä¸æ¬¡æ§ï¼one-shotï¼6Då§¿æä¼°è®¡ãè¿ä¸ä»»å¡çé¾ç¹å¨äºï¼3Dæ¨¡åéå¸¸é¾ä»¥è·åï¼åè§å¾éå»ºç¼ºä¹åº¦éå°ºåº¦ä¿¡æ¯ï¼ä»¥åçææ¨¡åä¸çå®ä¸çå¾åä¹é´å­å¨çåå·®è·ä¼ä¸¥éå½±åå§¿æä¼°è®¡çé²æ£æ§ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
ä½èæåºäºä¸ä¸ªåä¸º <strong>OnePoseViaGen</strong> çç«¯å°ç«¯ç®¡éï¼éè¿ä¸¤ä¸ªå³é®ç»ä»¶æ¥åºå¯¹ä¸è¿°ææï¼</p>
<ul>
<li><strong>çæå¼åè§å¾3Dçº¹çç½æ ¼æ¨¡åï¼</strong> è®ºæé¦åå©ç¨æ¹è¿çHi3DGenæ¨¡åï¼ä»ä¸å¼ RGB-Déç¹å¾åçæä¸ä¸ªé«ä¿çãçº¹çåç3Då¯¹è±¡æ¨¡åãè¿ä¸ªæ¨¡åå¨å¯¹è±¡ä¸­å¿åæ ç³»ä¸­æ¯æ ååï¼æ åº¦éå°ºåº¦ï¼çï¼ä¸ºåç»­çå§¿æä¼°è®¡å¥ å®äºåºç¡ã</li>
<li><strong>ç²å°ç²¾çåº¦éå¯¹é½æ¨¡åï¼</strong> éå¯¹çææ¨¡åç¼ºä¹åº¦éå°ºåº¦çé®é¢ï¼OnePoseViaGenè®¾è®¡äºä¸ä¸ªç²å°ç²¾çå¯¹é½ç­ç¥ãå®ç»åäºå¤è§å¾ç¹å¾å¹éï¼ä½¿ç¨SuperGlueï¼åPnPæ±è§£å¨è¿è¡ç²ç¥å§¿æåå°ºåº¦ä¼°è®¡ï¼ç¶åéè¿æ¸²æ-æ¯è¾ï¼render-and-compareï¼ç»åï¼åºäºFoundationPoseï¼è¿­ä»£å°ä¼åå§¿æååº¦éå°ºåº¦ï¼ä»èå°æ ååæ¨¡åä¸çå®ä¸çåºæ¯ä¸­çå¯¹è±¡ç²¾ç¡®å¯¹é½ã</li>
<li><strong>ææ¬å¼å¯¼ççæå¼åéæºåç­ç¥ï¼</strong> ä¸ºäºå¼¥åçææ¨¡åä¸çå®å¾åä¹é´çåå·®è·ï¼è®ºæå¼å¥äºä¸ç§ææ¬é©±å¨ççæå¼æ°æ®å¢å¼ºæ¹æ³ãè¯¥æ¹æ³å©ç¨ææ¬æç¤ºï¼éè¿VLMçæï¼å3Dçææ¨¡åï¼Trellisï¼çæç»æä¸è´ä½çº¹çå¤æ ·ç3Dåä½ãè¿äºåä½å¨éæºåçåç§ãèæ¯åé®æ¡æ¡ä»¶ä¸æ¸²æï¼å½¢æå¤§è§æ¨¡åææ°æ®éï¼ç¨äºå§¿æä¼°è®¡å¨çææå¾®è°ï¼æ¾èæåäºé²æ£æ§åæ³åè½åã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
OnePoseViaGenå¨å¤ä¸ªå·ææææ§çåºåæ°æ®éä¸ï¼YCBInEOAT, Toyota-Light, LM-Oï¼åå¾äºæ¾èä¼äºç°ææ¹æ³çææ°æ§è½ãä¾å¦ï¼å¨YCBInEOATæ°æ®éä¸ï¼å¶å¹³åADDåæ°è¾¾å°äº81.27ï¼è¿è¶å¶ä»æ¹æ³å¨å¤æå¯¹è±¡ä¸çè¡¨ç°ãå¨LM-OåTOYLæ°æ®éä¸ï¼è¯¥æ¹æ³å¨BOPåºåææ ä¸ä¹æ¾ç¤ºåºä¸è´çæ¹è¿ãæ­¤å¤ï¼è®ºæéè¿å¨çå®æºå¨äººæä¸è¿è¡é²æ£ççµå·§æåå®éªï¼éªè¯äºè¯¥æ¹æ³å¨å®éæºå¨äººæä½ä¸­çå®ç¨æ§åæææ§ãè¿äºç»æè¡¨æï¼çæå¼å»ºæ¨¡å¯ä»¥æ¾èæåä¸æ¬¡æ§6Då§¿æä¼°è®¡çæ§è½ï¼å°¤å¶æ¯å¨å¤çæªè§å¯¹è±¡åå¤æåºæ¯æ¶ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
å°½ç®¡OnePoseViaGenåå¾äºä»¤äººé¼èçææï¼ä½è®ºæä¹æåºå¶å¨å¤ç<strong>å¯åå½¢æå³èå¼å¯¹è±¡</strong>æ¶ä»é¢ä¸´ææãå¨è¿ç§æåµä¸ï¼å¯¹è±¡å½¢ç¶çååå¯è½å¯¼è´6Då§¿æä¼°è®¡ä¸åç¡®ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
æªæ¥çå·¥ä½å°ä¾§éäºå°<strong>æµè¯æ¶è®­ç»ï¼test-time trainingï¼</strong>æ´åå°æ¨çç®¡éä¸­ï¼ä»¥å®ç°å¯¹å¯åå½¢å¯¹è±¡å ä½å½¢ç¶çæç»­ç»åååç¡®å§¿æä¼°è®¡ãè¿å°ååå©ç¨çææ¨¡åå¨6Då§¿æä¼°è®¡ä»»å¡ä¸­ççµæ´»æ§åæ³åè½åã</p>
<hr />
<p>æ»èè¨ä¹ï¼OnePoseViaGenéè¿ç»ååæ°çåè§å¾3Dæ¨¡åçæãç²¾ç»çåº¦éå¯¹é½ä»¥åææ¬å¼å¯¼çåéæºåç­ç¥ï¼ä¸ºä¸æ¬¡æ§6Då§¿æä¼°è®¡æä¾äºä¸ä¸ªå¼ºå¤§ä¸å®ç¨çè§£å³æ¹æ¡ï¼æ¾èæ¨å¨äºæºå¨äººæç¥é¢åçåå±ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>On challenging
benchmarks (YCBInEOAT, Toyota-Light, LM-O), OnePoseViaGen achieves
state-of-the-art performance far surpassing prior approaches.</li>
<li>We further
demonstrate robust dexterous grasping with a real robot hand, validating the
practicality of our method in real-world manipulation.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.07978v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.07978v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.07966v1'></a></p>
<h2 id="visual-tableqa-open-domain-benchmark-for-reasoning-over-table-images"><a href="https://arxiv.org/abs/2509.07966v1">Visual-TableQA: Open-Domain Benchmark for Reasoning over Table Images</a></h2>
<p><strong>Authors:</strong> Boammani Aser Lompo, Marc Haraoui</p>
<p><strong>Published:</strong> 2025-09-09</p>
<p><strong>Categories:</strong> cs.CV, cs.CL</p>
<p><strong>Abstract:</strong></p>
<p>Visual reasoning over structured data such as tables is a critical capability
for modern vision-language models (VLMs), yet current benchmarks remain limited
in scale, diversity, or reasoning depth, especially when it comes to rendered
table images. Addressing this gap, we introduce Visual-TableQA, a large-scale,
open-domain multimodal dataset specifically designed to evaluate and enhance
visual reasoning over complex tabular data. Our generation pipeline is modular,
scalable, and fully autonomous, involving multiple reasoning LLMs collaborating
across distinct roles: generation, validation, and inspiration. Visual-TableQA
comprises 2.5k richly structured LaTeX-rendered tables and 6k
reasoning-intensive QA pairs, all produced at a cost of under USD 100. To
promote diversity and creativity, our pipeline performs multi-model
collaborative data generation via cross-model prompting ('inspiration') and
LLM-jury filtering. Stronger models seed layouts and topics that weaker models
elaborate, collectively distilling diverse reasoning patterns and visual
structures into the dataset. Empirical results show that models fine-tuned on
Visual-TableQA generalize robustly to external benchmarks, outperforming
several proprietary models despite the dataset's synthetic nature. The full
pipeline and resources are publicly available at
https://github.com/AI-4-Everyone/Visual-TableQA.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼è¿æ¯ä¸ç¯å³äºâVisual-TableQA: Open-Domain Benchmark for Reasoning over Table Imagesâè®ºæçå¨é¢æè¦ï¼ç¨ä¸­æåæï¼</p>
<p><strong>è®ºææè¦ï¼Visual-TableQAï¼åºäºè¡¨æ ¼å¾åæ¨ççå¼æ¾ååºå</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢</strong>
è¯¥è®ºææ¨å¨è§£å³ç°æè§è§è¯­è¨æ¨¡åï¼VLMsï¼å¨å¤çæ¸²æè¡¨æ ¼å¾åæ¶ï¼å¨è§æ¨¡ãå¤æ ·æ§åæ¨çæ·±åº¦æ¹é¢å­å¨çå±éæ§ãå°½ç®¡è¡¨æ ¼å¾åçè§è§æ¨çæ¯ç°ä»£VLMsçå³é®è½åï¼ä½ç°æåºåæ°æ®éæªè½ååæææ¨¡åçè§£å¤æè¡¨æ ¼ç»æåè¿è¡æ·±åº¦æ¨ççè½åã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®</strong>
*   <strong>å¼å¥Visual-TableQAæ°æ®éï¼</strong> è®ºææåºäºä¸ä¸ªå¤§è§æ¨¡ãå¼æ¾åçå¤æ¨¡ææ°æ®éVisual-TableQAï¼ä¸é¨ç¨äºè¯ä¼°åå¢å¼ºå¯¹å¤æè¡¨æ ¼æ°æ®çè§è§æ¨çè½åãè¯¥æ°æ®éåå«2.5kä¸ªç»æä¸°å¯çLaTeXæ¸²æè¡¨æ ¼å6kä¸ªæ¨çå¯éåé®ç­å¯¹ã
*   <strong>æ¨¡ååãå¯æ©å±ä¸å®å¨èªä¸»ççæç®¡éï¼</strong> æ°æ®ééè¿ä¸ä¸ªåæ°ççæç®¡éåå»ºï¼è¯¥ç®¡éææ¬ä½å»ï¼æ»ææ¬ä½äº100ç¾åï¼ï¼å¹¶æ¶åå¤ä¸ªæ¨çåå¤§åè¯­è¨æ¨¡åï¼LLMsï¼ååå·¥ä½ï¼æ®æ¼çæãéªè¯åå¯åç­ä¸åè§è²ã
*   <strong>å¤æ¨¡åååæ°æ®çæï¼âå¯åâï¼åLLMè¯å®¡è¿æ»¤ï¼</strong> ä¸ºäºä¿è¿å¤æ ·æ§ååé æ§ï¼è¯¥ç®¡ééç¨âè·¨æ¨¡åå¯åâæºå¶ï¼å³æ´å¼ºçæ¨¡åæä¾å¸å±åä¸»é¢çâç§å­âï¼è¾å¼±çæ¨¡åå¨æ­¤åºç¡ä¸è¿è¡ç»ååæ©å±ï¼ä»èå°å¤æ ·åçæ¨çæ¨¡å¼åè§è§ç»ææç¼å°æ°æ®éä¸­ãLLMè¯å®¡å¢ç¨äºè¿æ»¤åéªè¯çæçæ°æ®è´¨éã
*   <strong>LaTeXä½ä¸ºä¸­é´è¡¨ç¤ºï¼</strong> è®ºæå©ç¨LLMsçæå¤æçLaTeXè¡¨æ ¼ä»£ç ï¼èéç´æ¥çææ¸²æå¾åï¼è¿å¤§å¤§éä½äºçæææ¬å¹¶æé«äºå¤ææ§ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹</strong>
*   <strong>æ¨¡åæ³åè½åï¼</strong> ç»éªç»æè¡¨æï¼å¨Visual-TableQAä¸å¾®è°çæ¨¡åè½å¤ç¨³å¥å°æ³åå°å¤é¨åºåæµè¯ï¼å°½ç®¡æ°æ®éæ¯åæçï¼ä½å¶æ§è½ä¼äºä¸äºä¸ææ¨¡åã
*   <strong>ææè¯ä¼°è§è§æ¨çï¼</strong> Visual-TableQAè½ææè¯ä¼°VLMsçè§è§æ¨çè½åï¼å¶æ¨¡åæåä¸ReachQAç­å¹³è¡¡è§è§è¯å«åæ¨ççæ°æ®éé«åº¦ç¸å³ï¼ä½ä¸ChartQAï¼ä¾§éè¯å«ï¼æMATH-Visionï¼ä¾§éæ¨çï¼çå³èæ§è¾å¼±ï¼è¡¨æå¶ä½ä¸ºç»¼åæ§è§è§æ¨çåºåçç¬ç¹å°ä½ã
*   <strong>å¾åæ ¼å¼çæææ§ï¼</strong> æ¨¡åå¨Visual-TableQA-CITï¼ææ¬ä»£ç æ ¼å¼ï¼ä¸çè¡¨ç°å¹³åæ¯å¨Visual-TableQAï¼å¾åæ ¼å¼ï¼ä¸é«åº6.26%ï¼çªæ¾äºå¾åæ ¼å¼å¨è§è§æ¨çæ¹é¢å¸¦æ¥çé¢å¤ææã
*   <strong>å¯¹æ¨çè½åçæ¾èæåï¼</strong> å¨Visual-TableQAä¸å¾®è°çæ¨¡åå¨æ¨çä»»å¡ä¸çå¹³åå¢çæ¾èé«äºå¨ReachQAä¸å¾®è°çæ¨¡åï¼è¡¨æVisual-TableQAå¨ç¥è¯è¸é¦ï¼ç¹å«æ¯éè¦ç¬¦å·è§£éåå¤æ­¥æ¨ççä»»å¡æ¹é¢æ´ææã</p>
<p><strong>4. å±éæ§</strong>
*   <strong>LaTeXè¡¨è¾¾è½åçå±éæ§ï¼</strong> å°½ç®¡LaTeXä½ä¸ºä¸­é´è¡¨ç¤ºææï¼ä½å¨å¤çæ´å¤ææè§è§ä¸°å¯çå¾åæ¶ï¼å¶è¡¨è¾¾è½åæéã
*   <strong>æ°æ®è´¨éè¯ä¼°ï¼</strong> å°½ç®¡ROSCOEç­èªå¨ææ æä¾äºæç¨çè§è§£ï¼ä½å®ä»¬ä»ä¸å¦äººç±»å¤æ­å¯é ï¼äººç±»æ æ³¨èå¨ç¡®ä¿åææ°æ®éè´¨éæ¹é¢ä»æ®æ¼å³é®è§è²ã
*   <strong>æ¨¡åæ³åä¸è´æ§ï¼</strong> æäºæ¨¡åï¼å¦Qwen2.5-VL-7B-Instructï¼æªè½å§ç»ä»Visual-TableQAççç£ä¸­åçï¼è¿è¡¨æå¨æ³åæ¹é¢å¯è½å­å¨å±éæ§ã
*   <strong>è¯ä¼°ä¸­çæ¨¡ç³æ§ï¼</strong> å¤ææ¨çä»»å¡çè¯ä¼°å¹¶éå®ç¾æ ç¼ºï¼ä¸ä¸ªé®é¢å¯è½å­å¨å¤ç§ææç­æ¡ï¼è¿ä½¿å¾LLMè¯å®¡å¢é¾ä»¥å®å¨èªä¿¡å°è¯ä¼°æ­£ç¡®æ§ï¼å¯è½å¯¼è´è¯ä¼°ä¸­çæ¨¡ç³æ§ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹å</strong>
*   <strong>é²æ£çååå¾åå°ææ¬ç¼ç ç³»ç»ï¼</strong> å¼åä¸ä¸ªè½å¤æ´ææå°å¤çå¤æè§è§åå®¹çååå¾åå°ææ¬ç¼ç ç³»ç»æ¯ä¸ä¸ªå¼æ¾ä¸æåæ¯çç ç©¶é¢åã
*   <strong>éå¯¹ç¹å®éè¯¯ç±»åçåæçç£ï¼</strong> éè¿åæçç£æ¥è§£å³æ¨¡åå¨ç¹å®éè¯¯ç±»åï¼å¦æ°æ®æåä¸å®æ´ãå¹»è§ç­ï¼ä¸çé®é¢ã
*   <strong>è¿ä¸æ­¥æ¢ç´¢è·¨æ¨¡åå¯åæºå¶ï¼</strong> æç»­ä¼ååæ¢ç´¢è·¨æ¨¡åå¯åæºå¶ï¼ä»¥è¿ä¸æ­¥æé«çææ°æ®çå¤æ ·æ§ååé æ§ã</p>
<p>æ»èè¨ä¹ï¼è¿ç¯è®ºæéè¿å¼å¥Visual-TableQAæ°æ®éåå¶åæ°ççæç®¡éï¼ä¸ºè¯ä¼°åæåVLMså¨å¤æè¡¨æ ¼å¾åè§è§æ¨çæ¹é¢çè½åæä¾äºéè¦è´¡ç®ï¼å¹¶ä¸ºæªæ¥çç ç©¶ææäºæ¹åã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Addressing this gap, we introduce Visual-TableQA, a large-scale,
open-domain multimodal dataset specifically designed to evaluate and enhance
visual reasoning over complex tabular data.</li>
<li>Empirical results show that models fine-tuned on
Visual-TableQA generalize robustly to external benchmarks, outperforming
several proprietary models despite the dataset's synthetic nature.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.07966v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.07966v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.07782v1'></a></p>
<h2 id="raygaussx-accelerating-gaussian-based-ray-marching-for-real-time-and-high-quality-novel-view-synthesis"><a href="https://arxiv.org/abs/2509.07782v1">RayGaussX: Accelerating Gaussian-Based Ray Marching for Real-Time and High-Quality Novel View Synthesis</a></h2>
<p><strong>Authors:</strong> Hugo Blanc, Jean-Emmanuel Deschaud, Alexis Paljic</p>
<p><strong>Published:</strong> 2025-09-09</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>RayGauss has achieved state-of-the-art rendering quality for novel-view
synthesis on synthetic and indoor scenes by representing radiance and density
fields with irregularly distributed elliptical basis functions, rendered via
volume ray casting using a Bounding Volume Hierarchy (BVH). However, its
computational cost prevents real-time rendering on real-world scenes. Our
approach, RayGaussX, builds on RayGauss by introducing key contributions that
accelerate both training and inference. Specifically, we incorporate volumetric
rendering acceleration strategies such as empty-space skipping and adaptive
sampling, enhance ray coherence, and introduce scale regularization to reduce
false-positive intersections. Additionally, we propose a new densification
criterion that improves density distribution in distant regions, leading to
enhanced graphical quality on larger scenes. As a result, RayGaussX achieves 5x
to 12x faster training and 50x to 80x higher rendering speeds (FPS) on
real-world datasets while improving visual quality by up to +0.56 dB in PSNR.
Project page with videos and code: https://raygaussx.github.io/.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Hugo Blanc, Jean-Emmanuel Deschaud, Alexis Paljicæ°åçè®ºæâRayGaussX: Accelerating Gaussian-Based Ray Marching for Real-Time and High-Quality Novel View Synthesisâçå¨é¢æè¦ã</p>
<hr />
<h3 id="raygaussx">RayGaussX: å éåºäºé«æ¯çåçº¿æ­¥è¿ä»¥å®ç°å®æ¶é«è´¨éæ°è§å¾åæ</h3>
<p><strong>1. è®ºæè§£å³çä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³ç°æRayGaussæ¹æ³å¨å®æ¶æ¸²æåå¤ççå®ä¸çåºæ¯æ¶çè®¡ç®ææ¬è¿é«é®é¢ãRayGausséè¿ä½¿ç¨ä¸è§ååå¸çæ¤­ååºå½æ°ç»åè¾¹çä½ç§¯å±æ¬¡ç»æï¼BVHï¼è¿è¡ä½æ¸²æåçº¿æå°ï¼å¨åæåå®¤ååºæ¯ä¸­å®ç°äºæåè¿çæ¸²æè´¨éãç¶èï¼å¶é«è®¡ç®ææ¬é»ç¢äºå¨çå®ä¸çåºæ¯ä¸­çå®æ¶åºç¨ï¼å¹¶ä¸å¨æ·å¤ç¯å¢ä¸­æ§è½ç¥æä¸éï¼è®­ç»åæ¨çæ¶é´ä»ç¶æ¾èã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
RayGaussXå¨RayGaussçåºç¡ä¸ï¼å¼å¥äºå¤é¡¹å³é®åæ°ä»¥æ¾èå éè®­ç»åæ¨çï¼
*   <strong>é«æåçº¿éæ ·ï¼Efficient Ray Samplingï¼ï¼</strong> æ´åäº<strong>ç©ºé²ç©ºé´è·³è¿ï¼empty-space skippingï¼</strong>å<strong>èªéåºéæ ·ï¼adaptive samplingï¼</strong>ç­ç¥ï¼ä»¥åå°æ¸²ææ¹ç¨è®¡ç®æéçæ ·æ¬æ°éï¼ä»èå éæ¸²æãç©ºé²ç©ºé´è·³è¿å©ç¨BVHé¿åéæ ·å®å¨éæåºåï¼èªéåºéæ ·åæ ¹æ®éå°çåå°æåæºçè·ç¦»å¨æè°æ´éæ ·æ­¥é¿ã
*   <strong>ä¼ååçº¿ä¸è´æ§ååå­è®¿é®æçï¼</strong> éè¿<strong>é«æ¯ç©ºé´éæåºï¼spatial reordering of Gaussiansï¼</strong>ï¼ä½¿ç¨Z-orderæ²çº¿ï¼å<strong>åçº¿éæåºï¼ray reorderingï¼</strong>æ¥å¢å¼ºåçº¿ä¸è´æ§ï¼ä»¥æ´å¥½å°éåºGPUå¹¶è¡è®¡ç®ï¼æé«åå­è®¿é®æçå¹¶åå°warpåæ£ã
*   <strong>éå¶é«åº¦ååå¼æ§é«æ¯ï¼Limiting Highly Anisotropic Gaussianï¼ï¼</strong> å¼å¥äº<strong>å°ºåº¦æ­£ååå½æ°ï¼scale regularization functionï¼</strong>ï¼åååæ§æå¤±ï¼ï¼ä»¥æå°ååé³æ§äº¤éãè¿éè¿çº¦æé«æ¯è½´å¯¹é½è¾¹çæ¡ï¼AABBï¼ä¸æ¤­çä½æ¬èº«çä½ç§¯æ¯æ¥å®ç°ï¼åå°äºBVHéåä¸­çä¸å¿è¦è®¡ç®ã
*   <strong>æ°é¢çç¨ å¯åååï¼Novel Densification Criterionï¼ï¼</strong> æåºäºä¸ç§æ°çç¨ å¯åååï¼éè¿å¼å¥æ ¡æ­£å å­æ¥å æ3Dç©ºé´ä¸­çæ¢¯åº¦ï¼æ¹åäºè¿è·ç¦»åºåçå¯åº¦åå¸ï¼ä»èå¨æ´å¤§åºæ¯ä¸­å®ç°å¢å¼ºçå¾å½¢è´¨éã</p>
<p><strong>3. ä¸»è¦ç»æåå¶éè¦æ§ï¼</strong>
RayGaussXå¨çå®ä¸çæ°æ®éä¸åå¾äºæ¾èçæ§è½æåï¼
*   <strong>è®­ç»éåº¦ï¼</strong> å®ç°äº5å°12åçè®­ç»å éã
*   <strong>æ¸²æéåº¦ï¼FPSï¼ï¼</strong> å®ç°äº50å°80åçæ¸²æéåº¦æåã
*   <strong>è§è§è´¨éï¼</strong> å¨PSNRæ¹é¢ï¼è§è§è´¨éæé«äºé«è¾¾+0.56 dBã
*   <strong>ä¸ç°ææ¹æ³å¯¹æ¯ï¼</strong> å¨NeRF SyntheticåNSVF Syntheticæ°æ®éä¸ï¼RayGaussXå¨æ¸²æè´¨éä¸ç¥ä¼äºRayGaussï¼åæ¶æ¸²æéåº¦å¿«äºä¸åãå¨Mip-NeRF360ãTanks&amp;TemplesåDeep Blendingç­çå®ä¸çæ°æ®éä¸ï¼RayGaussXå¨ä¿ææç¥å¾®æåè§è§è´¨éçåæ¶ï¼æ¾èè¶è¶äºRayGausså3D Gaussian Splattingç­æ¹æ³ï¼å°¤å¶å¨æ·å¤åºæ¯ä¸­è¡¨ç°æ´ä½³ã</p>
<p>è¿äºç»æè¡¨æRayGaussXæåå°å°RayGaussçæ¸²æè´¨éä¸å®æ¶æ§è½ç¸ç»åï¼ä½¿å¶éç¨äºæ´å¹¿æ³ççå®ä¸çåºæ¯ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>ç¡¬ä»¶è¦æ±ï¼</strong> è¯¥æ¹æ³éè¦é«ç«¯GPUï¼å®éªä¸­ä½¿ç¨NVIDIA RTX 4090ï¼ï¼èGaussian Splattingåå¶åä½å¯ä»¥å¨ç§»å¨è®¾å¤æWeb-GLç¯å¢ä¸­è¿è¡ã
*   <strong>æé¯é½¿å¤çï¼</strong> ç®åçæ¹æ³æªè½å¦¥åå¤çæé¯é½¿é®é¢ï¼è¿è¶åºäºæ¬è®ºæçèå´ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>è¿ä¸æ­¥å éæ¸²æï¼</strong> æªæ¥çå·¥ä½å¯ä»¥éè¿æ°çä¼åè¿ä¸æ­¥å éæ¸²æã
*   <strong>æé¯é½¿å¤çï¼</strong> è§£å³æé¯é½¿é®é¢ï¼ä»¥è¿ä¸æ­¥æåæ¸²æè´¨éã
*   <strong>åºç¨æ©å±ï¼</strong> RayGaussXçå¿«éè®­ç»åé«è´¨éæ¸²æä½¿å¶æä¸ºéè¦é«ç²¾åº¦åºç¨ï¼å¦è¡¨é¢éå»ºãéæ¸²æãSLAMãç¸æºä¼ååéæåï¼çå¼ºå¤§æ¡æ¶ã</p>
<hr />
<p>æ»èè¨ä¹ï¼RayGaussXéè¿å¼å¥ä¸ç³»ååæ°çå éç­ç¥ï¼æåå°å°åºäºé«æ¯çåçº¿æ­¥è¿æ¹æ³ä»è®¡ç®å¯éåæåä¸ºå®æ¶å¯ç¨çæ°è§å¾åæè§£å³æ¹æ¡ï¼åæ¶ä¿æäºåè¶çæ¸²æè´¨éï¼ç¹å«æ¯å¨å¤çå¤æçå®ä¸çåºæ¯æ¹é¢è¡¨ç°åºè²ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>RayGauss has achieved state-of-the-art rendering quality for novel-view
synthesis on synthetic and indoor scenes by representing radiance and density
fields with irregularly distributed elliptical basis functions, rendered via
volume ray casting using a Bounding Volume Hierarchy (BVH).</li>
<li>Our
approach, RayGaussX, builds on RayGauss by introducing key contributions that
accelerate both training and inference.</li>
<li>Additionally, we propose a new densification
criterion that improves density distribution in distant regions, leading to
enhanced graphical quality on larger scenes.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.07782v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.07782v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.07774v1'></a></p>
<h2 id="hairgs-hair-strand-reconstruction-based-on-3d-gaussian-splatting"><a href="https://arxiv.org/abs/2509.07774v1">HairGS: Hair Strand Reconstruction based on 3D Gaussian Splatting</a></h2>
<p><strong>Authors:</strong> Yimin Pan, Matthias NieÃner, Tobias Kirschstein</p>
<p><strong>Published:</strong> 2025-09-09</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Human hair reconstruction is a challenging problem in computer vision, with
growing importance for applications in virtual reality and digital human
modeling. Recent advances in 3D Gaussians Splatting (3DGS) provide efficient
and explicit scene representations that naturally align with the structure of
hair strands. In this work, we extend the 3DGS framework to enable strand-level
hair geometry reconstruction from multi-view images. Our multi-stage pipeline
first reconstructs detailed hair geometry using a differentiable Gaussian
rasterizer, then merges individual Gaussian segments into coherent strands
through a novel merging scheme, and finally refines and grows the strands under
photometric supervision.
  While existing methods typically evaluate reconstruction quality at the
geometric level, they often neglect the connectivity and topology of hair
strands. To address this, we propose a new evaluation metric that serves as a
proxy for assessing topological accuracy in strand reconstruction. Extensive
experiments on both synthetic and real-world datasets demonstrate that our
method robustly handles a wide range of hairstyles and achieves efficient
reconstruction, typically completing within one hour.
  The project page can be found at: https://yimin-pan.github.io/hair-gs/</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Yimin Pan, Matthias Niessner, Tobias Kirschsteinæ°åçè®ºæâHairGS: Hair Strand Reconstruction based on 3D Gaussian Splattingâçå¨é¢æè¦ã</p>
<hr />
<h3 id="hairgs-3d">HairGS: åºäº3Dé«æ¯æ³¼æºçå¤´åè¡éå»º</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³è®¡ç®æºè§è§é¢åä¸­äººåéå»ºçæææ§é®é¢ãç°ææ¹æ³å¨å¤çå¤æååãé¢ç¹é®æ¡ä»¥åå¤´åè¡çè¿æ¥æ§åææç»ææ¹é¢å­å¨å±éæ§ãç¹å«æ¯ï¼è®¸å¤æ¹æ³ä¾§éäºå ä½ç²¾åº¦ï¼èå¿½ç¥äºå¤´åè¡åºæçææç»æåè¿éæ§ï¼è¿å¯¹äºèæç°å®åæ°å­äººå»ºæ¨¡ç­åºç¨è³å³éè¦ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è´¡ç®ï¼</strong>
HairGSæåºäºä¸ç§æ°é¢çå¤é¶æ®µä¼åæµç¨ï¼å°3Dé«æ¯æ³¼æºï¼3DGSï¼æ¡æ¶æ©å±å°è¡çº§å¤´åå ä½éå»ºï¼å¶ä¸»è¦è´¡ç®åæ¬ï¼
*   <strong>å¤é¶æ®µä¼åæµç¨ï¼</strong>
    *   <strong>å ä½éå»ºï¼ç¬¬ä¸é¶æ®µï¼ï¼</strong> å©ç¨å¯å¾®åé«æ¯åæ åå¨åèªéåºç¨ å¯åï¼ä»å¤è§å¾å¾åä¸­éå»ºè¯¦ç»çå¤´åå ä½ç»æãéè¿ç»åRGBãæ¹ååæ©èæå¤±ï¼åæ¬æ°å¼å¥çååæ¹åæå¤±åæ©èæå¤±ï¼è¿è¡ä¼åï¼ä»¥ç¡®ä¿åç¡®çå ä½åæ¹åã
    *   <strong>è¡çæï¼ç¬¬äºé¶æ®µï¼ï¼</strong> å¼å¥äºä¸ç§æ°é¢çåå¹¶æ¹æ¡ï¼åºäºè·ç¦»åè§åº¦å¯åå¼æ¹æ³ï¼å°åä¸ªé«æ¯æ®µåå¹¶æè¿è´¯çå¤´åè¡ãæ¯ä¸ªå¤´åè¡è¢«è¡¨ç¤ºä¸ºé¾æ¥å³èçé¾ï¼æ¯ä¸ªæ®µè¢«å»ºæ¨¡ä¸ºåæ±ä½ã
    *   <strong>çé¿ä¸ç»åï¼ç¬¬ä¸é¶æ®µï¼ï¼</strong> å¨ååº¦çç£ä¸å¯¹å¤´åè¡è¿è¡ç»ååçé¿ãéè¿ç»åååº¦æå¤±åè§åº¦å¹³æ»åº¦æå¤±æ¥ä¼åå³èä½ç½®ï¼ä»¥é²æ­¢å½¢æå°éè§åº¦ãéè¿éæ¸æ¾å®½åå¹¶éå¼æ¥ä¿è¿æ´é¿çè¡å½¢æã
*   <strong>ææåç¡®æ§è¯ä¼°æ°ææ ï¼</strong> æåºäºä¸ç§åä¸ºâè¡ä¸è´æ§âï¼Strand Consistency, SCï¼çæ°è¯ä¼°ææ ï¼ç¨äºéåè¡éå»ºä¸­çææåç¡®æ§ï¼è§£å³äºç°æææ å¿½ç¥å¤´åè¡è¿æ¥æ§åææç»æçé®é¢ãè¯¥ææ éè¿è¡¡éæ¯ä¸ªçå®è¡ä¸­ä¸åä¸ªé¢æµè¡å¹éç¹çæé«æ¯ä¾æ¥è¯ä¼°è¿éæ§ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>é²æ£æ§åæçï¼</strong> å¨åæï¼USC-HairSalon, Cem Yukselï¼åçå®ä¸çï¼NeRSembleï¼æ°æ®éä¸çå¹¿æ³å®éªè¡¨æï¼HairGSè½å¤é²æ£å°å¤çåç§ååï¼åæ¬ç´åãå·ååé¿åï¼å¹¶è½åç¡®ææç»å¾®ç»èåæµ®å¨è¡ã
*   <strong>æ§è½ä¼å¿ï¼</strong> è¯¥æ¹æ³å¨ææè¯ä¼°ææ ä¸åä¼äºç°æçåºäºSfMï¼å¦LP-MVSãStrand Integrationï¼åæ°æ®é©±å¨ï¼å¦Neural Haircutï¼æ¹æ³ï¼å°¤å¶æ¯å¨å·ææææ§çå·åæ ·æ¬ä¸è¡¨ç°åºè²ï¼å¹¶å®ç°äºæé«çè¡ä¸è´æ§ã
*   <strong>éå»ºéåº¦ï¼</strong> HairGSçéå»ºè¿ç¨é«æï¼éå¸¸å¨ä¸å°æ¶åå®æï¼æ¾èå¿«äºè®¸å¤è®¡ç®å¯éåãåºäºå­¦ä¹ çæ¹æ³ï¼ä¾å¦ï¼Neural Strandséè¦48å°æ¶ï¼Neural Haircutéè¦120å°æ¶ï¼ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>åå¹¶æ åï¼</strong> å½åçåå¹¶æ åå¯è½æ æ³å§ç»ææå°å°åä¸è¡ä¸­çé«æ¯ç¹åå¹¶ï¼å¯¼è´éå»ºçè¡å¯è½æ¯å®éæ´ç­ã
*   <strong>ä¸å¤´ç®çè¿æ¥ï¼</strong> éå»ºçè¡ä¸ä¸å®éçå¨å¤´ç®ä¸ï¼è¿éå¶äºå®ä»¬å¨æ¸²æå¼æä¸­çç´æ¥ä½¿ç¨ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>æ¹è¿åå¹¶ç®æ³ï¼</strong> å¯ä»¥éè¿ä½¿ç¨åçå©ç®æ³çåä½è¿è¡æä¼å¹éæ¥è§£å³åå¹¶æ åçé®é¢ï¼ä»èå®ç°æ´ææçè¡åå¹¶ã
*   <strong>å¤´ç®è¿æ¥ï¼</strong> æªæ¥çå·¥ä½å¯ä»¥éè¿å°è¡æ ¹é¨åºå®å°è¡¨é¢å¹¶å[28, 34]ä¸­é£æ ·çé¿è¡æ¥è§£å³ä¸å¤´ç®è¿æ¥çé®é¢ï¼ä»èæé«å¶å¨æ¸²æåºç¨ä¸­çå®ç¨æ§ã
*   <strong>æ©å±åºç¨ï¼</strong> è¯¥æ¡æ¶å¯ä»¥èªç¶å°æ©å±å°å¶ä»çº¿ç¶ç»æï¼å¦çµç¼æçµçº¿ï¼çéå»ºï¼åªéå¯¹åå²æ¨¡åè¿è¡æå°çè°æ´ã</p>
<hr />
<p>æ»èè¨ä¹ï¼HairGSéè¿å°3Dé«æ¯æ³¼æºæ¡æ¶ä¸åæ°çå¤é¶æ®µä¼åæµç¨åè¡çº§ææè¯ä¼°ç¸ç»åï¼ä¸ºå¤è§å¾å¾åçäººåéå»ºæä¾äºä¸ä¸ªé«æä¸é²æ£çè§£å³æ¹æ¡ãå¶æåºçè¡ä¸è´æ§ææ å¡«è¡¥äºç°æè¯ä¼°æ¹æ³å¨ææåç¡®æ§æ¹é¢çç©ºç½ï¼ä¸ºè¯¥é¢åçç ç©¶æä¾äºæ°çè§è§ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Our multi-stage pipeline
first reconstructs detailed hair geometry using a differentiable Gaussian
rasterizer, then merges individual Gaussian segments into coherent strands
through a novel merging scheme, and finally refines and grows the strands under
photometric supervision.</li>
<li>To address this, we propose a new evaluation metric that serves as a
proxy for assessing topological accuracy in strand reconstruction.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.07774v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.07774v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.07704v1'></a></p>
<h2 id="seec-segmentation-assisted-multi-entropy-models-for-learned-lossless-image-compression"><a href="https://arxiv.org/abs/2509.07704v1">SEEC: Segmentation-Assisted Multi-Entropy Models for Learned Lossless Image Compression</a></h2>
<p><strong>Authors:</strong> Chunhang Zheng, Zichang Ren, Dou Li</p>
<p><strong>Published:</strong> 2025-09-09</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Recently, learned image compression has attracted considerable attention due
to its superior performance over traditional methods. However, most existing
approaches employ a single entropy model to estimate the probability
distribution of pixel values across the entire image, which limits their
ability to capture the diverse statistical characteristics of different
semantic regions. To overcome this limitation, we propose Segmentation-Assisted
Multi-Entropy Models for Lossless Image Compression (SEEC). Our framework
utilizes semantic segmentation to guide the selection and adaptation of
multiple entropy models, enabling more accurate probability distribution
estimation for distinct semantic regions. Specifically, SEEC first extracts
image features and then applies semantic segmentation to identify different
regions, each assigned a specialized entropy model to better capture its unique
statistical properties. Finally, a multi-channel discrete logistic mixture
likelihood is employed to model the pixel value distributions effectively.
Experimental results on benchmark datasets demonstrate that SEEC achieves
state-of-the-art compression ratios while introducing only minimal encoding and
decoding latency. With superior performance, the proposed model also supports
Regions of Interest (ROIs) coding condition on the provided segmentation mask.
Our code is available at https://github.com/chunbaobao/SEEC.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Chunhang Zheng, Zichang Ren, Dou Liæ°åçè®ºæâSEEC: Segmentation-Assisted Multi-Entropy Models for Learned Lossless Image Compressionâçå¨é¢æè¦ã</p>
<hr />
<h3 id="seec-segmentation-assisted-multi-entropy-models-for-learned-lossless-image-compression_1">è®ºæãSEEC: Segmentation-Assisted Multi-Entropy Models for Learned Lossless Image Compressionãæè¦</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
å½ååºäºå­¦ä¹ çå¾åæ æåç¼©æ¹æ³å¤§å¤éç¨åä¸çµæ¨¡åæ¥ä¼°è®¡æ´ä¸ªå¾åçåç´ å¼æ¦çåå¸ãç¶èï¼è¿ç§åä¸æ¨¡åé¾ä»¥ææææå¾åä¸­ä¸åè¯­ä¹åºåï¼å¦å¤©ç©ºãéè·¯ãæ¤è¢«ç­ï¼ä¹é´å¤æ ·åçç»è®¡ç¹æ§ï¼ä»èéå¶äºåç¼©æ§è½çè¿ä¸æ­¥æåãè®ºææ¨å¨è§£å³å¦ä½æ´åç¡®å°å»ºæ¨¡å¾åä¸­å¼æè¯­ä¹åºåçåç´ å¼åå¸ï¼ä»¥å®ç°æ´é«çæ æå¾ååç¼©æ¯ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è´¡ç®ï¼</strong>
*   <strong>åå²è¾å©å¤çµæ¨¡åï¼SMEMï¼ï¼</strong> è®ºææåºäºSEECï¼Segmentation-Assisted Multi-Entropy Models for Lossless Image Compressionï¼æ¡æ¶ï¼å¶æ ¸å¿åæ°å¨äºå©ç¨è¯­ä¹åå²æ¥æå¯¼å¤ä¸ªçµæ¨¡åçéæ©åèªéåºãéè¿å°å¾åååä¸ºä¸åçè¯­ä¹åºåï¼å¹¶ä¸ºæ¯ä¸ªåºååéä¸é¨ççµæ¨¡åï¼SEECè½å¤æ´åç¡®å°ä¼°è®¡åç´ å¼åå¸ï¼ä»èæ¾èæé«åç¼©æ§è½ã
*   <strong>å¤ééç¦»æ£é»è¾æ··åä¼¼ç¶ï¼Multi-channel Discrete Logistic Mixture Likelihoodï¼ï¼</strong> ä¸ºäºææå»ºæ¨¡èªç¶å¾åä¸­åç´ å¼çå¤æåå¸ï¼SEECéç¨äºä¸ç§å¤ééç¦»æ£é»è¾æ··åä¼¼ç¶æ¨¡åãè¯¥æ¨¡åè½å¤é¢æµæ¯ä¸ªå¾åééçæ··åç³»æ°ï¼å¹¶ä¸ºæ¯ä¸ªééåæ¯ä¸ªæ··ååéåéç¹å®çæ··åæéï¼ä»¥æ´å¥½å°ææä¸åè¯­ä¹åºåçç¬ç¹ç»è®¡ç¹æ§ã
*   <strong>åºåå´è¶£ï¼ROIsï¼ç¼ç ç­ç¥ï¼</strong> è®ºææåºäºä¸ç§åºäºåå²æ©ç çROIsç¼ç ç­ç¥ãè¯¥ç­ç¥åè®¸å¯¹æå´è¶£åºåï¼åæ¯ï¼è¿è¡æ æåç¼©ï¼èå¯¹éæå´è¶£åºåï¼èæ¯ï¼åéç¨æ´å®½æ¾çä¿çåº¦è¿è¡éå»ºï¼ä»èå¨ä¿æéè¦åºåæ æçåæ¶ï¼éä½æ´ä½æ¯ç¹çåç¼ç /è§£ç æ¶é´ã
*   <strong>è¯­ä¹æç¥å¾ååç¼©å¨ï¼SICï¼ï¼</strong> SEECæ¡æ¶åå«SICæ¨¡åï¼ç¨äºä»è¾å¥å¾åä¸­æåè¯­ä¹æç¥ç¹å¾ï¼å¹¶å°å¶åç¼©ä¸ºæ½å¨è¡¨ç¤ºãè¯¥æ¨¡åç»åäºè¶åéªæ¨¡ååSwin-Attentionæºå¶ï¼ä»¥å¢å¼ºç¹å¾æåè½åã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>æåè¿çåç¼©æ¯ï¼</strong> å¨DIV2KãCLIC.pãCLIC.mãKodakãAdobe PortraitåUrban100ç­åºåæ°æ®éä¸çå®éªç»æè¡¨æï¼SEECå¨æ æå¾ååç¼©æ¹é¢åå¾äºæåè¿çåç¼©æ¯ãä¸ä¼ ç»æ¹æ³ï¼å¦FLIFï¼ç¸æ¯ï¼SEECçæ¯ç¹çéä½äº5.2%è³14.1%ãä¸DLPRç­åè¿å­¦ä¹ æ¹æ³ç¸æ¯ï¼SEECçæ¯ç¹çæå¤å¯éä½3.0%ã
*   <strong>æå°çç¼ç åè§£ç å»¶è¿ï¼</strong> å°½ç®¡å¼å¥äºè¯­ä¹åå²åå¤çµæ¨¡åï¼SEECä»è½ä¿ææå°çç¼ç åè§£ç å»¶è¿ãä¸åä¸çµæ¨¡ååä½ç¸æ¯ï¼å¤çµæ¨¡åä»å¼å¥äºå¾®å°çé¢å¤æ¶é´å¼éã
*   <strong>è¯­ä¹åå²çæææ§ï¼</strong> æ¶èç ç©¶è¯å®ï¼åå²è¾å©å¤çµæ¨¡ååå¤ééç¦»æ£é»è¾æ··åä¼¼ç¶é½å¯¹SEECçæ§è½æåååºäºå®è´¨æ§è´¡ç®ãä½¿ç¨æ­£ç¡®çåå²æ©ç å¯¹äºæ§è½è³å³éè¦ï¼éæºæä¸æ­£ç¡®çæ©ç ä¼å¯¼è´æ§è½ä¸éã
*   <strong>ROIsç¼ç çæçï¼</strong> ROIsç¼ç ç­ç¥éè¿è·³è¿éROIsççµç¼ç é¶æ®µï¼å°è¿è¡æ¶åå°äº25%ï¼åæ¶ä¿æäºROIsåçæ æéå»ºï¼å±ç¤ºäºå¶çµæ´»æ§åæçã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
è®ºæä¸­æ²¡ææç¡®æåæ¾èçå±éæ§ãç¶èï¼å¯ä»¥æ¨æ­çæ½å¨å±éæ§å¯è½åæ¬ï¼
*   <strong>å¯¹åå²æ¨¡åæ§è½çä¾èµï¼</strong> SEECçæ§è½å¨ä¸å®ç¨åº¦ä¸ä¾èµäºè¯­ä¹åå²æ¨¡åçåç¡®æ§ãå¦æåå²æ¨¡åäº§çä¸åç¡®çæ©ç ï¼å¯è½ä¼å½±åçµæ¨¡åçéæ©ååç´ å¼åå¸ä¼°è®¡çåç¡®æ§ã
*   <strong>è®¡ç®å¼éï¼å°½ç®¡å·²ä¼åï¼ï¼</strong> å°½ç®¡è®ºææåºåå²æ¨¡ååæ©ç å­å¨å¼å¥çå¼éå¾å°ï¼å¹³å0.02 bppï¼ï¼ä½å¯¹äºèµæºåéçè®¾å¤æå®æ¶åºç¨ï¼é¢å¤çåå²æ­¥éª¤ä»å¯è½æ¯ä¸ä¸ªèèå ç´ ã
*   <strong>Nå¼ï¼è¯­ä¹ç±»å«æ°éï¼çéæ©ï¼</strong> è®ºæå°è¯­ä¹ç±»å«æ°éNè®¾ç½®ä¸º2ï¼åæ¯åèæ¯ï¼ï¼ä»¥å¹³è¡¡æ¨¡åå¤ææ§åæ§è½ãå¯¹äºæ´å¤æçåºæ¯æéè¦æ´ç»ç²åº¦åç¼©çåºç¨ï¼Nå¼çéæ©å¯è½éè¦è¿ä¸æ­¥ç ç©¶åä¼åã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>æ´ç²¾ç»çè¯­ä¹åå²éæï¼</strong> æ¢ç´¢æ´åè¿ææ´ç»ç²åº¦çè¯­ä¹åå²ææ¯ï¼ä»¥è¯å«æ´å¤è¯­ä¹ç±»å«ï¼å¹¶ä¸ºæ¯ä¸ªç±»å«åéæ´ä¸ä¸ççµæ¨¡åï¼ä»èè¿ä¸æ­¥æé«åç¼©æ§è½ã
*   <strong>èªéåºNå¼éæ©ï¼</strong> ç ç©¶å¦ä½æ ¹æ®å¾ååå®¹æåºç¨éæ±ï¼èªéåºå°ç¡®å®è¯­ä¹ç±»å«Nçæ°éï¼èä¸æ¯åºå®ä¸º2ã
*   <strong>å¨æçµæ¨¡åéæ©ï¼</strong> æ¢ç´¢æ´å¨æççµæ¨¡åéæ©æºå¶ï¼ä¾å¦åºäºå¾åç¹å¾æåºåå¤ææ§ï¼èªå¨éæ©æéåççµæ¨¡åï¼èä¸æ¯ç¡¬æ§åéã
*   <strong>ç»åå¶ä»åè¿åç¼©ææ¯ï¼</strong> å°SEECæ¡æ¶ä¸çææ¨¡åãæ³¨æåæºå¶æå¶ä»åè¿çå¾ååç¼©ææ¯ç¸ç»åï¼ä»¥æ¢ç´¢è¿ä¸æ­¥çæ§è½æåã
*   <strong>æ©å±å°è§é¢åç¼©ï¼</strong> å°åå²è¾å©å¤çµæ¨¡åçææ³æ©å±å°è§é¢åç¼©é¢åï¼ä»¥å¤çè§é¢å¸§ä¸­å¨æååçè¯­ä¹åå®¹ã</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>To overcome this limitation, we propose Segmentation-Assisted
Multi-Entropy Models for Lossless Image Compression (SEEC).</li>
<li>Experimental results on benchmark datasets demonstrate that SEEC achieves
state-of-the-art compression ratios while introducing only minimal encoding and
decoding latency.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.07704v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.07704v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.07647v1'></a></p>
<h2 id="semantic-watermarking-reinvented-enhancing-robustness-and-generation-quality-with-fourier-integrity"><a href="https://arxiv.org/abs/2509.07647v1">Semantic Watermarking Reinvented: Enhancing Robustness and Generation Quality with Fourier Integrity</a></h2>
<p><strong>Authors:</strong> Sung Ju Lee, Nam Ik Cho</p>
<p><strong>Published:</strong> 2025-09-09</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Semantic watermarking techniques for latent diffusion models (LDMs) are
robust against regeneration attacks, but often suffer from detection
performance degradation due to the loss of frequency integrity. To tackle this
problem, we propose a novel embedding method called Hermitian Symmetric Fourier
Watermarking (SFW), which maintains frequency integrity by enforcing Hermitian
symmetry. Additionally, we introduce a center-aware embedding strategy that
reduces the vulnerability of semantic watermarking due to cropping attacks by
ensuring robust information retention. To validate our approach, we apply these
techniques to existing semantic watermarking schemes, enhancing their
frequency-domain structures for better robustness and retrieval accuracy.
Extensive experiments demonstrate that our methods achieve state-of-the-art
verification and identification performance, surpassing previous approaches
across various attack scenarios. Ablation studies confirm the impact of SFW on
detection capabilities, the effectiveness of the center-aware embedding against
cropping, and how message capacity influences identification accuracy. Notably,
our method achieves the highest detection accuracy while maintaining superior
image fidelity, as evidenced by FID and CLIP scores. Conclusively, our proposed
SFW is shown to be an effective framework for balancing robustness and image
fidelity, addressing the inherent trade-offs in semantic watermarking. Code
available at https://github.com/thomas11809/SFWMark</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Sung Ju LeeåNam Ik Choæ°åçè®ºæâSemantic Watermarking Reinvented: Enhancing Robustness and Generation Quality with Fourier Integrityâçå¨é¢æè¦ã</p>
<hr />
<h3 id="_1">è®ºææè¦ï¼è¯­ä¹æ°´å°çéå¡ï¼éè¿åéå¶å®æ´æ§å¢å¼ºé²æ£æ§åçæè´¨é</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³ç°æéå¯¹æ½å¨æ©æ£æ¨¡åï¼LDMsï¼çè¯­ä¹æ°´å°ææ¯æé¢ä¸´çæ ¸å¿ææãå°½ç®¡è¿äºææ¯å¯¹åçæ»å»å·æé²æ£æ§ï¼ä½å®ä»¬éå¸¸ç±äºé¢çå®æ´æ§çä¸§å¤±èå¯¼è´æ£æµæ§è½ä¸éãæ­¤å¤ï¼è¿äºæ¹æ³å¨é¢å¯¹è£åªæ»å»æ¶ä¹è¡¨ç°åºèå¼±æ§ãå æ­¤ï¼ç ç©¶é®é¢æ¯å¦ä½å¼åä¸ç§è¯­ä¹æ°´å°æ¡æ¶ï¼æ¢è½ä¿æé¢çå®æ´æ§ä»¥æé«æ£æµæ§è½åçæè´¨éï¼åè½å¢å¼ºå¯¹ç©ºé´æ»å»ï¼å¦è£åªï¼çé²æ£æ§ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
è¯¥è®ºææåºäºä¸¤é¡¹å³é®åæ°æ¥è§£å³ä¸è¿°é®é¢ï¼</p>
<ul>
<li><strong>Hermitianå¯¹ç§°åéå¶æ°´å°ï¼SFWï¼ï¼</strong> éå¯¹é¢çå®æ´æ§ä¸§å¤±çé®é¢ï¼ä½èæåºäºä¸ç§æ°é¢çåµå¥æ¹æ³SFWãéè¿å¼ºå¶æ§è¡Hermitianå¯¹ç§°æ§ï¼SFWç¡®ä¿æ°´å°åµå¥å¨åéå¶åä¸­ä¿æç»è®¡ä¸è´æ§ï¼ä»èå¨éåéå¶åæ¢åè·å¾å®å¼ä¿¡å·ãè¿ä¸ä»æé«äºæ°´å°çå¯æ£ç´¢æ§åçææ¨¡åçç¨³å®æ§ï¼è¿ååå©ç¨äºåéå¶åçå®é¨åèé¨ä¿¡æ¯è¿è¡æ£æµã</li>
<li><strong>ä¸­å¿æç¥åµå¥ç­ç¥ï¼</strong> ä¸ºäºåå°è¯­ä¹æ°´å°å¯¹è£åªæ»å»çèå¼±æ§ï¼è®ºæå¼å¥äºä¸ç§ä¸­å¿æç¥åµå¥ç­ç¥ãè¯¥ç­ç¥ä»å¯¹æ½å¨åéç©ºé´åçä¸­å¿åºååºç¨åéå¶åæ¢è¿è¡æ°´å°åµå¥ï¼èä¸æ¯å¯¹æ´ä¸ªç©ºé´ç©éµè¿è¡æä½ãè¿ç§æ¹æ³ç¡®ä¿äºæ°´å°ä¿¡æ¯å¨ç©ºé´ä¸æ´å·å¼¹æ§åºåçä¿çï¼ä»èå¢å¼ºäºå¯¹è£åªæ»å»çé²æ£æ§ã</li>
</ul>
<p>è®ºæå°è¿äºææ¯åºç¨äºç°æè¯­ä¹æ°´å°æ¹æ¡ï¼å¦Tree-RingåRingIDï¼ï¼å¹¶æåºäºä¸¤ç§å·ä½å®ç°ï¼<strong>Hermitianå¯¹ç§°Tree-Ring (HSTR)</strong> å <strong>Hermitianå¯¹ç§°QRç  (HSQR)</strong>ï¼åèå°QRç çäºè¿å¶æ¨¡å¼æåå¹¶åå«åµå¥å°åéå¶åèªç±ååºåçå®é¨åèé¨ä¸­ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶éè¦æ§ï¼</strong>
éè¿å¹¿æ³çå®éªï¼è®ºæå±ç¤ºäºå¶æ¹æ³çåè¶æ§è½ï¼</p>
<ul>
<li><strong>æåè¿çæ£æµæ§è½ï¼</strong> HSTRåHSQRå¨éªè¯åè¯å«ä»»å¡ä¸­åå®ç°äºæåè¿çæ£æµæ§è½ï¼å¨åç§æ»å»åºæ¯ï¼åæ¬ä¿¡å·å¤çå¤±çãåçæ»å»åè£åªæ»å»ï¼ä¸åè¶è¶äºç°ææ¹æ³ã</li>
<li><strong>é¢çå®æ´æ§ä¸çæè´¨éçå¹³è¡¡ï¼</strong> æåºçSFWæ¹æ³éè¿ä¿æé¢çå®æ´æ§ï¼å¨æ°´å°é²æ£æ§åçæè´¨éä¹é´åå¾äºæ´å¥½çå¹³è¡¡ãFIDåCLIPåæ°è¯æï¼HSTRåHSQRå¨ä¿æé«æ£æµåç¡®æ§çåæ¶ï¼ä¹ä¿æäºåè¶çå¾åä¿çåº¦ï¼é¿åäºç°ææ¹æ³ï¼å¦RingIDï¼ä¸­åºç°çå¯è§ç¯ç¶ä¼ªå½±ã</li>
<li><strong>å¯¹è£åªæ»å»çé²æ£æ§ï¼</strong> ä¸­å¿æç¥åµå¥ç­ç¥æ¾èæé«äºå¯¹è£åªæ»å»çé²æ£æ§ï¼å³ä½¿å¨æç«¯è£åªï¼å¦0.2çè£åªæ¯ä¾ï¼ä¸ï¼HSTRåHSQRä¹æ¯RingIDç­æ¹æ³è¡¨ç°åºæ´å¹³ç¨³çæ§è½ä¸éã</li>
<li><strong>æ¶æ¯å®¹éçå¯æ©å±æ§ï¼</strong> HSQRå¨ä¸åæ°´å°æ¶æ¯å®¹éä¸è¡¨ç°åºæé«çè¯å«åç¡®æ§ï¼å³ä½¿å¨æå¤§å®¹éä¸ä¹è½ä¿ææ¥è¿å®ç¾çåç¡®æ§ï¼å±ç¤ºäºå¶åè¶çå¯æ©å±æ§ã</li>
<li><strong>åéå¶åçååå©ç¨ï¼</strong> è®ºæç»æè¡¨æï¼æ½å¨ç©ºé´æ°´å°ä¸åä¼ ç»ä½ä¸­é¢çº¦æçéå¶ï¼éè¿ç»è®¡ç»æåç¼ç ï¼æ°´å°ä¿¡æ¯å¯ä»¥åå¸å¨æ´å®½çé¢çèå´åï¼åæ¶ä¿æé²æ£æ§ã</li>
</ul>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
è®ºæä¸­æç¡®æåçå±éæ§ä¸»è¦åæ¬ï¼</p>
<ul>
<li><strong>éæ¶æç¯¡æ¹çèå´ï¼</strong> è®ºæçæ¹æ³ä¸»è¦è®¾è®¡ç¨äºåºå¯¹å¸åçãéæ¶æçååæè½¬æ¢è¿ç¨ä¸­çåå®¹ååï¼èéæ£æµå¤å´ç¯¡æ¹æå¯¹ææ§æ»å»ãåèéè¦ä¸åçå¨èæ¨¡ååè®¾è®¡èèã</li>
<li><strong>è®¡ç®ææ¬ï¼éå¯¹åºçº¿ï¼ï¼</strong> æäºåºçº¿æ¹æ³ï¼å¦Zodiacï¼ç±äºéè¦å¤æ¬¡æ©æ£çæåæ½å¨åéä¼åè¿­ä»£ï¼å¶è®¡ç®ææ¬è¿é«ï¼ä¸éåå®éåºç¨ãè½ç¶æ¬ææåºçæ¹æ³éè¿âçææ¶åå¹¶âæ¹æ¡é¿åäºè¿ä¸é®é¢ï¼ä½è¿æ¯å¯¹ç°ææ¹æ³çä¸ä¸ªè§å¯ã</li>
<li><strong>åçæ»å»å¼ºåº¦ï¼</strong> è®ºæéè¿æ¶èç ç©¶æ¢è®¨äºæ©æ£æ¨¡åä¸­åçæ»å»çåªå£°å¼ºåº¦ï¼ä»¥ç¡®ä¿æ»å»å¼ºåº¦è¶³å¤ï¼ä½å¹¶æªæç¡®æåºå¶æ¹æ³å¨é¢å¯¹æªæ¥å¯è½åºç°çæ´æç«¯ææ´å¤æçå¯¹ææ§æ»å»æ¶çè¡¨ç°ã</li>
</ul>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
è®ºææåºäºä»¥ä¸æªæ¥ç ç©¶æ¹åï¼</p>
<ul>
<li><strong>èªéåºåµå¥ç­ç¥ï¼</strong> æ¢ç´¢èªéåºåµå¥ç­ç¥ï¼ä»¥è¿ä¸æ­¥å¢å¼ºå¯¹å¯¹ææ§æ»å»åæç«¯å¤±ççé²æ£æ§ã</li>
<li><strong>å¤æ ·åçææ¶æï¼</strong> å°ææåºçæ¹æ³æ©å±å°æ½å¨æ©æ£æ¨¡åä¹å¤çæ´å¤æ ·åççææ¶æã</li>
<li><strong>å®æ¶é¨ç½²ï¼</strong> è¿ä¸æ­¥ä¼åæ¹æ³ï¼ä½¿å¶ä¸ä½åèãé«ååéçAIæ¨çå éå¨ï¼å¦NPUï¼å¼å®¹ï¼ä»¥å®ç°å¯æ©å±ãèè½ç¯å¢ä¸­çæ ç¼é¨ç½²ã</li>
</ul>
<hr />
<p>æ»èè¨ä¹ï¼è¿ç¯è®ºæéè¿å¼å¥Hermitianå¯¹ç§°åéå¶æ°´å°åä¸­å¿æç¥åµå¥ç­ç¥ï¼æåå°éå¡äºæ½å¨æ©æ£æ¨¡åä¸­çè¯­ä¹æ°´å°ææ¯ãå®ä¸ä»è§£å³äºé¢çå®æ´æ§ä¸§å¤±åè£åªæ»å»èå¼±æ§çå³é®é®é¢ï¼è¿å¨é²æ£æ§ãçæè´¨éåå¯æ©å±æ§ä¹é´åå¾äºåè¶çå¹³è¡¡ï¼ä¸ºæ°å­åå®¹æº¯æºåçæä¿æ¤é¢åæä¾äºéè¦è¿å±ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To tackle this
problem, we propose a novel embedding method called Hermitian Symmetric Fourier
Watermarking (SFW), which maintains frequency integrity by enforcing Hermitian
symmetry.</li>
<li>Additionally, we introduce a center-aware embedding strategy that
reduces the vulnerability of semantic watermarking due to cropping attacks by
ensuring robust information retention.</li>
<li>To validate our approach, we apply these
techniques to existing semantic watermarking schemes, enhancing their
frequency-domain structures for better robustness and retrieval accuracy.</li>
<li>Extensive experiments demonstrate that our methods achieve state-of-the-art
verification and identification performance, surpassing previous approaches
across various attack scenarios.</li>
<li>Notably,
our method achieves the highest detection accuracy while maintaining superior
image fidelity, as evidenced by FID and CLIP scores.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.07647v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.07647v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.07530v1'></a></p>
<h2 id="universal-few-shot-spatial-control-for-diffusion-models"><a href="https://arxiv.org/abs/2509.07530v1">Universal Few-Shot Spatial Control for Diffusion Models</a></h2>
<p><strong>Authors:</strong> Kiet T. Nguyen, Chanhuyk Lee, Donggyun Kim, Dong Hoon Lee, Seunghoon Hong</p>
<p><strong>Published:</strong> 2025-09-09</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Spatial conditioning in pretrained text-to-image diffusion models has
significantly improved fine-grained control over the structure of generated
images. However, existing control adapters exhibit limited adaptability and
incur high training costs when encountering novel spatial control conditions
that differ substantially from the training tasks. To address this limitation,
we propose Universal Few-Shot Control (UFC), a versatile few-shot control
adapter capable of generalizing to novel spatial conditions. Given a few
image-condition pairs of an unseen task and a query condition, UFC leverages
the analogy between query and support conditions to construct task-specific
control features, instantiated by a matching mechanism and an update on a small
set of task-specific parameters. Experiments on six novel spatial control tasks
show that UFC, fine-tuned with only 30 annotated examples of novel tasks,
achieves fine-grained control consistent with the spatial conditions. Notably,
when fine-tuned with 0.1% of the full training data, UFC achieves competitive
performance with the fully supervised baselines in various control tasks. We
also show that UFC is applicable agnostically to various diffusion backbones
and demonstrate its effectiveness on both UNet and DiT architectures. Code is
available at https://github.com/kietngt00/UFC.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Kiet T. Nguyenç­äººæ°åçè®ºæâUniversal Few-Shot Spatial Control for Diffusion Modelsâçå¨é¢æè¦ã</p>
<hr />
<h3 id="universal-few-shot-spatial-control-for-diffusion-models_1">è®ºæãUniversal Few-Shot Spatial Control for Diffusion Modelsãå¨é¢æè¦</h3>
<p><strong>1. è®ºæä¸»è¦é®é¢æç ç©¶é®é¢</strong></p>
<p>è¯¥è®ºææ¨å¨è§£å³é¢è®­ç»ææ¬å°å¾åï¼T2Iï¼æ©æ£æ¨¡åå¨ç©ºé´æ§å¶æ¹é¢çå±éæ§ãå°½ç®¡ç°ææ¹æ³ï¼å¦ControlNetï¼éè¿å¼å¥æ§å¶ééå¨æ¾èæåäºå¾åçæçç²¾ç»æ§å¶è½åï¼ä½è¿äºééå¨éå¸¸éè¦éå¯¹æ¯ç§æ°çç©ºé´æ§å¶ä»»å¡è¿è¡ç¬ç«è®­ç»ï¼è¿ä¸ä»è®¡ç®ææ¬é«æï¼èä¸éè¦å¤§éçæ æ³¨æ°æ®ãå½éå°ä¸è®­ç»ä»»å¡æ¾èä¸åçæ°é¢ç©ºé´æ¡ä»¶æ¶ï¼ç°æééå¨çéåºæ§æéï¼æ³åè½åè¾å·®ãå æ­¤ï¼æ ¸å¿ç ç©¶é®é¢æ¯å¦ä½å¼åä¸ç§éç¨ãæ°æ®é«æçå°æ ·æ¬æ§å¶ééå¨ï¼ä½¿å¶è½å¤ä»¥æå°çæ æ³¨æ°æ®ï¼ä¾å¦ï¼å åä¸ªå¾å-æ¡ä»¶å¯¹ï¼æ³åå°åç§æ°é¢çç©ºé´æ¡ä»¶ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®</strong></p>
<p>ä¸ºäºè§£å³ä¸è¿°ææï¼ä½èæåºäº<strong>éç¨å°æ ·æ¬æ§å¶ï¼Universal Few-Shot Control, UFCï¼</strong>æ¡æ¶ï¼å¶ä¸»è¦åæ°ç¹åæ¬ï¼</p>
<ul>
<li><strong>éç¨æ§å¶ééå¨è®¾è®¡ï¼</strong> UFCå¼å¥äºä¸ä¸ªéç¨çæ§å¶ééå¨ï¼è½å¤å°å¼æç©ºé´æ¡ä»¶ï¼å¦è¾¹ç¼å¾ãæ·±åº¦å¾ãå§¿æç­ï¼ç»ä¸ä¸ºä¸å¾åç¹å¾å¼å®¹çæ§å¶ç¹å¾ãè¿éè¿<strong>è¡¥ä¸çº§å¹éæºå¶</strong>å®ç°ï¼è¯¥æºå¶å©ç¨æ¥è¯¢æ¡ä»¶åæ¯æéå¾å-æ¡ä»¶å¯¹ä¹é´çç±»æ¯å³ç³»ï¼æå»ºä»»å¡ç¹å®çæ§å¶ç¹å¾ãæ¯æéå¾åçè§è§ç¹å¾ä½ä¸ºä»»å¡æ å³çåºåºï¼èæ¡ä»¶ä¹é´çè¡¥ä¸çº§ç¸ä¼¼æ§åæ°åç¨äºè®¡ç®éæ©ç¸å³ç¹å¾çæéã</li>
<li><strong>é«æçå°æ ·æ¬éåºæºå¶ï¼</strong> ä¸ºäºå¨æ°æ®ç¨ç¼ºçæåµä¸å¿«ééåºæ°ä»»å¡èä¸åçè¿æåï¼UFCç»åäº<strong>ææ¯å¼åå­¦ä¹ ï¼episodic meta-learningï¼</strong>å<strong>åæ°é«æå¾®è°ï¼parameter-efficient fine-tuningï¼</strong>ãå¨åè®­ç»é¶æ®µï¼æ¨¡åå­¦ä¹ ä¸ä¸ªéç¨çåæ°éï¼å¹¶å¨æ°ä»»å¡ä¸ä»å¾®è°ä¸å°ç»ä»»å¡ç¹å®åæ°ï¼ä¾å¦ï¼åç½®æLoRAåæ°ï¼ï¼ä»èå®ç°é«æçæµè¯æ¶éåºã</li>
<li><strong>æ¶ææ å³æ§ï¼</strong> UFCçè®¾è®¡å·æéç¨æ§ï¼å¯ä»¥ä¸ä¸åçæ©æ£æ¨¡åéª¨å¹²ï¼å¦UNetåDiTï¼ä»¥åç°æçééå¨æ¶æï¼å¦ControlNetï¼å¼å®¹ï¼éè¿å¨å¤å±æ³¨å¥æ§å¶ç¹å¾æ¥å¢å¼ºæ¨¡åçç©ºé´æ§å¶è½åã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹</strong></p>
<p>è®ºæéè¿å¨å­ä¸ªæ°é¢ç©ºé´æ§å¶ä»»å¡ï¼CannyãHEDãæ·±åº¦ãæ³çº¿ãå§¿æãDenseposeï¼ä¸çå¹¿æ³å®éªï¼éªè¯äºUFCçæææ§ï¼</p>
<ul>
<li><strong>åè¶çå°æ ·æ¬æ§è½ï¼</strong> UFCå¨ä»ä½¿ç¨30ä¸ªæ æ³¨ç¤ºä¾è¿è¡å¾®è°çæåµä¸ï¼å®ç°äºä¸ç©ºé´æ¡ä»¶é«åº¦ä¸è´çç²¾ç»æ§å¶ãå¨å°æ ·æ¬è®¾ç½®ä¸ï¼UFCå¨å¯æ§æ§æ¹é¢æ¾èä¼äºææç°æå°æ ·æ¬ååè®­ç»åºçº¿æ¹æ³ã</li>
<li><strong>ä¸å¨çç£åºçº¿ç«äºï¼</strong> å¼å¾æ³¨æçæ¯ï¼å½ä»ä½¿ç¨0.1%çå®æ´è®­ç»æ°æ®è¿è¡å¾®è°æ¶ï¼UFCå¨åç§æ§å¶ä»»å¡ä¸­è¾¾å°äºä¸å¨çç£åºçº¿ï¼ControlNetåUni-ControlNetï¼ç¸å½çæ§è½ï¼å°¤å¶æ¯å¨Denseposeç­å¯éæ¡ä»¶ä»»å¡ä¸è¡¨ç°åºè²ãè¿è¡¨æUFCå¨æ°æ®æçæ¹é¢å·ææ¾èä¼å¿ã</li>
<li><strong>å¯¹ä¸åéª¨å¹²çå¼å®¹æ§ï¼</strong> å®éªè¯æUFCè½å¤æååºç¨äºUNetåDiTä¸¤ç§ä¸åçæ©æ£æ¨¡åéª¨å¹²ï¼å¹¶è½å©ç¨æ´å¼ºå¤§çDiTéª¨å¹²å®ç°æ´ç²¾ç»çç©ºé´æ§å¶ã</li>
<li><strong>å¯¹æ°é¢3Dç»ææ¡ä»¶çæ³åï¼</strong> UFCè¿å±ç¤ºäºå¶å¨3Dç½æ ¼ãçº¿æ¡åç¹äºç­æ´å·æææ§çæ°é¢ç©ºé´æ¡ä»¶ä¸çæææ§ï¼è¿ä¸æ­¥éªè¯äºå¶å¼ºå¤§çæ³åè½åã</li>
</ul>
<p>è¿äºç»æè¡¨æï¼UFCä¸ºT2Iæ©æ£æ¨¡åæä¾äºä¸ç§éç¨ãæ°æ®é«æä¸çµæ´»çå°æ ·æ¬ç©ºé´æ§å¶è§£å³æ¹æ¡ï¼æå¤§å°æåäºç©ºé´æ§å¶æ¹æ³çå®ç¨æ§åçµæ´»æ§ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§</strong></p>
<p>è®ºæä¹å¦è¯å°æåºäºUFCçå ä¸ªå±éæ§ï¼</p>
<ul>
<li><strong>ä¸»è¦é¢åç©ºé´æ§å¶çæï¼</strong> UFCæ¡æ¶ä¸»è¦è®¾è®¡ç¨äºç©ºé´æ§å¶çæï¼èééè¦ä¿çæ¡ä»¶å¾åå¤è§çä»»å¡ï¼ä¾å¦é£æ ¼è¿ç§»ãå¾åä¿®å¤æå»æ¨¡ç³ç­éé®é¢ã</li>
<li><strong>éè¦å°éæ æ³¨æ°æ®è¿è¡å¾®è°ï¼</strong> å°½ç®¡æ¯å°æ ·æ¬æ¹æ³ï¼UFCä»éè¦ä¸ºæ¯ä¸ªæ°ä»»å¡æä¾å°éæ æ³¨æ°æ®è¿è¡å¾®è°ãè¿ä¸å¤§åè¯­è¨æ¨¡åéè¿ä¸ä¸æå­¦ä¹ ï¼in-context learningï¼ç´æ¥éåºæ°ä»»å¡çæ¹å¼ä¸åï¼åèæ éå¾®è°ã</li>
</ul>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹å</strong></p>
<p>åºäºä¸è¿°å±éæ§ï¼è®ºææåºäºä»¥ä¸æªæ¥ç ç©¶æ¹åï¼</p>
<ul>
<li><strong>æ©å±æ¡æ¶ä»¥å¤çå¤è§ä¿çä»»å¡ï¼</strong> å°UFCæ¡æ¶æ©å±å°è½å¤å¤çé£æ ¼è¿ç§»ãå¾åä¿®å¤ç­éè¦ä¿çæ¡ä»¶å¾åå¤è§çä»»å¡ã</li>
<li><strong>å®ç°æ éå¾®è°çä¸ä¸æå­¦ä¹ ï¼</strong> æ¢ç´¢å¼åç±»ä¼¼å¤§åè¯­è¨æ¨¡åçè½åï¼ä½¿ç©ºé´æ§å¶å¾åçæè½å¤éè¿ä¸ä¸æå­¦ä¹ ï¼ä»ä»å°éç¤ºä¾ä¸­éåºæ°ä»»å¡ï¼èæ éä»»ä½å¾®è°ã</li>
</ul>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>However, existing control adapters exhibit limited adaptability and
incur high training costs when encountering novel spatial control conditions
that differ substantially from the training tasks.</li>
<li>To address this limitation,
we propose Universal Few-Shot Control (UFC), a versatile few-shot control
adapter capable of generalizing to novel spatial conditions.</li>
<li>Experiments on six novel spatial control tasks
show that UFC, fine-tuned with only 30 annotated examples of novel tasks,
achieves fine-grained control consistent with the spatial conditions.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.07530v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.07530v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.07507v1'></a></p>
<h2 id="mvat-multi-view-aware-teacher-for-weakly-supervised-3d-object-detection"><a href="https://arxiv.org/abs/2509.07507v1">MVAT: Multi-View Aware Teacher for Weakly Supervised 3D Object Detection</a></h2>
<p><strong>Authors:</strong> Saad Lahlali, Alexandre Fournier Montgieux, Nicolas Granger, HervÃ© Le Borgne, Quoc Cuong Pham</p>
<p><strong>Published:</strong> 2025-09-09</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Annotating 3D data remains a costly bottleneck for 3D object detection,
motivating the development of weakly supervised annotation methods that rely on
more accessible 2D box annotations. However, relying solely on 2D boxes
introduces projection ambiguities since a single 2D box can correspond to
multiple valid 3D poses. Furthermore, partial object visibility under a single
viewpoint setting makes accurate 3D box estimation difficult. We propose MVAT,
a novel framework that leverages temporal multi-view present in sequential data
to address these challenges. Our approach aggregates object-centric point
clouds across time to build 3D object representations as dense and complete as
possible. A Teacher-Student distillation paradigm is employed: The Teacher
network learns from single viewpoints but targets are derived from temporally
aggregated static objects. Then the Teacher generates high quality
pseudo-labels that the Student learns to predict from a single viewpoint for
both static and moving objects. The whole framework incorporates a multi-view
2D projection loss to enforce consistency between predicted 3D boxes and all
available 2D annotations. Experiments on the nuScenes and Waymo Open datasets
demonstrate that MVAT achieves state-of-the-art performance for weakly
supervised 3D object detection, significantly narrowing the gap with fully
supervised methods without requiring any 3D box annotations. % \footnote{Code
available upon acceptance} Our code is available in our public repository
(\href{https://github.com/CEA-LIST/MVAT}{code}).</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Saad Lahlaliç­äººæ°åçè®ºæâMVAT: Multi-View Aware Teacher for Weakly Supervised 3D Object Detectionâçå¨é¢æè¦ã</p>
<hr />
<p><strong>è®ºææè¦ï¼MVAT: Multi-View Aware Teacher for Weakly Supervised 3D Object Detection</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
3Dç®æ æ£æµå¨èªå¨é©¾é©¶åæºå¨äººç­é¢åè³å³éè¦ï¼ä½å¶ä¸»è¦ç¶é¢å¨äºè·åç²¾ç¡®3Dè¾¹çæ¡æ æ³¨çé«æææ¬ãè½ç¶å¼±çç£æ¹æ³éè¿å©ç¨ææ¬è¾ä½ç2Dè¾¹çæ¡æ æ³¨æ¥ç¼è§£è¿ä¸é®é¢ï¼ä½ä»ä¾èµ2Dæ¡ä¼å¼å¥æå½±æ¨¡ç³æ§ï¼ä¸ä¸ª2Dæ¡å¯è½å¯¹åºå¤ä¸ªææ3Då§¿æï¼ååè§è§ä¸ç©ä½é¨åå¯è§æ§å¯¼è´ç3Dæ¡ä¼°è®¡å°é¾ãç°ææ¹æ³éå¸¸ä¾èµå¯åå¼æåéªç¥è¯æ¥è§£å³è¿äºæ¨¡ç³æ§ï¼ä½å¿½ç¥äºåºåæ°æ®ä¸­èªç¶å­å¨çæ¶é´å¤è§è§ä¿¡æ¯ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
MVATï¼Multi-View Aware Teacherï¼æåºäºä¸ç§æ°é¢çå¼±çç£3Dç®æ æ£æµæ¡æ¶ï¼éè¿å©ç¨åºåæ°æ®ä¸­åºæçæ¶é´å¤è§è§ä¿¡æ¯æ¥è§£å³ä¸è¿°ææãå¶æ ¸å¿åæ°åè´¡ç®åæ¬ï¼
*   <strong>æ¶é´å¤è§è§èåï¼</strong> MVATèåè·¨æ¶é´çä»¥ç©ä½ä¸ºä¸­å¿çç¹äºï¼ä»¥æå»ºå°½å¯è½å¯éåå®æ´ç3Dç©ä½è¡¨ç¤ºï¼ä»èè§£å³åè§è§ä¸çç¨çæ§åæ¨¡ç³æ§é®é¢ã
*   <strong>Teacher-Studentè¸é¦èå¼ï¼</strong> éç¨ä¸¤é¶æ®µçTeacher-Studentè¸é¦æ¡æ¶ã
    *   <strong>Teacherç½ç»è®­ç»ï¼</strong> Teacherç½ç»ä»åè§è§å­¦ä¹ ï¼ä½å¶ç®æ æ¯ä»æ¶é´èåçéæç©ä½ä¸­æ¨å¯¼åºæ¥çãè¿ä½¿å¾Teacherè½å¤å­¦ä¹ é²æ£ç3Då ä½ã
    *   <strong>ä¼ªæ ç­¾çæä¸Studentç½ç»è®­ç»ï¼</strong> Teacherçæé«è´¨éçä¼ªæ ç­¾ï¼Studentç½ç»åä»åè§è§è¾å¥ä¸­å­¦ä¹ é¢æµéæåç§»å¨ç©ä½ãè¿ç§ç­ç¥ä½¿Studentè½å¤ææå­¦ä¹ åºå±3Då ä½å¹¶å¤çé®æ¡åç§»å¨ç©ä½ç­æææ§æåµã
*   <strong>å¤è§è§2Dæå½±æå¤±ï¼</strong> æ´ä¸ªæ¡æ¶èå¥äºå¤è§è§2Dæå½±æå¤±ï¼ä»¥å¼ºå¶é¢æµç3Dè¾¹çæ¡ä¸ææå¯ç¨ç2Dæ æ³¨ä¹é´ä¿æä¸è´æ§ï¼ä½ä¸ºå¼ºå¤§ççç£ä¿¡å·ã
*   <strong>éæ/ç§»å¨ç©ä½åç¦»ï¼</strong> éå¶èåå°éæå®ä¾ï¼å ä¸ºç§»å¨ç©ä½å¨æ²¡æå°é¢çå¼è¿å¨ä¿¡æ¯çæåµä¸é¾ä»¥å¯¹é½ãéè¿åæç¹äºè´¨å¿çæ¶é´ä¸è´æ§æ¥è¯å«éæç©ä½ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
MVATå¨nuScenesåWaymo Openæ°æ®éä¸çå®éªç»æè¡¨æï¼å®å¨å¼±çç£3Dç®æ æ£æµæ¹é¢åå¾äºæåè¿çæ§è½ï¼æ¾èç¼©å°äºä¸å¨çç£æ¹æ³ä¹é´çå·®è·ï¼ä¸æ éä»»ä½3Dè¾¹çæ¡æ æ³¨ã
*   <strong>nuScenesæ°æ®éï¼</strong> MVATå¨nuSceneséªè¯éä¸å®ç°äº47.6%çmAPå49.1%çNDSï¼æ¾èä¼äºååçé¢åæ¹æ³ALPI [5]ï¼mAPæåäº+5.8%ãè¿ä½¿å¾å¼±çç£æ¹æ³è¾¾å°äºå¨çç£Oracleæ§è½ç81.0%ï¼è¯æäºå¨ä»ä½¿ç¨2Dæ æ³¨çæåµä¸ï¼å¨å¼¥åå·®è·æ¹é¢è¿åºäºéè¦ä¸æ­¥ã
*   <strong>æææ§ç±»å«è¡¨ç°ï¼</strong> å¨å¡è½¦ï¼+7.2ï¼ãå·´å£«ï¼+5.4ï¼ãéç¢ç©ï¼+15.8ï¼åäº¤éé¥ï¼+8.6ï¼ç­ç»å¸¸è¢«é®æ¡æå ä½ç¨ççæææ§ç©ä½ç±»å«ä¸ï¼MVATçè¡¨ç°å°¤ä¸ºçªåºï¼éªè¯äºæ¶é´èåçæææ§ã
*   <strong>Waymo Openæ°æ®éï¼</strong> MVATæ¯ç¬¬ä¸ä¸ªå¨è¯¥æ°æ®éä¸æ¥åå¼±çç£æ§è½ææ çæ¹æ³ï¼å¨L1é¾åº¦ä¸ï¼è½¦è¾ãè¡äººåéªè¡èçAPåå«è¾¾å°äºOracleç91.3%ã89.4%å87.4%ï¼å±ç¤ºäºæ¹æ³çéç¨æ§ã
*   <strong>åå¼±çç£è®¾ç½®ï¼</strong> å¨ä»ä½¿ç¨2%ç3Då°é¢çå¼æ¡çæåµä¸ï¼MVATçæ§è½ä¼äºä½¿ç¨æ´å¼ºå¼±æ ç­¾ï¼3Dç¹æ æ³¨ï¼çPoint-DETR3Dï¼è¿ä¸æ­¥éªè¯äºå¶å¤è§è§æ¶é´ç­ç¥çä¿¡æ¯éã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>ç®¡éå¤ææ§ï¼</strong> MVATå¯¹éæåç§»å¨ç©ä½çåç¦»å¼å¥äºç®¡éå¤ææ§ï¼è¿å¯è½éå¶å¶å¨é«éå¬è·¯ç­å¨æç¯å¢ä¸­çæ§è½ã
*   <strong>éåæ§ç±»å«ï¼</strong> å°½ç®¡MVATå¨éæåç§»å¨ç©ä½ä¸è¡¨ç°è¯å¥½ï¼ä½å¯¹äºè¡äººç­éåæ§ç±»å«ï¼å¶æ§è½å¯è½ä»ææåç©ºé´ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>æ¾å¼è¿å¨å»ºæ¨¡ï¼</strong> ç»åæ¾å¼è¿å¨å»ºæ¨¡å¯ä»¥æ¹åç§»å¨ç©ä½çèåè´¨éï¼å¯è½æ¶é¤éæ/ç§»å¨åç¦»çéè¦ï¼å¹¶æ¹è¿éåæ§ç±»å«ï¼å¦è¡äººï¼çæ¨¡åã
*   <strong>è¾å©ä»»å¡ï¼</strong> éè¿å¼å¥è¾å©ä»»å¡æ¥é¢æµèå3Dç©ä½è§å¾çç©ºé´ç¹åå¸ï¼ä»¥åå¸§è¾å¥ä¸ºæ¡ä»¶ï¼å¯ä»¥ä¸°å¯TeacheråStudentç½ç»çè®­ç»ç®æ ãè¿å°æä¾è¶è¶3Dè¾¹çæ¡çé¢å¤å ä½çç£ï¼é¼å±ç½ç»å­¦ä¹ æ´å¨é¢ç3Då½¢ç¶è¡¨ç¤ºï¼å¹¶å¯è½æé«å¶ä»ç¨çãé¨åè§æµä¸­æ¨æ­å®æ´ç©ä½å ä½å½¢ç¶çè½åã</p>
<hr />
<p>è¿ç¯è®ºæéè¿å·§å¦å°å©ç¨æ¶é´å¤è§è§æ°æ®ï¼ä¸ºå¼±çç£3Dç®æ æ£æµé¢åå¸¦æ¥äºçªç ´ï¼ææå°è§£å³äºé¿æå­å¨çæå½±æ¨¡ç³æ§åé¨åå¯è§æ§é®é¢ï¼ä¸ºæªæ¥è¯¥é¢åçç ç©¶å¼è¾äºæ°çæ¹åã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose MVAT,
a novel framework that leverages temporal multi-view present in sequential data
to address these challenges.</li>
<li>Our approach aggregates object-centric point
clouds across time to build 3D object representations as dense and complete as
possible.</li>
<li>Experiments on the nuScenes and Waymo Open datasets
demonstrate that MVAT achieves state-of-the-art performance for weakly
supervised 3D object detection, significantly narrowing the gap with fully
supervised methods without requiring any 3D box annotations.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.07507v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.07507v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.07435v1'></a></p>
<h2 id="dreamlifting-a-plug-in-module-lifting-mv-diffusion-models-for-3d-asset-generation"><a href="https://arxiv.org/abs/2509.07435v1">DreamLifting: A Plug-in Module Lifting MV Diffusion Models for 3D Asset Generation</a></h2>
<p><strong>Authors:</strong> Ze-Xin Yin, Jiaxiong Qiu, Liu Liu, Xinjie Wang, Wei Sui, Zhizhong Su, Jian Yang, Jin Xie</p>
<p><strong>Published:</strong> 2025-09-09</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>The labor- and experience-intensive creation of 3D assets with physically
based rendering (PBR) materials demands an autonomous 3D asset creation
pipeline. However, most existing 3D generation methods focus on geometry
modeling, either baking textures into simple vertex colors or leaving texture
synthesis to post-processing with image diffusion models. To achieve end-to-end
PBR-ready 3D asset generation, we present Lightweight Gaussian Asset Adapter
(LGAA), a novel framework that unifies the modeling of geometry and PBR
materials by exploiting multi-view (MV) diffusion priors from a novel
perspective. The LGAA features a modular design with three components.
Specifically, the LGAA Wrapper reuses and adapts network layers from MV
diffusion models, which encapsulate knowledge acquired from billions of images,
enabling better convergence in a data-efficient manner. To incorporate multiple
diffusion priors for geometry and PBR synthesis, the LGAA Switcher aligns
multiple LGAA Wrapper layers encapsulating different knowledge. Then, a tamed
variational autoencoder (VAE), termed LGAA Decoder, is designed to predict 2D
Gaussian Splatting (2DGS) with PBR channels. Finally, we introduce a dedicated
post-processing procedure to effectively extract high-quality, relightable mesh
assets from the resulting 2DGS. Extensive quantitative and qualitative
experiments demonstrate the superior performance of LGAA with both text-and
image-conditioned MV diffusion models. Additionally, the modular design enables
flexible incorporation of multiple diffusion priors, and the
knowledge-preserving scheme leads to efficient convergence trained on merely
69k multi-view instances. Our code, pre-trained weights, and the dataset used
will be publicly available via our project page:
https://zx-yin.github.io/dreamlifting/.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Ze-Xin Yinç­äººæ°åçè®ºæâDreamLifting: A Plug-in Module Lifting MV Diffusion Models for 3D Asset Generationâçå¨é¢æè¦ã</p>
<hr />
<h3 id="dreamlifting-3d">è®ºææè¦ï¼DreamLifting: ä¸ç§æåå¤è§è§æ©æ£æ¨¡åç¨äº3Dèµäº§çæçæä»¶æ¨¡å</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
å½å3Dèµäº§çææ¹æ³ä¸»è¦éä¸­å¨å ä½å»ºæ¨¡ï¼éå¸¸å°çº¹çççä¸ºç®åçé¡¶ç¹é¢è²ï¼æå°çº¹çåæçç»å¾åæ©æ£æ¨¡åçåå¤çé¶æ®µãè¿å¯¼è´çæç3Dèµäº§ç¼ºä¹ç©çæ¸²æï¼PBRï¼ææï¼æ æ³å®ç°é¼ççéæååæ¸²æãå æ­¤ï¼è¯¥è®ºææ¨å¨è§£å³å¦ä½å®ç°ç«¯å°ç«¯ãå¯ç¨äºPBRç3Dèµäº§çæï¼ä»¥æ»¡è¶³ç°ä»£å¾å½¢ç®¡çº¿å¯¹é«è´¨éãå¯éæå3Dèµäº§çéæ±ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
è®ºææåºäº<strong>è½»éçº§é«æ¯èµäº§ééå¨ï¼Lightweight Gaussian Asset Adapter, LGAAï¼</strong>ï¼è¿æ¯ä¸ä¸ªæ°é¢çæ¡æ¶ï¼éè¿å©ç¨å¤è§è§ï¼MVï¼æ©æ£åéªï¼ä»¥ä¸ç§æ°é¢çè§è§ç»ä¸äºå ä½åPBRææçå»ºæ¨¡ãLGAAéç¨æ¨¡ååè®¾è®¡ï¼åå«ä¸ä¸ªæ ¸å¿ç»ä»¶ï¼</p>
<ul>
<li><strong>LGAA Wrapperï¼</strong> éç¨å¹¶ééMVæ©æ£æ¨¡åçç½ç»å±ãè¿äºå±å°è£äºä»æ°åäº¿å¾åä¸­è·å¾çç¥è¯ï¼ä»èå®ç°äºæ°æ®é«æçæ´å¥½æ¶æãå®éè¿å»ç»é¢è®­ç»å±å¹¶æ³¨å¥å¯å­¦ä¹ çé¶åå§åå·ç§¯å±æ¥éåºç¥è¯æµï¼æå¤§åå°ä¿çäºé¢è®­ç»åéªã</li>
<li><strong>LGAA Switcherï¼</strong> ä¸ºäºæ´åå ä½åPBRåæçå¤ä¸ªæ©æ£åéªï¼åæ¬MV RGBæ©æ£åéªåMV PBRæææ©æ£åéªï¼ï¼LGAA Switcheréè¿å¯å­¦ä¹ çé¶åå§åå·ç§¯å±ï¼ä»¥å±çº§æ¹å¼å¯¹é½ä¸ååéªãè¿é¿åäºæ©æè®­ç»é¶æ®µçåéªå²çªï¼å¹¶å®ç°äºå¯¹é½çæ¸è¿å¼èªéåºå¢é¿ã</li>
<li><strong>LGAA Decoderï¼</strong> è®¾è®¡äºä¸ä¸ªç»è¿é©¯åçååèªç¼ç å¨ï¼VAEï¼ï¼ç¨äºé¢æµå¸¦æPBRééç2Dé«æ¯æ³¼æºï¼2DGSï¼ãéè¿è§£ç å°æ´é«çç©ºé´åè¾¨çï¼å®è½å¤çææ´å¤çé«æ¯åºåï¼ä»èæè·æ´è¯¦ç»çå ä½åå¤è§ä¿¡æ¯ã</li>
<li><strong>å¾ååºå¯å¾®åå»¶è¿çè²æ¹æ¡ï¼</strong> å¼å¥è¯¥æ¹æ¡ä»¥å°æ¸²æçG-bufferä¿¡æ¯ä¸æç»çRGBå¤è§èç³»èµ·æ¥ï¼ä»èåå°å ä½åå¤è§åæ¶çæåºæçæ¨¡ç³æ§ï¼å¢å¼ºäºPBRææççå®æã</li>
<li><strong>ä¸ç¨åå¤çç¨åºï¼</strong> å¼å¥äºä¸ä¸ªä¸é¨çåå¤çç¨åºï¼å¯ä»¥ææå°ä»çæç2DGSä¸­æåé«è´¨éãå¯éæåçç½æ ¼èµäº§ãè¿åæ¬éè¿TSDFèåæåç½æ ¼ãè¿ç»­éç½æ ¼åä»¥è·å¾æ°´å¯ç½æ ¼ï¼ä»¥åå©ç¨å¯å¾®åæ¸²æå¨åå§ååä¼åPBRçº¹çè´´å¾ã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>åè¶çæ§è½ï¼</strong> å¹¿æ³çå®éåå®æ§å®éªè¡¨æï¼LGAAå¨ææ¬åå¾åæ¡ä»¶ä¸çMVæ©æ£æ¨¡åä¸åè¡¨ç°åºåè¶çæ§è½ãå®è½å¤çæåç¡®çå ä½å½¢ç¶åç²¾ç»çPBRè´´å¾ï¼è¶è¶äºç°ææåè¿çæ¹æ³ï¼å¦3DTopia-XLï¼ã
*   <strong>æ°æ®é«ææ§ï¼</strong> ç¥è¯ä¿çæ¹æ¡ä½¿å¾æ¨¡åå¨ä»6.9ä¸ä¸ªå¤è§è§å®ä¾ä¸è¿è¡è®­ç»å³å¯å®ç°é«ææ¶æï¼èå¶ä»æ¹æ³ï¼å¦3DTopia-XLï¼éè¦25.6ä¸ä¸ª3Då®ä¾ã
*   <strong>æ¨¡åååçµæ´»æ§ï¼</strong> æ¨¡ååè®¾è®¡åè®¸çµæ´»å°æ´åå¤ä¸ªæ©æ£åéªï¼å¹¶è½ä¸æ´å¼ºå¤§çåºç¡æ¨¡åæ ç¼éæï¼ä»èå®ç°å¯æ©å±çæ§è½æ¹è¿ã
*   <strong>ç«¯å°ç«¯PBRèµäº§çæï¼</strong> å®ç°äºç«¯å°ç«¯ãé«è´¨éãå¯ç¨äºPBRç3Dèµäº§çæï¼çæçèµäº§å·æåç¡®çPBRææï¼æ¯æé¼ççéæåã
*   <strong>é«æçç½æ ¼æåï¼</strong> æ´ä¸ªæµç¨ï¼ä»é«æ¯æ³¼æºå°é«è´¨éãUVæ å°ç3Dç½æ ¼ï¼å¨NVIDIA GeForce RTX 4090 GPUä¸ä»éä¸å°30ç§ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>æ°æ®ééå¶ï¼</strong> ç±äºè¯¥æ¹æ³éè¿è®­ç»é¢å¤çééå¨æ¥å©ç¨é¢è®­ç»çMVæ©æ£æ¨¡åè¿è¡3Dèµäº§çæï¼å æ­¤ç¨äºè®­ç»ééå¨çæ°æ®éå¿é¡»ç¬¦åMVæ©æ£æ¨¡åæåè®­ç»æç¨çæ°æ®éççº¦å®ã
*   <strong>åé¨ç»æç¼ºä¹æ­£ååï¼</strong> è¯¥æ¹æ³ä»éè¿åç´ çº§æå¤±è¿è¡çç£ï¼å¯¼è´å®ä¾çåé¨ç»æç¼ºä¹éå½çæ­£ååã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   æ¢ç´¢å°LGAAæ¹æ³ä¸åç3Dçææ¹æ¡ç¸ç»åï¼ä»¥æ¹è¿ç»æå»ºæ¨¡ã</p>
<hr />
<p>æ»èè¨ä¹ï¼è¿ç¯è®ºæéè¿å¼å¥LGAAæ¡æ¶ï¼ä¸ºPBRå°±ç»ªç3Dèµäº§çææä¾äºä¸ç§æ°é¢ä¸é«æçè§£å³æ¹æ¡ãå®å·§å¦å°å©ç¨äºé¢è®­ç»å¤è§è§æ©æ£æ¨¡åä¸­å°è£çä¸°å¯åéªç¥è¯ï¼å¹¶éè¿æ¨¡ååè®¾è®¡ãæ°æ®é«æçè®­ç»ä»¥åå¾ååºå¯å¾®åæ¸²æï¼å®ç°äºé«è´¨éãå¯éæåç3Dèµäº§çç«¯å°ç«¯çæãå°½ç®¡å­å¨ä¸äºå±éæ§ï¼ä½å¶å¨æ§è½åæçä¸çæ¾èæåï¼ä¸ºè®¡ç®æºè§è§åå¾å½¢å­¦é¢åç3Dåå®¹çæå¼è¾äºæ°çéè·¯ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To achieve end-to-end
PBR-ready 3D asset generation, we present Lightweight Gaussian Asset Adapter
(LGAA), a novel framework that unifies the modeling of geometry and PBR
materials by exploiting multi-view (MV) diffusion priors from a novel
perspective.</li>
<li>Finally, we introduce a dedicated
post-processing procedure to effectively extract high-quality, relightable mesh
assets from the resulting 2DGS.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.07435v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.07435v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.07184v1'></a></p>
<h2 id="dimensionally-reduced-open-world-clustering-drowcula"><a href="https://arxiv.org/abs/2509.07184v1">Dimensionally Reduced Open-World Clustering: DROWCULA</a></h2>
<p><strong>Authors:</strong> Erencem Ozbey, Dimitrios I. Diochnos</p>
<p><strong>Published:</strong> 2025-09-08</p>
<p><strong>Categories:</strong> cs.CV, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Working with annotated data is the cornerstone of supervised learning.
Nevertheless, providing labels to instances is a task that requires significant
human effort. Several critical real-world applications make things more
complicated because no matter how many labels may have been identified in a
task of interest, it could be the case that examples corresponding to novel
classes may appear in the future. Not unsurprisingly, prior work in this,
so-called, `open-world' context has focused a lot on semi-supervised
approaches.
  Focusing on image classification, somehow paradoxically, we propose a fully
unsupervised approach to the problem of determining the novel categories in a
particular dataset. Our approach relies on estimating the number of clusters
using Vision Transformers, which utilize attention mechanisms to generate
vector embeddings. Furthermore, we incorporate manifold learning techniques to
refine these embeddings by exploiting the intrinsic geometry of the data,
thereby enhancing the overall image clustering performance. Overall, we
establish new State-of-the-Art results on single-modal clustering and Novel
Class Discovery on CIFAR-10, CIFAR-100, ImageNet-100, and Tiny ImageNet. We do
so, both when the number of clusters is known or unknown ahead of time. The
code is available at: https://github.com/DROWCULA/DROWCULA.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Erencem OzbeyåDimitrios I. Diochnosæ°åçè®ºæâDimensionally Reduced Open-World Clustering: DROWCULAâçå¨é¢æè¦ã</p>
<hr />
<h3 id="dimensionally-reduced-open-world-clustering-drowcula_1">è®ºææè¦ï¼Dimensionally Reduced Open-World Clustering: DROWCULA</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³å¼æ¾ä¸çï¼Open-Worldï¼ç¯å¢ä¸çå¾ååç±»åèç±»é®é¢ï¼ç¹å«æ¯âæ°é¢ç±»å«åç°âï¼Novel Class Discovery, NCDï¼ãå¨å¼æ¾ä¸çä¸­ï¼æ°æ®éä¸­å¯è½åºç°æªç¥çãæªæ è®°çæ°ç±»å«å®ä¾ï¼è¿ä½¿å¾ä¼ ç»ççç£å­¦ä¹ ååçç£å­¦ä¹ æ¹æ³é¢ä¸´ææãç°æNCDæ¹æ³ä¸»è¦ä¾èµåçç£å­¦ä¹ ï¼éè¦å°éæ è®°æ°æ®ä½ä¸ºå¼å¯¼ãè¯¥è®ºæçæ ¸å¿ç ç©¶é®é¢æ¯ï¼å¦ä½å¨å®å¨æ çç£çå¼æ¾ä¸çè®¾å®ä¸ï¼ææå°è¯å«åèç±»æ°æ®éä¸­çæ°é¢ç±»å«ï¼çè³å¨èç±»æ°éæªç¥çæåµä¸ä¹è½å®ç°ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
DROWCULAï¼Dimensionally Reduced Open-World Clusteringï¼æ¹æ³æåºäºä¸ä¸ªå®å¨æ çç£çæ¡æ¶ï¼å¶ä¸»è¦åæ°ç¹åæ¬ï¼
*   <strong>å®å¨æ çç£çNCDï¼</strong> é¦æ¬¡æåºå¨å®å¨æ çç£çå¼æ¾ä¸çè®¾å®ä¸è¿è¡æ°é¢ç±»å«åç°ï¼æ éä»»ä½é¢åæ è®°çæ°æ®ã
*   <strong>Vision Transformers (ViT) åµå¥ï¼</strong> å©ç¨é¢è®­ç»çVision Transformersï¼å¦DINOv2ï¼ä½ä¸ºç¹å¾æåå¨ï¼çæé«è´¨éçå¾ååéåµå¥ï¼ææææå¾åçå¤æç¹å¾ã
*   <strong>æµå½¢å­¦ä¹ éç»´ï¼</strong> ç»åéçº¿æ§éç»´ææ¯ï¼å¦UMAPåt-SNEï¼æ¥å¤çé«ç»´ViTåµå¥ãè¿äºææ¯éè¿å©ç¨æ°æ®çåå¨å ä½ç»ææ¥ç»ååµå¥ï¼åæäºç»´åº¦ç¾é¾ï¼å¹¶æ¾èæé«äºèç±»æ§è½åè®¡ç®æçã
*   <strong>èç±»æ°éä¼°è®¡ï¼</strong> æåºäºä¸ç§å¨èç±»æ°éæªç¥æ¶ä¼°è®¡æä½³èç±»æ°éçæ¹æ³ï¼éè¿ä¼ååé¨èç±»æææ§ææ ï¼å¦Silhouette Scoreï¼å¹¶ç»åè´å¶æ¯ä¼åæ¥å®ç°ã
*   <strong>éæ¬§å éå¾è·ç¦»åº¦éï¼</strong> å¨åæ­¥æ¢ç´¢ä¸­ï¼ç ç©¶äºéæ¬§å éå¾è·ç¦»åº¦éï¼å¦æµå°è·ç¦»ï¼ï¼åç°å¶å¨ä¿çå±é¨ç»ææ¹é¢ä¼äºæ¬§å éå¾è·ç¦»ï¼å°¤å¶æ¯å¨é«ç»´ç©ºé´ä¸­ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶éè¦æ§ï¼</strong>
DROWCULAå¨å¤ä¸ªåºåæ°æ®éï¼CIFAR-10ãCIFAR-100ãImageNet-100åTiny ImageNetï¼ä¸åå¾äºæ¾èçSOTAï¼State-of-the-Artï¼ç»æï¼æ è®ºèç±»æ°éå·²ç¥ææªç¥ï¼
*   <strong>åæ¨¡æèç±»åNCDçSOTAï¼</strong> å¨æææµè¯æ°æ®éä¸ï¼DROWCULAå¨å®å¨æ çç£è®¾å®ä¸ï¼è¶è¶äºç°æçåçç£NCDæ¹æ³åææ°çèç±»ç®æ³ï¼å°¤å¶æ¯å¨æ°é¢ç±»å«åç°çåç¡®æ§æ¹é¢è¡¨ç°åºè²ãä¾å¦ï¼å¨CIFAR-100æ°æ®éä¸ï¼DROWCULAçåç¡®æ§æ¯ORCAçä¸¤åï¼RankStatsç2.6åï¼DTCç4.2åã
*   <strong>éç»´çæææ§ï¼</strong> å®éªè¯æï¼UMAPåt-SNEç­æµå½¢å­¦ä¹ ææ¯æ¾èæåäºèç±»æ§è½ï¼å¹¶ä¸å¨åå­æçä¸ä¹æå·¨å¤§ä¼å¿ã
*   <strong>åé¨æææ§ææ ä¸å¤é¨ææ çç¸å³æ§ï¼</strong> è®ºæå±ç¤ºäºSilhouette ScoreãCalinski-Harabasz IndexåDavies-Bouldin Indexç­åé¨ææ ä¸å¤é¨èç±»åç¡®æ§ï¼ACCï¼ä¹é´çé«åº¦ç¸å³æ§ï¼è¿å¯¹äºå¨æ çç£ç¯å¢ä¸­éæ©æä½³èç±»æ°éè³å³éè¦ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>é¢è®­ç»æ¨¡åä¾èµï¼</strong> å°½ç®¡DROWCULAå¨èç±»é¶æ®µæ¯æ çç£çï¼ä½å¶æ§è½é«åº¦ä¾èµäºé¢è®­ç»çVision Transformersï¼å¦DINOv2ï¼ï¼è¿äºæ¨¡åæ¬èº«å¯è½æ¯å¨çç£æèªçç£æ¹å¼ä¸è®­ç»çï¼è¿å¼å¥äºæ½å¨çâéæ§çç£âã
*   <strong>è®¡ç®ææ¬ï¼</strong> æäºéç»´ææ¯ï¼å¦t-SNEï¼åèç±»æææ§ææ ï¼å¦Silhouette Scoreï¼å¨è®¡ç®ä¸å¯è½è¾ä¸ºæè´µï¼å°¤å¶æ¯å¨å¤§è§æ¨¡æ°æ®éä¸ã
*   <strong>åé¨ææ çå±éæ§ï¼</strong> å°½ç®¡åé¨ææ ä¸å¤é¨ææ é«åº¦ç¸å³ï¼ä½å¨æäºæåµä¸ï¼åé¨ææ çæå¤§å¼/æå°å¼å¯è½ä¸å®å¨å¯¹åºäºæä½³èç±»æ§è½ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>è¿ä¸æ­¥å¼åæ çç£å­¦ä¹ ææ¯ï¼</strong> æ¢ç´¢å¶ä»æ çç£å­¦ä¹ ææ¯ï¼ä»¥è¿ä¸æ­¥å¢å¼ºç¹å¾è¡¨ç¤ºçè´¨éï¼ä»èæé«èç±»æ§è½ã
*   <strong>å©ç¨å·²ç¥æ°æ®éç¹å¾ï¼</strong> ç ç©¶å¦ä½å©ç¨å·²ç¥æ°æ®éçç¹å¾æ¥æåç¹å¾è¡¨ç¤ºçè´¨éï¼å¯è½éè¿æ´åè¿çèªçç£ææ çç£é¢è®­ç»æ¹æ³ã
*   <strong>ç´æ¥å®ç°NCDç®æ³ï¼</strong> å¨DROWCULAçæ¡æ¶ä¸ï¼ç´æ¥å®ç°æ°çNCDç®æ³ï¼ä»¥è¿ä¸æ­¥æ©å±è¯¥é¢åçç ç©¶ã
*   <strong>æ©å±å°å¶ä»æ°æ®æ¨¡æï¼</strong> å°DROWCULAçæ çç£æ¡æ¶åºç¨äºå¶ä»æ°æ®æ¨¡æï¼å¦ææ¬ãé³é¢ç­ï¼ä»¥éªè¯å¶éç¨æ§ã</p>
<hr />
<p>æ»èè¨ä¹ï¼DROWCULAè®ºæä¸ºå¼æ¾ä¸çç¯å¢ä¸çæ çç£å¾åèç±»åæ°é¢ç±»å«åç°æä¾äºä¸ä¸ªå¼ºå¤§ä¸éç¨çæ¡æ¶ãéè¿ç»åVision Transformersçå¼ºå¤§åµå¥è½ååæµå½¢å­¦ä¹ çéç»´ä¼å¿ï¼è¯¥æ¹æ³å¨æ éä»»ä½äººå·¥æ æ³¨çæåµä¸ï¼å¨å¤ä¸ªåºåæ°æ®éä¸åå¾äºæ¾èçSOTAæ§è½ï¼ä¸ºæªæ¥æ çç£åºç¡æ¨¡ååæ°æ®å¯éåè®¡ç®æºè§è§ä»»å¡çåå±å¥ å®äºåºç¡ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Several critical real-world applications make things more
complicated because no matter how many labels may have been identified in a
task of interest, it could be the case that examples corresponding to novel
classes may appear in the future.</li>
<li>Focusing on image classification, somehow paradoxically, we propose a fully
unsupervised approach to the problem of determining the novel categories in a
particular dataset.</li>
<li>Our approach relies on estimating the number of clusters
using Vision Transformers, which utilize attention mechanisms to generate
vector embeddings.</li>
<li>Overall, we
establish new State-of-the-Art results on single-modal clustering and Novel
Class Discovery on CIFAR-10, CIFAR-100, ImageNet-100, and Tiny ImageNet.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.07184v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.07184v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-09-10 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
