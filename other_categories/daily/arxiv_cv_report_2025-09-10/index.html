<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-09-10 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-23
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-09-09/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-09-11/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-09-10">Arxiv Computer Vision Papers - 2025-09-10</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#one-view-many-worlds-single-image-to-3d-object-meets-generative-domain-randomization-for-one-shot-6d-pose-estimation" class="nav-link">One View, Many Worlds: Single-Image to 3D Object Meets Generative Domain Randomization for One-Shot 6D Pose Estimation</a>
                </li>
                <li class="nav-item">
                    <a href="#visual-tableqa-open-domain-benchmark-for-reasoning-over-table-images" class="nav-link">Visual-TableQA: Open-Domain Benchmark for Reasoning over Table Images</a>
                </li>
                <li class="nav-item">
                    <a href="#raygaussx-accelerating-gaussian-based-ray-marching-for-real-time-and-high-quality-novel-view-synthesis" class="nav-link">RayGaussX: Accelerating Gaussian-Based Ray Marching for Real-Time and High-Quality Novel View Synthesis</a>
                </li>
                <li class="nav-item">
                    <a href="#hairgs-hair-strand-reconstruction-based-on-3d-gaussian-splatting" class="nav-link">HairGS: Hair Strand Reconstruction based on 3D Gaussian Splatting</a>
                </li>
                <li class="nav-item">
                    <a href="#seec-segmentation-assisted-multi-entropy-models-for-learned-lossless-image-compression" class="nav-link">SEEC: Segmentation-Assisted Multi-Entropy Models for Learned Lossless Image Compression</a>
                </li>
                <li class="nav-item">
                    <a href="#semantic-watermarking-reinvented-enhancing-robustness-and-generation-quality-with-fourier-integrity" class="nav-link">Semantic Watermarking Reinvented: Enhancing Robustness and Generation Quality with Fourier Integrity</a>
                </li>
                <li class="nav-item">
                    <a href="#universal-few-shot-spatial-control-for-diffusion-models" class="nav-link">Universal Few-Shot Spatial Control for Diffusion Models</a>
                </li>
                <li class="nav-item">
                    <a href="#mvat-multi-view-aware-teacher-for-weakly-supervised-3d-object-detection" class="nav-link">MVAT: Multi-View Aware Teacher for Weakly Supervised 3D Object Detection</a>
                </li>
                <li class="nav-item">
                    <a href="#dreamlifting-a-plug-in-module-lifting-mv-diffusion-models-for-3d-asset-generation" class="nav-link">DreamLifting: A Plug-in Module Lifting MV Diffusion Models for 3D Asset Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#dimensionally-reduced-open-world-clustering-drowcula" class="nav-link">Dimensionally Reduced Open-World Clustering: DROWCULA</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-09-10">Arxiv Computer Vision Papers - 2025-09-10</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>好的，这是一份针对2025年9月9日Arxiv计算机视觉论文的执行摘要，旨在帮助忙碌的研究人员快速了解最新进展：</p>
<hr />
<p><strong>Arxiv 计算机视觉每日报告：2025年9月9日</strong></p>
<p><strong>执行摘要</strong></p>
<p>今天的Arxiv计算机视觉论文集展示了该领域在<strong>3D视觉与生成、多模态推理以及效率与鲁棒性</strong>方面的持续快速发展。特别值得关注的是，<strong>3D重建和新视角合成</strong>技术正通过结合高斯溅射（Gaussian Splatting）和生成模型实现显著的质量和速度提升。同时，<strong>多模态理解</strong>，尤其是图像与表格数据的结合，以及<strong>扩散模型在3D和可控生成</strong>方面的应用，也显示出强大的潜力。</p>
<p><strong>1. 主要主题与趋势：</strong></p>
<ul>
<li><strong>3D视觉与生成：</strong> 显著关注从2D图像生成3D对象、6D姿态估计、3D重建（特别是毛发）以及新视角合成。高斯溅射技术在多个3D任务中被广泛应用，以提高效率和质量。</li>
<li><strong>多模态与推理：</strong> 出现了结合视觉和文本（尤其是表格数据）进行复杂推理的基准和方法。</li>
<li><strong>生成模型与控制：</strong> 扩散模型在实现精细的空间控制和3D资产生成方面持续演进，并探索了如何提升其效率和多视图一致性。</li>
<li><strong>效率与鲁棒性：</strong> 在图像压缩、语义水印和3D检测等领域，研究人员致力于提升模型的效率、鲁棒性和泛化能力。</li>
</ul>
<p><strong>2. 显著或创新论文：</strong></p>
<ul>
<li><strong>"RayGaussX: Accelerating Gaussian-Based Ray Marching for Real-Time and High-Quality Novel View Synthesis" (Blanc et al.)</strong>：该论文通过优化高斯溅射的渲染过程，实现了实时、高质量的新视角合成，是该领域效率提升的重要一步。</li>
<li><strong>"DreamLifting: A Plug-in Module Lifting MV Diffusion Models for 3D Asset Generation" (Yin et al.)</strong>：展示了如何有效地将多视图扩散模型“提升”到3D资产生成，为3D内容创作提供了强大的新工具。其即插即用的特性具有很高的实用价值。</li>
<li><strong>"One View, Many Worlds: Single-Image to 3D Object Meets Generative Domain Randomization for One-Shot 6D Pose Estimation" (Geng et al.)</strong>：结合了单图像到3D生成和生成式域随机化，以解决单次6D姿态估计的挑战，为机器人和AR/VR应用提供了新的思路。</li>
</ul>
<p><strong>3. 新兴研究方向或技术：</strong></p>
<ul>
<li><strong>高斯溅射（Gaussian Splatting）的泛化与优化：</strong> 不仅用于新视角合成，还扩展到毛发重建等更精细的3D结构，并持续进行渲染效率优化。</li>
<li><strong>扩散模型在3D领域的深度融合：</strong> 从2D图像生成3D对象、多视图一致性到直接生成3D资产，扩散模型正成为3D内容生成的核心驱动力。</li>
<li><strong>多模态表格图像理解：</strong> "Visual-TableQA"的出现表明，对复杂视觉信息（如表格图像）进行开放域推理的需求日益增长，这将推动更高级的视觉语言模型发展。</li>
<li><strong>生成式域随机化（Generative Domain Randomization）：</strong> 在数据稀缺或泛化能力要求高的任务中，利用生成模型创建多样化训练数据，以提升模型鲁棒性。</li>
</ul>
<p><strong>4. 建议阅读全文的论文：</strong></p>
<p>对于不同兴趣的研究人员，以下论文值得深入阅读：</p>
<ul>
<li><strong>对于3D视觉和实时渲染研究者：</strong><ul>
<li><strong>"RayGaussX: Accelerating Gaussian-Based Ray Marching for Real-Time and High-Quality Novel View Synthesis" (Blanc et al.)</strong> - 了解高斯溅射的最新优化技术。</li>
<li><strong>"DreamLifting: A Plug-in Module Lifting MV Diffusion Models for 3D Asset Generation" (Yin et al.)</strong> - 探索扩散模型在3D资产生成中的应用。</li>
<li><strong>"HairGS: Hair Strand Reconstruction based on 3D Gaussian Splatting" (Pan et al.)</strong> - 了解高斯溅射在精细结构重建中的创新应用。</li>
</ul>
</li>
<li><strong>对于多模态和推理研究者：</strong><ul>
<li><strong>"Visual-TableQA: Open-Domain Benchmark for Reasoning over Table Images" (Lompo, Haraoui)</strong> - 了解表格图像理解的最新基准和挑战。</li>
</ul>
</li>
<li><strong>对于生成模型和控制研究者：</strong><ul>
<li><strong>"Universal Few-Shot Spatial Control for Diffusion Models" (Nguyen et al.)</strong> - 探索扩散模型在少量样本下实现通用空间控制的方法。</li>
</ul>
</li>
<li><strong>对于鲁棒性和泛化研究者：</strong><ul>
<li><strong>"One View, Many Worlds: Single-Image to 3D Object Meets Generative Domain Randomization for One-Shot 6D Pose Estimation" (Geng et al.)</strong> - 了解生成式域随机化在6D姿态估计中的应用。</li>
</ul>
</li>
</ul>
<hr />
<p>这份摘要旨在提供一个高层次的概览，帮助您快速识别与您研究方向相关的关键进展。</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2509.07978v1">One View, Many Worlds: Single-Image to 3D Object Meets Generative Domain Randomization for One-Shot 6D Pose Estimation</a></li>
<li><a href="#2509.07966v1">Visual-TableQA: Open-Domain Benchmark for Reasoning over Table Images</a></li>
<li><a href="#2509.07782v1">RayGaussX: Accelerating Gaussian-Based Ray Marching for Real-Time and High-Quality Novel View Synthesis</a></li>
<li><a href="#2509.07774v1">HairGS: Hair Strand Reconstruction based on 3D Gaussian Splatting</a></li>
<li><a href="#2509.07704v1">SEEC: Segmentation-Assisted Multi-Entropy Models for Learned Lossless Image Compression</a></li>
<li><a href="#2509.07647v1">Semantic Watermarking Reinvented: Enhancing Robustness and Generation Quality with Fourier Integrity</a></li>
<li><a href="#2509.07530v1">Universal Few-Shot Spatial Control for Diffusion Models</a></li>
<li><a href="#2509.07507v1">MVAT: Multi-View Aware Teacher for Weakly Supervised 3D Object Detection</a></li>
<li><a href="#2509.07435v1">DreamLifting: A Plug-in Module Lifting MV Diffusion Models for 3D Asset Generation</a></li>
<li><a href="#2509.07184v1">Dimensionally Reduced Open-World Clustering: DROWCULA</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2509.07978v1'></a></p>
<h2 id="one-view-many-worlds-single-image-to-3d-object-meets-generative-domain-randomization-for-one-shot-6d-pose-estimation"><a href="https://arxiv.org/abs/2509.07978v1">One View, Many Worlds: Single-Image to 3D Object Meets Generative Domain Randomization for One-Shot 6D Pose Estimation</a></h2>
<p><strong>Authors:</strong> Zheng Geng, Nan Wang, Shaocong Xu, Chongjie Ye, Bohan Li, Zhaoxi Chen, Sida Peng, Hao Zhao</p>
<p><strong>Published:</strong> 2025-09-09</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Estimating the 6D pose of arbitrary unseen objects from a single reference
image is critical for robotics operating in the long-tail of real-world
instances. However, this setting is notoriously challenging: 3D models are
rarely available, single-view reconstructions lack metric scale, and domain
gaps between generated models and real-world images undermine robustness. We
propose OnePoseViaGen, a pipeline that tackles these challenges through two key
components. First, a coarse-to-fine alignment module jointly refines scale and
pose by combining multi-view feature matching with render-and-compare
refinement. Second, a text-guided generative domain randomization strategy
diversifies textures, enabling effective fine-tuning of pose estimators with
synthetic data. Together, these steps allow high-fidelity single-view 3D
generation to support reliable one-shot 6D pose estimation. On challenging
benchmarks (YCBInEOAT, Toyota-Light, LM-O), OnePoseViaGen achieves
state-of-the-art performance far surpassing prior approaches. We further
demonstrate robust dexterous grasping with a real robot hand, validating the
practicality of our method in real-world manipulation. Project page:
https://gzwsama.github.io/OnePoseviaGen.github.io/</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Zheng Geng等人撰写的论文“One View, Many Worlds: Single-Image to 3D Object Meets Generative Domain Randomization for One-Shot 6D Pose Estimation”的全面摘要。</p>
<hr />
<p><strong>论文摘要：One View, Many Worlds: Single-Image to 3D Object Meets Generative Domain Randomization for One-Shot 6D Pose Estimation</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决机器人技术中一个核心且极具挑战性的问题：如何仅凭一张参考图像，对任意未见过的物体进行一次性（one-shot）6D姿态估计。这一任务的难点在于：3D模型通常难以获取；单视图重建缺乏度量尺度信息；以及生成模型与真实世界图像之间存在的域差距会严重影响姿态估计的鲁棒性。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
作者提出了一个名为 <strong>OnePoseViaGen</strong> 的端到端管道，通过两个关键组件来应对上述挑战：</p>
<ul>
<li><strong>生成式单视图3D纹理网格模型：</strong> 论文首先利用改进的Hi3DGen模型，从一张RGB-D锚点图像生成一个高保真、纹理化的3D对象模型。这个模型在对象中心坐标系中是标准化（无度量尺度）的，为后续的姿态估计奠定了基础。</li>
<li><strong>粗到精的度量对齐模块：</strong> 针对生成模型缺乏度量尺度的问题，OnePoseViaGen设计了一个粗到精的对齐策略。它结合了多视图特征匹配（使用SuperGlue）和PnP求解器进行粗略姿态和尺度估计，然后通过渲染-比较（render-and-compare）细化（基于FoundationPose）迭代地优化姿态和度量尺度，从而将标准化模型与真实世界场景中的对象精确对齐。</li>
<li><strong>文本引导的生成式域随机化策略：</strong> 为了弥合生成模型与真实图像之间的域差距，论文引入了一种文本驱动的生成式数据增强方法。该方法利用文本提示（通过VLM生成）和3D生成模型（Trellis）生成结构一致但纹理多样的3D变体。这些变体在随机化的光照、背景和遮挡条件下渲染，形成大规模合成数据集，用于姿态估计器的有效微调，显著提升了鲁棒性和泛化能力。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong>
OnePoseViaGen在多个具有挑战性的基准数据集上（YCBInEOAT, Toyota-Light, LM-O）取得了显著优于现有方法的最新性能。例如，在YCBInEOAT数据集上，其平均ADD分数达到了81.27，远超其他方法在复杂对象上的表现。在LM-O和TOYL数据集上，该方法在BOP基准指标上也显示出一致的改进。此外，论文通过在真实机器人手上进行鲁棒的灵巧抓取实验，验证了该方法在实际机器人操作中的实用性和有效性。这些结果表明，生成式建模可以显著提升一次性6D姿态估计的性能，尤其是在处理未见对象和复杂场景时。</p>
<p><strong>4. 论文中提及的局限性：</strong>
尽管OnePoseViaGen取得了令人鼓舞的成果，但论文也指出其在处理<strong>可变形或关节式对象</strong>时仍面临挑战。在这种情况下，对象形状的变化可能导致6D姿态估计不准确。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
未来的工作将侧重于将<strong>测试时训练（test-time training）</strong>整合到推理管道中，以实现对可变形对象几何形状的持续细化和准确姿态估计。这将充分利用生成模型在6D姿态估计任务中的灵活性和泛化能力。</p>
<hr />
<p>总而言之，OnePoseViaGen通过结合创新的单视图3D模型生成、精细的度量对齐以及文本引导的域随机化策略，为一次性6D姿态估计提供了一个强大且实用的解决方案，显著推动了机器人感知领域的发展。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>On challenging
benchmarks (YCBInEOAT, Toyota-Light, LM-O), OnePoseViaGen achieves
state-of-the-art performance far surpassing prior approaches.</li>
<li>We further
demonstrate robust dexterous grasping with a real robot hand, validating the
practicality of our method in real-world manipulation.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.07978v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.07978v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.07966v1'></a></p>
<h2 id="visual-tableqa-open-domain-benchmark-for-reasoning-over-table-images"><a href="https://arxiv.org/abs/2509.07966v1">Visual-TableQA: Open-Domain Benchmark for Reasoning over Table Images</a></h2>
<p><strong>Authors:</strong> Boammani Aser Lompo, Marc Haraoui</p>
<p><strong>Published:</strong> 2025-09-09</p>
<p><strong>Categories:</strong> cs.CV, cs.CL</p>
<p><strong>Abstract:</strong></p>
<p>Visual reasoning over structured data such as tables is a critical capability
for modern vision-language models (VLMs), yet current benchmarks remain limited
in scale, diversity, or reasoning depth, especially when it comes to rendered
table images. Addressing this gap, we introduce Visual-TableQA, a large-scale,
open-domain multimodal dataset specifically designed to evaluate and enhance
visual reasoning over complex tabular data. Our generation pipeline is modular,
scalable, and fully autonomous, involving multiple reasoning LLMs collaborating
across distinct roles: generation, validation, and inspiration. Visual-TableQA
comprises 2.5k richly structured LaTeX-rendered tables and 6k
reasoning-intensive QA pairs, all produced at a cost of under USD 100. To
promote diversity and creativity, our pipeline performs multi-model
collaborative data generation via cross-model prompting ('inspiration') and
LLM-jury filtering. Stronger models seed layouts and topics that weaker models
elaborate, collectively distilling diverse reasoning patterns and visual
structures into the dataset. Empirical results show that models fine-tuned on
Visual-TableQA generalize robustly to external benchmarks, outperforming
several proprietary models despite the dataset's synthetic nature. The full
pipeline and resources are publicly available at
https://github.com/AI-4-Everyone/Visual-TableQA.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于“Visual-TableQA: Open-Domain Benchmark for Reasoning over Table Images”论文的全面摘要，用中文写成：</p>
<p><strong>论文摘要：Visual-TableQA：基于表格图像推理的开放域基准</strong></p>
<p><strong>1. 主要问题或研究问题</strong>
该论文旨在解决现有视觉语言模型（VLMs）在处理渲染表格图像时，在规模、多样性和推理深度方面存在的局限性。尽管表格图像的视觉推理是现代VLMs的关键能力，但现有基准数据集未能充分挑战模型理解复杂表格结构和进行深度推理的能力。</p>
<p><strong>2. 关键创新或方法论贡献</strong>
*   <strong>引入Visual-TableQA数据集：</strong> 论文提出了一个大规模、开放域的多模态数据集Visual-TableQA，专门用于评估和增强对复杂表格数据的视觉推理能力。该数据集包含2.5k个结构丰富的LaTeX渲染表格和6k个推理密集型问答对。
*   <strong>模块化、可扩展且完全自主的生成管道：</strong> 数据集通过一个创新的生成管道创建，该管道成本低廉（总成本低于100美元），并涉及多个推理型大型语言模型（LLMs）协同工作，扮演生成、验证和启发等不同角色。
*   <strong>多模型协同数据生成（“启发”）和LLM评审过滤：</strong> 为了促进多样性和创造性，该管道采用“跨模型启发”机制，即更强的模型提供布局和主题的“种子”，较弱的模型在此基础上进行细化和扩展，从而将多样化的推理模式和视觉结构提炼到数据集中。LLM评审团用于过滤和验证生成的数据质量。
*   <strong>LaTeX作为中间表示：</strong> 论文利用LLMs生成复杂的LaTeX表格代码，而非直接生成渲染图像，这大大降低了生成成本并提高了复杂性。</p>
<p><strong>3. 主要结果及其意义</strong>
*   <strong>模型泛化能力：</strong> 经验结果表明，在Visual-TableQA上微调的模型能够稳健地泛化到外部基准测试，尽管数据集是合成的，但其性能优于一些专有模型。
*   <strong>有效评估视觉推理：</strong> Visual-TableQA能有效评估VLMs的视觉推理能力，其模型排名与ReachQA等平衡视觉识别和推理的数据集高度相关，但与ChartQA（侧重识别）或MATH-Vision（侧重推理）的关联性较弱，表明其作为综合性视觉推理基准的独特地位。
*   <strong>图像格式的挑战性：</strong> 模型在Visual-TableQA-CIT（文本代码格式）上的表现平均比在Visual-TableQA（图像格式）上高出6.26%，突显了图像格式在视觉推理方面带来的额外挑战。
*   <strong>对推理能力的显著提升：</strong> 在Visual-TableQA上微调的模型在推理任务上的平均增益显著高于在ReachQA上微调的模型，表明Visual-TableQA在知识蒸馏，特别是需要符号解释和多步推理的任务方面更有效。</p>
<p><strong>4. 局限性</strong>
*   <strong>LaTeX表达能力的局限性：</strong> 尽管LaTeX作为中间表示有效，但在处理更复杂或视觉丰富的图像时，其表达能力有限。
*   <strong>数据质量评估：</strong> 尽管ROSCOE等自动指标提供了有用的见解，但它们仍不如人类判断可靠，人类标注者在确保合成数据集质量方面仍扮演关键角色。
*   <strong>模型泛化一致性：</strong> 某些模型（如Qwen2.5-VL-7B-Instruct）未能始终从Visual-TableQA的监督中受益，这表明在泛化方面可能存在局限性。
*   <strong>评估中的模糊性：</strong> 复杂推理任务的评估并非完美无缺，一个问题可能存在多种有效答案，这使得LLM评审团难以完全自信地评估正确性，可能导致评估中的模糊性。</p>
<p><strong>5. 潜在的未来研究方向</strong>
*   <strong>鲁棒的双向图像到文本编码系统：</strong> 开发一个能够更有效地处理复杂视觉内容的双向图像到文本编码系统是一个开放且有前景的研究领域。
*   <strong>针对特定错误类型的合成监督：</strong> 通过合成监督来解决模型在特定错误类型（如数据提取不完整、幻觉等）上的问题。
*   <strong>进一步探索跨模型启发机制：</strong> 持续优化和探索跨模型启发机制，以进一步提高生成数据的多样性和创造性。</p>
<p>总而言之，这篇论文通过引入Visual-TableQA数据集及其创新的生成管道，为评估和提升VLMs在复杂表格图像视觉推理方面的能力提供了重要贡献，并为未来的研究指明了方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Addressing this gap, we introduce Visual-TableQA, a large-scale,
open-domain multimodal dataset specifically designed to evaluate and enhance
visual reasoning over complex tabular data.</li>
<li>Empirical results show that models fine-tuned on
Visual-TableQA generalize robustly to external benchmarks, outperforming
several proprietary models despite the dataset's synthetic nature.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.07966v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.07966v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.07782v1'></a></p>
<h2 id="raygaussx-accelerating-gaussian-based-ray-marching-for-real-time-and-high-quality-novel-view-synthesis"><a href="https://arxiv.org/abs/2509.07782v1">RayGaussX: Accelerating Gaussian-Based Ray Marching for Real-Time and High-Quality Novel View Synthesis</a></h2>
<p><strong>Authors:</strong> Hugo Blanc, Jean-Emmanuel Deschaud, Alexis Paljic</p>
<p><strong>Published:</strong> 2025-09-09</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>RayGauss has achieved state-of-the-art rendering quality for novel-view
synthesis on synthetic and indoor scenes by representing radiance and density
fields with irregularly distributed elliptical basis functions, rendered via
volume ray casting using a Bounding Volume Hierarchy (BVH). However, its
computational cost prevents real-time rendering on real-world scenes. Our
approach, RayGaussX, builds on RayGauss by introducing key contributions that
accelerate both training and inference. Specifically, we incorporate volumetric
rendering acceleration strategies such as empty-space skipping and adaptive
sampling, enhance ray coherence, and introduce scale regularization to reduce
false-positive intersections. Additionally, we propose a new densification
criterion that improves density distribution in distant regions, leading to
enhanced graphical quality on larger scenes. As a result, RayGaussX achieves 5x
to 12x faster training and 50x to 80x higher rendering speeds (FPS) on
real-world datasets while improving visual quality by up to +0.56 dB in PSNR.
Project page with videos and code: https://raygaussx.github.io/.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Hugo Blanc, Jean-Emmanuel Deschaud, Alexis Paljic撰写的论文“RayGaussX: Accelerating Gaussian-Based Ray Marching for Real-Time and High-Quality Novel View Synthesis”的全面摘要。</p>
<hr />
<h3 id="raygaussx">RayGaussX: 加速基于高斯的光线步进以实现实时高质量新视图合成</h3>
<p><strong>1. 论文解决的主要问题或研究问题：</strong>
该论文旨在解决现有RayGauss方法在实时渲染和处理真实世界场景时的计算成本过高问题。RayGauss通过使用不规则分布的椭圆基函数结合边界体积层次结构（BVH）进行体渲染光线投射，在合成和室内场景中实现了最先进的渲染质量。然而，其高计算成本阻碍了在真实世界场景中的实时应用，并且在户外环境中性能略有下降，训练和推理时间仍然显著。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
RayGaussX在RayGauss的基础上，引入了多项关键创新以显著加速训练和推理：
*   <strong>高效光线采样（Efficient Ray Sampling）：</strong> 整合了<strong>空闲空间跳过（empty-space skipping）</strong>和<strong>自适应采样（adaptive sampling）</strong>策略，以减少渲染方程计算所需的样本数量，从而加速渲染。空闲空间跳过利用BVH避免采样完全透明区域，自适应采样则根据透射率和到摄像机的距离动态调整采样步长。
*   <strong>优化光线一致性和内存访问效率：</strong> 通过<strong>高斯空间重排序（spatial reordering of Gaussians）</strong>（使用Z-order曲线）和<strong>光线重排序（ray reordering）</strong>来增强光线一致性，以更好地适应GPU并行计算，提高内存访问效率并减少warp发散。
*   <strong>限制高度各向异性高斯（Limiting Highly Anisotropic Gaussian）：</strong> 引入了<strong>尺度正则化函数（scale regularization function）</strong>（各向同性损失），以最小化假阳性交集。这通过约束高斯轴对齐边界框（AABB）与椭球体本身的体积比来实现，减少了BVH遍历中的不必要计算。
*   <strong>新颖的稠密化准则（Novel Densification Criterion）：</strong> 提出了一种新的稠密化准则，通过引入校正因子来加权3D空间中的梯度，改善了远距离区域的密度分布，从而在更大场景中实现增强的图形质量。</p>
<p><strong>3. 主要结果及其重要性：</strong>
RayGaussX在真实世界数据集上取得了显著的性能提升：
*   <strong>训练速度：</strong> 实现了5到12倍的训练加速。
*   <strong>渲染速度（FPS）：</strong> 实现了50到80倍的渲染速度提升。
*   <strong>视觉质量：</strong> 在PSNR方面，视觉质量提高了高达+0.56 dB。
*   <strong>与现有方法对比：</strong> 在NeRF Synthetic和NSVF Synthetic数据集上，RayGaussX在渲染质量上略优于RayGauss，同时渲染速度快了三倍。在Mip-NeRF360、Tanks&amp;Temples和Deep Blending等真实世界数据集上，RayGaussX在保持或略微提升视觉质量的同时，显著超越了RayGauss和3D Gaussian Splatting等方法，尤其在户外场景中表现更佳。</p>
<p>这些结果表明RayGaussX成功地将RayGauss的渲染质量与实时性能相结合，使其适用于更广泛的真实世界场景。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>硬件要求：</strong> 该方法需要高端GPU（实验中使用NVIDIA RTX 4090），而Gaussian Splatting及其变体可以在移动设备或Web-GL环境中运行。
*   <strong>抗锯齿处理：</strong> 目前的方法未能妥善处理抗锯齿问题，这超出了本论文的范围。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>进一步加速渲染：</strong> 未来的工作可以通过新的优化进一步加速渲染。
*   <strong>抗锯齿处理：</strong> 解决抗锯齿问题，以进一步提升渲染质量。
*   <strong>应用扩展：</strong> RayGaussX的快速训练和高质量渲染使其成为需要高精度应用（如表面重建、逆渲染、SLAM、相机优化和重打光）的强大框架。</p>
<hr />
<p>总而言之，RayGaussX通过引入一系列创新的加速策略，成功地将基于高斯的光线步进方法从计算密集型提升为实时可用的新视图合成解决方案，同时保持了卓越的渲染质量，特别是在处理复杂真实世界场景方面表现出色。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>RayGauss has achieved state-of-the-art rendering quality for novel-view
synthesis on synthetic and indoor scenes by representing radiance and density
fields with irregularly distributed elliptical basis functions, rendered via
volume ray casting using a Bounding Volume Hierarchy (BVH).</li>
<li>Our
approach, RayGaussX, builds on RayGauss by introducing key contributions that
accelerate both training and inference.</li>
<li>Additionally, we propose a new densification
criterion that improves density distribution in distant regions, leading to
enhanced graphical quality on larger scenes.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.07782v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.07782v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.07774v1'></a></p>
<h2 id="hairgs-hair-strand-reconstruction-based-on-3d-gaussian-splatting"><a href="https://arxiv.org/abs/2509.07774v1">HairGS: Hair Strand Reconstruction based on 3D Gaussian Splatting</a></h2>
<p><strong>Authors:</strong> Yimin Pan, Matthias Nießner, Tobias Kirschstein</p>
<p><strong>Published:</strong> 2025-09-09</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Human hair reconstruction is a challenging problem in computer vision, with
growing importance for applications in virtual reality and digital human
modeling. Recent advances in 3D Gaussians Splatting (3DGS) provide efficient
and explicit scene representations that naturally align with the structure of
hair strands. In this work, we extend the 3DGS framework to enable strand-level
hair geometry reconstruction from multi-view images. Our multi-stage pipeline
first reconstructs detailed hair geometry using a differentiable Gaussian
rasterizer, then merges individual Gaussian segments into coherent strands
through a novel merging scheme, and finally refines and grows the strands under
photometric supervision.
  While existing methods typically evaluate reconstruction quality at the
geometric level, they often neglect the connectivity and topology of hair
strands. To address this, we propose a new evaluation metric that serves as a
proxy for assessing topological accuracy in strand reconstruction. Extensive
experiments on both synthetic and real-world datasets demonstrate that our
method robustly handles a wide range of hairstyles and achieves efficient
reconstruction, typically completing within one hour.
  The project page can be found at: https://yimin-pan.github.io/hair-gs/</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Yimin Pan, Matthias Niessner, Tobias Kirschstein撰写的论文“HairGS: Hair Strand Reconstruction based on 3D Gaussian Splatting”的全面摘要。</p>
<hr />
<h3 id="hairgs-3d">HairGS: 基于3D高斯泼溅的头发股重建</h3>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决计算机视觉领域中人发重建的挑战性问题。现有方法在处理复杂发型、频繁遮挡以及头发股的连接性和拓扑结构方面存在局限性。特别是，许多方法侧重于几何精度，而忽略了头发股固有的拓扑结构和连通性，这对于虚拟现实和数字人建模等应用至关重要。</p>
<p><strong>2. 关键创新或方法贡献：</strong>
HairGS提出了一种新颖的多阶段优化流程，将3D高斯泼溅（3DGS）框架扩展到股级头发几何重建，其主要贡献包括：
*   <strong>多阶段优化流程：</strong>
    *   <strong>几何重建（第一阶段）：</strong> 利用可微分高斯光栅化器和自适应稠密化，从多视图图像中重建详细的头发几何结构。通过结合RGB、方向和掩膜损失（包括新引入的双向方向损失和掩膜损失）进行优化，以确保准确的几何和方向。
    *   <strong>股生成（第二阶段）：</strong> 引入了一种新颖的合并方案，基于距离和角度启发式方法，将单个高斯段合并成连贯的头发股。每个头发股被表示为链接关节的链，每个段被建模为圆柱体。
    *   <strong>生长与细化（第三阶段）：</strong> 在光度监督下对头发股进行细化和生长。通过结合光度损失和角度平滑度损失来优化关节位置，以防止形成尖锐角度。通过逐渐放宽合并阈值来促进更长的股形成。
*   <strong>拓扑准确性评估新指标：</strong> 提出了一种名为“股一致性”（Strand Consistency, SC）的新评估指标，用于量化股重建中的拓扑准确性，解决了现有指标忽略头发股连接性和拓扑结构的问题。该指标通过衡量每个真实股中与单个预测股匹配点的最高比例来评估连通性。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>鲁棒性和效率：</strong> 在合成（USC-HairSalon, Cem Yuksel）和真实世界（NeRSemble）数据集上的广泛实验表明，HairGS能够鲁棒地处理各种发型，包括直发、卷发和长发，并能准确捕捉细微细节和浮动股。
*   <strong>性能优势：</strong> 该方法在所有评估指标上均优于现有的基于SfM（如LP-MVS、Strand Integration）和数据驱动（如Neural Haircut）方法，尤其是在具有挑战性的卷发样本上表现出色，并实现了最高的股一致性。
*   <strong>重建速度：</strong> HairGS的重建过程高效，通常在一小时内完成，显著快于许多计算密集型、基于学习的方法（例如，Neural Strands需要48小时，Neural Haircut需要120小时）。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>合并标准：</strong> 当前的合并标准可能无法始终有效地将同一股中的高斯点合并，导致重建的股可能比实际更短。
*   <strong>与头皮的连接：</strong> 重建的股不一定附着在头皮上，这限制了它们在渲染引擎中的直接使用。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>改进合并算法：</strong> 可以通过使用匈牙利算法的变体进行最优匹配来解决合并标准的问题，从而实现更有效的股合并。
*   <strong>头皮连接：</strong> 未来的工作可以通过将股根部固定到表面并像[28, 34]中那样生长股来解决与头皮连接的问题，从而提高其在渲染应用中的实用性。
*   <strong>扩展应用：</strong> 该框架可以自然地扩展到其他线状结构（如电缆或电线）的重建，只需对分割模型进行最小的调整。</p>
<hr />
<p>总而言之，HairGS通过将3D高斯泼溅框架与创新的多阶段优化流程和股级拓扑评估相结合，为多视图图像的人发重建提供了一个高效且鲁棒的解决方案。其提出的股一致性指标填补了现有评估方法在拓扑准确性方面的空白，为该领域的研究提供了新的视角。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Our multi-stage pipeline
first reconstructs detailed hair geometry using a differentiable Gaussian
rasterizer, then merges individual Gaussian segments into coherent strands
through a novel merging scheme, and finally refines and grows the strands under
photometric supervision.</li>
<li>To address this, we propose a new evaluation metric that serves as a
proxy for assessing topological accuracy in strand reconstruction.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.07774v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.07774v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.07704v1'></a></p>
<h2 id="seec-segmentation-assisted-multi-entropy-models-for-learned-lossless-image-compression"><a href="https://arxiv.org/abs/2509.07704v1">SEEC: Segmentation-Assisted Multi-Entropy Models for Learned Lossless Image Compression</a></h2>
<p><strong>Authors:</strong> Chunhang Zheng, Zichang Ren, Dou Li</p>
<p><strong>Published:</strong> 2025-09-09</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Recently, learned image compression has attracted considerable attention due
to its superior performance over traditional methods. However, most existing
approaches employ a single entropy model to estimate the probability
distribution of pixel values across the entire image, which limits their
ability to capture the diverse statistical characteristics of different
semantic regions. To overcome this limitation, we propose Segmentation-Assisted
Multi-Entropy Models for Lossless Image Compression (SEEC). Our framework
utilizes semantic segmentation to guide the selection and adaptation of
multiple entropy models, enabling more accurate probability distribution
estimation for distinct semantic regions. Specifically, SEEC first extracts
image features and then applies semantic segmentation to identify different
regions, each assigned a specialized entropy model to better capture its unique
statistical properties. Finally, a multi-channel discrete logistic mixture
likelihood is employed to model the pixel value distributions effectively.
Experimental results on benchmark datasets demonstrate that SEEC achieves
state-of-the-art compression ratios while introducing only minimal encoding and
decoding latency. With superior performance, the proposed model also supports
Regions of Interest (ROIs) coding condition on the provided segmentation mask.
Our code is available at https://github.com/chunbaobao/SEEC.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Chunhang Zheng, Zichang Ren, Dou Li撰写的论文“SEEC: Segmentation-Assisted Multi-Entropy Models for Learned Lossless Image Compression”的全面摘要。</p>
<hr />
<h3 id="seec-segmentation-assisted-multi-entropy-models-for-learned-lossless-image-compression_1">论文《SEEC: Segmentation-Assisted Multi-Entropy Models for Learned Lossless Image Compression》摘要</h3>
<p><strong>1. 主要问题或研究问题：</strong>
当前基于学习的图像无损压缩方法大多采用单一熵模型来估计整个图像的像素值概率分布。然而，这种单一模型难以有效捕捉图像中不同语义区域（如天空、道路、植被等）之间多样化的统计特性，从而限制了压缩性能的进一步提升。论文旨在解决如何更准确地建模图像中异构语义区域的像素值分布，以实现更高的无损图像压缩比。</p>
<p><strong>2. 关键创新或方法贡献：</strong>
*   <strong>分割辅助多熵模型（SMEM）：</strong> 论文提出了SEEC（Segmentation-Assisted Multi-Entropy Models for Lossless Image Compression）框架，其核心创新在于利用语义分割来指导多个熵模型的选择和自适应。通过将图像划分为不同的语义区域，并为每个区域分配专门的熵模型，SEEC能够更准确地估计像素值分布，从而显著提高压缩性能。
*   <strong>多通道离散逻辑混合似然（Multi-channel Discrete Logistic Mixture Likelihood）：</strong> 为了有效建模自然图像中像素值的复杂分布，SEEC采用了一种多通道离散逻辑混合似然模型。该模型能够预测每个图像通道的混合系数，并为每个通道和每个混合分量分配特定的混合权重，以更好地捕捉不同语义区域的独特统计特性。
*   <strong>区域兴趣（ROIs）编码策略：</strong> 论文提出了一种基于分割掩码的ROIs编码策略。该策略允许对感兴趣区域（前景）进行无损压缩，而对非感兴趣区域（背景）则采用更宽松的保真度进行重建，从而在保持重要区域无损的同时，降低整体比特率和编码/解码时间。
*   <strong>语义感知图像压缩器（SIC）：</strong> SEEC框架包含SIC模块，用于从输入图像中提取语义感知特征，并将其压缩为潜在表示。该模块结合了超先验模型和Swin-Attention机制，以增强特征提取能力。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>最先进的压缩比：</strong> 在DIV2K、CLIC.p、CLIC.m、Kodak、Adobe Portrait和Urban100等基准数据集上的实验结果表明，SEEC在无损图像压缩方面取得了最先进的压缩比。与传统方法（如FLIF）相比，SEEC的比特率降低了5.2%至14.1%。与DLPR等先进学习方法相比，SEEC的比特率最多可降低3.0%。
*   <strong>最小的编码和解码延迟：</strong> 尽管引入了语义分割和多熵模型，SEEC仍能保持最小的编码和解码延迟。与单一熵模型变体相比，多熵模型仅引入了微小的额外时间开销。
*   <strong>语义分割的有效性：</strong> 消融研究证实，分割辅助多熵模型和多通道离散逻辑混合似然都对SEEC的性能提升做出了实质性贡献。使用正确的分割掩码对于性能至关重要，随机或不正确的掩码会导致性能下降。
*   <strong>ROIs编码的效率：</strong> ROIs编码策略通过跳过非ROIs的熵编码阶段，将运行时减少了25%，同时保持了ROIs内的无损重建，展示了其灵活性和效率。</p>
<p><strong>4. 论文中提及的局限性：</strong>
论文中没有明确提及显著的局限性。然而，可以推断的潜在局限性可能包括：
*   <strong>对分割模型性能的依赖：</strong> SEEC的性能在一定程度上依赖于语义分割模型的准确性。如果分割模型产生不准确的掩码，可能会影响熵模型的选择和像素值分布估计的准确性。
*   <strong>计算开销（尽管已优化）：</strong> 尽管论文指出分割模型和掩码存储引入的开销很小（平均0.02 bpp），但对于资源受限的设备或实时应用，额外的分割步骤仍可能是一个考虑因素。
*   <strong>N值（语义类别数量）的选择：</strong> 论文将语义类别数量N设置为2（前景和背景），以平衡模型复杂性和性能。对于更复杂的场景或需要更细粒度压缩的应用，N值的选择可能需要进一步研究和优化。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>更精细的语义分割集成：</strong> 探索更先进或更细粒度的语义分割技术，以识别更多语义类别，并为每个类别分配更专业的熵模型，从而进一步提高压缩性能。
*   <strong>自适应N值选择：</strong> 研究如何根据图像内容或应用需求，自适应地确定语义类别N的数量，而不是固定为2。
*   <strong>动态熵模型选择：</strong> 探索更动态的熵模型选择机制，例如基于图像特征或区域复杂性，自动选择最适合的熵模型，而不是硬性分配。
*   <strong>结合其他先进压缩技术：</strong> 将SEEC框架与生成模型、注意力机制或其他先进的图像压缩技术相结合，以探索进一步的性能提升。
*   <strong>扩展到视频压缩：</strong> 将分割辅助多熵模型的思想扩展到视频压缩领域，以处理视频帧中动态变化的语义内容。</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>To overcome this limitation, we propose Segmentation-Assisted
Multi-Entropy Models for Lossless Image Compression (SEEC).</li>
<li>Experimental results on benchmark datasets demonstrate that SEEC achieves
state-of-the-art compression ratios while introducing only minimal encoding and
decoding latency.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.07704v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.07704v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.07647v1'></a></p>
<h2 id="semantic-watermarking-reinvented-enhancing-robustness-and-generation-quality-with-fourier-integrity"><a href="https://arxiv.org/abs/2509.07647v1">Semantic Watermarking Reinvented: Enhancing Robustness and Generation Quality with Fourier Integrity</a></h2>
<p><strong>Authors:</strong> Sung Ju Lee, Nam Ik Cho</p>
<p><strong>Published:</strong> 2025-09-09</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Semantic watermarking techniques for latent diffusion models (LDMs) are
robust against regeneration attacks, but often suffer from detection
performance degradation due to the loss of frequency integrity. To tackle this
problem, we propose a novel embedding method called Hermitian Symmetric Fourier
Watermarking (SFW), which maintains frequency integrity by enforcing Hermitian
symmetry. Additionally, we introduce a center-aware embedding strategy that
reduces the vulnerability of semantic watermarking due to cropping attacks by
ensuring robust information retention. To validate our approach, we apply these
techniques to existing semantic watermarking schemes, enhancing their
frequency-domain structures for better robustness and retrieval accuracy.
Extensive experiments demonstrate that our methods achieve state-of-the-art
verification and identification performance, surpassing previous approaches
across various attack scenarios. Ablation studies confirm the impact of SFW on
detection capabilities, the effectiveness of the center-aware embedding against
cropping, and how message capacity influences identification accuracy. Notably,
our method achieves the highest detection accuracy while maintaining superior
image fidelity, as evidenced by FID and CLIP scores. Conclusively, our proposed
SFW is shown to be an effective framework for balancing robustness and image
fidelity, addressing the inherent trade-offs in semantic watermarking. Code
available at https://github.com/thomas11809/SFWMark</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Sung Ju Lee和Nam Ik Cho撰写的论文“Semantic Watermarking Reinvented: Enhancing Robustness and Generation Quality with Fourier Integrity”的全面摘要。</p>
<hr />
<h3 id="_1">论文摘要：语义水印的重塑：通过傅里叶完整性增强鲁棒性和生成质量</h3>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决现有针对潜在扩散模型（LDMs）的语义水印技术所面临的核心挑战。尽管这些技术对再生攻击具有鲁棒性，但它们通常由于频率完整性的丧失而导致检测性能下降。此外，这些方法在面对裁剪攻击时也表现出脆弱性。因此，研究问题是如何开发一种语义水印框架，既能保持频率完整性以提高检测性能和生成质量，又能增强对空间攻击（如裁剪）的鲁棒性。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
该论文提出了两项关键创新来解决上述问题：</p>
<ul>
<li><strong>Hermitian对称傅里叶水印（SFW）：</strong> 针对频率完整性丧失的问题，作者提出了一种新颖的嵌入方法SFW。通过强制执行Hermitian对称性，SFW确保水印嵌入在傅里叶域中保持统计一致性，从而在逆傅里叶变换后获得实值信号。这不仅提高了水印的可检索性和生成模型的稳定性，还充分利用了傅里叶域的实部和虚部信息进行检测。</li>
<li><strong>中心感知嵌入策略：</strong> 为了减少语义水印对裁剪攻击的脆弱性，论文引入了一种中心感知嵌入策略。该策略仅对潜在向量空间域的中心区域应用傅里叶变换进行水印嵌入，而不是对整个空间矩阵进行操作。这种方法确保了水印信息在空间上更具弹性区域的保留，从而增强了对裁剪攻击的鲁棒性。</li>
</ul>
<p>论文将这些技术应用于现有语义水印方案（如Tree-Ring和RingID），并提出了两种具体实现：<strong>Hermitian对称Tree-Ring (HSTR)</strong> 和 <strong>Hermitian对称QR码 (HSQR)</strong>，后者将QR码的二进制模式拆分并分别嵌入到傅里叶域自由半区域的实部和虚部中。</p>
<p><strong>3. 主要结果及其重要性：</strong>
通过广泛的实验，论文展示了其方法的卓越性能：</p>
<ul>
<li><strong>最先进的检测性能：</strong> HSTR和HSQR在验证和识别任务中均实现了最先进的检测性能，在各种攻击场景（包括信号处理失真、再生攻击和裁剪攻击）下均超越了现有方法。</li>
<li><strong>频率完整性与生成质量的平衡：</strong> 提出的SFW方法通过保持频率完整性，在水印鲁棒性和生成质量之间取得了更好的平衡。FID和CLIP分数证明，HSTR和HSQR在保持高检测准确性的同时，也保持了卓越的图像保真度，避免了现有方法（如RingID）中出现的可见环状伪影。</li>
<li><strong>对裁剪攻击的鲁棒性：</strong> 中心感知嵌入策略显著提高了对裁剪攻击的鲁棒性，即使在极端裁剪（如0.2的裁剪比例）下，HSTR和HSQR也比RingID等方法表现出更平稳的性能下降。</li>
<li><strong>消息容量的可扩展性：</strong> HSQR在不同水印消息容量下表现出最高的识别准确性，即使在最大容量下也能保持接近完美的准确性，展示了其卓越的可扩展性。</li>
<li><strong>傅里叶域的充分利用：</strong> 论文结果表明，潜在空间水印不受传统低中频约束的限制，通过统计结构化编码，水印信息可以分布在更宽的频率范围内，同时保持鲁棒性。</li>
</ul>
<p><strong>4. 论文中提及的局限性：</strong>
论文中明确提及的局限性主要包括：</p>
<ul>
<li><strong>非恶意篡改的范围：</strong> 论文的方法主要设计用于应对典型的、非恶意的分发或转换过程中的内容变化，而非检测外围篡改或对抗性攻击。后者需要不同的威胁模型和设计考虑。</li>
<li><strong>计算成本（针对基线）：</strong> 某些基线方法，如Zodiac，由于需要多次扩散生成和潜在向量优化迭代，其计算成本过高，不适合实际应用。虽然本文提出的方法通过“生成时合并”方案避免了这一问题，但这是对现有方法的一个观察。</li>
<li><strong>再生攻击强度：</strong> 论文通过消融研究探讨了扩散模型中再生攻击的噪声强度，以确保攻击强度足够，但并未明确指出其方法在面对未来可能出现的更极端或更复杂的对抗性攻击时的表现。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong>
论文提出了以下未来研究方向：</p>
<ul>
<li><strong>自适应嵌入策略：</strong> 探索自适应嵌入策略，以进一步增强对对抗性攻击和极端失真的鲁棒性。</li>
<li><strong>多样化生成架构：</strong> 将所提出的方法扩展到潜在扩散模型之外的更多样化的生成架构。</li>
<li><strong>实时部署：</strong> 进一步优化方法，使其与低功耗、高吞吐量的AI推理加速器（如NPU）兼容，以实现可扩展、节能环境中的无缝部署。</li>
</ul>
<hr />
<p>总而言之，这篇论文通过引入Hermitian对称傅里叶水印和中心感知嵌入策略，成功地重塑了潜在扩散模型中的语义水印技术。它不仅解决了频率完整性丧失和裁剪攻击脆弱性的关键问题，还在鲁棒性、生成质量和可扩展性之间取得了卓越的平衡，为数字内容溯源和版权保护领域提供了重要进展。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To tackle this
problem, we propose a novel embedding method called Hermitian Symmetric Fourier
Watermarking (SFW), which maintains frequency integrity by enforcing Hermitian
symmetry.</li>
<li>Additionally, we introduce a center-aware embedding strategy that
reduces the vulnerability of semantic watermarking due to cropping attacks by
ensuring robust information retention.</li>
<li>To validate our approach, we apply these
techniques to existing semantic watermarking schemes, enhancing their
frequency-domain structures for better robustness and retrieval accuracy.</li>
<li>Extensive experiments demonstrate that our methods achieve state-of-the-art
verification and identification performance, surpassing previous approaches
across various attack scenarios.</li>
<li>Notably,
our method achieves the highest detection accuracy while maintaining superior
image fidelity, as evidenced by FID and CLIP scores.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.07647v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.07647v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.07530v1'></a></p>
<h2 id="universal-few-shot-spatial-control-for-diffusion-models"><a href="https://arxiv.org/abs/2509.07530v1">Universal Few-Shot Spatial Control for Diffusion Models</a></h2>
<p><strong>Authors:</strong> Kiet T. Nguyen, Chanhuyk Lee, Donggyun Kim, Dong Hoon Lee, Seunghoon Hong</p>
<p><strong>Published:</strong> 2025-09-09</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Spatial conditioning in pretrained text-to-image diffusion models has
significantly improved fine-grained control over the structure of generated
images. However, existing control adapters exhibit limited adaptability and
incur high training costs when encountering novel spatial control conditions
that differ substantially from the training tasks. To address this limitation,
we propose Universal Few-Shot Control (UFC), a versatile few-shot control
adapter capable of generalizing to novel spatial conditions. Given a few
image-condition pairs of an unseen task and a query condition, UFC leverages
the analogy between query and support conditions to construct task-specific
control features, instantiated by a matching mechanism and an update on a small
set of task-specific parameters. Experiments on six novel spatial control tasks
show that UFC, fine-tuned with only 30 annotated examples of novel tasks,
achieves fine-grained control consistent with the spatial conditions. Notably,
when fine-tuned with 0.1% of the full training data, UFC achieves competitive
performance with the fully supervised baselines in various control tasks. We
also show that UFC is applicable agnostically to various diffusion backbones
and demonstrate its effectiveness on both UNet and DiT architectures. Code is
available at https://github.com/kietngt00/UFC.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Kiet T. Nguyen等人撰写的论文“Universal Few-Shot Spatial Control for Diffusion Models”的全面摘要。</p>
<hr />
<h3 id="universal-few-shot-spatial-control-for-diffusion-models_1">论文《Universal Few-Shot Spatial Control for Diffusion Models》全面摘要</h3>
<p><strong>1. 论文主要问题或研究问题</strong></p>
<p>该论文旨在解决预训练文本到图像（T2I）扩散模型在空间控制方面的局限性。尽管现有方法（如ControlNet）通过引入控制适配器显著提升了图像生成的精细控制能力，但这些适配器通常需要针对每种新的空间控制任务进行独立训练，这不仅计算成本高昂，而且需要大量的标注数据。当遇到与训练任务显著不同的新颖空间条件时，现有适配器的适应性有限，泛化能力较差。因此，核心研究问题是如何开发一种通用、数据高效的少样本控制适配器，使其能够以最少的标注数据（例如，几十个图像-条件对）泛化到各种新颖的空间条件。</p>
<p><strong>2. 关键创新或方法论贡献</strong></p>
<p>为了解决上述挑战，作者提出了<strong>通用少样本控制（Universal Few-Shot Control, UFC）</strong>框架，其主要创新点包括：</p>
<ul>
<li><strong>通用控制适配器设计：</strong> UFC引入了一个通用的控制适配器，能够将异构空间条件（如边缘图、深度图、姿态等）统一为与图像特征兼容的控制特征。这通过<strong>补丁级匹配机制</strong>实现，该机制利用查询条件和支持集图像-条件对之间的类比关系，构建任务特定的控制特征。支持集图像的视觉特征作为任务无关的基底，而条件之间的补丁级相似性分数则用于计算选择相关特征的权重。</li>
<li><strong>高效的少样本适应机制：</strong> 为了在数据稀缺的情况下快速适应新任务而不发生过拟合，UFC结合了<strong>情景式元学习（episodic meta-learning）</strong>和<strong>参数高效微调（parameter-efficient fine-tuning）</strong>。在元训练阶段，模型学习一个通用的参数集，并在新任务上仅微调一小组任务特定参数（例如，偏置或LoRA参数），从而实现高效的测试时适应。</li>
<li><strong>架构无关性：</strong> UFC的设计具有通用性，可以与不同的扩散模型骨干（如UNet和DiT）以及现有的适配器架构（如ControlNet）兼容，通过在多层注入控制特征来增强模型的空间控制能力。</li>
</ul>
<p><strong>3. 主要结果及其意义</strong></p>
<p>论文通过在六个新颖空间控制任务（Canny、HED、深度、法线、姿态、Densepose）上的广泛实验，验证了UFC的有效性：</p>
<ul>
<li><strong>卓越的少样本性能：</strong> UFC在仅使用30个标注示例进行微调的情况下，实现了与空间条件高度一致的精细控制。在少样本设置下，UFC在可控性方面显著优于所有现有少样本和免训练基线方法。</li>
<li><strong>与全监督基线竞争：</strong> 值得注意的是，当仅使用0.1%的完整训练数据进行微调时，UFC在各种控制任务中达到了与全监督基线（ControlNet和Uni-ControlNet）相当的性能，尤其是在Densepose等密集条件任务上表现出色。这表明UFC在数据效率方面具有显著优势。</li>
<li><strong>对不同骨干的兼容性：</strong> 实验证明UFC能够成功应用于UNet和DiT两种不同的扩散模型骨干，并能利用更强大的DiT骨干实现更精细的空间控制。</li>
<li><strong>对新颖3D结构条件的泛化：</strong> UFC还展示了其在3D网格、线框和点云等更具挑战性的新颖空间条件下的有效性，进一步验证了其强大的泛化能力。</li>
</ul>
<p>这些结果表明，UFC为T2I扩散模型提供了一种通用、数据高效且灵活的少样本空间控制解决方案，极大地提升了空间控制方法的实用性和灵活性。</p>
<p><strong>4. 论文中提及的局限性</strong></p>
<p>论文也坦诚地指出了UFC的几个局限性：</p>
<ul>
<li><strong>主要面向空间控制生成：</strong> UFC框架主要设计用于空间控制生成，而非需要保留条件图像外观的任务，例如风格迁移、图像修复或去模糊等逆问题。</li>
<li><strong>需要少量标注数据进行微调：</strong> 尽管是少样本方法，UFC仍需要为每个新任务提供少量标注数据进行微调。这与大型语言模型通过上下文学习（in-context learning）直接适应新任务的方式不同，后者无需微调。</li>
</ul>
<p><strong>5. 潜在的未来研究方向</strong></p>
<p>基于上述局限性，论文提出了以下未来研究方向：</p>
<ul>
<li><strong>扩展框架以处理外观保留任务：</strong> 将UFC框架扩展到能够处理风格迁移、图像修复等需要保留条件图像外观的任务。</li>
<li><strong>实现无需微调的上下文学习：</strong> 探索开发类似大型语言模型的能力，使空间控制图像生成能够通过上下文学习，仅从少量示例中适应新任务，而无需任何微调。</li>
</ul>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>However, existing control adapters exhibit limited adaptability and
incur high training costs when encountering novel spatial control conditions
that differ substantially from the training tasks.</li>
<li>To address this limitation,
we propose Universal Few-Shot Control (UFC), a versatile few-shot control
adapter capable of generalizing to novel spatial conditions.</li>
<li>Experiments on six novel spatial control tasks
show that UFC, fine-tuned with only 30 annotated examples of novel tasks,
achieves fine-grained control consistent with the spatial conditions.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.07530v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.07530v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.07507v1'></a></p>
<h2 id="mvat-multi-view-aware-teacher-for-weakly-supervised-3d-object-detection"><a href="https://arxiv.org/abs/2509.07507v1">MVAT: Multi-View Aware Teacher for Weakly Supervised 3D Object Detection</a></h2>
<p><strong>Authors:</strong> Saad Lahlali, Alexandre Fournier Montgieux, Nicolas Granger, Hervé Le Borgne, Quoc Cuong Pham</p>
<p><strong>Published:</strong> 2025-09-09</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Annotating 3D data remains a costly bottleneck for 3D object detection,
motivating the development of weakly supervised annotation methods that rely on
more accessible 2D box annotations. However, relying solely on 2D boxes
introduces projection ambiguities since a single 2D box can correspond to
multiple valid 3D poses. Furthermore, partial object visibility under a single
viewpoint setting makes accurate 3D box estimation difficult. We propose MVAT,
a novel framework that leverages temporal multi-view present in sequential data
to address these challenges. Our approach aggregates object-centric point
clouds across time to build 3D object representations as dense and complete as
possible. A Teacher-Student distillation paradigm is employed: The Teacher
network learns from single viewpoints but targets are derived from temporally
aggregated static objects. Then the Teacher generates high quality
pseudo-labels that the Student learns to predict from a single viewpoint for
both static and moving objects. The whole framework incorporates a multi-view
2D projection loss to enforce consistency between predicted 3D boxes and all
available 2D annotations. Experiments on the nuScenes and Waymo Open datasets
demonstrate that MVAT achieves state-of-the-art performance for weakly
supervised 3D object detection, significantly narrowing the gap with fully
supervised methods without requiring any 3D box annotations. % \footnote{Code
available upon acceptance} Our code is available in our public repository
(\href{https://github.com/CEA-LIST/MVAT}{code}).</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Saad Lahlali等人撰写的论文“MVAT: Multi-View Aware Teacher for Weakly Supervised 3D Object Detection”的全面摘要。</p>
<hr />
<p><strong>论文摘要：MVAT: Multi-View Aware Teacher for Weakly Supervised 3D Object Detection</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
3D目标检测在自动驾驶和机器人等领域至关重要，但其主要瓶颈在于获取精确3D边界框标注的高昂成本。虽然弱监督方法通过利用成本较低的2D边界框标注来缓解这一问题，但仅依赖2D框会引入投影模糊性（一个2D框可能对应多个有效3D姿态）和单视角下物体部分可见性导致的3D框估计困难。现有方法通常依赖启发式或先验知识来解决这些模糊性，但忽略了序列数据中自然存在的时间多视角信息。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
MVAT（Multi-View Aware Teacher）提出了一种新颖的弱监督3D目标检测框架，通过利用序列数据中固有的时间多视角信息来解决上述挑战。其核心创新和贡献包括：
*   <strong>时间多视角聚合：</strong> MVAT聚合跨时间的以物体为中心的点云，以构建尽可能密集和完整的3D物体表示，从而解决单视角下的稀疏性和模糊性问题。
*   <strong>Teacher-Student蒸馏范式：</strong> 采用两阶段的Teacher-Student蒸馏框架。
    *   <strong>Teacher网络训练：</strong> Teacher网络从单视角学习，但其目标是从时间聚合的静态物体中推导出来的。这使得Teacher能够学习鲁棒的3D几何。
    *   <strong>伪标签生成与Student网络训练：</strong> Teacher生成高质量的伪标签，Student网络则从单视角输入中学习预测静态和移动物体。这种策略使Student能够有效学习底层3D几何并处理遮挡和移动物体等挑战性情况。
*   <strong>多视角2D投影损失：</strong> 整个框架融入了多视角2D投影损失，以强制预测的3D边界框与所有可用的2D标注之间保持一致性，作为强大的监督信号。
*   <strong>静态/移动物体分离：</strong> 限制聚合到静态实例，因为移动物体在没有地面真值运动信息的情况下难以对齐。通过分析点云质心的时间一致性来识别静态物体。</p>
<p><strong>3. 主要结果及其意义：</strong>
MVAT在nuScenes和Waymo Open数据集上的实验结果表明，它在弱监督3D目标检测方面取得了最先进的性能，显著缩小了与全监督方法之间的差距，且无需任何3D边界框标注。
*   <strong>nuScenes数据集：</strong> MVAT在nuScenes验证集上实现了47.6%的mAP和49.1%的NDS，显著优于先前的领先方法ALPI [5]，mAP提升了+5.8%。这使得弱监督方法达到了全监督Oracle性能的81.0%，证明了在仅使用2D标注的情况下，在弥合差距方面迈出了重要一步。
*   <strong>挑战性类别表现：</strong> 在卡车（+7.2）、巴士（+5.4）、障碍物（+15.8）和交通锥（+8.6）等经常被遮挡或几何稀疏的挑战性物体类别上，MVAT的表现尤为突出，验证了时间聚合的有效性。
*   <strong>Waymo Open数据集：</strong> MVAT是第一个在该数据集上报告弱监督性能指标的方法，在L1难度下，车辆、行人和骑行者的AP分别达到了Oracle的91.3%、89.4%和87.4%，展示了方法的通用性。
*   <strong>半弱监督设置：</strong> 在仅使用2%的3D地面真值框的情况下，MVAT的性能优于使用更强弱标签（3D点标注）的Point-DETR3D，进一步验证了其多视角时间策略的信息量。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>管道复杂性：</strong> MVAT对静态和移动物体的分离引入了管道复杂性，这可能限制其在高速公路等动态环境中的性能。
*   <strong>非刚性类别：</strong> 尽管MVAT在静态和移动物体上表现良好，但对于行人等非刚性类别，其性能可能仍有提升空间。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>显式运动建模：</strong> 结合显式运动建模可以改善移动物体的聚合质量，可能消除静态/移动分离的需要，并改进非刚性类别（如行人）的模型。
*   <strong>辅助任务：</strong> 通过引入辅助任务来预测聚合3D物体视图的空间点分布，以单帧输入为条件，可以丰富Teacher和Student网络的训练目标。这将提供超越3D边界框的额外几何监督，鼓励网络学习更全面的3D形状表示，并可能提高其从稀疏、部分观测中推断完整物体几何形状的能力。</p>
<hr />
<p>这篇论文通过巧妙地利用时间多视角数据，为弱监督3D目标检测领域带来了突破，有效地解决了长期存在的投影模糊性和部分可见性问题，为未来该领域的研究开辟了新的方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose MVAT,
a novel framework that leverages temporal multi-view present in sequential data
to address these challenges.</li>
<li>Our approach aggregates object-centric point
clouds across time to build 3D object representations as dense and complete as
possible.</li>
<li>Experiments on the nuScenes and Waymo Open datasets
demonstrate that MVAT achieves state-of-the-art performance for weakly
supervised 3D object detection, significantly narrowing the gap with fully
supervised methods without requiring any 3D box annotations.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.07507v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.07507v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.07435v1'></a></p>
<h2 id="dreamlifting-a-plug-in-module-lifting-mv-diffusion-models-for-3d-asset-generation"><a href="https://arxiv.org/abs/2509.07435v1">DreamLifting: A Plug-in Module Lifting MV Diffusion Models for 3D Asset Generation</a></h2>
<p><strong>Authors:</strong> Ze-Xin Yin, Jiaxiong Qiu, Liu Liu, Xinjie Wang, Wei Sui, Zhizhong Su, Jian Yang, Jin Xie</p>
<p><strong>Published:</strong> 2025-09-09</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>The labor- and experience-intensive creation of 3D assets with physically
based rendering (PBR) materials demands an autonomous 3D asset creation
pipeline. However, most existing 3D generation methods focus on geometry
modeling, either baking textures into simple vertex colors or leaving texture
synthesis to post-processing with image diffusion models. To achieve end-to-end
PBR-ready 3D asset generation, we present Lightweight Gaussian Asset Adapter
(LGAA), a novel framework that unifies the modeling of geometry and PBR
materials by exploiting multi-view (MV) diffusion priors from a novel
perspective. The LGAA features a modular design with three components.
Specifically, the LGAA Wrapper reuses and adapts network layers from MV
diffusion models, which encapsulate knowledge acquired from billions of images,
enabling better convergence in a data-efficient manner. To incorporate multiple
diffusion priors for geometry and PBR synthesis, the LGAA Switcher aligns
multiple LGAA Wrapper layers encapsulating different knowledge. Then, a tamed
variational autoencoder (VAE), termed LGAA Decoder, is designed to predict 2D
Gaussian Splatting (2DGS) with PBR channels. Finally, we introduce a dedicated
post-processing procedure to effectively extract high-quality, relightable mesh
assets from the resulting 2DGS. Extensive quantitative and qualitative
experiments demonstrate the superior performance of LGAA with both text-and
image-conditioned MV diffusion models. Additionally, the modular design enables
flexible incorporation of multiple diffusion priors, and the
knowledge-preserving scheme leads to efficient convergence trained on merely
69k multi-view instances. Our code, pre-trained weights, and the dataset used
will be publicly available via our project page:
https://zx-yin.github.io/dreamlifting/.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Ze-Xin Yin等人撰写的论文“DreamLifting: A Plug-in Module Lifting MV Diffusion Models for 3D Asset Generation”的全面摘要。</p>
<hr />
<h3 id="dreamlifting-3d">论文摘要：DreamLifting: 一种提升多视角扩散模型用于3D资产生成的插件模块</h3>
<p><strong>1. 主要问题或研究问题：</strong>
当前3D资产生成方法主要集中在几何建模，通常将纹理烘焙为简单的顶点颜色，或将纹理合成留给图像扩散模型的后处理阶段。这导致生成的3D资产缺乏物理渲染（PBR）材料，无法实现逼真的重打光和渲染。因此，该论文旨在解决如何实现端到端、可用于PBR的3D资产生成，以满足现代图形管线对高质量、可重打光3D资产的需求。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
论文提出了<strong>轻量级高斯资产适配器（Lightweight Gaussian Asset Adapter, LGAA）</strong>，这是一个新颖的框架，通过利用多视角（MV）扩散先验，以一种新颖的视角统一了几何和PBR材料的建模。LGAA采用模块化设计，包含三个核心组件：</p>
<ul>
<li><strong>LGAA Wrapper：</strong> 重用并适配MV扩散模型的网络层。这些层封装了从数十亿图像中获得的知识，从而实现了数据高效的更好收敛。它通过冻结预训练层并注入可学习的零初始化卷积层来适应知识流，最大化地保留了预训练先验。</li>
<li><strong>LGAA Switcher：</strong> 为了整合几何和PBR合成的多个扩散先验（包括MV RGB扩散先验和MV PBR材料扩散先验），LGAA Switcher通过可学习的零初始化卷积层，以层级方式对齐不同先验。这避免了早期训练阶段的先验冲突，并实现了对齐的渐进式自适应增长。</li>
<li><strong>LGAA Decoder：</strong> 设计了一个经过驯化的变分自编码器（VAE），用于预测带有PBR通道的2D高斯泼溅（2DGS）。通过解码到更高的空间分辨率，它能够生成更多的高斯基元，从而捕获更详细的几何和外观信息。</li>
<li><strong>图像基可微分延迟着色方案：</strong> 引入该方案以将渲染的G-buffer信息与最终的RGB外观联系起来，从而减少几何和外观同时生成固有的模糊性，增强了PBR材料的真实感。</li>
<li><strong>专用后处理程序：</strong> 引入了一个专门的后处理程序，可以有效地从生成的2DGS中提取高质量、可重打光的网格资产。这包括通过TSDF融合提取网格、连续重网格化以获得水密网格，以及利用可微分渲染器初始化和优化PBR纹理贴图。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>卓越的性能：</strong> 广泛的定量和定性实验表明，LGAA在文本和图像条件下的MV扩散模型上均表现出卓越的性能。它能够生成准确的几何形状和精细的PBR贴图，超越了现有最先进的方法（如3DTopia-XL）。
*   <strong>数据高效性：</strong> 知识保留方案使得模型在仅6.9万个多视角实例上进行训练即可实现高效收敛，而其他方法（如3DTopia-XL）需要25.6万个3D实例。
*   <strong>模块化和灵活性：</strong> 模块化设计允许灵活地整合多个扩散先验，并能与更强大的基础模型无缝集成，从而实现可扩展的性能改进。
*   <strong>端到端PBR资产生成：</strong> 实现了端到端、高质量、可用于PBR的3D资产生成，生成的资产具有准确的PBR材料，支持逼真的重打光。
*   <strong>高效的网格提取：</strong> 整个流程（从高斯泼溅到高质量、UV映射的3D网格）在NVIDIA GeForce RTX 4090 GPU上仅需不到30秒。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>数据集限制：</strong> 由于该方法通过训练额外的适配器来利用预训练的MV扩散模型进行3D资产生成，因此用于训练适配器的数据集必须符合MV扩散模型最初训练所用的数据集的约定。
*   <strong>内部结构缺乏正则化：</strong> 该方法仅通过像素级损失进行监督，导致实例的内部结构缺乏适当的正则化。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   探索将LGAA方法与原生3D生成方案相结合，以改进结构建模。</p>
<hr />
<p>总而言之，这篇论文通过引入LGAA框架，为PBR就绪的3D资产生成提供了一种新颖且高效的解决方案。它巧妙地利用了预训练多视角扩散模型中封装的丰富先验知识，并通过模块化设计、数据高效的训练以及图像基可微分渲染，实现了高质量、可重打光的3D资产的端到端生成。尽管存在一些局限性，但其在性能和效率上的显著提升，为计算机视觉和图形学领域的3D内容生成开辟了新的道路。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To achieve end-to-end
PBR-ready 3D asset generation, we present Lightweight Gaussian Asset Adapter
(LGAA), a novel framework that unifies the modeling of geometry and PBR
materials by exploiting multi-view (MV) diffusion priors from a novel
perspective.</li>
<li>Finally, we introduce a dedicated
post-processing procedure to effectively extract high-quality, relightable mesh
assets from the resulting 2DGS.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.07435v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.07435v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.07184v1'></a></p>
<h2 id="dimensionally-reduced-open-world-clustering-drowcula"><a href="https://arxiv.org/abs/2509.07184v1">Dimensionally Reduced Open-World Clustering: DROWCULA</a></h2>
<p><strong>Authors:</strong> Erencem Ozbey, Dimitrios I. Diochnos</p>
<p><strong>Published:</strong> 2025-09-08</p>
<p><strong>Categories:</strong> cs.CV, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Working with annotated data is the cornerstone of supervised learning.
Nevertheless, providing labels to instances is a task that requires significant
human effort. Several critical real-world applications make things more
complicated because no matter how many labels may have been identified in a
task of interest, it could be the case that examples corresponding to novel
classes may appear in the future. Not unsurprisingly, prior work in this,
so-called, `open-world' context has focused a lot on semi-supervised
approaches.
  Focusing on image classification, somehow paradoxically, we propose a fully
unsupervised approach to the problem of determining the novel categories in a
particular dataset. Our approach relies on estimating the number of clusters
using Vision Transformers, which utilize attention mechanisms to generate
vector embeddings. Furthermore, we incorporate manifold learning techniques to
refine these embeddings by exploiting the intrinsic geometry of the data,
thereby enhancing the overall image clustering performance. Overall, we
establish new State-of-the-Art results on single-modal clustering and Novel
Class Discovery on CIFAR-10, CIFAR-100, ImageNet-100, and Tiny ImageNet. We do
so, both when the number of clusters is known or unknown ahead of time. The
code is available at: https://github.com/DROWCULA/DROWCULA.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Erencem Ozbey和Dimitrios I. Diochnos撰写的论文“Dimensionally Reduced Open-World Clustering: DROWCULA”的全面摘要。</p>
<hr />
<h3 id="dimensionally-reduced-open-world-clustering-drowcula_1">论文摘要：Dimensionally Reduced Open-World Clustering: DROWCULA</h3>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决开放世界（Open-World）环境下的图像分类和聚类问题，特别是“新颖类别发现”（Novel Class Discovery, NCD）。在开放世界中，数据集中可能出现未知的、未标记的新类别实例，这使得传统的监督学习和半监督学习方法面临挑战。现有NCD方法主要依赖半监督学习，需要少量标记数据作为引导。该论文的核心研究问题是：如何在完全无监督的开放世界设定下，有效地识别和聚类数据集中的新颖类别，甚至在聚类数量未知的情况下也能实现。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
DROWCULA（Dimensionally Reduced Open-World Clustering）方法提出了一个完全无监督的框架，其主要创新点包括：
*   <strong>完全无监督的NCD：</strong> 首次提出在完全无监督的开放世界设定下进行新颖类别发现，无需任何预先标记的数据。
*   <strong>Vision Transformers (ViT) 嵌入：</strong> 利用预训练的Vision Transformers（如DINOv2）作为特征提取器，生成高质量的图像向量嵌入，有效捕捉图像的复杂特征。
*   <strong>流形学习降维：</strong> 结合非线性降维技术（如UMAP和t-SNE）来处理高维ViT嵌入。这些技术通过利用数据的内在几何结构来细化嵌入，克服了维度灾难，并显著提高了聚类性能和计算效率。
*   <strong>聚类数量估计：</strong> 提出了一种在聚类数量未知时估计最佳聚类数量的方法，通过优化内部聚类有效性指标（如Silhouette Score）并结合贝叶斯优化来实现。
*   <strong>非欧几里得距离度量：</strong> 在初步探索中，研究了非欧几里得距离度量（如测地距离），发现其在保留局部结构方面优于欧几里得距离，尤其是在高维空间中。</p>
<p><strong>3. 主要结果及其重要性：</strong>
DROWCULA在多个基准数据集（CIFAR-10、CIFAR-100、ImageNet-100和Tiny ImageNet）上取得了显著的SOTA（State-of-the-Art）结果，无论聚类数量已知或未知：
*   <strong>单模态聚类和NCD的SOTA：</strong> 在所有测试数据集上，DROWCULA在完全无监督设定下，超越了现有的半监督NCD方法和最新的聚类算法，尤其是在新颖类别发现的准确性方面表现出色。例如，在CIFAR-100数据集上，DROWCULA的准确性是ORCA的两倍，RankStats的2.6倍，DTC的4.2倍。
*   <strong>降维的有效性：</strong> 实验证明，UMAP和t-SNE等流形学习技术显著提升了聚类性能，并且在内存效率上也有巨大优势。
*   <strong>内部有效性指标与外部指标的相关性：</strong> 论文展示了Silhouette Score、Calinski-Harabasz Index和Davies-Bouldin Index等内部指标与外部聚类准确性（ACC）之间的高度相关性，这对于在无监督环境中选择最佳聚类数量至关重要。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>预训练模型依赖：</strong> 尽管DROWCULA在聚类阶段是无监督的，但其性能高度依赖于预训练的Vision Transformers（如DINOv2），这些模型本身可能是在监督或自监督方式下训练的，这引入了潜在的“隐性监督”。
*   <strong>计算成本：</strong> 某些降维技术（如t-SNE）和聚类有效性指标（如Silhouette Score）在计算上可能较为昂贵，尤其是在大规模数据集上。
*   <strong>内部指标的局限性：</strong> 尽管内部指标与外部指标高度相关，但在某些情况下，内部指标的最大值/最小值可能不完全对应于最佳聚类性能。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>进一步开发无监督学习技术：</strong> 探索其他无监督学习技术，以进一步增强特征表示的质量，从而提高聚类性能。
*   <strong>利用已知数据集特征：</strong> 研究如何利用已知数据集的特征来提升特征表示的质量，可能通过更先进的自监督或无监督预训练方法。
*   <strong>直接实现NCD算法：</strong> 在DROWCULA的框架下，直接实现新的NCD算法，以进一步扩展该领域的研究。
*   <strong>扩展到其他数据模态：</strong> 将DROWCULA的无监督框架应用于其他数据模态，如文本、音频等，以验证其通用性。</p>
<hr />
<p>总而言之，DROWCULA论文为开放世界环境下的无监督图像聚类和新颖类别发现提供了一个强大且通用的框架。通过结合Vision Transformers的强大嵌入能力和流形学习的降维优势，该方法在无需任何人工标注的情况下，在多个基准数据集上取得了显著的SOTA性能，为未来无监督基础模型和数据密集型计算机视觉任务的发展奠定了基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Several critical real-world applications make things more
complicated because no matter how many labels may have been identified in a
task of interest, it could be the case that examples corresponding to novel
classes may appear in the future.</li>
<li>Focusing on image classification, somehow paradoxically, we propose a fully
unsupervised approach to the problem of determining the novel categories in a
particular dataset.</li>
<li>Our approach relies on estimating the number of clusters
using Vision Transformers, which utilize attention mechanisms to generate
vector embeddings.</li>
<li>Overall, we
establish new State-of-the-Art results on single-modal clustering and Novel
Class Discovery on CIFAR-10, CIFAR-100, ImageNet-100, and Tiny ImageNet.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.07184v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.07184v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-09-10 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
