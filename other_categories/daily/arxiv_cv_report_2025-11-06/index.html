<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-11-06 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-05
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-11-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-15
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-11-05/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-11-07/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-11-06">Arxiv Computer Vision Papers - 2025-11-06</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#human-mesh-modeling-for-anny-body" class="nav-link">Human Mesh Modeling for Anny Body</a>
                </li>
                <li class="nav-item">
                    <a href="#oneocc-semantic-occupancy-prediction-for-legged-robots-with-a-single-panoramic-camera" class="nav-link">OneOcc: Semantic Occupancy Prediction for Legged Robots with a Single Panoramic Camera</a>
                </li>
                <li class="nav-item">
                    <a href="#decoupling-augmentation-bias-in-prompt-learning-for-vision-language-models" class="nav-link">Decoupling Augmentation Bias in Prompt Learning for Vision-Language Models</a>
                </li>
                <li class="nav-item">
                    <a href="#diffusion-sdpo-safeguarded-direct-preference-optimization-for-diffusion-models" class="nav-link">Diffusion-SDPO: Safeguarded Direct Preference Optimization for Diffusion Models</a>
                </li>
                <li class="nav-item">
                    <a href="#3d-cal-an-open-source-software-library-for-calibrating-tactile-sensors" class="nav-link">3D Cal: An Open-Source Software Library for Calibrating Tactile Sensors</a>
                </li>
                <li class="nav-item">
                    <a href="#evtslowtv-a-large-and-diverse-dataset-for-event-based-depth-estimation" class="nav-link">EvtSlowTV - A Large and Diverse Dataset for Event-Based Depth Estimation</a>
                </li>
                <li class="nav-item">
                    <a href="#prom3e-probabilistic-masked-multimodal-embedding-model-for-ecology" class="nav-link">ProM3E: Probabilistic Masked MultiModal Embedding Model for Ecology</a>
                </li>
                <li class="nav-item">
                    <a href="#vcode-a-multimodal-coding-benchmark-with-svg-as-symbolic-visual-representation" class="nav-link">VCode: a Multimodal Coding Benchmark with SVG as Symbolic Visual Representation</a>
                </li>
                <li class="nav-item">
                    <a href="#perchead-perceptual-head-model-for-single-image-3d-head-reconstruction-editing" class="nav-link">PercHead: Perceptual Head Model for Single-Image 3D Head Reconstruction &amp; Editing</a>
                </li>
                <li class="nav-item">
                    <a href="#xr-1-towards-versatile-vision-language-action-models-via-learning-unified-vision-motion-representations" class="nav-link">XR-1: Towards Versatile Vision-Language-Action Models via Learning Unified Vision-Motion Representations</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-11-06">Arxiv Computer Vision Papers - 2025-11-06</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>好的，这是一份为忙碌的研究人员准备的 Arxiv 计算机视觉领域最新论文的每日执行摘要，涵盖了2025年11月5日发布的10篇论文。</p>
<hr />
<p><strong>Arxiv 计算机视觉每日执行摘要 (2025年11月5日)</strong></p>
<p><strong>1. 核心主题与趋势概览：</strong></p>
<p>今天的论文集展现了计算机视觉领域持续的多元化发展，主要集中在以下几个关键趋势：</p>
<ul>
<li><strong>多模态与跨领域融合：</strong> 视觉-语言模型 (VLM) 的进步及其在行动、编码和生态学等领域的应用日益突出。</li>
<li><strong>3D 感知与建模：</strong> 从人体网格、语义占据预测到3D头部重建和触觉传感器校准，3D理解和交互能力是重要焦点。</li>
<li><strong>数据效率与鲁棒性：</strong> 针对数据增强偏差、扩散模型安全性和事件相机数据利用的研究，旨在提升模型在真实世界场景中的性能和泛化能力。</li>
<li><strong>具身智能与机器人：</strong> 机器人感知（如腿式机器人占据预测）和通用视觉-语言-动作模型是推动具身智能发展的关键。</li>
</ul>
<p><strong>2. 重点突出与创新论文：</strong></p>
<ul>
<li><strong>"XR-1: Towards Versatile Vision-Language-Action Models via Learning Unified Vision-Motion Representations" (Shichao Fan et al.)</strong>：这篇论文极具潜力，它旨在通过学习统一的视觉-运动表示来构建多功能的视觉-语言-动作模型。这代表了具身智能和通用人工智能方向的一个重要进展，有望弥合感知、语言理解和物理世界交互之间的鸿沟。</li>
<li><strong>"Diffusion-SDPO: Safeguarded Direct Preference Optimization for Diffusion Models" (Minghao Fu et al.)</strong>：在扩散模型日益普及的背景下，这篇论文通过引入“安全防护的直接偏好优化”来解决扩散模型的安全性和对齐问题，对于确保生成内容的负责任和可控性具有重要意义。</li>
<li><strong>"OneOcc: Semantic Occupancy Prediction for Legged Robots with a Single Panoramic Camera" (Hao Shi et al.)</strong>：针对腿式机器人在复杂环境中导航的关键挑战，该论文提出了一种高效的语义占据预测方法，仅使用单个全景相机，这对于资源受限的机器人系统具有很高的实用价值。</li>
</ul>
<p><strong>3. 新兴研究方向与技术：</strong></p>
<ul>
<li><strong>统一的视觉-运动表示学习：</strong> "XR-1" 提出的概念，旨在将视觉、语言和动作整合到一个统一的框架中，预示着未来具身智能和通用机器人学习的范式转变。</li>
<li><strong>扩散模型的安全与对齐：</strong> "Diffusion-SDPO" 强调了在生成模型中融入偏好优化和安全机制的重要性，这将在未来成为生成AI研究的关键组成部分。</li>
<li><strong>事件相机在深度估计中的应用：</strong> "EvtSlowTV" 数据集的发布，表明事件相机在低延迟、高动态范围感知方面的潜力正被进一步挖掘，尤其是在需要精细深度信息的场景中。</li>
<li><strong>符号化视觉表示 (SVG) 在多模态编码中的应用：</strong> "VCode" 利用 SVG 作为符号视觉表示，为多模态编码和代码生成提供了一个新颖的视角，可能开启新的代码智能研究方向。</li>
</ul>
<p><strong>4. 建议阅读全文的论文：</strong></p>
<p>对于希望深入了解并可能应用于自身研究的学者，我强烈建议阅读以下论文：</p>
<ul>
<li><strong>"XR-1: Towards Versatile Vision-Language-Action Models via Learning Unified Vision-Motion Representations" (Shichao Fan et al.)</strong>：如果您对具身智能、通用AI或多模态学习感兴趣，这篇论文提供了前瞻性的研究方向和潜在的突破。</li>
<li><strong>"Diffusion-SDPO: Safeguarded Direct Preference Optimization for Diffusion Models" (Minghao Fu et al.)</strong>：如果您正在研究生成模型、AI安全或模型对齐，这篇论文提供了解决关键挑战的创新方法。</li>
<li><strong>"OneOcc: Semantic Occupancy Prediction for Legged Robots with a Single Panoramic Camera" (Hao Shi et al.)</strong>：对于机器人学、自主导航或高效感知系统研究人员，这篇论文展示了在资源受限环境下实现高性能感知的实用解决方案。</li>
<li><strong>"Decoupling Augmentation Bias in Prompt Learning for Vision-Language Models" (Gahyeon Kim, Sohee Kim, Seokju Lee)</strong>：如果您关注VLM的鲁棒性、泛化能力或提示学习的优化，这篇论文提供了对数据增强偏差的深刻见解和解决方案。</li>
</ul>
<hr />
<p>希望这份摘要能帮助您快速把握今日 Arxiv 计算机视觉领域的最新动态！</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2511.03589v1">Human Mesh Modeling for Anny Body</a></li>
<li><a href="#2511.03571v1">OneOcc: Semantic Occupancy Prediction for Legged Robots with a Single Panoramic Camera</a></li>
<li><a href="#2511.03367v1">Decoupling Augmentation Bias in Prompt Learning for Vision-Language Models</a></li>
<li><a href="#2511.03317v1">Diffusion-SDPO: Safeguarded Direct Preference Optimization for Diffusion Models</a></li>
<li><a href="#2511.03078v1">3D Cal: An Open-Source Software Library for Calibrating Tactile Sensors</a></li>
<li><a href="#2511.02953v1">EvtSlowTV - A Large and Diverse Dataset for Event-Based Depth Estimation</a></li>
<li><a href="#2511.02946v1">ProM3E: Probabilistic Masked MultiModal Embedding Model for Ecology</a></li>
<li><a href="#2511.02778v1">VCode: a Multimodal Coding Benchmark with SVG as Symbolic Visual Representation</a></li>
<li><a href="#2511.02777v1">PercHead: Perceptual Head Model for Single-Image 3D Head Reconstruction &amp; Editing</a></li>
<li><a href="#2511.02776v1">XR-1: Towards Versatile Vision-Language-Action Models via Learning Unified Vision-Motion Representations</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2511.03589v1'></a></p>
<h2 id="human-mesh-modeling-for-anny-body"><a href="https://arxiv.org/abs/2511.03589v1">Human Mesh Modeling for Anny Body</a></h2>
<p><strong>Authors:</strong> Romain Brégier, Guénolé Fiche, Laura Bravo-Sánchez, Thomas Lucas, Matthieu Armando, Philippe Weinzaepfel, Grégory Rogez, Fabien Baradel</p>
<p><strong>Published:</strong> 2025-11-05</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Parametric body models are central to many human-centric tasks, yet existing
models often rely on costly 3D scans and learned shape spaces that are
proprietary and demographically narrow. We introduce Anny, a simple, fully
differentiable, and scan-free human body model grounded in anthropometric
knowledge from the MakeHuman community. Anny defines a continuous,
interpretable shape space, where phenotype parameters (e.g. gender, age,
height, weight) control blendshapes spanning a wide range of human forms --
across ages (from infants to elders), body types, and proportions. Calibrated
using WHO population statistics, it provides realistic and demographically
grounded human shape variation within a single unified model. Thanks to its
openness and semantic control, Anny serves as a versatile foundation for 3D
human modeling -- supporting millimeter-accurate scan fitting, controlled
synthetic data generation, and Human Mesh Recovery (HMR). We further introduce
Anny-One, a collection of 800k photorealistic humans generated with Anny,
showing that despite its simplicity, HMR models trained with Anny can match the
performance of those trained with scan-based body models, while remaining
interpretable and broadly representative. The Anny body model and its code are
released under the Apache 2.0 license, making Anny an accessible foundation for
human-centric 3D modeling.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Romain Brégier等人撰写的论文“Human Mesh Modeling for Anny Body”的全面摘要。</p>
<hr />
<h3 id="anny-body">《Anny Body的人体网格建模》论文摘要</h3>
<p><strong>1. 主要问题或研究问题：</strong>
现有的参数化人体模型（如SMPL系列）在人体中心任务中至关重要，但它们通常依赖于昂贵的3D扫描数据和专有的、人口统计学上狭窄的学习形状空间。这导致模型在表示人类形态多样性（尤其是儿童、老年人和不常见体型）方面存在局限性，且缺乏可解释性和开放性。本研究旨在解决如何创建一个统一、开放、可解释且能捕捉全生命周期人类形态多样性的参数化人体模型的问题。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
*   <strong>Anny模型：</strong> 论文引入了Anny，一个简单、完全可微分且无需扫描的人体模型。它基于MakeHuman社区的人体测量学知识，而非3D扫描数据。Anny定义了一个连续、可解释的形状空间，其中表型参数（如性别、年龄、身高、体重）直接控制混合形状（blendshapes），从而覆盖从婴儿到老年的广泛人类形态、体型和比例。
*   <strong>WHO人口统计学校准：</strong> Anny模型通过WHO人口统计学数据进行校准，确保了其生成的人体形状变化具有现实性和人口统计学基础。
*   <strong>开放性和语义控制：</strong> Anny模型及其代码在Apache 2.0许可下发布，使其成为3D人体建模的开放基础，支持毫米级扫描拟合、受控合成数据生成和人体网格恢复（HMR）。
*   <strong>Anny-One数据集：</strong> 论文还引入了Anny-One，一个包含80万个使用Anny生成的光真实感人类的大规模合成数据集，具有多样化的3D姿态和形状。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>与现有模型的竞争力：</strong> 尽管Anny模型设计简单，且非数据驱动，但在3DPW和EHF等标准基准测试中，使用Anny训练的HMR模型（如HMR2.0和Multi-HMR）能够达到或超越基于扫描的SMPL-X模型的性能。
*   <strong>对多样化体型的建模能力：</strong> 在包含儿童和成人的AGORA数据集上，Anny模型在建模多样化体型方面表现出色，尤其在处理儿童方面优于SMPL-X和SMIL-X。
*   <strong>合成数据训练的有效性：</strong> Anny-One数据集在训练HMR模型时带来了显著的性能提升，特别是在儿童评估方面。结合Anny-One和Anny模型，在AGORA上显著优于现有数据集和人体模型。
*   <strong>对HMR任务的普适性：</strong> Anny模型在所有基准测试中均实现了最先进的性能，证明了其作为HMR任务的紧凑、可解释和几何一致的替代方案的有效性。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>表型参数的刻板印象：</strong> Anny的表型参数基于MakeHuman艺术家对特定人类特征的预设观念，因此可能编码了艺术家的刻板印象，不应期望这些参数忠实地编码任何与身份相关的特征（如性别、年龄或种族）。
*   <strong>拓扑偏见：</strong> 像Anny在内的所有网格模型都假设个体有四肢，这本身就是一种拓扑偏见。
*   <strong>训练分布偏见：</strong> 现有数据驱动模型（包括Anny）的训练分布可能存在偏见，例如大多数模型仅适用于成人。
*   <strong>扫描数据质量限制：</strong> 在3DBodyTex数据集上进行扫描拟合时，头部和手部的扫描质量较低，导致这些区域的数值评估被排除。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>扩展表型参数：</strong> 进一步研究如何改进和扩展表型参数，使其能更准确、更少偏见地编码人类特征，超越艺术家预设的观念。
*   <strong>更广泛的人口统计学代表性：</strong> 探索如何通过整合更多元化的人体测量学知识和数据，进一步提升模型在表示全球人口多样性方面的能力。
*   <strong>与服装和配饰的集成：</strong> Anny模型目前不直接建模服装或其引起的形状变形，未来可以研究如何将这些因素整合到模型中。
*   <strong>动态和交互建模：</strong> 探索Anny在动态场景和人机交互中的应用，例如通过结合运动捕捉数据来模拟更复杂的行为。
*   <strong>减少计算成本：</strong> 进一步优化HMR模型的训练和推理效率，尤其是在处理大规模数据集和复杂场景时。</p>
<hr />
<p>总而言之，这篇论文通过引入Anny模型，为3D人体建模领域提供了一个新颖且重要的方向。它摆脱了对昂贵3D扫描数据的依赖，转而利用开放的人体测量学知识，构建了一个可解释、可微分且能捕捉全生命周期人类形态多样性的模型。Anny及其伴随的Anny-One数据集不仅在HMR任务中展现出强大的性能，而且其开放性和语义控制为未来的研究和应用奠定了坚实的基础，有望推动人类中心3D建模领域的发展。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce Anny, a simple, fully
differentiable, and scan-free human body model grounded in anthropometric
knowledge from the MakeHuman community.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.03589v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.03589v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.03571v1'></a></p>
<h2 id="oneocc-semantic-occupancy-prediction-for-legged-robots-with-a-single-panoramic-camera"><a href="https://arxiv.org/abs/2511.03571v1">OneOcc: Semantic Occupancy Prediction for Legged Robots with a Single Panoramic Camera</a></h2>
<p><strong>Authors:</strong> Hao Shi, Ze Wang, Shangwei Guo, Mengfei Duan, Song Wang, Teng Chen, Kailun Yang, Lin Wang, Kaiwei Wang</p>
<p><strong>Published:</strong> 2025-11-05</p>
<p><strong>Categories:</strong> cs.RO, cs.CV, eess.IV</p>
<p><strong>Abstract:</strong></p>
<p>Robust 3D semantic occupancy is crucial for legged/humanoid robots, yet most
semantic scene completion (SSC) systems target wheeled platforms with
forward-facing sensors. We present OneOcc, a vision-only panoramic SSC
framework designed for gait-introduced body jitter and 360{\deg} continuity.
OneOcc combines: (i) Dual-Projection fusion (DP-ER) to exploit the annular
panorama and its equirectangular unfolding, preserving 360{\deg} continuity and
grid alignment; (ii) Bi-Grid Voxelization (BGV) to reason in Cartesian and
cylindrical-polar spaces, reducing discretization bias and sharpening
free/occupied boundaries; (iii) a lightweight decoder with Hierarchical AMoE-3D
for dynamic multi-scale fusion and better long-range/occlusion reasoning; and
(iv) plug-and-play Gait Displacement Compensation (GDC) learning feature-level
motion correction without extra sensors. We also release two panoramic
occupancy benchmarks: QuadOcc (real quadruped, first-person 360{\deg}) and
Human360Occ (H3O) (CARLA human-ego 360{\deg} with RGB, Depth, semantic
occupancy; standardized within-/cross-city splits). OneOcc sets new
state-of-the-art (SOTA): on QuadOcc it beats strong vision baselines and
popular LiDAR ones; on H3O it gains +3.83 mIoU (within-city) and +8.08
(cross-city). Modules are lightweight, enabling deployable full-surround
perception for legged/humanoid robots. Datasets and code will be publicly
available at https://github.com/MasterHow/OneOcc.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Hao Shi等人撰写的论文“OneOcc: Semantic Occupancy Prediction for Legged Robots with a Single Panoramic Camera”的全面摘要。</p>
<hr />
<p><strong>论文摘要：OneOcc: 针对足式机器人的单全景相机语义占据预测</strong></p>
<p><strong>1. 主要问题或研究问题</strong>
该论文旨在解决足式/人形机器人进行鲁棒3D语义占据预测的关键挑战。现有的语义场景补全（SSC）系统大多针对轮式平台和前向传感器设计，无法很好地适应足式机器人因步态引入的身体抖动、对360度全方位感知需求以及轻量级、单传感器、低延迟解决方案的限制。因此，核心研究问题是如何为足式机器人开发一个仅基于视觉的全景SSC框架，该框架能够有效应对步态引起的运动抖动并保持360度连续性。</p>
<p><strong>2. 关键创新或方法贡献</strong>
OneOcc框架通过以下四个核心创新点解决了上述问题：</p>
<ul>
<li><strong>双投影融合（Dual-Projection fusion, DP-ER）</strong>：该方法同时利用环形全景图及其等距展开图，以保留360度连续性和网格对齐，从而充分利用全景相机的独特特性，解决传统展开图带来的环形失真和接缝伪影问题。</li>
<li><strong>双网格体素化（Bi-Grid Voxelization, BGV）</strong>：OneOcc在笛卡尔和圆柱-极坐标空间中进行推理，有效减少了离散化偏差，并锐化了自由/占据边界。这使得近场接触几何（如落脚点、障碍物）能够利用笛卡尔坐标的精确性，而远场环形上下文则受益于圆柱坐标的效率和方位连贯性。</li>
<li><strong>带有分层AMoE-3D的轻量级解码器</strong>：该解码器采用分层AMoE-3D（Adaptive Mixture-of-Experts 3D）模块，实现动态多尺度融合和更好的长距离/遮挡推理。AMoE-3D通过双路径体素显著性（通道和空间门）和MoE融合，在平坦区域抑制过度平滑，同时增强对运动至关重要的边缘/接触。</li>
<li><strong>即插即用步态位移补偿（Gait Displacement Compensation, GDC）</strong>：GDC模块在特征层面学习运动校正，无需额外传感器。它将步态引起的相位误差路由回2D图像，在提升之前进行补偿，从而避免了量化误差，并稳定了早期训练。</li>
</ul>
<p>此外，论文还发布了两个新的全景占据基准：
*   <strong>QuadOcc</strong>：一个真实的四足机器人第一人称360度数据集，包含10个场景和24K帧，用于评估在步态引入抖动下的性能。
*   <strong>Human360Occ (H3O)</strong>：一个基于CARLA的类人自我360度数据集，包含RGB、深度和语义占据信息，并提供标准化的城内/跨城划分，用于评估泛化能力。</p>
<p><strong>3. 主要结果及其意义</strong>
OneOcc在所提出的基准上取得了新的最先进（SOTA）性能：
*   <strong>在QuadOcc上</strong>：OneOcc达到了20.56 mIoU，超越了最佳LiDAR基线（LMSCNet，18.44 mIoU）和最佳视觉基线（MonoScene，19.19 mIoU），分别提升了+2.12 mIoU和+1.37 mIoU。这表明仅基于视觉的全景SSC方法可以与甚至超越流行的LiDAR堆栈。
*   <strong>在H3O上</strong>：OneOcc在城内（within-city）设置下获得了37.29 mIoU，在跨城（cross-city）设置下获得了32.23 mIoU，分别比最佳视觉基线提升了+3.83 mIoU和+8.08 mIoU。这突显了OneOcc在分布偏移下的强大鲁棒性。
*   <strong>效率</strong>：OneOcc模块轻量化，平均推理延迟为69.93毫秒（约14.30 FPS），在混合精度下可降至52.84毫秒（约18.92 FPS），内存占用适中（峰值1.86GB），使其适用于足式/人形机器人的板载全方位感知部署。</p>
<p>这些结果证明了OneOcc在处理足式机器人特有挑战方面的有效性，并为未来足式机器人的全方位感知提供了可部署的解决方案。</p>
<p><strong>4. 论文中提及的局限性</strong>
论文中提到了OneOcc的一个主要局限性：
*   <strong>依赖精确校准</strong>：OneOcc假设相机校准是准确且漂移有限的。在实际应用中，这可能是一个挑战。</p>
<p><strong>5. 潜在的未来研究方向</strong>
针对上述局限性，论文提出了以下潜在的未来研究方向：
*   <strong>在线外参自校准</strong>：通过在线外参自校准，并结合轻量级里程计先验进行正则化，可以解决对精确校准的依赖问题。
*   <strong>作为世界模型中间层</strong>：将机器人占据预测作为世界模型和视觉-语言-动作模型的中间层，可以用于语言控制的tokens以及跨机器人迁移的占据序列预训练。</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present OneOcc, a vision-only panoramic SSC
framework designed for gait-introduced body jitter and 360{\deg} continuity.</li>
<li>OneOcc sets new
state-of-the-art (SOTA): on QuadOcc it beats strong vision baselines and
popular LiDAR ones; on H3O it gains +3.83 mIoU (within-city) and +8.08
(cross-city).</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.03571v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.03571v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.03367v1'></a></p>
<h2 id="decoupling-augmentation-bias-in-prompt-learning-for-vision-language-models"><a href="https://arxiv.org/abs/2511.03367v1">Decoupling Augmentation Bias in Prompt Learning for Vision-Language Models</a></h2>
<p><strong>Authors:</strong> Gahyeon Kim, Sohee Kim, Seokju Lee</p>
<p><strong>Published:</strong> 2025-11-05</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Recent advances in large-scale vision and language models have led to
significant progress in zero-shot learning tasks. Methods such as CoOp and
CoCoOp have shown that replacing handcrafted prompts with learnable vectors,
known as prompt learning, can result in improved performance. However, these
models often struggle to generalize to entirely unseen categories. While
traditional zero-shot learning techniques benefit from various data
augmentation strategies, prompt learning has primarily focused on text-based
modifications, leaving the potential of image-based augmentation largely
unexplored. In this work, we explore how image-level augmentations,
particularly those that introduce attribute-specific variations, can support
and enhance prompt learning. Our analysis examines the interaction between
these augmentations and soft prompt frameworks, revealing their potential to
improve generalization. We also identify a limitation in existing methods, such
as CoCoOp, which do not provide explicit guidance for learning prompts that
focus on semantically meaningful visual features. To address this, we propose
Adding Attributes to Prompt Learning, AAPL, a novel method that introduces
adversarial token embeddings to decouple superficial visual variations
introduced by augmentation from class-relevant semantic representations. This
decoupling enables the learned prompts to concentrate on visually
discriminative features that align with the target categories. We conduct
comprehensive experiments on eleven benchmark datasets, and AAPL consistently
outperforms existing methods across few-shot, zero-shot, cross-dataset, and
domain generalization settings. Our source code is publicly available at:
https://github.com/Gahyeonkim09/AAPL</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Gahyeon Kim, Sohee Kim, Seokju Lee撰写的论文“Decoupling Augmentation Bias in Prompt Learning for Vision-Language Models”的全面摘要。</p>
<hr />
<h3 id="_1">论文摘要：解耦视觉语言模型提示学习中的增强偏差</h3>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决当前视觉语言模型（VLMs）在零样本学习任务中，通过提示学习（如CoOp和CoCoOp）虽然取得了显著进展，但仍面临的泛化能力不足问题，尤其是在面对完全未见类别时。现有方法主要关注基于文本的提示修改，而图像层面的数据增强潜力，特别是引入属性特定变化的增强，尚未得到充分探索。具体来说，作者指出CoCoOp等现有方法未能提供明确指导，以学习专注于语义上有意义的视觉特征的提示，导致模型可能将表面视觉变化（由增强引入）与类别相关的语义表示混淆。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
为了解决上述问题，作者提出了<strong>Adding Attributes to Prompt Learning (AAPL)</strong>，一种新颖的框架，其核心创新点包括：
*   <strong>属性特定条件可学习提示（Attribute-specific conditional learnable prompts）：</strong> AAPL系统地将图像增强作为视觉提示整合到提示空间中，通过编码由受控图像扰动产生的属性特定变化来增强提示学习。
*   <strong>对抗性令牌嵌入机制（Adversarial token embedding mechanism）：</strong> 引入了“delta meta token”作为专用表示，用于捕获属性引起的变异。通过对抗性令牌嵌入，AAPL能够将低级增强特征与高级语义内容解耦，使模型专注于有意义的属性，同时抑制对不相关视觉噪声的过拟合。
*   <strong>AdTriplet损失（AdTriplet loss）：</strong> 提出了一种对抗性三元组损失，以进一步强制增强视图之间条件偏差的语义一致性。这种损失通过将提示的条件偏差与跨视图的一致类别语义对齐，从而调节提示的条件偏差。
*   <strong>增强分析（Augmentation Profiling）：</strong> 论文通过详细的增强分析，验证了这些组件如何促进语义一致性，同时抑制属性级别转换带来的噪声。</p>
<p><strong>3. 主要结果及其意义：</strong>
AAPL在11个基准数据集上进行了全面的实验，并取得了以下显著成果：
*   <strong>一致优于现有方法：</strong> AAPL在零样本、少样本、跨数据集和域泛化设置下，始终优于CoOp、CoCoOp等现有方法，在大多数数据集上取得了竞争性甚至更好的性能。
*   <strong>更好的泛化能力：</strong> 通过解耦增强引入的表面视觉变化与类别相关的语义表示，AAPL使学习到的提示能够专注于与目标类别对齐的视觉判别特征，从而显著提高了模型在属性丰富、新颖组合和未见类别分布上的泛化能力。
*   <strong>语义一致性：</strong> AdTriplet损失和delta meta token的引入，使得模型能够更好地捕获属性级别信息，并保持语义一致性，尤其是在处理细粒度视觉特征时。
*   <strong>计算开销可控：</strong> 尽管AAPL的训练时间比CoCoOp长约1.25倍，但其推理速度几乎相同（1.01倍），表明性能提升是在可接受的计算成本下实现的。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>对骨干网络能力的依赖：</strong> AAPL的强大性能在很大程度上依赖于骨干网络编码细粒度语义的能力。在抽象或视觉噪声较多的场景中，其效果会降低。
*   <strong>在特定数据集上的表现：</strong> 在以宽泛纹理或布局级别结构为主的数据集（如DTD和EuroSAT）上，AAPL的性能有所下降，这表明它在捕获全局线索方面存在困难。
*   <strong>对增强选择的敏感性：</strong> AAPL的有效性受增强选择的影响；精心选择的增强可以提高泛化能力，而信息量较少的增强则会限制收益。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>扩展到其他提示范式：</strong> 未来工作可以包括将AAPL扩展到软提示调整之外的其他提示范式。
*   <strong>更复杂的转换：</strong> 将AAPL应用于更复杂的图像转换。
*   <strong>更广泛的评估：</strong> 在更广泛的视觉语言任务上评估AAPL的性能。
*   <strong>解决局限性：</strong> 解决处理以全局纹理或场景布局为主的数据集以及减少对增强选择的依赖性等剩余挑战。</p>
<hr />
<p>总而言之，这篇论文通过引入属性特定条件可学习提示、对抗性令牌嵌入和AdTriplet损失，成功地解决了视觉语言模型在提示学习中泛化能力不足的问题，特别是在解耦增强偏差方面取得了显著进展。AAPL为未来在零样本、少样本和域泛化场景下的视觉语言模型研究提供了新的视角和强大的基线。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this, we propose
Adding Attributes to Prompt Learning, AAPL, a novel method that introduces
adversarial token embeddings to decouple superficial visual variations
introduced by augmentation from class-relevant semantic representations.</li>
<li>We conduct
comprehensive experiments on eleven benchmark datasets, and AAPL consistently
outperforms existing methods across few-shot, zero-shot, cross-dataset, and
domain generalization settings.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.03367v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.03367v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.03317v1'></a></p>
<h2 id="diffusion-sdpo-safeguarded-direct-preference-optimization-for-diffusion-models"><a href="https://arxiv.org/abs/2511.03317v1">Diffusion-SDPO: Safeguarded Direct Preference Optimization for Diffusion Models</a></h2>
<p><strong>Authors:</strong> Minghao Fu, Guo-Hua Wang, Tianyu Cui, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang</p>
<p><strong>Published:</strong> 2025-11-05</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Text-to-image diffusion models deliver high-quality images, yet aligning them
with human preferences remains challenging. We revisit diffusion-based Direct
Preference Optimization (DPO) for these models and identify a critical
pathology: enlarging the preference margin does not necessarily improve
generation quality. In particular, the standard Diffusion-DPO objective can
increase the reconstruction error of both winner and loser branches.
Consequently, degradation of the less-preferred outputs can become sufficiently
severe that the preferred branch is also adversely affected even as the margin
grows. To address this, we introduce Diffusion-SDPO, a safeguarded update rule
that preserves the winner by adaptively scaling the loser gradient according to
its alignment with the winner gradient. A first-order analysis yields a
closed-form scaling coefficient that guarantees the error of the preferred
output is non-increasing at each optimization step. Our method is simple,
model-agnostic, broadly compatible with existing DPO-style alignment frameworks
and adds only marginal computational overhead. Across standard text-to-image
benchmarks, Diffusion-SDPO delivers consistent gains over preference-learning
baselines on automated preference, aesthetic, and prompt alignment metrics.
Code is publicly available at https://github.com/AIDC-AI/Diffusion-SDPO.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供论文“Diffusion-SDPO: Safeguarded Direct Preference Optimization for Diffusion Models”的全面摘要。</p>
<hr />
<p><strong>论文摘要：Diffusion-SDPO: Safeguarded Direct Preference Optimization for Diffusion Models</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
文本到图像扩散模型在生成高质量图像方面取得了显著成功，但将其与人类偏好对齐仍然是一个关键挑战。现有的基于扩散模型的直接偏好优化（DPO）方法存在一个核心问题：简单地扩大偏好差距（即“赢家”和“输家”输出之间的差距）并不一定能提高生成质量。具体而言，标准的Diffusion-DPO目标可能同时增加赢家和输家分支的重建误差，导致“输家”输出的退化严重到即使偏好差距扩大，也会对“赢家”输出产生不利影响，从而在相对对齐和绝对质量控制之间产生矛盾。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
为了解决上述问题，作者提出了Diffusion-SDPO，一种为扩散模型设计的受保护的直接偏好优化方法。其核心创新在于引入了一个简单而有效的“赢家”保留更新规则，通过自适应地缩放“输家”梯度来控制其在每个训练步骤中的影响，具体取决于其与“赢家”梯度的对齐程度。</p>
<ul>
<li><strong>自适应缩放系数：</strong> 通过一阶分析，Diffusion-SDSDO推导出了一个闭式缩放系数 <script type="math/tex">\lambda_{safe}</script>。这个系数是基于“赢家”和“输家”梯度内积的几何对齐，它保证了在每个优化步骤中，“赢家”输出的重建误差不会增加。</li>
<li><strong>“赢家”保留：</strong> 该方法通过在输出空间中调整“输家”梯度，确保了在扩大偏好差距的同时，严格控制了“赢家”输出的绝对误差，从而避免了现有DPO方法中可能出现的质量下降。</li>
<li><strong>模型无关性和低开销：</strong> Diffusion-SDPO是一种模型无关的方法，可以广泛兼容现有的DPO风格对齐框架，并且只增加了微不足道的计算开销。它作为一个插件式优化器，能够稳定训练过程。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong>
Diffusion-SDPO在标准文本到图像基准测试（如SD 1.5、SDXL和工业级Ovis-U1模型）上取得了显著且一致的性能提升。</p>
<ul>
<li><strong>一致性增益：</strong> 在自动化偏好、美学和提示对齐指标上，Diffusion-SDPO始终优于现有的偏好学习基线。</li>
<li><strong>稳定训练和避免崩溃：</strong> 实验结果表明，该方法能够稳定训练过程，避免模型崩溃，并保持或增强美学质量。</li>
<li><strong>跨模型泛化：</strong> 无论是在文本到图像模型、统一生成器还是图像编辑设置中，该方法都能带来收益，显示出其强大的泛化能力。</li>
</ul>
<p><strong>4. 论文中提及的局限性：</strong>
论文中提到了Diffusion-SDPO的保证仅在一阶（线性）近似的损失景观下成立。在实际中，如果损失景观的曲率很强，或者梯度步长不是无穷小，那么“赢家”的重建损失仍有可能略微增加。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
未来的研究工作可能包括：</p>
<ul>
<li><strong>二阶或信任区域保护：</strong> 引入更复杂的机制，如二阶或信任区域保护，以更好地处理损失景观的强曲率问题。</li>
<li><strong>μ的自动或数据驱动调整：</strong> 探索自动或数据驱动的方式来调整安全松弛参数μ，以进一步优化性能。</li>
<li><strong>扩展到自回归偏好设置：</strong> 将Diffusion-SDPO方法扩展到自回归偏好设置，以适应更广泛的生成模型任务。</li>
</ul>
<hr />
<p>总而言之，这篇论文通过引入Diffusion-SDPO，成功解决了现有DPO方法在对齐扩散模型时可能导致生成质量下降的关键问题。其核心贡献在于通过自适应地缩放“输家”梯度，在保证“赢家”输出质量不下降的前提下，有效地扩大了偏好差距。这一创新不仅提高了模型性能，还稳定了训练过程，为未来基于偏好优化的生成模型研究开辟了新方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this, we introduce Diffusion-SDPO, a safeguarded update rule
that preserves the winner by adaptively scaling the loser gradient according to
its alignment with the winner gradient.</li>
<li>Our method is simple,
model-agnostic, broadly compatible with existing DPO-style alignment frameworks
and adds only marginal computational overhead.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.03317v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.03317v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.03078v1'></a></p>
<h2 id="3d-cal-an-open-source-software-library-for-calibrating-tactile-sensors"><a href="https://arxiv.org/abs/2511.03078v1">3D Cal: An Open-Source Software Library for Calibrating Tactile Sensors</a></h2>
<p><strong>Authors:</strong> Rohan Kota, Kaival Shah, J. Edward Colgate, Gregory Reardon</p>
<p><strong>Published:</strong> 2025-11-04</p>
<p><strong>Categories:</strong> cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>Tactile sensing plays a key role in enabling dexterous and reliable robotic
manipulation, but realizing this capability requires substantial calibration to
convert raw sensor readings into physically meaningful quantities. Despite its
near-universal necessity, the calibration process remains ad hoc and
labor-intensive. Here, we introduce \libname{}, an open-source library that
transforms a low-cost 3D printer into an automated probing device capable of
generating large volumes of labeled training data for tactile sensor
calibration. We demonstrate the utility of \libname{} by calibrating two
commercially available vision-based tactile sensors, DIGIT and GelSight Mini,
to reconstruct high-quality depth maps using the collected data and a custom
convolutional neural network. In addition, we perform a data ablation study to
determine how much data is needed for accurate calibration, providing practical
guidelines for researchers working with these specific sensors, and we
benchmark the trained models on previously unseen objects to evaluate
calibration accuracy and generalization performance. By automating tactile
sensor calibration, \libname{} can accelerate tactile sensing research,
simplify sensor deployment, and promote the practical integration of tactile
sensing in robotic platforms.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将对这篇论文摘要进行分析。</p>
<hr />
<h3 id="3d-cal-an-open-source-software-library-for-calibrating-tactile-sensors_1">论文摘要分析：3D Cal: An Open-Source Software Library for Calibrating Tactile Sensors</h3>
<p><strong>1. 论文主要贡献的简洁总结 (Concise Summary of Main Contribution)</strong></p>
<p>这篇论文引入了 \libname{}，一个开源软件库，它能将低成本的3D打印机转换为自动化探测设备，用于生成大量带标签的触觉传感器校准训练数据。通过自动化校准过程，\libname{} 旨在加速触觉传感研究，简化传感器部署，并促进触觉传感在机器人平台中的实际集成。</p>
<p><strong>2. 关键创新或方法学方法 (Key Innovation or Methodological Approach)</strong></p>
<p>核心创新在于<strong>利用低成本的3D打印机作为自动化探测设备</strong>来生成大规模的触觉传感器校准数据。这种方法将一个普遍存在的、劳动密集型的“临时”校准过程，转化为一个自动化、可重复且高效的数据生成流程。它结合了硬件（3D打印机）的现有能力与软件（\libname{} 库）的智能控制，以系统地收集触觉传感器的原始读数及其对应的物理量（例如，深度图的真实值）。此外，论文还利用定制的卷积神经网络（CNN）来处理这些数据，以实现高质量的深度图重建，并进行了数据消融研究以优化数据量需求。</p>
<p><strong>3. 对领域潜在影响 (Potential Impact on the Field)</strong></p>
<ul>
<li><strong>降低触觉传感器的使用门槛：</strong> 自动化校准过程将大大减少研究人员和工程师在部署触觉传感器时所需的时间和精力，从而加速触觉传感技术的普及和应用。</li>
<li><strong>提高研究效率和可重复性：</strong> 提供标准化的、自动化的校准方法，可以确保不同实验室和研究项目之间的数据质量和校准结果的一致性，促进研究成果的比较和验证。</li>
<li><strong>推动触觉传感在机器人领域的集成：</strong> 简化校准意味着触觉传感器更容易被集成到实际的机器人系统中，从而增强机器人的灵巧性、抓取能力和对环境的感知能力。</li>
<li><strong>促进数据驱动的触觉感知发展：</strong> 能够生成大规模、高质量的带标签数据，将为开发更先进的机器学习模型（如深度学习）提供坚实的基础，从而提升触觉传感器的性能。</li>
</ul>
<p><strong>4. 相关领域或应用受益 (Related Areas or Applications that Might Benefit)</strong></p>
<ul>
<li><strong>机器人操作与抓取 (Robotic Manipulation and Grasping)：</strong> 机器人需要精确的触觉反馈来实现灵巧的抓取、组装和操作任务。</li>
<li><strong>医疗机器人 (Medical Robotics)：</strong> 在手术、康复等领域，触觉传感器可以提供关键的力反馈和接触信息。</li>
<li><strong>人机交互 (Human-Robot Interaction)：</strong> 触觉反馈可以使机器人与人类的交互更加自然和安全。</li>
<li><strong>远程呈现与虚拟现实 (Telepresence and Virtual Reality)：</strong> 触觉反馈设备需要精确校准以提供逼真的触觉体验。</li>
<li><strong>材料科学与质量控制 (Materials Science and Quality Control)：</strong> 触觉传感器可用于检测材料表面缺陷、纹理分析等。</li>
<li><strong>计算机视觉 (Computer Vision)：</strong> 尽管是触觉传感器，但其输出（如深度图）与计算机视觉的3D重建、表面分析等任务紧密相关。高质量的触觉深度图可以作为视觉系统的补充或替代，尤其是在光照不足或遮挡的环境中。</li>
</ul>
<p><strong>5. 从摘要中可推断的局限性 (Limitations Inferred from the Abstract)</strong></p>
<ul>
<li><strong>传感器类型限制：</strong> 摘要中明确提到了“视觉基触觉传感器，DIGIT和GelSight Mini”。这可能意味着该库主要针对这类传感器进行了优化，对于其他类型的触觉传感器（如电阻式、电容式、压电式等）的适用性或效果可能需要进一步验证。</li>
<li><strong>3D打印机依赖性：</strong> 尽管3D打印机成本低廉，但其精度、稳定性以及与特定触觉传感器的物理集成方式可能仍需用户进行一定的定制和调整。</li>
<li><strong>数据量与泛化能力：</strong> 尽管进行了数据消融研究，但“多少数据量”的结论可能仅限于摘要中提到的特定传感器和任务。对于新的传感器或更复杂的环境，仍可能需要大量数据。</li>
<li><strong>环境因素：</strong> 摘要未提及环境因素（如温度、湿度、光照变化）对校准准确性的影响，这些因素在实际部署中可能很重要。</li>
<li><strong>计算资源需求：</strong> 使用定制的卷积神经网络进行深度图重建，可能对计算资源有一定的要求，尤其是在实时应用中。</li>
<li><strong>开源库的成熟度：</strong> 作为开源库，其社区支持、文档完善程度、易用性以及未来维护情况，都将影响其长期影响力。</li>
</ul>
<hr />
<p>总而言之，这篇论文通过自动化触觉传感器校准，解决了该领域的一个长期痛点，为触觉传感技术在机器人和相关应用中的普及和发展铺平了道路。其方法学上的创新性在于巧妙地利用了现有低成本硬件，并结合了数据驱动的机器学习方法，具有显著的实用价值和广阔的应用前景。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Here, we introduce \libname{}, an open-source library that
transforms a low-cost 3D printer into an automated probing device capable of
generating large volumes of labeled training data for tactile sensor
calibration.</li>
<li>We demonstrate the utility of \libname{} by calibrating two
commercially available vision-based tactile sensors, DIGIT and GelSight Mini,
to reconstruct high-quality depth maps using the collected data and a custom
convolutional neural network.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.03078v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.03078v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.02953v1'></a></p>
<h2 id="evtslowtv-a-large-and-diverse-dataset-for-event-based-depth-estimation"><a href="https://arxiv.org/abs/2511.02953v1">EvtSlowTV - A Large and Diverse Dataset for Event-Based Depth Estimation</a></h2>
<p><strong>Authors:</strong> Sadiq Layi Macaulay, Nimet Kaygusuz, Simon Hadfield</p>
<p><strong>Published:</strong> 2025-11-04</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.LG, cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>Event cameras, with their high dynamic range (HDR) and low latency, offer a
promising alternative for robust depth estimation in challenging environments.
However, many event-based depth estimation approaches are constrained by
small-scale annotated datasets, limiting their generalizability to real-world
scenarios. To bridge this gap, we introduce EvtSlowTV, a large-scale event
camera dataset curated from publicly available YouTube footage, which contains
more than 13B events across various environmental conditions and motions,
including seasonal hiking, flying, scenic driving, and underwater exploration.
EvtSlowTV is an order of magnitude larger than existing event datasets,
providing an unconstrained, naturalistic setting for event-based depth
learning. This work shows the suitability of EvtSlowTV for a self-supervised
learning framework to capitalise on the HDR potential of raw event streams. We
further demonstrate that training with EvtSlowTV enhances the model's ability
to generalise to complex scenes and motions. Our approach removes the need for
frame-based annotations and preserves the asynchronous nature of event data.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Sadiq Layi Macaulay, Nimet Kaygusuz, Simon Hadfield撰写的论文“EvtSlowTV - A Large and Diverse Dataset for Event-Based Depth Estimation”的全面摘要。</p>
<hr />
<h3 id="evtslowtv-">EvtSlowTV - 基于事件的深度估计的大规模多样化数据集</h3>
<p><strong>摘要</strong></p>
<p>这篇论文介绍了EvtSlowTV，一个大规模、多样化的事件相机数据集，旨在解决现有事件数据集规模小、缺乏多样性，从而限制了基于事件的深度估计模型在真实世界场景中的泛化能力的问题。</p>
<p><strong>1. 主要问题或研究问题</strong>
事件相机因其高动态范围（HDR）和低延迟特性，在挑战性环境下进行鲁棒深度估计方面具有巨大潜力。然而，现有的基于事件的深度估计方法往往受限于小规模、带标注的数据集，这严重阻碍了它们在真实世界场景中的泛化能力。因此，核心问题是如何构建一个足够大规模且多样化的事件相机数据集，以支持更通用、更鲁棒的基于事件的深度学习模型，特别是自监督深度估计。</p>
<p><strong>2. 关键创新或方法论贡献</strong>
该论文的主要贡献包括：
*   <strong>大规模多样化数据集EvtSlowTV的引入：</strong> EvtSlowTV是一个从公开YouTube视频中整理出来的大规模事件相机数据集，包含超过130亿个事件。它涵盖了多种环境条件和运动模式，如季节性徒步、飞行、风景驾驶和水下探索。该数据集比现有事件数据集大一个数量级，为基于事件的深度学习提供了一个无约束、自然主义的设置。
*   <strong>自监督深度学习框架：</strong> 论文提出了一个自监督学习框架，该框架消除了对RGB、LiDAR或立体传感器等外部传感器的依赖，并保留了事件数据的异步特性。这使得模型能够直接从时空事件表示中学习深度图。
*   <strong>自适应帧采样和事件生成：</strong> EvtSlowTV数据集通过自适应帧采样策略从真实世界视频序列生成高保真事件流。这种方法根据亮度变化和场景运动调整采样率，确保了事件的稀疏性和时间精度，同时模拟了真实事件相机的异步特性。
*   <strong>基于对比度最大化的损失函数：</strong> 为了训练深度和姿态估计，论文采用了一种对比度最大化（CM）损失函数，该函数强制事件进行时间对齐，以确保准确的运动补偿。通过最大化累积事件中的对比度，可以提高运动估计的准确性。
*   <strong>教师-学生训练策略：</strong> 论文采用了一种教师-学生学习框架，将知识从一个更鲁棒、更稳定的监督教师模型转移到学生模型，同时在没有真值监督的数据上进行微调。这有助于学生模型适应事件数据的稀疏性和高时间分辨率特性，并提高在多样化运动场景中的泛化能力。</p>
<p><strong>3. 主要结果及其意义</strong>
*   <strong>显著的泛化能力提升：</strong> 论文展示了使用EvtSlowTV进行训练可以增强模型对复杂场景和运动的泛化能力。这表明数据集的规模和多样性对于训练鲁棒的深度估计模型至关重要。
*   <strong>优于现有方法：</strong> 在MVSEC室内飞行序列上的实验结果显示，所提出的方法在绝对平均误差（Abs mean）和对数均方根误差（rms_log）等深度估计指标上优于现有的基线方法，尤其是在绝对误差估计方面表现出色。
*   <strong>保留事件数据的异步性：</strong> 该方法消除了对基于帧的标注的需求，并保留了事件数据的异步特性，这对于低延迟应用至关重要。
*   <strong>自监督学习的有效性：</strong> 论文验证了EvtSlowTV适用于自监督学习框架，能够利用原始事件流的HDR潜力。</p>
<p><strong>4. 论文中提及的局限性</strong>
*   <strong>比例深度估计的挑战：</strong> 尽管该方法在估计绝对误差方面表现出色，但在保持比例深度估计方面仍存在挑战。这表明，如果教师模型仅暴露于有限的数据变异性，教师-学生训练策略可能不可靠。
*   <strong>对教师模型预训练的依赖：</strong> 教师-学生训练策略的有效性部分依赖于一个高质量的、预训练的监督深度模型（EvtSL），这在一定程度上引入了对监督数据的间接依赖。</p>
<p><strong>5. 潜在的未来研究方向</strong>
*   <strong>解决比例深度估计问题：</strong> 进一步研究如何改进自监督模型在保持比例深度估计方面的能力，可能通过引入新的损失函数或训练策略。
*   <strong>探索更广泛的自监督方法：</strong> 尽管论文提出了一个有效的自监督框架，但仍有空间探索其他自监督范式，以进一步提高性能和泛化能力。
*   <strong>结合其他传感器数据：</strong> 虽然论文强调了纯事件驱动的方法，但未来研究可以探索如何以一种保留异步性的方式，将事件数据与其他传感器（如IMU）进行更深层次的融合，以进一步提高鲁棒性。
*   <strong>实时部署和效率优化：</strong> 进一步优化模型和数据集生成流程，以实现更高效的实时深度估计，使其更适用于机器人和自动驾驶等实际应用。</p>
<hr />
<p>总而言之，EvtSlowTV数据集的引入及其配套的自监督学习框架，为基于事件的深度估计领域带来了显著的进步。它通过提供一个前所未有的大规模、多样化数据集，解决了现有方法在泛化能力上的关键限制，并为未来在挑战性环境中实现鲁棒、低延迟深度感知铺平了道路。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To bridge this gap, we introduce EvtSlowTV, a large-scale event
camera dataset curated from publicly available YouTube footage, which contains
more than 13B events across various environmental conditions and motions,
including seasonal hiking, flying, scenic driving, and underwater exploration.</li>
<li>Our approach removes the need for
frame-based annotations and preserves the asynchronous nature of event data.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.02953v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.02953v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.02946v1'></a></p>
<h2 id="prom3e-probabilistic-masked-multimodal-embedding-model-for-ecology"><a href="https://arxiv.org/abs/2511.02946v1">ProM3E: Probabilistic Masked MultiModal Embedding Model for Ecology</a></h2>
<p><strong>Authors:</strong> Srikumar Sastry, Subash Khanal, Aayush Dhakal, Jiayu Lin, Dan Cher, Phoenix Jarosz, Nathan Jacobs</p>
<p><strong>Published:</strong> 2025-11-04</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We introduce ProM3E, a probabilistic masked multimodal embedding model for
any-to-any generation of multimodal representations for ecology. ProM3E is
based on masked modality reconstruction in the embedding space, learning to
infer missing modalities given a few context modalities. By design, our model
supports modality inversion in the embedding space. The probabilistic nature of
our model allows us to analyse the feasibility of fusing various modalities for
given downstream tasks, essentially learning what to fuse. Using these features
of our model, we propose a novel cross-modal retrieval approach that mixes
inter-modal and intra-modal similarities to achieve superior performance across
all retrieval tasks. We further leverage the hidden representation from our
model to perform linear probing tasks and demonstrate the superior
representation learning capability of our model. All our code, datasets and
model will be released at https://vishu26.github.io/prom3e.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将对这篇论文摘要进行深入分析。</p>
<hr />
<h3 id="prom3e-probabilistic-masked-multimodal-embedding-model-for-ecology_1">论文摘要分析：ProM3E: Probabilistic Masked MultiModal Embedding Model for Ecology</h3>
<p><strong>1. 论文主要贡献的简洁总结 (Concise Summary of Main Contribution):</strong></p>
<p>ProM3E 引入了一个概率掩码多模态嵌入模型，旨在为生态学领域实现多模态表示的任意到任意生成。该模型通过在嵌入空间中进行掩码模态重建，学习在给定少量上下文模态的情况下推断缺失模态，并支持嵌入空间中的模态反演。其概率特性使其能够分析不同模态融合的可行性，从而学习“融合什么”，并在此基础上提出了一种结合模态间和模态内相似性的新型跨模态检索方法，以及展示了其卓越的表示学习能力。</p>
<p><strong>2. 关键创新或方法学方法 (Key Innovation or Methodological Approach):</strong></p>
<p>核心创新在于 <strong>“概率掩码多模态嵌入模型 (Probabilistic Masked MultiModal Embedding Model)”</strong> 和 <strong>“在嵌入空间中的掩码模态重建 (masked modality reconstruction in the embedding space)”</strong>。具体来说：</p>
<ul>
<li><strong>掩码模态重建 (Masked Modality Reconstruction):</strong> 类似于BERT等模型的掩码语言建模，ProM3E在嵌入空间中随机掩盖部分模态，然后训练模型去预测或重建这些被掩盖的模态。这使得模型能够学习模态之间的深层关联和互补信息。</li>
<li><strong>概率特性 (Probabilistic Nature):</strong> 这是其区别于许多确定性多模态模型的重要一点。通过引入概率性，模型不仅能生成表示，还能量化不同模态融合的“可行性”或“有用性”，从而智能地决定“融合什么 (what to fuse)”以优化下游任务。这对于处理模态冗余、噪声或信息不对称的情况非常有价值。</li>
<li><strong>嵌入空间中的模态反演 (Modality Inversion in the Embedding Space):</strong> 这意味着模型不仅能从原始模态生成嵌入，还能从嵌入反向生成或推断出原始模态的信息，这对于理解模型内部表示和生成任务至关重要。</li>
<li><strong>新型跨模态检索 (Novel Cross-Modal Retrieval Approach):</strong> 结合了模态间 (inter-modal) 和模态内 (intra-modal) 相似性，这通常能提供更鲁棒和全面的检索结果，尤其是在多模态数据存在复杂关系时。</li>
</ul>
<p><strong>3. 对领域潜在影响 (Potential Impact on the Field):</strong></p>
<ul>
<li><strong>推动多模态学习范式：</strong> ProM3E的概率特性和“学习融合什么”的能力，有望超越传统的多模态融合方法，为更智能、自适应的多模态系统提供新思路。</li>
<li><strong>赋能生态学研究：</strong> 专门针对生态学领域，这意味着该模型可以处理和整合来自传感器数据（如温度、湿度）、图像（如物种识别）、声音（如动物叫声）、文本（如物种描述）等多种异构数据，极大地提升生态学数据分析和理解的效率和深度。例如，通过少量图像和声音数据推断出缺失的物种行为模式。</li>
<li><strong>提升跨模态检索和生成能力：</strong> 其提出的新型检索方法和模态反演能力，将直接提升多模态信息检索的准确性和多模态内容生成的灵活性。</li>
<li><strong>更强大的表示学习：</strong> 通过线性探测任务验证的卓越表示学习能力，表明ProM3E可以生成高质量的通用多模态特征，这些特征可以迁移到各种下游任务中，减少对大量标注数据的依赖。</li>
</ul>
<p><strong>4. 相关领域或应用 (Related Areas or Applications):</strong></p>
<ul>
<li><strong>环境监测与保护：</strong> 整合卫星图像、地面传感器数据、生物多样性数据等，进行生态系统健康评估、物种迁徙预测、灾害预警等。</li>
<li><strong>智慧农业：</strong> 结合土壤数据、作物图像、天气信息等，优化作物生长、病虫害检测和产量预测。</li>
<li><strong>医疗健康：</strong> 融合医学影像（MRI, CT）、电子病历、基因组数据等，进行疾病诊断、预后分析和个性化治疗。</li>
<li><strong>机器人与具身智能：</strong> 机器人需要处理视觉、听觉、触觉等多种模态信息来理解环境和执行任务。</li>
<li><strong>多媒体内容理解与生成：</strong> 提升图像-文本、视频-文本等跨模态内容的检索、摘要和生成质量。</li>
<li><strong>遥感图像分析：</strong> 结合多光谱、高光谱、SAR等不同传感器数据，进行地物分类、变化检测等。</li>
</ul>
<p><strong>5. 从摘要中可推断的局限性 (Limitations Inferable from the Abstract):</strong></p>
<ul>
<li><strong>计算复杂性：</strong> 掩码模态重建和概率建模通常需要大量的计算资源，尤其是在处理高维多模态数据时。</li>
<li><strong>数据需求：</strong> 尽管模型旨在处理缺失模态，但训练一个强大的多模态模型仍然需要大量的多模态配对数据，尤其是在生态学这种数据获取可能困难的领域。</li>
<li><strong>“学习融合什么”的解释性：</strong> 模型的概率特性如何具体量化“融合可行性”以及其决策过程的解释性如何，摘要中未详细说明。这可能是一个黑箱过程，难以完全理解其内部逻辑。</li>
<li><strong>泛化能力：</strong> 虽然声称是“任意到任意生成”，但其在生态学领域的成功是否能直接泛化到其他领域（如医疗、金融）仍需验证，因为不同领域的数据特性和模态关联可能大相径庭。</li>
<li><strong>模态数量限制：</strong> 摘要中未明确指出模型能处理的模态数量上限。随着模态数量的增加，模型复杂性和训练难度可能会显著上升。</li>
<li><strong>下游任务的依赖性：</strong> 模型的性能可能在一定程度上依赖于下游任务的定义和评估指标。</li>
</ul>
<hr />
<p>总而言之，ProM3E 提出了一种新颖且具有前瞻性的多模态学习方法，其概率特性和在嵌入空间中进行掩码重建的能力，有望在生态学乃至更广泛的多模态领域带来显著的进步。其对“融合什么”的智能决策能力，是当前多模态研究中一个非常有趣且重要的方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce ProM3E, a probabilistic masked multimodal embedding model for
any-to-any generation of multimodal representations for ecology.</li>
<li>Using these features
of our model, we propose a novel cross-modal retrieval approach that mixes
inter-modal and intra-modal similarities to achieve superior performance across
all retrieval tasks.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.02946v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.02946v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.02778v1'></a></p>
<h2 id="vcode-a-multimodal-coding-benchmark-with-svg-as-symbolic-visual-representation"><a href="https://arxiv.org/abs/2511.02778v1">VCode: a Multimodal Coding Benchmark with SVG as Symbolic Visual Representation</a></h2>
<p><strong>Authors:</strong> Kevin Qinghong Lin, Yuhao Zheng, Hangyu Ran, Dantong Zhu, Dongxing Mao, Linjie Li, Philip Torr, Alex Jinpeng Wang</p>
<p><strong>Published:</strong> 2025-11-04</p>
<p><strong>Categories:</strong> cs.CV, cs.CL</p>
<p><strong>Abstract:</strong></p>
<p>Code has emerged as a precise and executable medium for reasoning and action
in the agent era. Yet, progress has largely focused on language-centric tasks
such as program synthesis and debugging, leaving visual-centric coding
underexplored. Inspired by how humans reason over sketches, we advocate SVG
code as a compact, interpretable, and executable visual representation. We
introduce VCode, a benchmark that reframes multimodal understanding as code
generation: given an image, a model must produce SVG that preserves symbolic
meaning for downstream reasoning. VCode covers three domains - general
commonsense (MM-Vet), professional disciplines (MMMU), and visual-centric
perception (CV-Bench). To assess symbolic fidelity, we propose CodeVQA, a novel
evaluation protocol in which a policy model answers questions over rendered
SVGs; correct answers indicate faithful symbolic preservation. Empirically,
frontier VLMs struggle to generate faithful SVGs, revealing a persistent gap
between language-centric and visual-centric coding. To close this gap, we
introduce VCoder, an agentic framework that augments VLMs along two axes: (i)
Thinking with Revision, which iteratively analyzes discrepancies and refines
SVG code; and (ii) Acting with Visual Tools, where detectors and parsers supply
structured cues such as objects, shapes, and text beyond the model's intrinsic
capacity. Across benchmarks, frontier VLMs with strong reasoning capabilities
score well overall yet remain limited in professional knowledge and 3D
reasoning. VCoder delivers a 12.3-point overall gain over the top-performing
Claude-4-Opus. Human studies show that both humans and VLMs perform worse on
rendered SVGs, their consistency reveals the promise of symbolic visual
representation. The benchmark and code are available at
https://github.com/CSU-JPG/VCode.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将对这篇论文摘要进行详细分析。</p>
<hr />
<h3 id="vcode-a-multimodal-coding-benchmark-with-svg-as-symbolic-visual-representation_1">论文摘要分析：VCode: a Multimodal Coding Benchmark with SVG as Symbolic Visual Representation</h3>
<p><strong>1. 论文主要贡献的简洁总结 (2-3 句话)</strong></p>
<p>这篇论文的核心贡献在于引入了 <strong>VCode</strong>，一个开创性的多模态编码基准，它将多模态理解重新定义为 <strong>SVG 代码生成</strong>。通过将图像转换为可解释、可执行的 SVG 代码，VCode 旨在评估模型在视觉中心编码任务中的符号保真度，并揭示了当前视觉语言模型（VLMs）在此类任务上的显著不足。为了弥补这一差距，论文还提出了 <strong>VCoder</strong>，一个结合了迭代修正和视觉工具的智能体框架，显著提升了SVG代码生成性能。</p>
<p><strong>2. 关键创新或方法论方法</strong></p>
<ul>
<li><strong>SVG 作为符号视觉表示：</strong> 核心创新在于将 SVG（可缩放矢量图形）代码作为一种紧凑、可解释且可执行的符号视觉表示。这与传统的像素级或特征级表示不同，SVG 能够捕捉图像的结构和语义，使其适用于下游推理。</li>
<li><strong>VCode 基准：</strong> 提出了一个全新的基准，将多模态理解任务转化为“给定图像生成 SVG 代码”。这个基准涵盖了通用常识（MM-Vet）、专业领域（MMMU）和视觉中心感知（CV-Bench）三个多样化领域，确保了评估的全面性。</li>
<li><strong>CodeVQA 评估协议：</strong> 为了评估生成 SVG 的符号保真度，引入了 CodeVQA。这是一种新颖的评估方法，通过让策略模型回答基于渲染 SVG 的问题来判断 SVG 是否忠实地保留了原始图像的符号意义。如果问题回答正确，则表明 SVG 具有高保真度。</li>
<li><strong>VCoder 智能体框架：</strong> 针对现有 VLMs 在 SVG 生成上的不足，提出了 VCoder。其创新点在于两个方面：<ul>
<li><strong>Thinking with Revision (思考与修正)：</strong> 迭代分析生成 SVG 与原始图像之间的差异，并对 SVG 代码进行精炼。这引入了类似人类的“反思”机制。</li>
<li><strong>Acting with Visual Tools (使用视觉工具行动)：</strong> 整合了外部视觉工具（如检测器和解析器），以提供模型自身能力之外的结构化线索，例如对象、形状和文本信息。这增强了模型的感知能力。</li>
</ul>
</li>
</ul>
<p><strong>3. 对领域潜在影响</strong></p>
<ul>
<li><strong>推动视觉中心编码研究：</strong> VCode 基准的引入将极大地推动计算机视觉和多模态领域对“视觉中心编码”的研究，填补了当前主要关注语言中心任务的空白。</li>
<li><strong>新的评估范式：</strong> CodeVQA 提出了一种新颖的、基于下游任务的符号保真度评估方法，这可能成为未来评估生成模型（尤其是生成结构化或符号表示的模型）的标准。</li>
<li><strong>智能体和具身智能的发展：</strong> VCoder 框架，特别是其“思考与修正”和“使用视觉工具”的理念，为开发更强大、更具推理能力的视觉智能体提供了新的方向。这对于具身智能体在复杂视觉环境中进行规划和行动至关重要。</li>
<li><strong>可解释性和可控性：</strong> SVG 作为一种符号表示，比黑盒神经网络输出更具可解释性。这有助于理解模型决策过程，并可能为生成内容的精细控制提供途径。</li>
<li><strong>人机交互和设计自动化：</strong> 能够从图像生成精确 SVG 代码的模型，在自动化设计、用户界面生成、数据可视化等领域具有巨大潜力。</li>
</ul>
<p><strong>4. 可能受益于这项研究的相关领域或应用</strong></p>
<ul>
<li><strong>多模态大模型 (VLMs) 和基础模型：</strong> 这项研究直接挑战并推动了 VLMs 在视觉理解和生成方面的能力边界。</li>
<li><strong>具身智能和机器人学：</strong> 智能体需要理解视觉场景并将其转化为可执行的指令，SVG 可以作为一种高级的场景表示，用于规划和任务执行。</li>
<li><strong>计算机辅助设计 (CAD) 和图形学：</strong> 自动化从草图或图像生成矢量图形，极大地提高设计效率。</li>
<li><strong>数据可视化：</strong> 从图像或数据描述生成定制化的 SVG 图表。</li>
<li><strong>人机交互 (HCI)：</strong> 允许用户通过图像输入来生成可编辑的视觉元素。</li>
<li><strong>程序合成和代码生成：</strong> 将视觉信息融入到代码生成中，扩展了程序合成的范围。</li>
<li><strong>图像编辑和风格迁移：</strong> 通过编辑 SVG 代码来实现对图像内容的精确控制。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性</strong></p>
<ul>
<li><strong>SVG 的表达能力限制：</strong> 尽管 SVG 强大，但它主要用于二维矢量图形。对于复杂的3D场景、光影效果、纹理细节或非矢量艺术（如照片），SVG 的表达能力可能有限。摘要中提到“3D reasoning”是当前VLMs的限制，这可能也暗示了SVG在某些3D表示上的不足。</li>
<li><strong>CodeVQA 的评估粒度：</strong> CodeVQA 通过“策略模型回答问题”来评估符号保真度。这种方法可能无法捕捉所有细微的视觉差异或语义错误，尤其是在问题设计不完善的情况下。</li>
<li><strong>VCoder 的泛化性：</strong> VCoder 依赖于“视觉工具”（检测器和解析器）。这些工具的性能和覆盖范围会直接影响 VCoder 的整体表现。如果工具在特定领域或新颖场景中表现不佳，VCoder 的效果也会受限。</li>
<li><strong>计算成本：</strong> 迭代修正过程（Thinking with Revision）可能会增加计算成本和推理时间，尤其是在需要多次迭代才能达到满意结果的情况下。</li>
<li><strong>人类研究的规模和范围：</strong> 摘要提到“Human studies show that both humans and VLMs perform worse on rendered SVGs”，这很有趣，但没有说明人类研究的规模、参与者背景以及具体任务，这可能影响结论的普遍性。</li>
<li><strong>“符号意义”的定义：</strong> 论文强调“preserving symbolic meaning”，但“符号意义”在不同上下文和任务中可能具有不同的解释。如何量化和评估这种“意义”的保留是一个持续的挑战。</li>
</ul>
<hr />
<p>总的来说，这篇论文提出了一种新颖且具有挑战性的范式，将计算机视觉和代码生成紧密结合。它不仅揭示了当前多模态模型的不足，还提供了一个有前景的解决方案，有望在未来推动智能体和多模态理解领域的发展。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To assess symbolic fidelity, we propose CodeVQA, a novel
evaluation protocol in which a policy model answers questions over rendered
SVGs; correct answers indicate faithful symbolic preservation.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.02778v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.02778v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.02777v1'></a></p>
<h2 id="perchead-perceptual-head-model-for-single-image-3d-head-reconstruction-editing"><a href="https://arxiv.org/abs/2511.02777v1">PercHead: Perceptual Head Model for Single-Image 3D Head Reconstruction &amp; Editing</a></h2>
<p><strong>Authors:</strong> Antonio Oroz, Matthias Nießner, Tobias Kirschstein</p>
<p><strong>Published:</strong> 2025-11-04</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We present PercHead, a method for single-image 3D head reconstruction and
semantic 3D editing - two tasks that are inherently challenging due to severe
view occlusions, weak perceptual supervision, and the ambiguity of editing in
3D space. We develop a unified base model for reconstructing view-consistent 3D
heads from a single input image. The model employs a dual-branch encoder
followed by a ViT-based decoder that lifts 2D features into 3D space through
iterative cross-attention. Rendering is performed using Gaussian Splatting. At
the heart of our approach is a novel perceptual supervision strategy based on
DINOv2 and SAM2.1, which provides rich, generalized signals for both geometric
and appearance fidelity. Our model achieves state-of-the-art performance in
novel-view synthesis and, furthermore, exhibits exceptional robustness to
extreme viewing angles compared to established baselines. Furthermore, this
base model can be seamlessly extended for semantic 3D editing by swapping the
encoder and finetuning the network. In this variant, we disentangle geometry
and style through two distinct input modalities: a segmentation map to control
geometry and either a text prompt or a reference image to specify appearance.
We highlight the intuitive and powerful 3D editing capabilities of our model
through a lightweight, interactive GUI, where users can effortlessly sculpt
geometry by drawing segmentation maps and stylize appearance via natural
language or image prompts.
  Project Page: https://antoniooroz.github.io/PercHead Video:
https://www.youtube.com/watch?v=4hFybgTk4kE</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将对这篇论文摘要进行详细分析。</p>
<hr />
<p><strong>论文分析：PercHead: Perceptual Head Model for Single-Image 3D Head Reconstruction &amp; Editing</strong></p>
<p><strong>1. 论文主要贡献的简洁总结 (2-3 句话)</strong></p>
<p>PercHead 提出了一种从单张图像进行 3D 头部重建和语义 3D 编辑的统一方法。它通过一个双分支编码器、基于 ViT 的解码器和高斯泼溅渲染，实现了视图一致的 3D 头部重建，并引入了基于 DINOv2 和 SAM2.1 的新型感知监督策略，显著提升了几何和外观的真实感。该模型在新视角合成方面达到了 SOTA 性能，并能无缝扩展到直观的语义 3D 编辑，通过分割图控制几何，文本或参考图像控制外观。</p>
<p><strong>2. 关键创新或方法论方法</strong></p>
<ul>
<li><strong>统一的单图像 3D 头部重建与编辑模型：</strong> 论文的核心在于提出了一个能够同时处理 3D 头部重建和语义 3D 编辑的统一基础模型，这在处理单图像 3D 任务的复杂性方面是一个显著进步。</li>
<li><strong>双分支编码器与 ViT-based 解码器：</strong> 模型采用双分支编码器来提取 2D 特征，并通过基于 ViT 的解码器，利用迭代交叉注意力将这些 2D 特征提升到 3D 空间，这是一种新颖的特征提升机制。</li>
<li><strong>高斯泼溅 (Gaussian Splatting) 渲染：</strong> 利用高斯泼溅进行渲染，这是一种近年来在实时渲染和新视角合成中表现出色的技术，有助于生成高质量、视图一致的 3D 头部。</li>
<li><strong>新型感知监督策略 (DINOv2 &amp; SAM2.1)：</strong> 这是该方法的核心创新之一。通过利用 DINOv2（自监督视觉变换器）和 SAM2.1（Segment Anything Model 的最新版本）提供的丰富、泛化的感知信号，模型能够获得更强的几何和外观真实感监督，克服了传统弱感知监督的挑战。</li>
<li><strong>几何与风格解耦的语义 3D 编辑：</strong> 在编辑模式下，模型通过两种不同的输入模态实现了几何和风格的解耦：分割图用于控制几何形状，而文本提示或参考图像用于指定外观。这种解耦使得编辑更加直观和强大。</li>
</ul>
<p><strong>3. 对领域潜在影响</strong></p>
<ul>
<li><strong>提升单图像 3D 重建的质量和鲁棒性：</strong> PercHead 在新视角合成和极端视角下的鲁棒性表现 SOTA，将显著推动单图像 3D 头部重建的实用性，尤其是在面对复杂姿态和遮挡时。</li>
<li><strong>开创性的感知监督范式：</strong> 结合 DINOv2 和 SAM2.1 的感知监督策略为其他 3D 重建任务提供了新的思路，可能成为未来处理弱监督或无监督 3D 任务的通用范式。</li>
<li><strong>更直观、强大的 3D 内容创作工具：</strong> 其语义 3D 编辑功能，特别是通过分割图和自然语言/图像提示进行几何和外观控制，将极大地简化 3D 头部模型的创建和修改过程，降低 3D 内容创作的门槛。</li>
<li><strong>推动实时 3D 交互应用：</strong> 高斯泼溅的引入以及模型的高效性，可能使其在虚拟现实、增强现实、游戏和虚拟形象等需要实时 3D 头部交互的领域具有巨大潜力。</li>
</ul>
<p><strong>4. 相关领域或应用受益</strong></p>
<ul>
<li><strong>虚拟现实 (VR) 和增强现实 (AR)：</strong> 用于创建高度逼真和可定制的虚拟形象，提升用户沉浸感。</li>
<li><strong>游戏开发：</strong> 快速生成游戏角色头部模型，并进行风格化编辑。</li>
<li><strong>电影和动画制作：</strong> 辅助角色建模和面部动画，加速内容生产。</li>
<li><strong>视频会议和直播：</strong> 实时生成或美化用户的 3D 头部模型，实现更丰富的交互体验。</li>
<li><strong>数字人 (Digital Humans) 和虚拟助手：</strong> 构建更具表现力和个性化的数字人。</li>
<li><strong>医学影像和面部整形模拟：</strong> 潜在地用于面部重建的预可视化和模拟（尽管需要进一步验证其精度和医学适用性）。</li>
<li><strong>计算机图形学研究：</strong> 为 3D 重建、渲染和编辑算法提供新的基准和研究方向。</li>
</ul>
<p><strong>5. 从摘要中推断出的任何局限性</strong></p>
<ul>
<li><strong>仅限于头部模型：</strong> 摘要明确指出是“3D Head Reconstruction &amp; Editing”，这意味着该方法可能专门针对头部区域进行了优化，不一定能直接泛化到全身或其他复杂物体。</li>
<li><strong>计算资源需求：</strong> 尽管摘要未直接提及，但基于 ViT 的解码器、迭代交叉注意力以及高斯泼溅渲染通常需要较高的计算资源，尤其是在训练阶段。实时编辑和渲染的效率可能仍是一个需要关注的问题，尽管高斯泼溅本身在渲染速度上有优势。</li>
<li><strong>编辑的精细度限制：</strong> 尽管语义编辑很强大，但通过“分割图”控制几何可能在某些极端精细的几何细节调整上不如传统 3D 建模工具灵活。例如，微调鼻子或嘴唇的特定曲线可能需要更精细的输入方式。</li>
<li><strong>泛化性挑战：</strong> 尽管感知监督策略很强大，但模型在面对训练数据中未见过的极端人脸特征、种族多样性或特殊面部表情时，其重建和编辑的质量和鲁棒性仍需进一步验证。</li>
<li><strong>“无缝扩展”的成本：</strong> 摘要提到“通过交换编码器和微调网络可以无缝扩展进行语义 3D 编辑”，这暗示了编辑功能可能需要额外的训练或微调步骤，并非完全零成本。</li>
<li><strong>DINOv2和SAM2.1的依赖性：</strong> 模型的性能在很大程度上依赖于 DINOv2和SAM2.1的泛化能力和鲁棒性。如果这些基础模型存在偏见或局限性，PercHead也可能继承这些问题。</li>
</ul>
<hr />
<p>总而言之，PercHead 是一项令人兴奋的研究，它在单图像 3D 头部重建和编辑方面取得了显著进展，特别是其创新的感知监督策略和直观的语义编辑功能，使其在计算机视觉和 3D 内容创作领域具有巨大的潜力。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present PercHead, a method for single-image 3D head reconstruction and
semantic 3D editing - two tasks that are inherently challenging due to severe
view occlusions, weak perceptual supervision, and the ambiguity of editing in
3D space.</li>
<li>We develop a unified base model for reconstructing view-consistent 3D
heads from a single input image.</li>
<li>At
the heart of our approach is a novel perceptual supervision strategy based on
DINOv2 and SAM2.1, which provides rich, generalized signals for both geometric
and appearance fidelity.</li>
<li>Our model achieves state-of-the-art performance in
novel-view synthesis and, furthermore, exhibits exceptional robustness to
extreme viewing angles compared to established baselines.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.02777v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.02777v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.02776v1'></a></p>
<h2 id="xr-1-towards-versatile-vision-language-action-models-via-learning-unified-vision-motion-representations"><a href="https://arxiv.org/abs/2511.02776v1">XR-1: Towards Versatile Vision-Language-Action Models via Learning Unified Vision-Motion Representations</a></h2>
<p><strong>Authors:</strong> Shichao Fan, Kun Wu, Zhengping Che, Xinhua Wang, Di Wu, Fei Liao, Ning Liu, Yixue Zhang, Zhen Zhao, Zhiyuan Xu, Meng Li, Qingjie Liu, Shanghang Zhang, Min Wan, Jian Tang</p>
<p><strong>Published:</strong> 2025-11-04</p>
<p><strong>Categories:</strong> cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>Recent progress in large-scale robotic datasets and vision-language models
(VLMs) has advanced research on vision-language-action (VLA) models. However,
existing VLA models still face two fundamental challenges: (i) producing
precise low-level actions from high-dimensional observations, (ii) bridging
domain gaps across heterogeneous data sources, including diverse robot
embodiments and human demonstrations. Existing methods often encode latent
variables from either visual dynamics or robotic actions to guide policy
learning, but they fail to fully exploit the complementary multi-modal
knowledge present in large-scale, heterogeneous datasets. In this work, we
present X Robotic Model 1 (XR-1), a novel framework for versatile and scalable
VLA learning across diverse robots, tasks, and environments. XR-1 introduces
the \emph{Unified Vision-Motion Codes (UVMC)}, a discrete latent representation
learned via a dual-branch VQ-VAE that jointly encodes visual dynamics and
robotic motion. UVMC addresses these challenges by (i) serving as an
intermediate representation between the observations and actions, and (ii)
aligning multimodal dynamic information from heterogeneous data sources to
capture complementary knowledge. To effectively exploit UVMC, we propose a
three-stage training paradigm: (i) self-supervised UVMC learning, (ii)
UVMC-guided pretraining on large-scale cross-embodiment robotic datasets, and
(iii) task-specific post-training. We validate XR-1 through extensive
real-world experiments with more than 14,000 rollouts on six different robot
embodiments, spanning over 120 diverse manipulation tasks. XR-1 consistently
outperforms state-of-the-art baselines such as <script type="math/tex">\pi_{0.5}</script>, <script type="math/tex">\pi_0</script>, RDT,
UniVLA, and GR00T-N1.5 while demonstrating strong generalization to novel
objects, background variations, distractors, and illumination changes. Our
project is at https://xr-1-vla.github.io/.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将对这篇论文摘要进行详细分析。</p>
<hr />
<h3 id="xr-1-towards-versatile-vision-language-action-models-via-learning-unified-vision-motion-representations_1">论文摘要分析：XR-1: Towards Versatile Vision-Language-Action Models via Learning Unified Vision-Motion Representations</h3>
<p><strong>1. 论文主要贡献的简洁总结 (2-3 句话)</strong></p>
<p>这篇论文提出了XR-1框架，旨在解决现有视觉-语言-动作 (VLA) 模型在从高维观测生成精确低级动作以及跨异构数据源（如不同机器人形态和人类演示）弥合领域鸿沟的挑战。其核心贡献在于引入了“统一视觉-运动编码 (UVMC)”，这是一种通过双分支VQ-VAE学习的离散潜在表示，能够联合编码视觉动态和机器人运动，从而作为观测与动作之间的中间表示，并对齐多模态动态信息。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<p>关键创新在于<strong>统一视觉-运动编码 (UVMC)</strong> 及其学习范式。
*   <strong>UVMC (Unified Vision-Motion Codes)</strong>：这是一种新颖的离散潜在表示，通过一个双分支VQ-VAE（Vector Quantized Variational Autoencoder）学习。它独特地将视觉动态（即环境变化、物体运动）和机器人运动（即机器人关节或末端执行器轨迹）联合编码到一个统一的潜在空间中。
    *   <strong>双分支VQ-VAE</strong>：这意味着模型有两个输入分支，一个处理视觉信息，一个处理机器人运动信息，然后通过VQ-VAE机制将它们压缩成离散的编码。这种联合编码是其核心，因为它能够捕捉视觉和运动之间的互补知识，而现有方法往往只关注其中之一。
*   <strong>UVMC-guided 三阶段训练范式</strong>：
    1.  <strong>自监督UVMC学习</strong>：首先，模型在没有特定任务标签的情况下，通过自监督方式学习生成有效的UVMC。这可能涉及预测未来的视觉帧或机器人运动，或者重建输入。
    2.  <strong>UVMC引导的预训练</strong>：在大规模、跨形态的机器人数据集上进行预训练，利用UVMC作为中间表示来指导策略学习。这有助于模型从多样化的数据中学习通用技能。
    3.  <strong>任务特定后训练</strong>：在特定任务上进行微调，以优化模型在该任务上的性能。</p>
<p><strong>3. 对领域潜在影响</strong></p>
<ul>
<li><strong>提升VLA模型的通用性和可扩展性</strong>：XR-1通过UVMC有效地桥接了高维观测与低级动作之间的鸿沟，并解决了异构数据源的领域差距，这对于构建能够处理多样化机器人、任务和环境的通用机器人模型至关重要。</li>
<li><strong>促进多模态学习在机器人领域的应用</strong>：UVMC明确地将视觉和运动信息融合到一个统一的表示中，为多模态学习在机器人控制中的应用提供了新的范式，可能启发更多结合不同模态信息的表示学习方法。</li>
<li><strong>降低机器人部署的复杂性</strong>：通过在大量不同机器人形态上进行预训练，XR-1有望减少为每个新机器人或新任务从头开始训练的需要，从而加速机器人技术的部署和应用。</li>
<li><strong>为未来大规模机器人基础模型奠定基础</strong>：XR-1的“统一视觉-运动表示”概念，以及其在多样化数据集上的表现，使其成为构建类似大型语言模型（LLMs）的“大型机器人模型”的关键一步。</li>
</ul>
<p><strong>4. 相关领域或应用受益</strong></p>
<ul>
<li><strong>通用机器人操作</strong>：例如，工业自动化、服务机器人、家庭机器人，需要处理各种物体、环境和任务。</li>
<li><strong>具身智能 (Embodied AI)</strong>：需要智能体在物理世界中感知、推理和行动的领域，如机器人导航、人机交互。</li>
<li><strong>远程操作和人机协作</strong>：通过学习人类演示，可以更好地理解和执行人类指令。</li>
<li><strong>合成数据生成和模拟器训练</strong>：UVMC作为一种紧凑的动态表示，可能有助于更高效地生成逼真的机器人交互数据或在模拟器中进行训练。</li>
<li><strong>医疗和辅助机器人</strong>：在复杂且多变的环境中执行精细操作。</li>
</ul>
<p><strong>5. 从摘要中可推断的局限性</strong></p>
<ul>
<li><strong>UVMC的解释性</strong>：虽然UVMC是离散的，但其内部表示的语义可解释性如何？是否能直观地理解某个UVMC编码代表了何种视觉-运动模式？摘要中未提及。</li>
<li><strong>计算资源需求</strong>：训练一个双分支VQ-VAE，并在14,000次真实世界rollout和120个任务上进行验证，暗示了巨大的计算资源需求，这可能限制了小型研究团队的复现和进一步研究。</li>
<li><strong>泛化能力的边界</strong>：尽管摘要声称对新物体、背景变化、干扰物和光照变化具有强大的泛化能力，但其泛化到完全未见过的机器人形态或任务类型（例如，需要复杂规划或长期记忆的任务）的能力仍有待进一步探讨。</li>
<li><strong>低级动作的精确性</strong>：摘要提到“产生精确的低级动作”是一个挑战，UVMC旨在解决此问题。但“精确”的定义和在极端精细操作（如微操作）中的表现如何，摘要中没有详细说明。</li>
<li><strong>语言模态的整合程度</strong>：虽然是“视觉-语言-动作”模型，但摘要主要强调了视觉和运动的融合。语言模态在UVMC学习和策略生成中的具体作用和深度整合方式，摘要中未详细阐述。它可能更多地体现在任务指令的理解上，而非直接影响UVMC的编码。</li>
</ul>
<hr />
<p>总而言之，XR-1通过其创新的UVMC表示和三阶段训练范式，为解决VLA模型的核心挑战提供了有前景的解决方案。它在多模态表示学习和通用机器人控制方面迈出了重要一步，有望成为未来机器人基础模型发展的重要里程碑。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this work, we
present X Robotic Model 1 (XR-1), a novel framework for versatile and scalable
VLA learning across diverse robots, tasks, and environments.</li>
<li>To effectively exploit UVMC, we propose a
three-stage training paradigm: (i) self-supervised UVMC learning, (ii)
UVMC-guided pretraining on large-scale cross-embodiment robotic datasets, and
(iii) task-specific post-training.</li>
<li>XR-1 consistently
outperforms state-of-the-art baselines such as <script type="math/tex">\pi_{0.5}</script>, <script type="math/tex">\pi_0</script>, RDT,
UniVLA, and GR00T-N1.5 while demonstrating strong generalization to novel
objects, background variations, distractors, and illumination changes.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.02776v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.02776v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-11-06 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
