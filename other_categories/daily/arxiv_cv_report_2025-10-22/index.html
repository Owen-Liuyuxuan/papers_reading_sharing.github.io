<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-10-22 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DEâ«¶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Birdâs-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">è®ºæéè¯»</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-Î±
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-10-21/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-10-23/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-10-22">Arxiv Computer Vision Papers - 2025-10-22</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#from-volume-rendering-to-3d-gaussian-splatting-theory-and-applications" class="nav-link">From Volume Rendering to 3D Gaussian Splatting: Theory and Applications</a>
                </li>
                <li class="nav-item">
                    <a href="#proclip-progressive-vision-language-alignment-via-llm-based-embedder" class="nav-link">ProCLIP: Progressive Vision-Language Alignment via LLM-based Embedder</a>
                </li>
                <li class="nav-item">
                    <a href="#exploring-a-unified-vision-centric-contrastive-alternatives-on-multi-modal-web-documents" class="nav-link">Exploring a Unified Vision-Centric Contrastive Alternatives on Multi-Modal Web Documents</a>
                </li>
                <li class="nav-item">
                    <a href="#think-with-3d-geometric-imagination-grounded-spatial-reasoning-from-limited-views" class="nav-link">Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views</a>
                </li>
                <li class="nav-item">
                    <a href="#descriptor-occluded-nuscenes-a-multi-sensor-dataset-for-evaluating-perception-robustness-in-automated-driving" class="nav-link">Descriptor: Occluded nuScenes: A Multi-Sensor Dataset for Evaluating Perception Robustness in Automated Driving</a>
                </li>
                <li class="nav-item">
                    <a href="#beyond-single-images-retrieval-self-augmented-unsupervised-camouflaged-object-detection" class="nav-link">Beyond Single Images: Retrieval Self-Augmented Unsupervised Camouflaged Object Detection</a>
                </li>
                <li class="nav-item">
                    <a href="#cross-modal-scene-semantic-alignment-for-image-complexity-assessment" class="nav-link">Cross-Modal Scene Semantic Alignment for Image Complexity Assessment</a>
                </li>
                <li class="nav-item">
                    <a href="#ranking-based-preference-optimization-for-diffusion-models-from-implicit-user-feedback" class="nav-link">Ranking-based Preference Optimization for Diffusion Models from Implicit User Feedback</a>
                </li>
                <li class="nav-item">
                    <a href="#vise-a-systematic-approach-to-vision-only-street-view-extrapolation" class="nav-link">ViSE: A Systematic Approach to Vision-Only Street-View Extrapolation</a>
                </li>
                <li class="nav-item">
                    <a href="#beyond-single-models-mitigating-multimodal-hallucinations-via-adaptive-token-ensemble-decoding" class="nav-link">Beyond Single Models: Mitigating Multimodal Hallucinations via Adaptive Token Ensemble Decoding</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-10-22">Arxiv Computer Vision Papers - 2025-10-22</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>å¥½çï¼è¿æ¯ä¸ä»½éå¯¹2025å¹´10æ20æ¥Arxivè®¡ç®æºè§è§é¢åè®ºæçæ¯æ¥æ¥åæ§è¡æè¦ï¼æ¨å¨å¸®å©å¿ç¢çç ç©¶äººåå¿«éäºè§£å³é®åå±ã</p>
<hr />
<p><strong>Arxiv è®¡ç®æºè§è§æ¯æ¥æ¥åæ§è¡æè¦ (2025-10-20)</strong></p>
<p><strong>1. ä¸»è¦ä¸»é¢ä¸è¶å¿æ¦è¿°ï¼</strong></p>
<p>ä»å¤©çè®ºæå±ç¤ºäºè®¡ç®æºè§è§é¢åå ä¸ªæ´»è·ä¸ç¸äºå³èçç ç©¶æ¹åãæ ¸å¿ä¸»é¢åæ¬ï¼</p>
<ul>
<li><strong>3D è§è§ä¸è¡¨ç¤ºï¼</strong> 3D éå»ºãåºæ¯çè§£åç©ºé´æ¨çæ¯æ¾èçç¦ç¹ï¼ç¹å«æ¯å¯¹æ°é¢ç3Dè¡¨ç¤ºæ¹æ³ï¼å¦é«æ¯æ³¼æºï¼çæ¢ç´¢ã</li>
<li><strong>å¤æ¨¡æå­¦ä¹ ä¸å¯¹é½ï¼</strong> è§è§-è¯­è¨æ¨¡åçè¿æ­¥ï¼ä»¥åè·¨æ¨¡ææ°æ®ï¼å¦å¾åãææ¬ã3Då ä½ï¼çå¯¹é½åèåï¼æ¯å¦ä¸ä¸ªéè¦è¶å¿ãè¿åæ¬å©ç¨å¤§åè¯­è¨æ¨¡åï¼LLMsï¼å¢å¼ºè§è§çè§£ï¼ä»¥åå¤çå¤æ¨¡æå¹»è§ã</li>
<li><strong>é²æ£æ§ä¸æ³åï¼</strong> éå¯¹å¤æç°å®ä¸çåºæ¯ï¼å¦é®æ¡ãæéè§è§ï¼çæç¥é²æ£æ§è¯ä¼°åæåï¼ä»¥åå¨æ çç£æå°æ ·æ¬è®¾ç½®ä¸çæ§è½ä¼åï¼æ¯æç»­å³æ³¨çé¢åã</li>
<li><strong>æ©æ£æ¨¡åä¸çæï¼</strong> æ©æ£æ¨¡åå¨å¾åçæåä¼åæ¹é¢çåºç¨ç»§ç»­æ·±åï¼ç¹å«æ¯å¨ç»åç¨æ·åé¦è¿è¡ä¸ªæ§åçææ¹é¢ã</li>
</ul>
<p><strong>2. æ¾èæåæ°æ§è®ºæäº®ç¹ï¼</strong></p>
<ul>
<li><strong>"From Volume Rendering to 3D Gaussian Splatting: Theory and Applications" (Vitor Pereira Matias et al.)</strong>: è¿ç¯è®ºæå¯è½ä»£è¡¨äº3Dè¡¨ç¤ºé¢åçä¸ä¸ªéè¦è¿å±ãå¦æå®æ·±å¥æ¢è®¨äº3Dé«æ¯æ³¼æºççè®ºåºç¡åå¹¿æ³åºç¨ï¼é£ä¹å®å¯è½ä¸ºå®æ¶ãé«è´¨éç3Déå»ºåæ¸²ææä¾äºæ°çèå¼ï¼å·æå¾é«çæ½å¨å½±ååã</li>
<li><strong>"ProCLIP: Progressive Vision-Language Alignment via LLM-based Embedder" (Xiaoxing Hu et al.)</strong>: ç»åLLMæ¥æ¸è¿å¼å°å¯¹é½è§è§-è¯­è¨åµå¥ï¼è¿æ¯ä¸ç§å©ç¨LLMå¼ºå¤§è¯­ä¹çè§£è½åæ¥æåå¤æ¨¡æå¯¹é½çåæ°æ¹æ³ï¼ææè§£å³ç°æCLIPæ¨¡åçä¸äºå±éæ§ã</li>
<li><strong>"Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views" (Zhangquan Chen et al.)</strong>: è¿ç¯è®ºææ¢è®¨äºä»æéè§è§è¿è¡3Då ä½æ³è±¡åç©ºé´æ¨ççè½åï¼è¿å¯¹äºæºå¨äººãAR/VRä»¥åå¤æåºæ¯çè§£è³å³éè¦ï¼ä»£è¡¨äºè®¤ç¥æºè½å¨è§è§é¢åçåºç¨ã</li>
</ul>
<p><strong>3. æ°å´ç ç©¶æ¹åæææ¯ï¼</strong></p>
<ul>
<li><strong>3D Gaussian Splatting (3DGS) çæ·±å¥ç ç©¶ååºç¨ï¼</strong> è®ºæ1è¡¨æ3DGSæ­£ä»åæ­¥æ¢ç´¢èµ°åæ´æ·±å¥ççè®ºååºç¨ç ç©¶ï¼é¢ç¤ºçå®å¯è½æä¸ºæªæ¥3Dè§è§é¢åçéè¦åºç³ã</li>
<li><strong>LLMå¨è§è§-è¯­è¨å¯¹é½åå¤æ¨¡æå¹»è§ç¼è§£ä¸­çä½ç¨ï¼</strong> è®ºæ2åè®ºæ10é½å¼ºè°äºLLMå¨æåå¤æ¨¡ææ¨¡åæ§è½åè§£å³å¶åºæé®é¢ï¼å¦å¹»è§ï¼æ¹é¢çæ½åï¼è¡¨æLLMä¸åä»ä»æ¯ææ¬å¤çå·¥å·ï¼èæ¯å¤æ¨¡æAIçå³é®ç»ä»¶ã</li>
<li><strong>æ£ç´¢å¢å¼ºï¼Retrieval Self-Augmentedï¼å­¦ä¹ ï¼</strong> è®ºæ6éè¿æ£ç´¢èªå¢å¼ºæ¥æåæ çç£ä¼ªè£ç®æ æ£æµï¼è¿æ¯ä¸ç§å©ç¨æ°æ®æ¬èº«è¿è¡ææå­¦ä¹ çç­ç¥ï¼ææå¨æ°æ®ç¨ç¼ºææ æ æ³¨åºæ¯ä¸åæ¥éè¦ä½ç¨ã</li>
<li><strong>éå¼ç¨æ·åé¦ä¼åæ©æ£æ¨¡åï¼</strong> è®ºæ8å©ç¨éå¼ç¨æ·åé¦æ¥ä¼åæ©æ£æ¨¡åï¼è¿ä¸ºä¸ªæ§ååç¨æ·å¯¼åççææ¨¡åæä¾äºæ°çä¼åéå¾ï¼å·æå®éåºç¨ä»·å¼ã</li>
</ul>
<p><strong>4. ææä»·å¼éè¯»çè®ºæå»ºè®®ï¼</strong></p>
<p>å¯¹äºå¸ææ·±å¥äºè§£ææ°è¿å±çç ç©¶äººåï¼å»ºè®®ä¼åéè¯»ä»¥ä¸è®ºæï¼</p>
<ul>
<li><strong>"From Volume Rendering to 3D Gaussian Splatting: Theory and Applications" (Vitor Pereira Matias et al.)</strong>: å¦ææ¨å¯¹3Dè§è§ãæ°é¢çåºæ¯è¡¨ç¤ºåå®æ¶æ¸²ææå´è¶£ï¼è¿ç¯è®ºææ¯å¿è¯»çï¼å ä¸ºå®å¯è½å®ä¹äºè¯¥é¢åçæ°æ¹åã</li>
<li><strong>"ProCLIP: Progressive Vision-Language Alignment via LLM-based Embedder" (Xiaoxing Hu et al.)</strong>: å¯¹äºå³æ³¨å¤æ¨¡æå­¦ä¹ ãè§è§-è¯­è¨æ¨¡åä»¥åLLMå¨CVä¸­åºç¨çç ç©¶èï¼è¿ç¯è®ºææä¾äºå©ç¨LLMæåå¯¹é½çæ°æè·¯ã</li>
<li><strong>"Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views" (Zhangquan Chen et al.)</strong>: å¦ææ¨çç ç©¶æ¶åé«çº§åºæ¯çè§£ãæºå¨äººæéè¦ä»ä¸å®æ´ä¿¡æ¯ä¸­è¿è¡æ¨ççåºç¨ï¼è¿ç¯è®ºæå°æä¾æä»·å¼çè§è§£ã</li>
<li><strong>"Descriptor: Occluded nuScenes: A Multi-Sensor Dataset for Evaluating Perception Robustness in Automated Driving" (Sanjay Kumar et al.)</strong>: å¯¹äºèªå¨é©¾é©¶ãé²æ£æ§è¯ä¼°åæ°æ®éæå»ºçç ç©¶äººåï¼è¿ç¯è®ºææä¾äºä¸ä¸ªéè¦çèµæºåè¯ä¼°åºåã</li>
</ul>
<hr />
<p>è¿ä»½æè¦æ¨å¨æä¾ä¸ä¸ªé«å±æ¬¡çæ¦è§ï¼å¸®å©æ¨å¿«éè¯å«ä¸æ¨ç ç©¶å´è¶£æç¸å³çè®ºæã</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2510.18101v1">From Volume Rendering to 3D Gaussian Splatting: Theory and Applications</a></li>
<li><a href="#2510.18795v1">ProCLIP: Progressive Vision-Language Alignment via LLM-based Embedder</a></li>
<li><a href="#2510.18703v1">Exploring a Unified Vision-Centric Contrastive Alternatives on Multi-Modal Web Documents</a></li>
<li><a href="#2510.18632v1">Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views</a></li>
<li><a href="#2510.18552v1">Descriptor: Occluded nuScenes: A Multi-Sensor Dataset for Evaluating Perception Robustness in Automated Driving</a></li>
<li><a href="#2510.18437v1">Beyond Single Images: Retrieval Self-Augmented Unsupervised Camouflaged Object Detection</a></li>
<li><a href="#2510.18377v1">Cross-Modal Scene Semantic Alignment for Image Complexity Assessment</a></li>
<li><a href="#2510.18353v1">Ranking-based Preference Optimization for Diffusion Models from Implicit User Feedback</a></li>
<li><a href="#2510.18341v1">ViSE: A Systematic Approach to Vision-Only Street-View Extrapolation</a></li>
<li><a href="#2510.18321v1">Beyond Single Models: Mitigating Multimodal Hallucinations via Adaptive Token Ensemble Decoding</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2510.18101v1'></a></p>
<h2 id="from-volume-rendering-to-3d-gaussian-splatting-theory-and-applications"><a href="https://arxiv.org/abs/2510.18101v1">From Volume Rendering to 3D Gaussian Splatting: Theory and Applications</a></h2>
<p><strong>Authors:</strong> Vitor Pereira Matias, Daniel Perazzo, Vinicius Silva, Alberto Raposo, Luiz Velho, Afonso Paiva, Tiago Novello</p>
<p><strong>Published:</strong> 2025-10-20</p>
<p><strong>Categories:</strong> cs.CV, 68-01, A.1</p>
<p><strong>Abstract:</strong></p>
<p>The problem of 3D reconstruction from posed images is undergoing a
fundamental transformation, driven by continuous advances in 3D Gaussian
Splatting (3DGS). By modeling scenes explicitly as collections of 3D Gaussians,
3DGS enables efficient rasterization through volumetric splatting, offering
thus a seamless integration with common graphics pipelines. Despite its
real-time rendering capabilities for novel view synthesis, 3DGS suffers from a
high memory footprint, the tendency to bake lighting effects directly into its
representation, and limited support for secondary-ray effects. This tutorial
provides a concise yet comprehensive overview of the 3DGS pipeline, starting
from its splatting formulation and then exploring the main efforts in
addressing its limitations. Finally, we survey a range of applications that
leverage 3DGS for surface reconstruction, avatar modeling, animation, and
content generation-highlighting its efficient rendering and suitability for
feed-forward pipelines.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼è¿æ¯ä¸ç¯å³äºâFrom Volume Rendering to 3D Gaussian Splatting: Theory and Applicationsâçå¨é¢æè¦ï¼ç±Vitor Pereira Matiasç­äººæ°åã</p>
<p><strong>è®ºææè¦ï¼ä»ä½æ¸²æå°3Dé«æ¯æ³¼æºï¼çè®ºä¸åºç¨</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢</strong>
è¯¥è®ºæä¸»è¦å³æ³¨3Déå»ºé¢åï¼ç¹å«æ¯å¦ä½ä»å§¿æå¾åä¸­é«æãé«è´¨éå°éå»º3Dåºæ¯ãä¼ ç»æ¹æ³ï¼å¦åºäºç½æ ¼çè¡¨ç¤ºï¼å¨å¯å¾®åæ§æ¹é¢å­å¨ææï¼èç¥ç»è¾å°åºï¼NeRFsï¼è½ç¶æä¾äºé«è´¨éçæ°è§è§åæï¼ä½è®¡ç®ææ¬é«æï¼é¾ä»¥å®ç°å®æ¶æ¸²æã3Dé«æ¯æ³¼æºï¼3DGSï¼ä½ä¸ºä¸ç§æ°å´ææ¯ï¼æ¨å¨è§£å³è¿äºéå¶ï¼å®ç°å®æ¶æ¸²æåé«æç3Déå»ºã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®</strong>
è¯¥è®ºæä½ä¸ºä¸ç¯æç¨æ§ç»¼è¿°ï¼å¶ä¸»è¦è´¡ç®å¨äºï¼
*   <strong>3DGSçç´è§æ°å­¦æ¨å¯¼ï¼</strong> è®ºæä»ä½æ¸²ææ¹ç¨åºåï¼ç´è§å°æ¨å¯¼äº3DGSçæ³¼æºï¼splattingï¼å¬å¼ï¼è§£éäºé«æ¯åå§ååè®­ç»è¿ç¨ä¸­çèªéåºææ¯ã
*   <strong>3DGSçå¨é¢æ¦è¿°ï¼</strong> è¯¦ç»ä»ç»äº3DGSçæ´ä¸ªæµç¨ï¼ä»SfMç¹äºåå§åé«æ¯ï¼å°éè¿ä½æ³¼æºè¿è¡æ¸²æï¼åå°å©ç¨ååº¦æå¤±è¿è¡ä¼ååé«æ¯èªéåºï¼åè£ãåéãä¿®åªï¼ã
*   <strong>å¯¹3DGSå±éæ§çæ¢è®¨åç°æè§£å³æ¹æ¡çç»¼è¿°ï¼</strong> è®ºææ·±å¥åæäºåå§3DGSæ¹æ³çå±éæ§ï¼åæ¬é«åå­å ç¨ãåç§ææççå°è¡¨ç¤ºä¸­ãå¯¹äºæ¬¡åçº¿æææ¯ææéç­ãå¹¶è¯¦ç»ä»ç»äºéå¯¹è¿äºé®é¢çææ°æ©å±åæ¹è¿ï¼ä¾å¦ï¼
    *   <strong>åå­ä¼åï¼</strong> ä»ç»äºSCAFFOLDç­éè¿å¤å±æç¥å¨åéç¹æ¥åå°é«æ¯æ°éçæ¹æ³ã
    *   <strong>æé¯é½¿ä¸å¤åè¾¨çï¼</strong> è®¨è®ºäºMIP-Splattingç­éè¿2Då3Dé«æ¯æ»¤æ³¢å¨è§£å³åè¾¨çååå¼èµ·çé¯é½¿é®é¢ã
    *   <strong>éé¢åå°ä¸éæåï¼</strong> ä»ç»äºGaussian-Shaderã3DGS-DRãIRGSç­å°ç»å¸åå°åçè²æ¦å¿µèå¥3DGSæ¡æ¶çæ¹æ³ï¼ä»¥åå©ç¨BRDFåæ°ååçº¿è¿½è¸ªææ¯å®ç°ç©çæ¸²æã
    *   <strong>ä½æ³¼æºçæ¹è¿ï¼</strong> æ¢è®¨äºStop-the-popç­è§£å³æåºä¼ªå½±çæ¹æ³ï¼ä»¥åéè¿è¡¥å¿é¡¹æé«ä½æ¸²æåç¡®æ§çæ¹æ³ã
*   <strong>3DGSåºç¨é¢åçå¹¿æ³è°æ¥ï¼</strong> è®ºæå¨é¢åé¡¾äº3DGSå¨è¡¨é¢éå»ºãå¤´åå»ºæ¨¡ãå¨ç»ååå®¹çæç­å¤ä¸ªåºç¨ä¸­çææ°è¿å±ï¼çªåºäºå¶é«ææ¸²æåéç¨äºåé¦ç®¡éçç¹æ§ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹</strong>
è¯¥è®ºæå¼ºè°äº3DGSå¨3Déå»ºé¢åå¸¦æ¥çæ ¹æ¬æ§åé©ãéè¿å°åºæ¯å»ºæ¨¡ä¸º3Dé«æ¯éåï¼3DGSå®ç°äºé«æçä½æ³¼æºæ¸²æï¼å¹¶ä¸ç°æå¾å½¢ç®¡éæ ç¼éæãå¶å®æ¶æ¸²æè½åå¯¹äºæ°è§è§åæè³å³éè¦ãè®ºæéè¿å¯¹ç°ææç®çå¨é¢ç»¼è¿°ï¼å±ç¤ºäº3DGSåå¶æ©å±å¨è§£å³å¤æåºæ¯éå»ºãå¨æåå®¹çæãé«ä¿çå¤´åå»ºæ¨¡ç­æ¹é¢çå¼ºå¤§æ½åãè¿äºè¿å±ä½¿å¾3DGSæä¸ºè®¡ç®æºè§è§åå¾å½¢å­¦é¢åä¸ä¸ªå¿«éåå±ä¸æå·åæ¯çç ç©¶æ¹åã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§</strong>
å°½ç®¡3DGSåå¾äºæ¾èè¿å±ï¼ä½è®ºæä¹æç¡®æåºäºå¶åºæçå±éæ§ï¼
*   <strong>é«åå­å ç¨ï¼</strong> å¤æåºæ¯éè¦å¤§éç3Dé«æ¯ï¼20ä¸å°50ä¸ï¼ï¼å¯¼è´æ¾èçåå­åå­å¨éæ±ã
*   <strong>åç§ææççï¼</strong> åå§3DGSæ¨¡åå°åç§æ¡ä»¶ç´æ¥ççå°è¡¨ç¤ºä¸­ï¼éå¶äºå¶å¨é«åº¦åå°è¡¨é¢ä¸çæ§è½ï¼å¹¶é»ç¢äºéæåè½åã
*   <strong>äºæ¬¡åçº¿æææ¯ææéï¼</strong> ç¼ºä¹å¯¹åå°ãæå°ç­äºæ¬¡åçº¿ææçç´æ¥æ¯æï¼å½±åäºæ¸²æççå®æã
*   <strong>æ¸²æç²¾åº¦é®é¢ï¼</strong> ä½æ³¼æºå¼å¥äºä¸äºè¿ä¼¼ï¼å¯è½å½±åæ¸²æç²¾åº¦ï¼å¯¼è´âå¼¹åºâä¼ªå½±ç­é®é¢ã
*   <strong>ç¨çè§å¾éå»ºææï¼</strong> å¨ç¨çå¾åéä¸ï¼3DGSçä¼åå®¹æé·å¥å±é¨æå°å¼ã
*   <strong>ä¸éç¨äºç½æ ¼æåï¼</strong> åå§3DGSå¹¶éç´æ¥ä¸ºç½æ ¼æåèè®¾è®¡ï¼éè¦é¢å¤çææ¯æ¥å¢å¼ºå¶è¡¨é¢éå»ºè½åã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹å</strong>
è®ºææåºäº3DGSé¢åæªæ¥ç ç©¶çå ä¸ªæåæ¯çæ¹åï¼
*   <strong>ä¼åé«æ¯æ°éï¼</strong> å¯»æ¾ç¡®å®æä½³é«æ¯æ°éçæ¹æ³ï¼ä»¥å¹³è¡¡åå­æçåæ¸²æè´¨éã
*   <strong>æ¹è¿æ³¼æºå¬å¼ï¼</strong> è¿ä¸æ­¥å®åä½æ³¼æºå¬å¼ï¼æé«æ¸²æç²¾åº¦ï¼åå°è¿ä¼¼å¸¦æ¥çä¼ªå½±ã
*   <strong>å¼ååé¦3Déå»ºæ¨¡åï¼</strong> æ¢ç´¢æ´å¿«éãæ´é²æ£çåé¦3Déå»ºæ¨¡åï¼å°¤å¶æ¯å¨ç¨çè¾å¥è§å¾ä¸ã
*   <strong>å¢å¼ºäºæ¬¡åçº¿ææï¼</strong> è¿ä¸æ­¥æ´ååçº¿è¿½è¸ªææ¯ï¼ä»¥æ´å¥½å°æ¨¡æåå°ãæå°ç­å¤æåç§ç°è±¡ã
*   <strong>ä¸çææ¨¡åçç»åï¼</strong> è¿ä¸æ­¥æ¢ç´¢3DGSä¸å¾å/è§é¢æ©æ£æ¨¡åãçæå¯¹æç½ç»ï¼GANsï¼ç­çææ¨¡åçç»åï¼ä»¥å®ç°æ´é«æãæ´çµæ´»ç3Dåå®¹åå»ºåå¨ç»ã
*   <strong>æ´å¹¿æ³çåºç¨ï¼</strong> æ¢ç´¢3DGSå¨æ´å¤å®éåºç¨åºæ¯ä¸­çæ½åï¼ä¾å¦æºå¨äººãèªå¨é©¾é©¶ãèæç°å®/å¢å¼ºç°å®ç­ã</p>
<p>æ»èè¨ä¹ï¼è¿ç¯æç¨æ§è®ºæä¸ºçè§£3DGSæä¾äºä¸ä¸ªå¨é¢èæ·±å¥çè§è§ï¼ä»å¶çè®ºåºç¡å°å®éåºç¨ï¼å¹¶æåºäºè¯¥é¢åé¢ä¸´çææåæªæ¥çåå±æ¹åï¼å¯¹äºè®¡ç®æºè§è§åå¾å½¢å­¦ç ç©¶èå·æéè¦çåèä»·å¼ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Despite its
real-time rendering capabilities for novel view synthesis, 3DGS suffers from a
high memory footprint, the tendency to bake lighting effects directly into its
representation, and limited support for secondary-ray effects.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.18101v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.18101v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.18795v1'></a></p>
<h2 id="proclip-progressive-vision-language-alignment-via-llm-based-embedder"><a href="https://arxiv.org/abs/2510.18795v1">ProCLIP: Progressive Vision-Language Alignment via LLM-based Embedder</a></h2>
<p><strong>Authors:</strong> Xiaoxing Hu, Kaicheng Yang, Ziyong Feng, Qi Ming, Zonghao Guo, Xiang An, Ziyong Feng, Junchi Yan, Xue Yang</p>
<p><strong>Published:</strong> 2025-10-21</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>The original CLIP text encoder is limited by a maximum input length of 77
tokens, which hampers its ability to effectively process long texts and perform
fine-grained semantic understanding. In addition, the CLIP text encoder lacks
support for multilingual inputs. All these limitations significantly restrict
its applicability across a broader range of tasks. Recent studies have
attempted to replace the CLIP text encoder with an LLM-based embedder to
enhance its ability in processing long texts, multilingual understanding, and
fine-grained semantic comprehension. However, because the representation spaces
of LLMs and the vision-language space of CLIP are pretrained independently
without alignment priors, direct alignment using contrastive learning can
disrupt the intrinsic vision-language alignment in the CLIP image encoder,
leading to an underutilization of the knowledge acquired during pre-training.
To address this challenge, we propose ProCLIP, a curriculum learning-based
progressive vision-language alignment framework to effectively align the CLIP
image encoder with an LLM-based embedder. Specifically, ProCLIP first distills
knowledge from CLIP's text encoder into the LLM-based embedder to leverage
CLIP's rich pretrained knowledge while establishing initial alignment between
the LLM embedder and CLIP image encoder. Subsequently, ProCLIP further aligns
the CLIP image encoder with the LLM-based embedder through image-text
contrastive tuning, employing self-distillation regularization to avoid
overfitting. To achieve a more effective alignment, instance semantic alignment
loss and embedding structure alignment loss are employed during representation
inheritance and contrastive tuning. The Code is available at
https://github.com/VisionXLab/ProCLIP</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾å¯¹Xiaoxing Huç­äººæ°åçè®ºæâProCLIP: Progressive Vision-Language Alignment via LLM-based Embedderâçå¨é¢æè¦ã</p>
<hr />
<h3 id="proclip-progressive-vision-language-alignment-via-llm-based-embedder_1">è®ºææè¦ï¼ProCLIP: Progressive Vision-Language Alignment via LLM-based Embedder</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢</strong></p>
<p>åå§CLIPæ¨¡åçææ¬ç¼ç å¨å­å¨æ¾èå±éæ§ï¼
*   <strong>è¾å¥é¿åº¦éå¶ï¼</strong> æå¤åªè½å¤ç77ä¸ªtokenï¼è¿ä¸¥ééå¶äºå¶å¤çé¿ææ¬åè¿è¡ç»ç²åº¦è¯­ä¹çè§£çè½åã
*   <strong>å¤è¯­è¨æ¯æç¼ºå¤±ï¼</strong> æ æ³å¤çå¤è¯­è¨è¾å¥ã
*   <strong>è¯­ä¹çè§£ä¸è¶³ï¼</strong> ç¼ºä¹å¯¹ç»ç²åº¦ææ¬è¯­ä¹ççç£ï¼è¿ä¸æ­¥é»ç¢äºå¶è¯­ä¹çè§£è½åã</p>
<p>è¿æç ç©¶å°è¯ç¨åºäºå¤§åè¯­è¨æ¨¡åï¼LLMï¼çåµå¥å¨æ¿æ¢CLIPææ¬ç¼ç å¨ï¼ä»¥è§£å³é¿ææ¬ãå¤è¯­è¨åç»ç²åº¦è¯­ä¹çè§£é®é¢ãç¶èï¼ç±äºLLMçè¡¨ç¤ºç©ºé´ä¸CLIPçè§è§-è¯­è¨ç©ºé´æ¯ç¬ç«é¢è®­ç»çï¼ä¸ç¼ºä¹å¯¹é½åéªï¼ç´æ¥ä½¿ç¨å¯¹æ¯å­¦ä¹ è¿è¡å¯¹é½å¯è½ä¼ç ´åCLIPå¾åç¼ç å¨ä¸­åºæçè§è§-è¯­è¨å¯¹é½ï¼å¯¼è´é¢è®­ç»ç¥è¯çå©ç¨ä¸è¶³ã</p>
<p>å æ­¤ï¼æ¬ææ¨å¨è§£å³çå³é®ç ç©¶é®é¢æ¯ï¼<strong>å¦ä½ç³»ç»å°å©ç¨CLIPçé¢è®­ç»ç¥è¯ï¼ä»¥å®ç°ä¸LLM-basedåµå¥å¨çé«æè·¨æ¨¡æå¯¹é½ï¼åæ¶ä¿ææ¨¡åçæ³åè½åï¼</strong></p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®</strong></p>
<p>æ¬ææåºäº<strong>ProCLIP</strong>ï¼ä¸ä¸ªåºäºè¯¾ç¨å­¦ä¹ çæ¸è¿å¼è§è§-è¯­è¨å¯¹é½æ¡æ¶ï¼æ¨å¨ææå¯¹é½CLIPå¾åç¼ç å¨ä¸LLM-basedåµå¥å¨ãå¶æ ¸å¿æ¹æ³è®ºè´¡ç®åæ¬ï¼</p>
<ul>
<li>
<p><strong>ä¸¤é¶æ®µæ¸è¿å¼å¯¹é½æ¡æ¶ï¼</strong></p>
<ul>
<li><strong>é¶æ®µ1ï¼è·¨æ¶æè¸é¦çè¡¨ç¤ºç»§æ¿ï¼Representation Inheritance via Cross-Architecture Distillationï¼ï¼</strong> ProCLIPé¦åå°ç¥è¯ä»åå§CLIPææ¬ç¼ç å¨è¸é¦å°LLM-basedåµå¥å¨ï¼ä»MLPå¯è®­ç»ï¼ï¼ä»èå©ç¨CLIPä¸°å¯çé¢è®­ç»ç¥è¯ï¼å¹¶å»ºç«LLMåµå¥å¨ä¸CLIPå¾åç¼ç å¨ä¹é´çåæ­¥å¯¹é½ã<ul>
<li><strong>å®ä¾è¯­ä¹å¯¹é½æå¤±ï¼Instance Semantic Alignment Loss, Linsï¼ï¼</strong> ç¨äºå¨å®ä¾çº§å«ä¸å¯¹é½LLMåµå¥å¨åCLIPææ¬åµå¥å¨çç»´åº¦ã</li>
<li><strong>åµå¥ç»æå¯¹é½æå¤±ï¼Embedding Structure Alignment Loss, Lstructï¼ï¼</strong> è¡¡éæ¹æ¬¡åæ ·æ¬é´çè·ç¦»å·®å¼ï¼éè¿æå°åæå¯¹è·ç¦»å·®å¼å®ç°å¨å±å¯¹é½ã</li>
</ul>
</li>
<li><strong>é¶æ®µ2ï¼éæèªè¸é¦æ­£ååçå¯¹æ¯å¾®è°ï¼Contrastive Tuning Integrated with Self-Distillation Regularizationï¼ï¼</strong> å¨åæ­¥å¯¹é½çåºç¡ä¸ï¼ProCLIPéè¿å¾å-ææ¬å¯¹æ¯å­¦ä¹ è¿ä¸æ­¥å¯¹é½CLIPå¾åç¼ç å¨ä¸LLM-basedåµå¥å¨ãä¸ºé¿åè¿æåï¼å¼å¥èªè¸é¦æ­£ååï¼Self-Distillation Regularizationï¼çº¦æCLIPå¾åç¼ç å¨ï¼ç¨³å®è®­ç»å¹¶æé«æ³åè½åã<ul>
<li><strong>ä¿¡æ¯å¯¹æ¯æå¤±ï¼InfoNCE Loss, Linfoï¼ï¼</strong> ç¨äºå¾ååµå¥åLLMåµå¥ä¹é´çå¯¹æ¯å­¦ä¹ ã</li>
<li><strong>æ­£ååæå¤±ï¼Lregï¼ï¼</strong> å¯¹CLIPå¾åç¼ç å¨æ½å å¯¹ç§°çæ­£ååæå¤±ï¼ä»¥åè½»è®­ç»è¿ç¨ä¸­çç¾é¾æ§éå¿ï¼å¹¶ä¿çæ¨¡åçæ³åè½åã</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>å¼ºè°é¢è®­ç»ç¥è¯çå©ç¨ï¼</strong> æç¡®æåºå¹¶è§£å³äºç°ææ¹æ³æªè½ååå©ç¨CLIPé¢è®­ç»ç¥è¯çå±éæ§ï¼éè¿è¸é¦åèªè¸é¦æºå¶æææ´åäºè¿äºç¥è¯ã</p>
</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹</strong></p>
<p>ProCLIPå¨å¤é¡¹ä»»å¡åä¸åæ°æ®è§æ¨¡ä¸åè¡¨ç°åºåè¶çæ§è½åé²æ£æ§ï¼</p>
<ul>
<li><strong>é¶æ ·æ¬åç±»ï¼Zero-Shot Classificationï¼ï¼</strong> ç¸æ¯åºçº¿æ¨¡åï¼ProCLIPå¨é¶æ ·æ¬åç±»ä»»å¡ä¸å®ç°äº6.8%è³13.5%çæ¾èæåãå°¤å¶æ¯å¨30Mæ°æ®æ ·æ¬ä¸ï¼å¹³åæ§è½æåäºçº¦10%-13.5%ã</li>
<li><strong>è·¨æ¨¡ææ£ç´¢ï¼Cross-Modal Retrievalï¼ï¼</strong> å¨ç­ææ¬åé¿ææ¬æ£ç´¢ä»»å¡ä¸­ï¼ProCLIPå§ç»ä¼äºLLM2CLIPãä¾å¦ï¼å¨Flickr30kæ°æ®éä¸ï¼ä½¿ç¨ViT-L/14å30Mè®­ç»æ ·æ¬ï¼å¾åå°ææ¬ï¼I2Tï¼æ£ç´¢çRecall@1è¾¾å°äº95.0%ï¼æ¯LLM2CLIPé«åºè¿2ä¸ªç¾åç¹ãå¨é¿ææ¬åºåæµè¯ï¼å¦DOCCIãDCIãUrban-1kï¼ä¸ï¼ProCLIPä¹è¡¨ç°åºææ¾ä¼å¿ï¼ç¹å«æ¯å¨T2Iæ£ç´¢æ¹é¢æå¼ºå²æåã</li>
<li><strong>å¤è¯­è¨è·¨æ¨¡ææ£ç´¢ï¼Multilingual Cross-Modal Retrievalï¼ï¼</strong> åçäºLLM-basedåµå¥å¨ï¼ProCLIPå¨XM3600åºåæµè¯ä¸å®ç°äºä¼è¶çå¤è¯­è¨æ§è½ï¼è¿å½å äºCLIPå¾åç¼ç å¨ä¸LLM-basedåµå¥å¨ä¹é´æ¹è¿çå¯¹é½ã</li>
<li><strong>é²æ£æ§ï¼Robustnessï¼ï¼</strong> å¨ImageNet-AåImageNet-Rç­å·ææææ§çåå¸å¤æ°æ®éä¸ï¼ProCLIPçæ§è½ä¼äºLLM2CLIPè¶è¿10ä¸ªç¾åç¹ï¼å¹³åæå5.9%-9.3%ï¼è¡¨æå¶å¤çåå¸åç§»åå¤ææ°å¨çè½åå¢å¼ºã</li>
<li><strong>ç»ç²åº¦çè§£ï¼Fine-Grained Understandingï¼ï¼</strong> å¨MMVP-VLMåºåæµè¯ä¸ï¼ProCLIPæ¨¡åå¨ç¸åºæ°æ®è§æ¨¡ä¸å®ç°äº3.0%ã2.2%å10.4%çæåï¼è¡¨æLLM-basedåµå¥å¨å¢å¼ºäºç»ç²åº¦è¯­ä¹åºåè½åï¼ä¸æ¸è¿å¼å¯¹é½ç­ç¥ææã</li>
<li><strong>æ°æ®åæ¨¡åè§æ¨¡åæï¼</strong> æ§è½éæ°æ®è§æ¨¡å¢å èæåï¼ä¸ProCLIPå¨ç¸åæ°æ®è§æ¨¡ä¸å§ç»ä¼äºLLM2CLIPï¼å±ç°äºæ°æ®æçãéè¿å¢å çº¿æ§å±æ°éï¼12å±ï¼ï¼æ¨¡åæ§è½è¿ä¸æ­¥æåï¼è¡¨æProCLIPå¯ä»ç®åçåæ°æ©å±ä¸­æç»­åçã</li>
</ul>
<p>è¿äºç»æå±åè¯æäºProCLIPçæææ§åé²æ£æ§ï¼å®æåå°è§£å³äºç°ææ¹æ³çå±éæ§ï¼å¹¶å¨å¹¿æ³çè§è§-è¯­è¨ä»»å¡ä¸­åå¾äºæ¾èè¿å±ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§</strong></p>
<ul>
<li><strong>è®­ç»æçï¼</strong> ProCLIPå¨ç¬¬äºé¶æ®µéè¦è§£å»è§è§ç¼ç å¨å¹¶è¿è¡å¨çº¿èªè¸é¦ï¼è¿å¢å äºè®¡ç®ææ¬ãå®éªä¸­ï¼è®­ç»éåº¦çº¦ä¸ºåºçº¿ç0.74åã</li>
<li><strong>ç»ç²åº¦è§è§å¯¹é½ï¼</strong> ProCLIPç®åä»åºäºå¨å±è¯­ä¹è¿è¡å¯¹æ¯å­¦ä¹ ãä½èæåºï¼å°å±é¨è§è§è¡¥ä¸ä¸ææ¬è¯­ä¹å¯¹é½å¯ä»¥è¿ä¸æ­¥å¢å¼ºè§è§ç¼ç å¨çå±é¨æç¥è½åï¼ä»èæçäºå¼æ¾è¯æ±è¯­ä¹åå²åç®æ æ£æµç­ä»»å¡ã</li>
<li><strong>æ¨¡åæ¶æï¼</strong> æ¬æä¸»è¦å³æ³¨æ¿æ¢CLIPææ¬ç¼ç å¨ãä½èæåºï¼æ¯å¦ä¹å¯ä»¥ç±»ä¼¼å°æ¿æ¢åå¡æ¶æä¸­çè§è§ç¼ç å¨ï¼ä»¥è§£å³è§è§è¡¨ç¤ºçå±éæ§ï¼ä¾å¦CLIPå¾åç¼ç å¨ç¼ºä¹å±é¨æ§ï¼ã</li>
</ul>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹å</strong></p>
<ul>
<li><strong>æé«è®­ç»æçï¼</strong><ul>
<li>å¨ç¬¬äºé¶æ®µéç¨åºäºPEFTï¼Parameter-Efficient Fine-Tuningï¼çæ¹æ³æ¥å¾®è°è§è§ç¼ç å¨ã</li>
<li>ä»å¾®è°è§è§ç¼ç å¨åæ°çä¸é¨åï¼ä¾å¦æåå ä¸ªTransformeråã</li>
<li>ç¨ç¦»çº¿è¸é¦æ¿æ¢å¨çº¿è¸é¦ï¼ä»¥å¤§å¹åå°ç¬¬äºé¶æ®µå¼å¥çé¢å¤è®¡ç®ææ¬ã</li>
</ul>
</li>
<li><strong>å¢å¼ºç»ç²åº¦è§è§å¯¹é½ï¼</strong> æ¢ç´¢å°å±é¨è§è§è¡¥ä¸ä¸ææ¬è¯­ä¹å¯¹é½çæ¹æ³ï¼ä»¥æåè§è§ç¼ç å¨çå±é¨æç¥è½åï¼ä»èæ´å¥½å°æ¯æå¼æ¾è¯æ±è¯­ä¹åå²åç®æ æ£æµç­ä»»å¡ã</li>
<li><strong>æ¢ç´¢æ°çæ¨¡åæ¶æï¼</strong> ç ç©¶æ¯å¦å¯ä»¥æ¿æ¢åå¡æ¶æä¸­çè§è§ç¼ç å¨ï¼ä»¥è§£å³ç°æè§è§è¡¨ç¤ºçå±éæ§ï¼ä¾å¦CLIPå¾åç¼ç å¨ç¼ºä¹å±é¨æ§çé®é¢ã</li>
</ul>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this challenge, we propose ProCLIP, a curriculum learning-based
progressive vision-language alignment framework to effectively align the CLIP
image encoder with an LLM-based embedder.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.18795v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.18795v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.18703v1'></a></p>
<h2 id="exploring-a-unified-vision-centric-contrastive-alternatives-on-multi-modal-web-documents"><a href="https://arxiv.org/abs/2510.18703v1">Exploring a Unified Vision-Centric Contrastive Alternatives on Multi-Modal Web Documents</a></h2>
<p><strong>Authors:</strong> Yiqi Lin, Alex Jinpeng Wang, Linjie Li, Zhengyuan Yang, Mike Zheng Shou</p>
<p><strong>Published:</strong> 2025-10-21</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Contrastive vision-language models such as CLIP have demonstrated strong
performance across a wide range of multimodal tasks by learning from aligned
image-text pairs. However, their ability to handle complex, real-world web
documents remains limited, particularly in scenarios where text and images are
interleaved, loosely aligned, or embedded in visual form. To address these
challenges, we propose Vision-Centric Contrastive Learning (VC2L), a unified
framework that models text, images, and their combinations using a single
vision transformer. VC2L operates entirely in pixel space by rendering all
inputs, whether textual, visual, or combined, as images, thus eliminating the
need for OCR, text tokenization, or modality fusion strategy. To capture
complex cross-modal relationships in multimodal web documents, VC2L employs a
snippet-level contrastive learning objective that aligns consecutive multimodal
segments, leveraging the inherent coherence of documents without requiring
explicitly paired image-text data. To assess the effectiveness of this
approach, we introduce three retrieval benchmarks, AnyCIR, SeqCIR, and CSR,
designed to evaluate cross-modal retrieval, fine-grained sequential
understanding, and generalization to unseen data, respectively. Empirical
results show that VC2L achieves competitive or superior performance compared to
CLIP-style models on both the proposed benchmarks and established datasets such
as M-BEIR and MTEB. These findings underscore the potential of multimodal web
data as a valuable training resource for contrastive learning and illustrate
the scalability of a unified, vision-centric approach for multimodal
representation learning. Code and models are available at:
https://github.com/showlab/VC2L.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼è¿æ¯ä¸ç¯å³äºYiqi Lin, Alex Jinpeng Wang, Linjie Li, Zhengyuan Yang, Mike Zheng Shouæ°åçè®ºæâExploring a Unified Vision-Centric Contrastive Alternatives on Multi-Modal Web Documentsâçå¨é¢æè¦ï¼</p>
<p><strong>è®ºæé¢ç®ï¼</strong> æ¢ç´¢å¤æ¨¡æç½ç»ææ¡£ä¸­ç»ä¸çä»¥è§è§ä¸ºä¸­å¿çå¯¹æ¯å­¦ä¹ æ¿ä»£æ¹æ¡</p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
ç°æçå¯¹æ¯è§è§-è¯­è¨æ¨¡åï¼å¦CLIPï¼å¨å¤çå¤æãçå®ä¸ççç½ç»ææ¡£æ¶é¢ä¸´å±éæ§ãè¿äºææ¡£éå¸¸åå«äº¤éçææ¬åå¾åãæ¾æ£å¯¹é½çåå®¹ï¼æèææ¬ä»¥è§è§å½¢å¼åµå¥ï¼ä¾å¦æ«æææ¡£æå¾åPDFï¼ãè¿äºæ¨¡åé¾ä»¥ææå¤çè¿äºåºæ¯ï¼å ä¸ºå®ä»¬éå¸¸éè¦OCRãææ¬åè¯æå¤æçæ¨¡æèåç­ç¥ï¼å¹¶ä¸ä¾èµäºæ¾å¼å¯¹é½çå¾å-ææ¬å¯¹ãè®ºææ¨å¨è§£å³å¦ä½æå»ºä¸ä¸ªç»ä¸çãä»¥è§è§ä¸ºä¸­å¿çæ¡æ¶ï¼è½å¤ç´æ¥ä»åç´ ç©ºé´çè§£å¤æ¨¡æç½ç»ææ¡£ï¼ä»èåæè¿äºææã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
*   <strong>Vision-Centric Contrastive Learning (VC2L) æ¡æ¶ï¼</strong> è®ºææåºäºVC2Lï¼ä¸ä¸ªç»ä¸çæ¡æ¶ï¼éè¿åä¸ªè§è§Transformerå¤çææ¬ãå¾ååå¶ç»åãå®å°ææè¾å¥ï¼æ è®ºæ¯ææ¬ãè§è§è¿æ¯ç»ååå®¹ï¼æ¸²ææå¾åï¼å®å¨å¨åç´ ç©ºé´ä¸­æä½ï¼ä»èæ¶é¤äºå¯¹OCRãææ¬åè¯ææ¨¡æèåç­ç¥çéæ±ã
*   <strong>çæ®µçº§å¯¹æ¯å­¦ä¹ ç®æ ï¼</strong> VC2Léç¨çæ®µçº§å¯¹æ¯å­¦ä¹ ç®æ ï¼éè¿å¯¹é½ææ¡£ä¸­è¿ç»­çå¤æ¨¡æçæ®µæ¥ææå¤æçè·¨æ¨¡æå³ç³»ãè¿ç§æ¹æ³å©ç¨ææ¡£åºæçè¿è´¯æ§ï¼æ éæ¾å¼éå¯¹çå¾å-ææ¬æ°æ®ã
*   <strong>æ°æ®å¢å¼ºç­ç¥ï¼</strong> å¼å¥äºæ¨¡ææ©è½ï¼modality maskingï¼åææ¬æ©è½ï¼text maskingï¼å¢å¼ºï¼ä»¥å¢å å¯¹æ¯ç®æ ç ë¤ìæ§ï¼å¹¶æåæ¨¡åå¯¹è¯­è¨ççè§£è½åã
*   <strong>æ°åºåæµè¯ï¼</strong> ä¸ºäºè¯ä¼°VC2Lçæææ§ï¼è®ºæå¼å¥äºä¸ä¸ªæ°çæ£ç´¢åºåï¼
    *   <strong>AnyCIR (Any-to-Any Consecutive Information Retrieval)ï¼</strong> è¯ä¼°è·¨æ¨¡ææ£ç´¢è½åã
    *   <strong>SeqCIR (Sequential Consecutive Information Retrieval)ï¼</strong> è¯ä¼°ç»ç²åº¦åºåçè§£è½åï¼éè¿é¡ºåºæ£ç´¢è¿ç»­çæ®µã
    *   <strong>CSR (Zero-Shot Consecutive Slide Retrieval)ï¼</strong> è¯ä¼°æ¨¡åå¯¹æªè§æ°æ®çæ³åè½åï¼ç¹å«æ¯å¨å¤æå¾å-ææ¬äº¤éçå¹»ç¯çæ°æ®ä¸ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>åè¶çæ§è½ï¼</strong> å®éªç»æè¡¨æï¼VC2Lå¨æåºçAnyCIRãSeqCIRåCSRåºåæµè¯ä»¥åM-BEIRåMTEBç­ç°ææ°æ®éä¸ï¼ä¸CLIPé£æ ¼çæ¨¡åç¸æ¯ï¼åå¾äºå·æç«äºåçè³æ´ä¼å¼çæ§è½ã
*   <strong>ç»ä¸è§è§æ¹æ³çæææ§ï¼</strong> ç»æå¼ºè°äºå¤æ¨¡æç½ç»æ°æ®ä½ä¸ºå¯¹æ¯å­¦ä¹ å®è´µè®­ç»èµæºçæ½åï¼å¹¶å±ç¤ºäºç»ä¸çãä»¥è§è§ä¸ºä¸­å¿çæ¹æ³å¨å¤æ¨¡æè¡¨ç¤ºå­¦ä¹ æ¹é¢çå¯æ©å±æ§ã
*   <strong>åç´ ç©ºé´è¯­è¨çè§£çæåï¼</strong> èåå¾å-ææ¬äº¤éè®­ç»è½å¤è¿ä¸æ­¥æé«æ¨¡åå¨åç´ ç©ºé´ä¸­çè¯­è¨çè§£è½åã
*   <strong>å¯¹å¤æå¸å±çæ³åè½åï¼</strong> VC2Lå³ä½¿å¨æ¸²ææ°æ®ä¸è¿è¡è®­ç»ï¼ä¹è½ææå°æ³åå°å·æä¸åå­ä½å¤§å°åæ ·å¼ççå®ä¸çå¤æå¸å±ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>åºå®è¾å¥å°ºå¯¸çéå¶ï¼</strong> å°½ç®¡VC2Lè½å¤ä½¿ç¨åä¸ªæ¨¡åå¤çä»»ä½æ¨¡æè¾å¥ï¼ä½å¶æçåå¯æ©å±æ§åéäºåºå®çè¾å¥å°ºå¯¸ã
*   <strong>è®­ç»æ°æ®è§æ¨¡ï¼</strong> è®ºæä¸­ä½¿ç¨çè®­ç»æ°æ®ï¼500ä¸ææ¡£ï¼çº¦1700ä¸å¾åï¼ç¸å¯¹äºWIT-400Mç­å¤§åæ°æ®éæ¥è¯´ç¸å¯¹è¾å°ï¼è¿éå¶äºæ¨¡åå¨æ´å¤§è§æ¨¡ä¸çæ¢ç´¢ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>å¨æè¾å¥ç­ç¥åæ°æ¶æï¼</strong> æªæ¥çå·¥ä½å¯ä»¥æ¢ç´¢è®¾è®¡å¨æè¾å¥ç­ç¥ææ°çæ¶æï¼ä»¥æ¾èæé«æ§è½ï¼å¹¶è§£éæ´å¤ä»¥è§è§ä¸ºä¸­å¿çå¤æ¨¡æç½ç»æ°æ®çè§£åºç¨ã
*   <strong>å¤§è§æ¨¡è®­ç»ï¼</strong> é´äºå½åè®­ç»æ°æ®è§æ¨¡çéå¶ï¼æªæ¥å¯ä»¥è¿è¡æ´å¤§è§æ¨¡çè®­ç»å®éªã
*   <strong>è´è´£ä»»çé¨ç½²åçç£ï¼</strong> å¼ºè°å¨é¨ç½²åºäºVC2Lçæ£ç´¢ç³»ç»æ¶ï¼éè¦æ´ååå®¹è¿æ»¤ãç¨æ·è®¿é®æ§å¶åå¯è§£éæ§åè½ï¼ç¡®ä¿è®­ç»æ°æ®çå¤æ ·æ§åéå¾·æ¥æºï¼å¹¶å»ºç«å¥å¨ççæ§ç¨åºï¼ä»¥æ£æµååºå¯¹æ»¥ç¨è¡ä¸ºã</p>
<p>æ»èè¨ä¹ï¼VC2Lä¸ºä»å¤æææ¡£æºè¿è¡å¤æ¨¡ææ£ç´¢æä¾äºä¸ä¸ªæ°é¢ä¸å®ç¨çè§£å³æ¹æ¡ï¼éè¿å°ææåå®¹æ¸²æå°åç´ ç©ºé´å¹¶å©ç¨ææ¡£çåºæè¿è´¯æ§è¿è¡çæ®µçº§å¯¹æ¯å­¦ä¹ ï¼å®ç°äºç»ä¸çè§è§-è¯­è¨çè§£ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address these
challenges, we propose Vision-Centric Contrastive Learning (VC2L), a unified
framework that models text, images, and their combinations using a single
vision transformer.</li>
<li>To assess the effectiveness of this
approach, we introduce three retrieval benchmarks, AnyCIR, SeqCIR, and CSR,
designed to evaluate cross-modal retrieval, fine-grained sequential
understanding, and generalization to unseen data, respectively.</li>
<li>Empirical
results show that VC2L achieves competitive or superior performance compared to
CLIP-style models on both the proposed benchmarks and established datasets such
as M-BEIR and MTEB.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.18703v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.18703v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.18632v1'></a></p>
<h2 id="think-with-3d-geometric-imagination-grounded-spatial-reasoning-from-limited-views"><a href="https://arxiv.org/abs/2510.18632v1">Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views</a></h2>
<p><strong>Authors:</strong> Zhangquan Chen, Manyuan Zhang, Xinlei Yu, Xufang Luo, Mingze Sun, Zihao Pan, Yan Feng, Peng Pei, Xunliang Cai, Ruqi Huang</p>
<p><strong>Published:</strong> 2025-10-21</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, I.2.10</p>
<p><strong>Abstract:</strong></p>
<p>Though recent advances in vision-language models (VLMs) have achieved
remarkable progress across a wide range of multimodal tasks, understanding 3D
spatial relationships from limited views remains a significant challenge.
Previous reasoning methods typically rely on pure text (e.g., topological
cognitive maps) or on 2D visual cues. However, their limited representational
capacity hinders performance in specific tasks that require 3D spatial
imagination. To address this limitation, we propose 3DThinker, a framework that
can effectively exploits the rich geometric information embedded within images
while reasoning, like humans do. Our framework is the first to enable 3D
mentaling during reasoning without any 3D prior input, and it does not rely on
explicitly labeled 3D data for training. Specifically, our training consists of
two stages. First, we perform supervised training to align the 3D latent
generated by VLM while reasoning with that of a 3D foundation model (e.g.,
VGGT). Then, we optimize the entire reasoning trajectory solely based on
outcome signals, thereby refining the underlying 3D mentaling. Extensive
experiments across multiple benchmarks show that 3DThinker consistently
outperforms strong baselines and offers a new perspective toward unifying 3D
representations into multimodal reasoning. Our code will be available at
https://github.com/zhangquanchen/3DThinker.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾è®ºæãThink with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Viewsãçå¨é¢æè¦ã</p>
<p><strong>è®ºææè¦ï¼Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views</strong></p>
<p>è¿ç¯ç±Zhangquan Chenåå¶å¢éæ°åçè®ºæãThink with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Viewsãæåºäºä¸ç§åä¸º3DThinkerçæ°åæ¡æ¶ï¼æ¨å¨è§£å³è§è§-è¯­è¨æ¨¡åï¼VLMsï¼å¨ä»æéè§è§çè§£3Dç©ºé´å³ç³»æ¹é¢çæ¾èææã</p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
å°½ç®¡è§è§-è¯­è¨æ¨¡åå¨å¤æ¨¡æä»»å¡ä¸­åå¾äºå·¨å¤§è¿å±ï¼ä½å®ä»¬å¨ä»æéè§è§çè§£3Dç©ºé´å³ç³»æ¹é¢ä»ç¶é¢ä¸´éå¤§ææãç°æçæ¨çæ¹æ³ä¸»è¦ä¾èµçº¯ææ¬æ2Dè§è§çº¿ç´¢ï¼å¶æéçè¡¨ç¤ºè½åé»ç¢äºéè¦3Dç©ºé´æ³è±¡åçç¹å®ä»»å¡çæ§è½ãæ ¸å¿é®é¢å¨äºVLMæ æ³ä»å¾åä¸­æå3Då ä½ä¿¡æ¯ï¼ä¸å¶ç©ºé´æ³è±¡è½ååéã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
3DThinkeræ¡æ¶çæ ¸å¿åæ°å¨äºä½¿VLMè½å¤å¨æ¨çè¿ç¨ä¸­âåäººç±»ä¸æ ·âææå°å©ç¨å¾åä¸­åµå¥çä¸°å¯å ä½ä¿¡æ¯ï¼å¹¶è¿è¡3Då¿æºæ³è±¡ï¼3D mentalingï¼ï¼èæ éä»»ä½3Dåéªè¾å¥ææ¾å¼æ æ³¨ç3Dæ°æ®è¿è¡è®­ç»ãå¶ä¸»è¦è´¡ç®åæ¬ï¼
*   <strong>3Då¿æºæ³è±¡æ¡æ¶ï¼</strong> é¦æ¬¡å¼å¥âthink with 3D mentalingâæ¡æ¶ï¼è¯¥æ¡æ¶æ éä¾èµå¯éçæ æ³¨è®­ç»æ°æ®ï¼å¦è®¤ç¥å°å¾ï¼ã
*   <strong>ä¸¤é¶æ®µè®­ç»æ¡æ¶ï¼</strong>
    *   <strong>ç¬¬ä¸é¶æ®µï¼çç£è®­ç»ï¼ï¼</strong> å°VLMå¨æ¨çè¿ç¨ä¸­çæç3Dæ½å¨è¡¨ç¤ºä¸3Dåºç¡æ¨¡åï¼å¦VGGTï¼çç¹å¾ç©ºé´è¿è¡å¯¹é½ãè¿éè¿3Dæ½å¨å¯¹é½æå¤±åäº¤åçµæå¤±å®ç°ï¼ä½¿æ¨¡åå¨ä¿æææ¬è¿è´¯æ§çåæ¶ï¼è½å¤è¿è¡3Då¿æºæ³è±¡ã
    *   <strong>ç¬¬äºé¶æ®µï¼å¼ºåå­¦ä¹ ï¼ï¼</strong> å¨çç£è®­ç»ä¹åï¼ä»åºäºç»æä¿¡å·ä¼åæ´ä¸ªæ¨çè½¨è¿¹ï¼ä»èå¨è½¨è¿¹ä¸­ç»ååºå±ç3Då¿æºæ³è±¡ï¼åæ¶ä¿æ3Dæ½å¨è¡¨ç¤ºçå¯¹é½ã
*   <strong>å¯è§£éæ§å¢å¼ºï¼</strong> 3DThinkeréè¿æå½±å¨è½å¤ä»æ½å¨ç©ºé´ä¸­æ¢å¤3Dè¡¨ç¤ºï¼ä¾å¦ç¹äºï¼ï¼æ¾èå¢å¼ºäºå¤§åæ¨çæ¨¡åçå¯è§£éæ§ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>æ§è½æ¾èæåï¼</strong> å¨MindCube-TinyåEgo3D-Benchç­å¤ä¸ªåºåæµè¯ä¸­ï¼3DThinkerå§ç»ä¼äºå¼ºå¤§çåºçº¿æ¨¡åãä¾å¦ï¼å¨MindCube-Tinyä¸ï¼3DThinker-fullçæ´ä½æ§è½æåèå´ä¸º51.8%è³108.8%ï¼å¨Ego3D-Benchä¸ï¼æåèå´ä¸º18.1%è³36.9%ã
*   <strong>è·¨æ°æ®éæ³åè½åï¼</strong> å³ä½¿å¨æ²¡æEgo3Dç¹å®æ°æ®è®­ç»çæåµä¸ï¼æ¨¡åå¨Ego3D-Benchä¸ä»åå¾äºæå¸æçç»æï¼è¡¨æå¶å¨ä¸åç©ºé´çè§£åºæ¯ä¸­å·æå¼ºå¤§çæ³åè½åã
*   <strong>ç»ä¸3Dè¡¨ç¤ºçæ°è§è§ï¼</strong> è®ºæä¸ºå°3Dè¡¨ç¤ºç»ä¸å°å¤æ¨¡ææ¨çä¸­æä¾äºæ°è§è§ï¼çªåºäºå¶å¹¿æ³éç¨æ§ã
*   <strong>æ¶èç ç©¶ï¼</strong> å®éªè¡¨æï¼æä½³ç3Dæ½å¨å¤§å°çº¦ä¸º12ï¼ä¸å°3Dç¹æ®tokenæ¾ç½®å¨æ¨çè½¨è¿¹çå¼å¤´æç»å°¾è½è·å¾æ´å¥½çæ§è½ãæ­¤å¤ï¼3Då¯¹é½åæç»ç­æ¡å¥å±å¯¹æ§è½è³å³éè¦ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>3Dè¡¨ç¤ºçèªåå½æ´åï¼</strong> ç®åï¼3DThinkerä»ç¹æ®tokençæåä¸å±éèç¶æä¸­æ¢å¤3Då¿æºè¡¨ç¤ºï¼ä½è¿äºæ½å¨è¡¨ç¤ºå¹¶æªèªåå½å°æ´åå°æ¡æ¶ä¸­ã
*   <strong>è¿­ä»£3Då¿æºæ³è±¡ï¼</strong> è®ºæå°æªæ¢ç´¢å¨æ¨çè½¨è¿¹ä¸­è¿è¡è¿­ä»£3Då¿æºæ³è±¡ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>ç»ä¸ç»æå¼åï¼</strong> å¼åä¸ä¸ªç»ä¸çç»æï¼ä¾å¦ï¼ç»ä¸çtokenizerï¼ï¼ä»¥èªåå½å°æ´å3Dæ½å¨è¡¨ç¤ºï¼è¿å°æ¯æªæ¥æ¹è¿çå³é®é¢åã
*   <strong>è¿­ä»£3Då¿æºæ³è±¡æ¢ç´¢ï¼</strong> æ¢ç´¢å¨æ¨çè½¨è¿¹ä¸­è¿è¡è¿­ä»£3Då¿æºæ³è±¡å¯è½ä¼å¸¦æ¥é¢å¤ççå¤ã</p>
<p>æ»èè¨ä¹ï¼3DThinkeræ¡æ¶éè¿å¶ç¬ç¹çä¸¤é¶æ®µè®­ç»æ¹æ³ï¼ä½¿è§è§-è¯­è¨æ¨¡åè½å¤è¿è¡åå¨ç3Då¿æºæ³è±¡ï¼ä»èå¨æ éæ¾å¼3Dæ æ³¨æ°æ®çæåµä¸ï¼æ¾èæåäºæ¨¡åå¨3Dç©ºé´æ¨çä»»å¡ä¸­çæ§è½åæ³åè½åãè¿ä¸ºå¤æ¨¡ææ¨çä¸­3Dè¡¨ç¤ºçç»ä¸æä¾äºä¸ä¸ªæåæ¯çæ°æ¹åã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this limitation, we propose 3DThinker, a framework that
can effectively exploits the rich geometric information embedded within images
while reasoning, like humans do.</li>
<li>Extensive
experiments across multiple benchmarks show that 3DThinker consistently
outperforms strong baselines and offers a new perspective toward unifying 3D
representations into multimodal reasoning.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.18632v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.18632v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.18552v1'></a></p>
<h2 id="descriptor-occluded-nuscenes-a-multi-sensor-dataset-for-evaluating-perception-robustness-in-automated-driving"><a href="https://arxiv.org/abs/2510.18552v1">Descriptor: Occluded nuScenes: A Multi-Sensor Dataset for Evaluating Perception Robustness in Automated Driving</a></h2>
<p><strong>Authors:</strong> Sanjay Kumar, Tim Brophy, Reenu Mohandas, Eoin Martino Grua, Ganesh Sistu, Valentina Donzella, Ciaran Eising</p>
<p><strong>Published:</strong> 2025-10-21</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Robust perception in automated driving requires reliable performance under
adverse conditions, where sensors may be affected by partial failures or
environmental occlusions. Although existing autonomous driving datasets
inherently contain sensor noise and environmental variability, very few enable
controlled, parameterised, and reproducible degradations across multiple
sensing modalities. This gap limits the ability to systematically evaluate how
perception and fusion architectures perform under well-defined adverse
conditions. To address this limitation, we introduce the Occluded nuScenes
Dataset, a novel extension of the widely used nuScenes benchmark. For the
camera modality, we release both the full and mini versions with four types of
occlusions, two adapted from public implementations and two newly designed. For
radar and LiDAR, we provide parameterised occlusion scripts that implement
three types of degradations each, enabling flexible and repeatable generation
of occluded data. This resource supports consistent, reproducible evaluation of
perception models under partial sensor failures and environmental interference.
By releasing the first multi-sensor occlusion dataset with controlled and
reproducible degradations, we aim to advance research on robust sensor fusion,
resilience analysis, and safety-critical perception in automated driving.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Sanjay Kumarç­äººæ°åçè®ºæâDescriptor: Occluded nuScenes: A Multi-Sensor Dataset for Evaluating Perception Robustness in Automated Drivingâçå¨é¢æè¦ã</p>
<hr />
<p><strong>è®ºææè¦ï¼Occluded nuScenesï¼ç¨äºè¯ä¼°èªå¨é©¾é©¶ä¸­æç¥é²æ£æ§çå¤ä¼ æå¨æ°æ®é</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
èªå¨é©¾é©¶ç³»ç»å¨æ¶å£æ¡ä»¶ä¸ï¼å¦ä¼ æå¨é¨åæéæç¯å¢é®æ¡ï¼éè¦é²æ£çæç¥è½åãç¶èï¼ç°æèªå¨é©¾é©¶æ°æ®éè½ç¶åå«ä¼ æå¨åªå£°åç¯å¢åå¼æ§ï¼ä½å¾å°è½å¯¹å¤ç§ä¼ ææ¨¡æè¿è¡åæ§ãåæ°ååå¯å¤ç°çéçº§ãè¿éå¶äºç³»ç»æ§è¯ä¼°æç¥åèåæ¶æå¨æç¡®å®ä¹çæ¶å£æ¡ä»¶ä¸çè¡¨ç°ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
ä¸ºäºè§£å³ä¸è¿°éå¶ï¼ä½èå¼å¥äº<strong>Occluded nuScenesæ°æ®é</strong>ï¼è¿æ¯å¯¹å¹¿æ³ä½¿ç¨çnuScenesåºåæµè¯çä¸ä¸ªæ°é¢æ©å±ãå¶ä¸»è¦åæ°åè´¡ç®åæ¬ï¼
*   <strong>é¦ä¸ªå¤ä¼ æå¨é®æ¡æ°æ®éï¼</strong> é¦æ¬¡å¨ç¸æºãé·è¾¾åLiDARä¸ç§ä¸»è¦ä¼ æå¨æ¨¡æä¸åºç¨åæé®æ¡ã
*   <strong>ç¸æºæ¨¡æçåç§é®æ¡ç±»åï¼</strong> éå¯¹ç¸æºæ¨¡æï¼åå¸äºå®æ´çåè¿·ä½ çæ°æ®éï¼åå«åç§é®æ¡ç±»åââä¸¤ç§æ¹ç¼èªç°æå¬å±å®ç°ï¼ç°å°åæ°´æ¨¡ç³ï¼ï¼ä¸¤ç§æ¯æ°è®¾è®¡çï¼WoodScapeæ±¡å¢æ¨¡å¼ååçï¼ãè¿äºé®æ¡æ¨¡æäºéå¤´æ±¡æåè¡¨é¢ç£¨æç­çå®ä¸çæåºã
*   <strong>é·è¾¾åLiDARæ¨¡æçåæ°åé®æ¡èæ¬ï¼</strong> éå¯¹é·è¾¾åLiDARï¼æä¾äºåæ°åé®æ¡èæ¬ï¼æ¯ç§æ¨¡æå®ç°ä¸ç§éçº§ç±»åã
    *   <strong>é·è¾¾ï¼</strong> åä¼ æå¨æéï¼éæºç¦ç¨ä¸ä¸ªé·è¾¾ï¼ãç¹äºä¸¢å¼ï¼æ¨¡æä¿¡å·ä¸å®æ´ï¼åç¯å¢åªå£°ï¼æ·»å é«æ¯åªå£°ï¼ã
    *   <strong>LiDARï¼</strong> åºäºåºåçé®æ¡ï¼æ¨¡æç²åºï¼ãé¨åç¹äºä¸¢å¼ï¼æ¨¡ææ¶å£å¤©æ°ï¼ååºäºè§åº¦çé®æ¡ï¼æ¨¡æé¥å½¢ç²åºï¼ã
*   <strong>åæ§åå¯å¤ç°çéçº§ï¼</strong> è¿äºèæ¬åè®¸ç¨æ·çµæ´»ãå¯å¤ç°å°çæä¸åä¸¥éç¨åº¦çé®æ¡æ°æ®ï¼ç¡®ä¿äºä¸ç°ænuScenesæ æ³¨åæç¥ç®¡éçå®å¨å¼å®¹æ§ã
*   <strong>å¨é¢çéªè¯ï¼</strong> éè¿ç»æä¸è´æ§æ£æ¥ãè§è§æ£æ¥ãåæ°éªè¯ä»¥åå¨è½¦è¾åå²ãå°å¾åå²å3Dç®æ æ£æµç­ä¸æ¸¸æç¥ä»»å¡ä¸çæ§è½è¯ä¼°ï¼éªè¯äºæ°æ®éçæææ§åè´¨éã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>æç¥æ¨¡åæ§è½ä¸éï¼</strong> å¨åé®æ¡æ°æ®ä¸è¯ä¼°æ¶ï¼åºçº¿æç¥æ¶æï¼å¦SimpleBEVãBEVFusionåBEVCarï¼åè¡¨ç°åºå¯æµéçæ§è½ä¸éï¼è¯å®äºé®æ¡æ¨¡æäºä¼ æå¨çº§å«çå¹²æ°ï¼å¹¶ä¸ºé²æ£æ§åºåæµè¯æä¾äºææä¹çææã
*   <strong>SSIMææ éªè¯ï¼</strong> éè¿è®¡ç®æ¸æ´å¾åä¸é®æ¡å¾åä¹é´çå¹³åç»æç¸ä¼¼æ§ææ°ï¼SSIMï¼ï¼éåäºæç¥éçº§ãç»ææ¾ç¤ºï¼ä¸åé®æ¡ç±»ååä¸¥éç¨åº¦å¯¼è´äºæ¾èçSSIMä¸éï¼ä¾å¦ç°å°é®æ¡å¯¼è´SSIMä¸é0.43è³0.88ï¼æ°´æ¨¡ç³ä¸é0.28è³0.45ï¼åçä¸é0.34ï¼WoodScapeæ±¡å¢æ¨¡å¼ä¸é0.074ãè¿è¡¨æé®æ¡ææå°æ¨¡æäºè§è§è´¨éæå¤±ã
*   <strong>ä¿è¿é²æ£æ§ç ç©¶ï¼</strong> è¯¥æ°æ®éä¸ºç ç©¶é²æ£ä¼ æå¨èåãå¼¹æ§åæåèªå¨é©¾é©¶ä¸­å®å¨å³é®æç¥æä¾äºç»ä¸ãå¯å¤ç°çè¯ä¼°èµæºã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>åæé®æ¡çè¿ä¼¼æ§ï¼</strong> è®ºææç¡®æåºï¼è½ç¶é®æ¡æ¯åæçï¼å®ä»¬è¿ä¼¼äºçå®ä¸ççä¼ æå¨éçº§ï¼ä½ä¸è½å®å¨å¤å¶ã
*   <strong>æç«¯åæ°è®¾ç½®ï¼</strong> æäºæç«¯åæ°è®¾ç½®ï¼å¦éå¸¸é«çä¸¢å¼çï¼æ¨å¨ä½ä¸ºååæµè¯ï¼èéå¸åçé©¾é©¶åºæ¯ã
*   <strong>å­å¨éå¶ï¼</strong> ä¸ºäºé¿åè¿å¤§çå­å¨éæ±ï¼é·è¾¾åLiDARçé®æ¡æ°æ®æªé¢åçæå¹¶ååï¼èæ¯éè¿èæ¬æéçæã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>é²æ£ä¼ æå¨èåæ¨¡åå¼åï¼</strong> å©ç¨è¯¥æ°æ®éè®­ç»åæµè¯å¨é®æ¡æ¡ä»¶ä¸è¡¨ç°æ´ä½³çå¤ä¼ æå¨èåæ¨¡åã
*   <strong>å¼¹æ§åæåæéå®¹éï¼</strong> æ¢ç´¢ä¼ æå¨åä½åæéå®¹éç­ç¥ï¼ä»¥æé«èªå¨é©¾é©¶ç³»ç»çæ´ä½å¼¹æ§ã
*   <strong>å®å¨å³é®æç¥ï¼</strong> å¨åæ§æ¡ä»¶ä¸å¯¹æç¥æ¨¡åçé²æ£æ§è¿è¡å¯å¤ç°çæµè¯ï¼ä»¥æ»¡è¶³å®å¨å³é®åºç¨çéæ±ã
*   <strong>é®æ¡æç¥è®­ç»ï¼</strong> å©ç¨è¯¥æ°æ®éè¿è¡é®æ¡æç¥è®­ç»ï¼ä½¿æ¨¡åè½å¤æ´å¥½å°å¤ççå®ä¸çä¸­çä¼ æå¨éçº§ã
*   <strong>æ´çå®çé®æ¡æ¨¡æï¼</strong> è¿ä¸æ­¥ç ç©¶åå¼åæ´æ¥è¿çå®ä¸çä¼ æå¨éçº§çé®æ¡æ¨¡ææ¹æ³ã</p>
<hr />
<p>è¿ç¯è®ºæéè¿æä¾ä¸ä¸ªç¬ç¹ä¸åæ§çé®æ¡æ°æ®éï¼ä¸ºèªå¨é©¾é©¶é¢åå¨æ¶å£æ¡ä»¶ä¸çæç¥é²æ£æ§ç ç©¶å¥ å®äºéè¦åºç¡ï¼å·ææ¾èççè®ºåå®è·µæä¹ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this limitation, we introduce the Occluded nuScenes
Dataset, a novel extension of the widely used nuScenes benchmark.</li>
<li>For the
camera modality, we release both the full and mini versions with four types of
occlusions, two adapted from public implementations and two newly designed.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.18552v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.18552v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.18437v1'></a></p>
<h2 id="beyond-single-images-retrieval-self-augmented-unsupervised-camouflaged-object-detection"><a href="https://arxiv.org/abs/2510.18437v1">Beyond Single Images: Retrieval Self-Augmented Unsupervised Camouflaged Object Detection</a></h2>
<p><strong>Authors:</strong> Ji Du, Xin Wang, Fangwei Hao, Mingyang Yu, Chunyuan Chen, Jiesheng Wu, Bin Wang, Jing Xu, Ping Li</p>
<p><strong>Published:</strong> 2025-10-21</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>At the core of Camouflaged Object Detection (COD) lies segmenting objects
from their highly similar surroundings. Previous efforts navigate this
challenge primarily through image-level modeling or annotation-based
optimization. Despite advancing considerably, this commonplace practice hardly
taps valuable dataset-level contextual information or relies on laborious
annotations. In this paper, we propose RISE, a RetrIeval SElf-augmented
paradigm that exploits the entire training dataset to generate pseudo-labels
for single images, which could be used to train COD models. RISE begins by
constructing prototype libraries for environments and camouflaged objects using
training images (without ground truth), followed by K-Nearest Neighbor (KNN)
retrieval to generate pseudo-masks for each image based on these libraries. It
is important to recognize that using only training images without annotations
exerts a pronounced challenge in crafting high-quality prototype libraries. In
this light, we introduce a Clustering-then-Retrieval (CR) strategy, where
coarse masks are first generated through clustering, facilitating subsequent
histogram-based image filtering and cross-category retrieval to produce
high-confidence prototypes. In the KNN retrieval stage, to alleviate the effect
of artifacts in feature maps, we propose Multi-View KNN Retrieval (MVKR), which
integrates retrieval results from diverse views to produce more robust and
precise pseudo-masks. Extensive experiments demonstrate that RISE outperforms
state-of-the-art unsupervised and prompt-based methods. Code is available at
https://github.com/xiaohainku/RISE.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾å¯¹Ji Duç­äººæ°åçè®ºæâBeyond Single Images: Retrieval Self-Augmented Unsupervised Camouflaged Object Detectionâçå¨é¢æè¦ã</p>
<hr />
<p><strong>è®ºææè¦ï¼è¶è¶åå¾åï¼æ£ç´¢èªå¢å¼ºæ çç£ä¼ªè£ç®æ æ£æµ</strong></p>
<p><strong>1. è§£å³çä¸»è¦é®é¢æç ç©¶é®é¢</strong>
è¯¥è®ºæä¸»è¦è§£å³ä¼ªè£ç®æ æ£æµï¼CODï¼ä¸­çæ ¸å¿ææï¼å¦ä½ä»ä¸å¨å´ç¯å¢é«åº¦ç¸ä¼¼çèæ¯ä¸­åç¡®åå²åºä¼ªè£ç©ä½ãç°ææ¹æ³ä¸»è¦ä¾èµäºåå¾åå»ºæ¨¡æåºäºæ æ³¨çä¼åï¼ä½è¿äºæ¹æ³æªè½ååå©ç¨æ°æ®éå±é¢çä¸ä¸æä¿¡æ¯ï¼ä¸éå¸¸éè¦èè´¹å¤§éäººå·¥æ æ³¨ãå æ­¤ï¼è®ºææ¨å¨æ¢ç´¢ä¸ç§æ éæ æ³¨ãè½å¤ææå©ç¨æ°æ®éå¨å±ä¿¡æ¯æ¥çæé«è´¨éä¼ªè£ç©ä½ä¼ªæ ç­¾çæ çç£æ¹æ³ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®</strong>
è®ºææåºäºåä¸º <strong>RISE (RetrIeval SElf-augmented)</strong> çæ°èå¼ï¼å¶æ ¸å¿åæ°åæ¬ï¼</p>
<ul>
<li><strong>æ°æ®éå±é¢çä¸ä¸æå©ç¨ï¼</strong> ä¸ä»¥å¾ä¾èµåå¾åç¹å¾ç¸ä¼¼æ§çæ¹æ³ä¸åï¼RISEéè¿æå»ºåååºæ¥å©ç¨æ´ä¸ªè®­ç»æ°æ®éçå¨å±ä¿¡æ¯ï¼ä»èæ´å¥½å°åºåä¼ªè£ç©ä½ä¸èæ¯ã</li>
<li><strong>èç±»-ç¶å-æ£ç´¢ (Clustering-then-Retrieval, CR) ç­ç¥ï¼</strong> éå¯¹æ æ æ³¨ä¸æå»ºé«è´¨éåååºçææï¼CRç­ç¥é¦åéè¿è°±èç±»çæç²ç¥æ©ç ï¼åæ­¥åºåä¼ªè£ç©ä½åç¯å¢ãéåï¼éè¿åºäºç´æ¹å¾çå¾åè¿æ»¤åè·¨ç±»å«æ£ç´¢ï¼ä»å±é¨ç¹å¾ä¸­æååºé«ç½®ä¿¡åº¦çåæ¯åèæ¯ååãå¼å¾æ³¨æçæ¯ï¼åæ¯ååéæ©çæ¯ä¸å¨å±èæ¯ç¹å¾ç¸ä¼¼åº¦æä½çç¹å¾ï¼ä»¥å¢å¼ºåºååº¦ã</li>
<li><strong>å¤è§è§KNNæ£ç´¢ (Multi-View KNN Retrieval, MVKR)ï¼</strong> ä¸ºäºç¼è§£ç¹å¾å¾ä¸­ä¼ªå½±å¯¹æ£ç´¢ç»æçå½±åï¼MVKRéè¿æ´åæ¥èªä¸åè§è§çå¾åï¼éè¿ç¿»è½¬ãæè½¬ç­åæ¢çæï¼çæ£ç´¢ç»æï¼å¹¶éè¿æç¥¨æºå¶èåï¼çææ´é²æ£åç²¾ç¡®çä¼ªæ©ç ï¼é¿åäºæ¨¡åå¾®è°çéè¦ã</li>
<li><strong>é«æçä¼ªæ ç­¾çæï¼</strong> RISEè½å¤ä»¥å°æ¶çº§èéå¤©çº§çæ¶é´å®ææ´ä¸ªæ°æ®éçä¼ªæ ç­¾çæï¼ä¸GPUåå­ä½¿ç¨éè¿ä½äºåºäºPromptçæ¹æ³ã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹</strong>
*   <strong>è¶è¶ç°ææ çç£æ¹æ³ï¼</strong> å®éªç»æè¡¨æï¼RISEå¨CHAMELEONãCAMOãCOD10KåNC4Kç­å¤ä¸ªåºåæ°æ®éä¸ï¼æ§è½æ¾èä¼äºæåè¿çæ çç£ååºäºPromptçæ¹æ³ãå¨COD10Kæ°æ®éä¸ï¼RISEå¨E<script type="math/tex">åF</script>ææ ä¸è³å°æåäº8%å9%ã
*   <strong>æ°æ®éå±é¢ä¿¡æ¯çæææ§ï¼</strong> RISEçæ§è½æåè¯æäºååå©ç¨æ°æ®éå±é¢çè¯­ä¹ä¿¡æ¯å¯¹äºåºåé«åº¦ç¸ä¼¼çåæ¯åèæ¯è³å³éè¦ã
*   <strong>CRæ¨¡åçæææ§ï¼</strong> è·¨ç±»å«æ£ç´¢ååºäºç´æ¹å¾çå¾åè¿æ»¤æ¾èæåäºRISEçæ§è½ï¼å¹¶åå°äºåååºçè§æ¨¡ï¼æé«äºæ¨çæçã
*   <strong>MVKRçé²æ£æ§ï¼</strong> MVKRéè¿å¤è§è§èåï¼ææåè½»äºç¹å¾å¾ä¼ªå½±çå½±åï¼çæäºæ´ç¨³å¥çä¼ªæ©ç ã
*   <strong>å¯¹å°ç®æ çè¯å¥½å®ä½è½åï¼</strong> è®ºæçå®æ§æ¯è¾å±ç¤ºäºRISEå¨å¤æç¯å¢ä¸­åç¡®å®ä½ååå²ä¼ªè£ç©ä½ï¼åæ¬å°ç®æ ï¼çè½åã
*   <strong>è¶åæ°åæ°æ®éè§æ¨¡çé²æ£æ§ï¼</strong> RISEå¯¹top-Kè¶åæ°ä¸ææï¼å¹¶ä¸å³ä½¿åªä½¿ç¨10%çè®­ç»å¾åæå»ºåååºï¼ä¹è½ä¿æä¸è´ä¸é²æ£çæ§è½ï¼è¿å¸æ¾äºå¶å¨æ°æ®æéåºæ¯ä¸çæçåæææ§ã
*   <strong>å³æå³ç¨ç¹æ§ï¼</strong> RISEçæ¹æ³å¯ä»¥ä¸ä¸åçèç±»æ¹æ³ï¼å¦KMeansãGMMãHCAï¼ä»¥åå¶ä»æ çç£æ¹æ³éæï¼å¹¶æ¾èæåå®ä»¬çæ§è½ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§</strong>
è®ºæä¸­æ²¡ææç¡®æåRISEæ¹æ³çå·ä½å±éæ§ï¼ä½å¯ä»¥ä»å¶è®¾è®¡åå®éªä¸­æ¨æ­åºä¸äºæ½å¨çæ¹é¢ï¼
*   <strong>å¯¹åºç¡æ¨¡åçä¾èµï¼</strong> RISEä¾èµäºé¢è®­ç»çèªçç£æ¨¡åï¼å¦DINOv2ï¼æ¥æåç¹å¾ãè¿äºåºç¡æ¨¡åçæ§è½åæ³åè½åä¼ç´æ¥å½±åRISEçä¼ªæ ç­¾è´¨éãå¦æåºç¡æ¨¡åå¨æäºç¹å®ä¼ªè£åºæ¯ä¸è¡¨ç°ä¸ä½³ï¼RISEçæ§è½ä¹å¯è½åéã
*   <strong>èç±»åååéæ©çæææ§ï¼</strong> å°½ç®¡è®ºææåºäºCRç­ç¥æ¥æé«ååè´¨éï¼ä½èç±»è¿ç¨ï¼ç¹å«æ¯ç²ç¥æ©ç ççæï¼ä»å¯è½å¼å¥åªå£°ï¼ä»èå½±ååååºçåç¡®æ§ãååéæ©ï¼ä¾å¦éæ©ä¸èæ¯æä¸ç¸ä¼¼çåæ¯ç¹å¾ï¼çæææ§ä¹ä¾èµäºç¹å¾ç©ºé´ä¸­åºååº¦çå­å¨ã
*   <strong>è®¡ç®ææ¬ï¼</strong> å°½ç®¡RISEå¨ä¼ªæ ç­¾çææ¶é´ä¸ä¼äºåºäºPromptçæ¹æ³ï¼ä½æå»ºåååºåè¿è¡KNNæ£ç´¢ï¼å°¤å¶æ¯å¨å¤§è§æ¨¡æ°æ®éä¸ï¼ä»ç¶éè¦ä¸å®çè®¡ç®èµæºåæ¶é´ãFAISSåºçä½¿ç¨ç¼è§£äºæ£ç´¢æçé®é¢ï¼ä½æ´ä½æµç¨ä»éä¸å®å¼éã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹å</strong>
*   <strong>æ´åè¿çç¹å¾æåå¨ï¼</strong> æ¢ç´¢ä½¿ç¨æ´åè¿çèªçç£æåºç¡æ¨¡åä½ä¸ºç¹å¾æåå¨ï¼ä»¥è·åæ´å·åºååº¦åé²æ£æ§çç¹å¾è¡¨ç¤ºï¼ä»èè¿ä¸æ­¥æåä¼ªæ ç­¾è´¨éã
*   <strong>èªéåºååæ´æ°æºå¶ï¼</strong> ç ç©¶å¨ææèªéåºçååæ´æ°æºå¶ï¼ä»¥åºå¯¹æ°æ®éä¸­çç±»å«ä¸å¹³è¡¡ææ°åºç°çä¼ªè£æ¨¡å¼ï¼ä½¿åååºè½å¤éæ¶é´æ¼åå¹¶ä¿æé«ç½®ä¿¡åº¦ã
*   <strong>å¤æ¨¡æä¿¡æ¯èåï¼</strong> ç»åææ¬ãæ·±åº¦æå¶ä»æ¨¡æä¿¡æ¯ï¼è¿ä¸æ­¥å¢å¼ºä¼ªè£ç©ä½åç¯å¢ä¹é´çåºåè½åï¼å°¤å¶æ¯å¨è§è§ä¿¡æ¯é«åº¦ç¸ä¼¼çæç«¯ä¼ªè£åºæ¯ä¸ã
*   <strong>ç«¯å°ç«¯å­¦ä¹ ï¼</strong> æ¢ç´¢å°RISEçä¼ªæ ç­¾çæè¿ç¨ä¸CODæ¨¡åè®­ç»ç¸ç»åçç«¯å°ç«¯å­¦ä¹ æ¡æ¶ï¼å¯è½éè¿è¿­ä»£ä¼åä¼ªæ ç­¾åæ¨¡åæ§è½ã
*   <strong>æ³åè½åè¯ä¼°ï¼</strong> å¯¹RISEå¨æ´å¹¿æ³ãæ´å¤æ ·åçä¼ªè£åºæ¯ï¼ä¾å¦ä¸åç±»åçä¼ªè£ãä¸åç¯å¢ï¼ä¸çæ³åè½åè¿è¡æ·±å¥è¯ä¼°ã
*   <strong>çè®ºåæï¼</strong> å¯¹RISEä¸­æ°æ®éå±é¢ä¸ä¸æä¿¡æ¯å©ç¨çæææ§è¿è¡æ´æ·±å¥ççè®ºåæï¼ä»¥çè§£å¶å¨åºåé«åº¦ç¸ä¼¼åæ¯åèæ¯æ¹é¢çåå¨æºå¶ã</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this paper, we propose RISE, a RetrIeval SElf-augmented
paradigm that exploits the entire training dataset to generate pseudo-labels
for single images, which could be used to train COD models.</li>
<li>In
this light, we introduce a Clustering-then-Retrieval (CR) strategy, where
coarse masks are first generated through clustering, facilitating subsequent
histogram-based image filtering and cross-category retrieval to produce
high-confidence prototypes.</li>
<li>In the KNN retrieval stage, to alleviate the effect
of artifacts in feature maps, we propose Multi-View KNN Retrieval (MVKR), which
integrates retrieval results from diverse views to produce more robust and
precise pseudo-masks.</li>
<li>Extensive experiments demonstrate that RISE outperforms
state-of-the-art unsupervised and prompt-based methods.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.18437v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.18437v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.18377v1'></a></p>
<h2 id="cross-modal-scene-semantic-alignment-for-image-complexity-assessment"><a href="https://arxiv.org/abs/2510.18377v1">Cross-Modal Scene Semantic Alignment for Image Complexity Assessment</a></h2>
<p><strong>Authors:</strong> Yuqing Luo, Yixiao Li, Jiang Liu, Jun Fu, Hadi Amirpour, Guanghui Yue, Baoquan Zhao, Padraig Corcoran, Hantao Liu, Wei Zhou</p>
<p><strong>Published:</strong> 2025-10-21</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Image complexity assessment (ICA) is a challenging task in perceptual
evaluation due to the subjective nature of human perception and the inherent
semantic diversity in real-world images. Existing ICA methods predominantly
rely on hand-crafted or shallow convolutional neural network-based features of
a single visual modality, which are insufficient to fully capture the perceived
representations closely related to image complexity. Recently, cross-modal
scene semantic information has been shown to play a crucial role in various
computer vision tasks, particularly those involving perceptual understanding.
However, the exploration of cross-modal scene semantic information in the
context of ICA remains unaddressed. Therefore, in this paper, we propose a
novel ICA method called Cross-Modal Scene Semantic Alignment (CM-SSA), which
leverages scene semantic alignment from a cross-modal perspective to enhance
ICA performance, enabling complexity predictions to be more consistent with
subjective human perception. Specifically, the proposed CM-SSA consists of a
complexity regression branch and a scene semantic alignment branch. The
complexity regression branch estimates image complexity levels under the
guidance of the scene semantic alignment branch, while the scene semantic
alignment branch is used to align images with corresponding text prompts that
convey rich scene semantic information by pair-wise learning. Extensive
experiments on several ICA datasets demonstrate that the proposed CM-SSA
significantly outperforms state-of-the-art approaches. Codes are available at
https://github.com/XQ2K/First-Cross-Model-ICA.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Yuqing Luoç­äººæ°åçè®ºæâCross-Modal Scene Semantic Alignment for Image Complexity Assessmentâçå¨é¢æè¦ã</p>
<hr />
<p><strong>è®ºææè¦ï¼è·¨æ¨¡æåºæ¯è¯­ä¹å¯¹é½ç¨äºå¾åå¤æåº¦è¯ä¼°</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
å¾åå¤æåº¦è¯ä¼°ï¼ICAï¼æ¯ä¸ä¸ªå·ææææ§çä»»å¡ï¼å ä¸ºå®æ¶åäººç±»æç¥çä¸»è§æ§ä»¥åçå®ä¸çå¾ååºæçè¯­ä¹å¤æ ·æ§ãç°æçICAæ¹æ³ä¸»è¦ä¾èµäºåä¸è§è§æ¨¡æçæå·¥ç¹å¾ææµå±å·ç§¯ç¥ç»ç½ç»ç¹å¾ï¼è¿äºç¹å¾ä¸è¶³ä»¥ååææä¸å¾åå¤æåº¦å¯åç¸å³çæç¥è¡¨ç¤ºãå°½ç®¡è·¨æ¨¡æåºæ¯è¯­ä¹ä¿¡æ¯å¨åç§è®¡ç®æºè§è§ä»»å¡ï¼ç¹å«æ¯æ¶åæç¥çè§£çä»»å¡ï¼ä¸­å·²è¢«è¯æè³å³éè¦ï¼ä½å¨ICAèæ¯ä¸ï¼å¯¹è·¨æ¨¡æåºæ¯è¯­ä¹ä¿¡æ¯çæ¢ç´¢ä»æªå¾å°è§£å³ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
ä¸ºäºè§£å³ä¸è¿°é®é¢ï¼æ¬ææåºäºä¸ç§æ°é¢çICAæ¹æ³ï¼ç§°ä¸º<strong>è·¨æ¨¡æåºæ¯è¯­ä¹å¯¹é½ï¼CM-SSAï¼</strong>ãå¶æ ¸å¿åæ°å¨äºï¼
*   <strong>é¦æ¬¡å¼å¥è·¨æ¨¡ææ¹æ³è¿è¡ICAï¼</strong> CM-SSAæ¯ç¬¬ä¸ä¸ªå©ç¨è·¨æ¨¡æåºæ¯è¯­ä¹å¯¹é½æ¥å¢å¼ºICAæ§è½çæ¹æ³ï¼æ¨å¨ä½¿å¤æåº¦é¢æµæ´ç¬¦åäººç±»ä¸»è§æç¥ã
*   <strong>ååæ¯æ¡æ¶ï¼</strong> CM-SSAåå«ä¸¤ä¸ªä¸»è¦åæ¯ï¼
    *   <strong>å¤æåº¦åå½åæ¯ï¼</strong> è¯¥åæ¯ç´æ¥é¢æµå¾åå¤æåº¦çº§å«ãå®å©ç¨å¾å-æç¤ºå¯¹å­¦ä¹ ï¼å¶ä¸­æç¤ºæ¯ç²ç²åº¦çå¤æåº¦ç±»å«ï¼å¦âé«å¤æåº¦âãâä¸­ç­å¤æåº¦âç­ï¼ã
    *   <strong>åºæ¯è¯­ä¹å¯¹é½åæ¯ï¼</strong> è¯¥åæ¯éè¿å°å¾åä¸ç±InstructBLIPç­è§è§-è¯­è¨æ¨¡åèªå¨çæçææ¬æç¤ºï¼ä¼ è¾¾ä¸°å¯çåºæ¯è¯­ä¹ä¿¡æ¯ï¼å¦è¿å¨ãæåãæç»ªãç©ºé´ä½ç½®åæ°éï¼è¿è¡å¯¹é½ï¼æ¥ç²¾ç¼å¾åç¹å¾ãè¿ç§å¯¹é½éè¿æå¯¹å­¦ä¹ å®ç°ï¼æ¨å¨å°å¾åç¹å¾ä¸é«å±ææ¬é©±å¨çåºæ¯è¯­ä¹å¯¹é½ã
*   <strong>å©ç¨é¢è®­ç»è§è§-è¯­è¨æ¨¡åï¼</strong> éè¿InstructBLIPçæåºæ¯è¯­ä¹æè¿°ï¼è§£å³äºç°æICAæ°æ®éç¼ºä¹æ­¤ç±»è¯­ä¹æ æ³¨çé®é¢ï¼ä»èè½å¤å°è·¨æ¨¡æä¿¡æ¯æ´åå°æ¨¡åä¸­ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>åè¶çæ§è½ï¼</strong> å¨IC9600ãVISC-CåSAVOIASç­å¤ä¸ªICAæ°æ®éä¸è¿è¡çå¹¿æ³å®éªè¡¨æï¼ææåºçCM-SSAæ¾èä¼äºç°ææåè¿çæ¹æ³ï¼åæ¬æå·¥ç¹å¾æ¹æ³ãæ·±åº¦å­¦ä¹ ICAæ¨¡åä»¥ååè¿çå¾åè´¨éè¯ä¼°ï¼IQAï¼æ¨¡åã
*   <strong>ä¸è´æ§ä¸äººç±»æç¥ï¼</strong> CM-SSAéè¿è·¨æ¨¡æåºæ¯è¯­ä¹å¯¹é½ï¼ä½¿å¤æåº¦é¢æµä¸äººç±»ä¸»è§æç¥æ´å ä¸è´ã
*   <strong>æ³åè½åï¼</strong> è·¨æ°æ®ééªè¯ç»ææ¾ç¤ºï¼CM-SSAå¨å¤§å¤æ°è·¨æ°æ®éåºæ¯ä¸­è¡¨ç°åºæ´å¼ºçé²æ£æ§ï¼å°¤å¶æ¯å¨IC9600ä¸è®­ç»å¹¶å¨VISC-Cä¸æµè¯æ¶ï¼åå¾äºæé«çSRCCåPLCCå¼ã
*   <strong>æ¶èç ç©¶ï¼</strong> æ¶èç ç©¶éªè¯äºæ¯ä¸ªåæ¯çæææ§ãç»æè¡¨æï¼å¤æåº¦åå½åæ¯å¨é¢æµä¸­èµ·çéè¦ä½ç¨ï¼èåºæ¯è¯­ä¹å¯¹é½åæ¯åéè¿ç²¾ç¼å¾åç¹å¾æ¥è¡¥åå¶ä½ç¨ï¼ä¸¤èç»ååå¾äºæä½³æ§è½ãæç¤ºçº§å«åé¿åº¦çåæä¹æ­ç¤ºäºå¶å¯¹æ¨¡åæ§è½çå½±åã</p>
<p><strong>4. è®ºæä¸­æå°çå±éæ§ï¼</strong>
*   <strong>æ°æ®ééå¶ï¼</strong> è®ºææå°ï¼CM-SSAå¨VISC-Cæ°æ®éä¸è®­ç»å¹¶å¨å¶ä»æ°æ®éä¸æµè¯æ¶ï¼æ³åæ§è½ææä¸éãè¿å¯è½æ¯ç±äºVISC-Cæ°æ®éçè§æ¨¡åå¤æ ·æ§æéï¼ç¹å«æ¯èèå°CM-SSAä¸­ç¸å¯¹è¾å¤çåæ°ä½¿å¶æ´å®¹æåå°å°æ°æ®éæ³åå°å¤§æ°æ®éçå½±åã
*   <strong>è¯­ä¹æè¿°çå®ç¾å¯¹é½åè®¾ï¼</strong> æå¤±è®¡ç®åè®¾InstructBLIPçæçææ¬æè¿°ä¸ç¸åºå¾åå®ç¾å¯¹é½ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
è®ºæä¸­æªæç¡®æåºæªæ¥çç ç©¶æ¹åï¼ä½æ ¹æ®å¶åå®¹ï¼å¯ä»¥æ¨æ­åºä»¥ä¸å ç¹ï¼
*   <strong>æ´é²æ£çè¯­ä¹æè¿°çæï¼</strong> æ¢ç´¢æ´åè¿ææ´é²æ£çè§è§-è¯­è¨æ¨¡åï¼ä»¥çææ´åç¡®ãæ´ç»è´çåºæ¯è¯­ä¹æè¿°ï¼ä»èè¿ä¸æ­¥æé«å¯¹é½åæ¯çæ§è½ã
*   <strong>èªéåºæéè°æ´ï¼</strong> è¿ä¸æ­¥ç ç©¶è¶åæ°Î±åÎ²çèªéåºè°æ´æºå¶ï¼ä»¥æ´å¥½å°å¹³è¡¡å¤æåº¦åå½åæ¯ååºæ¯è¯­ä¹å¯¹é½åæ¯çè´¡ç®ï¼å°¤å¶æ¯å¨ä¸åæ°æ®éæå¤æåº¦åå¸ä¸ã
*   <strong>å¤æ¨¡æèåç­ç¥ï¼</strong> æ¢ç´¢é¤äºå½åæå¯¹å­¦ä¹ ä¹å¤çæ´å¤æçå¤æ¨¡æèåç­ç¥ï¼ä»¥æ´æ·±å¥å°æ´åè§è§åææ¬ä¿¡æ¯ã
*   <strong>è§£éæ§ç ç©¶ï¼</strong> æ·±å¥ç ç©¶CM-SSAæ¨¡ååé¨ï¼ä»¥æ´å¥½å°çè§£åºæ¯è¯­ä¹ä¿¡æ¯å¦ä½å·ä½å½±åå¾åå¤æåº¦é¢æµï¼ä»èæé«æ¨¡åçå¯è§£éæ§ã
*   <strong>æ´å¹¿æ³çåºç¨ï¼</strong> å°CM-SSAçè·¨æ¨¡æå¯¹é½ææ³åºç¨äºå¶ä»éè¦ä¸»è§æç¥è¯ä¼°çè®¡ç®æºè§è§ä»»å¡ä¸­ã</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>Therefore, in this paper, we propose a
novel ICA method called Cross-Modal Scene Semantic Alignment (CM-SSA), which
leverages scene semantic alignment from a cross-modal perspective to enhance
ICA performance, enabling complexity predictions to be more consistent with
subjective human perception.</li>
<li>Extensive
experiments on several ICA datasets demonstrate that the proposed CM-SSA
significantly outperforms state-of-the-art approaches.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.18377v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.18377v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.18353v1'></a></p>
<h2 id="ranking-based-preference-optimization-for-diffusion-models-from-implicit-user-feedback"><a href="https://arxiv.org/abs/2510.18353v1">Ranking-based Preference Optimization for Diffusion Models from Implicit User Feedback</a></h2>
<p><strong>Authors:</strong> Yi-Lun Wu, Bo-Kai Ruan, Chiang Tseng, Hong-Han Shuai</p>
<p><strong>Published:</strong> 2025-10-21</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Direct preference optimization (DPO) methods have shown strong potential in
aligning text-to-image diffusion models with human preferences by training on
paired comparisons. These methods improve training stability by avoiding the
REINFORCE algorithm but still struggle with challenges such as accurately
estimating image probabilities due to the non-linear nature of the sigmoid
function and the limited diversity of offline datasets. In this paper, we
introduce Diffusion Denoising Ranking Optimization (Diffusion-DRO), a new
preference learning framework grounded in inverse reinforcement learning.
Diffusion-DRO removes the dependency on a reward model by casting preference
learning as a ranking problem, thereby simplifying the training objective into
a denoising formulation and overcoming the non-linear estimation issues found
in prior methods. Moreover, Diffusion-DRO uniquely integrates offline expert
demonstrations with online policy-generated negative samples, enabling it to
effectively capture human preferences while addressing the limitations of
offline data. Comprehensive experiments show that Diffusion-DRO delivers
improved generation quality across a range of challenging and unseen prompts,
outperforming state-of-the-art baselines in both both quantitative metrics and
user studies. Our source code and pre-trained models are available at
https://github.com/basiclab/DiffusionDRO.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°å¯¹è¿ç¯è®ºææè¦è¿è¡æ·±å¥åæã</p>
<hr />
<p><strong>è®ºææè¦åæï¼Ranking-based Preference Optimization for Diffusion Models from Implicit User Feedback</strong></p>
<p><strong>1. è®ºæä¸»è¦è´¡ç®çç®æ´æ»ç» (2-3 å¥è¯)</strong></p>
<p>è¿ç¯è®ºææåºäºä¸ç§åä¸º Diffusion Denoising Ranking Optimization (Diffusion-DRO) çæ°ååå¥½å­¦ä¹ æ¡æ¶ï¼ç¨äºå°ææ¬å°å¾åæ©æ£æ¨¡åä¸äººç±»åå¥½å¯¹é½ãå®éè¿å°åå¥½å­¦ä¹ éæ°å®ä¹ä¸ºæåºé®é¢ï¼å¹¶å°å¶è®­ç»ç®æ ç®åä¸ºå»åªå¬å¼ï¼ä»èé¿åäºå¯¹å¥å±æ¨¡åçä¾èµä»¥åç°ææ¹æ³ä¸­éçº¿æ§ä¼°è®¡çææãDiffusion-DRO ç¬ç¹å°ç»åäºç¦»çº¿ä¸å®¶æ¼ç¤ºåå¨çº¿ç­ç¥çæçè´æ ·æ¬ï¼æ¾èæåäºçæè´¨éã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºæ¹æ³</strong></p>
<ul>
<li><strong>å°åå¥½å­¦ä¹ è½¬åä¸ºæåºé®é¢åå»åªå¬å¼ï¼</strong> è¿æ¯æ ¸å¿åæ°ãä¼ ç»ç DPO æ¹æ³ä¾èµäºä¼°è®¡å¾åæ¦çï¼è¿å¨æ©æ£æ¨¡åä¸­ç±äº sigmoid å½æ°çéçº¿æ§ç¹æ§èåå¾å°é¾ãDiffusion-DRO éè¿å°åå¥½å­¦ä¹ éæ°æå»ºä¸ºæåºä»»å¡ï¼å¹¶å°å¶ç®æ ç®åä¸ºå»åªè¿ç¨ï¼å®å¨è§é¿äºå¯¹å¥å±æ¨¡åçä¾èµåå¤æçæ¦çä¼°è®¡é®é¢ãè¿ä½¿å¾è®­ç»æ´å ç¨³å®åé«æã</li>
<li><strong>ç»åç¦»çº¿ä¸å®¶æ¼ç¤ºä¸å¨çº¿ç­ç¥çæçè´æ ·æ¬ï¼</strong> ç°ææ¹æ³å¸¸åéäºç¦»çº¿æ°æ®éçå¤æ ·æ§ãDiffusion-DRO éè¿æ´åé«è´¨éçç¦»çº¿âä¸å®¶âæ ·æ¬ï¼æ­£æ ·æ¬ï¼åæ¨¡åèªèº«å¨çº¿çæçâè´âæ ·æ¬ï¼ææå°æ©å±äºè®­ç»æ°æ®ï¼å¹¶è½æ´å¥½å°ææäººç±»åå¥½ï¼åæ¶åæäºç¦»çº¿æ°æ®å¤æ ·æ§ä¸è¶³çéå¶ã</li>
<li><strong>åºäºéå¼ºåå­¦ä¹  (Inverse Reinforcement Learning, IRL) çåºç¡ï¼</strong> æè¦æåºè¯¥æ¡æ¶æ¤æ ¹äº IRLï¼è¿æå³çå®è¯å¾ä»è§å¯å°çåå¥½ä¸­æ¨æ­åºæ½å¨çå¥å±å½æ°ï¼ä½åå·§å¦å°é¿åäºæ¾å¼å°å­¦ä¹ è¿ä¸ªå¥å±å½æ°ï¼èæ¯ç´æ¥ä¼åæåºã</li>
</ul>
<p><strong>3. å¯¹é¢åæ½å¨å½±å</strong></p>
<ul>
<li><strong>æåæ©æ£æ¨¡åçäººç±»åå¥½å¯¹é½è½åï¼</strong> Diffusion-DRO æä¾äºä¸ç§æ´ç¨³å®ãæ´ææçæ¹æ³æ¥å°æ©æ£æ¨¡åä¸äººç±»åå¥½å¯¹é½ï¼è¿å°ç´æ¥å¯¼è´æ´é«è´¨éãæ´ç¬¦åç¨æ·ææçå¾åçæã</li>
<li><strong>æ¨å¨åå¥½å­¦ä¹ å¨çææ¨¡åä¸­çåå±ï¼</strong> éè¿è§£å³ DPO æ¹æ³å¨æ©æ£æ¨¡åä¸­éå°çå³é®ææï¼å¦æ¦çä¼°è®¡åæ°æ®å¤æ ·æ§ï¼ï¼è¯¥å·¥ä½ä¸ºåå¥½å­¦ä¹ å¨æ´å¹¿æ³ççææ¨¡åä¸­çåºç¨å¼è¾äºæ°çéå¾ã</li>
<li><strong>ç®åè®­ç»æµç¨ï¼</strong> ç§»é¤å¯¹å¥å±æ¨¡åçä¾èµåå¤æçéçº¿æ§ä¼°è®¡ï¼å°ä½¿åå¥½ä¼åè¿ç¨æ´æäºå®ç°åæ©å±ã</li>
<li><strong>ä¿è¿æ´å·åé æ§åå®ç¨æ§ç AI èºæ¯åè®¾è®¡å·¥å·ï¼</strong> æ´å¥½çåå¥½å¯¹é½æå³çç¨æ·å¯ä»¥æ´è½»æ¾å°è·å¾ä»ä»¬æ³è¦çå¾åï¼ä»èä½¿ AI æä¸ºæ´å¼ºå¤§çåæè¾å©å·¥å·ã</li>
</ul>
<p><strong>4. å¯è½åçäºè¿é¡¹ç ç©¶çç¸å³é¢åæåºç¨</strong></p>
<ul>
<li><strong>ææ¬å°å¾åçæ (Text-to-Image Generation)ï¼</strong> è¿æ¯æç´æ¥çåºç¨ï¼å°æ¾èæ¹å Stable DiffusionãDALL-E ç­æ¨¡åççæè´¨éåç¨æ·ä½éªã</li>
<li><strong>å¾åç¼è¾åé£æ ¼è¿ç§»ï¼</strong> ç¨æ·å¯ä»¥æ´ç²¾ç¡®å°æå¯¼æ¨¡åè¿è¡å¾åç¼è¾æé£æ ¼è½¬æ¢ï¼ä»¥ç¬¦åå¶å®¡ç¾åå¥½ã</li>
<li><strong>ä¸ªæ§ååå®¹çæï¼</strong> æ ¹æ®ç¨æ·çéå¼ææ¾å¼åé¦ï¼çææ´ç¬¦åä¸ªäººåå³çå¾åãè®¾è®¡æèºæ¯ä½åã</li>
<li><strong>å¤æ¨¡æåå®¹çæï¼</strong> è¿ç§åå¥½ä¼åæ¹æ³å¯è½æ¨å¹¿å°å¶ä»å¤æ¨¡æçæä»»å¡ï¼ä¾å¦ææ¬å°è§é¢ãææ¬å°3Dæ¨¡åç­ã</li>
<li><strong>å¼ºåå­¦ä¹ åéå¼ºåå­¦ä¹ ï¼</strong> è¯¥æ¹æ³å¨ IRL æ¡æ¶ä¸é¿åæ¾å¼å¥å±æ¨¡åå­¦ä¹ çç­ç¥ï¼å¯è½ä¸ºå¶ä» IRL åºç¨æä¾æ°çæè·¯ã</li>
</ul>
<p><strong>5. å¯ä»¥ä»æè¦ä¸­æ¨æ­åºçä»»ä½å±éæ§</strong></p>
<ul>
<li><strong>âéå¼ç¨æ·åé¦âçå®ä¹åè·åï¼</strong> æè¦ä¸­æå°âä»éå¼ç¨æ·åé¦ä¸­è¿è¡æåºåå¥½ä¼åâï¼ä½æ²¡æè¯¦ç»è¯´æå¦ä½è·åæå»ºæ¨¡è¿ç§éå¼åé¦ãè¿å¯è½æ¯ä¸ä¸ªå®éé¨ç½²ä¸­çææï¼ä¾å¦ï¼å®æ¯å¦éè¦ç¨æ·ç¹å»ãåçæ¶é´æå¶ä»è¡ä¸ºæ°æ®ï¼</li>
<li><strong>è®¡ç®ææ¬ï¼</strong> è½ç¶é¿åäºå¥å±æ¨¡åï¼ä½ç»åâå¨çº¿ç­ç¥çæçè´æ ·æ¬âå¯è½æå³çå¨è®­ç»è¿ç¨ä¸­éè¦è¿è¡é¢å¤çæ¨çæçææ­¥éª¤ï¼è¿å¯è½ä¼å¢å è®¡ç®ææ¬ï¼å°¤å¶æ¯å¨å¤§è§æ¨¡æ¨¡åä¸ã</li>
<li><strong>æ³åè½åï¼</strong> å°½ç®¡å®éªè¡¨æå¨âå·ææææ§åæªè§è¿çæç¤ºâä¸è¡¨ç°è¯å¥½ï¼ä½å¶å¨æç«¯ç½è§æé«åº¦ä¸ä¸åé¢åçæ³åè½åä»éè¿ä¸æ­¥éªè¯ã</li>
<li><strong>âå»åªå¬å¼âçå·ä½å½¢å¼ï¼</strong> æè¦æ²¡æè¯¦ç»è¯´æè¿ä¸ªå»åªå¬å¼çå·ä½æ°å­¦å½¢å¼ï¼è¿å¯è½éèäºä¸äºæ½å¨çå¤ææ§æåè®¾ã</li>
<li><strong>å¯¹âä¸å®¶æ¼ç¤ºâçä¾èµï¼</strong> ç¦»çº¿ä¸å®¶æ¼ç¤ºçè´¨éåå¤æ ·æ§ä»ç¶æ¯æ¨¡åæ§è½çå³é®å ç´ ãå¦æé«è´¨éçä¸å®¶æ°æ®é¾ä»¥è·åï¼å¯è½ä¼éå¶å¶ææã</li>
</ul>
<hr />
<p>æ»èè¨ä¹ï¼è¿ç¯è®ºæéè¿æåº Diffusion-DROï¼ä¸ºæ©æ£æ¨¡åçäººç±»åå¥½å¯¹é½æä¾äºä¸ä¸ªæ°é¢ä¸æåæ¯çè§£å³æ¹æ¡ãå®å·§å¦å°è§é¿äºç°æ DPO æ¹æ³ççç¹ï¼å¹¶ææå¨ææ¬å°å¾åçæé¢åå¸¦æ¥æ¾èçè´¨éæååæ´å¹¿æ³çåºç¨ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this paper, we
introduce Diffusion Denoising Ranking Optimization (Diffusion-DRO), a new
preference learning framework grounded in inverse reinforcement learning.</li>
<li>Comprehensive experiments show that Diffusion-DRO delivers
improved generation quality across a range of challenging and unseen prompts,
outperforming state-of-the-art baselines in both both quantitative metrics and
user studies.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.18353v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.18353v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.18341v1'></a></p>
<h2 id="vise-a-systematic-approach-to-vision-only-street-view-extrapolation"><a href="https://arxiv.org/abs/2510.18341v1">ViSE: A Systematic Approach to Vision-Only Street-View Extrapolation</a></h2>
<p><strong>Authors:</strong> Kaiyuan Tan, Yingying Shen, Haiyang Sun, Bing Wang, Guang Chen, Hangjun Ye</p>
<p><strong>Published:</strong> 2025-10-21</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Realistic view extrapolation is critical for closed-loop simulation in
autonomous driving, yet it remains a significant challenge for current Novel
View Synthesis (NVS) methods, which often produce distorted and inconsistent
images beyond the original trajectory. This report presents our winning
solution which ctook first place in the RealADSim Workshop NVS track at ICCV
2025. To address the core challenges of street view extrapolation, we introduce
a comprehensive four-stage pipeline. First, we employ a data-driven
initialization strategy to generate a robust pseudo-LiDAR point cloud, avoiding
local minima. Second, we inject strong geometric priors by modeling the road
surface with a novel dimension-reduced SDF termed 2D-SDF. Third, we leverage a
generative prior to create pseudo ground truth for extrapolated viewpoints,
providing auxilary supervision. Finally, a data-driven adaptation network
removes time-specific artifacts. On the RealADSim-NVS benchmark, our method
achieves a final score of 0.441, ranking first among all participants.</p>
<p><strong>Analysis:</strong></p>
<p>ä»¥ä¸æ¯å¯¹Kaiyuan Tanç­äººæ°åçè®ºæâViSE: A Systematic Approach to Vision-Only Street-View Extrapolationâçå¨é¢æè¦ï¼</p>
<p><strong>1. è®ºæè§£å³çä¸»è¦é®é¢æç ç©¶é®é¢</strong></p>
<p>è¯¥è®ºææ¨å¨è§£å³èªå¨é©¾é©¶é­ç¯ä»¿çä¸­è¡æ¯è§å¾å¤æ¨çå³é®ææãç°æçæ°é¢è§å¾åæï¼NVSï¼æ¹æ³å¨è¶åºåå§è½¨è¿¹çè§å¾å¤æ¨æ¶ï¼å¾å¾ä¼çææ­æ²ä¸ä¸ä¸è´çå¾åãè¿éå¶äºNVSå¨æå»ºäº¤äºå¼ã4Dé©¾é©¶ç¯å¢ä¸­çåºç¨ï¼å ä¸ºé©¾é©¶æ¥å¿åºæçç¨çæ§ä½¿å¾å¤§å¤æ°NVSæ¹æ³æé¿è§å¾æå¼ï¼ä½å¨è§å¾å¤æ¨æ¹é¢è¡¨ç°ä¸ä½³ï¼å¯¼è´å ä½å¤±çåçº¹ççå®æå´©æºã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®</strong></p>
<p>ViSEæåºäºä¸ç§å¨é¢çåé¶æ®µç®¡éï¼ç³»ç»å°è§£å³äºè¡æ¯è§å¾å¤æ¨çæ ¸å¿ææï¼</p>
<ul>
<li><strong>é²æ£çæ LiDARåå§åç­ç¥ï¼</strong> éå¯¹LiDARæ°æ®ç¼ºå¤±å¯¼è´3Dé«æ¯åå§åå°é¾çé®é¢ï¼ViSEéç¨äºä¸ç§æ°æ®é©±å¨çåå§åç­ç¥ï¼çæåºäºè§è§çä¼ªLiDARç¹äºãè¿æä¾äºå¼ºå¤§çå ä½åéªï¼é¿åäºå±é¨æå°å¼ï¼å¹¶è½æ´å¿«å°æ¶æ3DGSï¼æ¢å¤æ´ç²¾ç»çç»èï¼é²æ­¢å¤æ¨è¿ç¨ä¸­åºç°ä¸åççå ä½å¤±çã</li>
<li><strong>å ä½æç¥3Dåºæ¯éå»ºï¼2D-SDFï¼ï¼</strong> ä¸ºäºåç¡®å»ºæ¨¡è·¯é¢ï¼åå«å³é®è§è§åç´ å¦è·¯æ åè½¦éçº¿ï¼ï¼è®ºæå¼å¥äºä¸ç§æ°é¢çéç»´2Dç¬¦å·è·ç¦»å½æ°ï¼2D-SDFï¼ãè¯¥æ¹æ³å¼ºå¶è·¯é¢å·æå±é¨å¹³é¢åéªåå¨å±å¹³æ»çå¡åº¦è¿æ¸¡ï¼ä»èå¨å¤§çè§ç¹ååä¸æé«å ä½ä¸è´æ§ï¼å¹¶æé«äºä¼åæçã</li>
<li><strong>è¿­ä»£ä¼ªçå¼çæï¼</strong> éå¯¹æªè§æµåºåï¼å¦æ¤è¢«ãè·¯ç¼ãå»ºç­ç©ï¼ç¼ºä¹éç¨å ä½åéªå¯¼è´å¤æ¨æ¶åºç°ä¸¥éå¤±çææµ®å¨ä¼ªå½±çé®é¢ï¼ViSEå©ç¨é¢è®­ç»çæ©æ£æ¨¡åä½ä¸ºçææ§ä¿®å¤å¨ï¼éè¿è¿­ä»£çæä¼ªçå¼å¾åï¼ä¸ºå¤æ¨è§ç¹æä¾è¾å©çç£ï¼ææä¿®å¤ä¼ªå½±ã</li>
<li><strong>æ°æ®é©±å¨çæ¶é´ä¸åæ§éåºç½ç»ï¼TIA-Netï¼ï¼</strong> ä¸ºäºè§£å³è·¨ä¸åé©¾é©¶æ¥å¿ï¼å¨ä¸åæ¶é´æè·ï¼å¯¼è´çåç§ãå¤©æ°åç¬æç©ä½ï¼å¦æ°´åãé´å½±ï¼ååå¸¦æ¥çæ¶é´ç¹å¼æ§ä¼ªå½±é®é¢ï¼ViSEå¼å¥äºä¸ä¸ªè½»éçº§çæ¶é´ä¸åæ§éåºç½ç»ãè¯¥ç½ç»ä½ä¸ºä¸ä¸ªåå¤çæ¨¡åï¼å­¦ä¹ å»é¤æ¸²æå¾åä¸­çæ¶é´ç¹å¼æ§ä¼ªå½±ï¼ç¡®ä¿å¨ä¸åæ¡ä»¶ä¸æè·çæ¥å¿ä¹é´å·æé²æ£ä¸ä¸è´çæ§è½ã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹</strong></p>
<p>ViSEæ¹æ³å¨RealADSim-NVSåºåæµè¯ä¸­åå¾äºæ¾èææï¼æç»å¾å0.441ï¼å¨ææåä¸èä¸­æåç¬¬ä¸ãæ¶èç ç©¶è¿ä¸æ­¥éªè¯äºæ¯ä¸ªç»ä»¶çè´¡ç®ï¼</p>
<ul>
<li><strong>2D-SDFè·¯é¢åéªï¼</strong> æ¾èæé«äºLPIPSï¼ä»0.513å°0.500ï¼ï¼è¡¨ææ¨¡åæååè½»äºå ä½å¤±çï¼äº§çäºæ´å·ç»æä¸è´æ§åçå®æçè¾åºã</li>
<li><strong>ä¼ªLiDARåå§åï¼</strong> è¿ä¸æ­¥æåäºææææ ï¼ç¼è§£äºè®­ç»è§ç¹çè¿æåï¼å¸®å©æ¨¡åæè±äºç³ç³çå±é¨æå°å¼ï¼ä»èå¨è¿è¿åºæ¯ä¸­é½è·å¾äºæ´åç¡®çå ä½ç»æã</li>
<li><strong>å¸¦æçæåéªçä¼ªçå¼ï¼</strong> æ¾èæåäºæ§è½ï¼LPIPSç¸å¯¹æ¹è¿äº20%ï¼ä»0.47å°0.396ï¼ãéè¿ä¸ºæªè§æµåºåæä¾æç¡®çç£ï¼çæåéªææå°âä¿®å¤âäºåççåºæ¯åå®¹ï¼æ¶é¤äºæµ®å¨ä¼ªå½±ï¼æå¤§å°å¢å¼ºäºå¤æ¨è§å¾çè§è§åçæ§ã</li>
<li><strong>TIAç½ç»ï¼</strong> å®ç°äºæé«çæ¹è¿ï¼éè¿å»é¤ç¬æãæ¶é´ç¹å¼æ§åç´ ï¼TIAç½ç»çæäºç¨³å®ãæ¶é´æ å³çåºæ¯è¡¨ç¤ºï¼è¿å¯¹äºåºåæµè¯æéçæ¶ç©ºå¤æ¨çé²æ£æ§è½è³å³éè¦ã</li>
</ul>
<p>è¿äºç»æè¡¨æï¼ViSEçç»æåãåºäºè§è§çç®¡éè½å¤å¨å¤æ¨è§å¾ä¸­çæçå®ä¸å ä½ä¸è´çå¾åï¼è¾¾å°äºæåè¿çæ§è½ã</p>
<p><strong>4. è®ºæä¸­æå°çå±éæ§</strong></p>
<p>è®ºæä¸­æ²¡ææç¡®æåViSEæ¹æ³çå·ä½å±éæ§ãç¶èï¼ä»å¶è§£å³çé®é¢åæ¹æ³è®ºæ¥çï¼å¯è½å­å¨çéæ§å±éåæ¬ï¼</p>
<ul>
<li><strong>è®¡ç®ææ¬ï¼</strong> å°½ç®¡2D-SDFæé«äºä¼åæçï¼ä½æ´ä¸ªåé¶æ®µç®¡éï¼ç¹å«æ¯æ¶åæ©æ£æ¨¡åè¿è¡ä¼ªçå¼çæåæ°æ®é©±å¨éåºç½ç»ï¼å¯è½ä»ç¶å·æè¾é«çè®¡ç®ææ¬ï¼å°¤å¶æ¯å¨å¤§è§æ¨¡åºæ¯æå®æ¶åºç¨ä¸­ã</li>
<li><strong>çæåéªçæ³åè½åï¼</strong> å°½ç®¡çæåéªæå©äºä¿®å¤ä¼ªå½±ï¼ä½å¶ææå¯è½åéäºé¢è®­ç»æ©æ£æ¨¡åçæ³åè½åãå¨è®­ç»æ°æ®ä¸­æªååè¡¨ç¤ºçæç«¯ææ°é¢åºæ¯ä¸ï¼çææ¨¡åå¯è½ä¼äº§çä¸çå®çç»èæå¹»è§ã</li>
<li><strong>æ¶é´ä¸åæ§éåºçå®æ´æ§ï¼</strong> TIAç½ç»æ¨å¨å»é¤æ¶é´ç¹å¼æ§ä¼ªå½±ï¼ä½å®å¨æ¶é¤ææç¬æãæ¶é´ç¸å³çç¹å¾å¯è½æ¯ä¸ä¸ªæç»­çææï¼å°¤å¶æ¯å¨éå¸¸å¨ææå¤æçå¤©æ°/åç§æ¡ä»¶ä¸ã</li>
<li><strong>å¯¹ä¼ªLiDARç¹äºè´¨éçä¾èµï¼</strong> å°½ç®¡ä¼ªLiDARç¹äºæä¾äºå³é®çå ä½åå§åï¼ä½å¦æå¶åå§è´¨éè¾å·®æå­å¨å¤§éåªå£°ï¼å¯è½ä¼å½±ååç»­3DGSçæ¶æåæç»éå»ºè´¨éã</li>
</ul>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹å</strong></p>
<p>åºäºæ¬è®ºæçå·¥ä½ï¼æªæ¥ç ç©¶å¯ä»¥æ¢ç´¢ä»¥ä¸æ¹åï¼</p>
<ul>
<li><strong>å®æ¶æ§è½ä¼åï¼</strong> è¿ä¸æ­¥ä¼åç®¡éçè®¡ç®æçï¼ä½¿å¶æ´éç¨äºå®æ¶èªå¨é©¾é©¶ä»¿çæå¨çº¿åºæ¯éå»ºãè¿å¯è½æ¶åæ´è½»éçº§ççææ¨¡åãæ´é«æç3Dè¡¨ç¤ºææ´å¿«çéåºç½ç»ã</li>
<li><strong>æ´å¼ºå¤§çå ä½åéªï¼</strong> æ¢ç´¢é¤äºè·¯é¢ä¹å¤ï¼å¶ä»åºæ¯åç´ ï¼å¦å»ºç­ç©ãè½¦è¾ï¼çæ´éç¨æèªéåºçå ä½åéªï¼ä»¥è¿ä¸æ­¥æé«å¤æ¨è§å¾çå ä½ä¸è´æ§ã</li>
<li><strong>å¤æ¨¡ææ°æ®èåï¼</strong> è½ç¶è®ºæå¼ºè°äºâä»è§è§âçæ¹æ³ï¼ä½æªæ¥å¯ä»¥ç ç©¶å¦ä½å°å¶ä»ä¼ æå¨æ°æ®ï¼å¦ä½åè¾¨çLiDARãé·è¾¾ï¼ä»¥æ´æºè½çæ¹å¼èå¥å°åå§åæéå»ºé¶æ®µï¼ä»¥è¿ä¸æ­¥å¢å¼ºé²æ£æ§ååç¡®æ§ï¼åæ¶ä¿æè§è§ä¸»å¯¼çä¼å¿ã</li>
<li><strong>å¨æåºæ¯å»ºæ¨¡ï¼</strong> è®ºæä¸»è¦å³æ³¨éæåºæ¯çéå»ºåå¤æ¨ï¼æªæ¥å¯ä»¥æ·±å¥ç ç©¶å¦ä½æ´ææå°å»ºæ¨¡åå¤æ¨å¨æç©ä½ï¼å¦è¡äººãå¶ä»è½¦è¾ï¼ï¼è¿å¯¹äºèªå¨é©¾é©¶ä»¿çè³å³éè¦ã</li>
<li><strong>å¯æ§çåºæ¯ç¼è¾ï¼</strong> ç»åçææ¨¡åçè½åï¼æ¢ç´¢å¨éå»ºçåºæ¯ä¸­è¿è¡å¯æ§ç¼è¾ï¼ä¾å¦æ¹åå¤©æ°æ¡ä»¶ãæ·»å æç§»é¤ç©ä½ï¼ä»¥å®ç°æ´çµæ´»çä»¿çåæµè¯ã</li>
<li><strong>æ´ç²¾ç»çæ¶é´ä¸åæ§ï¼</strong> è¿ä¸æ­¥ç ç©¶æ¶é´ä¸åæ§éåºç½ç»ï¼ä½¿å¶è½å¤åºåå¹¶ä¿çåºæ¯ä¸­éæ¶é´ååççå®å¨æåç´ ï¼å¦æ å¶ææ³ï¼ï¼åæ¶å»é¤ä¸å¿è¦çç¬æä¼ªå½±ã</li>
</ul>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Realistic view extrapolation is critical for closed-loop simulation in
autonomous driving, yet it remains a significant challenge for current Novel
View Synthesis (NVS) methods, which often produce distorted and inconsistent
images beyond the original trajectory.</li>
<li>To address the core challenges of street view extrapolation, we introduce
a comprehensive four-stage pipeline.</li>
<li>Second, we inject strong geometric priors by modeling the road
surface with a novel dimension-reduced SDF termed 2D-SDF.</li>
<li>On the RealADSim-NVS benchmark, our method
achieves a final score of 0.441, ranking first among all participants.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.18341v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.18341v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.18321v1'></a></p>
<h2 id="beyond-single-models-mitigating-multimodal-hallucinations-via-adaptive-token-ensemble-decoding"><a href="https://arxiv.org/abs/2510.18321v1">Beyond Single Models: Mitigating Multimodal Hallucinations via Adaptive Token Ensemble Decoding</a></h2>
<p><strong>Authors:</strong> Jinlin Li, Yuran Wang, Yifei Yuan, Xiao Zhou, Yingying Zhang, Xixian Yong, Yefeng Zheng, Xian Wu</p>
<p><strong>Published:</strong> 2025-10-21</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Large Vision-Language Models (LVLMs) have recently achieved impressive
results in multimodal tasks such as image captioning and visual question
answering. However, they remain prone to object hallucination -- generating
descriptions of nonexistent or misidentified objects. Prior work has partially
mitigated this via auxiliary training objectives or external modules, but
challenges remain in terms of scalability, adaptability, and model
independence. To address these limitations, we propose Adaptive Token Ensemble
Decoding (ATED), a training-free, token-level ensemble framework that mitigates
hallucination by aggregating predictions from multiple LVLMs during inference.
ATED dynamically computes uncertainty-based weights for each model, reflecting
their reliability at each decoding step. It also integrates diverse decoding
paths to improve contextual grounding and semantic consistency. Experiments on
standard hallucination detection benchmarks demonstrate that ATED significantly
outperforms state-of-the-art methods, reducing hallucination without
compromising fluency or relevance. Our findings highlight the benefits of
adaptive ensembling and point to a promising direction for improving LVLM
robustness in high-stakes applications. The code is available at
https://github.com/jinlin2021/ATED.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼è¿æ¯ä¸ç¯å³äºJinlin Liç­äººæ°åçè®ºæâBeyond Single Models: Mitigating Multimodal Hallucinations via Adaptive Token Ensemble Decodingâçå¨é¢æè¦ï¼</p>
<p><strong>è®ºææè¦ï¼è¶è¶åä¸æ¨¡åï¼éè¿èªéåºä»¤çéæè§£ç ç¼è§£å¤æ¨¡æå¹»è§</strong></p>
<p><strong>1. è§£å³çä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³å¤§åè§è§-è¯­è¨æ¨¡åï¼LVLMsï¼ä¸­æ®éå­å¨çâå¯¹è±¡å¹»è§âé®é¢ãå°½ç®¡LVLMså¨å¾åå­å¹åè§è§é®ç­ç­å¤æ¨¡æä»»å¡ä¸­åå¾äºæ¾èè¿å±ï¼ä½å®ä»¬ä»ç¶å®¹æçæä¸å­å¨æè¢«éè¯¯è¯å«çå¯¹è±¡çæè¿°ãç°ææ¹æ³å¨å¯æ©å±æ§ãéåºæ§åæ¨¡åç¬ç«æ§æ¹é¢å­å¨ææã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
ä¸ºäºåæç°ææ¹æ³çå±éæ§ï¼ä½èæåºäº<strong>èªéåºä»¤çéæè§£ç ï¼Adaptive Token Ensemble Decoding, ATEDï¼</strong>ãATEDæ¯ä¸ä¸ªæ éè®­ç»ãä»¤ççº§å«çéææ¡æ¶ï¼éè¿å¨æ¨çè¿ç¨ä¸­èåæ¥èªå¤ä¸ªLVLMsçé¢æµæ¥ç¼è§£å¹»è§ãå¶ä¸»è¦åæ°ç¹åæ¬ï¼
*   <strong>è®­ç»æ å³çä»¤ççº§èåï¼</strong> ATEDæ éé¢å¤è®­ç»ï¼éè¿ç»ç²åº¦çä»¤ççº§èåæ¥ç¼è§£å¹»è§ã
*   <strong>ä¸ç¡®å®æ§å¼å¯¼çå ææºå¶ï¼</strong> ATEDå¨æè®¡ç®åºäºä¸ç¡®å®æ§çæ¨¡åæéï¼åæ æ¯ä¸ªæ¨¡åå¨æ¯ä¸ªè§£ç æ­¥éª¤çå¯é æ§ãå®éè¿æå°åæ´ä½ä¸ç¡®å®æ§æ¥ä¼åæéåéã
*   <strong>å¤è·¯å¾å¯¹æ¯è§£ç ç­ç¥ï¼</strong> è¯¥æ¹æ³æ´åäºå¤æ ·åçè§£ç è·¯å¾ï¼éè¿åºç¨é«æ¯åªå£°æ©ç çæå¾åçæ°å¨åä½ï¼å¢å¼ºäºä¸ä¸æåºç¡åè¯­ä¹ä¸è´æ§ï¼ä»èæé«äºæ¨¡åå¨è§è§ä¸ç¡®å®æ¡ä»¶ä¸çé²æ£æ§ã
*   <strong>ä¸ç¡®å®æ§è´ªå©ªä¼åï¼</strong> å¼å¥äºä¸ç§é«æçè´ªå©ªä¼åç®æ³ï¼éè¿è¿­ä»£å°è°æ´æ¨¡åæéæ¥æå°åéæçä¸ç¡®å®æ§ï¼å¹¶è®¾ç½®äºæ©åæ åä»¥é¿ååä½è®¡ç®ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶éè¦æ§ï¼</strong>
*   <strong>æ¾èä¼äºç°ææ¹æ³ï¼</strong> å¨æ åå¹»è§æ£æµåºåï¼å¦POPEãCHAIRåMMEï¼ä¸çå®éªè¡¨æï¼ATEDæ¾èä¼äºæåè¿çæ¹æ³ï¼å¨ä¸æå®³æµçæ§æç¸å³æ§çåæä¸åå°äºå¹»è§ã
*   <strong>POPEåºåï¼</strong> ATEDå¨AccuracyåF1-scoreä¸å®ç°äº4.20%-6.29%å6.29%-6.97%çæåï¼å¹¶æç»­è¶è¶äºICDåVCDç­ç°ææ¹æ³ã
*   <strong>CHAIRåºåï¼</strong> ATEDå¨CHAIRsææ ä¸è¡¨ç°åºè²ï¼æ¯æå¼ºåºçº¿æé«äº21.13%-41.24%ï¼å¨çæé¿åº¦å¢å æ¶ä»ä¿ææä½³æ§è½ã
*   <strong>MMEåºåï¼</strong> ATEDå¨å¯¹è±¡å±æ§çº§å«çå¹»è§æ£æµï¼åæ¬å­å¨è¯å«ãæ°éå¤æ­ãä½ç½®è¯å«åé¢è²åç±»ï¼ä¸­è¡¨ç°åºæé«æ§è½ï¼Accuracy+ææ æåè³å°+61.7%å+54.2%ã
*   <strong>é²æ£æ§åéåºæ§ï¼</strong> å®éªç»æå¼ºè°äºèªéåºéæçå¥½å¤ï¼å¹¶è¯æäºATEDå¨åç§å¤æ¨¡æä»»å¡ä¸­å·æå¼ºå¤§çéåºæ§åå¯æ©å±æ§ã</p>
<p><strong>4. è®ºæä¸­æå°çå±éæ§ï¼</strong>
*   <strong>æ¨çå»¶è¿ä¸çææ§è½çæè¡¡ï¼</strong> å°½ç®¡ATEDå¨åç¡®æ§æ¹é¢è¡¨ç°åºè²ï¼ä½ä¸é»è®¤æ¹æ³ç¸æ¯ï¼å®ä¼å¼å¥ä¸å®çæ¨çå»¶è¿ãè½ç¶ATEDæä¾äºçµæ´»çæè¡¡æºå¶ï¼ä½ä»éå¨æçåæ§è½ä¹é´è¿è¡å¹³è¡¡ã
*   <strong>æ¨¡åç¬ç«æ§ï¼</strong> å°½ç®¡ATEDæ¨å¨æé«æ¨¡åç¬ç«æ§ï¼ä½å¶æ§è½ä»å¯è½åå°éæä¸­LVLMsä¹é´æ§è½å·®è·çå½±åãå½æ¨¡åä¹é´æ§è½å·®è·è¾å¤§æ¶ï¼ç®åçç»ä¸å¹³åå¯è½æ æ³æææåæ§è½ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>è¿ä¸æ­¥ä¼åæ¨çæçï¼</strong> æ¢ç´¢æ´é«æçéæç­ç¥æä¼åç®æ³ï¼ä»¥å¨ä¿æé«æ§è½çåæ¶è¿ä¸æ­¥åå°æ¨çå»¶è¿ã
*   <strong>æ´å¹¿æ³çæ¨¡åéæï¼</strong> æ©å±ATEDä»¥éææ´å¤æ ·åçLVLMsï¼åæ¬ä¸åæ¶æåè§æ¨¡çæ¨¡åï¼ä»¥è¿ä¸æ­¥æåé²æ£æ§åæ³åè½åã
*   <strong>å¨æè°æ´è¶åæ°ï¼</strong> ç ç©¶æ´æºè½çæºå¶æ¥å¨æè°æ´è¶åæ°ï¼å¦è§è§å¯¹æ¯è§£ç å¼ºåº¦Î±åä¸ç¡®å®æ§è´ªå©ªä¼åæ­¥é¿sï¼ï¼ä»¥éåºä¸åçä»»å¡åæ°æ®ç¹æ§ã
*   <strong>æ¢ç´¢å¶ä»ç±»åçå¹»è§ï¼</strong> å°ATEDåºç¨äºç¼è§£é¤å¯¹è±¡å¹»è§ä¹å¤çå¶ä»ç±»åçå¤æ¨¡æå¹»è§ï¼ä¾å¦å±æ§å¹»è§åå³ç³»å¹»è§ã
*   <strong>é«é£é©åºç¨ï¼</strong> è¿ä¸æ­¥æ¢ç´¢ATEDå¨é«é£é©åºç¨ï¼å¦èªå¨é©¾é©¶åå»å­¦å¾ååæï¼ä¸­çæ½åï¼å¶ä¸­äºå®æ­£ç¡®æ§åè§è§åºç¡è³å³éè¦ã</p>
<p>æ»èè¨ä¹ï¼è¿ç¯è®ºææåºäºä¸ç§æ°é¢ä¸é«æçè®­ç»æ å³æ¹æ³ATEDï¼éè¿èªéåºå°éæå¤ä¸ªLVLMsçé¢æµï¼æ¾èç¼è§£äºå¤æ¨¡æå¹»è§é®é¢ï¼ä¸ºæåLVLMså¨å®éåºç¨ä¸­çé²æ£æ§å¼è¾äºæ°çæ¹åã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address these limitations, we propose Adaptive Token Ensemble
Decoding (ATED), a training-free, token-level ensemble framework that mitigates
hallucination by aggregating predictions from multiple LVLMs during inference.</li>
<li>Experiments on
standard hallucination detection benchmarks demonstrate that ATED significantly
outperforms state-of-the-art methods, reducing hallucination without
compromising fluency or relevance.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.18321v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.18321v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-10-22 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
