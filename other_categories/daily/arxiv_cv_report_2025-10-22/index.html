<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-10-22 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-31
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-10-21/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-10-23/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-10-22">Arxiv Computer Vision Papers - 2025-10-22</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#from-volume-rendering-to-3d-gaussian-splatting-theory-and-applications" class="nav-link">From Volume Rendering to 3D Gaussian Splatting: Theory and Applications</a>
                </li>
                <li class="nav-item">
                    <a href="#proclip-progressive-vision-language-alignment-via-llm-based-embedder" class="nav-link">ProCLIP: Progressive Vision-Language Alignment via LLM-based Embedder</a>
                </li>
                <li class="nav-item">
                    <a href="#exploring-a-unified-vision-centric-contrastive-alternatives-on-multi-modal-web-documents" class="nav-link">Exploring a Unified Vision-Centric Contrastive Alternatives on Multi-Modal Web Documents</a>
                </li>
                <li class="nav-item">
                    <a href="#think-with-3d-geometric-imagination-grounded-spatial-reasoning-from-limited-views" class="nav-link">Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views</a>
                </li>
                <li class="nav-item">
                    <a href="#descriptor-occluded-nuscenes-a-multi-sensor-dataset-for-evaluating-perception-robustness-in-automated-driving" class="nav-link">Descriptor: Occluded nuScenes: A Multi-Sensor Dataset for Evaluating Perception Robustness in Automated Driving</a>
                </li>
                <li class="nav-item">
                    <a href="#beyond-single-images-retrieval-self-augmented-unsupervised-camouflaged-object-detection" class="nav-link">Beyond Single Images: Retrieval Self-Augmented Unsupervised Camouflaged Object Detection</a>
                </li>
                <li class="nav-item">
                    <a href="#cross-modal-scene-semantic-alignment-for-image-complexity-assessment" class="nav-link">Cross-Modal Scene Semantic Alignment for Image Complexity Assessment</a>
                </li>
                <li class="nav-item">
                    <a href="#ranking-based-preference-optimization-for-diffusion-models-from-implicit-user-feedback" class="nav-link">Ranking-based Preference Optimization for Diffusion Models from Implicit User Feedback</a>
                </li>
                <li class="nav-item">
                    <a href="#vise-a-systematic-approach-to-vision-only-street-view-extrapolation" class="nav-link">ViSE: A Systematic Approach to Vision-Only Street-View Extrapolation</a>
                </li>
                <li class="nav-item">
                    <a href="#beyond-single-models-mitigating-multimodal-hallucinations-via-adaptive-token-ensemble-decoding" class="nav-link">Beyond Single Models: Mitigating Multimodal Hallucinations via Adaptive Token Ensemble Decoding</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-10-22">Arxiv Computer Vision Papers - 2025-10-22</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>好的，这是一份针对2025年10月20日Arxiv计算机视觉领域论文的每日报告执行摘要，旨在帮助忙碌的研究人员快速了解关键发展。</p>
<hr />
<p><strong>Arxiv 计算机视觉每日报告执行摘要 (2025-10-20)</strong></p>
<p><strong>1. 主要主题与趋势概述：</strong></p>
<p>今天的论文展示了计算机视觉领域几个活跃且相互关联的研究方向。核心主题包括：</p>
<ul>
<li><strong>3D 视觉与表示：</strong> 3D 重建、场景理解和空间推理是显著的焦点，特别是对新颖的3D表示方法（如高斯泼溅）的探索。</li>
<li><strong>多模态学习与对齐：</strong> 视觉-语言模型的进步，以及跨模态数据（如图像、文本、3D几何）的对齐和融合，是另一个重要趋势。这包括利用大型语言模型（LLMs）增强视觉理解，以及处理多模态幻觉。</li>
<li><strong>鲁棒性与泛化：</strong> 针对复杂现实世界场景（如遮挡、有限视角）的感知鲁棒性评估和提升，以及在无监督或少样本设置下的性能优化，是持续关注的领域。</li>
<li><strong>扩散模型与生成：</strong> 扩散模型在图像生成和优化方面的应用继续深化，特别是在结合用户反馈进行个性化生成方面。</li>
</ul>
<p><strong>2. 显著或创新性论文亮点：</strong></p>
<ul>
<li><strong>"From Volume Rendering to 3D Gaussian Splatting: Theory and Applications" (Vitor Pereira Matias et al.)</strong>: 这篇论文可能代表了3D表示领域的一个重要进展。如果它深入探讨了3D高斯泼溅的理论基础和广泛应用，那么它可能为实时、高质量的3D重建和渲染提供了新的范式，具有很高的潜在影响力。</li>
<li><strong>"ProCLIP: Progressive Vision-Language Alignment via LLM-based Embedder" (Xiaoxing Hu et al.)</strong>: 结合LLM来渐进式地对齐视觉-语言嵌入，这是一种利用LLM强大语义理解能力来提升多模态对齐的创新方法，有望解决现有CLIP模型的一些局限性。</li>
<li><strong>"Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views" (Zhangquan Chen et al.)</strong>: 这篇论文探讨了从有限视角进行3D几何想象和空间推理的能力，这对于机器人、AR/VR以及复杂场景理解至关重要，代表了认知智能在视觉领域的应用。</li>
</ul>
<p><strong>3. 新兴研究方向或技术：</strong></p>
<ul>
<li><strong>3D Gaussian Splatting (3DGS) 的深入研究和应用：</strong> 论文1表明3DGS正从初步探索走向更深入的理论和应用研究，预示着它可能成为未来3D视觉领域的重要基石。</li>
<li><strong>LLM在视觉-语言对齐和多模态幻觉缓解中的作用：</strong> 论文2和论文10都强调了LLM在提升多模态模型性能和解决其固有问题（如幻觉）方面的潜力，表明LLM不再仅仅是文本处理工具，而是多模态AI的关键组件。</li>
<li><strong>检索增强（Retrieval Self-Augmented）学习：</strong> 论文6通过检索自增强来提升无监督伪装目标检测，这是一种利用数据本身进行有效学习的策略，有望在数据稀缺或无标注场景下发挥重要作用。</li>
<li><strong>隐式用户反馈优化扩散模型：</strong> 论文8利用隐式用户反馈来优化扩散模型，这为个性化和用户导向的生成模型提供了新的优化途径，具有实际应用价值。</li>
</ul>
<p><strong>4. 最有价值阅读的论文建议：</strong></p>
<p>对于希望深入了解最新进展的研究人员，建议优先阅读以下论文：</p>
<ul>
<li><strong>"From Volume Rendering to 3D Gaussian Splatting: Theory and Applications" (Vitor Pereira Matias et al.)</strong>: 如果您对3D视觉、新颖的场景表示和实时渲染感兴趣，这篇论文是必读的，因为它可能定义了该领域的新方向。</li>
<li><strong>"ProCLIP: Progressive Vision-Language Alignment via LLM-based Embedder" (Xiaoxing Hu et al.)</strong>: 对于关注多模态学习、视觉-语言模型以及LLM在CV中应用的研究者，这篇论文提供了利用LLM提升对齐的新思路。</li>
<li><strong>"Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views" (Zhangquan Chen et al.)</strong>: 如果您的研究涉及高级场景理解、机器人或需要从不完整信息中进行推理的应用，这篇论文将提供有价值的见解。</li>
<li><strong>"Descriptor: Occluded nuScenes: A Multi-Sensor Dataset for Evaluating Perception Robustness in Automated Driving" (Sanjay Kumar et al.)</strong>: 对于自动驾驶、鲁棒性评估和数据集构建的研究人员，这篇论文提供了一个重要的资源和评估基准。</li>
</ul>
<hr />
<p>这份摘要旨在提供一个高层次的概览，帮助您快速识别与您研究兴趣最相关的论文。</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2510.18101v1">From Volume Rendering to 3D Gaussian Splatting: Theory and Applications</a></li>
<li><a href="#2510.18795v1">ProCLIP: Progressive Vision-Language Alignment via LLM-based Embedder</a></li>
<li><a href="#2510.18703v1">Exploring a Unified Vision-Centric Contrastive Alternatives on Multi-Modal Web Documents</a></li>
<li><a href="#2510.18632v1">Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views</a></li>
<li><a href="#2510.18552v1">Descriptor: Occluded nuScenes: A Multi-Sensor Dataset for Evaluating Perception Robustness in Automated Driving</a></li>
<li><a href="#2510.18437v1">Beyond Single Images: Retrieval Self-Augmented Unsupervised Camouflaged Object Detection</a></li>
<li><a href="#2510.18377v1">Cross-Modal Scene Semantic Alignment for Image Complexity Assessment</a></li>
<li><a href="#2510.18353v1">Ranking-based Preference Optimization for Diffusion Models from Implicit User Feedback</a></li>
<li><a href="#2510.18341v1">ViSE: A Systematic Approach to Vision-Only Street-View Extrapolation</a></li>
<li><a href="#2510.18321v1">Beyond Single Models: Mitigating Multimodal Hallucinations via Adaptive Token Ensemble Decoding</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2510.18101v1'></a></p>
<h2 id="from-volume-rendering-to-3d-gaussian-splatting-theory-and-applications"><a href="https://arxiv.org/abs/2510.18101v1">From Volume Rendering to 3D Gaussian Splatting: Theory and Applications</a></h2>
<p><strong>Authors:</strong> Vitor Pereira Matias, Daniel Perazzo, Vinicius Silva, Alberto Raposo, Luiz Velho, Afonso Paiva, Tiago Novello</p>
<p><strong>Published:</strong> 2025-10-20</p>
<p><strong>Categories:</strong> cs.CV, 68-01, A.1</p>
<p><strong>Abstract:</strong></p>
<p>The problem of 3D reconstruction from posed images is undergoing a
fundamental transformation, driven by continuous advances in 3D Gaussian
Splatting (3DGS). By modeling scenes explicitly as collections of 3D Gaussians,
3DGS enables efficient rasterization through volumetric splatting, offering
thus a seamless integration with common graphics pipelines. Despite its
real-time rendering capabilities for novel view synthesis, 3DGS suffers from a
high memory footprint, the tendency to bake lighting effects directly into its
representation, and limited support for secondary-ray effects. This tutorial
provides a concise yet comprehensive overview of the 3DGS pipeline, starting
from its splatting formulation and then exploring the main efforts in
addressing its limitations. Finally, we survey a range of applications that
leverage 3DGS for surface reconstruction, avatar modeling, animation, and
content generation-highlighting its efficient rendering and suitability for
feed-forward pipelines.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于“From Volume Rendering to 3D Gaussian Splatting: Theory and Applications”的全面摘要，由Vitor Pereira Matias等人撰写。</p>
<p><strong>论文摘要：从体渲染到3D高斯泼溅：理论与应用</strong></p>
<p><strong>1. 主要问题或研究问题</strong>
该论文主要关注3D重建领域，特别是如何从姿态图像中高效、高质量地重建3D场景。传统方法（如基于网格的表示）在可微分性方面存在挑战，而神经辐射场（NeRFs）虽然提供了高质量的新视角合成，但计算成本高昂，难以实现实时渲染。3D高斯泼溅（3DGS）作为一种新兴技术，旨在解决这些限制，实现实时渲染和高效的3D重建。</p>
<p><strong>2. 关键创新或方法论贡献</strong>
该论文作为一篇教程性综述，其主要贡献在于：
*   <strong>3DGS的直观数学推导：</strong> 论文从体渲染方程出发，直观地推导了3DGS的泼溅（splatting）公式，解释了高斯初始化和训练过程中的自适应技术。
*   <strong>3DGS的全面概述：</strong> 详细介绍了3DGS的整个流程，从SfM点云初始化高斯，到通过体泼溅进行渲染，再到利用光度损失进行优化和高斯自适应（分裂、克隆、修剪）。
*   <strong>对3DGS局限性的探讨及现有解决方案的综述：</strong> 论文深入分析了原始3DGS方法的局限性，包括高内存占用、光照效果烘焙到表示中、对二次光线效果支持有限等。并详细介绍了针对这些问题的最新扩展和改进，例如：
    *   <strong>内存优化：</strong> 介绍了SCAFFOLD等通过多层感知器和锚点来减少高斯数量的方法。
    *   <strong>抗锯齿与多分辨率：</strong> 讨论了MIP-Splatting等通过2D和3D高斯滤波器解决分辨率变化引起的锯齿问题。
    *   <strong>镜面反射与重打光：</strong> 介绍了Gaussian-Shader、3DGS-DR、IRGS等将经典反射和着色概念融入3DGS框架的方法，以及利用BRDF参数和光线追踪技术实现物理渲染。
    *   <strong>体泼溅的改进：</strong> 探讨了Stop-the-pop等解决排序伪影的方法，以及通过补偿项提高体渲染准确性的方法。
*   <strong>3DGS应用领域的广泛调查：</strong> 论文全面回顾了3DGS在表面重建、头像建模、动画和内容生成等多个应用中的最新进展，突出了其高效渲染和适用于前馈管道的特性。</p>
<p><strong>3. 主要结果及其意义</strong>
该论文强调了3DGS在3D重建领域带来的根本性变革。通过将场景建模为3D高斯集合，3DGS实现了高效的体泼溅渲染，并与现有图形管道无缝集成。其实时渲染能力对于新视角合成至关重要。论文通过对现有文献的全面综述，展示了3DGS及其扩展在解决复杂场景重建、动态内容生成、高保真头像建模等方面的强大潜力。这些进展使得3DGS成为计算机视觉和图形学领域一个快速发展且极具前景的研究方向。</p>
<p><strong>4. 论文中提及的局限性</strong>
尽管3DGS取得了显著进展，但论文也明确指出了其固有的局限性：
*   <strong>高内存占用：</strong> 复杂场景需要大量的3D高斯（20万到50万），导致显著的内存和存储需求。
*   <strong>光照效果烘焙：</strong> 原始3DGS模型将光照条件直接烘焙到表示中，限制了其在高度反射表面上的性能，并阻碍了重打光能力。
*   <strong>二次光线效果支持有限：</strong> 缺乏对反射、折射等二次光线效果的直接支持，影响了渲染的真实感。
*   <strong>渲染精度问题：</strong> 体泼溅引入了一些近似，可能影响渲染精度，导致“弹出”伪影等问题。
*   <strong>稀疏视图重建挑战：</strong> 在稀疏图像集下，3DGS的优化容易陷入局部最小值。
*   <strong>不适用于网格提取：</strong> 原始3DGS并非直接为网格提取而设计，需要额外的技术来增强其表面重建能力。</p>
<p><strong>5. 潜在的未来研究方向</strong>
论文指出了3DGS领域未来研究的几个有前景的方向：
*   <strong>优化高斯数量：</strong> 寻找确定最佳高斯数量的方法，以平衡内存效率和渲染质量。
*   <strong>改进泼溅公式：</strong> 进一步完善体泼溅公式，提高渲染精度，减少近似带来的伪影。
*   <strong>开发前馈3D重建模型：</strong> 探索更快速、更鲁棒的前馈3D重建模型，尤其是在稀疏输入视图下。
*   <strong>增强二次光线效果：</strong> 进一步整合光线追踪技术，以更好地模拟反射、折射等复杂光照现象。
*   <strong>与生成模型的结合：</strong> 进一步探索3DGS与图像/视频扩散模型、生成对抗网络（GANs）等生成模型的结合，以实现更高效、更灵活的3D内容创建和动画。
*   <strong>更广泛的应用：</strong> 探索3DGS在更多实际应用场景中的潜力，例如机器人、自动驾驶、虚拟现实/增强现实等。</p>
<p>总而言之，这篇教程性论文为理解3DGS提供了一个全面而深入的视角，从其理论基础到实际应用，并指出了该领域面临的挑战和未来的发展方向，对于计算机视觉和图形学研究者具有重要的参考价值。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Despite its
real-time rendering capabilities for novel view synthesis, 3DGS suffers from a
high memory footprint, the tendency to bake lighting effects directly into its
representation, and limited support for secondary-ray effects.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.18101v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.18101v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.18795v1'></a></p>
<h2 id="proclip-progressive-vision-language-alignment-via-llm-based-embedder"><a href="https://arxiv.org/abs/2510.18795v1">ProCLIP: Progressive Vision-Language Alignment via LLM-based Embedder</a></h2>
<p><strong>Authors:</strong> Xiaoxing Hu, Kaicheng Yang, Ziyong Feng, Qi Ming, Zonghao Guo, Xiang An, Ziyong Feng, Junchi Yan, Xue Yang</p>
<p><strong>Published:</strong> 2025-10-21</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>The original CLIP text encoder is limited by a maximum input length of 77
tokens, which hampers its ability to effectively process long texts and perform
fine-grained semantic understanding. In addition, the CLIP text encoder lacks
support for multilingual inputs. All these limitations significantly restrict
its applicability across a broader range of tasks. Recent studies have
attempted to replace the CLIP text encoder with an LLM-based embedder to
enhance its ability in processing long texts, multilingual understanding, and
fine-grained semantic comprehension. However, because the representation spaces
of LLMs and the vision-language space of CLIP are pretrained independently
without alignment priors, direct alignment using contrastive learning can
disrupt the intrinsic vision-language alignment in the CLIP image encoder,
leading to an underutilization of the knowledge acquired during pre-training.
To address this challenge, we propose ProCLIP, a curriculum learning-based
progressive vision-language alignment framework to effectively align the CLIP
image encoder with an LLM-based embedder. Specifically, ProCLIP first distills
knowledge from CLIP's text encoder into the LLM-based embedder to leverage
CLIP's rich pretrained knowledge while establishing initial alignment between
the LLM embedder and CLIP image encoder. Subsequently, ProCLIP further aligns
the CLIP image encoder with the LLM-based embedder through image-text
contrastive tuning, employing self-distillation regularization to avoid
overfitting. To achieve a more effective alignment, instance semantic alignment
loss and embedding structure alignment loss are employed during representation
inheritance and contrastive tuning. The Code is available at
https://github.com/VisionXLab/ProCLIP</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供对Xiaoxing Hu等人撰写的论文“ProCLIP: Progressive Vision-Language Alignment via LLM-based Embedder”的全面摘要。</p>
<hr />
<h3 id="proclip-progressive-vision-language-alignment-via-llm-based-embedder_1">论文摘要：ProCLIP: Progressive Vision-Language Alignment via LLM-based Embedder</h3>
<p><strong>1. 主要问题或研究问题</strong></p>
<p>原始CLIP模型的文本编码器存在显著局限性：
*   <strong>输入长度限制：</strong> 最多只能处理77个token，这严重限制了其处理长文本和进行细粒度语义理解的能力。
*   <strong>多语言支持缺失：</strong> 无法处理多语言输入。
*   <strong>语义理解不足：</strong> 缺乏对细粒度文本语义的监督，进一步阻碍了其语义理解能力。</p>
<p>近期研究尝试用基于大型语言模型（LLM）的嵌入器替换CLIP文本编码器，以解决长文本、多语言和细粒度语义理解问题。然而，由于LLM的表示空间与CLIP的视觉-语言空间是独立预训练的，且缺乏对齐先验，直接使用对比学习进行对齐可能会破坏CLIP图像编码器中固有的视觉-语言对齐，导致预训练知识的利用不足。</p>
<p>因此，本文旨在解决的关键研究问题是：<strong>如何系统地利用CLIP的预训练知识，以实现与LLM-based嵌入器的高效跨模态对齐，同时保持模型的泛化能力？</strong></p>
<p><strong>2. 关键创新或方法论贡献</strong></p>
<p>本文提出了<strong>ProCLIP</strong>，一个基于课程学习的渐进式视觉-语言对齐框架，旨在有效对齐CLIP图像编码器与LLM-based嵌入器。其核心方法论贡献包括：</p>
<ul>
<li>
<p><strong>两阶段渐进式对齐框架：</strong></p>
<ul>
<li><strong>阶段1：跨架构蒸馏的表示继承（Representation Inheritance via Cross-Architecture Distillation）：</strong> ProCLIP首先将知识从原始CLIP文本编码器蒸馏到LLM-based嵌入器（仅MLP可训练），从而利用CLIP丰富的预训练知识，并建立LLM嵌入器与CLIP图像编码器之间的初步对齐。<ul>
<li><strong>实例语义对齐损失（Instance Semantic Alignment Loss, Lins）：</strong> 用于在实例级别上对齐LLM嵌入器和CLIP文本嵌入器的维度。</li>
<li><strong>嵌入结构对齐损失（Embedding Structure Alignment Loss, Lstruct）：</strong> 衡量批次内样本间的距离差异，通过最小化成对距离差异实现全局对齐。</li>
</ul>
</li>
<li><strong>阶段2：集成自蒸馏正则化的对比微调（Contrastive Tuning Integrated with Self-Distillation Regularization）：</strong> 在初步对齐的基础上，ProCLIP通过图像-文本对比学习进一步对齐CLIP图像编码器与LLM-based嵌入器。为避免过拟合，引入自蒸馏正则化（Self-Distillation Regularization）约束CLIP图像编码器，稳定训练并提高泛化能力。<ul>
<li><strong>信息对比损失（InfoNCE Loss, Linfo）：</strong> 用于图像嵌入和LLM嵌入之间的对比学习。</li>
<li><strong>正则化损失（Lreg）：</strong> 对CLIP图像编码器施加对称的正则化损失，以减轻训练过程中的灾难性遗忘，并保留模型的泛化能力。</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>强调预训练知识的利用：</strong> 明确指出并解决了现有方法未能充分利用CLIP预训练知识的局限性，通过蒸馏和自蒸馏机制有效整合了这些知识。</p>
</li>
</ul>
<p><strong>3. 主要结果及其意义</strong></p>
<p>ProCLIP在多项任务和不同数据规模上均表现出卓越的性能和鲁棒性：</p>
<ul>
<li><strong>零样本分类（Zero-Shot Classification）：</strong> 相比基线模型，ProCLIP在零样本分类任务上实现了6.8%至13.5%的显著提升。尤其是在30M数据样本下，平均性能提升了约10%-13.5%。</li>
<li><strong>跨模态检索（Cross-Modal Retrieval）：</strong> 在短文本和长文本检索任务中，ProCLIP始终优于LLM2CLIP。例如，在Flickr30k数据集上，使用ViT-L/14和30M训练样本，图像到文本（I2T）检索的Recall@1达到了95.0%，比LLM2CLIP高出近2个百分点。在长文本基准测试（如DOCCI、DCI、Urban-1k）上，ProCLIP也表现出明显优势，特别是在T2I检索方面有强劲提升。</li>
<li><strong>多语言跨模态检索（Multilingual Cross-Modal Retrieval）：</strong> 受益于LLM-based嵌入器，ProCLIP在XM3600基准测试上实现了优越的多语言性能，这归因于CLIP图像编码器与LLM-based嵌入器之间改进的对齐。</li>
<li><strong>鲁棒性（Robustness）：</strong> 在ImageNet-A和ImageNet-R等具有挑战性的分布外数据集上，ProCLIP的性能优于LLM2CLIP超过10个百分点，平均提升5.9%-9.3%，表明其处理分布偏移和复杂扰动的能力增强。</li>
<li><strong>细粒度理解（Fine-Grained Understanding）：</strong> 在MMVP-VLM基准测试上，ProCLIP模型在相应数据规模上实现了3.0%、2.2%和10.4%的提升，表明LLM-based嵌入器增强了细粒度语义区分能力，且渐进式对齐策略有效。</li>
<li><strong>数据和模型规模分析：</strong> 性能随数据规模增加而提升，且ProCLIP在相同数据规模下始终优于LLM2CLIP，展现了数据效率。通过增加线性层数量（12层），模型性能进一步提升，表明ProCLIP可从简单的参数扩展中持续受益。</li>
</ul>
<p>这些结果共同证明了ProCLIP的有效性和鲁棒性，它成功地解决了现有方法的局限性，并在广泛的视觉-语言任务中取得了显著进展。</p>
<p><strong>4. 论文中提及的局限性</strong></p>
<ul>
<li><strong>训练效率：</strong> ProCLIP在第二阶段需要解冻视觉编码器并进行在线自蒸馏，这增加了计算成本。实验中，训练速度约为基线的0.74倍。</li>
<li><strong>细粒度视觉对齐：</strong> ProCLIP目前仍基于全局语义进行对比学习。作者指出，将局部视觉补丁与文本语义对齐可以进一步增强视觉编码器的局部感知能力，从而有益于开放词汇语义分割和目标检测等任务。</li>
<li><strong>模型架构：</strong> 本文主要关注替换CLIP文本编码器。作者提出，是否也可以类似地替换双塔架构中的视觉编码器，以解决视觉表示的局限性（例如CLIP图像编码器缺乏局部性）。</li>
</ul>
<p><strong>5. 潜在的未来研究方向</strong></p>
<ul>
<li><strong>提高训练效率：</strong><ul>
<li>在第二阶段采用基于PEFT（Parameter-Efficient Fine-Tuning）的方法来微调视觉编码器。</li>
<li>仅微调视觉编码器参数的一部分，例如最后几个Transformer块。</li>
<li>用离线蒸馏替换在线蒸馏，以大幅减少第二阶段引入的额外计算成本。</li>
</ul>
</li>
<li><strong>增强细粒度视觉对齐：</strong> 探索将局部视觉补丁与文本语义对齐的方法，以提升视觉编码器的局部感知能力，从而更好地支持开放词汇语义分割和目标检测等任务。</li>
<li><strong>探索新的模型架构：</strong> 研究是否可以替换双塔架构中的视觉编码器，以解决现有视觉表示的局限性，例如CLIP图像编码器缺乏局部性的问题。</li>
</ul>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this challenge, we propose ProCLIP, a curriculum learning-based
progressive vision-language alignment framework to effectively align the CLIP
image encoder with an LLM-based embedder.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.18795v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.18795v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.18703v1'></a></p>
<h2 id="exploring-a-unified-vision-centric-contrastive-alternatives-on-multi-modal-web-documents"><a href="https://arxiv.org/abs/2510.18703v1">Exploring a Unified Vision-Centric Contrastive Alternatives on Multi-Modal Web Documents</a></h2>
<p><strong>Authors:</strong> Yiqi Lin, Alex Jinpeng Wang, Linjie Li, Zhengyuan Yang, Mike Zheng Shou</p>
<p><strong>Published:</strong> 2025-10-21</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Contrastive vision-language models such as CLIP have demonstrated strong
performance across a wide range of multimodal tasks by learning from aligned
image-text pairs. However, their ability to handle complex, real-world web
documents remains limited, particularly in scenarios where text and images are
interleaved, loosely aligned, or embedded in visual form. To address these
challenges, we propose Vision-Centric Contrastive Learning (VC2L), a unified
framework that models text, images, and their combinations using a single
vision transformer. VC2L operates entirely in pixel space by rendering all
inputs, whether textual, visual, or combined, as images, thus eliminating the
need for OCR, text tokenization, or modality fusion strategy. To capture
complex cross-modal relationships in multimodal web documents, VC2L employs a
snippet-level contrastive learning objective that aligns consecutive multimodal
segments, leveraging the inherent coherence of documents without requiring
explicitly paired image-text data. To assess the effectiveness of this
approach, we introduce three retrieval benchmarks, AnyCIR, SeqCIR, and CSR,
designed to evaluate cross-modal retrieval, fine-grained sequential
understanding, and generalization to unseen data, respectively. Empirical
results show that VC2L achieves competitive or superior performance compared to
CLIP-style models on both the proposed benchmarks and established datasets such
as M-BEIR and MTEB. These findings underscore the potential of multimodal web
data as a valuable training resource for contrastive learning and illustrate
the scalability of a unified, vision-centric approach for multimodal
representation learning. Code and models are available at:
https://github.com/showlab/VC2L.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于Yiqi Lin, Alex Jinpeng Wang, Linjie Li, Zhengyuan Yang, Mike Zheng Shou撰写的论文“Exploring a Unified Vision-Centric Contrastive Alternatives on Multi-Modal Web Documents”的全面摘要：</p>
<p><strong>论文题目：</strong> 探索多模态网络文档中统一的以视觉为中心的对比学习替代方案</p>
<p><strong>1. 主要问题或研究问题：</strong>
现有的对比视觉-语言模型（如CLIP）在处理复杂、真实世界的网络文档时面临局限性。这些文档通常包含交错的文本和图像、松散对齐的内容，或者文本以视觉形式嵌入（例如扫描文档或图像PDF）。这些模型难以有效处理这些场景，因为它们通常需要OCR、文本分词或复杂的模态融合策略，并且依赖于显式对齐的图像-文本对。论文旨在解决如何构建一个统一的、以视觉为中心的框架，能够直接从像素空间理解多模态网络文档，从而克服这些挑战。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
*   <strong>Vision-Centric Contrastive Learning (VC2L) 框架：</strong> 论文提出了VC2L，一个统一的框架，通过单个视觉Transformer处理文本、图像及其组合。它将所有输入（无论是文本、视觉还是组合内容）渲染成图像，完全在像素空间中操作，从而消除了对OCR、文本分词或模态融合策略的需求。
*   <strong>片段级对比学习目标：</strong> VC2L采用片段级对比学习目标，通过对齐文档中连续的多模态片段来捕捉复杂的跨模态关系。这种方法利用文档固有的连贯性，无需显式配对的图像-文本数据。
*   <strong>数据增强策略：</strong> 引入了模态掩蔽（modality masking）和文本掩蔽（text masking）增强，以增加对比目标的 다양性，并提升模型对语言的理解能力。
*   <strong>新基准测试：</strong> 为了评估VC2L的有效性，论文引入了三个新的检索基准：
    *   <strong>AnyCIR (Any-to-Any Consecutive Information Retrieval)：</strong> 评估跨模态检索能力。
    *   <strong>SeqCIR (Sequential Consecutive Information Retrieval)：</strong> 评估细粒度序列理解能力，通过顺序检索连续片段。
    *   <strong>CSR (Zero-Shot Consecutive Slide Retrieval)：</strong> 评估模型对未见数据的泛化能力，特别是在复杂图像-文本交错的幻灯片数据上。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>卓越的性能：</strong> 实验结果表明，VC2L在提出的AnyCIR、SeqCIR和CSR基准测试以及M-BEIR和MTEB等现有数据集上，与CLIP风格的模型相比，取得了具有竞争力甚至更优异的性能。
*   <strong>统一视觉方法的有效性：</strong> 结果强调了多模态网络数据作为对比学习宝贵训练资源的潜力，并展示了统一的、以视觉为中心的方法在多模态表示学习方面的可扩展性。
*   <strong>像素空间语言理解的提升：</strong> 联合图像-文本交错训练能够进一步提高模型在像素空间中的语言理解能力。
*   <strong>对复杂布局的泛化能力：</strong> VC2L即使在渲染数据上进行训练，也能有效地泛化到具有不同字体大小和样式的真实世界复杂布局。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>固定输入尺寸的限制：</strong> 尽管VC2L能够使用单个模型处理任何模态输入，但其效率和可扩展性受限于固定的输入尺寸。
*   <strong>训练数据规模：</strong> 论文中使用的训练数据（500万文档，约1700万图像）相对于WIT-400M等大型数据集来说相对较小，这限制了模型在更大规模上的探索。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>动态输入策略和新架构：</strong> 未来的工作可以探索设计动态输入策略或新的架构，以显著提高性能，并解锁更多以视觉为中心的多模态网络数据理解应用。
*   <strong>大规模训练：</strong> 鉴于当前训练数据规模的限制，未来可以进行更大规模的训练实验。
*   <strong>负责任的部署和监督：</strong> 强调在部署基于VC2L的检索系统时，需要整合内容过滤、用户访问控制和可解释性功能，确保训练数据的多样性和道德来源，并建立健全的监控程序，以检测和应对滥用行为。</p>
<p>总而言之，VC2L为从复杂文档源进行多模态检索提供了一个新颖且实用的解决方案，通过将所有内容渲染到像素空间并利用文档的固有连贯性进行片段级对比学习，实现了统一的视觉-语言理解。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address these
challenges, we propose Vision-Centric Contrastive Learning (VC2L), a unified
framework that models text, images, and their combinations using a single
vision transformer.</li>
<li>To assess the effectiveness of this
approach, we introduce three retrieval benchmarks, AnyCIR, SeqCIR, and CSR,
designed to evaluate cross-modal retrieval, fine-grained sequential
understanding, and generalization to unseen data, respectively.</li>
<li>Empirical
results show that VC2L achieves competitive or superior performance compared to
CLIP-style models on both the proposed benchmarks and established datasets such
as M-BEIR and MTEB.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.18703v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.18703v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.18632v1'></a></p>
<h2 id="think-with-3d-geometric-imagination-grounded-spatial-reasoning-from-limited-views"><a href="https://arxiv.org/abs/2510.18632v1">Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views</a></h2>
<p><strong>Authors:</strong> Zhangquan Chen, Manyuan Zhang, Xinlei Yu, Xufang Luo, Mingze Sun, Zihao Pan, Yan Feng, Peng Pei, Xunliang Cai, Ruqi Huang</p>
<p><strong>Published:</strong> 2025-10-21</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, I.2.10</p>
<p><strong>Abstract:</strong></p>
<p>Though recent advances in vision-language models (VLMs) have achieved
remarkable progress across a wide range of multimodal tasks, understanding 3D
spatial relationships from limited views remains a significant challenge.
Previous reasoning methods typically rely on pure text (e.g., topological
cognitive maps) or on 2D visual cues. However, their limited representational
capacity hinders performance in specific tasks that require 3D spatial
imagination. To address this limitation, we propose 3DThinker, a framework that
can effectively exploits the rich geometric information embedded within images
while reasoning, like humans do. Our framework is the first to enable 3D
mentaling during reasoning without any 3D prior input, and it does not rely on
explicitly labeled 3D data for training. Specifically, our training consists of
two stages. First, we perform supervised training to align the 3D latent
generated by VLM while reasoning with that of a 3D foundation model (e.g.,
VGGT). Then, we optimize the entire reasoning trajectory solely based on
outcome signals, thereby refining the underlying 3D mentaling. Extensive
experiments across multiple benchmarks show that 3DThinker consistently
outperforms strong baselines and offers a new perspective toward unifying 3D
representations into multimodal reasoning. Our code will be available at
https://github.com/zhangquanchen/3DThinker.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供论文《Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views》的全面摘要。</p>
<p><strong>论文摘要：Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views</strong></p>
<p>这篇由Zhangquan Chen及其团队撰写的论文《Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views》提出了一种名为3DThinker的新型框架，旨在解决视觉-语言模型（VLMs）在从有限视角理解3D空间关系方面的显著挑战。</p>
<p><strong>1. 主要问题或研究问题：</strong>
尽管视觉-语言模型在多模态任务中取得了巨大进展，但它们在从有限视角理解3D空间关系方面仍然面临重大挑战。现有的推理方法主要依赖纯文本或2D视觉线索，其有限的表示能力阻碍了需要3D空间想象力的特定任务的性能。核心问题在于VLM无法从图像中提取3D几何信息，且其空间想象能力受限。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
3DThinker框架的核心创新在于使VLM能够在推理过程中“像人类一样”有效地利用图像中嵌入的丰富几何信息，并进行3D心智想象（3D mentaling），而无需任何3D先验输入或显式标注的3D数据进行训练。其主要贡献包括：
*   <strong>3D心智想象框架：</strong> 首次引入“think with 3D mentaling”框架，该框架无需依赖密集的标注训练数据（如认知地图）。
*   <strong>两阶段训练框架：</strong>
    *   <strong>第一阶段（监督训练）：</strong> 将VLM在推理过程中生成的3D潜在表示与3D基础模型（如VGGT）的特征空间进行对齐。这通过3D潜在对齐损失和交叉熵损失实现，使模型在保持文本连贯性的同时，能够进行3D心智想象。
    *   <strong>第二阶段（强化学习）：</strong> 在监督训练之后，仅基于结果信号优化整个推理轨迹，从而在轨迹中细化底层的3D心智想象，同时保持3D潜在表示的对齐。
*   <strong>可解释性增强：</strong> 3DThinker通过投影器能够从潜在空间中恢复3D表示（例如点云），显著增强了大型推理模型的可解释性。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>性能显著提升：</strong> 在MindCube-Tiny和Ego3D-Bench等多个基准测试中，3DThinker始终优于强大的基线模型。例如，在MindCube-Tiny上，3DThinker-full的整体性能提升范围为51.8%至108.8%；在Ego3D-Bench上，提升范围为18.1%至36.9%。
*   <strong>跨数据集泛化能力：</strong> 即使在没有Ego3D特定数据训练的情况下，模型在Ego3D-Bench上仍取得了有希望的结果，表明其在不同空间理解场景中具有强大的泛化能力。
*   <strong>统一3D表示的新视角：</strong> 论文为将3D表示统一到多模态推理中提供了新视角，突出了其广泛适用性。
*   <strong>消融研究：</strong> 实验表明，最佳的3D潜在大小约为12，且将3D特殊token放置在推理轨迹的开头或结尾能获得更好的性能。此外，3D对齐和最终答案奖励对性能至关重要。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>3D表示的自回归整合：</strong> 目前，3DThinker从特殊token的最后一层隐藏状态中恢复3D心智表示，但这些潜在表示并未自回归地整合到框架中。
*   <strong>迭代3D心智想象：</strong> 论文尚未探索在推理轨迹中进行迭代3D心智想象。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>统一结构开发：</strong> 开发一个统一的结构（例如，统一的tokenizer），以自回归地整合3D潜在表示，这将是未来改进的关键领域。
*   <strong>迭代3D心智想象探索：</strong> 探索在推理轨迹中进行迭代3D心智想象可能会带来额外的益处。</p>
<p>总而言之，3DThinker框架通过其独特的两阶段训练方法，使视觉-语言模型能够进行内在的3D心智想象，从而在无需显式3D标注数据的情况下，显著提升了模型在3D空间推理任务中的性能和泛化能力。这为多模态推理中3D表示的统一提供了一个有前景的新方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this limitation, we propose 3DThinker, a framework that
can effectively exploits the rich geometric information embedded within images
while reasoning, like humans do.</li>
<li>Extensive
experiments across multiple benchmarks show that 3DThinker consistently
outperforms strong baselines and offers a new perspective toward unifying 3D
representations into multimodal reasoning.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.18632v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.18632v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.18552v1'></a></p>
<h2 id="descriptor-occluded-nuscenes-a-multi-sensor-dataset-for-evaluating-perception-robustness-in-automated-driving"><a href="https://arxiv.org/abs/2510.18552v1">Descriptor: Occluded nuScenes: A Multi-Sensor Dataset for Evaluating Perception Robustness in Automated Driving</a></h2>
<p><strong>Authors:</strong> Sanjay Kumar, Tim Brophy, Reenu Mohandas, Eoin Martino Grua, Ganesh Sistu, Valentina Donzella, Ciaran Eising</p>
<p><strong>Published:</strong> 2025-10-21</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Robust perception in automated driving requires reliable performance under
adverse conditions, where sensors may be affected by partial failures or
environmental occlusions. Although existing autonomous driving datasets
inherently contain sensor noise and environmental variability, very few enable
controlled, parameterised, and reproducible degradations across multiple
sensing modalities. This gap limits the ability to systematically evaluate how
perception and fusion architectures perform under well-defined adverse
conditions. To address this limitation, we introduce the Occluded nuScenes
Dataset, a novel extension of the widely used nuScenes benchmark. For the
camera modality, we release both the full and mini versions with four types of
occlusions, two adapted from public implementations and two newly designed. For
radar and LiDAR, we provide parameterised occlusion scripts that implement
three types of degradations each, enabling flexible and repeatable generation
of occluded data. This resource supports consistent, reproducible evaluation of
perception models under partial sensor failures and environmental interference.
By releasing the first multi-sensor occlusion dataset with controlled and
reproducible degradations, we aim to advance research on robust sensor fusion,
resilience analysis, and safety-critical perception in automated driving.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Sanjay Kumar等人撰写的论文“Descriptor: Occluded nuScenes: A Multi-Sensor Dataset for Evaluating Perception Robustness in Automated Driving”的全面摘要。</p>
<hr />
<p><strong>论文摘要：Occluded nuScenes：用于评估自动驾驶中感知鲁棒性的多传感器数据集</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
自动驾驶系统在恶劣条件下（如传感器部分故障或环境遮挡）需要鲁棒的感知能力。然而，现有自动驾驶数据集虽然包含传感器噪声和环境变异性，但很少能对多种传感模态进行受控、参数化和可复现的降级。这限制了系统性评估感知和融合架构在明确定义的恶劣条件下的表现。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
为了解决上述限制，作者引入了<strong>Occluded nuScenes数据集</strong>，这是对广泛使用的nuScenes基准测试的一个新颖扩展。其主要创新和贡献包括：
*   <strong>首个多传感器遮挡数据集：</strong> 首次在相机、雷达和LiDAR三种主要传感器模态上应用合成遮挡。
*   <strong>相机模态的四种遮挡类型：</strong> 针对相机模态，发布了完整版和迷你版数据集，包含四种遮挡类型——两种改编自现有公共实现（灰尘和水模糊），两种是新设计的（WoodScape污垢模式和划痕）。这些遮挡模拟了镜头污染和表面磨损等真实世界效应。
*   <strong>雷达和LiDAR模态的参数化遮挡脚本：</strong> 针对雷达和LiDAR，提供了参数化遮挡脚本，每种模态实现三种降级类型。
    *   <strong>雷达：</strong> 单传感器故障（随机禁用一个雷达）、点云丢弃（模拟信号不完整）和环境噪声（添加高斯噪声）。
    *   <strong>LiDAR：</strong> 基于区域的遮挡（模拟盲区）、部分点云丢弃（模拟恶劣天气）和基于角度的遮挡（模拟锥形盲区）。
*   <strong>受控和可复现的降级：</strong> 这些脚本允许用户灵活、可复现地生成不同严重程度的遮挡数据，确保了与现有nuScenes标注和感知管道的完全兼容性。
*   <strong>全面的验证：</strong> 通过结构一致性检查、视觉检查、参数验证以及在车辆分割、地图分割和3D目标检测等下游感知任务上的性能评估，验证了数据集的有效性和质量。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>感知模型性能下降：</strong> 在受遮挡数据上评估时，基线感知架构（如SimpleBEV、BEVFusion和BEVCar）均表现出可测量的性能下降，证实了遮挡模拟了传感器级别的干扰，并为鲁棒性基准测试提供了有意义的挑战。
*   <strong>SSIM指标验证：</strong> 通过计算清洁图像与遮挡图像之间的平均结构相似性指数（SSIM），量化了感知降级。结果显示，不同遮挡类型和严重程度导致了显著的SSIM下降，例如灰尘遮挡导致SSIM下降0.43至0.88，水模糊下降0.28至0.45，划痕下降0.34，WoodScape污垢模式下降0.074。这表明遮挡有效地模拟了视觉质量损失。
*   <strong>促进鲁棒性研究：</strong> 该数据集为研究鲁棒传感器融合、弹性分析和自动驾驶中安全关键感知提供了统一、可复现的评估资源。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>合成遮挡的近似性：</strong> 论文明确指出，虽然遮挡是合成的，它们近似了真实世界的传感器降级，但不能完全复制。
*   <strong>极端参数设置：</strong> 某些极端参数设置（如非常高的丢弃率）旨在作为压力测试，而非典型的驾驶场景。
*   <strong>存储限制：</strong> 为了避免过大的存储需求，雷达和LiDAR的遮挡数据未预先生成并分发，而是通过脚本按需生成。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>鲁棒传感器融合模型开发：</strong> 利用该数据集训练和测试在遮挡条件下表现更佳的多传感器融合模型。
*   <strong>弹性分析和故障容错：</strong> 探索传感器冗余和故障容错策略，以提高自动驾驶系统的整体弹性。
*   <strong>安全关键感知：</strong> 在受控条件下对感知模型的鲁棒性进行可复现的测试，以满足安全关键应用的需求。
*   <strong>遮挡感知训练：</strong> 利用该数据集进行遮挡感知训练，使模型能够更好地处理真实世界中的传感器降级。
*   <strong>更真实的遮挡模拟：</strong> 进一步研究和开发更接近真实世界传感器降级的遮挡模拟方法。</p>
<hr />
<p>这篇论文通过提供一个独特且受控的遮挡数据集，为自动驾驶领域在恶劣条件下的感知鲁棒性研究奠定了重要基础，具有显著的理论和实践意义。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this limitation, we introduce the Occluded nuScenes
Dataset, a novel extension of the widely used nuScenes benchmark.</li>
<li>For the
camera modality, we release both the full and mini versions with four types of
occlusions, two adapted from public implementations and two newly designed.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.18552v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.18552v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.18437v1'></a></p>
<h2 id="beyond-single-images-retrieval-self-augmented-unsupervised-camouflaged-object-detection"><a href="https://arxiv.org/abs/2510.18437v1">Beyond Single Images: Retrieval Self-Augmented Unsupervised Camouflaged Object Detection</a></h2>
<p><strong>Authors:</strong> Ji Du, Xin Wang, Fangwei Hao, Mingyang Yu, Chunyuan Chen, Jiesheng Wu, Bin Wang, Jing Xu, Ping Li</p>
<p><strong>Published:</strong> 2025-10-21</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>At the core of Camouflaged Object Detection (COD) lies segmenting objects
from their highly similar surroundings. Previous efforts navigate this
challenge primarily through image-level modeling or annotation-based
optimization. Despite advancing considerably, this commonplace practice hardly
taps valuable dataset-level contextual information or relies on laborious
annotations. In this paper, we propose RISE, a RetrIeval SElf-augmented
paradigm that exploits the entire training dataset to generate pseudo-labels
for single images, which could be used to train COD models. RISE begins by
constructing prototype libraries for environments and camouflaged objects using
training images (without ground truth), followed by K-Nearest Neighbor (KNN)
retrieval to generate pseudo-masks for each image based on these libraries. It
is important to recognize that using only training images without annotations
exerts a pronounced challenge in crafting high-quality prototype libraries. In
this light, we introduce a Clustering-then-Retrieval (CR) strategy, where
coarse masks are first generated through clustering, facilitating subsequent
histogram-based image filtering and cross-category retrieval to produce
high-confidence prototypes. In the KNN retrieval stage, to alleviate the effect
of artifacts in feature maps, we propose Multi-View KNN Retrieval (MVKR), which
integrates retrieval results from diverse views to produce more robust and
precise pseudo-masks. Extensive experiments demonstrate that RISE outperforms
state-of-the-art unsupervised and prompt-based methods. Code is available at
https://github.com/xiaohainku/RISE.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供对Ji Du等人撰写的论文“Beyond Single Images: Retrieval Self-Augmented Unsupervised Camouflaged Object Detection”的全面摘要。</p>
<hr />
<p><strong>论文摘要：超越单图像：检索自增强无监督伪装目标检测</strong></p>
<p><strong>1. 解决的主要问题或研究问题</strong>
该论文主要解决伪装目标检测（COD）中的核心挑战：如何从与周围环境高度相似的背景中准确分割出伪装物体。现有方法主要依赖于单图像建模或基于标注的优化，但这些方法未能充分利用数据集层面的上下文信息，且通常需要耗费大量人工标注。因此，论文旨在探索一种无需标注、能够有效利用数据集全局信息来生成高质量伪装物体伪标签的无监督方法。</p>
<p><strong>2. 关键创新或方法论贡献</strong>
论文提出了名为 <strong>RISE (RetrIeval SElf-augmented)</strong> 的新范式，其核心创新包括：</p>
<ul>
<li><strong>数据集层面的上下文利用：</strong> 与以往依赖单图像特征相似性的方法不同，RISE通过构建原型库来利用整个训练数据集的全局信息，从而更好地区分伪装物体与背景。</li>
<li><strong>聚类-然后-检索 (Clustering-then-Retrieval, CR) 策略：</strong> 针对无标注下构建高质量原型库的挑战，CR策略首先通过谱聚类生成粗略掩码，初步区分伪装物体和环境。随后，通过基于直方图的图像过滤和跨类别检索，从局部特征中提取出高置信度的前景和背景原型。值得注意的是，前景原型选择的是与全局背景特征相似度最低的特征，以增强区分度。</li>
<li><strong>多视角KNN检索 (Multi-View KNN Retrieval, MVKR)：</strong> 为了缓解特征图中伪影对检索结果的影响，MVKR通过整合来自不同视角的图像（通过翻转、旋转等变换生成）的检索结果，并通过投票机制融合，生成更鲁棒和精确的伪掩码，避免了模型微调的需要。</li>
<li><strong>高效的伪标签生成：</strong> RISE能够以小时级而非天级的时间完成整个数据集的伪标签生成，且GPU内存使用量远低于基于Prompt的方法。</li>
</ul>
<p><strong>3. 主要结果及其意义</strong>
*   <strong>超越现有无监督方法：</strong> 实验结果表明，RISE在CHAMELEON、CAMO、COD10K和NC4K等多个基准数据集上，性能显著优于最先进的无监督和基于Prompt的方法。在COD10K数据集上，RISE在E<script type="math/tex">和F</script>指标上至少提升了8%和9%。
*   <strong>数据集层面信息的有效性：</strong> RISE的性能提升证明了充分利用数据集层面的语义信息对于区分高度相似的前景和背景至关重要。
*   <strong>CR模块的有效性：</strong> 跨类别检索和基于直方图的图像过滤显著提升了RISE的性能，并减少了原型库的规模，提高了推理效率。
*   <strong>MVKR的鲁棒性：</strong> MVKR通过多视角融合，有效减轻了特征图伪影的影响，生成了更稳健的伪掩码。
*   <strong>对小目标的良好定位能力：</strong> 论文的定性比较展示了RISE在复杂环境中准确定位和分割伪装物体（包括小目标）的能力。
*   <strong>超参数和数据集规模的鲁棒性：</strong> RISE对top-K超参数不敏感，并且即使只使用10%的训练图像构建原型库，也能保持一致且鲁棒的性能，这凸显了其在数据有限场景下的效率和有效性。
*   <strong>即插即用特性：</strong> RISE的方法可以与不同的聚类方法（如KMeans、GMM、HCA）以及其他无监督方法集成，并显著提升它们的性能。</p>
<p><strong>4. 论文中提及的局限性</strong>
论文中没有明确提及RISE方法的具体局限性，但可以从其设计和实验中推断出一些潜在的方面：
*   <strong>对基础模型的依赖：</strong> RISE依赖于预训练的自监督模型（如DINOv2）来提取特征。这些基础模型的性能和泛化能力会直接影响RISE的伪标签质量。如果基础模型在某些特定伪装场景下表现不佳，RISE的性能也可能受限。
*   <strong>聚类和原型选择的敏感性：</strong> 尽管论文提出了CR策略来提高原型质量，但聚类过程（特别是粗略掩码的生成）仍可能引入噪声，从而影响原型库的准确性。原型选择（例如选择与背景最不相似的前景特征）的有效性也依赖于特征空间中区分度的存在。
*   <strong>计算成本：</strong> 尽管RISE在伪标签生成时间上优于基于Prompt的方法，但构建原型库和进行KNN检索（尤其是在大规模数据集上）仍然需要一定的计算资源和时间。FAISS库的使用缓解了检索效率问题，但整体流程仍需一定开销。</p>
<p><strong>5. 潜在的未来研究方向</strong>
*   <strong>更先进的特征提取器：</strong> 探索使用更先进的自监督或基础模型作为特征提取器，以获取更具区分度和鲁棒性的特征表示，从而进一步提升伪标签质量。
*   <strong>自适应原型更新机制：</strong> 研究动态或自适应的原型更新机制，以应对数据集中的类别不平衡或新出现的伪装模式，使原型库能够随时间演化并保持高置信度。
*   <strong>多模态信息融合：</strong> 结合文本、深度或其他模态信息，进一步增强伪装物体和环境之间的区分能力，尤其是在视觉信息高度相似的极端伪装场景下。
*   <strong>端到端学习：</strong> 探索将RISE的伪标签生成过程与COD模型训练相结合的端到端学习框架，可能通过迭代优化伪标签和模型性能。
*   <strong>泛化能力评估：</strong> 对RISE在更广泛、更多样化的伪装场景（例如不同类型的伪装、不同环境）下的泛化能力进行深入评估。
*   <strong>理论分析：</strong> 对RISE中数据集层面上下文信息利用的有效性进行更深入的理论分析，以理解其在区分高度相似前景和背景方面的内在机制。</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this paper, we propose RISE, a RetrIeval SElf-augmented
paradigm that exploits the entire training dataset to generate pseudo-labels
for single images, which could be used to train COD models.</li>
<li>In
this light, we introduce a Clustering-then-Retrieval (CR) strategy, where
coarse masks are first generated through clustering, facilitating subsequent
histogram-based image filtering and cross-category retrieval to produce
high-confidence prototypes.</li>
<li>In the KNN retrieval stage, to alleviate the effect
of artifacts in feature maps, we propose Multi-View KNN Retrieval (MVKR), which
integrates retrieval results from diverse views to produce more robust and
precise pseudo-masks.</li>
<li>Extensive experiments demonstrate that RISE outperforms
state-of-the-art unsupervised and prompt-based methods.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.18437v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.18437v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.18377v1'></a></p>
<h2 id="cross-modal-scene-semantic-alignment-for-image-complexity-assessment"><a href="https://arxiv.org/abs/2510.18377v1">Cross-Modal Scene Semantic Alignment for Image Complexity Assessment</a></h2>
<p><strong>Authors:</strong> Yuqing Luo, Yixiao Li, Jiang Liu, Jun Fu, Hadi Amirpour, Guanghui Yue, Baoquan Zhao, Padraig Corcoran, Hantao Liu, Wei Zhou</p>
<p><strong>Published:</strong> 2025-10-21</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Image complexity assessment (ICA) is a challenging task in perceptual
evaluation due to the subjective nature of human perception and the inherent
semantic diversity in real-world images. Existing ICA methods predominantly
rely on hand-crafted or shallow convolutional neural network-based features of
a single visual modality, which are insufficient to fully capture the perceived
representations closely related to image complexity. Recently, cross-modal
scene semantic information has been shown to play a crucial role in various
computer vision tasks, particularly those involving perceptual understanding.
However, the exploration of cross-modal scene semantic information in the
context of ICA remains unaddressed. Therefore, in this paper, we propose a
novel ICA method called Cross-Modal Scene Semantic Alignment (CM-SSA), which
leverages scene semantic alignment from a cross-modal perspective to enhance
ICA performance, enabling complexity predictions to be more consistent with
subjective human perception. Specifically, the proposed CM-SSA consists of a
complexity regression branch and a scene semantic alignment branch. The
complexity regression branch estimates image complexity levels under the
guidance of the scene semantic alignment branch, while the scene semantic
alignment branch is used to align images with corresponding text prompts that
convey rich scene semantic information by pair-wise learning. Extensive
experiments on several ICA datasets demonstrate that the proposed CM-SSA
significantly outperforms state-of-the-art approaches. Codes are available at
https://github.com/XQ2K/First-Cross-Model-ICA.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Yuqing Luo等人撰写的论文“Cross-Modal Scene Semantic Alignment for Image Complexity Assessment”的全面摘要。</p>
<hr />
<p><strong>论文摘要：跨模态场景语义对齐用于图像复杂度评估</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
图像复杂度评估（ICA）是一个具有挑战性的任务，因为它涉及人类感知的主观性以及真实世界图像固有的语义多样性。现有的ICA方法主要依赖于单一视觉模态的手工特征或浅层卷积神经网络特征，这些特征不足以充分捕捉与图像复杂度密切相关的感知表示。尽管跨模态场景语义信息在各种计算机视觉任务（特别是涉及感知理解的任务）中已被证明至关重要，但在ICA背景下，对跨模态场景语义信息的探索仍未得到解决。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
为了解决上述问题，本文提出了一种新颖的ICA方法，称为<strong>跨模态场景语义对齐（CM-SSA）</strong>。其核心创新在于：
*   <strong>首次引入跨模态方法进行ICA：</strong> CM-SSA是第一个利用跨模态场景语义对齐来增强ICA性能的方法，旨在使复杂度预测更符合人类主观感知。
*   <strong>双分支框架：</strong> CM-SSA包含两个主要分支：
    *   <strong>复杂度回归分支：</strong> 该分支直接预测图像复杂度级别。它利用图像-提示对学习，其中提示是粗粒度的复杂度类别（如“高复杂度”、“中等复杂度”等）。
    *   <strong>场景语义对齐分支：</strong> 该分支通过将图像与由InstructBLIP等视觉-语言模型自动生成的文本提示（传达丰富的场景语义信息，如运动、文化、情绪、空间位置和数量）进行对齐，来精炼图像特征。这种对齐通过成对学习实现，旨在将图像特征与高层文本驱动的场景语义对齐。
*   <strong>利用预训练视觉-语言模型：</strong> 通过InstructBLIP生成场景语义描述，解决了现有ICA数据集缺乏此类语义标注的问题，从而能够将跨模态信息整合到模型中。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>卓越的性能：</strong> 在IC9600、VISC-C和SAVOIAS等多个ICA数据集上进行的广泛实验表明，所提出的CM-SSA显著优于现有最先进的方法，包括手工特征方法、深度学习ICA模型以及先进的图像质量评估（IQA）模型。
*   <strong>一致性与人类感知：</strong> CM-SSA通过跨模态场景语义对齐，使复杂度预测与人类主观感知更加一致。
*   <strong>泛化能力：</strong> 跨数据集验证结果显示，CM-SSA在大多数跨数据集场景中表现出更强的鲁棒性，尤其是在IC9600上训练并在VISC-C上测试时，取得了最高的SRCC和PLCC值。
*   <strong>消融研究：</strong> 消融研究验证了每个分支的有效性。结果表明，复杂度回归分支在预测中起着重要作用，而场景语义对齐分支则通过精炼图像特征来补充其作用，两者结合取得了最佳性能。提示级别和长度的分析也揭示了其对模型性能的影响。</p>
<p><strong>4. 论文中提到的局限性：</strong>
*   <strong>数据集限制：</strong> 论文提到，CM-SSA在VISC-C数据集上训练并在其他数据集上测试时，泛化性能有所下降。这可能是由于VISC-C数据集的规模和多样性有限，特别是考虑到CM-SSA中相对较多的参数使其更容易受到小数据集泛化到大数据集的影响。
*   <strong>语义描述的完美对齐假设：</strong> 损失计算假设InstructBLIP生成的文本描述与相应图像完美对齐。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
论文中未明确提出未来的研究方向，但根据其内容，可以推断出以下几点：
*   <strong>更鲁棒的语义描述生成：</strong> 探索更先进或更鲁棒的视觉-语言模型，以生成更准确、更细致的场景语义描述，从而进一步提高对齐分支的性能。
*   <strong>自适应权重调整：</strong> 进一步研究超参数α和β的自适应调整机制，以更好地平衡复杂度回归分支和场景语义对齐分支的贡献，尤其是在不同数据集或复杂度分布下。
*   <strong>多模态融合策略：</strong> 探索除了当前成对学习之外的更复杂的多模态融合策略，以更深入地整合视觉和文本信息。
*   <strong>解释性研究：</strong> 深入研究CM-SSA模型内部，以更好地理解场景语义信息如何具体影响图像复杂度预测，从而提高模型的可解释性。
*   <strong>更广泛的应用：</strong> 将CM-SSA的跨模态对齐思想应用于其他需要主观感知评估的计算机视觉任务中。</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>Therefore, in this paper, we propose a
novel ICA method called Cross-Modal Scene Semantic Alignment (CM-SSA), which
leverages scene semantic alignment from a cross-modal perspective to enhance
ICA performance, enabling complexity predictions to be more consistent with
subjective human perception.</li>
<li>Extensive
experiments on several ICA datasets demonstrate that the proposed CM-SSA
significantly outperforms state-of-the-art approaches.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.18377v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.18377v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.18353v1'></a></p>
<h2 id="ranking-based-preference-optimization-for-diffusion-models-from-implicit-user-feedback"><a href="https://arxiv.org/abs/2510.18353v1">Ranking-based Preference Optimization for Diffusion Models from Implicit User Feedback</a></h2>
<p><strong>Authors:</strong> Yi-Lun Wu, Bo-Kai Ruan, Chiang Tseng, Hong-Han Shuai</p>
<p><strong>Published:</strong> 2025-10-21</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Direct preference optimization (DPO) methods have shown strong potential in
aligning text-to-image diffusion models with human preferences by training on
paired comparisons. These methods improve training stability by avoiding the
REINFORCE algorithm but still struggle with challenges such as accurately
estimating image probabilities due to the non-linear nature of the sigmoid
function and the limited diversity of offline datasets. In this paper, we
introduce Diffusion Denoising Ranking Optimization (Diffusion-DRO), a new
preference learning framework grounded in inverse reinforcement learning.
Diffusion-DRO removes the dependency on a reward model by casting preference
learning as a ranking problem, thereby simplifying the training objective into
a denoising formulation and overcoming the non-linear estimation issues found
in prior methods. Moreover, Diffusion-DRO uniquely integrates offline expert
demonstrations with online policy-generated negative samples, enabling it to
effectively capture human preferences while addressing the limitations of
offline data. Comprehensive experiments show that Diffusion-DRO delivers
improved generation quality across a range of challenging and unseen prompts,
outperforming state-of-the-art baselines in both both quantitative metrics and
user studies. Our source code and pre-trained models are available at
https://github.com/basiclab/DiffusionDRO.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将对这篇论文摘要进行深入分析。</p>
<hr />
<p><strong>论文摘要分析：Ranking-based Preference Optimization for Diffusion Models from Implicit User Feedback</strong></p>
<p><strong>1. 论文主要贡献的简洁总结 (2-3 句话)</strong></p>
<p>这篇论文提出了一种名为 Diffusion Denoising Ranking Optimization (Diffusion-DRO) 的新型偏好学习框架，用于将文本到图像扩散模型与人类偏好对齐。它通过将偏好学习重新定义为排序问题，并将其训练目标简化为去噪公式，从而避免了对奖励模型的依赖以及现有方法中非线性估计的挑战。Diffusion-DRO 独特地结合了离线专家演示和在线策略生成的负样本，显著提升了生成质量。</p>
<p><strong>2. 关键创新或方法论方法</strong></p>
<ul>
<li><strong>将偏好学习转化为排序问题和去噪公式：</strong> 这是核心创新。传统的 DPO 方法依赖于估计图像概率，这在扩散模型中由于 sigmoid 函数的非线性特性而变得困难。Diffusion-DRO 通过将偏好学习重新构建为排序任务，并将其目标简化为去噪过程，完全规避了对奖励模型的依赖和复杂的概率估计问题。这使得训练更加稳定和高效。</li>
<li><strong>结合离线专家演示与在线策略生成的负样本：</strong> 现有方法常受限于离线数据集的多样性。Diffusion-DRO 通过整合高质量的离线“专家”样本（正样本）和模型自身在线生成的“负”样本，有效地扩展了训练数据，并能更好地捕捉人类偏好，同时克服了离线数据多样性不足的限制。</li>
<li><strong>基于逆强化学习 (Inverse Reinforcement Learning, IRL) 的基础：</strong> 摘要指出该框架植根于 IRL，这意味着它试图从观察到的偏好中推断出潜在的奖励函数，但又巧妙地避免了显式地学习这个奖励函数，而是直接优化排序。</li>
</ul>
<p><strong>3. 对领域潜在影响</strong></p>
<ul>
<li><strong>提升扩散模型的人类偏好对齐能力：</strong> Diffusion-DRO 提供了一种更稳定、更有效的方法来将扩散模型与人类偏好对齐，这将直接导致更高质量、更符合用户期望的图像生成。</li>
<li><strong>推动偏好学习在生成模型中的发展：</strong> 通过解决 DPO 方法在扩散模型中遇到的关键挑战（如概率估计和数据多样性），该工作为偏好学习在更广泛的生成模型中的应用开辟了新的途径。</li>
<li><strong>简化训练流程：</strong> 移除对奖励模型的依赖和复杂的非线性估计，将使偏好优化过程更易于实现和扩展。</li>
<li><strong>促进更具创造性和实用性的 AI 艺术和设计工具：</strong> 更好的偏好对齐意味着用户可以更轻松地获得他们想要的图像，从而使 AI 成为更强大的创意辅助工具。</li>
</ul>
<p><strong>4. 可能受益于这项研究的相关领域或应用</strong></p>
<ul>
<li><strong>文本到图像生成 (Text-to-Image Generation)：</strong> 这是最直接的应用，将显著改善 Stable Diffusion、DALL-E 等模型的生成质量和用户体验。</li>
<li><strong>图像编辑和风格迁移：</strong> 用户可以更精确地指导模型进行图像编辑或风格转换，以符合其审美偏好。</li>
<li><strong>个性化内容生成：</strong> 根据用户的隐式或显式反馈，生成更符合个人品味的图像、设计或艺术作品。</li>
<li><strong>多模态内容生成：</strong> 这种偏好优化方法可能推广到其他多模态生成任务，例如文本到视频、文本到3D模型等。</li>
<li><strong>强化学习和逆强化学习：</strong> 该方法在 IRL 框架下避免显式奖励模型学习的策略，可能为其他 IRL 应用提供新的思路。</li>
</ul>
<p><strong>5. 可以从摘要中推断出的任何局限性</strong></p>
<ul>
<li><strong>“隐式用户反馈”的定义和获取：</strong> 摘要中提到“从隐式用户反馈中进行排序偏好优化”，但没有详细说明如何获取或建模这种隐式反馈。这可能是一个实际部署中的挑战，例如，它是否需要用户点击、停留时间或其他行为数据？</li>
<li><strong>计算成本：</strong> 虽然避免了奖励模型，但结合“在线策略生成的负样本”可能意味着在训练过程中需要进行额外的推理或生成步骤，这可能会增加计算成本，尤其是在大规模模型上。</li>
<li><strong>泛化能力：</strong> 尽管实验表明在“具有挑战性和未见过的提示”上表现良好，但其在极端罕见或高度专业化领域的泛化能力仍需进一步验证。</li>
<li><strong>“去噪公式”的具体形式：</strong> 摘要没有详细说明这个去噪公式的具体数学形式，这可能隐藏了一些潜在的复杂性或假设。</li>
<li><strong>对“专家演示”的依赖：</strong> 离线专家演示的质量和多样性仍然是模型性能的关键因素。如果高质量的专家数据难以获取，可能会限制其效果。</li>
</ul>
<hr />
<p>总而言之，这篇论文通过提出 Diffusion-DRO，为扩散模型的人类偏好对齐提供了一个新颖且有前景的解决方案。它巧妙地规避了现有 DPO 方法的痛点，并有望在文本到图像生成领域带来显著的质量提升和更广泛的应用。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this paper, we
introduce Diffusion Denoising Ranking Optimization (Diffusion-DRO), a new
preference learning framework grounded in inverse reinforcement learning.</li>
<li>Comprehensive experiments show that Diffusion-DRO delivers
improved generation quality across a range of challenging and unseen prompts,
outperforming state-of-the-art baselines in both both quantitative metrics and
user studies.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.18353v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.18353v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.18341v1'></a></p>
<h2 id="vise-a-systematic-approach-to-vision-only-street-view-extrapolation"><a href="https://arxiv.org/abs/2510.18341v1">ViSE: A Systematic Approach to Vision-Only Street-View Extrapolation</a></h2>
<p><strong>Authors:</strong> Kaiyuan Tan, Yingying Shen, Haiyang Sun, Bing Wang, Guang Chen, Hangjun Ye</p>
<p><strong>Published:</strong> 2025-10-21</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Realistic view extrapolation is critical for closed-loop simulation in
autonomous driving, yet it remains a significant challenge for current Novel
View Synthesis (NVS) methods, which often produce distorted and inconsistent
images beyond the original trajectory. This report presents our winning
solution which ctook first place in the RealADSim Workshop NVS track at ICCV
2025. To address the core challenges of street view extrapolation, we introduce
a comprehensive four-stage pipeline. First, we employ a data-driven
initialization strategy to generate a robust pseudo-LiDAR point cloud, avoiding
local minima. Second, we inject strong geometric priors by modeling the road
surface with a novel dimension-reduced SDF termed 2D-SDF. Third, we leverage a
generative prior to create pseudo ground truth for extrapolated viewpoints,
providing auxilary supervision. Finally, a data-driven adaptation network
removes time-specific artifacts. On the RealADSim-NVS benchmark, our method
achieves a final score of 0.441, ranking first among all participants.</p>
<p><strong>Analysis:</strong></p>
<p>以下是对Kaiyuan Tan等人撰写的论文“ViSE: A Systematic Approach to Vision-Only Street-View Extrapolation”的全面摘要：</p>
<p><strong>1. 论文解决的主要问题或研究问题</strong></p>
<p>该论文旨在解决自动驾驶闭环仿真中街景视图外推的关键挑战。现有的新颖视图合成（NVS）方法在超出原始轨迹的视图外推时，往往会生成扭曲且不一致的图像。这限制了NVS在构建交互式、4D驾驶环境中的应用，因为驾驶日志固有的稀疏性使得大多数NVS方法擅长视图插值，但在视图外推方面表现不佳，导致几何失真和纹理真实感崩溃。</p>
<p><strong>2. 关键创新或方法论贡献</strong></p>
<p>ViSE提出了一种全面的四阶段管道，系统地解决了街景视图外推的核心挑战：</p>
<ul>
<li><strong>鲁棒的无LiDAR初始化策略：</strong> 针对LiDAR数据缺失导致3D高斯初始化困难的问题，ViSE采用了一种数据驱动的初始化策略，生成基于视觉的伪LiDAR点云。这提供了强大的几何先验，避免了局部最小值，并能更快地收敛3DGS，恢复更精细的细节，防止外推过程中出现不合理的几何失真。</li>
<li><strong>几何感知3D场景重建（2D-SDF）：</strong> 为了准确建模路面（包含关键视觉元素如路标和车道线），论文引入了一种新颖的降维2D符号距离函数（2D-SDF）。该方法强制路面具有局部平面先验和全局平滑的坡度过渡，从而在大的视点变化下提高几何一致性，并提高了优化效率。</li>
<li><strong>迭代伪真值生成：</strong> 针对未观测区域（如植被、路缘、建筑物）缺乏通用几何先验导致外推时出现严重失真或浮动伪影的问题，ViSE利用预训练的扩散模型作为生成性修复器，通过迭代生成伪真值图像，为外推视点提供辅助监督，有效修复伪影。</li>
<li><strong>数据驱动的时间不变性适应网络（TIA-Net）：</strong> 为了解决跨不同驾驶日志（在不同时间捕获）导致的光照、天气和瞬态物体（如水坑、阴影）变化带来的时间特异性伪影问题，ViSE引入了一个轻量级的时间不变性适应网络。该网络作为一个后处理模块，学习去除渲染图像中的时间特异性伪影，确保在不同条件下捕获的日志之间具有鲁棒且一致的性能。</li>
</ul>
<p><strong>3. 主要结果及其意义</strong></p>
<p>ViSE方法在RealADSim-NVS基准测试中取得了显著成果，最终得分0.441，在所有参与者中排名第一。消融研究进一步验证了每个组件的贡献：</p>
<ul>
<li><strong>2D-SDF路面先验：</strong> 显著提高了LPIPS（从0.513到0.500），表明模型成功减轻了几何失真，产生了更具结构一致性和真实感的输出。</li>
<li><strong>伪LiDAR初始化：</strong> 进一步提升了所有指标，缓解了训练视点的过拟合，帮助模型摆脱了糟糕的局部最小值，从而在远近场景中都获得了更准确的几何结构。</li>
<li><strong>带有生成先验的伪真值：</strong> 显著提升了性能，LPIPS相对改进了20%（从0.47到0.396）。通过为未观测区域提供明确监督，生成先验有效地“修复”了合理的场景内容，消除了浮动伪影，极大地增强了外推视图的视觉合理性。</li>
<li><strong>TIA网络：</strong> 实现了最高的改进，通过去除瞬态、时间特异性元素，TIA网络生成了稳定、时间无关的场景表示，这对于基准测试所需的时空外推的鲁棒性能至关重要。</li>
</ul>
<p>这些结果表明，ViSE的结构化、基于视觉的管道能够在外推视图中生成真实且几何一致的图像，达到了最先进的性能。</p>
<p><strong>4. 论文中提到的局限性</strong></p>
<p>论文中没有明确提及ViSE方法的具体局限性。然而，从其解决的问题和方法论来看，可能存在的隐性局限包括：</p>
<ul>
<li><strong>计算成本：</strong> 尽管2D-SDF提高了优化效率，但整个四阶段管道，特别是涉及扩散模型进行伪真值生成和数据驱动适应网络，可能仍然具有较高的计算成本，尤其是在大规模场景或实时应用中。</li>
<li><strong>生成先验的泛化能力：</strong> 尽管生成先验有助于修复伪影，但其效果可能受限于预训练扩散模型的泛化能力。在训练数据中未充分表示的极端或新颖场景下，生成模型可能会产生不真实的细节或幻觉。</li>
<li><strong>时间不变性适应的完整性：</strong> TIA网络旨在去除时间特异性伪影，但完全消除所有瞬态、时间相关的特征可能是一个持续的挑战，尤其是在非常动态或复杂的天气/光照条件下。</li>
<li><strong>对伪LiDAR点云质量的依赖：</strong> 尽管伪LiDAR点云提供了关键的几何初始化，但如果其初始质量较差或存在大量噪声，可能会影响后续3DGS的收敛和最终重建质量。</li>
</ul>
<p><strong>5. 潜在的未来研究方向</strong></p>
<p>基于本论文的工作，未来研究可以探索以下方向：</p>
<ul>
<li><strong>实时性能优化：</strong> 进一步优化管道的计算效率，使其更适用于实时自动驾驶仿真或在线场景重建。这可能涉及更轻量级的生成模型、更高效的3D表示或更快的适应网络。</li>
<li><strong>更强大的几何先验：</strong> 探索除了路面之外，其他场景元素（如建筑物、车辆）的更通用或自适应的几何先验，以进一步提高外推视图的几何一致性。</li>
<li><strong>多模态数据融合：</strong> 虽然论文强调了“仅视觉”的方法，但未来可以研究如何将其他传感器数据（如低分辨率LiDAR、雷达）以更智能的方式融入到初始化或重建阶段，以进一步增强鲁棒性和准确性，同时保持视觉主导的优势。</li>
<li><strong>动态场景建模：</strong> 论文主要关注静态场景的重建和外推，未来可以深入研究如何更有效地建模和外推动态物体（如行人、其他车辆），这对于自动驾驶仿真至关重要。</li>
<li><strong>可控的场景编辑：</strong> 结合生成模型的能力，探索在重建的场景中进行可控编辑，例如改变天气条件、添加或移除物体，以实现更灵活的仿真和测试。</li>
<li><strong>更精细的时间不变性：</strong> 进一步研究时间不变性适应网络，使其能够区分并保留场景中随时间变化的真实动态元素（如树叶摇曳），同时去除不必要的瞬态伪影。</li>
</ul>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Realistic view extrapolation is critical for closed-loop simulation in
autonomous driving, yet it remains a significant challenge for current Novel
View Synthesis (NVS) methods, which often produce distorted and inconsistent
images beyond the original trajectory.</li>
<li>To address the core challenges of street view extrapolation, we introduce
a comprehensive four-stage pipeline.</li>
<li>Second, we inject strong geometric priors by modeling the road
surface with a novel dimension-reduced SDF termed 2D-SDF.</li>
<li>On the RealADSim-NVS benchmark, our method
achieves a final score of 0.441, ranking first among all participants.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.18341v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.18341v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.18321v1'></a></p>
<h2 id="beyond-single-models-mitigating-multimodal-hallucinations-via-adaptive-token-ensemble-decoding"><a href="https://arxiv.org/abs/2510.18321v1">Beyond Single Models: Mitigating Multimodal Hallucinations via Adaptive Token Ensemble Decoding</a></h2>
<p><strong>Authors:</strong> Jinlin Li, Yuran Wang, Yifei Yuan, Xiao Zhou, Yingying Zhang, Xixian Yong, Yefeng Zheng, Xian Wu</p>
<p><strong>Published:</strong> 2025-10-21</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Large Vision-Language Models (LVLMs) have recently achieved impressive
results in multimodal tasks such as image captioning and visual question
answering. However, they remain prone to object hallucination -- generating
descriptions of nonexistent or misidentified objects. Prior work has partially
mitigated this via auxiliary training objectives or external modules, but
challenges remain in terms of scalability, adaptability, and model
independence. To address these limitations, we propose Adaptive Token Ensemble
Decoding (ATED), a training-free, token-level ensemble framework that mitigates
hallucination by aggregating predictions from multiple LVLMs during inference.
ATED dynamically computes uncertainty-based weights for each model, reflecting
their reliability at each decoding step. It also integrates diverse decoding
paths to improve contextual grounding and semantic consistency. Experiments on
standard hallucination detection benchmarks demonstrate that ATED significantly
outperforms state-of-the-art methods, reducing hallucination without
compromising fluency or relevance. Our findings highlight the benefits of
adaptive ensembling and point to a promising direction for improving LVLM
robustness in high-stakes applications. The code is available at
https://github.com/jinlin2021/ATED.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于Jinlin Li等人撰写的论文“Beyond Single Models: Mitigating Multimodal Hallucinations via Adaptive Token Ensemble Decoding”的全面摘要：</p>
<p><strong>论文摘要：超越单一模型：通过自适应令牌集成解码缓解多模态幻觉</strong></p>
<p><strong>1. 解决的主要问题或研究问题：</strong>
该论文旨在解决大型视觉-语言模型（LVLMs）中普遍存在的“对象幻觉”问题。尽管LVLMs在图像字幕和视觉问答等多模态任务中取得了显著进展，但它们仍然容易生成不存在或被错误识别的对象的描述。现有方法在可扩展性、适应性和模型独立性方面存在挑战。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
为了克服现有方法的局限性，作者提出了<strong>自适应令牌集成解码（Adaptive Token Ensemble Decoding, ATED）</strong>。ATED是一个无需训练、令牌级别的集成框架，通过在推理过程中聚合来自多个LVLMs的预测来缓解幻觉。其主要创新点包括：
*   <strong>训练无关的令牌级融合：</strong> ATED无需额外训练，通过细粒度的令牌级融合来缓解幻觉。
*   <strong>不确定性引导的加权机制：</strong> ATED动态计算基于不确定性的模型权重，反映每个模型在每个解码步骤的可靠性。它通过最小化整体不确定性来优化权重分配。
*   <strong>多路径对比解码策略：</strong> 该方法整合了多样化的解码路径，通过应用高斯噪声掩码生成图像的扰动变体，增强了上下文基础和语义一致性，从而提高了模型在视觉不确定条件下的鲁棒性。
*   <strong>不确定性贪婪优化：</strong> 引入了一种高效的贪婪优化算法，通过迭代地调整模型权重来最小化集成的不确定性，并设置了早停标准以避免冗余计算。</p>
<p><strong>3. 主要结果及其重要性：</strong>
*   <strong>显著优于现有方法：</strong> 在标准幻觉检测基准（如POPE、CHAIR和MME）上的实验表明，ATED显著优于最先进的方法，在不损害流畅性或相关性的前提下减少了幻觉。
*   <strong>POPE基准：</strong> ATED在Accuracy和F1-score上实现了4.20%-6.29%和6.29%-6.97%的提升，并持续超越了ICD和VCD等现有方法。
*   <strong>CHAIR基准：</strong> ATED在CHAIRs指标上表现出色，比最强基线提高了21.13%-41.24%，在生成长度增加时仍保持最佳性能。
*   <strong>MME基准：</strong> ATED在对象属性级别的幻觉检测（包括存在识别、数量判断、位置识别和颜色分类）中表现出最高性能，Accuracy+指标提升至少+61.7%和+54.2%。
*   <strong>鲁棒性和适应性：</strong> 实验结果强调了自适应集成的好处，并证明了ATED在各种多模态任务中具有强大的适应性和可扩展性。</p>
<p><strong>4. 论文中提到的局限性：</strong>
*   <strong>推理延迟与生成性能的权衡：</strong> 尽管ATED在准确性方面表现出色，但与默认方法相比，它会引入一定的推理延迟。虽然ATED提供了灵活的权衡机制，但仍需在效率和性能之间进行平衡。
*   <strong>模型独立性：</strong> 尽管ATED旨在提高模型独立性，但其性能仍可能受到集成中LVLMs之间性能差距的影响。当模型之间性能差距较大时，简单的统一平均可能无法有效提升性能。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>进一步优化推理效率：</strong> 探索更高效的集成策略或优化算法，以在保持高性能的同时进一步减少推理延迟。
*   <strong>更广泛的模型集成：</strong> 扩展ATED以集成更多样化的LVLMs，包括不同架构和规模的模型，以进一步提升鲁棒性和泛化能力。
*   <strong>动态调整超参数：</strong> 研究更智能的机制来动态调整超参数（如视觉对比解码强度α和不确定性贪婪优化步长s），以适应不同的任务和数据特性。
*   <strong>探索其他类型的幻觉：</strong> 将ATED应用于缓解除对象幻觉之外的其他类型的多模态幻觉，例如属性幻觉和关系幻觉。
*   <strong>高风险应用：</strong> 进一步探索ATED在高风险应用（如自动驾驶和医学图像分析）中的潜力，其中事实正确性和视觉基础至关重要。</p>
<p>总而言之，这篇论文提出了一种新颖且高效的训练无关方法ATED，通过自适应地集成多个LVLMs的预测，显著缓解了多模态幻觉问题，为提升LVLMs在实际应用中的鲁棒性开辟了新的方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address these limitations, we propose Adaptive Token Ensemble
Decoding (ATED), a training-free, token-level ensemble framework that mitigates
hallucination by aggregating predictions from multiple LVLMs during inference.</li>
<li>Experiments on
standard hallucination detection benchmarks demonstrate that ATED significantly
outperforms state-of-the-art methods, reducing hallucination without
compromising fluency or relevance.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.18321v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.18321v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-10-22 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
