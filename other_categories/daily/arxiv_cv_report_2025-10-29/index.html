<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-10-29 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DEâ«¶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Birdâs-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">è®ºæéè¯»</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-Î±
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-10-24/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-10-30/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-10-29">Arxiv Computer Vision Papers - 2025-10-29</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#generative-view-stitching" class="nav-link">Generative View Stitching</a>
                </li>
                <li class="nav-item">
                    <a href="#uniform-discrete-diffusion-with-metric-path-for-video-generation" class="nav-link">Uniform Discrete Diffusion with Metric Path for Video Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#mic-bev-multi-infrastructure-camera-birds-eye-view-transformer-with-relation-aware-fusion-for-3d-object-detection" class="nav-link">MIC-BEV: Multi-Infrastructure Camera Bird's-Eye-View Transformer with Relation-Aware Fusion for 3D Object Detection</a>
                </li>
                <li class="nav-item">
                    <a href="#group-relative-attention-guidance-for-image-editing" class="nav-link">Group Relative Attention Guidance for Image Editing</a>
                </li>
                <li class="nav-item">
                    <a href="#groundloc-efficient-large-scale-outdoor-lidar-only-localization" class="nav-link">GroundLoc: Efficient Large-Scale Outdoor LiDAR-Only Localization</a>
                </li>
                <li class="nav-item">
                    <a href="#latent-sketchpad-sketching-visual-thoughts-to-elicit-multimodal-reasoning-in-mllms" class="nav-link">Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal Reasoning in MLLMs</a>
                </li>
                <li class="nav-item">
                    <a href="#kineo-calibration-free-metric-motion-capture-from-sparse-rgb-cameras" class="nav-link">Kineo: Calibration-Free Metric Motion Capture From Sparse RGB Cameras</a>
                </li>
                <li class="nav-item">
                    <a href="#a-hybrid-approach-for-visual-multi-object-tracking" class="nav-link">A Hybrid Approach for Visual Multi-Object Tracking</a>
                </li>
                <li class="nav-item">
                    <a href="#gentrack-a-new-generation-of-multi-object-tracking" class="nav-link">GenTrack: A New Generation of Multi-Object Tracking</a>
                </li>
                <li class="nav-item">
                    <a href="#scope-saliency-coverage-oriented-token-pruning-for-efficient-multimodel-llms" class="nav-link">SCOPE: Saliency-Coverage Oriented Token Pruning for Efficient Multimodel LLMs</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-10-29">Arxiv Computer Vision Papers - 2025-10-29</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>å¥½çï¼è¿æ¯ä¸ä»½éå¯¹æ¨æä¾ç Arxiv è®ºæåè¡¨çæ¯æ¥æ¥åæ§è¡æè¦ï¼æ¨å¨å¸®å©å¿ç¢çç ç©¶äººåå¿«éäºè§£è®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçææ°è¿å±ã</p>
<hr />
<p><strong>æ¯æ¥ Arxiv è®¡ç®æºè§è§ä¸æºå¨å­¦ä¹ æ¥åæ§è¡æè¦ (2025-10-28)</strong></p>
<p><strong>æ¦è¿°ä¸ä¸»è¦è¶å¿ï¼</strong></p>
<p>ä»å¤©çæ¥åæ¶µçäºè®¡ç®æºè§è§é¢åå¹¿æ³èæ´»è·çç ç©¶ï¼ä¸»è¦è¶å¿éä¸­å¨<strong>çææ¨¡åä¸å¤æ¨¡æåºç¨</strong>ã<strong>3D æç¥ä¸èªå¨é©¾é©¶</strong>ä»¥å<strong>æçä¸ä¼å</strong>ãçææ¨¡åå¨å¾åãè§é¢åå¤æ¨¡ææ¨çæ¹é¢æç»­åå¾çªç ´ï¼ç¹å«æ¯å¨è§å¾åæãè§é¢çæåå¾åç¼è¾æ¹é¢ã3D æç¥åèªå¨é©¾é©¶é¢ååä¾§éäºå¤ä¼ æå¨èåãé«æå®ä½åç®æ æ£æµãæ­¤å¤ï¼ä¸ºäºåºå¯¹å¤§åæ¨¡åå¸¦æ¥çè®¡ç®ææï¼ç ç©¶äººåæ­£ç§¯ææ¢ç´¢æ¨¡ååªæåé«ææ¶æã</p>
<p><strong>ç¹å«éè¦æåæ°çè®ºæï¼</strong></p>
<ol>
<li><strong>"Generative View Stitching" (Chonghyuk Song et al.)</strong>: è¿ç¯è®ºæå¨<strong>æ°é¢è§å¾åæ</strong>æ¹é¢å±ç°äºæ¾èçåæ°ãå®å¯è½éè¿çææ¨¡åå°ä¸åè§è§çå¾åæ ç¼æ¼æ¥ï¼è§£å³ä¼ ç»æ¹æ³å¨è§è§ä¸ä¸è´æ§æé®æ¡æ¹é¢çææï¼å¯¹äºèæç°å®ã3D éå»ºååå®¹åä½å·æå·¨å¤§æ½åã</li>
<li><strong>"Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal Reasoning in MLLMs" (Huanyu Zhang et al.)</strong>: è¿æ¯ä¸é¡¹<strong>å¤æ¨¡æå¤§è¯­è¨æ¨¡å (MLLMs) äº¤äº</strong>çå¼åæ§å·¥ä½ãéè¿å¼å¥âæ½å¨ç»æ¿âçæ¦å¿µï¼åè®¸ç¨æ·ä»¥è§è§èå¾çå½¢å¼å¼å¯¼ MLLMs è¿è¡æ¨çï¼æå¤§å°å¢å¼ºäºäººæºäº¤äºçç´è§æ§åè¡¨è¾¾åï¼ä¸º MLLMs çåºç¨æå¼äºæ°çå¤§é¨ã</li>
<li><strong>"Uniform Discrete Diffusion with Metric Path for Video Generation" (Haoge Deng et al.)</strong>: å¨<strong>è§é¢çæ</strong>é¢åï¼è¿ç¯è®ºæå¯è½éè¿å¼å¥æ°çæ©æ£æ¨¡åèå¼ï¼è§£å³äºè§é¢çæä¸­æ¶é´ä¸è´æ§åè´¨éçé¾é¢ï¼æ¯è§é¢åå®¹åä½ååæçéè¦è¿å±ã</li>
</ol>
<p><strong>æ°å´ç ç©¶æ¹åæææ¯ï¼</strong></p>
<ol>
<li><strong>å¤æ¨¡æå¤§è¯­è¨æ¨¡å (MLLMs) çäº¤äºä¸æ¨çå¢å¼º</strong>: "Latent Sketchpad" æç¡®æåº MLLMs ä¸ä»ä»æ¯çè§£å¤æ¨¡æè¾å¥ï¼æ´è¦è½éè¿å¤æ¨¡æè¾åºï¼å¦èå¾ï¼è¿è¡äº¤äºåå¼å¯¼æ¨çï¼è¿æ¯æªæ¥ MLLMs åå±çéè¦æ¹åã</li>
<li><strong>é«æä¸é²æ£ç 3D æç¥ä¸å®ä½</strong>: "MIC-BEV" å "GroundLoc" å¼ºè°äºå¨å¤æç¯å¢ä¸­ï¼å©ç¨å¤åºç¡è®¾æ½æåå¤´èåå LiDAR-only çé«æå®ä½ï¼æ¯èªå¨é©¾é©¶åæºå¨äººé¢åæç»­å³æ³¨çç¦ç¹ã</li>
<li><strong>çææ¨¡åå¨å¤æä»»å¡ä¸­çåºç¨</strong>: ä»è§å¾æ¼æ¥ ("Generative View Stitching") å°è§é¢çæ ("Uniform Discrete Diffusion")ï¼çææ¨¡åæ­£è¢«åºç¨äºè¶æ¥è¶å¤æçè§è§ä»»å¡ï¼å¹¶å±ç°åºå¼ºå¤§çè½åã</li>
<li><strong>å¤§åæ¨¡åæçä¼å</strong>: "SCOPE" æåºç Saliency-Coverage Oriented Token Pruning ææ¯ï¼é¢ç¤ºçæªæ¥å¯¹å¤§åå¤æ¨¡ææ¨¡åè¿è¡é«æåªæåæ¨çä¼åçéæ±å°æ¥çå¢é¿ã</li>
</ol>
<p><strong>å»ºè®®éè¯»å¨æçè®ºæï¼</strong></p>
<p>ä¸ºäºå¨é¢äºè§£å½åé¢åçéè¦è¿å±ï¼å»ºè®®ä¼åéè¯»ä»¥ä¸è®ºæï¼</p>
<ol>
<li><strong>"Generative View Stitching"</strong>: å¯¹äºå³æ³¨æ°é¢è§å¾åæã3D åå®¹çæåçææ¨¡ååºç¨çè¯»èã</li>
<li><strong>"Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal Reasoning in MLLMs"</strong>: å¯¹äºå³æ³¨å¤æ¨¡æå¤§è¯­è¨æ¨¡åãäººæºäº¤äºåé«çº§æ¨ççè¯»èã</li>
<li><strong>"Uniform Discrete Diffusion with Metric Path for Video Generation"</strong>: å¯¹äºå³æ³¨è§é¢çæãæ©æ£æ¨¡ååæ¶é´åºåå»ºæ¨¡çè¯»èã</li>
<li><strong>"MIC-BEV: Multi-Infrastructure Camera Bird's-Eye-View Transformer with Relation-Aware Fusion for 3D Object Detection"</strong>: å¯¹äºå³æ³¨èªå¨é©¾é©¶ã3D ç®æ æ£æµåå¤ä¼ æå¨èåçè¯»èã</li>
<li><strong>"SCOPE: Saliency-Coverage Oriented Token Pruning for Efficient Multimodel LLMs"</strong>: å¯¹äºå³æ³¨å¤§åæ¨¡åæçãæ¨¡ååç¼©åä¼åæ¨ççè¯»èã</li>
</ol>
<hr />
<p>è¿ä»½æè¦æ¨å¨æä¾ä¸ä¸ªé«å±æ¬¡çæ¦è§ï¼å¸®å©æ¨å¿«éè¯å«ä¸æ¨ç ç©¶å´è¶£æç¸å³çè®ºæã</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2510.24718v1">Generative View Stitching</a></li>
<li><a href="#2510.24717v1">Uniform Discrete Diffusion with Metric Path for Video Generation</a></li>
<li><a href="#2510.24688v1">MIC-BEV: Multi-Infrastructure Camera Bird's-Eye-View Transformer with Relation-Aware Fusion for 3D Object Detection</a></li>
<li><a href="#2510.24657v1">Group Relative Attention Guidance for Image Editing</a></li>
<li><a href="#2510.24623v1">GroundLoc: Efficient Large-Scale Outdoor LiDAR-Only Localization</a></li>
<li><a href="#2510.24514v1">Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal Reasoning in MLLMs</a></li>
<li><a href="#2510.24464v1">Kineo: Calibration-Free Metric Motion Capture From Sparse RGB Cameras</a></li>
<li><a href="#2510.24410v1">A Hybrid Approach for Visual Multi-Object Tracking</a></li>
<li><a href="#2510.24399v1">GenTrack: A New Generation of Multi-Object Tracking</a></li>
<li><a href="#2510.24214v1">SCOPE: Saliency-Coverage Oriented Token Pruning for Efficient Multimodel LLMs</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2510.24718v1'></a></p>
<h2 id="generative-view-stitching"><a href="https://arxiv.org/abs/2510.24718v1">Generative View Stitching</a></h2>
<p><strong>Authors:</strong> Chonghyuk Song, Michal Stary, Boyuan Chen, George Kopanas, Vincent Sitzmann</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Categories:</strong> cs.CV, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Autoregressive video diffusion models are capable of long rollouts that are
stable and consistent with history, but they are unable to guide the current
generation with conditioning from the future. In camera-guided video generation
with a predefined camera trajectory, this limitation leads to collisions with
the generated scene, after which autoregression quickly collapses. To address
this, we propose Generative View Stitching (GVS), which samples the entire
sequence in parallel such that the generated scene is faithful to every part of
the predefined camera trajectory. Our main contribution is a sampling algorithm
that extends prior work on diffusion stitching for robot planning to video
generation. While such stitching methods usually require a specially trained
model, GVS is compatible with any off-the-shelf video model trained with
Diffusion Forcing, a prevalent sequence diffusion framework that we show
already provides the affordances necessary for stitching. We then introduce
Omni Guidance, a technique that enhances the temporal consistency in stitching
by conditioning on both the past and future, and that enables our proposed
loop-closing mechanism for delivering long-range coherence. Overall, GVS
achieves camera-guided video generation that is stable, collision-free,
frame-to-frame consistent, and closes loops for a variety of predefined camera
paths, including Oscar Reutersv\"ard's Impossible Staircase. Results are best
viewed as videos at https://andrewsonga.github.io/gvs.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Chonghyuk Songç­äººæ°åçè®ºæâGenerative View Stitchingâçå¨é¢æè¦ã</p>
<hr />
<h3 id="generative-view-stitching_1">ãGenerative View Stitchingãè®ºææè¦</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
å½åèªåå½è§é¢æ©æ£æ¨¡åå¨çæé¿åºåè§é¢æ¶ï¼è½ç¶è½ä¿æä¸åå²çç¨³å®æ§åä¸è´æ§ï¼ä½æ æ³å©ç¨æªæ¥çæ¡ä»¶ä¿¡æ¯æ¥æå¯¼å½åå¸§ççæãå¨é¢å®ä¹æåæºè½¨è¿¹çæåæºå¼å¯¼è§é¢çæä»»å¡ä¸­ï¼è¿ä¸éå¶å¯¼è´çæçåºæ¯ä¸æåæºè½¨è¿¹åçç¢°æï¼è¿èä½¿èªåå½çæè¿éå´©æºãè®ºææ¨å¨è§£å³è¿ä¸é®é¢ï¼å®ç°ä¸é¢å®ä¹æåæºè½¨è¿¹å®å¨ä¸è´ãæ ç¢°æãå¸§é´ä¸è´ä¸è½é­åå¾ªç¯çç¨³å®é¿åºåæåæºå¼å¯¼è§é¢çæã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong></p>
<ul>
<li><strong>Generative View Stitching (GVS) æ¡æ¶ï¼</strong> è®ºææåºäºGVSï¼ä¸ç§è®­ç»æ å³çæ©æ£æ¼æ¥æ¹æ³ï¼è½å¤å¹¶è¡éæ ·æ´ä¸ªè§é¢åºåï¼ç¡®ä¿çæçåºæ¯ä¸é¢å®ä¹çæåæºè½¨è¿¹çæ¯ä¸ªé¨åé½ä¿æä¸è´ã</li>
<li><strong>ä¸ç°ææ©æ£æ¨¡åçå¼å®¹æ§ï¼</strong> GVSçä¸é¡¹æ ¸å¿è´¡ç®æ¯ï¼å®å¼å®¹ä»»ä½ä½¿ç¨Diffusion Forcingï¼DFï¼æ¡æ¶è®­ç»çç°æè§é¢æ¨¡åï¼èæ éä¸é¨è®­ç»æ°çæ¨¡åãè®ºææåºï¼DFæ¡æ¶æ¬èº«å·²æä¾äºå®ç°æ¼æ¥æéçå¿è¦åè½ã</li>
<li><strong>Omni Guidanceï¼å¨æ¹ä½å¼å¯¼ï¼ï¼</strong> ä¸ºäºå¢å¼ºæ¼æ¥è¿ç¨ä¸­çæ¶é´ä¸è´æ§ï¼è®ºæå¼å¥äºOmni Guidanceææ¯ãè¯¥ææ¯éè¿åæ¶å©ç¨è¿å»åæªæ¥çæ¡ä»¶ä¿¡æ¯æ¥æå¯¼çæï¼ä»èå å¼ºäºæ¶é´è¿è´¯æ§ã</li>
<li><strong>å¾ªç¯é­åæºå¶ï¼</strong> Omni Guidanceè¿ä¸æ­¥æ¯æäºè®ºææåºçå¾ªç¯é­åæºå¶ï¼è¿å¯¹äºå®ç°é¿è·ç¦»è¿è´¯æ§è³å³éè¦ï¼å°¤å¶æ¯å¨æåæºè½¨è¿¹å½¢æé­åå¾ªç¯çåºæ¯ä¸­ãGVSéè¿âå¾ªç¯æ¡ä»¶åâï¼Cyclic Conditioningï¼å®ç°å¾ªç¯é­åï¼å³å¨å»åªè¿ç¨ä¸­äº¤æ¿ä½¿ç¨æ¶é´çªå£ï¼å³æ³¨æ¶é´é»å±ï¼åç©ºé´çªå£ï¼å³æ³¨æ¶é´é¥è¿ä½ç©ºé´æ¥è¿çé»å±ï¼ã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶éè¦æ§ï¼</strong></p>
<ul>
<li><strong>ç¨³å®ãæ ç¢°æçè§é¢çæï¼</strong> GVSæåå®ç°äºç¨³å®çãæ ç¢°æçæåæºå¼å¯¼è§é¢çæï¼è§£å³äºèªåå½æ¹æ³ä¸­å¸¸è§çç¢°æé®é¢ã</li>
<li><strong>å¸§é´ä¸è´æ§åé¿è·ç¦»è¿è´¯æ§ï¼</strong> è¯¥æ¹æ³å¨åç§é¢å®ä¹æåæºè·¯å¾ï¼åæ¬å¥¥æ¯å¡Â·è·¯éæ¯ç¦å¾·çâä¸å¯è½çæ¥¼æ¢¯âï¼ä¸ï¼å®ç°äºå¸§é´çé«åº¦ä¸è´æ§ï¼å¹¶è½ææå°é­åå¾ªç¯ï¼å±ç°åºåè¶çé¿è·ç¦»è¿è´¯æ§ã</li>
<li><strong>ä¼äºåºçº¿æ¹æ³ï¼</strong> å¨å®éåå®æ§è¯ä¼°ä¸­ï¼GVSå¨æ¶é´ä¸è´æ§ï¼F2FCï¼ãé¿è·ç¦»ä¸è´æ§ï¼LRCï¼åç¢°æé¿åï¼CAï¼æ¹é¢åä¼äºåå²å¼å¯¼èªåå½éæ ·ï¼ARï¼åStochSyncç­åºçº¿æ¹æ³ï¼åæ¶ä¿æäºå¯æ¯çè§é¢çæè´¨éã</li>
<li><strong>Omni Guidanceåéæºæ§çäºè¡¥ä½ç¨ï¼</strong> å®éªè¡¨æï¼Omni Guidanceå¨ä¸åéæºæ§æ°´å¹³ä¸é½è½å¢å¼ºæ¶é´ä¸è´æ§ï¼å¹¶æä¾äºé¢å¤ççµæ´»æ§æ¥åå°è¿åº¦å¹³æ»ï¼è¿å¨æ²¡æOmni Guidanceçæåµä¸ï¼å¢å éæºæ§è½ç¶è½æé«ä¸è´æ§ä½å¸¸å¯¼è´è¿åº¦å¹³æ»ã</li>
</ul>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong></p>
<ul>
<li><strong>å¤é¨å¾åæ¡ä»¶åçä¼ æ­å°é¾ï¼</strong> GVSå¨å°å¤é¨æä¾çä¸ä¸æå¸§ææä¼ æ­å°ç®æ è§é¢çå¶ä½é¨åæ¶å­å¨å°é¾ãä¸ä¸æå¸§çä¿¡æ¯ä¼ æ­èå´æéï¼å¯¼è´è§é¢ä¸­ä¸ååºåå¯è½åºç°ä¸è¿è´¯çåºæ¯ã</li>
<li><strong>å®½åºçº¿è§è§çå¾ªç¯é­åå¤±è´¥ï¼</strong> GVSå¨å¤çå®½åºçº¿è§è§ï¼ä¾å¦ï¼æåæºè½¨è¿¹åå«180åº¦è½¨éæ®µï¼æ¶ï¼æ æ³æåé­åå¾ªç¯ãè¿è¢«å½å äºæä½¿ç¨çDiffusion Forcingéª¨å¹²æ¨¡åæ¯å¨è§è§ååè¾å°çæ°æ®éï¼å¦RE10Kï¼ä¸è®­ç»çï¼å¯¼è´å¶å¨å¤çåå¸å¤ï¼out-of-distributionï¼çå®½åºçº¿æåæºæ¶è¡¨ç°ä¸ä½³ã</li>
<li><strong>ç»æç¸ä¼¼æåæºè½¨è¿¹æ®µçåºåå°é¾ï¼</strong> å¨æäºæåµä¸ï¼GVSé¾ä»¥åºåç»æç¸ä¼¼çæåæºè½¨è¿¹æ®µï¼ä¾å¦åä¸æ¥¼æ¢¯çèµ·ç¹ååä¸æ¥¼æ¢¯çç»ç¹ãè¿æºäºéª¨å¹²æ¨¡åæéçä¸ä¸æçªå£åå¯¹ç¸å¯¹å§¿æçä¾èµï¼å¯è½å¯¼è´æ­§ä¹ã</li>
</ul>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong></p>
<ul>
<li><strong>æ§å¶ä¿¡æ¯ä¼ æ­ï¼</strong> æ¢ç´¢éè¿è°èDiffusion Forcingéª¨å¹²æ¨¡åçæ¯å¸§åªå£°æ°´å¹³æ¥æ§å¶æ¼æ¥ä¸­çä¿¡æ¯ä¼ æ­ï¼ä»¥å®ç°æ´å¥½çå¤é¨å¾åæ¡ä»¶åãè§é¢è´¨éåç¨æ·å¯æ§æ§ã</li>
<li><strong>å¤è§è§æ°æ®éè®­ç»ï¼</strong> è®­ç»Diffusion Forcingéª¨å¹²æ¨¡åå¨å·ææ´å®½åºçº¿çå¤è§è§æ°æ®éï¼å¦DL3DVåScanNet++ï¼ä¸ï¼ä»¥è§£å³å®½åºçº¿è§è§å¾ªç¯é­åå¤±è´¥çé®é¢ã</li>
<li><strong>æ©å±æ¡ä»¶åå½¢å¼ï¼</strong> å°GVSæ©å±å°æ¥åå¶ä»å½¢å¼çæ¡ä»¶åï¼å¦ä¸ä¸æå¾ååææ¬ï¼ä»¥å¸®å©è§£å³ç»æç¸ä¼¼è½¨è¿¹æ®µçæ­§ä¹é®é¢ã</li>
<li><strong>æ°åºç¨é¢åï¼</strong> å°GVSåºç¨äºå¶ä»é¢åï¼å¦ç®æ å¯¼åçæºå¨äººè§åååå±æºå¨äººè§åã</li>
</ul>
<hr />
<p>æ»èè¨ä¹ï¼Chonghyuk Songç­äººçè¿ç¯è®ºææåºäºä¸ç§æ°é¢ä¸å®ç¨çGenerative View Stitching (GVS) æ¹æ³ï¼ä¸ºé¿åºåæåæºå¼å¯¼è§é¢çææä¾äºä¸ä¸ªå¼ºå¤§çè§£å³æ¹æ¡ãéè¿å¶è®­ç»æ å³çç¹æ§ãä¸ç°æDFæ¨¡åçå¼å®¹æ§ãOmni Guidanceä»¥åå¾ªç¯é­åæºå¶ï¼GVSå¨çæç¨³å®ãä¸è´ä¸æ ç¢°æçè§é¢æ¹é¢åå¾äºæ¾èè¿å±ï¼å¹¶ä¸ºæªæ¥å¨è§é¢çæåæºå¨äººè§åç­é¢åçè¿ä¸æ­¥ç ç©¶å¥ å®äºåºç¡ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address
this, we propose Generative View Stitching (GVS), which samples the entire
sequence in parallel such that the generated scene is faithful to every part of
the predefined camera trajectory.</li>
<li>Our main contribution is a sampling algorithm
that extends prior work on diffusion stitching for robot planning to video
generation.</li>
<li>While such stitching methods usually require a specially trained
model, GVS is compatible with any off-the-shelf video model trained with
Diffusion Forcing, a prevalent sequence diffusion framework that we show
already provides the affordances necessary for stitching.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.24718v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.24718v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.24717v1'></a></p>
<h2 id="uniform-discrete-diffusion-with-metric-path-for-video-generation"><a href="https://arxiv.org/abs/2510.24717v1">Uniform Discrete Diffusion with Metric Path for Video Generation</a></h2>
<p><strong>Authors:</strong> Haoge Deng, Ting Pan, Fan Zhang, Yang Liu, Zhuoyan Luo, Yufeng Cui, Wenxuan Wang, Chunhua Shen, Shiguang Shan, Zhaoxiang Zhang, Xinlong Wang</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Continuous-space video generation has advanced rapidly, while discrete
approaches lag behind due to error accumulation and long-context inconsistency.
In this work, we revisit discrete generative modeling and present Uniform
discRete diffuSion with metric pAth (URSA), a simple yet powerful framework
that bridges the gap with continuous approaches for the scalable video
generation. At its core, URSA formulates the video generation task as an
iterative global refinement of discrete spatiotemporal tokens. It integrates
two key designs: a Linearized Metric Path and a Resolution-dependent Timestep
Shifting mechanism. These designs enable URSA to scale efficiently to
high-resolution image synthesis and long-duration video generation, while
requiring significantly fewer inference steps. Additionally, we introduce an
asynchronous temporal fine-tuning strategy that unifies versatile tasks within
a single model, including interpolation and image-to-video generation.
Extensive experiments on challenging video and image generation benchmarks
demonstrate that URSA consistently outperforms existing discrete methods and
achieves performance comparable to state-of-the-art continuous diffusion
methods. Code and models are available at https://github.com/baaivision/URSA</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Haoge Dengç­äººæ°åçè®ºæâUniform Discrete Diffusion with Metric Path for Video Generationâçå¨é¢æè¦ã</p>
<hr />
<h3 id="uniform-discrete-diffusion-with-metric-path-for-video-generation_1">è®ºææè¦ï¼Uniform Discrete Diffusion with Metric Path for Video Generation</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³ç¦»æ£ç©ºé´è§é¢çææ¹æ³ä¸­å­å¨çä¸¤å¤§ææï¼è¯¯å·®ç´¯ç§¯åé¿ä¸ä¸æä¸ä¸è´æ§ï¼è¿å¯¼è´å¶æ§è½è½åäºè¿ç»­ç©ºé´è§é¢çææ¹æ³ãå·ä½æ¥è¯´ï¼ä¼ ç»çç¦»æ£æ¹æ³ï¼å¦èªåå½æ¨¡ååæ©ç æ©æ£æ¨¡åï¼éç¨ä¸å¯åä¼åçå±é¨çæï¼ä¸æ¦çæä»¤çä¾¿åºå®ä¸æ¥ï¼éå¶äºå¶çæé«è´¨éãé¿æ¶åºè§é¢çè½åã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
ä½èæåºäº <strong>Uniform discRete diffuSion with metric pAth (URSA)</strong>ï¼ä¸ä¸ªç®åèå¼ºå¤§çæ¡æ¶ï¼éè¿ä»¥ä¸å³é®åæ°å¼¥åäºç¦»æ£ä¸è¿ç»­æ¹æ³ä¹é´çå·®è·ï¼</p>
<ul>
<li><strong>è¿­ä»£å¨å±ç¦»æ£æ¶ç©ºä»¤çç»åï¼</strong> URSAå°è§é¢çæä»»å¡éæ°å®ä¹ä¸ºç¦»æ£æ¶ç©ºä»¤ççè¿­ä»£å¨å±ç»åè¿ç¨ãè¿ä½¿å¾ç¦»æ£æ¹æ³è½å¤æ¦å¿µä¸ä¸è¿ç»­æ¹æ³å¯¹é½ï¼ä»èæ¾èç¼©å°æ§è½å·®è·ã</li>
<li><strong>çº¿æ§ååº¦éè·¯å¾ (Linearized Metric Path)ï¼</strong> å¼å¥äºä¸ç§åºäºä»¤çåµå¥è·ç¦»çæ°åæ¦çè·¯å¾ï¼è½å¤å¯¹æ°æ®æ°å¨è¿è¡ç²¾ç¡®æ§å¶ï¼è¿å¯¹äºææå­¦ä¹ åå±æ°æ®æµå½¢è³å³éè¦ã</li>
<li><strong>åè¾¨çä¾èµçæ¶é´æ­¥é¿åç§»æºå¶ (Resolution-dependent Timestep Shifting)ï¼</strong> è¯¥æºå¶æ ¹æ®è§é¢åè¾¨çè°æ´æ¶é´æ­¥é¿ï¼ç¡®ä¿æ°å¨è¿ç¨è½æ ¹æ®åºåé¿åº¦ï¼å¦é«åè¾¨çå¾åæé¿è§é¢ï¼è¿è¡éå½è°æ´ï¼ä»èæé«è®­ç»ç¨³å®æ§åé¿è§é¢åºåçè¡¨ç¤ºå­¦ä¹ ã</li>
<li><strong>å¼æ­¥æ¶é´æ­¥é¿è°åº¦ç­ç¥ (Asynchronous Temporal Fine-tuning Strategy)ï¼</strong> éå¯¹å¤ä»»å¡è®­ç»åéæ ·ï¼è¯¥ç­ç¥åè®¸æ¯ä¸ªå¸§ç¬ç«éæ ·æ¶é´æ­¥é¿ãè¿ä½¿å¾URSAè½å¤å¨ä¸ä¸ªç»ä¸æ¨¡åä¸­å¤çå¤ç§ä»»å¡ï¼åæ¬è§é¢æå¼ãå¾åå°è§é¢çæãè§é¢å¤æ¨ä»¥åèµ·å§-ç»æå¸§æ§å¶ï¼å¹¶è½çæåéçº§é¿è§é¢ã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>æ§è½è¶è¶ç°æç¦»æ£æ¹æ³ï¼</strong> å¨å·ææææ§çè§é¢åå¾åçæåºåæµè¯ä¸­ï¼URSAå§ç»ä¼äºç°æçç¦»æ£æ¹æ³ã
*   <strong>ä¸æåè¿è¿ç»­æ©æ£æ¹æ³åª²ç¾ï¼</strong> URSAå¨æ§è½ä¸è¾¾å°äºä¸æåè¿çè¿ç»­æ©æ£æ¹æ³ç¸å½çæ°´å¹³ï¼å°¤å¶æ¯å¨ææ¬å°è§é¢çæä»»å¡ä¸­ï¼å¶VBenchå¾åè¾¾å°82.4ï¼å¨å¾åå°è§é¢çæä»»å¡ä¸­è¾¾å°86.2ï¼å¨ææ¬å°å¾åçæä»»å¡ä¸­DPG-Benchå¾åè¾¾å°86.0ã
*   <strong>é«æä¸å¯æ©å±ï¼</strong> URSAè½å¤é«æå°æ©å±å°é«åè¾¨çå¾ååæåé¿æ¶ç¨è§é¢çæï¼åæ¶æ¾èåå°æ¨çæ­¥éª¤ã
*   <strong>å¼ºå¤§çé¶æ ·æ¬æ³åè½åï¼</strong> URSAå¨å¯åé¿åº¦ä¸ä¸æä¸è¡¨ç°åºå¼ºå¤§çé¶æ ·æ¬æ³åè½åï¼å¸æ¾äºå¶å¤åè½æ§ã</p>
<p>è¿äºç»æè¡¨æï¼URSAå¨ç¦»æ£è§é¢çæé¢ååå¾äºéå¤§çªç ´ï¼ä¸ä»æåäºç¦»æ£æ¹æ³çæ§è½ä¸éï¼è¿è¯æäºå¶å¨å¤æè§é¢çæä»»å¡ä¸­çå®ç¨æ§åæçï¼ä¸ºå¯æ©å±ãå¤åè½åé«æçè§é¢çæå¼è¾äºæ°æ¹åã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>ç¦»æ£è§è§tokenizerçè¡¨ç¤ºè½åï¼</strong> è®ºææå°ï¼å°½ç®¡å¢å æ¨¡åå°ºå¯¸å¯ä»¥æ¾èæé«è¯­ä¹æ§è½ï¼ä½çæè¾åºçä¿çåº¦å¯è½æç»åéäºç¦»æ£è§è§tokenizerçè¡¨ç¤ºè½åãè¿æå³çï¼å³ä½¿æ¨¡ååå¤§ï¼å¦æåºå±ä»¤çåå¨çè½åæéï¼çæè´¨éä¹å¯è½è¾¾å°ç¶é¢ã
*   <strong>ç¦»æ£æ©æ£æ¨¡åçåºæéæ ·è¯¯å·®ï¼</strong> è®ºææåºï¼ç¦»æ£æ©æ£æ¨¡ååºæå°å­å¨è¾é«çéæ ·è¯¯å·®ï¼è¿å¨å¾ååè§é¢çæä¸­éè¦ç³»ç»æ§å°è§£å³ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>æ¹è¿ç¦»æ£è§è§tokenizerï¼</strong> é´äºå½åç¦»æ£tokenizerçè¡¨ç¤ºè½åå¯è½éå¶çæè´¨éï¼æªæ¥çç ç©¶å¯ä»¥ä¸æ³¨äºå¼åæ´å¼ºå¤§ãæ´ç²¾ç»çç¦»æ£è§è§tokenizerï¼ä»¥è¿ä¸æ­¥æåçæåå®¹çä¿çåº¦ã
*   <strong>ä¼åéæ ·ç­ç¥ä»¥åå°è¯¯å·®ï¼</strong> å°½ç®¡URSAéè¿è¿­ä»£ç»ååå°äºéæ ·è¯¯å·®ï¼ä½ç¦»æ£æ©æ£æ¨¡ååºæçè¯¯å·®ç´¯ç§¯é®é¢ä»æè¿ä¸æ­¥ä¼åçç©ºé´ï¼ä¾å¦æ¢ç´¢æ´åè¿çéæ ·ç®æ³æè¯¯å·®æ ¡æ­£æºå¶ã
*   <strong>æ¢ç´¢æ´å¤æçåº¦éè·¯å¾åæ¶é´æ­¥é¿è°åº¦ï¼</strong> è®ºææåºççº¿æ§ååº¦éè·¯å¾ååè¾¨çä¾èµçæ¶é´æ­¥é¿åç§»æºå¶å·²ç»åå¾äºæ¾èææï¼æªæ¥å¯ä»¥æ¢ç´¢æ´å¤æãèªéåºçåº¦éè·¯å¾åè°åº¦ç­ç¥ï¼ä»¥æ´å¥½å°éåºä¸åæ°æ®ç¹æ§åä»»å¡éæ±ã
*   <strong>æ©å±å°æ´å¤å¤æ¨¡æä»»å¡ï¼</strong> å¼æ­¥æ¶é´æ­¥é¿è°åº¦ç­ç¥ä½¿å¾URSAè½å¤å¤çå¤ç§ä»»å¡ï¼æªæ¥å¯ä»¥è¿ä¸æ­¥æ¢ç´¢å¶å¨æ´å¹¿æ³çå¤æ¨¡æçæä»»å¡ä¸­çåºç¨ï¼ä¾å¦3Dè§é¢çæãäº¤äºå¼åå®¹çæç­ã
*   <strong>ç»åå¶ä»çæèå¼ï¼</strong> å°½ç®¡URSAæ¨å¨å¼¥åç¦»æ£ä¸è¿ç»­æ¹æ³ä¹é´çå·®è·ï¼ä½ä»å¯ä»¥æ¢ç´¢ä¸å¶ä»çæèå¼ï¼å¦GANsãVAEï¼çç»åï¼ä»¥æå®ç°æ´å¼ºå¤§ççæè½åã</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>Additionally, we introduce an
asynchronous temporal fine-tuning strategy that unifies versatile tasks within
a single model, including interpolation and image-to-video generation.</li>
<li>Extensive experiments on challenging video and image generation benchmarks
demonstrate that URSA consistently outperforms existing discrete methods and
achieves performance comparable to state-of-the-art continuous diffusion
methods.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.24717v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.24717v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.24688v1'></a></p>
<h2 id="mic-bev-multi-infrastructure-camera-birds-eye-view-transformer-with-relation-aware-fusion-for-3d-object-detection"><a href="https://arxiv.org/abs/2510.24688v1">MIC-BEV: Multi-Infrastructure Camera Bird's-Eye-View Transformer with Relation-Aware Fusion for 3D Object Detection</a></h2>
<p><strong>Authors:</strong> Yun Zhang, Zhaoliang Zheng, Johnson Liu, Zhiyu Huang, Zewei Zhou, Zonglin Meng, Tianhui Cai, Jiaqi Ma</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Infrastructure-based perception plays a crucial role in intelligent
transportation systems, offering global situational awareness and enabling
cooperative autonomy. However, existing camera-based detection models often
underperform in such scenarios due to challenges such as multi-view
infrastructure setup, diverse camera configurations, degraded visual inputs,
and various road layouts. We introduce MIC-BEV, a Transformer-based
bird's-eye-view (BEV) perception framework for infrastructure-based
multi-camera 3D object detection. MIC-BEV flexibly supports a variable number
of cameras with heterogeneous intrinsic and extrinsic parameters and
demonstrates strong robustness under sensor degradation. The proposed
graph-enhanced fusion module in MIC-BEV integrates multi-view image features
into the BEV space by exploiting geometric relationships between cameras and
BEV cells alongside latent visual cues. To support training and evaluation, we
introduce M2I, a synthetic dataset for infrastructure-based object detection,
featuring diverse camera configurations, road layouts, and environmental
conditions. Extensive experiments on both M2I and the real-world dataset
RoScenes demonstrate that MIC-BEV achieves state-of-the-art performance in 3D
object detection. It also remains robust under challenging conditions,
including extreme weather and sensor degradation. These results highlight the
potential of MIC-BEV for real-world deployment. The dataset and source code are
available at: https://github.com/HandsomeYun/MIC-BEV.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼è¿æ¯ä¸ç¯å³äºâMIC-BEV: Multi-Infrastructure Camera Bird's-Eye-View Transformer with Relation-Aware Fusion for 3D Object Detectionâè®ºæçæè¦ï¼åå®¹æ¶µçäºæ¨è¦æ±çæææ¹é¢ï¼</p>
<p><strong>è®ºææè¦ï¼MIC-BEVï¼å¤åºç¡è®¾æ½ç¸æºé¸ç°å¾Transformerä¸å³ç³»æç¥èåç¨äº3Dç®æ æ£æµ</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³åºç¡è®¾æ½æç¥ç³»ç»å¨æºè½äº¤éä¸­é¢ä¸´çå³é®ææãç°æåºäºæåå¤´ç3Dç®æ æ£æµæ¨¡åå¨å¤è§è§åºç¡è®¾æ½è®¾ç½®ãå¤æ ·åçæåå¤´éç½®ãéåçè§è§è¾å¥ä»¥ååç§éè·¯å¸å±ç­å¤æåºæ¯ä¸è¡¨ç°ä¸ä½³ãå·ä½æ¥è¯´ï¼ææåæ¬ï¼1) ç©ºé´åå¸ä¼ æå¨å¯¼è´è§è§å·®å¼å¤§ãé®æ¡å¤ï¼é¾ä»¥è¿è¡ç©ºé´å¯¹é½åç¹å¾èåï¼2) åºç¡è®¾æ½æåå¤´éç½®ï¼æ°éãå¸å±ãæåãè§åºè§ï¼é«åº¦å¼æï¼æ¨¡åé¾ä»¥éåºï¼3) ä¼ æå¨å¯é æ§åé²æ£æ§ä¸è¶³ï¼æ¨¡åéåºå¯¹ç¼ºå¤±ãæåæä½è´¨éçè§è§è¾å¥ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
*   <strong>MIC-BEVæ¡æ¶ï¼</strong> æåºäºä¸ç§åºäºTransformerçé¸ç°å¾ï¼BEVï¼æç¥æ¡æ¶ï¼ç¨äºåºç¡è®¾æ½å¤æåå¤´3Dç®æ æ£æµãè¯¥æ¡æ¶çµæ´»æ¯æå¯åæ°éçæåå¤´ï¼å·æå¼æçåå¤åï¼å¹¶å¨ä¼ æå¨éåä¸è¡¨ç°åºå¼ºå¤§çé²æ£æ§ã
*   <strong>å³ç³»å¢å¼ºç©ºé´äº¤åæ³¨æåï¼ReSCAï¼ï¼</strong> å¼å¥äºä¸ç§æ°é¢çå¾å¢å¼ºèåæ¨¡åï¼éè¿å¾ç¥ç»ç½ç»ï¼GNNï¼å©ç¨æåå¤´ä¸BEVååä¹é´çå ä½å³ç³»ä»¥åæ½å¨è§è§çº¿ç´¢ï¼å°å¤è§è§å¾åç¹å¾èåå°BEVç©ºé´ä¸­ãè¿ä½¿å¾æ¨¡åè½å¤èªéåºå°å ææ¥èªä¸åæåå¤´çç¹å¾ï¼å¹¶æé«å¤è§è§ç¹å¾èåçè´¨éã
*   <strong>åå±BEVåå²å¤´ï¼</strong> ç»åå°å¾çº§åç©ä½çº§BEVåå²ä»»å¡ï¼ä»¥å¢å¼ºç©ºé´çè§£åå®ä½ç²¾åº¦ã
*   <strong>é²æ£æ§å¢å¼ºç­ç¥ï¼</strong> éç¨æåå¤´é®ç½©ç­ç¥ï¼å¦éæºä¸¢å¼åé«æ¯æ¨¡ç³ï¼è¿è¡è®­ç»ï¼ä»¥æ¨¡æä¼ æå¨éååé®æ¡ï¼æé«æ¨¡åå¨æææ§æ¡ä»¶ä¸çé²æ£æ§ã
*   <strong>M2Iåææ°æ®éï¼</strong> ä¸ºäºåæçå®ä¸çæ°æ®éä¸­åºç¡è®¾æ½éç½®ãå¤©æ°æ¡ä»¶åæåå¤´å¸å±å¤æ ·æ§ä¸è¶³çé®é¢ï¼å¼å¥äºä¸ä¸ªå¤§è§æ¨¡åææ°æ®éM2Iãè¯¥æ°æ®éæ¶µçäºå¹¿æ³çäº¤åå£ç±»åãæåå¤´éç½®åç¯å¢æ¡ä»¶ï¼ä¸ºæ¨¡åè®­ç»åè¯ä¼°æä¾äºå¨é¢çåºåã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>æåè¿çæ§è½ï¼</strong> å¨M2Iåææ°æ®éåçå®ä¸çRoScenesæ°æ®éä¸è¿è¡çå¹¿æ³å®éªè¡¨æï¼MIC-BEVå¨3Dç®æ æ£æµæ¹é¢åå¾äºæåè¿çæ§è½ã
*   <strong>å¼ºå¤§çé²æ£æ§ï¼</strong> MIC-BEVå¨æææ§æ¡ä»¶ä¸ï¼åæ¬æç«¯å¤©æ°åä¼ æå¨éåï¼ä»ä¿æé²æ£æ§ï¼æ¾èä¼äºç°æåºçº¿æ¹æ³ãä¾å¦ï¼å¨M2IçâNormalâæ¡ä»¶ä¸ï¼MIC-BEVçmAPè¾¾å°0.767ï¼æ¯æå¼ºåºçº¿DETR3Dé«åº9.4%ãå¨âRobustâåâExtreme Weatherâæ¡ä»¶ä¸ï¼æ§è½ä¸éå¹åº¦ä¹å°äºåºçº¿æ¨¡åã
*   <strong>å³ç³»æç¥èåçæææ§ï¼</strong> å®æ§åæåæ¶èç ç©¶è¯å®ï¼MIC-BEVçæåå¤´-ç½æ ¼å³ç³»å¢å¼ºæ³¨æåæºå¶è½ææèåè·¨è§è§ä¿¡æ¯ï¼å¹¶å¨æææ§å¯è§æ§ãå¤§ç©ºé´èå´åå¼ææåå¤´é¨ç½²ä¸ä¿æå ä½ä¸è´æ§ãGNNè½å¤å­¦ä¹ è§å¾ä¾èµçéè¦æ§ï¼æå¶å¤±çæé®æ¡çè§æµï¼å¹¶å¼ºè°å ä½å¯é åè¯­ä¹ä¿¡æ¯ä¸°å¯çè§å¾ã
*   <strong>å®éé¨ç½²æ½åï¼</strong> è¿äºç»æçªåºäºMIC-BEVå¨çå®ä¸çé¨ç½²ä¸­çå·¨å¤§æ½åï¼è½å¤å¹³è¡¡æ£æµç²¾åº¦åè®¡ç®æçã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>ç°ææ°æ®éçå±éæ§ï¼</strong> è®ºææåºï¼ç°æåºç¡è®¾æ½æç¥æ°æ®éæªè½åååæ çå®ä¸çåºç¡è®¾æ½æç¥çå¤ææ§ï¼ä¸»è¦ä½ç°å¨ï¼æåå¤´éç½®æéï¼éå¸¸æ¯åºå®æåè§è§ï¼ãåºæ¯å¤æ ·æ§æéï¼éå¸¸å±éäºåä¸äº¤åå£æé«éå¬è·¯ï¼ãä»¥åå¨ææ¡ä»¶æéï¼ä¸»è¦å¨æ´æç½å¤©æ¶éæ°æ®ï¼ç¼ºä¹æç«¯å¤©æ°ååç§ååï¼ãM2Iæ°æ®éçå¼å¥æ­£æ¯ä¸ºäºå¼¥è¡¥è¿äºå±éã
*   <strong>æªæ¥ç ç©¶æ¹åï¼</strong> è®ºææå°ï¼è¿åº¦é«çé®ç½©çï¼pm â¥ 0.5ï¼ä¼å¯¼è´M2I-Normalæ°æ®éæ§è½ä¸éï¼è¡¨æè¿åº¦é¢ç¹çè§å¾ç§»é¤ä¼éå¶æ¨¡åååå©ç¨å¤è§å¾åä½çè½åã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>å¤ç®æ è·è¸ªåè½¨è¿¹é¢æµï¼</strong> å°MIC-BEVæ©å±å°å¤ç®æ è·è¸ªåè½¨è¿¹é¢æµï¼ä»¥ææéè·¯ä½¿ç¨èä¹é´çå¨æäº¤äºã
*   <strong>å®æ¶é¨ç½²ï¼</strong> éè¿è½»éçº§éª¨å¹²ç½ç»åè¾¹ç¼è®¾å¤çç¥è¯è¸é¦ï¼æ¢ç´¢å®æ¶é¨ç½²ã
*   <strong>å¼¥åä»¿çä¸ç°å®å·®è·ï¼</strong> æ©å±M2Iåºåï¼å¢å é¢å¤ççå®ä¸çæ°æ®ï¼ä»¥å¼¥åä»¿çä¸ç°å®ä¹é´çå·®è·ï¼å¹¶å®ç°å¯¹å¤æ ·ååå¸ç¯å¢çå¨é¢è¯ä¼°ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce MIC-BEV, a Transformer-based
bird's-eye-view (BEV) perception framework for infrastructure-based
multi-camera 3D object detection.</li>
<li>Extensive experiments on both M2I and the real-world dataset
RoScenes demonstrate that MIC-BEV achieves state-of-the-art performance in 3D
object detection.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.24688v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.24688v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.24657v1'></a></p>
<h2 id="group-relative-attention-guidance-for-image-editing"><a href="https://arxiv.org/abs/2510.24657v1">Group Relative Attention Guidance for Image Editing</a></h2>
<p><strong>Authors:</strong> Xuanpu Zhang, Xuesong Niu, Ruidong Chen, Dan Song, Jianhao Zeng, Penghui Du, Haoxiang Cao, Kai Wu, An-an Liu</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Recently, image editing based on Diffusion-in-Transformer models has
undergone rapid development. However, existing editing methods often lack
effective control over the degree of editing, limiting their ability to achieve
more customized results. To address this limitation, we investigate the
MM-Attention mechanism within the DiT model and observe that the Query and Key
tokens share a bias vector that is only layer-dependent. We interpret this bias
as representing the model's inherent editing behavior, while the delta between
each token and its corresponding bias encodes the content-specific editing
signals. Based on this insight, we propose Group Relative Attention Guidance, a
simple yet effective method that reweights the delta values of different tokens
to modulate the focus of the model on the input image relative to the editing
instruction, enabling continuous and fine-grained control over editing
intensity without any tuning. Extensive experiments conducted on existing image
editing frameworks demonstrate that GRAG can be integrated with as few as four
lines of code, consistently enhancing editing quality. Moreover, compared to
the commonly used Classifier-Free Guidance, GRAG achieves smoother and more
precise control over the degree of editing. Our code will be released at
https://github.com/little-misfit/GRAG-Image-Editing.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°å¯¹è¿ç¯è®ºææè¦è¿è¡æ·±å¥åæã</p>
<hr />
<p><strong>è®ºææè¦åæï¼Group Relative Attention Guidance for Image Editing</strong></p>
<p><strong>1. è®ºæä¸»è¦è´¡ç®çç®æ´æ»ç» (2-3 å¥è¯)</strong></p>
<p>è¿ç¯è®ºææåºäºä¸ç§åä¸ºâGroup Relative Attention Guidance (GRAG)âçæ°æ¹æ³ï¼ç¨äºå¨åºäºDiffusion-in-Transformer (DiT) çå¾åç¼è¾æ¨¡åä¸­å®ç°å¯¹ç¼è¾ç¨åº¦çç²¾ç»åãè¿ç»­æ§æ§å¶ãéè¿éæ°å ææ³¨æåæºå¶ä¸­åå®¹ç¸å³çâdeltaâå¼ï¼GRAGè½å¤è°èæ¨¡åå¯¹è¾å¥å¾ååç¼è¾æä»¤çå³æ³¨ç¦ç¹ï¼ä»èå¨ä¸è¿è¡é¢å¤è®­ç»çæåµä¸æ¾èæåç¼è¾è´¨éå¹¶æä¾æ´å¹³æ»ãæ´ç²¾ç¡®çç¼è¾å¼ºåº¦æ§å¶ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³å­¦æ¹æ³</strong></p>
<ul>
<li><strong>å¯¹MM-Attentionæºå¶çæ·±åº¦æ´å¯ï¼</strong> è®ºæçæ ¸å¿åæ°å¨äºå¯¹DiTæ¨¡åä¸­MM-Attentionæºå¶çç¬ç¹è§£è¯»ãä½èè§å¯å°QueryåKey tokenå±äº«ä¸ä¸ªä»ä¾èµäºå±çåç½®åéãä»ä»¬å°è¿ä¸ªåç½®è§£éä¸ºæ¨¡ååºæçç¼è¾è¡ä¸ºï¼å³æ¨¡åé»è®¤çç¼è¾å¾åï¼ï¼èæ¯ä¸ªtokenä¸å¶å¯¹åºåç½®ä¹é´çâdeltaâåç¼ç äºåå®¹ç¹å®çç¼è¾ä¿¡å·ã</li>
<li><strong>Group Relative Attention Guidance (GRAG)ï¼</strong> åºäºä¸è¿°æ´å¯ï¼GRAGæ¹æ³éè¿éæ°å æè¿äºåå®¹ç¸å³çâdeltaâå¼æ¥å®ç°å¯¹ç¼è¾å¼ºåº¦çæ§å¶ãè¿æå³çå®ä¸æ¯ç´æ¥ä¿®æ¹æ³¨æåæéï¼èæ¯è°æ´æ¨¡åå¯¹ç¹å®åå®¹ç¹å¾çå³æ³¨ç¨åº¦ï¼ä½¿å¶æ´å¾åäºéµå¾ªç¼è¾æä»¤æä¿çåå§å¾åç¹å¾ã</li>
<li><strong>æ è®­ç»ï¼tuning-freeï¼çç¹æ§ï¼</strong> GRAGçä¸ä¸ªæ¾èä¼å¿æ¯å®ä¸éè¦ä»»ä½é¢å¤çè®­ç»æå¾®è°ãè¿ä½¿å¾å®éå¸¸å®¹æéæå°ç°ææ¡æ¶ä¸­ï¼å¹¶ä¸å·æå¾é«çå®ç¨æ§ã</li>
<li><strong>ä¸Classifier-Free Guidance (CFG) çå¯¹æ¯ï¼</strong> è®ºææç¡®æåºGRAGå¨ç¼è¾ç¨åº¦æ§å¶ä¸æ¯å¸¸ç¨çClassifier-Free Guidance (CFG) å®ç°äºæ´å¹³æ»åæ´ç²¾ç¡®çææï¼è¿è¡¨æå®å¨ç¨æ·ä½éªåç¼è¾è´¨éæ¹é¢å·ææ½å¨ä¼å¿ã</li>
</ul>
<p><strong>3. å¯¹é¢åæ½å¨å½±å</strong></p>
<ul>
<li><strong>æåç¨æ·æ§å¶åï¼</strong> GRAGç´æ¥è§£å³äºç°æå¾åç¼è¾æ¹æ³å¨ç¼è¾ç¨åº¦æ§å¶ä¸çä¸è¶³ï¼ä¸ºç¨æ·æä¾äºåææªæçç²¾ç»ååè¿ç»­æ§æ§å¶ï¼è¿å°æå¤§å°æåå¾åç¼è¾å·¥å·çç¨æ·ä½éªåå®ç¨æ§ã</li>
<li><strong>å éç ç©¶ä¸åºç¨ï¼</strong> å¶æ è®­ç»åæäºéæçç¹æ§ï¼ä»éåè¡ä»£ç ï¼æå³çç ç©¶äººååå¼åèå¯ä»¥è¿éå°å¶åºç¨äºåç§DiTåºçå¾åç¼è¾ä»»å¡ä¸­ï¼å éæ°åºç¨åæ°æ¹æ³çå¼åã</li>
<li><strong>å¯åæ°çæ³¨æåæºå¶ç ç©¶ï¼</strong> å¯¹MM-Attentionæºå¶ä¸­åç½®ådeltaçç¬ç¹è§£éï¼å¯è½ä¼å¯åæªæ¥å¯¹Transformeræ³¨æåæºå¶æ´æ·±å±æ¬¡ççè§£åæ¹è¿ï¼å°¤å¶æ¯å¨æ¡ä»¶çæåç¼è¾ä»»å¡ä¸­ã</li>
<li><strong>è¶è¶CFGçæ½å¨æ¿ä»£æ¹æ¡ï¼</strong> å¦æGRAGå¨å¹¿æ³åºæ¯ä¸è¡¨ç°åºä¼äºCFGçæ§å¶è½åï¼å®å¯è½æä¸ºæ¡ä»¶çææ¨¡åä¸­å¼å¯¼æºå¶çä¸ä¸ªéè¦è¡¥åçè³æ¿ä»£æ¹æ¡ã</li>
</ul>
<p><strong>4. ç¸å³é¢åæåºç¨åç</strong></p>
<ul>
<li><strong>ææ¬å°å¾åçæï¼Text-to-Image Generationï¼ï¼</strong> å°¤å¶æ¯å¨éè¦å¯¹çæå¾åçç¹å®å±æ§è¿è¡å¾®è°æ¶ï¼ä¾å¦æ¹åç©ä½çå¼ºåº¦ãé£æ ¼çç¨åº¦ç­ã</li>
<li><strong>å¾åä¿®å¤/è¡¥å¨ï¼Inpainting/Outpaintingï¼ï¼</strong> æ§å¶ä¿®å¤åºåä¸å¨å´ç¯å¢çèåç¨åº¦ï¼æçæåå®¹çå¼ºåº¦ã</li>
<li><strong>å¾åé£æ ¼è¿ç§»ï¼Image Style Transferï¼ï¼</strong> ç²¾ç»æ§å¶é£æ ¼è¿ç§»çå¼ºåº¦ï¼ä»è½»å¾®çé£æ ¼åå°å®å¨çé£æ ¼è½¬æ¢ã</li>
<li><strong>å¾åå±æ§ç¼è¾ï¼Image Attribute Editingï¼ï¼</strong> ä¾å¦ï¼æ¹åäººç©çè¡¨æå¼ºåº¦ãåè²æ·±æµãæè£çº¹ççææ¾ç¨åº¦ç­ã</li>
<li><strong>äº¤äºå¼å¾åç¼è¾å·¥å·ï¼</strong> ä»»ä½éè¦ç¨æ·éè¿æ»åæå¶ä»æ¹å¼å®æ¶è°æ´ç¼è¾ææçåºç¨ç¨åºé½å°åçåªæµã</li>
<li><strong>å¤æ¨¡æåå®¹çæï¼</strong> ä¸ä»éäºå¾åï¼æªæ¥å¯è½æ©å±å°è§é¢ç¼è¾æå¶ä»å¤æ¨¡æçæä»»å¡ä¸­ï¼åªè¦å¶åºå±ä½¿ç¨Transformer-basedçæ©æ£æ¨¡åã</li>
</ul>
<p><strong>5. ä»æè¦ä¸­å¯æ¨æ­åºçå±éæ§</strong></p>
<ul>
<li><strong>ä»éäºDiTæ¨¡åï¼</strong> æè¦æç¡®æåºå¶ç ç©¶å¯¹è±¡æ¯âDiffusion-in-Transformer modelsâä¸­çâMM-Attentionæºå¶âãè¿æå³çGRAGçç´æ¥éç¨æ§å¯è½å±éäºä½¿ç¨DiTæ¶æçæ©æ£æ¨¡åï¼å¯¹äºå¶ä»ç±»åçæ©æ£æ¨¡åï¼å¦U-Net basedï¼å¯è½éè¦è¿ä¸æ­¥çééæç ç©¶ã</li>
<li><strong>âç¼è¾è´¨éâçå®ä¹ï¼</strong> æè¦æå°âconsistently enhancing editing qualityâï¼ä½æ²¡æå·ä½è¯´æâç¼è¾è´¨éâçè¡¡éæ åãè¿å¯è½åæ¬è§è§çå®æãä¸æä»¤çä¸è´æ§ãæ ä¼ªå½±ç­ï¼ä½å·ä½ä¾§éåªäºæ¹é¢éè¦éè¿è®ºææ­£æçå®éªé¨åæ¥éªè¯ã</li>
<li><strong>âè¿ç»­åç²¾ç»æ§å¶âçç²åº¦ï¼</strong> å°½ç®¡æè¦å£°ç§°å®ç°äºâcontinuous and fine-grained controlâï¼ä½å®éçæ§å¶ç²åº¦ï¼ä¾å¦ï¼æå¤å°ä¸ªå¯åºåçç¼è¾å¼ºåº¦çº§å«ï¼ä»¥åç¨æ·ä½éªä¸çå¹³æ»åº¦ï¼ä»ééè¿å®éæ¼ç¤ºåç¨æ·ç ç©¶æ¥è¯ä¼°ã</li>
<li><strong>å¤æç¼è¾åºæ¯çæ³åè½åï¼</strong> æè¦æªæåGRAGå¨å¤çé«åº¦å¤æãå¤å¯¹è±¡ãå¤å±æ§åæ¶ç¼è¾çåºæ¯ä¸çè¡¨ç°ãå¨è¿äºæåµä¸ï¼ç®åçdeltaéå ææ¯å¦è½ææè§£è¦åæ§å¶ææç¼è¾ç»´åº¦ï¼ä»æ¯ä¸ä¸ªå¼æ¾é®é¢ã</li>
<li><strong>è®¡ç®å¼éï¼</strong> å°½ç®¡å£°ç§°âas few as four lines of codeâï¼ä½éæ°å ædeltaå¼æ¯å¦ä¼å¼å¥é¢å¤çè®¡ç®å¼éï¼å³ä½¿å¾å°ï¼ï¼ä»¥åè¿æ¯å¦ä¼å½±åå®æ¶ç¼è¾çæ§è½ï¼æè¦ä¸­æ²¡ææåã</li>
</ul>
<hr />
<p>æ»çæ¥è¯´ï¼è¿ç¯è®ºææåºäºä¸ç§ä¼éä¸é«æçæ¹æ³æ¥è§£å³æ©æ£æ¨¡åå¾åç¼è¾ä¸­ä¸ä¸ªå³é®çç¨æ·æ§å¶é®é¢ãå¶å¯¹æ³¨æåæºå¶çç¬ç¹è§£è¯»åæ è®­ç»çç¹æ§ä½¿å¶å·æå¾é«çå®ç¨ä»·å¼åæ½å¨å½±ååï¼æææ¨å¨å¾åç¼è¾é¢ååæ´ç²¾ç»åãç¨æ·åå¥½çæ¹ååå±ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Based on this insight, we propose Group Relative Attention Guidance, a
simple yet effective method that reweights the delta values of different tokens
to modulate the focus of the model on the input image relative to the editing
instruction, enabling continuous and fine-grained control over editing
intensity without any tuning.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.24657v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.24657v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.24623v1'></a></p>
<h2 id="groundloc-efficient-large-scale-outdoor-lidar-only-localization"><a href="https://arxiv.org/abs/2510.24623v1">GroundLoc: Efficient Large-Scale Outdoor LiDAR-Only Localization</a></h2>
<p><strong>Authors:</strong> Nicolai Steinke, Daniel Goehring</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Categories:</strong> cs.RO, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>In this letter, we introduce GroundLoc, a LiDAR-only localization pipeline
designed to localize a mobile robot in large-scale outdoor environments using
prior maps. GroundLoc employs a Bird's-Eye View (BEV) image projection focusing
on the perceived ground area and utilizes the place recognition network R2D2,
or alternatively, the non-learning approach Scale-Invariant Feature Transform
(SIFT), to identify and select keypoints for BEV image map registration. Our
results demonstrate that GroundLoc outperforms state-of-the-art methods on the
SemanticKITTI and HeLiPR datasets across various sensors. In the multi-session
localization evaluation, GroundLoc reaches an Average Trajectory Error (ATE)
well below 50 cm on all Ouster OS2 128 sequences while meeting online runtime
requirements. The system supports various sensor models, as evidenced by
evaluations conducted with Velodyne HDL-64E, Ouster OS2 128, Aeva Aeries II,
and Livox Avia sensors. The prior maps are stored as 2D raster image maps,
which can be created from a single drive and require only 4 MB of storage per
square kilometer. The source code is available at
https://github.com/dcmlr/groundloc.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Nicolai SteinkeåDaniel Goehringæ°åçè®ºæâGroundLoc: Efficient Large-Scale Outdoor LiDAR-Only Localizationâçå¨é¢æè¦ã</p>
<hr />
<h3 id="groundloc">è®ºææè¦ï¼GroundLoc: é«æå¤§è§æ¨¡å®¤å¤æ¿åé·è¾¾çº¯å®ä½</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³ç§»å¨æºå¨äººå¨å¤§è§æ¨¡å®¤å¤ç¯å¢ä¸­è¿è¡ç²¾ç¡®ãé«æä¸ä»ä¾èµæ¿åé·è¾¾çèªå®ä½é®é¢ãä¼ ç»æ¹æ³å¨å­å¨éæ±ãå¯¹å¨æç©ä½çé²æ£æ§ä»¥åå¨ç¼ºä¹ç¨³å®åç´ç¹å¾çéå¤æéç¬ç¹åºæ¯ä¸­çæ§è½æ¹é¢å­å¨ææãå·ä½æ¥è¯´ï¼ä½èå³æ³¨å¦ä½å©ç¨åéªå°å¾å®ç°ä½å­å¨ææ¬ãé«å®æ¶æ§è½ï¼å¹¶æ¯æå¤ç§ä¼ æå¨æ¨¡åã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
GroundLoc æåºäºä¸ä¸ªæ°é¢çæ¿åé·è¾¾çº¯å®ä½ç®¡éï¼å¶ä¸»è¦åæ°ç¹åæ¬ï¼
*   <strong>ä¸ééé¸ç°å¾ (BEV) å¾åæå½±ï¼</strong> ç³»ç»å°ç¹äºåå²ä¸ºå°é¢åéå°é¢ç¹ï¼å¹¶å°å¶æå½±å° BEV å¾åä¸­ãè¿äº BEV å¾ååå«ä¸ä¸ªééï¼å¼ºåº¦ãå°é¢å¡åº¦ä»¥å Z è½´é«åº¦æ¹å·®ãè¿ç§å¤ééè¡¨ç¤ºå¢å¼ºäºå¯¹å¨æç©ä½çé²æ£æ§ï¼å¹¶ä¿çäºå°å½¢ç²ç³åº¦åéæåç´ç»æçä¿¡æ¯ï¼è§£å³äºä»ä½¿ç¨å¼ºåº¦ééå¨æäºåºæ¯ä¸­å®ä½ä¸é²æ£çé®é¢ã
*   <strong>ç¹å¾æåä¸å¹éï¼</strong> GroundLoc å©ç¨æ·±åº¦å­¦ä¹ ç R2D2 ç½ç»ï¼æéå­¦ä¹ ç SIFT æ¹æ³ï¼ä»çæç BEV å¾åååèå°å¾ä¸­æåå³é®ç¹åæè¿°ç¬¦ãR2D2 å¨å¤ä¼è¯å®ä½ä¸­è¡¨ç°åºæ´ä¼å¼çå¹éè½åï¼å°¤å¶æ¯å¨ç¨çä¼ æå¨æ°æ®ä¸ã
*   <strong>é«æçå°å¾è¡¨ç¤ºï¼</strong> åéªå°å¾ä»¥ 2D æ æ ¼å¾åå°å¾çå½¢å¼å­å¨ï¼å¹³åæ¯å¹³æ¹å¬éä»é 4 MB çå­å¨ç©ºé´ãè¿äºå°å¾å¯ä»¥ä»åæ¬¡é©¾é©¶çæï¼å¹¶å©ç¨ ZSTD åç¼©å GeoTIFF æ ¼å¼çåç½®åè½ã
*   <strong>é²æ£çä½å§¿ä¼°è®¡ï¼</strong> éç¨è¿ä¼¼æè¿é» KD-Tree è¿è¡æè¿°ç¬¦å¹éï¼å¹¶éè¿å¨ææç´¢åå¾è¿è¡ä½ç½®è·ç¦»è¿æ»¤ï¼ä»¥æ¶é¤ä¸åççå¯¹åºå³ç³»ãæç»çä½å§¿ä¼°è®¡ä½¿ç¨ Quatro ä¼°è®¡å¨è®¡ç®ï¼è¯¥ä¼°è®¡å¨å¯¹å¼å¸¸å¼å·ææ´å¼ºçæµæåã
*   <strong>ä½å§¿æ ¡æ­£æºå¶ï¼</strong> å¼å¥äºåºäºåç¹æ°éåå½åéåº¦çä½å§¿æ ¡æ­£æºå¶ï¼ä»¥ç¼è§£éåè¯¯å·®åé«éè¡é©¶æ¶çæ¼ç§»ç´¯ç§¯ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>åè¶çå®ä½ç²¾åº¦ï¼</strong> å¨ SemanticKITTI å HeLiPR æ°æ®éä¸ï¼GroundLoc å¨å¤ä¼è¯å®ä½è¯ä¼°ä¸­ï¼ææ Ouster OS2 128 åºåçå¹³åè½¨è¿¹è¯¯å·® (ATE) åè¿ä½äº 50 åç±³ï¼æ¾èä¼äºç°ææåè¿çæ¹æ³ï¼å¦ KISS-ICPãKISS-SLAM ååºäºæçº¹çå®ä½æ¹æ³ï¼ã
*   <strong>å®æ¶æ§è½ï¼</strong> ç³»ç»å¨ææå®éªä¸­åå®ç°äºè¶è¿ 14 Hz çå¤çéçï¼æ»¡è¶³äºå¨çº¿è¿è¡æ¶é´è¦æ±ã
*   <strong>ä½å­å¨éæ±ï¼</strong> åéªå°å¾çå­å¨éæ±æä½ï¼å¹³åæ¯å¹³æ¹å¬éä»é 4.09 MBï¼è¿ä½äºæçº¹å®ä½æ¹æ³ï¼33.75 MB/kmÂ²ï¼åä¸éæ ·ç¹äºå°å¾ï¼15.32 MB/kmÂ²ï¼ã
*   <strong>å¤ä¼ æå¨æ¯æï¼</strong> è®ºæéè¿å¯¹ Velodyne HDL-64EãOuster OS2 128ãAeva Aeries II å Livox Avia ä¼ æå¨è¿è¡è¯ä¼°ï¼è¯æäºç³»ç»å¯¹å¤ç§ä¼ æå¨æ¨¡åçæ¯æè½åã
*   <strong>æ³åè½åï¼</strong> æ¨¡åå¨ä¸åå°ç¹ä¹é´è¡¨ç°åºé²æ£çæ³åè½åï¼ä½å¨ä¸åä¼ æå¨æ¨¡åä¹é´ï¼ç¹å«æ¯å¼ºåº¦ååºåæ«ææ¨¡å¼å·®å¼æ¾èæ¶ï¼æ§è½å·®å¼ä¼å¢å¤§ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>3 èªç±åº¦ (3-DOF) éå¶ï¼</strong> GroundLoc æ¯ä¸ç§ 3-DOF å®ä½æ¹æ³ï¼æ æ³çº æ­£ä¿¯ä»°ãæ¨ªæ»å Z è½´é«åº¦çè¯¯å·®ãè¿ä½¿å¾å®å¨ä¸ç¼ºä¹ 360Â° è§åºè§çä¼ æå¨ï¼å¦ Aeva Aeries II å Livox Aviaï¼ç»åä½¿ç¨æ¶ï¼å¯è½ä¼éå°æ¾èçä¿¯ä»°æ¼ç§»ã
*   <strong>å°é¢çå¼æ°æ®ä¸è´æ§ï¼</strong> å¨å¤ä¼è¯å®ä½ä¸­ï¼ä¸äºç»æï¼ç¹å«æ¯ä½¿ç¨ Ouster ä¼ æå¨æ¶ï¼ç ATE éä¸­å¨ 0.3-0.4 ç±³å·¦å³ï¼ä½èè®¤ä¸ºè¿å¯è½å½å äºå°é¢çå¼æ°æ®å¨å¤ä¼è¯ä¸è´æ§æ¹é¢çå±éæ§ã
*   <strong>ä¼ æå¨æ¨¡åæ³åï¼</strong> å°½ç®¡æ¨¡åå¨ä¸åå°ç¹ä¹é´æ³åè¯å¥½ï¼ä½å¨ä¸åä¼ æå¨æ¨¡åä¹é´ï¼å°¤å¶æ¯å¼ºåº¦ååºåæ«ææ¨¡å¼å·®å¼æ¾èæ¶ï¼æ§è½å·®å¼ä¼å¢å¤§ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
è®ºæä¸­æ²¡ææç¡®æåºæªæ¥çç ç©¶æ¹åï¼ä½ä»å¶å±éæ§åè´¡ç®ä¸­å¯ä»¥æ¨æ­åºä¸äºæ½å¨æ¹åï¼
*   <strong>æ©å±å° 6 èªç±åº¦ (6-DOF) å®ä½ï¼</strong> ç»å IMU æå¶ä»ä¼ æå¨ä¿¡æ¯ï¼æèå¼åæ°ç BEV è¡¨ç¤ºåç¹å¾æåæ¹æ³ï¼ä»¥å®ç°æ´å¨é¢ç 6-DOF å®ä½ï¼ä»èè§£å³ä¿¯ä»°ãæ¨ªæ»å Z è½´é«åº¦çæ¼ç§»é®é¢ã
*   <strong>å¢å¼ºä¼ æå¨æ¨¡åé´çæ³åè½åï¼</strong> è¿ä¸æ­¥ç ç©¶å¦ä½æé«æ¨¡åå¨ä¸åæ¿åé·è¾¾ä¼ æå¨ï¼ç¹å«æ¯æ«ææ¨¡å¼åå¼ºåº¦ååºå·®å¼å¤§æ¶ï¼ä¹é´çæ³åè½åï¼å¯è½éè¿æ´åè¿çåéåºææ¯æå¤æ¨¡æèåæ¹æ³ã
*   <strong>å¨æç¯å¢ä¸çé²æ£æ§ï¼</strong> å°½ç®¡ GroundLoc éè¿å°é¢åå²æé«äºå¯¹å¨æç©ä½çé²æ£æ§ï¼ä½è¿ä¸æ­¥æ¢ç´¢å¨é«åº¦å¨ææå¤æåºæ¯ï¼ä¾å¦äº¤éç¹å¿çåå¸ä¸­å¿ï¼ä¸­ä¿æå®ä½ç²¾åº¦çç­ç¥ã
*   <strong>å°å¾æ´æ°åç»´æ¤ï¼</strong> ç ç©¶å¦ä½é«æå°æ´æ°åç»´æ¤å¤§è§æ¨¡åéªå°å¾ï¼ä»¥éåºç¯å¢ååï¼å¦æ½å·¥ãæ¤è¢«ååç­ï¼ï¼åæ¶ä¿æä½å­å¨åå®æ¶æ§è½ã
*   <strong>ç»åå¶ä»æç¥ä»»å¡ï¼</strong> å°å®ä½ç»æä¸æç¥ãè§åç­å¶ä»èªå¨é©¾é©¶ä»»å¡æ´ç´§å¯å°ç»åï¼æå»ºæ´å¨é¢çèªä¸»ç³»ç»ã</p>
<hr />
<p>æ»èè¨ä¹ï¼GroundLoc è®ºææåºäºä¸ç§é«æãä½å­å¨ãé«ç²¾åº¦çæ¿åé·è¾¾çº¯å®ä½è§£å³æ¹æ¡ï¼éè¿åæ°çä¸éé BEV å¾åè¡¨ç¤ºååè¿çç¹å¾æåæ¹æ³ï¼å¨å¤ç§ä¼ æå¨åå¤æç¯å¢ä¸­åå¾äºæ¾èææï¼ä¸ºå¤§è§æ¨¡ç§»å¨æºå¨äººå®ä½é¢åååºäºéè¦è´¡ç®ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this letter, we introduce GroundLoc, a LiDAR-only localization pipeline
designed to localize a mobile robot in large-scale outdoor environments using
prior maps.</li>
<li>Our
results demonstrate that GroundLoc outperforms state-of-the-art methods on the
SemanticKITTI and HeLiPR datasets across various sensors.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.24623v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.24623v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.24514v1'></a></p>
<h2 id="latent-sketchpad-sketching-visual-thoughts-to-elicit-multimodal-reasoning-in-mllms"><a href="https://arxiv.org/abs/2510.24514v1">Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal Reasoning in MLLMs</a></h2>
<p><strong>Authors:</strong> Huanyu Zhang, Wenshan Wu, Chengzu Li, Ning Shang, Yan Xia, Yangyu Huang, Yifan Zhang, Li Dong, Zhang Zhang, Liang Wang, Tieniu Tan, Furu Wei</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Categories:</strong> cs.CV, cs.CL</p>
<p><strong>Abstract:</strong></p>
<p>While Multimodal Large Language Models (MLLMs) excel at visual understanding,
they often struggle in complex scenarios that require visual planning and
imagination. Inspired by how humans use sketching as a form of visual thinking
to develop and communicate ideas, we introduce Latent Sketchpad, a framework
that equips MLLMs with an internal visual scratchpad. The internal visual
representations of MLLMs have traditionally been confined to perceptual
understanding. We repurpose them to support generative visual thought without
compromising reasoning ability. Building on frontier MLLMs, our approach
integrates visual generation directly into their native autoregressive
reasoning process. It allows the model to interleave textual reasoning with the
generation of visual latents. These latents guide the internal thought process
and can be translated into sketch images for interpretability. To realize this,
we introduce two components: a Context-Aware Vision Head autoregressively
produces visual representations, and a pretrained Sketch Decoder renders these
into human-interpretable images. We evaluate the framework on our new dataset
MazePlanning. Experiments across various MLLMs show that Latent Sketchpad
delivers comparable or even superior reasoning performance to their backbone.
It further generalizes across distinct frontier MLLMs, including Gemma3 and
Qwen2.5-VL. By extending model's textual reasoning to visual thinking, our
framework opens new opportunities for richer human-computer interaction and
broader applications. More details and resources are available on our project
page: https://latent-sketchpad.github.io/.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Huanyu Zhangç­äººçè®ºæâLatent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal Reasoning in MLLMsâçå¨é¢æè¦ã</p>
<hr />
<h3 id="latent-sketchpad-mllms">ãLatent Sketchpad: ç»å¶è§è§æç»´ä»¥æ¿åå¤æ¨¡ææ¨ççMLLMsãè®ºææè¦</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
å°½ç®¡å¤æ¨¡æå¤§åè¯­è¨æ¨¡åï¼MLLMsï¼å¨è§è§çè§£æ¹é¢è¡¨ç°åºè²ï¼ä½å¨éè¦è§è§è§ååæ³è±¡åçå¤æåºæ¯ä¸­ï¼å®ä»¬å¾å¾åä¸ä»å¿ãä¼ ç»ä¸ï¼MLLMsçåé¨è§è§è¡¨ç¤ºä»éäºæç¥çè§£ãè¯¥ç ç©¶æ¨å¨è§£å³å¦ä½ä½¿MLLMsè½å¤åäººç±»ä¸æ ·ï¼å©ç¨åé¨è§è§âèå¾âä½ä¸ºè§è§æç»´å½¢å¼ï¼ä»¥æ¯æçææ§çè§è§æèï¼ä»èå¢å¼ºå¶å¤æ¨¡ææ¨çè½åï¼å°¤å¶æ¯å¨éè¦ç²¾ç¡®ç©ºé´æ¨çåå¨æè§è§æ¥å°çå¤æä»»å¡ä¸­ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
ä¸ºäºè§£å³ä¸è¿°é®é¢ï¼è®ºæå¼å¥äº<strong>Latent Sketchpad</strong>æ¡æ¶ï¼å¶æ ¸å¿åæ°åæ¬ï¼</p>
<ul>
<li><strong>åé¨è§è§èå¾ï¼Internal Visual Scratchpadï¼ï¼</strong> Latent Sketchpadä½¿MLLMsè½å¤å¨å¶æ¨çè¿ç¨ä¸­çæè¿ç»­çè§è§æ½å¨è¡¨ç¤ºï¼visual latentsï¼ï¼è¿äºæ½å¨è¡¨ç¤ºå¨æ¨çè¿ç¨ä¸­ä¿æå¨æ½å¨ç©ºé´ä¸­ï¼èä¸æ¯ç«å³è§£ç ä¸ºå¾åãè¿ä½¿å¾æ¨¡åè½å¤å°ææ¬æ¨çä¸è§è§æ½å¨è¡¨ç¤ºççæäº¤ç»å¨ä¸èµ·ï¼ä»èå®ç°æ´ä¸°å¯çå¤æ¨¡ææ¨çã</li>
<li><strong>ä¸ä¸ææç¥è§è§å¤´ï¼Context-Aware Vision Headï¼ï¼</strong> è¿æ¯ä¸ä¸ªéæå°MLLMéª¨å¹²ç½ç»ä¸­çç»ä»¶ï¼è´è´£å¨æ¯ä¸ªæ¨çæ­¥éª¤ä¸­èªåå½å°çæè§è§æ½å¨è¡¨ç¤ºãå®ä¸ä»åºäºå½åçéèç¶æï¼è¿åºäºååçè§è§è¡¨ç¤ºè¿è¡æ¡ä»¶åï¼ä»èç¡®ä¿è§è§è¿è´¯æ§ï¼å¹¶æ ¹æ®å¾åååå¾åé´çä¸ä¸æçº¿ç´¢ç»ååé¨è§è§è¡¨ç¤ºã</li>
<li><strong>é¢è®­ç»èå¾è§£ç å¨ï¼Pretrained Sketch Decoderï¼ï¼</strong> è¿æ¯ä¸ä¸ªç¬ç«çæ¨¡åï¼ç¨äºå°çæçè§è§æ½å¨è¡¨ç¤ºæ¸²ææäººç±»å¯è§£éçèå¾å¾åãå®éè¿å¯¹é½é¢è®­ç»è§è§ç¼ç å¨çç¹å¾ç©ºé´ä¸é¢è®­ç»VAEçæ½å¨ç©ºé´ï¼å°è§è§æ½å¨è¡¨ç¤ºè½¬åä¸ºèå¾é£æ ¼çå¾åï¼ä»èå®ç°æ¨¡ååé¨è§è§æç»´è¿ç¨çå¯è§£éæ§ã</li>
<li><strong>MAZEPLANNINGæ°æ®éï¼</strong> ä¸ºäºè¯ä¼°æ¡æ¶çæææ§ï¼ç ç©¶æå»ºäºä¸ä¸ªæ°çMAZEPLANNINGæ°æ®éï¼è¯¥æ°æ®éåå«å¤æçãäº¤ç»çå¤æ¨¡ææ¨çè½¨è¿¹ï¼ç¨äºè®­ç»åè¯ä¼°æ¨¡åå¨è§è§è§ååå¯¼èªä»»å¡ä¸­çè¡¨ç°ã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
å®éªç»æå¨å¤ä¸ªMLLMsï¼åæ¬Gemma3åQwen2.5-VLï¼ä¸è¿è¡ï¼å¹¶å±ç¤ºäºä»¥ä¸ä¸»è¦åç°ï¼</p>
<ul>
<li><strong>æ¨çæ§è½æåï¼</strong> Latent Sketchpadå¨MAZEPLANNINGæ°æ®éä¸çæ¨çæ§è½ä¸å¶éª¨å¹²æ¨¡åç¸æ¯ï¼è¾¾å°äºçè³è¶è¶çæ°´å¹³ãè¿è¡¨æéè¿æ´åè§è§æç»´ï¼æ¨¡åå¨å¤æå¤æ¨¡ææ¨çä»»å¡ä¸­çè¡¨ç°å¾å°äºå¢å¼ºã</li>
<li><strong>å¹¿æ³éç¨æ§åå³æå³ç¨è½åï¼</strong> è¯¥æ¡æ¶è½å¤æ³åå°ä¸åçåæ²¿MLLMsï¼å¦Gemma3åQwen2.5-VLï¼è¯æäºå¶æ¨¡ååæ¶æçä¼å¿ãè§è§å¤´å¯ä»¥ç¬ç«è®­ç»å¹¶éå å°MLLMsä¸ï¼èæ éä¿®æ¹å¶åæ°ï¼ä»èä¿çäºéª¨å¹²æ¨¡åçåå§æ¨çè½åï¼åæ¶æ ç¼å¢å¼ºäºè§è§çæè½åã</li>
<li><strong>å¯è§£éçè§è§è½¨è¿¹ï¼</strong> çæçè§è§æ½å¨è¡¨ç¤ºå¯ä»¥è¢«è§£ç ä¸ºå¯è§£éçèå¾ï¼ä¸ºæ¨¡åçåé¨è§è§æç»´è¿ç¨æä¾äºéæçæ´å¯åï¼å¢å¼ºäºäººæºäº¤äºåæ¨¡åçå¯ä¿¡åº¦ã</li>
<li><strong>ç©ºé´ä¸è´æ§ä¸é²æ£æ§ï¼</strong> Latent Sketchpadå¨çæå¾åæ¶ä¿æäºè¾é«çå¸å±ä¸è´æ§çï¼LCRï¼åè§è§æåçï¼VSRï¼ï¼è¡¨æå¶å¨æ¨çæ­¥éª¤ä¸­è½å¤ä¿æç©ºé´ç»æï¼å¹¶æ¯æéè¿è§è§çæè¿è¡æ¨çã</li>
</ul>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
è®ºæä¸­ä¹è®¨è®ºäºä¸äºå±éæ§ï¼</p>
<ul>
<li><strong>OODæ³åè½åï¼</strong> å¨åå¸å¤ï¼OODï¼æ°æ®éï¼æ´å¤§çãæªè§è¿çè¿·å®«ï¼ä¸ï¼æ¨¡åçæ§è½æ¾èä¸éãç¹å«æ¯Qwen2.5-VLç±äºå¶è§è§ç¼ç å¨äº§çæ¯Gemma3å¤§ååçç¹å¾ï¼æéçå¾®è°æ°æ®ä¸è¶³ä»¥ç¡®ä¿æ³åï¼å¯¼è´å¶æªè½å¯é å°ä¿çè¿·å®«å¸å±ã</li>
<li><strong>è§è§è´¨éï¼</strong> å°½ç®¡çæçèå¾å¨ç»æä¸ç¨³å®ï¼ä½å¨æç¥è´¨éä¸ï¼å¦ç®­å¤´ææ°å­ï¼å¯è½æ¾å¾è¾ä½ãè½ç¶è¿è¶³ä»¥æ¯æå¤æ¨¡ææ¨çï¼ä½å¯¹äºéè¦æ´é«ç²¾åº¦æç¥ç»èçä»»å¡ï¼å¯è½éè¦è¿ä¸æ­¥æ¹è¿ã</li>
<li><strong>ç»ææ§è¿è§ï¼</strong> å¨æäºæåµä¸ï¼æ¨¡åçæçè·¯å¾å¯è½ç©¿è¿è¿·å®«å¢å£æçªç¶ç¬ç§»å°è¿å¤ä½ç½®ï¼å¯¼è´æç»è®¡åä¸æ­£ç¡®ï¼å³ä½¿åä¸ªå¨ä½å¨å±é¨ä¸è¿è´¯ã</li>
</ul>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
åºäºä¸è¿°åç°åå±éæ§ï¼è®ºæä¸ºæªæ¥çç ç©¶ææäºæ¹åï¼</p>
<ul>
<li><strong>å¢å¼ºç©ºé´ä¸è´æ§åå¯¹åå¸ååçé²æ£æ§ï¼</strong> è§£å³æ¨¡åå¨å¤æææ°é¢ç¯å¢ä¸­åºç°çç»ææ§è¿è§åç´¯ç§¯æ§éåé®é¢ï¼ä»¥æé«ç©ºé´ä¸è´æ§åé²æ£æ§ã</li>
<li><strong>æåè§è§ä¿çåº¦ï¼</strong> è¿ä¸æ­¥æé«çæèå¾çè§è§ä¿çåº¦ï¼ä»¥æ©å±å¶å¨éè¦æ´ç²¾ç»æç¥ç²¾åº¦çä»»å¡ä¸­çéç¨æ§ã</li>
<li><strong>æ¢ç´¢æ´ä¸°å¯çMLLMåé¨è§è§è¡¨ç¤ºï¼</strong> è¿ä¸æ­¥ç ç©¶å¦ä½å©ç¨MLLMsçåé¨è§è§è¡¨ç¤ºï¼ä»¥å®ç°æ´å¤æãæ´å·åé æ§çè§è§æç»´åè§åã</li>
<li><strong>ä¼åè¿æ¥å¨éåºæ§ï¼</strong> è¿ä¸æ­¥ç ç©¶è¿æ¥å¨éåºæ§å¨ä¸æ¸¸ä»»å¡å¾®è°ä¸­çå³é®ä½ç¨ï¼ä»¥ç¡®ä¿è§è§è¡¨ç¤ºå¨è®­ç»è¿ç¨ä¸­å¾å°æææ´æ°ã</li>
</ul>
<hr />
<p>æ»èè¨ä¹ï¼Latent Sketchpadæ¡æ¶éè¿ä¸ºMLLMsæä¾åé¨è§è§èå¾ï¼æåå°å°ææ¬æ¨çä¸è§è§æç»´ç¸ç»åï¼æ¾èæåäºæ¨¡åå¨å¤æå¤æ¨¡ææ¨çä»»å¡ä¸­çè¡¨ç°ãè¿ä¸åæ°ä¸ä»å¢å¼ºäºæ¨çè½åï¼è¿éè¿å¯è§£éçèå¾æä¾äºéæçè§è§è½¨è¿¹ï¼ä¸ºæªæ¥æ´ä¸°å¯çäººæºäº¤äºåæ´å¹¿æ³çåºç¨å¼è¾äºæ°éå¾ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Inspired by how humans use sketching as a form of visual thinking
to develop and communicate ideas, we introduce Latent Sketchpad, a framework
that equips MLLMs with an internal visual scratchpad.</li>
<li>Building on frontier MLLMs, our approach
integrates visual generation directly into their native autoregressive
reasoning process.</li>
<li>To realize this,
we introduce two components: a Context-Aware Vision Head autoregressively
produces visual representations, and a pretrained Sketch Decoder renders these
into human-interpretable images.</li>
<li>We evaluate the framework on our new dataset
MazePlanning.</li>
<li>By extending model's textual reasoning to visual thinking, our
framework opens new opportunities for richer human-computer interaction and
broader applications.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.24514v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.24514v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.24464v1'></a></p>
<h2 id="kineo-calibration-free-metric-motion-capture-from-sparse-rgb-cameras"><a href="https://arxiv.org/abs/2510.24464v1">Kineo: Calibration-Free Metric Motion Capture From Sparse RGB Cameras</a></h2>
<p><strong>Authors:</strong> Charles Javerliat, Pierre Raimbaud, Guillaume LavouÃ©</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Markerless multiview motion capture is often constrained by the need for
precise camera calibration, limiting accessibility for non-experts and
in-the-wild captures. Existing calibration-free approaches mitigate this
requirement but suffer from high computational cost and reduced reconstruction
accuracy.
  We present Kineo, a fully automatic, calibration-free pipeline for markerless
motion capture from videos captured by unsynchronized, uncalibrated,
consumer-grade RGB cameras. Kineo leverages 2D keypoints from off-the-shelf
detectors to simultaneously calibrate cameras, including Brown-Conrady
distortion coefficients, and reconstruct 3D keypoints and dense scene point
maps at metric scale. A confidence-driven spatio-temporal keypoint sampling
strategy, combined with graph-based global optimization, ensures robust
calibration at a fixed computational cost independent of sequence length. We
further introduce a pairwise reprojection consensus score to quantify 3D
reconstruction reliability for downstream tasks.
  Evaluations on EgoHumans and Human3.6M demonstrate substantial improvements
over prior calibration-free methods. Compared to previous state-of-the-art
approaches, Kineo reduces camera translation error by approximately 83-85%,
camera angular error by 86-92%, and world mean-per-joint error (W-MPJPE) by
83-91%.
  Kineo is also efficient in real-world scenarios, processing multi-view
sequences faster than their duration in specific configuration (e.g., 36min to
process 1h20min of footage). The full pipeline and evaluation code are openly
released to promote reproducibility and practical adoption at
https://liris-xr.github.io/kineo/.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°å¯¹è¿ç¯è®ºææè¦è¿è¡åæã</p>
<hr />
<p><strong>è®ºææè¦åæï¼Kineo: Calibration-Free Metric Motion Capture From Sparse RGB Cameras</strong></p>
<p><strong>1. è®ºæä¸»è¦è´¡ç®çç®æ´æ»ç» (2-3 å¥è¯)</strong></p>
<p>Kineo æåºäºä¸ç§å¨èªå¨ãåæ å®ãæ æ è®°çå¤è§è§è¿å¨æææµæ°´çº¿ï¼è½å¤ä»éåæ­¥ãæªæ å®çæ¶è´¹çº§RGBç¸æºè§é¢ä¸­ï¼ä»¥åº¦éå°ºåº¦åæ¶å®æç¸æºæ å®ï¼åæ¬ç¸åç³»æ°ï¼å3Då³é®ç¹åå¯éåºæ¯ç¹å¾çéå»ºãå®éè¿ç½®ä¿¡åº¦é©±å¨çæ¶ç©ºå³é®ç¹éæ ·ç­ç¥ååºäºå¾çå¨å±ä¼åï¼æ¾èæåäºæ å®åéå»ºçé²æ£æ§ååç¡®æ§ï¼å¹¶å¤§å¹éä½äºè®¡ç®ææ¬ï¼ä½¿å¶å¨å®éåºç¨ä¸­æ´é«æã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®º</strong></p>
<p>Kineo çæ ¸å¿åæ°å¨äºå¶ç»åäºä»¥ä¸å ä¸ªå³é®ç¹ï¼</p>
<ul>
<li><strong>åæ¶æ å®ä¸éå»ºï¼</strong> å®ä¸ä¾èµäºé¢åçç¸æºæ å®ï¼èæ¯å©ç¨ç°æç2Då³é®ç¹æ£æµå¨ï¼å¨åä¸ä¼åæ¡æ¶ä¸åæ¶ä¼°è®¡ç¸æºåæ°ï¼åæ¬Brown-Conradyç¸åç³»æ°ï¼ã3Då³é®ç¹åå¯éåºæ¯ç¹å¾ï¼å¹¶ä»¥åº¦éå°ºåº¦è¿è¡ã</li>
<li><strong>ç½®ä¿¡åº¦é©±å¨çæ¶ç©ºå³é®ç¹éæ ·ç­ç¥ï¼</strong> è¿ç§ç­ç¥è½å¤æºè½å°éæ©åå©ç¨2Då³é®ç¹æ°æ®ï¼ç¡®ä¿å¨å¤æåºæ¯ä¸ä¹è½è¿è¡é²æ£çæ å®ï¼å¹¶ææå¤çéåæ­¥ç¸æºæ°æ®ã</li>
<li><strong>åºäºå¾çå¨å±ä¼åï¼</strong> éç¨å¨å±ä¼åæ¹æ³ï¼å°ææå¯ç¨çä¿¡æ¯ï¼2Då³é®ç¹ãç¸æºæ¨¡åç­ï¼æ´åå°ä¸ä¸ªç»ä¸çä¼åé®é¢ä¸­ï¼ä»¥å®ç°æ´åç¡®åä¸è´çè§£å³æ¹æ¡ã</li>
<li><strong>è®¡ç®ææ¬ç¬ç«äºåºåé¿åº¦ï¼</strong> æè¦ä¸­æå°âfixed computational cost independent of sequence lengthâï¼è¿æç¤ºäºå¶ä¼åç­ç¥è½å¤ææå°å¤çé¿è§é¢åºåï¼é¿åäºä¼ ç»æ¹æ³ä¸­è®¡ç®ææ¬éæ¶é´çº¿æ§å¢é¿çé®é¢ã</li>
<li><strong>æå¯¹éæå½±ä¸è´æ§è¯åï¼</strong> å¼å¥äºæ°çåº¦éæ åæ¥éå3Déå»ºçå¯é æ§ï¼è¿å¯¹äºä¸æ¸¸ä»»å¡çè´¨éæ§å¶è³å³éè¦ã</li>
</ul>
<p><strong>3. å¯¹é¢åæ½å¨å½±å</strong></p>
<p>Kineo å¯¹è®¡ç®æºè§è§é¢åå·ææ·±è¿çæ½å¨å½±åï¼</p>
<ul>
<li><strong>æ°ä¸»åè¿å¨ææï¼</strong> éè¿æ¶é¤å¯¹ç²¾ç¡®ç¸æºæ å®çéæ±ï¼Kineo æå¤§å°éä½äºä¸ä¸è¿å¨ææç³»ç»çé¨æ§ï¼ä½¿å¾éä¸å®¶ç¨æ·ä¹è½å¨âéå¤âæéåæ§ç¯å¢ä¸­è¿è¡é«è´¨éçè¿å¨ææãè¿å°æ¨å¨è¿å¨ææææ¯å¨æ´å¹¿æ³çåºç¨åºæ¯ä¸­æ®åã</li>
<li><strong>æåâéå¤âåºæ¯çæ§è½ï¼</strong> ç°æåæ å®æ¹æ³å¨è®¡ç®ææ¬åéå»ºç²¾åº¦ä¸å­å¨ä¸è¶³ï¼Kineo å¨è¿ä¸¤æ¹é¢é½åå¾äºæ¾èè¿æ­¥ï¼å°¤å¶æ¯å¨ç¸æºå§¿æå3Då³é®ç¹éå»ºç²¾åº¦ä¸ï¼ä½¿å¶å¨çå®ä¸çãéçæ³æ¡ä»¶ä¸çåºç¨æ´å·å¯è¡æ§ã</li>
<li><strong>æ¨å¨æ¶è´¹çº§ç¡¬ä»¶çåºç¨ï¼</strong> è½å¤å©ç¨éåæ­¥ãæªæ å®çæ¶è´¹çº§RGBç¸æºè¿è¡é«ç²¾åº¦è¿å¨ææï¼å°å éè¿å¨ææææ¯ä¸æºè½ææºãå®¶ç¨æåå¤´ç­è®¾å¤çç»åï¼å¬çæ°çåºç¨ã</li>
<li><strong>ä¸ºä¸æ¸¸ä»»å¡æä¾æ´å¯é çè¾å¥ï¼</strong> æåºçæå¯¹éæå½±ä¸è´æ§è¯åï¼ä¸ºåç»­çäººä½å§¿æä¼°è®¡ãå¨ä½åæãèæç°å®/å¢å¼ºç°å®ç­ä»»å¡æä¾äºæ´å¯é ç3Déå»ºæ°æ®ï¼æå©äºæåè¿äºåºç¨çæ´ä½æ§è½ã</li>
</ul>
<p><strong>4. ç¸å³é¢åæåºç¨</strong></p>
<p>ä»¥ä¸é¢åæåºç¨å°ä»è¿é¡¹ç ç©¶ä¸­åçï¼</p>
<ul>
<li><strong>èæç°å® (VR) åå¢å¼ºç°å® (AR)ï¼</strong> å®æ¶ãåæ å®çè¿å¨ææå¯ä»¥ç¨äºæ´èªç¶çèæåèº«æ§å¶ãæå¿è¯å«åä¸èæç¯å¢çäº¤äºã</li>
<li><strong>ä½è²ç§å­¦ä¸è®­ç»ï¼</strong> è¿å¨åå¨ä½åæãçç©åå­¦ç ç©¶ï¼æ éæè´µçä¸ä¸è®¾å¤å³å¯è¿è¡ã</li>
<li><strong>å»çåº·å¤ï¼</strong> æ£èæ­¥æåæãåº·å¤è®­ç»ææè¯ä¼°ï¼æ¹ä¾¿å»çåæ£èä½¿ç¨ã</li>
<li><strong>çµå½±ä¸æ¸¸æå¶ä½ï¼</strong> è§è²å¨ç»ãé¢å¯è§åï¼éä½å¶ä½ææ¬åæ¶é´ã</li>
<li><strong>æºå¨äººå­¦ï¼</strong> äººæºäº¤äºãæºå¨äººæ¨¡ä»¿å­¦ä¹ ï¼ä½¿æºå¨äººè½æ´å¥½å°çè§£åå¤å¶äººç±»å¨ä½ã</li>
<li><strong>å®å¨çæ§ï¼</strong> å¼å¸¸è¡ä¸ºæ£æµãäººç¾¤åæã</li>
<li><strong>äººæºäº¤äº (HCI)ï¼</strong> æ´èªç¶ãç´è§çäº¤äºçé¢è®¾è®¡ã</li>
</ul>
<p><strong>5. ä»æè¦ä¸­æ¨æ­åºçå±éæ§</strong></p>
<p>å°½ç®¡ Kineo åå¾äºæ¾èè¿æ­¥ï¼ä½ä»æè¦ä¸­ä»å¯æ¨æ­åºä¸äºæ½å¨çå±éæ§ï¼</p>
<ul>
<li><strong>âSparse RGB Camerasâï¼</strong> æè¦ä¸­æå°âSparse RGB Camerasâï¼è¿å¯è½æå³çå®å¨ç¸æºæ°ééå¸¸å°ï¼ä¾å¦ï¼åªæä¸¤ä¸ä¸ªï¼çæåµä¸æ§è½å¯è½ä¼åå°éå¶ï¼æèå¯¹ç¸æºè§è§çè¦çèå´æä¸å®è¦æ±ã</li>
<li><strong>âFixed computational cost independent of sequence lengthâï¼</strong> è½ç¶è¿æ¯ä¸ä¸ªä¼ç¹ï¼ä½âfixedâçææ¬æ¬èº«å¯è½ä»ç¶è¾é«ï¼å°¤å¶æ¯å¨å¤çå¤§éç¸æºæé«åè¾¨çè§é¢æ¶ãæè¦ä¸­æå°âprocessing multi-view sequences faster than their duration in specific configuration (e.g., 36min to process 1h20min of footage)âï¼è¿è¡¨æè½ç¶æ¯å®æ¶å¿«ï¼ä½å¯¹äºæäºå¯¹å»¶è¿è¦æ±æé«çåºç¨ï¼å¦å®æ¶VRï¼ï¼å¯è½ä»éè¿ä¸æ­¥ä¼åã</li>
<li><strong>âConsumer-grade RGB camerasâï¼</strong> æ¶è´¹çº§ç¸æºéå¸¸å¨ä½åç§ãå¿«éè¿å¨æ¨¡ç³æå¤æèæ¯ä¸è¡¨ç°ä¸ä½³ãè½ç¶ Kineo æåäºé²æ£æ§ï¼ä½è¿äºåºæçç¸æºéå¶ä»å¯è½å½±åæç»çéå»ºè´¨éã</li>
<li><strong>âLeverages 2D keypoints from off-the-shelf detectorsâï¼</strong> Kineo çæ§è½å¨ä¸å®ç¨åº¦ä¸ä¾èµäºæä½¿ç¨ç2Då³é®ç¹æ£æµå¨çåç¡®æ§åé²æ£æ§ãå¦æ2Dæ£æµå¨å¨ç¹å®åºæ¯ä¸è¡¨ç°ä¸ä½³ï¼å¯è½ä¼å½±åæ´ä½ç³»ç»çæ§è½ã</li>
<li><strong>âConfidence-driven spatio-temporal keypoint sampling strategyâï¼</strong> è¿ç§ç­ç¥çæææ§å¯è½ä¸åºæ¯çå¤ææ§ãé®æ¡ç¨åº¦ä»¥åå³é®ç¹æ£æµçç½®ä¿¡åº¦åå¸æå³ãå¨æç«¯é®æ¡æå³é®ç¹ç½®ä¿¡åº¦æ®éè¾ä½çåºæ¯ä¸ï¼å¶æ§è½å¯è½ä¼ä¸éã</li>
<li><strong>æªæåçåºæ¯éå¶ï¼</strong> æè¦ä¸­æªæç¡®è¯´æ Kineo å¨æç«¯ç¯å¢ï¼å¦æ°´ä¸ãçé¾ãæç«¯åç§ååï¼æéäººç±»å¯¹è±¡ï¼å¦å¨ç©ï¼ä¸çè¡¨ç°ã</li>
</ul>
<p>æ»çæ¥è¯´ï¼Kineo æ¯ä¸é¡¹ä»¤äººå´å¥çç ç©¶ï¼å®éè¿åæ°çæ¹æ³è§£å³äºå¤è§è§è¿å¨ææé¢åçä¸ä¸ªæ ¸å¿ææï¼å³ç¸æºæ å®é®é¢ï¼å¹¶æ¾èæåäºæ§è½åå¯ç¨æ§ãå¶å¼æºä»£ç çåå¸ä¹å°æå¤§å°ä¿è¿è¯¥é¢åçè¿ä¸æ­¥ç ç©¶åå®éåºç¨ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present Kineo, a fully automatic, calibration-free pipeline for markerless
motion capture from videos captured by unsynchronized, uncalibrated,
consumer-grade RGB cameras.</li>
<li>Compared to previous state-of-the-art
approaches, Kineo reduces camera translation error by approximately 83-85%,
camera angular error by 86-92%, and world mean-per-joint error (W-MPJPE) by
83-91%.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.24464v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.24464v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.24410v1'></a></p>
<h2 id="a-hybrid-approach-for-visual-multi-object-tracking"><a href="https://arxiv.org/abs/2510.24410v1">A Hybrid Approach for Visual Multi-Object Tracking</a></h2>
<p><strong>Authors:</strong> Toan Van Nguyen, Rasmus G. K. Christiansen, Dirk Kraft, Leon Bodenhagen</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Categories:</strong> cs.CV, cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>This paper proposes a visual multi-object tracking method that jointly
employs stochastic and deterministic mechanisms to ensure identifier
consistency for unknown and time-varying target numbers under nonlinear
dynamics. A stochastic particle filter addresses nonlinear dynamics and
non-Gaussian noise, with support from particle swarm optimization (PSO) to
guide particles toward state distribution modes and mitigate divergence through
proposed fitness measures incorporating motion consistency, appearance
similarity, and social-interaction cues with neighboring targets. Deterministic
association further enforces identifier consistency via a proposed cost matrix
incorporating spatial consistency between particles and current detections,
detection confidences, and track penalties. Subsequently, a novel scheme is
proposed for the smooth updating of target states while preserving their
identities, particularly for weak tracks during interactions with other targets
and prolonged occlusions. Moreover, velocity regression over past states
provides trend-seed velocities, enhancing particle sampling and state updates.
The proposed tracker is designed to operate flexibly for both pre-recorded
videos and camera live streams, where future frames are unavailable.
Experimental results confirm superior performance compared to state-of-the-art
trackers. The source-code reference implementations of both the proposed method
and compared-trackers are provided on GitHub:
https://github.com/SDU-VelKoTek/GenTrack2</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Toan Van Nguyen, Rasmus G. K. Christiansen, Dirk Kraft, Leon Bodenhagenæ°åçè®ºæâA Hybrid Approach for Visual Multi-Object Trackingâçå¨é¢æè¦ã</p>
<hr />
<p><strong>è®ºææè¦ï¼è§è§å¤ç®æ è·è¸ªçæ··åæ¹æ³</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³è§è§å¤ç®æ è·è¸ªï¼MOTï¼ä¸­çæ ¸å¿ææï¼å³å¨ç®æ æ°éæªç¥ä¸éæ¶é´ååãè¿å¨éçº¿æ§åå­å¨éé«æ¯åªå£°çæåµä¸ï¼å¦ä½ç¡®ä¿ç®æ èº«ä»½çä¸è´æ§ãMOTé¢ä¸´ç©ä½ç¸ä¼¼æ§ãé®æ¡åä¸è§åè¿å¨ç­é®é¢ï¼è¿äºå ç´ å¯è½å¯¼è´èº«ä»½æ··æ·ãè·è¸ªä¸¢å¤±åå³èå°é¾ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
è¯¥è®ºææåºäºä¸ç§æ··åè§è§å¤ç®æ è·è¸ªæ¡æ¶ï¼ç»åäºéæºåç¡®å®æ§æºå¶ï¼ä»¥å®ç°é²æ£ååç¡®çè·è¸ªï¼
*   <strong>æ··åè·è¸ªæ¡æ¶ï¼</strong> ç»åäºéæºç²å­æ»¤æ³¢å¨åç¡®å®æ§å³èãç²å­æ»¤æ³¢å¨å¤çéçº¿æ§å¨åå­¦åéé«æ¯åªå£°ï¼èç¡®å®æ§å³èåç¡®ä¿èº«ä»½ä¸è´æ§ã
*   <strong>åºäºPSOçç²å­ä¼åï¼</strong> å¼å¥ç²å­ç¾¤ä¼åï¼PSOï¼æ¥å¼å¯¼ç²å­åç¶æåå¸æ¨¡å¼æ¶æï¼å¹¶éè¿æåºçéåºåº¦å½æ°ï¼ç»åè¿å¨ä¸è´æ§ãå¤è§ç¸ä¼¼æ§åä¸é»è¿ç®æ çç¤¾äº¤äºå¨çº¿ç´¢ï¼æ¥åè½»åæ£ã
*   <strong>æ¹è¿çææ¬ç©éµï¼</strong> æåºäºä¸ç§æ°çææ¬ç©éµï¼ç¨äºç¡®å®æ§å³èï¼è¯¥ç©éµæ´åäºç²å­ä¸å½åæ£æµä¹é´çç©ºé´ä¸è´æ§ãæ£æµç½®ä¿¡åº¦ä»¥åè·è¸ªæ©ç½ï¼ä»èå¢å¼ºäºèº«ä»½ä¸è´æ§ã
*   <strong>å¹³æ»ç¶ææ´æ°æ¹æ¡ï¼</strong> æåºäºä¸ç§æ°é¢çæ¹æ¡ï¼ç¨äºå¹³æ»æ´æ°ç®æ ç¶æï¼ç¹å«æ¯å¨ä¸å¶ä»ç®æ äºå¨åé¿æ¶é´é®æ¡æé´ï¼ä¸ºå¼±è·è¸ªä¿çå¶èº«ä»½ã
*   <strong>åºäºåå²ç¶æçéåº¦åå½ï¼</strong> å©ç¨è¿å»çç¶æè¿è¡éåº¦åå½ï¼æä¾è¶å¿ç§å­éåº¦ï¼ä»èæ¹è¿ç²å­éæ ·åç¶ææ´æ°ï¼å°¤å¶æ¯å¨é®æ¡ææ£æµå¨ä¿¡å·å¼±/åªå£°æ¶ã
*   <strong>çµæ´»çæä½æ¨¡å¼ï¼</strong> è¯¥è·è¸ªå¨è®¾è®¡ä¸ºå¯çµæ´»ç¨äºé¢å½è§é¢åå®æ¶æåå¤´æµï¼å¨åä¸ç§æåµä¸ï¼æªæ¥å¸§æ¯ä¸å¯ç¨çã</p>
<p><strong>3. ä¸»è¦ç»æåå¶éè¦æ§ï¼</strong>
å®éªç»æè¡¨æï¼ä¸ç°ææåè¿çè·è¸ªå¨ç¸æ¯ï¼ææåºçæ¹æ³è¡¨ç°åºåè¶çæ§è½ãå¨MOT17æ°æ®éä¸è¿è¡çè¯ä¼°æ¾ç¤ºï¼è¯¥æ¹æ³å¨ATAï¼å¹³åè·è¸ªç²¾åº¦ï¼ãIDF1ãHOTAï¼é«é¶è·è¸ªç²¾åº¦ï¼åMOTAï¼å¤ç®æ è·è¸ªç²¾åº¦ï¼ç­å¤ä¸ªææ ä¸åä¼äºå¶ä»æ¹æ³ï¼å¹¶ä¸IDSWï¼èº«ä»½åæ¢ï¼æ°éæ´å°ãè¿è¡¨æè¯¥æ¹æ³å¨ä¿æé²æ£æ§è½çåæ¶ï¼æ¾èæé«äºè·è¸ªç²¾åº¦åèº«ä»½ä¸è´æ§ãè®ºæè¿æä¾äºæºä»£ç ï¼æ¹ä¾¿å¤ç°åæ¯è¾è¯ä¼°ã</p>
<p><strong>4. è®ºæä¸­æå°çå±éæ§ï¼</strong>
*   å°½ç®¡ç²å­æ»¤æ³¢å¨è½å¤å¤çä¸ç¡®å®æ§åéçº¿æ§å¨åå­¦ï¼ä½å®ä»¬å¯è½äº§çä¸ä¸è´çç»æã
*   ä¼ ç»çç²å­æ»¤æ³¢å¨æ¹æ³å¨ç®æ æ°éå¢å æ¶æ©å±æ§è¾å·®ï¼å¹¶ä¸å¨é¢ç¹ç®æ ååçæåµä¸å¯é æ§éä½ã
*   åºäºæ£æµçæ¹æ³è½ç¶è½æ´ç¡®å®å°å¤çç®æ æ·»å åç§»é¤ï¼ä½å¾å¾å¿½ç¥äºè·è¸ªæ¨æ­çä¼åã
*   å¨PSOä¸­ï¼å¦æç²å­æ°éå¾å°ï¼ä¸¢å¼éå¼éè¦è°¨æä½¿ç¨ï¼å¦åå¯è½å¯¼è´æ§è½ä¸éã
*   å¨é¿æ¶é´é®æ¡æé´ï¼è¾¹çæ¡å°ºå¯¸çæ´æ°å¯è½å¯¼è´ä¸åå®éçå¢é¿ææ¶ç¼©ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   æªæ¥çç ç©¶å°ä¾§éäºç®æ äº¤äºæ¨¡åï¼ä»¥è¿ä¸æ­¥æ¹è¿é®æ¡æé´çç¶ææ´æ°ã
*   æ¢ç´¢æåç®æ ç¤¾äº¤è¡ä¸ºæ¨¡å¼çæ¹æ³ã</p>
<hr />
<p>è¿ç¯è®ºæéè¿å¶æ··åæ¹æ³ï¼å¨å¤çå¤ç®æ è·è¸ªçå¤ææ§æ¹é¢åå¾äºæ¾èè¿å±ï¼ç¹å«æ¯å¨éçº¿æ§å¨æãæªç¥ç®æ æ°éåé®æ¡åºæ¯ä¸ãéè¿ç»åPSOä¼åãæ¹è¿çå³èææ¬åæºè½çç¶ææ´æ°æºå¶ï¼è¯¥æ¹æ³ææå°è§£å³äºç°æè·è¸ªå¨çä¸äºå³é®éå¶ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Subsequently, a novel scheme is
proposed for the smooth updating of target states while preserving their
identities, particularly for weak tracks during interactions with other targets
and prolonged occlusions.</li>
<li>Experimental results confirm superior performance compared to state-of-the-art
trackers.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.24410v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.24410v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.24399v1'></a></p>
<h2 id="gentrack-a-new-generation-of-multi-object-tracking"><a href="https://arxiv.org/abs/2510.24399v1">GenTrack: A New Generation of Multi-Object Tracking</a></h2>
<p><strong>Authors:</strong> Toan Van Nguyen, Rasmus G. K. Christiansen, Dirk Kraft, Leon Bodenhagen</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Categories:</strong> cs.CV, cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>This paper introduces a novel multi-object tracking (MOT) method, dubbed
GenTrack, whose main contributions include: a hybrid tracking approach
employing both stochastic and deterministic manners to robustly handle unknown
and time-varying numbers of targets, particularly in maintaining target
identity (ID) consistency and managing nonlinear dynamics, leveraging particle
swarm optimization (PSO) with some proposed fitness measures to guide
stochastic particles toward their target distribution modes, enabling effective
tracking even with weak and noisy object detectors, integration of social
interactions among targets to enhance PSO-guided particles as well as improve
continuous updates of both strong (matched) and weak (unmatched) tracks,
thereby reducing ID switches and track loss, especially during occlusions, a
GenTrack-based redefined visual MOT baseline incorporating a comprehensive
state and observation model based on space consistency, appearance, detection
confidence, track penalties, and social scores for systematic and efficient
target updates, and the first-ever publicly available source-code reference
implementation with minimal dependencies, featuring three variants, including
GenTrack Basic, PSO, and PSO-Social, facilitating flexible reimplementation.
Experimental results have shown that GenTrack provides superior performance on
standard benchmarks and real-world scenarios compared to state-of-the-art
trackers, with integrated implementations of baselines for fair comparison.
Potential directions for future work are also discussed. The source-code
reference implementations of both the proposed method and compared-trackers are
provided on GitHub: https://github.com/SDU-VelKoTek/GenTrack</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Toan Van Nguyen, Rasmus G. K. Christiansen, Dirk Kraft, Leon Bodenhagenæ°åçè®ºæâGenTrack: A New Generation of Multi-Object Trackingâçå¨é¢æè¦ã</p>
<hr />
<p><strong>è®ºææè¦ï¼GenTrack: ä¸ç§æ°ä¸ä»£å¤ç®æ è·è¸ªæ¹æ³</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
å¤ç®æ è·è¸ªï¼MOTï¼æ¨å¨åç¡®ãè¿ç»­å°è·è¸ªå¾åæè§é¢ä¸­å¤ä¸ªæå´è¶£çç®æ ãç¶èï¼ç±äºç®æ è¿å¨ä¸è§åãé¢ç¹é®æ¡ãç®æ é´è§è§ç¸ä¼¼æ§ä»¥åç®æ æ°ééæ¶é´ååï¼è¿å¥/ç¦»å¼åºæ¯ï¼ç­å ç´ ï¼å®ç°é²æ£çMOTä»ç¶æ¯ä¸ä¸ªææãç°ææ¹æ³ï¼ç¹å«æ¯åºäºå¡å°æ¼æ»¤æ³¢å¨çæ¹æ³ï¼éå¸¸åè®¾çº¿æ§è¿å¨åé«æ¯åªå£°ï¼å¹¶ä¸å¨å¤çéçº¿æ§å¨åå­¦åéé«æ¯åªå£°æ¹é¢å­å¨å±éæ§ãåºäºç²å­æ»¤æ³¢å¨çæ¹æ³è½ç¶è½å¤çéçº¿æ§é®é¢ï¼ä½å¾å¾è®¡ç®ææ¬é«æï¼ä¸å¨ç®æ æ°éå¯åæ¶æ§è½ä¸ä½³ï¼å®¹æåºç°IDåæ¢åè½¨è¿¹ä¸¢å¤±ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
GenTrackæåºäºä¸ç§æ°é¢çMOTæ¹æ³ï¼å¶ä¸»è¦è´¡ç®åæ¬ï¼</p>
<ul>
<li><strong>æ··åè·è¸ªæ¹æ³ï¼</strong> ç»åäºéæºï¼ç²å­ç¾¤ä¼åPSOï¼åç¡®å®æ§æ¹æ³ï¼ä»¥é²æ£å°å¤çæªç¥åæ¶åçç®æ æ°éï¼ç¹å«æ¯å¨ä¿æç®æ èº«ä»½ï¼IDï¼ä¸è´æ§åç®¡çéçº¿æ§å¨åå­¦æ¹é¢ã</li>
<li><strong>PSOå¼å¯¼çç²å­ï¼</strong> å©ç¨ç²å­ç¾¤ä¼åï¼PSOï¼ç®æ³ï¼ç»åæåºçéåºåº¦åº¦éï¼å¼å¯¼éæºç²å­è¶åå¶ç®æ åå¸æ¨¡å¼ï¼å³ä½¿å¨æ£æµå¨è¾å¼±ååªå£°è¾å¤§æ¶ä¹è½å®ç°ææè·è¸ªã</li>
<li><strong>ç¤¾ä¼äº¤äºéæï¼</strong> æ´åç®æ é´çç¤¾ä¼äº¤äºï¼ä»¥å¢å¼ºPSOå¼å¯¼çç²å­ï¼å¹¶æ¹è¿å¼ºï¼å¹éï¼åå¼±ï¼æªå¹éï¼è½¨è¿¹çè¿ç»­æ´æ°ï¼ä»èåå°IDåæ¢åè½¨è¿¹ä¸¢å¤±ï¼å°¤å¶æ¯å¨é®æ¡æé´ã</li>
<li><strong>éæ°å®ä¹çè§è§MOTåºçº¿ï¼</strong> å¼å¥äºä¸ä¸ªåºäºGenTrackçè§è§MOTåºçº¿ï¼åå«ä¸ä¸ªå¨é¢çç¶æåè§æµæ¨¡åï¼è¯¥æ¨¡ååºäºç©ºé´ä¸è´æ§ãå¤è§ãæ£æµç½®ä¿¡åº¦ãè½¨è¿¹æ©ç½åç¤¾ä¼åæ°ï¼ç¨äºç³»ç»é«æçç®æ æ´æ°ã</li>
<li><strong>å¼æºå®ç°ï¼</strong> æä¾äºé¦ä¸ªå¬å¼å¯ç¨çæºä»£ç åèå®ç°ï¼å·ææå°çä¾èµæ§ï¼å¹¶åå«GenTrack BasicãPSOåPSO-Socialä¸ç§åä½ï¼ä¾¿äºçµæ´»å¤ç°åæ¯è¾ã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
å®éªç»æè¡¨æï¼GenTrackå¨æ ååºååçå®åºæ¯ä¸­ï¼ä¸æåè¿çè·è¸ªå¨ç¸æ¯ï¼æä¾äºåè¶çæ§è½ãå·ä½æ¥è¯´ï¼</p>
<ul>
<li><strong>ä¼è¶çè·è¸ªæ§è½ï¼</strong> å¨äººç±»è·è¸ªï¼MOT17-04ï¼åå¥¶çè·è¸ªï¼MooTrack360ï¼åºæ¯ä¸­ï¼GenTrackå§ç»ä¼äºç°ææåè¿çè·è¸ªå¨ãå¨å¥¶çè·è¸ªä¸­ï¼GenTrackå®ç°äº100%çæåçï¼æ IDåæ¢ãå¨äººç±»è·è¸ªä¸­ï¼GenTrack PSO-Socialä¹ä»¥æå°çIDåæ¢ï¼ä»4æ¬¡ï¼å®ç°äºæä½³æ§è½ã</li>
<li><strong>å¯¹åæ°è®¾ç½®ä¸ææï¼</strong> GenTrackçæ§è½å¯¹åæ°è®¾ç½®è¡¨ç°åºé«åº¦ä¸æææ§ï¼è¿å¢å¼ºäºå¶å¨ä¸ååºç¨ä¸­çé²æ£æ§ã</li>
<li><strong>è®¡ç®æçï¼</strong> å°½ç®¡æ¯åºäºç²å­æ»¤æ³¢å¨çæ¹æ³ï¼GenTrackå¨ä¿æé²æ£æ§è½çåæ¶ï¼ä»ä½¿ç¨å°éç²å­ï¼GenTrack Basic 8ä¸ªï¼PSOåPSO-Social 6ä¸ªï¼ï¼å®ç°äºè¾ä½çCPUå»¶è¿ï¼å¥¶çè·è¸ªåºæ¯ä¸­åå«ä¸º3.67msã6.35mså6.74msï¼äººç±»è·è¸ªåºæ¯ä¸­åå«ä¸º56msã64mså92msï¼ï¼è¡¨æå¶éç¨äºå®æ¶è·è¸ªã</li>
</ul>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
è®ºæä¸­æ²¡ææç¡®æåºGenTrackçå±éæ§ï¼ä½éè¿è®¨è®ºæªæ¥å·¥ä½æ¹åï¼é´æ¥æç¤ºäºå½åæ¹æ³çæ¹è¿ç©ºé´ï¼</p>
<ul>
<li><strong>å¤è§ç¸ä¼¼æ§åº¦éï¼</strong> å½åä½¿ç¨HoGç¹å¾çä½å¼¦ç¸ä¼¼åº¦ï¼å¯è½æªååèèç®æ ç¹å®æåºç¨ç¹å®çå¤è§ç¹å¾ï¼ä¸æªå®å¨å¿½ç¥èæ¯æåºã</li>
<li><strong>è¿å¨æ¨¡åï¼</strong> å½åéç¨éæºè¿å¨æ¨¡åï¼è½ç¶éè¿PSOå¼å¯¼æææ¹è¿ï¼ä½ä»å¯éè¿æ´ååºäºè·¯å¾çä¸»å¯¼éèç±»ç­æ¹æ³è¿ä¸æ­¥å¢å¼ºï¼ä»¥æ´å¥½å°å¤çé®æ¡åIDåæ¢ã</li>
<li><strong>ç²å­åå§åç­ç¥ï¼</strong> å°½ç®¡ä½éåºåº¦ç²å­å¯ä»¥è¢«ä¸¢å¼ææ¿æ¢ï¼ä½åªå£°ä¾èµçç²å­åå§åç­ç¥å¯è½è¿ä¸æ­¥åå°åä½å¹¶æé«å¨åªå£°ç¯å¢ä¸­çæ§è½ã</li>
</ul>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
è®ºææåºäºå ä¸ªæåæ¯çæªæ¥ç ç©¶æ¹åï¼</p>
<ul>
<li><strong>æ¹è¿å¤è§ç¸ä¼¼æ§ï¼</strong> éå¯¹ç¹å®ç®æ ååºç¨å®å¶å¤è§ç¸ä¼¼æ§åº¦éï¼å¹¶å¨ç®æ åºååè¿è¡æ¯è¾ï¼ä»¥å¿½ç¥èæ¯æåºã</li>
<li><strong>åªå£°èªéåºéæ ·ï¼</strong> å¼å¥åªå£°ä¾èµçç²å­åå§åç­ç¥ï¼ä¾å¦ä½¿ç¨Metropolis-Hastingsç®æ³è¿è¡æè®®çæï¼ä»¥åå°åä½å¹¶æé«å¨åªå£°ç¯å¢ä¸­çæ§è½ã</li>
<li><strong>æ¹è¿è¿å¨æ¨¡åï¼</strong> éè¿æ´ååºäºè·¯å¾çä¸»å¯¼éèç±»æ¥å¢å¼ºè¿å¨æ¨¡åï¼ä»¥æ´å¥½å°å¤çé®æ¡åIDåæ¢ã</li>
<li><strong>ç¾¤ç»è·è¸ªï¼</strong> å¼åä¸ç§æ°çGenTrackåä½ï¼éè¿æ´åèç±»ç®æ³ï¼å¦DBSCANï¼åç¾¤ç»æä½ï¼æåæ·»å /ç§»é¤ãç¾¤ç»åå¹¶/æåï¼æ¥å®ç°ç¾¤ç»è·è¸ªã</li>
<li><strong>å¤æåå¤´å¤ç®æ è·è¸ªï¼</strong> æ©å±GenTrackæ¡æ¶ä»¥æ¯æå¤æåå¤´MOTï¼åæ¬æéå åæ éå çåºæ¯ï¼éè¿å¸§åè·è¸ªåè·¨æåå¤´å³èï¼æå°å¤æåå¤´æ£æµæ´åå°ç»ä¸æ¡æ¶ä¸­ã</li>
<li><strong>ä¸ç»´ï¼3Dï¼å¤ç®æ è·è¸ªï¼</strong> éæ°å®ä¹å¯¹è±¡ç¶æåè§æµæ¨¡åï¼ä»¥æ´å3Dæ°æ®ï¼å©ç¨3Då¯¹è±¡æ£æµå¨æèç±»æ¹æ³è¿è¡å¯¹è±¡æ£æµã</li>
</ul>
<hr />
<p>æ»èè¨ä¹ï¼GenTrackéè¿å¶æ··åè·è¸ªæ¡æ¶ãPSOå¼å¯¼çç²å­ãç¤¾ä¼äº¤äºéæä»¥åå¨é¢çç¶æåè§æµæ¨¡åï¼ä¸ºå¤ç®æ è·è¸ªé¢åå¸¦æ¥äºæ¾èè¿æ­¥ãå®å¨ä¿æé«ç²¾åº¦åIDä¸è´æ§çåæ¶ï¼å±ç°äºè®¡ç®æçåå¯¹åæ°ä¸ææçç¹æ§ï¼ä¸ºæªæ¥çMOTç ç©¶åå®éåºç¨å¥ å®äºåå®çåºç¡ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>This paper introduces a novel multi-object tracking (MOT) method, dubbed
GenTrack, whose main contributions include: a hybrid tracking approach
employing both stochastic and deterministic manners to robustly handle unknown
and time-varying numbers of targets, particularly in maintaining target
identity (ID) consistency and managing nonlinear dynamics, leveraging particle
swarm optimization (PSO) with some proposed fitness measures to guide
stochastic particles toward their target distribution modes, enabling effective
tracking even with weak and noisy object detectors, integration of social
interactions among targets to enhance PSO-guided particles as well as improve
continuous updates of both strong (matched) and weak (unmatched) tracks,
thereby reducing ID switches and track loss, especially during occlusions, a
GenTrack-based redefined visual MOT baseline incorporating a comprehensive
state and observation model based on space consistency, appearance, detection
confidence, track penalties, and social scores for systematic and efficient
target updates, and the first-ever publicly available source-code reference
implementation with minimal dependencies, featuring three variants, including
GenTrack Basic, PSO, and PSO-Social, facilitating flexible reimplementation.</li>
<li>Experimental results have shown that GenTrack provides superior performance on
standard benchmarks and real-world scenarios compared to state-of-the-art
trackers, with integrated implementations of baselines for fair comparison.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.24399v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.24399v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.24214v1'></a></p>
<h2 id="scope-saliency-coverage-oriented-token-pruning-for-efficient-multimodel-llms"><a href="https://arxiv.org/abs/2510.24214v1">SCOPE: Saliency-Coverage Oriented Token Pruning for Efficient Multimodel LLMs</a></h2>
<p><strong>Authors:</strong> Jinhong Deng, Wen Li, Joey Tianyi Zhou, Yang He</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Multimodal Large Language Models (MLLMs) typically process a large number of
visual tokens, leading to considerable computational overhead, even though many
of these tokens are redundant. Existing visual token pruning methods primarily
focus on selecting the most salient tokens based on attention scores, resulting
in the semantic incompleteness of the selected tokens. In this paper, we
propose a novel visual token pruning strategy, called
\textbf{S}aliency-\textbf{C}overage \textbf{O}riented token \textbf{P}runing
for \textbf{E}fficient MLLMs (SCOPE), to jointly model both the saliency and
coverage of the selected visual tokens to better preserve semantic
completeness. Specifically, we introduce a set-coverage for a given set of
selected tokens, computed based on the token relationships. We then define a
token-coverage gain for each unselected token, quantifying how much additional
coverage would be obtained by including it. By integrating the saliency score
into the token-coverage gain, we propose our SCOPE score and iteratively select
the token with the highest SCOPE score. We conduct extensive experiments on
multiple vision-language understanding benchmarks using the LLaVA-1.5 and
LLaVA-Next models. Experimental results demonstrate that our method
consistently outperforms prior approaches. Our code is available at
\href{https://github.com/kinredon/SCOPE}{https://github.com/kinredon/SCOPE}.</p>
<p><strong>Analysis:</strong></p>
<p>ä»¥ä¸æ¯Jinhong Deng, Wen Li, Joey Tianyi Zhou, Yang Heæ°åçè®ºæâSCOPE: Saliency-Coverage Oriented Token Pruning for Efficient Multimodel LLMsâçå¨é¢æè¦ï¼</p>
<p><strong>1. è®ºæä¸»è¦é®é¢æç ç©¶é®é¢</strong></p>
<p>å¤æ¨¡æå¤§åè¯­è¨æ¨¡åï¼MLLMsï¼å¨å¤çè§è§è¾å¥æ¶éå¸¸ä¼çæå¤§éçè§è§tokenï¼è¿å¯¼è´äºæ¾èçè®¡ç®å¼éï¼å°¤å¶æ¯å¨é«åè¾¨çå¾åæå¯éè§é¢åºæ¯ä¸­ãç°æè§è§tokenåªææ¹æ³ä¸»è¦å³æ³¨åºäºæ³¨æååæ°éæ©ææ¾èçtokenï¼ä½è¿å¾å¾å¯¼è´è¯­ä¹ä¸å®æ´æ§ï¼å ä¸ºå®ä»¬å¿½ç¥äºå³é®çä¸ä¸æä¿¡æ¯ãå æ­¤ï¼è®ºææ¨å¨è§£å³å¦ä½å¨å¤§å¹åå°è§è§tokenæ°éçåæ¶ï¼ææä¿çè¯­ä¹å®æ´æ§ï¼ä»¥æé«MLLMsçè®¡ç®æçåæ§è½ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®</strong></p>
<p>è®ºææåºäºä¸ç§æ°é¢çè§è§tokenåªæç­ç¥ï¼åä¸º<strong>S</strong>aliency-<strong>C</strong>overage <strong>O</strong>riented token <strong>P</strong>runing for <strong>E</strong>fficient MLLMs (SCOPE)ãå¶æ ¸å¿åæ°å¨äºï¼</p>
<ul>
<li><strong>èåå»ºæ¨¡æ¾èæ§ä¸è¦çåº¦ï¼</strong> SCOPEæ¹æ³çªç ´äºä¼ ç»åªææ¹æ³ä»å³æ³¨æ¾èæ§çå±éï¼é¦æ¬¡èåèèäºæéè§è§tokençæ¾èæ§åè¦çåº¦ï¼ä»¥æ´å¥½å°ä¿çè¯­ä¹å®æ´æ§ã</li>
<li><strong>å¼å¥éåè¦çåº¦ï¼Set-Coverageï¼æ¦å¿µï¼</strong> è®ºæä¸ºç»å®çä¸ç»å·²étokenå®ä¹äºéåè¦çåº¦ï¼è¯¥è¦çåº¦åºäºtokenä¹é´çå³ç³»è®¡ç®ï¼éåäºå·²étokenå¯¹æ´ä¸ªè¯­ä¹ç©ºé´çä»£è¡¨æ§ç¨åº¦ã</li>
<li><strong>å®ä¹Tokenè¦çåº¦å¢çï¼</strong> å¯¹äºæ¯ä¸ªæªétokenï¼è®ºæå®ä¹äºtokenè¦çåº¦å¢çï¼éåäºå°å¶åå«è¿æ¥è½å¸¦æ¥å¤å°é¢å¤çè¦çåº¦ã</li>
<li><strong>æåºSCOPEåæ°å¹¶è¿­ä»£éæ©ï¼</strong> éè¿å°æ¾èæ§åæ°æ´åå°tokenè¦çåº¦å¢çä¸­ï¼è®ºææåºäºSCOPEåæ°ï¼å¹¶è¿­ä»£éæ©å·ææé«SCOPEåæ°çtokenãè¿ç§ç­ç¥ç¡®ä¿äºä¸ä»ä¿çæå·ä¿¡æ¯éçtokenï¼åæ¶ä¹ä¿è¯äºå¹¿æ³çè¯­ä¹è¦çã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹</strong></p>
<p>SCOPEæ¹æ³å¨å¤ä¸ªè§è§-è¯­è¨çè§£åºåæµè¯ï¼åæ¬LLaVA-1.5åLLaVA-Nextæ¨¡åï¼ä¸è¿è¡äºå¹¿æ³å®éªï¼å¹¶åå¾äºæ¾èææï¼</p>
<ul>
<li><strong>æ§è½è¶è¶ç°ææ¹æ³ï¼</strong> SCOPEå¨æætokenéç½®ä¸åæç»­ä¼äºç°æåªææ¹æ³ãä¾å¦ï¼å¨LLaVA-1.5 7Bæ¨¡åä¸ï¼å³ä½¿å°è§è§tokenæ°éåå°9åï¼ä¿ç64ä¸ªtokenï¼ï¼SCOPEä»è½ä¿æåå§æ§è½ç96.0%ï¼æ¾èä¼äºVisionZipï¼93.5%ï¼åSparseVLMï¼85.1%ï¼ç­åºçº¿ã</li>
<li><strong>å¨æç«¯åç¼©ä¸è¡¨ç°ç¨³å®ï¼</strong> å³ä½¿å¨æç«¯åç¼©ï¼å¦ä»ä¿ç8ä¸ªtokenï¼çæåµä¸ï¼SCOPEä¹è¡¨ç°åºåè¶çæ§è½ç¨³å®æ§ï¼å¹¶ä»¥è¶æ¥è¶å¤§çä¼å¿æç»­ä¼äºVisionZipã</li>
<li><strong>å¨è§é¢åºåæµè¯ä¸­è¡¨ç°åºè²ï¼</strong> å¨Video-LLaVAä¸ï¼SCOPEå¨å¤§å¹åªæï¼ä»ä¿ç136ä¸ªtokenï¼åå§2048ä¸ªï¼åï¼å ä¹å®å¨ä¿çäºåå§æ§è½ï¼è¯æäºå¶å¨è§é¢-è¯­è¨ä»»å¡ä¸­çå¼ºå¤§æææ§ã</li>
<li><strong>æåæ¨¡åæ§è½ï¼</strong> å¨æäºåºåæµè¯ï¼å¦POPEåMMVetï¼ä¸ï¼SCOPEçè³è¶è¶äºåå§æ¨¡åçæ§è½ï¼è¿è¡¨æMLLMsä¸­çè§è§tokenå­å¨åä½ï¼èSCOPEä¸ä»åå°äºåä½ï¼è¿éè¿æ¶é¤åä½ä¿¡æ¯æé«äºæ§è½ã</li>
<li><strong>è®¡ç®æçé«ï¼</strong> å°½ç®¡SCOPEçå»¶è¿ç¥é«äºPDropï¼ä½å®ç¸å¯¹äºå¨tokenåºçº¿ä»å®ç°äº3.2åçå éï¼è¯æäºå¶å¨ä¿ææ§è½çåæ¶å·æè®¡ç®æçã</li>
</ul>
<p>è¿äºç»æè¡¨æï¼SCOPEå¨å¤§å¹åå°è§è§tokenæ°éçåæ¶ï¼è½å¤ææä¿çè¯­ä¹å®æ´æ§ï¼æ¾èæé«äºMLLMsçè®¡ç®æçåå®ç¨æ§ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§</strong></p>
<p>è®ºæä¹å¦è¯å°æåºäºSCOPEçå ä¸ªå±éæ§ï¼</p>
<ul>
<li><strong>æ½å¨çç»ç²åº¦ä¿¡æ¯ä¸¢å¤±ï¼</strong> å°½ç®¡SCOPEåªåå¹³è¡¡æ¾èæ§åè¦çåº¦ï¼ä½æ¿è¿çtokenåªæä»å¯è½å¯¼è´ç»ç²åº¦æç¨æè¯­ä¹ä¿¡æ¯çä¸¢å¤±ï¼è¿å¯è½ä¼å½±åéè¦è¯¦ç»è§è§çè§£çä»»å¡ã</li>
<li><strong>æ³åè½åæå¾è¿ä¸æ­¥éªè¯ï¼</strong> å®éªä¸»è¦åºäºå¹¿æ³ä½¿ç¨çè§è§-è¯­è¨åºåæµè¯åä¸¤ä¸ªä»£è¡¨æ§çMLLMï¼LLaVA 1.5åLLaVA-Nextï¼ãå æ­¤ï¼SCOPEå¯¹å¶ä»ä»»å¡ææ¨¡åæ¶æçæ³åè½åå°å¾ååéªè¯ã</li>
</ul>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹å</strong></p>
<p>è®ºæå¹¶æªæç¡®æåºæªæ¥çç ç©¶æ¹åï¼ä½ä»å¶å±éæ§åè´¡ç®ä¸­å¯ä»¥æ¨æ­åºä»¥ä¸æ½å¨æ¹åï¼</p>
<ul>
<li><strong>ä¼åç»ç²åº¦ä¿¡æ¯ä¿çï¼</strong> æ¢ç´¢æ´ç²¾ç»çåªæç­ç¥ï¼ä»¥å¨æç«¯åç¼©ä¸æ´å¥½å°ä¿çç»ç²åº¦æç¨æè¯­ä¹ä¿¡æ¯ï¼ä¾å¦éè¿èªéåºå°è°æ´æ¾èæ§ä¸è¦çåº¦çæéï¼æå¼å¥æ´å¤æçtokenå³ç³»å»ºæ¨¡ã</li>
<li><strong>æ©å±å°æ´å¤æ¨¡ååä»»å¡ï¼</strong> å¨æ´å¹¿æ³çMLLMæ¶æï¼å¦Qwen2-VLä»¥å¤çå¶ä»æ¨¡åï¼åæ´å¤æ ·åçè§è§-è¯­è¨ä»»å¡ï¼å¦3Dè§è§çè§£ãå·èº«AIç­ï¼ä¸éªè¯SCOPEçæ³åè½åã</li>
<li><strong>å¨æåªæç­ç¥ï¼</strong> ç ç©¶æ ¹æ®è¾å¥åå®¹æä»»å¡éæ±å¨æè°æ´åªææ¯ä¾çç­ç¥ï¼ä»¥å®ç°æ´çµæ´»åé«æçtokenç®¡çã</li>
<li><strong>çè®ºåæä¸å¯è§£éæ§ï¼</strong> å¯¹SCOPEçæ§è½è¾¹çååªæå³ç­è¿è¡æ´æ·±å¥ççè®ºåæï¼å¹¶æé«å¶å¯è§£éæ§ï¼ä»¥æ´å¥½å°çè§£å¶å¨ä¸ååºæ¯ä¸çè¡ä¸ºã</li>
<li><strong>ç»åå¶ä»åç¼©ææ¯ï¼</strong> æ¢ç´¢å°SCOPEä¸tokenåå¹¶ãéåç­å¶ä»è§è§tokenåç¼©ææ¯ç¸ç»åï¼ä»¥å®ç°æ´å¤§çåç¼©çåæçæåã</li>
</ul>
<p>æ»èè¨ä¹ï¼SCOPEä¸ºé«æMLLMsçè§è§tokenåªææä¾äºä¸ä¸ªååæ§ä¸ææçæ°æ¡æ¶ï¼éè¿èåä¼åæ¾èæ§åè¦çåº¦ï¼å¨è®¡ç®æçåè¯­ä¹å®æ´æ§ä¹é´åå¾äºåè¶çå¹³è¡¡ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this paper, we
propose a novel visual token pruning strategy, called
\textbf{S}aliency-\textbf{C}overage \textbf{O}riented token \textbf{P}runing
for \textbf{E}fficient MLLMs (SCOPE), to jointly model both the saliency and
coverage of the selected visual tokens to better preserve semantic
completeness.</li>
<li>Specifically, we introduce a set-coverage for a given set of
selected tokens, computed based on the token relationships.</li>
<li>By integrating the saliency score
into the token-coverage gain, we propose our SCOPE score and iteratively select
the token with the highest SCOPE score.</li>
<li>Experimental results demonstrate that our method
consistently outperforms prior approaches.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.24214v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.24214v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-10-29 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
