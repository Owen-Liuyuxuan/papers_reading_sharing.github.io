<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-10-29 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-10-24/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-10-30/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-10-29">Arxiv Computer Vision Papers - 2025-10-29</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#generative-view-stitching" class="nav-link">Generative View Stitching</a>
                </li>
                <li class="nav-item">
                    <a href="#uniform-discrete-diffusion-with-metric-path-for-video-generation" class="nav-link">Uniform Discrete Diffusion with Metric Path for Video Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#mic-bev-multi-infrastructure-camera-birds-eye-view-transformer-with-relation-aware-fusion-for-3d-object-detection" class="nav-link">MIC-BEV: Multi-Infrastructure Camera Bird's-Eye-View Transformer with Relation-Aware Fusion for 3D Object Detection</a>
                </li>
                <li class="nav-item">
                    <a href="#group-relative-attention-guidance-for-image-editing" class="nav-link">Group Relative Attention Guidance for Image Editing</a>
                </li>
                <li class="nav-item">
                    <a href="#groundloc-efficient-large-scale-outdoor-lidar-only-localization" class="nav-link">GroundLoc: Efficient Large-Scale Outdoor LiDAR-Only Localization</a>
                </li>
                <li class="nav-item">
                    <a href="#latent-sketchpad-sketching-visual-thoughts-to-elicit-multimodal-reasoning-in-mllms" class="nav-link">Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal Reasoning in MLLMs</a>
                </li>
                <li class="nav-item">
                    <a href="#kineo-calibration-free-metric-motion-capture-from-sparse-rgb-cameras" class="nav-link">Kineo: Calibration-Free Metric Motion Capture From Sparse RGB Cameras</a>
                </li>
                <li class="nav-item">
                    <a href="#a-hybrid-approach-for-visual-multi-object-tracking" class="nav-link">A Hybrid Approach for Visual Multi-Object Tracking</a>
                </li>
                <li class="nav-item">
                    <a href="#gentrack-a-new-generation-of-multi-object-tracking" class="nav-link">GenTrack: A New Generation of Multi-Object Tracking</a>
                </li>
                <li class="nav-item">
                    <a href="#scope-saliency-coverage-oriented-token-pruning-for-efficient-multimodel-llms" class="nav-link">SCOPE: Saliency-Coverage Oriented Token Pruning for Efficient Multimodel LLMs</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-10-29">Arxiv Computer Vision Papers - 2025-10-29</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>好的，这是一份针对您提供的 Arxiv 论文列表的每日报告执行摘要，旨在帮助忙碌的研究人员快速了解计算机视觉和机器学习领域的最新进展。</p>
<hr />
<p><strong>每日 Arxiv 计算机视觉与机器学习报告执行摘要 (2025-10-28)</strong></p>
<p><strong>概述与主要趋势：</strong></p>
<p>今天的报告涵盖了计算机视觉领域广泛而活跃的研究，主要趋势集中在<strong>生成模型与多模态应用</strong>、<strong>3D 感知与自动驾驶</strong>以及<strong>效率与优化</strong>。生成模型在图像、视频和多模态推理方面持续取得突破，特别是在视图合成、视频生成和图像编辑方面。3D 感知和自动驾驶领域则侧重于多传感器融合、高效定位和目标检测。此外，为了应对大型模型带来的计算挑战，研究人员正积极探索模型剪枝和高效架构。</p>
<p><strong>特别重要或创新的论文：</strong></p>
<ol>
<li><strong>"Generative View Stitching" (Chonghyuk Song et al.)</strong>: 这篇论文在<strong>新颖视图合成</strong>方面展现了显著的创新。它可能通过生成模型将不同视角的图像无缝拼接，解决传统方法在视角不一致性或遮挡方面的挑战，对于虚拟现实、3D 重建和内容创作具有巨大潜力。</li>
<li><strong>"Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal Reasoning in MLLMs" (Huanyu Zhang et al.)</strong>: 这是一项<strong>多模态大语言模型 (MLLMs) 交互</strong>的开创性工作。通过引入“潜在画板”的概念，允许用户以视觉草图的形式引导 MLLMs 进行推理，极大地增强了人机交互的直观性和表达力，为 MLLMs 的应用打开了新的大门。</li>
<li><strong>"Uniform Discrete Diffusion with Metric Path for Video Generation" (Haoge Deng et al.)</strong>: 在<strong>视频生成</strong>领域，这篇论文可能通过引入新的扩散模型范式，解决了视频生成中时间一致性和质量的难题，是视频内容创作和合成的重要进展。</li>
</ol>
<p><strong>新兴研究方向或技术：</strong></p>
<ol>
<li><strong>多模态大语言模型 (MLLMs) 的交互与推理增强</strong>: "Latent Sketchpad" 明确指出 MLLMs 不仅仅是理解多模态输入，更要能通过多模态输出（如草图）进行交互和引导推理，这是未来 MLLMs 发展的重要方向。</li>
<li><strong>高效且鲁棒的 3D 感知与定位</strong>: "MIC-BEV" 和 "GroundLoc" 强调了在复杂环境中，利用多基础设施摄像头融合和 LiDAR-only 的高效定位，是自动驾驶和机器人领域持续关注的焦点。</li>
<li><strong>生成模型在复杂任务中的应用</strong>: 从视图拼接 ("Generative View Stitching") 到视频生成 ("Uniform Discrete Diffusion")，生成模型正被应用于越来越复杂的视觉任务，并展现出强大的能力。</li>
<li><strong>大型模型效率优化</strong>: "SCOPE" 提出的 Saliency-Coverage Oriented Token Pruning 技术，预示着未来对大型多模态模型进行高效剪枝和推理优化的需求将日益增长。</li>
</ol>
<p><strong>建议阅读全文的论文：</strong></p>
<p>为了全面了解当前领域的重要进展，建议优先阅读以下论文：</p>
<ol>
<li><strong>"Generative View Stitching"</strong>: 对于关注新颖视图合成、3D 内容生成和生成模型应用的读者。</li>
<li><strong>"Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal Reasoning in MLLMs"</strong>: 对于关注多模态大语言模型、人机交互和高级推理的读者。</li>
<li><strong>"Uniform Discrete Diffusion with Metric Path for Video Generation"</strong>: 对于关注视频生成、扩散模型和时间序列建模的读者。</li>
<li><strong>"MIC-BEV: Multi-Infrastructure Camera Bird's-Eye-View Transformer with Relation-Aware Fusion for 3D Object Detection"</strong>: 对于关注自动驾驶、3D 目标检测和多传感器融合的读者。</li>
<li><strong>"SCOPE: Saliency-Coverage Oriented Token Pruning for Efficient Multimodel LLMs"</strong>: 对于关注大型模型效率、模型压缩和优化推理的读者。</li>
</ol>
<hr />
<p>这份摘要旨在提供一个高层次的概览，帮助您快速识别与您研究兴趣最相关的论文。</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2510.24718v1">Generative View Stitching</a></li>
<li><a href="#2510.24717v1">Uniform Discrete Diffusion with Metric Path for Video Generation</a></li>
<li><a href="#2510.24688v1">MIC-BEV: Multi-Infrastructure Camera Bird's-Eye-View Transformer with Relation-Aware Fusion for 3D Object Detection</a></li>
<li><a href="#2510.24657v1">Group Relative Attention Guidance for Image Editing</a></li>
<li><a href="#2510.24623v1">GroundLoc: Efficient Large-Scale Outdoor LiDAR-Only Localization</a></li>
<li><a href="#2510.24514v1">Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal Reasoning in MLLMs</a></li>
<li><a href="#2510.24464v1">Kineo: Calibration-Free Metric Motion Capture From Sparse RGB Cameras</a></li>
<li><a href="#2510.24410v1">A Hybrid Approach for Visual Multi-Object Tracking</a></li>
<li><a href="#2510.24399v1">GenTrack: A New Generation of Multi-Object Tracking</a></li>
<li><a href="#2510.24214v1">SCOPE: Saliency-Coverage Oriented Token Pruning for Efficient Multimodel LLMs</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2510.24718v1'></a></p>
<h2 id="generative-view-stitching"><a href="https://arxiv.org/abs/2510.24718v1">Generative View Stitching</a></h2>
<p><strong>Authors:</strong> Chonghyuk Song, Michal Stary, Boyuan Chen, George Kopanas, Vincent Sitzmann</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Categories:</strong> cs.CV, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Autoregressive video diffusion models are capable of long rollouts that are
stable and consistent with history, but they are unable to guide the current
generation with conditioning from the future. In camera-guided video generation
with a predefined camera trajectory, this limitation leads to collisions with
the generated scene, after which autoregression quickly collapses. To address
this, we propose Generative View Stitching (GVS), which samples the entire
sequence in parallel such that the generated scene is faithful to every part of
the predefined camera trajectory. Our main contribution is a sampling algorithm
that extends prior work on diffusion stitching for robot planning to video
generation. While such stitching methods usually require a specially trained
model, GVS is compatible with any off-the-shelf video model trained with
Diffusion Forcing, a prevalent sequence diffusion framework that we show
already provides the affordances necessary for stitching. We then introduce
Omni Guidance, a technique that enhances the temporal consistency in stitching
by conditioning on both the past and future, and that enables our proposed
loop-closing mechanism for delivering long-range coherence. Overall, GVS
achieves camera-guided video generation that is stable, collision-free,
frame-to-frame consistent, and closes loops for a variety of predefined camera
paths, including Oscar Reutersv\"ard's Impossible Staircase. Results are best
viewed as videos at https://andrewsonga.github.io/gvs.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Chonghyuk Song等人撰写的论文“Generative View Stitching”的全面摘要。</p>
<hr />
<h3 id="generative-view-stitching_1">《Generative View Stitching》论文摘要</h3>
<p><strong>1. 主要问题或研究问题：</strong>
当前自回归视频扩散模型在生成长序列视频时，虽然能保持与历史的稳定性和一致性，但无法利用未来的条件信息来指导当前帧的生成。在预定义摄像机轨迹的摄像机引导视频生成任务中，这一限制导致生成的场景与摄像机轨迹发生碰撞，进而使自回归生成迅速崩溃。论文旨在解决这一问题，实现与预定义摄像机轨迹完全一致、无碰撞、帧间一致且能闭合循环的稳定长序列摄像机引导视频生成。</p>
<p><strong>2. 关键创新或方法论贡献：</strong></p>
<ul>
<li><strong>Generative View Stitching (GVS) 框架：</strong> 论文提出了GVS，一种训练无关的扩散拼接方法，能够并行采样整个视频序列，确保生成的场景与预定义的摄像机轨迹的每个部分都保持一致。</li>
<li><strong>与现有扩散模型的兼容性：</strong> GVS的一项核心贡献是，它兼容任何使用Diffusion Forcing（DF）框架训练的现成视频模型，而无需专门训练新的模型。论文指出，DF框架本身已提供了实现拼接所需的必要功能。</li>
<li><strong>Omni Guidance（全方位引导）：</strong> 为了增强拼接过程中的时间一致性，论文引入了Omni Guidance技术。该技术通过同时利用过去和未来的条件信息来指导生成，从而加强了时间连贯性。</li>
<li><strong>循环闭合机制：</strong> Omni Guidance进一步支持了论文提出的循环闭合机制，这对于实现长距离连贯性至关重要，尤其是在摄像机轨迹形成闭合循环的场景中。GVS通过“循环条件化”（Cyclic Conditioning）实现循环闭合，即在去噪过程中交替使用时间窗口（关注时间邻居）和空间窗口（关注时间遥远但空间接近的邻居）。</li>
</ul>
<p><strong>3. 主要结果及其重要性：</strong></p>
<ul>
<li><strong>稳定、无碰撞的视频生成：</strong> GVS成功实现了稳定的、无碰撞的摄像机引导视频生成，解决了自回归方法中常见的碰撞问题。</li>
<li><strong>帧间一致性和长距离连贯性：</strong> 该方法在各种预定义摄像机路径（包括奥斯卡·路透斯瓦德的“不可能的楼梯”）上，实现了帧间的高度一致性，并能有效地闭合循环，展现出卓越的长距离连贯性。</li>
<li><strong>优于基线方法：</strong> 在定量和定性评估中，GVS在时间一致性（F2FC）、长距离一致性（LRC）和碰撞避免（CA）方面均优于历史引导自回归采样（AR）和StochSync等基线方法，同时保持了可比的视频生成质量。</li>
<li><strong>Omni Guidance和随机性的互补作用：</strong> 实验表明，Omni Guidance在不同随机性水平下都能增强时间一致性，并提供了额外的灵活性来减少过度平滑，这在没有Omni Guidance的情况下，增加随机性虽然能提高一致性但常导致过度平滑。</li>
</ul>
<p><strong>4. 论文中提及的局限性：</strong></p>
<ul>
<li><strong>外部图像条件化的传播困难：</strong> GVS在将外部提供的上下文帧有效传播到目标视频的其余部分时存在困难。上下文帧的信息传播范围有限，导致视频中不同区域可能出现不连贯的场景。</li>
<li><strong>宽基线视角的循环闭合失败：</strong> GVS在处理宽基线视角（例如，摄像机轨迹包含180度轨道段）时，无法成功闭合循环。这被归因于所使用的Diffusion Forcing骨干模型是在视角变化较小的数据集（如RE10K）上训练的，导致其在处理分布外（out-of-distribution）的宽基线摄像机时表现不佳。</li>
<li><strong>结构相似摄像机轨迹段的区分困难：</strong> 在某些情况下，GVS难以区分结构相似的摄像机轨迹段，例如向上楼梯的起点和向下楼梯的终点。这源于骨干模型有限的上下文窗口和对相对姿态的依赖，可能导致歧义。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>控制信息传播：</strong> 探索通过调节Diffusion Forcing骨干模型的每帧噪声水平来控制拼接中的信息传播，以实现更好的外部图像条件化、视频质量和用户可控性。</li>
<li><strong>多视角数据集训练：</strong> 训练Diffusion Forcing骨干模型在具有更宽基线的多视角数据集（如DL3DV和ScanNet++）上，以解决宽基线视角循环闭合失败的问题。</li>
<li><strong>扩展条件化形式：</strong> 将GVS扩展到接受其他形式的条件化，如上下文图像和文本，以帮助解决结构相似轨迹段的歧义问题。</li>
<li><strong>新应用领域：</strong> 将GVS应用于其他领域，如目标导向的机器人规划和分层机器人规划。</li>
</ul>
<hr />
<p>总而言之，Chonghyuk Song等人的这篇论文提出了一种新颖且实用的Generative View Stitching (GVS) 方法，为长序列摄像机引导视频生成提供了一个强大的解决方案。通过其训练无关的特性、与现有DF模型的兼容性、Omni Guidance以及循环闭合机制，GVS在生成稳定、一致且无碰撞的视频方面取得了显著进展，并为未来在视频生成和机器人规划等领域的进一步研究奠定了基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address
this, we propose Generative View Stitching (GVS), which samples the entire
sequence in parallel such that the generated scene is faithful to every part of
the predefined camera trajectory.</li>
<li>Our main contribution is a sampling algorithm
that extends prior work on diffusion stitching for robot planning to video
generation.</li>
<li>While such stitching methods usually require a specially trained
model, GVS is compatible with any off-the-shelf video model trained with
Diffusion Forcing, a prevalent sequence diffusion framework that we show
already provides the affordances necessary for stitching.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.24718v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.24718v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.24717v1'></a></p>
<h2 id="uniform-discrete-diffusion-with-metric-path-for-video-generation"><a href="https://arxiv.org/abs/2510.24717v1">Uniform Discrete Diffusion with Metric Path for Video Generation</a></h2>
<p><strong>Authors:</strong> Haoge Deng, Ting Pan, Fan Zhang, Yang Liu, Zhuoyan Luo, Yufeng Cui, Wenxuan Wang, Chunhua Shen, Shiguang Shan, Zhaoxiang Zhang, Xinlong Wang</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Continuous-space video generation has advanced rapidly, while discrete
approaches lag behind due to error accumulation and long-context inconsistency.
In this work, we revisit discrete generative modeling and present Uniform
discRete diffuSion with metric pAth (URSA), a simple yet powerful framework
that bridges the gap with continuous approaches for the scalable video
generation. At its core, URSA formulates the video generation task as an
iterative global refinement of discrete spatiotemporal tokens. It integrates
two key designs: a Linearized Metric Path and a Resolution-dependent Timestep
Shifting mechanism. These designs enable URSA to scale efficiently to
high-resolution image synthesis and long-duration video generation, while
requiring significantly fewer inference steps. Additionally, we introduce an
asynchronous temporal fine-tuning strategy that unifies versatile tasks within
a single model, including interpolation and image-to-video generation.
Extensive experiments on challenging video and image generation benchmarks
demonstrate that URSA consistently outperforms existing discrete methods and
achieves performance comparable to state-of-the-art continuous diffusion
methods. Code and models are available at https://github.com/baaivision/URSA</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Haoge Deng等人撰写的论文“Uniform Discrete Diffusion with Metric Path for Video Generation”的全面摘要。</p>
<hr />
<h3 id="uniform-discrete-diffusion-with-metric-path-for-video-generation_1">论文摘要：Uniform Discrete Diffusion with Metric Path for Video Generation</h3>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决离散空间视频生成方法中存在的两大挑战：误差累积和长上下文不一致性，这导致其性能落后于连续空间视频生成方法。具体来说，传统的离散方法（如自回归模型和掩码扩散模型）采用不可再优化的局部生成，一旦生成令牌便固定下来，限制了其生成高质量、长时序视频的能力。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
作者提出了 <strong>Uniform discRete diffuSion with metric pAth (URSA)</strong>，一个简单而强大的框架，通过以下关键创新弥合了离散与连续方法之间的差距：</p>
<ul>
<li><strong>迭代全局离散时空令牌细化：</strong> URSA将视频生成任务重新定义为离散时空令牌的迭代全局细化过程。这使得离散方法能够概念上与连续方法对齐，从而显著缩小性能差距。</li>
<li><strong>线性化度量路径 (Linearized Metric Path)：</strong> 引入了一种基于令牌嵌入距离的新型概率路径，能够对数据扰动进行精确控制，这对于有效学习分层数据流形至关重要。</li>
<li><strong>分辨率依赖的时间步长偏移机制 (Resolution-dependent Timestep Shifting)：</strong> 该机制根据视频分辨率调整时间步长，确保扰动过程能根据序列长度（如高分辨率图像或长视频）进行适当调整，从而提高训练稳定性和长视频序列的表示学习。</li>
<li><strong>异步时间步长调度策略 (Asynchronous Temporal Fine-tuning Strategy)：</strong> 针对多任务训练和采样，该策略允许每个帧独立采样时间步长。这使得URSA能够在一个统一模型中处理多种任务，包括视频插值、图像到视频生成、视频外推以及起始-结束帧控制，并能生成分钟级长视频。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>性能超越现有离散方法：</strong> 在具有挑战性的视频和图像生成基准测试中，URSA始终优于现有的离散方法。
*   <strong>与最先进连续扩散方法媲美：</strong> URSA在性能上达到了与最先进的连续扩散方法相当的水平，尤其是在文本到视频生成任务中，其VBench得分达到82.4，在图像到视频生成任务中达到86.2，在文本到图像生成任务中DPG-Bench得分达到86.0。
*   <strong>高效且可扩展：</strong> URSA能够高效地扩展到高分辨率图像合成和长时程视频生成，同时显著减少推理步骤。
*   <strong>强大的零样本泛化能力：</strong> URSA在可变长度上下文下表现出强大的零样本泛化能力，凸显了其多功能性。</p>
<p>这些结果表明，URSA在离散视频生成领域取得了重大突破，不仅提升了离散方法的性能上限，还证明了其在复杂视频生成任务中的实用性和效率，为可扩展、多功能和高效的视频生成开辟了新方向。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>离散视觉tokenizer的表示能力：</strong> 论文提到，尽管增加模型尺寸可以显著提高语义性能，但生成输出的保真度可能最终受限于离散视觉tokenizer的表示能力。这意味着，即使模型再大，如果底层令牌化器的能力有限，生成质量也可能达到瓶颈。
*   <strong>离散扩散模型的固有采样误差：</strong> 论文指出，离散扩散模型固有地存在较高的采样误差，这在图像和视频生成中需要系统性地解决。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>改进离散视觉tokenizer：</strong> 鉴于当前离散tokenizer的表示能力可能限制生成质量，未来的研究可以专注于开发更强大、更精细的离散视觉tokenizer，以进一步提升生成内容的保真度。
*   <strong>优化采样策略以减少误差：</strong> 尽管URSA通过迭代细化减少了采样误差，但离散扩散模型固有的误差累积问题仍有进一步优化的空间，例如探索更先进的采样算法或误差校正机制。
*   <strong>探索更复杂的度量路径和时间步长调度：</strong> 论文提出的线性化度量路径和分辨率依赖的时间步长偏移机制已经取得了显著效果，未来可以探索更复杂、自适应的度量路径和调度策略，以更好地适应不同数据特性和任务需求。
*   <strong>扩展到更多多模态任务：</strong> 异步时间步长调度策略使得URSA能够处理多种任务，未来可以进一步探索其在更广泛的多模态生成任务中的应用，例如3D视频生成、交互式内容生成等。
*   <strong>结合其他生成范式：</strong> 尽管URSA旨在弥合离散与连续方法之间的差距，但仍可以探索与其他生成范式（如GANs、VAE）的结合，以期实现更强大的生成能力。</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>Additionally, we introduce an
asynchronous temporal fine-tuning strategy that unifies versatile tasks within
a single model, including interpolation and image-to-video generation.</li>
<li>Extensive experiments on challenging video and image generation benchmarks
demonstrate that URSA consistently outperforms existing discrete methods and
achieves performance comparable to state-of-the-art continuous diffusion
methods.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.24717v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.24717v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.24688v1'></a></p>
<h2 id="mic-bev-multi-infrastructure-camera-birds-eye-view-transformer-with-relation-aware-fusion-for-3d-object-detection"><a href="https://arxiv.org/abs/2510.24688v1">MIC-BEV: Multi-Infrastructure Camera Bird's-Eye-View Transformer with Relation-Aware Fusion for 3D Object Detection</a></h2>
<p><strong>Authors:</strong> Yun Zhang, Zhaoliang Zheng, Johnson Liu, Zhiyu Huang, Zewei Zhou, Zonglin Meng, Tianhui Cai, Jiaqi Ma</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Infrastructure-based perception plays a crucial role in intelligent
transportation systems, offering global situational awareness and enabling
cooperative autonomy. However, existing camera-based detection models often
underperform in such scenarios due to challenges such as multi-view
infrastructure setup, diverse camera configurations, degraded visual inputs,
and various road layouts. We introduce MIC-BEV, a Transformer-based
bird's-eye-view (BEV) perception framework for infrastructure-based
multi-camera 3D object detection. MIC-BEV flexibly supports a variable number
of cameras with heterogeneous intrinsic and extrinsic parameters and
demonstrates strong robustness under sensor degradation. The proposed
graph-enhanced fusion module in MIC-BEV integrates multi-view image features
into the BEV space by exploiting geometric relationships between cameras and
BEV cells alongside latent visual cues. To support training and evaluation, we
introduce M2I, a synthetic dataset for infrastructure-based object detection,
featuring diverse camera configurations, road layouts, and environmental
conditions. Extensive experiments on both M2I and the real-world dataset
RoScenes demonstrate that MIC-BEV achieves state-of-the-art performance in 3D
object detection. It also remains robust under challenging conditions,
including extreme weather and sensor degradation. These results highlight the
potential of MIC-BEV for real-world deployment. The dataset and source code are
available at: https://github.com/HandsomeYun/MIC-BEV.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于“MIC-BEV: Multi-Infrastructure Camera Bird's-Eye-View Transformer with Relation-Aware Fusion for 3D Object Detection”论文的摘要，内容涵盖了您要求的所有方面：</p>
<p><strong>论文摘要：MIC-BEV：多基础设施相机鸟瞰图Transformer与关系感知融合用于3D目标检测</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决基础设施感知系统在智能交通中面临的关键挑战。现有基于摄像头的3D目标检测模型在多视角基础设施设置、多样化的摄像头配置、退化的视觉输入以及各种道路布局等复杂场景下表现不佳。具体来说，挑战包括：1) 空间分布传感器导致视角差异大、遮挡多，难以进行空间对齐和特征融合；2) 基础设施摄像头配置（数量、布局、朝向、视场角）高度异构，模型难以适应；3) 传感器可靠性和鲁棒性不足，模型需应对缺失、损坏或低质量的视觉输入。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
*   <strong>MIC-BEV框架：</strong> 提出了一种基于Transformer的鸟瞰图（BEV）感知框架，用于基础设施多摄像头3D目标检测。该框架灵活支持可变数量的摄像头，具有异构的内外参，并在传感器退化下表现出强大的鲁棒性。
*   <strong>关系增强空间交叉注意力（ReSCA）：</strong> 引入了一种新颖的图增强融合模块，通过图神经网络（GNN）利用摄像头与BEV单元之间的几何关系以及潜在视觉线索，将多视角图像特征融合到BEV空间中。这使得模型能够自适应地加权来自不同摄像头的特征，并提高多视角特征聚合的质量。
*   <strong>双层BEV分割头：</strong> 结合地图级和物体级BEV分割任务，以增强空间理解和定位精度。
*   <strong>鲁棒性增强策略：</strong> 采用摄像头遮罩策略（如随机丢弃和高斯模糊）进行训练，以模拟传感器退化和遮挡，提高模型在挑战性条件下的鲁棒性。
*   <strong>M2I合成数据集：</strong> 为了克服真实世界数据集中基础设施配置、天气条件和摄像头布局多样性不足的问题，引入了一个大规模合成数据集M2I。该数据集涵盖了广泛的交叉口类型、摄像头配置和环境条件，为模型训练和评估提供了全面的基准。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>最先进的性能：</strong> 在M2I合成数据集和真实世界RoScenes数据集上进行的广泛实验表明，MIC-BEV在3D目标检测方面取得了最先进的性能。
*   <strong>强大的鲁棒性：</strong> MIC-BEV在挑战性条件下（包括极端天气和传感器退化）仍保持鲁棒性，显著优于现有基线方法。例如，在M2I的“Normal”条件下，MIC-BEV的mAP达到0.767，比最强基线DETR3D高出9.4%。在“Robust”和“Extreme Weather”条件下，性能下降幅度也小于基线模型。
*   <strong>关系感知融合的有效性：</strong> 定性分析和消融研究证实，MIC-BEV的摄像头-网格关系增强注意力机制能有效聚合跨视角信息，并在挑战性可见性、大空间范围和异构摄像头部署下保持几何一致性。GNN能够学习视图依赖的重要性，抑制失真或遮挡的观测，并强调几何可靠和语义信息丰富的视图。
*   <strong>实际部署潜力：</strong> 这些结果突出了MIC-BEV在真实世界部署中的巨大潜力，能够平衡检测精度和计算效率。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>现有数据集的局限性：</strong> 论文指出，现有基础设施感知数据集未能充分反映真实世界基础设施感知的复杂性，主要体现在：摄像头配置有限（通常是固定或单视角）、场景多样性有限（通常局限于单一交叉口或高速公路）、以及动态条件有限（主要在晴朗白天收集数据，缺乏极端天气和光照变化）。M2I数据集的引入正是为了弥补这些局限。
*   <strong>未来研究方向：</strong> 论文提到，过度高的遮罩率（pm ≥ 0.5）会导致M2I-Normal数据集性能下降，表明过度频繁的视图移除会限制模型充分利用多视图冗余的能力。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>多目标跟踪和轨迹预测：</strong> 将MIC-BEV扩展到多目标跟踪和轨迹预测，以捕捉道路使用者之间的动态交互。
*   <strong>实时部署：</strong> 通过轻量级骨干网络和边缘设备的知识蒸馏，探索实时部署。
*   <strong>弥合仿真与现实差距：</strong> 扩展M2I基准，增加额外的真实世界数据，以弥合仿真与现实之间的差距，并实现对多样化城市环境的全面评估。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce MIC-BEV, a Transformer-based
bird's-eye-view (BEV) perception framework for infrastructure-based
multi-camera 3D object detection.</li>
<li>Extensive experiments on both M2I and the real-world dataset
RoScenes demonstrate that MIC-BEV achieves state-of-the-art performance in 3D
object detection.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.24688v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.24688v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.24657v1'></a></p>
<h2 id="group-relative-attention-guidance-for-image-editing"><a href="https://arxiv.org/abs/2510.24657v1">Group Relative Attention Guidance for Image Editing</a></h2>
<p><strong>Authors:</strong> Xuanpu Zhang, Xuesong Niu, Ruidong Chen, Dan Song, Jianhao Zeng, Penghui Du, Haoxiang Cao, Kai Wu, An-an Liu</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Recently, image editing based on Diffusion-in-Transformer models has
undergone rapid development. However, existing editing methods often lack
effective control over the degree of editing, limiting their ability to achieve
more customized results. To address this limitation, we investigate the
MM-Attention mechanism within the DiT model and observe that the Query and Key
tokens share a bias vector that is only layer-dependent. We interpret this bias
as representing the model's inherent editing behavior, while the delta between
each token and its corresponding bias encodes the content-specific editing
signals. Based on this insight, we propose Group Relative Attention Guidance, a
simple yet effective method that reweights the delta values of different tokens
to modulate the focus of the model on the input image relative to the editing
instruction, enabling continuous and fine-grained control over editing
intensity without any tuning. Extensive experiments conducted on existing image
editing frameworks demonstrate that GRAG can be integrated with as few as four
lines of code, consistently enhancing editing quality. Moreover, compared to
the commonly used Classifier-Free Guidance, GRAG achieves smoother and more
precise control over the degree of editing. Our code will be released at
https://github.com/little-misfit/GRAG-Image-Editing.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将对这篇论文摘要进行深入分析。</p>
<hr />
<p><strong>论文摘要分析：Group Relative Attention Guidance for Image Editing</strong></p>
<p><strong>1. 论文主要贡献的简洁总结 (2-3 句话)</strong></p>
<p>这篇论文提出了一种名为“Group Relative Attention Guidance (GRAG)”的新方法，用于在基于Diffusion-in-Transformer (DiT) 的图像编辑模型中实现对编辑程度的精细化、连续性控制。通过重新加权注意力机制中内容相关的“delta”值，GRAG能够调节模型对输入图像和编辑指令的关注焦点，从而在不进行额外训练的情况下显著提升编辑质量并提供更平滑、更精确的编辑强度控制。</p>
<p><strong>2. 关键创新或方法学方法</strong></p>
<ul>
<li><strong>对MM-Attention机制的深度洞察：</strong> 论文的核心创新在于对DiT模型中MM-Attention机制的独特解读。作者观察到Query和Key token共享一个仅依赖于层的偏置向量。他们将这个偏置解释为模型固有的编辑行为（即模型默认的编辑倾向），而每个token与其对应偏置之间的“delta”则编码了内容特定的编辑信号。</li>
<li><strong>Group Relative Attention Guidance (GRAG)：</strong> 基于上述洞察，GRAG方法通过重新加权这些内容相关的“delta”值来实现对编辑强度的控制。这意味着它不是直接修改注意力权重，而是调整模型对特定内容特征的关注程度，使其更倾向于遵循编辑指令或保留原始图像特征。</li>
<li><strong>无训练（tuning-free）的特性：</strong> GRAG的一个显著优势是它不需要任何额外的训练或微调。这使得它非常容易集成到现有框架中，并且具有很高的实用性。</li>
<li><strong>与Classifier-Free Guidance (CFG) 的对比：</strong> 论文明确指出GRAG在编辑程度控制上比常用的Classifier-Free Guidance (CFG) 实现了更平滑和更精确的效果，这表明它在用户体验和编辑质量方面具有潜在优势。</li>
</ul>
<p><strong>3. 对领域潜在影响</strong></p>
<ul>
<li><strong>提升用户控制力：</strong> GRAG直接解决了现有图像编辑方法在编辑程度控制上的不足，为用户提供了前所未有的精细化和连续性控制，这将极大地提升图像编辑工具的用户体验和实用性。</li>
<li><strong>加速研究与应用：</strong> 其无训练和易于集成的特性（仅需四行代码）意味着研究人员和开发者可以迅速将其应用于各种DiT基的图像编辑任务中，加速新应用和新方法的开发。</li>
<li><strong>启发新的注意力机制研究：</strong> 对MM-Attention机制中偏置和delta的独特解释，可能会启发未来对Transformer注意力机制更深层次的理解和改进，尤其是在条件生成和编辑任务中。</li>
<li><strong>超越CFG的潜在替代方案：</strong> 如果GRAG在广泛场景下表现出优于CFG的控制能力，它可能成为条件生成模型中引导机制的一个重要补充甚至替代方案。</li>
</ul>
<p><strong>4. 相关领域或应用受益</strong></p>
<ul>
<li><strong>文本到图像生成（Text-to-Image Generation）：</strong> 尤其是在需要对生成图像的特定属性进行微调时，例如改变物体的强度、风格的程度等。</li>
<li><strong>图像修复/补全（Inpainting/Outpainting）：</strong> 控制修复区域与周围环境的融合程度，或生成内容的强度。</li>
<li><strong>图像风格迁移（Image Style Transfer）：</strong> 精细控制风格迁移的强度，从轻微的风格化到完全的风格转换。</li>
<li><strong>图像属性编辑（Image Attribute Editing）：</strong> 例如，改变人物的表情强度、发色深浅、服装纹理的明显程度等。</li>
<li><strong>交互式图像编辑工具：</strong> 任何需要用户通过滑块或其他方式实时调整编辑效果的应用程序都将受益匪浅。</li>
<li><strong>多模态内容生成：</strong> 不仅限于图像，未来可能扩展到视频编辑或其他多模态生成任务中，只要其底层使用Transformer-based的扩散模型。</li>
</ul>
<p><strong>5. 从摘要中可推断出的局限性</strong></p>
<ul>
<li><strong>仅限于DiT模型：</strong> 摘要明确指出其研究对象是“Diffusion-in-Transformer models”中的“MM-Attention机制”。这意味着GRAG的直接适用性可能局限于使用DiT架构的扩散模型，对于其他类型的扩散模型（如U-Net based）可能需要进一步的适配或研究。</li>
<li><strong>“编辑质量”的定义：</strong> 摘要提到“consistently enhancing editing quality”，但没有具体说明“编辑质量”的衡量标准。这可能包括视觉真实感、与指令的一致性、无伪影等，但具体侧重哪些方面需要通过论文正文的实验部分来验证。</li>
<li><strong>“连续和精细控制”的粒度：</strong> 尽管摘要声称实现了“continuous and fine-grained control”，但实际的控制粒度（例如，有多少个可区分的编辑强度级别）以及用户体验上的平滑度，仍需通过实际演示和用户研究来评估。</li>
<li><strong>复杂编辑场景的泛化能力：</strong> 摘要未提及GRAG在处理高度复杂、多对象、多属性同时编辑的场景下的表现。在这些情况下，简单的delta重加权是否能有效解耦和控制所有编辑维度，仍是一个开放问题。</li>
<li><strong>计算开销：</strong> 尽管声称“as few as four lines of code”，但重新加权delta值是否会引入额外的计算开销（即使很小），以及这是否会影响实时编辑的性能，摘要中没有提及。</li>
</ul>
<hr />
<p>总的来说，这篇论文提出了一种优雅且高效的方法来解决扩散模型图像编辑中一个关键的用户控制问题。其对注意力机制的独特解读和无训练的特性使其具有很高的实用价值和潜在影响力，有望推动图像编辑领域向更精细化、用户友好的方向发展。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Based on this insight, we propose Group Relative Attention Guidance, a
simple yet effective method that reweights the delta values of different tokens
to modulate the focus of the model on the input image relative to the editing
instruction, enabling continuous and fine-grained control over editing
intensity without any tuning.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.24657v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.24657v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.24623v1'></a></p>
<h2 id="groundloc-efficient-large-scale-outdoor-lidar-only-localization"><a href="https://arxiv.org/abs/2510.24623v1">GroundLoc: Efficient Large-Scale Outdoor LiDAR-Only Localization</a></h2>
<p><strong>Authors:</strong> Nicolai Steinke, Daniel Goehring</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Categories:</strong> cs.RO, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>In this letter, we introduce GroundLoc, a LiDAR-only localization pipeline
designed to localize a mobile robot in large-scale outdoor environments using
prior maps. GroundLoc employs a Bird's-Eye View (BEV) image projection focusing
on the perceived ground area and utilizes the place recognition network R2D2,
or alternatively, the non-learning approach Scale-Invariant Feature Transform
(SIFT), to identify and select keypoints for BEV image map registration. Our
results demonstrate that GroundLoc outperforms state-of-the-art methods on the
SemanticKITTI and HeLiPR datasets across various sensors. In the multi-session
localization evaluation, GroundLoc reaches an Average Trajectory Error (ATE)
well below 50 cm on all Ouster OS2 128 sequences while meeting online runtime
requirements. The system supports various sensor models, as evidenced by
evaluations conducted with Velodyne HDL-64E, Ouster OS2 128, Aeva Aeries II,
and Livox Avia sensors. The prior maps are stored as 2D raster image maps,
which can be created from a single drive and require only 4 MB of storage per
square kilometer. The source code is available at
https://github.com/dcmlr/groundloc.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Nicolai Steinke和Daniel Goehring撰写的论文“GroundLoc: Efficient Large-Scale Outdoor LiDAR-Only Localization”的全面摘要。</p>
<hr />
<h3 id="groundloc">论文摘要：GroundLoc: 高效大规模室外激光雷达纯定位</h3>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决移动机器人在大规模室外环境中进行精确、高效且仅依赖激光雷达的自定位问题。传统方法在存储需求、对动态物体的鲁棒性以及在缺乏稳定垂直特征的重复或非独特场景中的性能方面存在挑战。具体来说，作者关注如何利用先验地图实现低存储成本、高实时性能，并支持多种传感器模型。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
GroundLoc 提出了一个新颖的激光雷达纯定位管道，其主要创新点包括：
*   <strong>三通道鸟瞰图 (BEV) 图像投影：</strong> 系统将点云分割为地面和非地面点，并将其投影到 BEV 图像中。这些 BEV 图像包含三个通道：强度、地面坡度以及 Z 轴高度方差。这种多通道表示增强了对动态物体的鲁棒性，并保留了地形粗糙度和静态垂直结构的信息，解决了仅使用强度通道在某些场景中定位不鲁棒的问题。
*   <strong>特征提取与匹配：</strong> GroundLoc 利用深度学习的 R2D2 网络（或非学习的 SIFT 方法）从生成的 BEV 图像和参考地图中提取关键点和描述符。R2D2 在多会话定位中表现出更优异的匹配能力，尤其是在稀疏传感器数据上。
*   <strong>高效的地图表示：</strong> 先验地图以 2D 栅格图像地图的形式存储，平均每平方公里仅需 4 MB 的存储空间。这些地图可以从单次驾驶生成，并利用 ZSTD 压缩和 GeoTIFF 格式的内置功能。
*   <strong>鲁棒的位姿估计：</strong> 采用近似最近邻 KD-Tree 进行描述符匹配，并通过动态搜索半径进行位置距离过滤，以消除不合理的对应关系。最终的位姿估计使用 Quatro 估计器计算，该估计器对异常值具有更强的抵抗力。
*   <strong>位姿校正机制：</strong> 引入了基于内点数量和当前速度的位姿校正机制，以缓解量化误差和高速行驶时的漂移累积。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>卓越的定位精度：</strong> 在 SemanticKITTI 和 HeLiPR 数据集上，GroundLoc 在多会话定位评估中，所有 Ouster OS2 128 序列的平均轨迹误差 (ATE) 均远低于 50 厘米，显著优于现有最先进的方法（如 KISS-ICP、KISS-SLAM 和基于指纹的定位方法）。
*   <strong>实时性能：</strong> 系统在所有实验中均实现了超过 14 Hz 的处理速率，满足了在线运行时间要求。
*   <strong>低存储需求：</strong> 先验地图的存储需求极低，平均每平方公里仅需 4.09 MB，远低于指纹定位方法（33.75 MB/km²）和下采样点云地图（15.32 MB/km²）。
*   <strong>多传感器支持：</strong> 论文通过对 Velodyne HDL-64E、Ouster OS2 128、Aeva Aeries II 和 Livox Avia 传感器进行评估，证明了系统对多种传感器模型的支持能力。
*   <strong>泛化能力：</strong> 模型在不同地点之间表现出鲁棒的泛化能力，但在不同传感器模型之间（特别是强度响应和扫描模式差异显著时）性能差异会增大。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>3 自由度 (3-DOF) 限制：</strong> GroundLoc 是一种 3-DOF 定位方法，无法纠正俯仰、横滚和 Z 轴高度的误差。这使得它在与缺乏 360° 视场角的传感器（如 Aeva Aeries II 和 Livox Avia）结合使用时，可能会遇到显著的俯仰漂移。
*   <strong>地面真值数据一致性：</strong> 在多会话定位中，一些结果（特别是使用 Ouster 传感器时）的 ATE 集中在 0.3-0.4 米左右，作者认为这可能归因于地面真值数据在多会话一致性方面的局限性。
*   <strong>传感器模型泛化：</strong> 尽管模型在不同地点之间泛化良好，但在不同传感器模型之间（尤其是强度响应和扫描模式差异显著时）性能差异会增大。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
论文中没有明确提出未来的研究方向，但从其局限性和贡献中可以推断出一些潜在方向：
*   <strong>扩展到 6 自由度 (6-DOF) 定位：</strong> 结合 IMU 或其他传感器信息，或者开发新的 BEV 表示和特征提取方法，以实现更全面的 6-DOF 定位，从而解决俯仰、横滚和 Z 轴高度的漂移问题。
*   <strong>增强传感器模型间的泛化能力：</strong> 进一步研究如何提高模型在不同激光雷达传感器（特别是扫描模式和强度响应差异大时）之间的泛化能力，可能通过更先进的域适应技术或多模态融合方法。
*   <strong>动态环境下的鲁棒性：</strong> 尽管 GroundLoc 通过地面分割提高了对动态物体的鲁棒性，但进一步探索在高度动态或复杂场景（例如交通繁忙的城市中心）中保持定位精度的策略。
*   <strong>地图更新和维护：</strong> 研究如何高效地更新和维护大规模先验地图，以适应环境变化（如施工、植被变化等），同时保持低存储和实时性能。
*   <strong>结合其他感知任务：</strong> 将定位结果与感知、规划等其他自动驾驶任务更紧密地结合，构建更全面的自主系统。</p>
<hr />
<p>总而言之，GroundLoc 论文提出了一种高效、低存储、高精度的激光雷达纯定位解决方案，通过创新的三通道 BEV 图像表示和先进的特征提取方法，在多种传感器和复杂环境中取得了显著成果，为大规模移动机器人定位领域做出了重要贡献。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this letter, we introduce GroundLoc, a LiDAR-only localization pipeline
designed to localize a mobile robot in large-scale outdoor environments using
prior maps.</li>
<li>Our
results demonstrate that GroundLoc outperforms state-of-the-art methods on the
SemanticKITTI and HeLiPR datasets across various sensors.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.24623v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.24623v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.24514v1'></a></p>
<h2 id="latent-sketchpad-sketching-visual-thoughts-to-elicit-multimodal-reasoning-in-mllms"><a href="https://arxiv.org/abs/2510.24514v1">Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal Reasoning in MLLMs</a></h2>
<p><strong>Authors:</strong> Huanyu Zhang, Wenshan Wu, Chengzu Li, Ning Shang, Yan Xia, Yangyu Huang, Yifan Zhang, Li Dong, Zhang Zhang, Liang Wang, Tieniu Tan, Furu Wei</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Categories:</strong> cs.CV, cs.CL</p>
<p><strong>Abstract:</strong></p>
<p>While Multimodal Large Language Models (MLLMs) excel at visual understanding,
they often struggle in complex scenarios that require visual planning and
imagination. Inspired by how humans use sketching as a form of visual thinking
to develop and communicate ideas, we introduce Latent Sketchpad, a framework
that equips MLLMs with an internal visual scratchpad. The internal visual
representations of MLLMs have traditionally been confined to perceptual
understanding. We repurpose them to support generative visual thought without
compromising reasoning ability. Building on frontier MLLMs, our approach
integrates visual generation directly into their native autoregressive
reasoning process. It allows the model to interleave textual reasoning with the
generation of visual latents. These latents guide the internal thought process
and can be translated into sketch images for interpretability. To realize this,
we introduce two components: a Context-Aware Vision Head autoregressively
produces visual representations, and a pretrained Sketch Decoder renders these
into human-interpretable images. We evaluate the framework on our new dataset
MazePlanning. Experiments across various MLLMs show that Latent Sketchpad
delivers comparable or even superior reasoning performance to their backbone.
It further generalizes across distinct frontier MLLMs, including Gemma3 and
Qwen2.5-VL. By extending model's textual reasoning to visual thinking, our
framework opens new opportunities for richer human-computer interaction and
broader applications. More details and resources are available on our project
page: https://latent-sketchpad.github.io/.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Huanyu Zhang等人的论文“Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal Reasoning in MLLMs”的全面摘要。</p>
<hr />
<h3 id="latent-sketchpad-mllms">《Latent Sketchpad: 绘制视觉思维以激发多模态推理的MLLMs》论文摘要</h3>
<p><strong>1. 主要问题或研究问题：</strong>
尽管多模态大型语言模型（MLLMs）在视觉理解方面表现出色，但在需要视觉规划和想象力的复杂场景中，它们往往力不从心。传统上，MLLMs的内部视觉表示仅限于感知理解。该研究旨在解决如何使MLLMs能够像人类一样，利用内部视觉“草图”作为视觉思维形式，以支持生成性的视觉思考，从而增强其多模态推理能力，尤其是在需要精确空间推理和动态视觉接地的复杂任务中。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
为了解决上述问题，论文引入了<strong>Latent Sketchpad</strong>框架，其核心创新包括：</p>
<ul>
<li><strong>内部视觉草图（Internal Visual Scratchpad）：</strong> Latent Sketchpad使MLLMs能够在其推理过程中生成连续的视觉潜在表示（visual latents），这些潜在表示在推理过程中保持在潜在空间中，而不是立即解码为图像。这使得模型能够将文本推理与视觉潜在表示的生成交织在一起，从而实现更丰富的多模态推理。</li>
<li><strong>上下文感知视觉头（Context-Aware Vision Head）：</strong> 这是一个集成到MLLM骨干网络中的组件，负责在每个推理步骤中自回归地生成视觉潜在表示。它不仅基于当前的隐藏状态，还基于先前的视觉表示进行条件化，从而确保视觉连贯性，并根据图像内和图像间的上下文线索细化内部视觉表示。</li>
<li><strong>预训练草图解码器（Pretrained Sketch Decoder）：</strong> 这是一个独立的模块，用于将生成的视觉潜在表示渲染成人类可解释的草图图像。它通过对齐预训练视觉编码器的特征空间与预训练VAE的潜在空间，将视觉潜在表示转化为草图风格的图像，从而实现模型内部视觉思维过程的可解释性。</li>
<li><strong>MAZEPLANNING数据集：</strong> 为了评估框架的有效性，研究构建了一个新的MAZEPLANNING数据集，该数据集包含复杂的、交织的多模态推理轨迹，用于训练和评估模型在视觉规划和导航任务中的表现。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong>
实验结果在多个MLLMs（包括Gemma3和Qwen2.5-VL）上进行，并展示了以下主要发现：</p>
<ul>
<li><strong>推理性能提升：</strong> Latent Sketchpad在MAZEPLANNING数据集上的推理性能与其骨干模型相比，达到了甚至超越的水平。这表明通过整合视觉思维，模型在复杂多模态推理任务中的表现得到了增强。</li>
<li><strong>广泛适用性和即插即用能力：</strong> 该框架能够泛化到不同的前沿MLLMs，如Gemma3和Qwen2.5-VL，证明了其模块化架构的优势。视觉头可以独立训练并附加到MLLMs上，而无需修改其参数，从而保留了骨干模型的原始推理能力，同时无缝增强了视觉生成能力。</li>
<li><strong>可解释的视觉轨迹：</strong> 生成的视觉潜在表示可以被解码为可解释的草图，为模型的内部视觉思维过程提供了透明的洞察力，增强了人机交互和模型的可信度。</li>
<li><strong>空间一致性与鲁棒性：</strong> Latent Sketchpad在生成图像时保持了较高的布局一致性率（LCR）和视觉成功率（VSR），表明其在推理步骤中能够保持空间结构，并支持通过视觉生成进行推理。</li>
</ul>
<p><strong>4. 论文中提及的局限性：</strong>
论文中也讨论了一些局限性：</p>
<ul>
<li><strong>OOD泛化能力：</strong> 在分布外（OOD）数据集（更大的、未见过的迷宫）上，模型的性能显著下降。特别是Qwen2.5-VL由于其视觉编码器产生比Gemma3大四倍的特征，有限的微调数据不足以确保泛化，导致其未能可靠地保留迷宫布局。</li>
<li><strong>视觉质量：</strong> 尽管生成的草图在结构上稳定，但在感知质量上（如箭头或数字）可能显得较低。虽然这足以支持多模态推理，但对于需要更高精度感知细节的任务，可能需要进一步改进。</li>
<li><strong>结构性违规：</strong> 在某些情况下，模型生成的路径可能穿过迷宫墙壁或突然瞬移到远处位置，导致最终计划不正确，即使单个动作在局部上连贯。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong>
基于上述发现和局限性，论文为未来的研究指明了方向：</p>
<ul>
<li><strong>增强空间一致性和对分布变化的鲁棒性：</strong> 解决模型在复杂或新颖环境中出现的结构性违规和累积性退化问题，以提高空间一致性和鲁棒性。</li>
<li><strong>提升视觉保真度：</strong> 进一步提高生成草图的视觉保真度，以扩展其在需要更精细感知精度的任务中的适用性。</li>
<li><strong>探索更丰富的MLLM内部视觉表示：</strong> 进一步研究如何利用MLLMs的内部视觉表示，以实现更复杂、更具创造性的视觉思维和规划。</li>
<li><strong>优化连接器适应性：</strong> 进一步研究连接器适应性在下游任务微调中的关键作用，以确保视觉表示在训练过程中得到有效更新。</li>
</ul>
<hr />
<p>总而言之，Latent Sketchpad框架通过为MLLMs提供内部视觉草图，成功地将文本推理与视觉思维相结合，显著提升了模型在复杂多模态推理任务中的表现。这一创新不仅增强了推理能力，还通过可解释的草图提供了透明的视觉轨迹，为未来更丰富的人机交互和更广泛的应用开辟了新途径。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Inspired by how humans use sketching as a form of visual thinking
to develop and communicate ideas, we introduce Latent Sketchpad, a framework
that equips MLLMs with an internal visual scratchpad.</li>
<li>Building on frontier MLLMs, our approach
integrates visual generation directly into their native autoregressive
reasoning process.</li>
<li>To realize this,
we introduce two components: a Context-Aware Vision Head autoregressively
produces visual representations, and a pretrained Sketch Decoder renders these
into human-interpretable images.</li>
<li>We evaluate the framework on our new dataset
MazePlanning.</li>
<li>By extending model's textual reasoning to visual thinking, our
framework opens new opportunities for richer human-computer interaction and
broader applications.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.24514v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.24514v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.24464v1'></a></p>
<h2 id="kineo-calibration-free-metric-motion-capture-from-sparse-rgb-cameras"><a href="https://arxiv.org/abs/2510.24464v1">Kineo: Calibration-Free Metric Motion Capture From Sparse RGB Cameras</a></h2>
<p><strong>Authors:</strong> Charles Javerliat, Pierre Raimbaud, Guillaume Lavoué</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Markerless multiview motion capture is often constrained by the need for
precise camera calibration, limiting accessibility for non-experts and
in-the-wild captures. Existing calibration-free approaches mitigate this
requirement but suffer from high computational cost and reduced reconstruction
accuracy.
  We present Kineo, a fully automatic, calibration-free pipeline for markerless
motion capture from videos captured by unsynchronized, uncalibrated,
consumer-grade RGB cameras. Kineo leverages 2D keypoints from off-the-shelf
detectors to simultaneously calibrate cameras, including Brown-Conrady
distortion coefficients, and reconstruct 3D keypoints and dense scene point
maps at metric scale. A confidence-driven spatio-temporal keypoint sampling
strategy, combined with graph-based global optimization, ensures robust
calibration at a fixed computational cost independent of sequence length. We
further introduce a pairwise reprojection consensus score to quantify 3D
reconstruction reliability for downstream tasks.
  Evaluations on EgoHumans and Human3.6M demonstrate substantial improvements
over prior calibration-free methods. Compared to previous state-of-the-art
approaches, Kineo reduces camera translation error by approximately 83-85%,
camera angular error by 86-92%, and world mean-per-joint error (W-MPJPE) by
83-91%.
  Kineo is also efficient in real-world scenarios, processing multi-view
sequences faster than their duration in specific configuration (e.g., 36min to
process 1h20min of footage). The full pipeline and evaluation code are openly
released to promote reproducibility and practical adoption at
https://liris-xr.github.io/kineo/.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将对这篇论文摘要进行分析。</p>
<hr />
<p><strong>论文摘要分析：Kineo: Calibration-Free Metric Motion Capture From Sparse RGB Cameras</strong></p>
<p><strong>1. 论文主要贡献的简洁总结 (2-3 句话)</strong></p>
<p>Kineo 提出了一种全自动、免标定、无标记的多视角运动捕捉流水线，能够从非同步、未标定的消费级RGB相机视频中，以度量尺度同时完成相机标定（包括畸变系数）和3D关键点及密集场景点图的重建。它通过置信度驱动的时空关键点采样策略和基于图的全局优化，显著提升了标定和重建的鲁棒性和准确性，并大幅降低了计算成本，使其在实际应用中更高效。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<p>Kineo 的核心创新在于其结合了以下几个关键点：</p>
<ul>
<li><strong>同时标定与重建：</strong> 它不依赖于预先的相机标定，而是利用现成的2D关键点检测器，在同一优化框架下同时估计相机参数（包括Brown-Conrady畸变系数）、3D关键点和密集场景点图，并以度量尺度进行。</li>
<li><strong>置信度驱动的时空关键点采样策略：</strong> 这种策略能够智能地选择和利用2D关键点数据，确保在复杂场景下也能进行鲁棒的标定，并有效处理非同步相机数据。</li>
<li><strong>基于图的全局优化：</strong> 采用全局优化方法，将所有可用的信息（2D关键点、相机模型等）整合到一个统一的优化问题中，以实现更准确和一致的解决方案。</li>
<li><strong>计算成本独立于序列长度：</strong> 摘要中提到“fixed computational cost independent of sequence length”，这暗示了其优化策略能够有效地处理长视频序列，避免了传统方法中计算成本随时间线性增长的问题。</li>
<li><strong>成对重投影一致性评分：</strong> 引入了新的度量标准来量化3D重建的可靠性，这对于下游任务的质量控制至关重要。</li>
</ul>
<p><strong>3. 对领域潜在影响</strong></p>
<p>Kineo 对计算机视觉领域具有深远的潜在影响：</p>
<ul>
<li><strong>民主化运动捕捉：</strong> 通过消除对精确相机标定的需求，Kineo 极大地降低了专业运动捕捉系统的门槛，使得非专家用户也能在“野外”或非受控环境中进行高质量的运动捕捉。这将推动运动捕捉技术在更广泛的应用场景中普及。</li>
<li><strong>提升“野外”场景的性能：</strong> 现有免标定方法在计算成本和重建精度上存在不足，Kineo 在这两方面都取得了显著进步，尤其是在相机姿态和3D关键点重建精度上，使其在真实世界、非理想条件下的应用更具可行性。</li>
<li><strong>推动消费级硬件的应用：</strong> 能够利用非同步、未标定的消费级RGB相机进行高精度运动捕捉，将加速运动捕捉技术与智能手机、家用摄像头等设备的结合，催生新的应用。</li>
<li><strong>为下游任务提供更可靠的输入：</strong> 提出的成对重投影一致性评分，为后续的人体姿态估计、动作分析、虚拟现实/增强现实等任务提供了更可靠的3D重建数据，有助于提升这些应用的整体性能。</li>
</ul>
<p><strong>4. 相关领域或应用</strong></p>
<p>以下领域或应用将从这项研究中受益：</p>
<ul>
<li><strong>虚拟现实 (VR) 和增强现实 (AR)：</strong> 实时、免标定的运动捕捉可以用于更自然的虚拟化身控制、手势识别和与虚拟环境的交互。</li>
<li><strong>体育科学与训练：</strong> 运动员动作分析、生物力学研究，无需昂贵的专业设备即可进行。</li>
<li><strong>医疗康复：</strong> 患者步态分析、康复训练效果评估，方便医生和患者使用。</li>
<li><strong>电影与游戏制作：</strong> 角色动画、预可视化，降低制作成本和时间。</li>
<li><strong>机器人学：</strong> 人机交互、机器人模仿学习，使机器人能更好地理解和复制人类动作。</li>
<li><strong>安全监控：</strong> 异常行为检测、人群分析。</li>
<li><strong>人机交互 (HCI)：</strong> 更自然、直观的交互界面设计。</li>
</ul>
<p><strong>5. 从摘要中推断出的局限性</strong></p>
<p>尽管 Kineo 取得了显著进步，但从摘要中仍可推断出一些潜在的局限性：</p>
<ul>
<li><strong>“Sparse RGB Cameras”：</strong> 摘要中提到“Sparse RGB Cameras”，这可能意味着它在相机数量非常少（例如，只有两三个）的情况下性能可能会受到限制，或者对相机视角的覆盖范围有一定要求。</li>
<li><strong>“Fixed computational cost independent of sequence length”：</strong> 虽然这是一个优点，但“fixed”的成本本身可能仍然较高，尤其是在处理大量相机或高分辨率视频时。摘要中提到“processing multi-view sequences faster than their duration in specific configuration (e.g., 36min to process 1h20min of footage)”，这表明虽然比实时快，但对于某些对延迟要求极高的应用（如实时VR），可能仍需进一步优化。</li>
<li><strong>“Consumer-grade RGB cameras”：</strong> 消费级相机通常在低光照、快速运动模糊或复杂背景下表现不佳。虽然 Kineo 提升了鲁棒性，但这些固有的相机限制仍可能影响最终的重建质量。</li>
<li><strong>“Leverages 2D keypoints from off-the-shelf detectors”：</strong> Kineo 的性能在一定程度上依赖于所使用的2D关键点检测器的准确性和鲁棒性。如果2D检测器在特定场景下表现不佳，可能会影响整体系统的性能。</li>
<li><strong>“Confidence-driven spatio-temporal keypoint sampling strategy”：</strong> 这种策略的有效性可能与场景的复杂性、遮挡程度以及关键点检测的置信度分布有关。在极端遮挡或关键点置信度普遍较低的场景下，其性能可能会下降。</li>
<li><strong>未提及的场景限制：</strong> 摘要中未明确说明 Kineo 在极端环境（如水下、烟雾、极端光照变化）或非人类对象（如动物）上的表现。</li>
</ul>
<p>总的来说，Kineo 是一项令人兴奋的研究，它通过创新的方法解决了多视角运动捕捉领域的一个核心挑战，即相机标定问题，并显著提升了性能和可用性。其开源代码的发布也将极大地促进该领域的进一步研究和实际应用。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present Kineo, a fully automatic, calibration-free pipeline for markerless
motion capture from videos captured by unsynchronized, uncalibrated,
consumer-grade RGB cameras.</li>
<li>Compared to previous state-of-the-art
approaches, Kineo reduces camera translation error by approximately 83-85%,
camera angular error by 86-92%, and world mean-per-joint error (W-MPJPE) by
83-91%.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.24464v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.24464v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.24410v1'></a></p>
<h2 id="a-hybrid-approach-for-visual-multi-object-tracking"><a href="https://arxiv.org/abs/2510.24410v1">A Hybrid Approach for Visual Multi-Object Tracking</a></h2>
<p><strong>Authors:</strong> Toan Van Nguyen, Rasmus G. K. Christiansen, Dirk Kraft, Leon Bodenhagen</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Categories:</strong> cs.CV, cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>This paper proposes a visual multi-object tracking method that jointly
employs stochastic and deterministic mechanisms to ensure identifier
consistency for unknown and time-varying target numbers under nonlinear
dynamics. A stochastic particle filter addresses nonlinear dynamics and
non-Gaussian noise, with support from particle swarm optimization (PSO) to
guide particles toward state distribution modes and mitigate divergence through
proposed fitness measures incorporating motion consistency, appearance
similarity, and social-interaction cues with neighboring targets. Deterministic
association further enforces identifier consistency via a proposed cost matrix
incorporating spatial consistency between particles and current detections,
detection confidences, and track penalties. Subsequently, a novel scheme is
proposed for the smooth updating of target states while preserving their
identities, particularly for weak tracks during interactions with other targets
and prolonged occlusions. Moreover, velocity regression over past states
provides trend-seed velocities, enhancing particle sampling and state updates.
The proposed tracker is designed to operate flexibly for both pre-recorded
videos and camera live streams, where future frames are unavailable.
Experimental results confirm superior performance compared to state-of-the-art
trackers. The source-code reference implementations of both the proposed method
and compared-trackers are provided on GitHub:
https://github.com/SDU-VelKoTek/GenTrack2</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Toan Van Nguyen, Rasmus G. K. Christiansen, Dirk Kraft, Leon Bodenhagen撰写的论文“A Hybrid Approach for Visual Multi-Object Tracking”的全面摘要。</p>
<hr />
<p><strong>论文摘要：视觉多目标跟踪的混合方法</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决视觉多目标跟踪（MOT）中的核心挑战，即在目标数量未知且随时间变化、运动非线性和存在非高斯噪声的情况下，如何确保目标身份的一致性。MOT面临物体相似性、遮挡和不规则运动等问题，这些因素可能导致身份混淆、跟踪丢失和关联困难。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
该论文提出了一种混合视觉多目标跟踪框架，结合了随机和确定性机制，以实现鲁棒和准确的跟踪：
*   <strong>混合跟踪框架：</strong> 结合了随机粒子滤波器和确定性关联。粒子滤波器处理非线性动力学和非高斯噪声，而确定性关联则确保身份一致性。
*   <strong>基于PSO的粒子优化：</strong> 引入粒子群优化（PSO）来引导粒子向状态分布模式收敛，并通过提出的适应度函数（结合运动一致性、外观相似性和与邻近目标的社交互动线索）来减轻发散。
*   <strong>改进的成本矩阵：</strong> 提出了一种新的成本矩阵，用于确定性关联，该矩阵整合了粒子与当前检测之间的空间一致性、检测置信度以及跟踪惩罚，从而增强了身份一致性。
*   <strong>平滑状态更新方案：</strong> 提出了一种新颖的方案，用于平滑更新目标状态，特别是在与其他目标互动和长时间遮挡期间，为弱跟踪保留其身份。
*   <strong>基于历史状态的速度回归：</strong> 利用过去的状态进行速度回归，提供趋势种子速度，从而改进粒子采样和状态更新，尤其是在遮挡或检测器信号弱/噪声时。
*   <strong>灵活的操作模式：</strong> 该跟踪器设计为可灵活用于预录视频和实时摄像头流，在后一种情况下，未来帧是不可用的。</p>
<p><strong>3. 主要结果及其重要性：</strong>
实验结果表明，与现有最先进的跟踪器相比，所提出的方法表现出卓越的性能。在MOT17数据集上进行的评估显示，该方法在ATA（平均跟踪精度）、IDF1、HOTA（高阶跟踪精度）和MOTA（多目标跟踪精度）等多个指标上均优于其他方法，并且IDSW（身份切换）数量更少。这表明该方法在保持鲁棒性能的同时，显著提高了跟踪精度和身份一致性。论文还提供了源代码，方便复现和比较评估。</p>
<p><strong>4. 论文中提到的局限性：</strong>
*   尽管粒子滤波器能够处理不确定性和非线性动力学，但它们可能产生不一致的结果。
*   传统的粒子滤波器方法在目标数量增加时扩展性较差，并且在频繁目标变化的情况下可靠性降低。
*   基于检测的方法虽然能更确定地处理目标添加和移除，但往往忽略了跟踪推断的优化。
*   在PSO中，如果粒子数量很少，丢弃阈值需要谨慎使用，否则可能导致性能下降。
*   在长时间遮挡期间，边界框尺寸的更新可能导致不切实际的增长或收缩。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   未来的研究将侧重于目标交互模型，以进一步改进遮挡期间的状态更新。
*   探索提取目标社交行为模式的方法。</p>
<hr />
<p>这篇论文通过其混合方法，在处理多目标跟踪的复杂性方面取得了显著进展，特别是在非线性动态、未知目标数量和遮挡场景下。通过结合PSO优化、改进的关联成本和智能的状态更新机制，该方法有效地解决了现有跟踪器的一些关键限制。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Subsequently, a novel scheme is
proposed for the smooth updating of target states while preserving their
identities, particularly for weak tracks during interactions with other targets
and prolonged occlusions.</li>
<li>Experimental results confirm superior performance compared to state-of-the-art
trackers.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.24410v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.24410v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.24399v1'></a></p>
<h2 id="gentrack-a-new-generation-of-multi-object-tracking"><a href="https://arxiv.org/abs/2510.24399v1">GenTrack: A New Generation of Multi-Object Tracking</a></h2>
<p><strong>Authors:</strong> Toan Van Nguyen, Rasmus G. K. Christiansen, Dirk Kraft, Leon Bodenhagen</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Categories:</strong> cs.CV, cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>This paper introduces a novel multi-object tracking (MOT) method, dubbed
GenTrack, whose main contributions include: a hybrid tracking approach
employing both stochastic and deterministic manners to robustly handle unknown
and time-varying numbers of targets, particularly in maintaining target
identity (ID) consistency and managing nonlinear dynamics, leveraging particle
swarm optimization (PSO) with some proposed fitness measures to guide
stochastic particles toward their target distribution modes, enabling effective
tracking even with weak and noisy object detectors, integration of social
interactions among targets to enhance PSO-guided particles as well as improve
continuous updates of both strong (matched) and weak (unmatched) tracks,
thereby reducing ID switches and track loss, especially during occlusions, a
GenTrack-based redefined visual MOT baseline incorporating a comprehensive
state and observation model based on space consistency, appearance, detection
confidence, track penalties, and social scores for systematic and efficient
target updates, and the first-ever publicly available source-code reference
implementation with minimal dependencies, featuring three variants, including
GenTrack Basic, PSO, and PSO-Social, facilitating flexible reimplementation.
Experimental results have shown that GenTrack provides superior performance on
standard benchmarks and real-world scenarios compared to state-of-the-art
trackers, with integrated implementations of baselines for fair comparison.
Potential directions for future work are also discussed. The source-code
reference implementations of both the proposed method and compared-trackers are
provided on GitHub: https://github.com/SDU-VelKoTek/GenTrack</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Toan Van Nguyen, Rasmus G. K. Christiansen, Dirk Kraft, Leon Bodenhagen撰写的论文“GenTrack: A New Generation of Multi-Object Tracking”的全面摘要。</p>
<hr />
<p><strong>论文摘要：GenTrack: 一种新一代多目标跟踪方法</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
多目标跟踪（MOT）旨在准确、连续地跟踪图像或视频中多个感兴趣的目标。然而，由于目标运动不规则、频繁遮挡、目标间视觉相似性以及目标数量随时间变化（进入/离开场景）等因素，实现鲁棒的MOT仍然是一个挑战。现有方法，特别是基于卡尔曼滤波器的方法，通常假设线性运动和高斯噪声，并且在处理非线性动力学和非高斯噪声方面存在局限性。基于粒子滤波器的方法虽然能处理非线性问题，但往往计算成本高昂，且在目标数量可变时性能不佳，容易出现ID切换和轨迹丢失。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
GenTrack提出了一种新颖的MOT方法，其主要贡献包括：</p>
<ul>
<li><strong>混合跟踪方法：</strong> 结合了随机（粒子群优化PSO）和确定性方法，以鲁棒地处理未知和时变的目标数量，特别是在保持目标身份（ID）一致性和管理非线性动力学方面。</li>
<li><strong>PSO引导的粒子：</strong> 利用粒子群优化（PSO）算法，结合提出的适应度度量，引导随机粒子趋向其目标分布模式，即使在检测器较弱和噪声较大时也能实现有效跟踪。</li>
<li><strong>社会交互集成：</strong> 整合目标间的社会交互，以增强PSO引导的粒子，并改进强（匹配）和弱（未匹配）轨迹的连续更新，从而减少ID切换和轨迹丢失，尤其是在遮挡期间。</li>
<li><strong>重新定义的视觉MOT基线：</strong> 引入了一个基于GenTrack的视觉MOT基线，包含一个全面的状态和观测模型，该模型基于空间一致性、外观、检测置信度、轨迹惩罚和社会分数，用于系统高效的目标更新。</li>
<li><strong>开源实现：</strong> 提供了首个公开可用的源代码参考实现，具有最少的依赖性，并包含GenTrack Basic、PSO和PSO-Social三种变体，便于灵活复现和比较。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong>
实验结果表明，GenTrack在标准基准和真实场景中，与最先进的跟踪器相比，提供了卓越的性能。具体来说：</p>
<ul>
<li><strong>优越的跟踪性能：</strong> 在人类跟踪（MOT17-04）和奶牛跟踪（MooTrack360）场景中，GenTrack始终优于现有最先进的跟踪器。在奶牛跟踪中，GenTrack实现了100%的成功率，无ID切换。在人类跟踪中，GenTrack PSO-Social也以最少的ID切换（仅4次）实现了最佳性能。</li>
<li><strong>对参数设置不敏感：</strong> GenTrack的性能对参数设置表现出高度不敏感性，这增强了其在不同应用中的鲁棒性。</li>
<li><strong>计算效率：</strong> 尽管是基于粒子滤波器的方法，GenTrack在保持鲁棒性能的同时，仅使用少量粒子（GenTrack Basic 8个，PSO和PSO-Social 6个），实现了较低的CPU延迟（奶牛跟踪场景中分别为3.67ms、6.35ms和6.74ms；人类跟踪场景中分别为56ms、64ms和92ms），表明其适用于实时跟踪。</li>
</ul>
<p><strong>4. 论文中提及的局限性：</strong>
论文中没有明确指出GenTrack的局限性，但通过讨论未来工作方向，间接暗示了当前方法的改进空间：</p>
<ul>
<li><strong>外观相似性度量：</strong> 当前使用HoG特征的余弦相似度，可能未充分考虑目标特定或应用特定的外观特征，且未完全忽略背景效应。</li>
<li><strong>运动模型：</strong> 当前采用随机运动模型，虽然通过PSO引导有所改进，但仍可通过整合基于路径的主导集聚类等方法进一步增强，以更好地处理遮挡和ID切换。</li>
<li><strong>粒子初始化策略：</strong> 尽管低适应度粒子可以被丢弃或替换，但噪声依赖的粒子初始化策略可能进一步减少冗余并提高在噪声环境中的性能。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong>
论文提出了几个有前景的未来研究方向：</p>
<ul>
<li><strong>改进外观相似性：</strong> 针对特定目标和应用定制外观相似性度量，并在目标区域内进行比较，以忽略背景效应。</li>
<li><strong>噪声自适应采样：</strong> 引入噪声依赖的粒子初始化策略，例如使用Metropolis-Hastings算法进行提议生成，以减少冗余并提高在噪声环境中的性能。</li>
<li><strong>改进运动模型：</strong> 通过整合基于路径的主导集聚类来增强运动模型，以更好地处理遮挡和ID切换。</li>
<li><strong>群组跟踪：</strong> 开发一种新的GenTrack变体，通过整合聚类算法（如DBSCAN）和群组操作（成员添加/移除、群组合并/拆分）来实现群组跟踪。</li>
<li><strong>多摄像头多目标跟踪：</strong> 扩展GenTrack框架以支持多摄像头MOT，包括有重叠和无重叠的场景，通过帧内跟踪和跨摄像头关联，或将多摄像头检测整合到统一框架中。</li>
<li><strong>三维（3D）多目标跟踪：</strong> 重新定义对象状态和观测模型，以整合3D数据，利用3D对象检测器或聚类方法进行对象检测。</li>
</ul>
<hr />
<p>总而言之，GenTrack通过其混合跟踪框架、PSO引导的粒子、社会交互集成以及全面的状态和观测模型，为多目标跟踪领域带来了显著进步。它在保持高精度和ID一致性的同时，展现了计算效率和对参数不敏感的特性，为未来的MOT研究和实际应用奠定了坚实的基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>This paper introduces a novel multi-object tracking (MOT) method, dubbed
GenTrack, whose main contributions include: a hybrid tracking approach
employing both stochastic and deterministic manners to robustly handle unknown
and time-varying numbers of targets, particularly in maintaining target
identity (ID) consistency and managing nonlinear dynamics, leveraging particle
swarm optimization (PSO) with some proposed fitness measures to guide
stochastic particles toward their target distribution modes, enabling effective
tracking even with weak and noisy object detectors, integration of social
interactions among targets to enhance PSO-guided particles as well as improve
continuous updates of both strong (matched) and weak (unmatched) tracks,
thereby reducing ID switches and track loss, especially during occlusions, a
GenTrack-based redefined visual MOT baseline incorporating a comprehensive
state and observation model based on space consistency, appearance, detection
confidence, track penalties, and social scores for systematic and efficient
target updates, and the first-ever publicly available source-code reference
implementation with minimal dependencies, featuring three variants, including
GenTrack Basic, PSO, and PSO-Social, facilitating flexible reimplementation.</li>
<li>Experimental results have shown that GenTrack provides superior performance on
standard benchmarks and real-world scenarios compared to state-of-the-art
trackers, with integrated implementations of baselines for fair comparison.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.24399v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.24399v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.24214v1'></a></p>
<h2 id="scope-saliency-coverage-oriented-token-pruning-for-efficient-multimodel-llms"><a href="https://arxiv.org/abs/2510.24214v1">SCOPE: Saliency-Coverage Oriented Token Pruning for Efficient Multimodel LLMs</a></h2>
<p><strong>Authors:</strong> Jinhong Deng, Wen Li, Joey Tianyi Zhou, Yang He</p>
<p><strong>Published:</strong> 2025-10-28</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Multimodal Large Language Models (MLLMs) typically process a large number of
visual tokens, leading to considerable computational overhead, even though many
of these tokens are redundant. Existing visual token pruning methods primarily
focus on selecting the most salient tokens based on attention scores, resulting
in the semantic incompleteness of the selected tokens. In this paper, we
propose a novel visual token pruning strategy, called
\textbf{S}aliency-\textbf{C}overage \textbf{O}riented token \textbf{P}runing
for \textbf{E}fficient MLLMs (SCOPE), to jointly model both the saliency and
coverage of the selected visual tokens to better preserve semantic
completeness. Specifically, we introduce a set-coverage for a given set of
selected tokens, computed based on the token relationships. We then define a
token-coverage gain for each unselected token, quantifying how much additional
coverage would be obtained by including it. By integrating the saliency score
into the token-coverage gain, we propose our SCOPE score and iteratively select
the token with the highest SCOPE score. We conduct extensive experiments on
multiple vision-language understanding benchmarks using the LLaVA-1.5 and
LLaVA-Next models. Experimental results demonstrate that our method
consistently outperforms prior approaches. Our code is available at
\href{https://github.com/kinredon/SCOPE}{https://github.com/kinredon/SCOPE}.</p>
<p><strong>Analysis:</strong></p>
<p>以下是Jinhong Deng, Wen Li, Joey Tianyi Zhou, Yang He撰写的论文“SCOPE: Saliency-Coverage Oriented Token Pruning for Efficient Multimodel LLMs”的全面摘要：</p>
<p><strong>1. 论文主要问题或研究问题</strong></p>
<p>多模态大型语言模型（MLLMs）在处理视觉输入时通常会生成大量的视觉token，这导致了显著的计算开销，尤其是在高分辨率图像或密集视频场景中。现有视觉token剪枝方法主要关注基于注意力分数选择最显著的token，但这往往导致语义不完整性，因为它们忽略了关键的上下文信息。因此，论文旨在解决如何在大幅减少视觉token数量的同时，有效保留语义完整性，以提高MLLMs的计算效率和性能。</p>
<p><strong>2. 关键创新或方法论贡献</strong></p>
<p>论文提出了一种新颖的视觉token剪枝策略，名为<strong>S</strong>aliency-<strong>C</strong>overage <strong>O</strong>riented token <strong>P</strong>runing for <strong>E</strong>fficient MLLMs (SCOPE)。其核心创新在于：</p>
<ul>
<li><strong>联合建模显著性与覆盖度：</strong> SCOPE方法突破了传统剪枝方法仅关注显著性的局限，首次联合考虑了所选视觉token的显著性和覆盖度，以更好地保留语义完整性。</li>
<li><strong>引入集合覆盖度（Set-Coverage）概念：</strong> 论文为给定的一组已选token定义了集合覆盖度，该覆盖度基于token之间的关系计算，量化了已选token对整个语义空间的代表性程度。</li>
<li><strong>定义Token覆盖度增益：</strong> 对于每个未选token，论文定义了token覆盖度增益，量化了将其包含进来能带来多少额外的覆盖度。</li>
<li><strong>提出SCOPE分数并迭代选择：</strong> 通过将显著性分数整合到token覆盖度增益中，论文提出了SCOPE分数，并迭代选择具有最高SCOPE分数的token。这种策略确保了不仅保留最具信息量的token，同时也保证了广泛的语义覆盖。</li>
</ul>
<p><strong>3. 主要结果及其意义</strong></p>
<p>SCOPE方法在多个视觉-语言理解基准测试（包括LLaVA-1.5和LLaVA-Next模型）上进行了广泛实验，并取得了显著成果：</p>
<ul>
<li><strong>性能超越现有方法：</strong> SCOPE在所有token配置下均持续优于现有剪枝方法。例如，在LLaVA-1.5 7B模型上，即使将视觉token数量减少9倍（保留64个token），SCOPE仍能保持原始性能的96.0%，显著优于VisionZip（93.5%）和SparseVLM（85.1%）等基线。</li>
<li><strong>在极端压缩下表现稳定：</strong> 即使在极端压缩（如仅保留8个token）的情况下，SCOPE也表现出卓越的性能稳定性，并以越来越大的优势持续优于VisionZip。</li>
<li><strong>在视频基准测试中表现出色：</strong> 在Video-LLaVA上，SCOPE在大幅剪枝（仅保留136个token，原始2048个）后，几乎完全保留了原始性能，证明了其在视频-语言任务中的强大有效性。</li>
<li><strong>提升模型性能：</strong> 在某些基准测试（如POPE和MMVet）上，SCOPE甚至超越了原始模型的性能，这表明MLLMs中的视觉token存在冗余，而SCOPE不仅减少了冗余，还通过消除冗余信息提高了性能。</li>
<li><strong>计算效率高：</strong> 尽管SCOPE的延迟略高于PDrop，但它相对于全token基线仍实现了3.2倍的加速，证明了其在保持性能的同时具有计算效率。</li>
</ul>
<p>这些结果表明，SCOPE在大幅减少视觉token数量的同时，能够有效保留语义完整性，显著提高了MLLMs的计算效率和实用性。</p>
<p><strong>4. 论文中提及的局限性</strong></p>
<p>论文也坦诚地指出了SCOPE的几个局限性：</p>
<ul>
<li><strong>潜在的细粒度信息丢失：</strong> 尽管SCOPE努力平衡显著性和覆盖度，但激进的token剪枝仍可能导致细粒度或稀有语义信息的丢失，这可能会影响需要详细视觉理解的任务。</li>
<li><strong>泛化能力有待进一步验证：</strong> 实验主要基于广泛使用的视觉-语言基准测试和两个代表性的MLLM（LLaVA 1.5和LLaVA-Next）。因此，SCOPE对其他任务或模型架构的泛化能力尚待充分验证。</li>
</ul>
<p><strong>5. 潜在的未来研究方向</strong></p>
<p>论文并未明确提出未来的研究方向，但从其局限性和贡献中可以推断出以下潜在方向：</p>
<ul>
<li><strong>优化细粒度信息保留：</strong> 探索更精细的剪枝策略，以在极端压缩下更好地保留细粒度或稀有语义信息，例如通过自适应地调整显著性与覆盖度的权重，或引入更复杂的token关系建模。</li>
<li><strong>扩展到更多模型和任务：</strong> 在更广泛的MLLM架构（如Qwen2-VL以外的其他模型）和更多样化的视觉-语言任务（如3D视觉理解、具身AI等）上验证SCOPE的泛化能力。</li>
<li><strong>动态剪枝策略：</strong> 研究根据输入内容或任务需求动态调整剪枝比例的策略，以实现更灵活和高效的token管理。</li>
<li><strong>理论分析与可解释性：</strong> 对SCOPE的性能边界和剪枝决策进行更深入的理论分析，并提高其可解释性，以更好地理解其在不同场景下的行为。</li>
<li><strong>结合其他压缩技术：</strong> 探索将SCOPE与token合并、量化等其他视觉token压缩技术相结合，以实现更大的压缩率和效率提升。</li>
</ul>
<p>总而言之，SCOPE为高效MLLMs的视觉token剪枝提供了一个原则性且有效的新框架，通过联合优化显著性和覆盖度，在计算效率和语义完整性之间取得了卓越的平衡。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this paper, we
propose a novel visual token pruning strategy, called
\textbf{S}aliency-\textbf{C}overage \textbf{O}riented token \textbf{P}runing
for \textbf{E}fficient MLLMs (SCOPE), to jointly model both the saliency and
coverage of the selected visual tokens to better preserve semantic
completeness.</li>
<li>Specifically, we introduce a set-coverage for a given set of
selected tokens, computed based on the token relationships.</li>
<li>By integrating the saliency score
into the token-coverage gain, we propose our SCOPE score and iteratively select
the token with the highest SCOPE score.</li>
<li>Experimental results demonstrate that our method
consistently outperforms prior approaches.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.24214v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.24214v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-10-29 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
