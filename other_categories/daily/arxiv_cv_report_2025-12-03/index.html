<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-12-03 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-02
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-12-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-08
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-12-02/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-12-04/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-12-03">Arxiv Computer Vision Papers - 2025-12-03</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#nuscenes-revisited-progress-and-challenges-in-autonomous-driving" class="nav-link">nuScenes Revisited: Progress and Challenges in Autonomous Driving</a>
                </li>
                <li class="nav-item">
                    <a href="#magicquillv2-precise-and-interactive-image-editing-with-layered-visual-cues" class="nav-link">MagicQuillV2: Precise and Interactive Image Editing with Layered Visual Cues</a>
                </li>
                <li class="nav-item">
                    <a href="#cameo-correspondence-attention-alignment-for-multi-view-diffusion-models" class="nav-link">CAMEO: Correspondence-Attention Alignment for Multi-View Diffusion Models</a>
                </li>
                <li class="nav-item">
                    <a href="#video2act-a-dual-system-video-diffusion-policy-with-robotic-spatio-motional-modeling" class="nav-link">Video2Act: A Dual-System Video Diffusion Policy with Robotic Spatio-Motional Modeling</a>
                </li>
                <li class="nav-item">
                    <a href="#onethinker-all-in-one-reasoning-model-for-image-and-video" class="nav-link">OneThinker: All-in-one Reasoning Model for Image and Video</a>
                </li>
                <li class="nav-item">
                    <a href="#multishotmaster-a-controllable-multi-shot-video-generation-framework" class="nav-link">MultiShotMaster: A Controllable Multi-Shot Video Generation Framework</a>
                </li>
                <li class="nav-item">
                    <a href="#video4spatial-towards-visuospatial-intelligence-with-context-guided-video-generation" class="nav-link">Video4Spatial: Towards Visuospatial Intelligence with Context-Guided Video Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#mavid-a-multimodal-framework-for-audio-visual-dialogue-understanding-and-generation" class="nav-link">MAViD: A Multimodal Framework for Audio-Visual Dialogue Understanding and Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#instant-video-models-universal-adapters-for-stabilizing-image-based-networks" class="nav-link">Instant Video Models: Universal Adapters for Stabilizing Image-Based Networks</a>
                </li>
                <li class="nav-item">
                    <a href="#in-context-sync-lora-for-portrait-video-editing" class="nav-link">In-Context Sync-LoRA for Portrait Video Editing</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-12-03">Arxiv Computer Vision Papers - 2025-12-03</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>好的，作为一名专注于计算机视觉和机器学习的研究助理，我将为您提供一份简明的 Arxiv 计算机视觉领域论文的每日报告执行摘要。</p>
<hr />
<p><strong>Arxiv 计算机视觉领域论文每日报告 - 执行摘要 (2025-12-02)</strong></p>
<p><strong>1. 主要主题与趋势：</strong></p>
<p>本期 Arxiv 论文集中体现了计算机视觉领域在<strong>多模态理解与生成</strong>、<strong>视频内容创作与编辑</strong>以及<strong>自主驾驶技术</strong>方面的显著进展。特别是，<strong>扩散模型 (Diffusion Models)</strong> 在视频生成、图像编辑和多视图对齐方面展现出强大的潜力，并被进一步优化和应用。同时，<strong>具身智能 (Embodied AI)</strong> 和<strong>空间智能 (Spatial Intelligence)</strong> 的概念也开始在视频理解和生成中得到体现。</p>
<p><strong>2. 亮点与创新：</strong></p>
<ul>
<li><strong>视频生成与编辑的精细化与可控性：</strong><ul>
<li><strong>MagicQuillV2</strong> 和 <strong>MultiShotMaster</strong> 在图像编辑和多镜头视频生成方面提供了更精细的控制和交互能力，预示着内容创作工具的智能化升级。</li>
<li><strong>Video2Act</strong> 和 <strong>Video4Spatial</strong> 将视频内容与机器人运动建模及空间理解相结合，为具身智能和更具上下文感知的视频生成开辟了新途径。</li>
<li><strong>In-Context Sync-LoRA</strong> 针对人像视频编辑提出了高效的解决方案，展示了在特定领域应用中的创新。</li>
</ul>
</li>
<li><strong>多模态融合的深化：</strong><ul>
<li><strong>MAViD</strong> 在音频-视觉对话理解与生成方面的探索，强调了跨模态信息融合在复杂任务中的重要性。</li>
</ul>
</li>
<li><strong>通用性与效率的提升：</strong><ul>
<li><strong>Instant Video Models</strong> 提出的通用适配器，为稳定和高效地将图像网络应用于视频任务提供了新的思路。</li>
<li><strong>OneThinker</strong> 旨在构建一个能够处理图像和视频的统一推理模型，展现了模型通用性的发展方向。</li>
</ul>
</li>
</ul>
<p><strong>3. 新兴研究方向与技术：</strong></p>
<ul>
<li><strong>扩散模型的泛化与应用拓展：</strong> 扩散模型不仅在图像生成领域表现出色，正被积极应用于视频生成、多视图对齐（CAMEO）和交互式编辑。</li>
<li><strong>具身智能与空间智能的融合：</strong> 将视频内容与物理世界的交互和空间关系联系起来，是未来视频理解和生成的重要方向。</li>
<li><strong>可控与交互式内容生成：</strong> 研究重点正从生成内容转向如何更精细地控制生成过程，并提供用户友好的交互方式。</li>
<li><strong>统一多模态模型：</strong> 探索能够同时处理多种模态（如图像、视频、音频）并进行推理的通用模型。</li>
<li><strong>高效的视频处理技术：</strong> 针对视频数据量大、处理复杂的挑战，涌现出如适配器等提高效率的技术。</li>
</ul>
<p><strong>4. 建议阅读全文的论文：</strong></p>
<p>考虑到其对当前热门领域（如视频生成、多模态、具身智能）的贡献以及潜在的创新性，以下论文值得优先阅读全文：</p>
<ul>
<li><strong>MagicQuillV2: Precise and Interactive Image Editing with Layered Visual Cues</strong> (图像编辑的交互性和精细度是重要趋势)</li>
<li><strong>Video2Act: A Dual-System Video Diffusion Policy with Robotic Spatio-Motional Modeling</strong> (具身智能与视频生成结合，具有前瞻性)</li>
<li><strong>OneThinker: All-in-one Reasoning Model for Image and Video</strong> (统一模型是重要的研究方向)</li>
<li><strong>MAViD: A Multimodal Framework for Audio-Visual Dialogue Understanding and Generation</strong> (多模态融合的深入探索)</li>
<li><strong>nuScenes Revisited: Progress and Challenges in Autonomous Driving</strong> (虽然是回顾性，但对自动驾驶领域的现状和未来挑战的总结非常有价值)</li>
</ul>
<hr />
<p>这份摘要旨在帮助您快速了解本期 Arxiv 论文的核心内容和发展趋势。希望对您的研究工作有所助益！</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2512.02448v1">nuScenes Revisited: Progress and Challenges in Autonomous Driving</a></li>
<li><a href="#2512.03046v1">MagicQuillV2: Precise and Interactive Image Editing with Layered Visual Cues</a></li>
<li><a href="#2512.03045v1">CAMEO: Correspondence-Attention Alignment for Multi-View Diffusion Models</a></li>
<li><a href="#2512.03044v1">Video2Act: A Dual-System Video Diffusion Policy with Robotic Spatio-Motional Modeling</a></li>
<li><a href="#2512.03043v1">OneThinker: All-in-one Reasoning Model for Image and Video</a></li>
<li><a href="#2512.03041v1">MultiShotMaster: A Controllable Multi-Shot Video Generation Framework</a></li>
<li><a href="#2512.03040v1">Video4Spatial: Towards Visuospatial Intelligence with Context-Guided Video Generation</a></li>
<li><a href="#2512.03034v1">MAViD: A Multimodal Framework for Audio-Visual Dialogue Understanding and Generation</a></li>
<li><a href="#2512.03014v1">Instant Video Models: Universal Adapters for Stabilizing Image-Based Networks</a></li>
<li><a href="#2512.03013v1">In-Context Sync-LoRA for Portrait Video Editing</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2512.02448v1'></a></p>
<h2 id="nuscenes-revisited-progress-and-challenges-in-autonomous-driving"><a href="https://arxiv.org/abs/2512.02448v1">nuScenes Revisited: Progress and Challenges in Autonomous Driving</a></h2>
<p><strong>Authors:</strong> Whye Kit Fong, Venice Erin Liong, Kok Seang Tan, Holger Caesar</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Categories:</strong> cs.CV, cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>Autonomous Vehicles (AV) and Advanced Driver Assistance Systems (ADAS) have been revolutionized by Deep Learning. As a data-driven approach, Deep Learning relies on vast amounts of driving data, typically labeled in great detail. As a result, datasets, alongside hardware and algorithms, are foundational building blocks for the development of AVs. In this work we revisit one of the most widely used autonomous driving datasets: the nuScenes dataset. nuScenes exemplifies key trends in AV development, being the first dataset to include radar data, to feature diverse urban driving scenes from two continents, and to be collected using a fully autonomous vehicle operating on public roads, while also promoting multi-modal sensor fusion, standardized benchmarks, and a broad range of tasks including perception, localization \&amp; mapping, prediction and planning. We provide an unprecedented look into the creation of nuScenes, as well as its extensions nuImages and Panoptic nuScenes, summarizing many technical details that have hitherto not been revealed in academic publications. Furthermore, we trace how the influence of nuScenes impacted a large number of other datasets that were released later and how it defined numerous standards that are used by the community to this day. Finally, we present an overview of both official and unofficial tasks using the nuScenes dataset and review major methodological developments, thereby offering a comprehensive survey of the autonomous driving literature, with a particular focus on nuScenes.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“nuScenes Revisited: Progress and Challenges in Autonomous Driving”的全面摘要，重点关注其对计算机视觉领域的新颖性和重要性：</p>
<p><strong>论文题目：</strong> nuScenes Revisited: Progress and Challenges in Autonomous Driving
<strong>作者：</strong> Whye Kit Fong, Venice Erin Liong, Kok Seang Tan, Holger Caesar</p>
<p><strong>摘要：</strong></p>
<p>这篇论文对自动驾驶领域中广泛使用的 nuScenes 数据集进行了深入的回顾和分析。作者们旨在通过提供对 nuScenes 数据集创建过程的详细洞察，以及分析其对后续数据集和研究的影响，来全面梳理自动驾驶领域的发展现状和面临的挑战。</p>
<p><strong>1. 主要问题/研究问题：</strong></p>
<p>论文的核心研究问题在于：
* <strong>nuScenes 数据集是如何创建的，其设计理念和技术细节是什么？</strong>
* <strong>nuScenes 数据集如何影响了自动驾驶领域的研究和数据集发展？</strong>
* <strong>nuScenes 数据集在推动自动驾驶技术（如感知、定位、预测、规划等）方面取得了哪些进展，又面临哪些挑战？</strong>
* <strong>当前基于 nuScenes 的各项任务（如 3D 检测、跟踪、分割等）的最新方法和性能如何？</strong></p>
<p><strong>2. 关键创新或方法论贡献：</strong></p>
<ul>
<li><strong>深入揭示 nuScenes 数据集的创建细节：</strong> 论文提供了许多此前未在学术出版物中披露的技术细节，包括数据采集车辆的传感器配置、数据后处理流程（如图像分辨率调整、运动补偿）、同步和校准方法、以及数据标注的详细过程。</li>
<li><strong>分析 nuScenes 的影响力和标准化作用：</strong> 作者们追溯了 nuScenes 如何启发了后续大量数据集的发布，并定义了许多行业标准，例如多模态传感器融合、统一的基准测试任务和评估指标。</li>
<li><strong>全面回顾和分析 nuScenes 的优缺点：</strong> 论文不仅肯定了 nuScenes 在引入雷达数据、多样化场景、多模态融合等方面的贡献，也批判性地讨论了其设计选择的优缺点，例如雷达传感器同步问题、数据标注偏差等。</li>
<li><strong>系统梳理基于 nuScenes 的各项任务和方法：</strong> 论文详细介绍了 nuScenes 数据集支持的官方和非官方任务，并对这些任务的最新方法论进展进行了深入的概述和分析，包括 3D 检测、跟踪、Lidar 分割、语义占用预测、运动预测等。</li>
<li><strong>提供对未来研究方向的建议：</strong> 基于对当前进展和挑战的分析，论文为未来的研究提供了有价值的见解和方向。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong></p>
<ul>
<li><strong>nuScenes 的广泛影响：</strong> 论文证实了 nuScenes 作为自动驾驶领域事实上的标准数据集之一的地位，其多模态传感器配置、详细的 3D 注释以及对多种任务的支持，极大地推动了该领域的研究。</li>
<li><strong>多模态融合的趋势：</strong> 论文强调了相机-雷达融合在 3D 检测任务中的重要性，并指出其性能正在快速追赶甚至超越纯 Lidar 方法，这对于降低自动驾驶系统的成本具有重要意义。</li>
<li><strong>对特定任务的深入分析：</strong> 论文展示了在 3D 检测、跟踪、Lidar 分割等任务上，基于 nuScenes 的方法取得了显著的进步，并识别了当前性能的瓶颈和发展趋势（如 Transformer 的应用）。</li>
<li><strong>对数据集设计的反思：</strong> 通过对 nuScenes 的优缺点进行分析，论文为未来数据集的设计提供了宝贵的经验教训，例如在数据分布、传感器同步、标注质量等方面需要注意的问题。</li>
<li><strong>推动了更精细化的研究：</strong> nuScenes 及其扩展数据集（如 Panoptic nuScenes, nuImages）为更精细化的感知任务（如语义占用、实例分割）以及更具挑战性的预测和规划任务提供了基础。</li>
</ul>
<p><strong>4. 提及的局限性：</strong></p>
<ul>
<li><strong>雷达传感器同步问题：</strong> 雷达传感器并非与相机和 Lidar 同步触发，导致了时间上的不一致，增加了标注的难度。</li>
<li><strong>数据标注偏差：</strong> 存在“雷达/Lidar 中心”的设计偏见，即仅在雷达或 Lidar 中可见但相机中不可见的物体可能导致纯相机方法受到不公平的惩罚。</li>
<li><strong>地图数据的局限性：</strong> 几何地图缺乏 Z 轴信息，导致投影到相机帧时可能出现错位。地图的全局配准也存在一定的不精确性。</li>
<li><strong>训练/验证/测试集划分问题：</strong> 论文指出，虽然 nuScenes 的数据集划分在一定程度上保证了数据分布的相似性，但对于某些特定任务（如映射），可能需要更精细的划分策略。</li>
<li><strong>对长尾场景的覆盖不足：</strong> 论文在讨论未来研究方向时提到，当前数据集在处理罕见的“角落场景”（corner-case scenarios）方面仍有不足，而这些场景往往是导致自动驾驶事故的主要原因。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>更先进的传感器融合技术：</strong> 尤其是在相机-雷达融合方面，进一步提升性能并降低成本。</li>
<li><strong>更鲁棒的感知和预测模型：</strong> 应对恶劣天气、长距离感知等挑战。</li>
<li><strong>更精细化的语义理解：</strong> 如语义占用预测、实例分割等，以实现对环境更全面的理解。</li>
<li><strong>端到端的自动驾驶系统：</strong> 结合感知、预测、规划等模块，实现更流畅和安全的自动驾驶。</li>
<li><strong>对罕见场景（Corner Cases）的深入研究：</strong> 开发能够有效处理和泛化到罕见场景的模型。</li>
<li><strong>闭环模拟（Closed-loop Simulation）：</strong> 论文强烈建议将研究重心从开环评估转向闭环模拟，以更真实地评估规划和控制系统的性能。</li>
<li><strong>多模态数据与语言的结合：</strong> 利用自然语言指令来辅助自动驾驶任务，例如视觉问答（VQA）和指令导航。</li>
<li><strong>更高效的数据标注和模型训练方法：</strong> 探索半监督学习、自监督学习和主动学习等技术，以降低数据标注成本并提高模型效率。</li>
</ul>
<p>总而言之，这篇论文不仅对 nuScenes 数据集进行了全面的回顾和分析，还深入探讨了其在自动驾驶领域的影响力、方法论贡献以及未来的发展方向，为该领域的研究人员提供了宝贵的参考和洞察。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Finally, we present an overview of both official and unofficial tasks using the nuScenes dataset and review major methodological developments, thereby offering a comprehensive survey of the autonomous driving literature, with a particular focus on nuScenes.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.02448v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.02448v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.03046v1'></a></p>
<h2 id="magicquillv2-precise-and-interactive-image-editing-with-layered-visual-cues"><a href="https://arxiv.org/abs/2512.03046v1">MagicQuillV2: Precise and Interactive Image Editing with Layered Visual Cues</a></h2>
<p><strong>Authors:</strong> Zichen Liu, Yue Yu, Hao Ouyang, Qiuyu Wang, Shuailei Ma, Ka Leong Cheng, Wen Wang, Qingyan Bai, Yuxuan Zhang, Yanhong Zeng, Yixuan Li, Xing Zhu, Yujun Shen, Qifeng Chen</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We propose MagicQuill V2, a novel system that introduces a \textbf{layered composition} paradigm to generative image editing, bridging the gap between the semantic power of diffusion models and the granular control of traditional graphics software. While diffusion transformers excel at holistic generation, their use of singular, monolithic prompts fails to disentangle distinct user intentions for content, position, and appearance. To overcome this, our method deconstructs creative intent into a stack of controllable visual cues: a content layer for what to create, a spatial layer for where to place it, a structural layer for how it is shaped, and a color layer for its palette. Our technical contributions include a specialized data generation pipeline for context-aware content integration, a unified control module to process all visual cues, and a fine-tuned spatial branch for precise local editing, including object removal. Extensive experiments validate that this layered approach effectively resolves the user intention gap, granting creators direct, intuitive control over the generative process.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为一名计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行深入分析。</p>
<p><strong>论文摘要分析：MagicQuillV2: Precise and Interactive Image Editing with Layered Visual Cues</strong></p>
<p><strong>1. 论文的主要贡献（2-3句话的简洁总结）</strong></p>
<p>MagicQuill V2 提出了一种新颖的“分层合成”范式，用于生成式图像编辑。该系统将用户的创意意图分解为内容、空间、结构和颜色等可控的视觉线索，从而弥合了大型扩散模型在整体生成能力与传统图形软件在精细控制之间的差距。通过这种方式，MagicQuill V2 实现了对生成过程的直接、直观的控制，并能进行精确的局部编辑，包括对象移除。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<p>MagicQuill V2 的核心创新在于其<strong>分层合成（layered composition）的范式</strong>。与传统的单一提示词（monolithic prompts）驱动的扩散模型不同，该方法将复杂的编辑意图解耦成多个独立的、可控的视觉层：</p>
<ul>
<li><strong>内容层（Content Layer）</strong>: 定义要生成或修改的内容是什么。</li>
<li><strong>空间层（Spatial Layer）</strong>: 指定内容放置的位置和范围。</li>
<li><strong>结构层（Structural Layer）</strong>: 控制内容的形状和形态。</li>
<li><strong>颜色层（Color Layer）</strong>: 定义内容的调色板和色彩风格。</li>
</ul>
<p>为了实现这一范式，论文提出了以下技术贡献：</p>
<ul>
<li><strong>专门的数据生成管道（Specialized Data Generation Pipeline）</strong>: 用于实现上下文感知的（context-aware）内容集成，这意味着模型能够理解在特定场景下添加新内容时需要考虑周围环境的语义和视觉信息。</li>
<li><strong>统一的控制模块（Unified Control Module）</strong>: 能够处理和融合所有这些不同的视觉线索，将它们有效地转化为生成模型的输入。</li>
<li><strong>精调的空间分支（Fine-tuned Spatial Branch）</strong>: 专门用于实现精确的局部编辑，包括对象移除（object removal），这通常是现有生成模型难以精细控制的方面。</li>
</ul>
<p><strong>3. 对该领域的潜在影响</strong></p>
<p>MagicQuill V2 的潜在影响是深远的，尤其是在以下几个方面：</p>
<ul>
<li><strong>提升生成式AI的可控性与交互性</strong>: 这是当前生成模型面临的最大挑战之一。通过将编辑过程分解为可控的层，用户可以像使用Photoshop等传统工具一样，对生成的内容进行精细的、局部的、多方面的调整，极大地增强了用户对生成结果的掌控力。</li>
<li><strong>弥合语义理解与精细操作的鸿沟</strong>: 扩散模型在理解复杂的语义和生成高质量图像方面表现出色，但缺乏对细节的精细控制。MagicQuill V2 通过引入分层控制，有效地将强大的语义理解能力与像素级的精确操作结合起来，为更复杂的创意工作流程打开了大门。</li>
<li><strong>推动更直观、更高效的图像编辑工具</strong>: 这种分层方法有望催生新一代的图像编辑软件，它们将兼具AI的强大生成能力和传统软件的易用性与精确性，降低创意工作的门槛。</li>
<li><strong>为更复杂的生成任务奠定基础</strong>: 这种解耦和分层控制的思想，可以推广到视频生成、3D模型生成等更复杂的领域，实现更精细化的控制。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用</strong></p>
<ul>
<li><strong>专业图像编辑与设计</strong>: 摄影师、平面设计师、插画师等可以利用MagicQuill V2 进行更高效、更具创意的图像后期处理和创作。</li>
<li><strong>虚拟现实（VR）/增强现实（AR）内容创作</strong>: 在构建沉浸式环境时，需要精确地放置和修改虚拟对象，MagicQuill V2 的空间控制能力将非常有用。</li>
<li><strong>游戏开发</strong>: 游戏美术师可以快速生成和修改游戏中的纹理、角色或场景元素。</li>
<li><strong>电子商务</strong>: 自动生成和修改商品图片，以适应不同的营销需求。</li>
<li><strong>内容生成与个性化</strong>: 根据用户特定的需求和偏好，生成高度定制化的图像。</li>
<li><strong>教育与培训</strong>: 帮助初学者学习图像编辑和设计，通过直观的交互式工具理解概念。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性</strong></p>
<p>尽管摘要描绘了一个令人兴奋的系统，但仍可以推断出一些潜在的局限性：</p>
<ul>
<li><strong>数据依赖性</strong>: 论文提到了“专门的数据生成管道”，这暗示了该方法可能高度依赖于特定类型和质量的数据集来训练其各个模块，特别是上下文感知的集成。如果训练数据不足或存在偏差，可能会影响模型的泛化能力。</li>
<li><strong>计算复杂度</strong>: 引入分层控制和精调的空间分支可能会增加模型的计算复杂度和推理时间，尤其是在处理高分辨率图像或进行实时交互时。</li>
<li><strong>用户界面设计挑战</strong>: 虽然摘要强调了“直观控制”，但如何设计一个真正直观且易于用户理解和操作的多层级控制界面，本身就是一个挑战。用户需要理解每层的作用以及它们之间的交互关系。</li>
<li><strong>“完美”解耦的难度</strong>: 尽管目标是解耦用户意图，但在实际操作中，内容、空间、结构和颜色之间可能存在复杂的相互依赖关系。例如，改变内容可能会影响其合适的空间位置或结构。模型在多层级交互时的鲁棒性仍需验证。</li>
<li><strong>对象移除的局限性</strong>: 虽然提到了对象移除，但摘要并未说明其移除的“干净程度”和对周围环境的“修复能力”。对于复杂背景下的对象移除，可能仍然存在挑战。</li>
<li><strong>对“创意意图”的理解深度</strong>: 尽管分层控制提供了精细化，但模型对用户更深层次的、抽象的创意意图的理解能力，可能仍然受限于其训练数据和模型架构。</li>
</ul>
<p>总而言之，MagicQuill V2 是一项非常有前景的研究，它通过引入创新的分层合成范式，有效地解决了当前生成式AI在图像编辑领域面临的可控性难题，有望对该领域产生重要影响。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose MagicQuill V2, a novel system that introduces a \textbf{layered composition} paradigm to generative image editing, bridging the gap between the semantic power of diffusion models and the granular control of traditional graphics software.</li>
<li>To overcome this, our method deconstructs creative intent into a stack of controllable visual cues: a content layer for what to create, a spatial layer for where to place it, a structural layer for how it is shaped, and a color layer for its palette.</li>
<li>Our technical contributions include a specialized data generation pipeline for context-aware content integration, a unified control module to process all visual cues, and a fine-tuned spatial branch for precise local editing, including object removal.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.03046v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.03046v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.03045v1'></a></p>
<h2 id="cameo-correspondence-attention-alignment-for-multi-view-diffusion-models"><a href="https://arxiv.org/abs/2512.03045v1">CAMEO: Correspondence-Attention Alignment for Multi-View Diffusion Models</a></h2>
<p><strong>Authors:</strong> Minkyung Kwon, Jinhyeok Choi, Jiho Park, Seonghu Jeon, Jinhyuk Jang, Junyoung Seo, Minseop Kwak, Jin-Hwa Kim, Seungryong Kim</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Multi-view diffusion models have recently emerged as a powerful paradigm for novel view synthesis, yet the underlying mechanism that enables their view-consistency remains unclear. In this work, we first verify that the attention maps of these models acquire geometric correspondence throughout training, attending to the geometrically corresponding regions across reference and target views for view-consistent generation. However, this correspondence signal remains incomplete, with its accuracy degrading under large viewpoint changes. Building on these findings, we introduce CAMEO, a simple yet effective training technique that directly supervises attention maps using geometric correspondence to enhance both the training efficiency and generation quality of multi-view diffusion models. Notably, supervising a single attention layer is sufficient to guide the model toward learning precise correspondences, thereby preserving the geometry and structure of reference images, accelerating convergence, and improving novel view synthesis performance. CAMEO reduces the number of training iterations required for convergence by half while achieving superior performance at the same iteration counts. We further demonstrate that CAMEO is model-agnostic and can be applied to any multi-view diffusion model.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“CAMEO: Correspondence-Attention Alignment for Multi-View Diffusion Models”的全面中文摘要：</p>
<p><strong>论文题目：</strong> CAMEO: Correspondence-Attention Alignment for Multi-View Diffusion Models</p>
<p><strong>作者：</strong> Minkyung Kwon, Jinhyeok Choi, Jiho Park, Seonghu Jeon, Jinhyuk Jang, Junyoung Seo, Minseop Kwak, Jin-Hwa Kim, Seungryong Kim</p>
<hr />
<p><strong>摘要：</strong></p>
<p><strong>1. 主要问题/研究问题：</strong>
多视图扩散模型（Multi-view diffusion models）在生成新视图方面展现出强大潜力，但其实现视图一致性的内在机制尚不明确。现有模型在处理大视角变化或复杂几何形状的场景时，视图一致性会下降，导致生成的新视图在几何上不准确。本文旨在深入理解多视图扩散模型如何学习和维持视图一致性，并在此基础上提出一种改进方法。</p>
<p><strong>2. 关键创新/方法贡献：</strong>
作者首先通过分析多视图扩散模型的注意力图，发现模型在训练过程中会自然地学习到跨视图的几何对应关系，并且这种对应关系集中在特定的注意力层。然而，这种自发学习到的对应关系信号是不完整的，在视角变化较大时会失效。</p>
<p>基于此发现，作者提出了 <strong>CAMEO (Correspondence-Attention Alignment)</strong>，一种简单而有效的训练技术。CAMEO 的核心思想是<strong>直接使用几何对应关系来监督模型的注意力图</strong>。具体来说，CAMEO 识别出能够捕捉到最强几何对应关系的特定注意力层（例如，在 CAT3D 模型中是第 10 层），并引入一个 <strong>Correspondence-Attention Alignment Loss (L<sub>CAMEO</sub>)</strong> 来强制模型的注意力图与预先计算的几何对应图对齐。作者发现，仅监督一个注意力层就足以引导模型学习精确的对应关系，从而提升训练效率和生成质量。CAMEO 还引入了一个 MLP 投影头来保留多头注意力的表达能力。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>加速训练：</strong> CAMEO 将多视图扩散模型收敛所需的训练迭代次数减少了一半，实现了 2 倍的加速。
*   <strong>提升生成质量：</strong> 在相同的训练迭代次数下，CAMEO 取得了比基线模型（如 CAT3D）以及其他对比方法（如 REPA、Geometry Forcing）更优的性能，尤其是在 PSNR、SSIM 等指标上。
*   <strong>增强几何一致性：</strong> CAMEO 生成的新视图在几何结构上更加准确，即使在具有挑战性的视角变化场景下也能保持良好的细节和形状。定性结果表明，CAMEO 能够生成更精细、更准确的几何结构，例如在手持栏的生成上表现优异。
*   <strong>模型无关性：</strong> CAMEO 被证明是模型无关的，可以应用于不同的多视图扩散模型架构（如 CAT3D、MVGenMaster、Hunyuan-DiT），并都能带来性能提升。
*   <strong>3D 重建能力：</strong> 通过 CAMEO 生成的视图一致性更强的图像，在后续的 3D 重建任务（如使用 3DGS）中也表现出更好的结果。</p>
<p><strong>4. 论文中提到的局限性：</strong>
*   当参考视图和目标视图之间的视觉重叠极少，或者视角变化非常极端时，建立跨视图的对应关系变得非常困难，CAMEO 的有效性会受到限制。这反映了新视图合成本身的一个根本性挑战。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>超越新视图合成：</strong> 将 CAMEO 的思想扩展到视频扩散、4D 重建或其他多模态任务。
*   <strong>语义对应关系：</strong> 探索其他层是否编码了语义对应关系，并利用几何对齐来进一步提升生成质量和语义理解。</p>
<p><strong>总结：</strong>
CAMEO 论文的核心贡献在于揭示了多视图扩散模型中注意力图学习到的几何对应关系对于视图一致性的关键作用，并提出了一种简单而有效的监督方法来强化这一对应关系。通过直接对注意力图施加几何约束，CAMEO 显著提高了训练效率和新视图合成的质量，同时保持了模型无关性，为多视图生成领域的研究提供了新的思路和有效的工具。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Multi-view diffusion models have recently emerged as a powerful paradigm for novel view synthesis, yet the underlying mechanism that enables their view-consistency remains unclear.</li>
<li>Building on these findings, we introduce CAMEO, a simple yet effective training technique that directly supervises attention maps using geometric correspondence to enhance both the training efficiency and generation quality of multi-view diffusion models.</li>
<li>Notably, supervising a single attention layer is sufficient to guide the model toward learning precise correspondences, thereby preserving the geometry and structure of reference images, accelerating convergence, and improving novel view synthesis performance.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.03045v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.03045v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.03044v1'></a></p>
<h2 id="video2act-a-dual-system-video-diffusion-policy-with-robotic-spatio-motional-modeling"><a href="https://arxiv.org/abs/2512.03044v1">Video2Act: A Dual-System Video Diffusion Policy with Robotic Spatio-Motional Modeling</a></h2>
<p><strong>Authors:</strong> Yueru Jia, Jiaming Liu, Shengbang Liu, Rui Zhou, Wanhe Yu, Yuyang Yan, Xiaowei Chi, Yandong Guo, Boxin Shi, Shanghang Zhang</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Categories:</strong> cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>Robust perception and dynamics modeling are fundamental to real-world robotic policy learning. Recent methods employ video diffusion models (VDMs) to enhance robotic policies, improving their understanding and modeling of the physical world. However, existing approaches overlook the coherent and physically consistent motion representations inherently encoded across frames in VDMs. To this end, we propose Video2Act, a framework that efficiently guides robotic action learning by explicitly integrating spatial and motion-aware representations. Building on the inherent representations of VDMs, we extract foreground boundaries and inter-frame motion variations while filtering out background noise and task-irrelevant biases. These refined representations are then used as additional conditioning inputs to a diffusion transformer (DiT) action head, enabling it to reason about what to manipulate and how to move. To mitigate inference inefficiency, we propose an asynchronous dual-system design, where the VDM functions as the slow System 2 and the DiT head as the fast System 1, working collaboratively to generate adaptive actions. By providing motion-aware conditions to System 1, Video2Act maintains stable manipulation even with low-frequency updates from the VDM. For evaluation, Video2Act surpasses previous state-of-the-art VLA methods by 7.7% in simulation and 21.7% in real-world tasks in terms of average success rate, further exhibiting strong generalization capabilities.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Video2Act: A Dual-System Video Diffusion Policy with Robotic Spatio-Motional Modeling”的全面中文摘要，重点关注其在计算机视觉和机器学习领域的贡献：</p>
<p><strong>论文题目：</strong> Video2Act: A Dual-System Video Diffusion Policy with Robotic Spatio-Motional Modeling</p>
<p><strong>作者：</strong> Yueru Jia, Jiaming Liu, Shengbang Liu, Rui Zhou, Wanhe Yu, Yuyang Yan, Xiaowei Chi, Yandong Guo, Boxin Shi, Shanghang Zhang</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 主要问题/研究问题：</strong></p>
<p>现实世界机器人策略学习的核心在于鲁棒的感知和动力学建模。尽管视频扩散模型（VDMs）在增强机器人策略方面展现出潜力，但现有方法未能充分利用 VDMs 中固有的、跨帧一致的空间和运动表征。这些被忽视的表征包含了关于“操作什么”和“如何移动”的关键信息，而现有方法往往直接使用原始 VDM 特征，未能有效解耦和利用这些有价值的线索，导致策略学习的效率和性能受限。</p>
<p><strong>2. 关键创新/方法贡献：</strong></p>
<p>Video2Act 提出了一种新颖的框架，旨在通过显式地整合从 VDM 中提取的空间和运动感知表征来指导机器人动作学习。其核心贡献包括：</p>
<ul>
<li><strong>精炼的空间与运动表征提取：</strong> Video2Act 创新性地利用 Sobel 算子提取精细的空间结构边界，并结合快速傅里叶变换（FFT）来捕捉跨帧的运动动态。这些操作能够有效过滤背景噪声和任务无关的偏差，得到更干净、更有信息量的表征。</li>
<li><strong>异步双系统设计：</strong> 为了解决 VDM 计算成本高昂的问题，Video2Act 采用异步双系统架构。VDM 被设计为慢速的“系统 2”（感知模块），负责提取低频的空间-运动表征；而一个快速的扩散 Transformer（DiT）动作头则作为“系统 1”（执行模块），负责高频的实时动作生成。这种设计允许 VDM 的表征被周期性地更新并作为条件输入给系统 1，从而在保持实时控制的同时，显著降低了计算开销。</li>
<li><strong>跨注意力融合：</strong> 提取到的空间-运动表征（VDM tokens）与高频图像 tokens 和语言指令（text tokens）通过跨注意力机制有效地融合到 DiT 动作头中，使模型能够全面理解任务目标和执行细节。</li>
<li><strong>系统性分析 VDM 表征：</strong> 论文首先进行了定性分析，通过 Grad-CAM 可视化证明了 VDM 特征比传统的图像编码器（如 DINOv2, SigLIP）更能稳定地关注前景物体，即使在存在机器人自身运动和视角变化的情况下也表现出更强的空间结构和运动一致性。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong></p>
<p>Video2Act 在模拟和真实世界的机器人操作任务中均取得了显著的性能提升：</p>
<ul>
<li><strong>模拟实验：</strong> 在 RoboTwin 模拟基准上，Video2Act 的平均成功率比现有最先进（SOTA）的 VLA 方法高出 7.7%，在四个任务中表现最优。</li>
<li><strong>真实世界实验：</strong> 在 Agilex Cobot Magic 平台上的六个真实世界操作任务中，Video2Act 的平均成功率达到 73.3%，比基线方法（RDT 和 VPP）高出 21.7%，尤其在需要精确空间推理和动态协调的双臂操作任务中表现突出。</li>
<li><strong>泛化能力：</strong> Video2Act 在面对物体变化、背景变化和光照变化等未见过场景时，仍能保持稳定的成功率，证明了其良好的零样本泛化能力。</li>
<li><strong>效率与稳定性：</strong> 异步双系统设计使得 Video2Act 能够在保持高频动作生成的同时，有效利用 VDM 的丰富表征，实现了计算效率和控制精度的良好平衡。</li>
</ul>
<p>这些结果表明，Video2Act 通过显式地提取和利用 VDM 中蕴含的空间-运动信息，能够更准确地理解和建模物理世界，从而实现更鲁棒、更高效的机器人策略学习。</p>
<p><strong>4. 提及的局限性：</strong></p>
<ul>
<li><strong>VDM 的计算成本：</strong> 尽管采用了异步双系统设计，VDM 本身仍然是计算密集型的，其推理速度相对较慢，是系统 2 的瓶颈。</li>
<li><strong>特定任务的挑战：</strong> 在“pick dual flowers”任务中，细小的花茎几何形状导致了初始抓取时的微小位置偏移，这种偏移在后续轨迹中累积，最终导致插入失败。这表明在处理极度精细或易受干扰的几何形状时，仍存在挑战。</li>
<li><strong>硬件精度限制：</strong> 在“push triangle”任务中，模型在微小的硬件精度误差下表现出过冲（overshooting）的问题，导致未能精确完成目标形状。这表明模型对运动终止的精细控制仍有提升空间。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>进一步提升 VDM 推理效率：</strong> 探索更高效的 VDM 模型或蒸馏技术，以进一步降低系统 2 的计算负担。</li>
<li><strong>增强对精细几何和运动终止的控制：</strong> 通过增加高质量演示数据、引入更严格的训练约束，或开发更精细的运动终止策略来解决在处理精细几何和精确运动控制方面的局限性。</li>
<li><strong>自主检测和纠正错误：</strong> 使系统 2 能够自主检测和纠正系统 1 在执行过程中出现的错误动作，从而进一步提高系统的鲁棒性和自适应能力。</li>
<li><strong>更广泛的泛化能力：</strong> 探索将 Video2Act 应用于更复杂、更多样化的操作任务和环境，以验证其在更广泛场景下的泛化潜力。</li>
</ul>
<p><strong>在计算机视觉领域的意义：</strong></p>
<p>Video2Act 的工作在计算机视觉领域具有重要意义，它展示了如何有效地从视频扩散模型中提取和利用<strong>高层次的空间和运动表征</strong>，以解决机器人操作这一复杂且具有挑战性的下游任务。论文的贡献在于：</p>
<ul>
<li><strong>深化对 VDM 表征的理解：</strong> 通过定性分析，揭示了 VDM 在机器人场景下捕捉稳定、对象中心化表征的能力，为理解和应用 VDM 提供了新的视角。</li>
<li><strong>提出有效的表征提取方法：</strong> Sobel 和 FFT 的结合提供了一种新颖且有效的方式来解耦和精炼 VDM 中的空间和运动信息，这对于需要精确感知和动态理解的任务至关重要。</li>
<li><strong>推动 VLA 模型的发展：</strong> 通过将这些精炼的表征作为条件输入，Video2Act 显著提升了 VLA 模型在复杂操作任务中的性能，为构建更智能、更通用的机器人助手开辟了道路。</li>
<li><strong>异步双系统架构的范例：</strong> 论文提出的异步双系统设计为处理计算密集型模型（如 VDM）在实时机器人应用中的集成提供了一个有效的解决方案，平衡了性能和效率。</li>
</ul>
<p>总而言之，Video2Act 是一项重要的研究工作，它不仅在机器人策略学习领域取得了 SOTA 成果，而且为计算机视觉模型（特别是 VDM）在机器人领域的应用提供了新的思路和方法，突显了从视频中提取结构化、动态信息的重要性。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>However, existing approaches overlook the coherent and physically consistent motion representations inherently encoded across frames in VDMs. To this end, we propose Video2Act, a framework that efficiently guides robotic action learning by explicitly integrating spatial and motion-aware representations.</li>
<li>To mitigate inference inefficiency, we propose an asynchronous dual-system design, where the VDM functions as the slow System 2 and the DiT head as the fast System 1, working collaboratively to generate adaptive actions.</li>
<li>For evaluation, Video2Act surpasses previous state-of-the-art VLA methods by 7.7% in simulation and 21.7% in real-world tasks in terms of average success rate, further exhibiting strong generalization capabilities.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.03044v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.03044v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.03043v1'></a></p>
<h2 id="onethinker-all-in-one-reasoning-model-for-image-and-video"><a href="https://arxiv.org/abs/2512.03043v1">OneThinker: All-in-one Reasoning Model for Image and Video</a></h2>
<p><strong>Authors:</strong> Kaituo Feng, Manyuan Zhang, Hongyu Li, Kaixuan Fan, Shuang Chen, Yilei Jiang, Dian Zheng, Peiwen Sun, Yiyuan Zhang, Haoze Sun, Yan Feng, Peng Pei, Xunliang Cai, Xiangyu Yue</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Reinforcement learning (RL) has recently achieved remarkable success in eliciting visual reasoning within Multimodal Large Language Models (MLLMs). However, existing approaches typically train separate models for different tasks and treat image and video reasoning as disjoint domains. This results in limited scalability toward a multimodal reasoning generalist, which restricts practical versatility and hinders potential knowledge sharing across tasks and modalities. To this end, we propose OneThinker, an all-in-one reasoning model that unifies image and video understanding across diverse fundamental visual tasks, including question answering, captioning, spatial and temporal grounding, tracking, and segmentation. To achieve this, we construct the OneThinker-600k training corpus covering all these tasks and employ commercial models for CoT annotation, resulting in OneThinker-SFT-340k for SFT cold start. Furthermore, we propose EMA-GRPO to handle reward heterogeneity in multi-task RL by tracking task-wise moving averages of reward standard deviations for balanced optimization. Extensive experiments on diverse visual benchmarks show that OneThinker delivers strong performance on 31 benchmarks, across 10 fundamental visual understanding tasks. Moreover, it exhibits effective knowledge transfer between certain tasks and preliminary zero-shot generalization ability, marking a step toward a unified multimodal reasoning generalist. All code, model, and data are released.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“OneThinker: All-in-one Reasoning Model for Image and Video”的全面中文摘要，重点关注其在计算机视觉领域的新颖性和重要性：</p>
<p><strong>论文题目：</strong> OneThinker: All-in-one Reasoning Model for Image and Video</p>
<p><strong>作者：</strong> Kaituo Feng, Manyuan Zhang, Hongyu Li, Kaixuan Fan, Shuang Chen, Yilei Jiang, Dian Zheng, Peiwen Sun, Yiyuan Zhang, Haoze Sun, Yan Feng, Peng Pei, Xunliang Cai, Xiangyu Yue</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题与动机：</strong>
当前，多模态大语言模型（MLLMs）在视觉推理方面取得了显著进展，但现有方法通常为不同任务训练独立的模型，并将图像和视频推理视为孤立的领域。这种方法限制了模型向多模态推理通才发展的可扩展性，降低了其实际通用性，并阻碍了跨任务和跨模态知识共享的潜力。因此，研究的核心问题在于：<strong>能否训练一个统一的多模态推理通才模型，使其能够同时处理图像和视频，并涵盖广泛的基础视觉任务？</strong></p>
<p><strong>2. 主要创新与方法贡献：</strong>
为了解决上述问题，论文提出了 <strong>OneThinker</strong>，一个统一的、全能型的视觉推理模型，能够处理图像和视频的多种基础视觉任务，包括：<strong>问答（QA）、字幕生成（Captioning）、空间和时间定位（Spatial and Temporal Grounding）、目标跟踪（Tracking）和图像/视频分割（Segmentation）</strong>。</p>
<p>其关键创新和方法贡献包括：</p>
<ul>
<li><strong>统一的多模态推理框架：</strong> OneThinker是第一个旨在统一图像和视频理解，并涵盖如此广泛基础视觉任务的单一模型。</li>
<li><strong>大规模数据集构建：</strong> 论文构建了 <strong>OneThinker-600k</strong> 数据集，包含约60万个多模态样本，覆盖了上述所有基础视觉任务。此外，还构建了 <strong>OneThinker-SFT-340k</strong> 数据集，用于SFT（监督微调）的冷启动。</li>
<li><strong>EMA-GRPO 算法：</strong> 针对多任务强化学习中不同任务奖励异质性带来的不平衡问题，论文提出了 <strong>EMA-GRPO (Exponential Moving Average Group Relative Policy Optimization)</strong> 算法。该算法通过追踪任务级别的奖励标准差的指数移动平均值，实现自适应的奖励归一化，从而平衡了<strong>任务内不平衡（intra-task imbalance）</strong>和<strong>任务间不平衡（inter-task imbalance）</strong>，确保了稳定且均衡的优化。</li>
<li><strong>统一的文本接口：</strong> 所有任务都通过统一的文本接口进行处理，模型在 <code>&lt;think&gt;</code> 和 <code>&lt;/think&gt;</code> 标签内生成推理过程，并在 <code>&lt;answer&gt;</code> 和 <code>&lt;/answer&gt;</code> 标签内输出任务特定的结果。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong>
通过在31个基准测试、10个基础视觉理解任务上的广泛实验，OneThinker取得了以下显著成果：</p>
<ul>
<li><strong>卓越的性能表现：</strong> OneThinker在多个图像和视频问答、字幕生成、空间/时间定位、跟踪和分割任务上均取得了<strong>最先进（state-of-the-art）的性能</strong>，显著优于现有的模型。例如，在图像QA任务上，OneThinker-8B在MMMU上达到70.6%的准确率；在视频QA任务上，在LongVideo-Reason上获得79.2%的得分。</li>
<li><strong>有效的知识迁移：</strong> 模型展示了在某些任务之间<strong>有效的知识迁移能力</strong>，表明统一训练框架促进了不同任务之间推理技能的共享。</li>
<li><strong>初步的零样本泛化能力：</strong> OneThinker表现出<strong>初步的零样本泛化能力</strong>，能够在未见过的任务上取得有竞争力的结果，这标志着其向通用多模态推理迈出了重要一步。</li>
<li><strong>模型的可扩展性与通用性：</strong> 论文证明了构建一个能够处理多样化视觉任务的通用模型是可行的，这为未来开发更强大的多模态AI系统奠定了基础。</li>
</ul>
<p><strong>4. 局限性：</strong>
论文中提到了一些潜在的局限性：</p>
<ul>
<li><strong>零样本泛化能力仍需提升：</strong> 虽然模型展示了初步的零样本泛化能力，但其在“unseen tasks”上的表现仍有提升空间，尤其是在更复杂或更具挑战性的新任务上。</li>
<li><strong>计算资源需求：</strong> 训练如此大规模的统一模型需要大量的计算资源（如论文中提到的32个NVIDIA H800 GPU）。</li>
<li><strong>部分任务的奖励设计：</strong> 对于某些任务（如视频分割），由于计算延迟，论文省略了基于掩码的奖励，这可能影响了该任务的优化效果。</li>
</ul>
<p><strong>5. 未来研究方向：</strong>
基于OneThinker的成功，未来的研究可以从以下几个方面展开：</p>
<ul>
<li><strong>进一步提升零样本泛化能力：</strong> 探索更有效的跨任务和跨模态学习机制，以增强模型在全新任务上的适应性。</li>
<li><strong>更精细的奖励设计与优化：</strong> 针对不同任务的特点，设计更精细的奖励函数，并探索更高效的强化学习优化策略。</li>
<li><strong>模型效率与部署：</strong> 研究如何提高模型的训练和推理效率，使其更容易部署到实际应用中。</li>
<li><strong>更广泛的任务覆盖：</strong> 将OneThinker的能力扩展到更多样的视觉理解任务，例如视觉推理、视觉对话、视觉常识推理等。</li>
<li><strong>可解释性增强：</strong> 进一步研究模型内部的推理过程，提高其决策的可解释性。</li>
</ul>
<p><strong>总结：</strong></p>
<p>OneThinker论文的核心贡献在于提出了一个<strong>首创的、统一的多模态推理模型</strong>，成功地将图像和视频的多种基础视觉任务整合到一个单一框架下。通过构建大规模数据集和创新的EMA-GRPO算法，OneThinker在多个基准测试中取得了<strong>卓越的性能</strong>，并展示了<strong>有效的知识迁移和初步的零样本泛化能力</strong>。这标志着在构建通用多模态推理AI方面迈出了重要一步，为未来更强大、更通用的视觉智能系统奠定了坚实的基础。论文的开放代码、模型和数据也为社区的研究提供了宝贵的资源。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To this end, we propose OneThinker, an all-in-one reasoning model that unifies image and video understanding across diverse fundamental visual tasks, including question answering, captioning, spatial and temporal grounding, tracking, and segmentation.</li>
<li>Furthermore, we propose EMA-GRPO to handle reward heterogeneity in multi-task RL by tracking task-wise moving averages of reward standard deviations for balanced optimization.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.03043v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.03043v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.03041v1'></a></p>
<h2 id="multishotmaster-a-controllable-multi-shot-video-generation-framework"><a href="https://arxiv.org/abs/2512.03041v1">MultiShotMaster: A Controllable Multi-Shot Video Generation Framework</a></h2>
<p><strong>Authors:</strong> Qinghe Wang, Xiaoyu Shi, Baolu Li, Weikang Bian, Quande Liu, Huchuan Lu, Xintao Wang, Pengfei Wan, Kun Gai, Xu Jia</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Current video generation techniques excel at single-shot clips but struggle to produce narrative multi-shot videos, which require flexible shot arrangement, coherent narrative, and controllability beyond text prompts. To tackle these challenges, we propose MultiShotMaster, a framework for highly controllable multi-shot video generation. We extend a pretrained single-shot model by integrating two novel variants of RoPE. First, we introduce Multi-Shot Narrative RoPE, which applies explicit phase shift at shot transitions, enabling flexible shot arrangement while preserving the temporal narrative order. Second, we design Spatiotemporal Position-Aware RoPE to incorporate reference tokens and grounding signals, enabling spatiotemporal-grounded reference injection. In addition, to overcome data scarcity, we establish an automated data annotation pipeline to extract multi-shot videos, captions, cross-shot grounding signals and reference images. Our framework leverages the intrinsic architectural properties to support multi-shot video generation, featuring text-driven inter-shot consistency, customized subject with motion control, and background-driven customized scene. Both shot count and duration are flexibly configurable. Extensive experiments demonstrate the superior performance and outstanding controllability of our framework.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“MultiShotMaster: A Controllable Multi-Shot Video Generation Framework”的全面中文摘要：</p>
<p><strong>论文题目：</strong> MultiShotMaster: A Controllable Multi-Shot Video Generation Framework</p>
<p><strong>作者：</strong> Qinghe Wang, Xiaoyu Shi, Baolu Li, Weikang Bian, Quande Liu, Huchuan Lu, Xintao Wang, Pengfei Wan, Kun Gai, Xu Jia</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
当前视频生成技术在生成单镜头短视频方面表现出色，但在生成需要灵活镜头编排、连贯叙事以及超越文本提示的控制能力的多镜头叙事视频方面存在显著不足。这阻碍了其在实际视频内容创作中的应用。</p>
<p><strong>2. 主要创新与方法贡献：</strong>
为了解决上述挑战，作者提出了 <strong>MultiShotMaster</strong>，一个高度可控的多镜头视频生成框架。其核心创新在于对预训练的单镜头模型进行了两项关键的 RoPE (Rotary Position Embedding) 变体改进：</p>
<ul>
<li><strong>Multi-Shot Narrative RoPE：</strong> 该方法在镜头过渡处引入了显式的相位偏移，使得模型能够识别镜头边界，从而实现灵活的镜头编排，同时保持时间叙事顺序。</li>
<li><strong>Spatiotemporal Position-Aware RoPE：</strong> 该方法将参考信息（如主体和背景）及其时空位置信号整合到 RoPE 中，实现了时空感知的参考注入。这使得用户能够通过提供参考图像（主体、背景）和地面信息来精确控制视频内容，实现主体运动和背景的定制化生成。</li>
</ul>
<p>此外，为了克服数据稀缺问题，作者还建立了一个自动化的数据标注流程，用于提取多镜头视频、字幕、跨镜头地面信息和参考图像。该框架利用了现有模型架构的内在属性，实现了文本驱动的镜头间一致性、可定制的主体运动控制以及背景驱动的场景定制。</p>
<p><strong>3. 主要结果与意义：</strong>
通过大量的实验评估，MultiShotMaster 在多镜头视频生成方面展现出优越的性能和出色的可控性。具体而言：</p>
<ul>
<li><strong>高度可控性：</strong> 用户可以灵活配置镜头数量和时长，并通过文本提示、主体图像、地面信息和背景图像来精细控制视频内容。</li>
<li><strong>镜头间一致性：</strong> 框架能够有效保持跨镜头的叙事连贯性和场景一致性。</li>
<li><strong>参考注入能力：</strong> 能够实现时空感知的参考注入，精确控制主体在视频中的出现位置和运动。</li>
<li><strong>自动化数据处理：</strong> 提出的数据标注流程为多镜头视频生成提供了必要的数据支持。</li>
</ul>
<p>该框架的意义在于，它为生成更具叙事性和艺术性的多镜头视频内容提供了强大的工具，弥合了当前视频生成技术与实际内容创作之间的差距，并为未来的研究开辟了新的方向。</p>
<p><strong>4. 提及的局限性：</strong>
论文中提到了一些局限性：</p>
<ul>
<li><strong>模型规模：</strong> 作者在分辨率为 384x672 的 ~1B 参数的预训练单镜头 T2V 模型上进行了实验，这与当前一些基线模型（如 WAN 系列）使用的 480x832 分辨率相比，生成质量仍有提升空间。</li>
<li><strong>相机控制与主体运动耦合：</strong> 作者目前仅显式控制主体运动，而相机位置由文本提示控制。这可能导致相机和物体运动耦合，使得生成的视频在遵循地面信息的同时，也受到相机运动的影响。</li>
</ul>
<p><strong>5. 未来研究方向：</strong>
基于上述局限性，作者提出了未来的研究方向：</p>
<ul>
<li><strong>提升模型规模和生成质量：</strong> 将框架应用于更大规模的模型（如 WAN 2.1/2.2）并发布代码，以进一步提升生成视频的质量。</li>
<li><strong>解耦相机控制与主体运动：</strong> 探索更精细的控制机制，以解耦相机运动和主体运动，实现更自由的视频创作。</li>
</ul>
<p>总而言之，MultiShotMaster 是一个在多镜头视频生成领域具有开创性的工作，它通过创新的 RoPE 变体和数据处理方法，显著提升了视频生成的可控性和叙事能力，为未来多镜头视频生成的研究和应用奠定了坚实的基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To tackle these challenges, we propose MultiShotMaster, a framework for highly controllable multi-shot video generation.</li>
<li>We extend a pretrained single-shot model by integrating two novel variants of RoPE.</li>
<li>First, we introduce Multi-Shot Narrative RoPE, which applies explicit phase shift at shot transitions, enabling flexible shot arrangement while preserving the temporal narrative order.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.03041v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.03041v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.03040v1'></a></p>
<h2 id="video4spatial-towards-visuospatial-intelligence-with-context-guided-video-generation"><a href="https://arxiv.org/abs/2512.03040v1">Video4Spatial: Towards Visuospatial Intelligence with Context-Guided Video Generation</a></h2>
<p><strong>Authors:</strong> Zeqi Xiao, Yiwei Zhao, Lingxiao Li, Yushi Lan, Yu Ning, Rahul Garg, Roshni Cooper, Mohammad H. Taghavi, Xingang Pan</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>We investigate whether video generative models can exhibit visuospatial intelligence, a capability central to human cognition, using only visual data. To this end, we present Video4Spatial, a framework showing that video diffusion models conditioned solely on video-based scene context can perform complex spatial tasks. We validate on two tasks: scene navigation - following camera-pose instructions while remaining consistent with 3D geometry of the scene, and object grounding - which requires semantic localization, instruction following, and planning. Both tasks use video-only inputs, without auxiliary modalities such as depth or poses. With simple yet effective design choices in the framework and data curation, Video4Spatial demonstrates strong spatial understanding from video context: it plans navigation and grounds target objects end-to-end, follows camera-pose instructions while maintaining spatial consistency, and generalizes to long contexts and out-of-domain environments. Taken together, these results advance video generative models toward general visuospatial reasoning.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Video4Spatial: Towards Visuospatial Intelligence with Context-Guided Video Generation”的全面摘要，包含您要求的各个部分：</p>
<p><strong>论文题目：</strong> Video4Spatial: Towards Visuospatial Intelligence with Context-Guided Video Generation</p>
<p><strong>作者：</strong> Zeqi Xiao, Yiwei Zhao, Lingxiao Li, Yushi Lan, Yu Ning, Rahul Garg, Roshni Cooper, Mohammad H. Taghavi, Xingang Pan</p>
<hr />
<p><strong>全面摘要</strong></p>
<p><strong>1. 主要问题/研究问题：</strong>
该论文的核心研究问题是：视频生成模型是否能够仅凭视觉数据展现出人类所具备的“视觉空间智能”（visuospatial intelligence）？具体来说，研究者们旨在探索视频生成模型在仅依赖视频场景上下文信息的情况下，能否执行复杂的空间任务，而无需额外的模态信息（如深度图或相机位姿）。</p>
<p><strong>2. 关键创新点/方法论贡献：</strong>
*   <strong>Video4Spatial 框架：</strong> 提出了一种名为 Video4Spatial 的新框架，该框架利用视频扩散模型，并仅以视频场景上下文作为条件进行训练和推理。
*   <strong>端到端空间任务执行：</strong> 该框架能够端到端地执行两种复杂的空间任务：
    *   <strong>基于视频的场景导航 (Scene Navigation)：</strong> 模型能够遵循相机位姿指令，生成在保持场景三维几何一致性的同时，符合指定相机轨迹的视频。
    *   <strong>基于视频的目标定位 (Object Grounding)：</strong> 模型能够理解文本指令，进行语义定位、指令跟随和规划，最终在生成的视频中定位并突出显示目标对象。
*   <strong>纯视觉输入：</strong> 关键在于，Video4Spatial 完全依赖于原始的 RGB 视频序列，不依赖于任何辅助的 3D 信号（如深度图、点云或显式的相机位姿）。
*   <strong>关键设计选择：</strong>
    *   <strong>联合分类器无关的引导 (Joint Classifier-Free Guidance - CFG)：</strong> 结合了对视频上下文和指令的 CFG，显著提高了生成视频的上下文连贯性。
    *   <strong>辅助边界框 (Auxiliary Bounding Box)：</strong> 为目标定位任务引入了辅助边界框的监督信号，作为一种显式的推理先验，显著提高了定位的准确性。
    *   <strong>非连续上下文采样 (Non-contiguous Context Sampling)：</strong> 通过对上下文视频进行稀疏采样，减少冗余，提高信息含量，同时利用 RoPE 保持时间连贯性，并增强了模型在推理时外推到更长上下文的能力。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>强大的空间理解能力：</strong> Video4Spatial 在仅使用视频上下文的情况下，展现出了强大的空间理解能力，能够推断场景的三维结构，并保持几何一致性。
*   <strong>优于现有方法：</strong> 在目标定位和场景导航任务上，Video4Spatial 取得了与现有最先进方法（包括依赖外部 3D 信息的模型）相当甚至更好的性能，尤其在视觉质量 (IQ) 和相机可控性方面表现突出。
*   <strong>泛化能力：</strong> 模型不仅在训练的室内场景中表现出色，还能很好地泛化到训练时未见过的室外场景（如公园），并能识别训练时未见过的物体类别。
*   <strong>长上下文外推：</strong> 即使在较短的上下文长度下进行训练，模型也能在推理时成功地外推到更长的上下文窗口，进一步提升性能。
*   <strong>推动视频生成模型向通用视觉空间推理发展：</strong> 该研究表明，视频生成模型有潜力发展出通用性的视觉空间推理能力，这是迈向更高级人工智能的关键一步。</p>
<p><strong>4. 论文中提到的局限性：</strong>
*   <strong>分辨率限制：</strong> 当前模型在 416x256 的分辨率下运行，这是由于缺乏上下文压缩技术。这限制了生成视频的视觉保真度。
*   <strong>伪影问题：</strong> 模型仍然会产生一些伪影，例如时间上的不连续性，以及在长尾类别上的不准确的物体定位。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>提高分辨率：</strong> 利用上下文压缩技术，在更高分辨率下实现更精细的视觉保真度。
*   <strong>改进时间建模和数据增强：</strong> 探索更强的时序建模技术，并进行有针对性的数据增强，以减少时间不连续性和改进长尾类别的定位。
*   <strong>改进定位目标：</strong> 开发更优的物体定位目标函数，以进一步提高定位的准确性。
*   <strong>扩展到复杂动态环境：</strong> 将框架扩展到更复杂的动态环境，而不仅仅是相对静态的场景。
*   <strong>更广泛的任务范围：</strong> 将该框架应用于更广泛的视觉空间推理任务。</p>
<p><strong>总结：</strong>
Video4Spatial 论文提出了一种新颖的视频生成框架，成功地展示了视频扩散模型仅凭视频上下文即可实现复杂的视觉空间推理能力。通过创新的设计选择，如联合 CFG、辅助边界框和非连续上下文采样，该模型在场景导航和目标定位任务上取得了显著成果，并且展现了良好的泛化能力。这项工作为视频生成模型在理解和与三维空间交互方面开辟了新的可能性，是迈向更通用人工智能的重要一步。论文也指出了当前方法的局限性，并为未来的研究提供了明确的方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To this end, we present Video4Spatial, a framework showing that video diffusion models conditioned solely on video-based scene context can perform complex spatial tasks.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.03040v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.03040v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.03034v1'></a></p>
<h2 id="mavid-a-multimodal-framework-for-audio-visual-dialogue-understanding-and-generation"><a href="https://arxiv.org/abs/2512.03034v1">MAViD: A Multimodal Framework for Audio-Visual Dialogue Understanding and Generation</a></h2>
<p><strong>Authors:</strong> Youxin Pang, Jiajun Liu, Lingfeng Tan, Yong Zhang, Feng Gao, Xiang Deng, Zhuoliang Kang, Xiaoming Wei, Yebin Liu</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We propose MAViD, a novel Multimodal framework for Audio-Visual Dialogue understanding and generation. Existing approaches primarily focus on non-interactive systems and are limited to producing constrained and unnatural human speech.The primary challenge of this task lies in effectively integrating understanding and generation capabilities, as well as achieving seamless multimodal audio-video fusion. To solve these problems, we propose a Conductor-Creator architecture that divides the dialogue system into two primary components.The Conductor is tasked with understanding, reasoning, and generating instructions by breaking them down into motion and speech components, thereby enabling fine-grained control over interactions. The Creator then delivers interactive responses based on these instructions.Furthermore, to address the difficulty of generating long videos with consistent identity, timbre, and tone using dual DiT structures, the Creator adopts a structure that combines autoregressive (AR) and diffusion models. The AR model is responsible for audio generation, while the diffusion model ensures high-quality video generation.Additionally, we propose a novel fusion module to enhance connections between contextually consecutive clips and modalities, enabling synchronized long-duration audio-visual content generation.Extensive experiments demonstrate that our framework can generate vivid and contextually coherent long-duration dialogue interactions and accurately interpret users' multimodal queries.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“MAViD: A Multimodal Framework for Audio-Visual Dialogue Understanding and Generation”的全面中文摘要，重点关注其在计算机视觉和机器学习领域的贡献：</p>
<p><strong>论文题目：</strong> MAViD: A Multimodal Framework for Audio-Visual Dialogue Understanding and Generation</p>
<p><strong>作者：</strong> Youxin Pang, Jiajun Liu, Lingfeng Tan, Yong Zhang, Feng Gao, Xiang Deng, Zhuoliang Kang, Xiaoming Wei, Yebin Liu</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong></p>
<p>该论文旨在解决当前多模态（文本、音频、视频）对话理解与生成领域存在的关键挑战。现有方法主要集中在非交互式系统，生成的语音受限且不自然。核心难题在于如何有效地整合理解与生成能力，并实现无缝的多模态音视频融合，尤其是在生成长时序、身份、音色和语调一致的视频方面。</p>
<p><strong>2. 主要创新与方法贡献：</strong></p>
<ul>
<li><strong>Conductor-Creator 架构：</strong> 论文提出了一个新颖的 Conductor-Creator 两阶段架构。<ul>
<li><strong>Conductor（理解与指令生成）：</strong> 负责理解用户的多模态输入（文本、音频、视频），并生成全局文本指令。关键创新在于将这些指令进一步细分为<strong>语音指令</strong>（提供听觉线索）和<strong>运动指令</strong>（提供环境和上下文的视觉线索），从而实现对交互的精细化控制，提升生成内容的真实感和人性化。</li>
<li><strong>Creator（音视频内容生成）：</strong> 负责将 Conductor 生成的指令转化为用户可识别的交互信息。为了解决长视频生成中身份、音色和语调一致性的难题，Creator 结合了<strong>自回归（AR）模型</strong>（擅长长序列和多模态建模）和<strong>扩散模型</strong>（保证高质量视觉生成）。</li>
</ul>
</li>
<li><strong>新颖的融合模块：</strong> 为了增强 AR 和扩散模型之间以及连续片段和模态之间的联系，论文设计了一个<strong>专门的融合注意力模块</strong>，以实现同步的长时序音视频内容生成。</li>
<li><strong>长时序生成能力：</strong> MAViD 能够一次性生成约 30 秒的视频，显著优于其他只能生成 5 秒片段的 DiT（Diffusion Transformer）方法，有效解决了长视频生成中的一致性问题。</li>
<li><strong>通用环境声音建模：</strong> 该框架还能模拟背景噪音等通用环境声音，进一步增强了生成内容的真实感。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong></p>
<ul>
<li><strong>实验结果：</strong> 论文通过广泛的实验证明，MAViD 能够生成生动且上下文连贯的长时序对话交互，并能准确理解用户的多模态查询。在定量评估中，MAViD 在音频质量、视频质量和音视频一致性方面均取得了优异的性能，尤其是在长视频生成方面，其一致性表现优于现有方法。</li>
<li><strong>研究意义：</strong> MAViD 的提出标志着在多模态音视频对话理解与生成领域取得了重要进展。它不仅提升了生成内容的真实感和流畅度，还为构建更智能、更具交互性的数字人代理奠定了坚实的基础。其 Conductor-Creator 架构和 AR-扩散结合的 Creator 模型为解决长时序、高保真多模态生成问题提供了新的思路。</li>
</ul>
<p><strong>4. 提及的局限性：</strong></p>
<ul>
<li><strong>生成内容中的细微变化：</strong> 论文提到，由于 AR 模型累积误差，即使是长时序生成，内容也可能出现“温和且轻微的变化”，这与 OVI 方法中因未建模历史片段而导致的“突兀的音频变化”形成对比，但仍表明存在进一步优化的空间。</li>
<li><strong>图像质量的轻微下降：</strong> 在追求更强的动态表现时，图像质量可能出现轻微下降。</li>
<li><strong>对参考图像的依赖（可选）：</strong> 在生成第一帧视频时，如果提供了参考图像，模型会将其作为指导，这表明在某些情况下，模型仍需要外部的视觉提示。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>进一步提升视频质量：</strong> 尽管 MAViD 在视觉质量上表现良好，但论文也提到在动态表现增强时，图像质量有所下降，这暗示了在保持高动态性的同时进一步提升视觉保真度的可能性。</li>
<li><strong>更广泛的模态整合：</strong> 论文提到 AR 模型为整合更多模态（如人体姿态和相机信息）提供了基础，这为未来扩展模型能力指明了方向。</li>
<li><strong>更精细的交互控制：</strong> 虽然 Conductor 已经实现了语音和运动指令的分离，但未来可以探索更精细的指令控制，以实现更复杂和多样化的交互行为。</li>
<li><strong>实时性与效率：</strong> 尽管 MAViD 在长视频生成方面取得了突破，但对于需要实时交互的应用，进一步优化生成效率仍是重要的研究方向。</li>
</ul>
<p>总而言之，MAViD 是一个在多模态音视频对话理解与生成领域具有开创性的工作，它通过创新的架构和模型设计，有效解决了长时序生成中的一致性问题，并显著提升了生成内容的真实感和人性化，为构建更智能的虚拟助手和数字人代理提供了强大的技术支撑。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose MAViD, a novel Multimodal framework for Audio-Visual Dialogue understanding and generation.</li>
<li>To solve these problems, we propose a Conductor-Creator architecture that divides the dialogue system into two primary components.The Conductor is tasked with understanding, reasoning, and generating instructions by breaking them down into motion and speech components, thereby enabling fine-grained control over interactions.</li>
<li>The AR model is responsible for audio generation, while the diffusion model ensures high-quality video generation.Additionally, we propose a novel fusion module to enhance connections between contextually consecutive clips and modalities, enabling synchronized long-duration audio-visual content generation.Extensive experiments demonstrate that our framework can generate vivid and contextually coherent long-duration dialogue interactions and accurately interpret users' multimodal queries.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.03034v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.03034v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.03014v1'></a></p>
<h2 id="instant-video-models-universal-adapters-for-stabilizing-image-based-networks"><a href="https://arxiv.org/abs/2512.03014v1">Instant Video Models: Universal Adapters for Stabilizing Image-Based Networks</a></h2>
<p><strong>Authors:</strong> Matthew Dutson, Nathan Labiosa, Yin Li, Mohit Gupta</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>When applied sequentially to video, frame-based networks often exhibit temporal inconsistency - for example, outputs that flicker between frames. This problem is amplified when the network inputs contain time-varying corruptions. In this work, we introduce a general approach for adapting frame-based models for stable and robust inference on video. We describe a class of stability adapters that can be inserted into virtually any architecture and a resource-efficient training process that can be performed with a frozen base network. We introduce a unified conceptual framework for describing temporal stability and corruption robustness, centered on a proposed accuracy-stability-robustness loss. By analyzing the theoretical properties of this loss, we identify the conditions where it produces well-behaved stabilizer training. Our experiments validate our approach on several vision tasks including denoising (NAFNet), image enhancement (HDRNet), monocular depth (Depth Anything v2), and semantic segmentation (DeepLabv3+). Our method improves temporal stability and robustness against a range of image corruptions (including compression artifacts, noise, and adverse weather), while preserving or improving the quality of predictions.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为一名计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行深入分析。</p>
<p><strong>论文摘要分析：</strong></p>
<p><strong>Title:</strong> Instant Video Models: Universal Adapters for Stabilizing Image-Based Networks
<strong>Authors:</strong> Matthew Dutson, Nathan Labiosa, Yin Li, Mohit Gupta
<strong>Categories:</strong> cs.CV
<strong>Published Date:</strong> 2025-12-02</p>
<p><strong>Abstract:</strong>
When applied sequentially to video, frame-based networks often exhibit temporal inconsistency - for example, outputs that flicker between frames. This problem is amplified when the network inputs contain time-varying corruptions. In this work, we introduce a general approach for adapting frame-based models for stable and robust inference on video. We describe a class of stability adapters that can be inserted into virtually any architecture and a resource-efficient training process that can be performed with a frozen base network. We introduce a unified conceptual framework for describing temporal stability and corruption robustness, centered on a proposed accuracy-stability-robustness loss. By analyzing the theoretical properties of this loss, we identify the conditions where it produces well-behaved stabilizer training. Our experiments validate our approach on several vision tasks including denoising (NAFNet), image enhancement (HDRNet), monocular depth (Depth Anything v2), and semantic segmentation (DeepLabv3+). Our method improves temporal stability and robustness against a range of image corruptions (including compression artifacts, noise, and adverse weather), while preserving or improving the quality of predictions.</p>
<hr />
<p><strong>我的分析如下：</strong></p>
<p><strong>1. 论文的主要贡献（2-3句话的简洁总结）：</strong></p>
<p>该论文提出了一种通用的“稳定性适配器”（stability adapters）方法，能够有效地将现有的基于帧的图像处理网络适配到视频处理任务中，显著提升输出的<strong>时间一致性</strong>和对<strong>时变腐蚀</strong>的鲁棒性。其核心在于一个新颖的“准确性-稳定性-鲁棒性”（accuracy-stability-robustness）联合损失函数，并支持在冻结基础网络的情况下进行高效训练。</p>
<p><strong>2. 关键创新或方法论：</strong></p>
<ul>
<li><strong>通用稳定性适配器（Universal Stability Adapters）：</strong> 这是最核心的创新点。论文提出了一种模块化的适配器，可以插入到几乎任何现有的基于帧的网络架构中。这意味着研究者无需从头开始设计视频模型，而是可以利用现有成熟的图像模型，通过添加这个适配器来获得视频处理能力。</li>
<li><strong>资源高效的训练过程（Resource-efficient Training）：</strong> 适配器可以在<strong>冻结基础网络</strong>的情况下进行训练。这极大地降低了训练成本和计算资源需求，使得将大量现有图像模型快速适配到视频任务成为可能。</li>
<li><strong>统一的准确性-稳定性-鲁棒性损失函数（Accuracy-Stability-Robustness Loss）：</strong> 论文提出了一个统一的框架来衡量和优化模型的准确性、时间稳定性和对腐蚀的鲁棒性。通过理论分析该损失函数的性质，找到了能够有效训练稳定化器的条件。这提供了一个更全面、更系统的优化目标，而不仅仅是关注单帧的准确性。</li>
</ul>
<p><strong>3. 对该领域的潜在影响：</strong></p>
<ul>
<li><strong>降低视频模型开发的门槛：</strong> 极大地简化了将现有强大的图像模型应用于视频任务的过程。研究者和开发者可以更快地利用成熟的图像模型来解决视频相关问题，而无需深入研究复杂的视频模型设计。</li>
<li><strong>提升视频处理的通用性和鲁棒性：</strong> 使得许多原本只适用于静态图像的任务（如图像增强、深度估计、语义分割）能够以更稳定、更可靠的方式应用于动态视频，尤其是在存在噪声、压缩伪影或恶劣天气等实际场景下。</li>
<li><strong>促进跨任务的知识迁移：</strong> 通过提供一个通用的适配器框架，有助于将不同图像任务中的技术和经验迁移到视频领域，加速相关研究的进展。</li>
<li><strong>加速实时视频应用的部署：</strong> 由于训练效率高且适配器本身可能设计得轻量级，有望加速开发和部署需要实时处理的视频应用，例如自动驾驶、视频监控、增强现实等。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用：</strong></p>
<ul>
<li><strong>自动驾驶：</strong> 视频中的传感器数据（如摄像头）需要高度的时间一致性和对各种环境腐蚀（雨、雪、雾、光照变化）的鲁棒性，例如用于目标检测、跟踪、语义分割和深度估计。</li>
<li><strong>视频监控与安防：</strong> 提升视频分析的稳定性，减少误报和漏报，尤其是在低光照、模糊或有干扰的视频流中。</li>
<li><strong>增强现实（AR）/虚拟现实（VR）：</strong> 需要精确且稳定的场景理解和跟踪，以实现沉浸式体验。</li>
<li><strong>视频编辑与后期制作：</strong> 自动进行视频去噪、增强、稳定等操作，提高工作效率。</li>
<li><strong>医学影像分析：</strong> 在处理动态医学影像（如超声、MRI序列）时，时间一致性对于诊断至关重要。</li>
<li><strong>机器人视觉：</strong> 机器人需要稳定可靠的视觉感知来导航和与环境交互。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性：</strong></p>
<ul>
<li><strong>适配器的通用性限制：</strong> 虽然论文声称适配器可以插入“几乎任何”架构，但实际的适配效果可能因基础网络的设计和任务的特性而异。某些高度特化的网络结构可能需要更精细的适配。</li>
<li><strong>训练数据的需求：</strong> 尽管训练过程资源高效，但仍然需要视频数据集来训练适配器，特别是包含各种腐蚀情况的视频。数据的质量和多样性将直接影响适配器的性能。</li>
<li><strong>理论与实践的差距：</strong> 尽管论文分析了损失函数的理论性质，但实际应用中，理论上的“良好行为”可能受到计算精度、数值稳定性等实际因素的影响。</li>
<li><strong>性能权衡：</strong> 论文提到“保留或提高预测质量”，但可能存在一个性能上的权衡。为了实现更好的时间稳定性和鲁棒性，单帧的预测精度可能会略有下降，或者需要更复杂的适配器设计来最小化这种影响。</li>
<li><strong>“Instant”的含义：</strong> 标题中的“Instant”可能暗示了推理速度上的优势，但摘要中并未明确说明适配器对推理速度的具体影响，以及是否会引入显著的延迟。</li>
</ul>
<p>总而言之，这篇论文提出了一种非常有前景的通用方法，能够显著提升现有图像模型在视频处理任务中的表现，尤其是在稳定性和鲁棒性方面。其模块化和高效训练的特性使其具有广泛的应用潜力，有望成为视频AI领域的一个重要技术基石。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this work, we introduce a general approach for adapting frame-based models for stable and robust inference on video.</li>
<li>We introduce a unified conceptual framework for describing temporal stability and corruption robustness, centered on a proposed accuracy-stability-robustness loss.</li>
<li>Our experiments validate our approach on several vision tasks including denoising (NAFNet), image enhancement (HDRNet), monocular depth (Depth Anything v2), and semantic segmentation (DeepLabv3+).</li>
<li>Our method improves temporal stability and robustness against a range of image corruptions (including compression artifacts, noise, and adverse weather), while preserving or improving the quality of predictions.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.03014v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.03014v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.03013v1'></a></p>
<h2 id="in-context-sync-lora-for-portrait-video-editing"><a href="https://arxiv.org/abs/2512.03013v1">In-Context Sync-LoRA for Portrait Video Editing</a></h2>
<p><strong>Authors:</strong> Sagi Polaczek, Or Patashnik, Ali Mahdavi-Amiri, Daniel Cohen-Or</p>
<p><strong>Published:</strong> 2025-12-02</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.GR</p>
<p><strong>Abstract:</strong></p>
<p>Editing portrait videos is a challenging task that requires flexible yet precise control over a wide range of modifications, such as appearance changes, expression edits, or the addition of objects. The key difficulty lies in preserving the subject's original temporal behavior, demanding that every edited frame remains precisely synchronized with the corresponding source frame. We present Sync-LoRA, a method for editing portrait videos that achieves high-quality visual modifications while maintaining frame-accurate synchronization and identity consistency. Our approach uses an image-to-video diffusion model, where the edit is defined by modifying the first frame and then propagated to the entire sequence. To enable accurate synchronization, we train an in-context LoRA using paired videos that depict identical motion trajectories but differ in appearance. These pairs are automatically generated and curated through a synchronization-based filtering process that selects only the most temporally aligned examples for training. This training setup teaches the model to combine motion cues from the source video with the visual changes introduced in the edited first frame. Trained on a compact, highly curated set of synchronized human portraits, Sync-LoRA generalizes to unseen identities and diverse edits (e.g., modifying appearance, adding objects, or changing backgrounds), robustly handling variations in pose and expression. Our results demonstrate high visual fidelity and strong temporal coherence, achieving a robust balance between edit fidelity and precise motion preservation.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“In-Context Sync-LoRA for Portrait Video Editing”的全面中文摘要：</p>
<p><strong>论文题目：</strong> In-Context Sync-LoRA for Portrait Video Editing</p>
<p><strong>作者：</strong> Sagi Polaczek, Or Patashnik, Ali Mahdavi-Amiri, Daniel Cohen-Or</p>
<p><strong>摘要</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong></p>
<p>论文旨在解决肖像视频编辑中的一个核心难题：如何在进行外观、表情或添加对象等修改的同时，精确地保持视频中人物的原始时间行为和身份信息。现有的视频编辑技术往往难以在保持编辑保真度（edit fidelity）和精确的时间同步性（temporal synchronization）之间取得平衡，尤其是在处理复杂的运动和表情时。</p>
<p><strong>2. 关键创新与方法贡献：</strong></p>
<ul>
<li><strong>Sync-LoRA 方法：</strong> 论文提出了一种名为 Sync-LoRA 的新方法，它基于图像到视频的扩散模型，并利用“In-Context LoRA”（IC-LoRA）的范式。</li>
<li><strong>编辑流程：</strong> 编辑过程始于用户修改视频的第一帧，然后 Sync-LoRA 将此编辑传播到整个视频序列。</li>
<li><strong>数据生成与筛选：</strong> 为了实现精确的时间同步，论文开发了一个两阶段的数据生成和筛选流程：<ul>
<li><strong>生成阶段：</strong> 利用视觉语言模型生成多样化的主题提示和编辑指令，通过文本到图像生成和图像编辑技术创建匹配的肖像图像对。</li>
<li><strong>筛选阶段：</strong> 将生成的图像对转换为视频，并利用基于面部和身体姿态关键点的运动指标（包括语音、注视、眨眼和姿态）来评估视频对的时间对齐度。通过一个精细的同步评分系统，仅保留时间上最对齐的视频对用于训练。</li>
</ul>
</li>
<li><strong>训练机制：</strong> Sync-LoRA 在一个精心筛选的、高度同步的肖像视频对数据集上进行训练。这种训练方式使模型能够学习将源视频的运动线索与编辑后的第一帧中的视觉变化相结合，从而在保持时间同步性的前提下，精确地传播局部编辑。</li>
<li><strong>通用性：</strong> Sync-LoRA 能够泛化到未见过的人物身份和多种多样的编辑类型（如外观改变、对象添加、背景更换），并能鲁棒地处理姿态和表情的变化。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong></p>
<ul>
<li><strong>高质量编辑与同步：</strong> Sync-LoRA 在视觉保真度和时间同步性之间取得了出色的平衡。实验结果表明，该方法能够生成具有高视觉质量、时间连贯性强的编辑视频，同时精确地保留源视频的运动轨迹和人物身份。</li>
<li><strong>优于现有方法：</strong> 与 VACE、LucyEdit、FlowEdit 和 AnyV2V 等现有方法相比，Sync-LoRA 在用户研究和定量评估中均表现出更优越的性能，尤其是在身份保持和时间同步方面。</li>
<li><strong>解决关键挑战：</strong> 该方法成功解决了肖像视频编辑中保持时间行为和身份一致性的关键难题，为生成式视频编辑领域带来了新的进展。</li>
</ul>
<p><strong>4. 提及的局限性：</strong></p>
<ul>
<li><strong>几何不匹配：</strong> 当编辑后的第一帧与源视频在几何上存在显著不匹配时（例如，非对齐的缩放或平移），模型可能会难以调和两个冲突的空间信号，导致时间对齐下降和局部伪影。</li>
<li><strong>快速运动的退化：</strong> 在涉及非常快速或大范围运动的序列中（如快速手部动作、舞蹈或镜头平移），源视频的光流引导可能变得模糊，导致纹理模糊或时间连贯性下降。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>解决几何不匹配问题：</strong> 未来工作可以探索更鲁棒的方法来处理编辑帧与源视频之间的几何不匹配。</li>
<li><strong>增强对快速运动的处理：</strong> 结合更强大的基础模型，特别是那些具有增强时间推理能力和多视图一致性的模型，以改善对快速运动场景的处理。</li>
<li><strong>多模态视频模型：</strong> 将 Sync-LoRA 的 in-context 范式扩展到新兴的多模态视频模型，这些模型能够同时推理视频和音频信号，这将是一个令人兴奋且具有挑战性的研究方向。</li>
<li><strong>更广泛的应用：</strong> Sync-LoRA 的框架为可控且时间上保真的视频编辑提供了新的范式，尤其适用于需要精确动作、语音和表演一致性的个性化对话头（talking-head）应用。</li>
</ul>
<p>总而言之，Sync-LoRA 是一项重要的研究成果，它通过创新的数据驱动方法和 in-context 学习范式，显著提升了肖像视频编辑的质量和可控性，尤其是在保持时间同步性和身份一致性方面取得了突破。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present Sync-LoRA, a method for editing portrait videos that achieves high-quality visual modifications while maintaining frame-accurate synchronization and identity consistency.</li>
<li>Our approach uses an image-to-video diffusion model, where the edit is defined by modifying the first frame and then propagated to the entire sequence.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.03013v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.03013v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-12-03 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
