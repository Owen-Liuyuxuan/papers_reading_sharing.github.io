<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-10-20 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DEâ«¶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Birdâs-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">è®ºæéè¯»</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-Î±
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-10-17/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-10-21/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-10-20">Arxiv Computer Vision Papers - 2025-10-20</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#blip3o-next-next-frontier-of-native-image-generation" class="nav-link">BLIP3o-NEXT: Next Frontier of Native Image Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#omnivinci-enhancing-architecture-and-data-for-omni-modal-understanding-llm" class="nav-link">OmniVinci: Enhancing Architecture and Data for Omni-Modal Understanding LLM</a>
                </li>
                <li class="nav-item">
                    <a href="#lightsout-diffusion-based-outpainting-for-enhanced-lens-flare-removal" class="nav-link">LightsOut: Diffusion-based Outpainting for Enhanced Lens Flare Removal</a>
                </li>
                <li class="nav-item">
                    <a href="#3dpr-single-image-3d-portrait-relight-using-generative-priors" class="nav-link">3DPR: Single Image 3D Portrait Relight using Generative Priors</a>
                </li>
                <li class="nav-item">
                    <a href="#recon-region-controllable-data-augmentation-with-rectification-and-alignment-for-object-detection" class="nav-link">ReCon: Region-Controllable Data Augmentation with Rectification and Alignment for Object Detection</a>
                </li>
                <li class="nav-item">
                    <a href="#dgme-t-directional-grid-motion-encoding-for-transformer-based-historical-camera-movement-classification" class="nav-link">DGME-T: Directional Grid Motion Encoding for Transformer-Based Historical Camera Movement Classification</a>
                </li>
                <li class="nav-item">
                    <a href="#valeo-near-field-a-novel-dataset-for-pedestrian-intent-detection" class="nav-link">Valeo Near-Field: a novel dataset for pedestrian intent detection</a>
                </li>
                <li class="nav-item">
                    <a href="#quantized-fca-efficient-zero-shot-texture-anomaly-detection" class="nav-link">Quantized FCA: Efficient Zero-Shot Texture Anomaly Detection</a>
                </li>
                <li class="nav-item">
                    <a href="#imaginarium-vision-guided-high-quality-3d-scene-layout-generation" class="nav-link">Imaginarium: Vision-guided High-Quality 3D Scene Layout Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#clappertext-a-benchmark-for-text-recognition-in-low-resource-archival-documents" class="nav-link">ClapperText: A Benchmark for Text Recognition in Low-Resource Archival Documents</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-10-20">Arxiv Computer Vision Papers - 2025-10-20</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>å¥½çï¼è¿æ¯ä¸ä»½éå¯¹2025å¹´10æ17æ¥ArXivè®¡ç®æºè§è§è®ºæçæ¯æ¥æ¥åæ§è¡æè¦ï¼æ¨å¨å¸®å©å¿ç¢çç ç©¶äººåå¿«éäºè§£ææ°è¿å±ã</p>
<hr />
<p><strong>æ¯æ¥ArXivè®¡ç®æºè§è§è®ºææ¥åæ§è¡æè¦ (2025å¹´10æ17æ¥)</strong></p>
<p><strong>1. ä¸»è¦ä¸»é¢åè¶å¿æ¦è¿°ï¼</strong></p>
<p>ä»å¤©çè®ºæå±ç¤ºäºè®¡ç®æºè§è§é¢åå ä¸ªæ´»è·ä¸ç¸äºå³èçç ç©¶æ¹åãçæå¼AIï¼ç¹å«æ¯æ©æ£æ¨¡åï¼å¨å¾åçæãç¼è¾å3Dåå®¹åå»ºæ¹é¢æç»­å æ®ä¸»å¯¼å°ä½ãå¤æ¨¡æå­¦ä¹ ï¼å°¤å¶æ¯ç»åå¤§åè¯­è¨æ¨¡åï¼LLMï¼è¿è¡è§è§çè§£ï¼æ¯å¦ä¸ä¸ªæ¾èè¶å¿ãæ­¤å¤ï¼æä»¬çå°äºå¯¹å®éåºç¨é®é¢çå³æ³¨ï¼ä¾å¦å¾åè´¨éå¢å¼ºãç¹å®åºæ¯ä¸çç®æ æ£æµãä»¥åéå¯¹ç¹å®æ°æ®éï¼å¦åå²ææ¡£åèªå¨é©¾é©¶ï¼çè§£å³æ¹æ¡ã3Dè§è§åæ°æ®å¢å¼ºææ¯ä¹ç»§ç»­å¾å°æ·±å¥æ¢ç´¢ã</p>
<p><strong>2. ç¹å«éè¦æåæ°çè®ºæäº®ç¹ï¼</strong></p>
<ul>
<li><strong>"BLIP3o-NEXT: Next Frontier of Native Image Generation" (Jiuhai Chen et al.)</strong>: è¿ç¯è®ºæä¼¼ä¹ä»£è¡¨äºåçå¾åçæé¢åçéå¤§é£è·ï¼å¯è½å¨å¾åè´¨éãå¤æ ·æ§ææ§å¶ç²åº¦æ¹é¢è®¾å®äºæ°çåºåãèèå°BLIPç³»åå¨å¤æ¨¡æçè§£åçææ¹é¢çå¼ºå¤§èæ¯ï¼BLIP3o-NEXTçåºç°é¢ç¤ºççææ¨¡åè½åçè¿ä¸æ­¥æåï¼å¯è½å¯¹AIGCï¼äººå·¥æºè½çæåå®¹ï¼äº§çæ·±è¿å½±åã</li>
<li><strong>"OmniVinci: Enhancing Architecture and Data for Omni-Modal Understanding LLM" (Hanrong Ye et al.)</strong>: è¿ç¯è®ºæèç¦äºå¨æ¨¡æçè§£LLMï¼è¡¨æç ç©¶äººåæ­£å¨ç§¯ææ¢ç´¢å¦ä½å°è§è§ãææ¬çè³å¶ä»æ¨¡æï¼å¦é³é¢ãè§¦è§ç­ï¼æ ç¼æ´åå°ç»ä¸çAIæ¨¡åä¸­ãå¶å¯¹æ¶æåæ°æ®çåéå¼ºè°ï¼é¢ç¤ºçå¨æå»ºæ´éç¨ãæ´å¼ºå¤§çAIæºè½ä½æ¹é¢åå¾äºè¿å±ã</li>
<li><strong>"LightsOut: Diffusion-based Outpainting for Enhanced Lens Flare Removal" (Shr-Ruei Tsai et al.)</strong>: è¿ç¯è®ºæå·§å¦å°å°æ©æ£æ¨¡ååºç¨äºä¸ä¸ªå·ä½çå¾åç¼è¾ææââéå¤´åæå»é¤ãéè¿ç»åoutpaintingææ¯ï¼å®å¯è½æä¾äºä¸ç§æ¯ä¼ ç»æ¹æ³æ´èªç¶ãæ´é²æ£çè§£å³æ¹æ¡ï¼å±ç¤ºäºçææ¨¡åå¨å®éå¾åä¿®å¤ä¸­çå·¨å¤§æ½åã</li>
</ul>
<p><strong>3. æ°å´ç ç©¶æ¹åæææ¯ï¼</strong></p>
<ul>
<li><strong>æ©æ£æ¨¡åå¨ç¹å®å¾åç¼è¾å3Dçæä¸­çç²¾ç»ååºç¨ï¼</strong> "LightsOut" å "3DPR" åå©ç¨æ©æ£æ¨¡åè§£å³ç¹å®é®é¢ï¼è¡¨ææ©æ£æ¨¡åä¸åä»ä»ç¨äºéç¨å¾åçæï¼èæ¯è¢«ç²¾ç»åå°åºç¨äºå¾åä¿®å¤ãé£æ ¼è¿ç§»ã3Dåå®¹åå»ºç­æ´å·ä½çä»»å¡ã</li>
<li><strong>å¤æ¨¡æLLMçâå¨æ¨¡æâæ©å±ï¼</strong> "OmniVinci" å¼ºè°âOmni-Modalâï¼å¨æ¨¡æï¼çè§£ï¼è¿è¶è¶äºä¼ ç»çè§è§-è¯­è¨ç»åï¼æç¤ºçæªæ¥AIæ¨¡åå°è½å¤å¤çåçè§£æ´å¤æ ·åçæ°æ®ç±»åã</li>
<li><strong>çæåéªå¨3Déå»ºåç¼è¾ä¸­çä½ç¨ï¼</strong> "3DPR" å©ç¨çæåéªè¿è¡åå¾å3Dèåéæåï¼è¿è¡¨æé¢è®­ç»ççææ¨¡åï¼å¦GANsææ©æ£æ¨¡åï¼çéå¼ç¥è¯æ­£è¢«ææå°ç¨äºè§£å³éåå¾å½¢å­¦é®é¢ï¼åå°å¯¹å¤è§å¾æå¤æä¼ æå¨æ°æ®çä¾èµã</li>
<li><strong>éå¯¹ç¹å®é¢åçæ°æ®éååºåï¼</strong> "Valeo Near-Field" å "ClapperText" çåå¸ï¼çªæ¾äºå¨èªå¨é©¾é©¶ï¼è¡äººæå¾æ£æµï¼ååå²ææ¡£ï¼ä½èµæºææ¬è¯å«ï¼ç­ç¹å®åºç¨é¢åï¼å¯¹é«è´¨éãæéå¯¹æ§æ°æ®éçæç»­éæ±ï¼ä»¥åè¿äºæ°æ®éå¯¹æ¨å¨é¢åè¿æ­¥çå³é®ä½ç¨ã</li>
</ul>
<p><strong>4. å»ºè®®éè¯»å¨æçè®ºæï¼</strong></p>
<p>å¯¹äºä¸åå´è¶£çç ç©¶äººåï¼ä»¥ä¸è®ºæå¯è½ææä»·å¼ï¼</p>
<ul>
<li><strong>å¯¹äºçæå¼AIåå¤æ¨¡æå­¦ä¹ ç ç©¶èï¼</strong><ul>
<li><strong>"BLIP3o-NEXT: Next Frontier of Native Image Generation"</strong>: äºè§£å¾åçæé¢åçææ°çªç ´ã</li>
<li><strong>"OmniVinci: Enhancing Architecture and Data for Omni-Modal Understanding LLM"</strong>: æ¢ç´¢å¤æ¨¡æLLMçæªæ¥åå±æ¹åã</li>
</ul>
</li>
<li><strong>å¯¹äºå¾åå¤çåç¼è¾ç ç©¶èï¼</strong><ul>
<li><strong>"LightsOut: Diffusion-based Outpainting for Enhanced Lens Flare Removal"</strong>: å­¦ä¹ æ©æ£æ¨¡åå¨å¾åä¿®å¤ä¸­çåæ°åºç¨ã</li>
<li><strong>"3DPR: Single Image 3D Portrait Relight using Generative Priors"</strong>: äºè§£å¦ä½å©ç¨çæåéªè¿è¡åå¾å3Dç¼è¾ã</li>
</ul>
</li>
<li><strong>å¯¹äºç®æ æ£æµåæ°æ®å¢å¼ºç ç©¶èï¼</strong><ul>
<li><strong>"ReCon: Region-Controllable Data Augmentation with Rectification and Alignment for Object Detection"</strong>: æ¢ç´¢æ°çæ°æ®å¢å¼ºç­ç¥ä»¥æåç®æ æ£æµæ§è½ã</li>
</ul>
</li>
<li><strong>å¯¹äºèªå¨é©¾é©¶åç¹å®é¢ååºç¨ç ç©¶èï¼</strong><ul>
<li><strong>"Valeo Near-Field: a novel dataset for pedestrian intent detection"</strong>: å¦ææ¨çç ç©¶æ¶åèªå¨é©¾é©¶æè¡äººè¡ä¸ºé¢æµï¼è¿ä¸ªæ°æ°æ®éè³å³éè¦ã</li>
</ul>
</li>
<li><strong>å¯¹äº3Dè§è§ååºæ¯çè§£ç ç©¶èï¼</strong><ul>
<li><strong>"Imaginarium: Vision-guided High-Quality 3D Scene Layout Generation"</strong>: äºè§£å¦ä½ä»è§è§è¾å¥çæé«è´¨éç3Dåºæ¯å¸å±ã</li>
</ul>
</li>
</ul>
<hr />
<p>è¿ä»½æè¦å¸æè½ä¸ºæ¨æä¾ä¸ä¸ªå¿«éèå¨é¢çæ¦è§ï¼å¸®å©æ¨ä¼åéè¯»æç¸å³çè®ºæã</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2510.15857v1">BLIP3o-NEXT: Next Frontier of Native Image Generation</a></li>
<li><a href="#2510.15870v1">OmniVinci: Enhancing Architecture and Data for Omni-Modal Understanding LLM</a></li>
<li><a href="#2510.15868v1">LightsOut: Diffusion-based Outpainting for Enhanced Lens Flare Removal</a></li>
<li><a href="#2510.15846v1">3DPR: Single Image 3D Portrait Relight using Generative Priors</a></li>
<li><a href="#2510.15783v1">ReCon: Region-Controllable Data Augmentation with Rectification and Alignment for Object Detection</a></li>
<li><a href="#2510.15725v1">DGME-T: Directional Grid Motion Encoding for Transformer-Based Historical Camera Movement Classification</a></li>
<li><a href="#2510.15673v1">Valeo Near-Field: a novel dataset for pedestrian intent detection</a></li>
<li><a href="#2510.15602v1">Quantized FCA: Efficient Zero-Shot Texture Anomaly Detection</a></li>
<li><a href="#2510.15564v1">Imaginarium: Vision-guided High-Quality 3D Scene Layout Generation</a></li>
<li><a href="#2510.15557v1">ClapperText: A Benchmark for Text Recognition in Low-Resource Archival Documents</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2510.15857v1'></a></p>
<h2 id="blip3o-next-next-frontier-of-native-image-generation"><a href="https://arxiv.org/abs/2510.15857v1">BLIP3o-NEXT: Next Frontier of Native Image Generation</a></h2>
<p><strong>Authors:</strong> Jiuhai Chen, Le Xue, Zhiyang Xu, Xichen Pan, Shusheng Yang, Can Qin, An Yan, Honglu Zhou, Zeyuan Chen, Lifu Huang, Tianyi Zhou, Junnan Li, Silvio Savarese, Caiming Xiong, Ran Xu</p>
<p><strong>Published:</strong> 2025-10-17</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We present BLIP3o-NEXT, a fully open-source foundation model in the BLIP3
series that advances the next frontier of native image generation. BLIP3o-NEXT
unifies text-to-image generation and image editing within a single
architecture, demonstrating strong image generation and image editing
capabilities. In developing the state-of-the-art native image generation model,
we identify four key insights: (1) Most architectural choices yield comparable
performance; an architecture can be deemed effective provided it scales
efficiently and supports fast inference; (2) The successful application of
reinforcement learning can further push the frontier of native image
generation; (3) Image editing still remains a challenging task, yet instruction
following and the consistency between generated and reference images can be
significantly enhanced through post-training and data engine; (4) Data quality
and scale continue to be decisive factors that determine the upper bound of
model performance. Building upon these insights, BLIP3o-NEXT leverages an
Autoregressive + Diffusion architecture in which an autoregressive model first
generates discrete image tokens conditioned on multimodal inputs, whose hidden
states are then used as conditioning signals for a diffusion model to generate
high-fidelity images. This architecture integrates the reasoning strength and
instruction following of autoregressive models with the fine-detail rendering
ability of diffusion models, achieving a new level of coherence and realism.
Extensive evaluations of various text-to-image and image-editing benchmarks
show that BLIP3o-NEXT achieves superior performance over existing models.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾ç±Jiuhai Chenç­äººæ°åçè®ºæâBLIP3o-NEXT: Next Frontier of Native Image Generationâçå¨é¢æè¦ã</p>
<hr />
<h3 id="blip3o-next">BLIP3o-NEXT: åçå¾åçæçæ°åæ²¿</h3>
<p><strong>1. è®ºæä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³åçå¾åçæé¢åçåæ²¿ææï¼ç¹å«æ¯å¦ä½æå»ºä¸ä¸ªç»ä¸çãé«æ§è½çãè½å¤åæ¶è¿è¡ææ¬å°å¾åçæåå¾åç¼è¾çå¼æºåºç¡æ¨¡åãç ç©¶çæ ¸å¿é®é¢æ¯å¦ä½å¨ä¿æå¾åè´¨éãè¯­ä¹è¿è´¯æ§åæä»¤éµå¾ªè½åçåæ¶ï¼æææ´åä¸åççæèå¼ï¼å¦èªåå½åæ©æ£æ¨¡åï¼ï¼å¹¶åæç°ææ¨¡åå¨è¿äºä»»å¡ä¸çå±éæ§ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³å­¦è´¡ç®ï¼</strong>
BLIP3o-NEXTæåºäºä»¥ä¸å³é®åæ°åæ¹æ³å­¦è´¡ç®ï¼
*   <strong>æ°é¢ä¸å¯æ©å±çèªåå½+æ©æ£æ¶æï¼</strong> BLIP3o-NEXTéç¨äºä¸ç§æ··åæ¶æï¼å¶ä¸­èªåå½æ¨¡åé¦åæ ¹æ®å¤æ¨¡æè¾å¥çæç¦»æ£å¾åtokenï¼ç¶åè¿äºtokençéèç¶æä½ä¸ºæ©æ£æ¨¡åçæ¡ä»¶ä¿¡å·ï¼ç¨äºçæé«ä¿çå¾åãè¿ç§è®¾è®¡ç»åäºèªåå½æ¨¡åçæ¨çè½ååæä»¤éµå¾ªè½åï¼ä»¥åæ©æ£æ¨¡åçç²¾ç»ç»èæ¸²æè½åï¼å®ç°äºæ´é«çè¿è´¯æ§åçå®æã
*   <strong>é«æçå¼ºåå­¦ä¹ æ¹æ³ï¼</strong> è®ºææåºäºä¸ç§éå¯¹èªåå½æ¨¡åçé«æå¼ºåå­¦ä¹ ï¼RLï¼æ¡æ¶ï¼éè¿å©ç¨ç¦»æ£å¾åtokenï¼ä½¿å¶è½å¤æ ç¼éæå°ç°æçè¯­è¨æ¨¡åRLåºç¡è®¾æ½ä¸­ãè¿æ¾èæåäºæ¨¡åçææ¬æ¸²æè´¨éåæä»¤éµå¾ªè½åã
*   <strong>ç³»ç»æ§å¾åç¼è¾ä¸è´æ§ç ç©¶ï¼</strong> è®ºææ·±å¥ç ç©¶äºæé«å¾åç¼è¾ä¸è´æ§çç­ç¥ï¼åæ¬å°VAEï¼ååèªç¼ç å¨ï¼ç¹å¾ä½ä¸ºè·¨æ³¨æåè¾å¥ååªå£°ç©ºé´æ³¨å¥ä¸¤ç§æ¹å¼æ´åå°æ©æ£æ¨¡åä¸­ï¼ä»¥ä¿çç²¾ç»çåç´ çº§ä¿¡æ¯å¹¶å¢å¼ºçæå¾åä¸åèå¾åä¹é´çä¸è´æ§ã
*   <strong>å³é®æ´å¯çæç¼ï¼</strong> è®ºææ»ç»äºå¼åæåè¿åçå¾åçææ¨¡åçåä¸ªå³é®æ´å¯ï¼1) å¤§å¤æ°æ¶æéæ©æ§è½ç¸ä¼¼ï¼å³é®å¨äºæçåå¿«éæ¨çï¼2) å¼ºåå­¦ä¹ è½è¿ä¸æ­¥æ¨å¨åçå¾åçæåæ²¿ï¼3) å¾åç¼è¾ä»å·ææï¼ä½åæè®­ç»åæ°æ®å¼æå¯æ¾èå¢å¼ºæä»¤éµå¾ªåä¸è´æ§ï¼4) æ°æ®è´¨éåè§æ¨¡æ¯æ¨¡åæ§è½ä¸éçå³å®æ§å ç´ ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>åè¶çæ§è½ï¼</strong> å¨åç§ææ¬å°å¾åçæåå¾åç¼è¾åºåæµè¯ä¸è¿è¡çå¹¿æ³è¯ä¼°è¡¨æï¼BLIP3o-NEXTå¨ç°ææ¨¡åä¸­åå¾äºåè¶çæ§è½ãå°½ç®¡å¶3Bæ¨¡åå¨æäºç¼è¾ä»»å¡ä¸ç¥ä½äºGPT-ImageåQwen-Imageç­æ´å¤§æ¨¡åï¼ä½å®ä¸BAGELåOmniGen2ç­æ¨¡åè¡¨ç°ç¸å½ã
*   <strong>å¼ºåå­¦ä¹ çæææ§ï¼</strong> å®éªç»æï¼å¦GenEvalåOCRè®­ç»å¥å±æ²çº¿ï¼æ¸æ°å±ç¤ºäºéè¿GRPOè®­ç»åï¼æ¨¡åå¨å¤ç®æ ç»ååè§è§ææ¬æ¸²æè´¨éæ¹é¢çæ¾èæåã
*   <strong>VAEç¹å¾å¯¹ä¸è´æ§çå¢å¼ºï¼</strong> è®ºæéè¿å®æ§ç»æå±ç¤ºï¼æ´åVAEæ½å¨ç¹å¾è½ææå¢å¼ºå¾åç¼è¾ä»»å¡ä¸­çä¸è´æ§ï¼å°½ç®¡å¨æäºæåµä¸ä»å­å¨è½»å¾®ä¸ä¸è´ã
*   <strong>å¼æºè´¡ç®ï¼</strong> ä½ä¸ºBLIP3ç³»åä¸­çä¸ä¸ªå®å¨å¼æºçåºç¡æ¨¡åï¼BLIP3o-NEXTåå¸äºé¢è®­ç»ååæè®­ç»çæ¨¡åæéãæ°æ®éãè¯¦ç»çè®­ç»åæ¨çä»£ç ä»¥åè¯ä¼°æµç¨ï¼ç¡®ä¿äºå®å¨å¯å¤ç°æ§ï¼å¯¹ç¤¾åºå·æéè¦æä¹ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>VAEç¹å¾ä¸ä¸è´æ§ï¼</strong> å°½ç®¡VAEç¹å¾çå¼å¥æé«äºå¾åç¼è¾çä¸è´æ§ï¼ä½ç±äºSANAä¸­VAEçä¸éæ ·æ¯çè¾é«ï¼32ï¼ï¼çæçå¾åä¸åèå¾åä¹é´ä»å­å¨è½»å¾®çä¸ä¸è´ã
*   <strong>RLå¨æ©æ£æ¨¡åä¸çéåº¦ï¼</strong> å¨æ²¡ææ©æ£å éçæåµä¸ï¼å°RLåºç¨äºæ©æ£æ¨¡åä¼è¾æ¢ï¼å ä¸ºç¼ºä¹KVç¼å­æ¯æä¸éè¦å¤ä¸ªæ¶é´æ­¥ã
*   <strong>RLå¥å±æ¨¡åè®¾è®¡ï¼</strong> å°å¼ºåå­¦ä¹ åºç¨äºåçå¾åçæçæ ¸å¿ææå¨äºå¥å±æ¨¡åçè®¾è®¡ï¼éè¦å¼åè½å¤ææææåå¹³è¡¡å¾åè´¨éãæä»¤éµå¾ªåäººç±»åå¥½å¯¹é½ç­å¤ä¸ªç»´åº¦çå¥å±æ¨¡åã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>å¾åç¼è¾çå¼ºåå­¦ä¹ ï¼</strong> è¿ä¸æ­¥æ¢ç´¢å°RLåºç¨äºå¾åç¼è¾ï¼ä»¥æé«æä»¤éµå¾ªè½ååçæè¾åºä¸åèè¾å¥ä¹é´çä¸è´æ§ãç¹å«æ¯ï¼éè¦æ·±å¥ç ç©¶æµéä¸è´æ§çå¥å±æ¨¡åè®¾è®¡ã
*   <strong>ç³»ç»æç¤ºè®¾è®¡ï¼</strong> éå¯¹å¾åä¿®å¤åä¸»ä½é©±å¨çæä»»å¡ï¼è®¾è®¡æç¡®åºåä»»å¡çç³»ç»æç¤ºï¼ä»¥æ´å¥½å°æå¯¼æ¨¡åå¤çè¿äºä¸åç±»åçç¼è¾ä»»å¡ã
*   <strong>æç¤ºéåï¼</strong> å©ç¨æç¤ºéåæ¥ä¸°å¯æç¤ºç»èï¼æé«å¾åçæåç¼è¾ä»»å¡çæä»¤éµå¾ªè½åã
*   <strong>æ°æ®å·¥ç¨åRLæ©å±ï¼</strong> å¼¥è¡¥èªåå½+æ©æ£æ¶æå¨æä»¤éµå¾ªæ¹é¢è¡¨ç°åºè²ï¼ä½å¨ç¼è¾ä¸è´æ§æ¹é¢ä»æä¸è¶³çå·®è·ï¼éè¦è¿ä¸æ­¥å¨æ°æ®å·¥ç¨åä¸é¨éå¯¹å¾åç¼è¾çå¼ºåå­¦ä¹ æ¹é¢åå¾è¿å±ã
*   <strong>ç»ä¸æ¶æãå¼ºåå­¦ä¹ åå¯æ©å±åæè®­ç»ï¼</strong> æªæ¥åºç¡æ¨¡ååºå¨ç»ä¸æ¶æãå¼ºåå­¦ä¹ åå¯æ©å±åæè®­ç»æ¹é¢å±åæ¨å¨å¯æ§ãæä»¤å¯¹é½åé«è´¨éåçå¾åçæç³»ç»çåå±ã</p>
<hr />
<p>è¿ç¯è®ºæä¸ºåçå¾åçæé¢åæä¾äºä¸ä¸ªå¼ºå¤§çå¼æºåºç¡æ¨¡åï¼å¹¶éè¿å¶æ··åæ¶æãå¼ºåå­¦ä¹ åºç¨åå¯¹å¾åç¼è¾ä¸è´æ§çæ·±å¥ç ç©¶ï¼ä¸ºæªæ¥çç ç©¶å¥ å®äºåå®çåºç¡ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present BLIP3o-NEXT, a fully open-source foundation model in the BLIP3
series that advances the next frontier of native image generation.</li>
<li>In developing the state-of-the-art native image generation model,
we identify four key insights: (1) Most architectural choices yield comparable
performance; an architecture can be deemed effective provided it scales
efficiently and supports fast inference; (2) The successful application of
reinforcement learning can further push the frontier of native image
generation; (3) Image editing still remains a challenging task, yet instruction
following and the consistency between generated and reference images can be
significantly enhanced through post-training and data engine; (4) Data quality
and scale continue to be decisive factors that determine the upper bound of
model performance.</li>
<li>This architecture integrates the reasoning strength and
instruction following of autoregressive models with the fine-detail rendering
ability of diffusion models, achieving a new level of coherence and realism.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.15857v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.15857v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.15870v1'></a></p>
<h2 id="omnivinci-enhancing-architecture-and-data-for-omni-modal-understanding-llm"><a href="https://arxiv.org/abs/2510.15870v1">OmniVinci: Enhancing Architecture and Data for Omni-Modal Understanding LLM</a></h2>
<p><strong>Authors:</strong> Hanrong Ye, Chao-Han Huck Yang, Arushi Goel, Wei Huang, Ligeng Zhu, Yuanhang Su, Sean Lin, An-Chieh Cheng, Zhen Wan, Jinchuan Tian, Yuming Lou, Dong Yang, Zhijian Liu, Yukang Chen, Ambrish Dantrey, Ehsan Jahangiri, Sreyan Ghosh, Daguang Xu, Ehsan Hosseini-Asl, Danial Mohseni Taheri, Vidya Murali, Sifei Liu, Jason Lu, Oluwatobi Olabiyi, Frank Wang, Rafael Valle, Bryan Catanzaro, Andrew Tao, Song Han, Jan Kautz, Hongxu Yin, Pavlo Molchanov</p>
<p><strong>Published:</strong> 2025-10-17</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.CL</p>
<p><strong>Abstract:</strong></p>
<p>Advancing machine intelligence requires developing the ability to perceive
across multiple modalities, much as humans sense the world. We introduce
OmniVinci, an initiative to build a strong, open-source, omni-modal LLM. We
carefully study the design choices across model architecture and data curation.
For model architecture, we present three key innovations: (i) OmniAlignNet for
strengthening alignment between vision and audio embeddings in a shared
omni-modal latent space; (ii) Temporal Embedding Grouping for capturing
relative temporal alignment between vision and audio signals; and (iii)
Constrained Rotary Time Embedding for encoding absolute temporal information in
omni-modal embeddings. We introduce a curation and synthesis pipeline that
generates 24M single-modal and omni-modal conversations. We find that
modalities reinforce one another in both perception and reasoning. Our model,
OmniVinci, outperforms Qwen2.5-Omni with +19.05 on DailyOmni (cross-modal
understanding), +1.7 on MMAR (audio), and +3.9 on Video-MME (vision), while
using just 0.2T training tokens - a 6 times reduction compared to
Qwen2.5-Omni's 1.2T. We finally demonstrate omni-modal advantages in downstream
applications spanning robotics, medical AI, and smart factory.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼è¿æ¯ä¸ç¯å³äºâOmniVinci: Enhancing Architecture and Data for Omni-Modal Understanding LLMâè®ºæçå¨é¢æè¦ï¼</p>
<p><strong>è®ºææè¦ï¼OmniVinciï¼å¢å¼ºå¨æ¨¡æçè§£å¤§åè¯­è¨æ¨¡åçæ¶æåæ°æ®</strong></p>
<p><strong>1. è§£å³çä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³å½åå¤æ¨¡æå¤§åè¯­è¨æ¨¡åï¼LLMsï¼å¨å®ç°è·¨å¤ç§æ¨¡æï¼è§è§ãå¬è§ãææ¬ï¼çå¨é¢æç¥åçè§£æ¹é¢çææãå°½ç®¡ç°ææ¨¡åå¨åä¸æ¨¡ææé¨åæ¨¡æèåæ¹é¢åå¾äºè¿å±ï¼ä½æå»ºä¸ä¸ªå¼ºå¤§çãå¼æºçãè½å¤åçå¤çå¹¶æ¨çæææ¨¡æä¿¡æ¯çå¨æ¨¡æLLMä»ç¶æ¯ä¸ä¸ªæªååæ¢ç´¢çé¢åãå·ä½æ¥è¯´ï¼è®ºæå³æ³¨å¦ä½éè¿åæ°çæ¨¡åæ¶æåé«æçæ°æ®ç­ç¥ï¼æåå¨æ¨¡æLLMçæ§è½åè®­ç»æçã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
OmniVinciå¼å¥äºä¸é¡¹å³é®çæ¨¡åæ¶æåæ°åä¸å¥æ°æ®ç­å±ä¸åææµç¨ï¼</p>
<ul>
<li><strong>OmniAlignNetï¼</strong> æ¨å¨å å¼ºè§è§åé³é¢åµå¥å¨å±äº«å¨æ¨¡ææ½å¨ç©ºé´ä¸­çå¯¹é½ãå®éè¿å¯¹æ¯å­¦ä¹ ï¼å©ç¨è§é¢ä¸­è§è§åé³é¢æµåºæçè¯­ä¹å³èï¼å°ä¸åæ¨¡æçåµå¥æ å°å°ç»ä¸çæ½å¨ç©ºé´ä¸­ï¼ä»èæ´ææå°å­¦ä¹ åå¯¹é½ã</li>
<li><strong>æ¶é´åµå¥åç»ï¼Temporal Embedding Grouping, TEGï¼ï¼</strong> éè¿æ ¹æ®æ¶é´æ³å°è§è§åé³é¢åµå¥ç»ç»æç»ï¼æè·æ¨¡æä¹é´ç¸å¯¹æ¶é´å¯¹é½ä¿¡æ¯ãè¿æå©äºLLMéª¨å¹²ç½ç»æ´å¥½å°çè§£ä¸åæ¨¡æåµå¥ä¹é´çæ¶é´å³ç³»ã</li>
<li><strong>åéæè½¬æ¶é´åµå¥ï¼Constrained Rotary Time Embedding, CRTEï¼ï¼</strong> ç¨äºå¨å¨æ¨¡æåµå¥ä¸­ç¼ç ç»å¯¹æ¶é´ä¿¡æ¯ãå®éè¿å®ä¹æå¤§æ¶é´èå´ï¼Tmaxï¼æ¥å¹³è¡¡å±é¨åå¨å±æ¶é´æææ§ï¼è§£å³äºç°ææ¹æ³å¨å¤çè¾å¤§æ¶é´åç§»æ¶çå±éæ§ã</li>
<li><strong>æ°æ®ç­å±ä¸åææµç¨ï¼</strong> è®ºæå¼åäºä¸ä¸ªçæ2400ä¸åæ¨¡æåå¨æ¨¡æå¯¹è¯çæ°æ®ç®¡éãè¯¥ç®¡ééè¿å©ç¨ç°æè§é¢-é³é¢é®ç­æ°æ®è¿è¡éå¼å­¦ä¹ ï¼å¹¶çæå¸¦ææ¾å¼å¨æ¨¡ææ ç­¾çåæå¯¹è¯ï¼ä»¥è§£å³å¨æ¨¡ææ°æ®ç¨ç¼ºçé®é¢ãç¹å«å°ï¼å®å¼å¥äºä¸ä¸ªå¨æ¨¡ææ°æ®å¼æï¼éè¿LLMè¿è¡è·¨æ¨¡ææ ¡æ­£åæ»ç»ï¼çæåç¡®çå¨æ¨¡æå­å¹ï¼ä»¥åæåä¸æ¨¡æå­å¹çâæ¨¡æç¹å¼æ§å¹»è§âé®é¢ã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
OmniVinciå¨å¤ä¸ªåºåæµè¯ä¸­å±ç°äºåè¶çæ§è½ï¼å¹¶æ¾èæåäºè®­ç»æçï¼</p>
<ul>
<li><strong>æ§è½æåï¼</strong> OmniVinciå¨DailyOmniï¼è·¨æ¨¡æçè§£ï¼ä¸æ¯Qwen2.5-Omnié«åº+19.05åï¼å¨MMARï¼é³é¢ï¼ä¸é«åº+1.7åï¼å¨Video-MMEï¼è§è§ï¼ä¸é«åº+3.9åãå¨å¨æ¨¡æçè§£åºåæµè¯ä¸­ï¼OmniVinciçå¹³åå¾åè¾¾å°53.73ï¼æ¯æ¬¡ä¼æ¨¡åQwen2.5-Omnié«åº+4.07ã</li>
<li><strong>è®­ç»æçï¼</strong> OmniVinciä»ä½¿ç¨0.2Tè®­ç»tokenï¼æ¯Qwen2.5-Omniç1.2Tåå°äº6åï¼æ¾èéä½äºè®­ç»ææ¬åæ¨çææ¬ã</li>
<li><strong>æ¨¡æååæåºï¼</strong> å®éªåç°ï¼é³é¢åè§é¢æ¨¡æå¨æç¥åæ¨çæ¹é¢ç¸äºå¢å¼ºï¼è¯æäºå¨æ¨¡æå­¦ä¹ çä¼å¿ã</li>
<li><strong>ä¸æ¸¸åºç¨ï¼</strong> OmniVinciå¨æºå¨äººãå»çAIåæºè½å·¥åç­ä¸æ¸¸åºç¨ä¸­å±ç¤ºäºå¨æ¨¡æä¼å¿ï¼ä¾å¦è¯­é³é©±å¨çè§è§è¯­è¨å¯¼èªãä½è²è§é¢çè§£ãè·¨è¯­è¨è¯­é³ç¿»è¯ãå»å­¦å½±ååæååå¯¼ä½å·¥åçæ§ã</li>
</ul>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
è®ºæä¸»è¦å³æ³¨æ¨¡åæ¶æåæ°æ®ç­å±çåæ°ï¼ä½å¹¶æªæç¡®æåå½åOmniVinciæ¨¡åçå·ä½å±éæ§ãç¶èï¼ä»å¶å¯¹æ¨¡åéååé«æé¨ç½²çè®¨è®ºä¸­å¯ä»¥çåºï¼å¤§åæ¨¡ååé¿è§é¢åºåçåå­å®¹ééå¶ä»¥åäº¤äºå¼åºç¨å¯¹ä½å»¶è¿çéæ±æ¯å®éé¨ç½²ä¸­é¢ä¸´çææãæ­¤å¤ï¼è½ç¶æ¨¡åå¨å¤æ¨¡æä»»å¡ä¸è¡¨ç°åºè²ï¼ä½å¶å¨å¤çç¹å®å¤æåºæ¯ï¼å¦æåº¦åæç¯å¢ä¸çè¯­é³è¯å«ï¼æ¶ï¼å¯è½ä»éåå©å¤é¨ASRæ¨¡åè¿è¡è¾å©ï¼è¿æç¤ºäºæ¨¡åå¨æäºæç«¯æ¡ä»¶ä¸çé²æ£æ§ä»ææåç©ºé´ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>æç»­ä¼åæ¨¡åæçï¼</strong> è¿ä¸æ­¥æ¢ç´¢æ´åè¿çæ¨¡åéååé¨ç½²ç­ç¥ï¼ä»¥å¨ä¿æé«æ§è½çåæ¶ï¼è¿ä¸æ­¥éä½åå­æ¶èåæ¨çå»¶è¿ï¼ä½¿å¶æ´éç¨äºèµæºåéçè¾¹ç¼è®¾å¤åå®æ¶åºç¨ã
*   <strong>å¢å¼ºæç«¯æ¡ä»¶ä¸çé²æ£æ§ï¼</strong> éå¯¹æ´å¤æçåªå£°ç¯å¢ãå£é³å¤æ ·æ§æéå è¯­é³ç­åºæ¯ï¼è¿ä¸æ­¥æåæ¨¡åå¨è¯­é³çè§£ä»»å¡ä¸­çé²æ£æ§ï¼åå°å¯¹å¤é¨è¾å©ç³»ç»çä¾èµã
*   <strong>æ©å±å¨æ¨¡æè½åï¼</strong> æ¢ç´¢å°æ´å¤æ¨¡æï¼å¦è§¦è§ãåè§ç­ï¼éæå°å¨æ¨¡ææ¡æ¶ä¸­ï¼ä»¥å®ç°æ´å¨é¢çéç¨æºè½ã
*   <strong>æ·±åè·¨æ¨¡ææ¨çè½åï¼</strong> è¿ä¸æ­¥ç ç©¶åå¼åæ´å¤æçè·¨æ¨¡ææ¨çæºå¶ï¼ä½¿å¶è½å¤å¤çæ´æ½è±¡ãæ´æ·±å±æ¬¡çæ¨¡æé´å³ç³»ï¼ä»èå¨æ´å¹¿æ³çä¸æ¸¸ä»»å¡ä¸­åæ¥ä½ç¨ã
*   <strong>æ°æ®çæä¸åæçæç»­æ¹è¿ï¼</strong> æç»­ä¼åå¨æ¨¡ææ°æ®å¼æï¼çææ´é«è´¨éãæ´å¤æ ·åãæ´å·æææ§çå¨æ¨¡æå¯¹è¯åæ ç­¾ï¼ä»¥æ¨å¨æ¨¡åæ§è½çè¿ä¸æ­¥æåã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce
OmniVinci, an initiative to build a strong, open-source, omni-modal LLM.</li>
<li>For model architecture, we present three key innovations: (i) OmniAlignNet for
strengthening alignment between vision and audio embeddings in a shared
omni-modal latent space; (ii) Temporal Embedding Grouping for capturing
relative temporal alignment between vision and audio signals; and (iii)
Constrained Rotary Time Embedding for encoding absolute temporal information in
omni-modal embeddings.</li>
<li>We introduce a curation and synthesis pipeline that
generates 24M single-modal and omni-modal conversations.</li>
<li>Our model,
OmniVinci, outperforms Qwen2.5-Omni with +19.05 on DailyOmni (cross-modal
understanding), +1.7 on MMAR (audio), and +3.9 on Video-MME (vision), while
using just 0.2T training tokens - a 6 times reduction compared to
Qwen2.5-Omni's 1.2T.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.15870v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.15870v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.15868v1'></a></p>
<h2 id="lightsout-diffusion-based-outpainting-for-enhanced-lens-flare-removal"><a href="https://arxiv.org/abs/2510.15868v1">LightsOut: Diffusion-based Outpainting for Enhanced Lens Flare Removal</a></h2>
<p><strong>Authors:</strong> Shr-Ruei Tsai, Wei-Cheng Chang, Jie-Ying Lee, Chih-Hai Su, Yu-Lun Liu</p>
<p><strong>Published:</strong> 2025-10-17</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Lens flare significantly degrades image quality, impacting critical computer
vision tasks like object detection and autonomous driving. Recent Single Image
Flare Removal (SIFR) methods perform poorly when off-frame light sources are
incomplete or absent. We propose LightsOut, a diffusion-based outpainting
framework tailored to enhance SIFR by reconstructing off-frame light sources.
Our method leverages a multitask regression module and LoRA fine-tuned
diffusion model to ensure realistic and physically consistent outpainting
results. Comprehensive experiments demonstrate LightsOut consistently boosts
the performance of existing SIFR methods across challenging scenarios without
additional retraining, serving as a universally applicable plug-and-play
preprocessing solution. Project page: https://ray-1026.github.io/lightsout/</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Shr-Ruei Tsaiç­äººæ°åçè®ºæâLightsOut: Diffusion-based Outpainting for Enhanced Lens Flare Removalâçå¨é¢æè¦ã</p>
<hr />
<h3 id="lightsout">LightsOut: åºäºæ©æ£çå¤ç»å¢å¼ºéå¤´ç©åå»é¤çå¨é¢æè¦</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
éå¤´ç©åï¼Lens Flareï¼ä¸¥éå½±åå¾åè´¨éï¼å¹¶å¯¹ç©ä½æ£æµåèªå¨é©¾é©¶ç­å³é®è®¡ç®æºè§è§ä»»å¡é æè´é¢å½±åãå°½ç®¡ç°æçåå¾åç©åå»é¤ï¼SIFRï¼æ¹æ³å¨ä¸ç¨æ°æ®éä¸åå¾äºæ¾èè¿å±ï¼ä½å½å¾åä¸­å­å¨ä¸å®æ´æç¼ºå¤±çç»æ¡å¤ï¼off-frameï¼åæºæ¶ï¼è¿äºæ¹æ³çæ§è½ä¼æ¾èä¸éãè¿æ¯å ä¸ºä¸å®æ´çåæºä¸ä¸æå¯¼è´SIFRæ¨¡åé¾ä»¥åç¡®è¯å«åå»é¤ç©åä¼ªå½±ãæ¬ç ç©¶æ¨å¨è§£å³è¿ä¸å³é®éå¶ï¼å³å¨åæºä¿¡æ¯ä¸å®æ´çæåµä¸ï¼å¦ä½ææå»é¤éå¤´ç©åã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è´¡ç®ï¼</strong>
LightsOut æåºäºä¸ç§åºäºæ©æ£ï¼diffusion-basedï¼çå¤ç»ï¼outpaintingï¼æ¡æ¶ï¼æ¨å¨éè¿éå»ºç»æ¡å¤åæºæ¥å¢å¼ºç°æSIFRæ¹æ³çæ§è½ãå¶ä¸»è¦åæ°åæ¹æ³è´¡ç®åæ¬ï¼</p>
<ul>
<li><strong>ä¸é¨çåè§£ç­ç¥ï¼</strong> è¯å«å¹¶è§£å³äºSIFRæ¹æ³å¨å¤çä¸å®æ´ç»æ¡å¤åæºæ¶çå³é®éå¶ï¼éè¿ä¸ç§ä¸é¨çåè§£ç­ç¥æ¥è§£å³ã</li>
<li><strong>å¤ä»»å¡åå½æ¨¡åï¼</strong> å¼å¥ä¸ä¸ªå¤ä»»å¡åå½æ¨¡åï¼ç¨äºç²¾ç¡®é¢æµç»æ¡å¤åæºçåæ°ï¼ä½ç½®ãåå¾åç½®ä¿¡åº¦ï¼ãè¿ç¡®ä¿äºçæåå®¹ä¸çå®ä¸ççç©ååç§æåå¸ç©çä¸ä¸è´ã</li>
<li><strong>LoRAå¾®è°æ©æ£æ¨¡åï¼</strong> å©ç¨LoRAï¼Low-Rank Adaptationï¼å¾®è°çç¨³å®æ©æ£ï¼Stable Diffusionï¼ä¿®å¤æ¨¡åï¼å¹¶æç¡®å°ä»¥é¢æµçåæºåæ°ä¸ºæ¡ä»¶ãè¿ä½¿å¾æ¨¡åè½å¤åç¡®éå»ºç©çä¸ä¸è´çç»æ¡å¤åæºåç©åä¼ªå½±ã</li>
<li><strong>åªå£°åæ³¨å¥ç­ç¥ï¼</strong> éç¨åªå£°åæ³¨å¥ï¼Noise Reinjectionï¼ææ¯ï¼ä»¥ç¼è§£å¨RGBç©ºé´è¿è¡åææ¶ï¼é®ç½©åºååæªé®ç½©åºåä¹é´å¯è½åºç°çè§è§ä¸ä¸è´ï¼ä»èæé«å¤ç»ç»æçè§è§è¿è´¯æ§åçå®æã</li>
<li><strong>å³æå³ç¨é¢å¤çæ¡æ¶ï¼</strong> LightsOut ä½ä¸ºç°æSIFRæ¡æ¶çå³æå³ç¨é¢å¤çæ­¥éª¤ï¼æ éé¢å¤éæ°è®­ç»å³å¯æ®éå¢å¼ºç°æSIFRæ¨¡åçæ§è½ï¼ä½¿å¶å¨æææ§åºæ¯ä¸­è¡¨ç°æ´ä½³ã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
LightsOut å¨å®éåå®æ§è¯ä¼°ä¸­åå±ç¤ºäºå¶åè¶çæ§è½ï¼</p>
<ul>
<li><strong>æ¾èæåSIFRæ§è½ï¼</strong> å¨Flare7Kæ°æ®éä¸ï¼LightsOut æ¾èæåäºç°æSIFRæ¹æ³çæ§è½ï¼å°¤å¶æ¯å¨æ²¡æåæºæåæºä¸å®æ´çåºæ¯ä¸­ãä¾å¦ï¼å¨æ²¡æåæºççå®å¾åä¸ï¼PSNRä»26.29 dBæåå°28.41 dBãè¿è¡¨æLightsOutè½å¤ææå¤çä¸å®æ´ç§ææåµã</li>
<li><strong>ä¼è¶çåæºé¢æµï¼</strong> å¤ä»»å¡åå½æ¨¡åå¨åæºé¢æµæ¹é¢è¡¨ç°åºè²ï¼mIoUåæ°é«äºåºçº¿U-Netæ¹æ³ï¼çå®å¾å0.6310 vs 0.6216ï¼åæå¾å0.6619 vs 0.6563ï¼ï¼éªè¯äºå¶åå½ç­ç¥çæææ§åå¯é æ§ã</li>
<li><strong>é«è´¨éçå¤ç»ç»æï¼</strong> å®æ§è¯ä¼°æ¾ç¤ºï¼LightsOut çæçå¤ç»å¾åå·ææ´é«çè§è§è¿è´¯æ§åçå®æï¼è½å¤åç¡®ææç©åä¼ªå½±ï¼å¹¶ä¸çå®åºæ¯ç´§å¯å¯¹é½ï¼ä¼äºå¶ä»æ åæ©æ£å¤ç»æ¹æ³ã</li>
<li><strong>å¯¹ä¸æ¸¸ä»»å¡çç§¯æå½±åï¼</strong> ç©åå»é¤çæ¹è¿ä¹æåäºä¸æ¸¸ä»»å¡ï¼å¦ç©ä½æ£æµï¼çæ§è½ï¼æé«äºæ£æµç½®ä¿¡åº¦ï¼å¹¶ä½¿ä¹åå ç©åä¼ªå½±èæ æ³æ£æµçç©ä½å¾ä»¥è¯å«ã</li>
<li><strong>æ¶èç ç©¶éªè¯äºåç»ä»¶çéè¦æ§ï¼</strong> æ¶èç ç©¶è¯å®äºåªå£°åæ³¨å¥ãRGBç©ºé´æ··åãåæºæ¡ä»¶æ¨¡åä»¥åLoRAå¾®è°ç­æ¯ä¸ªç»ä»¶å¯¹æç»æ§è½çè´¡ç®é½æ¯è³å³éè¦çã</li>
</ul>
<p>è¿äºç»æå±åè¯æäºLightsOutå¨è§£å³ä¸å®æ´åæºä¸ä¸æå¯¼è´çç©åå»é¤æææ¹é¢çæææ§ï¼ä¸ºSIFRé¢åæä¾äºä¸ä¸ªéç¨ä¸å¼ºå¤§çè§£å³æ¹æ¡ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
è®ºæä¸­æå°äºLightsOutçä¸ä¸ªä¸»è¦å±éæ§ï¼</p>
<ul>
<li><strong>è®¡ç®å¼éï¼</strong> å¢å çå¤ç»é¶æ®µä¼å¼å¥é¢å¤çè®¡ç®å¼éã</li>
</ul>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
ä¸ºäºè§£å³ä¸è¿°å±éæ§ï¼è®ºææåºäºæªæ¥çç ç©¶æ¹åï¼</p>
<ul>
<li><strong>ç«¯å°ç«¯ä¼åç­ç¥ï¼</strong> æªæ¥çå·¥ä½å¯ä»¥æ¢ç´¢ç«¯å°ç«¯ä¼åç­ç¥ï¼ä»¥åå°LightsOutæ¡æ¶å¸¦æ¥çè®¡ç®å¼éã</li>
</ul>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose LightsOut, a diffusion-based outpainting
framework tailored to enhance SIFR by reconstructing off-frame light sources.</li>
<li>Our method leverages a multitask regression module and LoRA fine-tuned
diffusion model to ensure realistic and physically consistent outpainting
results.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.15868v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.15868v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.15846v1'></a></p>
<h2 id="3dpr-single-image-3d-portrait-relight-using-generative-priors"><a href="https://arxiv.org/abs/2510.15846v1">3DPR: Single Image 3D Portrait Relight using Generative Priors</a></h2>
<p><strong>Authors:</strong> Pramod Rao, Abhimitra Meka, Xilong Zhou, Gereon Fox, Mallikarjun B R, Fangneng Zhan, Tim Weyrich, Bernd Bickel, Hanspeter Pfister, Wojciech Matusik, Thabo Beeler, Mohamed Elgharib, Marc Habermann, Christian Theobalt</p>
<p><strong>Published:</strong> 2025-10-17</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Rendering novel, relit views of a human head, given a monocular portrait
image as input, is an inherently underconstrained problem. The traditional
graphics solution is to explicitly decompose the input image into geometry,
material and lighting via differentiable rendering; but this is constrained by
the multiple assumptions and approximations of the underlying models and
parameterizations of these scene components. We propose 3DPR, an image-based
relighting model that leverages generative priors learnt from multi-view
One-Light-at-A-Time (OLAT) images captured in a light stage. We introduce a new
diverse and large-scale multi-view 4K OLAT dataset of 139 subjects to learn a
high-quality prior over the distribution of high-frequency face reflectance. We
leverage the latent space of a pre-trained generative head model that provides
a rich prior over face geometry learnt from in-the-wild image datasets. The
input portrait is first embedded in the latent manifold of such a model through
an encoder-based inversion process. Then a novel triplane-based reflectance
network trained on our lightstage data is used to synthesize high-fidelity OLAT
images to enable image-based relighting. Our reflectance network operates in
the latent space of the generative head model, crucially enabling a relatively
small number of lightstage images to train the reflectance model. Combining the
generated OLATs according to a given HDRI environment maps yields physically
accurate environmental relighting results. Through quantitative and qualitative
evaluations, we demonstrate that 3DPR outperforms previous methods,
particularly in preserving identity and in capturing lighting effects such as
specularities, self-shadows, and subsurface scattering. Project Page:
https://vcai.mpi-inf.mpg.de/projects/3dpr/</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Pramod Raoç­äººæ°åçè®ºæâ3DPR: Single Image 3D Portrait Relighting with Generative Priorsâçå¨é¢æè¦ã</p>
<hr />
<h3 id="3dpr-single-image-3d-portrait-relighting-with-generative-priors">è®ºææè¦ï¼3DPR: Single Image 3D Portrait Relighting with Generative Priors</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³ä»åå¼ èåå¾åçæå·ææ°é¢è§è§åéæ°æåææçäººå¤´è§å¾è¿ä¸åºææ¬ çº¦æé®é¢ãä¼ ç»çå¾å½¢å­¦æ¹æ³éè¿å¯å¾®åæ¸²æå°è¾å¥å¾ååè§£ä¸ºå ä½ãæè´¨ååç§ï¼ä½è¿ç§æ¹æ³åéäºåºå±æ¨¡åååºæ¯ç»ä»¶åæ°åçå¤ç§åè®¾åè¿ä¼¼ãæ ¸å¿ææå¨äºå¦ä½å¨ä¿æèº«ä»½ä¸è´æ§ãææå¤æåç§ææï¼å¦é«åãèªé´å½±åæ¬¡è¡¨é¢æ£å°ï¼çåæ¶ï¼å®ç°ç©çåç¡®çéæ°æååæ°é¢è§å¾åæã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
*   <strong>3DPRæ¨¡åï¼</strong> æåºäºä¸ç§åºäºå¾åçéæ°æåæ¨¡åï¼è¯¥æ¨¡åå©ç¨ä»å¤è§è§âä¸æ¬¡ä¸åç§âï¼One-Light-at-A-Time, OLATï¼å¾åä¸­å­¦ä¹ å°ççæåéªã
*   <strong>FaceOLATæ°æ®éï¼</strong> å¼å¥äºä¸ä¸ªæ°çãå¤æ ·åãå¤§è§æ¨¡ãå¤è§è§4Kåè¾¨çOLATæ°æ®éï¼åå«139ä¸ªåè¯èï¼ç¨äºå­¦ä¹ é«é¢é¢é¨åå°çåå¸çé«è´¨éåéªãè¯¥æ°æ®éå¨è§æ¨¡ãå¤æ ·æ§ååè¾¨çä¸è¶è¶äºç°æå¬å¼æ°æ®éï¼å¹¶é¦æ¬¡æä¾äºå¨é¢çå¤´é¨åå°çå»ºæ¨¡ï¼åæ¬å¤´åã
*   <strong>çæåéªçå©ç¨ï¼</strong> è®ºæå©ç¨é¢è®­ç»çæå¤´é¨æ¨¡åçæ½å¨ç©ºé´ï¼è¯¥æ¨¡åæä¾äºä»âéå¤âå¾åæ°æ®ä¸­å­¦ä¹ å°çä¸°å¯é¢é¨å ä½åéªãéè¿åºäºç¼ç å¨çåæ¼è¿ç¨ï¼å°è¾å¥èååµå¥å°è¯¥æ¨¡åçæ½å¨æµå½¢ä¸­ã
*   <strong>ä¸å¹³é¢åå°ç½ç»ï¼</strong> å¼å¥äºä¸ä¸ªæ°é¢çåºäºä¸å¹³é¢çåå°ç½ç»ï¼è¯¥ç½ç»å¨FaceOLATæ°æ®ä¸è¿è¡è®­ç»ï¼ç¨äºåæé«ä¿çOLATå¾åï¼ä»èå®ç°åºäºå¾åçéæ°æåãè¯¥åå°ç½ç»å¨çæå¤´é¨æ¨¡åçæ½å¨ç©ºé´ä¸­æä½ï¼ä½¿å¾ä»éç¸å¯¹è¾å°çåç§èå°å¾åå³å¯è®­ç»åå°æ¨¡åã
*   <strong>ç©çåç¡®çéæ°æåï¼</strong> éè¿å°çæçOLATå¾åæ ¹æ®ç»å®çHDRIç¯å¢å¾è¿è¡çº¿æ§ç»åï¼å®ç°äºç©çåç¡®çç¯å¢éæ°æåç»æã
*   <strong>SRç¼ç å¨åID-MRFæå¤±ï¼</strong> å¼å¥äºç¹å¾èåæ¨¡åï¼ESRï¼ä»¥ç»åé«é¢èº«ä»½ç¹å¾ååå°ç¹å¾ï¼é²æ­¢æ¨¡åè¿æåãåæ¶ï¼éç¨éå¼å¤æ ·åé©¬å°å¯å¤«éæºåºï¼ID-MRFï¼æå¤±ï¼é¼å±å±é¨ç¹å¾çº§å«çç¸ä¼¼æ§ï¼æææ¢å¤é«é¢ç»èã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>æ§è½è¶è¶ï¼</strong> éè¿å®éåå®æ§è¯ä¼°ï¼3DPRå¨èº«ä»½ä¿æåææå¤æåç§ææï¼å¦é«åãèªé´å½±åæ¬¡è¡¨é¢æ£å°ï¼æ¹é¢ä¼äºç°ææ¹æ³ã
*   <strong>é²æ£æ§åæ³åè½åï¼</strong> 3DPRå¨åç§åç§æ¡ä»¶ä¸ï¼åæ¬ç¨çåå½©è²åç§ï¼åè¡¨ç°åºé²æ£æ§ï¼è½å¤åç¡®åç°é´å½±ãé«ååå¶ä»å¤æææï¼å³ä½¿å¨éä¼ ç»åç§ä¸ä¹è½ä¿æèåççå®æåä¸è´æ§ã
*   <strong>OLATåæè´¨éï¼</strong> 3DPRåæçOLATå¾åè´¨éæ¾èä¼äºç°ææ¹æ³ï¼è½å¤ç¨³å¥å°æ³åå°è¯ä¼°æ°æ®éåâéå¤âåè¯èã
*   <strong>æçï¼</strong> 3DPRè½å¤ä»¥åæ¬¡ååä¼ æ­çæOLATå¾åï¼æ éèæ¶çæµè¯æ¶ä¼åï¼ä»èå¨æçåè´¨éä¹é´åå¾äºå®ç¨å¹³è¡¡ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>å¤´é¨åé¨è´¨éä¸éï¼</strong> å°½ç®¡FaceOLATæä¾äºå¨å¤´è¦çï¼ä½ç±äºEG3Dåéªçéå¶ï¼å¤´é¨åé¨çéæ°æåè´¨éææä¸éã
*   <strong>èå´éå¶ï¼</strong> ç®åçå·¥ä½ä»éäºé¢é¨åå°çï¼é¢é¨ãç¼çãå¤´ç®æ¯åï¼ï¼ä¸åæ¬å¤´é¥°åéä»¶ï¼å¦å¤´çãå¤ªé³éï¼ã
*   <strong>å¤´åç»èä¸ä¸è´ï¼</strong> ç»§æ¿äºEG3Då¨ä¸è´å»ºæ¨¡ç»åçº¤ç»´æ¹é¢çå°é¾ï¼æ°é¢è§å¾åæå¨å¤´ååºåå¯è½å­å¨å±é¨ä¸ä¸è´ï¼OLATæ¸²æçå°éä½å¨çº¿æ§ç»åæ¶å¯è½ç´¯ç§¯ä¸ºåªå£°æéªçï¼è¶åè¾¨çé¶æ®µå¨å¤´é¨æè½¬æ¶å¯è½åºç°âè·³å¨âç°è±¡ã
*   <strong>è§å¾ä¾èµææä¸ææ¾ï¼</strong> å°½ç®¡OLATè§£ç å¨Rdecä»¥è§è§ä¸ºæ¡ä»¶ï¼ä½è§å¾ä¾èµææï¼å¦é¼»æ¢åè¸é¢ä¸çææï¼ç¸å¯¹ä¸ææ¾ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>æ´å¨é¢ç3Dçæåéªï¼</strong> æ´åæ´å¨é¢ç3Dçæåéªä¸FaceOLATï¼ä»¥è§£å³å¤´é¨åé¨éæ°æåçå±éæ§ï¼å¹¶å®ç°å¨å¤´éæ°æåã
*   <strong>æ´å¹¿æ³çç©ä½åæè´¨ï¼</strong> å°åå°åéªæ©å±å°æ´å¹¿æ³çç©ä½åæè´¨ï¼ä»¥æ¯æå¤´é¥°åéä»¶çéæ°æåã
*   <strong>å¤´åå»ºæ¨¡æ¹è¿ï¼</strong> è§£å³å¤´ååºåçå±é¨ä¸ä¸è´æ§ãåªå£°åéªçé®é¢ï¼å¯è½éè¦æ´å¼ºçå¤´åé«é¢åéªåä¸é¨çå¯¹é½ç­ç¥ã
*   <strong>å¢å¼ºè§å¾ä¾èµæ§ï¼</strong> æ¹è¿è§å¾ä¾èµæ§ççç£åç®æ å½æ°ï¼ä»¥æ´å¼ºçå°è¡¨è¾¾è§å¾ä¾èµçº¿ç´¢ã</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>Rendering novel, relit views of a human head, given a monocular portrait
image as input, is an inherently underconstrained problem.</li>
<li>We propose 3DPR, an image-based
relighting model that leverages generative priors learnt from multi-view
One-Light-at-A-Time (OLAT) images captured in a light stage.</li>
<li>We introduce a new
diverse and large-scale multi-view 4K OLAT dataset of 139 subjects to learn a
high-quality prior over the distribution of high-frequency face reflectance.</li>
<li>Then a novel triplane-based reflectance
network trained on our lightstage data is used to synthesize high-fidelity OLAT
images to enable image-based relighting.</li>
<li>Through quantitative and qualitative
evaluations, we demonstrate that 3DPR outperforms previous methods,
particularly in preserving identity and in capturing lighting effects such as
specularities, self-shadows, and subsurface scattering.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.15846v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.15846v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.15783v1'></a></p>
<h2 id="recon-region-controllable-data-augmentation-with-rectification-and-alignment-for-object-detection"><a href="https://arxiv.org/abs/2510.15783v1">ReCon: Region-Controllable Data Augmentation with Rectification and Alignment for Object Detection</a></h2>
<p><strong>Authors:</strong> Haowei Zhu, Tianxiang Pan, Rui Qin, Jun-Hai Yong, Bin Wang</p>
<p><strong>Published:</strong> 2025-10-17</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>The scale and quality of datasets are crucial for training robust perception
models. However, obtaining large-scale annotated data is both costly and
time-consuming. Generative models have emerged as a powerful tool for data
augmentation by synthesizing samples that adhere to desired distributions.
However, current generative approaches often rely on complex post-processing or
extensive fine-tuning on massive datasets to achieve satisfactory results, and
they remain prone to content-position mismatches and semantic leakage. To
overcome these limitations, we introduce ReCon, a novel augmentation framework
that enhances the capacity of structure-controllable generative models for
object detection. ReCon integrates region-guided rectification into the
diffusion sampling process, using feedback from a pre-trained perception model
to rectify misgenerated regions within diffusion sampling process. We further
propose region-aligned cross-attention to enforce spatial-semantic alignment
between image regions and their textual cues, thereby improving both semantic
consistency and overall image fidelity. Extensive experiments demonstrate that
ReCon substantially improve the quality and trainability of generated data,
achieving consistent performance gains across various datasets, backbone
architectures, and data scales. Our code is available at
https://github.com/haoweiz23/ReCon .</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Haowei Zhuç­äººæ°åçè®ºæâReCon: Region-Controllable Data Augmentation with Rectification and Alignment for Object Detectionâçå¨é¢æè¦ã</p>
<hr />
<p><strong>è®ºææè¦ï¼ReCon: Region-Controllable Data Augmentation with Rectification and Alignment for Object Detection</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³è®­ç»é²æ£æç¥æ¨¡åï¼ç¹å«æ¯ç®æ æ£æµæ¨¡åï¼å¯¹å¤§è§æ¨¡é«è´¨éæ æ³¨æ°æ®éçä¸¥éä¾èµé®é¢ãè·åæ­¤ç±»æ°æ®éææ¬é«æä¸èæ¶ãå°½ç®¡çææ¨¡åå·²æä¸ºæ°æ®å¢å¼ºçå¼ºå¤§å·¥å·ï¼ä½ç°ææ¹æ³éå¸¸ä¾èµå¤æçåå¤çæå¨æµ·éæ°æ®éä¸è¿è¡å¤§éå¾®è°ï¼å¹¶ä¸å®¹æåºç°åå®¹-ä½ç½®ä¸å¹éåè¯­ä¹æ³é²é®é¢ï¼è¿éå¶äºçææ°æ®å¨ç®æ æ£æµä»»å¡ä¸­çæææ§ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
ä¸ºäºåæä¸è¿°éå¶ï¼ä½èæåºäº <strong>ReCon (Region-Controllable)</strong>ï¼ä¸ä¸ªæ°é¢çæ°æ®å¢å¼ºæ¡æ¶ï¼æ¨å¨å¢å¼ºç»æå¯æ§çææ¨¡åå¨ç®æ æ£æµä¸­çè½åãReCon çæ ¸å¿åæ°åæ¬ï¼</p>
<ul>
<li><strong>åºåå¼å¯¼çä¿®æ­£ (Region-Guided Rectification, RGR)ï¼</strong> ReCon å°åºåå¼å¯¼çä¿®æ­£ç´æ¥éæå°æ©æ£éæ ·è¿ç¨ä¸­ãå®å©ç¨é¢è®­ç»æç¥æ¨¡åçåé¦ï¼å¨æ©æ£éæ ·è¿ç¨ä¸­è¯å«å¹¶ä¿®æ­£å¾åä¸­éè¯¯çæçåºåãéè¿å°éæ ·å¾åä¸çå®æ æ³¨è¿è¡æ¯è¾ï¼å¹¶æ³¨å¥å¸¦åªå£°ççå®æ°æ®ç¹æ¥ä¿®æ­£éè¯¯åºåï¼ä»èæé«åç¡®æ§ï¼åæ¶ä¿æåå®¹å¤æ ·æ§ã</li>
<li><strong>åºåå¯¹é½çäº¤åæ³¨æå (Region-Aligned Cross-Attention, RACA)ï¼</strong> ä¸ºäºç¼è§£è¯­ä¹æ³é²é®é¢ï¼ReCon å¼å¥äºåºåå¯¹é½çäº¤åæ³¨æåæºå¶ãè¯¥æºå¶å¨çæè¿ç¨ä¸­å¼ºå¶å¾ååºåä¸å¶å¯¹åºçææ¬æè¿°ï¼æå¶ä»çº¿ç´¢ï¼ä¹é´å®ç°ç©ºé´-è¯­ä¹å¯¹é½ï¼ä»èæé«è¯­ä¹ä¸è´æ§åæ´ä½å¾åä¿çåº¦ã</li>
<li><strong>è®­ç»æ å³åå³æå³ç¨ï¼</strong> ReCon çä¸ä¸ªæ¾èä¼å¿æ¯å®æ éé¢å¤çè®­ç»å³å¯å¢å¼ºç°æç»æå¯æ§çææ¨¡åçè½åï¼ä½¿å¶æä¸ºä¸ä¸ªå³æå³ç¨çè§£å³æ¹æ¡ï¼å°¤å¶éç¨äºæ°æ®ç¨ç¼ºçåºæ¯ã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
å¹¿æ³çå®éªè¯æ ReCon æ¾èæé«äºçææ°æ®çè´¨éåå¯è®­ç»æ§ï¼å¹¶å¨åç§æ°æ®éãéª¨å¹²æ¶æåæ°æ®è§æ¨¡ä¸å®ç°äºæç»­çæ§è½æåã</p>
<ul>
<li><strong>ç®æ æ£æµæ§è½æåï¼</strong> ReCon ä¸ ControlNet ç»ååï¼å¨ COCO æ°æ®éä¸ç mAP è¾¾å° 35.5ï¼è¶è¶äº GeoDiffusion ç 34.8ãä¸ GLIGEN ç»ååï¼mAP ä» 34.6 æåå° 35.5ãè¿è¡¨æ ReCon è½å¤çææ´é«è´¨éçè®­ç»æ ·æ¬ï¼ä»èæ¾èæåç®æ æ£æµæ§è½ã</li>
<li><strong>æ°æ®ç¨ç¼ºåºæ¯ä¸çæçï¼</strong> å¨æ°æ®ç¨ç¼ºè®¾ç½®ä¸­ï¼ä¾å¦ä»ä½¿ç¨ 10% ç COCO æ°æ®ï¼ï¼ReCon å° mAP ä» 18.5% æåå° 21.7%ãä¸åºçº¿æ¹æ³ç¸æ¯ï¼ReCon å¨ä½¿ç¨æ´å°å¢å¼ºæ ·æ¬çæåµä¸ï¼å®ç°äºå¯æ¯çè³æ´å¥½çæ§è½ï¼çªåºäºå¶æ°æ®å¢å¼ºçæçã</li>
<li><strong>ç»ä»¶æææ§ï¼</strong> æ¶èå®éªè¯å®ï¼RGR å RACA åæ¾èæåäºä¸æ¸¸æ¨¡åçæ§è½ï¼éè¿ç¡®ä¿çææ ·æ¬ä¸æ æ³¨ä¹é´çä¸è´æ§ï¼æé«äºåææ°æ®çè´¨éã</li>
<li><strong>å¾åä¿çåº¦åæ æ³¨ä¸è´æ§ï¼</strong> å®æ§ç»æè¡¨æï¼ReCon æ¾èæ¹åäºçææ ·æ¬çå¾åä¿çåº¦åå®ä½åç¡®æ§ï¼ææé¿åäºç°æç»ææ§å¶æ¹æ³ä¸­å¸¸è§çåºåä¿®æ­£ä¸ç²¾ç¡®åè¯­ä¹æ³é²é®é¢ã</li>
</ul>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
è®ºæå¨éå½Aä¸­è®¨è®ºäºReConçå±éæ§ï¼</p>
<ul>
<li><strong>è®¡ç®æ¶é´å¢å ï¼</strong> å°½ç®¡ ReCon æé«äº FID åæ°å¹¶æ¹åäºæ£æµå¨ç mAPï¼ä½å®å¯è½ä¼å¢å è®¡ç®æ¶é´ï¼å°¤å¶æ¯å¨æ°æ®éå¢é¿æ¶ã</li>
<li><strong>é¢å¤çæç¥æ¨¡åï¼</strong> éè¦ä¸ä¸ªé¢å¤çæç¥æ¨¡åï¼è¿å¢å äºå¼åææ¬ãä½èæå°å·²å¼å¥å éææ¯ï¼å¦å¿«ééæ ·å¨åè½»éçº§æç¥æ¨¡åï¼æ¥éä½è¿äºææ¬ã</li>
<li><strong>æ½å¨çç¤¾ä¼å½±åï¼</strong> çææ¨¡åå¯è½ç»§æ¿å¤§åãæªç­éçè§è§-è¯­è¨æ°æ®éä¸­çç¤¾ä¼åè§åå»æ¿å°è±¡ï¼ä»èäº§çæ­§è§æ§è¾åºãReCon éè¿åºåä¿®æ­£åå¯¹é½æ¥æé«åå®¹åç¡®æ§å¹¶åå°åè§ã</li>
<li><strong>æ»¥ç¨é£é©ï¼</strong> åæå¾åå¯è½è¢«ç¨äºæ·±åº¦ä¼ªé ç­ç®çï¼ä¼ æ­éè¯¯ä¿¡æ¯å¹¶ç ´åç¤¾ä¼ä¿¡ä»»ãä½èå¼ºè°éè¦å»ºç«æ³è§åæä½³å®è·µæ¥ç¡®ä¿åææ°æ®æ¨¡åçè´è´£ä»»åå»ºåä½¿ç¨ã</li>
</ul>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
å°½ç®¡è®ºææ²¡ææç¡®ååºæªæ¥çç ç©¶æ¹åï¼ä½ä»å¶å±éæ§åè´¡ç®ä¸­å¯ä»¥æ¨æ­åºä»¥ä¸å ç¹ï¼</p>
<ul>
<li><strong>è¿ä¸æ­¥ä¼åè®¡ç®æçï¼</strong> æ¢ç´¢æ´é«æçæç¥æ¨¡åï¼å¦ EfficientSAMï¼æéæåå­åè®¡ç®é«æçæ³¨æååºï¼å¦ xFormersï¼ï¼ä»¥è¿ä¸æ­¥éä½ ReCon å¼å¥çé¢å¤è®¡ç®å¼éã</li>
<li><strong>åè§æ£æµä¸ç¼è§£ï¼</strong> æ·±å¥ç ç©¶çææ¨¡åä¸­çåè§æ£æµåç¼è§£æºå¶ï¼ä»¥ç¡®ä¿åææ°æ®å¨ç¤¾ä¼å¬å¹³åä¼¦çæ¹é¢æ´å è´è´£ã</li>
<li><strong>ä¸å¶ä»çææ¡æ¶çç»åï¼</strong> æ¢ç´¢ ReCon ä¸å¶ä»æ°å´çç»æå¯æ§çææ¨¡åææ´åè¿çæ©æ£æ¨¡åç»åçæ½åï¼ä»¥è¿ä¸æ­¥æåæ§è½åå¤æ ·æ§ã</li>
<li><strong>æ©å±å°å¶ä»ä»»å¡ï¼</strong> å° ReCon çåºåå¯æ§æ°æ®å¢å¼ºæ¡æ¶åºç¨äºé¤äºç®æ æ£æµä¹å¤çå¶ä»æç¥ä»»å¡ï¼ä¾å¦å®ä¾åå²æè¯­ä¹åå²ï¼ä»¥éªè¯å¶éç¨æ§ã</li>
<li><strong>æ´ç²¾ç»çåºåæ§å¶ï¼</strong> æ¢ç´¢æ´ç²¾ç»çåºåæ§å¶ç­ç¥ï¼ä¾å¦åºäºæ´å¤æè¯­ä¹å±æ§çä¿®æ­£ï¼ä»¥åºå¯¹æ´å·æææ§ççæåºæ¯ã</li>
</ul>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>To
overcome these limitations, we introduce ReCon, a novel augmentation framework
that enhances the capacity of structure-controllable generative models for
object detection.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.15783v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.15783v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.15725v1'></a></p>
<h2 id="dgme-t-directional-grid-motion-encoding-for-transformer-based-historical-camera-movement-classification"><a href="https://arxiv.org/abs/2510.15725v1">DGME-T: Directional Grid Motion Encoding for Transformer-Based Historical Camera Movement Classification</a></h2>
<p><strong>Authors:</strong> Tingyu Lin, Armin Dadras, Florian Kleber, Robert Sablatnig</p>
<p><strong>Published:</strong> 2025-10-17</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, eess.IV</p>
<p><strong>Abstract:</strong></p>
<p>Camera movement classification (CMC) models trained on contemporary,
high-quality footage often degrade when applied to archival film, where noise,
missing frames, and low contrast obscure motion cues. We bridge this gap by
assembling a unified benchmark that consolidates two modern corpora into four
canonical classes and restructures the HISTORIAN collection into five balanced
categories. Building on this benchmark, we introduce DGME-T, a lightweight
extension to the Video Swin Transformer that injects directional grid motion
encoding, derived from optical flow, via a learnable and normalised late-fusion
layer. DGME-T raises the backbone's top-1 accuracy from 81.78% to 86.14% and
its macro F1 from 82.08% to 87.81% on modern clips, while still improving the
demanding World-War-II footage from 83.43% to 84.62% accuracy and from 81.72%
to 82.63% macro F1. A cross-domain study further shows that an intermediate
fine-tuning stage on modern data increases historical performance by more than
five percentage points. These results demonstrate that structured motion priors
and transformer representations are complementary and that even a small,
carefully calibrated motion head can substantially enhance robustness in
degraded film analysis. Related resources are available at
https://github.com/linty5/DGME-T.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°å¯¹è¿ç¯è®ºææè¦è¿è¡åæã</p>
<hr />
<p><strong>è®ºææè¦åæï¼DGME-T: Directional Grid Motion Encoding for Transformer-Based Historical Camera Movement Classification</strong></p>
<p><strong>1. è®ºæä¸»è¦è´¡ç®çç®æ´æ»ç» (2-3 å¥è¯)</strong></p>
<p>è¿ç¯è®ºæä¸»è¦è§£å³äºå¨åå²æ¡£æ¡å½±çä¸è¿è¡æåæºè¿å¨åç±» (CMC) æ¶ï¼ç±äºä½è´¨éãåªå£°åç¼ºå¤±å¸§å¯¼è´çæ§è½ä¸éé®é¢ãä½èéè¿æå»ºä¸ä¸ªç»ä¸çåºåæ°æ®éï¼å¹¶å¼å¥äºDGME-Tæ¨¡åï¼è¿æ¯ä¸ä¸ªåºäºVideo Swin Transformerçè½»éçº§æ©å±ï¼å®éè¿å¯å­¦ä¹ çææèåå±æ³¨å¥äºåºäºåæµçæ¹åç½æ ¼è¿å¨ç¼ç ãDGME-Tæ¾èæåäºå¨ç°ä»£ååå²å½±çä¸çCMCæ§è½ï¼è¯æäºç»æåè¿å¨åéªä¸Transformerè¡¨ç¤ºçäºè¡¥æ§ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³å­¦æ¹æ³</strong></p>
<p>å³é®åæ°å¨äº <strong>âDirectional Grid Motion Encoding (DGME)â</strong> ä»¥åå¶ä¸ <strong>Transformer (Video Swin Transformer)</strong> çç»åæ¹å¼ãå·ä½æ¥è¯´ï¼</p>
<ul>
<li><strong>DGMEï¼</strong> ä»åæµä¸­æåæ¹åç½æ ¼è¿å¨ç¼ç ï¼è¿æ¯ä¸ç§ç»æåçè¿å¨åéªä¿¡æ¯ãå®å°åå§åç´ çº§å«çè¿å¨ä¿¡æ¯æ½è±¡ä¸ºæ´é²æ£ãæ´å·æ¹åæ§çç½æ ¼è¡¨ç¤ºï¼ä»èæ´å¥½å°åºå¯¹åå²å½±çä¸­çåªå£°åæ¨¡ç³ã</li>
<li><strong>è½»éçº§æ©å±ä¸ææèåï¼</strong> DGME-Tä¸æ¯ä»å¤´æå»ºä¸ä¸ªå¤æçæ¨¡åï¼èæ¯ä½ä¸ºVideo Swin Transformerçä¸ä¸ªâè½»éçº§æ©å±âï¼éè¿ä¸ä¸ªâå¯å­¦ä¹ åå½ä¸åçææèåå±âå°DGMEæ³¨å¥å°ä¸»å¹²ç½ç»ä¸­ãè¿ç§è®¾è®¡æ¢å©ç¨äºTransformerå¼ºå¤§çæ¶ç©ºç¹å¾å­¦ä¹ è½åï¼åé¿åäºå¯¹æ´ä¸ªæ¨¡åè¿è¡å¤§è§æ¨¡ä¿®æ¹ï¼åæ¶éè¿ææèåç¡®ä¿äºè¿å¨ä¿¡æ¯ä¸è§è§ç¹å¾çææç»åã</li>
<li><strong>ç»ä¸åºåæ°æ®éï¼</strong> è®ºæè¿æå»ºäºä¸ä¸ªç»ä¸çåºåæ°æ®éï¼æ´åäºç°ä»£è¯­æåºå¹¶éæ°ç»ç»äºHISTORIANæ°æ®éï¼ä¸ºåå²å½±çCMCç ç©¶æä¾äºæ´æ ååçè¯ä¼°å¹³å°ã</li>
</ul>
<p><strong>3. å¯¹è¯¥é¢åçæ½å¨å½±å</strong></p>
<ul>
<li><strong>æååå²å½±çåæè½åï¼</strong> è¿æ¯æç´æ¥çå½±åãéè¿æä¾æ´é²æ£çCMCæ¨¡åï¼å¯ä»¥æ´å¥½å°çè§£ååæå¤§éæªè¢«ååå©ç¨çåå²å½±åèµæï¼ä¾å¦çºªå½çãæ°é»æ¡£æ¡ãå®¶åº­å½åç­ã</li>
<li><strong>æ¨å¨ä½è´¨éè§é¢åæï¼</strong> DGME-Tçæ¹æ³å­¦å¯è½ä¸ä»ä»å±éäºåå²å½±çï¼å¯¹äºå¶ä»ä½è´¨éãé«åªå£°ææ°æ®ä¸å®æ´çè§é¢åæä»»å¡ï¼å¦çæ§è§é¢ãæ äººæºææãåç¼©è§é¢ç­ï¼ä¹å·æåé´æä¹ã</li>
<li><strong>è¿å¨åéªä¸æ·±åº¦å­¦ä¹ çèåï¼</strong> è®ºæå¼ºè°äºâç»æåè¿å¨åéªåTransformerè¡¨ç¤ºæ¯äºè¡¥çâï¼è¿ä¸ºæªæ¥å°ä¼ ç»è®¡ç®æºè§è§ä¸­çè¿å¨åæææ¯ä¸ç°ä»£æ·±åº¦å­¦ä¹ æ¨¡åç»åæä¾äºæ°çæè·¯åå®è¯æ¯æã</li>
<li><strong>åºåæ°æ®éçè´¡ç®ï¼</strong> ç»ä¸çåºåæ°æ®éå°ä¿è¿è¯¥é¢åçç ç©¶è¿å±ï¼ä¸ºä¸åæ¨¡åæä¾å¬å¹³çæ¯è¾å¹³å°ã</li>
</ul>
<p><strong>4. å¯è½åçäºè¿é¡¹ç ç©¶çç¸å³é¢åæåºç¨</strong></p>
<ul>
<li><strong>çµå½±å­¦ä¸åªä½æ¡£æ¡ç®¡çï¼</strong> èªå¨åç±»åæ£ç´¢åå²å½±çä¸­çç¹å®æåæºè¿å¨ï¼å¦å¹³ç§»ãå¾æãç¼©æ¾ï¼ï¼æå©äºçµå½±åæãåå®¹ç´¢å¼åæ°å­åæ¡£æ¡çç»ç»ã</li>
<li><strong>è§é¢ä¿®å¤ä¸å¢å¼ºï¼</strong> äºè§£æåæºè¿å¨æå©äºå¨è§é¢ä¿®å¤è¿ç¨ä¸­æ´åç¡®å°è¿è¡è¿å¨è¡¥å¿ãå»æå¨æè¶åè¾¨çã</li>
<li><strong>åå®¹çè§£ä¸æ£ç´¢ï¼</strong> å¨å¤§è§æ¨¡è§é¢æ°æ®åºä¸­ï¼æåæºè¿å¨æ¯éè¦çè¯­ä¹ä¿¡æ¯ï¼å¯ä»¥ç¨äºæ´ç²¾ç»çåå®¹æ£ç´¢ã</li>
<li><strong>è¿å¨åæä¸è¡ä¸ºè¯å«ï¼</strong> è½ç¶ä¸»è¦éå¯¹æåæºè¿å¨ï¼ä½å¶å¤çåªå£°è¿å¨ä¿¡æ¯çæ¹æ³å¯è½å¯¹å¶ä»éè¦ä»ä½è´¨éè§é¢ä¸­æåè¿å¨æ¨¡å¼çä»»å¡ï¼å¦äººä½è¡ä¸ºè¯å«ãç©ä½è½¨è¿¹è·è¸ªï¼ææå¯åã</li>
<li><strong>èºæ¯å²ä¸æåéäº§ï¼</strong> åæåå²èºæ¯ä½åææåéäº§è§é¢ä¸­çææææ³åé£æ ¼ã</li>
</ul>
<p><strong>5. ä»æè¦ä¸­å¯ä»¥æ¨æ­åºçå±éæ§</strong></p>
<ul>
<li><strong>âè½»éçº§æ©å±âçå±éæ§ï¼</strong> å°½ç®¡è½»éçº§æ¯ä¼ç¹ï¼ä½å®å¯è½æå³çDGME-Tçè¿å¨ç¼ç è½ååéäºå¶ä½ä¸ºâæ©å±âçå®ä½ï¼å¯è½ä¸å¦ä¸ä¸ªä»å¤´è®¾è®¡ãæ´å¤æçè¿å¨ç¹å¾æåç½ç»ã</li>
<li><strong>åæµçä¾èµæ§ï¼</strong> DGMEæ¯âderived from optical flowâãåæµæ¬èº«å¨ä½å¯¹æ¯åº¦ãå¿«éè¿å¨æé®æ¡ä¸¥éçåºæ¯ä¸å¯è½ä¸åç¡®ãè½ç¶DGMEæ¨å¨æé«é²æ£æ§ï¼ä½å¶ä¸æ¸¸çåæµä¼°è®¡è¯¯å·®ä»å¯è½ä¼ éã</li>
<li><strong>ç¹å®æ°æ®éçæ³åæ§ï¼</strong> å°½ç®¡æå»ºäºç»ä¸åºåï¼ä½ä¸»è¦å³æ³¨çæ¯âWorld-War-II footageâåâmodern clipsâãå¯¹äºå¶ä»ç±»åçåå²å½±çï¼å¦æ´æ©æçæ å£°çµå½±ãä¸åå°åçæ¡£æ¡ï¼ææ´æç«¯éåçè§é¢ï¼å¶æ§è½æ¯å¦è½ä¿æä»ééªè¯ã</li>
<li><strong>âææèåâçæ½å¨éå¶ï¼</strong> ææèåè½ç¶ç®åææï¼ä½å¯è½ä¸å¦æ©ææä¸­æèåè½å¤æ´æ·±å±æ¬¡å°æ´åè¿å¨åè§è§ä¿¡æ¯ãè¿å¯è½éå¶äºæ¨¡åå¨æäºå¤æåºæ¯ä¸å¯¹è¿å¨ç»èççè§£ã</li>
<li><strong>è®¡ç®ææ¬æªæåï¼</strong> æè¦ä¸­æªæåDGME-Tç¸å¯¹äºVideo Swin Transformerçé¢å¤è®¡ç®ææ¬ï¼åæ¬åæµè®¡ç®åDGMEæ¨¡åï¼ãè½ç¶è¯´æ¯âè½»éçº§âï¼ä½å·ä½å½±åå¦ä½ä»éæ¥éå¨æã</li>
</ul>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Building on this benchmark, we introduce DGME-T, a lightweight
extension to the Video Swin Transformer that injects directional grid motion
encoding, derived from optical flow, via a learnable and normalised late-fusion
layer.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.15725v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.15725v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.15673v1'></a></p>
<h2 id="valeo-near-field-a-novel-dataset-for-pedestrian-intent-detection"><a href="https://arxiv.org/abs/2510.15673v1">Valeo Near-Field: a novel dataset for pedestrian intent detection</a></h2>
<p><strong>Authors:</strong> Antonyo Musabini, Rachid Benmokhtar, Jagdish Bhanushali, Victor Galizzi, Bertrand Luvison, Xavier Perrotton</p>
<p><strong>Published:</strong> 2025-10-17</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>This paper presents a novel dataset aimed at detecting pedestrians'
intentions as they approach an ego-vehicle. The dataset comprises synchronized
multi-modal data, including fisheye camera feeds, lidar laser scans, ultrasonic
sensor readings, and motion capture-based 3D body poses, collected across
diverse real-world scenarios. Key contributions include detailed annotations of
3D body joint positions synchronized with fisheye camera images, as well as
accurate 3D pedestrian positions extracted from lidar data, facilitating robust
benchmarking for perception algorithms. We release a portion of the dataset
along with a comprehensive benchmark suite, featuring evaluation metrics for
accuracy, efficiency, and scalability on embedded systems. By addressing
real-world challenges such as sensor occlusions, dynamic environments, and
hardware constraints, this dataset offers a unique resource for developing and
evaluating state-of-the-art algorithms in pedestrian detection, 3D pose
estimation and 4D trajectory and intention prediction. Additionally, we provide
baseline performance metrics using custom neural network architectures and
suggest future research directions to encourage the adoption and enhancement of
the dataset. This work aims to serve as a foundation for researchers seeking to
advance the capabilities of intelligent vehicles in near-field scenarios.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Antonyo Musabiniç­äººæ°åçè®ºæâValeo Near-Field: a novel dataset for pedestrian intent detectionâçå¨é¢æè¦ã</p>
<p><strong>è®ºææè¦ï¼Valeo Near-Field: è¡äººæå¾æ£æµçæ°åæ°æ®é</strong></p>
<p>è¿ç¯è®ºæä»ç»äºâValeo Near-Fieldâæ°æ®éï¼æ¨å¨è§£å³æºè½è½¦è¾å¨è¿åºåºæ¯ä¸­å®å¨ææå°ä¸è¡äººäº¤äºçå³é®ææï¼ç¹å«æ¯é¢æµè¡äººæ¥è¿åæ¾è½¦è¾æ¶çæå¾ã</p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥ç ç©¶æ¨å¨è§£å³ç°ææ°æ®éå¨è¡äººæ£æµåæå¾é¢æµæ¹é¢çå±éæ§ï¼è¿äºæ°æ®ééå¸¸ä¾§éäºåºæ¬æ£æµæéæå§¿æï¼ç¼ºä¹å¤æ¨¡æä¼ æå¨éæï¼ä¸æªä¸é¨éå¯¹è½¦è¾è¿åºåºæ¯ä¸­è¡äººä¸éæ­¢è½¦è¾çå¨æäº¤äºãæ ¸å¿é®é¢æ¯ï¼å¦ä½åç¡®ãé²æ£å°æ£æµè¡äººæå¾ï¼å°¤å¶æ¯å¨ä¼ æå¨é®æ¡ãå¨æç¯å¢åç¡¬ä»¶éå¶ç­çå®ä¸çææä¸ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
*   <strong>æ°åå¤æ¨¡ææ°æ®éï¼</strong> Valeo Near-Fieldæ¯é¦ä¸ªå¬å¼å¯ç¨çæ°æ®éï¼ä¸é¨ç¨äºæ£æµè¡äººæ¥è¿èªå¨é©¾é©¶è½¦è¾æ¶çæå¾ãå®åå«åæ­¥çå¤æ¨¡ææ°æ®ï¼åæ¬é±¼ç¼æåå¤´ãæ¿åé·è¾¾æ«æãè¶å£°æ³¢ä¼ æå¨è¯»æ°ä»¥ååºäºè¿å¨ææç3Dèº«ä½å§¿æã
*   <strong>è¯¦ç»ä¸åæ­¥çæ æ³¨ï¼</strong> æ°æ®éæä¾äºä¸é±¼ç¼å¾ååæ­¥çè¯¦ç»3Dèº«ä½å³èä½ç½®æ æ³¨ï¼ä»¥åä»æ¿åé·è¾¾æ°æ®ä¸­æåçç²¾ç¡®3Dè¡äººä½ç½®ï¼è¿ä¸ºæç¥ç®æ³æä¾äºå¼ºå¤§çåºåæµè¯è½åã
*   <strong>çå®ä¸çåºæ¯è¦çï¼</strong> æ°æ®æ¶éæ¶µçäºå¤æ ·åççå®ä¸çåºæ¯ï¼åæ¬å®¤ååå®¤å¤åè½¦åºï¼å¹¶ç±13ååä¸èæ§è¡æå¾åéæå¾åºæ¯ï¼æ¨¡æäºè¡äººä¸éæ­¢è½¦è¾çäº¤äºã
*   <strong>åæ­¥ææ¯ï¼</strong> è®ºææåºäºä¸ç§å®å¶çåæ­¥ææ¯ï¼éè¿ç¹å®çæèå¨ä½å°è¿å¨æææ°æ®ä¸è½¦è¾ä¼ æå¨æ°æ®è¿è¡æ¶é´åæ­¥ï¼å¹¶ç»åæ¿åé·è¾¾æ«æè¿è¡3Dä½ç½®åæ¹åçæå¨æ æ³¨ï¼ä»¥æå°åè¿å¨æææ°æ®ä¸­ç´¯ç§¯çä½ç½®è¯¯å·®ã
*   <strong>ç»¼ååºåæµè¯å¥ä»¶ï¼</strong> éæ°æ®éä¸èµ·åå¸äºä¸ä¸ªå¨é¢çåºåæµè¯å¥ä»¶ï¼åå«éå¯¹åç¡®æ§ãæçååµå¥å¼ç³»ç»å¯æ©å±æ§çè¯ä¼°ææ ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
è®ºææä¾äºä½¿ç¨YOLOXè¿è¡2Dè¡äººæ£æµãViTPoseè¿è¡2Déª¨æ¶ä¼°è®¡ãä»¥ååºäºTransformerçæ¹æ³è¿è¡3Då§¿æä¼°è®¡å3Dè¡äººå®ä½çåºçº¿æ§è½ææ ã
*   <strong>3Dè¡äººå®ä½ï¼</strong> å¨VNFå¬å±æµè¯éä¸ï¼3Då®ä½çå¹³åè·ç¦»è¯¯å·®ï¼ADEï¼å¨ä¸åæ£æµåºååç¨ä¾ï¼å®¤å/å®¤å¤ãç·æ§/å¥³æ§ï¼ä¸­ä¿æä¸è´ãå¨5-10ç±³åºåçåç¡®æ§é«äº0-5ç±³åºåï¼è¿å¯è½ä¸è¯¥åºåè¡äººéå¸¸è½è¢«å¤ä¸ªæåå¤´æè·ä»¥åè½¦è¾å¼é¨é æçé®æ¡è¾å°æå³ã
*   <strong>éª¨æ¶å§¿æä¼°è®¡ï¼</strong> 3Då§¿æä¼°è®¡çå¹³åå³èä½ç½®è¯¯å·®ï¼MPJPEï¼éè·ç¦»å¢å èå¢å ï¼ä½å¨0-5ç±³åºåï¼éå¸¸è¿çåºåï¼è¯¯å·®ä¹è¾é«ï¼è¿å¯è½æ¯å ä¸ºé¨åå³èå¨æåå¤´è§éä¹å¤ãå®¤ååå®¤å¤åºæ¯ä»¥åä¸åæ§å«åä¸èçæ§è½ç¸ä¼¼ã
*   <strong>èº«ä½é«åº¦è¯¯å·®ï¼</strong> 3Då§¿æä¼°è®¡å¨é¢æµçè¡äººé«åº¦éå¸¸æ¯å®éå¼é«çº¦7%ãå¥³æ§åä¸èçè¯¯å·®éå¸¸æ´å¤§ï¼è¿å¯è½ä¸æ°æ®éä¸­å¥³æ§èº«é«æ®éä½äºç·æ§æå³ã
è¿äºåºçº¿ç»æä¸ºæªæ¥çç ç©¶ååºåæµè¯æä¾äºåèç¹ï¼å±ç¤ºäºå¨è¿åºåºæ¯ä¸­è¿è¡è¡äººæç¥ä»»å¡çåæ­¥å¯è¡æ§ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>åºæ¯éå¶ï¼</strong> æ°æ®éä¸»è¦å³æ³¨éæ­¢è½¦è¾çäº¤äºï¼å¨æåºæ¯ï¼å¦è½¦è¾ç§»å¨ï¼å°æªæ¶µçã
*   <strong>è¿å¨æææ¼ç§»ï¼</strong> å°½ç®¡è¿å¨æææä¾äºç²¾ç¡®ç3Då§¿æä¼°è®¡ï¼ä½ç´¯ç§¯çæ¼ç§»å¯è½éæ¶é´å¼å¥è½»å¾®çå®ä½è¯¯å·®ã
*   <strong>ç¯å¢æ¡ä»¶ï¼</strong> æ°æ®éçå½åèå´å¹¶æªå¹¿æ³æ¶µçå¤æ ·åçå¤©æ°æ¡ä»¶ï¼è¿å¨çå®ä¸çåºç¨ä¸­æ¯ä¸ä¸ªå³é®å ç´ ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>å¢å¼ºä¸ªä½è¡äººæç¥ä»»å¡ï¼</strong> æ¹è¿2Déª¨æ¶æ£æµåè·è¸ªã3Då§¿ææåãå¤æåå¤´3Då®ä½ä»¥åè¡äººè½¨è¿¹åæå¾é¢æµã
*   <strong>ç«¯å°ç«¯å¤çæµç¨ï¼</strong> ç ç©¶äººåå¯ä»¥æ¢ç´¢èåä¼åææè¿äºä»»å¡çç«¯å°ç«¯å¤çæµç¨ï¼ä»¥æé«é²æ£æ§ã
*   <strong>æ°æ®å¢å¼ºï¼</strong> å©ç¨åææ°æ®å¢å¼ºæèªçç£å­¦ä¹ ææ¯ï¼ä»¥æé«æ¨¡åçæ³åè½åã
*   <strong>æ©å±æ°æ®éï¼</strong> æªæ¥çæ°æ®éè¿­ä»£åºæ¨å¨åå«æ´å¹¿æ³çç¯å¢æ¡ä»¶ï¼ä»¥æé«æ¨¡åå¨çå®ä¸çåºç¨ä¸­çé²æ£æ§åæ³åè½åã</p>
<p>æ»èè¨ä¹ï¼Valeo Near-Fieldæ°æ®éä¸ºæºè½è½¦è¾å¨è¿åºåºæ¯ä¸­çè¡äººæç¥åå®å¨ç ç©¶å¥ å®äºå®è´µçåºç¡ï¼éè¿æä¾ç¬ç¹çå¤æ¨¡ææ°æ®ååºåæµè¯ï¼æ¨å¨äºè¡äººæå¾æ£æµã3Då§¿æä¼°è®¡å4Dè½¨è¿¹ä¸æå¾é¢æµé¢åçåå±ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>This paper presents a novel dataset aimed at detecting pedestrians'
intentions as they approach an ego-vehicle.</li>
<li>Key contributions include detailed annotations of
3D body joint positions synchronized with fisheye camera images, as well as
accurate 3D pedestrian positions extracted from lidar data, facilitating robust
benchmarking for perception algorithms.</li>
<li>By addressing
real-world challenges such as sensor occlusions, dynamic environments, and
hardware constraints, this dataset offers a unique resource for developing and
evaluating state-of-the-art algorithms in pedestrian detection, 3D pose
estimation and 4D trajectory and intention prediction.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.15673v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.15673v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.15602v1'></a></p>
<h2 id="quantized-fca-efficient-zero-shot-texture-anomaly-detection"><a href="https://arxiv.org/abs/2510.15602v1">Quantized FCA: Efficient Zero-Shot Texture Anomaly Detection</a></h2>
<p><strong>Authors:</strong> Andrei-Timotei Ardelean, Patrick RÃ¼ckbeil, Tim Weyrich</p>
<p><strong>Published:</strong> 2025-10-17</p>
<p><strong>Categories:</strong> cs.CV, I.4.7; I.2.10; I.3.8</p>
<p><strong>Abstract:</strong></p>
<p>Zero-shot anomaly localization is a rising field in computer vision research,
with important progress in recent years. This work focuses on the problem of
detecting and localizing anomalies in textures, where anomalies can be defined
as the regions that deviate from the overall statistics, violating the
stationarity assumption. The main limitation of existing methods is their high
running time, making them impractical for deployment in real-world scenarios,
such as assembly line monitoring. We propose a real-time method, named QFCA,
which implements a quantized version of the feature correspondence analysis
(FCA) algorithm. By carefully adapting the patch statistics comparison to work
on histograms of quantized values, we obtain a 10x speedup with little to no
loss in accuracy. Moreover, we introduce a feature preprocessing step based on
principal component analysis, which enhances the contrast between normal and
anomalous features, improving the detection precision on complex textures. Our
method is thoroughly evaluated against prior art, comparing favorably with
existing methods. Project page:
https://reality.tf.fau.de/pub/ardelean2025quantized.html</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Andrei-Timotei Ardelean, Patrick RÃ¼ckbeil, Tim Weyrichæ°åçè®ºæâQuantized FCA: Efficient Zero-Shot Texture Anomaly Detectionâçå¨é¢æè¦ã</p>
<hr />
<p><strong>è®ºææè¦ï¼Quantized FCA: Efficient Zero-Shot Texture Anomaly Detection</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³é¶æ ·æ¬çº¹çå¼å¸¸å®ä½ï¼Zero-Shot Texture Anomaly Localization, ZSALï¼çé®é¢ãZSALçç®æ æ¯å¨æ²¡æé¢åè®­ç»æä»»ä½å¼å¸¸æç¤ºçæåµä¸ï¼å¨ä¸å¼ å¾åä¸­æ£æµå¹¶å®ä½çº¹çä¸­çå¼å¸¸åºåãç°ææ¹æ³çä¸»è¦éå¶æ¯è¿è¡æ¶é´è¿é¿ï¼ä½¿å¶å¨å®éåºç¨ï¼å¦çäº§çº¿çæ§ï¼ä¸­ä¸åå®éã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
è¯¥è®ºææåºäºä¸ä¸ªåä¸º<strong>Quantized Feature Correspondence Analysis (QFCA)</strong>çå®æ¶æ¹æ³ï¼å®å¯¹ç°ææåè¿çç¹å¾å¯¹åºåæï¼Feature Correspondence Analysis, FCAï¼ç®æ³è¿è¡äºéåæ¹è¿ãä¸»è¦è´¡ç®åæ¬ï¼</p>
<ul>
<li><strong>éåè¡¥ä¸ç»è®¡æ¯è¾ï¼</strong> QFCAéè¿å°è¡¥ä¸ç»è®¡æ¯è¾éåºäºéåå¼çç´æ¹å¾ï¼å®ç°äºæ¾èçæçæåãè¿ç§æ¹æ³å¨åç¡®æ§æå¤±å¾å°çæåµä¸ï¼å®ç°äº10åçå éã</li>
<li><strong>åºäºä¸»æååæï¼PCAï¼çç¹å¾é¢å¤çï¼</strong> å¼å¥äºä¸ä¸ªæ°çç¹å¾é¢å¤çæ­¥éª¤ï¼éè¿PCAå¢å¼ºæ­£å¸¸ç¹å¾åå¼å¸¸ç¹å¾ä¹é´çå¯¹æ¯åº¦ãè¿æ¾èæé«äºå¤æçº¹çä¸çæ£æµç²¾åº¦ï¼å°¤å¶æ¯å¨çº¹çå¨æå¤§äºè¡¥ä¸å¤§å°çåå³°çº¹çä¸­ã</li>
<li><strong>é«æçå±é¨å¹³åæ± åï¼</strong> è§£å³äºç°ä»£æºå¨å­¦ä¹ åºä¸­å±é¨å¹³åæ± åå®ç°æçä½ä¸çè®¡ç®ç¶é¢ï¼éè¿ä½¿ç¨æ±åé¢ç§¯è¡¨ï¼integral imageï¼å®ç°äºä¸æ ¸å¤§å°æ å³çO(H x W)å¤æåº¦ï¼è¿ä¸æ­¥ç¼©ç­äºè¿è¡æ¶é´ã</li>
<li><strong>ç®æ³æ­£ç¡®æ§è¯æï¼</strong> è®ºææä¾äºç®æ³æ­£ç¡®æ§çè¯æï¼è¡¨æå¶éåæ¹æ³ä¸åå§FCAç®æ³å¨éåå¼ä¸äº§çç¸åçå¤±éåæ°ï¼å¹¶ä¸ä¸2-Wassersteinè·ç¦»çæ¢¯åº¦ç¸å³ã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
QFCAå¨MVTec ADãDTD-SyntheticåWoven Fabric Texturesç­æ°æ®éä¸è¿è¡äºå¨é¢è¯ä¼°ï¼å¹¶ä¸ç°æé¶æ ·æ¬æ¹æ³ï¼åæ¬åºäºVLMåçº¹çç¹å®æ¹æ³ï¼è¿è¡äºæ¯è¾ã</p>
<ul>
<li><strong>æ¾èçè¿è¡æ¶é´æåï¼</strong> QFCAå®ç°äºæ¯ç°ææ¹æ³å¿«10åçè¿è¡éåº¦ï¼ä½¿å¶è½å¤å®æ¶è¿è¡å¼å¸¸å®ä½ï¼è§£å³äºç°ææ¹æ³çä¸»è¦éå¶ã</li>
<li><strong>ä¿ææè¶è¶ç°ææ¹æ³çåç¡®æ§ï¼</strong> å¨MVTec ADæ°æ®éä¸ï¼QFCAå¨PROãAUROCSåF1ç­ææ ä¸ä¸FCAç¸å½ï¼çè³å¨æäºæåµä¸ï¼QFCA+ï¼è¡¨ç°æ´ä¼ãå¨DTD-SyntheticåWFTæ°æ®éä¸ï¼QFCA+ä¹åå¾äºæä½³ææ ã</li>
<li><strong>å¤æçº¹çä¸çæ¹è¿ï¼</strong> å¼å¥çç¹å¾é¢å¤çï¼QFCA+ï¼å¨å¤æçº¹çä¸å¸¦æ¥äºæ¾èçæ§è½æåï¼åå°äºè¯¯æ¥ã</li>
<li><strong>éåæçï¼</strong> å³ä½¿ä½¿ç¨å°éï¼ä¾å¦16ä¸ªï¼ç´æ¹å¾binï¼éåæ¹æ³ä¹è½å¹éå¨ç²¾åº¦ææ ï¼è¯æäºå¶é«ææ§ã</li>
</ul>
<p>è¿äºç»æè¡¨æï¼QFCAå¨åç¡®æ§åè¿è¡æ¶é´ä¹é´åå¾äºæä½³æè¡¡ï¼ä½¿å¶æä¸ºå®éé¨ç½²ä¸­é¶æ ·æ¬çº¹çå¼å¸¸å®ä½çæåå·¥å·ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>å¯¹çº¹çæ°æ®çéç¨æ§ï¼</strong> QFCAä½ä¸ºä¸ç§ä¼åçé¶æ ·æ¬çº¹çå¼å¸¸å®ä½ç®æ³ï¼ç»§æ¿äºè¿ç±»æ¹æ³çå±éæ§ãå®ä¸»è¦éç¨äºçº¹çç¶æ°æ®æè¡¨é¢ï¼èä¸éç¨äºä»»æå¯¹è±¡ã
*   <strong>ä¸ä½¿ç¨å¤§åVLMï¼</strong> ç±äºä¸ä½¿ç¨å¤§åè§è§è¯­è¨æ¨¡åï¼VLMï¼æ¥æ³¨å¥éç¨å¼å¸¸ç¥è¯ï¼QFCAå¨å¤çéçº¹çå¾åæ¶å¯è½ä¸å¦åºäºVLMçæ¹æ³éç¨ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>å°QFCAçæçä¼å¿æ¨å¹¿å°å¶ä»ç®¡éï¼</strong> è®ºæä¸­æåºçé«æå±é¨å¹³åæ± åç®æ³å¯ä»¥æ½å¨å°åºç¨äºå¶ä»è®¡ç®æºè§è§ç®¡éï¼ä»¥è·å¾ç±»ä¼¼çå éæçã
*   <strong>è¿ä¸æ­¥æ¢ç´¢ç¹å¾é¢å¤çï¼</strong> è®ºæä¸­å¼å¥çç¹å¾é¢å¤çæ­¥éª¤ï¼QFCA+ï¼å¨å¤æçº¹çä¸è¡¨ç°åºè²ï¼æªæ¥å¯ä»¥è¿ä¸æ­¥ç ç©¶å¶å¨æ´å¹¿æ³çå¼å¸¸æ£æµç®æ³ä¸­çåºç¨ã
*   <strong>æ©å±å°éçº¹çå¾åï¼</strong> è½ç¶ç®åä¸»è¦éå¯¹çº¹çï¼ä½éè¿ç»åæ´éç¨çç¹å¾è¡¨ç¤ºæéåºæ§æ´å¼ºçåèéæ©æºå¶ï¼æªæ¥å¯è½æ¢ç´¢QFCAå¨æ´å¹¿æ³çå¾åç±»åä¸çåºç¨ã</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose a real-time method, named QFCA,
which implements a quantized version of the feature correspondence analysis
(FCA) algorithm.</li>
<li>Moreover, we introduce a feature preprocessing step based on
principal component analysis, which enhances the contrast between normal and
anomalous features, improving the detection precision on complex textures.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.15602v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.15602v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.15564v1'></a></p>
<h2 id="imaginarium-vision-guided-high-quality-3d-scene-layout-generation"><a href="https://arxiv.org/abs/2510.15564v1">Imaginarium: Vision-guided High-Quality 3D Scene Layout Generation</a></h2>
<p><strong>Authors:</strong> Xiaoming Zhu, Xu Huang, Qinghongbing Xie, Zhi Deng, Junsheng Yu, Yirui Guan, Zhongyuan Liu, Lin Zhu, Qijun Zhao, Ligang Liu, Long Zeng</p>
<p><strong>Published:</strong> 2025-10-17</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Generating artistic and coherent 3D scene layouts is crucial in digital
content creation. Traditional optimization-based methods are often constrained
by cumbersome manual rules, while deep generative models face challenges in
producing content with richness and diversity. Furthermore, approaches that
utilize large language models frequently lack robustness and fail to accurately
capture complex spatial relationships. To address these challenges, this paper
presents a novel vision-guided 3D layout generation system. We first construct
a high-quality asset library containing 2,037 scene assets and 147 3D scene
layouts. Subsequently, we employ an image generation model to expand prompt
representations into images, fine-tuning it to align with our asset library. We
then develop a robust image parsing module to recover the 3D layout of scenes
based on visual semantics and geometric information. Finally, we optimize the
scene layout using scene graphs and overall visual semantics to ensure logical
coherence and alignment with the images. Extensive user testing demonstrates
that our algorithm significantly outperforms existing methods in terms of
layout richness and quality. The code and dataset will be available at
https://github.com/HiHiAllen/Imaginarium.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Xiaoming Zhuç­äººæ°åçè®ºæâImaginarium: Vision-guided High-Quality 3D Scene Layout Generationâçå¨é¢æè¦ã</p>
<hr />
<p><strong>è®ºææè¦ï¼Imaginarium: è§è§å¼å¯¼çé«è´¨é3Dåºæ¯å¸å±çæ</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³æ°å­åå®¹åä½ä¸­çæèºæ¯æ§åè¿è´¯ç3Dåºæ¯å¸å±çå³é®ææãä¼ ç»æ¹æ³åéäºç¹ççæå¨è§åï¼èæ·±åº¦çææ¨¡åå¨çæåå®¹ä¸°å¯æ§åå¤æ ·æ§æ¹é¢é¢ä¸´å°é¾ãæ­¤å¤ï¼å©ç¨å¤§åè¯­è¨æ¨¡åï¼LLMï¼çæ¹æ³å¾å¾ç¼ºä¹é²æ£æ§ï¼æ æ³åç¡®ææå¤æçç©ºé´å³ç³»ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
ä¸ºäºè§£å³ä¸è¿°é®é¢ï¼ä½èæåºäºä¸ä¸ªæ°é¢çè§è§å¼å¯¼3Då¸å±çæç³»ç»ï¼å¶ä¸»è¦åæ°åè´¡ç®åæ¬ï¼
*   <strong>é«è´¨é3Dåºæ¯å¸å±æ°æ®éçæå»ºï¼</strong> è®ºæé¦åæå»ºäºä¸ä¸ªåå«2,037ä¸ªé«è´¨éåºæ¯èµäº§å147ä¸ª3Dåºæ¯å¸å±çèµäº§åºï¼å¹¶è®¡åå¼æºè¯¥æ°æ®éï¼ä»¥é ç¦ç ç©¶ç¤¾åºã
*   <strong>è§è§å¼å¯¼çæç¤ºæ©å±ï¼</strong> ç³»ç»å©ç¨å¾åçææ¨¡åï¼Fluxæ¨¡åï¼å°ç¨æ·è¾å¥çææ¬æç¤ºæ©å±ä¸ºå¼å¯¼å¾åãéè¿å¯¹Fluxæ¨¡åè¿è¡å¾®è°ï¼ä½¿å¶ä¸èµäº§åºå¯¹é½ï¼ç¡®ä¿çæå¾åçé£æ ¼ä¸è´æ§ï¼ä»èå¢å¼ºåç»­èµäº§æ£ç´¢åå¸å±è½¬æ¢ä¼°è®¡çé²æ£æ§ã
*   <strong>é²æ£çå¾åè§£ææ¨¡åï¼</strong> å¼åäºä¸ä¸ªåºäºé¢è®­ç»è§è§æ¨¡åçå¾åè§£ææ¨¡åï¼è¯¥æ¨¡åéæäºè§è§è¯­ä¹åå²ãä»åå¹å¾åä¸­è¿è¡å ä½è§£æï¼åæ¬3Då®ååå´çãå¢å£ãå°æ¿åå¤©è±æ¿çå¹³é¢æ£æµï¼ä»¥ååºäºå¾çåºæ¯å¾é»è¾æå»ºã
*   <strong>è¯­ä¹ç¹å¾å¹éä¸åæ¢ä¼°è®¡ï¼</strong> éç¨è¯­ä¹ç¹å¾å¹éç­ç¥ä»èµäº§åºä¸­æ£ç´¢ä¸å¼å¯¼å¾åæç¸ä¼¼çå¯¹è±¡ãç¶åï¼ç»åè§è§è¯­ä¹ç¹å¾ãå ä½ä¿¡æ¯ååºæ¯å¸å±é»è¾ï¼è¿­ä»£æ±è§£æ¯ä¸ªåæ¯å¯¹è±¡çæè½¬ãå¹³ç§»åç¼©æ¾åæ¢ã
*   <strong>åºæ¯å¸å±ä¼åï¼</strong> æåï¼éè¿åºæ¯å¾é»è¾åå¾åè¯­ä¹è§£æå¯¹æ´ä½3Dåºæ¯å¸å±è¿è¡ä¸è´æ§ä¼åï¼ç¡®ä¿æç»å¸å±å¨è§è§ä¸åé»è¾ä¸ä¸å¼å¯¼å¾åé«åº¦ä¸è´ã
*   <strong>åé¨å¸å±åè½ï¼</strong> å¼å¥äºåé¨å¸å±åè½ï¼åè®¸èµäº§å¨å¶ä»èµäº§åé¨è¿è¡æåï¼ä¼åç©ºé´å©ç¨å¹¶æé«åºæ¯çå®æã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
å¹¿æ³çç¨æ·æµè¯è¡¨æï¼è¯¥ç®æ³å¨å¸å±ä¸°å¯æ§åè´¨éæ¹é¢æ¾èä¼äºç°ææ¹æ³ã
*   <strong>ç¨æ·è¯ä¼°ï¼</strong> å¨åçæ§ãçå®æ§ãè¿è´¯æ§åç¾å­¦å¸å¼åæ¹é¢ï¼è¯¥æ¹æ³å¨èºæ¯å­¦çåä¸ä¸èºæ¯å®¶è¯ä¼°ä¸­åä¼äºç°æåºçº¿æ¹æ³ï¼å¦DiffuScene, HOLODECK, LayoutGPT, InstructSceneï¼ã
*   <strong>3Dåºæ¯å¸å±éå»ºçä¿çåº¦åç¸ä¼¼æ§ï¼</strong> å¨æ°æ®éåºæ¯ä¸çè¯ä¼°æ¾ç¤ºï¼è¯¥æ¹æ³å¨ä¸»è¦å¯¹è±¡æ¢å¤ï¼92.31%ï¼ãç±»å«ä¿çï¼95.83%ï¼ãæè½¬ï¼AUC@60Â°ä¸º74.83%ï¼åä½ç§»ï¼AUC@0.5mä¸º84.32%ï¼æ¹é¢è¡¨ç°åºé«ä¿çåº¦ï¼åºæ¯å¾åç¡®çè¾¾å°93.26%ã
*   <strong>æè½¬åæ¢ä¼°è®¡ï¼</strong> å¨3D-Futureæ°æ®éä¸çè¯ä¼°æ¾ç¤ºï¼è¯¥æ¹æ³å¨ç±»å«çº§å«ï¼70.06%ï¼åå®ä¾çº§å«ï¼81.44%ï¼çæè½¬ä¼°è®¡æ¹é¢æ¾èä¼äºå¶ä»åºçº¿æ¹æ³ã
*   <strong>æ¶èç ç©¶ï¼</strong> è¯¦ç»çæ¶èç ç©¶éªè¯äºFluxæ¨¡åå¾®è°ãç»ååæåå ä½ä¿¡æ¯çæè½¬åæ¢ä¼°è®¡ä»¥ååºæ¯å¸å±ç»åç®¡éä¸­æ¯ä¸ªç»ä»¶çæææ§ï¼è¯æäºæ¯ä¸ªç»ä»¶å¯¹ç³»ç»æ§è½çç§¯æè´¡ç®ã</p>
<p>è¿äºç»æè¡¨æï¼è¯¥ç³»ç»è½å¤çææ´èªç¶ãè¯¦ç»ä¸è§è§å¸å¼äººç3Dåºæ¯å¸å±ï¼æ¾èæé«äºå¸å±è´¨éï¼å¹¶ææå°ä¸ä¸å·¥ä½æµç¨ä¸­å¸åç2.5å°æ¶æ¶é´ç¼©ç­è³240ç§åã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>å¾åçææ¨¡åçä¸è´æ§ææï¼</strong> å°½ç®¡åå¾äºè¿å±ï¼ä½å¨å¤æåºæ¯ä¸­ï¼å¾åçææ¨¡åå¨å¤ä¸ªå¯¹è±¡ä¹é´å®ç°é«ä¸è´æ§ä»ç¶æ¯ä¸ä¸ªä¸»è¦ææã
*   <strong>åå¹å¾åå§¿æä¼°è®¡çåç¡®æ§ï¼</strong> ä»åå¹å¾åä¸­åç¡®ä¼°è®¡å§¿æä»ç¶å·ææææ§ï¼å°¤å¶æ¯å¨ä¸¥éé®æ¡çæåµä¸ã
*   <strong>è¯­ä¹-ç»æä¸å¹éï¼</strong> å¾åçæå¨å¯è½äº§çèµäº§åºä¸­ä¸å­å¨çæ°ææç»æçå¯¹è±¡ï¼å¯¼è´èµäº§æ£ç´¢ä¸æ­£ç¡®ï¼è¿èä½¿åºæ¯å¾æ´¾ççå ä½åå³ç³»çº¦æå¤±æã
*   <strong>ä¸¥éé®æ¡å¯¼è´çå§¿ææ¨¡ç³æ§ï¼</strong> é®æ¡å¯¼è´çå±é¨è§å¾ä¿¡æ¯æéï¼å¯è½å¯¼è´åå§å§¿æä¼°è®¡ä¸å¯é ï¼åç»­ä¼åé¶æ®µå¯è½å¤±è´¥ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>è§è§åºç¡æ¨¡åçè¿æ­¥ï¼</strong> éçè§è§åºç¡æ¨¡åçè¿ä¸æ­¥åå±ï¼ä¸è¿°å±éæ§ææå¾å°ç¼è§£ã
*   <strong>å¤è§å¾éè§ä¿¡æ¯çæ´åï¼</strong> å¼å¥å¤è§å¾éè§ä¿¡æ¯ï¼å¦MVDæ¹æ³ï¼å¯ä»¥ä¸ºæ´é²æ£çåºæ¯åææä¾æåæ¯çéå¾ï¼ä»¥è§£å³å§¿ææ¨¡ç³æ§é®é¢ã
*   <strong>èªå¨å3Dæ°æ®çæå¼æï¼</strong> å°ä¸°å¯ç2Dè§è§æ¨¡åæ¾ç½®ç¥è¯è½¬åä¸º3Dèµäº§æ¾ç½®æ°æ®ï¼ä»¥è§£å³3Dåºæ¯çæä»»å¡ä¸­çæ°æ®ç¨ç¼ºé®é¢ï¼ä»èæ´ææå°è®­ç»3Dåºæ¯çè§£åå¸å±çææ¨¡åã
*   <strong>2Då3Dä¹é´æ´è¿è´¯çç¼è¾è½åï¼</strong> æ¢ç´¢2Då3Dä¹é´æ´è¿è´¯çç¼è¾è½åï¼ä»¥ä½¿æªæ¥çç³»ç»æ´å ç¨æ·åå¥½ã</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address these challenges, this paper
presents a novel vision-guided 3D layout generation system.</li>
<li>Extensive user testing demonstrates
that our algorithm significantly outperforms existing methods in terms of
layout richness and quality.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.15564v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.15564v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.15557v1'></a></p>
<h2 id="clappertext-a-benchmark-for-text-recognition-in-low-resource-archival-documents"><a href="https://arxiv.org/abs/2510.15557v1">ClapperText: A Benchmark for Text Recognition in Low-Resource Archival Documents</a></h2>
<p><strong>Authors:</strong> Tingyu Lin, Marco Peer, Florian Kleber, Robert Sablatnig</p>
<p><strong>Published:</strong> 2025-10-17</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, eess.IV</p>
<p><strong>Abstract:</strong></p>
<p>This paper presents ClapperText, a benchmark dataset for handwritten and
printed text recognition in visually degraded and low-resource settings. The
dataset is derived from 127 World War II-era archival video segments containing
clapperboards that record structured production metadata such as date,
location, and camera-operator identity. ClapperText includes 9,813 annotated
frames and 94,573 word-level text instances, 67% of which are handwritten and
1,566 are partially occluded. Each instance includes transcription, semantic
category, text type, and occlusion status, with annotations available as
rotated bounding boxes represented as 4-point polygons to support spatially
precise OCR applications. Recognizing clapperboard text poses significant
challenges, including motion blur, handwriting variation, exposure
fluctuations, and cluttered backgrounds, mirroring broader challenges in
historical document analysis where structured content appears in degraded,
non-standard forms. We provide both full-frame annotations and cropped word
images to support downstream tasks. Using a consistent per-video evaluation
protocol, we benchmark six representative recognition and seven detection
models under zero-shot and fine-tuned conditions. Despite the small training
set (18 videos), fine-tuning leads to substantial performance gains,
highlighting ClapperText's suitability for few-shot learning scenarios. The
dataset offers a realistic and culturally grounded resource for advancing
robust OCR and document understanding in low-resource archival contexts. The
dataset and evaluation code are available at
https://github.com/linty5/ClapperText.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼è¿æ¯ä¸ç¯å³äºTingyu Lin, Marco Peer, Florian Kleber, Robert Sablatnigæ°åçè®ºæâClapperText: A Benchmark for Text Recognition in Low-Resource Archival Documentsâçå¨é¢æè¦ï¼ç¨ä¸­æåæï¼</p>
<p><strong>è®ºææè¦ï¼ClapperTextï¼ä½èµæºæ¡£æ¡ææ¡£ä¸­çææ¬è¯å«åºå</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³åå²æ¡£æ¡è§é¢ææä¸­æååå°å·ææ¬è¯å«çææï¼å°¤å¶æ¯å¨è§è§éååä½èµæºè®¾ç½®ä¸ãç°æçOCRç³»ç»å¨å¤çè¿å¨æ¨¡ç³ãæåæ³¢å¨ãæååå¼åæä¹±èæ¯ç­é®é¢æ¶è¡¨ç°åºå±éæ§ï¼èè¿äºé®é¢å¨åå²ææ¡£åæä¸­æ®éå­å¨ãç ç©¶çæ ¸å¿é®é¢æ¯ï¼å¦ä½å¨è¿äºå·ææææ§çæ¡ä»¶ä¸ï¼ææå°æ£æµåè¯å«æ¡£æ¡è§é¢ä¸­åºè®°æ¿ä¸çç»æåææ¬ï¼å¹¶ä¸ºæªæ¥çOCRç ç©¶æä¾ä¸ä¸ªå¯é çåºåã</p>
<p><strong>2. ä¸»è¦åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
*   <strong>ClapperTextæ°æ®éçåå»ºï¼</strong> è®ºæå¼å¥äºä¸ä¸ªæ°çåºåæ°æ®éClapperTextï¼å®ä»127ä¸ªäºææ¶æçæ¡£æ¡è§é¢çæ®µä¸­æåï¼åå«9,813ä¸ªå¸¦æ æ³¨çå¸§å94,573ä¸ªè¯çº§ææ¬å®ä¾ãè¿äºå®ä¾åæ¬è½¬å½ãè¯­ä¹ç±»å«ãææ¬ç±»ååé®æ¡ç¶æï¼å¹¶ä»¥æè½¬è¾¹çæ¡ï¼4ç¹å¤è¾¹å½¢ï¼çå½¢å¼æä¾ï¼æ¯æç²¾ç¡®çOCRåºç¨ã
*   <strong>ä½èµæºåè§è§å¤æ ·æ§ï¼</strong> æ°æ®éç¹å«å¼ºè°ä½èµæºç¯å¢ï¼è®­ç»éä»18ä¸ªè§é¢ï¼ï¼å¹¶åå«å¤§éæåææ¬ï¼67%ï¼åé¨åé®æ¡ææ¬ï¼1,566ä¸ªï¼ï¼ä»¥åè¿å¨æ¨¡ç³ãæååå¼ãæåæ³¢å¨åæä¹±èæ¯ç­è§è§éåç¹å¾ï¼ä½¿å¶æä¸ºä¸ä¸ªçå®ä¸å·ææåæä¹çèµæºã
*   <strong>å¨é¢çåºåæµè¯ï¼</strong> è®ºæå¯¹å­ç§ææ¬è¯å«æ¨¡ååä¸ç§ææ¬æ£æµæ¨¡åè¿è¡äºé¶æ ·æ¬åå¾®è°æ¡ä»¶ä¸çåºåæµè¯ï¼è¯ä¼°äºç°ä»£OCRç³»ç»å¨ClapperTextæ°æ®éä¸çæ§è½ã
*   <strong>è¯ä¼°åè®®ï¼</strong> éç¨ä¸è´çæ¯è§é¢è¯ä¼°åè®®ï¼ç¡®ä¿äºè¯ä¼°çå¹³è¡¡æ§ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>é¶æ ·æ¬æ§è½æ¾èä¸éï¼</strong> æææ¨¡åå¨ClapperTextæ°æ®éä¸çé¶æ ·æ¬æ§è½åæ¾èä¸éï¼å³ä½¿å¨ä¼ ç»åºåä¸è¡¨ç°åºè²ï¼ä¹å¸æ¾äºClapperTextä¸ç°ææ°æ®éä¹é´çé¢åå·®è·ãä¾å¦ï¼NRTR-R31 (1/8)å¨å¸¸è§åºåä¸è¾¾å°94%ä»¥ä¸ï¼ä½å¨ClapperTextä¸ä»ä¸º67.46%ã
*   <strong>å¾®è°çæ¾èæ§è½æåï¼</strong> å°½ç®¡è®­ç»éè§æ¨¡è¾å°ï¼18ä¸ªè§é¢ï¼ï¼ä½å¾®è°æ¾èæé«äºæææ¨¡åçæ§è½ãä¾å¦ï¼NRTR-R31 (1/16)å¨å¾®è°åä»65.56%æåå°77.24%ï¼è¡¨æClapperTextéå¸¸éåå°æ ·æ¬å­¦ä¹ åºæ¯ã
*   <strong>æåææ¬çæææ§ï¼</strong> æåæ ·æ¬å¨é¶æ ·æ¬è®¾ç½®ä¸ææäºæ´å¤§çææï¼å¾®è°å¨æåææ¬ä¸çæ§è½æåæ¯å°å·ææ¬æ´æ¾èï¼è¡¨æé¢åéåºå¯¹æåOCRççå¤ã
*   <strong>é®æ¡ææ¬çé¾åº¦ï¼</strong> è®ºææåºï¼é®æ¡ææ¬çè¯å«é¾åº¦å¾é«ï¼NRTR (Mod-Trans.)å¨é¶æ ·æ¬è®¾ç½®ä¸å¯¹é®æ¡è¯çåç¡®çä»ä¸º18.06%ï¼å¾®è°åä¹åªæ30.14%ã
*   <strong>æ£æµæ¨¡åçè¡¨ç°ï¼</strong> æ£æµæ¨¡åå¨ClapperTextä¸çé¶æ ·æ¬æ§è½ä¹å¤§å¹ä¸éï¼ä½å¾®è°åæ ·å¸¦æ¥äºæ¾èæåãTextSnake (R50+OCLIP)å¨æ¨çéåº¦ååç¡®æ§ä¹é´åå¾äºè¾å¥½çå¹³è¡¡ï¼éç¨äºå®æ¶å¤çã</p>
<p><strong>4. è®ºæä¸­æå°çå±éæ§ï¼</strong>
*   <strong>è®­ç»éè§æ¨¡å°ï¼</strong> å°½ç®¡å¾®è°å¸¦æ¥äºæ¾èæåï¼ä½è®­ç»éè§æ¨¡ï¼18ä¸ªè§é¢ï¼ä»ç¶å¾å°ï¼è¿éå¶äºæ¨¡åå¨æ´å¹¿æ³åºæ¯ä¸çæ³åè½åã
*   <strong>é®æ¡ææ¬çææï¼</strong> å³ä½¿ç»è¿å¾®è°ï¼é®æ¡ææ¬çè¯å«ä»ç¶æ¯ä¸ä¸ªé¾é¢ï¼éè¦è¿ä¸æ­¥ç ç©¶ã
*   <strong>æ¨¡åå¯¹è¯­è¨åéªçä¾èµï¼</strong> æäºæ¨¡åï¼å¦NRTRï¼å¨å¤çç¼©åæéè¯æ±æ è®°æ¶ï¼å¯è½ä¼è¡¨ç°åºè¯­è¨é©±å¨çåå·®ï¼å¯¼è´è¯­ä¹ä¸çä¼¼åçä½å®ééè¯¯çé¢æµã
*   <strong>èæ¯å¹²æ°åç»æåºåï¼</strong> å¨æä¹±èæ¯ä¸ï¼æ¨¡åå¯è½åºç°è¯¯æ£ææ¼æ£ï¼TextSnakeå¨å¤çå¤è¯å­æ®µæ¶å¯è½å°ç¸é»è¯è¿åº¦åç»ï¼èDBNet++å¨å¸å±æææ§æ¹é¢è¡¨ç°æ´å¥½ï¼ä½ä»ææªæ£æµå°çè¯ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>é¢åéåºï¼</strong> è¿ä¸æ­¥ç ç©¶å¦ä½å°ææ¬æ¨¡åéåºå°éä¼ ç»é¢åï¼ä»¥æé«å¨ä½èµæºæ¡£æ¡ä¸ä¸æä¸­çOCRæ§è½ã
*   <strong>æ¶é´å»ºæ¨¡ï¼</strong> æ¢ç´¢å©ç¨è§é¢åºåä¸­çæ¶é´ä¿¡æ¯æ¥å¢å¼ºææ¬è¯å«åæ£æµçé²æ£æ§ã
*   <strong>è¯­ä¹ä¸ä¸æéæï¼</strong> æ´åè·¨å¸§çè¯­ä¹ä¸ä¸æä¿¡æ¯ï¼ä»¥æ´å¥½å°çè§£åè¯å«åºè®°æ¿ä¸çç»æååå®¹ã
*   <strong>å¤çé®æ¡åè§è§åªå£°ï¼</strong> å¼åæ´é²æ£çæ¹æ³æ¥å¤çä¸¥éé®æ¡ååç§è§è§åªå£°ï¼è¿æ¯æ¡£æ¡ææä¸­çå¸¸è§ææã
*   <strong>å°æ ·æ¬å­¦ä¹ ï¼</strong> ClapperTextæ°æ®éçç¹æ§ä½¿å¶æä¸ºå°æ ·æ¬å­¦ä¹ åºæ¯ççæ³æµè¯å¹³å°ï¼æªæ¥å¯ä»¥æ¢ç´¢æ´åè¿çå°æ ·æ¬å­¦ä¹ ææ¯ã</p>
<p>æ»èè¨ä¹ï¼ClapperTextæ°æ®éåå¶åºåæµè¯ä¸ºæ¨å¨ä½èµæºæ¡£æ¡ææ¡£ä¸­çOCRåææ¡£çè§£ç ç©¶æä¾äºä¸ä¸ªå®è´µä¸å·ææææ§çèµæºï¼å¹¶æ­ç¤ºäºç°ææ¨¡åå¨å¤çæåãé®æ¡åè§è§éåææ¬æ¹é¢çæç»­å±éæ§ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>The
dataset offers a realistic and culturally grounded resource for advancing
robust OCR and document understanding in low-resource archival contexts.</li>
<li>The
dataset and evaluation code are available at
https://github.com/linty5/ClapperText.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.15557v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.15557v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-10-20 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
