<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-10-20 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-10-17/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-10-21/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-10-20">Arxiv Computer Vision Papers - 2025-10-20</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#blip3o-next-next-frontier-of-native-image-generation" class="nav-link">BLIP3o-NEXT: Next Frontier of Native Image Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#omnivinci-enhancing-architecture-and-data-for-omni-modal-understanding-llm" class="nav-link">OmniVinci: Enhancing Architecture and Data for Omni-Modal Understanding LLM</a>
                </li>
                <li class="nav-item">
                    <a href="#lightsout-diffusion-based-outpainting-for-enhanced-lens-flare-removal" class="nav-link">LightsOut: Diffusion-based Outpainting for Enhanced Lens Flare Removal</a>
                </li>
                <li class="nav-item">
                    <a href="#3dpr-single-image-3d-portrait-relight-using-generative-priors" class="nav-link">3DPR: Single Image 3D Portrait Relight using Generative Priors</a>
                </li>
                <li class="nav-item">
                    <a href="#recon-region-controllable-data-augmentation-with-rectification-and-alignment-for-object-detection" class="nav-link">ReCon: Region-Controllable Data Augmentation with Rectification and Alignment for Object Detection</a>
                </li>
                <li class="nav-item">
                    <a href="#dgme-t-directional-grid-motion-encoding-for-transformer-based-historical-camera-movement-classification" class="nav-link">DGME-T: Directional Grid Motion Encoding for Transformer-Based Historical Camera Movement Classification</a>
                </li>
                <li class="nav-item">
                    <a href="#valeo-near-field-a-novel-dataset-for-pedestrian-intent-detection" class="nav-link">Valeo Near-Field: a novel dataset for pedestrian intent detection</a>
                </li>
                <li class="nav-item">
                    <a href="#quantized-fca-efficient-zero-shot-texture-anomaly-detection" class="nav-link">Quantized FCA: Efficient Zero-Shot Texture Anomaly Detection</a>
                </li>
                <li class="nav-item">
                    <a href="#imaginarium-vision-guided-high-quality-3d-scene-layout-generation" class="nav-link">Imaginarium: Vision-guided High-Quality 3D Scene Layout Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#clappertext-a-benchmark-for-text-recognition-in-low-resource-archival-documents" class="nav-link">ClapperText: A Benchmark for Text Recognition in Low-Resource Archival Documents</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-10-20">Arxiv Computer Vision Papers - 2025-10-20</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>好的，这是一份针对2025年10月17日ArXiv计算机视觉论文的每日报告执行摘要，旨在帮助忙碌的研究人员快速了解最新进展。</p>
<hr />
<p><strong>每日ArXiv计算机视觉论文报告执行摘要 (2025年10月17日)</strong></p>
<p><strong>1. 主要主题和趋势概述：</strong></p>
<p>今天的论文展示了计算机视觉领域几个活跃且相互关联的研究方向。生成式AI，特别是扩散模型，在图像生成、编辑和3D内容创建方面持续占据主导地位。多模态学习，尤其是结合大型语言模型（LLM）进行视觉理解，是另一个显著趋势。此外，我们看到了对实际应用问题的关注，例如图像质量增强、特定场景下的目标检测、以及针对特定数据集（如历史文档和自动驾驶）的解决方案。3D视觉和数据增强技术也继续得到深入探索。</p>
<p><strong>2. 特别重要或创新的论文亮点：</strong></p>
<ul>
<li><strong>"BLIP3o-NEXT: Next Frontier of Native Image Generation" (Jiuhai Chen et al.)</strong>: 这篇论文似乎代表了原生图像生成领域的重大飞跃，可能在图像质量、多样性或控制粒度方面设定了新的基准。考虑到BLIP系列在多模态理解和生成方面的强大背景，BLIP3o-NEXT的出现预示着生成模型能力的进一步提升，可能对AIGC（人工智能生成内容）产生深远影响。</li>
<li><strong>"OmniVinci: Enhancing Architecture and Data for Omni-Modal Understanding LLM" (Hanrong Ye et al.)</strong>: 这篇论文聚焦于全模态理解LLM，表明研究人员正在积极探索如何将视觉、文本甚至其他模态（如音频、触觉等）无缝整合到统一的AI模型中。其对架构和数据的双重强调，预示着在构建更通用、更强大的AI智能体方面取得了进展。</li>
<li><strong>"LightsOut: Diffusion-based Outpainting for Enhanced Lens Flare Removal" (Shr-Ruei Tsai et al.)</strong>: 这篇论文巧妙地将扩散模型应用于一个具体的图像编辑挑战——镜头光斑去除。通过结合outpainting技术，它可能提供了一种比传统方法更自然、更鲁棒的解决方案，展示了生成模型在实际图像修复中的巨大潜力。</li>
</ul>
<p><strong>3. 新兴研究方向或技术：</strong></p>
<ul>
<li><strong>扩散模型在特定图像编辑和3D生成中的精细化应用：</strong> "LightsOut" 和 "3DPR" 均利用扩散模型解决特定问题，表明扩散模型不再仅仅用于通用图像生成，而是被精细化地应用于图像修复、风格迁移、3D内容创建等更具体的任务。</li>
<li><strong>多模态LLM的“全模态”扩展：</strong> "OmniVinci" 强调“Omni-Modal”（全模态）理解，这超越了传统的视觉-语言结合，暗示着未来AI模型将能够处理和理解更多样化的数据类型。</li>
<li><strong>生成先验在3D重建和编辑中的作用：</strong> "3DPR" 利用生成先验进行单图像3D肖像重打光，这表明预训练的生成模型（如GANs或扩散模型）的隐式知识正被有效地用于解决逆向图形学问题，减少对多视图或复杂传感器数据的依赖。</li>
<li><strong>针对特定领域的数据集和基准：</strong> "Valeo Near-Field" 和 "ClapperText" 的发布，突显了在自动驾驶（行人意图检测）和历史文档（低资源文本识别）等特定应用领域，对高质量、有针对性数据集的持续需求，以及这些数据集对推动领域进步的关键作用。</li>
</ul>
<p><strong>4. 建议阅读全文的论文：</strong></p>
<p>对于不同兴趣的研究人员，以下论文可能最有价值：</p>
<ul>
<li><strong>对于生成式AI和多模态学习研究者：</strong><ul>
<li><strong>"BLIP3o-NEXT: Next Frontier of Native Image Generation"</strong>: 了解图像生成领域的最新突破。</li>
<li><strong>"OmniVinci: Enhancing Architecture and Data for Omni-Modal Understanding LLM"</strong>: 探索多模态LLM的未来发展方向。</li>
</ul>
</li>
<li><strong>对于图像处理和编辑研究者：</strong><ul>
<li><strong>"LightsOut: Diffusion-based Outpainting for Enhanced Lens Flare Removal"</strong>: 学习扩散模型在图像修复中的创新应用。</li>
<li><strong>"3DPR: Single Image 3D Portrait Relight using Generative Priors"</strong>: 了解如何利用生成先验进行单图像3D编辑。</li>
</ul>
</li>
<li><strong>对于目标检测和数据增强研究者：</strong><ul>
<li><strong>"ReCon: Region-Controllable Data Augmentation with Rectification and Alignment for Object Detection"</strong>: 探索新的数据增强策略以提升目标检测性能。</li>
</ul>
</li>
<li><strong>对于自动驾驶和特定领域应用研究者：</strong><ul>
<li><strong>"Valeo Near-Field: a novel dataset for pedestrian intent detection"</strong>: 如果您的研究涉及自动驾驶或行人行为预测，这个新数据集至关重要。</li>
</ul>
</li>
<li><strong>对于3D视觉和场景理解研究者：</strong><ul>
<li><strong>"Imaginarium: Vision-guided High-Quality 3D Scene Layout Generation"</strong>: 了解如何从视觉输入生成高质量的3D场景布局。</li>
</ul>
</li>
</ul>
<hr />
<p>这份摘要希望能为您提供一个快速而全面的概览，帮助您优先阅读最相关的论文。</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2510.15857v1">BLIP3o-NEXT: Next Frontier of Native Image Generation</a></li>
<li><a href="#2510.15870v1">OmniVinci: Enhancing Architecture and Data for Omni-Modal Understanding LLM</a></li>
<li><a href="#2510.15868v1">LightsOut: Diffusion-based Outpainting for Enhanced Lens Flare Removal</a></li>
<li><a href="#2510.15846v1">3DPR: Single Image 3D Portrait Relight using Generative Priors</a></li>
<li><a href="#2510.15783v1">ReCon: Region-Controllable Data Augmentation with Rectification and Alignment for Object Detection</a></li>
<li><a href="#2510.15725v1">DGME-T: Directional Grid Motion Encoding for Transformer-Based Historical Camera Movement Classification</a></li>
<li><a href="#2510.15673v1">Valeo Near-Field: a novel dataset for pedestrian intent detection</a></li>
<li><a href="#2510.15602v1">Quantized FCA: Efficient Zero-Shot Texture Anomaly Detection</a></li>
<li><a href="#2510.15564v1">Imaginarium: Vision-guided High-Quality 3D Scene Layout Generation</a></li>
<li><a href="#2510.15557v1">ClapperText: A Benchmark for Text Recognition in Low-Resource Archival Documents</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2510.15857v1'></a></p>
<h2 id="blip3o-next-next-frontier-of-native-image-generation"><a href="https://arxiv.org/abs/2510.15857v1">BLIP3o-NEXT: Next Frontier of Native Image Generation</a></h2>
<p><strong>Authors:</strong> Jiuhai Chen, Le Xue, Zhiyang Xu, Xichen Pan, Shusheng Yang, Can Qin, An Yan, Honglu Zhou, Zeyuan Chen, Lifu Huang, Tianyi Zhou, Junnan Li, Silvio Savarese, Caiming Xiong, Ran Xu</p>
<p><strong>Published:</strong> 2025-10-17</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We present BLIP3o-NEXT, a fully open-source foundation model in the BLIP3
series that advances the next frontier of native image generation. BLIP3o-NEXT
unifies text-to-image generation and image editing within a single
architecture, demonstrating strong image generation and image editing
capabilities. In developing the state-of-the-art native image generation model,
we identify four key insights: (1) Most architectural choices yield comparable
performance; an architecture can be deemed effective provided it scales
efficiently and supports fast inference; (2) The successful application of
reinforcement learning can further push the frontier of native image
generation; (3) Image editing still remains a challenging task, yet instruction
following and the consistency between generated and reference images can be
significantly enhanced through post-training and data engine; (4) Data quality
and scale continue to be decisive factors that determine the upper bound of
model performance. Building upon these insights, BLIP3o-NEXT leverages an
Autoregressive + Diffusion architecture in which an autoregressive model first
generates discrete image tokens conditioned on multimodal inputs, whose hidden
states are then used as conditioning signals for a diffusion model to generate
high-fidelity images. This architecture integrates the reasoning strength and
instruction following of autoregressive models with the fine-detail rendering
ability of diffusion models, achieving a new level of coherence and realism.
Extensive evaluations of various text-to-image and image-editing benchmarks
show that BLIP3o-NEXT achieves superior performance over existing models.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供由Jiuhai Chen等人撰写的论文“BLIP3o-NEXT: Next Frontier of Native Image Generation”的全面摘要。</p>
<hr />
<h3 id="blip3o-next">BLIP3o-NEXT: 原生图像生成的新前沿</h3>
<p><strong>1. 论文主要问题或研究问题：</strong>
该论文旨在解决原生图像生成领域的前沿挑战，特别是如何构建一个统一的、高性能的、能够同时进行文本到图像生成和图像编辑的开源基础模型。研究的核心问题是如何在保持图像质量、语义连贯性和指令遵循能力的同时，有效整合不同的生成范式（如自回归和扩散模型），并克服现有模型在这些任务上的局限性。</p>
<p><strong>2. 关键创新或方法学贡献：</strong>
BLIP3o-NEXT提出了以下关键创新和方法学贡献：
*   <strong>新颖且可扩展的自回归+扩散架构：</strong> BLIP3o-NEXT采用了一种混合架构，其中自回归模型首先根据多模态输入生成离散图像token，然后这些token的隐藏状态作为扩散模型的条件信号，用于生成高保真图像。这种设计结合了自回归模型的推理能力和指令遵循能力，以及扩散模型的精细细节渲染能力，实现了更高的连贯性和真实感。
*   <strong>高效的强化学习方法：</strong> 论文提出了一种针对自回归模型的高效强化学习（RL）框架，通过利用离散图像token，使其能够无缝集成到现有的语言模型RL基础设施中。这显著提升了模型的文本渲染质量和指令遵循能力。
*   <strong>系统性图像编辑一致性研究：</strong> 论文深入研究了提高图像编辑一致性的策略，包括将VAE（变分自编码器）特征作为跨注意力输入和噪声空间注入两种方式整合到扩散模型中，以保留精细的像素级信息并增强生成图像与参考图像之间的一致性。
*   <strong>关键洞察的提炼：</strong> 论文总结了开发最先进原生图像生成模型的四个关键洞察：1) 大多数架构选择性能相似，关键在于效率和快速推理；2) 强化学习能进一步推动原生图像生成前沿；3) 图像编辑仍具挑战，但后期训练和数据引擎可显著增强指令遵循和一致性；4) 数据质量和规模是模型性能上限的决定性因素。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>卓越的性能：</strong> 在各种文本到图像生成和图像编辑基准测试上进行的广泛评估表明，BLIP3o-NEXT在现有模型中取得了卓越的性能。尽管其3B模型在某些编辑任务上略低于GPT-Image和Qwen-Image等更大模型，但它与BAGEL和OmniGen2等模型表现相当。
*   <strong>强化学习的有效性：</strong> 实验结果（如GenEval和OCR训练奖励曲线）清晰展示了通过GRPO训练后，模型在多目标组合和视觉文本渲染质量方面的显著提升。
*   <strong>VAE特征对一致性的增强：</strong> 论文通过定性结果展示，整合VAE潜在特征能有效增强图像编辑任务中的一致性，尽管在某些情况下仍存在轻微不一致。
*   <strong>开源贡献：</strong> 作为BLIP3系列中的一个完全开源的基础模型，BLIP3o-NEXT发布了预训练和后期训练的模型权重、数据集、详细的训练和推理代码以及评估流程，确保了完全可复现性，对社区具有重要意义。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>VAE特征与一致性：</strong> 尽管VAE特征的引入提高了图像编辑的一致性，但由于SANA中VAE的下采样比率较高（32），生成的图像与参考图像之间仍存在轻微的不一致。
*   <strong>RL在扩散模型上的速度：</strong> 在没有扩散加速的情况下，将RL应用于扩散模型会较慢，因为缺乏KV缓存支持且需要多个时间步。
*   <strong>RL奖励模型设计：</strong> 将强化学习应用于原生图像生成的核心挑战在于奖励模型的设计，需要开发能够有效捕捉和平衡图像质量、指令遵循和人类偏好对齐等多个维度的奖励模型。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>图像编辑的强化学习：</strong> 进一步探索将RL应用于图像编辑，以提高指令遵循能力和生成输出与参考输入之间的一致性。特别是，需要深入研究测量一致性的奖励模型设计。
*   <strong>系统提示设计：</strong> 针对图像修复和主体驱动生成任务，设计明确区分任务的系统提示，以更好地指导模型处理这些不同类型的编辑任务。
*   <strong>提示重写：</strong> 利用提示重写来丰富提示细节，提高图像生成和编辑任务的指令遵循能力。
*   <strong>数据工程和RL扩展：</strong> 弥补自回归+扩散架构在指令遵循方面表现出色，但在编辑一致性方面仍有不足的差距，需要进一步在数据工程和专门针对图像编辑的强化学习方面取得进展。
*   <strong>统一架构、强化学习和可扩展后期训练：</strong> 未来基础模型应在统一架构、强化学习和可扩展后期训练方面共同推动可控、指令对齐和高质量原生图像生成系统的发展。</p>
<hr />
<p>这篇论文为原生图像生成领域提供了一个强大的开源基础模型，并通过其混合架构、强化学习应用和对图像编辑一致性的深入研究，为未来的研究奠定了坚实的基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present BLIP3o-NEXT, a fully open-source foundation model in the BLIP3
series that advances the next frontier of native image generation.</li>
<li>In developing the state-of-the-art native image generation model,
we identify four key insights: (1) Most architectural choices yield comparable
performance; an architecture can be deemed effective provided it scales
efficiently and supports fast inference; (2) The successful application of
reinforcement learning can further push the frontier of native image
generation; (3) Image editing still remains a challenging task, yet instruction
following and the consistency between generated and reference images can be
significantly enhanced through post-training and data engine; (4) Data quality
and scale continue to be decisive factors that determine the upper bound of
model performance.</li>
<li>This architecture integrates the reasoning strength and
instruction following of autoregressive models with the fine-detail rendering
ability of diffusion models, achieving a new level of coherence and realism.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.15857v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.15857v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.15870v1'></a></p>
<h2 id="omnivinci-enhancing-architecture-and-data-for-omni-modal-understanding-llm"><a href="https://arxiv.org/abs/2510.15870v1">OmniVinci: Enhancing Architecture and Data for Omni-Modal Understanding LLM</a></h2>
<p><strong>Authors:</strong> Hanrong Ye, Chao-Han Huck Yang, Arushi Goel, Wei Huang, Ligeng Zhu, Yuanhang Su, Sean Lin, An-Chieh Cheng, Zhen Wan, Jinchuan Tian, Yuming Lou, Dong Yang, Zhijian Liu, Yukang Chen, Ambrish Dantrey, Ehsan Jahangiri, Sreyan Ghosh, Daguang Xu, Ehsan Hosseini-Asl, Danial Mohseni Taheri, Vidya Murali, Sifei Liu, Jason Lu, Oluwatobi Olabiyi, Frank Wang, Rafael Valle, Bryan Catanzaro, Andrew Tao, Song Han, Jan Kautz, Hongxu Yin, Pavlo Molchanov</p>
<p><strong>Published:</strong> 2025-10-17</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.CL</p>
<p><strong>Abstract:</strong></p>
<p>Advancing machine intelligence requires developing the ability to perceive
across multiple modalities, much as humans sense the world. We introduce
OmniVinci, an initiative to build a strong, open-source, omni-modal LLM. We
carefully study the design choices across model architecture and data curation.
For model architecture, we present three key innovations: (i) OmniAlignNet for
strengthening alignment between vision and audio embeddings in a shared
omni-modal latent space; (ii) Temporal Embedding Grouping for capturing
relative temporal alignment between vision and audio signals; and (iii)
Constrained Rotary Time Embedding for encoding absolute temporal information in
omni-modal embeddings. We introduce a curation and synthesis pipeline that
generates 24M single-modal and omni-modal conversations. We find that
modalities reinforce one another in both perception and reasoning. Our model,
OmniVinci, outperforms Qwen2.5-Omni with +19.05 on DailyOmni (cross-modal
understanding), +1.7 on MMAR (audio), and +3.9 on Video-MME (vision), while
using just 0.2T training tokens - a 6 times reduction compared to
Qwen2.5-Omni's 1.2T. We finally demonstrate omni-modal advantages in downstream
applications spanning robotics, medical AI, and smart factory.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于“OmniVinci: Enhancing Architecture and Data for Omni-Modal Understanding LLM”论文的全面摘要：</p>
<p><strong>论文摘要：OmniVinci：增强全模态理解大型语言模型的架构和数据</strong></p>
<p><strong>1. 解决的主要问题或研究问题：</strong>
该论文旨在解决当前多模态大型语言模型（LLMs）在实现跨多种模态（视觉、听觉、文本）的全面感知和理解方面的挑战。尽管现有模型在单一模态或部分模态融合方面取得了进展，但构建一个强大的、开源的、能够原生处理并推理所有模态信息的全模态LLM仍然是一个未充分探索的领域。具体来说，论文关注如何通过创新的模型架构和高效的数据策略，提升全模态LLM的性能和训练效率。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
OmniVinci引入了三项关键的模型架构创新和一套数据策展与合成流程：</p>
<ul>
<li><strong>OmniAlignNet：</strong> 旨在加强视觉和音频嵌入在共享全模态潜在空间中的对齐。它通过对比学习，利用视频中视觉和音频流固有的语义关联，将不同模态的嵌入映射到统一的潜在空间中，从而更有效地学习和对齐。</li>
<li><strong>时间嵌入分组（Temporal Embedding Grouping, TEG）：</strong> 通过根据时间戳将视觉和音频嵌入组织成组，捕获模态之间相对时间对齐信息。这有助于LLM骨干网络更好地理解不同模态嵌入之间的时间关系。</li>
<li><strong>受限旋转时间嵌入（Constrained Rotary Time Embedding, CRTE）：</strong> 用于在全模态嵌入中编码绝对时间信息。它通过定义最大时间范围（Tmax）来平衡局部和全局时间敏感性，解决了现有方法在处理较大时间偏移时的局限性。</li>
<li><strong>数据策展与合成流程：</strong> 论文开发了一个生成2400万单模态和全模态对话的数据管道。该管道通过利用现有视频-音频问答数据进行隐式学习，并生成带有显式全模态标签的合成对话，以解决全模态数据稀缺的问题。特别地，它引入了一个全模态数据引擎，通过LLM进行跨模态校正和总结，生成准确的全模态字幕，以克服单一模态字幕的“模态特异性幻觉”问题。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong>
OmniVinci在多个基准测试中展现了卓越的性能，并显著提升了训练效率：</p>
<ul>
<li><strong>性能提升：</strong> OmniVinci在DailyOmni（跨模态理解）上比Qwen2.5-Omni高出+19.05分，在MMAR（音频）上高出+1.7分，在Video-MME（视觉）上高出+3.9分。在全模态理解基准测试中，OmniVinci的平均得分达到53.73，比次优模型Qwen2.5-Omni高出+4.07。</li>
<li><strong>训练效率：</strong> OmniVinci仅使用0.2T训练token，比Qwen2.5-Omni的1.2T减少了6倍，显著降低了训练成本和推理成本。</li>
<li><strong>模态协同效应：</strong> 实验发现，音频和视频模态在感知和推理方面相互增强，证明了全模态学习的优势。</li>
<li><strong>下游应用：</strong> OmniVinci在机器人、医疗AI和智能工厂等下游应用中展示了全模态优势，例如语音驱动的视觉语言导航、体育视频理解、跨语言语音翻译、医学影像分析和半导体工厂监控。</li>
</ul>
<p><strong>4. 论文中提及的局限性：</strong>
论文主要关注模型架构和数据策展的创新，但并未明确提及当前OmniVinci模型的具体局限性。然而，从其对模型量化和高效部署的讨论中可以看出，大型模型和长视频序列的内存容量限制以及交互式应用对低延迟的需求是实际部署中面临的挑战。此外，虽然模型在多模态任务上表现出色，但其在处理特定复杂场景（如极度嘈杂环境下的语音识别）时，可能仍需借助外部ASR模型进行辅助，这暗示了模型在某些极端条件下的鲁棒性仍有提升空间。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>持续优化模型效率：</strong> 进一步探索更先进的模型量化和部署策略，以在保持高性能的同时，进一步降低内存消耗和推理延迟，使其更适用于资源受限的边缘设备和实时应用。
*   <strong>增强极端条件下的鲁棒性：</strong> 针对更复杂的噪声环境、口音多样性或重叠语音等场景，进一步提升模型在语音理解任务中的鲁棒性，减少对外部辅助系统的依赖。
*   <strong>扩展全模态能力：</strong> 探索将更多模态（如触觉、嗅觉等）集成到全模态框架中，以实现更全面的通用智能。
*   <strong>深化跨模态推理能力：</strong> 进一步研究和开发更复杂的跨模态推理机制，使其能够处理更抽象、更深层次的模态间关系，从而在更广泛的下游任务中发挥作用。
*   <strong>数据生成与合成的持续改进：</strong> 持续优化全模态数据引擎，生成更高质量、更多样化、更具挑战性的全模态对话和标签，以推动模型性能的进一步提升。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce
OmniVinci, an initiative to build a strong, open-source, omni-modal LLM.</li>
<li>For model architecture, we present three key innovations: (i) OmniAlignNet for
strengthening alignment between vision and audio embeddings in a shared
omni-modal latent space; (ii) Temporal Embedding Grouping for capturing
relative temporal alignment between vision and audio signals; and (iii)
Constrained Rotary Time Embedding for encoding absolute temporal information in
omni-modal embeddings.</li>
<li>We introduce a curation and synthesis pipeline that
generates 24M single-modal and omni-modal conversations.</li>
<li>Our model,
OmniVinci, outperforms Qwen2.5-Omni with +19.05 on DailyOmni (cross-modal
understanding), +1.7 on MMAR (audio), and +3.9 on Video-MME (vision), while
using just 0.2T training tokens - a 6 times reduction compared to
Qwen2.5-Omni's 1.2T.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.15870v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.15870v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.15868v1'></a></p>
<h2 id="lightsout-diffusion-based-outpainting-for-enhanced-lens-flare-removal"><a href="https://arxiv.org/abs/2510.15868v1">LightsOut: Diffusion-based Outpainting for Enhanced Lens Flare Removal</a></h2>
<p><strong>Authors:</strong> Shr-Ruei Tsai, Wei-Cheng Chang, Jie-Ying Lee, Chih-Hai Su, Yu-Lun Liu</p>
<p><strong>Published:</strong> 2025-10-17</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Lens flare significantly degrades image quality, impacting critical computer
vision tasks like object detection and autonomous driving. Recent Single Image
Flare Removal (SIFR) methods perform poorly when off-frame light sources are
incomplete or absent. We propose LightsOut, a diffusion-based outpainting
framework tailored to enhance SIFR by reconstructing off-frame light sources.
Our method leverages a multitask regression module and LoRA fine-tuned
diffusion model to ensure realistic and physically consistent outpainting
results. Comprehensive experiments demonstrate LightsOut consistently boosts
the performance of existing SIFR methods across challenging scenarios without
additional retraining, serving as a universally applicable plug-and-play
preprocessing solution. Project page: https://ray-1026.github.io/lightsout/</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Shr-Ruei Tsai等人撰写的论文“LightsOut: Diffusion-based Outpainting for Enhanced Lens Flare Removal”的全面摘要。</p>
<hr />
<h3 id="lightsout">LightsOut: 基于扩散的外绘增强镜头眩光去除的全面摘要</h3>
<p><strong>1. 主要问题或研究问题：</strong>
镜头眩光（Lens Flare）严重影响图像质量，并对物体检测和自动驾驶等关键计算机视觉任务造成负面影响。尽管现有的单图像眩光去除（SIFR）方法在专用数据集上取得了显著进展，但当图像中存在不完整或缺失的画框外（off-frame）光源时，这些方法的性能会显著下降。这是因为不完整的光源上下文导致SIFR模型难以准确识别和去除眩光伪影。本研究旨在解决这一关键限制，即在光源信息不完整的情况下，如何有效去除镜头眩光。</p>
<p><strong>2. 关键创新或方法贡献：</strong>
LightsOut 提出了一种基于扩散（diffusion-based）的外绘（outpainting）框架，旨在通过重建画框外光源来增强现有SIFR方法的性能。其主要创新和方法贡献包括：</p>
<ul>
<li><strong>专门的分解策略：</strong> 识别并解决了SIFR方法在处理不完整画框外光源时的关键限制，通过一种专门的分解策略来解决。</li>
<li><strong>多任务回归模块：</strong> 引入一个多任务回归模块，用于精确预测画框外光源的参数（位置、半径和置信度）。这确保了生成内容与真实世界的眩光和照明分布物理上一致。</li>
<li><strong>LoRA微调扩散模型：</strong> 利用LoRA（Low-Rank Adaptation）微调的稳定扩散（Stable Diffusion）修复模型，并明确地以预测的光源参数为条件。这使得模型能够准确重建物理上一致的画框外光源和眩光伪影。</li>
<li><strong>噪声再注入策略：</strong> 采用噪声再注入（Noise Reinjection）技术，以缓解在RGB空间进行合成时，遮罩区域和未遮罩区域之间可能出现的视觉不一致，从而提高外绘结果的视觉连贯性和真实感。</li>
<li><strong>即插即用预处理框架：</strong> LightsOut 作为现有SIFR框架的即插即用预处理步骤，无需额外重新训练即可普遍增强现有SIFR模型的性能，使其在挑战性场景中表现更佳。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong>
LightsOut 在定量和定性评估中均展示了其卓越的性能：</p>
<ul>
<li><strong>显著提升SIFR性能：</strong> 在Flare7K数据集上，LightsOut 显著提升了现有SIFR方法的性能，尤其是在没有光源或光源不完整的场景中。例如，在没有光源的真实图像上，PSNR从26.29 dB提升到28.41 dB。这表明LightsOut能够有效处理不完整照明情况。</li>
<li><strong>优越的光源预测：</strong> 多任务回归模块在光源预测方面表现出色，mIoU分数高于基线U-Net方法（真实图像0.6310 vs 0.6216，合成图像0.6619 vs 0.6563），验证了其回归策略的有效性和可靠性。</li>
<li><strong>高质量的外绘结果：</strong> 定性评估显示，LightsOut 生成的外绘图像具有更高的视觉连贯性和真实感，能够准确捕捉眩光伪影，并与真实场景紧密对齐，优于其他标准扩散外绘方法。</li>
<li><strong>对下游任务的积极影响：</strong> 眩光去除的改进也提升了下游任务（如物体检测）的性能，提高了检测置信度，并使之前因眩光伪影而无法检测的物体得以识别。</li>
<li><strong>消融研究验证了各组件的重要性：</strong> 消融研究证实了噪声再注入、RGB空间混合、光源条件模块以及LoRA微调等每个组件对最终性能的贡献都是至关重要的。</li>
</ul>
<p>这些结果共同证明了LightsOut在解决不完整光源上下文导致的眩光去除挑战方面的有效性，为SIFR领域提供了一个通用且强大的解决方案。</p>
<p><strong>4. 论文中提及的局限性：</strong>
论文中提到了LightsOut的一个主要局限性：</p>
<ul>
<li><strong>计算开销：</strong> 增加的外绘阶段会引入额外的计算开销。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong>
为了解决上述局限性，论文提出了未来的研究方向：</p>
<ul>
<li><strong>端到端优化策略：</strong> 未来的工作可以探索端到端优化策略，以减少LightsOut框架带来的计算开销。</li>
</ul>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose LightsOut, a diffusion-based outpainting
framework tailored to enhance SIFR by reconstructing off-frame light sources.</li>
<li>Our method leverages a multitask regression module and LoRA fine-tuned
diffusion model to ensure realistic and physically consistent outpainting
results.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.15868v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.15868v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.15846v1'></a></p>
<h2 id="3dpr-single-image-3d-portrait-relight-using-generative-priors"><a href="https://arxiv.org/abs/2510.15846v1">3DPR: Single Image 3D Portrait Relight using Generative Priors</a></h2>
<p><strong>Authors:</strong> Pramod Rao, Abhimitra Meka, Xilong Zhou, Gereon Fox, Mallikarjun B R, Fangneng Zhan, Tim Weyrich, Bernd Bickel, Hanspeter Pfister, Wojciech Matusik, Thabo Beeler, Mohamed Elgharib, Marc Habermann, Christian Theobalt</p>
<p><strong>Published:</strong> 2025-10-17</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Rendering novel, relit views of a human head, given a monocular portrait
image as input, is an inherently underconstrained problem. The traditional
graphics solution is to explicitly decompose the input image into geometry,
material and lighting via differentiable rendering; but this is constrained by
the multiple assumptions and approximations of the underlying models and
parameterizations of these scene components. We propose 3DPR, an image-based
relighting model that leverages generative priors learnt from multi-view
One-Light-at-A-Time (OLAT) images captured in a light stage. We introduce a new
diverse and large-scale multi-view 4K OLAT dataset of 139 subjects to learn a
high-quality prior over the distribution of high-frequency face reflectance. We
leverage the latent space of a pre-trained generative head model that provides
a rich prior over face geometry learnt from in-the-wild image datasets. The
input portrait is first embedded in the latent manifold of such a model through
an encoder-based inversion process. Then a novel triplane-based reflectance
network trained on our lightstage data is used to synthesize high-fidelity OLAT
images to enable image-based relighting. Our reflectance network operates in
the latent space of the generative head model, crucially enabling a relatively
small number of lightstage images to train the reflectance model. Combining the
generated OLATs according to a given HDRI environment maps yields physically
accurate environmental relighting results. Through quantitative and qualitative
evaluations, we demonstrate that 3DPR outperforms previous methods,
particularly in preserving identity and in capturing lighting effects such as
specularities, self-shadows, and subsurface scattering. Project Page:
https://vcai.mpi-inf.mpg.de/projects/3dpr/</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Pramod Rao等人撰写的论文“3DPR: Single Image 3D Portrait Relighting with Generative Priors”的全面摘要。</p>
<hr />
<h3 id="3dpr-single-image-3d-portrait-relighting-with-generative-priors">论文摘要：3DPR: Single Image 3D Portrait Relighting with Generative Priors</h3>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决从单张肖像图像生成具有新颖视角和重新打光效果的人头视图这一固有欠约束问题。传统的图形学方法通过可微分渲染将输入图像分解为几何、材质和光照，但这种方法受限于底层模型和场景组件参数化的多种假设和近似。核心挑战在于如何在保持身份一致性、捕捉复杂光照效果（如高光、自阴影和次表面散射）的同时，实现物理准确的重新打光和新颖视图合成。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
*   <strong>3DPR模型：</strong> 提出了一种基于图像的重新打光模型，该模型利用从多视角“一次一光照”（One-Light-at-A-Time, OLAT）图像中学习到的生成先验。
*   <strong>FaceOLAT数据集：</strong> 引入了一个新的、多样化、大规模、多视角4K分辨率OLAT数据集，包含139个受试者，用于学习高频面部反射率分布的高质量先验。该数据集在规模、多样性和分辨率上超越了现有公开数据集，并首次提供了全面的头部反射率建模，包括头发。
*   <strong>生成先验的利用：</strong> 论文利用预训练生成头部模型的潜在空间，该模型提供了从“野外”图像数据中学习到的丰富面部几何先验。通过基于编码器的反演过程，将输入肖像嵌入到该模型的潜在流形中。
*   <strong>三平面反射网络：</strong> 引入了一个新颖的基于三平面的反射网络，该网络在FaceOLAT数据上进行训练，用于合成高保真OLAT图像，从而实现基于图像的重新打光。该反射网络在生成头部模型的潜在空间中操作，使得仅需相对较少的光照舞台图像即可训练反射模型。
*   <strong>物理准确的重新打光：</strong> 通过将生成的OLAT图像根据给定的HDRI环境图进行线性组合，实现了物理准确的环境重新打光结果。
*   <strong>SR编码器和ID-MRF损失：</strong> 引入了特征融合模块（ESR）以结合高频身份特征和反射特征，防止模型过拟合。同时，采用隐式多样化马尔可夫随机场（ID-MRF）损失，鼓励局部特征级别的相似性，有效恢复高频细节。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>性能超越：</strong> 通过定量和定性评估，3DPR在身份保持和捕捉复杂光照效果（如高光、自阴影和次表面散射）方面优于现有方法。
*   <strong>鲁棒性和泛化能力：</strong> 3DPR在各种光照条件下（包括稀疏和彩色光照）均表现出鲁棒性，能够准确再现阴影、高光和其他复杂效果，即使在非传统光照下也能保持肖像的真实感和一致性。
*   <strong>OLAT合成质量：</strong> 3DPR合成的OLAT图像质量显著优于现有方法，能够稳健地泛化到评估数据集和“野外”受试者。
*   <strong>效率：</strong> 3DPR能够以单次前向传播生成OLAT图像，无需耗时的测试时优化，从而在效率和质量之间取得了实用平衡。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>头部后部质量下降：</strong> 尽管FaceOLAT提供了全头覆盖，但由于EG3D先验的限制，头部后部的重新打光质量有所下降。
*   <strong>范围限制：</strong> 目前的工作仅限于面部反射率（面部、眼睛、头皮毛发），不包括头饰和配件（如头盔、太阳镜）。
*   <strong>头发细节不一致：</strong> 继承了EG3D在一致建模细发纤维方面的困难，新颖视图合成在头发区域可能存在局部不一致，OLAT渲染的小错位在线性组合时可能累积为噪声或闪烁，超分辨率阶段在头部旋转时可能出现“跳动”现象。
*   <strong>视图依赖效果不明显：</strong> 尽管OLAT解码器Rdec以视角为条件，但视图依赖效果（如鼻梁和脸颊上的效果）相对不明显。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>更全面的3D生成先验：</strong> 整合更全面的3D生成先验与FaceOLAT，以解决头部后部重新打光的局限性，并实现全头重新打光。
*   <strong>更广泛的物体和材质：</strong> 将反射先验扩展到更广泛的物体和材质，以支持头饰和配件的重新打光。
*   <strong>头发建模改进：</strong> 解决头发区域的局部不一致性、噪声和闪烁问题，可能需要更强的头发高频先验和专门的对齐策略。
*   <strong>增强视图依赖性：</strong> 改进视图依赖性的监督和目标函数，以更强烈地表达视图依赖线索。</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>Rendering novel, relit views of a human head, given a monocular portrait
image as input, is an inherently underconstrained problem.</li>
<li>We propose 3DPR, an image-based
relighting model that leverages generative priors learnt from multi-view
One-Light-at-A-Time (OLAT) images captured in a light stage.</li>
<li>We introduce a new
diverse and large-scale multi-view 4K OLAT dataset of 139 subjects to learn a
high-quality prior over the distribution of high-frequency face reflectance.</li>
<li>Then a novel triplane-based reflectance
network trained on our lightstage data is used to synthesize high-fidelity OLAT
images to enable image-based relighting.</li>
<li>Through quantitative and qualitative
evaluations, we demonstrate that 3DPR outperforms previous methods,
particularly in preserving identity and in capturing lighting effects such as
specularities, self-shadows, and subsurface scattering.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.15846v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.15846v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.15783v1'></a></p>
<h2 id="recon-region-controllable-data-augmentation-with-rectification-and-alignment-for-object-detection"><a href="https://arxiv.org/abs/2510.15783v1">ReCon: Region-Controllable Data Augmentation with Rectification and Alignment for Object Detection</a></h2>
<p><strong>Authors:</strong> Haowei Zhu, Tianxiang Pan, Rui Qin, Jun-Hai Yong, Bin Wang</p>
<p><strong>Published:</strong> 2025-10-17</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>The scale and quality of datasets are crucial for training robust perception
models. However, obtaining large-scale annotated data is both costly and
time-consuming. Generative models have emerged as a powerful tool for data
augmentation by synthesizing samples that adhere to desired distributions.
However, current generative approaches often rely on complex post-processing or
extensive fine-tuning on massive datasets to achieve satisfactory results, and
they remain prone to content-position mismatches and semantic leakage. To
overcome these limitations, we introduce ReCon, a novel augmentation framework
that enhances the capacity of structure-controllable generative models for
object detection. ReCon integrates region-guided rectification into the
diffusion sampling process, using feedback from a pre-trained perception model
to rectify misgenerated regions within diffusion sampling process. We further
propose region-aligned cross-attention to enforce spatial-semantic alignment
between image regions and their textual cues, thereby improving both semantic
consistency and overall image fidelity. Extensive experiments demonstrate that
ReCon substantially improve the quality and trainability of generated data,
achieving consistent performance gains across various datasets, backbone
architectures, and data scales. Our code is available at
https://github.com/haoweiz23/ReCon .</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Haowei Zhu等人撰写的论文“ReCon: Region-Controllable Data Augmentation with Rectification and Alignment for Object Detection”的全面摘要。</p>
<hr />
<p><strong>论文摘要：ReCon: Region-Controllable Data Augmentation with Rectification and Alignment for Object Detection</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决训练鲁棒感知模型（特别是目标检测模型）对大规模高质量标注数据集的严重依赖问题。获取此类数据集成本高昂且耗时。尽管生成模型已成为数据增强的强大工具，但现有方法通常依赖复杂的后处理或在海量数据集上进行大量微调，并且容易出现内容-位置不匹配和语义泄露问题，这限制了生成数据在目标检测任务中的有效性。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
为了克服上述限制，作者提出了 <strong>ReCon (Region-Controllable)</strong>，一个新颖的数据增强框架，旨在增强结构可控生成模型在目标检测中的能力。ReCon 的核心创新包括：</p>
<ul>
<li><strong>区域引导的修正 (Region-Guided Rectification, RGR)：</strong> ReCon 将区域引导的修正直接集成到扩散采样过程中。它利用预训练感知模型的反馈，在扩散采样过程中识别并修正图像中错误生成的区域。通过将采样图像与真实标注进行比较，并注入带噪声的真实数据点来修正错误区域，从而提高准确性，同时保持内容多样性。</li>
<li><strong>区域对齐的交叉注意力 (Region-Aligned Cross-Attention, RACA)：</strong> 为了缓解语义泄露问题，ReCon 引入了区域对齐的交叉注意力机制。该机制在生成过程中强制图像区域与其对应的文本描述（或其他线索）之间实现空间-语义对齐，从而提高语义一致性和整体图像保真度。</li>
<li><strong>训练无关和即插即用：</strong> ReCon 的一个显著优势是它无需额外的训练即可增强现有结构可控生成模型的能力，使其成为一个即插即用的解决方案，尤其适用于数据稀缺的场景。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong>
广泛的实验证明 ReCon 显著提高了生成数据的质量和可训练性，并在各种数据集、骨干架构和数据规模上实现了持续的性能提升。</p>
<ul>
<li><strong>目标检测性能提升：</strong> ReCon 与 ControlNet 结合后，在 COCO 数据集上的 mAP 达到 35.5，超越了 GeoDiffusion 的 34.8。与 GLIGEN 结合后，mAP 从 34.6 提升到 35.5。这表明 ReCon 能够生成更高质量的训练样本，从而显著提升目标检测性能。</li>
<li><strong>数据稀缺场景下的效率：</strong> 在数据稀缺设置中（例如仅使用 10% 的 COCO 数据），ReCon 将 mAP 从 18.5% 提升到 21.7%。与基线方法相比，ReCon 在使用更少增强样本的情况下，实现了可比甚至更好的性能，突出了其数据增强的效率。</li>
<li><strong>组件有效性：</strong> 消融实验证实，RGR 和 RACA 均显著提升了下游模型的性能，通过确保生成样本与标注之间的一致性，提高了合成数据的质量。</li>
<li><strong>图像保真度和标注一致性：</strong> 定性结果表明，ReCon 显著改善了生成样本的图像保真度和定位准确性，有效避免了现有结构控制方法中常见的区域修正不精确和语义泄露问题。</li>
</ul>
<p><strong>4. 论文中提及的局限性：</strong>
论文在附录A中讨论了ReCon的局限性：</p>
<ul>
<li><strong>计算时间增加：</strong> 尽管 ReCon 提高了 FID 分数并改善了检测器的 mAP，但它可能会增加计算时间，尤其是在数据量增长时。</li>
<li><strong>额外的感知模型：</strong> 需要一个额外的感知模型，这增加了开发成本。作者提到已引入加速技术（如快速采样器和轻量级感知模型）来降低这些成本。</li>
<li><strong>潜在的社会影响：</strong> 生成模型可能继承大型、未筛选的视觉-语言数据集中的社会偏见和刻板印象，从而产生歧视性输出。ReCon 通过区域修正和对齐来提高内容准确性并减少偏见。</li>
<li><strong>滥用风险：</strong> 合成图像可能被用于深度伪造等目的，传播错误信息并破坏社会信任。作者强调需要建立法规和最佳实践来确保合成数据模型的负责任创建和使用。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong>
尽管论文没有明确列出未来的研究方向，但从其局限性和贡献中可以推断出以下几点：</p>
<ul>
<li><strong>进一步优化计算效率：</strong> 探索更高效的感知模型（如 EfficientSAM）或集成内存和计算高效的注意力库（如 xFormers），以进一步降低 ReCon 引入的额外计算开销。</li>
<li><strong>偏见检测与缓解：</strong> 深入研究生成模型中的偏见检测和缓解机制，以确保合成数据在社会公平和伦理方面更加负责。</li>
<li><strong>与其他生成框架的结合：</strong> 探索 ReCon 与其他新兴的结构可控生成模型或更先进的扩散模型结合的潜力，以进一步提升性能和多样性。</li>
<li><strong>扩展到其他任务：</strong> 将 ReCon 的区域可控数据增强框架应用于除了目标检测之外的其他感知任务，例如实例分割或语义分割，以验证其通用性。</li>
<li><strong>更精细的区域控制：</strong> 探索更精细的区域控制策略，例如基于更复杂语义属性的修正，以应对更具挑战性的生成场景。</li>
</ul>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>To
overcome these limitations, we introduce ReCon, a novel augmentation framework
that enhances the capacity of structure-controllable generative models for
object detection.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.15783v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.15783v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.15725v1'></a></p>
<h2 id="dgme-t-directional-grid-motion-encoding-for-transformer-based-historical-camera-movement-classification"><a href="https://arxiv.org/abs/2510.15725v1">DGME-T: Directional Grid Motion Encoding for Transformer-Based Historical Camera Movement Classification</a></h2>
<p><strong>Authors:</strong> Tingyu Lin, Armin Dadras, Florian Kleber, Robert Sablatnig</p>
<p><strong>Published:</strong> 2025-10-17</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, eess.IV</p>
<p><strong>Abstract:</strong></p>
<p>Camera movement classification (CMC) models trained on contemporary,
high-quality footage often degrade when applied to archival film, where noise,
missing frames, and low contrast obscure motion cues. We bridge this gap by
assembling a unified benchmark that consolidates two modern corpora into four
canonical classes and restructures the HISTORIAN collection into five balanced
categories. Building on this benchmark, we introduce DGME-T, a lightweight
extension to the Video Swin Transformer that injects directional grid motion
encoding, derived from optical flow, via a learnable and normalised late-fusion
layer. DGME-T raises the backbone's top-1 accuracy from 81.78% to 86.14% and
its macro F1 from 82.08% to 87.81% on modern clips, while still improving the
demanding World-War-II footage from 83.43% to 84.62% accuracy and from 81.72%
to 82.63% macro F1. A cross-domain study further shows that an intermediate
fine-tuning stage on modern data increases historical performance by more than
five percentage points. These results demonstrate that structured motion priors
and transformer representations are complementary and that even a small,
carefully calibrated motion head can substantially enhance robustness in
degraded film analysis. Related resources are available at
https://github.com/linty5/DGME-T.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将对这篇论文摘要进行分析。</p>
<hr />
<p><strong>论文摘要分析：DGME-T: Directional Grid Motion Encoding for Transformer-Based Historical Camera Movement Classification</strong></p>
<p><strong>1. 论文主要贡献的简洁总结 (2-3 句话)</strong></p>
<p>这篇论文主要解决了在历史档案影片上进行摄像机运动分类 (CMC) 时，由于低质量、噪声和缺失帧导致的性能下降问题。作者通过构建一个统一的基准数据集，并引入了DGME-T模型，这是一个基于Video Swin Transformer的轻量级扩展，它通过可学习的晚期融合层注入了基于光流的方向网格运动编码。DGME-T显著提升了在现代和历史影片上的CMC性能，证明了结构化运动先验与Transformer表示的互补性。</p>
<p><strong>2. 关键创新或方法学方法</strong></p>
<p>关键创新在于 <strong>“Directional Grid Motion Encoding (DGME)”</strong> 以及其与 <strong>Transformer (Video Swin Transformer)</strong> 的结合方式。具体来说：</p>
<ul>
<li><strong>DGME：</strong> 从光流中提取方向网格运动编码，这是一种结构化的运动先验信息。它将原始像素级别的运动信息抽象为更鲁棒、更具方向性的网格表示，从而更好地应对历史影片中的噪声和模糊。</li>
<li><strong>轻量级扩展与晚期融合：</strong> DGME-T不是从头构建一个复杂的模型，而是作为Video Swin Transformer的一个“轻量级扩展”，通过一个“可学习和归一化的晚期融合层”将DGME注入到主干网络中。这种设计既利用了Transformer强大的时空特征学习能力，又避免了对整个模型进行大规模修改，同时通过晚期融合确保了运动信息与视觉特征的有效结合。</li>
<li><strong>统一基准数据集：</strong> 论文还构建了一个统一的基准数据集，整合了现代语料库并重新组织了HISTORIAN数据集，为历史影片CMC研究提供了更标准化的评估平台。</li>
</ul>
<p><strong>3. 对该领域的潜在影响</strong></p>
<ul>
<li><strong>提升历史影片分析能力：</strong> 这是最直接的影响。通过提供更鲁棒的CMC模型，可以更好地理解和分析大量未被充分利用的历史影像资料，例如纪录片、新闻档案、家庭录像等。</li>
<li><strong>推动低质量视频分析：</strong> DGME-T的方法学可能不仅仅局限于历史影片，对于其他低质量、高噪声或数据不完整的视频分析任务（如监控视频、无人机拍摄、压缩视频等）也具有借鉴意义。</li>
<li><strong>运动先验与深度学习的融合：</strong> 论文强调了“结构化运动先验和Transformer表示是互补的”，这为未来将传统计算机视觉中的运动分析技术与现代深度学习模型结合提供了新的思路和实证支持。</li>
<li><strong>基准数据集的贡献：</strong> 统一的基准数据集将促进该领域的研究进展，为不同模型提供公平的比较平台。</li>
</ul>
<p><strong>4. 可能受益于这项研究的相关领域或应用</strong></p>
<ul>
<li><strong>电影学与媒体档案管理：</strong> 自动分类和检索历史影片中的特定摄像机运动（如平移、倾斜、缩放），有助于电影分析、内容索引和数字化档案的组织。</li>
<li><strong>视频修复与增强：</strong> 了解摄像机运动有助于在视频修复过程中更准确地进行运动补偿、去抖动或超分辨率。</li>
<li><strong>内容理解与检索：</strong> 在大规模视频数据库中，摄像机运动是重要的语义信息，可以用于更精细的内容检索。</li>
<li><strong>运动分析与行为识别：</strong> 虽然主要针对摄像机运动，但其处理噪声运动信息的方法可能对其他需要从低质量视频中提取运动模式的任务（如人体行为识别、物体轨迹跟踪）有所启发。</li>
<li><strong>艺术史与文化遗产：</strong> 分析历史艺术作品或文化遗产视频中的拍摄手法和风格。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性</strong></p>
<ul>
<li><strong>“轻量级扩展”的局限性：</strong> 尽管轻量级是优点，但它可能意味着DGME-T的运动编码能力受限于其作为“扩展”的定位，可能不如一个从头设计、更复杂的运动特征提取网络。</li>
<li><strong>光流的依赖性：</strong> DGME是“derived from optical flow”。光流本身在低对比度、快速运动或遮挡严重的场景下可能不准确。虽然DGME旨在提高鲁棒性，但其上游的光流估计误差仍可能传递。</li>
<li><strong>特定数据集的泛化性：</strong> 尽管构建了统一基准，但主要关注的是“World-War-II footage”和“modern clips”。对于其他类型的历史影片（如更早期的无声电影、不同地域的档案）或更极端退化的视频，其性能是否能保持仍需验证。</li>
<li><strong>“晚期融合”的潜在限制：</strong> 晚期融合虽然简单有效，但可能不如早期或中期融合能够更深层次地整合运动和视觉信息。这可能限制了模型在某些复杂场景下对运动细节的理解。</li>
<li><strong>计算成本未提及：</strong> 摘要中未提及DGME-T相对于Video Swin Transformer的额外计算成本（包括光流计算和DGME模块）。虽然说是“轻量级”，但具体影响如何仍需查阅全文。</li>
</ul>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Building on this benchmark, we introduce DGME-T, a lightweight
extension to the Video Swin Transformer that injects directional grid motion
encoding, derived from optical flow, via a learnable and normalised late-fusion
layer.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.15725v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.15725v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.15673v1'></a></p>
<h2 id="valeo-near-field-a-novel-dataset-for-pedestrian-intent-detection"><a href="https://arxiv.org/abs/2510.15673v1">Valeo Near-Field: a novel dataset for pedestrian intent detection</a></h2>
<p><strong>Authors:</strong> Antonyo Musabini, Rachid Benmokhtar, Jagdish Bhanushali, Victor Galizzi, Bertrand Luvison, Xavier Perrotton</p>
<p><strong>Published:</strong> 2025-10-17</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>This paper presents a novel dataset aimed at detecting pedestrians'
intentions as they approach an ego-vehicle. The dataset comprises synchronized
multi-modal data, including fisheye camera feeds, lidar laser scans, ultrasonic
sensor readings, and motion capture-based 3D body poses, collected across
diverse real-world scenarios. Key contributions include detailed annotations of
3D body joint positions synchronized with fisheye camera images, as well as
accurate 3D pedestrian positions extracted from lidar data, facilitating robust
benchmarking for perception algorithms. We release a portion of the dataset
along with a comprehensive benchmark suite, featuring evaluation metrics for
accuracy, efficiency, and scalability on embedded systems. By addressing
real-world challenges such as sensor occlusions, dynamic environments, and
hardware constraints, this dataset offers a unique resource for developing and
evaluating state-of-the-art algorithms in pedestrian detection, 3D pose
estimation and 4D trajectory and intention prediction. Additionally, we provide
baseline performance metrics using custom neural network architectures and
suggest future research directions to encourage the adoption and enhancement of
the dataset. This work aims to serve as a foundation for researchers seeking to
advance the capabilities of intelligent vehicles in near-field scenarios.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Antonyo Musabini等人撰写的论文“Valeo Near-Field: a novel dataset for pedestrian intent detection”的全面摘要。</p>
<p><strong>论文摘要：Valeo Near-Field: 行人意图检测的新型数据集</strong></p>
<p>这篇论文介绍了“Valeo Near-Field”数据集，旨在解决智能车辆在近场场景中安全有效地与行人交互的关键挑战，特别是预测行人接近停放车辆时的意图。</p>
<p><strong>1. 主要问题或研究问题：</strong>
该研究旨在解决现有数据集在行人检测和意图预测方面的局限性，这些数据集通常侧重于基本检测或静态姿态，缺乏多模态传感器集成，且未专门针对车辆近场场景中行人与静止车辆的动态交互。核心问题是，如何准确、鲁棒地检测行人意图，尤其是在传感器遮挡、动态环境和硬件限制等真实世界挑战下。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
*   <strong>新型多模态数据集：</strong> Valeo Near-Field是首个公开可用的数据集，专门用于检测行人接近自动驾驶车辆时的意图。它包含同步的多模态数据，包括鱼眼摄像头、激光雷达扫描、超声波传感器读数以及基于运动捕捉的3D身体姿态。
*   <strong>详细且同步的标注：</strong> 数据集提供了与鱼眼图像同步的详细3D身体关节位置标注，以及从激光雷达数据中提取的精确3D行人位置，这为感知算法提供了强大的基准测试能力。
*   <strong>真实世界场景覆盖：</strong> 数据收集涵盖了多样化的真实世界场景，包括室内和室外停车场，并由13名参与者执行意图和非意图场景，模拟了行人与静止车辆的交互。
*   <strong>同步技术：</strong> 论文提出了一种定制的同步技术，通过特定的手臂动作将运动捕捉数据与车辆传感器数据进行时间同步，并结合激光雷达扫描进行3D位置和方向的手动标注，以最小化运动捕捉数据中累积的位置误差。
*   <strong>综合基准测试套件：</strong> 随数据集一起发布了一个全面的基准测试套件，包含针对准确性、效率和嵌入式系统可扩展性的评估指标。</p>
<p><strong>3. 主要结果及其意义：</strong>
论文提供了使用YOLOX进行2D行人检测、ViTPose进行2D骨架估计、以及基于Transformer的方法进行3D姿态估计和3D行人定位的基线性能指标。
*   <strong>3D行人定位：</strong> 在VNF公共测试集上，3D定位的平均距离误差（ADE）在不同检测区域和用例（室内/室外、男性/女性）中保持一致。在5-10米区域的准确性高于0-5米区域，这可能与该区域行人通常能被多个摄像头捕获以及车辆开门造成的遮挡较少有关。
*   <strong>骨架姿态估计：</strong> 3D姿态估计的平均关节位置误差（MPJPE）随距离增加而增加，但在0-5米区域（非常近的区域）误差也较高，这可能是因为部分关节在摄像头视野之外。室内和室外场景以及不同性别参与者的性能相似。
*   <strong>身体高度误差：</strong> 3D姿态估计器预测的行人高度通常比实际值高约7%。女性参与者的误差通常更大，这可能与数据集中女性身高普遍低于男性有关。
这些基线结果为未来的研究和基准测试提供了参考点，展示了在近场场景中进行行人感知任务的初步可行性。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>场景限制：</strong> 数据集主要关注静止车辆的交互，动态场景（如车辆移动）尚未涵盖。
*   <strong>运动捕捉漂移：</strong> 尽管运动捕捉提供了精确的3D姿态估计，但累积的漂移可能随时间引入轻微的定位误差。
*   <strong>环境条件：</strong> 数据集的当前范围并未广泛涵盖多样化的天气条件，这在真实世界应用中是一个关键因素。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>增强个体行人感知任务：</strong> 改进2D骨架检测和跟踪、3D姿态提升、多摄像头3D定位以及行人轨迹和意图预测。
*   <strong>端到端处理流程：</strong> 研究人员可以探索联合优化所有这些任务的端到端处理流程，以提高鲁棒性。
*   <strong>数据增强：</strong> 利用合成数据增强或自监督学习技术，以提高模型的泛化能力。
*   <strong>扩展数据集：</strong> 未来的数据集迭代应旨在包含更广泛的环境条件，以提高模型在真实世界应用中的鲁棒性和泛化能力。</p>
<p>总而言之，Valeo Near-Field数据集为智能车辆在近场场景中的行人感知和安全研究奠定了宝贵的基础，通过提供独特的多模态数据和基准测试，推动了行人意图检测、3D姿态估计和4D轨迹与意图预测领域的发展。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>This paper presents a novel dataset aimed at detecting pedestrians'
intentions as they approach an ego-vehicle.</li>
<li>Key contributions include detailed annotations of
3D body joint positions synchronized with fisheye camera images, as well as
accurate 3D pedestrian positions extracted from lidar data, facilitating robust
benchmarking for perception algorithms.</li>
<li>By addressing
real-world challenges such as sensor occlusions, dynamic environments, and
hardware constraints, this dataset offers a unique resource for developing and
evaluating state-of-the-art algorithms in pedestrian detection, 3D pose
estimation and 4D trajectory and intention prediction.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.15673v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.15673v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.15602v1'></a></p>
<h2 id="quantized-fca-efficient-zero-shot-texture-anomaly-detection"><a href="https://arxiv.org/abs/2510.15602v1">Quantized FCA: Efficient Zero-Shot Texture Anomaly Detection</a></h2>
<p><strong>Authors:</strong> Andrei-Timotei Ardelean, Patrick Rückbeil, Tim Weyrich</p>
<p><strong>Published:</strong> 2025-10-17</p>
<p><strong>Categories:</strong> cs.CV, I.4.7; I.2.10; I.3.8</p>
<p><strong>Abstract:</strong></p>
<p>Zero-shot anomaly localization is a rising field in computer vision research,
with important progress in recent years. This work focuses on the problem of
detecting and localizing anomalies in textures, where anomalies can be defined
as the regions that deviate from the overall statistics, violating the
stationarity assumption. The main limitation of existing methods is their high
running time, making them impractical for deployment in real-world scenarios,
such as assembly line monitoring. We propose a real-time method, named QFCA,
which implements a quantized version of the feature correspondence analysis
(FCA) algorithm. By carefully adapting the patch statistics comparison to work
on histograms of quantized values, we obtain a 10x speedup with little to no
loss in accuracy. Moreover, we introduce a feature preprocessing step based on
principal component analysis, which enhances the contrast between normal and
anomalous features, improving the detection precision on complex textures. Our
method is thoroughly evaluated against prior art, comparing favorably with
existing methods. Project page:
https://reality.tf.fau.de/pub/ardelean2025quantized.html</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Andrei-Timotei Ardelean, Patrick Rückbeil, Tim Weyrich撰写的论文“Quantized FCA: Efficient Zero-Shot Texture Anomaly Detection”的全面摘要。</p>
<hr />
<p><strong>论文摘要：Quantized FCA: Efficient Zero-Shot Texture Anomaly Detection</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决零样本纹理异常定位（Zero-Shot Texture Anomaly Localization, ZSAL）的问题。ZSAL的目标是在没有预先训练或任何异常指示的情况下，在一张图像中检测并定位纹理中的异常区域。现有方法的主要限制是运行时间过长，使其在实际应用（如生产线监控）中不切实际。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
该论文提出了一个名为<strong>Quantized Feature Correspondence Analysis (QFCA)</strong>的实时方法，它对现有最先进的特征对应分析（Feature Correspondence Analysis, FCA）算法进行了量化改进。主要贡献包括：</p>
<ul>
<li><strong>量化补丁统计比较：</strong> QFCA通过将补丁统计比较适应于量化值的直方图，实现了显著的效率提升。这种方法在准确性损失很小的情况下，实现了10倍的加速。</li>
<li><strong>基于主成分分析（PCA）的特征预处理：</strong> 引入了一个新的特征预处理步骤，通过PCA增强正常特征和异常特征之间的对比度。这显著提高了复杂纹理上的检测精度，尤其是在纹理周期大于补丁大小的双峰纹理中。</li>
<li><strong>高效的局部平均池化：</strong> 解决了现代机器学习库中局部平均池化实现效率低下的计算瓶颈，通过使用求和面积表（integral image）实现了与核大小无关的O(H x W)复杂度，进一步缩短了运行时间。</li>
<li><strong>算法正确性证明：</strong> 论文提供了算法正确性的证明，表明其量化方法与原始FCA算法在量化值上产生相同的失配分数，并且与2-Wasserstein距离的梯度相关。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong>
QFCA在MVTec AD、DTD-Synthetic和Woven Fabric Textures等数据集上进行了全面评估，并与现有零样本方法（包括基于VLM和纹理特定方法）进行了比较。</p>
<ul>
<li><strong>显著的运行时间提升：</strong> QFCA实现了比现有方法快10倍的运行速度，使其能够实时进行异常定位，解决了现有方法的主要限制。</li>
<li><strong>保持或超越现有方法的准确性：</strong> 在MVTec AD数据集上，QFCA在PRO、AUROCS和F1等指标上与FCA相当，甚至在某些情况下（QFCA+）表现更优。在DTD-Synthetic和WFT数据集上，QFCA+也取得了最佳指标。</li>
<li><strong>复杂纹理上的改进：</strong> 引入的特征预处理（QFCA+）在复杂纹理上带来了显著的性能提升，减少了误报。</li>
<li><strong>量化效率：</strong> 即使使用少量（例如16个）直方图bin，量化方法也能匹配全精度指标，证明了其高效性。</li>
</ul>
<p>这些结果表明，QFCA在准确性和运行时间之间取得了最佳权衡，使其成为实际部署中零样本纹理异常定位的有力工具。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>对纹理数据的适用性：</strong> QFCA作为一种优化的零样本纹理异常定位算法，继承了这类方法的局限性。它主要适用于纹理状数据或表面，而不适用于任意对象。
*   <strong>不使用大型VLM：</strong> 由于不使用大型视觉语言模型（VLM）来注入通用异常知识，QFCA在处理非纹理图像时可能不如基于VLM的方法通用。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>将QFCA的效率优势推广到其他管道：</strong> 论文中提出的高效局部平均池化算法可以潜在地应用于其他计算机视觉管道，以获得类似的加速效益。
*   <strong>进一步探索特征预处理：</strong> 论文中引入的特征预处理步骤（QFCA+）在复杂纹理上表现出色，未来可以进一步研究其在更广泛的异常检测算法中的应用。
*   <strong>扩展到非纹理图像：</strong> 虽然目前主要针对纹理，但通过结合更通用的特征表示或适应性更强的参考选择机制，未来可能探索QFCA在更广泛的图像类型上的应用。</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose a real-time method, named QFCA,
which implements a quantized version of the feature correspondence analysis
(FCA) algorithm.</li>
<li>Moreover, we introduce a feature preprocessing step based on
principal component analysis, which enhances the contrast between normal and
anomalous features, improving the detection precision on complex textures.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.15602v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.15602v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.15564v1'></a></p>
<h2 id="imaginarium-vision-guided-high-quality-3d-scene-layout-generation"><a href="https://arxiv.org/abs/2510.15564v1">Imaginarium: Vision-guided High-Quality 3D Scene Layout Generation</a></h2>
<p><strong>Authors:</strong> Xiaoming Zhu, Xu Huang, Qinghongbing Xie, Zhi Deng, Junsheng Yu, Yirui Guan, Zhongyuan Liu, Lin Zhu, Qijun Zhao, Ligang Liu, Long Zeng</p>
<p><strong>Published:</strong> 2025-10-17</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Generating artistic and coherent 3D scene layouts is crucial in digital
content creation. Traditional optimization-based methods are often constrained
by cumbersome manual rules, while deep generative models face challenges in
producing content with richness and diversity. Furthermore, approaches that
utilize large language models frequently lack robustness and fail to accurately
capture complex spatial relationships. To address these challenges, this paper
presents a novel vision-guided 3D layout generation system. We first construct
a high-quality asset library containing 2,037 scene assets and 147 3D scene
layouts. Subsequently, we employ an image generation model to expand prompt
representations into images, fine-tuning it to align with our asset library. We
then develop a robust image parsing module to recover the 3D layout of scenes
based on visual semantics and geometric information. Finally, we optimize the
scene layout using scene graphs and overall visual semantics to ensure logical
coherence and alignment with the images. Extensive user testing demonstrates
that our algorithm significantly outperforms existing methods in terms of
layout richness and quality. The code and dataset will be available at
https://github.com/HiHiAllen/Imaginarium.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Xiaoming Zhu等人撰写的论文“Imaginarium: Vision-guided High-Quality 3D Scene Layout Generation”的全面摘要。</p>
<hr />
<p><strong>论文摘要：Imaginarium: 视觉引导的高质量3D场景布局生成</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决数字内容创作中生成艺术性和连贯的3D场景布局的关键挑战。传统方法受限于繁琐的手动规则，而深度生成模型在生成内容丰富性和多样性方面面临困难。此外，利用大型语言模型（LLM）的方法往往缺乏鲁棒性，无法准确捕捉复杂的空间关系。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
为了解决上述问题，作者提出了一个新颖的视觉引导3D布局生成系统，其主要创新和贡献包括：
*   <strong>高质量3D场景布局数据集的构建：</strong> 论文首先构建了一个包含2,037个高质量场景资产和147个3D场景布局的资产库，并计划开源该数据集，以造福研究社区。
*   <strong>视觉引导的提示扩展：</strong> 系统利用图像生成模型（Flux模型）将用户输入的文本提示扩展为引导图像。通过对Flux模型进行微调，使其与资产库对齐，确保生成图像的风格一致性，从而增强后续资产检索和布局转换估计的鲁棒性。
*   <strong>鲁棒的图像解析模块：</strong> 开发了一个基于预训练视觉模型的图像解析模块，该模块集成了视觉语义分割、从单幅图像中进行几何解析（包括3D定向包围盒、墙壁、地板和天花板的平面检测）以及基于图的场景图逻辑构建。
*   <strong>语义特征匹配与变换估计：</strong> 采用语义特征匹配策略从资产库中检索与引导图像最相似的对象。然后，结合视觉语义特征、几何信息和场景布局逻辑，迭代求解每个前景对象的旋转、平移和缩放变换。
*   <strong>场景布局优化：</strong> 最后，通过场景图逻辑和图像语义解析对整体3D场景布局进行一致性优化，确保最终布局在视觉上和逻辑上与引导图像高度一致。
*   <strong>内部布局功能：</strong> 引入了内部布局功能，允许资产在其他资产内部进行排列，优化空间利用并提高场景真实感。</p>
<p><strong>3. 主要结果及其意义：</strong>
广泛的用户测试表明，该算法在布局丰富性和质量方面显著优于现有方法。
*   <strong>用户评估：</strong> 在合理性、真实性、连贯性和美学吸引力方面，该方法在艺术学生和专业艺术家评估中均优于现有基线方法（如DiffuScene, HOLODECK, LayoutGPT, InstructScene）。
*   <strong>3D场景布局重建的保真度和相似性：</strong> 在数据集场景上的评估显示，该方法在主要对象恢复（92.31%）、类别保留（95.83%）、旋转（AUC@60°为74.83%）和位移（AUC@0.5m为84.32%）方面表现出高保真度，场景图准确率达到93.26%。
*   <strong>旋转变换估计：</strong> 在3D-Future数据集上的评估显示，该方法在类别级别（70.06%）和实例级别（81.44%）的旋转估计方面显著优于其他基线方法。
*   <strong>消融研究：</strong> 详细的消融研究验证了Flux模型微调、结合同态和几何信息的旋转变换估计以及场景布局细化管道中每个组件的有效性，证明了每个组件对系统性能的积极贡献。</p>
<p>这些结果表明，该系统能够生成更自然、详细且视觉吸引人的3D场景布局，显著提高了布局质量，并有望将专业工作流程中典型的2.5小时时间缩短至240秒内。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>图像生成模型的一致性挑战：</strong> 尽管取得了进展，但在复杂场景中，图像生成模型在多个对象之间实现高一致性仍然是一个主要挑战。
*   <strong>单幅图像姿态估计的准确性：</strong> 从单幅图像中准确估计姿态仍然具有挑战性，尤其是在严重遮挡的情况下。
*   <strong>语义-结构不匹配：</strong> 图像生成器可能产生资产库中不存在的新拓扑结构的对象，导致资产检索不正确，进而使场景图派生的几何和关系约束失效。
*   <strong>严重遮挡导致的姿态模糊性：</strong> 遮挡导致的局部视图信息有限，可能导致初始姿态估计不可靠，后续优化阶段可能失败。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>视觉基础模型的进步：</strong> 随着视觉基础模型的进一步发展，上述局限性有望得到缓解。
*   <strong>多视图透视信息的整合：</strong> 引入多视图透视信息（如MVD方法）可以为更鲁棒的场景分析提供有前景的途径，以解决姿态模糊性问题。
*   <strong>自动化3D数据生成引擎：</strong> 将丰富的2D视觉模型放置知识转化为3D资产放置数据，以解决3D场景生成任务中的数据稀缺问题，从而更有效地训练3D场景理解和布局生成模型。
*   <strong>2D和3D之间更连贯的编辑能力：</strong> 探索2D和3D之间更连贯的编辑能力，以使未来的系统更加用户友好。</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address these challenges, this paper
presents a novel vision-guided 3D layout generation system.</li>
<li>Extensive user testing demonstrates
that our algorithm significantly outperforms existing methods in terms of
layout richness and quality.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.15564v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.15564v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.15557v1'></a></p>
<h2 id="clappertext-a-benchmark-for-text-recognition-in-low-resource-archival-documents"><a href="https://arxiv.org/abs/2510.15557v1">ClapperText: A Benchmark for Text Recognition in Low-Resource Archival Documents</a></h2>
<p><strong>Authors:</strong> Tingyu Lin, Marco Peer, Florian Kleber, Robert Sablatnig</p>
<p><strong>Published:</strong> 2025-10-17</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, eess.IV</p>
<p><strong>Abstract:</strong></p>
<p>This paper presents ClapperText, a benchmark dataset for handwritten and
printed text recognition in visually degraded and low-resource settings. The
dataset is derived from 127 World War II-era archival video segments containing
clapperboards that record structured production metadata such as date,
location, and camera-operator identity. ClapperText includes 9,813 annotated
frames and 94,573 word-level text instances, 67% of which are handwritten and
1,566 are partially occluded. Each instance includes transcription, semantic
category, text type, and occlusion status, with annotations available as
rotated bounding boxes represented as 4-point polygons to support spatially
precise OCR applications. Recognizing clapperboard text poses significant
challenges, including motion blur, handwriting variation, exposure
fluctuations, and cluttered backgrounds, mirroring broader challenges in
historical document analysis where structured content appears in degraded,
non-standard forms. We provide both full-frame annotations and cropped word
images to support downstream tasks. Using a consistent per-video evaluation
protocol, we benchmark six representative recognition and seven detection
models under zero-shot and fine-tuned conditions. Despite the small training
set (18 videos), fine-tuning leads to substantial performance gains,
highlighting ClapperText's suitability for few-shot learning scenarios. The
dataset offers a realistic and culturally grounded resource for advancing
robust OCR and document understanding in low-resource archival contexts. The
dataset and evaluation code are available at
https://github.com/linty5/ClapperText.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于Tingyu Lin, Marco Peer, Florian Kleber, Robert Sablatnig撰写的论文“ClapperText: A Benchmark for Text Recognition in Low-Resource Archival Documents”的全面摘要，用中文写成：</p>
<p><strong>论文摘要：ClapperText：低资源档案文档中的文本识别基准</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决历史档案视频材料中手写和印刷文本识别的挑战，尤其是在视觉退化和低资源设置下。现有的OCR系统在处理运动模糊、曝光波动、手写变异和杂乱背景等问题时表现出局限性，而这些问题在历史文档分析中普遍存在。研究的核心问题是：如何在这些具有挑战性的条件下，有效地检测和识别档案视频中场记板上的结构化文本，并为未来的OCR研究提供一个可靠的基准。</p>
<p><strong>2. 主要创新或方法论贡献：</strong>
*   <strong>ClapperText数据集的创建：</strong> 论文引入了一个新的基准数据集ClapperText，它从127个二战时期的档案视频片段中提取，包含9,813个带标注的帧和94,573个词级文本实例。这些实例包括转录、语义类别、文本类型和遮挡状态，并以旋转边界框（4点多边形）的形式提供，支持精确的OCR应用。
*   <strong>低资源和视觉多样性：</strong> 数据集特别强调低资源环境（训练集仅18个视频），并包含大量手写文本（67%）和部分遮挡文本（1,566个），以及运动模糊、手写变异、曝光波动和杂乱背景等视觉退化特征，使其成为一个真实且具有文化意义的资源。
*   <strong>全面的基准测试：</strong> 论文对六种文本识别模型和七种文本检测模型进行了零样本和微调条件下的基准测试，评估了现代OCR系统在ClapperText数据集上的性能。
*   <strong>评估协议：</strong> 采用一致的每视频评估协议，确保了评估的平衡性。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>零样本性能显著下降：</strong> 所有模型在ClapperText数据集上的零样本性能均显著下降，即使在传统基准上表现出色，也凸显了ClapperText与现有数据集之间的领域差距。例如，NRTR-R31 (1/8)在常规基准上达到94%以上，但在ClapperText上仅为67.46%。
*   <strong>微调的显著性能提升：</strong> 尽管训练集规模较小（18个视频），但微调显著提高了所有模型的性能。例如，NRTR-R31 (1/16)在微调后从65.56%提升到77.24%，表明ClapperText非常适合少样本学习场景。
*   <strong>手写文本的挑战性：</strong> 手写样本在零样本设置下构成了更大的挑战，微调在手写文本上的性能提升比印刷文本更显著，表明领域适应对手写OCR的益处。
*   <strong>遮挡文本的难度：</strong> 论文指出，遮挡文本的识别难度很高，NRTR (Mod-Trans.)在零样本设置下对遮挡词的准确率仅为18.06%，微调后也只有30.14%。
*   <strong>检测模型的表现：</strong> 检测模型在ClapperText上的零样本性能也大幅下降，但微调同样带来了显著提升。TextSnake (R50+OCLIP)在推理速度和准确性之间取得了较好的平衡，适用于实时处理。</p>
<p><strong>4. 论文中提到的局限性：</strong>
*   <strong>训练集规模小：</strong> 尽管微调带来了显著提升，但训练集规模（18个视频）仍然很小，这限制了模型在更广泛场景下的泛化能力。
*   <strong>遮挡文本的挑战：</strong> 即使经过微调，遮挡文本的识别仍然是一个难题，需要进一步研究。
*   <strong>模型对语言先验的依赖：</strong> 某些模型（如NRTR）在处理缩写或非词汇标记时，可能会表现出语言驱动的偏差，导致语义上看似合理但实际错误的预测。
*   <strong>背景干扰和结构区分：</strong> 在杂乱背景下，模型可能出现误检或漏检，TextSnake在处理多词字段时可能将相邻词过度分组，而DBNet++在布局敏感性方面表现更好，但仍有未检测到的词。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>领域适应：</strong> 进一步研究如何将文本模型适应到非传统领域，以提高在低资源档案上下文中的OCR性能。
*   <strong>时间建模：</strong> 探索利用视频序列中的时间信息来增强文本识别和检测的鲁棒性。
*   <strong>语义上下文集成：</strong> 整合跨帧的语义上下文信息，以更好地理解和识别场记板上的结构化内容。
*   <strong>处理遮挡和视觉噪声：</strong> 开发更鲁棒的方法来处理严重遮挡和各种视觉噪声，这是档案材料中的常见挑战。
*   <strong>少样本学习：</strong> ClapperText数据集的特性使其成为少样本学习场景的理想测试平台，未来可以探索更先进的少样本学习技术。</p>
<p>总而言之，ClapperText数据集及其基准测试为推动低资源档案文档中的OCR和文档理解研究提供了一个宝贵且具有挑战性的资源，并揭示了现有模型在处理手写、遮挡和视觉退化文本方面的持续局限性。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>The
dataset offers a realistic and culturally grounded resource for advancing
robust OCR and document understanding in low-resource archival contexts.</li>
<li>The
dataset and evaluation code are available at
https://github.com/linty5/ClapperText.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.15557v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.15557v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-10-20 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
