<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2026-01-14 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-13
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2026-01-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-30
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2026-01-13/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2026-01-15/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2026-01-14">Arxiv Computer Vision Papers - 2026-01-14</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#video-generation-models-in-robotics-applications-research-challenges-future-directions" class="nav-link">Video Generation Models in Robotics -- Applications, Research Challenges, Future Directions</a>
                </li>
                <li class="nav-item">
                    <a href="#raven-erasing-invisible-watermarks-via-novel-view-synthesis" class="nav-link">RAVEN: Erasing Invisible Watermarks via Novel View Synthesis</a>
                </li>
                <li class="nav-item">
                    <a href="#3am-segment-anything-with-geometric-consistency-in-videos" class="nav-link">3AM: Segment Anything with Geometric Consistency in Videos</a>
                </li>
                <li class="nav-item">
                    <a href="#motion-attribution-for-video-generation" class="nav-link">Motion Attribution for Video Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#reasoning-matters-for-3d-visual-grounding" class="nav-link">Reasoning Matters for 3D Visual Grounding</a>
                </li>
                <li class="nav-item">
                    <a href="#near-perfect-photo-id-of-the-hula-painted-frog-with-zero-shot-deep-local-feature-matching" class="nav-link">Near-perfect photo-ID of the Hula painted frog with zero-shot deep local-feature matching</a>
                </li>
                <li class="nav-item">
                    <a href="#aggregating-diverse-cue-experts-for-ai-generated-image-detection" class="nav-link">Aggregating Diverse Cue Experts for AI-Generated Image Detection</a>
                </li>
                <li class="nav-item">
                    <a href="#vlingnav-embodied-navigation-with-adaptive-reasoning-and-visual-assisted-linguistic-memory" class="nav-link">VLingNav: Embodied Navigation with Adaptive Reasoning and Visual-Assisted Linguistic Memory</a>
                </li>
                <li class="nav-item">
                    <a href="#saferedir-prompt-embedding-redirection-for-robust-unlearning-in-image-generation-models" class="nav-link">SafeRedir: Prompt Embedding Redirection for Robust Unlearning in Image Generation Models</a>
                </li>
                <li class="nav-item">
                    <a href="#vidore-v3-a-comprehensive-evaluation-of-retrieval-augmented-generation-in-complex-real-world-scenarios" class="nav-link">ViDoRe V3: A Comprehensive Evaluation of Retrieval Augmented Generation in Complex Real-World Scenarios</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2026-01-14">Arxiv Computer Vision Papers - 2026-01-14</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>好的，这是一份针对近期 Arxiv 计算机视觉领域论文的简明执行摘要，旨在帮助忙碌的研究人员快速了解该领域最重要的发展：</p>
<hr />
<p><strong>执行摘要：Arxiv 计算机视觉论文速览 (2026-01-12)</strong></p>
<p><strong>主要趋势与主题：</strong></p>
<p>本期 Arxiv 论文集聚焦于<strong>视频理解与生成</strong>、<strong>3D 视觉应用</strong>以及<strong>模型鲁棒性与安全性</strong>。其中，视频生成模型在机器人领域的应用、视频中的几何一致性分割、以及视频生成中的运动归因是突出亮点。同时，AI 生成图像的检测与溯源，以及具身导航中的推理与记忆能力也得到了深入探讨。</p>
<p><strong>亮点与创新：</strong></p>
<ul>
<li><strong>视频生成与机器人应用 (论文 1):</strong> "Video Generation Models in Robotics" 明确指出了视频生成模型在机器人领域的巨大潜力，并梳理了当前的研究挑战与未来方向，预示着该领域将迎来快速发展。</li>
<li><strong>视频中的几何一致性分割 (论文 3):</strong> "3AM: Segment Anything with Geometric Consistency in Videos" 提出了一种在视频中实现几何一致性分割的新方法，这对于视频内容理解和编辑具有重要意义。</li>
<li><strong>AI 生成图像检测与溯源 (论文 7 &amp; 2):</strong> "Aggregating Diverse Cue Experts for AI-Generated Image Detection" 和 "RAVEN: Erasing Invisible Watermarks via Novel View Synthesis" 分别从多角度探讨了 AI 生成图像的检测和水印擦除问题，显示出对内容真实性验证的日益重视。</li>
<li><strong>具身导航与推理 (论文 8):</strong> "VLingNav: Embodied Navigation with Adaptive Reasoning and Visual-Assisted Linguistic Memory" 在具身导航领域引入了自适应推理和视觉辅助语言记忆，是迈向更智能自主导航的重要一步。</li>
</ul>
<p><strong>新兴研究方向与技术：</strong></p>
<ul>
<li><strong>视频生成中的运动理解：</strong> 论文 4 "Motion Attribution for Video Generation" 强调了运动在视频生成中的关键作用，预示着未来研究将更侧重于对运动的精细控制和理解。</li>
<li><strong>3D 视觉推理：</strong> 论文 5 "Reasoning Matters for 3D Visual Grounding" 指出在 3D 视觉任务中，推理能力至关重要，这表明 3D 视觉正从单纯的感知向更深层次的理解和推理发展。</li>
<li><strong>模型的可解释性与安全性：</strong> 论文 9 "SafeRedir: Prompt Embedding Redirection for Robust Unlearning in Image Generation Models" 关注模型在图像生成中的“遗忘”能力，是提升模型安全性和可控性的重要研究方向。</li>
<li><strong>零样本学习与跨模态匹配：</strong> 论文 6 "Near-perfect photo-ID of the Hula painted frog with zero-shot deep local-feature matching" 展示了零样本学习在生物识别等领域的强大能力，预示着跨模态匹配和零样本学习将有更广泛的应用。</li>
</ul>
<p><strong>建议阅读论文：</strong></p>
<p>为了快速把握当前研究前沿，建议重点阅读以下论文：</p>
<ol>
<li><strong>"Video Generation Models in Robotics -- Applications, Research Challenges, Future Directions" (论文 1):</strong> 提供对视频生成在机器人领域应用的宏观视角和未来展望。</li>
<li><strong>"3AM: Segment Anything with Geometric Consistency in Videos" (论文 3):</strong> 介绍了一种新颖的视频分割技术，具有重要的实际应用价值。</li>
<li><strong>"VLingNav: Embodied Navigation with Adaptive Reasoning and Visual-Assisted Linguistic Memory" (论文 8):</strong> 展示了具身导航领域前沿的推理和记忆技术。</li>
<li><strong>"Aggregating Diverse Cue Experts for AI-Generated Image Detection" (论文 7):</strong> 提供了应对 AI 生成内容挑战的有效方法。</li>
</ol>
<hr />
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2601.07823v1">Video Generation Models in Robotics -- Applications, Research Challenges, Future Directions</a></li>
<li><a href="#2601.08832v1">RAVEN: Erasing Invisible Watermarks via Novel View Synthesis</a></li>
<li><a href="#2601.08831v1">3AM: Segment Anything with Geometric Consistency in Videos</a></li>
<li><a href="#2601.08828v1">Motion Attribution for Video Generation</a></li>
<li><a href="#2601.08811v1">Reasoning Matters for 3D Visual Grounding</a></li>
<li><a href="#2601.08798v1">Near-perfect photo-ID of the Hula painted frog with zero-shot deep local-feature matching</a></li>
<li><a href="#2601.08790v1">Aggregating Diverse Cue Experts for AI-Generated Image Detection</a></li>
<li><a href="#2601.08665v1">VLingNav: Embodied Navigation with Adaptive Reasoning and Visual-Assisted Linguistic Memory</a></li>
<li><a href="#2601.08623v1">SafeRedir: Prompt Embedding Redirection for Robust Unlearning in Image Generation Models</a></li>
<li><a href="#2601.08620v1">ViDoRe V3: A Comprehensive Evaluation of Retrieval Augmented Generation in Complex Real-World Scenarios</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2601.07823v1'></a></p>
<h2 id="video-generation-models-in-robotics-applications-research-challenges-future-directions"><a href="https://arxiv.org/abs/2601.07823v1">Video Generation Models in Robotics -- Applications, Research Challenges, Future Directions</a></h2>
<p><strong>Authors:</strong> Zhiting Mei, Tenny Yin, Ola Shorinwa, Apurva Badithela, Zhonghe Zheng, Joseph Bruno, Madison Bland, Lihan Zha, Asher Hancock, Jaime Fernández Fisac, Philip Dames, Anirudha Majumdar</p>
<p><strong>Published:</strong> 2026-01-12</p>
<p><strong>Categories:</strong> eess.SY, cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>Video generation models have emerged as high-fidelity models of the physical world, capable of synthesizing high-quality videos capturing fine-grained interactions between agents and their environments conditioned on multi-modal user inputs. Their impressive capabilities address many of the long-standing challenges faced by physics-based simulators, driving broad adoption in many problem domains, e.g., robotics. For example, video models enable photorealistic, physically consistent deformable-body simulation without making prohibitive simplifying assumptions, which is a major bottleneck in physics-based simulation. Moreover, video models can serve as foundation world models that capture the dynamics of the world in a fine-grained and expressive way. They thus overcome the limited expressiveness of language-only abstractions in describing intricate physical interactions. In this survey, we provide a review of video models and their applications as embodied world models in robotics, encompassing cost-effective data generation and action prediction in imitation learning, dynamics and rewards modeling in reinforcement learning, visual planning, and policy evaluation. Further, we highlight important challenges hindering the trustworthy integration of video models in robotics, which include poor instruction following, hallucinations such as violations of physics, and unsafe content generation, in addition to fundamental limitations such as significant data curation, training, and inference costs. We present potential future directions to address these open research challenges to motivate research and ultimately facilitate broader applications, especially in safety-critical settings.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Video Generation Models in Robotics: Applications, Research Challenges, Future Directions”的全面中文摘要：</p>
<p><strong>论文题目：</strong> Video Generation Models in Robotics: Applications, Research Challenges, Future Directions</p>
<p><strong>作者：</strong> Zhiting Mei, Tenny Yin, Ola Shorinwa, Apurva Badithela, Zhonghe Zheng, Joseph Bruno, Madison Bland, Lihan Zha, Asher Hancock, Jaime Fernández Fisac, Philip Dames, Anirudha Majumdar</p>
<p><strong>摘要：</strong></p>
<p>这篇综述论文深入探讨了视频生成模型在机器人领域的应用、面临的挑战以及未来的发展方向。论文的核心在于<strong>将视频生成模型视为高保真度的“具身世界模型”（embodied world models）</strong>，它们能够生成逼真的、物理上一致的视频，捕捉精细的代理-环境交互，从而克服传统基于物理的模拟器在复杂场景（如可变形物体模拟）中的局限性。</p>
<p><strong>1. 主要问题或研究问题：</strong></p>
<p>论文主要关注以下几个核心问题：
* <strong>如何利用视频生成模型来构建更强大、更具表现力的机器人世界模型？</strong> 传统方法（如基于语言的抽象或简化的物理模拟）在描述复杂物理交互方面存在不足。
* <strong>视频生成模型在机器人领域的具体应用有哪些？</strong> 包括数据生成、策略学习、策略评估和视觉规划等。
* <strong>视频生成模型在机器人领域的集成面临哪些关键挑战？</strong> 这些挑战阻碍了其在安全关键型应用中的可靠部署。
* <strong>如何克服这些挑战，推动视频生成模型在机器人领域的更广泛应用？</strong></p>
<p><strong>2. 关键创新或方法论贡献：</strong></p>
<p>这篇论文本身是一篇<strong>综述性文章</strong>，其主要贡献在于：
* <strong>系统性地梳理了视频生成模型在机器人领域的最新进展。</strong> 论文对视频生成模型进行了分类（如扩散/流匹配模型、联合嵌入预测架构），并详细介绍了其在机器人领域的四类主要应用：
    * <strong>模仿学习中的数据生成与动作预测：</strong> 利用视频模型生成逼真的专家演示，降低数据收集成本。
    * <strong>强化学习中的动力学与奖励建模：</strong> 利用视频模型预测环境动力学和奖励信号，提高训练效率。
    * <strong>可扩展的策略评估：</strong> 利用视频模型进行策略评估，避免昂贵的真实世界实验。
    * <strong>视觉规划：</strong> 利用视频模型生成未来场景预测，指导机器人进行规划。
* <strong>深入分析了视频生成模型在机器人应用中的局限性。</strong> 论文详细列举了诸如指令遵循能力差、物理不一致的“幻觉”生成、不安全内容生成、以及高昂的数据采集、训练和推理成本等问题。
* <strong>提出了未来研究方向。</strong> 论文为解决上述挑战提供了富有洞察力的未来研究方向，旨在推动视频生成模型在机器人领域的进一步发展。</p>
<p><strong>3. 主要结果及其意义：</strong></p>
<p>论文的主要“结果”体现在其对现有研究的<strong>全面梳理和分析</strong>上，其意义在于：
* <strong>为机器人研究者提供了一个关于视频生成模型在机器人领域应用的全面指南。</strong> 论文清晰地展示了视频模型如何成为强大的具身世界模型，能够捕捉精细的时空动力学，从而实现更通用的机器人策略学习、策略评估和视觉规划。
* <strong>揭示了视频生成模型在机器人领域应用的巨大潜力。</strong> 论文强调了视频模型在模拟复杂物理交互、生成逼真数据以及提供可扩展评估方面的优势，这些优势对于解决机器人领域的长期挑战至关重要。
* <strong>指出了当前技术存在的关键瓶颈。</strong> 通过详细列举挑战，论文为未来的研究提供了明确的焦点，有助于研究者集中精力解决实际问题。
* <strong>为机器人领域的研究和开发提供了重要的参考和启示。</strong> 论文的分析和建议将有助于推动该领域的研究进展，并最终促进视频生成模型在机器人领域的广泛应用，尤其是在安全关键型场景中。</p>
<p><strong>4. 论文中提到的局限性：</strong></p>
<p>论文详细讨论了视频生成模型在机器人应用中的多方面局限性：
* <strong>指令遵循能力差：</strong> 模型难以准确理解和执行用户指定的复杂指令，尤其是在长视频生成任务中。
* <strong>物理不一致的“幻觉”生成：</strong> 模型可能生成违反物理定律的视频，例如物体凭空出现/消失、变形不合理等。
* <strong>不安全内容生成：</strong> 模型可能生成包含犯罪、暴力、歧视性内容等不安全信息，限制了其在敏感应用中的使用。
* <strong>数据采集成本高昂：</strong> 训练高质量的视频生成模型需要海量且标注准确的数据，这在机器人领域尤为昂贵。
* <strong>训练和推理成本高：</strong> 视频生成模型通常拥有数十亿参数，训练和推理过程需要巨大的计算资源和时间。
* <strong>对数据质量和多样性要求高：</strong> 模型性能很大程度上依赖于训练数据的质量和覆盖范围，尤其对于文本或动作条件下的生成模型。
* <strong>长视频生成能力有限：</strong> 当前模型生成的视频时长通常较短，难以满足机器人任务的长期规划需求。
* <strong>对训练数据分布敏感：</strong> 模型在训练分布之外的场景下表现可能不佳，泛化能力有待提高。</p>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<p>论文提出了以下几个关键的未来研究方向：
* <strong>提升物理一致性：</strong>
    * <strong>集成物理先验和模拟器：</strong> 将物理定律编码到模型训练中，或利用物理模拟器进行约束。
    * <strong>探索新的模型架构和训练技术：</strong> 设计能够内在地理解物理规律的模型。
    * <strong>利用大型语言模型（LLMs）辅助理解物理属性和交互。</strong>
* <strong>提高不确定性量化能力：</strong>
    * <strong>开发更高效、可证明的 UQ 方法：</strong> 尤其是在分布外场景下。
    * <strong>使模型能够表达和量化其置信度。</strong>
* <strong>改进指令遵循能力：</strong>
    * <strong>多模态条件下的指令理解：</strong> 结合语言、图像、轨迹等多种模态信息。
    * <strong>探索“推理”机制：</strong> 使模型能够通过推理来更好地理解和执行指令。
    * <strong>利用偏好模型进行指令微调。</strong>
* <strong>开发统一的评估框架和机器人中心基准：</strong>
    * <strong>设计更全面的评估指标：</strong> 涵盖感知质量、物理一致性、语义对齐、任务性能等。
    * <strong>构建针对机器人操作任务的基准测试集。</strong>
* <strong>增强安全内容生成能力：</strong>
    * <strong>开发更通用、更灵活的安全防护机制。</strong>
    * <strong>建立更全面的安全基准测试。</strong>
* <strong>实现安全的机器人交互：</strong>
    * <strong>将视频模型应用于安全策略评估和预测。</strong>
    * <strong>探索在时空潜在空间中实现安全过滤。</strong>
* <strong>提升动作估计的准确性和效率：</strong>
    * <strong>探索更具可解释性的潜在动作模型。</strong>
    * <strong>开发更鲁棒的训练程序，利用半监督学习。</strong>
* <strong>实现高保真度长视频生成：</strong>
    * <strong>设计高效的视频一致性技术（如帧压缩、层次化框架）。</strong>
    * <strong>扩展模型的上下文窗口，降低计算成本。</strong>
* <strong>降低数据采集成本：</strong>
    * <strong>开发更智能的视频分割、过滤和标注技术。</strong>
    * <strong>探索利用 LLMs 减少幻觉并提高数据质量。</strong>
    * <strong>研究高效的少样本场景数据采集技术。</strong>
* <strong>降低训练和推理成本：</strong>
    * <strong>探索更高效的模型压缩和加速技术。</strong>
    * <strong>开发更快的推理方法，满足实时应用需求。</strong></p>
<p><strong>总结：</strong></p>
<p>这篇综述论文为理解视频生成模型在机器人领域的现状、挑战和未来机遇提供了一个全面的视角。论文强调了视频生成模型作为具身世界模型的巨大潜力，但同时也清晰地指出了当前技术在物理一致性、指令遵循、安全性和效率等方面存在的关键挑战。通过提出一系列富有前瞻性的研究方向，论文为该领域未来的发展奠定了坚实的基础，并有望推动视频生成模型在机器人领域的广泛应用，尤其是在对可靠性和安全性要求极高的场景中。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present potential future directions to address these open research challenges to motivate research and ultimately facilitate broader applications, especially in safety-critical settings.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.07823v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.07823v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.08832v1'></a></p>
<h2 id="raven-erasing-invisible-watermarks-via-novel-view-synthesis"><a href="https://arxiv.org/abs/2601.08832v1">RAVEN: Erasing Invisible Watermarks via Novel View Synthesis</a></h2>
<p><strong>Authors:</strong> Fahad Shamshad, Nils Lukas, Karthik Nandakumar</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Invisible watermarking has become a critical mechanism for authenticating AI-generated image content, with major platforms deploying watermarking schemes at scale. However, evaluating the vulnerability of these schemes against sophisticated removal attacks remains essential to assess their reliability and guide robust design. In this work, we expose a fundamental vulnerability in invisible watermarks by reformulating watermark removal as a view synthesis problem. Our key insight is that generating a perceptually consistent alternative view of the same semantic content, akin to re-observing a scene from a shifted perspective, naturally removes the embedded watermark while preserving visual fidelity. This reveals a critical gap: watermarks robust to pixel-space and frequency-domain attacks remain vulnerable to semantic-preserving viewpoint transformations. We introduce a zero-shot diffusion-based framework that applies controlled geometric transformations in latent space, augmented with view-guided correspondence attention to maintain structural consistency during reconstruction. Operating on frozen pre-trained models without detector access or watermark knowledge, our method achieves state-of-the-art watermark suppression across 15 watermarking methods--outperforming 14 baseline attacks while maintaining superior perceptual quality across multiple datasets.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“RAVEN: Erasing Invisible Watermarks via Novel View Synthesis”的全面中文摘要：</p>
<p><strong>论文摘要：RAVEN：通过新视角合成擦除不可见水印</strong></p>
<p><strong>1. 研究问题/核心问题</strong></p>
<p>随着人工智能生成内容（AIGC）的普及，不可见水印技术被广泛应用于验证AI生成图像的来源和防止滥用。然而，现有水印方案的鲁棒性评估仍是关键挑战，特别是针对复杂的移除攻击。本文旨在解决一个核心问题：<strong>现有的不可见水印方案，即使能抵抗像素空间和频域的攻击，是否仍然容易受到语义保持的视角变换攻击？</strong></p>
<p><strong>2. 主要创新点/方法论贡献</strong></p>
<p>该论文提出了一种名为 <strong>RAVEN</strong> 的新颖方法，将水印移除问题重新定义为<strong>新视角合成（Novel View Synthesis, NVS）</strong>问题。其核心洞察在于：通过生成同一语义内容的、感知上一致的替代“视角”，可以自然地移除嵌入的水印，同时保持图像的视觉保真度。</p>
<p>RAVEN 的关键技术贡献包括：</p>
<ul>
<li><strong>新颖的攻击向量（新视角合成）：</strong> 提出了一种全新的水印移除思路，即通过合成新的图像视角来打破水印的语义关联，而非直接对抗水印信号。</li>
<li><strong>潜在空间视角调制框架：</strong> 设计了一个零样本（zero-shot）的、基于扩散模型的框架。该框架在潜在空间中应用受控的几何变换，模拟视角变化，从而干扰水印的对齐，同时保持图像的语义内容。</li>
<li><strong>视角引导的对应注意力机制：</strong> 引入了一种特殊的注意力机制，用于在去噪过程中保持原始图像和合成视角之间的结构一致性。这取代了标准的自注意力机制，确保了即使在视角变换后，语义信息和细节也能得到保留。</li>
<li><strong>无需检测器或水印知识：</strong> RAVEN 作为一个黑盒攻击方法，不需要访问水印检测器、水印密钥，也不需要成对的干净-水印图像进行训练，使其在实际场景中具有很高的可用性。</li>
</ul>
<p><strong>3. 主要结果及其意义</strong></p>
<p>RAVEN 在实验中取得了显著的成果：</p>
<ul>
<li><strong>最先进的水印移除效果：</strong> 在 15 种不同的水印方法和 14 种基线攻击方法上，RAVEN 实现了最先进的水印抑制效果，平均 TPR@1%FPR 显著低于最接近的基线（在 MS-COCO 数据集上平均 TPR 为 0.026，比最接近的基线提高了 60% 以上）。</li>
<li><strong>卓越的感知质量：</strong> 在水印移除的同时，RAVEN 能够保持极高的视觉保真度和语义一致性。与许多其他方法（如 VAE-C、Regen、Rinse、UnMarker、CtrlGen+）相比，RAVEN 产生的图像在细节、纹理和自然度方面表现更优。</li>
<li><strong>模型无关的泛化能力：</strong> RAVEN 在不同的 Stable Diffusion 模型（v1.5, v2.0, v2.1）上都表现出一致的强大性能，无需针对特定模型进行微调，证明了其方法的通用性。</li>
<li><strong>计算效率高：</strong> RAVEN 可以在大约 6 秒内处理一张图像（在 A100 GPU 上），远快于一些需要数分钟甚至多 GPU 训练的基线方法。</li>
</ul>
<p>这些结果表明，RAVEN 成功地揭示了当前水印技术的一个关键漏洞，即对语义保持的视角变换的脆弱性。这对于评估现有水印方案的可靠性以及指导未来更鲁棒的水印设计具有重要意义。</p>
<p><strong>4. 提及的局限性</strong></p>
<p>论文中提及的局限性主要体现在：</p>
<ul>
<li><strong>参数敏感性：</strong> 论文中提到了“强度参数 s”的敏感性，它在水印移除和视觉质量之间存在权衡。过高的 s 值会引入伪影并降低 FID，而过低的值可能无法完全移除水印。</li>
<li><strong>对特定视角变换的依赖：</strong> 虽然论文强调了其方法对“语义保持的视角变换”的有效性，但其具体实现依赖于对潜在空间进行“小幅、一致的视角偏移”，这可能不是所有水印方案都能被此种方式有效攻击。</li>
<li><strong>对“未知距离度量”的假设：</strong> 论文在问题定义中假设攻击者不知道检测器确切的距离度量 <script type="math/tex">\phi</script>，这是一种现实的假设，但如果攻击者能获得关于 <script type="math/tex">\phi</script> 的信息，可能会影响攻击策略。</li>
</ul>
<p><strong>5. 潜在的未来研究方向</strong></p>
<p>基于本文的研究，可以推导出以下潜在的未来研究方向：</p>
<ul>
<li><strong>更广泛的视角变换攻击：</strong> 探索更复杂、更具挑战性的视角变换（例如，非线性变换、更大幅度的视角变化）对水印移除的影响。</li>
<li><strong>对抗性视角合成：</strong> 结合对抗性学习，设计更具针对性的视角变换，以最大化水印的移除效果并最小化感知损失。</li>
<li><strong>水印鲁棒性设计：</strong> 基于 RAVEN 揭示的漏洞，研究如何设计对视角变换更鲁棒的水印方案，例如，通过将水印与更深层次的语义结构绑定，或采用多视角一致性的水印嵌入策略。</li>
<li><strong>实时水印移除：</strong> 进一步优化 RAVEN 的计算效率，使其能够满足更严格的实时水印移除需求。</li>
<li><strong>多模态水印：</strong> 探索将此视角合成的攻击思路应用于其他模态（如视频、3D 模型）的水印移除。</li>
</ul>
<p>总而言之，RAVEN 论文通过将水印移除视为一个新视角合成问题，提出了一种高效且有效的黑盒攻击方法，显著提升了水印移除的性能，并对当前水印技术的鲁棒性提出了新的挑战，为未来的研究提供了重要的方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce a zero-shot diffusion-based framework that applies controlled geometric transformations in latent space, augmented with view-guided correspondence attention to maintain structural consistency during reconstruction.</li>
<li>Operating on frozen pre-trained models without detector access or watermark knowledge, our method achieves state-of-the-art watermark suppression across 15 watermarking methods--outperforming 14 baseline attacks while maintaining superior perceptual quality across multiple datasets.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.08832v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.08832v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.08831v1'></a></p>
<h2 id="3am-segment-anything-with-geometric-consistency-in-videos"><a href="https://arxiv.org/abs/2601.08831v1">3AM: Segment Anything with Geometric Consistency in Videos</a></h2>
<p><strong>Authors:</strong> Yang-Che Sun, Cheng Sun, Chin-Yang Lin, Fu-En Yang, Min-Hung Chen, Yen-Yu Lin, Yu-Lun Liu</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Video object segmentation methods like SAM2 achieve strong performance through memory-based architectures but struggle under large viewpoint changes due to reliance on appearance features. Traditional 3D instance segmentation methods address viewpoint consistency but require camera poses, depth maps, and expensive preprocessing. We introduce 3AM, a training-time enhancement that integrates 3D-aware features from MUSt3R into SAM2. Our lightweight Feature Merger fuses multi-level MUSt3R features that encode implicit geometric correspondence. Combined with SAM2's appearance features, the model achieves geometry-consistent recognition grounded in both spatial position and visual similarity. We propose a field-of-view aware sampling strategy ensuring frames observe spatially consistent object regions for reliable 3D correspondence learning. Critically, our method requires only RGB input at inference, with no camera poses or preprocessing. On challenging datasets with wide-baseline motion (ScanNet++, Replica), 3AM substantially outperforms SAM2 and extensions, achieving 90.6% IoU and 71.7% Positive IoU on ScanNet++'s Selected Subset, improving over state-of-the-art VOS methods by +15.9 and +30.4 points. Project page: https://jayisaking.github.io/3AM-Page/</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于“3AM: Segment Anything with Geometric Consistency in Videos”论文的全面中文摘要，重点突出了其研究问题、创新点、结果、局限性和未来方向：</p>
<p><strong>论文题目：</strong> 3AM: Segment Anything with Geometric Consistency in Videos</p>
<p><strong>作者：</strong> Yang-Che Sun, Cheng Sun, Chin-Yang Lin, Fu-En Yang, Min-Hung Chen, Yen-Yu Lin, Yu-Lun Liu</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong></p>
<p>该论文主要解决了视频目标分割（Video Object Segmentation, VOS）领域的一个关键挑战：<strong>如何在视频中实现目标在大幅度视角变化下的鲁棒性分割和跟踪</strong>。现有的基于内存的2D VOS方法（如SAM2）虽然在外观特征上表现出色，但在视角变化剧烈时容易丢失目标。而传统的3D实例分割方法虽然能处理视角一致性，但通常需要相机位姿、深度图等昂贵的先验信息和复杂的预处理，限制了其实用性。因此，研究的核心问题是如何在<strong>不依赖额外3D信息（如相机位姿、深度图）</strong>的情况下，实现<strong>几何一致性强、视角鲁棒性高</strong>的视频目标分割。</p>
<p><strong>2. 关键创新点/方法贡献：</strong></p>
<p>3AM方法的核心创新在于其<strong>训练时增强（training-time enhancement）</strong>策略，将3D感知能力融入现有的2D VOS框架（SAM2）中。具体创新点包括：</p>
<ul>
<li><strong>3D感知特征融合：</strong> 论文引入了来自MUSt3R（一个多视图一致性学习模型）的3D感知特征，并通过一个轻量级的<strong>特征融合器（Feature Merger）</strong>将其与SAM2的外观特征相结合。MUSt3R的特征能够编码隐式的几何对应关系，而特征融合器则通过跨注意力（cross-attention）和卷积细化，将多层次的MUSt3R特征与SAM2的2D特征有效融合，生成<strong>几何感知且高分辨率的融合特征（Fmerged）</strong>。</li>
<li><strong>几何一致性识别：</strong> 融合后的特征使得模型能够同时基于空间位置和视觉相似性来识别目标，从而实现几何一致性的识别，即使在视角大幅变化、场景混乱或目标暂时消失后重新出现的情况下也能保持目标身份。</li>
<li><strong>视场角感知采样策略（Field-of-View Aware Sampling Strategy）：</strong> 为了在训练时更有效地学习3D对应关系，论文提出了一种新的采样策略。该策略确保训练帧中的目标区域在3D空间中具有足够的重叠，从而避免了因视角差异过大而导致的几何学习模糊。</li>
<li><strong>无需额外3D信息：</strong> 关键在于，3AM在<strong>推理时仅需RGB输入</strong>，无需相机位姿、深度图或任何预处理，这大大提高了其通用性和实用性。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong></p>
<ul>
<li><strong>显著的性能提升：</strong> 在具有挑战性的宽基线运动数据集（如ScanNet++和Replica）上，3AM取得了显著的性能提升。在ScanNet++的Selected Subset上，3AM实现了90.6%的IoU和71.7%的Positive IoU，<strong>大幅超越了SAM2及其扩展方法（如SAM2Long）</strong>，分别提高了+15.9和+30.4个百分点。</li>
<li><strong>鲁棒性验证：</strong> 实验结果表明，3AM在处理目标重新出现和大幅视角变化等困难场景时表现出色，证明了其几何感知能力带来的鲁棒性。</li>
<li><strong>3D实例分割的潜力：</strong> 论文还展示了3AM在3D实例分割任务上的潜力，证明了通过几何一致性的2D跟踪，可以无需重度的3D监督就能获得可靠的3D实例分割结果。</li>
<li><strong>意义：</strong> 该研究为视频目标分割领域提供了一种新颖且实用的解决方案，克服了传统2D方法在视角变化下的局限性，同时避免了3D方法对额外先验信息的依赖，为需要鲁棒目标跟踪的下游应用（如自动驾驶、机器人、AR/VR）开辟了新的可能性。</li>
</ul>
<p><strong>4. 提及的局限性：</strong></p>
<ul>
<li><strong>训练时的3D信息依赖：</strong> 虽然推理时不需要3D信息，但<strong>训练时仍然需要利用包含相机位姿和深度信息的3D数据集</strong>（如ScanNet++, ASE）来学习MUSt3R的3D感知特征。</li>
<li><strong>内存槽数量限制：</strong> 论文提到SAM2的内存模块最多支持8个内存槽，这可能限制了模型在处理极长视频序列时的长期记忆能力。</li>
<li><strong>FOV采样策略的权衡：</strong> 论文提到完全依赖FOV采样可能会导致模型丢失部分原始的特征匹配能力，因此采用了混合采样策略。</li>
</ul>
<p><strong>5. 未来研究方向：</strong></p>
<ul>
<li><strong>内存选择机制的优化：</strong> 论文在消融实验中提到，虽然3AM本身表现优异，但结合DAM4SAM或SAM2Long的内存选择机制可以带来微小的性能提升。未来可以研究<strong>专门为3AM设计的内存选择机制</strong>，以进一步提升性能。</li>
<li><strong>更广泛的3D基础模型集成：</strong> 论文在消融实验中对比了不同的3D基础模型（如CUT3R和MUSt3R），并选择了MUSt3R。未来可以探索集成其他更先进或更适合特定场景的3D基础模型。</li>
<li><strong>在线3D实例分割的进一步探索：</strong> 论文展示了3AM在3D实例分割上的潜力，未来可以进一步研究如何更高效地利用其几何感知能力进行<strong>端到端的在线3D实例分割</strong>。</li>
<li><strong>更具挑战性的场景：</strong> 虽然在宽基线数据集上表现优异，但未来可以探索在更复杂、动态变化更剧烈或目标遮挡更严重的场景下进一步验证和提升3AM的性能。</li>
</ul>
<p>总而言之，3AM通过巧妙地融合3D感知特征和2D外观特征，并辅以创新的训练策略，成功地解决了视频目标分割在视角变化下的鲁棒性问题，同时保持了推理的简洁性，是该领域的一项重要进展。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce 3AM, a training-time enhancement that integrates 3D-aware features from MUSt3R into SAM2.</li>
<li>We propose a field-of-view aware sampling strategy ensuring frames observe spatially consistent object regions for reliable 3D correspondence learning.</li>
<li>Critically, our method requires only RGB input at inference, with no camera poses or preprocessing.</li>
<li>On challenging datasets with wide-baseline motion (ScanNet++, Replica), 3AM substantially outperforms SAM2 and extensions, achieving 90.6% IoU and 71.7% Positive IoU on ScanNet++'s Selected Subset, improving over state-of-the-art VOS methods by +15.9 and +30.4 points.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.08831v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.08831v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.08828v1'></a></p>
<h2 id="motion-attribution-for-video-generation"><a href="https://arxiv.org/abs/2601.08828v1">Motion Attribution for Video Generation</a></h2>
<p><strong>Authors:</strong> Xindi Wu, Despoina Paschalidou, Jun Gao, Antonio Torralba, Laura Leal-Taixé, Olga Russakovsky, Sanja Fidler, Jonathan Lorraine</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.LG, cs.MM, cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>Despite the rapid progress of video generation models, the role of data in influencing motion is poorly understood. We present Motive (MOTIon attribution for Video gEneration), a motion-centric, gradient-based data attribution framework that scales to modern, large, high-quality video datasets and models. We use this to study which fine-tuning clips improve or degrade temporal dynamics. Motive isolates temporal dynamics from static appearance via motion-weighted loss masks, yielding efficient and scalable motion-specific influence computation. On text-to-video models, Motive identifies clips that strongly affect motion and guides data curation that improves temporal consistency and physical plausibility. With Motive-selected high-influence data, our method improves both motion smoothness and dynamic degree on VBench, achieving a 74.1% human preference win rate compared with the pretrained base model. To our knowledge, this is the first framework to attribute motion rather than visual appearance in video generative models and to use it to curate fine-tuning data.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Motion Attribution for Video Generation”的全面中文摘要：</p>
<p><strong>论文题目：</strong> Motion Attribution for Video Generation (视频生成中的运动归因)</p>
<p><strong>作者：</strong> Xindi Wu, Despoina Paschalidou, Jun Gao, Antonio Torralba, Laura Leal-Taixé, Olga Russakovsky, Sanja Fidler, Jonathan Lorraine</p>
<p><strong>摘要</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
尽管视频生成模型取得了显著进展，但训练数据如何影响生成视频中的运动（即时间动态）仍然是一个未被充分理解的问题。现有的数据归因方法主要关注图像，将视频中的运动简单地视为额外的空间维度，这导致它们无法有效地区分运动和静态外观，并且在处理现代大规模视频数据集时计算成本高昂。因此，迫切需要一种能够专门针对视频中的运动进行归因，并能有效指导数据选择以提升生成视频运动质量的方法。</p>
<p><strong>2. 主要创新点/方法贡献：</strong>
本文提出了 <strong>Motive (MOTIon attribution for Video gEneration)</strong>，一个<strong>以运动为中心、基于梯度的数据归因框架</strong>，专门用于视频生成模型。其核心创新包括：</p>
<ul>
<li><strong>运动感知损失掩码 (Motion-weighted Loss Masks)：</strong> 通过利用运动幅度信息，Motive 能够将归因信号聚焦于视频中的动态区域，有效分离运动与静态外观的影响。</li>
<li><strong>可扩展的梯度计算：</strong> 采用逆 Hessian 近似、低方差梯度相似性估计器、低成本单样本估计器以及 Fastfood 投影等技术，使得 Motive 能够高效地处理现代大规模视频数据集和模型。</li>
<li><strong>帧长偏差修正：</strong> 提出了一种针对视频帧数差异的偏差修正方法，确保了不同时长视频的归因公平性。</li>
<li><strong>运动归因而非外观归因：</strong> Motive 是第一个专门针对视频生成模型中的运动进行归因的框架，而非仅仅关注视觉外观。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong>
Motive 在多个方面取得了显著成果：</p>
<ul>
<li><strong>识别关键训练片段：</strong> Motive 能够识别出对生成视频的运动动态有显著影响的训练片段，从而指导数据筛选。</li>
<li><strong>提升运动质量：</strong> 使用 Motive 选择的高影响力数据进行微调，显著提高了生成视频的运动平滑度和动态程度。在 VBench 评估中，使用 Motive 选择的 10% 数据进行微调，其性能超越了使用全部数据进行微调的模型，并在人类评估中获得了 74.1% 的偏好率。</li>
<li><strong>效率与可扩展性：</strong> 该方法在计算效率和可扩展性方面表现出色，能够处理大规模数据集和模型，并且计算成本可控。</li>
<li><strong>跨模型泛化性：</strong> Motive 在不同模型架构（如 Wan2.1-T2V-1.3B 和 Wan2.2-TI2V-5B）上均表现出有效性，证明了其通用性。</li>
</ul>
<p><strong>意义：</strong> 该研究为理解视频生成模型中的数据如何影响运动提供了新的视角，并提供了一个实用的工具来指导数据 curation，从而生成更具物理合理性和时间一致性的视频。这对于构建更可控、更可解释的视频生成模型至关重要。</p>
<p><strong>4. 论文中提到的局限性：</strong>
*   <strong>运动显著性依赖于追踪器：</strong> 运动掩码的质量依赖于所选的运动追踪器，严重的遮挡或透明度可能影响掩码效果。
*   <strong>相机运动与物体运动的解耦挑战：</strong> 仅通过运动掩码可能难以完全区分相机自身的运动和物体本身的运动，尤其是在相机运动占主导的情况下。
*   <strong>忽略分类器无关引导 (CFG)：</strong> 该框架未显式考虑 CFG，而 CFG 在视频生成中常用于引导运动，这可能导致训练时归因与推理时动态之间的差异。
*   <strong>可能引入权衡：</strong> 针对特定运动的微调可能会影响模型的整体生成能力。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>追踪器鲁棒性：</strong> 探索更鲁棒的运动估计器，并利用其置信度/可见性通道来加权掩码。
*   <strong>闭环数据策选：</strong> 从一次性排序转向主动选择，即迭代地进行归因、微调和再归因。
*   <strong>安全与治理：</strong> 利用负面影响过滤来抑制不期望的动态，并记录和审计模型暴露的运动行为。
*   <strong>更复杂的微调：</strong> 探索更高级的微调策略，如多学生蒸馏。
*   <strong>多模态融合：</strong> 将方法扩展到其他模态，如视频+音频。
*   <strong>自生成视频查询：</strong> 使用模型生成的视频作为查询，追溯不真实的物理现象等问题，以进行迭代诊断和改进。
*   <strong>细粒度运动归因：</strong> 探索在运动片段或事件级别进行归因，以获得更精细的见解。</p>
<p>总而言之，这篇论文提出了一个创新的、以运动为中心的视频生成数据归因框架 Motive，解决了现有方法的局限性，并在提升视频生成运动质量方面取得了显著成果，为理解和控制视频生成模型开辟了新的途径。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present Motive (MOTIon attribution for Video gEneration), a motion-centric, gradient-based data attribution framework that scales to modern, large, high-quality video datasets and models.</li>
<li>With Motive-selected high-influence data, our method improves both motion smoothness and dynamic degree on VBench, achieving a 74.1% human preference win rate compared with the pretrained base model.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.08828v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.08828v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.08811v1'></a></p>
<h2 id="reasoning-matters-for-3d-visual-grounding"><a href="https://arxiv.org/abs/2601.08811v1">Reasoning Matters for 3D Visual Grounding</a></h2>
<p><strong>Authors:</strong> Hsiang-Wei Huang, Kuang-Ming Chen, Wenhao Chai, Cheng-Yen Yang, Jen-Hao Cheng, Jenq-Neng Hwang</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>The recent development of Large Language Models (LLMs) with strong reasoning ability has driven research in various domains such as mathematics, coding, and scientific discovery. Meanwhile, 3D visual grounding, as a fundamental task in 3D understanding, still remains challenging due to the limited reasoning ability of recent 3D visual grounding models. Most of the current methods incorporate a text encoder and visual feature encoder to generate cross-modal fuse features and predict the referring object. These models often require supervised training on extensive 3D annotation data. On the other hand, recent research also focus on scaling synthetic data to train stronger 3D visual grounding LLM, however, the performance gain remains limited and non-proportional to the data collection cost. In this work, we propose a 3D visual grounding data pipeline, which is capable of automatically synthesizing 3D visual grounding data along with corresponding reasoning process. Additionally, we leverage the generated data for LLM fine-tuning and introduce Reason3DVG-8B, a strong 3D visual grounding LLM that outperforms previous LLM-based method 3D-GRAND using only 1.6% of their training data, demonstrating the effectiveness of our data and the importance of reasoning in 3D visual grounding.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Reasoning Matters for 3D Visual Grounding”的全面中文摘要：</p>
<p><strong>论文题目：</strong> Reasoning Matters for 3D Visual Grounding (推理至关重要：3D视觉定位)</p>
<p><strong>作者：</strong> Hsiang-Wei Huang, Kuang-Ming Chen, Wenhao Chai, Cheng-Yen Yang, Jen-Hao Cheng, Jenq-Neng Hwang</p>
<p><strong>1. 研究问题/核心挑战：</strong></p>
<p>该论文主要关注3D视觉定位（3D Visual Grounding）任务，该任务旨在根据文本描述在3D场景中准确地识别出目标物体。当前3D视觉定位模型面临的主要挑战在于其有限的推理能力，这使得它们难以理解复杂的文本查询和场景关系。现有方法通常需要大量的3D标注数据进行监督训练，而通过生成合成数据来训练模型的方法，其性能提升与数据收集成本不成比例，且提升有限。此外，许多先进方法依赖于专有的LLM，导致推理成本高昂。</p>
<p><strong>2. 关键创新/方法贡献：</strong></p>
<p>该研究的核心贡献在于提出了一个<strong>全自动化的3D视觉定位数据生成流水线</strong>，该流水线能够自动合成包含<strong>详细推理过程</strong>的3D视觉定位数据。其主要创新点包括：</p>
<ul>
<li><strong>全自动化数据生成流水线：</strong> 该流水线无需人工标注，能够高效地生成包含3D场景、文本查询以及详细推理步骤的数据。这极大地降低了数据收集成本。</li>
<li><strong>结构化推理监督：</strong> 生成的数据不仅包含最终的定位结果，还提供了详细的、分阶段的（选择相关对象、情况估计、推理、结论）推理过程。这种结构化的推理监督对于LLM的精调至关重要。</li>
<li><strong>Reason3DVG-8B模型：</strong> 基于生成的数据，作者对开源LLM Llama-3.1-8B进行了精调，提出了Reason3DVG-8B模型。该模型在3D视觉定位任务上展现出强大的推理和定位能力。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong></p>
<ul>
<li><strong>性能超越：</strong> Reason3DVG-8B在ScanRefer和NR3D等多个3D视觉定位基准上，超越了现有的LLM方法（如3D-GRAND），并且在仅使用3D-GRAND训练数据量的1.6%的情况下，取得了25%的性能提升。这有力地证明了其数据生成流水线和推理监督的有效性。</li>
<li><strong>成本效益：</strong> 该方法显著降低了数据收集成本，同时提高了模型的性能，为大规模3D视觉定位数据集的构建提供了一种更具成本效益的解决方案。</li>
<li><strong>推理的重要性：</strong> 研究结果强调了推理能力在3D视觉定位任务中的关键作用，表明仅仅增加数据规模不足以解决问题，而结构化的推理监督是提升模型性能的关键。</li>
<li><strong>泛化能力：</strong> 即使在仅使用简单空间关系的数据进行训练后，Reason3DVG模型也能很好地泛化到复杂、未见过的真实世界查询和空间关系上，展现出良好的泛化能力。</li>
</ul>
<p><strong>4. 论文提及的局限性：</strong></p>
<ul>
<li><strong>检测器依赖：</strong> 作者指出，模型的准确性在一定程度上受限于3D物体检测器的质量。</li>
<li><strong>数据复杂度：</strong> 生成的数据主要包含常见的空间关系，并未模拟真实世界中极其复杂的查询和空间关系。</li>
</ul>
<p><strong>5. 未来研究方向：</strong></p>
<ul>
<li><strong>集成更强的检测器：</strong> 结合更先进的物体检测器和更丰富的语义信息（如更精细的物体描述），有望进一步提升模型的性能。</li>
<li><strong>更复杂的场景和查询：</strong> 未来可以探索生成更复杂、更贴近真实世界场景和查询的数据，以应对更具挑战性的3D视觉定位任务。</li>
<li><strong>通用3D理解LLM：</strong> 该研究为开发更强大的、具备推理能力的通用3D理解LLM奠定了基础。</li>
</ul>
<p><strong>总结：</strong></p>
<p>这篇论文通过提出一个创新的全自动化数据生成流水线，并强调了结构化推理监督的重要性，成功地训练了一个强大的3D视觉定位LLM——Reason3DVG-8B。该模型在性能、成本效益和泛化能力方面均取得了显著进展，为3D视觉理解领域的研究开辟了新的方向，并证明了“推理”在3D视觉定位任务中的核心价值。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this work, we propose a 3D visual grounding data pipeline, which is capable of automatically synthesizing 3D visual grounding data along with corresponding reasoning process.</li>
<li>Additionally, we leverage the generated data for LLM fine-tuning and introduce Reason3DVG-8B, a strong 3D visual grounding LLM that outperforms previous LLM-based method 3D-GRAND using only 1.6% of their training data, demonstrating the effectiveness of our data and the importance of reasoning in 3D visual grounding.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.08811v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.08811v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.08798v1'></a></p>
<h2 id="near-perfect-photo-id-of-the-hula-painted-frog-with-zero-shot-deep-local-feature-matching"><a href="https://arxiv.org/abs/2601.08798v1">Near-perfect photo-ID of the Hula painted frog with zero-shot deep local-feature matching</a></h2>
<p><strong>Authors:</strong> Maayan Yesharim, R. G. Bina Perl, Uri Roll, Sarig Gafny, Eli Geffen, Yoav Ram</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Categories:</strong> cs.CV, q-bio.QM</p>
<p><strong>Abstract:</strong></p>
<p>Accurate individual identification is essential for monitoring rare amphibians, yet invasive marking is often unsuitable for critically endangered species. We evaluate state-of-the-art computer-vision methods for photographic re-identification of the Hula painted frog (Latonia nigriventer) using 1,233 ventral images from 191 individuals collected during 2013-2020 capture-recapture surveys. We compare deep local-feature matching in a zero-shot setting with deep global-feature embedding models. The local-feature pipeline achieves 98% top-1 closed-set identification accuracy, outperforming all global-feature models; fine-tuning improves the best global-feature model to 60% top-1 (91% top-10) but remains below local matching. To combine scalability with accuracy, we implement a two-stage workflow in which a fine-tuned global-feature model retrieves a short candidate list that is re-ranked by local-feature matching, reducing end-to-end runtime from 6.5-7.8 hours to ~38 minutes while maintaining ~96% top-1 closed-set accuracy on the labeled dataset. Separation of match scores between same- and different-individual pairs supports thresholding for open-set identification, enabling practical handling of novel individuals. We deploy this pipeline as a web application for routine field use, providing rapid, standardized, non-invasive identification to support conservation monitoring and capture-recapture analyses. Overall, in this species, zero-shot deep local-feature matching outperformed global-feature embedding and provides a strong default for photo-identification.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Near-perfect photo-ID of the Hula painted frog with zero-shot deep local-feature matching”的全面中文摘要：</p>
<p><strong>论文题目：</strong> Near-perfect photo-ID of the Hula painted frog with zero-shot deep local-feature matching (利用零样本深度局部特征匹配实现近乎完美的 Hula 蛙照片身份识别)</p>
<p><strong>作者：</strong> Maayan Yesharim, R. G. Bina Perl, Uri Roll, Sarig Gafny, Eli Geffen, Yoav Ram</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/主要挑战：</strong>
该研究旨在解决对濒危物种 Hula 蛙（<em>Latonia nigriventer</em>）进行准确的个体身份识别问题。传统的标记方法（如剪趾或植入芯片）对这种濒危物种而言不适用，因为它们具有侵入性且可能引起福利和检测问题。因此，需要一种非侵入性的、高精度的照片身份识别方法来支持其长期监测和种群数量估计。手动匹配照片耗时且容易出错，尤其是在图像数据库不断增长的情况下。</p>
<p><strong>2. 关键创新/方法论贡献：</strong>
该研究评估了最先进的计算机视觉方法，特别是深度局部特征匹配和深度全局特征嵌入模型，用于 Hula 蛙的摄影身份识别。主要贡献包括：
*   <strong>零样本深度局部特征匹配的有效性：</strong> 研究表明，在零样本设置下（即模型未在 Hula 蛙数据上进行训练），基于 ALIKED 和 LightGlue 的深度局部特征匹配管道取得了近乎完美的 97.8% 的 top-1 闭集识别准确率，显著优于所有全局特征模型。
*   <strong>两阶段工作流程的提出：</strong> 为了平衡准确性和计算效率，研究者提出了一种两阶段工作流程。第一阶段使用微调后的全局特征模型（MiewID-FT）快速检索候选列表，第二阶段使用计算成本较高的局部特征匹配（ALIKED+LightGlue）对候选列表进行重新排序，从而大幅缩短了运行时间（从 6.5-7.8 小时缩短至约 38 分钟），同时保持了约 96% 的 top-1 准确率。
*   <strong>开放集识别的支持：</strong> 通过分析匹配分数，研究证明了区分同一和不同个体对的有效性，这为开放集识别（即识别未知个体）提供了阈值设置的基础，从而能够实用化地处理新个体。
*   <strong>实际部署的 Web 应用：</strong> 该研究将提出的两阶段管道集成到一个 Web 应用程序中，用于常规的野外使用，为研究人员提供快速、标准化、非侵入性的身份识别工具。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>局部特征匹配的卓越性能：</strong> 在零样本设置下，ALIKED+LightGlue 管道实现了 97.8% 的 top-1 闭集识别准确率，远超全局特征模型。即使经过微调，全局特征模型（MiewID-FT）的最高准确率也仅达到 60.2% 的 top-1，仍低于局部特征方法。这表明对于具有高度区分性腹部斑纹且图像质量不一的物种，深度局部特征匹配是更优的选择。
*   <strong>两阶段方法的效率提升：</strong> 两阶段方法在保持高准确率（96% top-1）的同时，将运行时间缩短了约 20 倍，使其在实际应用中更具可扩展性，尤其是在处理大型图像库时。
*   <strong>开放集识别的可行性：</strong> 匹配分数的分离性表明，可以通过设置阈值来有效区分同一和不同个体，这对于监测种群中新出现的个体至关重要。
*   <strong>实际应用价值：</strong> 该研究成功地将先进的计算机视觉技术转化为一个实用的 Web 工具，为 Hula 蛙的保护监测和科学研究提供了强大的支持。</p>
<p><strong>4. 论文中提到的局限性：</strong>
*   <strong>研究范围的限制：</strong> 研究仅限于单一物种（Hula 蛙）、单一地点和一种成像协议（高分辨率腹部照片），这可能限制了其在低分辨率、运动模糊或遮挡严重的相机陷阱图像等不同场景下的泛化能力。
*   <strong>数据集规模：</strong> 标记的数据集包含 191 个个体，且多年来被重复捕获的个体相对较少，这限制了对更复杂的长期时间效应（如年龄或疾病导致的花纹变化）的探索。
*   <strong>地面真实性噪声：</strong> 使用半自动化的 Wild-ID 管道结合人工确认来建立地面真实性身份，可能存在少量残留的标签噪声。
*   <strong>计算成本：</strong> 尽管两阶段方法已显著优化，但其计算成本仍会随着数据集规模的增长而增加，对于大规模监测可能需要进一步的工程优化。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>跨物种和跨场景的泛化性测试：</strong> 未来研究应测试深度局部几何匹配在其他变形性强的类群（如蝾螈、小型鱼类）以及不同解剖视图、成像设备和环境条件下的优势是否能得以延续。
*   <strong>改进全局特征模型：</strong> 开发能够保留更高空间分辨率或显式编码变形的全局特征模型，以及在表示层面融合全局和局部线索的混合模型，可能有助于缩小全局和局部特征模型之间的性能差距。
*   <strong>处理更复杂的图像条件：</strong> 探索在低分辨率、运动模糊或遮挡严重的相机陷阱图像等更具挑战性的场景下，局部特征模型与全局特征模型各自的优势和劣势。
*   <strong>大规模数据库的优化：</strong> 对于非常大的数据库，需要进一步的工程优化来降低计算成本，以实现高通量的监测。</p>
<p>总而言之，该研究通过引入和评估一种基于零样本深度局部特征匹配的先进方法，显著提升了 Hula 蛙个体身份识别的准确性和效率，并成功将其转化为一个实用的监测工具，为濒危物种的保护和研究开辟了新的可能性。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We evaluate state-of-the-art computer-vision methods for photographic re-identification of the Hula painted frog (Latonia nigriventer) using 1,233 ventral images from 191 individuals collected during 2013-2020 capture-recapture surveys.</li>
<li>Separation of match scores between same- and different-individual pairs supports thresholding for open-set identification, enabling practical handling of novel individuals.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.08798v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.08798v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.08790v1'></a></p>
<h2 id="aggregating-diverse-cue-experts-for-ai-generated-image-detection"><a href="https://arxiv.org/abs/2601.08790v1">Aggregating Diverse Cue Experts for AI-Generated Image Detection</a></h2>
<p><strong>Authors:</strong> Lei Tan, Shuwei Li, Mohan Kankanhalli, Robby T. Tan</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>The rapid emergence of image synthesis models poses challenges to the generalization of AI-generated image detectors. However, existing methods often rely on model-specific features, leading to overfitting and poor generalization. In this paper, we introduce the Multi-Cue Aggregation Network (MCAN), a novel framework that integrates different yet complementary cues in a unified network. MCAN employs a mixture-of-encoders adapter to dynamically process these cues, enabling more adaptive and robust feature representation. Our cues include the input image itself, which represents the overall content, and high-frequency components that emphasize edge details. Additionally, we introduce a Chromatic Inconsistency (CI) cue, which normalizes intensity values and captures noise information introduced during the image acquisition process in real images, making these noise patterns more distinguishable from those in AI-generated content. Unlike prior methods, MCAN's novelty lies in its unified multi-cue aggregation framework, which integrates spatial, frequency-domain, and chromaticity-based information for enhanced representation learning. These cues are intrinsically more indicative of real images, enhancing cross-model generalization. Extensive experiments on the GenImage, Chameleon, and UniversalFakeDetect benchmark validate the state-of-the-art performance of MCAN. In the GenImage dataset, MCAN outperforms the best state-of-the-art method by up to 7.4% in average ACC across eight different image generators.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Aggregating Diverse Cue Experts for AI-Generated Image Detection”的全面中文摘要，其中包含了您要求的各个部分：</p>
<p><strong>论文题目：</strong> Aggregating Diverse Cue Experts for AI-Generated Image Detection (聚合多样化线索专家用于AI生成图像检测)</p>
<p><strong>作者：</strong> Lei Tan, Shuwei Li, Mohan Kankanhalli, Robby T. Tan</p>
<hr />
<p><strong>摘要</strong></p>
<p><strong>1. 主要问题/研究问题：</strong></p>
<p>随着AI图像生成技术的飞速发展，AI生成图像检测器在面对不同生成模型时，其泛化能力面临严峻挑战。现有方法往往依赖于模型特定的特征，容易导致过拟合和泛化能力差。因此，研究如何构建一个能够有效区分真实图像和AI生成图像，并且具有良好跨模型泛化能力的检测器是本文的核心研究问题。</p>
<p><strong>2. 关键创新/方法论贡献：</strong></p>
<p>本文提出了一种新颖的<strong>多线索聚合网络 (Multi-Cue Aggregation Network, MCAN)</strong>。其核心贡献在于：</p>
<ul>
<li><strong>多线索融合框架：</strong> MCAN整合了三种互补的视觉线索：<ul>
<li><strong>原始图像内容 (Image Content)：</strong> 代表图像的整体语义信息。</li>
<li><strong>高频信息 (High-Frequency Information)：</strong> 强调图像的边缘细节，对检测生成伪影有帮助。</li>
<li><strong>色度不一致性 (Chromatic Inconsistency, CI)：</strong> 这是本文的一项重要创新。CI通过对图像进行色度变换，旨在消除光照强度变化的影响，从而更清晰地捕捉真实图像在成像过程中引入的噪声差异，而AI生成图像通常更平滑，缺乏这种噪声。</li>
</ul>
</li>
<li><strong>混合编码器适配器 (Mixture-of-Encoder Adapter, MoEA)：</strong> 为了动态地处理和整合这些不同的线索，MCAN引入了MoEA。MoEA借鉴了“混合专家”的思想，能够根据不同线索的特性动态地调整特征提取和表示，实现更具适应性和鲁棒性的特征学习。</li>
<li><strong>统一的聚合策略：</strong> MCAN将空间、频域和色度信息整合到一个统一的网络框架中，通过MoEA实现对这些线索的有效聚合，从而提升检测性能。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong></p>
<ul>
<li><strong>卓越的性能：</strong> 在GenImage、Chameleon和UniversalFakeDetect三个具有挑战性的基准数据集上，MCAN均取得了最先进的性能。</li>
<li><strong>显著的泛化能力：</strong> 在GenImage数据集上，MCAN在八种不同图像生成器上的平均准确率（ACC）比最佳的现有方法高出7.4%。在Chameleon数据集上，MCAN在ProGAN和SDV1.4训练协议下分别超越AIDE 2.44%和7.01%。在UniversalFakeDetect数据集上，MCAN的平均准确率（mAcc）也显著优于其他方法，并且比UniFD高出11.9%。</li>
<li><strong>CI线索的重要性：</strong> 消融实验表明，CI线索对于提升检测性能至关重要，与其他线索（如Img和HF）结合时，能带来显著的性能提升。</li>
<li><strong>MoEA的有效性：</strong> MoEA能够有效地捕捉和整合不同线索的独特特征，从而提升模型的泛化能力。</li>
</ul>
<p>这些结果表明，MCAN通过整合多样化的、互补的视觉线索，并利用MoEA进行动态聚合，成功解决了AI生成图像检测中的泛化性问题，为构建更鲁棒的检测器提供了新的思路。</p>
<p><strong>4. 提及的局限性：</strong></p>
<ul>
<li><strong>泛化性挑战：</strong> 尽管MCAN表现出色，但论文也提到，所有线索在面对泛化性挑战时都存在一定的困难，MCAN通过互补性来克服这些挑战。</li>
<li><strong>MoEA层数选择：</strong> 实验表明，将MoEA层插入到所有层并非最优选择，最佳性能是通过将MoEA层插入到最后四层来实现的，这暗示了网络结构设计的重要性。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>更精细的线索设计：</strong> 尽管CI线索取得了成功，但未来可以继续探索和设计更多能够捕捉生成模型独特痕迹的线索。</li>
<li><strong>MoEA的优化：</strong> 进一步研究MoEA的结构和训练策略，以期在更少的计算资源下获得更好的性能。</li>
<li><strong>跨领域/跨模态检测：</strong> 将MCAN的思路扩展到更广泛的领域，例如视频生成图像检测，或者结合文本信息进行检测。</li>
<li><strong>对抗性鲁棒性：</strong> 研究MCAN在面对对抗性攻击时的鲁棒性，并探索相应的防御策略。</li>
</ul>
<hr />
<p>总而言之，这篇论文提出了一种创新的多线索聚合网络MCAN，通过引入色度不一致性（CI）这一新颖线索，并结合混合编码器适配器（MoEA）来动态整合图像内容、高频信息和CI信息，显著提升了AI生成图像检测器的泛化能力和鲁棒性，为该领域的研究提供了重要的贡献。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this paper, we introduce the Multi-Cue Aggregation Network (MCAN), a novel framework that integrates different yet complementary cues in a unified network.</li>
<li>Additionally, we introduce a Chromatic Inconsistency (CI) cue, which normalizes intensity values and captures noise information introduced during the image acquisition process in real images, making these noise patterns more distinguishable from those in AI-generated content.</li>
<li>Unlike prior methods, MCAN's novelty lies in its unified multi-cue aggregation framework, which integrates spatial, frequency-domain, and chromaticity-based information for enhanced representation learning.</li>
<li>Extensive experiments on the GenImage, Chameleon, and UniversalFakeDetect benchmark validate the state-of-the-art performance of MCAN.</li>
<li>In the GenImage dataset, MCAN outperforms the best state-of-the-art method by up to 7.4% in average ACC across eight different image generators.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.08790v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.08790v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.08665v1'></a></p>
<h2 id="vlingnav-embodied-navigation-with-adaptive-reasoning-and-visual-assisted-linguistic-memory"><a href="https://arxiv.org/abs/2601.08665v1">VLingNav: Embodied Navigation with Adaptive Reasoning and Visual-Assisted Linguistic Memory</a></h2>
<p><strong>Authors:</strong> Shaoan Wang, Yuanfei Luo, Xingyu Chen, Aocheng Luo, Dongyue Li, Chang Liu, Sheng Chen, Yangang Zhang, Junzhi Yu</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Categories:</strong> cs.RO, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>VLA models have shown promising potential in embodied navigation by unifying perception and planning while inheriting the strong generalization abilities of large VLMs. However, most existing VLA models rely on reactive mappings directly from observations to actions, lacking the explicit reasoning capabilities and persistent memory required for complex, long-horizon navigation tasks. To address these challenges, we propose VLingNav, a VLA model for embodied navigation grounded in linguistic-driven cognition. First, inspired by the dual-process theory of human cognition, we introduce an adaptive chain-of-thought mechanism, which dynamically triggers explicit reasoning only when necessary, enabling the agent to fluidly switch between fast, intuitive execution and slow, deliberate planning. Second, to handle long-horizon spatial dependencies, we develop a visual-assisted linguistic memory module that constructs a persistent, cross-modal semantic memory, enabling the agent to recall past observations to prevent repetitive exploration and infer movement trends for dynamic environments. For the training recipe, we construct Nav-AdaCoT-2.9M, the largest embodied navigation dataset with reasoning annotations to date, enriched with adaptive CoT annotations that induce a reasoning paradigm capable of adjusting both when to think and what to think about. Moreover, we incorporate an online expert-guided reinforcement learning stage, enabling the model to surpass pure imitation learning and to acquire more robust, self-explored navigation behaviors. Extensive experiments demonstrate that VLingNav achieves state-of-the-art performance across a wide range of embodied navigation benchmarks. Notably, VLingNav transfers to real-world robotic platforms in a zero-shot manner, executing various navigation tasks and demonstrating strong cross-domain and cross-task generalization.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“VLingNav: Embodied Navigation with Adaptive Reasoning and Visual-Assisted Linguistic Memory”的全面中文摘要：</p>
<p><strong>论文题目：</strong> VLingNav: Embodied Navigation with Adaptive Reasoning and Visual-Assisted Linguistic Memory</p>
<p><strong>作者：</strong> Shaoan Wang, Yuanfei Luo, Xingyu Chen, Aocheng Luo, Dongyue Li, Chang Liu, Sheng Chen, Yangang Zhang, Junzhi Yu</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
现有基于视觉-语言-动作（VLA）的模型在具身导航领域展现出巨大潜力，但它们通常依赖于从观测到动作的直接反应式映射，缺乏复杂、长时程导航任务所需的显式推理能力和持久记忆。这导致模型在处理不确定性、避免重复探索以及适应动态环境时存在局限。</p>
<p><strong>2. 主要创新与方法贡献：</strong>
为了解决上述挑战，本文提出了 <strong>VLingNav</strong>，一个基于语言驱动认知的具身导航模型，其核心创新包括：</p>
<ul>
<li><strong>自适应思维链（Adaptive Chain-of-Thought, AdaCoT）机制：</strong> 受人类双过程认知理论启发，AdaCoT 能够动态触发显式推理，仅在必要时进行深入思考，从而使智能体能够在快速直观执行和缓慢审慎规划之间流畅切换，平衡了推理效率和导航性能。</li>
<li><strong>视觉辅助语言记忆（Visual-Assisted Linguistic Memory, VLingMem）模块：</strong> 该模块构建了一个持久的跨模态语义记忆，能够回忆过去的观察结果，以防止重复探索并推断动态环境中的运动趋势。它将关键视觉信息提炼为简洁的语言摘要，比压缩的视觉特征更鲁棒，能有效防止智能体陷入循环或重复访问区域。</li>
<li><strong>Nav-AdaCoT-2.9M 数据集：</strong> 构建了迄今为止最大的包含推理标注的具身导航数据集，其中包含自适应思维链标注，用于训练模型何时以及如何进行推理。</li>
<li><strong>在线专家引导强化学习（Online Expert-guided Reinforcement Learning）：</strong> 在监督学习之后，引入了在线专家引导的强化学习阶段，使模型能够超越纯粹的模仿学习，获得更鲁棒、自主探索的导航行为。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong>
*   <strong>SOTA 性能：</strong> VLingNav 在多个标准具身导航基准测试中取得了最先进（State-of-the-Art, SOTA）的性能，显著优于现有 VLA 模型，尤其在长时程推理和成功率方面表现突出。
*   <strong>零样本迁移到真实世界：</strong> VLingNav 能够零样本（zero-shot）迁移到真实世界机器人平台，成功执行了之前未见过且未训练过的导航任务，展现了强大的跨领域和跨任务泛化能力。
*   <strong>鲁棒性与效率：</strong> AdaCoT 和 VLingMem 的结合，使得模型在复杂环境中能够做出更优的决策，并以更高效的路径完成导航任务。
*   <strong>泛化能力：</strong> 多任务联合训练（ObjNav, EVT, ImageNav）以及与开放世界视频数据的协同训练，显著提升了模型的泛化能力，使其能够处理更广泛的导航场景和指令。</p>
<p><strong>4. 论文中提到的局限性：</strong>
*   <strong>单目输入限制：</strong> 当前模型主要依赖单目、自视角观测作为输入，这限制了其感知能力。
*   <strong>单系统架构：</strong> 模型采用单一系统架构，限制了其预测频率，这在处理高度动态环境和障碍物规避时可能成为瓶颈。
*   <strong>MPC 控制器：</strong> 模型使用基于 MPC 的航点控制器，缺乏更灵活的运动模型，可能影响移动速度和可达区域。</p>
<p><strong>5. 未来研究方向：</strong>
*   <strong>多视角整合：</strong> 探索整合多视角观测以提升导航效率。
*   <strong>双系统架构：</strong> 升级为双系统结构以支持高频动作输出，提升动态环境下的导航性能。
*   <strong>灵活的运动控制：</strong> 集成更灵活的运动控制器，以提高移动速度和机器人可达性。
*   <strong>持续探索：</strong> 进一步研究如何利用强化学习和专家知识来优化具身智能体的决策能力。</p>
<p><strong>总结：</strong>
VLingNav 是一项重要的研究成果，它通过引入自适应推理和视觉辅助语言记忆，显著提升了具身导航模型的性能和泛化能力。该模型不仅在模拟环境中取得了 SOTA 结果，更重要的是，它能够零样本迁移到真实机器人，并成功执行复杂导航任务，为构建更智能、高效、可解释的具身智能体提供了新的范式。其提出的数据集和训练方法也为该领域的研究提供了宝贵资源。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address these challenges, we propose VLingNav, a VLA model for embodied navigation grounded in linguistic-driven cognition.</li>
<li>First, inspired by the dual-process theory of human cognition, we introduce an adaptive chain-of-thought mechanism, which dynamically triggers explicit reasoning only when necessary, enabling the agent to fluidly switch between fast, intuitive execution and slow, deliberate planning.</li>
<li>Second, to handle long-horizon spatial dependencies, we develop a visual-assisted linguistic memory module that constructs a persistent, cross-modal semantic memory, enabling the agent to recall past observations to prevent repetitive exploration and infer movement trends for dynamic environments.</li>
<li>Extensive experiments demonstrate that VLingNav achieves state-of-the-art performance across a wide range of embodied navigation benchmarks.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.08665v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.08665v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.08623v1'></a></p>
<h2 id="saferedir-prompt-embedding-redirection-for-robust-unlearning-in-image-generation-models"><a href="https://arxiv.org/abs/2601.08623v1">SafeRedir: Prompt Embedding Redirection for Robust Unlearning in Image Generation Models</a></h2>
<p><strong>Authors:</strong> Renyang Liu, Kangjie Chen, Han Qiu, Jie Zhang, Kwok-Yan Lam, Tianwei Zhang, See-Kiong Ng</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.CR, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Image generation models (IGMs), while capable of producing impressive and creative content, often memorize a wide range of undesirable concepts from their training data, leading to the reproduction of unsafe content such as NSFW imagery and copyrighted artistic styles. Such behaviors pose persistent safety and compliance risks in real-world deployments and cannot be reliably mitigated by post-hoc filtering, owing to the limited robustness of such mechanisms and a lack of fine-grained semantic control. Recent unlearning methods seek to erase harmful concepts at the model level, which exhibit the limitations of requiring costly retraining, degrading the quality of benign generations, or failing to withstand prompt paraphrasing and adversarial attacks. To address these challenges, we introduce SafeRedir, a lightweight inference-time framework for robust unlearning via prompt embedding redirection. Without modifying the underlying IGMs, SafeRedir adaptively routes unsafe prompts toward safe semantic regions through token-level interventions in the embedding space. The framework comprises two core components: a latent-aware multi-modal safety classifier for identifying unsafe generation trajectories, and a token-level delta generator for precise semantic redirection, equipped with auxiliary predictors for token masking and adaptive scaling to localize and regulate the intervention. Empirical results across multiple representative unlearning tasks demonstrate that SafeRedir achieves effective unlearning capability, high semantic and perceptual preservation, robust image quality, and enhanced resistance to adversarial attacks. Furthermore, SafeRedir generalizes effectively across a variety of diffusion backbones and existing unlearned models, validating its plug-and-play compatibility and broad applicability. Code and data are available at https://github.com/ryliu68/SafeRedir.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“SafeRedir: Prompt Embedding Redirection for Robust Unlearning in Image Generation Models”的全面中文摘要：</p>
<p><strong>论文题目：</strong> SafeRedir: Prompt Embedding Redirection for Robust Unlearning in Image Generation Models</p>
<p><strong>作者：</strong> Renyang Liu, Kangjie Chen, Han Qiu, Jie Zhang, Kwok-Yan Lam, Tianwei Zhang, See-Kiong Ng</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 主要问题/研究问题：</strong></p>
<p>图像生成模型（IGMs），如 Stable Diffusion，虽然能够生成令人印象深刻的内容，但常常会“记住”训练数据中的不安全概念（如 NSFW 内容、侵权艺术风格、敏感信息等），并在用户提示下重现这些内容。现有的安全防护方法，如事后过滤，因其鲁棒性不足和缺乏细粒度语义控制而效果有限。模型级别的“遗忘”（unlearning）方法虽然旨在从模型内部消除这些不安全概念，但面临着成本高昂的重新训练、损害生成质量、以及容易被提示词的变体或对抗性攻击绕过等挑战。因此，研究如何实现一种<strong>高效、鲁棒且易于部署的图像生成模型遗忘方法，以有效消除不安全内容，同时保留生成质量和对提示词变化的抵抗力</strong>，是本文要解决的核心问题。</p>
<p><strong>2. 关键创新点/方法论贡献：</strong></p>
<p>SafeRedir 提出了一种<strong>轻量级的、在推理时运行的框架，通过提示词嵌入重定向（prompt embedding redirection）来实现鲁棒的遗忘</strong>。其核心创新点在于：</p>
<ul>
<li><strong>模型无关性与即插即用性：</strong> SafeRedir <strong>无需修改底层图像生成模型（IGM）的参数或架构</strong>，而是作为一个独立的模块，在推理过程中通过钩子（hooks）介入。这使其能够轻松应用于各种扩散模型（如 Stable Diffusion 的不同版本、OpenJourney 等），并与现有已遗忘的模型兼容。</li>
<li><strong>嵌入空间中的细粒度干预：</strong> SafeRedir 在<strong>提示词嵌入空间</strong>进行操作，而不是直接修改模型权重。它通过<strong>令牌级别（token-level）的干预</strong>，将不安全提示词的嵌入动态地引导至安全的语义区域。</li>
<li><strong>多模态安全检测器：</strong> 框架包含一个<strong>轻量级的、多模态的安全分类器</strong>，它能够联合分析图像潜在特征（latent features）、扩散时间步（timestep）以及提示词嵌入，从而实现对不安全生成轨迹的<strong>上下文感知和早期检测</strong>，即使面对隐晦或经过释义的提示词也能有效识别。</li>
<li><strong>自适应令牌重定向：</strong> SafeRedir 引入了一个<strong>令牌级别的 Delta 生成器</strong>，用于精确的语义重定向。该生成器利用辅助预测器来：<ul>
<li><strong>令牌掩码（token masking）：</strong> 精准定位对不安全内容负责的令牌。</li>
<li><strong>自适应缩放（adaptive scaling）：</strong> 动态调整重定向的强度，以实现最小化干预，避免对安全内容造成不必要的干扰。</li>
<li><strong>方向向量预测：</strong> 学习从不安全到安全的语义转移方向。</li>
</ul>
</li>
<li><strong>损失函数设计：</strong> 采用多任务损失函数，包括分类损失、均方误差损失、余弦相似度损失、掩码损失和对齐损失，以同时优化安全检测、语义重定向的准确性、鲁棒性和对原始语义的保留。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong></p>
<p>SafeRedir 在多个代表性的遗忘任务（如 NSFW 内容、艺术风格移除、物体移除）上进行了广泛的实验评估，并取得了显著成果：</p>
<ul>
<li><strong>卓越的遗忘能力：</strong> 在 NSFW、Van Gogh 风格和 Church 物体移除等任务上，SafeRedir 实现了<strong>最高或接近最高的遗忘成功率（FSR）</strong>，有效抑制了不安全内容的生成。</li>
<li><strong>高质量的语义和感知保留：</strong> SafeRedir 在移除不安全内容的同时，<strong>最大限度地保留了原始图像的语义和感知质量</strong>。其 CSDR 和 LPIPS 指标均优于或媲美现有方法，表明对安全内容的干扰极小。</li>
<li><strong>鲁棒的图像质量：</strong> 对安全提示词的生成，SafeRedir 保持了<strong>高水平的图像质量</strong>，FID、Q-Align 等指标表现出色，避免了遗忘过程带来的模糊或失真。</li>
<li><strong>强大的对抗性鲁棒性：</strong> SafeRedir 对抗性攻击（如提示词变体、对抗性提示词）表现出<strong>显著的抵抗能力</strong>，显著降低了攻击成功率（ASR），并增加了攻击难度。</li>
<li><strong>广泛的通用性和可扩展性：</strong> SafeRedir <strong>能够有效地泛化到多种不同的扩散模型骨干网络</strong>，并且可以<strong>增强现有遗忘方法的性能</strong>，使其成为一个高度通用和可扩展的解决方案。</li>
<li><strong>轻量级和高效：</strong> 框架模型大小仅为 50MB，推理延迟低，<strong>部署成本低廉，易于大规模应用</strong>。</li>
</ul>
<p>这些结果表明，SafeRedir 是一种<strong>高效、鲁棒且易于部署的图像生成模型遗忘框架</strong>，能够有效解决当前模型面临的安全和合规性挑战，为负责任的 AI 部署提供了重要支持。</p>
<p><strong>4. 论文中提到的局限性：</strong></p>
<ul>
<li><strong>对特定敏感词的依赖：</strong> 虽然 SafeRedir 能够处理提示词的变体，但其训练数据构建依赖于“安全-不安全”的提示词对，对于完全未见过的、高度隐晦的敏感概念，其检测和遗忘能力可能受到影响。</li>
<li><strong>计算成本：</strong> 尽管 SafeRedir 在推理时是轻量级的，但其训练过程仍然需要构建大量的安全-不安全提示词对，并进行多模态特征的提取和处理，这可能需要一定的计算资源。</li>
<li><strong>潜在的误用：</strong> 论文也提到了，遗忘技术本身也可能被滥用，例如用于压制合法的、非敏感的内容，这需要谨慎使用和负责任的部署。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>扩展到更多生成模型家族：</strong> 将 SafeRedir 的方法扩展到<strong>非扩散模型</strong>（如 Transformer-based 或自回归模型）以及<strong>其他模态的生成模型</strong>。</li>
<li><strong>更高级的语义解耦：</strong> 探索更先进的技术来<strong>解耦更抽象或全局的场景语义</strong>，例如政治偏见或隐含的暴力内容。</li>
<li><strong>任务自适应调优：</strong> 研究<strong>任务自适应调优或强化学习引导的重定向策略</strong>，以进一步增强 SafeRedir 的灵活性和安全性。</li>
<li><strong>更精细的控制：</strong> 开发更精细的控制机制，允许用户在<strong>安全性和内容创造性之间进行更灵活的权衡</strong>。</li>
<li><strong>对抗性鲁棒性的进一步提升：</strong> 持续研究更强大的对抗性攻击，并开发更具韧性的防御机制。</li>
</ul>
<p>总而言之，SafeRedir 是一项重要的研究成果，它通过创新的提示词嵌入重定向技术，有效地解决了图像生成模型中存在的安全隐患，为实现更安全、更负责任的 AI 应用奠定了坚实的基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address these challenges, we introduce SafeRedir, a lightweight inference-time framework for robust unlearning via prompt embedding redirection.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.08623v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.08623v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.08620v1'></a></p>
<h2 id="vidore-v3-a-comprehensive-evaluation-of-retrieval-augmented-generation-in-complex-real-world-scenarios"><a href="https://arxiv.org/abs/2601.08620v1">ViDoRe V3: A Comprehensive Evaluation of Retrieval Augmented Generation in Complex Real-World Scenarios</a></h2>
<p><strong>Authors:</strong> António Loison, Quentin Macé, Antoine Edy, Victor Xing, Tom Balough, Gabriel Moreira, Bo Liu, Manuel Faysse, Céline Hudelot, Gautier Viaud</p>
<p><strong>Published:</strong> 2026-01-13</p>
<p><strong>Categories:</strong> cs.AI, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Retrieval-Augmented Generation (RAG) pipelines must address challenges beyond simple single-document retrieval, such as interpreting visual elements (tables, charts, images), synthesizing information across documents, and providing accurate source grounding. Existing benchmarks fail to capture this complexity, often focusing on textual data, single-document comprehension, or evaluating retrieval and generation in isolation. We introduce ViDoRe v3, a comprehensive multimodal RAG benchmark featuring multi-type queries over visually rich document corpora. It covers 10 datasets across diverse professional domains, comprising ~26,000 document pages paired with 3,099 human-verified queries, each available in 6 languages. Through 12,000 hours of human annotation effort, we provide high-quality annotations for retrieval relevance, bounding box localization, and verified reference answers. Our evaluation of state-of-the-art RAG pipelines reveals that visual retrievers outperform textual ones, late-interaction models and textual reranking substantially improve performance, and hybrid or purely visual contexts enhance answer generation quality. However, current models still struggle with non-textual elements, open-ended queries, and fine-grained visual grounding. To encourage progress in addressing these challenges, the benchmark is released under a commercially permissive license at https://hf.co/vidore.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“ViDoRe V3: A Comprehensive Evaluation of Retrieval Augmented Generation in Complex Real-World Scenarios”的全面摘要：</p>
<p><strong>论文标题：</strong> ViDoRe V3: A Comprehensive Evaluation of Retrieval Augmented Generation in Complex Real-World Scenarios</p>
<p><strong>作者：</strong> António Loison, Quentin Macé, Antoine Edy, Victor Xing, Tom Balough, Gabriel Moreira, Bo Liu, Manuel Faysse, Céline Hudelot, Gautier Viaud</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 主要问题或研究问题：</strong></p>
<p>现有的检索增强生成（RAG）基准在评估复杂、真实世界场景下的多模态RAG系统时存在不足。这些基准往往过于侧重文本数据、单文档理解，或将检索和生成孤立评估。然而，实际应用中的RAG系统需要处理更复杂的挑战，包括：
*   <strong>理解视觉元素：</strong> 如表格、图表和图像中的信息。
*   <strong>跨文档信息综合：</strong> 将来自多个文档的信息整合起来。
*   <strong>精确的来源追溯（Grounding）：</strong> 能够准确地指出答案的来源，例如通过定位文档中的特定区域（bounding boxes）。</p>
<p><strong>2. 关键创新或方法论贡献：</strong></p>
<ul>
<li><strong>ViDoRe V3基准的构建：</strong> 作者提出了ViDoRe V3，一个全面的多模态RAG基准，专门设计用于评估在视觉丰富的文档语料库上的端到端RAG系统。<ul>
<li><strong>大规模多领域语料库：</strong> 包含10个不同专业领域的语料库，总计约26,000页文档。</li>
<li><strong>高质量人工标注：</strong> 经过12,000小时的人工标注，生成了3,099个经过验证的人类查询，并提供了检索相关性、边界框定位和验证过的参考答案。</li>
<li><strong>多语言支持：</strong> 查询提供6种语言版本，以评估跨语言检索能力。</li>
<li><strong>详细的查询分类：</strong> 引入了包含7种查询类型（如开放式、抽取式、多跳等）和3种查询格式（如问题、关键词、指令）的分类体系，以进行更细致的性能分析。</li>
</ul>
</li>
<li><strong>创新的人工标注方法论：</strong> 设计了一种“人机协作”的标注流程，结合了视觉语言模型（VLM）的预标注和人类专家的验证，以高效、高质量地生成标注数据，并尽量减少偏见。</li>
<li><strong>全面的评估框架：</strong> ViDoRe V3支持对RAG管道的三个核心组件（检索、生成和视觉追溯）进行独立和端到端的评估。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong></p>
<ul>
<li><strong>视觉检索器的优势：</strong> 实验表明，视觉检索器在页面级检索能力上优于纯文本检索器。</li>
<li><strong>后期交互和文本重排序的提升：</strong> 后期交互模型和文本重排序显著提高了检索性能。</li>
<li><strong>混合或纯视觉上下文的生成优势：</strong> 混合或纯视觉上下文能够提升答案生成质量。</li>
<li><strong>模型在非文本元素、开放式查询和精细视觉追溯方面的挑战：</strong> 尽管取得了进展，但当前模型在处理表格、图表等非文本元素，理解开放式查询，以及实现精细的视觉追溯方面仍存在困难。</li>
<li><strong>跨语言检索的性能下降：</strong> 跨语言查询会降低检索性能，表明模型需要更好地适应多语言环境。</li>
<li><strong>查询复杂性与检索性能的关系：</strong> 查询复杂度越高（如开放式、多跳查询），检索性能越低。</li>
<li><strong>内容类型对检索的影响：</strong> 视觉内容（如图像、表格）的检索比纯文本更具挑战性。</li>
<li><strong>人工标注的价值：</strong> 实验结果强调了高质量、多模态、跨语言标注的重要性，以及现有基准在捕捉这些复杂性方面的不足。</li>
</ul>
<p><strong>4. 论文中提到的局限性：</strong></p>
<ul>
<li><strong>语言覆盖范围有限：</strong> 基准主要基于英语和法语文档，并覆盖6种高资源西欧语言，可能对其他语言和非拉丁语系脚本存在偏见。</li>
<li><strong>文档分布偏差：</strong> 主要使用公开可用的长篇文档，未能完全代表企业RAG可能遇到的更广泛的文档类型（如电子邮件、支持票据、扫描笔记等）。</li>
<li><strong>人工标注的主观性：</strong> 开放式推理和视觉追溯的标注 inherently 存在一定主观性，可能存在未被标注的“正确”答案。</li>
<li><strong>计算资源消耗：</strong> 构建和评估基准需要大量的计算资源和人力投入。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>提升跨语言和开放式查询的检索能力：</strong> 需要改进模型以更好地处理跨语言场景和需要视觉解释的开放式查询。</li>
<li><strong>改进多页面上下文下的答案生成：</strong> 模型在处理多页面上下文时的答案生成能力仍需提升。</li>
<li><strong>增强精细的视觉追溯能力：</strong> 模型在准确地定位答案来源（bounding boxes）方面仍有很大提升空间。</li>
<li><strong>扩展语言覆盖范围：</strong> 未来应包含更多语言家族和非拉丁语系脚本的文档。</li>
<li><strong>处理更多样化的文档类型：</strong> 纳入企业环境中常见的各种文档格式。</li>
<li><strong>进一步探索混合检索和重排序策略：</strong> 结合不同模态的检索结果，并优化重排序方法。</li>
</ul>
<p><strong>对计算机视觉领域的贡献和新颖性：</strong></p>
<p>ViDoRe V3的提出是计算机视觉领域在<strong>多模态文档理解和检索增强生成（RAG）</strong>方面的重要进展。其新颖性体现在：</p>
<ul>
<li><strong>填补了真实世界复杂场景下多模态RAG评估的空白：</strong> 现有的基准往往过于简化，而ViDoRe V3通过引入视觉丰富的文档、多样的查询类型和精细的标注，更真实地模拟了用户在实际场景中与文档交互的需求。</li>
<li><strong>强调了视觉信息在RAG中的关键作用：</strong> 论文通过实验证明了视觉检索器的优越性以及视觉内容对答案生成质量的影响，突出了视觉信息在知识密集型任务中的不可或缺性。</li>
<li><strong>推动了对视觉追溯（Visual Grounding）的深入研究：</strong> ViDoRe V3不仅评估答案生成，还对模型进行精细的视觉追溯能力评估，这对于构建可信赖的AI系统至关重要，因为它可以让用户验证答案的来源。</li>
<li><strong>构建了一个可扩展、多语言、多领域的研究平台：</strong> 该基准的发布，特别是其在MTEB排行榜上的集成，为研究社区提供了一个标准化的评估框架，有望加速多模态RAG领域的研究进展。</li>
</ul>
<p>总而言之，ViDoRe V3通过其全面、真实且多模态的设计，为评估和推动下一代RAG系统（特别是那些需要理解和利用视觉信息的系统）的发展奠定了坚实的基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce ViDoRe v3, a comprehensive multimodal RAG benchmark featuring multi-type queries over visually rich document corpora.</li>
<li>Our evaluation of state-of-the-art RAG pipelines reveals that visual retrievers outperform textual ones, late-interaction models and textual reranking substantially improve performance, and hybrid or purely visual contexts enhance answer generation quality.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.08620v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.08620v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2026-01-14 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
