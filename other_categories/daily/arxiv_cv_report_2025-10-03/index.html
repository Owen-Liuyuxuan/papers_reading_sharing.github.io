<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-10-03 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-02
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-10-02/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-10-06/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-10-03">Arxiv Computer Vision Papers - 2025-10-03</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#from-behavioral-performance-to-internal-competence-interpreting-vision-language-models-with-vlm-lens" class="nav-link">From Behavioral Performance to Internal Competence: Interpreting Vision-Language Models with VLM-Lens</a>
                </li>
                <li class="nav-item">
                    <a href="#learning-to-generate-object-interactions-with-physics-guided-video-diffusion" class="nav-link">Learning to Generate Object Interactions with Physics-Guided Video Diffusion</a>
                </li>
                <li class="nav-item">
                    <a href="#self-forcing-towards-minute-scale-high-quality-video-generation" class="nav-link">Self-Forcing++: Towards Minute-Scale High-Quality Video Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#vidguard-r1-ai-generated-video-detection-and-explanation-via-reasoning-mllms-and-rl" class="nav-link">VidGuard-R1: AI-Generated Video Detection and Explanation via Reasoning MLLMs and RL</a>
                </li>
                <li class="nav-item">
                    <a href="#microclip-unsupervised-clip-adaptation-via-coarse-fine-token-fusion-for-fine-grained-image-classification" class="nav-link">microCLIP: Unsupervised CLIP Adaptation via Coarse-Fine Token Fusion for Fine-Grained Image Classification</a>
                </li>
                <li class="nav-item">
                    <a href="#do-you-know-where-your-camera-is-view-invariant-policy-learning-with-camera-conditioning" class="nav-link">Do You Know Where Your Camera Is? View-Invariant Policy Learning with Camera Conditioning</a>
                </li>
                <li class="nav-item">
                    <a href="#from-frames-to-clips-efficient-key-clip-selection-for-long-form-video-understanding" class="nav-link">From Frames to Clips: Efficient Key Clip Selection for Long-Form Video Understanding</a>
                </li>
                <li class="nav-item">
                    <a href="#dragflow-unleashing-dit-priors-with-region-based-supervision-for-drag-editing" class="nav-link">DragFlow: Unleashing DiT Priors with Region Based Supervision for Drag Editing</a>
                </li>
                <li class="nav-item">
                    <a href="#geopurify-a-data-efficient-geometric-distillation-framework-for-open-vocabulary-3d-segmentation" class="nav-link">GeoPurify: A Data-Efficient Geometric Distillation Framework for Open-Vocabulary 3D Segmentation</a>
                </li>
                <li class="nav-item">
                    <a href="#gaussianmorphing-mesh-guided-3d-gaussians-for-semantic-aware-object-morphing" class="nav-link">GaussianMorphing: Mesh-Guided 3D Gaussians for Semantic-Aware Object Morphing</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-10-03">Arxiv Computer Vision Papers - 2025-10-03</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>好的，这是一份针对2025年10月2日Arxiv计算机视觉论文的执行摘要，旨在帮助忙碌的研究人员快速了解关键发展：</p>
<hr />
<p><strong>Arxiv 计算机视觉每日报告执行摘要 (2025年10月2日)</strong></p>
<p><strong>1. 主要主题与趋势概述：</strong></p>
<p>今天的论文集展现了计算机视觉领域几个活跃且相互关联的趋势。<strong>多模态学习（特别是视觉-语言模型 VLM）</strong>的解释性、适应性和应用是核心焦点，体现在对VLM内部机制的理解、微调以及其在视频检测中的应用。<strong>视频生成与理解</strong>是另一个突出主题，涵盖了从高质量、长时视频生成到高效视频内容分析的多个方面。此外，<strong>3D视觉</strong>在语义分割和对象操作方面也取得了进展，并与生成模型相结合。<strong>数据效率和无监督/自监督学习</strong>的理念贯穿于多个研究方向，旨在减少对大量标注数据的依赖。</p>
<p><strong>2. 显著或创新性论文亮点：</strong></p>
<ul>
<li><strong>"From Behavioral Performance to Internal Competence: Interpreting Vision-Language Models with VLM-Lens" (Hala Sheta et al.)</strong>：这篇论文通过引入“VLM-Lens”框架，深入探讨了VLM的内部工作机制，旨在从行为表现推断其内在能力。这对于理解和改进VLM的鲁棒性、公平性和可信赖性至关重要，是VLM可解释性研究的重要一步。</li>
<li><strong>"Self-Forcing++: Towards Minute-Scale High-Quality Video Generation" (Justin Cui et al.)</strong>：在视频生成领域，实现长时间、高质量视频一直是一个挑战。Self-Forcing++的出现，预示着视频生成技术在生成时长和质量上取得了显著突破，有望推动视频内容创作和模拟的边界。</li>
<li><strong>"VidGuard-R1: AI-Generated Video Detection and Explanation via Reasoning MLLMs and RL" (Kyoungjun Park et al.)</strong>：这篇论文不仅关注AI生成视频的检测，更进一步结合了多模态大语言模型（MLLMs）和强化学习（RL）进行解释。在深度伪造（deepfake）日益普遍的背景下，提供检测结果的解释性对于建立信任和理解AI生成内容的来源至关重要，具有重要的社会和技术意义。</li>
<li><strong>"DragFlow: Unleashing DiT Priors with Region Based Supervision for Drag Editing" (Zihan Zhou et al.)</strong>：该工作将DiT（Diffusion Transformer）的强大先验知识与区域监督相结合，实现了直观的“拖拽式”图像编辑。这种交互式编辑方式极大地提升了用户对生成模型的控制力，是人机交互与生成模型结合的优秀范例。</li>
</ul>
<p><strong>3. 新兴研究方向或技术：</strong></p>
<ul>
<li><strong>VLM的深入解释与内部机制探究：</strong> 不再满足于VLM的表面性能，而是深入理解其决策过程和内在表征。</li>
<li><strong>长时、高保真视频生成：</strong> 视频生成正从短片段向更长、更连贯、更高质量的视频发展。</li>
<li><strong>AI生成内容检测的解释性：</strong> 结合推理能力和多模态模型，不仅检测AI生成内容，还能提供检测依据。</li>
<li><strong>数据高效的3D几何学习：</strong> 利用几何蒸馏等技术，在有限数据下实现开放词汇的3D分割。</li>
<li><strong>生成模型与交互式编辑的融合：</strong> 赋予用户更精细、更直观的生成内容控制能力。</li>
</ul>
<p><strong>4. 建议阅读全文的论文：</strong></p>
<p>对于不同兴趣的研究人员，建议阅读以下论文：</p>
<ul>
<li><strong>对VLM可解释性、鲁棒性感兴趣：</strong><ul>
<li>"From Behavioral Performance to Internal Competence: Interpreting Vision-Language Models with VLM-Lens"</li>
</ul>
</li>
<li><strong>对视频生成技术前沿感兴趣：</strong><ul>
<li>"Self-Forcing++: Towards Minute-Scale High-Quality Video Generation"</li>
<li>"Learning to Generate Object Interactions with Physics-Guided Video Diffusion" (如果对物理世界交互的视频生成有兴趣)</li>
</ul>
</li>
<li><strong>对AI生成内容检测及伦理问题感兴趣：</strong><ul>
<li>"VidGuard-R1: AI-Generated Video Detection and Explanation via Reasoning MLLMs and RL"</li>
</ul>
</li>
<li><strong>对图像/视频编辑与生成模型交互感兴趣：</strong><ul>
<li>"DragFlow: Unleashing DiT Priors with Region Based Supervision for Drag Editing"</li>
</ul>
</li>
<li><strong>对3D视觉、数据高效学习和开放词汇分割感兴趣：</strong><ul>
<li>"GeoPurify: A Data-Efficient Geometric Distillation Framework for Open-Vocabulary 3D Segmentation"</li>
</ul>
</li>
<li><strong>对VLM在细粒度分类中的无监督适应感兴趣：</strong><ul>
<li>"microCLIP: Unsupervised CLIP Adaptation via Coarse-Fine Token Fusion for Fine-Grained Image Classification"</li>
</ul>
</li>
</ul>
<hr />
<p>这份摘要旨在提供一个高层次的概览，帮助您快速识别与您研究方向最相关的论文。</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2510.02292v1">From Behavioral Performance to Internal Competence: Interpreting Vision-Language Models with VLM-Lens</a></li>
<li><a href="#2510.02284v1">Learning to Generate Object Interactions with Physics-Guided Video Diffusion</a></li>
<li><a href="#2510.02283v1">Self-Forcing++: Towards Minute-Scale High-Quality Video Generation</a></li>
<li><a href="#2510.02282v1">VidGuard-R1: AI-Generated Video Detection and Explanation via Reasoning MLLMs and RL</a></li>
<li><a href="#2510.02270v1">microCLIP: Unsupervised CLIP Adaptation via Coarse-Fine Token Fusion for Fine-Grained Image Classification</a></li>
<li><a href="#2510.02268v1">Do You Know Where Your Camera Is? View-Invariant Policy Learning with Camera Conditioning</a></li>
<li><a href="#2510.02262v1">From Frames to Clips: Efficient Key Clip Selection for Long-Form Video Understanding</a></li>
<li><a href="#2510.02253v1">DragFlow: Unleashing DiT Priors with Region Based Supervision for Drag Editing</a></li>
<li><a href="#2510.02186v1">GeoPurify: A Data-Efficient Geometric Distillation Framework for Open-Vocabulary 3D Segmentation</a></li>
<li><a href="#2510.02034v1">GaussianMorphing: Mesh-Guided 3D Gaussians for Semantic-Aware Object Morphing</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2510.02292v1'></a></p>
<h2 id="from-behavioral-performance-to-internal-competence-interpreting-vision-language-models-with-vlm-lens"><a href="https://arxiv.org/abs/2510.02292v1">From Behavioral Performance to Internal Competence: Interpreting Vision-Language Models with VLM-Lens</a></h2>
<p><strong>Authors:</strong> Hala Sheta, Eric Huang, Shuyu Wu, Ilia Alenabi, Jiajun Hong, Ryker Lin, Ruoxi Ning, Daniel Wei, Jialin Yang, Jiawei Zhou, Ziqiao Ma, Freda Shi</p>
<p><strong>Published:</strong> 2025-10-02</p>
<p><strong>Categories:</strong> cs.CL, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We introduce VLM-Lens, a toolkit designed to enable systematic benchmarking,
analysis, and interpretation of vision-language models (VLMs) by supporting the
extraction of intermediate outputs from any layer during the forward pass of
open-source VLMs. VLM-Lens provides a unified, YAML-configurable interface that
abstracts away model-specific complexities and supports user-friendly operation
across diverse VLMs. It currently supports 16 state-of-the-art base VLMs and
their over 30 variants, and is extensible to accommodate new models without
changing the core logic.
  The toolkit integrates easily with various interpretability and analysis
methods. We demonstrate its usage with two simple analytical experiments,
revealing systematic differences in the hidden representations of VLMs across
layers and target concepts. VLM-Lens is released as an open-sourced project to
accelerate community efforts in understanding and improving VLMs.</p>
<p><strong>Analysis:</strong></p>
<p>以下是论文“From Behavioral Performance to Internal Competence: Interpreting Vision-Language Models with VLM-Lens”的全面摘要：</p>
<p><strong>1. 主要问题或研究问题</strong>
现有VLM基准测试主要依赖基于精确匹配的准确率评估模型性能，这可能忽略了模型隐藏表示中嵌入的信息，或因捷径利用导致误导性评估。此外，VLM的解释性研究和工具包相对不成熟，缺乏统一的框架来提取VLM的内部表示，使得系统理解其内部知识和决策过程面临挑战。本文旨在解决这些挑战，提供一个统一的工具包，以系统地基准测试、分析和解释VLM。</p>
<p><strong>2. 关键创新或方法论贡献</strong>
*   <strong>VLM-LENS工具包：</strong> 引入了一个名为VLM-LENS的工具包，旨在通过支持从开源VLM前向传播的任何层中提取中间输出，实现对VLM的系统基准测试、分析和解释。
*   <strong>统一接口：</strong> VLM-LENS提供了一个统一的、YAML可配置的接口，抽象了模型特定的复杂性，支持跨不同VLM的用户友好操作。
*   <strong>广泛的模型覆盖和灵活性：</strong> 目前支持16个最先进的基础VLM及其30多个变体，并且设计具有高度可扩展性，可以轻松添加新模型。
*   <strong>模型特定环境支持：</strong> 为不同VLM提供模型特定环境设置，每个都可以通过单行pip安装命令轻松安装。
*   <strong>与现有解释性方法集成：</strong> 该工具包易于与各种解释性和分析方法集成，例如探针（probing）、神经回路检查和知识追踪。</p>
<p><strong>3. 主要结果及其意义</strong>
*   <strong>探针实验：</strong> 通过在提取的表示上训练探针，评估VLM识别一组原始概念的内部能力。结果显示，Qwen-7b和MiniCPM-o的探针在许多数据集分割中几乎达到完美准确率，且与控制探针性能存在显著差异，尤其是在最后一层表示中。这表明这些模型编码了任务相关信息。
*   <strong>概念相似性实验：</strong> 受到Stroop效应的启发，通过构建具有不一致颜色线索的图像和模糊文本指令，探究VLM如何在这种歧义下理解颜色概念。结果显示，模型可靠地编码了词汇内容、前景字体颜色和背景颜色这三种信息，且背景颜色产生了最强的匹配与不匹配对比。词汇内容比字体颜色更突出。
*   <strong>性能和能力分析：</strong> VLM-LENS通过探针评估，补充了现有基于准确率的基准测试，提供了对内部状态所表示内容的更详细理解，将模型性能建立在可解释的原始知识之上。
*   <strong>效率评估：</strong> CLIP在推理时间和内存使用方面表现最佳，这归因于其紧凑的架构和较少的参数。InternVL、InternLM-XComposer-2.5和MolMo是速度最慢的模型。</p>
<p><strong>4. 论文中提到的局限性</strong>
*   <strong>下游任务支持有限：</strong> 当前工具包不直接支持除探针之外的更多下游任务，例如注意力解释和神经回路发现。
*   <strong>梯度分析限制：</strong> 当前的推理和数据库存储方法阻止了使用基于梯度的显著性分析（如Grad-CAM）。
*   <strong>用户自定义功能：</strong> 当前用户仍需实现其自定义功能。</p>
<p><strong>5. 潜在的未来研究方向</strong>
*   <strong>社区贡献：</strong> 期望社区对存储库进行快速贡献，以支持各种任务，作者致力于长期支持这些努力。
*   <strong>扩展下游任务支持：</strong> 进一步开发工具包，以支持更广泛的解释性方法，如注意力解释和神经回路发现。
*   <strong>改进梯度分析：</strong> 探索新的推理和数据库存储方法，以支持基于梯度的显著性分析。
*   <strong>持续模型分析和改进：</strong> VLM-LENS将积极支持对VLM的分析和改进，以解决现有基准测试中模型性能与实际能力之间的差距。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce VLM-Lens, a toolkit designed to enable systematic benchmarking,
analysis, and interpretation of vision-language models (VLMs) by supporting the
extraction of intermediate outputs from any layer during the forward pass of
open-source VLMs. VLM-Lens provides a unified, YAML-configurable interface that
abstracts away model-specific complexities and supports user-friendly operation
across diverse VLMs. It currently supports 16 state-of-the-art base VLMs and
their over 30 variants, and is extensible to accommodate new models without
changing the core logic.</li>
<li>We demonstrate its usage with two simple analytical experiments,
revealing systematic differences in the hidden representations of VLMs across
layers and target concepts.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.02292v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.02292v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.02284v1'></a></p>
<h2 id="learning-to-generate-object-interactions-with-physics-guided-video-diffusion"><a href="https://arxiv.org/abs/2510.02284v1">Learning to Generate Object Interactions with Physics-Guided Video Diffusion</a></h2>
<p><strong>Authors:</strong> David Romero, Ariana Bermudez, Hao Li, Fabio Pizzati, Ivan Laptev</p>
<p><strong>Published:</strong> 2025-10-02</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Recent models for video generation have achieved remarkable progress and are
now deployed in film, social media production, and advertising. Beyond their
creative potential, such models also hold promise as world simulators for
robotics and embodied decision making. Despite strong advances, however,
current approaches still struggle to generate physically plausible object
interactions and lack physics-grounded control mechanisms. To address this
limitation, we introduce KineMask, an approach for physics-guided video
generation that enables realistic rigid body control, interactions, and
effects. Given a single image and a specified object velocity, our method
generates videos with inferred motions and future object interactions. We
propose a two-stage training strategy that gradually removes future motion
supervision via object masks. Using this strategy we train video diffusion
models (VDMs) on synthetic scenes of simple interactions and demonstrate
significant improvements of object interactions in real scenes. Furthermore,
KineMask integrates low-level motion control with high-level textual
conditioning via predictive scene descriptions, leading to effective support
for synthesis of complex dynamical phenomena. Extensive experiments show that
KineMask achieves strong improvements over recent models of comparable size.
Ablation studies further highlight the complementary roles of low- and
high-level conditioning in VDMs. Our code, model, and data will be made
publicly available.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供David Romero等人撰写的论文“Learning to Generate Object Interactions with Physics-Guided Video Diffusion”的全面摘要。</p>
<hr />
<h3 id="learning-to-generate-object-interactions-with-physics-guided-video-diffusion_1">论文摘要：Learning to Generate Object Interactions with Physics-Guided Video Diffusion</h3>
<p><strong>1. 主要问题或研究问题：</strong>
尽管视频生成模型（特别是视频扩散模型，VDMs）在视觉质量和时间一致性方面取得了显著进展，并在电影、社交媒体和广告等领域得到应用，但它们在生成物理上可信的对象交互和缺乏物理基础的控制机制方面仍然面临挑战。现有的方法难以捕捉对象永久性、因果交互以及刚体动力学等基本物理特性，导致生成的视频中物体交互不真实或形状失真。因此，本文旨在解决两个核心问题：(1) 视频扩散模型能否在给定初始动态条件下生成对象之间真实的交互？(2) 数据和文本条件如何影响生成视频中因果物理效应的出现？</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
为了解决上述问题，作者提出了 <strong>KineMask</strong>，一个物理引导的视频生成框架，其主要创新点包括：</p>
<ul>
<li><strong>KineMask 框架：</strong> 引入了一种基于对象运动条件化的机制，通过新颖的两阶段训练和条件编码，使VDMs能够生成物理上真实的刚体控制、交互和效果。</li>
<li><strong>两阶段训练策略：</strong><ul>
<li><strong>第一阶段训练：</strong> 在合成数据集上训练ControlNet，该数据集包含物理有效的动态和明确的对象交互，以及底层事件的文本描述。运动控制信号被编码为对象的速度掩码（mf），其中三个通道对应于x、y、z轴上的瞬时速度。模型在所有帧上都接收速度监督。</li>
<li><strong>第二阶段训练（掩码丢弃策略）：</strong> 在第一阶段训练的基础上，引入了掩码丢弃策略，在训练时随机丢弃速度掩码的最后一部分帧。这意味着只有初始帧包含速度监督，序列的其余部分被设置为零。这迫使模型从初始条件推断未来的运动和交互，从而在推理时仅依赖初始速度信息。</li>
</ul>
</li>
<li><strong>低级运动控制与高级文本条件结合：</strong> KineMask 将低级运动控制（通过初始对象速度掩码）与高级文本条件（通过预测场景描述）相结合。在推理时，给定单个图像和指定对象速度，模型利用SAM（Segment Anything Model）提取对象掩码，并使用GPT-5推断未来场景动态的文本描述，从而生成具有推断运动和未来对象交互的视频。</li>
</ul>
<p><strong>3. 主要结果及其重要性：</strong>
*   <strong>显著的性能提升：</strong> KineMask 在合成场景（简单交互）和真实场景（复杂交互）中均表现出对对象交互的显著改进，超越了同等规模的最新模型（如CogVideoX、Wan和Force Prompting）。
*   <strong>物理真实性和因果效应：</strong> 实验证明，KineMask 能够生成物理上真实的碰撞、因果效应（例如，物体速度增加导致碰撞后第二个物体移动更远）以及复杂效果（如玻璃破碎或液体溅出）。这表明模型能够捕捉运动的因果结构，对于世界建模和知情规划具有重要价值。
*   <strong>数据和文本条件的影响：</strong>
    *   <strong>训练数据：</strong> 训练在包含对象交互的合成数据集上（Interactions数据集）的模型，比仅在简单运动数据集上训练的模型，在生成复杂对象交互方面表现更优，并能泛化到真实世界图像。
    *   <strong>文本条件：</strong> 丰富的文本描述（c）在训练时能够帮助模型利用VDM的先验知识，生成超出训练数据范围的复杂交互效果，例如花瓶破碎或水波形成。消融研究强调了低级和高级条件在VDMs中互补作用的重要性。
*   <strong>用户研究：</strong> 用户研究结果表明，KineMask 在运动保真度、交互质量和整体物理一致性方面显著优于基线模型。</p>
<p><strong>4. 论文中提到的局限性：</strong>
*   <strong>低级控制的局限性：</strong> KineMask 的低级控制目前仅限于速度，而真实世界的运动还取决于摩擦力、形状、质量和空气阻力等因素。
*   <strong>失败案例：</strong>
    *   对于高度不明显的物体，模型可能在运动过程中忽略其他物体，导致碰撞未能发生。
    *   在包含多个物体的复杂场景中，有时会出现歧义，导致物体重复或消失。作者推测文本提示可能也加剧了这种歧义。
    *   当控制应用于具有薄支撑结构或堆叠物体时，Force Prompting等基线方法会遇到困难，而KineMask通过基于掩码的控制机制能够正确处理这些边缘情况。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>整合更全面的物理控制：</strong> 将摩擦力、形状、质量和空气阻力等更多物理因素纳入VDM生成的运动控制中，以提高物理准确性。
*   <strong>改进复杂场景处理：</strong> 解决多物体复杂场景中的歧义问题，减少物体重复或消失的失败案例。
*   <strong>多模态语言模型与物理推理的结合：</strong> 进一步探索多模态语言模型在文本基础物理推理方面的进展，以补充视频生成，从而实现更强大的物理世界建模。
*   <strong>扩展到更广泛的交互类型：</strong> 将KineMask的能力扩展到处理除了刚体交互之外的更广泛的物理现象。</p>
<hr />
<p>总而言之，KineMask通过其创新的两阶段训练策略和低级运动控制与高级文本条件的结合，为物理引导的视频生成领域带来了重大突破。它不仅显著提高了生成视频中对象交互的真实性和物理一致性，还为构建更可靠的世界模型和支持机器人技术、具身决策等应用奠定了基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this
limitation, we introduce KineMask, an approach for physics-guided video
generation that enables realistic rigid body control, interactions, and
effects.</li>
<li>Given a single image and a specified object velocity, our method
generates videos with inferred motions and future object interactions.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.02284v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.02284v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.02283v1'></a></p>
<h2 id="self-forcing-towards-minute-scale-high-quality-video-generation"><a href="https://arxiv.org/abs/2510.02283v1">Self-Forcing++: Towards Minute-Scale High-Quality Video Generation</a></h2>
<p><strong>Authors:</strong> Justin Cui, Jie Wu, Ming Li, Tao Yang, Xiaojie Li, Rui Wang, Andrew Bai, Yuanhao Ban, Cho-Jui Hsieh</p>
<p><strong>Published:</strong> 2025-10-02</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>Diffusion models have revolutionized image and video generation, achieving
unprecedented visual quality. However, their reliance on transformer
architectures incurs prohibitively high computational costs, particularly when
extending generation to long videos. Recent work has explored autoregressive
formulations for long video generation, typically by distilling from
short-horizon bidirectional teachers. Nevertheless, given that teacher models
cannot synthesize long videos, the extrapolation of student models beyond their
training horizon often leads to pronounced quality degradation, arising from
the compounding of errors within the continuous latent space. In this paper, we
propose a simple yet effective approach to mitigate quality degradation in
long-horizon video generation without requiring supervision from long-video
teachers or retraining on long video datasets. Our approach centers on
exploiting the rich knowledge of teacher models to provide guidance for the
student model through sampled segments drawn from self-generated long videos.
Our method maintains temporal consistency while scaling video length by up to
20x beyond teacher's capability, avoiding common issues such as over-exposure
and error-accumulation without recomputing overlapping frames like previous
methods. When scaling up the computation, our method shows the capability of
generating videos up to 4 minutes and 15 seconds, equivalent to 99.9% of the
maximum span supported by our base model's position embedding and more than 50x
longer than that of our baseline model. Experiments on standard benchmarks and
our proposed improved benchmark demonstrate that our approach substantially
outperforms baseline methods in both fidelity and consistency. Our long-horizon
videos demo can be found at https://self-forcing-plus-plus.github.io/</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于“Self-Forcing++: Towards Minute-Scale High-Quality Video Generation”论文的全面摘要，内容涵盖了您要求的所有要点：</p>
<p><strong>论文摘要：Self-Forcing++：迈向分钟级高质量视频生成</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决扩散模型在生成长视频时面临的质量退化问题。尽管扩散模型在图像和短视频生成方面取得了前所未有的成功，但由于其对Transformer架构的依赖，将生成扩展到长视频会带来高昂的计算成本。现有方法通常通过从短时程双向教师模型中提取知识来生成长视频，但由于教师模型无法合成长视频，学生模型超出其训练范围的推断往往会导致连续潜在空间中错误累积，从而造成显著的质量退化。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
Self-Forcing++ 提出了一种简单而有效的方法来缓解长时程视频生成中的质量退化，而无需长视频教师的监督或在长视频数据集上重新训练。其核心创新在于：
*   <strong>利用教师模型知识指导学生模型：</strong> 通过从自生成的长视频中采样的片段，利用教师模型的丰富知识来指导学生模型。
*   <strong>反向噪声初始化（Backwards Noise Initialization）：</strong> 引入反向噪声初始化策略，通过将噪声重新注入去噪后的潜在向量作为起始噪声，以确保生成轨迹保持时间一致性，并正确结构化。这有助于缓解长视频生成中的上下文错位问题。
*   <strong>扩展分布匹配蒸馏（Extended Distribution Matching Distillation）：</strong> 将训练持续时间扩展到远超教师模型限制的帧数（例如，100秒），并在生成的序列中均匀采样一个固定长度的窗口，计算学生和教师模型之间的分布差异。这种滑动窗口蒸馏过程使学生模型能够从退化状态中恢复，并维持高质量、连贯的视频生成。
*   <strong>滚动KV缓存训练：</strong> 在训练和推理过程中都采用滚动KV缓存，自然消除了训练-推理不匹配问题，避免了传统方法中重叠帧的重新计算或潜在帧掩蔽。
*   <strong>通过GRPO改进长期平滑性：</strong> 当出现时间不一致（如物体突然出现或消失、场景过渡不自然）时，利用强化学习技术——群组相对策略优化（GRPO）来增强时间平滑性，通过光学流的相对大小作为运动连续性的代理来指导优化。
*   <strong>新的评估指标（Visual Stability）：</strong> 提出了一种新的指标“视觉稳定性”，旨在系统地捕捉长视频生成中的质量退化和过度曝光问题，以解决现有VBench基准的偏差。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>显著的视频长度扩展：</strong> Self-Forcing++ 能够将视频长度扩展到教师模型能力的20倍以上，最长可达4分钟15秒，相当于其基础模型位置嵌入支持的最大范围的99.9%，比基线模型长50倍以上。
*   <strong>卓越的性能：</strong> 在标准基准和提出的改进基准上，该方法在保真度和一致性方面均显著优于基线方法。
*   <strong>保持时间一致性：</strong> 在扩展视频长度的同时，避免了过度曝光和错误累积等常见问题，无需像以前的方法那样重新计算重叠帧。
*   <strong>训练预算扩展能力：</strong> 实验表明，通过扩展训练预算，模型能够生成高质量、长持续时间的视频，从而规避了对大规模真实视频数据集的依赖。</p>
<p><strong>4. 论文中提到的局限性：</strong>
*   <strong>训练速度较慢：</strong> 相较于教师强制（teacher-forcing）方法，Self-Forcing++ 的训练速度较慢。
*   <strong>缺乏长期记忆：</strong> 尽管有所改进，但仍存在长期记忆不足的问题，可能导致在长时间遮挡区域出现内容分歧。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>并行化训练过程：</strong> 探索并行化训练过程以解决自生成（self-rollout）的高训练成本问题。
*   <strong>控制潜在向量的保真度：</strong> 研究控制潜在向量保真度的技术，例如量化存储在KV缓存中的潜在表示，或对KV缓存进行归一化以防止分布偏移。
*   <strong>整合长期记忆机制：</strong> 将长期记忆机制整合到自回归框架中，以实现真正的长距离时间连贯性。</p>
<p>总而言之，Self-Forcing++ 为高质量长视频生成提供了一个创新且实用的解决方案，通过巧妙地利用教师模型的知识和自生成过程中的错误校正，显著扩展了视频生成的时间范围，并提升了生成内容的质量和一致性。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Our approach centers on
exploiting the rich knowledge of teacher models to provide guidance for the
student model through sampled segments drawn from self-generated long videos.</li>
<li>Our method maintains temporal consistency while scaling video length by up to
20x beyond teacher's capability, avoiding common issues such as over-exposure
and error-accumulation without recomputing overlapping frames like previous
methods.</li>
<li>When scaling up the computation, our method shows the capability of
generating videos up to 4 minutes and 15 seconds, equivalent to 99.9% of the
maximum span supported by our base model's position embedding and more than 50x
longer than that of our baseline model.</li>
<li>Experiments on standard benchmarks and
our proposed improved benchmark demonstrate that our approach substantially
outperforms baseline methods in both fidelity and consistency.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.02283v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.02283v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.02282v1'></a></p>
<h2 id="vidguard-r1-ai-generated-video-detection-and-explanation-via-reasoning-mllms-and-rl"><a href="https://arxiv.org/abs/2510.02282v1">VidGuard-R1: AI-Generated Video Detection and Explanation via Reasoning MLLMs and RL</a></h2>
<p><strong>Authors:</strong> Kyoungjun Park, Yifan Yang, Juheon Yi, Shicheng Zheng, Yifei Shen, Dongqi Han, Caihua Shan, Muhammad Muaz, Lili Qiu</p>
<p><strong>Published:</strong> 2025-10-02</p>
<p><strong>Categories:</strong> cs.CV, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>With the rapid advancement of AI-generated videos, there is an urgent need
for effective detection tools to mitigate societal risks such as misinformation
and reputational harm. In addition to accurate classification, it is essential
that detection models provide interpretable explanations to ensure transparency
for regulators and end users. To address these challenges, we introduce
VidGuard-R1, the first video authenticity detector that fine-tunes a
multi-modal large language model (MLLM) using group relative policy
optimization (GRPO). Our model delivers both highly accurate judgments and
insightful reasoning. We curate a challenging dataset of 140k real and
AI-generated videos produced by state-of-the-art generation models, carefully
designing the generation process to maximize discrimination difficulty. We then
fine-tune Qwen-VL using GRPO with two specialized reward models that target
temporal artifacts and generation complexity. Extensive experiments demonstrate
that VidGuard-R1 achieves state-of-the-art zero-shot performance on existing
benchmarks, with additional training pushing accuracy above 95%. Case studies
further show that VidGuard-R1 produces precise and interpretable rationales
behind its predictions. The code is publicly available at
https://VidGuard-R1.github.io.</p>
<p><strong>Analysis:</strong></p>
<p>以下是Kyoungjun Park等人撰写的论文“VidGuard-R1: AI-Generated Video Detection and Explanation via Reasoning MLLMs and RL”的全面摘要：</p>
<p><strong>1. 主要问题或研究问题</strong>
随着AI生成视频的快速发展，社会面临着虚假信息和声誉损害等风险。因此，迫切需要有效的检测工具来识别AI生成视频。除了准确分类，这些检测模型还需要提供可解释的解释，以确保监管机构和最终用户的透明度。现有方法在检测最新生成模型产生的视频时表现不佳，且通常只提供二元判断，缺乏可解释的推理能力。</p>
<p><strong>2. 主要创新或方法论贡献</strong>
VidGuard-R1是首个通过推理多模态大型语言模型（MLLMs）和强化学习（RL）实现AI生成视频检测和解释的框架。其主要创新包括：
*   <strong>首个基于MLLM的视频真实性检测器</strong>：VidGuard-R1通过使用群组相对策略优化（GRPO）对MLLM进行微调，实现了高精度判断和富有洞察力的推理。
*   <strong>挑战性数据集的构建</strong>：作者精心策划了一个包含14万真实和AI生成视频的挑战性数据集，这些视频由最先进的生成模型生成，并经过精心设计以最大化区分难度，避免模型依赖分辨率、帧率等表面线索。
*   <strong>专门的奖励模型</strong>：为了增强性能，VidGuard-R1在GRPO微调Qwen-VL时引入了两个专门的奖励模型：
    *   <strong>GRPO-TA（Temporal Artifacts）</strong>：通过引入时间伪影（重复或反转视频片段）并根据检测难度调整奖励，显式促进时间推理。
    *   <strong>GRPO-Q（Quality Evolutionary Videos）</strong>：通过系统地改变逆向扩散步数来生成不同质量的视频，并根据预测与真实扩散步数的距离分配部分奖励，使模型能够进行细粒度的视频质量分析。
*   <strong>两阶段训练框架</strong>：首先进行监督微调（SFT）以初始化链式思考（CoT）能力，然后进行基于强化学习的微调（DPO和GRPO）以实现更深层次的推理。</p>
<p><strong>3. 主要结果及其意义</strong>
*   <strong>卓越的检测性能</strong>：VidGuard-R1在现有基准测试上实现了最先进的零样本性能，准确率超过95%。经过额外训练后，其准确率进一步提升。在作者构建的挑战性数据集上，VidGuard-R1的准确率达到约85%。
*   <strong>可解释的推理</strong>：案例研究表明，VidGuard-R1能够为其预测提供精确且可解释的理由，通过多方面推理（运动一致性、光照一致性、纹理伪影和物理合理性）来识别AI生成视频，而非依赖单一线索。
*   <strong>泛化能力强</strong>：零样本模型在GenVidBench上表现出强大的泛化能力，突显了在多样化生成内容上进行预训练的有效性。</p>
<p><strong>4. 论文中提及的局限性</strong>
*   <strong>数据集局限性</strong>：目前的数据集仅包含由HunyuanVideo和CogVideoX生成的虚假视频。尽管数据集采用配对方式构建以确保真实和虚假视频之间的上下文相似性，但其泛化能力仍有限。
*   <strong>生成模型范围</strong>：数据集未能涵盖更广泛的生成模型，这限制了模型在现实世界场景中的适用性。</p>
<p><strong>5. 潜在的未来研究方向</strong>
*   <strong>扩展数据集</strong>：将数据集扩展到包含更广泛的生成模型所产生的虚假视频，以构建更具多样性和鲁棒性的训练集，从而提高模型在现实世界场景中的适用性。
*   <strong>增强MLLM推理能力</strong>：进一步研究如何通过强化学习等方法加强MLLMs的推理能力，以应对更复杂、更细微的AI生成视频检测挑战。
*   <strong>多模态视频分析</strong>：继续推动基于MLLM的视频分析，以更好地理解和解释AI生成视频的复杂特征。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address these challenges, we introduce
VidGuard-R1, the first video authenticity detector that fine-tunes a
multi-modal large language model (MLLM) using group relative policy
optimization (GRPO).</li>
<li>We curate a challenging dataset of 140k real and
AI-generated videos produced by state-of-the-art generation models, carefully
designing the generation process to maximize discrimination difficulty.</li>
<li>Extensive experiments demonstrate
that VidGuard-R1 achieves state-of-the-art zero-shot performance on existing
benchmarks, with additional training pushing accuracy above 95%.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.02282v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.02282v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.02270v1'></a></p>
<h2 id="microclip-unsupervised-clip-adaptation-via-coarse-fine-token-fusion-for-fine-grained-image-classification"><a href="https://arxiv.org/abs/2510.02270v1">microCLIP: Unsupervised CLIP Adaptation via Coarse-Fine Token Fusion for Fine-Grained Image Classification</a></h2>
<p><strong>Authors:</strong> Sathira Silva, Eman Ali, Chetan Arora, Muhammad Haris Khan</p>
<p><strong>Published:</strong> 2025-10-02</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>Unsupervised adaptation of CLIP-based vision-language models (VLMs) for
fine-grained image classification requires sensitivity to microscopic local
cues. While CLIP exhibits strong zero-shot transfer, its reliance on coarse
global features restricts its performance on fine-grained classification tasks.
Prior efforts inject fine-grained knowledge by aligning large language model
(LLM) descriptions with the CLIP <script type="math/tex">\texttt{[CLS]}</script> token; however, this approach
overlooks spatial precision. We propose <script type="math/tex">\textbf{microCLIP}</script>, a self-training
framework that jointly refines CLIP's visual and textual representations using
fine-grained cues. At its core is Saliency-Oriented Attention Pooling (SOAP)
within a lightweight TokenFusion module, which builds a saliency-guided
<script type="math/tex">\texttt{[FG]}</script> token from patch embeddings and fuses it with the global
<script type="math/tex">\texttt{[CLS]}</script> token for coarse-fine alignment. To stabilize adaptation, we
introduce a two-headed LLM-derived classifier: a frozen classifier that, via
multi-view alignment, provides a stable text-based prior for pseudo-labeling,
and a learnable classifier initialized from LLM descriptions and fine-tuned
with TokenFusion. We further develop Dynamic Knowledge Aggregation, which
convexly combines fixed LLM/CLIP priors with TokenFusion's evolving logits to
iteratively refine pseudo-labels. Together, these components uncover latent
fine-grained signals in CLIP, yielding a consistent <script type="math/tex">2.90\%</script> average accuracy
gain across 13 fine-grained benchmarks while requiring only light adaptation.
Our code is available at https://github.com/sathiiii/microCLIP.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于Sathira Silva, Eman Ali, Chetan Arora, Muhammad Haris Khan撰写的论文“microCLIP: Unsupervised CLIP Adaptation via Coarse-Fine Token Fusion for Fine-Grained Image Classification”的全面摘要：</p>
<p><strong>论文摘要：microCLIP：通过粗细粒度Token融合实现无监督CLIP适应，用于细粒度图像分类</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决CLIP（Contrastive Language-Image Pre-training）模型在细粒度图像分类任务中进行无监督适应（Unsupervised Adaptation, UA）时面临的挑战。尽管CLIP在零样本迁移方面表现出色，但其主要依赖粗粒度的全局特征（通常是<code>[CLS]</code> token）限制了其捕捉细微局部线索的能力，而这些线索对于区分高度相似的细粒度类别至关重要。现有方法尝试通过将大型语言模型（LLM）描述与<code>[CLS]</code> token对齐来注入细粒度知识，但往往忽略了空间精度。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
microCLIP提出了一个自训练框架，通过以下关键创新共同优化CLIP的视觉和文本表示：</p>
<ul>
<li><strong>Saliency-Oriented Attention Pooling (SOAP) 和 TokenFusion 模块：</strong> 这是microCLIP的核心。SOAP机制在一个轻量级的TokenFusion模块内，通过对CLIP的patch embeddings进行显著性引导的注意力池化，生成一个紧凑的<code>[FG]</code>（fine-grained）token。然后，这个<code>[FG]</code> token与全局<code>[CLS]</code> token融合，实现粗细粒度对齐，从而在保留全局上下文的同时，注入局部、细粒度的视觉信息。</li>
<li><strong>双头LLM派生分类器：</strong> 为了稳定适应过程，microCLIP引入了一个双头分类器：<ul>
<li>一个<strong>冻结的分类器</strong>（<script type="math/tex">W_{LLM}</script>），通过多视图对齐提供稳定的基于文本的先验知识，用于伪标签生成。</li>
<li>一个<strong>可学习的分类器</strong>（<script type="math/tex">W_{LLM}^*</script>），它从LLM描述初始化，并与TokenFusion模块一起进行微调。</li>
</ul>
</li>
<li><strong>动态知识聚合（Dynamic Knowledge Aggregation）：</strong> 论文开发了一种迭代的伪标签方案，它凸组合了固定的LLM/CLIP先验知识（通过多视图对齐获得）与TokenFusion模块不断演进的logits，从而稳定而自适应地细化伪标签，以实现细粒度区分。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong>
microCLIP在13个细粒度基准测试中取得了显著的性能提升，平均准确率提高了2.90%，同时仅需要轻量级的适应。具体来说：</p>
<ul>
<li><strong>细粒度性能提升：</strong> microCLIP在Cars、RESISC、UCF101、CIFAR100和DTD等数据集上均实现了显著的准确率提升，尤其是在FGVC数据集上，其性能提升了+2.64%，Cars数据集上提升了+8.98%。</li>
<li><strong>显著性引导的注意力：</strong> 论文通过可视化展示，SOAP机制能够一致地突出类别定义的局部语义区域（例如，鸟类的特定身体部位、棒球场的内场布局），这表明其有效捕捉了细粒度线索。</li>
<li><strong>超越现有方法：</strong> microCLIP在ViT-B/32骨干网络上，整体准确率达到68.68%，超越了最强的无监督适应方法DPA，并优于零样本和少样本基线。在使用ViT-B/16骨干网络时，microCLIP相对于DPA的性能提升更是高达5.36%。</li>
<li><strong>轻量级适应：</strong> 该方法仅需轻量级适应，证明了其在实际应用中的高效性。</li>
</ul>
<p><strong>4. 论文中提及的局限性：</strong>
论文指出microCLIP在某些场景下存在局限性，特别是在需要局部和全局信息之间进行仔细平衡时：</p>
<ul>
<li><strong>对粗粒度、空间扩散特征的依赖：</strong> 当数据集主要包含粗粒度、空间扩散的特征，而非局部、细粒度线索时（例如DTD和Flowers数据集），microCLIP的伪标签生成器完全依赖于模型自身进行微调，这可能导致性能受限。在这种情况下，细粒度与粗粒度token预测的对称融合可能会无意中引入局部空间偏差，与数据的整体结构对齐不佳。</li>
<li><strong>融合策略的灵活性：</strong> 论文发现，在某些数据集上，DPA（一种基于原型对齐的方法）在配备可学习的GPT-3派生分类器时，性能与microCLIP相当甚至超越。这表明TokenFusion模块可能受益于更灵活的融合策略，例如粗粒度与细粒度预测之间的自适应加权机制，而非当前的对称融合方案。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong>
基于上述局限性，论文提出了以下未来研究方向：</p>
<ul>
<li><strong>自适应加权融合机制：</strong> 探索一种更灵活的融合策略，例如在粗粒度与细粒度预测之间引入自适应加权机制，以更好地平衡局部和全局信息，尤其是在特征空间分布广泛的数据集上。</li>
<li><strong>结合图像原型：</strong> 考虑在分类器构建中明确利用图像原型，类似于DPA，这可能有助于进一步提升性能，特别是在MetaCLIP等预训练数据质量高且规模大的情况下。</li>
</ul>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose <script type="math/tex">\textbf{microCLIP}</script>, a self-training
framework that jointly refines CLIP's visual and textual representations using
fine-grained cues.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.02270v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.02270v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.02268v1'></a></p>
<h2 id="do-you-know-where-your-camera-is-view-invariant-policy-learning-with-camera-conditioning"><a href="https://arxiv.org/abs/2510.02268v1">Do You Know Where Your Camera Is? View-Invariant Policy Learning with Camera Conditioning</a></h2>
<p><strong>Authors:</strong> Tianchong Jiang, Jingtian Ji, Xiangshan Tan, Jiading Fang, Anand Bhattad, Vitor Guizilini, Matthew R. Walter</p>
<p><strong>Published:</strong> 2025-10-02</p>
<p><strong>Categories:</strong> cs.RO, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We study view-invariant imitation learning by explicitly conditioning
policies on camera extrinsics. Using Plucker embeddings of per-pixel rays, we
show that conditioning on extrinsics significantly improves generalization
across viewpoints for standard behavior cloning policies, including ACT,
Diffusion Policy, and SmolVLA. To evaluate policy robustness under realistic
viewpoint shifts, we introduce six manipulation tasks in RoboSuite and
ManiSkill that pair "fixed" and "randomized" scene variants, decoupling
background cues from camera pose. Our analysis reveals that policies without
extrinsics often infer camera pose using visual cues from static backgrounds in
fixed scenes; this shortcut collapses when workspace geometry or camera
placement shifts. Conditioning on extrinsics restores performance and yields
robust RGB-only control without depth. We release the tasks, demonstrations,
and code at https://ripl.github.io/know_your_camera/ .</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于“Do You Know Where Your Camera Is? View-Invariant Policy Learning with Camera Conditioning”论文的中文摘要，由Tianchong Jiang等人撰写：</p>
<p><strong>论文摘要：你知道你的相机在哪里吗？基于相机条件化的视角不变策略学习</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决机器人模仿学习中的一个核心挑战：如何使机器人策略对相机视角的改变具有鲁棒性，即实现“视角不变性”。传统的模仿学习策略通常在固定视角下训练，当相机位置在部署时发生变化时，这些策略往往会失效。论文的核心问题是：机器人策略是应该在训练过程中隐式地推断相机几何信息，还是应该在相机几何信息可用时显式地对其进行条件化？</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
*   <strong>新颖的相机条件化方法：</strong> 论文提出了一种有效的方法，通过使用像素级Plücker嵌入（per-pixel Plücker embeddings）来显式地将相机几何信息作为条件引入到模仿学习策略中。Plücker坐标用于表示空间中的有向线，具有对沿射线原点选择不变的优势。
*   <strong>两种编码Plücker射线图的方式：</strong> 针对是否使用预训练视觉编码器，论文提出了两种集成Plücker射线图的方法：
    *   对于没有预训练编码器的策略（如Diffusion Policy），Plücker图与图像进行通道级联，并修改图像编码器的第一层以接受9通道输入。
    *   对于带有预训练编码器的策略，使用一个小型卷积网络将Plücker图编码到与图像潜在特征相同的维度，然后进行通道级联。
*   <strong>新的视角泛化基准：</strong> 论文引入了六个新的操作任务（RoboSuite中的Lift, Pick Place Can, Assembly Square和ManiSkill中的Push, Lift Upright, Roll Ball），每个任务都有“固定”和“随机化”两种场景变体。这些变体旨在解耦背景视觉线索与相机姿态，以更真实地评估策略在视角变化下的鲁棒性。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>显著的泛化性能提升：</strong> 实验结果表明，对相机外参进行条件化处理，显著提高了ACT、Diffusion Policy和SmolVLA等标准行为克隆策略在不同视角下的泛化能力。
*   <strong>揭示了策略的“捷径”问题：</strong> 分析发现，在固定场景中，没有外参条件化的策略通常会利用静态背景的视觉线索来推断相机姿态。当工作空间几何或相机位置发生变化时，这种“捷径”会失效，导致策略性能崩溃。
*   <strong>恢复性能和鲁棒的RGB-only控制：</strong> 显式地对相机外参进行条件化处理，能够恢复策略性能，并实现仅依赖RGB图像的鲁棒控制，而无需深度信息。
*   <strong>其他发现：</strong>
    *   随机裁剪（Random Cropping）能一致性地提高所有任务的性能，这被解释为有效地增加了具有不同参数的虚拟相机。
    *   Delta End-Effector Pose作为动作空间时，策略表现最佳。
    *   在大多数情况下，晚期融合（late fusion）比早期融合（early fusion）更有效，这可能是因为将Plücker图与图像直接级联会使预训练的ResNet输入分布发生变化。
    *   预训练视觉编码器对成功率的影响不大。
    *   在训练数据量（相机视角数量）增加时，相机姿态条件化仍然能提供持续的性能提升，并且达到相同性能所需的相机数量远少于没有条件化的策略。
*   <strong>真实机器人实验验证：</strong> 在真实UR5机器人上进行的实验也验证了相机姿态条件化对所有策略的性能提升。</p>
<p><strong>4. 论文中提到的局限性：</strong>
*   <strong>姿态估计方法的局限性：</strong> 论文承认当前的姿态估计方法（如AprilTags）仍不完善，尤其是在特征较少的表面、有限的视角重叠或高度动态的场景中。在这种情况下，姿态估计误差可能会与下游控制任务复合，如果未采取缓解策略，可能导致性能下降。
*   <strong>关注点：</strong> 论文主要关注姿态条件化，而其他方向（如泛化到具有不同内参的相机）仍未探索。
*   <strong>Diffusion Policy的性能：</strong> 论文观察到Diffusion Policy的性能显著低于ACT，作者认为这部分是由于其在需要精度的任务中具有随机性，当任务可以通过多条轨迹完成时，Diffusion Policy常在这些轨迹之间振荡，可能导致失败。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>解决姿态估计误差：</strong> 随着广义姿态估计的快速发展，姿态估计的子问题有望在端到端机器人策略学习的更广泛挑战解决之前得到解决。
*   <strong>泛化到不同内参的相机：</strong> 论文方法已能使策略泛化到新的相机姿态，自然地可以扩展到支持泛化到具有不同内参的相机。
*   <strong>探索其他条件化方法：</strong> 除了姿态条件化，还有许多其他方向值得探索。
*   <strong>利用新基准：</strong> 论文发布的六个新的RoboSuite和ManiSkill基准为推进视角鲁棒模仿学习提供了实用的评估工具。
*   <strong>行动空间设计和数据增强：</strong> 论文的发现强调了行动空间设计和数据增强对策略训练的关键考虑因素。</p>
<p>总而言之，这篇论文通过显式地将相机几何信息（使用Plücker嵌入）作为条件引入模仿学习策略，有效地解决了机器人策略在不同视角下泛化能力差的问题。它不仅提高了现有策略的性能，还揭示了策略可能利用背景视觉线索的“捷径”问题，并为未来的视角不变机器人学习研究提供了新的基准和方法论见解。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To evaluate policy robustness under realistic
viewpoint shifts, we introduce six manipulation tasks in RoboSuite and
ManiSkill that pair "fixed" and "randomized" scene variants, decoupling
background cues from camera pose.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.02268v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.02268v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.02262v1'></a></p>
<h2 id="from-frames-to-clips-efficient-key-clip-selection-for-long-form-video-understanding"><a href="https://arxiv.org/abs/2510.02262v1">From Frames to Clips: Efficient Key Clip Selection for Long-Form Video Understanding</a></h2>
<p><strong>Authors:</strong> Guangyu Sun, Archit Singhal, Burak Uzkent, Mubarak Shah, Chen Chen, Garin Kessler</p>
<p><strong>Published:</strong> 2025-10-02</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Video Large Language Models (VLMs) have achieved remarkable results on a
variety of vision language tasks, yet their practical use is limited by the
"needle in a haystack" problem: the massive number of visual tokens produced
from raw video frames exhausts the model's context window. Existing solutions
alleviate this issue by selecting a sparse set of frames, thereby reducing
token count, but such frame-wise selection discards essential temporal
dynamics, leading to suboptimal reasoning about motion and event continuity. In
this work we systematically explore the impact of temporal information and
demonstrate that extending selection from isolated key frames to key clips,
which are short, temporally coherent segments, improves video understanding. To
maintain a fixed computational budget while accommodating the larger token
footprint of clips, we propose an adaptive resolution strategy that dynamically
balances spatial resolution and clip length, ensuring a constant token count
per video. Experiments on three long-form video benchmarks demonstrate that our
training-free approach, F2C, outperforms uniform sampling up to 8.1%, 5.6%, and
10.3% on Video-MME, LongVideoBench and MLVU benchmarks, respectively. These
results highlight the importance of preserving temporal coherence in frame
selection and provide a practical pathway for scaling Video LLMs to real world
video understanding applications. Project webpage is available at
https://guangyusun.com/f2c .</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于Guangyu Sun, Archit Singhal, Burak Uzkent, Mubarak Shah, Chen Chen, Garin Kessler撰写的论文“From Frames to Clips: Efficient Key Clip Selection for Long-Form Video Understanding”的全面摘要：</p>
<p><strong>论文摘要：从帧到片段：长视频理解的高效关键片段选择</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
视频大型语言模型（VLMs）在各种视觉语言任务中表现出色，但其在处理长视频时的实际应用受到“大海捞针”问题的限制。原始视频帧产生的大量视觉token会迅速耗尽模型的上下文窗口。现有解决方案通常通过稀疏采样帧来减少token数量，但这会丢弃重要的时间动态，导致在推理运动和事件连续性方面表现不佳。因此，核心问题是如何在保持计算预算不变的情况下，有效地从长视频中选择既具有语义相关性又保留时间连续性的视觉信息，以提高VLMs的视频理解能力。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
本文提出了一个名为<strong>Frames-to-Clips (F2C)</strong>的免训练框架，用于高效的关键片段选择，其主要创新点包括：</p>
<ul>
<li><strong>从关键帧到关键片段的扩展：</strong> 论文系统地探索了时间信息的影响，并证明将选择范围从孤立的关键帧扩展到短的、时间连贯的“关键片段”可以显著改善视频理解。</li>
<li><strong>自适应分辨率策略：</strong> 为了在固定计算预算下适应更大token足迹的片段，F2C提出了一种自适应分辨率策略。该策略动态平衡空间分辨率和片段长度，确保每个视频的token数量恒定。这意味着可以通过降低空间分辨率来包含更长的片段，反之亦然，从而实现时空平衡。</li>
<li><strong>锚点关键帧选择：</strong> F2C首先通过结合相关性（使用预训练的对比视觉-语言编码器计算帧与文本查询的余弦相似度）和多样性（通过局部极大值识别和K-means聚类）来选择初始的锚点关键帧。</li>
<li><strong>片段特定选择：</strong> 针对每个锚点关键帧，F2C通过优化目标函数来确定其最佳片段长度。该目标函数综合考虑了片段内帧的平均余弦相似度（相关性）、帧之间的平均成对相似度（冗余度）以及与相对片段长度成比例的时间奖励。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong>
F2C在三个长视频基准测试（Video-MME、LongVideoBench和MLVU）上的实验结果表明：</p>
<ul>
<li><strong>显著优于均匀采样：</strong> F2C在Video-MME、LongVideoBench和MLVU基准测试上分别比均匀采样高出8.1%、5.6%和10.3%。这突出了保留时间连贯性在帧选择中的重要性。</li>
<li><strong>对不同问题类型的普遍有效性：</strong> F2C在Video-MME的六种问题类别（计数、识别、推理、感知、OCR、信息概要）中均取得了持续改进，尤其在需要丰富运动和上下文线索的计数、识别和推理任务中表现突出。</li>
<li><strong>计算效率：</strong> 尽管增加了时间上下文，F2C通过避免重复编码重叠帧，实现了与基于帧的选择器相似或更少的token数量，尤其在预算增加时，token数量的减少更为显著。这表明F2C在准确性和计算效率之间取得了良好平衡。</li>
<li><strong>锚点选择和多样性的重要性：</strong> 实验表明，锚点关键帧的质量至关重要，多样性高的选择器（如Watershed和BOLT）在F2C增强后表现更好。同时，适中的锚点数量（Kanchor = K）能在空间细节和时间覆盖之间取得最佳平衡。</li>
</ul>
<p>这些结果共同强调了在帧选择中保留时间连贯性的重要性，并为将VLMs扩展到实际世界的视频理解应用提供了一条实用途径。</p>
<p><strong>4. 论文中提及的局限性：</strong>
尽管F2C通过提供时间连贯和分辨率自适应的输入有效改善了上下文管理，但其性能上限仍受限于下游VLM的能力。即使选择了正确的帧或片段，最终答案也取决于VLM本身的推理和理解能力。因此，骨干模型在时间推理、空间理解或多模态对齐方面的局限性仍然是瓶颈。F2C方法是辅助性的，其优势可能会随着更强大的骨干模型出现而进一步放大。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
论文指出，F2C为长视频理解提供了一个简单且可扩展的解决方案，并为VLMs的未来发展指明了方向。未来的研究可以探索如何进一步优化F2C与更强大的骨干VLM的结合，以及如何将F2C的自适应策略应用于其他多模态任务，以进一步提升性能。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To
maintain a fixed computational budget while accommodating the larger token
footprint of clips, we propose an adaptive resolution strategy that dynamically
balances spatial resolution and clip length, ensuring a constant token count
per video.</li>
<li>Experiments on three long-form video benchmarks demonstrate that our
training-free approach, F2C, outperforms uniform sampling up to 8.1%, 5.6%, and
10.3% on Video-MME, LongVideoBench and MLVU benchmarks, respectively.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.02262v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.02262v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.02253v1'></a></p>
<h2 id="dragflow-unleashing-dit-priors-with-region-based-supervision-for-drag-editing"><a href="https://arxiv.org/abs/2510.02253v1">DragFlow: Unleashing DiT Priors with Region Based Supervision for Drag Editing</a></h2>
<p><strong>Authors:</strong> Zihan Zhou, Shilin Lu, Shuli Leng, Shaocong Zhang, Zhuming Lian, Xinlei Yu, Adams Wai-Kin Kong</p>
<p><strong>Published:</strong> 2025-10-02</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Drag-based image editing has long suffered from distortions in the target
region, largely because the priors of earlier base models, Stable Diffusion,
are insufficient to project optimized latents back onto the natural image
manifold. With the shift from UNet-based DDPMs to more scalable DiT with flow
matching (e.g., SD3.5, FLUX), generative priors have become significantly
stronger, enabling advances across diverse editing tasks. However, drag-based
editing has yet to benefit from these stronger priors. This work proposes the
first framework to effectively harness FLUX's rich prior for drag-based
editing, dubbed DragFlow, achieving substantial gains over baselines. We first
show that directly applying point-based drag editing to DiTs performs poorly:
unlike the highly compressed features of UNets, DiT features are insufficiently
structured to provide reliable guidance for point-wise motion supervision. To
overcome this limitation, DragFlow introduces a region-based editing paradigm,
where affine transformations enable richer and more consistent feature
supervision. Additionally, we integrate pretrained open-domain personalization
adapters (e.g., IP-Adapter) to enhance subject consistency, while preserving
background fidelity through gradient mask-based hard constraints. Multimodal
large language models (MLLMs) are further employed to resolve task ambiguities.
For evaluation, we curate a novel Region-based Dragging benchmark (ReD Bench)
featuring region-level dragging instructions. Extensive experiments on
DragBench-DR and ReD Bench show that DragFlow surpasses both point-based and
region-based baselines, setting a new state-of-the-art in drag-based image
editing. Code and datasets will be publicly available upon publication.</p>
<p><strong>Analysis:</strong></p>
<p>以下是Zihan Zhou等人撰写的论文“DragFlow: Unleashing DiT Priors with Region Based Supervision for Drag Editing”的全面摘要：</p>
<p><strong>1. 主要问题或研究问题</strong>
拖拽式图像编辑长期以来存在目标区域失真问题，这主要是因为早期基础模型（如Stable Diffusion）的先验不足以将优化后的潜在表示投影回自然图像流形。尽管基于UNet的DDPMs已转向更具可扩展性的DiT模型（如FLUX），生成先验显著增强，但拖拽式编辑尚未充分利用这些更强的先验。直接将基于点的拖拽编辑应用于DiT模型效果不佳，因为DiT的特征结构不足以提供可靠的逐点运动监督。</p>
<p><strong>2. 关键创新或方法论贡献</strong>
DragFlow提出了一个新颖的区域级编辑框架，有效利用了FLUX强大的生成先验，并解决了DiT模型中的挑战：
*   <strong>区域级运动监督：</strong> 引入了基于区域的编辑范式，通过仿射变换实现更丰富、更一致的特征监督，克服了逐点监督的局限性。
*   <strong>背景保真度：</strong> 通过基于梯度掩码的硬约束来保持背景的忠实度，仅更新可编辑区域，取代了传统的背景一致性损失。
*   <strong>增强的主体一致性：</strong> 集成了预训练的开放域个性化适配器（如IP-Adapter），以增强主体一致性，同时通过适配器增强的反演机制缓解了DiT模型中常见的反演漂移问题。
*   <strong>多模态大语言模型（MLLM）的应用：</strong> 利用MLLM来解析任务歧义，更好地理解用户意图。
*   <strong>新基准数据集：</strong> 策劃了一个新的区域级拖拽基准（ReD Bench），包含区域级拖拽指令，用于评估方法。</p>
<p><strong>3. 主要结果及其重要性</strong>
DragFlow在DragBench-DR和ReD Bench上进行了广泛实验，结果表明它在拖拽式图像编辑方面超越了现有的基于点和基于区域的基线方法，在空间对应性、结构一致性和内容完整性方面取得了显著提升。它成功地利用了FLUX更强的生成先验，消除了以往方法在复杂场景中产生的失真，并实现了更强的可控性、更低的变形伪影和更高的图像保真度。</p>
<p><strong>4. 论文中提及的局限性</strong>
*   <strong>反演漂移：</strong> 由于FLUX模型是CFG-distilled变体，其反演漂移比非distilled模型更大。尽管DragFlow通过适配器增强的反演缓解了这个问题，但对于高度复杂的图像结构，重建过程中仍存在细节损失。
*   <strong>视觉质量下降：</strong> 由反演漂移导致的细节损失会影响拖拽编辑结果的视觉质量。</p>
<p><strong>5. 潜在的未来研究方向</strong>
未来的研究可以受益于开发更先进的技术或适配器架构，以进一步增强反演保真度，从而减少上述伪影并提升整体编辑性能。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>For evaluation, we curate a novel Region-based Dragging benchmark (ReD Bench)
featuring region-level dragging instructions.</li>
<li>Extensive experiments on
DragBench-DR and ReD Bench show that DragFlow surpasses both point-based and
region-based baselines, setting a new state-of-the-art in drag-based image
editing.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.02253v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.02253v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.02186v1'></a></p>
<h2 id="geopurify-a-data-efficient-geometric-distillation-framework-for-open-vocabulary-3d-segmentation"><a href="https://arxiv.org/abs/2510.02186v1">GeoPurify: A Data-Efficient Geometric Distillation Framework for Open-Vocabulary 3D Segmentation</a></h2>
<p><strong>Authors:</strong> Weijia Dou, Xu Zhang, Yi Bin, Jian Liu, Bo Peng, Guoqing Wang, Yang Yang, Heng Tao Shen</p>
<p><strong>Published:</strong> 2025-10-02</p>
<p><strong>Categories:</strong> cs.CV, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Recent attempts to transfer features from 2D Vision-Language Models (VLMs) to
3D semantic segmentation expose a persistent trade-off. Directly projecting 2D
features into 3D yields noisy and fragmented predictions, whereas enforcing
geometric coherence necessitates costly training pipelines and large-scale
annotated 3D data. We argue that this limitation stems from the dominant
segmentation-and-matching paradigm, which fails to reconcile 2D semantics with
3D geometric structure. The geometric cues are not eliminated during the
2D-to-3D transfer but remain latent within the noisy and view-aggregated
features. To exploit this property, we propose GeoPurify that applies a small
Student Affinity Network to purify 2D VLM-generated 3D point features using
geometric priors distilled from a 3D self-supervised teacher model. During
inference, we devise a Geometry-Guided Pooling module to further denoise the
point cloud and ensure the semantic and structural consistency. Benefiting from
latent geometric information and the learned affinity network, GeoPurify
effectively mitigates the trade-off and achieves superior data efficiency.
Extensive experiments on major 3D benchmarks demonstrate that GeoPurify
achieves or surpasses state-of-the-art performance while utilizing only about
1.5% of the training data. Our codes and checkpoints are available at
<a href="https://github.com/tj12323/GeoPurify">https://github.com/tj12323/GeoPurify</a>.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供对Weijia Dou等人撰写的论文“GeoPurify: A Data-Efficient Geometric Distillation Framework for Open-Vocabulary 3D Segmentation”的全面摘要。</p>
<hr />
<h3 id="geopurify-3d">GeoPurify: 一种用于开放词汇3D分割的数据高效几何蒸馏框架</h3>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决开放词汇3D语义分割领域中一个长期存在的权衡问题：如何将2D视觉-语言模型（VLMs）的丰富语义信息有效地迁移到3D点云，同时克服直接投影2D特征所导致的几何不一致性（如噪声和碎片化预测），而又不依赖于昂贵的大规模3D标注数据和复杂的训练流程。作者认为，现有方法（如“分割与匹配”范式）未能有效协调2D语义与3D几何结构，导致几何线索在2D到3D迁移中变得潜在而非显式。</p>
<p><strong>2. 关键创新或方法贡献：</strong>
GeoPurify框架的核心创新在于其“理解即分割”范式和数据高效的几何蒸馏机制，具体包括：</p>
<ul>
<li><strong>重新定义范式：</strong> 将开放词汇3D分割从传统的“分割与匹配”范式重新定义为“理解即分割”。通过利用通用的VLM（如X-Decoder）建立更丰富的统一视觉-语言嵌入空间，克服了传统“先定位后识别”方法在语义天花板上的限制。</li>
<li><strong>几何对比蒸馏（Geometric Contrastive Distillation）：</strong> 提出一个数据高效的框架，其核心是使用一个小型学生亲和网络（Student Affinity Network）来净化2D VLM生成的3D点特征。该学生网络通过知识蒸馏从一个冻结的3D自监督教师模型（如Sonata）中学习几何先验。学生网络的目标不是复制教师的特征，而是学习点之间的几何亲和力，从而在无标签的3D扫描上进行训练。</li>
<li><strong>几何引导池化（Geometry-Guided Pooling）：</strong> 在推理阶段，设计了一个几何引导池化模块，利用学习到的几何亲和力进一步去噪点云，并确保语义和结构的一致性。该模块通过迭代池化过程，根据学习到的结构亲和力平均特征，从而强制局部几何一致性，有效地去噪初始表示并保留语义丰富性。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong>
GeoPurify在多个主要的3D基准测试（ScanNetV2、Matterport3D和ScanNet200）上取得了显著成果：</p>
<ul>
<li><strong>卓越的数据效率：</strong> GeoPurify仅使用约1.5%的训练数据（通过精心策划的子集选择策略获得），就实现了与现有最先进方法相当甚至超越的性能。这显著缓解了大规模3D标注数据的需求。</li>
<li><strong>性能提升：</strong> 在ScanNetV2上，GeoPurify的mIoU从基线的50.2%提升到55.1%（+4.9%），mAcc从68.1%提升到72.5%（+4.4%）。在Matterport3D上，mIoU从37.5%提升到40.2%（+2.7%），mAcc从59.8%提升到62.4%（+2.6%）。</li>
<li><strong>长尾泛化能力：</strong> 在ScanNet200和Matterport3D的M160长尾基准测试中，GeoPurify也达到了新的最先进水平，这得益于其语义和几何模块的协同作用，使得模型对稀有类别具有更强的鲁棒性。</li>
<li><strong>跨数据集泛化：</strong> 在零样本跨数据集泛化任务中，GeoPurify显著优于现有方法，尤其是在从Matterport3D迁移到ScanNetV2时，mIoU领先16.3个百分点。这表明其解耦的几何先验学习到了更基本、领域无关的3D几何理解。</li>
</ul>
<p><strong>4. 论文中提及的局限性：</strong>
论文中也坦诚地指出了GeoPurify的局限性：</p>
<ul>
<li><strong>过平滑伪影（Over-smoothing Artifacts）：</strong> 在某些情况下，迭代池化过程可能会微妙地模糊不同但紧密相邻物体之间的边界，导致局部不准确。</li>
<li><strong>2D骨干网络的语义错误（Semantic Errors from 2D Backbone）：</strong> GeoPurify的性能依赖于初始2D特征的质量。当2D骨干网络产生根本性的语义错误时，几何净化模块通常无法纠正这种错误分类。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong>
这些局限性也为未来的研究指明了方向：</p>
<ul>
<li>开发更鲁棒的特征融合技术。</li>
<li>研究错误校正机制，以解决2D骨干网络带来的语义错误。</li>
<li>进一步探索如何处理极细粒度、表面级别的细节，以克服当前局部池化策略的限制。</li>
</ul>
<hr />
<p>总而言之，GeoPurify通过引入一种新颖的几何对比蒸馏机制和几何引导池化模块，成功地将2D VLMs的丰富语义与3D几何结构相结合，有效地解决了开放词汇3D语义分割中长期存在的语义丰富性与几何一致性之间的矛盾。其卓越的数据效率和在各种基准测试上的优异表现，使其成为无需3D标注的3D感知领域中一个可扩展的基础性方法。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To exploit this property, we propose GeoPurify that applies a small
Student Affinity Network to purify 2D VLM-generated 3D point features using
geometric priors distilled from a 3D self-supervised teacher model.</li>
<li>Extensive experiments on major 3D benchmarks demonstrate that GeoPurify
achieves or surpasses state-of-the-art performance while utilizing only about
1.5% of the training data.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.02186v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.02186v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.02034v1'></a></p>
<h2 id="gaussianmorphing-mesh-guided-3d-gaussians-for-semantic-aware-object-morphing"><a href="https://arxiv.org/abs/2510.02034v1">GaussianMorphing: Mesh-Guided 3D Gaussians for Semantic-Aware Object Morphing</a></h2>
<p><strong>Authors:</strong> Mengtian Li, Yunshu Bai, Yimin Chu, Yijun Shen, Zhongmei Li, Weifeng Ge, Zhifeng Xie, Chaofeng Chen</p>
<p><strong>Published:</strong> 2025-10-02</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We introduce GaussianMorphing, a novel framework for semantic-aware 3D shape
and texture morphing from multi-view images. Previous approaches usually rely
on point clouds or require pre-defined homeomorphic mappings for untextured
data. Our method overcomes these limitations by leveraging mesh-guided 3D
Gaussian Splatting (3DGS) for high-fidelity geometry and appearance modeling.
The core of our framework is a unified deformation strategy that anchors
3DGaussians to reconstructed mesh patches, ensuring geometrically consistent
transformations while preserving texture fidelity through topology-aware
constraints. In parallel, our framework establishes unsupervised semantic
correspondence by using the mesh topology as a geometric prior and maintains
structural integrity via physically plausible point trajectories. This
integrated approach preserves both local detail and global semantic coherence
throughout the morphing process with out requiring labeled data. On our
proposed TexMorph benchmark, GaussianMorphing substantially outperforms prior
2D/3D methods, reducing color consistency error (<script type="math/tex">\Delta E</script>) by 22.2% and EI by
26.2%. Project page: https://baiyunshu.github.io/GAUSSIANMORPHING.github.io/</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Mengtian Li等人撰写的论文“GaussianMorphing: Mesh-Guided 3D Gaussians for Semantic-Aware Object Morphing”的全面摘要。</p>
<hr />
<h3 id="gaussianmorphing-mesh-guided-3d-gaussians-for-semantic-aware-object-morphing_1">论文摘要：GaussianMorphing: Mesh-Guided 3D Gaussians for Semantic-Aware Object Morphing</h3>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决从多视角图像进行语义感知的三维形状和纹理变形（morphing）的挑战。现有方法通常依赖于点云或预定义同胚映射（homeomorphic mappings），且多用于无纹理数据，这限制了它们在处理复杂几何和丰富纹理对象时的适用性。核心问题在于如何实现几何一致的变形，同时通过拓扑感知约束保持纹理保真度，并且无需预先标注数据或高保真三维输入。</p>
<p><strong>2. 关键创新或方法贡献：</strong>
GaussianMorphing引入了一个新颖的混合框架，通过以下方式克服了现有方法的局限性：
*   <strong>网格引导的三维高斯溅射（3DGS）</strong>：该方法将3DGS的渲染效率与网格引导变形的结构优势相结合。它将三维高斯锚定到重建的网格面片上，确保几何一致的变换，同时通过拓扑感知约束保持纹理保真度。
*   <strong>无监督语义对应</strong>：利用网格拓扑作为几何先验，建立无监督的语义对应关系，并通过物理上合理的点轨迹保持结构完整性。这避免了对标注数据的需求。
*   <strong>混合网格-高斯表示</strong>：首先从优化的3DGS中提取高质量的初始网格，然后将高斯锚定到这些网格面片上。这种绑定确保了网格顶点变形时，锚定的高斯能与表面协同移动，从而保留精细的几何和外观细节。
*   <strong>神经变形流（Neural Morphing Flow）</strong>：通过一个神经网络学习连续的非线性变形场，预测变形轨迹，而非简单的线性插值。
*   <strong>多目标优化策略</strong>：结合了几何一致性（通过测地距离和ARAP能量项）、外观一致性（通过测地线感知的顶点颜色平滑度损失）和语义对齐约束，确保变形过程的平滑性和真实感。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>性能显著提升</strong>：在作者提出的TexMorph基准测试上，GaussianMorphing在颜色一致性误差（ΔE）上降低了22.2%，在边缘完整性（EI）上降低了26.2%，显著优于先前的2D/3D方法。
*   <strong>高保真纹理三维变形</strong>：该方法能够直接从图像输入生成完全纹理化的三维输出，提供完整的几何和纹理保真度。
*   <strong>鲁棒性和泛化能力</strong>：实验证明，该方法在复杂拓扑和纹理丰富的对象上表现出鲁棒性，即使在存在显著非等距变形的情况下也能产生平滑且真实的变形结果。
*   <strong>用户研究验证</strong>：用户研究结果显示，超过80%的用户认为GaussianMorphing在结构相似性、纹理一致性和边缘连续性等所有指标上表现更优。</p>
<p><strong>4. 论文中提及的局限性：</strong>
论文中没有明确列出当前方法的局限性，但从其创新点和与其他方法的对比中可以推断出一些潜在的挑战或未来改进方向：
*   <strong>计算成本</strong>：虽然论文提到生成初始混合网格-高斯表示需要约1小时，优化过程需要500-1000次迭代，但对于超大规模或实时应用，其计算效率可能仍有提升空间。
*   <strong>网格质量依赖</strong>：虽然方法通过泊松重建和正则化项确保网格质量，但初始网格提取的质量仍可能影响最终变形效果。
*   <strong>拓扑变化限制</strong>：尽管方法通过网格引导解决了拓扑连接性问题，但对于极端拓扑变化（例如，一个物体分裂成多个部分或多个物体合并成一个）的场景，其处理能力可能仍需进一步探索。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>更复杂的拓扑变化</strong>：探索如何处理更极端或动态的拓扑变化，例如物体分裂或合并。
*   <strong>实时性能优化</strong>：进一步优化算法，以实现更快的重建和变形速度，满足实时应用的需求。
*   <strong>多模态输入</strong>：探索结合其他模态（如文本描述、音频）来指导变形过程，实现更丰富的语义控制。
*   <strong>用户交互性</strong>：开发更直观的用户界面和工具，允许用户更精细地控制变形过程中的特定区域或属性。
*   <strong>大规模数据集和泛化</strong>：在更大、更多样化的数据集上进行训练和评估，进一步提升模型的泛化能力。</p>
<hr />
<p>这篇论文为三维对象变形领域设定了新标准，通过将3DGS与网格引导变形相结合，实现了前所未有的几何鲁棒性、纹理连贯性和输入可访问性，为视觉效果和数字内容创作开辟了新的可能性。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce GaussianMorphing, a novel framework for semantic-aware 3D shape
and texture morphing from multi-view images.</li>
<li>Our method overcomes these limitations by leveraging mesh-guided 3D
Gaussian Splatting (3DGS) for high-fidelity geometry and appearance modeling.</li>
<li>On our
proposed TexMorph benchmark, GaussianMorphing substantially outperforms prior
2D/3D methods, reducing color consistency error (<script type="math/tex">\Delta E</script>) by 22.2% and EI by
26.2%.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.02034v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.02034v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-10-03 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
