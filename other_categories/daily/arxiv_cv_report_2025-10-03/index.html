<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-10-03 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DEâ«¶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Birdâs-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">è®ºæéè¯»</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-Î±
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-10-02/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../../depth_completion/DNet/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-10-03">Arxiv Computer Vision Papers - 2025-10-03</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#from-behavioral-performance-to-internal-competence-interpreting-vision-language-models-with-vlm-lens" class="nav-link">From Behavioral Performance to Internal Competence: Interpreting Vision-Language Models with VLM-Lens</a>
                </li>
                <li class="nav-item">
                    <a href="#learning-to-generate-object-interactions-with-physics-guided-video-diffusion" class="nav-link">Learning to Generate Object Interactions with Physics-Guided Video Diffusion</a>
                </li>
                <li class="nav-item">
                    <a href="#self-forcing-towards-minute-scale-high-quality-video-generation" class="nav-link">Self-Forcing++: Towards Minute-Scale High-Quality Video Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#vidguard-r1-ai-generated-video-detection-and-explanation-via-reasoning-mllms-and-rl" class="nav-link">VidGuard-R1: AI-Generated Video Detection and Explanation via Reasoning MLLMs and RL</a>
                </li>
                <li class="nav-item">
                    <a href="#microclip-unsupervised-clip-adaptation-via-coarse-fine-token-fusion-for-fine-grained-image-classification" class="nav-link">microCLIP: Unsupervised CLIP Adaptation via Coarse-Fine Token Fusion for Fine-Grained Image Classification</a>
                </li>
                <li class="nav-item">
                    <a href="#do-you-know-where-your-camera-is-view-invariant-policy-learning-with-camera-conditioning" class="nav-link">Do You Know Where Your Camera Is? View-Invariant Policy Learning with Camera Conditioning</a>
                </li>
                <li class="nav-item">
                    <a href="#from-frames-to-clips-efficient-key-clip-selection-for-long-form-video-understanding" class="nav-link">From Frames to Clips: Efficient Key Clip Selection for Long-Form Video Understanding</a>
                </li>
                <li class="nav-item">
                    <a href="#dragflow-unleashing-dit-priors-with-region-based-supervision-for-drag-editing" class="nav-link">DragFlow: Unleashing DiT Priors with Region Based Supervision for Drag Editing</a>
                </li>
                <li class="nav-item">
                    <a href="#geopurify-a-data-efficient-geometric-distillation-framework-for-open-vocabulary-3d-segmentation" class="nav-link">GeoPurify: A Data-Efficient Geometric Distillation Framework for Open-Vocabulary 3D Segmentation</a>
                </li>
                <li class="nav-item">
                    <a href="#gaussianmorphing-mesh-guided-3d-gaussians-for-semantic-aware-object-morphing" class="nav-link">GaussianMorphing: Mesh-Guided 3D Gaussians for Semantic-Aware Object Morphing</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-10-03">Arxiv Computer Vision Papers - 2025-10-03</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>å¥½çï¼è¿æ¯ä¸ä»½éå¯¹2025å¹´10æ2æ¥Arxivè®¡ç®æºè§è§è®ºæçæ§è¡æè¦ï¼æ¨å¨å¸®å©å¿ç¢çç ç©¶äººåå¿«éäºè§£å³é®åå±ï¼</p>
<hr />
<p><strong>Arxiv è®¡ç®æºè§è§æ¯æ¥æ¥åæ§è¡æè¦ (2025å¹´10æ2æ¥)</strong></p>
<p><strong>1. ä¸»è¦ä¸»é¢ä¸è¶å¿æ¦è¿°ï¼</strong></p>
<p>ä»å¤©çè®ºæéå±ç°äºè®¡ç®æºè§è§é¢åå ä¸ªæ´»è·ä¸ç¸äºå³èçè¶å¿ã<strong>å¤æ¨¡æå­¦ä¹ ï¼ç¹å«æ¯è§è§-è¯­è¨æ¨¡å VLMï¼</strong>çè§£éæ§ãéåºæ§ååºç¨æ¯æ ¸å¿ç¦ç¹ï¼ä½ç°å¨å¯¹VLMåé¨æºå¶ççè§£ãå¾®è°ä»¥åå¶å¨è§é¢æ£æµä¸­çåºç¨ã<strong>è§é¢çæä¸çè§£</strong>æ¯å¦ä¸ä¸ªçªåºä¸»é¢ï¼æ¶µçäºä»é«è´¨éãé¿æ¶è§é¢çæå°é«æè§é¢åå®¹åæçå¤ä¸ªæ¹é¢ãæ­¤å¤ï¼<strong>3Dè§è§</strong>å¨è¯­ä¹åå²åå¯¹è±¡æä½æ¹é¢ä¹åå¾äºè¿å±ï¼å¹¶ä¸çææ¨¡åç¸ç»åã<strong>æ°æ®æçåæ çç£/èªçç£å­¦ä¹ </strong>ççå¿µè´¯ç©¿äºå¤ä¸ªç ç©¶æ¹åï¼æ¨å¨åå°å¯¹å¤§éæ æ³¨æ°æ®çä¾èµã</p>
<p><strong>2. æ¾èæåæ°æ§è®ºæäº®ç¹ï¼</strong></p>
<ul>
<li><strong>"From Behavioral Performance to Internal Competence: Interpreting Vision-Language Models with VLM-Lens" (Hala Sheta et al.)</strong>ï¼è¿ç¯è®ºæéè¿å¼å¥âVLM-Lensâæ¡æ¶ï¼æ·±å¥æ¢è®¨äºVLMçåé¨å·¥ä½æºå¶ï¼æ¨å¨ä»è¡ä¸ºè¡¨ç°æ¨æ­å¶åå¨è½åãè¿å¯¹äºçè§£åæ¹è¿VLMçé²æ£æ§ãå¬å¹³æ§åå¯ä¿¡èµæ§è³å³éè¦ï¼æ¯VLMå¯è§£éæ§ç ç©¶çéè¦ä¸æ­¥ã</li>
<li><strong>"Self-Forcing++: Towards Minute-Scale High-Quality Video Generation" (Justin Cui et al.)</strong>ï¼å¨è§é¢çæé¢åï¼å®ç°é¿æ¶é´ãé«è´¨éè§é¢ä¸ç´æ¯ä¸ä¸ªææãSelf-Forcing++çåºç°ï¼é¢ç¤ºçè§é¢çæææ¯å¨çææ¶é¿åè´¨éä¸åå¾äºæ¾èçªç ´ï¼æææ¨å¨è§é¢åå®¹åä½åæ¨¡æçè¾¹çã</li>
<li><strong>"VidGuard-R1: AI-Generated Video Detection and Explanation via Reasoning MLLMs and RL" (Kyoungjun Park et al.)</strong>ï¼è¿ç¯è®ºæä¸ä»å³æ³¨AIçæè§é¢çæ£æµï¼æ´è¿ä¸æ­¥ç»åäºå¤æ¨¡æå¤§è¯­è¨æ¨¡åï¼MLLMsï¼åå¼ºåå­¦ä¹ ï¼RLï¼è¿è¡è§£éãå¨æ·±åº¦ä¼ªé ï¼deepfakeï¼æ¥çæ®éçèæ¯ä¸ï¼æä¾æ£æµç»æçè§£éæ§å¯¹äºå»ºç«ä¿¡ä»»åçè§£AIçæåå®¹çæ¥æºè³å³éè¦ï¼å·æéè¦çç¤¾ä¼åææ¯æä¹ã</li>
<li><strong>"DragFlow: Unleashing DiT Priors with Region Based Supervision for Drag Editing" (Zihan Zhou et al.)</strong>ï¼è¯¥å·¥ä½å°DiTï¼Diffusion Transformerï¼çå¼ºå¤§åéªç¥è¯ä¸åºåçç£ç¸ç»åï¼å®ç°äºç´è§çâææ½å¼âå¾åç¼è¾ãè¿ç§äº¤äºå¼ç¼è¾æ¹å¼æå¤§å°æåäºç¨æ·å¯¹çææ¨¡åçæ§å¶åï¼æ¯äººæºäº¤äºä¸çææ¨¡åç»åçä¼ç§èä¾ã</li>
</ul>
<p><strong>3. æ°å´ç ç©¶æ¹åæææ¯ï¼</strong></p>
<ul>
<li><strong>VLMçæ·±å¥è§£éä¸åé¨æºå¶æ¢ç©¶ï¼</strong> ä¸åæ»¡è¶³äºVLMçè¡¨é¢æ§è½ï¼èæ¯æ·±å¥çè§£å¶å³ç­è¿ç¨ååå¨è¡¨å¾ã</li>
<li><strong>é¿æ¶ãé«ä¿çè§é¢çæï¼</strong> è§é¢çææ­£ä»ç­çæ®µåæ´é¿ãæ´è¿è´¯ãæ´é«è´¨éçè§é¢åå±ã</li>
<li><strong>AIçæåå®¹æ£æµçè§£éæ§ï¼</strong> ç»åæ¨çè½ååå¤æ¨¡ææ¨¡åï¼ä¸ä»æ£æµAIçæåå®¹ï¼è¿è½æä¾æ£æµä¾æ®ã</li>
<li><strong>æ°æ®é«æç3Då ä½å­¦ä¹ ï¼</strong> å©ç¨å ä½è¸é¦ç­ææ¯ï¼å¨æéæ°æ®ä¸å®ç°å¼æ¾è¯æ±ç3Dåå²ã</li>
<li><strong>çææ¨¡åä¸äº¤äºå¼ç¼è¾çèåï¼</strong> èµäºç¨æ·æ´ç²¾ç»ãæ´ç´è§ççæåå®¹æ§å¶è½åã</li>
</ul>
<p><strong>4. å»ºè®®éè¯»å¨æçè®ºæï¼</strong></p>
<p>å¯¹äºä¸åå´è¶£çç ç©¶äººåï¼å»ºè®®éè¯»ä»¥ä¸è®ºæï¼</p>
<ul>
<li><strong>å¯¹VLMå¯è§£éæ§ãé²æ£æ§æå´è¶£ï¼</strong><ul>
<li>"From Behavioral Performance to Internal Competence: Interpreting Vision-Language Models with VLM-Lens"</li>
</ul>
</li>
<li><strong>å¯¹è§é¢çæææ¯åæ²¿æå´è¶£ï¼</strong><ul>
<li>"Self-Forcing++: Towards Minute-Scale High-Quality Video Generation"</li>
<li>"Learning to Generate Object Interactions with Physics-Guided Video Diffusion" (å¦æå¯¹ç©çä¸çäº¤äºçè§é¢çææå´è¶£)</li>
</ul>
</li>
<li><strong>å¯¹AIçæåå®¹æ£æµåä¼¦çé®é¢æå´è¶£ï¼</strong><ul>
<li>"VidGuard-R1: AI-Generated Video Detection and Explanation via Reasoning MLLMs and RL"</li>
</ul>
</li>
<li><strong>å¯¹å¾å/è§é¢ç¼è¾ä¸çææ¨¡åäº¤äºæå´è¶£ï¼</strong><ul>
<li>"DragFlow: Unleashing DiT Priors with Region Based Supervision for Drag Editing"</li>
</ul>
</li>
<li><strong>å¯¹3Dè§è§ãæ°æ®é«æå­¦ä¹ åå¼æ¾è¯æ±åå²æå´è¶£ï¼</strong><ul>
<li>"GeoPurify: A Data-Efficient Geometric Distillation Framework for Open-Vocabulary 3D Segmentation"</li>
</ul>
</li>
<li><strong>å¯¹VLMå¨ç»ç²åº¦åç±»ä¸­çæ çç£éåºæå´è¶£ï¼</strong><ul>
<li>"microCLIP: Unsupervised CLIP Adaptation via Coarse-Fine Token Fusion for Fine-Grained Image Classification"</li>
</ul>
</li>
</ul>
<hr />
<p>è¿ä»½æè¦æ¨å¨æä¾ä¸ä¸ªé«å±æ¬¡çæ¦è§ï¼å¸®å©æ¨å¿«éè¯å«ä¸æ¨ç ç©¶æ¹åæç¸å³çè®ºæã</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2510.02292v1">From Behavioral Performance to Internal Competence: Interpreting Vision-Language Models with VLM-Lens</a></li>
<li><a href="#2510.02284v1">Learning to Generate Object Interactions with Physics-Guided Video Diffusion</a></li>
<li><a href="#2510.02283v1">Self-Forcing++: Towards Minute-Scale High-Quality Video Generation</a></li>
<li><a href="#2510.02282v1">VidGuard-R1: AI-Generated Video Detection and Explanation via Reasoning MLLMs and RL</a></li>
<li><a href="#2510.02270v1">microCLIP: Unsupervised CLIP Adaptation via Coarse-Fine Token Fusion for Fine-Grained Image Classification</a></li>
<li><a href="#2510.02268v1">Do You Know Where Your Camera Is? View-Invariant Policy Learning with Camera Conditioning</a></li>
<li><a href="#2510.02262v1">From Frames to Clips: Efficient Key Clip Selection for Long-Form Video Understanding</a></li>
<li><a href="#2510.02253v1">DragFlow: Unleashing DiT Priors with Region Based Supervision for Drag Editing</a></li>
<li><a href="#2510.02186v1">GeoPurify: A Data-Efficient Geometric Distillation Framework for Open-Vocabulary 3D Segmentation</a></li>
<li><a href="#2510.02034v1">GaussianMorphing: Mesh-Guided 3D Gaussians for Semantic-Aware Object Morphing</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2510.02292v1'></a></p>
<h2 id="from-behavioral-performance-to-internal-competence-interpreting-vision-language-models-with-vlm-lens"><a href="https://arxiv.org/abs/2510.02292v1">From Behavioral Performance to Internal Competence: Interpreting Vision-Language Models with VLM-Lens</a></h2>
<p><strong>Authors:</strong> Hala Sheta, Eric Huang, Shuyu Wu, Ilia Alenabi, Jiajun Hong, Ryker Lin, Ruoxi Ning, Daniel Wei, Jialin Yang, Jiawei Zhou, Ziqiao Ma, Freda Shi</p>
<p><strong>Published:</strong> 2025-10-02</p>
<p><strong>Categories:</strong> cs.CL, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We introduce VLM-Lens, a toolkit designed to enable systematic benchmarking,
analysis, and interpretation of vision-language models (VLMs) by supporting the
extraction of intermediate outputs from any layer during the forward pass of
open-source VLMs. VLM-Lens provides a unified, YAML-configurable interface that
abstracts away model-specific complexities and supports user-friendly operation
across diverse VLMs. It currently supports 16 state-of-the-art base VLMs and
their over 30 variants, and is extensible to accommodate new models without
changing the core logic.
  The toolkit integrates easily with various interpretability and analysis
methods. We demonstrate its usage with two simple analytical experiments,
revealing systematic differences in the hidden representations of VLMs across
layers and target concepts. VLM-Lens is released as an open-sourced project to
accelerate community efforts in understanding and improving VLMs.</p>
<p><strong>Analysis:</strong></p>
<p>ä»¥ä¸æ¯è®ºæâFrom Behavioral Performance to Internal Competence: Interpreting Vision-Language Models with VLM-Lensâçå¨é¢æè¦ï¼</p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢</strong>
ç°æVLMåºåæµè¯ä¸»è¦ä¾èµåºäºç²¾ç¡®å¹éçåç¡®çè¯ä¼°æ¨¡åæ§è½ï¼è¿å¯è½å¿½ç¥äºæ¨¡åéèè¡¨ç¤ºä¸­åµå¥çä¿¡æ¯ï¼æå æ·å¾å©ç¨å¯¼è´è¯¯å¯¼æ§è¯ä¼°ãæ­¤å¤ï¼VLMçè§£éæ§ç ç©¶åå·¥å·åç¸å¯¹ä¸æçï¼ç¼ºä¹ç»ä¸çæ¡æ¶æ¥æåVLMçåé¨è¡¨ç¤ºï¼ä½¿å¾ç³»ç»çè§£å¶åé¨ç¥è¯åå³ç­è¿ç¨é¢ä¸´ææãæ¬ææ¨å¨è§£å³è¿äºææï¼æä¾ä¸ä¸ªç»ä¸çå·¥å·åï¼ä»¥ç³»ç»å°åºåæµè¯ãåæåè§£éVLMã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®</strong>
*   <strong>VLM-LENSå·¥å·åï¼</strong> å¼å¥äºä¸ä¸ªåä¸ºVLM-LENSçå·¥å·åï¼æ¨å¨éè¿æ¯æä»å¼æºVLMååä¼ æ­çä»»ä½å±ä¸­æåä¸­é´è¾åºï¼å®ç°å¯¹VLMçç³»ç»åºåæµè¯ãåæåè§£éã
*   <strong>ç»ä¸æ¥å£ï¼</strong> VLM-LENSæä¾äºä¸ä¸ªç»ä¸çãYAMLå¯éç½®çæ¥å£ï¼æ½è±¡äºæ¨¡åç¹å®çå¤ææ§ï¼æ¯æè·¨ä¸åVLMçç¨æ·åå¥½æä½ã
*   <strong>å¹¿æ³çæ¨¡åè¦çåçµæ´»æ§ï¼</strong> ç®åæ¯æ16ä¸ªæåè¿çåºç¡VLMåå¶30å¤ä¸ªåä½ï¼å¹¶ä¸è®¾è®¡å·æé«åº¦å¯æ©å±æ§ï¼å¯ä»¥è½»æ¾æ·»å æ°æ¨¡åã
*   <strong>æ¨¡åç¹å®ç¯å¢æ¯æï¼</strong> ä¸ºä¸åVLMæä¾æ¨¡åç¹å®ç¯å¢è®¾ç½®ï¼æ¯ä¸ªé½å¯ä»¥éè¿åè¡pipå®è£å½ä»¤è½»æ¾å®è£ã
*   <strong>ä¸ç°æè§£éæ§æ¹æ³éæï¼</strong> è¯¥å·¥å·åæäºä¸åç§è§£éæ§ååææ¹æ³éæï¼ä¾å¦æ¢éï¼probingï¼ãç¥ç»åè·¯æ£æ¥åç¥è¯è¿½è¸ªã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹</strong>
*   <strong>æ¢éå®éªï¼</strong> éè¿å¨æåçè¡¨ç¤ºä¸è®­ç»æ¢éï¼è¯ä¼°VLMè¯å«ä¸ç»åå§æ¦å¿µçåé¨è½åãç»ææ¾ç¤ºï¼Qwen-7båMiniCPM-oçæ¢éå¨è®¸å¤æ°æ®éåå²ä¸­å ä¹è¾¾å°å®ç¾åç¡®çï¼ä¸ä¸æ§å¶æ¢éæ§è½å­å¨æ¾èå·®å¼ï¼å°¤å¶æ¯å¨æåä¸å±è¡¨ç¤ºä¸­ãè¿è¡¨æè¿äºæ¨¡åç¼ç äºä»»å¡ç¸å³ä¿¡æ¯ã
*   <strong>æ¦å¿µç¸ä¼¼æ§å®éªï¼</strong> åå°Stroopæåºçå¯åï¼éè¿æå»ºå·æä¸ä¸è´é¢è²çº¿ç´¢çå¾ååæ¨¡ç³ææ¬æä»¤ï¼æ¢ç©¶VLMå¦ä½å¨è¿ç§æ­§ä¹ä¸çè§£é¢è²æ¦å¿µãç»ææ¾ç¤ºï¼æ¨¡åå¯é å°ç¼ç äºè¯æ±åå®¹ãåæ¯å­ä½é¢è²åèæ¯é¢è²è¿ä¸ç§ä¿¡æ¯ï¼ä¸èæ¯é¢è²äº§çäºæå¼ºçå¹éä¸ä¸å¹éå¯¹æ¯ãè¯æ±åå®¹æ¯å­ä½é¢è²æ´çªåºã
*   <strong>æ§è½åè½ååæï¼</strong> VLM-LENSéè¿æ¢éè¯ä¼°ï¼è¡¥åäºç°æåºäºåç¡®ççåºåæµè¯ï¼æä¾äºå¯¹åé¨ç¶ææè¡¨ç¤ºåå®¹çæ´è¯¦ç»çè§£ï¼å°æ¨¡åæ§è½å»ºç«å¨å¯è§£éçåå§ç¥è¯ä¹ä¸ã
*   <strong>æçè¯ä¼°ï¼</strong> CLIPå¨æ¨çæ¶é´ååå­ä½¿ç¨æ¹é¢è¡¨ç°æä½³ï¼è¿å½å äºå¶ç´§åçæ¶æåè¾å°çåæ°ãInternVLãInternLM-XComposer-2.5åMolMoæ¯éåº¦ææ¢çæ¨¡åã</p>
<p><strong>4. è®ºæä¸­æå°çå±éæ§</strong>
*   <strong>ä¸æ¸¸ä»»å¡æ¯ææéï¼</strong> å½åå·¥å·åä¸ç´æ¥æ¯æé¤æ¢éä¹å¤çæ´å¤ä¸æ¸¸ä»»å¡ï¼ä¾å¦æ³¨æåè§£éåç¥ç»åè·¯åç°ã
*   <strong>æ¢¯åº¦åæéå¶ï¼</strong> å½åçæ¨çåæ°æ®åºå­å¨æ¹æ³é»æ­¢äºä½¿ç¨åºäºæ¢¯åº¦çæ¾èæ§åæï¼å¦Grad-CAMï¼ã
*   <strong>ç¨æ·èªå®ä¹åè½ï¼</strong> å½åç¨æ·ä»éå®ç°å¶èªå®ä¹åè½ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹å</strong>
*   <strong>ç¤¾åºè´¡ç®ï¼</strong> ææç¤¾åºå¯¹å­å¨åºè¿è¡å¿«éè´¡ç®ï¼ä»¥æ¯æåç§ä»»å¡ï¼ä½èè´åäºé¿ææ¯æè¿äºåªåã
*   <strong>æ©å±ä¸æ¸¸ä»»å¡æ¯æï¼</strong> è¿ä¸æ­¥å¼åå·¥å·åï¼ä»¥æ¯ææ´å¹¿æ³çè§£éæ§æ¹æ³ï¼å¦æ³¨æåè§£éåç¥ç»åè·¯åç°ã
*   <strong>æ¹è¿æ¢¯åº¦åæï¼</strong> æ¢ç´¢æ°çæ¨çåæ°æ®åºå­å¨æ¹æ³ï¼ä»¥æ¯æåºäºæ¢¯åº¦çæ¾èæ§åæã
*   <strong>æç»­æ¨¡ååæåæ¹è¿ï¼</strong> VLM-LENSå°ç§¯ææ¯æå¯¹VLMçåæåæ¹è¿ï¼ä»¥è§£å³ç°æåºåæµè¯ä¸­æ¨¡åæ§è½ä¸å®éè½åä¹é´çå·®è·ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce VLM-Lens, a toolkit designed to enable systematic benchmarking,
analysis, and interpretation of vision-language models (VLMs) by supporting the
extraction of intermediate outputs from any layer during the forward pass of
open-source VLMs. VLM-Lens provides a unified, YAML-configurable interface that
abstracts away model-specific complexities and supports user-friendly operation
across diverse VLMs. It currently supports 16 state-of-the-art base VLMs and
their over 30 variants, and is extensible to accommodate new models without
changing the core logic.</li>
<li>We demonstrate its usage with two simple analytical experiments,
revealing systematic differences in the hidden representations of VLMs across
layers and target concepts.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.02292v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.02292v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.02284v1'></a></p>
<h2 id="learning-to-generate-object-interactions-with-physics-guided-video-diffusion"><a href="https://arxiv.org/abs/2510.02284v1">Learning to Generate Object Interactions with Physics-Guided Video Diffusion</a></h2>
<p><strong>Authors:</strong> David Romero, Ariana Bermudez, Hao Li, Fabio Pizzati, Ivan Laptev</p>
<p><strong>Published:</strong> 2025-10-02</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Recent models for video generation have achieved remarkable progress and are
now deployed in film, social media production, and advertising. Beyond their
creative potential, such models also hold promise as world simulators for
robotics and embodied decision making. Despite strong advances, however,
current approaches still struggle to generate physically plausible object
interactions and lack physics-grounded control mechanisms. To address this
limitation, we introduce KineMask, an approach for physics-guided video
generation that enables realistic rigid body control, interactions, and
effects. Given a single image and a specified object velocity, our method
generates videos with inferred motions and future object interactions. We
propose a two-stage training strategy that gradually removes future motion
supervision via object masks. Using this strategy we train video diffusion
models (VDMs) on synthetic scenes of simple interactions and demonstrate
significant improvements of object interactions in real scenes. Furthermore,
KineMask integrates low-level motion control with high-level textual
conditioning via predictive scene descriptions, leading to effective support
for synthesis of complex dynamical phenomena. Extensive experiments show that
KineMask achieves strong improvements over recent models of comparable size.
Ablation studies further highlight the complementary roles of low- and
high-level conditioning in VDMs. Our code, model, and data will be made
publicly available.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾David Romeroç­äººæ°åçè®ºæâLearning to Generate Object Interactions with Physics-Guided Video Diffusionâçå¨é¢æè¦ã</p>
<hr />
<h3 id="learning-to-generate-object-interactions-with-physics-guided-video-diffusion_1">è®ºææè¦ï¼Learning to Generate Object Interactions with Physics-Guided Video Diffusion</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
å°½ç®¡è§é¢çææ¨¡åï¼ç¹å«æ¯è§é¢æ©æ£æ¨¡åï¼VDMsï¼å¨è§è§è´¨éåæ¶é´ä¸è´æ§æ¹é¢åå¾äºæ¾èè¿å±ï¼å¹¶å¨çµå½±ãç¤¾äº¤åªä½åå¹¿åç­é¢åå¾å°åºç¨ï¼ä½å®ä»¬å¨çæç©çä¸å¯ä¿¡çå¯¹è±¡äº¤äºåç¼ºä¹ç©çåºç¡çæ§å¶æºå¶æ¹é¢ä»ç¶é¢ä¸´ææãç°æçæ¹æ³é¾ä»¥ææå¯¹è±¡æ°¸ä¹æ§ãå æäº¤äºä»¥ååä½å¨åå­¦ç­åºæ¬ç©çç¹æ§ï¼å¯¼è´çæçè§é¢ä¸­ç©ä½äº¤äºä¸çå®æå½¢ç¶å¤±çãå æ­¤ï¼æ¬ææ¨å¨è§£å³ä¸¤ä¸ªæ ¸å¿é®é¢ï¼(1) è§é¢æ©æ£æ¨¡åè½å¦å¨ç»å®åå§å¨ææ¡ä»¶ä¸çæå¯¹è±¡ä¹é´çå®çäº¤äºï¼(2) æ°æ®åææ¬æ¡ä»¶å¦ä½å½±åçæè§é¢ä¸­å æç©çæåºçåºç°ï¼</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
ä¸ºäºè§£å³ä¸è¿°é®é¢ï¼ä½èæåºäº <strong>KineMask</strong>ï¼ä¸ä¸ªç©çå¼å¯¼çè§é¢çææ¡æ¶ï¼å¶ä¸»è¦åæ°ç¹åæ¬ï¼</p>
<ul>
<li><strong>KineMask æ¡æ¶ï¼</strong> å¼å¥äºä¸ç§åºäºå¯¹è±¡è¿å¨æ¡ä»¶åçæºå¶ï¼éè¿æ°é¢çä¸¤é¶æ®µè®­ç»åæ¡ä»¶ç¼ç ï¼ä½¿VDMsè½å¤çæç©çä¸çå®çåä½æ§å¶ãäº¤äºåææã</li>
<li><strong>ä¸¤é¶æ®µè®­ç»ç­ç¥ï¼</strong><ul>
<li><strong>ç¬¬ä¸é¶æ®µè®­ç»ï¼</strong> å¨åææ°æ®éä¸è®­ç»ControlNetï¼è¯¥æ°æ®éåå«ç©çææçå¨æåæç¡®çå¯¹è±¡äº¤äºï¼ä»¥ååºå±äºä»¶çææ¬æè¿°ãè¿å¨æ§å¶ä¿¡å·è¢«ç¼ç ä¸ºå¯¹è±¡çéåº¦æ©ç ï¼mfï¼ï¼å¶ä¸­ä¸ä¸ªééå¯¹åºäºxãyãzè½´ä¸çç¬æ¶éåº¦ãæ¨¡åå¨ææå¸§ä¸é½æ¥æ¶éåº¦çç£ã</li>
<li><strong>ç¬¬äºé¶æ®µè®­ç»ï¼æ©ç ä¸¢å¼ç­ç¥ï¼ï¼</strong> å¨ç¬¬ä¸é¶æ®µè®­ç»çåºç¡ä¸ï¼å¼å¥äºæ©ç ä¸¢å¼ç­ç¥ï¼å¨è®­ç»æ¶éæºä¸¢å¼éåº¦æ©ç çæåä¸é¨åå¸§ãè¿æå³çåªæåå§å¸§åå«éåº¦çç£ï¼åºåçå¶ä½é¨åè¢«è®¾ç½®ä¸ºé¶ãè¿è¿«ä½¿æ¨¡åä»åå§æ¡ä»¶æ¨æ­æªæ¥çè¿å¨åäº¤äºï¼ä»èå¨æ¨çæ¶ä»ä¾èµåå§éåº¦ä¿¡æ¯ã</li>
</ul>
</li>
<li><strong>ä½çº§è¿å¨æ§å¶ä¸é«çº§ææ¬æ¡ä»¶ç»åï¼</strong> KineMask å°ä½çº§è¿å¨æ§å¶ï¼éè¿åå§å¯¹è±¡éåº¦æ©ç ï¼ä¸é«çº§ææ¬æ¡ä»¶ï¼éè¿é¢æµåºæ¯æè¿°ï¼ç¸ç»åãå¨æ¨çæ¶ï¼ç»å®åä¸ªå¾ååæå®å¯¹è±¡éåº¦ï¼æ¨¡åå©ç¨SAMï¼Segment Anything Modelï¼æåå¯¹è±¡æ©ç ï¼å¹¶ä½¿ç¨GPT-5æ¨æ­æªæ¥åºæ¯å¨æçææ¬æè¿°ï¼ä»èçæå·ææ¨æ­è¿å¨åæªæ¥å¯¹è±¡äº¤äºçè§é¢ã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶éè¦æ§ï¼</strong>
*   <strong>æ¾èçæ§è½æåï¼</strong> KineMask å¨åæåºæ¯ï¼ç®åäº¤äºï¼åçå®åºæ¯ï¼å¤æäº¤äºï¼ä¸­åè¡¨ç°åºå¯¹å¯¹è±¡äº¤äºçæ¾èæ¹è¿ï¼è¶è¶äºåç­è§æ¨¡çææ°æ¨¡åï¼å¦CogVideoXãWanåForce Promptingï¼ã
*   <strong>ç©ççå®æ§åå ææåºï¼</strong> å®éªè¯æï¼KineMask è½å¤çæç©çä¸çå®çç¢°æãå ææåºï¼ä¾å¦ï¼ç©ä½éåº¦å¢å å¯¼è´ç¢°æåç¬¬äºä¸ªç©ä½ç§»å¨æ´è¿ï¼ä»¥åå¤æææï¼å¦ç»çç ´ç¢ææ¶²ä½æºåºï¼ãè¿è¡¨ææ¨¡åè½å¤ææè¿å¨çå æç»æï¼å¯¹äºä¸çå»ºæ¨¡åç¥æè§åå·æéè¦ä»·å¼ã
*   <strong>æ°æ®åææ¬æ¡ä»¶çå½±åï¼</strong>
    *   <strong>è®­ç»æ°æ®ï¼</strong> è®­ç»å¨åå«å¯¹è±¡äº¤äºçåææ°æ®éä¸ï¼Interactionsæ°æ®éï¼çæ¨¡åï¼æ¯ä»å¨ç®åè¿å¨æ°æ®éä¸è®­ç»çæ¨¡åï¼å¨çæå¤æå¯¹è±¡äº¤äºæ¹é¢è¡¨ç°æ´ä¼ï¼å¹¶è½æ³åå°çå®ä¸çå¾åã
    *   <strong>ææ¬æ¡ä»¶ï¼</strong> ä¸°å¯çææ¬æè¿°ï¼cï¼å¨è®­ç»æ¶è½å¤å¸®å©æ¨¡åå©ç¨VDMçåéªç¥è¯ï¼çæè¶åºè®­ç»æ°æ®èå´çå¤æäº¤äºææï¼ä¾å¦è±ç¶ç ´ç¢ææ°´æ³¢å½¢æãæ¶èç ç©¶å¼ºè°äºä½çº§åé«çº§æ¡ä»¶å¨VDMsä¸­äºè¡¥ä½ç¨çéè¦æ§ã
*   <strong>ç¨æ·ç ç©¶ï¼</strong> ç¨æ·ç ç©¶ç»æè¡¨æï¼KineMask å¨è¿å¨ä¿çåº¦ãäº¤äºè´¨éåæ´ä½ç©çä¸è´æ§æ¹é¢æ¾èä¼äºåºçº¿æ¨¡åã</p>
<p><strong>4. è®ºæä¸­æå°çå±éæ§ï¼</strong>
*   <strong>ä½çº§æ§å¶çå±éæ§ï¼</strong> KineMask çä½çº§æ§å¶ç®åä»éäºéåº¦ï¼èçå®ä¸ççè¿å¨è¿åå³äºæ©æ¦åãå½¢ç¶ãè´¨éåç©ºæ°é»åç­å ç´ ã
*   <strong>å¤±è´¥æ¡ä¾ï¼</strong>
    *   å¯¹äºé«åº¦ä¸ææ¾çç©ä½ï¼æ¨¡åå¯è½å¨è¿å¨è¿ç¨ä¸­å¿½ç¥å¶ä»ç©ä½ï¼å¯¼è´ç¢°ææªè½åçã
    *   å¨åå«å¤ä¸ªç©ä½çå¤æåºæ¯ä¸­ï¼ææ¶ä¼åºç°æ­§ä¹ï¼å¯¼è´ç©ä½éå¤ææ¶å¤±ãä½èæ¨æµææ¬æç¤ºå¯è½ä¹å å§äºè¿ç§æ­§ä¹ã
    *   å½æ§å¶åºç¨äºå·æèæ¯æç»ææå å ç©ä½æ¶ï¼Force Promptingç­åºçº¿æ¹æ³ä¼éå°å°é¾ï¼èKineMaskéè¿åºäºæ©ç çæ§å¶æºå¶è½å¤æ­£ç¡®å¤çè¿äºè¾¹ç¼æåµã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>æ´åæ´å¨é¢çç©çæ§å¶ï¼</strong> å°æ©æ¦åãå½¢ç¶ãè´¨éåç©ºæ°é»åç­æ´å¤ç©çå ç´ çº³å¥VDMçæçè¿å¨æ§å¶ä¸­ï¼ä»¥æé«ç©çåç¡®æ§ã
*   <strong>æ¹è¿å¤æåºæ¯å¤çï¼</strong> è§£å³å¤ç©ä½å¤æåºæ¯ä¸­çæ­§ä¹é®é¢ï¼åå°ç©ä½éå¤ææ¶å¤±çå¤±è´¥æ¡ä¾ã
*   <strong>å¤æ¨¡æè¯­è¨æ¨¡åä¸ç©çæ¨ççç»åï¼</strong> è¿ä¸æ­¥æ¢ç´¢å¤æ¨¡æè¯­è¨æ¨¡åå¨ææ¬åºç¡ç©çæ¨çæ¹é¢çè¿å±ï¼ä»¥è¡¥åè§é¢çæï¼ä»èå®ç°æ´å¼ºå¤§çç©çä¸çå»ºæ¨¡ã
*   <strong>æ©å±å°æ´å¹¿æ³çäº¤äºç±»åï¼</strong> å°KineMaskçè½åæ©å±å°å¤çé¤äºåä½äº¤äºä¹å¤çæ´å¹¿æ³çç©çç°è±¡ã</p>
<hr />
<p>æ»èè¨ä¹ï¼KineMaskéè¿å¶åæ°çä¸¤é¶æ®µè®­ç»ç­ç¥åä½çº§è¿å¨æ§å¶ä¸é«çº§ææ¬æ¡ä»¶çç»åï¼ä¸ºç©çå¼å¯¼çè§é¢çæé¢åå¸¦æ¥äºéå¤§çªç ´ãå®ä¸ä»æ¾èæé«äºçæè§é¢ä¸­å¯¹è±¡äº¤äºççå®æ§åç©çä¸è´æ§ï¼è¿ä¸ºæå»ºæ´å¯é çä¸çæ¨¡ååæ¯ææºå¨äººææ¯ãå·èº«å³ç­ç­åºç¨å¥ å®äºåºç¡ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this
limitation, we introduce KineMask, an approach for physics-guided video
generation that enables realistic rigid body control, interactions, and
effects.</li>
<li>Given a single image and a specified object velocity, our method
generates videos with inferred motions and future object interactions.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.02284v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.02284v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.02283v1'></a></p>
<h2 id="self-forcing-towards-minute-scale-high-quality-video-generation"><a href="https://arxiv.org/abs/2510.02283v1">Self-Forcing++: Towards Minute-Scale High-Quality Video Generation</a></h2>
<p><strong>Authors:</strong> Justin Cui, Jie Wu, Ming Li, Tao Yang, Xiaojie Li, Rui Wang, Andrew Bai, Yuanhao Ban, Cho-Jui Hsieh</p>
<p><strong>Published:</strong> 2025-10-02</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>Diffusion models have revolutionized image and video generation, achieving
unprecedented visual quality. However, their reliance on transformer
architectures incurs prohibitively high computational costs, particularly when
extending generation to long videos. Recent work has explored autoregressive
formulations for long video generation, typically by distilling from
short-horizon bidirectional teachers. Nevertheless, given that teacher models
cannot synthesize long videos, the extrapolation of student models beyond their
training horizon often leads to pronounced quality degradation, arising from
the compounding of errors within the continuous latent space. In this paper, we
propose a simple yet effective approach to mitigate quality degradation in
long-horizon video generation without requiring supervision from long-video
teachers or retraining on long video datasets. Our approach centers on
exploiting the rich knowledge of teacher models to provide guidance for the
student model through sampled segments drawn from self-generated long videos.
Our method maintains temporal consistency while scaling video length by up to
20x beyond teacher's capability, avoiding common issues such as over-exposure
and error-accumulation without recomputing overlapping frames like previous
methods. When scaling up the computation, our method shows the capability of
generating videos up to 4 minutes and 15 seconds, equivalent to 99.9% of the
maximum span supported by our base model's position embedding and more than 50x
longer than that of our baseline model. Experiments on standard benchmarks and
our proposed improved benchmark demonstrate that our approach substantially
outperforms baseline methods in both fidelity and consistency. Our long-horizon
videos demo can be found at https://self-forcing-plus-plus.github.io/</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼è¿æ¯ä¸ç¯å³äºâSelf-Forcing++: Towards Minute-Scale High-Quality Video Generationâè®ºæçå¨é¢æè¦ï¼åå®¹æ¶µçäºæ¨è¦æ±çææè¦ç¹ï¼</p>
<p><strong>è®ºææè¦ï¼Self-Forcing++ï¼è¿ååéçº§é«è´¨éè§é¢çæ</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³æ©æ£æ¨¡åå¨çæé¿è§é¢æ¶é¢ä¸´çè´¨ééåé®é¢ãå°½ç®¡æ©æ£æ¨¡åå¨å¾ååç­è§é¢çææ¹é¢åå¾äºåææªæçæåï¼ä½ç±äºå¶å¯¹Transformeræ¶æçä¾èµï¼å°çææ©å±å°é¿è§é¢ä¼å¸¦æ¥é«æçè®¡ç®ææ¬ãç°ææ¹æ³éå¸¸éè¿ä»ç­æ¶ç¨ååæå¸æ¨¡åä¸­æåç¥è¯æ¥çæé¿è§é¢ï¼ä½ç±äºæå¸æ¨¡åæ æ³åæé¿è§é¢ï¼å­¦çæ¨¡åè¶åºå¶è®­ç»èå´çæ¨æ­å¾å¾ä¼å¯¼è´è¿ç»­æ½å¨ç©ºé´ä¸­éè¯¯ç´¯ç§¯ï¼ä»èé ææ¾èçè´¨ééåã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
Self-Forcing++ æåºäºä¸ç§ç®åèææçæ¹æ³æ¥ç¼è§£é¿æ¶ç¨è§é¢çæä¸­çè´¨ééåï¼èæ éé¿è§é¢æå¸ççç£æå¨é¿è§é¢æ°æ®éä¸éæ°è®­ç»ãå¶æ ¸å¿åæ°å¨äºï¼
*   <strong>å©ç¨æå¸æ¨¡åç¥è¯æå¯¼å­¦çæ¨¡åï¼</strong> éè¿ä»èªçæçé¿è§é¢ä¸­éæ ·ççæ®µï¼å©ç¨æå¸æ¨¡åçä¸°å¯ç¥è¯æ¥æå¯¼å­¦çæ¨¡åã
*   <strong>åååªå£°åå§åï¼Backwards Noise Initializationï¼ï¼</strong> å¼å¥åååªå£°åå§åç­ç¥ï¼éè¿å°åªå£°éæ°æ³¨å¥å»åªåçæ½å¨åéä½ä¸ºèµ·å§åªå£°ï¼ä»¥ç¡®ä¿çæè½¨è¿¹ä¿ææ¶é´ä¸è´æ§ï¼å¹¶æ­£ç¡®ç»æåãè¿æå©äºç¼è§£é¿è§é¢çæä¸­çä¸ä¸æéä½é®é¢ã
*   <strong>æ©å±åå¸å¹éè¸é¦ï¼Extended Distribution Matching Distillationï¼ï¼</strong> å°è®­ç»æç»­æ¶é´æ©å±å°è¿è¶æå¸æ¨¡åéå¶çå¸§æ°ï¼ä¾å¦ï¼100ç§ï¼ï¼å¹¶å¨çæçåºåä¸­ååéæ ·ä¸ä¸ªåºå®é¿åº¦ççªå£ï¼è®¡ç®å­¦çåæå¸æ¨¡åä¹é´çåå¸å·®å¼ãè¿ç§æ»å¨çªå£è¸é¦è¿ç¨ä½¿å­¦çæ¨¡åè½å¤ä»éåç¶æä¸­æ¢å¤ï¼å¹¶ç»´æé«è´¨éãè¿è´¯çè§é¢çæã
*   <strong>æ»å¨KVç¼å­è®­ç»ï¼</strong> å¨è®­ç»åæ¨çè¿ç¨ä¸­é½éç¨æ»å¨KVç¼å­ï¼èªç¶æ¶é¤äºè®­ç»-æ¨çä¸å¹éé®é¢ï¼é¿åäºä¼ ç»æ¹æ³ä¸­éå å¸§çéæ°è®¡ç®ææ½å¨å¸§æ©è½ã
*   <strong>éè¿GRPOæ¹è¿é¿æå¹³æ»æ§ï¼</strong> å½åºç°æ¶é´ä¸ä¸è´ï¼å¦ç©ä½çªç¶åºç°ææ¶å¤±ãåºæ¯è¿æ¸¡ä¸èªç¶ï¼æ¶ï¼å©ç¨å¼ºåå­¦ä¹ ææ¯ââç¾¤ç»ç¸å¯¹ç­ç¥ä¼åï¼GRPOï¼æ¥å¢å¼ºæ¶é´å¹³æ»æ§ï¼éè¿åå­¦æµçç¸å¯¹å¤§å°ä½ä¸ºè¿å¨è¿ç»­æ§çä»£çæ¥æå¯¼ä¼åã
*   <strong>æ°çè¯ä¼°ææ ï¼Visual Stabilityï¼ï¼</strong> æåºäºä¸ç§æ°çææ âè§è§ç¨³å®æ§âï¼æ¨å¨ç³»ç»å°ææé¿è§é¢çæä¸­çè´¨ééååè¿åº¦æåé®é¢ï¼ä»¥è§£å³ç°æVBenchåºåçåå·®ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>æ¾èçè§é¢é¿åº¦æ©å±ï¼</strong> Self-Forcing++ è½å¤å°è§é¢é¿åº¦æ©å±å°æå¸æ¨¡åè½åç20åä»¥ä¸ï¼æé¿å¯è¾¾4åé15ç§ï¼ç¸å½äºå¶åºç¡æ¨¡åä½ç½®åµå¥æ¯æçæå¤§èå´ç99.9%ï¼æ¯åºçº¿æ¨¡åé¿50åä»¥ä¸ã
*   <strong>åè¶çæ§è½ï¼</strong> å¨æ ååºååæåºçæ¹è¿åºåä¸ï¼è¯¥æ¹æ³å¨ä¿çåº¦åä¸è´æ§æ¹é¢åæ¾èä¼äºåºçº¿æ¹æ³ã
*   <strong>ä¿ææ¶é´ä¸è´æ§ï¼</strong> å¨æ©å±è§é¢é¿åº¦çåæ¶ï¼é¿åäºè¿åº¦æååéè¯¯ç´¯ç§¯ç­å¸¸è§é®é¢ï¼æ éåä»¥åçæ¹æ³é£æ ·éæ°è®¡ç®éå å¸§ã
*   <strong>è®­ç»é¢ç®æ©å±è½åï¼</strong> å®éªè¡¨æï¼éè¿æ©å±è®­ç»é¢ç®ï¼æ¨¡åè½å¤çæé«è´¨éãé¿æç»­æ¶é´çè§é¢ï¼ä»èè§é¿äºå¯¹å¤§è§æ¨¡çå®è§é¢æ°æ®éçä¾èµã</p>
<p><strong>4. è®ºæä¸­æå°çå±éæ§ï¼</strong>
*   <strong>è®­ç»éåº¦è¾æ¢ï¼</strong> ç¸è¾äºæå¸å¼ºå¶ï¼teacher-forcingï¼æ¹æ³ï¼Self-Forcing++ çè®­ç»éåº¦è¾æ¢ã
*   <strong>ç¼ºä¹é¿æè®°å¿ï¼</strong> å°½ç®¡æææ¹è¿ï¼ä½ä»å­å¨é¿æè®°å¿ä¸è¶³çé®é¢ï¼å¯è½å¯¼è´å¨é¿æ¶é´é®æ¡åºååºç°åå®¹åæ­§ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>å¹¶è¡åè®­ç»è¿ç¨ï¼</strong> æ¢ç´¢å¹¶è¡åè®­ç»è¿ç¨ä»¥è§£å³èªçæï¼self-rolloutï¼çé«è®­ç»ææ¬é®é¢ã
*   <strong>æ§å¶æ½å¨åéçä¿çåº¦ï¼</strong> ç ç©¶æ§å¶æ½å¨åéä¿çåº¦çææ¯ï¼ä¾å¦éåå­å¨å¨KVç¼å­ä¸­çæ½å¨è¡¨ç¤ºï¼æå¯¹KVç¼å­è¿è¡å½ä¸åä»¥é²æ­¢åå¸åç§»ã
*   <strong>æ´åé¿æè®°å¿æºå¶ï¼</strong> å°é¿æè®°å¿æºå¶æ´åå°èªåå½æ¡æ¶ä¸­ï¼ä»¥å®ç°çæ­£çé¿è·ç¦»æ¶é´è¿è´¯æ§ã</p>
<p>æ»èè¨ä¹ï¼Self-Forcing++ ä¸ºé«è´¨éé¿è§é¢çææä¾äºä¸ä¸ªåæ°ä¸å®ç¨çè§£å³æ¹æ¡ï¼éè¿å·§å¦å°å©ç¨æå¸æ¨¡åçç¥è¯åèªçæè¿ç¨ä¸­çéè¯¯æ ¡æ­£ï¼æ¾èæ©å±äºè§é¢çæçæ¶é´èå´ï¼å¹¶æåäºçæåå®¹çè´¨éåä¸è´æ§ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Our approach centers on
exploiting the rich knowledge of teacher models to provide guidance for the
student model through sampled segments drawn from self-generated long videos.</li>
<li>Our method maintains temporal consistency while scaling video length by up to
20x beyond teacher's capability, avoiding common issues such as over-exposure
and error-accumulation without recomputing overlapping frames like previous
methods.</li>
<li>When scaling up the computation, our method shows the capability of
generating videos up to 4 minutes and 15 seconds, equivalent to 99.9% of the
maximum span supported by our base model's position embedding and more than 50x
longer than that of our baseline model.</li>
<li>Experiments on standard benchmarks and
our proposed improved benchmark demonstrate that our approach substantially
outperforms baseline methods in both fidelity and consistency.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.02283v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.02283v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.02282v1'></a></p>
<h2 id="vidguard-r1-ai-generated-video-detection-and-explanation-via-reasoning-mllms-and-rl"><a href="https://arxiv.org/abs/2510.02282v1">VidGuard-R1: AI-Generated Video Detection and Explanation via Reasoning MLLMs and RL</a></h2>
<p><strong>Authors:</strong> Kyoungjun Park, Yifan Yang, Juheon Yi, Shicheng Zheng, Yifei Shen, Dongqi Han, Caihua Shan, Muhammad Muaz, Lili Qiu</p>
<p><strong>Published:</strong> 2025-10-02</p>
<p><strong>Categories:</strong> cs.CV, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>With the rapid advancement of AI-generated videos, there is an urgent need
for effective detection tools to mitigate societal risks such as misinformation
and reputational harm. In addition to accurate classification, it is essential
that detection models provide interpretable explanations to ensure transparency
for regulators and end users. To address these challenges, we introduce
VidGuard-R1, the first video authenticity detector that fine-tunes a
multi-modal large language model (MLLM) using group relative policy
optimization (GRPO). Our model delivers both highly accurate judgments and
insightful reasoning. We curate a challenging dataset of 140k real and
AI-generated videos produced by state-of-the-art generation models, carefully
designing the generation process to maximize discrimination difficulty. We then
fine-tune Qwen-VL using GRPO with two specialized reward models that target
temporal artifacts and generation complexity. Extensive experiments demonstrate
that VidGuard-R1 achieves state-of-the-art zero-shot performance on existing
benchmarks, with additional training pushing accuracy above 95%. Case studies
further show that VidGuard-R1 produces precise and interpretable rationales
behind its predictions. The code is publicly available at
https://VidGuard-R1.github.io.</p>
<p><strong>Analysis:</strong></p>
<p>ä»¥ä¸æ¯Kyoungjun Parkç­äººæ°åçè®ºæâVidGuard-R1: AI-Generated Video Detection and Explanation via Reasoning MLLMs and RLâçå¨é¢æè¦ï¼</p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢</strong>
éçAIçæè§é¢çå¿«éåå±ï¼ç¤¾ä¼é¢ä¸´çèåä¿¡æ¯åå£°èªæå®³ç­é£é©ãå æ­¤ï¼è¿«åéè¦ææçæ£æµå·¥å·æ¥è¯å«AIçæè§é¢ãé¤äºåç¡®åç±»ï¼è¿äºæ£æµæ¨¡åè¿éè¦æä¾å¯è§£éçè§£éï¼ä»¥ç¡®ä¿çç®¡æºæåæç»ç¨æ·çéæåº¦ãç°ææ¹æ³å¨æ£æµææ°çææ¨¡åäº§ççè§é¢æ¶è¡¨ç°ä¸ä½³ï¼ä¸éå¸¸åªæä¾äºåå¤æ­ï¼ç¼ºä¹å¯è§£éçæ¨çè½åã</p>
<p><strong>2. ä¸»è¦åæ°ææ¹æ³è®ºè´¡ç®</strong>
VidGuard-R1æ¯é¦ä¸ªéè¿æ¨çå¤æ¨¡æå¤§åè¯­è¨æ¨¡åï¼MLLMsï¼åå¼ºåå­¦ä¹ ï¼RLï¼å®ç°AIçæè§é¢æ£æµåè§£éçæ¡æ¶ãå¶ä¸»è¦åæ°åæ¬ï¼
*   <strong>é¦ä¸ªåºäºMLLMçè§é¢çå®æ§æ£æµå¨</strong>ï¼VidGuard-R1éè¿ä½¿ç¨ç¾¤ç»ç¸å¯¹ç­ç¥ä¼åï¼GRPOï¼å¯¹MLLMè¿è¡å¾®è°ï¼å®ç°äºé«ç²¾åº¦å¤æ­åå¯ææ´å¯åçæ¨çã
*   <strong>æææ§æ°æ®éçæå»º</strong>ï¼ä½èç²¾å¿ç­åäºä¸ä¸ªåå«14ä¸çå®åAIçæè§é¢çæææ§æ°æ®éï¼è¿äºè§é¢ç±æåè¿ççææ¨¡åçæï¼å¹¶ç»è¿ç²¾å¿è®¾è®¡ä»¥æå¤§ååºåé¾åº¦ï¼é¿åæ¨¡åä¾èµåè¾¨çãå¸§çç­è¡¨é¢çº¿ç´¢ã
*   <strong>ä¸é¨çå¥å±æ¨¡å</strong>ï¼ä¸ºäºå¢å¼ºæ§è½ï¼VidGuard-R1å¨GRPOå¾®è°Qwen-VLæ¶å¼å¥äºä¸¤ä¸ªä¸é¨çå¥å±æ¨¡åï¼
    *   <strong>GRPO-TAï¼Temporal Artifactsï¼</strong>ï¼éè¿å¼å¥æ¶é´ä¼ªå½±ï¼éå¤æåè½¬è§é¢çæ®µï¼å¹¶æ ¹æ®æ£æµé¾åº¦è°æ´å¥å±ï¼æ¾å¼ä¿è¿æ¶é´æ¨çã
    *   <strong>GRPO-Qï¼Quality Evolutionary Videosï¼</strong>ï¼éè¿ç³»ç»å°æ¹åéåæ©æ£æ­¥æ°æ¥çæä¸åè´¨éçè§é¢ï¼å¹¶æ ¹æ®é¢æµä¸çå®æ©æ£æ­¥æ°çè·ç¦»åéé¨åå¥å±ï¼ä½¿æ¨¡åè½å¤è¿è¡ç»ç²åº¦çè§é¢è´¨éåæã
*   <strong>ä¸¤é¶æ®µè®­ç»æ¡æ¶</strong>ï¼é¦åè¿è¡çç£å¾®è°ï¼SFTï¼ä»¥åå§åé¾å¼æèï¼CoTï¼è½åï¼ç¶åè¿è¡åºäºå¼ºåå­¦ä¹ çå¾®è°ï¼DPOåGRPOï¼ä»¥å®ç°æ´æ·±å±æ¬¡çæ¨çã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹</strong>
*   <strong>åè¶çæ£æµæ§è½</strong>ï¼VidGuard-R1å¨ç°æåºåæµè¯ä¸å®ç°äºæåè¿çé¶æ ·æ¬æ§è½ï¼åç¡®çè¶è¿95%ãç»è¿é¢å¤è®­ç»åï¼å¶åç¡®çè¿ä¸æ­¥æåãå¨ä½èæå»ºçæææ§æ°æ®éä¸ï¼VidGuard-R1çåç¡®çè¾¾å°çº¦85%ã
*   <strong>å¯è§£éçæ¨ç</strong>ï¼æ¡ä¾ç ç©¶è¡¨æï¼VidGuard-R1è½å¤ä¸ºå¶é¢æµæä¾ç²¾ç¡®ä¸å¯è§£éççç±ï¼éè¿å¤æ¹é¢æ¨çï¼è¿å¨ä¸è´æ§ãåç§ä¸è´æ§ãçº¹çä¼ªå½±åç©çåçæ§ï¼æ¥è¯å«AIçæè§é¢ï¼èéä¾èµåä¸çº¿ç´¢ã
*   <strong>æ³åè½åå¼º</strong>ï¼é¶æ ·æ¬æ¨¡åå¨GenVidBenchä¸è¡¨ç°åºå¼ºå¤§çæ³åè½åï¼çªæ¾äºå¨å¤æ ·åçæåå®¹ä¸è¿è¡é¢è®­ç»çæææ§ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§</strong>
*   <strong>æ°æ®éå±éæ§</strong>ï¼ç®åçæ°æ®éä»åå«ç±HunyuanVideoåCogVideoXçæçèåè§é¢ãå°½ç®¡æ°æ®ééç¨éå¯¹æ¹å¼æå»ºä»¥ç¡®ä¿çå®åèåè§é¢ä¹é´çä¸ä¸æç¸ä¼¼æ§ï¼ä½å¶æ³åè½åä»æéã
*   <strong>çææ¨¡åèå´</strong>ï¼æ°æ®éæªè½æ¶µçæ´å¹¿æ³ççææ¨¡åï¼è¿éå¶äºæ¨¡åå¨ç°å®ä¸çåºæ¯ä¸­çéç¨æ§ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹å</strong>
*   <strong>æ©å±æ°æ®é</strong>ï¼å°æ°æ®éæ©å±å°åå«æ´å¹¿æ³ççææ¨¡åæäº§ççèåè§é¢ï¼ä»¥æå»ºæ´å·å¤æ ·æ§åé²æ£æ§çè®­ç»éï¼ä»èæé«æ¨¡åå¨ç°å®ä¸çåºæ¯ä¸­çéç¨æ§ã
*   <strong>å¢å¼ºMLLMæ¨çè½å</strong>ï¼è¿ä¸æ­¥ç ç©¶å¦ä½éè¿å¼ºåå­¦ä¹ ç­æ¹æ³å å¼ºMLLMsçæ¨çè½åï¼ä»¥åºå¯¹æ´å¤æãæ´ç»å¾®çAIçæè§é¢æ£æµææã
*   <strong>å¤æ¨¡æè§é¢åæ</strong>ï¼ç»§ç»­æ¨å¨åºäºMLLMçè§é¢åæï¼ä»¥æ´å¥½å°çè§£åè§£éAIçæè§é¢çå¤æç¹å¾ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address these challenges, we introduce
VidGuard-R1, the first video authenticity detector that fine-tunes a
multi-modal large language model (MLLM) using group relative policy
optimization (GRPO).</li>
<li>We curate a challenging dataset of 140k real and
AI-generated videos produced by state-of-the-art generation models, carefully
designing the generation process to maximize discrimination difficulty.</li>
<li>Extensive experiments demonstrate
that VidGuard-R1 achieves state-of-the-art zero-shot performance on existing
benchmarks, with additional training pushing accuracy above 95%.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.02282v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.02282v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.02270v1'></a></p>
<h2 id="microclip-unsupervised-clip-adaptation-via-coarse-fine-token-fusion-for-fine-grained-image-classification"><a href="https://arxiv.org/abs/2510.02270v1">microCLIP: Unsupervised CLIP Adaptation via Coarse-Fine Token Fusion for Fine-Grained Image Classification</a></h2>
<p><strong>Authors:</strong> Sathira Silva, Eman Ali, Chetan Arora, Muhammad Haris Khan</p>
<p><strong>Published:</strong> 2025-10-02</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>Unsupervised adaptation of CLIP-based vision-language models (VLMs) for
fine-grained image classification requires sensitivity to microscopic local
cues. While CLIP exhibits strong zero-shot transfer, its reliance on coarse
global features restricts its performance on fine-grained classification tasks.
Prior efforts inject fine-grained knowledge by aligning large language model
(LLM) descriptions with the CLIP <script type="math/tex">\texttt{[CLS]}</script> token; however, this approach
overlooks spatial precision. We propose <script type="math/tex">\textbf{microCLIP}</script>, a self-training
framework that jointly refines CLIP's visual and textual representations using
fine-grained cues. At its core is Saliency-Oriented Attention Pooling (SOAP)
within a lightweight TokenFusion module, which builds a saliency-guided
<script type="math/tex">\texttt{[FG]}</script> token from patch embeddings and fuses it with the global
<script type="math/tex">\texttt{[CLS]}</script> token for coarse-fine alignment. To stabilize adaptation, we
introduce a two-headed LLM-derived classifier: a frozen classifier that, via
multi-view alignment, provides a stable text-based prior for pseudo-labeling,
and a learnable classifier initialized from LLM descriptions and fine-tuned
with TokenFusion. We further develop Dynamic Knowledge Aggregation, which
convexly combines fixed LLM/CLIP priors with TokenFusion's evolving logits to
iteratively refine pseudo-labels. Together, these components uncover latent
fine-grained signals in CLIP, yielding a consistent <script type="math/tex">2.90\%</script> average accuracy
gain across 13 fine-grained benchmarks while requiring only light adaptation.
Our code is available at https://github.com/sathiiii/microCLIP.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼è¿æ¯ä¸ç¯å³äºSathira Silva, Eman Ali, Chetan Arora, Muhammad Haris Khanæ°åçè®ºæâmicroCLIP: Unsupervised CLIP Adaptation via Coarse-Fine Token Fusion for Fine-Grained Image Classificationâçå¨é¢æè¦ï¼</p>
<p><strong>è®ºææè¦ï¼microCLIPï¼éè¿ç²ç»ç²åº¦Tokenèåå®ç°æ çç£CLIPéåºï¼ç¨äºç»ç²åº¦å¾ååç±»</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³CLIPï¼Contrastive Language-Image Pre-trainingï¼æ¨¡åå¨ç»ç²åº¦å¾ååç±»ä»»å¡ä¸­è¿è¡æ çç£éåºï¼Unsupervised Adaptation, UAï¼æ¶é¢ä¸´çææãå°½ç®¡CLIPå¨é¶æ ·æ¬è¿ç§»æ¹é¢è¡¨ç°åºè²ï¼ä½å¶ä¸»è¦ä¾èµç²ç²åº¦çå¨å±ç¹å¾ï¼éå¸¸æ¯<code>[CLS]</code> tokenï¼éå¶äºå¶ææç»å¾®å±é¨çº¿ç´¢çè½åï¼èè¿äºçº¿ç´¢å¯¹äºåºåé«åº¦ç¸ä¼¼çç»ç²åº¦ç±»å«è³å³éè¦ãç°ææ¹æ³å°è¯éè¿å°å¤§åè¯­è¨æ¨¡åï¼LLMï¼æè¿°ä¸<code>[CLS]</code> tokenå¯¹é½æ¥æ³¨å¥ç»ç²åº¦ç¥è¯ï¼ä½å¾å¾å¿½ç¥äºç©ºé´ç²¾åº¦ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
microCLIPæåºäºä¸ä¸ªèªè®­ç»æ¡æ¶ï¼éè¿ä»¥ä¸å³é®åæ°å±åä¼åCLIPçè§è§åææ¬è¡¨ç¤ºï¼</p>
<ul>
<li><strong>Saliency-Oriented Attention Pooling (SOAP) å TokenFusion æ¨¡åï¼</strong> è¿æ¯microCLIPçæ ¸å¿ãSOAPæºå¶å¨ä¸ä¸ªè½»éçº§çTokenFusionæ¨¡ååï¼éè¿å¯¹CLIPçpatch embeddingsè¿è¡æ¾èæ§å¼å¯¼çæ³¨æåæ± åï¼çæä¸ä¸ªç´§åç<code>[FG]</code>ï¼fine-grainedï¼tokenãç¶åï¼è¿ä¸ª<code>[FG]</code> tokenä¸å¨å±<code>[CLS]</code> tokenèåï¼å®ç°ç²ç»ç²åº¦å¯¹é½ï¼ä»èå¨ä¿çå¨å±ä¸ä¸æçåæ¶ï¼æ³¨å¥å±é¨ãç»ç²åº¦çè§è§ä¿¡æ¯ã</li>
<li><strong>åå¤´LLMæ´¾çåç±»å¨ï¼</strong> ä¸ºäºç¨³å®éåºè¿ç¨ï¼microCLIPå¼å¥äºä¸ä¸ªåå¤´åç±»å¨ï¼<ul>
<li>ä¸ä¸ª<strong>å»ç»çåç±»å¨</strong>ï¼<script type="math/tex">W_{LLM}</script>ï¼ï¼éè¿å¤è§å¾å¯¹é½æä¾ç¨³å®çåºäºææ¬çåéªç¥è¯ï¼ç¨äºä¼ªæ ç­¾çæã</li>
<li>ä¸ä¸ª<strong>å¯å­¦ä¹ çåç±»å¨</strong>ï¼<script type="math/tex">W_{LLM}^*</script>ï¼ï¼å®ä»LLMæè¿°åå§åï¼å¹¶ä¸TokenFusionæ¨¡åä¸èµ·è¿è¡å¾®è°ã</li>
</ul>
</li>
<li><strong>å¨æç¥è¯èåï¼Dynamic Knowledge Aggregationï¼ï¼</strong> è®ºæå¼åäºä¸ç§è¿­ä»£çä¼ªæ ç­¾æ¹æ¡ï¼å®å¸ç»åäºåºå®çLLM/CLIPåéªç¥è¯ï¼éè¿å¤è§å¾å¯¹é½è·å¾ï¼ä¸TokenFusionæ¨¡åä¸æ­æ¼è¿çlogitsï¼ä»èç¨³å®èèªéåºå°ç»åä¼ªæ ç­¾ï¼ä»¥å®ç°ç»ç²åº¦åºåã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
microCLIPå¨13ä¸ªç»ç²åº¦åºåæµè¯ä¸­åå¾äºæ¾èçæ§è½æåï¼å¹³ååç¡®çæé«äº2.90%ï¼åæ¶ä»éè¦è½»éçº§çéåºãå·ä½æ¥è¯´ï¼</p>
<ul>
<li><strong>ç»ç²åº¦æ§è½æåï¼</strong> microCLIPå¨CarsãRESISCãUCF101ãCIFAR100åDTDç­æ°æ®éä¸åå®ç°äºæ¾èçåç¡®çæåï¼å°¤å¶æ¯å¨FGVCæ°æ®éä¸ï¼å¶æ§è½æåäº+2.64%ï¼Carsæ°æ®éä¸æåäº+8.98%ã</li>
<li><strong>æ¾èæ§å¼å¯¼çæ³¨æåï¼</strong> è®ºæéè¿å¯è§åå±ç¤ºï¼SOAPæºå¶è½å¤ä¸è´å°çªåºç±»å«å®ä¹çå±é¨è¯­ä¹åºåï¼ä¾å¦ï¼é¸ç±»çç¹å®èº«ä½é¨ä½ãæ£çåºçååºå¸å±ï¼ï¼è¿è¡¨æå¶ææææäºç»ç²åº¦çº¿ç´¢ã</li>
<li><strong>è¶è¶ç°ææ¹æ³ï¼</strong> microCLIPå¨ViT-B/32éª¨å¹²ç½ç»ä¸ï¼æ´ä½åç¡®çè¾¾å°68.68%ï¼è¶è¶äºæå¼ºçæ çç£éåºæ¹æ³DPAï¼å¹¶ä¼äºé¶æ ·æ¬åå°æ ·æ¬åºçº¿ãå¨ä½¿ç¨ViT-B/16éª¨å¹²ç½ç»æ¶ï¼microCLIPç¸å¯¹äºDPAçæ§è½æåæ´æ¯é«è¾¾5.36%ã</li>
<li><strong>è½»éçº§éåºï¼</strong> è¯¥æ¹æ³ä»éè½»éçº§éåºï¼è¯æäºå¶å¨å®éåºç¨ä¸­çé«ææ§ã</li>
</ul>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
è®ºææåºmicroCLIPå¨æäºåºæ¯ä¸å­å¨å±éæ§ï¼ç¹å«æ¯å¨éè¦å±é¨åå¨å±ä¿¡æ¯ä¹é´è¿è¡ä»ç»å¹³è¡¡æ¶ï¼</p>
<ul>
<li><strong>å¯¹ç²ç²åº¦ãç©ºé´æ©æ£ç¹å¾çä¾èµï¼</strong> å½æ°æ®éä¸»è¦åå«ç²ç²åº¦ãç©ºé´æ©æ£çç¹å¾ï¼èéå±é¨ãç»ç²åº¦çº¿ç´¢æ¶ï¼ä¾å¦DTDåFlowersæ°æ®éï¼ï¼microCLIPçä¼ªæ ç­¾çæå¨å®å¨ä¾èµäºæ¨¡åèªèº«è¿è¡å¾®è°ï¼è¿å¯è½å¯¼è´æ§è½åéãå¨è¿ç§æåµä¸ï¼ç»ç²åº¦ä¸ç²ç²åº¦tokené¢æµçå¯¹ç§°èåå¯è½ä¼æ æä¸­å¼å¥å±é¨ç©ºé´åå·®ï¼ä¸æ°æ®çæ´ä½ç»æå¯¹é½ä¸ä½³ã</li>
<li><strong>èåç­ç¥ççµæ´»æ§ï¼</strong> è®ºæåç°ï¼å¨æäºæ°æ®éä¸ï¼DPAï¼ä¸ç§åºäºååå¯¹é½çæ¹æ³ï¼å¨éå¤å¯å­¦ä¹ çGPT-3æ´¾çåç±»å¨æ¶ï¼æ§è½ä¸microCLIPç¸å½çè³è¶è¶ãè¿è¡¨æTokenFusionæ¨¡åå¯è½åçäºæ´çµæ´»çèåç­ç¥ï¼ä¾å¦ç²ç²åº¦ä¸ç»ç²åº¦é¢æµä¹é´çèªéåºå ææºå¶ï¼èéå½åçå¯¹ç§°èåæ¹æ¡ã</li>
</ul>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
åºäºä¸è¿°å±éæ§ï¼è®ºææåºäºä»¥ä¸æªæ¥ç ç©¶æ¹åï¼</p>
<ul>
<li><strong>èªéåºå æèåæºå¶ï¼</strong> æ¢ç´¢ä¸ç§æ´çµæ´»çèåç­ç¥ï¼ä¾å¦å¨ç²ç²åº¦ä¸ç»ç²åº¦é¢æµä¹é´å¼å¥èªéåºå ææºå¶ï¼ä»¥æ´å¥½å°å¹³è¡¡å±é¨åå¨å±ä¿¡æ¯ï¼å°¤å¶æ¯å¨ç¹å¾ç©ºé´åå¸å¹¿æ³çæ°æ®éä¸ã</li>
<li><strong>ç»åå¾åååï¼</strong> èèå¨åç±»å¨æå»ºä¸­æç¡®å©ç¨å¾åååï¼ç±»ä¼¼äºDPAï¼è¿å¯è½æå©äºè¿ä¸æ­¥æåæ§è½ï¼ç¹å«æ¯å¨MetaCLIPç­é¢è®­ç»æ°æ®è´¨éé«ä¸è§æ¨¡å¤§çæåµä¸ã</li>
</ul>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose <script type="math/tex">\textbf{microCLIP}</script>, a self-training
framework that jointly refines CLIP's visual and textual representations using
fine-grained cues.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.02270v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.02270v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.02268v1'></a></p>
<h2 id="do-you-know-where-your-camera-is-view-invariant-policy-learning-with-camera-conditioning"><a href="https://arxiv.org/abs/2510.02268v1">Do You Know Where Your Camera Is? View-Invariant Policy Learning with Camera Conditioning</a></h2>
<p><strong>Authors:</strong> Tianchong Jiang, Jingtian Ji, Xiangshan Tan, Jiading Fang, Anand Bhattad, Vitor Guizilini, Matthew R. Walter</p>
<p><strong>Published:</strong> 2025-10-02</p>
<p><strong>Categories:</strong> cs.RO, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We study view-invariant imitation learning by explicitly conditioning
policies on camera extrinsics. Using Plucker embeddings of per-pixel rays, we
show that conditioning on extrinsics significantly improves generalization
across viewpoints for standard behavior cloning policies, including ACT,
Diffusion Policy, and SmolVLA. To evaluate policy robustness under realistic
viewpoint shifts, we introduce six manipulation tasks in RoboSuite and
ManiSkill that pair "fixed" and "randomized" scene variants, decoupling
background cues from camera pose. Our analysis reveals that policies without
extrinsics often infer camera pose using visual cues from static backgrounds in
fixed scenes; this shortcut collapses when workspace geometry or camera
placement shifts. Conditioning on extrinsics restores performance and yields
robust RGB-only control without depth. We release the tasks, demonstrations,
and code at https://ripl.github.io/know_your_camera/ .</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼è¿æ¯ä¸ç¯å³äºâDo You Know Where Your Camera Is? View-Invariant Policy Learning with Camera Conditioningâè®ºæçä¸­ææè¦ï¼ç±Tianchong Jiangç­äººæ°åï¼</p>
<p><strong>è®ºææè¦ï¼ä½ ç¥éä½ çç¸æºå¨åªéåï¼åºäºç¸æºæ¡ä»¶åçè§è§ä¸åç­ç¥å­¦ä¹ </strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³æºå¨äººæ¨¡ä»¿å­¦ä¹ ä¸­çä¸ä¸ªæ ¸å¿ææï¼å¦ä½ä½¿æºå¨äººç­ç¥å¯¹ç¸æºè§è§çæ¹åå·æé²æ£æ§ï¼å³å®ç°âè§è§ä¸åæ§âãä¼ ç»çæ¨¡ä»¿å­¦ä¹ ç­ç¥éå¸¸å¨åºå®è§è§ä¸è®­ç»ï¼å½ç¸æºä½ç½®å¨é¨ç½²æ¶åçååæ¶ï¼è¿äºç­ç¥å¾å¾ä¼å¤±æãè®ºæçæ ¸å¿é®é¢æ¯ï¼æºå¨äººç­ç¥æ¯åºè¯¥å¨è®­ç»è¿ç¨ä¸­éå¼å°æ¨æ­ç¸æºå ä½ä¿¡æ¯ï¼è¿æ¯åºè¯¥å¨ç¸æºå ä½ä¿¡æ¯å¯ç¨æ¶æ¾å¼å°å¯¹å¶è¿è¡æ¡ä»¶åï¼</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
*   <strong>æ°é¢çç¸æºæ¡ä»¶åæ¹æ³ï¼</strong> è®ºææåºäºä¸ç§ææçæ¹æ³ï¼éè¿ä½¿ç¨åç´ çº§PlÃ¼ckeråµå¥ï¼per-pixel PlÃ¼cker embeddingsï¼æ¥æ¾å¼å°å°ç¸æºå ä½ä¿¡æ¯ä½ä¸ºæ¡ä»¶å¼å¥å°æ¨¡ä»¿å­¦ä¹ ç­ç¥ä¸­ãPlÃ¼ckeråæ ç¨äºè¡¨ç¤ºç©ºé´ä¸­çæåçº¿ï¼å·æå¯¹æ²¿å°çº¿åç¹éæ©ä¸åçä¼å¿ã
*   <strong>ä¸¤ç§ç¼ç PlÃ¼ckerå°çº¿å¾çæ¹å¼ï¼</strong> éå¯¹æ¯å¦ä½¿ç¨é¢è®­ç»è§è§ç¼ç å¨ï¼è®ºææåºäºä¸¤ç§éæPlÃ¼ckerå°çº¿å¾çæ¹æ³ï¼
    *   å¯¹äºæ²¡æé¢è®­ç»ç¼ç å¨çç­ç¥ï¼å¦Diffusion Policyï¼ï¼PlÃ¼ckerå¾ä¸å¾åè¿è¡ééçº§èï¼å¹¶ä¿®æ¹å¾åç¼ç å¨çç¬¬ä¸å±ä»¥æ¥å9ééè¾å¥ã
    *   å¯¹äºå¸¦æé¢è®­ç»ç¼ç å¨çç­ç¥ï¼ä½¿ç¨ä¸ä¸ªå°åå·ç§¯ç½ç»å°PlÃ¼ckerå¾ç¼ç å°ä¸å¾åæ½å¨ç¹å¾ç¸åçç»´åº¦ï¼ç¶åè¿è¡ééçº§èã
*   <strong>æ°çè§è§æ³ååºåï¼</strong> è®ºæå¼å¥äºå­ä¸ªæ°çæä½ä»»å¡ï¼RoboSuiteä¸­çLift, Pick Place Can, Assembly SquareåManiSkillä¸­çPush, Lift Upright, Roll Ballï¼ï¼æ¯ä¸ªä»»å¡é½æâåºå®âåâéæºåâä¸¤ç§åºæ¯åä½ãè¿äºåä½æ¨å¨è§£è¦èæ¯è§è§çº¿ç´¢ä¸ç¸æºå§¿æï¼ä»¥æ´çå®å°è¯ä¼°ç­ç¥å¨è§è§ååä¸çé²æ£æ§ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>æ¾èçæ³åæ§è½æåï¼</strong> å®éªç»æè¡¨æï¼å¯¹ç¸æºå¤åè¿è¡æ¡ä»¶åå¤çï¼æ¾èæé«äºACTãDiffusion PolicyåSmolVLAç­æ åè¡ä¸ºåéç­ç¥å¨ä¸åè§è§ä¸çæ³åè½åã
*   <strong>æ­ç¤ºäºç­ç¥çâæ·å¾âé®é¢ï¼</strong> åæåç°ï¼å¨åºå®åºæ¯ä¸­ï¼æ²¡æå¤åæ¡ä»¶åçç­ç¥éå¸¸ä¼å©ç¨éæèæ¯çè§è§çº¿ç´¢æ¥æ¨æ­ç¸æºå§¿æãå½å·¥ä½ç©ºé´å ä½æç¸æºä½ç½®åçååæ¶ï¼è¿ç§âæ·å¾âä¼å¤±æï¼å¯¼è´ç­ç¥æ§è½å´©æºã
*   <strong>æ¢å¤æ§è½åé²æ£çRGB-onlyæ§å¶ï¼</strong> æ¾å¼å°å¯¹ç¸æºå¤åè¿è¡æ¡ä»¶åå¤çï¼è½å¤æ¢å¤ç­ç¥æ§è½ï¼å¹¶å®ç°ä»ä¾èµRGBå¾åçé²æ£æ§å¶ï¼èæ éæ·±åº¦ä¿¡æ¯ã
*   <strong>å¶ä»åç°ï¼</strong>
    *   éæºè£åªï¼Random Croppingï¼è½ä¸è´æ§å°æé«ææä»»å¡çæ§è½ï¼è¿è¢«è§£éä¸ºææå°å¢å äºå·æä¸ååæ°çèæç¸æºã
    *   Delta End-Effector Poseä½ä¸ºå¨ä½ç©ºé´æ¶ï¼ç­ç¥è¡¨ç°æä½³ã
    *   å¨å¤§å¤æ°æåµä¸ï¼ææèåï¼late fusionï¼æ¯æ©æèåï¼early fusionï¼æ´ææï¼è¿å¯è½æ¯å ä¸ºå°PlÃ¼ckerå¾ä¸å¾åç´æ¥çº§èä¼ä½¿é¢è®­ç»çResNetè¾å¥åå¸åçååã
    *   é¢è®­ç»è§è§ç¼ç å¨å¯¹æåççå½±åä¸å¤§ã
    *   å¨è®­ç»æ°æ®éï¼ç¸æºè§è§æ°éï¼å¢å æ¶ï¼ç¸æºå§¿ææ¡ä»¶åä»ç¶è½æä¾æç»­çæ§è½æåï¼å¹¶ä¸è¾¾å°ç¸åæ§è½æéçç¸æºæ°éè¿å°äºæ²¡ææ¡ä»¶åçç­ç¥ã
*   <strong>çå®æºå¨äººå®éªéªè¯ï¼</strong> å¨çå®UR5æºå¨äººä¸è¿è¡çå®éªä¹éªè¯äºç¸æºå§¿ææ¡ä»¶åå¯¹ææç­ç¥çæ§è½æåã</p>
<p><strong>4. è®ºæä¸­æå°çå±éæ§ï¼</strong>
*   <strong>å§¿æä¼°è®¡æ¹æ³çå±éæ§ï¼</strong> è®ºææ¿è®¤å½åçå§¿æä¼°è®¡æ¹æ³ï¼å¦AprilTagsï¼ä»ä¸å®åï¼å°¤å¶æ¯å¨ç¹å¾è¾å°çè¡¨é¢ãæéçè§è§éå æé«åº¦å¨æçåºæ¯ä¸­ãå¨è¿ç§æåµä¸ï¼å§¿æä¼°è®¡è¯¯å·®å¯è½ä¼ä¸ä¸æ¸¸æ§å¶ä»»å¡å¤åï¼å¦ææªéåç¼è§£ç­ç¥ï¼å¯è½å¯¼è´æ§è½ä¸éã
*   <strong>å³æ³¨ç¹ï¼</strong> è®ºæä¸»è¦å³æ³¨å§¿ææ¡ä»¶åï¼èå¶ä»æ¹åï¼å¦æ³åå°å·æä¸åååçç¸æºï¼ä»æªæ¢ç´¢ã
*   <strong>Diffusion Policyçæ§è½ï¼</strong> è®ºæè§å¯å°Diffusion Policyçæ§è½æ¾èä½äºACTï¼ä½èè®¤ä¸ºè¿é¨åæ¯ç±äºå¶å¨éè¦ç²¾åº¦çä»»å¡ä¸­å·æéæºæ§ï¼å½ä»»å¡å¯ä»¥éè¿å¤æ¡è½¨è¿¹å®ææ¶ï¼Diffusion Policyå¸¸å¨è¿äºè½¨è¿¹ä¹é´æ¯è¡ï¼å¯è½å¯¼è´å¤±è´¥ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>è§£å³å§¿æä¼°è®¡è¯¯å·®ï¼</strong> éçå¹¿ä¹å§¿æä¼°è®¡çå¿«éåå±ï¼å§¿æä¼°è®¡çå­é®é¢ææå¨ç«¯å°ç«¯æºå¨äººç­ç¥å­¦ä¹ çæ´å¹¿æ³ææè§£å³ä¹åå¾å°è§£å³ã
*   <strong>æ³åå°ä¸åååçç¸æºï¼</strong> è®ºææ¹æ³å·²è½ä½¿ç­ç¥æ³åå°æ°çç¸æºå§¿æï¼èªç¶å°å¯ä»¥æ©å±å°æ¯ææ³åå°å·æä¸åååçç¸æºã
*   <strong>æ¢ç´¢å¶ä»æ¡ä»¶åæ¹æ³ï¼</strong> é¤äºå§¿ææ¡ä»¶åï¼è¿æè®¸å¤å¶ä»æ¹åå¼å¾æ¢ç´¢ã
*   <strong>å©ç¨æ°åºåï¼</strong> è®ºæåå¸çå­ä¸ªæ°çRoboSuiteåManiSkillåºåä¸ºæ¨è¿è§è§é²æ£æ¨¡ä»¿å­¦ä¹ æä¾äºå®ç¨çè¯ä¼°å·¥å·ã
*   <strong>è¡å¨ç©ºé´è®¾è®¡åæ°æ®å¢å¼ºï¼</strong> è®ºæçåç°å¼ºè°äºè¡å¨ç©ºé´è®¾è®¡åæ°æ®å¢å¼ºå¯¹ç­ç¥è®­ç»çå³é®èèå ç´ ã</p>
<p>æ»èè¨ä¹ï¼è¿ç¯è®ºæéè¿æ¾å¼å°å°ç¸æºå ä½ä¿¡æ¯ï¼ä½¿ç¨PlÃ¼ckeråµå¥ï¼ä½ä¸ºæ¡ä»¶å¼å¥æ¨¡ä»¿å­¦ä¹ ç­ç¥ï¼ææå°è§£å³äºæºå¨äººç­ç¥å¨ä¸åè§è§ä¸æ³åè½åå·®çé®é¢ãå®ä¸ä»æé«äºç°æç­ç¥çæ§è½ï¼è¿æ­ç¤ºäºç­ç¥å¯è½å©ç¨èæ¯è§è§çº¿ç´¢çâæ·å¾âé®é¢ï¼å¹¶ä¸ºæªæ¥çè§è§ä¸åæºå¨äººå­¦ä¹ ç ç©¶æä¾äºæ°çåºååæ¹æ³è®ºè§è§£ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To evaluate policy robustness under realistic
viewpoint shifts, we introduce six manipulation tasks in RoboSuite and
ManiSkill that pair "fixed" and "randomized" scene variants, decoupling
background cues from camera pose.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.02268v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.02268v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.02262v1'></a></p>
<h2 id="from-frames-to-clips-efficient-key-clip-selection-for-long-form-video-understanding"><a href="https://arxiv.org/abs/2510.02262v1">From Frames to Clips: Efficient Key Clip Selection for Long-Form Video Understanding</a></h2>
<p><strong>Authors:</strong> Guangyu Sun, Archit Singhal, Burak Uzkent, Mubarak Shah, Chen Chen, Garin Kessler</p>
<p><strong>Published:</strong> 2025-10-02</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Video Large Language Models (VLMs) have achieved remarkable results on a
variety of vision language tasks, yet their practical use is limited by the
"needle in a haystack" problem: the massive number of visual tokens produced
from raw video frames exhausts the model's context window. Existing solutions
alleviate this issue by selecting a sparse set of frames, thereby reducing
token count, but such frame-wise selection discards essential temporal
dynamics, leading to suboptimal reasoning about motion and event continuity. In
this work we systematically explore the impact of temporal information and
demonstrate that extending selection from isolated key frames to key clips,
which are short, temporally coherent segments, improves video understanding. To
maintain a fixed computational budget while accommodating the larger token
footprint of clips, we propose an adaptive resolution strategy that dynamically
balances spatial resolution and clip length, ensuring a constant token count
per video. Experiments on three long-form video benchmarks demonstrate that our
training-free approach, F2C, outperforms uniform sampling up to 8.1%, 5.6%, and
10.3% on Video-MME, LongVideoBench and MLVU benchmarks, respectively. These
results highlight the importance of preserving temporal coherence in frame
selection and provide a practical pathway for scaling Video LLMs to real world
video understanding applications. Project webpage is available at
https://guangyusun.com/f2c .</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼è¿æ¯ä¸ç¯å³äºGuangyu Sun, Archit Singhal, Burak Uzkent, Mubarak Shah, Chen Chen, Garin Kessleræ°åçè®ºæâFrom Frames to Clips: Efficient Key Clip Selection for Long-Form Video Understandingâçå¨é¢æè¦ï¼</p>
<p><strong>è®ºææè¦ï¼ä»å¸§å°çæ®µï¼é¿è§é¢çè§£çé«æå³é®çæ®µéæ©</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è§é¢å¤§åè¯­è¨æ¨¡åï¼VLMsï¼å¨åç§è§è§è¯­è¨ä»»å¡ä¸­è¡¨ç°åºè²ï¼ä½å¶å¨å¤çé¿è§é¢æ¶çå®éåºç¨åå°âå¤§æµ·æéâé®é¢çéå¶ãåå§è§é¢å¸§äº§ççå¤§éè§è§tokenä¼è¿éèå°½æ¨¡åçä¸ä¸æçªå£ãç°æè§£å³æ¹æ¡éå¸¸éè¿ç¨çéæ ·å¸§æ¥åå°tokenæ°éï¼ä½è¿ä¼ä¸¢å¼éè¦çæ¶é´å¨æï¼å¯¼è´å¨æ¨çè¿å¨åäºä»¶è¿ç»­æ§æ¹é¢è¡¨ç°ä¸ä½³ãå æ­¤ï¼æ ¸å¿é®é¢æ¯å¦ä½å¨ä¿æè®¡ç®é¢ç®ä¸åçæåµä¸ï¼ææå°ä»é¿è§é¢ä¸­éæ©æ¢å·æè¯­ä¹ç¸å³æ§åä¿çæ¶é´è¿ç»­æ§çè§è§ä¿¡æ¯ï¼ä»¥æé«VLMsçè§é¢çè§£è½åã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
æ¬ææåºäºä¸ä¸ªåä¸º<strong>Frames-to-Clips (F2C)</strong>çåè®­ç»æ¡æ¶ï¼ç¨äºé«æçå³é®çæ®µéæ©ï¼å¶ä¸»è¦åæ°ç¹åæ¬ï¼</p>
<ul>
<li><strong>ä»å³é®å¸§å°å³é®çæ®µçæ©å±ï¼</strong> è®ºæç³»ç»å°æ¢ç´¢äºæ¶é´ä¿¡æ¯çå½±åï¼å¹¶è¯æå°éæ©èå´ä»å­¤ç«çå³é®å¸§æ©å±å°ç­çãæ¶é´è¿è´¯çâå³é®çæ®µâå¯ä»¥æ¾èæ¹åè§é¢çè§£ã</li>
<li><strong>èªéåºåè¾¨çç­ç¥ï¼</strong> ä¸ºäºå¨åºå®è®¡ç®é¢ç®ä¸éåºæ´å¤§tokenè¶³è¿¹ççæ®µï¼F2Cæåºäºä¸ç§èªéåºåè¾¨çç­ç¥ãè¯¥ç­ç¥å¨æå¹³è¡¡ç©ºé´åè¾¨çåçæ®µé¿åº¦ï¼ç¡®ä¿æ¯ä¸ªè§é¢çtokenæ°éæå®ãè¿æå³çå¯ä»¥éè¿éä½ç©ºé´åè¾¨çæ¥åå«æ´é¿ççæ®µï¼åä¹äº¦ç¶ï¼ä»èå®ç°æ¶ç©ºå¹³è¡¡ã</li>
<li><strong>éç¹å³é®å¸§éæ©ï¼</strong> F2Cé¦åéè¿ç»åç¸å³æ§ï¼ä½¿ç¨é¢è®­ç»çå¯¹æ¯è§è§-è¯­è¨ç¼ç å¨è®¡ç®å¸§ä¸ææ¬æ¥è¯¢çä½å¼¦ç¸ä¼¼åº¦ï¼åå¤æ ·æ§ï¼éè¿å±é¨æå¤§å¼è¯å«åK-meansèç±»ï¼æ¥éæ©åå§çéç¹å³é®å¸§ã</li>
<li><strong>çæ®µç¹å®éæ©ï¼</strong> éå¯¹æ¯ä¸ªéç¹å³é®å¸§ï¼F2Céè¿ä¼åç®æ å½æ°æ¥ç¡®å®å¶æä½³çæ®µé¿åº¦ãè¯¥ç®æ å½æ°ç»¼åèèäºçæ®µåå¸§çå¹³åä½å¼¦ç¸ä¼¼åº¦ï¼ç¸å³æ§ï¼ãå¸§ä¹é´çå¹³åæå¯¹ç¸ä¼¼åº¦ï¼åä½åº¦ï¼ä»¥åä¸ç¸å¯¹çæ®µé¿åº¦ææ¯ä¾çæ¶é´å¥å±ã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
F2Cå¨ä¸ä¸ªé¿è§é¢åºåæµè¯ï¼Video-MMEãLongVideoBenchåMLVUï¼ä¸çå®éªç»æè¡¨æï¼</p>
<ul>
<li><strong>æ¾èä¼äºååéæ ·ï¼</strong> F2Cå¨Video-MMEãLongVideoBenchåMLVUåºåæµè¯ä¸åå«æ¯ååéæ ·é«åº8.1%ã5.6%å10.3%ãè¿çªåºäºä¿çæ¶é´è¿è´¯æ§å¨å¸§éæ©ä¸­çéè¦æ§ã</li>
<li><strong>å¯¹ä¸åé®é¢ç±»åçæ®éæææ§ï¼</strong> F2Cå¨Video-MMEçå­ç§é®é¢ç±»å«ï¼è®¡æ°ãè¯å«ãæ¨çãæç¥ãOCRãä¿¡æ¯æ¦è¦ï¼ä¸­ååå¾äºæç»­æ¹è¿ï¼å°¤å¶å¨éè¦ä¸°å¯è¿å¨åä¸ä¸æçº¿ç´¢çè®¡æ°ãè¯å«åæ¨çä»»å¡ä¸­è¡¨ç°çªåºã</li>
<li><strong>è®¡ç®æçï¼</strong> å°½ç®¡å¢å äºæ¶é´ä¸ä¸æï¼F2Céè¿é¿åéå¤ç¼ç éå å¸§ï¼å®ç°äºä¸åºäºå¸§çéæ©å¨ç¸ä¼¼ææ´å°çtokenæ°éï¼å°¤å¶å¨é¢ç®å¢å æ¶ï¼tokenæ°éçåå°æ´ä¸ºæ¾èãè¿è¡¨æF2Cå¨åç¡®æ§åè®¡ç®æçä¹é´åå¾äºè¯å¥½å¹³è¡¡ã</li>
<li><strong>éç¹éæ©åå¤æ ·æ§çéè¦æ§ï¼</strong> å®éªè¡¨æï¼éç¹å³é®å¸§çè´¨éè³å³éè¦ï¼å¤æ ·æ§é«çéæ©å¨ï¼å¦WatershedåBOLTï¼å¨F2Cå¢å¼ºåè¡¨ç°æ´å¥½ãåæ¶ï¼éä¸­çéç¹æ°éï¼Kanchor = Kï¼è½å¨ç©ºé´ç»èåæ¶é´è¦çä¹é´åå¾æä½³å¹³è¡¡ã</li>
</ul>
<p>è¿äºç»æå±åå¼ºè°äºå¨å¸§éæ©ä¸­ä¿çæ¶é´è¿è´¯æ§çéè¦æ§ï¼å¹¶ä¸ºå°VLMsæ©å±å°å®éä¸ççè§é¢çè§£åºç¨æä¾äºä¸æ¡å®ç¨éå¾ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
å°½ç®¡F2Céè¿æä¾æ¶é´è¿è´¯ååè¾¨çèªéåºçè¾å¥æææ¹åäºä¸ä¸æç®¡çï¼ä½å¶æ§è½ä¸éä»åéäºä¸æ¸¸VLMçè½åãå³ä½¿éæ©äºæ­£ç¡®çå¸§æçæ®µï¼æç»ç­æ¡ä¹åå³äºVLMæ¬èº«çæ¨çåçè§£è½åãå æ­¤ï¼éª¨å¹²æ¨¡åå¨æ¶é´æ¨çãç©ºé´çè§£æå¤æ¨¡æå¯¹é½æ¹é¢çå±éæ§ä»ç¶æ¯ç¶é¢ãF2Cæ¹æ³æ¯è¾å©æ§çï¼å¶ä¼å¿å¯è½ä¼éçæ´å¼ºå¤§çéª¨å¹²æ¨¡ååºç°èè¿ä¸æ­¥æ¾å¤§ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
è®ºææåºï¼F2Cä¸ºé¿è§é¢çè§£æä¾äºä¸ä¸ªç®åä¸å¯æ©å±çè§£å³æ¹æ¡ï¼å¹¶ä¸ºVLMsçæªæ¥åå±ææäºæ¹åãæªæ¥çç ç©¶å¯ä»¥æ¢ç´¢å¦ä½è¿ä¸æ­¥ä¼åF2Cä¸æ´å¼ºå¤§çéª¨å¹²VLMçç»åï¼ä»¥åå¦ä½å°F2Cçèªéåºç­ç¥åºç¨äºå¶ä»å¤æ¨¡æä»»å¡ï¼ä»¥è¿ä¸æ­¥æåæ§è½ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To
maintain a fixed computational budget while accommodating the larger token
footprint of clips, we propose an adaptive resolution strategy that dynamically
balances spatial resolution and clip length, ensuring a constant token count
per video.</li>
<li>Experiments on three long-form video benchmarks demonstrate that our
training-free approach, F2C, outperforms uniform sampling up to 8.1%, 5.6%, and
10.3% on Video-MME, LongVideoBench and MLVU benchmarks, respectively.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.02262v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.02262v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.02253v1'></a></p>
<h2 id="dragflow-unleashing-dit-priors-with-region-based-supervision-for-drag-editing"><a href="https://arxiv.org/abs/2510.02253v1">DragFlow: Unleashing DiT Priors with Region Based Supervision for Drag Editing</a></h2>
<p><strong>Authors:</strong> Zihan Zhou, Shilin Lu, Shuli Leng, Shaocong Zhang, Zhuming Lian, Xinlei Yu, Adams Wai-Kin Kong</p>
<p><strong>Published:</strong> 2025-10-02</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Drag-based image editing has long suffered from distortions in the target
region, largely because the priors of earlier base models, Stable Diffusion,
are insufficient to project optimized latents back onto the natural image
manifold. With the shift from UNet-based DDPMs to more scalable DiT with flow
matching (e.g., SD3.5, FLUX), generative priors have become significantly
stronger, enabling advances across diverse editing tasks. However, drag-based
editing has yet to benefit from these stronger priors. This work proposes the
first framework to effectively harness FLUX's rich prior for drag-based
editing, dubbed DragFlow, achieving substantial gains over baselines. We first
show that directly applying point-based drag editing to DiTs performs poorly:
unlike the highly compressed features of UNets, DiT features are insufficiently
structured to provide reliable guidance for point-wise motion supervision. To
overcome this limitation, DragFlow introduces a region-based editing paradigm,
where affine transformations enable richer and more consistent feature
supervision. Additionally, we integrate pretrained open-domain personalization
adapters (e.g., IP-Adapter) to enhance subject consistency, while preserving
background fidelity through gradient mask-based hard constraints. Multimodal
large language models (MLLMs) are further employed to resolve task ambiguities.
For evaluation, we curate a novel Region-based Dragging benchmark (ReD Bench)
featuring region-level dragging instructions. Extensive experiments on
DragBench-DR and ReD Bench show that DragFlow surpasses both point-based and
region-based baselines, setting a new state-of-the-art in drag-based image
editing. Code and datasets will be publicly available upon publication.</p>
<p><strong>Analysis:</strong></p>
<p>ä»¥ä¸æ¯Zihan Zhouç­äººæ°åçè®ºæâDragFlow: Unleashing DiT Priors with Region Based Supervision for Drag Editingâçå¨é¢æè¦ï¼</p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢</strong>
ææ½å¼å¾åç¼è¾é¿æä»¥æ¥å­å¨ç®æ åºåå¤±çé®é¢ï¼è¿ä¸»è¦æ¯å ä¸ºæ©æåºç¡æ¨¡åï¼å¦Stable Diffusionï¼çåéªä¸è¶³ä»¥å°ä¼ååçæ½å¨è¡¨ç¤ºæå½±åèªç¶å¾åæµå½¢ãå°½ç®¡åºäºUNetçDDPMså·²è½¬åæ´å·å¯æ©å±æ§çDiTæ¨¡åï¼å¦FLUXï¼ï¼çæåéªæ¾èå¢å¼ºï¼ä½ææ½å¼ç¼è¾å°æªååå©ç¨è¿äºæ´å¼ºçåéªãç´æ¥å°åºäºç¹çææ½ç¼è¾åºç¨äºDiTæ¨¡åææä¸ä½³ï¼å ä¸ºDiTçç¹å¾ç»æä¸è¶³ä»¥æä¾å¯é çéç¹è¿å¨çç£ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®</strong>
DragFlowæåºäºä¸ä¸ªæ°é¢çåºåçº§ç¼è¾æ¡æ¶ï¼ææå©ç¨äºFLUXå¼ºå¤§ççæåéªï¼å¹¶è§£å³äºDiTæ¨¡åä¸­çææï¼
*   <strong>åºåçº§è¿å¨çç£ï¼</strong> å¼å¥äºåºäºåºåçç¼è¾èå¼ï¼éè¿ä»¿å°åæ¢å®ç°æ´ä¸°å¯ãæ´ä¸è´çç¹å¾çç£ï¼åæäºéç¹çç£çå±éæ§ã
*   <strong>èæ¯ä¿çåº¦ï¼</strong> éè¿åºäºæ¢¯åº¦æ©ç çç¡¬çº¦ææ¥ä¿æèæ¯çå¿ å®åº¦ï¼ä»æ´æ°å¯ç¼è¾åºåï¼åä»£äºä¼ ç»çèæ¯ä¸è´æ§æå¤±ã
*   <strong>å¢å¼ºçä¸»ä½ä¸è´æ§ï¼</strong> éæäºé¢è®­ç»çå¼æ¾åä¸ªæ§åééå¨ï¼å¦IP-Adapterï¼ï¼ä»¥å¢å¼ºä¸»ä½ä¸è´æ§ï¼åæ¶éè¿ééå¨å¢å¼ºçåæ¼æºå¶ç¼è§£äºDiTæ¨¡åä¸­å¸¸è§çåæ¼æ¼ç§»é®é¢ã
*   <strong>å¤æ¨¡æå¤§è¯­è¨æ¨¡åï¼MLLMï¼çåºç¨ï¼</strong> å©ç¨MLLMæ¥è§£æä»»å¡æ­§ä¹ï¼æ´å¥½å°çè§£ç¨æ·æå¾ã
*   <strong>æ°åºåæ°æ®éï¼</strong> ç­åäºä¸ä¸ªæ°çåºåçº§ææ½åºåï¼ReD Benchï¼ï¼åå«åºåçº§ææ½æä»¤ï¼ç¨äºè¯ä¼°æ¹æ³ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶éè¦æ§</strong>
DragFlowå¨DragBench-DRåReD Benchä¸è¿è¡äºå¹¿æ³å®éªï¼ç»æè¡¨æå®å¨ææ½å¼å¾åç¼è¾æ¹é¢è¶è¶äºç°æçåºäºç¹ååºäºåºåçåºçº¿æ¹æ³ï¼å¨ç©ºé´å¯¹åºæ§ãç»æä¸è´æ§ååå®¹å®æ´æ§æ¹é¢åå¾äºæ¾èæåãå®æåå°å©ç¨äºFLUXæ´å¼ºççæåéªï¼æ¶é¤äºä»¥å¾æ¹æ³å¨å¤æåºæ¯ä¸­äº§ççå¤±çï¼å¹¶å®ç°äºæ´å¼ºçå¯æ§æ§ãæ´ä½çåå½¢ä¼ªå½±åæ´é«çå¾åä¿çåº¦ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§</strong>
*   <strong>åæ¼æ¼ç§»ï¼</strong> ç±äºFLUXæ¨¡åæ¯CFG-distilledåä½ï¼å¶åæ¼æ¼ç§»æ¯édistilledæ¨¡åæ´å¤§ãå°½ç®¡DragFlowéè¿ééå¨å¢å¼ºçåæ¼ç¼è§£äºè¿ä¸ªé®é¢ï¼ä½å¯¹äºé«åº¦å¤æçå¾åç»æï¼éå»ºè¿ç¨ä¸­ä»å­å¨ç»èæå¤±ã
*   <strong>è§è§è´¨éä¸éï¼</strong> ç±åæ¼æ¼ç§»å¯¼è´çç»èæå¤±ä¼å½±åææ½ç¼è¾ç»æçè§è§è´¨éã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹å</strong>
æªæ¥çç ç©¶å¯ä»¥åçäºå¼åæ´åè¿çææ¯æééå¨æ¶æï¼ä»¥è¿ä¸æ­¥å¢å¼ºåæ¼ä¿çåº¦ï¼ä»èåå°ä¸è¿°ä¼ªå½±å¹¶æåæ´ä½ç¼è¾æ§è½ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>For evaluation, we curate a novel Region-based Dragging benchmark (ReD Bench)
featuring region-level dragging instructions.</li>
<li>Extensive experiments on
DragBench-DR and ReD Bench show that DragFlow surpasses both point-based and
region-based baselines, setting a new state-of-the-art in drag-based image
editing.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.02253v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.02253v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.02186v1'></a></p>
<h2 id="geopurify-a-data-efficient-geometric-distillation-framework-for-open-vocabulary-3d-segmentation"><a href="https://arxiv.org/abs/2510.02186v1">GeoPurify: A Data-Efficient Geometric Distillation Framework for Open-Vocabulary 3D Segmentation</a></h2>
<p><strong>Authors:</strong> Weijia Dou, Xu Zhang, Yi Bin, Jian Liu, Bo Peng, Guoqing Wang, Yang Yang, Heng Tao Shen</p>
<p><strong>Published:</strong> 2025-10-02</p>
<p><strong>Categories:</strong> cs.CV, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Recent attempts to transfer features from 2D Vision-Language Models (VLMs) to
3D semantic segmentation expose a persistent trade-off. Directly projecting 2D
features into 3D yields noisy and fragmented predictions, whereas enforcing
geometric coherence necessitates costly training pipelines and large-scale
annotated 3D data. We argue that this limitation stems from the dominant
segmentation-and-matching paradigm, which fails to reconcile 2D semantics with
3D geometric structure. The geometric cues are not eliminated during the
2D-to-3D transfer but remain latent within the noisy and view-aggregated
features. To exploit this property, we propose GeoPurify that applies a small
Student Affinity Network to purify 2D VLM-generated 3D point features using
geometric priors distilled from a 3D self-supervised teacher model. During
inference, we devise a Geometry-Guided Pooling module to further denoise the
point cloud and ensure the semantic and structural consistency. Benefiting from
latent geometric information and the learned affinity network, GeoPurify
effectively mitigates the trade-off and achieves superior data efficiency.
Extensive experiments on major 3D benchmarks demonstrate that GeoPurify
achieves or surpasses state-of-the-art performance while utilizing only about
1.5% of the training data. Our codes and checkpoints are available at
<a href="https://github.com/tj12323/GeoPurify">https://github.com/tj12323/GeoPurify</a>.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾å¯¹Weijia Douç­äººæ°åçè®ºæâGeoPurify: A Data-Efficient Geometric Distillation Framework for Open-Vocabulary 3D Segmentationâçå¨é¢æè¦ã</p>
<hr />
<h3 id="geopurify-3d">GeoPurify: ä¸ç§ç¨äºå¼æ¾è¯æ±3Dåå²çæ°æ®é«æå ä½è¸é¦æ¡æ¶</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³å¼æ¾è¯æ±3Dè¯­ä¹åå²é¢åä¸­ä¸ä¸ªé¿æå­å¨çæè¡¡é®é¢ï¼å¦ä½å°2Dè§è§-è¯­è¨æ¨¡åï¼VLMsï¼çä¸°å¯è¯­ä¹ä¿¡æ¯ææå°è¿ç§»å°3Dç¹äºï¼åæ¶åæç´æ¥æå½±2Dç¹å¾æå¯¼è´çå ä½ä¸ä¸è´æ§ï¼å¦åªå£°åç¢çåé¢æµï¼ï¼èåä¸ä¾èµäºæè´µçå¤§è§æ¨¡3Dæ æ³¨æ°æ®åå¤æçè®­ç»æµç¨ãä½èè®¤ä¸ºï¼ç°ææ¹æ³ï¼å¦âåå²ä¸å¹éâèå¼ï¼æªè½ææåè°2Dè¯­ä¹ä¸3Då ä½ç»æï¼å¯¼è´å ä½çº¿ç´¢å¨2Då°3Dè¿ç§»ä¸­åå¾æ½å¨èéæ¾å¼ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è´¡ç®ï¼</strong>
GeoPurifyæ¡æ¶çæ ¸å¿åæ°å¨äºå¶âçè§£å³åå²âèå¼åæ°æ®é«æçå ä½è¸é¦æºå¶ï¼å·ä½åæ¬ï¼</p>
<ul>
<li><strong>éæ°å®ä¹èå¼ï¼</strong> å°å¼æ¾è¯æ±3Dåå²ä»ä¼ ç»çâåå²ä¸å¹éâèå¼éæ°å®ä¹ä¸ºâçè§£å³åå²âãéè¿å©ç¨éç¨çVLMï¼å¦X-Decoderï¼å»ºç«æ´ä¸°å¯çç»ä¸è§è§-è¯­è¨åµå¥ç©ºé´ï¼åæäºä¼ ç»âåå®ä½åè¯å«âæ¹æ³å¨è¯­ä¹å¤©è±æ¿ä¸çéå¶ã</li>
<li><strong>å ä½å¯¹æ¯è¸é¦ï¼Geometric Contrastive Distillationï¼ï¼</strong> æåºä¸ä¸ªæ°æ®é«æçæ¡æ¶ï¼å¶æ ¸å¿æ¯ä½¿ç¨ä¸ä¸ªå°åå­¦çäº²åç½ç»ï¼Student Affinity Networkï¼æ¥åå2D VLMçæç3Dç¹ç¹å¾ãè¯¥å­¦çç½ç»éè¿ç¥è¯è¸é¦ä»ä¸ä¸ªå»ç»ç3Dèªçç£æå¸æ¨¡åï¼å¦Sonataï¼ä¸­å­¦ä¹ å ä½åéªãå­¦çç½ç»çç®æ ä¸æ¯å¤å¶æå¸çç¹å¾ï¼èæ¯å­¦ä¹ ç¹ä¹é´çå ä½äº²ååï¼ä»èå¨æ æ ç­¾ç3Dæ«æä¸è¿è¡è®­ç»ã</li>
<li><strong>å ä½å¼å¯¼æ± åï¼Geometry-Guided Poolingï¼ï¼</strong> å¨æ¨çé¶æ®µï¼è®¾è®¡äºä¸ä¸ªå ä½å¼å¯¼æ± åæ¨¡åï¼å©ç¨å­¦ä¹ å°çå ä½äº²ååè¿ä¸æ­¥å»åªç¹äºï¼å¹¶ç¡®ä¿è¯­ä¹åç»æçä¸è´æ§ãè¯¥æ¨¡åéè¿è¿­ä»£æ± åè¿ç¨ï¼æ ¹æ®å­¦ä¹ å°çç»æäº²ååå¹³åç¹å¾ï¼ä»èå¼ºå¶å±é¨å ä½ä¸è´æ§ï¼ææå°å»åªåå§è¡¨ç¤ºå¹¶ä¿çè¯­ä¹ä¸°å¯æ§ã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
GeoPurifyå¨å¤ä¸ªä¸»è¦ç3Dåºåæµè¯ï¼ScanNetV2ãMatterport3DåScanNet200ï¼ä¸åå¾äºæ¾èææï¼</p>
<ul>
<li><strong>åè¶çæ°æ®æçï¼</strong> GeoPurifyä»ä½¿ç¨çº¦1.5%çè®­ç»æ°æ®ï¼éè¿ç²¾å¿ç­åçå­ééæ©ç­ç¥è·å¾ï¼ï¼å°±å®ç°äºä¸ç°ææåè¿æ¹æ³ç¸å½çè³è¶è¶çæ§è½ãè¿æ¾èç¼è§£äºå¤§è§æ¨¡3Dæ æ³¨æ°æ®çéæ±ã</li>
<li><strong>æ§è½æåï¼</strong> å¨ScanNetV2ä¸ï¼GeoPurifyçmIoUä»åºçº¿ç50.2%æåå°55.1%ï¼+4.9%ï¼ï¼mAccä»68.1%æåå°72.5%ï¼+4.4%ï¼ãå¨Matterport3Dä¸ï¼mIoUä»37.5%æåå°40.2%ï¼+2.7%ï¼ï¼mAccä»59.8%æåå°62.4%ï¼+2.6%ï¼ã</li>
<li><strong>é¿å°¾æ³åè½åï¼</strong> å¨ScanNet200åMatterport3DçM160é¿å°¾åºåæµè¯ä¸­ï¼GeoPurifyä¹è¾¾å°äºæ°çæåè¿æ°´å¹³ï¼è¿å¾çäºå¶è¯­ä¹åå ä½æ¨¡åçååä½ç¨ï¼ä½¿å¾æ¨¡åå¯¹ç¨æç±»å«å·ææ´å¼ºçé²æ£æ§ã</li>
<li><strong>è·¨æ°æ®éæ³åï¼</strong> å¨é¶æ ·æ¬è·¨æ°æ®éæ³åä»»å¡ä¸­ï¼GeoPurifyæ¾èä¼äºç°ææ¹æ³ï¼å°¤å¶æ¯å¨ä»Matterport3Dè¿ç§»å°ScanNetV2æ¶ï¼mIoUé¢å16.3ä¸ªç¾åç¹ãè¿è¡¨æå¶è§£è¦çå ä½åéªå­¦ä¹ å°äºæ´åºæ¬ãé¢åæ å³ç3Då ä½çè§£ã</li>
</ul>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
è®ºæä¸­ä¹å¦è¯å°æåºäºGeoPurifyçå±éæ§ï¼</p>
<ul>
<li><strong>è¿å¹³æ»ä¼ªå½±ï¼Over-smoothing Artifactsï¼ï¼</strong> å¨æäºæåµä¸ï¼è¿­ä»£æ± åè¿ç¨å¯è½ä¼å¾®å¦å°æ¨¡ç³ä¸åä½ç´§å¯ç¸é»ç©ä½ä¹é´çè¾¹çï¼å¯¼è´å±é¨ä¸åç¡®ã</li>
<li><strong>2Déª¨å¹²ç½ç»çè¯­ä¹éè¯¯ï¼Semantic Errors from 2D Backboneï¼ï¼</strong> GeoPurifyçæ§è½ä¾èµäºåå§2Dç¹å¾çè´¨éãå½2Déª¨å¹²ç½ç»äº§çæ ¹æ¬æ§çè¯­ä¹éè¯¯æ¶ï¼å ä½ååæ¨¡åéå¸¸æ æ³çº æ­£è¿ç§éè¯¯åç±»ã</li>
</ul>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
è¿äºå±éæ§ä¹ä¸ºæªæ¥çç ç©¶ææäºæ¹åï¼</p>
<ul>
<li>å¼åæ´é²æ£çç¹å¾èåææ¯ã</li>
<li>ç ç©¶éè¯¯æ ¡æ­£æºå¶ï¼ä»¥è§£å³2Déª¨å¹²ç½ç»å¸¦æ¥çè¯­ä¹éè¯¯ã</li>
<li>è¿ä¸æ­¥æ¢ç´¢å¦ä½å¤çæç»ç²åº¦ãè¡¨é¢çº§å«çç»èï¼ä»¥åæå½åå±é¨æ± åç­ç¥çéå¶ã</li>
</ul>
<hr />
<p>æ»èè¨ä¹ï¼GeoPurifyéè¿å¼å¥ä¸ç§æ°é¢çå ä½å¯¹æ¯è¸é¦æºå¶åå ä½å¼å¯¼æ± åæ¨¡åï¼æåå°å°2D VLMsçä¸°å¯è¯­ä¹ä¸3Då ä½ç»æç¸ç»åï¼ææå°è§£å³äºå¼æ¾è¯æ±3Dè¯­ä¹åå²ä¸­é¿æå­å¨çè¯­ä¹ä¸°å¯æ§ä¸å ä½ä¸è´æ§ä¹é´ççç¾ãå¶åè¶çæ°æ®æçåå¨åç§åºåæµè¯ä¸çä¼å¼è¡¨ç°ï¼ä½¿å¶æä¸ºæ é3Dæ æ³¨ç3Dæç¥é¢åä¸­ä¸ä¸ªå¯æ©å±çåºç¡æ§æ¹æ³ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To exploit this property, we propose GeoPurify that applies a small
Student Affinity Network to purify 2D VLM-generated 3D point features using
geometric priors distilled from a 3D self-supervised teacher model.</li>
<li>Extensive experiments on major 3D benchmarks demonstrate that GeoPurify
achieves or surpasses state-of-the-art performance while utilizing only about
1.5% of the training data.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.02186v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.02186v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2510.02034v1'></a></p>
<h2 id="gaussianmorphing-mesh-guided-3d-gaussians-for-semantic-aware-object-morphing"><a href="https://arxiv.org/abs/2510.02034v1">GaussianMorphing: Mesh-Guided 3D Gaussians for Semantic-Aware Object Morphing</a></h2>
<p><strong>Authors:</strong> Mengtian Li, Yunshu Bai, Yimin Chu, Yijun Shen, Zhongmei Li, Weifeng Ge, Zhifeng Xie, Chaofeng Chen</p>
<p><strong>Published:</strong> 2025-10-02</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We introduce GaussianMorphing, a novel framework for semantic-aware 3D shape
and texture morphing from multi-view images. Previous approaches usually rely
on point clouds or require pre-defined homeomorphic mappings for untextured
data. Our method overcomes these limitations by leveraging mesh-guided 3D
Gaussian Splatting (3DGS) for high-fidelity geometry and appearance modeling.
The core of our framework is a unified deformation strategy that anchors
3DGaussians to reconstructed mesh patches, ensuring geometrically consistent
transformations while preserving texture fidelity through topology-aware
constraints. In parallel, our framework establishes unsupervised semantic
correspondence by using the mesh topology as a geometric prior and maintains
structural integrity via physically plausible point trajectories. This
integrated approach preserves both local detail and global semantic coherence
throughout the morphing process with out requiring labeled data. On our
proposed TexMorph benchmark, GaussianMorphing substantially outperforms prior
2D/3D methods, reducing color consistency error (<script type="math/tex">\Delta E</script>) by 22.2% and EI by
26.2%. Project page: https://baiyunshu.github.io/GAUSSIANMORPHING.github.io/</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Mengtian Liç­äººæ°åçè®ºæâGaussianMorphing: Mesh-Guided 3D Gaussians for Semantic-Aware Object Morphingâçå¨é¢æè¦ã</p>
<hr />
<h3 id="gaussianmorphing-mesh-guided-3d-gaussians-for-semantic-aware-object-morphing_1">è®ºææè¦ï¼GaussianMorphing: Mesh-Guided 3D Gaussians for Semantic-Aware Object Morphing</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³ä»å¤è§è§å¾åè¿è¡è¯­ä¹æç¥çä¸ç»´å½¢ç¶åçº¹çåå½¢ï¼morphingï¼çææãç°ææ¹æ³éå¸¸ä¾èµäºç¹äºæé¢å®ä¹åèæ å°ï¼homeomorphic mappingsï¼ï¼ä¸å¤ç¨äºæ çº¹çæ°æ®ï¼è¿éå¶äºå®ä»¬å¨å¤çå¤æå ä½åä¸°å¯çº¹çå¯¹è±¡æ¶çéç¨æ§ãæ ¸å¿é®é¢å¨äºå¦ä½å®ç°å ä½ä¸è´çåå½¢ï¼åæ¶éè¿æææç¥çº¦æä¿æçº¹çä¿çåº¦ï¼å¹¶ä¸æ éé¢åæ æ³¨æ°æ®æé«ä¿çä¸ç»´è¾å¥ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è´¡ç®ï¼</strong>
GaussianMorphingå¼å¥äºä¸ä¸ªæ°é¢çæ··åæ¡æ¶ï¼éè¿ä»¥ä¸æ¹å¼åæäºç°ææ¹æ³çå±éæ§ï¼
*   <strong>ç½æ ¼å¼å¯¼çä¸ç»´é«æ¯æºå°ï¼3DGSï¼</strong>ï¼è¯¥æ¹æ³å°3DGSçæ¸²ææçä¸ç½æ ¼å¼å¯¼åå½¢çç»æä¼å¿ç¸ç»åãå®å°ä¸ç»´é«æ¯éå®å°éå»ºçç½æ ¼é¢çä¸ï¼ç¡®ä¿å ä½ä¸è´çåæ¢ï¼åæ¶éè¿æææç¥çº¦æä¿æçº¹çä¿çåº¦ã
*   <strong>æ çç£è¯­ä¹å¯¹åº</strong>ï¼å©ç¨ç½æ ¼ææä½ä¸ºå ä½åéªï¼å»ºç«æ çç£çè¯­ä¹å¯¹åºå³ç³»ï¼å¹¶éè¿ç©çä¸åççç¹è½¨è¿¹ä¿æç»æå®æ´æ§ãè¿é¿åäºå¯¹æ æ³¨æ°æ®çéæ±ã
*   <strong>æ··åç½æ ¼-é«æ¯è¡¨ç¤º</strong>ï¼é¦åä»ä¼åç3DGSä¸­æåé«è´¨éçåå§ç½æ ¼ï¼ç¶åå°é«æ¯éå®å°è¿äºç½æ ¼é¢çä¸ãè¿ç§ç»å®ç¡®ä¿äºç½æ ¼é¡¶ç¹åå½¢æ¶ï¼éå®çé«æ¯è½ä¸è¡¨é¢ååç§»å¨ï¼ä»èä¿çç²¾ç»çå ä½åå¤è§ç»èã
*   <strong>ç¥ç»åå½¢æµï¼Neural Morphing Flowï¼</strong>ï¼éè¿ä¸ä¸ªç¥ç»ç½ç»å­¦ä¹ è¿ç»­çéçº¿æ§åå½¢åºï¼é¢æµåå½¢è½¨è¿¹ï¼èéç®åççº¿æ§æå¼ã
*   <strong>å¤ç®æ ä¼åç­ç¥</strong>ï¼ç»åäºå ä½ä¸è´æ§ï¼éè¿æµå°è·ç¦»åARAPè½éé¡¹ï¼ãå¤è§ä¸è´æ§ï¼éè¿æµå°çº¿æç¥çé¡¶ç¹é¢è²å¹³æ»åº¦æå¤±ï¼åè¯­ä¹å¯¹é½çº¦æï¼ç¡®ä¿åå½¢è¿ç¨çå¹³æ»æ§åçå®æã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>æ§è½æ¾èæå</strong>ï¼å¨ä½èæåºçTexMorphåºåæµè¯ä¸ï¼GaussianMorphingå¨é¢è²ä¸è´æ§è¯¯å·®ï¼ÎEï¼ä¸éä½äº22.2%ï¼å¨è¾¹ç¼å®æ´æ§ï¼EIï¼ä¸éä½äº26.2%ï¼æ¾èä¼äºååç2D/3Dæ¹æ³ã
*   <strong>é«ä¿ççº¹çä¸ç»´åå½¢</strong>ï¼è¯¥æ¹æ³è½å¤ç´æ¥ä»å¾åè¾å¥çæå®å¨çº¹çåçä¸ç»´è¾åºï¼æä¾å®æ´çå ä½åçº¹çä¿çåº¦ã
*   <strong>é²æ£æ§åæ³åè½å</strong>ï¼å®éªè¯æï¼è¯¥æ¹æ³å¨å¤æææåçº¹çä¸°å¯çå¯¹è±¡ä¸è¡¨ç°åºé²æ£æ§ï¼å³ä½¿å¨å­å¨æ¾èéç­è·åå½¢çæåµä¸ä¹è½äº§çå¹³æ»ä¸çå®çåå½¢ç»æã
*   <strong>ç¨æ·ç ç©¶éªè¯</strong>ï¼ç¨æ·ç ç©¶ç»ææ¾ç¤ºï¼è¶è¿80%çç¨æ·è®¤ä¸ºGaussianMorphingå¨ç»æç¸ä¼¼æ§ãçº¹çä¸è´æ§åè¾¹ç¼è¿ç»­æ§ç­ææææ ä¸è¡¨ç°æ´ä¼ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
è®ºæä¸­æ²¡ææç¡®ååºå½åæ¹æ³çå±éæ§ï¼ä½ä»å¶åæ°ç¹åä¸å¶ä»æ¹æ³çå¯¹æ¯ä¸­å¯ä»¥æ¨æ­åºä¸äºæ½å¨çææææªæ¥æ¹è¿æ¹åï¼
*   <strong>è®¡ç®ææ¬</strong>ï¼è½ç¶è®ºææå°çæåå§æ··åç½æ ¼-é«æ¯è¡¨ç¤ºéè¦çº¦1å°æ¶ï¼ä¼åè¿ç¨éè¦500-1000æ¬¡è¿­ä»£ï¼ä½å¯¹äºè¶å¤§è§æ¨¡æå®æ¶åºç¨ï¼å¶è®¡ç®æçå¯è½ä»ææåç©ºé´ã
*   <strong>ç½æ ¼è´¨éä¾èµ</strong>ï¼è½ç¶æ¹æ³éè¿æ³æ¾éå»ºåæ­£ååé¡¹ç¡®ä¿ç½æ ¼è´¨éï¼ä½åå§ç½æ ¼æåçè´¨éä»å¯è½å½±åæç»åå½¢ææã
*   <strong>ææååéå¶</strong>ï¼å°½ç®¡æ¹æ³éè¿ç½æ ¼å¼å¯¼è§£å³äºææè¿æ¥æ§é®é¢ï¼ä½å¯¹äºæç«¯ææååï¼ä¾å¦ï¼ä¸ä¸ªç©ä½åè£æå¤ä¸ªé¨åæå¤ä¸ªç©ä½åå¹¶æä¸ä¸ªï¼çåºæ¯ï¼å¶å¤çè½åå¯è½ä»éè¿ä¸æ­¥æ¢ç´¢ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>æ´å¤æçææåå</strong>ï¼æ¢ç´¢å¦ä½å¤çæ´æç«¯æå¨æçææååï¼ä¾å¦ç©ä½åè£æåå¹¶ã
*   <strong>å®æ¶æ§è½ä¼å</strong>ï¼è¿ä¸æ­¥ä¼åç®æ³ï¼ä»¥å®ç°æ´å¿«çéå»ºååå½¢éåº¦ï¼æ»¡è¶³å®æ¶åºç¨çéæ±ã
*   <strong>å¤æ¨¡æè¾å¥</strong>ï¼æ¢ç´¢ç»åå¶ä»æ¨¡æï¼å¦ææ¬æè¿°ãé³é¢ï¼æ¥æå¯¼åå½¢è¿ç¨ï¼å®ç°æ´ä¸°å¯çè¯­ä¹æ§å¶ã
*   <strong>ç¨æ·äº¤äºæ§</strong>ï¼å¼åæ´ç´è§çç¨æ·çé¢åå·¥å·ï¼åè®¸ç¨æ·æ´ç²¾ç»å°æ§å¶åå½¢è¿ç¨ä¸­çç¹å®åºåæå±æ§ã
*   <strong>å¤§è§æ¨¡æ°æ®éåæ³å</strong>ï¼å¨æ´å¤§ãæ´å¤æ ·åçæ°æ®éä¸è¿è¡è®­ç»åè¯ä¼°ï¼è¿ä¸æ­¥æåæ¨¡åçæ³åè½åã</p>
<hr />
<p>è¿ç¯è®ºæä¸ºä¸ç»´å¯¹è±¡åå½¢é¢åè®¾å®äºæ°æ åï¼éè¿å°3DGSä¸ç½æ ¼å¼å¯¼åå½¢ç¸ç»åï¼å®ç°äºåææªæçå ä½é²æ£æ§ãçº¹çè¿è´¯æ§åè¾å¥å¯è®¿é®æ§ï¼ä¸ºè§è§ææåæ°å­åå®¹åä½å¼è¾äºæ°çå¯è½æ§ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce GaussianMorphing, a novel framework for semantic-aware 3D shape
and texture morphing from multi-view images.</li>
<li>Our method overcomes these limitations by leveraging mesh-guided 3D
Gaussian Splatting (3DGS) for high-fidelity geometry and appearance modeling.</li>
<li>On our
proposed TexMorph benchmark, GaussianMorphing substantially outperforms prior
2D/3D methods, reducing color consistency error (<script type="math/tex">\Delta E</script>) by 22.2% and EI by
26.2%.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2510.02034v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2510.02034v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-10-03 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
