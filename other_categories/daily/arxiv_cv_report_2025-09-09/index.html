<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-09-09 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-09-08/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../../depth_completion/DNet/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-09-09">Arxiv Computer Vision Papers - 2025-09-09</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#arxiv-2025-09-08" class="nav-link">Arxiv 计算机视觉每日报告执行摘要 (2025-09-08)</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#h_2ot-hierarchical-hourglass-tokenizer-for-efficient-video-pose-transformers" class="nav-link">H_{2}OT: Hierarchical Hourglass Tokenizer for Efficient Video Pose Transformers</a>
                </li>
                <li class="nav-item">
                    <a href="#deep-reactive-policy-learning-reactive-manipulator-motion-planning-for-dynamic-environments" class="nav-link">Deep Reactive Policy: Learning Reactive Manipulator Motion Planning for Dynamic Environments</a>
                </li>
                <li class="nav-item">
                    <a href="#scaling-transformer-based-novel-view-synthesis-models-with-token-disentanglement-and-synthetic-data" class="nav-link">Scaling Transformer-Based Novel View Synthesis Models with Token Disentanglement and Synthetic Data</a>
                </li>
                <li class="nav-item">
                    <a href="#interleaving-reasoning-for-better-text-to-image-generation" class="nav-link">Interleaving Reasoning for Better Text-to-Image Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#fomo4wheat-toward-reliable-crop-vision-foundation-models-with-globally-curated-data" class="nav-link">FoMo4Wheat: Toward reliable crop vision foundation models with globally curated data</a>
                </li>
                <li class="nav-item">
                    <a href="#umo-scaling-multi-identity-consistency-for-image-customization-via-matching-reward" class="nav-link">UMO: Scaling Multi-Identity Consistency for Image Customization via Matching Reward</a>
                </li>
                <li class="nav-item">
                    <a href="#urbantwin-high-fidelity-synthetic-replicas-of-roadside-lidar-datasets" class="nav-link">UrbanTwin: High-Fidelity Synthetic Replicas of Roadside Lidar Datasets</a>
                </li>
                <li class="nav-item">
                    <a href="#causnvs-autoregressive-multi-view-diffusion-for-flexible-3d-novel-view-synthesis" class="nav-link">CausNVS: Autoregressive Multi-view Diffusion for Flexible 3D Novel View Synthesis</a>
                </li>
                <li class="nav-item">
                    <a href="#tide-achieving-balanced-subject-driven-image-generation-via-target-instructed-diffusion-enhancement" class="nav-link">TIDE: Achieving Balanced Subject-Driven Image Generation via Target-Instructed Diffusion Enhancement</a>
                </li>
                <li class="nav-item">
                    <a href="#ws2-weakly-supervised-segmentation-using-before-after-supervision-in-waste-sorting" class="nav-link">WS^2: Weakly Supervised Segmentation using Before-After Supervision in Waste Sorting</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-09-09">Arxiv Computer Vision Papers - 2025-09-09</h1>
<h2 id="executive-summary">Executive Summary</h2>
<h2 id="arxiv-2025-09-08">Arxiv 计算机视觉每日报告执行摘要 (2025-09-08)</h2>
<p><strong>概述：</strong></p>
<p>今天的 Arxiv 计算机视觉论文主要围绕以下几个核心主题：<strong>高效且可扩展的生成模型</strong>（特别是针对图像、视频和3D内容）、<strong>机器人操作与具身智能</strong>、<strong>领域特定应用</strong>（如农业和废物分类），以及<strong>数据效率与合成数据利用</strong>。生成式AI持续占据主导地位，研究人员致力于提升其在复杂任务中的性能、效率和可控性。</p>
<p><strong>主要主题与趋势：</strong></p>
<ol>
<li><strong>高效与可扩展的生成模型：</strong> 多篇论文聚焦于如何使Transformer和扩散模型在处理高维数据（如视频、3D场景）时更高效、更具可扩展性。例如，通过分层结构、Token解耦或自回归方法来优化性能。</li>
<li><strong>3D 视觉与新视角合成 (NVS)：</strong> 3D内容生成和新视角合成是热门领域，研究人员探索了如何利用Transformer和扩散模型生成高质量、一致的3D场景，并解决数据稀疏性问题。</li>
<li><strong>机器人操作与具身智能：</strong> 机器人学习和动态环境下的运动规划是另一个重要方向，旨在提高机器人在复杂、不确定环境中的决策和执行能力。</li>
<li><strong>领域特定应用与基础模型：</strong> 计算机视觉技术正被应用于更具体的领域，如农业（作物健康监测）和工业（废物分类），并尝试构建领域特定的基础模型。</li>
<li><strong>数据效率与合成数据：</strong> 面对真实数据采集的挑战，合成数据和弱监督学习被广泛用于弥补数据不足，提升模型泛化能力。</li>
</ol>
<p><strong>特别重要或创新的论文：</strong></p>
<ul>
<li><strong>"Scaling Transformer-Based Novel View Synthesis Models with Token Disentanglement and Synthetic Data" (Nithin Gopalakrishnan Nair et al.)：</strong> 这篇论文通过Token解耦和大规模合成数据，显著提升了Transformer在新视角合成任务中的可扩展性和性能，为未来3D内容生成提供了重要思路。</li>
<li><strong>"Interleaving Reasoning for Better Text-to-Image Generation" (Wenxuan Huang et al.)：</strong> 引入交错推理机制，旨在提高文本到图像生成模型理解复杂指令和生成逻辑一致图像的能力，对提升生成质量和可控性具有重要意义。</li>
<li><strong>"FoMo4Wheat: Toward reliable crop vision foundation models with globally curated data" (Bing Han et al.)：</strong> 这是一个构建领域特定基础模型的优秀案例，通过全球策展数据为农业视觉任务提供可靠的基础模型，展示了CV在垂直领域的巨大潜力。</li>
<li><strong>"CausNVS: Autoregressive Multi-view Diffusion for Flexible 3D Novel View Synthesis" (Xin Kong et al.)：</strong> 将自回归多视图扩散模型引入3D新视角合成，提供了更灵活、高质量的3D内容生成方法，是扩散模型在3D领域应用的又一进展。</li>
</ul>
<p><strong>新兴研究方向或技术：</strong></p>
<ul>
<li><strong>分层/解耦Token化：</strong> 在处理高维数据（如视频、3D）时，通过分层或解耦Token来提高Transformer的效率和可扩展性。</li>
<li><strong>交错推理 (Interleaving Reasoning)：</strong> 在生成模型中引入更复杂的推理机制，以更好地理解输入并生成更具逻辑性的输出。</li>
<li><strong>匹配奖励 (Matching Reward) 用于多身份一致性：</strong> 在图像定制和生成中，通过匹配奖励来确保生成内容在多身份场景下的一致性。</li>
<li><strong>自回归多视图扩散模型：</strong> 将扩散模型与自回归范式结合，用于生成多视图数据，特别是在3D新视角合成中展现出潜力。</li>
<li><strong>领域特定基础模型：</strong> 针对特定应用领域（如农业、工业）构建大规模预训练的基础模型，以提高泛化能力和数据效率。</li>
</ul>
<p><strong>建议阅读的论文：</strong></p>
<p>对于希望深入了解最新进展的研究人员，建议优先阅读以下论文：</p>
<ol>
<li><strong>"Scaling Transformer-Based Novel View Synthesis Models with Token Disentanglement and Synthetic Data" (Nithin Gopalakrishnan Nair et al.)：</strong> 了解3D生成和Transformer可扩展性的前沿。</li>
<li><strong>"Interleaving Reasoning for Better Text-to-Image Generation" (Wenxuan Huang et al.)：</strong> 关注文本到图像生成中推理能力和可控性的提升。</li>
<li><strong>"CausNVS: Autoregressive Multi-view Diffusion for Flexible 3D Novel View Synthesis" (Xin Kong et al.)：</strong> 探索扩散模型在3D生成中的新应用。</li>
<li><strong>"FoMo4Wheat: Toward reliable crop vision foundation models with globally curated data" (Bing Han et al.)：</strong> 了解领域特定基础模型和CV在农业中的应用。</li>
<li><strong>"Deep Reactive Policy: Learning Reactive Manipulator Motion Planning for Dynamic Environments" (Jiahui Yang et al.)：</strong> 如果对机器人操作和具身智能感兴趣，这篇论文提供了动态环境下运动规划的新方法。</li>
</ol>
<p>这些论文代表了当前计算机视觉领域在生成模型、3D理解和实际应用方面的重要进展，为未来的研究提供了宝贵的见解。</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2509.06956v1">H<script type="math/tex">_{2}</script>OT: Hierarchical Hourglass Tokenizer for Efficient Video Pose Transformers</a></li>
<li><a href="#2509.06953v1">Deep Reactive Policy: Learning Reactive Manipulator Motion Planning for Dynamic Environments</a></li>
<li><a href="#2509.06950v1">Scaling Transformer-Based Novel View Synthesis Models with Token Disentanglement and Synthetic Data</a></li>
<li><a href="#2509.06945v1">Interleaving Reasoning for Better Text-to-Image Generation</a></li>
<li><a href="#2509.06907v1">FoMo4Wheat: Toward reliable crop vision foundation models with globally curated data</a></li>
<li><a href="#2509.06818v1">UMO: Scaling Multi-Identity Consistency for Image Customization via Matching Reward</a></li>
<li><a href="#2509.06781v1">UrbanTwin: High-Fidelity Synthetic Replicas of Roadside Lidar Datasets</a></li>
<li><a href="#2509.06579v1">CausNVS: Autoregressive Multi-view Diffusion for Flexible 3D Novel View Synthesis</a></li>
<li><a href="#2509.06499v1">TIDE: Achieving Balanced Subject-Driven Image Generation via Target-Instructed Diffusion Enhancement</a></li>
<li><a href="#2509.06485v1">WS<script type="math/tex">^2</script>: Weakly Supervised Segmentation using Before-After Supervision in Waste Sorting</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2509.06956v1'></a></p>
<h2 id="h_2ot-hierarchical-hourglass-tokenizer-for-efficient-video-pose-transformers"><a href="https://arxiv.org/abs/2509.06956v1">H<script type="math/tex">_{2}</script>OT: Hierarchical Hourglass Tokenizer for Efficient Video Pose Transformers</a></h2>
<p><strong>Authors:</strong> Wenhao Li, Mengyuan Liu, Hong Liu, Pichao Wang, Shijian Lu, Nicu Sebe</p>
<p><strong>Published:</strong> 2025-09-08</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Transformers have been successfully applied in the field of video-based 3D
human pose estimation. However, the high computational costs of these video
pose transformers (VPTs) make them impractical on resource-constrained devices.
In this paper, we present a hierarchical plug-and-play pruning-and-recovering
framework, called Hierarchical Hourglass Tokenizer (H<script type="math/tex">_{2}</script>OT), for efficient
transformer-based 3D human pose estimation from videos. H<script type="math/tex">_{2}</script>OT begins with
progressively pruning pose tokens of redundant frames and ends with recovering
full-length sequences, resulting in a few pose tokens in the intermediate
transformer blocks and thus improving the model efficiency. It works with two
key modules, namely, a Token Pruning Module (TPM) and a Token Recovering Module
(TRM). TPM dynamically selects a few representative tokens to eliminate the
redundancy of video frames, while TRM restores the detailed spatio-temporal
information based on the selected tokens, thereby expanding the network output
to the original full-length temporal resolution for fast inference. Our method
is general-purpose: it can be easily incorporated into common VPT models on
both seq2seq and seq2frame pipelines while effectively accommodating different
token pruning and recovery strategies. In addition, our H<script type="math/tex">_{2}</script>OT reveals that
maintaining the full pose sequence is unnecessary, and a few pose tokens of
representative frames can achieve both high efficiency and estimation accuracy.
Extensive experiments on multiple benchmark datasets demonstrate both the
effectiveness and efficiency of the proposed method. Code and models are
available at https://github.com/NationalGAILab/HoT.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于“H<script type="math/tex">_{2}</script>OT: Hierarchical Hourglass Tokenizer for Efficient Video Pose Transformers”的全面摘要，由Wenhao Li, Mengyuan Liu, Hong Liu, Pichao Wang, Shijian Lu, Nicu Sebe撰写。</p>
<hr />
<h3 id="h_2ot-transformertoken">H<script type="math/tex">_{2}</script>OT: 用于高效视频姿态Transformer的层次化沙漏型Token分词器</h3>
<p><strong>1. 主要问题或研究问题：</strong>
视频姿态Transformer (VPTs) 在视频3D人体姿态估计领域取得了显著成功，但其高昂的计算成本（特别是自注意力机制的二次复杂度）使其在资源受限设备上部署不切实际。现有方法要么导致时间感受野过小，要么引入冗余计算。因此，核心问题是如何在保持高精度姿态估计的同时，显著提高VPTs的计算效率，尤其是在处理长视频序列时。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
本文提出了一个名为<strong>层次化沙漏型Token分词器 (Hierarchical Hourglass Tokenizer, H<script type="math/tex">_{2}</script>OT)</strong> 的即插即用剪枝与恢复框架，旨在解决VPTs的效率问题。其主要创新点包括：</p>
<ul>
<li><strong>层次化剪枝设计（Pyramidal Feature Hierarchy）：</strong> H<script type="math/tex">_{2}</script>OT采用渐进式剪枝策略，随着Transformer层级的深入，逐步减少冗余帧的姿态Token数量，形成一个“奖杯状”（金字塔形）的特征层次结构。这比一次性剪枝更有效地保留了有用信息，并进一步提高了效率。</li>
<li><strong>Token剪枝模块 (Token Pruning Module, TPM)：</strong> TPM负责动态选择少量具有代表性的Token，以消除视频帧中的冗余。论文探索了四种剪枝策略：<ul>
<li><strong>Token Pruning Cluster (TPC)：</strong> 基于k近邻密度峰值聚类算法，选择密度高且与其他高密度Token距离远的Token作为代表。</li>
<li><strong>Token Pruning Attention (TPA)：</strong> 利用Transformer的自注意力分数来选择信息量大的Token。</li>
<li><strong>Token Pruning Motion (TPMo)：</strong> 基于人体运动变化来选择代表性帧，保留运动变化显著的帧。</li>
<li><strong>Token Pruning Sampler (TPS)：</strong> 采用线性采样策略，均匀地从时间维度上采样Token。TPS被证明是参数无关、高效且适合插值恢复的策略。</li>
</ul>
</li>
<li><strong>Token恢复模块 (Token Recovering Module, TRM)：</strong> TRM旨在将剪枝操作导致的低时间分辨率恢复到原始全长序列，以实现快速推理。论文提出了两种恢复策略：<ul>
<li><strong>Token Recovering Attention (TRA)：</strong> 使用轻量级多头交叉注意力层，将可学习的Token作为查询，将剪枝后的代表性Token作为键和值，恢复全长Token。</li>
<li><strong>Token Recovering Interpolation (TRI)：</strong> 在剪枝后的Token经过回归头估计3D姿态序列后，使用简单的线性插值操作恢复全长3D姿态。TRI与TPS结合使用时，被证明是参数无关且高效的。</li>
</ul>
</li>
<li><strong>通用性和兼容性：</strong> H<script type="math/tex">_{2}</script>OT是一个通用框架，可以轻松集成到现有的VPT模型中，并支持seq2seq和seq2frame两种推理管道，同时灵活适应不同的Token剪枝和恢复策略。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong>
通过在Human3.6M和MPI-INF-3DHP等多个基准数据集上进行大量实验，H<script type="math/tex">_{2}</script>OT展示了其有效性和效率：</p>
<ul>
<li><strong>显著的效率提升：</strong> H<script type="math/tex">_{2}</script>OT能够大幅降低计算成本。例如，在MixSTE模型上，H<script type="math/tex">_{2}</script>OT将FLOPs减少了57.4%，FPS提升了87.8%，同时GPU内存消耗和训练时间也显著减少。与作者之前的会议版本HoT [21]相比，H<script type="math/tex">_{2}</script>OT在FLOPs、GPU内存和训练时间上进一步减少，FPS更高，性能也更好。</li>
<li><strong>保持甚至提升精度：</strong> 尽管大幅减少了Token数量，H<script type="math/tex">_{2}</script>OT在Human3.6M数据集上仍能保持甚至略微提升姿态估计精度（例如，MixSTE上MPJPE提升0.5mm）。这表明维持完整的姿态序列是不必要的，少量代表性帧的姿态Token足以实现高效率和高精度。</li>
<li><strong>对不同VPT模型的普适性：</strong> H<script type="math/tex">_{2}</script>OT成功应用于MHFormer、MixSTE、MotionBERT和MotionAGFormer等SOTA VPT模型，证明了其即插即用的通用性。</li>
<li><strong>对推理管道的适应性：</strong> H<script type="math/tex">_{2}</script>OT在seq2seq和seq2frame两种推理管道下均表现出色，尤其是在seq2seq管道中，效率提升更为显著。</li>
</ul>
<p><strong>4. 论文中提到的局限性：</strong>
*   <strong>低FPS场景下的性能略有下降：</strong> 在低帧率（稀疏时间信息）场景下，H<script type="math/tex">_{2}</script>OT在MixSTE上的性能略低于原始MixSTE。这可能是因为在稀疏数据中，Token剪枝能去除的冗余信息较少。
*   <strong>TPC和TPA的推理负担：</strong> TPC和TPA虽然是动态剪枝方法，但相比于TPS，它们在推理时会引入额外的计算负担，导致FPS较低。
*   <strong>挑战性场景下的失败案例：</strong> 在某些复杂场景（如部分身体可见、罕见姿态或2D检测器存在显著误差）下，H<script type="math/tex">_{2}</script>OT仍可能无法准确估计3D人体姿态。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>更智能的Token选择策略：</strong> 探索更先进的动态Token剪枝策略，在保持效率的同时，进一步优化在稀疏时间信息或复杂运动场景下的性能。
*   <strong>自适应剪枝与恢复：</strong> 研究如何根据视频内容或运动模式自适应地调整剪枝率和恢复策略，以实现更优的性能-效率权衡。
*   <strong>与其他高效Transformer技术的结合：</strong> 探索H<script type="math/tex">_{2}</script>OT与其它Transformer压缩或加速技术（如量化、知识蒸馏等）的结合，以进一步提升效率。
*   <strong>扩展到其他视频理解任务：</strong> 将H<script type="math/tex">_{2}</script>OT的理念和方法推广到其他需要处理长视频序列的计算机视觉任务中，如动作识别、行为分析等。</p>
<hr />
<p>总而言之，H<script type="math/tex">_{2}</script>OT通过引入层次化剪枝和高效恢复机制，成功地解决了视频姿态Transformer的计算效率瓶颈，证明了在3D人体姿态估计中，无需维持完整的姿态序列，少量代表性帧的Token即可实现高效率和高精度。这为未来开发更强大、更快速的VPT模型提供了重要的方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this paper, we present a hierarchical plug-and-play pruning-and-recovering
framework, called Hierarchical Hourglass Tokenizer (H<script type="math/tex">_{2}</script>OT), for efficient
transformer-based 3D human pose estimation from videos.</li>
<li>Our method
is general-purpose: it can be easily incorporated into common VPT models on
both seq2seq and seq2frame pipelines while effectively accommodating different
token pruning and recovery strategies.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.06956v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.06956v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.06953v1'></a></p>
<h2 id="deep-reactive-policy-learning-reactive-manipulator-motion-planning-for-dynamic-environments"><a href="https://arxiv.org/abs/2509.06953v1">Deep Reactive Policy: Learning Reactive Manipulator Motion Planning for Dynamic Environments</a></h2>
<p><strong>Authors:</strong> Jiahui Yang, Jason Jingzhou Liu, Yulong Li, Youssef Khaky, Kenneth Shaw, Deepak Pathak</p>
<p><strong>Published:</strong> 2025-09-08</p>
<p><strong>Categories:</strong> cs.RO, cs.AI, cs.CV, cs.LG, cs.SY, eess.SY</p>
<p><strong>Abstract:</strong></p>
<p>Generating collision-free motion in dynamic, partially observable
environments is a fundamental challenge for robotic manipulators. Classical
motion planners can compute globally optimal trajectories but require full
environment knowledge and are typically too slow for dynamic scenes. Neural
motion policies offer a promising alternative by operating in closed-loop
directly on raw sensory inputs but often struggle to generalize in complex or
dynamic settings. We propose Deep Reactive Policy (DRP), a visuo-motor neural
motion policy designed for reactive motion generation in diverse dynamic
environments, operating directly on point cloud sensory input. At its core is
IMPACT, a transformer-based neural motion policy pretrained on 10 million
generated expert trajectories across diverse simulation scenarios. We further
improve IMPACT's static obstacle avoidance through iterative student-teacher
finetuning. We additionally enhance the policy's dynamic obstacle avoidance at
inference time using DCP-RMP, a locally reactive goal-proposal module. We
evaluate DRP on challenging tasks featuring cluttered scenes, dynamic moving
obstacles, and goal obstructions. DRP achieves strong generalization,
outperforming prior classical and neural methods in success rate across both
simulated and real-world settings. Video results and code available at
https://deep-reactive-policy.com</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Jiahui Yang等人撰写的论文“Deep Reactive Policy: Learning Reactive Manipulator Motion Planning for Dynamic Environments”的全面摘要。</p>
<hr />
<h3 id="deep-reactive-policy-learning-reactive-manipulator-motion-planning-for-dynamic-environments_1">论文摘要：Deep Reactive Policy: Learning Reactive Manipulator Motion Planning for Dynamic Environments</h3>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决机器人机械手在动态、部分可观测环境中生成无碰撞运动的根本性挑战。传统的运动规划器虽然能计算全局最优轨迹，但需要完整的环境知识且在动态场景中速度过慢。而现有的神经运动策略虽然能直接从原始传感器输入进行闭环操作，但在复杂或动态环境中泛化能力不足。因此，研究问题是如何开发一种能够实现反应式、无碰撞运动规划的策略，使其在多样化动态环境中具有强大的泛化能力和实时性能。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
该论文提出了<strong>深度反应式策略（Deep Reactive Policy, DRP）</strong>，这是一种基于点云感官输入的视觉-运动神经运动策略，用于在多样化动态环境中生成反应式运动。DRP的核心创新包括：</p>
<ul>
<li><strong>大规模运动预训练的IMPACT：</strong> DRP的核心是IMPACT（Imitating Motion Planning with Action-Chunking Transformer），一个基于Transformer的神经运动策略。它通过行为克隆在包含1000万条由最先进的GPU加速运动规划器cuRobo生成的专家轨迹的大规模离线数据集上进行预训练。这使得策略能够学习到全局规划能力。</li>
<li><strong>迭代式师生微调（Student-Teacher Finetuning）：</strong> 为了提高IMPACT的静态障碍物避障能力，论文引入了迭代式师生微调方法。教师策略结合了预训练的IMPACT和Geometric Fabrics（一种擅长局部避障的闭环控制器），将Geometric Fabrics的局部避障行为蒸馏到IMPACT策略中，使其能够直接从点云输入进行操作。</li>
<li><strong>动态最近点RMP（DCP-RMP）模块：</strong> 为了在推理时进一步增强策略的动态障碍物避障性能，DRP整合了一个局部反应式目标提议模块——DCP-RMP。DCP-RMP是一种非学习型组件，它利用局部障碍物信息，通过识别动态障碍物中的最近点并生成排斥运动，实时调整原始关节空间目标，从而优先处理动态障碍物避障。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong>
DRP在DRPBench基准测试（包括杂乱场景、动态移动障碍物和目标阻塞等挑战性任务）以及MπNets数据集上进行了广泛评估，并在模拟和真实世界环境中均取得了显著成果：</p>
<ul>
<li><strong>强大的泛化能力：</strong> DRP在各种动态和目标阻塞任务中表现出色，显著优于先前的经典和基于学习的方法。例如，在浮动动态障碍物（FDO）和动态目标阻塞（DGB）任务中，DRP的成功率远高于仅使用IMPACT或cuRobo。</li>
<li><strong>实时反应性：</strong> DRP的闭环性质和DCP-RMP模块的集成使其能够对动态环境变化做出快速反应，解决了传统规划器在动态场景中速度慢的问题。</li>
<li><strong>超越预训练数据源的性能：</strong> 尽管DRP使用cuRobo生成的轨迹进行预训练，但通过师生微调，其性能显著超越了原始数据源，表明该方法不仅继承了有用行为，而且在复杂环境中实现了更有效的泛化。</li>
<li><strong>真实世界部署能力：</strong> 尽管完全在模拟中训练，DRP在真实世界环境中表现出良好的适应性，在静态环境、突然出现的障碍物和目标阻塞等任务中取得了接近完美的成功率。</li>
</ul>
<p>这些结果表明，DRP成功地结合了全局规划能力和局部反应性，为机器人机械手在复杂动态环境中的运动规划提供了一个鲁棒、高效且可泛化的解决方案。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>对点云观测的依赖：</strong> DRP的有效规划依赖于相对准确的点云观测。在严重的感知失败（例如在狭窄环境中频繁遮挡）下，性能可能会下降。多摄像头设置有助于缓解此问题，但可能不足以应对所有情况。
*   <strong>单一机器人平台：</strong> 实验仅限于单个机器人平台（Franka Panda），未在其他机器人平台上进行评估。这限制了其在多机器人平台上的可扩展性。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>多机器人平台泛化：</strong> 未来的工作目标是解决多机器人平台的可扩展性问题，可以通过为每个机器人生成单独的规划器，或者训练一个能够泛化到多个机器人平台的单一DRP策略来实现。
*   <strong>更丰富的传感器输入：</strong> 利用RGB或RGB-D输入可以进一步提高在非结构化环境中的性能。</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose Deep Reactive Policy (DRP), a visuo-motor neural
motion policy designed for reactive motion generation in diverse dynamic
environments, operating directly on point cloud sensory input.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.06953v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.06953v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.06950v1'></a></p>
<h2 id="scaling-transformer-based-novel-view-synthesis-models-with-token-disentanglement-and-synthetic-data"><a href="https://arxiv.org/abs/2509.06950v1">Scaling Transformer-Based Novel View Synthesis Models with Token Disentanglement and Synthetic Data</a></h2>
<p><strong>Authors:</strong> Nithin Gopalakrishnan Nair, Srinivas Kaza, Xuan Luo, Vishal M. Patel, Stephen Lombardi, Jungyeon Park</p>
<p><strong>Published:</strong> 2025-09-08</p>
<p><strong>Categories:</strong> cs.GR, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Large transformer-based models have made significant progress in
generalizable novel view synthesis (NVS) from sparse input views, generating
novel viewpoints without the need for test-time optimization. However, these
models are constrained by the limited diversity of publicly available scene
datasets, making most real-world (in-the-wild) scenes out-of-distribution. To
overcome this, we incorporate synthetic training data generated from diffusion
models, which improves generalization across unseen domains. While synthetic
data offers scalability, we identify artifacts introduced during data
generation as a key bottleneck affecting reconstruction quality. To address
this, we propose a token disentanglement process within the transformer
architecture, enhancing feature separation and ensuring more effective
learning. This refinement not only improves reconstruction quality over
standard transformers but also enables scalable training with synthetic data.
As a result, our method outperforms existing models on both in-dataset and
cross-dataset evaluations, achieving state-of-the-art results across multiple
benchmarks while significantly reducing computational costs. Project page:
https://scaling3dnvs.github.io/</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Nithin Gopalakrishnan Nair等人撰写的论文“Scaling Transformer-Based Novel View Synthesis Models with Token Disentanglement and Synthetic Data”的全面摘要。</p>
<hr />
<h3 id="scaling-transformer-based-novel-view-synthesis-models-with-token-disentanglement-and-synthetic-data_1">论文摘要：Scaling Transformer-Based Novel View Synthesis Models with Token Disentanglement and Synthetic Data</h3>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决通用型新视角合成（NVS）模型在处理稀疏输入视图时面临的两个核心挑战：
*   <strong>数据多样性限制：</strong> 现有基于Transformer的NVS模型虽然在生成新视角方面取得了显著进展，但受限于公开场景数据集的有限多样性，导致模型在真实世界（in-the-wild）场景中泛化能力不足，因为这些场景往往是“分布外”的。
*   <strong>合成数据引入的伪影：</strong> 尽管合成数据为解决数据稀缺问题提供了可扩展的途径，但扩散模型生成的数据中常常包含伪影，这些伪影会严重影响重建质量，成为模型性能提升的关键瓶颈。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
为了克服上述挑战，作者提出了以下关键创新：</p>
<ul>
<li><strong>Token Disentangled (Tok-D) Transformer Block：</strong> 针对合成数据中存在的伪影问题，作者引入了一种名为Tok-D的Transformer块。该模块通过层级调制（layer-wise modulation）显式区分源令牌（source tokens）和目标令牌（target tokens），增强了特征分离，确保更有效的学习。这种设计使得模型能够更鲁棒地处理合成数据中的伪影，并更高效地分配表示容量。</li>
<li><strong>改进的合成数据生成与训练方案：</strong> 论文提出了一种新颖的数据生成策略，显著提高了合成样本的质量。具体而言，他们利用CAT3D多视图扩散模型生成大规模合成多视图样本，并将条件图像作为目标视图，生成视图作为输入视图。这种方法强制Transformer始终生成逼真的图像，从而使模型对合成数据中的伪影具有鲁棒性。</li>
<li><strong>增强Transformer架构的可扩展性和效率：</strong> Tok-D Transformer块通过减少冗余并提高数据效率，使得Transformer架构在NVS任务中更具可扩展性和效率，从而在更低的计算成本下实现更高的重建质量。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong>
该方法在多个基准测试中取得了最先进（state-of-the-art）的结果，并具有以下显著意义：</p>
<ul>
<li><strong>重建质量提升：</strong> 相较于标准Transformer，Tok-D Transformer不仅提高了重建质量，而且能够利用合成数据进行可扩展训练。</li>
<li><strong>泛化能力增强：</strong> 通过结合合成训练数据，模型在未见过的领域（跨数据集）上的泛化能力得到显著改善。实验结果表明，该方法在数据集内和跨数据集评估中均优于现有模型。</li>
<li><strong>计算成本降低：</strong> 该方法显著降低了计算成本，同时保持或超越了现有模型的性能。例如，在Re10K数据集上，Tok-D-Plus比LVSM（使用8个GPU训练）的性能提高了1.2 dB，甚至超越了使用64个GPU训练的LVSM。</li>
<li><strong>Token解耦的涌现特性：</strong> 论文发现，Tok-D Transformer块具有解耦源令牌和目标令牌的涌现特性，这使得模型能够更好地利用合成数据，并有效丢弃合成数据中的伪影，从而避免了伪影传播。</li>
</ul>
<p><strong>4. 论文中提及的局限性：</strong>
论文也坦诚地指出了当前方法的局限性：</p>
<ul>
<li><strong>处理遮挡区域的挑战：</strong> 当源图像中被遮挡的区域在新视角中变得可见时，模型难以准确重建这些区域，有时会出现幻觉。作者认为这是一个固有的不适定问题，缺乏明确的解决方案。</li>
<li><strong>内存消耗：</strong> 模型使用较大的令牌尺寸（8x8），导致每个源图像有1024个令牌，这需要大量的内存。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong>
基于上述局限性，论文提出了以下未来研究方向：</p>
<ul>
<li><strong>架构优化：</strong> 探索进一步的架构优化，例如分层Transformer（hierarchical transformers）和更高效的网络（如线性注意力机制和状态空间模型，例如Mamba），以减少内存消耗并提高效率。</li>
<li><strong>解决遮挡问题：</strong> 寻找更有效的方法来处理新视角中出现的遮挡对象，这仍然是一个开放的研究问题。</li>
</ul>
<hr />
<p>总而言之，这篇论文通过引入Token Disentangled Transformer块和改进的合成数据训练策略，成功解决了通用型新视角合成模型在数据多样性和合成数据伪影方面的挑战。其提出的方法不仅显著提升了重建质量和泛化能力，还在降低计算成本方面取得了突破，为未来基于Transformer的NVS模型发展奠定了坚实基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Large transformer-based models have made significant progress in
generalizable novel view synthesis (NVS) from sparse input views, generating
novel viewpoints without the need for test-time optimization.</li>
<li>To address
this, we propose a token disentanglement process within the transformer
architecture, enhancing feature separation and ensuring more effective
learning.</li>
<li>As a result, our method outperforms existing models on both in-dataset and
cross-dataset evaluations, achieving state-of-the-art results across multiple
benchmarks while significantly reducing computational costs.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.06950v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.06950v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.06945v1'></a></p>
<h2 id="interleaving-reasoning-for-better-text-to-image-generation"><a href="https://arxiv.org/abs/2509.06945v1">Interleaving Reasoning for Better Text-to-Image Generation</a></h2>
<p><strong>Authors:</strong> Wenxuan Huang, Shuang Chen, Zheyong Xie, Shaosheng Cao, Shixiang Tang, Yufan Shen, Qingyu Yin, Wenbo Hu, Xiaoman Wang, Yuntian Tang, Junbo Qiao, Yue Guo, Yao Hu, Zhenfei Yin, Philip Torr, Yu Cheng, Wanli Ouyang, Shaohui Lin</p>
<p><strong>Published:</strong> 2025-09-08</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.CL, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Unified multimodal understanding and generation models recently have achieve
significant improvement in image generation capability, yet a large gap remains
in instruction following and detail preservation compared to systems that
tightly couple comprehension with generation such as GPT-4o. Motivated by
recent advances in interleaving reasoning, we explore whether such reasoning
can further improve Text-to-Image (T2I) generation. We introduce Interleaving
Reasoning Generation (IRG), a framework that alternates between text-based
thinking and image synthesis: the model first produces a text-based thinking to
guide an initial image, then reflects on the result to refine fine-grained
details, visual quality, and aesthetics while preserving semantics. To train
IRG effectively, we propose Interleaving Reasoning Generation Learning (IRGL),
which targets two sub-goals: (1) strengthening the initial think-and-generate
stage to establish core content and base quality, and (2) enabling high-quality
textual reflection and faithful implementation of those refinements in a
subsequent image. We curate IRGL-300K, a dataset organized into six decomposed
learning modes that jointly cover learning text-based thinking, and full
thinking-image trajectories. Starting from a unified foundation model that
natively emits interleaved text-image outputs, our two-stage training first
builds robust thinking and reflection, then efficiently tunes the IRG pipeline
in the full thinking-image trajectory data. Extensive experiments show SoTA
performance, yielding absolute gains of 5-10 points on GenEval, WISE, TIIF,
GenAI-Bench, and OneIG-EN, alongside substantial improvements in visual quality
and fine-grained fidelity. The code, model weights and datasets will be
released in: https://github.com/Osilly/Interleaving-Reasoning-Generation .</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供对Wenxuan Huang等人撰写的论文“Interleaving Reasoning for Better Text-to-Image Generation”的全面摘要。</p>
<hr />
<p><strong>论文摘要：Interleaving Reasoning for Better Text-to-Image Generation</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
尽管统一多模态理解和生成模型在图像生成能力方面取得了显著进步，但在指令遵循和细节保留方面，与GPT-4o等紧密结合理解与生成的系统相比，仍存在较大差距。本文旨在探讨交错推理（interleaving reasoning）是否能进一步提升文本到图像（T2I）生成质量，特别是在细粒度细节、视觉质量和美学方面。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
*   <strong>引入交错推理生成（Interleaving Reasoning Generation, IRG）框架：</strong> IRG是一个在文本思考和图像合成之间交替进行的框架。模型首先生成文本思考过程来指导初始图像的生成，然后基于初始图像进行反思，以改进细粒度细节、视觉质量和美学，同时保留语义。
*   <strong>提出交错推理生成学习（Interleaving Reasoning Generation Learning, IRGL）：</strong> 为了有效训练IRG，IRGL设定了两个子目标：
    1.  强化初始“思考-生成”阶段，以建立核心内容和基础质量。
    2.  实现高质量的文本反思，并忠实地将这些改进落实到后续图像中。
*   <strong>构建IRGL-300K数据集：</strong> 该数据集包含300K个样本，并被组织成六种分解学习模式，共同涵盖了基于文本的思考学习和完整的“思考-图像”轨迹。
*   <strong>两阶段训练策略：</strong>
    1.  <strong>第一阶段：</strong> 在统一基础模型（能原生输出交错文本-图像）上，通过六种分解学习模式，构建鲁棒的思考和反思能力。
    2.  <strong>第二阶段：</strong> 利用完整的“思考-图像”轨迹数据，高效地微调IRG管道。
*   <strong>定制的CFG条件设计：</strong> 在推理阶段，针对IRG的改进图像生成步骤，引入了CFG（Classifier-Free Guidance）条件设计，以处理多源条件（提示、初始推理、初始图像和改进推理）。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>SoTA性能：</strong> 广泛的实验表明，IRG在GenEval、WISE、TIIF、GenAI-Bench和OneIG-EN等多个主流T2I基准测试上实现了最先进的性能，绝对增益达到5-10个点。
*   <strong>显著提升视觉质量和细粒度保真度：</strong> IRG不仅在语义正确性上表现出色，还在渲染纹理、阴影真实感和手指等精细结构方面实现了显著改进（如图1(a)和(b)所示）。
*   <strong>多模态评估器一致认可：</strong> 通过多模态大语言模型（MLLM）作为评估器的排名研究（表7），表明两轮IRG生成的图像改进得到了不同MLLM评估器的一致认可，平均排名分数从36.7%提升到63.3%。
*   <strong>强大的指令遵循和世界知识推理能力：</strong> 在TIIF和WISE基准上的表现证明了IRG在解释和准确遵循复杂自然语言指令以及整合世界知识方面的卓越能力。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>数据稀缺性：</strong> 构建完整的交错IRG数据（特别是高质量的“初始图像-改进图像”对）非常困难，现有T2I数据集的质量次优，且从GPT-4o蒸馏的数据无法直接生成此类配对。
*   <strong>推理过程中的潜在失败模式：</strong>
    *   <strong>微结构饱和：</strong> 在重复纹理（如织物、树叶）上，改进步骤有时会过度平滑高频细节。
    *   <strong>文本渲染漂移：</strong> 在密集约束下，细化过程可能牺牲文本的可读性以换取风格一致性。
    *   <strong>全局-局部张力：</strong> 在拥挤场景中，局部编辑可能会轻微扰动全局布局。
    *   当T_out^(2) 引入许多同步编辑时，这些问题尤为突出。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>扩展IRG管道：</strong> 将IRG框架应用于更多类型的统一模型，并探索多轮推理而非仅限于单次细化迭代。
*   <strong>数据增强与合成：</strong> 探索更有效的方法来合成高质量的IRG轨迹数据，以缓解数据稀缺问题。
*   <strong>优化编辑策略：</strong> 研究更保守的编辑策略，以提高生成稳定性，同时最大化可实现增益。
*   <strong>解决失败模式：</strong> 针对微结构饱和、文本渲染漂移和全局-局部张力等现有失败模式进行深入研究和改进。</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce Interleaving
Reasoning Generation (IRG), a framework that alternates between text-based
thinking and image synthesis: the model first produces a text-based thinking to
guide an initial image, then reflects on the result to refine fine-grained
details, visual quality, and aesthetics while preserving semantics.</li>
<li>To train
IRG effectively, we propose Interleaving Reasoning Generation Learning (IRGL),
which targets two sub-goals: (1) strengthening the initial think-and-generate
stage to establish core content and base quality, and (2) enabling high-quality
textual reflection and faithful implementation of those refinements in a
subsequent image.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.06945v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.06945v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.06907v1'></a></p>
<h2 id="fomo4wheat-toward-reliable-crop-vision-foundation-models-with-globally-curated-data"><a href="https://arxiv.org/abs/2509.06907v1">FoMo4Wheat: Toward reliable crop vision foundation models with globally curated data</a></h2>
<p><strong>Authors:</strong> Bing Han, Chen Zhu, Dong Han, Rui Yu, Songliang Cao, Jianhui Wu, Scott Chapman, Zijian Wang, Bangyou Zheng, Wei Guo, Marie Weiss, Benoit de Solan, Andreas Hund, Lukas Roth, Kirchgessner Norbert, Andrea Visioni, Yufeng Ge, Wenjuan Li, Alexis Comar, Dong Jiang, Dejun Han, Fred Baret, Yanfeng Ding, Hao Lu, Shouyang Liu</p>
<p><strong>Published:</strong> 2025-09-08</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Vision-driven field monitoring is central to digital agriculture, yet models
built on general-domain pretrained backbones often fail to generalize across
tasks, owing to the interaction of fine, variable canopy structures with
fluctuating field conditions. We present FoMo4Wheat, one of the first
crop-domain vision foundation model pretrained with self-supervision on
ImAg4Wheat, the largest and most diverse wheat image dataset to date (2.5
million high-resolution images collected over a decade at 30 global sites,
spanning &gt;2,000 genotypes and &gt;500 environmental conditions). This
wheat-specific pretraining yields representations that are robust for wheat and
transferable to other crops and weeds. Across ten in-field vision tasks at
canopy and organ levels, FoMo4Wheat models consistently outperform
state-of-the-art models pretrained on general-domain dataset. These results
demonstrate the value of crop-specific foundation models for reliable in-field
perception and chart a path toward a universal crop foundation model with
cross-species and cross-task capabilities. FoMo4Wheat models and the ImAg4Wheat
dataset are publicly available online: https://github.com/PheniX-Lab/FoMo4Wheat
and https://huggingface.co/PheniX-Lab/FoMo4Wheat. The demonstration website is:
https://fomo4wheat.phenix-lab.com/.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供论文“FoMo4Wheat: Toward reliable crop vision foundation models with globally curated data”的全面摘要。</p>
<p><strong>论文摘要：FoMo4Wheat：利用全球精选数据构建可靠的作物视觉基础模型</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
数字农业中，基于视觉的田间监测至关重要。然而，现有模型通常依赖于在通用领域数据集（如ImageNet）上预训练的骨干网络。这些模型在处理精细、多变的作物冠层结构和波动的田间条件时，泛化能力不足，导致在实际农业任务中表现不佳。论文旨在解决这一问题，即如何开发一个在作物领域具有强大泛化能力和鲁棒性的视觉基础模型，以实现可靠的田间感知。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
*   <strong>ImAg4Wheat数据集：</strong> 论文构建并发布了迄今为止最大、最多样化的麦类作物图像数据集ImAg4Wheat。该数据集包含250万张高分辨率图像，跨越十年，在30个全球地点收集，涵盖2000多种基因型和500多种环境条件。这种大规模、多样化的数据集是训练作物领域基础模型的关键。
*   <strong>FoMo4Wheat模型：</strong> 论文提出了FoMo4Wheat，这是首个专门针对作物领域（特别是小麦）的视觉基础模型。它基于标准的Vision Transformer (ViT) 架构，并采用自监督学习（结合Masked Image Modeling (MIM) 和对比学习策略）在ImAg4Wheat数据集上进行预训练。
*   <strong>参数高效微调：</strong> FoMo4Wheat模型在下游任务中采用参数高效微调策略，即在适应特定任务时冻结骨干网络参数，只训练轻量级的任务特定适配器和头部。这提高了计算效率，并保留了骨干网络的通用特征表示。
*   <strong>跨任务和跨物种泛化评估：</strong> 论文在十个田间视觉任务上（包括冠层和器官级别的小麦任务、水稻任务以及其他作物和杂草任务）对FoMo4Wheat进行了系统评估，以验证其跨任务和跨物种的泛化能力。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>卓越的性能：</strong> FoMo4Wheat模型在所有十个评估任务中始终优于在通用领域数据集上预训练的最新（SOTA）模型，尤其在具有挑战性的像素级分割任务上表现出显著改进。
*   <strong>数据稀缺场景下的鲁棒性：</strong> 在数据量减少（如仅使用30%训练数据）的情况下，FoMo4Wheat在生长阶段和疾病分类任务中仍能保持其性能优势，表明其在数据稀缺场景下的优越鲁棒性。
*   <strong>跨平台泛化能力：</strong> FoMo4Wheat在检测不同GSD（地面采样距离）的无人机图像中的麦穗时表现出卓越的性能，证明了其在不同采集平台和配置下的泛化能力。
*   <strong>作物特定基础模型的价值：</strong> 这些结果有力地证明了作物特定基础模型在实现可靠的田间感知方面的价值，并为开发具有跨物种和跨任务能力的通用作物基础模型指明了方向。
*   <strong>特征可视化：</strong> FoMo4Wheat提取的特征嵌入能够形成清晰的聚类，有效区分不同生长阶段的关键植物器官，并能高精度地区分作物物种和杂草类型，优于DINOv2。</p>
<p><strong>4. 论文中提到的局限性：</strong>
*   <strong>计算资源需求：</strong> 尽管FoMo4Wheat Base模型（86M参数）在相对紧凑的架构下实现了具有竞争力的精度，但其计算需求对于边缘设备来说仍然很高。
*   <strong>通用性与专业性的权衡：</strong> 在当前模型容量和训练数据规模下，通用领域基础模型（如DINOv2）在保持广泛领域通用性与实现特定领域（如小麦）的强大专业性之间存在固有权衡。FoMo4Wheat专注于小麦，虽然在小麦任务上表现出色，但未来通用作物基础模型（FoMo4Crop）需要更精细地平衡这种权衡。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>FoMo4Crop：</strong> 论文为开发具有跨物种和跨任务能力的通用作物基础模型FoMo4Crop铺平了道路。未来的研究将致力于实现这种平衡，可能需要新的训练策略、优化的架构以及更大、更多样化的数据集。
*   <strong>模型压缩和优化：</strong> 针对边缘设备部署的需求，需要进一步研究模型压缩、量化和知识蒸馏技术，以在不牺牲精度或泛化能力的情况下，提供轻量级、田间就绪的模型。
*   <strong>多模态数据集成：</strong> 鼓励高通量监测平台的快速扩展，以提供大规模、多模态数据收集，这将有助于实现通用作物基础模型的愿景。
*   <strong>更精细的特征学习：</strong> 尽管FoMo4Wheat在像素级任务中表现出色，但仍需进一步探索如何更有效地捕捉和利用精细的植物特征，以应对更复杂的农业视觉挑战。</p>
<p>总而言之，这篇论文通过构建大规模、多样化的ImAg4Wheat数据集和开发FoMo4Wheat模型，在作物领域视觉基础模型方面取得了显著进展，为数字农业的未来发展奠定了坚实基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present FoMo4Wheat, one of the first
crop-domain vision foundation model pretrained with self-supervision on
ImAg4Wheat, the largest and most diverse wheat image dataset to date (2.5
million high-resolution images collected over a decade at 30 global sites,
spanning &gt;2,000 genotypes and &gt;500 environmental conditions).</li>
<li>Across ten in-field vision tasks at
canopy and organ levels, FoMo4Wheat models consistently outperform
state-of-the-art models pretrained on general-domain dataset.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.06907v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.06907v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.06818v1'></a></p>
<h2 id="umo-scaling-multi-identity-consistency-for-image-customization-via-matching-reward"><a href="https://arxiv.org/abs/2509.06818v1">UMO: Scaling Multi-Identity Consistency for Image Customization via Matching Reward</a></h2>
<p><strong>Authors:</strong> Yufeng Cheng, Wenxu Wu, Shaojin Wu, Mengqi Huang, Fei Ding, Qian He</p>
<p><strong>Published:</strong> 2025-09-08</p>
<p><strong>Categories:</strong> cs.CV, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Recent advancements in image customization exhibit a wide range of
application prospects due to stronger customization capabilities. However,
since we humans are more sensitive to faces, a significant challenge remains in
preserving consistent identity while avoiding identity confusion with
multi-reference images, limiting the identity scalability of customization
models. To address this, we present UMO, a Unified Multi-identity Optimization
framework, designed to maintain high-fidelity identity preservation and
alleviate identity confusion with scalability. With "multi-to-multi matching"
paradigm, UMO reformulates multi-identity generation as a global assignment
optimization problem and unleashes multi-identity consistency for existing
image customization methods generally through reinforcement learning on
diffusion models. To facilitate the training of UMO, we develop a scalable
customization dataset with multi-reference images, consisting of both
synthesised and real parts. Additionally, we propose a new metric to measure
identity confusion. Extensive experiments demonstrate that UMO not only
improves identity consistency significantly, but also reduces identity
confusion on several image customization methods, setting a new
state-of-the-art among open-source methods along the dimension of identity
preserving. Code and model: https://github.com/bytedance/UMO</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Yufeng Cheng等人撰写的论文“UMO: Scaling Multi-Identity Consistency for Image Customization via Matching Reward”的全面摘要。</p>
<hr />
<h3 id="umo">论文摘要：UMO: 通过匹配奖励实现图像定制中的多身份一致性扩展</h3>
<p><strong>1. 主要问题或研究问题：</strong>
图像定制（Image Customization）在生成图像时，需要同时遵循文本提示的语义内容和参考图像的视觉外观。特别是在人类身份定制方面，现有方法面临两大挑战：
*   <strong>身份一致性（Identity Preservation）：</strong> 即使是细微的外观差异也可能导致身份保真度显著下降，人类对人脸的敏感性使得这一问题尤为突出。
*   <strong>多身份混淆（Identity Confusion）：</strong> 当需要同时定制多个身份时，模型不仅要保留每个个体身份的独特特征，还要在生成图像中保持它们之间清晰的区别，避免身份混淆。现有方法通常采用“一对一映射”范式，随着身份数量的增加，这种范式在处理“身份内变异性”（intra-ID variability）和“身份间区分度”（inter-ID distinction）方面表现不佳，限制了模型的扩展性。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
为了解决上述挑战，论文提出了 <strong>UMO (Unified Multi-identity Optimization)</strong> 框架，其核心创新点包括：</p>
<ul>
<li><strong>多对多匹配范式（Multi-to-Multi Matching Paradigm）：</strong> UMO 将多身份生成重新定义为一个全局分配优化问题。与传统的一对一映射不同，UMO 旨在最大化多个生成身份与多个参考身份之间的整体匹配质量，从而为每个生成身份找到最合适的参考身份，以最大化身份间区分度并最小化身份内变异性的影响。</li>
<li><strong>参考奖励反馈学习（Reference Reward Feedback Learning, ReReFL）：</strong> UMO 通过一种新颖的强化学习机制来操作多对多匹配范式，以提高现有图像定制方法的身份一致性。<ul>
<li><strong>单身份奖励（Single Identity Reward, SIR）：</strong> 基于身份嵌入之间的余弦距离，确保高保真度。</li>
<li><strong>多身份匹配奖励（Multi-Identity Matching Reward, MIMR）：</strong> 针对多身份场景，将匹配问题建模为二分图的全局分配问题，通过匈牙利算法高效计算最优分配，并基于此分配定义 MIMR，以同时提高多身份保真度并减轻混淆。</li>
</ul>
</li>
<li><strong>可扩展的多参考图像定制数据集：</strong> 为了有效训练 UMO，团队开发了一个包含合成和真实图像的可扩展数据集，其中包含每个身份的多个参考图像。</li>
<li><strong>新的身份混淆度量（ID-Conf）：</strong> 提出了一种新的度量标准 ID-Conf，用于精确评估多身份混淆的程度。它定义为给定参考身份，两个最相似的生成候选人脸之间的相对裕度。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>显著提升身份一致性并减少混淆：</strong> 广泛的实验（在 XVerseBench 和 OmniContext 等基准上）表明，UMO 在各种图像定制方法上显著提高了身份相似性（ID-Sim）并减轻了身份混淆（ID-Conf）。
*   <strong>达到最先进水平（SOTA）：</strong> UMO 在身份保持维度上，在开源方法中取得了新的最先进结果，展示了其强大的通用性和高保真度身份生成能力。
*   <strong>可扩展性：</strong> UMO 框架在单身份到多身份场景中均表现出良好的泛化能力，有效解决了随着身份数量增加而导致的身份保真度下降和混淆问题。
*   <strong>消融研究：</strong> 消融实验证明了 ReReFL 和 MIMR 的有效性，特别是 MIMR 在多身份场景中通过正确的面部监督分配，显著提升了身份一致性并减轻了混淆。
*   <strong>用户研究：</strong> 用户研究问卷结果显示，UMO 在身份一致性、提示遵循、美学和整体性能等多个维度上获得了最佳偏好。</p>
<p><strong>4. 论文中提及的局限性：</strong>
尽管 UMO 旨在保持高保真度身份保留并减轻多身份混淆，但作者指出，<strong>稳定地扩展到更多身份仍然受到预训练模型参考能力急剧下降的限制</strong>，当参考图像或身份数量增加时，这种限制变得更加明显。这与 [40] 中提出的观点相似。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
论文明确指出了当前模型在处理“更多身份”时的局限性，这为未来的研究提供了明确的方向：
*   <strong>进一步提升模型在超大规模多身份场景下的可扩展性：</strong> 如何在参考图像或身份数量大幅增加时，保持甚至提升预训练模型的参考能力，是未来需要攻克的关键难题。这可能涉及更高效的身份编码机制、更鲁棒的匹配算法，或者全新的模型架构设计。
*   <strong>探索更复杂的身份交互和关系：</strong> 目前的匹配奖励主要关注个体身份的匹配，未来可以探索如何更好地建模和利用多身份之间的复杂交互和关系，以生成更自然、更符合语境的多人图像。
*   <strong>结合更先进的扩散模型架构：</strong> 随着扩散模型技术的不断发展，将 UMO 框架与最新的、更强大的扩散模型架构相结合，可能会进一步提升性能和生成质量。</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this, we present UMO, a Unified Multi-identity Optimization
framework, designed to maintain high-fidelity identity preservation and
alleviate identity confusion with scalability.</li>
<li>To facilitate the training of UMO, we develop a scalable
customization dataset with multi-reference images, consisting of both
synthesised and real parts.</li>
<li>Additionally, we propose a new metric to measure
identity confusion.</li>
<li>Extensive experiments demonstrate that UMO not only
improves identity consistency significantly, but also reduces identity
confusion on several image customization methods, setting a new
state-of-the-art among open-source methods along the dimension of identity
preserving.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.06818v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.06818v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.06781v1'></a></p>
<h2 id="urbantwin-high-fidelity-synthetic-replicas-of-roadside-lidar-datasets"><a href="https://arxiv.org/abs/2509.06781v1">UrbanTwin: High-Fidelity Synthetic Replicas of Roadside Lidar Datasets</a></h2>
<p><strong>Authors:</strong> Muhammad Shahbaz, Shaurya Agarwal</p>
<p><strong>Published:</strong> 2025-09-08</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>This article presents UrbanTwin datasets - high-fidelity, realistic replicas
of three public roadside lidar datasets: LUMPI, V2X-Real-IC, and TUMTraf-I.
Each UrbanTwin dataset contains 10K annotated frames corresponding to one of
the public datasets. Annotations include 3D bounding boxes, instance
segmentation labels, and tracking IDs for six object classes, along with
semantic segmentation labels for nine classes. These datasets are synthesized
using emulated lidar sensors within realistic digital twins, modeled based on
surrounding geometry, road alignment at lane level, and the lane topology and
vehicle movement patterns at intersections of the actual locations
corresponding to each real dataset. Due to the precise digital twin modeling,
the synthetic datasets are well aligned with their real counterparts, offering
strong standalone and augmentative value for training deep learning models on
tasks such as 3D object detection, tracking, and semantic and instance
segmentation. We evaluate the alignment of the synthetic replicas through
statistical and structural similarity analysis with real data, and further
demonstrate their utility by training 3D object detection models solely on
synthetic data and testing them on real, unseen data. The high similarity
scores and improved detection performance, compared to the models trained on
real data, indicate that the UrbanTwin datasets effectively enhance existing
benchmark datasets by increasing sample size and scene diversity. In addition,
the digital twins can be adapted to test custom scenarios by modifying the
design and dynamics of the simulations. To our knowledge, these are the first
digitally synthesized datasets that can replace in-domain real-world datasets
for lidar perception tasks. UrbanTwin datasets are publicly available at
https://dataverse.harvard.edu/dataverse/ucf-ut.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Muhammad Shahbaz和Shaurya Agarwal撰写的论文“UrbanTwin: High-Fidelity Synthetic Replicas of Roadside Lidar Datasets”的全面摘要。</p>
<hr />
<h3 id="urbantwin">UrbanTwin: 高保真路边激光雷达数据集合成副本 论文摘要</h3>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决当前路边激光雷达数据集在智能交通系统（ITS）感知算法开发中的核心挑战：高质量、大规模、带标注的真实世界激光雷达数据集的稀缺性、高昂的获取成本和耗时性。现有的模拟器往往存在“模拟到现实”的领域鸿沟，导致在合成数据上训练的模型在真实世界中表现不佳。因此，研究问题是如何创建高保真、逼真且与真实世界数据高度对齐的合成激光雷达数据集，以有效支持3D目标检测、跟踪、语义和实例分割等感知任务，并弥补模拟与现实之间的差距。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
该论文提出了UrbanTwin数据集，其核心创新在于：
*   <strong>高保真数字孪生建模：</strong> 论文提出了一种新颖的数字孪生建模方法，将真实世界的静态元素（如精确的3D几何结构、车道级道路特征、建筑物、植被）和动态行为（如典型的交通机动模式、传感器规格）集成到模拟环境中。这种精确的建模确保了合成数据在空间和感官上与真实世界数据高度对齐。
*   <strong>复制现有基准数据集：</strong> UrbanTwin数据集并非通用合成数据，而是专门设计用于复制三个现有公共路边激光雷达数据集（LUMPI、V2X-Real-IC和TUMTraf-I）的核心特征，从而增强现有基准数据集的价值。
*   <strong>多任务支持：</strong> 合成数据集原生支持3D目标检测、跟踪、语义分割和实例分割这四项核心感知任务，提供了丰富的标注信息（3D边界框、实例分割标签、跟踪ID和语义分割标签）。
*   <strong>基于CARLA模拟器：</strong> 利用CARLA模拟器，在根据公开地理和结构数据构建的自定义地图上运行模拟，生成合成数据。传感器配置与真实传感器规格精确匹配。
*   <strong>随机但真实的动态内容：</strong> 交通流和道路使用者类型（汽车、卡车、自行车、摩托车、公共汽车）以随机但符合真实世界交通规则和物理交互的方式生成，增加了场景多样性。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>高结构和分布相似性：</strong> 通过对合成数据与真实数据进行广泛的统计和结构相似性分析（包括场景复杂度、点密度、目标大小和类别分布），结果表明UrbanTwin数据集与真实世界数据高度对齐，具有最小的领域鸿沟。
*   <strong>在感知模型中的实用性：</strong> 在3D目标检测任务中，使用UrbanTwin合成数据训练的模型在真实、未见数据上进行了测试。
    *   在LUMPI数据集上，仅使用合成数据训练的SEED模型性能优于在真实数据上训练的SEED模型，这表明合成数据不仅捕捉了真实数据集的结构复杂性，而且可能提供了更高的一致性标注质量。
    *   在V2X-Real数据集上，仅使用合成数据训练的SECOND模型在汽车类别上的平均精度（AP）达到了63.18%@3D IoU=0.5，超越了原始V2X-Real论文中报告的大多数基准模型，尽管没有使用任何真实数据进行训练。
*   <strong>显著意义：</strong> 这些结果表明UrbanTwin数据集能够有效增强现有基准数据集，通过增加样本量和场景多样性，为训练深度学习模型提供强大的独立和增强价值。它们为激光雷达感知任务提供了可替代真实世界数据的数字合成数据集，大大降低了数据集创建的成本和人力投入。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>行人建模限制：</strong> 当前版本的UrbanTwin数据集由于CARLA在准确建模人类行为方面的已知限制，暂时省略了行人相关类别。论文计划在未来更新中加入更真实的行人模型。
*   <strong>特定场景限制：</strong> 虽然数据集旨在复制特定交叉口场景，但其动态元素是随机生成的，不完全复制真实世界中特定车辆的精确轨迹。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>整合行人和VRU类别：</strong> 计划在未来版本中加入行人和易受伤害道路使用者（VRU）类别，以进一步完善数据集。
*   <strong>扩展验证任务：</strong> 将验证范围扩展到跟踪、分割和传感器融合任务，以全面评估合成数据的实用性。
*   <strong>创建更多合成副本：</strong> 作为UCF数字孪生计划的一部分，将为其他公共激光雷达数据集创建更多合成副本。
*   <strong>自定义场景测试：</strong> 数字孪生可以修改模拟的设计和动态，以测试自定义场景，例如罕见事件注入、天气变化和运动扰动，从而帮助研究人员开发能够泛化到边缘案例的模型。
*   <strong>领域适应和迁移学习：</strong> 准确建模的数字孪生为模拟到现实迁移学习、领域适应和跨任务学习框架等方法论创新开辟了新途径。</p>
<hr />
<p>总而言之，UrbanTwin论文通过引入高保真数字孪生建模方法，成功创建了与真实世界路边激光雷达数据集高度对齐的合成数据集。这些数据集不仅在结构和统计上与真实数据相似，而且在实际感知任务（如3D目标检测）中表现出色，甚至在某些情况下超越了在真实数据上训练的模型。这为解决真实世界数据集获取的挑战提供了一个可扩展、经济高效的解决方案，并为智能交通系统中的激光雷达感知研究和部署开辟了新的可能性。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To our knowledge, these are the first
digitally synthesized datasets that can replace in-domain real-world datasets
for lidar perception tasks.</li>
<li>UrbanTwin datasets are publicly available at
https://dataverse.harvard.edu/dataverse/ucf-ut.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.06781v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.06781v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.06579v1'></a></p>
<h2 id="causnvs-autoregressive-multi-view-diffusion-for-flexible-3d-novel-view-synthesis"><a href="https://arxiv.org/abs/2509.06579v1">CausNVS: Autoregressive Multi-view Diffusion for Flexible 3D Novel View Synthesis</a></h2>
<p><strong>Authors:</strong> Xin Kong, Daniel Watson, Yannick Strümpler, Michael Niemeyer, Federico Tombari</p>
<p><strong>Published:</strong> 2025-09-08</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Multi-view diffusion models have shown promise in 3D novel view synthesis,
but most existing methods adopt a non-autoregressive formulation. This limits
their applicability in world modeling, as they only support a fixed number of
views and suffer from slow inference due to denoising all frames
simultaneously. To address these limitations, we propose CausNVS, a multi-view
diffusion model in an autoregressive setting, which supports arbitrary
input-output view configurations and generates views sequentially. We train
CausNVS with causal masking and per-frame noise, using pairwise-relative camera
pose encodings (CaPE) for precise camera control. At inference time, we combine
a spatially-aware sliding-window with key-value caching and noise conditioning
augmentation to mitigate drift. Our experiments demonstrate that CausNVS
supports a broad range of camera trajectories, enables flexible autoregressive
novel view synthesis, and achieves consistently strong visual quality across
diverse settings. Project page: https://kxhit.github.io/CausNVS.html.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Xin Kong等人撰写的论文“CausNVS: Autoregressive Multi-view Diffusion for Flexible 3D Novel View Synthesis”的全面摘要。</p>
<hr />
<h3 id="causnvs-autoregressive-multi-view-diffusion-for-flexible-3d-novel-view-synthesis_1">CausNVS: Autoregressive Multi-view Diffusion for Flexible 3D Novel View Synthesis 论文摘要</h3>
<p><strong>1. 主要问题或研究问题：</strong>
现有的多视角扩散模型在3D新颖视角合成（NVS）中表现出色，但大多采用非自回归（non-autoregressive）形式。这意味着它们只能支持固定数量的视角输入，并且需要同时对所有帧进行去噪，导致推理速度慢，限制了其在世界建模（world modeling）等需要灵活、序列化视角生成的场景中的应用。核心挑战在于如何实现灵活的输入-输出视角配置、序列化生成，同时解决自回归模型常见的“漂移”（drift）问题和KV缓存（KV Caching）在绝对姿态编码下的不兼容性。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
论文提出了CausNVS，一个自回归多视角扩散模型，通过以下创新解决了上述问题：
*   <strong>自回归设置与因果掩码（Causal Masking）：</strong> CausNVS采用自回归框架，通过在帧级注意力层中引入因果掩码，确保每个视角仅基于先前观测到的输入和已合成的输出进行条件化生成。这使得模型能够支持任意输入-输出视角配置，并在单次训练中实现序列化生成。
*   <strong>逐帧噪声条件（Per-frame Noise Conditioning）：</strong> 在训练期间，模型对每帧应用独立的噪声水平，使其学习从不确定或不完美的上下文进行去噪。这有助于缩小训练与推理之间的差距，并增强模型对累积误差的鲁棒性。
*   <strong>成对相对相机姿态编码（CaPE）：</strong> 采用CaPE来编码视角之间的相对姿态关系，而非绝对姿态。CaPE确保注意力分数对全局坐标变化保持不变，从而使KV缓存能够在参考帧移动或滑动窗口推理时保持有效，无需重新计算。
*   <strong>推理时漂移缓解策略：</strong> 在推理时，CausNVS结合了空间感知滑动窗口（spatially-aware sliding-window）和KV缓存，以高效聚合上下文。同时，通过噪声条件增强（noise conditioning augmentation），为先前生成的视角分配较小的噪声水平，使其被视为带噪声的输入，进一步稳定后续预测并缓解漂移。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>强大的视觉质量和泛化能力：</strong> CausNVS在标准固定视角基准测试中实现了具有竞争力的视觉质量，并在灵活的NVS设置中超越了最先进的基线模型。
*   <strong>稳定的自回归生成：</strong> 即使在比训练序列长度长10倍的rollout中，模型也能保持稳定的自回归生成，展示了其强大的泛化能力。
*   <strong>高效的3D空间记忆：</strong> 通过结合CaPE、KV缓存和滑动窗口注意力，CausNVS实现了隐式且高效的3D空间记忆，确保在任意相机运动下（包括重复访问的视角）的生成具有空间一致性。
*   <strong>灵活的相机轨迹支持：</strong> 实验证明CausNVS支持广泛的相机轨迹，包括前进、后退、旋转和环形平移，并能保持3D一致性。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>多步去噪的实时性限制：</strong> 模型依赖于多步去噪过程，这限制了其在实时应用中的性能。
*   <strong>数据集规模和多样性：</strong> 尽管表现出色，但模型仍可能受益于更大规模、更多样化的数据集（例如包含物体和视频），以进一步提升泛化能力和先验知识。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>加速生成：</strong> 探索通过一致性训练（consistency training）或蒸馏（distillation）等方法实现更快的生成速度。
*   <strong>扩展到更长序列和多样化数据：</strong> 进一步扩展模型以处理更长的序列和更多样化的数据集，以增强其在复杂世界建模中的泛化能力。
*   <strong>多模态集成：</strong> 将音频、语言和动作信号集成到模型中，以构建具有空间基础和可控rollout的完全多模态世界模型。</p>
<hr />
<p>这篇论文为自回归多视角扩散模型在3D新颖视角合成领域开辟了新途径，通过创新的架构和训练策略，有效解决了现有方法的局限性，使其在灵活性、效率和生成质量方面均取得了显著进步，为未来世界建模和交互式应用奠定了基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Multi-view diffusion models have shown promise in 3D novel view synthesis,
but most existing methods adopt a non-autoregressive formulation.</li>
<li>To address these limitations, we propose CausNVS, a multi-view
diffusion model in an autoregressive setting, which supports arbitrary
input-output view configurations and generates views sequentially.</li>
<li>Our experiments demonstrate that CausNVS
supports a broad range of camera trajectories, enables flexible autoregressive
novel view synthesis, and achieves consistently strong visual quality across
diverse settings.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.06579v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.06579v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.06499v1'></a></p>
<h2 id="tide-achieving-balanced-subject-driven-image-generation-via-target-instructed-diffusion-enhancement"><a href="https://arxiv.org/abs/2509.06499v1">TIDE: Achieving Balanced Subject-Driven Image Generation via Target-Instructed Diffusion Enhancement</a></h2>
<p><strong>Authors:</strong> Jibai Lin, Bo Ma, Yating Yang, Rong Ma, Turghun Osman, Ahtamjan Ahmat, Rui Dong, Lei Wang, Xi Zhou</p>
<p><strong>Published:</strong> 2025-09-08</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Subject-driven image generation (SDIG) aims to manipulate specific subjects
within images while adhering to textual instructions, a task crucial for
advancing text-to-image diffusion models. SDIG requires reconciling the tension
between maintaining subject identity and complying with dynamic edit
instructions, a challenge inadequately addressed by existing methods. In this
paper, we introduce the Target-Instructed Diffusion Enhancing (TIDE) framework,
which resolves this tension through target supervision and preference learning
without test-time fine-tuning. TIDE pioneers target-supervised triplet
alignment, modelling subject adaptation dynamics using a (reference image,
instruction, target images) triplet. This approach leverages the Direct Subject
Diffusion (DSD) objective, training the model with paired "winning" (balanced
preservation-compliance) and "losing" (distorted) targets, systematically
generated and evaluated via quantitative metrics. This enables implicit reward
modelling for optimal preservation-compliance balance. Experimental results on
standard benchmarks demonstrate TIDE's superior performance in generating
subject-faithful outputs while maintaining instruction compliance,
outperforming baseline methods across multiple quantitative metrics. TIDE's
versatility is further evidenced by its successful application to diverse
tasks, including structural-conditioned generation, image-to-image generation,
and text-image interpolation. Our code is available at
https://github.com/KomJay520/TIDE.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Jibai Lin等人撰写的论文“TIDE: Achieving Balanced Subject-Driven Image Generation via Target-Instructed Diffusion Enhancement”的全面摘要。</p>
<hr />
<p><strong>论文摘要：TIDE: 通过目标指导扩散增强实现平衡的主体驱动图像生成</strong></p>
<p><strong>1. 主要问题或研究问题</strong>
主体驱动图像生成（Subject-Driven Image Generation, SDIG）旨在根据文本指令对图像中的特定主体进行操作，同时保持主体身份。现有方法在保持主体身份和遵循动态编辑指令之间存在固有的矛盾，未能充分解决这一挑战。具体来说，基于微调的方法牺牲了泛化能力，而免微调方法则缺乏对编辑解耦的明确监督，导致主体保留和指令遵循之间的平衡不足。</p>
<p><strong>2. 关键创新或方法论贡献</strong>
本文提出了<strong>目标指导扩散增强（Target-Instructed Diffusion Enhancing, TIDE）</strong>框架，通过以下关键创新解决了上述问题：</p>
<ul>
<li><strong>目标监督和偏好学习（Target Supervision and Preference Learning）：</strong> TIDE引入了<strong>目标监督三元组对齐</strong>，使用（参考图像、指令、目标图像）三元组来建模主体适应动态。这与以往仅使用参考图像作为伪目标的方法不同，目标图像明确编码了所需的属性编辑。</li>
<li><strong>直接主体扩散（Direct Subject Diffusion, DSD）目标：</strong> TIDE提出了专门针对SDIG的DSD目标，它将成对偏好学习（源自DPO）应用于图像生成。模型通过成对的“获胜”（平衡了保留和遵循）和“失败”（扭曲）目标进行训练，这些目标通过定量指标系统生成和评估。这使得模型能够隐式地学习奖励模型，以实现最佳的保留-遵循平衡。</li>
<li><strong>轻量级多模态适配器（Lightweight Multimodal Adapter）：</strong> TIDE通过一个包含图像投影模块（IPM）和图像交叉注意力模块（ICAM）的轻量级适配器来融合视觉和文本特征。该适配器仅更新基础扩散模型参数的极小部分（1.33%），从而在保持模型原始生成能力的同时，实现对主体操作的精确控制。</li>
</ul>
<p><strong>3. 主要结果及其意义</strong>
实验结果表明，TIDE在标准基准测试上表现出色，在生成忠实于主体的输出并保持指令遵循方面优于所有基线方法，并在多个定量指标上取得了卓越性能。</p>
<ul>
<li><strong>定量优势：</strong> 在Concept101和DreamBench基准测试中，TIDE在CLIP-I（图像质量）和CLIP-T（文本对齐）指标上均取得了领先，并在DINO分数上具有竞争力，显示出其在主体保留和指令遵循方面的卓越平衡。</li>
<li><strong>泛化能力：</strong> TIDE能够处理未见过的场景、稀有对象类别和新颖的姿态描述，其DINO分数比基线模型高出6.8%，突显了其强大的泛化能力。</li>
<li><strong>多任务通用性：</strong> TIDE的通用性通过其在多种任务中的成功应用得到进一步证明，包括：<ul>
<li><strong>结构条件生成：</strong> TIDE兼容ControlNet等现有控制机制，能够处理深度图、法线图、人体姿态骨架、Canny边缘和自由形式涂鸦等多种控制模态，无需额外微调。</li>
<li><strong>图像到图像生成：</strong> TIDE能够无缝地将艺术属性（如笔触、调色板）从风格参考图像转移到内容图像，同时保持原始主体的结构完整性。</li>
<li><strong>文本-图像插值：</strong> 通过可调参数γ，TIDE实现了主体、材质和风格属性之间的平滑插值，同时保持背景一致性和主体保真度。</li>
</ul>
</li>
</ul>
<p><strong>4. 论文中提及的局限性</strong>
尽管TIDE实现了对任意开放域主体的零样本生成，但仍存在一些局限性：</p>
<ul>
<li><strong>文本理解限制：</strong> 框架继承了CLIP的文本理解限制，未能解决超出典型token阈值的片段或复杂段落长度指令。</li>
<li><strong>少数民族语言支持：</strong> 由于多语言训练数据不足，处理少数民族语言可能会导致主体-指令错位，并在非主导语言场景中产生幻觉输出。</li>
</ul>
<p><strong>5. 潜在的未来研究方向</strong>
未来工作计划整合额外的技术来克服上述局限性，同时保持当前范式的效率优势，例如解决复杂指令理解和多语言支持问题。</p>
<hr />
<p>总而言之，TIDE通过引入目标监督三元组对齐和DSD目标，为SDIG领域带来了范式转变，有效地解决了主体保留和指令遵循之间的核心矛盾。其轻量级适配器设计和卓越的泛化能力使其成为个性化广告、教育内容生成等创意产业的强大工具，为指令感知生成系统开辟了新途径。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this
paper, we introduce the Target-Instructed Diffusion Enhancing (TIDE) framework,
which resolves this tension through target supervision and preference learning
without test-time fine-tuning.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.06499v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.06499v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.06485v1'></a></p>
<h2 id="ws2-weakly-supervised-segmentation-using-before-after-supervision-in-waste-sorting"><a href="https://arxiv.org/abs/2509.06485v1">WS<script type="math/tex">^2</script>: Weakly Supervised Segmentation using Before-After Supervision in Waste Sorting</a></h2>
<p><strong>Authors:</strong> Andrea Marelli, Alberto Foresti, Leonardo Pesce, Giacomo Boracchi, Mario Grosso</p>
<p><strong>Published:</strong> 2025-09-08</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>In industrial quality control, to visually recognize unwanted items within a
moving heterogeneous stream, human operators are often still indispensable.
Waste-sorting stands as a significant example, where operators on multiple
conveyor belts manually remove unwanted objects to select specific materials.
To automate this recognition problem, computer vision systems offer great
potential in accurately identifying and segmenting unwanted items in such
settings. Unfortunately, considering the multitude and the variety of sorting
tasks, fully supervised approaches are not a viable option to address this
challange, as they require extensive labeling efforts. Surprisingly, weakly
supervised alternatives that leverage the implicit supervision naturally
provided by the operator in his removal action are relatively unexplored. In
this paper, we define the concept of Before-After Supervision, illustrating how
to train a segmentation network by leveraging only the visual differences
between images acquired \textit{before} and \textit{after} the operator. To
promote research in this direction, we introduce WS<script type="math/tex">^2</script> (Weakly Supervised
segmentation for Waste-Sorting), the first multiview dataset consisting of more
than 11 000 high-resolution video frames captured on top of a conveyor belt,
including "before" and "after" images. We also present a robust end-to-end
pipeline, used to benchmark several state-of-the-art weakly supervised
segmentation methods on WS<script type="math/tex">^2</script>.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Andrea Marelli等人撰写的论文“WS<script type="math/tex">^2</script>: Weakly Supervised Segmentation using Before-After Supervision in Waste Sorting”的全面摘要。</p>
<hr />
<h3 id="ws2-weakly-supervised-segmentation-using-before-after-supervision-in-waste-sorting_1">论文《WS<script type="math/tex">^2</script>: Weakly Supervised Segmentation using Before-After Supervision in Waste Sorting》摘要</h3>
<p><strong>1. 解决的主要问题或研究问题：</strong>
该论文旨在解决工业质量控制（特别是垃圾分类）中识别和分割移动异质物料流中“不需要的物品”的挑战。传统上，这项任务高度依赖人工操作员。尽管计算机视觉系统具有自动化潜力，但由于分类任务的多样性和复杂性，以及全监督方法所需的大量像素级标注工作，使得现有方法难以推广和应用。因此，论文的核心研究问题是：如何在不依赖昂贵的人工标注的情况下，利用操作员移除物品这一“隐式监督”来训练一个有效的语义分割网络。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
*   <strong>“Before-After”监督概念的提出：</strong> 论文引入了“Before-After”监督范式，利用操作员移除不需要的物品前后图像之间的视觉差异来训练分割网络。这种隐式监督避免了昂贵的像素级标注。
*   <strong>WS<script type="math/tex">^2</script>数据集的发布：</strong> 为了促进该领域的研究，论文发布了WS<script type="math/tex">^2</script>（Weakly Supervised segmentation for Waste-Sorting）数据集。这是首个多视角数据集，包含超过11,000帧高分辨率视频帧，捕获了传送带上“操作前”和“操作后”的图像，专门用于弱监督垃圾分类分割任务。该数据集在工业环境中收集，并包含视频序列，这在现有数据集中是独一无二的。
*   <strong>端到端弱监督分割流程：</strong> 论文设计了一个鲁棒的端到端流程，用于利用“Before-After”监督训练分割网络。该流程包括三个主要阶段：
    1.  <strong>辅助分类器训练：</strong> 训练一个辅助分类器<script type="math/tex">K_ø</script>来区分“操作前”和“操作后”图像。<script type="math/tex">K_ø</script>通过学习与不需要的物品相关的判别性特征来完成此任务，因为这些物品仅出现在“操作前”图像中。
    2.  <strong>伪掩码生成与精炼：</strong> 利用<script type="math/tex">K_ø</script>生成的显著性图（SMs）来识别“操作前”图像中的不需要的物品区域，生成粗略的二值掩码。为了提高掩码质量，引入了一个两步SAM（Segment Anything Model）增强模块，利用SAM2生成精细的实例掩码，并结合显著性区域进行精炼，以获得高保真度的伪掩码。
    3.  <strong>全监督分割模型训练：</strong> 将精炼后的伪掩码作为标注，训练一个全监督的SegFormer分割模型。
*   <strong>背景移除三分类训练策略（BR）：</strong> 为了解决辅助分类器可能依赖背景线索而非目标差异的问题，论文提出了一种新颖的三分类训练策略。通过计算前景/背景掩码，将训练集扩展为三类：背景掩码的“操作前”图像、背景掩码的“操作后”图像以及纯背景图像。这有助于分类器专注于对象级差异，提高显著性图的准确性。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>POF-CAM的卓越性能：</strong> 在WS<script type="math/tex">^2</script>数据集上对多种最先进的弱监督分割方法进行基准测试后，结果显示，利用时间一致性的POF-CAM方法显著优于其他方法。这强调了视频序列中时间信息在处理此类数据集时的关键作用。
*   <strong>SAM精炼的有效性：</strong> SAM增强模块对粗略掩码的精炼显著提高了所有方法的性能，表明视觉基础模型带来的细节水平对于弱监督分割任务至关重要。
*   <strong>背景偏差缓解的重要性：</strong> 提出的背景移除三分类训练策略有效缓解了背景偏差问题，使得辅助分类器能够更准确地定位不需要的物品，从而生成更准确的显著性图。
*   <strong>对工业应用的影响：</strong> 论文的贡献为开发基于深度学习的自动化手动分类和质量控制活动提供了可推广的弱监督分割解决方案，有望提高垃圾分类工厂的效率，降低受伤风险，并减轻操作员的工作压力。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>现有弱监督方法的局限性：</strong> 尽管SAM精炼有所帮助，但仅凭弱监督分割方法本身难以达到视觉基础模型所能提供的细节水平。
*   <strong>WeakTr的泛化能力：</strong> 尽管WeakTr在CAM生成方面是先进的，但在WS<script type="math/tex">^2</script>数据集上的表现相对较差，这表明其对特定任务的泛化能力有限，尤其是在与基于卷积的方法相比时。
*   <strong>“After”图像的性能下降：</strong> 由于弱监督方法识别图像中与辅助分类器最相关的区域，而不需要的物品是“before”类的关键区域，因此在“after”图像上的分割掩码性能较差。这种性能差距和由此产生的类别不平衡可能导致SegFormer网络学习不一致，泛化能力受损。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>进一步探索时间一致性：</strong> 鉴于POF-CAM的成功，未来研究可以更深入地探索视频序列中时间相关性在弱监督分割中的应用。
*   <strong>改进弱监督方法以处理类别不平衡：</strong> 解决“before”和“after”图像之间性能差距和类别不平衡问题，以提高分割模型的泛化能力。
*   <strong>结合更多领域知识：</strong> 探索如何将更多关于垃圾分类任务的领域知识融入弱监督框架，以进一步提高性能。
*   <strong>扩展到其他工业场景：</strong> 将“Before-After”监督范式和提出的流程应用于其他需要利用操作员隐式监督的工业质量控制和活动理解任务。</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>To
promote research in this direction, we introduce WS<script type="math/tex">^2</script> (Weakly Supervised
segmentation for Waste-Sorting), the first multiview dataset consisting of more
than 11 000 high-resolution video frames captured on top of a conveyor belt,
including "before" and "after" images.</li>
<li>We also present a robust end-to-end
pipeline, used to benchmark several state-of-the-art weakly supervised
segmentation methods on WS<script type="math/tex">^2</script>.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.06485v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.06485v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-09-09 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
