<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-09-09 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DEâ«¶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Birdâs-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">è®ºæéè¯»</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-Î±
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-09-08/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../../depth_completion/DNet/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-09-09">Arxiv Computer Vision Papers - 2025-09-09</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#arxiv-2025-09-08" class="nav-link">Arxiv è®¡ç®æºè§è§æ¯æ¥æ¥åæ§è¡æè¦ (2025-09-08)</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#h_2ot-hierarchical-hourglass-tokenizer-for-efficient-video-pose-transformers" class="nav-link">H_{2}OT: Hierarchical Hourglass Tokenizer for Efficient Video Pose Transformers</a>
                </li>
                <li class="nav-item">
                    <a href="#deep-reactive-policy-learning-reactive-manipulator-motion-planning-for-dynamic-environments" class="nav-link">Deep Reactive Policy: Learning Reactive Manipulator Motion Planning for Dynamic Environments</a>
                </li>
                <li class="nav-item">
                    <a href="#scaling-transformer-based-novel-view-synthesis-models-with-token-disentanglement-and-synthetic-data" class="nav-link">Scaling Transformer-Based Novel View Synthesis Models with Token Disentanglement and Synthetic Data</a>
                </li>
                <li class="nav-item">
                    <a href="#interleaving-reasoning-for-better-text-to-image-generation" class="nav-link">Interleaving Reasoning for Better Text-to-Image Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#fomo4wheat-toward-reliable-crop-vision-foundation-models-with-globally-curated-data" class="nav-link">FoMo4Wheat: Toward reliable crop vision foundation models with globally curated data</a>
                </li>
                <li class="nav-item">
                    <a href="#umo-scaling-multi-identity-consistency-for-image-customization-via-matching-reward" class="nav-link">UMO: Scaling Multi-Identity Consistency for Image Customization via Matching Reward</a>
                </li>
                <li class="nav-item">
                    <a href="#urbantwin-high-fidelity-synthetic-replicas-of-roadside-lidar-datasets" class="nav-link">UrbanTwin: High-Fidelity Synthetic Replicas of Roadside Lidar Datasets</a>
                </li>
                <li class="nav-item">
                    <a href="#causnvs-autoregressive-multi-view-diffusion-for-flexible-3d-novel-view-synthesis" class="nav-link">CausNVS: Autoregressive Multi-view Diffusion for Flexible 3D Novel View Synthesis</a>
                </li>
                <li class="nav-item">
                    <a href="#tide-achieving-balanced-subject-driven-image-generation-via-target-instructed-diffusion-enhancement" class="nav-link">TIDE: Achieving Balanced Subject-Driven Image Generation via Target-Instructed Diffusion Enhancement</a>
                </li>
                <li class="nav-item">
                    <a href="#ws2-weakly-supervised-segmentation-using-before-after-supervision-in-waste-sorting" class="nav-link">WS^2: Weakly Supervised Segmentation using Before-After Supervision in Waste Sorting</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-09-09">Arxiv Computer Vision Papers - 2025-09-09</h1>
<h2 id="executive-summary">Executive Summary</h2>
<h2 id="arxiv-2025-09-08">Arxiv è®¡ç®æºè§è§æ¯æ¥æ¥åæ§è¡æè¦ (2025-09-08)</h2>
<p><strong>æ¦è¿°ï¼</strong></p>
<p>ä»å¤©ç Arxiv è®¡ç®æºè§è§è®ºæä¸»è¦å´ç»ä»¥ä¸å ä¸ªæ ¸å¿ä¸»é¢ï¼<strong>é«æä¸å¯æ©å±ççææ¨¡å</strong>ï¼ç¹å«æ¯éå¯¹å¾åãè§é¢å3Dåå®¹ï¼ã<strong>æºå¨äººæä½ä¸å·èº«æºè½</strong>ã<strong>é¢åç¹å®åºç¨</strong>ï¼å¦åä¸ååºç©åç±»ï¼ï¼ä»¥å<strong>æ°æ®æçä¸åææ°æ®å©ç¨</strong>ãçæå¼AIæç»­å æ®ä¸»å¯¼å°ä½ï¼ç ç©¶äººåè´åäºæåå¶å¨å¤æä»»å¡ä¸­çæ§è½ãæçåå¯æ§æ§ã</p>
<p><strong>ä¸»è¦ä¸»é¢ä¸è¶å¿ï¼</strong></p>
<ol>
<li><strong>é«æä¸å¯æ©å±ççææ¨¡åï¼</strong> å¤ç¯è®ºæèç¦äºå¦ä½ä½¿Transformeråæ©æ£æ¨¡åå¨å¤çé«ç»´æ°æ®ï¼å¦è§é¢ã3Dåºæ¯ï¼æ¶æ´é«æãæ´å·å¯æ©å±æ§ãä¾å¦ï¼éè¿åå±ç»æãTokenè§£è¦æèªåå½æ¹æ³æ¥ä¼åæ§è½ã</li>
<li><strong>3D è§è§ä¸æ°è§è§åæ (NVS)ï¼</strong> 3Dåå®¹çæåæ°è§è§åææ¯ç­é¨é¢åï¼ç ç©¶äººåæ¢ç´¢äºå¦ä½å©ç¨Transformeråæ©æ£æ¨¡åçæé«è´¨éãä¸è´ç3Dåºæ¯ï¼å¹¶è§£å³æ°æ®ç¨çæ§é®é¢ã</li>
<li><strong>æºå¨äººæä½ä¸å·èº«æºè½ï¼</strong> æºå¨äººå­¦ä¹ åå¨æç¯å¢ä¸çè¿å¨è§åæ¯å¦ä¸ä¸ªéè¦æ¹åï¼æ¨å¨æé«æºå¨äººå¨å¤æãä¸ç¡®å®ç¯å¢ä¸­çå³ç­åæ§è¡è½åã</li>
<li><strong>é¢åç¹å®åºç¨ä¸åºç¡æ¨¡åï¼</strong> è®¡ç®æºè§è§ææ¯æ­£è¢«åºç¨äºæ´å·ä½çé¢åï¼å¦åä¸ï¼ä½ç©å¥åº·çæµï¼åå·¥ä¸ï¼åºç©åç±»ï¼ï¼å¹¶å°è¯æå»ºé¢åç¹å®çåºç¡æ¨¡åã</li>
<li><strong>æ°æ®æçä¸åææ°æ®ï¼</strong> é¢å¯¹çå®æ°æ®ééçææï¼åææ°æ®åå¼±çç£å­¦ä¹ è¢«å¹¿æ³ç¨äºå¼¥è¡¥æ°æ®ä¸è¶³ï¼æåæ¨¡åæ³åè½åã</li>
</ol>
<p><strong>ç¹å«éè¦æåæ°çè®ºæï¼</strong></p>
<ul>
<li><strong>"Scaling Transformer-Based Novel View Synthesis Models with Token Disentanglement and Synthetic Data" (Nithin Gopalakrishnan Nair et al.)ï¼</strong> è¿ç¯è®ºæéè¿Tokenè§£è¦åå¤§è§æ¨¡åææ°æ®ï¼æ¾èæåäºTransformerå¨æ°è§è§åæä»»å¡ä¸­çå¯æ©å±æ§åæ§è½ï¼ä¸ºæªæ¥3Dåå®¹çææä¾äºéè¦æè·¯ã</li>
<li><strong>"Interleaving Reasoning for Better Text-to-Image Generation" (Wenxuan Huang et al.)ï¼</strong> å¼å¥äº¤éæ¨çæºå¶ï¼æ¨å¨æé«ææ¬å°å¾åçææ¨¡åçè§£å¤ææä»¤åçæé»è¾ä¸è´å¾åçè½åï¼å¯¹æåçæè´¨éåå¯æ§æ§å·æéè¦æä¹ã</li>
<li><strong>"FoMo4Wheat: Toward reliable crop vision foundation models with globally curated data" (Bing Han et al.)ï¼</strong> è¿æ¯ä¸ä¸ªæå»ºé¢åç¹å®åºç¡æ¨¡åçä¼ç§æ¡ä¾ï¼éè¿å¨çç­å±æ°æ®ä¸ºåä¸è§è§ä»»å¡æä¾å¯é çåºç¡æ¨¡åï¼å±ç¤ºäºCVå¨åç´é¢åçå·¨å¤§æ½åã</li>
<li><strong>"CausNVS: Autoregressive Multi-view Diffusion for Flexible 3D Novel View Synthesis" (Xin Kong et al.)ï¼</strong> å°èªåå½å¤è§å¾æ©æ£æ¨¡åå¼å¥3Dæ°è§è§åæï¼æä¾äºæ´çµæ´»ãé«è´¨éç3Dåå®¹çææ¹æ³ï¼æ¯æ©æ£æ¨¡åå¨3Dé¢ååºç¨çåä¸è¿å±ã</li>
</ul>
<p><strong>æ°å´ç ç©¶æ¹åæææ¯ï¼</strong></p>
<ul>
<li><strong>åå±/è§£è¦Tokenåï¼</strong> å¨å¤çé«ç»´æ°æ®ï¼å¦è§é¢ã3Dï¼æ¶ï¼éè¿åå±æè§£è¦Tokenæ¥æé«Transformerçæçåå¯æ©å±æ§ã</li>
<li><strong>äº¤éæ¨ç (Interleaving Reasoning)ï¼</strong> å¨çææ¨¡åä¸­å¼å¥æ´å¤æçæ¨çæºå¶ï¼ä»¥æ´å¥½å°çè§£è¾å¥å¹¶çææ´å·é»è¾æ§çè¾åºã</li>
<li><strong>å¹éå¥å± (Matching Reward) ç¨äºå¤èº«ä»½ä¸è´æ§ï¼</strong> å¨å¾åå®å¶åçæä¸­ï¼éè¿å¹éå¥å±æ¥ç¡®ä¿çæåå®¹å¨å¤èº«ä»½åºæ¯ä¸çä¸è´æ§ã</li>
<li><strong>èªåå½å¤è§å¾æ©æ£æ¨¡åï¼</strong> å°æ©æ£æ¨¡åä¸èªåå½èå¼ç»åï¼ç¨äºçæå¤è§å¾æ°æ®ï¼ç¹å«æ¯å¨3Dæ°è§è§åæä¸­å±ç°åºæ½åã</li>
<li><strong>é¢åç¹å®åºç¡æ¨¡åï¼</strong> éå¯¹ç¹å®åºç¨é¢åï¼å¦åä¸ãå·¥ä¸ï¼æå»ºå¤§è§æ¨¡é¢è®­ç»çåºç¡æ¨¡åï¼ä»¥æé«æ³åè½ååæ°æ®æçã</li>
</ul>
<p><strong>å»ºè®®éè¯»çè®ºæï¼</strong></p>
<p>å¯¹äºå¸ææ·±å¥äºè§£ææ°è¿å±çç ç©¶äººåï¼å»ºè®®ä¼åéè¯»ä»¥ä¸è®ºæï¼</p>
<ol>
<li><strong>"Scaling Transformer-Based Novel View Synthesis Models with Token Disentanglement and Synthetic Data" (Nithin Gopalakrishnan Nair et al.)ï¼</strong> äºè§£3DçæåTransformerå¯æ©å±æ§çåæ²¿ã</li>
<li><strong>"Interleaving Reasoning for Better Text-to-Image Generation" (Wenxuan Huang et al.)ï¼</strong> å³æ³¨ææ¬å°å¾åçæä¸­æ¨çè½ååå¯æ§æ§çæåã</li>
<li><strong>"CausNVS: Autoregressive Multi-view Diffusion for Flexible 3D Novel View Synthesis" (Xin Kong et al.)ï¼</strong> æ¢ç´¢æ©æ£æ¨¡åå¨3Dçæä¸­çæ°åºç¨ã</li>
<li><strong>"FoMo4Wheat: Toward reliable crop vision foundation models with globally curated data" (Bing Han et al.)ï¼</strong> äºè§£é¢åç¹å®åºç¡æ¨¡ååCVå¨åä¸ä¸­çåºç¨ã</li>
<li><strong>"Deep Reactive Policy: Learning Reactive Manipulator Motion Planning for Dynamic Environments" (Jiahui Yang et al.)ï¼</strong> å¦æå¯¹æºå¨äººæä½åå·èº«æºè½æå´è¶£ï¼è¿ç¯è®ºææä¾äºå¨æç¯å¢ä¸è¿å¨è§åçæ°æ¹æ³ã</li>
</ol>
<p>è¿äºè®ºæä»£è¡¨äºå½åè®¡ç®æºè§è§é¢åå¨çææ¨¡åã3Dçè§£åå®éåºç¨æ¹é¢çéè¦è¿å±ï¼ä¸ºæªæ¥çç ç©¶æä¾äºå®è´µçè§è§£ã</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2509.06956v1">H<script type="math/tex">_{2}</script>OT: Hierarchical Hourglass Tokenizer for Efficient Video Pose Transformers</a></li>
<li><a href="#2509.06953v1">Deep Reactive Policy: Learning Reactive Manipulator Motion Planning for Dynamic Environments</a></li>
<li><a href="#2509.06950v1">Scaling Transformer-Based Novel View Synthesis Models with Token Disentanglement and Synthetic Data</a></li>
<li><a href="#2509.06945v1">Interleaving Reasoning for Better Text-to-Image Generation</a></li>
<li><a href="#2509.06907v1">FoMo4Wheat: Toward reliable crop vision foundation models with globally curated data</a></li>
<li><a href="#2509.06818v1">UMO: Scaling Multi-Identity Consistency for Image Customization via Matching Reward</a></li>
<li><a href="#2509.06781v1">UrbanTwin: High-Fidelity Synthetic Replicas of Roadside Lidar Datasets</a></li>
<li><a href="#2509.06579v1">CausNVS: Autoregressive Multi-view Diffusion for Flexible 3D Novel View Synthesis</a></li>
<li><a href="#2509.06499v1">TIDE: Achieving Balanced Subject-Driven Image Generation via Target-Instructed Diffusion Enhancement</a></li>
<li><a href="#2509.06485v1">WS<script type="math/tex">^2</script>: Weakly Supervised Segmentation using Before-After Supervision in Waste Sorting</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2509.06956v1'></a></p>
<h2 id="h_2ot-hierarchical-hourglass-tokenizer-for-efficient-video-pose-transformers"><a href="https://arxiv.org/abs/2509.06956v1">H<script type="math/tex">_{2}</script>OT: Hierarchical Hourglass Tokenizer for Efficient Video Pose Transformers</a></h2>
<p><strong>Authors:</strong> Wenhao Li, Mengyuan Liu, Hong Liu, Pichao Wang, Shijian Lu, Nicu Sebe</p>
<p><strong>Published:</strong> 2025-09-08</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Transformers have been successfully applied in the field of video-based 3D
human pose estimation. However, the high computational costs of these video
pose transformers (VPTs) make them impractical on resource-constrained devices.
In this paper, we present a hierarchical plug-and-play pruning-and-recovering
framework, called Hierarchical Hourglass Tokenizer (H<script type="math/tex">_{2}</script>OT), for efficient
transformer-based 3D human pose estimation from videos. H<script type="math/tex">_{2}</script>OT begins with
progressively pruning pose tokens of redundant frames and ends with recovering
full-length sequences, resulting in a few pose tokens in the intermediate
transformer blocks and thus improving the model efficiency. It works with two
key modules, namely, a Token Pruning Module (TPM) and a Token Recovering Module
(TRM). TPM dynamically selects a few representative tokens to eliminate the
redundancy of video frames, while TRM restores the detailed spatio-temporal
information based on the selected tokens, thereby expanding the network output
to the original full-length temporal resolution for fast inference. Our method
is general-purpose: it can be easily incorporated into common VPT models on
both seq2seq and seq2frame pipelines while effectively accommodating different
token pruning and recovery strategies. In addition, our H<script type="math/tex">_{2}</script>OT reveals that
maintaining the full pose sequence is unnecessary, and a few pose tokens of
representative frames can achieve both high efficiency and estimation accuracy.
Extensive experiments on multiple benchmark datasets demonstrate both the
effectiveness and efficiency of the proposed method. Code and models are
available at https://github.com/NationalGAILab/HoT.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼è¿æ¯ä¸ç¯å³äºâH<script type="math/tex">_{2}</script>OT: Hierarchical Hourglass Tokenizer for Efficient Video Pose Transformersâçå¨é¢æè¦ï¼ç±Wenhao Li, Mengyuan Liu, Hong Liu, Pichao Wang, Shijian Lu, Nicu Sebeæ°åã</p>
<hr />
<h3 id="h_2ot-transformertoken">H<script type="math/tex">_{2}</script>OT: ç¨äºé«æè§é¢å§¿æTransformerçå±æ¬¡åæ²æ¼åTokenåè¯å¨</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è§é¢å§¿æTransformer (VPTs) å¨è§é¢3Däººä½å§¿æä¼°è®¡é¢ååå¾äºæ¾èæåï¼ä½å¶é«æçè®¡ç®ææ¬ï¼ç¹å«æ¯èªæ³¨æåæºå¶çäºæ¬¡å¤æåº¦ï¼ä½¿å¶å¨èµæºåéè®¾å¤ä¸é¨ç½²ä¸åå®éãç°ææ¹æ³è¦ä¹å¯¼è´æ¶é´æåéè¿å°ï¼è¦ä¹å¼å¥åä½è®¡ç®ãå æ­¤ï¼æ ¸å¿é®é¢æ¯å¦ä½å¨ä¿æé«ç²¾åº¦å§¿æä¼°è®¡çåæ¶ï¼æ¾èæé«VPTsçè®¡ç®æçï¼å°¤å¶æ¯å¨å¤çé¿è§é¢åºåæ¶ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
æ¬ææåºäºä¸ä¸ªåä¸º<strong>å±æ¬¡åæ²æ¼åTokenåè¯å¨ (Hierarchical Hourglass Tokenizer, H<script type="math/tex">_{2}</script>OT)</strong> çå³æå³ç¨åªæä¸æ¢å¤æ¡æ¶ï¼æ¨å¨è§£å³VPTsçæçé®é¢ãå¶ä¸»è¦åæ°ç¹åæ¬ï¼</p>
<ul>
<li><strong>å±æ¬¡ååªæè®¾è®¡ï¼Pyramidal Feature Hierarchyï¼ï¼</strong> H<script type="math/tex">_{2}</script>OTéç¨æ¸è¿å¼åªæç­ç¥ï¼éçTransformerå±çº§çæ·±å¥ï¼éæ­¥åå°åä½å¸§çå§¿æTokenæ°éï¼å½¢æä¸ä¸ªâå¥æ¯ç¶âï¼éå­å¡å½¢ï¼çç¹å¾å±æ¬¡ç»æãè¿æ¯ä¸æ¬¡æ§åªææ´ææå°ä¿çäºæç¨ä¿¡æ¯ï¼å¹¶è¿ä¸æ­¥æé«äºæçã</li>
<li><strong>Tokenåªææ¨¡å (Token Pruning Module, TPM)ï¼</strong> TPMè´è´£å¨æéæ©å°éå·æä»£è¡¨æ§çTokenï¼ä»¥æ¶é¤è§é¢å¸§ä¸­çåä½ãè®ºææ¢ç´¢äºåç§åªæç­ç¥ï¼<ul>
<li><strong>Token Pruning Cluster (TPC)ï¼</strong> åºäºkè¿é»å¯åº¦å³°å¼èç±»ç®æ³ï¼éæ©å¯åº¦é«ä¸ä¸å¶ä»é«å¯åº¦Tokenè·ç¦»è¿çTokenä½ä¸ºä»£è¡¨ã</li>
<li><strong>Token Pruning Attention (TPA)ï¼</strong> å©ç¨Transformerçèªæ³¨æååæ°æ¥éæ©ä¿¡æ¯éå¤§çTokenã</li>
<li><strong>Token Pruning Motion (TPMo)ï¼</strong> åºäºäººä½è¿å¨ååæ¥éæ©ä»£è¡¨æ§å¸§ï¼ä¿çè¿å¨ååæ¾èçå¸§ã</li>
<li><strong>Token Pruning Sampler (TPS)ï¼</strong> éç¨çº¿æ§éæ ·ç­ç¥ï¼ååå°ä»æ¶é´ç»´åº¦ä¸éæ ·TokenãTPSè¢«è¯ææ¯åæ°æ å³ãé«æä¸éåæå¼æ¢å¤çç­ç¥ã</li>
</ul>
</li>
<li><strong>Tokenæ¢å¤æ¨¡å (Token Recovering Module, TRM)ï¼</strong> TRMæ¨å¨å°åªææä½å¯¼è´çä½æ¶é´åè¾¨çæ¢å¤å°åå§å¨é¿åºåï¼ä»¥å®ç°å¿«éæ¨çãè®ºææåºäºä¸¤ç§æ¢å¤ç­ç¥ï¼<ul>
<li><strong>Token Recovering Attention (TRA)ï¼</strong> ä½¿ç¨è½»éçº§å¤å¤´äº¤åæ³¨æåå±ï¼å°å¯å­¦ä¹ çTokenä½ä¸ºæ¥è¯¢ï¼å°åªæåçä»£è¡¨æ§Tokenä½ä¸ºé®åå¼ï¼æ¢å¤å¨é¿Tokenã</li>
<li><strong>Token Recovering Interpolation (TRI)ï¼</strong> å¨åªæåçTokenç»è¿åå½å¤´ä¼°è®¡3Då§¿æåºååï¼ä½¿ç¨ç®åççº¿æ§æå¼æä½æ¢å¤å¨é¿3Då§¿æãTRIä¸TPSç»åä½¿ç¨æ¶ï¼è¢«è¯ææ¯åæ°æ å³ä¸é«æçã</li>
</ul>
</li>
<li><strong>éç¨æ§åå¼å®¹æ§ï¼</strong> H<script type="math/tex">_{2}</script>OTæ¯ä¸ä¸ªéç¨æ¡æ¶ï¼å¯ä»¥è½»æ¾éæå°ç°æçVPTæ¨¡åä¸­ï¼å¹¶æ¯æseq2seqåseq2frameä¸¤ç§æ¨çç®¡éï¼åæ¶çµæ´»éåºä¸åçTokenåªæåæ¢å¤ç­ç¥ã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
éè¿å¨Human3.6MåMPI-INF-3DHPç­å¤ä¸ªåºåæ°æ®éä¸è¿è¡å¤§éå®éªï¼H<script type="math/tex">_{2}</script>OTå±ç¤ºäºå¶æææ§åæçï¼</p>
<ul>
<li><strong>æ¾èçæçæåï¼</strong> H<script type="math/tex">_{2}</script>OTè½å¤å¤§å¹éä½è®¡ç®ææ¬ãä¾å¦ï¼å¨MixSTEæ¨¡åä¸ï¼H<script type="math/tex">_{2}</script>OTå°FLOPsåå°äº57.4%ï¼FPSæåäº87.8%ï¼åæ¶GPUåå­æ¶èåè®­ç»æ¶é´ä¹æ¾èåå°ãä¸ä½èä¹åçä¼è®®çæ¬HoT [21]ç¸æ¯ï¼H<script type="math/tex">_{2}</script>OTå¨FLOPsãGPUåå­åè®­ç»æ¶é´ä¸è¿ä¸æ­¥åå°ï¼FPSæ´é«ï¼æ§è½ä¹æ´å¥½ã</li>
<li><strong>ä¿æçè³æåç²¾åº¦ï¼</strong> å°½ç®¡å¤§å¹åå°äºTokenæ°éï¼H<script type="math/tex">_{2}</script>OTå¨Human3.6Mæ°æ®éä¸ä»è½ä¿æçè³ç¥å¾®æåå§¿æä¼°è®¡ç²¾åº¦ï¼ä¾å¦ï¼MixSTEä¸MPJPEæå0.5mmï¼ãè¿è¡¨æç»´æå®æ´çå§¿æåºåæ¯ä¸å¿è¦çï¼å°éä»£è¡¨æ§å¸§çå§¿æTokenè¶³ä»¥å®ç°é«æçåé«ç²¾åº¦ã</li>
<li><strong>å¯¹ä¸åVPTæ¨¡åçæ®éæ§ï¼</strong> H<script type="math/tex">_{2}</script>OTæååºç¨äºMHFormerãMixSTEãMotionBERTåMotionAGFormerç­SOTA VPTæ¨¡åï¼è¯æäºå¶å³æå³ç¨çéç¨æ§ã</li>
<li><strong>å¯¹æ¨çç®¡éçéåºæ§ï¼</strong> H<script type="math/tex">_{2}</script>OTå¨seq2seqåseq2frameä¸¤ç§æ¨çç®¡éä¸åè¡¨ç°åºè²ï¼å°¤å¶æ¯å¨seq2seqç®¡éä¸­ï¼æçæåæ´ä¸ºæ¾èã</li>
</ul>
<p><strong>4. è®ºæä¸­æå°çå±éæ§ï¼</strong>
*   <strong>ä½FPSåºæ¯ä¸çæ§è½ç¥æä¸éï¼</strong> å¨ä½å¸§çï¼ç¨çæ¶é´ä¿¡æ¯ï¼åºæ¯ä¸ï¼H<script type="math/tex">_{2}</script>OTå¨MixSTEä¸çæ§è½ç¥ä½äºåå§MixSTEãè¿å¯è½æ¯å ä¸ºå¨ç¨çæ°æ®ä¸­ï¼Tokenåªæè½å»é¤çåä½ä¿¡æ¯è¾å°ã
*   <strong>TPCåTPAçæ¨çè´æï¼</strong> TPCåTPAè½ç¶æ¯å¨æåªææ¹æ³ï¼ä½ç¸æ¯äºTPSï¼å®ä»¬å¨æ¨çæ¶ä¼å¼å¥é¢å¤çè®¡ç®è´æï¼å¯¼è´FPSè¾ä½ã
*   <strong>æææ§åºæ¯ä¸çå¤±è´¥æ¡ä¾ï¼</strong> å¨æäºå¤æåºæ¯ï¼å¦é¨åèº«ä½å¯è§ãç½è§å§¿ææ2Dæ£æµå¨å­å¨æ¾èè¯¯å·®ï¼ä¸ï¼H<script type="math/tex">_{2}</script>OTä»å¯è½æ æ³åç¡®ä¼°è®¡3Däººä½å§¿æã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>æ´æºè½çTokenéæ©ç­ç¥ï¼</strong> æ¢ç´¢æ´åè¿çå¨æTokenåªæç­ç¥ï¼å¨ä¿ææççåæ¶ï¼è¿ä¸æ­¥ä¼åå¨ç¨çæ¶é´ä¿¡æ¯æå¤æè¿å¨åºæ¯ä¸çæ§è½ã
*   <strong>èªéåºåªæä¸æ¢å¤ï¼</strong> ç ç©¶å¦ä½æ ¹æ®è§é¢åå®¹æè¿å¨æ¨¡å¼èªéåºå°è°æ´åªæçåæ¢å¤ç­ç¥ï¼ä»¥å®ç°æ´ä¼çæ§è½-æçæè¡¡ã
*   <strong>ä¸å¶ä»é«æTransformerææ¯çç»åï¼</strong> æ¢ç´¢H<script type="math/tex">_{2}</script>OTä¸å¶å®Transformeråç¼©æå éææ¯ï¼å¦éåãç¥è¯è¸é¦ç­ï¼çç»åï¼ä»¥è¿ä¸æ­¥æåæçã
*   <strong>æ©å±å°å¶ä»è§é¢çè§£ä»»å¡ï¼</strong> å°H<script type="math/tex">_{2}</script>OTççå¿µåæ¹æ³æ¨å¹¿å°å¶ä»éè¦å¤çé¿è§é¢åºåçè®¡ç®æºè§è§ä»»å¡ä¸­ï¼å¦å¨ä½è¯å«ãè¡ä¸ºåæç­ã</p>
<hr />
<p>æ»èè¨ä¹ï¼H<script type="math/tex">_{2}</script>OTéè¿å¼å¥å±æ¬¡ååªæåé«ææ¢å¤æºå¶ï¼æåå°è§£å³äºè§é¢å§¿æTransformerçè®¡ç®æçç¶é¢ï¼è¯æäºå¨3Däººä½å§¿æä¼°è®¡ä¸­ï¼æ éç»´æå®æ´çå§¿æåºåï¼å°éä»£è¡¨æ§å¸§çTokenå³å¯å®ç°é«æçåé«ç²¾åº¦ãè¿ä¸ºæªæ¥å¼åæ´å¼ºå¤§ãæ´å¿«éçVPTæ¨¡åæä¾äºéè¦çæ¹åã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this paper, we present a hierarchical plug-and-play pruning-and-recovering
framework, called Hierarchical Hourglass Tokenizer (H<script type="math/tex">_{2}</script>OT), for efficient
transformer-based 3D human pose estimation from videos.</li>
<li>Our method
is general-purpose: it can be easily incorporated into common VPT models on
both seq2seq and seq2frame pipelines while effectively accommodating different
token pruning and recovery strategies.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.06956v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.06956v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.06953v1'></a></p>
<h2 id="deep-reactive-policy-learning-reactive-manipulator-motion-planning-for-dynamic-environments"><a href="https://arxiv.org/abs/2509.06953v1">Deep Reactive Policy: Learning Reactive Manipulator Motion Planning for Dynamic Environments</a></h2>
<p><strong>Authors:</strong> Jiahui Yang, Jason Jingzhou Liu, Yulong Li, Youssef Khaky, Kenneth Shaw, Deepak Pathak</p>
<p><strong>Published:</strong> 2025-09-08</p>
<p><strong>Categories:</strong> cs.RO, cs.AI, cs.CV, cs.LG, cs.SY, eess.SY</p>
<p><strong>Abstract:</strong></p>
<p>Generating collision-free motion in dynamic, partially observable
environments is a fundamental challenge for robotic manipulators. Classical
motion planners can compute globally optimal trajectories but require full
environment knowledge and are typically too slow for dynamic scenes. Neural
motion policies offer a promising alternative by operating in closed-loop
directly on raw sensory inputs but often struggle to generalize in complex or
dynamic settings. We propose Deep Reactive Policy (DRP), a visuo-motor neural
motion policy designed for reactive motion generation in diverse dynamic
environments, operating directly on point cloud sensory input. At its core is
IMPACT, a transformer-based neural motion policy pretrained on 10 million
generated expert trajectories across diverse simulation scenarios. We further
improve IMPACT's static obstacle avoidance through iterative student-teacher
finetuning. We additionally enhance the policy's dynamic obstacle avoidance at
inference time using DCP-RMP, a locally reactive goal-proposal module. We
evaluate DRP on challenging tasks featuring cluttered scenes, dynamic moving
obstacles, and goal obstructions. DRP achieves strong generalization,
outperforming prior classical and neural methods in success rate across both
simulated and real-world settings. Video results and code available at
https://deep-reactive-policy.com</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Jiahui Yangç­äººæ°åçè®ºæâDeep Reactive Policy: Learning Reactive Manipulator Motion Planning for Dynamic Environmentsâçå¨é¢æè¦ã</p>
<hr />
<h3 id="deep-reactive-policy-learning-reactive-manipulator-motion-planning-for-dynamic-environments_1">è®ºææè¦ï¼Deep Reactive Policy: Learning Reactive Manipulator Motion Planning for Dynamic Environments</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³æºå¨äººæºæ¢°æå¨å¨æãé¨åå¯è§æµç¯å¢ä¸­çææ ç¢°æè¿å¨çæ ¹æ¬æ§ææãä¼ ç»çè¿å¨è§åå¨è½ç¶è½è®¡ç®å¨å±æä¼è½¨è¿¹ï¼ä½éè¦å®æ´çç¯å¢ç¥è¯ä¸å¨å¨æåºæ¯ä¸­éåº¦è¿æ¢ãèç°æçç¥ç»è¿å¨ç­ç¥è½ç¶è½ç´æ¥ä»åå§ä¼ æå¨è¾å¥è¿è¡é­ç¯æä½ï¼ä½å¨å¤ææå¨æç¯å¢ä¸­æ³åè½åä¸è¶³ãå æ­¤ï¼ç ç©¶é®é¢æ¯å¦ä½å¼åä¸ç§è½å¤å®ç°ååºå¼ãæ ç¢°æè¿å¨è§åçç­ç¥ï¼ä½¿å¶å¨å¤æ ·åå¨æç¯å¢ä¸­å·æå¼ºå¤§çæ³åè½ååå®æ¶æ§è½ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
è¯¥è®ºææåºäº<strong>æ·±åº¦ååºå¼ç­ç¥ï¼Deep Reactive Policy, DRPï¼</strong>ï¼è¿æ¯ä¸ç§åºäºç¹äºæå®è¾å¥çè§è§-è¿å¨ç¥ç»è¿å¨ç­ç¥ï¼ç¨äºå¨å¤æ ·åå¨æç¯å¢ä¸­çæååºå¼è¿å¨ãDRPçæ ¸å¿åæ°åæ¬ï¼</p>
<ul>
<li><strong>å¤§è§æ¨¡è¿å¨é¢è®­ç»çIMPACTï¼</strong> DRPçæ ¸å¿æ¯IMPACTï¼Imitating Motion Planning with Action-Chunking Transformerï¼ï¼ä¸ä¸ªåºäºTransformerçç¥ç»è¿å¨ç­ç¥ãå®éè¿è¡ä¸ºåéå¨åå«1000ä¸æ¡ç±æåè¿çGPUå éè¿å¨è§åå¨cuRoboçæçä¸å®¶è½¨è¿¹çå¤§è§æ¨¡ç¦»çº¿æ°æ®éä¸è¿è¡é¢è®­ç»ãè¿ä½¿å¾ç­ç¥è½å¤å­¦ä¹ å°å¨å±è§åè½åã</li>
<li><strong>è¿­ä»£å¼å¸çå¾®è°ï¼Student-Teacher Finetuningï¼ï¼</strong> ä¸ºäºæé«IMPACTçéæéç¢ç©é¿éè½åï¼è®ºæå¼å¥äºè¿­ä»£å¼å¸çå¾®è°æ¹æ³ãæå¸ç­ç¥ç»åäºé¢è®­ç»çIMPACTåGeometric Fabricsï¼ä¸ç§æé¿å±é¨é¿éçé­ç¯æ§å¶å¨ï¼ï¼å°Geometric Fabricsçå±é¨é¿éè¡ä¸ºè¸é¦å°IMPACTç­ç¥ä¸­ï¼ä½¿å¶è½å¤ç´æ¥ä»ç¹äºè¾å¥è¿è¡æä½ã</li>
<li><strong>å¨ææè¿ç¹RMPï¼DCP-RMPï¼æ¨¡åï¼</strong> ä¸ºäºå¨æ¨çæ¶è¿ä¸æ­¥å¢å¼ºç­ç¥çå¨æéç¢ç©é¿éæ§è½ï¼DRPæ´åäºä¸ä¸ªå±é¨ååºå¼ç®æ æè®®æ¨¡åââDCP-RMPãDCP-RMPæ¯ä¸ç§éå­¦ä¹ åç»ä»¶ï¼å®å©ç¨å±é¨éç¢ç©ä¿¡æ¯ï¼éè¿è¯å«å¨æéç¢ç©ä¸­çæè¿ç¹å¹¶çæææ¥è¿å¨ï¼å®æ¶è°æ´åå§å³èç©ºé´ç®æ ï¼ä»èä¼åå¤çå¨æéç¢ç©é¿éã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
DRPå¨DRPBenchåºåæµè¯ï¼åæ¬æä¹±åºæ¯ãå¨æç§»å¨éç¢ç©åç®æ é»å¡ç­æææ§ä»»å¡ï¼ä»¥åMÏNetsæ°æ®éä¸è¿è¡äºå¹¿æ³è¯ä¼°ï¼å¹¶å¨æ¨¡æåçå®ä¸çç¯å¢ä¸­ååå¾äºæ¾èææï¼</p>
<ul>
<li><strong>å¼ºå¤§çæ³åè½åï¼</strong> DRPå¨åç§å¨æåç®æ é»å¡ä»»å¡ä¸­è¡¨ç°åºè²ï¼æ¾èä¼äºååçç»å¸ååºäºå­¦ä¹ çæ¹æ³ãä¾å¦ï¼å¨æµ®å¨å¨æéç¢ç©ï¼FDOï¼åå¨æç®æ é»å¡ï¼DGBï¼ä»»å¡ä¸­ï¼DRPçæåçè¿é«äºä»ä½¿ç¨IMPACTæcuRoboã</li>
<li><strong>å®æ¶ååºæ§ï¼</strong> DRPçé­ç¯æ§è´¨åDCP-RMPæ¨¡åçéæä½¿å¶è½å¤å¯¹å¨æç¯å¢ååååºå¿«éååºï¼è§£å³äºä¼ ç»è§åå¨å¨å¨æåºæ¯ä¸­éåº¦æ¢çé®é¢ã</li>
<li><strong>è¶è¶é¢è®­ç»æ°æ®æºçæ§è½ï¼</strong> å°½ç®¡DRPä½¿ç¨cuRoboçæçè½¨è¿¹è¿è¡é¢è®­ç»ï¼ä½éè¿å¸çå¾®è°ï¼å¶æ§è½æ¾èè¶è¶äºåå§æ°æ®æºï¼è¡¨æè¯¥æ¹æ³ä¸ä»ç»§æ¿äºæç¨è¡ä¸ºï¼èä¸å¨å¤æç¯å¢ä¸­å®ç°äºæ´ææçæ³åã</li>
<li><strong>çå®ä¸çé¨ç½²è½åï¼</strong> å°½ç®¡å®å¨å¨æ¨¡æä¸­è®­ç»ï¼DRPå¨çå®ä¸çç¯å¢ä¸­è¡¨ç°åºè¯å¥½çéåºæ§ï¼å¨éæç¯å¢ãçªç¶åºç°çéç¢ç©åç®æ é»å¡ç­ä»»å¡ä¸­åå¾äºæ¥è¿å®ç¾çæåçã</li>
</ul>
<p>è¿äºç»æè¡¨æï¼DRPæåå°ç»åäºå¨å±è§åè½ååå±é¨ååºæ§ï¼ä¸ºæºå¨äººæºæ¢°æå¨å¤æå¨æç¯å¢ä¸­çè¿å¨è§åæä¾äºä¸ä¸ªé²æ£ãé«æä¸å¯æ³åçè§£å³æ¹æ¡ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>å¯¹ç¹äºè§æµçä¾èµï¼</strong> DRPçææè§åä¾èµäºç¸å¯¹åç¡®çç¹äºè§æµãå¨ä¸¥éçæç¥å¤±è´¥ï¼ä¾å¦å¨ç­çªç¯å¢ä¸­é¢ç¹é®æ¡ï¼ä¸ï¼æ§è½å¯è½ä¼ä¸éãå¤æåå¤´è®¾ç½®æå©äºç¼è§£æ­¤é®é¢ï¼ä½å¯è½ä¸è¶³ä»¥åºå¯¹æææåµã
*   <strong>åä¸æºå¨äººå¹³å°ï¼</strong> å®éªä»éäºåä¸ªæºå¨äººå¹³å°ï¼Franka Pandaï¼ï¼æªå¨å¶ä»æºå¨äººå¹³å°ä¸è¿è¡è¯ä¼°ãè¿éå¶äºå¶å¨å¤æºå¨äººå¹³å°ä¸çå¯æ©å±æ§ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>å¤æºå¨äººå¹³å°æ³åï¼</strong> æªæ¥çå·¥ä½ç®æ æ¯è§£å³å¤æºå¨äººå¹³å°çå¯æ©å±æ§é®é¢ï¼å¯ä»¥éè¿ä¸ºæ¯ä¸ªæºå¨äººçæåç¬çè§åå¨ï¼æèè®­ç»ä¸ä¸ªè½å¤æ³åå°å¤ä¸ªæºå¨äººå¹³å°çåä¸DRPç­ç¥æ¥å®ç°ã
*   <strong>æ´ä¸°å¯çä¼ æå¨è¾å¥ï¼</strong> å©ç¨RGBæRGB-Dè¾å¥å¯ä»¥è¿ä¸æ­¥æé«å¨éç»æåç¯å¢ä¸­çæ§è½ã</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose Deep Reactive Policy (DRP), a visuo-motor neural
motion policy designed for reactive motion generation in diverse dynamic
environments, operating directly on point cloud sensory input.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.06953v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.06953v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.06950v1'></a></p>
<h2 id="scaling-transformer-based-novel-view-synthesis-models-with-token-disentanglement-and-synthetic-data"><a href="https://arxiv.org/abs/2509.06950v1">Scaling Transformer-Based Novel View Synthesis Models with Token Disentanglement and Synthetic Data</a></h2>
<p><strong>Authors:</strong> Nithin Gopalakrishnan Nair, Srinivas Kaza, Xuan Luo, Vishal M. Patel, Stephen Lombardi, Jungyeon Park</p>
<p><strong>Published:</strong> 2025-09-08</p>
<p><strong>Categories:</strong> cs.GR, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Large transformer-based models have made significant progress in
generalizable novel view synthesis (NVS) from sparse input views, generating
novel viewpoints without the need for test-time optimization. However, these
models are constrained by the limited diversity of publicly available scene
datasets, making most real-world (in-the-wild) scenes out-of-distribution. To
overcome this, we incorporate synthetic training data generated from diffusion
models, which improves generalization across unseen domains. While synthetic
data offers scalability, we identify artifacts introduced during data
generation as a key bottleneck affecting reconstruction quality. To address
this, we propose a token disentanglement process within the transformer
architecture, enhancing feature separation and ensuring more effective
learning. This refinement not only improves reconstruction quality over
standard transformers but also enables scalable training with synthetic data.
As a result, our method outperforms existing models on both in-dataset and
cross-dataset evaluations, achieving state-of-the-art results across multiple
benchmarks while significantly reducing computational costs. Project page:
https://scaling3dnvs.github.io/</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Nithin Gopalakrishnan Nairç­äººæ°åçè®ºæâScaling Transformer-Based Novel View Synthesis Models with Token Disentanglement and Synthetic Dataâçå¨é¢æè¦ã</p>
<hr />
<h3 id="scaling-transformer-based-novel-view-synthesis-models-with-token-disentanglement-and-synthetic-data_1">è®ºææè¦ï¼Scaling Transformer-Based Novel View Synthesis Models with Token Disentanglement and Synthetic Data</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³éç¨åæ°è§è§åæï¼NVSï¼æ¨¡åå¨å¤çç¨çè¾å¥è§å¾æ¶é¢ä¸´çä¸¤ä¸ªæ ¸å¿ææï¼
*   <strong>æ°æ®å¤æ ·æ§éå¶ï¼</strong> ç°æåºäºTransformerçNVSæ¨¡åè½ç¶å¨çææ°è§è§æ¹é¢åå¾äºæ¾èè¿å±ï¼ä½åéäºå¬å¼åºæ¯æ°æ®éçæéå¤æ ·æ§ï¼å¯¼è´æ¨¡åå¨çå®ä¸çï¼in-the-wildï¼åºæ¯ä¸­æ³åè½åä¸è¶³ï¼å ä¸ºè¿äºåºæ¯å¾å¾æ¯âåå¸å¤âçã
*   <strong>åææ°æ®å¼å¥çä¼ªå½±ï¼</strong> å°½ç®¡åææ°æ®ä¸ºè§£å³æ°æ®ç¨ç¼ºé®é¢æä¾äºå¯æ©å±çéå¾ï¼ä½æ©æ£æ¨¡åçæçæ°æ®ä¸­å¸¸å¸¸åå«ä¼ªå½±ï¼è¿äºä¼ªå½±ä¼ä¸¥éå½±åéå»ºè´¨éï¼æä¸ºæ¨¡åæ§è½æåçå³é®ç¶é¢ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
ä¸ºäºåæä¸è¿°ææï¼ä½èæåºäºä»¥ä¸å³é®åæ°ï¼</p>
<ul>
<li><strong>Token Disentangled (Tok-D) Transformer Blockï¼</strong> éå¯¹åææ°æ®ä¸­å­å¨çä¼ªå½±é®é¢ï¼ä½èå¼å¥äºä¸ç§åä¸ºTok-DçTransformeråãè¯¥æ¨¡åéè¿å±çº§è°å¶ï¼layer-wise modulationï¼æ¾å¼åºåæºä»¤çï¼source tokensï¼åç®æ ä»¤çï¼target tokensï¼ï¼å¢å¼ºäºç¹å¾åç¦»ï¼ç¡®ä¿æ´ææçå­¦ä¹ ãè¿ç§è®¾è®¡ä½¿å¾æ¨¡åè½å¤æ´é²æ£å°å¤çåææ°æ®ä¸­çä¼ªå½±ï¼å¹¶æ´é«æå°åéè¡¨ç¤ºå®¹éã</li>
<li><strong>æ¹è¿çåææ°æ®çæä¸è®­ç»æ¹æ¡ï¼</strong> è®ºææåºäºä¸ç§æ°é¢çæ°æ®çæç­ç¥ï¼æ¾èæé«äºåææ ·æ¬çè´¨éãå·ä½èè¨ï¼ä»ä»¬å©ç¨CAT3Då¤è§å¾æ©æ£æ¨¡åçæå¤§è§æ¨¡åæå¤è§å¾æ ·æ¬ï¼å¹¶å°æ¡ä»¶å¾åä½ä¸ºç®æ è§å¾ï¼çæè§å¾ä½ä¸ºè¾å¥è§å¾ãè¿ç§æ¹æ³å¼ºå¶Transformerå§ç»çæé¼ççå¾åï¼ä»èä½¿æ¨¡åå¯¹åææ°æ®ä¸­çä¼ªå½±å·æé²æ£æ§ã</li>
<li><strong>å¢å¼ºTransformeræ¶æçå¯æ©å±æ§åæçï¼</strong> Tok-D Transformeråéè¿åå°åä½å¹¶æé«æ°æ®æçï¼ä½¿å¾Transformeræ¶æå¨NVSä»»å¡ä¸­æ´å·å¯æ©å±æ§åæçï¼ä»èå¨æ´ä½çè®¡ç®ææ¬ä¸å®ç°æ´é«çéå»ºè´¨éã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
è¯¥æ¹æ³å¨å¤ä¸ªåºåæµè¯ä¸­åå¾äºæåè¿ï¼state-of-the-artï¼çç»æï¼å¹¶å·æä»¥ä¸æ¾èæä¹ï¼</p>
<ul>
<li><strong>éå»ºè´¨éæåï¼</strong> ç¸è¾äºæ åTransformerï¼Tok-D Transformerä¸ä»æé«äºéå»ºè´¨éï¼èä¸è½å¤å©ç¨åææ°æ®è¿è¡å¯æ©å±è®­ç»ã</li>
<li><strong>æ³åè½åå¢å¼ºï¼</strong> éè¿ç»ååæè®­ç»æ°æ®ï¼æ¨¡åå¨æªè§è¿çé¢åï¼è·¨æ°æ®éï¼ä¸çæ³åè½åå¾å°æ¾èæ¹åãå®éªç»æè¡¨æï¼è¯¥æ¹æ³å¨æ°æ®éååè·¨æ°æ®éè¯ä¼°ä¸­åä¼äºç°ææ¨¡åã</li>
<li><strong>è®¡ç®ææ¬éä½ï¼</strong> è¯¥æ¹æ³æ¾èéä½äºè®¡ç®ææ¬ï¼åæ¶ä¿ææè¶è¶äºç°ææ¨¡åçæ§è½ãä¾å¦ï¼å¨Re10Kæ°æ®éä¸ï¼Tok-D-Plusæ¯LVSMï¼ä½¿ç¨8ä¸ªGPUè®­ç»ï¼çæ§è½æé«äº1.2 dBï¼çè³è¶è¶äºä½¿ç¨64ä¸ªGPUè®­ç»çLVSMã</li>
<li><strong>Tokenè§£è¦çæ¶ç°ç¹æ§ï¼</strong> è®ºæåç°ï¼Tok-D Transformeråå·æè§£è¦æºä»¤çåç®æ ä»¤ççæ¶ç°ç¹æ§ï¼è¿ä½¿å¾æ¨¡åè½å¤æ´å¥½å°å©ç¨åææ°æ®ï¼å¹¶ææä¸¢å¼åææ°æ®ä¸­çä¼ªå½±ï¼ä»èé¿åäºä¼ªå½±ä¼ æ­ã</li>
</ul>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
è®ºæä¹å¦è¯å°æåºäºå½åæ¹æ³çå±éæ§ï¼</p>
<ul>
<li><strong>å¤çé®æ¡åºåçææï¼</strong> å½æºå¾åä¸­è¢«é®æ¡çåºåå¨æ°è§è§ä¸­åå¾å¯è§æ¶ï¼æ¨¡åé¾ä»¥åç¡®éå»ºè¿äºåºåï¼ææ¶ä¼åºç°å¹»è§ãä½èè®¤ä¸ºè¿æ¯ä¸ä¸ªåºæçä¸éå®é®é¢ï¼ç¼ºä¹æç¡®çè§£å³æ¹æ¡ã</li>
<li><strong>åå­æ¶èï¼</strong> æ¨¡åä½¿ç¨è¾å¤§çä»¤çå°ºå¯¸ï¼8x8ï¼ï¼å¯¼è´æ¯ä¸ªæºå¾åæ1024ä¸ªä»¤çï¼è¿éè¦å¤§éçåå­ã</li>
</ul>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
åºäºä¸è¿°å±éæ§ï¼è®ºææåºäºä»¥ä¸æªæ¥ç ç©¶æ¹åï¼</p>
<ul>
<li><strong>æ¶æä¼åï¼</strong> æ¢ç´¢è¿ä¸æ­¥çæ¶æä¼åï¼ä¾å¦åå±Transformerï¼hierarchical transformersï¼åæ´é«æçç½ç»ï¼å¦çº¿æ§æ³¨æåæºå¶åç¶æç©ºé´æ¨¡åï¼ä¾å¦Mambaï¼ï¼ä»¥åå°åå­æ¶èå¹¶æé«æçã</li>
<li><strong>è§£å³é®æ¡é®é¢ï¼</strong> å¯»æ¾æ´ææçæ¹æ³æ¥å¤çæ°è§è§ä¸­åºç°çé®æ¡å¯¹è±¡ï¼è¿ä»ç¶æ¯ä¸ä¸ªå¼æ¾çç ç©¶é®é¢ã</li>
</ul>
<hr />
<p>æ»èè¨ä¹ï¼è¿ç¯è®ºæéè¿å¼å¥Token Disentangled Transformerååæ¹è¿çåææ°æ®è®­ç»ç­ç¥ï¼æåè§£å³äºéç¨åæ°è§è§åææ¨¡åå¨æ°æ®å¤æ ·æ§ååææ°æ®ä¼ªå½±æ¹é¢çææãå¶æåºçæ¹æ³ä¸ä»æ¾èæåäºéå»ºè´¨éåæ³åè½åï¼è¿å¨éä½è®¡ç®ææ¬æ¹é¢åå¾äºçªç ´ï¼ä¸ºæªæ¥åºäºTransformerçNVSæ¨¡ååå±å¥ å®äºåå®åºç¡ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Large transformer-based models have made significant progress in
generalizable novel view synthesis (NVS) from sparse input views, generating
novel viewpoints without the need for test-time optimization.</li>
<li>To address
this, we propose a token disentanglement process within the transformer
architecture, enhancing feature separation and ensuring more effective
learning.</li>
<li>As a result, our method outperforms existing models on both in-dataset and
cross-dataset evaluations, achieving state-of-the-art results across multiple
benchmarks while significantly reducing computational costs.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.06950v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.06950v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.06945v1'></a></p>
<h2 id="interleaving-reasoning-for-better-text-to-image-generation"><a href="https://arxiv.org/abs/2509.06945v1">Interleaving Reasoning for Better Text-to-Image Generation</a></h2>
<p><strong>Authors:</strong> Wenxuan Huang, Shuang Chen, Zheyong Xie, Shaosheng Cao, Shixiang Tang, Yufan Shen, Qingyu Yin, Wenbo Hu, Xiaoman Wang, Yuntian Tang, Junbo Qiao, Yue Guo, Yao Hu, Zhenfei Yin, Philip Torr, Yu Cheng, Wanli Ouyang, Shaohui Lin</p>
<p><strong>Published:</strong> 2025-09-08</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.CL, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Unified multimodal understanding and generation models recently have achieve
significant improvement in image generation capability, yet a large gap remains
in instruction following and detail preservation compared to systems that
tightly couple comprehension with generation such as GPT-4o. Motivated by
recent advances in interleaving reasoning, we explore whether such reasoning
can further improve Text-to-Image (T2I) generation. We introduce Interleaving
Reasoning Generation (IRG), a framework that alternates between text-based
thinking and image synthesis: the model first produces a text-based thinking to
guide an initial image, then reflects on the result to refine fine-grained
details, visual quality, and aesthetics while preserving semantics. To train
IRG effectively, we propose Interleaving Reasoning Generation Learning (IRGL),
which targets two sub-goals: (1) strengthening the initial think-and-generate
stage to establish core content and base quality, and (2) enabling high-quality
textual reflection and faithful implementation of those refinements in a
subsequent image. We curate IRGL-300K, a dataset organized into six decomposed
learning modes that jointly cover learning text-based thinking, and full
thinking-image trajectories. Starting from a unified foundation model that
natively emits interleaved text-image outputs, our two-stage training first
builds robust thinking and reflection, then efficiently tunes the IRG pipeline
in the full thinking-image trajectory data. Extensive experiments show SoTA
performance, yielding absolute gains of 5-10 points on GenEval, WISE, TIIF,
GenAI-Bench, and OneIG-EN, alongside substantial improvements in visual quality
and fine-grained fidelity. The code, model weights and datasets will be
released in: https://github.com/Osilly/Interleaving-Reasoning-Generation .</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾å¯¹Wenxuan Huangç­äººæ°åçè®ºæâInterleaving Reasoning for Better Text-to-Image Generationâçå¨é¢æè¦ã</p>
<hr />
<p><strong>è®ºææè¦ï¼Interleaving Reasoning for Better Text-to-Image Generation</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
å°½ç®¡ç»ä¸å¤æ¨¡æçè§£åçææ¨¡åå¨å¾åçæè½åæ¹é¢åå¾äºæ¾èè¿æ­¥ï¼ä½å¨æä»¤éµå¾ªåç»èä¿çæ¹é¢ï¼ä¸GPT-4oç­ç´§å¯ç»åçè§£ä¸çæçç³»ç»ç¸æ¯ï¼ä»å­å¨è¾å¤§å·®è·ãæ¬ææ¨å¨æ¢è®¨äº¤éæ¨çï¼interleaving reasoningï¼æ¯å¦è½è¿ä¸æ­¥æåææ¬å°å¾åï¼T2Iï¼çæè´¨éï¼ç¹å«æ¯å¨ç»ç²åº¦ç»èãè§è§è´¨éåç¾å­¦æ¹é¢ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
*   <strong>å¼å¥äº¤éæ¨ççæï¼Interleaving Reasoning Generation, IRGï¼æ¡æ¶ï¼</strong> IRGæ¯ä¸ä¸ªå¨ææ¬æèåå¾ååæä¹é´äº¤æ¿è¿è¡çæ¡æ¶ãæ¨¡åé¦åçæææ¬æèè¿ç¨æ¥æå¯¼åå§å¾åççæï¼ç¶ååºäºåå§å¾åè¿è¡åæï¼ä»¥æ¹è¿ç»ç²åº¦ç»èãè§è§è´¨éåç¾å­¦ï¼åæ¶ä¿çè¯­ä¹ã
*   <strong>æåºäº¤éæ¨ççæå­¦ä¹ ï¼Interleaving Reasoning Generation Learning, IRGLï¼ï¼</strong> ä¸ºäºææè®­ç»IRGï¼IRGLè®¾å®äºä¸¤ä¸ªå­ç®æ ï¼
    1.  å¼ºååå§âæè-çæâé¶æ®µï¼ä»¥å»ºç«æ ¸å¿åå®¹ååºç¡è´¨éã
    2.  å®ç°é«è´¨éçææ¬åæï¼å¹¶å¿ å®å°å°è¿äºæ¹è¿è½å®å°åç»­å¾åä¸­ã
*   <strong>æå»ºIRGL-300Kæ°æ®éï¼</strong> è¯¥æ°æ®éåå«300Kä¸ªæ ·æ¬ï¼å¹¶è¢«ç»ç»æå­ç§åè§£å­¦ä¹ æ¨¡å¼ï¼å±åæ¶µçäºåºäºææ¬çæèå­¦ä¹ åå®æ´çâæè-å¾åâè½¨è¿¹ã
*   <strong>ä¸¤é¶æ®µè®­ç»ç­ç¥ï¼</strong>
    1.  <strong>ç¬¬ä¸é¶æ®µï¼</strong> å¨ç»ä¸åºç¡æ¨¡åï¼è½åçè¾åºäº¤éææ¬-å¾åï¼ä¸ï¼éè¿å­ç§åè§£å­¦ä¹ æ¨¡å¼ï¼æå»ºé²æ£çæèååæè½åã
    2.  <strong>ç¬¬äºé¶æ®µï¼</strong> å©ç¨å®æ´çâæè-å¾åâè½¨è¿¹æ°æ®ï¼é«æå°å¾®è°IRGç®¡éã
*   <strong>å®å¶çCFGæ¡ä»¶è®¾è®¡ï¼</strong> å¨æ¨çé¶æ®µï¼éå¯¹IRGçæ¹è¿å¾åçææ­¥éª¤ï¼å¼å¥äºCFGï¼Classifier-Free Guidanceï¼æ¡ä»¶è®¾è®¡ï¼ä»¥å¤çå¤æºæ¡ä»¶ï¼æç¤ºãåå§æ¨çãåå§å¾ååæ¹è¿æ¨çï¼ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>SoTAæ§è½ï¼</strong> å¹¿æ³çå®éªè¡¨æï¼IRGå¨GenEvalãWISEãTIIFãGenAI-BenchåOneIG-ENç­å¤ä¸ªä¸»æµT2Iåºåæµè¯ä¸å®ç°äºæåè¿çæ§è½ï¼ç»å¯¹å¢çè¾¾å°5-10ä¸ªç¹ã
*   <strong>æ¾èæåè§è§è´¨éåç»ç²åº¦ä¿çåº¦ï¼</strong> IRGä¸ä»å¨è¯­ä¹æ­£ç¡®æ§ä¸è¡¨ç°åºè²ï¼è¿å¨æ¸²æçº¹çãé´å½±çå®æåææç­ç²¾ç»ç»ææ¹é¢å®ç°äºæ¾èæ¹è¿ï¼å¦å¾1(a)å(b)æç¤ºï¼ã
*   <strong>å¤æ¨¡æè¯ä¼°å¨ä¸è´è®¤å¯ï¼</strong> éè¿å¤æ¨¡æå¤§è¯­è¨æ¨¡åï¼MLLMï¼ä½ä¸ºè¯ä¼°å¨çæåç ç©¶ï¼è¡¨7ï¼ï¼è¡¨æä¸¤è½®IRGçæçå¾åæ¹è¿å¾å°äºä¸åMLLMè¯ä¼°å¨çä¸è´è®¤å¯ï¼å¹³åæååæ°ä»36.7%æåå°63.3%ã
*   <strong>å¼ºå¤§çæä»¤éµå¾ªåä¸çç¥è¯æ¨çè½åï¼</strong> å¨TIIFåWISEåºåä¸çè¡¨ç°è¯æäºIRGå¨è§£éååç¡®éµå¾ªå¤æèªç¶è¯­è¨æä»¤ä»¥åæ´åä¸çç¥è¯æ¹é¢çåè¶è½åã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>æ°æ®ç¨ç¼ºæ§ï¼</strong> æå»ºå®æ´çäº¤éIRGæ°æ®ï¼ç¹å«æ¯é«è´¨éçâåå§å¾å-æ¹è¿å¾åâå¯¹ï¼éå¸¸å°é¾ï¼ç°æT2Iæ°æ®éçè´¨éæ¬¡ä¼ï¼ä¸ä»GPT-4oè¸é¦çæ°æ®æ æ³ç´æ¥çææ­¤ç±»éå¯¹ã
*   <strong>æ¨çè¿ç¨ä¸­çæ½å¨å¤±è´¥æ¨¡å¼ï¼</strong>
    *   <strong>å¾®ç»æé¥±åï¼</strong> å¨éå¤çº¹çï¼å¦ç»ç©ãæ å¶ï¼ä¸ï¼æ¹è¿æ­¥éª¤ææ¶ä¼è¿åº¦å¹³æ»é«é¢ç»èã
    *   <strong>ææ¬æ¸²ææ¼ç§»ï¼</strong> å¨å¯éçº¦æä¸ï¼ç»åè¿ç¨å¯è½çºç²ææ¬çå¯è¯»æ§ä»¥æ¢åé£æ ¼ä¸è´æ§ã
    *   <strong>å¨å±-å±é¨å¼ åï¼</strong> å¨æ¥æ¤åºæ¯ä¸­ï¼å±é¨ç¼è¾å¯è½ä¼è½»å¾®æ°å¨å¨å±å¸å±ã
    *   å½T_out^(2) å¼å¥è®¸å¤åæ­¥ç¼è¾æ¶ï¼è¿äºé®é¢å°¤ä¸ºçªåºã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>æ©å±IRGç®¡éï¼</strong> å°IRGæ¡æ¶åºç¨äºæ´å¤ç±»åçç»ä¸æ¨¡åï¼å¹¶æ¢ç´¢å¤è½®æ¨çèéä»éäºåæ¬¡ç»åè¿­ä»£ã
*   <strong>æ°æ®å¢å¼ºä¸åæï¼</strong> æ¢ç´¢æ´ææçæ¹æ³æ¥åæé«è´¨éçIRGè½¨è¿¹æ°æ®ï¼ä»¥ç¼è§£æ°æ®ç¨ç¼ºé®é¢ã
*   <strong>ä¼åç¼è¾ç­ç¥ï¼</strong> ç ç©¶æ´ä¿å®çç¼è¾ç­ç¥ï¼ä»¥æé«çæç¨³å®æ§ï¼åæ¶æå¤§åå¯å®ç°å¢çã
*   <strong>è§£å³å¤±è´¥æ¨¡å¼ï¼</strong> éå¯¹å¾®ç»æé¥±åãææ¬æ¸²ææ¼ç§»åå¨å±-å±é¨å¼ åç­ç°æå¤±è´¥æ¨¡å¼è¿è¡æ·±å¥ç ç©¶åæ¹è¿ã</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce Interleaving
Reasoning Generation (IRG), a framework that alternates between text-based
thinking and image synthesis: the model first produces a text-based thinking to
guide an initial image, then reflects on the result to refine fine-grained
details, visual quality, and aesthetics while preserving semantics.</li>
<li>To train
IRG effectively, we propose Interleaving Reasoning Generation Learning (IRGL),
which targets two sub-goals: (1) strengthening the initial think-and-generate
stage to establish core content and base quality, and (2) enabling high-quality
textual reflection and faithful implementation of those refinements in a
subsequent image.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.06945v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.06945v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.06907v1'></a></p>
<h2 id="fomo4wheat-toward-reliable-crop-vision-foundation-models-with-globally-curated-data"><a href="https://arxiv.org/abs/2509.06907v1">FoMo4Wheat: Toward reliable crop vision foundation models with globally curated data</a></h2>
<p><strong>Authors:</strong> Bing Han, Chen Zhu, Dong Han, Rui Yu, Songliang Cao, Jianhui Wu, Scott Chapman, Zijian Wang, Bangyou Zheng, Wei Guo, Marie Weiss, Benoit de Solan, Andreas Hund, Lukas Roth, Kirchgessner Norbert, Andrea Visioni, Yufeng Ge, Wenjuan Li, Alexis Comar, Dong Jiang, Dejun Han, Fred Baret, Yanfeng Ding, Hao Lu, Shouyang Liu</p>
<p><strong>Published:</strong> 2025-09-08</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Vision-driven field monitoring is central to digital agriculture, yet models
built on general-domain pretrained backbones often fail to generalize across
tasks, owing to the interaction of fine, variable canopy structures with
fluctuating field conditions. We present FoMo4Wheat, one of the first
crop-domain vision foundation model pretrained with self-supervision on
ImAg4Wheat, the largest and most diverse wheat image dataset to date (2.5
million high-resolution images collected over a decade at 30 global sites,
spanning &gt;2,000 genotypes and &gt;500 environmental conditions). This
wheat-specific pretraining yields representations that are robust for wheat and
transferable to other crops and weeds. Across ten in-field vision tasks at
canopy and organ levels, FoMo4Wheat models consistently outperform
state-of-the-art models pretrained on general-domain dataset. These results
demonstrate the value of crop-specific foundation models for reliable in-field
perception and chart a path toward a universal crop foundation model with
cross-species and cross-task capabilities. FoMo4Wheat models and the ImAg4Wheat
dataset are publicly available online: https://github.com/PheniX-Lab/FoMo4Wheat
and https://huggingface.co/PheniX-Lab/FoMo4Wheat. The demonstration website is:
https://fomo4wheat.phenix-lab.com/.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾è®ºæâFoMo4Wheat: Toward reliable crop vision foundation models with globally curated dataâçå¨é¢æè¦ã</p>
<p><strong>è®ºææè¦ï¼FoMo4Wheatï¼å©ç¨å¨çç²¾éæ°æ®æå»ºå¯é çä½ç©è§è§åºç¡æ¨¡å</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
æ°å­åä¸ä¸­ï¼åºäºè§è§çç°é´çæµè³å³éè¦ãç¶èï¼ç°ææ¨¡åéå¸¸ä¾èµäºå¨éç¨é¢åæ°æ®éï¼å¦ImageNetï¼ä¸é¢è®­ç»çéª¨å¹²ç½ç»ãè¿äºæ¨¡åå¨å¤çç²¾ç»ãå¤åçä½ç©å å±ç»æåæ³¢å¨çç°é´æ¡ä»¶æ¶ï¼æ³åè½åä¸è¶³ï¼å¯¼è´å¨å®éåä¸ä»»å¡ä¸­è¡¨ç°ä¸ä½³ãè®ºææ¨å¨è§£å³è¿ä¸é®é¢ï¼å³å¦ä½å¼åä¸ä¸ªå¨ä½ç©é¢åå·æå¼ºå¤§æ³åè½ååé²æ£æ§çè§è§åºç¡æ¨¡åï¼ä»¥å®ç°å¯é çç°é´æç¥ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
*   <strong>ImAg4Wheatæ°æ®éï¼</strong> è®ºææå»ºå¹¶åå¸äºè¿ä»ä¸ºæ­¢æå¤§ãæå¤æ ·åçéº¦ç±»ä½ç©å¾åæ°æ®éImAg4Wheatãè¯¥æ°æ®éåå«250ä¸å¼ é«åè¾¨çå¾åï¼è·¨è¶åå¹´ï¼å¨30ä¸ªå¨çå°ç¹æ¶éï¼æ¶µç2000å¤ç§åºå åå500å¤ç§ç¯å¢æ¡ä»¶ãè¿ç§å¤§è§æ¨¡ãå¤æ ·åçæ°æ®éæ¯è®­ç»ä½ç©é¢ååºç¡æ¨¡åçå³é®ã
*   <strong>FoMo4Wheatæ¨¡åï¼</strong> è®ºææåºäºFoMo4Wheatï¼è¿æ¯é¦ä¸ªä¸é¨éå¯¹ä½ç©é¢åï¼ç¹å«æ¯å°éº¦ï¼çè§è§åºç¡æ¨¡åãå®åºäºæ åçVision Transformer (ViT) æ¶æï¼å¹¶éç¨èªçç£å­¦ä¹ ï¼ç»åMasked Image Modeling (MIM) åå¯¹æ¯å­¦ä¹ ç­ç¥ï¼å¨ImAg4Wheatæ°æ®éä¸è¿è¡é¢è®­ç»ã
*   <strong>åæ°é«æå¾®è°ï¼</strong> FoMo4Wheatæ¨¡åå¨ä¸æ¸¸ä»»å¡ä¸­éç¨åæ°é«æå¾®è°ç­ç¥ï¼å³å¨éåºç¹å®ä»»å¡æ¶å»ç»éª¨å¹²ç½ç»åæ°ï¼åªè®­ç»è½»éçº§çä»»å¡ç¹å®ééå¨åå¤´é¨ãè¿æé«äºè®¡ç®æçï¼å¹¶ä¿çäºéª¨å¹²ç½ç»çéç¨ç¹å¾è¡¨ç¤ºã
*   <strong>è·¨ä»»å¡åè·¨ç©ç§æ³åè¯ä¼°ï¼</strong> è®ºæå¨åä¸ªç°é´è§è§ä»»å¡ä¸ï¼åæ¬å å±åå¨å®çº§å«çå°éº¦ä»»å¡ãæ°´ç¨»ä»»å¡ä»¥åå¶ä»ä½ç©åæèä»»å¡ï¼å¯¹FoMo4Wheatè¿è¡äºç³»ç»è¯ä¼°ï¼ä»¥éªè¯å¶è·¨ä»»å¡åè·¨ç©ç§çæ³åè½åã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>åè¶çæ§è½ï¼</strong> FoMo4Wheatæ¨¡åå¨ææåä¸ªè¯ä¼°ä»»å¡ä¸­å§ç»ä¼äºå¨éç¨é¢åæ°æ®éä¸é¢è®­ç»çææ°ï¼SOTAï¼æ¨¡åï¼å°¤å¶å¨å·ææææ§çåç´ çº§åå²ä»»å¡ä¸è¡¨ç°åºæ¾èæ¹è¿ã
*   <strong>æ°æ®ç¨ç¼ºåºæ¯ä¸çé²æ£æ§ï¼</strong> å¨æ°æ®éåå°ï¼å¦ä»ä½¿ç¨30%è®­ç»æ°æ®ï¼çæåµä¸ï¼FoMo4Wheatå¨çé¿é¶æ®µåç¾çåç±»ä»»å¡ä¸­ä»è½ä¿æå¶æ§è½ä¼å¿ï¼è¡¨æå¶å¨æ°æ®ç¨ç¼ºåºæ¯ä¸çä¼è¶é²æ£æ§ã
*   <strong>è·¨å¹³å°æ³åè½åï¼</strong> FoMo4Wheatå¨æ£æµä¸åGSDï¼å°é¢éæ ·è·ç¦»ï¼çæ äººæºå¾åä¸­çéº¦ç©æ¶è¡¨ç°åºåè¶çæ§è½ï¼è¯æäºå¶å¨ä¸åééå¹³å°åéç½®ä¸çæ³åè½åã
*   <strong>ä½ç©ç¹å®åºç¡æ¨¡åçä»·å¼ï¼</strong> è¿äºç»ææåå°è¯æäºä½ç©ç¹å®åºç¡æ¨¡åå¨å®ç°å¯é çç°é´æç¥æ¹é¢çä»·å¼ï¼å¹¶ä¸ºå¼åå·æè·¨ç©ç§åè·¨ä»»å¡è½åçéç¨ä½ç©åºç¡æ¨¡åææäºæ¹åã
*   <strong>ç¹å¾å¯è§åï¼</strong> FoMo4Wheatæåçç¹å¾åµå¥è½å¤å½¢ææ¸æ°çèç±»ï¼ææåºåä¸åçé¿é¶æ®µçå³é®æ¤ç©å¨å®ï¼å¹¶è½é«ç²¾åº¦å°åºåä½ç©ç©ç§åæèç±»åï¼ä¼äºDINOv2ã</p>
<p><strong>4. è®ºæä¸­æå°çå±éæ§ï¼</strong>
*   <strong>è®¡ç®èµæºéæ±ï¼</strong> å°½ç®¡FoMo4Wheat Baseæ¨¡åï¼86Måæ°ï¼å¨ç¸å¯¹ç´§åçæ¶æä¸å®ç°äºå·æç«äºåçç²¾åº¦ï¼ä½å¶è®¡ç®éæ±å¯¹äºè¾¹ç¼è®¾å¤æ¥è¯´ä»ç¶å¾é«ã
*   <strong>éç¨æ§ä¸ä¸ä¸æ§çæè¡¡ï¼</strong> å¨å½åæ¨¡åå®¹éåè®­ç»æ°æ®è§æ¨¡ä¸ï¼éç¨é¢ååºç¡æ¨¡åï¼å¦DINOv2ï¼å¨ä¿æå¹¿æ³é¢åéç¨æ§ä¸å®ç°ç¹å®é¢åï¼å¦å°éº¦ï¼çå¼ºå¤§ä¸ä¸æ§ä¹é´å­å¨åºææè¡¡ãFoMo4Wheatä¸æ³¨äºå°éº¦ï¼è½ç¶å¨å°éº¦ä»»å¡ä¸è¡¨ç°åºè²ï¼ä½æªæ¥éç¨ä½ç©åºç¡æ¨¡åï¼FoMo4Cropï¼éè¦æ´ç²¾ç»å°å¹³è¡¡è¿ç§æè¡¡ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>FoMo4Cropï¼</strong> è®ºæä¸ºå¼åå·æè·¨ç©ç§åè·¨ä»»å¡è½åçéç¨ä½ç©åºç¡æ¨¡åFoMo4Cropéºå¹³äºéè·¯ãæªæ¥çç ç©¶å°è´åäºå®ç°è¿ç§å¹³è¡¡ï¼å¯è½éè¦æ°çè®­ç»ç­ç¥ãä¼åçæ¶æä»¥åæ´å¤§ãæ´å¤æ ·åçæ°æ®éã
*   <strong>æ¨¡ååç¼©åä¼åï¼</strong> éå¯¹è¾¹ç¼è®¾å¤é¨ç½²çéæ±ï¼éè¦è¿ä¸æ­¥ç ç©¶æ¨¡ååç¼©ãéååç¥è¯è¸é¦ææ¯ï¼ä»¥å¨ä¸çºç²ç²¾åº¦ææ³åè½åçæåµä¸ï¼æä¾è½»éçº§ãç°é´å°±ç»ªçæ¨¡åã
*   <strong>å¤æ¨¡ææ°æ®éæï¼</strong> é¼å±é«ééçæµå¹³å°çå¿«éæ©å±ï¼ä»¥æä¾å¤§è§æ¨¡ãå¤æ¨¡ææ°æ®æ¶éï¼è¿å°æå©äºå®ç°éç¨ä½ç©åºç¡æ¨¡åçæ¿æ¯ã
*   <strong>æ´ç²¾ç»çç¹å¾å­¦ä¹ ï¼</strong> å°½ç®¡FoMo4Wheatå¨åç´ çº§ä»»å¡ä¸­è¡¨ç°åºè²ï¼ä½ä»éè¿ä¸æ­¥æ¢ç´¢å¦ä½æ´ææå°ææåå©ç¨ç²¾ç»çæ¤ç©ç¹å¾ï¼ä»¥åºå¯¹æ´å¤æçåä¸è§è§ææã</p>
<p>æ»èè¨ä¹ï¼è¿ç¯è®ºæéè¿æå»ºå¤§è§æ¨¡ãå¤æ ·åçImAg4Wheatæ°æ®éåå¼åFoMo4Wheatæ¨¡åï¼å¨ä½ç©é¢åè§è§åºç¡æ¨¡åæ¹é¢åå¾äºæ¾èè¿å±ï¼ä¸ºæ°å­åä¸çæªæ¥åå±å¥ å®äºåå®åºç¡ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present FoMo4Wheat, one of the first
crop-domain vision foundation model pretrained with self-supervision on
ImAg4Wheat, the largest and most diverse wheat image dataset to date (2.5
million high-resolution images collected over a decade at 30 global sites,
spanning &gt;2,000 genotypes and &gt;500 environmental conditions).</li>
<li>Across ten in-field vision tasks at
canopy and organ levels, FoMo4Wheat models consistently outperform
state-of-the-art models pretrained on general-domain dataset.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.06907v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.06907v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.06818v1'></a></p>
<h2 id="umo-scaling-multi-identity-consistency-for-image-customization-via-matching-reward"><a href="https://arxiv.org/abs/2509.06818v1">UMO: Scaling Multi-Identity Consistency for Image Customization via Matching Reward</a></h2>
<p><strong>Authors:</strong> Yufeng Cheng, Wenxu Wu, Shaojin Wu, Mengqi Huang, Fei Ding, Qian He</p>
<p><strong>Published:</strong> 2025-09-08</p>
<p><strong>Categories:</strong> cs.CV, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Recent advancements in image customization exhibit a wide range of
application prospects due to stronger customization capabilities. However,
since we humans are more sensitive to faces, a significant challenge remains in
preserving consistent identity while avoiding identity confusion with
multi-reference images, limiting the identity scalability of customization
models. To address this, we present UMO, a Unified Multi-identity Optimization
framework, designed to maintain high-fidelity identity preservation and
alleviate identity confusion with scalability. With "multi-to-multi matching"
paradigm, UMO reformulates multi-identity generation as a global assignment
optimization problem and unleashes multi-identity consistency for existing
image customization methods generally through reinforcement learning on
diffusion models. To facilitate the training of UMO, we develop a scalable
customization dataset with multi-reference images, consisting of both
synthesised and real parts. Additionally, we propose a new metric to measure
identity confusion. Extensive experiments demonstrate that UMO not only
improves identity consistency significantly, but also reduces identity
confusion on several image customization methods, setting a new
state-of-the-art among open-source methods along the dimension of identity
preserving. Code and model: https://github.com/bytedance/UMO</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Yufeng Chengç­äººæ°åçè®ºæâUMO: Scaling Multi-Identity Consistency for Image Customization via Matching Rewardâçå¨é¢æè¦ã</p>
<hr />
<h3 id="umo">è®ºææè¦ï¼UMO: éè¿å¹éå¥å±å®ç°å¾åå®å¶ä¸­çå¤èº«ä»½ä¸è´æ§æ©å±</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
å¾åå®å¶ï¼Image Customizationï¼å¨çæå¾åæ¶ï¼éè¦åæ¶éµå¾ªææ¬æç¤ºçè¯­ä¹åå®¹ååèå¾åçè§è§å¤è§ãç¹å«æ¯å¨äººç±»èº«ä»½å®å¶æ¹é¢ï¼ç°ææ¹æ³é¢ä¸´ä¸¤å¤§ææï¼
*   <strong>èº«ä»½ä¸è´æ§ï¼Identity Preservationï¼ï¼</strong> å³ä½¿æ¯ç»å¾®çå¤è§å·®å¼ä¹å¯è½å¯¼è´èº«ä»½ä¿çåº¦æ¾èä¸éï¼äººç±»å¯¹äººè¸çæææ§ä½¿å¾è¿ä¸é®é¢å°¤ä¸ºçªåºã
*   <strong>å¤èº«ä»½æ··æ·ï¼Identity Confusionï¼ï¼</strong> å½éè¦åæ¶å®å¶å¤ä¸ªèº«ä»½æ¶ï¼æ¨¡åä¸ä»è¦ä¿çæ¯ä¸ªä¸ªä½èº«ä»½çç¬ç¹ç¹å¾ï¼è¿è¦å¨çæå¾åä¸­ä¿æå®ä»¬ä¹é´æ¸æ°çåºå«ï¼é¿åèº«ä»½æ··æ·ãç°ææ¹æ³éå¸¸éç¨âä¸å¯¹ä¸æ å°âèå¼ï¼éçèº«ä»½æ°éçå¢å ï¼è¿ç§èå¼å¨å¤çâèº«ä»½ååå¼æ§âï¼intra-ID variabilityï¼åâèº«ä»½é´åºååº¦âï¼inter-ID distinctionï¼æ¹é¢è¡¨ç°ä¸ä½³ï¼éå¶äºæ¨¡åçæ©å±æ§ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
ä¸ºäºè§£å³ä¸è¿°ææï¼è®ºææåºäº <strong>UMO (Unified Multi-identity Optimization)</strong> æ¡æ¶ï¼å¶æ ¸å¿åæ°ç¹åæ¬ï¼</p>
<ul>
<li><strong>å¤å¯¹å¤å¹éèå¼ï¼Multi-to-Multi Matching Paradigmï¼ï¼</strong> UMO å°å¤èº«ä»½çæéæ°å®ä¹ä¸ºä¸ä¸ªå¨å±åéä¼åé®é¢ãä¸ä¼ ç»çä¸å¯¹ä¸æ å°ä¸åï¼UMO æ¨å¨æå¤§åå¤ä¸ªçæèº«ä»½ä¸å¤ä¸ªåèèº«ä»½ä¹é´çæ´ä½å¹éè´¨éï¼ä»èä¸ºæ¯ä¸ªçæèº«ä»½æ¾å°æåéçåèèº«ä»½ï¼ä»¥æå¤§åèº«ä»½é´åºååº¦å¹¶æå°åèº«ä»½ååå¼æ§çå½±åã</li>
<li><strong>åèå¥å±åé¦å­¦ä¹ ï¼Reference Reward Feedback Learning, ReReFLï¼ï¼</strong> UMO éè¿ä¸ç§æ°é¢çå¼ºåå­¦ä¹ æºå¶æ¥æä½å¤å¯¹å¤å¹éèå¼ï¼ä»¥æé«ç°æå¾åå®å¶æ¹æ³çèº«ä»½ä¸è´æ§ã<ul>
<li><strong>åèº«ä»½å¥å±ï¼Single Identity Reward, SIRï¼ï¼</strong> åºäºèº«ä»½åµå¥ä¹é´çä½å¼¦è·ç¦»ï¼ç¡®ä¿é«ä¿çåº¦ã</li>
<li><strong>å¤èº«ä»½å¹éå¥å±ï¼Multi-Identity Matching Reward, MIMRï¼ï¼</strong> éå¯¹å¤èº«ä»½åºæ¯ï¼å°å¹éé®é¢å»ºæ¨¡ä¸ºäºåå¾çå¨å±åéé®é¢ï¼éè¿åçå©ç®æ³é«æè®¡ç®æä¼åéï¼å¹¶åºäºæ­¤åéå®ä¹ MIMRï¼ä»¥åæ¶æé«å¤èº«ä»½ä¿çåº¦å¹¶åè½»æ··æ·ã</li>
</ul>
</li>
<li><strong>å¯æ©å±çå¤åèå¾åå®å¶æ°æ®éï¼</strong> ä¸ºäºææè®­ç» UMOï¼å¢éå¼åäºä¸ä¸ªåå«åæåçå®å¾åçå¯æ©å±æ°æ®éï¼å¶ä¸­åå«æ¯ä¸ªèº«ä»½çå¤ä¸ªåèå¾åã</li>
<li><strong>æ°çèº«ä»½æ··æ·åº¦éï¼ID-Confï¼ï¼</strong> æåºäºä¸ç§æ°çåº¦éæ å ID-Confï¼ç¨äºç²¾ç¡®è¯ä¼°å¤èº«ä»½æ··æ·çç¨åº¦ãå®å®ä¹ä¸ºç»å®åèèº«ä»½ï¼ä¸¤ä¸ªæç¸ä¼¼ççæåéäººè¸ä¹é´çç¸å¯¹è£åº¦ã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>æ¾èæåèº«ä»½ä¸è´æ§å¹¶åå°æ··æ·ï¼</strong> å¹¿æ³çå®éªï¼å¨ XVerseBench å OmniContext ç­åºåä¸ï¼è¡¨æï¼UMO å¨åç§å¾åå®å¶æ¹æ³ä¸æ¾èæé«äºèº«ä»½ç¸ä¼¼æ§ï¼ID-Simï¼å¹¶åè½»äºèº«ä»½æ··æ·ï¼ID-Confï¼ã
*   <strong>è¾¾å°æåè¿æ°´å¹³ï¼SOTAï¼ï¼</strong> UMO å¨èº«ä»½ä¿æç»´åº¦ä¸ï¼å¨å¼æºæ¹æ³ä¸­åå¾äºæ°çæåè¿ç»æï¼å±ç¤ºäºå¶å¼ºå¤§çéç¨æ§åé«ä¿çåº¦èº«ä»½çæè½åã
*   <strong>å¯æ©å±æ§ï¼</strong> UMO æ¡æ¶å¨åèº«ä»½å°å¤èº«ä»½åºæ¯ä¸­åè¡¨ç°åºè¯å¥½çæ³åè½åï¼ææè§£å³äºéçèº«ä»½æ°éå¢å èå¯¼è´çèº«ä»½ä¿çåº¦ä¸éåæ··æ·é®é¢ã
*   <strong>æ¶èç ç©¶ï¼</strong> æ¶èå®éªè¯æäº ReReFL å MIMR çæææ§ï¼ç¹å«æ¯ MIMR å¨å¤èº«ä»½åºæ¯ä¸­éè¿æ­£ç¡®çé¢é¨çç£åéï¼æ¾èæåäºèº«ä»½ä¸è´æ§å¹¶åè½»äºæ··æ·ã
*   <strong>ç¨æ·ç ç©¶ï¼</strong> ç¨æ·ç ç©¶é®å·ç»ææ¾ç¤ºï¼UMO å¨èº«ä»½ä¸è´æ§ãæç¤ºéµå¾ªãç¾å­¦åæ´ä½æ§è½ç­å¤ä¸ªç»´åº¦ä¸è·å¾äºæä½³åå¥½ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
å°½ç®¡ UMO æ¨å¨ä¿æé«ä¿çåº¦èº«ä»½ä¿çå¹¶åè½»å¤èº«ä»½æ··æ·ï¼ä½ä½èæåºï¼<strong>ç¨³å®å°æ©å±å°æ´å¤èº«ä»½ä»ç¶åå°é¢è®­ç»æ¨¡ååèè½åæ¥å§ä¸éçéå¶</strong>ï¼å½åèå¾åæèº«ä»½æ°éå¢å æ¶ï¼è¿ç§éå¶åå¾æ´å ææ¾ãè¿ä¸ [40] ä¸­æåºçè§ç¹ç¸ä¼¼ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
è®ºææç¡®æåºäºå½åæ¨¡åå¨å¤çâæ´å¤èº«ä»½âæ¶çå±éæ§ï¼è¿ä¸ºæªæ¥çç ç©¶æä¾äºæç¡®çæ¹åï¼
*   <strong>è¿ä¸æ­¥æåæ¨¡åå¨è¶å¤§è§æ¨¡å¤èº«ä»½åºæ¯ä¸çå¯æ©å±æ§ï¼</strong> å¦ä½å¨åèå¾åæèº«ä»½æ°éå¤§å¹å¢å æ¶ï¼ä¿æçè³æåé¢è®­ç»æ¨¡åçåèè½åï¼æ¯æªæ¥éè¦æ»åçå³é®é¾é¢ãè¿å¯è½æ¶åæ´é«æçèº«ä»½ç¼ç æºå¶ãæ´é²æ£çå¹éç®æ³ï¼æèå¨æ°çæ¨¡åæ¶æè®¾è®¡ã
*   <strong>æ¢ç´¢æ´å¤æçèº«ä»½äº¤äºåå³ç³»ï¼</strong> ç®åçå¹éå¥å±ä¸»è¦å³æ³¨ä¸ªä½èº«ä»½çå¹éï¼æªæ¥å¯ä»¥æ¢ç´¢å¦ä½æ´å¥½å°å»ºæ¨¡åå©ç¨å¤èº«ä»½ä¹é´çå¤æäº¤äºåå³ç³»ï¼ä»¥çææ´èªç¶ãæ´ç¬¦åè¯­å¢çå¤äººå¾åã
*   <strong>ç»åæ´åè¿çæ©æ£æ¨¡åæ¶æï¼</strong> éçæ©æ£æ¨¡åææ¯çä¸æ­åå±ï¼å° UMO æ¡æ¶ä¸ææ°çãæ´å¼ºå¤§çæ©æ£æ¨¡åæ¶æç¸ç»åï¼å¯è½ä¼è¿ä¸æ­¥æåæ§è½åçæè´¨éã</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this, we present UMO, a Unified Multi-identity Optimization
framework, designed to maintain high-fidelity identity preservation and
alleviate identity confusion with scalability.</li>
<li>To facilitate the training of UMO, we develop a scalable
customization dataset with multi-reference images, consisting of both
synthesised and real parts.</li>
<li>Additionally, we propose a new metric to measure
identity confusion.</li>
<li>Extensive experiments demonstrate that UMO not only
improves identity consistency significantly, but also reduces identity
confusion on several image customization methods, setting a new
state-of-the-art among open-source methods along the dimension of identity
preserving.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.06818v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.06818v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.06781v1'></a></p>
<h2 id="urbantwin-high-fidelity-synthetic-replicas-of-roadside-lidar-datasets"><a href="https://arxiv.org/abs/2509.06781v1">UrbanTwin: High-Fidelity Synthetic Replicas of Roadside Lidar Datasets</a></h2>
<p><strong>Authors:</strong> Muhammad Shahbaz, Shaurya Agarwal</p>
<p><strong>Published:</strong> 2025-09-08</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>This article presents UrbanTwin datasets - high-fidelity, realistic replicas
of three public roadside lidar datasets: LUMPI, V2X-Real-IC, and TUMTraf-I.
Each UrbanTwin dataset contains 10K annotated frames corresponding to one of
the public datasets. Annotations include 3D bounding boxes, instance
segmentation labels, and tracking IDs for six object classes, along with
semantic segmentation labels for nine classes. These datasets are synthesized
using emulated lidar sensors within realistic digital twins, modeled based on
surrounding geometry, road alignment at lane level, and the lane topology and
vehicle movement patterns at intersections of the actual locations
corresponding to each real dataset. Due to the precise digital twin modeling,
the synthetic datasets are well aligned with their real counterparts, offering
strong standalone and augmentative value for training deep learning models on
tasks such as 3D object detection, tracking, and semantic and instance
segmentation. We evaluate the alignment of the synthetic replicas through
statistical and structural similarity analysis with real data, and further
demonstrate their utility by training 3D object detection models solely on
synthetic data and testing them on real, unseen data. The high similarity
scores and improved detection performance, compared to the models trained on
real data, indicate that the UrbanTwin datasets effectively enhance existing
benchmark datasets by increasing sample size and scene diversity. In addition,
the digital twins can be adapted to test custom scenarios by modifying the
design and dynamics of the simulations. To our knowledge, these are the first
digitally synthesized datasets that can replace in-domain real-world datasets
for lidar perception tasks. UrbanTwin datasets are publicly available at
https://dataverse.harvard.edu/dataverse/ucf-ut.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Muhammad ShahbazåShaurya Agarwalæ°åçè®ºæâUrbanTwin: High-Fidelity Synthetic Replicas of Roadside Lidar Datasetsâçå¨é¢æè¦ã</p>
<hr />
<h3 id="urbantwin">UrbanTwin: é«ä¿çè·¯è¾¹æ¿åé·è¾¾æ°æ®éåæå¯æ¬ è®ºææè¦</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³å½åè·¯è¾¹æ¿åé·è¾¾æ°æ®éå¨æºè½äº¤éç³»ç»ï¼ITSï¼æç¥ç®æ³å¼åä¸­çæ ¸å¿ææï¼é«è´¨éãå¤§è§æ¨¡ãå¸¦æ æ³¨ççå®ä¸çæ¿åé·è¾¾æ°æ®éçç¨ç¼ºæ§ãé«æçè·åææ¬åèæ¶æ§ãç°æçæ¨¡æå¨å¾å¾å­å¨âæ¨¡æå°ç°å®âçé¢åé¸¿æ²ï¼å¯¼è´å¨åææ°æ®ä¸è®­ç»çæ¨¡åå¨çå®ä¸çä¸­è¡¨ç°ä¸ä½³ãå æ­¤ï¼ç ç©¶é®é¢æ¯å¦ä½åå»ºé«ä¿çãé¼çä¸ä¸çå®ä¸çæ°æ®é«åº¦å¯¹é½çåææ¿åé·è¾¾æ°æ®éï¼ä»¥æææ¯æ3Dç®æ æ£æµãè·è¸ªãè¯­ä¹åå®ä¾åå²ç­æç¥ä»»å¡ï¼å¹¶å¼¥è¡¥æ¨¡æä¸ç°å®ä¹é´çå·®è·ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
è¯¥è®ºææåºäºUrbanTwinæ°æ®éï¼å¶æ ¸å¿åæ°å¨äºï¼
*   <strong>é«ä¿çæ°å­å­ªçå»ºæ¨¡ï¼</strong> è®ºææåºäºä¸ç§æ°é¢çæ°å­å­ªçå»ºæ¨¡æ¹æ³ï¼å°çå®ä¸ççéæåç´ ï¼å¦ç²¾ç¡®ç3Då ä½ç»æãè½¦éçº§éè·¯ç¹å¾ãå»ºç­ç©ãæ¤è¢«ï¼åå¨æè¡ä¸ºï¼å¦å¸åçäº¤éæºå¨æ¨¡å¼ãä¼ æå¨è§æ ¼ï¼éæå°æ¨¡æç¯å¢ä¸­ãè¿ç§ç²¾ç¡®çå»ºæ¨¡ç¡®ä¿äºåææ°æ®å¨ç©ºé´åæå®ä¸ä¸çå®ä¸çæ°æ®é«åº¦å¯¹é½ã
*   <strong>å¤å¶ç°æåºåæ°æ®éï¼</strong> UrbanTwinæ°æ®éå¹¶ééç¨åææ°æ®ï¼èæ¯ä¸é¨è®¾è®¡ç¨äºå¤å¶ä¸ä¸ªç°æå¬å±è·¯è¾¹æ¿åé·è¾¾æ°æ®éï¼LUMPIãV2X-Real-ICåTUMTraf-Iï¼çæ ¸å¿ç¹å¾ï¼ä»èå¢å¼ºç°æåºåæ°æ®éçä»·å¼ã
*   <strong>å¤ä»»å¡æ¯æï¼</strong> åææ°æ®éåçæ¯æ3Dç®æ æ£æµãè·è¸ªãè¯­ä¹åå²åå®ä¾åå²è¿åé¡¹æ ¸å¿æç¥ä»»å¡ï¼æä¾äºä¸°å¯çæ æ³¨ä¿¡æ¯ï¼3Dè¾¹çæ¡ãå®ä¾åå²æ ç­¾ãè·è¸ªIDåè¯­ä¹åå²æ ç­¾ï¼ã
*   <strong>åºäºCARLAæ¨¡æå¨ï¼</strong> å©ç¨CARLAæ¨¡æå¨ï¼å¨æ ¹æ®å¬å¼å°çåç»ææ°æ®æå»ºçèªå®ä¹å°å¾ä¸è¿è¡æ¨¡æï¼çæåææ°æ®ãä¼ æå¨éç½®ä¸çå®ä¼ æå¨è§æ ¼ç²¾ç¡®å¹éã
*   <strong>éæºä½çå®çå¨æåå®¹ï¼</strong> äº¤éæµåéè·¯ä½¿ç¨èç±»åï¼æ±½è½¦ãå¡è½¦ãèªè¡è½¦ãæ©æè½¦ãå¬å±æ±½è½¦ï¼ä»¥éæºä½ç¬¦åçå®ä¸çäº¤éè§ååç©çäº¤äºçæ¹å¼çæï¼å¢å äºåºæ¯å¤æ ·æ§ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>é«ç»æååå¸ç¸ä¼¼æ§ï¼</strong> éè¿å¯¹åææ°æ®ä¸çå®æ°æ®è¿è¡å¹¿æ³çç»è®¡åç»æç¸ä¼¼æ§åæï¼åæ¬åºæ¯å¤æåº¦ãç¹å¯åº¦ãç®æ å¤§å°åç±»å«åå¸ï¼ï¼ç»æè¡¨æUrbanTwinæ°æ®éä¸çå®ä¸çæ°æ®é«åº¦å¯¹é½ï¼å·ææå°çé¢åé¸¿æ²ã
*   <strong>å¨æç¥æ¨¡åä¸­çå®ç¨æ§ï¼</strong> å¨3Dç®æ æ£æµä»»å¡ä¸­ï¼ä½¿ç¨UrbanTwinåææ°æ®è®­ç»çæ¨¡åå¨çå®ãæªè§æ°æ®ä¸è¿è¡äºæµè¯ã
    *   å¨LUMPIæ°æ®éä¸ï¼ä»ä½¿ç¨åææ°æ®è®­ç»çSEEDæ¨¡åæ§è½ä¼äºå¨çå®æ°æ®ä¸è®­ç»çSEEDæ¨¡åï¼è¿è¡¨æåææ°æ®ä¸ä»ææäºçå®æ°æ®éçç»æå¤ææ§ï¼èä¸å¯è½æä¾äºæ´é«çä¸è´æ§æ æ³¨è´¨éã
    *   å¨V2X-Realæ°æ®éä¸ï¼ä»ä½¿ç¨åææ°æ®è®­ç»çSECONDæ¨¡åå¨æ±½è½¦ç±»å«ä¸çå¹³åç²¾åº¦ï¼APï¼è¾¾å°äº63.18%@3D IoU=0.5ï¼è¶è¶äºåå§V2X-Realè®ºæä¸­æ¥åçå¤§å¤æ°åºåæ¨¡åï¼å°½ç®¡æ²¡æä½¿ç¨ä»»ä½çå®æ°æ®è¿è¡è®­ç»ã
*   <strong>æ¾èæä¹ï¼</strong> è¿äºç»æè¡¨æUrbanTwinæ°æ®éè½å¤ææå¢å¼ºç°æåºåæ°æ®éï¼éè¿å¢å æ ·æ¬éååºæ¯å¤æ ·æ§ï¼ä¸ºè®­ç»æ·±åº¦å­¦ä¹ æ¨¡åæä¾å¼ºå¤§çç¬ç«åå¢å¼ºä»·å¼ãå®ä»¬ä¸ºæ¿åé·è¾¾æç¥ä»»å¡æä¾äºå¯æ¿ä»£çå®ä¸çæ°æ®çæ°å­åææ°æ®éï¼å¤§å¤§éä½äºæ°æ®éåå»ºçææ¬åäººåæå¥ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>è¡äººå»ºæ¨¡éå¶ï¼</strong> å½åçæ¬çUrbanTwinæ°æ®éç±äºCARLAå¨åç¡®å»ºæ¨¡äººç±»è¡ä¸ºæ¹é¢çå·²ç¥éå¶ï¼ææ¶çç¥äºè¡äººç¸å³ç±»å«ãè®ºæè®¡åå¨æªæ¥æ´æ°ä¸­å å¥æ´çå®çè¡äººæ¨¡åã
*   <strong>ç¹å®åºæ¯éå¶ï¼</strong> è½ç¶æ°æ®éæ¨å¨å¤å¶ç¹å®äº¤åå£åºæ¯ï¼ä½å¶å¨æåç´ æ¯éæºçæçï¼ä¸å®å¨å¤å¶çå®ä¸çä¸­ç¹å®è½¦è¾çç²¾ç¡®è½¨è¿¹ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>æ´åè¡äººåVRUç±»å«ï¼</strong> è®¡åå¨æªæ¥çæ¬ä¸­å å¥è¡äººåæåä¼¤å®³éè·¯ä½¿ç¨èï¼VRUï¼ç±»å«ï¼ä»¥è¿ä¸æ­¥å®åæ°æ®éã
*   <strong>æ©å±éªè¯ä»»å¡ï¼</strong> å°éªè¯èå´æ©å±å°è·è¸ªãåå²åä¼ æå¨èåä»»å¡ï¼ä»¥å¨é¢è¯ä¼°åææ°æ®çå®ç¨æ§ã
*   <strong>åå»ºæ´å¤åæå¯æ¬ï¼</strong> ä½ä¸ºUCFæ°å­å­ªçè®¡åçä¸é¨åï¼å°ä¸ºå¶ä»å¬å±æ¿åé·è¾¾æ°æ®éåå»ºæ´å¤åæå¯æ¬ã
*   <strong>èªå®ä¹åºæ¯æµè¯ï¼</strong> æ°å­å­ªçå¯ä»¥ä¿®æ¹æ¨¡æçè®¾è®¡åå¨æï¼ä»¥æµè¯èªå®ä¹åºæ¯ï¼ä¾å¦ç½è§äºä»¶æ³¨å¥ãå¤©æ°åååè¿å¨æ°å¨ï¼ä»èå¸®å©ç ç©¶äººåå¼åè½å¤æ³åå°è¾¹ç¼æ¡ä¾çæ¨¡åã
*   <strong>é¢åéåºåè¿ç§»å­¦ä¹ ï¼</strong> åç¡®å»ºæ¨¡çæ°å­å­ªçä¸ºæ¨¡æå°ç°å®è¿ç§»å­¦ä¹ ãé¢åéåºåè·¨ä»»å¡å­¦ä¹ æ¡æ¶ç­æ¹æ³è®ºåæ°å¼è¾äºæ°éå¾ã</p>
<hr />
<p>æ»èè¨ä¹ï¼UrbanTwinè®ºæéè¿å¼å¥é«ä¿çæ°å­å­ªçå»ºæ¨¡æ¹æ³ï¼æååå»ºäºä¸çå®ä¸çè·¯è¾¹æ¿åé·è¾¾æ°æ®éé«åº¦å¯¹é½çåææ°æ®éãè¿äºæ°æ®éä¸ä»å¨ç»æåç»è®¡ä¸ä¸çå®æ°æ®ç¸ä¼¼ï¼èä¸å¨å®éæç¥ä»»å¡ï¼å¦3Dç®æ æ£æµï¼ä¸­è¡¨ç°åºè²ï¼çè³å¨æäºæåµä¸è¶è¶äºå¨çå®æ°æ®ä¸è®­ç»çæ¨¡åãè¿ä¸ºè§£å³çå®ä¸çæ°æ®éè·åçæææä¾äºä¸ä¸ªå¯æ©å±ãç»æµé«æçè§£å³æ¹æ¡ï¼å¹¶ä¸ºæºè½äº¤éç³»ç»ä¸­çæ¿åé·è¾¾æç¥ç ç©¶åé¨ç½²å¼è¾äºæ°çå¯è½æ§ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To our knowledge, these are the first
digitally synthesized datasets that can replace in-domain real-world datasets
for lidar perception tasks.</li>
<li>UrbanTwin datasets are publicly available at
https://dataverse.harvard.edu/dataverse/ucf-ut.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.06781v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.06781v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.06579v1'></a></p>
<h2 id="causnvs-autoregressive-multi-view-diffusion-for-flexible-3d-novel-view-synthesis"><a href="https://arxiv.org/abs/2509.06579v1">CausNVS: Autoregressive Multi-view Diffusion for Flexible 3D Novel View Synthesis</a></h2>
<p><strong>Authors:</strong> Xin Kong, Daniel Watson, Yannick StrÃ¼mpler, Michael Niemeyer, Federico Tombari</p>
<p><strong>Published:</strong> 2025-09-08</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Multi-view diffusion models have shown promise in 3D novel view synthesis,
but most existing methods adopt a non-autoregressive formulation. This limits
their applicability in world modeling, as they only support a fixed number of
views and suffer from slow inference due to denoising all frames
simultaneously. To address these limitations, we propose CausNVS, a multi-view
diffusion model in an autoregressive setting, which supports arbitrary
input-output view configurations and generates views sequentially. We train
CausNVS with causal masking and per-frame noise, using pairwise-relative camera
pose encodings (CaPE) for precise camera control. At inference time, we combine
a spatially-aware sliding-window with key-value caching and noise conditioning
augmentation to mitigate drift. Our experiments demonstrate that CausNVS
supports a broad range of camera trajectories, enables flexible autoregressive
novel view synthesis, and achieves consistently strong visual quality across
diverse settings. Project page: https://kxhit.github.io/CausNVS.html.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Xin Kongç­äººæ°åçè®ºæâCausNVS: Autoregressive Multi-view Diffusion for Flexible 3D Novel View Synthesisâçå¨é¢æè¦ã</p>
<hr />
<h3 id="causnvs-autoregressive-multi-view-diffusion-for-flexible-3d-novel-view-synthesis_1">CausNVS: Autoregressive Multi-view Diffusion for Flexible 3D Novel View Synthesis è®ºææè¦</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
ç°æçå¤è§è§æ©æ£æ¨¡åå¨3Dæ°é¢è§è§åæï¼NVSï¼ä¸­è¡¨ç°åºè²ï¼ä½å¤§å¤éç¨éèªåå½ï¼non-autoregressiveï¼å½¢å¼ãè¿æå³çå®ä»¬åªè½æ¯æåºå®æ°éçè§è§è¾å¥ï¼å¹¶ä¸éè¦åæ¶å¯¹ææå¸§è¿è¡å»åªï¼å¯¼è´æ¨çéåº¦æ¢ï¼éå¶äºå¶å¨ä¸çå»ºæ¨¡ï¼world modelingï¼ç­éè¦çµæ´»ãåºååè§è§çæçåºæ¯ä¸­çåºç¨ãæ ¸å¿ææå¨äºå¦ä½å®ç°çµæ´»çè¾å¥-è¾åºè§è§éç½®ãåºååçæï¼åæ¶è§£å³èªåå½æ¨¡åå¸¸è§çâæ¼ç§»âï¼driftï¼é®é¢åKVç¼å­ï¼KV Cachingï¼å¨ç»å¯¹å§¿æç¼ç ä¸çä¸å¼å®¹æ§ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
è®ºææåºäºCausNVSï¼ä¸ä¸ªèªåå½å¤è§è§æ©æ£æ¨¡åï¼éè¿ä»¥ä¸åæ°è§£å³äºä¸è¿°é®é¢ï¼
*   <strong>èªåå½è®¾ç½®ä¸å ææ©ç ï¼Causal Maskingï¼ï¼</strong> CausNVSéç¨èªåå½æ¡æ¶ï¼éè¿å¨å¸§çº§æ³¨æåå±ä¸­å¼å¥å ææ©ç ï¼ç¡®ä¿æ¯ä¸ªè§è§ä»åºäºååè§æµå°çè¾å¥åå·²åæçè¾åºè¿è¡æ¡ä»¶åçæãè¿ä½¿å¾æ¨¡åè½å¤æ¯æä»»æè¾å¥-è¾åºè§è§éç½®ï¼å¹¶å¨åæ¬¡è®­ç»ä¸­å®ç°åºååçæã
*   <strong>éå¸§åªå£°æ¡ä»¶ï¼Per-frame Noise Conditioningï¼ï¼</strong> å¨è®­ç»æé´ï¼æ¨¡åå¯¹æ¯å¸§åºç¨ç¬ç«çåªå£°æ°´å¹³ï¼ä½¿å¶å­¦ä¹ ä»ä¸ç¡®å®æä¸å®ç¾çä¸ä¸æè¿è¡å»åªãè¿æå©äºç¼©å°è®­ç»ä¸æ¨çä¹é´çå·®è·ï¼å¹¶å¢å¼ºæ¨¡åå¯¹ç´¯ç§¯è¯¯å·®çé²æ£æ§ã
*   <strong>æå¯¹ç¸å¯¹ç¸æºå§¿æç¼ç ï¼CaPEï¼ï¼</strong> éç¨CaPEæ¥ç¼ç è§è§ä¹é´çç¸å¯¹å§¿æå³ç³»ï¼èéç»å¯¹å§¿æãCaPEç¡®ä¿æ³¨æååæ°å¯¹å¨å±åæ ååä¿æä¸åï¼ä»èä½¿KVç¼å­è½å¤å¨åèå¸§ç§»å¨ææ»å¨çªå£æ¨çæ¶ä¿æææï¼æ ééæ°è®¡ç®ã
*   <strong>æ¨çæ¶æ¼ç§»ç¼è§£ç­ç¥ï¼</strong> å¨æ¨çæ¶ï¼CausNVSç»åäºç©ºé´æç¥æ»å¨çªå£ï¼spatially-aware sliding-windowï¼åKVç¼å­ï¼ä»¥é«æèåä¸ä¸æãåæ¶ï¼éè¿åªå£°æ¡ä»¶å¢å¼ºï¼noise conditioning augmentationï¼ï¼ä¸ºååçæçè§è§åéè¾å°çåªå£°æ°´å¹³ï¼ä½¿å¶è¢«è§ä¸ºå¸¦åªå£°çè¾å¥ï¼è¿ä¸æ­¥ç¨³å®åç»­é¢æµå¹¶ç¼è§£æ¼ç§»ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>å¼ºå¤§çè§è§è´¨éåæ³åè½åï¼</strong> CausNVSå¨æ ååºå®è§è§åºåæµè¯ä¸­å®ç°äºå·æç«äºåçè§è§è´¨éï¼å¹¶å¨çµæ´»çNVSè®¾ç½®ä¸­è¶è¶äºæåè¿çåºçº¿æ¨¡åã
*   <strong>ç¨³å®çèªåå½çæï¼</strong> å³ä½¿å¨æ¯è®­ç»åºåé¿åº¦é¿10åçrolloutä¸­ï¼æ¨¡åä¹è½ä¿æç¨³å®çèªåå½çæï¼å±ç¤ºäºå¶å¼ºå¤§çæ³åè½åã
*   <strong>é«æç3Dç©ºé´è®°å¿ï¼</strong> éè¿ç»åCaPEãKVç¼å­åæ»å¨çªå£æ³¨æåï¼CausNVSå®ç°äºéå¼ä¸é«æç3Dç©ºé´è®°å¿ï¼ç¡®ä¿å¨ä»»æç¸æºè¿å¨ä¸ï¼åæ¬éå¤è®¿é®çè§è§ï¼ççæå·æç©ºé´ä¸è´æ§ã
*   <strong>çµæ´»çç¸æºè½¨è¿¹æ¯æï¼</strong> å®éªè¯æCausNVSæ¯æå¹¿æ³çç¸æºè½¨è¿¹ï¼åæ¬åè¿ãåéãæè½¬åç¯å½¢å¹³ç§»ï¼å¹¶è½ä¿æ3Dä¸è´æ§ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>å¤æ­¥å»åªçå®æ¶æ§éå¶ï¼</strong> æ¨¡åä¾èµäºå¤æ­¥å»åªè¿ç¨ï¼è¿éå¶äºå¶å¨å®æ¶åºç¨ä¸­çæ§è½ã
*   <strong>æ°æ®éè§æ¨¡åå¤æ ·æ§ï¼</strong> å°½ç®¡è¡¨ç°åºè²ï¼ä½æ¨¡åä»å¯è½åçäºæ´å¤§è§æ¨¡ãæ´å¤æ ·åçæ°æ®éï¼ä¾å¦åå«ç©ä½åè§é¢ï¼ï¼ä»¥è¿ä¸æ­¥æåæ³åè½åååéªç¥è¯ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>å éçæï¼</strong> æ¢ç´¢éè¿ä¸è´æ§è®­ç»ï¼consistency trainingï¼æè¸é¦ï¼distillationï¼ç­æ¹æ³å®ç°æ´å¿«ççæéåº¦ã
*   <strong>æ©å±å°æ´é¿åºååå¤æ ·åæ°æ®ï¼</strong> è¿ä¸æ­¥æ©å±æ¨¡åä»¥å¤çæ´é¿çåºååæ´å¤æ ·åçæ°æ®éï¼ä»¥å¢å¼ºå¶å¨å¤æä¸çå»ºæ¨¡ä¸­çæ³åè½åã
*   <strong>å¤æ¨¡æéæï¼</strong> å°é³é¢ãè¯­è¨åå¨ä½ä¿¡å·éæå°æ¨¡åä¸­ï¼ä»¥æå»ºå·æç©ºé´åºç¡åå¯æ§rolloutçå®å¨å¤æ¨¡æä¸çæ¨¡åã</p>
<hr />
<p>è¿ç¯è®ºæä¸ºèªåå½å¤è§è§æ©æ£æ¨¡åå¨3Dæ°é¢è§è§åæé¢åå¼è¾äºæ°éå¾ï¼éè¿åæ°çæ¶æåè®­ç»ç­ç¥ï¼ææè§£å³äºç°ææ¹æ³çå±éæ§ï¼ä½¿å¶å¨çµæ´»æ§ãæçåçæè´¨éæ¹é¢ååå¾äºæ¾èè¿æ­¥ï¼ä¸ºæªæ¥ä¸çå»ºæ¨¡åäº¤äºå¼åºç¨å¥ å®äºåºç¡ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Multi-view diffusion models have shown promise in 3D novel view synthesis,
but most existing methods adopt a non-autoregressive formulation.</li>
<li>To address these limitations, we propose CausNVS, a multi-view
diffusion model in an autoregressive setting, which supports arbitrary
input-output view configurations and generates views sequentially.</li>
<li>Our experiments demonstrate that CausNVS
supports a broad range of camera trajectories, enables flexible autoregressive
novel view synthesis, and achieves consistently strong visual quality across
diverse settings.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.06579v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.06579v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.06499v1'></a></p>
<h2 id="tide-achieving-balanced-subject-driven-image-generation-via-target-instructed-diffusion-enhancement"><a href="https://arxiv.org/abs/2509.06499v1">TIDE: Achieving Balanced Subject-Driven Image Generation via Target-Instructed Diffusion Enhancement</a></h2>
<p><strong>Authors:</strong> Jibai Lin, Bo Ma, Yating Yang, Rong Ma, Turghun Osman, Ahtamjan Ahmat, Rui Dong, Lei Wang, Xi Zhou</p>
<p><strong>Published:</strong> 2025-09-08</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Subject-driven image generation (SDIG) aims to manipulate specific subjects
within images while adhering to textual instructions, a task crucial for
advancing text-to-image diffusion models. SDIG requires reconciling the tension
between maintaining subject identity and complying with dynamic edit
instructions, a challenge inadequately addressed by existing methods. In this
paper, we introduce the Target-Instructed Diffusion Enhancing (TIDE) framework,
which resolves this tension through target supervision and preference learning
without test-time fine-tuning. TIDE pioneers target-supervised triplet
alignment, modelling subject adaptation dynamics using a (reference image,
instruction, target images) triplet. This approach leverages the Direct Subject
Diffusion (DSD) objective, training the model with paired "winning" (balanced
preservation-compliance) and "losing" (distorted) targets, systematically
generated and evaluated via quantitative metrics. This enables implicit reward
modelling for optimal preservation-compliance balance. Experimental results on
standard benchmarks demonstrate TIDE's superior performance in generating
subject-faithful outputs while maintaining instruction compliance,
outperforming baseline methods across multiple quantitative metrics. TIDE's
versatility is further evidenced by its successful application to diverse
tasks, including structural-conditioned generation, image-to-image generation,
and text-image interpolation. Our code is available at
https://github.com/KomJay520/TIDE.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Jibai Linç­äººæ°åçè®ºæâTIDE: Achieving Balanced Subject-Driven Image Generation via Target-Instructed Diffusion Enhancementâçå¨é¢æè¦ã</p>
<hr />
<p><strong>è®ºææè¦ï¼TIDE: éè¿ç®æ æå¯¼æ©æ£å¢å¼ºå®ç°å¹³è¡¡çä¸»ä½é©±å¨å¾åçæ</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢</strong>
ä¸»ä½é©±å¨å¾åçæï¼Subject-Driven Image Generation, SDIGï¼æ¨å¨æ ¹æ®ææ¬æä»¤å¯¹å¾åä¸­çç¹å®ä¸»ä½è¿è¡æä½ï¼åæ¶ä¿æä¸»ä½èº«ä»½ãç°ææ¹æ³å¨ä¿æä¸»ä½èº«ä»½åéµå¾ªå¨æç¼è¾æä»¤ä¹é´å­å¨åºæççç¾ï¼æªè½ååè§£å³è¿ä¸ææãå·ä½æ¥è¯´ï¼åºäºå¾®è°çæ¹æ³çºç²äºæ³åè½åï¼èåå¾®è°æ¹æ³åç¼ºä¹å¯¹ç¼è¾è§£è¦çæç¡®çç£ï¼å¯¼è´ä¸»ä½ä¿çåæä»¤éµå¾ªä¹é´çå¹³è¡¡ä¸è¶³ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®</strong>
æ¬ææåºäº<strong>ç®æ æå¯¼æ©æ£å¢å¼ºï¼Target-Instructed Diffusion Enhancing, TIDEï¼</strong>æ¡æ¶ï¼éè¿ä»¥ä¸å³é®åæ°è§£å³äºä¸è¿°é®é¢ï¼</p>
<ul>
<li><strong>ç®æ çç£ååå¥½å­¦ä¹ ï¼Target Supervision and Preference Learningï¼ï¼</strong> TIDEå¼å¥äº<strong>ç®æ çç£ä¸åç»å¯¹é½</strong>ï¼ä½¿ç¨ï¼åèå¾åãæä»¤ãç®æ å¾åï¼ä¸åç»æ¥å»ºæ¨¡ä¸»ä½éåºå¨æãè¿ä¸ä»¥å¾ä»ä½¿ç¨åèå¾åä½ä¸ºä¼ªç®æ çæ¹æ³ä¸åï¼ç®æ å¾åæç¡®ç¼ç äºæéçå±æ§ç¼è¾ã</li>
<li><strong>ç´æ¥ä¸»ä½æ©æ£ï¼Direct Subject Diffusion, DSDï¼ç®æ ï¼</strong> TIDEæåºäºä¸é¨éå¯¹SDIGçDSDç®æ ï¼å®å°æå¯¹åå¥½å­¦ä¹ ï¼æºèªDPOï¼åºç¨äºå¾åçæãæ¨¡åéè¿æå¯¹çâè·èâï¼å¹³è¡¡äºä¿çåéµå¾ªï¼åâå¤±è´¥âï¼æ­æ²ï¼ç®æ è¿è¡è®­ç»ï¼è¿äºç®æ éè¿å®éææ ç³»ç»çæåè¯ä¼°ãè¿ä½¿å¾æ¨¡åè½å¤éå¼å°å­¦ä¹ å¥å±æ¨¡åï¼ä»¥å®ç°æä½³çä¿ç-éµå¾ªå¹³è¡¡ã</li>
<li><strong>è½»éçº§å¤æ¨¡æééå¨ï¼Lightweight Multimodal Adapterï¼ï¼</strong> TIDEéè¿ä¸ä¸ªåå«å¾åæå½±æ¨¡åï¼IPMï¼åå¾åäº¤åæ³¨æåæ¨¡åï¼ICAMï¼çè½»éçº§ééå¨æ¥èåè§è§åææ¬ç¹å¾ãè¯¥ééå¨ä»æ´æ°åºç¡æ©æ£æ¨¡ååæ°çæå°é¨åï¼1.33%ï¼ï¼ä»èå¨ä¿ææ¨¡ååå§çæè½åçåæ¶ï¼å®ç°å¯¹ä¸»ä½æä½çç²¾ç¡®æ§å¶ã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹</strong>
å®éªç»æè¡¨æï¼TIDEå¨æ ååºåæµè¯ä¸è¡¨ç°åºè²ï¼å¨çæå¿ å®äºä¸»ä½çè¾åºå¹¶ä¿ææä»¤éµå¾ªæ¹é¢ä¼äºææåºçº¿æ¹æ³ï¼å¹¶å¨å¤ä¸ªå®éææ ä¸åå¾äºåè¶æ§è½ã</p>
<ul>
<li><strong>å®éä¼å¿ï¼</strong> å¨Concept101åDreamBenchåºåæµè¯ä¸­ï¼TIDEå¨CLIP-Iï¼å¾åè´¨éï¼åCLIP-Tï¼ææ¬å¯¹é½ï¼ææ ä¸ååå¾äºé¢åï¼å¹¶å¨DINOåæ°ä¸å·æç«äºåï¼æ¾ç¤ºåºå¶å¨ä¸»ä½ä¿çåæä»¤éµå¾ªæ¹é¢çåè¶å¹³è¡¡ã</li>
<li><strong>æ³åè½åï¼</strong> TIDEè½å¤å¤çæªè§è¿çåºæ¯ãç¨æå¯¹è±¡ç±»å«åæ°é¢çå§¿ææè¿°ï¼å¶DINOåæ°æ¯åºçº¿æ¨¡åé«åº6.8%ï¼çªæ¾äºå¶å¼ºå¤§çæ³åè½åã</li>
<li><strong>å¤ä»»å¡éç¨æ§ï¼</strong> TIDEçéç¨æ§éè¿å¶å¨å¤ç§ä»»å¡ä¸­çæååºç¨å¾å°è¿ä¸æ­¥è¯æï¼åæ¬ï¼<ul>
<li><strong>ç»ææ¡ä»¶çæï¼</strong> TIDEå¼å®¹ControlNetç­ç°ææ§å¶æºå¶ï¼è½å¤å¤çæ·±åº¦å¾ãæ³çº¿å¾ãäººä½å§¿æéª¨æ¶ãCannyè¾¹ç¼åèªç±å½¢å¼æ¶é¸¦ç­å¤ç§æ§å¶æ¨¡æï¼æ éé¢å¤å¾®è°ã</li>
<li><strong>å¾åå°å¾åçæï¼</strong> TIDEè½å¤æ ç¼å°å°èºæ¯å±æ§ï¼å¦ç¬è§¦ãè°è²æ¿ï¼ä»é£æ ¼åèå¾åè½¬ç§»å°åå®¹å¾åï¼åæ¶ä¿æåå§ä¸»ä½çç»æå®æ´æ§ã</li>
<li><strong>ææ¬-å¾åæå¼ï¼</strong> éè¿å¯è°åæ°Î³ï¼TIDEå®ç°äºä¸»ä½ãæè´¨åé£æ ¼å±æ§ä¹é´çå¹³æ»æå¼ï¼åæ¶ä¿æèæ¯ä¸è´æ§åä¸»ä½ä¿çåº¦ã</li>
</ul>
</li>
</ul>
<p><strong>4. è®ºæä¸­æåçå±éæ§</strong>
å°½ç®¡TIDEå®ç°äºå¯¹ä»»æå¼æ¾åä¸»ä½çé¶æ ·æ¬çæï¼ä½ä»å­å¨ä¸äºå±éæ§ï¼</p>
<ul>
<li><strong>ææ¬çè§£éå¶ï¼</strong> æ¡æ¶ç»§æ¿äºCLIPçææ¬çè§£éå¶ï¼æªè½è§£å³è¶åºå¸åtokenéå¼ççæ®µæå¤ææ®µè½é¿åº¦æä»¤ã</li>
<li><strong>å°æ°æ°æè¯­è¨æ¯æï¼</strong> ç±äºå¤è¯­è¨è®­ç»æ°æ®ä¸è¶³ï¼å¤çå°æ°æ°æè¯­è¨å¯è½ä¼å¯¼è´ä¸»ä½-æä»¤éä½ï¼å¹¶å¨éä¸»å¯¼è¯­è¨åºæ¯ä¸­äº§çå¹»è§è¾åºã</li>
</ul>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹å</strong>
æªæ¥å·¥ä½è®¡åæ´åé¢å¤çææ¯æ¥åæä¸è¿°å±éæ§ï¼åæ¶ä¿æå½åèå¼çæçä¼å¿ï¼ä¾å¦è§£å³å¤ææä»¤çè§£åå¤è¯­è¨æ¯æé®é¢ã</p>
<hr />
<p>æ»èè¨ä¹ï¼TIDEéè¿å¼å¥ç®æ çç£ä¸åç»å¯¹é½åDSDç®æ ï¼ä¸ºSDIGé¢åå¸¦æ¥äºèå¼è½¬åï¼ææå°è§£å³äºä¸»ä½ä¿çåæä»¤éµå¾ªä¹é´çæ ¸å¿çç¾ãå¶è½»éçº§ééå¨è®¾è®¡ååè¶çæ³åè½åä½¿å¶æä¸ºä¸ªæ§åå¹¿åãæè²åå®¹çæç­åæäº§ä¸çå¼ºå¤§å·¥å·ï¼ä¸ºæä»¤æç¥çæç³»ç»å¼è¾äºæ°éå¾ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this
paper, we introduce the Target-Instructed Diffusion Enhancing (TIDE) framework,
which resolves this tension through target supervision and preference learning
without test-time fine-tuning.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.06499v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.06499v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.06485v1'></a></p>
<h2 id="ws2-weakly-supervised-segmentation-using-before-after-supervision-in-waste-sorting"><a href="https://arxiv.org/abs/2509.06485v1">WS<script type="math/tex">^2</script>: Weakly Supervised Segmentation using Before-After Supervision in Waste Sorting</a></h2>
<p><strong>Authors:</strong> Andrea Marelli, Alberto Foresti, Leonardo Pesce, Giacomo Boracchi, Mario Grosso</p>
<p><strong>Published:</strong> 2025-09-08</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>In industrial quality control, to visually recognize unwanted items within a
moving heterogeneous stream, human operators are often still indispensable.
Waste-sorting stands as a significant example, where operators on multiple
conveyor belts manually remove unwanted objects to select specific materials.
To automate this recognition problem, computer vision systems offer great
potential in accurately identifying and segmenting unwanted items in such
settings. Unfortunately, considering the multitude and the variety of sorting
tasks, fully supervised approaches are not a viable option to address this
challange, as they require extensive labeling efforts. Surprisingly, weakly
supervised alternatives that leverage the implicit supervision naturally
provided by the operator in his removal action are relatively unexplored. In
this paper, we define the concept of Before-After Supervision, illustrating how
to train a segmentation network by leveraging only the visual differences
between images acquired \textit{before} and \textit{after} the operator. To
promote research in this direction, we introduce WS<script type="math/tex">^2</script> (Weakly Supervised
segmentation for Waste-Sorting), the first multiview dataset consisting of more
than 11 000 high-resolution video frames captured on top of a conveyor belt,
including "before" and "after" images. We also present a robust end-to-end
pipeline, used to benchmark several state-of-the-art weakly supervised
segmentation methods on WS<script type="math/tex">^2</script>.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Andrea Marelliç­äººæ°åçè®ºæâWS<script type="math/tex">^2</script>: Weakly Supervised Segmentation using Before-After Supervision in Waste Sortingâçå¨é¢æè¦ã</p>
<hr />
<h3 id="ws2-weakly-supervised-segmentation-using-before-after-supervision-in-waste-sorting_1">è®ºæãWS<script type="math/tex">^2</script>: Weakly Supervised Segmentation using Before-After Supervision in Waste Sortingãæè¦</h3>
<p><strong>1. è§£å³çä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³å·¥ä¸è´¨éæ§å¶ï¼ç¹å«æ¯åå¾åç±»ï¼ä¸­è¯å«ååå²ç§»å¨å¼è´¨ç©ææµä¸­âä¸éè¦çç©åâçææãä¼ ç»ä¸ï¼è¿é¡¹ä»»å¡é«åº¦ä¾èµäººå·¥æä½åãå°½ç®¡è®¡ç®æºè§è§ç³»ç»å·æèªå¨åæ½åï¼ä½ç±äºåç±»ä»»å¡çå¤æ ·æ§åå¤ææ§ï¼ä»¥åå¨çç£æ¹æ³æéçå¤§éåç´ çº§æ æ³¨å·¥ä½ï¼ä½¿å¾ç°ææ¹æ³é¾ä»¥æ¨å¹¿ååºç¨ãå æ­¤ï¼è®ºæçæ ¸å¿ç ç©¶é®é¢æ¯ï¼å¦ä½å¨ä¸ä¾èµæè´µçäººå·¥æ æ³¨çæåµä¸ï¼å©ç¨æä½åç§»é¤ç©åè¿ä¸âéå¼çç£âæ¥è®­ç»ä¸ä¸ªææçè¯­ä¹åå²ç½ç»ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
*   <strong>âBefore-Afterâçç£æ¦å¿µçæåºï¼</strong> è®ºæå¼å¥äºâBefore-Afterâçç£èå¼ï¼å©ç¨æä½åç§»é¤ä¸éè¦çç©åååå¾åä¹é´çè§è§å·®å¼æ¥è®­ç»åå²ç½ç»ãè¿ç§éå¼çç£é¿åäºæè´µçåç´ çº§æ æ³¨ã
*   <strong>WS<script type="math/tex">^2</script>æ°æ®éçåå¸ï¼</strong> ä¸ºäºä¿è¿è¯¥é¢åçç ç©¶ï¼è®ºæåå¸äºWS<script type="math/tex">^2</script>ï¼Weakly Supervised segmentation for Waste-Sortingï¼æ°æ®éãè¿æ¯é¦ä¸ªå¤è§è§æ°æ®éï¼åå«è¶è¿11,000å¸§é«åè¾¨çè§é¢å¸§ï¼æè·äºä¼ éå¸¦ä¸âæä½åâåâæä½åâçå¾åï¼ä¸é¨ç¨äºå¼±çç£åå¾åç±»åå²ä»»å¡ãè¯¥æ°æ®éå¨å·¥ä¸ç¯å¢ä¸­æ¶éï¼å¹¶åå«è§é¢åºåï¼è¿å¨ç°ææ°æ®éä¸­æ¯ç¬ä¸æ äºçã
*   <strong>ç«¯å°ç«¯å¼±çç£åå²æµç¨ï¼</strong> è®ºæè®¾è®¡äºä¸ä¸ªé²æ£çç«¯å°ç«¯æµç¨ï¼ç¨äºå©ç¨âBefore-Afterâçç£è®­ç»åå²ç½ç»ãè¯¥æµç¨åæ¬ä¸ä¸ªä¸»è¦é¶æ®µï¼
    1.  <strong>è¾å©åç±»å¨è®­ç»ï¼</strong> è®­ç»ä¸ä¸ªè¾å©åç±»å¨<script type="math/tex">K_Ã¸</script>æ¥åºåâæä½åâåâæä½åâå¾åã<script type="math/tex">K_Ã¸</script>éè¿å­¦ä¹ ä¸ä¸éè¦çç©åç¸å³çå¤å«æ§ç¹å¾æ¥å®ææ­¤ä»»å¡ï¼å ä¸ºè¿äºç©åä»åºç°å¨âæä½åâå¾åä¸­ã
    2.  <strong>ä¼ªæ©ç çæä¸ç²¾ç¼ï¼</strong> å©ç¨<script type="math/tex">K_Ã¸</script>çæçæ¾èæ§å¾ï¼SMsï¼æ¥è¯å«âæä½åâå¾åä¸­çä¸éè¦çç©ååºåï¼çæç²ç¥çäºå¼æ©ç ãä¸ºäºæé«æ©ç è´¨éï¼å¼å¥äºä¸ä¸ªä¸¤æ­¥SAMï¼Segment Anything Modelï¼å¢å¼ºæ¨¡åï¼å©ç¨SAM2çæç²¾ç»çå®ä¾æ©ç ï¼å¹¶ç»åæ¾èæ§åºåè¿è¡ç²¾ç¼ï¼ä»¥è·å¾é«ä¿çåº¦çä¼ªæ©ç ã
    3.  <strong>å¨çç£åå²æ¨¡åè®­ç»ï¼</strong> å°ç²¾ç¼åçä¼ªæ©ç ä½ä¸ºæ æ³¨ï¼è®­ç»ä¸ä¸ªå¨çç£çSegFormeråå²æ¨¡åã
*   <strong>èæ¯ç§»é¤ä¸åç±»è®­ç»ç­ç¥ï¼BRï¼ï¼</strong> ä¸ºäºè§£å³è¾å©åç±»å¨å¯è½ä¾èµèæ¯çº¿ç´¢èéç®æ å·®å¼çé®é¢ï¼è®ºææåºäºä¸ç§æ°é¢çä¸åç±»è®­ç»ç­ç¥ãéè¿è®¡ç®åæ¯/èæ¯æ©ç ï¼å°è®­ç»éæ©å±ä¸ºä¸ç±»ï¼èæ¯æ©ç çâæä½åâå¾åãèæ¯æ©ç çâæä½åâå¾åä»¥åçº¯èæ¯å¾åãè¿æå©äºåç±»å¨ä¸æ³¨äºå¯¹è±¡çº§å·®å¼ï¼æé«æ¾èæ§å¾çåç¡®æ§ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>POF-CAMçåè¶æ§è½ï¼</strong> å¨WS<script type="math/tex">^2</script>æ°æ®éä¸å¯¹å¤ç§æåè¿çå¼±çç£åå²æ¹æ³è¿è¡åºåæµè¯åï¼ç»ææ¾ç¤ºï¼å©ç¨æ¶é´ä¸è´æ§çPOF-CAMæ¹æ³æ¾èä¼äºå¶ä»æ¹æ³ãè¿å¼ºè°äºè§é¢åºåä¸­æ¶é´ä¿¡æ¯å¨å¤çæ­¤ç±»æ°æ®éæ¶çå³é®ä½ç¨ã
*   <strong>SAMç²¾ç¼çæææ§ï¼</strong> SAMå¢å¼ºæ¨¡åå¯¹ç²ç¥æ©ç çç²¾ç¼æ¾èæé«äºæææ¹æ³çæ§è½ï¼è¡¨æè§è§åºç¡æ¨¡åå¸¦æ¥çç»èæ°´å¹³å¯¹äºå¼±çç£åå²ä»»å¡è³å³éè¦ã
*   <strong>èæ¯åå·®ç¼è§£çéè¦æ§ï¼</strong> æåºçèæ¯ç§»é¤ä¸åç±»è®­ç»ç­ç¥ææç¼è§£äºèæ¯åå·®é®é¢ï¼ä½¿å¾è¾å©åç±»å¨è½å¤æ´åç¡®å°å®ä½ä¸éè¦çç©åï¼ä»èçææ´åç¡®çæ¾èæ§å¾ã
*   <strong>å¯¹å·¥ä¸åºç¨çå½±åï¼</strong> è®ºæçè´¡ç®ä¸ºå¼ååºäºæ·±åº¦å­¦ä¹ çèªå¨åæå¨åç±»åè´¨éæ§å¶æ´»å¨æä¾äºå¯æ¨å¹¿çå¼±çç£åå²è§£å³æ¹æ¡ï¼æææé«åå¾åç±»å·¥åçæçï¼éä½åä¼¤é£é©ï¼å¹¶åè½»æä½åçå·¥ä½ååã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>ç°æå¼±çç£æ¹æ³çå±éæ§ï¼</strong> å°½ç®¡SAMç²¾ç¼ææå¸®å©ï¼ä½ä»å­å¼±çç£åå²æ¹æ³æ¬èº«é¾ä»¥è¾¾å°è§è§åºç¡æ¨¡åæè½æä¾çç»èæ°´å¹³ã
*   <strong>WeakTrçæ³åè½åï¼</strong> å°½ç®¡WeakTrå¨CAMçææ¹é¢æ¯åè¿çï¼ä½å¨WS<script type="math/tex">^2</script>æ°æ®éä¸çè¡¨ç°ç¸å¯¹è¾å·®ï¼è¿è¡¨æå¶å¯¹ç¹å®ä»»å¡çæ³åè½åæéï¼å°¤å¶æ¯å¨ä¸åºäºå·ç§¯çæ¹æ³ç¸æ¯æ¶ã
*   <strong>âAfterâå¾åçæ§è½ä¸éï¼</strong> ç±äºå¼±çç£æ¹æ³è¯å«å¾åä¸­ä¸è¾å©åç±»å¨æç¸å³çåºåï¼èä¸éè¦çç©åæ¯âbeforeâç±»çå³é®åºåï¼å æ­¤å¨âafterâå¾åä¸çåå²æ©ç æ§è½è¾å·®ãè¿ç§æ§è½å·®è·åç±æ­¤äº§ççç±»å«ä¸å¹³è¡¡å¯è½å¯¼è´SegFormerç½ç»å­¦ä¹ ä¸ä¸è´ï¼æ³åè½ååæã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>è¿ä¸æ­¥æ¢ç´¢æ¶é´ä¸è´æ§ï¼</strong> é´äºPOF-CAMçæåï¼æªæ¥ç ç©¶å¯ä»¥æ´æ·±å¥å°æ¢ç´¢è§é¢åºåä¸­æ¶é´ç¸å³æ§å¨å¼±çç£åå²ä¸­çåºç¨ã
*   <strong>æ¹è¿å¼±çç£æ¹æ³ä»¥å¤çç±»å«ä¸å¹³è¡¡ï¼</strong> è§£å³âbeforeâåâafterâå¾åä¹é´æ§è½å·®è·åç±»å«ä¸å¹³è¡¡é®é¢ï¼ä»¥æé«åå²æ¨¡åçæ³åè½åã
*   <strong>ç»åæ´å¤é¢åç¥è¯ï¼</strong> æ¢ç´¢å¦ä½å°æ´å¤å³äºåå¾åç±»ä»»å¡çé¢åç¥è¯èå¥å¼±çç£æ¡æ¶ï¼ä»¥è¿ä¸æ­¥æé«æ§è½ã
*   <strong>æ©å±å°å¶ä»å·¥ä¸åºæ¯ï¼</strong> å°âBefore-Afterâçç£èå¼åæåºçæµç¨åºç¨äºå¶ä»éè¦å©ç¨æä½åéå¼çç£çå·¥ä¸è´¨éæ§å¶åæ´»å¨çè§£ä»»å¡ã</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>To
promote research in this direction, we introduce WS<script type="math/tex">^2</script> (Weakly Supervised
segmentation for Waste-Sorting), the first multiview dataset consisting of more
than 11 000 high-resolution video frames captured on top of a conveyor belt,
including "before" and "after" images.</li>
<li>We also present a robust end-to-end
pipeline, used to benchmark several state-of-the-art weakly supervised
segmentation methods on WS<script type="math/tex">^2</script>.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.06485v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.06485v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-09-09 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
