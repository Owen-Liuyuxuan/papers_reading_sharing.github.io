<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-10-02 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DEâ«¶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Birdâs-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">è®ºæéè¯»</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-Î±
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-10-01/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-10-03/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-10-02">Arxiv Computer Vision Papers - 2025-10-02</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#stitch-training-free-position-control-in-multimodal-diffusion-transformers" class="nav-link">Stitch: Training-Free Position Control in Multimodal Diffusion Transformers</a>
                </li>
                <li class="nav-item">
                    <a href="#ttt3r-3d-reconstruction-as-test-time-training" class="nav-link">TTT3R: 3D Reconstruction as Test-Time Training</a>
                </li>
                <li class="nav-item">
                    <a href="#benchmarking-egocentric-visual-inertial-slam-at-city-scale" class="nav-link">Benchmarking Egocentric Visual-Inertial SLAM at City Scale</a>
                </li>
                <li class="nav-item">
                    <a href="#learning-generalizable-shape-completion-with-sim3-equivariance" class="nav-link">Learning Generalizable Shape Completion with SIM(3) Equivariance</a>
                </li>
                <li class="nav-item">
                    <a href="#learning-to-see-before-seeing-demystifying-llm-visual-priors-from-language-pre-training" class="nav-link">Learning to See Before Seeing: Demystifying LLM Visual Priors from Language Pre-training</a>
                </li>
                <li class="nav-item">
                    <a href="#hart-human-aligned-reconstruction-transformer" class="nav-link">HART: Human Aligned Reconstruction Transformer</a>
                </li>
                <li class="nav-item">
                    <a href="#da2-depth-anything-in-any-direction" class="nav-link">DA^2: Depth Anything in Any Direction</a>
                </li>
                <li class="nav-item">
                    <a href="#video-object-segmentation-aware-audio-generation" class="nav-link">Video Object Segmentation-Aware Audio Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#stable-cinemetrics-structured-taxonomy-and-evaluation-for-professional-video-generation" class="nav-link">Stable Cinemetrics : Structured Taxonomy and Evaluation for Professional Video Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#automated-and-scalable-sem-image-analysis-of-perovskite-solar-cell-materials-via-a-deep-segmentation-framework" class="nav-link">Automated and Scalable SEM Image Analysis of Perovskite Solar Cell Materials via a Deep Segmentation Framework</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-10-02">Arxiv Computer Vision Papers - 2025-10-02</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>å¥½çï¼è¿æ¯ä¸ä»½éå¯¹2025å¹´9æ30æ¥Arxivè®¡ç®æºè§è§é¢åè®ºæçæ¯æ¥æ¥åæ§è¡æè¦ï¼æ¨å¨å¸®å©å¿ç¢çç ç©¶äººåå¿«éäºè§£ææ°è¿å±ã</p>
<hr />
<p><strong>æ¯æ¥Arxivè®¡ç®æºè§è§è®ºææ¥åæ§è¡æè¦ (2025å¹´9æ30æ¥)</strong></p>
<p><strong>1. ä¸»è¦ä¸»é¢åè¶å¿æ¦è¿°ï¼</strong></p>
<p>ä»å¤©çè®ºææ¶µçäºè®¡ç®æºè§è§é¢åçå¤ä¸ªåæ²¿æ¹åï¼ä¸»è¦è¶å¿åæ¬ï¼</p>
<ul>
<li><strong>å¤æ¨¡æä¸çææ¨¡åï¼</strong> æ©æ£æ¨¡åå¨å¾åçæãç¼è¾åæ§å¶æ¹é¢æç»­æ¼è¿ï¼å¹¶å¼å§ä¸è¯­è¨æ¨¡åæ·±åº¦èåï¼æ¢ç´¢è§è§åéªçæ¥æºã</li>
<li><strong>3Dè§è§ä¸éå»ºï¼</strong> 3Déå»ºææ¯å¨æ³åæ§ãæµè¯æ¶è®­ç»åäººä½å§¿æéå»ºæ¹é¢åå¾äºè¿å±ï¼åæ¶SLAMç³»ç»å¨åå¸åºæ¯ä¸çåºåæµè¯ä¹å¤åå³æ³¨ã</li>
<li><strong>å·èº«æºè½ä¸æç¥ï¼</strong> éå¯¹ç¬¬ä¸äººç§°è§è§ï¼Egocentricï¼çSLAMåå¤æ¨¡ææç¥ï¼å¦è§é¢å¯¹è±¡åå²ä¸é³é¢çæï¼æ¯éè¦çç ç©¶æ¹åã</li>
<li><strong>é¢ååºç¨ä¸èªå¨åï¼</strong> è®¡ç®æºè§è§ææ¯å¨ææç§å­¦ï¼å¦ééç¿å¤ªé³è½çµæ± SEMå¾ååæï¼åä¸ä¸è§é¢çæè¯ä¼°ç­å·ä½åºç¨ä¸­å±ç°åºå·¨å¤§æ½åã</li>
<li><strong>æ¨¡åæ³åä¸é²æ£æ§ï¼</strong> è®ºæå³æ³¨å¦ä½æåæ¨¡åå¨ä¸ååºæ¯ãä¸åæ°æ®åå¸ä¸çæ³åè½åï¼ä¾å¦éè¿ç­åæ§å­¦ä¹ åæµè¯æ¶è®­ç»ã</li>
</ul>
<p><strong>2. ç¹å«éè¦æåæ°çè®ºæäº®ç¹ï¼</strong></p>
<ul>
<li><strong>"Stitch: Training-Free Position Control in Multimodal Diffusion Transformers" (Jessica Bader et al.)ï¼</strong> è¿ç¯è®ºæå¨å¤æ¨¡ææ©æ£æ¨¡åä¸­å®ç°äº<strong>æ éè®­ç»çä½ç½®æ§å¶</strong>ï¼è¿æ¯ä¸ä¸ªéå¸¸å®ç¨çåæ°ãå®åè®¸ç¨æ·å¨çæè¿ç¨ä¸­ç²¾ç¡®æ§å¶å¾ååç´ çä½ç½®ï¼æå¤§å°æåäºæ©æ£æ¨¡åçå¯ç¨æ§åå¯æ§æ§ï¼å¯¹äºå¾åç¼è¾ååå®¹åä½å·æéè¦æä¹ã</li>
<li><strong>"Learning to See Before Seeing: Demystifying LLM Visual Priors from Language Pre-training" (Junlin Han et al.)ï¼</strong> è¿ç¯è®ºææ·±å¥æ¢è®¨äº<strong>å¤§åè¯­è¨æ¨¡åï¼LLMï¼å¦ä½ä»è¯­è¨é¢è®­ç»ä¸­è·å¾è§è§åéªç¥è¯</strong>ãå®æ­ç¤ºäºLLMå¨æ²¡æç´æ¥è§è§è¾å¥çæåµä¸ï¼éè¿ææ¬å­¦ä¹ å°çä¸çç¥è¯å¦ä½è½¬åä¸ºå¯¹è§è§æ¦å¿µççè§£ï¼ä¸ºå¤æ¨¡æAIçæªæ¥åå±æä¾äºæ°çè§è§åçè®ºåºç¡ã</li>
<li><strong>"DA<script type="math/tex">^2</script>: Depth Anything in Any Direction" (Haodong Li et al.)ï¼</strong> ä½ä¸º"Depth Anything"ç³»åçå»¶ç»­ï¼è¿ç¯è®ºæå¯è½å¨<strong>ä»»ææ¹åçæ·±åº¦ä¼°è®¡</strong>ä¸åå¾äºçªç ´ï¼è¿ä¸æ­¥æåäºæ·±åº¦ä¼°è®¡æ¨¡åçæ³åæ§åå®ç¨æ§ï¼å¯¹äºèªå¨é©¾é©¶ãæºå¨äººå¯¼èªç­é¢åè³å³éè¦ã</li>
</ul>
<p><strong>3. æ°å´ç ç©¶æ¹åæææ¯ï¼</strong></p>
<ul>
<li><strong>æµè¯æ¶è®­ç» (Test-Time Training, TTT) å¨3Déå»ºä¸­çåºç¨ï¼</strong> "TTT3R: 3D Reconstruction as Test-Time Training" æ¢ç´¢äºå°æµè¯æ¶è®­ç»èå¼å¼å¥3Déå»ºï¼ä»¥æé«æ¨¡åå¯¹æªç¥åºæ¯çéåºæ§ï¼è¿å¯è½æä¸ºæå3Dè§è§ç³»ç»é²æ£æ§çæ°éå¾ã</li>
<li><strong>LLMè§è§åéªçè§£å¯ä¸å©ç¨ï¼</strong> "Learning to See Before Seeing" å¼å¯äºæ·±å¥çè§£åå©ç¨LLMåé¨è§è§ç¥è¯çå¤§é¨ï¼é¢ç¤ºçæªæ¥å¤æ¨¡ææ¨¡åå¯è½éè¿æ´é´æ¥ãæ´æ½è±¡çæ¹å¼è·åè§è§çè§£è½åã</li>
<li><strong>æ©æ£æ¨¡åä¸ç²¾ç¡®æ§å¶çç»åï¼</strong> "Stitch" å±ç¤ºäºæ©æ£æ¨¡åå¨çæè´¨éåå¯æ§æ§ä¹é´åå¾æ´å¥½å¹³è¡¡çæ½åï¼æªæ¥å¯è½ä¼ææ´å¤å·¥ä½ä¸æ³¨äºæåçææ¨¡åçç²¾ç»æ§å¶è½åã</li>
<li><strong>ä¸ä¸è§é¢çæä¸è¯ä¼°çæ ååï¼</strong> "Stable Cinemetrics" è¯å¾å»ºç«ä¸ä¸è§é¢çæçç»æååç±»åè¯ä¼°ä½ç³»ï¼è¿å¯¹äºæ¨å¨é«è´¨éè§é¢çæææ¯çåå±ååºç¨è³å³éè¦ã</li>
</ul>
<p><strong>4. å»ºè®®éè¯»å¨æçè®ºæï¼</strong></p>
<p>åºäºå¶åæ°æ§ãæ½å¨å½±ååå¯¹é¢åè¶å¿çä»£è¡¨æ§ï¼æå»ºè®®ä¼åéè¯»ä»¥ä¸è®ºæï¼</p>
<ul>
<li><strong>"Stitch: Training-Free Position Control in Multimodal Diffusion Transformers" (Jessica Bader et al.)ï¼</strong> å¯¹äºå³æ³¨çææ¨¡ååå¾åç¼è¾çç ç©¶äººåï¼è¿ç¯è®ºææä¾äºå³æ¶å¯ç¨çææ¯åéè¦çæè·¯ã</li>
<li><strong>"Learning to See Before Seeing: Demystifying LLM Visual Priors from Language Pre-training" (Junlin Han et al.)ï¼</strong> å¯¹äºå¯¹å¤æ¨¡æAIãLLMä¸è§è§ç»åæå´è¶£çç ç©¶äººåï¼è¿ç¯è®ºææä¾äºæ·±å»çè§è§£åæ°çç ç©¶æ¹åã</li>
<li><strong>"DA<script type="math/tex">^2</script>: Depth Anything in Any Direction" (Haodong Li et al.)ï¼</strong> å¯¹äºä»äº3Dæç¥ãæºå¨äººåèªå¨é©¾é©¶çç ç©¶äººåï¼è¿ç¯è®ºæå¯è½ä»£è¡¨äºæ·±åº¦ä¼°è®¡é¢åçææ°è¿å±ã</li>
<li><strong>"HART: Human Aligned Reconstruction Transformer" (Xiyi Chen et al.)ï¼</strong> å¯¹äºå³æ³¨äººä½å§¿æãå½¢ç¶éå»ºåå·èº«æºè½çç ç©¶äººåï¼è¿ç¯è®ºæå¯è½æä¾äºæ°çSOTAæ¹æ³ã</li>
</ul>
<hr />
<p>è¿ä»½æè¦æ¨å¨æä¾ä¸ä¸ªé«å±æ¬¡çæ¦è§ï¼å¸®å©æ¨å¿«éè¯å«ä¸æ¨ç ç©¶å´è¶£æç¸å³çè®ºæã</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2509.26644v1">Stitch: Training-Free Position Control in Multimodal Diffusion Transformers</a></li>
<li><a href="#2509.26645v1">TTT3R: 3D Reconstruction as Test-Time Training</a></li>
<li><a href="#2509.26639v1">Benchmarking Egocentric Visual-Inertial SLAM at City Scale</a></li>
<li><a href="#2509.26631v1">Learning Generalizable Shape Completion with SIM(3) Equivariance</a></li>
<li><a href="#2509.26625v1">Learning to See Before Seeing: Demystifying LLM Visual Priors from Language Pre-training</a></li>
<li><a href="#2509.26621v1">HART: Human Aligned Reconstruction Transformer</a></li>
<li><a href="#2509.26618v1">DA<script type="math/tex">^2</script>: Depth Anything in Any Direction</a></li>
<li><a href="#2509.26604v1">Video Object Segmentation-Aware Audio Generation</a></li>
<li><a href="#2509.26555v1">Stable Cinemetrics : Structured Taxonomy and Evaluation for Professional Video Generation</a></li>
<li><a href="#2509.26548v1">Automated and Scalable SEM Image Analysis of Perovskite Solar Cell Materials via a Deep Segmentation Framework</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2509.26644v1'></a></p>
<h2 id="stitch-training-free-position-control-in-multimodal-diffusion-transformers"><a href="https://arxiv.org/abs/2509.26644v1">Stitch: Training-Free Position Control in Multimodal Diffusion Transformers</a></h2>
<p><strong>Authors:</strong> Jessica Bader, Mateusz Pach, Maria A. Bravo, Serge Belongie, Zeynep Akata</p>
<p><strong>Published:</strong> 2025-09-30</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Text-to-Image (T2I) generation models have advanced rapidly in recent years,
but accurately capturing spatial relationships like "above" or "to the right
of" poses a persistent challenge. Earlier methods improved spatial relationship
following with external position control. However, as architectures evolved to
enhance image quality, these techniques became incompatible with modern models.
We propose Stitch, a training-free method for incorporating external position
control into Multi-Modal Diffusion Transformers (MMDiT) via
automatically-generated bounding boxes. Stitch produces images that are both
spatially accurate and visually appealing by generating individual objects
within designated bounding boxes and seamlessly stitching them together. We
find that targeted attention heads capture the information necessary to isolate
and cut out individual objects mid-generation, without needing to fully
complete the image. We evaluate Stitch on PosEval, our benchmark for
position-based T2I generation. Featuring five new tasks that extend the concept
of Position beyond the basic GenEval task, PosEval demonstrates that even top
models still have significant room for improvement in position-based
generation. Tested on Qwen-Image, FLUX, and SD3.5, Stitch consistently enhances
base models, even improving FLUX by 218% on GenEval's Position task and by 206%
on PosEval. Stitch achieves state-of-the-art results with Qwen-Image on
PosEval, improving over previous models by 54%, all accomplished while
integrating position control into leading models training-free. Code is
available at https://github.com/ExplainableML/Stitch.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼è¿æ¯ä¸ç¯å³äºJessica Baderç­äººæ°åçè®ºæâStitch: Training-Free Position Control in Multimodal Diffusion Transformersâçå¨é¢æè¦ã</p>
<p><strong>è®ºææè¦ï¼Stitchï¼å¤æ¨¡ææ©æ£Transformerä¸­çåè®­ç»ä½ç½®æ§å¶</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢</strong>
å°½ç®¡è¿å¹´æ¥ææ¬å°å¾åï¼T2Iï¼çææ¨¡ååå¾äºæ¾èè¿å±ï¼ä½å¨åç¡®ææâä¸æ¹âãâå³ä¾§âç­ç©ºé´å³ç³»æ¹é¢ä»ç¶é¢ä¸´æç»­ææãä¼ ç»çå¤é¨ä½ç½®æ§å¶æ¹æ³ä¸ç°ä»£T2Iæ¨¡åï¼ç¹å«æ¯åºäºå¤æ¨¡ææ©æ£Transformer (MMDiT) æ¶æçæ¨¡åï¼ä¸å¼å®¹ï¼å¯¼è´è¿äºæ¨¡åå¨å¤çå¤æç©ºé´å¸å±æ¶è¡¨ç°ä¸ä½³ãå æ­¤ï¼è®ºææ¨å¨è§£å³å¦ä½å¨ä¸çºç²å¾åè´¨éåçæéåº¦çåæä¸ï¼ä¸ºMMDiTæ¨¡åå¼å¥ç²¾ç¡®ä¸åè®­ç»çä½ç½®æ§å¶ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®</strong>
è®ºææåºäºåä¸ºâStitchâçåè®­ç»æ¹æ³ï¼ç¨äºå¨MMDiTæ¨¡åä¸­å®ç°å¤é¨ä½ç½®æ§å¶ãå¶æ ¸å¿åæ°ç¹åæ¬ï¼</p>
<ul>
<li><strong>LLMé©±å¨çè¾¹çæ¡åè§£ä¸åºåç»å®ï¼Region Bindingï¼ï¼</strong> Stitché¦åå©ç¨å¤§åè¯­è¨æ¨¡åï¼LLMï¼å°å®æ´çææ¬æç¤ºï¼promptï¼åè§£ä¸ºå¤ä¸ªå­æç¤ºï¼sub-promptsï¼ï¼æ¯ä¸ªå­æç¤ºå¯¹åºä¸ä¸ªç±LLMçæçè¾¹çæ¡ãå¨çæçåSæ­¥ä¸­ï¼MMDiTæ¨¡åå¨è¿äºæå®è¾¹çæ¡åç¬ç«çæåä¸ªå¯¹è±¡ï¼å¹¶éè¿æ³¨æåæ©ç çº¦æï¼Region Bindingï¼ç¡®ä¿å¯¹è±¡å®å¨å¨åèªåºååçæï¼ä»èææéç¦»å¯¹è±¡ä¸å¨å´ä¸ä¸æã</li>
<li><strong>æ³¨æåå¼å¯¼çåªè£ï¼Cutoutï¼ï¼</strong> ä¸ºäºé¿åå¯¹è±¡æ¼æ¥æ¶åºç°å¯è§çç¼éï¼Stitchå¨çæè¿ç¨ä¸­ï¼Sæ­¥ä¹åï¼å©ç¨ç®æ æ³¨æåå¤´æååæ¯å¯¹è±¡çæ½å¨è¡¨ç¤ºãç ç©¶åç°ï¼ç¹å®çæ³¨æåå¤´è½å¤æè·è¶³å¤çä¿¡æ¯ï¼å¨å¾åå®å¨çæä¹åï¼ä»æ½å¨ç©ºé´ä¸­åç¦»å¹¶åªè£åºåæ¯å¯¹è±¡ï¼èæ éå¤é¨åå²æ¨¡åã</li>
<li><strong>æ ç¼æ¼æ¥ä¸å¨å±ç²¾ä¿®ï¼</strong> æååºçåæ¯å¯¹è±¡æ½å¨è¡¨ç¤ºä¸èæ¯æ½å¨è¡¨ç¤ºåå¹¶ï¼å½¢æä¸ä¸ªå¤åæ½å¨è¡¨ç¤ºãå¨å©ä½ççææ­¥éª¤ä¸­ï¼æ¨¡åå¨å®æ´æç¤ºççº¦æä¸ï¼æ éå¶å°å¯¹å¾åè¿è¡ç²¾ä¿®ï¼ä»èå°åä¸ªå¯¹è±¡èå¾æ ç¼å°æ´åä¸ºè¿è´¯ä¸è§è§å¸å¼åçå¾åã</li>
<li><strong>PosEvalåºåæµè¯ï¼</strong> è®ºæå¼å¥äºä¸ä¸ªæ°çåºåæµè¯PosEvalï¼ä½ä¸ºGenEvalçæ©å±ï¼ä¸é¨ç¨äºæ·±å¥è¯ä¼°T2Iæ¨¡åçä½ç½®çæè½åãPosEvalåå«äºä¸ªæ°ä»»å¡ï¼æ¨å¨æ¢æµT2Iæ¨¡åå¨ä½ç½®çè§£æ¹é¢çç¹å®å¤±è´¥æ¨¡å¼ï¼è¶è¶äºGenEvalä¸­åºæ¬çâä½ç½®âä»»å¡ã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹</strong>
Stitchå¨Qwen-ImageãFLUXåSD3.5ç­é¢åçMMDiTæ¨¡åä¸è¿è¡äºå¹¿æ³è¯ä¼°ï¼å¹¶åå¾äºæ¾èææï¼</p>
<ul>
<li><strong>æ¾èæåä½ç½®åç¡®æ§ï¼</strong> Stitchå¨PosEvalåºåæµè¯ä¸­æç»­æåäºåºç¡æ¨¡åçæ§è½ãä¾å¦ï¼å¨GenEvalçâä½ç½®âä»»å¡ä¸ï¼FLUXçæ§è½æåäº218%ï¼å¨PosEvalä¸ï¼FLUXçæ§è½æåäº206%ãStitchå¨Qwen-Imageä¸çPosEvalè¡¨ç°æ´æ¯è¶è¶äºç°ææ¨¡å54%ï¼è¾¾å°äºæåè¿æ°´å¹³ã</li>
<li><strong>ä¿æå¾åè´¨éåå¤æ ·æ§ï¼</strong> Stitchå¨å¢å¼ºä½ç½®æ§å¶çåæ¶ï¼å¹¶æªéä½å¾åçè§è§è´¨éåå¤æ ·æ§ãç¾å­¦è¯ååDINOv2åµå¥ç©ºé´ä¸­çå¹³åæå¯¹è·ç¦»åæè¡¨æï¼Stitchå¯¹å¾åè´¨éåå¤æ ·æ§å½±åçå¾®ã</li>
<li><strong>åè®­ç»çä¼å¿ï¼</strong> Stitchä½ä¸ºä¸ç§åè®­ç»æ¹æ³ï¼è½å¤å¿«éä¸ç»æµå°åçº§ç°æT2Iæ¨¡åçä½ç½®æ§è½ï¼ä½¿å¶ä¸æåè¿çå¾åè´¨éåçæéåº¦ç¸ç»åã</li>
<li><strong>æ­ç¤ºç°ææ¨¡åçå±éæ§ï¼</strong> PosEvalåºåæµè¯æ­ç¤ºäºå³ä½¿æ¯é¡¶çº§çT2Iæ¨¡åå¨å¤çå¤æçä½ç½®æç¤ºæ¶ä»æå¾å¤§çæ¹è¿ç©ºé´ï¼å°¤å¶æ¯å¨å¤å¯¹è±¡ãç¸å¯¹å³ç³»åå¦å®å³ç³»ç­ä»»å¡ä¸ã</li>
</ul>
<p><strong>4. è®ºæä¸­æåçå±éæ§</strong>
è®ºæä¸­å¹¶æªæç¡®æåºStitchæ¹æ³çå·ä½å±éæ§ï¼ä½å¯ä»¥ä»å¶è®¾è®¡åè¯ä¼°ä¸­æ¨æ­åºä¸äºæ½å¨çæ¹é¢ï¼</p>
<ul>
<li><strong>LLMçä¾èµæ§ï¼</strong> Stitchçæ§è½å¨å¾å¤§ç¨åº¦ä¸ä¾èµäºLLMçæè¾¹çæ¡åå­æç¤ºçåç¡®æ§ãå¦æLLMå¨çè§£å¤ææç¤ºæçæç²¾ç¡®è¾¹çæ¡æ¶åºéï¼å¯è½ä¼å½±åStitchçæç»çæè´¨éã</li>
<li><strong>æ³¨æåå¤´éæ©ï¼</strong> åªè£ï¼Cutoutï¼æºå¶ä¾èµäºéæ©è½å¤ææéç¦»åæ¯å¯¹è±¡çç¹å®æ³¨æåå¤´ãè½ç¶è®ºææä¾äºéæ©æ¹æ³ï¼ä½è¿ç§éæ©çé²æ£æ§åæ³åæ§å¯è½å¨æ´å¹¿æ³çåºæ¯ä¸­éè¦è¿ä¸æ­¥éªè¯ã</li>
<li><strong>Sæ­¥æ°çéæ©ï¼</strong> è®ºææå°Sæ­¥æ°ï¼å³å¨è§£é¤çº¦æåç¬ç«çæå¯¹è±¡çæ­¥æ°ï¼çéæ©å¯¹ä½ç½®åç¡®æ§åå¾åèååº¦æå½±åãè½ç¶è®ºæä¸ºä¸åæ¨¡åæä¾äºæä½³Så¼ï¼ä½è¿ä¸ªåæ°å¯è½éè¦éå¯¹ä¸åçä»»å¡ææ¨¡åè¿è¡è°æ´ã</li>
<li><strong>å¤æåºæ¯çæ³åï¼</strong> å°½ç®¡Stitchå¨PosEvalä¸è¡¨ç°åºè²ï¼ä½å¯¹äºæå¶å¤æãé«åº¦æ½è±¡æåå«å¤§éå¯¹è±¡çåºæ¯ï¼å¶æ§è½æ¯å¦è½ä¿æä¸è´ä»éè¿ä¸æ­¥æ¢ç´¢ã</li>
</ul>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹å</strong>
åºäºæ¬è®ºæçå·¥ä½ï¼æªæ¥çç ç©¶å¯ä»¥æ¢ç´¢ä»¥ä¸æ¹åï¼</p>
<ul>
<li><strong>æ´æºè½çLLMéæï¼</strong> æ¢ç´¢æ´åè¿çLLMéææ¹å¼ï¼ä»¥æé«è¾¹çæ¡çæåå­æç¤ºåè§£çé²æ£æ§ååç¡®æ§ï¼å°¤å¶æ¯å¨å¤çæ¨¡ç³æé«åº¦å¤æçææ¬æè¿°æ¶ã</li>
<li><strong>èªéåºæ³¨æåå¤´éæ©ï¼</strong> å¼åèªéåºæºå¶ï¼æ ¹æ®è¾å¥æç¤ºåçæè¿åº¦èªå¨éæ©æä½³æ³¨æåå¤´è¿è¡åªè£ï¼èä¸æ¯ä¾èµé¢åç¡®å®çéæ©ã</li>
<li><strong>å¨æSæ­¥æ°è°æ´ï¼</strong> ç ç©¶å¨æè°æ´Sæ­¥æ°çæ¹æ³ï¼ä½¿å¶è½å¤æ ¹æ®çæåå®¹çå¤ææ§åæ¨¡åç¶æè¿è¡ä¼åï¼ä»¥å¨ä½ç½®åç¡®æ§åå¾åèååº¦ä¹é´åå¾æ´å¥½çå¹³è¡¡ã</li>
<li><strong>æ©å±PosEvalï¼</strong> è¿ä¸æ­¥æ©å±PosEvalåºåæµè¯ï¼çº³å¥æ´å¤æ ·åãæ´å·æææ§çä½ç½®ä»»å¡ï¼ä¾å¦æ¶åä¸ç»´ç©ºé´å³ç³»ãæ¶é´åºåä½ç½®ååææ´ç²¾ç»çè¯­ä¹-ç©ºé´äº¤äºã</li>
<li><strong>ä¸å¶ä»æ§å¶æ¹æ³çç»åï¼</strong> æ¢ç´¢Stitchä¸å¶ä»åè®­ç»æè½»éçº§æ§å¶æ¹æ³çç»åï¼ä»¥å®ç°æ´å¨é¢çå¾åçææ§å¶ï¼ä¾å¦å§¿æãé£æ ¼æåç§ã</li>
<li><strong>çè®ºåæï¼</strong> å¯¹Stitchä¸­åºåç»å®ååªè£æºå¶ççè®ºåºç¡è¿è¡æ´æ·±å¥çåæï¼ä»¥æ´å¥½å°çè§£å®ä»¬å¦ä½å½±åMMDiTæ¨¡åçåé¨è¡¨ç¤ºåçæè¿ç¨ã</li>
</ul>
<p>æ»èè¨ä¹ï¼Stitchä¸ºå¤æ¨¡ææ©æ£Transformerä¸­çä½ç½®æ§å¶æä¾äºä¸ä¸ªåæ°ä¸é«æçåè®­ç»è§£å³æ¹æ¡ï¼æ¾èæåäºç°ææ¨¡åå¨å¤æç©ºé´å³ç³»ä»»å¡ä¸çè¡¨ç°ï¼å¹¶ä¸ºæªæ¥çT2Içæç ç©¶å¼è¾äºæ°çæ¹åã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose Stitch, a training-free method for incorporating external position
control into Multi-Modal Diffusion Transformers (MMDiT) via
automatically-generated bounding boxes.</li>
<li>Featuring five new tasks that extend the concept
of Position beyond the basic GenEval task, PosEval demonstrates that even top
models still have significant room for improvement in position-based
generation.</li>
<li>Stitch achieves state-of-the-art results with Qwen-Image on
PosEval, improving over previous models by 54%, all accomplished while
integrating position control into leading models training-free.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.26644v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.26644v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.26645v1'></a></p>
<h2 id="ttt3r-3d-reconstruction-as-test-time-training"><a href="https://arxiv.org/abs/2509.26645v1">TTT3R: 3D Reconstruction as Test-Time Training</a></h2>
<p><strong>Authors:</strong> Xingyu Chen, Yue Chen, Yuliang Xiu, Andreas Geiger, Anpei Chen</p>
<p><strong>Published:</strong> 2025-09-30</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Modern Recurrent Neural Networks have become a competitive architecture for
3D reconstruction due to their linear-time complexity. However, their
performance degrades significantly when applied beyond the training context
length, revealing limited length generalization. In this work, we revisit the
3D reconstruction foundation models from a Test-Time Training perspective,
framing their designs as an online learning problem. Building on this
perspective, we leverage the alignment confidence between the memory state and
incoming observations to derive a closed-form learning rate for memory updates,
to balance between retaining historical information and adapting to new
observations. This training-free intervention, termed TTT3R, substantially
improves length generalization, achieving a <script type="math/tex">2\times</script> improvement in global
pose estimation over baselines, while operating at 20 FPS with just 6 GB of GPU
memory to process thousands of images. Code available in
https://rover-xingyu.github.io/TTT3R</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Xingyu Chen, Yue Chen, Yuliang Xiu, Andreas Geiger, Anpei Chenæ°åçè®ºæâTTT3R: 3D Reconstruction as Test-Time Trainingâçå¨é¢æè¦ã</p>
<hr />
<h3 id="ttt3r-3d-reconstruction-as-test-time-training_1">è®ºææè¦ï¼TTT3R: 3D Reconstruction as Test-Time Training</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºæä¸»è¦å³æ³¨ç°ä»£å¾ªç¯ç¥ç»ç½ç»ï¼RNNï¼å¨3Déå»ºé¢åä¸­çâé¿åº¦æ³åâé®é¢ãå°½ç®¡RNNå å¶çº¿æ§æ¶é´å¤æåº¦èæä¸º3Déå»ºçç«äºæ§æ¶æï¼ä½å¨è®­ç»ä¸ä¸æé¿åº¦ä¹å¤åºç¨æ¶ï¼å¶æ§è½ä¼æ¾èä¸éï¼è¿è¡¨æå®ä»¬å¨å¤çé¿åºåæ°æ®æ¶å­å¨å±éæ§ï¼å³âéå¿é®é¢âãç°ææ¹æ³å¦CUT3Rå¨å¤çæ°ç¾å¼ å¾åçåºåæ¶ä»é¾ä»¥æ³åã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
TTT3Rçæ ¸å¿åæ°å¨äºå°3Déå»ºåºç¡æ¨¡åçè®¾è®¡éæ°å®ä¹ä¸ºâæµè¯æ¶è®­ç»âï¼Test-Time Training, TTTï¼çå¨çº¿å­¦ä¹ é®é¢ãå·ä½è´¡ç®åæ¬ï¼
*   <strong>TTTè§è§ä¸çç¶ææ´æ°ï¼</strong> å°RNNçç¶ææ´æ°è§åéæ°è§£éä¸ºä¸ç§TTTé£æ ¼çå¨çº¿å­¦ä¹ è¿ç¨ï¼å¶ä¸­ç¶æè¢«è§ä¸ºå¨æµè¯æ¶éè¿æ¢¯åº¦ä¸éå­¦ä¹ çâå¿«éæéâï¼èéè®­ç»æ°æ®éä¸­çâæ¢éæéâã
*   <strong>ç½®ä¿¡åº¦å¼å¯¼çå­¦ä¹ çï¼</strong> è®ºæå©ç¨è®°å¿ç¶æä¸ä¼ å¥è§æµå¼ä¹é´çå¯¹é½ç½®ä¿¡åº¦ï¼æ¨å¯¼åºä¸ä¸ªé­å¼å­¦ä¹ çï¼<script type="math/tex">\beta_t</script>ï¼ç¨äºè®°å¿æ´æ°ãè¿ä¸ªå­¦ä¹ çè½å¤å¹³è¡¡ä¿çåå²ä¿¡æ¯åéåºæ°è§æµå¼ï¼ä»èææç¼è§£ç¾é¾æ§éå¿ã
*   <strong>è®­ç»æ å³çå¹²é¢ï¼</strong> TTT3Ræ¯ä¸ç§âè®­ç»æ å³ãå³æå³ç¨âçå¹²é¢æªæ½ï¼æ éå¯¹ç°ææ¨¡åè¿è¡å¾®è°ææ·»å é¢å¤åæ°ï¼å³å¯ç´æ¥åºç¨äºä¸æ¸¸ä»»å¡ã
*   <strong>åé¨ç½®ä¿¡åº¦ä¿¡å·çå©ç¨ï¼</strong> éè¿å©ç¨è·¨æ³¨æåç»è®¡æ°æ®æ¥ä¼°è®¡ç¶ææ´æ°çå¯¹é½ç½®ä¿¡åº¦ï¼å¹¶ç¸åºå°åéæ¯tokençå­¦ä¹ çï¼TTT3Rè½å¤éæ©æ§å°æå¶ä½è´¨éçç¶ææ´æ°ï¼ä»èæé«æ§è½ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>æ¾èçé¿åº¦æ³åæ¹è¿ï¼</strong> TTT3Rå¨é¿åºåè¾å¥ä¸æ¾èæé«äºé¿åº¦æ³åè½åï¼å¨å¨å±å§¿æä¼°è®¡æ¹é¢æ¯åºçº¿æ¹æ³å®ç°äº2åçæ¹è¿ã
*   <strong>é«æçæ§è½ï¼</strong> è¯¥æ¹æ³å¨å¤çæ°åå¼ å¾åæ¶ï¼ä»è½ä»¥20 FPSçéåº¦è¿è¡ï¼å¹¶ä¸ä»é6 GBçGPUåå­ï¼ä¿æäºä¸åºçº¿CUT3Rç¸åçæ¨çéåº¦ååå­æçã
*   <strong>ç«äºæ§è¡¨ç°ï¼</strong> å¨æ å3Déå»ºåºåæµè¯ä¸­ï¼TTT3Rä¸æåè¿çå¨çº¿éå»ºæ¨¡åï¼å¦CUT3RãPoint3Rï¼è¡¨ç°åºç«äºæ§ï¼å¹¶å¨é¿åºåè¾å¥ä¸å±ç°åºæ¾èä¼å¿ã
*   <strong>å®æ§ç»æï¼</strong> TTT3Rå®ç°äºæ´åç¡®çéå»ºï¼ç¼è§£äºéå¿é®é¢ï¼å¹¶æ¯æå¨çº¿é­ç¯ï¼é¿åäºCUT3Rä¸­åºç°çç¸æºå§¿ææ¼ç§»ãå ä½ç»ææååé¬¼å½±ä¼ªå½±ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>æªå®å¨è§£å³ç¶æéå¿ï¼</strong> å°½ç®¡TTT3Rç¼è§£äºç¶æéå¿é®é¢ï¼ä½å¹¶æªå®å¨è§£å³ã
*   <strong>ä¸ç¦»çº¿æ¹æ³çå·®è·ï¼</strong> TTT3Rçéå»ºç²¾åº¦å°æªè¾¾å°å¼ºå¤§çç¦»çº¿æ¹æ³ï¼å¦VGGTï¼çæ°´å¹³ãç¦»çº¿æ¹æ³è½ç¶éåº¦è¾æ¢ä¸åå­éæ±æ´é«ï¼ä½éè¿å®å¨æ³¨æåæºå¶ä¿çäºå®æ´çåå²ä¸ä¸æã
*   <strong>è®¾è®¡ç©ºé´å°å¾æ¢ç´¢ï¼</strong> ä½ä¸ºä¸ç§æµè¯æ¶åå½æ¹æ³ï¼TTT3Rçå³èè®°å¿è®¾è®¡ç©ºé´ä»æå¾å¹¿æ³æ¢ç´¢ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>å¼åæ´ææãç¨³å®åå¯å¹¶è¡åçå¾ªç¯æ¶æï¼</strong> è®ºææåºï¼TTT3Rçè®¾è®¡ç©ºé´ä»æªååæ¢ç´¢ï¼æªæ¥çç ç©¶å¯ä»¥è´åäºå¼åæ´ææãç¨³å®åå¯å¹¶è¡åçå¾ªç¯æ¶æï¼ä»¥è¿ä¸æ­¥æé«éå»ºç²¾åº¦åé¿åº¦æ³åè½åã
*   <strong>éæ°å®¡è§3Déå»ºæ¨¡åçåºç¡ï¼</strong> è®ºæå¸æå¶åç°è½æ¿å±æªæ¥çç ç©¶éæ°å®¡è§3Déå»ºæ¨¡åçåºç¡ï¼ä»¥å®ç°è¿ä¸æ­¥çæ§è½æåã</p>
<hr />
<p>æ»èè¨ä¹ï¼TTT3Réè¿å°3Déå»ºè§ä¸ºæµè¯æ¶è®­ç»é®é¢ï¼å¹¶å¼å¥ä¸ç§ç½®ä¿¡åº¦å¼å¯¼çé­å¼å­¦ä¹ çæ¥æ´æ°è®°å¿ç¶æï¼æåè§£å³äºRNNå¨é¿åºå3Déå»ºä¸­çé¿åº¦æ³ååéå¿é®é¢ãè¿ç§è®­ç»æ å³çå¹²é¢æªæ½å¨ä¿æé«æççåæ¶ï¼æ¾èæåäºéå»ºè´¨éåå§¿æä¼°è®¡ç²¾åº¦ï¼ä¸ºæªæ¥å¯æ©å±çå¨çº¿3Déå»ºç³»ç»æä¾äºæ°çç ç©¶æ¹åã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Building on this
perspective, we leverage the alignment confidence between the memory state and
incoming observations to derive a closed-form learning rate for memory updates,
to balance between retaining historical information and adapting to new
observations.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.26645v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.26645v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.26639v1'></a></p>
<h2 id="benchmarking-egocentric-visual-inertial-slam-at-city-scale"><a href="https://arxiv.org/abs/2509.26639v1">Benchmarking Egocentric Visual-Inertial SLAM at City Scale</a></h2>
<p><strong>Authors:</strong> Anusha Krishnan, Shaohui Liu, Paul-Edouard Sarlin, Oscar Gentilhomme, David Caruso, Maurizio Monge, Richard Newcombe, Jakob Engel, Marc Pollefeys</p>
<p><strong>Published:</strong> 2025-09-30</p>
<p><strong>Categories:</strong> cs.CV, cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>Precise 6-DoF simultaneous localization and mapping (SLAM) from onboard
sensors is critical for wearable devices capturing egocentric data, which
exhibits specific challenges, such as a wider diversity of motions and
viewpoints, prevalent dynamic visual content, or long sessions affected by
time-varying sensor calibration. While recent progress on SLAM has been swift,
academic research is still driven by benchmarks that do not reflect these
challenges or do not offer sufficiently accurate ground truth poses. In this
paper, we introduce a new dataset and benchmark for visual-inertial SLAM with
egocentric, multi-modal data. We record hours and kilometers of trajectories
through a city center with glasses-like devices equipped with various sensors.
We leverage surveying tools to obtain control points as indirect pose
annotations that are metric, centimeter-accurate, and available at city scale.
This makes it possible to evaluate extreme trajectories that involve walking at
night or traveling in a vehicle. We show that state-of-the-art systems
developed by academia are not robust to these challenges and we identify
components that are responsible for this. In addition, we design tracks with
different levels of difficulty to ease in-depth analysis and evaluation of less
mature approaches. The dataset and benchmark are available at
https://www.lamaria.ethz.ch.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Anusha Krishnanç­äººæ°åçè®ºæâBenchmarking Egocentric Visual-Inertial SLAM at City Scaleâçå¨é¢æè¦ã</p>
<hr />
<h3 id="benchmarking-egocentric-visual-inertial-slam-at-city-scale_1">è®ºææè¦ï¼Benchmarking Egocentric Visual-Inertial SLAM at City Scale</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³ç°æè§è§æ¯æ§SLAMï¼VI-SLAMï¼åºåæ°æ®éæªè½åååæ ä»¥èªæä¸ºä¸­å¿ï¼egocentricï¼æ°æ®æå¸¦æ¥çç¬ç¹ææçé®é¢ãè¿äºææåæ¬ï¼è¿å¨åè§è§å¤æ ·æ§æ´å¤§ãå¨æè§è§åå®¹æ®éå­å¨ãé¿æ¶é´ä¼è¯ä¸­ä¼ æå¨æ ¡åéæ¶é´ååï¼ä»¥åç¼ºä¹è¶³å¤ç²¾ç¡®ççå¼å§¿æãç°æçå­¦æ¯ç ç©¶å¾å¾ä¾èµäºå¨åæ§ç¯å¢åè¿å¨ä¸æ¶éçæ°æ®ï¼è¿ä½¿å¾æåè¿çVI-SLAMç³»ç»å¨å¤ççå®ä¸çãä»¥èªæä¸ºä¸­å¿çæ°æ®æ¶è¡¨ç°ä¸ä½³ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
*   <strong>LaMAriaæ°æ®éï¼</strong> å¼å¥äºä¸ä¸ªæ°çãå¤§è§æ¨¡ãä»¥èªæä¸ºä¸­å¿çè§è§æ¯æ§SLAMæ°æ®éââLaMAriaãè¯¥æ°æ®éä½¿ç¨Project Ariaç¼éå¼è®¾å¤å¨èé»ä¸å¸ä¸­å¿è®°å½äºæ°å°æ¶ãæ°åå¬éçè½¨è¿¹ï¼æ¶µçäºä½åç§ãæåååãç§»å¨å¹³å°ï¼å¦çµè½¦ãç¼è½¦ï¼ãæ¶é´ååæ ¡ååå¨æç¯å¢ç­å¤ç§ææã
*   <strong>åç±³çº§çå¼å§¿æï¼</strong> å©ç¨æµéå·¥å·ï¼åæ¬GNSS-RTKåå¨ç«ä»ªï¼è·åç¨çæ§å¶ç¹ï¼CPsï¼ï¼è¿äºCPså·æåº¦éãåç±³çº§çç²¾åº¦ï¼å¹¶åå¸å¨åå¸è§æ¨¡çåºåãéè¿å°è¿äºCPsä¸è®¾å¤è§æµå°çAprilTagæ è®°ç¸ç»åï¼è®ºæè½å¤è®¡ç®åºé«ç²¾åº¦çç¨ççå¼å§¿æã
*   <strong>ä¼ªå¯éçå¼å§¿æçæï¼</strong> ä¸ºäºè¿è¡æ´ç»ç²åº¦çè¯ä¼°å3Déå»ºç­ä»»å¡ï¼è®ºæéè¿èåè§è§ãæ¯æ§ä¼ æå¨æ°æ®åç¨çCPä¿¡æ¯ï¼å¹¶è¿è¡èåä¼åï¼çæäºä¼ªå¯éçå¼å§¿æãè¿ç§æ¹æ³å¨ä¿è¯è¶³å¤ç²¾åº¦çåæ¶ï¼åæäºä¼ ç»çå¼è·åæ¹æ³çå±éæ§ã
*   <strong>åçº§é¾åº¦åºåï¼</strong> è®¾è®¡äºä¸åé¾åº¦çº§å«çå®éªè½¨è¿¹ï¼ä»åæ§çå¹³å°è¿å¨å°å·ææææ§çä¸åéå¶çå¤´æ´å¼è¿å¨ï¼ä»¥ä¾¿äºæ·±å¥åæåè¯ä¼°ä¸åæçåº¦çæ¹æ³ã
*   <strong>è¯ä¼°æ¡æ¶ï¼</strong> æåºäºåºäºCPè¯¯å·®çè¯åå½æ°ï¼è½å¤å¯é å°è¯ä¼°SLAMç³»ç»å¨ä¸åææä¸çæ§è½ï¼å¹¶æ¥åäºCPå¯¹é½è¯¯å·®åè®¾å¤å§¿æè¯¯å·®çå¬åçã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
*   <strong>ç°æç³»ç»é²æ£æ§ä¸è¶³ï¼</strong> è¯ä¼°ç»æè¡¨æï¼ç°ææåè¿çå­¦æ¯VI-SLAMç³»ç»å¨é¢å¯¹LaMAriaæ°æ®éä¸­çç¬ç¹æææ¶ï¼å¦ä½åç§ãå¨æç¯å¢ãç§»å¨å¹³å°ãå¿«éè¿å¨ç­ï¼ï¼è¡¨ç°åºæ¾èçé²æ£æ§ä¸è¶³ï¼ç»å¸¸åºç°æ¼ç§»æè·è¸ªå¤±è´¥ã
*   <strong>å¤ä¼ æå¨èåçä¼å¿ï¼</strong> å®éªè¯å®ï¼ä¾èµå¤æåå¤´åæ¯æ§ä¼ æå¨çç³»ç»æ¾èä¼äºåç®æåç®æ¯æ§ç³»ç»ã
*   <strong>å¨çº¿æ ¡åçéè¦æ§ï¼</strong> ç ç©¶åç°ï¼å¨çº¿ä¼åéæ¶é´ååçä¼ æå¨æ ¡åå¯¹äºæé«å§¿æç²¾åº¦è³å³éè¦ï¼è¿æ¯Project Ariaçåä¸SLAMç³»ç»ä¼äºå¤§å¤æ°å­¦æ¯åºçº¿çä¸ä¸ªå³é®å ç´ ã
*   <strong>æ°æ®éæªé¥±åï¼</strong> å³ä½¿å¯¹äºé«åº¦å·¥ç¨åçProject Ariaåä¸SLAMç³»ç»ï¼æ°æ®éä¹æªè¾¾å°é¥±åï¼å°¤å¶æ¯å¨æ¶åç§»å¨å¹³å°çåºåä¸­ï¼è¿è¡¨æLaMAriaæ°æ®éä¸ºæªæ¥VI-SLAMç ç©¶æä¾äºå¹¿éçç©ºé´ã
*   <strong>æ­ç¤ºç ç©¶æ¹åï¼</strong> è®ºæéè¿è¯ä¼°ç»æï¼æç¡®æåºäºæªæ¥ä»¥èªæä¸ºä¸­å¿çSLAMç ç©¶çå ä¸ªæåæ¯çæ¹åï¼ä¾å¦å¨çº¿æ ¡åãåç¯æ£æµãé²æ£çå¼å¸¸å¼åé¤ååºäºæºå¨å­¦ä¹ çå¾åå¹éã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>ä¼ªå¯éçå¼å§¿æçç²¾åº¦ï¼</strong> å°½ç®¡è®ºæçæçä¼ªå¯éçå¼å§¿æå¯¹äºè¯ä¼°å³é®å¸§è¯¯å·®è¶³å¤åç¡®ï¼ä½å¶ç²¾åº¦ä¸å¦æµéçº§çç¨çæ§å¶ç¹ï¼å¨æäºæåµä¸ï¼å¦ç§»å¨å¹³å°ï¼çç²¾åº¦ä¿è¯æéã
*   <strong>RGBä¼ æå¨æªç¨äºè¯ä¼°ï¼</strong> ç±äºRGBä¼ æå¨éç¨å·å¸å¿«é¨ï¼å¨æ¬æçè¯ä¼°ä¸­æªè¢«ä½¿ç¨ï¼ä½å¶æ°æ®ä½ä¸ºä¸ç§æ¨¡æåå«å¨æ°æ®éä¸­ã
*   <strong>SLAMç¸æºéå æéï¼</strong> Project Ariaè®¾å¤çä¸¤ä¸ªSLAMç¸æºä½äºç¼éä¸¤ä¾§ï¼éå åºåä¸è¶³ä»¥æ¯ææ°´å¹³ç«ä½è®¾ç½®ï¼è¿éå¶äºå¶å¨æ åç«ä½æ¨¡å¼ä¸çè¯ä¼°ã
*   <strong>é¨åç³»ç»æ æ³å¤çé¿åºåï¼</strong> æäºè®¡ç®éå¤§çæ¹æ³ï¼å¦SfMæ¹æ³ï¼æ æ³å¾å¥½å°æ©å±å°é¿åºåï¼å æ­¤ä»å¨ç­çæ®µä¸è¿è¡è¯ä¼°ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>å¨çº¿æ¶é´ååæ ¡åä¼åï¼</strong> å¼åè½å¤éåºå¯ç©¿æ´è®¾å¤å¨å¤©åä½¿ç¨ç¹æ§çå¨çº¿æ ¡åä¼åæ¹æ³ã
*   <strong>åç¯æ£æµåVIæç»è°æ´ï¼</strong> æ¹è¿åç¯æ£æµåè§è§æ¯æ§æç»è°æ´ï¼ä»¥åå°å¼ç¯é¢æµä¸­çéç¨è®¡æ¼ç§»ã
*   <strong>é²æ£çå¼å¸¸å¼åé¤åè·è¸ªä¸¢å¤±å¤çï¼</strong> éå¯¹ç§»å¨å¹³å°ç­åºæ¯ï¼è®¾è®¡æ´é²æ£çå¼å¸¸å¼åé¤ç­ç¥åæ´å¥½çè·è¸ªä¸¢å¤±å¤çæºå¶ã
*   <strong>åºäºæºå¨å­¦ä¹ çå¾åå¹éåç¹è·è¸ªï¼</strong> å©ç¨å¨å¤§è§æ¨¡æ°æ®éä¸è®­ç»çæºå¨å­¦ä¹ æ¨¡åï¼å¼åæ´åè¿çå¾åå¹éåç¹è·è¸ªææ¯ã</p>
<hr />
<p>è¿ç¯è®ºæéè¿å¼å¥ä¸ä¸ªå·ææææ§ä¸é«ç²¾åº¦çå¼çæ°æ°æ®éï¼ä¸ºä»¥èªæä¸ºä¸­å¿çè§è§æ¯æ§SLAMé¢åè®¾å®äºä¸ä¸ªæ°çåºåãå®ä¸ä»æ­ç¤ºäºç°ææ¹æ³çå±éæ§ï¼è¿ä¸ºæªæ¥ç ç©¶ææäºæ¹åï¼å¯¹äºæ¨å¨å¯ç©¿æ´è®¾å¤ä¸çSLAMææ¯åå±å·æéè¦æä¹ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this
paper, we introduce a new dataset and benchmark for visual-inertial SLAM with
egocentric, multi-modal data.</li>
<li>We show that state-of-the-art systems
developed by academia are not robust to these challenges and we identify
components that are responsible for this.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.26639v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.26639v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.26631v1'></a></p>
<h2 id="learning-generalizable-shape-completion-with-sim3-equivariance"><a href="https://arxiv.org/abs/2509.26631v1">Learning Generalizable Shape Completion with SIM(3) Equivariance</a></h2>
<p><strong>Authors:</strong> Yuqing Wang, Zhaiyu Chen, Xiao Xiang Zhu</p>
<p><strong>Published:</strong> 2025-09-30</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>3D shape completion methods typically assume scans are pre-aligned to a
canonical frame. This leaks pose and scale cues that networks may exploit to
memorize absolute positions rather than inferring intrinsic geometry. When such
alignment is absent in real data, performance collapses. We argue that robust
generalization demands architectural equivariance to the similarity group,
SIM(3), so the model remains agnostic to pose and scale. Following this
principle, we introduce the first SIM(3)-equivariant shape completion network,
whose modular layers successively canonicalize features, reason over
similarity-invariant geometry, and restore the original frame. Under a
de-biased evaluation protocol that removes the hidden cues, our model
outperforms both equivariant and augmentation baselines on the PCN benchmark.
It also sets new cross-domain records on real driving and indoor scans,
lowering minimal matching distance on KITTI by 17% and Chamfer distance <script type="math/tex">\ell1</script>
on OmniObject3D by 14%. Perhaps surprisingly, ours under the stricter protocol
still outperforms competitors under their biased settings. These results
establish full SIM(3) equivariance as an effective route to truly generalizable
shape completion. Project page: https://sime-completion.github.io.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Yuqing Wang, Zhaiyu Chen, Xiao Xiang Zhuæ°åçè®ºæâLearning Generalizable Shape Completion with SIM(3) Equivarianceâçå¨é¢æè¦ã</p>
<hr />
<h3 id="learning-generalizable-shape-completion-with-sim3-equivariance_1">è®ºææè¦ï¼Learning Generalizable Shape Completion with SIM(3) Equivariance</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
å½å3Då½¢ç¶è¡¥å¨æ¹æ³æ®éå­å¨ä¸ä¸ªæ ¸å¿é®é¢ï¼å®ä»¬éå¸¸åè®¾è¾å¥æ«æå·²é¢åå¯¹é½å°è§èåæ ç³»ãè¿ç§åæ³å¯¼è´ç½ç»å¾åäºè®°å¿ç»å¯¹ä½ç½®åå°ºåº¦ä¿¡æ¯ï¼èéå­¦ä¹ å½¢ç¶çåå¨å ä½ç¹æ§ãå½å¨å®éæ°æ®ä¸­ç¼ºä¹è¿ç§é¢å¯¹é½æ¶ï¼æ¨¡åçæ§è½ä¼æ¥å§ä¸éï¼è¿ä¸¥ééå¶äºå¶æ³åè½åãè®ºææ¨å¨è§£å³å¦ä½å®ç°çæ­£å¯æ³åçå½¢ç¶è¡¥å¨ï¼ä½¿å¶å¯¹ä»»æå§¿æåå°ºåº¦ååï¼å³SIM(3)åæ¢ï¼ä¿æä¸åæ§ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
*   <strong>é®é¢è¯å«ä¸SIM(3)ç­åæ§éæ±ï¼</strong> è®ºææç¡®æåºç°ææ¹æ³ä¸­å§¿æåå°ºåº¦åå·®çé®é¢ï¼å¹¶é¦æ¬¡æåºSIM(3)ç­åæ§æ¯å®ç°éå¤æ³åï¼in-the-wild generalizationï¼çå³é®åå³æ¡ä»¶ã
*   <strong>é¦ä¸ªSIM(3)ç­åå½¢ç¶è¡¥å¨ç½ç»ï¼</strong> è®ºæå¼å¥äºç¬¬ä¸ä¸ªå®å¨SIM(3)ç­åå½¢ç¶è¡¥å¨ç½ç»ãè¯¥ç½ç»éç¨æ¨¡ååè®¾è®¡ï¼åå«ä¸ä¸ªæ ¸å¿é¶æ®µï¼
    *   <strong>ç¹å¾è§èåï¼Canonicalizationï¼ï¼</strong> å°ç¹å¾è½¬æ¢ä¸ºå¹³ç§»åå°ºåº¦ä¸åçå½¢å¼ã
    *   <strong>ç¸ä¼¼æ§ä¸åå ä½æ¨çï¼Shape Reasoningï¼ï¼</strong> å¨è§èåç¹å¾ä¸è¿è¡å ä½æ¨çï¼ç¡®ä¿å¯¹å½¢ç¶åå¨å ä½çå­¦ä¹ ã
    *   <strong>åæ¢æ¢å¤ï¼Transform Restorationï¼ï¼</strong> éæ­¥å°åå§çå§¿æåå°ºåº¦ä¿¡æ¯éæ°æ³¨å¥ï¼ä»¥å¨ä¼ æå¨åæ ç³»ä¸­æ¢å¤å®æ´çå½¢ç¶ã
*   <strong>å»åç½®è¯ä¼°åè®®ï¼</strong> è®ºæå»ºç«äºä¸ä¸ªä¸¥æ ¼çè¯ä¼°åè®®ï¼è¯¥åè®®ç§»é¤äºä¼ ç»åºåæµè¯ä¸­å­å¨çéèå§¿æåå°ºåº¦çº¿ç´¢ï¼ä»èå¯¹æ¨¡åççå®æ³åè½åè¿è¡å¬å¹³è¯ä¼°ã</p>
<p><strong>3. ä¸»è¦ç»æåå¶éè¦æ§ï¼</strong>
*   <strong>è¶è¶åºçº¿ï¼</strong> å¨å»åç½®è¯ä¼°åè®®ä¸ï¼è¯¥æ¨¡åå¨PCNåºåæµè¯ä¸æ¾èä¼äºç°æçç­ååæ°æ®å¢å¼ºåºçº¿ã
*   <strong>è·¨é¢åæ³åè½åï¼</strong> å¨çå®é©¾é©¶ï¼KITTIï¼åå®¤åï¼OmniObject3Dï¼æ«ææ°æ®ä¸ï¼è¯¥æ¨¡ååé äºæ°çè·¨é¢åè®°å½ï¼å°KITTIä¸çæå°å¹éè·ç¦»ï¼MMDï¼éä½äº17%ï¼å¹¶å°OmniObject3Dä¸çChamferè·ç¦»<script type="math/tex">\ell1</script>éä½äº14%ã
*   <strong>ä¸¥æ ¼åè®®ä¸çåè¶è¡¨ç°ï¼</strong> ä»¤äººæè®¶çæ¯ï¼å³ä½¿å¨æ´ä¸¥æ ¼çå»åç½®åè®®ä¸ï¼è¯¥æ¨¡åä»è½è¶è¶å¨åç½®è®¾ç½®ä¸è®­ç»çç«äºå¯¹æã
*   <strong>SIM(3)ç­åæ§çæææ§ï¼</strong> è¿äºç»ææåå°è¯æäºå®å¨SIM(3)ç­åæ§æ¯å®ç°çæ­£å¯æ³åå½¢ç¶è¡¥å¨çææéå¾ã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>å§¿æåå°ºåº¦ä¾èµç¹å¾çæ½å¨ä¸¢å¤±ï¼</strong> éè¿è®¾è®¡ç§»é¤å¯¹ç»å¯¹å§¿æåå°ºåº¦çä¾èµï¼è½ç¶å¢å¼ºäºé²æ£æ§ï¼ä½ä¹å¯è½å¨ç©ä½å§ç»åºç°å¨è§èåæ ç³»ä¸­æ¶ï¼ä¸¢å¼ä¸äºæç¨ççº¿ç´¢ãä¾å¦ï¼æ²¡æå¯è§è¿çæ¤å­èé¢å¯è½è¢«è¯¯è®¤ä¸ºæ¯æ²åã
*   <strong>é¨åè§æµçå¯¹ç§°æ§ï¼</strong> æ¡æ¶ä¸­çç­åæ§æ¯éå¯¹åä¸ªé¨åæ«æå®ä¹çãå¯¹äºåä¸ç©ä½çä¸åé¨åè§æµï¼åå§ååå¼æ§æ æ³å®å¨æ¶é¤ï¼å æ­¤è·¨è§å¾å¯¹ç§°æ§å¿é¡»éè¿æ°æ®éå¼å­¦ä¹ ã
*   <strong>é°æ¥å¤æåºæ¯ï¼</strong> è¯¥æ¹æ³æé¿è¡¥å¨ä»»æç¸ä¼¼åæ¢ä¸çå½¢ç¶ï¼ä½æªæç¡®èèç¬ç«ç§»å¨çå­é¨ä»¶ï¼å¦äººä½å³èãæºæ¢°èæå¤ç©ä½åºæ¯ï¼ã
*   <strong>è®¡ç®å¼éï¼</strong> åéå¼ç¹å¾åå®å¨ç­åæ¨¡åç¸æ¯æ éå¼å±ä¼å¸¦æ¥çº¦ä¸åçè®¡ç®å¼éï¼å¯¼è´è¿è¡æ¶å»¶è¿é«äºéç­ååºçº¿ï¼è¿å¯è½éå¶å¶å¨å®æ¶æèµæºåéé¨ç½²ä¸­çåºç¨ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   å°è¯¥æ¡æ¶æ©å±å°å¤ç©ä½åå¤§è§æ¨¡åºæ¯å»ºæ¨¡ã
*   ç»åç±»å«ç¹å®çå½¢ç¶åéªæåè®¸ä½¿ç¨å¤ä¸ªå±é¨åæ¢ï¼ä»¥å¤çæ´å¤æçé°æ¥åºæ¯ã</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>Following this
principle, we introduce the first SIM(3)-equivariant shape completion network,
whose modular layers successively canonicalize features, reason over
similarity-invariant geometry, and restore the original frame.</li>
<li>Under a
de-biased evaluation protocol that removes the hidden cues, our model
outperforms both equivariant and augmentation baselines on the PCN benchmark.</li>
<li>It also sets new cross-domain records on real driving and indoor scans,
lowering minimal matching distance on KITTI by 17% and Chamfer distance <script type="math/tex">\ell1</script>
on OmniObject3D by 14%.</li>
<li>Perhaps surprisingly, ours under the stricter protocol
still outperforms competitors under their biased settings.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.26631v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.26631v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.26625v1'></a></p>
<h2 id="learning-to-see-before-seeing-demystifying-llm-visual-priors-from-language-pre-training"><a href="https://arxiv.org/abs/2509.26625v1">Learning to See Before Seeing: Demystifying LLM Visual Priors from Language Pre-training</a></h2>
<p><strong>Authors:</strong> Junlin Han, Shengbang Tong, David Fan, Yufan Ren, Koustuv Sinha, Philip Torr, Filippos Kokkinos</p>
<p><strong>Published:</strong> 2025-09-30</p>
<p><strong>Categories:</strong> cs.LG, cs.AI, cs.CV, cs.MM</p>
<p><strong>Abstract:</strong></p>
<p>Large Language Models (LLMs), despite being trained on text alone,
surprisingly develop rich visual priors. These priors allow latent visual
capabilities to be unlocked for vision tasks with a relatively small amount of
multimodal data, and in some cases, to perform visual tasks without ever having
seen an image. Through systematic analysis, we reveal that visual priors-the
implicit, emergent knowledge about the visual world acquired during language
pre-training-are composed of separable perception and reasoning priors with
unique scaling trends and origins. We show that an LLM's latent visual
reasoning ability is predominantly developed by pre-training on
reasoning-centric data (e.g., code, math, academia) and scales progressively.
This reasoning prior acquired from language pre-training is transferable and
universally applicable to visual reasoning. In contrast, a perception prior
emerges more diffusely from broad corpora, and perception ability is more
sensitive to the vision encoder and visual instruction tuning data. In
parallel, text describing the visual world proves crucial, though its
performance impact saturates rapidly. Leveraging these insights, we propose a
data-centric recipe for pre-training vision-aware LLMs and verify it in 1T
token scale pre-training. Our findings are grounded in over 100 controlled
experiments consuming 500,000 GPU-hours, spanning the full MLLM construction
pipeline-from LLM pre-training to visual alignment and supervised multimodal
fine-tuning-across five model scales, a wide range of data categories and
mixtures, and multiple adaptation setups. Along with our main findings, we
propose and investigate several hypotheses, and introduce the Multi-Level
Existence Bench (MLE-Bench). Together, this work provides a new way of
deliberately cultivating visual priors from language pre-training, paving the
way for the next generation of multimodal LLMs.</p>
<p><strong>Analysis:</strong></p>
<p>è¿ç¯è®ºæãLearning to See Before Seeing: Demystifying LLM Visual Priors from Language Pre-trainingãæ·±å¥æ¢è®¨äºå¤§åè¯­è¨æ¨¡åï¼LLMsï¼å¦ä½å¨ä»éè¿ææ¬è®­ç»çæåµä¸ï¼åå±åºä¸°å¯çè§è§åéªç¥è¯ã</p>
<p>ä»¥ä¸æ¯è®ºæçå¨é¢æè¦ï¼</p>
<ol>
<li>
<p><strong>ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
    è¯¥ç ç©¶æ¨å¨ç³»ç»å°æ­ç¤ºLLMså¨ä»éè¿ææ¬é¢è®­ç»åï¼å¦ä½ä»¥åä¸ºä½è½å¤åå±åºä¸°å¯çè§è§åéªç¥è¯ãå·ä½æ¥è¯´ï¼å®æ¢ç©¶äºè¿äºè§è§åéªçææï¼æ¯åä¸è½åè¿æ¯å¯åç¦»çç»ä»¶ï¼ãæ¥æºãä»¥åå¦ä½ææå©ç¨è¿äºåéªæ¥æå»ºæ´å¼ºå¤§çå¤æ¨¡æLLMsï¼MLLMsï¼ã</p>
</li>
<li>
<p><strong>å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong></p>
<ul>
<li><strong>ç³»ç»æ§è§£æè§è§åéªï¼</strong> è®ºæéè¿è¶è¿100é¡¹åæ§å®éªï¼æ¶è500,000 GPUå°æ¶ï¼ç³»ç»å°åæäºæ¨¡åè§æ¨¡ãæ°æ®è§æ¨¡ãæ°æ®æ¥æºãè§è§ä¸çåæ¨çæ°æ®æ··åæ¯ä¾ç­åéå¯¹LLMè§è§åéªçå½±åã</li>
<li><strong>åç¦»æç¥åæ¨çåéªï¼</strong> ç ç©¶åç°è§è§åéªç±å¯åç¦»çæç¥åéªåæ¨çåéªç»æï¼å¹¶æ­ç¤ºäºå®ä»¬ç¬ç¹çæ©å±è¶å¿åèµ·æºã</li>
<li><strong>æ°æ®ä¸­å¿åé¢è®­ç»ç­ç¥ï¼</strong> æåºäºä¸ç§æ°æ®ä¸­å¿åçæ¹æ³ï¼éè¿ç­ç¥æ§å°å¹³è¡¡æ¨çä¸­å¿æ°æ®åè§è§æè¿°æ§ææ¬çæ··åæ¯ä¾ï¼æ¥é¢è®­ç»å·æè§è§æè¯çLLMsã</li>
<li><strong>å¼å¥Multi-Level Existence Bench (MLE-Bench)ï¼</strong> åå»ºäºä¸ä¸ªæ°çåºåæµè¯ï¼ç¨äºç»ç²åº¦è¯ä¼°æ¨¡åçæç¥è½åï¼ç¹å«æ¯éå¯¹ä¸åå¤§å°ï¼å°ãä¸­ãå¤§ï¼ç©ä½çå­å¨æ§è¯å«ã</li>
<li><strong>ç²è§è§æä»¤å¾®è°ï¼Blind Visual Instruction Tuningï¼ï¼</strong> æåºäºä¸ç§å¨è§è§éåºåæä»ä½¿ç¨ææ¬æ°æ®è¿è¡æä»¤å¾®è°çæå·§ï¼ä»¥å¸®å©æ¨¡åæ´å¥½å°å­¦ä¹ æä»¤éµå¾ªæ ¼å¼ï¼å¹¶ä½ä¸ºæ¢æµæ¨¡åå¦ä½å©ç¨è¯­è¨âæ·å¾âè§£å³è§è§ä»»å¡çå·¥å·ã</li>
</ul>
</li>
<li>
<p><strong>ä¸»è¦ç»æåå¶æä¹ï¼</strong></p>
<ul>
<li><strong>è§è§åéªçç»æåèµ·æºï¼</strong> LLMçæ½å¨è§è§æ¨çè½åä¸»è¦éè¿æ¨çä¸­å¿æ°æ®ï¼å¦ä»£ç ãæ°å­¦ãå­¦æ¯è®ºæï¼çé¢è®­ç»éæ­¥åå±åæ©å±ï¼è¿ç§æ¨çåéªå·æå¯è¿ç§»æ§åæ®ééç¨æ§ãç¸æ¯ä¹ä¸ï¼æç¥åéªæ´å¹¿æ³å°ä»éç¨è¯­æåºä¸­åºç°ï¼ä¸å¯¹è§è§ç¼ç å¨åè§è§æä»¤å¾®è°æ°æ®æ´ææãæè¿°è§è§ä¸ççææ¬ä¹å¾å³é®ï¼ä½å¶æ§è½å½±åè¿éé¥±åã</li>
<li><strong>æ°æ®æ··åçä¼åï¼</strong> ç ç©¶åç°ï¼æå¤§åMLLMçVQAæ§è½çæä½³æ°æ®æ··åæ¯ä¸¥éååæ¨çä¸­å¿åå®¹ï¼ä½åæ¶åå«å¿è¦çè§è§ä¸çç¥è¯ãéè¿ç²¾å¿æ ¡åçæ°æ®æ··åï¼å¯ä»¥å¨ä¸æ¾èæå®³æ ¸å¿è¯­è¨è½åçæåµä¸å¹å»å¼ºå¤§çè§è§åéªã</li>
<li><strong>æ¨çåéªçæ®éæ§ï¼</strong> è§è§æ¨çåéªæ¯åºç¡æ§çãæ¨¡ææ å³çï¼æ è®ºä½¿ç¨ä½ç§è§è§ç¼ç å¨ï¼é½è½ä½¿å¤æ¨¡æç³»ç»åçã</li>
<li><strong>æç¥åéªçè§æ¨¡ä¾èµæ§ï¼</strong> æç¥åéªè¡¨ç°åºè§æ¨¡ä¾èµæ§ï¼å¶ä¼å¿å¨æç¥ä¸­å°å°ºå¯¸ç©ä½æ¶æä¸ºæ¾èï¼è¿è¡¨æå¤æ ·åçææ¬æ°æ®æå©äºæ¨¡åå­¦ä¹ ç»ç²åº¦çè§è§æ¦å¿µã</li>
<li><strong>è½åæ¥æºçåºåï¼</strong> è§è§æ¨çè½åä¸»è¦ç±è¯­è¨é¢è®­ç»è·å¾çæ¨çåéªå¡é ï¼èæç¥è½åæ´ä¾èµäºåè®­ç»é¶æ®µï¼è§è§æä»¤å¾®è°ï¼ã</li>
</ul>
</li>
<li>
<p><strong>è®ºæä¸­æåçå±éæ§ï¼</strong></p>
<ul>
<li><strong>æ¶æéå¶ï¼</strong> ç ç©¶ä¸»è¦éä¸­å¨ééå¨é£æ ¼çMLLMæ¶æï¼å¶åç°å¯è½æ æ³å®å¨æ¨å¹¿å°å¶ä»æ¹æ³ï¼å¦éç¨ç¦»æ£è§è§æ è®°åæç«¯å°ç«¯èåè®­ç»è§è§åè¯­è¨ç»ä»¶çæ¨¡åã</li>
<li><strong>å®å¨åä¼¦çé®é¢ï¼</strong> è®ºææªæ·±å¥æ¢è®¨è¿äºå­¦ä¹ å°çè§è§åéªå¯è½åå«çç¤¾ä¼åè§ãå»æ¿å°è±¡åæ½å¨æå®³åå®¹ï¼è¿éè¦è¿ä¸æ­¥çå½»åºå®¡è®¡ã</li>
<li><strong>æ¨¡æéå¶ï¼</strong> ç ç©¶ä»éäºéæå¾åé¢åï¼æªæ¢ç´¢å¨ææ¨¡æï¼å¦è§é¢çè§£ï¼çè§è§åéªã</li>
</ul>
</li>
<li>
<p><strong>æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong></p>
<ul>
<li><strong>å¶ä»MLLMæ¶æçè§è§åéªç ç©¶ï¼</strong> æ¢ç´¢ç¦»æ£è§è§æ è®°åæç«¯å°ç«¯èåè®­ç»æ¨¡åä¸­è§è§åéªçå½¢æåå©ç¨å¨æã</li>
<li><strong>è§è§åéªçå®å¨æ§åä¼¦çå®¡è®¡ï¼</strong> å¯¹LLMsä¸­å­¦ä¹ å°çè§è§åéªè¿è¡å½»åºçå¬å¹³æ§åå®å¨æ§å®¡è®¡ï¼ä»¥è¯å«åç¼è§£æ½å¨çåè§ã</li>
<li><strong>å¨ææ¨¡æçè§è§åéªï¼</strong> æ¢ç´¢ææ¬æ°æ®å¦ä½ä¿è¿è§é¢çè§£ä¸­çæ¶é´æ¨çãå¨ä½è¯å«åå æå³ç³»ç­åéªç¥è¯ã</li>
<li><strong>æ½è±¡ç»æä¸è¯­ä¹åºç¡çç¸äºä½ç¨ï¼</strong> è¿ä¸æ­¥ç ç©¶æ½è±¡ç»æåè¯­ä¹åºç¡å¨å½¢æè·¨æ¨¡æè¡¨ç¤ºä¸­çç²¾ç¡®ç¸äºä½ç¨ã</li>
</ul>
</li>
</ol>
<p>æ»èè¨ä¹ï¼è¿ç¯è®ºæéè¿ä¸¥è°¨çå®éªååæï¼ä¸ºçè§£LLMså¦ä½ä»çº¯ææ¬ä¸­âå­¦ä¼çâæä¾äºå¼åæ§çè§è§£ãå®ä¸ä»æ­ç¤ºäºè§è§åéªçåå¨ç»æåèµ·æºï¼è¿æåºäºä¸ç§æ°æ®ä¸­å¿åçé¢è®­ç»æ¹æ³ï¼ä¸ºä¸ä¸ä»£å¤æ¨¡æLLMsçåå±å¥ å®äºåå®åºç¡ï¼å¹¶ä¸ºæªæ¥çç ç©¶ææäºæ¹åã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We show that an LLM's latent visual
reasoning ability is predominantly developed by pre-training on
reasoning-centric data (e.g., code, math, academia) and scales progressively.</li>
<li>Leveraging these insights, we propose a
data-centric recipe for pre-training vision-aware LLMs and verify it in 1T
token scale pre-training.</li>
<li>Together, this work provides a new way of
deliberately cultivating visual priors from language pre-training, paving the
way for the next generation of multimodal LLMs.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.26625v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.26625v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.26621v1'></a></p>
<h2 id="hart-human-aligned-reconstruction-transformer"><a href="https://arxiv.org/abs/2509.26621v1">HART: Human Aligned Reconstruction Transformer</a></h2>
<p><strong>Authors:</strong> Xiyi Chen, Shaofei Wang, Marko Mihajlovic, Taewon Kang, Sergey Prokudin, Ming Lin</p>
<p><strong>Published:</strong> 2025-09-30</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We introduce HART, a unified framework for sparse-view human reconstruction.
Given a small set of uncalibrated RGB images of a person as input, it outputs a
watertight clothed mesh, the aligned SMPL-X body mesh, and a Gaussian-splat
representation for photorealistic novel-view rendering. Prior methods for
clothed human reconstruction either optimize parametric templates, which
overlook loose garments and human-object interactions, or train implicit
functions under simplified camera assumptions, limiting applicability in real
scenes. In contrast, HART predicts per-pixel 3D point maps, normals, and body
correspondences, and employs an occlusion-aware Poisson reconstruction to
recover complete geometry, even in self-occluded regions. These predictions
also align with a parametric SMPL-X body model, ensuring that reconstructed
geometry remains consistent with human structure while capturing loose clothing
and interactions. These human-aligned meshes initialize Gaussian splats to
further enable sparse-view rendering. While trained on only 2.3K synthetic
scans, HART achieves state-of-the-art results: Chamfer Distance improves by
18-23 percent for clothed-mesh reconstruction, PA-V2V drops by 6-27 percent for
SMPL-X estimation, LPIPS decreases by 15-27 percent for novel-view synthesis on
a wide range of datasets. These results suggest that feed-forward transformers
can serve as a scalable model for robust human reconstruction in real-world
settings. Code and models will be released.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Xiyi Chenç­äººæ°åçè®ºæâHART: Human Aligned Reconstruction Transformerâçå¨é¢æè¦ã</p>
<p><strong>è®ºææè¦ï¼HART: Human Aligned Reconstruction Transformer</strong></p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
è¯¥è®ºææ¨å¨è§£å³ç¨çè§è§ä¸äººä½ä¸ç»´éå»ºçææãç°ææ¹æ³å¨å¤çå®½æ¾è¡£ç©ãäººä¸ç©ä½äº¤äºæ¶å­å¨å±éæ§ï¼æèå¯¹ç¸æºåè®¾è¿äºç®åï¼éå¶äºå¶å¨çå®åºæ¯ä¸­çåºç¨ãå·ä½æ¥è¯´ï¼ç ç©¶é®é¢æ¯å¦ä½ä»å°éæªæ ¡åçRGBå¾åä¸­ï¼é²æ£å°éå»ºåºå®æ´ãé¼çä¸ä¸äººä½ç»æå¯¹é½ççè£ç½æ ¼ãSMPL-Xèº«ä½ç½æ ¼ï¼å¹¶æ¯æé«è´¨éçæ°è§è§æ¸²æã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
HARTï¼Human Aligned Reconstruction Transformerï¼æ¯ä¸ä¸ªç»ä¸çæ¡æ¶ï¼å¶ä¸»è¦åæ°åæ¬ï¼
*   <strong>ç»ä¸çTransformeræ¡æ¶ï¼</strong> HARTéç¨Transformeréª¨å¹²ç½ç»ï¼è½å¤åæ¶é¢æµæ¯åç´ 3Dç¹å¾ãæ³çº¿åèº«ä½å¯¹åºå³ç³»ã
*   <strong>é®æ¡æç¥æ³æ¾éå»ºï¼</strong> éå¯¹ç¹å¾æ¹æ³å¨å¤çèªé®æ¡åºåæ¶çå±éæ§ï¼HARTå¼å¥äºä¸ä¸ª3D U-Netå°å¯å¾®åæ³æ¾è¡¨é¢éå»ºï¼DPSRï¼æ¨¡åä¸­ï¼éè¿æ®å·®æ ¡æ­£ç»åæç¤ºç½æ ¼ï¼ä»èæ¢å¤å®æ´ä¸æ°´å¯ççè£å ä½ä½ã
*   <strong>äººä½å¯¹é½çå ä½ä½ï¼</strong> é¢æµç»æä¸åæ°åçSMPL-Xèº«ä½æ¨¡åå¯¹é½ï¼ç¡®ä¿éå»ºå ä½ä½ä¸äººä½ç»æä¿æä¸è´ï¼åæ¶ææå®½æ¾è¡£ç©åäº¤äºãè¿éè¿é¢æµç´§å¯åº¦åéåèº«ä½é¨ä½æ ç­¾æ¥å®ç°ï¼ç¨äºSMPL-Xåæ°ä¼°è®¡ã
*   <strong>å ä½ä½å¼å¯¼çæ°è§è§æ¸²æï¼</strong> éå»ºçäººä½ç½æ ¼ï¼çè£ç½æ ¼ï¼è¢«ç¨ä½åå§ååæ­£ååé«æ¯splattingï¼ä»¥å®ç°é¼ççæ°è§è§æ¸²æï¼æ¾èæé«äºæ¸²æè´¨éå¹¶ç¼è§£äºè¿æåã
*   <strong>æ®å·®æ³çº¿å¤´ï¼</strong> éç¨æ®å·®å­¦ä¹ ç­ç¥ï¼éè¿é¢æµç¸å¯¹äºç°æSOTAäººä½æ³çº¿ä¼°è®¡å¨çæ®å·®æ³çº¿ï¼åæäºç´æ¥é¢æµæ³çº¿å¯è½è¿äºå¹³æ»ææ¨¡ç³çé®é¢ï¼ä»èè·å¾æ´è¿è´¯åè¯¦ç»çè¡¨é¢éå»ºã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
å°½ç®¡ä»å¨2.3Kåææ«æä¸è¿è¡è®­ç»ï¼HARTå¨å¤ä¸ªåºåæµè¯ä¸­åå¾äºæåè¿çç»æï¼
*   <strong>çè£ç½æ ¼éå»ºï¼</strong> Chamfer Distanceæé«äº18-23%ã
*   <strong>SMPL-Xä¼°è®¡ï¼</strong> PA-V2Vï¼é¡¶ç¹å°é¡¶ç¹è¯¯å·®ï¼ä¸éäº6-27%ã
*   <strong>æ°è§è§åæï¼</strong> LPIPSï¼æç¥è·ç¦»ï¼å¨åç§æ°æ®éä¸éä½äº15-27%ã
è¿äºç»æè¡¨æï¼åé¦Transformerå¯ä»¥ä½ä¸ºä¸ç§å¯æ©å±æ¨¡åï¼ç¨äºå¨çå®ä¸çç¯å¢ä¸­è¿è¡é²æ£çäººä½éå»ºãå®æ§åå®éè¯ä¼°è¿ä¸æ­¥è¯æäºHARTå¨å¤çå¤ææè£åäººä¸ç©ä½äº¤äºççå®ä¸çå¾åæ¶çæ³åè½åã</p>
<p><strong>4. è®ºæä¸­æåçå±éæ§ï¼</strong>
*   <strong>ç»èæ¢å¤ï¼</strong> éå»ºç»æå¨ç²¾ç»å°ºåº¦ç»èï¼å¦ææãå¤´åï¼æ¹é¢ä»æä¸è¶³ï¼è¿å¯è½ä¸æç¤ºç½æ ¼åè¾¨çæéæå³ã
*   <strong>ç¨çè§è§åæææ§åç§ï¼</strong> å¨éå¸¸ç¨ççè§è§ï¼ä¾å¦3ä¸ªè§è§ï¼ææææ§åç§æ¡ä»¶ä¸ï¼æ¸²æè´¨éä¼æ¾èä¸éã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
*   <strong>åå±æå¤å°ºåº¦æ¶æï¼</strong> æ¢ç´¢åå±æå¤å°ºåº¦æ¶æä»¥æé«ç»èæ¢å¤è½åã
*   <strong>æ©æ£åéªï¼</strong> å©ç¨æ©æ£åéªæ¥æ¹è¿é®æ¡åºåçæ¸²æã
*   <strong>åºäºè§é¢çè®­ç»ï¼</strong> éç¨åºäºè§é¢çè®­ç»æ¹æ³ï¼ä»¥å¢å¼ºæ¶é´ä¸è´æ§å¹¶å®ç°å¯å¨ç»çéå»ºã</p>
<p>æ»èè¨ä¹ï¼HARTè®ºææåºäºä¸ç§æ°é¢ä¸ç»ä¸çTransformeræ¡æ¶ï¼éè¿ç»åé®æ¡æç¥å ä½éå»ºåäººä½å¯¹é½æºå¶ï¼æ¾èæåäºç¨çè§è§ä¸äººä½çè£ç½æ ¼éå»ºãSMPL-Xä¼°è®¡åæ°è§è§åæçæ§è½ãå¶å¨çå®åºæ¯ä¸­çé²æ£æ§åæ³åè½åï¼ä¸ºæªæ¥çäººä½ä¸ç»´éå»ºç ç©¶å¥ å®äºåå®çåºç¡ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce HART, a unified framework for sparse-view human reconstruction.</li>
<li>Given a small set of uncalibrated RGB images of a person as input, it outputs a
watertight clothed mesh, the aligned SMPL-X body mesh, and a Gaussian-splat
representation for photorealistic novel-view rendering.</li>
<li>While trained on only 2.3K synthetic
scans, HART achieves state-of-the-art results: Chamfer Distance improves by
18-23 percent for clothed-mesh reconstruction, PA-V2V drops by 6-27 percent for
SMPL-X estimation, LPIPS decreases by 15-27 percent for novel-view synthesis on
a wide range of datasets.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.26621v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.26621v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.26618v1'></a></p>
<h2 id="da2-depth-anything-in-any-direction"><a href="https://arxiv.org/abs/2509.26618v1">DA<script type="math/tex">^2</script>: Depth Anything in Any Direction</a></h2>
<p><strong>Authors:</strong> Haodong Li, Wangguangdong Zheng, Jing He, Yuhao Liu, Xin Lin, Xin Yang, Ying-Cong Chen, Chunchao Guo</p>
<p><strong>Published:</strong> 2025-09-30</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Panorama has a full FoV (360<script type="math/tex">^\circ\times</script>180<script type="math/tex">^\circ</script>), offering a more
complete visual description than perspective images. Thanks to this
characteristic, panoramic depth estimation is gaining increasing traction in 3D
vision. However, due to the scarcity of panoramic data, previous methods are
often restricted to in-domain settings, leading to poor zero-shot
generalization. Furthermore, due to the spherical distortions inherent in
panoramas, many approaches rely on perspective splitting (e.g., cubemaps),
which leads to suboptimal efficiency. To address these challenges, we propose
<script type="math/tex">\textbf{DA}</script>
<script type="math/tex">^{\textbf{2}}</script>: <script type="math/tex">\textbf{D}</script>epth <script type="math/tex">\textbf{A}</script>nything in
<script type="math/tex">\textbf{A}</script>ny <script type="math/tex">\textbf{D}</script>irection, an accurate, zero-shot generalizable, and
fully end-to-end panoramic depth estimator. Specifically, for scaling up
panoramic data, we introduce a data curation engine for generating high-quality
panoramic depth data from perspective, and create <script type="math/tex">\sim</script>543K panoramic
RGB-depth pairs, bringing the total to <script type="math/tex">\sim</script>607K. To further mitigate the
spherical distortions, we present SphereViT, which explicitly leverages
spherical coordinates to enforce the spherical geometric consistency in
panoramic image features, yielding improved performance. A comprehensive
benchmark on multiple datasets clearly demonstrates DA<script type="math/tex">^{2}</script>'s SoTA
performance, with an average 38% improvement on AbsRel over the strongest
zero-shot baseline. Surprisingly, DA<script type="math/tex">^{2}</script> even outperforms prior in-domain
methods, highlighting its superior zero-shot generalization. Moreover, as an
end-to-end solution, DA<script type="math/tex">^{2}</script> exhibits much higher efficiency over fusion-based
approaches. Both the code and the curated panoramic data will be released.
Project page: https://depth-any-in-any-dir.github.io/.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Haodong Liç­äººæ°åçè®ºæâDA<script type="math/tex">^2</script>: Depth Anything in Any Directionâçå¨é¢æè¦ã</p>
<hr />
<p><strong>è®ºææè¦ï¼DA<script type="math/tex">^2</script>: Depth Anything in Any Direction</strong></p>
<p>è¿ç¯è®ºæãDA<script type="math/tex">^2</script>: Depth Anything in Any Directionãæåºäºä¸ç§æ°é¢çãç«¯å°ç«¯çå¨æ¯æ·±åº¦ä¼°è®¡å¨ï¼æ¨å¨è§£å³ç°æå¨æ¯æ·±åº¦ä¼°è®¡æ¹æ³å¨é¶æ ·æ¬æ³åè½ååæçæ¹é¢çå±éæ§ã</p>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢</strong>
å¨æ¯å¾åï¼360Â°x180Â° FoVï¼æä¾äºæ¯éè§å¾åæ´å®æ´çè§è§æè¿°ï¼ä½¿å¶å¨3Dè§è§ä¸­è¶æ¥è¶åå³æ³¨ãç¶èï¼å¨æ¯æ·±åº¦ä¼°è®¡é¢ä¸´ä¸¤å¤§ææï¼
*   <strong>æ°æ®ç¨ç¼ºæ§å¯¼è´æ³åè½åå·®ï¼</strong> ç±äºé«è´¨éå¨æ¯æ·±åº¦æ°æ®ç¨ç¼ºï¼ç°ææ¹æ³éå¸¸å±éäºç¹å®é¢åï¼å¯¼è´é¶æ ·æ¬æ³åè½åä¸è¶³ã
*   <strong>çé¢ç¸åå¯¼è´æçä½ä¸ï¼</strong> å¨æ¯å¾ååºæççé¢ç¸åä½¿å¾è®¸å¤æ¹æ³ä¾èµäºéè§åå²ï¼å¦ç«æ¹ä½è´´å¾ï¼ï¼è¿å¯¼è´äºæ¬¡ä¼çæçåé¢å¤çæ¨¡åã
è®ºææ¨å¨å¼åä¸ä¸ªåç¡®ãé¶æ ·æ¬æ³åä¸é«æçç«¯å°ç«¯å¨æ¯æ·±åº¦ä¼°è®¡å¨ï¼è½å¤å¤çè¿äºææã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®</strong>
ä¸ºè§£å³ä¸è¿°é®é¢ï¼DA<script type="math/tex">^2</script>æåºäºä»¥ä¸å³é®åæ°ï¼
*   <strong>å¨æ¯æ°æ®ç­å±å¼æï¼Panoramic Data Curation Engineï¼ï¼</strong> éå¯¹å¨æ¯æ°æ®ç¨ç¼ºé®é¢ï¼è®ºæå¼å¥äºä¸ä¸ªæ°æ®ç­å±å¼æãè¯¥å¼æè½å¤ä»ç°æçéè§æ·±åº¦æ°æ®ä¸­çæé«è´¨éçå¨æ¯æ·±åº¦æ°æ®ãéè¿éè§å°ç­è·æ±ç¶æå½±ï¼P2Eï¼åå¨æ¯å¾åå¤ç»ï¼ä½¿ç¨FLUX-I2Pæ¨¡åï¼ï¼è¯¥å¼æå°éè§RGB-æ·±åº¦å¯¹è½¬æ¢ä¸ºå¨æ¯RGB-æ·±åº¦å¯¹ï¼å°è®­ç»æ°æ®æ»éä»çº¦63Kå¢å å°çº¦607Kï¼å¶ä¸­çº¦543Kä¸ºæ°çææ°æ®ï¼ï¼æå¤§å°æ©å±äºå¨æ¯æ·±åº¦è®­ç»æ°æ®éçè§æ¨¡åå¤æ ·æ§ã
*   <strong>SphereViTæ¨¡åæ¶æï¼</strong> ä¸ºç¼è§£çé¢ç¸åçå½±åï¼è®ºææåºäºSphereViTä½ä¸ºDA<script type="math/tex">^2</script>çä¸»è¦éª¨å¹²ç½ç»ãSphereViTéè¿æ¾å¼å©ç¨å¨æ¯å¾åççé¢åæ ï¼æ¹ä½è§åæè§ï¼ï¼å¹¶å°å¶ç¼ç ä¸ºçé¢åµå¥ï¼Spherical Embeddingï¼ï¼ç¶åéè¿äº¤åæ³¨æåæºå¶å°å¶æ³¨å¥å°å¾åç¹å¾ä¸­ãè¿ç§è®¾è®¡ä½¿å¾å¾åç¹å¾è½å¤âæç¥âå¨æ¯ççé¢å ä½ç»æï¼ä»èäº§çç¸åæç¥è¡¨ç¤ºï¼æé«äºå ä½ä¼°è®¡çåç¡®æ§ï¼ä¸æ éé¢å¤çè¾å©æ¨¡åã
*   <strong>ç»¼ååºåæµè¯ï¼</strong> è®ºææå»ºäºä¸ä¸ªå¨é¢çåºåæµè¯ï¼æ¯è¾äºé¶æ ·æ¬/é¢ååãå¨æ¯/éè§æ¹æ³ï¼ä»¥å¨é¢è¯ä¼°å¨æ¯æ·±åº¦ä¼°è®¡çæ§è½ã
*   <strong>è®­ç»æå¤±å½æ°ï¼</strong> ç»åäºè·ç¦»æå¤±ï¼Ldisï¼ä»¥ç¡®ä¿å¨å±åç¡®çè·ç¦»å¼ï¼ä»¥åæ³çº¿æå¤±ï¼Lnorï¼ä»¥ä¿è¿å±é¨å¹³æ»åéå©çè¡¨é¢ï¼å°¤å¶æ¯å¨è·ç¦»å¼ç¸ä¼¼ä½è¡¨é¢æ³çº¿å·®å¼æ¾èçåºåã</p>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹</strong>
DA<script type="math/tex">^2</script>å¨å¤ä¸ªæ°æ®éä¸çç»¼ååºåæµè¯ä¸­è¡¨ç°åºæåè¿ï¼SoTAï¼çæ§è½ï¼
*   <strong>åè¶çé¶æ ·æ¬æ³åè½åï¼</strong> DA<script type="math/tex">^2</script>å¨æå¼ºçé¶æ ·æ¬åºçº¿ä¸ï¼AbsRelææ å¹³åæé«äº38%ãä»¤äººæè®¶çæ¯ï¼DA<script type="math/tex">^2</script>çè³è¶è¶äºååçé¢ååæ¹æ³ï¼è¿å¸æ¾äºå¶åè¶çé¶æ ·æ¬æ³åè½åã
*   <strong>é«æçï¼</strong> ä½ä¸ºä¸ç§ç«¯å°ç«¯è§£å³æ¹æ¡ï¼DA<script type="math/tex">^2</script>æ¯åºäºèåçæ¹æ³ï¼å¦UniK3DåMoGev2ï¼è¡¨ç°åºæ´é«çæçï¼æ¨çéåº¦æ¾èå å¿«ï¼ä¾å¦ï¼DA<script type="math/tex">^2</script>çº¦0.3ç§ï¼MoGev2çº¦28ç§ï¼ã
*   <strong>æ°æ®è§æ¨¡æåºï¼</strong> å®éªç»ææ¸æ°å°è¡¨æï¼éçè®­ç»æ°æ®è§æ¨¡çæ©å¤§ï¼éè¿æ°æ®ç­å±å¼æå°éè§æ°æ®è½¬æ¢ä¸ºå¨æ¯æ°æ®ï¼ï¼DA<script type="math/tex">^2</script>çæ§è½ç¨³æ­¥æåï¼éªè¯äºæ°æ®ç­å±å¼æçæææ§ã
*   <strong>å ä½ä¿çåº¦ï¼</strong> SphereViTéè¿å¼å¥çé¢åµå¥ï¼æ¾èæé«äºå¨æ¯å¾åçå ä½çè§£ï¼é¿åäºä¼ ç»æ¹æ³ä¸­å¸¸è§çå¼¯æ²åæ­æ²å ä½ç»æãæ³çº¿æå¤±è¿ä¸æ­¥ç¡®ä¿äºè¡¨é¢å¹³æ»åº¦åè¿è´¯æ§ï¼åå°äºæ¨¡ç³åºåçä¼ªå½±ã</p>
<p>è¿äºç»æè¡¨æï¼éè¿å¤§è§æ¨¡å¨æ¯æ°æ®åæ¾å¼å»ºæ¨¡çé¢å ä½ï¼å¯ä»¥å®ç°é«è´¨éãé²æ£ç360Â°x180Â°å ä½ä¼°è®¡ï¼ä¸ºé«ä¿ç3Dåºæ¯åºç¨ï¼å¦æ²æµ¸å¼3Dåºæ¯åå»ºãAR/VRãæºå¨äººä»¿çãç©çä»¿çç­ï¼éºå¹³äºéè·¯ã</p>
<p><strong>4. è®ºæä¸­æå°çå±éæ§</strong>
å°½ç®¡DA<script type="math/tex">^2</script>è¡¨ç°åºè²ï¼ä½è®ºæä¹æåºäºä¸äºå±éæ§ï¼
*   <strong>åè¾¨çéå¶ï¼</strong> è®­ç»åè¾¨çï¼1024x512ï¼ä½äºæ´é«æ¸æ°åº¦æ ¼å¼ï¼å¦2Kæ4Kï¼ï¼å¯è½å¯¼è´DA<script type="math/tex">^2</script>å¶å°ä¼éæ¼ç²¾ç»ç»èï¼ä¾å¦ï¼å¾7(a)ä¸­ç¯çé¢æµè·ç¦»ä¸æ¡é¢è¡¨é¢éè¯¯å¯¹é½ï¼ã
*   <strong>GTæ·±åº¦æ°æ®ä¸å®æ´ï¼</strong> ç­å±çéè§æ°æ®ä»æä¾çé¢ç©ºé´ä¸­é¨åå¯ç¨çGTæ·±åº¦ï¼è¿å¯è½å¯¼è´å¨å¨æ¯å¾åçå·¦å³è¾¹çå¤åºç°å¯è§çæ¥ç¼ï¼å¾7(b)ï¼ã</p>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹å</strong>
è®ºææ²¡ææç¡®æåºæªæ¥çç ç©¶æ¹åï¼ä½ä»å¶å±éæ§å¯ä»¥æ¨æ­åºä»¥ä¸æ½å¨æ¹åï¼
*   <strong>æ´é«åè¾¨ççè®­ç»åæ¨çï¼</strong> æ¢ç´¢å¨æ´é«åè¾¨çä¸è®­ç»DA<script type="math/tex">^2</script>ï¼ä»¥æææ´ç²¾ç»çå ä½ç»èã
*   <strong>æ´å®åçGTæ·±åº¦çæï¼</strong> æ¹è¿æ°æ®ç­å±å¼æï¼ä»¥çææ´å®æ´ãæ´åç¡®ççé¢GTæ·±åº¦ï¼ä»èè§£å³è¾¹çæ¥ç¼é®é¢ã
*   <strong>æ´å¤æççé¢å ä½å»ºæ¨¡ï¼</strong> è¿ä¸æ­¥ç ç©¶åå¼åæ´åè¿ççé¢å ä½å»ºæ¨¡ææ¯ï¼ä»¥åºå¯¹æ´å¤æçç¸åååºæ¯ã
*   <strong>å¤æ¨¡ææ°æ®èåï¼</strong> æ¢ç´¢å°å¨æ¯æ·±åº¦ä¼°è®¡ä¸å¶ä»æ¨¡ææ°æ®ï¼å¦è¯­ä¹ä¿¡æ¯ãæ¿åé·è¾¾æ°æ®ç­ï¼èåï¼ä»¥è¿ä¸æ­¥æé«æ§è½åé²æ£æ§ã</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address these challenges, we propose
<script type="math/tex">\textbf{DA}</script>
<script type="math/tex">^{\textbf{2}}</script>: <script type="math/tex">\textbf{D}</script>epth <script type="math/tex">\textbf{A}</script>nything in
<script type="math/tex">\textbf{A}</script>ny <script type="math/tex">\textbf{D}</script>irection, an accurate, zero-shot generalizable, and
fully end-to-end panoramic depth estimator.</li>
<li>Specifically, for scaling up
panoramic data, we introduce a data curation engine for generating high-quality
panoramic depth data from perspective, and create <script type="math/tex">\sim</script>543K panoramic
RGB-depth pairs, bringing the total to <script type="math/tex">\sim</script>607K.</li>
<li>To further mitigate the
spherical distortions, we present SphereViT, which explicitly leverages
spherical coordinates to enforce the spherical geometric consistency in
panoramic image features, yielding improved performance.</li>
<li>Surprisingly, DA<script type="math/tex">^{2}</script> even outperforms prior in-domain
methods, highlighting its superior zero-shot generalization.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.26618v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.26618v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.26604v1'></a></p>
<h2 id="video-object-segmentation-aware-audio-generation"><a href="https://arxiv.org/abs/2509.26604v1">Video Object Segmentation-Aware Audio Generation</a></h2>
<p><strong>Authors:</strong> Ilpo Viertola, Vladimir Iashin, Esa Rahtu</p>
<p><strong>Published:</strong> 2025-09-30</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Existing multimodal audio generation models often lack precise user control,
which limits their applicability in professional Foley workflows. In
particular, these models focus on the entire video and do not provide precise
methods for prioritizing a specific object within a scene, generating
unnecessary background sounds, or focusing on the wrong objects. To address
this gap, we introduce the novel task of video object segmentation-aware audio
generation, which explicitly conditions sound synthesis on object-level
segmentation maps. We present SAGANet, a new multimodal generative model that
enables controllable audio generation by leveraging visual segmentation masks
along with video and textual cues. Our model provides users with fine-grained
and visually localized control over audio generation. To support this task and
further research on segmentation-aware Foley, we propose Segmented Music Solos,
a benchmark dataset of musical instrument performance videos with segmentation
information. Our method demonstrates substantial improvements over current
state-of-the-art methods and sets a new standard for controllable,
high-fidelity Foley synthesis. Code, samples, and Segmented Music Solos are
available at https://saganet.notion.site</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Ilpo Viertola, Vladimir Iashin, Esa Rahtuæ°åçè®ºæâVideo Object Segmentation-Aware Audio Generationâçå¨é¢æè¦ã</p>
<hr />
<h3 id="_1">è®ºææè¦ï¼è§é¢å¯¹è±¡åå²æç¥é³é¢çæ</h3>
<p><strong>1. ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
ç°æçå¤§å¤æ°å¤æ¨¡æé³é¢çææ¨¡åå¨ä¸ä¸æé³ï¼Foleyï¼å·¥ä½æµç¨ä¸­ç¼ºä¹ç²¾ç¡®çç¨æ·æ§å¶ãè¿äºæ¨¡åéå¸¸å³æ³¨æ´ä¸ªè§é¢ï¼æ æ³ç²¾ç¡®å°ä¼åå¤çåºæ¯ä¸­çç¹å®å¯¹è±¡ï¼å¯¼è´çæä¸å¿è¦çèæ¯å£°é³æå³æ³¨éè¯¯çå¯¹è±¡ãè¿éå¶äºå®ä»¬å¨éè¦ç²¾ç»æ§å¶çåºæ¯ä¸­çå®éåºç¨ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®ï¼</strong>
ä¸ºäºè§£å³ä¸è¿°é®é¢ï¼ä½èå¼å¥äºä¸é¡¹æ°é¢çä»»å¡ï¼<strong>è§é¢å¯¹è±¡åå²æç¥é³é¢çæï¼video object segmentation-aware audio generationï¼</strong>ï¼å®æç¡®å°å°å£°é³åææ¡ä»¶åå°å¯¹è±¡çº§å«çåå²å¾ä¸ãä¸ºæ­¤ï¼è®ºææåºäºä»¥ä¸å³é®åæ°ï¼</p>
<ul>
<li><strong>SAGANet æ¨¡åï¼</strong> æåºäºä¸ç§æ°çå¤æ¨¡æçææ¨¡åSAGANetï¼éè¿å©ç¨è§è§åå²æ©ç ãè§é¢åææ¬æç¤ºï¼å®ç°å¯æ§çé³é¢çæãè¯¥æ¨¡åä¸ºç¨æ·æä¾äºå¯¹é³é¢çæè¿ç¨çç²¾ç»åè§è§å±é¨åæ§å¶ã</li>
<li><strong>èªçç£æ§å¶æ¨¡åï¼</strong> å¨é¢è®­ç»ç½ç»çåºç¡ä¸å¼åäºä¸ä¸ªèªçç£æ§å¶æ¨¡åï¼åè®¸ç¨æ·éæ©è§é¢ä¸­çç¹å®å¯¹è±¡æ¥çæå£°é³ãè¯¥æ¨¡åéè¿ç»åå¨å±åå±é¨è§è§ç¹å¾ï¼ä»¥åé¨æ§äº¤åæ³¨æåå±ï¼å°åå²ä¿¡æ¯å¼å¥ç¹å¾æåé¶æ®µï¼ä»èå®ç°æ´å¥½çå¯æ§æ§ãæ¶é´åæ­¥æ§åè¯­ä¹è´¨éã</li>
<li><strong>Focal Prompt æºå¶ï¼</strong> å¼å¥äºFocal Promptï¼å®ç»åäºåå§æªä¿®æ¹çè§è§æµåå¶æ©ç æµï¼ä»¥åå´ç»æå´è¶£åºåè£åªçè§è§æµåå¶æ©ç ï¼ä»èæä¾äºå¨å±æ¦è§åç®æ åºåçè¯¦ç»è§å¾ã</li>
<li><strong>Segmented Music Solos æ°æ®éï¼</strong> ä¸ºäºæ¯æè¿é¡¹ä»»å¡åè¿ä¸æ­¥ç ç©¶åå²æç¥çæé³ï¼ä½èæåºäºä¸ä¸ªåºåæ°æ®éSegmented Music Solosãè¯¥æ°æ®éåå«å¸¦æåå²ä¿¡æ¯çä¹å¨æ¼å¥è§é¢ï¼æ¨å¨æä¾é«è´¨éçãå·æå£°é³å¯¹è±¡åå²å¾åé«è§å¬å¯¹åºå³ç³»çæ°æ®ãè¯¥æ°æ®ééè¿å¤é¶æ®µç®¡éï¼åæ¬æºè§é¢æ¶éãè§è§éªè¯ãå¬è§éªè¯ãè§é¢åªè¾åæ©ç çæï¼æå»ºã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹ï¼</strong>
SAGANetæ¨¡åå¨å¤é¡¹è¯ä¼°ææ ä¸åæ¾èä¼äºç°ææåè¿çæ¹æ³ï¼å¹¶å¨å¯æ§ãé«ä¿çæé³åææ¹é¢æ ç«äºæ°æ åã</p>
<ul>
<li><strong>æ§è½æåï¼</strong> SAGANetå¨åå¸å¹éï¼FrÃ©chet Distance, Kullback-Leibler Distanceï¼ãé³é¢è´¨éï¼Inception Scoreï¼ãè¯­ä¹å¯¹é½ï¼ImageBind Scoreï¼åæ¶é´å¯¹é½ï¼DeSyncï¼ç­ææææ ä¸åè¡¨ç°åºæ¾èæ¹è¿ãå°¤å¶å¨æ¶é´åæ­¥æ¹é¢ï¼æ¹è¿æä¸ºæ¾èã</li>
<li><strong>æ³åè½åï¼</strong> å°½ç®¡æ¨¡åä»ä½¿ç¨åæºæ ·æ¬è¿è¡è®­ç»ï¼ä½å®å¨å¤æºåºæ¯ä¸­è¡¨ç°åºå¼ºå¤§çæ³åè½åï¼è½å¤ææå°èç¦äºç®æ å¯¹è±¡ï¼å³ä½¿åºæ¯ä¸­å­å¨å¤ä¸ªåå£°ä¹å¨ã</li>
<li><strong>LoRAå¾®è°ï¼</strong> éè¿ä½¿ç¨ä½ç§©éåºï¼LoRAï¼å¯¹ä¸è§è§åå²æç¥ç¹å¾ç¸å³çDiTå±è¿è¡å¾®è°ï¼è¿ä¸æ­¥æåäºçææ¨¡åçæ§è½ï¼ä½¿å¶æ´å¥½å°éåºè¿äºç¹å¾ã</li>
<li><strong>æ¶èç ç©¶ï¼</strong> ç»æè¡¨æï¼èåå±é¨åå¨å±ç¹å¾å¯¹äºå®ç°åè¶çæ¶é´æ§è½åä¿æå¯æ¯çé³é¢è´¨éè³å³éè¦ãç»åå¨å±ä¸ä¸æåè¯¦ç»å±é¨ä¿¡æ¯å¯¹äºçæé«è´¨éé³é¢å·æéè¦æä¹ã</li>
</ul>
<p><strong>4. è®ºæä¸­æå°çå±éæ§ï¼</strong>
è®ºæä¸­æ²¡ææç¡®æåºå½åæ¹æ³çå·ä½å±éæ§ï¼ä½å¯ä»¥ä»å¶ç ç©¶å¨æºåæ¹æ³ä¸­æ¨æ­åºä¸äºæ½å¨çæ¹é¢ï¼</p>
<ul>
<li><strong>æ°æ®æ æ³¨ææ¬ï¼</strong> å°½ç®¡Segmented Music Solosæ°æ®éçæå»ºéç¨äºåèªå¨åæµç¨ï¼ä½æµè¯æ°æ®çæ©ç çæä»éè¦æå¨æä¾ä½ç½®åæ æ¥æç¤ºSAM2ï¼è¿è¡¨æé«è´¨éçåå²æ©ç æ æ³¨ä»ç¶å¯è½æ¯ä¸ä¸ªèæ¶ä¸èµæºå¯éçè¿ç¨ã</li>
<li><strong>æ¨¡åå¤ææ§ï¼</strong> SAGANetå»ºç«å¨MMAudioæ¨¡åä¹ä¸ï¼å¹¶å¼å¥äºé¢å¤çæ§å¶æ¨¡ååç¹å¾æåæµç¨ï¼è¿å¯è½å¢å äºæ¨¡åçæ´ä½å¤ææ§åè®¡ç®å¼éã</li>
<li><strong>æ³åèå´ï¼</strong> å°½ç®¡æ¨¡åå¨å¤æºé³ä¹åºæ¯ä¸­è¡¨ç°åºè¯å¥½çæ³åè½åï¼ä½å¶å¨æ´å¹¿æ³ãæ´å¤æçæé³åºæ¯ï¼ä¾å¦éä¹å¨å£°é³ï¼ä¸­çæ³åè½åä»æå¾è¿ä¸æ­¥éªè¯ã</li>
</ul>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong>
è®ºæä¸ºæªæ¥çç ç©¶å¥ å®äºåºç¡ï¼å¯ä»¥ä»ä»¥ä¸å ä¸ªæ¹é¢è¿è¡æ¢ç´¢ï¼</p>
<ul>
<li><strong>æ´å¹¿æ³çæé³åºæ¯ï¼</strong> å°åå²æç¥é³é¢çææ©å±å°æ´å¹¿æ³çæé³åºæ¯ï¼åæ¬æ¥å¸¸ç©ä½ãç¯å¢å£°é³ç­ï¼èä¸ä»ä»æ¯ä¹å¨æ¼å¥ã</li>
<li><strong>æ´ç²¾ç»çç¨æ·äº¤äºï¼</strong> æ¢ç´¢æ´ç´è§ãæ´ç²¾ç»çç¨æ·äº¤äºæ¹å¼ï¼ä¾å¦éè¿æå¿ãç¼å¨è¿½è¸ªææ´å¤æçèªç¶è¯­è¨æè¿°æ¥æå®ç®æ å¯¹è±¡åå£°é³å±æ§ã</li>
<li><strong>å®æ¶çæï¼</strong> ä¼åæ¨¡åä»¥å®ç°æ´æ¥è¿å®æ¶çé³é¢çæï¼ä»¥æ»¡è¶³ä¸ä¸æé³å·¥ä½æµç¨ä¸­å¯¹æççéæ±ã</li>
<li><strong>å¤æ¨¡æèåçæ·±åº¦æ¢ç´¢ï¼</strong> è¿ä¸æ­¥ç ç©¶ä¸åæ¨¡æï¼è§é¢ãææ¬ãåå²æ©ç ï¼ä¹é´æ´æ·±å±æ¬¡çèåæºå¶ï¼ä»¥æé«çæé³é¢çè¯­ä¹åç¡®æ§åæ¶é´ä¸è´æ§ã</li>
<li><strong>èªå¨åæ©ç çæï¼</strong> æ¢ç´¢æ´åè¿çèªå¨ååå²æ©ç çæææ¯ï¼ä»¥åå°å¯¹äººå·¥æ æ³¨çä¾èµï¼ä»èéä½æ°æ®éæå»ºææ¬ã</li>
</ul>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address
this gap, we introduce the novel task of video object segmentation-aware audio
generation, which explicitly conditions sound synthesis on object-level
segmentation maps.</li>
<li>We present SAGANet, a new multimodal generative model that
enables controllable audio generation by leveraging visual segmentation masks
along with video and textual cues.</li>
<li>To support this task and
further research on segmentation-aware Foley, we propose Segmented Music Solos,
a benchmark dataset of musical instrument performance videos with segmentation
information.</li>
<li>Our method demonstrates substantial improvements over current
state-of-the-art methods and sets a new standard for controllable,
high-fidelity Foley synthesis.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.26604v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.26604v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.26555v1'></a></p>
<h2 id="stable-cinemetrics-structured-taxonomy-and-evaluation-for-professional-video-generation"><a href="https://arxiv.org/abs/2509.26555v1">Stable Cinemetrics : Structured Taxonomy and Evaluation for Professional Video Generation</a></h2>
<p><strong>Authors:</strong> Agneet Chatterjee, Rahim Entezari, Maksym Zhuravinskyi, Maksim Lapin, Reshinth Adithyan, Amit Raj, Chitta Baral, Yezhou Yang, Varun Jampani</p>
<p><strong>Published:</strong> 2025-09-30</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Recent advances in video generation have enabled high-fidelity video
synthesis from user provided prompts. However, existing models and benchmarks
fail to capture the complexity and requirements of professional video
generation. Towards that goal, we introduce Stable Cinemetrics, a structured
evaluation framework that formalizes filmmaking controls into four
disentangled, hierarchical taxonomies: Setup, Event, Lighting, and Camera.
Together, these taxonomies define 76 fine-grained control nodes grounded in
industry practices. Using these taxonomies, we construct a benchmark of prompts
aligned with professional use cases and develop an automated pipeline for
prompt categorization and question generation, enabling independent evaluation
of each control dimension. We conduct a large-scale human study spanning 10+
models and 20K videos, annotated by a pool of 80+ film professionals. Our
analysis, both coarse and fine-grained reveal that even the strongest current
models exhibit significant gaps, particularly in Events and Camera-related
controls. To enable scalable evaluation, we train an automatic evaluator, a
vision-language model aligned with expert annotations that outperforms existing
zero-shot baselines. SCINE is the first approach to situate professional video
generation within the landscape of video generative models, introducing
taxonomies centered around cinematic controls and supporting them with
structured evaluation pipelines and detailed analyses to guide future research.</p>
<p><strong>Analysis:</strong></p>
<p>ä»¥ä¸æ¯Agneet Chatterjeeç­äººæ°åçè®ºæâStable Cinemetrics : Structured Taxonomy and Evaluation for Professional Video Generationâçå¨é¢æè¦ï¼</p>
<p><strong>1. è®ºæè§£å³çä¸»è¦é®é¢æç ç©¶é®é¢</strong></p>
<p>è¯¥è®ºææ¨å¨è§£å³ç°æè§é¢çææ¨¡åååºåæªè½ææä¸ä¸è§é¢çææéçå¤ææ§åè¦æ±çé®é¢ãæ ¸å¿ç ç©¶é®é¢æ¯ï¼âå½åçè§é¢çææ¨¡åæ¯å¦å·²ä¸ºä¸ä¸ç¨éåå¥½åå¤ï¼âç°ææ¨¡åä¸»è¦å³æ³¨ç¨æ·æä¾çæç¤ºçæé«ä¿çè§é¢ï¼ä½ç¼ºä¹å¯¹çµå½±å¶ä½ä¸­å³é®ççµå½±æ§å¶ï¼å¦éå¤´æå¾ãç¯åä½ç½®ãäºä»¶æ¶åºç­ï¼çç»ç²åº¦æ§å¶åè¯ä¼°ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºè´¡ç®</strong></p>
<ul>
<li><strong>Stable Cinemetrics (SCINE) æ¡æ¶ï¼</strong> å¼å¥äºä¸ä¸ªç»æåçè¯ä¼°æ¡æ¶ï¼å°çµå½±å¶ä½æ§å¶å½¢å¼åä¸ºåä¸ªè§£è¦çãåå±çåç±»æ³ï¼<strong>è®¾ç½® (Setup)ãäºä»¶ (Event)ãç¯å (Lighting) åæåæº (Camera)</strong>ãè¿äºåç±»æ³å±åå®ä¹äº76ä¸ªåºäºè¡ä¸å®è·µçç»ç²åº¦æ§å¶èç¹ã</li>
<li><strong>ä¸ä¸å¯¹é½çåºåæç¤ºï¼</strong> å©ç¨è¿äºåç±»æ³æå»ºäºä¸ä¸ªä¸ä¸ä¸ç¨ä¾å¯¹é½çæç¤ºåºåï¼åæ¬âæäºé©±å¨åâåâè§è§éè¿°åâä¸¤ç§æç¤ºç±»åï¼ä»¥æ¨¡æå®éçµå½±å¶ä½æµç¨ã</li>
<li><strong>èªå¨åæç¤ºåç±»åé®é¢çæï¼</strong> å¼åäºä¸ä¸ªèªå¨åæµç¨ï¼ç¨äºå°æç¤ºä¸­çæ¯ä¸ªæ§å¶åç´ åç±»å°ç¸åºçåç±»æ³èç¹ï¼å¹¶çæç¬ç«çè¯ä¼°é®é¢ï¼ä»èå®ç°å¯¹æ¯ä¸ªæ§å¶ç»´åº¦çç¬ç«è¯ä¼°ã</li>
<li><strong>å¤§è§æ¨¡äººå·¥è¯ä¼°ï¼</strong> è¿è¡äºä¸é¡¹å¤§è§æ¨¡çäººå·¥ç ç©¶ï¼æ¶µçäº10å¤ä¸ªæ¨¡åå2ä¸ä¸ªè§é¢ï¼ç±80å¤åçµå½±ä¸ä¸äººå£«è¿è¡æ æ³¨ï¼ç¡®ä¿äºè¯ä¼°çé«è´¨éåä¸ä¸æ§ã</li>
<li><strong>èªå¨è¯ä¼°å¨è®­ç»ï¼</strong> ä¸ºäºå®ç°å¯æ©å±çè¯ä¼°ï¼è®­ç»äºä¸ä¸ªè§è§-è¯­è¨æ¨¡å (VLM) ä½ä¸ºèªå¨è¯ä¼°å¨ï¼è¯¥æ¨¡åä¸ä¸å®¶æ æ³¨å¯¹é½ï¼å¹¶ä¸ä¼äºç°æçé¶æ ·æ¬åºçº¿ã</li>
</ul>
<p><strong>3. ä¸»è¦ç»æåå¶æä¹</strong></p>
<ul>
<li><strong>æ¨¡åæ§è½å·®è·ï¼</strong> ç²ç²åº¦åç»ç²åº¦åæåæ¾ç¤ºï¼å³ä½¿æ¯æå¼ºçå½åæ¨¡åä¹å­å¨æ¾èå·®è·ï¼å°¤å¶æ¯å¨<strong>äºä»¶ (Events)</strong> å<strong>æåæº (Camera)</strong> ç¸å³çæ§å¶æ¹é¢ã</li>
<li><strong>åç±»æ³ç»´åº¦æ§è½å·®å¼ï¼</strong> æ¨¡åå¨âè®¾ç½®âåâç¯åâæ¹é¢è¡¨ç°ç¸å¯¹è¾å¥½ï¼ä½å¨å¤çâäºä»¶âåâæåæºâæ§å¶æ¶é¢ä¸´æ´å¤§ææãä¾å¦ï¼æ¨¡åå¨å¤çåå­åå¹¶åå¨ä½æ¹é¢è¡¨ç°è¯å¥½ï¼ä½å¨å æåéå äºä»¶æ¹é¢è¡¨ç°ä¸ä½³ï¼å¨æåæºæ§å¶ä¸­ï¼æ¨¡åå¨âå¤é¨åæ°âåâè½¨è¿¹âæ¹é¢è¡¨ç°æå·®ã</li>
<li><strong>VLM è¯ä¼°å¨æææ§ï¼</strong> è®­ç»çVLMå¨ä¸äººç±»æ æ³¨å¯¹é½æ¹é¢è¡¨ç°åºä¸è´æ§ï¼ä¼äºç°æé¶æ ·æ¬åºçº¿ï¼è¯æäºå¶å¨ä¸ä¸è§é¢çæè¯ä¼°ä¸­çå¯æ©å±æ§æ½åã</li>
<li><strong>å¯¹æªæ¥ç ç©¶çæå¯¼ï¼</strong> SCINEæ¯ç¬¬ä¸ä¸ªå°ä¸ä¸è§é¢çæç½®äºè§é¢çææ¨¡åé¢åçæ¹æ³ï¼éè¿å¼å¥ä»¥çµå½±æ§å¶ä¸ºä¸­å¿çåç±»æ³ãç»æåè¯ä¼°æµç¨åè¯¦ç»åæï¼ä¸ºæªæ¥çç ç©¶æä¾äºæå¯¼ã</li>
</ul>
<p><strong>4. è®ºæä¸­æå°çå±éæ§</strong></p>
<ul>
<li><strong>åç±»æ³èå´ï¼</strong> å°½ç®¡åç±»æ³æ¯ä¸é¢åä¸å®¶ååå¼åçï¼ä½å¶èå´åéäºåä½èç½ç»çå¹¿åº¦ãçµå½±å¶ä½æ¯è¯­åè§£éæ§ç»å¾®å·®å«å å°åºåæåèå¼ï¼æ´å¹¿æ³çä¸å®¶å¤æ ·æ§å°æå©äºçº³å¥å¨ççµå½±æ§å¶ã</li>
<li><strong>åç±»æ³èç¹æ½è±¡ï¼</strong> æäºåç±»æ³èç¹ï¼å¦è²æ¸©ãISOï¼è¢«æ½è±¡åå¤çï¼å ä¸ºæ æ³¨èé¾ä»¥æç»­æç¥ç»ç²åº¦å¼ã</li>
<li><strong>æç¤ºçæåå·®ï¼</strong> æç¤ºçæä¾èµäºå¤§åè¯­è¨æ¨¡å (LLM)ï¼å¶ä¸ææ§è´¨åæ½å¨åå·®å¯è½ä¼å½±åæç¤ºçè¯­è¨åç»æã</li>
<li><strong>VLM è¯ä¼°çè®¡ç®åæ°æ®éå¶ï¼</strong> é¶æ ·æ¬VLMè¯ä¼°åéäºè®¡ç®åæ°æ®èµæºï¼éå¶äºå®éªçè§æ¨¡åèå´ã</li>
</ul>
<p><strong>5. æ½å¨çæªæ¥ç ç©¶æ¹å</strong></p>
<ul>
<li><strong>åç±»æ³æ©å±ï¼</strong> è¿ä¸æ­¥æ©å±åç±»æ³ï¼çº³å¥æ´å¹¿æ³çå¨ççµå½±æ§å¶ï¼å¹¶æ¢ç´¢æ´ç»ç²åº¦çå¼ï¼ä¾å¦ç²¾ç¡®çf-stopå¼ãè²ç¸ãäº®åº¦ãé¥±ååº¦ï¼ã</li>
<li><strong>è§é¢æ°æ®éåæï¼</strong> å©ç¨SCINEåç±»æ³åæè§é¢æ°æ®éççµå½±å¤æ ·æ§ã</li>
<li><strong>è§é¢å­å¹ï¼</strong> å°SCINEåç±»æ³ä½ä¸ºè§é¢å­å¹çç»æåæ¡æ¶ã</li>
<li><strong>æ¨¡åå¾®è°åå®å¶ï¼</strong> æ¢ç´¢å¾®è°åå®å¶è§£å³æ¹æ¡ï¼ä»¥å¼¥åå½åè§é¢çææ¨¡åä¸ä¸ä¸å¶ä½è¦æ±ä¹é´çå·®è·ï¼ä½¿æ¨¡åæ´æ¥è¿å®éçäº§ç¨éã</li>
<li><strong>VLM è¯ä¼°å¨æ¹è¿ï¼</strong> è¿ä¸æ­¥æé«VLMè¯ä¼°å¨çå¯é æ§ååç¡®æ§ï¼ç¹å«æ¯éå¯¹é£äºç®åè¡¨ç°è¾å¼±çæ§å¶èç¹ã</li>
</ul>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Towards that goal, we introduce Stable Cinemetrics, a structured
evaluation framework that formalizes filmmaking controls into four
disentangled, hierarchical taxonomies: Setup, Event, Lighting, and Camera.</li>
<li>To enable scalable evaluation, we train an automatic evaluator, a
vision-language model aligned with expert annotations that outperforms existing
zero-shot baselines.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.26555v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.26555v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.26548v1'></a></p>
<h2 id="automated-and-scalable-sem-image-analysis-of-perovskite-solar-cell-materials-via-a-deep-segmentation-framework"><a href="https://arxiv.org/abs/2509.26548v1">Automated and Scalable SEM Image Analysis of Perovskite Solar Cell Materials via a Deep Segmentation Framework</a></h2>
<p><strong>Authors:</strong> Jian Guo Pan, Lin Wang, Xia Cai</p>
<p><strong>Published:</strong> 2025-09-30</p>
<p><strong>Categories:</strong> cond-mat.mtrl-sci, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Scanning Electron Microscopy (SEM) is indispensable for characterizing the
microstructure of thin films during perovskite solar cell fabrication. Accurate
identification and quantification of lead iodide and perovskite phases are
critical because residual lead iodide strongly influences crystallization
pathways and defect formation, while the morphology of perovskite grains
governs carrier transport and device stability. Yet current SEM image analysis
is still largely manual, limiting throughput and consistency. Here, we present
an automated deep learning-based framework for SEM image segmentation that
enables precise and efficient identification of lead iodide, perovskite and
defect domains across diverse morphologies. Built upon an improved YOLOv8x
architecture, our model named PerovSegNet incorporates two novel modules: (i)
Adaptive Shuffle Dilated Convolution Block, which enhances multi-scale and
fine-grained feature extraction through group convolutions and channel mixing;
and (ii) Separable Adaptive Downsampling module, which jointly preserves
fine-scale textures and large-scale structures for more robust boundary
recognition. Trained on an augmented dataset of 10,994 SEM images, PerovSegNet
achieves a mean Average Precision of 87.25% with 265.4 Giga Floating Point
Operations, outperforming the baseline YOLOv8x-seg by 4.08%, while reducing
model size and computational load by 24.43% and 25.22%, respectively. Beyond
segmentation, the framework provides quantitative grain-level metrics, such as
lead iodide/perovskite area and count, which can serve as reliable indicators
of crystallization efficiency and microstructural quality. These capabilities
establish PerovSegNet as a scalable tool for real-time process monitoring and
data-driven optimization of perovskite thin-film fabrication.The source code is
available at:https://github.com/wlyyj/PerovSegNet/tree/master.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°ä¸ºæ¨æä¾Jian Guo Pan, Lin Wang, Xia Caiæ°åçè®ºæâAutomated and Scalable SEM Image Analysis of Perovskite Solar Cell Materials via a Deep Segmentation Frameworkâçå¨é¢æè¦ã</p>
<p><strong>è®ºææè¦ï¼</strong></p>
<ol>
<li>
<p><strong>ä¸»è¦é®é¢æç ç©¶é®é¢ï¼</strong>
    ééç¿å¤ªé³è½çµæ± ï¼PSCsï¼çå¾®è§ç»æè¡¨å¾å¯¹äºå¶æ§è½åç¨³å®æ§è³å³éè¦ï¼ç¹å«æ¯åç¡®è¯å«åéåèèä¸­çç¢åéãééç¿ç¸åç¼ºé·åºåãç¶èï¼ç®åçæ«æçµå­æ¾å¾®éï¼SEMï¼å¾ååæä¸»è¦ä¾èµäººå·¥æä½ï¼æçä½ä¸ä¸ç¼ºä¹ä¸è´æ§ãå æ­¤ï¼ç ç©¶æ¨å¨å¼åä¸ç§èªå¨åãç²¾ç¡®ä¸å¯æ©å±çSEMå¾ååå²æ¡æ¶ï¼ä»¥åæè¿äºææã</p>
</li>
<li>
<p><strong>å³é®åæ°ææ¹æ³è´¡ç®ï¼</strong>
    è¯¥è®ºææåºäºä¸ç§åä¸º <strong>PerovSegNet</strong> çæ·±åº¦å­¦ä¹ åå²æ¡æ¶ï¼å®åºäºæ¹è¿çYOLOv8xæ¶æï¼å¹¶å¼å¥äºä¸¤ä¸ªæ°é¢çæ¨¡åï¼</p>
<ul>
<li><strong>(i) èªéåºæ··æ´è¨èå·ç§¯åï¼Adaptive Shuffle Dilated Convolution Block, ASDCBï¼ï¼</strong> è¯¥æ¨¡åéè¿ç»å·ç§¯åééæ··åå¢å¼ºå¤å°ºåº¦åç»ç²åº¦ç¹å¾æåï¼ä»èæé«å¯¹æ¶çãå°é¢ç²åç¼ºé·åºåçè¾¨å«è½åã</li>
<li><strong>(ii) å¯åç¦»èªéåºä¸éæ ·æ¨¡åï¼Separable Adaptive Downsampling module, SADï¼ï¼</strong> è¯¥æ¨¡åèåä¿çç»å°ºåº¦çº¹çåå¤§å°ºåº¦ç»æï¼ä»¥å®ç°æ´é²æ£çè¾¹çè¯å«ï¼ææç¼è§£ä¼ ç»ä¸éæ ·æ¹æ³ä¸­å¸¸è§çæ··å åå¯¹å°å°ºåº¦çº¹çææåº¦æéçé®é¢ã
è¿äºæ¨¡åçç»åä½¿å¾PerovSegNetè½å¤æ´ææå°å¤çSEMå¾åä¸­å¸¸è§çå¤æèæ¯åªå£°ãæ¨¡ç³è¾¹çåä½å¯¹æ¯åº¦åºåã</li>
</ul>
</li>
<li>
<p><strong>ä¸»è¦ç»æåå¶æä¹ï¼</strong></p>
<ul>
<li><strong>æ§è½æåï¼</strong> PerovSegNetå¨åå«10,994å¼ å¢å¼ºSEMå¾åçæ°æ®éä¸è¿è¡è®­ç»ï¼å®ç°äº87.25%çå¹³åç²¾åº¦ï¼mAPï¼ï¼ä¼äºåºçº¿YOLOv8x-segæ¨¡å4.08%ã</li>
<li><strong>æçæé«ï¼</strong> æ¨¡åå°ºå¯¸åè®¡ç®è´è½½åå«åå°äº24.43%å25.22%ï¼åæ¶ä¿æäºé«ç²¾åº¦ï¼è¯æäºå¶è½»éååé«ææ§ã</li>
<li><strong>å®éææ ï¼</strong> é¤äºåç´ çº§åå²ï¼è¯¥æ¡æ¶è¿è½æä¾å®éçæ¶ç²çº§ææ ï¼å¦ç¢åé/ééç¿çé¢ç§¯åæ°éï¼è¿äºææ å¯ä½ä¸ºç»æ¶æçåå¾®è§ç»æè´¨éçå¯é æç¤ºå¨ã</li>
<li><strong>å¾®è§ç»æä¸å¨ä»¶æ§è½å³èï¼</strong> è®ºæéè¿ç¸å³æ§åæï¼Pearson rï¼å±ç¤ºäºå¾åè¡ççå¾®è§ç»ææè¿°ç¬¦ä¸å¨ä»¶PCEä¹é´çå³ç³»ï¼ä¾å¦ééç¿é¢ç§¯ä¸PCEåå¼ºæ­£ç¸å³ï¼r=0.78ï¼ï¼ç¼ºé·é¢ç§¯åç¼ºé·æ°éä¸PCEåè´ç¸å³ï¼r=-0.27år=-0.32ï¼ãè¿è¡¨æPerovSegNetè½å¤æä¾ä¸å¨ä»¶æ§è½ç´æ¥ç¸å³çç»æä¿¡æ¯ã</li>
<li><strong>å¯æ©å±æ§åå®æ¶æ§ï¼</strong> è¿äºè½åä½¿PerovSegNetæä¸ºå®æ¶è¿ç¨çæ§åæ°æ®é©±å¨ä¼åééç¿èèå¶é çå¯æ©å±å·¥å·ã</li>
</ul>
</li>
<li>
<p><strong>è®ºæä¸­æå°çå±éæ§ï¼</strong></p>
<ul>
<li><strong>ç¸å³æ§éå ææ§ï¼</strong> æ¥åçç¸å³æ§æ¯è§å¯æ§çï¼å¹¶ä¸æå³çå æå³ç³»ãå¶ä»å ç´ ï¼å¦æåæçé¢å·¥ç¨ï¼ä¹å¯è½å½±åå½¢æåPCEã</li>
<li><strong>åå²è¯¯å·®åç±»å«ä¸å¹³è¡¡ï¼</strong> åå²è¯¯å·®åç±»å«ä¸å¹³è¡¡ï¼ç¹å«æ¯ç¼ºé·ç±»å«ï¼å¯è½ä¼ä¼ æ­å°æ´¾çææ ä¸­ã</li>
<li><strong>æåæ¡ä»¶å½±åï¼</strong> æåæ¡ä»¶ï¼çµåãæ¾å¤§åæ°ãæ ¡åï¼ä¹ä¼å½±åå®éè¾åºã</li>
<li><strong>æ°æ®éè§æ¨¡åå¤æ ·æ§ï¼</strong> è§£å³ä¸è¿°é®é¢éè¦æ´å¤§ãæ´å¤æ ·åçæ°æ®éï¼ä»¥åä¸ç¡®å®æ§éååè·¨æ¾å¤§åæ°éªè¯ã</li>
<li><strong>è®¡ç®æçä»éæåï¼</strong> å°½ç®¡è®¡ç®æçç¸å¯¹äºå¼ºåºçº¿æææé«ï¼ä½ä¸ºäºéæå°åä½æå¨çº¿å·¥ä½æµç¨ä¸­ï¼ä»éè¿ä¸æ­¥å éã</li>
</ul>
</li>
<li>
<p><strong>æ½å¨çæªæ¥ç ç©¶æ¹åï¼</strong></p>
<ul>
<li>å°æ¡æ¶æ©å±å°å¶ä»ææç³»ç»ã</li>
<li>è¿ä¸æ­¥å éæ¨çï¼ä»¥å®ç°åä½åå¨çº¿åºç¨ã</li>
<li>éè¿è½»éåè®¾è®¡åé²æ£çéåè½åï¼å°PerovSegNetåºç¨äºå®éªå®¤è§æ¨¡ç ç©¶ãé«ééç­éåééç¿å¶é ä¸­çå®æ¶è´¨éæ§å¶ã</li>
<li>éè¿æ´å¤§ãæ´å¤æ ·åçæ°æ®éï¼ä»¥åä¸ç¡®å®æ§éååè·¨æ¾å¤§åæ°éªè¯ï¼æ¥è§£å³ç¸å³æ§éå ææ§ãåå²è¯¯å·®åæåæ¡ä»¶å½±åç­å±éæ§ã</li>
</ul>
</li>
</ol>
<p>æ»èè¨ä¹ï¼PerovSegNetä¸ºééç¿å¤ªé³è½çµæ± ææçSEMå¾ååææä¾äºä¸ä¸ªèªå¨åãé«ç²¾åº¦åé«æçè§£å³æ¹æ¡ãéè¿å¶åæ°çæ·±åº¦å­¦ä¹ æ¶æåå®éåæè½åï¼è¯¥æ¡æ¶æææ¾èæ¨å¨ééç¿èèå¶é çä¼ååè´¨éæ§å¶ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Here, we present
an automated deep learning-based framework for SEM image segmentation that
enables precise and efficient identification of lead iodide, perovskite and
defect domains across diverse morphologies.</li>
<li>Built upon an improved YOLOv8x
architecture, our model named PerovSegNet incorporates two novel modules: (i)
Adaptive Shuffle Dilated Convolution Block, which enhances multi-scale and
fine-grained feature extraction through group convolutions and channel mixing;
and (ii) Separable Adaptive Downsampling module, which jointly preserves
fine-scale textures and large-scale structures for more robust boundary
recognition.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.26548v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.26548v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-10-02 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
