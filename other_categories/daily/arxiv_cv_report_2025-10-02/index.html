<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-10-02 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-27
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-10-01/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-10-03/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-10-02">Arxiv Computer Vision Papers - 2025-10-02</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#stitch-training-free-position-control-in-multimodal-diffusion-transformers" class="nav-link">Stitch: Training-Free Position Control in Multimodal Diffusion Transformers</a>
                </li>
                <li class="nav-item">
                    <a href="#ttt3r-3d-reconstruction-as-test-time-training" class="nav-link">TTT3R: 3D Reconstruction as Test-Time Training</a>
                </li>
                <li class="nav-item">
                    <a href="#benchmarking-egocentric-visual-inertial-slam-at-city-scale" class="nav-link">Benchmarking Egocentric Visual-Inertial SLAM at City Scale</a>
                </li>
                <li class="nav-item">
                    <a href="#learning-generalizable-shape-completion-with-sim3-equivariance" class="nav-link">Learning Generalizable Shape Completion with SIM(3) Equivariance</a>
                </li>
                <li class="nav-item">
                    <a href="#learning-to-see-before-seeing-demystifying-llm-visual-priors-from-language-pre-training" class="nav-link">Learning to See Before Seeing: Demystifying LLM Visual Priors from Language Pre-training</a>
                </li>
                <li class="nav-item">
                    <a href="#hart-human-aligned-reconstruction-transformer" class="nav-link">HART: Human Aligned Reconstruction Transformer</a>
                </li>
                <li class="nav-item">
                    <a href="#da2-depth-anything-in-any-direction" class="nav-link">DA^2: Depth Anything in Any Direction</a>
                </li>
                <li class="nav-item">
                    <a href="#video-object-segmentation-aware-audio-generation" class="nav-link">Video Object Segmentation-Aware Audio Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#stable-cinemetrics-structured-taxonomy-and-evaluation-for-professional-video-generation" class="nav-link">Stable Cinemetrics : Structured Taxonomy and Evaluation for Professional Video Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#automated-and-scalable-sem-image-analysis-of-perovskite-solar-cell-materials-via-a-deep-segmentation-framework" class="nav-link">Automated and Scalable SEM Image Analysis of Perovskite Solar Cell Materials via a Deep Segmentation Framework</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-10-02">Arxiv Computer Vision Papers - 2025-10-02</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>好的，这是一份针对2025年9月30日Arxiv计算机视觉领域论文的每日报告执行摘要，旨在帮助忙碌的研究人员快速了解最新进展。</p>
<hr />
<p><strong>每日Arxiv计算机视觉论文报告执行摘要 (2025年9月30日)</strong></p>
<p><strong>1. 主要主题和趋势概述：</strong></p>
<p>今天的论文涵盖了计算机视觉领域的多个前沿方向，主要趋势包括：</p>
<ul>
<li><strong>多模态与生成模型：</strong> 扩散模型在图像生成、编辑和控制方面持续演进，并开始与语言模型深度融合，探索视觉先验的来源。</li>
<li><strong>3D视觉与重建：</strong> 3D重建技术在泛化性、测试时训练和人体姿态重建方面取得了进展，同时SLAM系统在城市场景下的基准测试也备受关注。</li>
<li><strong>具身智能与感知：</strong> 针对第一人称视角（Egocentric）的SLAM和多模态感知（如视频对象分割与音频生成）是重要的研究方向。</li>
<li><strong>领域应用与自动化：</strong> 计算机视觉技术在材料科学（如钙钛矿太阳能电池SEM图像分析）和专业视频生成评估等具体应用中展现出巨大潜力。</li>
<li><strong>模型泛化与鲁棒性：</strong> 论文关注如何提升模型在不同场景、不同数据分布下的泛化能力，例如通过等变性学习和测试时训练。</li>
</ul>
<p><strong>2. 特别重要或创新的论文亮点：</strong></p>
<ul>
<li><strong>"Stitch: Training-Free Position Control in Multimodal Diffusion Transformers" (Jessica Bader et al.)：</strong> 这篇论文在多模态扩散模型中实现了<strong>无需训练的位置控制</strong>，这是一个非常实用的创新。它允许用户在生成过程中精确控制图像元素的位置，极大地提升了扩散模型的可用性和可控性，对于图像编辑和内容创作具有重要意义。</li>
<li><strong>"Learning to See Before Seeing: Demystifying LLM Visual Priors from Language Pre-training" (Junlin Han et al.)：</strong> 这篇论文深入探讨了<strong>大型语言模型（LLM）如何从语言预训练中获得视觉先验知识</strong>。它揭示了LLM在没有直接视觉输入的情况下，通过文本学习到的世界知识如何转化为对视觉概念的理解，为多模态AI的未来发展提供了新的视角和理论基础。</li>
<li><strong>"DA<script type="math/tex">^2</script>: Depth Anything in Any Direction" (Haodong Li et al.)：</strong> 作为"Depth Anything"系列的延续，这篇论文可能在<strong>任意方向的深度估计</strong>上取得了突破，进一步提升了深度估计模型的泛化性和实用性，对于自动驾驶、机器人导航等领域至关重要。</li>
</ul>
<p><strong>3. 新兴研究方向或技术：</strong></p>
<ul>
<li><strong>测试时训练 (Test-Time Training, TTT) 在3D重建中的应用：</strong> "TTT3R: 3D Reconstruction as Test-Time Training" 探索了将测试时训练范式引入3D重建，以提高模型对未知场景的适应性，这可能成为提升3D视觉系统鲁棒性的新途径。</li>
<li><strong>LLM视觉先验的解密与利用：</strong> "Learning to See Before Seeing" 开启了深入理解和利用LLM内部视觉知识的大门，预示着未来多模态模型可能通过更间接、更抽象的方式获取视觉理解能力。</li>
<li><strong>扩散模型与精确控制的结合：</strong> "Stitch" 展示了扩散模型在生成质量和可控性之间取得更好平衡的潜力，未来可能会有更多工作专注于提升生成模型的精细控制能力。</li>
<li><strong>专业视频生成与评估的标准化：</strong> "Stable Cinemetrics" 试图建立专业视频生成的结构化分类和评估体系，这对于推动高质量视频生成技术的发展和应用至关重要。</li>
</ul>
<p><strong>4. 建议阅读全文的论文：</strong></p>
<p>基于其创新性、潜在影响和对领域趋势的代表性，我建议优先阅读以下论文：</p>
<ul>
<li><strong>"Stitch: Training-Free Position Control in Multimodal Diffusion Transformers" (Jessica Bader et al.)：</strong> 对于关注生成模型和图像编辑的研究人员，这篇论文提供了即时可用的技术和重要的思路。</li>
<li><strong>"Learning to See Before Seeing: Demystifying LLM Visual Priors from Language Pre-training" (Junlin Han et al.)：</strong> 对于对多模态AI、LLM与视觉结合感兴趣的研究人员，这篇论文提供了深刻的见解和新的研究方向。</li>
<li><strong>"DA<script type="math/tex">^2</script>: Depth Anything in Any Direction" (Haodong Li et al.)：</strong> 对于从事3D感知、机器人和自动驾驶的研究人员，这篇论文可能代表了深度估计领域的最新进展。</li>
<li><strong>"HART: Human Aligned Reconstruction Transformer" (Xiyi Chen et al.)：</strong> 对于关注人体姿态、形状重建和具身智能的研究人员，这篇论文可能提供了新的SOTA方法。</li>
</ul>
<hr />
<p>这份摘要旨在提供一个高层次的概览，帮助您快速识别与您研究兴趣最相关的论文。</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2509.26644v1">Stitch: Training-Free Position Control in Multimodal Diffusion Transformers</a></li>
<li><a href="#2509.26645v1">TTT3R: 3D Reconstruction as Test-Time Training</a></li>
<li><a href="#2509.26639v1">Benchmarking Egocentric Visual-Inertial SLAM at City Scale</a></li>
<li><a href="#2509.26631v1">Learning Generalizable Shape Completion with SIM(3) Equivariance</a></li>
<li><a href="#2509.26625v1">Learning to See Before Seeing: Demystifying LLM Visual Priors from Language Pre-training</a></li>
<li><a href="#2509.26621v1">HART: Human Aligned Reconstruction Transformer</a></li>
<li><a href="#2509.26618v1">DA<script type="math/tex">^2</script>: Depth Anything in Any Direction</a></li>
<li><a href="#2509.26604v1">Video Object Segmentation-Aware Audio Generation</a></li>
<li><a href="#2509.26555v1">Stable Cinemetrics : Structured Taxonomy and Evaluation for Professional Video Generation</a></li>
<li><a href="#2509.26548v1">Automated and Scalable SEM Image Analysis of Perovskite Solar Cell Materials via a Deep Segmentation Framework</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2509.26644v1'></a></p>
<h2 id="stitch-training-free-position-control-in-multimodal-diffusion-transformers"><a href="https://arxiv.org/abs/2509.26644v1">Stitch: Training-Free Position Control in Multimodal Diffusion Transformers</a></h2>
<p><strong>Authors:</strong> Jessica Bader, Mateusz Pach, Maria A. Bravo, Serge Belongie, Zeynep Akata</p>
<p><strong>Published:</strong> 2025-09-30</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Text-to-Image (T2I) generation models have advanced rapidly in recent years,
but accurately capturing spatial relationships like "above" or "to the right
of" poses a persistent challenge. Earlier methods improved spatial relationship
following with external position control. However, as architectures evolved to
enhance image quality, these techniques became incompatible with modern models.
We propose Stitch, a training-free method for incorporating external position
control into Multi-Modal Diffusion Transformers (MMDiT) via
automatically-generated bounding boxes. Stitch produces images that are both
spatially accurate and visually appealing by generating individual objects
within designated bounding boxes and seamlessly stitching them together. We
find that targeted attention heads capture the information necessary to isolate
and cut out individual objects mid-generation, without needing to fully
complete the image. We evaluate Stitch on PosEval, our benchmark for
position-based T2I generation. Featuring five new tasks that extend the concept
of Position beyond the basic GenEval task, PosEval demonstrates that even top
models still have significant room for improvement in position-based
generation. Tested on Qwen-Image, FLUX, and SD3.5, Stitch consistently enhances
base models, even improving FLUX by 218% on GenEval's Position task and by 206%
on PosEval. Stitch achieves state-of-the-art results with Qwen-Image on
PosEval, improving over previous models by 54%, all accomplished while
integrating position control into leading models training-free. Code is
available at https://github.com/ExplainableML/Stitch.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于Jessica Bader等人撰写的论文“Stitch: Training-Free Position Control in Multimodal Diffusion Transformers”的全面摘要。</p>
<p><strong>论文摘要：Stitch：多模态扩散Transformer中的免训练位置控制</strong></p>
<p><strong>1. 主要问题或研究问题</strong>
尽管近年来文本到图像（T2I）生成模型取得了显著进展，但在准确捕捉“上方”、“右侧”等空间关系方面仍然面临持续挑战。传统的外部位置控制方法与现代T2I模型（特别是基于多模态扩散Transformer (MMDiT) 架构的模型）不兼容，导致这些模型在处理复杂空间布局时表现不佳。因此，论文旨在解决如何在不牺牲图像质量和生成速度的前提下，为MMDiT模型引入精确且免训练的位置控制。</p>
<p><strong>2. 关键创新或方法论贡献</strong>
论文提出了名为“Stitch”的免训练方法，用于在MMDiT模型中实现外部位置控制。其核心创新点包括：</p>
<ul>
<li><strong>LLM驱动的边界框分解与区域绑定（Region Binding）：</strong> Stitch首先利用大型语言模型（LLM）将完整的文本提示（prompt）分解为多个子提示（sub-prompts），每个子提示对应一个由LLM生成的边界框。在生成的前S步中，MMDiT模型在这些指定边界框内独立生成各个对象，并通过注意力掩码约束（Region Binding）确保对象完全在各自区域内生成，从而有效隔离对象与周围上下文。</li>
<li><strong>注意力引导的剪裁（Cutout）：</strong> 为了避免对象拼接时出现可见的缝隙，Stitch在生成过程中（S步之后）利用目标注意力头提取前景对象的潜在表示。研究发现，特定的注意力头能够捕获足够的信息，在图像完全生成之前，从潜在空间中分离并剪裁出前景对象，而无需外部分割模型。</li>
<li><strong>无缝拼接与全局精修：</strong> 提取出的前景对象潜在表示与背景潜在表示合并，形成一个复合潜在表示。在剩余的生成步骤中，模型在完整提示的约束下，无限制地对图像进行精修，从而将各个对象草图无缝地整合为连贯且视觉吸引力的图像。</li>
<li><strong>PosEval基准测试：</strong> 论文引入了一个新的基准测试PosEval，作为GenEval的扩展，专门用于深入评估T2I模型的位置生成能力。PosEval包含五个新任务，旨在探测T2I模型在位置理解方面的特定失败模式，超越了GenEval中基本的“位置”任务。</li>
</ul>
<p><strong>3. 主要结果及其意义</strong>
Stitch在Qwen-Image、FLUX和SD3.5等领先的MMDiT模型上进行了广泛评估，并取得了显著成果：</p>
<ul>
<li><strong>显著提升位置准确性：</strong> Stitch在PosEval基准测试中持续提升了基础模型的性能。例如，在GenEval的“位置”任务上，FLUX的性能提升了218%；在PosEval上，FLUX的性能提升了206%。Stitch在Qwen-Image上的PosEval表现更是超越了现有模型54%，达到了最先进水平。</li>
<li><strong>保持图像质量和多样性：</strong> Stitch在增强位置控制的同时，并未降低图像的视觉质量和多样性。美学评分和DINOv2嵌入空间中的平均成对距离分析表明，Stitch对图像质量和多样性影响甚微。</li>
<li><strong>免训练的优势：</strong> Stitch作为一种免训练方法，能够快速且经济地升级现有T2I模型的位置性能，使其与最先进的图像质量和生成速度相结合。</li>
<li><strong>揭示现有模型的局限性：</strong> PosEval基准测试揭示了即使是顶级的T2I模型在处理复杂的位置提示时仍有很大的改进空间，尤其是在多对象、相对关系和否定关系等任务上。</li>
</ul>
<p><strong>4. 论文中提及的局限性</strong>
论文中并未明确指出Stitch方法的具体局限性，但可以从其设计和评估中推断出一些潜在的方面：</p>
<ul>
<li><strong>LLM的依赖性：</strong> Stitch的性能在很大程度上依赖于LLM生成边界框和子提示的准确性。如果LLM在理解复杂提示或生成精确边界框时出错，可能会影响Stitch的最终生成质量。</li>
<li><strong>注意力头选择：</strong> 剪裁（Cutout）机制依赖于选择能够有效隔离前景对象的特定注意力头。虽然论文提供了选择方法，但这种选择的鲁棒性和泛化性可能在更广泛的场景中需要进一步验证。</li>
<li><strong>S步数的选择：</strong> 论文提到S步数（即在解除约束前独立生成对象的步数）的选择对位置准确性和图像融合度有影响。虽然论文为不同模型提供了最佳S值，但这个参数可能需要针对不同的任务或模型进行调整。</li>
<li><strong>复杂场景的泛化：</strong> 尽管Stitch在PosEval上表现出色，但对于极其复杂、高度抽象或包含大量对象的场景，其性能是否能保持一致仍需进一步探索。</li>
</ul>
<p><strong>5. 潜在的未来研究方向</strong>
基于本论文的工作，未来的研究可以探索以下方向：</p>
<ul>
<li><strong>更智能的LLM集成：</strong> 探索更先进的LLM集成方式，以提高边界框生成和子提示分解的鲁棒性和准确性，尤其是在处理模糊或高度复杂的文本描述时。</li>
<li><strong>自适应注意力头选择：</strong> 开发自适应机制，根据输入提示和生成进度自动选择最佳注意力头进行剪裁，而不是依赖预先确定的选择。</li>
<li><strong>动态S步数调整：</strong> 研究动态调整S步数的方法，使其能够根据生成内容的复杂性和模型状态进行优化，以在位置准确性和图像融合度之间取得更好的平衡。</li>
<li><strong>扩展PosEval：</strong> 进一步扩展PosEval基准测试，纳入更多样化、更具挑战性的位置任务，例如涉及三维空间关系、时间序列位置变化或更精细的语义-空间交互。</li>
<li><strong>与其他控制方法的结合：</strong> 探索Stitch与其他免训练或轻量级控制方法的结合，以实现更全面的图像生成控制，例如姿态、风格或光照。</li>
<li><strong>理论分析：</strong> 对Stitch中区域绑定和剪裁机制的理论基础进行更深入的分析，以更好地理解它们如何影响MMDiT模型的内部表示和生成过程。</li>
</ul>
<p>总而言之，Stitch为多模态扩散Transformer中的位置控制提供了一个创新且高效的免训练解决方案，显著提升了现有模型在复杂空间关系任务上的表现，并为未来的T2I生成研究开辟了新的方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose Stitch, a training-free method for incorporating external position
control into Multi-Modal Diffusion Transformers (MMDiT) via
automatically-generated bounding boxes.</li>
<li>Featuring five new tasks that extend the concept
of Position beyond the basic GenEval task, PosEval demonstrates that even top
models still have significant room for improvement in position-based
generation.</li>
<li>Stitch achieves state-of-the-art results with Qwen-Image on
PosEval, improving over previous models by 54%, all accomplished while
integrating position control into leading models training-free.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.26644v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.26644v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.26645v1'></a></p>
<h2 id="ttt3r-3d-reconstruction-as-test-time-training"><a href="https://arxiv.org/abs/2509.26645v1">TTT3R: 3D Reconstruction as Test-Time Training</a></h2>
<p><strong>Authors:</strong> Xingyu Chen, Yue Chen, Yuliang Xiu, Andreas Geiger, Anpei Chen</p>
<p><strong>Published:</strong> 2025-09-30</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Modern Recurrent Neural Networks have become a competitive architecture for
3D reconstruction due to their linear-time complexity. However, their
performance degrades significantly when applied beyond the training context
length, revealing limited length generalization. In this work, we revisit the
3D reconstruction foundation models from a Test-Time Training perspective,
framing their designs as an online learning problem. Building on this
perspective, we leverage the alignment confidence between the memory state and
incoming observations to derive a closed-form learning rate for memory updates,
to balance between retaining historical information and adapting to new
observations. This training-free intervention, termed TTT3R, substantially
improves length generalization, achieving a <script type="math/tex">2\times</script> improvement in global
pose estimation over baselines, while operating at 20 FPS with just 6 GB of GPU
memory to process thousands of images. Code available in
https://rover-xingyu.github.io/TTT3R</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Xingyu Chen, Yue Chen, Yuliang Xiu, Andreas Geiger, Anpei Chen撰写的论文“TTT3R: 3D Reconstruction as Test-Time Training”的全面摘要。</p>
<hr />
<h3 id="ttt3r-3d-reconstruction-as-test-time-training_1">论文摘要：TTT3R: 3D Reconstruction as Test-Time Training</h3>
<p><strong>1. 主要问题或研究问题：</strong>
该论文主要关注现代循环神经网络（RNN）在3D重建领域中的“长度泛化”问题。尽管RNN因其线性时间复杂度而成为3D重建的竞争性架构，但在训练上下文长度之外应用时，其性能会显著下降，这表明它们在处理长序列数据时存在局限性，即“遗忘问题”。现有方法如CUT3R在处理数百张图像的序列时仍难以泛化。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
TTT3R的核心创新在于将3D重建基础模型的设计重新定义为“测试时训练”（Test-Time Training, TTT）的在线学习问题。具体贡献包括：
*   <strong>TTT视角下的状态更新：</strong> 将RNN的状态更新规则重新解释为一种TTT风格的在线学习过程，其中状态被视为在测试时通过梯度下降学习的“快速权重”，而非训练数据集中的“慢速权重”。
*   <strong>置信度引导的学习率：</strong> 论文利用记忆状态与传入观测值之间的对齐置信度，推导出一个闭式学习率（<script type="math/tex">\beta_t</script>）用于记忆更新。这个学习率能够平衡保留历史信息和适应新观测值，从而有效缓解灾难性遗忘。
*   <strong>训练无关的干预：</strong> TTT3R是一种“训练无关、即插即用”的干预措施，无需对现有模型进行微调或添加额外参数，即可直接应用于下游任务。
*   <strong>内部置信度信号的利用：</strong> 通过利用跨注意力统计数据来估计状态更新的对齐置信度，并相应地分配每token的学习率，TTT3R能够选择性地抑制低质量的状态更新，从而提高性能。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>显著的长度泛化改进：</strong> TTT3R在长序列输入下显著提高了长度泛化能力，在全局姿态估计方面比基线方法实现了2倍的改进。
*   <strong>高效的性能：</strong> 该方法在处理数千张图像时，仍能以20 FPS的速度运行，并且仅需6 GB的GPU内存，保持了与基线CUT3R相同的推理速度和内存效率。
*   <strong>竞争性表现：</strong> 在标准3D重建基准测试中，TTT3R与最先进的在线重建模型（如CUT3R、Point3R）表现出竞争性，并在长序列输入下展现出显著优势。
*   <strong>定性结果：</strong> TTT3R实现了更准确的重建，缓解了遗忘问题，并支持在线闭环，避免了CUT3R中出现的相机姿态漂移、几何结构损坏和鬼影伪影。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>未完全解决状态遗忘：</strong> 尽管TTT3R缓解了状态遗忘问题，但并未完全解决。
*   <strong>与离线方法的差距：</strong> TTT3R的重建精度尚未达到强大的离线方法（如VGGT）的水平。离线方法虽然速度较慢且内存需求更高，但通过完全注意力机制保留了完整的历史上下文。
*   <strong>设计空间尚待探索：</strong> 作为一种测试时回归方法，TTT3R的关联记忆设计空间仍有待广泛探索。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>开发更有效、稳定和可并行化的循环架构：</strong> 论文指出，TTT3R的设计空间仍未充分探索，未来的研究可以致力于开发更有效、稳定和可并行化的循环架构，以进一步提高重建精度和长度泛化能力。
*   <strong>重新审视3D重建模型的基础：</strong> 论文希望其发现能激励未来的研究重新审视3D重建模型的基础，以实现进一步的性能提升。</p>
<hr />
<p>总而言之，TTT3R通过将3D重建视为测试时训练问题，并引入一种置信度引导的闭式学习率来更新记忆状态，成功解决了RNN在长序列3D重建中的长度泛化和遗忘问题。这种训练无关的干预措施在保持高效率的同时，显著提升了重建质量和姿态估计精度，为未来可扩展的在线3D重建系统提供了新的研究方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Building on this
perspective, we leverage the alignment confidence between the memory state and
incoming observations to derive a closed-form learning rate for memory updates,
to balance between retaining historical information and adapting to new
observations.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.26645v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.26645v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.26639v1'></a></p>
<h2 id="benchmarking-egocentric-visual-inertial-slam-at-city-scale"><a href="https://arxiv.org/abs/2509.26639v1">Benchmarking Egocentric Visual-Inertial SLAM at City Scale</a></h2>
<p><strong>Authors:</strong> Anusha Krishnan, Shaohui Liu, Paul-Edouard Sarlin, Oscar Gentilhomme, David Caruso, Maurizio Monge, Richard Newcombe, Jakob Engel, Marc Pollefeys</p>
<p><strong>Published:</strong> 2025-09-30</p>
<p><strong>Categories:</strong> cs.CV, cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>Precise 6-DoF simultaneous localization and mapping (SLAM) from onboard
sensors is critical for wearable devices capturing egocentric data, which
exhibits specific challenges, such as a wider diversity of motions and
viewpoints, prevalent dynamic visual content, or long sessions affected by
time-varying sensor calibration. While recent progress on SLAM has been swift,
academic research is still driven by benchmarks that do not reflect these
challenges or do not offer sufficiently accurate ground truth poses. In this
paper, we introduce a new dataset and benchmark for visual-inertial SLAM with
egocentric, multi-modal data. We record hours and kilometers of trajectories
through a city center with glasses-like devices equipped with various sensors.
We leverage surveying tools to obtain control points as indirect pose
annotations that are metric, centimeter-accurate, and available at city scale.
This makes it possible to evaluate extreme trajectories that involve walking at
night or traveling in a vehicle. We show that state-of-the-art systems
developed by academia are not robust to these challenges and we identify
components that are responsible for this. In addition, we design tracks with
different levels of difficulty to ease in-depth analysis and evaluation of less
mature approaches. The dataset and benchmark are available at
https://www.lamaria.ethz.ch.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Anusha Krishnan等人撰写的论文“Benchmarking Egocentric Visual-Inertial SLAM at City Scale”的全面摘要。</p>
<hr />
<h3 id="benchmarking-egocentric-visual-inertial-slam-at-city-scale_1">论文摘要：Benchmarking Egocentric Visual-Inertial SLAM at City Scale</h3>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决现有视觉惯性SLAM（VI-SLAM）基准数据集未能充分反映以自我为中心（egocentric）数据所带来的独特挑战的问题。这些挑战包括：运动和视角多样性更大、动态视觉内容普遍存在、长时间会话中传感器校准随时间变化，以及缺乏足够精确的真值姿态。现有的学术研究往往依赖于在受控环境和运动下收集的数据，这使得最先进的VI-SLAM系统在处理真实世界、以自我为中心的数据时表现不佳。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
*   <strong>LaMAria数据集：</strong> 引入了一个新的、大规模、以自我为中心的视觉惯性SLAM数据集——LaMAria。该数据集使用Project Aria眼镜式设备在苏黎世市中心记录了数小时、数十公里的轨迹，涵盖了低光照、曝光变化、移动平台（如电车、缆车）、时间变化校准和动态环境等多种挑战。
*   <strong>厘米级真值姿态：</strong> 利用测量工具（包括GNSS-RTK和全站仪）获取稀疏控制点（CPs），这些CPs具有度量、厘米级的精度，并分布在城市规模的区域。通过将这些CPs与设备观测到的AprilTag标记相结合，论文能够计算出高精度的稀疏真值姿态。
*   <strong>伪密集真值姿态生成：</strong> 为了进行更细粒度的评估和3D重建等任务，论文通过融合视觉、惯性传感器数据和稀疏CP信息，并进行联合优化，生成了伪密集真值姿态。这种方法在保证足够精度的同时，克服了传统真值获取方法的局限性。
*   <strong>分级难度基准：</strong> 设计了不同难度级别的实验轨迹，从受控的平台运动到具有挑战性的不受限制的头戴式运动，以便于深入分析和评估不同成熟度的方法。
*   <strong>评估框架：</strong> 提出了基于CP误差的评分函数，能够可靠地评估SLAM系统在不同挑战下的性能，并报告了CP对齐误差和设备姿态误差的召回率。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>现有系统鲁棒性不足：</strong> 评估结果表明，现有最先进的学术VI-SLAM系统在面对LaMAria数据集中的独特挑战时（如低光照、动态环境、移动平台、快速运动等），表现出显著的鲁棒性不足，经常出现漂移或跟踪失败。
*   <strong>多传感器融合的优势：</strong> 实验证实，依赖多摄像头和惯性传感器的系统显著优于单目或单目惯性系统。
*   <strong>在线校准的重要性：</strong> 研究发现，在线优化随时间变化的传感器校准对于提高姿态精度至关重要，这是Project Aria的商业SLAM系统优于大多数学术基线的一个关键因素。
*   <strong>数据集未饱和：</strong> 即使对于高度工程化的Project Aria商业SLAM系统，数据集也未达到饱和，尤其是在涉及移动平台的序列中，这表明LaMAria数据集为未来VI-SLAM研究提供了广阔的空间。
*   <strong>揭示研究方向：</strong> 论文通过评估结果，明确指出了未来以自我为中心的SLAM研究的几个有前景的方向，例如在线校准、回环检测、鲁棒的异常值剔除和基于机器学习的图像匹配。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>伪密集真值姿态的精度：</strong> 尽管论文生成的伪密集真值姿态对于评估关键帧误差足够准确，但其精度不如测量级的稀疏控制点，在某些情况下（如移动平台）的精度保证有限。
*   <strong>RGB传感器未用于评估：</strong> 由于RGB传感器采用卷帘快门，在本文的评估中未被使用，但其数据作为一种模态包含在数据集中。
*   <strong>SLAM相机重叠有限：</strong> Project Aria设备的两个SLAM相机位于眼镜两侧，重叠区域不足以支持水平立体设置，这限制了其在标准立体模式下的评估。
*   <strong>部分系统无法处理长序列：</strong> 某些计算量大的方法（如SfM方法）无法很好地扩展到长序列，因此仅在短片段上进行评估。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>在线时间变化校准优化：</strong> 开发能够适应可穿戴设备全天候使用特性的在线校准优化方法。
*   <strong>回环检测和VI捆绑调整：</strong> 改进回环检测和视觉惯性捆绑调整，以减少开环预测中的里程计漂移。
*   <strong>鲁棒的异常值剔除和跟踪丢失处理：</strong> 针对移动平台等场景，设计更鲁棒的异常值剔除策略和更好的跟踪丢失处理机制。
*   <strong>基于机器学习的图像匹配和点跟踪：</strong> 利用在大规模数据集上训练的机器学习模型，开发更先进的图像匹配和点跟踪技术。</p>
<hr />
<p>这篇论文通过引入一个具有挑战性且高精度真值的新数据集，为以自我为中心的视觉惯性SLAM领域设定了一个新的基准。它不仅揭示了现有方法的局限性，还为未来研究指明了方向，对于推动可穿戴设备上的SLAM技术发展具有重要意义。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this
paper, we introduce a new dataset and benchmark for visual-inertial SLAM with
egocentric, multi-modal data.</li>
<li>We show that state-of-the-art systems
developed by academia are not robust to these challenges and we identify
components that are responsible for this.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.26639v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.26639v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.26631v1'></a></p>
<h2 id="learning-generalizable-shape-completion-with-sim3-equivariance"><a href="https://arxiv.org/abs/2509.26631v1">Learning Generalizable Shape Completion with SIM(3) Equivariance</a></h2>
<p><strong>Authors:</strong> Yuqing Wang, Zhaiyu Chen, Xiao Xiang Zhu</p>
<p><strong>Published:</strong> 2025-09-30</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>3D shape completion methods typically assume scans are pre-aligned to a
canonical frame. This leaks pose and scale cues that networks may exploit to
memorize absolute positions rather than inferring intrinsic geometry. When such
alignment is absent in real data, performance collapses. We argue that robust
generalization demands architectural equivariance to the similarity group,
SIM(3), so the model remains agnostic to pose and scale. Following this
principle, we introduce the first SIM(3)-equivariant shape completion network,
whose modular layers successively canonicalize features, reason over
similarity-invariant geometry, and restore the original frame. Under a
de-biased evaluation protocol that removes the hidden cues, our model
outperforms both equivariant and augmentation baselines on the PCN benchmark.
It also sets new cross-domain records on real driving and indoor scans,
lowering minimal matching distance on KITTI by 17% and Chamfer distance <script type="math/tex">\ell1</script>
on OmniObject3D by 14%. Perhaps surprisingly, ours under the stricter protocol
still outperforms competitors under their biased settings. These results
establish full SIM(3) equivariance as an effective route to truly generalizable
shape completion. Project page: https://sime-completion.github.io.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Yuqing Wang, Zhaiyu Chen, Xiao Xiang Zhu撰写的论文“Learning Generalizable Shape Completion with SIM(3) Equivariance”的全面摘要。</p>
<hr />
<h3 id="learning-generalizable-shape-completion-with-sim3-equivariance_1">论文摘要：Learning Generalizable Shape Completion with SIM(3) Equivariance</h3>
<p><strong>1. 主要问题或研究问题：</strong>
当前3D形状补全方法普遍存在一个核心问题：它们通常假设输入扫描已预先对齐到规范坐标系。这种做法导致网络倾向于记忆绝对位置和尺度信息，而非学习形状的内在几何特性。当在实际数据中缺乏这种预对齐时，模型的性能会急剧下降，这严重限制了其泛化能力。论文旨在解决如何实现真正可泛化的形状补全，使其对任意姿态和尺度变化（即SIM(3)变换）保持不变性。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
*   <strong>问题识别与SIM(3)等变性需求：</strong> 论文明确指出现有方法中姿态和尺度偏差的问题，并首次提出SIM(3)等变性是实现野外泛化（in-the-wild generalization）的关键先决条件。
*   <strong>首个SIM(3)等变形状补全网络：</strong> 论文引入了第一个完全SIM(3)等变形状补全网络。该网络采用模块化设计，包含三个核心阶段：
    *   <strong>特征规范化（Canonicalization）：</strong> 将特征转换为平移和尺度不变的形式。
    *   <strong>相似性不变几何推理（Shape Reasoning）：</strong> 在规范化特征上进行几何推理，确保对形状内在几何的学习。
    *   <strong>变换恢复（Transform Restoration）：</strong> 逐步将原始的姿态和尺度信息重新注入，以在传感器坐标系中恢复完整的形状。
*   <strong>去偏置评估协议：</strong> 论文建立了一个严格的评估协议，该协议移除了传统基准测试中存在的隐藏姿态和尺度线索，从而对模型的真实泛化能力进行公平评估。</p>
<p><strong>3. 主要结果及其重要性：</strong>
*   <strong>超越基线：</strong> 在去偏置评估协议下，该模型在PCN基准测试上显著优于现有的等变和数据增强基线。
*   <strong>跨领域泛化能力：</strong> 在真实驾驶（KITTI）和室内（OmniObject3D）扫描数据上，该模型创造了新的跨领域记录，将KITTI上的最小匹配距离（MMD）降低了17%，并将OmniObject3D上的Chamfer距离<script type="math/tex">\ell1</script>降低了14%。
*   <strong>严格协议下的卓越表现：</strong> 令人惊讶的是，即使在更严格的去偏置协议下，该模型仍能超越在偏置设置下训练的竞争对手。
*   <strong>SIM(3)等变性的有效性：</strong> 这些结果有力地证明了完全SIM(3)等变性是实现真正可泛化形状补全的有效途径。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>姿态和尺度依赖特征的潜在丢失：</strong> 通过设计移除对绝对姿态和尺度的依赖，虽然增强了鲁棒性，但也可能在物体始终出现在规范坐标系中时，丢弃一些有用的线索。例如，没有可见腿的椅子背面可能被误认为是沙发。
*   <strong>部分观测的对称性：</strong> 框架中的等变性是针对单个部分扫描定义的。对于同一物体的不同部分观测，初始化变异性无法完全消除，因此跨视图对称性必须通过数据隐式学习。
*   <strong>铰接复杂场景：</strong> 该方法擅长补全任意相似变换下的形状，但未明确考虑独立移动的子部件（如人体关节、机械臂或多物体场景）。
*   <strong>计算开销：</strong> 向量值特征和完全等变模块相比标量值层会带来约三倍的计算开销，导致运行时延迟高于非等变基线，这可能限制其在实时或资源受限部署中的应用。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   将该框架扩展到多物体和大规模场景建模。
*   结合类别特定的形状先验或允许使用多个局部变换，以处理更复杂的铰接场景。</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>Following this
principle, we introduce the first SIM(3)-equivariant shape completion network,
whose modular layers successively canonicalize features, reason over
similarity-invariant geometry, and restore the original frame.</li>
<li>Under a
de-biased evaluation protocol that removes the hidden cues, our model
outperforms both equivariant and augmentation baselines on the PCN benchmark.</li>
<li>It also sets new cross-domain records on real driving and indoor scans,
lowering minimal matching distance on KITTI by 17% and Chamfer distance <script type="math/tex">\ell1</script>
on OmniObject3D by 14%.</li>
<li>Perhaps surprisingly, ours under the stricter protocol
still outperforms competitors under their biased settings.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.26631v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.26631v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.26625v1'></a></p>
<h2 id="learning-to-see-before-seeing-demystifying-llm-visual-priors-from-language-pre-training"><a href="https://arxiv.org/abs/2509.26625v1">Learning to See Before Seeing: Demystifying LLM Visual Priors from Language Pre-training</a></h2>
<p><strong>Authors:</strong> Junlin Han, Shengbang Tong, David Fan, Yufan Ren, Koustuv Sinha, Philip Torr, Filippos Kokkinos</p>
<p><strong>Published:</strong> 2025-09-30</p>
<p><strong>Categories:</strong> cs.LG, cs.AI, cs.CV, cs.MM</p>
<p><strong>Abstract:</strong></p>
<p>Large Language Models (LLMs), despite being trained on text alone,
surprisingly develop rich visual priors. These priors allow latent visual
capabilities to be unlocked for vision tasks with a relatively small amount of
multimodal data, and in some cases, to perform visual tasks without ever having
seen an image. Through systematic analysis, we reveal that visual priors-the
implicit, emergent knowledge about the visual world acquired during language
pre-training-are composed of separable perception and reasoning priors with
unique scaling trends and origins. We show that an LLM's latent visual
reasoning ability is predominantly developed by pre-training on
reasoning-centric data (e.g., code, math, academia) and scales progressively.
This reasoning prior acquired from language pre-training is transferable and
universally applicable to visual reasoning. In contrast, a perception prior
emerges more diffusely from broad corpora, and perception ability is more
sensitive to the vision encoder and visual instruction tuning data. In
parallel, text describing the visual world proves crucial, though its
performance impact saturates rapidly. Leveraging these insights, we propose a
data-centric recipe for pre-training vision-aware LLMs and verify it in 1T
token scale pre-training. Our findings are grounded in over 100 controlled
experiments consuming 500,000 GPU-hours, spanning the full MLLM construction
pipeline-from LLM pre-training to visual alignment and supervised multimodal
fine-tuning-across five model scales, a wide range of data categories and
mixtures, and multiple adaptation setups. Along with our main findings, we
propose and investigate several hypotheses, and introduce the Multi-Level
Existence Bench (MLE-Bench). Together, this work provides a new way of
deliberately cultivating visual priors from language pre-training, paving the
way for the next generation of multimodal LLMs.</p>
<p><strong>Analysis:</strong></p>
<p>这篇论文《Learning to See Before Seeing: Demystifying LLM Visual Priors from Language Pre-training》深入探讨了大型语言模型（LLMs）如何在仅通过文本训练的情况下，发展出丰富的视觉先验知识。</p>
<p>以下是论文的全面摘要：</p>
<ol>
<li>
<p><strong>主要问题或研究问题：</strong>
    该研究旨在系统地揭示LLMs在仅通过文本预训练后，如何以及为何能够发展出丰富的视觉先验知识。具体来说，它探究了这些视觉先验的构成（是单一能力还是可分离的组件）、来源、以及如何有效利用这些先验来构建更强大的多模态LLMs（MLLMs）。</p>
</li>
<li>
<p><strong>关键创新或方法论贡献：</strong></p>
<ul>
<li><strong>系统性解构视觉先验：</strong> 论文通过超过100项受控实验，消耗500,000 GPU小时，系统地分析了模型规模、数据规模、数据来源、视觉世界和推理数据混合比例等变量对LLM视觉先验的影响。</li>
<li><strong>分离感知和推理先验：</strong> 研究发现视觉先验由可分离的感知先验和推理先验组成，并揭示了它们独特的扩展趋势和起源。</li>
<li><strong>数据中心化预训练策略：</strong> 提出了一种数据中心化的方法，通过策略性地平衡推理中心数据和视觉描述性文本的混合比例，来预训练具有视觉意识的LLMs。</li>
<li><strong>引入Multi-Level Existence Bench (MLE-Bench)：</strong> 创建了一个新的基准测试，用于细粒度评估模型的感知能力，特别是针对不同大小（小、中、大）物体的存在性识别。</li>
<li><strong>盲视觉指令微调（Blind Visual Instruction Tuning）：</strong> 提出了一种在视觉适应初期仅使用文本数据进行指令微调的技巧，以帮助模型更好地学习指令遵循格式，并作为探测模型如何利用语言“捷径”解决视觉任务的工具。</li>
</ul>
</li>
<li>
<p><strong>主要结果及其意义：</strong></p>
<ul>
<li><strong>视觉先验的组成和起源：</strong> LLM的潜在视觉推理能力主要通过推理中心数据（如代码、数学、学术论文）的预训练逐步发展和扩展，这种推理先验具有可迁移性和普遍适用性。相比之下，感知先验更广泛地从通用语料库中出现，且对视觉编码器和视觉指令微调数据更敏感。描述视觉世界的文本也很关键，但其性能影响迅速饱和。</li>
<li><strong>数据混合的优化：</strong> 研究发现，最大化MLLM的VQA性能的最佳数据混合是严重偏向推理中心内容，但同时包含必要的视觉世界知识。通过精心校准的数据混合，可以在不显著损害核心语言能力的情况下培养强大的视觉先验。</li>
<li><strong>推理先验的普遍性：</strong> 视觉推理先验是基础性的、模态无关的，无论使用何种视觉编码器，都能使多模态系统受益。</li>
<li><strong>感知先验的规模依赖性：</strong> 感知先验表现出规模依赖性，其优势在感知中小尺寸物体时最为显著，这表明多样化的文本数据有助于模型学习细粒度的视觉概念。</li>
<li><strong>能力来源的区分：</strong> 视觉推理能力主要由语言预训练获得的推理先验塑造，而感知能力更依赖于后训练阶段（视觉指令微调）。</li>
</ul>
</li>
<li>
<p><strong>论文中提及的局限性：</strong></p>
<ul>
<li><strong>架构限制：</strong> 研究主要集中在适配器风格的MLLM架构，其发现可能无法完全推广到其他方法，如采用离散视觉标记化或端到端联合训练视觉和语言组件的模型。</li>
<li><strong>安全和伦理问题：</strong> 论文未深入探讨这些学习到的视觉先验可能包含的社会偏见、刻板印象和潜在有害内容，这需要进一步的彻底审计。</li>
<li><strong>模态限制：</strong> 研究仅限于静态图像领域，未探索动态模态（如视频理解）的视觉先验。</li>
</ul>
</li>
<li>
<p><strong>潜在的未来研究方向：</strong></p>
<ul>
<li><strong>其他MLLM架构的视觉先验研究：</strong> 探索离散视觉标记化或端到端联合训练模型中视觉先验的形成和利用动态。</li>
<li><strong>视觉先验的安全性和伦理审计：</strong> 对LLMs中学习到的视觉先验进行彻底的公平性和安全性审计，以识别和缓解潜在的偏见。</li>
<li><strong>动态模态的视觉先验：</strong> 探索文本数据如何促进视频理解中的时间推理、动作识别和因果关系等先验知识。</li>
<li><strong>抽象结构与语义基础的相互作用：</strong> 进一步研究抽象结构和语义基础在形成跨模态表示中的精确相互作用。</li>
</ul>
</li>
</ol>
<p>总而言之，这篇论文通过严谨的实验和分析，为理解LLMs如何从纯文本中“学会看”提供了开创性的见解。它不仅揭示了视觉先验的内在结构和起源，还提出了一种数据中心化的预训练方法，为下一代多模态LLMs的发展奠定了坚实基础，并为未来的研究指明了方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We show that an LLM's latent visual
reasoning ability is predominantly developed by pre-training on
reasoning-centric data (e.g., code, math, academia) and scales progressively.</li>
<li>Leveraging these insights, we propose a
data-centric recipe for pre-training vision-aware LLMs and verify it in 1T
token scale pre-training.</li>
<li>Together, this work provides a new way of
deliberately cultivating visual priors from language pre-training, paving the
way for the next generation of multimodal LLMs.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.26625v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.26625v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.26621v1'></a></p>
<h2 id="hart-human-aligned-reconstruction-transformer"><a href="https://arxiv.org/abs/2509.26621v1">HART: Human Aligned Reconstruction Transformer</a></h2>
<p><strong>Authors:</strong> Xiyi Chen, Shaofei Wang, Marko Mihajlovic, Taewon Kang, Sergey Prokudin, Ming Lin</p>
<p><strong>Published:</strong> 2025-09-30</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We introduce HART, a unified framework for sparse-view human reconstruction.
Given a small set of uncalibrated RGB images of a person as input, it outputs a
watertight clothed mesh, the aligned SMPL-X body mesh, and a Gaussian-splat
representation for photorealistic novel-view rendering. Prior methods for
clothed human reconstruction either optimize parametric templates, which
overlook loose garments and human-object interactions, or train implicit
functions under simplified camera assumptions, limiting applicability in real
scenes. In contrast, HART predicts per-pixel 3D point maps, normals, and body
correspondences, and employs an occlusion-aware Poisson reconstruction to
recover complete geometry, even in self-occluded regions. These predictions
also align with a parametric SMPL-X body model, ensuring that reconstructed
geometry remains consistent with human structure while capturing loose clothing
and interactions. These human-aligned meshes initialize Gaussian splats to
further enable sparse-view rendering. While trained on only 2.3K synthetic
scans, HART achieves state-of-the-art results: Chamfer Distance improves by
18-23 percent for clothed-mesh reconstruction, PA-V2V drops by 6-27 percent for
SMPL-X estimation, LPIPS decreases by 15-27 percent for novel-view synthesis on
a wide range of datasets. These results suggest that feed-forward transformers
can serve as a scalable model for robust human reconstruction in real-world
settings. Code and models will be released.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Xiyi Chen等人撰写的论文“HART: Human Aligned Reconstruction Transformer”的全面摘要。</p>
<p><strong>论文摘要：HART: Human Aligned Reconstruction Transformer</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决稀疏视角下人体三维重建的挑战。现有方法在处理宽松衣物、人与物体交互时存在局限性，或者对相机假设过于简化，限制了其在真实场景中的应用。具体来说，研究问题是如何从少量未校准的RGB图像中，鲁棒地重建出完整、逼真且与人体结构对齐的着装网格、SMPL-X身体网格，并支持高质量的新视角渲染。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
HART（Human Aligned Reconstruction Transformer）是一个统一的框架，其主要创新包括：
*   <strong>统一的Transformer框架：</strong> HART采用Transformer骨干网络，能够同时预测每像素3D点图、法线和身体对应关系。
*   <strong>遮挡感知泊松重建：</strong> 针对点图方法在处理自遮挡区域时的局限性，HART引入了一个3D U-Net到可微分泊松表面重建（DPSR）模块中，通过残差校正细化指示网格，从而恢复完整且水密的着装几何体。
*   <strong>人体对齐的几何体：</strong> 预测结果与参数化的SMPL-X身体模型对齐，确保重建几何体与人体结构保持一致，同时捕捉宽松衣物和交互。这通过预测紧密度向量和身体部位标签来实现，用于SMPL-X参数估计。
*   <strong>几何体引导的新视角渲染：</strong> 重建的人体网格（着装网格）被用作初始化和正则化高斯splatting，以实现逼真的新视角渲染，显著提高了渲染质量并缓解了过拟合。
*   <strong>残差法线头：</strong> 采用残差学习策略，通过预测相对于现有SOTA人体法线估计器的残差法线，克服了直接预测法线可能过于平滑或模糊的问题，从而获得更连贯和详细的表面重建。</p>
<p><strong>3. 主要结果及其意义：</strong>
尽管仅在2.3K合成扫描上进行训练，HART在多个基准测试中取得了最先进的结果：
*   <strong>着装网格重建：</strong> Chamfer Distance提高了18-23%。
*   <strong>SMPL-X估计：</strong> PA-V2V（顶点到顶点误差）下降了6-27%。
*   <strong>新视角合成：</strong> LPIPS（感知距离）在各种数据集上降低了15-27%。
这些结果表明，前馈Transformer可以作为一种可扩展模型，用于在真实世界环境中进行鲁棒的人体重建。定性和定量评估进一步证明了HART在处理复杂服装和人与物体交互的真实世界图像时的泛化能力。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>细节恢复：</strong> 重建结果在精细尺度细节（如手指、头发）方面仍有不足，这可能与指示网格分辨率有限有关。
*   <strong>稀疏视角和挑战性光照：</strong> 在非常稀疏的视角（例如3个视角）或挑战性光照条件下，渲染质量会显著下降。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>分层或多尺度架构：</strong> 探索分层或多尺度架构以提高细节恢复能力。
*   <strong>扩散先验：</strong> 利用扩散先验来改进遮挡区域的渲染。
*   <strong>基于视频的训练：</strong> 采用基于视频的训练方法，以增强时间一致性并实现可动画的重建。</p>
<p>总而言之，HART论文提出了一种新颖且统一的Transformer框架，通过结合遮挡感知几何重建和人体对齐机制，显著提升了稀疏视角下人体着装网格重建、SMPL-X估计和新视角合成的性能。其在真实场景中的鲁棒性和泛化能力，为未来的人体三维重建研究奠定了坚实的基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce HART, a unified framework for sparse-view human reconstruction.</li>
<li>Given a small set of uncalibrated RGB images of a person as input, it outputs a
watertight clothed mesh, the aligned SMPL-X body mesh, and a Gaussian-splat
representation for photorealistic novel-view rendering.</li>
<li>While trained on only 2.3K synthetic
scans, HART achieves state-of-the-art results: Chamfer Distance improves by
18-23 percent for clothed-mesh reconstruction, PA-V2V drops by 6-27 percent for
SMPL-X estimation, LPIPS decreases by 15-27 percent for novel-view synthesis on
a wide range of datasets.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.26621v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.26621v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.26618v1'></a></p>
<h2 id="da2-depth-anything-in-any-direction"><a href="https://arxiv.org/abs/2509.26618v1">DA<script type="math/tex">^2</script>: Depth Anything in Any Direction</a></h2>
<p><strong>Authors:</strong> Haodong Li, Wangguangdong Zheng, Jing He, Yuhao Liu, Xin Lin, Xin Yang, Ying-Cong Chen, Chunchao Guo</p>
<p><strong>Published:</strong> 2025-09-30</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Panorama has a full FoV (360<script type="math/tex">^\circ\times</script>180<script type="math/tex">^\circ</script>), offering a more
complete visual description than perspective images. Thanks to this
characteristic, panoramic depth estimation is gaining increasing traction in 3D
vision. However, due to the scarcity of panoramic data, previous methods are
often restricted to in-domain settings, leading to poor zero-shot
generalization. Furthermore, due to the spherical distortions inherent in
panoramas, many approaches rely on perspective splitting (e.g., cubemaps),
which leads to suboptimal efficiency. To address these challenges, we propose
<script type="math/tex">\textbf{DA}</script>
<script type="math/tex">^{\textbf{2}}</script>: <script type="math/tex">\textbf{D}</script>epth <script type="math/tex">\textbf{A}</script>nything in
<script type="math/tex">\textbf{A}</script>ny <script type="math/tex">\textbf{D}</script>irection, an accurate, zero-shot generalizable, and
fully end-to-end panoramic depth estimator. Specifically, for scaling up
panoramic data, we introduce a data curation engine for generating high-quality
panoramic depth data from perspective, and create <script type="math/tex">\sim</script>543K panoramic
RGB-depth pairs, bringing the total to <script type="math/tex">\sim</script>607K. To further mitigate the
spherical distortions, we present SphereViT, which explicitly leverages
spherical coordinates to enforce the spherical geometric consistency in
panoramic image features, yielding improved performance. A comprehensive
benchmark on multiple datasets clearly demonstrates DA<script type="math/tex">^{2}</script>'s SoTA
performance, with an average 38% improvement on AbsRel over the strongest
zero-shot baseline. Surprisingly, DA<script type="math/tex">^{2}</script> even outperforms prior in-domain
methods, highlighting its superior zero-shot generalization. Moreover, as an
end-to-end solution, DA<script type="math/tex">^{2}</script> exhibits much higher efficiency over fusion-based
approaches. Both the code and the curated panoramic data will be released.
Project page: https://depth-any-in-any-dir.github.io/.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Haodong Li等人撰写的论文“DA<script type="math/tex">^2</script>: Depth Anything in Any Direction”的全面摘要。</p>
<hr />
<p><strong>论文摘要：DA<script type="math/tex">^2</script>: Depth Anything in Any Direction</strong></p>
<p>这篇论文《DA<script type="math/tex">^2</script>: Depth Anything in Any Direction》提出了一种新颖的、端到端的全景深度估计器，旨在解决现有全景深度估计方法在零样本泛化能力和效率方面的局限性。</p>
<p><strong>1. 主要问题或研究问题</strong>
全景图像（360°x180° FoV）提供了比透视图像更完整的视觉描述，使其在3D视觉中越来越受关注。然而，全景深度估计面临两大挑战：
*   <strong>数据稀缺性导致泛化能力差：</strong> 由于高质量全景深度数据稀缺，现有方法通常局限于特定领域，导致零样本泛化能力不足。
*   <strong>球面畸变导致效率低下：</strong> 全景图像固有的球面畸变使得许多方法依赖于透视分割（如立方体贴图），这导致了次优的效率和额外的模块。
论文旨在开发一个准确、零样本泛化且高效的端到端全景深度估计器，能够处理这些挑战。</p>
<p><strong>2. 关键创新或方法论贡献</strong>
为解决上述问题，DA<script type="math/tex">^2</script>提出了以下关键创新：
*   <strong>全景数据策展引擎（Panoramic Data Curation Engine）：</strong> 针对全景数据稀缺问题，论文引入了一个数据策展引擎。该引擎能够从现有的透视深度数据中生成高质量的全景深度数据。通过透视到等距柱状投影（P2E）和全景图像外绘（使用FLUX-I2P模型），该引擎将透视RGB-深度对转换为全景RGB-深度对，将训练数据总量从约63K增加到约607K（其中约543K为新生成数据），极大地扩展了全景深度训练数据集的规模和多样性。
*   <strong>SphereViT模型架构：</strong> 为缓解球面畸变的影响，论文提出了SphereViT作为DA<script type="math/tex">^2</script>的主要骨干网络。SphereViT通过显式利用全景图像的球面坐标（方位角和极角），并将其编码为球面嵌入（Spherical Embedding），然后通过交叉注意力机制将其注入到图像特征中。这种设计使得图像特征能够“感知”全景的球面几何结构，从而产生畸变感知表示，提高了几何估计的准确性，且无需额外的辅助模块。
*   <strong>综合基准测试：</strong> 论文构建了一个全面的基准测试，比较了零样本/领域内、全景/透视方法，以全面评估全景深度估计的性能。
*   <strong>训练损失函数：</strong> 结合了距离损失（Ldis）以确保全局准确的距离值，以及法线损失（Lnor）以促进局部平滑和锐利的表面，尤其是在距离值相似但表面法线差异显著的区域。</p>
<p><strong>3. 主要结果及其意义</strong>
DA<script type="math/tex">^2</script>在多个数据集上的综合基准测试中表现出最先进（SoTA）的性能：
*   <strong>卓越的零样本泛化能力：</strong> DA<script type="math/tex">^2</script>在最强的零样本基线上，AbsRel指标平均提高了38%。令人惊讶的是，DA<script type="math/tex">^2</script>甚至超越了先前的领域内方法，这凸显了其卓越的零样本泛化能力。
*   <strong>高效率：</strong> 作为一种端到端解决方案，DA<script type="math/tex">^2</script>比基于融合的方法（如UniK3D和MoGev2）表现出更高的效率，推理速度显著加快（例如，DA<script type="math/tex">^2</script>约0.3秒，MoGev2约28秒）。
*   <strong>数据规模效应：</strong> 实验结果清晰地表明，随着训练数据规模的扩大（通过数据策展引擎将透视数据转换为全景数据），DA<script type="math/tex">^2</script>的性能稳步提升，验证了数据策展引擎的有效性。
*   <strong>几何保真度：</strong> SphereViT通过引入球面嵌入，显著提高了全景图像的几何理解，避免了传统方法中常见的弯曲和扭曲几何结构。法线损失进一步确保了表面平滑度和连贯性，减少了模糊区域的伪影。</p>
<p>这些结果表明，通过大规模全景数据和显式建模球面几何，可以实现高质量、鲁棒的360°x180°几何估计，为高保真3D场景应用（如沉浸式3D场景创建、AR/VR、机器人仿真、物理仿真等）铺平了道路。</p>
<p><strong>4. 论文中提到的局限性</strong>
尽管DA<script type="math/tex">^2</script>表现出色，但论文也指出了一些局限性：
*   <strong>分辨率限制：</strong> 训练分辨率（1024x512）低于更高清晰度格式（如2K或4K），可能导致DA<script type="math/tex">^2</script>偶尔会遗漏精细细节（例如，图7(a)中灯的预测距离与桌面表面错误对齐）。
*   <strong>GT深度数据不完整：</strong> 策展的透视数据仅提供球面空间中部分可用的GT深度，这可能导致在全景图像的左右边界处出现可见的接缝（图7(b)）。</p>
<p><strong>5. 潜在的未来研究方向</strong>
论文没有明确提出未来的研究方向，但从其局限性可以推断出以下潜在方向：
*   <strong>更高分辨率的训练和推理：</strong> 探索在更高分辨率下训练DA<script type="math/tex">^2</script>，以捕捉更精细的几何细节。
*   <strong>更完善的GT深度生成：</strong> 改进数据策展引擎，以生成更完整、更准确的球面GT深度，从而解决边界接缝问题。
*   <strong>更复杂的球面几何建模：</strong> 进一步研究和开发更先进的球面几何建模技术，以应对更复杂的畸变和场景。
*   <strong>多模态数据融合：</strong> 探索将全景深度估计与其他模态数据（如语义信息、激光雷达数据等）融合，以进一步提高性能和鲁棒性。</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address these challenges, we propose
<script type="math/tex">\textbf{DA}</script>
<script type="math/tex">^{\textbf{2}}</script>: <script type="math/tex">\textbf{D}</script>epth <script type="math/tex">\textbf{A}</script>nything in
<script type="math/tex">\textbf{A}</script>ny <script type="math/tex">\textbf{D}</script>irection, an accurate, zero-shot generalizable, and
fully end-to-end panoramic depth estimator.</li>
<li>Specifically, for scaling up
panoramic data, we introduce a data curation engine for generating high-quality
panoramic depth data from perspective, and create <script type="math/tex">\sim</script>543K panoramic
RGB-depth pairs, bringing the total to <script type="math/tex">\sim</script>607K.</li>
<li>To further mitigate the
spherical distortions, we present SphereViT, which explicitly leverages
spherical coordinates to enforce the spherical geometric consistency in
panoramic image features, yielding improved performance.</li>
<li>Surprisingly, DA<script type="math/tex">^{2}</script> even outperforms prior in-domain
methods, highlighting its superior zero-shot generalization.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.26618v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.26618v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.26604v1'></a></p>
<h2 id="video-object-segmentation-aware-audio-generation"><a href="https://arxiv.org/abs/2509.26604v1">Video Object Segmentation-Aware Audio Generation</a></h2>
<p><strong>Authors:</strong> Ilpo Viertola, Vladimir Iashin, Esa Rahtu</p>
<p><strong>Published:</strong> 2025-09-30</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Existing multimodal audio generation models often lack precise user control,
which limits their applicability in professional Foley workflows. In
particular, these models focus on the entire video and do not provide precise
methods for prioritizing a specific object within a scene, generating
unnecessary background sounds, or focusing on the wrong objects. To address
this gap, we introduce the novel task of video object segmentation-aware audio
generation, which explicitly conditions sound synthesis on object-level
segmentation maps. We present SAGANet, a new multimodal generative model that
enables controllable audio generation by leveraging visual segmentation masks
along with video and textual cues. Our model provides users with fine-grained
and visually localized control over audio generation. To support this task and
further research on segmentation-aware Foley, we propose Segmented Music Solos,
a benchmark dataset of musical instrument performance videos with segmentation
information. Our method demonstrates substantial improvements over current
state-of-the-art methods and sets a new standard for controllable,
high-fidelity Foley synthesis. Code, samples, and Segmented Music Solos are
available at https://saganet.notion.site</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Ilpo Viertola, Vladimir Iashin, Esa Rahtu撰写的论文“Video Object Segmentation-Aware Audio Generation”的全面摘要。</p>
<hr />
<h3 id="_1">论文摘要：视频对象分割感知音频生成</h3>
<p><strong>1. 主要问题或研究问题：</strong>
现有的大多数多模态音频生成模型在专业拟音（Foley）工作流程中缺乏精确的用户控制。这些模型通常关注整个视频，无法精确地优先处理场景中的特定对象，导致生成不必要的背景声音或关注错误的对象。这限制了它们在需要精细控制的场景中的实际应用。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
为了解决上述问题，作者引入了一项新颖的任务：<strong>视频对象分割感知音频生成（video object segmentation-aware audio generation）</strong>，它明确地将声音合成条件化到对象级别的分割图上。为此，论文提出了以下关键创新：</p>
<ul>
<li><strong>SAGANet 模型：</strong> 提出了一种新的多模态生成模型SAGANet，通过利用视觉分割掩码、视频和文本提示，实现可控的音频生成。该模型为用户提供了对音频生成过程的精细和视觉局部化控制。</li>
<li><strong>自监督控制模块：</strong> 在预训练网络的基础上开发了一个自监督控制模块，允许用户选择视频中的特定对象来生成声音。该模块通过结合全局和局部视觉特征，以及门控交叉注意力层，将分割信息引入特征提取阶段，从而实现更好的可控性、时间同步性和语义质量。</li>
<li><strong>Focal Prompt 机制：</strong> 引入了Focal Prompt，它结合了原始未修改的视觉流及其掩码流，以及围绕感兴趣区域裁剪的视觉流及其掩码，从而提供了全局概览和目标区域的详细视图。</li>
<li><strong>Segmented Music Solos 数据集：</strong> 为了支持这项任务和进一步研究分割感知的拟音，作者提出了一个基准数据集Segmented Music Solos。该数据集包含带有分割信息的乐器演奏视频，旨在提供高质量的、具有声音对象分割图和高视听对应关系的数据。该数据集通过多阶段管道（包括源视频收集、视觉验证、听觉验证、视频剪辑和掩码生成）构建。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong>
SAGANet模型在多项评估指标上均显著优于现有最先进的方法，并在可控、高保真拟音合成方面树立了新标准。</p>
<ul>
<li><strong>性能提升：</strong> SAGANet在分布匹配（Fréchet Distance, Kullback-Leibler Distance）、音频质量（Inception Score）、语义对齐（ImageBind Score）和时间对齐（DeSync）等所有指标上均表现出显著改进。尤其在时间同步方面，改进最为显著。</li>
<li><strong>泛化能力：</strong> 尽管模型仅使用单源样本进行训练，但它在多源场景中表现出强大的泛化能力，能够有效地聚焦于目标对象，即使场景中存在多个发声乐器。</li>
<li><strong>LoRA微调：</strong> 通过使用低秩适应（LoRA）对与视觉分割感知特征相关的DiT层进行微调，进一步提升了生成模型的性能，使其更好地适应这些特征。</li>
<li><strong>消融研究：</strong> 结果表明，融合局部和全局特征对于实现卓越的时间性能和保持可比的音频质量至关重要。结合全局上下文和详细局部信息对于生成高质量音频具有重要意义。</li>
</ul>
<p><strong>4. 论文中提到的局限性：</strong>
论文中没有明确指出当前方法的具体局限性，但可以从其研究动机和方法中推断出一些潜在的方面：</p>
<ul>
<li><strong>数据标注成本：</strong> 尽管Segmented Music Solos数据集的构建采用了半自动化流程，但测试数据的掩码生成仍需要手动提供位置坐标来提示SAM2，这表明高质量的分割掩码标注仍然可能是一个耗时且资源密集的过程。</li>
<li><strong>模型复杂性：</strong> SAGANet建立在MMAudio模型之上，并引入了额外的控制模块和特征提取流程，这可能增加了模型的整体复杂性和计算开销。</li>
<li><strong>泛化范围：</strong> 尽管模型在多源音乐场景中表现出良好的泛化能力，但其在更广泛、更复杂的拟音场景（例如非乐器声音）中的泛化能力仍有待进一步验证。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong>
论文为未来的研究奠定了基础，可以从以下几个方面进行探索：</p>
<ul>
<li><strong>更广泛的拟音场景：</strong> 将分割感知音频生成扩展到更广泛的拟音场景，包括日常物体、环境声音等，而不仅仅是乐器演奏。</li>
<li><strong>更精细的用户交互：</strong> 探索更直观、更精细的用户交互方式，例如通过手势、眼动追踪或更复杂的自然语言描述来指定目标对象和声音属性。</li>
<li><strong>实时生成：</strong> 优化模型以实现更接近实时的音频生成，以满足专业拟音工作流程中对效率的需求。</li>
<li><strong>多模态融合的深度探索：</strong> 进一步研究不同模态（视频、文本、分割掩码）之间更深层次的融合机制，以提高生成音频的语义准确性和时间一致性。</li>
<li><strong>自动化掩码生成：</strong> 探索更先进的自动化分割掩码生成技术，以减少对人工标注的依赖，从而降低数据集构建成本。</li>
</ul>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address
this gap, we introduce the novel task of video object segmentation-aware audio
generation, which explicitly conditions sound synthesis on object-level
segmentation maps.</li>
<li>We present SAGANet, a new multimodal generative model that
enables controllable audio generation by leveraging visual segmentation masks
along with video and textual cues.</li>
<li>To support this task and
further research on segmentation-aware Foley, we propose Segmented Music Solos,
a benchmark dataset of musical instrument performance videos with segmentation
information.</li>
<li>Our method demonstrates substantial improvements over current
state-of-the-art methods and sets a new standard for controllable,
high-fidelity Foley synthesis.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.26604v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.26604v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.26555v1'></a></p>
<h2 id="stable-cinemetrics-structured-taxonomy-and-evaluation-for-professional-video-generation"><a href="https://arxiv.org/abs/2509.26555v1">Stable Cinemetrics : Structured Taxonomy and Evaluation for Professional Video Generation</a></h2>
<p><strong>Authors:</strong> Agneet Chatterjee, Rahim Entezari, Maksym Zhuravinskyi, Maksim Lapin, Reshinth Adithyan, Amit Raj, Chitta Baral, Yezhou Yang, Varun Jampani</p>
<p><strong>Published:</strong> 2025-09-30</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Recent advances in video generation have enabled high-fidelity video
synthesis from user provided prompts. However, existing models and benchmarks
fail to capture the complexity and requirements of professional video
generation. Towards that goal, we introduce Stable Cinemetrics, a structured
evaluation framework that formalizes filmmaking controls into four
disentangled, hierarchical taxonomies: Setup, Event, Lighting, and Camera.
Together, these taxonomies define 76 fine-grained control nodes grounded in
industry practices. Using these taxonomies, we construct a benchmark of prompts
aligned with professional use cases and develop an automated pipeline for
prompt categorization and question generation, enabling independent evaluation
of each control dimension. We conduct a large-scale human study spanning 10+
models and 20K videos, annotated by a pool of 80+ film professionals. Our
analysis, both coarse and fine-grained reveal that even the strongest current
models exhibit significant gaps, particularly in Events and Camera-related
controls. To enable scalable evaluation, we train an automatic evaluator, a
vision-language model aligned with expert annotations that outperforms existing
zero-shot baselines. SCINE is the first approach to situate professional video
generation within the landscape of video generative models, introducing
taxonomies centered around cinematic controls and supporting them with
structured evaluation pipelines and detailed analyses to guide future research.</p>
<p><strong>Analysis:</strong></p>
<p>以下是Agneet Chatterjee等人撰写的论文“Stable Cinemetrics : Structured Taxonomy and Evaluation for Professional Video Generation”的全面摘要：</p>
<p><strong>1. 论文解决的主要问题或研究问题</strong></p>
<p>该论文旨在解决现有视频生成模型和基准未能捕捉专业视频生成所需的复杂性和要求的问题。核心研究问题是：“当前的视频生成模型是否已为专业用途做好准备？”现有模型主要关注用户提供的提示生成高保真视频，但缺乏对电影制作中关键的电影控制（如镜头构图、灯光位置、事件时序等）的细粒度控制和评估。</p>
<p><strong>2. 关键创新或方法论贡献</strong></p>
<ul>
<li><strong>Stable Cinemetrics (SCINE) 框架：</strong> 引入了一个结构化的评估框架，将电影制作控制形式化为四个解耦的、分层的分类法：<strong>设置 (Setup)、事件 (Event)、灯光 (Lighting) 和摄像机 (Camera)</strong>。这些分类法共同定义了76个基于行业实践的细粒度控制节点。</li>
<li><strong>专业对齐的基准提示：</strong> 利用这些分类法构建了一个与专业用例对齐的提示基准，包括“故事驱动型”和“视觉阐述型”两种提示类型，以模拟实际电影制作流程。</li>
<li><strong>自动化提示分类和问题生成：</strong> 开发了一个自动化流程，用于将提示中的每个控制元素分类到相应的分类法节点，并生成独立的评估问题，从而实现对每个控制维度的独立评估。</li>
<li><strong>大规模人工评估：</strong> 进行了一项大规模的人工研究，涵盖了10多个模型和2万个视频，由80多名电影专业人士进行标注，确保了评估的高质量和专业性。</li>
<li><strong>自动评估器训练：</strong> 为了实现可扩展的评估，训练了一个视觉-语言模型 (VLM) 作为自动评估器，该模型与专家标注对齐，并且优于现有的零样本基线。</li>
</ul>
<p><strong>3. 主要结果及其意义</strong></p>
<ul>
<li><strong>模型性能差距：</strong> 粗粒度和细粒度分析均显示，即使是最强的当前模型也存在显著差距，尤其是在<strong>事件 (Events)</strong> 和<strong>摄像机 (Camera)</strong> 相关的控制方面。</li>
<li><strong>分类法维度性能差异：</strong> 模型在“设置”和“灯光”方面表现相对较好，但在处理“事件”和“摄像机”控制时面临更大挑战。例如，模型在处理原子和并发动作方面表现良好，但在因果和重叠事件方面表现不佳；在摄像机控制中，模型在“外部参数”和“轨迹”方面表现最差。</li>
<li><strong>VLM 评估器有效性：</strong> 训练的VLM在与人类标注对齐方面表现出一致性，优于现有零样本基线，证明了其在专业视频生成评估中的可扩展性潜力。</li>
<li><strong>对未来研究的指导：</strong> SCINE是第一个将专业视频生成置于视频生成模型领域的方法，通过引入以电影控制为中心的分类法、结构化评估流程和详细分析，为未来的研究提供了指导。</li>
</ul>
<p><strong>4. 论文中提到的局限性</strong></p>
<ul>
<li><strong>分类法范围：</strong> 尽管分类法是与领域专家协商开发的，但其范围受限于合作者网络的广度。电影制作术语和解释性细微差别因地区和文化而异，更广泛的专家多样性将有助于纳入全球电影控制。</li>
<li><strong>分类法节点抽象：</strong> 某些分类法节点（如色温、ISO）被抽象化处理，因为标注者难以持续感知细粒度值。</li>
<li><strong>提示生成偏差：</strong> 提示生成依赖于大型语言模型 (LLM)，其专有性质和潜在偏差可能会影响提示的语言和结构。</li>
<li><strong>VLM 评估的计算和数据限制：</strong> 零样本VLM评估受限于计算和数据资源，限制了实验的规模和范围。</li>
</ul>
<p><strong>5. 潜在的未来研究方向</strong></p>
<ul>
<li><strong>分类法扩展：</strong> 进一步扩展分类法，纳入更广泛的全球电影控制，并探索更细粒度的值（例如精确的f-stop值、色相、亮度、饱和度）。</li>
<li><strong>视频数据集分析：</strong> 利用SCINE分类法分析视频数据集的电影多样性。</li>
<li><strong>视频字幕：</strong> 将SCINE分类法作为视频字幕的结构化框架。</li>
<li><strong>模型微调和定制：</strong> 探索微调和定制解决方案，以弥合当前视频生成模型与专业制作要求之间的差距，使模型更接近实际生产用途。</li>
<li><strong>VLM 评估器改进：</strong> 进一步提高VLM评估器的可靠性和准确性，特别是针对那些目前表现较弱的控制节点。</li>
</ul>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Towards that goal, we introduce Stable Cinemetrics, a structured
evaluation framework that formalizes filmmaking controls into four
disentangled, hierarchical taxonomies: Setup, Event, Lighting, and Camera.</li>
<li>To enable scalable evaluation, we train an automatic evaluator, a
vision-language model aligned with expert annotations that outperforms existing
zero-shot baselines.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.26555v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.26555v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2509.26548v1'></a></p>
<h2 id="automated-and-scalable-sem-image-analysis-of-perovskite-solar-cell-materials-via-a-deep-segmentation-framework"><a href="https://arxiv.org/abs/2509.26548v1">Automated and Scalable SEM Image Analysis of Perovskite Solar Cell Materials via a Deep Segmentation Framework</a></h2>
<p><strong>Authors:</strong> Jian Guo Pan, Lin Wang, Xia Cai</p>
<p><strong>Published:</strong> 2025-09-30</p>
<p><strong>Categories:</strong> cond-mat.mtrl-sci, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Scanning Electron Microscopy (SEM) is indispensable for characterizing the
microstructure of thin films during perovskite solar cell fabrication. Accurate
identification and quantification of lead iodide and perovskite phases are
critical because residual lead iodide strongly influences crystallization
pathways and defect formation, while the morphology of perovskite grains
governs carrier transport and device stability. Yet current SEM image analysis
is still largely manual, limiting throughput and consistency. Here, we present
an automated deep learning-based framework for SEM image segmentation that
enables precise and efficient identification of lead iodide, perovskite and
defect domains across diverse morphologies. Built upon an improved YOLOv8x
architecture, our model named PerovSegNet incorporates two novel modules: (i)
Adaptive Shuffle Dilated Convolution Block, which enhances multi-scale and
fine-grained feature extraction through group convolutions and channel mixing;
and (ii) Separable Adaptive Downsampling module, which jointly preserves
fine-scale textures and large-scale structures for more robust boundary
recognition. Trained on an augmented dataset of 10,994 SEM images, PerovSegNet
achieves a mean Average Precision of 87.25% with 265.4 Giga Floating Point
Operations, outperforming the baseline YOLOv8x-seg by 4.08%, while reducing
model size and computational load by 24.43% and 25.22%, respectively. Beyond
segmentation, the framework provides quantitative grain-level metrics, such as
lead iodide/perovskite area and count, which can serve as reliable indicators
of crystallization efficiency and microstructural quality. These capabilities
establish PerovSegNet as a scalable tool for real-time process monitoring and
data-driven optimization of perovskite thin-film fabrication.The source code is
available at:https://github.com/wlyyj/PerovSegNet/tree/master.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Jian Guo Pan, Lin Wang, Xia Cai撰写的论文“Automated and Scalable SEM Image Analysis of Perovskite Solar Cell Materials via a Deep Segmentation Framework”的全面摘要。</p>
<p><strong>论文摘要：</strong></p>
<ol>
<li>
<p><strong>主要问题或研究问题：</strong>
    钙钛矿太阳能电池（PSCs）的微观结构表征对于其性能和稳定性至关重要，特别是准确识别和量化薄膜中的碘化铅、钙钛矿相和缺陷区域。然而，目前的扫描电子显微镜（SEM）图像分析主要依赖人工操作，效率低下且缺乏一致性。因此，研究旨在开发一种自动化、精确且可扩展的SEM图像分割框架，以克服这些挑战。</p>
</li>
<li>
<p><strong>关键创新或方法贡献：</strong>
    该论文提出了一种名为 <strong>PerovSegNet</strong> 的深度学习分割框架，它基于改进的YOLOv8x架构，并引入了两个新颖的模块：</p>
<ul>
<li><strong>(i) 自适应混洗膨胀卷积块（Adaptive Shuffle Dilated Convolution Block, ASDCB）：</strong> 该模块通过组卷积和通道混合增强多尺度和细粒度特征提取，从而提高对晶界、小颗粒和缺陷区域的辨别能力。</li>
<li><strong>(ii) 可分离自适应下采样模块（Separable Adaptive Downsampling module, SAD）：</strong> 该模块联合保留细尺度纹理和大尺度结构，以实现更鲁棒的边界识别，有效缓解传统下采样方法中常见的混叠和对小尺度纹理敏感度有限的问题。
这些模块的结合使得PerovSegNet能够更有效地处理SEM图像中常见的复杂背景噪声、模糊边界和低对比度区域。</li>
</ul>
</li>
<li>
<p><strong>主要结果及其意义：</strong></p>
<ul>
<li><strong>性能提升：</strong> PerovSegNet在包含10,994张增强SEM图像的数据集上进行训练，实现了87.25%的平均精度（mAP），优于基线YOLOv8x-seg模型4.08%。</li>
<li><strong>效率提高：</strong> 模型尺寸和计算负载分别减少了24.43%和25.22%，同时保持了高精度，证明了其轻量化和高效性。</li>
<li><strong>定量指标：</strong> 除了像素级分割，该框架还能提供定量的晶粒级指标，如碘化铅/钙钛矿的面积和数量，这些指标可作为结晶效率和微观结构质量的可靠指示器。</li>
<li><strong>微观结构与器件性能关联：</strong> 论文通过相关性分析（Pearson r）展示了图像衍生的微观结构描述符与器件PCE之间的关系，例如钙钛矿面积与PCE呈强正相关（r=0.78），缺陷面积和缺陷数量与PCE呈负相关（r=-0.27和r=-0.32）。这表明PerovSegNet能够提供与器件性能直接相关的结构信息。</li>
<li><strong>可扩展性和实时性：</strong> 这些能力使PerovSegNet成为实时过程监控和数据驱动优化钙钛矿薄膜制造的可扩展工具。</li>
</ul>
</li>
<li>
<p><strong>论文中提到的局限性：</strong></p>
<ul>
<li><strong>相关性非因果性：</strong> 报告的相关性是观察性的，并不意味着因果关系。其他因素（如成分或界面工程）也可能影响形态和PCE。</li>
<li><strong>分割误差和类别不平衡：</strong> 分割误差和类别不平衡（特别是缺陷类别）可能会传播到派生指标中。</li>
<li><strong>成像条件影响：</strong> 成像条件（电压、放大倍数、校准）也会影响定量输出。</li>
<li><strong>数据集规模和多样性：</strong> 解决上述问题需要更大、更多样化的数据集，以及不确定性量化和跨放大倍数验证。</li>
<li><strong>计算效率仍需提升：</strong> 尽管计算效率相对于强基线有所提高，但为了集成到原位或在线工作流程中，仍需进一步加速。</li>
</ul>
</li>
<li>
<p><strong>潜在的未来研究方向：</strong></p>
<ul>
<li>将框架扩展到其他材料系统。</li>
<li>进一步加速推理，以实现原位和在线应用。</li>
<li>通过轻量化设计和鲁棒的量化能力，将PerovSegNet应用于实验室规模研究、高通量筛选和钙钛矿制造中的实时质量控制。</li>
<li>通过更大、更多样化的数据集，以及不确定性量化和跨放大倍数验证，来解决相关性非因果性、分割误差和成像条件影响等局限性。</li>
</ul>
</li>
</ol>
<p>总而言之，PerovSegNet为钙钛矿太阳能电池材料的SEM图像分析提供了一个自动化、高精度和高效的解决方案。通过其创新的深度学习架构和定量分析能力，该框架有望显著推动钙钛矿薄膜制造的优化和质量控制。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Here, we present
an automated deep learning-based framework for SEM image segmentation that
enables precise and efficient identification of lead iodide, perovskite and
defect domains across diverse morphologies.</li>
<li>Built upon an improved YOLOv8x
architecture, our model named PerovSegNet incorporates two novel modules: (i)
Adaptive Shuffle Dilated Convolution Block, which enhances multi-scale and
fine-grained feature extraction through group convolutions and channel mixing;
and (ii) Separable Adaptive Downsampling module, which jointly preserves
fine-scale textures and large-scale structures for more robust boundary
recognition.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2509.26548v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2509.26548v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-10-02 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
