<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-11-05 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-04
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-11-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-12
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-11-04/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-11-06/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-11-05">Arxiv Computer Vision Papers - 2025-11-05</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#arxiv-2025-11-04" class="nav-link">Arxiv 计算机视觉论文每日报告执行摘要 (2025-11-04)</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#vcode-a-multimodal-coding-benchmark-with-svg-as-symbolic-visual-representation" class="nav-link">VCode: a Multimodal Coding Benchmark with SVG as Symbolic Visual Representation</a>
                </li>
                <li class="nav-item">
                    <a href="#perchead-perceptual-head-model-for-single-image-3d-head-reconstruction-editing" class="nav-link">PercHead: Perceptual Head Model for Single-Image 3D Head Reconstruction &amp; Editing</a>
                </li>
                <li class="nav-item">
                    <a href="#xr-1-towards-versatile-vision-language-action-models-via-learning-unified-vision-motion-representations" class="nav-link">XR-1: Towards Versatile Vision-Language-Action Models via Learning Unified Vision-Motion Representations</a>
                </li>
                <li class="nav-item">
                    <a href="#dynamic-reflections-probing-video-representations-with-text-alignment" class="nav-link">Dynamic Reflections: Probing Video Representations with Text Alignment</a>
                </li>
                <li class="nav-item">
                    <a href="#zero-shot-multi-animal-tracking-in-the-wild" class="nav-link">Zero-Shot Multi-Animal Tracking in the Wild</a>
                </li>
                <li class="nav-item">
                    <a href="#seeing-across-time-and-views-multi-temporal-cross-view-learning-for-robust-video-person-re-identification" class="nav-link">Seeing Across Time and Views: Multi-Temporal Cross-View Learning for Robust Video Person Re-Identification</a>
                </li>
                <li class="nav-item">
                    <a href="#the-urban-vision-hackathon-dataset-and-models-towards-image-annotations-and-accurate-vision-models-for-indian-traffic" class="nav-link">The Urban Vision Hackathon Dataset and Models: Towards Image Annotations and Accurate Vision Models for Indian Traffic</a>
                </li>
                <li class="nav-item">
                    <a href="#detectiumfire-a-comprehensive-multi-modal-dataset-bridging-vision-and-language-for-fire-understanding" class="nav-link">DetectiumFire: A Comprehensive Multi-modal Dataset Bridging Vision and Language for Fire Understanding</a>
                </li>
                <li class="nav-item">
                    <a href="#olatverse-a-large-scale-real-world-object-dataset-with-precise-lighting-control" class="nav-link">OLATverse: A Large-scale Real-world Object Dataset with Precise Lighting Control</a>
                </li>
                <li class="nav-item">
                    <a href="#from-the-laboratory-to-real-world-application-evaluating-zero-shot-scene-interpretation-on-edge-devices-for-mobile-robotics" class="nav-link">From the Laboratory to Real-World Application: Evaluating Zero-Shot Scene Interpretation on Edge Devices for Mobile Robotics</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-11-05">Arxiv Computer Vision Papers - 2025-11-05</h1>
<h2 id="executive-summary">Executive Summary</h2>
<h2 id="arxiv-2025-11-04">Arxiv 计算机视觉论文每日报告执行摘要 (2025-11-04)</h2>
<p><strong>1. 主要主题和趋势概述：</strong></p>
<p>今天的论文展示了计算机视觉领域持续向更复杂、更实用的应用发展，并强调了以下几个主要趋势：</p>
<ul>
<li><strong>多模态学习与跨领域融合：</strong> 显著关注将视觉与其他模态（如语言、动作、符号表示）结合，以实现更全面的理解和交互。这体现在代码生成、视觉-语言-动作模型以及多模态数据集的构建上。</li>
<li><strong>真实世界鲁棒性与泛化能力：</strong> 大量工作致力于提升模型在复杂、非受控真实世界环境中的性能，包括零样本学习、跨视图/跨时间学习以及针对特定挑战（如印度交通、火灾检测）的数据集构建。</li>
<li><strong>数据驱动与大规模数据集：</strong> 多个项目专注于创建大规模、高质量、多样的真实世界数据集，以推动模型训练和评估，尤其是在特定应用领域（如交通、火灾、光照控制）。</li>
<li><strong>边缘设备部署与实际应用：</strong> 有论文探讨了将先进视觉模型部署到边缘设备上，以实现移动机器人等实际应用，强调了效率和实时性。</li>
<li><strong>3D 重建与感知：</strong> 持续关注从2D输入重建3D信息，并进行编辑，尤其是在人脸和头部模型方面。</li>
</ul>
<p><strong>2. 特别重要或创新的论文：</strong></p>
<ul>
<li><strong>"XR-1: Towards Versatile Vision-Language-Action Models via Learning Unified Vision-Motion Representations" (Shichao Fan et al.)：</strong> 这篇论文极具创新性，因为它旨在构建一个通用的视觉-语言-动作模型，通过统一的视觉-运动表示来弥合不同模态之间的鸿沟。这代表了迈向更通用人工智能代理的重要一步，具有广泛的应用潜力。</li>
<li><strong>"VCode: a Multimodal Coding Benchmark with SVG as Symbolic Visual Representation" (Kevin Qinghong Lin et al.)：</strong> 将SVG作为符号视觉表示引入多模态编码基准，为视觉理解与代码生成之间建立了新的桥梁。这对于自动化UI/UX设计、数据可视化以及更智能的视觉编程工具具有重要意义。</li>
<li><strong>"Zero-Shot Multi-Animal Tracking in the Wild" (Jan Frederik Meier, Timo Lüddecke)：</strong> 在野外实现零样本多动物追踪是一个极具挑战性的任务，这篇论文的贡献在于其在无需特定动物训练数据的情况下，实现对未知动物的鲁棒追踪，对于生态研究、野生动物保护等领域具有直接应用价值。</li>
</ul>
<p><strong>3. 新兴研究方向或技术：</strong></p>
<ul>
<li><strong>统一的视觉-运动表示学习：</strong> "XR-1" 提出的概念，旨在创建一个能够同时理解视觉输入和运动指令的通用表示，这可能成为未来多模态AI模型的核心。</li>
<li><strong>符号视觉表示与代码生成：</strong> "VCode" 强调了将视觉信息转化为可操作的符号（如SVG），并进一步生成代码的能力，预示着视觉与编程交叉领域的新发展。</li>
<li><strong>特定领域的大规模多模态数据集构建：</strong> "DetectiumFire" 和 "The Urban Vision Hackathon Dataset" 等论文表明，针对特定复杂场景（如火灾、印度交通）构建结合视觉和语言信息的大规模数据集，是推动这些领域AI应用的关键。</li>
<li><strong>边缘设备上的零样本场景解释：</strong> "From the Laboratory to Real-World Application" 强调了在资源受限的边缘设备上实现高级视觉理解的重要性，这对于移动机器人和物联网设备的发展至关重要。</li>
</ul>
<p><strong>4. 建议阅读全文的论文：</strong></p>
<p>对于忙碌的研究人员，我强烈建议优先阅读以下论文：</p>
<ul>
<li><strong>"XR-1: Towards Versatile Vision-Language-Action Models via Learning Unified Vision-Motion Representations" (Shichao Fan et al.)：</strong> 如果您对通用人工智能、多模态学习和具身智能感兴趣，这篇论文提供了未来研究的宏大愿景和潜在方向。</li>
<li><strong>"VCode: a Multimodal Coding Benchmark with SVG as Symbolic Visual Representation" (Kevin Qinghong Lin et al.)：</strong> 对于关注视觉与语言交叉、代码生成、UI/UX自动化或符号AI的研究人员，这篇论文提供了新颖的视角和基准。</li>
<li><strong>"Zero-Shot Multi-Animal Tracking in the Wild" (Jan Frederik Meier, Timo Lüddecke)：</strong> 如果您的研究涉及零样本学习、目标追踪或野生动物监测，这篇论文展示了在极具挑战性的真实世界场景中实现鲁棒性的方法。</li>
<li><strong>"PercHead: Perceptual Head Model for Single-Image 3D Head Reconstruction &amp; Editing" (Antonio Oroz, Matthias Nießner, Tobias Kirschstein)：</strong> 对于专注于3D重建、人脸/头部建模或虚拟现实/增强现实应用的研究人员，这篇论文提供了高质量的单图像3D重建和编辑方法。</li>
</ul>
<p>这些论文代表了当前计算机视觉领域的前沿进展，涵盖了从基础理论到实际应用的关键方向。深入阅读它们将有助于您全面了解该领域的最新动态和未来趋势。</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2511.02778v1">VCode: a Multimodal Coding Benchmark with SVG as Symbolic Visual Representation</a></li>
<li><a href="#2511.02777v1">PercHead: Perceptual Head Model for Single-Image 3D Head Reconstruction &amp; Editing</a></li>
<li><a href="#2511.02776v1">XR-1: Towards Versatile Vision-Language-Action Models via Learning Unified Vision-Motion Representations</a></li>
<li><a href="#2511.02767v1">Dynamic Reflections: Probing Video Representations with Text Alignment</a></li>
<li><a href="#2511.02591v1">Zero-Shot Multi-Animal Tracking in the Wild</a></li>
<li><a href="#2511.02564v1">Seeing Across Time and Views: Multi-Temporal Cross-View Learning for Robust Video Person Re-Identification</a></li>
<li><a href="#2511.02563v1">The Urban Vision Hackathon Dataset and Models: Towards Image Annotations and Accurate Vision Models for Indian Traffic</a></li>
<li><a href="#2511.02495v1">DetectiumFire: A Comprehensive Multi-modal Dataset Bridging Vision and Language for Fire Understanding</a></li>
<li><a href="#2511.02483v1">OLATverse: A Large-scale Real-world Object Dataset with Precise Lighting Control</a></li>
<li><a href="#2511.02427v1">From the Laboratory to Real-World Application: Evaluating Zero-Shot Scene Interpretation on Edge Devices for Mobile Robotics</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2511.02778v1'></a></p>
<h2 id="vcode-a-multimodal-coding-benchmark-with-svg-as-symbolic-visual-representation"><a href="https://arxiv.org/abs/2511.02778v1">VCode: a Multimodal Coding Benchmark with SVG as Symbolic Visual Representation</a></h2>
<p><strong>Authors:</strong> Kevin Qinghong Lin, Yuhao Zheng, Hangyu Ran, Dantong Zhu, Dongxing Mao, Linjie Li, Philip Torr, Alex Jinpeng Wang</p>
<p><strong>Published:</strong> 2025-11-04</p>
<p><strong>Categories:</strong> cs.CV, cs.CL</p>
<p><strong>Abstract:</strong></p>
<p>Code has emerged as a precise and executable medium for reasoning and action
in the agent era. Yet, progress has largely focused on language-centric tasks
such as program synthesis and debugging, leaving visual-centric coding
underexplored. Inspired by how humans reason over sketches, we advocate SVG
code as a compact, interpretable, and executable visual representation. We
introduce VCode, a benchmark that reframes multimodal understanding as code
generation: given an image, a model must produce SVG that preserves symbolic
meaning for downstream reasoning. VCode covers three domains - general
commonsense (MM-Vet), professional disciplines (MMMU), and visual-centric
perception (CV-Bench). To assess symbolic fidelity, we propose CodeVQA, a novel
evaluation protocol in which a policy model answers questions over rendered
SVGs; correct answers indicate faithful symbolic preservation. Empirically,
frontier VLMs struggle to generate faithful SVGs, revealing a persistent gap
between language-centric and visual-centric coding. To close this gap, we
introduce VCoder, an agentic framework that augments VLMs along two axes: (i)
Thinking with Revision, which iteratively analyzes discrepancies and refines
SVG code; and (ii) Acting with Visual Tools, where detectors and parsers supply
structured cues such as objects, shapes, and text beyond the model's intrinsic
capacity. Across benchmarks, frontier VLMs with strong reasoning capabilities
score well overall yet remain limited in professional knowledge and 3D
reasoning. VCoder delivers a 12.3-point overall gain over the top-performing
Claude-4-Opus. Human studies show that both humans and VLMs perform worse on
rendered SVGs, their consistency reveals the promise of symbolic visual
representation. The benchmark and code are available at
https://github.com/CSU-JPG/VCode.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将对这篇论文摘要进行详细分析。</p>
<hr />
<h3 id="vcode-a-multimodal-coding-benchmark-with-svg-as-symbolic-visual-representation_1">论文摘要分析：VCode: a Multimodal Coding Benchmark with SVG as Symbolic Visual Representation</h3>
<p><strong>1. 论文主要贡献的简洁总结 (2-3 句话)</strong></p>
<p>这篇论文的核心贡献是引入了 <strong>VCode</strong>，一个开创性的多模态编码基准，它将多模态理解重新定义为 <strong>SVG 代码生成</strong>。通过将图像转换为可解释、可执行的 SVG 代码，VCode 旨在评估模型在视觉中心编码任务中的能力，并揭示了当前视觉语言模型 (VLMs) 在符号保真度方面的不足。为了弥补这一差距，论文还提出了 <strong>VCoder</strong>，一个结合了迭代修订和视觉工具的智能体框架，显著提升了 SVG 生成的准确性。</p>
<p><strong>2. 关键创新或方法论方法</strong></p>
<ul>
<li><strong>SVG 作为符号视觉表示：</strong> 论文最核心的创新在于倡导并利用 <strong>SVG (Scalable Vector Graphics)</strong> 作为一种紧凑、可解释、可执行的符号视觉表示。这与传统的像素级或特征级表示不同，SVG 能够捕获图像的结构和语义信息，使其成为推理和行动的理想媒介。</li>
<li><strong>将多模态理解重构为代码生成：</strong> 论文将“给定图像生成 SVG 代码”这一任务作为评估多模态理解的新范式。这使得模型不仅要理解图像内容，还要将其转化为精确的、可执行的符号表示。</li>
<li><strong>CodeVQA 评估协议：</strong> 为了评估 SVG 生成的符号保真度，论文提出了 <strong>CodeVQA</strong>。这是一种新颖的评估方法，通过让一个策略模型对渲染的 SVG 图像进行问答，来判断生成的 SVG 是否忠实地保留了原始图像的符号意义。这比简单的像素级比较更能反映语义层面的准确性。</li>
<li><strong>VCoder 智能体框架：</strong> 为了解决现有 VLMs 在 SVG 生成上的不足，VCoder 引入了两个关键机制：<ul>
<li><strong>Thinking with Revision (迭代修订)：</strong> 模型能够分析生成 SVG 与原始图像之间的差异，并迭代地修正 SVG 代码，这模仿了人类在编程和设计中的调试过程。</li>
<li><strong>Acting with Visual Tools (视觉工具辅助)：</strong> VCoder 利用外部视觉工具（如检测器和解析器）来提取结构化线索，例如对象、形状和文本，这些信息超越了 VLM 自身的内在能力，为 SVG 生成提供了更丰富的上下文。</li>
</ul>
</li>
</ul>
<p><strong>3. 对领域潜在影响</strong></p>
<ul>
<li><strong>推动视觉中心编码研究：</strong> VCode 基准的引入将极大地推动计算机视觉和多模态学习领域对“视觉中心编码”的研究，填补了当前主要关注语言中心任务的空白。</li>
<li><strong>新的评估范式：</strong> CodeVQA 协议为评估多模态模型在符号理解和生成方面的能力提供了一个更严格、更具语义的框架，超越了传统的图像字幕或视觉问答。</li>
<li><strong>促进可解释和可控的视觉生成：</strong> SVG 作为一种符号表示，其可解释性和可编辑性远超像素图像。这项研究可能启发更多基于符号表示的视觉生成模型，从而实现更可控、更易于调试的视觉内容创作。</li>
<li><strong>智能体和具身智能的发展：</strong> 将视觉理解转化为可执行代码的能力，对于构建能够与环境进行复杂交互的智能体（Agent）至关重要。VCode 和 VCoder 为智能体在视觉推理和行动方面提供了新的方向。</li>
<li><strong>人机交互和设计自动化：</strong> 能够将草图或图像转化为可编辑的矢量图形，对于自动化设计、用户界面生成以及更自然的人机交互具有巨大潜力。</li>
</ul>
<p><strong>4. 相关领域或应用受益</strong></p>
<ul>
<li><strong>多模态大模型 (VLMs) 的发展：</strong> VCode 将成为评估和训练下一代 VLMs 的重要基准，促使它们在视觉中心推理和代码生成方面取得突破。</li>
<li><strong>具身智能和机器人：</strong> 机器人需要将视觉感知转化为可执行的动作指令。SVG 作为一种符号表示，可以作为机器人规划和控制的中间语言。</li>
<li><strong>图形设计和用户界面 (UI) 生成：</strong> 自动将草图或图像转化为 SVG 代码，可以极大地加速图形设计和 UI 原型开发过程。</li>
<li><strong>数据可视化：</strong> 从图像中提取结构化信息并生成 SVG，有助于自动化数据图表的创建和编辑。</li>
<li><strong>计算机辅助设计 (CAD)：</strong> 将手绘草图或照片转化为 CAD 软件可用的矢量图形，简化设计流程。</li>
<li><strong>教育和辅助技术：</strong> 将复杂视觉信息转化为可编辑的符号表示，可能有助于视觉障碍者理解图像内容，或用于教学目的。</li>
</ul>
<p><strong>5. 从摘要中推断出的局限性</strong></p>
<ul>
<li><strong>SVG 的表达能力限制：</strong> 尽管 SVG 强大，但它主要擅长表示几何形状和文本。对于高度复杂的、纹理丰富的、或具有微妙光影变化的真实世界图像，将其完全忠实地转换为 SVG 可能会面临挑战，或者生成的 SVG 会异常复杂。摘要中提到“保留符号意义”，暗示可能并非所有视觉细节都能被完美编码。</li>
<li><strong>CodeVQA 评估的策略模型依赖：</strong> CodeVQA 的有效性依赖于“策略模型”回答问题的能力。如果策略模型本身存在偏差或局限性，可能会影响对 SVG 符号保真度的准确评估。</li>
<li><strong>VCoder 的工具依赖性：</strong> VCoder 框架依赖于外部的“视觉工具”（检测器和解析器）。这些工具的性能上限将直接限制 VCoder 的整体表现。如果这些工具在特定领域（如专业知识或 3D 推理）表现不佳，VCoder 也会受限。</li>
<li><strong>专业知识和 3D 推理的挑战：</strong> 摘要明确指出，“frontier VLMs with strong reasoning capabilities score well overall yet remain limited in professional knowledge and 3D reasoning”。这表明即使是 VCoder 这样的增强框架，在处理需要深厚专业领域知识或复杂三维几何理解的图像时，可能仍然面临显著挑战。</li>
<li><strong>人类研究的局限性：</strong> 摘要提到“Human studies show that both humans and VLMs perform worse on rendered SVGs, their consistency reveals the promise of symbolic visual representation.” 这句话有点模棱两可。它可能意味着：<ul>
<li>人类和 VLM 在直接理解渲染的 SVG 图像时，比理解原始图像更困难（这可能暗示 SVG 渲染后丢失了某些人类或 VLM 习惯的视觉线索）。</li>
<li>或者，人类和 VLM 在对 SVG 进行问答时，表现不如对原始图像进行问答。</li>
<li>无论哪种情况，这都可能暗示 SVG 作为一种“视觉”表示，在某些方面可能不如原始像素图像直观或信息丰富，至少对于当前的人类和模型而言。这可能需要进一步的研究来优化 SVG 的生成和渲染，使其更易于理解。</li>
</ul>
</li>
</ul>
<hr />
<p>总的来说，VCode 是一项非常有趣且具有前瞻性的工作，它为多模态理解和生成开辟了一个新的方向，强调了符号表示在智能体时代的重要性。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To assess symbolic fidelity, we propose CodeVQA, a novel
evaluation protocol in which a policy model answers questions over rendered
SVGs; correct answers indicate faithful symbolic preservation.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.02778v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.02778v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.02777v1'></a></p>
<h2 id="perchead-perceptual-head-model-for-single-image-3d-head-reconstruction-editing"><a href="https://arxiv.org/abs/2511.02777v1">PercHead: Perceptual Head Model for Single-Image 3D Head Reconstruction &amp; Editing</a></h2>
<p><strong>Authors:</strong> Antonio Oroz, Matthias Nießner, Tobias Kirschstein</p>
<p><strong>Published:</strong> 2025-11-04</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We present PercHead, a method for single-image 3D head reconstruction and
semantic 3D editing - two tasks that are inherently challenging due to severe
view occlusions, weak perceptual supervision, and the ambiguity of editing in
3D space. We develop a unified base model for reconstructing view-consistent 3D
heads from a single input image. The model employs a dual-branch encoder
followed by a ViT-based decoder that lifts 2D features into 3D space through
iterative cross-attention. Rendering is performed using Gaussian Splatting. At
the heart of our approach is a novel perceptual supervision strategy based on
DINOv2 and SAM2.1, which provides rich, generalized signals for both geometric
and appearance fidelity. Our model achieves state-of-the-art performance in
novel-view synthesis and, furthermore, exhibits exceptional robustness to
extreme viewing angles compared to established baselines. Furthermore, this
base model can be seamlessly extended for semantic 3D editing by swapping the
encoder and finetuning the network. In this variant, we disentangle geometry
and style through two distinct input modalities: a segmentation map to control
geometry and either a text prompt or a reference image to specify appearance.
We highlight the intuitive and powerful 3D editing capabilities of our model
through a lightweight, interactive GUI, where users can effortlessly sculpt
geometry by drawing segmentation maps and stylize appearance via natural
language or image prompts.
  Project Page: https://antoniooroz.github.io/PercHead Video:
https://www.youtube.com/watch?v=4hFybgTk4kE</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将对这篇论文摘要进行详细分析。</p>
<hr />
<h3 id="perchead-perceptual-head-model-for-single-image-3d-head-reconstruction-editing_1">论文摘要分析：PercHead: Perceptual Head Model for Single-Image 3D Head Reconstruction &amp; Editing</h3>
<p><strong>1. 论文主要贡献的简洁总结 (2-3 句话)</strong></p>
<p>PercHead 提出了一种从单张图像进行 3D 头部重建和语义 3D 编辑的统一方法。它通过结合双分支编码器、基于 ViT 的解码器和高斯泼溅渲染，实现了视图一致的 3D 头部重建，并引入了基于 DINOv2 和 SAM2.1 的新型感知监督策略，显著提升了几何和外观的真实感。该模型在新视角合成方面达到了最先进水平，并能通过替换编码器无缝扩展到语义 3D 编辑，实现几何与风格的解耦控制。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<p>PercHead 的核心创新在于其<strong>新型感知监督策略</strong>和<strong>统一的重建与编辑框架</strong>。</p>
<ul>
<li><strong>感知监督策略：</strong> 论文利用了 DINOv2 和 SAM2.1 这两个强大的预训练模型。DINOv2 提供丰富的、泛化的特征表示，有助于捕捉细粒度的几何和外观信息，即使在弱监督或遮挡严重的情况下也能提供强大的信号。SAM2.1（或其前身 SAM）则提供高质量的语义分割能力，这对于理解头部结构和实现精确的几何编辑至关重要。这种结合利用了大型视觉模型（LVMs）的强大泛化能力，为 3D 重建提供了前所未有的“感知”指导，解决了传统方法中“弱感知监督”的挑战。</li>
<li><strong>统一的重建与编辑框架：</strong> 模型设计了一个双分支编码器和基于 ViT 的解码器，通过迭代交叉注意力将 2D 特征提升到 3D 空间，并使用高斯泼溅进行渲染。更重要的是，这个基础模型可以<strong>无缝扩展</strong>到语义 3D 编辑。通过替换编码器并微调网络，它能够解耦几何（通过分割图控制）和风格（通过文本提示或参考图像控制），这提供了一个高度灵活且直观的 3D 编辑界面。这种模块化设计使得一个模型能够同时解决两个复杂且相互关联的任务。</li>
<li><strong>鲁棒性：</strong> 摘要特别强调了模型在极端视角下的卓越鲁棒性，这表明其在处理复杂真实世界场景方面具有优势。</li>
</ul>
<p><strong>3. 对领域潜在影响</strong></p>
<ul>
<li><strong>推动单图像 3D 重建的边界：</strong> 解决单图像 3D 重建中长期存在的遮挡、弱监督和歧义问题，尤其是在头部这种复杂且具有高度可变性的对象上。</li>
<li><strong>LVMs 在 3D 领域的应用范式：</strong> 展示了如何有效利用 DINOv2 和 SAM2.1 等大型视觉模型（LVMs）的强大感知能力来指导 3D 几何和外观的生成，为未来 3D 任务中 LVMs 的应用开辟了新思路。</li>
<li><strong>交互式 3D 内容创作：</strong> 提供了一个直观且强大的 3D 编辑工具，通过自然语言和分割图实现对 3D 模型的精细控制，极大地降低了 3D 内容创作的门槛，使得非专业用户也能进行高质量的 3D 头部雕刻和风格化。</li>
<li><strong>高斯泼溅的进一步应用：</strong> 再次证明了高斯泼溅在实时渲染和新视角合成方面的强大潜力，并将其与更复杂的 3D 重建和编辑任务相结合。</li>
</ul>
<p><strong>4. 相关领域或应用</strong></p>
<ul>
<li><strong>虚拟现实 (VR) / 增强现实 (AR)：</strong> 快速生成高保真 3D 头像，用于虚拟社交、游戏或 AR 滤镜。</li>
<li><strong>电影和游戏产业：</strong> 简化 3D 角色建模和动画流程，实现快速原型设计和个性化定制。</li>
<li><strong>数字人与元宇宙：</strong> 为创建逼真、可编辑的数字人提供核心技术支持。</li>
<li><strong>人机交互：</strong> 通过 3D 头部模型实现更自然的交互体验，例如虚拟试戴、虚拟化妆等。</li>
<li><strong>计算机图形学：</strong> 探索新的 3D 表示和渲染技术，以及 2D 到 3D 的特征提升方法。</li>
<li><strong>医学影像：</strong> 潜在地可用于从单张 2D 图像重建 3D 解剖结构（尽管头部重建更侧重于外观，但其几何重建能力有借鉴意义）。</li>
</ul>
<p><strong>5. 从摘要中可推断的局限性</strong></p>
<ul>
<li><strong>计算资源需求：</strong> 结合 DINOv2、SAM2.1、ViT-based 解码器和高斯泼溅，模型可能具有较高的计算复杂度和内存需求，尤其是在训练和实时编辑时。</li>
<li><strong>泛化能力限制：</strong> 尽管使用了强大的感知监督，但模型的泛化能力可能仍受限于训练数据的多样性。例如，对于极端非人类头部特征或高度风格化的艺术形象，其重建和编辑效果可能不如对标准人脸。</li>
<li><strong>编辑粒度：</strong> 摘要提到通过分割图控制几何，通过文本/图像控制外观。虽然强大，但对于更细粒度的几何细节（例如，调整鼻子大小的微小变化，而非整体形状）或更复杂的材质属性（例如，皮肤的微观纹理），其控制精度和直观性可能仍有待进一步验证。</li>
<li><strong>实时性：</strong> 尽管高斯泼溅渲染速度快，但整个重建和编辑流程（特别是涉及迭代交叉注意力）是否能达到完全实时的交互体验，仍需在实际系统中进行评估。摘要中提到的“轻量级、交互式 GUI”暗示了其对实时性的追求，但具体性能未知。</li>
<li><strong>“无缝扩展”的成本：</strong> 尽管声称“无缝扩展”，但“替换编码器和微调网络”这一步骤仍需要额外的训练数据和计算资源，并非完全零成本。</li>
</ul>
<hr />
<p>总而言之，PercHead 是一项令人兴奋的研究，它巧妙地结合了最新的大型视觉模型和 3D 渲染技术，为单图像 3D 头部重建和语义编辑带来了显著的进步。其在感知监督和统一框架方面的创新，有望对 3D 内容创作和人机交互领域产生深远影响。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present PercHead, a method for single-image 3D head reconstruction and
semantic 3D editing - two tasks that are inherently challenging due to severe
view occlusions, weak perceptual supervision, and the ambiguity of editing in
3D space.</li>
<li>We develop a unified base model for reconstructing view-consistent 3D
heads from a single input image.</li>
<li>At
the heart of our approach is a novel perceptual supervision strategy based on
DINOv2 and SAM2.1, which provides rich, generalized signals for both geometric
and appearance fidelity.</li>
<li>Our model achieves state-of-the-art performance in
novel-view synthesis and, furthermore, exhibits exceptional robustness to
extreme viewing angles compared to established baselines.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.02777v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.02777v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.02776v1'></a></p>
<h2 id="xr-1-towards-versatile-vision-language-action-models-via-learning-unified-vision-motion-representations"><a href="https://arxiv.org/abs/2511.02776v1">XR-1: Towards Versatile Vision-Language-Action Models via Learning Unified Vision-Motion Representations</a></h2>
<p><strong>Authors:</strong> Shichao Fan, Kun Wu, Zhengping Che, Xinhua Wang, Di Wu, Fei Liao, Ning Liu, Yixue Zhang, Zhen Zhao, Zhiyuan Xu, Meng Li, Qingjie Liu, Shanghang Zhang, Min Wan, Jian Tang</p>
<p><strong>Published:</strong> 2025-11-04</p>
<p><strong>Categories:</strong> cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>Recent progress in large-scale robotic datasets and vision-language models
(VLMs) has advanced research on vision-language-action (VLA) models. However,
existing VLA models still face two fundamental challenges: (i) producing
precise low-level actions from high-dimensional observations, (ii) bridging
domain gaps across heterogeneous data sources, including diverse robot
embodiments and human demonstrations. Existing methods often encode latent
variables from either visual dynamics or robotic actions to guide policy
learning, but they fail to fully exploit the complementary multi-modal
knowledge present in large-scale, heterogeneous datasets. In this work, we
present X Robotic Model 1 (XR-1), a novel framework for versatile and scalable
VLA learning across diverse robots, tasks, and environments. XR-1 introduces
the \emph{Unified Vision-Motion Codes (UVMC)}, a discrete latent representation
learned via a dual-branch VQ-VAE that jointly encodes visual dynamics and
robotic motion. UVMC addresses these challenges by (i) serving as an
intermediate representation between the observations and actions, and (ii)
aligning multimodal dynamic information from heterogeneous data sources to
capture complementary knowledge. To effectively exploit UVMC, we propose a
three-stage training paradigm: (i) self-supervised UVMC learning, (ii)
UVMC-guided pretraining on large-scale cross-embodiment robotic datasets, and
(iii) task-specific post-training. We validate XR-1 through extensive
real-world experiments with more than 14,000 rollouts on six different robot
embodiments, spanning over 120 diverse manipulation tasks. XR-1 consistently
outperforms state-of-the-art baselines such as <script type="math/tex">\pi_{0.5}</script>, <script type="math/tex">\pi_0</script>, RDT,
UniVLA, and GR00T-N1.5 while demonstrating strong generalization to novel
objects, background variations, distractors, and illumination changes. Our
project is at https://xr-1-vla.github.io/.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将对这篇论文摘要进行深入分析。</p>
<hr />
<p><strong>论文摘要分析：XR-1: Towards Versatile Vision-Language-Action Models via Learning Unified Vision-Motion Representations</strong></p>
<p><strong>1. 论文主要贡献的简洁总结 (2-3 句话)</strong></p>
<p>这篇论文提出了XR-1框架，旨在解决现有视觉-语言-动作 (VLA) 模型在从高维观测生成精确低级动作以及跨异构数据源（如不同机器人形态和人类演示）弥合领域差距的挑战。其核心贡献是引入了“统一视觉-运动编码 (UVMC)”，这是一种通过双分支VQ-VAE学习的离散潜在表示，能够联合编码视觉动态和机器人运动，从而实现多功能和可扩展的VLA学习。</p>
<p><strong>2. 关键创新或方法论方法</strong></p>
<p>关键创新在于<strong>统一视觉-运动编码 (UVMC)</strong>。具体来说：</p>
<ul>
<li><strong>双分支VQ-VAE学习：</strong> UVMC通过一个双分支的VQ-VAE（Vector Quantized Variational Autoencoder）学习，该VQ-VAE能够同时对视觉动态（来自高维观测）和机器人运动（低级动作）进行编码。这种联合编码确保了视觉信息和动作信息之间的紧密对齐和互补利用。</li>
<li><strong>离散潜在表示：</strong> UVMC是一种离散的潜在表示，这对于处理异构数据源和实现更好的泛化能力通常是有益的，因为它能提供更结构化和可解释的中间表示。</li>
<li><strong>中间表示和多模态对齐：</strong> UVMC作为观测和动作之间的中间表示，有效地桥接了高维视觉输入和低级动作输出之间的鸿沟。同时，它通过对齐来自异构数据源的多模态动态信息，捕获了互补的知识，从而解决了领域差距问题。</li>
<li><strong>三阶段训练范式：</strong> 为了有效利用UVMC，论文提出了一种新颖的三阶段训练范式：<ol>
<li><strong>自监督UVMC学习：</strong> 首先，通过自监督方式学习UVMC，使其能够有效地捕捉视觉和运动的内在结构。</li>
<li><strong>UVMC引导的预训练：</strong> 接着，在大规模跨形态机器人数据集上进行UVMC引导的预训练，利用UVMC作为指导信号，提升模型在不同机器人上的泛化能力。</li>
<li><strong>任务特定后训练：</strong> 最后，针对特定任务进行微调，以优化模型在具体任务上的性能。</li>
</ol>
</li>
</ul>
<p><strong>3. 对领域潜在影响</strong></p>
<p>XR-1的提出对计算机视觉和机器人学习领域具有显著的潜在影响：</p>
<ul>
<li><strong>推动通用机器人学习：</strong> 通过解决跨异构数据源的领域差距问题，XR-1为构建更通用、更具适应性的机器人模型铺平了道路，使其能够从多样化的数据中学习，并应用于不同形态的机器人。</li>
<li><strong>提升VLA模型的精度和鲁棒性：</strong> UVMC作为观测和动作之间的有效中间表示，有望显著提高VLA模型从高维输入生成精确低级动作的能力，并增强其对新物体、背景变化、干扰物和光照变化的泛化能力。</li>
<li><strong>促进多模态学习的融合：</strong> 论文强调了充分利用大规模异构数据集中互补多模态知识的重要性，UVMC的设计正是这一理念的体现，将推动视觉、语言和动作之间更深层次的融合。</li>
<li><strong>为未来具身智能研究提供基础：</strong> 能够处理多样化机器人形态和任务的能力，是实现真正具身智能的关键一步。XR-1为构建能够理解和执行复杂任务的智能机器人系统提供了新的方法论。</li>
</ul>
<p><strong>4. 相关领域或应用受益</strong></p>
<ul>
<li><strong>具身智能 (Embodied AI)：</strong> 这是最直接受益的领域，XR-1旨在解决具身智能中的核心挑战，即如何让机器人从视觉输入中学习并执行物理动作。</li>
<li><strong>机器人操作 (Robotic Manipulation)：</strong> 论文中提到的120多种操作任务表明，XR-1对各种机器人抓取、放置、组装等操作任务具有广泛的应用潜力。</li>
<li><strong>人机协作 (Human-Robot Collaboration)：</strong> 如果模型能够从人类演示中学习并泛化到机器人，将极大地促进人机协作场景的发展，使机器人能更好地理解和辅助人类。</li>
<li><strong>自动驾驶 (Autonomous Driving) 的某些子任务：</strong> 虽然主要关注操作，但其处理高维视觉输入和生成低级动作的框架，在自动驾驶中处理复杂环境感知和车辆控制的某些方面可能具有借鉴意义。</li>
<li><strong>虚拟现实/增强现实中的智能代理 (Intelligent Agents in VR/AR)：</strong> 学习统一的视觉-运动表示对于在虚拟环境中创建能够理解和响应用户行为的智能代理也可能有用。</li>
</ul>
<p><strong>5. 从摘要中可推断的局限性</strong></p>
<ul>
<li><strong>计算资源需求：</strong> 训练双分支VQ-VAE、进行大规模跨形态预训练以及三阶段训练范式，很可能需要大量的计算资源（GPU/TPU），这可能限制其在资源受限环境中的应用。</li>
<li><strong>数据依赖性：</strong> 尽管论文强调了利用大规模异构数据集，但模型的性能仍然高度依赖于这些数据集的质量和多样性。如果数据集中存在偏差或不足，可能会影响模型的泛化能力。</li>
<li><strong>UVMC的解释性：</strong> 尽管UVMC是离散的潜在表示，但其内部编码的视觉动态和机器人运动的具体语义和可解释性在摘要中并未详细说明。理解这些编码的含义可能有助于进一步改进模型。</li>
<li><strong>实时性挑战：</strong> 摘要中没有提及模型的推理速度。对于某些需要实时响应的机器人任务，模型的计算延迟可能是一个潜在的挑战。</li>
<li><strong>任务复杂度的上限：</strong> 尽管在120多种操作任务上进行了验证，但这些任务的复杂性（例如，是否涉及长期规划、复杂的物理交互或高层次的语义理解）在摘要中没有详细说明。模型在更开放、更复杂的真实世界场景中的表现仍需进一步验证。</li>
</ul>
<hr />
<p>总而言之，XR-1通过引入UVMC和创新的三阶段训练范式，在解决VLA模型的核心挑战方面迈出了重要一步，特别是在处理异构数据和实现跨形态泛化方面。这篇论文预示着通用机器人学习和具身智能领域的新进展，值得计算机视觉和机器人学研究者密切关注。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this work, we
present X Robotic Model 1 (XR-1), a novel framework for versatile and scalable
VLA learning across diverse robots, tasks, and environments.</li>
<li>To effectively exploit UVMC, we propose a
three-stage training paradigm: (i) self-supervised UVMC learning, (ii)
UVMC-guided pretraining on large-scale cross-embodiment robotic datasets, and
(iii) task-specific post-training.</li>
<li>XR-1 consistently
outperforms state-of-the-art baselines such as <script type="math/tex">\pi_{0.5}</script>, <script type="math/tex">\pi_0</script>, RDT,
UniVLA, and GR00T-N1.5 while demonstrating strong generalization to novel
objects, background variations, distractors, and illumination changes.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.02776v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.02776v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.02767v1'></a></p>
<h2 id="dynamic-reflections-probing-video-representations-with-text-alignment"><a href="https://arxiv.org/abs/2511.02767v1">Dynamic Reflections: Probing Video Representations with Text Alignment</a></h2>
<p><strong>Authors:</strong> Tyler Zhu, Tengda Han, Leonidas Guibas, Viorica Pătrăucean, Maks Ovsjanikov</p>
<p><strong>Published:</strong> 2025-11-04</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>The alignment of representations from different modalities has recently been
shown to provide insights on the structural similarities and downstream
capabilities of different encoders across diverse data types. While significant
progress has been made in aligning images with text, the temporal nature of
video data remains largely unexplored in this context. In this work, we conduct
the first comprehensive study of video-text representation alignment, probing
the capabilities of modern video and language encoders. Our findings reveal
several key insights. First, we demonstrate that cross-modal alignment highly
depends on the richness of both visual (static images vs. multi-frame videos)
and text (single caption vs. a collection) data provided at test time,
especially when using state-of-the-art video encoders. We propose parametric
test-time scaling laws that capture this behavior and show remarkable
predictive power against empirical observations. Secondly, we investigate the
correlation between semantic alignment and performance on both semantic and
non-semantic downstream tasks, providing initial evidence that strong alignment
against text encoders may be linked to general-purpose video representation and
understanding. Finally, we correlate temporal reasoning with cross-modal
alignment providing a challenging test-bed for vision and language models.
Overall, our work introduces video-text alignment as an informative zero-shot
way to probe the representation power of different encoders for spatio-temporal
data. Project page can be found at https://video-prh.github.io/</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Tyler Zhu等人撰写的论文“Dynamic Reflections: Probing Video Representations with Text Alignment”的全面摘要。</p>
<hr />
<h3 id="dynamic-reflections-probing-video-representations-with-text-alignment_1">论文摘要：“Dynamic Reflections: Probing Video Representations with Text Alignment”</h3>
<p><strong>1. 主要问题或研究问题：</strong>
该论文主要探讨了视频数据中跨模态（视频与文本）表示对齐的问题，特别关注了视频的<strong>时间性</strong>。尽管图像与文本的对齐研究取得了显著进展，但视频数据中丰富的时空信息如何影响其与文本的对齐，以及这种对齐能力如何反映视频编码器的泛化能力，仍是一个未充分探索的领域。论文旨在通过对现代视频和语言编码器进行首次全面的视频-文本表示对齐研究，来深入探究这些问题。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
*   <strong>首次全面研究视频-文本对齐：</strong> 论文首次系统地将跨模态对齐研究扩展到时间域，填补了现有研究主要集中在静态图像模态的空白。
*   <strong>强调测试时数据丰富度的影响：</strong> 论文发现，跨模态对齐的质量高度依赖于测试时提供的视觉（静态图像 vs. 多帧视频）和文本（单个字幕 vs. 字幕集合）数据的丰富度，尤其是在使用最先进的视频编码器时。这表明，通过在推理阶段提供更丰富的数据，即使不修改预训练模型，也能显著提高对齐分数。
*   <strong>提出参数化测试时缩放定律：</strong> 为了量化数据丰富度对对齐分数的影响，论文提出了参数化的测试时缩放定律。这些定律能够捕捉对齐行为，并对经验观察具有显著的预测能力（R² &gt; 0.98），为多模态数据获取策略和编码器能力比较提供了工具。
*   <strong>关联语义对齐与下游任务性能：</strong> 论文首次探讨了视频-文本语义对齐与视频模型在语义和非语义下游任务（如动作分类、点跟踪、物体跟踪、相机姿态估计、深度估计）上的性能之间的相关性，为评估视频表示的通用性提供了初步证据。
*   <strong>引入时间推理的挑战性基准：</strong> 论文通过VideoComp和Test of Time数据集，将时间推理与跨模态对齐联系起来，为视觉和语言模型提供了一个具有挑战性的测试平台，以评估它们捕捉时间顺序信息的能力。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>对齐分数显著提升：</strong> 论文证明，通过利用多帧视频和多样化的字幕集合，对齐分数可以显著提高，在某些情况下甚至翻倍，远超以往静态图像-文本对齐报告的水平（例如，从0.16提高到接近0.4）。这表明视频数据中丰富的时空上下文和文本描述的多样性对于实现更强的跨模态对齐至关重要。
*   <strong>视频编码器优于图像编码器：</strong> 最先进的自监督视频编码器（如VideoMAEv2）在视频-文本对齐方面表现出与顶级图像编码器（如DINOv2）相当甚至更好的竞争力，尤其是在利用多帧信息时。
*   <strong>对齐与下游任务性能相关：</strong> 研究发现，自监督视频模型的跨模态对齐分数与语义任务（如SSv2和Kinetics上的动作分类）以及非语义感知任务（如相机姿态估计、深度预测、物体跟踪）的性能之间存在显著的正相关性。这初步表明，强大的视频-文本对齐可能与通用的视频表示和理解能力相关。
*   <strong>时间敏感性差异：</strong> 语言模型在处理时间顺序信息时，倾向于将具有相同词语但顺序不同的文本视为更接近的邻居（即表现出“词袋”行为），而视频模型则能更好地捕捉时间动态。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>语言模型的时间敏感性：</strong> 论文指出，语言模型在处理时间推理任务时，在浅层特征提取方面可能更倾向于“词袋”模型，对时间顺序的敏感性不足。
*   <strong>点跟踪任务的弱相关性：</strong> 视频-文本对齐分数与点跟踪任务的性能之间存在较弱的相关性，这可能与点跟踪任务的高度局部性有关，也暗示了通用视频编码器在这一领域仍有改进空间。
*   <strong>生成模型对齐能力弱：</strong> 论文提到，尽管生成模型是视频模型的一个有前景的方向，但它们目前的文本对齐能力相当弱。
*   <strong>跨模型对齐的全面性：</strong> 论文虽然初步探讨了跨模型对齐，但对其作为通用视觉模型多功能性指标的完整调查仍是未来的工作。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>提升语言模型的时间推理能力：</strong> 改进语言模型对时间顺序的敏感性，使其能够更好地理解视频中的动态和因果关系。
*   <strong>开发更通用的视频编码器：</strong> 针对点跟踪等局部性任务，进一步提升视频编码器的泛化能力。
*   <strong>探索生成模型在视频理解中的潜力：</strong> 研究如何更好地利用生成模型在视频理解中的潜在表示能力，以提高其与文本的对齐。
*   <strong>深入研究跨模型对齐：</strong> 对不同视频模型之间的跨模型对齐进行更全面的调查，以确定其是否能作为衡量模型多功能性和在广泛下游任务中表现的强预测指标。
*   <strong>优化测试时数据获取策略：</strong> 利用提出的测试时缩放定律，指导多模态数据（特别是高质量视频标注）的获取策略，以实现成本效益最大化。</p>
<hr />
<p>总而言之，这篇论文通过首次全面研究视频-文本表示对齐，为理解视频编码器的能力提供了一个新颖的零样本探究方法。它强调了测试时数据丰富度的关键作用，提出了预测性的缩放定律，并揭示了对齐与下游任务性能之间的重要关联，为未来视频理解和多模态AI系统的发展奠定了基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>First, we demonstrate that cross-modal alignment highly
depends on the richness of both visual (static images vs.</li>
<li>a collection) data provided at test time,
especially when using state-of-the-art video encoders.</li>
<li>We propose parametric
test-time scaling laws that capture this behavior and show remarkable
predictive power against empirical observations.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.02767v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.02767v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.02591v1'></a></p>
<h2 id="zero-shot-multi-animal-tracking-in-the-wild"><a href="https://arxiv.org/abs/2511.02591v1">Zero-Shot Multi-Animal Tracking in the Wild</a></h2>
<p><strong>Authors:</strong> Jan Frederik Meier, Timo Lüddecke</p>
<p><strong>Published:</strong> 2025-11-04</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Multi-animal tracking is crucial for understanding animal ecology and
behavior. However, it remains a challenging task due to variations in habitat,
motion patterns, and species appearance. Traditional approaches typically
require extensive model fine-tuning and heuristic design for each application
scenario. In this work, we explore the potential of recent vision foundation
models for zero-shot multi-animal tracking. By combining a Grounding Dino
object detector with the Segment Anything Model 2 (SAM 2) tracker and carefully
designed heuristics, we develop a tracking framework that can be applied to new
datasets without any retraining or hyperparameter adaptation. Evaluations on
ChimpAct, Bird Flock Tracking, AnimalTrack, and a subset of GMOT-40 demonstrate
strong and consistent performance across diverse species and environments. The
code is available at https://github.com/ecker-lab/SAM2-Animal-Tracking.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Jan Frederik Meier和Timo Lüddecke撰写的论文“Zero-Shot Multi-Animal Tracking in the Wild”的全面摘要。</p>
<hr />
<h3 id="zero-shot-multi-animal-tracking-in-the-wild_1">论文摘要：Zero-Shot Multi-Animal Tracking in the Wild</h3>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决多动物跟踪（Multi-Animal Tracking）的挑战。传统的多动物跟踪方法通常需要针对每个应用场景进行大量的模型微调和启发式设计，这在动物生态学和行为研究中，由于栖息地、运动模式和物种外观的多样性，使得该任务变得尤为困难且耗时。因此，研究的核心问题是如何开发一种无需重新训练或超参数调整即可应用于新数据集的零样本（zero-shot）多动物跟踪框架。</p>
<p><strong>2. 关键创新或方法贡献：</strong>
作者基于SAM2MOT（一个用于人类跟踪的模型）进行了扩展和改进，使其适用于多动物跟踪场景，并引入了以下关键创新：</p>
<ul>
<li><strong>自适应检测阈值（Adaptive Detection Thresholds）：</strong> 针对零样本检测器在不同数据集和序列间检测分数分布差异大的问题，论文提出了一种基于K-Means聚类的自适应阈值方法。该方法能自动调整检测置信度阈值，将检测结果分为“真阳性”和“假阳性”，从而提高检测和关联的准确性，无需手动调整。</li>
<li><strong>基于掩码的轨迹初始化（Mask-based Track Initialization）：</strong> 为了减少虚假轨迹的初始化，作者利用SAM 2生成的分割掩码质量来指导新轨迹的创建。通过计算新掩码与所有现有轨迹掩码之间的归一化掩码交集（NMI），只有当NMI低于特定阈值时才初始化新轨迹，有效解决了多个实例共享同一边界框的歧义问题。</li>
<li><strong>密度感知重建（Density-aware Reconstruction）：</strong> 针对拥挤场景中目标检测性能下降导致轨迹掩码质量退化的问题，论文限制了现有轨迹的重新提示（re-prompting）。只有当检测结果与单个现有轨迹的关联明确无误时（通过比较最佳和次佳边界框-掩码对分数之间的差异），才进行重新提示，从而提高了在挑战性环境中的跟踪鲁棒性。</li>
<li><strong>非极大值抑制（NMS）应用于轨迹掩码：</strong> 额外应用NMS来减少假阳性。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong>
该方法在ChimpAct、Bird Flock Tracking、AnimalTrack和GMOT-40子集等多个动物跟踪基准数据集上进行了广泛评估。结果表明：</p>
<ul>
<li><strong>卓越的零样本性能：</strong> 该方法在所有评估数据集上均显著优于已训练和零样本基线，在HOTA和AssA指标上取得了最高分。这证明了其在无需特定数据集训练的情况下，在不同物种和环境中的鲁棒性和泛化能力。</li>
<li><strong>各组件的有效性：</strong> 消融研究证实，所提出的每个组件（自适应检测阈值、基于掩码的轨迹初始化、密度感知重建）都持续提高了跟踪性能，尤其是在检测和关联准确性方面。</li>
<li><strong>对经典MOT数据集的泛化能力：</strong> 在DanceTrack和SportsMOT等经典MOT数据集上的评估也显示出良好的泛化能力，表明该框架不仅适用于动物跟踪，也适用于更广泛的多目标跟踪场景。</li>
</ul>
<p>这些结果突显了视觉基础模型在零样本多动物跟踪方面的巨大潜力，为可扩展的野生动物监测和行为分析提供了新的途径。</p>
<p><strong>4. 论文中提及的局限性：</strong>
论文中提到了以下局限性：</p>
<ul>
<li><strong>运行时长和可扩展性：</strong> 尽管SAM 2-based跟踪器在GPU内存使用方面有所优化，但其运行时长和VRAM消耗与跟踪对象数量呈线性关系。这意味着在高度拥挤的场景中，该方法的可扩展性有限，运行时长相对较长。</li>
<li><strong>对K-Means聚类假设的依赖：</strong> 自适应阈值方法假设检测分数呈双峰分布，尽管在实践中并非总是如此，但实验结果显示其具有鲁棒性。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong>
尽管论文没有明确列出未来的研究方向，但从其局限性和贡献中可以推断出：</p>
<ul>
<li><strong>提高在拥挤场景中的可扩展性：</strong> 优化SAM 2-based跟踪器在处理大量对象时的运行时长和内存消耗，例如通过更高效的内存管理或更智能的对象交互处理机制。</li>
<li><strong>探索更复杂的自适应阈值方法：</strong> 研究在检测分数分布不呈双峰时，如何进一步改进自适应阈值方法，以提高其普适性。</li>
<li><strong>结合语义理解：</strong> 论文提到其模型缺乏语义类别理解，这限制了其从错误类别中移除已跟踪对象的能力。未来的工作可以探索如何将更深层次的语义理解整合到框架中，以进一步提高跟踪精度和鲁棒性。</li>
<li><strong>更广泛的零样本泛化：</strong> 进一步测试和优化模型在更多样化、更具挑战性的“野外”数据集上的性能，以验证其在极端条件下的鲁棒性。</li>
</ul>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>By combining a Grounding Dino
object detector with the Segment Anything Model 2 (SAM 2) tracker and carefully
designed heuristics, we develop a tracking framework that can be applied to new
datasets without any retraining or hyperparameter adaptation.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.02591v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.02591v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.02564v1'></a></p>
<h2 id="seeing-across-time-and-views-multi-temporal-cross-view-learning-for-robust-video-person-re-identification"><a href="https://arxiv.org/abs/2511.02564v1">Seeing Across Time and Views: Multi-Temporal Cross-View Learning for Robust Video Person Re-Identification</a></h2>
<p><strong>Authors:</strong> Md Rashidunnabi, Kailash A. Hambarde, Vasco Lopes, Joao C. Neves, Hugo Proenca</p>
<p><strong>Published:</strong> 2025-11-04</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Video-based person re-identification (ReID) in cross-view domains (for
example, aerial-ground surveillance) remains an open problem because of extreme
viewpoint shifts, scale disparities, and temporal inconsistencies. To address
these challenges, we propose MTF-CVReID, a parameter-efficient framework that
introduces seven complementary modules over a ViT-B/16 backbone. Specifically,
we include: (1) Cross-Stream Feature Normalization (CSFN) to correct camera and
view biases; (2) Multi-Resolution Feature Harmonization (MRFH) for scale
stabilization across altitudes; (3) Identity-Aware Memory Module (IAMM) to
reinforce persistent identity traits; (4) Temporal Dynamics Modeling (TDM) for
motion-aware short-term temporal encoding; (5) Inter-View Feature Alignment
(IVFA) for perspective-invariant representation alignment; (6) Hierarchical
Temporal Pattern Learning (HTPL) to capture multi-scale temporal regularities;
and (7) Multi-View Identity Consistency Learning (MVICL) that enforces
cross-view identity coherence using a contrastive learning paradigm. Despite
adding only about 2 million parameters and 0.7 GFLOPs over the baseline,
MTF-CVReID maintains real-time efficiency (189 FPS) and achieves
state-of-the-art performance on the AG-VPReID benchmark across all altitude
levels, with strong cross-dataset generalization to G2A-VReID and MARS
datasets. These results show that carefully designed adapter-based modules can
substantially enhance cross-view robustness and temporal consistency without
compromising computational efficiency. The source code is available at
https://github.com/MdRashidunnabi/MTF-CVReID</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为一名计算机视觉和机器学习专家，我将为您提供Md Rashidunnabi等人撰写的论文“Seeing Across Time and Views: Multi-Temporal Cross-View Learning for Robust Video Person Re-Identification”的全面摘要。</p>
<hr />
<h3 id="seeing-across-time-and-views-multi-temporal-cross-view-learning-for-robust-video-person-re-identification_1">论文摘要：Seeing Across Time and Views: Multi-Temporal Cross-View Learning for Robust Video Person Re-Identification</h3>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决跨视角（例如，空中-地面监控）视频行人重识别（ReID）中的核心挑战。这些挑战主要源于极端的视角变化、尺度差异（如高空拍摄导致目标分辨率极低）以及时间不一致性（如遮挡、运动模糊或不完美的跟踪），这些因素严重阻碍了现有ReID方法的性能。核心研究问题是如何在保持计算效率的同时，设计一个鲁棒且参数高效的框架来克服这些跨视角和时间上的复杂性。</p>
<p><strong>2. 关键创新或方法学贡献：</strong>
作者提出了MTF-CVReID框架，这是一个模块化且参数高效的解决方案，它在冻结的ViT-B/16骨干网络基础上，通过七个轻量级适配器模块实现了创新：</p>
<ul>
<li><strong>跨流特征归一化 (CSFN)：</strong> 用于校正不同相机/视角带来的偏差（如光照、色偏、对比度），通过学习到的每视角偏移和残差MLP实现。</li>
<li><strong>多分辨率特征协调 (MRFH)：</strong> 用于在不同高度下稳定目标尺度。它生成三个并行的“虚拟缩放”表示（粗、原生、细），并根据内容自适应地融合它们，以应对高空小目标和地面大目标。</li>
<li><strong>身份感知记忆模块 (IAMM)：</strong> 用于强化持久的身份特征。它通过查询一个视角感知的记忆库来检索上下文向量，并将其与当前剪辑描述符融合，以在视角变化下保持身份一致性。</li>
<li><strong>时间动态建模 (TDM)：</strong> 用于运动感知的短期时间编码。它通过计算帧间差异并编码运动令牌，然后将运动信息与外观信息融合，以捕捉步态节奏和手臂摆动等判别性运动特征。</li>
<li><strong>跨视角特征对齐 (IVFA)：</strong> 用于实现透视不变的表示对齐。它通过在批次级别交换跨视角上下文，将不同视角的嵌入对齐到共享子空间。</li>
<li><strong>分层时间模式学习 (HTPL)：</strong> 用于捕捉多尺度时间规律。它并行于TDM运行，构建四个不同时间尺度的流（s=1, 2, 4, 8），以捕捉从瞬时变化到较慢动态的各种时间上下文。</li>
<li><strong>多视角身份一致性学习 (MVICL)：</strong> 使用对比学习范式来强制跨视角身份一致性，确保来自不同视角（空中、地面、可穿戴）的同一身份的剪辑被拉近，而不同身份被分开。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong>
MTF-CVReID在多个基准测试上取得了最先进的性能：</p>
<ul>
<li>在<strong>AG-VPReID</strong>基准测试上，A2G（空中到地面）方向达到73.3% Rank-1和65.2% mAP，G2A（地面到空中）方向达到75.4% Rank-1和59.3% mAP。这显著优于基于CLIP和非CLIP的竞争方法，尤其是在极端视角变化下表现出更强的鲁棒性。</li>
<li>在<strong>G2A-VReID</strong>上，达到69.3% Rank-1和78.4% mAP，证实了跨视角增益并非数据集特有。</li>
<li>在<strong>MARS</strong>数据集上，达到93.7% Rank-1和89.8% mAP，表明其适配器能够改进通用视频ReID特征。</li>
<li><strong>效率：</strong> 尽管增加了约2M参数和0.7 GFLOPs，MTF-CVReID仍保持了实时效率（189 FPS），证明了其参数高效性。</li>
<li><strong>定性分析：</strong> t-SNE可视化显示，MTF-CVReID生成的嵌入具有更紧密的类内聚类和更大的类间分离，轮廓分数（silhouette score）显著提高，这解释了更高的检索分数。</li>
</ul>
<p>这些结果表明，精心设计的基于适配器的模块可以显著增强跨视角鲁棒性和时间一致性，而不会牺牲计算效率，为实际多平台监控场景提供了实用解决方案。</p>
<p><strong>4. 论文中提到的局限性：</strong>
论文也坦诚地指出了MTF-CVReID的三个主要局限性：</p>
<ul>
<li><strong>CSFN的泛化性：</strong> 尽管CSFN无需相机元数据即可运行，但其在完全未见过的视角下的性能仍需进一步评估。</li>
<li><strong>IAMM的内存扩展性：</strong> IAMM的内存与身份数量和视角数量呈线性关系，这对于非常大规模的部署会带来挑战。</li>
<li><strong>极端条件下的鲁棒性：</strong> 该框架在未见过的模态（如红外、热成像）、极端条件（夜间、重度压缩）和极低分辨率下的鲁棒性尚未得到充分表征。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong>
为了解决上述局限性，未来的研究方向包括：</p>
<ul>
<li>通过无监督视角归一化来减少对相机ID监督的依赖。</li>
<li>改进可扩展性，通过压缩/剪枝IAMM原型来优化内存使用。</li>
<li>在更多样化的模态（如红外、低光视频）下评估模型的鲁棒性。</li>
</ul>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address
these challenges, we propose MTF-CVReID, a parameter-efficient framework that
introduces seven complementary modules over a ViT-B/16 backbone.</li>
<li>Despite
adding only about 2 million parameters and 0.7 GFLOPs over the baseline,
MTF-CVReID maintains real-time efficiency (189 FPS) and achieves
state-of-the-art performance on the AG-VPReID benchmark across all altitude
levels, with strong cross-dataset generalization to G2A-VReID and MARS
datasets.</li>
<li>These results show that carefully designed adapter-based modules can
substantially enhance cross-view robustness and temporal consistency without
compromising computational efficiency.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.02564v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.02564v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.02563v1'></a></p>
<h2 id="the-urban-vision-hackathon-dataset-and-models-towards-image-annotations-and-accurate-vision-models-for-indian-traffic"><a href="https://arxiv.org/abs/2511.02563v1">The Urban Vision Hackathon Dataset and Models: Towards Image Annotations and Accurate Vision Models for Indian Traffic</a></h2>
<p><strong>Authors:</strong> Akash Sharma, Chinmay Mhatre, Sankalp Gawali, Ruthvik Bokkasam, Brij Kishore, Vishwajeet Pattanaik, Tarun Rambha, Abdul R. Pinjari, Vijay Kovvali, Anirban Chakraborty, Punit Rathore, Raghu Krishnapuram, Yogesh Simmhan</p>
<p><strong>Published:</strong> 2025-11-04</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>This report describes the UVH-26 dataset, the first public release by
AIM@IISc of a large-scale dataset of annotated traffic-camera images from
India. The dataset comprises 26,646 high-resolution (1080p) images sampled from
2800 Bengaluru's Safe-City CCTV cameras over a 4-week period, and subsequently
annotated through a crowdsourced hackathon involving 565 college students from
across India. In total, 1.8 million bounding boxes were labeled across 14
vehicle classes specific to India: Cycle, 2-Wheeler (Motorcycle), 3-Wheeler
(Auto-rickshaw), LCV (Light Commercial Vehicles), Van, Tempo-traveller,
Hatchback, Sedan, SUV, MUV, Mini-bus, Bus, Truck and Other. Of these, 283k-316k
consensus ground truth bounding boxes and labels were derived for distinct
objects in the 26k images using Majority Voting and STAPLE algorithms. Further,
we train multiple contemporary detectors, including YOLO11-S/X, RT-DETR-S/X,
and DAMO-YOLO-T/L using these datasets, and report accuracy based on mAP50,
mAP75 and mAP50:95. Models trained on UVH-26 achieve 8.4-31.5% improvements in
mAP50:95 over equivalent baseline models trained on COCO dataset, with
RT-DETR-X showing the best performance at 0.67 (mAP50:95) as compared to 0.40
for COCO-trained weights for common classes (Car, Bus, and Truck). This
demonstrates the benefits of domain-specific training data for Indian traffic
scenarios. The release package provides the 26k images with consensus
annotations based on Majority Voting (UVH-26-MV) and STAPLE (UVH-26-ST) and the
6 fine-tuned YOLO and DETR models on each of these datasets. By capturing the
heterogeneity of Indian urban mobility directly from operational traffic-camera
streams, UVH-26 addresses a critical gap in existing global benchmarks, and
offers a foundation for advancing detection, classification, and deployment of
intelligent transportation systems in emerging nations with complex traffic
conditions.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Akash Sharma等人撰写的论文“The Urban Vision Hackathon Dataset and Models: Towards Image Annotations and Accurate Vision Models for Indian Traffic”的全面摘要。</p>
<hr />
<h3 id="_1">论文摘要：印度交通的城市视觉黑客马拉松数据集与模型</h3>
<p><strong>1. 主要问题或研究问题：</strong>
现有的大规模目标检测数据集（如COCO和Objects365）主要关注发达国家的城市环境和有组织的交通状况，这限制了它们在印度等发展中国家复杂、高密度和异构交通场景中的适用性。这些地区的交通具有独特的挑战，如极高的车辆密度、非标准驾驶行为以及包括自动人力车、摩托车、轻型商用车等在内的多样化车辆类型。因此，该研究旨在解决现有全球基准数据集中缺乏代表印度城市交通异构性的关键空白，并开发针对印度交通场景的、领域特定的图像标注和准确视觉模型。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
*   <strong>UVH-26数据集的创建：</strong> 首次公开发布大规模、领域特定的印度交通摄像头图像数据集UVH-26。该数据集包含26,646张1080p高分辨率图像，这些图像来自班加罗尔约2800个安全城市CCTV摄像头，涵盖了14种印度特有的车辆类别（如自行车、两轮车、三轮车、轻型商用车、厢式货车、Tempo-traveller、掀背车、轿车、SUV、MUV、小型巴士、巴士、卡车及其他）。
*   <strong>众包标注与质量控制：</strong> 通过一个为期四周的众包黑客马拉松，动员了565名印度大学生进行图像标注，共标注了180万个边界框。为确保标注质量，采用了模型辅助标注（使用预训练的RT-DETRv2-X模型生成预标注），并结合了多数投票（Majority Voting, MV）和STAPLE算法来生成28.3万至31.6万个共识地面真值边界框和标签。
*   <strong>隐私保护：</strong> 对图像中的车牌、人脸和摄像头文本叠加进行了模糊处理，以尊重隐私，遵循了公共驾驶和街景数据集的既定做法。
*   <strong>领域特定模型训练与评估：</strong> 使用UVH-26数据集对多种当代目标检测器（包括YOLOv11-S/X、RT-DETR-S/X和DAMO-YOLO-T/L）进行了微调，并报告了基于mAP50、mAP75和mAP50:95的准确性。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>显著的性能提升：</strong> 在UVH-26上训练的模型在mAP50:95方面比在COCO数据集上训练的等效基线模型实现了8.4%至31.5%的改进。
*   <strong>RT-DETR-X表现最佳：</strong> 对于常见类别（汽车、巴士和卡车），RT-DETR-X在UVH-26上训练后表现最佳，mAP50:95达到0.67，而COCO训练的权重仅为0.40。
*   <strong>领域特定数据的价值：</strong> 这些结果有力地证明了针对印度交通场景的领域特定训练数据对于提高车辆检测和分类模型性能的重要性。
*   <strong>异构交通场景的有效捕捉：</strong> UVH-26数据集直接从实际交通摄像头流中捕捉印度城市交通的异构性，填补了现有全球基准的空白。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>匿名化数据的影响：</strong> 论文提到，为了隐私原因，非匿名化的UVH-26数据集和模型不会公开发布。虽然在附录中报告了非匿名化数据上的模型训练结果，但这些结果仅用于学术比较，而非公开可用。这可能意味着公开版本的数据集在某些方面可能略有性能差异。
*   <strong>特定车辆类别的检测挑战：</strong> 某些车辆类别（如小型巴士）由于表示有限和与巴士的视觉相似性而表现出较低的检测性能。而细粒度的汽车类别（如掀背车和轿车）尽管表示充分，但由于与其他汽车子类型的视觉相似性，检测准确性较低。
*   <strong>众包标注的固有挑战：</strong> 尽管采用了质量控制机制（如金标准图像、多数投票和STAPLE算法），但众包标注的性质仍可能引入一定程度的噪声和偏差。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>UVH-26-ST模型的发布：</strong> 论文指出，未来将发布在UVH-26-ST（基于STAPLE共识算法）数据集上训练的模型结果，并将其公开发布。
*   <strong>更先进的AI驱动分析：</strong> UVH-26数据集为智能交通系统在复杂交通条件下的新兴国家中推进检测、分类和部署提供了基础。这包括开发更先进的计算机视觉模型，以更好地处理印度交通的独特挑战。
*   <strong>解决特定类别检测挑战：</strong> 针对小型巴士和细粒度汽车类别等检测性能较低的类别，可以探索更专门的数据增强、模型架构或少样本学习方法。
*   <strong>多模态数据融合：</strong> 结合其他类型的数据（如交通流量传感器数据、GPS数据等）可能进一步提升智能交通系统的性能。
*   <strong>实时部署和边缘计算优化：</strong> 论文提及选择的模型兼顾了准确性、计算效率和推理速度，未来的工作可以进一步优化这些模型，以实现更高效的边缘设备实时部署。</p>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>The release package provides the 26k images with consensus
annotations based on Majority Voting (UVH-26-MV) and STAPLE (UVH-26-ST) and the
6 fine-tuned YOLO and DETR models on each of these datasets.</li>
<li>By capturing the
heterogeneity of Indian urban mobility directly from operational traffic-camera
streams, UVH-26 addresses a critical gap in existing global benchmarks, and
offers a foundation for advancing detection, classification, and deployment of
intelligent transportation systems in emerging nations with complex traffic
conditions.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.02563v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.02563v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.02495v1'></a></p>
<h2 id="detectiumfire-a-comprehensive-multi-modal-dataset-bridging-vision-and-language-for-fire-understanding"><a href="https://arxiv.org/abs/2511.02495v1">DetectiumFire: A Comprehensive Multi-modal Dataset Bridging Vision and Language for Fire Understanding</a></h2>
<p><strong>Authors:</strong> Zixuan Liu, Siavash H. Khajavi, Guangkai Jiang</p>
<p><strong>Published:</strong> 2025-11-04</p>
<p><strong>Categories:</strong> cs.CV, cs.CL</p>
<p><strong>Abstract:</strong></p>
<p>Recent advances in multi-modal models have demonstrated strong performance in
tasks such as image generation and reasoning. However, applying these models to
the fire domain remains challenging due to the lack of publicly available
datasets with high-quality fire domain annotations. To address this gap, we
introduce DetectiumFire, a large-scale, multi-modal dataset comprising of 22.5k
high-resolution fire-related images and 2.5k real-world fire-related videos
covering a wide range of fire types, environments, and risk levels. The data
are annotated with both traditional computer vision labels (e.g., bounding
boxes) and detailed textual prompts describing the scene, enabling applications
such as synthetic data generation and fire risk reasoning. DetectiumFire offers
clear advantages over existing benchmarks in scale, diversity, and data
quality, significantly reducing redundancy and enhancing coverage of real-world
scenarios. We validate the utility of DetectiumFire across multiple tasks,
including object detection, diffusion-based image generation, and
vision-language reasoning. Our results highlight the potential of this dataset
to advance fire-related research and support the development of intelligent
safety systems. We release DetectiumFire to promote broader exploration of fire
understanding in the AI community. The dataset is available at
https://kaggle.com/datasets/38b79c344bdfc55d1eed3d22fbaa9c31fad45e27edbbe9e3c529d6e5c4f93890</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于Zixuan Liu, Siavash H. Khajavi, Guangkai Jiang撰写的论文“DetectiumFire: A Comprehensive Multi-modal Dataset Bridging Vision and Language for Fire Understanding”的全面摘要：</p>
<p><strong>论文摘要：DetectiumFire：弥合视觉与语言鸿沟，全面理解火灾的多模态数据集</strong></p>
<p><strong>1. 主要问题或研究问题：</strong>
尽管多模态模型在图像生成和推理等任务中取得了显著进展，但由于缺乏高质量的火灾领域标注数据集，这些模型在火灾理解领域的应用仍然面临挑战。现有的火灾数据集主要局限于传统的计算机视觉任务（如图像分类和目标检测），且规模有限、多样性不足，无法满足现代多模态模型训练和评估的需求，尤其是在火灾风险推理和合成数据生成等高级应用方面。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
为了解决上述问题，作者引入了<strong>DetectiumFire</strong>，这是一个大规模、多模态的火灾数据集，包含超过22.5k张高分辨率火灾相关图像和2.5k个真实世界的火灾相关视频。其主要创新和贡献包括：
*   <strong>大规模与多样性：</strong> 数据集涵盖了广泛的火灾类型、环境和风险等级，显著减少了现有数据集的冗余，并增强了对真实世界场景的覆盖。
*   <strong>多模态标注：</strong> 数据不仅包含传统的计算机视觉标签（如边界框），还包含详细的文本提示来描述场景，从而支持合成数据生成和火灾风险推理等高级应用。
*   <strong>专业策展：</strong> 数据集由具有领域专业知识和AI概念的消防安全专业人员精心策划，确保了高质量的标注和有意义的场景覆盖。
*   <strong>合成数据生成：</strong> 通过对扩散模型进行监督微调（SFT）和基于人类反馈的强化学习（RLHF）两种策略，利用DetectiumFire数据集生成了8k张高质量的合成火灾图像，以解决数据稀缺性问题。
*   <strong>评估基准：</strong> DetectiumFire被用作评估基准，验证了其在目标检测、扩散模型图像生成和视觉-语言推理等多项任务中的实用性。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>数据质量与多样性：</strong> DetectiumFire在规模、多样性和数据质量方面优于现有基准，显著降低了图像重复率（0.23%对比D-Fire的0.55%），并提供了更广泛的真实世界火灾场景覆盖。
*   <strong>目标检测性能提升：</strong> 在DetectiumFire上训练的模型在D-Fire测试集上表现出良好的泛化能力，性能与直接在D-Fire上训练的模型相当。而D-Fire上训练的模型在DetectiumFire上表现显著下降，表明DetectiumFire更具多样性和挑战性。结合真实世界和所有合成数据进行训练，目标检测性能略有提升，验证了合成数据作为增强手段的有效性。
*   <strong>合成数据实用性：</strong> 对扩散模型进行微调（SFT和RLHF）显著提高了生成图像的视觉保真度、真实感和提示对齐度。这些改进转化为下游任务（如目标检测）的更强性能。
*   <strong>多模态火灾推理能力：</strong> 对LLaMA-3.2-11B-Vision-Instruct模型在DetectiumFire上进行微调后，模型在火灾推理任务（燃烧对象、环境、火灾严重性）上的准确性大幅提高，尤其是在火灾严重性分类方面，准确率从56.06%提升到83.84%，表明模型能够更好地从视觉线索中解读风险等级。</p>
<p><strong>4. 论文中提到的局限性：</strong>
*   <strong>数据安全与伦理风险：</strong> 尽管已努力确保DetectiumFire的质量、安全性和多样性，但火灾场景固有的极端和不可预测性可能导致某些图像和描述包含不安全、误导性或潜在有害内容。这可能导致模型生成不安全或不适当的输出，尤其是在生成式设置中。
*   <strong>数据来源限制：</strong> 大部分真实世界数据通过网络搜索收集，这可能对下游使用施加法律或伦理限制，可能限制数据集在纯学术或非商业研究中的适用性。
*   <strong>场景覆盖不完全：</strong> 尽管DetectiumFire涵盖了广泛的火灾类型和场景，但仍不完全。边缘案例、代表性不足的地理区域和特定文化背景的火灾场景可能缺失或采样不足。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>火灾视频生成：</strong> 将扩散模型图像合成能力扩展到时间域，生成逼真、时间连贯的火灾行为序列，用于安全模拟、风险预测和合成训练。
*   <strong>高级火灾推理与AI智能体：</strong> 将视觉-语言模型集成到更高级的AI智能体中，使其能够进行时间推理、因果推理，并结合外部知识库和决策逻辑，提供实时决策支持。
*   <strong>可控火灾视频生成：</strong> 基于控制信号（如火灾类型、蔓延速度、环境条件）生成可定制的火灾模拟。
*   <strong>细粒度火灾评估和AI智能体安全响应系统：</strong> 进一步开发能够进行细粒度火灾评估和基于AI智能体的安全响应系统。
*   <strong>改进合成数据生成方法：</strong> 进一步完善合成数据生成方法，以提高多样性和上下文真实感，最大化结合真实数据和合成数据的优势。</p>
<p>总而言之，DetectiumFire数据集通过提供大规模、丰富标注的多模态数据，填补了火灾理解领域长期存在的空白。它不仅推动了判别式和生成式模型的发展，也为以推理为中心、对公共安全至关重要的应用奠定了基础，有望激发计算机视觉、语言和安全关键决策交叉领域的新研究方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We release DetectiumFire to promote broader exploration of fire
understanding in the AI community.</li>
<li>The dataset is available at
https://kaggle.com/datasets/38b79c344bdfc55d1eed3d22fbaa9c31fad45e27edbbe9e3c529d6e5c4f93890</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.02495v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.02495v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.02483v1'></a></p>
<h2 id="olatverse-a-large-scale-real-world-object-dataset-with-precise-lighting-control"><a href="https://arxiv.org/abs/2511.02483v1">OLATverse: A Large-scale Real-world Object Dataset with Precise Lighting Control</a></h2>
<p><strong>Authors:</strong> Xilong Zhou, Jianchun Chen, Pramod Rao, Timo Teufel, Linjie Lyu, Tigran Minasian, Oleksandr Sotnychenko, Xiaoxiao Long, Marc Habermann, Christian Theobalt</p>
<p><strong>Published:</strong> 2025-11-04</p>
<p><strong>Categories:</strong> cs.CV, cs.GR</p>
<p><strong>Abstract:</strong></p>
<p>We introduce OLATverse, a large-scale dataset comprising around 9M images of
765 real-world objects, captured from multiple viewpoints under a diverse set
of precisely controlled lighting conditions. While recent advances in
object-centric inverse rendering, novel view synthesis and relighting have
shown promising results, most techniques still heavily rely on the synthetic
datasets for training and small-scale real-world datasets for benchmarking,
which limits their realism and generalization. To address this gap, OLATverse
offers two key advantages over existing datasets: large-scale coverage of real
objects and high-fidelity appearance under precisely controlled illuminations.
Specifically, OLATverse contains 765 common and uncommon real-world objects,
spanning a wide range of material categories. Each object is captured using 35
DSLR cameras and 331 individually controlled light sources, enabling the
simulation of diverse illumination conditions. In addition, for each object, we
provide well-calibrated camera parameters, accurate object masks, photometric
surface normals, and diffuse albedo as auxiliary resources. We also construct
an extensive evaluation set, establishing the first comprehensive real-world
object-centric benchmark for inverse rendering and normal estimation. We
believe that OLATverse represents a pivotal step toward integrating the next
generation of inverse rendering and relighting methods with real-world data.
The full dataset, along with all post-processing workflows, will be publicly
released at https://vcai.mpi-inf.mpg.de/projects/OLATverse/.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于“OLATverse: A Large-scale Real-world Object Dataset with Precise Lighting Control”论文的全面摘要，内容涵盖了您要求的所有方面：</p>
<p><strong>论文摘要：OLATverse：一个具有精确光照控制的大规模真实世界物体数据集</strong></p>
<p><strong>1. 主要问题或研究问题</strong>
当前计算机视觉和图形学领域在物体中心逆渲染、新视角合成和重打光方面取得了显著进展，但这些技术在真实感和泛化能力上受到限制。主要原因是现有数据集要么是合成的（缺乏真实感），要么是小规模的真实世界数据集（限制了泛化和基准测试能力），缺乏大规模、高保真且具有精确光照控制的真实世界物体数据集。这导致了合成数据与真实世界数据之间存在显著的领域鸿沟。</p>
<p><strong>2. 关键创新或方法论贡献</strong>
OLATverse数据集通过以下创新解决了上述问题：
*   <strong>大规模真实世界物体覆盖与高保真外观：</strong> 数据集包含765个真实世界物体，涵盖广泛的材料类别（如木材、石头、皮革、塑料、金属等）和LVIS类别，总计约900万张图像。每个物体都在精确控制的光照条件下从多个视角捕获，确保了高真实感。
*   <strong>精确的光照控制：</strong> 每个物体使用35个DSLR相机和331个独立控制的光源进行捕获，能够模拟多样化的光照条件，包括均匀光照、单光源（OLATs）、梯度光照和预定义环境光照。
*   <strong>辅助资源：</strong> 为每个物体提供经过良好校准的相机参数、精确的物体遮罩、光度表面法线和漫反射反照率。这些辅助数据对于评估和监督多模态任务非常有价值。
*   <strong>半自动遮罩处理流程：</strong> 开发了高效的半自动遮罩处理流程，结合背景抠图（bgMatting）、SAM和RMBG-2.0的优势，以提取高质量的物体遮罩。
*   <strong>综合评估基准：</strong> 构建了一个广泛的评估集，为逆渲染和法线估计建立了首个全面的真实世界物体中心基准。</p>
<p><strong>3. 主要结果及其意义</strong>
*   <strong>数据集规模与多样性：</strong> OLATverse是首个同时提供大规模覆盖和高保真外观的真实世界物体数据集，显著超越了现有数据集在物体数量、材料多样性和光照控制方面的限制。
*   <strong>应用潜力：</strong> 论文展示了OLATverse在多个任务中的应用潜力，包括：
    *   <strong>重打光：</strong> 利用光传输的线性特性，可以通过OLAT图像合成在任意新颖光照下的物体外观，为生成式先验模型提供了大规模训练数据。
    *   <strong>逆渲染与新视角合成：</strong> 作为全面的基准，OLATverse用于评估多种逆渲染和新视角合成方法，并报告了定量指标（SSIM、PSNR、LPIPS），结果显示GS³在视觉和数值上均优于其他方法。
    *   <strong>法线估计：</strong> 数据集用于基准测试扩散模型法线估计方法，并提供了定量（平均和中值角度误差）和定性结果，突出了OLATverse对法线估计研究的重要性。
*   <strong>弥合领域鸿沟：</strong> OLATverse的发布被认为是将下一代逆渲染和重打光方法与真实世界数据相结合的关键一步，有助于弥合合成数据与真实世界数据之间的领域鸿沟，推动逼真3D视觉和重打光领域的研究。</p>
<p><strong>4. 论文中提及的局限性</strong>
*   <strong>法线和反照率的精确性：</strong> 尽管采用了线性偏振滤镜来消除镜面反射，但对于光泽材料或低反射纹理的物体，在法线提取过程中仍可能存在伪影。提取的表面法线和漫反射反照率并非精确的真实值，但仍可作为多模态训练任务的宝贵监督信号。
*   <strong>几何真实值缺失：</strong> 由于硬件限制，数据集中未包含几何真实值网格。</p>
<p><strong>5. 潜在的未来研究方向</strong>
*   <strong>集成先进扫描系统：</strong> 未来研究可以探索集成先进的扫描系统，以联合捕获真实物体的外观和几何信息，从而提供更精确的几何真实值。
*   <strong>生成式先验学习：</strong> 利用OLATverse数据集训练数据驱动的生成式先验模型，以实现更逼真的重打光和外观建模。
*   <strong>多模态任务的进一步探索：</strong> 充分利用数据集提供的辅助资源（如表面法线和漫反射反照率）来推动多模态训练任务的研究。</p>
<p>总而言之，OLATverse数据集通过其前所未有的大规模、高保真和精确光照控制，为计算机视觉和图形学领域提供了一个强大的新资源，有望加速逆渲染、重打光和新视角合成等关键技术的发展。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce OLATverse, a large-scale dataset comprising around 9M images of
765 real-world objects, captured from multiple viewpoints under a diverse set
of precisely controlled lighting conditions.</li>
<li>While recent advances in
object-centric inverse rendering, novel view synthesis and relighting have
shown promising results, most techniques still heavily rely on the synthetic
datasets for training and small-scale real-world datasets for benchmarking,
which limits their realism and generalization.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.02483v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.02483v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.02427v1'></a></p>
<h2 id="from-the-laboratory-to-real-world-application-evaluating-zero-shot-scene-interpretation-on-edge-devices-for-mobile-robotics"><a href="https://arxiv.org/abs/2511.02427v1">From the Laboratory to Real-World Application: Evaluating Zero-Shot Scene Interpretation on Edge Devices for Mobile Robotics</a></h2>
<p><strong>Authors:</strong> Nicolas Schuler, Lea Dewald, Nick Baldig, Jürgen Graf</p>
<p><strong>Published:</strong> 2025-11-04</p>
<p><strong>Categories:</strong> cs.CV, cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>Video Understanding, Scene Interpretation and Commonsense Reasoning are
highly challenging tasks enabling the interpretation of visual information,
allowing agents to perceive, interact with and make rational decisions in its
environment. Large Language Models (LLMs) and Visual Language Models (VLMs)
have shown remarkable advancements in these areas in recent years, enabling
domain-specific applications as well as zero-shot open vocabulary tasks,
combining multiple domains. However, the required computational complexity
poses challenges for their application on edge devices and in the context of
Mobile Robotics, especially considering the trade-off between accuracy and
inference time. In this paper, we investigate the capabilities of
state-of-the-art VLMs for the task of Scene Interpretation and Action
Recognition, with special regard to small VLMs capable of being deployed to
edge devices in the context of Mobile Robotics. The proposed pipeline is
evaluated on a diverse dataset consisting of various real-world cityscape,
on-campus and indoor scenarios. The experimental evaluation discusses the
potential of these small models on edge devices, with particular emphasis on
challenges, weaknesses, inherent model biases and the application of the gained
information. Supplementary material is provided via the following repository:
https://datahub.rz.rptu.de/hstr-csrl-public/publications/scene-interpretation-on-edge-devices/</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供Nicolas Schuler, Lea Dewald, Nick Baldig, Jürgen Graf撰写的论文“From the Laboratory to Real-World Application: Evaluating Zero-Shot Scene Interpretation on Edge Devices for Mobile Robotics”的全面摘要。</p>
<hr />
<h3 id="_2">论文摘要：从实验室到实际应用：在移动机器人边缘设备上评估零样本场景解释</h3>
<p><strong>1. 主要问题或研究问题：</strong>
该论文旨在解决在移动机器人背景下，将先进的视觉语言模型（VLMs）应用于边缘设备进行零样本场景解释和动作识别所面临的挑战。具体来说，研究关注如何在计算资源受限的边缘设备上部署和有效利用小型VLM，以实现对视觉信息的实时理解、交互和决策，同时平衡准确性和推理时间。</p>
<p><strong>2. 关键创新或方法论贡献：</strong>
*   <strong>边缘设备优化的VLM部署：</strong> 论文提出了一种集成零样本视频解释的架构，该架构将移动认知系统（边缘设备）与基于云的基础模型支持相结合。边缘设备利用小型VLM进行初步场景描述、目标检测和跟踪，从而在本地处理原始图像数据，保护隐私。
*   <strong>语义引导分割和跟踪：</strong> 提出的管道利用本地VLM生成的文本描述，将其分解为名词，然后用于提示式零样本分割和跟踪（结合Grounded DINO和SAM），从而为场景描述提供额外的洞察力，并实现更精细的对象定位和理解。
*   <strong>多领域真实世界数据集评估：</strong> 论文在一个多样化的真实世界数据集中评估了所提出的管道，该数据集包含城市、校园和室内场景，总时长234分钟，并进行了人工标注以评估生成描述的质量。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>小型VLM在边缘设备上的潜力：</strong> 实验结果表明，SmolVLM2等小型VLM原则上能够在边缘设备上进行实时场景解释，并在零样本任务中展现出强大的能力，尤其是在未知领域。
*   <strong>性能差异与领域相关性：</strong> 总体而言，65.4%的生成描述被认为是正确的。然而，不同领域之间的正确性存在显著差异，其中“Campus Indoor”领域正确率最低（53.3%），而“City”领域最高（79.6%）。这表明模型在更复杂、多样化的室内场景中表现较差。
*   <strong>动作、主体和客体识别能力：</strong> 在子类别评估中，主体（Agent）识别的正确率最高（93.7%），其次是客体（Object）（83.2%）和动作（Action）（78.9%）。
*   <strong>自动化指标的局限性：</strong> BERTScore和Sentence Similarity等自动化语义相似度指标与人工评估结果存在相关性，但其相关系数（R值0.229至0.483）较低，且四分位数重叠明显，表明这些指标在评估模型性能时存在局限性和偏差。</p>
<p><strong>4. 论文中提及的局限性：</strong>
*   <strong>计算复杂性与边缘设备限制：</strong> 大型VLM的计算复杂性使其难以直接部署在边缘设备上，需要采用小型化模型。
*   <strong>准确性与推理时间的权衡：</strong> 在边缘设备上，需要在模型准确性和推理时间之间进行权衡。
*   <strong>模型偏差和弱点：</strong> 小型VLM可能生成错误的描述并引入偏差，例如难以区分密切相关但不同的动作（如“坐下”和“站起”），以及对场景中特定类型对象的重度偏见（如将白板不相关地纳入动作描述）。
*   <strong>人工评估的主观性和挑战：</strong> 零样本场景解释的地面真值人工标注任务可能存在模糊性，尤其是在开放领域中，难以识别主要动作。
*   <strong>自动化评估指标的不足：</strong> 现有的自动化指标（如BERTScore）存在社会偏见，且在开放词汇任务中适用性有限，无法完全替代人工评估。
*   <strong>信息获取的延迟：</strong> 场景复杂性和系统负载可能导致从动作发生到认知代理接收信息之间存在长达8秒的延迟，限制了其在时间敏感任务中的应用。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>改进开放词汇任务的评估指标：</strong> 迫切需要开发更好的评估指标，以减少对人工评估的需求，并更可靠地评估真实世界领域中的开放词汇任务。
*   <strong>解决模型偏差：</strong> 需要进一步研究如何识别和减轻小型VLM中固有的模型偏差，尤其是在特定领域中，以提高预测的稳定性。
*   <strong>优化信息利用：</strong> 探索如何更有效地利用VLM生成的场景描述，例如将其用于非时间敏感的辅助任务或事件文档记录。
*   <strong>降低推理延迟：</strong> 研究方法以减少边缘设备上VLM的推理延迟，使其适用于更多时间关键的应用。</p>
<hr />
<p>这篇论文为在移动机器人领域部署和评估边缘设备上的零样本场景解释VLM提供了宝贵的见解。它不仅展示了小型VLM的潜力，也坦诚地指出了当前方法的局限性，并为未来的研究指明了方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this paper, we investigate the capabilities of
state-of-the-art VLMs for the task of Scene Interpretation and Action
Recognition, with special regard to small VLMs capable of being deployed to
edge devices in the context of Mobile Robotics.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.02427v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.02427v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-11-05 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
