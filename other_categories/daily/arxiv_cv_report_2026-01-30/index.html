<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2026-01-30 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-29
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2026-01-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-18
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2026-01-29/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2026-02-02/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2026-01-30">Arxiv Computer Vision Papers - 2026-01-30</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#compression-tells-intelligence-visual-coding-visual-token-technology-and-the-unification" class="nav-link">Compression Tells Intelligence: Visual Coding, Visual Token Technology, and the Unification</a>
                </li>
                <li class="nav-item">
                    <a href="#one-step-latent-free-image-generation-with-pixel-mean-flows" class="nav-link">One-step Latent-free Image Generation with Pixel Mean Flows</a>
                </li>
                <li class="nav-item">
                    <a href="#ueval-a-benchmark-for-unified-multimodal-generation" class="nav-link">UEval: A Benchmark for Unified Multimodal Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#dynamicvla-a-vision-language-action-model-for-dynamic-object-manipulation" class="nav-link">DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation</a>
                </li>
                <li class="nav-item">
                    <a href="#do-vlms-perceive-or-recall-probing-visual-perception-vs-memory-with-classic-visual-illusions" class="nav-link">Do VLMs Perceive or Recall? Probing Visual Perception vs. Memory with Classic Visual Illusions</a>
                </li>
                <li class="nav-item">
                    <a href="#just-dub-it-video-dubbing-via-joint-audio-visual-diffusion" class="nav-link">JUST-DUB-IT: Video Dubbing via Joint Audio-Visual Diffusion</a>
                </li>
                <li class="nav-item">
                    <a href="#pi-light-physics-inspired-diffusion-for-full-image-relighting" class="nav-link">PI-Light: Physics-Inspired Diffusion for Full-Image Relighting</a>
                </li>
                <li class="nav-item">
                    <a href="#pi-light" class="nav-link">论文方法分析与总结：PI-LIGHT</a>
                </li>
                <li class="nav-item">
                    <a href="#edityourself-audio-driven-generation-and-manipulation-of-talking-head-videos-with-diffusion-transformers" class="nav-link">EditYourself: Audio-Driven Generation and Manipulation of Talking Head Videos with Diffusion Transformers</a>
                </li>
                <li class="nav-item">
                    <a href="#edityourself-audio-driven-generation-and-manipulation-of-talking-head-videos-with-diffusion-transformers_1" class="nav-link">论文方法分析与总结：《EditYourself: Audio-Driven Generation and Manipulation of Talking Head Videos with Diffusion Transformers》</a>
                </li>
                <li class="nav-item">
                    <a href="#creative-image-generation-with-diffusion-model" class="nav-link">Creative Image Generation with Diffusion Model</a>
                </li>
                <li class="nav-item">
                    <a href="#refany3d-3d-asset-referenced-diffusion-models-for-image-generation" class="nav-link">RefAny3D: 3D Asset-Referenced Diffusion Models for Image Generation</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2026-01-30">Arxiv Computer Vision Papers - 2026-01-30</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>好的，这是一份针对近期 Arxiv 计算机视觉论文的简明执行摘要，旨在帮助忙碌的研究人员快速了解该领域的重要进展：</p>
<hr />
<p><strong>执行摘要：近期 Arxiv 计算机视觉论文速览 (2026-01-28)</strong></p>
<p>本期 Arxiv 计算机视觉论文集聚焦于<strong>生成模型、多模态理解与生成、以及视觉信息的高效处理</strong>。</p>
<p><strong>主要趋势与观察：</strong></p>
<ul>
<li><strong>生成模型的持续演进：</strong> 扩散模型（Diffusion Models）依然是生成领域的核心技术，在图像生成、视频编辑和光照重构等方面展现出强大的能力。同时，对生成模型效率和质量的提升，如“无潜在空间”生成和物理启发的生成，成为研究热点。</li>
<li><strong>多模态融合的深化：</strong> 视觉-语言（Vision-Language）模型在理解和生成方面取得了显著进展，尤其是在动态场景理解、感知与记忆的区分以及音频驱动的视频生成方面。</li>
<li><strong>视觉信息的高效编码与利用：</strong> 论文探讨了如何通过压缩技术来揭示模型的智能本质，以及如何更有效地利用视觉信息进行生成和理解。</li>
</ul>
<p><strong>亮点与创新：</strong></p>
<ul>
<li><strong>"Compression Tells Intelligence: Visual Coding, Visual Token Technology, and the Unification"</strong> 提出了一种新颖的视角，将视觉压缩与模型智能联系起来，可能为理解和设计更高效的视觉模型提供新思路。</li>
<li><strong>"One-step Latent-free Image Generation with Pixel Mean Flows"</strong> 实现了无需潜在空间的单步图像生成，显著提升了生成效率，是生成模型领域的一项重要突破。</li>
<li><strong>"UEval: A Benchmark for Unified Multimodal Generation"</strong> 提出了一种统一的多模态生成评估基准，对于推动多模态研究的标准化和发展至关重要。</li>
<li><strong>"DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation"</strong> 展示了在动态场景下理解和执行动作的能力，是机器人和具身智能领域的重要进展。</li>
</ul>
<p><strong>新兴研究方向与技术：</strong></p>
<ul>
<li><strong>物理启发的生成：</strong> "PI-Light: Physics-Inspired Diffusion for Full-Image Relighting" 展示了将物理规律融入生成模型以实现更逼真的效果。</li>
<li><strong>音频-视觉联合生成：</strong> "JUST-DUB-IT: Video Dubbing via Joint Audio-Visual Diffusion" 和 "EditYourself: Audio-Driven Generation and Manipulation of Talking Head Videos with Diffusion Transformers" 均利用扩散模型在音频驱动的视频生成和编辑方面取得突破。</li>
<li><strong>感知与记忆的区分：</strong> "Do VLMs Perceive or Recall? Probing Visual Perception vs. Memory with Classic Visual Illusions" 探索了视觉语言模型（VLMs）在感知和记忆方面的能力差异，为理解模型内部机制提供了新方法。</li>
<li><strong>3D感知与生成结合：</strong> "RefAny3D: 3D Asset-Referenced Diffusion Models for Image Generation" 将 3D 资产引入扩散模型，预示着未来生成模型将更深入地融合 3D 信息。</li>
</ul>
<p><strong>建议阅读全文的论文：</strong></p>
<p>鉴于其创新性和对未来研究方向的指导意义，以下论文值得深入阅读：</p>
<ol>
<li><strong>"Compression Tells Intelligence: Visual Coding, Visual Token Technology, and the Unification"</strong>: 提供了对模型智能本质的深刻洞察。</li>
<li><strong>"One-step Latent-free Image Generation with Pixel Mean Flows"</strong>: 在生成效率方面具有革命性潜力。</li>
<li><strong>"UEval: A Benchmark for Unified Multimodal Generation"</strong>: 对于多模态研究者来说，是理解和评估模型的重要工具。</li>
<li><strong>"DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation"</strong>: 对于机器人和具身智能领域的研究者具有重要参考价值。</li>
</ol>
<hr />
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2601.20742v1">Compression Tells Intelligence: Visual Coding, Visual Token Technology, and the Unification</a></li>
<li><a href="#2601.22158v1">One-step Latent-free Image Generation with Pixel Mean Flows</a></li>
<li><a href="#2601.22155v1">UEval: A Benchmark for Unified Multimodal Generation</a></li>
<li><a href="#2601.22153v1">DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation</a></li>
<li><a href="#2601.22150v1">Do VLMs Perceive or Recall? Probing Visual Perception vs. Memory with Classic Visual Illusions</a></li>
<li><a href="#2601.22143v1">JUST-DUB-IT: Video Dubbing via Joint Audio-Visual Diffusion</a></li>
<li><a href="#2601.22135v1">PI-Light: Physics-Inspired Diffusion for Full-Image Relighting</a></li>
<li><a href="#2601.22127v1">EditYourself: Audio-Driven Generation and Manipulation of Talking Head Videos with Diffusion Transformers</a></li>
<li><a href="#2601.22125v1">Creative Image Generation with Diffusion Model</a></li>
<li><a href="#2601.22094v1">RefAny3D: 3D Asset-Referenced Diffusion Models for Image Generation</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2601.20742v1'></a></p>
<h2 id="compression-tells-intelligence-visual-coding-visual-token-technology-and-the-unification"><a href="https://arxiv.org/abs/2601.20742v1">Compression Tells Intelligence: Visual Coding, Visual Token Technology, and the Unification</a></h2>
<p><strong>Authors:</strong> Xin Jin, Jinming Liu, Yuntao Wei, Junyan Lin, Zhicheng Wang, Jianguo Huang, Xudong Yang, Yanxiao Liu, Wenjun Zeng</p>
<p><strong>Published:</strong> 2026-01-28</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>"Compression Tells Intelligence", is supported by research in artificial intelligence, particularly concerning (multimodal) large language models (LLMs/MLLMs), where compression efficiency often correlates with improved model performance and capabilities. For compression, classical visual coding based on traditional information theory has developed over decades, achieving great success with numerous international industrial standards widely applied in multimedia (e.g., image/video) systems. Except that, the recent emergingvisual token technology of generative multi-modal large models also shares a similar fundamental objective like visual coding: maximizing semantic information fidelity during the representation learning while minimizing computational cost. Therefore, this paper provides a comprehensive overview of two dominant technique families first -- Visual Coding and Vision Token Technology -- then we further unify them from the aspect of optimization, discussing the essence of compression efficiency and model performance trade-off behind. Next, based on the proposed unified formulation bridging visual coding andvisual token technology, we synthesize bidirectional insights of themselves and forecast the next-gen visual codec and token techniques. Last but not least, we experimentally show a large potential of the task-oriented token developments in the more practical tasks like multimodal LLMs (MLLMs), AI-generated content (AIGC), and embodied AI, as well as shedding light on the future possibility of standardizing a general token technology like the traditional codecs (e.g., H.264/265) with high efficiency for a wide range of intelligent tasks in a unified and effective manner.</p>
<p><strong>Analysis:</strong></p>
<p>好的，我将以AI领域专业研究生的视角，深入分析您提供的论文，并遵循您提出的分析框架。请提供论文内容，我将为您进行详细的解读。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Next, based on the proposed unified formulation bridging visual coding andvisual token technology, we synthesize bidirectional insights of themselves and forecast the next-gen visual codec and token techniques.</li>
<li>Last but not least, we experimentally show a large potential of the task-oriented token developments in the more practical tasks like multimodal LLMs (MLLMs), AI-generated content (AIGC), and embodied AI, as well as shedding light on the future possibility of standardizing a general token technology like the traditional codecs (e.g., H.264/265) with high efficiency for a wide range of intelligent tasks in a unified and effective manner.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.20742v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.20742v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.22158v1'></a></p>
<h2 id="one-step-latent-free-image-generation-with-pixel-mean-flows"><a href="https://arxiv.org/abs/2601.22158v1">One-step Latent-free Image Generation with Pixel Mean Flows</a></h2>
<p><strong>Authors:</strong> Yiyang Lu, Susie Lu, Qiao Sun, Hanhong Zhao, Zhicheng Jiang, Xianbang Wang, Tianhong Li, Zhengyang Geng, Kaiming He</p>
<p><strong>Published:</strong> 2026-01-29</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Modern diffusion/flow-based models for image generation typically exhibit two core characteristics: (i) using multi-step sampling, and (ii) operating in a latent space. Recent advances have made encouraging progress on each aspect individually, paving the way toward one-step diffusion/flow without latents. In this work, we take a further step towards this goal and propose "pixel MeanFlow" (pMF). Our core guideline is to formulate the network output space and the loss space separately. The network target is designed to be on a presumed low-dimensional image manifold (i.e., x-prediction), while the loss is defined via MeanFlow in the velocity space. We introduce a simple transformation between the image manifold and the average velocity field. In experiments, pMF achieves strong results for one-step latent-free generation on ImageNet at 256x256 resolution (2.22 FID) and 512x512 resolution (2.48 FID), filling a key missing piece in this regime. We hope that our study will further advance the boundaries of diffusion/flow-based generative models.</p>
<p><strong>Analysis:</strong></p>
<p>作为一名计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行深入分析：</p>
<p><strong>论文标题：</strong> One-step Latent-free Image Generation with Pixel Mean Flows
<strong>作者：</strong> Yiyang Lu, Susie Lu, Qiao Sun, Hanhong Zhao, Zhicheng Jiang, Xianbang Zhao, Tianhong Li, Zhengyang Geng, Kaiming He
<strong>发表日期：</strong> 2026-01-29</p>
<hr />
<h3 id="1-2-3">1. 论文的主要贡献（2-3句话的简洁总结）</h3>
<p>本研究提出了一种名为“像素均值流”（pixel MeanFlow, pMF）的新型图像生成模型，它实现了<strong>一步式、无潜在空间</strong>的图像生成。通过将网络输出空间和损失空间进行分离设计，pMF能够直接在像素空间进行高效生成，并在ImageNet数据集上取得了具有竞争力的生成质量，为实现更高效的生成模型迈出了重要一步。</p>
<h3 id="2">2. 关键创新或方法论</h3>
<p>pMF的核心创新在于其<strong>网络输出空间和损失空间的独立设计</strong>。具体来说：</p>
<ul>
<li><strong>网络目标（输出空间）：</strong> 模型被训练来预测图像流形上的一个点（即直接预测图像像素值，x-prediction）。这与许多现有模型在潜在空间中操作不同，避免了潜在空间的编码和解码过程。</li>
<li><strong>损失定义（损失空间）：</strong> 损失函数被定义在“速度场”（velocity space）上，并利用“均值流”（MeanFlow）的概念。这意味着模型学习的是如何将一个点（当前状态）映射到另一个点（下一个状态）的“平均速度”，而不是直接预测下一个状态本身。</li>
<li><strong>图像流形与平均速度场之间的简单变换：</strong> 论文引入了一种机制来连接图像流形上的点（像素值）和平均速度场。这种变换使得模型能够有效地学习生成过程，即使是在像素空间直接操作。</li>
</ul>
<p>这种分离设计允许模型在更直观的像素空间进行学习，同时利用速度场来指导生成过程，从而实现高效的一步式生成。</p>
<h3 id="3">3. 对该领域的潜在影响</h3>
<p>pMF的提出可能对图像生成领域产生以下重要影响：</p>
<ul>
<li><strong>加速生成过程：</strong> 一步式生成极大地缩短了生成图像所需的时间，这对于需要实时生成或大规模生成应用的场景至关重要。</li>
<li><strong>简化模型架构：</strong> 消除潜在空间的设计可以简化模型的整体架构，减少参数量，并可能降低训练和推理的计算复杂度。</li>
<li><strong>推动无潜在空间生成研究：</strong> 尽管已有研究尝试无潜在空间生成，但pMF在性能上取得了显著突破，有望激发更多关于直接在像素空间进行高效生成的研究。</li>
<li><strong>为其他生成模型提供新思路：</strong> 其将输出空间和损失空间解耦的设计思想，可能为其他类型的生成模型（如GANs、VAE等）提供新的设计灵感。</li>
</ul>
<h3 id="4">4. 可能受益于此研究的相关领域或应用</h3>
<ul>
<li><strong>实时图像生成：</strong> 游戏、虚拟现实、增强现实等需要实时生成高质量图像的领域。</li>
<li><strong>视频生成：</strong> 将一步式生成技术扩展到视频领域，可以实现更流畅、更快速的视频合成。</li>
<li><strong>图像编辑和修复：</strong> 高效的生成能力可以加速图像编辑和修复任务的完成。</li>
<li><strong>内容创作：</strong> 艺术家和设计师可以利用更快的生成工具来探索创意。</li>
<li><strong>数据增强：</strong> 在训练机器学习模型时，可以更快地生成大量合成数据。</li>
<li><strong>低资源环境下的生成：</strong> 简化模型和加速生成过程，使其更容易在计算资源受限的设备上部署。</li>
</ul>
<h3 id="5">5. 可从摘要推断出的局限性</h3>
<p>尽管摘要展示了令人鼓舞的结果，但仍可以推断出一些潜在的局限性：</p>
<ul>
<li><strong>“平均速度场”的局限性：</strong> 尽管论文声称引入了“简单变换”，但如何精确地定义和学习这个“平均速度场”以及它是否能捕捉到所有复杂的图像细节和变化，仍需进一步研究。在某些高度复杂或多模态的生成任务中，平均速度场可能不足以完全描述生成过程。</li>
<li><strong>对“低维图像流形”的假设：</strong> 论文假设图像存在一个“低维图像流形”。虽然这是许多生成模型的基础假设，但对于非常高分辨率或包含大量噪声的图像，这个假设的有效性可能受到挑战。</li>
<li><strong>泛化能力：</strong> 摘要主要展示了在ImageNet上的结果。模型在其他类型的数据集（如人脸、医学图像、文本到图像等）上的泛化能力尚未明确提及。</li>
<li><strong>训练稳定性：</strong> 新颖的方法论有时会带来训练上的挑战，例如收敛速度、对超参数的敏感性等，这些在摘要中并未详细说明。</li>
<li><strong>“一步式”的定义：</strong> 虽然称为“一步式”，但实际的采样过程可能仍然涉及多次迭代（尽管比传统扩散模型少得多），或者“一步”的定义可能与直观理解有所不同。</li>
<li><strong>与现有方法的比较深度：</strong> 摘要提到了“filling a key missing piece in this regime”，暗示了在一步式无潜在空间生成领域取得了突破。但与最先进的多步生成模型相比，其在生成多样性、细节保真度等方面是否完全超越，仍需在论文正文中详细评估。</li>
</ul>
<p>总而言之，pMF通过其独特的设计理念，在一步式无潜在空间图像生成领域取得了显著进展，为未来的生成模型研究开辟了新的方向。其核心在于巧妙地解耦了网络输出和损失计算，使得模型能够直接在像素空间高效学习生成过程。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce a simple transformation between the image manifold and the average velocity field.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.22158v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.22158v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.22155v1'></a></p>
<h2 id="ueval-a-benchmark-for-unified-multimodal-generation"><a href="https://arxiv.org/abs/2601.22155v1">UEval: A Benchmark for Unified Multimodal Generation</a></h2>
<p><strong>Authors:</strong> Bo Li, Yida Yin, Wenhao Chai, Xingyu Fu, Zhuang Liu</p>
<p><strong>Published:</strong> 2026-01-29</p>
<p><strong>Categories:</strong> cs.CV, cs.CL</p>
<p><strong>Abstract:</strong></p>
<p>We introduce UEval, a benchmark to evaluate unified models, i.e., models capable of generating both images and text. UEval comprises 1,000 expert-curated questions that require both images and text in the model output, sourced from 8 real-world tasks. Our curated questions cover a wide range of reasoning types, from step-by-step guides to textbook explanations. Evaluating open-ended multimodal generation is non-trivial, as simple LLM-as-a-judge methods can miss the subtleties. Different from previous works that rely on multimodal Large Language Models (MLLMs) to rate image quality or text accuracy, we design a rubric-based scoring system in UEval. For each question, reference images and text answers are provided to a MLLM to generate an initial rubric, consisting of multiple evaluation criteria, and human experts then refine and validate these rubrics. In total, UEval contains 10,417 validated rubric criteria, enabling scalable and fine-grained automatic scoring. UEval is challenging for current unified models: GPT-5-Thinking scores only 66.4 out of 100, while the best open-source model reaches merely 49.1. We observe that reasoning models often outperform non-reasoning ones, and transferring reasoning traces from a reasoning model to a non-reasoning model significantly narrows the gap. This suggests that reasoning may be important for tasks requiring complex multimodal understanding and generation.</p>
<p><strong>Analysis:</strong></p>
<p>好的，我将扮演一名AI领域高水平研究生，专注于深入分析论文的方法部分，重点关注创新点和新视角，并提供结构化的分析。</p>
<p>请提供您希望我分析的论文。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce UEval, a benchmark to evaluate unified models, i.e., models capable of generating both images and text.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.22155v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.22155v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.22153v1'></a></p>
<h2 id="dynamicvla-a-vision-language-action-model-for-dynamic-object-manipulation"><a href="https://arxiv.org/abs/2601.22153v1">DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation</a></h2>
<p><strong>Authors:</strong> Haozhe Xie, Beichen Wen, Jiarui Zheng, Zhaoxi Chen, Fangzhou Hong, Haiwen Diao, Ziwei Liu</p>
<p><strong>Published:</strong> 2026-01-29</p>
<p><strong>Categories:</strong> cs.RO, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Manipulating dynamic objects remains an open challenge for Vision-Language-Action (VLA) models, which, despite strong generalization in static manipulation, struggle in dynamic scenarios requiring rapid perception, temporal anticipation, and continuous control. We present DynamicVLA, a framework for dynamic object manipulation that integrates temporal reasoning and closed-loop adaptation through three key designs: 1) a compact 0.4B VLA using a convolutional vision encoder for spatially efficient, structurally faithful encoding, enabling fast multimodal inference; 2) Continuous Inference, enabling overlapping reasoning and execution for lower latency and timely adaptation to object motion; and 3) Latent-aware Action Streaming, which bridges the perception-execution gap by enforcing temporally aligned action execution. To fill the missing foundation of dynamic manipulation data, we introduce the Dynamic Object Manipulation (DOM) benchmark, built from scratch with an auto data collection pipeline that efficiently gathers 200K synthetic episodes across 2.8K scenes and 206 objects, and enables fast collection of 2K real-world episodes without teleoperation. Extensive evaluations demonstrate remarkable improvements in response speed, perception, and generalization, positioning DynamicVLA as a unified framework for general dynamic object manipulation across embodiments.</p>
<p><strong>Analysis:</strong></p>
<p>好的，我将以一名AI领域高水平研究生的视角，深入分析这篇关于“DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation”的论文。</p>
<hr />
<h3 id="1">1. 摘要翻译</h3>
<p><strong>论文摘要翻译：</strong></p>
<p>“动态VLA：一种用于动态物体操作的视觉-语言-动作模型</p>
<p>操作动态物体仍然是视觉-语言-动作（VLA）模型的开放性挑战，尽管它们在静态操作中表现出强大的泛化能力，但在需要快速感知、时间预测和连续控制的动态场景中却举步维艰。我们提出了DynamicVLA，一个用于动态物体操作的框架，它通过三个关键设计集成了时间推理和闭环适应：1）一个紧凑的0.4B VLA，采用卷积视觉编码器进行空间高效、结构忠实的编码，以实现快速的多模态推理；2）连续推理（Continuous Inference），实现推理和执行的重叠，以实现低延迟和对物体运动的及时适应；3）潜在感知（Latent-aware）动作流，通过强制执行时间对齐的动作执行来弥合感知-执行的差距。为了填补动态操作数据的缺失基础，我们引入了动态物体操作（DOM）基准，该基准从头开始构建，具有自动数据收集管道，该管道能够高效地收集跨越2.8K场景和206个对象的200K合成数据，以及2K真实世界数据的快速收集，无需远程操作。广泛的评估证明了在响应速度、感知和泛化能力方面取得了显著的改进，使DynamicVLA成为跨不同载体的通用动态物体操作的统一框架。”</p>
<hr />
<h3 id="2_1">2. 方法动机分析</h3>
<ul>
<li>
<p><strong>驱动力</strong>：</p>
<ul>
<li><strong>核心问题</strong>：现有VLA模型在处理<strong>动态物体操作</strong>时存在显著的<strong>感知-执行（P.E.）差距</strong>和<strong>延迟</strong>问题。当物体在运动时，模型预测的动作可能与实际环境状态不同步，导致操作失败。</li>
<li><strong>现有方法的局限性</strong>：<ul>
<li><strong>静态操作的局限</strong>：现有VLA模型在静态场景下表现良好，因为物体状态在推理过程中保持不变，延迟影响较小。</li>
<li><strong>动态场景的挑战</strong>：动态场景需要<strong>快速感知</strong>、<strong>时间预测</strong>（anticipation）和<strong>连续控制</strong>，而现有模型在这方面能力不足。</li>
<li><strong>延迟问题</strong>：推理延迟导致感知到的物体状态与执行动作时的实际状态不匹配，尤其是在物体运动速度较快时。</li>
<li><strong>数据稀缺</strong>：缺乏大规模、多样化的动态物体操作数据集，阻碍了模型训练和评估。</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>现有方法痛点</strong>：</p>
<ul>
<li><strong>感知-执行（P.E.）差距</strong>：模型预测的动作与实际环境状态不同步。</li>
<li><strong>推理延迟</strong>：模型需要较长时间进行推理，导致动作执行滞后。</li>
<li><strong>动作执行中断</strong>：推理和执行是串行的，导致动作执行过程中出现等待（inter-chunk waiting）。</li>
<li><strong>时间对齐困难</strong>：在动态环境中，确保感知和执行在时间上对齐非常困难。</li>
<li><strong>数据不足</strong>：现有数据集多为静态场景，无法有效训练动态操作模型。</li>
</ul>
</li>
<li>
<p><strong>研究假设</strong>：</p>
<ul>
<li>通过<strong>紧凑的模型架构</strong>、<strong>高效的推理执行机制</strong>以及<strong>大规模的动态操作数据</strong>，可以显著提升VLA模型在动态物体操作任务上的性能。</li>
<li><strong>时间上的同步性</strong>（temporal alignment）和<strong>低延迟的闭环控制</strong>是解决动态物体操作问题的关键。</li>
</ul>
</li>
</ul>
<hr />
<h3 id="3_1">3. 方法设计详解</h3>
<p><strong>方法pipeline总结：</strong></p>
<p>DynamicVLA的核心在于解决动态物体操作中的<strong>延迟</strong>和<strong>时间不对齐</strong>问题，通过三个关键组件实现：<strong>紧凑模型</strong>、<strong>连续推理（CI）</strong>和<strong>潜在感知动作流（LAAS）</strong>，并辅以<strong>DOM基准</strong>进行训练和评估。</p>
<p><strong>整体架构（图2a）：</strong></p>
<p>DynamicVLA采用一个<strong>0.4B参数的VLA模型</strong>，由一个<strong>视觉-语言骨干网络（Vision-Language Backbone）</strong>和一个<strong>动作专家（Action Expert）</strong>组成。</p>
<ol>
<li>
<p><strong>视觉-语言骨干网络 (Vision-Language Backbone)</strong>:</p>
<ul>
<li><strong>目标</strong>：实现<strong>空间高效</strong>、<strong>结构忠实</strong>的编码，以支持<strong>快速多模态推理</strong>。</li>
<li><strong>模型选择</strong>：<ul>
<li><strong>语言骨干</strong>：采用<strong>SmolLM2-360M</strong>的<strong>前16层</strong>。<ul>
<li><strong>动机</strong>：SmolLM2本身是一个紧凑模型，选择前16层进一步<strong>减少参数量和计算量</strong>，从而降低推理延迟。这借鉴了SmolVLA的策略，在不显著影响多模态推理能力的情况下加速。</li>
</ul>
</li>
<li><strong>视觉编码器</strong>：采用<strong>FastViT</strong>。<ul>
<li><strong>动机</strong>：与Transformer类视觉编码器不同，FastViT是<strong>卷积神经网络</strong>，能够实现<strong>高效的空间压缩</strong>，避免了多帧输入时Transformer的二次方计算复杂度（quadratic token growth）。这有助于保持模型紧凑并加速处理。</li>
</ul>
</li>
</ul>
</li>
<li><strong>输入</strong>：<ul>
<li><strong>视觉观察</strong>：一个时间窗口内的视觉观察 <script type="math/tex">O_t = \{o_{t-k}, \dots, o_t\}</script>。</li>
<li><strong>语言指令</strong>：<script type="math/tex">L_t</script>。</li>
<li><strong>本体感受状态</strong>：<script type="math/tex">P_t</script>（如关节角度、末端执行器位姿）。</li>
</ul>
</li>
<li><strong>输出</strong>：<strong>多模态特征</strong>，用于喂给动作专家。</li>
<li><strong>多模态融合与投影</strong>：使用<strong>轻量级线性投影</strong>将机器人状态嵌入到多模态特征空间，将动作表示适配到扩散模型动作专家，并匹配骨干网络和动作专家的输出维度。</li>
</ul>
</li>
<li>
<p><strong>动作专家 (Action Expert)</strong>:</p>
<ul>
<li><strong>目标</strong>：根据骨干网络输出的多模态特征，预测一个<strong>动作序列</strong> <script type="math/tex">A_t = \{a_t, \dots, a_{t+n}\}</script>。</li>
<li><strong>模型选择</strong>：采用<strong>条件化流匹配Transformer（Conditional Flow Matching Transformer）</strong>。<ul>
<li><strong>动机</strong>：流匹配（Flow Matching）是一种生成模型方法，可以高效地学习从噪声到目标分布的映射，适用于动作生成。Transformer结构则能处理序列数据。</li>
</ul>
</li>
<li><strong>训练目标</strong>：使用流匹配的损失函数 <script type="math/tex">l(\theta) = E_{p(\hat{A}_t|A_t), q(A_t|A_t)} [\|E_\theta(A_t, O_t) - u(A_t|A_t)\|]</script>，其中 <script type="math/tex">E_\theta</script> 是动作专家， <script type="math/tex">u</script> 是去噪向量场。目标是让动作专家学习匹配去噪向量场，从而生成动作序列。</li>
<li><strong>动作表示</strong>：每个动作 <script type="math/tex">a_t</script> 是一个32维向量，代表末端执行器位姿和抓取器状态。</li>
</ul>
</li>
<li>
<p><strong>连续推理 (Continuous Inference, CI) (图2b)</strong>:</p>
<ul>
<li><strong>动机</strong>：解决传统VLA模型中<strong>推理和执行串行化</strong>导致<strong>动作执行等待</strong>（inter-chunk waiting）的问题。</li>
<li><strong>核心思想</strong>：<strong>重叠推理和执行</strong>。当一个推理周期（预测一个动作序列 <script type="math/tex">A_t</script>）完成时，即使前一个动作序列的执行尚未完全结束，立即开始下一个推理周期（预测 <script type="math/tex">A_{t+m}</script>）。</li>
<li><strong>流程</strong>：<ul>
<li>推理周期以 <script type="math/tex">t, t+m, t+2m, \dots</script> 的时间点触发，其中 <script type="math/tex">m</script> 是推理延迟。</li>
<li>在执行动作 <script type="math/tex">a_t, a_{t+1}, \dots</script> 的同时，模型正在推理计算 <script type="math/tex">A_{t+m} = \{a_{t+m}, \dots, a_{t+m+n}\}</script>。</li>
<li>假设动作序列长度 <script type="math/tex">n</script> 大于推理延迟 <script type="math/tex">m</script> (<script type="math/tex">n > m</script>)，这样新的动作序列总能在旧序列执行完毕前生成。</li>
</ul>
</li>
<li><strong>效果</strong>：消除了推理和执行之间的等待时间，实现了<strong>非阻塞的动作执行</strong>，提高了对动态物体运动的响应速度。</li>
</ul>
</li>
<li>
<p><strong>潜在感知动作流 (Latent-aware Action Streaming, LAAS) (图2c)</strong>:</p>
<ul>
<li><strong>动机</strong>：CI虽然解决了等待问题，但推理延迟 <script type="math/tex">m</script> 仍然会导致<strong>感知-执行之间的不匹配</strong>。当模型在时间 <script type="math/tex">t</script> 开始推理预测 <script type="math/tex">A_t</script> 时，动作将在 <script type="math/tex">t+m</script> 时刻可用，但此时环境状态已演变为 <script type="math/tex">O_{t+m}</script>。这导致预测的动作 <script type="math/tex">A_t</script> 可能与实际环境不符。</li>
<li><strong>核心思想</strong>：<strong>时间对齐的动作执行策略</strong>。通过丢弃过时的动作，并优先使用最新预测的动作，来维持时间上的一致性。</li>
<li><strong>流程</strong>：<ul>
<li><strong>感知-执行差距处理</strong>：当模型在时间 <script type="math/tex">t</script> 开始推理预测 <script type="math/tex">A_t</script> 时，这些动作将在 <script type="math/tex">t+m</script> 时刻可用。此时，环境已演变到 <script type="math/tex">O_{t+m}</script>。</li>
<li><strong>动作覆盖与丢弃</strong>：<ul>
<li>对于时间步 <script type="math/tex">t</script> 到 <script type="math/tex">t+m-1</script> 的动作（即 <script type="math/tex">a_t, \dots, a_{t+m-1}</script>），它们被认为是<strong>过时</strong>的，因为它们是在旧的环境状态下预测的。这些动作将被<strong>丢弃</strong>。</li>
<li>执行将从新预测的动作序列 <script type="math/tex">A_{t+m}</script> 的第一个动作 <script type="math/tex">a_{t+m}</script> 开始。</li>
<li>对于重叠的时间步（例如，当 <script type="math/tex">A_t</script> 的执行尚未完成，而 <script type="math/tex">A_{t+m}</script> 已经生成时），<strong>来自新序列 <script type="math/tex">A_{t+m}</script> 的动作将覆盖来自旧序列 <script type="math/tex">A_t</script> 的动作</strong>。</li>
</ul>
</li>
</ul>
</li>
<li><strong>效果</strong>：确保执行的动作始终与<strong>最近的感知信息</strong>对齐，即使存在推理延迟，也能实现<strong>时间上一致的控制</strong>，从而更及时地适应动态物体运动。</li>
</ul>
</li>
<li>
<p><strong>动态物体操作（DOM）基准 (Dynamic Object Manipulation Benchmark)</strong>:</p>
<ul>
<li><strong>动机</strong>：现有数据集缺乏动态物体操作的场景，无法有效训练和评估模型。</li>
<li><strong>数据收集</strong>：<ul>
<li><strong>自动化管道</strong>：通过<strong>模拟器（Isaac Sim）</strong>和<strong>真实世界“模拟器”</strong>（利用RGB-D传感器和状态估计）实现。</li>
<li><strong>模拟数据</strong>：生成<strong>200K合成数据</strong>，包含<strong>2.8K多样化场景</strong>和<strong>206个对象</strong>。</li>
<li><strong>真实世界数据</strong>：收集<strong>2K真实世界数据</strong>，无需远程操作，解决了真实世界中动态操作数据收集的挑战（人类反应速度不足以跟踪快速移动的物体）。</li>
</ul>
</li>
<li><strong>评估维度</strong>：<ul>
<li><strong>交互（Interaction）</strong>：评估模型对物体运动的响应速度、动态适应性和长时序协调能力。</li>
<li><strong>感知（Perception）</strong>：评估模型在动态环境中理解视觉、语言线索的能力，包括视觉理解、空间推理和运动感知。</li>
<li><strong>泛化（Generalization）</strong>：评估模型在面对未见过物体、场景和运动模式时的鲁棒性，包括视觉泛化、运动泛化和扰动鲁棒性。</li>
</ul>
</li>
</ul>
</li>
</ol>
<p><strong>关键公式/算法解释：</strong></p>
<ul>
<li><strong>流匹配损失 <script type="math/tex">l(\theta)</script></strong>:<ul>
<li><strong>意义</strong>：这是动作专家（一个去噪模型）的训练目标。它旨在学习一个<strong>向量场</strong> <script type="math/tex">u(A_t|O_t)</script>，该向量场能够将一个随机噪声动作 <script type="math/tex">A_t</script>
<strong>平滑地“去噪”</strong>（或称为“流向”）到与观察 <script type="math/tex">O_t</script> 相对应的真实动作序列。</li>
<li><strong><script type="math/tex">E_\theta(A_t, O_t)</script></strong>: 动作专家预测的去噪向量。</li>
<li><strong><script type="math/tex">u(A_t|O_t)</script></strong>: 目标去噪向量场（通常通过一个预定义的“流”函数计算得到）。</li>
<li><strong><script type="math/tex">A_t = \tau \hat{A}_t + (1-\tau)\epsilon</script></strong>: 这是流匹配中的一个插值过程，<script type="math/tex">\tau</script> 是一个从0到1变化的参数，<script type="math/tex">\hat{A}_t</script> 是一个随机噪声，<script type="math/tex">\epsilon</script> 是一个随机变量。这个插值生成了不同“噪声水平”的动作样本。</li>
<li><strong><script type="math/tex">q(A_t|A_t) = \mathcal{N}(\tau A_t, (1-\tau)I)</script></strong>: 这是流匹配中的一个概率分布，用于采样。</li>
<li><strong>核心作用</strong>：通过最小化预测向量场与目标向量场之间的差异，动作专家学会了如何根据视觉观察生成连贯、有意义的动作序列。</li>
</ul>
</li>
</ul>
<hr />
<h3 id="4_1">4. 方法对比分析</h3>
<ul>
<li>
<p><strong>本质区别</strong>：</p>
<ul>
<li><strong>动态处理</strong>：DynamicVLA的核心在于<strong>显式地处理动态物体操作中的时间延迟和不匹配问题</strong>。它不是简单地追求低延迟，而是通过CI和LAAS来<strong>管理和补偿延迟</strong>。</li>
<li><strong>执行机制</strong>：大多数现有VLA模型采用<strong>串行推理-执行</strong>模式，而DynamicVLA的<strong>CI</strong>实现了<strong>并行/重叠</strong>，<strong>LAAS</strong>则提供了<strong>智能的动作选择和覆盖策略</strong>。</li>
<li><strong>模型设计</strong>：DynamicVLA强调<strong>紧凑模型</strong>（0.4B参数）以支持<strong>高频推理</strong>，并结合<strong>卷积视觉编码器</strong>以加速处理。</li>
</ul>
</li>
<li>
<p><strong>创新贡献</strong>：</p>
<ul>
<li><strong>CI (Continuous Inference)</strong>：首次提出将推理和执行<strong>流水线化</strong>，消除动作执行中的等待时间。</li>
<li><strong>LAAS (Latent-aware Action Streaming)</strong>：提出一种<strong>智能的动作选择和覆盖机制</strong>，以补偿推理延迟带来的感知-执行不匹配。</li>
<li><strong>DOM基准</strong>：构建了一个<strong>大规模、自动化的动态物体操作数据集</strong>，解决了数据稀缺问题。</li>
<li><strong>统一框架</strong>：将紧凑模型、高效执行机制和数据收集整合，形成一个<strong>端到端的动态物体操作解决方案</strong>。</li>
</ul>
</li>
<li>
<p><strong>适用场景</strong>：</p>
<ul>
<li><strong>动态物体操作任务</strong>：如抓取、放置、稳定等需要机器人与运动物体进行交互的任务。</li>
<li><strong>对实时性要求高的场景</strong>：需要快速响应和连续控制的应用。</li>
<li><strong>机器人控制</strong>：尤其是在需要处理不确定性和快速变化的真实世界环境中。</li>
</ul>
</li>
</ul>
<hr />
<h3 id="5_1">5. 实验分析</h3>
<ul>
<li>
<p><strong>验证方法</strong>：</p>
<ul>
<li><strong>基准测试</strong>：在提出的<strong>DOM基准</strong>上进行评估，包括模拟和真实世界实验。</li>
<li><strong>对比模型</strong>：与多种代表性的VLA基线模型进行比较，包括Diffusion Policy, OpenVLA-OFT, π0, π0.5, SmolVLA, GR00T-N1.5, VLA-Adapter-Pro, VLASH等。</li>
<li><strong>消融研究</strong>：通过移除或替换DynamicVLA的关键组件（如LLM骨干大小、视觉编码器、CI、LAAS）来评估每个组件的贡献。</li>
<li><strong>评估指标</strong>：成功率（SR）、路径长度（PL）、任务完成时间（Time）。</li>
</ul>
</li>
<li>
<p><strong>关键结果</strong>：</p>
<ul>
<li><strong>整体性能优越</strong>：DynamicVLA在DOM基准的<strong>交互、感知和泛化</strong>维度上均取得了显著优于基线模型的性能（如表I所示，在Interaction-CR/DA/LS上，DynamicVLA的成功率分别为60.5%/38.5%/40.5%，远超基线）。</li>
<li><strong>实时性提升</strong>：CI和LAAS显著降低了<strong>任务完成时间</strong>（表I中DynamicVLA的Time为8.53s，远低于其他模型）。</li>
<li><strong>泛化能力强</strong>：在处理未见过物体、场景和运动模式时，DynamicVLA表现出更好的泛化能力。</li>
<li><strong>消融研究结果</strong>：<ul>
<li><strong>LLM骨干大小</strong>：360M参数的SmolLM2骨干在性能和效率之间取得了最佳平衡（表II）。</li>
<li><strong>视觉编码器</strong>：FastViT比Transformer编码器在降低延迟方面更有效（表II）。</li>
<li><strong>CI和LAAS的重要性</strong>：移除CI或LAAS都会导致性能显著下降（表II和表V），表明它们是动态操作成功的关键。CI和LAAS的组合效果最佳。</li>
<li><strong>时间上下文</strong>：使用稀疏但足够的时间上下文（如<script type="math/tex">O_t = \{o_{t-2}, o_t\}</script>）比单帧或密集上下文更有效（表III）。</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>优势场景</strong>：</p>
<ul>
<li><strong>高动态性场景</strong>：在物体运动速度快、变化剧烈（如方向改变、碰撞）的场景下，DynamicVLA的CI和LAAS机制能有效应对（图4和图5）。</li>
<li><strong>长时序任务</strong>：在需要连续执行多个动作以完成复杂任务时，DynamicVLA的鲁棒性更强（图4中的“Gather all ping pong balls”任务）。</li>
<li><strong>需要精确控制的场景</strong>：尽管是动态场景，但DOM基准也包含需要精确6DoF控制的任务，DynamicVLA在此类任务上表现出优于其他VLA模型的性能。</li>
</ul>
</li>
<li>
<p><strong>局限性</strong>：</p>
<ul>
<li><strong>扰动鲁棒性</strong>：在处理<strong>极端环境扰动</strong>（如表面不平整、意外碰撞）时，即使是DynamicVLA，其性能仍有待提高（表I中的DR维度）。</li>
<li><strong>模型容量与延迟的权衡</strong>：虽然DynamicVLA通过紧凑模型和高效执行机制取得了良好平衡，但更复杂的感知和推理任务可能仍需要更大的模型容量，这会增加延迟。</li>
<li><strong>非刚体动力学</strong>：论文中提到，数据管道假设<strong>刚体状态估计</strong>，对于非刚体或流体动力学任务（如抓取易变形物体）可能需要进一步扩展。</li>
</ul>
</li>
</ul>
<hr />
<h3 id="6">6. 实用指南</h3>
<ul>
<li><strong>开源情况</strong>：论文作者通常会提供代码和数据集。根据论文信息，可以查找其GitHub仓库（如摘要中提到的<code>https://haozhexie.com/project/dynamic-vla</code>）。</li>
<li>
<p><strong>实现/复现的关键步骤</strong>：</p>
<ul>
<li><strong>环境搭建</strong>：需要配置好模拟器（Isaac Sim）和真实机器人环境（Franka Emika Panda, AgileX PiPER）。</li>
<li><strong>数据准备</strong>：使用论文提供的DOM基准数据，或自行收集类似动态操作数据。</li>
<li><strong>模型训练</strong>：<ul>
<li><strong>预训练</strong>：使用COYO-700M等大规模图文数据集进行视觉-语言骨干的预训练。</li>
<li><strong>中训练</strong>：在DOM数据集上训练整个VLA模型，重点关注CI和LAAS的有效性。</li>
<li><strong>后训练</strong>：在特定机器人载体上进行微调，以适应其独特的传感器和执行器。</li>
</ul>
</li>
<li><strong>超参数调优</strong>：学习率、批大小、优化器参数（AdamW）、学习率调度器（cosine annealing）、时间窗口大小（<script type="math/tex">k</script>）、动作序列长度（<script type="math/tex">n</script>）、推理延迟（<script type="math/tex">m</script>）等都需要仔细调整。</li>
<li><strong>推理部署</strong>：确保模型推理速度能够满足实时性要求，通常需要GPU加速。</li>
</ul>
</li>
<li>
<p><strong>迁移可能</strong>：</p>
<ul>
<li><strong>迁移到其他动态操作任务</strong>：该框架（特别是CI和LAAS机制）可以很好地迁移到其他需要实时响应和处理动态物体的机器人任务中，如装配、物流搬运等。</li>
<li><strong>迁移到不同机器人载体</strong>：通过后训练阶段，模型可以适应不同的机器人手臂和传感器配置。</li>
<li><strong>迁移到其他VLA模型</strong>：CI和LAAS机制可以作为模块集成到其他现有的VLA模型中，以提升其在动态场景下的性能，前提是这些模型也具备一定的实时推理能力。</li>
</ul>
</li>
</ul>
<hr />
<h3 id="7">7. 总结</h3>
<ul>
<li>
<p><strong>核心思想</strong>：<strong>通过高效执行机制补偿延迟，实现动态物体操作的实时闭环控制。</strong></p>
</li>
<li>
<p><strong>速记版pipeline</strong>：</p>
<ol>
<li><strong>快速感知</strong>：用紧凑模型（SmolLM2+FastViT）快速理解物体和指令。</li>
<li><strong>并行推理执行</strong>：让模型一边思考一边动手（CI），避免等待。</li>
<li><strong>智能丢弃旧动作</strong>：只用最新的预测来指导行动（LAAS），确保同步。</li>
<li><strong>用海量数据训练</strong>：在大量动态场景数据上学习（DOM基准）。</li>
</ol>
</li>
</ul>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present DynamicVLA, a framework for dynamic object manipulation that integrates temporal reasoning and closed-loop adaptation through three key designs: 1) a compact 0.4B VLA using a convolutional vision encoder for spatially efficient, structurally faithful encoding, enabling fast multimodal inference; 2) Continuous Inference, enabling overlapping reasoning and execution for lower latency and timely adaptation to object motion; and 3) Latent-aware Action Streaming, which bridges the perception-execution gap by enforcing temporally aligned action execution.</li>
<li>To fill the missing foundation of dynamic manipulation data, we introduce the Dynamic Object Manipulation (DOM) benchmark, built from scratch with an auto data collection pipeline that efficiently gathers 200K synthetic episodes across 2.8K scenes and 206 objects, and enables fast collection of 2K real-world episodes without teleoperation.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.22153v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.22153v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.22150v1'></a></p>
<h2 id="do-vlms-perceive-or-recall-probing-visual-perception-vs-memory-with-classic-visual-illusions"><a href="https://arxiv.org/abs/2601.22150v1">Do VLMs Perceive or Recall? Probing Visual Perception vs. Memory with Classic Visual Illusions</a></h2>
<p><strong>Authors:</strong> Xiaoxiao Sun, Mingyang Li, Kun yuan, Min Woo Sun, Mark Endo, Shengguang Wu, Changlin Li, Yuhui Zhang, Zeyu Wang, Serena Yeung-Levy</p>
<p><strong>Published:</strong> 2026-01-29</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Large Vision-Language Models (VLMs) often answer classic visual illusions "correctly" on original images, yet persist with the same responses when illusion factors are inverted, even though the visual change is obvious to humans. This raises a fundamental question: do VLMs perceive visual changes or merely recall memorized patterns? While several studies have noted this phenomenon, the underlying causes remain unclear. To move from observations to systematic understanding, this paper introduces VI-Probe, a controllable visual-illusion framework with graded perturbations and matched visual controls (without illusion inducer) that disentangles visually grounded perception from language-driven recall. Unlike prior work that focuses on averaged accuracy, we measure stability and sensitivity using Polarity-Flip Consistency, Template Fixation Index, and an illusion multiplier normalized against matched controls. Experiments across different families reveal that response persistence arises from heterogeneous causes rather than a single mechanism. For instance, GPT-5 exhibits memory override, Claude-Opus-4.1 shows perception-memory competition, while Qwen variants suggest visual-processing limits. Our findings challenge single-cause views and motivate probing-based evaluation that measures both knowledge and sensitivity to controlled visual change. Data and code are available at https://sites.google.com/view/vi-probe/.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为一名计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行深入分析。</p>
<p><strong>论文摘要分析：</strong></p>
<p><strong>Title:</strong> Do VLMs Perceive or Recall? Probing Visual Perception vs. Memory with Classic Visual Illusions
<strong>Authors:</strong> Xiaoxiao Sun, Mingyang Li, Kun yuan, Min Woo Sun, Mark Endo, Shengguang Wu, Changlin Li, Yuhui Zhang, Zeyu Wang, Serena Yeung-Levy
<strong>Categories:</strong> cs.CV
<strong>Published Date:</strong> 2026-01-29</p>
<p><strong>Abstract:</strong>
Large Vision-Language Models (VLMs) often answer classic visual illusions "correctly" on original images, yet persist with the same responses when illusion factors are inverted, even though the visual change is obvious to humans. This raises a fundamental question: do VLMs perceive visual changes or merely recall memorized patterns? While several studies have noted this phenomenon, the underlying causes remain unclear. To move from observations to systematic understanding, this paper introduces VI-Probe, a controllable visual-illusion framework with graded perturbations and matched visual controls (without illusion inducer) that disentangles visually grounded perception from language-driven recall. Unlike prior work that focuses on averaged accuracy, we measure stability and sensitivity using Polarity-Flip Consistency, Template Fixation Index, and an illusion multiplier normalized against matched controls. Experiments across different families reveal that response persistence arises from heterogeneous causes rather than a single mechanism. For instance, GPT-5 exhibits memory override, Claude-Opus-4.1 shows perception-memory competition, while Qwen variants suggest visual-processing limits. Our findings challenge single-cause views and motivate probing-based evaluation that measures both knowledge and sensitivity to controlled visual change. Data and code are available at https://sites.google.com/view/vi-probe/.</p>
<hr />
<p><strong>我的分析如下：</strong></p>
<p><strong>1. 论文的主要贡献（2-3句话）：</strong></p>
<p>这篇论文通过引入一个名为 VI-Probe 的可控视觉错觉框架，系统地探究了大型视觉语言模型（VLMs）在面对视觉错觉时，是真正感知了视觉变化还是仅仅依赖于记忆。研究发现，VLMs 对错觉的“固执”反应并非源于单一机制，而是由多种原因导致，例如记忆覆盖、感知与记忆的竞争，以及视觉处理能力的限制。这为理解和评估 VLMs 的视觉感知能力提供了一种新的、更精细的视角。</p>
<p><strong>2. 关键创新或方法论：</strong></p>
<ul>
<li><strong>VI-Probe 框架：</strong> 这是论文的核心创新。该框架提供了一个可控的视觉错觉环境，能够进行“分级扰动”（graded perturbations），这意味着可以精细地调整错觉的强度。更重要的是，它引入了“匹配的视觉对照”（matched visual controls），这些对照在视觉上与错觉图像相似，但移除了诱发错觉的因素。这种设计使得研究者能够有效地将“视觉基础感知”（visually grounded perception）与“语言驱动的记忆回溯”（language-driven recall）分离开来。</li>
<li><strong>新的评估指标：</strong> 论文不局限于传统的平均准确率，而是提出了更具洞察力的指标来衡量 VLMs 的行为：<ul>
<li><strong>极性翻转一致性 (Polarity-Flip Consistency)：</strong> 衡量模型在错觉因素（如对比度、方向）翻转后，其输出是否保持一致。</li>
<li><strong>模板固定指数 (Template Fixation Index)：</strong> 可能用于衡量模型在面对不同错觉强度时，对特定视觉特征的“固着”程度。</li>
<li><strong>错觉乘数（Illusion Multiplier）归一化：</strong> 通过与对照组进行比较，量化错觉对模型输出的影响程度，从而更准确地评估感知能力。</li>
</ul>
</li>
</ul>
<p><strong>3. 对该领域的潜在影响：</strong></p>
<ul>
<li><strong>重新定义 VLM 评估标准：</strong> 这项研究挑战了当前 VLM 评估的局限性，即仅仅依赖于在标准数据集上的准确率。它强调了对模型“感知能力”和“对受控视觉变化的敏感性”进行更深入、更细致的探测的重要性。</li>
<li><strong>揭示 VLM 的内在机制：</strong> 通过将不同 VLM 在不同错觉场景下的行为归因于不同的根本原因（记忆覆盖、感知-记忆竞争、视觉处理限制），论文为理解 VLM 的内部工作机制提供了宝贵的线索。这有助于研究者更有针对性地改进模型架构和训练方法。</li>
<li><strong>推动 VLM 的鲁棒性研究：</strong> 视觉错觉本质上是模型对输入变化敏感性的一个极端测试。这项研究的方法论可以推广到其他形式的输入扰动和对抗性攻击，从而推动 VLM 在真实世界复杂场景下的鲁棒性研究。</li>
<li><strong>促进 VLM 的可解释性：</strong> 通过揭示模型行为背后的具体原因，这项工作为 VLM 的可解释性研究提供了新的方向。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用：</strong></p>
<ul>
<li><strong>人机交互 (Human-Computer Interaction)：</strong> 更好地理解 VLM 如何“感知”和“记忆”信息，有助于设计更自然、更可靠的 VLM 驱动的交互系统，尤其是在需要精确视觉理解的场景。</li>
<li><strong>自动驾驶和机器人视觉：</strong> 在这些领域，模型需要对环境进行准确、实时的感知，并能处理各种视觉干扰。这项研究的方法论可以帮助评估和提升这些系统的视觉感知鲁棒性。</li>
<li><strong>医学影像分析：</strong> 某些医学影像可能存在视觉上的“错觉”或微妙变化，理解 VLM 如何处理这些情况对于诊断和辅助决策至关重要。</li>
<li><strong>内容审核与安全：</strong> 识别和理解模型对特定视觉模式的反应，有助于开发更有效的机制来检测和过滤不当内容。</li>
<li><strong>教育和培训：</strong> 了解 VLM 的感知局限性，可以帮助设计更有效的教育工具，例如用于教授视觉感知原理的 VLM 应用。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性：</strong></p>
<ul>
<li><strong>研究范围的局限性：</strong> 论文主要关注“经典视觉错觉”。虽然这些错觉能揭示一些基本问题，但它们可能无法完全代表 VLM 在处理更复杂、更动态的真实世界视觉场景时的所有行为。</li>
<li><strong>模型数量和代表性：</strong> 摘要提到了 GPT-5, Claude-Opus-4.1, Qwen variants。虽然这些是当前领先的模型，但 VLM 的种类繁多，研究结果的普适性可能需要进一步在更广泛的模型家族中验证。</li>
<li><strong>“感知”与“记忆”的定义：</strong> 尽管论文试图区分感知和记忆，但这两个概念在复杂的神经网络模型中本身就难以完全解耦。研究中对这两个概念的界定和测量方式可能存在一定的解释空间。</li>
<li><strong>“固执”反应的根本原因的复杂性：</strong> 摘要指出“异质原因”，并列举了三种可能性。然而，这些原因之间可能存在相互作用，或者还有其他未被发现的根本原因，这需要更深入的分析。</li>
<li><strong>评估指标的有效性：</strong> 虽然提出了新的指标，但这些指标的有效性、鲁棒性以及与其他现有评估方法的关联性，需要在论文的完整实验部分得到充分论证。</li>
</ul>
<p><strong>总结来说，这篇论文在计算机视觉领域具有重要的理论和实践意义。它通过创新的方法论，深入剖析了 VLM 在视觉感知方面的深层问题，并为未来的 VLM 评估和发展指明了方向。其对 VLM“感知”与“记忆”的区分，以及对模型行为背后原因的探索，是该研究最吸引人的地方。</strong></p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Our findings challenge single-cause views and motivate probing-based evaluation that measures both knowledge and sensitivity to controlled visual change.</li>
<li>Data and code are available at https://sites.google.com/view/vi-probe/.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.22150v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.22150v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.22143v1'></a></p>
<h2 id="just-dub-it-video-dubbing-via-joint-audio-visual-diffusion"><a href="https://arxiv.org/abs/2601.22143v1">JUST-DUB-IT: Video Dubbing via Joint Audio-Visual Diffusion</a></h2>
<p><strong>Authors:</strong> Anthony Chen, Naomi Ken Korem, Tavi Halperin, Matan Ben Yosef, Urska Jelercic, Ofir Bibi, Or Patashnik, Daniel Cohen-Or</p>
<p><strong>Published:</strong> 2026-01-29</p>
<p><strong>Categories:</strong> cs.GR, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Audio-Visual Foundation Models, which are pretrained to jointly generate sound and visual content, have recently shown an unprecedented ability to model multi-modal generation and editing, opening new opportunities for downstream tasks. Among these tasks, video dubbing could greatly benefit from such priors, yet most existing solutions still rely on complex, task-specific pipelines that struggle in real-world settings. In this work, we introduce a single-model approach that adapts a foundational audio-video diffusion model for video-to-video dubbing via a lightweight LoRA. The LoRA enables the model to condition on an input audio-video while jointly generating translated audio and synchronized facial motion. To train this LoRA, we leverage the generative model itself to synthesize paired multilingual videos of the same speaker. Specifically, we generate multilingual videos with language switches within a single clip, and then inpaint the face and audio in each half to match the language of the other half. By leveraging the rich generative prior of the audio-visual model, our approach preserves speaker identity and lip synchronization while remaining robust to complex motion and real-world dynamics. We demonstrate that our approach produces high-quality dubbed videos with improved visual fidelity, lip synchronization, and robustness compared to existing dubbing pipelines.</p>
<p><strong>Analysis:</strong></p>
<p>None</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Audio-Visual Foundation Models, which are pretrained to jointly generate sound and visual content, have recently shown an unprecedented ability to model multi-modal generation and editing, opening new opportunities for downstream tasks.</li>
<li>In this work, we introduce a single-model approach that adapts a foundational audio-video diffusion model for video-to-video dubbing via a lightweight LoRA.</li>
<li>By leveraging the rich generative prior of the audio-visual model, our approach preserves speaker identity and lip synchronization while remaining robust to complex motion and real-world dynamics.</li>
<li>We demonstrate that our approach produces high-quality dubbed videos with improved visual fidelity, lip synchronization, and robustness compared to existing dubbing pipelines.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.22143v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.22143v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.22135v1'></a></p>
<h2 id="pi-light-physics-inspired-diffusion-for-full-image-relighting"><a href="https://arxiv.org/abs/2601.22135v1">PI-Light: Physics-Inspired Diffusion for Full-Image Relighting</a></h2>
<p><strong>Authors:</strong> Zhexin Liang, Zhaoxi Chen, Yongwei Chen, Tianyi Wei, Tengfei Wang, Xingang Pan</p>
<p><strong>Published:</strong> 2026-01-29</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Full-image relighting remains a challenging problem due to the difficulty of collecting large-scale structured paired data, the difficulty of maintaining physical plausibility, and the limited generalizability imposed by data-driven priors. Existing attempts to bridge the synthetic-to-real gap for full-scene relighting remain suboptimal. To tackle these challenges, we introduce Physics-Inspired diffusion for full-image reLight (<script type="math/tex">π</script>-Light, or PI-Light), a two-stage framework that leverages physics-inspired diffusion models. Our design incorporates (i) batch-aware attention, which improves the consistency of intrinsic predictions across a collection of images, (ii) a physics-guided neural rendering module that enforces physically plausible light transport, (iii) physics-inspired losses that regularize training dynamics toward a physically meaningful landscape, thereby enhancing generalizability to real-world image editing, and (iv) a carefully curated dataset of diverse objects and scenes captured under controlled lighting conditions. Together, these components enable efficient finetuning of pretrained diffusion models while also providing a solid benchmark for downstream evaluation. Experiments demonstrate that <script type="math/tex">π</script>-Light synthesizes specular highlights and diffuse reflections across a wide variety of materials, achieving superior generalization to real-world scenes compared with prior approaches.</p>
<p><strong>Analysis:</strong></p>
<p>好的，我将以AI领域高水平研究生的视角，深入分析这篇关于“PI-LIGHT: Physics-Inspired Diffusion for Full-Image Relighting”的论文，重点关注其方法创新点、设计逻辑、优势与不足，并提供实用的实现指南。</p>
<hr />
<h2 id="pi-light">论文方法分析与总结：PI-LIGHT</h2>
<h3 id="1_1">1. 摘要翻译</h3>
<p><strong>PI-LIGHT: 基于物理启发的扩散模型实现全图像重照明</strong></p>
<p>全图像重照明是一个具有挑战性的问题，其难点在于：难以收集大规模结构化的配对数据；难以保持物理上的合理性；以及数据驱动的先验知识带来的泛化能力受限。现有的尝试在弥合合成与真实世界全场景重照明的差距方面仍不尽如人意。为了应对这些挑战，我们提出了 <strong>π-Light (PI-Light)</strong>，一个两阶段的框架，它利用了物理启发的扩散模型。我们的设计包含：(i) <strong>批次感知注意力 (batch-aware attention)</strong>，它提高了图像集合中内在属性预测的一致性；(ii) 一个<strong>物理引导的神经渲染模块 (physics-guided neural rendering module)</strong>，它强制执行物理上合理的光传输；(iii) <strong>物理启发的损失函数 (physics-inspired losses)</strong>，它们将训练动态正则化到一个物理上有意义的景观，从而增强对真实世界图像编辑的泛化能力；以及 (iv) 一个精心策划的、在受控光照条件下捕获的、包含多样化物体和场景的数据集。总而言之，这些组件能够高效地微调预训练的扩散模型，同时为下游评估提供了一个坚实的基准。实验证明，π-Light能够合成具有各种材质的镜面高光和漫反射，在泛化到真实世界场景方面比先前的方法表现出更优越的性能。代码将提供。</p>
<h3 id="2_2">2. 方法动机分析</h3>
<ul>
<li>
<p><strong>驱动力</strong>：</p>
<ul>
<li><strong>真实世界应用需求</strong>：电影制作、增强现实、数字内容创作等领域对高质量、可控的图像重照明技术有迫切需求。</li>
<li><strong>现有方法局限性</strong>：当前方法在数据获取、物理合理性、泛化能力和对自发光物体处理等方面存在显著不足。</li>
</ul>
</li>
<li>
<p><strong>现有方法痛点</strong>：</p>
<ul>
<li><strong>数据稀缺性与结构化困难</strong>：难以获取大规模、多样化、结构化的真实世界配对数据（同一场景在不同光照下的图像）。</li>
<li><strong>物理合理性难以保证</strong>：纯数据驱动的方法容易生成违反基本光照传输规律的结果。</li>
<li><strong>泛化能力受限</strong>：模型难以适应未见过的材质、光照条件或复杂的场景。</li>
<li><strong>对自发光物体和内置光照的处理不佳</strong>：现有方法难以精确控制或区分场景本身的照明。</li>
<li><strong>合成到真实世界的鸿沟</strong>：合成数据训练的模型在真实世界场景上表现不佳。</li>
<li><strong>前景重照明的局限</strong>：即使是先进的前景重照明方法，也可能存在反照率不一致和光照控制不精确的问题。</li>
</ul>
</li>
<li>
<p><strong>研究假设</strong>：</p>
<ul>
<li><strong>扩散模型潜力</strong>：预训练的扩散模型具有强大的生成能力和丰富的先验知识，可以通过微调适应重照明任务。</li>
<li><strong>物理启发的约束</strong>：将物理光照传输的原理融入模型设计和损失函数中，可以显著提高结果的真实性和泛化能力。</li>
<li><strong>两阶段范式有效性</strong>：将图像分解（逆渲染）和图像重照明（前向渲染）解耦到两个阶段，可以分别优化和控制。</li>
<li><strong>批次感知注意力</strong>：通过跨批次的信息交互，可以提高内在属性预测的一致性和效率。</li>
</ul>
</li>
</ul>
<h3 id="3_2">3. 方法设计详解</h3>
<p><strong>流程总结</strong>：
π-Light 采用一个两阶段的框架：<strong>阶段 1：逆向神经渲染 (Inverse Neural Rendering)</strong>，用于预测图像的物理内在属性；<strong>阶段 2：前向神经渲染 (Forward Neural Rendering)</strong>，利用预测的内在属性和目标光照条件进行重照明。</p>
<ul>
<li>
<p><strong>阶段 1：逆向神经渲染 (Inverse Neural Rendering)</strong></p>
<ul>
<li><strong>输入</strong>：一张 RGB 输入图像 <code>I_in</code>。</li>
<li><strong>核心技术</strong>：利用预训练的扩散模型（如 Stable Diffusion）进行微调。</li>
<li><strong>模型结构</strong>：<ul>
<li><strong>批次感知注意力 (Batch-aware Attention)</strong>：借鉴 Wonder3D 的思想，将标准的自注意力层扩展为全局感知，允许跨批次的信息交互。这通过将输入图像的四个通道（例如，原始图像的 RGB 和噪声）进行批次连接，然后应用跨批次的注意力机制来实现。</li>
<li><strong>条件输入</strong>：原始图像的 CLIP 嵌入被用作跨注意力层的条件。</li>
</ul>
</li>
<li><strong>输出</strong>：预测四个物理内在属性：<strong>反照率 (Albedo)</strong> <code>A</code>，<strong>法线 (Normal)</strong> <code>N</code>，<strong>粗糙度 (Roughness)</strong> <code>R</code>，<strong>金属度 (Metallic)</strong> <code>M</code>。</li>
<li><strong>训练目标</strong>：<ul>
<li><strong>重构损失 (Reconstruction Loss)</strong>：使用 V-prediction（<code>L_v-pred</code>）来衡量预测的内在属性与真实值之间的差异。</li>
<li><strong>掩码损失 (Masked Loss)</strong>：针对数据集中可能存在的不准确或难以估计的区域（如透明物体、天空法线等），引入掩码来计算损失，确保模型在可靠区域上学习。损失函数为 <code>L_stage1 = MSE(vpred·Mz, vtarget · mz)</code>，其中 <code>mz</code> 是经过下采样的掩码。</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>阶段 2：前向神经渲染 (Forward Neural Rendering)</strong></p>
<ul>
<li><strong>输入</strong>：<ul>
<li>原始 RGB 图像 <code>I_in</code>。</li>
<li>阶段 1 预测的内在属性：反照率 <code>A</code>，法线 <code>N</code>，粗糙度 <code>R</code>，金属度 <code>M</code>。</li>
<li>目标光照条件 <code>L</code>（表示为环境贴图，具体实现为渲染的灰球）。</li>
</ul>
</li>
<li><strong>核心技术</strong>：微调预训练的扩散模型，以目标光照条件和预测的内在属性为条件，生成重照明后的图像。</li>
<li><strong>模型结构</strong>：<ul>
<li><strong>物理引导的神经渲染模块</strong>：将扩散模型作为生成器，但通过物理启发的损失函数进行约束。</li>
<li><strong>条件输入</strong>：将输入图像、预测的内在属性（反照率、法线、粗糙度、金属度）和目标光照条件进行组合，作为扩散模型的条件输入。具体来说，输入被设计为三个批次：<ul>
<li><code>I_in1 = (I_in, A)</code></li>
<li><code>I_in2 = (N, L, m)</code> (m为掩码)</li>
<li><code>I_in3 = (N, L, M, R, m)</code></li>
</ul>
</li>
<li><strong>输出</strong>：生成重照明后的图像 <code>I_relit</code>，以及其漫反射分量 <code>D_pred</code> 和镜面反射分量 <code>S_pred</code>。</li>
</ul>
</li>
<li><strong>训练目标</strong>：<ul>
<li><strong>物理启发的损失函数 (Physics-Inspired Losses)</strong>：这是该方法的核心创新之一。<ul>
<li><strong>漫反射着色损失 (Diffuse Shading Loss, <code>L_DS</code>)</strong>：基于 Lambertian 模型，利用法线图和环境光照计算理论上的漫反射图 <code>D_calculated</code>，并与模型预测的漫反射图 <code>D_pred</code> 进行 MSE 比较。此损失不需要真实漫反射的标注。</li>
<li><strong>物理着色损失 (Physical-based Shading Loss, <code>L_PS</code>)</strong>：基于物理渲染方程 <code>I_rendered = A * D + S</code>，将模型生成的 <code>A * D_pred + S_pred</code> 与重照明后的图像 <code>I_relit</code> 进行 MSE 比较。这强制模型生成的图像符合物理渲染模型。</li>
<li><strong>重构损失 (Reconstruction Loss, <code>L_rec</code>)</strong>：使用 DINO 特征提取器，计算重照明图像 <code>I_relit</code> 和输入图像 <code>I_in</code> 的特征差异，以确保图像内容在重照明前后保持一致，减少伪影。</li>
</ul>
</li>
<li><strong>总损失</strong>：<code>L_stage2 = L_v-pred + λ1*L_DS + λ2*L_PS + λ3*L_rec</code>。<code>L_v-pred</code> 是扩散模型本身的 V-prediction 损失。</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>光照表示</strong>：</p>
<ul>
<li>采用<strong>灰球渲染</strong>来表示光照条件。灰球位于相机位置，并根据 HDRI 环境光照进行渲染。这种表示只使用环境光照的<strong>前半球</strong>，避免了自发光物体和内置场景光照的干扰，使得用户能够更精确地控制光照方向和强度。</li>
</ul>
</li>
</ul>
<h3 id="4_2">4. 方法对比分析</h3>
<ul>
<li>
<p><strong>本质区别</strong>：</p>
<ul>
<li><strong>物理约束的深度</strong>：与纯数据驱动的方法不同，π-Light 显式地将物理光照传输原理（Lambertian、Cook-Torrance BRDF）融入到损失函数中，强制模型学习物理规律，而不是仅仅模仿数据。</li>
<li><strong>两阶段的内在属性预测</strong>：大多数方法要么直接进行重照明，要么只预测部分内在属性。π-Light 完整地预测了反照率、法线、粗糙度和金属度，为后续的物理渲染提供了更全面的基础。</li>
<li><strong>批次感知注意力</strong>：通过跨批次的信息共享，提高了内在属性预测的一致性，这是许多单图像方法难以达到的。</li>
<li><strong>光照表示的独特性</strong>：使用灰球渲染的前半球来表示光照，是一种新颖且有效的控制光照方向和避免场景内置光照干扰的方法。</li>
</ul>
</li>
<li>
<p><strong>创新贡献</strong>：</p>
<ul>
<li><strong>物理启发的扩散模型框架</strong>：将物理光照模型与扩散模型相结合，实现了高质量、物理合理的全图像重照明。</li>
<li><strong>两阶段的内在属性预测与重照明</strong>：提供了一个完整的、可控的重照明流程。</li>
<li><strong>批次感知注意力机制</strong>：提升了内在属性预测的效率和一致性。</li>
<li><strong>精心策划的物理启发的损失函数</strong>：包括漫反射着色损失和物理着色损失，有效引导模型学习物理规律。</li>
<li><strong>新颖的光照表示方法</strong>：灰球渲染的前半球，提供了更精确的光照控制。</li>
<li><strong>高质量数据集的构建</strong>：为研究提供了重要的资源。</li>
</ul>
</li>
<li>
<p><strong>适用场景</strong>：</p>
<ul>
<li><strong>静态场景重照明</strong>：尤其适用于需要精确控制光照方向和效果的场景，如产品渲染、室内设计可视化。</li>
<li><strong>需要物理真实感的应用</strong>：如电影特效、游戏开发中的资产制作。</li>
<li><strong>对自发光物体不敏感的场景</strong>：由于光照表示的限制，在处理具有复杂内置光源的场景时可能需要额外考虑。</li>
</ul>
</li>
</ul>
<h3 id="5_2">5. 实验分析</h3>
<ul>
<li>
<p><strong>验证方法</strong>：</p>
<ul>
<li><strong>数据集</strong>：构建了一个包含物体和场景的新数据集（Object50, Scene200），并在现有数据集上进行评估。</li>
<li><strong>评估指标</strong>：使用 PSNR, SSIM, LPIPS 等标准指标进行定量评估。</li>
<li><strong>定性比较</strong>：与多种 SOTA 方法（如 RGB↔X, Neural Gaffer, IC-Light 等）在视觉效果上进行对比。</li>
<li><strong>消融实验</strong>：分析了批次感知注意力、物理着色损失（<code>L_DS</code>, <code>L_PS</code>）、重构损失 (<code>L_rec</code>) 以及不同 CFG 权重对模型性能的影响。</li>
</ul>
</li>
<li>
<p><strong>关键结果</strong>：</p>
<ul>
<li><strong>定量结果</strong>：在 Object50 和 Scene200 数据集上，π-Light 在大多数指标上均优于现有方法，尤其是在内在属性预测（如法线、反照率）和重照明效果上。</li>
<li><strong>定性结果</strong>：<ul>
<li>能够生成更精细、物理更合理的内在属性（如反照率、法线、金属度），尤其是在镜面反射区域。</li>
<li>重照明结果能够准确地反映目标光照条件，并保持物体原有的材质属性（如反照率、粗糙度）。</li>
<li>在处理复杂材质（如金属）和生成阴影方面表现出色。</li>
<li>消融实验证明了物理启发的损失函数和批次感知注意力对提升性能至关重要。</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>优势场景</strong>：</p>
<ul>
<li><strong>具有复杂材质的物体</strong>：如金属、玻璃等，能够准确预测其高光和反射。</li>
<li><strong>需要精确光照控制的场景</strong>：如改变室内照明方向和强度。</li>
<li><strong>需要保持物体原有材质属性的重照明</strong>：避免了反照率漂移等问题。</li>
</ul>
</li>
<li>
<p><strong>局限性</strong>：</p>
<ul>
<li><strong>内在属性预测的模糊性</strong>：第一阶段的内在属性分解本身是病态问题，即使模型表现良好，也可能存在一定误差，影响最终重照明效果。</li>
<li><strong>对非 Principled BRDF 模型材质的限制</strong>：在某些情况下，模型可能无法完全准确地预测材质属性，例如将高 IOR 的非金属表面误判为高金属度。</li>
<li><strong>光照表示的局限</strong>：仅使用前半球光照，无法模拟后半球的光照，对于有后方光源的场景效果受限。</li>
<li><strong>场景遮挡问题</strong>：当墙壁等物体遮挡了光照时，结果可能与预期不符。</li>
<li><strong>潜在的伪影</strong>：虽然通过重构损失缓解，但在某些极端情况下仍可能出现轻微伪影。</li>
<li><strong>潜在的语义对齐问题</strong>：在 VAE 潜在空间进行操作时，可能存在像素级对齐不精确的问题，影响掩码损失的准确性。</li>
</ul>
</li>
</ul>
<h3 id="6_1">6. 实用指南</h3>
<ul>
<li><strong>开源情况</strong>：论文提到“代码将提供”，表明有开源计划，可以关注作者的 GitHub 仓库。</li>
<li><strong>实现细节</strong>：<ul>
<li><strong>两阶段训练</strong>：需要分别训练阶段 1（内在属性预测）和阶段 2（重照明）。</li>
<li><strong>预训练模型</strong>：阶段 1 和阶段 2 都基于预训练的扩散模型（如 Stable Diffusion）进行微调。</li>
<li><strong>数据集</strong>：需要准备或使用论文中构建的（或类似结构的）包含 RGB 图像及其对应内在属性和光照条件的数据集。</li>
<li><strong>损失函数权重</strong>：<code>λ1, λ2, λ3</code> 需要根据具体任务和数据集进行调整。</li>
<li><strong>CFG 权重</strong>：在阶段 1 中禁用 CFG（CFG=1.0），在阶段 2 中使用较小的 CFG 值（如 1.0-1.5），以平衡细节和物理真实性。</li>
<li><strong>光照表示</strong>：需要实现灰球渲染的前半球光照表示方法。</li>
</ul>
</li>
<li><strong>迁移可能</strong>：<ul>
<li><strong>其他图像生成任务</strong>：批次感知注意力机制和物理启发的损失函数可以借鉴到其他需要多模态条件输入和物理约束的图像生成任务中。</li>
<li><strong>3D 重建与渲染</strong>：该方法的核心思想（内在属性分解 + 物理渲染）可以应用于 3D 重建和渲染领域，特别是需要从单张图像恢复 3D 信息并进行光照编辑的场景。</li>
<li><strong>材质估计</strong>：阶段 1 的内在属性预测模块本身可以作为独立的材质估计器。</li>
</ul>
</li>
</ul>
<h3 id="7_1">7. 总结</h3>
<ul>
<li>
<p><strong>核心思想</strong>：<strong>扩散模型+物理约束，实现可控高保真图像重照明</strong>。</p>
</li>
<li>
<p><strong>速记版 pipeline</strong>：</p>
<ol>
<li><strong>分解</strong>：用扩散模型预测图像的材质属性（反照率、法线等）。</li>
<li><strong>光照</strong>：定义目标光照（用灰球表示）。</li>
<li><strong>渲染</strong>：用扩散模型结合材质和光照，生成重照明后的图像。</li>
<li><strong>约束</strong>：用物理规律（如漫反射、BRDF）指导生成过程。</li>
</ol>
</li>
</ul>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>To tackle these challenges, we introduce Physics-Inspired diffusion for full-image reLight (<script type="math/tex">π</script>-Light, or PI-Light), a two-stage framework that leverages physics-inspired diffusion models.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.22135v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.22135v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.22127v1'></a></p>
<h2 id="edityourself-audio-driven-generation-and-manipulation-of-talking-head-videos-with-diffusion-transformers"><a href="https://arxiv.org/abs/2601.22127v1">EditYourself: Audio-Driven Generation and Manipulation of Talking Head Videos with Diffusion Transformers</a></h2>
<p><strong>Authors:</strong> John Flynn, Wolfgang Paier, Dimitar Dinev, Sam Nhut Nguyen, Hayk Poghosyan, Manuel Toribio, Sandipan Banerjee, Guy Gafni</p>
<p><strong>Published:</strong> 2026-01-29</p>
<p><strong>Categories:</strong> cs.CV, cs.GR, cs.LG, cs.MM</p>
<p><strong>Abstract:</strong></p>
<p>Current generative video models excel at producing novel content from text and image prompts, but leave a critical gap in editing existing pre-recorded videos, where minor alterations to the spoken script require preserving motion, temporal coherence, speaker identity, and accurate lip synchronization. We introduce EditYourself, a DiT-based framework for audio-driven video-to-video (V2V) editing that enables transcript-based modification of talking head videos, including the seamless addition, removal, and retiming of visually spoken content. Building on a general-purpose video diffusion model, EditYourself augments its V2V capabilities with audio conditioning and region-aware, edit-focused training extensions. This enables precise lip synchronization and temporally coherent restructuring of existing performances via spatiotemporal inpainting, including the synthesis of realistic human motion in newly added segments, while maintaining visual fidelity and identity consistency over long durations. This work represents a foundational step toward generative video models as practical tools for professional video post-production.</p>
<p><strong>Analysis:</strong></p>
<p>好的，我将以一名AI领域高水平研究生的视角，深入分析这篇论文的方法部分，并遵循您提供的分析框架。</p>
<hr />
<h2 id="edityourself-audio-driven-generation-and-manipulation-of-talking-head-videos-with-diffusion-transformers_1">论文方法分析与总结：《EditYourself: Audio-Driven Generation and Manipulation of Talking Head Videos with Diffusion Transformers》</h2>
<h3 id="1_2">1. 摘要翻译</h3>
<p><strong>《EditYourself：基于音频驱动的带表情视频生成与编辑》</strong></p>
<p>当前生成式视频模型在从文本和图像提示生成新颖内容方面表现出色，但在编辑现有预录视频方面存在关键差距，其中对口语脚本的微小改动需要保留运动、时间连贯性、说话人身份和准确的唇部同步。我们提出了 EditYourself，一个基于扩散 Transformer (DiT) 的框架，用于音频驱动的视频到视频 (V2V) 编辑，支持基于文本的对话编辑。该框架基于一个通用的视频扩散模型，并增强了其 V2V 能力，通过音频条件化和区域感知、聚焦编辑的训练扩展。这使得能够通过时空修复实现精确的唇部同步和时间连贯的重构，包括在新增片段中合成逼真的人类运动，同时保持长时段的视觉保真度和身份一致性。这项工作代表了将生成式视频模型作为专业视频后期制作实用工具的奠基性一步。</p>
<h3 id="2_3">2. 方法动机分析</h3>
<ul>
<li>
<p><strong>驱动力</strong>：</p>
<ul>
<li><strong>现有视频编辑的痛点</strong>：传统的视频编辑工具在修改口语内容（如修正错误、添加/删除词语、调整语速）时，往往难以保持视频的自然流畅性、说话人身份和精确的唇部同步。这导致需要大量手动工作，且效果不佳。</li>
<li><strong>生成式模型在编辑领域的潜力</strong>：近年来，扩散模型在生成高质量、时间连贯的视频方面取得了巨大成功。作者认为，这些模型不仅能用于内容创作，还能作为强大的编辑引擎，以内容感知的方式修复、扩展或重塑现有视频。</li>
<li><strong>对更精细化编辑的需求</strong>：用户（尤其是视频创作者）需要一种直观、高效的方式来修改视频中的口语内容，例如通过文本脚本进行精确的词语级编辑，这比直接操作视频帧更方便。</li>
</ul>
</li>
<li>
<p><strong>现有方法痛点</strong>：</p>
<ul>
<li><strong>Image-to-Video (I2V) 方法的局限性</strong>：虽然 I2V 方法能生成逼真的视频，但容易出现身份漂移，且难以捕捉真实表演中的细微表情和说话风格。生成的视频可能包含不准确的细节（如牙齿、皱纹），尤其是在生成用户自己的视频时。</li>
<li><strong>Video-to-Video (V2V) 唇部同步方法的局限性</strong>：现有的 V2V 唇部同步模型虽然能很好地保持视觉保真度和身份，但编辑灵活性差。它们通常依赖固定的时间结构，难以在不破坏时间连贯性的情况下插入或删除语音片段。</li>
<li><strong>缺乏脚本驱动的精细化编辑</strong>：现有方法难以实现基于文本脚本的、对视频内容进行精确修改（如插入、删除、重定时长）的功能，而这正是专业视频后期制作的核心需求。</li>
</ul>
</li>
<li>
<p><strong>研究假设</strong>：</p>
<ul>
<li>通过将一个通用的视频扩散模型（如 LTX-Video）进行修改和训练，可以使其具备强大的音频驱动的 V2V 编辑能力，从而实现对现有视频的精细化修改。</li>
<li>结合音频信息、文本脚本和区域感知的编辑策略，可以在保持视觉保真度和身份一致性的同时，实现精确的唇部同步和时间连贯的视频内容重构。</li>
<li>在潜在空间（latent space）进行编辑操作，可以更有效地处理视频的时空结构，实现平滑的添加、删除和重定时长等操作。</li>
</ul>
</li>
</ul>
<h3 id="3_3">3. 方法设计详解</h3>
<p><strong>流程总结</strong>：</p>
<p>EditYourself 是一个基于扩散 Transformer (DiT) 的框架，它在预训练的通用视频扩散模型（LTX-Video [35]）的基础上，通过引入一系列扩展来实现音频驱动的 V2V 编辑。其核心流程可以概括为：</p>
<ol>
<li>
<p><strong>基线模型 (LTX-Video)</strong>：</p>
<ul>
<li>使用一个 14B 参数的 3D 扩散 Transformer (DiT) 模型，配合一个预训练的 3D 视频 VAE（Video-VAE）进行视频的编码和解码。</li>
<li>DiT 模型在高度压缩的潜在空间中操作，以提高效率。</li>
<li>采用两阶段的生成过程：先在粗糙的低分辨率潜在空间中进行去噪，然后进行学习到的上采样和更高分辨率的去噪。</li>
<li>预训练时支持多种条件（文本 T2V, 图像 I2V, 视频 V2V, 关键帧生成, 时空修复），通过掩码和不同的时间步条件注入。</li>
<li>训练目标是 Flow Matching [71]，通过预测速度场将噪声映射回数据。</li>
</ul>
</li>
<li>
<p><strong>核心扩展与训练策略</strong>：</p>
<ul>
<li>
<p><strong>(i) 音频条件化与 V2V 唇部同步训练 (Section 3.2)</strong>：</p>
<ul>
<li><strong>音频特征提取</strong>：使用 Whisper [87] 等模型提取音频特征。</li>
<li><strong>音频投影模块 (Audio Projection)</strong>：将提取的音频特征（例如，Whisper 嵌入）通过一个学习的投影和池化模块进行处理，生成与视频帧率对齐的唇部同步嵌入（lip-sync embeddings）。</li>
<li><strong>窗口化音频条件化</strong>：为了处理音频采样率与视频帧率不匹配的问题，作者提出了一种<strong>相位偏移的窗口采样策略</strong>。对于每个视频帧 <code>i</code>，从音频特征序列 <code>čaudio</code> 中提取一个包含 <code>W</code> 个音频特征的窗口 <code>čaudio[n]</code>。这个窗口的中心对齐到视频帧 <code>i</code>，并且使用线性插值 <code>un = i * fa + (n - W/2)</code> 来计算音频特征的索引 <code>un</code>，其中 <code>fa</code> 是音频采样率。这确保了音频窗口的语义在不同帧率的视频中保持一致的时间跨度。</li>
<li><strong>位置编码</strong>：引入一个学习的、固定大小的位置嵌入张量 <code>P</code>，<code>P[n]</code> 对应于音频窗口中的索引 <code>n</code>，用于编码音频特征在窗口内的相对位置。<code>čaudio+pos[n] = čaudio[n] + P[n]</code>。</li>
<li><strong>音频交叉注意力层</strong>：在 DiT 的每个 Transformer 块中，插入音频交叉注意力层。音频特征 <code>čaudio+pos</code> 作为键（keys）和值（values），与视频潜在表示进行交互。</li>
<li><strong>V2V 唇部同步训练</strong>：<ul>
<li><strong>区域感知掩码</strong>：检测视频中说话人的下半脸区域（使用 MediaPipe [76]），并生成一个时空掩码 <code>M</code>。</li>
<li><strong>时空修复 (Inpainting)</strong>：在训练时，只对掩码区域 <code>M</code> 内的潜在表示 <code>xt</code> 添加噪声（<code>xt = M ⊙ [(1 – t)xo + te] + (1 − M) ⊙ xo</code>），模型被训练来修复（inpainting）这些区域，从而实现唇部同步。掩码区域外的部分保持不变（<code>x0</code>）。</li>
<li><strong>面部区域限制</strong>：在交叉注意力输出 <code>Zout</code> 中，通过乘以面部掩码 <code>M</code> 来限制音频信息只影响面部区域的潜在表示 (<code>Zout = Zin + M ⊙ AudioAttn(zin, ca)</code>)。</li>
<li><strong>条件随机丢弃 (Conditional Dropout)</strong>：为了提高模型的鲁棒性，随机丢弃音频、第一帧或 V2V 条件，以支持不同的输入组合。</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>(ii) 潜在空间视觉对话编辑 (Section 3.3)</strong>：</p>
<ul>
<li><strong>核心思想</strong>：将视频编辑操作（添加、删除、重定时长）映射到 DiT 的潜在空间中进行。</li>
<li><strong>潜在空间操作</strong>：<ul>
<li><strong>添加 (Addition)</strong>：在目标位置插入完全加噪的潜在帧。</li>
<li><strong>删除 (Removal)</strong>：删除现有的潜在帧。为了平滑过渡，会对被删除帧的相邻潜在帧添加额外噪声，让扩散过程进行修复。</li>
<li><strong>重定时长 (Retime)</strong>：通过在整个片段中均匀地添加或删除潜在帧来实现。</li>
</ul>
</li>
<li><strong>时间映射</strong>：利用 VAE 的因果性，定义了潜在帧索引 <code>n</code> 与输入视频帧范围 <code>8(n-1)+1</code> 到 <code>8n+1</code> 的映射关系，为潜在空间编辑提供了 8 帧分辨率的代理。</li>
<li><strong>面部区域掩码</strong>：在进行添加/删除/重定时长操作后，对于需要进行唇部同步的区域（如面部），会使用面部掩码进行限制，而对于完全合成的区域，则使用 <code>M=1</code>（无掩码）。</li>
</ul>
</li>
<li>
<p><strong>(iii) 缓存感知长推理策略 (Section 3.4)</strong>：</p>
<ul>
<li><strong>挑战</strong>：生成长视频时，内存限制和时间连贯性是主要问题。</li>
<li><strong>TAPSF (Time-aware position shift fusion)</strong>：借鉴 Sonic [10, 47] 的策略，将长视频分割成不重叠的推理块（例如，17 个潜在帧，对应 136 个视频帧）。</li>
<li><strong>迭代去噪与块移位</strong>：在每个时间步，对每个块进行一次去噪。然后，将块的划分向前（或向后）移动一个固定步长（例如 5 个潜在帧），以便在下一个去噪步骤中，块能够包含相邻块的上下文信息，从而实现跨块的上下文融合和稳定性。</li>
<li><strong>TeaCache 优化</strong>：在中间的去噪步骤（约 75%），作者<strong>禁用块移位</strong>，并利用 TeaCache 技术进行缓存，以加速推理，同时保持 TAPSF 的长程连贯性优势。这种策略在保持长程连贯性的同时，实现了约 1.6 倍的速度提升。</li>
</ul>
</li>
<li>
<p><strong>(iv) 参考式身份保持（Forward-Backward RoPE Conditioning）(Section 3.4)</strong>：</p>
<ul>
<li><strong>动机</strong>：在长视频生成或编辑过程中，尤其是在完全合成的片段中，容易出现身份漂移。</li>
<li><strong>训练阶段</strong>：<ul>
<li><strong>面部参考令牌 (Face Reference Tokens)</strong>：在训练时，从目标片段周围的 ±5 秒时间窗口中随机采样 6 个潜在帧，提取其下半脸区域的令牌，并将其作为<strong>未加噪的参考令牌</strong> <code>zface</code> 连接到输入序列中。这有助于模型学习保持下半脸的身份。</li>
<li><strong>全帧参考令牌 (Full-frame Reference Tokens)</strong>：为了防止全局外观漂移，作者提出了<strong>前向-后向 RoPE 条件化</strong>。在推理时，对于完全合成的块，将来自最近的过去和未来的<strong>全帧参考令牌</strong> <code>zref</code>（例如，块边界附近的帧）添加到输入序列中。</li>
</ul>
</li>
<li><strong>RoPE 调整</strong>：为这些参考令牌分配“伪”时间索引 <code>tref</code>，使其在 RoPE 中与当前块的时间位置对齐，但又不强制完全复制。<code>tref</code> 的计算方式考虑了参考帧与当前块的相对时间距离 <code>Δt</code>，以确保其在时间上是合理的。</li>
<li><strong>目的</strong>：通过提供参考帧（面部或全帧），模型可以更好地保持身份和外观一致性，尤其是在长视频或编辑产生的合成区域。</li>
</ul>
</li>
</ul>
</li>
</ol>
<p><strong>模型结构</strong>：</p>
<ul>
<li><strong>核心</strong>：基于 LTX-Video 的 DiT 模型。</li>
<li><strong>新增模块</strong>：<ul>
<li><strong>音频投影模块 (Audio Projection)</strong>：将音频特征映射到适合 DiT 的嵌入空间。</li>
<li><strong>音频交叉注意力层</strong>：集成到 DiT 的每个 Transformer 块中，用于注入音频信息。</li>
<li><strong>面部/全帧参考令牌注入机制</strong>：在训练和推理时，将参考帧的潜在表示作为额外输入添加到序列中。</li>
</ul>
</li>
<li><strong>训练策略</strong>：<ul>
<li><strong>两阶段训练</strong>：<ul>
<li>第一阶段：仅优化音频投影模块和音频交叉注意力层（冻结 DiT 主体），约 20k 步。</li>
<li>第二阶段：对整个 DiT 模型进行 LoRA（Low-Rank Adaptation）微调，约 10k 步，以整合身份条件化并进一步提升性能。</li>
</ul>
</li>
<li><strong>掩码训练</strong>：用于 V2V 唇部同步，只在特定区域（如嘴部）添加噪声。</li>
<li><strong>时间窗口采样</strong>：用于音频特征与视频帧的对齐。</li>
</ul>
</li>
</ul>
<p><strong>算法解释</strong>：</p>
<ul>
<li>
<p><strong>窗口化音频条件化 (Eq. 3 &amp; 4)</strong>：</p>
<ul>
<li><code>un = i * fa + (n - W/2)</code>：计算音频特征的采样索引。<code>i</code> 是视频帧索引，<code>fa</code> 是音频采样率，<code>n</code> 是窗口内索引，<code>W</code> 是窗口大小。这个公式确保了音频窗口的中心与视频帧 <code>i</code> 对齐，并且通过线性插值 <code>un</code> 来处理不同采样率。</li>
<li><code>čaudio[n] = (1 - an) Caudio[kn] + an Caudio[kn + 1]</code>：线性插值，其中 <code>kn = [un]</code> 是向下取整的索引，<code>an = un - kn</code> 是小数部分。这用于获取精确的音频特征值。</li>
<li><code>čaudio+pos[n] = čaudio[n] + P[n]</code>：将窗口内的音频特征与学习到的位置嵌入相加，提供相对位置信息。</li>
</ul>
</li>
<li>
<p><strong>V2V 唇部同步掩码 (Eq. 5 &amp; 6)</strong>：</p>
<ul>
<li><code>xt = M ⊙ [(1 – t)xo + te] + (1 − M) ⊙ xo</code>：在潜在空间中，只对掩码区域 <code>M</code> 内的 <code>xt</code> 添加噪声（<code>te</code> 是噪声），而 <code>M</code> 外的区域保持原始数据 <code>xo</code>。<code>⊙</code> 表示逐元素乘法。这实现了对特定区域的修复。</li>
<li><code>Zout = Zin + M ⊙ AudioAttn(zin, ca)</code>：将音频交叉注意力的输出 <code>AudioAttn</code> 乘以面部掩码 <code>M</code>，确保音频信息只影响面部区域的潜在表示 <code>Zin</code>。</li>
</ul>
</li>
<li>
<p><strong>前向-后向 RoPE 条件化 (Eq. 7)</strong>：</p>
<ul>
<li><code>tref = tblock (end) + 3</code> (if <code>Δt &lt; 3</code>)</li>
<li><code>tref = tblock (start) - 3</code> (if <code>Δt &gt; 3</code> (forward))</li>
<li><code>tref = tblock (start) - 3</code> (if <code>Δt &gt; 3</code> (backward))</li>
<li>这里的 <code>tref</code> 是为参考帧分配的“伪”时间索引。<code>tblock (start)</code> 和 <code>tblock (end)</code> 是当前处理的潜在块的起始和结束时间索引。<code>Δt</code> 是参考帧与块边界的时间距离。这个公式旨在为参考帧分配一个在时间上“合理”的索引，使其在 RoPE 中能够被模型正确地利用，同时又不会强制完全复制，从而允许一定程度的灵活性。</li>
</ul>
</li>
</ul>
<h3 id="4_3">4. 方法对比分析</h3>
<ul>
<li>
<p><strong>本质区别</strong>：</p>
<ul>
<li><strong>与 I2V 方法</strong>：EditYourself 是 V2V 编辑，它以现有视频为基础，保留了大部分原始信息（如身份、背景、风格），而 I2V 方法是从单张图片生成全新视频，容易丢失身份信息。</li>
<li><strong>与传统 V2V 唇部同步方法</strong>：传统方法通常只关注唇部同步，且编辑灵活性差。EditYourself 实现了基于文本脚本的精细化编辑（添加、删除、重定时长），并能处理更复杂的时空重构。</li>
<li><strong>与现有视频编辑方法</strong>：许多视频编辑方法（如基于光流的修复、基于 GAN 的编辑）可能难以处理长视频的全局一致性或身份保持。EditYourself 利用了扩散模型的强大生成能力和其提出的长推理、身份保持机制。</li>
<li><strong>与纯生成模型</strong>：EditYourself 并非从头生成视频，而是对现有视频进行“编辑”，这在保持真实感和身份方面具有天然优势。</li>
</ul>
</li>
<li>
<p><strong>创新贡献</strong>：</p>
<ul>
<li><strong>音频驱动的 V2V 唇部同步与编辑框架</strong>：首次将音频驱动的 V2V 编辑能力集成到一个通用的扩散模型中，实现了脚本驱动的视频内容修改。</li>
<li><strong>窗口化音频条件化策略</strong>：解决了音频采样率与视频帧率不匹配的问题，实现了精确的音频-视频对齐。</li>
<li><strong>潜在空间视觉对话编辑</strong>：将复杂的视频编辑操作（添加、删除、重定时长）转化为潜在空间的简单操作，并实现了平滑的过渡。</li>
<li><strong>前向-后向 RoPE 条件化</strong>：一种新颖的身份保持机制，通过引入参考帧的伪时间索引，有效防止了长视频生成中的身份漂移。</li>
<li><strong>缓存感知长推理策略 (TAPSF + TeaCache)</strong>：在保持长程连贯性的同时，显著提高了长视频生成的推理速度。</li>
</ul>
</li>
<li>
<p><strong>适用场景</strong>：</p>
<ul>
<li><strong>视频后期制作</strong>：修正口语错误、调整语速、为视频添加新的对话、删除不必要的片段。</li>
<li><strong>内容本地化/配音</strong>：将视频翻译成不同语言，并自动调整口型和语速以匹配新的音频。</li>
<li><strong>虚拟人/数字替身</strong>：为虚拟人生成逼真的口型同步视频，并能根据脚本进行动态编辑。</li>
<li><strong>个性化视频生成</strong>：为用户生成具有特定口语内容的视频变体。</li>
</ul>
</li>
</ul>
<h3 id="5_3">5. 实验分析</h3>
<ul>
<li>
<p><strong>验证方法</strong>：</p>
<ul>
<li><strong>数据集</strong>：主要使用了 TalkVid [9] 数据集，这是一个包含大量说话人视频的数据集。还使用了 YouTube 上收集的“野外”视频数据。</li>
<li><strong>评估指标</strong>：<ul>
<li><strong>V2V (Self-Reenactment &amp; Novel Audio)</strong>：FID, FVD (视觉保真度), CSIM (身份保持), Sync-C/D (唇部同步准确性), Pose Preservation (头部姿态保持)。</li>
<li><strong>I2V</strong>：FID, FVD, CSIM, Sync-C/D, 以及 VBench [45] 的 Subject/Background Consistency, Aesthetic Quality, Motion Smoothness。</li>
</ul>
</li>
<li><strong>对比方法</strong>：与多种 SOTA 的 V2V 唇部同步方法（如 LatentSync, InfiniteTalk, MuseTalk）和 I2V 方法（如 Hallo3, Sonic, StableAvatar）进行了比较。</li>
<li><strong>消融实验 (Ablation Study)</strong>：<ul>
<li><strong>身份保持的必要性</strong>：通过对比 V2V, V2V+FR, I2V, I2V+FR, I2V+FR+FF 等配置，展示了面部参考令牌 (FR) 和全帧参考令牌 (FF) 在保持身份和外观一致性方面的作用。</li>
<li><strong>训练时参考条件的作用</strong>：对比了训练时是否使用参考令牌对渲染质量的影响。</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>关键结果</strong>：</p>
<ul>
<li><strong>V2V 性能</strong>：在 V2V 设置下，EditYourself 在 Novel Audio 场景下取得了最优的 FID, FVD, CSIM 和 Sync-C 指标，并且在 Pose Preservation 方面也表现出色。在 Self-Reenactment 场景下，也取得了非常有竞争力的结果，尤其是在 CSIM 和 Sync-C 方面。</li>
<li><strong>I2V 性能</strong>：在 I2V 设置下，EditYourself 在 TalkVid 数据集上取得了最优的 Sync-C 指标，并且在 VBench 指标上也表现出色，尤其是在 Subject/Background Consistency, Aesthetic Quality, Motion Smoothness 方面。</li>
<li><strong>身份保持</strong>：消融实验清晰地表明，使用面部参考令牌 (FR) 和全帧参考令牌 (FF) 对于防止身份漂移至关重要，尤其是在长视频和 I2V 生成场景下。</li>
<li><strong>推理速度</strong>：通过 VAE Tiling 和 Latent Frame Blocking 等优化，模型在 H100 GPU 上实现了 10 秒视频 225 秒的推理速度，远快于 InfiniteTalk。</li>
</ul>
</li>
<li>
<p><strong>优势场景</strong>：</p>
<ul>
<li><strong>长视频生成与编辑</strong>：TAPSF 和参考条件化机制使其在生成和编辑长视频时，能保持较高的身份和外观一致性。</li>
<li><strong>精确的唇部同步</strong>：窗口化音频条件化和 V2V 训练策略确保了高精度的唇部同步。</li>
<li><strong>脚本驱动的精细化编辑</strong>：潜在空间编辑能力使其能够实现基于文本脚本的添加、删除、重定时长等操作。</li>
</ul>
</li>
<li>
<p><strong>局限性</strong>：</p>
<ul>
<li><strong>计算开销</strong>：尽管有优化，但扩散模型本身仍然需要较高的计算资源进行训练和推理。</li>
<li><strong>数据依赖</strong>：模型的性能很大程度上依赖于训练数据的质量和多样性。</li>
<li><strong>潜在的伪影</strong>：在处理非常复杂的背景或快速、剧烈的头部运动时，仍可能出现微小的伪影。</li>
<li><strong>对口语内容的理解</strong>：虽然可以基于文本脚本进行编辑，但模型本身并不理解口语内容的语义，只是根据脚本的修改来驱动视频的变化。</li>
</ul>
</li>
</ul>
<h3 id="5_4">5. 实用指南</h3>
<ul>
<li><strong>开源情况</strong>：论文提到了 LTX-Video 的开源实现 [69]，但 EditYourself 的具体代码实现并未在论文中明确说明是否开源。通常，这类研究会发布代码以供复现。</li>
<li><strong>实现细节</strong>：<ul>
<li><strong>基线模型</strong>：需要获取 LTX-Video [35] 的预训练模型。</li>
<li><strong>音频特征提取</strong>：需要集成 Whisper [87] 或类似的语音识别模型。</li>
<li><strong>面部区域检测</strong>：需要使用 MediaPipe [76] 或其他面部检测工具。</li>
<li><strong>训练超参数</strong>：论文中提供了训练阶段的超参数（如学习率、步数、条件丢弃概率等），复现时需要参考 Table 1。</li>
<li><strong>窗口大小 <code>W</code></strong>：需要根据实际需求和音频特征的采样率来调整。</li>
<li><strong>RoPE 参数</strong>：参考帧与块边界的时间距离 <code>Δt</code> 的阈值（如 3）需要仔细调整。</li>
</ul>
</li>
<li><strong>迁移可能</strong>：<ul>
<li><strong>迁移到其他扩散模型</strong>：EditYourself 的核心思想（音频条件化、窗口化对齐、潜在空间编辑、参考式身份保持）可以迁移到其他基于扩散 Transformer 的视频生成模型上。</li>
<li><strong>迁移到其他编辑任务</strong>：其潜在空间编辑的思路也可以用于其他类型的视频编辑任务，例如风格迁移、内容替换等，只需调整掩码和编辑操作。</li>
<li><strong>迁移到音频处理</strong>：音频窗口化和对齐的策略可能对其他需要精确同步音频和视频的任务有借鉴意义。</li>
</ul>
</li>
</ul>
<h3 id="6_2">6. 总结</h3>
<ul>
<li><strong>核心思想</strong>：基于扩散模型，通过音频和脚本驱动，实现视频的精细化编辑与唇部同步。</li>
<li><strong>速记版 pipeline</strong>：<ol>
<li><strong>提取音频</strong>：获取目标语音。</li>
<li><strong>对齐音频</strong>：用窗口策略精确匹配视频帧。</li>
<li><strong>编辑脚本</strong>：修改文本，生成编辑指令。</li>
<li><strong>潜在空间操作</strong>：在视频的潜在表示上执行添加/删除/重定时长。</li>
<li><strong>身份保持</strong>：利用参考帧防止漂移。</li>
<li><strong>生成视频</strong>：扩散模型根据音频和编辑指令重构视频。</li>
</ol>
</li>
</ul>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>Current generative video models excel at producing novel content from text and image prompts, but leave a critical gap in editing existing pre-recorded videos, where minor alterations to the spoken script require preserving motion, temporal coherence, speaker identity, and accurate lip synchronization.</li>
<li>We introduce EditYourself, a DiT-based framework for audio-driven video-to-video (V2V) editing that enables transcript-based modification of talking head videos, including the seamless addition, removal, and retiming of visually spoken content.</li>
<li>This enables precise lip synchronization and temporally coherent restructuring of existing performances via spatiotemporal inpainting, including the synthesis of realistic human motion in newly added segments, while maintaining visual fidelity and identity consistency over long durations.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.22127v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.22127v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.22125v1'></a></p>
<h2 id="creative-image-generation-with-diffusion-model"><a href="https://arxiv.org/abs/2601.22125v1">Creative Image Generation with Diffusion Model</a></h2>
<p><strong>Authors:</strong> Kunpeng Song, Ahmed Elgammal</p>
<p><strong>Published:</strong> 2026-01-29</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Creative image generation has emerged as a compelling area of research, driven by the need to produce novel and high-quality images that expand the boundaries of imagination. In this work, we propose a novel framework for creative generation using diffusion models, where creativity is associated with the inverse probability of an image's existence in the CLIP embedding space. Unlike prior approaches that rely on a manual blending of concepts or exclusion of subcategories, our method calculates the probability distribution of generated images and drives it towards low-probability regions to produce rare, imaginative, and visually captivating outputs. We also introduce pullback mechanisms, achieving high creativity without sacrificing visual fidelity. Extensive experiments on text-to-image diffusion models demonstrate the effectiveness and efficiency of our creative generation framework, showcasing its ability to produce unique, novel, and thought-provoking images. This work provides a new perspective on creativity in generative models, offering a principled method to foster innovation in visual content synthesis.</p>
<p><strong>Analysis:</strong></p>
<p>作为一名计算机视觉和机器学习领域的专家，我对这篇论文摘要进行了深入分析，并提供以下中文解读：</p>
<p><strong>1. 论文主要贡献的简洁总结 (2-3句话)</strong></p>
<p>该论文提出了一种新颖的创意图像生成框架，其核心在于将“创意”定义为在CLIP嵌入空间中图像存在的逆概率。通过驱动生成图像的概率分布趋向低概率区域，该方法能够生成罕见、富有想象力且视觉上引人入胜的图像，同时通过引入“回拉机制”来保证高创意性而不牺牲视觉保真度。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<ul>
<li><strong>“创意”的量化定义：</strong> 最核心的创新是将“创意”与图像在CLIP嵌入空间中的<strong>逆概率</strong>相关联。这意味着论文不再依赖于人工概念混合或排除子类别的方式来追求创意，而是通过一种量化的、基于概率分布的方式来定义和驱动创意生成。</li>
<li><strong>概率分布驱动生成：</strong> 方法的关键在于计算生成图像的概率分布，并将其<strong>驱动至低概率区域</strong>。这与传统生成模型（如GANs或早期扩散模型）倾向于生成高概率、常见样本的模式形成鲜明对比。低概率区域通常对应于更罕见、更具独特性和想象力的组合。</li>
<li><strong>回拉机制 (Pullback Mechanisms)：</strong> 为了在追求高创意性的同时不牺牲图像的视觉质量，论文引入了“回拉机制”。虽然摘要未详细说明具体机制，但可以推测这是一种能够将低概率区域的“创意”信息有效地“拉回”到可生成且视觉上令人愉悦的图像空间的技术，从而解决创意生成中常见的“失真”或“不可理解”的问题。</li>
</ul>
<p><strong>3. 对该领域的潜在影响</strong></p>
<ul>
<li><strong>重新定义生成模型的“创意”：</strong> 该研究为生成模型中的“创意”提供了一个更具理论基础和可操作性的定义。这可能促使研究人员从新的角度思考和设计生成模型，不再仅仅追求逼真度或多样性，而是将“创意性”作为一个可量化的目标。</li>
<li><strong>推动更具想象力的内容生成：</strong> 通过量化和驱动创意，该框架有望生成真正新颖、独特且能激发思考的视觉内容，这对于艺术创作、设计、娱乐等领域具有重要意义。</li>
<li><strong>为文本到图像生成模型注入新活力：</strong> 摘要提到在文本到图像扩散模型上的实验有效性，表明该方法可以显著提升现有文本到图像生成模型的创意输出能力，使其生成的图像更具艺术性和独特性，而不仅仅是字面上的匹配。</li>
<li><strong>为评估生成模型提供新视角：</strong> 这种基于概率的创意度量方式，也可能为评估生成模型的“创意性”提供一种新的、更客观的基准。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用</strong></p>
<ul>
<li><strong>艺术创作与设计：</strong> 艺术家和设计师可以利用该框架生成前所未有的视觉概念和灵感，探索新的艺术风格和表现形式。</li>
<li><strong>游戏开发：</strong> 游戏中的角色、场景、道具等元素的创意生成，可以极大地丰富游戏世界的想象力。</li>
<li><strong>广告与营销：</strong> 创造引人注目的、独特的视觉广告素材，以吸引消费者注意力。</li>
<li><strong>虚拟现实/增强现实内容生成：</strong> 构建更具想象力和沉浸感的虚拟环境和体验。</li>
<li><strong>内容推荐系统：</strong> 生成更具新颖性和吸引力的推荐内容缩略图或视觉元素。</li>
<li><strong>教育与科普：</strong> 以更具创意和吸引力的方式呈现复杂的概念或信息。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性</strong></p>
<ul>
<li><strong>“低概率”的定义与计算成本：</strong> CLIP嵌入空间的概率分布可能非常复杂，计算其精确的逆概率并驱动生成可能面临计算效率和稳定性问题。摘要中提到“效率”，暗示这可能是一个需要解决的挑战。</li>
<li><strong>“回拉机制”的细节未知：</strong> 摘要中对“回拉机制”的描述较为笼统，其具体实现方式、有效性以及是否会引入新的问题（如引入人工偏见）尚不清楚。</li>
<li><strong>“创意”的普适性：</strong> 虽然论文将创意与低概率关联，但“低概率”是否总是等同于“有价值的创意”仍需进一步验证。某些低概率的组合可能只是无意义的噪声或错误。</li>
<li><strong>对CLIP模型的依赖：</strong> 该方法高度依赖于CLIP模型对图像和文本的理解能力。如果CLIP模型本身存在偏见或局限性，可能会影响生成结果的创意性和质量。</li>
<li><strong>主观性评估：</strong> 尽管有量化指标，但“创意”本身具有一定的主观性。最终的评估仍需要人类的感知和判断，这可能引入评估的主观性。</li>
</ul>
<p>总而言之，这篇论文提出的将“创意”量化为CLIP嵌入空间逆概率的思路，以及通过概率分布驱动生成和引入回拉机制的策略，为生成模型的研究开辟了一个新的方向，尤其是在追求“新颖性”和“想象力”方面具有重要潜力。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Creative image generation has emerged as a compelling area of research, driven by the need to produce novel and high-quality images that expand the boundaries of imagination.</li>
<li>In this work, we propose a novel framework for creative generation using diffusion models, where creativity is associated with the inverse probability of an image's existence in the CLIP embedding space.</li>
<li>Unlike prior approaches that rely on a manual blending of concepts or exclusion of subcategories, our method calculates the probability distribution of generated images and drives it towards low-probability regions to produce rare, imaginative, and visually captivating outputs.</li>
<li>Extensive experiments on text-to-image diffusion models demonstrate the effectiveness and efficiency of our creative generation framework, showcasing its ability to produce unique, novel, and thought-provoking images.</li>
<li>This work provides a new perspective on creativity in generative models, offering a principled method to foster innovation in visual content synthesis.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.22125v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.22125v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.22094v1'></a></p>
<h2 id="refany3d-3d-asset-referenced-diffusion-models-for-image-generation"><a href="https://arxiv.org/abs/2601.22094v1">RefAny3D: 3D Asset-Referenced Diffusion Models for Image Generation</a></h2>
<p><strong>Authors:</strong> Hanzhuo Huang, Qingyang Bao, Zekai Gu, Zhongshuo Du, Cheng Lin, Yuan Liu, Sibei Yang</p>
<p><strong>Published:</strong> 2026-01-29</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>In this paper, we propose a 3D asset-referenced diffusion model for image generation, exploring how to integrate 3D assets into image diffusion models. Existing reference-based image generation methods leverage large-scale pretrained diffusion models and demonstrate strong capability in generating diverse images conditioned on a single reference image. However, these methods are limited to single-image references and cannot leverage 3D assets, constraining their practical versatility. To address this gap, we present a cross-domain diffusion model with dual-branch perception that leverages multi-view RGB images and point maps of 3D assets to jointly model their colors and canonical-space coordinates, achieving precise consistency between generated images and the 3D references. Our spatially aligned dual-branch generation architecture and domain-decoupled generation mechanism ensure the simultaneous generation of two spatially aligned but content-disentangled outputs, RGB images and point maps, linking 2D image attributes with 3D asset attributes. Experiments show that our approach effectively uses 3D assets as references to produce images consistent with the given assets, opening new possibilities for combining diffusion models with 3D content creation.</p>
<p><strong>Analysis:</strong></p>
<p>作为一名计算机视觉和机器学习领域的专家，我对这篇论文摘要进行了深入分析，并为您提供以下解读：</p>
<p><strong>1. 论文的主要贡献（2-3句话）：</strong></p>
<p>本论文提出了一种新颖的3D资产引导的扩散模型（RefAny3D），用于图像生成。其核心贡献在于首次将3D资产（通过多视图RGB图像和点云图表示）有效地整合到现有的基于参考的图像生成模型中，解决了现有方法仅限于单图像参考的局限性。通过这种方式，RefAny3D能够生成与给定3D资产在颜色和空间结构上高度一致的图像，极大地扩展了扩散模型在3D内容创作领域的应用潜力。</p>
<p><strong>2. 关键创新或方法论：</strong></p>
<p>RefAny3D的关键创新在于其<strong>跨域扩散模型（cross-domain diffusion model）</strong>和<strong>双分支感知（dual-branch perception）</strong>架构。</p>
<ul>
<li><strong>跨域扩散模型：</strong> 论文引入了一种能够处理不同模态数据（2D图像和3D资产表示）的扩散模型。这使得模型能够理解和生成与3D资产属性相匹配的2D图像。</li>
<li><strong>双分支感知架构：</strong> 该架构能够同时处理3D资产的多视图RGB图像和点云图。通过联合建模颜色信息（RGB图像）和规范空间坐标（点云图），模型能够精确地捕捉3D资产的几何和外观特征。</li>
<li><strong>空间对齐的双分支生成架构：</strong> 这是实现精确一致性的关键。它确保了模型能够同时生成在空间上对齐但内容解耦的两个输出：RGB图像和点云图。这种解耦机制使得2D图像属性与3D资产属性能够被独立建模和关联，从而实现更精细的控制。</li>
<li><strong>领域解耦生成机制（domain-decoupled generation mechanism）：</strong> 这个机制允许模型在生成过程中区分和处理来自不同域（2D和3D）的信息，但又能确保它们之间的协同作用，最终生成与3D资产高度匹配的2D图像。</li>
</ul>
<p><strong>3. 对该领域的潜在影响：</strong></p>
<p>RefAny3D的提出对计算机视觉和生成模型领域具有重要的潜在影响：</p>
<ul>
<li><strong>拓展了参考图像生成的边界：</strong> 从单图像参考扩展到3D资产参考，极大地增强了生成图像的控制力和多样性，尤其是在需要精确几何和外观一致性的场景下。</li>
<li><strong>促进了2D与3D内容的融合：</strong> 该研究为如何有效地利用3D信息来指导2D图像生成提供了一个成功的范例，为未来2D和3D内容创作的无缝集成奠定了基础。</li>
<li><strong>提升了3D资产的可视化和应用：</strong> 能够根据3D资产生成高质量、一致性的2D图像，将极大地便利3D资产的展示、营销、游戏开发、虚拟现实/增强现实内容制作等应用。</li>
<li><strong>推动了多模态生成模型的发展：</strong> RefAny3D的跨域建模和生成方法为开发更强大的多模态生成模型提供了新的思路和技术路径。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用：</strong></p>
<ul>
<li><strong>3D内容创作与可视化：</strong> 游戏开发、电影特效、产品设计、建筑可视化等领域，可以利用3D模型快速生成高质量的渲染图或概念图。</li>
<li><strong>虚拟现实（VR）与增强现实（AR）：</strong> 快速生成与虚拟3D场景或真实世界3D扫描对象匹配的2D图像，用于UI设计、场景预览等。</li>
<li><strong>电子商务：</strong> 为3D商品模型生成不同角度、不同光照下的高质量产品图片，提升用户体验。</li>
<li><strong>数字时尚与虚拟试穿：</strong> 根据3D服装模型生成逼真的2D试穿效果图。</li>
<li><strong>机器人与自动驾驶：</strong> 利用3D场景模型生成逼真的传感器数据（如摄像头图像），用于训练和测试算法。</li>
<li><strong>内容生成与编辑：</strong> 为用户提供更直观、更精确的图像编辑工具，例如基于3D模型的场景替换或物体添加。</li>
</ul>
<p><strong>5. 可从摘要推断的局限性：</strong></p>
<p>尽管摘要描述了该方法的优势，但仍可推断出一些潜在的局限性：</p>
<ul>
<li><strong>对3D资产表示的依赖：</strong> 该方法高度依赖于输入3D资产的质量和表示形式（多视图RGB和点云图）。如果3D资产本身存在缺陷或表示不完整，可能会影响生成图像的质量。</li>
<li><strong>计算复杂度：</strong> 扩散模型本身通常计算量较大，而引入3D资产的跨域建模和双分支处理可能会进一步增加模型的训练和推理成本。</li>
<li><strong>数据需求：</strong> 训练这样一个跨域模型可能需要大量的3D资产及其对应的多视图图像和点云数据，数据的获取和标注可能是一个挑战。</li>
<li><strong>“内容解耦”的程度：</strong> 摘要提到“内容解耦”，但实际应用中，完全解耦2D图像属性与3D资产属性可能非常困难，某些属性（如纹理细节）可能难以完全从3D模型中提取并精确映射到2D。</li>
<li><strong>泛化能力：</strong> 模型在未见过的3D资产类型或风格上的泛化能力有待验证。</li>
</ul>
<p>总而言之，RefAny3D是一项令人兴奋的研究，它成功地将3D资产的丰富信息引入到强大的2D图像生成模型中，为计算机视觉领域带来了新的可能性，尤其是在3D内容创作和2D-3D融合应用方面。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this paper, we propose a 3D asset-referenced diffusion model for image generation, exploring how to integrate 3D assets into image diffusion models.</li>
<li>To address this gap, we present a cross-domain diffusion model with dual-branch perception that leverages multi-view RGB images and point maps of 3D assets to jointly model their colors and canonical-space coordinates, achieving precise consistency between generated images and the 3D references.</li>
<li>Experiments show that our approach effectively uses 3D assets as references to produce images consistent with the given assets, opening new possibilities for combining diffusion models with 3D content creation.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.22094v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.22094v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2026-01-30 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
