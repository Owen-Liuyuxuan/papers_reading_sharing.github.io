<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-12-22 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-19
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-12-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-08
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-12-19/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-12-23/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-12-22">Arxiv Computer Vision Papers - 2025-12-22</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#both-semantics-and-reconstruction-matter-making-representation-encoders-ready-for-text-to-image-generation-and-editing" class="nav-link">Both Semantics and Reconstruction Matter: Making Representation Encoders Ready for Text-to-Image Generation and Editing</a>
                </li>
                <li class="nav-item">
                    <a href="#re-depth-anything-test-time-depth-refinement-via-self-supervised-re-lighting" class="nav-link">Re-Depth Anything: Test-Time Depth Refinement via Self-Supervised Re-lighting</a>
                </li>
                <li class="nav-item">
                    <a href="#dexterous-world-models" class="nav-link">Dexterous World Models</a>
                </li>
                <li class="nav-item">
                    <a href="#adversarial-robustness-of-vision-in-open-foundation-models" class="nav-link">Adversarial Robustness of Vision in Open Foundation Models</a>
                </li>
                <li class="nav-item">
                    <a href="#radargen-automotive-radar-point-cloud-generation-from-cameras" class="nav-link">RadarGen: Automotive Radar Point Cloud Generation from Cameras</a>
                </li>
                <li class="nav-item">
                    <a href="#keypoint-counting-classifiers-turning-vision-transformers-into-self-explainable-models-without-training" class="nav-link">Keypoint Counting Classifiers: Turning Vision Transformers into Self-Explainable Models Without Training</a>
                </li>
                <li class="nav-item">
                    <a href="#visually-prompted-benchmarks-are-surprisingly-fragile" class="nav-link">Visually Prompted Benchmarks Are Surprisingly Fragile</a>
                </li>
                <li class="nav-item">
                    <a href="#inspect-invariant-spectral-features-preservation-of-diffusion-models" class="nav-link">InSPECT: Invariant Spectral Features Preservation of Diffusion Models</a>
                </li>
                <li class="nav-item">
                    <a href="#anytask-an-automated-task-and-data-generation-framework-for-advancing-sim-to-real-policy-learning" class="nav-link">AnyTask: an Automated Task and Data Generation Framework for Advancing Sim-to-Real Policy Learning</a>
                </li>
                <li class="nav-item">
                    <a href="#infsplign-inference-time-spatial-alignment-of-text-to-image-diffusion-models" class="nav-link">InfSplign: Inference-Time Spatial Alignment of Text-to-Image Diffusion Models</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-12-22">Arxiv Computer Vision Papers - 2025-12-22</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>好的，作为一名专注于计算机视觉和机器学习的研究助理，我将为您提供一份关于2025年12月19日 Arxiv 计算机视觉领域论文的简明执行摘要。</p>
<hr />
<p><strong>执行摘要：2025年12月19日 Arxiv 计算机视觉论文精选</strong></p>
<p><strong>日期：</strong> 2025年12月19日</p>
<p><strong>主要趋势与主题：</strong></p>
<p>本期 Arxiv 论文集聚焦于<strong>多模态理解与生成</strong>、<strong>模型鲁棒性与可解释性</strong>，以及<strong>高效的视觉任务解决方案</strong>。特别值得注意的是，<strong>文本到图像生成与编辑</strong>的进一步深化，以及<strong>自监督学习</strong>在各种视觉任务中的广泛应用，包括深度估计和模型对齐。此外，<strong>模拟到真实（Sim-to-Real）的鸿沟</strong>的弥合以及<strong>模型在开放环境下的鲁棒性</strong>也成为研究热点。</p>
<p><strong>亮点与创新：</strong></p>
<ul>
<li><strong>文本到图像生成与编辑的精细化：</strong> "Both Semantics and Reconstruction Matter" 和 "InfSplign" 论文共同展示了在文本到图像生成和编辑领域，对语义理解和几何重建的同等重视，以及在推理时进行空间对齐的技术，预示着更精确、更可控的图像生成。</li>
<li><strong>自监督深度估计的突破：</strong> "Re-Depth Anything" 提出了一种在测试时通过自监督重照明来精炼深度估计的方法，显示了在无需额外标注数据的情况下提升深度感知能力的潜力。</li>
<li><strong>可解释性与鲁棒性的新方法：</strong> "Keypoint Counting Classifiers" 提出了一种无需训练即可将 Vision Transformers 转化为自解释模型的方法，而 "Adversarial Robustness of Vision in Open Foundation Models" 则深入探讨了开放基础模型在对抗攻击下的鲁棒性问题，这对于模型的安全部署至关重要。</li>
<li><strong>多模态融合的创新应用：</strong> "RadarGen" 实现了从摄像头数据生成汽车雷达点云，为自动驾驶中的多传感器融合提供了新的思路。</li>
</ul>
<p><strong>新兴研究方向与技术：</strong></p>
<ul>
<li><strong>多模态生成与编辑的深度融合：</strong> 结合语义理解、几何重建和推理时对齐，以实现更精细的文本到图像生成和编辑。</li>
<li><strong>自监督学习在各种视觉任务中的泛化应用：</strong> 从深度估计到模型对齐，自监督方法正成为减少对标注数据依赖的关键。</li>
<li><strong>开放世界基础模型的鲁棒性与安全性：</strong> 关注模型在复杂、不可控环境下的表现，以及对抗攻击的防御策略。</li>
<li><strong>模拟到真实（Sim-to-Real）的自动化与泛化：</strong> 开发自动化框架以生成多样化的任务和数据，加速机器人和自动驾驶等领域的策略学习。</li>
<li><strong>模型的可解释性与透明度：</strong> 探索无需额外训练即可实现模型自解释的方法，增强模型的信任度。</li>
</ul>
<p><strong>建议阅读论文：</strong></p>
<p>基于其潜在影响和创新性，以下论文值得深入阅读：</p>
<ol>
<li><strong>"Both Semantics and Reconstruction Matter: Making Representation Encoders Ready for Text-to-Image Generation and Editing"</strong>: 对于理解当前文本到图像生成和编辑技术的核心挑战以及未来的发展方向至关重要。</li>
<li><strong>"Re-Depth Anything: Test-Time Depth Refinement via Self-Supervised Re-lighting"</strong>: 在自监督深度估计领域具有显著的创新性，可能对三维重建和场景理解产生广泛影响。</li>
<li><strong>"Adversarial Robustness of Vision in Open Foundation Models"</strong>: 对于关注模型安全性和可靠性的研究人员来说，这篇论文提供了对当前基础模型脆弱性的重要见解。</li>
<li><strong>"Keypoint Counting Classifiers: Turning Vision Transformers into Self-Explainable Models Without Training"</strong>: 提供了实现模型可解释性的新颖且高效的方法，对于理解和信任深度学习模型具有重要意义。</li>
</ol>
<hr />
<p>希望这份执行摘要能帮助您快速了解该领域的最新进展。</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2512.17909v1">Both Semantics and Reconstruction Matter: Making Representation Encoders Ready for Text-to-Image Generation and Editing</a></li>
<li><a href="#2512.17908v1">Re-Depth Anything: Test-Time Depth Refinement via Self-Supervised Re-lighting</a></li>
<li><a href="#2512.17907v1">Dexterous World Models</a></li>
<li><a href="#2512.17902v1">Adversarial Robustness of Vision in Open Foundation Models</a></li>
<li><a href="#2512.17897v1">RadarGen: Automotive Radar Point Cloud Generation from Cameras</a></li>
<li><a href="#2512.17891v1">Keypoint Counting Classifiers: Turning Vision Transformers into Self-Explainable Models Without Training</a></li>
<li><a href="#2512.17875v1">Visually Prompted Benchmarks Are Surprisingly Fragile</a></li>
<li><a href="#2512.17873v1">InSPECT: Invariant Spectral Features Preservation of Diffusion Models</a></li>
<li><a href="#2512.17853v1">AnyTask: an Automated Task and Data Generation Framework for Advancing Sim-to-Real Policy Learning</a></li>
<li><a href="#2512.17851v1">InfSplign: Inference-Time Spatial Alignment of Text-to-Image Diffusion Models</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2512.17909v1'></a></p>
<h2 id="both-semantics-and-reconstruction-matter-making-representation-encoders-ready-for-text-to-image-generation-and-editing"><a href="https://arxiv.org/abs/2512.17909v1">Both Semantics and Reconstruction Matter: Making Representation Encoders Ready for Text-to-Image Generation and Editing</a></h2>
<p><strong>Authors:</strong> Shilong Zhang, He Zhang, Zhifei Zhang, Chongjian Ge, Shuchen Xue, Shaoteng Liu, Mengwei Ren, Soo Ye Kim, Yuqian Zhou, Qing Liu, Daniil Pakhomov, Kai Zhang, Zhe Lin, Ping Luo</p>
<p><strong>Published:</strong> 2025-12-19</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Modern Latent Diffusion Models (LDMs) typically operate in low-level Variational Autoencoder (VAE) latent spaces that are primarily optimized for pixel-level reconstruction. To unify vision generation and understanding, a burgeoning trend is to adopt high-dimensional features from representation encoders as generative latents. However, we empirically identify two fundamental obstacles in this paradigm: (1) the discriminative feature space lacks compact regularization, making diffusion models prone to off-manifold latents that lead to inaccurate object structures; and (2) the encoder's inherently weak pixel-level reconstruction hinders the generator from learning accurate fine-grained geometry and texture. In this paper, we propose a systematic framework to adapt understanding-oriented encoder features for generative tasks. We introduce a semantic-pixel reconstruction objective to regularize the latent space, enabling the compression of both semantic information and fine-grained details into a highly compact representation (96 channels with 16x16 spatial downsampling). This design ensures that the latent space remains semantically rich and achieves state-of-the-art image reconstruction, while remaining compact enough for accurate generation. Leveraging this representation, we design a unified Text-to-Image (T2I) and image editing model. Benchmarking against various feature spaces, we demonstrate that our approach achieves state-of-the-art reconstruction, faster convergence, and substantial performance gains in both T2I and editing tasks, validating that representation encoders can be effectively adapted into robust generative components.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Both Semantics and Reconstruction Matter: Making Representation Encoders Ready for Text-to-Image Generation and Editing”的全面中文摘要：</p>
<p><strong>论文题目：</strong> Both Semantics and Reconstruction Matter: Making Representation Encoders Ready for Text-to-Image Generation and Editing</p>
<p><strong>作者：</strong> Shilong Zhang, He Zhang, Zhifei Zhang, Chongjian Ge, Shuchen Xue, Shaoteng Liu, Mengwei Ren, Soo Ye Kim, Yuqian Zhou, Qing Liu, Daniil Pakhomov, Kai Zhang, Zhe Lin, Ping Luo</p>
<hr />
<p><strong>摘要：</strong></p>
<p>这篇论文旨在解决将强大的视觉理解编码器（representation encoders）应用于文本到图像生成和图像编辑任务时遇到的挑战。当前主流的生成模型（如 Latent Diffusion Models, LDMs）通常依赖于低维度的变分自编码器（VAE）的潜在空间，这些空间主要针对像素级重建进行优化。然而，直接使用高维度的、为理解任务设计的表示编码器的特征作为生成潜在空间存在两个主要问题：</p>
<ol>
<li><strong>缺乏紧凑的正则化：</strong> 表示编码器的特征空间维度高但内在信息量相对较低，缺乏有效的正则化，导致扩散模型容易生成“离流形”（off-manifold）的潜在向量，从而产生结构不准确或失真的对象。</li>
<li><strong>像素级重建能力弱：</strong> 表示编码器本身通常不优化像素级重建，其输出的特征丢失了精细的几何和纹理细节，这阻碍了生成器学习准确的细节。</li>
</ol>
<p>为了克服这些障碍，论文提出了一种系统性的框架，将理解导向的编码器特征适配到生成任务中。</p>
<p><strong>核心创新与方法贡献：</strong></p>
<ol>
<li><strong>语义-像素重建目标：</strong> 论文引入了一个创新的“语义-像素重建”（semantic-pixel reconstruction）目标。该目标首先通过一个语义自编码器（S-VAE）将高维度的、无约束的表示特征映射到一个紧凑的、经过 KL 散度正则化的潜在空间（例如，96通道，16x16空间分辨率）。这解决了离流形问题，并保留了丰富的语义信息。</li>
<li><strong>联合优化与精细化：</strong> 在此基础上，论文进一步解冻表示编码器，并联合优化一个像素级重建损失和一个语义重建损失。这使得表示编码器在捕获高层语义的同时，也能学习保留输入图像的精细几何和纹理细节。最终的模型被称为 <strong>Pixel-Semantic VAE (PS-VAE)</strong>。</li>
<li><strong>统一的生成架构：</strong> 利用 PS-VAE 产生的紧凑且语义丰富的潜在空间，论文设计了一个统一的文本到图像（T2I）和图像编辑模型。</li>
</ol>
<p><strong>主要结果与意义：</strong></p>
<ul>
<li><strong>卓越的重建性能：</strong> PS-VAE 在图像重建任务上达到了最先进的性能，显著优于其他基于表示编码器的生成方法，并且在某些方面可以媲美甚至超越专门为重建设计的 VAE。</li>
<li><strong>更快的收敛速度与更强的生成能力：</strong> 在文本到图像生成任务上，PS-VAE 展现出更快的收敛速度和更优越的最终性能，优于 RAE 等基线模型。</li>
<li><strong>显著提升的图像编辑能力：</strong> 在需要精确理解指令和保留图像细节的图像编辑任务上，PS-VAE 取得了大幅度的性能提升，显著优于仅依赖像素重建或仅依赖语义的基线模型。这表明其结合了语义理解和细节保留的能力。</li>
<li><strong>统一的编码器潜力：</strong> 论文证明了通过 PS-VAE 优化的表示编码器可以作为视觉理解和生成任务的统一编码器，为未来构建更通用的视觉模型提供了方向。</li>
</ul>
<p><strong>论文中提到的局限性：</strong></p>
<ul>
<li><strong>模型容量与细节权衡：</strong> 论文提到，虽然 96 通道的 PS-VAE 提供了良好的重建质量，但在生成指标上略逊于 32 通道的版本，这可能是因为过多的通道容量在建模精细细节时可能消耗模型能力，并干扰语义学习。</li>
<li><strong>LLM 微调的潜力：</strong> 在将 SigLIP2 作为统一编码器进行评估时，论文指出，在没有对 LLM 进行任何微调的情况下，PS-VAE 已经表现出色，但进一步的 LLM 微调可能带来更优越的性能。</li>
</ul>
<p><strong>潜在的未来研究方向：</strong></p>
<ul>
<li><strong>更高分辨率的生成：</strong> 论文提到，将 PS-VAE 应用于更高分辨率的生成任务将进一步提升其能力。</li>
<li><strong>LLM 与统一编码器的联合训练：</strong> 探索 LLM 与经过 PS-VAE 优化的统一编码器进行联合训练，以期获得超越当前基线模型的性能。</li>
<li><strong>更深入的架构探索：</strong> 对编码器和解码器的架构进行更深入的研究，以优化计算效率和性能。</li>
<li><strong>更广泛的预训练模型适配：</strong> 探索将 PS-VAE 框架应用于更多不同类型的预训练表示编码器。</li>
</ul>
<p><strong>总结：</strong></p>
<p>这篇论文成功地解决了将表示编码器应用于生成任务的关键挑战，通过引入创新的语义-像素重建目标，实现了对潜在空间的有效正则化和精细细节的保留。其提出的 PS-VAE 模型在图像重建、文本到图像生成和图像编辑等多个任务上均取得了显著的性能提升，并展示了其作为统一视觉理解和生成编码器的巨大潜力。这项工作为构建更强大、更通用的视觉模型提供了重要的理论和实践基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this paper, we propose a systematic framework to adapt understanding-oriented encoder features for generative tasks.</li>
<li>We introduce a semantic-pixel reconstruction objective to regularize the latent space, enabling the compression of both semantic information and fine-grained details into a highly compact representation (96 channels with 16x16 spatial downsampling).</li>
<li>This design ensures that the latent space remains semantically rich and achieves state-of-the-art image reconstruction, while remaining compact enough for accurate generation.</li>
<li>Benchmarking against various feature spaces, we demonstrate that our approach achieves state-of-the-art reconstruction, faster convergence, and substantial performance gains in both T2I and editing tasks, validating that representation encoders can be effectively adapted into robust generative components.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.17909v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.17909v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.17908v1'></a></p>
<h2 id="re-depth-anything-test-time-depth-refinement-via-self-supervised-re-lighting"><a href="https://arxiv.org/abs/2512.17908v1">Re-Depth Anything: Test-Time Depth Refinement via Self-Supervised Re-lighting</a></h2>
<p><strong>Authors:</strong> Ananta R. Bhattarai, Helge Rhodin</p>
<p><strong>Published:</strong> 2025-12-19</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Monocular depth estimation remains challenging as recent foundation models, such as Depth Anything V2 (DA-V2), struggle with real-world images that are far from the training distribution. We introduce Re-Depth Anything, a test-time self-supervision framework that bridges this domain gap by fusing DA-V2 with the powerful priors of large-scale 2D diffusion models. Our method performs label-free refinement directly on the input image by re-lighting predicted depth maps and augmenting the input. This re-synthesis method replaces classical photometric reconstruction by leveraging shape from shading (SfS) cues in a new, generative context with Score Distillation Sampling (SDS). To prevent optimization collapse, our framework employs a targeted optimization strategy: rather than optimizing depth directly or fine-tuning the full model, we freeze the encoder and only update intermediate embeddings while also fine-tuning the decoder. Across diverse benchmarks, Re-Depth Anything yields substantial gains in depth accuracy and realism over the DA-V2, showcasing new avenues for self-supervision by augmenting geometric reasoning.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Re-Depth Anything: Test-Time Depth Refinement via Self-Supervised Re-lighting”的全面中文摘要：</p>
<p><strong>论文题目：</strong> Re-Depth Anything: Test-Time Depth Refinement via Self-Supervised Re-lighting</p>
<p><strong>作者：</strong> Ananta R. Bhattarai, Helge Rhodin</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
单目深度估计（Monocular Depth Estimation, MDE）在处理与训练数据分布差异较大的真实世界图像时，即使是先进的基础模型（如 Depth Anything V2, DA-V2），也常常表现出不准确性。这种“域间隙”（domain gap）导致预测的深度图在细节、真实感等方面存在不足。</p>
<p><strong>2. 主要创新点/方法贡献：</strong>
本文提出了一种名为 <strong>Re-Depth Anything</strong> 的新颖的 <strong>测试时（test-time）自监督框架</strong>，旨在弥合这一域间隙。其核心创新在于：</p>
<ul>
<li><strong>融合2D扩散模型先验：</strong> 将 DA-V2 的几何推理能力与大型2D扩散模型（如 Stable Diffusion）强大的图像生成先验相结合。</li>
<li><strong>基于重照明（Re-lighting）的自监督方法：</strong> 引入了一种新颖的“重合成”（re-synthesis）方法，通过随机改变输入图像的光照条件来生成新的视角，从而替代了传统的基于光度重建（photometric reconstruction）的自监督方法。这种方法利用了<strong>形状来自阴影（Shape from Shading, SfS）</strong>的线索，并在<strong>生成式上下文</strong>中结合<strong>得分蒸馏采样（Score Distillation Sampling, SDS）</strong>损失来实现。</li>
<li><strong>目标化优化策略：</strong> 为了防止优化崩溃并保留预训练模型的几何知识，该框架采用了<strong>目标化优化策略</strong>。具体来说，它<strong>冻结了编码器</strong>，仅更新<strong>中间特征嵌入（intermediate embeddings）</strong>和<strong>解码器（decoder）的权重</strong>。</li>
<li><strong>可微分重照明渲染器：</strong> 开发了一个可微分的渲染器，能够将预测的深度图与输入图像联系起来，实现基于 SDS 损失的几何细化。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong>
Re-Depth Anything 在多个基准数据集（CO3D, KITTI, ETH3D）上均取得了显著的性能提升，<strong>在深度准确性和真实感方面均超越了 DA-V2 基线模型</strong>。具体而言，该方法能够：</p>
<ul>
<li><strong>增强细节：</strong> 显著提升了精细几何细节的恢复，例如物体边缘、纹理等。</li>
<li><strong>去除噪声和伪影：</strong> 有效地消除了 DA-V2 在平坦区域产生的噪声，并纠正了不准确的预测。</li>
<li><strong>修正偏差：</strong> 能够修正因训练数据偏差导致的错误预测，例如将狗的形状修正为更像老虎的形状（如图1所示）。</li>
<li><strong>泛化能力强：</strong> 即使在处理与训练数据分布差异较大的图像时，也能取得良好的效果，证明了其强大的泛化能力。</li>
</ul>
<p>这项工作展示了利用2D生成模型进行自监督几何推理的新途径，为提升单目深度估计在复杂场景下的性能提供了新的解决方案。</p>
<p><strong>4. 提及的局限性：</strong>
*   <strong>细微伪影：</strong> 在某些情况下，可能会观察到小的幻觉边缘（hallucinated edges），例如卡车上的贴纸。
*   <strong>过度平滑：</strong> 在某些区域，方法可能会过度平滑细节，例如在暗部区域的树木，或者在某些场景中出现轻微的过度平滑（如图10和图11所示）。
*   <strong>天空区域幻觉：</strong> 在 KITTI 数据集中，有时会在天空区域出现幻觉（如图12所示）。
*   <strong>对相机模型和参数的敏感性：</strong> 虽然作者进行了消融实验，但相机模型（如正交投影 vs. 透视投影）和参数（如 b 值）的选择仍然会影响最终结果。</p>
<p><strong>5. 未来研究方向：</strong>
*   <strong>探索替代的重合成方法：</strong> 除了重照明，还可以探索其他方式来生成多样的训练信号。
*   <strong>大规模模型微调：</strong> 计划在更大规模的真实世界视频数据上探索微调基础模型。
*   <strong>处理更复杂的场景：</strong> 进一步研究如何处理更具挑战性的场景，例如包含复杂光照、反射和透明物体的场景。
*   <strong>结合更精细的相机模型：</strong> 在已知相机参数的情况下，探索更精细的相机模型以获得更准确的深度估计。</p>
<p>总而言之，Re-Depth Anything 是一项重要的研究工作，它通过创新的测试时自监督重照明方法，有效解决了现有单目深度估计模型在处理真实世界图像时的域间隙问题，显著提升了深度估计的准确性和真实感，并为未来的研究开辟了新的方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce Re-Depth Anything, a test-time self-supervision framework that bridges this domain gap by fusing DA-V2 with the powerful priors of large-scale 2D diffusion models.</li>
<li>Our method performs label-free refinement directly on the input image by re-lighting predicted depth maps and augmenting the input.</li>
<li>This re-synthesis method replaces classical photometric reconstruction by leveraging shape from shading (SfS) cues in a new, generative context with Score Distillation Sampling (SDS).</li>
<li>Across diverse benchmarks, Re-Depth Anything yields substantial gains in depth accuracy and realism over the DA-V2, showcasing new avenues for self-supervision by augmenting geometric reasoning.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.17908v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.17908v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.17907v1'></a></p>
<h2 id="dexterous-world-models"><a href="https://arxiv.org/abs/2512.17907v1">Dexterous World Models</a></h2>
<p><strong>Authors:</strong> Byungjun Kim, Taeksoo Kim, Junyoung Lee, Hanbyul Joo</p>
<p><strong>Published:</strong> 2025-12-19</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Recent progress in 3D reconstruction has made it easy to create realistic digital twins from everyday environments. However, current digital twins remain largely static and are limited to navigation and view synthesis without embodied interactivity. To bridge this gap, we introduce Dexterous World Model (DWM), a scene-action-conditioned video diffusion framework that models how dexterous human actions induce dynamic changes in static 3D scenes.   Given a static 3D scene rendering and an egocentric hand motion sequence, DWM generates temporally coherent videos depicting plausible human-scene interactions. Our approach conditions video generation on (1) static scene renderings following a specified camera trajectory to ensure spatial consistency, and (2) egocentric hand mesh renderings that encode both geometry and motion cues to model action-conditioned dynamics directly. To train DWM, we construct a hybrid interaction video dataset. Synthetic egocentric interactions provide fully aligned supervision for joint locomotion and manipulation learning, while fixed-camera real-world videos contribute diverse and realistic object dynamics.   Experiments demonstrate that DWM enables realistic and physically plausible interactions, such as grasping, opening, and moving objects, while maintaining camera and scene consistency. This framework represents a first step toward video diffusion-based interactive digital twins and enables embodied simulation from egocentric actions.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行深入分析。</p>
<p><strong>论文摘要分析：Dexterous World Models</strong></p>
<p><strong>1. 论文的主要贡献（2-3句话）</strong></p>
<p>该论文提出了 Dexterous World Model (DWM)，一个创新的场景-动作条件视频扩散框架，能够模拟人类灵巧动作如何引起静态三维场景的动态变化。DWM 能够根据静态三维场景渲染和以自我为中心的（egocentric）手部运动序列，生成时间连贯且具有物理合理性的人体与场景交互视频。这标志着迈向基于视频扩散的交互式数字孪生和从以自我为中心动作进行具身仿真的重要一步。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<p>DWM 的核心创新在于其<strong>场景-动作条件视频扩散框架</strong>，以及如何有效地将这些条件融入视频生成过程：</p>
<ul>
<li><strong>条件化视频生成：</strong> DWM 巧妙地将视频生成过程条件化在两个关键要素上：<ul>
<li><strong>静态场景渲染（Spatial Consistency）：</strong> 通过指定相机轨迹的静态三维场景渲染，确保了生成视频在空间上的连贯性和一致性。这意味着生成的交互不会脱离预设的场景几何和视角。</li>
<li><strong>以自我为中心的手部网格渲染（Action-Conditioned Dynamics）：</strong> 这是该方法的一个重要亮点。通过输入编码了几何和运动信息的手部网格渲染，DWM 直接将动作的动态信息注入到视频生成中。这种方式比仅仅依赖文本描述或高级动作标签更精细，能够捕捉到更细微、更具“灵巧性”的手部操作。</li>
</ul>
</li>
<li><strong>混合交互视频数据集：</strong> 为了训练这样一个复杂的模型，作者构建了一个创新的混合数据集。<ul>
<li><strong>合成以自我为中心的交互：</strong> 提供完全对齐的监督信号，用于联合学习身体运动（locomotion）和物体操作（manipulation）。这使得模型能够学习到精确的因果关系。</li>
<li><strong>固定摄像机的真实世界视频：</strong> 引入了多样性和真实感，捕捉了现实世界中物体动态的复杂性，弥补了纯合成数据的不足。</li>
</ul>
</li>
</ul>
<p><strong>3. 对该领域的潜在影响</strong></p>
<p>DWM 的研究对计算机视觉领域具有重要的潜在影响：</p>
<ul>
<li><strong>推动具身智能（Embodied AI）的发展：</strong> 该框架为构建更逼真、更具交互性的具身智能体提供了基础。能够从以自我为中心的视角模拟和预测人类的交互行为，是实现智能体在复杂环境中进行自主操作的关键。</li>
<li><strong>提升三维场景理解和生成能力：</strong> DWM 不仅生成视频，更重要的是它在学习“场景-动作-动态变化”之间的因果关系。这有助于更深入地理解三维场景的物理属性以及人类如何与之交互。</li>
<li><strong>加速数字孪生（Digital Twins）的应用：</strong> 当前的数字孪生多为静态，DWM 的工作是实现真正意义上的“交互式数字孪生”的关键一步。这将极大地扩展数字孪生的应用范围，例如在虚拟现实（VR）、增强现实（AR）中的沉浸式体验，以及在机器人训练、远程操作等领域。</li>
<li><strong>视频生成技术的进步：</strong> 将扩散模型应用于更复杂的、条件化的视频生成任务，特别是涉及精细的物理交互和多模态条件（场景几何、手部动作），是视频生成领域的一个重要突破。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用</strong></p>
<ul>
<li><strong>虚拟现实（VR）/增强现实（AR）：</strong> 创建更逼真、更具交互性的虚拟环境，用户可以更自然地与虚拟物体互动。</li>
<li><strong>机器人学：</strong> 训练机器人进行精细操作，通过模拟人类的灵巧动作来学习和优化抓取、装配等任务。</li>
<li><strong>游戏开发：</strong> 生成更真实的虚拟角色与环境的交互动画，提升游戏体验。</li>
<li><strong>影视制作：</strong> 辅助生成复杂的交互场景动画，降低制作成本。</li>
<li><strong>人机交互（HCI）：</strong> 设计更直观、更自然的交互方式，尤其是在需要精细操作的场景。</li>
<li><strong>物理仿真：</strong> 为复杂的物理交互提供更逼真的视觉输出，用于研究和验证。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性</strong></p>
<p>尽管摘要描绘了一个令人兴奋的框架，但仍可以推断出一些潜在的局限性：</p>
<ul>
<li><strong>计算成本：</strong> 扩散模型通常计算成本较高，尤其是在生成长序列或高分辨率视频时。训练和推理的效率可能是一个挑战。</li>
<li><strong>泛化能力：</strong> 模型在多大程度上能够泛化到训练数据中未见过的场景、物体或动作类型，仍需进一步验证。例如，对于非常规的物体形状或极其复杂的、非人类标准的动作，模型表现如何？</li>
<li><strong>物理真实性的精确度：</strong> 尽管摘要提到“物理上可行”，但“物理上精确”是另一个层面的要求。模型生成的交互是否能完全符合所有物理定律（如摩擦力、惯性、形变等）的精确模拟，可能存在一定差距。</li>
<li><strong>数据依赖性：</strong> 模型的性能高度依赖于训练数据的质量和多样性。混合数据集的构建虽然有创新，但其覆盖范围和真实性仍是关键。</li>
<li><strong>“灵巧性”的定义和捕捉：</strong> “灵巧”是一个相对概念。摘要中提到“灵巧的人类动作”，但模型如何精确地捕捉和复现人类手部动作的细微之处（如手指的精细配合、触觉反馈的模拟等）可能仍有待深入研究。</li>
<li><strong>场景复杂性：</strong> 摘要提到“静态三维场景”，但对于包含大量动态元素、复杂光照或高度遮挡的场景，模型的表现可能受到影响。</li>
</ul>
<p>总而言之，Dexterous World Models 是一项非常有前景的研究，它通过创新的条件化视频扩散方法，有效地连接了静态三维场景和动态的以自我为中心的人类交互，为具身智能和交互式数字孪生领域开辟了新的可能性。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To bridge this gap, we introduce Dexterous World Model (DWM), a scene-action-conditioned video diffusion framework that models how dexterous human actions induce dynamic changes in static 3D scenes.</li>
<li>Our approach conditions video generation on (1) static scene renderings following a specified camera trajectory to ensure spatial consistency, and (2) egocentric hand mesh renderings that encode both geometry and motion cues to model action-conditioned dynamics directly.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.17907v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.17907v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.17902v1'></a></p>
<h2 id="adversarial-robustness-of-vision-in-open-foundation-models"><a href="https://arxiv.org/abs/2512.17902v1">Adversarial Robustness of Vision in Open Foundation Models</a></h2>
<p><strong>Authors:</strong> Jonathon Fox, William J Buchanan, Pavlos Papadopoulos</p>
<p><strong>Published:</strong> 2025-12-19</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.CR</p>
<p><strong>Abstract:</strong></p>
<p>With the increase in deep learning, it becomes increasingly difficult to understand the model in which AI systems can identify objects. Thus, an adversary could aim to modify an image by adding unseen elements, which will confuse the AI in its recognition of an entity. This paper thus investigates the adversarial robustness of LLaVA-1.5-13B and Meta's Llama 3.2 Vision-8B-2. These are tested for untargeted PGD (Projected Gradient Descent) against the visual input modality, and empirically evaluated on the Visual Question Answering (VQA) v2 dataset subset. The results of these adversarial attacks are then quantified using the standard VQA accuracy metric. This evaluation is then compared with the accuracy degradation (accuracy drop) of LLaVA and Llama 3.2 Vision. A key finding is that Llama 3.2 Vision, despite a lower baseline accuracy in this setup, exhibited a smaller drop in performance under attack compared to LLaVA, particularly at higher perturbation levels. Overall, the findings confirm that the vision modality represents a viable attack vector for degrading the performance of contemporary open-weight VLMs, including Meta's Llama 3.2 Vision. Furthermore, they highlight that adversarial robustness does not necessarily correlate directly with standard benchmark performance and may be influenced by underlying architectural and training factors.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Adversarial Robustness of Vision in Open Foundation Models”的全面中文摘要：</p>
<p><strong>论文题目：</strong> Adversarial Robustness of Vision in Open Foundation Models (视觉在开放基础模型中的对抗鲁棒性)</p>
<p><strong>作者：</strong> Jonathon Fox, William J Buchanan, Pavlos Papadopoulos</p>
<p><strong>摘要</strong></p>
<p><strong>1. 主要问题/研究问题：</strong>
随着深度学习模型在识别物体方面的能力日益增强，理解这些模型的内部工作机制变得愈发困难。这为攻击者提供了机会，他们可以通过修改图像来引入难以察觉的元素，从而欺骗AI系统，使其无法正确识别物体。本文旨在研究当前流行的开放权重视觉语言模型（VLMs）在视觉输入受到对抗性攻击时的鲁棒性。具体来说，研究关注的是 LLaVA-1.5-13B 和 Meta 的 Llama 3.2 Vision-8B-2 这两个模型，它们在视觉输入模态上，使用无目标投影梯度下降（PGD）攻击下的表现。</p>
<p><strong>2. 关键创新/方法论贡献：</strong>
*   <strong>模型选择与对比：</strong> 本研究选择了两个具有代表性的开放权重 VLM 进行对比分析：LLaVA-1.5-13B（一个已建立的 VLM 架构）和 Llama 3.2 Vision-8B-2（Meta 最新、采用适配器方法的模型）。这种选择允许研究者探究不同架构和训练规模对模型鲁棒性的影响。
*   <strong>对抗攻击方法：</strong> 采用了无目标投影梯度下降（PGD）攻击，这是一种广泛认可且强大的对抗攻击方法，用于评估模型的视觉输入鲁棒性。
*   <strong>评估指标与数据集：</strong> 使用了标准的视觉问答（VQA）准确率作为评估指标，并在 VQA v2 数据集的子集上进行了实证评估。通过比较模型在干净图像和对抗性图像上的准确率下降（accuracy drop）来量化其鲁棒性。
*   <strong>细致的超参数控制：</strong> PGD 攻击的超参数（如扰动预算 ε、步长 α 和迭代次数）被精心调整，以确保攻击强度与预算相匹配，从而进行公平的比较。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>普遍的脆弱性：</strong> 研究发现，两个模型都对视觉输入的对抗性攻击表现出明显的脆弱性。即使是细微的、近乎不可察觉的扰动（ε &lt; 16/255），也会导致 VQA 准确率的下降。
*   <strong>不同的鲁棒性表现：</strong>
    *   LLaVA-1.5-13B 在干净数据集上表现出更高的基线准确率（87.4%），但在对抗性攻击下，其准确率随扰动增大而显著下降，最大下降幅度达到 36.0 个百分点。
    *   Llama 3.2 Vision-8B-2 的基线准确率较低（42.8%），但在对抗性攻击下，其准确率下降幅度相对较小，尤其是在高扰动水平下（最大下降 10.2 个百分点，相对于其在该运行中的 41.6% 基线）。这表明 Llama 3.2 Vision 在面对视觉对抗性攻击时，表现出更强的相对鲁棒性。
*   <strong>架构与训练的影响：</strong> 研究推测，Llama 3.2 Vision 更复杂的跨注意力适配器机制、更大的预训练数据集以及可能更先进的对齐过程，可能有助于其更稳定的内部表示，从而在对抗性攻击下表现出更好的鲁棒性。
*   <strong>重要发现：</strong>
    *   视觉模态是降级当前开放权重 VLM 性能的可行攻击向量。
    *   对抗鲁棒性并不一定直接与标准基准性能相关，它可能受到底层架构和训练因素的影响。</p>
<p><strong>4. 论文中提到的局限性：</strong>
*   <strong>数据集规模：</strong> 由于计算资源的限制，实验仅在 VQA v2 数据集的 500 个样本子集上进行。在完整数据集或其他数据集上的结果可能有所不同。
*   <strong>攻击类型限制：</strong> 研究仅关注了无目标 PGD 攻击和 L∞ 范数约束。其他攻击算法（如 Carlini &amp; Wagner 攻击）或范数约束（L2, L∞）可能会揭示不同的脆弱性。
*   <strong>目标攻击缺失：</strong> 攻击是无目标的，旨在降低整体性能，而非诱导特定的错误输出。有针对性的攻击可能带来不同的挑战。
*   <strong>任务范围限制：</strong> 鲁棒性仅在 VQA 任务上进行了评估。在图像描述或复杂推理等其他多模态任务上的性能下降可能有所不同。
*   <strong>超参数探索有限：</strong> 尽管努力使用了合适的超参数，但计算成本限制了对 PGD 参数空间（迭代次数、步长）的详尽探索。</p>
<p><strong>5. 未来研究方向：</strong>
*   <strong>更全面的评估：</strong> 在更大的数据集子集或完整的 VQA v2 数据集上重复实验，并扩展到其他多模态基准测试，以获得更全面的鲁棒性评估。
*   <strong>多样化的攻击方法：</strong> 探索更广泛的攻击算法，包括 Carlini &amp; Wagner (CW) 攻击、Image Hijacks 等，以及有针对性的攻击，以更全面地评估模型的安全性。
*   <strong>性能差异的深入分析：</strong> 深入探究 Llama 3.2 Vision 在 VQA 子集上基线性能较低的原因，可能涉及不同的提示配置或预处理步骤。
*   <strong>架构与训练的细致分析：</strong> 更深入地分析特定组件（如多模态适配器的设计）和训练阶段（预训练数据规模、对齐技术如 RLHF）如何量化影响对抗鲁棒性。
*   <strong>防御策略的研究：</strong> 开发和评估专门针对 VLM 的有效防御策略，以减轻视觉对抗性攻击带来的风险。
*   <strong>原生多模态 vs. 适配器方法：</strong> 进一步研究原生多模态架构与适配器方法的对比，例如 Meta 的 Llama 4 等新架构，以理解它们在鲁棒性方面的影响。</p>
<p><strong>总结：</strong>
本文通过实证研究，揭示了当前开放权重视觉语言模型（LLaVA 和 Llama 3.2 Vision）在视觉输入受到对抗性攻击时的脆弱性。研究强调了视觉模态作为攻击向量的重要性，并指出对抗鲁棒性并非总是与标准性能指标直接相关，而是受到模型架构、训练规模和方法等多种因素的影响。研究结果为理解和提升这些强大模型的安全性提供了重要的见解，并为未来的研究指明了方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Overall, the findings confirm that the vision modality represents a viable attack vector for degrading the performance of contemporary open-weight VLMs, including Meta's Llama 3.2 Vision.</li>
<li>Furthermore, they highlight that adversarial robustness does not necessarily correlate directly with standard benchmark performance and may be influenced by underlying architectural and training factors.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.17902v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.17902v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.17897v1'></a></p>
<h2 id="radargen-automotive-radar-point-cloud-generation-from-cameras"><a href="https://arxiv.org/abs/2512.17897v1">RadarGen: Automotive Radar Point Cloud Generation from Cameras</a></h2>
<p><strong>Authors:</strong> Tomer Borreda, Fangqiang Ding, Sanja Fidler, Shengyu Huang, Or Litany</p>
<p><strong>Published:</strong> 2025-12-19</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.LG, cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>We present RadarGen, a diffusion model for synthesizing realistic automotive radar point clouds from multi-view camera imagery. RadarGen adapts efficient image-latent diffusion to the radar domain by representing radar measurements in bird's-eye-view form that encodes spatial structure together with radar cross section (RCS) and Doppler attributes. A lightweight recovery step reconstructs point clouds from the generated maps. To better align generation with the visual scene, RadarGen incorporates BEV-aligned depth, semantic, and motion cues extracted from pretrained foundation models, which guide the stochastic generation process toward physically plausible radar patterns. Conditioning on images makes the approach broadly compatible, in principle, with existing visual datasets and simulation frameworks, offering a scalable direction for multimodal generative simulation. Evaluations on large-scale driving data show that RadarGen captures characteristic radar measurement distributions and reduces the gap to perception models trained on real data, marking a step toward unified generative simulation across sensing modalities.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行深入分析。</p>
<p><strong>论文摘要分析：RadarGen: Automotive Radar Point Cloud Generation from Cameras</strong></p>
<p><strong>1. 论文的主要贡献（2-3句话）</strong></p>
<p>本研究提出了RadarGen，一个创新的扩散模型，能够从多视角摄像头图像生成逼真的汽车雷达点云。该模型通过将雷达测量值表示为鸟瞰图（BEV）形式，并融合视觉线索，实现了从图像到雷达点云的跨模态生成，为多模态生成式仿真提供了一个可扩展的解决方案。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<p>RadarGen的核心创新在于其<strong>将高效的图像-潜在扩散模型（Image-Latent Diffusion）成功适配到雷达点云生成领域</strong>。具体而言，其方法论的关键点包括：</p>
<ul>
<li><strong>雷达数据表示的创新：</strong> 将雷达测量值（包括空间结构、雷达截面积 RCS 和多普勒信息）编码到鸟瞰图（BEV）表示中。这种BEV表示能够有效地捕捉雷达数据的空间分布和物理属性。</li>
<li><strong>轻量级恢复步骤：</strong> 在生成BEV图之后，采用一个轻量级的恢复步骤将其转换回三维雷达点云。这使得模型在生成逼真雷达数据的同时，保持了计算效率。</li>
<li><strong>多模态融合与引导：</strong> 引入了从预训练基础模型中提取的BEV对齐的深度、语义和运动线索。这些视觉线索作为生成过程的引导，确保生成的雷达模式在物理上是合理的，并与视觉场景更加一致。</li>
<li><strong>条件生成能力：</strong> 通过以摄像头图像作为条件进行生成，RadarGen具有广泛的兼容性，可以与现有的视觉数据集和仿真框架集成，为多模态生成式仿真开辟了新的途径。</li>
</ul>
<p><strong>3. 对该领域的潜在影响</strong></p>
<p>RadarGen的潜在影响是深远的，主要体现在以下几个方面：</p>
<ul>
<li><strong>推动多模态生成式仿真：</strong> 长期以来，为自动驾驶系统生成逼真的传感器数据一直是研究的重点。RadarGen的出现，使得从易于获取的摄像头数据生成雷达点云成为可能，极大地降低了生成高质量、多模态仿真数据的门槛。这对于训练和评估自动驾驶感知算法，尤其是在数据稀缺或危险场景下，具有重要意义。</li>
<li><strong>提升雷达感知模型的鲁棒性：</strong> 通过生成大量逼真的雷达数据，可以用于扩充训练数据集，从而提高雷达感知模型的泛化能力和鲁棒性，使其在各种真实世界条件下表现更佳。</li>
<li><strong>促进跨传感器数据融合研究：</strong> RadarGen为研究不同传感器（如摄像头和雷达）之间的关联性提供了新的工具。通过生成同步的、相互一致的跨模态数据，可以更好地探索和优化多传感器融合算法。</li>
<li><strong>加速自动驾驶系统的开发和测试：</strong> 逼真的仿真数据能够加速自动驾驶系统的开发周期，允许在虚拟环境中进行更广泛、更安全的测试，从而更快地迭代和改进算法。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用</strong></p>
<ul>
<li><strong>自动驾驶感知系统开发与测试：</strong> 这是最直接的应用领域，用于生成训练和测试雷达目标检测、跟踪、分类等算法的数据。</li>
<li><strong>机器人导航与感知：</strong> 机器人也常常依赖雷达进行环境感知和导航，RadarGen可以为机器人领域提供逼真的雷达数据仿真。</li>
<li><strong>计算机视觉中的生成模型研究：</strong> 该研究将扩散模型成功应用于雷达点云这一非传统数据类型，为跨模态生成和扩散模型在不同领域的应用提供了新的思路。</li>
<li><strong>虚拟现实（VR）和增强现实（AR）：</strong> 在构建逼真的虚拟环境时，能够模拟不同传感器的输出将有助于提升沉浸感和交互性。</li>
<li><strong>遥感和测绘：</strong> 虽然论文聚焦于汽车雷达，但其核心技术可能可以推广到其他类型的雷达数据生成，例如用于遥感和测绘的雷达数据。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性</strong></p>
<p>尽管摘要展示了RadarGen的强大能力，但仍可以推断出一些潜在的局限性：</p>
<ul>
<li><strong>对预训练基础模型的依赖：</strong> RadarGen依赖于预训练的视觉基础模型来提取深度、语义和运动线索。这些基础模型的性能和泛化能力将直接影响RadarGen的生成质量。如果基础模型在特定场景下表现不佳，可能会导致生成的雷达数据不准确。</li>
<li><strong>“逼真性”的定义和评估：</strong> 摘要提到“逼真性”和“捕捉特征雷达测量分布”，但“逼真”的定义可能是一个主观或相对的概念。如何全面、客观地量化生成的雷达点云的逼真度，以及与真实雷达数据的差距，仍需要进一步的深入评估。</li>
<li><strong>计算资源需求：</strong> 扩散模型通常计算量较大，尽管摘要提到了“高效的图像-潜在扩散”，但生成高质量的雷达点云可能仍然需要可观的计算资源，尤其是在大规模数据集上进行训练和推理时。</li>
<li><strong>对特定雷达类型的适配性：</strong> 摘要主要关注“汽车雷达点云”。RadarGen是否能直接适用于其他类型的雷达（如气象雷达、SAR雷达等）或不同配置的汽车雷达，可能需要进一步的研究和调整。</li>
<li><strong>“轻量级恢复步骤”的性能边界：</strong> 虽然恢复步骤是轻量级的，但其性能上限可能会影响最终点云的细节和精度。如果恢复步骤丢失了过多的信息，生成的点云可能不够精细。</li>
</ul>
<p>总而言之，RadarGen是一项令人兴奋的研究，它巧妙地将先进的生成模型技术应用于跨模态传感器数据生成，为自动驾驶和相关领域的研究与开发带来了巨大的潜力。其核心在于将扩散模型的能力扩展到雷达领域，并利用视觉信息进行有效引导，从而实现从图像到雷达点云的高质量生成。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present RadarGen, a diffusion model for synthesizing realistic automotive radar point clouds from multi-view camera imagery.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.17897v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.17897v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.17891v1'></a></p>
<h2 id="keypoint-counting-classifiers-turning-vision-transformers-into-self-explainable-models-without-training"><a href="https://arxiv.org/abs/2512.17891v1">Keypoint Counting Classifiers: Turning Vision Transformers into Self-Explainable Models Without Training</a></h2>
<p><strong>Authors:</strong> Kristoffer Wickstrøm, Teresa Dorszewski, Siyan Chen, Michael Kampffmeyer, Elisabeth Wetzer, Robert Jenssen</p>
<p><strong>Published:</strong> 2025-12-19</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Current approaches for designing self-explainable models (SEMs) require complicated training procedures and specific architectures which makes them impractical. With the advance of general purpose foundation models based on Vision Transformers (ViTs), this impracticability becomes even more problematic. Therefore, new methods are necessary to provide transparency and reliability to ViT-based foundation models. In this work, we present a new method for turning any well-trained ViT-based model into a SEM without retraining, which we call Keypoint Counting Classifiers (KCCs). Recent works have shown that ViTs can automatically identify matching keypoints between images with high precision, and we build on these results to create an easily interpretable decision process that is inherently visualizable in the input. We perform an extensive evaluation which show that KCCs improve the human-machine communication compared to recent baselines. We believe that KCCs constitute an important step towards making ViT-based foundation models more transparent and reliable.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Keypoint Counting Classifiers: Turning Vision Transformers into Self-Explainable Models Without Training”的全面中文摘要：</p>
<p><strong>论文题目：</strong> Keypoint Counting Classifiers: Turning Vision Transformers into Self-Explainable Models Without Training (关键点计数分类器：将 Vision Transformers 转化为无需训练的自解释模型)</p>
<p><strong>作者：</strong> Kristoffer Wickstrøm, Teresa Dorszewski, Siyan Chen, Michael Kampffmeyer, Elisabeth Wetzer, Robert Jenssen</p>
<hr />
<p><strong>全面摘要</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong></p>
<p>当前设计自解释模型（Self-Explainable Models, SEMs）的方法通常需要复杂的训练流程和特定的模型架构，这使得它们在实践中难以应用，尤其是在通用性强的 Vision Transformers (ViTs) 基础模型日益普及的背景下，这一问题更为突出。现有的 SEMs 在灵活性和可视化解释方面存在局限性，例如它们通常依赖于卷积神经网络（CNN）架构，或者需要额外的训练来适应 ViTs，并且其可视化解释（如边界框和热力图）常常不够精确或信息量不足。因此，研究界迫切需要一种能够为 ViT 基础模型提供透明度和可靠性的新方法，同时保持其灵活性。</p>
<p><strong>2. 关键创新/方法论贡献：</strong></p>
<p>该论文提出了一种名为<strong>关键点计数分类器（Keypoint Counting Classifiers, KCCs）</strong>的新方法，旨在将任何已训练好的 ViT 基础模型转化为自解释模型，而<strong>无需进行任何额外的训练</strong>。KCCs 的核心创新在于：</p>
<ul>
<li><strong>无需训练的转换：</strong> KCCs 能够直接利用预训练的 ViT 模型，无需修改其特征提取器或训练额外的分类头，从而极大地提高了灵活性。</li>
<li><strong>基于关键点的解释：</strong> KCCs 利用 ViT 的内部表示（tokens）来自动识别图像中的语义部分，并将这些部分视为“关键点”。通过比较查询图像与原型图像之间的关键点匹配，生成直观且可解释的决策过程。</li>
<li><strong>关键点匹配与计数：</strong> 该方法通过计算查询图像和原型图像之间关键点的互为最近邻（Mutual Nearest Neighbors, MNNs）来识别匹配的语义区域。最终的分类决策是通过计数匹配到的关键点数量来完成的。</li>
<li><strong>可视化解释：</strong> KCCs 生成的可视化解释是匹配的关键点，这是一种全新的可视化方式，旨在提高解释的直观性和用户理解。</li>
<li><strong>利用 Vision-Language 能力（可选）：</strong> 在结合了具有视觉-语言能力的 ViTs 时，KCCs 可以自动为关键点生成文本描述，进一步减少读者的主观偏见。</li>
</ul>
<p>KCCs 的实现过程分为三个主要部分：
    a. <strong>图像关键点识别：</strong> 利用 ViT 的 tokens 来识别图像中的语义部分，并将每个部分的中心点定义为关键点。
    b. <strong>匹配关键点识别：</strong> 使用互为最近邻（MNNs）的方法来寻找查询图像和原型图像之间匹配的关键点。
    c. <strong>通过计数分类：</strong> 根据匹配到的关键点数量来决定最终的分类结果。</p>
<p><strong>3. 主要结果与意义：</strong></p>
<ul>
<li><strong>用户研究结果：</strong> 用户研究表明，KCCs 在解释质量和理解性方面显著优于现有的基线方法（如 PIP-Net 和 KMEx）。用户对 KCCs 的解释感到更自信，并且在纠正模型错误预测时更有信心。KCCs 在用户偏好方面与 KMEx 并列第一。</li>
<li><strong>定量评估结果：</strong> 在 CUB200、CARS 和 PETS 等数据集上的定量评估显示，KCCs 在准确性和复杂度方面与一些需要训练的 SEMs 相当，甚至在某些情况下表现更优，例如在 CUB200 数据集上，KCCs 的准确率高于 ProtoPNet，尽管 KCCs 未经训练。</li>
<li><strong>意义：</strong> KCCs 提供了一种新颖的范式，能够将强大的 ViT 基础模型转化为易于理解和解释的自解释模型，而无需额外的训练成本。这对于提高模型的可信度、透明度和用户交互性具有重要意义，尤其是在对安全性要求高的领域。</li>
</ul>
<p><strong>4. 提及的局限性：</strong></p>
<ul>
<li><strong>关键点权重问题：</strong> 目前 KCCs 中所有关键点被赋予相同的权重，但实际上某些关键点（如鸟类的喙部形状）可能对特定类别的识别更重要。如何有效地为关键点分配权重是一个潜在的研究方向。</li>
<li><strong>计算复杂度：</strong> 当原型数量非常多时，计算相似度可能会变得内存密集。论文中提到通过仅考虑距离最近的几个原型来缓解这个问题。</li>
<li><strong>细粒度分类的挑战：</strong> 论文提到，在细粒度分类任务中，尽管 KCCs 表现良好，但仍然需要监督信号来达到最佳性能。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>加权关键点：</strong> 探索如何为关键点引入权重，以更好地反映其在分类中的重要性。</li>
<li><strong>识别类特异性关键点：</strong> 研究如何识别并利用类特异性的关键点，以进一步提高解释的精确度。</li>
<li><strong>结合更先进的 Vision-Language 模型：</strong> 进一步探索如何利用最新的视觉-语言模型来增强 KCCs 的解释能力，例如生成更丰富、更具上下文的文本描述。</li>
<li><strong>应用于更多任务：</strong> 将 KCCs 的方法扩展到其他计算机视觉任务，如目标检测、分割等。</li>
</ul>
<p>总而言之，这篇论文提出了一种名为 KCCs 的创新方法，成功地将预训练的 ViT 模型转化为无需训练的自解释模型。通过利用关键点匹配和计数，KCCs 提供了直观、可解释的决策过程，并在用户研究和定量评估中展现出优越的性能和用户体验，为提高深度学习模型的透明度和可靠性开辟了新的途径。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Therefore, new methods are necessary to provide transparency and reliability to ViT-based foundation models.</li>
<li>In this work, we present a new method for turning any well-trained ViT-based model into a SEM without retraining, which we call Keypoint Counting Classifiers (KCCs).</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.17891v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.17891v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.17875v1'></a></p>
<h2 id="visually-prompted-benchmarks-are-surprisingly-fragile"><a href="https://arxiv.org/abs/2512.17875v1">Visually Prompted Benchmarks Are Surprisingly Fragile</a></h2>
<p><strong>Authors:</strong> Haiwen Feng, Long Lian, Lisa Dunlap, Jiahao Shu, XuDong Wang, Renhao Wang, Trevor Darrell, Alane Suhr, Angjoo Kanazawa</p>
<p><strong>Published:</strong> 2025-12-19</p>
<p><strong>Categories:</strong> cs.CV, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>A key challenge in evaluating VLMs is testing models' ability to analyze visual content independently from their textual priors. Recent benchmarks such as BLINK probe visual perception through visual prompting, where questions about visual content are paired with coordinates to which the question refers, with the coordinates explicitly marked in the image itself. While these benchmarks are an important part of VLM evaluation, we find that existing models are surprisingly fragile to seemingly irrelevant details of visual prompting: simply changing a visual marker from red to blue can completely change rankings among models on a leaderboard. By evaluating nine commonly-used open- and closed-source VLMs on two visually prompted tasks, we demonstrate how details in benchmark setup, including visual marker design and dataset size, have a significant influence on model performance and leaderboard rankings. These effects can even be exploited to lift weaker models above stronger ones; for instance, slightly increasing the size of the visual marker results in open-source InternVL3-8B ranking alongside or better than much larger proprietary models like Gemini 2.5 Pro. We further show that low-level inference choices that are often ignored in benchmarking, such as JPEG compression levels in API calls, can also cause model lineup changes. These details have substantially larger impacts on visually prompted benchmarks than on conventional semantic VLM evaluations. To mitigate this instability, we curate existing datasets to create VPBench, a larger visually prompted benchmark with 16 visual marker variants. VPBench and additional analysis tools are released at https://lisadunlap.github.io/vpbench/.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Visually Prompted Benchmarks Are Surprisingly Fragile”的全面中文摘要：</p>
<p><strong>论文题目：</strong> Visually Prompted Benchmarks Are Surprisingly Fragile</p>
<p><strong>作者：</strong> Haiwen Feng, Long Lian, Lisa Dunlap, Jiahao Shu, XuDong Wang, Renhao Wang, Trevor Darrell, Alane Suhr, Angjoo Kanazawa</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 主要问题/研究问题：</strong>
这篇论文的核心问题在于，当前用于评估视觉语言模型（VLMs）的视觉提示（visually prompted）基准测试存在显著的脆弱性。这些基准测试通过在图像中标记特定区域并提出相关问题来评估模型的视觉感知能力，旨在独立于文本先验知识。然而，研究发现，即使是视觉提示中看似无关紧要的细节，例如标记的颜色、大小、形状或位置，以及数据集的大小和低级别的推理设置（如JPEG压缩），都可能极大地影响模型的性能和排行榜排名。这种脆弱性使得当前的评估结果可能更多地反映了基准测试的设计细节而非模型真正的感知能力。</p>
<p><strong>2. 关键创新/方法贡献：</strong>
*   <strong>系统性地揭示视觉提示的脆弱性：</strong> 作者通过在多个常用的开源和闭源VLM上进行实验，系统地量化了视觉提示设计（如标记样式）和基准测试设置（如数据集大小、JPEG压缩）对模型性能和排名的影响。
*   <strong>引入VPBench基准测试：</strong> 为了解决现有基准测试的脆弱性问题，作者创建了一个名为VPBench的新基准测试。VPBench是一个扩展的视觉提示基准，包含16种不同的视觉标记变体，旨在提供更稳定和可靠的评估。
*   <strong>提供分析工具和建议：</strong> 作者不仅发布了VPBench数据集，还提供了相应的推理代码，支持不同的视觉标记和图像压缩设置，以帮助研究人员进行更稳健的评估。同时，论文也提出了标准化和多样化视觉提示、使用一致的低级别设置以及报告不确定性和排名稳定性等建议。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>微小变化导致排名剧烈波动：</strong> 研究发现，仅仅改变视觉标记的颜色（如从红色变为蓝色）就可能完全改变模型在排行榜上的排名。这种现象在视觉提示任务中尤为明显，其影响远大于传统语义评估任务。
*   <strong>标记样式和数据集大小是关键因素：</strong> 作者证明了视觉标记的设计（如大小、形状、颜色、文本位置）和数据集的大小对模型性能和排名有显著影响。甚至可以通过策略性地选择标记样式来“操纵”排行榜，使得较弱的模型（如InternVL3-8B）能够超越更强大的模型（如Gemini 2.5 Pro）。
*   <strong>低级别推理设置也影响排名：</strong> 即使是人类难以察觉的低级别推理设置，如JPEG压缩级别，也能导致模型排名的变化。
*   <strong>意义：</strong> 这些发现表明，当前许多VLM基准测试的评估结果可能受到非语义因素的干扰，而非模型真正的视觉理解能力。这削弱了对现有排行榜的信任，并强调了开发更稳健、更可靠的评估方法的重要性。VPBench的发布为解决这一问题提供了一个重要的工具。</p>
<p><strong>4. 提及的局限性：</strong>
*   <strong>VPBench的局限性：</strong> 虽然VPBench旨在提高稳定性，但它目前主要集中在相对深度估计和语义对应任务上。
*   <strong>模型对特定提示的过拟合：</strong> 研究暗示，一些模型可能对特定视觉提示（如默认的红色圆圈）存在过拟合现象，导致在其他提示下性能急剧下降。
*   <strong>评估的复杂性：</strong> 即使在VPBench中，不同模型对不同标记的反应仍然存在个体差异，表明模型在视觉提示处理方面存在内在的偏见。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>更广泛的视觉提示任务和数据集：</strong> 将VPBench的理念扩展到更多类型的视觉提示任务，并构建更大规模、更多样化的数据集。
*   <strong>鲁棒性评估的标准化：</strong> 进一步研究和标准化用于评估VLM鲁棒性的方法，包括对各种图像扰动和提示变化的敏感性。
*   <strong>模型内在偏见的分析：</strong> 深入研究模型为何对特定的视觉提示表现出不同的反应，以及如何减轻这种对特定提示的偏见。
*   <strong>开发更少依赖特定提示的评估方法：</strong> 探索不依赖于显式视觉标记的评估范式，或者能够自动适应不同提示的评估方法。
*   <strong>结合人类感知进行评估：</strong> 进一步探索如何将人类的视觉感知能力与VLM的评估相结合，以获得更具参考价值的评估结果。</p>
<p>总而言之，这篇论文通过揭示视觉提示基准测试的脆弱性，为VLM评估领域带来了重要的警示和贡献。它不仅指出了当前评估方法中存在的关键问题，还通过引入VPBench等工具和提出改进建议，为未来更可靠、更有意义的VLM评估奠定了基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>By evaluating nine commonly-used open- and closed-source VLMs on two visually prompted tasks, we demonstrate how details in benchmark setup, including visual marker design and dataset size, have a significant influence on model performance and leaderboard rankings.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.17875v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.17875v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.17873v1'></a></p>
<h2 id="inspect-invariant-spectral-features-preservation-of-diffusion-models"><a href="https://arxiv.org/abs/2512.17873v1">InSPECT: Invariant Spectral Features Preservation of Diffusion Models</a></h2>
<p><strong>Authors:</strong> Baohua Yan, Qingyuan Liu, Jennifer Kava, Xuan Di</p>
<p><strong>Published:</strong> 2025-12-19</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Modern diffusion models (DMs) have achieved state-of-the-art image generation. However, the fundamental design choice of diffusing data all the way to white noise and then reconstructing it leads to an extremely difficult and computationally intractable prediction task. To overcome this limitation, we propose InSPECT (Invariant Spectral Feature-Preserving Diffusion Model), a novel diffusion model that keeps invariant spectral features during both the forward and backward processes. At the end of the forward process, the Fourier coefficients smoothly converge to a specified random noise, enabling features preservation while maintaining diversity and randomness. By preserving invariant features, InSPECT demonstrates enhanced visual diversity, faster convergence rate, and a smoother diffusion process. Experiments on CIFAR-10, Celeb-A, and LSUN demonstrate that InSPECT achieves on average a 39.23% reduction in FID and 45.80% improvement in IS against DDPM for 10K iterations under specified parameter settings, which demonstrates the significant advantages of preserving invariant features: achieving superior generation quality and diversity, while enhancing computational efficiency and enabling faster convergence rate. To the best of our knowledge, this is the first attempt to analyze and preserve invariant spectral features in diffusion models.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为一名计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行深入分析。</p>
<p><strong>论文摘要分析：InSPECT: Invariant Spectral Features Preservation of Diffusion Models</strong></p>
<p><strong>1. 论文的主要贡献（2-3句话的简洁总结）</strong></p>
<p>本论文提出了一种名为 InSPECT 的新型扩散模型，其核心贡献在于在扩散模型的正向和反向过程中保持了不变的谱特征。通过使傅里叶系数平滑地收敛到指定的随机噪声，InSPECT 在保留图像特征的同时，实现了更高的视觉多样性、更快的收敛速度和更平滑的生成过程。实验结果表明，InSPECT 在生成质量和多样性方面显著优于现有的 DDPM 模型，并提高了计算效率。</p>
<p><strong>2. 关键创新点或方法论</strong></p>
<p>InSPECT 的关键创新点在于其对<strong>不变谱特征（Invariant Spectral Features）</strong>的引入和保持。传统的扩散模型（如 DDPM）将数据完全扩散到白噪声，这使得从噪声中重建原始数据成为一个极其困难且计算量巨大的任务。InSPECT 通过以下方式克服了这一限制：</p>
<ul>
<li><strong>谱特征的显式处理：</strong> 论文的核心思想是识别并保留图像在频域（例如通过傅里叶变换）中的关键谱特征。这些特征可能代表了图像的结构、纹理等本质信息。</li>
<li><strong>前向和反向过程中的不变性：</strong> InSPECT 确保这些谱特征在数据扩散（前向过程）和噪声重建（反向过程）的每一步都保持相对稳定或以可控的方式演变。</li>
<li><strong>平滑收敛到噪声：</strong> 论文提到，傅里叶系数平滑地收敛到指定的随机噪声。这意味着模型并没有完全破坏所有信息，而是以一种保留关键谱结构的方式进行扩散，从而为反向重建提供了更有用的起点。</li>
<li><strong>“不变性”的定义：</strong> 虽然摘要没有详细说明“不变性”的具体数学定义，但可以推断，这是一种在扩散过程中对某些频率分量或其组合的鲁棒性，使得它们在噪声扰动下依然能够被识别和重建。</li>
</ul>
<p><strong>3. 对该领域的潜在影响</strong></p>
<p>InSPECT 的研究对扩散模型领域具有重要的潜在影响：</p>
<ul>
<li><strong>提升生成质量和多样性：</strong> 通过保留关键的谱特征，模型能够生成更逼真、细节更丰富且具有更高视觉多样性的图像，这对于图像生成任务至关重要。</li>
<li><strong>提高计算效率和收敛速度：</strong> 摘要中提到的“更快的收敛速度”和“计算效率”是巨大的优势。这意味着在更少的迭代次数下就能达到更好的生成效果，从而降低了训练和推理成本，使得扩散模型在实际应用中更具可行性。</li>
<li><strong>新的理论视角：</strong> 这是首次尝试分析和保留扩散模型中的不变谱特征，为理解和改进扩散模型提供了新的理论框架和研究方向。这可能会激发更多关于信息保留和特征不变性在生成模型中作用的研究。</li>
<li><strong>更鲁棒的模型：</strong> 保持不变的谱特征可能意味着模型对输入噪声或扰动的鲁棒性更强，从而在更广泛的应用场景中表现更好。</li>
</ul>
<p><strong>4. 可能受益于此研究的相关领域或应用</strong></p>
<ul>
<li><strong>高分辨率图像生成：</strong> 能够生成细节丰富、结构清晰的图像，对于需要高质量图像的应用至关重要，如艺术创作、设计、虚拟现实等。</li>
<li><strong>图像编辑和风格迁移：</strong> 保留关键特征的能力可能有助于更精确地控制图像的编辑和风格迁移过程，避免破坏原始图像的核心内容。</li>
<li><strong>数据增强：</strong> 生成多样化且逼真的合成数据，用于训练其他计算机视觉模型，尤其是在数据稀缺的领域。</li>
<li><strong>医学影像分析：</strong> 生成高质量的医学影像，用于诊断、模拟或数据增强，同时保留重要的病理特征。</li>
<li><strong>视频生成：</strong> 将谱特征保留的思想扩展到视频领域，有望生成更连贯、更具动态细节的视频。</li>
<li><strong>科学可视化：</strong> 生成逼真的科学模拟结果或可视化数据。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性</strong></p>
<p>尽管摘要充满了积极的成果，但仍可以推断出一些潜在的局限性：</p>
<ul>
<li><strong>“不变性”的定义和实现细节：</strong> 摘要并未详细说明如何精确地定义和实现“不变谱特征”。这可能是一个复杂的技术挑战，并且具体的实现方式可能对模型的性能产生重要影响。</li>
<li><strong>计算开销的权衡：</strong> 虽然声称提高了计算效率，但引入谱特征分析和处理本身可能也会增加一定的计算开销。摘要中提到的“10K iterations under specified parameter settings”表明其效率提升是在特定条件下实现的，需要进一步研究其在不同参数和模型规模下的表现。</li>
<li><strong>对特定类型特征的偏好：</strong> 谱特征可能更擅长捕捉全局结构和周期性信息，而对于高度局部化、非结构化的细节（如精细的纹理、随机噪声模式）的保留能力可能需要进一步验证。</li>
<li><strong>泛化性：</strong> 摘要展示了在 CIFAR-10, Celeb-A, 和 LSUN 上的实验结果，这些数据集具有一定的代表性，但其在更复杂、更多样化的数据集上的泛化能力仍需评估。</li>
<li><strong>参数设置的敏感性：</strong> “under specified parameter settings”暗示了模型的性能可能对参数选择较为敏感，找到最优参数组合可能需要大量的实验。</li>
<li><strong>理论分析的深度：</strong> 摘要提到“这是第一份尝试分析和保留不变谱特征的论文”，这表明其理论分析可能仍处于初步阶段，未来需要更深入的数学证明和理论支撑。</li>
</ul>
<p>总而言之，InSPECT 提出了一种新颖且有前景的方法来改进扩散模型，通过关注谱特征的不变性来解决现有模型的效率和生成质量问题。其潜在的计算效率提升和生成质量的显著改善，使其成为一个值得关注的研究方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Modern diffusion models (DMs) have achieved state-of-the-art image generation.</li>
<li>To overcome this limitation, we propose InSPECT (Invariant Spectral Feature-Preserving Diffusion Model), a novel diffusion model that keeps invariant spectral features during both the forward and backward processes.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.17873v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.17873v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.17853v1'></a></p>
<h2 id="anytask-an-automated-task-and-data-generation-framework-for-advancing-sim-to-real-policy-learning"><a href="https://arxiv.org/abs/2512.17853v1">AnyTask: an Automated Task and Data Generation Framework for Advancing Sim-to-Real Policy Learning</a></h2>
<p><strong>Authors:</strong> Ran Gong, Xiaohan Zhang, Jinghuan Shang, Maria Vittoria Minniti, Jigarkumar Patel, Valerio Pepe, Riedana Yan, Ahmet Gundogdu, Ivan Kapelyukh, Ali Abbas, Xiaoqiang Yan, Harsh Patel, Laura Herlant, Karl Schmeckpeper</p>
<p><strong>Published:</strong> 2025-12-19</p>
<p><strong>Categories:</strong> cs.RO, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>Generalist robot learning remains constrained by data: large-scale, diverse, and high-quality interaction data are expensive to collect in the real world. While simulation has become a promising way for scaling up data collection, the related tasks, including simulation task design, task-aware scene generation, expert demonstration synthesis, and sim-to-real transfer, still demand substantial human effort. We present AnyTask, an automated framework that pairs massively parallel GPU simulation with foundation models to design diverse manipulation tasks and synthesize robot data. We introduce three AnyTask agents for generating expert demonstrations aiming to solve as many tasks as possible: 1) ViPR, a novel task and motion planning agent with VLM-in-the-loop Parallel Refinement; 2) ViPR-Eureka, a reinforcement learning agent with generated dense rewards and LLM-guided contact sampling; 3) ViPR-RL, a hybrid planning and learning approach that jointly produces high-quality demonstrations with only sparse rewards. We train behavior cloning policies on generated data, validate them in simulation, and deploy them directly on real robot hardware. The policies generalize to novel object poses, achieving 44% average success across a suite of real-world pick-and-place, drawer opening, contact-rich pushing, and long-horizon manipulation tasks. Our project website is at https://anytask.rai-inst.com .</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“AnyTask: an Automated Task and Data Generation Framework for Advancing Sim-to-Real Policy Learning”的全面中文摘要：</p>
<p><strong>论文题目：</strong> AnyTask: an Automated Task and Data Generation Framework for Advancing Sim-to-Real Policy Learning</p>
<p><strong>作者：</strong> Ran Gong, Xiaohan Zhang, Jinghuan Shang, Maria Vittoria Minniti, Jigarkumar Patel, Valerio Pepe, Riedana Yan, Ahmet Gundogdu, Ivan Kapelyukh, Ali Abbas, Xiaoqiang Yan, Harsh Patel, Laura Herlant, Karl Schmeckpeper</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
通用机器人学习面临数据瓶颈：大规模、多样化且高质量的真实世界交互数据收集成本高昂且耗时。尽管仿真提供了扩展数据收集的潜力，但任务设计、场景生成、专家演示合成以及仿真到现实（sim-to-real）的迁移等环节仍需大量人工干预。</p>
<p><strong>2. 主要创新点/方法论贡献：</strong>
本文提出了 <strong>AnyTask</strong>，一个自动化框架，旨在解决上述挑战。其核心创新在于：
*   <strong>自动化任务和数据生成流水线：</strong> AnyTask 集成了大规模并行 GPU 仿真与基础模型（如 VLM 和 LLM），能够自动设计多样化的操作任务、生成逼真的场景，并合成高质量的机器人演示数据。
*   <strong>多样化的演示生成代理：</strong> 引入了三种代理来自动生成专家演示，以最大化任务解决能力：
    *   <strong>ViPR：</strong> 一个新颖的任务与运动规划（TAMP）代理，结合了视觉语言模型（VLM）的迭代式并行精炼，以提高规划的鲁棒性。
    *   <strong>ViPR-Eureka：</strong> 一个强化学习（RL）代理，利用 LLM 生成的密集奖励和 LLM 指导的接触采样，以处理复杂接触任务。
    *   <strong>ViPR-RL：</strong> 一个混合规划与学习方法，结合了 TAMP 和 RL 的优势，能够仅凭稀疏奖励生成高质量演示。
*   <strong>智能对象数据库和任务生成器：</strong> 利用 VLM 和 LLM 构建了包含对象属性和语义信息的数据库，并能根据高层任务指令自动生成详细的任务描述和场景配置。
*   <strong>仿真生成器与 API：</strong> 能够将 LLM 生成的任务描述转化为可执行的仿真代码，并提供一套标准化的环境和机器人技能 API。
*   <strong>大规模并行仿真：</strong> 利用 GPU 加速的仿真环境，实现大规模、高效的数据收集。
*   <strong>零样本仿真到现实迁移：</strong> 训练的策略可以直接部署到物理机器人上，无需真实世界数据进行微调。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   AnyTask 框架能够生成多样化的任务和高质量的演示数据，显著减少了人工干预。
*   通过 AnyTask 生成的数据训练的策略在仿真环境中表现良好，并且能够泛化到新的物体姿态。
*   在真实世界机器人上进行了零样本仿真到现实迁移实验，在包括拾取-放置、开门、接触式推动和长时序操作等一系列任务中，取得了 <strong>44% 的平均成功率</strong>。
*   研究表明，AnyTask 生成的数据对于训练通用机器人策略至关重要，并且证明了仅使用合成数据实现有效的仿真到现实迁移是可行的。</p>
<p><strong>4. 论文中提到的局限性：</strong>
*   尽管代理展现了广泛的能力，但在需要高精度或复杂物理推理的任务（如任意物体堆叠）上，其性能仍有待提高。
*   成功的仿真到现实迁移依赖于点云观测。将此扩展到 RGB 图像作为输入将降低对硬件的要求。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   将框架扩展到更多种类的物体和机器人形态。
*   将框架应用于更复杂的长时序移动操作任务。
*   探索使用 RGB 图像作为输入，以降低对传感器硬件的要求。
*   进一步提升代理在复杂物理交互任务上的性能。</p>
<p><strong>对计算机视觉领域的意义：</strong>
这篇论文在计算机视觉领域具有重要意义，因为它展示了如何利用大型基础模型（VLM 和 LLM）与大规模并行仿真相结合，<strong>自动化机器人学习中的数据生成过程</strong>。这不仅解决了机器人领域长期存在的数据获取难题，还为开发更通用、更鲁棒的机器人策略提供了新的途径。特别是，它强调了：
*   <strong>VLM/LLM 在理解和生成复杂任务指令、对象属性以及指导机器人行为方面的强大能力。</strong>
*   <strong>仿真在生成大规模、多样化、标注丰富的数据集方面的潜力，以及如何通过仿真来弥合与现实世界的差距。</strong>
*   <strong>点云作为一种有效的视觉输入，在实现零样本仿真到现实迁移中的作用。</strong></p>
<p>AnyTask 的方法论为未来机器人学习研究开辟了新的方向，尤其是在如何利用 AI 的进步来加速和简化机器人系统的开发和部署方面。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present AnyTask, an automated framework that pairs massively parallel GPU simulation with foundation models to design diverse manipulation tasks and synthesize robot data.</li>
<li>We introduce three AnyTask agents for generating expert demonstrations aiming to solve as many tasks as possible: 1) ViPR, a novel task and motion planning agent with VLM-in-the-loop Parallel Refinement; 2) ViPR-Eureka, a reinforcement learning agent with generated dense rewards and LLM-guided contact sampling; 3) ViPR-RL, a hybrid planning and learning approach that jointly produces high-quality demonstrations with only sparse rewards.</li>
<li>The policies generalize to novel object poses, achieving 44% average success across a suite of real-world pick-and-place, drawer opening, contact-rich pushing, and long-horizon manipulation tasks.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.17853v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.17853v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.17851v1'></a></p>
<h2 id="infsplign-inference-time-spatial-alignment-of-text-to-image-diffusion-models"><a href="https://arxiv.org/abs/2512.17851v1">InfSplign: Inference-Time Spatial Alignment of Text-to-Image Diffusion Models</a></h2>
<p><strong>Authors:</strong> Sarah Rastegar, Violeta Chatalbasheva, Sieger Falkena, Anuj Singh, Yanbo Wang, Tejas Gokhale, Hamid Palangi, Hadi Jamali-Rad</p>
<p><strong>Published:</strong> 2025-12-19</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>Text-to-image (T2I) diffusion models generate high-quality images but often fail to capture the spatial relations specified in text prompts. This limitation can be traced to two factors: lack of fine-grained spatial supervision in training data and inability of text embeddings to encode spatial semantics. We introduce InfSplign, a training-free inference-time method that improves spatial alignment by adjusting the noise through a compound loss in every denoising step. Proposed loss leverages different levels of cross-attention maps extracted from the backbone decoder to enforce accurate object placement and a balanced object presence during sampling. The method is lightweight, plug-and-play, and compatible with any diffusion backbone. Our comprehensive evaluations on VISOR and T2I-CompBench show that InfSplign establishes a new state-of-the-art (to the best of our knowledge), achieving substantial performance gains over the strongest existing inference-time baselines and even outperforming the fine-tuning-based methods. Codebase is available at GitHub.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“InfSplign: Inference-Time Spatial Alignment of Text-to-Image Diffusion Models”的全面中文摘要：</p>
<p><strong>论文题目：</strong> InfSplign: 推理时文本到图像扩散模型的空间对齐</p>
<p><strong>作者：</strong> Sarah Rastegar, Violeta Chatalbasheva, Sieger Falkena, Anuj Singh, Yanbo Wang, Tejas Gokhale, Hamid Palangi, Hadi Jamali-Rad</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 主要问题/研究问题：</strong>
文本到图像（T2I）扩散模型在生成高质量图像方面取得了显著进展，但它们在准确捕捉文本提示中指定的物体空间关系方面存在固有缺陷。这种局限性主要源于两个因素：训练数据中缺乏细粒度的空间监督，以及文本嵌入本身难以有效编码空间语义。这导致模型在生成图像时，物体的位置、相对关系（如“左边”、“右边”、“上面”、“下面”）经常出现错误，甚至完全忽略了这些空间指令。</p>
<p><strong>2. 关键创新/方法贡献：</strong>
为了解决上述问题，作者提出了 <strong>InfSplign</strong>，一种<strong>无需训练、在推理时</strong>对T2I扩散模型进行空间对齐的方法。其核心创新在于：</p>
<ul>
<li><strong>推理时空间对齐损失：</strong> InfSplign 在每次去噪步骤中引入一个复合损失函数，通过调整去噪过程中的噪声来引导生成过程。</li>
<li><strong>利用多层级交叉注意力图：</strong> 该方法巧妙地从扩散模型 U-Net 解码器的不同层级（粗粒度、中粒度）提取交叉注意力图。这些注意力图被视为物体空间信息的代理。</li>
<li><strong>三个关键损失项：</strong><ul>
<li><strong>空间对齐损失 (Lspatial)：</strong> 通过计算物体在注意力图中的质心（centroid）来估计物体的位置，并根据文本提示中的空间关系（如“左”、“右”、“上”、“下”）来惩罚偏离预期位置的行为。</li>
<li><strong>物体存在损失 (Lpresence)：</strong> 通过最小化物体注意力图的方差来确保物体在最终图像中清晰可见，防止物体被弱化或消失。</li>
<li><strong>表示平衡损失 (Lbalance)：</strong> 通过平衡不同物体在注意力图中的分散程度，防止一个物体过度主导而另一个物体被抑制，确保所有物体都能得到充分表示。</li>
</ul>
</li>
<li><strong>轻量级、即插即用：</strong> InfSplign 是一种轻量级的方法，不需要对预训练的扩散模型进行任何微调或重新训练，可以轻松地集成到任何现有的扩散模型骨干网络中。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong>
作者在两个广泛使用的空间理解基准测试集——VISOR 和 T2I-CompBench 上进行了全面的评估。结果表明：</p>
<ul>
<li><strong>状态艺术（State-of-the-Art）性能：</strong> InfSplign 在这两个基准测试上都取得了显著的性能提升，在空间对齐方面达到了新的最先进水平（据作者所知）。</li>
<li><strong>超越现有方法：</strong> 相较于最强的现有推理时基线方法，InfSplign 实现了大幅度的性能提升（例如，在VISOR-4上提升高达24.81%）。更重要的是，它甚至超越了一些需要额外训练或微调的方法。</li>
<li><strong>鲁棒性：</strong> InfSplign 在处理不同物体组合和空间关系时都表现出良好的鲁棒性，即使是那些不常见的物体组合也能生成更具空间一致性的图像。</li>
<li><strong>意义：</strong> InfSplign 的成功表明，通过在推理时巧妙地利用模型内部的注意力机制，可以在不增加训练成本的情况下，显著提升T2I模型在空间理解方面的能力，这对于需要精确空间布局的应用（如机器人导航、增强现实）至关重要。</li>
</ul>
<p><strong>4. 提及的局限性：</strong>
*   <strong>罕见物体组合的挑战：</strong> 对于在自然场景中极少共同出现的物体组合，基础扩散模型本身就难以生成，InfSplign 在这种情况下也难以完全纠正空间对齐，因为物体的存在本身就受到限制。在这种情况下，物体准确率（object accuracy）成为瓶颈。
*   <strong>依赖于注意力图质量：</strong> 方法的有效性在一定程度上依赖于扩散模型骨干网络生成的高质量注意力图。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>扩展到 Transformer 架构：</strong> 作者正在积极探索将 InfSplign 扩展到 Transformer 架构，并已取得初步成果。
*   <strong>更复杂的空间关系：</strong> 研究将进一步探索更复杂的多维空间关系，例如三维空间中的相对位置。</p>
<p>总而言之，InfSplign 是一项重要的研究成果，它提供了一种高效且易于实现的推理时方法，显著改善了文本到图像扩散模型在理解和生成物体空间关系方面的能力，为实现更具可控性和准确性的图像生成开辟了新的途径。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce InfSplign, a training-free inference-time method that improves spatial alignment by adjusting the noise through a compound loss in every denoising step.</li>
<li>Our comprehensive evaluations on VISOR and T2I-CompBench show that InfSplign establishes a new state-of-the-art (to the best of our knowledge), achieving substantial performance gains over the strongest existing inference-time baselines and even outperforming the fine-tuning-based methods.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.17851v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.17851v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-12-22 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
