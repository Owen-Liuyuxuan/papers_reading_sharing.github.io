<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-12-22 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-19
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-12-22
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-12-19/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../../depth_completion/DNet/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-12-22">Arxiv Computer Vision Papers - 2025-12-22</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#both-semantics-and-reconstruction-matter-making-representation-encoders-ready-for-text-to-image-generation-and-editing" class="nav-link">Both Semantics and Reconstruction Matter: Making Representation Encoders Ready for Text-to-Image Generation and Editing</a>
                </li>
                <li class="nav-item">
                    <a href="#re-depth-anything-test-time-depth-refinement-via-self-supervised-re-lighting" class="nav-link">Re-Depth Anything: Test-Time Depth Refinement via Self-Supervised Re-lighting</a>
                </li>
                <li class="nav-item">
                    <a href="#dexterous-world-models" class="nav-link">Dexterous World Models</a>
                </li>
                <li class="nav-item">
                    <a href="#adversarial-robustness-of-vision-in-open-foundation-models" class="nav-link">Adversarial Robustness of Vision in Open Foundation Models</a>
                </li>
                <li class="nav-item">
                    <a href="#radargen-automotive-radar-point-cloud-generation-from-cameras" class="nav-link">RadarGen: Automotive Radar Point Cloud Generation from Cameras</a>
                </li>
                <li class="nav-item">
                    <a href="#keypoint-counting-classifiers-turning-vision-transformers-into-self-explainable-models-without-training" class="nav-link">Keypoint Counting Classifiers: Turning Vision Transformers into Self-Explainable Models Without Training</a>
                </li>
                <li class="nav-item">
                    <a href="#visually-prompted-benchmarks-are-surprisingly-fragile" class="nav-link">Visually Prompted Benchmarks Are Surprisingly Fragile</a>
                </li>
                <li class="nav-item">
                    <a href="#inspect-invariant-spectral-features-preservation-of-diffusion-models" class="nav-link">InSPECT: Invariant Spectral Features Preservation of Diffusion Models</a>
                </li>
                <li class="nav-item">
                    <a href="#anytask-an-automated-task-and-data-generation-framework-for-advancing-sim-to-real-policy-learning" class="nav-link">AnyTask: an Automated Task and Data Generation Framework for Advancing Sim-to-Real Policy Learning</a>
                </li>
                <li class="nav-item">
                    <a href="#infsplign-inference-time-spatial-alignment-of-text-to-image-diffusion-models" class="nav-link">InfSplign: Inference-Time Spatial Alignment of Text-to-Image Diffusion Models</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-12-22">Arxiv Computer Vision Papers - 2025-12-22</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>好的，作为一名专注于计算机视觉和机器学习的研究助理，我将为您提供一份关于2025年12月19日 Arxiv 计算机视觉领域论文的简明执行摘要。</p>
<hr />
<p><strong>执行摘要：2025年12月19日 Arxiv 计算机视觉论文速览</strong></p>
<p><strong>日期：</strong> 2025年12月19日</p>
<p><strong>主要主题与趋势：</strong></p>
<p>本期 Arxiv 论文涵盖了计算机视觉领域的多个前沿方向，其中尤为突出的是：</p>
<ul>
<li><strong>生成模型与编辑的融合：</strong> 多篇论文致力于提升文本到图像生成和编辑的质量与可控性，强调了表示编码器在语义理解和重建中的关键作用。</li>
<li><strong>多模态与跨模态学习：</strong> 从摄像头生成雷达点云，以及利用视觉提示进行模型评估，都体现了多模态信息融合的重要性。</li>
<li><strong>模型鲁棒性与可解释性：</strong> 对抗性鲁棒性、视觉模型在开放基础模型中的表现，以及无需训练即可实现模型自解释性的方法，是提升模型可靠性和透明度的重要研究方向。</li>
<li><strong>模拟到现实（Sim-to-Real）的挑战与解决方案：</strong> 自动化任务和数据生成框架的出现，旨在弥合模拟环境与真实世界之间的差距。</li>
<li><strong>深度估计与场景理解的精进：</strong> 通过自监督重光照等技术，进一步提升深度估计的精度和鲁棒性。</li>
</ul>
<p><strong>亮点与创新：</strong></p>
<ul>
<li><strong>“Both Semantics and Reconstruction Matter: Making Representation Encoders Ready for Text-to-Image Generation and Editing”</strong> 提出了一种新的表示编码器设计，显著提升了文本到图像生成和编辑的质量，是生成模型领域的重要进展。</li>
<li><strong>“Re-Depth Anything: Test-Time Depth Refinement via Self-Supervised Re-lighting”</strong> 引入了一种新颖的测试时深度精炼方法，利用自监督重光照技术，有望大幅提高现有深度估计模型的性能。</li>
<li><strong>“Keypoint Counting Classifiers: Turning Vision Transformers into Self-Explainable Models Without Training”</strong> 提出了一种无需额外训练即可使 Vision Transformers 具备自解释性的方法，为模型的可解释性研究开辟了新途径。</li>
<li><strong>“RadarGen: Automotive Radar Point Cloud Generation from Cameras”</strong> 实现了从摄像头图像生成汽车雷达点云，为自动驾驶中的传感器融合和数据增强提供了新的解决方案。</li>
</ul>
<p><strong>新兴研究方向与技术：</strong></p>
<ul>
<li><strong>更精细的文本到图像控制：</strong> 通过对表示编码器的优化和推理时空间对齐，实现更精确的图像生成和编辑。</li>
<li><strong>测试时模型自适应：</strong> 利用自监督学习等技术，在测试阶段对模型进行优化，以适应特定场景或数据分布。</li>
<li><strong>无监督/自监督的可解释性方法：</strong> 探索在不依赖标注数据的情况下，提升模型透明度和可信度的新技术。</li>
<li><strong>多模态数据生成与融合：</strong> 跨越不同传感器模态（如视觉与雷达）的数据生成与融合，以应对更复杂的现实世界场景。</li>
<li><strong>自动化模拟环境与任务生成：</strong> 为强化学习和机器人领域提供更高效、更具泛化性的训练数据和任务。</li>
</ul>
<p><strong>建议阅读论文：</strong></p>
<p>考虑到其潜在影响和创新性，以下论文值得深入阅读：</p>
<ol>
<li><strong>“Both Semantics and Reconstruction Matter: Making Representation Encoders Ready for Text-to-Image Generation and Editing”</strong>：对于关注文本到图像生成和编辑的研究人员至关重要。</li>
<li><strong>“Re-Depth Anything: Test-Time Depth Refinement via Self-Supervised Re-lighting”</strong>：对于深度估计和3D计算机视觉领域的研究人员具有重要价值。</li>
<li><strong>“Keypoint Counting Classifiers: Turning Vision Transformers into Self-Explainable Models Without Training”</strong>：对于模型可解释性研究以及 Vision Transformer 的应用探索具有开创性意义。</li>
<li><strong>“RadarGen: Automotive Radar Point Cloud Generation from Cameras”</strong>：对于自动驾驶、传感器融合和多模态学习的研究人员具有直接的应用价值。</li>
</ol>
<hr />
<p>希望这份执行摘要能帮助您快速了解近期 Arxiv 计算机视觉领域的最新动态。</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2512.17909v1">Both Semantics and Reconstruction Matter: Making Representation Encoders Ready for Text-to-Image Generation and Editing</a></li>
<li><a href="#2512.17908v1">Re-Depth Anything: Test-Time Depth Refinement via Self-Supervised Re-lighting</a></li>
<li><a href="#2512.17907v1">Dexterous World Models</a></li>
<li><a href="#2512.17902v1">Adversarial Robustness of Vision in Open Foundation Models</a></li>
<li><a href="#2512.17897v1">RadarGen: Automotive Radar Point Cloud Generation from Cameras</a></li>
<li><a href="#2512.17891v1">Keypoint Counting Classifiers: Turning Vision Transformers into Self-Explainable Models Without Training</a></li>
<li><a href="#2512.17875v1">Visually Prompted Benchmarks Are Surprisingly Fragile</a></li>
<li><a href="#2512.17873v1">InSPECT: Invariant Spectral Features Preservation of Diffusion Models</a></li>
<li><a href="#2512.17853v1">AnyTask: an Automated Task and Data Generation Framework for Advancing Sim-to-Real Policy Learning</a></li>
<li><a href="#2512.17851v1">InfSplign: Inference-Time Spatial Alignment of Text-to-Image Diffusion Models</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2512.17909v1'></a></p>
<h2 id="both-semantics-and-reconstruction-matter-making-representation-encoders-ready-for-text-to-image-generation-and-editing"><a href="https://arxiv.org/abs/2512.17909v1">Both Semantics and Reconstruction Matter: Making Representation Encoders Ready for Text-to-Image Generation and Editing</a></h2>
<p><strong>Authors:</strong> Shilong Zhang, He Zhang, Zhifei Zhang, Chongjian Ge, Shuchen Xue, Shaoteng Liu, Mengwei Ren, Soo Ye Kim, Yuqian Zhou, Qing Liu, Daniil Pakhomov, Kai Zhang, Zhe Lin, Ping Luo</p>
<p><strong>Published:</strong> 2025-12-19</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Modern Latent Diffusion Models (LDMs) typically operate in low-level Variational Autoencoder (VAE) latent spaces that are primarily optimized for pixel-level reconstruction. To unify vision generation and understanding, a burgeoning trend is to adopt high-dimensional features from representation encoders as generative latents. However, we empirically identify two fundamental obstacles in this paradigm: (1) the discriminative feature space lacks compact regularization, making diffusion models prone to off-manifold latents that lead to inaccurate object structures; and (2) the encoder's inherently weak pixel-level reconstruction hinders the generator from learning accurate fine-grained geometry and texture. In this paper, we propose a systematic framework to adapt understanding-oriented encoder features for generative tasks. We introduce a semantic-pixel reconstruction objective to regularize the latent space, enabling the compression of both semantic information and fine-grained details into a highly compact representation (96 channels with 16x16 spatial downsampling). This design ensures that the latent space remains semantically rich and achieves state-of-the-art image reconstruction, while remaining compact enough for accurate generation. Leveraging this representation, we design a unified Text-to-Image (T2I) and image editing model. Benchmarking against various feature spaces, we demonstrate that our approach achieves state-of-the-art reconstruction, faster convergence, and substantial performance gains in both T2I and editing tasks, validating that representation encoders can be effectively adapted into robust generative components.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Both Semantics and Reconstruction Matter: Making Representation Encoders Ready for Text-to-Image Generation and Editing”的全面中文摘要：</p>
<p><strong>论文题目：</strong> Both Semantics and Reconstruction Matter: Making Representation Encoders Ready for Text-to-Image Generation and Editing</p>
<p><strong>作者：</strong> Shilong Zhang, He Zhang, Zhifei Zhang, Chongjian Ge, Shuchen Xue, Shaoteng Liu, Mengwei Ren, Soo Ye Kim, Yuqian Zhou, Qing Liu, Daniil Pakhomov, Kai Zhang, Zhe Lin, Ping Luo</p>
<p><strong>摘要：</strong></p>
<p>这篇论文旨在解决将强大的视觉理解编码器（representation encoders）应用于文本到图像生成和图像编辑任务时遇到的挑战。当前主流的生成模型（如 Latent Diffusion Models, LDMs）通常依赖于低维度的变分自编码器（VAE）的潜在空间，这些空间主要针对像素级重建进行优化。然而，直接使用高维度的、为理解任务设计的表示编码器的特征作为生成潜在空间存在两个主要问题：</p>
<ol>
<li><strong>缺乏紧凑的正则化：</strong> 表示编码器的特征空间维度高但内在信息含量低，缺乏有效的正则化，导致扩散模型容易生成“离流形”（off-manifold）的潜在向量，从而产生结构不准确或扭曲的对象。</li>
<li><strong>像素级重建能力弱：</strong> 表示编码器本身通常不优化像素级重建，导致其生成的特征细节不足，难以让生成器学习到精确的几何形状和纹理。</li>
</ol>
<p>为了克服这些障碍，论文提出了一种系统性的框架，将理解导向的编码器特征适配到生成任务中。</p>
<p><strong>核心创新与方法贡献：</strong></p>
<ol>
<li><strong>语义-像素重建目标：</strong> 论文引入了一个创新的“语义-像素重建”（semantic-pixel reconstruction）目标。该目标旨在正则化潜在空间，将语义信息和精细的像素细节压缩到一个紧凑的表示中（例如，96通道，16x16空间下采样）。</li>
<li><strong>像素-语义 VAE (PS-VAE)：</strong> 论文设计了一个名为 PS-VAE 的模型。它首先通过一个语义 VAE（S-VAE）将高维度的、未正则化的表示编码器特征映射到一个紧凑的、KL 正则化的潜在空间。然后，通过解冻编码器并联合优化像素解码器和语义重建损失，进一步丰富该潜在空间，使其同时包含丰富的语义信息和高保真的像素细节。</li>
<li><strong>统一的生成架构：</strong> 基于 PS-VAE，论文设计了一个统一的文本到图像（T2I）和图像编辑模型。</li>
</ol>
<p><strong>主要结果与意义：</strong></p>
<ul>
<li><strong>卓越的重建性能：</strong> PS-VAE 在图像重建任务上达到了最先进的性能，显著优于其他基于表示编码器的生成方法，并且在某些方面甚至超越了纯粹的 VAE 方法。</li>
<li><strong>更快的收敛速度与更优的生成性能：</strong> 在文本到图像生成任务中，PS-VAE 表现出更快的收敛速度和更高的最终性能（GenEval 和 DPG-Bench 分数）。</li>
<li><strong>显著提升的图像编辑能力：</strong> 在需要精确理解和执行指令的图像编辑任务中，PS-VAE 取得了大幅度的性能提升，显著优于 RAE 等基线模型。这表明其语义结构和高保真细节的结合对于理解复杂指令至关重要。</li>
<li><strong>统一的编码器潜力：</strong> 论文证明了经过 PS-VAE 优化的表示编码器可以作为理解和生成任务的统一编码器，无需对大型语言模型（LLM）进行额外的微调，即可保持强大的理解能力。</li>
</ul>
<p><strong>论文中提到的局限性：</strong></p>
<ul>
<li><strong>模型容量与细节的权衡：</strong> 论文提到，虽然 96 通道的 PS-VAE 提供了更好的重建质量，但在生成指标上略逊于 32 通道的 PS-VAE，这可能是因为模型容量有限，难以同时建模过多的精细细节。</li>
<li><strong>高维特征直接增强的失败：</strong> 论文尝试直接在原始高维特征空间上进行像素重建，发现虽然重建质量有所提升，但生成性能却大幅下降，出现严重的结构伪影，表明直接处理高维特征空间存在固有的困难。</li>
<li><strong>SigLIP2 的重建性能：</strong> 在使用 SigLIP2 作为编码器时，其重建性能相比 DINOv2 略有饱和，可能与其更高级别的抽象表示有关。</li>
</ul>
<p><strong>潜在的未来研究方向：</strong></p>
<ul>
<li><strong>更高分辨率的生成：</strong> 论文提到，将 PS-VAE 应用于更高分辨率的生成将进一步提升其能力。</li>
<li><strong>LLM 微调的协同作用：</strong> 论文推测，如果对 LLM 进行联合微调，可能会使 SigLIP2 编码器在理解任务上表现得更好。</li>
<li><strong>更深入的架构探索：</strong> 论文中提到了一些架构上的探索，例如对称设计和直接在高维空间增强细节，这些方向仍有待进一步深入研究。</li>
<li><strong>模型容量的扩展：</strong> 探索如何通过增加模型容量来更好地利用高通道数潜在空间中的丰富信息，以实现更高的生成性能上限。</li>
</ul>
<p>总而言之，这篇论文成功地弥合了视觉理解编码器与生成任务之间的鸿沟，通过引入创新的语义-像素重建目标和 PS-VAE 模型，实现了在文本到图像生成和图像编辑任务上的显著进步，并为构建统一的视觉理解与生成模型提供了有力的支持。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this paper, we propose a systematic framework to adapt understanding-oriented encoder features for generative tasks.</li>
<li>We introduce a semantic-pixel reconstruction objective to regularize the latent space, enabling the compression of both semantic information and fine-grained details into a highly compact representation (96 channels with 16x16 spatial downsampling).</li>
<li>This design ensures that the latent space remains semantically rich and achieves state-of-the-art image reconstruction, while remaining compact enough for accurate generation.</li>
<li>Benchmarking against various feature spaces, we demonstrate that our approach achieves state-of-the-art reconstruction, faster convergence, and substantial performance gains in both T2I and editing tasks, validating that representation encoders can be effectively adapted into robust generative components.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.17909v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.17909v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.17908v1'></a></p>
<h2 id="re-depth-anything-test-time-depth-refinement-via-self-supervised-re-lighting"><a href="https://arxiv.org/abs/2512.17908v1">Re-Depth Anything: Test-Time Depth Refinement via Self-Supervised Re-lighting</a></h2>
<p><strong>Authors:</strong> Ananta R. Bhattarai, Helge Rhodin</p>
<p><strong>Published:</strong> 2025-12-19</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Monocular depth estimation remains challenging as recent foundation models, such as Depth Anything V2 (DA-V2), struggle with real-world images that are far from the training distribution. We introduce Re-Depth Anything, a test-time self-supervision framework that bridges this domain gap by fusing DA-V2 with the powerful priors of large-scale 2D diffusion models. Our method performs label-free refinement directly on the input image by re-lighting predicted depth maps and augmenting the input. This re-synthesis method replaces classical photometric reconstruction by leveraging shape from shading (SfS) cues in a new, generative context with Score Distillation Sampling (SDS). To prevent optimization collapse, our framework employs a targeted optimization strategy: rather than optimizing depth directly or fine-tuning the full model, we freeze the encoder and only update intermediate embeddings while also fine-tuning the decoder. Across diverse benchmarks, Re-Depth Anything yields substantial gains in depth accuracy and realism over the DA-V2, showcasing new avenues for self-supervision by augmenting geometric reasoning.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Re-Depth Anything: Test-Time Depth Refinement via Self-Supervised Re-lighting”的全面中文摘要：</p>
<p><strong>论文题目：</strong> Re-Depth Anything: Test-Time Depth Refinement via Self-Supervised Re-lighting</p>
<p><strong>作者：</strong> Ananta R. Bhattarai, Helge Rhodin</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
单目深度估计（Monocular Depth Estimation, MDE）在处理与训练数据分布差异较大的真实世界图像时，即使是先进的基础模型（如Depth Anything V2, DA-V2），也常常表现出不准确性。这种“域间隙”（domain gap）导致预测的深度图在细节、真实感和几何结构上存在不足。</p>
<p><strong>2. 主要创新点/方法论贡献：</strong>
本文提出了一种名为 <strong>Re-Depth Anything</strong> 的新颖 <strong>测试时自监督框架</strong>，旨在弥合这一域间隙。其核心创新在于：</p>
<ul>
<li><strong>融合2D扩散模型先验：</strong> 将强大的2D扩散模型（如Stable Diffusion）的先验知识与DA-V2模型相结合，利用扩散模型对图像光照和纹理的丰富理解来指导深度图的精炼。</li>
<li><strong>基于重照明（Re-lighting）的自监督信号：</strong> 引入了一种创新的 <strong>重合成（re-synthesis）方法</strong>，通过随机改变输入图像的光照条件来生成重照明的图像。这种方法取代了传统的、对渲染器要求极高的光度重建（photometric reconstruction）。</li>
<li><strong>利用形状阴影（SfS）和SDS损失：</strong> 借鉴了形状阴影（Shape-from-Shading, SfS）的原理，在生成式上下文中使用 <strong>分数蒸馏采样（Score Distillation Sampling, SDS）</strong> 损失来评估重照明图像与原始图像的匹配度，从而实现无标签的深度图精炼。</li>
<li><strong>目标化优化策略：</strong> 为防止优化崩溃和保留几何结构，作者提出了一种 <strong>目标化优化策略</strong>。具体来说，冻结了DA-V2的编码器，仅更新中间特征嵌入（embeddings）和解码器（DPT head）的权重。这种方法在保留了预训练模型强大的几何知识的同时，有效地调整了模型以适应特定输入图像。</li>
<li><strong>多运行平均（Ensembling）：</strong> 为了应对SDS损失的随机性带来的结果方差，通过多次运行优化并对结果进行平均来稳定最终的深度图预测。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong>
Re-Depth Anything 在多个基准数据集（CO3D, KITTI, ETH3D）上均取得了显著的性能提升，相较于DA-V2基线模型，在深度准确性和真实感方面均有substantial gains。</p>
<ul>
<li><strong>定量评估：</strong> 在多个评估指标上（如AbsRel, RMSE, log10, SI log, SqRel等）均显示出相对误差的显著降低，例如在KITTI数据集的SI log和RMSE log上降低了8.5%，在ETH3D数据集的AbsRel上降低了8.4%。</li>
<li><strong>定性评估：</strong> 实验结果表明，Re-Depth Anything 能够有效地增强细节（如纹理、边缘），去除平坦区域的噪声，并纠正模型在处理特定物体（如论文中的老虎图像）时可能出现的偏差（如从狗的形状纠正为老虎的形状）。</li>
<li><strong>意义：</strong> 该方法展示了利用2D生成模型作为先验，通过测试时自监督学习来提升现有深度估计模型泛化能力的新途径，尤其是在处理“in-the-wild”图像时。它证明了在不依赖额外标注数据的情况下，可以有效地弥合模型训练数据与实际应用场景之间的差距。</li>
</ul>
<p><strong>4. 提及的局限性：</strong>
论文中提到了一些局限性：</p>
<ul>
<li><strong>小范围的幻觉边缘：</strong> 在某些情况下，模型可能会产生小的幻觉边缘，例如卡车上的贴纸。</li>
<li><strong>过度平滑：</strong> 在某些区域，模型可能会过度平滑细节，例如在黑暗区域的树木，或者在某些场景中（如图10和图11所示的红色方框区域）。</li>
<li><strong>天空区域的幻觉：</strong> 在KITTI数据集的某些场景中，模型可能会在天空区域产生幻觉（如图12所示的红色方框区域）。</li>
<li><strong>对物体细节的提升更明显：</strong> 在CO3D数据集中，模型对单个物体细节的提升更为显著，而在房间和街道场景中，最大的收益来自于去除初始DA-V2预测中的可疑细节，从而产生更逼真的重照明效果，但可能保留了实际细节。</li>
</ul>
<p><strong>5. 未来研究方向：</strong>
作者表示，未来工作将探索：</p>
<ul>
<li><strong>替代性的重合成方法：</strong> 寻找其他能够生成逼真光照效果的重合成技术。</li>
<li><strong>大规模模型微调：</strong> 探索在更大规模的真实世界视频数据上对基础模型进行微调的可能性。</li>
</ul>
<p><strong>总结：</strong>
Re-Depth Anything 是一项重要的工作，它通过一种创新的测试时自监督重照明方法，有效地解决了现有单目深度估计模型在处理分布外图像时的局限性。该方法巧妙地利用了2D扩散模型的强大先验，并采用目标化优化策略，在不增加标注成本的情况下，显著提升了深度图的准确性和真实感，为提升现有深度估计模型的泛化能力开辟了新的方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce Re-Depth Anything, a test-time self-supervision framework that bridges this domain gap by fusing DA-V2 with the powerful priors of large-scale 2D diffusion models.</li>
<li>Our method performs label-free refinement directly on the input image by re-lighting predicted depth maps and augmenting the input.</li>
<li>This re-synthesis method replaces classical photometric reconstruction by leveraging shape from shading (SfS) cues in a new, generative context with Score Distillation Sampling (SDS).</li>
<li>Across diverse benchmarks, Re-Depth Anything yields substantial gains in depth accuracy and realism over the DA-V2, showcasing new avenues for self-supervision by augmenting geometric reasoning.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.17908v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.17908v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.17907v1'></a></p>
<h2 id="dexterous-world-models"><a href="https://arxiv.org/abs/2512.17907v1">Dexterous World Models</a></h2>
<p><strong>Authors:</strong> Byungjun Kim, Taeksoo Kim, Junyoung Lee, Hanbyul Joo</p>
<p><strong>Published:</strong> 2025-12-19</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Recent progress in 3D reconstruction has made it easy to create realistic digital twins from everyday environments. However, current digital twins remain largely static and are limited to navigation and view synthesis without embodied interactivity. To bridge this gap, we introduce Dexterous World Model (DWM), a scene-action-conditioned video diffusion framework that models how dexterous human actions induce dynamic changes in static 3D scenes.   Given a static 3D scene rendering and an egocentric hand motion sequence, DWM generates temporally coherent videos depicting plausible human-scene interactions. Our approach conditions video generation on (1) static scene renderings following a specified camera trajectory to ensure spatial consistency, and (2) egocentric hand mesh renderings that encode both geometry and motion cues to model action-conditioned dynamics directly. To train DWM, we construct a hybrid interaction video dataset. Synthetic egocentric interactions provide fully aligned supervision for joint locomotion and manipulation learning, while fixed-camera real-world videos contribute diverse and realistic object dynamics.   Experiments demonstrate that DWM enables realistic and physically plausible interactions, such as grasping, opening, and moving objects, while maintaining camera and scene consistency. This framework represents a first step toward video diffusion-based interactive digital twins and enables embodied simulation from egocentric actions.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为一名计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行深入分析。</p>
<p><strong>论文摘要分析：Dexterous World Models</strong></p>
<p><strong>1. 论文的主要贡献（2-3句话）</strong></p>
<p>该论文提出了 Dexterous World Model (DWM)，一个创新的场景-动作条件视频扩散框架，能够模拟人类灵巧动作如何引起静态三维场景的动态变化。DWM 能够根据静态三维场景渲染和以自我为中心的（egocentric）手部运动序列，生成时间连贯且具有物理合理性的人体与场景交互视频。这标志着在构建具有交互能力的数字孪生方面迈出了重要一步，实现了从自我中心动作出发的具身模拟。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<p>DWM 的核心创新在于其<strong>场景-动作条件视频扩散模型</strong>的设计，以及如何有效地将<strong>空间一致性</strong>和<strong>动作动态性</strong>融入视频生成过程。具体来说：</p>
<ul>
<li><strong>双重条件生成：</strong> DWM 巧妙地将视频生成条件化为两个关键部分：<ul>
<li><strong>静态场景渲染（遵循指定相机轨迹）：</strong> 这确保了生成的视频在空间上保持一致性，即场景的几何结构和相机视角不会随意变化，从而模拟了真实世界中观察者在固定场景中的视角。</li>
<li><strong>以自我为中心的手部网格渲染（编码几何和运动）：</strong> 这是实现“灵巧交互”的关键。通过直接输入手部网格的几何形状和运动信息，模型能够直接学习和预测手部动作如何影响场景中的物体，从而捕捉到精细的操纵动态。</li>
</ul>
</li>
<li><strong>混合交互视频数据集：</strong> 为了训练如此复杂的模型，作者构建了一个创新的混合数据集。<ul>
<li><strong>合成的以自我为中心的交互：</strong> 提供完全对齐的监督信号，用于联合学习身体运动（locomotion）和物体操纵（manipulation）。这使得模型能够学习到从手部动作到物体响应的精确映射。</li>
<li><strong>固定摄像机的真实世界视频：</strong> 引入了多样性和真实感，捕捉了现实世界中物体动力学的复杂性，弥补了纯合成数据的不足。</li>
</ul>
</li>
<li><strong>视频扩散框架：</strong> 利用了视频扩散模型强大的生成能力，能够生成高质量、时间连贯的视频序列，并且能够学习到复杂的概率分布，从而生成逼真的交互效果。</li>
</ul>
<p><strong>3. 对该领域的潜在影响</strong></p>
<p>DWM 的研究对计算机视觉领域具有重要的潜在影响，主要体现在：</p>
<ul>
<li><strong>推动具身智能（Embodied AI）的发展：</strong> 该框架为构建能够理解和执行复杂交互的具身智能体提供了新的途径。通过模拟人类的灵巧动作，可以训练出更具适应性和交互能力的机器人或虚拟代理。</li>
<li><strong>提升数字孪生的交互性：</strong> 现有的数字孪生多为静态模型，DWM 的出现使得数字孪生能够具备动态的、可交互的属性，极大地增强了其在模拟、训练、设计等领域的应用价值。</li>
<li><strong>促进人机交互（Human-Computer Interaction）的研究：</strong> 通过生成逼真的人体与虚拟环境的交互视频，可以用于研究和评估新的交互方式，以及训练能够理解和响应人类意图的交互系统。</li>
<li><strong>为视频生成领域注入新的活力：</strong> 将具身交互的复杂性引入视频生成，为视频生成模型的设计和训练提供了新的挑战和方向，有望催生更强大、更通用的视频生成模型。</li>
<li><strong>加速物理模拟与视觉的融合：</strong> DWM 通过学习动作如何影响场景，实际上是在隐式地学习物理规律。这有助于弥合纯粹的视觉模型和需要物理理解的模拟之间的差距。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用</strong></p>
<ul>
<li><strong>机器人学：</strong> 训练机器人进行精细的抓取、操作和组装任务，尤其是在复杂或未知环境中。</li>
<li><strong>虚拟现实（VR）/增强现实（AR）：</strong> 创建更具沉浸感和交互性的虚拟环境，让用户能够以更自然的方式与虚拟物体互动。</li>
<li><strong>游戏开发：</strong> 生成更逼真、更具动态性的游戏场景和角色交互。</li>
<li><strong>影视特效：</strong> 自动化生成复杂的物理交互动画，降低制作成本。</li>
<li><strong>产品设计与原型制作：</strong> 在虚拟环境中模拟产品的使用过程，评估设计可行性。</li>
<li><strong>教育与培训：</strong> 创建交互式模拟环境，用于技能培训，例如外科手术模拟或设备操作培训。</li>
<li><strong>内容创作：</strong> 允许用户通过简单的手部动作生成复杂的场景交互视频。</li>
</ul>
<p><strong>5. 可从摘要推断的局限性</strong></p>
<p>尽管 DWM 取得了显著进展，但从摘要中仍可推断出一些潜在的局限性：</p>
<ul>
<li><strong>对输入数据的依赖性：</strong> 模型性能高度依赖于输入的静态场景渲染质量、相机轨迹的指定以及手部网格渲染的准确性。如果输入数据存在噪声或不准确，可能会影响生成视频的质量。</li>
<li><strong>计算成本：</strong> 视频扩散模型通常计算成本较高，训练和推理可能需要大量的计算资源。</li>
<li><strong>泛化能力：</strong> 虽然混合数据集旨在提高多样性，但模型在未见过的新颖场景、物体或动作上的泛化能力仍需进一步验证。摘要中提到“plausible human-scene interactions”，这表明模型可能更侧重于生成“看起来合理”的交互，而非严格的物理精确性。</li>
<li><strong>“灵巧”的定义和范围：</strong> 摘要中提到了“dexterous human actions”，但“灵巧”的定义和模型能够处理的动作复杂度范围可能有限。例如，非常精细的、需要多指协同的复杂操作可能仍是挑战。</li>
<li><strong>全局场景理解的深度：</strong> 模型主要关注手部动作与局部场景的交互，对于需要全局场景理解才能完成的复杂任务（例如，需要规划长距离移动和多步操作的任务）可能存在不足。</li>
<li><strong>真实世界复杂性的捕捉：</strong> 尽管引入了真实世界视频，但要完全捕捉所有现实世界中物体交互的细微之处（如材质、摩擦力、形变等）仍然是一个巨大的挑战。</li>
</ul>
<p>总而言之，Dexterous World Model (DWM) 是一项令人兴奋的研究，它通过将场景-动作条件视频扩散与以自我为中心的手部运动相结合，有效地解决了数字孪生缺乏交互性的问题。其创新的方法论和对具身智能及数字孪生领域的潜在影响使其成为计算机视觉领域值得关注的亮点。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To bridge this gap, we introduce Dexterous World Model (DWM), a scene-action-conditioned video diffusion framework that models how dexterous human actions induce dynamic changes in static 3D scenes.</li>
<li>Our approach conditions video generation on (1) static scene renderings following a specified camera trajectory to ensure spatial consistency, and (2) egocentric hand mesh renderings that encode both geometry and motion cues to model action-conditioned dynamics directly.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.17907v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.17907v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.17902v1'></a></p>
<h2 id="adversarial-robustness-of-vision-in-open-foundation-models"><a href="https://arxiv.org/abs/2512.17902v1">Adversarial Robustness of Vision in Open Foundation Models</a></h2>
<p><strong>Authors:</strong> Jonathon Fox, William J Buchanan, Pavlos Papadopoulos</p>
<p><strong>Published:</strong> 2025-12-19</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.CR</p>
<p><strong>Abstract:</strong></p>
<p>With the increase in deep learning, it becomes increasingly difficult to understand the model in which AI systems can identify objects. Thus, an adversary could aim to modify an image by adding unseen elements, which will confuse the AI in its recognition of an entity. This paper thus investigates the adversarial robustness of LLaVA-1.5-13B and Meta's Llama 3.2 Vision-8B-2. These are tested for untargeted PGD (Projected Gradient Descent) against the visual input modality, and empirically evaluated on the Visual Question Answering (VQA) v2 dataset subset. The results of these adversarial attacks are then quantified using the standard VQA accuracy metric. This evaluation is then compared with the accuracy degradation (accuracy drop) of LLaVA and Llama 3.2 Vision. A key finding is that Llama 3.2 Vision, despite a lower baseline accuracy in this setup, exhibited a smaller drop in performance under attack compared to LLaVA, particularly at higher perturbation levels. Overall, the findings confirm that the vision modality represents a viable attack vector for degrading the performance of contemporary open-weight VLMs, including Meta's Llama 3.2 Vision. Furthermore, they highlight that adversarial robustness does not necessarily correlate directly with standard benchmark performance and may be influenced by underlying architectural and training factors.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Adversarial Robustness of Vision in Open Foundation Models”的全面中文摘要：</p>
<p><strong>论文题目：</strong> Adversarial Robustness of Vision in Open Foundation Models (视觉在开放基础模型中的对抗鲁棒性)</p>
<p><strong>作者：</strong> Jonathan Fox, William J Buchanan, Pavlos Papadopoulos</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 主要问题/研究问题：</strong>
随着深度学习模型在识别物体方面的能力日益增强，理解这些模型的内部工作机制变得愈发困难。攻击者可以通过在图像中添加不易察觉的元素来欺骗AI，从而干扰其识别能力。本文旨在研究当前主流的开放权重视觉语言模型（VLMs）在视觉输入受到对抗性攻击时的鲁棒性。具体来说，研究关注的是LLaVA-1.5-13B和Meta的Llama 3.2 Vision-8B-2这两个模型，它们在视觉输入模态上，使用无目标投影梯度下降（PGD）攻击方法进行测试，并在视觉问答（VQA）v2数据集子集上进行实证评估。</p>
<p><strong>2. 关键创新/方法论贡献：</strong>
*   <strong>实证评估：</strong> 本文首次系统性地比较了两种重要的开放权重VLM（LLaVA和Llama 3.2 Vision）在视觉对抗攻击下的鲁棒性。
*   <strong>对抗攻击方法：</strong> 使用了无目标PGD攻击方法，该方法被认为是评估对抗鲁棒性的标准基准。
*   <strong>评估指标：</strong> 采用标准的VQA准确率作为评估指标，并通过比较模型在干净图像和对抗性图像上的准确率下降（accuracy drop）来量化鲁棒性。
*   <strong>模型对比：</strong> 深入分析了两种模型在不同架构（LLaVA的简单投影层 vs. Llama 3.2 Vision的交叉注意力适配器）和训练规模上的差异如何影响其对抗鲁棒性。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>普遍脆弱性：</strong> 研究发现，LLaVA和Llama 3.2 Vision模型都对视觉输入的对抗性攻击表现出明显的脆弱性。即使是细微的、近乎不可察觉的扰动（ε &lt; 16/255），也会导致VQA准确率的下降。
*   <strong>不同的鲁棒性表现：</strong> 尽管LLaVA在干净数据集上表现出更高的基线准确率（87.4%），但在对抗攻击下，其准确率下降幅度更大（最高可达36.0%）。相比之下，Llama 3.2 Vision虽然基线准确率较低（42.8%），但在对抗攻击下表现出更小的准确率下降（最高10.4%），尤其是在高扰动水平下，其性能下降幅度相对较小。这表明Llama 3.2 Vision在一定程度上表现出更强的相对鲁棒性。
*   <strong>架构与训练的影响：</strong> 研究推测，Llama 3.2 Vision更复杂的交叉注意力适配器机制、更大的预训练数据集以及可能更先进的对齐过程，可能有助于其形成更稳定的内部表示，从而在对抗攻击下表现出更好的稳定性。
*   <strong>重要发现：</strong>
    *   视觉模态是降级当前开放权重VLM性能的可行攻击途径。
    *   对抗鲁棒性并不总是与标准的基准性能直接相关。
    *   模型架构和训练因素可能对对抗鲁棒性产生重要影响。</p>
<p><strong>4. 论文中提到的局限性：</strong>
*   <strong>数据集规模：</strong> 由于计算资源的限制，实验仅在VQA v2数据集的500个样本子集上进行，这可能无法完全代表模型在完整数据集或其他数据集上的表现。
*   <strong>攻击方法限制：</strong> 研究仅使用了无目标PGD攻击和L∞范数约束。其他攻击算法（如CW攻击）或范数约束（L2, L∞）可能揭示不同的脆弱性。
*   <strong>攻击目标：</strong> 攻击是无目标的，旨在降低整体性能，而非诱导特定的错误输出。有针对性的攻击可能带来不同的挑战。
*   <strong>任务范围：</strong> 鲁棒性仅在VQA任务上进行了评估，在图像描述或复杂推理等其他多模态任务上的表现可能有所不同。
*   <strong>超参数探索：</strong> 计算成本限制了对PGD参数空间（迭代次数、步长）的详尽探索。</p>
<p><strong>5. 未来研究方向：</strong>
*   <strong>更全面的评估：</strong> 在更大的数据集子集或完整的VQA v2数据集上重复实验，并扩展到其他多模态基准测试。
*   <strong>多样化的攻击方法：</strong> 探索更广泛的攻击算法，包括Carlini &amp; Wagner (CW) 攻击、Image Hijacks等，以及有针对性的攻击。
*   <strong>性能差异分析：</strong> 深入探究Llama 3.2 Vision在VQA子集上基线性能较低的原因，可能涉及不同的提示配置或预处理步骤。
*   <strong>架构与训练的深入分析：</strong> 详细分析多模态适配器的设计、预训练数据规模、对齐技术（如RLHF）等因素如何量化地影响对抗鲁棒性。
*   <strong>防御机制研究：</strong> 开发和评估专门针对VLM的有效防御策略，以减轻视觉对抗攻击的风险。
*   <strong>原生多模态 vs. 适配器方法：</strong> 进一步研究原生多模态架构与适配器方法的区别，以及它们对模型脆弱性的影响。</p>
<p><strong>总结：</strong>
这篇论文通过对LLaVA-1.5-13B和Llama 3.2 Vision-8B-2这两个重要的开放权重视觉语言模型进行实证评估，揭示了视觉模态在对抗性攻击下的普遍脆弱性。研究结果表明，对抗鲁棒性并非总是与基线性能成正比，并且模型架构和训练策略在其中扮演着关键角色。这项研究为理解和提升当前和未来多模态基础模型的安全性提供了重要的见解。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Overall, the findings confirm that the vision modality represents a viable attack vector for degrading the performance of contemporary open-weight VLMs, including Meta's Llama 3.2 Vision.</li>
<li>Furthermore, they highlight that adversarial robustness does not necessarily correlate directly with standard benchmark performance and may be influenced by underlying architectural and training factors.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.17902v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.17902v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.17897v1'></a></p>
<h2 id="radargen-automotive-radar-point-cloud-generation-from-cameras"><a href="https://arxiv.org/abs/2512.17897v1">RadarGen: Automotive Radar Point Cloud Generation from Cameras</a></h2>
<p><strong>Authors:</strong> Tomer Borreda, Fangqiang Ding, Sanja Fidler, Shengyu Huang, Or Litany</p>
<p><strong>Published:</strong> 2025-12-19</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.LG, cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>We present RadarGen, a diffusion model for synthesizing realistic automotive radar point clouds from multi-view camera imagery. RadarGen adapts efficient image-latent diffusion to the radar domain by representing radar measurements in bird's-eye-view form that encodes spatial structure together with radar cross section (RCS) and Doppler attributes. A lightweight recovery step reconstructs point clouds from the generated maps. To better align generation with the visual scene, RadarGen incorporates BEV-aligned depth, semantic, and motion cues extracted from pretrained foundation models, which guide the stochastic generation process toward physically plausible radar patterns. Conditioning on images makes the approach broadly compatible, in principle, with existing visual datasets and simulation frameworks, offering a scalable direction for multimodal generative simulation. Evaluations on large-scale driving data show that RadarGen captures characteristic radar measurement distributions and reduces the gap to perception models trained on real data, marking a step toward unified generative simulation across sensing modalities.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行深入分析。</p>
<p><strong>论文摘要分析：RadarGen: Automotive Radar Point Cloud Generation from Cameras</strong></p>
<p><strong>1. 论文的主要贡献 (2-3句话的简洁总结)</strong></p>
<p>本论文提出了 RadarGen，一个创新的扩散模型，能够从多视角摄像头图像生成逼真的汽车雷达点云。该模型通过将雷达测量值表示为鸟瞰图（BEV）形式，并结合视觉线索进行引导，实现了从视觉数据到雷达数据的跨模态生成，为多模态生成式仿真提供了一个可扩展的方向。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<p>RadarGen 的核心创新在于其将<strong>扩散模型</strong>这一强大的生成技术成功应用于<strong>雷达点云的生成</strong>，并解决了跨模态生成中的关键挑战。具体来说，其方法论的关键点包括：</p>
<ul>
<li><strong>雷达数据在BEV空间的表示：</strong> 将雷达测量值（包括空间结构、雷达截面积 RCS 和多普勒信息）编码到鸟瞰图（BEV）表示中。这种表示方式能够有效地捕捉雷达数据的空间特性，并为扩散模型的处理奠定基础。</li>
<li><strong>高效的图像-潜在扩散模型适配：</strong> 将高效的图像领域扩散模型（Image-Latent Diffusion）适配到雷达领域。这意味着模型在潜在空间中进行扩散过程，从而提高了生成效率和质量。</li>
<li><strong>轻量级的点云恢复：</strong> 在生成BEV表示后，通过一个轻量级的恢复步骤将其转换回三维雷达点云。这保证了生成结果的可用性，可以直接用于下游任务。</li>
<li><strong>多模态线索的融合与引导：</strong> 这是 RadarGen 最具吸引力的创新点之一。模型利用预训练的<strong>基础模型（Foundation Models）</strong>提取的<strong>BEV对齐的深度、语义和运动信息</strong>来指导扩散过程。这些视觉线索能够确保生成的雷达模式在物理上是合理的，并与真实的视觉场景更加匹配。这种显式的跨模态对齐是实现高质量生成和仿真一致性的关键。</li>
<li><strong>条件生成（Conditioning on Images）：</strong> 模型以摄像头图像作为条件进行生成，这使得 RadarGen 能够<strong>原则上兼容现有的视觉数据集和仿真框架</strong>。这极大地降低了应用门槛，并为大规模多模态生成式仿真铺平了道路。</li>
</ul>
<p><strong>3. 对该领域的潜在影响</strong></p>
<p>RadarGen 的研究对计算机视觉和自动驾驶领域具有重要的潜在影响：</p>
<ul>
<li><strong>推动多模态生成式仿真：</strong> 长期以来，为自动驾驶系统生成逼真的传感器数据一直是研究的热点。RadarGen 的工作为<strong>统一的多模态生成式仿真</strong>提供了一个可行的路径，能够同时生成摄像头图像和雷达点云，从而实现更真实、更全面的训练和测试环境。</li>
<li><strong>缓解真实数据稀缺问题：</strong> 尤其是在特定场景或极端天气条件下，获取高质量的真实雷达数据可能非常困难。RadarGen 的生成能力可以<strong>合成大量多样化的雷达数据</strong>，用于扩充训练集，提高模型的鲁棒性。</li>
<li><strong>加速传感器融合研究：</strong> 通过生成与视觉信息高度对齐的雷达数据，RadarGen 可以为研究<strong>更先进的传感器融合算法</strong>提供高质量的合成数据，从而加速相关研究的进展。</li>
<li><strong>降低数据采集和标注成本：</strong> 相较于手动采集和标注真实传感器数据，生成式方法可以显著<strong>降低数据相关的成本</strong>。</li>
<li><strong>为雷达感知模型提供更优的训练数据：</strong> 摘要中提到，RadarGen 生成的数据能够<strong>减小与真实数据训练的感知模型的差距</strong>，这意味着使用合成数据训练的模型在真实世界中的表现会更好。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用</strong></p>
<ul>
<li><strong>自动驾驶（Autonomous Driving）：</strong> 这是最直接的应用领域。RadarGen 可以用于生成训练数据、测试场景、以及开发和验证雷达感知算法、传感器融合算法、以及端到端的自动驾驶系统。</li>
<li><strong>机器人技术（Robotics）：</strong> 机器人也常常需要感知周围环境，雷达是一种重要的传感器。RadarGen 可以为机器人提供逼真的雷达感知数据，用于导航、避障、目标跟踪等任务的训练和仿真。</li>
<li><strong>计算机视觉（Computer Vision）：</strong> 尽管主要关注雷达，但其跨模态生成能力也可能对其他需要从视觉生成其他模态数据的任务产生启发。</li>
<li><strong>虚拟现实/增强现实（VR/AR）：</strong> 在构建逼真的虚拟环境时，模拟各种传感器数据是重要的组成部分。RadarGen 可以为VR/AR应用提供逼真的雷达模拟。</li>
<li><strong>遥感（Remote Sensing）：</strong> 某些遥感应用也可能使用雷达技术，生成式方法可以用于模拟不同的地物和环境下的雷达回波。</li>
</ul>
<p><strong>5. 可从摘要推断的局限性</strong></p>
<p>尽管 RadarGen 展现了巨大的潜力，但从摘要中可以推断出一些潜在的局限性：</p>
<ul>
<li><strong>对基础模型的依赖：</strong> 模型依赖于预训练的基础模型来提取视觉线索。如果基础模型的性能不佳或在特定场景下失效，可能会影响 RadarGen 的生成质量。</li>
<li><strong>“物理上可信”的定义：</strong> 摘要中提到“物理上可信的雷达模式”。虽然这是目标，但“可信”的程度和如何量化其物理准确性可能是一个挑战。生成的雷达点云是否能完全捕捉到所有细微的物理现象（如多径效应、杂波等）仍需进一步验证。</li>
<li><strong>计算成本：</strong> 扩散模型通常计算成本较高，尽管摘要提到了“高效的图像-潜在扩散”，但生成大规模、高分辨率的雷达点云可能仍然需要大量的计算资源。</li>
<li><strong>泛化能力：</strong> 虽然模型兼容现有数据集，但其在<strong>未见过</strong>的极端场景、天气条件或传感器配置下的泛化能力仍需在实际评估中验证。</li>
<li><strong>雷达特有的复杂性：</strong> 雷达数据具有其独特的物理特性，例如角度分辨率、距离分辨率、以及不同目标对雷达信号的散射特性。RadarGen 在多大程度上能够精确模拟这些复杂性有待进一步研究。</li>
<li><strong>“减小差距”的程度：</strong> 摘要提到“减小了与感知模型训练在真实数据的差距”。这表明差距仍然存在，合成数据可能还不能完全替代真实数据，尤其是在对模型性能要求极高的场景下。</li>
</ul>
<p>总而言之，RadarGen 是一项令人兴奋的研究，它将先进的生成模型与跨模态融合技术相结合，为解决自动驾驶领域中雷达数据生成和仿真的一大难题提供了新的思路和强大的工具。其对视觉线索的巧妙利用是实现逼真雷达点云生成的关键。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present RadarGen, a diffusion model for synthesizing realistic automotive radar point clouds from multi-view camera imagery.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.17897v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.17897v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.17891v1'></a></p>
<h2 id="keypoint-counting-classifiers-turning-vision-transformers-into-self-explainable-models-without-training"><a href="https://arxiv.org/abs/2512.17891v1">Keypoint Counting Classifiers: Turning Vision Transformers into Self-Explainable Models Without Training</a></h2>
<p><strong>Authors:</strong> Kristoffer Wickstrøm, Teresa Dorszewski, Siyan Chen, Michael Kampffmeyer, Elisabeth Wetzer, Robert Jenssen</p>
<p><strong>Published:</strong> 2025-12-19</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Current approaches for designing self-explainable models (SEMs) require complicated training procedures and specific architectures which makes them impractical. With the advance of general purpose foundation models based on Vision Transformers (ViTs), this impracticability becomes even more problematic. Therefore, new methods are necessary to provide transparency and reliability to ViT-based foundation models. In this work, we present a new method for turning any well-trained ViT-based model into a SEM without retraining, which we call Keypoint Counting Classifiers (KCCs). Recent works have shown that ViTs can automatically identify matching keypoints between images with high precision, and we build on these results to create an easily interpretable decision process that is inherently visualizable in the input. We perform an extensive evaluation which show that KCCs improve the human-machine communication compared to recent baselines. We believe that KCCs constitute an important step towards making ViT-based foundation models more transparent and reliable.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Keypoint Counting Classifiers: Turning Vision Transformers into Self-Explainable Models Without Training”的全面中文摘要：</p>
<p><strong>论文题目：</strong> Keypoint Counting Classifiers: Turning Vision Transformers into Self-Explainable Models Without Training (关键点计数分类器：将 Vision Transformers 转化为无需训练的自解释模型)</p>
<p><strong>作者：</strong> Kristoffer Wickstrøm, Teresa Dorszewski, Siyan Chen, Michael Kampffmeyer, Elisabeth Wetzer, Robert Jenssen</p>
<hr />
<p><strong>摘要</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong></p>
<p>当前设计自解释模型（SEMs）的方法通常需要复杂的训练流程和特定的模型架构，这使得它们在实际应用中难以推广，尤其是在日益重要的 Vision Transformer (ViT) 基础模型领域。ViT 的通用性和强大的表征能力带来了便利，但其缺乏透明度和可解释性限制了其在安全关键领域的应用。现有的 SEMs 方法往往与 ViT 架构不兼容，或者需要额外的训练，这削弱了 ViT 的灵活性。此外，现有的 SEMs 可视化解释方法（如边界框和热力图）存在精度不足、信息量少、用户研究表现不佳等问题，阻碍了人机之间的有效沟通。因此，研究如何为 ViT 基础模型提供透明度和可靠性，并改进解释的可视化方式，是本文要解决的核心问题。</p>
<p><strong>2. 主要创新点/方法论贡献：</strong></p>
<p>本文提出了一种名为<strong>关键点计数分类器（Keypoint Counting Classifiers, KCCs）</strong>的新方法，旨在将任何已训练好的 ViT 模型转化为自解释模型（SEM），且<strong>无需进行任何额外的训练</strong>。KCCs 的核心创新在于：</p>
<ul>
<li><strong>无需训练的 ViT 解释：</strong> KCCs 利用 ViT 本身在识别图像语义部分（keypoints）方面的能力，通过比较查询图像与原型图像的关键点匹配情况来进行分类。整个过程不涉及对 ViT 特征提取器或分类头的任何修改或再训练，从而保留了 ViT 的灵活性。</li>
<li><strong>基于关键点的可视化解释：</strong> KCCs 将解释可视化为匹配的关键点，这是一种新颖的可视化方法。这种方法受到教学材料和视觉学习中常用关键点表示的启发，旨在提供更直观、更易于理解的解释，避免了边界框和热力图的局限性。</li>
<li><strong>关键点识别与匹配：</strong><ul>
<li><strong>关键点识别：</strong> KCCs 首先利用 ViT 的 token 能够捕捉语义部分信息的特性，结合前景分割和 SLIC 超像素分割等技术，将图像分割成语义上连贯的区域，并将每个区域的中心定义为一个关键点。</li>
<li><strong>关键点匹配：</strong> 利用互斥最近邻（Mutual NNs）的方法，比较查询图像和原型图像的关键点表示，找出相互匹配的关键点对。</li>
</ul>
</li>
<li><strong>基于计数的分类：</strong> 最终的分类决策是通过计算查询图像与属于某个类别的原型图像之间匹配到的关键点数量来完成的。</li>
<li><strong>利用 Vision-Language 模型增强解释：</strong> 在特定情况下，当 ViT 具备视觉-语言能力时，KCCs 可以利用这些能力自动为关键点生成文本描述，从而进一步减少读者的解释偏差，提高解释的清晰度。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong></p>
<ul>
<li><strong>用户研究结果：</strong> 用户研究表明，KCCs 在解释质量和理解性方面显著优于现有的基线方法（PIP-Net 和 KMEx）。用户对 KCCs 的解释质量评分最高，并且更容易理解。在用户偏好方面，KCCs 与 KMEx 并列获得最高评分，表明 KCCs 在提供高质量解释的同时，也具有良好的用户友好性。更重要的是，KCCs 使得用户在纠正模型预测时更有信心，有助于缓解自动化偏差。</li>
<li><strong>定量评估结果：</strong> 在 CUB200、CARS 和 PETS 等数据集上的定量评估显示，KCCs 在准确性和复杂度方面与许多需要额外训练的 SEMs 方法相比具有竞争力，甚至在某些情况下表现更优（例如，在 CUB200 数据集上，KCCs 的准确率高于未训练的 ProtoPNet）。这证明了无需训练的方法也能达到良好的性能。</li>
<li><strong>减少读者偏差的潜力：</strong> 通过结合视觉-语言模型，KCCs 能够为关键点提供自动文本描述，这是一种减少解释偏差的有效途径，使得解释更加明确和客观。</li>
<li><strong>意义：</strong> KCCs 为 ViT 基础模型提供了一种新颖、灵活且无需训练的自解释方法，显著提升了模型的可解释性和人机沟通效率。它为构建更透明、更可靠的深度学习模型开辟了新的方向。</li>
</ul>
<p><strong>4. 提及的局限性：</strong></p>
<ul>
<li><strong>关键点权重问题：</strong> KCCs 目前对所有关键点赋予相同的权重，但实际上某些关键点对特定类别的区分度可能更高（例如，鸟类的喙部形状）。如何为关键点引入更具信息量的权重是一个潜在的研究方向。</li>
<li><strong>计算复杂度：</strong> 当原型数量非常多时，计算关键点之间的相似性可能会变得内存密集。虽然论文提出了一种优化策略（仅考虑与最近的 J 个原型相关的 token），但计算成本仍需考虑。</li>
<li><strong>对 fine-grained 分类的挑战：</strong> 在高度细粒度的分类任务中，监督信号仍然是必要的，这表明 KCCs 在这些场景下可能需要与监督学习方法结合。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>引入关键点权重：</strong> 研究如何为关键点引入自适应的权重，以提高分类性能和解释的准确性。</li>
<li><strong>识别类特定关键点：</strong> 探索如何识别并利用类特定的关键点，以减少冗余并增强解释的特异性。</li>
<li><strong>更广泛的 ViT 模型和任务应用：</strong> 将 KCCs 应用于更多不同类型的 ViT 模型和更广泛的计算机视觉任务。</li>
<li><strong>结合其他 XAI 技术：</strong> 探索 KCCs 与其他可解释性技术（如注意力机制分析）的结合，以提供更全面的解释。</li>
<li><strong>进一步减少读者偏差：</strong> 探索更多利用视觉-语言模型的方法，以实现更鲁棒和更少偏差的自动文本解释。</li>
</ul>
<p>总而言之，这篇论文提出了一种创新的 KCCs 方法，成功地将强大的 ViT 模型转化为无需训练的自解释模型，并通过直观的关键点可视化和自动文本描述，显著提升了模型的可解释性和用户体验，为 XAI 领域带来了重要的贡献。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Therefore, new methods are necessary to provide transparency and reliability to ViT-based foundation models.</li>
<li>In this work, we present a new method for turning any well-trained ViT-based model into a SEM without retraining, which we call Keypoint Counting Classifiers (KCCs).</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.17891v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.17891v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.17875v1'></a></p>
<h2 id="visually-prompted-benchmarks-are-surprisingly-fragile"><a href="https://arxiv.org/abs/2512.17875v1">Visually Prompted Benchmarks Are Surprisingly Fragile</a></h2>
<p><strong>Authors:</strong> Haiwen Feng, Long Lian, Lisa Dunlap, Jiahao Shu, XuDong Wang, Renhao Wang, Trevor Darrell, Alane Suhr, Angjoo Kanazawa</p>
<p><strong>Published:</strong> 2025-12-19</p>
<p><strong>Categories:</strong> cs.CV, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>A key challenge in evaluating VLMs is testing models' ability to analyze visual content independently from their textual priors. Recent benchmarks such as BLINK probe visual perception through visual prompting, where questions about visual content are paired with coordinates to which the question refers, with the coordinates explicitly marked in the image itself. While these benchmarks are an important part of VLM evaluation, we find that existing models are surprisingly fragile to seemingly irrelevant details of visual prompting: simply changing a visual marker from red to blue can completely change rankings among models on a leaderboard. By evaluating nine commonly-used open- and closed-source VLMs on two visually prompted tasks, we demonstrate how details in benchmark setup, including visual marker design and dataset size, have a significant influence on model performance and leaderboard rankings. These effects can even be exploited to lift weaker models above stronger ones; for instance, slightly increasing the size of the visual marker results in open-source InternVL3-8B ranking alongside or better than much larger proprietary models like Gemini 2.5 Pro. We further show that low-level inference choices that are often ignored in benchmarking, such as JPEG compression levels in API calls, can also cause model lineup changes. These details have substantially larger impacts on visually prompted benchmarks than on conventional semantic VLM evaluations. To mitigate this instability, we curate existing datasets to create VPBench, a larger visually prompted benchmark with 16 visual marker variants. VPBench and additional analysis tools are released at https://lisadunlap.github.io/vpbench/.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Visually Prompted Benchmarks Are Surprisingly Fragile”的全面中文摘要：</p>
<p><strong>论文题目：</strong> Visually Prompted Benchmarks Are Surprisingly Fragile (视觉提示基准测试出奇地脆弱)</p>
<p><strong>作者：</strong> Haiwen Feng, Long Lian, Lisa Dunlap, Jiahao Shu, XuDong Wang, Renhao Wang, Trevor Darrell, Alane Suhr, Angjoo Kanazawa</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 主要问题/研究问题：</strong></p>
<p>该论文的核心研究问题在于，当前用于评估视觉语言模型（VLMs）的视觉提示（visually prompted）基准测试，在多大程度上受到非语义性因素（如视觉标记的设计、数据集大小、甚至低级推理设置）的影响，以及这些因素如何导致模型性能和排行榜的<strong>脆弱性</strong>。研究旨在揭示这些基准测试在多大程度上衡量的是模型的真实视觉感知能力，而非其对基准测试设计细节的敏感度。</p>
<p><strong>2. 关键创新/方法贡献：</strong></p>
<ul>
<li><strong>系统性地揭示视觉提示基准的脆弱性：</strong> 作者通过对九个常用开源和闭源 VLM 进行实验，系统性地展示了视觉提示的细微变化（如标记颜色、大小、形状、位置）如何显著影响模型性能和排行榜。</li>
<li><strong>引入 VPBench 基准测试：</strong> 为了解决现有基准测试的局限性，作者从现有大型数据集中（DA2K 和 SPair-71k）构建了一个更大、更具多样性的视觉提示基准测试——VPBench。VPBench 包含 16 种不同的视觉标记变体，旨在提供更稳定、更可靠的评估。</li>
<li><strong>量化不同因素的影响：</strong> 论文量化了数据采样、视觉标记设计和低级推理设置（如 JPEG 压缩）对模型性能和排行榜的影响，并将其与传统语义基准测试进行了对比。</li>
<li><strong>提出缓解策略：</strong> 作者提出了一系列建议，以提高视觉提示基准测试的稳健性，包括标准化和多样化视觉提示、使用一致的数据源和低级设置、报告不确定性和排行榜稳定性等。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong></p>
<ul>
<li><strong>视觉提示的细微变化可导致排行榜剧烈变动：</strong> 研究发现，即使是简单的视觉标记颜色从红色变为蓝色，也可能完全改变模型在排行榜上的排名。这种敏感性远超传统语义基准测试。</li>
<li><strong>弱模型可被“操纵”以超越强模型：</strong> 通过精心选择视觉标记（例如，将标记改为方形），较弱的模型（如 InternVL3-8B）可以超越更强大的专有模型（如 Gemini 2.5 Pro）。</li>
<li><strong>低级推理设置影响显著：</strong> JPEG 压缩等通常被忽略的低级推理设置，也会对视觉提示基准测试产生显著影响，导致模型排名变化。</li>
<li><strong>VPBench 提供了更稳定的评估：</strong> VPBench 基准测试通过增加数据量和提供多种标记变体，显著降低了评估的方差，使得模型性能差异更容易被区分，排行榜也更加稳定。</li>
<li><strong>意义：</strong> 这些发现表明，当前视觉提示基准测试的有效性受到严重质疑，它们可能更多地反映了基准测试本身的“设计缺陷”而非模型的真实能力。这迫切需要改进评估方法，以确保对 VLM 视觉感知能力的公平和准确评估。</li>
</ul>
<p><strong>4. 提及的局限性：</strong></p>
<ul>
<li><strong>VPBench 的局限性：</strong> 虽然 VPBench 旨在提高稳定性，但目前它主要集中在相对深度和语义对应任务上，可能无法完全代表所有类型的视觉提示任务。</li>
<li><strong>模型对特定提示的过拟合：</strong> 研究暗示，一些模型可能对特定视觉提示（如 BLINK 默认的红色圆圈标记）存在过拟合现象，导致其在其他标记下性能下降。</li>
<li><strong>评估的复杂性：</strong> 即使在 VPBench 中，要完全消除所有不确定性仍然具有挑战性，尤其是在模型性能接近的情况下。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>开发更鲁棒的 VLM 评估框架：</strong> 基于本研究的发现，未来需要开发更全面的评估框架，能够抵御非语义性因素的干扰，并提供更可靠的模型能力衡量。</li>
<li><strong>研究模型对视觉提示的内在敏感性：</strong> 深入探究不同 VLM 模型为何对特定的视觉提示设计表现出不同的敏感性，这有助于理解模型的内部机制和潜在的偏见。</li>
<li><strong>探索更多类型的视觉提示任务：</strong> 将本研究的发现推广到更广泛的视觉提示任务中，以验证这种脆弱性是否普遍存在。</li>
<li><strong>开发对抗性鲁棒性评估：</strong> 进一步研究如何通过操纵视觉提示来“欺骗”模型，并开发相应的对抗性鲁棒性评估方法。</li>
<li><strong>标准化视觉提示设计：</strong> 推动社区在设计视觉提示基准测试时，采用更统一和标准化的方法，以减少评估的不确定性。</li>
</ul>
<p>总而言之，这篇论文通过揭示视觉提示基准测试的脆弱性，对当前 VLM 评估方法提出了严峻的挑战，并为未来开发更可靠、更具信息量的评估工具和方法奠定了基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>By evaluating nine commonly-used open- and closed-source VLMs on two visually prompted tasks, we demonstrate how details in benchmark setup, including visual marker design and dataset size, have a significant influence on model performance and leaderboard rankings.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.17875v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.17875v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.17873v1'></a></p>
<h2 id="inspect-invariant-spectral-features-preservation-of-diffusion-models"><a href="https://arxiv.org/abs/2512.17873v1">InSPECT: Invariant Spectral Features Preservation of Diffusion Models</a></h2>
<p><strong>Authors:</strong> Baohua Yan, Qingyuan Liu, Jennifer Kava, Xuan Di</p>
<p><strong>Published:</strong> 2025-12-19</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Modern diffusion models (DMs) have achieved state-of-the-art image generation. However, the fundamental design choice of diffusing data all the way to white noise and then reconstructing it leads to an extremely difficult and computationally intractable prediction task. To overcome this limitation, we propose InSPECT (Invariant Spectral Feature-Preserving Diffusion Model), a novel diffusion model that keeps invariant spectral features during both the forward and backward processes. At the end of the forward process, the Fourier coefficients smoothly converge to a specified random noise, enabling features preservation while maintaining diversity and randomness. By preserving invariant features, InSPECT demonstrates enhanced visual diversity, faster convergence rate, and a smoother diffusion process. Experiments on CIFAR-10, Celeb-A, and LSUN demonstrate that InSPECT achieves on average a 39.23% reduction in FID and 45.80% improvement in IS against DDPM for 10K iterations under specified parameter settings, which demonstrates the significant advantages of preserving invariant features: achieving superior generation quality and diversity, while enhancing computational efficiency and enabling faster convergence rate. To the best of our knowledge, this is the first attempt to analyze and preserve invariant spectral features in diffusion models.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为一名计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行深入分析。</p>
<p><strong>论文摘要分析：InSPECT: Invariant Spectral Features Preservation of Diffusion Models</strong></p>
<p><strong>1. 论文的主要贡献（2-3句话的简洁总结）</strong></p>
<p>本论文提出了一种名为 InSPECT 的新型扩散模型，其核心贡献在于在扩散模型的正向和反向过程中保持了不变的谱特征。通过确保傅里叶系数平滑地收敛到指定的随机噪声，InSPECT 在保留图像特征的同时，实现了更高的视觉多样性、更快的收敛速度和更平滑的扩散过程。实验结果表明，InSPECT 在生成质量和多样性方面显著优于现有方法，并提高了计算效率。</p>
<p><strong>2. 关键创新点或方法论</strong></p>
<p>InSPECT 的关键创新在于其<strong>不变谱特征保持（Invariant Spectral Features Preservation）</strong>的理念。传统的扩散模型（如 DDPM）将数据完全扩散到白噪声，这使得从噪声中重建原始数据成为一个极其困难且计算量巨大的任务。InSPECT 提出的方法打破了这一范式，通过在扩散过程中主动地保持数据的谱特征（例如，通过傅里叶变换得到的频率成分）不变。</p>
<p>具体来说，其方法论的核心在于：</p>
<ul>
<li><strong>谱特征的定义与保持：</strong> 论文明确提出要保持“不变的谱特征”。虽然摘要中没有详细说明具体是哪些谱特征，但可以推断是指在数据扩散过程中，某些关键的频率成分（如低频信息代表的整体结构、轮廓等）能够被保留下来，而不是像传统方法那样完全被噪声淹没。</li>
<li><strong>傅里叶系数的平滑收敛：</strong> 论文提到“傅里叶系数平滑地收敛到指定的随机噪声”。这意味着在正向扩散的末端，数据虽然变成了噪声，但其谱特征的“残余”或“模式”是以一种可控、平滑的方式融入到噪声中的，而不是完全随机地消失。反向过程则利用这些保留的谱特征来指导重建。</li>
<li><strong>正向与反向过程的统一：</strong> 这种谱特征的保持贯穿于正向和反向两个过程，使得模型在学习和生成时都能够利用这些稳定的特征信息。</li>
</ul>
<p><strong>3. 对该领域的潜在影响</strong></p>
<p>InSPECT 的提出可能对扩散模型领域产生深远影响，主要体现在以下几个方面：</p>
<ul>
<li><strong>提升生成质量与多样性：</strong> 通过保留关键的谱特征，模型能够更好地捕捉数据的本质结构和细节，从而生成更逼真、更多样化的图像。摘要中提到的 FID 和 IS 指标的显著提升（平均 FID 降低 39.23%，IS 提高 45.80%）直接证明了这一点。</li>
<li><strong>提高计算效率与收敛速度：</strong> 传统扩散模型需要大量的迭代才能达到令人满意的生成效果。InSPECT 通过保持不变的谱特征，简化了预测任务的难度，从而实现了更快的收敛速度和更高的计算效率。这对于实际应用中部署大型扩散模型至关重要。</li>
<li><strong>新的研究方向：</strong> 论文声称这是“首次尝试分析和保留不变谱特征在扩散模型中”。这为扩散模型的研究开辟了一个新的视角，鼓励研究人员探索其他类型的“不变特征”或“结构化噪声”在扩散模型中的应用。</li>
<li><strong>理论理解的深化：</strong> 这种对谱特征的关注可能有助于更深入地理解扩散模型的内部工作机制，以及数据在扩散过程中的信息丢失与保留机制。</li>
</ul>
<p><strong>4. 可能受益于该研究的相关领域或应用</strong></p>
<ul>
<li><strong>图像生成与编辑：</strong> 任何需要高质量、高多样性图像生成的应用，如艺术创作、虚拟现实内容生成、游戏资产制作等。</li>
<li><strong>图像超分辨率与修复：</strong> 保留关键的谱特征有助于在低分辨率图像或损坏图像中恢复细节，从而提升超分辨率和修复的效果。</li>
<li><strong>数据增强：</strong> InSPECT 的生成能力可以用于生成更多样化的训练数据，以提高其他下游任务模型的鲁棒性。</li>
<li><strong>医学影像分析：</strong> 在医学影像领域，保留关键的解剖结构信息（可能与谱特征相关）对于诊断和分析至关重要。</li>
<li><strong>科学模拟与可视化：</strong> 在需要生成复杂物理现象或科学数据的场景下，保留关键的模式和结构信息将非常有益。</li>
<li><strong>视频生成：</strong> 将谱特征保持的思想扩展到视频领域，可能有助于生成更连贯、更真实的视频序列。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性</strong></p>
<p>尽管摘要展示了显著的优势，但仍可以推断出一些潜在的局限性：</p>
<ul>
<li><strong>谱特征的具体定义与选择：</strong> 摘要中并未详细说明“不变谱特征”具体是指哪些频率成分，以及如何选择和量化这些特征。不同的谱特征选择可能会对模型性能产生不同影响。</li>
<li><strong>计算成本的权衡：</strong> 虽然摘要声称提高了计算效率，但“保持不变谱特征”可能需要在正向和反向过程中引入额外的计算步骤（例如，进行傅里叶变换、特征提取和注入等），这可能在某些方面增加计算复杂度，需要仔细权衡。</li>
<li><strong>参数设置的敏感性：</strong> 摘要中提到“在指定参数设置下”。这暗示了模型的性能可能对参数设置比较敏感，需要精细的调优才能达到最佳效果。</li>
<li><strong>泛化能力：</strong> 摘要中列举了 CIFAR-10, Celeb-A, LSUN 等数据集，这些数据集在复杂度和领域上有所不同。论文需要进一步证明其方法在更广泛、更复杂的数据集上的泛化能力。</li>
<li><strong>理论证明的深度：</strong> 摘要强调了“首次尝试分析”，但具体的理论分析和数学证明可能还需要在论文正文中详细阐述，以支持其方法的有效性。</li>
<li><strong>“平滑收敛”的实现细节：</strong> 如何实现傅里叶系数的“平滑收敛”到随机噪声，以及这种平滑性如何被反向过程有效利用，是实现其性能的关键，其具体实现细节可能需要进一步研究。</li>
</ul>
<p>总而言之，InSPECT 提出的不变谱特征保持理念为扩散模型的研究提供了一个令人兴奋的新方向，有望在生成质量、效率和多样性方面带来显著的突破。其对谱特征的关注，以及在正向和反向过程中保持这些特征的创新方法，使其成为计算机视觉领域一项值得关注的研究。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Modern diffusion models (DMs) have achieved state-of-the-art image generation.</li>
<li>To overcome this limitation, we propose InSPECT (Invariant Spectral Feature-Preserving Diffusion Model), a novel diffusion model that keeps invariant spectral features during both the forward and backward processes.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.17873v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.17873v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.17853v1'></a></p>
<h2 id="anytask-an-automated-task-and-data-generation-framework-for-advancing-sim-to-real-policy-learning"><a href="https://arxiv.org/abs/2512.17853v1">AnyTask: an Automated Task and Data Generation Framework for Advancing Sim-to-Real Policy Learning</a></h2>
<p><strong>Authors:</strong> Ran Gong, Xiaohan Zhang, Jinghuan Shang, Maria Vittoria Minniti, Jigarkumar Patel, Valerio Pepe, Riedana Yan, Ahmet Gundogdu, Ivan Kapelyukh, Ali Abbas, Xiaoqiang Yan, Harsh Patel, Laura Herlant, Karl Schmeckpeper</p>
<p><strong>Published:</strong> 2025-12-19</p>
<p><strong>Categories:</strong> cs.RO, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>Generalist robot learning remains constrained by data: large-scale, diverse, and high-quality interaction data are expensive to collect in the real world. While simulation has become a promising way for scaling up data collection, the related tasks, including simulation task design, task-aware scene generation, expert demonstration synthesis, and sim-to-real transfer, still demand substantial human effort. We present AnyTask, an automated framework that pairs massively parallel GPU simulation with foundation models to design diverse manipulation tasks and synthesize robot data. We introduce three AnyTask agents for generating expert demonstrations aiming to solve as many tasks as possible: 1) ViPR, a novel task and motion planning agent with VLM-in-the-loop Parallel Refinement; 2) ViPR-Eureka, a reinforcement learning agent with generated dense rewards and LLM-guided contact sampling; 3) ViPR-RL, a hybrid planning and learning approach that jointly produces high-quality demonstrations with only sparse rewards. We train behavior cloning policies on generated data, validate them in simulation, and deploy them directly on real robot hardware. The policies generalize to novel object poses, achieving 44% average success across a suite of real-world pick-and-place, drawer opening, contact-rich pushing, and long-horizon manipulation tasks. Our project website is at https://anytask.rai-inst.com .</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“AnyTask: an Automated Task and Data Generation Framework for Advancing Sim-to-Real Policy Learning”的全面中文摘要：</p>
<p><strong>论文题目：</strong> AnyTask: an Automated Task and Data Generation Framework for Advancing Sim-to-Real Policy Learning</p>
<p><strong>作者：</strong> Ran Gong, Xiaohan Zhang, Jinghuan Shang, Maria Vittoria Minniti, Jigarkumar Patel, Valerio Pepe, Riedana Yan, Ahmet Gundogdu, Ivan Kapelyukh, Ali Abbas, Xiaoqiang Yan, Harsh Patel, Laura Herlant, Karl Schmeckpeper</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
通用机器人学习面临数据瓶颈：大规模、多样化且高质量的真实世界交互数据收集成本高昂且耗时。尽管模拟环境是扩展数据收集的有力工具，但任务设计、场景生成、专家演示合成以及从模拟到现实的迁移等环节仍需大量人工干预，这限制了生成数据的多样性和规模。</p>
<p><strong>2. 主要创新点/方法论贡献：</strong>
本文提出了 <strong>AnyTask</strong>，一个自动化框架，旨在解决上述挑战。其核心创新在于：</p>
<ul>
<li><strong>端到端自动化数据生成流程：</strong> AnyTask 集成了大规模并行 GPU 模拟与基础模型（如大型语言模型 VLM/LLM），实现了从高层任务指令到机器人数据合成的整个流程的自动化，显著减少了人工干预。</li>
<li><strong>智能对象数据库与任务生成：</strong> 利用 VLM 自动为对象生成多视角、多部件的渲染以及详细的元数据，并基于此数据库，通过 LLM 自动生成多样化的机器人任务描述和场景配置。</li>
<li><strong>多样化的专家演示生成代理：</strong> 引入了三种代理来自动生成高质量的专家演示：<ul>
<li><strong>VIPR：</strong> 一个新颖的任务与运动规划 (TAMP) 代理，结合了 VLM 进行迭代式并行精炼。</li>
<li><strong>VIPR-EUREKA：</strong> 一个强化学习 (RL) 代理，利用 LLM 生成的密集奖励和 LLM 指导的接触采样。</li>
<li><strong>VIPR-RL：</strong> 一个混合规划与学习的代理，结合了 TAMP 和 RL 的优势，能够仅凭稀疏奖励生成高质量演示。</li>
</ul>
</li>
<li><strong>大规模并行模拟与数据合成：</strong> 利用大规模并行 GPU 模拟器（如 IsaacLab）高效生成大量模拟数据，并应用在线域随机化来增强场景和视觉观察的多样性。</li>
<li><strong>零样本模拟到现实迁移：</strong> 训练的策略可以直接部署到物理机器人上，无需真实世界数据进行微调。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>策略性能：</strong> 在真实世界中，训练在 AnyTask 生成数据上的行为克隆策略在多种任务（包括拾取与放置、开门、接触式推挤和长时序操作）上实现了 <strong>44% 的平均成功率</strong>。这些策略能够泛化到新颖的物体姿态。
*   <strong>数据生成效率：</strong> AnyTask 框架能够高效地生成大规模、多样化的数据，显著降低了数据收集的成本和人力投入。
*   <strong>代理能力：</strong> 三种代理（VIPR, VIPR-EUREKA, VIPR-RL）在不同类型的任务上展现出互补的能力，共同提高了整体任务解决能力。
*   <strong>模拟到现实的有效性：</strong> 实验证明，仅使用合成数据训练的策略可以成功迁移到物理机器人上，验证了 AnyTask 框架在弥合模拟与现实差距方面的有效性。</p>
<p><strong>4. 论文中提到的局限性：</strong>
*   <strong>高精度或复杂物理推理任务的性能：</strong> 在需要高精度或复杂物理推理的任务（如堆叠任意物体）上，代理的性能仍有待提高。
*   <strong>RGB 输入的局限性：</strong> 成功的模拟到现实迁移依赖于点云观测。将框架扩展到 RGB 输入的策略将降低真实世界部署的门槛。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>扩展对象和机器人形态：</strong> 增加更多样的对象和机器人形态，以提高框架的通用性。
*   <strong>更复杂的长时序移动操作任务：</strong> 将框架扩展到更复杂的、长时序的移动操作任务。
*   <strong>RGB 输入的策略：</strong> 研究使用 RGB 图像作为输入来训练策略，以降低对特定传感器（如点云）的依赖。
*   <strong>提升高精度和复杂物理推理能力：</strong> 进一步改进代理在复杂物理交互任务上的表现。</p>
<p><strong>对计算机视觉领域的意义：</strong>
AnyTask 的工作对计算机视觉领域具有重要意义，因为它：</p>
<ul>
<li><strong>推动了视觉-语言模型在机器人领域的应用：</strong> 论文展示了如何有效地利用 VLM/LLM 来理解自然语言指令，生成任务描述，并为机器人提供丰富的上下文信息，这为视觉-语言模型在机器人领域的更广泛应用奠定了基础。</li>
<li><strong>促进了大规模合成数据集的生成：</strong> 通过自动化任务设计和数据生成流程，AnyTask 为训练更强大、更通用的机器人策略提供了大规模、多样化的合成数据集，这对于克服真实世界数据收集的限制至关重要。</li>
<li><strong>提升了模拟到现实迁移的鲁棒性：</strong> 论文通过引入域随机化和点云增强策略，有效提升了模拟训练策略在真实世界中的泛化能力，这对于计算机视觉在机器人感知和控制中的应用具有重要价值。</li>
<li><strong>为机器人学习提供了新的范式：</strong> AnyTask 提供了一个端到端的框架，将 LLM 的语言理解能力与大规模模拟器的物理仿真能力相结合，为机器人学习提供了一种新的、更高效的范式。</li>
</ul>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present AnyTask, an automated framework that pairs massively parallel GPU simulation with foundation models to design diverse manipulation tasks and synthesize robot data.</li>
<li>We introduce three AnyTask agents for generating expert demonstrations aiming to solve as many tasks as possible: 1) ViPR, a novel task and motion planning agent with VLM-in-the-loop Parallel Refinement; 2) ViPR-Eureka, a reinforcement learning agent with generated dense rewards and LLM-guided contact sampling; 3) ViPR-RL, a hybrid planning and learning approach that jointly produces high-quality demonstrations with only sparse rewards.</li>
<li>The policies generalize to novel object poses, achieving 44% average success across a suite of real-world pick-and-place, drawer opening, contact-rich pushing, and long-horizon manipulation tasks.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.17853v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.17853v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.17851v1'></a></p>
<h2 id="infsplign-inference-time-spatial-alignment-of-text-to-image-diffusion-models"><a href="https://arxiv.org/abs/2512.17851v1">InfSplign: Inference-Time Spatial Alignment of Text-to-Image Diffusion Models</a></h2>
<p><strong>Authors:</strong> Sarah Rastegar, Violeta Chatalbasheva, Sieger Falkena, Anuj Singh, Yanbo Wang, Tejas Gokhale, Hamid Palangi, Hadi Jamali-Rad</p>
<p><strong>Published:</strong> 2025-12-19</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>Text-to-image (T2I) diffusion models generate high-quality images but often fail to capture the spatial relations specified in text prompts. This limitation can be traced to two factors: lack of fine-grained spatial supervision in training data and inability of text embeddings to encode spatial semantics. We introduce InfSplign, a training-free inference-time method that improves spatial alignment by adjusting the noise through a compound loss in every denoising step. Proposed loss leverages different levels of cross-attention maps extracted from the backbone decoder to enforce accurate object placement and a balanced object presence during sampling. The method is lightweight, plug-and-play, and compatible with any diffusion backbone. Our comprehensive evaluations on VISOR and T2I-CompBench show that InfSplign establishes a new state-of-the-art (to the best of our knowledge), achieving substantial performance gains over the strongest existing inference-time baselines and even outperforming the fine-tuning-based methods. Codebase is available at GitHub.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“InfSplign: Inference-Time Spatial Alignment of Text-to-Image Diffusion Models”的全面中文摘要：</p>
<p><strong>论文题目：</strong> InfSplign: 推理时文本到图像扩散模型的空间对齐</p>
<p><strong>作者：</strong> Sarah Rastegar, Violeta Chatalbasheva, Sieger Falkena, Anuj Singh, Yanbo Wang, Tejas Gokhale, Hamid Palangi, Hadi Jamali-Rad</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 主要问题/研究问题：</strong>
文本到图像（T2I）扩散模型虽然能够生成高质量、逼真的图像，但在准确捕捉文本提示中描述的空间关系方面存在显著不足。这种局限性主要源于两个方面：训练数据缺乏细粒度的空间监督，以及文本嵌入本身难以有效编码空间语义。这导致模型在生成图像时，物体的位置、相对关系（如“左边”、“右边”、“上面”、“下面”）经常出错，甚至完全忽略。</p>
<p><strong>2. 关键创新/方法贡献：</strong>
为了解决上述问题，作者提出了 <strong>InfSplign</strong>，一种<strong>无需训练、在推理时</strong>进行空间对齐的方法。其核心贡献在于：</p>
<ul>
<li><strong>推理时空间对齐损失：</strong> InfSplign 在每个去噪步骤中，通过引入一个复合损失函数来调整噪声预测，从而引导生成过程朝着更符合空间语义的方向发展。</li>
<li><strong>利用多层级交叉注意力图：</strong> 该方法巧妙地从扩散模型 U-Net 解码器的不同层级（粗粒度、中粒度）提取交叉注意力图。这些注意力图被视为物体空间信息的代理。</li>
<li><strong>复合损失函数：</strong> 损失函数包含三个关键组成部分：<ul>
<li><strong>空间对齐损失 (Lspatial)：</strong> 通过计算物体在注意力图中的质心（centroid）差异，来惩罚违反文本提示中指定空间关系的情况。</li>
<li><strong>物体存在损失 (Lpresence)：</strong> 通过最小化物体注意力图的方差，来确保物体在生成的图像中清晰可见且不会被忽略。</li>
<li><strong>表示平衡损失 (Lbalance)：</strong> 通过平衡不同物体注意力图的方差，来防止一个物体过度主导而另一个物体被抑制，确保所有物体都能得到充分表示。</li>
</ul>
</li>
<li><strong>轻量级、即插即用：</strong> InfSplign 是一种轻量级的方法，不需要对预训练的扩散模型进行任何修改或重新训练，可以轻松地集成到任何现有的扩散模型（如 Stable Diffusion）中。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong>
作者在 <strong>VISOR</strong> 和 <strong>T2I-CompBench</strong> 这两个广泛使用的 T2I 空间理解基准上进行了全面评估。结果表明：</p>
<ul>
<li><strong>显著性能提升：</strong> InfSplign 在这两个基准上均取得了<strong>最先进（state-of-the-art）的性能</strong>。</li>
<li><strong>超越现有方法：</strong> 相较于最强的现有推理时基线方法，InfSplign 在空间对齐方面取得了显著的性能提升（例如，在 VISOR-4 上提升高达 24.81%）。</li>
<li><strong>媲美甚至超越微调方法：</strong> 令人印象深刻的是，InfSplign 甚至在性能上<strong>超越了那些需要昂贵微调的先进方法</strong>（例如，在 VISOR-4 上提升高达 14.33%）。</li>
<li><strong>定性结果：</strong> 定性实验结果（如图 4-12 所示）直观地展示了 InfSplign 在生成具有准确空间关系和物体组合的图像方面的强大能力，尤其是在处理不常见物体组合时表现出色。</li>
</ul>
<p><strong>4. 提及的局限性：</strong>
论文中提到了一些局限性：</p>
<ul>
<li><strong>罕见物体组合的挑战：</strong> 对于自然场景中很少共同出现的物体组合，基础扩散模型本身就难以生成，InfSplign 在这种情况下也难以完全纠正空间对齐，因为物体本身可能就无法在图像中出现。在这种情况下，物体准确率（object accuracy）成为瓶颈。</li>
<li><strong>对物体存在性的依赖：</strong> InfSplign 的有效性在一定程度上依赖于物体能够被成功生成和检测。如果物体本身就无法出现，那么其空间对齐就无从谈起。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong>
作者指出了未来的研究方向：</p>
<ul>
<li><strong>扩展到 Transformer 架构：</strong> 作者目前正在将 InfSplign 扩展到 Transformer 架构，并已取得初步成果。</li>
<li><strong>更复杂的空间关系：</strong> 探索处理更复杂的多维空间关系，例如三维空间中的相对位置。</li>
</ul>
<p><strong>总结：</strong>
InfSplign 是一项重要的研究成果，它通过一种创新性的、无需训练的推理时方法，显著提升了文本到图像扩散模型在理解和生成空间关系方面的能力。该方法利用交叉注意力图的丰富信息，通过精心设计的复合损失函数，在保证物体存在和平衡表示的同时，实现了精确的空间对齐。其优异的性能和即插即用的特性，使其成为 T2I 领域一个非常有价值的贡献，为实现更具可控性和准确性的图像生成开辟了新的道路。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce InfSplign, a training-free inference-time method that improves spatial alignment by adjusting the noise through a compound loss in every denoising step.</li>
<li>Our comprehensive evaluations on VISOR and T2I-CompBench show that InfSplign establishes a new state-of-the-art (to the best of our knowledge), achieving substantial performance gains over the strongest existing inference-time baselines and even outperforming the fine-tuning-based methods.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.17851v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.17851v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-12-22 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
