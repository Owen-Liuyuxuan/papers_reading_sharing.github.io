<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2026-01-05 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-02
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2026-01-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-06
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2026-01-02/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2026-01-06/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2026-01-05">Arxiv Computer Vision Papers - 2026-01-05</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#adagar-adaptive-gabor-representation-for-dynamic-scene-reconstruction" class="nav-link">AdaGaR: Adaptive Gabor Representation for Dynamic Scene Reconstruction</a>
                </li>
                <li class="nav-item">
                    <a href="#unified-primitive-proxies-for-structured-shape-completion" class="nav-link">Unified Primitive Proxies for Structured Shape Completion</a>
                </li>
                <li class="nav-item">
                    <a href="#grading-handwritten-engineering-exams-with-multimodal-large-language-models" class="nav-link">Grading Handwritten Engineering Exams with Multimodal Large Language Models</a>
                </li>
                <li class="nav-item">
                    <a href="#pixel-to-4d-camera-controlled-image-to-video-generation-with-dynamic-3d-gaussians" class="nav-link">Pixel-to-4D: Camera-Controlled Image-to-Video Generation with Dynamic 3D Gaussians</a>
                </li>
                <li class="nav-item">
                    <a href="#roboreward-general-purpose-vision-language-reward-models-for-robotics" class="nav-link">RoboReward: General-Purpose Vision-Language Reward Models for Robotics</a>
                </li>
                <li class="nav-item">
                    <a href="#avatar-forcing-real-time-interactive-head-avatar-generation-for-natural-conversation" class="nav-link">Avatar Forcing: Real-Time Interactive Head Avatar Generation for Natural Conversation</a>
                </li>
                <li class="nav-item">
                    <a href="#modality-dominance-aware-optimization-for-embodied-rgb-infrared-perception" class="nav-link">Modality Dominance-Aware Optimization for Embodied RGB-Infrared Perception</a>
                </li>
                <li class="nav-item">
                    <a href="#granalign-granularity-aware-alignment-framework-for-zero-shot-video-moment-retrieval" class="nav-link">GranAlign: Granularity-Aware Alignment Framework for Zero-Shot Video Moment Retrieval</a>
                </li>
                <li class="nav-item">
                    <a href="#aegis-exploring-the-limit-of-world-knowledge-capabilities-for-unified-mulitmodal-models" class="nav-link">AEGIS: Exploring the Limit of World Knowledge Capabilities for Unified Mulitmodal Models</a>
                </li>
                <li class="nav-item">
                    <a href="#a-comprehensive-dataset-for-human-vs-ai-generated-image-detection" class="nav-link">A Comprehensive Dataset for Human vs. AI Generated Image Detection</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2026-01-05">Arxiv Computer Vision Papers - 2026-01-05</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>好的，作为一名专注于计算机视觉和机器学习的研究助理，我将为您提供一份关于2026年1月2日Arxiv计算机视觉领域论文的简明执行摘要。</p>
<hr />
<p><strong>执行摘要：2026年1月2日 Arxiv 计算机视觉论文精选</strong></p>
<p><strong>主要主题与趋势：</strong></p>
<p>本期Arxiv论文集展现了计算机视觉领域在<strong>动态场景理解与重建、多模态融合、具身智能以及生成模型</strong>等方面的显著进展。特别值得注意的是，<strong>3D表示与生成</strong>（如动态3D高斯）以及<strong>利用大型语言模型（LLMs）增强视觉任务</strong>成为突出趋势。此外，<strong>零样本学习和通用性奖励模型</strong>的探索也预示着模型能力的进一步提升。</p>
<p><strong>亮点与创新：</strong></p>
<ul>
<li><strong>Pixel-to-4D: Camera-Controlled Image-to-Video Generation with Dynamic 3D Gaussians</strong> 是一项突破性工作，它将静态图像转化为动态视频，并引入了<strong>动态3D高斯</strong>的概念，为高质量、可控的视频生成开辟了新途径。</li>
<li><strong>RoboReward: General-Purpose Vision-Language Reward Models for Robotics</strong> 提出了<strong>通用的视觉-语言奖励模型</strong>，旨在解决机器人领域中奖励函数设计的挑战，有望加速机器人学习的通用性和效率。</li>
<li><strong>AdaGaR: Adaptive Gabor Representation for Dynamic Scene Reconstruction</strong> 在动态场景重建方面提出了<strong>自适应Gabor表示</strong>，为处理复杂动态场景提供了新的视角。</li>
</ul>
<p><strong>新兴研究方向与技术：</strong></p>
<ul>
<li><strong>动态3D表示与生成：</strong> 以Pixel-to-4D为代表，动态3D高斯等技术正在成为生成逼真动态场景的关键。</li>
<li><strong>多模态LLMs在视觉任务中的应用：</strong> 从工程考试评分到具身感知，LLMs正被广泛集成以提升视觉任务的理解和执行能力。</li>
<li><strong>具身智能与感知：</strong> RoboReward和Modality Dominance-Aware Optimization等论文表明，对机器人和具身智能的感知能力（尤其是在多模态环境下）的研究正在深化。</li>
<li><strong>零样本与泛化能力：</strong> GranAlign在视频检索中的零样本能力探索，以及RoboReward的通用性目标，都指向了模型泛化能力的提升。</li>
<li><strong>AI生成内容检测：</strong> A Comprehensive Dataset for Human vs. AI Generated Image Detection 的出现，标志着对AI生成内容真实性验证的需求日益增长。</li>
</ul>
<p><strong>建议阅读论文：</strong></p>
<p>为了快速了解当前研究热点和前沿技术，建议优先阅读以下论文：</p>
<ol>
<li><strong>Pixel-to-4D: Camera-Controlled Image-to-Video Generation with Dynamic 3D Gaussians</strong> (对于视频生成和3D表示的最新进展)</li>
<li><strong>RoboReward: General-Purpose Vision-Language Reward Models for Robotics</strong> (对于具身智能和多模态奖励模型的研究)</li>
<li><strong>AdaGaR: Adaptive Gabor Representation for Dynamic Scene Reconstruction</strong> (对于动态场景理解和重建的创新方法)</li>
<li><strong>Unified Primitive Proxies for Structured Shape Completion</strong> (对于3D形状理解和补全的结构化方法)</li>
</ol>
<hr />
<p>希望这份摘要能帮助您快速掌握近期Arxiv计算机视觉领域的最新动态。</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2601.00796v1">AdaGaR: Adaptive Gabor Representation for Dynamic Scene Reconstruction</a></li>
<li><a href="#2601.00759v1">Unified Primitive Proxies for Structured Shape Completion</a></li>
<li><a href="#2601.00730v1">Grading Handwritten Engineering Exams with Multimodal Large Language Models</a></li>
<li><a href="#2601.00678v1">Pixel-to-4D: Camera-Controlled Image-to-Video Generation with Dynamic 3D Gaussians</a></li>
<li><a href="#2601.00675v1">RoboReward: General-Purpose Vision-Language Reward Models for Robotics</a></li>
<li><a href="#2601.00664v1">Avatar Forcing: Real-Time Interactive Head Avatar Generation for Natural Conversation</a></li>
<li><a href="#2601.00598v1">Modality Dominance-Aware Optimization for Embodied RGB-Infrared Perception</a></li>
<li><a href="#2601.00584v1">GranAlign: Granularity-Aware Alignment Framework for Zero-Shot Video Moment Retrieval</a></li>
<li><a href="#2601.00561v1">AEGIS: Exploring the Limit of World Knowledge Capabilities for Unified Mulitmodal Models</a></li>
<li><a href="#2601.00553v1">A Comprehensive Dataset for Human vs. AI Generated Image Detection</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2601.00796v1'></a></p>
<h2 id="adagar-adaptive-gabor-representation-for-dynamic-scene-reconstruction"><a href="https://arxiv.org/abs/2601.00796v1">AdaGaR: Adaptive Gabor Representation for Dynamic Scene Reconstruction</a></h2>
<p><strong>Authors:</strong> Jiewen Chan, Zhenjun Zhao, Yu-Lun Liu</p>
<p><strong>Published:</strong> 2026-01-02</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Reconstructing dynamic 3D scenes from monocular videos requires simultaneously capturing high-frequency appearance details and temporally continuous motion. Existing methods using single Gaussian primitives are limited by their low-pass filtering nature, while standard Gabor functions introduce energy instability. Moreover, lack of temporal continuity constraints often leads to motion artifacts during interpolation. We propose AdaGaR, a unified framework addressing both frequency adaptivity and temporal continuity in explicit dynamic scene modeling. We introduce Adaptive Gabor Representation, extending Gaussians through learnable frequency weights and adaptive energy compensation to balance detail capture and stability. For temporal continuity, we employ Cubic Hermite Splines with Temporal Curvature Regularization to ensure smooth motion evolution. An Adaptive Initialization mechanism combining depth estimation, point tracking, and foreground masks establishes stable point cloud distributions in early training. Experiments on Tap-Vid DAVIS demonstrate state-of-the-art performance (PSNR 35.49, SSIM 0.9433, LPIPS 0.0723) and strong generalization across frame interpolation, depth consistency, video editing, and stereo view synthesis. Project page: https://jiewenchan.github.io/AdaGaR/</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“AdaGaR: Adaptive Gabor Representation for Dynamic Scene Reconstruction”的全面中文摘要：</p>
<p><strong>论文题目：</strong> AdaGaR: Adaptive Gabor Representation for Dynamic Scene Reconstruction</p>
<p><strong>作者：</strong> Jiewen Chan, Zhenjun Zhao, Yu-Lun Liu</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
该论文旨在解决从单目视频中重建动态三维场景的核心挑战。这需要同时捕捉高频的外观细节和时间上连续的运动。现有方法在处理这两个方面时都存在局限性：
*   <strong>高频细节捕捉不足：</strong> 基于高斯原语的方法具有低通滤波的特性，难以保留精细的纹理细节。而标准 Gabor 函数虽然能增强高频细节，但容易引入能量不稳定性。
*   <strong>时间连续性不足：</strong> 缺乏严格的时间连续性约束，容易在插值过程中产生运动伪影，导致几何和运动的不连续性。</p>
<p><strong>2. 主要创新点/方法贡献：</strong>
为了克服这些挑战，作者提出了 AdaGaR（Adaptive Gabor Representation for Dynamic Scene Reconstruction）框架，这是一个统一的解决方案，同时解决了频率适应性和时间连续性问题：</p>
<ul>
<li>
<p><strong>自适应 Gabor 表示 (Adaptive Gabor Representation)：</strong></p>
<ul>
<li><strong>频率适应性：</strong> 扩展了传统的高斯原语，引入了可学习的频率权重 (wi)，使其能够自适应地在低频（高斯）和高频（Gabor）之间切换。这使得模型能够根据场景需求捕捉不同频率的细节。</li>
<li><strong>能量稳定性：</strong> 通过引入自适应能量补偿机制，确保了 Gabor 表示的能量稳定性，避免了因频率调制带来的不稳定性。</li>
<li><strong>平滑过渡：</strong> 通过一个补偿项 <code>b</code>，使得模型能够平滑地从纯高斯（低频）过渡到 Gabor（高频）模式，并在不需要高频细节时自然退化为标准高斯。</li>
</ul>
</li>
<li>
<p><strong>时间连续性约束：</strong></p>
<ul>
<li><strong>三次 Hermite 样条插值 (Cubic Hermite Splines)：</strong> 用于插值动态原语的时间演化，确保了平滑的运动轨迹。</li>
<li><strong>时间曲率正则化 (Temporal Curvature Regularization)：</strong> 通过约束轨迹的二阶导数，进一步保证了运动的平滑性和几何连续性，有效避免了插值伪影。</li>
</ul>
</li>
<li>
<p><strong>自适应初始化 (Adaptive Initialization)：</strong></p>
<ul>
<li><strong>多模态融合：</strong> 结合了深度估计、点追踪和前景掩码等多种信息，在训练早期生成密集且时间上一致的点云分布，为后续的显式表示奠定稳固的几何基础。</li>
<li><strong>自适应采样：</strong> 根据场景的运动和深度分布动态调整采样密度，实现了更均衡的前景/背景覆盖，减少了早期闪烁。</li>
</ul>
</li>
</ul>
<p><strong>3. 主要结果与意义：</strong>
*   <strong>卓越的性能：</strong> 在 Tap-Vid DAVIS 数据集上取得了最先进的性能，PSNR 达到 35.49 dB，SSIM 达到 0.9433，LPIPS 达到 0.0723。与第二名相比，PSNR 提高了 6.86 dB。
*   <strong>强大的泛化能力：</strong> AdaGaR 在帧插值、深度一致性、视频编辑和立体视图合成等下游应用中展现出强大的泛化能力。
*   <strong>细节保留与运动平滑：</strong> 实验结果表明，AdaGaR 能够有效地捕捉精细的纹理细节（如毛发、车窗边缘）并保持时间上的运动连续性，尤其是在具有挑战性的场景（如快速运动、遮挡和复杂形变）下。
*   <strong>统一的框架：</strong> 提供了一个紧凑、端到端的解决方案，能够同时建模显式动态表示中的时间和频率信息。</p>
<p><strong>4. 提及的局限性：</strong>
*   <strong>非线性运动的挑战：</strong> 基于样条的运动建模在处理突变或高度非线性运动时可能存在对齐问题。
*   <strong>高频区域的潜在振荡：</strong> 自适应 Gabor 表示在高频区域可能由于能量约束而出现振荡。</p>
<p><strong>5. 未来研究方向：</strong>
*   <strong>自适应时间控制点：</strong> 探索更灵活的自适应时间控制点，以更好地处理非线性运动。
*   <strong>运动感知频率调制：</strong> 开发运动感知的频率调制方法，以进一步优化高频细节的表示。</p>
<p>总而言之，AdaGaR 提出了一种新颖的自适应 Gabor 表示方法，并结合了三次 Hermite 样条和时间曲率正则化，成功地解决了单目视频动态场景重建中的高频细节捕捉和时间连续性两大难题。该方法在多个评估指标上取得了显著的领先，并展现了在多种下游任务中的强大应用潜力。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose AdaGaR, a unified framework addressing both frequency adaptivity and temporal continuity in explicit dynamic scene modeling.</li>
<li>We introduce Adaptive Gabor Representation, extending Gaussians through learnable frequency weights and adaptive energy compensation to balance detail capture and stability.</li>
<li>Experiments on Tap-Vid DAVIS demonstrate state-of-the-art performance (PSNR 35.49, SSIM 0.9433, LPIPS 0.0723) and strong generalization across frame interpolation, depth consistency, video editing, and stereo view synthesis.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.00796v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.00796v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.00759v1'></a></p>
<h2 id="unified-primitive-proxies-for-structured-shape-completion"><a href="https://arxiv.org/abs/2601.00759v1">Unified Primitive Proxies for Structured Shape Completion</a></h2>
<p><strong>Authors:</strong> Zhaiyu Chen, Yuqing Wang, Xiao Xiang Zhu</p>
<p><strong>Published:</strong> 2026-01-02</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Structured shape completion recovers missing geometry as primitives rather than as unstructured points, which enables primitive-based surface reconstruction. Instead of following the prevailing cascade, we rethink how primitives and points should interact, and find it more effective to decode primitives in a dedicated pathway that attends to shared shape features. Following this principle, we present UniCo, which in a single feed-forward pass predicts a set of primitives with complete geometry, semantics, and inlier membership. To drive this unified representation, we introduce primitive proxies, learnable queries that are contextualized to produce assembly-ready outputs. To ensure consistent optimization, our training strategy couples primitives and points with online target updates. Across synthetic and real-world benchmarks with four independent assembly solvers, UniCo consistently outperforms recent baselines, lowering Chamfer distance by up to 50% and improving normal consistency by up to 7%. These results establish an attractive recipe for structured 3D understanding from incomplete data. Project page: https://unico-completion.github.io.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为一名计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行深入分析。</p>
<p><strong>论文摘要分析：Unified Primitive Proxies for Structured Shape Completion</strong></p>
<p><strong>1. 论文的主要贡献（2-3句话）：</strong></p>
<p>该论文提出了一种名为 UniCo 的新颖方法，用于结构化形状补全。与传统的级联方法不同，UniCo 在单一前馈通道中统一预测具有完整几何、语义和内点成员资格的原始体集合。其核心在于引入了“原始体代理”（primitive proxies），这些可学习的查询能够根据上下文生成可组装的输出，从而实现更有效的结构化形状理解。</p>
<p><strong>2. 关键创新或方法论：</strong></p>
<ul>
<li><strong>统一的原始体预测通道：</strong> UniCo 的核心创新在于打破了传统方法中点云和原始体之间分离处理的级联模式。它设计了一个专门的通道来解码原始体，该通道能够关注共享的形状特征，从而实现更紧密的点与原始体之间的交互。</li>
<li><strong>原始体代理（Primitive Proxies）：</strong> 这是论文的另一项关键创新。原始体代理被设计为可学习的查询，它们能够被上下文信息“情境化”（contextualized），从而直接生成“组装就绪”（assembly-ready）的输出。这意味着代理不仅仅是占位符，而是能够根据输入数据动态地生成具有完整几何、语义和内点成员资格的原始体。</li>
<li><strong>耦合训练策略与在线目标更新：</strong> 为了确保优化的一致性，论文采用了将原始体和点云耦合在一起的训练策略，并结合了在线目标更新机制。这有助于在训练过程中保持点云和预测的原始体之间的良好对齐和一致性。</li>
</ul>
<p><strong>3. 对该领域的潜在影响：</strong></p>
<ul>
<li><strong>提升结构化3D理解的效率和准确性：</strong> UniCo 的统一预测范式有望显著提高结构化形状补全的效率，因为它避免了多阶段的级联处理。同时，通过更紧密的点与原始体交互以及原始体代理的引入，有望提升补全的准确性，尤其是在几何和语义方面。</li>
<li><strong>推动基于原始体的3D表面重建：</strong> 论文明确指出，其方法能够生成用于原始体基表面重建的输出。这意味着 UniCo 可以为下游的3D重建任务提供更优质、更结构化的输入，从而推动该领域的发展。</li>
<li><strong>为不完整数据下的3D理解提供新范式：</strong> 该研究为如何从不完整数据中理解3D形状提供了一个新的、更具吸引力的“配方”。它表明，通过精心设计的统一表示和学习机制，可以更有效地提取和利用结构化信息。</li>
<li><strong>潜在的通用性：</strong> 论文在合成和真实世界基准上进行了测试，并与多种独立的组装求解器进行了比较，这表明 UniCo 的方法具有一定的通用性，能够适应不同的下游任务和评估标准。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用：</strong></p>
<ul>
<li><strong>3D模型检索与识别：</strong> 结构化形状补全可以为不完整或损坏的3D模型提供更完整的表示，从而提高检索和识别的准确性。</li>
<li><strong>机器人感知与导航：</strong> 机器人需要准确理解周围环境的3D结构，结构化形状补全可以帮助机器人从传感器数据中恢复出更完整的场景几何信息。</li>
<li><strong>虚拟现实/增强现实（VR/AR）：</strong> 在 VR/AR 应用中，需要高质量的3D模型来构建沉浸式体验。UniCo 的方法可以用于生成更逼真、更完整的虚拟对象。</li>
<li><strong>计算机辅助设计（CAD）：</strong> 在设计过程中，可能需要对不完整的模型进行补全和修复，UniCo 的方法可以为设计师提供更智能的工具。</li>
<li><strong>3D打印：</strong> 结构化形状补全可以帮助修复3D模型中的缺陷，使其更适合3D打印。</li>
<li><strong>医学影像分析：</strong> 在医学影像中，可能存在部分缺失的器官或结构，结构化形状补全可以帮助重建完整的解剖结构。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性：</strong></p>
<ul>
<li><strong>对原始体类型的依赖性：</strong> 尽管摘要没有明确说明，但“原始体”（primitives）的定义和预设类型对方法的性能至关重要。如果输入的形状主要由摘要中未考虑的复杂或非标准原始体构成，方法的性能可能会受到影响。</li>
<li><strong>“组装就绪”的定义和挑战：</strong> “组装就绪”是一个相对的概念。虽然论文声称原始体代理能够生成这样的输出，但其在实际组装过程中的鲁棒性和通用性仍需进一步验证。不同的组装求解器可能对输出的格式和精度有不同的要求。</li>
<li><strong>计算复杂度：</strong> 虽然是单一前馈通道，但“情境化”原始体代理以及处理共享形状特征可能仍然需要一定的计算资源，尤其是在处理高分辨率或非常复杂的场景时。</li>
<li><strong>对训练数据的需求：</strong> 任何机器学习方法都依赖于训练数据。UniCo 的性能将很大程度上取决于其训练数据的质量和多样性，特别是包含各种不完整形状和对应完整结构化表示的数据集。</li>
<li><strong>泛化到极端不完整情况的挑战：</strong> 摘要提到“不完整数据”，但对于数据缺失程度非常严重的情况，即使是 UniCo 这种先进的方法，也可能面临挑战，其补全的准确性和可靠性可能会下降。</li>
</ul>
<p>总而言之，这篇论文提出的 UniCo 方法在结构化形状补全领域具有重要的理论和实践意义。其核心创新在于统一的原始体预测和创新的原始体代理机制，有望在效率和准确性上取得显著突破，并为3D理解的多个下游应用带来积极影响。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Following this principle, we present UniCo, which in a single feed-forward pass predicts a set of primitives with complete geometry, semantics, and inlier membership.</li>
<li>To drive this unified representation, we introduce primitive proxies, learnable queries that are contextualized to produce assembly-ready outputs.</li>
<li>Across synthetic and real-world benchmarks with four independent assembly solvers, UniCo consistently outperforms recent baselines, lowering Chamfer distance by up to 50% and improving normal consistency by up to 7%.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.00759v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.00759v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.00730v1'></a></p>
<h2 id="grading-handwritten-engineering-exams-with-multimodal-large-language-models"><a href="https://arxiv.org/abs/2601.00730v1">Grading Handwritten Engineering Exams with Multimodal Large Language Models</a></h2>
<p><strong>Authors:</strong> Janez Perš, Jon Muhovič, Andrej Košir, Boštjan Murovec</p>
<p><strong>Published:</strong> 2026-01-02</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Handwritten STEM exams capture open-ended reasoning and diagrams, but manual grading is slow and difficult to scale. We present an end-to-end workflow for grading scanned handwritten engineering quizzes with multimodal large language models (LLMs) that preserves the standard exam process (A4 paper, unconstrained student handwriting). The lecturer provides only a handwritten reference solution (100%) and a short set of grading rules; the reference is converted into a text-only summary that conditions grading without exposing the reference scan. Reliability is achieved through a multi-stage design with a format/presence check to prevent grading blank answers, an ensemble of independent graders, supervisor aggregation, and rigid templates with deterministic validation to produce auditable, machine-parseable reports. We evaluate the frozen pipeline in a clean-room protocol on a held-out real course quiz in Slovenian, including hand-drawn circuit schematics. With state-of-the-art backends (GPT-5.2 and Gemini-3 Pro), the full pipeline achieves <script type="math/tex">\approx</script>8-point mean absolute difference to lecturer grades with low bias and an estimated manual-review trigger rate of <script type="math/tex">\approx</script>17% at <script type="math/tex">D_{\max}=40</script>. Ablations show that trivial prompting and removing the reference solution substantially degrade accuracy and introduce systematic over-grading, confirming that structured prompting and reference grounding are essential.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于使用多模态大语言模型（LLMs）对工程考试进行自动评分的论文的全面中文摘要，重点关注其在计算机视觉领域的创新性和重要性。</p>
<p><strong>论文题目：</strong> Grading Handwritten Engineering Exams with Multimodal Large Language Models (使用多模态大语言模型对工程考试进行评分)</p>
<p><strong>作者：</strong> Janez Perš, Jon Muhovič, Andrej Košir, Boštjan Murovec</p>
<hr />
<p><strong>全面摘要</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
手写工程考试（包含开放式推理和图表）在STEM教育中仍然普遍存在，但其手动评分过程耗时且难以规模化。传统方法要么要求学生将答案转换为机器可读格式，要么依赖于易出错的手写识别（OCR）技术，并且往往无法端到端地处理手写文本和图表。本研究旨在解决这一问题，提出一种能够直接处理扫描的手写工程考试（包括图表）的自动化评分流程，同时保留标准的考试形式（A4纸，无约束手写）。</p>
<p><strong>2. 关键创新/方法论贡献：</strong>
该论文的核心贡献在于提出了一种<strong>端到端的、多阶段的、多模态大语言模型（LLM）驱动的评分工作流程</strong>，其创新性体现在以下几个方面：</p>
<ul>
<li><strong>保留标准考试流程：</strong> 流程设计旨在最小化对现有考试流程的干扰，学生只需提供标准的A4纸手写答案。</li>
<li><strong>参考条件化（Reference Conditioning）：</strong> 讲师仅需提供一份手写参考答案（100%正确）和一套评分规则。参考答案被转换为文本摘要，用于指导评分，但原始参考图像本身不直接暴露给LLM，这在一定程度上解决了隐私问题。</li>
<li><strong>多阶段设计与鲁棒性：</strong><ul>
<li><strong>格式/存在性检查（Format/Presence Check）：</strong> 在评分前，通过一个检查器来识别包含实际学生答案的任务，以防止对空白答案进行评分。</li>
<li><strong>独立评分器集成（Ensemble of Independent Graders）：</strong> 每个任务由多个独立的LLM调用进行评分，生成结构化的草稿。</li>
<li><strong>监督者聚合（Supervisor Aggregation）：</strong> 一个监督模型负责合并多个评分器的草稿，形成最终的考试级别输出，并强制执行模板合规性。</li>
<li><strong>刚性模板与确定性验证（Rigid Templates &amp; Deterministic Validation）：</strong> 使用预定义的Markdown模板来规范LLM的输出格式，确保输出是机器可解析的，从而实现可审计性和一致性。</li>
</ul>
</li>
<li><strong>零样本（Zero-shot）评估：</strong> 该系统是不可训练的，完全在零样本设置下运行，避免了迭代式提示工程和评分标准的偏差。</li>
<li><strong>语言无关性：</strong> 尽管实验在斯洛文尼亚语环境下进行，但该流程本身是语言无关的，只需翻译文本工件即可适应其他语言。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong>
在对一个真实的、包含电路图的斯洛文尼亚语工程考试（Class B）进行评估时，使用最先进的LLM后端（GPT-5.2和Gemini-3 Pro），该完整工作流程取得了以下成果：</p>
<ul>
<li><strong>高准确性：</strong> 平均绝对差（MAD）约为8个点，与讲师评分的偏差很小（低偏差）。</li>
<li><strong>可接受的手动复核率：</strong> 在Dmax=40时，估计的手动复核触发率为约17%，表明系统在大多数情况下能够可靠评分，仅需少量人工干预。</li>
<li><strong>消融实验的重要性：</strong> 消融实验（Ablation studies）表明，简单的提示（trivial prompting）和移除参考答案显著降低了准确性，并引入了系统性的过度评分（over-grading）。这有力地证实了结构化提示和参考条件化对于实现可靠评分至关重要。</li>
<li><strong>模型能力筛选：</strong> 实验还对不同的LLM后端进行了基线性能评估，表明GPT-5.2、GPT-5.2-pro和Gemini-3 Pro在MAD和偏差方面表现最佳。</li>
</ul>
<p>这些结果表明，通过精心设计的系统级工作流程，现代多模态LLM已经能够实现对包含手写文本和图表的工程考试进行接近人类评分者水平的自动评分，这对于解决STEM教育中评分的规模化和效率问题具有重要意义。</p>
<p><strong>4. 提及的局限性：</strong>
*   <strong>数据集限制：</strong> 论文使用了作者自己收集的、私有的课程考试数据进行评估，因为公开可用的、支持端到端评估的手写考试评分数据集非常稀少。
*   <strong>评估规模：</strong> 评估仅基于一个“干净房间”（clean-room）协议下的一个实际课程测验，且仅有一个人类评分者作为地面真实（ground truth）。
*   <strong>隐私问题：</strong> 虽然参考答案被转换为文本摘要，但原始扫描件的隐私处理仍需谨慎。
*   <strong>学生反馈：</strong> 学生反馈显示，虽然大多数学生对AI评分持积极态度，但也有关于漏判答案、评分错误和考试难度变化的担忧。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>扩展验证：</strong> 在更大、更多样化的数据集上进行验证，并计划在隐私审查和机构批准后发布代码、提示和数据集，以支持标准化评估。
*   <strong>更细粒度的评估：</strong> 在实际部署中，可以将手动复核标准应用于更细粒度的层面，如单个问题或答案，以进一步精确定位和减少人工干预。
*   <strong>多模态理解的深入：</strong> 进一步探索LLM在理解和评估复杂图表（如电路图）方面的能力，以及如何更有效地将其与文本推理结合。
*   <strong>用户体验优化：</strong> 结合学生反馈，进一步优化评分反馈的质量和形式，解决学生担忧的问题。
*   <strong>跨语言和跨学科应用：</strong> 验证和扩展该框架在不同语言和不同STEM学科中的适用性。</p>
<p><strong>对计算机视觉领域的意义：</strong></p>
<p>这篇论文对计算机视觉领域具有重要意义，因为它展示了<strong>多模态LLM在理解和处理包含视觉信息（手写文本和图表）的文档方面的强大能力</strong>。它不仅将LLM的应用从纯文本领域扩展到了包含复杂视觉元素的场景，而且通过<strong>“参考条件化”</strong>和<strong>“刚性模板”</strong>等方法，有效地解决了LLM输出不确定性和不可靠性的问题，使其能够用于需要高精度和可审计性的实际应用场景，如教育评估。这为未来计算机视觉与自然语言处理的深度融合，以及在教育、文档分析等领域的创新应用提供了新的思路和实践范例。特别是，它证明了<strong>视觉信息（图表）与文本推理的结合，对于理解和评估工程问题至关重要</strong>，这正是多模态模型的核心优势所在。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present an end-to-end workflow for grading scanned handwritten engineering quizzes with multimodal large language models (LLMs) that preserves the standard exam process (A4 paper, unconstrained student handwriting).</li>
<li>With state-of-the-art backends (GPT-5.2 and Gemini-3 Pro), the full pipeline achieves <script type="math/tex">\approx</script>8-point mean absolute difference to lecturer grades with low bias and an estimated manual-review trigger rate of <script type="math/tex">\approx</script>17% at <script type="math/tex">D_{\max}=40</script>.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.00730v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.00730v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.00678v1'></a></p>
<h2 id="pixel-to-4d-camera-controlled-image-to-video-generation-with-dynamic-3d-gaussians"><a href="https://arxiv.org/abs/2601.00678v1">Pixel-to-4D: Camera-Controlled Image-to-Video Generation with Dynamic 3D Gaussians</a></h2>
<p><strong>Authors:</strong> Melonie de Almeida, Daniela Ivanova, Tong Shi, John H. Williamson, Paul Henderson</p>
<p><strong>Published:</strong> 2026-01-02</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Humans excel at forecasting the future dynamics of a scene given just a single image. Video generation models that can mimic this ability are an essential component for intelligent systems. Recent approaches have improved temporal coherence and 3D consistency in single-image-conditioned video generation. However, these methods often lack robust user controllability, such as modifying the camera path, limiting their applicability in real-world applications. Most existing camera-controlled image-to-video models struggle with accurately modeling camera motion, maintaining temporal consistency, and preserving geometric integrity. Leveraging explicit intermediate 3D representations offers a promising solution by enabling coherent video generation aligned with a given camera trajectory. Although these methods often use 3D point clouds to render scenes and introduce object motion in a later stage, this two-step process still falls short in achieving full temporal consistency, despite allowing precise control over camera movement. We propose a novel framework that constructs a 3D Gaussian scene representation and samples plausible object motion, given a single image in a single forward pass. This enables fast, camera-guided video generation without the need for iterative denoising to inject object motion into render frames. Extensive experiments on the KITTI, Waymo, RealEstate10K and DL3DV-10K datasets demonstrate that our method achieves state-of-the-art video quality and inference efficiency. The project page is available at https://melonienimasha.github.io/Pixel-to-4D-Website.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于“Pixel-to-4D: Camera-Controlled Image-to-Video Generation with Dynamic 3D Gaussians”论文的中文摘要，其中包含了您要求的各个部分：</p>
<p><strong>论文题目：</strong> Pixel-to-4D: Camera-Controlled Image-to-Video Generation with Dynamic 3D Gaussians</p>
<p><strong>作者：</strong> Melonie de Almeida, Daniela Ivanova, Tong Shi, John H. Williamson, Paul Henderson</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
该论文主要解决了从单张图像生成具有精确相机控制和动态物体运动的视频这一核心挑战。现有方法在模仿人类预测场景未来动态的能力方面取得了进展，但普遍存在用户可控性不足（如相机路径修改困难）、相机运动建模不准确、时间一致性差以及几何完整性难以保证等问题。尤其是在需要精确相机控制的实际应用中，这些限制尤为突出。</p>
<p><strong>2. 主要创新点/方法贡献：</strong>
作者提出了一个名为 <strong>Pixel-to-4D</strong> 的新颖框架，其核心创新在于：
*   <strong>动态3D高斯表示 (Dynamic 3D Gaussian Representation)：</strong> 引入了一种新的4D场景表示，该表示由像素对齐的静态和动态高斯参数组成，能够捕捉多层级的场景信息，并为每个高斯点赋予线速度、角速度及其加速度，从而实现动态场景的建模。
*   <strong>单次前向传播的生成架构：</strong> 设计了一个高效的前馈神经网络架构，能够从单张输入图像一次性生成上述4D高斯表示。该架构利用了预训练的DINOv2特征，并融合了静态和动态高斯参数的预测。
*   <strong>无需迭代去噪的快速生成：</strong> 与依赖迭代去噪来注入物体运动的方法不同，Pixel-to-4D直接在4D表示层面处理物体运动，从而实现了快速、相机引导的视频生成，无需复杂的后处理步骤。
*   <strong>基于3D高斯喷绘 (3D Gaussian Splatting) 的渲染：</strong> 利用了3D高斯喷绘技术，这是一种先进的3D重建方法，能够高效地渲染场景，并能通过自适应地调整高斯尺度来填补点云的空隙，从而生成高质量的视频帧。</p>
<p><strong>3. 主要结果与意义：</strong>
*   <strong>state-of-the-art 性能：</strong> 在KITTI、Waymo Open、RealEstate10K和DL3DV-10K等多个大规模数据集上进行了广泛实验，结果表明Pixel-to-4D在PSNR、LPIPS、SSIM和FVD等指标上均优于现有的相机控制图像到视频生成模型，同时实现了更快的推理速度。
*   <strong>卓越的相机控制和视觉质量：</strong> 该方法能够生成具有高度时间一致性、精确相机运动跟踪以及逼真物体动态的视频，有效解决了现有方法的痛点。
*   <strong>高效性：</strong> 单次前向传播的生成方式大大降低了推理成本，使其更适用于实际应用。
*   <strong>意义：</strong> 该研究为实现更具可控性、真实感和效率的图像到视频生成提供了新的解决方案，尤其是在需要精确相机控制的自动驾驶、虚拟现实等领域具有重要应用潜力。</p>
<p><strong>4. 提及的局限性：</strong>
*   <strong>物体运动的不确定性：</strong> 论文提到，给定单张图像，物体运动的参数本身就存在不确定性，因此需要从学习到的条件分布中进行采样，而不是直接回归。
*   <strong>对预训练模型的依赖：</strong> 尽管利用DINOv2等预训练模型带来了优势，但也意味着模型在一定程度上依赖于这些模型的泛化能力。
*   <strong>对特定场景的适应性：</strong> 虽然在多个数据集上进行了评估，但对于极端复杂或完全未见过的新场景，其性能仍可能受到影响。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>更精细的物体运动建模：</strong> 进一步探索更复杂的物体运动模式，例如非刚性形变或更精细的交互行为。
*   <strong>更强的语义理解：</strong> 结合更深层次的语义理解，以实现更智能的场景动态预测和内容生成。
*   <strong>交互式编辑和控制：</strong> 进一步增强用户交互能力，允许用户对生成的视频进行更细粒度的编辑和控制。
*   <strong>跨模态生成：</strong> 将该方法扩展到结合文本或其他模态进行更丰富的视频生成。
*   <strong>实时生成：</strong> 进一步优化模型以实现更高帧率的实时视频生成。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose a novel framework that constructs a 3D Gaussian scene representation and samples plausible object motion, given a single image in a single forward pass.</li>
<li>Extensive experiments on the KITTI, Waymo, RealEstate10K and DL3DV-10K datasets demonstrate that our method achieves state-of-the-art video quality and inference efficiency.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.00678v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.00678v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.00675v1'></a></p>
<h2 id="roboreward-general-purpose-vision-language-reward-models-for-robotics"><a href="https://arxiv.org/abs/2601.00675v1">RoboReward: General-Purpose Vision-Language Reward Models for Robotics</a></h2>
<p><strong>Authors:</strong> Tony Lee, Andrew Wagenmaker, Karl Pertsch, Percy Liang, Sergey Levine, Chelsea Finn</p>
<p><strong>Published:</strong> 2026-01-02</p>
<p><strong>Categories:</strong> cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>A well-designed reward is critical for effective reinforcement learning-based policy improvement. In real-world robotic domains, obtaining such rewards typically requires either labor-intensive human labeling or brittle, handcrafted objectives. Vision-language models (VLMs) have shown promise as automatic reward models, yet their effectiveness on real robot tasks is poorly understood. In this work, we aim to close this gap by introducing (1) \textbf{RoboReward}, a robotics reward dataset and benchmark built on large-scale real-robot corpora from Open X-Embodiment (OXE) and RoboArena, and (2) vision-language reward models trained on this dataset (RoboReward 4B/8B). Because OXE is success-heavy and lacks failure examples, we propose a \emph{negative examples data augmentation} pipeline that generates calibrated \emph{negatives} and \emph{near-misses} via counterfactual relabeling of successful episodes and temporal clipping to create partial-progress outcomes from the same videos. Using this framework, we produce an extensive training and evaluation dataset that spans diverse tasks and embodiments and enables systematic evaluation of whether state-of-the-art VLMs can reliably provide rewards for robotics. Our evaluation of leading open-weight and proprietary VLMs reveals that no model excels across all tasks, underscoring substantial room for improvement. We then train general-purpose 4B- and 8B-parameter models that outperform much larger VLMs in assigning rewards for short-horizon robotic tasks. Finally, we deploy the 8B-parameter reward VLM in real-robot reinforcement learning and find that it improves policy learning over Gemini Robotics-ER 1.5, a frontier physical reasoning VLM trained on robotics data, by a large margin, while substantially narrowing the gap to RL training with human-provided rewards.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“RoboReward: General-Purpose Vision-Language Reward Models for Robotics”的全面中文摘要：</p>
<p><strong>论文题目：</strong> RoboReward: 通用视觉语言奖励模型用于机器人学</p>
<p><strong>作者：</strong> Tony Lee, Andrew Wagenmaker, Karl Pertsch, Percy Liang, Sergey Levine, Chelsea Finn</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
在现实世界的机器人学中，强化学习（RL）策略的有效改进严重依赖于准确且信息丰富的奖励模型。然而，获取这些奖励通常需要耗费人力的人工标注或脆弱的手工设计奖励函数。尽管视觉语言模型（VLMs）在自动奖励建模方面展现出潜力，但它们在真实机器人任务上的有效性仍未得到充分理解。本研究旨在解决这一关键瓶颈，即如何为机器人学开发通用且可靠的奖励模型。</p>
<p><strong>2. 主要创新与方法贡献：</strong>
*   <strong>RoboReward 数据集与基准：</strong> 作者引入了一个名为 RoboReward 的大规模机器人奖励数据集和基准。该数据集构建于 Open X-Embodiment (OXE) 和 RoboArena 的真实机器人语料库之上，涵盖了多样化的任务和机器人实体。
*   <strong>负样本数据增强管道：</strong> 针对现有数据集（如 OXE）主要包含成功案例而缺乏失败案例的问题，作者提出了一种创新的负样本数据增强管道。该管道通过<strong>反事实重标注</strong>（即在同一视频中，通过改变指令来生成部分成功或失败的场景）和<strong>时间剪辑</strong>（将成功视频截断以创建部分进展的片段）来生成校准过的负样本和近乎成功的样本。
*   <strong>通用奖励模型训练：</strong> 基于 RoboReward 数据集，作者训练了两个通用型视觉语言奖励模型：RoboReward 4B 和 RoboReward 8B。
*   <strong>全面的 VLM 评估：</strong> 作者构建了一个名为 RoboRewardBench 的标准化评估基准，用于评估 22 种（包括开源和闭源）先进 VLM 作为机器人奖励模型的能力。</p>
<p><strong>3. 主要研究结果与意义：</strong>
*   <strong>现有 VLM 的局限性：</strong> 评估结果表明，当前的通用 VLM 在机器人控制的各个场景下都无法可靠地提供奖励，存在显著的泛化能力差距。
*   <strong>RoboReward 模型的优越性：</strong> 训练出的 RoboReward 4B 和 8B 模型在 RoboRewardBench 上表现出色，优于许多参数量更大的现有 VLM，尤其是在短时机器人任务的奖励分配方面。
*   <strong>真实机器人 RL 的改进：</strong> 将 RoboReward 8B 模型部署到真实机器人强化学习任务中，显著提高了策略学习的性能，大幅缩小了与人工奖励的差距，并远超了专门为机器人设计的 Gemini Robotics-ER 1.5 模型。
*   <strong>奖励质量与 RL 性能的相关性：</strong> 研究证实，奖励模型的准确性与下游 RL 策略的性能之间存在强烈的正相关关系，强调了高质量奖励模型的重要性。
*   <strong>数据集的价值：</strong> RoboReward 数据集和评估套件的发布，为机器人学领域通用奖励模型的开发和评估提供了宝贵的资源。</p>
<p><strong>4. 论文中提到的局限性：</strong>
*   <strong>泛化能力差距：</strong> 尽管 RoboReward 模型表现优异，但评估显示，即使是先进的 VLM，在不同机器人实体、场景和视角下仍存在显著的泛化能力差距。
*   <strong>奖励的细微差别：</strong> 论文提到，即使是最好的模型，也可能在细微的空间和时间细节上出错（例如，未能准确判断物体是否真正放置到位），这可能导致奖励分配的误差。
*   <strong>长时序任务的挑战：</strong> 作者指出，将奖励建模扩展到更长时序、多阶段的任务将更具挑战性，因为信用分配和进度估计会变得更加困难。</p>
<p><strong>5. 未来研究方向：</strong>
*   <strong>长时序和多阶段任务：</strong> 将奖励建模扩展到更复杂的、需要长期规划和信用分配的任务。
*   <strong>更精细的奖励信号：</strong> 探索更精细的奖励信号，以捕捉机器人任务中更微妙的成功和失败。
*   <strong>提高泛化能力：</strong> 进一步研究如何提高 VLM 在不同机器人实体、场景和视角下的泛化能力。
*   <strong>实时奖励生成：</strong> 探索更高效的奖励生成方法，使其能够集成到实时的 RL 训练循环中。</p>
<p><strong>总结：</strong>
这篇论文在机器人学领域做出了重要贡献，通过引入 RoboReward 数据集、创新的数据增强方法以及高性能的通用视觉语言奖励模型，显著推动了机器人强化学习的发展。研究结果表明，高质量的奖励模型是实现高效机器人 RL 的关键，并为未来开发更强大、更通用的机器人智能指明了方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Because OXE is success-heavy and lacks failure examples, we propose a \emph{negative examples data augmentation} pipeline that generates calibrated \emph{negatives} and \emph{near-misses} via counterfactual relabeling of successful episodes and temporal clipping to create partial-progress outcomes from the same videos.</li>
<li>Using this framework, we produce an extensive training and evaluation dataset that spans diverse tasks and embodiments and enables systematic evaluation of whether state-of-the-art VLMs can reliably provide rewards for robotics.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.00675v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.00675v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.00664v1'></a></p>
<h2 id="avatar-forcing-real-time-interactive-head-avatar-generation-for-natural-conversation"><a href="https://arxiv.org/abs/2601.00664v1">Avatar Forcing: Real-Time Interactive Head Avatar Generation for Natural Conversation</a></h2>
<p><strong>Authors:</strong> Taekyung Ki, Sangwon Jang, Jaehyeong Jo, Jaehong Yoon, Sung Ju Hwang</p>
<p><strong>Published:</strong> 2026-01-02</p>
<p><strong>Categories:</strong> cs.LG, cs.AI, cs.CV, cs.HC, cs.MM</p>
<p><strong>Abstract:</strong></p>
<p>Talking head generation creates lifelike avatars from static portraits for virtual communication and content creation. However, current models do not yet convey the feeling of truly interactive communication, often generating one-way responses that lack emotional engagement. We identify two key challenges toward truly interactive avatars: generating motion in real-time under causal constraints and learning expressive, vibrant reactions without additional labeled data. To address these challenges, we propose Avatar Forcing, a new framework for interactive head avatar generation that models real-time user-avatar interactions through diffusion forcing. This design allows the avatar to process real-time multimodal inputs, including the user's audio and motion, with low latency for instant reactions to both verbal and non-verbal cues such as speech, nods, and laughter. Furthermore, we introduce a direct preference optimization method that leverages synthetic losing samples constructed by dropping user conditions, enabling label-free learning of expressive interaction. Experimental results demonstrate that our framework enables real-time interaction with low latency (approximately 500ms), achieving 6.8X speedup compared to the baseline, and produces reactive and expressive avatar motion, which is preferred over 80% against the baseline.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Avatar Forcing: Real-Time Interactive Head Avatar Generation for Natural Conversation”的全面中文摘要：</p>
<p><strong>论文题目：</strong> Avatar Forcing: Real-Time Interactive Head Avatar Generation for Natural Conversation</p>
<p><strong>作者：</strong> Taekyung Ki, Sangwon Jang, Jaehyeong Jo, Jaehong Yoon, Sung Ju Hwang</p>
<p><strong>摘要</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
当前的面部头像生成技术虽然能够创建逼真的虚拟形象，但普遍缺乏真正交互式的沟通体验。现有的模型通常生成单向响应，情感参与度低，无法捕捉真实对话中用户与虚拟形象之间的动态互动。论文指出了实现真正交互式头像的两个关键挑战：
*   <strong>实时性与因果约束下的运动生成：</strong> 需要在低延迟下生成头像的运动，同时满足因果关系，即只能依赖过去和当前的信息，不能预知未来。
*   <strong>无额外标注数据的表达性反应学习：</strong> 学习生动、富有表现力的头像反应，而无需昂贵且难以获取的标注数据。</p>
<p><strong>2. 主要创新与方法贡献：</strong>
为了解决上述挑战，作者提出了 <strong>Avatar Forcing</strong> 框架，其核心创新包括：</p>
<ul>
<li><strong>基于扩散强制（Diffusion Forcing）的实时交互式头像生成：</strong> 该框架利用因果扩散强制技术，在运动潜在空间中建模用户与头像之间的实时交互。这使得头像能够处理实时的多模态输入（用户音频和运动），实现低延迟的即时响应。</li>
<li><strong>双重运动编码器（Dual Motion Encoder）：</strong> 该编码器能够融合用户的音频、运动以及头像自身的音频，生成一个统一的条件输入，以驱动头像的运动生成。</li>
<li><strong>因果扩散强制运动生成器（Causal DFoT Motion Generator）：</strong> 采用基于块的因果注意力机制，并引入了“前瞻性因果掩码”（Blockwise Causal Look-ahead Mask），在保证因果性的同时，实现了更平滑的帧间过渡，有效缓解了运动抖动问题。</li>
<li><strong>基于偏好优化的表达性交互学习：</strong> 引入了一种<strong>直接偏好优化（Direct Preference Optimization, DPO）</strong>方法，通过构造“劣势样本”（即仅基于头像音频生成的运动）来训练模型，从而在无需人工标注的情况下，学习更具表现力和响应性的头像运动。这种方法能够有效提升头像的交互性和丰富性。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong>
*   <strong>实时性：</strong> Avatar Forcing 实现了约 500ms 的低延迟，相比基线模型提速 6.8 倍，能够进行真正的实时交互。
*   <strong>表达性与响应性：</strong> 实验结果表明，Avatar Forcing 生成的头像运动更加生动、富有表现力，并且对用户的非语言线索（如微笑、点头）反应更灵敏。
*   <strong>用户偏好：</strong> 人类偏好研究显示，80% 以上的参与者更倾向于 Avatar Forcing 生成的头像，认为其在自然度、响应性和整体交互质量方面表现更优。
*   <strong>技术优势：</strong> 在定量评估中，Avatar Forcing 在 Reactiveness（响应性）和 Motion Richness（运动丰富性）等关键指标上显著优于现有模型（如 INFP*）。</p>
<p><strong>4. 论文中提到的局限性：</strong>
*   <strong>身体姿态的限制：</strong> 该系统主要关注头部运动，无法捕捉更丰富的身体姿态（如手势），这限制了更动态的沟通表现。
*   <strong>显式控制的不足：</strong> 在某些场景下，可能需要更精细的控制，例如直接引导眼球注视或强调情感变化，而当前模型在这方面能力有限。</p>
<p><strong>5. 未来研究方向：</strong>
*   <strong>整合更丰富的用户信号：</strong> 探索整合眼球追踪或情感追踪等额外用户信号，以实现更精细的控制和更具表现力的交互。
*   <strong>扩展到全身模型：</strong> 将模型扩展到全身，以捕捉更丰富的肢体语言，实现更动态和全面的沟通。
*   <strong>伦理考量与滥用防范：</strong> 论文也提到了该技术可能被用于身份欺骗或制作深度伪造内容（deepfakes）的风险，并鼓励社区开发相关的检测模型。</p>
<p><strong>总结：</strong>
Avatar Forcing 是一个在实时交互式头部头像生成领域的重要进展。它通过创新的扩散强制和偏好优化方法，成功解决了现有模型在实时性、表达性和交互性方面的不足。该框架能够生成高度响应和富有表现力的头像，为虚拟交流、内容创作等应用带来了更沉浸式的用户体验，并为未来更智能、更自然的虚拟人交互奠定了基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address these challenges, we propose Avatar Forcing, a new framework for interactive head avatar generation that models real-time user-avatar interactions through diffusion forcing.</li>
<li>Furthermore, we introduce a direct preference optimization method that leverages synthetic losing samples constructed by dropping user conditions, enabling label-free learning of expressive interaction.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.00664v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.00664v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.00598v1'></a></p>
<h2 id="modality-dominance-aware-optimization-for-embodied-rgb-infrared-perception"><a href="https://arxiv.org/abs/2601.00598v1">Modality Dominance-Aware Optimization for Embodied RGB-Infrared Perception</a></h2>
<p><strong>Authors:</strong> Xianhui Liu, Siqi Jiang, Yi Xie, Yuqing Lin, Siao Liu</p>
<p><strong>Published:</strong> 2026-01-02</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>RGB-Infrared (RGB-IR) multimodal perception is fundamental to embodied multimedia systems operating in complex physical environments. Although recent cross-modal fusion methods have advanced RGB-IR detection, the optimization dynamics caused by asymmetric modality characteristics remain underexplored. In practice, disparities in information density and feature quality introduce persistent optimization bias, leading training to overemphasize a dominant modality and hindering effective fusion. To quantify this phenomenon, we propose the Modality Dominance Index (MDI), which measures modality dominance by jointly modeling feature entropy and gradient contribution. Based on MDI, we develop a Modality Dominance-Aware Cross-modal Learning (MDACL) framework that regulates cross-modal optimization. MDACL incorporates Hierarchical Cross-modal Guidance (HCG) to enhance feature alignment and Adversarial Equilibrium Regularization (AER) to balance optimization dynamics during fusion. Extensive experiments on three RGB-IR benchmarks demonstrate that MDACL effectively mitigates optimization bias and achieves SOTA performance.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于RGB-红外（RGB-IR）多模态感知中优化偏差问题的论文。以下是根据您提供的PDF内容撰写的中文摘要，包含您要求的各个部分：</p>
<p><strong>论文题目：</strong> Modality Dominance-Aware Optimization for Embodied RGB-Infrared Perception</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题：</strong>
本文聚焦于RGB-红外（RGB-IR）多模态感知在具身智能系统中的应用。研究的核心问题在于，尽管现有的跨模态融合方法在RGB-IR检测方面取得了显著进展，但由于RGB和红外模态在信息密度和特征质量上的不对称性，训练过程中常常出现“优化偏差”（optimization bias）。这种偏差导致模型过度依赖某个占主导地位的模态，从而阻碍了有效的跨模态特征融合，影响了整体感知性能。</p>
<p><strong>2. 主要创新与方法贡献：</strong>
为了解决上述问题，作者提出了以下关键创新和方法：</p>
<ul>
<li><strong>模态支配指数（Modality Dominance Index, MDI）：</strong> 提出了一种新颖的度量指标MDI，用于量化不同模态在训练过程中的支配程度。MDI通过联合建模模态特征的“信息丰富度”（Representational Diversity，通过特征熵衡量）和“任务响应敏感度”（Task-Response Sensitivity，通过对检测损失的影响衡量）来动态评估模态的支配性。</li>
<li><strong>模态支配感知跨模态学习框架（Modality Dominance-Aware Cross-modal Learning, MDACL）：</strong> 基于MDI，作者构建了一个MDACL框架，旨在调节跨模态的优化过程。该框架包含两个核心组件：<ul>
<li><strong>分层跨模态引导（Hierarchical Cross-modal Guidance, HCG）：</strong> HCG通过“低级特征映射与重投影”和“高级语义蒸馏”两个阶段，增强模态间的特征对齐。低级阶段侧重于结构对齐，高级阶段侧重于语义一致性，从而有效缓解模态间的特征级错位。</li>
<li><strong>对抗性均衡正则化（Adversarial Equilibrium Regularization, AER）：</strong> AER策略通过引入“最小逆权重”（Minimal Inverse Weight, MIW）方案，动态调整模态的融合权重，抑制占主导模态的过度影响，鼓励更均衡的学习过程，以达到一种“对抗性均衡”状态。</li>
</ul>
</li>
</ul>
<p><strong>3. 主要结果与意义：</strong>
作者在LLVIP、M3FD和FLIR三个广泛使用的RGB-IR检测基准数据集上进行了大量实验。结果表明：</p>
<ul>
<li>MDACL框架能够有效缓解RGB-IR检测中的优化偏差问题。</li>
<li>该方法在所有三个基准数据集上均取得了最先进（SOTA）的性能，并在mAP和mAP50指标上实现了显著提升，尤其是在LLVIP和M3FD数据集上表现突出。</li>
<li>消融实验证明了MDI、HCG和AER各组件的有效性及其协同作用。</li>
<li>定性分析显示，MDACL能够显著减少漏检和误检，尤其是在低光照和遮挡等复杂场景下，证明了其在提升鲁棒性和准确性方面的优势。</li>
<li>研究强调了在RGB-IR多模态感知中，显式地建模和调节优化动态的重要性，这超越了传统仅关注特征融合设计的方法。</li>
</ul>
<p><strong>4. 局限性：</strong>
论文中未明确提及具体的局限性，但从其研究方向来看，可能存在的潜在局限性包括：</p>
<ul>
<li><strong>计算复杂度：</strong> 引入MDI计算和HCG、AER策略可能会增加模型的训练和推理复杂度，尽管作者声称AER的MIW方案计算开销极小。</li>
<li><strong>超参数敏感性：</strong> HCG中的权重α, β, γ以及MDI中的δ等超参数可能需要仔细调优，以达到最佳性能。</li>
<li><strong>泛化性：</strong> 虽然在三个数据集上取得了良好结果，但其在其他更具挑战性的或不同类型的RGB-IR数据集上的泛化能力仍需进一步验证。</li>
</ul>
<p><strong>5. 未来研究方向：</strong>
论文在结论部分暗示了未来的研究方向：</p>
<ul>
<li><strong>更先进的正则化范式：</strong> 探索更高级的正则化方法来进一步增强动态优化均衡，以应对更复杂的模态不平衡问题。</li>
<li><strong>更广泛的应用：</strong> 将所提出的优化感知方法扩展到其他多模态学习任务，而不仅仅局限于RGB-IR检测。</li>
<li><strong>更深入的理论分析：</strong> 对模态支配和优化偏差之间的关系进行更深入的理论研究。</li>
</ul>
<p>总而言之，这篇论文通过引入模态支配指数（MDI）和提出模态支配感知跨模态学习框架（MDACL），有效地解决了RGB-IR多模态感知中的关键挑战——优化偏差问题。其提出的方法不仅在多个基准数据集上取得了SOTA性能，而且为理解和解决多模态学习中的不对称性问题提供了新的视角和有效的解决方案。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To quantify this phenomenon, we propose the Modality Dominance Index (MDI), which measures modality dominance by jointly modeling feature entropy and gradient contribution.</li>
<li>Based on MDI, we develop a Modality Dominance-Aware Cross-modal Learning (MDACL) framework that regulates cross-modal optimization.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.00598v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.00598v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.00584v1'></a></p>
<h2 id="granalign-granularity-aware-alignment-framework-for-zero-shot-video-moment-retrieval"><a href="https://arxiv.org/abs/2601.00584v1">GranAlign: Granularity-Aware Alignment Framework for Zero-Shot Video Moment Retrieval</a></h2>
<p><strong>Authors:</strong> Mingyu Jeon, Sunjae Yoon, Jonghee Kim, Junyeoung Kim</p>
<p><strong>Published:</strong> 2026-01-02</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Zero-shot video moment retrieval (ZVMR) is the task of localizing a temporal moment within an untrimmed video using a natural language query without relying on task-specific training data. The primary challenge in this setting lies in the mismatch in semantic granularity between textual queries and visual content. Previous studies in ZVMR have attempted to achieve alignment by leveraging high-quality pre-trained knowledge that represents video and language in a joint space. However, these approaches failed to balance the semantic granularity between the pre-trained knowledge provided by each modality for a given scene. As a result, despite the high quality of each modality's representations, the mismatch in granularity led to inaccurate retrieval. In this paper, we propose a training-free framework, called Granularity-Aware Alignment (GranAlign), that bridges this gap between coarse and fine semantic representations. Our approach introduces two complementary techniques: granularity-based query rewriting to generate varied semantic granularities, and query-aware caption generation to embed query intent into video content. By pairing multi-level queries with both query-agnostic and query-aware captions, we effectively resolve semantic mismatches. As a result, our method sets a new state-of-the-art across all three major benchmarks (QVHighlights, Charades-STA, ActivityNet-Captions), with a notable 3.23% mAP@avg improvement on the challenging QVHighlights dataset.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“GranAlign: Granularity-Aware Alignment Framework for Zero-Shot Video Moment Retrieval”的全面中文摘要：</p>
<p><strong>论文题目：</strong> GranAlign: Granularity-Aware Alignment Framework for Zero-Shot Video Moment Retrieval</p>
<p><strong>作者：</strong> Mingyu Jeon, Sunjae Yoon, Jonghee Kim, Junyeoung Kim</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
本文旨在解决零样本视频时刻检索（Zero-Shot Video Moment Retrieval, ZVMR）任务中的核心挑战——<strong>语义粒度不匹配（Granularity Mismatch）</strong>。在ZVMR中，用户可能使用不同详细程度的自然语言查询来描述视频中的同一事件（例如，“一只可爱的狗” vs. “一只金毛幼犬正在散步”）。现有的方法虽然利用了高质量的预训练视觉语言模型（VLMs）和大型语言模型（LLMs），但往往未能有效平衡文本查询和视频内容之间粗粒度（高召回率）与细粒度（高精确率）的语义表示，导致检索结果不准确。</p>
<p><strong>2. 主要创新点/方法贡献：</strong>
作者提出了一个名为 <strong>Granularity-Aware Alignment (GranAlign)</strong> 的训练免费框架，以弥合粗粒度和细粒度语义表示之间的差距。其核心创新在于：</p>
<ul>
<li><strong>双路径粒度感知对齐：</strong> GranAlign摒弃了单一的查询处理路径，而是采用双路径策略：<ul>
<li><strong>查询侧（Query Side）：</strong> 利用LLM（如LLaMA-3）将原始查询重写为两个不同粒度的查询：<strong>简化查询（Simplified Query, Qs）</strong>，侧重于捕捉核心意图以实现高召回率；<strong>详细查询（Detailed Query, Qd）</strong>，保留精细的语义信息以实现高精确率。</li>
<li><strong>视频侧（Video Side）：</strong> 生成两种类型的视频描述：<strong>查询无关的字幕（Query-Agnostic Caption, Cagn）</strong>，提供通用的场景描述；<strong>查询感知的字幕（Query-Aware Caption, Cawr）</strong>，针对关键帧生成，更贴合查询意图。</li>
</ul>
</li>
<li><strong>粒度感知配对与评分：</strong> 通过将简化查询与查询无关字幕（Qs, Cagn）配对，以及将详细查询与查询感知字幕（Qd, Cawr）配对，GranAlign能够协同利用两种路径的优势。最终的时刻得分（Moment Score）是这两种配对的语义相似度得分的平均值，从而在召回率和精确率之间取得平衡。</li>
<li><strong>训练免费框架：</strong> GranAlign不需要在特定任务上进行额外的训练，而是依赖于预训练模型的能力，使其易于部署和应用。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong>
GranAlign在三个主要的ZVMR基准数据集（QVHighlights, Charades-STA, ActivityNet-Captions）上均取得了<strong>最先进（State-of-the-Art, SOTA）</strong>的性能。特别是在挑战性的QVHighlights数据集上，mAP@avg指标取得了<strong>3.23%</strong>的显著提升。实验结果表明，GranAlign有效解决了粒度不匹配问题，显著提高了检索的准确性和鲁棒性。该方法在不同类型的查询（Error, Simple, Detail, Else）上均表现出良好的泛化能力，并且对超参数设置不敏感。</p>
<p><strong>4. 论文中提到的局限性：</strong>
论文中提到，GranAlign的局限性主要源于其核心组件（如查询感知字幕生成）的生成性质。具体来说：
*   <strong>查询感知字幕的幻觉（Hallucination）：</strong> 查询感知字幕有时可能生成视频中不存在的内容，或者过度模仿查询的语言结构，导致误导。
*   <strong>对LLM和VLM的依赖：</strong> 框架的性能在一定程度上依赖于所使用的LLM和VLM的质量和能力。</p>
<p><strong>5. 未来研究方向：</strong>
基于上述局限性，论文提出了以下未来研究方向：
*   <strong>事实核查机制：</strong> 开发一种机制来验证查询感知字幕中生成的细节是否与视觉证据相符，以减少幻觉。
*   <strong>语义验证：</strong> 引入一个语义验证步骤，以确保LLM重写的查询能够准确反映原始查询的意图。
*   <strong>更通用的粒度对齐策略：</strong> 探索更通用的方法来处理不同粒度的表示，并将其应用于更广泛的多模态理解任务。</p>
<p>总而言之，GranAlign通过引入创新的双路径粒度感知对齐框架，成功解决了零样本视频时刻检索中的关键挑战，并在多个基准测试中取得了显著的性能提升，为未来的研究开辟了新的方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this paper, we propose a training-free framework, called Granularity-Aware Alignment (GranAlign), that bridges this gap between coarse and fine semantic representations.</li>
<li>Our approach introduces two complementary techniques: granularity-based query rewriting to generate varied semantic granularities, and query-aware caption generation to embed query intent into video content.</li>
<li>As a result, our method sets a new state-of-the-art across all three major benchmarks (QVHighlights, Charades-STA, ActivityNet-Captions), with a notable 3.23% mAP@avg improvement on the challenging QVHighlights dataset.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.00584v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.00584v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.00561v1'></a></p>
<h2 id="aegis-exploring-the-limit-of-world-knowledge-capabilities-for-unified-mulitmodal-models"><a href="https://arxiv.org/abs/2601.00561v1">AEGIS: Exploring the Limit of World Knowledge Capabilities for Unified Mulitmodal Models</a></h2>
<p><strong>Authors:</strong> Jintao Lin, Bowen Dong, Weikang Shi, Chenyang Lei, Suiyun Zhang, Rui Liu, Xihui Liu</p>
<p><strong>Published:</strong> 2026-01-02</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>The capability of Unified Multimodal Models (UMMs) to apply world knowledge across diverse tasks remains a critical, unresolved challenge. Existing benchmarks fall short, offering only siloed, single-task evaluations with limited diagnostic power. To bridge this gap, we propose AEGIS (\emph{i.e.}, \textbf{A}ssessing \textbf{E}diting, \textbf{G}eneration, \textbf{I}nterpretation-Understanding for \textbf{S}uper-intelligence), a comprehensive multi-task benchmark covering visual understanding, generation, editing, and interleaved generation. AEGIS comprises 1,050 challenging, manually-annotated questions spanning 21 topics (including STEM, humanities, daily life, etc.) and 6 reasoning types. To concretely evaluate the performance of UMMs in world knowledge scope without ambiguous metrics, we further propose Deterministic Checklist-based Evaluation (DCE), a protocol that replaces ambiguous prompt-based scoring with atomic ``Y/N'' judgments, to enhance evaluation reliability. Our extensive experiments reveal that most UMMs exhibit severe world knowledge deficits and that performance degrades significantly with complex reasoning. Additionally, simple plug-in reasoning modules can partially mitigate these vulnerabilities, highlighting a promising direction for future research. These results highlight the importance of world-knowledge-based reasoning as a critical frontier for UMMs.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“AEGIS: Exploring the Limit of World Knowledge Capabilities for Unified Mulitmodal Models”的中文摘要，其中包含了您要求的关键信息：</p>
<p><strong>论文摘要：AEGIS：探索统一多模态模型世界知识能力的极限</strong></p>
<p><strong>1. 研究问题与动机</strong></p>
<p>统一多模态模型（UMMs）在跨越不同任务应用世界知识的能力方面仍面临严峻挑战。现有基准测试在评估UMMs的世界知识和推理能力时存在局限性，它们通常是单一任务导向的，诊断能力有限，且评估指标不够精确。这使得深入理解UMMs在处理复杂、现实世界场景时的弱点变得困难。</p>
<p><strong>2. 主要创新与方法贡献</strong></p>
<p>为了解决上述问题，作者提出了 <strong>AEGIS</strong>（Assessing Editing, Generation, Interpretation-Understanding for Super-intelligence）基准测试。AEGIS具有以下关键创新：</p>
<ul>
<li><strong>全面的多任务基准：</strong> AEGIS包含1050个精心设计、人工标注的问题，涵盖了视觉理解、生成、编辑和交错生成四种核心任务。</li>
<li><strong>广泛的世界知识覆盖：</strong> 该基准测试跨越STEM、人文和社会生活三大领域，细分为21个主题，确保了对模型世界知识广度的全面考察。</li>
<li><strong>多样的推理类型：</strong> AEGIS引入了6种不同的推理类型（空间、时间、因果、比较、类比、逻辑），以深入评估模型在复杂推理方面的能力。</li>
<li><strong>确定性清单式评估（DCE）：</strong> 为了克服现有评分方法的模糊性和主观性，AEGIS提出了一种创新的DCE协议。该协议将复杂的评估任务分解为一系列原子化的“是/否”判断问题，通过LLM生成检查清单，从而提高了评估的客观性和可靠性。</li>
</ul>
<p><strong>3. 主要结果与意义</strong></p>
<p>通过在AEGIS基准上进行的大量实验，作者发现：</p>
<ul>
<li><strong>UMMs的世界知识缺陷显著：</strong> 大多数UMMs在应用世界知识方面表现出严重的不足，尤其是在处理复杂推理时，性能会急剧下降。</li>
<li><strong>推理能力是关键瓶颈：</strong> 模型在理解任务上表现相对较好，但在生成和编辑任务上性能有所下降，而在需要复杂推理的交错生成任务上则出现大幅下滑。这表明理解能力限制了其他任务的上限。</li>
<li><strong>简单推理模块的缓解作用：</strong> 集成简单的插件式推理模块可以在一定程度上缓解模型在世界知识方面的不足，为未来的模型改进指明了方向。</li>
<li><strong>AEGIS的诊断价值：</strong> AEGIS基准测试及其DCE评估协议能够提供更精细的诊断能力，帮助研究人员定位模型在不同任务和推理类型中的具体弱点。</li>
</ul>
<p>这项研究强调了基于世界知识的推理对于提升UMMs能力的重要性，并为未来UMM的研究和开发提供了重要的方向和评估工具。</p>
<p><strong>4. 局限性</strong></p>
<p>论文中提到，在交错生成任务中，模型在“图像-文本一致性检查”方面的判断难度较高，这增加了评估的挑战性。此外，虽然AEGIS提供了详细的诊断信息，但模型在处理非常规或模糊指令时仍存在挑战。</p>
<p><strong>5. 未来研究方向</strong></p>
<ul>
<li><strong>改进多模态推理框架：</strong> 针对模型在处理复杂推理类型（如时间、因果推理）方面的弱点，需要设计更强大的多模态推理框架。</li>
<li><strong>提升视觉解码器能力：</strong> 研究发现视觉解码器是限制UMMs世界知识能力的一个瓶颈，未来需要关注如何增强解码器对知识的编码能力或提高其对输入变化的鲁棒性。</li>
<li><strong>更精细的模块化分析：</strong> 进一步研究不同模块（如LLM组件和视觉解码器）在世界知识应用中的具体作用和相互影响。</li>
<li><strong>探索更有效的知识整合方法：</strong> 研究如何更有效地将外部知识（如通过网络搜索）与模型内部知识相结合，以提升模型在动态或实时信息处理方面的能力。</li>
</ul>
<p>总而言之，AEGIS基准测试及其评估方法为衡量和提升统一多模态模型的世界知识和推理能力提供了一个重要的平台，并揭示了当前模型在该领域面临的关键挑战和未来研究的潜在方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To bridge this gap, we propose AEGIS (\emph{i.e.}, \textbf{A}ssessing \textbf{E}diting, \textbf{G}eneration, \textbf{I}nterpretation-Understanding for \textbf{S}uper-intelligence), a comprehensive multi-task benchmark covering visual understanding, generation, editing, and interleaved generation.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.00561v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.00561v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.00553v1'></a></p>
<h2 id="a-comprehensive-dataset-for-human-vs-ai-generated-image-detection"><a href="https://arxiv.org/abs/2601.00553v1">A Comprehensive Dataset for Human vs. AI Generated Image Detection</a></h2>
<p><strong>Authors:</strong> Rajarshi Roy, Nasrin Imanpour, Ashhar Aziz, Shashwat Bajpai, Gurpreet Singh, Shwetangshu Biswas, Kapil Wanaskar, Parth Patwa, Subhankar Ghosh, Shreyas Dixit, Nilesh Ranjan Pal, Vipula Rawte, Ritvik Garimella, Gaytri Jena, Vasu Sharma, Vinija Jain, Aman Chadha, Aishwarya Naresh Reganti, Amitava Das</p>
<p><strong>Published:</strong> 2026-01-02</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>Multimodal generative AI systems like Stable Diffusion, DALL-E, and MidJourney have fundamentally changed how synthetic images are created. These tools drive innovation but also enable the spread of misleading content, false information, and manipulated media. As generated images become harder to distinguish from photographs, detecting them has become an urgent priority. To combat this challenge, We release MS COCOAI, a novel dataset for AI generated image detection consisting of 96000 real and synthetic datapoints, built using the MS COCO dataset. To generate synthetic images, we use five generators: Stable Diffusion 3, Stable Diffusion 2.1, SDXL, DALL-E 3, and MidJourney v6. Based on the dataset, we propose two tasks: (1) classifying images as real or generated, and (2) identifying which model produced a given synthetic image. The dataset is available at https://huggingface.co/datasets/Rajarshi-Roy-research/Defactify_Image_Dataset.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为一名计算机视觉和机器学习领域的专家，我将为您提供这篇论文的全面中文摘要。</p>
<p><strong>论文题目：</strong> A Comprehensive Dataset for Human vs. AI Generated Image Detection</p>
<p><strong>作者：</strong> Rajarshi Roy, Nasrin Imanpour, Ashhar Aziz, Shashwat Bajpai, Gurpreet Singh, Shwetangshu Biswas, Kapil Wanaskar, Parth Patwa, Subhankar Ghosh, Shreyas Dixit, Nilesh Ranjan Pal, Vipula Rawte, Ritvik Garimella, Gaytri Jena, Vasu Sharma, Vinija Jain, Aman Chadha, Aishwarya Naresh Reganti, Amitava Das</p>
<hr />
<p><strong>论文全面中文摘要</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong></p>
<p>随着Stable Diffusion、DALL-E和MidJourney等生成式AI技术的飞速发展，合成图像的生成能力得到了质的飞跃，使得区分真实图像与AI生成图像变得越来越困难。这带来了严重的社会风险，包括误导性信息、虚假新闻和操纵性媒体的传播，对公众信任和信息生态系统构成了威胁。因此，开发鲁棒的AI生成图像检测技术已成为一项紧迫的任务。</p>
<p><strong>2. 主要创新点/方法论贡献：</strong></p>
<ul>
<li><strong>发布MS COCOAI数据集：</strong> 该论文的核心贡献是发布了一个名为MS COCOAI的新型数据集，专门用于AI生成图像检测。该数据集包含96,000个真实和合成图像对，并构建在MS COCO数据集之上。</li>
<li><strong>多模型覆盖与语义对齐：</strong> MS COCOAI数据集利用了五种主流的生成模型（Stable Diffusion 3, Stable Diffusion 2.1, SDXL, DALL-E 3, 和 MidJourney v6）来生成合成图像。关键创新在于，所有合成图像都是基于MS COCO数据集中真实图像的相同文本描述（caption）生成的，实现了“语义对齐”。这种对齐方式能够有效分离内容偏差与生成模型本身的伪影，为研究提供了更可控的环境。</li>
<li><strong>引入两种检测任务：</strong> 基于该数据集，论文提出了两个核心任务：<ul>
<li><strong>任务A（二分类）：</strong> 判断一张图像是真实图像还是AI生成图像。</li>
<li><strong>任务B（多分类/模型识别）：</strong> 识别出生成特定合成图像的具体AI模型。</li>
</ul>
</li>
<li><strong>鲁棒性评估：</strong> 数据集还包含了对生成图像进行的四种独立扰动（水平翻转、亮度降低、高斯噪声、JPEG压缩），旨在评估检测方法的鲁棒性。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong></p>
<ul>
<li><strong>基线模型性能：</strong> 论文使用ResNet-50模型作为基线，在频率域特征上进行了实验。<ul>
<li>在任务A（真实/AI二分类）上，基线模型取得了0.80144的得分，表明使用相对简单的方法进行二分类检测是可行的。</li>
<li>在任务B（模型识别）上，基线模型仅取得了0.44913的得分，这显著低于任务A的得分。</li>
</ul>
</li>
<li><strong>结果意义：</strong><ul>
<li><strong>检测难度差异：</strong> 结果清晰地表明，模型识别（多分类）比简单的真实/AI二分类要困难得多，凸显了区分不同生成模型的技术挑战。</li>
<li><strong>数据集价值：</strong> MS COCOAI数据集为AI生成图像检测和模型归因研究提供了一个大规模、高质量且具有语义对齐特性的基准。它能够帮助研究人员更深入地理解生成模型的特性，并开发更先进的检测技术。</li>
<li><strong>信息完整性：</strong> 数据集包含了图像、文本描述、真实/AI标签、模型标签以及扰动版本，为多模态分析和鲁棒性研究提供了全面的支持。</li>
</ul>
</li>
</ul>
<p><strong>4. 提及的局限性：</strong></p>
<ul>
<li><strong>模型识别的挑战：</strong> 论文明确指出，模型识别任务的基线性能较低，表明当前的技术在区分不同生成模型方面仍有很大提升空间。</li>
<li><strong>现有数据集的不足：</strong> 论文在“Related Work”部分回顾了现有数据集，并指出了它们的局限性，例如缺乏语义对齐、模型标签不精细、图像质量参差不齐等，从而突显了MS COCOAI的必要性。</li>
<li><strong>基线方法的局限：</strong> 论文使用的频率域特征基线方法虽然有效，但可能无法捕捉所有细微的生成伪影，尤其是在面对更先进的生成模型时。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>更精细的模型指纹技术：</strong> 开发更先进的“指纹”技术，以更精确地识别不同生成模型的细微差异。</li>
<li><strong>跨模态学习：</strong> 利用文本描述和图像之间的关联性，探索更有效的跨模态学习方法来提升检测性能。</li>
<li><strong>增强检测器的鲁棒性：</strong> 进一步研究如何提高检测器对常见图像变换（如压缩、裁剪、噪声等）的鲁棒性。</li>
<li><strong>探索新的检测特征：</strong> 除了频率域特征，还可以探索其他类型的特征，如语义特征、纹理特征、模型特定的伪影等。</li>
<li><strong>对抗性攻击与防御：</strong> 研究针对AI生成图像检测器的对抗性攻击，并开发相应的防御策略。</li>
</ul>
<p><strong>总结：</strong></p>
<p>这篇论文通过发布MS COCOAI数据集，为AI生成图像检测领域做出了重要贡献。该数据集的独特之处在于其大规模、多模型覆盖以及关键的语义对齐特性，为研究人员提供了一个强大的工具来评估和开发更先进的检测和模型归因技术。论文通过基线实验揭示了模型识别的巨大挑战，并为未来的研究指明了方向，强调了在日益复杂的AI生成内容环境中维护信息真实性的重要性。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To combat this challenge, We release MS COCOAI, a novel dataset for AI generated image detection consisting of 96000 real and synthetic datapoints, built using the MS COCO dataset.</li>
<li>Based on the dataset, we propose two tasks: (1) classifying images as real or generated, and (2) identifying which model produced a given synthetic image.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.00553v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.00553v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2026-01-05 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
