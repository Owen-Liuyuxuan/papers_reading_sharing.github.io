<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-11-21 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-20
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-11-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-25
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-11-20/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-11-24/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-11-21">Arxiv Computer Vision Papers - 2025-11-21</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#dataset-distillation-for-pre-trained-self-supervised-vision-models" class="nav-link">Dataset Distillation for Pre-Trained Self-Supervised Vision Models</a>
                </li>
                <li class="nav-item">
                    <a href="#evolmm-self-evolving-large-multimodal-models-with-continuous-rewards" class="nav-link">EvoLMM: Self-Evolving Large Multimodal Models with Continuous Rewards</a>
                </li>
                <li class="nav-item">
                    <a href="#nopo-avatar-generalizable-and-animatable-avatars-from-sparse-inputs-without-human-poses" class="nav-link">NoPo-Avatar: Generalizable and Animatable Avatars from Sparse Inputs without Human Poses</a>
                </li>
                <li class="nav-item">
                    <a href="#thinking-while-generating-interleaving-textual-reasoning-throughout-visual-generation" class="nav-link">Thinking-while-Generating: Interleaving Textual Reasoning throughout Visual Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#learning-to-think-fast-and-slow-for-visual-language-models" class="nav-link">Learning to Think Fast and Slow for Visual Language Models</a>
                </li>
                <li class="nav-item">
                    <a href="#video-as-answer-predict-and-generate-next-video-event-with-joint-grpo" class="nav-link">Video-as-Answer: Predict and Generate Next Video Event with Joint-GRPO</a>
                </li>
                <li class="nav-item">
                    <a href="#v-reasonbench-toward-unified-reasoning-benchmark-suite-for-video-generation-models" class="nav-link">V-ReasonBench: Toward Unified Reasoning Benchmark Suite for Video Generation Models</a>
                </li>
                <li class="nav-item">
                    <a href="#scenedesigner-controllable-multi-object-image-generation-with-9-dof-pose-manipulation" class="nav-link">SceneDesigner: Controllable Multi-Object Image Generation with 9-DoF Pose Manipulation</a>
                </li>
                <li class="nav-item">
                    <a href="#tridiff-4d-fast-4d-generation-through-diffusion-based-triplane-re-posing" class="nav-link">TriDiff-4D: Fast 4D Generation through Diffusion-based Triplane Re-posing</a>
                </li>
                <li class="nav-item">
                    <a href="#dexterity-from-smart-lenses-multi-fingered-robot-manipulation-with-in-the-wild-human-demonstrations" class="nav-link">Dexterity from Smart Lenses: Multi-Fingered Robot Manipulation with In-the-Wild Human Demonstrations</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-11-21">Arxiv Computer Vision Papers - 2025-11-21</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>好的，作为一名专注于计算机视觉和机器学习的研究助理，我将为您提供一份简明的 Arxiv 计算机视觉领域论文每日报告执行摘要。</p>
<hr />
<p><strong>Arxiv 计算机视觉领域论文每日报告 - 执行摘要 (2025-11-20)</strong></p>
<p><strong>主要主题与趋势：</strong></p>
<p>本期 Arxiv 论文集中体现了计算机视觉领域在以下几个关键方向的快速进展：</p>
<ul>
<li><strong>多模态模型与生成能力增强：</strong> 大量研究聚焦于提升多模态模型（特别是视觉-语言模型）的理解和生成能力，包括更强的推理、规划和内容生成。</li>
<li><strong>可控与泛化性生成：</strong> 强调对图像和视频生成过程的精细控制，以及模型在不同场景和输入下的泛化能力，尤其是在 avatar 生成和场景设计方面。</li>
<li><strong>自监督学习与数据效率：</strong> 探索如何更有效地利用预训练模型，通过数据蒸馏等技术来提升模型在下游任务上的表现，降低对大规模标注数据的依赖。</li>
<li><strong>机器人操作与具身智能：</strong> 将计算机视觉技术应用于机器人领域，实现更精细、更具适应性的操作，特别是从人类演示中学习。</li>
</ul>
<p><strong>亮点与创新：</strong></p>
<ul>
<li><strong>EvoLMM: Self-Evolving Large Multimodal Models with Continuous Rewards</strong> 提出了一种“自进化”的大型多模态模型，通过持续奖励机制实现模型的自主演进，这可能为未来模型能力的持续提升提供新思路。</li>
<li><strong>NoPo-Avatar: Generalizable and Animatable Avatars from Sparse Inputs without Human Poses</strong> 在 avatar 生成领域取得了重要突破，实现了在缺乏人体姿态信息的情况下，从稀疏输入生成可泛化且可驱动的 avatar，极大地降低了 avatar 创建的门槛。</li>
<li><strong>Thinking-while-Generating: Interleaving Textual Reasoning throughout Visual Generation</strong> 创新性地将文本推理过程与视觉生成过程相结合，使模型在生成图像时能够进行“思考”，有望提升生成内容的逻辑性和连贯性。</li>
</ul>
<p><strong>新兴研究方向与技术：</strong></p>
<ul>
<li><strong>连续奖励驱动的模型进化：</strong> EvoLMM 论文预示着利用连续奖励机制驱动模型自主学习和优化的新方向。</li>
<li><strong>无姿态约束的 avatar 生成：</strong> NoPo-Avatar 的工作表明，摆脱对精确人体姿态的依赖是实现更通用 avatar 生成的关键。</li>
<li><strong>视觉生成中的显式推理：</strong> Thinking-while-Generating 和 Learning to Think Fast and Slow for Visual Language Models 都强调了在视觉生成过程中融入显式推理步骤的重要性。</li>
<li><strong>4D 生成的效率提升：</strong> TriDiff-4D 论文展示了通过扩散模型和三平面表示来加速 4D 内容生成的技术潜力。</li>
</ul>
<p><strong>建议阅读全文的论文：</strong></p>
<p>考虑到其潜在的广泛影响和技术创新性，以下论文值得深入阅读：</p>
<ol>
<li><strong>EvoLMM: Self-Evolving Large Multimodal Models with Continuous Rewards:</strong> 对于关注模型自主学习和能力提升的研究者。</li>
<li><strong>NoPo-Avatar: Generalizable and Animatable Avatars from Sparse Inputs without Human Poses:</strong> 对于在 avatar 生成、3D 重建和内容创作领域的研究者。</li>
<li><strong>Thinking-while-Generating: Interleaving Textual Reasoning throughout Visual Generation:</strong> 对于希望提升视觉生成模型推理能力和逻辑性的研究者。</li>
<li><strong>Dataset Distillation for Pre-Trained Self-Supervised Vision Models:</strong> 对于关注数据效率和模型微调的研究者，特别是利用现有预训练模型。</li>
</ol>
<hr />
<p>这份摘要旨在帮助您快速了解本期 Arxiv 论文的重点，并指导您进一步深入研究。</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2511.16674v1">Dataset Distillation for Pre-Trained Self-Supervised Vision Models</a></li>
<li><a href="#2511.16672v1">EvoLMM: Self-Evolving Large Multimodal Models with Continuous Rewards</a></li>
<li><a href="#2511.16673v1">NoPo-Avatar: Generalizable and Animatable Avatars from Sparse Inputs without Human Poses</a></li>
<li><a href="#2511.16671v1">Thinking-while-Generating: Interleaving Textual Reasoning throughout Visual Generation</a></li>
<li><a href="#2511.16670v1">Learning to Think Fast and Slow for Visual Language Models</a></li>
<li><a href="#2511.16669v1">Video-as-Answer: Predict and Generate Next Video Event with Joint-GRPO</a></li>
<li><a href="#2511.16668v1">V-ReasonBench: Toward Unified Reasoning Benchmark Suite for Video Generation Models</a></li>
<li><a href="#2511.16666v1">SceneDesigner: Controllable Multi-Object Image Generation with 9-DoF Pose Manipulation</a></li>
<li><a href="#2511.16662v1">TriDiff-4D: Fast 4D Generation through Diffusion-based Triplane Re-posing</a></li>
<li><a href="#2511.16661v1">Dexterity from Smart Lenses: Multi-Fingered Robot Manipulation with In-the-Wild Human Demonstrations</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2511.16674v1'></a></p>
<h2 id="dataset-distillation-for-pre-trained-self-supervised-vision-models"><a href="https://arxiv.org/abs/2511.16674v1">Dataset Distillation for Pre-Trained Self-Supervised Vision Models</a></h2>
<p><strong>Authors:</strong> George Cazenavette, Antonio Torralba, Vincent Sitzmann</p>
<p><strong>Published:</strong> 2025-11-20</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>The task of dataset distillation aims to find a small set of synthetic images such that training a model on them reproduces the performance of the same model trained on a much larger dataset of real samples. Existing distillation methods focus on synthesizing datasets that enable training randomly initialized models. In contrast, state-of-the-art vision approaches are increasingly building on large, pre-trained self-supervised models rather than training from scratch. In this paper, we investigate the problem of distilling datasets that enable us to optimally train linear probes on top of such large, pre-trained vision models. We introduce a method of dataset distillation for this task called Linear Gradient Matching that optimizes the synthetic images such that, when passed through a pre-trained feature extractor, they induce gradients in the linear classifier similar to those produced by the real data. Our method yields synthetic data that outperform all real-image baselines and, remarkably, generalize across pre-trained vision models, enabling us, for instance, to train a linear CLIP probe that performs competitively using a dataset distilled via a DINO backbone. Further, we show that our distilled datasets are exceptionally effective for fine-grained classification and provide a valuable tool for model interpretability, predicting, among other things, how similar two models' embedding spaces are under the platonic representation hypothesis or whether a model is sensitive to spurious correlations in adversarial datasets.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为一名计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行深入分析。</p>
<p><strong>论文摘要分析：</strong></p>
<p><strong>Title:</strong> Dataset Distillation for Pre-Trained Self-Supervised Vision Models
<strong>Authors:</strong> George Cazenavette, Antonio Torralba, Vincent Sitzmann
<strong>Categories:</strong> cs.CV, cs.AI, cs.LG
<strong>Published Date:</strong> 2025-11-20</p>
<p><strong>Abstract:</strong>
The task of dataset distillation aims to find a small set of synthetic images such that training a model on them reproduces the performance of the same model trained on a much larger dataset of real samples. Existing distillation methods focus on synthesizing datasets that enable training randomly initialized models. In contrast, state-of-the-art vision approaches are increasingly building on large, pre-trained self-supervised models rather than training from scratch. In this paper, we investigate the problem of distilling datasets that enable us to optimally train linear probes on top of such large, pre-trained vision models. We introduce a method of dataset distillation for this task called Linear Gradient Matching that optimizes the synthetic images such that, when passed through a pre-trained feature extractor, they induce gradients in the linear classifier similar to those produced by the real data. Our method yields synthetic data that outperform all real-image baselines and, remarkably, generalize across pre-trained vision models, enabling us, for instance, to train a linear CLIP probe that performs competitively using a dataset distilled via a DINO backbone. Further, we show that our distilled datasets are exceptionally effective for fine-grained classification and provide a valuable tool for model interpretability, predicting, among other things, how similar two models' embedding spaces are under the platonic representation hypothesis or whether a model is sensitive to spurious correlations in adversarial datasets.</p>
<hr />
<p><strong>1. 论文的主要贡献（2-3句话的简洁总结）</strong></p>
<p>本研究提出了一种新颖的数据集蒸馏方法——线性梯度匹配（Linear Gradient Matching），专门用于优化预训练的自监督视觉模型。该方法能够生成少量合成图像，使得在这些合成图像上训练的线性探针（linear probes）能够达到与在海量真实数据上训练相当甚至更好的性能。这项工作解决了现有数据集蒸馏方法在处理预训练模型时的局限性，并展示了其在跨模型泛化、细粒度分类和模型可解释性方面的潜力。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<ul>
<li><strong>核心创新：</strong> 论文的核心创新在于将数据集蒸馏的目标从“训练随机初始化模型”转向“优化预训练模型的线性探针”。这与当前主流的自监督预训练模型的使用范式（即在预训练模型之上进行微调或训练线性探针）高度契合。</li>
<li><strong>方法论：线性梯度匹配 (Linear Gradient Matching)</strong><ul>
<li><strong>目标：</strong> 优化合成图像，使其在通过预训练特征提取器后，能够产生与真实数据在训练线性分类器时相似的梯度。</li>
<li><strong>机制：</strong> 论文没有直接优化分类准确率，而是通过匹配梯度来间接实现性能的提升。这意味着合成图像的设计是为了“模拟”真实数据在反向传播过程中对线性分类器权重更新的影响。这种方法更加精细，能够更好地捕捉预训练模型在特定任务上的学习动态。</li>
<li><strong>优势：</strong> 这种基于梯度的匹配方式，能够更有效地引导合成数据学习到预训练模型所提取的、对线性分类器至关重要的特征。</li>
</ul>
</li>
</ul>
<p><strong>3. 对该领域的潜在影响</strong></p>
<ul>
<li><strong>降低数据依赖性：</strong> 数据集蒸馏的本质是大幅减少训练所需的数据量。这项研究的成功将极大地降低使用大型预训练模型进行下游任务（尤其是线性探针任务）的数据需求，使得在数据受限的环境下也能高效地利用强大的预训练模型。</li>
<li><strong>加速模型开发和部署：</strong> 更小的数据集意味着更快的训练和评估周期，这对于快速迭代模型、进行A/B测试以及在资源受限的设备上部署模型具有重要意义。</li>
<li><strong>推动自监督学习的实用化：</strong> 自监督学习模型（如DINO, CLIP等）的强大能力需要大量数据来充分发挥。这项研究为更便捷地利用这些模型提供了新的途径，进一步推动了自监督学习在实际应用中的普及。</li>
<li><strong>促进模型比较和理解：</strong> 论文展示了蒸馏数据集在模型可解释性方面的应用，例如比较不同模型的嵌入空间相似性，或者检测模型对对抗性扰动的敏感性。这为深入理解和分析模型行为提供了新的工具。</li>
<li><strong>跨模型泛化能力：</strong> 蒸馏出的数据集能够跨不同的预训练模型（例如，用DINO蒸馏的数据训练CLIP的线性探针）表现良好，这表明蒸馏过程捕捉到了更通用的、与特定模型架构无关的“数据本质”，具有重要的理论和实践意义。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用</strong></p>
<ul>
<li><strong>资源受限的设备：</strong> 在移动端、嵌入式设备等计算和存储资源有限的场景下，使用蒸馏数据集进行模型训练或微调将是理想选择。</li>
<li><strong>隐私保护：</strong> 合成数据可以避免使用敏感的真实数据，从而在保护用户隐私的同时进行模型训练。</li>
<li><strong>数据增强和合成：</strong> 这种数据集蒸馏技术可以看作是一种高级的数据增强形式，能够生成更具代表性和学习价值的合成样本。</li>
<li><strong>模型压缩和知识蒸馏：</strong> 虽然与传统的模型压缩不同，但数据集蒸馏的目标是压缩数据，间接实现了模型训练的效率提升，与知识蒸馏在“压缩模型能力”上有异曲同工之妙。</li>
<li><strong>教育和研究：</strong> 研究人员和学生可以使用蒸馏数据集快速实验和学习，而无需处理庞大的真实数据集。</li>
<li><strong>细粒度分类：</strong> 论文明确提到在细粒度分类任务上的有效性，这在生物学（如物种识别）、工业（如缺陷检测）等领域有广泛应用。</li>
<li><strong>模型鲁棒性研究：</strong> 用于检测模型对对抗性样本敏感性的能力，有助于开发更鲁棒的模型。</li>
</ul>
<p><strong>5. 可从摘要推断的局限性</strong></p>
<ul>
<li><strong>计算成本：</strong> 数据集蒸馏本身通常是一个计算密集型的过程，需要迭代优化合成图像。虽然最终训练线性探针的成本很低，但生成蒸馏数据集的初始成本可能较高。</li>
<li><strong>“Platonic Representation Hypothesis”的假设：</strong> 论文提到“under the platonic representation hypothesis”，这意味着其模型解释性分析是建立在这个特定假设之上的。如果这个假设不成立或不适用于所有情况，那么相关的解释性结论的普适性可能会受到影响。</li>
<li><strong>对“线性探针”的侧重：</strong> 该方法专门优化用于训练线性探针。虽然线性探针是评估预训练模型特征提取能力的一种常用且有效的方式，但它并不能完全代表在预训练模型之上进行更复杂微调（如训练多层MLP或CNN）时的性能。因此，该方法在更复杂的下游任务上的直接效果仍需进一步验证。</li>
<li><strong>泛化性的边界：</strong> 尽管论文展示了跨模型的泛化能力，但这种泛化能力是否能无限扩展到完全不同架构或训练范式的模型，以及在多大程度上保持性能，仍是一个需要深入研究的问题。</li>
<li><strong>合成数据的“真实性”：</strong> 合成数据虽然能模拟真实数据的学习信号，但其视觉上的真实感和多样性可能与真实数据存在差异。在某些对视觉保真度要求极高的应用中，这可能是一个需要考虑的因素。</li>
</ul>
<p>总而言之，这篇论文提出了一种非常有前景的数据集蒸馏方法，它巧妙地解决了当前预训练模型使用中的一个关键痛点，并为数据效率、模型理解和应用开辟了新的可能性。其核心创新在于将蒸馏目标从训练整个模型转移到优化预训练特征提取器之上的线性层，通过匹配梯度来生成高效的合成数据。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In contrast, state-of-the-art vision approaches are increasingly building on large, pre-trained self-supervised models rather than training from scratch.</li>
<li>We introduce a method of dataset distillation for this task called Linear Gradient Matching that optimizes the synthetic images such that, when passed through a pre-trained feature extractor, they induce gradients in the linear classifier similar to those produced by the real data.</li>
<li>Our method yields synthetic data that outperform all real-image baselines and, remarkably, generalize across pre-trained vision models, enabling us, for instance, to train a linear CLIP probe that performs competitively using a dataset distilled via a DINO backbone.</li>
<li>Further, we show that our distilled datasets are exceptionally effective for fine-grained classification and provide a valuable tool for model interpretability, predicting, among other things, how similar two models' embedding spaces are under the platonic representation hypothesis or whether a model is sensitive to spurious correlations in adversarial datasets.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.16674v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.16674v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.16672v1'></a></p>
<h2 id="evolmm-self-evolving-large-multimodal-models-with-continuous-rewards"><a href="https://arxiv.org/abs/2511.16672v1">EvoLMM: Self-Evolving Large Multimodal Models with Continuous Rewards</a></h2>
<p><strong>Authors:</strong> Omkat Thawakar, Shravan Venkatraman, Ritesh Thawkar, Abdelrahman Shaker, Hisham Cholakkal, Rao Muhammad Anwer, Salman Khan, Fahad Khan</p>
<p><strong>Published:</strong> 2025-11-20</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Recent advances in large multimodal models (LMMs) have enabled impressive reasoning and perception abilities, yet most existing training pipelines still depend on human-curated data or externally verified reward models, limiting their autonomy and scalability. In this work, we strive to improve LMM reasoning capabilities in a purely unsupervised fashion (without any annotated data or reward distillation). To this end, we propose a self-evolving framework, named EvoLMM, that instantiates two cooperative agents from a single backbone model: a Proposer, which generates diverse, image-grounded questions, and a Solver, which solves them through internal consistency, where learning proceeds through a continuous self-rewarding process. This dynamic feedback encourages both the generation of informative queries and the refinement of structured reasoning without relying on ground-truth or human judgments. When using the popular Qwen2.5-VL as the base model, our EvoLMM yields consistent gains upto <script type="math/tex">\sim</script>3\% on multimodal math-reasoning benchmarks, including ChartQA, MathVista, and MathVision, using only raw training images. We hope our simple yet effective approach will serve as a solid baseline easing future research in self-improving LMMs in a fully-unsupervised fashion. Our code and models are available at https://github.com/mbzuai-oryx/EvoLMM.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“EvoLMM: Self-Evolving Large Multimodal Models with Continuous Rewards”的全面中文摘要：</p>
<p><strong>论文题目：</strong> EvoLMM: Self-Evolving Large Multimodal Models with Continuous Rewards</p>
<p><strong>作者：</strong> Omkar Thawakar, Shravan Venkatraman, Ritesh Thawkar, Abdelrahman M. Shaker, Hisham Cholakkal, Rao Muhammad Anwer, Salman Khan, Fahad Khan</p>
<p><strong>摘要：</strong></p>
<p>这篇论文提出了一种名为 <strong>EvoLMM</strong> 的 <strong>完全无监督的自演化框架</strong>，旨在提升大型多模态模型（LMMs）的推理能力，而无需依赖人类标注数据、元数据或外部奖励模型。</p>
<p><strong>1. 研究问题/核心挑战：</strong>
当前 LMM 的训练严重依赖于人类精心标注的数据集或外部验证的奖励模型，这限制了模型的自主性和可扩展性。尤其是在多模态推理任务中，如何让模型在缺乏明确监督的情况下自我提升，是一个关键的研究挑战。</p>
<p><strong>2. 主要创新/方法贡献：</strong>
EvoLMM 的核心创新在于其 <strong>自洽性连续奖励机制</strong> 和 <strong>Proposer-Solver 协同演化</strong> 的框架。
*   <strong>Proposer-Solver 架构：</strong> 从一个单一的 LMM 模型中实例化出两个协作代理：
    *   <strong>Proposer（提问者）：</strong> 根据输入的原始图像生成多样化且与图像内容相关的多模态问题。
    *   <strong>Solver（解答者）：</strong> 尝试回答 Proposer 生成的问题，并通过内部一致性来学习。
*   <strong>连续自洽性奖励：</strong> 论文摒弃了离散的、基于多数投票的奖励信号，而是引入了一种 <strong>连续的自洽性奖励</strong>。该奖励基于 Solver 对同一问题的多个独立回答之间的一致性程度。这种连续信号提供了更平滑的梯度，即使在早期阶段 Solver 的回答高度可变时也能稳定优化，从而避免了模型崩溃。
*   <strong>熵引导的 Proposer 奖励：</strong> 为了避免生成过于简单或过于困难的问题，Proposer 的奖励被设计成一个基于 Solver 回答熵的 <strong>带通函数</strong>。这鼓励 Proposer 生成中等难度的、能够有效激发 Solver 推理能力的问题，从而形成一个自适应的学习课程。
*   <strong>完全无监督：</strong> 整个训练过程仅使用原始图像，不依赖任何问答对、元数据或外部评估器。</p>
<p><strong>3. 主要结果与意义：</strong>
*   <strong>性能提升：</strong> 在使用 Qwen2.5-VL 作为基础模型时，EvoLMM 在多个多模态数学推理基准（如 ChartQA, MathVista, MathVision）上取得了显著的性能提升，<strong>最高可达约 3% 的绝对增益</strong>。
*   <strong>稳定性与泛化性：</strong> 实验表明，连续奖励信号比离散奖励信号更能稳定训练，并能有效提升 Solver 的推理一致性。该方法在多种 LMM 主干模型（如 Qwen2.5-VL, InternVL3-8B, Gemma-3-12B, Llama-3.2-11B）上都表现出良好的泛化能力，证明了其<strong>模型无关性</strong>。
*   <strong>自演化能力：</strong> 定性分析表明，Proposer 能够生成越来越复杂的问题，而 Solver 则能发展出更结构化的推理链，这表明模型确实实现了<strong>涌现式的自我演化</strong>。
*   <strong>意义：</strong> EvoLMM 的成功展示了 <strong>连续自洽性奖励</strong> 作为一种有效的无监督学习信号，能够驱动 LMM 在缺乏外部监督的情况下实现自我改进。这为在数据稀缺或标注成本高昂的场景下训练 LMMs 提供了一条有前景的路径。</p>
<p><strong>4. 局限性：</strong>
*   论文提到，在某些情况下，<strong>全参数微调</strong>（Full Fine-Tuning）虽然初始学习速度快，但可能导致模型不稳定，Proposer 容易出现重复或低多样性的问题，并且可能偏离预训练模型的对齐。
*   <strong>QLORA</strong> 方法虽然节省内存，但量化噪声可能会轻微削弱 Solver 的一致性。</p>
<p><strong>5. 未来研究方向：</strong>
*   <strong>课程学习的涌现：</strong> EvoLMM 展示了模型能够自动生成一个学习课程，未来可以进一步探索如何更精细地控制和引导这种课程的出现。
*   <strong>自生成数据扩展：</strong> 研究如何利用 EvoLMM 生成更多样化、更具挑战性的数据，以进一步扩展模型的学习范围。
*   <strong>长时域推理：</strong> 探索 EvoLMM 在需要更长推理链或更复杂逻辑的任务上的应用。
*   <strong>完全开放式的自改进智能：</strong> 将 EvoLMM 的思想扩展到更广泛的领域，以实现更通用的、无监督的自我改进智能体。</p>
<p>总而言之，EvoLMM 是一项重要的工作，它通过引入创新的连续自洽性奖励和 Proposer-Solver 架构，成功实现了 LMM 在完全无监督条件下的自我提升，为未来 LMM 的自主学习和发展开辟了新的可能性。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To this end, we propose a self-evolving framework, named EvoLMM, that instantiates two cooperative agents from a single backbone model: a Proposer, which generates diverse, image-grounded questions, and a Solver, which solves them through internal consistency, where learning proceeds through a continuous self-rewarding process.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.16672v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.16672v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.16673v1'></a></p>
<h2 id="nopo-avatar-generalizable-and-animatable-avatars-from-sparse-inputs-without-human-poses"><a href="https://arxiv.org/abs/2511.16673v1">NoPo-Avatar: Generalizable and Animatable Avatars from Sparse Inputs without Human Poses</a></h2>
<p><strong>Authors:</strong> Jing Wen, Alexander G. Schwing, Shenlong Wang</p>
<p><strong>Published:</strong> 2025-11-20</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We tackle the task of recovering an animatable 3D human avatar from a single or a sparse set of images. For this task, beyond a set of images, many prior state-of-the-art methods use accurate "ground-truth" camera poses and human poses as input to guide reconstruction at test-time. We show that pose-dependent reconstruction degrades results significantly if pose estimates are noisy. To overcome this, we introduce NoPo-Avatar, which reconstructs avatars solely from images, without any pose input. By removing the dependence of test-time reconstruction on human poses, NoPo-Avatar is not affected by noisy human pose estimates, making it more widely applicable. Experiments on challenging THuman2.0, XHuman, and HuGe100K data show that NoPo-Avatar outperforms existing baselines in practical settings (without ground-truth poses) and delivers comparable results in lab settings (with ground-truth poses).</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“NoPo-Avatar: Generalizable and Animatable Avatars from Sparse Inputs without Human Poses”的全面中文摘要：</p>
<p><strong>论文题目：</strong> NoPo-Avatar: Generalizable and Animatable Avatars from Sparse Inputs without Human Poses (NoPo-Avatar：来自稀疏输入且无需人体姿态的可泛化和可驱动化身)</p>
<p><strong>作者：</strong> Jing Wen, Alexander G. Schwing, Shenlong Wang</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
该论文旨在解决从单张或稀疏图像中恢复可驱动（animatable）的3D人体化身（avatar）的问题。现有的大多数先进方法在测试时需要准确的“真实”相机姿态和人体姿态作为输入来指导3D重建。然而，研究表明，这种对姿态的依赖性会导致重建结果对姿态估计的噪声非常敏感，从而显著降低性能。在实际应用场景中，姿态估计往往不准确，这使得现有方法难以获得高质量的重建。</p>
<p><strong>2. 主要创新点/方法贡献：</strong>
为了克服上述挑战，作者提出了 <strong>NoPo-Avatar</strong>，一个创新的模型，它<strong>完全消除了对测试时人体姿态和相机姿态的依赖</strong>，仅从输入图像中进行化身重建。其核心创新点包括：</p>
<ul>
<li><strong>无姿态重建：</strong> NoPo-Avatar 的关键在于其重建模块不接收任何姿态信息，从而使其对输入姿态的准确性完全不敏感，大大提高了其在真实世界场景中的鲁棒性和适用性。</li>
<li><strong>双分支架构：</strong> 模型采用一种新颖的双分支架构，包括一个<strong>模板分支</strong>和一个<strong>图像分支</strong>。<ul>
<li><strong>模板分支：</strong> 负责捕捉人体整体结构，并能够“修复”输入图像中缺失的区域（inpainting）。它基于一个平均的SMPL-X模板进行编码。</li>
<li><strong>图像分支：</strong> 负责捕捉输入图像中的精细细节，通过预测像素对齐的高斯（splatter images）来聚焦于可见区域。</li>
<li>这两种分支的结合，使得模型既能捕捉全局结构，又能还原局部细节，并能有效处理未见区域。</li>
</ul>
</li>
<li><strong>规范T-pose表示：</strong> 重建的化身表示为规范的T-pose，这使得化身可以方便地通过线性混合蒙皮（LBS）进行任意姿态的动画，而无需额外的后处理步骤。</li>
<li><strong>端到端训练：</strong> 整个模型（重建和渲染）采用端到端的方式进行训练，结合了光度损失、高斯辅助正则化以及LBS权重损失。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong>
*   <strong>性能优越性：</strong> 在THuman2.0、XHuman和HuGe100K等具有挑战性的数据集上进行实验，NoPo-Avatar 在<strong>实际场景（不使用真实姿态）下显著优于现有基线方法</strong>。
*   <strong>可比的实验室性能：</strong> 在<strong>实验室设置（使用真实姿态）下，NoPo-Avatar 也能达到与现有最先进方法相当的结果</strong>，有时甚至能捕捉到更锐利的细节。
*   <strong>鲁棒性：</strong> 实验证明，NoPo-Avatar 对输入姿态的噪声具有极强的鲁棒性，即使在姿态估计不准确的情况下，也能保持高质量的重建和渲染。这与依赖姿态的基线方法形成鲜明对比，后者在姿态不准确时性能急剧下降。
*   <strong>泛化能力：</strong> 模型在跨领域（cross-domain）的测试中也表现出良好的泛化能力。
*   <strong>效率：</strong> 尽管不依赖姿态，NoPo-Avatar 的重建速度也相对较快，并且在推理时仅需少量GPU内存。</p>
<p><strong>4. 提及的局限性：</strong>
*   <strong>表情和手部建模：</strong> 模型目前不支持表情的重定向（expression retargeting）。由于输入图像中手部像素占比小且常被遮挡，预测手部LBS权重和3D位置具有挑战性，导致手部细节不够锐利。
*   <strong>训练数据不一致性：</strong> 在使用HuGe100K等合成数据集训练时，如果数据缺乏多视图一致性（例如，由扩散模型生成），模型可能会在未见区域产生模糊的结果或半透明区域。
*   <strong>模糊的修复（Inpainting）：</strong> 作为一种基于回归的方法，模型在修复大片未见区域时，难以生成高频细节，例如衣服上的精细纹理。作者认为生成模型在这方面可能更具优势。</p>
<p><strong>5. 未来研究方向：</strong>
*   <strong>表情和手部细节增强：</strong> 针对表情和手部建模的局限性，未来的工作可以考虑训练专门的模型来处理这些细节，或者改进现有架构以更好地捕捉这些信息。
*   <strong>利用生成模型进行修复：</strong> 探索结合生成模型（如扩散模型）来处理大片未见区域的修复，以生成更锐利、更高频的细节。
*   <strong>更一致的训练数据：</strong> 使用多视图一致性更高的数据集进行训练，以解决在合成数据上出现的模糊和半透明问题。
*   <strong>更广泛的应用：</strong> 探索 NoPo-Avatar 在AR/VR、虚拟形象等领域的更广泛应用，并考虑如何解决与化身身份和真实性相关的伦理问题。</p>
<p><strong>总结：</strong>
NoPo-Avatar 是一项重要的研究进展，它成功地解决了现有3D人体化身重建方法对姿态信息的高度依赖问题。通过创新的无姿态重建方法和双分支架构，该模型在各种场景下都展现出了优越的鲁棒性和性能，尤其是在真实世界的应用中，为构建通用、可驱动且无需姿态输入的3D人体化身开辟了新的道路。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>For this task, beyond a set of images, many prior state-of-the-art methods use accurate "ground-truth" camera poses and human poses as input to guide reconstruction at test-time.</li>
<li>We show that pose-dependent reconstruction degrades results significantly if pose estimates are noisy.</li>
<li>To overcome this, we introduce NoPo-Avatar, which reconstructs avatars solely from images, without any pose input.</li>
<li>Experiments on challenging THuman2.0, XHuman, and HuGe100K data show that NoPo-Avatar outperforms existing baselines in practical settings (without ground-truth poses) and delivers comparable results in lab settings (with ground-truth poses).</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.16673v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.16673v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.16671v1'></a></p>
<h2 id="thinking-while-generating-interleaving-textual-reasoning-throughout-visual-generation"><a href="https://arxiv.org/abs/2511.16671v1">Thinking-while-Generating: Interleaving Textual Reasoning throughout Visual Generation</a></h2>
<p><strong>Authors:</strong> Ziyu Guo, Renrui Zhang, Hongyu Li, Manyuan Zhang, Xinyan Chen, Sifan Wang, Yan Feng, Peng Pei, Pheng-Ann Heng</p>
<p><strong>Published:</strong> 2025-11-20</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.CL</p>
<p><strong>Abstract:</strong></p>
<p>Recent advances in visual generation have increasingly explored the integration of reasoning capabilities. They incorporate textual reasoning, i.e., think, either before (as pre-planning) or after (as post-refinement) the generation process, yet they lack on-the-fly multimodal interaction during the generation itself. In this preliminary study, we introduce Thinking-while-Generating (TwiG), the first interleaved framework that enables co-evolving textual reasoning throughout the visual generation process. As visual content is progressively generating, textual reasoning is interleaved to both guide upcoming local regions and reflect on previously synthesized ones. This dynamic interplay produces more context-aware and semantically rich visual outputs. To unveil the potential of this framework, we investigate three candidate strategies, zero-shot prompting, supervised fine-tuning (SFT) on our curated TwiG-50K dataset, and reinforcement learning (RL) via a customized TwiG-GRPO strategy, each offering unique insights into the dynamics of interleaved reasoning. We hope this work inspires further research into interleaving textual reasoning for enhanced visual generation. Code will be released at: https://github.com/ZiyuGuo99/Thinking-while-Generating.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Thinking-while-Generating: Interleaving Textual Reasoning throughout Visual Generation”的全面中文摘要：</p>
<p><strong>论文题目：</strong> Thinking-while-Generating: Interleaving Textual Reasoning throughout Visual Generation</p>
<p><strong>作者：</strong> Ziyu Guo, Renrui Zhang, Hongyu Li, Manyuan Zhang, Xinyan Chen, Sifan Wang, Yan Feng, Peng Pei, Pheng-Ann Heng</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
当前视觉生成模型在整合推理能力方面取得了显著进展，但现有的方法通常将文本推理置于生成过程之前（预规划）或之后（后精炼），缺乏在生成过程中进行实时的多模态交互。这种“离线”的推理方式限制了生成过程的灵活性和对局部细节的精细控制，导致生成的图像在长距离组合、多实体关系和细微指令遵循方面存在不足。</p>
<p><strong>2. 关键创新/方法贡献：</strong>
本文提出了<strong>Thinking-while-Generating (TWIG)</strong>，这是首个能够将文本推理与视觉生成过程<strong>交织在一起</strong>的框架。其核心创新在于：</p>
<ul>
<li><strong>交织式推理与生成：</strong> TWIG 在视觉内容逐步生成的同时，动态地插入文本推理步骤。这些推理不仅指导即将生成的局部区域，还能反思已合成的部分。</li>
<li><strong>三阶段框架：</strong><ul>
<li><strong>何时思考 (When to Think)：</strong> 确定推理的调度，即在生成过程的哪些阶段进行推理，以及如何划分画布。</li>
<li><strong>说什么 (What to Say)：</strong> 生成细粒度的文本推理，作为局部子提示，指导当前区域的生成，并整合之前的推理和已生成的视觉内容。</li>
<li><strong>如何修正 (How to Refine)：</strong> 在生成每个视觉区域后，进行即时的、局部的反思和修正。这包括生成一个评估分数和可能的修正性子标题，以确保语义对齐和视觉连贯性。</li>
</ul>
</li>
<li><strong>统一模型架构：</strong> 该框架兼容多种模型架构，包括将独立的大型多模态模型（LMM）与文本到图像模型耦合，或使用统一理解-生成模型（ULM）。</li>
<li><strong>三种实现策略：</strong> 论文探索了三种实现 TWIG 的策略：<ul>
<li><strong>零样本提示 (Zero-shot Prompting)：</strong> 通过精心设计的提示词，直接引导模型进行交织式推理，无需参数更新。</li>
<li><strong>监督微调 (Supervised Fine-tuning, SFT)：</strong> 在新创建的 TWIG-50K 数据集上对模型进行微调，以学习结构化的推理、局部反思和区域化生成。</li>
<li><strong>强化学习 (Reinforcement Learning, RL)：</strong> 使用定制的 TWIG-GRPO 策略，通过强化学习优化推理策略，以进一步提升性能。</li>
</ul>
</li>
</ul>
<p><strong>3. 主要结果与意义：</strong>
*   <strong>零样本性能显著：</strong> 经过精心设计的零样本提示，TWIG 在 T2I-CompBench 数据集上显著优于基线模型 Janus-Pro-7B，证明了其框架的潜力以及现有大型多模态模型（ULM）的内在能力。
*   <strong>SFT 提升稳定性和性能：</strong> SFT 在零样本模型的基础上带来了适度但稳定的性能提升，尤其是在形状和空间属性方面，并显著提高了结果的可预测性。
*   <strong>RL 带来大幅改进：</strong> 通过 TWIG-GRPO 策略进行强化学习，模型性能得到了大幅提升，尤其是在属性绑定和空间理解方面，凸显了 RL 在优化交织式推理策略中的价值。
*   <strong>定性改进显著：</strong> 论文通过可视化结果展示了 TWIG 在组合保真度、对象计数和视觉真实感方面的渐进式改进，以及其反思能力在提升语义和视觉一致性方面的作用。
*   <strong>意义：</strong> TWIG 框架的提出，为视觉生成领域引入了一种全新的、更精细化的推理与生成协同模式，有望推动生成模型在复杂场景理解和指令遵循方面取得更大突破。</p>
<p><strong>4. 局限性：</strong>
*   <strong>固定推理步数：</strong> 当前的“何时思考”阶段采用固定的三步调度，虽然通用但并非最优。
*   <strong>自适应调度挑战：</strong> 目前的 ULMs 在生成完全自适应的调度方面仍有困难。
*   <strong>RL 设置的进一步探索：</strong> RL 设置可以进一步通过近期变体或更精细的奖励设计来增强。
*   <strong>多模态任务扩展：</strong> 该框架目前主要在文本到图像任务上进行验证，将其扩展到视频、3D 或图像到图像等任务是未来的方向。</p>
<p><strong>5. 未来研究方向：</strong>
*   <strong>学习自适应推理调度：</strong> 开发更强大的模型来学习动态、内容感知的推理调度。
*   <strong>探索更先进的 RL 技术：</strong> 结合最新的 RL 算法和奖励模型设计，进一步优化推理策略。
*   <strong>将 TWIG 应用于其他生成任务：</strong> 扩展 TWIG 框架以支持视频、3D 模型生成以及图像到图像转换等任务。
*   <strong>研究更精细的局部修正机制：</strong> 进一步提升“如何修正”阶段的效率和效果。</p>
<p>总而言之，这篇论文提出了一种创新的“边思考边生成”框架，通过将文本推理无缝地整合到视觉生成过程中，显著提升了生成图像的质量和对复杂指令的遵循能力。该框架通过零样本、SFT 和 RL 等多种策略进行了验证，并为未来更智能、更具交互性的视觉生成模型指明了方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this preliminary study, we introduce Thinking-while-Generating (TwiG), the first interleaved framework that enables co-evolving textual reasoning throughout the visual generation process.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.16671v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.16671v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.16670v1'></a></p>
<h2 id="learning-to-think-fast-and-slow-for-visual-language-models"><a href="https://arxiv.org/abs/2511.16670v1">Learning to Think Fast and Slow for Visual Language Models</a></h2>
<p><strong>Authors:</strong> Chenyu Lin, Cheng Chi, Jinlin Wu, Sharon Li, Kaiyang Zhou</p>
<p><strong>Published:</strong> 2025-11-20</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>When confronted with complex problems, we tend to think slowly; conversely, for simple questions, we think quickly. Such a two-system thinking mechanism allows us to efficiently allocate cognitive resources, enabling quick decision-making for straightforward issues while reserving deeper analytical thinking for more intricate challenges. However, existing reasoning-oriented visual language models (VLMs), whether trained with explicit chain-of-thought annotations or rule-based RL rewards, mainly pursue lengthy, detailed reasoning chains, which often lead to excessive computational costs. In this work, we propose a simple RL approach, which enables VLMs to automatically switch between fast and slow thinking modes depending on task difficulty. The approach consists of two stages: in the first stage, we label data as either requiring fast thinking or slow thinking based on the model output length, which is inspired by the observation that pre-trained VLMs typically produce answers of varying lengths for different types of questions; in the second stage, we train the model using GRPO along with the thinking mode labels to develop dual-mode thinking. Despite its simplicity, our model, named DualMindVLM, significantly outperforms the base model and achieves performance on par with state-of-the-art visual reasoning models, while maintaining exceptionally high token efficiency.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Learning to Think Fast and Slow for Visual Language Models”的全面中文摘要，重点关注其在计算机视觉领域的创新性和重要性：</p>
<p><strong>论文题目：</strong> Learning to Think Fast and Slow for Visual Language Models</p>
<p><strong>作者：</strong> Chenyu Lin, Cheng Chi, Jinlin Wu, Sharon Li, Kaiyang Zhou</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 主要问题/研究问题：</strong>
现有的大多数视觉语言模型（VLMs）在处理复杂问题时，倾向于生成冗长、详细的推理链，即使对于简单问题也是如此。这种“过度思考”导致了不必要的计算成本和token消耗，未能有效模仿人类“快速思考”（System 1）和“慢速思考”（System 2）的双模式认知机制。论文旨在解决如何让VLMs能够根据任务难度自适应地切换这两种思考模式，从而在保证准确性的同时显著提高效率。</p>
<p><strong>2. 关键创新/方法论贡献：</strong>
该论文提出了一种名为 <strong>DualMindVLM</strong> 的模型，其核心创新在于引入了一种简单的强化学习（RL）方法来实现VLMs的双模式思考能力。其方法论主要包含两个阶段：</p>
<ul>
<li>
<p><strong>思考模式自动标注（Thinking Mode Auto-Labeling）：</strong></p>
<ul>
<li>利用预训练VLMs在不同难度问题上输出长度的差异，将训练数据自动标注为“快速思考”（短输出）或“慢速思考”（长输出）。</li>
<li>这种方法避免了昂贵的人工标注或依赖外部模型，成本效益高。</li>
<li>通过设定长度阈值（例如，小于100 token为快速思考，大于200 token为慢速思考），并过滤掉中间长度或准确率过高/过低的数据，确保了清晰的模式区分。</li>
</ul>
</li>
<li>
<p><strong>学习双模式思考（Learning Dual-Mode Thinking）：</strong></p>
<ul>
<li>采用 <strong>Group Relative Policy Optimization (GRPO)</strong> 算法进行训练。</li>
<li>在RL训练阶段，模型被引导生成两种类型的响应：一种是强制使用根据自动标注的思考模式前缀（如“Short Thinking:”或“Long Thinking:”）引导生成的响应；另一种是模型自由生成的响应。</li>
<li>通过这种混合采样策略，模型能够学习到如何根据任务的内在难度，自主地选择并生成相应的思考模式前缀，从而实现快速和慢速思考的自适应切换。</li>
<li>引入了格式奖励（format reward），鼓励模型正确使用思考模式前缀。</li>
</ul>
</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>性能提升：</strong> DualMindVLM在多个视觉推理基准测试（如MathVista, MMStar, MMBench, ScienceQA, AI2D等）上显著优于基线模型（Qwen2.5-VL），并且在准确性上与最先进的视觉推理模型相当。
*   <strong>效率提升：</strong> 最重要的贡献在于其卓越的token效率。DualMindVLM在达到同等或更高准确性的同时，平均使用的token数量远少于其他模型，尤其是在与纯粹的慢速思考模型（如GRPO）相比时，token节省高达40%-60%。这表明模型能够有效地避免不必要的冗余推理。
*   <strong>自适应性：</strong> 实验结果（如图8和图9所示）表明，DualMindVLM能够根据任务类型（如数学问题倾向于慢速思考，而感知类问题倾向于快速思考）自动调整其思考模式，实现了真正的双模式思考。
*   <strong>缓解幻觉：</strong> 在HumbleBench（一个用于评估模型幻觉的基准）上的实验表明，DualMindVLM在减少幻觉方面表现出色，这可能与双模式思考能够更精确地控制推理深度有关。</p>
<p><strong>4. 提及的局限性：</strong>
*   <strong>模式选择偏差：</strong> 论文提到，自动标注策略可能引入与特定问题类型相关的模式选择偏差。例如，在图10的失败案例中，模型未能正确回答一个图表理解问题，尽管它识别了正确的推理步骤。这可能是因为图表任务通常与快速思考相关联，导致模型在该类问题上产生了选择快速思考的偏见。这种偏见类似于人类的System-1启发式思维，虽然高效但可能偶尔有偏差。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>数据中心化RL：</strong> 论文提到，数据中心化（data-centric）的RL方法（即更关注数据质量和标注）在推理任务中可能很重要，并表示这超出了当前工作的范围，是未来研究的方向。
*   <strong>进一步探索模式选择偏差：</strong> 针对图10所示的模式选择偏差问题，可能需要更精细的标注方法或训练策略来解决。
*   <strong>更广泛的应用：</strong> 将DualMindVLM的方法扩展到其他模态或更复杂的推理任务。</p>
<p><strong>总结：</strong>
这篇论文成功地提出了一种新颖且高效的 <strong>DualMindVLM</strong> 模型，通过创新的 <strong>思考模式自动标注</strong> 和 <strong>双模式RL训练</strong> 方法，使视觉语言模型能够模仿人类的双模式思考。该模型在保持顶尖性能的同时，显著提高了token效率，解决了现有模型在简单问题上的“过度思考”问题。其方法论简单有效，为构建更具认知对齐性的AI模型提供了重要思路，并在减少模型幻觉方面展现出潜力。论文也诚实地指出了模式选择偏差的局限性，并为未来的研究开辟了道路。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this work, we propose a simple RL approach, which enables VLMs to automatically switch between fast and slow thinking modes depending on task difficulty.</li>
<li>Despite its simplicity, our model, named DualMindVLM, significantly outperforms the base model and achieves performance on par with state-of-the-art visual reasoning models, while maintaining exceptionally high token efficiency.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.16670v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.16670v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.16669v1'></a></p>
<h2 id="video-as-answer-predict-and-generate-next-video-event-with-joint-grpo"><a href="https://arxiv.org/abs/2511.16669v1">Video-as-Answer: Predict and Generate Next Video Event with Joint-GRPO</a></h2>
<p><strong>Authors:</strong> Junhao Cheng, Liang Hou, Xin Tao, Jing Liao</p>
<p><strong>Published:</strong> 2025-11-20</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>While language models have become impactful in many real-world applications, video generation remains largely confined to entertainment. Motivated by video's inherent capacity to demonstrate physical-world information that is difficult to convey through language alone (e.g., imagine teaching someone to tie a tie using only text), we identify an underutilized opportunity to extend video as a new answer modality for Next-Event Prediction (NEP), formalized as Video-Next-Event Prediction (VNEP). While the established NEP task takes a video with a procedural or predictive question as input to predict the next event in text, VNEP requires dynamic video responses. This shift from telling to showing unlocks more intuitive and customized answers for procedural learning and creative exploration. However, this task remains challenging for existing models, as it demands an understanding of multimodal input, instruction-conditioned reasoning, and the generation of video with visual and semantic consistency. To address this, we introduce VANS, a model that leverages reinforcement learning to align a Vision-Language Model (VLM) with a Video Diffusion Model (VDM) for VNEP. The core of VANS is our proposed Joint-GRPO that orchestrates the VLM and VDM to function as a unit. Driven by a shared reward on their respective output, it optimizes the VLM to produce captions that are both accurate and friendly to visualize, while guiding the VDM to generate videos that are faithful to these captions and the input visual context. To enable this learning, we craft VANS-Data-100K, a dedicated dataset for the VNEP task. Experiments on procedural and predictive benchmarks demonstrate that VANS achieves state-of-the-art performance in both video event prediction and visualization. Codes are released in https://github.com/KlingTeam/VANS.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Video-as-Answer: Predict and Generate Next Video Event with Joint-GRPO”的中文摘要，重点关注其在计算机视觉领域的创新性和重要性：</p>
<p><strong>论文摘要：Video-as-Answer: Predict and Generate Next Video Event with Joint-GRPO</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
该论文旨在解决“视频-下一个事件预测”（Video-Next-Event Prediction, VNEP）这一新任务。与传统的文本回答式下一个事件预测（NEP）不同，VNEP要求模型不仅预测下一个事件，还要以动态视频的形式生成答案。这为程序化学习和创意探索提供了更直观、个性化的方式。然而，VNEP面临巨大挑战，需要模型理解多模态输入、进行指令引导的推理，并生成在视觉和语义上都保持一致的视频。现有方法在处理这种“从讲述到展示”的转变时存在困难，尤其是在弥合语言理解与视频生成之间的语义-视觉鸿沟方面。</p>
<p><strong>2. 主要创新与方法贡献：</strong>
*   <strong>提出VNEP任务：</strong> 论文首次定义并提出了VNEP任务，将下一个事件预测的答案形式从文本扩展到视频，强调了视频作为一种更直观、信息量更大的回答方式的潜力。
*   <strong>VANS模型：</strong> 论文提出了VANS（Video-as-Answer）模型，一个集成了视觉语言模型（VLM）和视频扩散模型（VDM）的框架，用于解决VNEP任务。
*   <strong>Joint-GRPO策略：</strong> 核心创新在于其提出的Joint-GRPO（Joint-Group-Relative Policy Optimization）策略。这是一种两阶段的强化学习（RL）方法，旨在协同优化VLM和VDM，实现它们之间的紧密对齐。
    *   <strong>阶段1（可视化友好的VLM调优）：</strong> 优化VLM，使其生成的文本描述（caption）不仅语义准确，而且对VDM来说是可视化可行且易于理解的。通过结合文本保真度奖励和视频保真度奖励来实现。
    *   <strong>阶段2（上下文感知的VDM适应）：</strong> 优化VDM，使其能够忠实地根据VLM生成的文本描述来生成视频，同时保持与输入视频的视觉一致性。通过结合视频保真度奖励和语义一致性奖励来实现。
    *   <strong>联合奖励机制：</strong> 这种两阶段的联合奖励设计，使得VLM和VDM能够相互促进，共同学习，弥合了语义和视觉之间的差距。
*   <strong>VANS-Data-100K数据集：</strong> 为了支持VNEP任务的研究，论文构建了一个包含100K（输入视频、问题、输出视频）三元组的大规模数据集，为模型训练和评估提供了基础。</p>
<p><strong>3. 主要结果与意义：</strong>
*   <strong>SOTA性能：</strong> VANS在程序化和预测性VNEP基准测试中均取得了最先进（SOTA）的性能。
*   <strong>显著提升：</strong> 相比于仅使用SFT（Supervised Fine-Tuning）的模型，Joint-GRPO策略显著提升了ROUGE-L（文本指标）和CLIP-V/CLIP-T（视频指标），证明了其联合优化策略的有效性。
*   <strong>弥合语义-视觉鸿沟：</strong> 实验结果表明，VANS能够生成在语义上准确且视觉上连贯的视频答案，有效解决了现有方法在理解和生成之间的不匹配问题。
*   <strong>更直观的回答：</strong> 通过视频回答，VANS能够更清晰地展示物理世界的动作和过程，例如演示如何打领带，这比纯文本描述更易于理解和学习。</p>
<p><strong>4. 提及的局限性：</strong>
*   <strong>计算成本：</strong> 论文提到VANS的推理时间与级联模型相当，生成视频需要约35秒，这可能在需要实时响应的应用中是一个限制。
*   <strong>奖励设计的复杂性：</strong> 虽然Joint-GRPO有效，但其两阶段的奖励设计和权重设置需要仔细调整，以确保各组件的有效协同。
*   <strong>对高质量数据的依赖：</strong> VANS-Data-100K数据集的构建是其成功的关键，但高质量视频数据的收集和标注本身就是一个挑战。</p>
<p><strong>5. 未来研究方向（隐含）：</strong>
*   <strong>提高推理效率：</strong> 进一步优化模型架构或采用更高效的生成技术，以缩短视频生成时间，实现更实时的VNEP。
*   <strong>多模态融合的深化：</strong> 探索更精细的VLM和VDM之间的融合机制，以进一步提升视频生成质量和语义一致性。
*   <strong>更广泛的应用场景：</strong> 将VNEP任务和VANS模型扩展到更多领域，如教育、培训、产品演示等，探索视频作为答案的更多可能性。
*   <strong>个性化与交互性：</strong> 研究如何使VANS能够根据用户的具体需求和反馈，生成更具交互性和个性化的视频答案。</p>
<p>总而言之，这篇论文在计算机视觉领域具有重要意义，它不仅开创了VNEP这一新任务，更提出了创新的Joint-GRPO策略，有效解决了视频理解与生成之间的核心挑战，为生成更具信息量和直观性的视频答案奠定了坚实基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Motivated by video's inherent capacity to demonstrate physical-world information that is difficult to convey through language alone (e.g., imagine teaching someone to tie a tie using only text), we identify an underutilized opportunity to extend video as a new answer modality for Next-Event Prediction (NEP), formalized as Video-Next-Event Prediction (VNEP).</li>
<li>To address this, we introduce VANS, a model that leverages reinforcement learning to align a Vision-Language Model (VLM) with a Video Diffusion Model (VDM) for VNEP.</li>
<li>Experiments on procedural and predictive benchmarks demonstrate that VANS achieves state-of-the-art performance in both video event prediction and visualization.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.16669v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.16669v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.16668v1'></a></p>
<h2 id="v-reasonbench-toward-unified-reasoning-benchmark-suite-for-video-generation-models"><a href="https://arxiv.org/abs/2511.16668v1">V-ReasonBench: Toward Unified Reasoning Benchmark Suite for Video Generation Models</a></h2>
<p><strong>Authors:</strong> Yang Luo, Xuanlei Zhao, Baijiong Lin, Lingting Zhu, Liyao Tang, Yuqi Liu, Ying-Cong Chen, Shengju Qian, Xin Wang, Yang You</p>
<p><strong>Published:</strong> 2025-11-20</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Recent progress in generative video models, such as Veo-3, has shown surprising zero-shot reasoning abilities, creating a growing need for systematic and reliable evaluation. We introduce V-ReasonBench, a benchmark designed to assess video reasoning across four key dimensions: structured problem-solving, spatial cognition, pattern-based inference, and physical dynamics. The benchmark is built from both synthetic and real-world image sequences and provides a diverse set of answer-verifiable tasks that are reproducible, scalable, and unambiguous. Evaluations of six state-of-the-art video models reveal clear dimension-wise differences, with strong variation in structured, spatial, pattern-based, and physical reasoning. We further compare video models with strong image models, analyze common hallucination behaviors, and study how video duration affects Chain-of-Frames reasoning. Overall, V-ReasonBench offers a unified and reproducible framework for measuring video reasoning and aims to support the development of models with more reliable, human-aligned reasoning skills.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“V-ReasonBench: Toward Unified Reasoning Benchmark Suite for Video Generation Models”的全面中文摘要：</p>
<p><strong>论文题目：</strong> V-ReasonBench: Toward Unified Reasoning Benchmark Suite for Video Generation Models (V-ReasonBench：面向视频生成模型的统一推理基准套件)</p>
<p><strong>作者：</strong> Yang Luo, Xuanlei Zhao, Baijiong Lin, Lingting Zhu, Liyao Tang, Yuqi Liu, Ying-Cong Chen, Shengju Qian, Xin Wang, Yang You</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
随着视频生成模型（如 Veo-3.1、Sora-2 等）在零样本推理能力上取得显著进展，对这些模型进行系统、可靠且全面的评估变得至关重要。现有的评估方法往往不足以量化和区分模型在不同推理维度上的能力，并且缺乏一个统一、可复现的框架来衡量视频推理。</p>
<p><strong>2. 主要创新与方法贡献：</strong>
*   <strong>V-ReasonBench 基准套件的提出：</strong> 作者构建了一个名为 V-ReasonBench 的新颖基准套件，专门用于评估视频生成模型的推理能力。
*   <strong>四维推理维度：</strong> V-ReasonBench 涵盖了四个核心推理维度：结构化问题解决（Structured Problem-Solving）、空间认知（Spatial Cognition）、基于模式的推理（Pattern-based Inference）和物理动力学（Physical Dynamics）。
*   <strong>混合数据源与任务设计：</strong> 该基准套件结合了程序化生成的合成数据和真实世界图像序列，包含多种可验证答案的任务，确保了可复现性、可扩展性和无歧义性。
*   <strong>Chain-of-Frame (CoF) 框架下的评估：</strong> V-ReasonBench 遵循 CoF 范式，将视频推理视为一系列推理步骤，并采用“最后一帧”的评估策略，提高了评估效率和可扩展性。
*   <strong>多样的评估方法：</strong> 为了应对不同任务的特点，V-ReasonBench 采用了三种评估策略：基于掩码（Mask-based）、基于网格（Grid-based）和基于视觉语言模型（VLM-based）的评估，以确保评估的准确性和鲁棒性。
*   <strong>统一的评估指标：</strong> 采用 pass@k 作为主要评估指标，提供了一种一致的性能衡量标准，便于不同模型之间的比较。</p>
<p><strong>3. 主要结果与意义：</strong>
*   <strong>模型能力差异显著：</strong> 对六个先进视频生成模型的评估揭示了它们在不同推理维度上的显著差异。例如，Sora-2 在结构化问题解决、空间认知和基于模式的推理方面表现出色，而 Hailuo-02 和 Vidu-Q2 在物理动力学方面得分较高。这表明当前的视频模型在推理能力上存在“多面性”，不同模型侧重于不同的推理方面。
*   <strong>揭示模型局限性：</strong> 研究发现，一些模型倾向于优先考虑视觉增强而非结构准确性，这可能源于其在开放域视频语料库上的预训练，导致在需要精确符号和空间约束的任务上表现不佳。
*   <strong>视频时长影响推理：</strong> 分析表明，增加 CoF 的视频时长并不总能带来更好的推理结果，有时反而会引入冗余信息或导致模型产生幻觉。
*   <strong>视频模型优于图像模型：</strong> 在物理动力学和程序性推理任务上，视频模型（如 Veo-3.1）通过模拟中间状态展现出比图像模型（如 NanoBanana）更强的能力，而图像模型在静态结构任务上表现更稳定。
*   <strong>VLM 评估的局限性：</strong> 研究强调了 VLM 在评估复杂视觉布局、精细结构和空间关系时的局限性，指出其可能因难以准确识别小单元格和细微结构而产生误判。
*   <strong>意义：</strong> V-ReasonBench 提供了一个统一、可复现的框架，能够系统地衡量视频生成模型的推理能力，为开发更可靠、更符合人类认知习惯的视频推理模型奠定了基础。</p>
<p><strong>4. 提及的局限性：</strong>
*   <strong>VLM 评估的局限性：</strong> 如前所述，VLM 在评估精细的视觉细节和空间关系时存在困难，可能导致评估不准确。
*   <strong>“正确答案，错误过程”的挑战：</strong> 模型有时可能在推理过程中产生不一致或不符合物理规律的中间帧，但最终输出正确。这种“正确答案，错误过程”的现象难以通过仅检查最终帧的 VLM 来检测。
*   <strong>模型对视觉丰富性的偏好：</strong> 一些模型倾向于生成视觉上更丰富、更具吸引力的场景，这可能与需要精确结构和符号的任务要求相冲突。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>更精细的推理能力评估：</strong> 进一步探索如何更全面地评估视频模型在不同推理维度上的能力，以及如何区分“正确答案，错误过程”的现象。
*   <strong>改进 VLM 评估的鲁棒性：</strong> 研究如何克服 VLM 在评估复杂视觉任务时的局限性，或者开发更适合视频推理评估的自动化评估方法。
*   <strong>理解和控制模型的生成偏好：</strong> 探索如何调整模型的训练目标和解码策略，以平衡视觉丰富性和结构准确性，使其在推理任务中表现更佳。
*   <strong>探索视频时长与推理的关系：</strong> 深入研究视频时长、采样策略和上下文窗口如何影响推理质量，以优化 CoF 的效率和有效性。
*   <strong>开发更具泛化能力的推理模型：</strong> 基于 V-ReasonBench 的评估结果，指导未来模型的设计，使其能够更全面地整合抽象、空间和物理推理能力。</p>
<p>总而言之，V-ReasonBench 的提出是视频生成模型推理能力评估领域的一项重要贡献，它提供了一个标准化的平台，有助于深入理解当前模型的优势与不足，并指明了未来研究的方向，即构建更具鲁棒性、可解释性和人类对齐的视频推理系统。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce V-ReasonBench, a benchmark designed to assess video reasoning across four key dimensions: structured problem-solving, spatial cognition, pattern-based inference, and physical dynamics.</li>
<li>Evaluations of six state-of-the-art video models reveal clear dimension-wise differences, with strong variation in structured, spatial, pattern-based, and physical reasoning.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.16668v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.16668v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.16666v1'></a></p>
<h2 id="scenedesigner-controllable-multi-object-image-generation-with-9-dof-pose-manipulation"><a href="https://arxiv.org/abs/2511.16666v1">SceneDesigner: Controllable Multi-Object Image Generation with 9-DoF Pose Manipulation</a></h2>
<p><strong>Authors:</strong> Zhenyuan Qin, Xincheng Shuai, Henghui Ding</p>
<p><strong>Published:</strong> 2025-11-20</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Controllable image generation has attracted increasing attention in recent years, enabling users to manipulate visual content such as identity and style. However, achieving simultaneous control over the 9D poses (location, size, and orientation) of multiple objects remains an open challenge. Despite recent progress, existing methods often suffer from limited controllability and degraded quality, falling short of comprehensive multi-object 9D pose control. To address these limitations, we propose SceneDesigner, a method for accurate and flexible multi-object 9-DoF pose manipulation. SceneDesigner incorporates a branched network to the pre-trained base model and leverages a new representation, CNOCS map, which encodes 9D pose information from the camera view. This representation exhibits strong geometric interpretation properties, leading to more efficient and stable training. To support training, we construct a new dataset, ObjectPose9D, which aggregates images from diverse sources along with 9D pose annotations. To further address data imbalance issues, particularly performance degradation on low-frequency poses, we introduce a two-stage training strategy with reinforcement learning, where the second stage fine-tunes the model using a reward-based objective on rebalanced data. At inference time, we propose Disentangled Object Sampling, a technique that mitigates insufficient object generation and concept confusion in complex multi-object scenes. Moreover, by integrating user-specific personalization weights, SceneDesigner enables customized pose control for reference subjects. Extensive qualitative and quantitative experiments demonstrate that SceneDesigner significantly outperforms existing approaches in both controllability and quality. Code is publicly available at https://github.com/FudanCVL/SceneDesigner.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“SceneDesigner: Controllable Multi-Object Image Generation with 9-DoF Pose Manipulation”的全面中文摘要：</p>
<p><strong>论文摘要：SceneDesigner: 可控的多物体9-DoF姿态操纵图像生成</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
该论文旨在解决当前可控图像生成领域的一个关键挑战：<strong>如何实现对多物体场景中物体9-DoF（位置、尺寸和朝向）姿态的精确、灵活且同步的操纵</strong>。现有方法在控制精度、物体数量和生成质量方面存在不足，尤其是在处理复杂的多物体场景时，容易出现概念混淆和生成不足的问题。</p>
<p><strong>2. 主要创新点/方法贡献：</strong>
SceneDesigner 提出了一种新颖的框架，其核心创新点包括：</p>
<ul>
<li><strong>CNOCS Map（Cuboid Normalized Object Coordinate System Map）</strong>：一种新的姿态表示方法，它将物体的9-DoF姿态信息编码成一个图像表示。CNOCS Map 利用粗糙的立方体抽象，保留了关键的几何信息，并且具有良好的几何解释性，使得训练更高效稳定，同时支持类别无关的姿态编码。</li>
<li><strong>ObjectPose9D 数据集</strong>：构建了一个包含真实世界场景和丰富9-DoF姿态标注的新数据集，以支持多物体姿态可控生成模型的训练。该数据集通过整合现有数据集并进行大规模人工标注扩充而来。</li>
<li><strong>两阶段训练策略与强化学习</strong>：为了解决数据集中低频姿态（如物体背面朝向）的性能退化问题，论文引入了一个两阶段训练策略。第一阶段进行基础姿态控制学习，第二阶段利用强化学习和基于奖励的目标函数，在重平衡的数据上进行微调，以提升模型在低频姿态上的表现。</li>
<li><strong>Disentangled Object Sampling（解耦物体采样）</strong>：在推理阶段，提出了一种新的采样技术，用于解决复杂多物体场景中物体生成不足和概念混淆的问题。该技术通过区域掩码将全局和物体特定的条件解耦，确保每个物体都能准确匹配其指定的姿态。</li>
<li><strong>用户个性化权重</strong>：通过集成用户自定义的权重，SceneDesigner 能够实现对用户提供的参考主体的定制化姿态控制。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong>
论文通过广泛的定性和定量实验证明，SceneDesigner 在单物体和多物体场景下都显著优于现有方法，在<strong>姿态控制的准确性、生成图像的保真度以及文本对齐度</strong>方面均取得了领先的性能。</p>
<ul>
<li><strong>定量评估</strong>：在多个基准测试中，SceneDesigner 在位置、尺寸和朝向的对齐度指标上均取得了最高分数，尤其是在朝向控制方面表现突出。</li>
<li><strong>定性评估</strong>：生成的图像在视觉质量和对条件（如姿态）的遵循度上都表现出色，能够生成复杂多物体场景下具有精确姿态的对象。</li>
<li><strong>意义</strong>：SceneDesigner 的提出为实现更精细、更具创造性的图像生成打开了新的可能性，尤其是在需要精确控制物体空间布局的应用场景（如虚拟/增强现实、产品设计）中具有重要价值。</li>
</ul>
<p><strong>4. 提及的局限性：</strong>
论文中也指出了 SceneDesigner 的一些局限性：</p>
<ul>
<li><strong>物体形状控制</strong>：SceneDesigner 主要关注物体姿态（位置、尺寸、朝向）的控制，<strong>无法精确控制物体的具体形状</strong>。</li>
<li><strong>基础模型能力限制</strong>：在多物体场景下的性能，在一定程度上受到基础文本到图像生成模型（如 Stable Diffusion）固有能力的限制，例如在处理大量语义概念时可能出现的生成不足和属性泄露问题。</li>
<li><strong>计算成本</strong>：Disentangled Object Sampling 技术虽然有效，但会<strong>增加额外的计算开销</strong>。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong>
基于上述局限性，论文提出了未来的研究方向：</p>
<ul>
<li><strong>提升多物体场景下的对齐度与效率</strong>：探索如何进一步提升在多物体生成场景下，模型对各种条件的对齐能力，同时保持计算效率。</li>
<li><strong>精确的物体形状控制</strong>：将姿态控制扩展到更精细的物体形状控制。</li>
<li><strong>解决生成不足和属性泄露问题</strong>：进一步研究如何更有效地解决多语义概念生成中的挑战。</li>
</ul>
<p>总而言之，SceneDesigner 是一个在多物体9-DoF姿态操纵图像生成领域的重要进展，它通过创新的姿态表示、数据集构建、训练策略和推理技术，显著提升了生成图像的可控性和质量，为未来的研究奠定了坚实的基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address these limitations, we propose SceneDesigner, a method for accurate and flexible multi-object 9-DoF pose manipulation.</li>
<li>SceneDesigner incorporates a branched network to the pre-trained base model and leverages a new representation, CNOCS map, which encodes 9D pose information from the camera view.</li>
<li>To support training, we construct a new dataset, ObjectPose9D, which aggregates images from diverse sources along with 9D pose annotations.</li>
<li>To further address data imbalance issues, particularly performance degradation on low-frequency poses, we introduce a two-stage training strategy with reinforcement learning, where the second stage fine-tunes the model using a reward-based objective on rebalanced data.</li>
<li>At inference time, we propose Disentangled Object Sampling, a technique that mitigates insufficient object generation and concept confusion in complex multi-object scenes.</li>
<li>Extensive qualitative and quantitative experiments demonstrate that SceneDesigner significantly outperforms existing approaches in both controllability and quality.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.16666v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.16666v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.16662v1'></a></p>
<h2 id="tridiff-4d-fast-4d-generation-through-diffusion-based-triplane-re-posing"><a href="https://arxiv.org/abs/2511.16662v1">TriDiff-4D: Fast 4D Generation through Diffusion-based Triplane Re-posing</a></h2>
<p><strong>Authors:</strong> Eddie Pokming Sheung, Qihao Liu, Wufei Ma, Prakhar Kaushik, Jianwen Xie, Alan Yuille</p>
<p><strong>Published:</strong> 2025-11-20</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>With the increasing demand for 3D animation, generating high-fidelity, controllable 4D avatars from textual descriptions remains a significant challenge. Despite notable efforts in 4D generative modeling, existing methods exhibit fundamental limitations that impede their broader applicability, including temporal and geometric inconsistencies, perceptual artifacts, motion irregularities, high computational costs, and limited control over dynamics. To address these challenges, we propose TriDiff-4D, a novel 4D generative pipeline that employs diffusion-based triplane re-posing to produce high-quality, temporally coherent 4D avatars. Our model adopts an auto-regressive strategy to generate 4D sequences of arbitrary length, synthesizing each 3D frame with a single diffusion process. By explicitly learning 3D structure and motion priors from large-scale 3D and motion datasets, TriDiff-4D enables skeleton-driven 4D generation that excels in temporal consistency, motion accuracy, computational efficiency, and visual fidelity. Specifically, TriDiff-4D first generates a canonical 3D avatar and a corresponding motion sequence from a text prompt, then uses a second diffusion model to animate the avatar according to the motion sequence, supporting arbitrarily long 4D generation. Experimental results demonstrate that TriDiff-4D significantly outperforms existing methods, reducing generation time from hours to seconds by eliminating the optimization process, while substantially improving the generation of complex motions with high-fidelity appearance and accurate 3D geometry.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“TriDiff-4D: Fast 4D Generation through Diffusion-based Triplane Re-posing”的全面中文摘要，其中包含了您要求的各个方面：</p>
<p><strong>论文题目：</strong> TriDiff-4D: Fast 4D Generation through Diffusion-based Triplane Re-posing</p>
<p><strong>作者：</strong> Eddie Sheung, Qihao Liu, Wufei Ma, Prakhar Kaushik, Jianwen Xie, Alan Yuille</p>
<hr />
<p><strong>摘要</strong></p>
<p><strong>1. 研究问题/研究挑战：</strong></p>
<p>随着对高质量 3D 动画需求的不断增长，从文本描述生成高保真、可控的 4D 化身（avatar）仍然是一个重大挑战。现有的 4D 生成方法普遍存在一些根本性限制，阻碍了其广泛应用，包括：
*   <strong>时空不一致性：</strong> 动画在时间上和几何上可能不连贯，导致感知伪影和运动不规则。
*   <strong>计算成本高昂：</strong> 生成过程通常需要数小时的计算时间，限制了其在实时应用中的可行性。
*   <strong>控制能力有限：</strong> 难以精确控制化身的动态和姿势。
*   <strong>“果冻效应”和“Janus 问题”：</strong> 常见的伪影，前者指非刚性形变导致物体像果冻一样晃动，后者指从不同视角观察时出现多面体问题。</p>
<p><strong>2. 关键创新/方法论贡献：</strong></p>
<p>为了解决上述挑战，本文提出了 <strong>TriDiff-4D</strong>，一个新颖的 4D 生成流水线，其核心创新在于利用<strong>基于扩散的 Triplane 重定位（re-posing）</strong>技术来生成高质量、时空连贯的 4D 化身。主要贡献包括：</p>
<ul>
<li><strong>两阶段生成流程：</strong><ol>
<li><strong>初始静态 3D 化身生成：</strong> 利用文本提示生成一个标准的 3D 化身（以 Triplane 特征表示）。</li>
<li><strong>基于扩散的重定位：</strong> 利用一个独立的扩散模型，将初始静态化身根据文本提示生成的运动序列进行动画化，实现任意长度的 4D 序列生成。</li>
</ol>
</li>
<li><strong>显式 3D 结构和运动先验学习：</strong> 模型从大规模 3D 和运动数据集中学习先验知识，并将其融入扩散模型中。</li>
<li><strong>骨架驱动的 4D 生成：</strong> 直接以 3D 骨架作为条件来指导 Triplane 特征空间的重定位，确保了精确的姿势控制和时空一致性。</li>
<li><strong>Triplane 特征表示：</strong> 使用 Triplane 表示来编码 3D 几何和颜色信息，这种表示在不同视角下具有一致性，有助于解决 Janus 问题。</li>
<li><strong>高效的骨架编码：</strong> 将 3D 骨架信息编码为 2D Triplane 骨架表示（包括 Occupancy 和 Index 映射），使其能够高效地与扩散模型结合，用于姿势引导。</li>
<li><strong>消除优化过程：</strong> TriDiff-4D 采用单次前向传播（single forward pass）生成，无需耗时的迭代优化，从而大幅缩短了生成时间。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong></p>
<ul>
<li><strong>显著的生成速度提升：</strong> TriDiff-4D 能够将生成 14 帧的 4D 动画序列的时间从数小时缩短到仅需 <strong>36 秒</strong>（在单个 H100 GPU 上），这比现有方法快了几个数量级。</li>
<li><strong>高质量的 4D 化身：</strong> 生成的化身具有高视觉保真度、解剖学上的准确性、运动一致性、动态性和视觉连贯性。</li>
<li><strong>优于现有方法：</strong> 实验结果表明，TriDiff-4D 在几何一致性、运动一致性和整体用户偏好方面显著优于包括 DreamGaussian4D 在内的现有最先进方法。</li>
<li><strong>解决关键挑战：</strong> 有效地解决了时空不一致性、“果冻效应”和 Janus 问题，并实现了精确的骨架驱动姿势控制。</li>
<li><strong>灵活性：</strong> 支持任意长度的 4D 生成，并兼容 NeRF 和 Gaussian Splatting 等渲染器。</li>
</ul>
<p><strong>4. 论文中提到的局限性：</strong></p>
<ul>
<li><strong>缺乏布料动力学模拟：</strong> 当前模型无法模拟布料的动态行为，仅能生成人类化身。这主要是由于缺乏包含逼真布料行为的 4D 数据集。</li>
<li><strong>依赖标准扩散模型：</strong> 模型使用了标准的扩散模型，而更先进的方法（如 Flow Matching）尚未探索。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>布料动力学模拟：</strong> 纳入布料动力学模拟，以生成更逼真、更具表现力的化身。</li>
<li><strong>探索更先进的生成模型：</strong> 研究和应用 Flow Matching 等更先进的生成模型，以进一步提升性能。</li>
<li><strong>更广泛的应用：</strong> TriDiff-4D 的技术有望应用于虚拟现实体验、个性化化身和实时应用等领域。</li>
</ul>
<p><strong>总结：</strong></p>
<p>TriDiff-4D 是一项重要的研究成果，它通过创新的基于扩散的 Triplane 重定位方法，显著提高了 4D 化身生成的速度和质量。该方法通过显式学习 3D 结构和运动先验，并利用骨架驱动的条件生成，有效地解决了现有方法的诸多局限性，为实现高效、高质量的 4D 内容创作开辟了新的途径。其在生成速度和动画质量上的突破，使其在游戏、VR/AR 等领域具有巨大的应用潜力。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address these challenges, we propose TriDiff-4D, a novel 4D generative pipeline that employs diffusion-based triplane re-posing to produce high-quality, temporally coherent 4D avatars.</li>
<li>Experimental results demonstrate that TriDiff-4D significantly outperforms existing methods, reducing generation time from hours to seconds by eliminating the optimization process, while substantially improving the generation of complex motions with high-fidelity appearance and accurate 3D geometry.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.16662v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.16662v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.16661v1'></a></p>
<h2 id="dexterity-from-smart-lenses-multi-fingered-robot-manipulation-with-in-the-wild-human-demonstrations"><a href="https://arxiv.org/abs/2511.16661v1">Dexterity from Smart Lenses: Multi-Fingered Robot Manipulation with In-the-Wild Human Demonstrations</a></h2>
<p><strong>Authors:</strong> Irmak Guzey, Haozhi Qi, Julen Urain, Changhao Wang, Jessica Yin, Krishna Bodduluri, Mike Lambeta, Lerrel Pinto, Akshara Rai, Jitendra Malik, Tingfan Wu, Akash Sharma, Homanga Bharadhwaj</p>
<p><strong>Published:</strong> 2025-11-20</p>
<p><strong>Categories:</strong> cs.RO, cs.AI, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Learning multi-fingered robot policies from humans performing daily tasks in natural environments has long been a grand goal in the robotics community. Achieving this would mark significant progress toward generalizable robot manipulation in human environments, as it would reduce the reliance on labor-intensive robot data collection. Despite substantial efforts, progress toward this goal has been bottle-necked by the embodiment gap between humans and robots, as well as by difficulties in extracting relevant contextual and motion cues that enable learning of autonomous policies from in-the-wild human videos. We claim that with simple yet sufficiently powerful hardware for obtaining human data and our proposed framework AINA, we are now one significant step closer to achieving this dream. AINA enables learning multi-fingered policies from data collected by anyone, anywhere, and in any environment using Aria Gen 2 glasses. These glasses are lightweight and portable, feature a high-resolution RGB camera, provide accurate on-board 3D head and hand poses, and offer a wide stereo view that can be leveraged for depth estimation of the scene. This setup enables the learning of 3D point-based policies for multi-fingered hands that are robust to background changes and can be deployed directly without requiring any robot data (including online corrections, reinforcement learning, or simulation). We compare our framework against prior human-to-robot policy learning approaches, ablate our design choices, and demonstrate results across nine everyday manipulation tasks. Robot rollouts are best viewed on our website: https://aina-robot.github.io.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Dexterity from Smart Lenses: Multi-Fingered Robot Manipulation with In-the-Wild Human Demonstrations”的全面中文摘要：</p>
<p><strong>论文题目：</strong> Dexterity from Smart Lenses: Multi-Fingered Robot Manipulation with In-the-Wild Human Demonstrations</p>
<p><strong>作者：</strong> Irmak Guzey, Haozhi Qi, Julen Urain, Changhao Wang, Jessica Yin, Krishna Bodduluri, Mike Lambeta, Lerrel Pinto, Akshara Rai, Jitendra Malik, Tingfan Wu, Akash Sharma, Homanga Bharadhwaj</p>
<hr />
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
该论文旨在解决机器人领域一个长期存在的重大挑战：如何从人类在自然环境中的日常活动中学习多指灵巧操作的策略。当前的研究瓶颈在于人类与机器人之间的“具身鸿沟”（embodiment gap）以及从“野外”（in-the-wild）人类视频中提取有意义的上下文和运动线索的困难。这导致了对耗时且昂贵的机器人数据收集（包括在线纠正或模拟）的过度依赖，限制了机器人操作的泛化能力。</p>
<p><strong>2. 关键创新/方法论贡献：</strong>
论文提出了一个名为 <strong>AINA</strong>（意为“镜像”）的创新框架，它能够仅利用智能眼镜采集的“野外”人类演示数据来学习多指灵巧操作策略，而无需任何机器人数据（包括在线纠正、强化学习或模拟）。</p>
<ul>
<li><strong>数据采集：</strong> 利用 <strong>Project Aria Gen 2 智能眼镜</strong>进行数据采集。这些眼镜轻便、便携，配备高分辨率RGB摄像头、多个SLAM摄像头和IMU，能够实时估计用户头部和手部姿态，并提供立体视觉以进行深度估计。这使得在任何时间、任何地点、任何背景下采集人类演示成为可能。</li>
<li><strong>数据处理与对齐：</strong><ul>
<li><strong>3D 点云表示：</strong> AINA 将数据处理为3D点云表示，包括对象点和手部指尖点。这种表示对背景变化和人类与机器人之间的视觉差异具有鲁棒性。</li>
<li><strong>野外数据与场景内数据的对齐：</strong> 为了弥合“野外”演示与机器人部署环境之间的差距，AINA 引入了一个 <strong>单次场景内（in-scene）演示</strong>。该场景内演示用于将“野外”演示的坐标系对齐到机器人坐标系，通过计算对象质心之间的平移和使用 Kabsch 算法估计旋转来解决。</li>
</ul>
</li>
<li><strong>点基策略学习：</strong> AINA 采用基于 <strong>点云的策略学习</strong> 方法，构建了一个 <strong>Transformer-based 点云策略</strong>。该策略以手部指尖点和对象点的轨迹作为输入，预测后续的手部指尖轨迹。该模型利用了 Vector Neuron MLPs 来更好地捕捉3D几何信息，并使用 Transformer Encoder 来处理序列数据。</li>
<li><strong>无机器人数据训练：</strong> 整个训练过程完全基于人类的“野外”演示和一次场景内演示，<strong>不依赖任何机器人交互数据</strong>，这显著降低了数据收集的成本和难度。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong>
*   <strong>成功实现灵巧操作策略学习：</strong> AINA 在九项日常操作任务（如按压烤面包机、拾取玩具、开抽屉、擦拭等）上成功训练了多指灵巧机器人策略。
*   <strong>强大的泛化能力：</strong>
    *   <strong>空间泛化：</strong> AINA 在不同空间配置下表现出良好的泛化能力，即使在演示和部署场景存在高度差异时也能成功执行任务。
    *   <strong>对象泛化：</strong> 在测试新对象时，AINA 对形状相似的对象表现出良好的泛化能力，但对于形状和重量差异较大的对象则表现不佳。
    *   <strong>高度泛化：</strong> 通过收集少量额外场景内演示，AINA 能够适应不同高度的操作空间，证明了其灵活性。
*   <strong>超越现有方法：</strong> 与仅使用场景内数据、仅使用“野外”数据或结合场景内数据进行转换的基线方法相比，AINA 在大多数任务上取得了更高的成功率，尤其是在需要空间泛化的情况下。
*   <strong>重要意义：</strong> AINA 的成功标志着在实现通用机器人操作方面迈出了重要一步。它证明了利用智能眼镜采集的“野外”人类演示数据，可以有效地训练出能够直接部署到机器人上的灵巧操作策略，极大地减少了对昂贵机器人数据收集的依赖，为机器人学习更接近人类的日常操作能力开辟了新的途径。</p>
<p><strong>4. 提及的局限性：</strong>
*   <strong>缺乏力反馈：</strong> AINA 无法集成力反馈信息，因为手部姿态估计本身无法捕捉到触觉信息，而触觉信息对于精确的灵巧操作至关重要。
*   <strong>Aria Gen 2 相机同步问题：</strong> Aria Gen 2 眼镜的RGB和SLAM摄像头之间存在轻微的快门时间差异，快速的头部运动可能导致 RGB 图像中的对象像素与 SLAM 深度信息之间出现错位。
*   <strong>部署时的传感器差异：</strong> 部署时使用的 Realsense 相机与采集数据时使用的 Aria 眼镜在关键点上存在细微差异，这影响了部署的精确性。
*   <strong>对象泛化限制：</strong> 对于形状和重量差异较大的新对象，AINA 的泛化能力受到限制。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>集成力反馈：</strong> 探索集成其他可穿戴设备（如EMG传感器或力反馈手套）来捕捉触觉信息。
*   <strong>改进数据同步：</strong> 解决 Aria Gen 2 眼镜的相机同步问题，或采用更鲁棒的3D对象跟踪算法。
*   <strong>实时深度估计：</strong> 优化 FoundationStereo 等框架，以实现近乎实时的深度估计，从而在部署时直接使用 Aria 眼镜的输入。
*   <strong>更强的对象泛化能力：</strong> 研究如何让模型更好地泛化到形状和重量差异较大的新对象。
*   <strong>更广泛的任务和环境：</strong> 将 AINA 应用于更广泛、更复杂的日常操作任务和更具挑战性的环境。</p>
<p><strong>对计算机视觉领域的新颖性/重要性：</strong></p>
<p>AINA 的主要贡献在于其<strong>完全摆脱了对机器人交互数据的依赖</strong>，仅通过智能眼镜采集的“野外”人类演示数据就实现了多指灵巧操作策略的学习。这在以下几个方面对计算机视觉领域具有重要意义：</p>
<ul>
<li><strong>“野外”数据利用的突破：</strong> 它展示了如何有效地从非结构化、非约束性的“野外”人类视频中提取有用的3D几何和运动信息，并将其转化为机器人可用的策略。这比以往依赖结构化场景或特定数据集的方法更具可扩展性。</li>
<li><strong>3D 点云表示的有效性：</strong> AINA 证明了使用3D点云作为输入表示，能够有效弥合人类演示与机器人执行之间的具身鸿沟，并对背景变化具有鲁棒性，这对于需要精确空间理解的机器人任务至关重要。</li>
<li><strong>智能眼镜作为数据采集平台：</strong> 该工作突出了智能眼镜作为一种轻便、易于使用的平台，在采集丰富、高保真的“野外”人类演示数据方面的巨大潜力，为机器人学习开辟了新的数据来源。</li>
<li><strong>端到端学习的范式转变：</strong> AINA 提供了一种端到端的学习范式，将原始的“野外”人类视频直接转化为机器人可执行的策略，简化了整个流程，并减少了对人工特征工程或复杂中间步骤的依赖。</li>
</ul>
<p>总而言之，AINA 框架通过创新的数据采集、处理和学习方法，显著推动了从人类演示中学习机器人灵巧操作的研究，为实现更通用、更易于部署的机器人助手提供了有力的技术支撑。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We compare our framework against prior human-to-robot policy learning approaches, ablate our design choices, and demonstrate results across nine everyday manipulation tasks.</li>
<li>Robot rollouts are best viewed on our website: https://aina-robot.github.io.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.16661v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.16661v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-11-21 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
