<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-12-01 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-28
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-12-01
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-11-28/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../../depth_completion/DNet/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-12-01">Arxiv Computer Vision Papers - 2025-12-01</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#video-r2-reinforcing-consistent-and-grounded-reasoning-in-multimodal-language-models" class="nav-link">Video-R2: Reinforcing Consistent and Grounded Reasoning in Multimodal Language Models</a>
                </li>
                <li class="nav-item">
                    <a href="#video-com-interactive-video-reasoning-via-chain-of-manipulations" class="nav-link">Video-CoM: Interactive Video Reasoning via Chain of Manipulations</a>
                </li>
                <li class="nav-item">
                    <a href="#visual-generation-tuning" class="nav-link">Visual Generation Tuning</a>
                </li>
                <li class="nav-item">
                    <a href="#hunyuan-gamecraft-2-instruction-following-interactive-game-world-model" class="nav-link">Hunyuan-GameCraft-2: Instruction-following Interactive Game World Model</a>
                </li>
                <li class="nav-item">
                    <a href="#dismo-disentangled-motion-representations-for-open-world-motion-transfer" class="nav-link">DisMo: Disentangled Motion Representations for Open-World Motion Transfer</a>
                </li>
                <li class="nav-item">
                    <a href="#vqrae-representation-quantization-autoencoders-for-multimodal-understanding-generation-and-reconstruction" class="nav-link">VQRAE: Representation Quantization Autoencoders for Multimodal Understanding, Generation and Reconstruction</a>
                </li>
                <li class="nav-item">
                    <a href="#deal-300k-diffusion-based-editing-area-localization-with-a-300k-scale-dataset-and-frequency-prompted-baseline" class="nav-link">DEAL-300K: Diffusion-based Editing Area Localization with a 300K-Scale Dataset and Frequency-Prompted Baseline</a>
                </li>
                <li class="nav-item">
                    <a href="#optimizing-multimodal-language-models-through-attention-based-interpretability" class="nav-link">Optimizing Multimodal Language Models through Attention-based Interpretability</a>
                </li>
                <li class="nav-item">
                    <a href="#simscale-learning-to-drive-via-real-world-simulation-at-scale" class="nav-link">SimScale: Learning to Drive via Real-World Simulation at Scale</a>
                </li>
                <li class="nav-item">
                    <a href="#markovian-scale-prediction-a-new-era-of-visual-autoregressive-generation" class="nav-link">Markovian Scale Prediction: A New Era of Visual Autoregressive Generation</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-12-01">Arxiv Computer Vision Papers - 2025-12-01</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>好的，作为一名专注于计算机视觉和机器学习的研究助理，我将为您提供一份关于近期 Arxiv 计算机视觉领域论文的简明执行摘要。</p>
<hr />
<p><strong>执行摘要：2025年11月28日 Arxiv 计算机视觉论文精选</strong></p>
<p><strong>主要主题与趋势：</strong></p>
<p>本期 Arxiv 论文集聚焦于<strong>多模态理解与生成</strong>，特别是<strong>视频内容的处理和交互</strong>，以及<strong>视觉生成模型的创新</strong>。多模态语言模型在视频推理、游戏世界建模和运动迁移方面的应用是突出亮点。此外，<strong>可解释性</strong>和<strong>大规模数据驱动的模拟学习</strong>也展现出重要进展。</p>
<p><strong>亮点与创新：</strong></p>
<ul>
<li><strong>视频推理与交互：</strong> "Video-R2" 和 "Video-CoM" 均在视频的多模态推理能力上取得了显著进展，前者强调一致性和接地气的推理，后者则提出了交互式视频推理框架。</li>
<li><strong>生成模型调优与应用：</strong> "Visual Generation Tuning" 和 "DEAL-300K" 分别探索了通用视觉生成模型的微调策略和扩散模型在图像编辑区域定位上的创新应用，后者还引入了大规模数据集。</li>
<li><strong>大规模模拟与游戏AI：</strong> "Hunyuan-GameCraft-2" 和 "SimScale" 展示了在复杂交互环境（如游戏世界）中构建世界模型和通过大规模模拟进行学习的强大能力，预示着更智能的AI代理。</li>
<li><strong>运动表示与生成：</strong> "DisMo" 在解耦运动表示方面提供了新思路，为开放世界运动迁移奠定了基础。</li>
</ul>
<p><strong>新兴研究方向与技术：</strong></p>
<ul>
<li><strong>视频多模态推理的精细化：</strong> 从简单的视频理解向更深层次的、具备因果和交互能力的推理发展。</li>
<li><strong>生成模型的精细化控制与编辑：</strong> 探索更精确、更可控的图像和视频生成与编辑技术。</li>
<li><strong>大规模、真实感模拟环境的构建与利用：</strong> 为训练更鲁棒、泛化能力更强的AI模型提供新的途径。</li>
<li><strong>解耦表示学习在运动等特定领域的深化应用。</strong></li>
<li><strong>注意力机制在多模态模型可解释性中的作用日益凸显。</strong></li>
</ul>
<p><strong>建议阅读全文的论文：</strong></p>
<p>考虑到其对多模态理解、视频交互以及生成模型前沿的贡献，以下论文值得深入阅读：</p>
<ol>
<li><strong>"Video-R2: Reinforcing Consistent and Grounded Reasoning in Multimodal Language Models"</strong>: 对于理解多模态模型在视频推理中的一致性和接地气能力至关重要。</li>
<li><strong>"Video-CoM: Interactive Video Reasoning via Chain of Manipulations"</strong>: 提供了视频交互式推理的新范式，对需要与视频内容进行动态交互的应用具有启发意义。</li>
<li><strong>"DEAL-300K: Diffusion-based Editing Area Localization with a 300K-Scale Dataset and Frequency-Prompted Baseline"</strong>: 在扩散模型编辑和大规模数据集构建方面具有重要价值，对图像编辑领域的研究者尤为重要。</li>
<li><strong>"SimScale: Learning to Drive via Real-World Simulation at Scale"</strong>: 展示了大规模模拟在现实世界任务（如自动驾驶）中的潜力，对强化学习和机器人领域的研究者具有参考价值。</li>
</ol>
<hr />
<p>这份摘要旨在帮助您快速把握本期 Arxiv 论文的核心内容和发展趋势。希望它能为您节省宝贵的研究时间。</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2511.23478v1">Video-R2: Reinforcing Consistent and Grounded Reasoning in Multimodal Language Models</a></li>
<li><a href="#2511.23477v1">Video-CoM: Interactive Video Reasoning via Chain of Manipulations</a></li>
<li><a href="#2511.23469v1">Visual Generation Tuning</a></li>
<li><a href="#2511.23429v1">Hunyuan-GameCraft-2: Instruction-following Interactive Game World Model</a></li>
<li><a href="#2511.23428v1">DisMo: Disentangled Motion Representations for Open-World Motion Transfer</a></li>
<li><a href="#2511.23386v1">VQRAE: Representation Quantization Autoencoders for Multimodal Understanding, Generation and Reconstruction</a></li>
<li><a href="#2511.23377v1">DEAL-300K: Diffusion-based Editing Area Localization with a 300K-Scale Dataset and Frequency-Prompted Baseline</a></li>
<li><a href="#2511.23375v1">Optimizing Multimodal Language Models through Attention-based Interpretability</a></li>
<li><a href="#2511.23369v1">SimScale: Learning to Drive via Real-World Simulation at Scale</a></li>
<li><a href="#2511.23334v1">Markovian Scale Prediction: A New Era of Visual Autoregressive Generation</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2511.23478v1'></a></p>
<h2 id="video-r2-reinforcing-consistent-and-grounded-reasoning-in-multimodal-language-models"><a href="https://arxiv.org/abs/2511.23478v1">Video-R2: Reinforcing Consistent and Grounded Reasoning in Multimodal Language Models</a></h2>
<p><strong>Authors:</strong> Muhammad Maaz, Hanoona Rasheed, Fahad Shahbaz Khan, Salman Khan</p>
<p><strong>Published:</strong> 2025-11-28</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Reasoning over dynamic visual content remains a central challenge for multimodal large language models. Recent thinking models generate explicit reasoning traces for interpretability; however, their reasoning often appears convincing while being logically inconsistent or weakly grounded in visual evidence. We identify and formalize these issues through two diagnostic metrics: Think Answer Consistency (TAC), which measures the alignment between reasoning and answers, and Video Attention Score (VAS), which captures the extent to which reasoning depends on visual versus textual cues. Analysis across 11 video reasoning benchmarks shows that current models rely heavily on linguistic priors rather than visual content. To address this, we propose a reinforcement learning approach that enhances both temporal precision and reasoning consistency. Our approach combines timestamp aware supervised fine tuning with Group Relative Policy Optimization (GRPO) guided by a novel Temporal Alignment Reward (TAR). This dual step post training stage encourages temporally aligned and causally coherent video reasoning. The resulting model, Video R2, achieves consistently higher TAC, VAS, and accuracy across multiple benchmarks, demonstrating that improvements in temporal alignment and reasoning coherence lead to more accurate and trustworthy video understanding. Our code, dataset, and model will be open sourced.</p>
<p><strong>Analysis:</strong></p>
<p>作为一名计算机视觉和机器学习领域的专家，我对这篇论文摘要进行了深入分析，并为您提供以下中文解读：</p>
<p><strong>1. 论文的主要贡献（2-3句话）</strong></p>
<p>这篇论文“Video-R2”的核心贡献在于，它识别并解决了当前多模态大语言模型在视频推理中存在的“逻辑不一致”和“视觉证据不足”的问题。作者提出了一种新颖的强化学习方法，通过引入时间对齐奖励（TAR）和结合时间感知监督微调与组相对策略优化（GRPO），显著提升了模型在视频推理任务中的准确性、逻辑一致性以及对视觉信息的依赖程度。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<p>论文的关键创新在于其提出的强化学习框架，旨在提升视频推理的“时间精度”和“推理一致性”。具体方法论包括：</p>
<ul>
<li><strong>诊断指标：</strong> 引入了两个量化指标来评估模型问题：<ul>
<li><strong>Think Answer Consistency (TAC):</strong> 衡量推理过程与最终答案之间的一致性。</li>
<li><strong>Video Attention Score (VAS):</strong> 评估推理过程对视觉线索和文本线索的依赖程度。</li>
</ul>
</li>
<li><strong>问题诊断：</strong> 通过分析发现现有模型过度依赖语言先验，而对视觉内容依赖不足。</li>
<li><strong>强化学习框架：</strong><ul>
<li><strong>时间感知监督微调 (Timestamp Aware Supervised Fine Tuning):</strong> 确保模型在微调过程中能够理解和利用视频的时间信息。</li>
<li><strong>组相对策略优化 (Group Relative Policy Optimization - GRPO):</strong> 一种强化学习优化算法，用于指导模型学习。</li>
<li><strong>时间对齐奖励 (Temporal Alignment Reward - TAR):</strong> 这是一个新颖的奖励函数，专门设计用于鼓励模型生成在时间上对齐且因果连贯的视频推理。</li>
</ul>
</li>
</ul>
<p><strong>3. 对该领域的潜在影响</strong></p>
<p>这篇论文对多模态大语言模型在视频理解领域具有重要的潜在影响：</p>
<ul>
<li><strong>提升可信度：</strong> 通过解决逻辑不一致和视觉证据不足的问题，Video-R2有望生成更值得信赖的视频推理结果，这对于需要高可靠性的应用至关重要。</li>
<li><strong>推动更深层次的视觉理解：</strong> 促使模型从单纯的文本关联转向更深入的视觉内容分析，从而实现更强的视觉推理能力。</li>
<li><strong>为视频推理设定新的基准：</strong> 提出的诊断指标和改进方法，可能成为未来视频推理模型研究和评估的新标准。</li>
<li><strong>促进可解释性研究：</strong> 尽管摘要中提到“生成显式推理轨迹”，但其推理的“逻辑不一致”是现有模型的痛点。Video-R2的改进有望使生成的推理轨迹更加可靠和有意义。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用</strong></p>
<p>这项研究的成果可以广泛应用于以下领域：</p>
<ul>
<li><strong>视频问答 (Video Question Answering - Video QA):</strong> 提升模型回答复杂视频问题的准确性和逻辑性。</li>
<li><strong>视频摘要 (Video Summarization):</strong> 生成更具连贯性和信息量的视频摘要。</li>
<li><strong>视频内容分析与检索 (Video Content Analysis and Retrieval):</strong> 更好地理解视频内容，实现更精准的检索。</li>
<li><strong>自动驾驶 (Autonomous Driving):</strong> 提升自动驾驶系统对动态场景的理解和预测能力。</li>
<li><strong>机器人交互 (Robotics Interaction):</strong> 使机器人能够更好地理解和响应视频指令或场景。</li>
<li><strong>医疗影像分析 (Medical Imaging Analysis):</strong> 在分析动态医疗影像（如CT、MRI）时，提高诊断的准确性和可解释性。</li>
<li><strong>教育科技 (EdTech):</strong> 用于生成更具解释性的教学视频内容。</li>
</ul>
<p><strong>5. 从摘要中推断出的局限性</strong></p>
<p>尽管摘要展示了积极的成果，但仍可以推断出一些潜在的局限性：</p>
<ul>
<li><strong>计算成本：</strong> 强化学习方法，尤其是涉及策略优化和奖励设计的，通常计算成本较高，训练和部署可能需要大量的计算资源。</li>
<li><strong>泛化能力：</strong> 虽然在11个基准上取得了成功，但模型在未见过的新颖视频类型或推理任务上的泛化能力仍需进一步验证。</li>
<li><strong>奖励函数设计：</strong> TAR奖励函数的具体设计细节并未在摘要中披露，其有效性可能高度依赖于奖励函数的精细调优。</li>
<li><strong>对“逻辑一致性”的定义：</strong> 摘要中提到“逻辑不一致”，但“逻辑”的定义在复杂视频推理中可能存在主观性，其评估的全面性有待深入研究。</li>
<li><strong>对“视觉证据”的依赖程度：</strong> VAS指标衡量了依赖程度，但如何精确量化和区分“强视觉证据”与“弱视觉证据”仍是一个挑战。</li>
</ul>
<p>总而言之，这篇论文“Video-R2”通过引入创新的强化学习框架和诊断指标，有效地解决了多模态大语言模型在视频推理中的关键痛点，为构建更强大、更可信赖的视频理解系统奠定了基础。其对时间对齐和推理一致性的关注，使其在计算机视觉领域具有重要的研究价值和应用前景。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this, we propose a reinforcement learning approach that enhances both temporal precision and reasoning consistency.</li>
<li>Our approach combines timestamp aware supervised fine tuning with Group Relative Policy Optimization (GRPO) guided by a novel Temporal Alignment Reward (TAR).</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.23478v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.23478v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.23477v1'></a></p>
<h2 id="video-com-interactive-video-reasoning-via-chain-of-manipulations"><a href="https://arxiv.org/abs/2511.23477v1">Video-CoM: Interactive Video Reasoning via Chain of Manipulations</a></h2>
<p><strong>Authors:</strong> Hanoona Rasheed, Mohammed Zumri, Muhammad Maaz, Ming-Hsuan Yang, Fahad Shahbaz Khan, Salman Khan</p>
<p><strong>Published:</strong> 2025-11-28</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Recent multimodal large language models (MLLMs) have advanced video understanding, yet most still "think about videos" ie once a video is encoded, reasoning unfolds entirely in text, treating visual input as a static context. This passive paradigm creates a semantic bottleneck: models cannot rewatch, refocus, or verify evidence, leading to shallow visual reasoning on tasks requiring fine grained spatio temporal understanding. In this work, we introduce Interactive Video Reasoning, a new paradigm that transforms video into an active cognitive workspace, enabling models to "think with videos". Our model, Video CoM, reasons through a Chain of Manipulations (CoM), performing iterative visual actions to gather and refine evidence. To support this behavior, we construct Video CoM Instruct, an 18K instruction tuning dataset curated for multi step manipulation reasoning. Beyond supervised learning, we further optimize the manipulation policy via reinforcement learning with reasoning aware Group Relative Policy Optimization (GRPO). Unlike prior work that relies solely on sparse answer rewards, our method introduces step level reasoning rewards, guiding the model toward grounded and consistent reasoning. Video CoM achieves strong results across nine video reasoning benchmarks, improving average performance by 3.6 percent over recent state of the art models, while training on only 25K SFT and 3K GRPO video samples, significantly fewer than comparable large scale models. Ablation studies demonstrate that reasoning aware rewards improve both accuracy and interpretability. Code: https://github.com/mbzuai-oryx/Video-CoM</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Video-CoM: Interactive Video Reasoning via Chain of Manipulations”的全面中文摘要：</p>
<p><strong>论文题目：</strong> Video-CoM: Interactive Video Reasoning via Chain of Manipulations</p>
<p><strong>作者：</strong> Hanoona Rasheed, Mohammed Zumri, Muhammad Maaz, Ming-Hsuan Yang, Fahad Shahbaz Khan, Salman Khan</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 主要问题/研究挑战：</strong>
当前的多模态大型语言模型（MLLMs）在视频理解方面取得了显著进展，但它们大多采用“思考视频”（think about videos）的范式。这意味着视频一旦被编码，推理过程完全在文本空间中进行，将视觉输入视为静态上下文。这种被动的感知方式造成了语义瓶颈，使得模型无法重新观看、重新聚焦或验证证据，导致在需要精细时空理解的任务上进行浅层视觉推理。尤其是在需要精细时空理解、精确空间推理或多步证据聚合的任务上，模型容易偏离视觉证据，依赖世界知识，从而导致推理不一致且缺乏视觉基础。</p>
<p><strong>2. 关键创新/方法贡献：</strong>
为了解决上述问题，论文提出了<strong>交互式视频推理（Interactive Video Reasoning）</strong>的新范式，将视频转化为一个<strong>主动的认知工作空间</strong>，使模型能够“用视频思考”（think with videos）。其核心创新包括：</p>
<ul>
<li><strong>链式操作（Chain of Manipulations, CoM）机制：</strong> Video-CoM 模型通过一系列<strong>原子视觉操作</strong>（包括 <code>find-segment</code>、<code>find-frame</code> 和 <code>spatial-zoom</code>）来主动与视频交互，以收集和精炼视觉证据。这种操作序列构成了可解释的推理轨迹，每个步骤都以局部证据为基础。</li>
<li><strong>Video-CoM-Instruct 数据集：</strong> 构建了一个包含 18K 样本的<strong>指令调优数据集</strong>，专门用于多步操作推理，旨在引导模型进行操作驱动的视频推理。该数据集精心策划，要求模型执行一个或多个视觉操作来收集证据，模拟人类提取局部信息的方式。</li>
<li><strong>推理感知组相对策略优化（Reasoning-Aware Group Relative Policy Optimization, RA-GRPO）：</strong> 引入了一种新的强化学习目标函数，通过<strong>步级推理奖励</strong>来优化操作策略。与仅依赖稀疏答案奖励的传统方法不同，RA-GRPO 评估中间操作（如时间段 IoU、帧召回率、空间 IoU），即使最终答案不正确，也能为正确的中间步骤提供部分信用，从而引导模型进行更具视觉基础和一致性的推理。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong>
Video-CoM 在九个视频推理基准测试中取得了<strong>显著的性能提升</strong>，平均性能比最近的<strong>最先进模型提高了 3.6%</strong>。尤其值得注意的是，Video-CoM 在 <strong>Video-CoM-Bench</strong>（一个专门为操作中心推理设计的基准）上取得了最大的提升（68.7%），这突显了其交互式推理能力。更重要的是，Video-CoM 仅使用了 <strong>25K SFT 和 3K GRPO 视频样本</strong>进行训练，远少于同类大型模型，显示了其<strong>训练效率</strong>。消融研究表明，推理感知奖励显著提高了模型的<strong>准确性和可解释性</strong>。</p>
<p><strong>4. 论文中提到的局限性：</strong>
*   <strong>视频中的空间定位：</strong> 在视频中准确地进行空间定位仍然是一个挑战，尤其是在需要识别文本或数字等精细细节时。这需要大规模、高质量的标注数据，而这类数据目前相对稀缺。
*   <strong>视频源的局限性：</strong> 构建操作特定的数据集依赖于具有丰富时空变化和局部细节的视频。低场景多样性的视频可能难以生成需要迭代视觉交互的问题。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   进一步提升模型在复杂空间定位任务上的能力。
*   探索更广泛的视频源，以覆盖更多样化的场景和操作。
*   研究如何将 Video-CoM 的交互式推理范式扩展到其他模态或更复杂的任务。
*   进一步优化训练效率和模型的可扩展性。</p>
<p><strong>总结：</strong>
“Video-CoM: Interactive Video Reasoning via Chain of Manipulations” 论文提出了一种创新的<strong>交互式视频推理范式</strong>，通过<strong>链式操作（CoM）</strong>和<strong>推理感知奖励（RA-GRPO）</strong>，使 MLLMs 能够像人类一样“用视频思考”，主动收集和精炼视觉证据。该方法在多个视频推理任务上取得了优异的性能，并且训练效率高，为视频理解领域开辟了新的研究方向。该工作强调了从被动感知转向主动交互式推理的重要性，为构建更强大、更具解释性的视频理解模型奠定了基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this work, we introduce Interactive Video Reasoning, a new paradigm that transforms video into an active cognitive workspace, enabling models to "think with videos".</li>
<li>Unlike prior work that relies solely on sparse answer rewards, our method introduces step level reasoning rewards, guiding the model toward grounded and consistent reasoning.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.23477v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.23477v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.23469v1'></a></p>
<h2 id="visual-generation-tuning"><a href="https://arxiv.org/abs/2511.23469v1">Visual Generation Tuning</a></h2>
<p><strong>Authors:</strong> Jiahao Guo, Sinan Du, Jingfeng Yao, Wenyu Liu, Bo Li, Haoxiang Cao, Kun Gai, Chun Yuan, Kai Wu, Xinggang Wang</p>
<p><strong>Published:</strong> 2025-11-28</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Large Vision Language Models (VLMs) effectively bridge the modality gap through extensive pretraining, acquiring sophisticated visual representations aligned with language. However, it remains underexplored whether these representations, optimized for multimodal understanding tasks, harbor an inherent potential for visual generation. In this paper, we propose VGT, Visual Generation Tuning, a novel paradigm designed to stimulate the underlying capabilities of visual generation within any vision language models. By performing efficient visual generation tuning on well-pretrained VLMs, we significantly mitigate the alignment costs and accelerate the convergence of autoregressive modeling in the continuous space (20x speedup). Specifically, we dismiss the entangled pixel-level VAEs designed for diffusion transformers and formulate VGT-AE through aligning the semantic encoders from pretrained VLMs with the latent representations of pixel decoders. In image reconstruction tasks, we achieve 26.67 PSNR and 0.50 rFID at a 28x compression ratio, outperforming specialized VAEs; in visual generation tasks, we achieve state-of-the-art outcomes among autoregressive models, 0.77 on GenEval and 78.73 on DPG-Bench. Furthermore, our proposed VGT showcases significant scaling promise and is versatile for endowing any VLMs trained for multimodal understanding with the capabilities of visual generation, which paves the new avenue to explore next-generation unified multimodal foundation models. Models and codes are available at https://github.com/hustvl/VGT.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Visual Generation Tuning”的全面摘要：</p>
<p><strong>论文题目：</strong> Visual Generation Tuning</p>
<p><strong>作者：</strong> Jiahao Guo, Sinan Du, Jingfeng Yao, Wenyu Liu, Bo Li, Haoxiang Cao, Kun Gai, Chun Yuan, Kai Wu, Xinggang Wang</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心问题：</strong></p>
<p>该论文的核心研究问题是：<strong>大型视觉语言模型（VLMs）在多模态理解任务上经过预训练所获得的视觉表征，是否蕴含了生成图像的内在潜力？</strong> 尽管VLMs在连接视觉和语言模态方面取得了巨大成功，但它们在视觉生成方面的能力尚未得到充分探索。现有的视觉生成方法，特别是基于自回归的模型，通常依赖于与自回归建模不完全对齐的像素级VAE（变分自编码器），这导致了训练的低效和不稳定性。</p>
<p><strong>2. 主要创新点/方法贡献：</strong></p>
<p>作者提出了<strong>Visual Generation Tuning (VGT)</strong>，一种新颖的范式，旨在激发现有预训练VLMs中潜在的视觉生成能力。其关键创新点包括：</p>
<ul>
<li><strong>VGT-AE（Visual Generation Tuning-AutoEncoder）：</strong> 这是一个核心组件，通过将预训练VLM的语义编码器与轻量级像素解码器的潜在空间对齐来实现。这不同于传统扩散模型中用于像素级重建的VAE，而是专注于生成任务。VGT-AE采用两阶段训练策略：<ul>
<li><strong>第一阶段（语义保持重建）：</strong> 使用重建损失和语义自蒸馏损失来优化编码器和解码器，确保重建质量的同时保留语义结构。</li>
<li><strong>第二阶段（潜在空间正则化）：</strong> 冻结编码器，优化解码器，并引入通道归一化和高斯噪声注入，使潜在空间更符合自回归生成所需的标准高斯先验，提高其分布稳定性和生成友好性。</li>
</ul>
</li>
<li><strong>QueryAR（Query-based Autoregressive）：</strong> 针对自回归生成阶段，提出了一种创新的位置查询机制。它通过在输入序列中交错位置查询和潜在表示，允许在训练时保持因果关系，同时在推理时实现部分并行解码，显著提高了生成效率。</li>
<li><strong>高效的微调范式：</strong> VGT通过高效的视觉生成微调，显著降低了对齐成本，并加速了连续空间自回归建模的收敛速度（最高可达20倍加速）。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong></p>
<ul>
<li><strong>重建性能：</strong> VGT-AE在图像重建任务上取得了优异的性能，在28倍压缩比下达到了26.67 PSNR和0.50 rFID，优于专门的VAE。</li>
<li><strong>生成性能：</strong> 在视觉生成任务上，VGT实现了<strong>最先进（SOTA）的自回归模型性能</strong>，在GenEval上达到0.77，在DPG-Bench上达到78.73。</li>
<li><strong>数据效率：</strong> VGT在仅使用25M训练样本的情况下，就取得了与大型扩散模型（如SDXL、SD3-Medium）相媲美的性能，这挑战了传统观念，即自回归模型需要海量数据才能达到高质量生成。</li>
<li><strong>通用性与可扩展性：</strong> VGT被证明对各种预训练VLMs（如Qwen2.5-VL和InternVL3）都具有<strong>高度的通用性</strong>，能够赋予它们视觉生成能力，为构建下一代统一的多模态基础模型铺平了道路。</li>
<li><strong>效率提升：</strong> QueryAR通过位置查询机制，在保持生成质量的同时，实现了显著的推理加速（例如，4倍加速下仍能保持竞争力）。</li>
</ul>
<p><strong>4. 论文中提到的局限性：</strong></p>
<ul>
<li><strong>重建与生成的权衡：</strong> 论文的消融研究（Section 4.4.3）表明，在VGT-AE的训练中，存在重建保真度和生成能力之间的权衡。过度优化重建可能导致生成性能下降，反之亦然。虽然VGT-AE通过两阶段训练实现了良好的平衡，但这种权衡仍然存在。</li>
<li><strong>模型大小与性能：</strong> 虽然VGT展示了良好的可扩展性，但不同大小的模型在不同任务上的表现仍有差异。例如，在Table 6中，0.6B的模型在GenEval和DPG-Bench上的得分低于1.6B的模型。</li>
<li><strong>对齐的复杂性：</strong> 尽管VGT大大降低了对齐成本，但论文也提到，即使在AE和LLM来自不同VLM家族的“不匹配”情况下，VGT-AE仍然优于VAE基线，这表明跨模态对齐仍然是影响性能的一个因素。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>更精细的生成控制：</strong> 尽管VGT在生成方面表现出色，但未来可以探索更精细的控制机制，以实现对生成图像的风格、内容或特定属性的更精确调控。</li>
<li><strong>多模态融合的进一步探索：</strong> VGT为统一多模态模型提供了新的视角。未来的工作可以进一步探索如何更有效地融合视觉和语言信息，以实现更强大的理解和生成能力。</li>
<li><strong>更广泛的模态扩展：</strong> VGT的范式可以扩展到其他模态，例如音频或视频生成，以构建更全面的多模态生成模型。</li>
<li><strong>更高效的训练和推理技术：</strong> 尽管VGT已经实现了显著的效率提升，但持续探索更高效的训练算法和推理优化技术仍然是重要的研究方向。</li>
<li><strong>模型安全与伦理考量：</strong> 随着生成模型能力的增强，研究其潜在的滥用风险以及开发相应的安全和伦理对策也变得日益重要。</li>
</ul>
<p><strong>总结：</strong></p>
<p>“Visual Generation Tuning”论文提出了一种开创性的方法，成功地将大型视觉语言模型（VLMs）的强大视觉理解能力转化为高效的视觉生成能力。通过引入VGT-AE和QueryAR等创新组件，该方法不仅在重建和生成任务上取得了最先进的性能，而且显著提高了数据效率和训练速度。VGT范式的通用性和可扩展性使其成为构建下一代统一多模态基础模型的有力候选方案，为该领域的研究开辟了新的方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this paper, we propose VGT, Visual Generation Tuning, a novel paradigm designed to stimulate the underlying capabilities of visual generation within any vision language models.</li>
<li>In image reconstruction tasks, we achieve 26.67 PSNR and 0.50 rFID at a 28x compression ratio, outperforming specialized VAEs; in visual generation tasks, we achieve state-of-the-art outcomes among autoregressive models, 0.77 on GenEval and 78.73 on DPG-Bench.</li>
<li>Furthermore, our proposed VGT showcases significant scaling promise and is versatile for endowing any VLMs trained for multimodal understanding with the capabilities of visual generation, which paves the new avenue to explore next-generation unified multimodal foundation models.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.23469v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.23469v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.23429v1'></a></p>
<h2 id="hunyuan-gamecraft-2-instruction-following-interactive-game-world-model"><a href="https://arxiv.org/abs/2511.23429v1">Hunyuan-GameCraft-2: Instruction-following Interactive Game World Model</a></h2>
<p><strong>Authors:</strong> Junshu Tang, Jiacheng Liu, Jiaqi Li, Longhuang Wu, Haoyu Yang, Penghao Zhao, Siruis Gong, Xiang Yuan, Shuai Shao, Qinglin Lu</p>
<p><strong>Published:</strong> 2025-11-28</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Recent advances in generative world models have enabled remarkable progress in creating open-ended game environments, evolving from static scene synthesis toward dynamic, interactive simulation. However, current approaches remain limited by rigid action schemas and high annotation costs, restricting their ability to model diverse in-game interactions and player-driven dynamics. To address these challenges, we introduce Hunyuan-GameCraft-2, a new paradigm of instruction-driven interaction for generative game world modeling. Instead of relying on fixed keyboard inputs, our model allows users to control game video contents through natural language prompts, keyboard, or mouse signals, enabling flexible and semantically rich interaction within generated worlds. We formally defined the concept of interactive video data and developed an automated process to transform large-scale, unstructured text-video pairs into causally aligned interactive datasets. Built upon a 14B image-to-video Mixture-of-Experts(MoE) foundation model, our model incorporates a text-driven interaction injection mechanism for fine-grained control over camera motion, character behavior, and environment dynamics. We introduce an interaction-focused benchmark, InterBench, to evaluate interaction performance comprehensively. Extensive experiments demonstrate that our model generates temporally coherent and causally grounded interactive game videos that faithfully respond to diverse and free-form user instructions such as "open the door", "draw a torch", or "trigger an explosion".</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为一名计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行深入分析。</p>
<p><strong>论文摘要分析：Hunyuan-GameCraft-2: Instruction-following Interactive Game World Model</strong></p>
<p><strong>1. 论文的主要贡献（2-3句话的简洁总结）</strong></p>
<p>该论文提出了Hunyuan-GameCraft-2，一种新颖的指令驱动交互式游戏世界建模范式。它通过自然语言指令、键盘或鼠标信号实现对生成游戏视频内容的灵活控制，克服了现有方法在动作模式僵化和标注成本高昂方面的限制。该模型能够生成时间连贯且因果关系明确的交互式游戏视频，并能忠实响应多样化的自由形式用户指令。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<ul>
<li><strong>指令驱动的交互范式 (Instruction-Driven Interaction Paradigm):</strong> 这是最核心的创新。不同于以往依赖预设的、僵化的动作模式（如固定键盘输入），Hunyuan-GameCraft-2允许用户使用自然语言（如“开门”、“拔出火把”、“触发爆炸”）来控制游戏世界的动态。这种方式极大地提升了交互的灵活性和语义丰富度。</li>
<li><strong>交互式视频数据的定义与自动化处理:</strong> 论文正式定义了“交互式视频数据”的概念，并开发了一种自动化流程，将大规模、非结构化的文本-视频对转化为因果对齐的交互式数据集。这解决了构建高质量交互式数据集的难题，为模型训练提供了基础。</li>
<li><strong>基于14B MoE基础模型的文本驱动交互注入机制:</strong> 模型构建在一个强大的140亿参数的图像到视频MoE（Mixture-of-Experts）基础模型之上。关键在于其“文本驱动的交互注入机制”，该机制能够对相机运动、角色行为和环境动态进行精细化控制，将自然语言指令有效地转化为视频中的具体动作和变化。</li>
<li><strong>交互式基准测试 (InterBench):</strong> 论文引入了一个专门用于评估交互性能的基准测试集InterBench，这为衡量和比较不同交互式视频生成模型的能力提供了一个标准化的平台。</li>
</ul>
<p><strong>3. 对该领域的潜在影响</strong></p>
<ul>
<li><strong>推动生成式世界模型的进步:</strong> Hunyuan-GameCraft-2将生成式世界模型从静态场景合成和简单动态模拟提升到了一个全新的水平，使其能够理解并响应复杂的、语义丰富的用户指令，从而创造出更具沉浸感和可玩性的虚拟世界。</li>
<li><strong>降低交互式内容创作门槛:</strong> 通过自然语言指令控制，极大地降低了用户创建和编辑交互式视频内容的门槛，使得非专业人士也能轻松地生成复杂的交互场景。</li>
<li><strong>促进人机交互在多模态领域的融合:</strong> 该研究是多模态学习（文本、视觉、动作）在游戏领域深度融合的典范，为未来更自然、更直观的人机交互方式提供了新的思路。</li>
<li><strong>为游戏开发和虚拟现实/增强现实 (VR/AR) 领域带来新机遇:</strong> 这种技术可以极大地加速游戏关卡设计、NPC行为模拟以及VR/AR场景的动态生成，为游戏开发者和VR/AR内容创作者提供强大的工具。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用</strong></p>
<ul>
<li><strong>游戏开发:</strong> 自动化关卡设计、动态NPC行为生成、交互式剧情生成、游戏测试自动化。</li>
<li><strong>虚拟现实/增强现实 (VR/AR):</strong> 动态生成沉浸式VR/AR体验、交互式虚拟导览、虚拟培训模拟。</li>
<li><strong>内容创作:</strong> 自动化视频编辑、交互式故事叙述、教育内容生成。</li>
<li><strong>机器人学:</strong> 学习和执行自然语言指令来与环境交互，尤其是在模拟环境中进行训练。</li>
<li><strong>数字人与虚拟助手:</strong> 创造更具交互性和响应性的虚拟角色。</li>
<li><strong>电影与动画制作:</strong> 快速生成概念场景、模拟复杂动作。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性</strong></p>
<ul>
<li><strong>计算资源需求:</strong> 基于140亿参数的MoE模型，可以推断其训练和推理需要巨大的计算资源，这可能限制其在资源受限环境下的部署。</li>
<li><strong>对训练数据的依赖:</strong> 尽管论文提到了自动化处理，但模型的性能仍然高度依赖于训练数据的质量和规模，特别是因果对齐的交互式数据集。如果数据存在偏差或不足，模型可能在某些指令上表现不佳。</li>
<li><strong>“理解”的深度:</strong> 虽然模型能响应指令，但其对指令的“理解”程度可能仍是有限的。对于非常抽象、模糊或需要深层推理的指令，模型可能难以准确执行。摘要中给出的例子（“开门”、“拔出火把”、“触发爆炸”）相对具体，更复杂的指令可能面临挑战。</li>
<li><strong>因果关系的鲁棒性:</strong> 尽管论文强调了“因果关系”，但生成视频中的因果关系是否在所有情况下都绝对鲁棒和可信，仍需进一步验证。例如，是否会生成“开门”但门没有打开，或者“触发爆炸”但没有发生爆炸的错误。</li>
<li><strong>泛化能力:</strong> 模型在未见过的新颖游戏环境或指令组合上的泛化能力如何，摘要中并未详细说明，这通常是大型生成模型面临的挑战。</li>
<li><strong>交互的实时性:</strong> 对于需要实时交互的游戏应用，模型的推理速度是否足够快以满足实时性要求，这一点在摘要中没有明确提及。</li>
</ul>
<p>总而言之，Hunyuan-GameCraft-2在指令驱动的交互式视频生成领域取得了显著进展，其核心创新在于将自然语言指令转化为对游戏世界动态的精细控制，并为此构建了相应的数据集和评估框架。这为计算机视觉和多模态AI领域带来了令人兴奋的可能性，尤其是在游戏、VR/AR和内容创作等应用场景。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address these challenges, we introduce Hunyuan-GameCraft-2, a new paradigm of instruction-driven interaction for generative game world modeling.</li>
<li>We introduce an interaction-focused benchmark, InterBench, to evaluate interaction performance comprehensively.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.23429v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.23429v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.23428v1'></a></p>
<h2 id="dismo-disentangled-motion-representations-for-open-world-motion-transfer"><a href="https://arxiv.org/abs/2511.23428v1">DisMo: Disentangled Motion Representations for Open-World Motion Transfer</a></h2>
<p><strong>Authors:</strong> Thomas Ressler-Antal, Frank Fundel, Malek Ben Alaya, Stefan Andreas Baumann, Felix Krause, Ming Gui, Björn Ommer</p>
<p><strong>Published:</strong> 2025-11-28</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Recent advances in text-to-video (T2V) and image-to-video (I2V) models, have enabled the creation of visually compelling and dynamic videos from simple textual descriptions or initial frames. However, these models often fail to provide an explicit representation of motion separate from content, limiting their applicability for content creators. To address this gap, we propose DisMo, a novel paradigm for learning abstract motion representations directly from raw video data via an image-space reconstruction objective. Our representation is generic and independent of static information such as appearance, object identity, or pose. This enables open-world motion transfer, allowing motion to be transferred across semantically unrelated entities without requiring object correspondences, even between vastly different categories. Unlike prior methods, which trade off motion fidelity and prompt adherence, are overfitting to source structure or drifting from the described action, our approach disentangles motion semantics from appearance, enabling accurate transfer and faithful conditioning. Furthermore, our motion representation can be combined with any existing video generator via lightweight adapters, allowing us to effortlessly benefit from future advancements in video models. We demonstrate the effectiveness of our method through a diverse set of motion transfer tasks. Finally, we show that the learned representations are well-suited for downstream motion understanding tasks, consistently outperforming state-of-the-art video representation models such as V-JEPA in zero-shot action classification on benchmarks including Something-Something v2 and Jester. Project page: https://compvis.github.io/DisMo</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为一名计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行深入分析。</p>
<p><strong>论文摘要分析：DisMo: Disentangled Motion Representations for Open-World Motion Transfer</strong></p>
<p><strong>1. 论文的主要贡献 (2-3句话)</strong></p>
<p>该论文提出了DisMo，一种新颖的范式，旨在从原始视频数据中学习抽象的运动表示。其核心贡献在于能够将运动信息与内容（如外观、身份、姿态）完全解耦，从而实现“开放世界”的运动迁移，即在语义不相关的实体之间进行运动迁移，无需预先建立对应关系，甚至跨越不同类别。DisMo通过解耦运动语义与外观，解决了现有方法在运动保真度、提示遵循度、过拟合和漂移等方面的不足，并能与现有视频生成器无缝集成。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<ul>
<li><strong>图像空间重建目标 (Image-space Reconstruction Objective):</strong> 这是DisMo学习抽象运动表示的核心机制。通过在图像空间进行重建，模型被训练来捕捉视频中的动态变化，而忽略静态的视觉内容。</li>
<li><strong>运动与内容的解耦 (Disentangled Motion from Content):</strong> 这是DisMo最关键的创新点。它明确地将运动信息（如动作的发生、方向、速度）与静态的视觉特征（如物体的外观、身份、姿态）分离开来。这种解耦使得运动表示是通用的，不依赖于特定的物体或场景。</li>
<li><strong>开放世界运动迁移 (Open-World Motion Transfer):</strong> 基于运动与内容的解耦，DisMo能够实现跨越语义界限的运动迁移。这意味着可以将一个物体的运动模式应用到另一个完全不同的物体上，而无需事先知道它们之间的对应关系。例如，可以将一个人的跑步动作迁移到一个非生物物体上。</li>
<li><strong>轻量级适配器集成 (Lightweight Adapters for Integration):</strong> DisMo的运动表示可以通过轻量级适配器与任何现有的视频生成器（如T2V, I2V模型）结合。这使得研究人员和内容创作者能够轻松地利用DisMo的运动迁移能力，并受益于未来视频生成技术的进步。</li>
</ul>
<p><strong>3. 对该领域的潜在影响</strong></p>
<p>DisMo的提出可能对计算机视觉领域的视频生成、内容创作和运动理解产生深远影响：</p>
<ul>
<li><strong>提升视频生成的可控性与灵活性:</strong> 当前的视频生成模型虽然强大，但在精细控制运动方面仍有局限。DisMo提供的解耦运动表示将极大地增强用户对视频中动作的控制能力，实现更具创造性和个性化的视频内容。</li>
<li><strong>推动跨模态和跨领域的应用:</strong> 开放世界运动迁移的能力将为虚拟现实、增强现实、游戏开发、动画制作等领域带来新的可能性，例如，可以轻松地将现实世界的动作捕捉数据应用到虚拟角色上，或者将一种物体的运动风格迁移到另一种物体上。</li>
<li><strong>加速视频理解研究:</strong> DisMo学习到的通用运动表示，在零样本动作分类等下游任务中表现出色，表明其对运动语义的深刻理解。这有望推动视频理解模型在更广泛、更具挑战性的场景下的性能提升。</li>
<li><strong>降低内容创作门槛:</strong> 通过将复杂的运动迁移任务简化为使用解耦的运动表示，DisMo有望降低内容创作者的技术门槛，使更多人能够轻松地创作高质量的动态视频。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用</strong></p>
<ul>
<li><strong>视频生成与编辑:</strong> T2V, I2V模型，以及任何需要精细控制视频中动作的生成任务。</li>
<li><strong>内容创作:</strong> 电影、动画、广告、社交媒体视频的制作。</li>
<li><strong>虚拟现实 (VR) 和增强现实 (AR):</strong> 创建更逼真、更具交互性的虚拟环境和角色动画。</li>
<li><strong>游戏开发:</strong> 角色动画的生成和迁移，实现更丰富的游戏动作。</li>
<li><strong>机器人学:</strong> 运动规划和模仿学习，将人类的运动技能迁移到机器人上。</li>
<li><strong>人机交互:</strong> 设计更自然的交互方式，例如通过手势迁移来控制虚拟助手。</li>
<li><strong>体育分析:</strong> 动作识别、运动员表现分析和训练。</li>
<li><strong>医学影像:</strong> 分析和模拟生物体的运动。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性</strong></p>
<p>尽管摘要中强调了DisMo的优势，但仍可以推断出一些潜在的局限性：</p>
<ul>
<li><strong>对“原始视频数据”的依赖:</strong> 论文提到“直接从原始视频数据中学习”，这意味着模型的性能可能在很大程度上依赖于训练数据的质量和多样性。如果训练数据中缺乏某些类型的运动或场景，模型在处理这些情况时可能会遇到困难。</li>
<li><strong>“抽象运动表示”的解释性:</strong> 虽然表示是抽象的，但其具体的“语义”和“可解释性”可能仍是一个研究方向。如何精确地理解和控制这些抽象表示，使其符合人类的直观理解，可能需要进一步的研究。</li>
<li><strong>计算成本:</strong> 学习抽象表示通常需要大量的计算资源和时间。虽然摘要提到了“轻量级适配器”，但DisMo本身的训练过程可能仍然是计算密集型的。</li>
<li><strong>“开放世界”的边界:</strong> 尽管论文声称是“开放世界”运动迁移，但“开放世界”的定义和边界是模糊的。在某些极端情况下，例如迁移非常复杂、高度依赖特定物理属性的运动，或者在完全不相关的领域之间迁移，可能仍然会遇到挑战。</li>
<li><strong>对“提示遵循度”的权衡:</strong> 摘要提到“Unlike prior methods, which trade off motion fidelity and prompt adherence”，暗示DisMo在某些情况下可能仍然需要在运动保真度和对特定提示（例如文本描述）的遵循度之间进行权衡，尽管它声称做得更好。</li>
</ul>
<p>总而言之，DisMo是一项非常有前景的研究，它通过创新的解耦方法，为视频生成和理解领域带来了新的可能性，尤其是在实现通用、灵活的运动迁移方面。其对运动与内容的分离处理，是解决当前视频模型局限性的关键一步。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this gap, we propose DisMo, a novel paradigm for learning abstract motion representations directly from raw video data via an image-space reconstruction objective.</li>
<li>Unlike prior methods, which trade off motion fidelity and prompt adherence, are overfitting to source structure or drifting from the described action, our approach disentangles motion semantics from appearance, enabling accurate transfer and faithful conditioning.</li>
<li>We demonstrate the effectiveness of our method through a diverse set of motion transfer tasks.</li>
<li>Finally, we show that the learned representations are well-suited for downstream motion understanding tasks, consistently outperforming state-of-the-art video representation models such as V-JEPA in zero-shot action classification on benchmarks including Something-Something v2 and Jester.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.23428v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.23428v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.23386v1'></a></p>
<h2 id="vqrae-representation-quantization-autoencoders-for-multimodal-understanding-generation-and-reconstruction"><a href="https://arxiv.org/abs/2511.23386v1">VQRAE: Representation Quantization Autoencoders for Multimodal Understanding, Generation and Reconstruction</a></h2>
<p><strong>Authors:</strong> Sinan Du, Jiahao Guo, Bo Li, Shuhao Cui, Zhengzhuo Xu, Yifu Luo, Yongxian Wei, Kun Gai, Xinggang Wang, Kai Wu, Chun Yuan</p>
<p><strong>Published:</strong> 2025-11-28</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Unifying multimodal understanding, generation and reconstruction representation in a single tokenizer remains a key challenge in building unified models. Previous research predominantly attempts to address this in a dual encoder paradigm, e.g., utilizing the separate encoders for understanding and generation respectively or balancing semantic representations and low-level features with contrastive loss. In this paper, we propose VQRAE, a Vector Quantization version of Representation AutoEncoders, which pioneers the first exploration in unified representation to produce Continuous semantic features for image understanding and Discrete tokens for visual generation within a unified tokenizer. Specifically, we build upon pretrained vision foundation models with a symmetric ViT decoder and adopt a two-stage training strategy: first, it freezes the encoder and learns a high-dimensional semantic VQ codebook with pixel reconstruction objective; then jointly optimizes the encoder with self-distillation constraints. This design enables negligible semantic information for maintaining the ability of multimodal understanding, discrete tokens that are compatible for generation and fine-grained reconstruction. Besides, we identify the intriguing property in quantizing semantic encoders that rely on high-dimensional codebook in contrast to the previous common practice of low-dimensional codebook in image reconstruction. The semantic VQ codebook can achieve a 100% utilization ratio at a dimension of 1536. VQRAE presents competitive performance on several benchmarks of visual understanding, generation and reconstruction with promising scaling property in the autoregressive paradigm for its discrete merits.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“VQRAE: Representation Quantization Autoencoders for Multimodal Understanding, Generation and Reconstruction”的全面中文摘要：</p>
<p><strong>论文题目：</strong> VQRAE: 用于多模态理解、生成和重建的表示量化自编码器</p>
<p><strong>作者：</strong> Sinan Du, Jiahao Guo, Bo Li, Shuhao Cui, Zhengzhuo Xu, Yifu Luo, Yongxian Wei, Kun Gai, Xinggang Wang, Kai Wu, Chun Yuan</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
当前构建统一的多模态模型面临的关键挑战在于如何在一个单一的“tokenizer”（分词器）中统一表示理解、生成和重建任务。现有的方法通常采用双编码器范式，即为理解和生成任务分别设置独立的编码器，或者通过对比损失来平衡语义表示和低级特征。然而，这些方法往往增加了模型复杂度，阻碍了不同表示之间的深层交互，并且需要巨大的批次大小来平衡损失冲突。</p>
<p><strong>2. 主要创新点/方法贡献：</strong>
本文提出了VQRAE（Vector Quantization Representation AutoEncoders），一种创新的表示量化自编码器，旨在解决上述挑战。其核心贡献在于：
*   <strong>统一的Tokenizer：</strong> VQRAE 是首个能够在一个统一的 tokenizer 中同时产生用于图像理解的<strong>连续语义特征</strong>和用于视觉生成与重建的<strong>离散Tokens</strong>的模型。
*   <strong>两阶段训练策略：</strong>
    *   <strong>第一阶段：</strong> 冻结预训练的视觉基础模型（VFMs）编码器，学习一个高维度的语义 VQ 码本，并使用像素重建目标进行训练。
    *   <strong>第二阶段：</strong> 解冻 VFMs 编码器，并引入自蒸馏损失来增强语义理解能力，同时继续优化以实现精细的重建。
*   <strong>高维语义 VQ 码本：</strong> 论文强调了量化语义编码器时，采用高维度码本（例如 1536 维）的优势，这与以往在图像重建中常用低维度码本的做法形成对比。VQRAE 实现了码本的 100% 利用率。
*   <strong>无卷积结构：</strong> VQRAE 采用对称的 ViT 解码器，避免了卷积像素编码器的使用，简化了模型设计。</p>
<p><strong>3. 主要结果与意义：</strong>
VQRAE 在多个视觉理解、生成和重建的基准测试中取得了具有竞争力的性能。其主要优势在于：
*   <strong>性能优势：</strong> 在多模态理解任务上，VQRAE 显著优于其他统一 tokenizer 方法，并且在某些方面超越了双编码器方法。在重建任务上，VQRAE 也展现了出色的性能。
*   <strong>效率提升：</strong> VQRAE 简化了模型结构，降低了训练开销，并且无需为 tokenizer 进行额外的预训练或微调，即可直接集成到现有的 MLLMs 中。
*   <strong>可扩展性：</strong> VQRAE 的离散特性使其在自回归范式下具有良好的可扩展性，为构建更强大的统一多模态模型开辟了新途径。
*   <strong>码本利用率：</strong> 成功训练了一个高维度（1536）且利用率高达 100% 的语义 VQ 码本，这在以往的研究中是罕见的。</p>
<p><strong>4. 论文中提到的局限性：</strong>
*   <strong>理解与重建的权衡：</strong> VQRAE 在平衡理解和重建性能方面仍有改进空间，可能存在一定的折衷。
*   <strong>量化损失：</strong> 离散 tokenizer 的固有量化损失使其在与最先进的连续 VAEs 竞争时面临挑战。
*   <strong>生成质量：</strong> 在生成方面，尤其是在处理手指、人脸等细节时，仍然存在一些伪影，可能需要后处理或更深入的训练来解决。</p>
<p><strong>5. 未来研究方向：</strong>
*   <strong>更有效的权衡方法：</strong> 探索更有效的方法来平衡理解和重建性能，以最小化对理解能力的影响。
*   <strong>生成与理解的增强：</strong> 研究如何利用生成和重建能力来进一步增强模型的理解能力。
*   <strong>改进生成质量：</strong> 进一步提升生成图像的质量，特别是在空间关系、纹理渲染以及处理人脸和手指伪影方面。
*   <strong>集成与模型扩展：</strong> 探索如何将 VQRAE 产生的表示集成到更广泛的任务中，并研究高效的模型扩展策略。</p>
<p>总而言之，VQRAE 是一项重要的研究成果，它通过提出一种新颖的统一 tokenizer 架构，有效地解决了多模态理解、生成和重建任务之间的权衡问题，并为未来更强大、更高效的统一多模态模型奠定了基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this paper, we propose VQRAE, a Vector Quantization version of Representation AutoEncoders, which pioneers the first exploration in unified representation to produce Continuous semantic features for image understanding and Discrete tokens for visual generation within a unified tokenizer.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.23386v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.23386v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.23377v1'></a></p>
<h2 id="deal-300k-diffusion-based-editing-area-localization-with-a-300k-scale-dataset-and-frequency-prompted-baseline"><a href="https://arxiv.org/abs/2511.23377v1">DEAL-300K: Diffusion-based Editing Area Localization with a 300K-Scale Dataset and Frequency-Prompted Baseline</a></h2>
<p><strong>Authors:</strong> Rui Zhang, Hongxia Wang, Hangqing Liu, Yang Zhou, Qiang Zeng</p>
<p><strong>Published:</strong> 2025-11-28</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Diffusion-based image editing has made semantic level image manipulation easy for general users, but it also enables realistic local forgeries that are hard to localize. Existing benchmarks mainly focus on the binary detection of generated images or the localization of manually edited regions and do not reflect the properties of diffusion-based edits, which often blend smoothly into the original content. We present Diffusion-Based Image Editing Area Localization Dataset (DEAL-300K), a large scale dataset for diffusion-based image manipulation localization (DIML) with more than 300,000 annotated images. We build DEAL-300K by using a multi-modal large language model to generate editing instructions, a mask-free diffusion editor to produce manipulated images, and an active-learning change detection pipeline to obtain pixel-level annotations. On top of this dataset, we propose a localization framework that uses a frozen Visual Foundation Model (VFM) together with Multi Frequency Prompt Tuning (MFPT) to capture both semantic and frequency-domain cues of edited regions. Trained on DEAL-300K, our method reaches a pixel-level F1 score of 82.56% on our test split and 80.97% on the external CoCoGlide benchmark, providing strong baselines and a practical foundation for future DIML research.The dataset can be accessed via https://github.com/ymhzyj/DEAL-300K.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“DEAL-300K: Diffusion-based Editing Area Localization with a 300K-Scale Dataset and Frequency-Prompted Baseline”的全面中文摘要：</p>
<p><strong>论文题目：</strong> DEAL-300K: 基于扩散的编辑区域定位，包含一个300K规模的数据集和频率提示基线</p>
<p><strong>作者：</strong> Rui Zhang, Hongxia Wang, Hangqing Liu, Yang Zhou, Qiang Zeng</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 主要问题/研究问题：</strong>
随着扩散模型在图像编辑领域的广泛应用，用户可以轻松进行语义级别的图像编辑，但也带来了更逼真、更难检测的局部伪造图像。现有的基准数据集和方法主要关注二值化的生成图像检测或手动编辑区域的定位，未能充分反映扩散模型编辑的特点，即编辑内容往往能平滑地融入原始图像，缺乏明显的伪影。因此，如何准确地定位扩散模型生成的编辑区域（Diffusion-based Image Manipulation Localization, DIML）是一个亟待解决的问题。</p>
<p><strong>2. 关键创新/方法贡献：</strong>
*   <strong>DEAL-300K 数据集：</strong> 作者构建了一个大规模的、专门用于 DIML 任务的数据集，包含超过30万张标注图像。该数据集的生成过程具有创新性：
    *   <strong>多模态大语言模型（MLLM）驱动的指令生成：</strong> 利用微调后的 Qwen-VL 模型，根据图像内容自动生成高质量的编辑指令，确保指令的语义一致性。
    *   <strong>无掩码扩散模型进行图像编辑：</strong> 使用 InstructPix2Pix 等无掩码扩散模型生成编辑后的图像，更贴近实际的编辑流程，并避免了对掩码的依赖。
    *   <strong>主动学习与变化检测相结合的像素级标注：</strong> 采用 SAM-CD 模型进行变化检测，并结合主动学习策略，实现了高效且准确的像素级标注，大大减少了人工标注的成本。
*   <strong>多频段提示微调（MFPT）框架：</strong> 作者提出了一种新颖的定位框架，该框架：
    *   <strong>利用冻结的视觉基础模型（VFM）：</strong> 借鉴了大型视觉基础模型强大的先验知识，但通过参数高效的微调（PEFT）方式，避免了全参数微调的计算开销和灾难性遗忘。
    *   <strong>融合语义和频率域信息：</strong> 引入了“频率输入提示器”（FInP）和“特征频率提示器”（FFrP）两个核心组件。FInP 关注图像的低级纹理细节（高频信息），而 FFrP 则通过多头自注意力机制融合高频和低频信息，以捕捉扩散编辑中细微的语义和纹理异常。
    *   <strong>处理扩散编辑的独特性：</strong> 针对扩散模型编辑的特点，MFPT 框架能够捕捉到即使是平滑融入的编辑区域，也可能存在的频率域上的细微变化。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>数据集性能：</strong> DEAL-300K 数据集在作者提出的 MFPT 框架下，在内部测试集上达到了 82.56% 的像素级 F1 分数，在外部 CoCoGlide 基准测试集上达到了 80.97% 的像素级 F1 分数，显著优于现有方法，为 DIML 研究提供了强大的基线。
*   <strong>数据集价值：</strong> DEAL-300K 数据集规模庞大且多样化，涵盖了多种编辑场景，能够有效评估模型在不同扩散模型、不同编辑类型以及不同数据源上的泛化能力。
*   <strong>方法优势：</strong> MFPT 框架在各种评估场景下（包括单轮编辑、多轮编辑、真实图像检测等）均表现出优越的性能，并且在面对 JPEG 压缩和高斯模糊等图像退化时，展现出良好的鲁棒性。这表明该方法能够有效地区分扩散编辑的细微痕迹。
*   <strong>自动化标注的潜力：</strong> 作者提出的自动化标注流程，将人工标注时间从数千小时缩短到约42小时，为未来构建更大规模的数据集提供了可行方案。</p>
<p><strong>4. 提及的局限性：</strong>
*   <strong>细节精炼的不足：</strong> 在可视化结果中提到，虽然模型能够准确地勾勒出编辑区域，但对于非常细微的细节精炼方面仍有提升空间。
*   <strong>跨领域泛化挑战：</strong> 虽然 DEAL-300K 旨在提高模型的跨领域泛化能力，但作者也指出，在某些特定场景下（如人脸编辑），专门训练的模型（如 Patches）可能表现更好，表明不同领域的编辑特性仍存在差异。
*   <strong>预训练模型在不同数据集上的公平性：</strong> 作者在评估预训练模型时提到，直接比较不同数据集上训练的模型可能存在不公平性，因为数据集的领域和特点可能存在差异。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>视频篡改定位：</strong> 作者计划将 DEAL-300K 扩展到视频领域，进一步增强其在现实世界场景中的应用性。
*   <strong>更精细的细节定位：</strong> 进一步优化模型，以实现更精细的编辑区域定位，捕捉更微小的篡改痕迹。
*   <strong>更复杂的编辑场景：</strong> 探索和构建包含多轮、多类型编辑的更复杂数据集，以应对更具挑战性的篡改场景。
*   <strong>提升模型鲁棒性：</strong> 继续研究模型在各种图像退化和对抗性攻击下的鲁棒性。</p>
<p><strong>总结：</strong></p>
<p>这篇论文的核心贡献在于构建了一个大规模的、专门用于扩散模型编辑区域定位的 DEAL-300K 数据集，并提出了一种创新的多频段提示微调（MFPT）框架。DEAL-300K 数据集通过创新的自动化流程生成，解决了现有数据集规模小、标注成本高的问题。MFPT 框架则通过融合视觉基础模型的强大语义理解能力和频率域的低级纹理信息，有效地捕捉了扩散模型编辑的细微特征，在多个基准测试集上取得了最先进的性能，并展现出良好的泛化能力和鲁棒性。该研究为扩散模型图像篡改的检测和定位领域奠定了坚实的基础，并为未来的相关研究提供了宝贵的资源和方法。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present Diffusion-Based Image Editing Area Localization Dataset (DEAL-300K), a large scale dataset for diffusion-based image manipulation localization (DIML) with more than 300,000 annotated images.</li>
<li>On top of this dataset, we propose a localization framework that uses a frozen Visual Foundation Model (VFM) together with Multi Frequency Prompt Tuning (MFPT) to capture both semantic and frequency-domain cues of edited regions.</li>
<li>Trained on DEAL-300K, our method reaches a pixel-level F1 score of 82.56% on our test split and 80.97% on the external CoCoGlide benchmark, providing strong baselines and a practical foundation for future DIML research.The dataset can be accessed via https://github.com/ymhzyj/DEAL-300K.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.23377v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.23377v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.23375v1'></a></p>
<h2 id="optimizing-multimodal-language-models-through-attention-based-interpretability"><a href="https://arxiv.org/abs/2511.23375v1">Optimizing Multimodal Language Models through Attention-based Interpretability</a></h2>
<p><strong>Authors:</strong> Alexander Sergeev, Evgeny Kotelnikov</p>
<p><strong>Published:</strong> 2025-11-28</p>
<p><strong>Categories:</strong> cs.CL, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Modern large language models become multimodal, analyzing various data formats like text and images. While fine-tuning is effective for adapting these multimodal language models (MLMs) to downstream tasks, full fine-tuning is computationally expensive. Parameter-Efficient Fine-Tuning (PEFT) methods address this by training only a small portion of model weights. However, MLMs are difficult to interpret, making it challenging to identify which components are most effective for training to balance efficiency and performance. We propose an attention-based interpretability method for MLMs by analyzing attention scores relative to image tokens. The core idea is to identify attention heads that focus on image key objects. We utilize this information to select optimal model components for PEFT in multimodal models. Our contributions include a method for identifying attention heads associated with image key objects, its application to PEFT for image captioning, and the creation of a new dataset containing images, key object masks, and their textual descriptions. We conducted experiments on MLMs with 2-3 billion parameters to validate the method's effectiveness. By calculating Head Impact (HI) scores we quantify an attention head's focus on key objects, indicating its significance in image understanding. Our fine-tuning experiments demonstrate that adapting layers with the highest HI scores leads to the most significant shifts in metrics compared to pre-trained, randomly selected, or lowest-HI-score layers. This indicates that fine-tuning a small percentage (around 0.01%) of parameters in these crucial layers can substantially influence image understanding capabilities.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Optimizing Multimodal Language Models through Attention-based Interpretability”的全面中文摘要，重点关注其在计算机视觉领域的创新性和重要性：</p>
<p><strong>论文题目：</strong> 通过基于注意力的可解释性优化多模态语言模型</p>
<p><strong>作者：</strong> Alexander Sergeev, Evgeny Kotelnikov</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong></p>
<p>现代多模态语言模型（MLMs）能够同时处理文本和图像，在各种下游任务中表现出色。然而，对这些模型进行完全微调（full fine-tuning）计算成本高昂且效率低下。参数高效微调（PEFT）方法通过仅训练模型的一小部分权重来解决这个问题。但MLMs的“黑箱”特性使得理解哪些模型组件对提升效率和性能最有效变得困难，尤其是在平衡视觉和语言信息时。因此，研究的核心问题是如何<strong>有效地识别和利用MLMs中对图像理解至关重要的组件，以便进行高效的微调</strong>。</p>
<p><strong>2. 主要创新/方法贡献：</strong></p>
<p>该论文提出了一种<strong>基于注意力分数的可解释性方法</strong>，用于分析MLMs中注意力头（attention heads）对图像中关键对象（key objects）的关注程度。其核心创新点包括：</p>
<ul>
<li><strong>注意力头关键对象关注度量（Head Impact - HI）：</strong> 提出了一种量化方法，通过计算注意力分数与图像关键对象掩码（mask）的交并比（IoU）来衡量每个注意力头对图像关键对象的关注程度。HI分数越高，表示该注意力头越关注图像中的重要视觉元素。</li>
<li><strong>关键层识别与PEFT应用：</strong> 利用HI分数识别出对图像理解贡献最大的注意力头所在的层。然后，将这些高HI分数的层作为PEFT的优先选择对象，以实现更高效、更有效的模型适应。</li>
<li><strong>新数据集的创建：</strong> 构建了一个包含图像、关键对象掩码及其文本描述的新数据集，为研究和实验提供了基础。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong></p>
<p>通过在2-3十亿参数规模的MLMs上进行实验，研究取得了以下重要结果：</p>
<ul>
<li><strong>高HI分数层的重要性：</strong> 实验证明，对具有最高HI分数的层进行微调，能够带来比随机选择层或最低HI分数层更显著的模型性能提升。这表明这些层在图像理解中扮演着关键角色。</li>
<li><strong>PEFT的有效性：</strong> 仅微调约0.01%的参数（针对高HI分数的层）就能显著影响模型的图像理解能力，验证了所提出方法的有效性和PEFT的潜力。</li>
<li><strong>可解释性与效率的结合：</strong> 该方法成功地将模型的可解释性（识别关键组件）与模型优化（PEFT）相结合，为理解和改进MLMs提供了一条新途径。</li>
</ul>
<p><strong>4. 提及的局限性：</strong></p>
<ul>
<li><strong>模型架构限制：</strong> 实验主要集中在具有相似架构（Vision Transformer编码器和Transformer解码器）的模型上，并且图像被表示为嵌入到语言模型提示中的视觉令牌。</li>
<li><strong>模型规模限制：</strong> 由于计算资源的限制，实验仅限于2-3十亿参数规模的模型。</li>
<li><strong>任务类型限制：</strong> 实验主要集中在图像描述（Image Captioning）和封闭式视觉问答（Visual Question Answering）任务上，并未直接评估开放式文本生成任务。为了避免歧义，实验中使用了答案模板，这可能与真实开放式生成场景有所不同。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>评估开放式生成任务：</strong> 将所提出的微调方法应用于开放式生成任务，以更全面地评估其在真实世界场景中的表现。</li>
<li><strong>更广泛的模型和架构：</strong> 探索该方法在不同模型架构和更大规模模型上的适用性。</li>
<li><strong>更细粒度的分析：</strong> 进一步研究注意力头在不同层级和不同模型中的行为模式，以及它们如何协同工作以实现多模态理解。</li>
<li><strong>跨领域和跨任务的泛化性：</strong> 验证该方法在不同领域（如医学影像、自动驾驶）和不同多模态任务上的泛化能力。</li>
</ul>
<p><strong>总结：</strong></p>
<p>这篇论文为理解和优化多模态语言模型提供了一个新颖且实用的框架。通过<strong>基于注意力分数的可解释性方法，精确地识别出对图像理解至关重要的模型组件（即高HI分数的注意力头所在的层），并将其应用于参数高效微调（PEFT）</strong>，研究者们证明了这种方法能够以极低的计算成本实现显著的模型性能提升。这对于推动更高效、更易于理解和部署的多模态AI模型具有重要意义，尤其是在计算机视觉与自然语言处理交叉的领域。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose an attention-based interpretability method for MLMs by analyzing attention scores relative to image tokens.</li>
<li>Our contributions include a method for identifying attention heads associated with image key objects, its application to PEFT for image captioning, and the creation of a new dataset containing images, key object masks, and their textual descriptions.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.23375v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.23375v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.23369v1'></a></p>
<h2 id="simscale-learning-to-drive-via-real-world-simulation-at-scale"><a href="https://arxiv.org/abs/2511.23369v1">SimScale: Learning to Drive via Real-World Simulation at Scale</a></h2>
<p><strong>Authors:</strong> Haochen Tian, Tianyu Li, Haochen Liu, Jiazhi Yang, Yihang Qiu, Guang Li, Junli Wang, Yinfeng Gao, Zhang Zhang, Liang Wang, Hangjun Ye, Tieniu Tan, Long Chen, Hongyang Li</p>
<p><strong>Published:</strong> 2025-11-28</p>
<p><strong>Categories:</strong> cs.CV, cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>Achieving fully autonomous driving systems requires learning rational decisions in a wide span of scenarios, including safety-critical and out-of-distribution ones. However, such cases are underrepresented in real-world corpus collected by human experts. To complement for the lack of data diversity, we introduce a novel and scalable simulation framework capable of synthesizing massive unseen states upon existing driving logs. Our pipeline utilizes advanced neural rendering with a reactive environment to generate high-fidelity multi-view observations controlled by the perturbed ego trajectory. Furthermore, we develop a pseudo-expert trajectory generation mechanism for these newly simulated states to provide action supervision. Upon the synthesized data, we find that a simple co-training strategy on both real-world and simulated samples can lead to significant improvements in both robustness and generalization for various planning methods on challenging real-world benchmarks, up to +6.8 EPDMS on navhard and +2.9 on navtest. More importantly, such policy improvement scales smoothly by increasing simulation data only, even without extra real-world data streaming in. We further reveal several crucial findings of such a sim-real learning system, which we term SimScale, including the design of pseudo-experts and the scaling properties for different policy architectures. Our simulation data and code would be released.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“SimScale: Learning to Drive via Real-World Simulation at Scale”的全面中文摘要：</p>
<p><strong>论文题目：</strong> SimScale: Learning to Drive via Real-World Simulation at Scale</p>
<p><strong>作者：</strong> Haochen Tian, Tianyu Li, Haochen Liu, Jiazhi Yang, Yihang Qiu, Guang Li, Junli Wang, Yinfeng Gao, Zhang Zhang, Liang Wang, Hangjun Ye, Tieniu Tan, Long Chen, Hongyang Li</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
实现完全自动驾驶系统需要学习在各种场景下做出理性决策，包括安全关键和分布外（out-of-distribution, OOD）的场景。然而，这些场景在人类专家收集的真实世界数据集中代表性不足。仅依赖真实世界数据进行训练，自动驾驶模型在处理罕见或未见过的情况时会遇到泛化能力不足的问题。</p>
<p><strong>2. 主要创新与方法贡献：</strong>
为了解决数据多样性不足的问题，作者提出了一个名为 <strong>SimScale</strong> 的新颖且可扩展的<strong>模拟框架</strong>。其核心创新点包括：</p>
<ul>
<li><strong>大规模合成未见场景：</strong> 利用现有的驾驶日志，通过<strong>扰动</strong>（perturbation）的方式生成大量未见的OOD场景。</li>
<li><strong>高保真神经渲染：</strong> 采用先进的<strong>神经渲染</strong>技术（基于3DGS [39]）和<strong>反应式环境</strong>（reactive environment [70]），生成高保真的多视角观测，并使其他车辆能够响应式地与自主车辆互动，从而提高模拟的真实感和多样性。</li>
<li><strong>伪专家轨迹生成：</strong> 开发了一种<strong>伪专家轨迹生成机制</strong>，为新合成的模拟场景提供动作监督。作者比较了两种伪专家策略：<ul>
<li><strong>恢复式专家（Recovery-based Expert）：</strong> 旨在将轨迹引导回人类轨迹流形内，产生人类化但保守的行为。</li>
<li><strong>规划器式专家（Planner-based Expert）：</strong> 利用特权规划器（privileged planner）生成最优轨迹，代表一种探索性策略，具有较低的真实感但更强的多样性。</li>
</ul>
</li>
<li><strong>Sim-Real Co-training 策略：</strong> 提出了一种简单有效的<strong>Sim-Real Co-training</strong>（模拟-真实联合训练）策略，将真实世界数据与合成的模拟数据结合起来训练端到端规划器。该策略旨在保持人类驾驶分布的同时，减轻模拟数据可能带来的视觉域退化问题。</li>
<li><strong>可扩展的数据生成流程：</strong> SimScale框架能够通过逐步增加非重叠的模拟样本来扩展总训练数据量，同时保持真实世界数据的固定量，从而研究数据扩展的趋势。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong>
通过在两个具有挑战性的真实世界基准（navhard 和 navtest）上进行广泛实验，SimScale展现了显著的优势：</p>
<ul>
<li><strong>显著的性能提升：</strong> Sim-Real Co-training 策略能够为多种规划方法带来<strong>鲁棒性（robustness）和泛化能力（generalization）的协同提升</strong>。在navhard基准上，性能最高可提升 <strong>+6.8 EPDMS</strong>，在navtest上可提升 <strong>+2.9 EPDMS</strong>。</li>
<li><strong>可预测的数据扩展趋势：</strong> SimScale系统能够<strong>清晰且可预测地扩展</strong>，即使在真实世界数据量固定的情况下，增加模拟数据也能带来平滑的策略改进。</li>
<li><strong>伪专家设计的重要性：</strong> 实验表明，<strong>探索性更强的伪专家（如规划器式专家）比保守的恢复式专家更能带来持续的性能提升</strong>，尤其是在数据量增加时。</li>
<li><strong>多模态模型优势：</strong> 具有多模态建模能力（如DiffusionDrive）的规划器比单模态回归模型（如LTF）在数据扩展方面表现出<strong>更强的潜力</strong>。</li>
<li><strong>反应式模拟的价值：</strong> 反应式环境下的模拟数据比非反应式模拟更能<strong>提升真实感和多样性</strong>，从而带来更显著的性能增益。</li>
<li><strong>奖励信号的有效性：</strong> 对于评分式规划器，仅使用奖励信号进行训练（无伪专家轨迹）也能取得良好的效果，表明奖励信号在提供优化方向方面的重要性。</li>
</ul>
<p>SimScale的成果表明，通过大规模、高保真的模拟数据，可以有效弥补真实世界数据的不足，显著提升自动驾驶系统的鲁棒性和泛化能力，为实现更安全、更可靠的自动驾驶提供了新的途径。</p>
<p><strong>4. 论文中提到的局限性：</strong>
*   <strong>伪专家轨迹的静态性：</strong> 当前的伪专家轨迹扰动是静态的，作者提出未来可以探索<strong>自演化方法</strong>（self-evolving approach）来生成更具动态性和探索性的数据。
*   <strong>特权规划器的局限性：</strong> 文中使用的特权规划器是基于规则的，性能有限，可能导致舒适度指标（HC, EC）的下降，并且在极端情况下可能失效。更先进的<strong>基于学习的特权规划器</strong>可以改进生成效率和真实感。
*   <strong>反应式环境的简化：</strong> 交通行为模拟中的其他智能体由IDM [70] 控制，这限制了场景的多样性。作者建议可以采用<strong>扩散模型</strong>等方法来生成更丰富的交通行为。
*   <strong>传感器模拟的局限性：</strong> 尽管使用了3DGS，但模拟的视觉效果仍可能存在细微的<strong>不一致性</strong>，以及与真实世界在<strong>分布上的差异</strong>，这可能带来潜在风险。</p>
<p><strong>5. 未来研究方向：</strong>
*   <strong>自演化伪专家：</strong> 利用自演化方法生成更动态、更具探索性的伪专家轨迹。
*   <strong>先进的特权规划器：</strong> 采用基于学习的特权规划器来提高伪专家轨迹的真实感和多样性。
*   <strong>更丰富的交通行为模拟：</strong> 探索使用扩散模型等技术生成更具多样性的交通场景。
*   <strong>多模态传感器融合：</strong> 整合LiDAR等其他传感器模态以提高模拟的全面性。
*   <strong>在线强化学习与自玩：</strong> 结合在线RL和自玩技术，进一步提升模型的学习效率和泛化能力。
*   <strong>开放数据集与框架：</strong> 作者承诺开源其模拟数据集和训练框架，以促进学术界和工业界在自动驾驶模拟领域的研究。</p>
<p>总而言之，SimScale通过创新的模拟数据生成和Sim-Real Co-training策略，有效解决了自动驾驶领域数据稀疏性问题，为提升模型在复杂和OOD场景下的性能提供了强有力的解决方案，并为未来的研究开辟了新的方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To complement for the lack of data diversity, we introduce a novel and scalable simulation framework capable of synthesizing massive unseen states upon existing driving logs.</li>
<li>Furthermore, we develop a pseudo-expert trajectory generation mechanism for these newly simulated states to provide action supervision.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.23369v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.23369v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.23334v1'></a></p>
<h2 id="markovian-scale-prediction-a-new-era-of-visual-autoregressive-generation"><a href="https://arxiv.org/abs/2511.23334v1">Markovian Scale Prediction: A New Era of Visual Autoregressive Generation</a></h2>
<p><strong>Authors:</strong> Yu Zhang, Jingyi Liu, Yiwei Shi, Qi Zhang, Duoqian Miao, Changwei Wang, Longbing Cao</p>
<p><strong>Published:</strong> 2025-11-28</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Visual AutoRegressive modeling (VAR) based on next-scale prediction has revitalized autoregressive visual generation. Although its full-context dependency, i.e., modeling all previous scales for next-scale prediction, facilitates more stable and comprehensive representation learning by leveraging complete information flow, the resulting computational inefficiency and substantial overhead severely hinder VAR's practicality and scalability. This motivates us to develop a new VAR model with better performance and efficiency without full-context dependency. To address this, we reformulate VAR as a non-full-context Markov process, proposing Markov-VAR. It is achieved via Markovian Scale Prediction: we treat each scale as a Markov state and introduce a sliding window that compresses certain previous scales into a compact history vector to compensate for historical information loss owing to non-full-context dependency. Integrating the history vector with the Markov state yields a representative dynamic state that evolves under a Markov process. Extensive experiments demonstrate that Markov-VAR is extremely simple yet highly effective: Compared to VAR on ImageNet, Markov-VAR reduces FID by 10.5% (256 <script type="math/tex">\times</script> 256) and decreases peak memory consumption by 83.8% (1024 <script type="math/tex">\times</script> 1024). We believe that Markov-VAR can serve as a foundation for future research on visual autoregressive generation and other downstream tasks.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Markovian Scale Prediction: A New Era of Visual Autoregressive Generation”的全面摘要，重点关注其在计算机视觉和机器学习领域的贡献：</p>
<p><strong>论文题目：</strong> Markovian Scale Prediction: A New Era of Visual Autoregressive Generation</p>
<p><strong>作者：</strong> Yu Zhang, Jingyi Liu, Yiwei Shi, Qi Zhang, Duoqian Miao, Changwei Wang, Longbing Cao</p>
<p><strong>摘要：</strong></p>
<p>这篇论文提出了一种名为 <strong>Markov-VAR</strong> 的新型视觉自回归生成模型，旨在解决现有 Visual AutoRegressive (VAR) 模型在处理高分辨率图像生成时面临的计算效率低下和可扩展性差的问题。</p>
<p><strong>1. 研究问题/核心挑战：</strong></p>
<p>现有的 VAR 模型采用“全上下文依赖”的策略，即在预测当前尺度的图像特征时，会考虑所有之前的尺度信息。虽然这种方法有助于学习更稳定和全面的表示，但随着图像分辨率的增加，计算量呈平方级增长，导致训练和推理速度缓慢，严重限制了模型的实用性和可扩展性。此外，全上下文依赖还会导致跨尺度干扰，影响生成质量。</p>
<p><strong>2. 关键创新/方法贡献：</strong></p>
<ul>
<li><strong>将 VAR 重新定义为非全上下文马尔可夫过程：</strong> 作者将 VAR 的“下一个尺度预测”重新构想为“马尔可夫尺度预测”。他们将每个尺度视为一个马尔可夫状态，并假设当前状态的预测仅依赖于前一个状态，从而摆脱了对所有历史状态的依赖。</li>
<li><strong>提出马尔可夫尺度预测 (Markovian Scale Prediction)：</strong> 这是 Markov-VAR 的核心机制。它将每个尺度的预测视为一个马尔可夫链中的一个状态转移。</li>
<li><strong>引入历史补偿机制 (History Compensation Mechanism)：</strong> 为了弥补非全上下文依赖可能丢失的历史信息，作者设计了一个滑动窗口机制。该机制会压缩最近的几个尺度信息到一个紧凑的历史向量中，并将其与当前尺度的马尔可夫状态结合，形成一个代表性的动态状态，以增强预测能力。</li>
<li><strong>构建 Markov-VAR Transformer：</strong> 论文还提出了一个专门的 Transformer 架构来处理马尔可夫尺度预测和历史补偿。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong></p>
<ul>
<li><strong>显著提升效率：</strong> 在 ImageNet 数据集上，Markov-VAR 在 256x256 分辨率下将 FID 分数降低了 10.5%，在 1024x1024 分辨率下将峰值内存消耗降低了 83.8%。这表明 Markov-VAR 在保持生成质量的同时，大幅提高了计算效率和可扩展性。</li>
<li><strong>生成质量的提升：</strong> 实验结果表明，Markov-VAR 在生成质量上与 VAR 相当甚至更好，尤其是在某些情况下能产生更优的语义和更高的质量。</li>
<li><strong>参数效率：</strong> 即使在参数量相当的情况下，Markov-VAR 也展现出更优的性能，证明了其参数效率。</li>
<li><strong>基础模型潜力：</strong> 作者认为 Markov-VAR 可以作为未来视觉自回归生成和其他下游任务的基础模型。</li>
</ul>
<p><strong>4. 提及的局限性：</strong></p>
<ul>
<li>论文中提到，虽然马尔可夫尺度预测在效率上有所提升，但在某些情况下，与全上下文依赖相比，它可能无法完全保留所有历史信息，这可以通过历史补偿机制来缓解。</li>
<li>尽管论文展示了在 ImageNet 上的良好表现，但作者也指出，在更大、更高质量的数据集上训练，其生成质量可能会进一步提高。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>更广泛的应用：</strong> 作者希望 Markov-VAR 能作为基础模型，促进在各种下游任务中的研究。</li>
<li><strong>与其他模型的结合：</strong> 论文提到，Markov-VAR 的性能和效率在与其他增强或加速技术结合时可能更具前景。</li>
<li><strong>进一步优化历史补偿机制：</strong> 尽管滑动窗口机制有效，但可能还有更优化的方式来整合历史信息。</li>
</ul>
<p><strong>总结：</strong></p>
<p>Markov-VAR 论文的核心贡献在于成功地将视觉自回归生成从“全上下文依赖”的范式转变为更高效的“马尔可夫尺度预测”范式。通过引入历史补偿机制，它在显著降低计算成本和内存占用的同时，保持甚至提升了生成质量。这项工作为解决高分辨率视觉生成的可扩展性问题提供了一个有前景的解决方案，并有望成为未来相关研究的重要基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>This motivates us to develop a new VAR model with better performance and efficiency without full-context dependency.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.23334v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.23334v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-12-01 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
