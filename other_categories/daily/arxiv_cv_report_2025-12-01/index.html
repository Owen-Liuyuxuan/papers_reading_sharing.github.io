<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-12-01 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-28
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-12-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-03
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-11-28/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-12-02/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-12-01">Arxiv Computer Vision Papers - 2025-12-01</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#video-r2-reinforcing-consistent-and-grounded-reasoning-in-multimodal-language-models" class="nav-link">Video-R2: Reinforcing Consistent and Grounded Reasoning in Multimodal Language Models</a>
                </li>
                <li class="nav-item">
                    <a href="#video-com-interactive-video-reasoning-via-chain-of-manipulations" class="nav-link">Video-CoM: Interactive Video Reasoning via Chain of Manipulations</a>
                </li>
                <li class="nav-item">
                    <a href="#visual-generation-tuning" class="nav-link">Visual Generation Tuning</a>
                </li>
                <li class="nav-item">
                    <a href="#hunyuan-gamecraft-2-instruction-following-interactive-game-world-model" class="nav-link">Hunyuan-GameCraft-2: Instruction-following Interactive Game World Model</a>
                </li>
                <li class="nav-item">
                    <a href="#dismo-disentangled-motion-representations-for-open-world-motion-transfer" class="nav-link">DisMo: Disentangled Motion Representations for Open-World Motion Transfer</a>
                </li>
                <li class="nav-item">
                    <a href="#vqrae-representation-quantization-autoencoders-for-multimodal-understanding-generation-and-reconstruction" class="nav-link">VQRAE: Representation Quantization Autoencoders for Multimodal Understanding, Generation and Reconstruction</a>
                </li>
                <li class="nav-item">
                    <a href="#deal-300k-diffusion-based-editing-area-localization-with-a-300k-scale-dataset-and-frequency-prompted-baseline" class="nav-link">DEAL-300K: Diffusion-based Editing Area Localization with a 300K-Scale Dataset and Frequency-Prompted Baseline</a>
                </li>
                <li class="nav-item">
                    <a href="#optimizing-multimodal-language-models-through-attention-based-interpretability" class="nav-link">Optimizing Multimodal Language Models through Attention-based Interpretability</a>
                </li>
                <li class="nav-item">
                    <a href="#simscale-learning-to-drive-via-real-world-simulation-at-scale" class="nav-link">SimScale: Learning to Drive via Real-World Simulation at Scale</a>
                </li>
                <li class="nav-item">
                    <a href="#markovian-scale-prediction-a-new-era-of-visual-autoregressive-generation" class="nav-link">Markovian Scale Prediction: A New Era of Visual Autoregressive Generation</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-12-01">Arxiv Computer Vision Papers - 2025-12-01</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>好的，作为一名专注于计算机视觉和机器学习的研究助理，我将为您提供一份关于近期 Arxiv 计算机视觉领域论文的简明执行摘要。</p>
<hr />
<p><strong>执行摘要：2025年11月28日 Arxiv 计算机视觉论文精选</strong></p>
<p><strong>主要主题与趋势：</strong></p>
<p>本期 Arxiv 论文集主要聚焦于<strong>多模态理解与生成</strong>，特别是视频相关的任务，以及<strong>模型的可解释性与可控性</strong>。我们观察到以下几个关键趋势：</p>
<ul>
<li><strong>视频理解与推理的深化：</strong> 多篇论文致力于提升模型对视频内容的理解能力，包括细粒度的推理、交互式操作以及与语言模型的结合。</li>
<li><strong>生成模型的进步与控制：</strong> 在图像和视频生成领域，研究人员在提升生成质量、实现精细化编辑以及引入新的生成范式（如自回归生成）方面取得了进展。</li>
<li><strong>模型的可解释性与优化：</strong> 关注如何理解和优化多模态模型的内部工作机制，以提高其性能和可靠性。</li>
<li><strong>大规模数据与模拟的应用：</strong> 利用大规模数据集和逼真的模拟环境来训练更强大的模型，尤其是在游戏和自动驾驶等领域。</li>
</ul>
<p><strong>亮点与创新：</strong></p>
<ul>
<li><strong>Video-R2</strong> 和 <strong>Video-CoM</strong> 均在视频多模态理解方面展现了创新，前者强调一致性和接地气的推理，后者则聚焦于交互式视频推理。这表明视频理解正从静态分析走向动态、交互式的认知。</li>
<li><strong>Visual Generation Tuning</strong> 和 <strong>DEAL-300K</strong> 代表了在图像生成和编辑领域的显著进步。前者通过“视觉生成调优”来提升生成质量，后者则在扩散模型编辑方面取得了突破，并引入了大规模数据集。</li>
<li><strong>Hunyuan-GameCraft-2</strong> 和 <strong>SimScale</strong> 突显了在<strong>指令遵循</strong>和<strong>大规模模拟</strong>方面的应用价值，分别在游戏世界模型和自动驾驶领域展现了强大的潜力。</li>
<li><strong>Markovian Scale Prediction</strong> 提出了<strong>视觉自回归生成</strong>的新范式，预示着在生成模型领域可能出现新的发展方向。</li>
</ul>
<p><strong>新兴研究方向与技术：</strong></p>
<ul>
<li><strong>交互式视频推理：</strong> 模型不再仅仅是被动地理解视频，而是能够与视频内容进行交互，进行更深层次的推理。</li>
<li><strong>基于注意力机制的可解释性：</strong> 利用注意力机制来理解多模态模型决策过程，为模型优化提供指导。</li>
<li><strong>量化自编码器在多模态任务中的应用：</strong> <strong>VQRAE</strong> 探索了量化自编码器在多模态理解、生成和重建中的潜力。</li>
<li><strong>开放世界运动迁移：</strong> <strong>DisMo</strong> 提出的解耦运动表示为在不同场景下进行更灵活的运动迁移提供了可能。</li>
<li><strong>频率域的提示工程：</strong> <strong>DEAL-300K</strong> 中提出的“频率提示”为扩散模型编辑提供了新的思路。</li>
</ul>
<p><strong>建议阅读全文的论文：</strong></p>
<p>为了快速了解当前研究热点和潜在突破，建议重点阅读以下论文：</p>
<ol>
<li><strong>Video-R2: Reinforcing Consistent and Grounded Reasoning in Multimodal Language Models</strong> (深入理解视频多模态推理的最新进展)</li>
<li><strong>Video-CoM: Interactive Video Reasoning via Chain of Manipulations</strong> (探索视频交互式推理的创新方法)</li>
<li><strong>DEAL-300K: Diffusion-based Editing Area Localization with a 300K-Scale Dataset and Frequency-Prompted Baseline</strong> (了解扩散模型编辑的最新技术和大规模数据集的应用)</li>
<li><strong>Markovian Scale Prediction: A New Era of Visual Autoregressive Generation</strong> (关注视觉生成领域的新范式和潜在的未来方向)</li>
<li><strong>Optimizing Multimodal Language Models through Attention-based Interpretability</strong> (对于理解和优化多模态模型至关重要)</li>
</ol>
<hr />
<p>这份摘要旨在为忙碌的研究人员提供一个快速了解 Arxiv 计算机视觉领域最新动态的窗口。希望它能帮助您高效地把握该领域的最新发展。</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2511.23478v1">Video-R2: Reinforcing Consistent and Grounded Reasoning in Multimodal Language Models</a></li>
<li><a href="#2511.23477v1">Video-CoM: Interactive Video Reasoning via Chain of Manipulations</a></li>
<li><a href="#2511.23469v1">Visual Generation Tuning</a></li>
<li><a href="#2511.23429v1">Hunyuan-GameCraft-2: Instruction-following Interactive Game World Model</a></li>
<li><a href="#2511.23428v1">DisMo: Disentangled Motion Representations for Open-World Motion Transfer</a></li>
<li><a href="#2511.23386v1">VQRAE: Representation Quantization Autoencoders for Multimodal Understanding, Generation and Reconstruction</a></li>
<li><a href="#2511.23377v1">DEAL-300K: Diffusion-based Editing Area Localization with a 300K-Scale Dataset and Frequency-Prompted Baseline</a></li>
<li><a href="#2511.23375v1">Optimizing Multimodal Language Models through Attention-based Interpretability</a></li>
<li><a href="#2511.23369v1">SimScale: Learning to Drive via Real-World Simulation at Scale</a></li>
<li><a href="#2511.23334v1">Markovian Scale Prediction: A New Era of Visual Autoregressive Generation</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2511.23478v1'></a></p>
<h2 id="video-r2-reinforcing-consistent-and-grounded-reasoning-in-multimodal-language-models"><a href="https://arxiv.org/abs/2511.23478v1">Video-R2: Reinforcing Consistent and Grounded Reasoning in Multimodal Language Models</a></h2>
<p><strong>Authors:</strong> Muhammad Maaz, Hanoona Rasheed, Fahad Shahbaz Khan, Salman Khan</p>
<p><strong>Published:</strong> 2025-11-28</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Reasoning over dynamic visual content remains a central challenge for multimodal large language models. Recent thinking models generate explicit reasoning traces for interpretability; however, their reasoning often appears convincing while being logically inconsistent or weakly grounded in visual evidence. We identify and formalize these issues through two diagnostic metrics: Think Answer Consistency (TAC), which measures the alignment between reasoning and answers, and Video Attention Score (VAS), which captures the extent to which reasoning depends on visual versus textual cues. Analysis across 11 video reasoning benchmarks shows that current models rely heavily on linguistic priors rather than visual content. To address this, we propose a reinforcement learning approach that enhances both temporal precision and reasoning consistency. Our approach combines timestamp aware supervised fine tuning with Group Relative Policy Optimization (GRPO) guided by a novel Temporal Alignment Reward (TAR). This dual step post training stage encourages temporally aligned and causally coherent video reasoning. The resulting model, Video R2, achieves consistently higher TAC, VAS, and accuracy across multiple benchmarks, demonstrating that improvements in temporal alignment and reasoning coherence lead to more accurate and trustworthy video understanding. Our code, dataset, and model will be open sourced.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为一名计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行深入分析。</p>
<p><strong>论文摘要分析：Video-R2: Reinforcing Consistent and Grounded Reasoning in Multimodal Language Models</strong></p>
<p><strong>1. 论文的主要贡献（2-3句话）：</strong></p>
<p>这篇论文指出了当前多模态大语言模型在视频推理中存在逻辑不一致和视觉证据不足的问题。为了解决这些问题，作者提出了一个基于强化学习的方法，通过引入时间对齐奖励（TAR）来增强模型的时间精度和推理一致性，从而提升视频理解的准确性和可信度。</p>
<p><strong>2. 关键创新或方法论：</strong></p>
<ul>
<li><strong>诊断指标的提出与量化：</strong> 作者首先识别并形式化了当前模型在视频推理中的两个核心问题：<ul>
<li><strong>Think Answer Consistency (TAC)：</strong> 衡量推理过程与最终答案之间的一致性。</li>
<li><strong>Video Attention Score (VAS)：</strong> 衡量推理过程对视觉线索和文本线索的依赖程度。</li>
<li>通过这两个指标，作者揭示了现有模型过度依赖语言先验而非视觉内容的问题。</li>
</ul>
</li>
<li><strong>基于强化学习的改进方法：</strong> 核心创新在于其提出的后训练（post-training）阶段，该阶段结合了：<ul>
<li><strong>时间戳感知监督微调（timestamp aware supervised fine tuning）：</strong> 确保模型在学习过程中能够理解和利用视频的时间信息。</li>
<li><strong>Group Relative Policy Optimization (GRPO)：</strong> 一种强化学习策略优化方法，用于指导模型的学习。</li>
<li><strong>时间对齐奖励（Temporal Alignment Reward - TAR）：</strong> 这是最关键的新颖奖励函数，旨在鼓励模型在推理过程中生成与视频时间轴精确对齐且因果连贯的解释。</li>
</ul>
</li>
</ul>
<p><strong>3. 对该领域的潜在影响：</strong></p>
<ul>
<li><strong>提升多模态模型的可信度：</strong> 通过解决逻辑不一致和视觉接地不足的问题，Video-R2有望使多模态模型生成的推理过程更加可靠，减少“一本正经地胡说八道”的情况。</li>
<li><strong>推动视频理解的深入发展：</strong> 论文的发现表明，仅仅依赖文本信息不足以进行有效的视频推理。Video-R2的成功将激励研究者更加关注如何让模型真正“看懂”视频内容，并将其与语言推理相结合。</li>
<li><strong>为模型评估提供新视角：</strong> TAC和VAS这两个诊断指标为评估多模态模型在视频推理任务上的表现提供了更细致、更具洞察力的工具，有助于未来研究的进展。</li>
<li><strong>促进可解释性研究：</strong> 尽管摘要提到“recent thinking models generate explicit reasoning traces for interpretability”，但其推理过程往往不可靠。Video-R2通过提升推理的一致性和视觉接地，使得模型的可解释性更具价值。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用：</strong></p>
<ul>
<li><strong>视频问答（Video Question Answering - VQA）：</strong> 提高模型回答关于视频内容问题的准确性和推理过程的可靠性。</li>
<li><strong>视频摘要（Video Summarization）：</strong> 生成更具逻辑性和视觉依据的视频摘要。</li>
<li><strong>视频内容理解与检索：</strong> 提升模型对视频内容的深层理解能力，从而改进视频检索的精度。</li>
<li><strong>自动驾驶与机器人：</strong> 在需要理解动态环境和进行实时决策的应用中，模型能够更准确地推理视频信息。</li>
<li><strong>医疗影像分析：</strong> 对于需要理解时间序列变化的医学影像，该方法可能有助于生成更可靠的诊断推理。</li>
<li><strong>教育与培训：</strong> 用于生成教学视频的解释性内容，确保解释的准确性和与视频内容的对应。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性：</strong></p>
<ul>
<li><strong>计算成本：</strong> 强化学习方法，尤其是涉及策略优化和奖励设计的，通常计算成本较高，训练时间可能较长。</li>
<li><strong>泛化能力：</strong> 虽然在11个视频推理基准上取得了成功，但其在更广泛、更复杂或领域外的数据集上的泛化能力仍需验证。</li>
<li><strong>奖励函数的设计：</strong> TAR的有效性高度依赖于其设计是否能够全面捕捉“时间对齐”和“因果连贯”的精髓。如果奖励函数存在偏差，可能会导致模型学习到次优策略。</li>
<li><strong>对“视觉证据”的定义：</strong> 摘要中提到“weakly grounded in visual evidence”，但“grounded”的具体程度和标准可能需要进一步的定义和实验验证。模型是否真正理解了视觉内容，还是仅仅学会了在特定视觉模式下激活某些语言模式，仍需深入研究。</li>
<li><strong>模型规模与效率：</strong> 摘要未提及模型具体的规模和推理效率，这对于实际部署至关重要。</li>
</ul>
<p><strong>总结来说，这篇论文的亮点在于其对当前多模态视频推理模型核心痛点的精准诊断，以及通过创新的强化学习框架（特别是TAR奖励）来解决这些问题。它不仅提出了新的评估指标，更重要的是提供了一种能够显著提升模型推理一致性和视觉接地能力的方法，这对于构建更可靠、更具可信度的AI系统具有重要意义。</strong></p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this, we propose a reinforcement learning approach that enhances both temporal precision and reasoning consistency.</li>
<li>Our approach combines timestamp aware supervised fine tuning with Group Relative Policy Optimization (GRPO) guided by a novel Temporal Alignment Reward (TAR).</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.23478v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.23478v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.23477v1'></a></p>
<h2 id="video-com-interactive-video-reasoning-via-chain-of-manipulations"><a href="https://arxiv.org/abs/2511.23477v1">Video-CoM: Interactive Video Reasoning via Chain of Manipulations</a></h2>
<p><strong>Authors:</strong> Hanoona Rasheed, Mohammed Zumri, Muhammad Maaz, Ming-Hsuan Yang, Fahad Shahbaz Khan, Salman Khan</p>
<p><strong>Published:</strong> 2025-11-28</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Recent multimodal large language models (MLLMs) have advanced video understanding, yet most still "think about videos" ie once a video is encoded, reasoning unfolds entirely in text, treating visual input as a static context. This passive paradigm creates a semantic bottleneck: models cannot rewatch, refocus, or verify evidence, leading to shallow visual reasoning on tasks requiring fine grained spatio temporal understanding. In this work, we introduce Interactive Video Reasoning, a new paradigm that transforms video into an active cognitive workspace, enabling models to "think with videos". Our model, Video CoM, reasons through a Chain of Manipulations (CoM), performing iterative visual actions to gather and refine evidence. To support this behavior, we construct Video CoM Instruct, an 18K instruction tuning dataset curated for multi step manipulation reasoning. Beyond supervised learning, we further optimize the manipulation policy via reinforcement learning with reasoning aware Group Relative Policy Optimization (GRPO). Unlike prior work that relies solely on sparse answer rewards, our method introduces step level reasoning rewards, guiding the model toward grounded and consistent reasoning. Video CoM achieves strong results across nine video reasoning benchmarks, improving average performance by 3.6 percent over recent state of the art models, while training on only 25K SFT and 3K GRPO video samples, significantly fewer than comparable large scale models. Ablation studies demonstrate that reasoning aware rewards improve both accuracy and interpretability. Code: https://github.com/mbzuai-oryx/Video-CoM</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Video-CoM: Interactive Video Reasoning via Chain of Manipulations”的全面中文摘要：</p>
<p><strong>论文题目：</strong> Video-CoM: Interactive Video Reasoning via Chain of Manipulations</p>
<p><strong>作者：</strong> Hanoona Rasheed, Mohammed Zumri, Muhammad Maaz, Ming-Hsuan Yang, Fahad Shahbaz Khan, Salman Khan</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
当前的多模态大型语言模型（MLLMs）在视频理解方面取得了显著进展，但它们普遍采用“思考视频”（think about videos）的范式。这意味着模型在编码视频后，推理过程完全在文本空间进行，将视频视觉输入视为静态上下文。这种被动模式导致了语义瓶颈，模型无法重新观看、重新聚焦或验证证据，从而在需要精细时空理解的任务上表现出浅层视觉推理能力。</p>
<p><strong>2. 关键创新与方法贡献：</strong>
为了解决上述问题，本文提出了<strong>交互式视频推理（Interactive Video Reasoning）</strong>的新范式，将视频转化为一个主动的认知工作空间，使模型能够“用视频思考”（think with videos）。其核心创新包括：</p>
<ul>
<li><strong>链式操作（Chain of Manipulations, CoM）机制：</strong> Video-CoM 模型通过一系列迭代的视觉操作（原子操作包括 <code>find-segment</code>、<code>find-frame</code> 和 <code>spatial-zoom</code>）来主动与视频交互，以收集和精炼证据。这种操作序列形成了一个可解释的推理轨迹，每个步骤都以局部证据为基础。</li>
<li><strong>Video-CoM-Instruct 数据集：</strong> 构建了一个包含 18K 样本的指令微调数据集，专门用于多步操作推理。该数据集精心设计，旨在引导模型使用操作来解决问题，并暴露了多样化的推理轨迹。</li>
<li><strong>推理感知组相对策略优化（Reasoning-Aware Group Relative Policy Optimization, RA-GRPO）：</strong> 引入了一种新的强化学习目标，通过<strong>步级推理奖励（step-level reasoning rewards）</strong>来优化操作策略。与仅依赖稀疏答案奖励的传统方法不同，RA-GRPO 评估中间操作的正确性，从而引导模型进行更具根源性和一致性的推理。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong>
Video-CoM 在九个视频推理基准测试中取得了显著的成果，平均性能比最近的 SOTA 模型提高了 3.6%。尤为重要的是，该模型仅使用了 25K SFT 和 3K GRPO 视频样本进行训练，远少于同类大型模型。消融研究表明，推理感知奖励不仅提高了模型的准确性，还增强了其可解释性。这证明了交互式视频推理范式和 RA-GRPO 目标在提升模型理解和推理视频内容方面的有效性。</p>
<p><strong>4. 提及的局限性：</strong>
论文中提到了以下局限性：</p>
<ul>
<li><strong>视频中的空间定位：</strong> 在视频中准确地进行空间定位仍然是一个挑战，尤其是在需要识别文本或数字等精细细节时。这需要大规模、高质量的标注数据，而这类数据目前相对稀缺。</li>
<li><strong>视频源的局限性：</strong> 构建针对操作推理的数据集依赖于具有丰富时空变化的视频内容。具有有限场景多样性的视频（如单视角烹饪视频）可能难以生成需要迭代视觉交互的问题。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong>
虽然论文没有明确列出未来研究方向，但基于其提出的方法和发现，可以推断出以下潜在方向：</p>
<ul>
<li><strong>提升空间定位的鲁棒性：</strong> 进一步研究更有效的空间定位技术，以应对视频中复杂和精细的空间信息。</li>
<li><strong>扩大数据集规模和多样性：</strong> 收集更多具有丰富时空变化和操作相关性的视频，以构建更大、更多样化的数据集，支持更广泛的交互式推理任务。</li>
<li><strong>探索更复杂的推理轨迹：</strong> 研究更复杂的链式操作组合，以解决更具挑战性的多模态推理问题。</li>
<li><strong>模型效率的进一步优化：</strong> 尽管 Video-CoM 在效率上表现良好，但进一步优化模型以实现更快的推理速度和更低的计算成本仍然是重要的研究方向。</li>
<li><strong>跨模态交互的深化：</strong> 将交互式推理范式扩展到更多模态（如音频、文本），实现更全面的多模态理解。</li>
</ul>
<p><strong>总结：</strong>
“Video-CoM: Interactive Video Reasoning via Chain of Manipulations” 论文提出了一种创新的交互式视频推理范式，通过链式操作和推理感知强化学习，显著提升了 MLLMs 在视频理解任务中的精细时空推理能力。该方法克服了传统被动视频推理的局限性，实现了更具根源性、更可解释的推理过程，并在多个基准测试中取得了优异的成绩，同时显著减少了训练数据需求。该研究为未来的视频理解和多模态推理模型提供了新的思路和方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this work, we introduce Interactive Video Reasoning, a new paradigm that transforms video into an active cognitive workspace, enabling models to "think with videos".</li>
<li>Unlike prior work that relies solely on sparse answer rewards, our method introduces step level reasoning rewards, guiding the model toward grounded and consistent reasoning.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.23477v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.23477v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.23469v1'></a></p>
<h2 id="visual-generation-tuning"><a href="https://arxiv.org/abs/2511.23469v1">Visual Generation Tuning</a></h2>
<p><strong>Authors:</strong> Jiahao Guo, Sinan Du, Jingfeng Yao, Wenyu Liu, Bo Li, Haoxiang Cao, Kun Gai, Chun Yuan, Kai Wu, Xinggang Wang</p>
<p><strong>Published:</strong> 2025-11-28</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Large Vision Language Models (VLMs) effectively bridge the modality gap through extensive pretraining, acquiring sophisticated visual representations aligned with language. However, it remains underexplored whether these representations, optimized for multimodal understanding tasks, harbor an inherent potential for visual generation. In this paper, we propose VGT, Visual Generation Tuning, a novel paradigm designed to stimulate the underlying capabilities of visual generation within any vision language models. By performing efficient visual generation tuning on well-pretrained VLMs, we significantly mitigate the alignment costs and accelerate the convergence of autoregressive modeling in the continuous space (20x speedup). Specifically, we dismiss the entangled pixel-level VAEs designed for diffusion transformers and formulate VGT-AE through aligning the semantic encoders from pretrained VLMs with the latent representations of pixel decoders. In image reconstruction tasks, we achieve 26.67 PSNR and 0.50 rFID at a 28x compression ratio, outperforming specialized VAEs; in visual generation tasks, we achieve state-of-the-art outcomes among autoregressive models, 0.77 on GenEval and 78.73 on DPG-Bench. Furthermore, our proposed VGT showcases significant scaling promise and is versatile for endowing any VLMs trained for multimodal understanding with the capabilities of visual generation, which paves the new avenue to explore next-generation unified multimodal foundation models. Models and codes are available at https://github.com/hustvl/VGT.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Visual Generation Tuning”的全面摘要：</p>
<p><strong>论文题目：</strong> Visual Generation Tuning</p>
<p><strong>作者：</strong> Jiahao Guo, Sinan Du, Jingfeng Yao, Wenyu Liu, Bo Li, Haoxiang Cao, Kun Gai, Chun Yuan, Kai Wu, Xinggang Wang</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
该论文的核心研究问题在于，大型视觉语言模型（VLMs）在多模态理解任务上表现出色，其预训练获得的视觉表征是否蕴含了生成图像的潜力？现有方法通常依赖于为扩散模型设计的像素级 VAEs，这些 VAEs 与自回归建模在连续空间中的对齐存在固有的不匹配问题，导致训练不稳定且效率低下。</p>
<p><strong>2. 主要创新与方法贡献：</strong>
作者提出了 <strong>VGT (Visual Generation Tuning)</strong> 这一新范式，旨在激发预训练 VLM 中固有的视觉生成能力。其核心创新点包括：</p>
<ul>
<li><strong>VGT-AE（Visual Generation Tuning-AutoEncoder）：</strong> 这是一个关键的组件，它通过将预训练 VLM 的语义编码器与轻量级像素解码器的潜在表征对齐来构建。这解决了传统 VAEs 与自回归建模之间在语义结构和潜在空间上的不匹配问题。VGT-AE 采用两阶段训练策略：<ul>
<li><strong>第一阶段：语义保持重建。</strong> 通过结合像素重建损失和语义自蒸馏损失，确保重建的高保真度，同时将语义结构注入到紧凑的潜在空间中。</li>
<li><strong>第二阶段：潜在空间正则化。</strong> 通过冻结编码器，优化解码器和投影模块，并引入通道归一化和高斯噪声注入，使潜在分布更符合标准高斯先验，从而提高其对自回归学习的适应性。</li>
</ul>
</li>
<li><strong>QueryAR（Query Autoregressive）：</strong> 针对自回归生成，作者提出了 QueryAR，它利用位置查询机制来保持自回归的公式化，同时允许部分并行解码，从而提高推理效率。</li>
<li><strong>高效的视觉生成调优：</strong> VGT 通过高效的视觉生成调优，显著降低了对齐成本，并加速了连续空间自回归建模的收敛速度（最高可达 20 倍加速）。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong>
*   <strong>重建性能：</strong> VGT-AE 在图像重建任务上取得了优异的性能，在 28 倍压缩比下达到了 26.67 PSNR 和 0.50 rFID，优于专门的 VAEs。
*   <strong>生成性能：</strong> 在视觉生成任务上，VGT 达到了自回归模型中的最先进水平，在 GenEval 上获得 0.77 分，在 DPG-Bench 上获得 78.73 分。
*   <strong>数据效率：</strong> VGT 在训练数据量有限的情况下（仅 25M 样本）取得了卓越的性能，显著优于需要海量数据训练的传统自回归模型。
*   <strong>通用性与可扩展性：</strong> VGT 具有很强的可扩展性，能够赋能任何为多模态理解训练的 VLM，使其具备视觉生成能力，为探索下一代统一多模态基础模型开辟了新途径。
*   <strong>挑战传统观念：</strong> 该研究挑战了自回归模型在同等规模下通常生成质量不如扩散模型的观点，证明了通过有效的表征学习和对齐，自回归模型也能达到甚至超越扩散模型的性能。</p>
<p><strong>4. 提及的局限性：</strong>
*   论文中提到，在 <strong>重建与生成之间的权衡</strong> 是一个关键的考虑因素。高度优化的重建模型可能在生成性能上有所牺牲，反之亦然。VGT 通过其两阶段训练策略试图平衡这一点，但仍然存在这种内在的权衡。
*   虽然 VGT 在数据效率上表现出色，但论文也暗示了 <strong>模型规模</strong> 对最终性能的影响（例如，0.6B 和 1.6B 参数的模型对比）。</p>
<p><strong>5. 未来研究方向：</strong>
*   <strong>统一多模态基础模型：</strong> VGT 为构建能够无缝融合感知和生成能力的新一代统一多模态基础模型奠定了基础。
*   <strong>扩展 VGT 框架：</strong> 未来工作可以进一步探索 VGT 框架在更多模态和更复杂任务上的应用，以及与其他多模态发展方向的结合。
*   <strong>更精细的潜在空间控制：</strong> 尽管 VGT 取得了显著进展，但对潜在空间的进一步精细控制，以实现更可控和多样化的生成，仍是潜在的研究方向。</p>
<p><strong>总结：</strong>
“Visual Generation Tuning” 论文提出了一种创新的范式 VGT，成功地将预训练视觉语言模型（VLMs）的强大视觉理解能力转化为高效的视觉生成能力。通过 VGT-AE 和 QueryAR 等关键组件，该方法有效解决了传统方法在连续空间自回归建模中的对齐和效率问题，并在重建和生成任务上均取得了最先进的性能，同时展现了显著的数据效率和可扩展性。这项工作为开发更强大、更通用的统一多模态基础模型开辟了新的道路。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this paper, we propose VGT, Visual Generation Tuning, a novel paradigm designed to stimulate the underlying capabilities of visual generation within any vision language models.</li>
<li>In image reconstruction tasks, we achieve 26.67 PSNR and 0.50 rFID at a 28x compression ratio, outperforming specialized VAEs; in visual generation tasks, we achieve state-of-the-art outcomes among autoregressive models, 0.77 on GenEval and 78.73 on DPG-Bench.</li>
<li>Furthermore, our proposed VGT showcases significant scaling promise and is versatile for endowing any VLMs trained for multimodal understanding with the capabilities of visual generation, which paves the new avenue to explore next-generation unified multimodal foundation models.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.23469v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.23469v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.23429v1'></a></p>
<h2 id="hunyuan-gamecraft-2-instruction-following-interactive-game-world-model"><a href="https://arxiv.org/abs/2511.23429v1">Hunyuan-GameCraft-2: Instruction-following Interactive Game World Model</a></h2>
<p><strong>Authors:</strong> Junshu Tang, Jiacheng Liu, Jiaqi Li, Longhuang Wu, Haoyu Yang, Penghao Zhao, Siruis Gong, Xiang Yuan, Shuai Shao, Qinglin Lu</p>
<p><strong>Published:</strong> 2025-11-28</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Recent advances in generative world models have enabled remarkable progress in creating open-ended game environments, evolving from static scene synthesis toward dynamic, interactive simulation. However, current approaches remain limited by rigid action schemas and high annotation costs, restricting their ability to model diverse in-game interactions and player-driven dynamics. To address these challenges, we introduce Hunyuan-GameCraft-2, a new paradigm of instruction-driven interaction for generative game world modeling. Instead of relying on fixed keyboard inputs, our model allows users to control game video contents through natural language prompts, keyboard, or mouse signals, enabling flexible and semantically rich interaction within generated worlds. We formally defined the concept of interactive video data and developed an automated process to transform large-scale, unstructured text-video pairs into causally aligned interactive datasets. Built upon a 14B image-to-video Mixture-of-Experts(MoE) foundation model, our model incorporates a text-driven interaction injection mechanism for fine-grained control over camera motion, character behavior, and environment dynamics. We introduce an interaction-focused benchmark, InterBench, to evaluate interaction performance comprehensively. Extensive experiments demonstrate that our model generates temporally coherent and causally grounded interactive game videos that faithfully respond to diverse and free-form user instructions such as "open the door", "draw a torch", or "trigger an explosion".</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为一名计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行深入分析。</p>
<p><strong>论文摘要分析：Hunyuan-GameCraft-2</strong></p>
<p><strong>1. 论文的主要贡献（2-3句话）：</strong></p>
<p>Hunyuan-GameCraft-2 提出了一种新颖的指令驱动交互范式，用于生成动态、可交互的游戏世界模型。该模型能够通过自然语言指令、键盘或鼠标信号，实现对游戏视频内容进行灵活且语义丰富的控制，克服了现有方法在动作模式僵化和标注成本高昂方面的限制。其核心在于构建了大规模、因果对齐的交互式视频数据集，并利用一个14B参数的MoE基础模型实现了对相机运动、角色行为和环境动态的精细化控制。</p>
<p><strong>2. 关键创新或方法论：</strong></p>
<ul>
<li><strong>指令驱动的交互范式：</strong> 这是最核心的创新。不同于传统的固定动作模式（如键盘按键），Hunyuan-GameCraft-2 允许用户使用自然语言（如“打开门”、“拔出火把”、“触发爆炸”）来控制游戏世界的动态。这种方式极大地提升了交互的灵活性和语义的丰富性。</li>
<li><strong>交互式视频数据的定义与自动化处理：</strong> 论文正式定义了“交互式视频数据”的概念，并开发了一种自动化流程，将大规模、非结构化的文本-视频对转化为因果对齐的交互式数据集。这解决了构建高质量交互式数据集的难题，为模型训练提供了坚实基础。</li>
<li><strong>基于14B MoE基础模型的架构：</strong> 模型构建在一个强大的140亿参数的图像到视频MoE（Mixture-of-Experts）基础模型之上。MoE架构通常在处理大规模数据和复杂任务时表现出色，能够有效地学习和泛化。</li>
<li><strong>文本驱动的交互注入机制：</strong> 该机制是实现指令控制的关键。它能够将文本指令精确地映射到对相机运动、角色行为和环境动态的精细化控制上，确保模型能够忠实地响应用户的意图。</li>
<li><strong>InterBench 交互式基准测试：</strong> 论文引入了一个专门用于评估交互性能的基准测试集。这为衡量和比较不同交互式游戏世界模型的性能提供了一个标准化的平台。</li>
</ul>
<p><strong>3. 对该领域的潜在影响：</strong></p>
<ul>
<li><strong>推动生成式世界模型的进步：</strong> Hunyuan-GameCraft-2 显著提升了生成式世界模型的交互性和可控性，使其能够模拟更复杂、更贴近真实玩家体验的游戏场景。</li>
<li><strong>降低内容创作门槛：</strong> 通过自然语言指令控制游戏世界，极大地降低了游戏内容创作和测试的门槛，使得非专业人士也能更便捷地参与到游戏世界的构建和探索中。</li>
<li><strong>促进人机交互研究：</strong> 该研究为研究更自然、更直观的人机交互方式提供了新的思路和平台，尤其是在虚拟环境和游戏领域。</li>
<li><strong>为多模态AI提供新范例：</strong> 将自然语言指令与动态视频生成相结合，为多模态AI的研究提供了新的应用场景和技术路径，尤其是在理解和生成复杂时序性内容方面。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用：</strong></p>
<ul>
<li><strong>游戏开发与测试：</strong> 自动化生成和测试游戏场景，快速验证游戏机制和玩家体验。</li>
<li><strong>虚拟现实 (VR) 和增强现实 (AR)：</strong> 创建更具沉浸感和交互性的虚拟环境，用户可以通过自然语言与虚拟世界进行互动。</li>
<li><strong>教育与培训：</strong> 构建交互式模拟环境，用于技能培训、历史场景重现或科学实验模拟。</li>
<li><strong>内容生成：</strong> 自动生成电影、动画或短视频中的动态场景，并允许用户通过指令进行修改。</li>
<li><strong>机器人控制与仿真：</strong> 将自然语言指令转化为机器人或虚拟代理在复杂环境中的行为。</li>
<li><strong>AI助手与虚拟角色：</strong> 创造能够理解并响应用户指令的更智能的虚拟角色。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性：</strong></p>
<ul>
<li><strong>计算资源需求：</strong> 基于14B参数的MoE模型，其训练和推理的计算资源需求可能非常高。</li>
<li><strong>数据集的覆盖范围和偏差：</strong> 尽管论文提到了大规模数据集，但其覆盖的游戏类型、交互模式以及潜在的偏差（如文化、语言上的）仍是潜在的限制因素。</li>
<li><strong>指令理解的鲁棒性：</strong> 虽然摘要提到模型能响应“自由形式”的指令，但对于非常复杂、模糊或矛盾的指令，其理解和执行的鲁棒性仍需进一步验证。</li>
<li><strong>生成视频的真实感和细节：</strong> 尽管摘要强调了“时间连贯”和“因果接地”，但生成视频的视觉真实感、细节丰富度以及是否会出现不自然的伪影等问题，通常是这类生成模型需要面对的挑战。</li>
<li><strong>“因果对齐”的定义与实现：</strong> 摘要中提到的“因果对齐”是一个关键概念，但其具体的定义、实现方法以及在多大程度上真正实现了因果关系建模，需要通过论文的详细内容来评估。</li>
<li><strong>交互的实时性：</strong> 对于需要实时交互的游戏场景，模型的推理速度和延迟是至关重要的，摘要中并未明确提及。</li>
</ul>
<p><strong>总结：</strong></p>
<p>Hunyuan-GameCraft-2 在计算机视觉领域具有显著的趣味性和重要性，因为它成功地将自然语言指令的灵活性引入了动态、可交互的游戏世界模型生成。这不仅是技术上的一个飞跃，也为未来更智能、更具交互性的AI应用打开了新的大门。其核心创新在于定义和构建了因果对齐的交互式视频数据集，并利用强大的MoE基础模型实现了精细化的指令控制。尽管存在潜在的计算资源和鲁棒性挑战，但该研究无疑是生成式AI和人机交互领域的一个重要进展。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address these challenges, we introduce Hunyuan-GameCraft-2, a new paradigm of instruction-driven interaction for generative game world modeling.</li>
<li>We introduce an interaction-focused benchmark, InterBench, to evaluate interaction performance comprehensively.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.23429v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.23429v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.23428v1'></a></p>
<h2 id="dismo-disentangled-motion-representations-for-open-world-motion-transfer"><a href="https://arxiv.org/abs/2511.23428v1">DisMo: Disentangled Motion Representations for Open-World Motion Transfer</a></h2>
<p><strong>Authors:</strong> Thomas Ressler-Antal, Frank Fundel, Malek Ben Alaya, Stefan Andreas Baumann, Felix Krause, Ming Gui, Björn Ommer</p>
<p><strong>Published:</strong> 2025-11-28</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Recent advances in text-to-video (T2V) and image-to-video (I2V) models, have enabled the creation of visually compelling and dynamic videos from simple textual descriptions or initial frames. However, these models often fail to provide an explicit representation of motion separate from content, limiting their applicability for content creators. To address this gap, we propose DisMo, a novel paradigm for learning abstract motion representations directly from raw video data via an image-space reconstruction objective. Our representation is generic and independent of static information such as appearance, object identity, or pose. This enables open-world motion transfer, allowing motion to be transferred across semantically unrelated entities without requiring object correspondences, even between vastly different categories. Unlike prior methods, which trade off motion fidelity and prompt adherence, are overfitting to source structure or drifting from the described action, our approach disentangles motion semantics from appearance, enabling accurate transfer and faithful conditioning. Furthermore, our motion representation can be combined with any existing video generator via lightweight adapters, allowing us to effortlessly benefit from future advancements in video models. We demonstrate the effectiveness of our method through a diverse set of motion transfer tasks. Finally, we show that the learned representations are well-suited for downstream motion understanding tasks, consistently outperforming state-of-the-art video representation models such as V-JEPA in zero-shot action classification on benchmarks including Something-Something v2 and Jester. Project page: https://compvis.github.io/DisMo</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为一名计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行深入分析。</p>
<p><strong>论文摘要分析：DisMo: Disentangled Motion Representations for Open-World Motion Transfer</strong></p>
<p><strong>1. 论文的主要贡献 (2-3句话)</strong></p>
<p>该论文提出了DisMo，一种新颖的框架，能够从原始视频数据中学习抽象的运动表示，并将其与内容（如外观、身份、姿态）解耦。这种解耦使得DisMo能够实现“开放世界”的运动迁移，即在语义不相关的实体之间进行运动迁移，无需预先建立对应关系，甚至跨越不同类别。DisMo通过解耦运动语义与外观，解决了现有方法在运动保真度、提示遵循度、过拟合和动作漂移等方面的挑战，并能与现有视频生成模型轻松集成。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<p>DisMo的核心创新在于其<strong>学习抽象运动表示的能力，并将其与内容信息进行有效解耦</strong>。具体方法论体现在：</p>
<ul>
<li><strong>图像空间重建目标 (Image-space reconstruction objective):</strong> 这是DisMo学习运动表示的关键。通过在图像空间进行重建，模型被鼓励去理解和捕捉视频中的动态信息，而无需直接依赖于显式的姿态估计或其他结构化信息。</li>
<li><strong>运动与内容的解耦 (Disentanglement of motion from content):</strong> 这是DisMo最突出的特点。通过这种解耦，运动表示变得通用且独立于静态信息（外观、身份、姿态）。这意味着学习到的运动可以被应用于任何内容，而内容也可以被赋予任何学习到的运动。</li>
<li><strong>开放世界运动迁移 (Open-world motion transfer):</strong> 这是解耦带来的直接应用。它允许在没有预定义对应关系的情况下，将运动从一个视频迁移到另一个视频，即使目标视频中的实体与源视频中的实体在类别上差异很大。这打破了传统运动迁移方法对特定对象或姿态的依赖。</li>
<li><strong>轻量级适配器集成 (Lightweight adapters):</strong> DisMo的运动表示可以与任何现有的视频生成器通过轻量级适配器进行集成。这极大地增强了其灵活性和可扩展性，使其能够利用未来视频生成技术的进步。</li>
</ul>
<p><strong>3. 对该领域的潜在影响</strong></p>
<p>DisMo的研究对计算机视觉领域具有重要的潜在影响，主要体现在：</p>
<ul>
<li><strong>推动更通用的视频内容创作:</strong> 通过实现开放世界的运动迁移，DisMo为视频内容创作者提供了前所未有的灵活性。用户可以更自由地控制视频的动态，将特定的动作或运动风格应用到各种场景和角色上，极大地降低了视频制作的门槛和成本。</li>
<li><strong>提升视频理解和生成模型的泛化能力:</strong> DisMo的学习范式强调了运动的抽象表示，这有助于构建更具泛化能力的视频理解模型。同时，其与现有视频生成模型的集成能力，也为提升现有T2V和I2V模型的运动控制能力提供了途径。</li>
<li><strong>为多模态理解和生成奠定基础:</strong> 运动是视频信息的重要组成部分，DisMo对运动的解耦和迁移研究，为未来更深层次的多模态理解（例如，将文本描述的动作与视觉内容相结合）和生成奠定了基础。</li>
<li><strong>促进零样本学习在视频任务中的应用:</strong> 论文中提到DisMo在零样本动作分类任务上表现出色，这表明其学习到的运动表示具有良好的泛化性，能够识别未见过的动作，为视频理解的零样本和少样本学习开辟了新的可能性。</li>
</ul>
<p><strong>4. 可能受益于此研究的相关领域或应用</strong></p>
<ul>
<li><strong>视频编辑和后期制作:</strong> 允许用户轻松地将一个视频中的动作应用到另一个视频中，例如将舞蹈动作迁移到不同角色上，或者将特定运动风格应用到现有素材。</li>
<li><strong>虚拟现实 (VR) 和增强现实 (AR):</strong> 在VR/AR环境中，可以更自然地驱动虚拟角色的动作，或者将现实世界中的运动捕捉并应用到虚拟场景中。</li>
<li><strong>游戏开发:</strong> 快速生成多样化的角色动画，或者将现有角色的动作迁移到新角色上。</li>
<li><strong>机器人学:</strong> 学习和迁移机器人动作，提高机器人的适应性和学习能力。</li>
<li><strong>体育分析:</strong> 识别和迁移运动员的特定动作模式，用于训练或分析。</li>
<li><strong>内容审核和安全:</strong> 识别和分析视频中的异常或特定类型的动作。</li>
<li><strong>人机交互:</strong> 设计更自然和直观的交互方式，通过运动来控制虚拟对象或系统。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性</strong></p>
<p>尽管摘要描述了DisMo的强大能力，但仍可以推断出一些潜在的局限性：</p>
<ul>
<li><strong>对“抽象运动表示”的定义和边界:</strong> 摘要中提到“抽象运动表示”，但其具体的数学定义和模型内部如何实现这种抽象并未详细说明。这种抽象的程度和质量将直接影响迁移效果。</li>
<li><strong>“开放世界”的定义和挑战:</strong> 虽然强调了“开放世界”的迁移能力，但“开放世界”的定义可能存在模糊性。例如，对于极其不相关的类别（如将水流的运动迁移到人物身上），其效果可能仍然有限，或者需要更复杂的处理。</li>
<li><strong>对“语义不相关实体”的界定:</strong> 摘要提到“语义不相关的实体”，但如何界定“语义不相关”以及模型在多大程度上能处理这种不相关性，仍需进一步的实验验证。</li>
<li><strong>计算成本和效率:</strong> 学习抽象表示和进行运动迁移通常需要大量的计算资源。虽然提到“轻量级适配器”，但DisMo本身的训练和推理成本可能仍然较高。</li>
<li><strong>对“内容”的定义和处理:</strong> 摘要强调运动与“内容”解耦，但“内容”的定义（外观、身份、姿态）可能并不完全穷尽。对于一些复杂的、与运动紧密相关的“内容”信息，解耦的难度可能会增加。</li>
<li><strong>潜在的伪影或不自然感:</strong> 尽管论文声称解决了“漂移”问题，但在复杂的运动迁移场景下，仍然可能出现一些视觉上的伪影或不自然感，尤其是在跨越巨大类别差异时。</li>
<li><strong>对训练数据的依赖:</strong> 学习抽象表示通常需要大量的、多样化的视频数据。DisMo的性能可能在很大程度上依赖于其训练数据的质量和覆盖范围。</li>
</ul>
<p>总而言之，DisMo在运动表示的解耦和开放世界运动迁移方面取得了显著的进展，为视频内容创作和理解带来了新的可能性。其核心创新在于通过图像空间重建目标学习通用的、与内容无关的运动表示，从而实现跨越语义鸿沟的运动迁移。然而，关于抽象表示的精确定义、处理极端不相关实体以及计算效率等方面的挑战，仍是未来研究和应用中需要关注的重点。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this gap, we propose DisMo, a novel paradigm for learning abstract motion representations directly from raw video data via an image-space reconstruction objective.</li>
<li>Unlike prior methods, which trade off motion fidelity and prompt adherence, are overfitting to source structure or drifting from the described action, our approach disentangles motion semantics from appearance, enabling accurate transfer and faithful conditioning.</li>
<li>We demonstrate the effectiveness of our method through a diverse set of motion transfer tasks.</li>
<li>Finally, we show that the learned representations are well-suited for downstream motion understanding tasks, consistently outperforming state-of-the-art video representation models such as V-JEPA in zero-shot action classification on benchmarks including Something-Something v2 and Jester.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.23428v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.23428v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.23386v1'></a></p>
<h2 id="vqrae-representation-quantization-autoencoders-for-multimodal-understanding-generation-and-reconstruction"><a href="https://arxiv.org/abs/2511.23386v1">VQRAE: Representation Quantization Autoencoders for Multimodal Understanding, Generation and Reconstruction</a></h2>
<p><strong>Authors:</strong> Sinan Du, Jiahao Guo, Bo Li, Shuhao Cui, Zhengzhuo Xu, Yifu Luo, Yongxian Wei, Kun Gai, Xinggang Wang, Kai Wu, Chun Yuan</p>
<p><strong>Published:</strong> 2025-11-28</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Unifying multimodal understanding, generation and reconstruction representation in a single tokenizer remains a key challenge in building unified models. Previous research predominantly attempts to address this in a dual encoder paradigm, e.g., utilizing the separate encoders for understanding and generation respectively or balancing semantic representations and low-level features with contrastive loss. In this paper, we propose VQRAE, a Vector Quantization version of Representation AutoEncoders, which pioneers the first exploration in unified representation to produce Continuous semantic features for image understanding and Discrete tokens for visual generation within a unified tokenizer. Specifically, we build upon pretrained vision foundation models with a symmetric ViT decoder and adopt a two-stage training strategy: first, it freezes the encoder and learns a high-dimensional semantic VQ codebook with pixel reconstruction objective; then jointly optimizes the encoder with self-distillation constraints. This design enables negligible semantic information for maintaining the ability of multimodal understanding, discrete tokens that are compatible for generation and fine-grained reconstruction. Besides, we identify the intriguing property in quantizing semantic encoders that rely on high-dimensional codebook in contrast to the previous common practice of low-dimensional codebook in image reconstruction. The semantic VQ codebook can achieve a 100% utilization ratio at a dimension of 1536. VQRAE presents competitive performance on several benchmarks of visual understanding, generation and reconstruction with promising scaling property in the autoregressive paradigm for its discrete merits.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“VQRAE: Representation Quantization Autoencoders for Multimodal Understanding, Generation and Reconstruction”的全面中文摘要：</p>
<p><strong>论文题目：</strong> VQRAE: 用于多模态理解、生成和重建的表示量化自编码器</p>
<p><strong>作者：</strong> Sinan Du, Jiahao Guo, Bo Li, Shuhao Cui, Zhengzhuo Xu, Yifu Luo, Yongxian Wei, Kun Gai, Xinggang Wang, Kai Wu, Chun Yuan</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
当前构建统一的多模态模型面临的关键挑战是如何在一个单一的“tokenizer”（分词器）中整合理解、生成和重建这三种任务的表示。现有的方法多采用双编码器范式，即为理解和生成任务分别设计编码器，或通过对比损失来平衡语义表示和低级特征。然而，这些方法往往增加了模型复杂性，阻碍了不同表示之间的深度交互，并且需要巨大的批次大小来平衡损失冲突。</p>
<p><strong>2. 主要创新与方法贡献：</strong>
本文提出了VQRAE（Vector Quantization Representation Autoencoders），一种创新的表示量化自编码器，旨在解决上述挑战。其核心贡献在于：
*   <strong>统一的Tokenizer：</strong> VQRAE 是首个能够在一个统一的Tokenizer中同时产生用于图像理解的<strong>连续语义特征</strong>和用于视觉生成与重建的<strong>离散Token</strong>的模型。
*   <strong>两阶段训练策略：</strong>
    *   <strong>第一阶段：</strong> 冻结预训练的视觉基础模型（VFMs）编码器，学习一个高维度的<strong>语义VQ码本</strong>，并使用像素重建目标进行训练。
    *   <strong>第二阶段：</strong> 解冻VFMs编码器，并引入<strong>自蒸馏损失</strong>来保持语义理解能力，同时优化编码器、码本和解码器以实现精细的重建。
*   <strong>高维语义码本：</strong> VQRAE 探索了量化语义编码器时使用高维码本的特性，与以往在图像重建中常用低维码本的做法形成对比。作者发现，高维码本（如1536维）能够实现100%的利用率，并且在训练过程中避免了码本崩溃的风险。
*   <strong>无卷积的ViT架构：</strong> 模型采用对称的ViT编码器-解码器结构，避免了对卷积像素编码器的依赖，简化了模型设计。</p>
<p><strong>3. 主要结果与意义：</strong>
VQRAE 在多个视觉理解、生成和重建的基准测试中取得了具有竞争力的性能。
*   <strong>性能优势：</strong> VQRAE 在多模态理解任务上超越了其他统一Tokenizer方法，并且在重建任务上表现出色。与双编码器方法相比，VQRAE 更高效，无需额外的多模态对齐或指令微调。
*   <strong>可扩展性：</strong> VQRAE 的离散特性使其在自回归范式下具有良好的可扩展性，为构建更强大的统一多模态模型开辟了新途径。
*   <strong>效率提升：</strong> 通过使用预训练的VFMs作为统一编码器，VQRAE 大大降低了训练开销，并能直接集成到现有的MLLMs中。</p>
<p><strong>4. 提及的局限性：</strong>
*   <strong>权衡问题：</strong> VQRAE 在平衡语义表示和重建性能方面仍有改进空间，可能存在一定的折衷。
*   <strong>量化损失：</strong> 离散Tokenizer固有的量化损失可能使其在与最先进的连续VAE模型竞争时面临挑战。
*   <strong>生成质量：</strong> 在生成任务中，特别是在处理手指和人脸等细节时，模型仍可能出现一些瑕疵。</p>
<p><strong>5. 未来研究方向：</strong>
*   <strong>更有效的权衡方法：</strong> 探索更有效的方法来平衡理解和重建性能，以最小化对理解能力的影响。
*   <strong>生成与理解的增强：</strong> 研究如何利用生成和重建能力来进一步增强理解能力。
*   <strong>改进生成质量：</strong> 进一步提升生成质量，尤其是在空间关系、纹理渲染以及人脸和手指细节的准确性方面。
*   <strong>集成与扩展：</strong> 研究如何将这些表示集成到更广泛的任务中，以及如何实现高效的模型扩展。</p>
<p><strong>总结：</strong>
VQRAE 是一项重要的研究成果，它成功地在一个统一的Tokenizer中实现了连续语义特征和离散Token的生成，为多模态理解、生成和重建任务提供了一个高效且强大的解决方案。该模型通过创新的两阶段训练策略和高维语义码本的应用，克服了现有方法的局限性，并在多个基准测试中展现出优越的性能和良好的可扩展性，为未来统一多模态模型的研究奠定了坚实的基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this paper, we propose VQRAE, a Vector Quantization version of Representation AutoEncoders, which pioneers the first exploration in unified representation to produce Continuous semantic features for image understanding and Discrete tokens for visual generation within a unified tokenizer.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.23386v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.23386v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.23377v1'></a></p>
<h2 id="deal-300k-diffusion-based-editing-area-localization-with-a-300k-scale-dataset-and-frequency-prompted-baseline"><a href="https://arxiv.org/abs/2511.23377v1">DEAL-300K: Diffusion-based Editing Area Localization with a 300K-Scale Dataset and Frequency-Prompted Baseline</a></h2>
<p><strong>Authors:</strong> Rui Zhang, Hongxia Wang, Hangqing Liu, Yang Zhou, Qiang Zeng</p>
<p><strong>Published:</strong> 2025-11-28</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Diffusion-based image editing has made semantic level image manipulation easy for general users, but it also enables realistic local forgeries that are hard to localize. Existing benchmarks mainly focus on the binary detection of generated images or the localization of manually edited regions and do not reflect the properties of diffusion-based edits, which often blend smoothly into the original content. We present Diffusion-Based Image Editing Area Localization Dataset (DEAL-300K), a large scale dataset for diffusion-based image manipulation localization (DIML) with more than 300,000 annotated images. We build DEAL-300K by using a multi-modal large language model to generate editing instructions, a mask-free diffusion editor to produce manipulated images, and an active-learning change detection pipeline to obtain pixel-level annotations. On top of this dataset, we propose a localization framework that uses a frozen Visual Foundation Model (VFM) together with Multi Frequency Prompt Tuning (MFPT) to capture both semantic and frequency-domain cues of edited regions. Trained on DEAL-300K, our method reaches a pixel-level F1 score of 82.56% on our test split and 80.97% on the external CoCoGlide benchmark, providing strong baselines and a practical foundation for future DIML research.The dataset can be accessed via https://github.com/ymhzyj/DEAL-300K.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“DEAL-300K: Diffusion-based Editing Area Localization with a 300K-Scale Dataset and Frequency-Prompted Baseline”的全面中文摘要：</p>
<p><strong>论文题目：</strong> DEAL-300K: 基于扩散的编辑区域定位，包含一个300K规模的数据集和频率提示基线</p>
<p><strong>作者：</strong> Rui Zhang, Hongxia Wang, Hangqing Liu, Yang Zhou, Qiang Zeng</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
随着扩散模型在图像编辑领域的广泛应用，用户可以轻松进行语义级别的图像编辑，但也带来了更逼真、更难检测的局部伪造图像。现有基准数据集主要关注生成图像的二元检测或手动编辑区域的定位，未能充分反映扩散模型编辑的特性——即编辑内容往往能平滑地融入原始图像，缺乏明显的伪影。因此，如何有效地定位扩散模型生成的编辑区域（Diffusion-based Image Manipulation Localization, DIML）是一个亟待解决的问题。</p>
<p><strong>2. 主要创新点/方法论贡献：</strong></p>
<ul>
<li><strong>DEAL-300K 数据集：</strong> 作者提出了一个大规模（超过30万张标注图像）的Diffusion-based Image Editing Area Localization Dataset (DEAL-300K)，专门用于DIML任务。该数据集的构建过程具有创新性：<ul>
<li><strong>多模态大语言模型（MLLM）驱动的指令生成：</strong> 利用微调后的Qwen-VL模型，结合图像和原型信息，自动生成高质量的、符合图像语义的编辑指令。</li>
<li><strong>无掩码扩散模型进行图像编辑：</strong> 使用InstructPix2Pix等无掩码扩散模型生成编辑后的图像，更贴合实际的编辑流程。</li>
<li><strong>主动学习与变化检测的标注流程：</strong> 结合SAM-CD模型和主动学习策略，实现了像素级标注的自动化，大大减少了人工标注的成本和时间。</li>
</ul>
</li>
<li><strong>多频段提示微调（MFPT）框架：</strong> 作者提出了一种新颖的定位框架，该框架利用预训练的视觉基础模型（VFM）作为冻结编码器，并结合多频段提示微调（MFPT）技术。<ul>
<li><strong>频率输入提示器（FInP）：</strong> 引入了将图像的低级纹理信息（通过高频傅里叶变换提取）与语义特征相结合的机制，以捕捉扩散编辑的细微之处。</li>
<li><strong>特征频率提示器（FFrP）：</strong> 进一步增强了模型对高频和低频信息的处理能力，通过多头自注意力机制分别提取和融合，以提升对局部编辑区域的关注度。</li>
<li><strong>参数高效微调（PEFT）：</strong> MFPT是一种参数高效的微调方法，仅训练少量参数，降低了计算成本，并利用了VFM强大的先验知识。</li>
</ul>
</li>
</ul>
<p><strong>3. 主要结果及意义：</strong></p>
<ul>
<li><strong>数据集性能：</strong> 在DEAL-300K数据集上，作者提出的MFPT方法取得了显著的成果，在测试集上达到了82.56%的像素级F1分数，并在外部CoCoGlide基准数据集上获得了80.97%的F1分数。</li>
<li><strong>基线建立：</strong> DEAL-300K数据集的构建为DIML领域提供了迄今为止最大规模的基准，为未来的研究奠定了坚实的基础。</li>
<li><strong>方法优势：</strong> MFPT框架在处理扩散模型编辑的挑战（如平滑融入、缺乏明显伪影）方面表现出色，能够有效捕捉语义和频率域的线索。该方法在各种数据集和场景下都展现出优越的性能和良好的泛化能力。</li>
<li><strong>鲁棒性：</strong> MFPT模型在JPEG压缩和高斯模糊等图像退化条件下表现出良好的鲁棒性，证明了其在实际应用中的潜力。</li>
<li><strong>数据集价值：</strong> DEAL-300K数据集的引入，特别是其自动化的标注流程，有望显著降低未来大规模数据集的构建成本，推动DIML领域的研究进展。</li>
</ul>
<p><strong>4. 提及的局限性：</strong></p>
<ul>
<li><strong>细节精炼不足：</strong> 作者提到，虽然模型能够准确地识别和勾勒出编辑区域，但在细节的精炼方面仍有提升空间，这可能影响最终的定位精度。</li>
<li><strong>跨领域泛化挑战：</strong> 虽然MFPT在多个数据集上表现良好，但作者也指出，直接比较不同领域训练的预训练模型可能存在公平性问题，并且某些模型在处理不同类型的编辑（如人脸编辑）时存在局限性。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>视频篡改定位：</strong> 作者计划将DEAL-300K扩展到视频领域，以处理视频中的扩散模型篡改问题，进一步扩展其在现实世界中的应用。</li>
<li><strong>更精细的细节处理：</strong> 进一步优化模型以实现更精细的编辑区域定位，提高边界的准确性。</li>
<li><strong>更复杂的编辑场景：</strong> 探索处理多轮编辑、更复杂的语义操纵以及更隐蔽的篡改技术。</li>
<li><strong>模型效率和可解释性：</strong> 进一步研究模型在效率和可解释性方面的改进。</li>
</ul>
<p><strong>总结：</strong></p>
<p>这篇论文的核心贡献在于提出了一个大规模的扩散模型编辑区域定位数据集（DEAL-300K）和一个创新的定位框架（MFPT）。DEAL-300K数据集通过创新的自动化流程构建，解决了传统数据集规模小、标注成本高的问题。MFPT框架则通过融合多模态大语言模型、视觉基础模型以及频率域信息，有效解决了扩散模型编辑难以定位的挑战。研究结果表明，该方法在多个基准数据集上均取得了领先的性能，并展现出良好的鲁棒性和泛化能力，为DIML领域的研究和应用提供了重要的基础和方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present Diffusion-Based Image Editing Area Localization Dataset (DEAL-300K), a large scale dataset for diffusion-based image manipulation localization (DIML) with more than 300,000 annotated images.</li>
<li>On top of this dataset, we propose a localization framework that uses a frozen Visual Foundation Model (VFM) together with Multi Frequency Prompt Tuning (MFPT) to capture both semantic and frequency-domain cues of edited regions.</li>
<li>Trained on DEAL-300K, our method reaches a pixel-level F1 score of 82.56% on our test split and 80.97% on the external CoCoGlide benchmark, providing strong baselines and a practical foundation for future DIML research.The dataset can be accessed via https://github.com/ymhzyj/DEAL-300K.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.23377v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.23377v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.23375v1'></a></p>
<h2 id="optimizing-multimodal-language-models-through-attention-based-interpretability"><a href="https://arxiv.org/abs/2511.23375v1">Optimizing Multimodal Language Models through Attention-based Interpretability</a></h2>
<p><strong>Authors:</strong> Alexander Sergeev, Evgeny Kotelnikov</p>
<p><strong>Published:</strong> 2025-11-28</p>
<p><strong>Categories:</strong> cs.CL, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Modern large language models become multimodal, analyzing various data formats like text and images. While fine-tuning is effective for adapting these multimodal language models (MLMs) to downstream tasks, full fine-tuning is computationally expensive. Parameter-Efficient Fine-Tuning (PEFT) methods address this by training only a small portion of model weights. However, MLMs are difficult to interpret, making it challenging to identify which components are most effective for training to balance efficiency and performance. We propose an attention-based interpretability method for MLMs by analyzing attention scores relative to image tokens. The core idea is to identify attention heads that focus on image key objects. We utilize this information to select optimal model components for PEFT in multimodal models. Our contributions include a method for identifying attention heads associated with image key objects, its application to PEFT for image captioning, and the creation of a new dataset containing images, key object masks, and their textual descriptions. We conducted experiments on MLMs with 2-3 billion parameters to validate the method's effectiveness. By calculating Head Impact (HI) scores we quantify an attention head's focus on key objects, indicating its significance in image understanding. Our fine-tuning experiments demonstrate that adapting layers with the highest HI scores leads to the most significant shifts in metrics compared to pre-trained, randomly selected, or lowest-HI-score layers. This indicates that fine-tuning a small percentage (around 0.01%) of parameters in these crucial layers can substantially influence image understanding capabilities.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Optimizing Multimodal Language Models through Attention-based Interpretability”的全面中文摘要，重点关注其在计算机视觉和机器学习领域的创新性和重要性：</p>
<p><strong>论文题目：</strong> 通过基于注意力的可解释性优化多模态语言模型</p>
<p><strong>作者：</strong> Alexander Sergeev, Evgeny Kotelnikov</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong></p>
<p>现代大型语言模型（LLMs）正朝着多模态方向发展，能够同时处理文本和图像。虽然<strong>参数高效微调（PEFT）</strong>是适应这些多模态语言模型（MLMs）到下游任务的有效方法，但<strong>完全微调计算成本高昂</strong>。然而，MLMs本身<strong>可解释性差</strong>，使得识别哪些模型组件对平衡效率和性能最有效变得困难。因此，研究的核心问题是如何<strong>有效地识别和利用MLMs中对图像理解至关重要的组件，以便进行高效的微调</strong>。</p>
<p><strong>2. 主要创新与方法贡献：</strong></p>
<ul>
<li><strong>基于注意力的可解释性方法：</strong> 论文提出了一种新颖的<strong>基于注意力分数分析</strong>的方法，用于解释MLMs。该方法的核心在于<strong>分析模型注意力分数与图像（特别是图像中的关键对象）token之间的关系</strong>。</li>
<li><strong>识别关键对象注意力头：</strong> 通过计算<strong>Head Impact (HI) 分数</strong>，量化每个注意力头对图像关键对象的关注程度。HI分数高的注意力头被认为是对图像理解更重要的。</li>
<li><strong>数据集构建：</strong> 论文创建了一个<strong>新的数据集</strong>，包含图像、图像关键对象的分割掩码（masks）以及它们的文本描述。</li>
<li><strong>PEFT策略优化：</strong> 利用识别出的关键对象注意力头信息，论文提出了一种<strong>选择最优模型组件进行PEFT的策略</strong>。具体而言，是优先微调具有最高HI分数的层。</li>
<li><strong>实验验证：</strong> 在2-30亿参数的MLMs上进行了实验，验证了该方法的有效性。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong></p>
<ul>
<li><strong>关键组件识别有效性：</strong> 实验证明，通过HI分数可以有效地识别出对图像理解至关重要的注意力头。</li>
<li><strong>PEFT性能提升：</strong> 微调具有最高HI分数的层（top-4）相比于随机选择或最低HI分数的层，能够带来<strong>最显著的模型性能提升</strong>。这表明，即使只微调模型中极小一部分（约0.01%）的参数，如果选择得当，也能显著影响模型的图像理解能力。</li>
<li><strong>模型泛化性：</strong> 该方法不局限于特定任务或领域，可以应用于<strong>各种多模态任务</strong>。</li>
<li><strong>对模型理解的贡献：</strong> 研究揭示了MLMs中存在专注于特定图像关键对象的注意力头，并且这些注意力头在<strong>Transformer层级上具有统计学上的显著性差异</strong>，表明某些Transformer块在图像理解中扮演更重要的角色。</li>
</ul>
<p><strong>4. 提及的局限性：</strong></p>
<ul>
<li><strong>模型架构限制：</strong> 实验主要限于具有相似架构（Vision Transformer编码器和Transformer解码器）的模型，并且图像被表示为嵌入到语言模型prompt中的视觉token。</li>
<li><strong>模型规模限制：</strong> 由于计算资源的限制，实验主要集中在2-30亿参数的模型上。</li>
<li><strong>任务类型限制：</strong> 实验主要集中在图像描述（Image Captioning）和封闭式视觉问答（Visual Question Answering）任务上，<strong>未直接评估开放式文本生成任务</strong>。为了避免歧义和便于度量，使用了答案模板，这可能与真实开放式生成有所不同。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>开放式生成任务评估：</strong> 将该微调方法应用于开放式生成任务，以评估其在更广泛场景下的性能。</li>
<li><strong>更广泛的模型和架构：</strong> 探索该方法在不同模型架构和更大模型规模上的适用性。</li>
<li><strong>跨模态理解的深入研究：</strong> 进一步研究注意力头在跨模态信息融合中的作用，以及如何利用这些洞察来改进多模态模型的整体性能。</li>
<li><strong>更精细的PEFT策略：</strong> 基于更细粒度的注意力分析，探索更精细的PEFT策略，例如仅微调特定的注意力头而非整个层。</li>
</ul>
<p><strong>总结：</strong></p>
<p>这篇论文提出了一种创新的基于注意力分数的可解释性方法，用于识别多模态语言模型（MLMs）中对图像理解至关重要的注意力头。通过计算Head Impact (HI)分数，研究者能够量化这些注意力头对图像关键对象的关注程度。该方法被成功应用于指导参数高效微调（PEFT），实验结果表明，优先微调具有最高HI分数的层能够带来显著的模型性能提升，即使只微调极小比例的参数。这项工作为理解和优化MLMs在多模态任务中的表现提供了新的视角和实用的技术，尤其是在计算资源有限的情况下，为高效地提升模型性能指明了方向。其对计算机视觉领域的重要贡献在于，它提供了一种量化和利用模型内部对视觉信息处理机制的方法，从而实现更智能、更高效的模型微调。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose an attention-based interpretability method for MLMs by analyzing attention scores relative to image tokens.</li>
<li>Our contributions include a method for identifying attention heads associated with image key objects, its application to PEFT for image captioning, and the creation of a new dataset containing images, key object masks, and their textual descriptions.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.23375v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.23375v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.23369v1'></a></p>
<h2 id="simscale-learning-to-drive-via-real-world-simulation-at-scale"><a href="https://arxiv.org/abs/2511.23369v1">SimScale: Learning to Drive via Real-World Simulation at Scale</a></h2>
<p><strong>Authors:</strong> Haochen Tian, Tianyu Li, Haochen Liu, Jiazhi Yang, Yihang Qiu, Guang Li, Junli Wang, Yinfeng Gao, Zhang Zhang, Liang Wang, Hangjun Ye, Tieniu Tan, Long Chen, Hongyang Li</p>
<p><strong>Published:</strong> 2025-11-28</p>
<p><strong>Categories:</strong> cs.CV, cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>Achieving fully autonomous driving systems requires learning rational decisions in a wide span of scenarios, including safety-critical and out-of-distribution ones. However, such cases are underrepresented in real-world corpus collected by human experts. To complement for the lack of data diversity, we introduce a novel and scalable simulation framework capable of synthesizing massive unseen states upon existing driving logs. Our pipeline utilizes advanced neural rendering with a reactive environment to generate high-fidelity multi-view observations controlled by the perturbed ego trajectory. Furthermore, we develop a pseudo-expert trajectory generation mechanism for these newly simulated states to provide action supervision. Upon the synthesized data, we find that a simple co-training strategy on both real-world and simulated samples can lead to significant improvements in both robustness and generalization for various planning methods on challenging real-world benchmarks, up to +6.8 EPDMS on navhard and +2.9 on navtest. More importantly, such policy improvement scales smoothly by increasing simulation data only, even without extra real-world data streaming in. We further reveal several crucial findings of such a sim-real learning system, which we term SimScale, including the design of pseudo-experts and the scaling properties for different policy architectures. Our simulation data and code would be released.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“SimScale: Learning to Drive via Real-World Simulation at Scale”的全面中文摘要：</p>
<p><strong>论文题目：</strong> SimScale: Learning to Drive via Real-World Simulation at Scale</p>
<p><strong>作者：</strong> Haochen Tian, Tianyu Li, Haochen Liu, Jiazhi Yang, Yihang Qiu, Guang Li, Junli Wang, Yinfeng Gao, Zhang Zhang, Liang Wang, Hangjun Ye, Tieniu Tan, Long Chen, Hongyang Li</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
实现完全自主驾驶系统需要模型能够学习在各种场景下做出理性决策，特别是那些安全关键和分布外（out-of-distribution, OOD）的场景。然而，人类专家收集的真实世界数据集中，这些 OOD 场景的代表性不足。仅仅依靠真实世界数据进行扩展效率低下，且难以覆盖这些稀有但重要的场景，导致模型泛化能力受限。</p>
<p><strong>2. 主要创新与方法贡献：</strong>
该论文提出了 <strong>SimScale</strong>，一个新颖且可扩展的模拟框架，用于在真实世界驾驶日志的基础上合成大规模的、未曾见过的场景。其核心创新包括：</p>
<ul>
<li><strong>可扩展的模拟数据生成框架：</strong> 利用现有的真实世界驾驶日志作为起点，通过扰动（perturbation）来生成新的场景。</li>
<li><strong>高保真度神经渲染与反应式环境：</strong> 使用先进的神经渲染技术（基于 3DGS [39]）和反应式环境（reactive environment [70]），生成高保真度的多视角观测，并确保模拟中的其他车辆能够响应式地与主车互动，增加了场景的真实性和多样性。</li>
<li><strong>伪专家轨迹生成机制：</strong> 为新合成的模拟场景开发了伪专家（pseudo-expert）轨迹生成方法，提供动作监督。论文对比了两种伪专家策略：<ul>
<li><strong>恢复式专家（Recovery-based Expert）：</strong> 旨在将轨迹引导回人类轨迹流形（human trajectory manifold），行为更保守，但能稳定分布外漂移。</li>
<li><strong>规划器式专家（Planner-based Expert）：</strong> 利用特权规划器（privileged planner）生成最优轨迹，探索性更强，能产生更多样化的轨迹。</li>
</ul>
</li>
<li><strong>Sim-Real 共训练策略：</strong> 提出了一种简单有效的 Sim-Real 共训练策略，将真实世界数据与合成的模拟数据结合进行训练，以提升模型的鲁棒性和泛化能力，同时缓解模拟到真实（sim-to-real）的视觉域迁移问题。</li>
<li><strong>数据缩放分析：</strong> 系统地分析了模拟数据对不同类型端到端规划器（回归、扩散、词汇评分）的性能影响，并研究了在固定真实世界数据量下，增加模拟数据量对模型性能的影响规律。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong>
*   <strong>显著的性能提升：</strong> SimScale 框架通过 Sim-Real 共训练，在挑战性的真实世界基准测试（navhard 和 navtest）上，显著提升了多种规划方法的鲁棒性和泛化能力，最高可达 +6.8 EPDMS（navhard）和 +2.9 EPDMS（navtest）。
*   <strong>可预测的数据缩放趋势：</strong> 研究表明，随着模拟数据的增加，模型性能可以平滑且可预测地提升，即使不增加真实世界数据。
*   <strong>伪专家的重要性：</strong> 实验揭示了探索性更强的伪专家（如规划器式专家）比保守的恢复式专家更能带来持续的性能提升，尤其是在数据量较大时。
*   <strong>多模态模型的优势：</strong> 具有多模态建模能力（如 DiffusionDrive）的规划器比单模态回归模型更能受益于模拟数据的扩展，展现出更强的缩放特性。
*   <strong>反应式环境的价值：</strong> 反应式环境能够生成更真实、更多样化的交通交互场景，从而提升模拟数据的有效性。
*   <strong>研究的普适性：</strong> SimScale 的方法对不同类型的端到端规划器都有效，表明其模型无关性。</p>
<p><strong>4. 论文中提到的局限性：</strong>
*   <strong>伪专家轨迹的静态性：</strong> 当前的伪专家轨迹扰动是静态的，未来可以考虑使用自演化（self-evolving）方法来生成更动态的轨迹。
*   <strong>特权规划器的局限性：</strong> 论文中使用的特权规划器是基于规则的，性能有限，可能导致舒适性指标（如 HC, EC）的下降，并且在极端情况下可能失效。更先进的基于学习的规划器可以改进这一点。
*   <strong>场景模拟的局限性：</strong> 交通行为模拟中的其他智能体由 IDM [70] 控制，这限制了场景的多样性。传感器模拟方面，虽然 3DGS 效果出色，但仍有改进空间，例如引入 LiDAR 等多模态信息。</p>
<p><strong>5. 未来研究方向：</strong>
*   <strong>自演化伪专家：</strong> 探索使用自演化方法来生成更动态、更具探索性的伪专家轨迹。
*   <strong>更先进的特权规划器：</strong> 集成更先进的基于学习的特权规划器，以提高伪专家轨迹的质量和真实感。
*   <strong>更丰富的场景模拟：</strong> 引入更先进的交通行为模拟器（如扩散模型）和多模态传感器模拟（如 LiDAR），以进一步提升模拟场景的真实性和多样性。
*   <strong>在线强化学习与自玩：</strong> 探索将 SimScale 与在线强化学习（RL）和自玩（self-play）相结合，以实现更高效的自主学习。
*   <strong>更广泛的应用：</strong> 将 SimScale 的理念推广到更广泛的机器人和自动驾驶领域，以解决数据稀疏性问题。</p>
<p><strong>总结：</strong>
SimScale 是一个重要的贡献，它提供了一个系统性的解决方案，通过大规模、高保真的模拟数据来克服真实世界自动驾驶数据稀缺的挑战。该框架通过创新的伪专家生成和 Sim-Real 共训练策略，显著提升了端到端规划器的鲁棒性和泛化能力，并且其数据缩放特性清晰可预测。这项工作为学术界和工业界提供了一个强大的工具，以加速自动驾驶系统的研发和部署，尤其是在处理罕见但关键的 OOD 场景方面。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To complement for the lack of data diversity, we introduce a novel and scalable simulation framework capable of synthesizing massive unseen states upon existing driving logs.</li>
<li>Furthermore, we develop a pseudo-expert trajectory generation mechanism for these newly simulated states to provide action supervision.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.23369v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.23369v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2511.23334v1'></a></p>
<h2 id="markovian-scale-prediction-a-new-era-of-visual-autoregressive-generation"><a href="https://arxiv.org/abs/2511.23334v1">Markovian Scale Prediction: A New Era of Visual Autoregressive Generation</a></h2>
<p><strong>Authors:</strong> Yu Zhang, Jingyi Liu, Yiwei Shi, Qi Zhang, Duoqian Miao, Changwei Wang, Longbing Cao</p>
<p><strong>Published:</strong> 2025-11-28</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Visual AutoRegressive modeling (VAR) based on next-scale prediction has revitalized autoregressive visual generation. Although its full-context dependency, i.e., modeling all previous scales for next-scale prediction, facilitates more stable and comprehensive representation learning by leveraging complete information flow, the resulting computational inefficiency and substantial overhead severely hinder VAR's practicality and scalability. This motivates us to develop a new VAR model with better performance and efficiency without full-context dependency. To address this, we reformulate VAR as a non-full-context Markov process, proposing Markov-VAR. It is achieved via Markovian Scale Prediction: we treat each scale as a Markov state and introduce a sliding window that compresses certain previous scales into a compact history vector to compensate for historical information loss owing to non-full-context dependency. Integrating the history vector with the Markov state yields a representative dynamic state that evolves under a Markov process. Extensive experiments demonstrate that Markov-VAR is extremely simple yet highly effective: Compared to VAR on ImageNet, Markov-VAR reduces FID by 10.5% (256 <script type="math/tex">\times</script> 256) and decreases peak memory consumption by 83.8% (1024 <script type="math/tex">\times</script> 1024). We believe that Markov-VAR can serve as a foundation for future research on visual autoregressive generation and other downstream tasks.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Markovian Scale Prediction: A New Era of Visual Autoregressive Generation”的全面中文摘要，重点关注其研究问题、方法、结果、局限性和未来方向：</p>
<p><strong>论文题目：</strong> Markovian Scale Prediction: A New Era of Visual Autoregressive Generation</p>
<p><strong>作者：</strong> Yu Zhang, Jingyi Liu, Yiwei Shi, Qi Zhang, Duoqian Miao, Changwei Wang, Longbing Cao</p>
<p><strong>摘要：</strong></p>
<p>这篇论文提出了一种名为 <strong>Markov-VAR</strong> 的新型视觉自回归生成模型，旨在解决现有 Visual AutoRegressive (VAR) 模型在处理高分辨率图像生成时面临的计算效率低下和可扩展性差的问题。</p>
<p><strong>1. 研究问题/研究目标：</strong></p>
<p>现有的 VAR 模型通过“下一尺度预测”实现视觉生成，其核心在于“全上下文依赖”，即在预测当前尺度的特征时，会考虑所有之前的尺度信息。这种方法虽然有利于学习稳定和全面的表示，但随着图像分辨率的提高，计算量呈平方级增长，导致训练和推理效率低下，严重限制了其在实际应用中的可行性和可扩展性。因此，研究目标是开发一种<strong>不依赖全上下文依赖</strong>的 VAR 模型，在<strong>保持或提升生成质量</strong>的同时，<strong>显著提高效率和可扩展性</strong>。</p>
<p><strong>2. 关键创新/方法贡献：</strong></p>
<ul>
<li><strong>将 VAR 重构为非全上下文马尔可夫过程：</strong> 论文的核心思想是将 VAR 的预测过程从依赖所有历史尺度转变为一个<strong>马尔可夫过程</strong>。这意味着当前尺度的预测仅依赖于前一个尺度（或有限的历史信息），而不是所有历史尺度。</li>
<li><strong>马尔可夫尺度预测 (Markovian Scale Prediction)：</strong> 论文将每个尺度视为一个马尔可夫状态，并引入了一种新的预测机制。</li>
<li><strong>历史补偿机制 (History Compensation Mechanism)：</strong> 为了弥补因去除全上下文依赖而可能丢失的历史信息，论文设计了一个轻量级的历史补偿机制。该机制使用一个<strong>滑动窗口</strong>来压缩最近的几个历史尺度，将其整合成一个紧凑的<strong>历史向量</strong>。</li>
<li><strong>动态状态表示：</strong> 将当前尺度的特征与历史向量结合，形成一个代表性的<strong>动态状态</strong>，该动态状态在马尔可夫过程中演化，用于进行下一尺度的预测。</li>
<li><strong>Markov-VAR Transformer：</strong> 论文还提出了一个 Markov-VAR Transformer 架构，以高效地实现马尔可夫尺度预测和历史补偿。</li>
</ul>
<p><strong>3. 主要结果及意义：</strong></p>
<ul>
<li><strong>显著的效率提升：</strong><ul>
<li>在 ImageNet 数据集上，与原始 VAR 模型相比，Markov-VAR 在 256x256 分辨率下将 <strong>FID 降低了 10.5%</strong>，表明生成质量有所提升。</li>
<li>在 1024x1024 分辨率下，Markov-VAR 的<strong>峰值内存消耗降低了 83.8%</strong>。</li>
<li>在推理速度方面，Markov-VAR 在 256x256 分辨率下比其他 VAR 模型（如 FlexVAR）<strong>加速了 1.33 倍</strong>。</li>
</ul>
</li>
<li><strong>保持或提升生成质量：</strong> 实验结果表明，Markov-VAR 在保持模型规模相当的情况下，在 FID、IS、Precision 和 Recall 等指标上与 VAR 及其变体模型相比，表现出<strong>相当或更优的性能</strong>。</li>
<li><strong>模型简洁性与有效性：</strong> 论文强调 Markov-VAR 模型<strong>非常简洁但效果显著</strong>，易于实现和扩展。</li>
<li><strong>作为基础模型：</strong> 作者认为 Markov-VAR 可以作为未来视觉自回归生成和其他下游任务的<strong>基础模型</strong>。</li>
</ul>
<p><strong>4. 提及的局限性：</strong></p>
<ul>
<li>虽然论文提出了一种历史补偿机制来缓解信息丢失，但作者也承认，与完全依赖全上下文的 VAR 模型相比，在某些情况下，非全上下文依赖<strong>可能仍然会丢失一些原始历史信息</strong>。</li>
<li>论文中提到，尽管滑动窗口大小为 3 时效果最佳，但对于某些特定任务或模型深度，可能需要进一步调整窗口大小以达到最优性能。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>更广泛的应用：</strong> 将 Markov-VAR 应用于更广泛的视觉生成任务，如图像编辑、超分辨率、3D 对象生成等。</li>
<li><strong>与其他生成模型的结合：</strong> 探索将 Markov-VAR 与扩散模型、GAN 等其他先进生成模型相结合的可能性，以期获得更优的性能。</li>
<li><strong>更精细的历史补偿机制：</strong> 研究更先进的历史信息压缩和补偿策略，以进一步减少信息丢失，同时保持计算效率。</li>
<li><strong>Scaling Law 的进一步探索：</strong> 论文初步分析了 Markov-VAR 的 scaling law，未来可以更深入地研究其在模型规模、数据量和计算资源等方面的扩展规律。</li>
<li><strong>模型权重公开：</strong> 作者已公开了 Markov-VAR 的模型权重，鼓励社区在此基础上进行进一步的研究和开发。</li>
</ul>
<p>总而言之，这篇论文通过引入马尔可夫尺度预测和历史补偿机制，成功地解决了 VAR 模型在计算效率和可扩展性方面的瓶颈，为高效、高质量的视觉自回归生成开辟了新的道路。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>This motivates us to develop a new VAR model with better performance and efficiency without full-context dependency.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2511.23334v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2511.23334v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-12-01 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
