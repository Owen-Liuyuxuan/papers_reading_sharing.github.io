<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-08-27 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-06
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../../undone/depth_prediction_citation/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-08-28/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-08-27">Arxiv Computer Vision Papers - 2025-08-27</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#diffusion-based-data-augmentation-for-medical-image-segmentation" class="nav-link">Diffusion-Based Data Augmentation for Medical Image Segmentation</a>
                </li>
                <li class="nav-item">
                    <a href="#beyond-flattening-a-geometrically-principled-positional-encoding-for-vision-transformers-with-weierstrass-elliptic-functions" class="nav-link">Beyond flattening: a geometrically principled positional encoding for vision transformers with Weierstrass elliptic functions</a>
                </li>
                <li class="nav-item">
                    <a href="#why-relational-graphs-will-save-the-next-generation-of-vision-foundation-models" class="nav-link">Why Relational Graphs Will Save the Next Generation of Vision Foundation Models?</a>
                </li>
                <li class="nav-item">
                    <a href="#enhanced-drift-aware-computer-vision-architecture-for-autonomous-driving" class="nav-link">Enhanced Drift-Aware Computer Vision Architecture for Autonomous Driving</a>
                </li>
                <li class="nav-item">
                    <a href="#scene-agnostic-traversability-labeling-and-estimation-via-a-multimodal-self-supervised-framework" class="nav-link">Scene-Agnostic Traversability Labeling and Estimation via a Multimodal Self-supervised Framework</a>
                </li>
                <li class="nav-item">
                    <a href="#learning-to-detect-label-errors-by-making-them-a-method-for-segmentation-and-object-detection-datasets" class="nav-link">Learning to Detect Label Errors by Making Them: A Method for Segmentation and Object Detection Datasets</a>
                </li>
                <li class="nav-item">
                    <a href="#catformer-contrastive-adversarial-transformer-for-image-super-resolution" class="nav-link">CATformer: Contrastive Adversarial Transformer for Image Super-Resolution</a>
                </li>
                <li class="nav-item">
                    <a href="#roofseg-an-edge-aware-transformer-based-network-for-end-to-end-roof-plane-segmentation" class="nav-link">RoofSeg: An edge-aware transformer-based network for end-to-end roof plane segmentation</a>
                </li>
                <li class="nav-item">
                    <a href="#endoufm-utilizing-foundation-models-for-monocular-depth-estimation-of-endoscopic-images" class="nav-link">EndoUFM: Utilizing Foundation Models for Monocular depth estimation of endoscopic images</a>
                </li>
                <li class="nav-item">
                    <a href="#isalux-illumination-and-segmentation-aware-transformer-employing-mixture-of-experts-for-low-light-image-enhancement" class="nav-link">ISALux: Illumination and Segmentation Aware Transformer Employing Mixture of Experts for Low Light Image Enhancement</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-08-27">Arxiv Computer Vision Papers - 2025-08-27</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>好的，这是一份针对2025年8月25日Arxiv计算机视觉领域最新论文的执行摘要。</p>
<hr />
<p><strong>Arxiv 计算机视觉领域最新论文执行摘要 (2025-08-25)</strong></p>
<p><strong>报告日期:</strong> 2025年8月25日
<strong>报告人:</strong> [您的姓名/研究助理]</p>
<p>本报告旨在为忙碌的研究人员提供一份关于2025年8月25日Arxiv上发布的10篇计算机视觉领域最新论文的简明概述，以帮助快速了解该领域的重要发展。</p>
<hr />
<p><strong>1. 主要主题与趋势概述</strong></p>
<p>本次发布的论文展现了计算机视觉领域几个显著的趋势：</p>
<ul>
<li><strong>Transformer架构的持续主导与深化:</strong> 几乎一半的论文（2, 7, 8, 10）直接或间接使用了Transformer架构，并对其进行创新性改进（如几何位置编码、MoE）或应用于特定任务（超分、分割、图像增强）。</li>
<li><strong>特定应用领域的深度探索:</strong> 医疗影像（1, 9）、自动驾驶（4, 5）、图像超分辨率（7）、低光照图像增强（10）和屋顶分割（8）等领域持续受到关注，研究人员致力于将先进模型落地到实际问题中。</li>
<li><strong>数据为中心的人工智能 (Data-Centric AI):</strong> 论文关注数据质量（6）和数据增强（1），强调了高质量数据和有效数据处理对模型性能的重要性。</li>
<li><strong>基础模型 (Foundation Models) 的演进与应用:</strong> 论文探讨了基础模型的未来发展方向（3）以及它们在特定、挑战性领域（如内窥镜深度估计，9）的应用潜力。</li>
<li><strong>自监督学习与生成模型:</strong> 自监督学习（5）和扩散模型（1）作为强大的范式，被用于解决数据稀缺和复杂场景理解等问题。</li>
</ul>
<p><strong>2. 特别显著或创新的论文</strong></p>
<ul>
<li><strong>论文2: "Beyond flattening: a geometrically principled positional encoding for vision transformers with Weierstrass elliptic functions"</strong><ul>
<li><strong>创新点:</strong> 这篇论文提出了一个基于魏尔斯特拉斯椭圆函数的几何原理位置编码，超越了传统Transformer中扁平化的位置编码方式。它可能为Transformer的底层机制带来更深刻的理论理解和性能提升，具有潜在的架构级影响。</li>
</ul>
</li>
<li><strong>论文3: "Why Relational Graphs Will Save the Next Generation of Vision Foundation Models?"</strong><ul>
<li><strong>创新点:</strong> 这是一篇具有前瞻性和概念性的论文，探讨了关系图在未来视觉基础模型中的关键作用。它挑战了当前基础模型仅依赖大规模数据和参数的范式，提出通过引入关系归纳偏置来提升模型的泛化能力和可解释性，对领域发展方向具有指导意义。</li>
</ul>
</li>
<li><strong>论文6: "Learning to Detect Label Errors by Making Them: A Method for Segmentation and Object Detection Datasets"</strong><ul>
<li><strong>创新点:</strong> 该研究提出了一种通过“制造”标签错误来学习检测真实标签错误的新颖方法。这直接解决了实际应用中数据集质量控制的痛点，对于提高模型鲁棒性和减少人工标注成本具有重要价值。</li>
</ul>
</li>
<li><strong>论文9: "EndoUFM: Utilizing Foundation Models for Monocular depth estimation of endoscopic images"</strong><ul>
<li><strong>创新点:</strong> 将基础模型应用于极具挑战性的内窥镜单目深度估计任务，展示了基础模型在特定、数据稀缺且形变严重的医疗场景中的强大适应性和潜力。</li>
</ul>
</li>
</ul>
<p><strong>3. 新兴研究方向或技术</strong></p>
<ul>
<li><strong>Transformer架构的几何化与理论深化:</strong> 论文2预示着对Transformer核心组件（如位置编码）进行更深层次的数学和几何原理探索，以突破现有性能瓶颈。</li>
<li><strong>将关系归纳偏置融入基础模型:</strong> 论文3强调了在基础模型中引入结构化知识（如关系图）的重要性，以提升其推理能力和泛化性，而非仅仅依赖规模。</li>
<li><strong>数据质量管理作为可学习任务:</strong> 论文6提出通过机器学习方法自动检测和纠正数据集中的标签错误，将数据清洗从人工密集型任务转变为自动化流程。</li>
<li><strong>扩散模型在数据增强中的应用:</strong> 论文1展示了扩散模型在生成高质量合成数据以增强医疗影像数据集方面的潜力，尤其适用于数据稀缺的领域。</li>
<li><strong>多模态自监督学习在复杂场景理解中的应用:</strong> 论文5利用多模态自监督框架进行场景无关的可通行性估计，为自动驾驶等领域提供了新的解决方案。</li>
</ul>
<p><strong>4. 建议阅读的论文</strong></p>
<p>根据研究兴趣和潜在影响，建议优先阅读以下论文：</p>
<ul>
<li><strong>对于关注Transformer底层机制和架构创新的研究人员:</strong><ul>
<li><strong>论文2: "Beyond flattening: a geometrically principled positional encoding for vision transformers with Weierstrass elliptic functions"</strong></li>
</ul>
</li>
<li><strong>对于关注基础模型未来发展和理论瓶颈的研究人员:</strong><ul>
<li><strong>论文3: "Why Relational Graphs Will Save the Next Generation of Vision Foundation Models?"</strong></li>
</ul>
</li>
<li><strong>对于从事数据集构建、质量控制或模型实际部署的研究人员:</strong><ul>
<li><strong>论文6: "Learning to Detect Label Errors by Making Them: A Method for Segmentation and Object Detection Datasets"</strong></li>
</ul>
</li>
<li><strong>对于医疗影像领域或对基础模型在特定领域落地感兴趣的研究人员:</strong><ul>
<li><strong>论文9: "EndoUFM: Utilizing Foundation Models for Monocular depth estimation of endoscopic images"</strong></li>
<li><strong>论文1: "Diffusion-Based Data Augmentation for Medical Image Segmentation"</strong></li>
</ul>
</li>
</ul>
<p>其他论文（4, 5, 7, 8, 10）对各自的特定应用领域具有直接价值，可根据个人研究方向选择性阅读。</p>
<hr />
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2508.17844v1">Diffusion-Based Data Augmentation for Medical Image Segmentation</a></li>
<li><a href="#2508.19167v1">Beyond flattening: a geometrically principled positional encoding for vision transformers with Weierstrass elliptic functions</a></li>
<li><a href="#2508.18421v1">Why Relational Graphs Will Save the Next Generation of Vision Foundation Models?</a></li>
<li><a href="#2508.17975v1">Enhanced Drift-Aware Computer Vision Architecture for Autonomous Driving</a></li>
<li><a href="#2508.18249v1">Scene-Agnostic Traversability Labeling and Estimation via a Multimodal Self-supervised Framework</a></li>
<li><a href="#2508.17930v1">Learning to Detect Label Errors by Making Them: A Method for Segmentation and Object Detection Datasets</a></li>
<li><a href="#2508.17708v1">CATformer: Contrastive Adversarial Transformer for Image Super-Resolution</a></li>
<li><a href="#2508.19003v1">RoofSeg: An edge-aware transformer-based network for end-to-end roof plane segmentation</a></li>
<li><a href="#2508.17916v1">EndoUFM: Utilizing Foundation Models for Monocular depth estimation of endoscopic images</a></li>
<li><a href="#2508.17885v1">ISALux: Illumination and Segmentation Aware Transformer Employing Mixture of Experts for Low Light Image Enhancement</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2508.17844v1'></a></p>
<h2 id="diffusion-based-data-augmentation-for-medical-image-segmentation"><a href="https://arxiv.org/abs/2508.17844v1">Diffusion-Based Data Augmentation for Medical Image Segmentation</a></h2>
<p><strong>Authors:</strong> Maham Nazir, Muhammad Aqeel, Francesco Setti</p>
<p><strong>Published:</strong> 2025-08-25</p>
<p><strong>Categories:</strong> cs.CV, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Medical image segmentation models struggle with rare abnormalities due to
scarce annotated pathological data. We propose DiffAug a novel framework that
combines textguided diffusion-based generation with automatic segmentation
validation to address this challenge. Our proposed approach uses latent
diffusion models conditioned on medical text descriptions and spatial masks to
synthesize abnormalities via inpainting on normal images. Generated samples
undergo dynamic quality validation through a latentspace segmentation network
that ensures accurate localization while enabling single-step inference. The
text prompts, derived from medical literature, guide the generation of diverse
abnormality types without requiring manual annotation. Our validation mechanism
filters synthetic samples based on spatial accuracy, maintaining quality while
operating efficiently through direct latent estimation. Evaluated on three
medical imaging benchmarks (CVC-ClinicDB, Kvasir-SEG, REFUGE2), our framework
achieves state-of-the-art performance with 8-10% Dice improvements over
baselines and reduces false negative rates by up to 28% for challenging cases
like small polyps and flat lesions critical for early detection in screening
applications.</p>
<p><strong>Analysis:</strong></p>
<p>作为计算机视觉和机器学习领域的专家，我对这篇论文摘要进行如下分析：</p>
<hr />
<h3 id="1-concise-summary">1. 论文主要贡献的简明摘要 (Concise Summary)</h3>
<p>这篇论文提出了一种名为 DiffAug 的新颖框架，旨在通过生成高质量的合成数据来解决医学图像分割模型在处理罕见异常时面临的数据稀缺问题。DiffAug 结合了文本引导的扩散模型进行异常生成（通过在正常图像上进行内绘），并利用一个高效的潜在空间分割网络进行动态质量验证，确保了生成样本的空间准确性。该方法在多个医学图像基准测试中取得了显著的性能提升，尤其是在处理小型和扁平病变等挑战性病例时。</p>
<h3 id="2-key-innovation-or-methodological-approach">2. 关键创新或方法学方法 (Key Innovation or Methodological Approach)</h3>
<p>该论文的关键创新在于其<strong>集成化的、双阶段方法</strong>：
1.  <strong>文本引导的潜在扩散模型进行条件生成：</strong> DiffAug 利用潜在扩散模型，通过医学文本描述和空间掩码作为条件，在正常图像上进行内绘来合成各种异常。这使得能够根据文本提示生成多样化的异常类型，而无需手动标注生成过程。
2.  <strong>高效的潜在空间分割网络进行动态质量验证：</strong> 论文引入了一个独特的验证机制，通过一个在潜在空间操作的分割网络来动态评估生成样本的质量和定位准确性。这种方法不仅确保了合成数据的实用性，而且通过单步推理和直接潜在估计实现了高效的过滤，避免了生成低质量或不准确的样本。</p>
<p>这种将智能生成与高效、自动验证相结合的策略，是其区别于传统数据增强和一般扩散模型应用的关键。</p>
<h3 id="3-potential-impact-on-the-field">3. 对领域潜在影响 (Potential Impact on the Field)</h3>
<p>这项研究对计算机视觉和医学图像分析领域具有深远影响：
*   <strong>解决数据稀缺的核心挑战：</strong> 它为医学图像分析中长期存在的罕见疾病和异常数据稀缺问题提供了一个强大且可扩展的解决方案，从而能够训练出更鲁棒、更准确的分割模型。
*   <strong>提升诊断准确性与早期检测：</strong> 显著降低了小息肉和扁平病变等挑战性病例的假阴性率，这意味着可以更早、更准确地检测到关键疾病，对癌症筛查等应用具有巨大的临床价值。
*   <strong>推动合成数据在医疗领域的应用：</strong> 证明了高质量、验证过的合成数据在医疗领域作为真实数据有效补充的潜力，可能加速新算法的开发和部署。
*   <strong>启发新的数据增强范式：</strong> 其结合生成模型与智能验证的框架，可以为其他需要高质量合成数据（尤其是在数据不平衡或稀缺场景下）的计算机视觉任务提供新的思路。</p>
<h3 id="4-related-areas-or-applications">4. 相关领域或应用 (Related Areas or Applications)</h3>
<p>除了医学图像分割本身，这项研究还可以惠及以下领域和应用：
*   <strong>医学图像检测与分类：</strong> 类似的方法可以用于生成罕见病变的检测或分类任务的训练数据。
*   <strong>少样本学习 (Few-Shot Learning) 和零样本学习 (Zero-Shot Learning)：</strong> 通过生成合成样本，可以有效扩展有限的真实数据集，从而改善少样本或零样本场景下的模型性能。
*   <strong>领域适应 (Domain Adaptation)：</strong> 生成特定领域或特定设备特征的合成数据，有助于模型在不同数据源之间进行泛化。
*   <strong>工业缺陷检测：</strong> 在工业生产中，罕见缺陷的图像数据通常非常稀缺，该方法可以用于生成这些缺陷的合成图像，以训练更准确的检测系统。
*   <strong>自动驾驶：</strong> 生成极端或罕见交通场景（如事故、异常天气条件）的图像数据，以提高自动驾驶系统的鲁棒性。
*   <strong>计算机图形学与虚拟现实：</strong> 生成具有特定属性或条件的图像内容，用于训练或模拟。</p>
<h3 id="5-limitations-inferable-from-the-abstract">5. 从摘要中可推断的局限性 (Limitations Inferable from the Abstract)</h3>
<p>尽管摘要展示了令人印象深刻的结果，但仍可推断出一些潜在局限性：
*   <strong>文本提示的质量和覆盖范围：</strong> 摘要提到文本提示来源于医学文献，但其能否完全捕捉所有罕见异常的细微特征和多样性，以及如何处理文献中未充分描述的异常，仍是一个问题。文本提示的质量直接影响生成样本的准确性和多样性。
*   <strong>生成样本的真实性与临床可信度：</strong> 尽管通过潜在空间验证确保了空间准确性，但生成样本的视觉真实感（即是否能被临床医生认为是真实的）仍需进一步评估。扩散模型可能引入微妙的伪影或不自然的纹理，这在临床应用中可能是敏感的。
*   <strong>计算资源需求：</strong> 扩散模型通常计算成本高昂，结合潜在空间分割网络，整个框架的训练和推理可能需要大量的计算资源和时间。
*   <strong>对“正常图像”的依赖：</strong> 该方法通过在正常图像上进行内绘来合成异常。这意味着它需要一个足够大且多样化的“正常”图像数据集作为基础。如果正常图像本身也稀缺或具有高度变异性，则可能会限制其应用。
*   <strong>潜在空间验证的局限性：</strong> 尽管高效，但潜在空间中的验证可能无法捕捉到像素空间中所有细微的错误或不一致性。它可能在某些情况下对生成质量的评估不够全面。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose DiffAug a novel framework that
combines textguided diffusion-based generation with automatic segmentation
validation to address this challenge.</li>
<li>Evaluated on three
medical imaging benchmarks (CVC-ClinicDB, Kvasir-SEG, REFUGE2), our framework
achieves state-of-the-art performance with 8-10% Dice improvements over
baselines and reduces false negative rates by up to 28% for challenging cases
like small polyps and flat lesions critical for early detection in screening
applications.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="http://arxiv.org/pdf/2508.17844v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2508.17844v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2508.19167v1'></a></p>
<h2 id="beyond-flattening-a-geometrically-principled-positional-encoding-for-vision-transformers-with-weierstrass-elliptic-functions"><a href="https://arxiv.org/abs/2508.19167v1">Beyond flattening: a geometrically principled positional encoding for vision transformers with Weierstrass elliptic functions</a></h2>
<p><strong>Authors:</strong> Zhihang Xin, Xitong Hu, Rui Wang</p>
<p><strong>Published:</strong> 2025-08-26</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Vision Transformers have demonstrated remarkable success in computer vision
tasks, yet their reliance on learnable one-dimensional positional embeddings
fundamentally disrupts the inherent two-dimensional spatial structure of images
through patch flattening procedures. Traditional positional encoding approaches
lack geometric constraints and fail to establish monotonic correspondence
between Euclidean spatial distances and sequential index distances, thereby
limiting the model's capacity to leverage spatial proximity priors effectively.
We propose Weierstrass Elliptic Function Positional Encoding (WEF-PE), a
mathematically principled approach that directly addresses two-dimensional
coordinates through natural complex domain representation, where the doubly
periodic properties of elliptic functions align remarkably with translational
invariance patterns commonly observed in visual data. Our method exploits the
non-linear geometric nature of elliptic functions to encode spatial distance
relationships naturally, while the algebraic addition formula enables direct
derivation of relative positional information between arbitrary patch pairs
from their absolute encodings. Comprehensive experiments demonstrate that
WEF-PE achieves superior performance across diverse scenarios, including
63.78\% accuracy on CIFAR-100 from-scratch training with ViT-Tiny architecture,
93.28\% on CIFAR-100 fine-tuning with ViT-Base, and consistent improvements on
VTAB-1k benchmark tasks. Theoretical analysis confirms the distance-decay
property through rigorous mathematical proof, while attention visualization
reveals enhanced geometric inductive bias and more coherent semantic focus
compared to conventional approaches.The source code implementing the methods
described in this paper is publicly available on GitHub.</p>
<p><strong>Analysis:</strong></p>
<p>这篇论文提出了一种新颖的、基于数学原理的视觉Transformer位置编码方法，旨在解决现有方法在处理图像二维空间结构时的局限性。</p>
<hr />
<p><strong>1. 论文主要贡献的简洁总结 (2-3 句话)</strong></p>
<p>本文提出了一种名为Weierstrass椭圆函数位置编码（WEF-PE）的新方法，旨在解决Vision Transformer中通过补丁展平导致的二维空间信息丢失问题。WEF-PE利用Weierstrass椭圆函数在复数域中直接编码二维坐标，其双周期性与视觉数据的平移不变性相吻合，并通过代数加法公式直接推导相对位置信息。实验证明，WEF-PE显著提升了ViT在多种任务上的性能，并增强了几何归纳偏置。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<p>核心创新在于将<strong>Weierstrass椭圆函数</strong>引入到Vision Transformer的二维位置编码中。具体方法论包括：
*   <strong>直接处理二维坐标与复数域表示：</strong> 摒弃了将二维图像展平为一维序列的做法，而是直接在复数域中处理图像的二维坐标，这与传统的基于正弦/余弦或可学习的一维嵌入形成鲜明对比。
*   <strong>利用椭圆函数的双周期性：</strong> Weierstrass椭圆函数的双周期性与视觉数据中常见的平移不变性模式高度契合，为位置编码提供了强大的几何约束。
*   <strong>非线性几何编码空间距离：</strong> 椭圆函数的非线性几何特性能够更自然地编码空间距离关系，解决了传统方法无法在欧几里得空间距离和序列索引距离之间建立单调对应关系的问题。
*   <strong>代数加法公式推导相对位置：</strong> 论文指出，椭圆函数的代数加法公式可以直接从绝对位置编码中推导出任意补丁对之间的相对位置信息，这对于Transformer的注意力机制至关重要。</p>
<p><strong>3. 对领域潜在影响</strong></p>
<ul>
<li><strong>提升Vision Transformer的性能和鲁棒性：</strong> 通过引入更强的几何归纳偏置，WEF-PE有望显著提升ViT在各种计算机视觉任务上的性能，尤其是在需要精细空间理解的任务中。</li>
<li><strong>推动位置编码研究的新范式：</strong> 本文为位置编码的设计提供了一个新的、基于数学原理的视角，可能会启发研究者探索更多高级数学工具来解决深度学习中的结构化数据编码问题。</li>
<li><strong>增强模型的可解释性：</strong> 理论分析和注意力可视化表明，WEF-PE能带来更连贯的语义焦点和更强的几何归纳偏置，有助于理解ViT如何处理空间信息。</li>
<li><strong>为其他结构化数据编码提供借鉴：</strong> 这种利用特定数学函数特性来编码数据结构的思想，可能被推广到其他需要保留复杂结构信息的领域，例如图神经网络、3D点云处理等。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用</strong></p>
<ul>
<li><strong>图像分类、目标检测、语义分割：</strong> 作为ViT的基础组件，WEF-PE将直接提升这些核心计算机视觉任务的性能。</li>
<li><strong>医学图像分析：</strong> 在医学图像中，精确的空间关系和几何结构至关重要，WEF-PE有望提高诊断和分割的准确性。</li>
<li><strong>遥感图像处理：</strong> 遥感图像通常具有大尺度、重复模式和平移不变性，WEF-PE的双周期性特性可能在此类任务中表现出色。</li>
<li><strong>视频理解：</strong> 如果能将二维的WEF-PE扩展到三维（空间+时间），则可能对视频Transformer中的时空位置编码产生积极影响。</li>
<li><strong>图像生成与编辑：</strong> 更好的空间理解有助于生成更具几何一致性和真实感的图像。</li>
</ul>
<p><strong>5. 从摘要中可推断的局限性</strong></p>
<ul>
<li><strong>计算复杂性：</strong> Weierstrass椭圆函数在数学上较为复杂，其计算开销可能高于简单的可学习嵌入或正弦/余弦编码，摘要中未提及具体的计算效率对比。</li>
<li><strong>参数选择与学习：</strong> 椭圆函数通常涉及周期、模数等参数。摘要中未详细说明这些参数是如何确定（例如，固定、可学习或通过某种优化过程得到），这可能引入额外的复杂性或调优难度。</li>
<li><strong>通用性与扩展性：</strong> 论文强调了“双周期性”与2D图像的契合，但其直接应用于3D数据（如点云、体素）或非网格结构数据的能力和方法未在摘要中提及，可能需要进一步的理论和方法扩展。</li>
<li><strong>理论与实践的平衡：</strong> 尽管强调了“数学原理”，但在实际应用中，如何平衡理论的严谨性与工程实现的效率和鲁棒性，是所有复杂数学方法面临的挑战。</li>
</ul>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose Weierstrass Elliptic Function Positional Encoding (WEF-PE), a
mathematically principled approach that directly addresses two-dimensional
coordinates through natural complex domain representation, where the doubly
periodic properties of elliptic functions align remarkably with translational
invariance patterns commonly observed in visual data.</li>
<li>Our method exploits the
non-linear geometric nature of elliptic functions to encode spatial distance
relationships naturally, while the algebraic addition formula enables direct
derivation of relative positional information between arbitrary patch pairs
from their absolute encodings.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="http://arxiv.org/pdf/2508.19167v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2508.19167v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2508.18421v1'></a></p>
<h2 id="why-relational-graphs-will-save-the-next-generation-of-vision-foundation-models"><a href="https://arxiv.org/abs/2508.18421v1">Why Relational Graphs Will Save the Next Generation of Vision Foundation Models?</a></h2>
<p><strong>Authors:</strong> Fatemeh Ziaeetabar</p>
<p><strong>Published:</strong> 2025-08-25</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Vision foundation models (FMs) have become the predominant architecture in
computer vision, providing highly transferable representations learned from
large-scale, multimodal corpora. Nonetheless, they exhibit persistent
limitations on tasks that require explicit reasoning over entities, roles, and
spatio-temporal relations. Such relational competence is indispensable for
fine-grained human activity recognition, egocentric video understanding, and
multimodal medical image analysis, where spatial, temporal, and semantic
dependencies are decisive for performance. We advance the position that
next-generation FMs should incorporate explicit relational interfaces,
instantiated as dynamic relational graphs (graphs whose topology and edge
semantics are inferred from the input and task context). We illustrate this
position with cross-domain evidence from recent systems in human manipulation
action recognition and brain tumor segmentation, showing that augmenting FMs
with lightweight, context-adaptive graph-reasoning modules improves
fine-grained semantic fidelity, out of distribution robustness,
interpretability, and computational efficiency relative to FM only baselines.
Importantly, by reasoning sparsely over semantic nodes, such hybrids also
achieve favorable memory and hardware efficiency, enabling deployment under
practical resource constraints. We conclude with a targeted research agenda for
FM graph hybrids, prioritizing learned dynamic graph construction, multi-level
relational reasoning (e.g., part object scene in activity understanding, or
region organ in medical imaging), cross-modal fusion, and evaluation protocols
that directly probe relational competence in structured vision tasks.</p>
<p><strong>Analysis:</strong></p>
<p>这篇论文摘要提出了一种关于下一代视觉基础模型（Vision Foundation Models, FMs）发展方向的深刻见解。作为计算机视觉和机器学习领域的专家，我对该摘要的分析如下：</p>
<hr />
<h3 id="why-relational-graphs-will-save-the-next-generation-of-vision-foundation-models_1">论文摘要分析：Why Relational Graphs Will Save the Next Generation of Vision Foundation Models?</h3>
<p><strong>1. 论文核心贡献的简明总结 (Concise Summary of Main Contribution)</strong></p>
<p>论文指出当前视觉基础模型在需要实体、角色和时空关系显式推理的任务上存在局限。为解决此问题，论文提出下一代基础模型应整合动态关系图（dynamic relational graphs）作为显式关系接口。这种混合模型能显著提升细粒度语义理解、泛化能力、可解释性及计算效率，从而克服现有FMs的不足。</p>
<p><strong>2. 关键创新或方法论 (Key Innovation or Methodological Approach)</strong></p>
<p>核心创新在于提出将“动态关系图”（dynamic relational graphs）作为显式关系接口整合到视觉基础模型中。这些图的拓扑结构和边语义是根据输入和任务上下文动态推断的。通过这种轻量级、上下文自适应的图推理模块增强基础模型，实现了对实体、角色和时空关系的稀疏且高效的推理，从而弥补了FMs在复杂关系理解上的不足。</p>
<p><strong>3. 对领域潜在影响 (Potential Impact on the Field)</strong></p>
<p>这项研究有望推动视觉基础模型范式的演进，使其从主要依赖大规模数据学习隐式表示，转向能够进行显式、结构化推理。这将显著拓宽基础模型在需要复杂关系理解（如人类活动识别、医疗影像分析）领域的应用范围和性能上限。同时，提升模型的鲁棒性、可解释性和资源效率，对于基础模型在实际场景中的部署具有重要意义，可能引领下一代FMs的设计方向。</p>
<p><strong>4. 相关领域或应用 (Related Areas or Applications that Might Benefit)</strong></p>
<ul>
<li><strong>细粒度人类活动识别 (Fine-grained Human Activity Recognition):</strong> 理解复杂动作序列中的实体、工具、交互和时序关系。</li>
<li><strong>第一人称视角视频理解 (Egocentric Video Understanding):</strong> 分析佩戴者视角下的物体交互、意图和环境关系。</li>
<li><strong>多模态医学图像分析 (Multimodal Medical Image Analysis):</strong> 整合不同模态（如MRI、CT）信息，推理病灶、器官之间的空间、语义关系，例如脑肿瘤分割和疾病诊断。</li>
<li><strong>机器人操作与具身智能 (Robotic Manipulation and Embodied AI):</strong> 规划和执行复杂任务，需要理解物体属性、环境约束和操作序列。</li>
<li><strong>场景图生成与视觉问答 (Scene Graph Generation and Visual Question Answering):</strong> 更准确地捕捉图像中的实体及其关系，支持更深层次的语义理解和推理。</li>
</ul>
<p><strong>5. 从摘要中可推断的局限性 (Limitations that Can Be Inferred from the Abstract)</strong></p>
<ul>
<li><strong>性质为展望性或综述性 (Prospective/Review Nature):</strong> 鉴于发布日期是2025年，且摘要中使用了“We advance the position”、“We illustrate this position with cross-domain evidence”和“We conclude with a targeted research agenda”等表述，这篇论文更像是一篇提出研究方向、总结现有证据并规划未来路线图的展望性或综述性文章，而非一篇提出全新模型架构并提供大量新实验结果的实证论文。其“证据”可能来自对现有工作的综合分析，而非本文首次提出的新实验。</li>
<li><strong>动态图构建的复杂性 (Complexity of Dynamic Graph Construction):</strong> 摘要中强调了“learned dynamic graph construction”是未来的研究重点，这暗示了如何高效、准确地从原始输入和任务上下文中动态推断出最优的图拓扑和边语义，本身就是一个复杂且尚未完全解决的挑战。</li>
<li><strong>“轻量级”的定义与权衡 (Definition and Trade-offs of "Lightweight"):</strong> 尽管声称“lightweight”和“favorable memory and hardware efficiency”，但在实际应用中，图推理模块的复杂性、图的大小以及与基础模型融合的开销，仍可能带来额外的计算负担，需要仔细的工程优化和权衡。</li>
<li><strong>评估协议的缺失 (Lack of Established Evaluation Protocols):</strong> 摘要中提到需要“evaluation protocols that directly probe relational competence”，这表明目前可能缺乏标准化的、能够充分衡量模型关系推理能力的基准和评估方法，这会给研究进展带来挑战。</li>
</ul>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>Importantly, by reasoning sparsely over semantic nodes, such hybrids also
achieve favorable memory and hardware efficiency, enabling deployment under
practical resource constraints.</li>
<li>We conclude with a targeted research agenda for
FM graph hybrids, prioritizing learned dynamic graph construction, multi-level
relational reasoning (e.g., part object scene in activity understanding, or
region organ in medical imaging), cross-modal fusion, and evaluation protocols
that directly probe relational competence in structured vision tasks.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="http://arxiv.org/pdf/2508.18421v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2508.18421v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2508.17975v1'></a></p>
<h2 id="enhanced-drift-aware-computer-vision-architecture-for-autonomous-driving"><a href="https://arxiv.org/abs/2508.17975v1">Enhanced Drift-Aware Computer Vision Architecture for Autonomous Driving</a></h2>
<p><strong>Authors:</strong> Md Shahi Amran Hossain, Abu Shad Ahammed, Sayeri Mukherjee, Roman Obermaisser</p>
<p><strong>Published:</strong> 2025-08-25</p>
<p><strong>Categories:</strong> cs.CV, math.LO</p>
<p><strong>Abstract:</strong></p>
<p>The use of computer vision in automotive is a trending research in which
safety and security are a primary concern. In particular, for autonomous
driving, preventing road accidents requires highly accurate object detection
under diverse conditions. To address this issue, recently the International
Organization for Standardization (ISO) released the 8800 norm, providing
structured frameworks for managing associated AI relevant risks. However,
challenging scenarios such as adverse weather or low lighting often introduce
data drift, leading to degraded model performance and potential safety
violations. In this work, we present a novel hybrid computer vision
architecture trained with thousands of synthetic image data from the road
environment to improve robustness in unseen drifted environments. Our dual mode
framework utilized YOLO version 8 for swift detection and incorporated a
five-layer CNN for verification. The system functioned in sequence and improved
the detection accuracy by more than 90\% when tested with drift-augmented road
images. The focus was to demonstrate how such a hybrid model can provide better
road safety when working together in a hybrid structure.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将对这篇论文摘要进行技术性分析。</p>
<hr />
<h3 id="enhanced-drift-aware-computer-vision-architecture-for-autonomous-driving_1">论文摘要分析：Enhanced Drift-Aware Computer Vision Architecture for Autonomous Driving</h3>
<p><strong>1. 论文主要贡献的简明总结 (2-3 句话)</strong></p>
<p>这篇论文提出了一种新颖的混合计算机视觉架构，旨在解决自动驾驶中因数据漂移（如恶劣天气、低光照）导致的物体检测性能下降和潜在安全问题。该架构结合了YOLOv8进行快速检测和一个五层CNN进行验证，通过序列化工作流程和合成数据训练，显著提升了在漂移增强图像上的检测准确率，从而增强了道路安全性。</p>
<p><strong>2. 关键创新或方法学方法</strong></p>
<p>该研究的核心创新在于其<strong>双模态混合架构（dual mode hybrid architecture）</strong>和<strong>漂移感知（drift-aware）</strong>的设计理念。具体方法包括：
*   <strong>混合架构与序列化验证：</strong> 系统采用YOLOv8作为第一阶段，实现快速物体检测；随后，一个专门的五层卷积神经网络（CNN）作为第二阶段，对YOLOv8的检测结果进行验证。这种“快速检测 + 深度验证”的序列化工作流是其独特之处，旨在平衡速度与鲁棒性。
*   <strong>合成数据训练：</strong> 为了提高模型在“未见过的漂移环境（unseen drifted environments）”中的鲁棒性，该架构利用了数千张合成图像数据进行训练。这有助于模型学习并适应各种潜在的漂移条件，而无需依赖难以获取的真实世界漂移数据。
*   <strong>专注于数据漂移鲁棒性：</strong> 论文明确将数据漂移（由恶劣天气、低光照等引起）作为核心挑战，并设计架构来直接解决这一问题，这与ISO 8800等新兴标准对AI风险管理的关注相吻合。</p>
<p><strong>3. 对领域潜在影响</strong></p>
<ul>
<li><strong>提升自动驾驶安全性：</strong> 通过有效应对数据漂移，该研究有望显著提高自动驾驶系统在复杂和恶劣条件下的物体检测可靠性，直接关系到行车安全，并可能成为未来AD系统设计中的一个重要考量。</li>
<li><strong>推动混合模型范式：</strong> 这种“快速检测器 + 验证器”的混合架构为其他对实时性、准确性和鲁棒性都有高要求的计算机视觉应用提供了新的设计范式。</li>
<li><strong>强调合成数据价值：</strong> 论文再次证明了合成数据在训练鲁棒模型，尤其是在处理罕见或难以获取的漂移数据方面的巨大潜力。</li>
<li><strong>符合行业标准：</strong> 提及ISO 8800表明该研究具有很强的实际应用导向，其成果可能有助于自动驾驶系统满足未来的安全和风险管理标准。</li>
</ul>
<p><strong>4. 相关领域或应用</strong></p>
<p>除了自动驾驶，以下领域或应用也可能从这项研究中受益：
*   <strong>工业自动化与机器人：</strong> 在生产线或仓库中，光照、灰尘、磨损等环境变化可能导致数据漂移，影响机器人的视觉感知和操作精度。
*   <strong>智能监控系统：</strong> 户外监控摄像头常面临天气、昼夜、季节变化等漂移，该方法可用于提高异常事件检测的鲁棒性。
*   <strong>医疗影像分析：</strong> 不同的设备、扫描参数或患者生理变化可能引入数据漂移，影响疾病诊断的准确性。
*   <strong>航空航天与无人机：</strong> 在复杂大气条件或未知环境中执行任务时，视觉系统需要极高的鲁棒性。
*   <strong>任何在非受控环境中部署的计算机视觉系统：</strong> 凡是环境条件多变、数据分布可能随时间或条件变化的场景，这种漂移感知和验证的架构都具有借鉴意义。</p>
<p><strong>5. 从摘要中可推断的局限性</strong></p>
<ul>
<li><strong>合成数据与真实世界的差距（Domain Gap）：</strong> 尽管合成数据有助于解决漂移问题，但合成数据与真实世界数据之间通常存在领域差距。模型在“漂移增强的道路图像”上表现出色，但其在真实世界、未见过的、高度复杂的漂移条件下的泛化能力仍需进一步验证。</li>
<li><strong>性能指标的相对性：</strong> “检测准确率提高了90%以上”是一个相对提升。摘要未提供基线模型的绝对准确率，也未说明提升90%是基于哪个初始值。例如，从10%提升到19%也是90%的提升，但绝对性能可能仍不足以满足自动驾驶的需求。</li>
<li><strong>序列化处理的实时性考量：</strong> 尽管YOLOv8以“swift detection”著称，但增加一个五层CNN的“验证”步骤，必然会引入额外的计算延迟。对于自动驾驶这种对实时性要求极高的应用，这种延迟是否在可接受范围内，以及如何优化以满足实时性要求，是需要关注的问题。</li>
<li><strong>五层CNN的验证能力：</strong> 摘要中对“五层CNN”的描述相对简单。其验证机制、复杂度和对各种漂移模式的鲁棒性如何，以及它是否足以应对自动驾驶中可能出现的极端和多样化的漂移情况，仍有待详细说明。</li>
<li><strong>漂移类型的覆盖范围：</strong> 摘要提到了“恶劣天气或低光照”作为漂移来源，但未详细说明模型能够应对的漂移类型和程度。例如，对于传感器故障、遮挡、对抗性攻击等其他形式的“漂移”或异常情况，该架构的有效性如何？</li>
</ul>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this work, we present a novel hybrid computer vision
architecture trained with thousands of synthetic image data from the road
environment to improve robustness in unseen drifted environments.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="http://arxiv.org/pdf/2508.17975v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2508.17975v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2508.18249v1'></a></p>
<h2 id="scene-agnostic-traversability-labeling-and-estimation-via-a-multimodal-self-supervised-framework"><a href="https://arxiv.org/abs/2508.18249v1">Scene-Agnostic Traversability Labeling and Estimation via a Multimodal Self-supervised Framework</a></h2>
<p><strong>Authors:</strong> Zipeng Fang, Yanbo Wang, Lei Zhao, Weidong Chen</p>
<p><strong>Published:</strong> 2025-08-25</p>
<p><strong>Categories:</strong> cs.RO, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Traversability estimation is critical for enabling robots to navigate across
diverse terrains and environments. While recent self-supervised learning
methods achieve promising results, they often fail to capture the
characteristics of non-traversable regions. Moreover, most prior works
concentrate on a single modality, overlooking the complementary strengths
offered by integrating heterogeneous sensory modalities for more robust
traversability estimation. To address these limitations, we propose a
multimodal self-supervised framework for traversability labeling and
estimation. First, our annotation pipeline integrates footprint, LiDAR, and
camera data as prompts for a vision foundation model, generating traversability
labels that account for both semantic and geometric cues. Then, leveraging
these labels, we train a dual-stream network that jointly learns from different
modalities in a decoupled manner, enhancing its capacity to recognize diverse
traversability patterns. In addition, we incorporate sparse LiDAR-based
supervision to mitigate the noise introduced by pseudo labels. Finally,
extensive experiments conducted across urban, off-road, and campus environments
demonstrate the effectiveness of our approach. The proposed automatic labeling
method consistently achieves around 88% IoU across diverse datasets. Compared
to existing self-supervised state-of-the-art methods, our multimodal
traversability estimation network yields consistently higher IoU, improving by
1.6-3.5% on all evaluated datasets.</p>
<p><strong>Analysis:</strong></p>
<p>这篇论文提出了一种新颖的多模态自监督框架，用于机器人可通行性区域的标注与估计，旨在解决现有方法在识别不可通行区域和有效利用多模态信息方面的不足。</p>
<hr />
<h3 id="1-concise-summary_1">1. 论文核心贡献总结 (Concise Summary)</h3>
<p>该论文提出了一种多模态自监督框架，用于机器人可通行性区域的标注与估计，旨在解决现有方法在识别不可通行区域和利用多模态信息方面的不足。它通过整合足迹、LiDAR和相机数据作为视觉基础模型的提示来生成高质量的伪标签，并利用这些标签训练一个双流网络，辅以稀疏LiDAR监督以提高鲁棒性，最终在多样化环境中实现了显著的性能提升。</p>
<h3 id="2-key-innovation-or-methodological-approach_1">2. 关键创新或方法论 (Key Innovation or Methodological Approach)</h3>
<p>核心创新在于其多模态自监督框架，特别是在伪标签生成和网络训练阶段。
1.  <strong>创新性伪标签生成：</strong> 该方法创新性地利用足迹（footprint）、LiDAR和相机数据作为提示（prompts），驱动一个<strong>视觉基础模型</strong>来生成结合语义和几何线索的可通行性标签。这是一种高效且场景无关的伪标签生成策略，尤其解决了传统自监督方法难以捕捉不可通行区域特征的问题。
2.  <strong>解耦式多模态学习网络：</strong> 提出的双流网络以<strong>解耦方式</strong>联合学习不同模态的特征，增强了对多样化可通行性模式的识别能力，充分利用了异构传感器的互补优势。
3.  <strong>稀疏LiDAR监督缓解噪声：</strong> 引入<strong>稀疏LiDAR监督</strong>来有效缓解伪标签带来的噪声，进一步提升了模型的鲁棒性和准确性，弥补了纯自监督方法可能存在的误差。</p>
<h3 id="3-potential-impact-on-the-field_1">3. 对领域潜在影响 (Potential Impact on the Field)</h3>
<p>该研究对机器人导航领域具有重要影响，它提供了一种更鲁棒、更通用的可通行性估计方法，使机器人在未知和复杂地形中自主导航的能力得到显著提升。其自监督的伪标签生成策略，特别是结合视觉基础模型和多模态提示，为减少数据标注成本开辟了新途径，对计算机视觉领域中其他需要大量标注的感知任务具有借鉴意义。此外，它也为多模态数据融合在机器人感知中的应用树立了新的范例。</p>
<h3 id="4-related-areas-or-applications_1">4. 相关领域或应用 (Related Areas or Applications)</h3>
<p>这项研究的成果可以广泛应用于以下领域：
*   <strong>自动驾驶与无人车：</strong> 特别是在非结构化道路、越野环境或复杂城市区域的路径规划和决策。
*   <strong>搜救机器人：</strong> 在灾后废墟、崎岖地形中进行搜索和救援任务。
*   <strong>空间探索与军事机器人：</strong> 在未知或极端环境中进行自主探测和行动。
*   <strong>农业机器人：</strong> 在农田中识别可通行区域，进行作物监测和管理。
*   <strong>物流与配送机器人：</strong> 提升在多样化城市和郊区环境中“最后一公里”配送的鲁棒性。</p>
<h3 id="5-inferred-limitations">5. 从摘要中可推断的局限性 (Inferred Limitations)</h3>
<p>尽管该方法取得了显著成果，但从摘要中仍可推断出一些潜在局限性：
*   <strong>伪标签质量的依赖性：</strong> 伪标签的生成质量高度依赖于所使用的“视觉基础模型”的性能及其对多模态提示的理解能力。如果基础模型在特定场景或物体上表现不佳，生成的伪标签可能会引入系统性误差，从而影响后续网络的训练效果。
*   <strong>“解耦”学习的权衡：</strong> 摘要提到双流网络以“解耦方式”学习不同模态，这可能有助于提取模态特定特征，但也可能在一定程度上限制了不同模态之间更深层次、更复杂的交互和信息融合，从而影响最终的决策鲁棒性。
*   <strong>稀疏LiDAR监督的性质：</strong> 尽管稀疏LiDAR监督用于缓解伪标签噪声，但其具体来源（是完全自动生成还是需要少量人工干预）和稀疏程度对模型性能的影响，以及其在不同场景下的泛化能力，仍需进一步探讨。
*   <strong>计算资源需求：</strong> 整合多模态数据、利用基础模型以及训练双流网络，可能对计算资源（训练和推理）有较高要求，这对于资源受限的机器人平台可能是一个挑战。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address these limitations, we propose a
multimodal self-supervised framework for traversability labeling and
estimation.</li>
<li>Finally,
extensive experiments conducted across urban, off-road, and campus environments
demonstrate the effectiveness of our approach.</li>
<li>Compared
to existing self-supervised state-of-the-art methods, our multimodal
traversability estimation network yields consistently higher IoU, improving by
1.6-3.5% on all evaluated datasets.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="http://arxiv.org/pdf/2508.18249v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2508.18249v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2508.17930v1'></a></p>
<h2 id="learning-to-detect-label-errors-by-making-them-a-method-for-segmentation-and-object-detection-datasets"><a href="https://arxiv.org/abs/2508.17930v1">Learning to Detect Label Errors by Making Them: A Method for Segmentation and Object Detection Datasets</a></h2>
<p><strong>Authors:</strong> Sarina Penquitt, Tobias Riedlinger, Timo Heller, Markus Reischl, Matthias Rottmann</p>
<p><strong>Published:</strong> 2025-08-25</p>
<p><strong>Categories:</strong> cs.LG, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Recently, detection of label errors and improvement of label quality in
datasets for supervised learning tasks has become an increasingly important
goal in both research and industry. The consequences of incorrectly annotated
data include reduced model performance, biased benchmark results, and lower
overall accuracy. Current state-of-the-art label error detection methods often
focus on a single computer vision task and, consequently, a specific type of
dataset, containing, for example, either bounding boxes or pixel-wise
annotations. Furthermore, previous methods are not learning-based. In this
work, we overcome this research gap. We present a unified method for detecting
label errors in object detection, semantic segmentation, and instance
segmentation datasets. In a nutshell, our approach - learning to detect label
errors by making them - works as follows: we inject different kinds of label
errors into the ground truth. Then, the detection of label errors, across all
mentioned primary tasks, is framed as an instance segmentation problem based on
a composite input. In our experiments, we compare the label error detection
performance of our method with various baselines and state-of-the-art
approaches of each task's domain on simulated label errors across multiple
tasks, datasets, and base models. This is complemented by a generalization
study on real-world label errors. Additionally, we release 459 real label
errors identified in the Cityscapes dataset and provide a benchmark for real
label error detection in Cityscapes.</p>
<p><strong>Analysis:</strong></p>
<p>作为计算机视觉和机器学习领域的专家，我对这篇论文摘要进行如下分析：</p>
<hr />
<h3 id="learning-to-detect-label-errors-by-making-them-a-method-for-segmentation-and-object-detection-datasets_1">论文摘要分析：Learning to Detect Label Errors by Making Them: A Method for Segmentation and Object Detection Datasets</h3>
<p><strong>1. 论文主要贡献的简洁总结 (2-3 句话)</strong></p>
<p>这篇论文提出了一种统一的、基于学习的方法，用于检测目标检测、语义分割和实例分割数据集中存在的标签错误。其核心思想是通过向真值中注入不同类型的合成错误来训练一个错误检测模型，并将错误检测任务建模为一个基于复合输入的实例分割问题。该方法旨在克服现有方法仅限于单一任务且非学习的局限性，并通过真实世界错误和基准测试来验证其泛化能力。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<ul>
<li><strong>统一的、基于学习的方法：</strong> 克服了现有方法通常专注于单一计算机视觉任务（如仅边界框或像素级标注）且非学习的局限性，提供了一个可同时处理目标检测、语义分割和实例分割的通用框架。</li>
<li><strong>“通过制造错误来学习检测错误” (Learning by Making Them)：</strong> 这是一个巧妙的自监督或数据增强策略。通过系统地向真值中注入不同类型的标签错误，为错误检测模型生成训练数据，从而避免了对大量真实错误标注数据的依赖。</li>
<li><strong>将错误检测框架为实例分割问题：</strong> 这种方法能够精确地定位和识别图像中不同类型的标签错误区域，将其视为独立的“错误实例”，这比简单的分类或检测框方法更具表现力。</li>
<li><strong>复合输入 (Composite Input)：</strong> 虽然摘要未详细说明，但暗示了通过结合原始图像信息和可能包含错误信息的表示来训练模型，以提高错误检测的鲁棒性。</li>
</ul>
<p><strong>3. 对领域潜在影响</strong></p>
<ul>
<li><strong>提升数据集质量和模型性能：</strong> 自动化、统一的标签错误检测将显著提高计算机视觉数据集的质量，从而训练出更鲁棒、性能更好的模型，并减少因数据错误导致的模型偏差。</li>
<li><strong>更可靠的基准测试：</strong> 减少数据集中的错误将使基准测试结果更加公平和可信，促进更有效的模型比较和研究进展。</li>
<li><strong>降低数据标注成本和时间：</strong> 自动化错误检测可以大幅减少人工质量控制的需求，降低数据集创建和维护的成本与时间。</li>
<li><strong>推动数据中心AI发展：</strong> 该研究与当前“数据中心AI”的趋势高度契合，强调了数据质量在机器学习中的核心作用。</li>
<li><strong>社区资源贡献：</strong> 论文发布了 Cityscapes 数据集中识别出的 459 个真实标签错误，并提供了真实标签错误检测的基准，这将为未来的研究提供宝贵的资源和评估标准。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用</strong></p>
<ul>
<li><strong>自动驾驶：</strong> 自动驾驶数据集庞大且对精度要求极高，标签错误可能导致严重后果。该方法可用于提高自动驾驶感知数据集的质量。</li>
<li><strong>医学影像分析：</strong> 医学图像标注的准确性直接关系到诊断和治疗，错误检测对于确保模型可靠性至关重要。</li>
<li><strong>大规模数据集提供商/标注服务：</strong> 专门从事数据集创建和质量控制的公司将直接受益于这种自动化工具。</li>
<li><strong>机器人感知：</strong> 机器人需要准确的环境感知来执行任务，高质量的训练数据是基础。</li>
<li><strong>任何依赖监督学习的工业应用：</strong> 凡是使用大量标注数据进行模型训练的行业（如零售、安防、农业等）都将从更可靠的数据质量中获益。</li>
<li><strong>主动学习 (Active Learning) 和数据策展 (Data Curation)：</strong> 识别出错误样本可以指导重新标注或优先处理，优化数据利用效率。</li>
</ul>
<p><strong>5. 从摘要中可推断的局限性</strong></p>
<ul>
<li><strong>合成错误与真实错误的差距：</strong> 尽管论文提到了对真实世界错误的泛化研究，但合成错误是否能完全覆盖所有类型、所有细微之处的真实世界错误，以及模型对未见过的真实错误类型的检测能力，仍是一个潜在的挑战。</li>
<li><strong>“复合输入”的细节：</strong> 摘要中未详细说明“复合输入”的具体构成，其设计对方法的有效性和泛化能力可能至关重要。</li>
<li><strong>计算成本：</strong> 训练一个基于学习的错误检测模型，特别是对于大规模数据集和复杂的实例分割任务，可能需要显著的计算资源和时间，摘要中未提及这方面的考量。</li>
<li><strong>错误类型的粒度：</strong> 摘要中提到“不同种类的标签错误”，但未具体说明能检测到哪些粒度的错误（例如，是明显的几何错误，还是更细微的语义不一致性）。</li>
<li><strong>错误修正机制：</strong> 论文专注于错误检测，但未提及如何将检测到的错误有效地反馈给标注员进行修正，或是否提供自动修正的建议。</li>
</ul>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Current state-of-the-art label error detection methods often
focus on a single computer vision task and, consequently, a specific type of
dataset, containing, for example, either bounding boxes or pixel-wise
annotations.</li>
<li>We present a unified method for detecting
label errors in object detection, semantic segmentation, and instance
segmentation datasets.</li>
<li>In a nutshell, our approach - learning to detect label
errors by making them - works as follows: we inject different kinds of label
errors into the ground truth.</li>
<li>In our experiments, we compare the label error detection
performance of our method with various baselines and state-of-the-art
approaches of each task's domain on simulated label errors across multiple
tasks, datasets, and base models.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="http://arxiv.org/pdf/2508.17930v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2508.17930v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2508.17708v1'></a></p>
<h2 id="catformer-contrastive-adversarial-transformer-for-image-super-resolution"><a href="https://arxiv.org/abs/2508.17708v1">CATformer: Contrastive Adversarial Transformer for Image Super-Resolution</a></h2>
<p><strong>Authors:</strong> Qinyi Tian, Spence Cox, Laura E. Dalton</p>
<p><strong>Published:</strong> 2025-08-25</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Super-resolution remains a promising technique to enhance the quality of
low-resolution images. This study introduces CATformer (Contrastive Adversarial
Transformer), a novel neural network integrating diffusion-inspired feature
refinement with adversarial and contrastive learning. CATformer employs a
dual-branch architecture combining a primary diffusion-inspired transformer,
which progressively refines latent representations, with an auxiliary
transformer branch designed to enhance robustness to noise through learned
latent contrasts. These complementary representations are fused and decoded
using deep Residual-in-Residual Dense Blocks for enhanced reconstruction
quality. Extensive experiments on benchmark datasets demonstrate that CATformer
outperforms recent transformer-based and diffusion-inspired methods both in
efficiency and visual image quality. This work bridges the performance gap
among transformer-, diffusion-, and GAN-based methods, laying a foundation for
practical applications of diffusion-inspired transformers in super-resolution.</p>
<p><strong>Analysis:</strong></p>
<p>作为计算机视觉和机器学习领域的专家，我对这篇关于CATformer的论文摘要进行如下分析：</p>
<hr />
<h3 id="catformer-contrastive-adversarial-transformer-for-image-super-resolution_1">CATformer: Contrastive Adversarial Transformer for Image Super-Resolution</h3>
<p><strong>1. 论文主要贡献的简洁总结 (2-3 句话)</strong></p>
<p>CATformer是一种新颖的图像超分辨率神经网络，它巧妙地将受扩散模型启发的特征细化、对抗性学习和对比学习集成到一个双分支Transformer架构中。该模型通过渐进式潜在表示细化和增强对噪声的鲁棒性，在效率和视觉质量上均超越了现有的Transformer和扩散启发方法。这项工作为超分辨率领域提供了一个强大的混合范式解决方案，并为扩散启发式Transformer的实际应用奠定了基础。</p>
<p><strong>2. 关键创新或方法论方法</strong></p>
<p>CATformer的核心创新在于其独特的<strong>双分支Transformer架构</strong>以及对<strong>多范式学习的深度融合</strong>。具体而言：
*   <strong>双分支架构：</strong> 包含一个<strong>主扩散启发式Transformer分支</strong>，负责对潜在表示进行渐进式细化，借鉴了扩散模型逐步去噪和生成高质量图像的核心思想。
*   <strong>辅助对比学习分支：</strong> 另一个<strong>辅助Transformer分支</strong>通过学习到的潜在对比来增强模型对噪声的鲁棒性，这是一种新颖的利用对比学习来解决超分辨率中常见噪声问题的策略。
*   <strong>多范式融合：</strong> 将<strong>扩散启发</strong>（用于特征细化）、<strong>对抗性学习</strong>（用于生成真实感图像）和<strong>对比学习</strong>（用于噪声鲁棒性）这三种强大的学习范式巧妙地集成在一个Transformer框架内，并通过Residual-in-Residual Dense Blocks进行融合和解码，以实现卓越的重建质量。</p>
<p><strong>3. 对该领域的潜在影响</strong></p>
<ul>
<li><strong>性能新标杆：</strong> CATformer声称在效率和视觉质量上均超越了现有的Transformer和扩散启发方法，这可能为图像超分辨率领域树立新的性能基准。</li>
<li><strong>弥合方法论鸿沟：</strong> 该研究明确指出其工作“弥合了Transformer、扩散模型和GAN这三种方法之间的性能差距”，这表明它成功地结合了各家之长，克服了单一方法的局限性，可能引领未来超分辨率乃至更广泛图像生成任务的混合架构设计趋势。</li>
<li><strong>推动扩散启发模型的实用化：</strong> 扩散模型虽然生成质量高，但通常推理速度较慢。CATformer通过“扩散启发”而非完整扩散模型的方式，在保持高质量的同时提升了效率，为将扩散模型的强大能力引入实际应用场景提供了可行路径。</li>
<li><strong>增强模型鲁棒性：</strong> 引入对比学习以增强对噪声的鲁棒性，对于真实世界中低质量、含噪图像的超分辨率具有重要意义，提升了模型的实用价值。</li>
</ul>
<p><strong>4. 可能受益于这项研究的相关领域或应用</strong></p>
<ul>
<li><strong>通用图像恢复与增强：</strong> 除了超分辨率，其对噪声的鲁棒性和高质量重建能力可能直接应用于图像去噪、去模糊、图像修复等任务。</li>
<li><strong>医学影像：</strong> 提高低分辨率医学扫描图像（如MRI、CT）的细节，辅助医生进行更精确的诊断。</li>
<li><strong>遥感与安防监控：</strong> 提升卫星图像、无人机航拍图或监控录像的清晰度，便于目标识别、态势感知和事件分析。</li>
<li><strong>计算摄影与视频处理：</strong> 改善消费级设备拍摄的低质量照片和视频，实现更清晰的放大和细节恢复。</li>
<li><strong>数字内容创作与娱乐：</strong> 提升老旧照片、视频的画质，或在游戏、VR/AR等场景中实现实时高质量渲染。</li>
<li><strong>其他条件图像生成任务：</strong> 其融合多种学习范式和架构的思路，可能为其他条件图像生成任务（如文本到图像、图像到图像转换）提供新的设计灵感。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的任何局限性</strong></p>
<ul>
<li><strong>架构复杂性与训练成本：</strong> 双分支Transformer架构结合三种学习范式（扩散启发、对抗、对比）以及RIRDB，意味着模型可能非常复杂，训练难度大，对计算资源的需求高，且超参数调优可能具有挑战性。</li>
<li><strong>“扩散启发”的深度：</strong> 论文强调“扩散启发”的特征细化，而非完整的扩散模型。这可能意味着它在生成多样性或处理极端低质量输入方面的能力，可能不如完整的扩散生成模型。其“渐进式细化”的具体机制和效果与完整扩散模型的差异，有待正文揭示。</li>
<li><strong>噪声鲁棒性的具体范围：</strong> 摘要提到通过学习到的潜在对比增强对噪声的鲁棒性，但未具体说明能处理的噪声类型（如高斯噪声、真实世界噪声、传感器噪声）和强度范围。在面对高度复杂或非典型噪声时，其表现如何仍是未知。</li>
<li><strong>“效率”的量化：</strong> 尽管声称在效率上优于现有方法，但具体的量化数据（如推理时间、参数量、FLOPs）在摘要中缺失，无法判断其“效率”的绝对水平，尤其是在与轻量级模型对比时。</li>
<li><strong>泛化能力：</strong> 尽管在基准数据集上表现出色，但在面对未见过的高度复杂或特定领域的真实世界低分辨率图像时，其泛化能力仍需进一步验证。例如，对于特定纹理、光照条件或内容（如人脸、文本）的超分辨率效果。</li>
</ul>
<p><strong>Key Findings:</strong></p>
<ul>
<li>This study introduces CATformer (Contrastive Adversarial
Transformer), a novel neural network integrating diffusion-inspired feature
refinement with adversarial and contrastive learning.</li>
<li>Extensive experiments on benchmark datasets demonstrate that CATformer
outperforms recent transformer-based and diffusion-inspired methods both in
efficiency and visual image quality.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="http://arxiv.org/pdf/2508.17708v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2508.17708v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2508.19003v1'></a></p>
<h2 id="roofseg-an-edge-aware-transformer-based-network-for-end-to-end-roof-plane-segmentation"><a href="https://arxiv.org/abs/2508.19003v1">RoofSeg: An edge-aware transformer-based network for end-to-end roof plane segmentation</a></h2>
<p><strong>Authors:</strong> Siyuan You, Guozheng Xu, Pengwei Zhou, Qiwen Jin, Jian Yao, Li Li</p>
<p><strong>Published:</strong> 2025-08-26</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>Roof plane segmentation is one of the key procedures for reconstructing
three-dimensional (3D) building models at levels of detail (LoD) 2 and 3 from
airborne light detection and ranging (LiDAR) point clouds. The majority of
current approaches for roof plane segmentation rely on the manually designed or
learned features followed by some specifically designed geometric clustering
strategies. Because the learned features are more powerful than the manually
designed features, the deep learning-based approaches usually perform better
than the traditional approaches. However, the current deep learning-based
approaches have three unsolved problems. The first is that most of them are not
truly end-to-end, the plane segmentation results may be not optimal. The second
is that the point feature discriminability near the edges is relatively low,
leading to inaccurate planar edges. The third is that the planar geometric
characteristics are not sufficiently considered to constrain the network
training. To solve these issues, a novel edge-aware transformer-based network,
named RoofSeg, is developed for segmenting roof planes from LiDAR point clouds
in a truly end-to-end manner. In the RoofSeg, we leverage a transformer
encoder-decoder-based framework to hierarchically predict the plane instance
masks with the use of a set of learnable plane queries. To further improve the
segmentation accuracy of edge regions, we also design an Edge-Aware Mask Module
(EAMM) that sufficiently incorporates planar geometric prior of edges to
enhance its discriminability for plane instance mask refinement. In addition,
we propose an adaptive weighting strategy in the mask loss to reduce the
influence of misclassified points, and also propose a new plane geometric loss
to constrain the network training.</p>
<p><strong>Analysis:</strong></p>
<p>作为计算机视觉和机器学习领域的专家，我对这篇关于RoofSeg的论文摘要进行了深入分析：</p>
<hr />
<h3 id="1-concise-summary_2">1. 论文主要贡献总结 (Concise summary)</h3>
<p>本文提出了一种名为RoofSeg的边缘感知Transformer网络，旨在实现从LiDAR点云中端到端的屋顶平面分割。它通过结合Transformer编码器-解码器架构、专门设计的边缘感知掩码模块（EAMM）以及新的几何损失函数，有效解决了现有深度学习方法在非端到端、边缘分割精度低和缺乏几何约束等方面的挑战。</p>
<h3 id="2-key-innovation-or-methodological-approach_2">2. 关键创新点或方法学 (Key innovation or methodological approach)</h3>
<ul>
<li><strong>端到端Transformer架构：</strong> 首次将Transformer编码器-解码器框架应用于屋顶平面分割，通过可学习的平面查询（learnable plane queries）直接预测平面实例掩码，实现了真正的端到端（end-to-end）分割，避免了后处理的次优性。</li>
<li><strong>边缘感知掩码模块（EAMM）：</strong> 针对边缘区域特征判别力低的问题，设计了EAMM，该模块充分融入了平面的几何先验知识，以显著提升边缘区域的分割精度和鲁棒性。</li>
<li><strong>综合损失函数设计：</strong> 提出了一种自适应加权策略的掩码损失，以降低误分类点的影响；同时引入了新的平面几何损失，用于在训练过程中显式地约束网络学习，确保分割结果的几何一致性。</li>
</ul>
<h3 id="3-potential-impact-on-the-field_2">3. 对领域潜在影响 (Potential impact on the field)</h3>
<ul>
<li><strong>提升3D建筑模型重建精度：</strong> 通过提供高精度的屋顶平面分割，特别是对边缘区域的精确处理，将直接提升LoD 2和LoD 3级别3D建筑模型的重建质量和自动化水平。</li>
<li><strong>推动点云语义/实例分割发展：</strong> 引入Transformer架构和几何先验约束，为点云数据中复杂几何体的端到端分割提供了一个新的范式，可能启发其他结构化场景（如室内、工业部件）的分割方法。</li>
<li><strong>简化工作流程：</strong> 真正的端到端方法减少了对复杂后处理的依赖，简化了从原始LiDAR点云到结构化屋顶模型的整个流程，提高了效率。</li>
</ul>
<h3 id="4-related-areas-or-applications-that-might-benefit-from-this-research">4. 相关应用领域 (Related areas or applications that might benefit from this research)</h3>
<ul>
<li><strong>3D城市建模与数字孪生：</strong> 高精度屋顶模型是构建精细化城市数字孪生和地理信息系统（GIS）的基础。</li>
<li><strong>智慧城市应用：</strong> 例如，屋顶太阳能板安装潜力评估、城市热岛效应分析、建筑能耗模拟等。</li>
<li><strong>灾害评估与管理：</strong> 快速准确地评估屋顶在自然灾害（如地震、飓风）后的损坏情况。</li>
<li><strong>自动驾驶与机器人导航：</strong> 帮助车辆和机器人更好地理解和感知周围的建筑环境。</li>
<li><strong>建筑信息模型（BIM）与测绘：</strong> 为建筑设计、施工和维护提供精确的几何数据。</li>
</ul>
<h3 id="5-any-limitations-that-can-be-inferred-from-the-abstract">5. 可推断的局限性 (Any limitations that can be inferred from the abstract)</h3>
<ul>
<li><strong>计算资源需求：</strong> Transformer模型通常具有较高的计算复杂度和内存消耗，尤其是在处理大规模点云数据时，训练和推理时间可能较长。</li>
<li><strong>对LiDAR点云质量的依赖：</strong> 尽管声称“边缘感知”，但LiDAR点云的密度、噪声和扫描角度仍可能影响边缘区域的精度，特别是在点云稀疏或存在遮挡的区域。</li>
<li><strong>复杂屋顶结构与泛化性：</strong> 抽象中未提及模型对极端复杂、非标准或具有大量附属物（如烟囱、天窗、植被覆盖）的屋顶结构的泛化能力。</li>
<li><strong>几何先验的定义与适用性：</strong> “平面几何先验”的具体实现方式和其在各种屋顶类型（如曲面屋顶）上的适用性尚不明确。如果先验过于刚性，可能会限制模型的灵活性。</li>
<li><strong>数据标注成本：</strong> 端到端训练通常需要大量的精确标注数据，特别是对于边缘区域和平面实例的标注，这可能是一个挑战。</li>
</ul>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To solve these issues, a novel edge-aware transformer-based network,
named RoofSeg, is developed for segmenting roof planes from LiDAR point clouds
in a truly end-to-end manner.</li>
<li>In addition,
we propose an adaptive weighting strategy in the mask loss to reduce the
influence of misclassified points, and also propose a new plane geometric loss
to constrain the network training.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="http://arxiv.org/pdf/2508.19003v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2508.19003v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2508.17916v1'></a></p>
<h2 id="endoufm-utilizing-foundation-models-for-monocular-depth-estimation-of-endoscopic-images"><a href="https://arxiv.org/abs/2508.17916v1">EndoUFM: Utilizing Foundation Models for Monocular depth estimation of endoscopic images</a></h2>
<p><strong>Authors:</strong> Xinning Yao, Bo Liu, Bojian Li, Jingjing Wang, Jinghua Yue, Fugen Zhou</p>
<p><strong>Published:</strong> 2025-08-25</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Depth estimation is a foundational component for 3D reconstruction in
minimally invasive endoscopic surgeries. However, existing monocular depth
estimation techniques often exhibit limited performance to the varying
illumination and complex textures of the surgical environment. While powerful
visual foundation models offer a promising solution, their training on natural
images leads to significant domain adaptability limitations and semantic
perception deficiencies when applied to endoscopy. In this study, we introduce
EndoUFM, an unsupervised monocular depth estimation framework that innovatively
integrating dual foundation models for surgical scenes, which enhance the depth
estimation performance by leveraging the powerful pre-learned priors. The
framework features a novel adaptive fine-tuning strategy that incorporates
Random Vector Low-Rank Adaptation (RVLoRA) to enhance model adaptability, and a
Residual block based on Depthwise Separable Convolution (Res-DSC) to improve
the capture of fine-grained local features. Furthermore, we design a
mask-guided smoothness loss to enforce depth consistency within anatomical
tissue structures. Extensive experiments on the SCARED, Hamlyn, SERV-CT, and
EndoNeRF datasets confirm that our method achieves state-of-the-art performance
while maintaining an efficient model size. This work contributes to augmenting
surgeons' spatial perception during minimally invasive procedures, thereby
enhancing surgical precision and safety, with crucial implications for
augmented reality and navigation systems.</p>
<p><strong>Analysis:</strong></p>
<p>作为计算机视觉和机器学习领域的专家，我对这篇关于EndoUFM的论文摘要进行了深入分析：</p>
<hr />
<h3 id="endoufm-utilizing-foundation-models-for-monocular-depth-estimation-of-endoscopic-images_1">EndoUFM: Utilizing Foundation Models for Monocular depth estimation of endoscopic images</h3>
<p><strong>1. 论文核心贡献总结 (Concise Summary)</strong></p>
<p>本文提出了EndoUFM，一个针对内窥镜图像的无监督单目深度估计算法。它创新性地整合了双视觉基础模型，并通过自适应微调策略（RVLoRA）、残差深度可分离卷积块（Res-DSC）以及掩膜引导平滑损失，有效克服了基础模型在内窥镜领域存在的域适应和语义感知问题，实现了最先进的深度估计性能。该方法旨在增强外科医生的空间感知能力，提高微创手术的精确性和安全性。</p>
<p><strong>2. 关键创新或方法学方法 (Key Innovation or Methodological Approach)</strong></p>
<p>EndoUFM的核心创新在于其<strong>双基础模型的创新性集成</strong>，旨在利用预训练的强大先验知识来解决内窥镜图像深度估计的挑战。为解决基础模型在内窥镜领域存在的显著域适应性限制和语义感知缺陷，它引入了以下关键方法：</p>
<ul>
<li><strong>基于随机向量低秩适应（RVLoRA）的自适应微调策略</strong>：这是一种高效的模型适应技术，允许在不完全重新训练整个大型基础模型的情况下，通过少量可训练参数来增强模型对内窥镜图像的适应性。</li>
<li><strong>基于深度可分离卷积的残差块（Res-DSC）</strong>：这种设计旨在更有效地捕获内窥镜图像中精细的局部特征，这对于区分复杂组织结构和微小病变至关重要。</li>
<li><strong>掩膜引导的平滑损失（Mask-guided smoothness loss）</strong>：通过引入解剖组织掩膜来指导深度图的平滑性，确保在同一组织结构内部的深度一致性，避免跨边界的模糊或不准确。</li>
</ul>
<p><strong>3. 对领域潜在影响 (Potential Impact on the Field)</strong></p>
<p>这项研究对微创手术领域具有显著影响。通过提供高精度的单目深度估计，它能<strong>显著增强外科医生在手术过程中的空间感知能力</strong>，从而<strong>提高手术的精确性和安全性</strong>。此外，其成果对于<strong>增强现实（AR）手术导航系统</strong>和<strong>术中三维重建</strong>至关重要，有望推动这些技术的临床应用。从更广泛的计算机视觉角度看，它为<strong>将通用视觉基础模型成功迁移并适应到特定、复杂且数据稀缺的医疗图像领域</strong>提供了有效范式，展示了基础模型在专业领域应用的巨大潜力。</p>
<p><strong>4. 相关领域或应用 (Related Areas or Applications that might benefit from this research)</strong></p>
<ul>
<li><strong>微创内窥镜手术（MIS）</strong>：直接应用，用于术中三维重建、目标定位、病灶测量和手术器械跟踪。</li>
<li><strong>增强现实（AR）/混合现实（MR）手术导航</strong>：将虚拟信息（如规划路径、病灶边界、重要结构）精确叠加到真实手术视野中，提供实时引导。</li>
<li><strong>机器人辅助手术</strong>：为手术机器人提供更准确的环境感知和避障能力，提高自动化水平。</li>
<li><strong>手术模拟与训练</strong>：创建更真实的三维手术场景，提高医学生和外科医生的培训效果。</li>
<li><strong>医疗图像分析与诊断</strong>：为其他基于三维信息的分析任务（如肿瘤体积测量、器官形变分析）提供基础数据。</li>
<li><strong>通用基础模型在特定领域的适应性研究</strong>：为其他专业领域（如工业检测、遥感、自动驾驶）的基础模型应用提供借鉴，尤其是在数据标注成本高昂或数据稀缺的场景。</li>
</ul>
<p><strong>5. 从摘要中推断出的局限性 (Any limitations that can be inferred from the abstract)</strong></p>
<ul>
<li><strong>无监督学习的固有挑战</strong>：尽管声称达到了SOTA，但无监督方法在理论上可能仍无法完全超越在大量高质量标注数据上训练的监督方法（如果此类数据可用的话）。其性能上限可能受限于自监督信号的质量和内窥镜图像的复杂性。</li>
<li><strong>基础模型的计算成本</strong>：虽然摘要提到“保持高效的模型尺寸”，但“双基础模型”的集成通常意味着相对较大的模型参数量和计算资源需求。这在资源受限的边缘设备或需要极低延迟的实时手术场景中，可能仍是一个需要仔细权衡的因素。</li>
<li><strong>掩膜生成依赖性</strong>：摘要中提到的“掩膜引导的平滑损失”需要准确的解剖组织掩膜。如果这些掩膜需要人工标注，将增加数据准备的成本；如果通过自动化方法生成，那么掩膜本身的准确性和鲁棒性将直接影响深度估计的性能，且自动化掩膜生成本身也是一个挑战。</li>
<li><strong>领域泛化性</strong>：尽管在多个数据集上进行了验证，内窥镜图像的复杂性和多样性（如不同病理、不同器械、不同医生操作习惯、极端光照变化、出血、烟雾等）可能远超现有数据集的覆盖范围。模型在未见过的极端临床条件下的泛化能力仍需进一步验证。</li>
<li><strong>实时性要求</strong>：对于手术导航和AR系统，实时性是关键。摘要未明确提及模型的推理速度，这对于实际临床应用至关重要。高效的模型尺寸并不等同于高速推理。</li>
</ul>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this study, we introduce
EndoUFM, an unsupervised monocular depth estimation framework that innovatively
integrating dual foundation models for surgical scenes, which enhance the depth
estimation performance by leveraging the powerful pre-learned priors.</li>
<li>The
framework features a novel adaptive fine-tuning strategy that incorporates
Random Vector Low-Rank Adaptation (RVLoRA) to enhance model adaptability, and a
Residual block based on Depthwise Separable Convolution (Res-DSC) to improve
the capture of fine-grained local features.</li>
<li>Extensive experiments on the SCARED, Hamlyn, SERV-CT, and
EndoNeRF datasets confirm that our method achieves state-of-the-art performance
while maintaining an efficient model size.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="http://arxiv.org/pdf/2508.17916v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2508.17916v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2508.17885v1'></a></p>
<h2 id="isalux-illumination-and-segmentation-aware-transformer-employing-mixture-of-experts-for-low-light-image-enhancement"><a href="https://arxiv.org/abs/2508.17885v1">ISALux: Illumination and Segmentation Aware Transformer Employing Mixture of Experts for Low Light Image Enhancement</a></h2>
<p><strong>Authors:</strong> Raul Balmez, Alexandru Brateanu, Ciprian Orhei, Codruta Ancuti, Cosmin Ancuti</p>
<p><strong>Published:</strong> 2025-08-25</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We introduce ISALux, a novel transformer-based approach for Low-Light Image
Enhancement (LLIE) that seamlessly integrates illumination and semantic priors.
Our architecture includes an original self-attention block, Hybrid Illumination
and Semantics-Aware Multi-Headed Self- Attention (HISA-MSA), which integrates
illumination and semantic segmentation maps for en- hanced feature extraction.
ISALux employs two self-attention modules to independently process illumination
and semantic features, selectively enriching each other to regulate luminance
and high- light structural variations in real-world scenarios. A Mixture of
Experts (MoE)-based Feed-Forward Network (FFN) enhances contextual learning,
with a gating mechanism conditionally activating the top K experts for
specialized processing. To address overfitting in LLIE methods caused by
distinct light patterns in benchmarking datasets, we enhance the HISA-MSA
module with low-rank matrix adaptations (LoRA). Extensive qualitative and
quantitative evaluations across multiple specialized datasets demonstrate that
ISALux is competitive with state-of-the-art (SOTA) methods. Addition- ally, an
ablation study highlights the contribution of each component in the proposed
model. Code will be released upon publication.</p>
<p><strong>Analysis:</strong></p>
<p>这篇论文介绍的ISALux是一个在低光图像增强（LLIE）领域具有创新性的工作。作为计算机视觉和机器学习领域的专家，以下是对其摘要的详细分析：</p>
<hr />
<h3 id="1-concise-summary_3">1. 论文主要贡献的简明摘要 (Concise Summary)</h3>
<p>ISALux提出了一种新颖的基于Transformer的低光图像增强（LLIE）方法，它通过一个混合光照和语义感知多头自注意力（HISA-MSA）模块，无缝地整合了光照和语义先验信息。该模型利用两个相互增强的自注意力模块来处理光照和语义特征，并通过基于专家混合（MoE）的前馈网络增强上下文学习，同时引入低秩矩阵适应（LoRA）来解决LLIE方法中常见的过拟合问题。</p>
<h3 id="2-key-innovation-or-methodological-approach_3">2. 关键创新或方法学方法 (Key Innovation or Methodological Approach)</h3>
<p>ISALux的核心创新在于其<strong>Hybrid Illumination and Semantics-Aware Multi-Headed Self-Attention (HISA-MSA)</strong> 模块。这个模块独特地将光照图和语义分割图直接整合到自注意力机制中，从而实现了对光照条件和物体语义内容都敏感的特征提取。具体来说：</p>
<ul>
<li><strong>双重自注意力机制与相互增强：</strong> ISALux采用两个独立的自注意力模块，分别处理光照特征和语义特征，但它们能够“选择性地相互丰富”，这表明它们之间存在一种协同作用，而非简单的并行处理，从而能够更精细地调节亮度和高光结构变化。</li>
<li><strong>基于专家混合（MoE）的前馈网络：</strong> 引入MoE-based FFN，通过一个门控机制有条件地激活顶部的K个专家，以实现更专业化和上下文感知的处理，这有助于模型在不同场景下进行自适应学习。</li>
<li><strong>LoRA用于解决过拟合：</strong> 针对LLIE方法中因基准数据集光照模式差异导致的过拟合问题，ISALux在HISA-MSA模块中融入了低秩矩阵适应（LoRA）。这是一种高效的参数微调技术，通常用于大型模型，能够有效提升模型在不同光照条件下的泛化能力。</li>
</ul>
<h3 id="3-potential-impact-on-the-field_3">3. 对领域潜在影响 (Potential Impact on the Field)</h3>
<ul>
<li><strong>LLIE性能提升：</strong> 通过深度整合语义和光照先验，并结合MoE和LoRA，ISALux有望在低光图像增强任务上达到或超越现有SOTA方法，提供更自然、细节更丰富的增强结果。</li>
<li><strong>多模态先验整合的新范式：</strong> 该研究为Transformer架构中如何有效融合不同类型的（如几何、语义、物理）先验信息提供了一个强有力的范例，这可能启发其他低级视觉任务（如去雾、去噪、超分辨率）的设计。</li>
<li><strong>解决LLIE泛化性挑战：</strong> LoRA的应用为解决LLIE模型在不同光照数据集之间泛化能力不足的问题提供了一个有效途径，有助于开发出更鲁棒、更具实用性的模型。</li>
<li><strong>Transformer在低级视觉中的进一步应用：</strong> 进一步巩固了Transformer在图像增强等低级视觉任务中的潜力，展示了其在捕捉长距离依赖和复杂上下文信息方面的优势。</li>
</ul>
<h3 id="4-related-areas-or-applications_2">4. 相关领域或应用 (Related Areas or Applications)</h3>
<ul>
<li><strong>计算机视觉在挑战性环境下的应用：</strong> 自动驾驶、监控系统、机器人视觉、无人机成像等，这些场景对夜间或低光照条件下的感知能力有极高要求。</li>
<li><strong>消费级摄影：</strong> 智能手机和其他相机设备在低光环境下的图像质量提升。</li>
<li><strong>医疗影像：</strong> 增强低光显微镜图像或内窥镜图像的质量，以辅助诊断。</li>
<li><strong>其他图像恢复任务：</strong> 论文中整合多模态先验的方法学思想可以推广到去雾、去噪、图像去雨等其他需要丰富上下文信息的图像恢复任务。</li>
<li><strong>多模态学习：</strong> 为如何将图像的像素级信息与高级语义信息有效结合提供了一个研究方向。</li>
</ul>
<h3 id="5-limitations-inferred-from-the-abstract">5. 从摘要中可推断的局限性 (Limitations Inferred from the Abstract)</h3>
<ul>
<li><strong>对语义先验的依赖：</strong> 模型依赖于语义分割图。这意味着在实际应用中，要么需要一个额外的、预训练的语义分割模型，这会增加计算开销和潜在的错误传播（如果分割不准确），要么模型需要同时学习分割和增强，这会增加模型的复杂性。摘要中提到“integrates...semantic priors”，更倾向于前者。</li>
<li><strong>计算资源需求：</strong> Transformer模型本身通常计算量较大，尤其是在处理高分辨率图像时。此外，MoE结构虽然在参数效率上可能有所优势，但在推理时激活多个专家也可能增加计算负担。摘要未提及模型的效率或实时性。</li>
<li><strong>训练复杂性：</strong> 结合了HISA-MSA、MoE和LoRA的复杂架构，其训练过程可能需要大量的计算资源和精细的超参数调优。</li>
<li><strong>LoRA的适用性：</strong> 尽管LoRA有助于缓解过拟合，但其效果可能仍受限于训练数据的多样性。在极端或未见的低光场景下，模型的泛化能力仍可能面临挑战。</li>
<li><strong>实时性与部署：</strong> 摘要未提供关于模型大小、推理速度或在边缘设备上部署潜力的信息，这对于实际应用至关重要。</li>
</ul>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce ISALux, a novel transformer-based approach for Low-Light Image
Enhancement (LLIE) that seamlessly integrates illumination and semantic priors.</li>
<li>Extensive qualitative and
quantitative evaluations across multiple specialized datasets demonstrate that
ISALux is competitive with state-of-the-art (SOTA) methods.</li>
<li>Addition- ally, an
ablation study highlights the contribution of each component in the proposed
model.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="http://arxiv.org/pdf/2508.17885v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2508.17885v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-08-27 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
