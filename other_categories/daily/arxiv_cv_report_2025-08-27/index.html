<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-08-27 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DEâ«¶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Birdâs-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">è®ºæéè¯»</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-Î±
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../../undone/depth_prediction_citation/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-08-28/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-08-27">Arxiv Computer Vision Papers - 2025-08-27</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#diffusion-based-data-augmentation-for-medical-image-segmentation" class="nav-link">Diffusion-Based Data Augmentation for Medical Image Segmentation</a>
                </li>
                <li class="nav-item">
                    <a href="#beyond-flattening-a-geometrically-principled-positional-encoding-for-vision-transformers-with-weierstrass-elliptic-functions" class="nav-link">Beyond flattening: a geometrically principled positional encoding for vision transformers with Weierstrass elliptic functions</a>
                </li>
                <li class="nav-item">
                    <a href="#why-relational-graphs-will-save-the-next-generation-of-vision-foundation-models" class="nav-link">Why Relational Graphs Will Save the Next Generation of Vision Foundation Models?</a>
                </li>
                <li class="nav-item">
                    <a href="#enhanced-drift-aware-computer-vision-architecture-for-autonomous-driving" class="nav-link">Enhanced Drift-Aware Computer Vision Architecture for Autonomous Driving</a>
                </li>
                <li class="nav-item">
                    <a href="#scene-agnostic-traversability-labeling-and-estimation-via-a-multimodal-self-supervised-framework" class="nav-link">Scene-Agnostic Traversability Labeling and Estimation via a Multimodal Self-supervised Framework</a>
                </li>
                <li class="nav-item">
                    <a href="#learning-to-detect-label-errors-by-making-them-a-method-for-segmentation-and-object-detection-datasets" class="nav-link">Learning to Detect Label Errors by Making Them: A Method for Segmentation and Object Detection Datasets</a>
                </li>
                <li class="nav-item">
                    <a href="#catformer-contrastive-adversarial-transformer-for-image-super-resolution" class="nav-link">CATformer: Contrastive Adversarial Transformer for Image Super-Resolution</a>
                </li>
                <li class="nav-item">
                    <a href="#roofseg-an-edge-aware-transformer-based-network-for-end-to-end-roof-plane-segmentation" class="nav-link">RoofSeg: An edge-aware transformer-based network for end-to-end roof plane segmentation</a>
                </li>
                <li class="nav-item">
                    <a href="#endoufm-utilizing-foundation-models-for-monocular-depth-estimation-of-endoscopic-images" class="nav-link">EndoUFM: Utilizing Foundation Models for Monocular depth estimation of endoscopic images</a>
                </li>
                <li class="nav-item">
                    <a href="#isalux-illumination-and-segmentation-aware-transformer-employing-mixture-of-experts-for-low-light-image-enhancement" class="nav-link">ISALux: Illumination and Segmentation Aware Transformer Employing Mixture of Experts for Low Light Image Enhancement</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-08-27">Arxiv Computer Vision Papers - 2025-08-27</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>å¥½çï¼è¿æ¯ä¸ä»½éå¯¹2025å¹´8æ25æ¥Arxivè®¡ç®æºè§è§é¢åææ°è®ºæçæ§è¡æè¦ã</p>
<hr />
<p><strong>Arxiv è®¡ç®æºè§è§é¢åææ°è®ºææ§è¡æè¦ (2025-08-25)</strong></p>
<p><strong>æ¥åæ¥æ:</strong> 2025å¹´8æ25æ¥
<strong>æ¥åäºº:</strong> [æ¨çå§å/ç ç©¶å©ç]</p>
<p>æ¬æ¥åæ¨å¨ä¸ºå¿ç¢çç ç©¶äººåæä¾ä¸ä»½å³äº2025å¹´8æ25æ¥Arxivä¸åå¸ç10ç¯è®¡ç®æºè§è§é¢åææ°è®ºæçç®ææ¦è¿°ï¼ä»¥å¸®å©å¿«éäºè§£è¯¥é¢åçéè¦åå±ã</p>
<hr />
<p><strong>1. ä¸»è¦ä¸»é¢ä¸è¶å¿æ¦è¿°</strong></p>
<p>æ¬æ¬¡åå¸çè®ºæå±ç°äºè®¡ç®æºè§è§é¢åå ä¸ªæ¾èçè¶å¿ï¼</p>
<ul>
<li><strong>Transformeræ¶æçæç»­ä¸»å¯¼ä¸æ·±å:</strong> å ä¹ä¸åçè®ºæï¼2, 7, 8, 10ï¼ç´æ¥æé´æ¥ä½¿ç¨äºTransformeræ¶æï¼å¹¶å¯¹å¶è¿è¡åæ°æ§æ¹è¿ï¼å¦å ä½ä½ç½®ç¼ç ãMoEï¼æåºç¨äºç¹å®ä»»å¡ï¼è¶åãåå²ãå¾åå¢å¼ºï¼ã</li>
<li><strong>ç¹å®åºç¨é¢åçæ·±åº¦æ¢ç´¢:</strong> å»çå½±åï¼1, 9ï¼ãèªå¨é©¾é©¶ï¼4, 5ï¼ãå¾åè¶åè¾¨çï¼7ï¼ãä½åç§å¾åå¢å¼ºï¼10ï¼åå±é¡¶åå²ï¼8ï¼ç­é¢åæç»­åå°å³æ³¨ï¼ç ç©¶äººåè´åäºå°åè¿æ¨¡åè½å°å°å®éé®é¢ä¸­ã</li>
<li><strong>æ°æ®ä¸ºä¸­å¿çäººå·¥æºè½ (Data-Centric AI):</strong> è®ºæå³æ³¨æ°æ®è´¨éï¼6ï¼åæ°æ®å¢å¼ºï¼1ï¼ï¼å¼ºè°äºé«è´¨éæ°æ®åæææ°æ®å¤çå¯¹æ¨¡åæ§è½çéè¦æ§ã</li>
<li><strong>åºç¡æ¨¡å (Foundation Models) çæ¼è¿ä¸åºç¨:</strong> è®ºææ¢è®¨äºåºç¡æ¨¡åçæªæ¥åå±æ¹åï¼3ï¼ä»¥åå®ä»¬å¨ç¹å®ãæææ§é¢åï¼å¦åçª¥éæ·±åº¦ä¼°è®¡ï¼9ï¼çåºç¨æ½åã</li>
<li><strong>èªçç£å­¦ä¹ ä¸çææ¨¡å:</strong> èªçç£å­¦ä¹ ï¼5ï¼åæ©æ£æ¨¡åï¼1ï¼ä½ä¸ºå¼ºå¤§çèå¼ï¼è¢«ç¨äºè§£å³æ°æ®ç¨ç¼ºåå¤æåºæ¯çè§£ç­é®é¢ã</li>
</ul>
<p><strong>2. ç¹å«æ¾èæåæ°çè®ºæ</strong></p>
<ul>
<li><strong>è®ºæ2: "Beyond flattening: a geometrically principled positional encoding for vision transformers with Weierstrass elliptic functions"</strong><ul>
<li><strong>åæ°ç¹:</strong> è¿ç¯è®ºææåºäºä¸ä¸ªåºäºé­å°æ¯ç¹ææ¯æ¤­åå½æ°çå ä½åçä½ç½®ç¼ç ï¼è¶è¶äºä¼ ç»Transformerä¸­æå¹³åçä½ç½®ç¼ç æ¹å¼ãå®å¯è½ä¸ºTransformerçåºå±æºå¶å¸¦æ¥æ´æ·±å»ççè®ºçè§£åæ§è½æåï¼å·ææ½å¨çæ¶æçº§å½±åã</li>
</ul>
</li>
<li><strong>è®ºæ3: "Why Relational Graphs Will Save the Next Generation of Vision Foundation Models?"</strong><ul>
<li><strong>åæ°ç¹:</strong> è¿æ¯ä¸ç¯å·æåç»æ§åæ¦å¿µæ§çè®ºæï¼æ¢è®¨äºå³ç³»å¾å¨æªæ¥è§è§åºç¡æ¨¡åä¸­çå³é®ä½ç¨ãå®ææäºå½ååºç¡æ¨¡åä»ä¾èµå¤§è§æ¨¡æ°æ®ååæ°çèå¼ï¼æåºéè¿å¼å¥å³ç³»å½çº³åç½®æ¥æåæ¨¡åçæ³åè½ååå¯è§£éæ§ï¼å¯¹é¢ååå±æ¹åå·ææå¯¼æä¹ã</li>
</ul>
</li>
<li><strong>è®ºæ6: "Learning to Detect Label Errors by Making Them: A Method for Segmentation and Object Detection Datasets"</strong><ul>
<li><strong>åæ°ç¹:</strong> è¯¥ç ç©¶æåºäºä¸ç§éè¿âå¶é âæ ç­¾éè¯¯æ¥å­¦ä¹ æ£æµçå®æ ç­¾éè¯¯çæ°é¢æ¹æ³ãè¿ç´æ¥è§£å³äºå®éåºç¨ä¸­æ°æ®éè´¨éæ§å¶ççç¹ï¼å¯¹äºæé«æ¨¡åé²æ£æ§ååå°äººå·¥æ æ³¨ææ¬å·æéè¦ä»·å¼ã</li>
</ul>
</li>
<li><strong>è®ºæ9: "EndoUFM: Utilizing Foundation Models for Monocular depth estimation of endoscopic images"</strong><ul>
<li><strong>åæ°ç¹:</strong> å°åºç¡æ¨¡ååºç¨äºæå·æææ§çåçª¥éåç®æ·±åº¦ä¼°è®¡ä»»å¡ï¼å±ç¤ºäºåºç¡æ¨¡åå¨ç¹å®ãæ°æ®ç¨ç¼ºä¸å½¢åä¸¥éçå»çåºæ¯ä¸­çå¼ºå¤§éåºæ§åæ½åã</li>
</ul>
</li>
</ul>
<p><strong>3. æ°å´ç ç©¶æ¹åæææ¯</strong></p>
<ul>
<li><strong>Transformeræ¶æçå ä½åä¸çè®ºæ·±å:</strong> è®ºæ2é¢ç¤ºçå¯¹Transformeræ ¸å¿ç»ä»¶ï¼å¦ä½ç½®ç¼ç ï¼è¿è¡æ´æ·±å±æ¬¡çæ°å­¦åå ä½åçæ¢ç´¢ï¼ä»¥çªç ´ç°ææ§è½ç¶é¢ã</li>
<li><strong>å°å³ç³»å½çº³åç½®èå¥åºç¡æ¨¡å:</strong> è®ºæ3å¼ºè°äºå¨åºç¡æ¨¡åä¸­å¼å¥ç»æåç¥è¯ï¼å¦å³ç³»å¾ï¼çéè¦æ§ï¼ä»¥æåå¶æ¨çè½ååæ³åæ§ï¼èéä»ä»ä¾èµè§æ¨¡ã</li>
<li><strong>æ°æ®è´¨éç®¡çä½ä¸ºå¯å­¦ä¹ ä»»å¡:</strong> è®ºæ6æåºéè¿æºå¨å­¦ä¹ æ¹æ³èªå¨æ£æµåçº æ­£æ°æ®éä¸­çæ ç­¾éè¯¯ï¼å°æ°æ®æ¸æ´ä»äººå·¥å¯éåä»»å¡è½¬åä¸ºèªå¨åæµç¨ã</li>
<li><strong>æ©æ£æ¨¡åå¨æ°æ®å¢å¼ºä¸­çåºç¨:</strong> è®ºæ1å±ç¤ºäºæ©æ£æ¨¡åå¨çæé«è´¨éåææ°æ®ä»¥å¢å¼ºå»çå½±åæ°æ®éæ¹é¢çæ½åï¼å°¤å¶éç¨äºæ°æ®ç¨ç¼ºçé¢åã</li>
<li><strong>å¤æ¨¡æèªçç£å­¦ä¹ å¨å¤æåºæ¯çè§£ä¸­çåºç¨:</strong> è®ºæ5å©ç¨å¤æ¨¡æèªçç£æ¡æ¶è¿è¡åºæ¯æ å³çå¯éè¡æ§ä¼°è®¡ï¼ä¸ºèªå¨é©¾é©¶ç­é¢åæä¾äºæ°çè§£å³æ¹æ¡ã</li>
</ul>
<p><strong>4. å»ºè®®éè¯»çè®ºæ</strong></p>
<p>æ ¹æ®ç ç©¶å´è¶£åæ½å¨å½±åï¼å»ºè®®ä¼åéè¯»ä»¥ä¸è®ºæï¼</p>
<ul>
<li><strong>å¯¹äºå³æ³¨Transformeråºå±æºå¶åæ¶æåæ°çç ç©¶äººå:</strong><ul>
<li><strong>è®ºæ2: "Beyond flattening: a geometrically principled positional encoding for vision transformers with Weierstrass elliptic functions"</strong></li>
</ul>
</li>
<li><strong>å¯¹äºå³æ³¨åºç¡æ¨¡åæªæ¥åå±åçè®ºç¶é¢çç ç©¶äººå:</strong><ul>
<li><strong>è®ºæ3: "Why Relational Graphs Will Save the Next Generation of Vision Foundation Models?"</strong></li>
</ul>
</li>
<li><strong>å¯¹äºä»äºæ°æ®éæå»ºãè´¨éæ§å¶ææ¨¡åå®éé¨ç½²çç ç©¶äººå:</strong><ul>
<li><strong>è®ºæ6: "Learning to Detect Label Errors by Making Them: A Method for Segmentation and Object Detection Datasets"</strong></li>
</ul>
</li>
<li><strong>å¯¹äºå»çå½±åé¢åæå¯¹åºç¡æ¨¡åå¨ç¹å®é¢åè½å°æå´è¶£çç ç©¶äººå:</strong><ul>
<li><strong>è®ºæ9: "EndoUFM: Utilizing Foundation Models for Monocular depth estimation of endoscopic images"</strong></li>
<li><strong>è®ºæ1: "Diffusion-Based Data Augmentation for Medical Image Segmentation"</strong></li>
</ul>
</li>
</ul>
<p>å¶ä»è®ºæï¼4, 5, 7, 8, 10ï¼å¯¹åèªçç¹å®åºç¨é¢åå·æç´æ¥ä»·å¼ï¼å¯æ ¹æ®ä¸ªäººç ç©¶æ¹åéæ©æ§éè¯»ã</p>
<hr />
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2508.17844v1">Diffusion-Based Data Augmentation for Medical Image Segmentation</a></li>
<li><a href="#2508.19167v1">Beyond flattening: a geometrically principled positional encoding for vision transformers with Weierstrass elliptic functions</a></li>
<li><a href="#2508.18421v1">Why Relational Graphs Will Save the Next Generation of Vision Foundation Models?</a></li>
<li><a href="#2508.17975v1">Enhanced Drift-Aware Computer Vision Architecture for Autonomous Driving</a></li>
<li><a href="#2508.18249v1">Scene-Agnostic Traversability Labeling and Estimation via a Multimodal Self-supervised Framework</a></li>
<li><a href="#2508.17930v1">Learning to Detect Label Errors by Making Them: A Method for Segmentation and Object Detection Datasets</a></li>
<li><a href="#2508.17708v1">CATformer: Contrastive Adversarial Transformer for Image Super-Resolution</a></li>
<li><a href="#2508.19003v1">RoofSeg: An edge-aware transformer-based network for end-to-end roof plane segmentation</a></li>
<li><a href="#2508.17916v1">EndoUFM: Utilizing Foundation Models for Monocular depth estimation of endoscopic images</a></li>
<li><a href="#2508.17885v1">ISALux: Illumination and Segmentation Aware Transformer Employing Mixture of Experts for Low Light Image Enhancement</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2508.17844v1'></a></p>
<h2 id="diffusion-based-data-augmentation-for-medical-image-segmentation"><a href="https://arxiv.org/abs/2508.17844v1">Diffusion-Based Data Augmentation for Medical Image Segmentation</a></h2>
<p><strong>Authors:</strong> Maham Nazir, Muhammad Aqeel, Francesco Setti</p>
<p><strong>Published:</strong> 2025-08-25</p>
<p><strong>Categories:</strong> cs.CV, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Medical image segmentation models struggle with rare abnormalities due to
scarce annotated pathological data. We propose DiffAug a novel framework that
combines textguided diffusion-based generation with automatic segmentation
validation to address this challenge. Our proposed approach uses latent
diffusion models conditioned on medical text descriptions and spatial masks to
synthesize abnormalities via inpainting on normal images. Generated samples
undergo dynamic quality validation through a latentspace segmentation network
that ensures accurate localization while enabling single-step inference. The
text prompts, derived from medical literature, guide the generation of diverse
abnormality types without requiring manual annotation. Our validation mechanism
filters synthetic samples based on spatial accuracy, maintaining quality while
operating efficiently through direct latent estimation. Evaluated on three
medical imaging benchmarks (CVC-ClinicDB, Kvasir-SEG, REFUGE2), our framework
achieves state-of-the-art performance with 8-10% Dice improvements over
baselines and reduces false negative rates by up to 28% for challenging cases
like small polyps and flat lesions critical for early detection in screening
applications.</p>
<p><strong>Analysis:</strong></p>
<p>ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå¯¹è¿ç¯è®ºææè¦è¿è¡å¦ä¸åæï¼</p>
<hr />
<h3 id="1-concise-summary">1. è®ºæä¸»è¦è´¡ç®çç®ææè¦ (Concise Summary)</h3>
<p>è¿ç¯è®ºææåºäºä¸ç§åä¸º DiffAug çæ°é¢æ¡æ¶ï¼æ¨å¨éè¿çæé«è´¨éçåææ°æ®æ¥è§£å³å»å­¦å¾ååå²æ¨¡åå¨å¤çç½è§å¼å¸¸æ¶é¢ä¸´çæ°æ®ç¨ç¼ºé®é¢ãDiffAug ç»åäºææ¬å¼å¯¼çæ©æ£æ¨¡åè¿è¡å¼å¸¸çæï¼éè¿å¨æ­£å¸¸å¾åä¸è¿è¡åç»ï¼ï¼å¹¶å©ç¨ä¸ä¸ªé«æçæ½å¨ç©ºé´åå²ç½ç»è¿è¡å¨æè´¨ééªè¯ï¼ç¡®ä¿äºçææ ·æ¬çç©ºé´åç¡®æ§ãè¯¥æ¹æ³å¨å¤ä¸ªå»å­¦å¾ååºåæµè¯ä¸­åå¾äºæ¾èçæ§è½æåï¼å°¤å¶æ¯å¨å¤çå°ååæå¹³çåç­æææ§çä¾æ¶ã</p>
<h3 id="2-key-innovation-or-methodological-approach">2. å³é®åæ°ææ¹æ³å­¦æ¹æ³ (Key Innovation or Methodological Approach)</h3>
<p>è¯¥è®ºæçå³é®åæ°å¨äºå¶<strong>éæåçãåé¶æ®µæ¹æ³</strong>ï¼
1.  <strong>ææ¬å¼å¯¼çæ½å¨æ©æ£æ¨¡åè¿è¡æ¡ä»¶çæï¼</strong> DiffAug å©ç¨æ½å¨æ©æ£æ¨¡åï¼éè¿å»å­¦ææ¬æè¿°åç©ºé´æ©ç ä½ä¸ºæ¡ä»¶ï¼å¨æ­£å¸¸å¾åä¸è¿è¡åç»æ¥åæåç§å¼å¸¸ãè¿ä½¿å¾è½å¤æ ¹æ®ææ¬æç¤ºçæå¤æ ·åçå¼å¸¸ç±»åï¼èæ éæå¨æ æ³¨çæè¿ç¨ã
2.  <strong>é«æçæ½å¨ç©ºé´åå²ç½ç»è¿è¡å¨æè´¨ééªè¯ï¼</strong> è®ºæå¼å¥äºä¸ä¸ªç¬ç¹çéªè¯æºå¶ï¼éè¿ä¸ä¸ªå¨æ½å¨ç©ºé´æä½çåå²ç½ç»æ¥å¨æè¯ä¼°çææ ·æ¬çè´¨éåå®ä½åç¡®æ§ãè¿ç§æ¹æ³ä¸ä»ç¡®ä¿äºåææ°æ®çå®ç¨æ§ï¼èä¸éè¿åæ­¥æ¨çåç´æ¥æ½å¨ä¼°è®¡å®ç°äºé«æçè¿æ»¤ï¼é¿åäºçæä½è´¨éæä¸åç¡®çæ ·æ¬ã</p>
<p>è¿ç§å°æºè½çæä¸é«æãèªå¨éªè¯ç¸ç»åçç­ç¥ï¼æ¯å¶åºå«äºä¼ ç»æ°æ®å¢å¼ºåä¸è¬æ©æ£æ¨¡ååºç¨çå³é®ã</p>
<h3 id="3-potential-impact-on-the-field">3. å¯¹é¢åæ½å¨å½±å (Potential Impact on the Field)</h3>
<p>è¿é¡¹ç ç©¶å¯¹è®¡ç®æºè§è§åå»å­¦å¾ååæé¢åå·ææ·±è¿å½±åï¼
*   <strong>è§£å³æ°æ®ç¨ç¼ºçæ ¸å¿ææï¼</strong> å®ä¸ºå»å­¦å¾ååæä¸­é¿æå­å¨çç½è§ç¾çåå¼å¸¸æ°æ®ç¨ç¼ºé®é¢æä¾äºä¸ä¸ªå¼ºå¤§ä¸å¯æ©å±çè§£å³æ¹æ¡ï¼ä»èè½å¤è®­ç»åºæ´é²æ£ãæ´åç¡®çåå²æ¨¡åã
*   <strong>æåè¯æ­åç¡®æ§ä¸æ©ææ£æµï¼</strong> æ¾èéä½äºå°æ¯èåæå¹³çåç­æææ§çä¾çåé´æ§çï¼è¿æå³çå¯ä»¥æ´æ©ãæ´åç¡®å°æ£æµå°å³é®ç¾çï¼å¯¹ççç­æ¥ç­åºç¨å·æå·¨å¤§çä¸´åºä»·å¼ã
*   <strong>æ¨å¨åææ°æ®å¨å»çé¢åçåºç¨ï¼</strong> è¯æäºé«è´¨éãéªè¯è¿çåææ°æ®å¨å»çé¢åä½ä¸ºçå®æ°æ®ææè¡¥åçæ½åï¼å¯è½å éæ°ç®æ³çå¼ååé¨ç½²ã
*   <strong>å¯åæ°çæ°æ®å¢å¼ºèå¼ï¼</strong> å¶ç»åçææ¨¡åä¸æºè½éªè¯çæ¡æ¶ï¼å¯ä»¥ä¸ºå¶ä»éè¦é«è´¨éåææ°æ®ï¼å°¤å¶æ¯å¨æ°æ®ä¸å¹³è¡¡æç¨ç¼ºåºæ¯ä¸ï¼çè®¡ç®æºè§è§ä»»å¡æä¾æ°çæè·¯ã</p>
<h3 id="4-related-areas-or-applications">4. ç¸å³é¢åæåºç¨ (Related Areas or Applications)</h3>
<p>é¤äºå»å­¦å¾ååå²æ¬èº«ï¼è¿é¡¹ç ç©¶è¿å¯ä»¥æ åä»¥ä¸é¢åååºç¨ï¼
*   <strong>å»å­¦å¾åæ£æµä¸åç±»ï¼</strong> ç±»ä¼¼çæ¹æ³å¯ä»¥ç¨äºçæç½è§çåçæ£æµæåç±»ä»»å¡çè®­ç»æ°æ®ã
*   <strong>å°æ ·æ¬å­¦ä¹  (Few-Shot Learning) åé¶æ ·æ¬å­¦ä¹  (Zero-Shot Learning)ï¼</strong> éè¿çæåææ ·æ¬ï¼å¯ä»¥æææ©å±æéççå®æ°æ®éï¼ä»èæ¹åå°æ ·æ¬æé¶æ ·æ¬åºæ¯ä¸çæ¨¡åæ§è½ã
*   <strong>é¢åéåº (Domain Adaptation)ï¼</strong> çæç¹å®é¢åæç¹å®è®¾å¤ç¹å¾çåææ°æ®ï¼æå©äºæ¨¡åå¨ä¸åæ°æ®æºä¹é´è¿è¡æ³åã
*   <strong>å·¥ä¸ç¼ºé·æ£æµï¼</strong> å¨å·¥ä¸çäº§ä¸­ï¼ç½è§ç¼ºé·çå¾åæ°æ®éå¸¸éå¸¸ç¨ç¼ºï¼è¯¥æ¹æ³å¯ä»¥ç¨äºçæè¿äºç¼ºé·çåæå¾åï¼ä»¥è®­ç»æ´åç¡®çæ£æµç³»ç»ã
*   <strong>èªå¨é©¾é©¶ï¼</strong> çææç«¯æç½è§äº¤éåºæ¯ï¼å¦äºæãå¼å¸¸å¤©æ°æ¡ä»¶ï¼çå¾åæ°æ®ï¼ä»¥æé«èªå¨é©¾é©¶ç³»ç»çé²æ£æ§ã
*   <strong>è®¡ç®æºå¾å½¢å­¦ä¸èæç°å®ï¼</strong> çæå·æç¹å®å±æ§ææ¡ä»¶çå¾ååå®¹ï¼ç¨äºè®­ç»ææ¨¡æã</p>
<h3 id="5-limitations-inferable-from-the-abstract">5. ä»æè¦ä¸­å¯æ¨æ­çå±éæ§ (Limitations Inferable from the Abstract)</h3>
<p>å°½ç®¡æè¦å±ç¤ºäºä»¤äººå°è±¡æ·±å»çç»æï¼ä½ä»å¯æ¨æ­åºä¸äºæ½å¨å±éæ§ï¼
*   <strong>ææ¬æç¤ºçè´¨éåè¦çèå´ï¼</strong> æè¦æå°ææ¬æç¤ºæ¥æºäºå»å­¦æç®ï¼ä½å¶è½å¦å®å¨ææææç½è§å¼å¸¸çç»å¾®ç¹å¾åå¤æ ·æ§ï¼ä»¥åå¦ä½å¤çæç®ä¸­æªååæè¿°çå¼å¸¸ï¼ä»æ¯ä¸ä¸ªé®é¢ãææ¬æç¤ºçè´¨éç´æ¥å½±åçææ ·æ¬çåç¡®æ§åå¤æ ·æ§ã
*   <strong>çææ ·æ¬ççå®æ§ä¸ä¸´åºå¯ä¿¡åº¦ï¼</strong> å°½ç®¡éè¿æ½å¨ç©ºé´éªè¯ç¡®ä¿äºç©ºé´åç¡®æ§ï¼ä½çææ ·æ¬çè§è§çå®æï¼å³æ¯å¦è½è¢«ä¸´åºå»çè®¤ä¸ºæ¯çå®çï¼ä»éè¿ä¸æ­¥è¯ä¼°ãæ©æ£æ¨¡åå¯è½å¼å¥å¾®å¦çä¼ªå½±æä¸èªç¶ççº¹çï¼è¿å¨ä¸´åºåºç¨ä¸­å¯è½æ¯ææçã
*   <strong>è®¡ç®èµæºéæ±ï¼</strong> æ©æ£æ¨¡åéå¸¸è®¡ç®ææ¬é«æï¼ç»åæ½å¨ç©ºé´åå²ç½ç»ï¼æ´ä¸ªæ¡æ¶çè®­ç»åæ¨çå¯è½éè¦å¤§éçè®¡ç®èµæºåæ¶é´ã
*   <strong>å¯¹âæ­£å¸¸å¾åâçä¾èµï¼</strong> è¯¥æ¹æ³éè¿å¨æ­£å¸¸å¾åä¸è¿è¡åç»æ¥åæå¼å¸¸ãè¿æå³çå®éè¦ä¸ä¸ªè¶³å¤å¤§ä¸å¤æ ·åçâæ­£å¸¸âå¾åæ°æ®éä½ä¸ºåºç¡ãå¦ææ­£å¸¸å¾åæ¬èº«ä¹ç¨ç¼ºæå·æé«åº¦åå¼æ§ï¼åå¯è½ä¼éå¶å¶åºç¨ã
*   <strong>æ½å¨ç©ºé´éªè¯çå±éæ§ï¼</strong> å°½ç®¡é«æï¼ä½æ½å¨ç©ºé´ä¸­çéªè¯å¯è½æ æ³ææå°åç´ ç©ºé´ä¸­ææç»å¾®çéè¯¯æä¸ä¸è´æ§ãå®å¯è½å¨æäºæåµä¸å¯¹çæè´¨éçè¯ä¼°ä¸å¤å¨é¢ã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose DiffAug a novel framework that
combines textguided diffusion-based generation with automatic segmentation
validation to address this challenge.</li>
<li>Evaluated on three
medical imaging benchmarks (CVC-ClinicDB, Kvasir-SEG, REFUGE2), our framework
achieves state-of-the-art performance with 8-10% Dice improvements over
baselines and reduces false negative rates by up to 28% for challenging cases
like small polyps and flat lesions critical for early detection in screening
applications.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="http://arxiv.org/pdf/2508.17844v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2508.17844v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2508.19167v1'></a></p>
<h2 id="beyond-flattening-a-geometrically-principled-positional-encoding-for-vision-transformers-with-weierstrass-elliptic-functions"><a href="https://arxiv.org/abs/2508.19167v1">Beyond flattening: a geometrically principled positional encoding for vision transformers with Weierstrass elliptic functions</a></h2>
<p><strong>Authors:</strong> Zhihang Xin, Xitong Hu, Rui Wang</p>
<p><strong>Published:</strong> 2025-08-26</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Vision Transformers have demonstrated remarkable success in computer vision
tasks, yet their reliance on learnable one-dimensional positional embeddings
fundamentally disrupts the inherent two-dimensional spatial structure of images
through patch flattening procedures. Traditional positional encoding approaches
lack geometric constraints and fail to establish monotonic correspondence
between Euclidean spatial distances and sequential index distances, thereby
limiting the model's capacity to leverage spatial proximity priors effectively.
We propose Weierstrass Elliptic Function Positional Encoding (WEF-PE), a
mathematically principled approach that directly addresses two-dimensional
coordinates through natural complex domain representation, where the doubly
periodic properties of elliptic functions align remarkably with translational
invariance patterns commonly observed in visual data. Our method exploits the
non-linear geometric nature of elliptic functions to encode spatial distance
relationships naturally, while the algebraic addition formula enables direct
derivation of relative positional information between arbitrary patch pairs
from their absolute encodings. Comprehensive experiments demonstrate that
WEF-PE achieves superior performance across diverse scenarios, including
63.78\% accuracy on CIFAR-100 from-scratch training with ViT-Tiny architecture,
93.28\% on CIFAR-100 fine-tuning with ViT-Base, and consistent improvements on
VTAB-1k benchmark tasks. Theoretical analysis confirms the distance-decay
property through rigorous mathematical proof, while attention visualization
reveals enhanced geometric inductive bias and more coherent semantic focus
compared to conventional approaches.The source code implementing the methods
described in this paper is publicly available on GitHub.</p>
<p><strong>Analysis:</strong></p>
<p>è¿ç¯è®ºææåºäºä¸ç§æ°é¢çãåºäºæ°å­¦åççè§è§Transformerä½ç½®ç¼ç æ¹æ³ï¼æ¨å¨è§£å³ç°ææ¹æ³å¨å¤çå¾åäºç»´ç©ºé´ç»ææ¶çå±éæ§ã</p>
<hr />
<p><strong>1. è®ºæä¸»è¦è´¡ç®çç®æ´æ»ç» (2-3 å¥è¯)</strong></p>
<p>æ¬ææåºäºä¸ç§åä¸ºWeierstrassæ¤­åå½æ°ä½ç½®ç¼ç ï¼WEF-PEï¼çæ°æ¹æ³ï¼æ¨å¨è§£å³Vision Transformerä¸­éè¿è¡¥ä¸å±å¹³å¯¼è´çäºç»´ç©ºé´ä¿¡æ¯ä¸¢å¤±é®é¢ãWEF-PEå©ç¨Weierstrassæ¤­åå½æ°å¨å¤æ°åä¸­ç´æ¥ç¼ç äºç»´åæ ï¼å¶åå¨ææ§ä¸è§è§æ°æ®çå¹³ç§»ä¸åæ§ç¸å»åï¼å¹¶éè¿ä»£æ°å æ³å¬å¼ç´æ¥æ¨å¯¼ç¸å¯¹ä½ç½®ä¿¡æ¯ãå®éªè¯æï¼WEF-PEæ¾èæåäºViTå¨å¤ç§ä»»å¡ä¸çæ§è½ï¼å¹¶å¢å¼ºäºå ä½å½çº³åç½®ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®º</strong></p>
<p>æ ¸å¿åæ°å¨äºå°<strong>Weierstrassæ¤­åå½æ°</strong>å¼å¥å°Vision Transformerçäºç»´ä½ç½®ç¼ç ä¸­ãå·ä½æ¹æ³è®ºåæ¬ï¼
*   <strong>ç´æ¥å¤çäºç»´åæ ä¸å¤æ°åè¡¨ç¤ºï¼</strong> æå¼äºå°äºç»´å¾åå±å¹³ä¸ºä¸ç»´åºåçåæ³ï¼èæ¯ç´æ¥å¨å¤æ°åä¸­å¤çå¾åçäºç»´åæ ï¼è¿ä¸ä¼ ç»çåºäºæ­£å¼¦/ä½å¼¦æå¯å­¦ä¹ çä¸ç»´åµå¥å½¢æé²æå¯¹æ¯ã
*   <strong>å©ç¨æ¤­åå½æ°çåå¨ææ§ï¼</strong> Weierstrassæ¤­åå½æ°çåå¨ææ§ä¸è§è§æ°æ®ä¸­å¸¸è§çå¹³ç§»ä¸åæ§æ¨¡å¼é«åº¦å¥åï¼ä¸ºä½ç½®ç¼ç æä¾äºå¼ºå¤§çå ä½çº¦æã
*   <strong>éçº¿æ§å ä½ç¼ç ç©ºé´è·ç¦»ï¼</strong> æ¤­åå½æ°çéçº¿æ§å ä½ç¹æ§è½å¤æ´èªç¶å°ç¼ç ç©ºé´è·ç¦»å³ç³»ï¼è§£å³äºä¼ ç»æ¹æ³æ æ³å¨æ¬§å éå¾ç©ºé´è·ç¦»ååºåç´¢å¼è·ç¦»ä¹é´å»ºç«åè°å¯¹åºå³ç³»çé®é¢ã
*   <strong>ä»£æ°å æ³å¬å¼æ¨å¯¼ç¸å¯¹ä½ç½®ï¼</strong> è®ºææåºï¼æ¤­åå½æ°çä»£æ°å æ³å¬å¼å¯ä»¥ç´æ¥ä»ç»å¯¹ä½ç½®ç¼ç ä¸­æ¨å¯¼åºä»»æè¡¥ä¸å¯¹ä¹é´çç¸å¯¹ä½ç½®ä¿¡æ¯ï¼è¿å¯¹äºTransformerçæ³¨æåæºå¶è³å³éè¦ã</p>
<p><strong>3. å¯¹é¢åæ½å¨å½±å</strong></p>
<ul>
<li><strong>æåVision Transformerçæ§è½åé²æ£æ§ï¼</strong> éè¿å¼å¥æ´å¼ºçå ä½å½çº³åç½®ï¼WEF-PEæææ¾èæåViTå¨åç§è®¡ç®æºè§è§ä»»å¡ä¸çæ§è½ï¼å°¤å¶æ¯å¨éè¦ç²¾ç»ç©ºé´çè§£çä»»å¡ä¸­ã</li>
<li><strong>æ¨å¨ä½ç½®ç¼ç ç ç©¶çæ°èå¼ï¼</strong> æ¬æä¸ºä½ç½®ç¼ç çè®¾è®¡æä¾äºä¸ä¸ªæ°çãåºäºæ°å­¦åççè§è§ï¼å¯è½ä¼å¯åç ç©¶èæ¢ç´¢æ´å¤é«çº§æ°å­¦å·¥å·æ¥è§£å³æ·±åº¦å­¦ä¹ ä¸­çç»æåæ°æ®ç¼ç é®é¢ã</li>
<li><strong>å¢å¼ºæ¨¡åçå¯è§£éæ§ï¼</strong> çè®ºåæåæ³¨æåå¯è§åè¡¨æï¼WEF-PEè½å¸¦æ¥æ´è¿è´¯çè¯­ä¹ç¦ç¹åæ´å¼ºçå ä½å½çº³åç½®ï¼æå©äºçè§£ViTå¦ä½å¤çç©ºé´ä¿¡æ¯ã</li>
<li><strong>ä¸ºå¶ä»ç»æåæ°æ®ç¼ç æä¾åé´ï¼</strong> è¿ç§å©ç¨ç¹å®æ°å­¦å½æ°ç¹æ§æ¥ç¼ç æ°æ®ç»æçææ³ï¼å¯è½è¢«æ¨å¹¿å°å¶ä»éè¦ä¿çå¤æç»æä¿¡æ¯çé¢åï¼ä¾å¦å¾ç¥ç»ç½ç»ã3Dç¹äºå¤çç­ã</li>
</ul>
<p><strong>4. å¯è½åççç¸å³é¢åæåºç¨</strong></p>
<ul>
<li><strong>å¾ååç±»ãç®æ æ£æµãè¯­ä¹åå²ï¼</strong> ä½ä¸ºViTçåºç¡ç»ä»¶ï¼WEF-PEå°ç´æ¥æåè¿äºæ ¸å¿è®¡ç®æºè§è§ä»»å¡çæ§è½ã</li>
<li><strong>å»å­¦å¾ååæï¼</strong> å¨å»å­¦å¾åä¸­ï¼ç²¾ç¡®çç©ºé´å³ç³»åå ä½ç»æè³å³éè¦ï¼WEF-PEæææé«è¯æ­ååå²çåç¡®æ§ã</li>
<li><strong>é¥æå¾åå¤çï¼</strong> é¥æå¾åéå¸¸å·æå¤§å°ºåº¦ãéå¤æ¨¡å¼åå¹³ç§»ä¸åæ§ï¼WEF-PEçåå¨ææ§ç¹æ§å¯è½å¨æ­¤ç±»ä»»å¡ä¸­è¡¨ç°åºè²ã</li>
<li><strong>è§é¢çè§£ï¼</strong> å¦æè½å°äºç»´çWEF-PEæ©å±å°ä¸ç»´ï¼ç©ºé´+æ¶é´ï¼ï¼åå¯è½å¯¹è§é¢Transformerä¸­çæ¶ç©ºä½ç½®ç¼ç äº§çç§¯æå½±åã</li>
<li><strong>å¾åçæä¸ç¼è¾ï¼</strong> æ´å¥½çç©ºé´çè§£æå©äºçææ´å·å ä½ä¸è´æ§åçå®æçå¾åã</li>
</ul>
<p><strong>5. ä»æè¦ä¸­å¯æ¨æ­çå±éæ§</strong></p>
<ul>
<li><strong>è®¡ç®å¤ææ§ï¼</strong> Weierstrassæ¤­åå½æ°å¨æ°å­¦ä¸è¾ä¸ºå¤æï¼å¶è®¡ç®å¼éå¯è½é«äºç®åçå¯å­¦ä¹ åµå¥ææ­£å¼¦/ä½å¼¦ç¼ç ï¼æè¦ä¸­æªæåå·ä½çè®¡ç®æçå¯¹æ¯ã</li>
<li><strong>åæ°éæ©ä¸å­¦ä¹ ï¼</strong> æ¤­åå½æ°éå¸¸æ¶åå¨æãæ¨¡æ°ç­åæ°ãæè¦ä¸­æªè¯¦ç»è¯´æè¿äºåæ°æ¯å¦ä½ç¡®å®ï¼ä¾å¦ï¼åºå®ãå¯å­¦ä¹ æéè¿æç§ä¼åè¿ç¨å¾å°ï¼ï¼è¿å¯è½å¼å¥é¢å¤çå¤ææ§æè°ä¼é¾åº¦ã</li>
<li><strong>éç¨æ§ä¸æ©å±æ§ï¼</strong> è®ºæå¼ºè°äºâåå¨ææ§âä¸2Då¾åçå¥åï¼ä½å¶ç´æ¥åºç¨äº3Dæ°æ®ï¼å¦ç¹äºãä½ç´ ï¼æéç½æ ¼ç»ææ°æ®çè½ååæ¹æ³æªå¨æè¦ä¸­æåï¼å¯è½éè¦è¿ä¸æ­¥ççè®ºåæ¹æ³æ©å±ã</li>
<li><strong>çè®ºä¸å®è·µçå¹³è¡¡ï¼</strong> å°½ç®¡å¼ºè°äºâæ°å­¦åçâï¼ä½å¨å®éåºç¨ä¸­ï¼å¦ä½å¹³è¡¡çè®ºçä¸¥è°¨æ§ä¸å·¥ç¨å®ç°çæçåé²æ£æ§ï¼æ¯ææå¤ææ°å­¦æ¹æ³é¢ä¸´çææã</li>
</ul>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose Weierstrass Elliptic Function Positional Encoding (WEF-PE), a
mathematically principled approach that directly addresses two-dimensional
coordinates through natural complex domain representation, where the doubly
periodic properties of elliptic functions align remarkably with translational
invariance patterns commonly observed in visual data.</li>
<li>Our method exploits the
non-linear geometric nature of elliptic functions to encode spatial distance
relationships naturally, while the algebraic addition formula enables direct
derivation of relative positional information between arbitrary patch pairs
from their absolute encodings.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="http://arxiv.org/pdf/2508.19167v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2508.19167v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2508.18421v1'></a></p>
<h2 id="why-relational-graphs-will-save-the-next-generation-of-vision-foundation-models"><a href="https://arxiv.org/abs/2508.18421v1">Why Relational Graphs Will Save the Next Generation of Vision Foundation Models?</a></h2>
<p><strong>Authors:</strong> Fatemeh Ziaeetabar</p>
<p><strong>Published:</strong> 2025-08-25</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Vision foundation models (FMs) have become the predominant architecture in
computer vision, providing highly transferable representations learned from
large-scale, multimodal corpora. Nonetheless, they exhibit persistent
limitations on tasks that require explicit reasoning over entities, roles, and
spatio-temporal relations. Such relational competence is indispensable for
fine-grained human activity recognition, egocentric video understanding, and
multimodal medical image analysis, where spatial, temporal, and semantic
dependencies are decisive for performance. We advance the position that
next-generation FMs should incorporate explicit relational interfaces,
instantiated as dynamic relational graphs (graphs whose topology and edge
semantics are inferred from the input and task context). We illustrate this
position with cross-domain evidence from recent systems in human manipulation
action recognition and brain tumor segmentation, showing that augmenting FMs
with lightweight, context-adaptive graph-reasoning modules improves
fine-grained semantic fidelity, out of distribution robustness,
interpretability, and computational efficiency relative to FM only baselines.
Importantly, by reasoning sparsely over semantic nodes, such hybrids also
achieve favorable memory and hardware efficiency, enabling deployment under
practical resource constraints. We conclude with a targeted research agenda for
FM graph hybrids, prioritizing learned dynamic graph construction, multi-level
relational reasoning (e.g., part object scene in activity understanding, or
region organ in medical imaging), cross-modal fusion, and evaluation protocols
that directly probe relational competence in structured vision tasks.</p>
<p><strong>Analysis:</strong></p>
<p>è¿ç¯è®ºææè¦æåºäºä¸ç§å³äºä¸ä¸ä»£è§è§åºç¡æ¨¡åï¼Vision Foundation Models, FMsï¼åå±æ¹åçæ·±å»è§è§£ãä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå¯¹è¯¥æè¦çåæå¦ä¸ï¼</p>
<hr />
<h3 id="why-relational-graphs-will-save-the-next-generation-of-vision-foundation-models_1">è®ºææè¦åæï¼Why Relational Graphs Will Save the Next Generation of Vision Foundation Models?</h3>
<p><strong>1. è®ºææ ¸å¿è´¡ç®çç®ææ»ç» (Concise Summary of Main Contribution)</strong></p>
<p>è®ºææåºå½åè§è§åºç¡æ¨¡åå¨éè¦å®ä½ãè§è²åæ¶ç©ºå³ç³»æ¾å¼æ¨ççä»»å¡ä¸å­å¨å±éãä¸ºè§£å³æ­¤é®é¢ï¼è®ºææåºä¸ä¸ä»£åºç¡æ¨¡ååºæ´åå¨æå³ç³»å¾ï¼dynamic relational graphsï¼ä½ä¸ºæ¾å¼å³ç³»æ¥å£ãè¿ç§æ··åæ¨¡åè½æ¾èæåç»ç²åº¦è¯­ä¹çè§£ãæ³åè½åãå¯è§£éæ§åè®¡ç®æçï¼ä»èåæç°æFMsçä¸è¶³ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®º (Key Innovation or Methodological Approach)</strong></p>
<p>æ ¸å¿åæ°å¨äºæåºå°âå¨æå³ç³»å¾âï¼dynamic relational graphsï¼ä½ä¸ºæ¾å¼å³ç³»æ¥å£æ´åå°è§è§åºç¡æ¨¡åä¸­ãè¿äºå¾çææç»æåè¾¹è¯­ä¹æ¯æ ¹æ®è¾å¥åä»»å¡ä¸ä¸æå¨ææ¨æ­çãéè¿è¿ç§è½»éçº§ãä¸ä¸æèªéåºçå¾æ¨çæ¨¡åå¢å¼ºåºç¡æ¨¡åï¼å®ç°äºå¯¹å®ä½ãè§è²åæ¶ç©ºå³ç³»çç¨çä¸é«æçæ¨çï¼ä»èå¼¥è¡¥äºFMså¨å¤æå³ç³»çè§£ä¸çä¸è¶³ã</p>
<p><strong>3. å¯¹é¢åæ½å¨å½±å (Potential Impact on the Field)</strong></p>
<p>è¿é¡¹ç ç©¶æææ¨å¨è§è§åºç¡æ¨¡åèå¼çæ¼è¿ï¼ä½¿å¶ä»ä¸»è¦ä¾èµå¤§è§æ¨¡æ°æ®å­¦ä¹ éå¼è¡¨ç¤ºï¼è½¬åè½å¤è¿è¡æ¾å¼ãç»æåæ¨çãè¿å°æ¾èæå®½åºç¡æ¨¡åå¨éè¦å¤æå³ç³»çè§£ï¼å¦äººç±»æ´»å¨è¯å«ãå»çå½±ååæï¼é¢åçåºç¨èå´åæ§è½ä¸éãåæ¶ï¼æåæ¨¡åçé²æ£æ§ãå¯è§£éæ§åèµæºæçï¼å¯¹äºåºç¡æ¨¡åå¨å®éåºæ¯ä¸­çé¨ç½²å·æéè¦æä¹ï¼å¯è½å¼é¢ä¸ä¸ä»£FMsçè®¾è®¡æ¹åã</p>
<p><strong>4. ç¸å³é¢åæåºç¨ (Related Areas or Applications that Might Benefit)</strong></p>
<ul>
<li><strong>ç»ç²åº¦äººç±»æ´»å¨è¯å« (Fine-grained Human Activity Recognition):</strong> çè§£å¤æå¨ä½åºåä¸­çå®ä½ãå·¥å·ãäº¤äºåæ¶åºå³ç³»ã</li>
<li><strong>ç¬¬ä¸äººç§°è§è§è§é¢çè§£ (Egocentric Video Understanding):</strong> åæä½©æ´èè§è§ä¸çç©ä½äº¤äºãæå¾åç¯å¢å³ç³»ã</li>
<li><strong>å¤æ¨¡æå»å­¦å¾ååæ (Multimodal Medical Image Analysis):</strong> æ´åä¸åæ¨¡æï¼å¦MRIãCTï¼ä¿¡æ¯ï¼æ¨ççç¶ãå¨å®ä¹é´çç©ºé´ãè¯­ä¹å³ç³»ï¼ä¾å¦èè¿ç¤åå²åç¾çè¯æ­ã</li>
<li><strong>æºå¨äººæä½ä¸å·èº«æºè½ (Robotic Manipulation and Embodied AI):</strong> è§ååæ§è¡å¤æä»»å¡ï¼éè¦çè§£ç©ä½å±æ§ãç¯å¢çº¦æåæä½åºåã</li>
<li><strong>åºæ¯å¾çæä¸è§è§é®ç­ (Scene Graph Generation and Visual Question Answering):</strong> æ´åç¡®å°ææå¾åä¸­çå®ä½åå¶å³ç³»ï¼æ¯ææ´æ·±å±æ¬¡çè¯­ä¹çè§£åæ¨çã</li>
</ul>
<p><strong>5. ä»æè¦ä¸­å¯æ¨æ­çå±éæ§ (Limitations that Can Be Inferred from the Abstract)</strong></p>
<ul>
<li><strong>æ§è´¨ä¸ºå±ææ§æç»¼è¿°æ§ (Prospective/Review Nature):</strong> é´äºåå¸æ¥ææ¯2025å¹´ï¼ä¸æè¦ä¸­ä½¿ç¨äºâWe advance the positionâãâWe illustrate this position with cross-domain evidenceâåâWe conclude with a targeted research agendaâç­è¡¨è¿°ï¼è¿ç¯è®ºææ´åæ¯ä¸ç¯æåºç ç©¶æ¹åãæ»ç»ç°æè¯æ®å¹¶è§åæªæ¥è·¯çº¿å¾çå±ææ§æç»¼è¿°æ§æç« ï¼èéä¸ç¯æåºå¨æ°æ¨¡åæ¶æå¹¶æä¾å¤§éæ°å®éªç»æçå®è¯è®ºæãå¶âè¯æ®âå¯è½æ¥èªå¯¹ç°æå·¥ä½çç»¼ååæï¼èéæ¬æé¦æ¬¡æåºçæ°å®éªã</li>
<li><strong>å¨æå¾æå»ºçå¤ææ§ (Complexity of Dynamic Graph Construction):</strong> æè¦ä¸­å¼ºè°äºâlearned dynamic graph constructionâæ¯æªæ¥çç ç©¶éç¹ï¼è¿æç¤ºäºå¦ä½é«æãåç¡®å°ä»åå§è¾å¥åä»»å¡ä¸ä¸æä¸­å¨ææ¨æ­åºæä¼çå¾ææåè¾¹è¯­ä¹ï¼æ¬èº«å°±æ¯ä¸ä¸ªå¤æä¸å°æªå®å¨è§£å³çææã</li>
<li><strong>âè½»éçº§âçå®ä¹ä¸æè¡¡ (Definition and Trade-offs of "Lightweight"):</strong> å°½ç®¡å£°ç§°âlightweightâåâfavorable memory and hardware efficiencyâï¼ä½å¨å®éåºç¨ä¸­ï¼å¾æ¨çæ¨¡åçå¤ææ§ãå¾çå¤§å°ä»¥åä¸åºç¡æ¨¡åèåçå¼éï¼ä»å¯è½å¸¦æ¥é¢å¤çè®¡ç®è´æï¼éè¦ä»ç»çå·¥ç¨ä¼ååæè¡¡ã</li>
<li><strong>è¯ä¼°åè®®çç¼ºå¤± (Lack of Established Evaluation Protocols):</strong> æè¦ä¸­æå°éè¦âevaluation protocols that directly probe relational competenceâï¼è¿è¡¨æç®åå¯è½ç¼ºä¹æ ååçãè½å¤ååè¡¡éæ¨¡åå³ç³»æ¨çè½åçåºååè¯ä¼°æ¹æ³ï¼è¿ä¼ç»ç ç©¶è¿å±å¸¦æ¥ææã</li>
</ul>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>Importantly, by reasoning sparsely over semantic nodes, such hybrids also
achieve favorable memory and hardware efficiency, enabling deployment under
practical resource constraints.</li>
<li>We conclude with a targeted research agenda for
FM graph hybrids, prioritizing learned dynamic graph construction, multi-level
relational reasoning (e.g., part object scene in activity understanding, or
region organ in medical imaging), cross-modal fusion, and evaluation protocols
that directly probe relational competence in structured vision tasks.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="http://arxiv.org/pdf/2508.18421v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2508.18421v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2508.17975v1'></a></p>
<h2 id="enhanced-drift-aware-computer-vision-architecture-for-autonomous-driving"><a href="https://arxiv.org/abs/2508.17975v1">Enhanced Drift-Aware Computer Vision Architecture for Autonomous Driving</a></h2>
<p><strong>Authors:</strong> Md Shahi Amran Hossain, Abu Shad Ahammed, Sayeri Mukherjee, Roman Obermaisser</p>
<p><strong>Published:</strong> 2025-08-25</p>
<p><strong>Categories:</strong> cs.CV, math.LO</p>
<p><strong>Abstract:</strong></p>
<p>The use of computer vision in automotive is a trending research in which
safety and security are a primary concern. In particular, for autonomous
driving, preventing road accidents requires highly accurate object detection
under diverse conditions. To address this issue, recently the International
Organization for Standardization (ISO) released the 8800 norm, providing
structured frameworks for managing associated AI relevant risks. However,
challenging scenarios such as adverse weather or low lighting often introduce
data drift, leading to degraded model performance and potential safety
violations. In this work, we present a novel hybrid computer vision
architecture trained with thousands of synthetic image data from the road
environment to improve robustness in unseen drifted environments. Our dual mode
framework utilized YOLO version 8 for swift detection and incorporated a
five-layer CNN for verification. The system functioned in sequence and improved
the detection accuracy by more than 90\% when tested with drift-augmented road
images. The focus was to demonstrate how such a hybrid model can provide better
road safety when working together in a hybrid structure.</p>
<p><strong>Analysis:</strong></p>
<p>å¥½çï¼ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå°å¯¹è¿ç¯è®ºææè¦è¿è¡ææ¯æ§åæã</p>
<hr />
<h3 id="enhanced-drift-aware-computer-vision-architecture-for-autonomous-driving_1">è®ºææè¦åæï¼Enhanced Drift-Aware Computer Vision Architecture for Autonomous Driving</h3>
<p><strong>1. è®ºæä¸»è¦è´¡ç®çç®ææ»ç» (2-3 å¥è¯)</strong></p>
<p>è¿ç¯è®ºææåºäºä¸ç§æ°é¢çæ··åè®¡ç®æºè§è§æ¶æï¼æ¨å¨è§£å³èªå¨é©¾é©¶ä¸­å æ°æ®æ¼ç§»ï¼å¦æ¶å£å¤©æ°ãä½åç§ï¼å¯¼è´çç©ä½æ£æµæ§è½ä¸éåæ½å¨å®å¨é®é¢ãè¯¥æ¶æç»åäºYOLOv8è¿è¡å¿«éæ£æµåä¸ä¸ªäºå±CNNè¿è¡éªè¯ï¼éè¿åºååå·¥ä½æµç¨ååææ°æ®è®­ç»ï¼æ¾èæåäºå¨æ¼ç§»å¢å¼ºå¾åä¸çæ£æµåç¡®çï¼ä»èå¢å¼ºäºéè·¯å®å¨æ§ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³å­¦æ¹æ³</strong></p>
<p>è¯¥ç ç©¶çæ ¸å¿åæ°å¨äºå¶<strong>åæ¨¡ææ··åæ¶æï¼dual mode hybrid architectureï¼</strong>å<strong>æ¼ç§»æç¥ï¼drift-awareï¼</strong>çè®¾è®¡çå¿µãå·ä½æ¹æ³åæ¬ï¼
*   <strong>æ··åæ¶æä¸åºååéªè¯ï¼</strong> ç³»ç»éç¨YOLOv8ä½ä¸ºç¬¬ä¸é¶æ®µï¼å®ç°å¿«éç©ä½æ£æµï¼éåï¼ä¸ä¸ªä¸é¨çäºå±å·ç§¯ç¥ç»ç½ç»ï¼CNNï¼ä½ä¸ºç¬¬äºé¶æ®µï¼å¯¹YOLOv8çæ£æµç»æè¿è¡éªè¯ãè¿ç§âå¿«éæ£æµ + æ·±åº¦éªè¯âçåºååå·¥ä½æµæ¯å¶ç¬ç¹ä¹å¤ï¼æ¨å¨å¹³è¡¡éåº¦ä¸é²æ£æ§ã
*   <strong>åææ°æ®è®­ç»ï¼</strong> ä¸ºäºæé«æ¨¡åå¨âæªè§è¿çæ¼ç§»ç¯å¢ï¼unseen drifted environmentsï¼âä¸­çé²æ£æ§ï¼è¯¥æ¶æå©ç¨äºæ°åå¼ åæå¾åæ°æ®è¿è¡è®­ç»ãè¿æå©äºæ¨¡åå­¦ä¹ å¹¶éåºåç§æ½å¨çæ¼ç§»æ¡ä»¶ï¼èæ éä¾èµé¾ä»¥è·åççå®ä¸çæ¼ç§»æ°æ®ã
*   <strong>ä¸æ³¨äºæ°æ®æ¼ç§»é²æ£æ§ï¼</strong> è®ºææç¡®å°æ°æ®æ¼ç§»ï¼ç±æ¶å£å¤©æ°ãä½åç§ç­å¼èµ·ï¼ä½ä¸ºæ ¸å¿ææï¼å¹¶è®¾è®¡æ¶ææ¥ç´æ¥è§£å³è¿ä¸é®é¢ï¼è¿ä¸ISO 8800ç­æ°å´æ åå¯¹AIé£é©ç®¡ççå³æ³¨ç¸å»åã</p>
<p><strong>3. å¯¹é¢åæ½å¨å½±å</strong></p>
<ul>
<li><strong>æåèªå¨é©¾é©¶å®å¨æ§ï¼</strong> éè¿ææåºå¯¹æ°æ®æ¼ç§»ï¼è¯¥ç ç©¶æææ¾èæé«èªå¨é©¾é©¶ç³»ç»å¨å¤æåæ¶å£æ¡ä»¶ä¸çç©ä½æ£æµå¯é æ§ï¼ç´æ¥å³ç³»å°è¡è½¦å®å¨ï¼å¹¶å¯è½æä¸ºæªæ¥ADç³»ç»è®¾è®¡ä¸­çä¸ä¸ªéè¦èéã</li>
<li><strong>æ¨å¨æ··åæ¨¡åèå¼ï¼</strong> è¿ç§âå¿«éæ£æµå¨ + éªè¯å¨âçæ··åæ¶æä¸ºå¶ä»å¯¹å®æ¶æ§ãåç¡®æ§åé²æ£æ§é½æé«è¦æ±çè®¡ç®æºè§è§åºç¨æä¾äºæ°çè®¾è®¡èå¼ã</li>
<li><strong>å¼ºè°åææ°æ®ä»·å¼ï¼</strong> è®ºæåæ¬¡è¯æäºåææ°æ®å¨è®­ç»é²æ£æ¨¡åï¼å°¤å¶æ¯å¨å¤çç½è§æé¾ä»¥è·åçæ¼ç§»æ°æ®æ¹é¢çå·¨å¤§æ½åã</li>
<li><strong>ç¬¦åè¡ä¸æ åï¼</strong> æåISO 8800è¡¨æè¯¥ç ç©¶å·æå¾å¼ºçå®éåºç¨å¯¼åï¼å¶ææå¯è½æå©äºèªå¨é©¾é©¶ç³»ç»æ»¡è¶³æªæ¥çå®å¨åé£é©ç®¡çæ åã</li>
</ul>
<p><strong>4. ç¸å³é¢åæåºç¨</strong></p>
<p>é¤äºèªå¨é©¾é©¶ï¼ä»¥ä¸é¢åæåºç¨ä¹å¯è½ä»è¿é¡¹ç ç©¶ä¸­åçï¼
*   <strong>å·¥ä¸èªå¨åä¸æºå¨äººï¼</strong> å¨çäº§çº¿æä»åºä¸­ï¼åç§ãç°å°ãç£¨æç­ç¯å¢ååå¯è½å¯¼è´æ°æ®æ¼ç§»ï¼å½±åæºå¨äººçè§è§æç¥åæä½ç²¾åº¦ã
*   <strong>æºè½çæ§ç³»ç»ï¼</strong> æ·å¤çæ§æåå¤´å¸¸é¢ä¸´å¤©æ°ãæ¼å¤ãå­£èååç­æ¼ç§»ï¼è¯¥æ¹æ³å¯ç¨äºæé«å¼å¸¸äºä»¶æ£æµçé²æ£æ§ã
*   <strong>å»çå½±ååæï¼</strong> ä¸åçè®¾å¤ãæ«æåæ°ææ£èççååå¯è½å¼å¥æ°æ®æ¼ç§»ï¼å½±åç¾çè¯æ­çåç¡®æ§ã
*   <strong>èªç©ºèªå¤©ä¸æ äººæºï¼</strong> å¨å¤æå¤§æ°æ¡ä»¶ææªç¥ç¯å¢ä¸­æ§è¡ä»»å¡æ¶ï¼è§è§ç³»ç»éè¦æé«çé²æ£æ§ã
*   <strong>ä»»ä½å¨éåæ§ç¯å¢ä¸­é¨ç½²çè®¡ç®æºè§è§ç³»ç»ï¼</strong> å¡æ¯ç¯å¢æ¡ä»¶å¤åãæ°æ®åå¸å¯è½éæ¶é´ææ¡ä»¶ååçåºæ¯ï¼è¿ç§æ¼ç§»æç¥åéªè¯çæ¶æé½å·æåé´æä¹ã</p>
<p><strong>5. ä»æè¦ä¸­å¯æ¨æ­çå±éæ§</strong></p>
<ul>
<li><strong>åææ°æ®ä¸çå®ä¸ççå·®è·ï¼Domain Gapï¼ï¼</strong> å°½ç®¡åææ°æ®æå©äºè§£å³æ¼ç§»é®é¢ï¼ä½åææ°æ®ä¸çå®ä¸çæ°æ®ä¹é´éå¸¸å­å¨é¢åå·®è·ãæ¨¡åå¨âæ¼ç§»å¢å¼ºçéè·¯å¾åâä¸è¡¨ç°åºè²ï¼ä½å¶å¨çå®ä¸çãæªè§è¿çãé«åº¦å¤æçæ¼ç§»æ¡ä»¶ä¸çæ³åè½åä»éè¿ä¸æ­¥éªè¯ã</li>
<li><strong>æ§è½ææ çç¸å¯¹æ§ï¼</strong> âæ£æµåç¡®çæé«äº90%ä»¥ä¸âæ¯ä¸ä¸ªç¸å¯¹æåãæè¦æªæä¾åºçº¿æ¨¡åçç»å¯¹åç¡®çï¼ä¹æªè¯´ææå90%æ¯åºäºåªä¸ªåå§å¼ãä¾å¦ï¼ä»10%æåå°19%ä¹æ¯90%çæåï¼ä½ç»å¯¹æ§è½å¯è½ä»ä¸è¶³ä»¥æ»¡è¶³èªå¨é©¾é©¶çéæ±ã</li>
<li><strong>åºååå¤ççå®æ¶æ§èéï¼</strong> å°½ç®¡YOLOv8ä»¥âswift detectionâèç§°ï¼ä½å¢å ä¸ä¸ªäºå±CNNçâéªè¯âæ­¥éª¤ï¼å¿ç¶ä¼å¼å¥é¢å¤çè®¡ç®å»¶è¿ãå¯¹äºèªå¨é©¾é©¶è¿ç§å¯¹å®æ¶æ§è¦æ±æé«çåºç¨ï¼è¿ç§å»¶è¿æ¯å¦å¨å¯æ¥åèå´åï¼ä»¥åå¦ä½ä¼åä»¥æ»¡è¶³å®æ¶æ§è¦æ±ï¼æ¯éè¦å³æ³¨çé®é¢ã</li>
<li><strong>äºå±CNNçéªè¯è½åï¼</strong> æè¦ä¸­å¯¹âäºå±CNNâçæè¿°ç¸å¯¹ç®åãå¶éªè¯æºå¶ãå¤æåº¦åå¯¹åç§æ¼ç§»æ¨¡å¼çé²æ£æ§å¦ä½ï¼ä»¥åå®æ¯å¦è¶³ä»¥åºå¯¹èªå¨é©¾é©¶ä¸­å¯è½åºç°çæç«¯åå¤æ ·åçæ¼ç§»æåµï¼ä»æå¾è¯¦ç»è¯´æã</li>
<li><strong>æ¼ç§»ç±»åçè¦çèå´ï¼</strong> æè¦æå°äºâæ¶å£å¤©æ°æä½åç§âä½ä¸ºæ¼ç§»æ¥æºï¼ä½æªè¯¦ç»è¯´ææ¨¡åè½å¤åºå¯¹çæ¼ç§»ç±»ååç¨åº¦ãä¾å¦ï¼å¯¹äºä¼ æå¨æéãé®æ¡ãå¯¹ææ§æ»å»ç­å¶ä»å½¢å¼çâæ¼ç§»âæå¼å¸¸æåµï¼è¯¥æ¶æçæææ§å¦ä½ï¼</li>
</ul>
<hr />
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this work, we present a novel hybrid computer vision
architecture trained with thousands of synthetic image data from the road
environment to improve robustness in unseen drifted environments.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="http://arxiv.org/pdf/2508.17975v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2508.17975v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2508.18249v1'></a></p>
<h2 id="scene-agnostic-traversability-labeling-and-estimation-via-a-multimodal-self-supervised-framework"><a href="https://arxiv.org/abs/2508.18249v1">Scene-Agnostic Traversability Labeling and Estimation via a Multimodal Self-supervised Framework</a></h2>
<p><strong>Authors:</strong> Zipeng Fang, Yanbo Wang, Lei Zhao, Weidong Chen</p>
<p><strong>Published:</strong> 2025-08-25</p>
<p><strong>Categories:</strong> cs.RO, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Traversability estimation is critical for enabling robots to navigate across
diverse terrains and environments. While recent self-supervised learning
methods achieve promising results, they often fail to capture the
characteristics of non-traversable regions. Moreover, most prior works
concentrate on a single modality, overlooking the complementary strengths
offered by integrating heterogeneous sensory modalities for more robust
traversability estimation. To address these limitations, we propose a
multimodal self-supervised framework for traversability labeling and
estimation. First, our annotation pipeline integrates footprint, LiDAR, and
camera data as prompts for a vision foundation model, generating traversability
labels that account for both semantic and geometric cues. Then, leveraging
these labels, we train a dual-stream network that jointly learns from different
modalities in a decoupled manner, enhancing its capacity to recognize diverse
traversability patterns. In addition, we incorporate sparse LiDAR-based
supervision to mitigate the noise introduced by pseudo labels. Finally,
extensive experiments conducted across urban, off-road, and campus environments
demonstrate the effectiveness of our approach. The proposed automatic labeling
method consistently achieves around 88% IoU across diverse datasets. Compared
to existing self-supervised state-of-the-art methods, our multimodal
traversability estimation network yields consistently higher IoU, improving by
1.6-3.5% on all evaluated datasets.</p>
<p><strong>Analysis:</strong></p>
<p>è¿ç¯è®ºææåºäºä¸ç§æ°é¢çå¤æ¨¡æèªçç£æ¡æ¶ï¼ç¨äºæºå¨äººå¯éè¡æ§åºåçæ æ³¨ä¸ä¼°è®¡ï¼æ¨å¨è§£å³ç°ææ¹æ³å¨è¯å«ä¸å¯éè¡åºååææå©ç¨å¤æ¨¡æä¿¡æ¯æ¹é¢çä¸è¶³ã</p>
<hr />
<h3 id="1-concise-summary_1">1. è®ºææ ¸å¿è´¡ç®æ»ç» (Concise Summary)</h3>
<p>è¯¥è®ºææåºäºä¸ç§å¤æ¨¡æèªçç£æ¡æ¶ï¼ç¨äºæºå¨äººå¯éè¡æ§åºåçæ æ³¨ä¸ä¼°è®¡ï¼æ¨å¨è§£å³ç°ææ¹æ³å¨è¯å«ä¸å¯éè¡åºååå©ç¨å¤æ¨¡æä¿¡æ¯æ¹é¢çä¸è¶³ãå®éè¿æ´åè¶³è¿¹ãLiDARåç¸æºæ°æ®ä½ä¸ºè§è§åºç¡æ¨¡åçæç¤ºæ¥çæé«è´¨éçä¼ªæ ç­¾ï¼å¹¶å©ç¨è¿äºæ ç­¾è®­ç»ä¸ä¸ªåæµç½ç»ï¼è¾ä»¥ç¨çLiDARçç£ä»¥æé«é²æ£æ§ï¼æç»å¨å¤æ ·åç¯å¢ä¸­å®ç°äºæ¾èçæ§è½æåã</p>
<h3 id="2-key-innovation-or-methodological-approach_1">2. å³é®åæ°ææ¹æ³è®º (Key Innovation or Methodological Approach)</h3>
<p>æ ¸å¿åæ°å¨äºå¶å¤æ¨¡æèªçç£æ¡æ¶ï¼ç¹å«æ¯å¨ä¼ªæ ç­¾çæåç½ç»è®­ç»é¶æ®µã
1.  <strong>åæ°æ§ä¼ªæ ç­¾çæï¼</strong> è¯¥æ¹æ³åæ°æ§å°å©ç¨è¶³è¿¹ï¼footprintï¼ãLiDARåç¸æºæ°æ®ä½ä¸ºæç¤ºï¼promptsï¼ï¼é©±å¨ä¸ä¸ª<strong>è§è§åºç¡æ¨¡å</strong>æ¥çæç»åè¯­ä¹åå ä½çº¿ç´¢çå¯éè¡æ§æ ç­¾ãè¿æ¯ä¸ç§é«æä¸åºæ¯æ å³çä¼ªæ ç­¾çæç­ç¥ï¼å°¤å¶è§£å³äºä¼ ç»èªçç£æ¹æ³é¾ä»¥ææä¸å¯éè¡åºåç¹å¾çé®é¢ã
2.  <strong>è§£è¦å¼å¤æ¨¡æå­¦ä¹ ç½ç»ï¼</strong> æåºçåæµç½ç»ä»¥<strong>è§£è¦æ¹å¼</strong>èåå­¦ä¹ ä¸åæ¨¡æçç¹å¾ï¼å¢å¼ºäºå¯¹å¤æ ·åå¯éè¡æ§æ¨¡å¼çè¯å«è½åï¼ååå©ç¨äºå¼æä¼ æå¨çäºè¡¥ä¼å¿ã
3.  <strong>ç¨çLiDARçç£ç¼è§£åªå£°ï¼</strong> å¼å¥<strong>ç¨çLiDARçç£</strong>æ¥ææç¼è§£ä¼ªæ ç­¾å¸¦æ¥çåªå£°ï¼è¿ä¸æ­¥æåäºæ¨¡åçé²æ£æ§ååç¡®æ§ï¼å¼¥è¡¥äºçº¯èªçç£æ¹æ³å¯è½å­å¨çè¯¯å·®ã</p>
<h3 id="3-potential-impact-on-the-field_1">3. å¯¹é¢åæ½å¨å½±å (Potential Impact on the Field)</h3>
<p>è¯¥ç ç©¶å¯¹æºå¨äººå¯¼èªé¢åå·æéè¦å½±åï¼å®æä¾äºä¸ç§æ´é²æ£ãæ´éç¨çå¯éè¡æ§ä¼°è®¡æ¹æ³ï¼ä½¿æºå¨äººå¨æªç¥åå¤æå°å½¢ä¸­èªä¸»å¯¼èªçè½åå¾å°æ¾èæåãå¶èªçç£çä¼ªæ ç­¾çæç­ç¥ï¼ç¹å«æ¯ç»åè§è§åºç¡æ¨¡ååå¤æ¨¡ææç¤ºï¼ä¸ºåå°æ°æ®æ æ³¨ææ¬å¼è¾äºæ°éå¾ï¼å¯¹è®¡ç®æºè§è§é¢åä¸­å¶ä»éè¦å¤§éæ æ³¨çæç¥ä»»å¡å·æåé´æä¹ãæ­¤å¤ï¼å®ä¹ä¸ºå¤æ¨¡ææ°æ®èåå¨æºå¨äººæç¥ä¸­çåºç¨æ ç«äºæ°çèä¾ã</p>
<h3 id="4-related-areas-or-applications_1">4. ç¸å³é¢åæåºç¨ (Related Areas or Applications)</h3>
<p>è¿é¡¹ç ç©¶çææå¯ä»¥å¹¿æ³åºç¨äºä»¥ä¸é¢åï¼
*   <strong>èªå¨é©¾é©¶ä¸æ äººè½¦ï¼</strong> ç¹å«æ¯å¨éç»æåéè·¯ãè¶éç¯å¢æå¤æåå¸åºåçè·¯å¾è§ååå³ç­ã
*   <strong>æææºå¨äººï¼</strong> å¨ç¾ååºå¢ãå´å²å°å½¢ä¸­è¿è¡æç´¢åææ´ä»»å¡ã
*   <strong>ç©ºé´æ¢ç´¢ä¸åäºæºå¨äººï¼</strong> å¨æªç¥ææç«¯ç¯å¢ä¸­è¿è¡èªä¸»æ¢æµåè¡å¨ã
*   <strong>åä¸æºå¨äººï¼</strong> å¨åç°ä¸­è¯å«å¯éè¡åºåï¼è¿è¡ä½ç©çæµåç®¡çã
*   <strong>ç©æµä¸ééæºå¨äººï¼</strong> æåå¨å¤æ ·ååå¸åéåºç¯å¢ä¸­âæåä¸å¬éâééçé²æ£æ§ã</p>
<h3 id="5-inferred-limitations">5. ä»æè¦ä¸­å¯æ¨æ­çå±éæ§ (Inferred Limitations)</h3>
<p>å°½ç®¡è¯¥æ¹æ³åå¾äºæ¾èææï¼ä½ä»æè¦ä¸­ä»å¯æ¨æ­åºä¸äºæ½å¨å±éæ§ï¼
*   <strong>ä¼ªæ ç­¾è´¨éçä¾èµæ§ï¼</strong> ä¼ªæ ç­¾ççæè´¨éé«åº¦ä¾èµäºæä½¿ç¨çâè§è§åºç¡æ¨¡åâçæ§è½åå¶å¯¹å¤æ¨¡ææç¤ºççè§£è½åãå¦æåºç¡æ¨¡åå¨ç¹å®åºæ¯æç©ä½ä¸è¡¨ç°ä¸ä½³ï¼çæçä¼ªæ ç­¾å¯è½ä¼å¼å¥ç³»ç»æ§è¯¯å·®ï¼ä»èå½±ååç»­ç½ç»çè®­ç»ææã
*   <strong>âè§£è¦âå­¦ä¹ çæè¡¡ï¼</strong> æè¦æå°åæµç½ç»ä»¥âè§£è¦æ¹å¼âå­¦ä¹ ä¸åæ¨¡æï¼è¿å¯è½æå©äºæåæ¨¡æç¹å®ç¹å¾ï¼ä½ä¹å¯è½å¨ä¸å®ç¨åº¦ä¸éå¶äºä¸åæ¨¡æä¹é´æ´æ·±å±æ¬¡ãæ´å¤æçäº¤äºåä¿¡æ¯èåï¼ä»èå½±åæç»çå³ç­é²æ£æ§ã
*   <strong>ç¨çLiDARçç£çæ§è´¨ï¼</strong> å°½ç®¡ç¨çLiDARçç£ç¨äºç¼è§£ä¼ªæ ç­¾åªå£°ï¼ä½å¶å·ä½æ¥æºï¼æ¯å®å¨èªå¨çæè¿æ¯éè¦å°éäººå·¥å¹²é¢ï¼åç¨çç¨åº¦å¯¹æ¨¡åæ§è½çå½±åï¼ä»¥åå¶å¨ä¸ååºæ¯ä¸çæ³åè½åï¼ä»éè¿ä¸æ­¥æ¢è®¨ã
*   <strong>è®¡ç®èµæºéæ±ï¼</strong> æ´åå¤æ¨¡ææ°æ®ãå©ç¨åºç¡æ¨¡åä»¥åè®­ç»åæµç½ç»ï¼å¯è½å¯¹è®¡ç®èµæºï¼è®­ç»åæ¨çï¼æè¾é«è¦æ±ï¼è¿å¯¹äºèµæºåéçæºå¨äººå¹³å°å¯è½æ¯ä¸ä¸ªææã</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address these limitations, we propose a
multimodal self-supervised framework for traversability labeling and
estimation.</li>
<li>Finally,
extensive experiments conducted across urban, off-road, and campus environments
demonstrate the effectiveness of our approach.</li>
<li>Compared
to existing self-supervised state-of-the-art methods, our multimodal
traversability estimation network yields consistently higher IoU, improving by
1.6-3.5% on all evaluated datasets.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="http://arxiv.org/pdf/2508.18249v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2508.18249v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2508.17930v1'></a></p>
<h2 id="learning-to-detect-label-errors-by-making-them-a-method-for-segmentation-and-object-detection-datasets"><a href="https://arxiv.org/abs/2508.17930v1">Learning to Detect Label Errors by Making Them: A Method for Segmentation and Object Detection Datasets</a></h2>
<p><strong>Authors:</strong> Sarina Penquitt, Tobias Riedlinger, Timo Heller, Markus Reischl, Matthias Rottmann</p>
<p><strong>Published:</strong> 2025-08-25</p>
<p><strong>Categories:</strong> cs.LG, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Recently, detection of label errors and improvement of label quality in
datasets for supervised learning tasks has become an increasingly important
goal in both research and industry. The consequences of incorrectly annotated
data include reduced model performance, biased benchmark results, and lower
overall accuracy. Current state-of-the-art label error detection methods often
focus on a single computer vision task and, consequently, a specific type of
dataset, containing, for example, either bounding boxes or pixel-wise
annotations. Furthermore, previous methods are not learning-based. In this
work, we overcome this research gap. We present a unified method for detecting
label errors in object detection, semantic segmentation, and instance
segmentation datasets. In a nutshell, our approach - learning to detect label
errors by making them - works as follows: we inject different kinds of label
errors into the ground truth. Then, the detection of label errors, across all
mentioned primary tasks, is framed as an instance segmentation problem based on
a composite input. In our experiments, we compare the label error detection
performance of our method with various baselines and state-of-the-art
approaches of each task's domain on simulated label errors across multiple
tasks, datasets, and base models. This is complemented by a generalization
study on real-world label errors. Additionally, we release 459 real label
errors identified in the Cityscapes dataset and provide a benchmark for real
label error detection in Cityscapes.</p>
<p><strong>Analysis:</strong></p>
<p>ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå¯¹è¿ç¯è®ºææè¦è¿è¡å¦ä¸åæï¼</p>
<hr />
<h3 id="learning-to-detect-label-errors-by-making-them-a-method-for-segmentation-and-object-detection-datasets_1">è®ºææè¦åæï¼Learning to Detect Label Errors by Making Them: A Method for Segmentation and Object Detection Datasets</h3>
<p><strong>1. è®ºæä¸»è¦è´¡ç®çç®æ´æ»ç» (2-3 å¥è¯)</strong></p>
<p>è¿ç¯è®ºææåºäºä¸ç§ç»ä¸çãåºäºå­¦ä¹ çæ¹æ³ï¼ç¨äºæ£æµç®æ æ£æµãè¯­ä¹åå²åå®ä¾åå²æ°æ®éä¸­å­å¨çæ ç­¾éè¯¯ãå¶æ ¸å¿ææ³æ¯éè¿åçå¼ä¸­æ³¨å¥ä¸åç±»åçåæéè¯¯æ¥è®­ç»ä¸ä¸ªéè¯¯æ£æµæ¨¡åï¼å¹¶å°éè¯¯æ£æµä»»å¡å»ºæ¨¡ä¸ºä¸ä¸ªåºäºå¤åè¾å¥çå®ä¾åå²é®é¢ãè¯¥æ¹æ³æ¨å¨åæç°ææ¹æ³ä»éäºåä¸ä»»å¡ä¸éå­¦ä¹ çå±éæ§ï¼å¹¶éè¿çå®ä¸çéè¯¯ååºåæµè¯æ¥éªè¯å¶æ³åè½åã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®º</strong></p>
<ul>
<li><strong>ç»ä¸çãåºäºå­¦ä¹ çæ¹æ³ï¼</strong> åæäºç°ææ¹æ³éå¸¸ä¸æ³¨äºåä¸è®¡ç®æºè§è§ä»»å¡ï¼å¦ä»è¾¹çæ¡æåç´ çº§æ æ³¨ï¼ä¸éå­¦ä¹ çå±éæ§ï¼æä¾äºä¸ä¸ªå¯åæ¶å¤çç®æ æ£æµãè¯­ä¹åå²åå®ä¾åå²çéç¨æ¡æ¶ã</li>
<li><strong>âéè¿å¶é éè¯¯æ¥å­¦ä¹ æ£æµéè¯¯â (Learning by Making Them)ï¼</strong> è¿æ¯ä¸ä¸ªå·§å¦çèªçç£ææ°æ®å¢å¼ºç­ç¥ãéè¿ç³»ç»å°åçå¼ä¸­æ³¨å¥ä¸åç±»åçæ ç­¾éè¯¯ï¼ä¸ºéè¯¯æ£æµæ¨¡åçæè®­ç»æ°æ®ï¼ä»èé¿åäºå¯¹å¤§éçå®éè¯¯æ æ³¨æ°æ®çä¾èµã</li>
<li><strong>å°éè¯¯æ£æµæ¡æ¶ä¸ºå®ä¾åå²é®é¢ï¼</strong> è¿ç§æ¹æ³è½å¤ç²¾ç¡®å°å®ä½åè¯å«å¾åä¸­ä¸åç±»åçæ ç­¾éè¯¯åºåï¼å°å¶è§ä¸ºç¬ç«çâéè¯¯å®ä¾âï¼è¿æ¯ç®åçåç±»ææ£æµæ¡æ¹æ³æ´å·è¡¨ç°åã</li>
<li><strong>å¤åè¾å¥ (Composite Input)ï¼</strong> è½ç¶æè¦æªè¯¦ç»è¯´æï¼ä½æç¤ºäºéè¿ç»ååå§å¾åä¿¡æ¯åå¯è½åå«éè¯¯ä¿¡æ¯çè¡¨ç¤ºæ¥è®­ç»æ¨¡åï¼ä»¥æé«éè¯¯æ£æµçé²æ£æ§ã</li>
</ul>
<p><strong>3. å¯¹é¢åæ½å¨å½±å</strong></p>
<ul>
<li><strong>æåæ°æ®éè´¨éåæ¨¡åæ§è½ï¼</strong> èªå¨åãç»ä¸çæ ç­¾éè¯¯æ£æµå°æ¾èæé«è®¡ç®æºè§è§æ°æ®éçè´¨éï¼ä»èè®­ç»åºæ´é²æ£ãæ§è½æ´å¥½çæ¨¡åï¼å¹¶åå°å æ°æ®éè¯¯å¯¼è´çæ¨¡ååå·®ã</li>
<li><strong>æ´å¯é çåºåæµè¯ï¼</strong> åå°æ°æ®éä¸­çéè¯¯å°ä½¿åºåæµè¯ç»ææ´å å¬å¹³åå¯ä¿¡ï¼ä¿è¿æ´ææçæ¨¡åæ¯è¾åç ç©¶è¿å±ã</li>
<li><strong>éä½æ°æ®æ æ³¨ææ¬åæ¶é´ï¼</strong> èªå¨åéè¯¯æ£æµå¯ä»¥å¤§å¹åå°äººå·¥è´¨éæ§å¶çéæ±ï¼éä½æ°æ®éåå»ºåç»´æ¤çææ¬ä¸æ¶é´ã</li>
<li><strong>æ¨å¨æ°æ®ä¸­å¿AIåå±ï¼</strong> è¯¥ç ç©¶ä¸å½åâæ°æ®ä¸­å¿AIâçè¶å¿é«åº¦å¥åï¼å¼ºè°äºæ°æ®è´¨éå¨æºå¨å­¦ä¹ ä¸­çæ ¸å¿ä½ç¨ã</li>
<li><strong>ç¤¾åºèµæºè´¡ç®ï¼</strong> è®ºæåå¸äº Cityscapes æ°æ®éä¸­è¯å«åºç 459 ä¸ªçå®æ ç­¾éè¯¯ï¼å¹¶æä¾äºçå®æ ç­¾éè¯¯æ£æµçåºåï¼è¿å°ä¸ºæªæ¥çç ç©¶æä¾å®è´µçèµæºåè¯ä¼°æ åã</li>
</ul>
<p><strong>4. å¯è½åççç¸å³é¢åæåºç¨</strong></p>
<ul>
<li><strong>èªå¨é©¾é©¶ï¼</strong> èªå¨é©¾é©¶æ°æ®éåºå¤§ä¸å¯¹ç²¾åº¦è¦æ±æé«ï¼æ ç­¾éè¯¯å¯è½å¯¼è´ä¸¥éåæãè¯¥æ¹æ³å¯ç¨äºæé«èªå¨é©¾é©¶æç¥æ°æ®éçè´¨éã</li>
<li><strong>å»å­¦å½±ååæï¼</strong> å»å­¦å¾åæ æ³¨çåç¡®æ§ç´æ¥å³ç³»å°è¯æ­åæ²»çï¼éè¯¯æ£æµå¯¹äºç¡®ä¿æ¨¡åå¯é æ§è³å³éè¦ã</li>
<li><strong>å¤§è§æ¨¡æ°æ®éæä¾å/æ æ³¨æå¡ï¼</strong> ä¸é¨ä»äºæ°æ®éåå»ºåè´¨éæ§å¶çå¬å¸å°ç´æ¥åçäºè¿ç§èªå¨åå·¥å·ã</li>
<li><strong>æºå¨äººæç¥ï¼</strong> æºå¨äººéè¦åç¡®çç¯å¢æç¥æ¥æ§è¡ä»»å¡ï¼é«è´¨éçè®­ç»æ°æ®æ¯åºç¡ã</li>
<li><strong>ä»»ä½ä¾èµçç£å­¦ä¹ çå·¥ä¸åºç¨ï¼</strong> å¡æ¯ä½¿ç¨å¤§éæ æ³¨æ°æ®è¿è¡æ¨¡åè®­ç»çè¡ä¸ï¼å¦é¶å®ãå®é²ãåä¸ç­ï¼é½å°ä»æ´å¯é çæ°æ®è´¨éä¸­è·çã</li>
<li><strong>ä¸»å¨å­¦ä¹  (Active Learning) åæ°æ®ç­å± (Data Curation)ï¼</strong> è¯å«åºéè¯¯æ ·æ¬å¯ä»¥æå¯¼éæ°æ æ³¨æä¼åå¤çï¼ä¼åæ°æ®å©ç¨æçã</li>
</ul>
<p><strong>5. ä»æè¦ä¸­å¯æ¨æ­çå±éæ§</strong></p>
<ul>
<li><strong>åæéè¯¯ä¸çå®éè¯¯çå·®è·ï¼</strong> å°½ç®¡è®ºææå°äºå¯¹çå®ä¸çéè¯¯çæ³åç ç©¶ï¼ä½åæéè¯¯æ¯å¦è½å®å¨è¦çææç±»åãææç»å¾®ä¹å¤ççå®ä¸çéè¯¯ï¼ä»¥åæ¨¡åå¯¹æªè§è¿ççå®éè¯¯ç±»åçæ£æµè½åï¼ä»æ¯ä¸ä¸ªæ½å¨çææã</li>
<li><strong>âå¤åè¾å¥âçç»èï¼</strong> æè¦ä¸­æªè¯¦ç»è¯´æâå¤åè¾å¥âçå·ä½ææï¼å¶è®¾è®¡å¯¹æ¹æ³çæææ§åæ³åè½åå¯è½è³å³éè¦ã</li>
<li><strong>è®¡ç®ææ¬ï¼</strong> è®­ç»ä¸ä¸ªåºäºå­¦ä¹ çéè¯¯æ£æµæ¨¡åï¼ç¹å«æ¯å¯¹äºå¤§è§æ¨¡æ°æ®éåå¤æçå®ä¾åå²ä»»å¡ï¼å¯è½éè¦æ¾èçè®¡ç®èµæºåæ¶é´ï¼æè¦ä¸­æªæåè¿æ¹é¢çèéã</li>
<li><strong>éè¯¯ç±»åçç²åº¦ï¼</strong> æè¦ä¸­æå°âä¸åç§ç±»çæ ç­¾éè¯¯âï¼ä½æªå·ä½è¯´æè½æ£æµå°åªäºç²åº¦çéè¯¯ï¼ä¾å¦ï¼æ¯ææ¾çå ä½éè¯¯ï¼è¿æ¯æ´ç»å¾®çè¯­ä¹ä¸ä¸è´æ§ï¼ã</li>
<li><strong>éè¯¯ä¿®æ­£æºå¶ï¼</strong> è®ºæä¸æ³¨äºéè¯¯æ£æµï¼ä½æªæåå¦ä½å°æ£æµå°çéè¯¯ææå°åé¦ç»æ æ³¨åè¿è¡ä¿®æ­£ï¼ææ¯å¦æä¾èªå¨ä¿®æ­£çå»ºè®®ã</li>
</ul>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Current state-of-the-art label error detection methods often
focus on a single computer vision task and, consequently, a specific type of
dataset, containing, for example, either bounding boxes or pixel-wise
annotations.</li>
<li>We present a unified method for detecting
label errors in object detection, semantic segmentation, and instance
segmentation datasets.</li>
<li>In a nutshell, our approach - learning to detect label
errors by making them - works as follows: we inject different kinds of label
errors into the ground truth.</li>
<li>In our experiments, we compare the label error detection
performance of our method with various baselines and state-of-the-art
approaches of each task's domain on simulated label errors across multiple
tasks, datasets, and base models.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="http://arxiv.org/pdf/2508.17930v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2508.17930v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2508.17708v1'></a></p>
<h2 id="catformer-contrastive-adversarial-transformer-for-image-super-resolution"><a href="https://arxiv.org/abs/2508.17708v1">CATformer: Contrastive Adversarial Transformer for Image Super-Resolution</a></h2>
<p><strong>Authors:</strong> Qinyi Tian, Spence Cox, Laura E. Dalton</p>
<p><strong>Published:</strong> 2025-08-25</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Super-resolution remains a promising technique to enhance the quality of
low-resolution images. This study introduces CATformer (Contrastive Adversarial
Transformer), a novel neural network integrating diffusion-inspired feature
refinement with adversarial and contrastive learning. CATformer employs a
dual-branch architecture combining a primary diffusion-inspired transformer,
which progressively refines latent representations, with an auxiliary
transformer branch designed to enhance robustness to noise through learned
latent contrasts. These complementary representations are fused and decoded
using deep Residual-in-Residual Dense Blocks for enhanced reconstruction
quality. Extensive experiments on benchmark datasets demonstrate that CATformer
outperforms recent transformer-based and diffusion-inspired methods both in
efficiency and visual image quality. This work bridges the performance gap
among transformer-, diffusion-, and GAN-based methods, laying a foundation for
practical applications of diffusion-inspired transformers in super-resolution.</p>
<p><strong>Analysis:</strong></p>
<p>ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå¯¹è¿ç¯å³äºCATformerçè®ºææè¦è¿è¡å¦ä¸åæï¼</p>
<hr />
<h3 id="catformer-contrastive-adversarial-transformer-for-image-super-resolution_1">CATformer: Contrastive Adversarial Transformer for Image Super-Resolution</h3>
<p><strong>1. è®ºæä¸»è¦è´¡ç®çç®æ´æ»ç» (2-3 å¥è¯)</strong></p>
<p>CATformeræ¯ä¸ç§æ°é¢çå¾åè¶åè¾¨çç¥ç»ç½ç»ï¼å®å·§å¦å°å°åæ©æ£æ¨¡åå¯åçç¹å¾ç»åãå¯¹ææ§å­¦ä¹ åå¯¹æ¯å­¦ä¹ éæå°ä¸ä¸ªååæ¯Transformeræ¶æä¸­ãè¯¥æ¨¡åéè¿æ¸è¿å¼æ½å¨è¡¨ç¤ºç»ååå¢å¼ºå¯¹åªå£°çé²æ£æ§ï¼å¨æçåè§è§è´¨éä¸åè¶è¶äºç°æçTransformeråæ©æ£å¯åæ¹æ³ãè¿é¡¹å·¥ä½ä¸ºè¶åè¾¨çé¢åæä¾äºä¸ä¸ªå¼ºå¤§çæ··åèå¼è§£å³æ¹æ¡ï¼å¹¶ä¸ºæ©æ£å¯åå¼Transformerçå®éåºç¨å¥ å®äºåºç¡ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³è®ºæ¹æ³</strong></p>
<p>CATformerçæ ¸å¿åæ°å¨äºå¶ç¬ç¹ç<strong>ååæ¯Transformeræ¶æ</strong>ä»¥åå¯¹<strong>å¤èå¼å­¦ä¹ çæ·±åº¦èå</strong>ãå·ä½èè¨ï¼
*   <strong>ååæ¯æ¶æï¼</strong> åå«ä¸ä¸ª<strong>ä¸»æ©æ£å¯åå¼Transformeråæ¯</strong>ï¼è´è´£å¯¹æ½å¨è¡¨ç¤ºè¿è¡æ¸è¿å¼ç»åï¼åé´äºæ©æ£æ¨¡åéæ­¥å»åªåçæé«è´¨éå¾åçæ ¸å¿ææ³ã
*   <strong>è¾å©å¯¹æ¯å­¦ä¹ åæ¯ï¼</strong> å¦ä¸ä¸ª<strong>è¾å©Transformeråæ¯</strong>éè¿å­¦ä¹ å°çæ½å¨å¯¹æ¯æ¥å¢å¼ºæ¨¡åå¯¹åªå£°çé²æ£æ§ï¼è¿æ¯ä¸ç§æ°é¢çå©ç¨å¯¹æ¯å­¦ä¹ æ¥è§£å³è¶åè¾¨çä¸­å¸¸è§åªå£°é®é¢çç­ç¥ã
*   <strong>å¤èå¼èåï¼</strong> å°<strong>æ©æ£å¯å</strong>ï¼ç¨äºç¹å¾ç»åï¼ã<strong>å¯¹ææ§å­¦ä¹ </strong>ï¼ç¨äºçæçå®æå¾åï¼å<strong>å¯¹æ¯å­¦ä¹ </strong>ï¼ç¨äºåªå£°é²æ£æ§ï¼è¿ä¸ç§å¼ºå¤§çå­¦ä¹ èå¼å·§å¦å°éæå¨ä¸ä¸ªTransformeræ¡æ¶åï¼å¹¶éè¿Residual-in-Residual Dense Blocksè¿è¡èååè§£ç ï¼ä»¥å®ç°åè¶çéå»ºè´¨éã</p>
<p><strong>3. å¯¹è¯¥é¢åçæ½å¨å½±å</strong></p>
<ul>
<li><strong>æ§è½æ°æ æï¼</strong> CATformerå£°ç§°å¨æçåè§è§è´¨éä¸åè¶è¶äºç°æçTransformeråæ©æ£å¯åæ¹æ³ï¼è¿å¯è½ä¸ºå¾åè¶åè¾¨çé¢åæ ç«æ°çæ§è½åºåã</li>
<li><strong>å¼¥åæ¹æ³è®ºé¸¿æ²ï¼</strong> è¯¥ç ç©¶æç¡®æåºå¶å·¥ä½âå¼¥åäºTransformerãæ©æ£æ¨¡ååGANè¿ä¸ç§æ¹æ³ä¹é´çæ§è½å·®è·âï¼è¿è¡¨æå®æåå°ç»åäºåå®¶ä¹é¿ï¼åæäºåä¸æ¹æ³çå±éæ§ï¼å¯è½å¼é¢æªæ¥è¶åè¾¨çä¹è³æ´å¹¿æ³å¾åçæä»»å¡çæ··åæ¶æè®¾è®¡è¶å¿ã</li>
<li><strong>æ¨å¨æ©æ£å¯åæ¨¡åçå®ç¨åï¼</strong> æ©æ£æ¨¡åè½ç¶çæè´¨éé«ï¼ä½éå¸¸æ¨çéåº¦è¾æ¢ãCATformeréè¿âæ©æ£å¯åâèéå®æ´æ©æ£æ¨¡åçæ¹å¼ï¼å¨ä¿æé«è´¨éçåæ¶æåäºæçï¼ä¸ºå°æ©æ£æ¨¡åçå¼ºå¤§è½åå¼å¥å®éåºç¨åºæ¯æä¾äºå¯è¡è·¯å¾ã</li>
<li><strong>å¢å¼ºæ¨¡åé²æ£æ§ï¼</strong> å¼å¥å¯¹æ¯å­¦ä¹ ä»¥å¢å¼ºå¯¹åªå£°çé²æ£æ§ï¼å¯¹äºçå®ä¸çä¸­ä½è´¨éãå«åªå¾åçè¶åè¾¨çå·æéè¦æä¹ï¼æåäºæ¨¡åçå®ç¨ä»·å¼ã</li>
</ul>
<p><strong>4. å¯è½åçäºè¿é¡¹ç ç©¶çç¸å³é¢åæåºç¨</strong></p>
<ul>
<li><strong>éç¨å¾åæ¢å¤ä¸å¢å¼ºï¼</strong> é¤äºè¶åè¾¨çï¼å¶å¯¹åªå£°çé²æ£æ§åé«è´¨ééå»ºè½åå¯è½ç´æ¥åºç¨äºå¾åå»åªãå»æ¨¡ç³ãå¾åä¿®å¤ç­ä»»å¡ã</li>
<li><strong>å»å­¦å½±åï¼</strong> æé«ä½åè¾¨çå»å­¦æ«æå¾åï¼å¦MRIãCTï¼çç»èï¼è¾å©å»çè¿è¡æ´ç²¾ç¡®çè¯æ­ã</li>
<li><strong>é¥æä¸å®é²çæ§ï¼</strong> æåå«æå¾åãæ äººæºèªæå¾æçæ§å½åçæ¸æ°åº¦ï¼ä¾¿äºç®æ è¯å«ãæå¿æç¥åäºä»¶åæã</li>
<li><strong>è®¡ç®æå½±ä¸è§é¢å¤çï¼</strong> æ¹åæ¶è´¹çº§è®¾å¤ææçä½è´¨éç§çåè§é¢ï¼å®ç°æ´æ¸æ°çæ¾å¤§åç»èæ¢å¤ã</li>
<li><strong>æ°å­åå®¹åä½ä¸å¨±ä¹ï¼</strong> æåèæ§ç§çãè§é¢çç»è´¨ï¼æå¨æ¸¸æãVR/ARç­åºæ¯ä¸­å®ç°å®æ¶é«è´¨éæ¸²æã</li>
<li><strong>å¶ä»æ¡ä»¶å¾åçæä»»å¡ï¼</strong> å¶èåå¤ç§å­¦ä¹ èå¼åæ¶æçæè·¯ï¼å¯è½ä¸ºå¶ä»æ¡ä»¶å¾åçæä»»å¡ï¼å¦ææ¬å°å¾åãå¾åå°å¾åè½¬æ¢ï¼æä¾æ°çè®¾è®¡çµæã</li>
</ul>
<p><strong>5. ä»æè¦ä¸­å¯ä»¥æ¨æ­åºçä»»ä½å±éæ§</strong></p>
<ul>
<li><strong>æ¶æå¤ææ§ä¸è®­ç»ææ¬ï¼</strong> ååæ¯Transformeræ¶æç»åä¸ç§å­¦ä¹ èå¼ï¼æ©æ£å¯åãå¯¹æãå¯¹æ¯ï¼ä»¥åRIRDBï¼æå³çæ¨¡åå¯è½éå¸¸å¤æï¼è®­ç»é¾åº¦å¤§ï¼å¯¹è®¡ç®èµæºçéæ±é«ï¼ä¸è¶åæ°è°ä¼å¯è½å·ææææ§ã</li>
<li><strong>âæ©æ£å¯åâçæ·±åº¦ï¼</strong> è®ºæå¼ºè°âæ©æ£å¯åâçç¹å¾ç»åï¼èéå®æ´çæ©æ£æ¨¡åãè¿å¯è½æå³çå®å¨çæå¤æ ·æ§æå¤çæç«¯ä½è´¨éè¾å¥æ¹é¢çè½åï¼å¯è½ä¸å¦å®æ´çæ©æ£çææ¨¡åãå¶âæ¸è¿å¼ç»åâçå·ä½æºå¶åææä¸å®æ´æ©æ£æ¨¡åçå·®å¼ï¼æå¾æ­£ææ­ç¤ºã</li>
<li><strong>åªå£°é²æ£æ§çå·ä½èå´ï¼</strong> æè¦æå°éè¿å­¦ä¹ å°çæ½å¨å¯¹æ¯å¢å¼ºå¯¹åªå£°çé²æ£æ§ï¼ä½æªå·ä½è¯´æè½å¤ççåªå£°ç±»åï¼å¦é«æ¯åªå£°ãçå®ä¸çåªå£°ãä¼ æå¨åªå£°ï¼åå¼ºåº¦èå´ãå¨é¢å¯¹é«åº¦å¤ææéå¸ååªå£°æ¶ï¼å¶è¡¨ç°å¦ä½ä»æ¯æªç¥ã</li>
<li><strong>âæçâçéåï¼</strong> å°½ç®¡å£°ç§°å¨æçä¸ä¼äºç°ææ¹æ³ï¼ä½å·ä½çéåæ°æ®ï¼å¦æ¨çæ¶é´ãåæ°éãFLOPsï¼å¨æè¦ä¸­ç¼ºå¤±ï¼æ æ³å¤æ­å¶âæçâçç»å¯¹æ°´å¹³ï¼å°¤å¶æ¯å¨ä¸è½»éçº§æ¨¡åå¯¹æ¯æ¶ã</li>
<li><strong>æ³åè½åï¼</strong> å°½ç®¡å¨åºåæ°æ®éä¸è¡¨ç°åºè²ï¼ä½å¨é¢å¯¹æªè§è¿çé«åº¦å¤ææç¹å®é¢åççå®ä¸çä½åè¾¨çå¾åæ¶ï¼å¶æ³åè½åä»éè¿ä¸æ­¥éªè¯ãä¾å¦ï¼å¯¹äºç¹å®çº¹çãåç§æ¡ä»¶æåå®¹ï¼å¦äººè¸ãææ¬ï¼çè¶åè¾¨çææã</li>
</ul>
<p><strong>Key Findings:</strong></p>
<ul>
<li>This study introduces CATformer (Contrastive Adversarial
Transformer), a novel neural network integrating diffusion-inspired feature
refinement with adversarial and contrastive learning.</li>
<li>Extensive experiments on benchmark datasets demonstrate that CATformer
outperforms recent transformer-based and diffusion-inspired methods both in
efficiency and visual image quality.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="http://arxiv.org/pdf/2508.17708v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2508.17708v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2508.19003v1'></a></p>
<h2 id="roofseg-an-edge-aware-transformer-based-network-for-end-to-end-roof-plane-segmentation"><a href="https://arxiv.org/abs/2508.19003v1">RoofSeg: An edge-aware transformer-based network for end-to-end roof plane segmentation</a></h2>
<p><strong>Authors:</strong> Siyuan You, Guozheng Xu, Pengwei Zhou, Qiwen Jin, Jian Yao, Li Li</p>
<p><strong>Published:</strong> 2025-08-26</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>Roof plane segmentation is one of the key procedures for reconstructing
three-dimensional (3D) building models at levels of detail (LoD) 2 and 3 from
airborne light detection and ranging (LiDAR) point clouds. The majority of
current approaches for roof plane segmentation rely on the manually designed or
learned features followed by some specifically designed geometric clustering
strategies. Because the learned features are more powerful than the manually
designed features, the deep learning-based approaches usually perform better
than the traditional approaches. However, the current deep learning-based
approaches have three unsolved problems. The first is that most of them are not
truly end-to-end, the plane segmentation results may be not optimal. The second
is that the point feature discriminability near the edges is relatively low,
leading to inaccurate planar edges. The third is that the planar geometric
characteristics are not sufficiently considered to constrain the network
training. To solve these issues, a novel edge-aware transformer-based network,
named RoofSeg, is developed for segmenting roof planes from LiDAR point clouds
in a truly end-to-end manner. In the RoofSeg, we leverage a transformer
encoder-decoder-based framework to hierarchically predict the plane instance
masks with the use of a set of learnable plane queries. To further improve the
segmentation accuracy of edge regions, we also design an Edge-Aware Mask Module
(EAMM) that sufficiently incorporates planar geometric prior of edges to
enhance its discriminability for plane instance mask refinement. In addition,
we propose an adaptive weighting strategy in the mask loss to reduce the
influence of misclassified points, and also propose a new plane geometric loss
to constrain the network training.</p>
<p><strong>Analysis:</strong></p>
<p>ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå¯¹è¿ç¯å³äºRoofSegçè®ºææè¦è¿è¡äºæ·±å¥åæï¼</p>
<hr />
<h3 id="1-concise-summary_2">1. è®ºæä¸»è¦è´¡ç®æ»ç» (Concise summary)</h3>
<p>æ¬ææåºäºä¸ç§åä¸ºRoofSegçè¾¹ç¼æç¥Transformerç½ç»ï¼æ¨å¨å®ç°ä»LiDARç¹äºä¸­ç«¯å°ç«¯çå±é¡¶å¹³é¢åå²ãå®éè¿ç»åTransformerç¼ç å¨-è§£ç å¨æ¶æãä¸é¨è®¾è®¡çè¾¹ç¼æç¥æ©ç æ¨¡åï¼EAMMï¼ä»¥åæ°çå ä½æå¤±å½æ°ï¼ææè§£å³äºç°ææ·±åº¦å­¦ä¹ æ¹æ³å¨éç«¯å°ç«¯ãè¾¹ç¼åå²ç²¾åº¦ä½åç¼ºä¹å ä½çº¦æç­æ¹é¢çææã</p>
<h3 id="2-key-innovation-or-methodological-approach_2">2. å³é®åæ°ç¹ææ¹æ³å­¦ (Key innovation or methodological approach)</h3>
<ul>
<li><strong>ç«¯å°ç«¯Transformeræ¶æï¼</strong> é¦æ¬¡å°Transformerç¼ç å¨-è§£ç å¨æ¡æ¶åºç¨äºå±é¡¶å¹³é¢åå²ï¼éè¿å¯å­¦ä¹ çå¹³é¢æ¥è¯¢ï¼learnable plane queriesï¼ç´æ¥é¢æµå¹³é¢å®ä¾æ©ç ï¼å®ç°äºçæ­£çç«¯å°ç«¯ï¼end-to-endï¼åå²ï¼é¿åäºåå¤ççæ¬¡ä¼æ§ã</li>
<li><strong>è¾¹ç¼æç¥æ©ç æ¨¡åï¼EAMMï¼ï¼</strong> éå¯¹è¾¹ç¼åºåç¹å¾å¤å«åä½çé®é¢ï¼è®¾è®¡äºEAMMï¼è¯¥æ¨¡åååèå¥äºå¹³é¢çå ä½åéªç¥è¯ï¼ä»¥æ¾èæåè¾¹ç¼åºåçåå²ç²¾åº¦åé²æ£æ§ã</li>
<li><strong>ç»¼åæå¤±å½æ°è®¾è®¡ï¼</strong> æåºäºä¸ç§èªéåºå æç­ç¥çæ©ç æå¤±ï¼ä»¥éä½è¯¯åç±»ç¹çå½±åï¼åæ¶å¼å¥äºæ°çå¹³é¢å ä½æå¤±ï¼ç¨äºå¨è®­ç»è¿ç¨ä¸­æ¾å¼å°çº¦æç½ç»å­¦ä¹ ï¼ç¡®ä¿åå²ç»æçå ä½ä¸è´æ§ã</li>
</ul>
<h3 id="3-potential-impact-on-the-field_2">3. å¯¹é¢åæ½å¨å½±å (Potential impact on the field)</h3>
<ul>
<li><strong>æå3Då»ºç­æ¨¡åéå»ºç²¾åº¦ï¼</strong> éè¿æä¾é«ç²¾åº¦çå±é¡¶å¹³é¢åå²ï¼ç¹å«æ¯å¯¹è¾¹ç¼åºåçç²¾ç¡®å¤çï¼å°ç´æ¥æåLoD 2åLoD 3çº§å«3Då»ºç­æ¨¡åçéå»ºè´¨éåèªå¨åæ°´å¹³ã</li>
<li><strong>æ¨å¨ç¹äºè¯­ä¹/å®ä¾åå²åå±ï¼</strong> å¼å¥Transformeræ¶æåå ä½åéªçº¦æï¼ä¸ºç¹äºæ°æ®ä¸­å¤æå ä½ä½çç«¯å°ç«¯åå²æä¾äºä¸ä¸ªæ°çèå¼ï¼å¯è½å¯åå¶ä»ç»æååºæ¯ï¼å¦å®¤åãå·¥ä¸é¨ä»¶ï¼çåå²æ¹æ³ã</li>
<li><strong>ç®åå·¥ä½æµç¨ï¼</strong> çæ­£çç«¯å°ç«¯æ¹æ³åå°äºå¯¹å¤æåå¤ççä¾èµï¼ç®åäºä»åå§LiDARç¹äºå°ç»æåå±é¡¶æ¨¡åçæ´ä¸ªæµç¨ï¼æé«äºæçã</li>
</ul>
<h3 id="4-related-areas-or-applications-that-might-benefit-from-this-research">4. ç¸å³åºç¨é¢å (Related areas or applications that might benefit from this research)</h3>
<ul>
<li><strong>3Dåå¸å»ºæ¨¡ä¸æ°å­å­ªçï¼</strong> é«ç²¾åº¦å±é¡¶æ¨¡åæ¯æå»ºç²¾ç»ååå¸æ°å­å­ªçåå°çä¿¡æ¯ç³»ç»ï¼GISï¼çåºç¡ã</li>
<li><strong>æºæ§åå¸åºç¨ï¼</strong> ä¾å¦ï¼å±é¡¶å¤ªé³è½æ¿å®è£æ½åè¯ä¼°ãåå¸ç­å²æåºåæãå»ºç­è½èæ¨¡æç­ã</li>
<li><strong>ç¾å®³è¯ä¼°ä¸ç®¡çï¼</strong> å¿«éåç¡®å°è¯ä¼°å±é¡¶å¨èªç¶ç¾å®³ï¼å¦å°éãé£é£ï¼åçæåæåµã</li>
<li><strong>èªå¨é©¾é©¶ä¸æºå¨äººå¯¼èªï¼</strong> å¸®å©è½¦è¾åæºå¨äººæ´å¥½å°çè§£åæç¥å¨å´çå»ºç­ç¯å¢ã</li>
<li><strong>å»ºç­ä¿¡æ¯æ¨¡åï¼BIMï¼ä¸æµç»ï¼</strong> ä¸ºå»ºç­è®¾è®¡ãæ½å·¥åç»´æ¤æä¾ç²¾ç¡®çå ä½æ°æ®ã</li>
</ul>
<h3 id="5-any-limitations-that-can-be-inferred-from-the-abstract">5. å¯æ¨æ­çå±éæ§ (Any limitations that can be inferred from the abstract)</h3>
<ul>
<li><strong>è®¡ç®èµæºéæ±ï¼</strong> Transformeræ¨¡åéå¸¸å·æè¾é«çè®¡ç®å¤æåº¦ååå­æ¶èï¼å°¤å¶æ¯å¨å¤çå¤§è§æ¨¡ç¹äºæ°æ®æ¶ï¼è®­ç»åæ¨çæ¶é´å¯è½è¾é¿ã</li>
<li><strong>å¯¹LiDARç¹äºè´¨éçä¾èµï¼</strong> å°½ç®¡å£°ç§°âè¾¹ç¼æç¥âï¼ä½LiDARç¹äºçå¯åº¦ãåªå£°åæ«æè§åº¦ä»å¯è½å½±åè¾¹ç¼åºåçç²¾åº¦ï¼ç¹å«æ¯å¨ç¹äºç¨çæå­å¨é®æ¡çåºåã</li>
<li><strong>å¤æå±é¡¶ç»æä¸æ³åæ§ï¼</strong> æ½è±¡ä¸­æªæåæ¨¡åå¯¹æç«¯å¤æãéæ åæå·æå¤§ééå±ç©ï¼å¦çå±ãå¤©çªãæ¤è¢«è¦çï¼çå±é¡¶ç»æçæ³åè½åã</li>
<li><strong>å ä½åéªçå®ä¹ä¸éç¨æ§ï¼</strong> âå¹³é¢å ä½åéªâçå·ä½å®ç°æ¹å¼åå¶å¨åç§å±é¡¶ç±»åï¼å¦æ²é¢å±é¡¶ï¼ä¸çéç¨æ§å°ä¸æç¡®ãå¦æåéªè¿äºåæ§ï¼å¯è½ä¼éå¶æ¨¡åççµæ´»æ§ã</li>
<li><strong>æ°æ®æ æ³¨ææ¬ï¼</strong> ç«¯å°ç«¯è®­ç»éå¸¸éè¦å¤§éçç²¾ç¡®æ æ³¨æ°æ®ï¼ç¹å«æ¯å¯¹äºè¾¹ç¼åºååå¹³é¢å®ä¾çæ æ³¨ï¼è¿å¯è½æ¯ä¸ä¸ªææã</li>
</ul>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To solve these issues, a novel edge-aware transformer-based network,
named RoofSeg, is developed for segmenting roof planes from LiDAR point clouds
in a truly end-to-end manner.</li>
<li>In addition,
we propose an adaptive weighting strategy in the mask loss to reduce the
influence of misclassified points, and also propose a new plane geometric loss
to constrain the network training.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="http://arxiv.org/pdf/2508.19003v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2508.19003v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2508.17916v1'></a></p>
<h2 id="endoufm-utilizing-foundation-models-for-monocular-depth-estimation-of-endoscopic-images"><a href="https://arxiv.org/abs/2508.17916v1">EndoUFM: Utilizing Foundation Models for Monocular depth estimation of endoscopic images</a></h2>
<p><strong>Authors:</strong> Xinning Yao, Bo Liu, Bojian Li, Jingjing Wang, Jinghua Yue, Fugen Zhou</p>
<p><strong>Published:</strong> 2025-08-25</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Depth estimation is a foundational component for 3D reconstruction in
minimally invasive endoscopic surgeries. However, existing monocular depth
estimation techniques often exhibit limited performance to the varying
illumination and complex textures of the surgical environment. While powerful
visual foundation models offer a promising solution, their training on natural
images leads to significant domain adaptability limitations and semantic
perception deficiencies when applied to endoscopy. In this study, we introduce
EndoUFM, an unsupervised monocular depth estimation framework that innovatively
integrating dual foundation models for surgical scenes, which enhance the depth
estimation performance by leveraging the powerful pre-learned priors. The
framework features a novel adaptive fine-tuning strategy that incorporates
Random Vector Low-Rank Adaptation (RVLoRA) to enhance model adaptability, and a
Residual block based on Depthwise Separable Convolution (Res-DSC) to improve
the capture of fine-grained local features. Furthermore, we design a
mask-guided smoothness loss to enforce depth consistency within anatomical
tissue structures. Extensive experiments on the SCARED, Hamlyn, SERV-CT, and
EndoNeRF datasets confirm that our method achieves state-of-the-art performance
while maintaining an efficient model size. This work contributes to augmenting
surgeons' spatial perception during minimally invasive procedures, thereby
enhancing surgical precision and safety, with crucial implications for
augmented reality and navigation systems.</p>
<p><strong>Analysis:</strong></p>
<p>ä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼æå¯¹è¿ç¯å³äºEndoUFMçè®ºææè¦è¿è¡äºæ·±å¥åæï¼</p>
<hr />
<h3 id="endoufm-utilizing-foundation-models-for-monocular-depth-estimation-of-endoscopic-images_1">EndoUFM: Utilizing Foundation Models for Monocular depth estimation of endoscopic images</h3>
<p><strong>1. è®ºææ ¸å¿è´¡ç®æ»ç» (Concise Summary)</strong></p>
<p>æ¬ææåºäºEndoUFMï¼ä¸ä¸ªéå¯¹åçª¥éå¾åçæ çç£åç®æ·±åº¦ä¼°è®¡ç®æ³ãå®åæ°æ§å°æ´åäºåè§è§åºç¡æ¨¡åï¼å¹¶éè¿èªéåºå¾®è°ç­ç¥ï¼RVLoRAï¼ãæ®å·®æ·±åº¦å¯åç¦»å·ç§¯åï¼Res-DSCï¼ä»¥åæ©èå¼å¯¼å¹³æ»æå¤±ï¼ææåæäºåºç¡æ¨¡åå¨åçª¥éé¢åå­å¨çåéåºåè¯­ä¹æç¥é®é¢ï¼å®ç°äºæåè¿çæ·±åº¦ä¼°è®¡æ§è½ãè¯¥æ¹æ³æ¨å¨å¢å¼ºå¤ç§å»ççç©ºé´æç¥è½åï¼æé«å¾®åææ¯çç²¾ç¡®æ§åå®å¨æ§ã</p>
<p><strong>2. å³é®åæ°ææ¹æ³å­¦æ¹æ³ (Key Innovation or Methodological Approach)</strong></p>
<p>EndoUFMçæ ¸å¿åæ°å¨äºå¶<strong>ååºç¡æ¨¡åçåæ°æ§éæ</strong>ï¼æ¨å¨å©ç¨é¢è®­ç»çå¼ºå¤§åéªç¥è¯æ¥è§£å³åçª¥éå¾åæ·±åº¦ä¼°è®¡çææãä¸ºè§£å³åºç¡æ¨¡åå¨åçª¥éé¢åå­å¨çæ¾èåéåºæ§éå¶åè¯­ä¹æç¥ç¼ºé·ï¼å®å¼å¥äºä»¥ä¸å³é®æ¹æ³ï¼</p>
<ul>
<li><strong>åºäºéæºåéä½ç§©éåºï¼RVLoRAï¼çèªéåºå¾®è°ç­ç¥</strong>ï¼è¿æ¯ä¸ç§é«æçæ¨¡åéåºææ¯ï¼åè®¸å¨ä¸å®å¨éæ°è®­ç»æ´ä¸ªå¤§ååºç¡æ¨¡åçæåµä¸ï¼éè¿å°éå¯è®­ç»åæ°æ¥å¢å¼ºæ¨¡åå¯¹åçª¥éå¾åçéåºæ§ã</li>
<li><strong>åºäºæ·±åº¦å¯åç¦»å·ç§¯çæ®å·®åï¼Res-DSCï¼</strong>ï¼è¿ç§è®¾è®¡æ¨å¨æ´ææå°æè·åçª¥éå¾åä¸­ç²¾ç»çå±é¨ç¹å¾ï¼è¿å¯¹äºåºåå¤æç»ç»ç»æåå¾®å°çåè³å³éè¦ã</li>
<li><strong>æ©èå¼å¯¼çå¹³æ»æå¤±ï¼Mask-guided smoothness lossï¼</strong>ï¼éè¿å¼å¥è§£åç»ç»æ©èæ¥æå¯¼æ·±åº¦å¾çå¹³æ»æ§ï¼ç¡®ä¿å¨åä¸ç»ç»ç»æåé¨çæ·±åº¦ä¸è´æ§ï¼é¿åè·¨è¾¹ççæ¨¡ç³æä¸åç¡®ã</li>
</ul>
<p><strong>3. å¯¹é¢åæ½å¨å½±å (Potential Impact on the Field)</strong></p>
<p>è¿é¡¹ç ç©¶å¯¹å¾®åææ¯é¢åå·ææ¾èå½±åãéè¿æä¾é«ç²¾åº¦çåç®æ·±åº¦ä¼°è®¡ï¼å®è½<strong>æ¾èå¢å¼ºå¤ç§å»çå¨ææ¯è¿ç¨ä¸­çç©ºé´æç¥è½å</strong>ï¼ä»è<strong>æé«ææ¯çç²¾ç¡®æ§åå®å¨æ§</strong>ãæ­¤å¤ï¼å¶ææå¯¹äº<strong>å¢å¼ºç°å®ï¼ARï¼ææ¯å¯¼èªç³»ç»</strong>å<strong>æ¯ä¸­ä¸ç»´éå»º</strong>è³å³éè¦ï¼æææ¨å¨è¿äºææ¯çä¸´åºåºç¨ãä»æ´å¹¿æ³çè®¡ç®æºè§è§è§åº¦çï¼å®ä¸º<strong>å°éç¨è§è§åºç¡æ¨¡åæåè¿ç§»å¹¶éåºå°ç¹å®ãå¤æä¸æ°æ®ç¨ç¼ºçå»çå¾åé¢å</strong>æä¾äºææèå¼ï¼å±ç¤ºäºåºç¡æ¨¡åå¨ä¸ä¸é¢ååºç¨çå·¨å¤§æ½åã</p>
<p><strong>4. ç¸å³é¢åæåºç¨ (Related Areas or Applications that might benefit from this research)</strong></p>
<ul>
<li><strong>å¾®ååçª¥éææ¯ï¼MISï¼</strong>ï¼ç´æ¥åºç¨ï¼ç¨äºæ¯ä¸­ä¸ç»´éå»ºãç®æ å®ä½ãçç¶æµéåææ¯å¨æ¢°è·è¸ªã</li>
<li><strong>å¢å¼ºç°å®ï¼ARï¼/æ··åç°å®ï¼MRï¼ææ¯å¯¼èª</strong>ï¼å°èæä¿¡æ¯ï¼å¦è§åè·¯å¾ãçç¶è¾¹çãéè¦ç»æï¼ç²¾ç¡®å å å°çå®ææ¯è§éä¸­ï¼æä¾å®æ¶å¼å¯¼ã</li>
<li><strong>æºå¨äººè¾å©ææ¯</strong>ï¼ä¸ºææ¯æºå¨äººæä¾æ´åç¡®çç¯å¢æç¥åé¿éè½åï¼æé«èªå¨åæ°´å¹³ã</li>
<li><strong>ææ¯æ¨¡æä¸è®­ç»</strong>ï¼åå»ºæ´çå®çä¸ç»´ææ¯åºæ¯ï¼æé«å»å­¦çåå¤ç§å»ççå¹è®­ææã</li>
<li><strong>å»çå¾ååæä¸è¯æ­</strong>ï¼ä¸ºå¶ä»åºäºä¸ç»´ä¿¡æ¯çåæä»»å¡ï¼å¦è¿ç¤ä½ç§¯æµéãå¨å®å½¢ååæï¼æä¾åºç¡æ°æ®ã</li>
<li><strong>éç¨åºç¡æ¨¡åå¨ç¹å®é¢åçéåºæ§ç ç©¶</strong>ï¼ä¸ºå¶ä»ä¸ä¸é¢åï¼å¦å·¥ä¸æ£æµãé¥æãèªå¨é©¾é©¶ï¼çåºç¡æ¨¡ååºç¨æä¾åé´ï¼å°¤å¶æ¯å¨æ°æ®æ æ³¨ææ¬é«æææ°æ®ç¨ç¼ºçåºæ¯ã</li>
</ul>
<p><strong>5. ä»æè¦ä¸­æ¨æ­åºçå±éæ§ (Any limitations that can be inferred from the abstract)</strong></p>
<ul>
<li><strong>æ çç£å­¦ä¹ çåºæææ</strong>ï¼å°½ç®¡å£°ç§°è¾¾å°äºSOTAï¼ä½æ çç£æ¹æ³å¨çè®ºä¸å¯è½ä»æ æ³å®å¨è¶è¶å¨å¤§éé«è´¨éæ æ³¨æ°æ®ä¸è®­ç»ççç£æ¹æ³ï¼å¦ææ­¤ç±»æ°æ®å¯ç¨çè¯ï¼ãå¶æ§è½ä¸éå¯è½åéäºèªçç£ä¿¡å·çè´¨éååçª¥éå¾åçå¤ææ§ã</li>
<li><strong>åºç¡æ¨¡åçè®¡ç®ææ¬</strong>ï¼è½ç¶æè¦æå°âä¿æé«æçæ¨¡åå°ºå¯¸âï¼ä½âååºç¡æ¨¡åâçéæéå¸¸æå³çç¸å¯¹è¾å¤§çæ¨¡ååæ°éåè®¡ç®èµæºéæ±ãè¿å¨èµæºåéçè¾¹ç¼è®¾å¤æéè¦æä½å»¶è¿çå®æ¶ææ¯åºæ¯ä¸­ï¼å¯è½ä»æ¯ä¸ä¸ªéè¦ä»ç»æè¡¡çå ç´ ã</li>
<li><strong>æ©èçæä¾èµæ§</strong>ï¼æè¦ä¸­æå°çâæ©èå¼å¯¼çå¹³æ»æå¤±âéè¦åç¡®çè§£åç»ç»æ©èãå¦æè¿äºæ©èéè¦äººå·¥æ æ³¨ï¼å°å¢å æ°æ®åå¤çææ¬ï¼å¦æéè¿èªå¨åæ¹æ³çæï¼é£ä¹æ©èæ¬èº«çåç¡®æ§åé²æ£æ§å°ç´æ¥å½±åæ·±åº¦ä¼°è®¡çæ§è½ï¼ä¸èªå¨åæ©èçææ¬èº«ä¹æ¯ä¸ä¸ªææã</li>
<li><strong>é¢åæ³åæ§</strong>ï¼å°½ç®¡å¨å¤ä¸ªæ°æ®éä¸è¿è¡äºéªè¯ï¼åçª¥éå¾åçå¤ææ§åå¤æ ·æ§ï¼å¦ä¸åççãä¸åå¨æ¢°ãä¸åå»çæä½ä¹ æ¯ãæç«¯åç§ååãåºè¡ãçé¾ç­ï¼å¯è½è¿è¶ç°ææ°æ®éçè¦çèå´ãæ¨¡åå¨æªè§è¿çæç«¯ä¸´åºæ¡ä»¶ä¸çæ³åè½åä»éè¿ä¸æ­¥éªè¯ã</li>
<li><strong>å®æ¶æ§è¦æ±</strong>ï¼å¯¹äºææ¯å¯¼èªåARç³»ç»ï¼å®æ¶æ§æ¯å³é®ãæè¦æªæç¡®æåæ¨¡åçæ¨çéåº¦ï¼è¿å¯¹äºå®éä¸´åºåºç¨è³å³éè¦ãé«æçæ¨¡åå°ºå¯¸å¹¶ä¸ç­åäºé«éæ¨çã</li>
</ul>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this study, we introduce
EndoUFM, an unsupervised monocular depth estimation framework that innovatively
integrating dual foundation models for surgical scenes, which enhance the depth
estimation performance by leveraging the powerful pre-learned priors.</li>
<li>The
framework features a novel adaptive fine-tuning strategy that incorporates
Random Vector Low-Rank Adaptation (RVLoRA) to enhance model adaptability, and a
Residual block based on Depthwise Separable Convolution (Res-DSC) to improve
the capture of fine-grained local features.</li>
<li>Extensive experiments on the SCARED, Hamlyn, SERV-CT, and
EndoNeRF datasets confirm that our method achieves state-of-the-art performance
while maintaining an efficient model size.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="http://arxiv.org/pdf/2508.17916v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2508.17916v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2508.17885v1'></a></p>
<h2 id="isalux-illumination-and-segmentation-aware-transformer-employing-mixture-of-experts-for-low-light-image-enhancement"><a href="https://arxiv.org/abs/2508.17885v1">ISALux: Illumination and Segmentation Aware Transformer Employing Mixture of Experts for Low Light Image Enhancement</a></h2>
<p><strong>Authors:</strong> Raul Balmez, Alexandru Brateanu, Ciprian Orhei, Codruta Ancuti, Cosmin Ancuti</p>
<p><strong>Published:</strong> 2025-08-25</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We introduce ISALux, a novel transformer-based approach for Low-Light Image
Enhancement (LLIE) that seamlessly integrates illumination and semantic priors.
Our architecture includes an original self-attention block, Hybrid Illumination
and Semantics-Aware Multi-Headed Self- Attention (HISA-MSA), which integrates
illumination and semantic segmentation maps for en- hanced feature extraction.
ISALux employs two self-attention modules to independently process illumination
and semantic features, selectively enriching each other to regulate luminance
and high- light structural variations in real-world scenarios. A Mixture of
Experts (MoE)-based Feed-Forward Network (FFN) enhances contextual learning,
with a gating mechanism conditionally activating the top K experts for
specialized processing. To address overfitting in LLIE methods caused by
distinct light patterns in benchmarking datasets, we enhance the HISA-MSA
module with low-rank matrix adaptations (LoRA). Extensive qualitative and
quantitative evaluations across multiple specialized datasets demonstrate that
ISALux is competitive with state-of-the-art (SOTA) methods. Addition- ally, an
ablation study highlights the contribution of each component in the proposed
model. Code will be released upon publication.</p>
<p><strong>Analysis:</strong></p>
<p>è¿ç¯è®ºæä»ç»çISALuxæ¯ä¸ä¸ªå¨ä½åå¾åå¢å¼ºï¼LLIEï¼é¢åå·æåæ°æ§çå·¥ä½ãä½ä¸ºè®¡ç®æºè§è§åæºå¨å­¦ä¹ é¢åçä¸å®¶ï¼ä»¥ä¸æ¯å¯¹å¶æè¦çè¯¦ç»åæï¼</p>
<hr />
<h3 id="1-concise-summary_3">1. è®ºæä¸»è¦è´¡ç®çç®ææè¦ (Concise Summary)</h3>
<p>ISALuxæåºäºä¸ç§æ°é¢çåºäºTransformerçä½åå¾åå¢å¼ºï¼LLIEï¼æ¹æ³ï¼å®éè¿ä¸ä¸ªæ··ååç§åè¯­ä¹æç¥å¤å¤´èªæ³¨æåï¼HISA-MSAï¼æ¨¡åï¼æ ç¼å°æ´åäºåç§åè¯­ä¹åéªä¿¡æ¯ãè¯¥æ¨¡åå©ç¨ä¸¤ä¸ªç¸äºå¢å¼ºçèªæ³¨æåæ¨¡åæ¥å¤çåç§åè¯­ä¹ç¹å¾ï¼å¹¶éè¿åºäºä¸å®¶æ··åï¼MoEï¼çåé¦ç½ç»å¢å¼ºä¸ä¸æå­¦ä¹ ï¼åæ¶å¼å¥ä½ç§©ç©éµéåºï¼LoRAï¼æ¥è§£å³LLIEæ¹æ³ä¸­å¸¸è§çè¿æåé®é¢ã</p>
<h3 id="2-key-innovation-or-methodological-approach_3">2. å³é®åæ°ææ¹æ³å­¦æ¹æ³ (Key Innovation or Methodological Approach)</h3>
<p>ISALuxçæ ¸å¿åæ°å¨äºå¶<strong>Hybrid Illumination and Semantics-Aware Multi-Headed Self-Attention (HISA-MSA)</strong> æ¨¡åãè¿ä¸ªæ¨¡åç¬ç¹å°å°åç§å¾åè¯­ä¹åå²å¾ç´æ¥æ´åå°èªæ³¨æåæºå¶ä¸­ï¼ä»èå®ç°äºå¯¹åç§æ¡ä»¶åç©ä½è¯­ä¹åå®¹é½ææçç¹å¾æåãå·ä½æ¥è¯´ï¼</p>
<ul>
<li><strong>åéèªæ³¨æåæºå¶ä¸ç¸äºå¢å¼ºï¼</strong> ISALuxéç¨ä¸¤ä¸ªç¬ç«çèªæ³¨æåæ¨¡åï¼åå«å¤çåç§ç¹å¾åè¯­ä¹ç¹å¾ï¼ä½å®ä»¬è½å¤âéæ©æ§å°ç¸äºä¸°å¯âï¼è¿è¡¨æå®ä»¬ä¹é´å­å¨ä¸ç§ååä½ç¨ï¼èéç®åçå¹¶è¡å¤çï¼ä»èè½å¤æ´ç²¾ç»å°è°èäº®åº¦åé«åç»æååã</li>
<li><strong>åºäºä¸å®¶æ··åï¼MoEï¼çåé¦ç½ç»ï¼</strong> å¼å¥MoE-based FFNï¼éè¿ä¸ä¸ªé¨æ§æºå¶ææ¡ä»¶å°æ¿æ´»é¡¶é¨çKä¸ªä¸å®¶ï¼ä»¥å®ç°æ´ä¸ä¸ååä¸ä¸ææç¥çå¤çï¼è¿æå©äºæ¨¡åå¨ä¸ååºæ¯ä¸è¿è¡èªéåºå­¦ä¹ ã</li>
<li><strong>LoRAç¨äºè§£å³è¿æåï¼</strong> éå¯¹LLIEæ¹æ³ä¸­å åºåæ°æ®éåç§æ¨¡å¼å·®å¼å¯¼è´çè¿æåé®é¢ï¼ISALuxå¨HISA-MSAæ¨¡åä¸­èå¥äºä½ç§©ç©éµéåºï¼LoRAï¼ãè¿æ¯ä¸ç§é«æçåæ°å¾®è°ææ¯ï¼éå¸¸ç¨äºå¤§åæ¨¡åï¼è½å¤æææåæ¨¡åå¨ä¸ååç§æ¡ä»¶ä¸çæ³åè½åã</li>
</ul>
<h3 id="3-potential-impact-on-the-field_3">3. å¯¹é¢åæ½å¨å½±å (Potential Impact on the Field)</h3>
<ul>
<li><strong>LLIEæ§è½æåï¼</strong> éè¿æ·±åº¦æ´åè¯­ä¹ååç§åéªï¼å¹¶ç»åMoEåLoRAï¼ISALuxææå¨ä½åå¾åå¢å¼ºä»»å¡ä¸è¾¾å°æè¶è¶ç°æSOTAæ¹æ³ï¼æä¾æ´èªç¶ãç»èæ´ä¸°å¯çå¢å¼ºç»æã</li>
<li><strong>å¤æ¨¡æåéªæ´åçæ°èå¼ï¼</strong> è¯¥ç ç©¶ä¸ºTransformeræ¶æä¸­å¦ä½ææèåä¸åç±»åçï¼å¦å ä½ãè¯­ä¹ãç©çï¼åéªä¿¡æ¯æä¾äºä¸ä¸ªå¼ºæåçèä¾ï¼è¿å¯è½å¯åå¶ä»ä½çº§è§è§ä»»å¡ï¼å¦å»é¾ãå»åªãè¶åè¾¨çï¼çè®¾è®¡ã</li>
<li><strong>è§£å³LLIEæ³åæ§ææï¼</strong> LoRAçåºç¨ä¸ºè§£å³LLIEæ¨¡åå¨ä¸ååç§æ°æ®éä¹é´æ³åè½åä¸è¶³çé®é¢æä¾äºä¸ä¸ªææéå¾ï¼æå©äºå¼ååºæ´é²æ£ãæ´å·å®ç¨æ§çæ¨¡åã</li>
<li><strong>Transformerå¨ä½çº§è§è§ä¸­çè¿ä¸æ­¥åºç¨ï¼</strong> è¿ä¸æ­¥å·©åºäºTransformerå¨å¾åå¢å¼ºç­ä½çº§è§è§ä»»å¡ä¸­çæ½åï¼å±ç¤ºäºå¶å¨ææé¿è·ç¦»ä¾èµåå¤æä¸ä¸æä¿¡æ¯æ¹é¢çä¼å¿ã</li>
</ul>
<h3 id="4-related-areas-or-applications_2">4. ç¸å³é¢åæåºç¨ (Related Areas or Applications)</h3>
<ul>
<li><strong>è®¡ç®æºè§è§å¨æææ§ç¯å¢ä¸çåºç¨ï¼</strong> èªå¨é©¾é©¶ãçæ§ç³»ç»ãæºå¨äººè§è§ãæ äººæºæåç­ï¼è¿äºåºæ¯å¯¹å¤é´æä½åç§æ¡ä»¶ä¸çæç¥è½åææé«è¦æ±ã</li>
<li><strong>æ¶è´¹çº§æå½±ï¼</strong> æºè½ææºåå¶ä»ç¸æºè®¾å¤å¨ä½åç¯å¢ä¸çå¾åè´¨éæåã</li>
<li><strong>å»çå½±åï¼</strong> å¢å¼ºä½åæ¾å¾®éå¾åæåçª¥éå¾åçè´¨éï¼ä»¥è¾å©è¯æ­ã</li>
<li><strong>å¶ä»å¾åæ¢å¤ä»»å¡ï¼</strong> è®ºæä¸­æ´åå¤æ¨¡æåéªçæ¹æ³å­¦ææ³å¯ä»¥æ¨å¹¿å°å»é¾ãå»åªãå¾åå»é¨ç­å¶ä»éè¦ä¸°å¯ä¸ä¸æä¿¡æ¯çå¾åæ¢å¤ä»»å¡ã</li>
<li><strong>å¤æ¨¡æå­¦ä¹ ï¼</strong> ä¸ºå¦ä½å°å¾åçåç´ çº§ä¿¡æ¯ä¸é«çº§è¯­ä¹ä¿¡æ¯ææç»åæä¾äºä¸ä¸ªç ç©¶æ¹åã</li>
</ul>
<h3 id="5-limitations-inferred-from-the-abstract">5. ä»æè¦ä¸­å¯æ¨æ­çå±éæ§ (Limitations Inferred from the Abstract)</h3>
<ul>
<li><strong>å¯¹è¯­ä¹åéªçä¾èµï¼</strong> æ¨¡åä¾èµäºè¯­ä¹åå²å¾ãè¿æå³çå¨å®éåºç¨ä¸­ï¼è¦ä¹éè¦ä¸ä¸ªé¢å¤çãé¢è®­ç»çè¯­ä¹åå²æ¨¡åï¼è¿ä¼å¢å è®¡ç®å¼éåæ½å¨çéè¯¯ä¼ æ­ï¼å¦æåå²ä¸åç¡®ï¼ï¼è¦ä¹æ¨¡åéè¦åæ¶å­¦ä¹ åå²åå¢å¼ºï¼è¿ä¼å¢å æ¨¡åçå¤ææ§ãæè¦ä¸­æå°âintegrates...semantic priorsâï¼æ´å¾åäºåèã</li>
<li><strong>è®¡ç®èµæºéæ±ï¼</strong> Transformeræ¨¡åæ¬èº«éå¸¸è®¡ç®éè¾å¤§ï¼å°¤å¶æ¯å¨å¤çé«åè¾¨çå¾åæ¶ãæ­¤å¤ï¼MoEç»æè½ç¶å¨åæ°æçä¸å¯è½ææä¼å¿ï¼ä½å¨æ¨çæ¶æ¿æ´»å¤ä¸ªä¸å®¶ä¹å¯è½å¢å è®¡ç®è´æãæè¦æªæåæ¨¡åçæçæå®æ¶æ§ã</li>
<li><strong>è®­ç»å¤ææ§ï¼</strong> ç»åäºHISA-MSAãMoEåLoRAçå¤ææ¶æï¼å¶è®­ç»è¿ç¨å¯è½éè¦å¤§éçè®¡ç®èµæºåç²¾ç»çè¶åæ°è°ä¼ã</li>
<li><strong>LoRAçéç¨æ§ï¼</strong> å°½ç®¡LoRAæå©äºç¼è§£è¿æåï¼ä½å¶ææå¯è½ä»åéäºè®­ç»æ°æ®çå¤æ ·æ§ãå¨æç«¯ææªè§çä½ååºæ¯ä¸ï¼æ¨¡åçæ³åè½åä»å¯è½é¢ä¸´ææã</li>
<li><strong>å®æ¶æ§ä¸é¨ç½²ï¼</strong> æè¦æªæä¾å³äºæ¨¡åå¤§å°ãæ¨çéåº¦æå¨è¾¹ç¼è®¾å¤ä¸é¨ç½²æ½åçä¿¡æ¯ï¼è¿å¯¹äºå®éåºç¨è³å³éè¦ã</li>
</ul>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce ISALux, a novel transformer-based approach for Low-Light Image
Enhancement (LLIE) that seamlessly integrates illumination and semantic priors.</li>
<li>Extensive qualitative and
quantitative evaluations across multiple specialized datasets demonstrate that
ISALux is competitive with state-of-the-art (SOTA) methods.</li>
<li>Addition- ally, an
ablation study highlights the contribution of each component in the proposed
model.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="http://arxiv.org/pdf/2508.17885v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2508.17885v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-08-27 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
