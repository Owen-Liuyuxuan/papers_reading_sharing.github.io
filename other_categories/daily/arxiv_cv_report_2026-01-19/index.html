<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2026-01-19 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-16
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2026-01-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-20
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2026-01-16/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2026-01-20/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2026-01-19">Arxiv Computer Vision Papers - 2026-01-19</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#arxiv-2026-01-16-" class="nav-link">Arxiv 计算机视觉论文每日报告 (2026-01-16) - 执行摘要</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#shaper-robust-conditional-3d-shape-generation-from-casual-captures" class="nav-link">ShapeR: Robust Conditional 3D Shape Generation from Casual Captures</a>
                </li>
                <li class="nav-item">
                    <a href="#generative-scenario-rollouts-for-end-to-end-autonomous-driving" class="nav-link">Generative Scenario Rollouts for End-to-End Autonomous Driving</a>
                </li>
                <li class="nav-item">
                    <a href="#mha2mla-vlm-enabling-deepseeks-economical-multi-head-latent-attention-across-vision-language-models" class="nav-link">MHA2MLA-VLM: Enabling DeepSeek's Economical Multi-Head Latent Attention across Vision-Language Models</a>
                </li>
                <li class="nav-item">
                    <a href="#map2thought-explicit-3d-spatial-reasoning-via-metric-cognitive-maps" class="nav-link">Map2Thought: Explicit 3D Spatial Reasoning via Metric Cognitive Maps</a>
                </li>
                <li class="nav-item">
                    <a href="#topology-guaranteed-image-segmentation-enforcing-connectivity-genus-and-width-constraints" class="nav-link">Topology-Guaranteed Image Segmentation: Enforcing Connectivity, Genus, and Width Constraints</a>
                </li>
                <li class="nav-item">
                    <a href="#acot-vla-action-chain-of-thought-for-vision-language-action-models" class="nav-link">ACoT-VLA: Action Chain-of-Thought for Vision-Language-Action Models</a>
                </li>
                <li class="nav-item">
                    <a href="#think-clip-sample-slow-fast-frame-selection-for-video-understanding" class="nav-link">Think-Clip-Sample: Slow-Fast Frame Selection for Video Understanding</a>
                </li>
                <li class="nav-item">
                    <a href="#enhancing-vision-language-models-with-logic-reasoning-for-situational-awareness" class="nav-link">Enhancing Vision Language Models with Logic Reasoning for Situational Awareness</a>
                </li>
                <li class="nav-item">
                    <a href="#samannot-a-memory-efficient-local-open-source-framework-for-interactive-video-instance-segmentation-based-on-sam2" class="nav-link">SAMannot: A Memory-Efficient, Local, Open-source Framework for Interactive Video Instance Segmentation based on SAM2</a>
                </li>
                <li class="nav-item">
                    <a href="#x-distill-cross-architecture-vision-distillation-for-visuomotor-learning" class="nav-link">X-Distill: Cross-Architecture Vision Distillation for Visuomotor Learning</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2026-01-19">Arxiv Computer Vision Papers - 2026-01-19</h1>
<h2 id="executive-summary">Executive Summary</h2>
<h2 id="arxiv-2026-01-16-">Arxiv 计算机视觉论文每日报告 (2026-01-16) - 执行摘要</h2>
<p><strong>主要趋势与观察：</strong></p>
<p>本期 Arxiv 计算机视觉论文集聚焦于<strong>多模态理解、三维场景生成与推理、以及高效模型设计</strong>。特别值得注意的是，研究人员正积极探索如何将<strong>语言模型的能力与视觉任务相结合</strong>，以实现更强的场景理解和决策能力。同时，<strong>三维形状生成和场景重建</strong>的技术也在不断进步，为自动驾驶和机器人等领域提供更强大的支持。此外，<strong>模型效率和可解释性</strong>也成为重要的研究方向。</p>
<p><strong>亮点论文与创新：</strong></p>
<ul>
<li><strong>ShapeR: Robust Conditional 3D Shape Generation from Casual Captures</strong> 提出了一种从随意捕捉的图像生成鲁棒三维形状的方法，有望简化三维内容创作。</li>
<li><strong>Generative Scenario Rollouts for End-to-End Autonomous Driving</strong> 探索了生成式方法在端到端自动驾驶中的应用，通过生成逼真的场景来训练和评估模型，具有重要的实际意义。</li>
<li><strong>Map2Thought: Explicit 3D Spatial Reasoning via Metric Cognitive Maps</strong> 引入了显式的三维空间推理机制，通过认知地图来增强模型的理解能力，为机器人导航和场景理解提供了新思路。</li>
<li><strong>ACoT-VLA: Action Chain-of-Thought for Vision-Language-Action Models</strong> 和 <strong>Enhancing Vision Language Models with Logic Reasoning for Situational Awareness</strong> 都强调了将<strong>逻辑推理和思维链</strong>引入视觉语言模型的重要性，以提升其在复杂场景下的决策和理解能力。</li>
</ul>
<p><strong>新兴研究方向与技术：</strong></p>
<ul>
<li><strong>视觉语言模型 (VLM) 的增强：</strong> 通过引入逻辑推理、思维链（Chain-of-Thought）以及多模态注意力机制（如 MHA2MLA-VLM），VLM 的理解能力和泛化能力正在被显著提升。</li>
<li><strong>三维生成与推理：</strong> 从二维图像生成高质量三维形状，以及在三维空间中进行显式推理，是当前研究的热点。</li>
<li><strong>高效模型设计与蒸馏：</strong> 关注模型在内存和计算效率上的优化，例如 SAMannot 的内存高效框架，以及 X-Distill 的跨架构蒸馏技术。</li>
<li><strong>视频理解的精细化：</strong> 针对视频理解，出现了如 Think-Clip-Sample 这样的方法，通过智能帧选择来提高效率和准确性。</li>
</ul>
<p><strong>推荐阅读论文：</strong></p>
<p>考虑到其潜在影响力和创新性，以下论文值得深入阅读：</p>
<ol>
<li><strong>ShapeR: Robust Conditional 3D Shape Generation from Casual Captures</strong> (三维生成领域的进展)</li>
<li><strong>Generative Scenario Rollouts for End-to-End Autonomous Driving</strong> (自动驾驶领域的关键技术)</li>
<li><strong>Map2Thought: Explicit 3D Spatial Reasoning via Metric Cognitive Maps</strong> (三维空间推理的创新方法)</li>
<li><strong>ACoT-VLA: Action Chain-of-Thought for Vision-Language-Action Models</strong> (VLM 结合逻辑推理的代表性工作)</li>
<li><strong>Enhancing Vision Language Models with Logic Reasoning for Situational Awareness</strong> (VLM 在复杂场景理解上的重要探索)</li>
</ol>
<p>这些论文代表了当前计算机视觉领域的前沿研究方向，对理解未来技术发展趋势具有重要价值。</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2601.11514v1">ShapeR: Robust Conditional 3D Shape Generation from Casual Captures</a></li>
<li><a href="#2601.11475v1">Generative Scenario Rollouts for End-to-End Autonomous Driving</a></li>
<li><a href="#2601.11464v1">MHA2MLA-VLM: Enabling DeepSeek's Economical Multi-Head Latent Attention across Vision-Language Models</a></li>
<li><a href="#2601.11442v1">Map2Thought: Explicit 3D Spatial Reasoning via Metric Cognitive Maps</a></li>
<li><a href="#2601.11409v1">Topology-Guaranteed Image Segmentation: Enforcing Connectivity, Genus, and Width Constraints</a></li>
<li><a href="#2601.11404v1">ACoT-VLA: Action Chain-of-Thought for Vision-Language-Action Models</a></li>
<li><a href="#2601.11359v1">Think-Clip-Sample: Slow-Fast Frame Selection for Video Understanding</a></li>
<li><a href="#2601.11322v1">Enhancing Vision Language Models with Logic Reasoning for Situational Awareness</a></li>
<li><a href="#2601.11301v1">SAMannot: A Memory-Efficient, Local, Open-source Framework for Interactive Video Instance Segmentation based on SAM2</a></li>
<li><a href="#2601.11269v1">X-Distill: Cross-Architecture Vision Distillation for Visuomotor Learning</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2601.11514v1'></a></p>
<h2 id="shaper-robust-conditional-3d-shape-generation-from-casual-captures"><a href="https://arxiv.org/abs/2601.11514v1">ShapeR: Robust Conditional 3D Shape Generation from Casual Captures</a></h2>
<p><strong>Authors:</strong> Yawar Siddiqui, Duncan Frost, Samir Aroudj, Armen Avetisyan, Henry Howard-Jenkins, Daniel DeTone, Pierre Moulon, Qirui Wu, Zhengqin Li, Julian Straub, Richard Newcombe, Jakob Engel</p>
<p><strong>Published:</strong> 2026-01-16</p>
<p><strong>Categories:</strong> cs.CV, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Recent advances in 3D shape generation have achieved impressive results, but most existing methods rely on clean, unoccluded, and well-segmented inputs. Such conditions are rarely met in real-world scenarios. We present ShapeR, a novel approach for conditional 3D object shape generation from casually captured sequences. Given an image sequence, we leverage off-the-shelf visual-inertial SLAM, 3D detection algorithms, and vision-language models to extract, for each object, a set of sparse SLAM points, posed multi-view images, and machine-generated captions. A rectified flow transformer trained to effectively condition on these modalities then generates high-fidelity metric 3D shapes. To ensure robustness to the challenges of casually captured data, we employ a range of techniques including on-the-fly compositional augmentations, a curriculum training scheme spanning object- and scene-level datasets, and strategies to handle background clutter. Additionally, we introduce a new evaluation benchmark comprising 178 in-the-wild objects across 7 real-world scenes with geometry annotations. Experiments show that ShapeR significantly outperforms existing approaches in this challenging setting, achieving an improvement of 2.7x in Chamfer distance compared to state of the art.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“ShapeR: Robust Conditional 3D Shape Generation from Casual Captures”的全面中文摘要：</p>
<p><strong>论文题目：</strong> ShapeR: Robust Conditional 3D Shape Generation from Casual Captures</p>
<p><strong>作者：</strong> Yawar Siddiqui, Duncan Frost, Samir Aroudj, Armen Avetisyan, Henry Howard-Jenkins, Daniel DeTone, Pierre Moulon, Qirui Wu, Zhengqin Li, Julian Straub, Richard Newcombe, Jakob Engel</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
该论文旨在解决当前 3D 形状生成方法在处理真实世界中“随意捕捉”的图像序列时遇到的挑战。现有方法通常依赖于干净、无遮挡且分割良好的输入，这在现实场景中很少见。随意捕捉的场景往往包含遮挡、背景杂乱、传感器噪声、低分辨率和不理想的视角等问题，导致现有方法性能显著下降。因此，研究的核心问题是如何从这些具有挑战性的、非结构化的图像序列中鲁棒地生成高保真、度量准确的 3D 对象形状。</p>
<p><strong>2. 主要创新点/方法贡献：</strong>
ShapeR 提出了一种新颖的条件式 3D 对象形状生成方法，其核心创新点在于：</p>
<ul>
<li><strong>多模态融合的条件化：</strong> ShapeR 巧妙地融合了多种信息源来指导形状生成，包括：<ul>
<li><strong>稀疏 SLAM 点云：</strong> 利用视觉惯性 SLAM 系统提取的稀疏 3D 点云，提供全局几何线索。</li>
<li><strong>带姿态的多视角图像：</strong> 从序列中提取的、已知相机姿态的多视角图像，提供丰富的视觉信息。</li>
<li><strong>机器生成的文本描述：</strong> 利用视觉语言模型（VLM）为每个对象生成文本描述，提供语义信息。</li>
</ul>
</li>
<li><strong>基于流匹配的生成模型：</strong> 采用一种“<strong>3D 流匹配（Rectified Flow Matching）</strong>”的生成模型，该模型建立在 VAE 的潜在空间之上。它通过一个解耦的 Transformer 网络（受 FLUX.1 DiT 启发）来学习将高斯噪声映射到 VAE 的潜在表示，从而生成 3D 形状。</li>
<li><strong>鲁棒性增强技术：</strong> 为了应对随意捕捉数据的挑战，ShapeR 采用了多种技术：<ul>
<li><strong>即时组合式数据增强：</strong> 在训练过程中对所有输入模态进行大量的、动态的数据增强，模拟真实世界的各种干扰。</li>
<li><strong>两阶段课程学习：</strong> 首先在大型、多样化的对象级数据集上进行预训练，然后在一个包含更真实场景（如 Aria 合成环境）的数据集上进行微调，以提高泛化能力。</li>
<li><strong>隐式对象识别：</strong> 利用 3D 点云和 2D 点投影掩码来隐式地识别和定位目标对象，无需显式的 2D 分割。</li>
</ul>
</li>
<li><strong>新的评估基准：</strong> 引入了一个名为“<strong>ShapeR Evaluation Dataset</strong>”的新型数据集，包含 178 个真实世界场景中的对象，具有完整的几何标注，专门用于评估在随意捕捉条件下的 3D 重建性能。</li>
</ul>
<p><strong>3. 主要结果及意义：</strong>
*   <strong>性能显著提升：</strong> 在提出的 ShapeR Evaluation Dataset 上，ShapeR 显著优于现有的最先进（SoTA）方法，在 Chamfer 距离上实现了 <strong>2.7 倍的提升</strong>。
*   <strong>鲁棒性强：</strong> ShapeR 在处理遮挡、杂乱背景和低质量输入方面表现出极强的鲁棒性，这得益于其多模态融合和数据增强策略。
*   <strong>度量准确性与完整性：</strong> ShapeR 能够生成度量准确且完整的 3D 对象形状，并且能够保持对象在场景中的一致尺度和布局。
*   <strong>无需手动干预：</strong> 与许多需要手动分割或交互式输入的基线方法不同，ShapeR 能够完全自动化地处理随意捕捉的序列。
*   <strong>统一性：</strong> ShapeR 弥合了生成式 3D 形状建模和度量 3D 场景重建之间的差距，实现了对象级的高保真重建，同时保持了场景的度量一致性。</p>
<p><strong>4. 提及的局限性：</strong>
论文中也提到了 ShapeR 的一些局限性：</p>
<ul>
<li><strong>低图像保真度/视角少：</strong> 对于图像保真度低或视角极少的对象，重建可能不完整或缺乏细节，因为几何和视觉证据不足。</li>
<li><strong>堆叠或紧密相邻的对象：</strong> 当对象堆叠或紧密相邻时（例如，桌子支撑其他物体），重建的网格有时会包含相邻结构的残留，而不是完全隔离目标对象。</li>
<li><strong>依赖于上游 3D 实例检测：</strong> ShapeR 依赖于上游的 3D 实例检测器。如果检测器漏检或边界框不准确，这些错误会直接传播到重建阶段，导致漏检的对象无法恢复。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong>
虽然论文没有明确列出未来研究方向，但从其局限性和贡献中可以推断出以下潜在方向：</p>
<ul>
<li><strong>提高对低质量输入的鲁棒性：</strong> 进一步研究如何处理更极端的低图像保真度、极少视角或更严重的传感器噪声。</li>
<li><strong>改进对复杂场景中对象关系的理解：</strong> 探索更精细的方法来处理对象之间的复杂空间关系，例如堆叠、嵌套或紧密接触，以实现更干净的对象隔离。</li>
<li><strong>增强对检测器误差的容忍度：</strong> 研究如何使模型对上游 3D 实例检测器的错误更加鲁棒，或者探索端到端的联合优化方法。</li>
<li><strong>更精细的纹理和材质生成：</strong> 目前的重点是几何形状，未来的工作可以扩展到生成更逼真的纹理和材质。</li>
<li><strong>实时或近实时重建：</strong> 探索优化模型以实现更快的推理速度，以支持实时应用。</li>
<li><strong>更广泛的场景类型和对象类别：</strong> 在更广泛的真实世界场景和更广泛的对象类别上进行评估和改进。</li>
</ul>
<p><strong>总结：</strong>
ShapeR 是一个重要的进展，它通过创新的多模态融合和鲁棒的生成模型，显著提高了在随意捕捉的图像序列中进行 3D 对象形状生成的性能和鲁棒性。该方法克服了现有方法的关键限制，为在真实世界场景中实现自动化、高保真的 3D 重建开辟了新的可能性。其引入的新数据集和评估方法也为该领域的研究提供了宝贵的资源。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present ShapeR, a novel approach for conditional 3D object shape generation from casually captured sequences.</li>
<li>Additionally, we introduce a new evaluation benchmark comprising 178 in-the-wild objects across 7 real-world scenes with geometry annotations.</li>
<li>Experiments show that ShapeR significantly outperforms existing approaches in this challenging setting, achieving an improvement of 2.7x in Chamfer distance compared to state of the art.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.11514v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.11514v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.11475v1'></a></p>
<h2 id="generative-scenario-rollouts-for-end-to-end-autonomous-driving"><a href="https://arxiv.org/abs/2601.11475v1">Generative Scenario Rollouts for End-to-End Autonomous Driving</a></h2>
<p><strong>Authors:</strong> Rajeev Yasarla, Deepti Hegde, Shizhong Han, Hsin-Pai Cheng, Yunxiao Shi, Meysam Sadeghigooghari, Shweta Mahajan, Apratim Bhattacharyya, Litian Liu, Risheek Garrepalli, Thomas Svantesson, Fatih Porikli, Hong Cai</p>
<p><strong>Published:</strong> 2026-01-16</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Vision-Language-Action (VLA) models are emerging as highly effective planning models for end-to-end autonomous driving systems. However, current works mostly rely on imitation learning from sparse trajectory annotations and under-utilize their potential as generative models. We propose Generative Scenario Rollouts (GeRo), a plug-and-play framework for VLA models that jointly performs planning and generation of language-grounded future traffic scenes through an autoregressive rollout strategy. First, a VLA model is trained to encode ego vehicle and agent dynamics into latent tokens under supervision from planning, motion, and language tasks, facilitating text-aligned generation. Next, GeRo performs language-conditioned autoregressive generation. Given multi-view images, a scenario description, and ego-action questions, it generates future latent tokens and textual responses to guide long-horizon rollouts. A rollout-consistency loss stabilizes predictions using ground truth or pseudo-labels, mitigating drift and preserving text-action alignment. This design enables GeRo to perform temporally consistent, language-grounded rollouts that support long-horizon reasoning and multi-agent planning. On Bench2Drive, GeRo improves driving score and success rate by +15.7 and +26.2, respectively. By integrating reinforcement learning with generative rollouts, GeRo achieves state-of-the-art closed-loop and open-loop performance, demonstrating strong zero-shot robustness. These results highlight the promise of generative, language-conditioned reasoning as a foundation for safer and more interpretable end-to-end autonomous driving.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Generative Scenario Rollouts for End-to-End Autonomous Driving”的全面中文摘要，重点关注其在计算机视觉领域的创新性和重要性：</p>
<p><strong>论文题目：</strong> Generative Scenario Rollouts for End-to-End Autonomous Driving (生成式场景回滚用于端到端自动驾驶)</p>
<p><strong>作者：</strong> Rajeev Yasarla, Deepti Hegde, Shizhong Han, Hsin-Pai Cheng, Yunxiao Shi, Meysam Sadeghigooghari, Shweta Mahajan, Apratim Bhattacharyya, Litian Liu, Risheek Garrepalli, Thomas Svantesson, Fatih Porikli, Hong Cai</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
当前端到端自动驾驶系统中的视觉-语言-动作（VLA）模型虽然在规划方面表现出色，但大多依赖于稀疏的轨迹标注进行模仿学习，未能充分发挥其作为生成模型的潜力。这导致模型在处理长时序推理、多智能体交互以及处理模糊或长尾场景时存在局限性，并且语言指令与实际动作之间可能存在不一致。</p>
<p><strong>2. 主要创新点/方法论贡献：</strong>
本文提出了<strong>GeRo (Generative Scenario Rollouts)</strong>，一个即插即用的框架，旨在解决上述问题。GeRo 的核心创新在于：</p>
<ul>
<li><strong>联合规划与生成：</strong> GeRo 首次将场景生成与 VLA 模型中的规划、运动预测和语言理解任务相结合，通过<strong>自回归回滚策略</strong>生成语言驱动的未来交通场景。</li>
<li><strong>两阶段训练框架：</strong><ul>
<li><strong>预训练阶段：</strong> VLA 模型被训练来将感知到的车辆和周围智能体的动态编码为紧凑的<strong>潜在（latent）令牌（tokens）</strong>。此阶段通过规划、运动预测和语言任务进行联合监督，旨在实现文本对齐的生成，减少语言-动作不匹配。</li>
<li><strong>语言条件化场景回滚阶段：</strong> 模型接收多视图图像、场景描述和关于自车动作的问题，自回归地生成未来的潜在令牌和文本响应，以指导<strong>长时序回滚</strong>。</li>
</ul>
</li>
<li><strong>回滚一致性损失 (Rollout-Consistency Loss)：</strong> 引入了回滚一致性损失来稳定预测，利用真实标签或伪标签来减轻漂移并保持文本-动作对齐。这包括<strong>时间一致性监督</strong>（通过 KL 散度对齐潜在分布）和<strong>基于模型的监督</strong>（使用预训练模型生成的伪标签）。</li>
<li><strong>强化学习集成 (GRPO)：</strong> 将<strong>广义回滚策略优化 (GRPO)</strong> 与生成式回滚相结合，引入了新的奖励函数，该函数联合优化轨迹准确性、语义对齐以及安全指标（如碰撞避免和碰撞时间 TTC），以实现高保真度和可解释的规划行为。</li>
<li><strong>交互式视觉问答 (VQA) 组件：</strong> GeRo 集成了一个 VQA 组件，用于在回滚过程中将自车意图与自然语言联系起来，回答场景特定的问题，增强了可解释性和语言引导的推理能力。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong>
*   <strong>性能提升显著：</strong> 在 Bench2Drive 数据集上，GeRo 在<strong>驾驶得分 (Driving Score)</strong> 和<strong>成功率 (Success Rate)</strong> 方面分别取得了 <strong>+15.7%</strong> 和 <strong>+26.2%</strong> 的显著提升。
*   <strong>达到 SOTA 性能：</strong> 通过集成强化学习和生成式回滚，GeRo 在<strong>闭环和开环评估</strong>中均取得了<strong>最先进 (state-of-the-art)</strong> 的性能。
*   <strong>零样本鲁棒性：</strong> GeRo 展示了强大的<strong>零样本（zero-shot）鲁棒性</strong>，在处理未见过或长尾场景时表现出色。
*   <strong>可解释性增强：</strong> 语言引导的场景回滚和 VQA 组件使得模型的决策过程更加<strong>可解释</strong>，能够生成与语言推理一致的驾驶行为。
*   <strong>通用性：</strong> GeRo 被设计为一个<strong>即插即用</strong>的框架，可以集成到现有的 VLA 模型中，如 Qwen2.5VL 和 ORION，证明了其通用性。</p>
<p><strong>4. 局限性：</strong>
论文中未明确指出明显的局限性，但其方法依赖于预训练模型和大量的训练数据。在极端的、前所未有的场景下，其泛化能力仍可能受到挑战。此外，虽然引入了 VQA 组件，但完全理解和生成复杂、多模态的语言指令仍是一个持续的研究方向。</p>
<p><strong>5. 未来研究方向：</strong>
*   <strong>更复杂的场景理解和生成：</strong> 进一步探索如何处理更复杂、更具挑战性的交通场景，例如涉及更多智能体交互、不确定性更高的环境。
*   <strong>更精细的语言指令理解：</strong> 提升模型对细微、抽象或隐含语言指令的理解能力。
*   <strong>实时性优化：</strong> 尽管 GeRo 取得了 SOTA 性能，但对于实际的自动驾驶应用，进一步优化模型的推理速度和实时性至关重要。
*   <strong>多模态融合的深度探索：</strong> 探索更深层次的多模态融合机制，以更有效地整合视觉、语言和动作信息。
*   <strong>真实世界部署的挑战：</strong> 将 GeRo 的能力从仿真环境迁移到真实世界的自动驾驶系统，需要解决传感器噪声、执行器延迟等实际问题。</p>
<p><strong>总结：</strong>
GeRo 论文提出了一种新颖的生成式场景回滚框架，通过将语言引导的场景生成与 VLA 模型相结合，显著提升了端到端自动驾驶系统的规划能力、鲁棒性和可解释性。其核心贡献在于自回归回滚策略、回滚一致性损失以及与强化学习的有效集成，为构建更安全、更智能的自动驾驶系统提供了重要的技术路径。该工作在计算机视觉领域，特别是在自动驾驶的 VLA 模型研究中，具有重要的理论和实践意义。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose Generative Scenario Rollouts (GeRo), a plug-and-play framework for VLA models that jointly performs planning and generation of language-grounded future traffic scenes through an autoregressive rollout strategy.</li>
<li>By integrating reinforcement learning with generative rollouts, GeRo achieves state-of-the-art closed-loop and open-loop performance, demonstrating strong zero-shot robustness.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.11475v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.11475v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.11464v1'></a></p>
<h2 id="mha2mla-vlm-enabling-deepseeks-economical-multi-head-latent-attention-across-vision-language-models"><a href="https://arxiv.org/abs/2601.11464v1">MHA2MLA-VLM: Enabling DeepSeek's Economical Multi-Head Latent Attention across Vision-Language Models</a></h2>
<p><strong>Authors:</strong> Xiaoran Fan, Zhichao Sun, Tao Ji, Lixing Shen, Tao Gui</p>
<p><strong>Published:</strong> 2026-01-16</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.CL, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>As vision-language models (VLMs) tackle increasingly complex and multimodal tasks, the rapid growth of Key-Value (KV) cache imposes significant memory and computational bottlenecks during inference. While Multi-Head Latent Attention (MLA) offers an effective means to compress the KV cache and accelerate inference, adapting existing VLMs to the MLA architecture without costly pretraining remains largely unexplored. In this work, we present MHA2MLA-VLM, a parameter-efficient and multimodal-aware framework for converting off-the-shelf VLMs to MLA. Our approach features two core techniques: (1) a modality-adaptive partial-RoPE strategy that supports both traditional and multimodal settings by selectively masking nonessential dimensions, and (2) a modality-decoupled low-rank approximation method that independently compresses the visual and textual KV spaces. Furthermore, we introduce parameter-efficient fine-tuning to minimize adaptation cost and demonstrate that minimizing output activation error, rather than parameter distance, substantially reduces performance loss. Extensive experiments on three representative VLMs show that MHA2MLA-VLM restores original model performance with minimal supervised data, significantly reduces KV cache footprint, and integrates seamlessly with KV quantization.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“MHA2MLA-VLM: Enabling DeepSeek's Economical Multi-Head Latent Attention across Vision-Language Models”的全面中文摘要：</p>
<p><strong>论文题目：</strong> MHA2MLA-VLM: Enabling DeepSeek's Economical Multi-Head Latent Attention across Vision-Language Models</p>
<p><strong>作者：</strong> Xiaoran Fan, Zhichao Sun, Tao Ji, Lixing Shen, Tao Gui</p>
<hr />
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong></p>
<p>随着视觉-语言模型（VLMs）在处理日益复杂和多模态任务方面的能力增强，其推理过程中关键值（KV）缓存的快速增长带来了显著的内存和计算瓶颈。虽然多头潜在注意力（MLA）机制能够有效压缩KV缓存并加速推理，但将现有VLMs适配到MLA架构而无需昂贵的预训练，这一领域的研究尚不充分。</p>
<p><strong>2. 主要创新点/方法论贡献：</strong></p>
<p>本文提出了 <strong>MHA2MLA-VLM</strong>，一个参数高效且多模态感知（multimodal-aware）的框架，用于将现有的VLMs转换为MLA架构。其核心创新包括：</p>
<ul>
<li><strong>多模态自适应部分RoPE策略 (Modality-Adaptive Partial-RoPE):</strong> 该策略能够根据输入模态（视觉或文本）选择性地屏蔽非关键的维度，从而支持传统和多模态设置，实现高效的架构迁移。</li>
<li><strong>多模态解耦低秩近似方法 (Modality-Decoupled Low-Rank Approximation):</strong> 该方法独立地压缩视觉和文本的KV空间，有效降低了截断损失，并最大化了预训练权重的复用。</li>
<li><strong>参数高效微调 (PEFT):</strong> 引入了参数高效的微调策略，以最小化适配成本。研究表明，最小化输出激活误差而非参数距离，能显著减少性能损失。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong></p>
<ul>
<li><strong>性能恢复与效率提升：</strong> MHA2MLA-VLM在三种代表性VLMs（LLaVA-1.5, LLaVA-NeXT, Qwen2.5-VL）上进行了广泛实验。结果表明，该框架在仅使用少量监督数据的情况下，能够恢复原始模型的性能，同时显著减小KV缓存的占用空间，从而提高推理效率。</li>
<li><strong>兼容性：</strong> 该方法能够无缝集成KV量化技术，进一步提升压缩效果。</li>
<li><strong>成本效益：</strong> 通过PEFT策略，显著降低了模型适配的计算和数据成本。例如，Qwen2.5-VL的适配时间从22小时缩短到9小时。</li>
<li><strong>通用性：</strong> MHA2MLA-VLM在不同架构的VLMs上都表现出有效性，证明了其通用性。</li>
</ul>
<p><strong>4. 提及的局限性：</strong></p>
<p>论文中未明确提及具体的局限性，但其研究重点在于解决KV缓存瓶颈和适配MLA架构，暗示了在更广泛的模态融合、更复杂的推理任务或极端压缩比下可能仍存在挑战。</p>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>更广泛的模态融合：</strong> 进一步探索如何更精细地处理不同模态之间的交互，以应对更复杂的跨模态任务。</li>
<li><strong>极端压缩比下的性能：</strong> 研究在极高的KV缓存压缩比下，如何进一步优化性能，减少潜在的性能损失。</li>
<li><strong>动态KV缓存管理：</strong> 结合动态KV缓存管理策略，实现更灵活和高效的推理。</li>
<li><strong>更广泛的模型适配：</strong> 将该框架扩展到更多类型的VLMs和大型语言模型。</li>
</ul>
<p><strong>论文的创新性与重要性：</strong></p>
<p>这篇论文在计算机视觉和自然语言处理交叉领域具有重要意义。它首次提出了一个系统性的框架，能够高效地将现有的多模态视觉-语言模型迁移到MLA这一更具成本效益的注意力架构，解决了困扰VLMs发展的关键瓶颈问题。通过创新的多模态自适应RoPE和解耦低秩近似方法，以及参数高效的微调策略，该研究为实现更高效、可扩展的多模态AI模型提供了重要的技术支撑，尤其是在资源受限的环境下。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this work, we present MHA2MLA-VLM, a parameter-efficient and multimodal-aware framework for converting off-the-shelf VLMs to MLA.</li>
<li>Our approach features two core techniques: (1) a modality-adaptive partial-RoPE strategy that supports both traditional and multimodal settings by selectively masking nonessential dimensions, and (2) a modality-decoupled low-rank approximation method that independently compresses the visual and textual KV spaces.</li>
<li>Furthermore, we introduce parameter-efficient fine-tuning to minimize adaptation cost and demonstrate that minimizing output activation error, rather than parameter distance, substantially reduces performance loss.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.11464v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.11464v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.11442v1'></a></p>
<h2 id="map2thought-explicit-3d-spatial-reasoning-via-metric-cognitive-maps"><a href="https://arxiv.org/abs/2601.11442v1">Map2Thought: Explicit 3D Spatial Reasoning via Metric Cognitive Maps</a></h2>
<p><strong>Authors:</strong> Xiangjun Gao, Zhensong Zhang, Dave Zhenyu Chen, Songcen Xu, Long Quan, Eduardo Pérez-Pellitero, Youngkyoon Jang</p>
<p><strong>Published:</strong> 2026-01-16</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>We propose Map2Thought, a framework that enables explicit and interpretable spatial reasoning for 3D VLMs. The framework is grounded in two key components: Metric Cognitive Map (Metric-CogMap) and Cognitive Chain-of-Thought (Cog-CoT). Metric-CogMap provides a unified spatial representation by integrating a discrete grid for relational reasoning with a continuous, metric-scale representation for precise geometric understanding. Building upon the Metric-CogMap, Cog-CoT performs explicit geometric reasoning through deterministic operations, including vector operations, bounding-box distances, and occlusion-aware appearance order cues, producing interpretable inference traces grounded in 3D structure. Experimental results show that Map2Thought enables explainable 3D understanding, achieving 59.9% accuracy using only half the supervision, closely matching the 60.9% baseline trained with the full dataset. It consistently outperforms state-of-the-art methods by 5.3%, 4.8%, and 4.0% under 10%, 25%, and 50% training subsets, respectively, on the VSI-Bench.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为一名计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行深入分析。</p>
<p><strong>论文摘要分析：Map2Thought: Explicit 3D Spatial Reasoning via Metric Cognitive Maps</strong></p>
<p><strong>1. 论文的主要贡献（2-3句话）：</strong></p>
<p>本研究提出了Map2Thought框架，旨在实现3D视觉语言模型（VLMs）的显式和可解释的三维空间推理。该框架通过结合离散关系推理和连续度量几何表示的Metric-CogMap，以及基于此进行的确定性几何推理的Cog-CoT，实现了对3D场景的深入理解。实验证明，Map2Thought在显著减少监督数据的情况下，仍能达到与全监督基线相当的性能，并在不同训练数据比例下均显著优于现有SOTA方法。</p>
<p><strong>2. 关键创新点或方法论：</strong></p>
<ul>
<li>
<p><strong>Metric Cognitive Map (Metric-CogMap):</strong> 这是该框架的核心创新之一。它巧妙地融合了两种空间表示：</p>
<ul>
<li><strong>离散网格（Discrete Grid）:</strong> 用于进行<strong>关系推理</strong>。这种表示方式适合捕捉物体之间的相对位置、连通性等抽象关系，类似于人类在思考时可能使用的概念性地图。</li>
<li><strong>连续、度量尺度表示（Continuous, Metric-Scale Representation）:</strong> 用于进行<strong>精确的几何理解</strong>。这部分提供了物体在三维空间中的实际尺寸、距离等信息，是进行量化推理的基础。</li>
<li><strong>统一性:</strong> 将这两种表示方式整合在一个框架下，使得模型能够同时处理抽象的空间关系和具体的几何度量，这是实现更强大空间推理能力的关键。</li>
</ul>
</li>
<li>
<p><strong>Cognitive Chain-of-Thought (Cog-CoT):</strong> 在Metric-CogMap的基础上，Cog-CoT实现了<strong>显式的几何推理</strong>。其特点在于：</p>
<ul>
<li><strong>确定性操作（Deterministic Operations）:</strong> 利用向量运算、包围盒距离计算以及考虑遮挡的视觉顺序线索等明确定义的几何操作来执行推理。这意味着推理过程是可追踪和可复现的，而非黑箱式的。</li>
<li><strong>可解释的推理轨迹（Interpretable Inference Traces）:</strong> 通过这些确定性操作，模型能够生成清晰的推理步骤，展示其是如何从输入信息推导出结论的，从而实现“可解释性”。</li>
<li><strong>3D结构接地（Grounded in 3D Structure）:</strong> 所有推理都直接基于3D场景的几何结构，确保了推理的准确性和鲁棒性。</li>
</ul>
</li>
</ul>
<p><strong>3. 对该领域的潜在影响：</strong></p>
<ul>
<li><strong>提升3D VLMs的空间推理能力:</strong> Map2Thought提供了一种更系统、更深入的方式来处理3D场景中的空间关系和几何信息，有望显著提升3D VLMs在理解复杂3D环境方面的能力。</li>
<li><strong>推动可解释AI在3D视觉中的应用:</strong> 通过Cog-CoT，该研究为3D视觉模型的可解释性开辟了新的道路。可解释性是AI走向实际应用的关键，尤其是在需要信任和理解的领域。</li>
<li><strong>降低对大规模标注数据的依赖:</strong> 实验结果表明，Map2Thought在仅使用一半监督数据的情况下，性能仍能与全监督基线相当，这预示着该方法可能具有更强的泛化能力和数据效率，对数据标注成本高昂的3D领域具有重要意义。</li>
<li><strong>为更复杂的3D任务奠定基础:</strong> 显式的空间推理能力是解决诸如3D导航、机器人操作、场景重建与理解等复杂任务的基础。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用：</strong></p>
<ul>
<li><strong>机器人导航与感知:</strong> 机器人需要在复杂的3D环境中理解自身位置、障碍物、目标位置等，Map2Thought的显式空间推理能力将极大地帮助机器人进行更安全、更高效的导航。</li>
<li><strong>增强现实（AR）/虚拟现实（VR）:</strong> 在AR/VR应用中，需要精确理解虚拟物体与真实世界3D空间的交互关系，以及用户在3D环境中的位置和姿态。</li>
<li><strong>自动驾驶:</strong> 自动驾驶汽车需要对周围环境进行精确的3D感知和预测，包括车辆、行人、道路等物体的空间关系和运动轨迹。</li>
<li><strong>3D内容创作与编辑:</strong> 艺术家和设计师可以利用更智能的工具来理解和操作3D模型，实现更直观的创作流程。</li>
<li><strong>医学影像分析:</strong> 在分析CT、MRI等3D医学影像时，理解器官之间的空间关系和病灶的位置对于诊断至关重要。</li>
<li><strong>智能家居与物联网:</strong> 理解家庭环境中设备的空间布局和交互关系，可以实现更智能化的控制和自动化。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性：</strong></p>
<ul>
<li><strong>计算复杂度:</strong> 尽管摘要强调了“确定性操作”，但显式的3D几何推理，尤其是在高分辨率的3D场景中，可能仍然面临计算资源和效率的挑战。Metric-CogMap的构建和Cog-CoT的执行都需要一定的计算开销。</li>
<li><strong>对输入数据的依赖:</strong> 框架的性能很大程度上依赖于输入3D数据的质量和表示方式。如果输入数据存在噪声、不完整或表示不准确，可能会影响Metric-CogMap的构建和后续的推理。</li>
<li><strong>泛化到极端复杂场景的能力:</strong> 摘要中提到的实验结果是在VSI-Bench上获得的。虽然表现优异，但其在处理极其复杂、动态变化或包含大量细粒度细节的3D场景时的泛化能力仍需进一步验证。</li>
<li><strong>“可解释性”的程度:</strong> 虽然Cog-CoT生成了“可解释的推理轨迹”，但这种可解释性是基于预定义的几何操作。对于更深层次的、人类直观的“理解”或“意图”的解释，可能仍有待探索。</li>
<li><strong>“隐式”与“显式”的权衡:</strong> 摘要强调了“显式”推理，这与许多现有VLMs依赖于隐式学习的模式不同。这种显式方法的优势在于可解释性，但可能在某些情况下不如隐式方法灵活或高效。</li>
</ul>
<p><strong>总结：</strong></p>
<p>Map2Thought是一项非常有前景的研究，它通过创新的Metric-CogMap和Cog-CoT机制，为3D视觉语言模型带来了更强大、更可解释的空间推理能力。其在数据效率和性能上的突破，以及对可解释性的关注，使其成为3D计算机视觉领域的一个重要进展，并有望在多个实际应用中产生深远影响。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose Map2Thought, a framework that enables explicit and interpretable spatial reasoning for 3D VLMs. The framework is grounded in two key components: Metric Cognitive Map (Metric-CogMap) and Cognitive Chain-of-Thought (Cog-CoT).</li>
<li>Experimental results show that Map2Thought enables explainable 3D understanding, achieving 59.9% accuracy using only half the supervision, closely matching the 60.9% baseline trained with the full dataset.</li>
<li>It consistently outperforms state-of-the-art methods by 5.3%, 4.8%, and 4.0% under 10%, 25%, and 50% training subsets, respectively, on the VSI-Bench.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.11442v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.11442v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.11409v1'></a></p>
<h2 id="topology-guaranteed-image-segmentation-enforcing-connectivity-genus-and-width-constraints"><a href="https://arxiv.org/abs/2601.11409v1">Topology-Guaranteed Image Segmentation: Enforcing Connectivity, Genus, and Width Constraints</a></h2>
<p><strong>Authors:</strong> Wenxiao Li, Xue-Cheng Tai, Jun Liu</p>
<p><strong>Published:</strong> 2026-01-16</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>Existing research highlights the crucial role of topological priors in image segmentation, particularly in preserving essential structures such as connectivity and genus. Accurately capturing these topological features often requires incorporating width-related information, including the thickness and length inherent to the image structures. However, traditional mathematical definitions of topological structures lack this dimensional width information, limiting methods like persistent homology from fully addressing practical segmentation needs. To overcome this limitation, we propose a novel mathematical framework that explicitly integrates width information into the characterization of topological structures. This method leverages persistent homology, complemented by smoothing concepts from partial differential equations (PDEs), to modify local extrema of upper-level sets. This approach enables the resulting topological structures to inherently capture width properties. We incorporate this enhanced topological description into variational image segmentation models. Using some proper loss functions, we are also able to design neural networks that can segment images with the required topological and width properties. Through variational constraints on the relevant topological energies, our approach successfully preserves essential topological invariants such as connectivity and genus counts, simultaneously ensuring that segmented structures retain critical width attributes, including line thickness and length. Numerical experiments demonstrate the effectiveness of our method, showcasing its capability to maintain topological fidelity while explicitly embedding width characteristics into segmented image structures.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Topology-Guaranteed Image Segmentation: Enforcing Connectivity, Genus, and Width Constraints”的全面中文摘要：</p>
<p><strong>论文题目：</strong> Topology-Guaranteed Image Segmentation: Enforcing Connectivity, Genus, and Width Constraints (拓扑保证的图像分割：强制连通性、属和宽度约束)</p>
<p><strong>作者：</strong> Wenxiao Li, Xue-Cheng Tai, Jun Liu</p>
<hr />
<p><strong>1. 研究问题/核心挑战：</strong></p>
<p>图像分割是计算机视觉中的一项基本任务，但现有方法在保留图像结构的关键拓扑属性（如连通性、孔洞数量/属）方面存在挑战。特别是，传统的拓扑分析方法（如持久同调）虽然能保证拓扑的正确性，但忽略了图像结构固有的宽度信息（如厚度和长度）。这种宽度信息的缺失限制了这些方法在实际应用中的有效性，尤其是在医学成像（如血管的真实厚度）和遥感（如道路的宽度）等领域，这些信息至关重要。因此，论文旨在解决如何在图像分割中<strong>同时保留拓扑结构和精确的宽度信息</strong>这一核心问题。</p>
<p><strong>2. 主要创新点/方法贡献：</strong></p>
<p>该论文提出了一种新颖的数学框架，将<strong>宽度信息显式地整合到拓扑结构的表征中</strong>。其核心贡献包括：</p>
<ul>
<li><strong>宽度感知拓扑能量 (Width-Aware Topological Energy - WT Energy)：</strong> 论文引入了一种新的拓扑能量，它基于持久同调，但通过结合<strong>偏微分方程 (PDE) 中的平滑概念</strong>来修改局部极值。具体来说，它利用<strong>平滑的形态学梯度</strong>来处理持久同调的关键点（出生点和死亡点），使得这些关键点在局部邻域内具有平滑的梯度，从而使拓扑结构能够自然地捕捉宽度属性。</li>
<li><strong>数学框架的构建：</strong> 论文详细推导了宽度感知拓扑能量的数学形式，并将其与<strong>变分图像分割模型</strong>和<strong>数据驱动的深度神经网络模型</strong>相结合。</li>
<li><strong>模型整合：</strong><ul>
<li><strong>变分模型：</strong> 提出了拓扑非局部软阈值动力学 (Topo-NLSTD) 模型，将宽度感知拓扑能量作为正则化项纳入其中。</li>
<li><strong>深度学习模型：</strong> 将宽度感知拓扑能量作为损失函数的一部分，用于训练深度神经网络，以实现拓扑和宽度约束下的分割。</li>
</ul>
</li>
<li><strong>平滑形态学梯度：</strong> 为了解决传统形态学操作（如侵蚀和膨胀）的不可微性问题，论文提出了平滑的形态学梯度，这对于在数据驱动模型中进行反向传播至关重要。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong></p>
<ul>
<li><strong>拓扑保真度与宽度精确性的统一：</strong> 实验结果表明，该方法能够<strong>同时有效地保留图像的拓扑结构（连通性、属）和精确的宽度信息</strong>。与仅关注拓扑（如 PH[22]）或仅关注分割精度的传统方法相比，该方法在保留结构细节方面表现出显著优势。</li>
<li><strong>克服传统方法的局限性：</strong> 论文成功解决了传统持久同调方法忽略宽度信息的问题，以及传统分割方法在拓扑约束下可能产生单像素宽度的“伪连接”问题。</li>
<li><strong>在多种模型上的有效性：</strong> 该方法在传统的变分模型和现代的深度学习模型（如 UNet）上都取得了良好的效果，证明了其通用性和鲁棒性。</li>
<li><strong>实际应用价值：</strong> 论文展示了该方法在医学成像（如膀胱壁分割）和道路分割等任务中的潜力，这些任务对拓扑和宽度信息的准确性要求很高。</li>
</ul>
<p><strong>4. 提及的局限性：</strong></p>
<ul>
<li><strong>计算复杂度：</strong> 论文提到，基于持久同调的方法通常计算复杂度较高，这可能会影响模型的推理速度。</li>
<li><strong>手动指定通道：</strong> 在深度学习模型中，需要手动指定具有明确拓扑特征的通道，并为其制定能量函数，这增加了模型设计的复杂性。</li>
<li><strong>宽度信息的一致性：</strong> 在当前框架中，宽度信息是全局一致的，即所有关键点的平滑处理使用相同的参数。论文指出，未来可以探索<strong>像素级别的宽度预测</strong>，以实现更精细的宽度控制。</li>
</ul>
<p><strong>5. 未来研究方向：</strong></p>
<ul>
<li><strong>结合数据驱动模型与反向传播：</strong> 论文计划将 Topo-NLSTD 模型与数据驱动模型通过<strong>反向传播（unrolling method）</strong>相结合，但目前面临计算时间过长的问题。</li>
<li><strong>像素级宽度预测：</strong> 未来研究将探索使用网络来预测<strong>像素级别的宽度信息</strong>，并为不同像素应用不同半径的结构元素，以实现更精细的宽度控制。</li>
<li><strong>降低计算复杂度：</strong> 进一步研究如何降低持久同调的计算复杂度，以提高模型的效率。</li>
</ul>
<hr />
<p><strong>总结：</strong></p>
<p>这篇论文在图像分割领域做出了重要贡献，成功地将<strong>拓扑约束和精确的宽度信息</strong>整合到一个统一的框架中。通过提出新颖的宽度感知拓扑能量，并将其应用于变分模型和深度学习模型，该方法克服了现有方法的局限性，实现了更准确、更具信息量的图像分割结果。这项工作对于需要精确几何和拓扑信息的应用场景具有重要的理论和实践意义。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To overcome this limitation, we propose a novel mathematical framework that explicitly integrates width information into the characterization of topological structures.</li>
<li>Through variational constraints on the relevant topological energies, our approach successfully preserves essential topological invariants such as connectivity and genus counts, simultaneously ensuring that segmented structures retain critical width attributes, including line thickness and length.</li>
<li>Numerical experiments demonstrate the effectiveness of our method, showcasing its capability to maintain topological fidelity while explicitly embedding width characteristics into segmented image structures.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.11409v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.11409v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.11404v1'></a></p>
<h2 id="acot-vla-action-chain-of-thought-for-vision-language-action-models"><a href="https://arxiv.org/abs/2601.11404v1">ACoT-VLA: Action Chain-of-Thought for Vision-Language-Action Models</a></h2>
<p><strong>Authors:</strong> Linqing Zhong, Yi Liu, Yifei Wei, Ziyu Xiong, Maoqing Yao, Si Liu, Guanghui Ren</p>
<p><strong>Published:</strong> 2026-01-16</p>
<p><strong>Categories:</strong> cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>Vision-Language-Action (VLA) models have emerged as essential generalist robot policies for diverse manipulation tasks, conventionally relying on directly translating multimodal inputs into actions via Vision-Language Model (VLM) embeddings. Recent advancements have introduced explicit intermediary reasoning, such as sub-task prediction (language) or goal image synthesis (vision), to guide action generation. However, these intermediate reasoning are often indirect and inherently limited in their capacity to convey the full, granular information required for precise action execution. Instead, we posit that the most effective form of reasoning is one that deliberates directly in the action space. We introduce Action Chain-of-Thought (ACoT), a paradigm where the reasoning process itself is formulated as a structured sequence of coarse action intents that guide the final policy. In this paper, we propose ACoT-VLA, a novel architecture that materializes the ACoT paradigm. Specifically, we introduce two complementary components: an Explicit Action Reasoner (EAR) and Implicit Action Reasoner (IAR). The former proposes coarse reference trajectories as explicit action-level reasoning steps, while the latter extracts latent action priors from internal representations of multimodal input, co-forming an ACoT that conditions the downstream action head to enable grounded policy learning. Extensive experiments in real-world and simulation environments demonstrate the superiority of our proposed method, which achieves 98.5%, 84.1%, and 47.4% on LIBERO, LIBERO-Plus and VLABench, respectively.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“ACoT-VLA: Action Chain-of-Thought for Vision-Language-Action Models”的全面中文摘要：</p>
<p><strong>论文题目：</strong> ACoT-VLA: Action Chain-of-Thought for Vision-Language-Action Models</p>
<p><strong>作者：</strong> Linqing Zhong, Yi Liu, Yifei Wei, Ziyu Xiong, Maoqing Yao, Si Liu, Guanghui Ren</p>
<hr />
<p><strong>摘要：</strong></p>
<p><strong>1. 主要问题/研究问题：</strong>
当前主流的视觉-语言-动作（VLA）模型在执行多样化机器人任务时，通常直接将多模态输入（视觉和语言）映射到动作。尽管近期研究引入了中间推理步骤（如语言子任务预测或视觉目标图像合成）来指导动作生成，但这些中间推理往往是间接的，且在传达精确动作执行所需全部细节信息方面存在局限性。论文的核心问题在于，如何更有效地进行推理，以实现更精确、更具泛化能力的机器人策略学习，尤其是在感知（输入）和动作（输出）之间存在显著差异的情况下。</p>
<p><strong>2. 关键创新/方法论贡献：</strong>
该论文提出了一种名为<strong>Action Chain-of-Thought (ACoT)</strong> 的新范式，其核心思想是将推理过程本身构建为一个<strong>粗粒度的、显式的、以动作空间为中心的意图序列</strong>，直接指导最终的动作策略。为了实现这一范式，论文提出了<strong>ACoT-VLA</strong> 架构，该架构包含两个关键组件：</p>
<ul>
<li><strong>显式动作推理器 (Explicit Action Reasoner, EAR)：</strong> 该组件生成粗粒度的参考轨迹，作为显式的动作层级推理步骤。它通过一个轻量级 Transformer 模型，将 VLM 的上下文信息与输入的动作序列相结合，生成可执行的动作参考。</li>
<li><strong>隐式动作推理器 (Implicit Action Reasoner, IAR)：</strong> 该组件从 VLM 的内部表示中提取隐式的动作先验。它利用交叉注意力机制，从 VLM 的键值缓存中提取与动作相关的语义信息，为策略学习提供补充性的行为指导。</li>
</ul>
<p>ACoT-VLA 架构将 EAR 和 IAR 产生的显式和隐式动作指导进行融合，通过一个动作引导预测（Action-Guided Prediction, AGP）头，最终生成可执行的动作序列。这种方法将推理直接置于动作空间，弥合了感知与动作之间的鸿沟，从而实现更扎实的策略学习。</p>
<p><strong>3. 主要结果及其意义：</strong>
论文在多个模拟和真实世界环境中进行了广泛的实验评估。
*   <strong>模拟实验：</strong> 在 LIBERO、LIBERO-Plus 和 VLABench 基准测试中，ACoT-VLA 取得了显著的性能提升，分别达到了 98.5%、84.1% 和 47.4% 的成功率。特别是在 LIBERO-Plus 上，ACoT-VLA 在面对各种扰动（如视角变化、初始状态变化等）时表现出更强的鲁棒性，显著优于现有方法。
*   <strong>真实世界实验：</strong> 在 AgiBot G1 机器人平台上进行的“擦拭污渍”、“倒水”和“开放式拾取”等任务中，ACoT-VLA 也取得了比基线方法更高的成功率，证明了其在真实世界中的有效性和跨实体适应性。</p>
<p>这些结果表明，将推理过程直接置于动作空间（ACoT 范式）是一种更有效的方式，能够显著提升 VLA 模型在复杂机器人任务中的性能、泛化能力和鲁棒性。</p>
<p><strong>4. 提及的局限性：</strong>
*   <strong>计算成本：</strong> 引入推理模块（EAR 和 IAR）会增加额外的计算成本，这可能对资源受限的机器人平台部署构成挑战。
*   <strong>动作表示的局限性：</strong> 当前社区普遍采用的动作表示（如关节角度或末端执行器姿态）缺乏显式的几何结构，这可能限制了 ACoT 推理潜力的充分发挥，尤其是在需要更高层次的空间推理（如物体中心协调和接触几何）的任务中。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>动作表示的增强：</strong> 将动作表示扩展到具有空间几何信息的三维空间，以支持更具几何可解释性的 ACoT 推理。
*   <strong>资源优化：</strong> 进一步优化模型以降低计算成本，使其更适合部署在资源受限的机器人平台上。
*   <strong>更广泛的应用：</strong> 探索 ACoT 范式在更广泛的机器人任务和更复杂的场景中的应用。</p>
<p><strong>论文的创新性/重要性：</strong>
该论文的核心贡献在于提出了<strong>Action Chain-of-Thought (ACoT)</strong> 这一新颖的推理范式，将机器人策略学习的推理过程从抽象的语言或视觉空间转移到更直接、更具物理意义的动作空间。通过显式和隐式的动作推理器（EAR 和 IAR）的协同工作，ACoT-VLA 架构有效地弥合了感知与动作之间的差距，显著提升了 VLA 模型在复杂任务中的性能和泛化能力。这项工作为开发更强大、更可靠的通用机器人策略提供了新的思路和方法。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce Action Chain-of-Thought (ACoT), a paradigm where the reasoning process itself is formulated as a structured sequence of coarse action intents that guide the final policy.</li>
<li>In this paper, we propose ACoT-VLA, a novel architecture that materializes the ACoT paradigm.</li>
<li>Specifically, we introduce two complementary components: an Explicit Action Reasoner (EAR) and Implicit Action Reasoner (IAR).</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.11404v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.11404v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.11359v1'></a></p>
<h2 id="think-clip-sample-slow-fast-frame-selection-for-video-understanding"><a href="https://arxiv.org/abs/2601.11359v1">Think-Clip-Sample: Slow-Fast Frame Selection for Video Understanding</a></h2>
<p><strong>Authors:</strong> Wenhui Tan, Ruihua Song, Jiaze Li, Jianzhong Ju, Zhenbo Luo</p>
<p><strong>Published:</strong> 2026-01-16</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>Recent progress in multi-modal large language models (MLLMs) has significantly advanced video understanding. However, their performance on long-form videos remains limited by computational constraints and suboptimal frame selection. We present Think-Clip-Sample (TCS), a training-free framework that enhances long video understanding through two key components: (i) Multi-Query Reasoning, which generates multiple queries to capture complementary aspects of the question and video; and (ii) Clip-level Slow-Fast Sampling, which adaptively balances dense local details and sparse global context. Extensive experiments on MLVU, LongVideoBench, and VideoMME demonstrate that TCS consistently improves performance across different MLLMs, boosting up to 6.9% accuracy, and is capable of achieving comparable accuracy with 50% fewer inference time cost, highlighting both efficiency and efficacy of TCS on long video understanding.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将为您提供这篇论文的全面中文摘要。</p>
<p><strong>论文题目：</strong> Think-Clip-Sample: Slow-Fast Frame Selection for Video Understanding</p>
<p><strong>作者：</strong> Wenhui Tan, Ruihua Song, Jiaze Li, Jianzhong Ju, Zhenbo Luo</p>
<hr />
<p><strong>论文摘要</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong></p>
<p>本文旨在解决多模态大语言模型（MLLMs）在理解长视频时面临的两个主要挑战：<strong>计算资源限制</strong>和<strong>次优的帧选择策略</strong>。长视频通常包含海量帧，直接处理会带来巨大的计算开销，而现有的统一帧采样方法（如每秒固定帧率采样）无法有效区分帧的重要性，导致模型性能不佳。现有的一些帧选择方法虽然有所改进，但仍存在局限：它们过度依赖直接的问句-帧相似度，忽略了问句本身的局限性和视频内容的复杂性，可能导致采样不均衡，遗漏关键信息。</p>
<p><strong>2. 主要创新/方法贡献：</strong></p>
<p>作者提出了一个名为 <strong>Think-Clip-Sample (TCS)</strong> 的训练无关（training-free）框架，通过两个核心组件来提升长视频理解能力：</p>
<ul>
<li><strong>多查询推理 (Multi-Query Reasoning)：</strong> 为了克服单一问句的局限性，TCS首先利用MLLM根据原始问句和视频的少量低分辨率帧，生成多个不同视角的查询（例如，关于物体、场景、动作等）。这些多视角查询能够更全面地捕捉视频内容，并引导模型检索更具互补性的信息，从而更有效地进行帧选择。</li>
<li><strong>片段级慢快采样 (Clip-level Slow-Fast Sampling)：</strong> 为了解决采样不均衡和遗漏全局信息的问题，TCS引入了一种新颖的采样策略。该策略首先识别出高相似度的“片段”（clips），并将大部分帧预算分配给这些包含关键细节的片段（慢采样）。剩余的帧则从非片段区域均匀采样（快采样），以确保全局上下文的覆盖。这种平衡的分配方式兼顾了局部细节和全局信息。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong></p>
<ul>
<li><strong>性能提升：</strong> 在MLVU、Long VideoBench和VideoMME三个长视频理解基准测试中，TCS在不同MLLMs上均取得了显著的性能提升。在MiMo-VL-7B模型上，TCS的准确率最高提升了 <strong>6.9%</strong>。</li>
<li><strong>效率提升：</strong> TCS在保持可比性能的同时，能够显著降低推理成本。在Qwen2-VL-7B模型上，推理时间减少了 <strong>50%</strong> 以上，这对于处理长视频至关重要。</li>
<li><strong>通用性：</strong> TCS作为一个通用的帧选择框架，能够与多种MLLMs集成，并有效提升其在长视频理解任务上的表现。</li>
<li><strong>方法验证：</strong> 消融实验表明，多查询推理和慢快采样这两个组件都对性能提升做出了贡献，并且它们之间存在互补性。</li>
</ul>
<p><strong>4. 提及的局限性：</strong></p>
<p>论文中并未明确指出TCS方法本身的局限性。然而，从其设计理念来看，该方法依赖于MLLM生成高质量的多视角查询，以及CLIP模型进行有效的帧相似度计算。如果MLLM生成查询的能力不足，或者CLIP在特定场景下表现不佳，可能会影响TCS的整体效果。此外，虽然效率有所提升，但对于极长或极高分辨率的视频，计算开销仍然可能是一个挑战。</p>
<p><strong>5. 未来研究方向：</strong></p>
<p>论文的结论部分提到了未来的研究方向：</p>
<ul>
<li><strong>融合多模态信息：</strong> 探索将<strong>音频</strong>和<strong>语音信号</strong>纳入考虑，以实现更复杂的视频理解。</li>
<li><strong>进一步优化：</strong> 尽管TCS已经展示了良好的效率，但对于资源受限的应用场景，可能还需要进一步探索更高效的计算方法。</li>
</ul>
<hr />
<p><strong>总结：</strong></p>
<p>“Think-Clip-Sample: Slow-Fast Frame Selection for Video Understanding” 论文提出了一种创新的、训练无关的帧选择框架TCS，有效解决了长视频理解中计算成本高和帧选择次优的问题。通过引入“多查询推理”来克服单一问句的局限性，以及“片段级慢快采样”来平衡局部细节和全局上下文，TCS显著提升了现有MLLMs在长视频理解任务上的性能，并大幅降低了推理时间。这项工作对于推动MLLMs在长视频内容分析领域的应用具有重要意义，并为未来的多模态视频理解研究开辟了新的方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present Think-Clip-Sample (TCS), a training-free framework that enhances long video understanding through two key components: (i) Multi-Query Reasoning, which generates multiple queries to capture complementary aspects of the question and video; and (ii) Clip-level Slow-Fast Sampling, which adaptively balances dense local details and sparse global context.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.11359v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.11359v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.11322v1'></a></p>
<h2 id="enhancing-vision-language-models-with-logic-reasoning-for-situational-awareness"><a href="https://arxiv.org/abs/2601.11322v1">Enhancing Vision Language Models with Logic Reasoning for Situational Awareness</a></h2>
<p><strong>Authors:</strong> Pavana Pradeep, Krishna Kant, Suya Yu</p>
<p><strong>Published:</strong> 2026-01-16</p>
<p><strong>Categories:</strong> cs.CV, cs.LO</p>
<p><strong>Abstract:</strong></p>
<p>Vision-Language Models (VLMs) offer the ability to generate high-level, interpretable descriptions of complex activities from images and videos, making them valuable for situational awareness (SA) applications. In such settings, the focus is on identifying infrequent but significant events with high reliability and accuracy, while also extracting fine-grained details and assessing recognition quality. In this paper, we propose an approach that integrates VLMs with traditional computer vision methods through explicit logic reasoning to enhance SA in three key ways: (a) extracting fine-grained event details, (b) employing an intelligent fine-tuning (FT) strategy that achieves substantially higher accuracy than uninformed selection, and (c) generating justifications for VLM outputs during inference. We demonstrate that our intelligent FT mechanism improves the accuracy and provides a valuable means, during inferencing, to either confirm the validity of the VLM output or indicate why it may be questionable.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于利用逻辑推理增强视觉语言模型（VLM）以实现态势感知（SA）能力的论文的全面中文摘要。</p>
<p><strong>论文题目：</strong> Enhancing Vision Language Models with Logic Reasoning for Situational Awareness (利用逻辑推理增强视觉语言模型以实现态势感知)</p>
<p><strong>作者：</strong> Pavana Pradeep Kumar, Krishna Kant, Suya You</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong></p>
<p>本研究旨在解决在态势感知（SA）应用中，如何提高视觉语言模型（VLM）对复杂场景和事件的理解能力，特别是识别罕见但重要的事件，并提取精细细节，同时确保输出的可靠性和可解释性。现有的VLM虽然能生成高层级的描述，但在捕捉精细的定量信息（如精确位置、距离、运动轨迹等）以及在处理不常见事件时存在不足。此外，VLM的微调（Fine-Tuning, FT）过程，尤其是针对罕见事件，成本高昂且效率不高。</p>
<p><strong>2. 主要创新点/方法论贡献：</strong></p>
<p>该论文提出了一种创新的方法，将VLM与传统计算机视觉（TCV）方法通过<strong>显式逻辑推理</strong>相结合，以增强态势感知能力。其核心贡献体现在三个方面：</p>
<ul>
<li><strong>精细事件细节提取：</strong> 利用TCV方法捕捉VLM难以处理的精细、定量信息，如物体属性、精确位置、运动轨迹等，并将其整合到VLM的输出中。</li>
<li><strong>智能微调（FT）策略：</strong> 提出了一种<strong>一致性驱动的微调（Consistency-Driven Fine-Tuning）</strong>机制。该机制利用一个辅助VLM（VLMª）和一个TCV模块来生成代理活动（proxy activities），并与主VLM（VLMm）的输出进行逻辑一致性检查。当检测到不一致时，才智能地选择需要微调的视频片段，从而显著提高微调效率和准确性，避免了对大量未标记数据的依赖。</li>
<li><strong>推理过程中的可解释性/合理性检查：</strong> 在推理阶段，利用逻辑推理机制对VLM的输出进行<strong>合理性检查（justification）</strong>。这不仅可以确认VLM输出的有效性，还能在输出可能存在疑问时提供指示，增强了系统的可靠性。</li>
</ul>
<p>具体来说，该方法构建了三个任务集：主任务（Am，目标活动）、辅助任务（Aª，由TCV识别的低级活动）和代理任务（AP，用于一致性检查的简单活动）。通过逻辑断言和SMT（Satisfiability Modulo Theories）求解器来执行一致性检查。</p>
<p><strong>3. 主要结果与意义：</strong></p>
<ul>
<li><strong>显著提高分类准确率：</strong> 实验结果表明，所提出的<strong>一致性驱动的智能微调</strong>方法在所有测试数据集（TU_DAT、Taekwondo、Kinetics）上，相比于无指导的随机选择微调，显著提高了VLMm和VLMª的分类准确率。</li>
<li><strong>大幅提升一致性：</strong> 一致性改进因子（CIF）显示，智能微调方法在TU_DAT和Taekwondo数据集上，将不一致性数量大幅降低，证明了其在提高模型内部一致性方面的有效性。</li>
<li><strong>高效的微调：</strong> 智能微调策略通过仅选择有问题的视频片段进行微调，大大减少了对标记数据的需求和微调成本。</li>
<li><strong>增强的可解释性：</strong> 推理阶段的合理性检查机制为VLM的输出提供了额外的验证，提高了系统的可信度，这对于安全关键的态势感知应用至关重要。</li>
<li><strong>通用性：</strong> 该方法在多种类型的VLM（包括基于Transformer和SSM的模型，以及是否包含LLM后端）和不同类型的数据集上都表现出良好的泛化能力。</li>
</ul>
<p>该研究的意义在于，它提供了一种更高效、更可靠的方式来训练和部署VLM，使其能够更好地服务于需要高精度、高可靠性和可解释性的态势感知场景，如安全监控、交通管理等。</p>
<p><strong>4. 提及的局限性：</strong></p>
<ul>
<li><strong>资源需求：</strong> 该方法在推理阶段需要运行额外的VLM（VLMª）来进行合理性检查，这会增加计算资源的需求，尤其是在需要实时推理的场景下。</li>
<li><strong>TCV的局限性：</strong> 如果TCV技术需要超越基本的物体/姿态检测和运动跟踪，去识别更复杂的概念，则需要额外的训练数据和训练。</li>
<li><strong>硬件依赖：</strong> 实验中使用了高性能GPU（NVIDIA RTX A6000），虽然作者提到模型可以在较低端硬件上运行，但推理时间会相应增加。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>优化资源利用：</strong> 探索更轻量级的VLMª或更高效的逻辑推理方法，以降低推理阶段的资源消耗。</li>
<li><strong>自动化TCV扩展：</strong> 研究如何更自动化地扩展TCV能力，以处理更复杂的场景和活动识别。</li>
<li><strong>实时性提升：</strong> 进一步优化算法和利用新兴硬件（如NPU），以实现更低的推理延迟，满足更严格的实时性要求。</li>
<li><strong>更复杂的逻辑推理：</strong> 探索更复杂的逻辑推理能力，例如结合时序逻辑，以处理更动态和复杂的态势变化。</li>
<li><strong>自适应合理性检查：</strong> 开发能够根据场景的风险级别或VLM输出的置信度，动态调整合理性检查频率和复杂度的机制。</li>
</ul>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this paper, we propose an approach that integrates VLMs with traditional computer vision methods through explicit logic reasoning to enhance SA in three key ways: (a) extracting fine-grained event details, (b) employing an intelligent fine-tuning (FT) strategy that achieves substantially higher accuracy than uninformed selection, and (c) generating justifications for VLM outputs during inference.</li>
<li>We demonstrate that our intelligent FT mechanism improves the accuracy and provides a valuable means, during inferencing, to either confirm the validity of the VLM output or indicate why it may be questionable.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.11322v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.11322v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.11301v1'></a></p>
<h2 id="samannot-a-memory-efficient-local-open-source-framework-for-interactive-video-instance-segmentation-based-on-sam2"><a href="https://arxiv.org/abs/2601.11301v1">SAMannot: A Memory-Efficient, Local, Open-source Framework for Interactive Video Instance Segmentation based on SAM2</a></h2>
<p><strong>Authors:</strong> Gergely Dinya, András Gelencsér, Krisztina Kupán, Clemens Küpper, Kristóf Karacs, Anna Gelencsér-Horváth</p>
<p><strong>Published:</strong> 2026-01-16</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Current research workflows for precise video segmentation are often forced into a compromise between labor-intensive manual curation, costly commercial platforms, and/or privacy-compromising cloud-based services. The demand for high-fidelity video instance segmentation in research is often hindered by the bottleneck of manual annotation and the privacy concerns of cloud-based tools. We present SAMannot, an open-source, local framework that integrates the Segment Anything Model 2 (SAM2) into a human-in-the-loop workflow. To address the high resource requirements of foundation models, we modified the SAM2 dependency and implemented a processing layer that minimizes computational overhead and maximizes throughput, ensuring a highly responsive user interface. Key features include persistent instance identity management, an automated ``lock-and-refine'' workflow with barrier frames, and a mask-skeletonization-based auto-prompting mechanism. SAMannot facilitates the generation of research-ready datasets in YOLO and PNG formats alongside structured interaction logs. Verified through animal behavior tracking use-cases and subsets of the LVOS and DAVIS benchmark datasets, the tool provides a scalable, private, and cost-effective alternative to commercial platforms for complex video annotation tasks.</p>
<p><strong>Analysis:</strong></p>
<p>作为一名计算机视觉和机器学习领域的专家，我对这篇论文摘要进行了分析，并提供以下中文解读：</p>
<p><strong>1. 论文的主要贡献（2-3句话）</strong></p>
<p>该论文提出了SAMannot，一个内存高效、本地化且开源的交互式视频实例分割框架。它集成了SAM2模型，通过优化处理流程和引入创新的“锁定与精炼”工作流，解决了现有视频分割研究中手动标注耗时、商业平台昂贵以及云服务隐私泄露的问题，为研究人员提供了一个可扩展、私密且经济高效的解决方案。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<p>SAMannot 的核心创新在于其对SAM2模型的优化和集成方法，以及围绕其构建的独特交互式工作流：</p>
<ul>
<li><strong>内存高效与低计算开销：</strong> 论文强调了对SAM2依赖的修改和引入的“处理层”，旨在最小化计算开销并最大化吞吐量，从而实现高响应的用户界面。这对于在本地运行大型基础模型至关重要。</li>
<li><strong>“锁定与精炼”（Lock-and-Refine）工作流与障碍帧（Barrier Frames）：</strong> 这是一个重要的交互式标注机制。它可能意味着用户可以“锁定”一个实例的分割结果，然后模型会尝试在后续帧中自动跟踪和更新该实例，同时允许用户在必要时通过“障碍帧”来纠正或指导模型，从而在效率和精度之间取得平衡。</li>
<li><strong>掩码骨架化自动提示（Mask-Skeletonization-based Auto-Prompting）：</strong> 这种机制利用分割掩码的骨架信息来生成提示，以辅助模型进行更精确的分割。这是一种新颖的利用几何信息来驱动分割的方法，可能比传统的点或框提示更具鲁棒性。</li>
<li><strong>持久实例身份管理：</strong> 确保在视频序列中，同一个实例在不同帧中能够保持一致的身份标识，这对于视频实例分割至关重要，也为后续的数据分析奠定了基础。</li>
</ul>
<p><strong>3. 对该领域的潜在影响</strong></p>
<p>SAMannot 的出现可能对视频实例分割领域产生以下影响：</p>
<ul>
<li><strong>降低研究门槛：</strong> 通过提供一个免费、本地化且易于使用的工具，SAMannot 将极大地降低研究人员进行高质量视频实例分割标注的成本和技术门槛，从而加速相关研究的进展。</li>
<li><strong>促进隐私敏感应用：</strong> 对于涉及敏感数据（如医疗影像、监控视频等）的视频实例分割任务，SAMannot 的本地化特性解决了隐私顾虑，使其成为更具吸引力的选择。</li>
<li><strong>推动数据集的生成和标准化：</strong> 框架支持生成YOLO和PNG格式的数据集，并记录结构化交互日志，这有助于生成更规范、可复用的数据集，并为模型评估提供更详细的依据。</li>
<li><strong>提升交互式标注的效率和质量：</strong> “锁定与精炼”和自动提示机制有望显著提高标注效率，同时通过人工干预和模型辅助的结合，保证标注的精度。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用</strong></p>
<p>除了其核心的视频实例分割研究，SAMannot 还可能在以下领域找到应用：</p>
<ul>
<li><strong>动物行为分析：</strong> 论文中提到的动物行为跟踪用例直接表明了其在该领域的潜力，可以用于精确追踪和量化动物的运动、姿态等。</li>
<li><strong>自动驾驶：</strong> 视频实例分割在识别和跟踪道路上的车辆、行人、自行车等目标方面至关重要，SAMannot 可以用于训练和评估自动驾驶系统的感知模块。</li>
<li><strong>视频编辑与特效：</strong> 精确的实例分割是视频抠像、背景替换、特效合成等高级视频编辑功能的基础。</li>
<li><strong>机器人视觉：</strong> 机器人需要理解和分割其周围环境中的物体，SAMannot 可以帮助机器人进行更精细的环境感知。</li>
<li><strong>医学影像分析：</strong> 在视频医学影像（如内窥镜检查、超声波等）中，精确分割病灶或器官对于诊断和治疗至关重要。</li>
<li><strong>增强现实（AR）/虚拟现实（VR）：</strong> AR/VR 应用需要对真实世界进行实时的场景理解和物体分割，SAMannot 可以为这些应用提供支持。</li>
</ul>
<p><strong>5. 从摘要中可推断的局限性</strong></p>
<p>尽管摘要强调了SAMannot的优势，但仍可推断出一些潜在的局限性：</p>
<ul>
<li><strong>对SAM2的依赖：</strong> SAMannot 的性能和能力很大程度上取决于其集成的SAM2模型。如果SAM2本身存在某些固有的局限性（例如，在某些复杂场景下的分割能力不足），SAMannot 也可能继承这些问题。</li>
<li><strong>“本地化”的硬件要求：</strong> 虽然SAMannot 旨在优化资源利用，但运行大型基础模型（如SAM2）仍然需要一定的计算资源（GPU内存和处理能力）。对于资源极其有限的设备，可能仍存在挑战。</li>
<li><strong>交互式标注的“人工”部分：</strong> 尽管有自动化机制，但“人机协作”的本质意味着标注过程仍然需要人工的参与和判断，尤其是在复杂或模糊的情况下。标注的效率和质量最终仍受限于操作员的熟练程度和投入的时间。</li>
<li><strong>“研究就绪”的定义：</strong> 摘要提到生成“研究就绪”的数据集，但“研究就绪”的定义可能因具体研究需求而异。对于某些高度专业化的任务，可能还需要额外的后处理或格式转换。</li>
<li><strong>SAM2的更新与兼容性：</strong> SAM2本身是一个不断发展的模型。SAMannot 的框架需要保持与SAM2新版本的兼容性，这可能需要持续的维护和更新工作。</li>
</ul>
<p>总而言之，SAMannot 是一项令人兴奋的进展，它通过技术创新解决了视频实例分割研究中的实际痛点，有望在多个领域产生积极影响。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present SAMannot, an open-source, local framework that integrates the Segment Anything Model 2 (SAM2) into a human-in-the-loop workflow.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.11301v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.11301v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.11269v1'></a></p>
<h2 id="x-distill-cross-architecture-vision-distillation-for-visuomotor-learning"><a href="https://arxiv.org/abs/2601.11269v1">X-Distill: Cross-Architecture Vision Distillation for Visuomotor Learning</a></h2>
<p><strong>Authors:</strong> Maanping Shao, Feihong Zhang, Gu Zhang, Baiye Cheng, Zhengrong Xue, Huazhe Xu</p>
<p><strong>Published:</strong> 2026-01-16</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>Visuomotor policies often leverage large pre-trained Vision Transformers (ViTs) for their powerful generalization capabilities. However, their significant data requirements present a major challenge in the data-scarce context of most robotic learning settings, where compact CNNs with strong inductive biases can be more easily optimized. To address this trade-off, we introduce X-Distill, a simple yet highly effective method that synergizes the strengths of both architectures. Our approach involves an offline, cross-architecture knowledge distillation, transferring the rich visual representations of a large, frozen DINOv2 teacher to a compact ResNet-18 student on the general-purpose ImageNet dataset. This distilled encoder, now endowed with powerful visual priors, is then jointly fine-tuned with a diffusion policy head on the target manipulation tasks. Extensive experiments on <script type="math/tex">34</script> simulated benchmarks and <script type="math/tex">5</script> challenging real-world tasks demonstrate that our method consistently outperforms policies equipped with from-scratch ResNet or fine-tuned DINOv2 encoders. Notably, X-Distill also surpasses 3D encoders that utilize privileged point cloud observations or much larger Vision-Language Models. Our work highlights the efficacy of a simple, well-founded distillation strategy for achieving state-of-the-art performance in data-efficient robotic manipulation.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“X-Distill: Cross-Architecture Vision Distillation for Visuomotor Learning”的全面中文摘要：</p>
<p><strong>论文摘要：X-Distill：面向视觉运动学习的跨架构视觉蒸馏</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
该论文旨在解决视觉运动策略（visuomotor policies）在数据稀疏环境下学习的挑战。现有的视觉运动策略通常依赖大型预训练的视觉 Transformer（ViT）模型，这些模型具有强大的泛化能力，但需要大量数据进行训练。而紧凑型卷积神经网络（CNN）虽然具有更强的归纳偏置（inductive biases），更适合数据稀疏场景，但其泛化能力不如ViT。如何在数据效率和模型性能之间取得平衡是核心问题。</p>
<p><strong>2. 主要创新/方法贡献：</strong>
作者提出了<strong>X-Distill</strong>，一种简单而有效的跨架构知识蒸馏方法。其核心创新在于：
*   <strong>跨架构蒸馏：</strong> 将一个大型、冻结的DINOv2（ViT）教师模型的丰富视觉表征，蒸馏到一个紧凑的ResNet-18（CNN）学生模型中。
*   <strong>通用数据集蒸馏：</strong> 蒸馏过程在通用的ImageNet数据集上进行，而非特定于机器人任务的数据集，从而使学生模型获得领域无关的视觉先验知识。
*   <strong>两阶段学习流程：</strong>
    *   <strong>阶段一（知识蒸馏）：</strong> 在ImageNet上进行离线蒸馏，使ResNet-18学生模型继承DINOv2的泛化能力。
    *   <strong>阶段二（策略微调）：</strong> 将蒸馏后的ResNet-18编码器与扩散策略（Diffusion Policy）头部联合进行端到端微调，以适应目标机器人操作任务。
*   <strong>利用CNN的归纳偏置：</strong> 通过蒸馏，学生模型（ResNet-18）不仅获得了ViT的泛化能力，还保留了CNN固有的空间局部性和平移等变性等归纳偏置，这对于数据效率至关重要。</p>
<p><strong>3. 主要结果及意义：</strong>
*   <strong>性能优越：</strong> 在34个模拟基准和5个具有挑战性的真实世界任务上，X-Distill方法显著优于使用从头训练的ResNet或微调DINOv2作为编码器的策略。
*   <strong>超越3D编码器和大型VLM：</strong> X-Distill甚至超越了使用特权点云观测的3D编码器以及大型视觉语言模型（VLM）的策略。
*   <strong>数据效率显著：</strong> 在仅有少量（20-25个）演示数据的情况下，X-Distill仍能取得优异的性能，证明了其在数据稀疏环境下的有效性。
*   <strong>语义可分性：</strong> 通过t-SNE可视化和显著图分析，论文表明X-Distill学习到的特征空间具有更好的语义可分性，能够准确区分任务的不同阶段，这对于长时序规划至关重要。
*   <strong>意义：</strong> X-Distill证明了一种简单、基础的蒸馏策略是实现数据高效视觉运动学习的关键。它为在数据受限的机器人学习场景中构建高性能策略提供了一条实用且有效的途径。</p>
<p><strong>4. 提及的局限性：</strong>
*   <strong>直接特征蒸馏的探索空间：</strong> 论文提到，直接的特征蒸馏仍有进一步探索的空间。
*   <strong>动态任务的应用：</strong> X-Distill在动态任务（如移动操作）上的应用仍是未来需要探索的问题。</p>
<p><strong>5. 未来研究方向：</strong>
*   <strong>更复杂的蒸馏技术：</strong> 探索更复杂的中间特征对齐技术，以及从多模态VLA教师那里整合语言先验。
*   <strong>数据丰富场景的扩展：</strong> 研究X-Distill在数据丰富场景下的可扩展性。
*   <strong>动态任务的应用：</strong> 将X-Distill应用于移动操作等动态任务。</p>
<p>总而言之，X-Distill通过一种创新的跨架构知识蒸馏方法，成功地将大型ViT模型的泛化能力与CNN的样本效率相结合，为数据稀疏的机器人学习领域提供了一种强大且高效的解决方案。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this trade-off, we introduce X-Distill, a simple yet highly effective method that synergizes the strengths of both architectures.</li>
<li>Our approach involves an offline, cross-architecture knowledge distillation, transferring the rich visual representations of a large, frozen DINOv2 teacher to a compact ResNet-18 student on the general-purpose ImageNet dataset.</li>
<li>Extensive experiments on <script type="math/tex">34</script> simulated benchmarks and <script type="math/tex">5</script> challenging real-world tasks demonstrate that our method consistently outperforms policies equipped with from-scratch ResNet or fine-tuned DINOv2 encoders.</li>
<li>Our work highlights the efficacy of a simple, well-founded distillation strategy for achieving state-of-the-art performance in data-efficient robotic manipulation.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.11269v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.11269v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2026-01-19 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
