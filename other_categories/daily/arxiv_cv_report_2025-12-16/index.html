<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-12-16 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-15
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-12-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-27
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-12-15/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-12-17/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-12-16">Arxiv Computer Vision Papers - 2025-12-16</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#diffusionbrowser-interactive-diffusion-previews-via-multi-branch-decoders" class="nav-link">DiffusionBrowser: Interactive Diffusion Previews via Multi-Branch Decoders</a>
                </li>
                <li class="nav-item">
                    <a href="#litept-lighter-yet-stronger-point-transformer" class="nav-link">LitePT: Lighter Yet Stronger Point Transformer</a>
                </li>
                <li class="nav-item">
                    <a href="#towards-scalable-pre-training-of-visual-tokenizers-for-generation" class="nav-link">Towards Scalable Pre-training of Visual Tokenizers for Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#recurrent-video-masked-autoencoders" class="nav-link">Recurrent Video Masked Autoencoders</a>
                </li>
                <li class="nav-item">
                    <a href="#i-scene-3d-instance-models-are-implicit-generalizable-spatial-learners" class="nav-link">I-Scene: 3D Instance Models are Implicit Generalizable Spatial Learners</a>
                </li>
                <li class="nav-item">
                    <a href="#laser-layer-wise-scale-alignment-for-training-free-streaming-4d-reconstruction" class="nav-link">LASER: Layer-wise Scale Alignment for Training-Free Streaming 4D Reconstruction</a>
                </li>
                <li class="nav-item">
                    <a href="#feedforward-3d-editing-via-text-steerable-image-to-3d" class="nav-link">Feedforward 3D Editing via Text-Steerable Image-to-3D</a>
                </li>
                <li class="nav-item">
                    <a href="#jova-unified-multimodal-learning-for-joint-video-audio-generation" class="nav-link">JoVA: Unified Multimodal Learning for Joint Video-Audio Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#towards-interactive-intelligence-for-digital-humans" class="nav-link">Towards Interactive Intelligence for Digital Humans</a>
                </li>
                <li class="nav-item">
                    <a href="#agentiad-tool-augmented-single-agent-for-industrial-anomaly-detection" class="nav-link">AgentIAD: Tool-Augmented Single-Agent for Industrial Anomaly Detection</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-12-16">Arxiv Computer Vision Papers - 2025-12-16</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>好的，作为一名专注于计算机视觉和机器学习的研究助理，我将为您提供一份关于2025年12月15日 Arxiv 计算机视觉领域论文的简明执行摘要。</p>
<hr />
<p><strong>执行摘要：2025年12月15日 Arxiv 计算机视觉论文精选</strong></p>
<p><strong>主要主题与趋势：</strong></p>
<p>本期 Arxiv 论文集展现了计算机视觉领域在<strong>生成模型、三维重建与编辑、多模态学习以及高效模型设计</strong>等方面的显著进展。特别是，<strong>扩散模型（Diffusion Models）</strong>在交互式预览和生成任务中展现出新的潜力；<strong>三维视觉</strong>正朝着更具泛化性、可编辑性和实时性的方向发展；<strong>多模态融合</strong>在视频和音频的联合生成方面取得了突破；同时，研究人员也在积极探索<strong>更轻量级、更具可扩展性的模型架构</strong>，以应对日益增长的数据和计算需求。</p>
<p><strong>亮点与创新：</strong></p>
<ul>
<li><strong>DiffusionBrowser (1)</strong> 提出了交互式扩散模型预览的新方法，通过多分支解码器显著提升了用户体验和生成效率，预示着更直观的AI内容创作工具。</li>
<li><strong>I-Scene (5)</strong> 和 <strong>LASER (6)</strong> 在三维重建领域带来了重要进展。I-Scene 强调了隐式三维实例模型在空间学习上的泛化能力，而 LASER 则通过层级尺度对齐实现了训练无关的流式四维重建，为实时三维场景理解和应用奠定了基础。</li>
<li><strong>Feedforward 3D Editing (7)</strong> 展示了通过文本指令实现三维模型编辑的创新，为用户提供了更便捷、更具创造性的三维内容生成方式。</li>
<li><strong>JoVA (8)</strong> 在多模态学习方面取得了突破，实现了视频和音频的联合生成，为更丰富、更具沉浸感的媒体内容创作打开了新的大门。</li>
</ul>
<p><strong>新兴研究方向与技术：</strong></p>
<ul>
<li><strong>交互式生成模型：</strong> 以 DiffusionBrowser 为代表，研究正从静态生成转向更具交互性和用户导向的生成过程。</li>
<li><strong>高效三维表示与重建：</strong> 隐式表示（如 I-Scene）和流式重建技术（如 LASER）是提升三维视觉效率和泛化能力的关键。</li>
<li><strong>文本驱动的三维内容创作：</strong> 文本指令在三维编辑和生成中的应用（如 Feedforward 3D Editing）将成为未来研究的热点。</li>
<li><strong>统一的多模态学习：</strong> 跨模态的联合生成（如 JoVA）将推动更复杂的媒体内容创作和理解。</li>
<li><strong>轻量化与可扩展性：</strong> LitePT (2) 和 Towards Scalable Pre-training (3) 表明了在保持性能的同时，降低模型复杂度和提高训练效率是重要的研究方向。</li>
<li><strong>智能体与工具增强：</strong> AgentIAD (10) 和 Towards Interactive Intelligence (9) 预示着AI在特定领域（如工业检测）和与人类的交互方面，将更加依赖于智能体和工具的协同。</li>
</ul>
<p><strong>建议阅读全文的论文：</strong></p>
<p>考虑到其潜在影响和创新性，以下论文值得深入阅读：</p>
<ol>
<li><strong>DiffusionBrowser (1):</strong> 对于关注生成模型交互性和效率的研究者。</li>
<li><strong>I-Scene (5):</strong> 对于在三维视觉表示和泛化性方面有深入研究需求的研究者。</li>
<li><strong>LASER (6):</strong> 对于需要实时、高效四维重建的研究者。</li>
<li><strong>Feedforward 3D Editing (7):</strong> 对于对文本驱动三维内容创作感兴趣的研究者。</li>
<li><strong>JoVA (8):</strong> 对于研究多模态生成和视频-音频联合学习的研究者。</li>
</ol>
<hr />
<p>希望这份摘要能帮助您快速了解该领域的最新动态。</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2512.13690v1">DiffusionBrowser: Interactive Diffusion Previews via Multi-Branch Decoders</a></li>
<li><a href="#2512.13689v1">LitePT: Lighter Yet Stronger Point Transformer</a></li>
<li><a href="#2512.13687v1">Towards Scalable Pre-training of Visual Tokenizers for Generation</a></li>
<li><a href="#2512.13684v1">Recurrent Video Masked Autoencoders</a></li>
<li><a href="#2512.13683v1">I-Scene: 3D Instance Models are Implicit Generalizable Spatial Learners</a></li>
<li><a href="#2512.13680v1">LASER: Layer-wise Scale Alignment for Training-Free Streaming 4D Reconstruction</a></li>
<li><a href="#2512.13678v1">Feedforward 3D Editing via Text-Steerable Image-to-3D</a></li>
<li><a href="#2512.13677v1">JoVA: Unified Multimodal Learning for Joint Video-Audio Generation</a></li>
<li><a href="#2512.13674v1">Towards Interactive Intelligence for Digital Humans</a></li>
<li><a href="#2512.13671v1">AgentIAD: Tool-Augmented Single-Agent for Industrial Anomaly Detection</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2512.13690v1'></a></p>
<h2 id="diffusionbrowser-interactive-diffusion-previews-via-multi-branch-decoders"><a href="https://arxiv.org/abs/2512.13690v1">DiffusionBrowser: Interactive Diffusion Previews via Multi-Branch Decoders</a></h2>
<p><strong>Authors:</strong> Susung Hong, Chongjian Ge, Zhifei Zhang, Jui-Hsien Wang</p>
<p><strong>Published:</strong> 2025-12-15</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.GR, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Video diffusion models have revolutionized generative video synthesis, but they are imprecise, slow, and can be opaque during generation -- keeping users in the dark for a prolonged period. In this work, we propose DiffusionBrowser, a model-agnostic, lightweight decoder framework that allows users to interactively generate previews at any point (timestep or transformer block) during the denoising process. Our model can generate multi-modal preview representations that include RGB and scene intrinsics at more than 4<script type="math/tex">\times</script> real-time speed (less than 1 second for a 4-second video) that convey consistent appearance and motion to the final video. With the trained decoder, we show that it is possible to interactively guide the generation at intermediate noise steps via stochasticity reinjection and modal steering, unlocking a new control capability. Moreover, we systematically probe the model using the learned decoders, revealing how scene, object, and other details are composed and assembled during the otherwise black-box denoising process.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为一名计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行深入分析。</p>
<p><strong>论文摘要分析：DiffusionBrowser: Interactive Diffusion Previews via Multi-Branch Decoders</strong></p>
<p><strong>1. 论文的主要贡献 (2-3句话的简洁总结):</strong></p>
<p>本论文提出了一种名为 DiffusionBrowser 的模型无关、轻量级的解码器框架，旨在解决现有视频扩散模型生成过程不透明、速度慢的问题。该框架允许用户在扩散过程的任意中间阶段（时间步长或 Transformer 块）交互式地生成预览，并支持多模态输出（如 RGB 和场景内在量），速度超过实时 4 倍。通过这种方式，DiffusionBrowser 实现了对生成过程的实时引导和深入理解，为视频生成带来了前所未有的交互性和可控性。</p>
<p><strong>2. 关键创新或方法论:</strong></p>
<ul>
<li><strong>模型无关的轻量级解码器框架:</strong> 这是 DiffusionBrowser 的核心创新。它不依赖于特定的视频扩散模型架构，而是作为一个通用的解码器层，可以插入到任何现有的视频扩散模型中。这种设计大大提高了其通用性和易用性。</li>
<li><strong>多分支解码器:</strong> 论文提到“multi-branch decoders”，这意味着解码器可能被设计成能够同时处理和生成不同模态的信息（例如，RGB 图像和场景内在量，如深度、法线等）。这使得预览更加丰富和信息量大。</li>
<li><strong>任意中间阶段的预览生成:</strong> 允许用户在扩散过程的任意时间步长或 Transformer 块进行预览，这是对传统“黑盒”生成过程的重大突破。用户不再需要等待整个生成过程完成才能看到结果。</li>
<li><strong>超过 4 倍的实时生成速度:</strong> 这是一个非常显著的性能提升，使得交互式生成成为可能。</li>
<li><strong>交互式引导能力:</strong> 通过“stochasticity reinjection”和“modal steering”等技术，用户可以对中间的生成过程进行干预和引导，从而实现更精细化的控制。</li>
<li><strong>对扩散过程的系统性探究:</strong> 利用训练好的解码器，可以深入分析扩散模型是如何逐步构建场景、对象和细节的，揭示了其内部工作机制。</li>
</ul>
<p><strong>3. 对该领域的潜在影响:</strong></p>
<ul>
<li><strong>提升视频生成的用户体验:</strong> 解决了当前视频扩散模型用户体验差、等待时间长的问题，使得视频生成过程更加直观、高效和可控。</li>
<li><strong>加速视频内容创作:</strong> 对于艺术家、设计师和内容创作者而言，能够实时预览和调整生成结果，将极大地提高工作效率和创意发挥空间。</li>
<li><strong>促进对扩散模型的理解:</strong> 通过交互式探究，可以更深入地理解扩散模型内部的表征学习和生成机制，为后续模型改进提供理论指导。</li>
<li><strong>推动更高级的视频编辑和控制:</strong> 交互式引导能力为实现更精细化的视频编辑和控制（例如，局部修改、风格迁移等）打开了新的可能性。</li>
<li><strong>为其他生成模型提供借鉴:</strong> 这种模型无关的解码器框架和交互式预览的思想，也可能被借鉴到其他类型的生成模型中。</li>
</ul>
<p><strong>4. 可能受益于此研究的相关领域或应用:</strong></p>
<ul>
<li><strong>内容创作与媒体制作:</strong> 电影、动画、广告、游戏等领域的视频内容生成和后期制作。</li>
<li><strong>虚拟现实 (VR) 和增强现实 (AR):</strong> 实时生成和编辑虚拟场景和对象。</li>
<li><strong>3D 内容生成:</strong> 结合场景内在量（如深度、法线）的生成，可以为 3D 重建和渲染提供基础。</li>
<li><strong>机器人和自动驾驶:</strong> 生成逼真的模拟场景，用于训练和测试。</li>
<li><strong>科学可视化:</strong> 生成复杂的动态模拟和可视化结果。</li>
<li><strong>教育和培训:</strong> 创建交互式学习材料和模拟环境。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性:</strong></p>
<ul>
<li><strong>“轻量级”的相对性:</strong> 虽然论文声称是“轻量级”，但其具体计算开销和对硬件的要求仍需进一步评估。与原始扩散模型相比，增加的解码器层可能会带来一定的计算负担。</li>
<li><strong>“模型无关”的实现细节:</strong> 虽然框架是模型无关的，但具体的集成和训练过程可能仍然需要针对不同的基础扩散模型进行一定的调整和优化。</li>
<li><strong>交互式引导的有效性:</strong> 论文提到了“stochasticity reinjection”和“modal steering”，但这些方法的具体实现细节、效果以及用户学习成本尚未明确。其引导的精度和鲁棒性可能是一个挑战。</li>
<li><strong>多模态预览的完整性:</strong> 摘要提到 RGB 和场景内在量，但可能还有其他重要的模态（如运动信息、语义信息等）未被完全覆盖，或者其生成质量需要进一步验证。</li>
<li><strong>“黑盒”的完全揭示:</strong> 尽管论文旨在“revealing how details are composed”，但扩散模型本质上仍然是复杂的，完全“揭示”其内部机制是一个长期挑战，DiffusionBrowser 可能只是提供了一种更有效的探究工具，而非终极解决方案。</li>
<li><strong>训练数据的需求:</strong> 训练这样一个多分支解码器框架可能需要大量的多模态视频数据，这可能会限制其在某些特定领域的应用。</li>
</ul>
<p><strong>总结:</strong></p>
<p>DiffusionBrowser 是一项非常有前景的研究，它通过引入创新的模型无关、轻量级解码器框架，显著改善了视频扩散模型的交互性和可控性。其核心价值在于将“黑盒”的生成过程变得透明和可操作，为视频内容创作和相关领域带来了巨大的潜力。然而，在实际应用中，其计算效率、引导方法的有效性以及对不同模型和数据的适应性仍是值得关注和进一步研究的方面。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this work, we propose DiffusionBrowser, a model-agnostic, lightweight decoder framework that allows users to interactively generate previews at any point (timestep or transformer block) during the denoising process.</li>
<li>With the trained decoder, we show that it is possible to interactively guide the generation at intermediate noise steps via stochasticity reinjection and modal steering, unlocking a new control capability.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.13690v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.13690v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.13689v1'></a></p>
<h2 id="litept-lighter-yet-stronger-point-transformer"><a href="https://arxiv.org/abs/2512.13689v1">LitePT: Lighter Yet Stronger Point Transformer</a></h2>
<p><strong>Authors:</strong> Yuanwen Yue, Damien Robert, Jianyuan Wang, Sunghwan Hong, Jan Dirk Wegner, Christian Rupprecht, Konrad Schindler</p>
<p><strong>Published:</strong> 2025-12-15</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Modern neural architectures for 3D point cloud processing contain both convolutional layers and attention blocks, but the best way to assemble them remains unclear. We analyse the role of different computational blocks in 3D point cloud networks and find an intuitive behaviour: convolution is adequate to extract low-level geometry at high-resolution in early layers, where attention is expensive without bringing any benefits; attention captures high-level semantics and context in low-resolution, deep layers more efficiently. Guided by this design principle, we propose a new, improved 3D point cloud backbone that employs convolutions in early stages and switches to attention for deeper layers. To avoid the loss of spatial layout information when discarding redundant convolution layers, we introduce a novel, training-free 3D positional encoding, PointROPE. The resulting LitePT model has <script type="math/tex">3.6\times</script> fewer parameters, runs <script type="math/tex">2\times</script> faster, and uses <script type="math/tex">2\times</script> less memory than the state-of-the-art Point Transformer V3, but nonetheless matches or even outperforms it on a range of tasks and datasets. Code and models are available at: https://github.com/prs-eth/LitePT.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“LitePT: Lighter Yet Stronger Point Transformer”的全面中文摘要：</p>
<p><strong>论文题目：</strong> LitePT: Lighter Yet Stronger Point Transformer</p>
<p><strong>作者：</strong> Yuanwen Yue, Damien Robert, Jianyuan Wang, Sunghwan Hong, Jan Dirk Wegner, Christian Rupprecht, Konrad Schindler</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 主要问题/研究问题：</strong>
该论文旨在解决当前3D点云处理领域中一个关键的未解之谜：如何最有效地组合卷积层和注意力（Transformer）模块来构建高性能的点云处理网络。现有的现代3D点云处理架构通常同时包含这两种计算单元，但其最佳的组装方式仍不明确。研究人员发现，卷积在早期高分辨率阶段提取局部几何信息方面非常有效且成本较低，而注意力机制在后期低分辨率阶段捕捉高层语义和全局上下文方面更具优势，但早期使用注意力会因计算成本高昂而效益不佳。</p>
<p><strong>2. 关键创新/方法论贡献：</strong>
*   <strong>分层混合架构设计：</strong> 论文提出了一种新的3D点云处理骨干网络 LitePT，其核心思想是根据处理阶段的抽象级别来选择最合适的计算模块。具体来说，LitePT 在网络的早期阶段（高分辨率）采用卷积层来提取局部几何特征，而在网络的后期阶段（低分辨率）切换到注意力模块来捕捉全局语义和长距离依赖关系。这种设计原则旨在最大化效率和性能。
*   <strong>PointROPE 位置编码：</strong> 为了解决在后期阶段丢弃卷积层可能导致空间布局信息丢失的问题，论文引入了一种新颖的、无需训练的3D位置编码方法——PointROPE（Point Rotary Positional Embedding）。PointROPE 是对 Transformer 中常用的 RoPE（Rotary Positional Embedding）的3D点云适应，它通过旋转特征空间来引入相对位置信息，并且是参数自由的，大大降低了模型的参数量和计算负担。
*   <strong>LitePT 模型变体：</strong> 论文提出了 LitePT 的几个变体（LitePT-S, LitePT-B, LitePT-L），以展示其在不同规模下的性能，并特别强调了 LitePT-S 作为主要实验变体，在保持轻量级的同时实现了卓越的性能。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>效率提升：</strong> LitePT 模型在参数量、运行速度和内存占用方面均取得了显著的提升。与最先进的 Point Transformer V3 (PTv3) 相比，LitePT-S 拥有 <strong>3.6倍更少的参数</strong>，运行速度 <strong>快2倍</strong>，内存占用 <strong>少2倍</strong>。即使是更大的 LitePT-L 模型，参数量是 PTv3 的两倍，但仍然比 PTv3 更快且内存占用更低。
*   <strong>性能匹配甚至超越：</strong> 尽管 LitePT 在效率上有了显著提升，但其在各种3D任务（包括语义分割、实例分割和物体检测）和数据集上，性能 <strong>匹配甚至超越</strong> 了最先进的模型，如 PTv3。例如，在 ScanNet 数据集上的实例分割任务中，LitePT-S<em> 取得了新的 SOTA 性能。
*   </em><em>设计原则的验证：</em><em> 通过消融实验，论文验证了其核心假设：卷积在早期阶段是高效且必要的，而注意力在后期阶段更为关键。移除早期阶段的注意力或后期阶段的卷积，对性能的影响与预期一致。
*   </em><em>PointROPE 的有效性：</em>* 消融实验表明，PointROPE 的引入对性能至关重要，移除它会导致性能显著下降。</p>
<p><strong>4. 提及的局限性：</strong>
*   <strong>解码器设计：</strong> 论文提到，对于不同的任务（如语义分割和实例分割），最佳的解码器设计可能有所不同。虽然 LitePT-S 的简化解码器在语义分割上表现良好，但实例分割可能需要更复杂的解码器（如 LitePT-S<em>）。
*   </em><em>注意力机制的局部性：</em>* 虽然论文通过将注意力限制在后期阶段来提高效率，但其注意力机制仍然是局部的（通过分组实现）。论文在结论中提到，在后期阶段，由于 token 数量少，可以考虑计算全局自注意力，这可能会进一步增强长距离上下文建模。</p>
<p><strong>5. 未来研究方向：</strong>
*   <strong>全局自注意力：</strong> 在后期阶段，由于 token 数量减少，可以探索使用全局自注意力机制，以进一步增强长距离上下文建模能力，并可能进一步降低推理时间。
*   <strong>任务相关的解码器优化：</strong> 根据具体任务（如语义分割、实例分割）的需求，进一步优化解码器的设计，以达到最佳的性能和效率平衡。
*   <strong>更广泛的应用探索：</strong> 将 LitePT 骨干网络应用于更广泛的3D点云处理任务和更复杂、更大规模的数据集，以验证其通用性和鲁棒性。</p>
<p><strong>总结：</strong>
LitePT 论文提出了一种创新的、分层的混合点云处理架构，通过在不同层级智能地选择卷积和注意力模块，并引入高效的 PointROPE 位置编码，显著提升了模型的效率（参数量、速度、内存），同时在多项3D点云任务上达到了最先进的性能。该研究不仅提供了一个轻量级且强大的点云骨干网络，还为未来点云网络的设计提供了重要的理论指导和实践依据。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Guided by this design principle, we propose a new, improved 3D point cloud backbone that employs convolutions in early stages and switches to attention for deeper layers.</li>
<li>To avoid the loss of spatial layout information when discarding redundant convolution layers, we introduce a novel, training-free 3D positional encoding, PointROPE.</li>
<li>The resulting LitePT model has <script type="math/tex">3.6\times</script> fewer parameters, runs <script type="math/tex">2\times</script> faster, and uses <script type="math/tex">2\times</script> less memory than the state-of-the-art Point Transformer V3, but nonetheless matches or even outperforms it on a range of tasks and datasets.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.13689v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.13689v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.13687v1'></a></p>
<h2 id="towards-scalable-pre-training-of-visual-tokenizers-for-generation"><a href="https://arxiv.org/abs/2512.13687v1">Towards Scalable Pre-training of Visual Tokenizers for Generation</a></h2>
<p><strong>Authors:</strong> Jingfeng Yao, Yuda Song, Yucong Zhou, Xinggang Wang</p>
<p><strong>Published:</strong> 2025-12-15</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>The quality of the latent space in visual tokenizers (e.g., VAEs) is crucial for modern generative models. However, the standard reconstruction-based training paradigm produces a latent space that is biased towards low-level information, leading to a foundation flaw: better pixel-level accuracy does not lead to higher-quality generation. This implies that pouring extensive compute into visual tokenizer pre-training translates poorly to improved performance in generation. We identify this as the <code>pre-training scaling problem</code> and suggest a necessary shift: to be effective for generation, a latent space must concisely represent high-level semantics. We present VTP, a unified visual tokenizer pre-training framework, pioneering the joint optimization of image-text contrastive, self-supervised, and reconstruction losses. Our large-scale study reveals two principal findings: (1) understanding is a key driver of generation, and (2) much better scaling properties, where generative performance scales effectively with compute, parameters, and data allocated to the pretraining of the visual tokenizer. After large-scale pre-training, our tokenizer delivers a competitive profile (78.2 zero-shot accuracy and 0.36 rFID on ImageNet) and 4.1 times faster convergence on generation compared to advanced distillation methods. More importantly, it scales effectively: without modifying standard DiT training specs, solely investing more FLOPS in pretraining VTP achieves 65.8\% FID improvement in downstream generation, while conventional autoencoder stagnates very early at 1/10 FLOPS. Our pre-trained models are available at https://github.com/MiniMax-AI/VTP.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Towards Scalable Pre-training of Visual Tokenizers for Generation”的全面摘要，重点关注其研究问题、方法、结果、局限性和未来方向：</p>
<p><strong>论文题目：</strong> Towards Scalable Pre-training of Visual Tokenizers for Generation (面向生成任务的可扩展视觉分词器预训练)</p>
<p><strong>作者：</strong> Jingfeng Yao, Yuda Song, Yucong Zhou, Xinggang Wang</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 主要问题/研究问题：</strong></p>
<p>该论文的核心研究问题是“预训练缩放问题”（pre-training scaling problem）。研究人员发现，当前用于生成模型（如潜在扩散模型 LDM）的视觉分词器（visual tokenizers），例如变分自编码器（VAE），通常采用基于重构（reconstruction-based）的预训练方法。这种方法虽然能提高像素级别的重构精度，但却导致学习到的潜在空间（latent space）偏向于低级信息，而未能有效捕捉高级语义。因此，即使投入更多的计算资源进行预训练，也无法显著提升生成模型的质量，甚至可能适得其反。论文旨在解决如何有效地预训练视觉分词器，使其学习到的潜在空间能够真正促进生成任务的性能，并实现计算、参数和数据规模的有效扩展。</p>
<p><strong>2. 关键创新/方法贡献：</strong></p>
<ul>
<li><strong>提出 VTP 框架：</strong> 作者提出了一个名为 VTP (Visual Tokenizer Pre-training) 的统一视觉分词器预训练框架。</li>
<li><strong>联合优化多重损失：</strong> VTP 的核心创新在于联合优化多种损失函数，包括：<ul>
<li><strong>图像-文本对比学习 (Image-Text Contrastive Learning, CLIP)：</strong> 用于注入全局语义理解。</li>
<li><strong>自监督学习 (Self-Supervised Learning, SSL)：</strong> 例如掩码图像建模 (MIM) 和自蒸馏 (self-distillation)，以增强模型的空间-语义感知能力。</li>
<li><strong>重构损失 (Reconstruction Loss)：</strong> 保持对像素级细节的捕捉。</li>
</ul>
</li>
<li><strong>ViT 架构的应用：</strong> 框架基于 Vision Transformer (ViT) 架构，利用其在表征学习方面的灵活性。</li>
<li><strong>解决 GAN 损失在 ViT 上的不稳定性：</strong> 针对 GAN 损失在 ViT 架构上可能导致训练不稳定的问题，作者提出了两阶段训练策略，并在预训练阶段使用 L1 损失和感知损失的组合。</li>
<li><strong>分析缩放属性：</strong> 论文深入分析了不同预训练策略（仅重构 vs. 包含理解任务）在计算量、模型大小和数据规模上的缩放属性，并展示了 VTP 在这些维度上的优越性。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong></p>
<ul>
<li><strong>理解是生成质量的关键驱动力：</strong> 实验证明，引入语义理解和感知任务（如 CLIP 和 SSL）能够显著提升生成能力。与仅基于重构训练的基线模型相比，VTP 在理解和生成方面都取得了更好的性能。</li>
<li><strong>VTP 具有出色的缩放属性：</strong> VTP 是第一个展示出生成性能与计算量、模型参数和数据规模有效扩展的视觉分词器。当计算预算增加 10 倍时，VTP 实现了 65.8% 的 FID 提升，而传统的仅重构的自编码器在早期就停滞不前。</li>
<li><strong>性能优越：</strong> 经过大规模预训练的 VTP 分词器在 ImageNet 上取得了具有竞争力的性能（78.2% 零样本准确率和 0.36 rFID）。与先进的蒸馏方法相比，VTP 在生成任务上收敛速度快 4.1 倍。</li>
<li><strong>更快的收敛速度：</strong> VTP 在下游生成任务上表现出更快的收敛速度，表明其预训练的潜在空间为生成模型提供了更好的起点。</li>
<li><strong>模型可用性：</strong> 作者公开了预训练模型，方便社区使用。</li>
</ul>
<p><strong>4. 提及的局限性：</strong></p>
<ul>
<li><strong>GAN 损失的挑战：</strong> 尽管作者提出了两阶段训练策略，但 GAN 损失在 ViT 架构上的应用仍然存在一定的挑战，可能影响训练的稳定性和效率。</li>
<li><strong>对特定数据集的依赖：</strong> 论文中使用了 DataComp-1B 和 ImageNet 等数据集，其性能可能在其他数据集上有所不同。</li>
<li><strong>对基础模型蒸馏方法的局限性分析：</strong> 论文指出，基于蒸馏的方法未能充分利用理解模型的能力，但并未深入探讨蒸馏方法本身在理论上的根本性限制。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>探索更多感知任务：</strong> 论文提出了一个开放性问题：除了 CLIP 和 SSL，还有哪些其他感知任务可以集成到预训练框架中，以进一步提升生成质量和缩放性？</li>
<li><strong>数据分布的影响：</strong> 论文强调了数据规模的重要性，并暗示数据分布也可能是一个关键因素。未来的研究可以探索不同数据分布（例如，包含特定属性的数据）对分词器性能的影响，以及如何利用数据分布来解锁特定的生成能力。</li>
<li><strong>更高效的 GAN 训练：</strong> 进一步研究如何更稳定、高效地将 GAN 损失应用于 ViT 架构，以优化像素级细节的生成。</li>
<li><strong>更广泛的生成模型应用：</strong> 探索 VTP 在其他类型的生成模型（如 GANs、自回归模型）中的应用潜力。</li>
</ul>
<p><strong>总结：</strong></p>
<p>这篇论文成功地识别并解决了视觉分词器预训练中的“预训练缩放问题”。通过提出 VTP 框架，作者证明了将图像-文本对比学习和自监督学习等理解任务与传统的重构任务联合优化，能够学习到更具语义信息的高质量潜在空间。这一方法不仅显著提升了生成模型的性能，而且实现了计算、参数和数据规模的有效扩展，为构建更强大、更具可扩展性的生成模型提供了新的方向。论文的实验结果和分析具有重要的理论和实践意义，为该领域的研究开辟了新的道路。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present VTP, a unified visual tokenizer pre-training framework, pioneering the joint optimization of image-text contrastive, self-supervised, and reconstruction losses.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.13687v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.13687v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.13684v1'></a></p>
<h2 id="recurrent-video-masked-autoencoders"><a href="https://arxiv.org/abs/2512.13684v1">Recurrent Video Masked Autoencoders</a></h2>
<p><strong>Authors:</strong> Daniel Zoran, Nikhil Parthasarathy, Yi Yang, Drew A Hudson, Joao Carreira, Andrew Zisserman</p>
<p><strong>Published:</strong> 2025-12-15</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We present Recurrent Video Masked-Autoencoders (RVM): a novel video representation learning approach that uses a transformer-based recurrent neural network to aggregate dense image features over time, effectively capturing the spatio-temporal structure of natural video data. RVM learns via an asymmetric masked prediction task requiring only a standard pixel reconstruction objective. This design yields a highly efficient ``generalist'' encoder: RVM achieves competitive performance with state-of-the-art video models (e.g. VideoMAE, V-JEPA) on video-level tasks like action recognition and point/object tracking, while also performing favorably against image models (e.g. DINOv2) on tasks that test geometric and dense spatial understanding. Notably, RVM achieves strong performance in the small-model regime without requiring knowledge distillation, exhibiting up to 30x greater parameter efficiency than competing video masked autoencoders. Moreover, we demonstrate that RVM's recurrent nature allows for stable feature propagation over long temporal horizons with linear computational cost, overcoming some of the limitations of standard spatio-temporal attention-based architectures. Finally, we use qualitative visualizations to highlight that RVM learns rich representations of scene semantics, structure, and motion.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Recurrent Video Masked Autoencoders (RVM)”的全面中文摘要：</p>
<p><strong>论文题目：</strong> Recurrent Video Masked Autoencoders (RVM)</p>
<p><strong>作者：</strong> Daniel Zoran, Nikhil Parthasarathy, Yi Yang, Drew A Hudson, João Carreira, Andrew Zisserman</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
该论文旨在解决如何有效地学习视频的表征，以捕捉其固有的时空结构和动态。现有的视频表征学习方法要么侧重于空间特征（如图像模型），要么在处理长时序依赖性方面存在局限性，或者需要复杂的训练策略（如知识蒸馏）。论文的核心问题是如何构建一个既能理解时空动态，又能保持长时序一致性，并且在参数效率和模型规模方面具有优势的通用视频表征模型。</p>
<p><strong>2. 主要创新点/方法贡献：</strong>
*   <strong>Recurrent Video Masked Autoencoders (RVM) 架构：</strong> 论文提出了一种新颖的视频表征学习方法，RVM。其核心创新在于结合了<strong>Transformer</strong>和<strong>循环神经网络 (RNN)</strong>。具体来说，RVM使用Transformer对视频的每一帧进行编码，然后利用一个基于Transformer的RNN核心来聚合这些帧级别的特征，从而有效地捕捉时空结构。
*   <strong>非对称掩码预测任务：</strong> RVM采用一种非对称的掩码预测任务进行训练，仅需标准的像素重构目标。这种设计使得模型能够高效地学习，并且不需要复杂的辅助任务。
*   <strong>混合RNN核心：</strong> 论文设计了一个结合了<strong>Transformer</strong>和<strong>门控循环单元 (GRU)</strong>的混合RNN核心。这个核心能够整合来自过去时间步的状态信息和当前帧的输入，从而实现信息的增量式学习、遗忘和精炼，并保持长时序的特征稳定性。
*   <strong>参数效率和通用性：</strong> RVM在小模型规模下表现出色，且无需知识蒸馏，展现出高达30倍的参数效率优势。它同时在视频任务（如动作识别、目标跟踪）和图像任务（如几何和密集空间理解）上均取得了优异的性能，成为一个“通才”编码器。
*   <strong>长时序特征稳定性：</strong> RVM的循环设计使其能够以线性的计算成本和内存消耗，在长时序范围内稳定地传播特征，克服了标准时空注意力架构的局限性。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>帕累托前沿表现：</strong> RVM在广泛的视频和图像任务上设定了新的帕累托前沿，其性能超越了其他强大的视频和图像编码器。
*   <strong>小模型下的强大性能：</strong> RVM在小模型规模下无需知识蒸馏，就能取得与大型模型相媲美的性能，这对于资源受限的应用场景具有重要意义。
*   <strong>跨任务的通用性：</strong> RVM在空间任务和时空任务上都取得了优异的平均性能，证明了其作为通用视觉表征模型的潜力。
*   <strong>长时序一致性：</strong> 定性评估表明，RVM能够生成具有出色时空一致性的特征，即使在处理长序列和非刚性运动时也能保持物体身份的稳定性，这对于需要理解视频动态的任务至关重要。
*   <strong>可视化证据：</strong> 通过PCA和K-means聚类可视化，论文展示了RVM学习到的特征能够捕捉到场景的语义、结构和运动信息，并且比其他模型更稳定和一致。</p>
<p><strong>4. 论文中提到的局限性：</strong>
*   <strong>计算效率（短序列）：</strong> 对于非常短的序列，RVM的计算量可能比其他方法（如VideoMAE，它通过时空块来减少token数量）更大，因为RVM需要逐帧处理。
*   <strong>训练内存消耗：</strong> 训练过程需要对ViT编码器进行随时间的反向传播（back-propagation through time），这会增加内存消耗。
*   <strong>数据饱和点未知：</strong> 论文提到，即使训练了2B个视频片段，模型性能仍在持续提升，表明尚未找到RVM模型的最佳数据饱和点。</p>
<p><strong>5. 未来研究方向：</strong>
*   <strong>更高效的计算分配：</strong> 探索如何更有效地分配计算资源，以优化RVM在不同序列长度下的性能。
*   <strong>更正式的缩放定律：</strong> 建立RVM模型更正式的缩放定律，以指导未来更大规模模型的训练。
*   <strong>多模态和世界建模：</strong> 将RVM的框架扩展到多模态（如视频+文本）和世界建模任务，例如在机器人控制等领域。</p>
<p>总而言之，RVM通过创新的Transformer-RNN混合架构和非对称掩码预测任务，成功地实现了高效、通用的视频表征学习，并在参数效率、长时序一致性和跨任务性能方面取得了显著突破，为未来的视频理解研究开辟了新的方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present Recurrent Video Masked-Autoencoders (RVM): a novel video representation learning approach that uses a transformer-based recurrent neural network to aggregate dense image features over time, effectively capturing the spatio-temporal structure of natural video data.</li>
<li>This design yields a highly efficient ``generalist'' encoder: RVM achieves competitive performance with state-of-the-art video models (e.g. VideoMAE, V-JEPA) on video-level tasks like action recognition and point/object tracking, while also performing favorably against image models (e.g. DINOv2) on tasks that test geometric and dense spatial understanding.</li>
<li>Moreover, we demonstrate that RVM's recurrent nature allows for stable feature propagation over long temporal horizons with linear computational cost, overcoming some of the limitations of standard spatio-temporal attention-based architectures.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.13684v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.13684v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.13683v1'></a></p>
<h2 id="i-scene-3d-instance-models-are-implicit-generalizable-spatial-learners"><a href="https://arxiv.org/abs/2512.13683v1">I-Scene: 3D Instance Models are Implicit Generalizable Spatial Learners</a></h2>
<p><strong>Authors:</strong> Lu Ling, Yunhao Ge, Yichen Sheng, Aniket Bera</p>
<p><strong>Published:</strong> 2025-12-15</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Generalization remains the central challenge for interactive 3D scene generation. Existing learning-based approaches ground spatial understanding in limited scene dataset, restricting generalization to new layouts. We instead reprogram a pre-trained 3D instance generator to act as a scene level learner, replacing dataset-bounded supervision with model-centric spatial supervision. This reprogramming unlocks the generator transferable spatial knowledge, enabling generalization to unseen layouts and novel object compositions. Remarkably, spatial reasoning still emerges even when the training scenes are randomly composed objects. This demonstrates that the generator's transferable scene prior provides a rich learning signal for inferring proximity, support, and symmetry from purely geometric cues. Replacing widely used canonical space, we instantiate this insight with a view-centric formulation of the scene space, yielding a fully feed-forward, generalizable scene generator that learns spatial relations directly from the instance model. Quantitative and qualitative results show that a 3D instance generator is an implicit spatial learner and reasoner, pointing toward foundation models for interactive 3D scene understanding and generation. Project page: https://luling06.github.io/I-Scene-project/</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“I-Scene: 3D Instance Models are Implicit Generalizable Spatial Learners”的全面中文摘要：</p>
<p><strong>论文摘要：I-Scene: 3D 实例模型是隐式可泛化的空间学习者</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
本文旨在解决当前交互式三维场景生成领域面临的核心挑战：<strong>泛化能力不足</strong>。现有的基于学习的方法通常依赖于有限的、经过精心策划的场景数据集来学习空间理解，这导致模型在面对新的场景布局或新颖的对象组合时，泛化能力受限。</p>
<p><strong>2. 主要创新与方法贡献：</strong>
作者提出了名为 <strong>I-Scene</strong> 的新颖方法，其核心创新在于：</p>
<ul>
<li><strong>将预训练的 3D 实例生成器重编程为场景级空间学习者：</strong> 论文的核心思想是将现有的、擅长生成单个 3D 实例的模型，通过一种新的训练范式，使其能够理解和生成整个场景的布局。这种方法用<strong>模型中心的空间监督</strong>取代了传统的<strong>数据集边界的监督</strong>。</li>
<li><strong>模型中心的空间监督：</strong> 通过重编程，I-Scene 能够解锁实例生成器中<strong>可迁移的空间知识</strong>（如深度、遮挡、尺度和支撑关系），从而实现对新颖布局和对象组合的泛化。</li>
<li><strong>视图中心（View-Centric）场景空间：</strong> 为了克服传统方法中“规范空间”（canonical space）将不同视角压缩到同一表示而丢失空间敏感性的问题，I-Scene 引入了<strong>视图中心场景空间</strong>。这种空间保留了相机视角与场景之间的严格空间关系，使得模型能够更好地学习和推理空间布局。</li>
<li><strong>全前馈、可泛化的场景生成器：</strong> 结合上述方法，I-Scene 构建了一个<strong>全前馈</strong>的场景生成器，能够直接从实例模型中学习空间关系，无需复杂的后处理或检索步骤。</li>
<li><strong>非语义合成场景的有效性：</strong> 论文的一个重要发现是，即使在<strong>随机组合的、非语义的合成场景</strong>上进行训练，I-Scene 也能涌现出强大的空间推理能力，这表明几何线索本身就足以提供丰富的学习信号。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong>
I-Scene 在多个评估指标上取得了优异的性能，尤其是在泛化能力方面：</p>
<ul>
<li><strong>强大的泛化能力：</strong> I-Scene 在训练数据集中（如 3D-FRONT）表现出色，并且在<strong>未见过的数据集（如 BlendSwap 和 Scenethesis）上展现出卓越的泛化能力</strong>，显著优于现有最先进（SOTA）方法。</li>
<li><strong>高质量的几何与布局：</strong> 实验结果表明，I-Scene 生成的场景在<strong>对象几何质量和整体布局准确性</strong>上均有显著提升，能够生成更干净、更连贯的场景，并避免了对象碰撞和浮动等常见问题。</li>
<li><strong>模型是隐式空间学习者：</strong> 研究结果有力地证明了预训练的 3D 实例生成器本身就蕴含了<strong>隐式的空间学习和推理能力</strong>，为构建更强大的三维场景理解和生成基础模型提供了新的方向。</li>
<li><strong>效率：</strong> I-Scene 采用全前馈设计，避免了检索或迭代优化，在保证高质量的同时，也具备了较高的效率。</li>
</ul>
<p><strong>4. 提及的局限性：</strong>
论文中提到，I-Scene 在<strong>极低分辨率输入</strong>和<strong>严重遮挡的单视图场景</strong>下表现相对较差。</p>
<p><strong>5. 未来研究方向：</strong>
基于上述局限性，论文提出了以下未来研究方向：</p>
<ul>
<li><strong>提高模型鲁棒性：</strong> 通过引入重度遮挡增强（heavy occlusion augmentations），并探索可选的多视图条件（optional multi-view conditioning），以提高模型在复杂遮挡场景下的表现。</li>
<li><strong>探索非语义场景的缩放规律：</strong> 进一步研究非语义随机场景的缩放规律，以更好地处理更具挑战性的“in-the-wild”场景布局。</li>
</ul>
<p><strong>总结：</strong>
I-Scene 论文提出了一种创新的方法，通过重编程预训练的 3D 实例生成器，并引入视图中心场景空间和模型中心监督，实现了对三维场景的<strong>高度泛化</strong>。该方法证明了即使是简单的几何线索和非语义数据，也能有效地训练出强大的空间学习者，为未来构建更通用、更灵活的三维场景生成和理解基础模型奠定了重要基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Existing learning-based approaches ground spatial understanding in limited scene dataset, restricting generalization to new layouts.</li>
<li>This reprogramming unlocks the generator transferable spatial knowledge, enabling generalization to unseen layouts and novel object compositions.</li>
<li>Quantitative and qualitative results show that a 3D instance generator is an implicit spatial learner and reasoner, pointing toward foundation models for interactive 3D scene understanding and generation.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.13683v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.13683v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.13680v1'></a></p>
<h2 id="laser-layer-wise-scale-alignment-for-training-free-streaming-4d-reconstruction"><a href="https://arxiv.org/abs/2512.13680v1">LASER: Layer-wise Scale Alignment for Training-Free Streaming 4D Reconstruction</a></h2>
<p><strong>Authors:</strong> Tianye Ding, Yiming Xie, Yiqing Liang, Moitreya Chatterjee, Pedro Miraldo, Huaizu Jiang</p>
<p><strong>Published:</strong> 2025-12-15</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Recent feed-forward reconstruction models like VGGT and <script type="math/tex">π^3</script> achieve impressive reconstruction quality but cannot process streaming videos due to quadratic memory complexity, limiting their practical deployment. While existing streaming methods address this through learned memory mechanisms or causal attention, they require extensive retraining and may not fully leverage the strong geometric priors of state-of-the-art offline models. We propose LASER, a training-free framework that converts an offline reconstruction model into a streaming system by aligning predictions across consecutive temporal windows. We observe that simple similarity transformation (<script type="math/tex">\mathrm{Sim}(3)</script>) alignment fails due to layer depth misalignment: monocular scale ambiguity causes relative depth scales of different scene layers to vary inconsistently between windows. To address this, we introduce layer-wise scale alignment, which segments depth predictions into discrete layers, computes per-layer scale factors, and propagates them across both adjacent windows and timestamps. Extensive experiments show that LASER achieves state-of-the-art performance on camera pose estimation and point map reconstruction %quality with offline models while operating at 14 FPS with 6 GB peak memory on a RTX A6000 GPU, enabling practical deployment for kilometer-scale streaming videos. Project website: <script type="math/tex">\href{https://neu-vi.github.io/LASER/}{\texttt{https://neu-vi.github.io/LASER/}}</script>
</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“LASER: Layer-wise Scale Alignment for Training-Free Streaming 4D Reconstruction”的全面中文摘要：</p>
<p><strong>论文题目：</strong> LASER: Layer-wise Scale Alignment for Training-Free Streaming 4D Reconstruction (LASER：用于无训练流式4D重建的逐层尺度对齐)</p>
<p><strong>作者：</strong> Tianye Ding, Yiming Xie, Yiqing Liang, Moitreya Chatterjee, Pedro Miraldo, Huaizu Jiang</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong></p>
<p>近年来，以VGGT和π³为代表的先进前馈式3D重建模型在离线场景下取得了令人印象深刻的重建质量。然而，这些模型由于其二次方的内存复杂度，无法直接处理视频流，这极大地限制了它们在实际应用中的部署。现有的流式重建方法通常需要大量的重新训练，并且可能无法充分利用现有离线模型所蕴含的强大几何先验知识。因此，论文的核心研究问题是如何在不进行模型重新训练的情况下，将强大的离线3D重建模型转化为高效、准确的流式重建系统。</p>
<p><strong>2. 主要创新点/方法贡献：</strong></p>
<p>作者提出了LASER（Layer-wise Scale Alignment）框架，这是一个<strong>无训练（training-free）</strong>的解决方案，能够将现有的离线3D重建模型转换为流式系统。其核心创新点在于：</p>
<ul>
<li><strong>逐层尺度对齐（Layer-wise Scale Alignment - LSA）：</strong> 作者发现，简单的Sim(3)（相似变换）对齐在流式重建中存在“层深度错位”（layer depth misalignment）的问题。这是由于单目尺度模糊性导致不同场景层（例如前景和背景）的相对深度尺度在连续帧之间不一致。为了解决这个问题，LASER引入了LSA模块。该模块首先将重建的深度图分割成离散的深度层，然后计算每个深度层的尺度因子，并将其在相邻窗口和时间戳之间进行传播和聚合。</li>
<li><strong>无训练框架：</strong> LASER的核心优势在于其“无训练”特性。它通过一个滑动窗口的方法，利用冻结的离线重建模型来处理视频流，并在窗口之间进行几何对齐，而无需对原始模型进行任何重新训练或微调。</li>
<li><strong>滑动窗口与增量式全局地图重建：</strong> LASER采用滑动窗口策略，处理视频流的重叠时间窗口。每个窗口的预测结果（局部子图）被增量式地注册到全局地图中，从而实现连续的4D重建。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong></p>
<ul>
<li><strong>性能卓越：</strong> 实验结果表明，LASER在相机位姿估计和点云地图重建方面均达到了<strong>最先进（state-of-the-art）的性能</strong>。</li>
<li><strong>高效性：</strong> LASER在RTX A6000 GPU上实现了<strong>14 FPS的流式处理速度</strong>，并且<strong>峰值内存占用仅为6 GB</strong>。这使得它能够处理数公里长的视频序列，远超离线模型的内存限制。</li>
<li><strong>保持离线模型质量：</strong> LASER在保持流式处理能力的同时，其重建质量与离线模型（如π³）非常接近，例如在7-Scenes数据集上的平均精度差异仅为0.002m。</li>
<li><strong>通用性：</strong> 该框架可以即插即用地应用于现有的离线重建模型（如VGGT和π³），无需重新训练，大大降低了部署门槛。</li>
<li><strong>实际应用价值：</strong> LASER的无训练、高效和高质量的特性，使其能够实际部署于自动驾驶、机器人和增强现实等需要实时3D感知和重建的领域。</li>
</ul>
<p><strong>4. 论文中提到的局限性：</strong></p>
<ul>
<li><strong>性能受限于骨干网络：</strong> LASER的性能在很大程度上依赖于其作为骨干的离线3D重建模型。如果骨干模型在处理动态或非刚性场景时存在局限性（例如VGGT在处理移动物体时），LASER也会继承这些局限性。</li>
<li><strong>超参数敏感性：</strong> 对于不同的室内外场景，LASER的超参数（如窗口大小、重叠比例、深度层置信度阈值）可能需要进行经验性调整，这降低了其在全新环境下的通用性。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>自适应超参数调整：</strong> 开发一种能够自动调整超参数以适应不同环境（室内/室外、不同场景动态性）的机制，以提高方法的通用性。</li>
<li><strong>提升骨干网络性能：</strong> 随着更强大的离线3D重建模型的发展，LASER能够直接受益并提升其整体性能。未来研究可以探索如何更好地整合先进的离线模型。</li>
<li><strong>处理更复杂的动态场景：</strong> 尽管LASER在一定程度上可以处理动态场景，但对于高度动态或非刚性的场景，其性能仍有待提高。</li>
</ul>
<p>总而言之，LASER通过引入创新的逐层尺度对齐方法，成功地解决了将强大的离线3D重建模型转化为高效、高质量流式系统的关键挑战，为大规模、连续的3D感知开辟了新的途径。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>While existing streaming methods address this through learned memory mechanisms or causal attention, they require extensive retraining and may not fully leverage the strong geometric priors of state-of-the-art offline models.</li>
<li>We propose LASER, a training-free framework that converts an offline reconstruction model into a streaming system by aligning predictions across consecutive temporal windows.</li>
<li>To address this, we introduce layer-wise scale alignment, which segments depth predictions into discrete layers, computes per-layer scale factors, and propagates them across both adjacent windows and timestamps.</li>
<li>Extensive experiments show that LASER achieves state-of-the-art performance on camera pose estimation and point map reconstruction %quality with offline models while operating at 14 FPS with 6 GB peak memory on a RTX A6000 GPU, enabling practical deployment for kilometer-scale streaming videos.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.13680v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.13680v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.13678v1'></a></p>
<h2 id="feedforward-3d-editing-via-text-steerable-image-to-3d"><a href="https://arxiv.org/abs/2512.13678v1">Feedforward 3D Editing via Text-Steerable Image-to-3D</a></h2>
<p><strong>Authors:</strong> Ziqi Ma, Hongqiao Chen, Yisong Yue, Georgia Gkioxari</p>
<p><strong>Published:</strong> 2025-12-15</p>
<p><strong>Categories:</strong> cs.CV, cs.AI</p>
<p><strong>Abstract:</strong></p>
<p>Recent progress in image-to-3D has opened up immense possibilities for design, AR/VR, and robotics. However, to use AI-generated 3D assets in real applications, a critical requirement is the capability to edit them easily. We present a feedforward method, Steer3D, to add text steerability to image-to-3D models, which enables editing of generated 3D assets with language. Our approach is inspired by ControlNet, which we adapt to image-to-3D generation to enable text steering directly in a forward pass. We build a scalable data engine for automatic data generation, and develop a two-stage training recipe based on flow-matching training and Direct Preference Optimization (DPO). Compared to competing methods, Steer3D more faithfully follows the language instruction and maintains better consistency with the original 3D asset, while being 2.4x to 28.5x faster. Steer3D demonstrates that it is possible to add a new modality (text) to steer the generation of pretrained image-to-3D generative models with 100k data. Project website: https://glab-caltech.github.io/steer3d/</p>
<p><strong>Analysis:</strong></p>
<p>作为一名计算机视觉和机器学习领域的专家，我对这篇论文摘要进行了深入分析，并为您提供以下内容：</p>
<p><strong>1. 论文的主要贡献（2-3句话）</strong></p>
<p>该论文提出了一种名为 Steer3D 的前馈方法，为现有的图像到 3D 生成模型引入了文本可控性。通过借鉴 ControlNet 的思想，Steer3D 能够在一次前向传播中直接利用文本指令编辑生成的 3D 模型，从而实现语言驱动的 3D 资产编辑。该方法通过可扩展的数据引擎和两阶段训练策略（流匹配和 DPO）实现，显著提高了编辑的忠实度和一致性，同时大幅提升了效率。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<ul>
<li><strong>前馈文本可控性（Feedforward Text Steerability）：</strong> 这是该论文的核心创新。不同于以往可能需要迭代或多阶段的编辑过程，Steer3D 实现了“一次性”的文本引导编辑。</li>
<li><strong>ControlNet 思想的适配（Adaptation of ControlNet）：</strong> 论文明确指出借鉴了 ControlNet 的思想，将其应用于图像到 3D 生成领域。ControlNet 的成功在于能够将额外的条件信息（如姿态、深度图等）注入到预训练的扩散模型中，而 Steer3D 将这一理念扩展到文本条件。这意味着它能够有效地将文本语义信息“注入”到 3D 生成过程中，指导模型进行修改。</li>
<li><strong>可扩展的数据引擎（Scalable Data Engine）：</strong> 为了支持这种新的可控性，论文开发了一个自动数据生成引擎。这对于训练能够理解和响应文本指令的 3D 模型至关重要，尤其是在需要大量多样化数据的情况下。</li>
<li><strong>两阶段训练策略（Two-Stage Training Recipe）：</strong><ul>
<li><strong>流匹配训练（Flow-Matching Training）：</strong> 这是一种用于训练生成模型的方法，通常能产生高质量的样本。将其应用于 3D 生成，可能有助于模型学习更精细的几何结构和纹理。</li>
<li><strong>直接偏好优化（Direct Preference Optimization - DPO）：</strong> DPO 是一种用于对齐大型语言模型（LLM）输出与人类偏好的技术。将其应用于 3D 编辑，意味着模型能够学习到更符合用户期望的编辑结果，例如更准确地遵循文本指令，并保持与原始 3D 模型的一致性。</li>
</ul>
</li>
</ul>
<p><strong>3. 对该领域的潜在影响</strong></p>
<ul>
<li><strong>降低 3D 内容创作门槛：</strong> 使得非专业用户也能通过简单的文本描述来修改和定制 3D 模型，极大地 democratized 了 3D 内容的创建和编辑过程。</li>
<li><strong>加速 3D 工作流：</strong> 前馈的编辑方式显著提高了效率，对于需要快速迭代和修改的 3D 设计、游戏开发、虚拟现实等领域具有重要意义。</li>
<li><strong>推动多模态 3D 生成：</strong> 证明了将文本这一高级语义模态有效融入 3D 生成和编辑的可能性，为未来更复杂的跨模态 3D 应用奠定了基础。</li>
<li><strong>促进预训练模型的二次开发：</strong> 表明即使是已经训练好的图像到 3D 模型，也可以通过相对较少的数据（100k）和巧妙的训练方法，有效地增加新的可控性，提高了现有模型的价值和灵活性。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用</strong></p>
<ul>
<li><strong>游戏开发：</strong> 快速生成和修改游戏中的 3D 资产，如角色、道具、场景等。</li>
<li><strong>虚拟现实/增强现实 (AR/VR)：</strong> 动态创建和编辑沉浸式环境中的 3D 对象，提升用户交互体验。</li>
<li><strong>机器人学：</strong> 机器人可以通过文本指令来理解和修改其感知到的 3D 环境中的物体，例如“把这个椅子移到桌子旁边”。</li>
<li><strong>产品设计与可视化：</strong> 设计师可以通过文本描述来调整产品模型的外观、材质、形状等。</li>
<li><strong>数字内容创作 (DCC)：</strong> 为艺术家和设计师提供更直观、更高效的 3D 编辑工具。</li>
<li><strong>3D 打印：</strong> 用户可以通过文本描述来定制 3D 打印模型。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性</strong></p>
<ul>
<li><strong>对原始 3D 资产的依赖性：</strong> 虽然论文声称能保持与原始 3D 资产的“更好一致性”，但“编辑”本质上是在现有基础上进行修改。对于完全从零开始生成或需要进行颠覆性修改的任务，其效果可能受限。</li>
<li><strong>文本指令的复杂性：</strong> 摘要提到“更忠实地遵循语言指令”，这暗示了可能存在指令理解的模糊性或复杂性问题。非常复杂、抽象或模棱两可的文本指令可能仍然难以精确实现。</li>
<li><strong>数据引擎的局限性：</strong> 虽然数据引擎是可扩展的，但其生成数据的质量和多样性将直接影响最终模型的性能。如果数据引擎无法覆盖所有可能的编辑场景，模型在某些情况下可能会表现不佳。</li>
<li><strong>“100k 数据”的含义：</strong> 100k 数据量相对而言不算巨大，但具体是指什么类型的数据（例如，文本-3D 模型对，文本-3D 编辑指令对等）以及其质量，将是影响模型泛化能力的关键。</li>
<li><strong>“前馈”的潜在权衡：</strong> 前馈方法通常以牺牲一定的灵活性或精度为代价来换取速度。虽然摘要强调了速度优势，但可能在某些精细的编辑任务上，其精度或控制力不如迭代式方法。</li>
<li><strong>对预训练模型的依赖：</strong> 该方法是“为预训练的图像到 3D 生成模型添加文本可控性”，这意味着其性能上限很大程度上取决于基础图像到 3D 模型的质量。</li>
</ul>
<p>总而言之，Steer3D 是一项令人兴奋的研究，它通过引入前馈的文本可控性，极大地提升了 3D 模型编辑的效率和易用性，有望在多个领域产生深远影响。其对 ControlNet 思想的巧妙应用以及结合 DPO 的训练策略是技术亮点。然而，任何新技术都存在其固有的局限性，未来的研究可能会进一步探索如何克服这些挑战。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We present a feedforward method, Steer3D, to add text steerability to image-to-3D models, which enables editing of generated 3D assets with language.</li>
<li>Our approach is inspired by ControlNet, which we adapt to image-to-3D generation to enable text steering directly in a forward pass.</li>
<li>Steer3D demonstrates that it is possible to add a new modality (text) to steer the generation of pretrained image-to-3D generative models with 100k data.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.13678v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.13678v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.13677v1'></a></p>
<h2 id="jova-unified-multimodal-learning-for-joint-video-audio-generation"><a href="https://arxiv.org/abs/2512.13677v1">JoVA: Unified Multimodal Learning for Joint Video-Audio Generation</a></h2>
<p><strong>Authors:</strong> Xiaohu Huang, Hao Zhou, Qiangpeng Yang, Shilei Wen, Kai Han</p>
<p><strong>Published:</strong> 2025-12-15</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>In this paper, we present JoVA, a unified framework for joint video-audio generation. Despite recent encouraging advances, existing methods face two critical limitations. First, most existing approaches can only generate ambient sounds and lack the capability to produce human speech synchronized with lip movements. Second, recent attempts at unified human video-audio generation typically rely on explicit fusion or modality-specific alignment modules, which introduce additional architecture design and weaken the model simplicity of the original transformers. To address these issues, JoVA employs joint self-attention across video and audio tokens within each transformer layer, enabling direct and efficient cross-modal interaction without the need for additional alignment modules. Furthermore, to enable high-quality lip-speech synchronization, we introduce a simple yet effective mouth-area loss based on facial keypoint detection, which enhances supervision on the critical mouth region during training without compromising architectural simplicity. Extensive experiments on benchmarks demonstrate that JoVA outperforms or is competitive with both unified and audio-driven state-of-the-art methods in lip-sync accuracy, speech quality, and overall video-audio generation fidelity. Our results establish JoVA as an elegant framework for high-quality multimodal generation.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“JoVA: Unified Multimodal Learning for Joint Video-Audio Generation”的全面中文摘要：</p>
<p><strong>论文题目：</strong> JoVA: Unified Multimodal Learning for Joint Video-Audio Generation (JoVA：用于联合视频-音频生成统一的多模态学习)</p>
<p><strong>作者：</strong> Xiaohu Huang, Hao Zhou, Qiangpeng Yang, Shilei Wen, Kai Han</p>
<p><strong>1. 研究问题/核心挑战：</strong></p>
<p>该论文旨在解决当前视频-音频联合生成领域面临的两个关键挑战：
*   <strong>语音同步能力不足：</strong> 现有方法大多只能生成环境音，难以生成与唇部运动精确同步的人类语音，这极大地限制了其在以人为中心的场景中的应用。
*   <strong>模型复杂性与可扩展性：</strong> 现有的统一视频-音频生成方法通常依赖于显式的融合或特定模态的对齐模块，这增加了模型的设计复杂性，并削弱了Transformer模型的简洁性，同时也限制了其向更多模态扩展的能力。</p>
<p><strong>2. 关键创新/方法贡献：</strong></p>
<p>为了应对上述挑战，研究者提出了JoVA（Joint Video-Audio Generation）框架，其核心创新点包括：</p>
<ul>
<li><strong>统一的联合自注意力机制：</strong> JoVA采用了一种新颖的统一架构，在Transformer的每一层中，视频、音频和文本的token通过联合自注意力（Joint Self-Attention）进行交互。这种设计实现了视频和音频token之间直接、高效的跨模态信息交换，无需额外的对齐或融合模块，极大地简化了模型架构，并提高了可扩展性。</li>
<li><strong>基于关键点检测的口部区域损失（Mouth-Area Loss）：</strong> 为了实现高质量的唇语同步，论文引入了一种简单而有效的口部区域损失策略。该策略利用面部关键点检测来定位视频中的口部区域，并在训练过程中增加对该关键区域的损失权重，从而引导模型更专注于学习唇部运动与语音之间的精确对齐，而不会增加模型架构的复杂性。</li>
<li><strong>时间对齐的ROPE（Temporal-Aligned ROPE）：</strong> 为了进一步增强视频和音频在时间维度上的同步性，论文采用了时间对齐的ROPE（Rotary Position Embedding），确保了两种模态在时间上的位置编码是同步的。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong></p>
<ul>
<li><strong>性能优越：</strong> 在UniAvatar-Bench和Universe-Bench等基准测试中，JoVA在唇语同步准确性（LSE-C）、语音质量（WER）和整体视频-音频生成保真度方面均取得了最先进或具有竞争力的结果。</li>
<li><strong>简化架构：</strong> JoVA的统一架构显著降低了模型复杂性，提高了训练效率和可扩展性，为未来的多模态生成研究奠定了基础。</li>
<li><strong>高效性：</strong> 即使在较小的模型规模（如3.2B参数）和有限的训练数据下，JoVA也能展现出强大的性能，证明了其架构的高效性和潜力。</li>
<li><strong>口部区域损失的重要性：</strong> 实验证明，口部区域损失策略对于实现精确的唇语同步至关重要，显著提升了LSE-C指标。</li>
</ul>
<p><strong>4. 提及的局限性：</strong></p>
<ul>
<li><strong>身份一致性（ID Consistency）：</strong> 在UniAvatar-Bench测试中，JoVA的身份一致性得分（0.78）略低于某些仅关注视频生成的方法，但论文认为这是在实现高质量联合多模态生成（尤其是从文本提示生成）这一更具挑战性任务时的一个合理权衡。</li>
<li><strong>FD指标的微小下降：</strong> 使用时间对齐的ROPE虽然提升了唇语同步，但导致FD（Fréchet Distance）指标有轻微下降，但论文认为为了实现精确的唇语同步，这是可以接受的权衡。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>扩展到更多模态：</strong> JoVA的简洁统一架构为未来扩展到更多模态（如文本、姿态等）提供了良好的基础，以实现更复杂的多模态生成任务。</li>
<li><strong>通用多模态内容创作：</strong> 该研究为实现“通用多模态内容创作”这一宏伟目标迈出了重要一步，未来可以进一步探索更广泛的应用场景。</li>
<li><strong>更精细的控制：</strong> 未来研究可以探索如何为生成内容提供更精细的控制，例如通过更细粒度的文本提示或交互式编辑。</li>
</ul>
<p><strong>总结：</strong></p>
<p>JoVA论文提出了一种创新的统一视频-音频生成框架，通过联合自注意力机制和口部区域损失策略，有效解决了现有方法在语音同步和模型复杂度方面的痛点。该框架不仅在多项关键指标上取得了最先进的性能，而且其简洁高效的设计为未来的多模态生成研究开辟了新的道路，预示着更强大的通用内容创作能力的实现。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>In this paper, we present JoVA, a unified framework for joint video-audio generation.</li>
<li>Furthermore, to enable high-quality lip-speech synchronization, we introduce a simple yet effective mouth-area loss based on facial keypoint detection, which enhances supervision on the critical mouth region during training without compromising architectural simplicity.</li>
<li>Extensive experiments on benchmarks demonstrate that JoVA outperforms or is competitive with both unified and audio-driven state-of-the-art methods in lip-sync accuracy, speech quality, and overall video-audio generation fidelity.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.13677v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.13677v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.13674v1'></a></p>
<h2 id="towards-interactive-intelligence-for-digital-humans"><a href="https://arxiv.org/abs/2512.13674v1">Towards Interactive Intelligence for Digital Humans</a></h2>
<p><strong>Authors:</strong> Yiyi Cai, Xuangeng Chu, Xiwei Gao, Sitong Gong, Yifei Huang, Caixin Kang, Kunhang Li, Haiyang Liu, Ruicong Liu, Yun Liu, Dianwen Ng, Zixiong Su, Erwin Wu, Yuhan Wu, Dingkun Yan, Tianyu Yan, Chang Zeng, Bo Zheng, You Zhou</p>
<p><strong>Published:</strong> 2025-12-15</p>
<p><strong>Categories:</strong> cs.CV, cs.CL, cs.GR, cs.HC</p>
<p><strong>Abstract:</strong></p>
<p>We introduce Interactive Intelligence, a novel paradigm of digital human that is capable of personality-aligned expression, adaptive interaction, and self-evolution. To realize this, we present Mio (Multimodal Interactive Omni-Avatar), an end-to-end framework composed of five specialized modules: Thinker, Talker, Face Animator, Body Animator, and Renderer. This unified architecture integrates cognitive reasoning with real-time multimodal embodiment to enable fluid, consistent interaction. Furthermore, we establish a new benchmark to rigorously evaluate the capabilities of interactive intelligence. Extensive experiments demonstrate that our framework achieves superior performance compared to state-of-the-art methods across all evaluated dimensions. Together, these contributions move digital humans beyond superficial imitation toward intelligent interaction.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是关于论文“Towards Interactive Intelligence for Digital Humans”的全面中文摘要：</p>
<p><strong>论文题目：</strong> Towards Interactive Intelligence for Digital Humans (迈向数字人类的交互式智能)</p>
<p><strong>作者：</strong> Yiyi Cai, Xuangeng Chu, Xiwei Gao, Sitong Gong, Yifei Huang, Caixin Kang, Kunhang Li, Haiyang Liu, Ruicong Liu, Yun Liu, Dianwen Ng, Zixiong Su, Erwin Wu, Yuhan Wu, Dingkun Yan, Tianyu Yan, Chang Zeng, Bo Zheng, You Zhou</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 主要问题/研究问题：</strong>
当前数字人类（digital humans）主要停留在模仿阶段，缺乏真正的交互逻辑和理解能力。尽管视觉保真度已大幅提升，但数字人类在响应用户输入、保持个性一致性、适应性交互以及自我演进方面存在显著差距。论文旨在解决如何让数字人类从被动的视觉呈现转变为能够进行有意义、有逻辑、有情感的交互式智能体。</p>
<p><strong>2. 关键创新/方法论贡献：</strong>
论文提出了“交互式智能”（Interactive Intelligence）这一新范式，并为此构建了一个名为 <strong>Mio (Multimodal Interactive Omni-Avatar)</strong> 的端到端框架。Mio 由五个核心模块组成，协同工作以实现高度智能化的数字人类：</p>
<ul>
<li><strong>Thinker (思考者):</strong> 作为认知核心，负责高层级的认知处理、多模态输入理解、对话内容生成、情感状态管理以及叙事因果关系的维护。它利用分层记忆系统（短期上下文缓冲区和长期叙事知识图谱）来确保叙事连贯性和个性保真度。</li>
<li><strong>Talker (说话者):</strong> 负责将 Thinker 的文本输出转化为自然、高保真度的语音。其核心是 Kodama Audio Tokenizer，一种高效的离散语音表示方法，实现了语义和声学信息的解耦，支持实时、富有表现力的对话。</li>
<li><strong>Face Animator (面部动画师):</strong> 负责生成逼真、实时的面部表情和动作，包括说话和倾听时的面部动态。其 UniLS 方法采用两阶段训练，分别学习内部运动先验和音频驱动的动态，解决了“僵尸脸”问题。</li>
<li><strong>Body Animator (身体动画师):</strong> 负责生成物理上可信、流畅的全身体运动。其 FloodDiffusion 框架基于扩散模型，专为流式运动合成设计，实现了低延迟、可编辑的实时身体动画。</li>
<li><strong>Renderer (渲染器):</strong> 负责将面部和身体动画参数转化为高保真度、身份一致的人类视频帧。AvatarDiT 框架利用扩散 Transformer，通过参数化控制（FLAME 和 SMPL 参数）实现精确的、多视角的、身份稳定的渲染。</li>
</ul>
<p>此外，论文还提出了一个名为 <strong>Interactive Intelligence Score (IIS)</strong> 的综合基准，用于全面评估数字人类在认知、听觉、面部、身体和视觉等多个维度上的表现。</p>
<p><strong>3. 主要结果与意义：</strong>
*   <strong>性能优越性：</strong> Mio 框架在各个模块的评估中均展现出优于现有最先进方法的性能。例如，Talker 在语音质量和可懂度上表现出色；Face Animator 在倾听自然度上获得用户高度认可；Body Animator 在运动质量和流式处理上达到 SOTA 水平；Thinker 在个性保真度上超越了 GPT-40；Renderer 在多视图一致性和身份保持方面表现突出。
*   <strong>交互式智能的实现：</strong> Mio 成功地将认知推理与实时多模态具身化能力相结合，实现了流畅、一致的交互。它能够根据用户输入自适应地调整行为，并具备一定程度的自我演进能力。
*   <strong>新基准的建立：</strong> IIS 基准的提出为交互式智能数字人类的评估提供了一个标准化的框架，促进了该领域的进一步研究和发展。
*   <strong>意义：</strong> 该研究标志着数字人类从“表面模仿”向“智能交互”的重大转变，为虚拟陪伴、交互式叙事和沉浸式游戏等应用开辟了新的可能性。</p>
<p><strong>4. 局限性：</strong>
论文中提到的一些局限性包括：
*   <strong>Talker 的扬声器相似性：</strong> 在某些数据集上，Kodama-Tokenizer 在扬声器相似性方面略低于一些基线模型，这表明在压缩效率和高保真度重建之间可能存在权衡。
*   <strong>Thinker 的训练数据：</strong> 虽然提出了数据无关的自我训练方法，但模型的初始能力和泛化能力仍依赖于预训练 LLM 的基础。
*   <strong>整体评估：</strong> 尽管 IIS 是一个综合指标，但其计算仍依赖于各个模块的客观度量，可能无法完全捕捉到所有细微的交互体验。</p>
<p><strong>5. 未来研究方向：</strong>
论文的贡献为未来的研究奠定了基础，潜在的研究方向包括：
*   <strong>更精细的情感和个性表达：</strong> 进一步提升 Thinker 模块对复杂情感和细微个性差异的理解与表达能力。
*   <strong>更自然的跨模态交互：</strong> 探索更深层次的跨模态融合，使数字人类能够更自然地理解和响应用户的情感、意图和上下文。
*   <strong>实时交互的鲁棒性：</strong> 在更复杂、更不可预测的用户交互场景下，进一步提升整个系统的鲁棒性和适应性。
*   <strong>自我演进能力的深化：</strong> 探索更有效的自我演进机制，使数字人类能够持续学习和适应，不断提升其交互智能。
*   <strong>更广泛的应用探索：</strong> 将该框架应用于更多实际场景，如教育、医疗、娱乐等，探索交互式数字人类的更多可能性。</p>
<p>总而言之，这篇论文是一项重要的工作，它不仅提出了一个强大的、模块化的数字人类框架 Mio，而且通过“交互式智能”这一新范式和 IIS 基准，为构建更具智能和交互性的数字人类设定了新的方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce Interactive Intelligence, a novel paradigm of digital human that is capable of personality-aligned expression, adaptive interaction, and self-evolution.</li>
<li>To realize this, we present Mio (Multimodal Interactive Omni-Avatar), an end-to-end framework composed of five specialized modules: Thinker, Talker, Face Animator, Body Animator, and Renderer.</li>
<li>Furthermore, we establish a new benchmark to rigorously evaluate the capabilities of interactive intelligence.</li>
<li>Extensive experiments demonstrate that our framework achieves superior performance compared to state-of-the-art methods across all evaluated dimensions.</li>
<li>Together, these contributions move digital humans beyond superficial imitation toward intelligent interaction.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.13674v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.13674v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.13671v1'></a></p>
<h2 id="agentiad-tool-augmented-single-agent-for-industrial-anomaly-detection"><a href="https://arxiv.org/abs/2512.13671v1">AgentIAD: Tool-Augmented Single-Agent for Industrial Anomaly Detection</a></h2>
<p><strong>Authors:</strong> Junwen Miao, Penghui Du, Yi Liu, Yu Wang, Yan Wang</p>
<p><strong>Published:</strong> 2025-12-15</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Industrial anomaly detection (IAD) is difficult due to the scarcity of normal reference samples and the subtle, localized nature of many defects. Single-pass vision-language models (VLMs) often overlook small abnormalities and lack explicit mechanisms to compare against canonical normal patterns. We propose AgentIAD, a tool-driven agentic framework that enables multi-stage visual inspection. The agent is equipped with a Perceptive Zoomer (PZ) for localized fine-grained analysis and a Comparative Retriever (CR) for querying normal exemplars when evidence is ambiguous. To teach these inspection behaviors, we construct structured perceptive and comparative trajectories from the MMAD dataset and train the model in two stages: supervised fine-tuning followed by reinforcement learning. A two-part reward design drives this process: a perception reward that supervises classification accuracy, spatial alignment, and type correctness, and a behavior reward that encourages efficient tool use. Together, these components enable the model to refine its judgment through step-wise observation, zooming, and verification. AgentIAD achieves a new state-of-the-art 97.62% classification accuracy on MMAD, surpassing prior MLLM-based approaches while producing transparent and interpretable inspection traces.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“AgentIAD: Tool-Augmented Single-Agent for Industrial Anomaly Detection”的全面摘要，重点关注其在计算机视觉领域的新颖性和重要性：</p>
<p><strong>论文摘要：AgentIAD - 一种工具增强的单智能体工业异常检测框架</strong></p>
<p><strong>1. 研究问题与挑战</strong></p>
<p>工业异常检测（IAD）面临两大核心挑战：一是正常参考样本的稀缺性，二是缺陷的细微、局部化特性。现有的单通道视觉语言模型（VLMs）往往难以捕捉微小的异常，并且缺乏与标准正常模式进行比较的机制。这导致它们在识别精细缺陷和理解上下文信息方面能力不足。</p>
<p><strong>2. 核心创新与方法贡献</strong></p>
<p>为了解决上述问题，论文提出了 <strong>AgentIAD</strong>，一个创新的<strong>工具驱动的单智能体框架</strong>，用于实现多阶段的视觉检查。AgentIAD 的关键创新点包括：</p>
<ul>
<li><strong>智能体与工具协同：</strong> AgentIAD 引入了两个核心工具：<ul>
<li><strong>感知缩放器 (Perceptive Zoomer, PZ)：</strong> 用于对局部区域进行精细化分析，捕捉细微的视觉线索。</li>
<li><strong>比较检索器 (Comparative Retriever, CR)：</strong> 当证据模糊时，用于查询正常样本以进行跨实例验证。</li>
</ul>
</li>
<li><strong>多阶段推理：</strong> AgentIAD 实现了类似人类专家的逐步推理过程：观察、缩放、比较和验证。这种交互式推理能力使其能够更准确地处理复杂和细微的异常。</li>
<li><strong>两阶段训练：</strong><ul>
<li><strong>感知监督微调 (Perceptive Supervised Fine-Tuning, SFT)：</strong> 通过结构化的多模态轨迹（由 GPT-4o 生成）训练模型，使其能够将语言推理与视觉工具使用对齐。</li>
<li><strong>智能体强化学习 (Agentic Reinforcement Learning, RL)：</strong> 通过一个两级奖励机制（感知奖励和行为奖励）进一步优化模型的决策策略，以实现长时序的决策能力。感知奖励关注准确性、空间对齐和类型正确性，而行为奖励则鼓励高效的工具使用。</li>
</ul>
</li>
<li><strong>结构化推理轨迹：</strong> 论文构建了“感知轨迹”（仅使用 PZ）和“比较轨迹”（使用 PZ 和 CR），这些轨迹显式地耦合了视觉动作、推理步骤和决策结果，为模型训练奠定了基础。</li>
</ul>
<p><strong>3. 主要结果与意义</strong></p>
<ul>
<li><strong>性能突破：</strong> AgentIAD 在 MMAD 基准测试上取得了 <strong>97.62% 的分类准确率</strong>，创下了新的<strong>最先进水平 (state-of-the-art)</strong>，显著超越了之前基于 MLLM 的方法。</li>
<li><strong>可解释性：</strong> AgentIAD 能够生成透明且可解释的检查轨迹，清晰地展示了模型的推理过程和决策依据，这对于工业应用至关重要。</li>
<li><strong>轻量级模型能力提升：</strong> 论文证明了即使使用相对紧凑的 3B 模型，通过智能体驱动的检查行为，也能实现比更大模型更优越的性能，强调了<strong>推理策略和奖励设计的重要性远超模型规模</strong>。</li>
<li><strong>通用性：</strong> 该框架为工业异常检测提供了一种可泛化且可解释的多模态推理范式，弥合了大型视觉语言模型与真实世界视觉认知之间的差距。</li>
</ul>
<p><strong>4. 局限性</strong></p>
<ul>
<li><strong>模型基础：</strong> AgentIAD 目前基于 Qwen2.5-VL-3B 模型，而非最新的 MLLM 模型。</li>
<li><strong>工具集：</strong> 论文中使用的工具集相对有限，未来可以通过集成更多高级视觉语言架构和扩展工具集来进一步提升性能。</li>
</ul>
<p><strong>5. 未来研究方向</strong></p>
<ul>
<li><strong>集成更先进的 MLLM：</strong> 将 AgentIAD 框架集成到最新的、更强大的 MLLM 模型中，以进一步提升其基础感知和推理能力。</li>
<li><strong>扩展工具集：</strong> 探索和集成更多样化的视觉工具，以应对更广泛的工业检测场景和更复杂的异常类型。</li>
<li><strong>跨领域适应性：</strong> 研究如何将 AgentIAD 的框架和方法推广到其他需要精细化、多阶段推理的视觉任务中，例如医学影像分析或材料科学。</li>
<li><strong>提升泛化性、鲁棒性和跨领域适应性：</strong> 通过上述改进，进一步提升 AgentIAD 在不同工业场景和复杂条件下的表现。</li>
</ul>
<p>总而言之，AgentIAD 通过引入工具增强的单智能体框架和创新的多阶段推理机制，显著提升了工业异常检测的性能和可解释性，为构建更智能、更可靠的自动化检测系统提供了新的思路和方法。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose AgentIAD, a tool-driven agentic framework that enables multi-stage visual inspection.</li>
<li>AgentIAD achieves a new state-of-the-art 97.62% classification accuracy on MMAD, surpassing prior MLLM-based approaches while producing transparent and interpretable inspection traces.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.13671v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.13671v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-12-16 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
