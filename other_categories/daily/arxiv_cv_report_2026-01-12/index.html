<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2026-01-12 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-09
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2026-01-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-02-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-02-04
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2026-01-09/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2026-01-13/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2026-01-12">Arxiv Computer Vision Papers - 2026-01-12</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#context-aware-decoding-for-faithful-vision-language-generation" class="nav-link">Context-Aware Decoding for Faithful Vision-Language Generation</a>
                </li>
                <li class="nav-item">
                    <a href="#adapting-vision-transformers-to-ultra-high-resolution-semantic-segmentation-with-relay-tokens" class="nav-link">Adapting Vision Transformers to Ultra-High Resolution Semantic Segmentation with Relay Tokens</a>
                </li>
                <li class="nav-item">
                    <a href="#layergs-decomposition-and-inpainting-of-layered-3d-human-avatars-via-2d-gaussian-splatting" class="nav-link">LayerGS: Decomposition and Inpainting of Layered 3D Human Avatars via 2D Gaussian Splatting</a>
                </li>
                <li class="nav-item">
                    <a href="#router-suggest-dynamic-routing-for-multimodal-auto-completion-in-visually-grounded-dialogs" class="nav-link">Router-Suggest: Dynamic Routing for Multimodal Auto-Completion in Visually-Grounded Dialogs</a>
                </li>
                <li class="nav-item">
                    <a href="#goal-force-teaching-video-models-to-accomplish-physics-conditioned-goals" class="nav-link">Goal Force: Teaching Video Models To Accomplish Physics-Conditioned Goals</a>
                </li>
                <li class="nav-item">
                    <a href="#dextercap-an-affordable-and-automated-system-for-capturing-dexterous-hand-object-manipulation" class="nav-link">DexterCap: An Affordable and Automated System for Capturing Dexterous Hand-Object Manipulation</a>
                </li>
                <li class="nav-item">
                    <a href="#boosting-latent-diffusion-models-via-disentangled-representation-alignment" class="nav-link">Boosting Latent Diffusion Models via Disentangled Representation Alignment</a>
                </li>
                <li class="nav-item">
                    <a href="#scenefoundry-generating-interactive-infinite-3d-worlds" class="nav-link">SceneFoundry: Generating Interactive Infinite 3D Worlds</a>
                </li>
                <li class="nav-item">
                    <a href="#flypose-towards-robust-human-pose-estimation-from-aerial-views" class="nav-link">FlyPose: Towards Robust Human Pose Estimation From Aerial Views</a>
                </li>
                <li class="nav-item">
                    <a href="#vitnt-fiqa-training-free-face-image-quality-assessment-with-vision-transformers" class="nav-link">ViTNT-FIQA: Training-Free Face Image Quality Assessment with Vision Transformers</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2026-01-12">Arxiv Computer Vision Papers - 2026-01-12</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>好的，作为一名专注于计算机视觉和机器学习的研究助理，我将为您提供一份关于2026年1月9日 Arxiv 计算机视觉领域论文的简明执行摘要。</p>
<hr />
<p><strong>执行摘要：2026年1月9日 Arxiv 计算机视觉论文精选</strong></p>
<p><strong>主要主题与趋势：</strong></p>
<p>本期 Arxiv 论文集展现了计算机视觉领域在<strong>多模态理解与生成</strong>、<strong>高分辨率与3D场景处理</strong>、以及<strong>特定任务的鲁棒性提升</strong>等方面的显著进展。特别值得注意的是，<strong>Vision Transformer (ViT)</strong> 及其变体在处理复杂视觉任务（如超高分辨率分割、姿态估计）中持续发挥重要作用，而<strong>生成模型（尤其是扩散模型）</strong> 的应用范围也在不断拓展，并朝着更精细的控制和解耦表示方向发展。</p>
<p><strong>亮点与创新：</strong></p>
<ul>
<li><strong>“Adapting Vision Transformers to Ultra-High Resolution Semantic Segmentation with Relay Tokens”</strong> 提出了一种新颖的 Relay Tokens 机制，使得 ViT 能够高效处理超高分辨率图像的语义分割，解决了现有模型在处理大规模图像时的内存和计算瓶颈。</li>
<li><strong>“LayerGS: Decomposition and Inpainting of Layered 3D Human Avatars via 2D Gaussian Splatting”</strong> 结合了 2D 高斯泼溅技术，实现了对分层 3D 人体化身的分解与修复，为 3D 内容创作和编辑提供了新的思路。</li>
<li><strong>“Goal Force: Teaching Video Models To Accomplish Physics-Conditioned Goals”</strong> 引入了一种新颖的训练范式，使视频模型能够理解并完成基于物理约束的目标，标志着视频理解向更具交互性和因果性的方向迈进。</li>
</ul>
<p><strong>新兴研究方向与技术：</strong></p>
<ul>
<li><strong>解耦表示学习：</strong> “Boosting Latent Diffusion Models via Disentangled Representation Alignment” 展示了通过解耦表示来提升扩散模型的生成质量和可控性，预示着未来生成模型将更加注重对潜在空间的精细控制。</li>
<li><strong>动态路由与自适应机制：</strong> “Router-Suggest: Dynamic Routing for Multimodal Auto-Completion in Visually-Grounded Dialogs” 提出了动态路由机制，以适应多模态对话中的不确定性，提升了自动补全的准确性。</li>
<li><strong>高效 3D 世界生成：</strong> “SceneFoundry: Generating Interactive Infinite 3D Worlds” 展示了生成可交互的无限 3D 世界的能力，为虚拟现实和游戏开发提供了新的可能性。</li>
<li><strong>鲁棒性姿态估计：</strong> “FlyPose: Towards Robust Human Pose Estimation From Aerial Views” 专注于解决从航拍视角进行人体姿态估计的挑战，体现了对特定应用场景下模型鲁棒性要求的提升。</li>
</ul>
<p><strong>推荐阅读论文：</strong></p>
<p>为了快速了解本期论文的核心贡献，建议重点阅读以下论文：</p>
<ol>
<li><strong>“Adapting Vision Transformers to Ultra-High Resolution Semantic Segmentation with Relay Tokens”</strong>: 对于关注大规模图像处理和语义分割的研究人员至关重要。</li>
<li><strong>“LayerGS: Decomposition and Inpainting of Layered 3D Human Avatars via 2D Gaussian Splatting”</strong>: 对于 3D 重建、虚拟化身和内容生成领域的研究者具有重要参考价值。</li>
<li><strong>“Goal Force: Teaching Video Models To Accomplish Physics-Conditioned Goals”</strong>: 对于视频理解、强化学习与视觉结合的研究者，提供了新的研究视角和方法。</li>
<li><strong>“Boosting Latent Diffusion Models via Disentangled Representation Alignment”</strong>: 对于正在探索扩散模型改进和可控生成的研究者，提供了重要的技术洞察。</li>
</ol>
<hr />
<p>希望这份摘要能帮助您快速掌握近期 Arxiv 计算机视觉领域的最新动态。</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2601.05939v1">Context-Aware Decoding for Faithful Vision-Language Generation</a></li>
<li><a href="#2601.05927v1">Adapting Vision Transformers to Ultra-High Resolution Semantic Segmentation with Relay Tokens</a></li>
<li><a href="#2601.05853v1">LayerGS: Decomposition and Inpainting of Layered 3D Human Avatars via 2D Gaussian Splatting</a></li>
<li><a href="#2601.05851v1">Router-Suggest: Dynamic Routing for Multimodal Auto-Completion in Visually-Grounded Dialogs</a></li>
<li><a href="#2601.05848v1">Goal Force: Teaching Video Models To Accomplish Physics-Conditioned Goals</a></li>
<li><a href="#2601.05844v1">DexterCap: An Affordable and Automated System for Capturing Dexterous Hand-Object Manipulation</a></li>
<li><a href="#2601.05823v1">Boosting Latent Diffusion Models via Disentangled Representation Alignment</a></li>
<li><a href="#2601.05810v1">SceneFoundry: Generating Interactive Infinite 3D Worlds</a></li>
<li><a href="#2601.05747v1">FlyPose: Towards Robust Human Pose Estimation From Aerial Views</a></li>
<li><a href="#2601.05741v1">ViTNT-FIQA: Training-Free Face Image Quality Assessment with Vision Transformers</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2601.05939v1'></a></p>
<h2 id="context-aware-decoding-for-faithful-vision-language-generation"><a href="https://arxiv.org/abs/2601.05939v1">Context-Aware Decoding for Faithful Vision-Language Generation</a></h2>
<p><strong>Authors:</strong> Mehrdad Fazli, Bowen Wei, Ziwei Zhu</p>
<p><strong>Published:</strong> 2026-01-09</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Hallucinations, generating responses inconsistent with the visual input, remain a critical limitation of large vision-language models (LVLMs), especially in open-ended tasks such as image captioning and visual reasoning. In this work, we probe the layer-wise generation dynamics that drive hallucinations and propose a training-free mitigation strategy. Employing the Logit Lens, we examine how LVLMs construct next-token distributions across decoder layers, uncovering a pronounced commitment-depth gap: truthful tokens accumulate probability mass on their final candidates earlier than hallucinatory ones. Drawing on this discovery, we introduce Context Embedding Injection (CEI), a lightweight method that harnesses the hidden state of the last input token-the context embedding-as a grounding signal to maintain visual fidelity throughout decoding and curb hallucinations. Evaluated on the CHAIR, AMBER, and MMHal-Bench benchmarks (with a maximum token length of 512), CEI outperforms state-of-the-art baselines across three LVLMs, with its dynamic variant yielding the lowest overall hallucination rates. By integrating novel mechanistic insights with a scalable intervention, this work advances the mitigation of hallucinations in LVLMs.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Context-Aware Decoding for Faithful Vision-Language Generation”的全面中文摘要：</p>
<p><strong>论文题目：</strong> Context-Aware Decoding for Faithful Vision-Language Generation (面向忠实视觉-语言生成的上下文感知解码)</p>
<p><strong>作者：</strong> Mehrdad Fazli, Bowen Wei, Ziwei Zhu</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
大型视觉-语言模型（LVLMs）在开放式任务（如图像描述和视觉推理）中存在一个关键的局限性，即“幻觉”（hallucinations），即模型生成的文本与输入的视觉信息不一致。这严重影响了LVLMs在需要高事实性和忠实性的应用中的可靠性。</p>
<p><strong>2. 主要创新点/方法贡献：</strong>
*   <strong>机制性洞察：</strong> 作者利用“Logit Lens”技术，深入探究了LVLMs在解码过程中逐层生成文本的动态机制。他们发现了一个显著的“承诺-深度差距”（commitment-depth gap）：<strong>真实（truthful）的词汇在更早的解码层就稳定了其最终决策的概率质量，而幻觉（hallucinatory）的词汇则需要更深的层才能达到相同的稳定性。</strong>
*   <strong>上下文嵌入注入（Context Embedding Injection, CEI）：</strong> 基于上述洞察，作者提出了一种名为CEI的轻量级、无需训练的干预方法。CEI的核心思想是利用<strong>初始输入（图像+提示）的最后一个提示词在最终解码层产生的隐藏状态（即“上下文嵌入”）</strong>，将其作为视觉基础信号，在后续的解码过程中持续注入，以维持文本与图像的视觉保真度，从而抑制幻觉。
*   <strong>静态与动态CEI：</strong>
    *   <strong>静态CEI：</strong> 在解码过程中，将预先计算好的上下文嵌入以恒定的权重注入到选定的解码层。
    *   <strong>动态CEI：</strong> 进一步改进了静态CEI，根据每一步生成的词汇的“承诺-深度”信号（通过计算平均Top-K概率质量MK来衡量），动态调整注入的权重。当MK值较低（表明幻觉风险较高）时，增加注入强度，以更有效地引导模型回到视觉基础。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>实验验证：</strong> 作者在CHAIR、AMBER和MMHal-Bench三个广泛使用的基准测试上评估了CEI方法。
*   <strong>性能提升：</strong> CEI在所有三个LVLMs（InstructBLIP, LLaVA-1.5, LLaVA-NeXT）上均优于最先进的基线方法，显著降低了幻觉率。
*   <strong>动态CEI的优势：</strong> 动态CEI变体在所有模型上实现了最低的整体幻觉率，表明其自适应的干预策略在抑制幻觉方面更为有效。
*   <strong>意义：</strong> 这项工作不仅提供了对LVLM幻觉产生机制的深入理解，还提出了一种高效且通用的方法来解决这一关键问题。CEI的训练无关性使其易于集成到现有模型中，为生成更忠实、更可靠的视觉-语言内容提供了新的途径。</p>
<p><strong>4. 论文中提到的局限性：</strong>
*   <strong>计算开销：</strong> 动态CEI在每次解码时需要额外的“探针前向传播”（probe forward pass）来计算MK值，这会增加推理的计算成本，可能不适用于对延迟高度敏感的场景。
*   <strong>泛化性：</strong> 实验主要集中在COCO风格的基准测试上，对于特定领域（如医学影像、自动驾驶）的泛化能力仍需进一步验证。
*   <strong>分析依赖：</strong> 初步的机制性分析依赖于带有真实/幻觉词汇标签的标注数据集，这可能限制了分析的范围。
*   <strong>白盒访问要求：</strong> CEI需要访问模型的内部隐藏状态和词汇映射矩阵，因此不适用于仅通过黑盒API访问的LVLMs。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>运行时优化：</strong> 探索更高效的计算方法，以降低动态CEI的推理开销。
*   <strong>领域适应性研究：</strong> 在特定领域的LVLM任务上评估CEI的有效性。
*   <strong>多模态场景扩展：</strong> 将CEI方法扩展到多图像或视频的理解与生成任务。
*   <strong>与其他方法的结合：</strong> 探索CEI与对比解码等其他幻觉缓解技术相结合的可能性。
*   <strong>更广泛的推理任务：</strong> 研究CEI在其他多模态推理任务中的应用。</p>
<p>总而言之，这篇论文通过深入的机制性分析，揭示了LVLM幻觉产生的关键因素——“承诺-深度差距”，并提出了一种创新的、无需训练的上下文感知解码方法CEI。CEI通过持续注入视觉基础信号，有效抑制了幻觉，并在多个基准测试上取得了显著的性能提升，为构建更可靠的视觉-语言模型提供了重要贡献。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Drawing on this discovery, we introduce Context Embedding Injection (CEI), a lightweight method that harnesses the hidden state of the last input token-the context embedding-as a grounding signal to maintain visual fidelity throughout decoding and curb hallucinations.</li>
<li>Evaluated on the CHAIR, AMBER, and MMHal-Bench benchmarks (with a maximum token length of 512), CEI outperforms state-of-the-art baselines across three LVLMs, with its dynamic variant yielding the lowest overall hallucination rates.</li>
<li>By integrating novel mechanistic insights with a scalable intervention, this work advances the mitigation of hallucinations in LVLMs.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.05939v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.05939v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.05927v1'></a></p>
<h2 id="adapting-vision-transformers-to-ultra-high-resolution-semantic-segmentation-with-relay-tokens"><a href="https://arxiv.org/abs/2601.05927v1">Adapting Vision Transformers to Ultra-High Resolution Semantic Segmentation with Relay Tokens</a></h2>
<p><strong>Authors:</strong> Yohann Perron, Vladyslav Sydorov, Christophe Pottier, Loic Landrieu</p>
<p><strong>Published:</strong> 2026-01-09</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Current approaches for segmenting ultra high resolution images either slide a window, thereby discarding global context, or downsample and lose fine detail. We propose a simple yet effective method that brings explicit multi scale reasoning to vision transformers, simultaneously preserving local details and global awareness. Concretely, we process each image in parallel at a local scale (high resolution, small crops) and a global scale (low resolution, large crops), and aggregate and propagate features between the two branches with a small set of learnable relay tokens. The design plugs directly into standard transformer backbones (eg ViT and Swin) and adds fewer than 2 % parameters. Extensive experiments on three ultra high resolution segmentation benchmarks, Archaeoscape, URUR, and Gleason, and on the conventional Cityscapes dataset show consistent gains, with up to 15 % relative mIoU improvement. Code and pretrained models are available at https://archaeoscape.ai/work/relay-tokens/ .</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Adapting Vision Transformers to Ultra-High Resolution Semantic Segmentation with Relay Tokens”的全面摘要，重点关注其在计算机视觉领域的新颖性和重要性：</p>
<p><strong>论文题目：</strong> Adapting Vision Transformers to Ultra-High Resolution Semantic Segmentation with Relay Tokens</p>
<p><strong>作者：</strong> Yohann Perron, Vladyslav Sydorov, Christophe Pottier, Loic Landrieu</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong></p>
<p>该论文旨在解决超高分辨率（Ultra-High Resolution, UHR）图像语义分割中的核心挑战。UHR图像（如地球观测、医学影像）具有海量像素，这使得传统的分割方法面临两难：
*   <strong>滑动窗口（Sliding Window）：</strong> 能够保留局部细节，但会丢失全局上下文信息，并且可能产生边界伪影。
*   <strong>下采样（Downsampling）：</strong> 能够捕捉全局信息，但会丢失精细的局部细节，导致对小目标或精细结构的分割困难。
现有的Transformer模型（如ViT）由于其二次方计算复杂度，难以直接处理UHR图像。</p>
<p><strong>2. 关键创新/方法贡献：</strong></p>
<p>作者提出了一种名为“<strong>继电器令牌（Relay Tokens）</strong>”的即插即用（plug-and-play）机制，用于增强Vision Transformer（ViT）在UHR图像语义分割中的多尺度推理能力。其核心思想是：</p>
<ul>
<li><strong>并行多尺度处理：</strong> 同时处理图像的两个尺度：一个<strong>局部尺度</strong>（高分辨率、小尺寸裁剪）和一个<strong>全局尺度</strong>（低分辨率、大尺寸裁剪）。</li>
<li><strong>可学习的继电器令牌：</strong> 引入少量（R个）可学习的“继电器令牌”。这些令牌在Transformer的每一层中，充当局部和全局分支之间的信息桥梁。</li>
<li><strong>跨尺度信息聚合与传播：</strong> 在Transformer的自注意力机制中，继电器令牌从一个尺度收集特征，然后将其传递到另一个尺度，从而实现局部细节和全局上下文的有效融合。</li>
<li><strong>轻量级设计：</strong> 该方法对标准的Transformer骨干网络（如ViT和Swin）进行修改，仅增加不到2%的参数（甚至在共享补丁嵌入时仅增加0.0005%），并且对运行时的开销影响极小。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong></p>
<ul>
<li><strong>性能提升显著：</strong> 在三个UHR分割基准（Archaeoscape, URUR, Gleason）以及经典的Cityscapes数据集上，继电器令牌方法均取得了显著的性能提升。在Gleason数据集上，相对mIoU（mean Intersection-over-Union）提升高达15%。</li>
<li><strong>超越现有方法：</strong> 该方法在UHR数据集上能够匹配甚至超越专门的多尺度方法（如ISDNet, GLNet, SGNet）。即使是线性注意力Transformer（如Flatten Swin），在加入继电器令牌后也能获得性能提升。</li>
<li><strong>内存效率高：</strong> 相较于全分辨率处理，该方法能够显著降低GPU内存需求，甚至可以在48GB的GPU上训练出优于需要80GB GPU的模型。</li>
<li><strong>通用性强：</strong> 该方法可以轻松地集成到现有的ViT和Swin等Transformer骨干网络中，无需从头训练，并且能够保留预训练权重。</li>
<li><strong>意义：</strong> 该研究提供了一种简单、高效且通用的方法，使得Vision Transformer能够有效地处理超高分辨率图像，解决了长期存在的局部细节与全局上下文难以兼顾的问题，为UHR图像分析开辟了新的可能性。</li>
</ul>
<p><strong>4. 提及的局限性：</strong></p>
<ul>
<li><strong>计算成本：</strong> 虽然继电器令牌方法比全分辨率处理更高效，但并行处理两个尺度仍然会增加计算量，大约是单尺度滑动窗口的两倍。</li>
<li><strong>数据集依赖性：</strong> 某些数据集（如Gleason）从共享投影层中获益更多，而另一些数据集（如Archaeoscape）则从不同投影层中获益更多，这表明不同数据集对尺度不变性的需求不同。</li>
<li><strong>部分场景下的性能提升有限：</strong> 在某些基线模型（如Vanilla ViT on Archaeoscape）性能本身较差的情况下，继电器令牌的提升效果可能不那么显著。</li>
<li><strong>对特定类别提升效果差异：</strong> 在Cityscapes数据集中，中等规模物体和功能性类别从全局上下文获益更多，而小型、精细结构物体（如栅栏、交通灯）的提升相对较小。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>更精细的跨尺度交互：</strong> 探索更复杂的继电器令牌交互机制，以进一步提升跨尺度信息的融合效率。</li>
<li><strong>自适应继电器令牌数量：</strong> 研究如何根据不同任务或数据集的特点，自适应地选择继电器令牌的数量，以在性能和效率之间取得更好的平衡。</li>
<li><strong>更广泛的应用：</strong> 将继电器令牌方法推广到其他计算机视觉任务，如目标检测、实例分割等，特别是在处理高分辨率或多尺度场景时。</li>
<li><strong>硬件优化：</strong> 进一步研究如何优化继电器令牌的实现，以最大化并行处理的效率，并探索其在边缘设备上的部署潜力。</li>
<li><strong>理解继电器令牌的内部机制：</strong> 深入分析继电器令牌在不同层级和不同数据集中的注意力模式，以更全面地理解其工作原理。</li>
</ul>
<p>总而言之，这篇论文提出了一种创新的“继电器令牌”机制，成功地解决了Vision Transformer在超高分辨率图像语义分割中的关键挑战，通过轻量级的多尺度融合，显著提升了分割精度和效率，为该领域的研究和应用提供了有价值的贡献。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose a simple yet effective method that brings explicit multi scale reasoning to vision transformers, simultaneously preserving local details and global awareness.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.05927v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.05927v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.05853v1'></a></p>
<h2 id="layergs-decomposition-and-inpainting-of-layered-3d-human-avatars-via-2d-gaussian-splatting"><a href="https://arxiv.org/abs/2601.05853v1">LayerGS: Decomposition and Inpainting of Layered 3D Human Avatars via 2D Gaussian Splatting</a></h2>
<p><strong>Authors:</strong> Yinghan Xu, John Dingliana</p>
<p><strong>Published:</strong> 2026-01-09</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.GR</p>
<p><strong>Abstract:</strong></p>
<p>We propose a novel framework for decomposing arbitrarily posed humans into animatable multi-layered 3D human avatars, separating the body and garments. Conventional single-layer reconstruction methods lock clothing to one identity, while prior multi-layer approaches struggle with occluded regions. We overcome both limitations by encoding each layer as a set of 2D Gaussians for accurate geometry and photorealistic rendering, and inpainting hidden regions with a pretrained 2D diffusion model via score-distillation sampling (SDS). Our three-stage training strategy first reconstructs the coarse canonical garment via single-layer reconstruction, followed by multi-layer training to jointly recover the inner-layer body and outer-layer garment details. Experiments on two 3D human benchmark datasets (4D-Dress, Thuman2.0) show that our approach achieves better rendering quality and layer decomposition and recomposition than the previous state-of-the-art, enabling realistic virtual try-on under novel viewpoints and poses, and advancing practical creation of high-fidelity 3D human assets for immersive applications. Our code is available at https://github.com/RockyXu66/LayerGS</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行深入分析。</p>
<p><strong>论文摘要分析：LayerGS: Decomposition and Inpainting of Layered 3D Human Avatars via 2D Gaussian Splatting</strong></p>
<p><strong>1. 论文的主要贡献（2-3句话）</strong></p>
<p>该论文提出了一种新颖的框架 LayerGS，能够将任意姿态的人体分解为可动画化的多层 3D 人体化身，并能精确地分离身体和服装。通过结合 2D 高斯泼溅（2D Gaussian Splatting）进行几何和渲染，以及利用预训练的 2D 扩散模型进行遮挡区域的修复（inpainting），LayerGS 克服了现有方法的局限性，实现了更高质量的渲染和更精细的层分解与重组。</p>
<p><strong>2. 关键创新点或方法论</strong></p>
<ul>
<li>
<p><strong>2D 高斯泼溅（2D Gaussian Splatting）作为核心表示：</strong> 这是该方法最显著的创新之一。传统的 3D 重建方法通常依赖于点云、网格或体素等表示。而 LayerGS 将每个层（身体和服装）表示为一组 2D 高斯分布。这种表示方式具有以下优势：</p>
<ul>
<li><strong>高效渲染：</strong> 2D 高斯泼溅以其极高的渲染速度和逼真的视觉效果而闻名，尤其是在处理复杂场景时。</li>
<li><strong>精确几何：</strong> 高斯分布能够有效地编码局部几何信息，即使在低分辨率输入下也能捕捉到细节。</li>
<li><strong>易于处理遮挡：</strong> 将 3D 对象分解为 2D 层，并利用 2D 图像作为基础，使得处理遮挡问题更加直观。</li>
</ul>
</li>
<li>
<p><strong>多层分解与修复（Decomposition and Inpainting）：</strong></p>
<ul>
<li><strong>层级分解：</strong> 论文明确地将人体分解为“内层身体”和“外层服装”两个或多个可动画化的层。这解决了传统单层重建方法将服装“锁定”在特定身份上的问题。</li>
<li><strong>遮挡区域修复（Inpainting）：</strong> 这是解决多层方法在处理遮挡区域时遇到的困难的关键。论文利用预训练的 2D 扩散模型（如 Stable Diffusion 等）通过<strong>得分蒸馏采样（Score Distillation Sampling, SDS）</strong>来生成被遮挡区域的细节。SDS 是一种将预训练的生成模型（如扩散模型）的生成能力“蒸馏”到 3D 表示（如高斯泼溅）中的技术，使得 3D 模型能够学习到与 2D 模型相似的纹理和细节。</li>
</ul>
</li>
<li>
<p><strong>三阶段训练策略：</strong></p>
<ol>
<li><strong>粗糙规范服装重建：</strong> 首先通过单层重建方法恢复服装的粗糙规范表示。</li>
<li><strong>多层联合训练：</strong> 接着，联合训练内层身体和外层服装的细节，实现更精确的几何和纹理。</li>
<li><strong>（推测）SDS 驱动的细节增强：</strong> 在前两个阶段的基础上，利用 SDS 技术进一步完善被遮挡区域的细节，提升整体的真实感。</li>
</ol>
</li>
</ul>
<p><strong>3. 对该领域的潜在影响</strong></p>
<ul>
<li><strong>提升 3D 人体化身创建的效率和质量：</strong> LayerGS 提供了一种更有效、更逼真的方法来创建可动画化的 3D 人体化身，尤其是在处理服装和身体分离方面。</li>
<li><strong>推动虚拟试穿和数字时尚的发展：</strong> 精确的服装和身体分离以及逼真的渲染能力，将极大地促进虚拟试穿应用的真实性和用户体验。用户可以更自由地更换服装，并看到其在不同姿态下的真实效果。</li>
<li><strong>为元宇宙和沉浸式应用提供高质量资产：</strong> 高保真度的 3D 人体资产是构建逼真元宇宙和沉浸式体验的基础。LayerGS 的方法有望降低创建这些资产的门槛，并提高其质量。</li>
<li><strong>促进 2D 生成模型在 3D 内容生成中的应用：</strong> 该研究展示了如何有效地将强大的 2D 扩散模型的能力迁移到 3D 场景中，为未来利用大型生成模型进行 3D 内容创作开辟了新的道路。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用</strong></p>
<ul>
<li><strong>虚拟现实 (VR) 和增强现实 (AR)：</strong> 创建逼真的虚拟化身，用于社交、游戏和远程协作。</li>
<li><strong>数字时尚和电子商务：</strong> 实现更具吸引力和准确性的虚拟试穿体验。</li>
<li><strong>电影和游戏制作：</strong> 快速生成高质量的 3D 人物模型，用于角色动画和特效。</li>
<li><strong>机器人和人机交互：</strong> 为机器人创建更具表现力的人体模型，用于模拟和交互。</li>
<li><strong>医学可视化：</strong> 在某些情况下，可能用于创建和操纵人体模型进行教学或模拟。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性</strong></p>
<ul>
<li><strong>对输入数据的依赖：</strong> 虽然摘要提到“任意姿态”，但 2D 高斯泼溅的质量通常与输入图像的质量和覆盖范围有关。如果输入图像存在严重的模糊、低分辨率或遮挡，修复效果可能会受到影响。</li>
<li><strong>扩散模型的计算成本：</strong> SDS 的过程通常计算量较大，尤其是在训练阶段。虽然 2D 高斯泼溅本身渲染速度快，但训练过程的整体效率可能仍是一个挑战。</li>
<li><strong>层级分解的鲁棒性：</strong> 尽管论文声称克服了多层方法的局限性，但对于极其复杂或交织的服装（例如，多层紧身衣、带有大量褶皱的裙子），精确的层级分解仍然可能是一个挑战。</li>
<li><strong>“规范服装”的定义：</strong> 论文提到“粗糙规范服装”，这可能意味着需要一个预定义的“规范”服装模型或模板，或者模型需要学习一个通用的服装表示，这可能在泛化到非常规服装时存在一定难度。</li>
<li><strong>潜在的“伪影”或不一致性：</strong> 尽管 SDS 可以修复遮挡，但生成的细节可能并非总是与原始场景完全一致，可能出现轻微的伪影或风格不匹配。</li>
</ul>
<p><strong>总结：</strong></p>
<p>LayerGS 是一篇在 3D 人体化身生成领域具有重要意义的研究。它巧妙地结合了 2D 高斯泼溅的高效渲染能力和 2D 扩散模型的强大生成能力，解决了传统方法在处理多层结构和遮挡区域时的痛点。其核心创新在于将 3D 问题转化为 2D 层面上的表示和修复，并通过精巧的三阶段训练策略实现高质量的分解和重构。这项工作有望在虚拟试穿、元宇宙等领域产生深远影响，同时也为利用大型生成模型进行 3D 内容创作提供了新的范式。然而，其性能仍可能受到输入数据质量、计算成本以及复杂服装结构的挑战。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose a novel framework for decomposing arbitrarily posed humans into animatable multi-layered 3D human avatars, separating the body and garments.</li>
<li>Experiments on two 3D human benchmark datasets (4D-Dress, Thuman2.0) show that our approach achieves better rendering quality and layer decomposition and recomposition than the previous state-of-the-art, enabling realistic virtual try-on under novel viewpoints and poses, and advancing practical creation of high-fidelity 3D human assets for immersive applications.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.05853v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.05853v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.05851v1'></a></p>
<h2 id="router-suggest-dynamic-routing-for-multimodal-auto-completion-in-visually-grounded-dialogs"><a href="https://arxiv.org/abs/2601.05851v1">Router-Suggest: Dynamic Routing for Multimodal Auto-Completion in Visually-Grounded Dialogs</a></h2>
<p><strong>Authors:</strong> Sandeep Mishra, Devichand Budagam, Anubhab Mandal, Bishal Santra, Pawan Goyal, Manish Gupta</p>
<p><strong>Published:</strong> 2026-01-09</p>
<p><strong>Categories:</strong> cs.CL, cs.AI, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Real-time multimodal auto-completion is essential for digital assistants, chatbots, design tools, and healthcare consultations, where user inputs rely on shared visual context. We introduce Multimodal Auto-Completion (MAC), a task that predicts upcoming characters in live chats using partially typed text and visual cues. Unlike traditional text-only auto-completion (TAC), MAC grounds predictions in multimodal context to better capture user intent. To enable this task, we adapt MMDialog and ImageChat to create benchmark datasets. We evaluate leading vision-language models (VLMs) against strong textual baselines, highlighting trade-offs in accuracy and efficiency. We present Router-Suggest, a router framework that dynamically selects between textual models and VLMs based on dialog context, along with a lightweight variant for resource-constrained environments. Router-Suggest achieves a 2.3x to 10x speedup over the best-performing VLM. A user study shows that VLMs significantly excel over textual models on user satisfaction, notably saving user typing effort and improving the quality of completions in multi-turn conversations. These findings underscore the need for multimodal context in auto-completions, leading to smarter, user-aware assistants.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Router-Suggest: Dynamic Routing for Multimodal Auto-Completion in Visually-Grounded Dialogs”的全面中文摘要：</p>
<p><strong>论文题目：</strong> Router-Suggest: Dynamic Routing for Multimodal Auto-Completion in Visually-Grounded Dialogs</p>
<p><strong>作者：</strong> Sandeep Mishra, Devichand Budagam, Anubhab Mandal, Bishal Santra, Pawan Goyal, Manish Gupta</p>
<hr />
<p><strong>论文摘要</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong></p>
<p>本文旨在解决在多模态对话系统中实现实时、智能的文本自动补全（auto-completion）问题。传统的文本自动补全（TAC）仅依赖文本信息，无法有效利用对话中的视觉上下文来理解用户意图，尤其是在用户输入部分的情况下。这导致在数字助手、聊天机器人、设计工具和医疗咨询等场景下，自动补全的准确性和用户体验受到限制。因此，研究的核心问题是如何在部分文本输入和视觉线索的共同作用下，准确预测用户接下来要输入的文本，并实现高效、低延迟的多模态自动补全。</p>
<p><strong>2. 主要创新点/方法贡献：</strong></p>
<ul>
<li><strong>提出多模态自动补全（MAC）任务：</strong> 作者首次定义了多模态自动补全（MAC）任务，该任务旨在利用部分输入的文本、对话历史以及视觉上下文来预测用户接下来的文本输入。这与传统的文本自动补全（TAC）和查询自动补全（QAC）有显著区别。</li>
<li><strong>构建多模态自动补全基准数据集：</strong> 作者通过改编现有的 MMDialog 和 ImageChat 数据集，并利用 GPT-4V 进行严格的图像相关性过滤，构建了用于 MAC 任务的标准化基准数据集。这些数据集确保了图像在对话中具有高度相关性，能够有效支持模型训练和评估。</li>
<li><strong>提出 Router-Suggest 动态路由框架：</strong> 为了平衡文本模型和视觉语言模型（VLMs）在准确性和效率上的权衡，作者提出了 Router-Suggest 框架。该框架能够根据对话上下文的视觉重要性，动态地在轻量级文本模型和更强大的 VLM 之间进行选择，从而实现高效且准确的自动补全。该框架还包含一个适用于资源受限环境的轻量级变体。</li>
<li><strong>设计MAC特有的评估指标：</strong> 作者引入了一套针对 MAC 任务的评估指标，包括触发率（TR）、句法匹配（SM）、部分召回率（PR-R）、部分精确率（PR-P）、部分 F1 分数（PR-F1）以及打字节省（TES）。这些指标能够更全面地评估自动补全系统的准确性、可用性和效率。</li>
</ul>
<p><strong>3. 主要研究结果与意义：</strong></p>
<ul>
<li><strong>VLM 的优势：</strong> 实验结果表明，在 MAC 任务上，视觉语言模型（VLMs）相比于纯文本模型具有显著优势，尤其是在处理未见过的（unseen）前缀时，VLMs 能够更好地利用视觉信息，提供更鲁棒的补全，并显著提高用户满意度和打字节省（TES）。</li>
<li><strong>Router-Suggest 的效率提升：</strong> Router-Suggest 框架能够实现比最佳 VLM 快 2.3 倍到 10 倍的速度提升，同时保持具有竞争力的准确性。这证明了动态路由策略在多模态自动补全中的有效性，能够显著降低延迟，提高系统响应速度。</li>
<li><strong>用户研究的验证：</strong> 用户研究结果证实，VLMs 提供的补全能够显著提高用户满意度，并有效节省用户的打字量。这进一步强调了在自动补全中融入多模态上下文的重要性，能够催生更智能、更懂用户的助手。</li>
<li><strong>对计算机视觉领域的重要性：</strong> 该研究将计算机视觉技术（如图像理解）与自然语言处理（如文本生成和自动补全）深度融合，为构建更具交互性和智能性的多模态数字助手开辟了新的道路。它展示了如何利用视觉信息来增强用户输入预测的准确性和用户体验，这是当前人机交互领域的一个重要发展方向。</li>
</ul>
<p><strong>4. 提及的局限性：</strong></p>
<ul>
<li><strong>数据集局限性：</strong> MAC 基准数据集虽然经过精心构建，但可能存在选择偏差，并且目前主要覆盖单图像上下文，这限制了其泛化到涉及多图像或动态视觉变化的真实世界多模态场景的能力。</li>
<li><strong>路由机制的解释性：</strong> Router-Suggest 框架依赖于嵌入式启发式方法进行路由，其决策过程可能缺乏可解释性，并且在领域迁移时可能面临性能下降的风险。</li>
<li><strong>用户研究的规模：</strong> 用户研究虽然提供了有价值的见解，但其样本量相对较小，可能无法完全捕捉到所有影响用户体验的关键因素，例如信息错误、文化差异或人口统计学匹配问题。</li>
<li><strong>公平性和成本分配：</strong> 路由器的调用模式可能导致某些输入类型或用户群体被分配到计算成本更高的 MAC 模型，从而引发不公平的延迟、计算成本或体验质量问题。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>扩展到多图像和动态视觉上下文：</strong> 研究如何处理包含多个图像或视频的对话，以实现更全面的多模态理解和补全。</li>
<li><strong>提高路由机制的可解释性：</strong> 开发更具可解释性的路由策略，以便更好地理解模型决策过程，并提高其在不同领域下的鲁棒性。</li>
<li><strong>更大规模和多样化的用户研究：</strong> 进行更广泛的用户研究，以更全面地评估 MAC 系统在不同用户群体和场景下的表现，并深入分析影响用户满意度的因素。</li>
<li><strong>公平性和成本优化：</strong> 探索更公平的路由策略，以确保所有用户都能获得一致的高质量体验，并进一步优化计算资源分配。</li>
<li><strong>端到端的多模态生成：</strong> 探索将自动补全与完整的响应生成相结合的端到端模型，以实现更流畅和连贯的多模态对话交互。</li>
</ul>
<hr />
<p>总而言之，这篇论文在多模态自动补全领域做出了重要贡献，不仅定义了一个新的任务，构建了新的数据集，还提出了创新的 Router-Suggest 框架，显著提升了多模态对话系统的效率和用户体验。研究结果强调了视觉信息在增强用户输入预测方面的关键作用，为未来更智能、更具交互性的人机交互系统奠定了基础。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce Multimodal Auto-Completion (MAC), a task that predicts upcoming characters in live chats using partially typed text and visual cues.</li>
<li>We present Router-Suggest, a router framework that dynamically selects between textual models and VLMs based on dialog context, along with a lightweight variant for resource-constrained environments.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.05851v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.05851v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.05848v1'></a></p>
<h2 id="goal-force-teaching-video-models-to-accomplish-physics-conditioned-goals"><a href="https://arxiv.org/abs/2601.05848v1">Goal Force: Teaching Video Models To Accomplish Physics-Conditioned Goals</a></h2>
<p><strong>Authors:</strong> Nate Gillman, Yinghua Zhou, Zitian Tang, Evan Luo, Arjan Chakravarthy, Daksh Aggarwal, Michael Freeman, Charles Herrmann, Chen Sun</p>
<p><strong>Published:</strong> 2026-01-09</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>Recent advancements in video generation have enabled the development of ``world models'' capable of simulating potential futures for robotics and planning. However, specifying precise goals for these models remains a challenge; text instructions are often too abstract to capture physical nuances, while target images are frequently infeasible to specify for dynamic tasks. To address this, we introduce Goal Force, a novel framework that allows users to define goals via explicit force vectors and intermediate dynamics, mirroring how humans conceptualize physical tasks. We train a video generation model on a curated dataset of synthetic causal primitives-such as elastic collisions and falling dominos-teaching it to propagate forces through time and space. Despite being trained on simple physics data, our model exhibits remarkable zero-shot generalization to complex, real-world scenarios, including tool manipulation and multi-object causal chains. Our results suggest that by grounding video generation in fundamental physical interactions, models can emerge as implicit neural physics simulators, enabling precise, physics-aware planning without reliance on external engines. We release all datasets, code, model weights, and interactive video demos at our project page.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Goal Force: Teaching Video Models To Accomplish Physics-Conditioned Goals”的全面中文摘要：</p>
<p><strong>论文题目：</strong> Goal Force: Teaching Video Models To Accomplish Physics-Conditioned Goals</p>
<p><strong>作者：</strong> Nate Gillman, Yinghua Zhou, Zitian Tang, Evan Luo, Arjan Chakravarthy, Daksh Aggarwal, Michael Freeman, Charles Herrmann, Chen Sun</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
随着视频生成技术的飞速发展，能够模拟未来场景的“世界模型”在机器人和规划领域展现出巨大潜力。然而，如何精确地为这些模型设定目标仍然是一个挑战。传统的文本指令往往过于抽象，难以捕捉物理世界的细微之处；而目标图像则对于动态任务来说，定义起来既困难又不切实际。</p>
<p><strong>2. 关键创新/方法论贡献：</strong>
本文提出了一种名为 <strong>Goal Force</strong> 的新颖框架，旨在解决上述问题。其核心创新在于：</p>
<ul>
<li><strong>以“目标力”定义目标：</strong> Goal Force 允许用户通过明确的<strong>力向量</strong>和<strong>中间动力学</strong>来定义目标，这更符合人类在概念化物理任务时的直观方式。用户不再需要指定最终的静态状态，而是定义一个期望的“力”，模型则需要找出实现该力的因果链条。</li>
<li><strong>多通道物理控制信号：</strong> 引入了一个三通道的物理控制信号张量，分别编码<strong>直接力（cause）</strong>、<strong>目标力（effect）</strong>和<strong>质量</strong>。这使得模型能够理解和生成物理交互的因果关系。</li>
<li><strong>隐式神经物理模拟器：</strong> 通过在包含弹性碰撞、多米诺骨牌倒塌等<strong>合成因果原语</strong>的数据集上进行训练，模型学会了在时间与空间上传播力，并能<strong>零样本泛化</strong>到复杂的真实世界场景。模型在推理时<strong>无需外部物理引擎</strong>，而是自身充当一个隐式的神经物理模拟器。</li>
<li><strong>训练策略：</strong> 采用随机掩码因果信息（直接力或目标力）的训练策略，迫使模型学习物理推理，理解“目标→计划”和“动作→结果”的关系。同时，随机掩码质量通道也促使模型在有额外信息时利用它，否则则依赖自身学到的物理先验。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>强大的零样本泛化能力：</strong> 尽管仅在简单的合成数据上训练，Goal Force 模型在工具使用（如高尔夫球杆击球、用手抓取玫瑰）、多物体因果链以及复杂场景（如台球、橡皮鸭）中展现出卓越的泛化能力，能够生成物理上合理且符合目标力的视频。
*   <strong>物理准确性与多样性：</strong> 实验表明，模型生成的视觉计划在物理上是准确的，能够正确识别和选择物理约束下的发起者，并能生成多样化的解决方案，避免了模式崩溃。
*   <strong>利用特权物理信息：</strong> 模型能够利用控制信号中的质量信息来指导其计划，例如在碰撞任务中根据物体质量调整速度，即使在分布外场景下也能取得良好效果。
*   <strong>超越现有方法：</strong> 与仅能直接施加力的 prior 方法（如 Force Prompting, PhysGen, PhysDreamer）不同，Goal Force 能够<strong>规划因果链条</strong>来达成目标力，实现了从“是什么”（what if）到“怎么做”（how-to）的转变。</p>
<p><strong>4. 提及的局限性：</strong>
论文中并未明确列出具体的局限性，但从其研究内容和方法来看，可以推测：
*   <strong>合成数据依赖：</strong> 尽管模型展现了强大的泛化能力，但其训练仍依赖于合成数据集。真实世界复杂性的完全捕捉可能仍需进一步探索。
*   <strong>“目标力”的定义粒度：</strong> 虽然比文本和图像更精确，但“目标力”的定义仍可能需要用户对物理交互有一定的理解。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>更复杂的物理交互：</strong> 探索更广泛、更复杂的物理现象，如流体动力学、变形物体等。
*   <strong>更精细的目标力控制：</strong> 进一步细化目标力的定义方式，使其能够表达更复杂的意图。
*   <strong>与真实机器人系统的结合：</strong> 将 Goal Force 框架应用于实际机器人控制，实现更智能、更具物理意识的规划和执行。
*   <strong>交互式学习：</strong> 允许模型通过与环境的交互来不断学习和改进其物理理解和规划能力。</p>
<p><strong>总结：</strong>
Goal Force 论文提出了一种创新的方法，通过定义“目标力”来指导视频生成模型完成物理任务。该方法通过在合成因果原语上进行训练，使模型能够学习到隐式的物理规律，并展现出强大的零样本泛化能力，能够规划复杂的因果链条以达成用户设定的物理目标。这项工作为构建更智能、更具物理意识的“世界模型”和机器人规划系统奠定了重要基础，是计算机视觉和机器学习领域在理解和生成物理交互方面的一项重要进展。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this, we introduce Goal Force, a novel framework that allows users to define goals via explicit force vectors and intermediate dynamics, mirroring how humans conceptualize physical tasks.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.05848v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.05848v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.05844v1'></a></p>
<h2 id="dextercap-an-affordable-and-automated-system-for-capturing-dexterous-hand-object-manipulation"><a href="https://arxiv.org/abs/2601.05844v1">DexterCap: An Affordable and Automated System for Capturing Dexterous Hand-Object Manipulation</a></h2>
<p><strong>Authors:</strong> Yutong Liang, Shiyi Xu, Yulong Zhang, Bowen Zhan, He Zhang, Libin Liu</p>
<p><strong>Published:</strong> 2026-01-09</p>
<p><strong>Categories:</strong> cs.GR, cs.AI, cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>Capturing fine-grained hand-object interactions is challenging due to severe self-occlusion from closely spaced fingers and the subtlety of in-hand manipulation motions. Existing optical motion capture systems rely on expensive camera setups and extensive manual post-processing, while low-cost vision-based methods often suffer from reduced accuracy and reliability under occlusion. To address these challenges, we present DexterCap, a low-cost optical capture system for dexterous in-hand manipulation. DexterCap uses dense, character-coded marker patches to achieve robust tracking under severe self-occlusion, together with an automated reconstruction pipeline that requires minimal manual effort. With DexterCap, we introduce DexterHand, a dataset of fine-grained hand-object interactions covering diverse manipulation behaviors and objects, from simple primitives to complex articulated objects such as a Rubik's Cube. We release the dataset and code to support future research on dexterous hand-object interaction.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于“DexterCap: An Affordable and Automated System for Capturing Dexterous Hand-Object Manipulation”的论文的中文摘要，其中包含了您要求的各个部分：</p>
<p><strong>论文题目：</strong> DexterCap: An Affordable and Automated System for Capturing Dexterous Hand-Object Manipulation
<strong>作者：</strong> Yutong Liang, Shiyi Xu, Yulong Zhang, Bowen Zhan, He Zhang, Libin Liu</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
该论文旨在解决捕捉精细手部与物体交互（特别是“手中操作”）的难题。传统的光学运动捕捉系统成本高昂且需要大量手动后处理，而低成本的视觉方法在遮挡情况下准确性和可靠性不足。这些挑战源于手指间的严重自遮挡以及手中操作动作的精细性。</p>
<p><strong>2. 主要创新点/方法论贡献：</strong>
*   <strong>DexterCap 系统：</strong> 作者提出了一种低成本、高精度的光学捕捉系统 DexterCap，用于捕捉精细的手部-物体交互。
    *   <strong>定制化标记系统：</strong> 使用密集、字符编码的棋盘格标记，这些标记具有高对比度和独特的字符ID，即使在严重自遮挡情况下也能实现鲁棒跟踪和识别。
    *   <strong>自动化重建流程：</strong> 结合深度学习模型（CornerNet, EdgeNet, BlockNet）进行标记检测、边缘和块识别，以及一个自动化的三维重建流程，大大减少了手动后处理的工作量。
    *   <strong>MANO 模型集成：</strong> 利用参数化的 MANO 手部模型来重建手部姿态，并结合物体特定的求解器来估计物体姿态。
    *   <strong>低成本硬件：</strong> 系统使用了相对经济的工业相机，使得整体硬件成本低于 6,000 美元。
*   <strong>DexterHand 数据集：</strong> 作者构建了一个名为 DexterHand 的大规模、细粒度手部-物体交互数据集。该数据集包含了多种多样的手中操作行为，涵盖了从简单物体到复杂的关节物体（如魔方）的交互。数据集的特点是捕捉细节丰富、控制精确且持续时间长。</p>
<p><strong>3. 主要结果及其意义：</strong>
*   <strong>系统性能：</strong> DexterCap 系统在标记检测方面取得了高精度和高召回率（例如，CornerNet 的 F1 分数达到 87.7%）。在手部-物体重建方面，系统在校准阶段实现了 0.77 ± 0.28 mm 的标记重建误差，在动态操作阶段为 2.06 ± 1.09 mm。物体姿态估计的平均误差为 1.512 mm。
*   <strong>数据质量：</strong> DexterHand 数据集提供了高质量的手部-物体交互数据，平均渗透量为 3.8mm±3.1mm，表明了捕捉的物理合理性。
*   <strong>基准对比：</strong> 与现有的商业系统（如 Vicon）和视觉方法（如 HaMeR, GigaHands）相比，DexterCap 在运动平滑度（低 Jerk）和重建质量（高 MSNR）方面表现出竞争力，并且是唯一能够捕捉精细手中操作的系统。
*   <strong>意义：</strong> 该工作显著降低了捕捉精细手部-物体交互的门槛，为研究人员提供了一个经济实惠且易于部署的解决方案，并发布了一个高质量的数据集，极大地推动了机器人学、计算机视觉和动画领域在理解和生成复杂手中操作方面的研究。</p>
<p><strong>4. 提及的局限性：</strong>
*   <strong>遮挡问题：</strong> 尽管使用了密集标记和多视角，但当大量标记同时被遮挡时（例如，手指完全插入物体内部），系统的重建质量会下降，可能导致伪影（如手指-物体穿透）。
*   <strong>对标记的依赖：</strong> 作为一种标记系统，其性能受到标记可见性的影响。
*   <strong>单视角性能：</strong> 在评估中提到，当单独使用一个相机视图时，性能会下降。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>克服遮挡：</strong> 探索更鲁棒的遮挡处理方法，例如使用遮挡感知的估计、基于学习的姿态先验，或集成额外的传感模态（如 IMU）。
*   <strong>扩展数据集：</strong> 增加更多样化的被试者、物体（包括可变形和关节物体）以及更复杂的操纵任务。
*   <strong>语义标注：</strong> 增加更细粒度的语义标注，如抓取类型、功能意图、接触区域和作用力，以支持更可控和上下文感知的生成模型。
*   <strong>物理模拟集成：</strong> 将数据捕捉和生成流程与物理模拟环境相结合，用于验证运动的物理合理性，并训练机器人代理。
*   <strong>Sim-to-Real 迁移：</strong> 探索将模拟中的策略迁移到真实机器人上的技术。</p>
<p>总而言之，DexterCap 系统和 DexterHand 数据集是该领域的一项重要贡献，它们以一种经济高效且自动化的方式解决了长期存在的挑战，为未来的研究开辟了新的可能性。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address these challenges, we present DexterCap, a low-cost optical capture system for dexterous in-hand manipulation.</li>
<li>With DexterCap, we introduce DexterHand, a dataset of fine-grained hand-object interactions covering diverse manipulation behaviors and objects, from simple primitives to complex articulated objects such as a Rubik's Cube.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.05844v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.05844v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.05823v1'></a></p>
<h2 id="boosting-latent-diffusion-models-via-disentangled-representation-alignment"><a href="https://arxiv.org/abs/2601.05823v1">Boosting Latent Diffusion Models via Disentangled Representation Alignment</a></h2>
<p><strong>Authors:</strong> John Page, Xuesong Niu, Kai Wu, Kun Gai</p>
<p><strong>Published:</strong> 2026-01-09</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Latent Diffusion Models (LDMs) generate high-quality images by operating in a compressed latent space, typically obtained through image tokenizers such as Variational Autoencoders (VAEs). In pursuit of a generation-friendly VAE, recent studies have explored leveraging Vision Foundation Models (VFMs) as representation alignment targets for VAEs, mirroring the approach commonly adopted for LDMs. Although this yields certain performance gains, using the same alignment target for both VAEs and LDMs overlooks their fundamentally different representational requirements. We advocate that while LDMs benefit from latents retaining high-level semantic concepts, VAEs should excel in semantic disentanglement, enabling encoding of attribute-level information in a structured way. To address this, we propose the Semantic disentangled VAE (Send-VAE), explicitly optimized for disentangled representation learning through aligning its latent space with the semantic hierarchy of pre-trained VFMs. Our approach employs a non-linear mapper network to transform VAE latents, aligning them with VFMs to bridge the gap between attribute-level disentanglement and high-level semantics, facilitating effective guidance for VAE learning. We evaluate semantic disentanglement via linear probing on attribute prediction tasks, showing strong correlation with improved generation performance. Finally, using Send-VAE, we train flow-based transformers SiTs; experiments show Send-VAE significantly speeds up training and achieves a state-of-the-art FID of 1.21 and 1.75 with and without classifier-free guidance on ImageNet 256x256.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Boosting Latent Diffusion Models via Disentangled Representation Alignment”的全面中文摘要，其中包含了您要求的各个方面：</p>
<p><strong>论文题目：</strong> Boosting Latent Diffusion Models via Disentangled Representation Alignment (通过解耦表示对齐提升潜空间扩散模型)</p>
<p><strong>作者：</strong> John Page, Xuesong Niu, Kai Wu, Kun Gai</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 主要问题/研究问题：</strong>
该论文旨在解决当前潜空间扩散模型 (LDMs) 在图像生成领域取得显著成功的同时，其核心组件——变分自编码器 (VAE) 的表示能力尚未得到充分挖掘的问题。现有研究尝试通过将预训练的视觉基础模型 (VFMs) 作为对齐目标来提升 VAE 的生成能力，但这种方法忽略了 VAE 和 LDM 在表示需求上的根本差异。LDM 需要高层语义信息来指导生成，而 VAE 则需要更强的语义解耦能力来编码结构化的、属性层面的信息。因此，论文的核心问题是：<strong>如何设计一个更适合生成任务的 VAE，使其能够更好地捕捉和利用图像的属性级语义信息，从而提升 LDM 的生成性能和训练效率？</strong></p>
<p><strong>2. 关键创新/方法论贡献：</strong>
该论文的核心贡献在于提出了 <strong>Semantic-disentangled VAE (Send-VAE)</strong>，一种新型的 VAE，其关键创新点在于：</p>
<ul>
<li><strong>强调语义解耦的重要性：</strong> 论文通过实验证明，VAE 的语义解耦能力与下游生成性能之间存在强烈的正相关关系，并将其作为衡量生成友好型 VAE 的关键指标。</li>
<li><strong>引入非线性映射器网络：</strong> Send-VAE 引入了一个复杂的非线性映射器网络（包含 patch embedding 层、ViT 层和 MLP 投影器），用于将 VAE 的潜空间表示与预训练 VFM 的表示进行对齐。这个映射器网络旨在弥合 VAE 的属性级解耦表示与 VFM 的高层语义表示之间的鸿沟，实现更有效的语义注入。</li>
<li><strong>提出 Send-VAE 框架：</strong> Send-VAE 通过对齐 VAE 的潜空间与 VFM 的语义层级来优化 VAE 的语义解耦能力。这种方法与直接对齐 VAE 和 LDM 的表示不同，而是专注于提升 VAE 本身的表示质量。</li>
<li><strong>创新的评估方法：</strong> 论文将线性探针（linear probing）在属性预测任务上的表现作为衡量 VAE 语义解耦能力的新颖且内在的指标，并证明了其与生成性能的相关性。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong>
论文通过在 ImageNet 256x256 数据集上的实验，取得了以下主要成果：</p>
<ul>
<li><strong>显著提升生成性能：</strong> 使用 Send-VAE 训练的流模型（如 SiTs）在 ImageNet 256x256 数据集上取得了 <strong>1.21 (有条件生成) 和 1.75 (无条件生成) 的 SOTA FID 分数</strong>，显著优于现有方法。</li>
<li><strong>加速训练过程：</strong> Send-VAE 能够显著加速 LDM 的训练过程，即使在较少的训练 epoch 下也能获得高质量的生成结果，如表 1 所示，80 epoch 的训练就已接近其他方法在更长训练时间下的表现。</li>
<li><strong>验证了语义解耦的有效性：</strong> 实验结果（如图 2 和表 6）强有力地支持了论文的假设，即语义解耦是生成友好型 VAE 的关键属性。</li>
<li><strong>证明了方法的泛化性：</strong> 消融研究（表 4 和表 5）表明，Send-VAE 在使用不同的 VFM 和 VAE 初始化时都能保持其有效性，证明了该方法的鲁棒性和泛化能力。</li>
</ul>
<p>这些结果表明，Send-VAE 是一种有效的方法，能够显著提升 LDM 的生成质量和训练效率，为构建更强大的图像生成模型提供了新的途径。</p>
<p><strong>4. 提及的局限性：</strong>
论文中提到了一些潜在的局限性：</p>
<ul>
<li><strong>重建性能略有下降：</strong> 论文指出，Send-VAE 的重建性能（rFID）相较于 VA-VAE 略有下降。作者将其归因于 Send-VAE 的解耦潜空间可能牺牲了捕捉极细粒度低级信息的能力，但认为这种权衡对于提升下游生成任务是值得的。</li>
<li><strong>映射器网络的设计：</strong> 虽然论文提出了一个复杂的非线性映射器网络，但其具体结构（如 ViT 层数）的选择仍然需要通过消融实验来确定最佳配置，这可能意味着存在进一步优化的空间。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong>
基于该论文的研究，可以推测出以下潜在的未来研究方向：</p>
<ul>
<li><strong>探索更先进的映射器网络：</strong> 研究更高效、更具表达力的非线性映射器网络结构，以进一步缩小 VAE 和 VFM 之间的表示差距。</li>
<li><strong>更广泛的应用场景：</strong> 将 Send-VAE 的思想应用于其他类型的生成模型，如 GANs 或自回归模型，以评估其通用性。</li>
<li><strong>更精细的属性控制：</strong> 利用 Send-VAE 增强的语义解耦能力，探索更精细的图像属性控制生成，例如通过操纵特定的解耦维度来控制生成图像的特定属性。</li>
<li><strong>跨模态生成：</strong> 将 Send-VAE 的概念扩展到跨模态生成任务，例如文本到图像生成，利用文本编码器的语义信息来指导 VAE 的解耦表示。</li>
<li><strong>更深入的理论分析：</strong> 对 Send-VAE 的语义解耦机制进行更深入的理论分析，以更好地理解其工作原理和性能边界。</li>
</ul>
<p>总而言之，这篇论文通过深入分析 VAE 在 LDM 中的作用，并提出 Send-VAE 这一创新的解决方案，成功地提升了图像生成模型的性能和训练效率，为该领域的研究做出了重要贡献。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this, we propose the Semantic disentangled VAE (Send-VAE), explicitly optimized for disentangled representation learning through aligning its latent space with the semantic hierarchy of pre-trained VFMs. Our approach employs a non-linear mapper network to transform VAE latents, aligning them with VFMs to bridge the gap between attribute-level disentanglement and high-level semantics, facilitating effective guidance for VAE learning.</li>
<li>Finally, using Send-VAE, we train flow-based transformers SiTs; experiments show Send-VAE significantly speeds up training and achieves a state-of-the-art FID of 1.21 and 1.75 with and without classifier-free guidance on ImageNet 256x256.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.05823v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.05823v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.05810v1'></a></p>
<h2 id="scenefoundry-generating-interactive-infinite-3d-worlds"><a href="https://arxiv.org/abs/2601.05810v1">SceneFoundry: Generating Interactive Infinite 3D Worlds</a></h2>
<p><strong>Authors:</strong> ChunTeng Chen, YiChen Hsu, YiWen Liu, WeiFang Sun, TsaiChing Ni, ChunYi Lee, Min Sun, YuanFu Yang</p>
<p><strong>Published:</strong> 2026-01-09</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.LG, cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>The ability to automatically generate large-scale, interactive, and physically realistic 3D environments is crucial for advancing robotic learning and embodied intelligence. However, existing generative approaches often fail to capture the functional complexity of real-world interiors, particularly those containing articulated objects with movable parts essential for manipulation and navigation. This paper presents SceneFoundry, a language-guided diffusion framework that generates apartment-scale 3D worlds with functionally articulated furniture and semantically diverse layouts for robotic training. From natural language prompts, an LLM module controls floor layout generation, while diffusion-based posterior sampling efficiently populates the scene with articulated assets from large-scale 3D repositories. To ensure physical usability, SceneFoundry employs differentiable guidance functions to regulate object quantity, prevent articulation collisions, and maintain sufficient walkable space for robotic navigation. Extensive experiments demonstrate that our framework generates structurally valid, semantically coherent, and functionally interactive environments across diverse scene types and conditions, enabling scalable embodied AI research.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“SceneFoundry: Generating Interactive Infinite 3D Worlds”的全面中文摘要，重点关注其在计算机视觉和机器学习领域的贡献：</p>
<p><strong>论文题目：</strong> SceneFoundry: Generating Interactive Infinite 3D Worlds
<strong>作者：</strong> ChunTeng Chen, YiChen Hsu, YiWen Liu, WeiFang Sun, TsaiChing Ni, ChunYi Lee, Min Sun, YuanFu Yang</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
该论文旨在解决当前3D室内环境生成方法在生成大规模、交互式且物理真实的虚拟世界方面存在的不足。现有方法往往难以捕捉真实室内环境的功能复杂性，特别是包含可动部件的关节式物体（如家具），这些物体对于机器人学习和具身智能至关重要。这限制了生成环境在机器人训练和具身AI研究中的实用性。</p>
<p><strong>2. 主要创新点/方法论贡献：</strong>
SceneFoundry 提出了一种多阶段、可控的生成框架，其核心创新点包括：</p>
<ul>
<li><strong>LLM驱动的参数空间引导：</strong> 利用大型语言模型（LLM）将抽象的自然语言指令转化为低级参数，从而实现对地板布局生成过程的语义化控制，并保留了底层生成器（如Infinigen）的随机多样性。</li>
<li><strong>基于扩散模型的后验采样：</strong> 采用扩散模型进行场景填充，通过后验采样高效地从大型3D资产库中选择和放置关节式家具。</li>
<li><strong>可微分的引导函数（Differentiable Guidance Functions）：</strong> 引入了一系列可微分的约束机制来确保生成场景的功能可用性：<ul>
<li><strong>物体数量控制（Object Quantity Control）：</strong> 精确控制场景中物体的数量。</li>
<li><strong>关节式物体碰撞约束（Articulated Object Collision Constraint）：</strong> 惩罚可动部件被遮挡的配置，确保其可交互性。</li>
<li><strong>可步行区域控制（Walkable Area Control）：</strong> 在后处理阶段优化场景，确保足够的步行空间，保证机器人导航的可用性。</li>
</ul>
</li>
<li><strong>新颖的评估指标：</strong> 提出了用于衡量生成场景可控性的新指标，包括LLM引导布局指标、物体数量控制指标、关节式碰撞比率和可步行区域可控性指标。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong>
通过大量的实验验证，SceneFoundry 能够生成：</p>
<ul>
<li><strong>结构有效、语义连贯且功能交互的3D世界：</strong> 生成的场景在结构上合理，语义上符合用户意图，并且关节式家具能够实现预期的功能（如抽屉可以打开）。</li>
<li><strong>公寓规模的3D场景：</strong> 能够生成完整的公寓尺度场景，而非局限于单个房间。</li>
<li><strong>可控性强：</strong> 通过LLM和引导函数，用户可以根据自然语言指令精确控制场景的布局、物体数量和功能性。</li>
<li><strong>显著优于基线方法：</strong> 在功能性和可控性方面，SceneFoundry 显著优于现有的ATISS、DiffuScene和PhyScene等方法。</li>
</ul>
<p><strong>意义：</strong> SceneFoundry 的工作为机器人学习和具身智能研究提供了大规模、高质量、可控且功能真实的训练环境，极大地降低了对真实世界数据收集的依赖，加速了相关领域的研究进展。</p>
<p><strong>4. 提及的局限性：</strong></p>
<ul>
<li><strong>推理延迟（Inference Latency）：</strong> 多阶段流水线，特别是扩散模型和约束计算，导致生成一个完整公寓场景需要较长时间（约300秒），目前尚不支持实时生成。</li>
<li><strong>关节式物体近似（Heuristic Approximation of Articulation）：</strong> 关节式物体碰撞约束依赖于对边界框的启发式扩展来近似运动空间，对于复杂的多关节物体可能过于保守或不足。</li>
<li><strong>数据集偏差（Dataset Bias）：</strong> 生成场景的风格和多样性受限于训练数据（3D-FRONT和GAPartNet），可能倾向于现代、西式室内设计，对其他文化或历史风格的覆盖不足。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>提高推理效率：</strong> 探索更快的生成模型或优化技术，以实现实时或近实时的场景生成。</li>
<li><strong>更精确的关节式物体建模：</strong> 开发更精细的关节式物体运动空间建模方法，以处理更复杂的机械结构。</li>
<li><strong>提升数据集多样性与泛化能力：</strong> 引入更多样化的数据集，以生成更广泛的建筑风格和文化背景的室内场景，并提高模型的泛化能力。</li>
<li><strong>更丰富的交互性：</strong> 探索生成更复杂的场景交互，例如动态物体、环境事件等，以支持更高级的机器人任务。</li>
<li><strong>用户交互的精细化：</strong> 进一步探索如何通过更直观的用户界面和更细粒度的指令来控制场景生成。</li>
</ul>
<p>总而言之，SceneFoundry 在生成功能性强、可控且逼真的3D室内环境方面取得了显著进展，为具身AI和机器人领域的研究奠定了坚实的基础，同时也指出了未来研究的几个重要方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To ensure physical usability, SceneFoundry employs differentiable guidance functions to regulate object quantity, prevent articulation collisions, and maintain sufficient walkable space for robotic navigation.</li>
<li>Extensive experiments demonstrate that our framework generates structurally valid, semantically coherent, and functionally interactive environments across diverse scene types and conditions, enabling scalable embodied AI research.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.05810v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.05810v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.05747v1'></a></p>
<h2 id="flypose-towards-robust-human-pose-estimation-from-aerial-views"><a href="https://arxiv.org/abs/2601.05747v1">FlyPose: Towards Robust Human Pose Estimation From Aerial Views</a></h2>
<p><strong>Authors:</strong> Hassaan Farooq, Marvin Brenner, Peter St\ütz</p>
<p><strong>Published:</strong> 2026-01-09</p>
<p><strong>Categories:</strong> cs.CV, cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>Unmanned Aerial Vehicles (UAVs) are increasingly deployed in close proximity to humans for applications such as parcel delivery, traffic monitoring, disaster response and infrastructure inspections. Ensuring safe and reliable operation in these human-populated environments demands accurate perception of human poses and actions from an aerial viewpoint. This perspective challenges existing methods with low resolution, steep viewing angles and (self-)occlusion, especially if the application demands realtime feasibile models. We train and deploy FlyPose, a lightweight top-down human pose estimation pipeline for aerial imagery. Through multi-dataset training, we achieve an average improvement of 6.8 mAP in person detection across the test-sets of Manipal-UAV, VisDrone, HIT-UAV as well as our custom dataset. For 2D human pose estimation we report an improvement of 16.3 mAP on the challenging UAV-Human dataset. FlyPose runs with an inference latency of ~20 milliseconds including preprocessing on a Jetson Orin AGX Developer Kit and is deployed onboard a quadrotor UAV during flight experiments. We also publish FlyPose-104, a small but challenging aerial human pose estimation dataset, that includes manual annotations from difficult aerial perspectives: https://github.com/farooqhassaan/FlyPose.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行深入分析。</p>
<p><strong>论文摘要分析：FlyPose: Towards Robust Human Pose Estimation From Aerial Views</strong></p>
<p><strong>1. 论文的主要贡献 (2-3句话的简洁总结):</strong></p>
<p>本研究提出了FlyPose，一个轻量级的、针对无人机（UAV）航拍图像设计的顶层（top-down）人体姿态估计流水线。通过多数据集训练，FlyPose在人脸检测和2D人体姿态估计方面均取得了显著的性能提升，并且能够实现实时推理，为UAV在复杂人类环境中安全可靠的运行提供了关键技术支持。</p>
<p><strong>2. 关键创新点或方法论:</strong></p>
<ul>
<li><strong>针对航拍视角的定制化流水线:</strong> 论文的核心创新在于其专门为航拍视角下的挑战（低分辨率、陡峭视角、遮挡）而设计的轻量级顶层人体姿态估计流水线。这与许多针对地面视角优化的现有方法不同。</li>
<li><strong>多数据集训练策略:</strong> 为了提高模型的泛化能力和鲁棒性，FlyPose采用了多数据集训练的方法，整合了Manipal-UAV、VisDrone、HIT-UAV以及作者自建的数据集。这种策略能够让模型学习到更广泛的航拍场景下的特征。</li>
<li><strong>轻量级设计与实时性能:</strong> FlyPose被设计为“轻量级”，并且在Jetson Orin AGX Developer Kit上实现了约20毫秒的推理延迟（包含预处理）。这对于需要实时决策的UAV应用至关重要。</li>
<li><strong>发布新数据集FlyPose-104:</strong> 作者发布了一个新的、具有挑战性的航拍人体姿态估计数据集FlyPose-104，其中包含了从困难航拍视角手动标注的数据。这为后续研究提供了宝贵的资源。</li>
</ul>
<p><strong>3. 对该领域的潜在影响:</strong></p>
<ul>
<li><strong>推动UAV在复杂环境下的应用:</strong> FlyPose的成功将极大地促进UAV在人口密集区域的安全部署，例如包裹递送、交通监控、灾难响应等。它解决了UAV感知人类行为的关键瓶颈。</li>
<li><strong>为航拍姿态估计设定新的基准:</strong> 通过在多个数据集上取得显著的性能提升，FlyPose有望成为航拍人体姿态估计领域的一个新的性能基准。</li>
<li><strong>促进轻量级、实时姿态估计研究:</strong> 论文强调了实时性能的重要性，这可能会激励更多研究者关注开发高效、轻量级的模型，以满足边缘计算和嵌入式系统的需求。</li>
<li><strong>丰富航拍数据集资源:</strong> FlyPose-104数据集的发布将为研究人员提供一个宝贵的资源，用于训练和评估针对航拍场景的姿态估计模型。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用:</strong></p>
<ul>
<li><strong>无人机监控与安全:</strong> 实时检测和跟踪人群，识别异常行为，用于公共安全、活动管理等。</li>
<li><strong>智能交通管理:</strong> 从空中监测交通流量，识别行人，预测潜在危险。</li>
<li><strong>灾难响应与搜救:</strong> 在灾难现场快速定位和评估受影响人员的位置和状态。</li>
<li><strong>机器人导航与交互:</strong> 使机器人能够理解和预测人类的意图，从而实现更安全的交互。</li>
<li><strong>体育赛事分析:</strong> 从空中视角分析运动员的动作和表现。</li>
<li><strong>农业监测:</strong> 监测农田中的工人活动。</li>
</ul>
<p><strong>5. 可从摘要推断的局限性:</strong></p>
<ul>
<li><strong>数据集的局限性:</strong> 尽管使用了多数据集训练，但摘要中提到的数据集（Manipal-UAV, VisDrone, HIT-UAV, FlyPose-104）可能仍然无法完全覆盖所有可能的航拍场景和环境条件。例如，极端天气、光照变化、不同类型的遮挡等可能仍然是挑战。</li>
<li><strong>“轻量级”的相对性:</strong> “轻量级”通常是相对于其他更庞大的模型而言，其具体计算资源需求和模型大小并未在摘要中明确给出。对于资源极其受限的平台，可能仍需进一步优化。</li>
<li><strong>“鲁棒性”的定义:</strong> 摘要中提到“鲁棒性”，但具体在哪些方面（如遮挡、低分辨率、视角变化）达到了何种程度的鲁棒性，需要通过论文的详细实验来验证。</li>
<li><strong>顶层（Top-down）方法的固有局限:</strong> 顶层方法通常需要先进行人脸检测，然后对检测到的人脸进行姿态估计。如果人脸检测性能不佳（例如，由于低分辨率或遮挡），则会影响整体姿态估计的准确性。</li>
<li><strong>对特定UAV平台的依赖:</strong> 摘要中提到了在Jetson Orin AGX Developer Kit上的性能，这表明模型可能针对该类硬件进行了优化。在其他不同硬件平台上，性能可能会有所差异。</li>
<li><strong>未提及3D姿态估计:</strong> 摘要主要关注2D人体姿态估计，而3D姿态估计在某些应用中可能更为重要，这可能是未来研究的方向。</li>
</ul>
<p>总而言之，FlyPose是一项非常有前景的研究，它有效地解决了无人机航拍视角下人体姿态估计的关键挑战，并为该领域的发展做出了重要贡献。其轻量级设计和实时性能使其在实际应用中具有很高的价值。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>Through multi-dataset training, we achieve an average improvement of 6.8 mAP in person detection across the test-sets of Manipal-UAV, VisDrone, HIT-UAV as well as our custom dataset.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.05747v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.05747v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2601.05741v1'></a></p>
<h2 id="vitnt-fiqa-training-free-face-image-quality-assessment-with-vision-transformers"><a href="https://arxiv.org/abs/2601.05741v1">ViTNT-FIQA: Training-Free Face Image Quality Assessment with Vision Transformers</a></h2>
<p><strong>Authors:</strong> Guray Ozgur, Eduarda Caldeira, Tahar Chettaoui, Jan Niklas Kolf, Marco Huber, Naser Damer, Fadi Boutros</p>
<p><strong>Published:</strong> 2026-01-09</p>
<p><strong>Categories:</strong> cs.CV, cs.LG</p>
<p><strong>Abstract:</strong></p>
<p>Face Image Quality Assessment (FIQA) is essential for reliable face recognition systems. Current approaches primarily exploit only final-layer representations, while training-free methods require multiple forward passes or backpropagation. We propose ViTNT-FIQA, a training-free approach that measures the stability of patch embedding evolution across intermediate Vision Transformer (ViT) blocks. We demonstrate that high-quality face images exhibit stable feature refinement trajectories across blocks, while degraded images show erratic transformations. Our method computes Euclidean distances between L2-normalized patch embeddings from consecutive transformer blocks and aggregates them into image-level quality scores. We empirically validate this correlation on a quality-labeled synthetic dataset with controlled degradation levels. Unlike existing training-free approaches, ViTNT-FIQA requires only a single forward pass without backpropagation or architectural modifications. Through extensive evaluation on eight benchmarks (LFW, AgeDB-30, CFP-FP, CALFW, Adience, CPLFW, XQLFW, IJB-C), we show that ViTNT-FIQA achieves competitive performance with state-of-the-art methods while maintaining computational efficiency and immediate applicability to any pre-trained ViT-based face recognition model.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“ViTNT-FIQA: Training-Free Face Image Quality Assessment with Vision Transformers”的全面中文摘要，其中包含了您要求的各个部分：</p>
<p><strong>论文题目：</strong> ViTNT-FIQA: Training-Free Face Image Quality Assessment with Vision Transformers</p>
<p><strong>作者：</strong> Guray Ozgur, Eduarda Caldeira, Tahar Chettaoui, Jan Niklas Kolf, Marco Huber, Naser Damer, Fadi Boutros</p>
<hr />
<p><strong>摘要</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong></p>
<p>人脸图像质量评估（FIQA）对于构建可靠的人脸识别（FR）系统至关重要。现有方法要么依赖于深度网络的最终层特征，要么需要多次前向传播或反向传播才能实现训练无关（training-free）的评估，这增加了计算开销且限制了其在预训练模型上的即时应用性。因此，研究一种高效、无需额外训练且能有效评估人脸图像质量的方法是当前面临的关键挑战。</p>
<p><strong>2. 关键创新与方法贡献：</strong></p>
<p>本文提出了一种名为 <strong>ViTNT-FIQA</strong> 的新型训练无关人脸图像质量评估方法。其核心创新在于：</p>
<ul>
<li><strong>利用 Vision Transformer (ViT) 的中间层特征：</strong> ViTNT-FIQA 假设高质量的人脸图像在经过 ViT 的多个连续 Transformer 块时，其面部特征表示（patch embeddings）会展现出更稳定、平滑的演化轨迹，而低质量图像则会表现出更 erratic（不规则）的变化。</li>
<li><strong>度量 Patch Embedding 的稳定性：</strong> 该方法通过计算 L2 归一化后的 patch embedding 在连续 Transformer 块之间的欧几里得距离来量化这种变化。距离越小，表示特征演化越稳定，对应图像质量越高。</li>
<li><strong>单次前向传播与无需反向传播：</strong> 与现有训练无关方法不同，ViTNT-FIQA <strong>仅需一次前向传播</strong>，无需反向传播或对预训练 ViT 模型进行任何架构修改或微调，极大地提高了效率和易用性。</li>
<li><strong>多层级特征聚合：</strong> 方法将计算得到的 patch 级质量分数通过两种方式聚合为图像级分数：一种是简单的均匀聚合，另一种是利用最后一个 Transformer 块的自注意力机制来加权聚合，以捕捉人脸中更重要的区域。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong></p>
<ul>
<li><strong>实证验证：</strong> 通过在 SynFIQA 数据集上的实验，作者们验证了高质量人脸图像的 patch embedding 距离确实随着图像质量的提升而系统性地减小，证明了 patch embedding 稳定性的有效性。</li>
<li><strong>广泛的基准测试：</strong> 在 LFW, AgeDB-30, CFP-FP, CALFW, Adience, CPLFW, XQLFW, IJB-C 等八个基准数据集上的广泛评估表明，ViTNT-FIQA 取得了与最先进（SOTA）方法<strong>具有竞争力的性能</strong>。</li>
<li><strong>高效与通用性：</strong> ViTNT-FIQA 的主要优势在于其<strong>计算效率高</strong>（单次前向传播）且<strong>即时可用</strong>于任何预训练的 ViT 模型，无需额外的训练或微调。这使其在实际应用中具有显著的优势。</li>
<li><strong>对 ViT 内部机制的洞察：</strong> 研究揭示了 ViT 中间层特征中蕴含着丰富的质量信息，这为理解和利用 Transformer 模型进行图像质量评估提供了新的视角。</li>
</ul>
<p><strong>4. 局限性：</strong></p>
<ul>
<li><strong>对预训练 ViT 模型的依赖：</strong> 该方法依赖于预训练的 ViT 模型，其性能会受到预训练模型质量和训练数据的影响。虽然在 CLIP 等非 FR 专用模型上也能工作，但性能有所下降，表明 FR 专用训练的模型效果更好。</li>
<li><strong>对特定 Transformer 块的选择：</strong> 虽然研究表明早期块（0-5）捕捉了大部分质量信息，但最佳的块数量和选择仍需通过消融实验来确定，这可能需要一定的调优。</li>
<li><strong>对特定退化因素的敏感性：</strong> 论文主要关注了模糊、遮挡等常见退化因素，但对于其他类型的图像质量问题（如伪造、合成等）的鲁棒性可能需要进一步验证。</li>
</ul>
<p><strong>5. 未来研究方向：</strong></p>
<ul>
<li><strong>探索更广泛的 ViT 架构：</strong> 将 ViTNT-FIQA 应用于不同大小、不同变体的 ViT 模型，以及其他基于 Transformer 的模型，以验证其通用性。</li>
<li><strong>更精细的注意力机制应用：</strong> 进一步研究如何更有效地利用 ViT 的自注意力机制来聚合 patch 级质量信息，以捕捉更细粒度的质量特征。</li>
<li><strong>与其他质量评估方法的融合：</strong> 探索将 ViTNT-FIQA 的方法与传统的图像质量评估（IQA）或更复杂的 FIQA 方法相结合，以期获得更全面的质量评估能力。</li>
<li><strong>对特定退化因素的鲁棒性研究：</strong> 深入分析 ViTNT-FIQA 在面对各种特定图像退化（如低光照、极端姿态、合成伪造等）时的表现，并探索提升其鲁棒性的方法。</li>
</ul>
<hr />
<p>总而言之，ViTNT-FIQA 是一项重要的研究成果，它巧妙地利用了 Vision Transformer 模型内部的特征演化稳定性来评估人脸图像质量，实现了高效、训练无关且即插即用的解决方案，为该领域的研究和应用开辟了新的途径。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose ViTNT-FIQA, a training-free approach that measures the stability of patch embedding evolution across intermediate Vision Transformer (ViT) blocks.</li>
<li>We demonstrate that high-quality face images exhibit stable feature refinement trajectories across blocks, while degraded images show erratic transformations.</li>
<li>Our method computes Euclidean distances between L2-normalized patch embeddings from consecutive transformer blocks and aggregates them into image-level quality scores.</li>
<li>Through extensive evaluation on eight benchmarks (LFW, AgeDB-30, CFP-FP, CALFW, Adience, CPLFW, XQLFW, IJB-C), we show that ViTNT-FIQA achieves competitive performance with state-of-the-art methods while maintaining computational efficiency and immediate applicability to any pre-trained ViT-based face recognition model.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2601.05741v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2601.05741v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2026-01-12 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
