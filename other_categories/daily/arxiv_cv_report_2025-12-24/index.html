<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Arxiv Computer Vision Papers - 2025-12-24 - Reading Collections</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-158625144-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-158625144-1');
</script>
        <!-- --> 
    </head>

    <body>

        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Reading Collections</a>
                </div>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Others <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          2D Object Detection
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../object_detection_2D/AP_loss/" class="dropdown-item">
            AP-Loss for Accurate One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Associative_Embedding%3AEnd-to-End_Learning_for_Joint_Detection_and_Grouping/" class="dropdown-item">
            Associative Embedding
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/BoFDetection/" class="dropdown-item">
            BoF for object detection (tricks)
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CPN/" class="dropdown-item">
            Cornet Proposal Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CenterNet%3A_Keypoint_Triplets_for_Object_Detection/" class="dropdown-item">
            CenterNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet-Lite_Efficient_Keypoint_Based_Object_Detection/" class="dropdown-item">
            CornetNet-Lite
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/CornerNet_Detecting_Objects_as_Paired_Keypoints/" class="dropdown-item">
            CornetNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DIoULoss/" class="dropdown-item">
            Distance-IoU Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DRN_and_SKU110K-R/" class="dropdown-item">
            Dynamic Refinement Network
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/DynamicRCNN/" class="dropdown-item">
            Dynamic RCNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/EfficientDet/" class="dropdown-item">
            EfficientDet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/FCOS/" class="dropdown-item">
            FCOS, anchorless one-stage object detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/GFocalLoss/" class="dropdown-item">
            Generalized Focal Loss
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Gaussian_YOLOv3%3A_An_Accurate_and_Fast_Object_Detector_Using_Localization_Uncertainty_for_Autonomous_Driving/" class="dropdown-item">
            Gaussian YOLOv3
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoU-uniform_R-CNN/" class="dropdown-item">
            IoU-uniform R-CNN
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/IoUNet%28s%29/" class="dropdown-item">
            IoU Net(s) - A summary
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/MMDetection/" class="dropdown-item">
            Some Collections around MMDetection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/NGA/" class="dropdown-item">
            Exploiting Event Cameras by Using a Network Grafting Algorithm
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Object_as_points/" class="dropdown-item">
            Detection and Tracking as Point
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/PAA/" class="dropdown-item">
            Probabilistic Anchor Assignment with IoU Prediction for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/RVT/" class="dropdown-item">
            Recurrent Vision Transformers for Object Detection with Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/Reppoints/" class="dropdown-item">
            The RepPoints Series
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/SSD/" class="dropdown-item">
            SSD
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/ThunderNet%3A_Towards_Real-time_Generic_Object_Detection_on_Mobile_Devices/" class="dropdown-item">
            ThunderNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/YOLOv4/" class="dropdown-item">
            YOLOv4
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/aLRPLoss/" class="dropdown-item">
            A Ranking-based, Balanced Loss Function Unifying Classification and Localisation in Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/anchordetr/" class="dropdown-item">
            Anchor DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/balanced_oriented_focal_loss/" class="dropdown-item">
            Balance-Oriented Focal Loss with Linear Scheduling for Anchor Free Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/deformable_detr/" class="dropdown-item">
            Deformable DETR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/detr/" class="dropdown-item">
            DE⫶TR
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/implicit_FPN/" class="dropdown-item">
            Implicit Feature Pyramid Network for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/onenet/" class="dropdown-item">
            OneNet: Towards End-to-End One-Stage Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/pix2seq/" class="dropdown-item">
            PIX2SEQ: A Language Modeling Framework for Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/polarnet/" class="dropdown-item">
            PolarNet
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/slender_object_detection/" class="dropdown-item">
            Slender Object Detection: Diagnoses and Improvements
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/toward_open_world_detection/" class="dropdown-item">
            Towards Open World Object Detection
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/uncertainty_detector/" class="dropdown-item">
            Estimating and Evaluating Regression Predictive Uncertainty in Deep Object Detectors
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolof/" class="dropdown-item">
            You Only Look One-level Feature
        </a>
    </li>
              
    <li>
        <a href="../../object_detection_2D/yolox/" class="dropdown-item">
            YOLOX: Exceeding YOLO Series in 2021
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Deep Navigation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Deep-Navigation/ChauffeurNet%3A%20Learning%20to%20Drive%20by%20Imitating%20the%20Best%20and%20Synthesizing%20the%20Worst/" class="dropdown-item">
            ChauffeurNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DriftingCaiRal/" class="dropdown-item">
            Drifting with RL (Cai, Mei)
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/DroNet%20Learning%20to%20Fly%20by%20Driving/" class="dropdown-item">
            DroNet
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/End-to-end%20Driving%20Deploying%20through%20Uncertainty-Aware%20ImitationLearning%20and%20Stochastic%20Visual%20Domain%20Adaptation/" class="dropdown-item">
            Domain Transfer Imitation - Tai Lei
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Gaze%20%20Training%20%20by%20%20Modulated%20%20Dropout%20%20Improves%20%20Imitation%20%20Learning/" class="dropdown-item">
            Gaze Training - Chen Yuying
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/ModEL/" class="dropdown-item">
            ModEL: A Modularized End-to-end Reinforcement Learning Framework for Autonomous Driving
        </a>
    </li>
              
    <li>
        <a href="../../Deep-Navigation/Robustness_to_Out-of-Distribution_Inputs_via_Task-Aware_Generative_Uncertainty/" class="dropdown-item">
            Task-Aware Generative Uncertainty
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Segmentation
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Segmentation/Actor-Critic%20Instance%20Segmentation/" class="dropdown-item">
            Actor-Critic Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/BEV_segmentation/" class="dropdown-item">
            Summary of Multiple Papers on BEV Fusion
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/CONVOLUTIONAL%20CRFS%20%20FOR%20SEMANTIC%20SEGMENTATION/" class="dropdown-item">
            Convolutional CRF
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Deep%20Multi-Sensor%20Lane%20Detection/" class="dropdown-item">
            Deep Multi-Sensor Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/DeepSnake/" class="dropdown-item">
            Deep Snake
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/FCN_panoptic_seg/" class="dropdown-item">
            Fully Convolutional Networks for Panoptic Segmentation (Panoptic FCN)
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/LRNNET/" class="dropdown-item">
            LRNNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Lane%20Detection%20and%20Classification%20usingCascaded%20CNNs/" class="dropdown-item">
            Cascade Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PointRend/" class="dropdown-item">
            PointRend
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/PolorMask/" class="dropdown-item">
            PolarMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/RDSNet/" class="dropdown-item">
            RDSNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SAUNet/" class="dropdown-item">
            SAUNet
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/SOLO/" class="dropdown-item">
            SOLO for instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TPV/" class="dropdown-item">
            Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/TensorMask%3A_A_Foundation_for_Dense_Object_Segmentation/" class="dropdown-item">
            TensorMask-Instance Seg
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/Ultra_Fast_Structure-aware_Deep_Lane_Detection/" class="dropdown-item">
            Ultra Fast Structure-aware Deep Lane Detection
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/YOLACT/" class="dropdown-item">
            YOLOACT
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/blenderMask/" class="dropdown-item">
            BlendMask
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/condInst/" class="dropdown-item">
            Conditional Convolutions for Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/focus_on_local/" class="dropdown-item">
            Focus on Local: Detecting Lane Marker from Bottom Up via Key Point
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/freeSOLO/" class="dropdown-item">
            FreeSOLO
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/hdmapnet/" class="dropdown-item">
            HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/lift_splat_shoot/" class="dropdown-item">
            Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/mmsegmentation/" class="dropdown-item">
            MMSegmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/nvidia-multi-scale-seg/" class="dropdown-item">
            Hierarchical Multi-scale Attention for Semantic Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/openOccupancy/" class="dropdown-item">
            OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy Perception
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/panopticBEV/" class="dropdown-item">
            PanopticBEV
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/pyorcc/" class="dropdown-item">
            PyrOccNet for BEV Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/skyeye/" class="dropdown-item">
            SkyEye: Self-Supervised Bird’s-Eye-View Semantic Mapping Using Monocular Frontal View Images
        </a>
    </li>
              
    <li>
        <a href="../../Segmentation/uniocc/" class="dropdown-item">
            UniOcc: Unifying Vision-Centric 3D Occupancy Prediction with Geometric and Semantic Rendering
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          SLAM
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../SLAM/%20interval_based_Visual-LiDAR_Sensor-Fusion/" class="dropdown-item">
            Interval-Based Visual-LiDAR Sensor Fusion
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/BAD-SLAM/" class="dropdown-item">
            BAD SLAM: Bundle Adjusted Direct RGB-D SLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/CubeSLAM/" class="dropdown-item">
            CubeSLAM
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DISK/" class="dropdown-item">
            DISK: Learning local features with policy gradient
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/DeepICP%3A_An_End-to-End_Deep_Neural_Network_for_3D_Point_Cloud_Registration/" class="dropdown-item">
            DeepICP
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Joint_Unsupervised_Learning_of_Optical_Flow_and_Depth_by_Watching_Stereo_Videos/" class="dropdown-item">
            UnDepthFlow
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/LO-Net%3A%20Deep%20Real-time%20Lidar%20Odometry/" class="dropdown-item">
            LO-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/PWC-Net_CNNs_for_Optical_Flow_Using_Pyramid_Warping_and_Cost_Volume/" class="dropdown-item">
            PWC-Net
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/SuperPoint%3ASelf-Supervised_Interest_Point_Detection_and_Description/" class="dropdown-item">
            SuperPoint
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/Unsupervised_Learning_of_Depth_and_Ego-Motion_from_Video/" class="dropdown-item">
            Unsupervised Learning of Depth and Ego-Motion from Video
        </a>
    </li>
              
    <li>
        <a href="../../SLAM/VOLDOR/" class="dropdown-item">
            VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Summaries
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../Summaries/Collections_StereoMatching_KITTI/" class="dropdown-item">
            Collections of Stereo Matching from KITTI
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/MachineLearningForRobotPlanningAndContrlGeorgiaTech/" class="dropdown-item">
            Robotics_DL Lecture from georgia Tech
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SelfAttentionandCNN/" class="dropdown-item">
            Self-Attention & CNN
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfMono3DDetection_in2019/" class="dropdown-item">
            Summary of Mono 3D Detection in 2019
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfSingleStageInstanceSeg/" class="dropdown-item">
            Summary of Single Stage Instance Segmentation
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/SummaryOfTemperalBEV/" class="dropdown-item">
            Summary of Temperal BEV
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_CVPR_2021/" class="dropdown-item">
            CVPR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICCV_2021/" class="dropdown-item">
            ICCV 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICLR_2021/" class="dropdown-item">
            ICLR 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_ICML_2021/" class="dropdown-item">
            ICML 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_MapExtraction/" class="dropdown-item">
            Summary of Several Map Extraction Papers
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2020_app/" class="dropdown-item">
            Summary of NIPS 2020 for application
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_NIPS_2021/" class="dropdown-item">
            NeurIPS 2021 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_ICRA_2020/" class="dropdown-item">
            ICRA 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_SegLoss/" class="dropdown-item">
            Segmentation Loss Odyssey
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_cvpr2020/" class="dropdown-item">
            CVPR 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_serveral_eccv2020/" class="dropdown-item">
            ECCV 2020 clips
        </a>
    </li>
              
    <li>
        <a href="../../Summaries/Summary_of_several_iccv2019/" class="dropdown-item">
            ICCV 2019 Clips
        </a>
    </li>
          
      </ul>
    </li>
                                    
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Daily
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../arxiv_cv_report_2025-08-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-08-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-08-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-09-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-09-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-10-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-10-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-24/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-27/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-27
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-11-28/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-11-28
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-03/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-03
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-04/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-04
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-10/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-10
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-11/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-11
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-17/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-17
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-18/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-18
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-23
        </a>
    </li>
              
    <li>
        <a href="./" class="dropdown-item active">
            Arxiv Computer Vision Papers - 2025-12-24
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-25/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-25
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-26
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-29/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-29
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-30/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-30
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2025-12-31/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2025-12-31
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-01/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-01
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-02/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-02
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-05/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-05
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-06/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-06
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-07/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-07
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-08/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-08
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-09/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-09
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-12/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-12
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-13/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-13
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-14/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-14
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-15/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-15
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-16/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-16
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-19/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-19
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-20/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-20
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-21/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-21
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-22/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-22
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-23/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-23
        </a>
    </li>
              
    <li>
        <a href="../arxiv_cv_report_2026-01-26/" class="dropdown-item">
            Arxiv Computer Vision Papers - 2026-01-26
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Depth Completion & Depth Prediction
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../depth_completion/DNet/" class="dropdown-item">
            DNet for Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/DeepLineEncode/" class="dropdown-item">
            Deep Line Encoding for Monocular 3D Object Detection and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Deterministic_Guided_LiDAR_Depth_Map_Completion/" class="dropdown-item">
            Deterministic Guided LiDAR Depth Map Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/OrdinalRegression/" class="dropdown-item">
            Soft Labels for Ordinal Regression
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/Sparse_and_noisy_LiDAR_completion/" class="dropdown-item">
            LiDAR completion with RGB uncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/advancing_monodepth_splidar/" class="dropdown-item">
            Advancing Self-supervised Monocular Depth Learning with Sparse LiDAR
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/depth_pred_before/" class="dropdown-item">
            Depth Prediction before DL
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/dorn/" class="dropdown-item">
            DORN Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/guideNet/" class="dropdown-item">
            GuideNet
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/image_synthesis_loss/" class="dropdown-item">
            On the Sins of Image Synthesis Loss for Self-supervised Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/manydepth/" class="dropdown-item">
            ManyDepth
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/mono_uncer/" class="dropdown-item">
            MonoUncertainty
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/monorec/" class="dropdown-item">
            MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments from a Single Moving Camera
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/movedepth/" class="dropdown-item">
            Crafting Monocular Cues and Velocity Guidance for Self-Supervised Multi-Frame Depth Learning
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/multi_view_depth_estimation_epipolar_spatio_temporal/" class="dropdown-item">
            Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/pRGB-D_SLAM/" class="dropdown-item">
            Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/steering_kernels/" class="dropdown-item">
            Learning Steering Kernels for Guided Depth Completion
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sub_depth/" class="dropdown-item">
            SUB-Depth: Self-distillation and Uncertainty Boosting Self-supervised Monocular Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/sungoesdown/" class="dropdown-item">
            When the Sun Goes Down: Repairing Photometric Losses for All-Day Depth Estimation
        </a>
    </li>
              
    <li>
        <a href="../../depth_completion/variational_monodepth/" class="dropdown-item">
            Variational Monocular Depth Estimation for Reliability Prediction
        </a>
    </li>
          
      </ul>
    </li>
                                    
    <li class="dropdown-submenu">
      <a href="#" class="dropdown-item">
          Others
      </a>
      <ul class="dropdown-menu">
              
    <li>
        <a href="../../others/Adaptive_Unimodal_Cost_Volume_Filtering_for_Deep_Stereo_Matching/" class="dropdown-item">
            AcfNet
        </a>
    </li>
              
    <li>
        <a href="../../others/Attacking_Optical_Flow/" class="dropdown-item">
            Attacking Optical Flow
        </a>
    </li>
              
    <li>
        <a href="../../others/CAM/" class="dropdown-item">
            CAM: Class Activation Map
        </a>
    </li>
              
    <li>
        <a href="../../others/Continuous-time_Intensity_Estimation_Using_Event_Cameras/" class="dropdown-item">
            Intensity Estimation With Event Cameras
        </a>
    </li>
              
    <li>
        <a href="../../others/FADNet/" class="dropdown-item">
            FADNet
        </a>
    </li>
              
    <li>
        <a href="../../others/GPLVM/" class="dropdown-item">
            GPLVM
        </a>
    </li>
              
    <li>
        <a href="../../others/Gated2Depth%3A_Real-Time_Dense_Lidar_From_Gated_Images/" class="dropdown-item">
            Gated2Depth
        </a>
    </li>
              
    <li>
        <a href="../../others/GaussianProcessVAE/" class="dropdown-item">
            Gaussian Process and Variantional Autoencoder
        </a>
    </li>
              
    <li>
        <a href="../../others/Hyperparameter_tuning/" class="dropdown-item">
            Hyperparameter Tuning
        </a>
    </li>
              
    <li>
        <a href="../../others/In_Defense_of_Classical_Image_Processing_Fast_Depth_Completion_on_the_CPU/" class="dropdown-item">
            Depth Completion on CPU
        </a>
    </li>
              
    <li>
        <a href="../../others/Jointly_derain_dehaze/" class="dropdown-item">
            Joint Deraining and Dehazing
        </a>
    </li>
              
    <li>
        <a href="../../others/OptNet_Differentiable_Optimization_as_a_Layer_in_Neural_Networks/" class="dropdown-item">
            OptNet, Optimization as Layer
        </a>
    </li>
              
    <li>
        <a href="../../others/PHOTOREALISTIC%20IMAGE%20RECONSTRUCTION%20FROM_HYBRID_INTENSITY_AND_EVENT_BASED_SENSOR/" class="dropdown-item">
            Image Reconstruction with Event Camera
        </a>
    </li>
              
    <li>
        <a href="../../others/PSMNet/" class="dropdown-item">
            PSMNet
        </a>
    </li>
              
    <li>
        <a href="../../others/PointFlow%20%3A%203D%20Point%20Cloud%20Generation%20with%20Continuous%20Normalizing%20Flows/" class="dropdown-item">
            PointFlow
        </a>
    </li>
              
    <li>
        <a href="../../others/R2D2/" class="dropdown-item">
            R2D2
        </a>
    </li>
              
    <li>
        <a href="../../others/SPM_SPR/" class="dropdown-item">
            SPM - SPR
        </a>
    </li>
              
    <li>
        <a href="../../others/SomePapersOnDifferentiableCvxOpt/" class="dropdown-item">
            More on Differentiable Convex Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/TRPO/" class="dropdown-item">
            Trust Region Policy Optimization
        </a>
    </li>
              
    <li>
        <a href="../../others/Unsupervised_depth_prediction/" class="dropdown-item">
            Unsupervised Mono Depth from stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/adversarialPatch/" class="dropdown-item">
            Adversarial Patch
        </a>
    </li>
              
    <li>
        <a href="../../others/detic/" class="dropdown-item">
            Detecting Twenty-thousand Classes using Image-level Supervision
        </a>
    </li>
              
    <li>
        <a href="../../others/flownet/" class="dropdown-item">
            FlowNet & More
        </a>
    </li>
              
    <li>
        <a href="../../others/gaussian_splatting/" class="dropdown-item">
            Gaussian Splatting
        </a>
    </li>
              
    <li>
        <a href="../../others/generate_model_score_matching/" class="dropdown-item">
            Generative Modeling by Estimating Gradients of the Data Distribution
        </a>
    </li>
              
    <li>
        <a href="../../others/meta_updater_tracking/" class="dropdown-item">
            High-Performance Long-Term Tracking with Meta-Updater
        </a>
    </li>
              
    <li>
        <a href="../../others/mixmatch/" class="dropdown-item">
            MixMatch: A Holistic Approach to Semi-Supervised Learning
        </a>
    </li>
              
    <li>
        <a href="../../others/monodepth_collections/" class="dropdown-item">
            Collections on Monodepth (unsupervised)
        </a>
    </li>
              
    <li>
        <a href="../../others/monolayout/" class="dropdown-item">
            MonoLayout
        </a>
    </li>
              
    <li>
        <a href="../../others/objectvivit/" class="dropdown-item">
            How can objects help action recognition?
        </a>
    </li>
              
    <li>
        <a href="../../others/octsqueeze/" class="dropdown-item">
            OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression
        </a>
    </li>
              
    <li>
        <a href="../../others/openpose_part_afinity_fileds/" class="dropdown-item">
            OpenPose: Part Affinity Fields
        </a>
    </li>
              
    <li>
        <a href="../../others/patchmatchnet/" class="dropdown-item">
            PatchmatchNet: Learned Multi-View Patchmatch Stereo
        </a>
    </li>
              
    <li>
        <a href="../../others/pruning/" class="dropdown-item">
            Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning
        </a>
    </li>
              
    <li>
        <a href="../../others/self_supervised_stereo/" class="dropdown-item">
            SsSMnet
        </a>
    </li>
              
    <li>
        <a href="../../others/socialgan/" class="dropdown-item">
            Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks
        </a>
    </li>
              
    <li>
        <a href="../../others/space_sweep/" class="dropdown-item">
            Plane sweeping for multi-image matching
        </a>
    </li>
              
    <li>
        <a href="../../others/surround_depth/" class="dropdown-item">
            About Surround Monodepth
        </a>
    </li>
              
    <li>
        <a href="../../others/track2detection_and_seg/" class="dropdown-item">
            TraDeS
        </a>
    </li>
              
    <li>
        <a href="../../others/vectornet/" class="dropdown-item">
            VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation
        </a>
    </li>
              
    <li>
        <a href="../../others/vip_deeplab/" class="dropdown-item">
            ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation
        </a>
    </li>
          
      </ul>
    </li>
                                </ul>
                            </li>
                            <!-- <li class="navitem">
                                <a href="../../.." class="nav-link">论文阅读</a>
                            </li> -->
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">3dDetection <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../3dDetection/AFDet/" class="dropdown-item">
            CenterNet for Point cloud
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/AM3D/" class="dropdown-item">
            Color-Embedded 3D Reconstructio Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CDN/" class="dropdown-item">
            Wasserstein Distances for Stereo Disparity Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/CameraDistanceAware/" class="dropdown-item">
            camera distance-aware Mono 3D pose estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/DSGN/" class="dropdown-item">
            DSGN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Disentangling_Monocular_3D_Object_Detection/" class="dropdown-item">
            MonoDIS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/EGFN/" class="dropdown-item">
            EGFN: Efficient Geometry Feature Network for Fast Stereo 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/End-to-end%20Learning%20of%20Multi-sensor%203D%20Tracking%20by%20Detection/" class="dropdown-item">
            End-to-end Learning of Multi-sensor 3D Tracking by Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/FCOS3D_mono/" class="dropdown-item">
            FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Fast%20and%20Furious%20Real%20Time%20End-to-End%203D%20Detection%2C%20Tracking%20and%20MotionForecasting%20with%20a%20Single%20Convolutional%20Net/" class="dropdown-item">
            Fast and Furious
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Frustum_PointNets_for_3D_Object_Detection_from_RGB-D_Data/" class="dropdown-item">
            Frustum PointNets
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Generalize3DDet/" class="dropdown-item">
            Generalization for 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GeneralizedIoU/" class="dropdown-item">
            Genearlized IoU 2D and 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/GroundAwareConvultion/" class="dropdown-item">
            Ground-aware Monocular 3D Object Detection for Autonomous Driving
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/H23D_RCNN/" class="dropdown-item">
            From Multi-View to Hollow-3D: Hallucinated Hollow-3D R-CNN for 3D Object Detection: (H23D-RCNN)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Improving%203D%20Object%20Detection%20for%20Pedestrians%20with%20Virtual%20Multi-View%20Synthesis%20Orientation%20Estimation/" class="dropdown-item">
            Multi-View Synthesis for Orientation Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/IoU%20Loss%20for%202D/" class="dropdown-item">
            IoU Loss for 2D/3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Kinematic_video3d/" class="dropdown-item">
            Kinematic 3D Object Detection in Monocular Video
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/LaserNet/" class="dropdown-item">
            LaserNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/M3D-RPN_Monocular_3D_Region_Proposal_Network_for_Object_Detection/" class="dropdown-item">
            M3D-RPN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Metric_3d/" class="dropdown-item">
            3D detection evaluation metric
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Mono3D_virtualcam/" class="dropdown-item">
            Mono3d with virtual cameras
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/MonoGRNet%3A%20A%20Geometric%20Reasoning%20Network%20for%20Monocular%203D%20Object%20Localization/" class="dropdown-item">
            MonoGRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Monocular_3D_Object_Detection_and_Box_Fitting_Trained_End-to-End_Using_Intersection-over-Union_Loss/" class="dropdown-item">
            Single Shot Mono 3D(SS3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Multi-View_3D_Detection_Network_for_autonomous_Driving/" class="dropdown-item">
            MV3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Orthographic_Feature_Transform_3D_detection/" class="dropdown-item">
            OftNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Pseudo-Lidar/" class="dropdown-item">
            Pseudo-Lidar with Instance Segmentation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForMono3D/" class="dropdown-item">
            Recent Collections for Mono3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RecentCollectionForStereo3D/" class="dropdown-item">
            Recent Collections for Stereo 3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/RefinedMPL/" class="dropdown-item">
            RefinedMPL
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SHIFT%20R-CNN%3A%20DEEP%20MONOCULAR%203D%20OBJECT%20DETECTION%20WITH%20CLOSED-FORM%20GEOMETRIC%20CONSTRAINTS/" class="dropdown-item">
            Shift RCNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SSL_RTM3D/" class="dropdown-item">
            SSL-RTM3D
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/SingleStage3DPoseEstimation/" class="dropdown-item">
            single-stage 3D Pose Estimation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/Triangulation%20Learning%20Network%3A%20from%20Monocular%20to%20Stereo%203D%20Object%20Detection/" class="dropdown-item">
            TLNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/VoteNetImVote/" class="dropdown-item">
            VoteNet & ImVoteNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/YOLOStereo3D/" class="dropdown-item">
            YOLOStereo3D: A Step Back to 2D for Efficient Stereo 3D Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/caddn/" class="dropdown-item">
            CaDDN
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/digmono3d/" class="dropdown-item">
            Digging Into Output Representation For Monocular 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/is_plidar_needed/" class="dropdown-item">
            Is Pseudo-Lidar needed for Monocular 3D Object detection? (DD3D)
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/mono_differentiable_rendering/" class="dropdown-item">
            Monocular Differentiable Rendering for Self-Supervised 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/monorun/" class="dropdown-item">
            MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/multisensor_refinement/" class="dropdown-item">
            Multi-Sensor Refinement - Li Peiliang
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/my_cookbook/" class="dropdown-item">
            Synthetic Cookbook for Using/Testing/Demonstrating VisualDet3D in ROS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pointnet_related/" class="dropdown-item">
            Collections on PointNet and follow-ups
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pvnet/" class="dropdown-item">
            PVNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/pyramid-rcnn/" class="dropdown-item">
            Pyramid R-CNN: Towards Better Performance and Adaptability for 3D Object Detection
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/renderocc/" class="dropdown-item">
            RenderOcc
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/siaNMS/" class="dropdown-item">
            siaNMS
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/voxelNet/" class="dropdown-item">
            VoxelNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../3dDetection/weakly_super/" class="dropdown-item">
            Weakly Supervised 3D Object Detection from Point Clouds
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Building Blocks <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Building_Blocks/4D%20Spatio-Temporal%20ConvNets%3A%20Minkowski%20Convolutional%20Neural%20Networks/" class="dropdown-item">
            Minkowski Convolutional Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ASL_loss/" class="dropdown-item">
            Asymmetric Loss For Multi-Label Classification (ASL Loss)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AUGMIX/" class="dropdown-item">
            AugMix
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/AdaIN/" class="dropdown-item">
            AdaIn Style Transfer
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_Augmented_Conv/" class="dropdown-item">
            Attention Augmented Convolution
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Attention_is_all_you_need/" class="dropdown-item">
            Self-Attention Mechanism
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Bpnp/" class="dropdown-item">
            backprob through PnP Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/CBAM%3AConvolutional_Block_Attention_Module/" class="dropdown-item">
            CBAM: Convolutional Block Attention Module
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Can%20GCNs%20Go%20as%20Deep%20as%20CNNs/" class="dropdown-item">
            Deep GCN
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Container_context_aggregation_network/" class="dropdown-item">
            Container: Context Aggregation Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DGCNN/" class="dropdown-item">
            DGCNN and edgeConv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DO-Conv/" class="dropdown-item">
            DO-Conv
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DiCENet/" class="dropdown-item">
            DiCENet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Discovery_of_Latent_3D_Keypoints_via_End-to-end_Geometric_Reasoning/" class="dropdown-item">
            keypointNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynamicFilteringNetwork_Fewshot/" class="dropdown-item">
            Dynamic Conditional Networks for Few-Shot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/DynanicFilteringNetwork/" class="dropdown-item">
            Dynamic Filtering Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/ELASTIC/" class="dropdown-item">
            Elastic: Dynamic Scaling CNNs
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/EfficientNet%3A_Rethinking_Model_Scaling_for_Convolutional_Neural_Network/" class="dropdown-item">
            EfficientNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GhostNet/" class="dropdown-item">
            Ghost Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/GumbelSoftmax/" class="dropdown-item">
            Gumbel_softmax; Differentiable Indexing
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/HRNet/" class="dropdown-item">
            HRNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Lookahead%20Optimizer%20ksteps%20forward%2C%201%20step%20back/" class="dropdown-item">
            New Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MDEQ/" class="dropdown-item">
            MDEQ
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/MLP_image_classification/" class="dropdown-item">
            MLP in Image Classification
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/NRI/" class="dropdown-item">
            Neural Relational Inference Model (NRI)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Non-local_Neural_Networks/" class="dropdown-item">
            Non-local Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/On_Multiplicative_Integration_with_Recurrent_Neural_Networks/" class="dropdown-item">
            On Multiplicative Integration with Recurrent Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PointAtrousNet/" class="dropdown-item">
            PointAtrousNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/PositionalNorm/" class="dropdown-item">
            Positional Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/RepVGG/" class="dropdown-item">
            RepVGG
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SPN_CSPN/" class="dropdown-item">
            SPN, CSPN and CSPN ++
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/SQUEEZENET_ALEXNET-LEVEL_ACCURACY_WITH_50X_FEWER_PARAMETERS_AND_0.5MB_MODEL_SIZE/" class="dropdown-item">
            SqueezeNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Songtao_Liu_Receptive_Field_Block_ECCV_2018_paper/" class="dropdown-item">
            Receptive Field Block
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Squeeze-and-Excitation_Networks/" class="dropdown-item">
            Squeeze-and-Excitation Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/Stacked_Hourglass_Networks_for_Human_Pose_Estimation/" class="dropdown-item">
            Stacked Hourglass Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/convNext/" class="dropdown-item">
            A ConvNet for the 2020s (ConvNeXt)
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/crossBatchNormalization/" class="dropdown-item">
            Cross-iteration BatchNormalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deepPruner/" class="dropdown-item">
            Deep Pruner & Differentiable Patch Match
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/deformable_convnet_v2/" class="dropdown-item">
            Deformable ConvNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/fcanet/" class="dropdown-item">
            FcaNet: Frequency Channel Attention Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/gabor_layer/" class="dropdown-item">
            Gabor Layers
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/involution/" class="dropdown-item">
            Involution: Inverting the Inherence of Convolution for Visual Recognition
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mpvit/" class="dropdown-item">
            MonoViT
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/mutualNet/" class="dropdown-item">
            MutualNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/omnivore/" class="dropdown-item">
            OMNIVORE: A Single Model for Many Visual Modalities
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/pyramid_vit/" class="dropdown-item">
            Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/replknet/" class="dropdown-item">
            RepLKNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/shape_adaptor/" class="dropdown-item">
            Shape Adaptor
        </a>
    </li>
                                    
    <li>
        <a href="../../../Building_Blocks/swinV2/" class="dropdown-item">
            Swin Transformer V2: Scaling Up Capacity and Resolution
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Planning Control DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../Planning_Control_DL/Aggressive_Driving_with_Model_Predictive_Path_Integral_Control/" class="dropdown-item">
            Model Predicvtive Path Integral Control
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Backprop%20KF%20Learning%20Discriminative%20DeterministicState%20Estimators/" class="dropdown-item">
            BackProp Kalman Filter
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Cognitive%20Mapping%20and%20Planning%20for%20Visual%20Navigation/" class="dropdown-item">
            Cognitive Mapping and Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Composable_Action-Conditioned_Predictors_Flexible_Off-Policy_Learning_for_Robot_Navigation/" class="dropdown-item">
            Composable Action-Conditioned Predictors
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/DESPOT-%CE%B1%3A%20Online%20POMDP%20Planning%20With%20Large%20State%20And%20Observation%20Spaces/" class="dropdown-item">
            DESPOT-α
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20Algorithm%20Networks%20forComposable%20Robot%20Learning/" class="dropdown-item">
            DAN for Composable Robot Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Differentiable%20MPC%20for%20End-to-end%20Planning%20and%20Control/" class="dropdown-item">
            Differentiable MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Diffusion_planner/" class="dropdown-item">
            Diffusion Planner
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/EUDM/" class="dropdown-item">
            EUDM Planning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Hierarchical_Imitation_and_Reinforcement_Learning/" class="dropdown-item">
            Hierarchical Imitation and Reinforcement Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Intention-Net%20Integrating%20Planning%20and%20DeepLearning%20for%20Goal-Directed%20Autonomous%20Navigation/" class="dropdown-item">
            Intention-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Learning-based_MPC/" class="dropdown-item">
            LMPC_GP
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/PMPNet/" class="dropdown-item">
            PMPNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Path%20Integral%20Networks_End-to-End%20Differentiable%20Optimal%20Control/" class="dropdown-item">
            Path Integral Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/QMDP-Net_%20Deep%20Learning%20for%20Planning%20underPartial%20Observability/" class="dropdown-item">
            QMDP-Net
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/SUNRISE_for_Ensemble_RL/" class="dropdown-item">
            SUNRISE
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/Universal%20Planning%20Networks/" class="dropdown-item">
            Universal Planning Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/diff_MPC_ARE/" class="dropdown-item">
            differentiable pre-stablized MPC
        </a>
    </li>
                                    
    <li>
        <a href="../../../Planning_Control_DL/end2end_challenge/" class="dropdown-item">
            End-to-end Autonomous Driving: Challenges and Frontiers
        </a>
    </li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Theorotical DL <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
    <li>
        <a href="../../../The_theory/CMA_ES/" class="dropdown-item">
            Covariance matrix adaptation evolution strategy: CMA-ES
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CNN_position_information/" class="dropdown-item">
            Position Information in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Channel_Pruning_for_Accelerating_Very_Deep_Neural_Networks/" class="dropdown-item">
            Channel Pruning for Accelerating Very Deep Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/CheckerBoardDeconv/" class="dropdown-item">
            Deconvolution and Checkerboard Artifacts
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ClosingGap/" class="dropdown-item">
            Train longer, generalize better: closing the generalization gap in large batch training of neural networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Continuous_learning/" class="dropdown-item">
            Continuous Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ConvexOptimization/" class="dropdown-item">
            Convex Optimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Designing_Network_Design_Spaces/" class="dropdown-item">
            Designing Network Design Spaces
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/DirectLossMinimization/" class="dropdown-item">
            Direct Loss Minimization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Do%20Better%20ImageNet%20Models%20Transfer%20Better/" class="dropdown-item">
            Do Better ImageNet Models Transfer Better?
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/First_order_methods_review_and_updates/" class="dropdown-item">
            First Order Optimizers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Framework_Uncertainty_Propagation/" class="dropdown-item">
            Uncertainty Propagation in Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/FreeAnchor_Learning_to_Match_Anchors_for_Visual_Object_Detection/" class="dropdown-item">
            FreeAnchor
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/LQF/" class="dropdown-item">
            LQF: Linear Quadratic Fine-Tuning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Localization-aware%20Channel%20Pruning%20for%20Object%20Detection/" class="dropdown-item">
            Localization-aware Channel Pruning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NIPS2020_networks/" class="dropdown-item">
            NIPS 2020 for Experimental NN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/NTK/" class="dropdown-item">
            Neural Tangent Kernel: Convergence and Generalization in Neural Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Rethinking%20ImageNet%20Pre-training/" class="dropdown-item">
            Rethinking ImageNet Pre-training
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ShuffleNet_V2%3A_Practical_Guidelines_for_Efficient_CNN_Architecture_Design/" class="dropdown-item">
            ShuffleNet V2
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Style_and_normalization/" class="dropdown-item">
            Style and Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/TranslationInvarianceinCNN/" class="dropdown-item">
            Translation invariance in CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Understanding_Deep_Learning_Requires_Rethinking_Generalization/" class="dropdown-item">
            Understanding Deep Learning Requires Rethinking Generalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VAE/" class="dropdown-item">
            Auto-Encoding Variational Bayes
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/VovNet/" class="dropdown-item">
            VovNet
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/Why_gradien_clip_norm/" class="dropdown-item">
            WHY GRADIENT CLIPPING ACCELERATES TRAINING: A THEORETICAL JUSTIFICATION FOR ADAPTIVITY
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/bayesianNetwork/" class="dropdown-item">
            Bayesian Neural Network
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/compondingTechforCNN/" class="dropdown-item">
            Assembled Techniques for CNN
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/ddn/" class="dropdown-item">
            Deep Declarative Networks
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/deeplearning_foundation/" class="dropdown-item">
            Deep Learning "Foundations"
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/diffussion_model/" class="dropdown-item">
            Denoising Diffusion Probabilistic Models
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/dino/" class="dropdown-item">
            Emerging Properties in Self-Supervised Vision Transformers
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/lle/" class="dropdown-item">
            An Introduction to Locally Linear Embedding
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mean_field_theory/" class="dropdown-item">
            Mean Field Theory in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/mindthepad/" class="dropdown-item">
            Mind The Pad - CNNs Can Develop Blind Spots
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/momentum_BN/" class="dropdown-item">
            Momentum Batch Normalization
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/understandingKD/" class="dropdown-item">
            Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning
        </a>
    </li>
                                    
    <li>
        <a href="../../../The_theory/visualizing_landscape/" class="dropdown-item">
            Visualizing the Loss Landscape of Neural Nets
        </a>
    </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../arxiv_cv_report_2025-12-23/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../arxiv_cv_report_2025-12-25/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            
                <div   class=row >
                            <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>
    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        
        <ul class="nav flex-column bs-sidenav">
        

        
            <li class="nav-item main"><a href="#arxiv-computer-vision-papers-2025-12-24">Arxiv Computer Vision Papers - 2025-12-24</a></li>
                <li class="nav-item">
                    <a href="#executive-summary" class="nav-link">Executive Summary</a>
                </li>
                <li class="nav-item">
                    <a href="#table-of-contents" class="nav-link">Table of Contents</a>
                </li>
                <li class="nav-item">
                    <a href="#papers" class="nav-link">Papers</a>
                </li>
                <li class="nav-item">
                    <a href="#semanticgen-video-generation-in-semantic-space" class="nav-link">SemanticGen: Video Generation in Semantic Space</a>
                </li>
                <li class="nav-item">
                    <a href="#longvideoagent-multi-agent-reasoning-with-long-videos" class="nav-link">LongVideoAgent: Multi-Agent Reasoning with Long Videos</a>
                </li>
                <li class="nav-item">
                    <a href="#spatialtree-how-spatial-abilities-branch-out-in-mllms" class="nav-link">SpatialTree: How Spatial Abilities Branch Out in MLLMs</a>
                </li>
                <li class="nav-item">
                    <a href="#repurposing-video-diffusion-transformers-for-robust-point-tracking" class="nav-link">Repurposing Video Diffusion Transformers for Robust Point Tracking</a>
                </li>
                <li class="nav-item">
                    <a href="#cube-bench-a-benchmark-for-spatial-visual-reasoning-in-mllms" class="nav-link">Cube Bench: A Benchmark for Spatial Visual Reasoning in MLLMs</a>
                </li>
                <li class="nav-item">
                    <a href="#lighttact-a-visual-tactile-fingertip-sensor-for-deformation-independent-contact-sensing" class="nav-link">LightTact: A Visual-Tactile Fingertip Sensor for Deformation-Independent Contact Sensing</a>
                </li>
                <li class="nav-item">
                    <a href="#lead-minimizing-learner-expert-asymmetry-in-end-to-end-driving" class="nav-link">LEAD: Minimizing Learner-Expert Asymmetry in End-to-End Driving</a>
                </li>
                <li class="nav-item">
                    <a href="#flashvlm-text-guided-visual-token-selection-for-large-multimodal-models" class="nav-link">FlashVLM: Text-Guided Visual Token Selection for Large Multimodal Models</a>
                </li>
                <li class="nav-item">
                    <a href="#learning-to-reason-in-4d-dynamic-spatial-understanding-for-vision-language-models" class="nav-link">Learning to Reason in 4D: Dynamic Spatial Understanding for Vision Language Models</a>
                </li>
                <li class="nav-item">
                    <a href="#alignpose-generalizable-6d-pose-estimation-via-multi-view-feature-metric-alignment" class="nav-link">AlignPose: Generalizable 6D Pose Estimation via Multi-view Feature-metric Alignment</a>
                </li>
        </ul>
    </div>
</div></div>
                            <div class="col-md-9" role="main"><span>
    


</span>
<h1 id="arxiv-computer-vision-papers-2025-12-24">Arxiv Computer Vision Papers - 2025-12-24</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>好的，作为一名专注于计算机视觉和机器学习的研究助理，我将为您撰写一份简明的 Arxiv 计算机视觉领域论文每日报告执行摘要。</p>
<hr />
<p><strong>Arxiv 计算机视觉领域论文每日报告 - 执行摘要 (2025-12-23)</strong></p>
<p><strong>报告日期：</strong> 2025-12-23
<strong>论文数量：</strong> 10</p>
<p><strong>1. 主要主题与趋势：</strong></p>
<p>本期 Arxiv 论文集中体现了计算机视觉领域在以下几个关键方向的快速进展：</p>
<ul>
<li><strong>多模态大模型 (MLLMs) 的空间与时序理解能力增强：</strong> 多篇论文致力于提升 MLLMs 在理解视频内容、进行时空推理以及处理长视频方面的能力。这包括对模型空间能力的深入分析，以及在动态 4D 环境中进行推理的探索。</li>
<li><strong>视频生成与理解的创新：</strong> 除了对现有视频生成模型（如 Diffusion Transformers）的改进和应用，还有针对视频语义空间生成的新方法出现，预示着更具控制力和理解力的视频生成技术。</li>
<li><strong>机器人感知与交互的进步：</strong> 视觉-触觉融合传感器在实现对变形不敏感的接触感知方面取得了突破，为机器人更精细的操作和交互提供了可能。</li>
<li><strong>自动驾驶领域的端到端学习优化：</strong> 针对端到端驾驶模型中的学习者-专家不对称问题，提出了新的解决方案，旨在提高模型的鲁棒性和性能。</li>
<li><strong>高效视觉表示与推理：</strong> 探索更高效的视觉 token 选择机制，以及利用多视图特征进行可泛化的 6D 姿态估计，都指向了提升模型效率和泛化能力的方向。</li>
</ul>
<p><strong>2. 亮点与创新论文：</strong></p>
<ul>
<li><strong>"SemanticGen: Video Generation in Semantic Space"</strong> 提出了一种在语义空间中进行视频生成的新方法，这可能为生成更具逻辑性和可控性的视频内容打开新的大门。</li>
<li><strong>"LongVideoAgent: Multi-Agent Reasoning with Long Videos"</strong> 解决了长视频理解和多智能体推理的挑战，对于需要处理复杂叙事或多视角信息的应用至关重要。</li>
<li><strong>"SpatialTree: How Spatial Abilities Branch Out in MLLMs"</strong> 和 <strong>"Cube Bench: A Benchmark for Spatial Visual Reasoning in MLLMs"</strong> 共同强调了 MLLMs 在空间推理方面的能力分析和基准测试，预示着对模型空间理解的深入研究将成为热点。</li>
<li><strong>"LightTact: A Visual-Tactile Fingertip Sensor for Deformation-Independent Contact Sensing"</strong> 在机器人感知领域具有重要意义，其对变形不敏感的接触感知能力，对于精细抓取和操作任务是关键的进步。</li>
</ul>
<p><strong>3. 新兴研究方向与技术：</strong></p>
<ul>
<li><strong>基于语义空间的视频生成：</strong> 从传统的像素空间转向语义空间进行视频生成，是视频生成领域的一个新趋势。</li>
<li><strong>长视频的多智能体推理：</strong> 结合多智能体系统和长视频理解，是处理复杂时序信息的新范式。</li>
<li><strong>MLLMs 的细粒度空间能力评估：</strong> 对 MLLMs 的空间推理能力进行更深入的分析和量化，将推动模型在空间理解方面的发展。</li>
<li><strong>视觉-触觉融合的鲁棒感知：</strong> 克服变形对触觉感知的影响，是实现更可靠机器人交互的关键。</li>
<li><strong>端到端学习中的不对称性缓解：</strong> 针对端到端模型训练中的固有挑战，提出更有效的优化策略。</li>
</ul>
<p><strong>4. 建议阅读全文的论文：</strong></p>
<p>考虑到其潜在影响和创新性，以下论文值得深入阅读：</p>
<ul>
<li><strong>"SemanticGen: Video Generation in Semantic Space"</strong>: 探索视频生成的新范式。</li>
<li><strong>"LongVideoAgent: Multi-Agent Reasoning with Long Videos"</strong>: 解决长视频理解和推理的挑战。</li>
<li><strong>"SpatialTree: How Spatial Abilities Branch Out in MLLMs"</strong> 和 <strong>"Cube Bench: A Benchmark for Spatial Visual Reasoning in MLLMs"</strong>: 深入了解 MLLMs 的空间能力及其评估方法。</li>
<li><strong>"LightTact: A Visual-Tactile Fingertip Sensor for Deformation-Independent Contact Sensing"</strong>: 关注机器人感知领域的关键技术突破。</li>
</ul>
<hr />
<p>希望这份执行摘要能帮助您快速掌握本期 Arxiv 论文的重点内容。</p>
<hr />
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#2512.20619v1">SemanticGen: Video Generation in Semantic Space</a></li>
<li><a href="#2512.20618v1">LongVideoAgent: Multi-Agent Reasoning with Long Videos</a></li>
<li><a href="#2512.20617v1">SpatialTree: How Spatial Abilities Branch Out in MLLMs</a></li>
<li><a href="#2512.20606v1">Repurposing Video Diffusion Transformers for Robust Point Tracking</a></li>
<li><a href="#2512.20595v1">Cube Bench: A Benchmark for Spatial Visual Reasoning in MLLMs</a></li>
<li><a href="#2512.20591v1">LightTact: A Visual-Tactile Fingertip Sensor for Deformation-Independent Contact Sensing</a></li>
<li><a href="#2512.20563v1">LEAD: Minimizing Learner-Expert Asymmetry in End-to-End Driving</a></li>
<li><a href="#2512.20561v1">FlashVLM: Text-Guided Visual Token Selection for Large Multimodal Models</a></li>
<li><a href="#2512.20557v1">Learning to Reason in 4D: Dynamic Spatial Understanding for Vision Language Models</a></li>
<li><a href="#2512.20538v1">AlignPose: Generalizable 6D Pose Estimation via Multi-view Feature-metric Alignment</a></li>
</ol>
<hr />
<h2 id="papers">Papers</h2>
<p><a id='2512.20619v1'></a></p>
<h2 id="semanticgen-video-generation-in-semantic-space"><a href="https://arxiv.org/abs/2512.20619v1">SemanticGen: Video Generation in Semantic Space</a></h2>
<p><strong>Authors:</strong> Jianhong Bai, Xiaoshi Wu, Xintao Wang, Fu Xiao, Yuanxing Zhang, Qinghe Wang, Xiaoyu Shi, Menghan Xia, Zuozhu Liu, Haoji Hu, Pengfei Wan, Kun Gai</p>
<p><strong>Published:</strong> 2025-12-23</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>State-of-the-art video generative models typically learn the distribution of video latents in the VAE space and map them to pixels using a VAE decoder. While this approach can generate high-quality videos, it suffers from slow convergence and is computationally expensive when generating long videos. In this paper, we introduce SemanticGen, a novel solution to address these limitations by generating videos in the semantic space. Our main insight is that, due to the inherent redundancy in videos, the generation process should begin in a compact, high-level semantic space for global planning, followed by the addition of high-frequency details, rather than directly modeling a vast set of low-level video tokens using bi-directional attention. SemanticGen adopts a two-stage generation process. In the first stage, a diffusion model generates compact semantic video features, which define the global layout of the video. In the second stage, another diffusion model generates VAE latents conditioned on these semantic features to produce the final output. We observe that generation in the semantic space leads to faster convergence compared to the VAE latent space. Our method is also effective and computationally efficient when extended to long video generation. Extensive experiments demonstrate that SemanticGen produces high-quality videos and outperforms state-of-the-art approaches and strong baselines.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“SemanticGen: Video Generation in Semantic Space”的全面中文摘要，重点突出了其研究问题、创新方法、主要结果、局限性以及未来研究方向：</p>
<p><strong>论文题目：</strong> SemanticGen: Video Generation in Semantic Space</p>
<p><strong>作者：</strong> Jianhong Bai, Xiaoshi Wu, Xintao Wang, Fu Xiao, Yuanxing Zhang, Qinghe Wang, Xiaoyu Shi, Menghan Xia, Zuozhu Liu, Haoji Hu, Pengfei Wan, Kun Gai</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
当前最先进的视频生成模型通常在变分自编码器（VAE）的潜在空间中学习视频的分布，并通过VAE解码器将其映射到像素空间。尽管这种方法可以生成高质量视频，但存在收敛速度慢和生成长视频时计算成本高昂的问题。这是因为直接在低级别的视频标记上进行双向注意力建模，对于长视频来说计算复杂度呈二次方增长，容易导致时间漂移或视觉质量下降。</p>
<p><strong>2. 关键创新/方法贡献：</strong>
为了解决上述问题，本文提出了 <strong>SemanticGen</strong>，一个新颖的视频生成框架。其核心思想是利用视频固有的冗余性，将生成过程首先置于一个紧凑、高层级的 <strong>语义空间</strong> 中进行全局规划，然后再添加高频细节，而不是直接建模大量的低级视频标记。</p>
<p>SemanticGen 采用 <strong>两阶段生成过程</strong>：
*   <strong>第一阶段：</strong> 使用一个扩散模型生成紧凑的 <strong>语义视频特征</strong>，这些特征定义了视频的全局布局。
*   <strong>第二阶段：</strong> 使用另一个扩散模型，以第一阶段生成的语义特征为条件，生成 <strong>VAE 潜在表示</strong>，最终输出视频。</p>
<p>此外，论文还提出了 <strong>语义空间压缩</strong> 的方法，通过一个轻量级的 MLP 来降低语义特征的维度，以实现更有效的训练和采样，并加速收敛。在长视频生成方面，SemanticGen 通过在语义空间中进行全注意力建模，并在映射到 VAE 潜在空间时使用移位窗口注意力（Swin-Attention）来解决计算复杂度问题，从而有效缓解了漂移问题。</p>
<p><strong>3. 主要结果与意义：</strong>
*   <strong>更快的收敛速度：</strong> 在语义空间中进行生成比在 VAE 潜在空间中具有更快的收敛速度，这在实验中得到了验证（如图 9 所示）。
*   <strong>高效的长视频生成：</strong> SemanticGen 能够有效地扩展到长视频生成（长达一分钟），并且在计算效率上表现出色，同时保持了高质量和时间一致性。
*   <strong>高质量视频生成：</strong> 实验结果表明，SemanticGen 能够生成高质量的视频，并且在文本遵循准确性、长期一致性方面优于现有最先进的方法和强基线。
*   <strong>缓解漂移问题：</strong> 对于长视频生成，SemanticGen 显著减轻了时间漂移问题，提高了视频的连贯性。</p>
<p><strong>4. 论文中提到的局限性：</strong>
*   <strong>语义编码器的限制：</strong> SemanticGen 的性能受到其使用的预训练视频理解分词器的限制。例如，在较低的帧率（fps）下进行采样会导致高频时间信息的丢失，从而影响生成视频的细节表现（如闪烁效果）。
*   <strong>纹理和细节的保持：</strong> 在长视频生成中，语义特征可能无法完全保留精细的纹理和细节，导致这些方面的一致性难以完全保持。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   <strong>系统性分析不同语义编码器：</strong> 探索使用不同类型、不同训练范式（如视觉-文本对齐、自监督学习等）的语义编码器对 SemanticGen 性能的影响。
*   <strong>开发更具信息量的视频语义编码器：</strong> 研究能够同时实现高时间压缩和高采样率的编码器，以更好地捕捉高频时间信息，从而提升生成视频的细节表现。
*   <strong>进一步提升长视频的细节一致性：</strong> 探索更有效的方法来保持长视频中精细纹理和微小物体的一致性。</p>
<p>总而言之，SemanticGen 提出了一种创新的视频生成范式，通过引入高层级语义空间作为中间表示，显著提高了视频生成的效率和质量，尤其是在长视频生成方面，为该领域带来了重要的进展。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>State-of-the-art video generative models typically learn the distribution of video latents in the VAE space and map them to pixels using a VAE decoder.</li>
<li>In this paper, we introduce SemanticGen, a novel solution to address these limitations by generating videos in the semantic space.</li>
<li>Our method is also effective and computationally efficient when extended to long video generation.</li>
<li>Extensive experiments demonstrate that SemanticGen produces high-quality videos and outperforms state-of-the-art approaches and strong baselines.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.20619v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.20619v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.20618v1'></a></p>
<h2 id="longvideoagent-multi-agent-reasoning-with-long-videos"><a href="https://arxiv.org/abs/2512.20618v1">LongVideoAgent: Multi-Agent Reasoning with Long Videos</a></h2>
<p><strong>Authors:</strong> Runtao Liu, Ziyi Liu, Jiaqi Tang, Yue Ma, Renjie Pi, Jipeng Zhang, Qifeng Chen</p>
<p><strong>Published:</strong> 2025-12-23</p>
<p><strong>Categories:</strong> cs.AI, cs.CV, cs.LG, cs.MA</p>
<p><strong>Abstract:</strong></p>
<p>Recent advances in multimodal LLMs and systems that use tools for long-video QA point to the promise of reasoning over hour-long episodes. However, many methods still compress content into lossy summaries or rely on limited toolsets, weakening temporal grounding and missing fine-grained cues. We propose a multi-agent framework in which a master LLM coordinates a grounding agent to localize question-relevant segments and a vision agent to extract targeted textual observations. The master agent plans with a step limit, and is trained with reinforcement learning to encourage concise, correct, and efficient multi-agent cooperation. This design helps the master agent focus on relevant clips via grounding, complements subtitles with visual detail, and yields interpretable trajectories. On our proposed LongTVQA and LongTVQA+ which are episode-level datasets aggregated from TVQA/TVQA+, our multi-agent system significantly outperforms strong non-agent baselines. Experiments also show reinforcement learning further strengthens reasoning and planning for the trained agent. Code and data will be shared at https://longvideoagent.github.io/.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“LongVideoAgent: Multi-Agent Reasoning with Long Videos”的全面中文摘要，其中包含了您要求的各项内容：</p>
<p><strong>论文题目：</strong> LongVideoAgent: Multi-Agent Reasoning with Long Videos</p>
<p><strong>作者：</strong> Runtao Liu, Ziyi Liu, Jiaqi Tang, Yue Ma, Renjie Pi, Jipeng Zhang, Qifeng Chen</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong></p>
<p>本文旨在解决长视频问答（Long-Video QA）中的核心挑战：如何有效地理解跨越数小时的视频内容，并从中提取细粒度的信息来回答复杂问题。现有方法普遍存在以下不足：
*   <strong>信息压缩与损失：</strong> 许多方法将长视频压缩成有损摘要，或依赖有限的工具集，导致时间定位不准确，丢失关键细节。
*   <strong>缺乏主动推理：</strong> 现有模型通常被动地处理预编码或降采样的视频，将时间推理的负担过早地转移到早期阶段，且这种压缩往往是不可逆的，难以恢复细粒度证据。
*   <strong>效率与完整性不足：</strong> 缺乏能够同时实现效率、多模态信息完整性和细粒度时间推理的解决方案。</p>
<p><strong>2. 主要创新与方法贡献：</strong></p>
<p>作者提出了一个名为 <strong>LongVideoAgent</strong> 的新颖 <strong>多智能体框架</strong>，用于解决长视频问答问题。其核心创新点包括：</p>
<ul>
<li><strong>多智能体架构：</strong> 该框架采用模块化的多智能体设计，由一个 <strong>主控智能体 (MASTER AGENT)</strong> 协调两个专业智能体：<ul>
<li><strong>定位智能体 (GROUNDING AGENT)：</strong> 负责在长视频中定位与问题相关的片段。</li>
<li><strong>视觉智能体 (VISION AGENT)：</strong> 负责从定位到的片段中提取详细的视觉信息（如对象、属性、动作、OCR文本等）。</li>
</ul>
</li>
<li><strong>迭代式推理与规划：</strong> 主控智能体通过一个有步数限制的循环（最多K步）来规划推理过程。在每一步，它会根据当前上下文生成子查询，调用定位或视觉智能体，并将返回的信息整合到上下文中，然后决定下一步行动。</li>
<li><strong>强化学习训练：</strong> 主控智能体采用 <strong>基于奖励的强化学习（GRPO）</strong> 进行训练，以鼓励其进行简洁、正确且高效的多智能体协作。奖励函数设计旨在惩罚不相关的工具使用和不连贯的推理，引导智能体学习“思考”的正确格式，并判断何时需要探索视频，何时已收集到足够证据。</li>
<li><strong>新数据集 LongTVQA 和 LongTVQA+：</strong> 为了评估长视频理解能力，作者构建了两个新的数据集，它们是基于现有TVQA/TVQA+数据集扩展而来的，涵盖了更长的视频时长（小时级别），为长视频问答提供了更具挑战性的测试平台。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong></p>
<ul>
<li><strong>显著的性能提升：</strong> LongVideoAgent 在提出的 LongTVQA 和 LongTVQA+ 数据集上取得了 <strong>显著优于</strong> 现有非智能体基线模型的性能。</li>
<li><strong>多智能体协同的有效性：</strong> 消融实验表明，多智能体架构（特别是结合了定位和视觉智能体）对性能提升至关重要。</li>
<li><strong>强化学习的增益：</strong> 强化学习训练进一步增强了主控智能体的推理和规划能力，尤其对于开源模型，RL带来了显著的准确率提升。</li>
<li><strong>可解释性：</strong> 该框架能够生成清晰、分步的推理轨迹，展示了智能体如何协调子智能体来选择相关片段和提取关键视觉信息，提高了系统的可解释性。</li>
<li><strong>模型无关性：</strong> 该框架可以与不同的闭源和开源LLM（作为主控智能体）结合使用，证明了其通用性。</li>
</ul>
<p><strong>4. 论文提及的局限性：</strong></p>
<ul>
<li><strong>依赖字幕：</strong> 研究主要依赖提供的字幕作为主要的文本信息来源，并未直接处理原始音频。</li>
<li><strong>固定子模块：</strong> 在强化学习训练过程中，定位和视觉智能体被固定住，联合优化它们可能进一步提升性能。</li>
<li><strong>奖励函数简化：</strong> 奖励函数相对简单，仅包含结构有效性和答案正确性，可能仍有改进空间。</li>
</ul>
<p><strong>5. 未来研究方向：</strong></p>
<ul>
<li><strong>集成更多模态：</strong> 整合原始音频（通过ASR模块）、知识背景等更多模态信息。</li>
<li><strong>联合优化：</strong> 探索对定位和视觉智能体进行联合优化，以进一步提升鲁棒性和准确性。</li>
<li><strong>更复杂的奖励设计：</strong> 设计更精细的奖励函数，以更全面地指导智能体的学习。</li>
<li><strong>更大规模的RL训练：</strong> 进行更大规模的强化学习训练，以探索更优的策略。</li>
<li><strong>更细粒度的定位：</strong> 进一步提升时间定位的精度。</li>
</ul>
<p><strong>论文的创新性与重要性：</strong></p>
<p>这篇论文在长视频理解领域做出了重要贡献，其核心价值在于：</p>
<ul>
<li><strong>开创性的多智能体框架：</strong> 首次提出了一种将主控智能体与专业定位和视觉智能体相结合的多智能体架构，有效解决了长视频中信息稀疏和细粒度推理的难题。</li>
<li><strong>有效的强化学习训练范式：</strong> 成功地将强化学习应用于指导多智能体协作，实现了更准确、简洁和高效的推理过程。</li>
<li><strong>高质量的长视频数据集：</strong> 提供了新的长视频问答数据集，为该领域的研究提供了重要的评估基准。</li>
<li><strong>提升了模型的可解释性：</strong> 通过生成可解释的推理轨迹，使得理解模型决策过程成为可能。</li>
</ul>
<p>LongVideoAgent 的方法论为处理长视频中的复杂推理任务提供了一个强大的新范式，并展示了多智能体系统在解决现实世界复杂问题中的巨大潜力。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose a multi-agent framework in which a master LLM coordinates a grounding agent to localize question-relevant segments and a vision agent to extract targeted textual observations.</li>
<li>On our proposed LongTVQA and LongTVQA+ which are episode-level datasets aggregated from TVQA/TVQA+, our multi-agent system significantly outperforms strong non-agent baselines.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.20618v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.20618v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.20617v1'></a></p>
<h2 id="spatialtree-how-spatial-abilities-branch-out-in-mllms"><a href="https://arxiv.org/abs/2512.20617v1">SpatialTree: How Spatial Abilities Branch Out in MLLMs</a></h2>
<p><strong>Authors:</strong> Yuxi Xiao, Longfei Li, Shen Yan, Xinhang Liu, Sida Peng, Yunchao Wei, Xiaowei Zhou, Bingyi Kang</p>
<p><strong>Published:</strong> 2025-12-23</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Cognitive science suggests that spatial ability develops progressively-from perception to reasoning and interaction. Yet in multimodal LLMs (MLLMs), this hierarchy remains poorly understood, as most studies focus on a narrow set of tasks. We introduce SpatialTree, a cognitive-science-inspired hierarchy that organizes spatial abilities into four levels: low-level perception (L1), mental mapping (L2), simulation (L3), and agentic competence (L4). Based on this taxonomy, we construct the first capability-centric hierarchical benchmark, thoroughly evaluating mainstream MLLMs across 27 sub-abilities. The evaluation results reveal a clear structure: L1 skills are largely orthogonal, whereas higher-level skills are strongly correlated, indicating increasing interdependency. Through targeted supervised fine-tuning, we uncover a surprising transfer dynamic-negative transfer within L1, but strong cross-level transfer from low- to high-level abilities with notable synergy. Finally, we explore how to improve the entire hierarchy. We find that naive RL that encourages extensive "thinking" is unreliable: it helps complex reasoning but hurts intuitive perception. We propose a simple auto-think strategy that suppresses unnecessary deliberation, enabling RL to consistently improve performance across all levels. By building SpatialTree, we provide a proof-of-concept framework for understanding and systematically scaling spatial abilities in MLLMs.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行深入分析。</p>
<p><strong>论文摘要分析：SpatialTree: How Spatial Abilities Branch Out in MLLMs</strong></p>
<p><strong>1. 论文的主要贡献 (2-3句话总结)</strong></p>
<p>这篇论文的核心贡献在于提出了一个受认知科学启发的、分层级的空间能力框架（SpatialTree），并基于此构建了一个全面的基准测试，用于系统性地评估多模态大型语言模型（MLLMs）在空间理解和推理方面的能力。研究揭示了不同层级空间能力之间的相互依赖关系，并探索了通过微调和强化学习来提升这些能力的新方法，为理解和发展 MLLMs 的空间智能提供了理论和实践指导。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<ul>
<li>
<p><strong>SpatialTree 框架：</strong> 这是论文最核心的创新。它借鉴了认知科学中空间能力从感知到推理再到交互的渐进式发展模型，将 MLLMs 的空间能力划分为四个层级：</p>
<ul>
<li><strong>L1: Low-level perception (低级感知):</strong> 图像中的基本视觉元素识别，如形状、颜色、纹理、物体位置等。</li>
<li><strong>L2: Mental mapping (心智映射):</strong> 在感知基础上构建场景的内部表征，理解物体之间的相对位置、空间关系，形成场景的“地图”。</li>
<li><strong>L3: Simulation (模拟):</strong> 基于心智映射，对场景进行动态模拟，预测物体的运动轨迹、交互结果等。</li>
<li><strong>L4: Agentic competence (代理能力):</strong> 将空间理解和推理能力应用于实际任务，例如导航、操作物体、规划路径等，展现出智能体的行为。
这种分层结构为系统性地分析和提升 MLLMs 的空间能力提供了一个清晰的理论基础。</li>
</ul>
</li>
<li>
<p><strong>能力中心化的分层基准测试：</strong> 基于 SpatialTree 框架，论文构建了一个“能力中心化”的基准测试，包含 27 个子能力。这与以往仅关注特定任务的评估方式不同，更侧重于解构和量化 MLLMs 在不同空间能力层级上的表现。这种方法能够更精确地诊断模型在哪些方面存在不足。</p>
</li>
<li>
<p><strong>深入的迁移学习和强化学习分析：</strong></p>
<ul>
<li><strong>监督微调 (SFT) 分析：</strong> 论文通过 SFT 揭示了空间能力迁移的有趣现象：L1 层级内部存在负迁移（即提升一个 L1 能力可能损害另一个 L1 能力），但从 L1 到 L2/L3/L4 存在强大的正向跨层迁移，且具有协同效应。这表明低级感知能力是构建高级空间智能的基础。</li>
<li><strong>强化学习 (RL) 探索与优化：</strong> 论文发现，简单的“鼓励思考”的 RL 方法（可能指生成大量中间推理步骤）在提升复杂推理（L3/L4）方面有效，但会损害直观感知（L1）。为了解决这个问题，他们提出了“自动思考”（auto-think）策略，通过抑制不必要的思考来优化 RL，使其能够一致地提升所有层级的性能。</li>
</ul>
</li>
</ul>
<p><strong>3. 对该领域的潜在影响</strong></p>
<ul>
<li><strong>标准化评估框架：</strong> SpatialTree 提供了一个更全面、更具结构性的 MLLMs 空间能力评估标准，有望成为该领域研究的基石，促进不同模型之间的公平比较。</li>
<li><strong>理解 MLLMs 的内在机制：</strong> 通过揭示不同层级空间能力之间的相互依赖和迁移规律，该研究有助于我们更深入地理解 MLLMs 在处理空间信息时的内部工作机制。</li>
<li><strong>指导模型设计与训练：</strong> 研究结果为 MLLMs 的模型设计和训练策略提供了重要启示。例如，强调了低级感知能力的重要性，以及如何通过更精细的 RL 策略来平衡不同层级的性能。</li>
<li><strong>推动通用人工智能（AGI）的发展：</strong> 空间智能是人类智能的重要组成部分，也是实现更通用人工智能的关键。SpatialTree 的研究为提升 MLLMs 的空间智能，进而向 AGI 迈进提供了重要的理论和技术支撑。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用</strong></p>
<ul>
<li><strong>机器人学与自动驾驶：</strong> MLLMs 需要强大的空间理解能力来感知环境、进行路径规划、与物理世界交互。SpatialTree 的研究可以直接应用于提升这些系统的空间智能。</li>
<li><strong>虚拟现实 (VR) 和增强现实 (AR)：</strong> MLLMs 在 VR/AR 中需要理解用户意图、场景布局，并进行交互。SpatialTree 的框架和评估方法可以帮助开发更智能、更具沉浸感的 VR/AR 体验。</li>
<li><strong>3D 内容生成与编辑：</strong> MLLMs 在理解和生成 3D 模型、场景时，需要深厚的空间推理能力。该研究可以指导如何训练模型更好地处理三维信息。</li>
<li><strong>智能助手与问答系统：</strong> 对于需要理解物理世界信息（如“桌子上的杯子在哪里？”、“如何从 A 点走到 B 点？”）的智能助手，SpatialTree 的研究至关重要。</li>
<li><strong>教育与培训：</strong> 模拟和交互式学习环境的开发，可以借鉴 SpatialTree 的分层能力模型来设计更有效的教学内容。</li>
</ul>
<p><strong>5. 可从摘要推断的局限性</strong></p>
<ul>
<li><strong>“认知科学启发”的局限性：</strong> 虽然借鉴了认知科学，但 MLLMs 的内部机制与人类大脑存在本质差异。SpatialTree 框架是否能完全捕捉人类的空间认知过程，仍需进一步验证。</li>
<li><strong>基准测试的覆盖范围：</strong> 尽管包含了 27 个子能力，但空间能力的范畴非常广泛。摘要中并未详细说明这些子能力是否能完全代表所有重要的空间能力，以及基准测试的复杂度和多样性是否足够。</li>
<li><strong>“主流 MLLMs”的代表性：</strong> 摘要提到评估了“主流 MLLMs”，但具体是哪些模型，以及这些模型在多大程度上代表了当前 MLLMs 的发展水平，需要进一步了解。</li>
<li><strong>“负迁移”的解释：</strong> 摘要提到 L1 层级存在负迁移，但未深入解释其根本原因，这可能与模型在不同低级感知任务上的优化目标冲突有关。</li>
<li><strong>“自动思考”策略的普适性：</strong> “自动思考”策略的有效性可能依赖于具体的 RL 算法和任务设置，其普适性和可扩展性有待进一步验证。</li>
<li><strong>数据和计算资源：</strong> 构建和运行如此全面的基准测试，以及进行大规模的 SFT 和 RL 训练，需要大量的计算资源和高质量的数据集，这可能是研究的一个潜在门槛。</li>
<li><strong>Published Date 2025-12-23：</strong> 这个日期表明该论文尚未公开发表，因此摘要中的信息是基于作者的初步报告，实际内容可能有所调整。</li>
</ul>
<p>总而言之，这篇论文通过构建一个创新的分层框架和全面的基准测试，为理解和提升 MLLMs 的空间能力提供了一个系统性的解决方案。其对能力迁移和 RL 优化的深入分析，以及提出的“自动思考”策略，都为该领域的研究和应用带来了重要的启示。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce SpatialTree, a cognitive-science-inspired hierarchy that organizes spatial abilities into four levels: low-level perception (L1), mental mapping (L2), simulation (L3), and agentic competence (L4).</li>
<li>We propose a simple auto-think strategy that suppresses unnecessary deliberation, enabling RL to consistently improve performance across all levels.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.20617v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.20617v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.20606v1'></a></p>
<h2 id="repurposing-video-diffusion-transformers-for-robust-point-tracking"><a href="https://arxiv.org/abs/2512.20606v1">Repurposing Video Diffusion Transformers for Robust Point Tracking</a></h2>
<p><strong>Authors:</strong> Soowon Son, Honggyu An, Chaehyun Kim, Hyunah Ko, Jisu Nam, Dahyun Chung, Siyoon Jin, Jung Yi, Jaewon Min, Junhwa Hur, Seungryong Kim</p>
<p><strong>Published:</strong> 2025-12-23</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Point tracking aims to localize corresponding points across video frames, serving as a fundamental task for 4D reconstruction, robotics, and video editing. Existing methods commonly rely on shallow convolutional backbones such as ResNet that process frames independently, lacking temporal coherence and producing unreliable matching costs under challenging conditions. Through systematic analysis, we find that video Diffusion Transformers (DiTs), pre-trained on large-scale real-world videos with spatio-temporal attention, inherently exhibit strong point tracking capability and robustly handle dynamic motions and frequent occlusions. We propose DiTracker, which adapts video DiTs through: (1) query-key attention matching, (2) lightweight LoRA tuning, and (3) cost fusion with a ResNet backbone. Despite training with 8 times smaller batch size, DiTracker achieves state-of-the-art performance on challenging ITTO benchmark and matches or outperforms state-of-the-art models on TAP-Vid benchmarks. Our work validates video DiT features as an effective and efficient foundation for point tracking.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Repurposing Video Diffusion Transformers for Robust Point Tracking”的全面中文摘要：</p>
<p><strong>论文题目：</strong> Repurposing Video Diffusion Transformers for Robust Point Tracking (视频扩散 Transformer 的再利用以实现鲁棒的点跟踪)</p>
<p><strong>作者：</strong> Soowon Son, Honggyu An, Chaehyun Kim, Hyunah Ko, Jisu Nam, Dahyun Chung, Siyoon Jin, Jung Yi, Jaewon Min, Junhwa Hur, Seungryong Kim</p>
<p><strong>1. 研究问题/核心挑战：</strong></p>
<p>点跟踪是计算机视觉中的一项基础任务，广泛应用于 4D 重建、机器人和视频编辑等领域。现有方法通常依赖于浅层卷积骨干网络（如 ResNet），这些网络独立处理视频帧，缺乏时间连贯性，在复杂场景（如动态运动和频繁遮挡）下容易产生不可靠的匹配成本。这限制了点跟踪在真实世界场景中的鲁棒性和泛化能力。</p>
<p><strong>2. 主要创新点/方法贡献：</strong></p>
<p>该论文提出了一种名为 <strong>DiTracker</strong> 的新颖点跟踪框架，其核心贡献在于：</p>
<ul>
<li><strong>利用预训练视频扩散 Transformer (DiT) 作为特征骨干：</strong> 作者通过系统分析发现，在海量真实世界视频上进行预训练的视频 DiT，由于其时空注意力机制，天然具备强大的点跟踪能力，能够鲁棒地处理动态运动和频繁遮挡。</li>
<li><strong>DiT 的适配方法：</strong><ul>
<li><strong>查询-键注意力匹配：</strong> 借鉴 DiT 的内部注意力机制，使用查询-键注意力来计算匹配成本，以保留其固有的匹配能力。</li>
<li><strong>轻量级 LoRA 微调：</strong> 采用低秩适应 (LoRA) 技术对 DiT 进行高效微调，从而在不破坏其学到的时间连贯性的前提下，将其适配到点跟踪任务。</li>
<li><strong>与 ResNet 的成本融合：</strong> 提出一种成本融合策略，将 DiT 的全局匹配能力（擅长处理大位移、遮挡等挑战）与 ResNet 的局部细节捕捉能力相结合，以实现互补优势。</li>
</ul>
</li>
<li><strong>高效训练：</strong> 尽管使用了 8 倍更小的批次大小，DiTracker 仍能取得优异的性能，表明了视频 DiT 特征在训练效率上的优势。</li>
</ul>
<p><strong>3. 主要结果及其意义：</strong></p>
<ul>
<li><strong>性能超越：</strong> DiTracker 在具有挑战性的 ITTO 基准测试中取得了最先进 (state-of-the-art) 的性能，并在 TAP-Vid 基准测试中与现有最先进模型持平或超越。</li>
<li><strong>鲁棒性提升：</strong> 在运动模糊、动态运动和频繁遮挡等真实世界挑战下，DiTracker 展现出显著的鲁棒性，优于传统的 ResNet 骨干网络。即使在 ImageNet-C 图像损坏测试中，DiTracker 的性能也保持稳定。</li>
<li><strong>训练效率：</strong> DiTracker 在更少的训练迭代次数和更小的批次大小下，实现了与更大型模型相当甚至更优的性能，证明了视频 DiT 特征作为点跟踪基础的有效性和高效性。</li>
<li><strong>理论意义：</strong> 该研究有力地证明了，大规模视频预训练（尤其是具有时空注意力机制的）能够赋予模型强大的泛化能力和对动态场景的理解能力，为点跟踪等下游任务提供了更优越的特征表示。</li>
</ul>
<p><strong>4. 论文中提到的局限性：</strong></p>
<ul>
<li><strong>推理时间和内存消耗：</strong> 论文指出，与 CoTracker3 等模型相比，DiTracker 需要更多的推理时间和内存。这是由于从大型视频扩散模型中提取特征本身计算成本较高，这是使用这类基础模型时常见的权衡。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>进一步优化计算效率：</strong> 尽管 DiTracker 在性能上取得了显著进步，但其计算成本仍然是一个挑战。未来的工作可以探索更高效的特征提取或模型压缩技术，以进一步降低推理时间和内存需求。</li>
<li><strong>探索更多 DiT 的应用：</strong> 视频 DiT 的强大能力可能适用于更多计算机视觉任务，如视频分割、目标检测等，可以进一步探索其在这些领域的潜力。</li>
<li><strong>更精细的融合策略：</strong> 虽然成本融合策略有效，但仍有空间进一步研究更精细的融合机制，以更好地利用 DiT 和 ResNet 的互补优势。</li>
</ul>
<p><strong>总结：</strong></p>
<p>这篇论文成功地展示了视频扩散 Transformer (DiT) 作为点跟踪任务的强大特征骨干的潜力。通过提出 DiTracker 框架，该研究不仅在多个基准测试中取得了最先进的性能，而且显著提升了点跟踪在复杂真实世界场景下的鲁棒性。其核心贡献在于有效地适配了预训练的视频 DiT 特征，并通过与 ResNet 的成本融合实现了性能的进一步提升。该工作为利用大型预训练模型解决下游视觉任务提供了新的思路，并为未来的研究开辟了道路。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose DiTracker, which adapts video DiTs through: (1) query-key attention matching, (2) lightweight LoRA tuning, and (3) cost fusion with a ResNet backbone.</li>
<li>Despite training with 8 times smaller batch size, DiTracker achieves state-of-the-art performance on challenging ITTO benchmark and matches or outperforms state-of-the-art models on TAP-Vid benchmarks.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.20606v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.20606v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.20595v1'></a></p>
<h2 id="cube-bench-a-benchmark-for-spatial-visual-reasoning-in-mllms"><a href="https://arxiv.org/abs/2512.20595v1">Cube Bench: A Benchmark for Spatial Visual Reasoning in MLLMs</a></h2>
<p><strong>Authors:</strong> Dhruv Anand, Ehsan Shareghi</p>
<p><strong>Published:</strong> 2025-12-23</p>
<p><strong>Categories:</strong> cs.CL, cs.AI, cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>We introduce Cube Bench, a Rubik's-cube benchmark for evaluating spatial and sequential reasoning in multimodal large language models (MLLMs). The benchmark decomposes performance into five skills: (i) reconstructing cube faces from images and text, (ii) choosing the optimal next move, (iii) predicting the outcome of a candidate move without applying it, (iv) executing multi-step plans while recovering from mistakes, and (v) detecting and revising one's own errors. Using a shared set of scrambled cube states, identical prompts and parsers, and a single distance-to-solved metric, we compare recent MLLMs side by side as a function of scramble depth. Across seven MLLMs, accuracy drops sharply with depth; once a trajectory stalls or diverges, models rarely recover, and high face-reconstruction accuracy does not guarantee competent action selection or multi-step execution. A pronounced closed- vs open-source gap emerges: the strongest closed model leads on both single-step perception tasks and multi-step control tasks, while open-weight models cluster near chance on the hardest settings; yet even the best MLLM degrades at higher cube complexity. A simple self-correction via reflective thinking yields modest gains but can also introduce overthinking. Cube Bench offers a compact, reproducible probe of sequential spatial reasoning in MLLMs.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Cube Bench: A Benchmark for Spatial Visual Reasoning in MLLMs”的全面中文摘要：</p>
<p><strong>论文题目：</strong> Cube Bench: A Benchmark for Spatial Visual Reasoning in MLLMs (Cube Bench：多模态大语言模型空间视觉推理基准)</p>
<p><strong>作者：</strong> Dhruv Anand, Ehsan Shareghi</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong></p>
<p>多模态大语言模型（MLLMs）在静态视觉理解任务（如图像识别、文本转录）上取得了显著进展，但其在需要<strong>长期、交互式决策</strong>的场景下的表现却鲜为人知。在这些场景中，模型需要能够<strong>规划、执行多步动作、观察结果并从错误中恢复</strong>。然而，现有的基准测试往往侧重于单步感知能力，无法充分揭示模型在连续决策和状态跟踪方面的弱点。论文旨在解决这一问题，即<strong>如何有效地评估MLLMs在序列化空间推理和闭环控制能力方面的真实水平</strong>。</p>
<p><strong>2. 主要创新与方法贡献：</strong></p>
<ul>
<li>
<p><strong>Cube Bench 基准的提出：</strong> 作者引入了一个新颖的、基于<strong>鲁班魔方</strong>的多模态基准测试——Cube Bench。该基准具有以下关键特点：</p>
<ul>
<li><strong>紧凑且可控：</strong> 基于生成器，可以按需生成无限数量的测试场景，确保可复现性。</li>
<li><strong>完全可观察：</strong> 避免了真实世界中的部分可观察性带来的混淆。</li>
<li><strong>精确的评估指标：</strong> 使用“到达目标状态的最短移动步数”（God's Number）作为核心评估指标，能够精确量化模型在每一步的进展。</li>
<li><strong>分解能力：</strong> 将MLLMs的性能分解为五个关键技能：<ol>
<li><strong>面部重建：</strong> 从图像和文本中重建魔方面。</li>
<li><strong>最优下一步预测：</strong> 选择能最快解决魔方的下一步动作。</li>
<li><strong>预判动作结果：</strong> 在不实际执行动作的情况下预测其对魔方状态的影响。</li>
<li><strong>多步规划与恢复：</strong> 执行多步计划，并能在出现错误后进行恢复。</li>
<li><strong>错误检测与修正：</strong> 检测并修正自身的错误。</li>
</ol>
</li>
<li><strong>公平性设计：</strong> 通过共享的魔方状态、相同的提示和解析器，以及单一的距离度量，确保了模型间的公平比较。</li>
</ul>
</li>
<li>
<p><strong>七项具体测试任务：</strong> Cube Bench 包含七项精心设计的任务，覆盖了从感知到决策再到反思的完整闭环过程。</p>
</li>
<li>
<p><strong>严格的评估协议：</strong> 论文采用了严格的解析规则和公平性控制，以避免模型通过捷径或偏见获得高分。</p>
</li>
</ul>
<p><strong>3. 主要结果与意义：</strong></p>
<ul>
<li><strong>深度效应显著：</strong> 实验结果表明，随着魔方打乱深度（即解决魔方所需的步数）的增加，MLLMs的准确率急剧下降。一旦模型轨迹停滞或偏离，它们很少能自行恢复。</li>
<li><strong>感知与推理的脱节：</strong> 高的面部重建准确率并不能保证模型在动作选择或多步执行方面的能力。这揭示了模型在将局部感知能力转化为全局推理和规划方面的不足。</li>
<li><strong>闭源与开源模型的显著差距：</strong> 最强的闭源模型在单步感知和多步控制任务上均表现出色，而开源模型在最困难的设置下表现接近随机猜测。</li>
<li><strong>反思的价值与局限：</strong> 简单的自我反思（通过“引导式（已编辑）”反思）可以带来适度的性能提升，但同时也可能导致“过度思考”和不稳定性。</li>
<li><strong>预判能力的重要性：</strong> 论文发现，在动作执行前的因果评估（Causal Move-Effect）能力（用 Cohen's κ 度量）与闭环控制能力（Teacher Adherence）之间存在强烈的相关性，尤其是在短序列问题中。这表明模型在执行动作前进行准确预测的能力是成功进行序列决策的关键。</li>
<li><strong>意义：</strong> Cube Bench 提供了一个紧凑、可复现的工具，能够精确地探测 MLLMs 在序列化空间推理方面的瓶颈，揭示了当前模型在状态跟踪、动作评估和长期规划方面的根本性弱点，这些弱点在传统的单步感知基准测试中是无法被发现的。</li>
</ul>
<p><strong>4. 局限性：</strong></p>
<ul>
<li><strong>任务范围限制：</strong> Cube Bench 主要测试空间感知和短期规划能力，其结果可能不完全适用于更广泛的任务，如机器人控制或网络代理任务。</li>
<li><strong>模型性能限制：</strong> 由于现有 MLLMs 的性能限制，评估仅限于较浅的打乱深度。</li>
<li><strong>多项选择格式：</strong> 测试任务采用多项选择格式，可能限制了对复杂思考和推理错误的深入洞察。</li>
</ul>
<p><strong>5. 未来研究方向：</strong></p>
<ul>
<li><strong>更深层次的序列推理：</strong> 探索更长的打乱深度和更复杂的任务，以研究模型在更长期的规划和决策能力。</li>
<li><strong>更广泛的任务应用：</strong> 将 Cube Bench 的思想和方法应用于其他领域，如机器人学、游戏 AI 等。</li>
<li><strong>改进模型架构与训练：</strong> 基于 Cube Bench 的评估结果，开发能够提升 MLLMs 在序列推理、错误恢复和因果预测方面能力的模型架构和训练方法。</li>
<li><strong>探索更复杂的反思机制：</strong> 研究更鲁棒和有效的反思机制，以平衡其带来的收益和潜在的负面影响。</li>
</ul>
<p><strong>总结：</strong></p>
<p>Cube Bench 是一个重要的贡献，它提供了一个标准化的、可控的基准来评估 MLLMs 在序列化空间推理方面的能力。研究结果清晰地表明，尽管 MLLMs 在感知任务上表现出色，但在需要长期规划、动作执行和错误恢复的交互式场景中仍存在显著的局限性。论文强调了预动作评估、决策意识指标以及对反思机制的审慎控制的重要性，为未来 MLLMs 在复杂推理和决策领域的进步指明了方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We introduce Cube Bench, a Rubik's-cube benchmark for evaluating spatial and sequential reasoning in multimodal large language models (MLLMs).</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.20595v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.20595v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.20591v1'></a></p>
<h2 id="lighttact-a-visual-tactile-fingertip-sensor-for-deformation-independent-contact-sensing"><a href="https://arxiv.org/abs/2512.20591v1">LightTact: A Visual-Tactile Fingertip Sensor for Deformation-Independent Contact Sensing</a></h2>
<p><strong>Authors:</strong> Changyi Lin, Boda Huo, Mingyang Yu, Emily Ruppel, Bingqing Chen, Jonathan Francis, Ding Zhao</p>
<p><strong>Published:</strong> 2025-12-23</p>
<p><strong>Categories:</strong> cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>Contact often occurs without macroscopic surface deformation, such as during interaction with liquids, semi-liquids, or ultra-soft materials. Most existing tactile sensors rely on deformation to infer contact, making such light-contact interactions difficult to perceive robustly. To address this, we present LightTact, a visual-tactile fingertip sensor that makes contact directly visible via a deformation-independent, optics-based principle. LightTact uses an ambient-blocking optical configuration that suppresses both external light and internal illumination at non-contact regions, while transmitting only the diffuse light generated at true contacts. As a result, LightTact produces high-contrast raw images in which non-contact pixels remain near-black (mean gray value &lt; 3) and contact pixels preserve the natural appearance of the contacting surface. Built on this, LightTact achieves accurate pixel-level contact segmentation that is robust to material properties, contact force, surface appearance, and environmental lighting. We further integrate LightTact on a robotic arm and demonstrate manipulation behaviors driven by extremely light contact, including water spreading, facial-cream dipping, and thin-film interaction. Finally, we show that LightTact's spatially aligned visual-tactile images can be directly interpreted by existing vision-language models, enabling resistor value reasoning for robotic sorting.</p>
<p><strong>Analysis:</strong></p>
<p>好的，作为计算机视觉和机器学习领域的专家，我将根据您提供的论文摘要进行深入分析。</p>
<p><strong>论文摘要分析：LightTact: A Visual-Tactile Fingertip Sensor for Deformation-Independent Contact Sensing</strong></p>
<p><strong>1. 论文的主要贡献（2-3句话）</strong></p>
<p>本论文提出了一种名为 LightTact 的新型视觉-触觉指尖传感器，其核心贡献在于实现了一种<strong>不依赖宏观形变</strong>的接触感知方法。通过创新的光学设计，LightTact 能够直接可视化极轻微的接触，即使在与液体、半液体或超软材料等难以通过形变检测的介质交互时也能实现鲁棒的接触识别。这为机器人提供了感知微弱接触的能力，并能直接利用其输出图像进行更高级别的任务。</p>
<p><strong>2. 关键创新或方法论</strong></p>
<p>LightTact 的关键创新在于其<strong>“形变无关、光学原理”</strong>的接触感知方法。具体来说，其核心方法论包括：</p>
<ul>
<li><strong>环境光阻挡光学配置 (Ambient-blocking optical configuration):</strong> 这是该传感器最核心的设计。它通过巧妙的光学设计，有效抑制了非接触区域的外部光线和内部照明。</li>
<li><strong>选择性光传输 (Selective light transmission):</strong> 传感器只允许在<strong>真实接触点</strong>产生的<strong>漫射光</strong>得以传输。这意味着只有当传感器与物体发生接触时，才会产生可被检测到的光信号。</li>
<li><strong>高对比度图像生成 (High-contrast image generation):</strong> 这种光学设计直接导致了传感器输出的原始图像具有极高的对比度。非接触区域的像素值接近于零（接近黑色），而接触区域的像素则能保留接触表面本身的自然外观。</li>
<li><strong>像素级接触分割 (Pixel-level contact segmentation):</strong> 基于高对比度的图像，可以实现精确的像素级接触区域分割，并且这种分割对接触材料的属性、接触力的大小、表面外观以及环境光照条件都表现出鲁棒性。</li>
</ul>
<p><strong>3. 对该领域的潜在影响</strong></p>
<p>LightTact 的出现可能对触觉感知领域产生深远影响，尤其是在以下几个方面：</p>
<ul>
<li><strong>拓展触觉感知的边界:</strong> 极大地扩展了触觉传感器能够感知的接触类型，特别是对于那些传统形变传感器难以处理的“轻接触”场景。</li>
<li><strong>提升机器人操作的精细度和鲁棒性:</strong> 使机器人能够执行更精细、更灵巧的操作，例如在液体表面进行操作、处理易碎或极软的物体，从而提高其在复杂环境中的适应性和鲁棒性。</li>
<li><strong>推动视觉-触觉融合研究:</strong> LightTact 生成的直接可解释的视觉-触觉图像，为更深层次的视觉-触觉融合提供了新的可能性，尤其是在与大型语言模型（LLMs）等先进AI模型结合时。</li>
<li><strong>降低触觉传感器的复杂性:</strong> 相较于一些依赖复杂形变测量或力反馈的传感器，LightTact 的光学原理可能在某些应用场景下提供一种更简洁、更易于实现的解决方案。</li>
</ul>
<p><strong>4. 可能受益的相关领域或应用</strong></p>
<ul>
<li><strong>机器人学:</strong><ul>
<li><strong>精细操作:</strong> 例如，在医疗领域进行微创手术、在食品工业中处理精细食材、在电子组装中抓取微小元件。</li>
<li><strong>人机交互:</strong> 提升机器人与人类的自然交互能力，例如在服务机器人中感知用户轻微的触碰。</li>
<li><strong>软体机器人:</strong> 更好地与柔软、易变形的物体进行交互。</li>
<li><strong>水下或液体环境操作:</strong> 机器人可以在不确定接触状态的液体环境中进行操作。</li>
</ul>
</li>
<li><strong>虚拟现实/增强现实 (VR/AR):</strong> 提供更真实的触觉反馈，增强沉浸感，尤其是在模拟与液体或柔软物体的交互时。</li>
<li><strong>假肢和外骨骼:</strong> 提高假肢的触觉感知能力，使使用者能更精细地控制假肢。</li>
<li><strong>材料科学:</strong> 用于研究材料的表面特性和微观相互作用。</li>
<li><strong>质量控制和检测:</strong> 检测产品表面是否存在微小缺陷或异常接触。</li>
</ul>
<p><strong>5. 从摘要中可以推断出的局限性</strong></p>
<p>尽管摘要展示了 LightTact 的强大能力，但仍可以从其描述中推断出一些潜在的局限性：</p>
<ul>
<li><strong>对接触表面特性的依赖:</strong> 虽然摘要声称对材料属性鲁棒，但“接触像素保留自然外观”的描述暗示，如果接触表面本身非常光滑、反光度极高或透明度极高，可能会影响漫射光的产生和检测，从而影响性能。</li>
<li><strong>对接触深度的感知能力:</strong> 摘要主要强调“接触”的检测，而对于接触的“深度”或“形变程度”的量化能力，从摘要中看不出其直接的测量机制。它似乎更侧重于“是否接触”以及“接触区域的表面信息”。</li>
<li><strong>传感器本身的物理限制:</strong> 作为指尖传感器，其尺寸、耐用性、对极端温度或化学腐蚀的抵抗力等物理特性并未在摘要中提及，这些都是实际应用中需要考虑的因素。</li>
<li><strong>对“极轻接触”的定义:</strong> 摘要中提到“extremely light contact”，但并未给出具体的力学量化标准。其“形变无关”的特性可能意味着它对非常微弱的力敏感，但这种敏感度的上限和下限需要进一步的实验验证。</li>
<li><strong>计算和处理需求:</strong> 虽然摘要提到可以直接被视觉-语言模型解释，但生成和处理高对比度图像可能仍需要一定的计算资源，尤其是在实时应用中。</li>
</ul>
<p><strong>总结 LightTact 对计算机视觉领域的趣味性和重要性：</strong></p>
<p>LightTact 之所以对计算机视觉领域具有潜在的趣味性和重要性，主要在于它<strong>打破了传统视觉感知对表面形变的依赖，并创造了一种全新的、直接可视化的接触信号生成机制</strong>。</p>
<ul>
<li><strong>新颖的视觉信号生成:</strong> 它不是通过分析物体表面的形变来推断接触，而是通过光学原理直接“看到”接触本身。这为计算机视觉提供了一种全新的、与传统图像信息互补的感知维度。</li>
<li><strong>形变无关的鲁棒性:</strong> 这种形变无关的特性意味着它能够处理传统视觉方法难以应对的场景，例如液体表面、半透明物体等，极大地扩展了视觉感知在现实世界中的应用范围。</li>
<li><strong>与现有视觉模型的兼容性:</strong> 论文强调其输出图像可以直接被现有视觉-语言模型解释，这表明它能够无缝集成到现有的深度学习框架中，为机器人提供更丰富的感知输入，从而驱动更智能、更精细的任务执行，例如论文中提到的电阻值推理。这预示着未来机器人感知将更加融合，视觉和触觉信息将以更直接、更有效的方式被AI模型利用。</li>
</ul>
<p>总而言之，LightTact 通过创新的光学设计，为计算机视觉提供了一种“看见接触”的新方式，解决了传统方法在轻接触场景下的瓶颈，并为更高级别的机器人智能和人机交互开辟了新的道路。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To address this, we present LightTact, a visual-tactile fingertip sensor that makes contact directly visible via a deformation-independent, optics-based principle.</li>
<li>Finally, we show that LightTact's spatially aligned visual-tactile images can be directly interpreted by existing vision-language models, enabling resistor value reasoning for robotic sorting.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.20591v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.20591v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.20563v1'></a></p>
<h2 id="lead-minimizing-learner-expert-asymmetry-in-end-to-end-driving"><a href="https://arxiv.org/abs/2512.20563v1">LEAD: Minimizing Learner-Expert Asymmetry in End-to-End Driving</a></h2>
<p><strong>Authors:</strong> Long Nguyen, Micha Fauth, Bernhard Jaeger, Daniel Dauner, Maximilian Igl, Andreas Geiger, Kashyap Chitta</p>
<p><strong>Published:</strong> 2025-12-23</p>
<p><strong>Categories:</strong> cs.CV, cs.AI, cs.LG, cs.RO</p>
<p><strong>Abstract:</strong></p>
<p>Simulators can generate virtually unlimited driving data, yet imitation learning policies in simulation still struggle to achieve robust closed-loop performance. Motivated by this gap, we empirically study how misalignment between privileged expert demonstrations and sensor-based student observations can limit the effectiveness of imitation learning. More precisely, experts have significantly higher visibility (e.g., ignoring occlusions) and far lower uncertainty (e.g., knowing other vehicles' actions), making them difficult to imitate reliably. Furthermore, navigational intent (i.e., the route to follow) is under-specified in student models at test time via only a single target point. We demonstrate that these asymmetries can measurably limit driving performance in CARLA and offer practical interventions to address them. After careful modifications to narrow the gaps between expert and student, our TransFuser v6 (TFv6) student policy achieves a new state of the art on all major publicly available CARLA closed-loop benchmarks, reaching 95 DS on Bench2Drive and more than doubling prior performances on Longest6~v2 and Town13. Additionally, by integrating perception supervision from our dataset into a shared sim-to-real pipeline, we show consistent gains on the NAVSIM and Waymo Vision-Based End-to-End driving benchmarks. Our code, data, and models are publicly available at https://github.com/autonomousvision/lead.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是一篇关于“LEAD: Minimizing Learner-Expert Asymmetry in End-to-End Driving”的论文的中文摘要，重点关注其研究问题、创新点、结果、局限性和未来方向：</p>
<p><strong>论文题目：</strong> LEAD: Minimizing Learner-Expert Asymmetry in End-to-End Driving</p>
<p><strong>作者：</strong> Long Nguyen, Micha Fauth, Bernhard Jaeger, Daniel Dauner, Maximilian Igl, Andreas Geiger, Kashyap Chitta</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题：</strong>
该论文旨在解决自动驾驶领域中，尽管模拟器可以生成海量驾驶数据，但基于模仿学习（Imitation Learning, IL）的策略在模拟环境中仍然难以实现鲁棒的闭环性能这一核心问题。研究发现，这种性能瓶颈主要源于“特权专家演示”与“基于传感器观察的学生模型”之间的<strong>不对称性（asymmetry）</strong>。具体来说，专家在<strong>可见性（visibility）</strong>和<strong>不确定性（uncertainty）</strong>方面拥有显著优势（例如，专家能看到被遮挡的物体，且对其他车辆的意图有更低的认知不确定性），这使得学生模型难以可靠地模仿。此外，导航<strong>意图（intent）</strong>在测试时通常通过单一目标点来指定，这对于复杂的多步驾驶任务来说信息不足。</p>
<p><strong>2. 关键创新与方法贡献：</strong>
为了解决上述不对称性问题，论文提出了以下主要贡献：</p>
<ul>
<li><strong>LEAD数据集与专家：</strong> 作者构建了一个名为LEAD的新型专家和数据集，该专家经过精心设计，以减少与学生模型之间的可见性和不确定性不对称。它通过约束专家使用的输入信号，使其更接近学生模型通过传感器能获取的信息，从而生成更易于模仿的演示。</li>
<li><strong>意图对齐（Intent Alignment）：</strong> 论文深入分析了目标点偏差（target point bias）问题，并提出通过改进导航意图的指定和注入方式来解决。具体而言，他们移除了原有的GRU模块，并将目标点作为显式token与BEV（Bird's Eye View）特征一同处理，以及采用了三点（过去、当前、未来）的导航点表示，以提供更丰富、更及时的导航信息。</li>
<li><strong>TransFuser v6 (TFv6) 模型：</strong> 基于上述改进，作者提出了TransFuser v6（TFv6）模型，该模型在对齐的专家演示和改进的意图对齐下进行训练，实现了显著的性能提升。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong>
*   <strong>CARLA闭环基准测试：</strong> TFv6在CARLA模拟器上的多个主要闭环基准测试中取得了<strong>新的SOTA（State-of-the-Art）性能</strong>。例如，在Bench2Drive上达到了95 DS，在Longest6 v2和Town13上性能翻倍。这表明通过解决学习者-专家不对称性，可以显著提升模仿学习在复杂驾驶任务中的闭环表现。
*   <strong>Sim-to-Real迁移：</strong> 将LEAD数据集的感知监督整合到共享的Sim-to-Real流水线中，在NAVSIM和Waymo等真实世界基准测试中也展示了<strong>一致的性能提升</strong>，证明了该方法在真实世界场景中的迁移能力。
*   <strong>对专家设计重要性的强调：</strong> 研究结果有力地证明了专家策略的设计对于模仿学习的有效性至关重要，尤其是在模拟环境中。这为未来研究如何设计更有效的专家策略提供了指导。</p>
<p><strong>4. 论文提及的局限性：</strong>
*   <strong>脱离路线恢复（Off-route Recovery）：</strong> TFv6主要在路线内数据上进行训练，对于大幅度偏离路线的情况，其恢复能力有限，可能需要DAgger或强化学习等额外的训练策略。
*   <strong>复杂机动（Complex Maneuvers）：</strong> 该模型在处理需要多次急剧变道的复杂高速公路出口等场景时表现不佳，这些场景在人类驾驶中不常见，但常出现在基准测试设计中。
*   <strong>Sim-to-Real的局限：</strong> 虽然研究展示了Sim-to-Real的迁移能力，但主要集中在感知协同训练，并未直接解决规划层面的Sim-to-Real问题。此外，当前的Sim-to-Real评估主要局限于开环和伪闭环基准。
*   <strong>专家设计范围：</strong> 当前的专家设计方法主要针对模拟环境中的规则型专家，其对学习型专家或真实人类演示的适用性仍需进一步研究。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   开发更有效的脱离路线恢复策略。
*   研究如何处理更复杂的、需要精细协调的驾驶机动。
*   探索更全面的Sim-to-Real方法，包括规划层面的协同训练，并实现真正的闭环真实世界验证。
*   将专家对齐的原则推广到学习型专家和真实人类演示，并探索其在其他机器人领域中的应用。</p>
<p>总而言之，这篇论文通过系统地分析和解决模仿学习中学习者与专家之间的不对称性，特别是可见性、不确定性和意图指定方面的问题，显著提升了端到端自动驾驶策略在模拟环境中的性能，并展示了其在真实世界数据上的迁移潜力。其提出的LEAD数据集和TFv6模型是该领域的重要贡献。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We demonstrate that these asymmetries can measurably limit driving performance in CARLA and offer practical interventions to address them.</li>
<li>After careful modifications to narrow the gaps between expert and student, our TransFuser v6 (TFv6) student policy achieves a new state of the art on all major publicly available CARLA closed-loop benchmarks, reaching 95 DS on Bench2Drive and more than doubling prior performances on Longest6~v2 and Town13.</li>
<li>Additionally, by integrating perception supervision from our dataset into a shared sim-to-real pipeline, we show consistent gains on the NAVSIM and Waymo Vision-Based End-to-End driving benchmarks.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.20563v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.20563v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.20561v1'></a></p>
<h2 id="flashvlm-text-guided-visual-token-selection-for-large-multimodal-models"><a href="https://arxiv.org/abs/2512.20561v1">FlashVLM: Text-Guided Visual Token Selection for Large Multimodal Models</a></h2>
<p><strong>Authors:</strong> Kaitong Cai, Jusheng Zhang, Jing Yang, Yijia Fan, Pengtao Xie, Jian Wang, Keze Wang</p>
<p><strong>Published:</strong> 2025-12-23</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Large vision-language models (VLMs) typically process hundreds or thousands of visual tokens per image or video frame, incurring quadratic attention cost and substantial redundancy. Existing token reduction methods often ignore the textual query or rely on deep attention maps, whose instability under aggressive pruning leads to degraded semantic alignment.   We propose FlashVLM, a text guided visual token selection framework that dynamically adapts visual inputs to the query. Instead of relying on noisy attention weights, FlashVLM computes an explicit cross modal similarity between projected image tokens and normalized text embeddings in the language model space. This extrinsic relevance is fused with intrinsic visual saliency using log domain weighting and temperature controlled sharpening. In addition, a diversity preserving partition retains a minimal yet representative set of background tokens to maintain global context.   Under identical token budgets and evaluation protocols, FlashVLM achieves beyond lossless compression, slightly surpassing the unpruned baseline while pruning up to 77.8 percent of visual tokens on LLaVA 1.5, and maintaining 92.8 percent accuracy even under 94.4 percent compression. Extensive experiments on 14 image and video benchmarks demonstrate that FlashVLM delivers state of the art efficiency performance trade offs while maintaining strong robustness and generalization across mainstream VLMs.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“FlashVLM: Text-Guided Visual Token Selection for Large Multimodal Models”的全面中文摘要：</p>
<p><strong>论文标题：</strong> FlashVLM: Text-Guided Visual Token Selection for Large Multimodal Models</p>
<p><strong>作者：</strong> Kaitong Cai, Jusheng Zhang, Jing Yang, Yijia Fan, Pengtao Xie, Jian Wang, Keze Wang</p>
<hr />
<p><strong>摘要</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
大型视觉语言模型（VLMs）在处理图像或视频时，通常会生成大量的视觉标记（tokens），这导致了二次方的注意力计算成本和显著的信息冗余。现有的标记减少方法要么忽略文本查询，要么依赖于深度注意力图，而这些方法在激进的剪枝下往往会导致语义对齐能力下降，性能不稳定。因此，研究如何高效且准确地选择视觉标记，以在降低计算成本的同时保持甚至提升模型性能，是当前面临的关键挑战。</p>
<p><strong>2. 关键创新与方法论贡献：</strong>
FlashVLM 提出了一种新颖的、<strong>文本引导的视觉标记选择框架</strong>，该框架能够动态地根据查询调整视觉输入。其核心创新点包括：</p>
<ul>
<li><strong>显式的跨模态相似性计算：</strong> FlashVLM 不依赖于不稳定的注意力权重，而是通过计算投影后的图像标记与语言模型（LLM）空间中归一化的文本嵌入之间的<strong>显式跨模态相似性</strong>，来衡量视觉标记与查询的关联度。</li>
<li><strong>融合内在与外在信号：</strong> 该方法将<strong>内在的视觉显著性</strong>（与查询无关）与<strong>外在的查询相关性</strong>（与查询相关）通过<strong>对数域加权</strong>和<strong>温度控制的锐化</strong>进行融合，生成一个稳定且可解释的查询信号。</li>
<li><strong>多样性保持的分割：</strong> 为了维持全局上下文，FlashVLM 引入了一个<strong>多样性保持的分割机制</strong>，保留一小组非冗余的背景标记，以防止信息丢失或“特征塌陷”。</li>
<li><strong>单次选择与架构无关：</strong> 整个选择过程在编码器-解码器接口处<strong>一次性完成</strong>，无需修改 Transformer 层，并且与 FlashAttention 等优化技术完全兼容，具有良好的部署性和通用性。</li>
</ul>
<p><strong>3. 主要研究成果与意义：</strong>
FlashVLM 在多个图像和视频基准测试中取得了显著的成果：</p>
<ul>
<li><strong>“超越无损”压缩：</strong> 在相同的标记预算和评估协议下，FlashVLM 实现了“超越无损”的压缩效果，即在剪枝高达 77.8% 的视觉标记（在 LLaVA-1.5 上保留 128 个标记）时，性能略微超过了未剪枝的基线模型。</li>
<li><strong>极端压缩下的鲁棒性：</strong> 即使在高达 94.4% 的压缩率下（保留 32 个标记），FlashVLM 仍能保持 92.8% 的准确率，并且在 LLaVA、Qwen-VL、InternVL 和 CogVLM 等主流 VLM 上表现出一致的性能提升。</li>
<li><strong>高效能权衡与通用性：</strong> FlashVLM 在 14 个图像-视频基准测试中展示了<strong>最先进的效率-性能权衡</strong>，在保持强大鲁棒性的同时，对主流 VLM 架构具有广泛的通用性。</li>
<li><strong>理论分析：</strong> 论文还提供了理论分析，证明了 FlashVLM 的多样性保持分割机制能够将计算复杂度降低到<strong>渐进次二次方</strong>（Õ(N log N)），并保证了<strong>δ-覆盖</strong>，防止了语义塌陷。</li>
</ul>
<p><strong>4. 论文提及的局限性：</strong>
*   FlashVLM 的性能在一定程度上<strong>依赖于投影后的视觉嵌入的质量</strong>。
*   对于需要<strong>极精细粒度</strong>的任务，可能需要更高的标记预算。
*   目前的单次选择机制<strong>尚未集成多步细化或时间反馈</strong>，这可能会限制其在某些复杂场景下的鲁棒性和适应性。</p>
<p><strong>5. 未来研究方向：</strong>
*   探索更高级的单次选择机制，例如集成多步细化或时间反馈，以进一步提升鲁棒性和适应性。
*   研究 FlashVLM 在需要更高标记预算的极精细粒度任务上的表现。
*   进一步探索 FlashVLM 在更广泛的 VLM 架构和更具挑战性的多模态任务上的应用潜力。</p>
<p><strong>总结：</strong>
FlashVLM 是一项重要的研究成果，它通过一种新颖的、文本引导的视觉标记选择方法，有效地解决了大型 VLM 中存在的计算成本高和信息冗余问题。其核心贡献在于利用显式的跨模态相似性来指导标记选择，并结合内在显著性与多样性保持策略，实现了在大幅降低计算量的同时，保持甚至提升模型性能。FlashVLM 的通用性、鲁棒性和效率使其成为未来高效多模态模型研究的重要方向。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We propose FlashVLM, a text guided visual token selection framework that dynamically adapts visual inputs to the query.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.20561v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.20561v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.20557v1'></a></p>
<h2 id="learning-to-reason-in-4d-dynamic-spatial-understanding-for-vision-language-models"><a href="https://arxiv.org/abs/2512.20557v1">Learning to Reason in 4D: Dynamic Spatial Understanding for Vision Language Models</a></h2>
<p><strong>Authors:</strong> Shengchao Zhou, Yuxin Chen, Yuying Ge, Wei Huang, Jiehong Lin, Ying Shan, Xiaojuan Qi</p>
<p><strong>Published:</strong> 2025-12-23</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Vision-language models (VLM) excel at general understanding yet remain weak at dynamic spatial reasoning (DSR), i.e., reasoning about the evolvement of object geometry and relationship in 3D space over time, largely due to the scarcity of scalable 4D-aware training resources. To bridge this gap across aspects of dataset, benchmark and model, we introduce DSR Suite. First, we propose an automated pipeline that generates multiple-choice question-answer pairs from in-the-wild videos for DSR. By leveraging modern vision foundation models, the pipeline extracts rich geometric and motion information, including camera poses, local point clouds, object masks, orientations, and 3D trajectories. These geometric cues enable the construction of DSR-Train for learning and further human-refined DSR-Bench for evaluation. Compared with previous works, our data emphasize (i) in-the-wild video sources, (ii) object- and scene-level 3D requirements, (iii) viewpoint transformations, (iv) multi-object interactions, and (v) fine-grained, procedural answers. Beyond data, we propose a lightweight Geometry Selection Module (GSM) to seamlessly integrate geometric priors into VLMs, which condenses question semantics and extracts question-relevant knowledge from pretrained 4D reconstruction priors into a compact set of geometry tokens. This targeted extraction avoids overwhelming the model with irrelevant knowledge. Experiments show that integrating DSR-Train and GSM into Qwen2.5-VL-7B significantly enhances its dynamic spatial reasoning capability, while maintaining accuracy on general video understanding benchmarks.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“Learning to Reason in 4D: Dynamic Spatial Understanding for Vision Language Models”的全面中文摘要：</p>
<p><strong>论文题目：</strong> Learning to Reason in 4D: Dynamic Spatial Understanding for Vision Language Models</p>
<p><strong>作者：</strong> Shengchao Zhou, Yuxin Chen, Yuying Ge, Wei Huang, Jiehong Lin, Ying Shan, Xiaojuan Qi</p>
<p><strong>摘要：</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong>
当前主流的视觉语言模型（VLMs）在理解通用场景方面表现出色，但在<strong>动态空间推理（DSR）</strong>方面存在显著不足。DSR是指在三维空间中理解物体几何形状和它们之间关系随时间演变的能力。这种能力对于机器人、自动驾驶、增强现实/虚拟现实以及具身智能等领域至关重要。然而，由于缺乏大规模、高质量的4D（三维空间+时间）感知训练资源，VLMs在DSR方面的发展受到严重阻碍。</p>
<p><strong>2. 主要创新点/方法论贡献：</strong>
为了解决上述问题，作者提出了一个名为<strong>DSR Suite</strong>的综合性框架，该框架涵盖了数据集、基准测试和模型三个关键方面：</p>
<ul>
<li><strong>自动化数据生成流水线：</strong> 作者开发了一个创新的自动化流水线，能够从<strong>真实世界视频（in-the-wild videos）</strong>中生成大量的多项选择题和精细化答案对，用于DSR任务。该流水线利用现代视觉基础模型提取丰富的几何和运动信息，包括：<ul>
<li><strong>相机位姿 (Camera Poses)</strong></li>
<li><strong>局部点云 (Local Point Clouds)</strong></li>
<li><strong>物体掩码 (Object Masks)</strong></li>
<li><strong>物体朝向 (Orientations)</strong></li>
<li><strong>三维轨迹 (3D Trajectories)</strong></li>
</ul>
</li>
<li><strong>DSR-Train 数据集：</strong> 基于上述流水线生成的大规模多项选择题数据集，专门用于训练VLMs掌握DSR能力。</li>
<li><strong>DSR-Bench 基准测试：</strong> 一个经过人工精炼的评估基准，用于全面评估模型在DSR方面的表现。DSR-Bench的特点包括：<ul>
<li><strong>真实世界视频源：</strong> 强调在复杂、动态的环境中进行评估。</li>
<li><strong>物体和场景级别的三维要求：</strong> 考察模型对物体和整体场景的三维理解。</li>
<li><strong>视角变换 (Viewpoint Transformations)：</strong> 评估模型在不同观察视角下的推理能力。</li>
<li><strong>多物体交互 (Multi-object Interactions)：</strong> 考察模型对多个物体之间复杂关系的理解。</li>
<li><strong>精细化、程序化的答案 (Fine-grained, Procedural Answers)：</strong> 要求模型给出详细、描述性的答案，而非简单的分类。</li>
</ul>
</li>
<li><strong>轻量级几何选择模块 (Geometry Selection Module, GSM)：</strong> 为了有效地将预训练的3D基础模型中的几何先验知识整合到VLMs中，作者提出了GSM。GSM采用双Q-Former设计：<ul>
<li>第一个Q-Former负责<strong>凝练问题语义</strong>。</li>
<li>第二个Q-Former则根据问题语义，从预训练的4D重建先验中<strong>提取与问题相关的几何知识</strong>，并将其压缩成一小组紧凑的几何Token。</li>
<li>这种<strong>选择性提取</strong>避免了将大量无关的几何信息涌入模型，从而减轻了对通用视频理解能力的负面影响，实现了在增强DSR能力的同时保持通用性能。</li>
</ul>
</li>
</ul>
<p><strong>3. 主要结果及意义：</strong>
*   <strong>DSR Suite的有效性：</strong> 作者通过实验证明，DSR Suite（包括DSR-Train和GSM）能够显著提升VLMs的动态空间推理能力。
*   <strong>模型性能提升：</strong> 将DSR-Train和GSM集成到Qwen2.5-VL-7B模型中后，在DSR-Bench基准测试上取得了<strong>最先进的性能</strong>。
*   <strong>通用能力保持：</strong> 重要的是，这种提升并没有以牺牲模型在通用视频理解基准上的性能为代价，证明了GSM的有效性。
*   <strong>数据集和基准的价值：</strong> DSR Suite为研究和评估VLMs的4D动态空间推理能力提供了一个<strong>可扩展、高质量的资源</strong>，填补了该领域的重要空白。</p>
<p><strong>4. 提及的局限性：</strong>
*   论文中提到，虽然GSM在整合几何先验方面表现出色，但<strong>过多的查询数量可能会损害通用视频理解性能</strong>，因此需要仔细设置查询数量。
*   在评估GSM时，作者提到<strong>增加查询数量会提高动态空间推理性能，但也会损害通用视频理解性能</strong>，表明在查询数量上需要权衡。</p>
<p><strong>5. 潜在的未来研究方向：</strong>
*   论文的结论部分展望了DSR Suite和所提出的方法可以<strong>促进4D多模态智能的未来工作</strong>，包括<strong>具身感知（embodied perception）、预测性推理（predictive reasoning）和动态环境中的世界建模（world modeling in dynamic environments）</strong>。
*   作者还通过实验探索了将DSR-Train与静态空间推理数据混合训练，以及将模型应用于<strong>下游的Agent任务（如MineDojo）</strong>，展示了DSR能力在更广泛领域的应用潜力。</p>
<p><strong>总结：</strong>
这篇论文<strong>“Learning to Reason in 4D: Dynamic Spatial Understanding for Vision Language Models”</strong> 提出了一个名为<strong>DSR Suite</strong>的创新框架，旨在解决当前视觉语言模型在动态空间推理（DSR）方面的不足。通过开发一个<strong>自动化数据生成流水线</strong>，作者构建了<strong>DSR-Train数据集</strong>和<strong>DSR-Bench基准测试</strong>，为DSR研究提供了急需的资源。此外，提出的<strong>轻量级几何选择模块（GSM）</strong>能够有效地将3D几何先验知识整合到VLMs中，并在增强DSR能力的同时保持通用性能。实验结果表明，DSR Suite和GSM显著提升了模型的DSR能力，并在DSR-Bench上取得了最先进的性能。这项工作为推动多模态模型在动态三维环境中的理解和推理能力迈出了重要一步。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>To bridge this gap across aspects of dataset, benchmark and model, we introduce DSR Suite.</li>
<li>First, we propose an automated pipeline that generates multiple-choice question-answer pairs from in-the-wild videos for DSR.</li>
<li>Beyond data, we propose a lightweight Geometry Selection Module (GSM) to seamlessly integrate geometric priors into VLMs, which condenses question semantics and extracts question-relevant knowledge from pretrained 4D reconstruction priors into a compact set of geometry tokens.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.20557v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.20557v1">arXiv</a></li>
</ul>
<hr />
<p><a id='2512.20538v1'></a></p>
<h2 id="alignpose-generalizable-6d-pose-estimation-via-multi-view-feature-metric-alignment"><a href="https://arxiv.org/abs/2512.20538v1">AlignPose: Generalizable 6D Pose Estimation via Multi-view Feature-metric Alignment</a></h2>
<p><strong>Authors:</strong> Anna Šárová Mikeštíková, Médéric Fourmy, Martin Cífka, Josef Sivic, Vladimir Petrik</p>
<p><strong>Published:</strong> 2025-12-23</p>
<p><strong>Categories:</strong> cs.CV</p>
<p><strong>Abstract:</strong></p>
<p>Single-view RGB model-based object pose estimation methods achieve strong generalization but are fundamentally limited by depth ambiguity, clutter, and occlusions. Multi-view pose estimation methods have the potential to solve these issues, but existing works rely on precise single-view pose estimates or lack generalization to unseen objects. We address these challenges via the following three contributions. First, we introduce AlignPose, a 6D object pose estimation method that aggregates information from multiple extrinsically calibrated RGB views and does not require any object-specific training or symmetry annotation. Second, the key component of this approach is a new multi-view feature-metric refinement specifically designed for object pose. It optimizes a single, consistent world-frame object pose minimizing the feature discrepancy between on-the-fly rendered object features and observed image features across all views simultaneously. Third, we report extensive experiments on four datasets (YCB-V, T-LESS, ITODD-MV, HouseCat6D) using the BOP benchmark evaluation and show that AlignPose outperforms other published methods, especially on challenging industrial datasets where multiple views are readily available in practice.</p>
<p><strong>Analysis:</strong></p>
<p>好的，这是对论文“AlignPose: Generalizable 6D Pose Estimation via Multi-view Feature-metric Alignment”的全面中文摘要：</p>
<p><strong>论文题目：</strong> AlignPose: 通过多视角特征度量对齐实现可泛化的6D姿态估计</p>
<p><strong>作者：</strong> Anna Šárová Mikeštíková, Médéric Fourmy, Martin Cífka, Josef Sivic, Vladimir Petrik</p>
<p><strong>摘要</strong></p>
<p><strong>1. 研究问题/核心挑战：</strong></p>
<p>该论文旨在解决单视角RGB模型驱动的6D物体姿态估计方法在深度模糊、遮挡和杂乱场景下的泛化能力受限的问题。现有的多视角方法要么依赖于精确的单视角姿态估计，要么缺乏对未见过的物体的泛化能力。因此，研究的核心问题是如何在不依赖物体特定训练的情况下，有效地利用多视角RGB图像来提高6D物体姿态估计的准确性和泛化能力。</p>
<p><strong>2. 主要创新点/方法论贡献：</strong></p>
<p>作者提出了AlignPose，一种新颖的多视角6D物体姿态估计方法，其核心创新点包括：</p>
<ul>
<li><strong>AlignPose方法：</strong> 该方法能够聚合来自多个外参标定的RGB视图的信息，并且不需要任何物体特定的训练或对称性标注。</li>
<li><strong>多视角特征度量精炼（Multi-view Feature-metric Refinement）：</strong> 这是AlignPose的关键组成部分。它通过优化一个单一、一致的世界坐标系下的物体姿态，最小化了在运行时渲染的物体特征与所有视图中观察到的图像特征之间的差异。这种方法能够同时处理来自多个视图的信息，从而提高姿态估计的鲁棒性和准确性。</li>
<li><strong>无监督泛化能力：</strong> AlignPose利用预训练的视觉基础模型（如DINOv2）作为特征提取器，实现了对未见过物体的零样本泛化能力，无需进行物体特定的训练。</li>
<li><strong>3D非极大值抑制（3D NMS）：</strong> 在聚合阶段，使用3D NMS来过滤冗余的单视角姿态候选，确保得到一个精简且唯一的物体姿态集合。</li>
</ul>
<p><strong>3. 主要结果与意义：</strong></p>
<ul>
<li><strong>性能优越：</strong> 在YCB-V、T-LESS、ITODD-MV和HouseCat6D四个数据集上进行了广泛的实验评估，AlignPose在BOP基准测试中显著优于其他已发表的多视角方法。</li>
<li><strong>工业场景优势：</strong> 该方法在具有挑战性的工业数据集上表现尤为突出，这些数据集通常提供多个视角，而AlignPose能够有效地利用这些多视角信息。</li>
<li><strong>泛化能力：</strong> AlignPose在处理未见过物体方面表现出色，证明了其强大的泛化能力，这对于实际应用至关重要。</li>
<li><strong>鲁棒性：</strong> 相较于CosyPose等基线方法，AlignPose在处理稀疏或有噪声的单视角估计时表现出更好的鲁棒性，能够恢复出更一致的姿态。</li>
</ul>
<p><strong>4. 提及的局限性：</strong></p>
<ul>
<li><strong>对单视角候选的依赖：</strong> 虽然AlignPose能够精炼单视角估计，但其最终性能仍受到初始单视角姿态候选的质量影响。</li>
<li><strong>计算成本：</strong> 虽然精炼过程很快（每秒可处理多个检测），但多视角处理本身会增加一定的计算开销。</li>
<li><strong>对相机标定的依赖：</strong> 该方法需要准确的相机内参和外参标定。</li>
<li><strong>对纹理信息的需求：</strong> 虽然方法对纹理较少的物体也有一定的鲁棒性，但其特征度量对齐的有效性在一定程度上依赖于图像中存在可提取的特征。</li>
</ul>
<p><strong>5. 潜在的未来研究方向：</strong></p>
<ul>
<li><strong>进一步提升对纹理稀少和反光物体的鲁棒性：</strong> 尽管论文在这些方面有所进展，但仍有进一步提升的空间。</li>
<li><strong>减少对精确相机标定的依赖：</strong> 探索在相机标定不那么精确的情况下也能获得良好性能的方法。</li>
<li><strong>端到端训练的优化：</strong> 研究是否可以将整个流程进行端到端训练，以进一步提升性能。</li>
<li><strong>实时性提升：</strong> 进一步优化算法以满足更严格的实时性要求，例如在机器人抓取等场景中。</li>
<li><strong>结合深度信息：</strong> 虽然是RGB方法，但探索如何更有效地融合深度信息（如果可用）来进一步提升精度。</li>
</ul>
<p>总而言之，AlignPose通过引入一种新颖的多视角特征度量精炼方法，有效地解决了单视角姿态估计的局限性，并在不依赖物体特定训练的情况下实现了优异的泛化能力和鲁棒性，为6D物体姿态估计领域带来了重要的进展，尤其是在需要多视角信息的工业应用场景中。</p>
<p><strong>Key Findings:</strong></p>
<ul>
<li>We address these challenges via the following three contributions.</li>
<li>First, we introduce AlignPose, a 6D object pose estimation method that aggregates information from multiple extrinsically calibrated RGB views and does not require any object-specific training or symmetry annotation.</li>
<li>Second, the key component of this approach is a new multi-view feature-metric refinement specifically designed for object pose.</li>
<li>Third, we report extensive experiments on four datasets (YCB-V, T-LESS, ITODD-MV, HouseCat6D) using the BOP benchmark evaluation and show that AlignPose outperforms other published methods, especially on challenging industrial datasets where multiple views are readily available in practice.</li>
</ul>
<p><strong>Links:</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/2512.20538v1">PDF</a></li>
<li><a href="https://arxiv.org/abs/2512.20538v1">arXiv</a></li>
</ul>
<hr />

</div>
                    
                </div>

            <script src="https://utteranc.es/client.js"
                repo="Owen-Liuyuxuan/papers_reading_sharing.github.io"
                issue-term="Arxiv Computer Vision Papers - 2025-12-24 comments"
                theme="github-light"
                crossorigin="anonymous"
                async>
            </script>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
